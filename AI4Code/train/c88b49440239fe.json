{"cell_type":{"a615766a":"code","b99b5a1c":"code","df70477d":"code","600ef414":"code","d3c8bb89":"code","5a640c8c":"code","9755fbb2":"code","7982b81d":"code","bcc445f4":"code","0ec778c3":"code","19794280":"code","fad430d3":"code","fcc5824a":"code","dc8d524a":"code","98c1ca5a":"code","9b084a35":"code","85e6f809":"code","21e0bcdb":"code","e6d3065c":"code","30ce4c93":"code","63733d54":"code","d6ad7b94":"code","c60ec2b9":"code","546c441c":"code","52bedc40":"code","025090ec":"code","cc50ab26":"code","d88ec901":"code","5bb00e63":"code","9fa23268":"code","32ee7a3d":"code","24d5a2f1":"code","4309e060":"code","417e2f05":"code","e76f239e":"code","47c124de":"code","7b466081":"code","4a3dcbce":"code","34a816d0":"code","4503218b":"code","3ecb806b":"code","081ae7b6":"code","e1c4ee1c":"code","24d047ee":"code","f672ee56":"code","cde19566":"code","811233a1":"code","784cc4ce":"code","2ec07734":"code","7748daa1":"code","62899d21":"code","d1b786af":"code","d1963b45":"code","d8939265":"code","f44e6aa5":"code","8623b313":"code","620bec32":"code","611b169f":"code","c733c273":"code","a619ac74":"markdown","4eda2b4b":"markdown","87214161":"markdown","b6df06c9":"markdown","c338eb75":"markdown","1b411e02":"markdown","b6eaa900":"markdown","1bc555c3":"markdown","40067a30":"markdown","af1448b4":"markdown","99690a08":"markdown","e43bc090":"markdown","98102585":"markdown","7a8a906e":"markdown","6241e523":"markdown","dcacc6bc":"markdown"},"source":{"a615766a":"#REQUIRED LIBRARIES\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression,RidgeClassifier\n# from sklearn.model_selection import GridSearchCV\nfrom sklearn.decomposition import PCA\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.metrics import roc_auc_score,accuracy_score\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.cluster import KMeans\nfrom sklearn.utils.extmath import softmax\n# import pickle\n# from sklearn.externals import joblib\n\n# import pandas_profiling as pp\n\nwarnings.filterwarnings('ignore')\ngc.enable()\n%matplotlib inline","b99b5a1c":"#CHECKING ALL AVAILABLE FILES\npath='\/kaggle\/input\/tabular-playground-series-oct-2021\/'\ndata_files=list(os.listdir(path))\ndf_files=pd.DataFrame(data_files,columns=['file_name'])\ndf_files['size_in_gb']=df_files.file_name.apply(lambda x: round(os.path.getsize(path+x)\/(1024*1024*1024),4))\ndf_files['type']=df_files.file_name.apply(lambda x:'file' if os.path.isfile(path+x) else 'directory')\ndf_files['file_count']=df_files[['file_name','type']].apply(lambda x: 0 if x['type']=='file' else len(os.listdir(path+x['file_name'])),axis=1)\n\nprint('Following files are available under path:',path)\ndisplay(df_files)","df70477d":"#ALL CUSTOM FUNCTIONS\n\n#FUNCTION FOR PROVIDING FEATURE SUMMARY\ndef feature_summary(df_fa):\n    print('DataFrame shape')\n    print('rows:',df_fa.shape[0])\n    print('cols:',df_fa.shape[1])\n    col_list=['null','unique_count','data_type','max\/min','mean','median','mode','std','skewness','sample_values']\n    df=pd.DataFrame(index=df_fa.columns,columns=col_list)\n    df['null']=list([len(df_fa[col][df_fa[col].isnull()]) for i,col in enumerate(df_fa.columns)])\n    #df['%_Null']=list([len(df_fa[col][df_fa[col].isnull()])\/df_fa.shape[0]*100 for i,col in enumerate(df_fa.columns)])\n    df['unique_count']=list([len(df_fa[col].unique()) for i,col in enumerate(df_fa.columns)])\n    df['data_type']=list([df_fa[col].dtype for i,col in enumerate(df_fa.columns)])\n    for i,col in enumerate(df_fa.columns):\n        if 'float' in str(df_fa[col].dtype) or 'int' in str(df_fa[col].dtype):\n            df.at[col,'max\/min']=str(round(df_fa[col].max(),2))+'\/'+str(round(df_fa[col].min(),2))\n            df.at[col,'mean']=round(df_fa[col].mean(),4)\n            df.at[col,'median']=round(df_fa[col].median(),4)\n            df.at[col,'mode']=round(df_fa[col].mode()[0],4)\n            df.at[col,'std']=round(df_fa[col].std(),4)\n            df.at[col,'skewness']=round(df_fa[col].skew(),4)\n        elif 'datetime64[ns]' in str(df_fa[col].dtype):\n            df.at[col,'max\/min']=str(df_fa[col].max())+'\/'+str(df_fa[col].min())\n        df.at[col,'sample_values']=list(df_fa[col].unique())\n    display(df_fa.head())      \n    return(df.fillna('-'))\n\n#EXTENDING RIDGE CLASSIFIER WITH PREDICT PROBABILITY FUNCITON\n\nclass RidgeClassifierwithProba(RidgeClassifier):\n    def predict_proba(self, X):\n        d = self.decision_function(X)\n        d_2d = np.c_[-d, d]\n        return softmax(d_2d)\n\n#PREDICTION FUNCTIONS\n\ndef response_predictor(X,y,test,model,model_name):  \n\n    df_preds=pd.DataFrame()\n    df_preds_x=pd.DataFrame()\n    k=1\n    splits=5\n    avg_score=0\n\n    #CREATING STRATIFIED FOLDS\n    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=200)\n    print('\\nStarting KFold iterations...')\n    for train_index,test_index in skf.split(X,y):\n        df_X=X[train_index,:]\n        df_y=y[train_index]\n        val_X=X[test_index,:]\n        val_y=y[test_index]\n       \n\n    #FITTING MODEL\n        model.fit(df_X,df_y)\n\n    #PREDICTING ON VALIDATION DATA\n        col_name=model_name+'xpreds_'+str(k)\n        preds_x=pd.Series(model.predict_proba(val_X)[:,1])\n        df_preds_x[col_name]=pd.Series(model.predict_proba(X)[:,1])\n\n    #CALCULATING ACCURACY\n        acc=roc_auc_score(val_y,preds_x)\n        print('Iteration:',k,'  roc_auc_score:',acc)\n        if k==1:\n            score=acc\n            best_model=model\n            preds=pd.Series(model.predict_proba(test)[:,1])\n            col_name=model_name+'preds_'+str(k)\n            df_preds[col_name]=preds\n        else:\n            preds1=pd.Series(model.predict_proba(test)[:,1])\n            preds=preds+preds1\n            col_name=model_name+'preds_'+str(k)\n            df_preds[col_name]=preds1\n            if score<acc:\n                score=acc\n                best_model=model\n        avg_score=avg_score+acc        \n        k=k+1\n    print('\\n Best score:',score,' Avg Score:',avg_score\/splits)\n    #TAKING AVERAGE OF PREDICTIONS\n    preds=preds\/splits\n    \n    print('Saving test and train predictions per iteration...')\n    df_preds.to_csv(model_name+'.csv',index=False)\n    df_preds_x.to_csv(model_name+'_.csv',index=False)\n    x_preds=df_preds_x.mean(axis=1)\n    del df_preds,df_preds_x\n    gc.collect()\n    return preds,best_model,x_preds ","600ef414":"%%time\n#READING TRAIN DATASET\n\ndf_train=pd.read_csv(path+'train.csv')","d3c8bb89":"pd.set_option('display.max_rows', len(df_train.columns))\nfeature_summary(df_train)\n\n#EXPAND BELOW SECTION TO SEE DETAILED ANALYSIS","5a640c8c":"#CREATING LIST OF CATEGORICAL AND CONTINUOUS FEATURES\nfeatures=[feature for i,feature in enumerate(df_train.columns) if feature!='id' and feature!='target']\ncat_features=[feature for i,feature in enumerate(df_train.columns) if df_train[feature].dtype=='int64' and feature!='id' and feature!='target']\ncont_features=[feature for i,feature in enumerate(df_train.columns) if df_train[feature].dtype=='float64']\n\nprint('Number of all features excluding id and target: ',len(features),'\\nNumber of Categorical features excluding id and target: ',len(cat_features),\n      '\\nNumber of Continuous features: ',len(cont_features))","9755fbb2":"gc.collect()","7982b81d":"#CHECKING TRAINING DATASET MEMORY USAGE BEFORE DOWNCASTING\ndf_train.info(memory_usage='deep')","bcc445f4":"%%time\n#DOWNCASTING TRAINING DATASET\nfor column in df_train.columns:\n    if df_train[column].dtype == \"float64\":\n        df_train[column]=pd.to_numeric(df_train[column], downcast=\"float\")\n    if df_train[column].dtype == \"int64\":\n        df_train[column]=pd.to_numeric(df_train[column], downcast=\"integer\")","0ec778c3":"##CHECKING TRAINING DATASET MEMORY USAGE AFTER DOWNCASTING\ndf_train.info(memory_usage='deep')","19794280":"gc.collect()","fad430d3":"%%time\n#READING TEST DATASET AND SUBMISSION FILE\ndf_test=pd.read_csv(path+'test.csv')\ndf_submission=pd.read_csv(path+'sample_submission.csv')","fcc5824a":"#CHECKING TESTING DATASET MEMORY USAGE BEFORE DOWNCASTING\ndf_test.info(memory_usage='deep')","dc8d524a":"%%time\n#DOWNCASTING TESTING DATASET\nfor column in df_test.columns:\n    if df_test[column].dtype == \"float64\":\n        df_test[column]=pd.to_numeric(df_test[column], downcast=\"float\")\n    if df_test[column].dtype == \"int64\":\n        df_test[column]=pd.to_numeric(df_test[column], downcast=\"integer\")","98c1ca5a":"#CHECKING TESTING DATASET MEMORY USAGE AFTER DOWNCASTING\ndf_test.info(memory_usage='deep')","9b084a35":"gc.collect()","85e6f809":"%%time\n#Understanding Target (claim) feature distribution\npie_labels=['Response-'+str(df_train['target'][df_train.target==1].count()),'No Response-'+\n            str(df_train['target'][df_train.target==0].count())]\npie_share=[df_train['target'][df_train.target==1].count()\/df_train['target'].count(),\n           df_train['target'][df_train.target==0].count()\/df_train['target'].count()]\nfigureObject, axesObject = plt.subplots(figsize=(6,6))\npie_colors=('lightblue','lightgrey')\npie_explode=(.01,.01)\naxesObject.pie(pie_share,labels=pie_labels,explode=pie_explode,autopct='%.2f%%',colors=pie_colors,startangle=30,shadow=True)\naxesObject.axis('equal')\nplt.title('Percentage of Response - No Response Observations',color='blue',fontsize=12)\nplt.show()","21e0bcdb":"#CORRELATION CHECK CATEGORICAL FEATURES\ncorr = df_train[cat_features+['target']].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Plotting correlation heatmap\nfig,ax=plt.subplots(figsize=(20,20))\nax.set_xticklabels(labels=corr.columns,fontsize=12)\nax.set_yticklabels(labels=corr.columns,fontsize=12)\n# plt.rcParams.update({'font.size': 12})\nsns.heatmap(corr,mask=mask,cmap='tab20c',linewidth=0.1)\nplt.show()","e6d3065c":"del corr\ngc.collect()","30ce4c93":"#CORRELATION CHECK CONTINUOUS FEATURES\ncorr = df_train[cont_features+['target']].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Plotting correlation heatmap\nfig,ax=plt.subplots(figsize=(30,30))\nax.set_xticklabels(labels=corr.columns,fontsize=12)\nax.set_yticklabels(labels=corr.columns,fontsize=12)\n\n# plt.rcParams.update({'font.size': 12})\nsns.heatmap(corr,mask=mask,cmap='tab20c',linewidth=0.5)\nplt.show()","63733d54":"del corr\ngc.collect()","d6ad7b94":"#CORRELATION CHECK BETWEEN TARGET AND CATEGORICAL FEATURES\ncorr=pd.DataFrame()\ncorr['target'] = df_train[cat_features].corrwith(df_train['target'])\nplt.subplots(figsize=(3,15))\ndf=corr.sort_values(by='target', ascending=False)\nheatmap = sns.heatmap(df,annot=True,cmap='tab20c',linewidth=0.5,xticklabels=df.columns,yticklabels=df.index)\n\nheatmap.set_title('Categorical Features Correlating with target', fontdict={'fontsize':18}, pad=16)\nplt.show()","c60ec2b9":"#CORRELATION CHECK BETWEEN TARGET AND CONTINUOUS FEATURES\ncorr=pd.DataFrame()\ncorr['target'] = df_train[cont_features].corrwith(df_train['target'])\nplt.subplots(figsize=(3,50))\ndf=corr.sort_values(by='target', ascending=False)\nheatmap = sns.heatmap(df,annot=True,cmap='tab20c',linewidth=0.5,xticklabels=df.columns,yticklabels=df.index)\n\nheatmap.set_title('Continous Features Correlating with target', fontdict={'fontsize':18}, pad=16)\nplt.show()","546c441c":"del corr\ngc.collect()","52bedc40":"%%time\nX=df_train.drop(['id','target'],axis=1)\n\npca = PCA(n_components=3,random_state=200)\nprincipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents,columns = ['principal_component_1','principal_component_2','principal_component_3'])\nprincipalDf['target']=df_train['target']\n\nfig = plt.figure(figsize=(15,15))\nax = fig.add_subplot(111, projection = '3d')\n\nax.set_xlabel(\"principal_component_1\")\nax.set_ylabel(\"principal_component_2\")\nax.set_zlabel(\"principal_component_3\")\n\nsc=ax.scatter(xs=principalDf['principal_component_1'], ys=principalDf['principal_component_2'],\n              zs=principalDf['principal_component_3'],c=principalDf['target'],cmap='OrRd')\nplt.legend(*sc.legend_elements(), bbox_to_anchor=(1.05, 1), loc=2)\nplt.show()","025090ec":"del X\ngc.collect()","cc50ab26":"%%time\n#SIMPLE FEATURE ENGINEERING, CREATING SOME AGGREGATION FEATURES\ndf_train['cat_sum']=df_train[cat_features].sum(axis=1)\ndf_test['cat_sum']=df_test[cat_features].sum(axis=1)\n\ndf_train['rcat_sum']=df_train.cat_sum.apply(lambda x:45-x)\ndf_test['rcat_sum']=df_test.cat_sum.apply(lambda x:45-x)\n\ndf_train['cont_std'] = df_train[cont_features].std(axis=1)\ndf_test['cont_std'] = df_test[cont_features].std(axis=1)\n\ndf_train['cont_mean'] = df_train[cont_features].mean(axis=1)\ndf_test['cont_mean'] = df_test[cont_features].mean(axis=1)\n\nfeatures += ['cat_sum','rcat_sum','cont_std','cont_mean']","d88ec901":"gc.collect()","5bb00e63":"%%time\nscaler = StandardScaler()\ndf_train[features] = scaler.fit_transform(df_train[features])\ndf_test[features] = scaler.transform(df_test[features])","9fa23268":"X=df_train.drop(['id','target'],axis=1).to_numpy()\ny=df_train['target'].values\ntest=df_test.drop(['id'],axis=1).to_numpy()","32ee7a3d":"del df_train,df_test,scaler\ngc.collect()","24d5a2f1":"X.shape,y.shape,test.shape","4309e060":"%%time\nmodel=LogisticRegression()\nprint('Logistic Regression parameters:\\n',model.get_params())\n\nlogistic_predictions,best_logistic_model,LRpreds=response_predictor(X,y,test,model,'LR')","417e2f05":"gc.collect()","e76f239e":"df_submission['target']=logistic_predictions\n#SAVING LOGISTIC PREDICTIONS\ndf_submission.to_csv('logistic_submission.csv',index=False)\ndf_submission","47c124de":"gc.collect()","7b466081":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features\ndf_feature_impt['importance']=best_logistic_model.coef_[0]\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (15,50))\nax=sns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt)\nax.bar_label(ax.containers[0]);","4a3dcbce":"%%time\nmodel=RidgeClassifierwithProba()\nprint('Ridge Classifier parameters:\\n',model.get_params())\n\nridge_predictions,best_ridge_model,ridge_preds=response_predictor(X,y,test,model,'RC')","34a816d0":"gc.collect()","4503218b":"df_submission['target']=ridge_predictions\n#SAVING LGBM PREDICTIONS\ndf_submission.to_csv('ridge_submission.csv',index=False)\ndf_submission","3ecb806b":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features\ndf_feature_impt['importance']=best_ridge_model.coef_[0]\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (15,50))\nax=sns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt)\nax.bar_label(ax.containers[0]);","081ae7b6":"gc.collect()","e1c4ee1c":"%%time\nlgbm_params = {\n    'metric' : 'auc',\n    'objective' : 'binary',\n    'device_type': 'gpu', \n    'n_estimators': 20000, \n    'learning_rate':  0.01, \n    'min_child_weight': 256,\n    'min_child_samples': 20, \n    'reg_alpha': 10, \n    'reg_lambda': 0.1, \n    'subsample': 0.6, \n    'subsample_freq': 1, \n    'colsample_bytree': 0.4\n   }\n\nmodel=lgb.LGBMClassifier(**lgbm_params)\nprint('LGBM parameters:\\n',model.get_params())\n\nlgb_predictions,best_lgb_model,LGBpreds=response_predictor(X,y,test,model,'LGB')","24d047ee":"gc.collect()","f672ee56":"df_submission['target']=lgb_predictions\n#SAVING LGBM PREDICTIONS\ndf_submission.to_csv('lgb_submission.csv',index=False)\ndf_submission","cde19566":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features\ndf_feature_impt['importance']=best_lgb_model.feature_importances_\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (15,50))\nax=sns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt)\nax.bar_label(ax.containers[0]);","811233a1":"gc.collect()","784cc4ce":"%%time\n\ncatb_params = {\n    'eval_metric' : 'AUC',\n    'iterations': 15585,\n    'objective': 'CrossEntropy',\n    'bootstrap_type': 'Bernoulli',\n    'od_wait': 1144,\n    'learning_rate': 0.023575206684596582,\n    'reg_lambda': 36.30433203563295,\n    'random_strength': 43.75597655616195,\n    'depth': 7,\n    'min_data_in_leaf': 11,\n    'leaf_estimation_iterations': 1,\n    'subsample': 0.8227911142845009,\n    'task_type' : 'GPU',\n    'devices' : '0',\n    'verbose' : 0\n}\n\nmodel=CatBoostClassifier(**catb_params)\nprint('CatBoost paramters:\\n',model.get_params())\n\ncatb_predictions,best_catb_model,CBpreds=response_predictor(X,y,test,model,'CB')","2ec07734":"gc.collect()","7748daa1":"df_submission['target']=catb_predictions\n#SAVING CATBOOST PREDICTIONS\ndf_submission.to_csv('catb_submission.csv',index=False)\ndf_submission","62899d21":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features\ndf_feature_impt['importance']=best_catb_model.feature_importances_\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (15,50))\nax=sns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt)\nax.bar_label(ax.containers[0]);","d1b786af":"gc.collect()","d1963b45":"%%time\nxgb_params = {\n    'objective': 'binary:logistic',\n    'eval_metric' : 'auc',\n    'tree_method': 'gpu_hist',\n    'learning_rate': 0.02,\n    'n_estimators': 20_000,\n    'random_state': 42,\n    'lambda': 0.003731399945310043,\n    'alpha': 0.1660536107526955,\n    'colsample_bytree': 0.5164889907489927,\n    'subsample': 0.5869840790716806,\n    'max_depth': 18,\n    'min_child_weight': 142,\n    }\n\nmodel=xgb.XGBClassifier(**xgb_params)\nprint('XGB parameters:\\n',model.get_params())\n\nxgb_predictions,best_xgb_model,XGBpreds=response_predictor(X,y,test,model,'XGB')","d8939265":"gc.collect()","f44e6aa5":"df_submission['target']=xgb_predictions\n#SAVING LGBM PREDICTIONS\ndf_submission.to_csv('xgb_submission.csv',index=False)\ndf_submission","8623b313":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features\ndf_feature_impt['importance']=best_xgb_model.feature_importances_\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (15,55))\nax=sns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt)\nax.bar_label(ax.containers[0]);","620bec32":"gc.collect()","611b169f":"#BLENDING PREDICTIONS\ndf_submission['target']=logistic_predictions*0.05+ridge_predictions*0.05+lgb_predictions*0.1+catb_predictions*0.6+xgb_predictions*0.2\n#CREATING SUMBISSION FILE\ndf_submission.to_csv('submission.csv',index=False)","c733c273":"df_submission","a619ac74":"<h2 id=\"Approach\">Approach to the problem<\/h2>\nIdea is to develop a generalized approach for solving any binary classification problem\n<ol>\n    <li>Performing exploratory data analysis (EDA) and Data Preparation (DP).<\/li>\n    <ol>\n        <li><a href=\"#FeatureSummary\">Understanding Training dataset features (EDA)<\/a><\/li>\n        <li><a href=\"#Downcasting\">Down Casting Training and Testing datasets (DP)<\/a><\/li>\n        <li><a href=\"#Target\">Understanding Target feature distribution (EDA)<\/a><\/li>\n        <li><a href=\"#Corr\">Correlation check (EDA)<\/a><\/li>\n        <li><a href=\"#TrainVisual\">Visualizing Training dataset (EDA)<\/a><\/li>\n        <li><a href=\"#Normalizing\">Normalizing dataset (DP)<\/a><\/li>\n    <\/ol>\n    <li>Feature Engineering.<\/li>\n    <ol>\n        <li><a href=\"#AggFeatures\">Creating Aggregated features<\/a><\/li>\n    <\/ol>\n    <li>Training Linear and Gradient Boost Base models.<\/li>\n    <ol>\n        <li><a href=\"#LogisticRegression\">Logistic Regression<\/a><\/li>\n         <li><a href=\"#Ridge\">Ridge Classifier<\/a><\/li>\n        <li><a href=\"#LGBM\">LGBM Classification<\/a><\/li>\n        <li><a href=\"#CatBoost\">CatBoost Classification<\/a><\/li>\n        <li><a href=\"#XGBoost\">XGBoost Classification<\/a><\/li>\n    <\/ol>\n<!--    <li>Basic Blending.<\/li>\n    <ol>\n        <li><a href=\"#Ratios\">Calculating best blending Ratios (using training preditions to calculate blending ratios)<\/a><\/li>\n        <li><a href=\"#FinalPred\">Calculating blended prediction<\/a><\/li>\n    <\/ol> -->\n<\/ol>","4eda2b4b":"<h2 id=\"Downcasting\">Down Casting Training and Testing datasets<\/h2>\nChecking possibility for down casting dataset datatypes. This will help in reducing overall dataset size.\n\n<h4>Observations<\/h4>\n<ul>\n    <li>We have only two data types in dataset float64 and int64<\/li>\n    <li>As in most of the cases values range between 0 to 1. Therefore this is an ideal case for downcasting data types<\/li>\n    <li>With downcasting able to reduce training dataset size from 2.1 GB to 963.2 MB<\/li>\n    <li>With downcasting able to reduce training dataset size from 1.1 GB to 481.1 MB<\/li>\n<\/ul>\n\n<br><a href=\"#Approach\">back to main menu<\/a>","87214161":"<h3>CORRELATION CHECK CONTINUOUS FEATURES<\/h3>","b6df06c9":"<h2 id=\"FeatureSummary\">Understanding Training dataset features<\/h2>\nUserstanding Training dataset features using basic statistical measures\n\n<h4>Observations<\/h4>\n<ul>\n    <li>No missing values in train dataset<\/li>\n<\/ul>\n\n<br><a href=\"#Approach\">back to main menu<\/a>","c338eb75":"<h2 id=\"Ridge\">Ridge Classifier<\/h2>\nStarting with base Ridge model, without any hyperparameter tuning.\n\n<br><a href=\"#Approach\">back to main menu<\/a>","1b411e02":"<h2 id=\"XGBoost\">XGBClassifier<\/h2>\n\nSimple XGBoost without any hyperparameter tunning.\n\n<br><a href=\"#Approach\">back to main menu<\/a>","b6eaa900":"<h2 id=\"CatBoost\">CatBoostClassifier<\/h2>\n\nSimple CatBoost without any hyperparameter tunning.\n\n<br><a href=\"#Approach\">back to main menu<\/a>","1bc555c3":"<h2 id=\"LGBM\">LGBMClassifier<\/h2>\nStarting with base LGBM model, without any hyperparameter tuning.\n\n<br><a href=\"#Approach\">back to main menu<\/a>","40067a30":"<h3>CORRELATION CHECK CATEGORICAL FEATURES<\/h3>","af1448b4":"<h2 id=\"Target\">Understanding Target feature distribution<\/h2>\nLets visualize Target feature.\n\n<h4>Observation<\/h4>\nAs observations have almost equal count of response and no response observations, this is a balanced dataset. \n\n<br><a href=\"#Approach\">back to main menu<\/a>","99690a08":"<h2 id=\"Normalization\">Normalizing dataset<\/h2>\nUsing Standard Scaler to normalize dataset\n\n<br><a href=\"#Approach\">back to main menu<\/a>\n","e43bc090":"<h2 id=\"AggFeatures\">Creating Aggregated features<\/h2>\nCreating aggregated features\n<h4>Observation<\/h4>\n\n\n<br><a href=\"#Approach\">back to main menu<\/a>","98102585":"<h2 id=\"LogisticRegression\">LogisticRegression<\/h2>\nStarting with base Linear Model, without any hyperparameter tuning.\n\n<h4>Observations<\/h4>\n\n<br><a href=\"#Approach\">back to main menu<\/a>","7a8a906e":"<b>Problem Statement:<\/b>  The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the biological response of molecules given various chemical properties. We have to predict molecule response probability.\n\n<b>Problem type:<\/b> A binary classification problem.\n\n<b>Evaluation matrix:<\/b> Submissions are evaluated on area under the <b>ROC(receiver operating characteristic)<\/b> curve between the predicted probability and the observed target.","6241e523":"<h2 id=\"TrainVisual\">Visualizating Training dataset<\/h2>\nWe are making use of PCA, dimentionality reduction technique to Visualize Training dataset.<br>\nVisualization is also helpful in understanding any grouping or patterns within dataset.\n<h4>Observation<\/h4>\nNo pattern or grouping observed in training dataset\n\n<br><a href=\"#Approach\">back to main menu<\/a>","dcacc6bc":"<h2 id=\"Corr\">Correlation Check<\/h2>\nLets check if there are any correlated features. If two features are highly correlated we can remove one of the feature.\nThis will help in dimentionality reduction.\n\n<h4>Observation<\/h4>\n<ul>\n    <li>We can see some resonable correlation with Target. Therefore, plotting target correlation with other features<\/li>\n    <li>No correlation is observed among Training dataset features.Target has considerable correlation with feature f22<\/li>\n<\/ul>\n\n<br><a href=\"#Approach\">back to main menu<\/a>"}}