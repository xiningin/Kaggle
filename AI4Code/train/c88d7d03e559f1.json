{"cell_type":{"71190b80":"code","a33dee42":"code","6c1c18bf":"code","df78c598":"code","8bffe42e":"code","06e2db66":"code","b93556ba":"code","a735cc0d":"code","dffd791c":"code","6f668096":"code","5ff042b5":"code","d4bfb7c9":"code","3e550394":"code","0b6b0b8a":"code","e7bb7bd1":"code","ffb32fe5":"code","460e4e5d":"markdown","1d6d2dcc":"markdown","9affd4fa":"markdown","aeb2eeb9":"markdown","82c661c5":"markdown","19b6fe9d":"markdown","862f26c7":"markdown"},"source":{"71190b80":"## For resnext,seresnext ... etc\n!pip install git+https:\/\/github.com\/qubvel\/classification_models.git","a33dee42":"# for keras\nfrom classification_models.keras import Classifiers\n\nClassifiers.models_names()","6c1c18bf":"import os\nimport re\nimport warnings\nimport random\nimport sklearn.exceptions\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import roc_auc_score\nfrom time import perf_counter\nfrom tensorflow.keras import backend as K\nfrom tqdm.notebook import tqdm\nfrom kaggle_datasets import KaggleDatasets\nfrom glob import glob\n\n\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\nRANDOM_SEED = 42\nCOMPETITION_DATASET_PATH = \"..\/input\/g2net-gravitational-wave-detection\"\nPRETRAINED_MODEL_PATH = \"..\/input\/gwave-seresnet50-baseline\"\nQUANTILE = 0.7\nFOLDS = (0, 1, 2, 3)\nIMG_SIZES = 256\nBATCH_SIZES = 32\n\ndef seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything()","df78c598":"# From https:\/\/www.kaggle.com\/xhlulu\/ranzcr-efficientnet-tpu-training\ndef auto_select_accelerator():\n    TPU_DETECTED = False\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n        TPU_DETECTED =True\n    except ValueError:\n        strategy = tf.distribute.get_strategy()\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    \n    return strategy, TPU_DETECTED","8bffe42e":"BACKBONE = 'seresnet50'","06e2db66":"strategy, TPU_DETECTED = auto_select_accelerator()\nAUTO = tf.data.experimental.AUTOTUNE\nREPLICAS = strategy.num_replicas_in_sync","b93556ba":"def get_set_paths(idxs, path_prefix: str ='cqt-g2net-test', file_prefix: str = 'test', sep: str='-'):\n    files = []\n    for i,k in tqdm(idxs):\n        GCS_PATH = KaggleDatasets().get_gcs_path(f'{path_prefix}-{i}{sep}{k}')\n        files.extend(np.sort(np.array(tf.io.gfile.glob(GCS_PATH + f'\/{file_prefix}*.tfrec'))).tolist())\n    print('Detected', len(files), file_prefix, 'files')\n    return files","a735cc0d":"files_train_g = get_set_paths([(0, 1), (2, 3), (4, 5), (6, 7), \n                               (8, 9), (10, 11), (12, 13), (14, 15)], \n                              path_prefix='cqt-g2net-v2', file_prefix='train', sep='-')\nfiles_test_g = get_set_paths([(0, 1), (2, 3), (4, 5), (6, 7)], file_prefix='test')","dffd791c":"def read_labeled_tfrecord(example):\n    tfrec_format = {\n        'image'                        : tf.io.FixedLenFeature([], tf.string),\n        'image_id'                     : tf.io.FixedLenFeature([], tf.string),\n        'target'                       : tf.io.FixedLenFeature([], tf.int64)\n    }           \n    example = tf.io.parse_single_example(example, tfrec_format)\n    return prepare_image(example['image']), tf.reshape(tf.cast(example['target'], tf.float32), [1])\n\n\ndef read_unlabeled_tfrecord(example, return_image_id):\n    tfrec_format = {\n        'image'                        : tf.io.FixedLenFeature([], tf.string),\n        'image_id'                     : tf.io.FixedLenFeature([], tf.string),\n    }\n    example = tf.io.parse_single_example(example, tfrec_format)\n    return prepare_image(example['image']), example['image_id'] if return_image_id else 0\n\n \ndef prepare_image(img, dim=IMG_SIZES):    \n    img = tf.image.resize(tf.image.decode_png(img, channels=3), size=(dim, dim))\n    img = tf.cast(img, tf.float32) \/ 255.0\n    img = tf.reshape(img, [dim,dim, 3])\n            \n    return img\n\ndef count_data_items(fileids):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(fileid).group(1)) \n         for fileid in fileids]\n    return np.sum(n)","6f668096":"def get_dataset(files, shuffle = False, repeat = False, \n                labeled=True, return_image_ids=True, batch_size=16, dim=IMG_SIZES):\n    \n    ds = tf.data.TFRecordDataset(files, num_parallel_reads=AUTO)\n    ds = ds.cache()\n    \n    if repeat:\n        ds = ds.repeat()\n    \n    if shuffle: \n        ds = ds.shuffle(1024*2)\n        opt = tf.data.Options()\n        opt.experimental_deterministic = False\n        ds = ds.with_options(opt)\n        \n    if labeled: \n        ds = ds.map(read_labeled_tfrecord, num_parallel_calls=AUTO)\n    else:\n        ds = ds.map(lambda example: read_unlabeled_tfrecord(example, return_image_ids), \n                    num_parallel_calls=AUTO)      \n    \n    ds = ds.batch(batch_size * REPLICAS)\n    ds = ds.prefetch(AUTO)\n    return ds","5ff042b5":"def build_model(size,path,backbone):\n    inp = tf.keras.layers.Input(shape=(size, size,3))\n    \n    ResNeXt, preprocess_input = Classifiers.get(backbone)\n    base_net = ResNeXt(include_top = False, input_shape=(size,size,3), weights=path)\n\n    x = base_net(inp)    \n    x = tf.keras.layers.GlobalAvgPool2D()(x)\n    x = tf.keras.layers.Dropout(0.)(x)\n    x = tf.keras.layers.Dense(1,activation='sigmoid')(x)\n    \n    model = tf.keras.Model(inputs=inp, outputs=x)\n    loss = tf.keras.losses.BinaryCrossentropy() \n    model.compile(optimizer='adam',loss=loss,metrics=['AUC'])\n    return model","d4bfb7c9":"def predict(paths, is_label=False):\n    pred = []; ids = []\n\n    ds = get_dataset(paths,labeled=False,return_image_ids=False,\n                repeat=False,shuffle=False,dim=IMG_SIZES,batch_size=BATCH_SIZES*2)\n        \n    for fold in FOLDS:\n    \n        print('#'*50); print('--> FOLD',fold+1);\n        start_time = perf_counter()\n    \n        K.clear_session()\n    \n        with strategy.scope():\n            model = build_model(IMG_SIZES, None , BACKBONE)\n            print('\\t-->Loading model...')\n            model.load_weights(f'{PRETRAINED_MODEL_PATH}\/fold-{fold}.h5')\n            print('\\t<--Model loaded.')\n    \n        print('\\t-->Start Predict...')\n    \n        pred.append(model.predict(ds, verbose=0).flatten())      \n        print('\\t<--Predict finished.')\n        print('<-- FOLD',fold+1, f'finished; duration = {perf_counter() - start_time} s')\n    \n    if is_label:\n        ds = get_dataset(paths,labeled=True,return_image_ids=False,\n                repeat=False,shuffle=False,dim=IMG_SIZES,batch_size=BATCH_SIZES*2)\n        ids = np.array([target.numpy() for _, target in tqdm(ds.unbatch())]).flatten()\n    else:\n        ds = get_dataset(paths,labeled=False,return_image_ids=True,\n                repeat=False,shuffle=False,dim=IMG_SIZES,batch_size=BATCH_SIZES*2)\n        ids = np.array([target.numpy().decode(\"utf-8\") for _, target in tqdm(ds.unbatch())]).flatten()\n    return pred, ids","3e550394":"pred, ids = predict(np.array(files_test_g), False)","0b6b0b8a":"np.mean(pred,axis = 0).shape","e7bb7bd1":"sub = pd.read_csv(f'{COMPETITION_DATASET_PATH}\/sample_submission.csv')\nsub['id'] = ids\nsub['target'] = np.mean(pred,axis = 0)\nsub = sub.sort_values('id') \nsub.head()","ffb32fe5":"sub.to_csv('submission.csv', index=False)","460e4e5d":"### Dataset Creation","1d6d2dcc":"### Build Model","9affd4fa":"<a id=\"5\"><\/a>\n# Search best quantile","aeb2eeb9":"<a id=\"6\"><\/a>\n# Inference","82c661c5":"<a id=\"4\"><\/a>\n# TPU Configuration & Utils scripts","19b6fe9d":"### Reading Tfrecords","862f26c7":"<a id=\"7\"><\/a>\n# Next steps\n* Add TTA Inference"}}