{"cell_type":{"403ccf59":"code","a2839468":"code","a4086e69":"code","e785af95":"code","a06eadcb":"code","5eabf9ee":"code","c59fa56f":"code","b6fab2af":"code","98863d1b":"code","9ad6b0f0":"code","007c9fab":"code","8616cf79":"code","2d11a8d5":"code","f947b9d3":"code","28d534fb":"code","b0c80e6d":"code","7ec50d85":"code","60e03569":"code","258688aa":"code","1b1be975":"code","c5d19254":"code","264cc0ea":"code","f89b846d":"code","89c22fe3":"code","b81c6540":"code","a0b82c8d":"code","ccb53a85":"code","93b544fc":"code","e82231c1":"code","759b7516":"code","4aac7f13":"code","9eb3e273":"code","30ab9c5d":"code","eb40c6a8":"code","6be88f52":"code","1e2c36b0":"code","e082c5e7":"code","2a52ab17":"code","78d78a58":"code","4ddef2c6":"code","f250b52a":"code","9c0ca4ba":"code","249323e5":"code","9c900057":"code","5b812ded":"code","a9676143":"code","e70cdb6e":"code","5a0de7ae":"code","5509d3d1":"code","6d50baab":"code","d7456bf9":"code","5f3c1e97":"code","f52d3230":"code","3e462b37":"code","3a9c7ed1":"markdown","80644c6e":"markdown","3bcd8b74":"markdown","f7af36e0":"markdown","1122e551":"markdown","f51d9c4c":"markdown","bfba53aa":"markdown","630dc616":"markdown","a5e8e95f":"markdown","77719d83":"markdown","4d10b034":"markdown"},"source":{"403ccf59":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","a2839468":"import gc\nimport os\nimport logging\nimport datetime\nimport warnings\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport lightgbm as lgb\nfrom tqdm import tqdm_notebook\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error, confusion_matrix\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.model_selection import StratifiedKFold\nwarnings.filterwarnings('ignore')","a4086e69":"train = pd.read_csv('..\/input\/X_train.csv')\ntest = pd.read_csv('..\/input\/X_test.csv')\ntarget = pd.read_csv('..\/input\/y_train.csv')\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')","e785af95":"train.head()","a06eadcb":"# No null values\nsns.heatmap(train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","5eabf9ee":"#checking the Target distribution\nplt.figure(figsize=(15,5))\nsns.countplot(y=target['surface'],order=target.surface.value_counts().index)","c59fa56f":"train.info()","b6fab2af":"def reduce_mem_usage(df):\n    # iterate through all the columns of a dataframe and modify the data type\n    #   to reduce memory usage.        \n    \n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","98863d1b":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","9ad6b0f0":"train.info()","007c9fab":"len(target.series_id.unique())","8616cf79":"print('There are {} rows and {} columns for training set'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} columns for test set'.format(test.shape[0],test.shape[1]))\nprint('There are {} rows and {} columns for test set'.format(target.shape[0],target.shape[1]))","2d11a8d5":"len(train['series_id'].value_counts())","f947b9d3":"len(test.series_id.unique())","28d534fb":"f,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(train.iloc[:,3:].corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","b0c80e6d":"def plot_feature_distribution(df1, df2, label1, label2, features,a=2,b=5):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(a,b,figsize=(17,9))\n\n    for feature in features:\n        i += 1\n        plt.subplot(a,b,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","7ec50d85":"features = train.columns.values[3:]\nplot_feature_distribution(train, test, 'train', 'test', features)","60e03569":"def plot_feature_class_distribution(classes,tt, features,a=5,b=2):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(a,b,figsize=(16,24))\n\n    for feature in features:\n        i += 1\n        plt.subplot(a,b,i)\n        for clas in classes:\n            ttc = tt[tt['surface']==clas]\n            sns.kdeplot(ttc[feature], bw=0.5,label=clas)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","258688aa":"def create_valid_set(label):\n    # Lets try creating a validation set of 10% of the total size.\n    ldict = {\n        'concrete': 0.16,\n        'soft_pvc': 0.18,\n        'wood': 0.06,\n        'tiled': 0.03,\n        'fine_concrete': 0.10,\n        'hard_tiles_large_space': 0.12,\n        'soft_tiles': 0.23,\n        'carpet': 0.05,\n        'hard_tiles': 0.07,\n    }\n    score = 0\n    print(\"Required count of target classes for the Valid Set :: \")\n    for key, value in ldict.items():\n        score += value\n        print(key,':',value)","1b1be975":"classes = (target['surface'].value_counts()).index\naux = train.merge(target, on='series_id', how='inner')\nplot_feature_class_distribution(classes, aux, features)","c5d19254":"#create_valid_set(target)","264cc0ea":"train.describe().T","f89b846d":"test.describe().T","89c22fe3":"# checking the difference between train series id and test series id\ndiff = (test.shape[0]-train.shape[0])\/128\nprint('Test has',diff,' extra series')","b81c6540":"train.columns","a0b82c8d":"# https:\/\/stackoverflow.com\/questions\/53033620\/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back-fr?rq=1\ndef quaternion_to_euler(x, y, z, w):\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n\n    return X, Y, Z","ccb53a85":"def fe_step0 (actual):\n    \n    # https:\/\/www.mathworks.com\/help\/aeroblks\/quaternionnorm.html\n    # https:\/\/www.mathworks.com\/help\/aeroblks\/quaternionmodulus.html\n    # https:\/\/www.mathworks.com\/help\/aeroblks\/quaternionnormalize.html\n        \n    actual['norm_quat'] = (actual['orientation_X']**2 + actual['orientation_Y']**2 + actual['orientation_Z']**2 + actual['orientation_W']**2)\n    actual['mod_quat'] = (actual['norm_quat'])**0.5\n    actual['norm_X'] = actual['orientation_X'] \/ actual['mod_quat']\n    actual['norm_Y'] = actual['orientation_Y'] \/ actual['mod_quat']\n    actual['norm_Z'] = actual['orientation_Z'] \/ actual['mod_quat']\n    actual['norm_W'] = actual['orientation_W'] \/ actual['mod_quat']\n    \n    return actual","93b544fc":"%%time\ntrain = fe_step0(train)\ntest = fe_step0(test)","e82231c1":"print(train.shape)\ntrain.head()","759b7516":"test.head()","4aac7f13":"def fe_step1 (actual):\n    \"\"\"Quaternions to Euler Angles\"\"\"\n    \n    x, y, z, w = actual['norm_X'].tolist(), actual['norm_Y'].tolist(), actual['norm_Z'].tolist(), actual['norm_W'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    \n    actual['euler_x'] = nx\n    actual['euler_y'] = ny\n    actual['euler_z'] = nz\n    return actual","9eb3e273":"train = fe_step1(train)\ntest = fe_step1(test)\nprint(train.shape)\ntrain.head()\n#test.head()","30ab9c5d":"len(test['series_id'].unique())","eb40c6a8":"sns.countplot(y='surface',data=target)","6be88f52":"def total_values_fe(data):\n    data['totl_anglr_vel'] = (data['angular_velocity_X']**2 + data['angular_velocity_Y']**2 +\n                             data['angular_velocity_Z'])** 0.5\n    data['totl_linr_acc'] = (data['linear_acceleration_X']**2 + data['linear_acceleration_Y']**2 +\n                             data['linear_acceleration_Z'])**0.5\n    data['totl_xyz'] = (data['orientation_X']**2 + data['orientation_Y']**2 +\n                             data['orientation_Z'])**0.5\n    #Lets derive one more column since there is a relationship in velocity and acceleration\n    # v = u + a*t , u is initial velocty. if u = 0, then v = at means t = v\/a\n    # but value of acceleration is more and value of velocity is less, lets do a\/v relation\n    data['acc_vs_vel'] = data['totl_linr_acc'] \/ data['totl_anglr_vel']\n    return data","1e2c36b0":"data = total_values_fe(train)\ntest = total_values_fe(test)\nprint(data.shape)\ndata.head()","e082c5e7":"def mean_change_of_abs_change(x):\n        return np.mean(np.diff(np.abs(np.diff(x))))","2a52ab17":"len(test['series_id'].unique())","78d78a58":"train_data = pd.DataFrame()\ntest_data = pd.DataFrame()","4ddef2c6":"%%time\n# columns for max, min, mean, median, abs_max, std, quartile(25%), quartile(50%), quartile(75%))\n# starting from the Orientation column\ncolumns = data.columns\nfor i in columns[1:]:\n    if i in ['row_id','series_id','measurement_number']:\n        continue\n    train_data[i+'_max'] = data.groupby(by='series_id')[i].max()\n    test_data[i+'_max'] = test.groupby(by='series_id')[i].max()\n    print(i)\n    train_data[i+'_min'] = data.groupby(by='series_id')[i].min()\n    test_data[i+'_min'] = test.groupby(by='series_id')[i].min()\n        \n    train_data[i+'_mean'] = data.groupby(by='series_id')[i].mean()\n    test_data[i+'_mean'] = test.groupby(by='series_id')[i].mean()\n        \n    train_data[i+'_median'] = data.groupby(by='series_id')[i].median()\n    test_data[i+'_median'] = test.groupby(by='series_id')[i].median()\n        \n    train_data[i+'_quantile_25'] = data.groupby(by='series_id')[i].quantile(0.25)\n    test_data[i+'_quantile_25'] = test.groupby(by='series_id')[i].quantile(0.25)\n        \n    train_data[i+'_quantile_50'] = data.groupby(by='series_id')[i].quantile(0.5)\n    test_data[i+'_quantile_50'] = test.groupby(by='series_id')[i].quantile(0.5)\n        \n    train_data[i+'_quantile_75'] = data.groupby(by='series_id')[i].quantile(0.75)\n    test_data[i+'_quantile_75'] = test.groupby(by='series_id')[i].quantile(0.75)\n    \n    #train_data[col + '_mean_change_of_abs_change'] = train.groupby('series_id')[col].apply(mean_change_of_abs_change)\n    train_data[i+'_abs_max'] = data.groupby(by='series_id')[i].apply(lambda x: np.max(np.abs(x)))\n    test_data[i+'_abs_max'] = test.groupby(by='series_id')[i].apply(lambda x: np.max(np.abs(x)))\n    \n    train_data[i + '_mean_abs_chg'] = data.groupby(['series_id'])[i].apply(lambda x: np.mean(np.abs(np.diff(x))))\n    test_data[i + '_mean_abs_chg'] = test.groupby(['series_id'])[i].apply(lambda x: np.mean(np.abs(np.diff(x))))\n    \n    train_data[i + '_mean_change_of_abs_change'] = data.groupby('series_id')[i].apply(mean_change_of_abs_change)\n    test_data[i + '_mean_change_of_abs_change'] = test.groupby('series_id')[i].apply(mean_change_of_abs_change)\n    \n    train_data[i + '_abs_min'] = data.groupby(['series_id'])[i].apply(lambda x: np.min(np.abs(x)))\n    test_data[i + '_abs_min'] = test.groupby(['series_id'])[i].apply(lambda x: np.min(np.abs(x)))\n    \n    train_data[i + '_abs_avg'] = (train_data[i + '_abs_min'] + train_data[i + '_abs_max'])\/2\n    test_data[i + '_abs_avg'] = (test_data[i + '_abs_min'] + test_data[i + '_abs_max'])\/2\n        \n    train_data[i+'_std'] = data.groupby(by='series_id')[i].std()\n    test_data[i+'_std'] = test.groupby(by='series_id')[i].std()\n         \n    train_data[i + '_range'] = train_data[i + '_max'] - train_data[i + '_min']\n    test_data[i + '_range'] = test_data[i + '_max'] - test_data[i + '_min']\n        \n    train_data[i + '_maxtoMin'] = train_data[i + '_max'] \/ train_data[i + '_min']\n    test_data[i + '_maxtoMin'] = test_data[i + '_max'] \/ test_data[i + '_min']\n","f250b52a":"print(train_data.shape)\ntrain_data.head()","9c0ca4ba":"# It seems no NaN values\ntrain_data.isnull().values.any()","249323e5":"# There is missing data, we shall replace the same by zeroes\ntrain_data.fillna(0,inplace=True)\ntrain_data.replace(-np.inf,0,inplace=True)\ntrain_data.replace(np.inf,0,inplace=True)\ntest_data.fillna(0,inplace=True)\ntest_data.replace(-np.inf,0,inplace=True)\ntest_data.replace(np.inf,0,inplace=True)","9c900057":"train_data.isnull().values.any()","5b812ded":"#label Encoding\nle = LabelEncoder()\ntarget['surface'] = le.fit_transform(target['surface'])","a9676143":"target['surface'].value_counts()","e70cdb6e":"# Using RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold","5a0de7ae":"folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\npredicted = np.zeros((test_data.shape[0],9)) # we have 9 labels\nmeasured= np.zeros((train_data.shape[0]))\nscore = 0","5509d3d1":"for times, (trn_idx,val_idx) in enumerate(folds.split(train_data.values,target['surface'].values)):\n    rf = RandomForestClassifier(n_estimators=500,n_jobs = -1)\n    rf.fit(train_data.iloc[trn_idx],target['surface'][trn_idx])\n    measured[val_idx] = rf.predict(train_data.iloc[val_idx])\n    predicted += rf.predict_proba(test_data)\/folds.n_splits\n    score += rf.score(train_data.iloc[val_idx],target['surface'][val_idx])\n    print(\"Fold: {} score: {}\".format(times,rf.score(train_data.iloc[val_idx],target['surface'][val_idx])))\n    gc.collect()","6d50baab":" print('Avg. accuracy',score \/folds.n_splits)","d7456bf9":"def plot_confusion_matrix(truth, pred, classes, normalize=False, title=''):\n    cm = confusion_matrix(truth, pred)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion matrix', size=15)\n    plt.colorbar(fraction=0.046, pad=0.04)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()","5f3c1e97":"plot_confusion_matrix(target['surface'], measured, le.classes_)","f52d3230":"submission['surface'] = le.inverse_transform(predicted.argmax(axis=1))\nsubmission.to_csv('submission_92.csv',index=False)\nsubmission.head()","3e462b37":"submission.head(20)","3a9c7ed1":"### EDA","80644c6e":"The idea of reducing memory usage of the kernel was borrowed from **@swagatron**. Thanks for this, a new tip to use","3bcd8b74":"If we look at the above graph, the information for hard tiles is very less\n\n![image.png](attachment:image.png)\n\nComparing with the leaderboard, we have a different distribution. This chances of increasing the score is to have a distribution similar to leaderboard score. The main difference which i noticed in the leaderboard is that:\n\n* There is significant amount of records for **hard_tiles** in leaderboard\n* For soft_tiles there are high predictions","f7af36e0":"The input data, is covering 10 sensor channels and 128 measurements per time series. \n\nThe orientation channels encode the current angles how the robot is oriented as a quaternion (see Wikipedia). Angular velocity describes the angle and speed of motion, and linear acceleration components describe how the speed is changing at different times.","1122e551":"There is strong correlatiom between:\n* orientation_X and orientation_W\n* orientation_Y and orientation_Z\n* orientation_Z and orientation_Y\n","f51d9c4c":"## Helping Robots","bfba53aa":"Most of the features follow a normal distribution pattern, but there are overlap of guassians. orientation_X , orientation_Y and linear_acceleration_Y have some variation. We can try to apply standard scaler function for the same","630dc616":"**@Nanashi** had a good explanation on why Euler angles are really important, and we have a problem with Z?\n\nWhy Orientation_Z (euler angle Z) is so important?\nWe have a robot moving around, imagine a robot moving straight through different surfaces (each with different features), for example concrete and hard tile floor. Our robot can can bounce or balance itself a little bit on if the surface is not flat and smooth, that's why we need to work with quaternions and take care of orientation_Z.\n\n","a5e8e95f":"There are no null values in the dataset","77719d83":"All through modelling the algo i was looking for chances to improve my score. Got inspired by the words of **@swagatron** to check the distribution of the results. So it was really great to go back to the drawing board and check the distribution as mentioned by **@donkeys** and **@ninoko**\n\nThe leaderboard distributions are given in the discussion threads :\n\n* https:\/\/www.kaggle.com\/c\/career-con-2019\/discussion\/84760\n* https:\/\/www.kaggle.com\/c\/career-con-2019\/discussion\/85204","4d10b034":"Not all the surfaces follow the normal distribution pattern"}}