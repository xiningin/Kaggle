{"cell_type":{"29303575":"code","96610899":"code","a144789a":"code","702db5ec":"code","654bae83":"code","16a29787":"code","49c4424c":"code","59c4f0d6":"code","a6c473e0":"code","c53c5528":"code","40411d9e":"code","697a15b9":"code","d9dc24b6":"code","48cc2db4":"code","c21ed28f":"code","07a0b93b":"code","79d4e0af":"code","48c9847f":"code","a70fa1fd":"code","869d497a":"code","19debb22":"code","b38e984d":"code","9dd0d937":"code","6518cbaf":"code","9acd7d34":"code","33da5563":"code","de23a048":"code","564ab18a":"code","d57e8bb0":"code","af378550":"code","d6cfddaa":"code","8375c734":"code","a219d3c8":"code","3b4261ec":"code","dacda5a0":"code","0df71914":"code","e55a2158":"markdown","2af0d7cf":"markdown","8518855e":"markdown","18da2041":"markdown","e98541e8":"markdown","30f45704":"markdown","a04f84bc":"markdown","03f05364":"markdown"},"source":{"29303575":"# IMPORTING NECESSARY MODULES FOR DATA ANALYSIS AND PREDICTIVE MODELLING\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score,confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport re\nimport gc\nimport os\nimport cv2\nimport argparse\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers import Dropout\nfrom keras.layers.convolutional import Conv2D\nfrom keras.layers.convolutional import MaxPooling2D\nfrom keras.utils import np_utils\nfrom keras import optimizers\nimport psutil\nimport humanize\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom IPython.display import HTML, display, clear_output\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)","96610899":"print(os.listdir(\"..\/input\"))","a144789a":"TrainDataPath = '..\/input\/train.csv'\nTestDataPath = '..\/input\/test.csv' \nSubDataPath = '..\/input\/sample_submission.csv'\n\n# Loading the Training and Test Dataset and Submission File\nTrainData = pd.read_csv(TrainDataPath)\nTestData = pd.read_csv(TestDataPath)\nSubData = pd.read_csv(SubDataPath)","702db5ec":"print(\"Training Dataset Shape:\")\nprint(TrainData.shape)\nTrainData.head()","654bae83":"print(\"Test Dataset Shape:\")\nprint(TestData.shape)\nTestData.head()","16a29787":"print(\"Submission Dataset Shape:\")\nprint(SubData.shape)\nprint(\"\\n\")\nprint(\"Submission Dataset Columns\/Features:\")\nprint(SubData.dtypes)\nSubData.head()","49c4424c":"# checking missing data percentage in train data\ntotal = TrainData.isnull().sum().sort_values(ascending = False)\npercent = (TrainData.isnull().sum()\/TrainData.isnull().count()*100).sort_values(ascending = False)\nmissing_TrainData  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_TrainData.head(10) # We are just doing a sanity check that each and every pixel value is present or not!!!","59c4f0d6":"# checking missing data percentage in test data\ntotal = TestData.isnull().sum().sort_values(ascending = False)\npercent = (TestData.isnull().sum()\/TestData.isnull().count()*100).sort_values(ascending = False)\nmissing_TrainData  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_TrainData.head(10) # Doing the same thing again for test data","a6c473e0":"def printmemusage():\n process = psutil.Process(os.getpid())\n print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n\nprintmemusage()","c53c5528":"def plot_bar_counts_categorical(data_se, title, figsize, sort_by_counts=False):\n    info = data_se.value_counts()\n    info_norm = data_se.value_counts(normalize=True)\n    categories = info.index.values\n    counts = info.values\n    counts_norm = info_norm.values\n    fig, ax = plt.subplots(figsize=figsize)\n    if data_se.dtype in ['object']:\n        if sort_by_counts == False:\n            inds = categories.argsort()\n            counts = counts[inds]\n            counts_norm = counts_norm[inds]\n            categories = categories[inds]\n        ax = sns.barplot(counts, categories, orient = \"h\", ax=ax)\n        ax.set(xlabel=\"count\", ylabel=data_se.name)\n        ax.set_title(\"Distribution of \" + title)\n        for n, da in enumerate(counts):\n            ax.text(da, n, str(da)+ \",  \" + str(round(counts_norm[n]*100,2)) + \" %\", fontsize=10, va='center')\n    else:\n        inds = categories.argsort()\n        counts_sorted = counts[inds]\n        counts_norm_sorted = counts_norm[inds]\n        ax = sns.barplot(categories, counts, orient = \"v\", ax=ax)\n        ax.set(xlabel=data_se.name, ylabel='count')\n        ax.set_title(\"Distribution of \" + title)\n        for n, da in enumerate(counts_sorted):\n            ax.text(n, da, str(da)+ \",  \" + str(round(counts_norm_sorted[n]*100,2)) + \" %\", fontsize=10, ha='center')","40411d9e":"def count_plot_by_hue(data_se, hue_se, title, figsize, sort_by_counts=False):\n    if sort_by_counts == False:\n        order = data_se.unique()\n        order.sort()\n    else:\n        order = data_se.value_counts().index.values\n    off_hue = hue_se.nunique()\n    off = len(order)\n    fig, ax = plt.subplots(figsize=figsize)\n    ax = sns.countplot(y=data_se, hue=hue_se, order=order, ax=ax)\n    ax.set_title(title)\n    patches = ax.patches\n    for i, p in enumerate(ax.patches):\n        x=p.get_bbox().get_points()[1,0]\n        y=p.get_bbox().get_points()[:,1]\n        total = x\n        p = i\n        q = i\n        while(q < (off_hue*off)):\n            p = p - off\n            if p >= 0:\n                total = total + (patches[p].get_bbox().get_points()[1,0] if not np.isnan(patches[p].get_bbox().get_points()[1,0]) else 0)\n            else:\n                q = q + off\n                if q < (off*off_hue):\n                    total = total + (patches[q].get_bbox().get_points()[1,0] if not np.isnan(patches[q].get_bbox().get_points()[1,0]) else 0)\n       \n        perc = str(round(100*(x\/total), 2)) + \" %\"\n        \n        if not np.isnan(x):\n            ax.text(x, y.mean(), str(int(x)) + \",  \" + perc, va='center')\n    plt.show()","697a15b9":"def show_unique(data_se):\n    display(HTML('<h5><font color=\"green\"> Shape Of Dataset Is: ' + str(data_se.shape) + '<\/font><\/h5>'))\n    for i in data_se.columns:\n        if data_se[i].nunique() == data_se.shape[0]:\n            display(HTML('<font color=\"red\"> ATTENTION!!! ' + str(i+' --> '+str(data_se[i].nunique())) + '<\/font>'))\n        elif (data_se[i].nunique() == 1):\n            display(HTML('<font color=\"Blue\"> ATTENTION!!! ' + str(i+' --> '+str(data_se[i].nunique())) + '<\/font>'))\n        else:\n            print(i+' -->', data_se[i].nunique())","d9dc24b6":"def show_countplot(data_se):\n    display(HTML('<h2><font color=\"blue\"> Dataset CountPlot Visualization: <\/font><\/h2>'))\n    for i in data_se.columns:\n        if (data_se[i].nunique() <= 10):\n            plot_bar_counts_categorical(data_se[i].astype(str), 'Dataset Column: '+ i, (15,7))\n        elif (data_se[i].nunique() > 10 and data_se[i].nunique() <= 20):\n            plot_bar_counts_categorical(data_se[i].astype(str), 'Dataset Column: '+ i, (15,12))\n        else:\n            print('Columns do not fit in display '+i+' -->', data_se[i].nunique())","48cc2db4":"gc.collect() # Python garbage collection module for dereferencing the memory pointers and making memory available for better usage","c21ed28f":"TrainData.head()","07a0b93b":"TestData.head()","79d4e0af":"print(TrainData.shape) \nprint(TestData.shape)","48c9847f":"plot_bar_counts_categorical(TrainData['label'], 'Train Dataset \"Target Variable\" Distribution Plot', figsize=(18,5), sort_by_counts=False)","a70fa1fd":"train_img = np.array(TrainData.drop(['label'], axis=1))\ntrain_labels = np.array(TrainData['label'])\n\ntest_img = np.array(TestData)","869d497a":"train_img","19debb22":"train_labels","b38e984d":"test_img","9dd0d937":"img_rows, img_cols = 28, 28 # Because we have 784 columns i.e. it is an image with size = 28x28\ntotal_classes = 10\ntrain_labels = np_utils.to_categorical(train_labels, 10) # Converting each label into one hot encoded label","6518cbaf":"train_labels","9acd7d34":"X_train, X_val, y_train, y_val = train_test_split(train_img, train_labels, test_size=0.1)  # Splitting the dataset into train and validation set","33da5563":"X_train_new = X_train.reshape(X_train.shape[0],28,28,1)\nX_val_new = X_val.reshape(X_val.shape[0],28,28,1)","de23a048":"def CNN(height, width, depth, total_classes):\n    # Model Initialize\n    model = Sequential()\n    # First layer Convultion2D ---> Relu Activation ---> MaxPooling2D\n    # The border_mode = \"same\", you get an output that is the \"same\" size as the input.\n    model.add(Conv2D(20, (3,3), activation='relu', input_shape=(width, height, depth), padding=\"same\"))\n    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n    # Second layer Convultion2D ---> Relu Activation ---> MaxPooling2D\n    model.add(Conv2D(50, (3,3), activation='relu', padding=\"same\"))\n    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n    # Third layer includes flattening the image into a columm of pixel values\n    model.add(Flatten())\n    model.add(Dense(100, activation='relu'))\n    # Fourth layer the output layer\n    model.add(Dense(total_classes, activation='softmax')) \n    model.summary()\n    return model","564ab18a":"print('Now Setting the Optimizer....\\n')\nsgd = optimizers.SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\nmodel = CNN(img_rows, img_cols, 1, total_classes)\nmodel.compile(loss=\"categorical_crossentropy\", optimizer=sgd, metrics=[\"accuracy\"])","d57e8bb0":"print(X_train_new.shape)\nprint(X_val_new.shape)","af378550":"print(y_train.shape)\nprint(y_val.shape)","d6cfddaa":"history = model.fit(X_train_new \/ 255.0 , y_train, batch_size=128, epochs=5, verbose=1)","8375c734":"loss, accuracy = model.evaluate(X_val_new \/ 255.0, y_val, batch_size=128, verbose=1)\nprint('Accuracy of Model on Validation Set: {:.2f}%'.format(accuracy * 100))","a219d3c8":"# list all data in history\nprint(history.history.keys())","3b4261ec":"# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['loss'])\nplt.title('Model Accuracy VS Loss')\nplt.ylabel('Score')\nplt.xlabel('epoch')\nplt.legend(['Accuracy', 'Loss'])\nplt.show()","dacda5a0":"test_img.shape","0df71914":"X_test = test_img.reshape(test_img.shape[0],28,28,1)\ntest_labels_pred = model.predict(X_test)","e55a2158":"# HELPER FUNCTION","2af0d7cf":"# Please upvote guys and keep supporting me.\n# New updates coming soon....","8518855e":"# Now Let's Begin The Modelling Part","18da2041":"The above code converts the dataset into numpy array which will be used for further analysis.","e98541e8":"In the above SGD optimizer, I have chosen the default parametes where lr : learning rate which is set equal to 0.1, decay : learning rate decay after each update which is set equal to 1e-6, and we have chosen nestrov momentum to be active and momentum rate is set equal to 0.9","30f45704":"So here we have **42000 training images** and **28000 test images**.","a04f84bc":"# Ok Now We Should Start With The Analysis Part Of The Dataset And Try Getting Out Some Insights","03f05364":"**As we can see that Class-'1' has the most number of samples in the dataset and class-'5' has the least number of samples in the dataset.** \n\n**Whereas this is fairly a good distribution of different class and we have all classes fairly balanced, so no issue of class skewness.**"}}