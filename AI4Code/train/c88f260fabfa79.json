{"cell_type":{"9cd12e59":"code","3323c558":"code","57da8bfe":"code","ee30a72e":"code","bc8a0f1c":"code","aef7e0b1":"code","269dacf3":"code","82de6991":"code","ad5ba3dd":"code","bc991ff3":"code","7f8a2e03":"code","1e143b29":"code","6c4a1912":"code","3149a87d":"code","2fabd609":"markdown","8f423b13":"markdown","b85e12cc":"markdown","390b121a":"markdown","376989bb":"markdown","945d236e":"markdown"},"source":{"9cd12e59":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3323c558":"%%time\ntrain_df = pd.read_parquet('..\/input\/ubiquant-parquet\/train.parquet')","57da8bfe":"train_df.head()","ee30a72e":"train_df.shape","bc8a0f1c":"train_df.dtypes","aef7e0b1":"discrete_cols = [\"row_id\", \"time_id\", \"investment_id\"]\nfor col in discrete_cols:\n    print(col, \" : \",train_df[col].nunique())","269dacf3":"first_part = []\nsecond_part = []\nfor row_id in train_df[\"row_id\"].values:\n    if row_id.split(\"_\")[0] not in first_part:\n        first_part.append(row_id.split(\"_\")[0])\n    if row_id.split(\"_\")[1] not in second_part:\n        second_part.append(row_id.split(\"_\")[1])","82de6991":"len(first_part), len(second_part)","ad5ba3dd":"investment_id_df = pd.read_parquet(\"..\/input\/ubiquant-parquet\/investment_ids\/1.parquet\")","bc991ff3":"investment_id_df.head()","7f8a2e03":"investment_id_df.shape","1e143b29":"investment_id_df.time_id.nunique()","6c4a1912":"cont_cols = [f\"f_{i}\" for i in range(10)]\nfig, axes = plt.subplots(5, 2, figsize=(20,20))\ni=0\nplt.subplots_adjust(left=0.1,\n                    bottom=0.1, \n                    right=0.9, \n                    top=0.9, \n                    wspace=0.4, \n                    hspace=0.4)\nfor col in cont_cols:\n    row = i\/\/2\n    column = i%2\n    sns.kdeplot(data = investment_id_df, x=col, ax=axes[row, column], shade=True)\n    i+=1\nfig.show()","3149a87d":"sns.kdeplot(investment_id_df[\"target\"], shade=True)","2fabd609":"As we can see, the row_id is nothing but a combination of time_id and investment_id, with each time_id containing <= investment_id number of entries. So, each of these investment_id data is already present in separate parquet files when we load the data that Rob Mulla Provided, so we can make use of those files to inspect any Individual investment_id. So, let's see if we can find any patterns by looking at first investment_id data. It can be loaded as shown below:","8f423b13":"The target seems to follow normal distribution for this particular investment_id.\n\n# Further Analysis will be continued. If you like my work, please consider to Upvote. Also, any suggestions or feedback would be really helpful","b85e12cc":"# Loading Data\nThe data is in total about 18GB in size.Hence, it is practically impossible to load the entire data into memory as we will be getting a memory crash error and session will be restarted again and again. Hence, I made use of the Parquet files which Rob Mulla made available over here:https:\/\/www.kaggle.com\/robikscube\/ubiquant-parquet.\n\n# Understanding Parquet Format\n\nParquet is an open source file format available to any project in the Hadoop ecosystem. It stores each column in a separate file as opposed to common row based storage formats like CSV. This approach is particularly helpful when we want to retrieve only a particular columns from the data, hence parquet format is optimized for such queries to greatly reduce the IO.\n\nInorder to read more about Parquet format, refer this link:https:\/\/databricks.com\/glossary\/what-is-parquet\n\nAlso, for some great starter info, refer this notebook by Rob Mulla : https:\/\/www.kaggle.com\/robikscube\/fast-data-loading-and-low-mem-with-parquet-files\/notebook","390b121a":"As we can see from above, there are 3114410 rows and 304 columns, out of which, the features f_0 to f_299 are anonymized features that have been generated using market data and all those are float64 format data, whereas time_id and investment_id are int64 format and row_id is object type which is nothing but row number.\n\nLet's look at the number of unique_values in row_id, time_id, investment_id columns below","376989bb":"Let's now look at some of the continuous_features and it's trend","945d236e":"From the above plots, my observations are that:-\n\n$f_1$, $f_2$, $f_5$, $f_7$ - close to normal distribution\n\n$f_8$, $f_0$ - left tailed distribution \n\n$f_3$ - right tailed distribution \n\n$f_9$, $f_6$, $f_4$ - mixture of distributions\n\nNow, let's look at target distribution"}}