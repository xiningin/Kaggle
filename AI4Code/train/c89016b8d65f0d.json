{"cell_type":{"79b4217e":"code","d0a4324a":"code","b1b1e653":"code","c3dd6f47":"code","2c7617c5":"code","f7a3e654":"code","688019f3":"code","00832464":"code","72e4dfe3":"code","7683368f":"code","9c0050d0":"code","588a5471":"code","f658848f":"code","17ed6c3c":"code","9fdfbff0":"code","fa07e173":"code","a6ba30e3":"code","1d3bceaa":"code","a060b27e":"code","56704b93":"code","d17dca08":"code","b38292d6":"code","d949db48":"code","82fda45e":"code","8146774b":"code","54ed0547":"code","f25ac5c6":"code","452c5653":"code","9fbaa705":"code","648b2ffa":"code","b1ce367b":"code","f60be639":"code","1818212c":"code","010ac8e9":"code","c281c3ac":"code","fdd3f610":"code","66216911":"code","ef3fbcdc":"code","094a3282":"code","9c667b3d":"code","306cd5cb":"code","12e32b43":"code","b02a0531":"code","ffa99c21":"code","ffb8307d":"code","2fdfee4e":"code","80dd67fd":"code","e92081c0":"code","37f2ac5e":"code","3d29d3b3":"code","c0fb095f":"code","b5c91c24":"code","ae684dfb":"code","0ecc12dd":"code","9e2c9c0b":"code","b31cbeda":"code","368fa00e":"code","2da55c3b":"code","2e3fe190":"code","84f1d68d":"code","0b4c0bb9":"code","5cb53237":"code","be0dbc55":"code","2485649b":"code","fd08fb66":"code","18b1773f":"code","e0f8deab":"code","61ce30da":"code","4377fc32":"code","1753ecf4":"code","9dc6e4b7":"code","11e2c89c":"code","d0b6792a":"code","b54f67df":"code","02846ba6":"code","b899d1f4":"code","7ae529a6":"code","de495cc3":"code","9dc11a42":"code","649875eb":"code","7c90b3e2":"code","3f8fd006":"markdown","a2089ebc":"markdown","aff18852":"markdown","76eaf5c0":"markdown","c6e86ae3":"markdown","c12bccfc":"markdown","32aa709e":"markdown","c7f74184":"markdown","c40b556b":"markdown","c6002686":"markdown","39f76a9a":"markdown","b69dd8ec":"markdown","c179a08a":"markdown","e4999fc2":"markdown","96434d28":"markdown","13e24ce2":"markdown","f0cf6105":"markdown","40b6c841":"markdown","ceb69950":"markdown","9c0f6c2f":"markdown","5097a4fc":"markdown","3e2dbf95":"markdown","23b51e3c":"markdown","64eec651":"markdown","883715a7":"markdown","338f41d3":"markdown","a98a2801":"markdown","7cee1896":"markdown","4b4e1641":"markdown","21562bab":"markdown","97d2af16":"markdown","df824447":"markdown","826db5ad":"markdown","0ae08a8d":"markdown","d2ca485c":"markdown","b2a985d5":"markdown","14f84539":"markdown","b57acac3":"markdown","a3901c33":"markdown","dd72d8e3":"markdown","7065138e":"markdown","e8479dff":"markdown"},"source":{"79b4217e":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport xgboost as xgb\nimport lightgbm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.api as s","d0a4324a":"# read both the train and test sets so we can perform feature engineering\n\ndfa = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndfb = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\nembark_index = dfa[dfa['Embarked'].isna()].index    # we'll take a note of the embark_index which we'll drop in the end...\ndfb['Fare'].fillna(dfb['Fare'].mean(), inplace=True)   # value of fare is missing so were gonna replace it with mean\nprint(embark_index)\n\ndfa['train'] = 1\ndfb['train'] = 0\ndf = pd.concat([dfa,dfb])\ny = dfa.Survived\ndf = df.drop(['Survived'], axis=1)\ndf.shape","b1b1e653":"df.isna().sum()","c3dd6f47":"df['cabin_multiple'] = df.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\ndf['cabin_adv'] = df.Cabin.apply(lambda x: str(x)[0])\ndf['numeric_ticket'] = df.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ndf['ticket_letters'] = df.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\n\ndf['name_title'] = df.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\ndf.drop(columns=['Cabin'], inplace=True)\ndf.shape","2c7617c5":"# a function that will return a dataframe with mean median and mode columns\n\ndef to_mmm(df,col):\n    \"\"\" pass dataframe and column name whose missing values you want replaced by mean, median, mode\"\"\"\n    df1 = df[col].fillna(df[col].mean())\n    df2 = df[col].fillna(df[col].median())\n    df3 = df[col].fillna(df[col].mode()[0])\n    df_mmm = pd.DataFrame({col+'_mean':df1,col+'_median':df2,col+'_mode':df3})\n    return df_mmm\n\n\ndf = pd.concat([df, to_mmm(df,'Age')], axis=1)\ndf.shape","f7a3e654":"# a function to view mean, median, mode\n\ndef view_mmm(df,col,type='bar'):\n    df1 = df[col].fillna(df[col].mean())\n    df2 = df[col].fillna(df[col].median())\n    df3 = df[col].fillna(df[col].mode())\n    if type=='bar':\n        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15,5))\n        sns.distplot(df1, ax=ax1)\n        sns.distplot(df[col],ax=ax1)\n        ax1.set_xlabel(col+\"_mean\")\n\n        sns.distplot(df2, ax=ax2) \n        sns.distplot(df[col],ax=ax2)\n        ax2.set_xlabel(col+\"_median\")\n\n        sns.distplot(df3, ax=ax3)\n        sns.distplot(df[col],ax=ax3)\n        ax3.set_xlabel(col+\"_mode\")\n        plt.tight_layout()\n    \n    else:\n        from scipy.stats import probplot\n        probplot(df1, dist=\"norm\", plot=plt)\n        plt.show()\n        probplot(df2, dist=\"norm\", plot=plt)   \n        plt.show()\n        probplot(df3, dist=\"norm\", plot=plt) \n        plt.show()","688019f3":"view_mmm(df,'Age','qqplot')","00832464":"# to fill up with random values \n\ndef rand_samp(df,col):\n\n    if df.shape[0]>2000:\n        print(df.shape)\n        return\n    \n    rand_samp = df[col].dropna().sample(df[col].isna().sum(),random_state=0)\n    rand_samp.index = df[df[col].isna()].index\n    df_rand = df[col].copy()\n    df_rand.loc[df[col].isna(),] = rand_samp\n    df_rand = pd.DataFrame({col+\"_rand_samp\":df_rand})\n    return df_rand\n\ndef view_rand_samp(df,col):\n    rand_samp = df[col].dropna().sample(df[col].isna().sum(),random_state=0)\n    rand_samp.index = df[df[col].isna()].index\n    df_rand = df[col].copy()\n    df_rand.loc[df[col].isna(),] = rand_samp\n    \n    fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(15,5))\n    sns.distplot(df_rand, ax=ax1)\n    sns.distplot(df[col],ax=ax1)\n    from scipy.stats import probplot \n    probplot(df_rand, dist=\"norm\", plot=plt)\n    plt.show()\n        ","72e4dfe3":"df = pd.concat([df, rand_samp(df,'Age')], axis=1) # view_rand_samp(df,'Age')\ndf.shape","7683368f":"# 1 where NaN and 0 where value is available\n\ndef cap_nan(df,col,target=None):\n    df_cap_nan = pd.DataFrame({col+\"_cap_nan\":np.where(df[col].isna(),1,0)})\n    print(df_cap_nan)\n    if target != None:\n        print(df_cap_nan.join(df[target]).corr())\n    return df_cap_nan","9c0050d0":"df = df.join(cap_nan(df,'Age'))\ndf.shape","588a5471":"# value to impute = mean + 3 times of std.deviation... (end of distribution)\n\ndef end_of_dist(df,col):\n    extreme = df[col].mean() + 3*(df[col].std())\n    df_end_of_dist = df[col].fillna(extreme)\n    df_end_of_dist = pd.DataFrame({col+\"_end_of_dist\":df_end_of_dist})\n    print(df_end_of_dist.shape)\n    return df_end_of_dist\n\ndef view_end_of_dist(df,col):\n    extreme = df[col].mean() + 3*df[col].std()\n    df_end_of_dist = df[col].fillna(extreme)\n    \n    fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(15,5))\n    sns.distplot(df_end_of_dist, ax=ax1)\n    sns.distplot(df[col],ax=ax1)\n    from scipy.stats import probplot \n    probplot(df_end_of_dist, dist=\"norm\", plot=plt)\n    plt.show()\n    \n\ndf = pd.concat([df,end_of_dist(df,'Age')],axis=1)\n\ndf.shape","f658848f":"plt.figure(figsize=(15,15))\n\nsns.heatmap(df.corr(),annot=True);","17ed6c3c":"df.info()","9fdfbff0":"df[df.train == 0].shape","fa07e173":"df.shape","a6ba30e3":"df1 = df.copy()","1d3bceaa":"df.head()","a060b27e":"dataset = df.copy()\ndataset['Title'] =  dataset['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)","56704b93":"dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\ndataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\ndataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","d17dca08":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\ndataset['Title'] = dataset['Title'].map(title_mapping)\ndataset['Title'] = dataset['Title'].fillna(0)","b38292d6":"dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","d949db48":"guess_ages = np.zeros((2,3))\nguess_ages\n\nfor i in range(0, 2):\n    for j in range(0, 3):\n        guess_df = dataset[(dataset['Sex'] == i) & \\\n                              (dataset['Pclass'] == j+1)]['Age'].dropna()\n\n        # age_mean = guess_df.mean()\n        # age_std = guess_df.std()\n        # age_guess = rnd.uniform(age_mean - age_std, age_mean + age_std)\n\n        age_guess = guess_df.median()\n\n        # Convert random age float to nearest .5 age\n        guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)","82fda45e":"train_df = dfa\ntest_df = dfb","8146774b":"train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","54ed0547":"dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\ndataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\ndataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\ndataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\ndataset.loc[ dataset['Age'] > 64, 'Age'] = 4","f25ac5c6":"dataset.head()","452c5653":"dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","9fbaa705":"dataset['IsAlone'] = 0\ndataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1","648b2ffa":"dataset['Age*Class'] = dataset.Age * dataset.Pclass","b1ce367b":"dataset['Embarked'] = df['Embarked']\ndataset['Embarked'].fillna('S', inplace=True)","f60be639":"dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)","1818212c":"dataset['Fare'].fillna(dataset['Fare'].dropna().median(), inplace=True)","010ac8e9":"train_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","c281c3ac":"dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\ndataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\ndataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\ndataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\ndataset['Fare'] = dataset['Fare'].astype(int)","fdd3f610":"dataset.isna().sum()","66216911":"# label encode\n\ndataset1 = dataset.copy()\nfor col in ['Sex','Ticket','Embarked','ticket_letters','name_title','cabin_adv']:\n    if dataset1[col].dtype == 'object':\n        encode = LabelEncoder()\n        dataset1[col] = encode.fit_transform(dataset1[col].astype('str')) # few values where both str and float so i used astype('str')        \n","ef3fbcdc":"dataset1.drop(['Name'],axis=1, inplace=True) # its useless","094a3282":"X = dataset1[dataset1['train'] == 1].drop(['PassengerId'],axis=1)\n\n\nmutual_info_vals = mutual_info_classif(X,y)\nmutual_val_df = pd.DataFrame({\"vals\":mutual_info_vals},index=X.columns) # we're keeping the passenger id\nplt.figure(figsize=(10,5))\nmutual_val_df.vals.sort_values(ascending=False).plot(kind='bar');","9c667b3d":"plt.figure(figsize=(18,18))\nsns.heatmap(dataset1.corr(), annot=True, cmap=\"GnBu_r\");","306cd5cb":"dataset1.columns","12e32b43":"plt.figure(figsize=(18,18))\ncor = pd.concat([dataset1[dataset1.train == 1],y],axis=1).corr()\nsns.heatmap(cor, annot=True, cmap=\"GnBu_r\");","b02a0531":"print(cor.Survived.nlargest(n=15),\"\\n\\n\",mutual_val_df.vals.nlargest(n=15))","ffa99c21":"# we'll go with these features\n\nfeatures = ['Title','Sex','Ticket','FamilySize','Fare','cabin_multiple','Embarked','Age*Class','Pclass','ticket_letters']","ffb8307d":"X_f = X[features]\n\nscaler = StandardScaler()\nX_scaled = pd.DataFrame(scaler.fit_transform(X_f), columns=X_f.columns)","2fdfee4e":"X_train,X_test,y_train,y_test = train_test_split(X_scaled, y, test_size=0.33)","80dd67fd":"log_clf = LogisticRegression()\nlog_clf.fit(X_train,y_train)\nlog_clf.score(X_test,y_test)","e92081c0":"dt_clf = DecisionTreeClassifier()\ndt_clf.fit(X_train,y_train)\ndt_clf.score(X_test,y_test)","37f2ac5e":"rf_clf = RandomForestClassifier()\n\nrf_clf.fit(X_train,y_train)\nrf_clf.score(X_test,y_test)","3d29d3b3":"\nvote_model = VotingClassifier(estimators=[\n         ('lr', LogisticRegression()), ('rf', RandomForestClassifier()), \n    ('gnb', xgb.XGBRFClassifier())], voting='hard')","c0fb095f":"vote_model.fit(X_train,y_train)\nvote_model.score(X_test,y_test)","b5c91c24":"\nmodel = GradientBoostingClassifier()\nmodel.fit(X_train,y_train)\nmodel.score(X_test,y_test)","ae684dfb":"df1.head()","0ecc12dd":"\ndf1 = df.copy()\ndf1.drop(['Age', 'Name',],axis=1,inplace=True)\ndf1.dropna(inplace=True)\n\nfor col in ['Sex','Ticket','Embarked','ticket_letters','name_title','cabin_adv']:\n    if df1[col].dtype == 'object':\n        encode = LabelEncoder()\n        df1[col] = encode.fit_transform(df1[col].astype('str')) # few values where both str and float so i used astype('str')        \n\ndf1.head()\ndf1[df1.train == 0].shape","9e2c9c0b":"# y = target\ny = y[y.index != embark_index[0]]\ny = y[y.index != embark_index[1]]","b31cbeda":"X = df1[df1['train'] == 1].drop(['PassengerId'],axis=1)\n\nmutual_info_vals = mutual_info_classif(X,y)\nmutual_val_df = pd.DataFrame({\"vals\":mutual_info_vals},index=X.columns) # we're keeping the passenger id\nplt.figure(figsize=(10,5))\nmutual_val_df.vals.sort_values(ascending=False).plot(kind='bar');","368fa00e":"selector = SelectKBest(chi2, k=10)\nselector.fit(X, y)\n# Get columns to keep and create new dataframe with those only\ncols = selector.get_support(indices=True)\nfeatures_df_new = X.iloc[:,cols]","2da55c3b":"# plt.figure(figsize=(10,5))\n\nfeatures_df_new.columns","2e3fe190":"from sklearn.feature_selection import SelectPercentile\n\nselector2 = SelectPercentile(chi2, percentile=60)\nselector2.fit(X, y)\n# Get columns to keep and create new dataframe with those only\ncols = selector2.get_support(indices=True)\nfeatures_df_new2 = X.iloc[:,cols]\nfeatures_df_new2.columns","84f1d68d":"from sklearn.model_selection import train_test_split\n\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.33, )","0b4c0bb9":"from sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.datasets import make_classification\n\n\nX1, y1 = make_classification(n_features=4, random_state=0)\nclf = ExtraTreesClassifier(n_estimators=100, random_state=0)\nclf.fit(X_train, y_train)\nExtraTreesClassifier(random_state=0)\nclf.score(X_test,y_test)","5cb53237":"# top_features = []\n\nprint(mutual_val_df.vals.sort_values(ascending=False)[:10])\nprint(features_df_new.columns[:10])\nprint(features_df_new2.columns[:10])","be0dbc55":"# getting the repetations... and finding top 15\n\ntop_15 = mutual_val_df.vals.sort_values(ascending=False).index[:15]\ntop_15 = top_15.append(features_df_new.columns[:15])\ntop_15 = top_15.append(features_df_new2.columns[:15])\ntop_15.value_counts()","2485649b":"top_15_f = ['Pclass','Sex','Fare','cabin_multiple','cabin_adv','Ticket','Age_mode','SibSp','ticket_letters','Parch','Embarked','name_title','numeric_ticket']","fd08fb66":"X_sub_test = df1[df1['train'] == 0]","18b1773f":"X_sub_test[top_15_f]","e0f8deab":"log_clf = LogisticRegression()\n\nlog_clf.fit(X_train,y_train)\nlog_clf.score(X_test,y_test)","61ce30da":"log_clf = LogisticRegression()\n\nlog_clf.fit(X_train[top_15_f],y_train)\nlog_clf.score(X_test[top_15_f],y_test)","4377fc32":"dt_clf = DecisionTreeClassifier()\n\ndt_clf.fit(X_train,y_train)\ndt_clf.score(X_test,y_test)","1753ecf4":"dt_clf = DecisionTreeClassifier()\n\ndt_clf.fit(X_train[top_15_f],y_train)\ndt_clf.score(X_test[top_15_f],y_test)","9dc6e4b7":"# just drop correlated age columns except age_mode\n\nrf_clf = RandomForestClassifier()\n\nrf_clf.fit(X_train.drop(['Age_median','Age_mean','Age_rand_samp','Age_cap_nan','Age_end_of_dist'],axis=1),y_train)\nrf_clf.score(X_test.drop(['Age_median','Age_mean','Age_rand_samp','Age_cap_nan','Age_end_of_dist'],axis=1),y_test)","11e2c89c":"\nvote_model = VotingClassifier(estimators=[\n         ('lr', LogisticRegression()), ('rf', RandomForestClassifier()), \n    ('gnb', xgb.XGBRFClassifier())], voting='hard')","d0b6792a":"vote_model.fit(X_train,y_train)","b54f67df":"vote_model.score(X_test,y_test)","02846ba6":"mutual_val_df.vals.sort_values(ascending=False).index","b899d1f4":"\nvote_model = VotingClassifier(estimators=[\n         ('lr', LogisticRegression()), ('rf', RandomForestClassifier()), \n    ('gn',KNeighborsClassifier(n_neighbors=6))], voting='hard')\n\nvote_model.fit(X_train[['Sex', 'name_title', 'Fare', 'Ticket', 'Pclass', 'ticket_letters',\n       'Parch', 'Age_end_of_dist', 'Embarked', 'cabin_adv', 'cabin_multiple', 'SibSp','numeric_ticket']],y_train)","7ae529a6":"vote_model.score(X_test[['Sex', 'name_title', 'Fare', 'Ticket', 'Pclass', 'ticket_letters',\n       'Parch', 'Age_end_of_dist', 'Embarked', 'cabin_adv', 'cabin_multiple', 'SibSp','numeric_ticket']],y_test)","de495cc3":"vote_model.fit(X[['Sex', 'name_title', 'Fare', 'Ticket', 'Pclass', 'ticket_letters',\n       'Parch', 'Age_end_of_dist', 'Embarked', 'cabin_adv', 'cabin_multiple', 'SibSp','numeric_ticket']],y)\ny_vote2_pred = vote_model.predict(X_sub_test[['Sex', 'name_title', 'Fare', 'Ticket', 'Pclass', 'ticket_letters',\n       'Parch', 'Age_end_of_dist', 'Embarked', 'cabin_adv', 'cabin_multiple', 'SibSp','numeric_ticket']])","9dc11a42":"model = xgb.XGBClassifier()\nmodel.fit(X_train[['Sex', 'name_title', 'Fare', 'Ticket', 'Pclass', 'ticket_letters',\n       'Parch', 'Age_mode', 'Embarked']],y_train)\nmodel.score(X_test[['Sex', 'name_title', 'Fare', 'Ticket', 'Pclass', 'ticket_letters',\n       'Parch', 'Age_mode', 'Embarked']],y_test)","649875eb":"# we first use knn to make clusters in our dataset, then use a couple of models on it to test which one works best...\n# in testing phase we use the knn model and the respective model which performed well for respective clusters to obtain the results\n\ndef clustering_approach(X,y, models,type = \"none\"):\n    \n    dfs = {}\n    X_cls = {}\n    y_cls = {}\n    X_scaled = {}\n    X_train, X_test, y_train, y_test = {},{},{},{}\n    y_pred = {}\n    models_out = {}\n    \n    # create knn model and predict\n    knn_clf = KNeighborsClassifier()\n    knn_clf.fit(X,y)\n    df = pd.concat([X,y],axis=1) # so we can later separate x and y for each cluster\n    df['knn_clf'] = knn_clf.predict(X)\n    no_cls = knn_clf.classes_\n    \n    # get the dataframes, apply std.scaler, form train, test sets, apply models\n    for cls in knn_clf.classes_:\n        print(\"--------------The {} cluster's results-------------------\".format(cls),end=\"\\n\\n\")\n        dfs[cls] = df[df['knn_clf'] == cls].iloc[:,:-1]\n        \n        X_cls[cls] = dfs[cls].iloc[:,:-1]\n        y_cls[cls] = dfs[cls].iloc[:,-1]\n        scaler = StandardScaler()\n        X_scaled[cls] = scaler.fit_transform(X_cls[cls])\n#         X_scaled[cls] = pd.DataFrame(X_scaled[cls],columns=df.columns[:-1])\n    \n        X_train[cls],X_test[cls],y_train[cls],y_test[cls] = train_test_split(X_scaled[cls],y_cls[cls],test_size=0.3,random_state=3)\n        print(\"here\")\n        # type can be used for analyzing... eg: confusion matrix\n        for model in models:\n            model.fit(X_train[cls], y_train[cls])\n            y_pred[cls] = model.predict(X_test[cls])\n            print(model)\n            print(model.score(X_test[cls],y_test[cls]))\n            print(confusion_matrix(y_pred[cls], y_test[cls]), end=\"\\n\\n\")\n            models_out[str(model) + str(cls)] = model\n            \n    \n    return [X_train, X_test, y_train, y_test,knn_clf, models_out]\n        ","7c90b3e2":"\nmodels = [LogisticRegression(), RandomForestClassifier(), KNeighborsClassifier(), XGBClassifier(verbosity = 0),LGBMClassifier(),SVC()] \nX_train, X_test, y_train, y_test,clusterer, models = clustering_approach(X,y,models)\n","3f8fd006":"### train_test_split","a2089ebc":"### now lets convert all features to numeric and do feature selection","aff18852":"### Random forest","76eaf5c0":"### Lets select the features...\n","c6e86ae3":"### The first 4 features we're by Ken Jee's notebook ( he's a Data Science YouTuber )","c12bccfc":"### VotingClassifier with (LogisticRegression, RandomForestClassifier, XGBRFClassifier)","32aa709e":"### feature selection","c7f74184":"### Gradient Boostinng","c40b556b":"### create a artificial column called age class","c6002686":"### 1.replacing with mean median and mode...\n","39f76a9a":"### 2. Random sample imputaion","b69dd8ec":"### -----------------------------------------","c179a08a":"### Train test split","e4999fc2":"### Logistic regression","96434d28":"### Trying VotingClassifier model","13e24ce2":"### vote model with top mutual val features","f0cf6105":"### Desision Tree with top 15","40b6c841":"### Decision tree","ceb69950":"### create a family size feature","9c0f6c2f":"## Trying some of the features by manav sehgal (most voted notebook on kaggle for titanic)","5097a4fc":"### Random forest\n","3e2dbf95":"### Lets start with feature engineering techniques we know","23b51e3c":"## imports","64eec651":"### Decision tree","883715a7":"### mapping categotical features","338f41d3":"### title mapping","a98a2801":"### 3. Capturing NaN values with a new feature","7cee1896":"### Logistic regression","4b4e1641":"### Now lets try with our features","21562bab":"### fill embarked","97d2af16":"### feature selection using select k best","df824447":"### scale the features","826db5ad":"### Using mutual_info_classif to find values columns that contribute more towards the output","0ae08a8d":"### creating age bands","d2ca485c":"### Get top 15 features from all 3 feature selection techniques","b2a985d5":"### create a \"isalone\" column","14f84539":"### Clustering approach","b57acac3":"### Feature selection using SelectPercentile","a3901c33":"### Logistic regression with our top_15 features","dd72d8e3":"### Lets check how our features perform for our train_test_split data on a few models","7065138e":"### 4. End of distribution imputation","e8479dff":"### label encode"}}