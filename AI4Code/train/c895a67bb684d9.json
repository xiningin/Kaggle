{"cell_type":{"765c9bf6":"code","46ec713a":"code","6c5dbb53":"code","29e74982":"code","a3b3fedd":"code","0eb2269a":"code","3b694201":"code","54c0e1f8":"code","c5bca1f3":"code","db8c68cc":"code","85a1f27d":"code","0f2229fb":"code","83d2b506":"code","8049bade":"code","7bcbebd9":"code","eb570b36":"code","8d3f7698":"code","6b68f8fa":"code","c1e6ccae":"code","1ab28182":"markdown","fe63d8c2":"markdown","39af3600":"markdown","01e8d36c":"markdown","55166cb2":"markdown","d3a8f210":"markdown","291dcee1":"markdown","dfcddba9":"markdown","8489f247":"markdown","c7e262f1":"markdown","abb4cd3f":"markdown","7be05230":"markdown","64d052d2":"markdown","dd376bd7":"markdown"},"source":{"765c9bf6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","46ec713a":"import numpy as np\nimport pandas as pd\nimport pandas_profiling\nfrom sklearn.linear_model import LogisticRegressionCV, SGDClassifier, LogisticRegression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix","6c5dbb53":"raw_csv_data = pd.read_csv(\"..\/input\/loan-predication\/train_u6lujuX_CVtuZ9i (1).csv\")","29e74982":"raw_csv_data.profile_report()","a3b3fedd":"raw_csv_data['Gender'] = raw_csv_data['Gender'].map({'Male':1,'Female':0})\nraw_csv_data['Married'] = raw_csv_data['Married'].map({'Yes':1,'No':0})\nraw_csv_data['Education'] = raw_csv_data['Education'].map({'Graduate':1,'Not Graduate':0})\nraw_csv_data['Self_Employed'] = raw_csv_data['Self_Employed'].map({'Yes':1,'No':0})\nraw_csv_data['Property_Area'] = raw_csv_data['Property_Area'].map({'Urban':1,'Rural':2,'Semiurban':3})\nraw_csv_data['Loan_Status'] = raw_csv_data['Loan_Status'].map({'Y':1,'N':0})","0eb2269a":"raw_csv_data","3b694201":"#Keeping a checkpoint\npre_process_data = raw_csv_data","54c0e1f8":"##Insert new columns for dependents\npre_process_data.insert(0, \"Dependents_1\", 0) \npre_process_data.insert(1, \"Dependents_2\", 0) \npre_process_data.insert(2, \"Dependents_3\", 0) \npre_process_data.insert(3, \"Dependents_4\", 0) ","c5bca1f3":"#Fill all the values for newly created Dependent Column.\nfor ind in pre_process_data.index: \n     if(pre_process_data['Dependents'][ind] == 0):\n        pre_process_data['Dependents_1'][ind] = 1\n     elif(pre_process_data['Dependents'][ind] == 1):\n        pre_process_data['Dependents_2'][ind] = 1\n     elif(pre_process_data['Dependents'][ind] == 2):\n        pre_process_data['Dependents_3'][ind] = 1\n     elif(pre_process_data['Dependents'][ind] == '3+'):\n        pre_process_data['Dependents_4'][ind] = 1","db8c68cc":"##Drop the Loan Id and the Dependent Column.\npre_process_data = pre_process_data.drop(['Dependents'], axis=1)\npre_process_data = pre_process_data.drop(['Loan_ID'], axis=1)","85a1f27d":"##Get Dummies\ndummies= pd.get_dummies(pre_process_data, drop_first=True)\n\n# We will now impute values\nSimImp = SimpleImputer()\npre_process_data = pd.DataFrame(SimImp.fit_transform(dummies), columns=dummies.columns)","0f2229fb":"##All the missing values have been filled.\npre_process_data.info()","83d2b506":"unscaled_inputs=pre_process_data.iloc[:,:-1]\n\n##Standardize the input\nfrom sklearn.preprocessing import StandardScaler\nloan_scaler = StandardScaler()\nloan_scaler.fit(unscaled_inputs)","8049bade":"scaled_inputs = loan_scaler.transform(unscaled_inputs)\nscaled_inputs","7bcbebd9":"##Splitting the Target\ntargets = pre_process_data['Loan_Status'] ","eb570b36":"##Split the data into train test and shuffle\nfrom sklearn.model_selection import train_test_split\ntrain_test_split(scaled_inputs,targets)","8d3f7698":" ##Logistic Regression with SKlearn\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nx_train, x_test, y_train, y_test = train_test_split(scaled_inputs,targets, train_size=0.8)\n\n##Training the Model\nreg=LogisticRegression()\nreg.fit(x_train,y_train)","6b68f8fa":"reg.score(x_train,y_train)","c1e6ccae":"##Testing the Model\nreg.score(x_test,y_test)","1ab28182":"As you see that there is 80% accuracy of the Model.\nNow we will test our model with the test data that we had splitted before.","fe63d8c2":"As we see that there are lot of missing values in the dataset, we will use the Simple Impute from the scikit library to fill the missing values with the dummy values.","39af3600":"Map all the categorical values with numerical","01e8d36c":"Importing the Relevant libraries to be used in the Kernel","55166cb2":"So the first step is preprocessing.\nIn preprocessing we will try to identify the following points.\n1. Checking the numerical and categorical variables.\n2. Checking for the missing values.\n3. Filling the missing values with dummy values.\n4. Mapping the categorical variables\n5. Deleting the unwanted columns.","d3a8f210":"It is a very basic kernel for someone who is gettiing started in the field of data science.\nI have tried to predict loan application approvals with a relatively small dataset.\nBelow are the steps that have been carried out in the Kernel.\n1. Pre Processing the Dataset\n2. Splitting the Input and the Target\n3. Standardizing the Dataset\n4. Splitting the Input and Target into Test and Train dataset\n5. Creating the Model\n6. Testing the Model","291dcee1":"I will use the Pandas Profiling to see the different variables used in the dataset.\nIt will help us identify the missing values, outliers in the dataset.","dfcddba9":"As the Column Loan Status is our Target, we will split the target.\nAfter that we will Standardize the Input by using Standard Scaler of Sklearn.","8489f247":"Our Model gives around 82% accuracy with the test data.\nThanks for seeing the kernel.\nPlease provide your Comments and suggestion as I am still in the initial learning phase.","c7e262f1":"Loading the Dataset","abb4cd3f":"Create the Model","7be05230":"Now we will split the data into Test and Train using test_train_split from Sklearn","64d052d2":"Check the Accuracy of the Model","dd376bd7":"As we can see that the column Dependents have some values as 3+.\nWe will create 4 cloumns as Dependent_1, Dependent_2, Dependent_3, Dependent_4 for each(0,1,2,3+) and map all the values with 1."}}