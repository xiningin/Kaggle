{"cell_type":{"2493a0ab":"code","dfe91b93":"code","7427ad4f":"code","9e428de7":"code","91aa2f77":"code","2f293e2b":"code","78fb97c6":"code","c92f869e":"code","c2df10d9":"code","3f948477":"code","feb8fdb9":"code","757888e7":"code","ecfbf4c9":"code","32621376":"code","5e1d5bcd":"code","581462a4":"code","d734f859":"code","74898863":"code","e71ea09e":"code","0c970552":"code","645e8a4c":"code","4a0fae3a":"code","f8e1150d":"code","cac77588":"code","1751ffe9":"code","e6825a68":"code","92ce8a3b":"code","7a1fefe1":"code","a8f9359b":"code","62e34dc1":"code","12350072":"code","79e7d39d":"code","13f9ce72":"code","01590582":"code","62769ce6":"code","5e2505b9":"code","9d907688":"code","46573a91":"code","6ddd7779":"code","d57dafec":"code","b8300129":"code","df10ec9b":"code","1b0fc926":"code","52b69639":"code","96cda010":"markdown","98cd0ae6":"markdown","de5127c1":"markdown","080c6b7f":"markdown","03ede402":"markdown","bf5484e6":"markdown","bf98a0c5":"markdown","20e1dc55":"markdown","8ecb51cd":"markdown","5fb85782":"markdown","cf45fd79":"markdown","fced8d82":"markdown","3edf5d1b":"markdown","0834cbdd":"markdown","f8fc67de":"markdown","6535200c":"markdown","49baf33a":"markdown","0014b38b":"markdown","09b877aa":"markdown","3554e479":"markdown","164ad1a4":"markdown","59b5ae30":"markdown","05ede699":"markdown","5c849504":"markdown","f41d64ef":"markdown","c88d0791":"markdown","25589139":"markdown","6e7e6668":"markdown","39a6d97e":"markdown","f56df5c9":"markdown","41878596":"markdown","b2fd0d95":"markdown","079d2796":"markdown","2ec40071":"markdown","4924b573":"markdown","4192375c":"markdown","75cf9fd5":"markdown","090d3ff0":"markdown","e1eaef87":"markdown","e38fbef3":"markdown","1fb05348":"markdown","4454e591":"markdown","52c8983d":"markdown","0963fdcf":"markdown","c685a547":"markdown","8ad976f3":"markdown","2891480e":"markdown","ec34013c":"markdown","349f75f2":"markdown","13cc6e7c":"markdown","029559bb":"markdown","58b0b45b":"markdown","3aa6c298":"markdown","dd4e845f":"markdown","bb1b9565":"markdown","b035eb7b":"markdown","7baf7c24":"markdown","160d1a2c":"markdown","8a919611":"markdown","8fd8bdf4":"markdown","fe654c93":"markdown","4f19b3c4":"markdown","e923d9c9":"markdown","642b5d9b":"markdown","10f960c1":"markdown","029c65a6":"markdown","ac84f7c8":"markdown","19c10966":"markdown"},"source":{"2493a0ab":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets as dt\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nimport matplotlib\n%matplotlib inline\n\nfrom skimage.io import imread, imshow\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\n\nfrom matplotlib.cbook import flatten","dfe91b93":"\n#from subprocess import check_output\n#print(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n","7427ad4f":"fashion_mnist = keras.datasets.fashion_mnist\n\n(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()","9e428de7":"\n#df = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\n#df.head()","91aa2f77":"\n#y = df.label.values\n#X = df.drop(\"label\",axis=1).values\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=0)\n","2f293e2b":"fig1, ax1 = plt.subplots(1,15, figsize=(15,10))\nfor i in range(15):\n    ax1[i].imshow(X_test[i].reshape((28,28)), cmap=\"gray_r\")\n    ax1[i].axis('off')\n    ax1[i].set_title(y_test[i])","78fb97c6":"X_train1=[]\nX_test1=[]\nfor imgnum in range(0,60000):\n    X_train1.append(list(flatten(X_train[imgnum])))\nfor imgnum in range(0,10000):\n    X_test1.append(list(flatten(X_test[imgnum])))\n\nX_train1=np.array(X_train1).reshape(-1, 784)\nX_test1=np.array(X_test1).reshape(-1, 784)\nX_train=X_train1\nX_test=X_test1","c92f869e":"class Attack:\n\n    def __init__(self, model):\n        self.fooling_targets = None\n        self.model = model\n\n    \n    def prepare(self, X_train, y_train, X_test, y_test):\n        self.images = X_test\n        self.true_targets = y_test\n        self.num_samples = X_test.shape[0]\n        self.train(X_train, y_train)\n        print(\"Model training finished.\")\n        self.test(X_test, y_test)\n        print(\"Model testing finished. Initial accuracy score: \" + str(self.initial_score))\n    \n    def set_fooling_targets(self, fooling_targets):\n        self.fooling_targets = fooling_targets\n    \n    def train(self, X_train, y_train):\n        self.model.fit(X_train, y_train)\n        self.weights = self.model.coef_\n        self.num_classes = self.weights.shape[0]\n\n    def test(self, X_test, y_test):\n        self.preds = self.model.predict(X_test)\n        self.preds_proba = self.model.predict_proba(X_test)\n        self.initial_score = accuracy_score(y_test, self.preds)\n    \n    def create_one_hot_targets(self, targets):\n        self.one_hot_targets = np.zeros(self.preds_proba.shape)\n        for n in range(targets.shape[0]):\n            self.one_hot_targets[n, targets[n]] = 1\n            \n    def attack(self, attackmethod, epsilon):\n        perturbed_images, highest_epsilon = self.perturb_images(epsilon, attackmethod)\n        perturbed_preds = self.model.predict(perturbed_images)\n        score = accuracy_score(self.true_targets, perturbed_preds)\n        return perturbed_images, perturbed_preds, score, highest_epsilon\n    \n    def perturb_images(self, epsilon, gradient_method):\n        perturbed = np.zeros(self.images.shape)\n        max_perturbations = []\n        for n in range(self.images.shape[0]):\n            perturbation = self.get_perturbation(epsilon, gradient_method, self.one_hot_targets[n], self.preds_proba[n])\n            perturbed[n] = self.images[n] + perturbation\n            max_perturbations.append(np.max(perturbation))\n        highest_epsilon = np.max(np.array(max_perturbations))\n        return perturbed, highest_epsilon\n    \n    def get_perturbation(self, epsilon, gradient_method, target, pred_proba):\n        gradient = gradient_method(target, pred_proba, self.weights)\n        inf_norm = np.max(gradient)\n        perturbation = epsilon\/inf_norm * gradient\n        return perturbation\n    \n    def attack_to_max_epsilon(self, attackmethod, max_epsilon):\n        self.max_epsilon = max_epsilon\n        self.scores = []\n        self.epsilons = []\n        self.perturbed_images_per_epsilon = []\n        self.perturbed_outputs_per_epsilon = []\n        for epsilon in range(0, self.max_epsilon):\n            perturbed_images, perturbed_preds, score, highest_epsilon = self.attack(attackmethod, epsilon)\n            self.epsilons.append(highest_epsilon)\n            self.scores.append(score)\n            self.perturbed_images_per_epsilon.append(perturbed_images)\n            self.perturbed_outputs_per_epsilon.append(perturbed_preds)","c2df10d9":"def calc_output_weighted_weights(output, w):\n    for c in range(len(output)):\n        if c == 0:\n            weighted_weights = output[c] * w[c]\n        else:\n            weighted_weights += output[c] * w[c]\n    return weighted_weights\n\ndef targeted_gradient(foolingtarget, output, w):\n    ww = calc_output_weighted_weights(output, w)\n    for k in range(len(output)):\n        if k == 0:\n            gradient = foolingtarget[k] * (w[k]-ww)\n        else:\n            gradient += foolingtarget[k] * (w[k]-ww)\n    return gradient\n\ndef non_targeted_gradient(target, output, w):\n    ww = calc_output_weighted_weights(output, w)\n    for k in range(len(target)):\n        if k == 0:\n            gradient = (1-target[k]) * (w[k]-ww)\n        else:\n            gradient += (1-target[k]) * (w[k]-ww)\n    return gradient\n\ndef non_targeted_sign_gradient(target, output, w):\n    gradient = non_targeted_gradient(target, output, w)\n    return np.sign(gradient)","3f948477":"model = LogisticRegression(multi_class='multinomial', solver='lbfgs', fit_intercept=False)","feb8fdb9":"attack = Attack(model)\nattack.prepare(X_train, y_train, X_test, y_test)","757888e7":"weights = attack.weights\nweights.shape","ecfbf4c9":"num_classes = len(np.unique(y_train))\nnum_classes","32621376":"attack.create_one_hot_targets(y_test)\nattack.attack_to_max_epsilon(non_targeted_gradient, 30)\nnon_targeted_scores = attack.scores","5e1d5bcd":"sns.set()\nplt.figure(figsize=(10,5))\nplt.plot(attack.epsilons, attack.scores, 'g*')\nplt.ylabel('accuracy_score')\nplt.xlabel('epsilon')\nplt.title('Accuracy score breakdown - non-targeted attack');","581462a4":"eps = 16\nattack.epsilons[eps]","d734f859":"example_images = attack.perturbed_images_per_epsilon[eps]\nexample_preds = attack.perturbed_outputs_per_epsilon[eps]","74898863":"example_results = pd.DataFrame(data=attack.true_targets, columns=['y_true'])\nexample_results['y_fooled'] = example_preds\nexample_results['y_predicted'] = attack.preds\nexample_results['id'] = example_results.index.values\nexample_results.head()","e71ea09e":"success_df = example_results[example_results.y_fooled != example_results.y_true]\nsuccess_df.head()","0c970552":"example_id = success_df.id.values[0]\nexample_id","645e8a4c":"fig2, ax2 = plt.subplots(4,4, figsize=(15,15))\nfor i in range(4):\n    for j in range(4):\n        image = attack.perturbed_images_per_epsilon[i*4 + j][example_id]\n        y_fooled = attack.perturbed_outputs_per_epsilon[i*4 + j][example_id]\n        epsilon = attack.epsilons[i*4 +j]\n        ax2[i,j].imshow(image.reshape((28,28)), cmap=\"gray_r\")\n        ax2[i,j].axis('off')\n        ax2[i,j].set_title(\"true: \" + str(y_test[example_id]) + \", fooled: \" + str(y_fooled)  + \"\\n\" \n                           + \"epsilon: \" + str(np.int(epsilon)))","4a0fae3a":"fig, (axA, axB, axC) = plt.subplots(1, 3, figsize=(15,5))\naxB.imshow(example_images[example_id].reshape((28,28)), cmap='Greens')\naxB.set_title(\"Non-targeted attack result: \" + str(example_preds[example_id]))\naxA.imshow(X_test[example_id].reshape((28,28)), cmap='Greens')\naxA.set_title(\"True label: \" + str(y_test[example_id]))\naxC.imshow((X_test[example_id]-example_images[example_id]).reshape((28,28)), cmap='Reds')\naxC.set_title(\"Perturbation: epsilon 16\");","f8e1150d":"plt.figure(figsize=(10,5))\nsns.countplot(x='y_fooled', data=example_results[example_results.y_true != example_results.y_fooled])","cac77588":"wrong_predictions = example_results[example_results.y_true != example_results.y_predicted]\nwrong_predictions.shape","1751ffe9":"X_test.shape","e6825a68":"plt.figure(figsize=(10,5))\nsns.countplot(x='y_predicted', data=wrong_predictions)","92ce8a3b":"plt.figure(figsize=(10,5))\nsns.countplot(x='y_true', data=wrong_predictions)","7a1fefe1":"attacktargets = example_results.loc[example_results.y_true != example_results.y_fooled].groupby(\n    'y_true').y_fooled.value_counts()\ncounts = example_results.loc[example_results.y_true != example_results.y_fooled].groupby(\n    'y_true').y_fooled.count()\nattacktargets = attacktargets\/counts * 100\nattacktargets = attacktargets.unstack()\nattacktargets = attacktargets.fillna(0.0) \nattacktargets = attacktargets.apply(np.round).astype(np.int)","a8f9359b":"f, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(attacktargets, annot=True, ax=ax, cbar=False, square=True, cmap=\"Reds\", fmt=\"g\");\nax.set_title(\"How often was y_true predicted as some y_fooled digit in percent?\");","62e34dc1":"example = X_test[0]\nimshow(example.reshape((28,28)), cmap='Greens');","12350072":"print(\"true label target: \" + str(y_test[0]))","79e7d39d":"fooling_classes = []\nfor k in range(num_classes):\n    if k != y_test[0]:\n        fooling_classes.append(k)\nfooling_classes","13f9ce72":"foolingtargets = np.zeros((len(fooling_classes), num_classes))\nfor n in range(len(fooling_classes)):\n    foolingtargets[n,fooling_classes[n]] = 1\nfoolingtargets","01590582":"eps=128\ntargeted_perturbed_images = []\ntargeted_perturbed_predictions = []\nfor fooling_target in foolingtargets:   \n    targeted_perturbation = attack.get_perturbation(eps, targeted_gradient, fooling_target, attack.preds_proba[0])\n    targeted_perturbed_image = X_test[0]+ targeted_perturbation\n    targeted_perturbed_prediction = attack.model.predict(targeted_perturbed_image.reshape(1, -1))\n    targeted_perturbed_images.append(targeted_perturbed_image)\n    targeted_perturbed_predictions.append(targeted_perturbed_prediction)","62769ce6":"targeted_perturbed_predictions","5e2505b9":"fig3, ax3 = plt.subplots(3,3, figsize=(9,9))\nfor i in range(3):\n    for j in range(3):\n        ax3[i,j].imshow(targeted_perturbed_images[i*3+j].reshape((28,28)), cmap=\"Greens\")\n        ax3[i,j].axis('off')\n        ax3[i,j].set_title(\"fooling result: \" + str(targeted_perturbed_predictions[i*3+j][0]))","9d907688":"f, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(attacktargets, annot=True, ax=ax, cbar=False, cmap=\"Purples\", fmt=\"g\");","46573a91":"natural_targets_dict = {}\nnon_natural_targets_dict = {}\nfor ix, series in attacktargets.iterrows():\n    natural_targets_dict[ix] = series.argmax()\n    non_natural_targets_dict[ix] = series.drop(ix).argmin()","6ddd7779":"natural_targets_dict","d57dafec":"natural_foolingtargets = np.zeros((y_test.shape[0]))\nnon_natural_foolingtargets = np.zeros((y_test.shape[0]))\n\nfor n in range(len(natural_foolingtargets)):\n    target = y_test[n]\n    natural_foolingtargets[n] = natural_targets_dict[target]\n    non_natural_foolingtargets[n] = non_natural_targets_dict[target]","b8300129":"attack.create_one_hot_targets(natural_foolingtargets.astype(np.int))\nattack.attack_to_max_epsilon(targeted_gradient, 30)\nnatural_scores = attack.scores\nattack.create_one_hot_targets(non_natural_foolingtargets.astype(np.int))\nattack.attack_to_max_epsilon(targeted_gradient, 30)\nnon_natural_scores = attack.scores","df10ec9b":"plt.figure(figsize=(10,5))\nnf, = plt.plot(attack.epsilons, natural_scores, 'g*', label='natural fooling')\nnnf, = plt.plot(attack.epsilons, non_natural_scores, 'b*', label='non-natural fooling')\nplt.legend(handles=[nf, nnf])\nplt.ylabel('accuracy_score')\nplt.xlabel('epsilon')\nplt.title('Accuracy score breakdown: natural vs non-natural targeted attack');","1b0fc926":"attack.create_one_hot_targets(y_test)\nattack.attack_to_max_epsilon(non_targeted_sign_gradient, 30)","52b69639":"plt.figure(figsize=(10,5))\ngm, = plt.plot(attack.epsilons, non_targeted_scores, 'g*', label='gradient method')\ngsm, = plt.plot(attack.epsilons, attack.scores, 'r*', label='gradient sign method')\nplt.ylabel('accuracy_score')\nplt.xlabel('eta')\nplt.legend(handles=[gm, gsm])\nplt.title('Accuracy score breakdown')","96cda010":"For the second part we should keep in mind the gradient of activations yields the class weight vector:\n\n$$ a_{c} = \\vec{w_{c}}^{T} \\vec{x} = \\vec{x}^{T} \\vec{w_{c}} $$\n\n$$ \\nabla_{x} a_{c} = \\vec{w_{c}} $$\n\n$$ y_{k} = \\frac{\\exp(a_{k})} {\\sum_{c=1}^{K}\\exp(a_{c})}$$","98cd0ae6":"This looks like playing with equations but we gained an insight: We have found that we could express the perturbation by a masking of the Jacobian matrix. The letter tells us more generally how the outputs of all classes change with input perturbations. This is already a hint that we could built attacks and defenses on approaches based on the Jacobian. Besides the Fast Gradient Attack you can find a Jacobian-based approach as well. Have fun to explore! :-)","de5127c1":"Some digits are not easily recognized by human eyes... ","080c6b7f":"Taking the derivative of the outputs for each class $\\partial y_{k}$ with respect to the input features $\\partial x_{d}$ (with d in range for 1 to 64 pixel) we have arrived at the **Jacobian matrix**: ","03ede402":"## Code - methods and classes <a class=\"anchor\" id=\"methods\"><\/a>\n\nAs I don't like to fill this kernel with same code used for different cases again and again, I will store it here. I hope, this makes it also easier for you to play with this kernel if you like to fork :-)","bf5484e6":"## Non-targeted Attack: Maximizing output-target discrepance <a class=\"anchor\" id=\"nontargeted\"><\/a>\n\nIn analogy to the maximum likelihood approach I define a discrepancy function as my objective. Usually the likelihood gives us the probability that our model output y matches the target t. Thus maximizing the likelihood yields us the best matches. Creating an attack one has to think the other way round: We want to maximize the probability that the outputs do not match the targets. Let's write down this descrepancy function by using the multinomial distribution again:\n\n$$ D(t|y(w,x)) = \\prod_{n=1}^{N} \\prod_{k=1}^{K} y_{n,k}^{1-t_{n,k}} $$","bf98a0c5":"## Comparison to Fast Gradient Method <a class=\"anchor\" id=\"fastgradient\"><\/a>\n\n\nSo far we have used gradient ascent to maximize the probability of no-matches (non-targeted & targeted) and we derived the gradient with respect to inputs for objectives that are only slightly changed in comparison to the model defining likelihood function. This way we are very close to the Fast Gradient Method given in the cleverhans library:\n","20e1dc55":"### Maximizing discrepance = minimizing likelihood?\n\nNormally we use the likelihood to maximize the probability and we want to count those outputs $y_{n,c}$ related to target-values of $t_{n,c}=1$. Look at one example image $m$ that belongs to class 2 of 3, then for this one we would have:\n\n$$ l = y_{1}^{0} * y_{2}^{1} * y_{3}^{0} = y_{2} $$\n\nHence this image contribute to the overall product over $n$ by a factor of $y_{2}$. Consequently each image $n$ would give us a y-factor of its true class whereas all the others only give us a factor of 1 such that it does not disturb our nice probability. If we would now use the same likelihood to represent the probability of no-matches we encounter a problem: To minimize the likelihood the only thing that can be done now is to set $y_{2}=0$ for image m. We would do this for all outputs that enter the likelihood by themselves. But what is with all the others? What to do with $y_{1}$ and $y_{3}$ for image m? Looking for a minimum of $y_{2}$ we could select anything for $y_{1}$ and $y_{3}$ as they just stay 1. That's ill-defined in contrast to our discrepance probability. There the situation for image m would look like this:\n\n$$ l = y_{1}^{1} * y_{2}^{0} * y_{3}^{1} = y_{1} * y_{3} $$\n\nAnd also our targeted-attack looks well for image m, if we define class 1 as our fooling class:\n\n$$ l = y_{1}^{1} * y_{2}^{0} * y_{3}^{0} = y_{1} $$\n\nWe have made sure that only those output-values enter the likelihood that are not related to the true target and we made sure that they carry their values and not just a value of 1. ","8ecb51cd":"Let's try to understand this: For the true label the summand is 0 whereas all other classes contribute to the gradient with their class weight vector $\\vec{w_{k}}$ reduced by an output \"weighted\" sum of all other class weights. What does that mean? ...","5fb85782":"Notice that in contrast to the likelihood function we now have 1-t instead of t. If our true target $t_{n,k}$ of class k for input $x_{n}$ is 1, but our model predicts $y_{n,k}=0$, then we have $0^{1-1}=1$ and vice versa $1^{1-0}=1$.  ","cf45fd79":"## Training the model <a class=\"anchor\" id=\"training\"><\/a>\n\nFirst of all we need a model for multiclass logistic regression:","fced8d82":"First of all, we need some fooling targets. For our example digit all others are possible:","3edf5d1b":"Before we start with building targeted and non-targeted attacks, let's have a look at the first digits of the test set:","0834cbdd":"Ok, so out of 16800 samples, the model failed to predict around 1600. That's why our intital accuracy score is close to 90 % (means 10 % failing). Now, which digit was selected as wrong prediction result most often?","f8fc67de":"The first part is easy:\n\n$$ \\frac {\\partial \\hat{D}}{\\partial y_{k}}  = \\frac {1-t_{k}} {y_{k}}$$","6535200c":"First of all we need to calculate the perturbations for each image in the test set. To do this we have to transform our true targets to one-hot-targets and call attack :-). As I want to see, how much epsilon we need to create a good breakdown, I use the attack_to_max_epsilon method. ","49baf33a":"### Attack methods ","0014b38b":"## Natural vs. non-natural targeted attack\n\nNow, I like to see what happens with the accuracy score if we fool the model for each image in the test set. \nBy analyzing non-targeted attacks we found that some digits are more used as \"fooling\" target than others and that each digit has its fooling digit counterpart. I assume that fooling takes place in regions where the model fails to draw good decision boundaries. Using targeted attack we should see that we can breakdown the accuracy score easier with natural fooling targets than with the other digits. Let's try this! :-)","09b877aa":"### Adding tiny perturbations","3554e479":"$$\n\\begin{bmatrix}\n\\frac{\\partial y_{1}}{\\partial x_{1}}\t& \\frac{\\partial y_{1}}{\\partial x_{2}}\t& \\dots\t & \\frac{\\partial y_{1}}{\\partial x_{64}}     \\\\\n\\frac{\\partial y_{2}}{\\partial x_{1}}\t& \\frac{\\partial y_{2}}{\\partial x_{2}} \t& \\dots  & \\frac{\\partial y_{2}}{\\partial x_{64}} \t  \\\\\n\\vdots\t& \\vdots \t& \\ddots & \\vdots \\\\\n\\frac{\\partial y_{10}}{\\partial x_{1}} \t& \\dots & \\dots\t & \\frac{\\partial y_{10}}{\\partial x_{64}}\n\\end{bmatrix}\n$$ ","164ad1a4":"** ... instead of defining a discrepance function one could have also just minimize the likelihood function to yield the lowest probability of matches... **","59b5ae30":"Ok, we see that 8 was selected most often as fooling target. But 9, 3, 5 and 2 have high counts as well in contrast to 0, 1, 6 and 7. If our assumption is true that the gradient drives us to targets where the model tends to fail in prediction we should see a similar pattern of counts for wrong predictions:","05ede699":"Ok, the label holds the true digit and the other columns all 784 pixel of an image with 28 times 28 pixels. Let's split our data intro train and test. This way we can measure our model performance on the test set and we can see how this score breaks down during the attack.","5c849504":"### Prepare natural and non-natural fooling targets\n\nThe gradient travel guide showed us the occurences of fooling target digits for each true digit. The highest count stands for the natural fooling target whereas the lowest corresponds to the non-natural fooling target. Given the heatmap we could create the targets by argmin and argmax per row (y_true) as follows:","f41d64ef":"To maximize the discrepance for playing around we can use gradient ascent. Though we have to find a sufficient rate $\\eta$. Given an input $x_{m}$ we will then add a perturbation with:","c88d0791":"### Generating the fooling\/attack classes and targets","25589139":"Finally I end up with:\n\n$$ \\nabla_{x} \\log D = \\sum_{k=1}^{K} (1-t_{k}) \\cdot  (\\vec{w}_{k} - \\sum_{c=1}^{K} y_{c} \\vec{w}_{c}) $$","6e7e6668":":-)","39a6d97e":"We found out that each digit has its natural fooling target. For example 9 was likely predicted as 5 or 7 which makes sense as all three have \"two bubbles\" on top of each other as a sketch shape.","f56df5c9":"$\\epsilon = 128$: is minumum value thatwe can chose for this image, with a lower epsilon value our model couldn't turn the 9(Ankle Boot) image into 0(T-shirt\/top) and 3(Dress). \n\nChange the epsilon value and see the results yourself :-) ","41878596":"Even though we can see high background noise the true label is not destroyed. I can still see the true label with my eyes whereas the model predicts the desired fooling target (0 to 9, except true label). :-) That's cool.","b2fd0d95":"We need the perturbed images as well as the fooling results of that epsilon:","079d2796":"Yeah! :-) We can still see the true target and not the fooling target. That's amazing. But we can also see, that the background has increased intensitiy. Let's visualize the difference between the original true label and the adversarial image for $\\epsilon = 16$: ","2ec40071":"Uiih!! :-o That's a surprise. ","4924b573":"What if we would use the likelihood instead of the discrepance function to yield the perturbations? If you look at the equations, you can find a clear similarity:","4192375c":"# Conclusion <a class=\"anchor\" id=\"conclusion\"><\/a>\n\nMy public journey has come to end. I have still ideas to play with but this kernel is already too long for an introduction. Feel free to fork, try different epsilons, only use the sign method, use dimensionality reductions as influence property.... there is so much to do :-) . For me it was the first time to fool a machine learning model and I learnt:\n\n1. One could easily fool a model if one has access to the learned weights and by defining own objectives that maximize the discrepance between targets and model outputs. (One could also fool without knowing weights or model architecture, but that's another topic...)\n2. Fooling takes place in regions where the model fails to draw good decision boundaries which of course depends on the model architecture\/flexibility but on the input data quality and preprocessing as well.\n3. Due to the fact that some inputs are closer to each other in meanings of decision boundaries, there exist natural and non-natural fooling targets.\n\n\nHave fun and good luck, so far :-)","75cf9fd5":"Ok, we will choose one of these successful examples and plot its related perturbed image over a range of epsilons: ","090d3ff0":"### One example image\n\nTo play around, let's select one input of $X_{test}$ and try to make targeted attacks for each class $f_{k}$ except for the true label target $t_{k}$. ","e1eaef87":"And we will pass it to our class and call prepare. This way we train our model on training data and we will obtain the initial accuracy score on test data. Later on we want to break down this score by perturbing the test data.","e38fbef3":"## Loading packages <a class=\"anchor\" id=\"load\"><\/a>\n\nFirst of all, let's load some packages...","1fb05348":"Whereas $J(w,x,y)$ stands for the cost function used to train the model.  In many cases this is $E=- \\log (L(t|y(x,w))$, the negative log likelihood function. One might think that this should also be the case for this example and that we could have also simply maximize the cost funtion. As this would be the same as to minimize the log-likelihood, one might think:","4454e591":"## Targeted-Attack <a class=\"anchor\" id=\"targeted\"><\/a>\n\nWe have seen that fooling the multiclass logistic regression model was easy with gradient ascent and the only cumbersome part was to calculate the gradient with respect to the inputs of our discrepance function. Instead of forcing the function to yield the maximum discrepance we could have also construct it such that outputs have to match a specific false target. To emphasize the difference to the true target $t_{n,k}$ let's call it $f_{n,k}$ for \"fooling\" target.\n\n$$ D(t|y(w,x)) = \\prod_{n=1}^{N} \\prod_{k=1}^{K} y_{n,k}^{f_{n,k}} $$\n\nLuckily we do not have to calculate everything again, the only thing that changes is:\n\n$$ \\frac {\\partial D}{\\partial y_{k}}  = \\frac {f_{k}} {y_{k}} $$\n\nConsequently we have:\n\n$$ \\nabla_{x} \\log D = \\sum_{k=1}^{K} f_{k} \\cdot  (\\vec{w}_{k} - \\sum_{c=1}^{K} y_{c} \\vec{w}_{c}) $$\n\n$$ x_{p, m} = x_{m} + \\eta \\sum_{k=1}^{K} f_{k} \\cdot  (\\vec{w}_{k} - \\sum_{c=1}^{K} y_{m,c} \\vec{w}_{c}) $$","52c8983d":"### Attack class\n\nI have written a small class that performs the attack of logistic regression:","0963fdcf":"$$ D(t|y(w,x)) = \\prod_{n=1}^{N} \\prod_{k=1}^{K} y_{n,k}^{f_{n,k}} $$\n\n$$ \\nabla_{x} \\log D = \\sum_{k=1}^{K} f_{k} \\cdot  (\\vec{w}_{k} - \\sum_{c=1}^{K} y_{c} \\vec{w}_{c}) $$\n\n$$ L(t|y(x,w)) = \\prod_{n=1}^{N} \\prod_{k=1}^{K} y_{n,k}^{t_{n,k}} $$\n\n$$ \\nabla_{x} \\log L = \\sum_{k=1}^{K} t_{k} \\cdot  (\\vec{w}_{k} - \\sum_{c=1}^{K} y_{c} \\vec{w}_{c}) $$","c685a547":"The difference is the use of either the true label $t_{n,k}$ or the fooling target $f_{n,k}$. What remains the same is the gradient of output $y_{k}$ of class $k$ with respect to all features $x$ (pixels) of one single image.\n\n$$ \\frac {\\partial y_{k}}{\\partial x} =  y_{k} \\vec{w}_{k} - y_{k} \\cdot \\sum_{c=1}^{K} y_{c} \\vec{w}_{c}  $$","8ad976f3":"## One step further: The Jacobian matrix <a class=\"anchor\" id=\"jacobian\"><\/a>","2891480e":"Ok, around 90 percent of test data was classified correctly and for us this is sufficient to play with. In multiclass logistic regression the probability that the model outputs $y_{n}$ of $N$ inputs $x_{n}$ matches their targets $t_{n}$ is given by:\n\n$$ \np(t|y(x,w)) = \\prod_{n=1}^{N} \\prod_{k=1}^{K} y_{n,k}^{t_{n,k}} \n$$\n\nWe assumed that inputs and class memberships are independent and identically distributed. The target $t_{n}$ of one input $x_{n}$ is a vector with K elements following one-hot-encoding (the true label class is 1, all others are 0). Maximizing the probability of matches above is also called the maximum likelihood approach. Each class in a multiclass logistic regression has its own weight vector and inputs are passed with weights through the softmax function to obtain the model output:\n\n$$ \ny_{n,k} = \\frac{\\exp(w_{k}^{T}x_{n})} {\\sum_{c=1}^{K}\\exp(w_{c}^{T}x_{n})}\n$$","ec34013c":"$$ x_{p, m} = x_{m} + \\delta x_{m} = x_{m} + \\eta \\cdot \\nabla_{x_{m,c}} D_{m,c} $$\n\n$$ x_{p, m} = x_{m} + \\eta \\sum_{k=1}^{K} (1-t_{m,k}) \\cdot  (\\vec{w}_{k} - \\sum_{c=1}^{K} y_{m,c} \\vec{w}_{c}) $$","349f75f2":"*This kernel flows.... work in progress! I currently work on better story telling & visualisations.* ;-)\n\n## Welcome Kaggler!\n\n...\n\n    Do you know what your machine model has learnt after training?\n\n...\n\n    Does your model know what a dog is if it makes perfect predictions for dog or cat classification?\n\n...\n\n    How vulnerable is a model with almost prefect prediction performance to hacker attacks?\n\n...\n\n    How can we attack and defense our algorithms?\n    \n...\n\nIt's some time ago, but there was a [competition on kaggle](https:\/\/www.kaggle.com\/c\/nips-2017-targeted-adversarial-attack) that addressed these questions and asked the community to build attacks and defenses for deep learning models. It was hosted by the google [brain group ](https:\/\/www.kaggle.com\/google-brain) and you can find more information as well as a github repository on their own website [cleverhans.io](http:\/\/www.cleverhans.io\/about\/). The time when this competition was active I heard the first time about fooling machine learning models and as I always like to understand why and how, I wrote this notebook that fools a simpler model, namely logistic regression, on the MNIST digit dataset.\n\n![Cleverhans](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/5\/57\/Osten_und_Hans.jpg)\n\nDo you know this guy and his horse? The man on this image was Wilhelm von Osten and his horse was named Kluger Hans (cleverhans). In years before the First World War this horse was famous as one belived that it was able to count and to do arithmetic computations. Which sounds like a miracle turned out to be a more nebulous skill of Kugler Hans: Instead of doing computations the horse analyzed the gestures and body postures of the audiance in the show. Dependent on their expressions Kluger Hans made its decisions which result to choose. \n\nEven though one is not sure about the true skills of Kluger Hans and on which factors the horse made its decisions, this is still an analogy to our loved machine learning models. Do you know what they really learn?\n\n\nWithin this kernel I invite you to open your eyes and get to know the illusion. If you like my kernel, you can make me very happy with an upvote! ;-) \n","13cc6e7c":"### The gradient travel guide - natural fooling targets\n\nI'm happy that it was possible to fool our model but it's still diffuse and unclear where the one-step-gradient guides us through (remember we do not iterate with gradient ascent, we just take one step and size is given by strength of gradient times eta). I assume that some numbers are closer to each other in weight space than to others. As the model training draws decision boundaries dependent on the quality of the input data and flexibility of model architecture, there will be regions where a 3 is not predicted as 3 but as 8. Those regions where the model makes an incorrect prediction. And I think, that there are preffered numbers to be wrong predictions given a digit input image. Perhaps the fooling gradients drives us to those \"natural\" fooling target numbers? ","029559bb":"* Maximizing probability of no-matches:\n\n$$ x_{p, m} = x_{m} + \\delta x_{m} = x_{m} + \\eta \\cdot \\nabla_{x_{m}} D(f_{m}|y_{m}(x_{m},w)) $$\n\n* Fast Gradient Method:\n\n$$ x_{p,m} = x_{m} + \\delta x_{m} = x_{m} + \\eta \\cdot sign (\\nabla_{x_{m}} J(w,x,y)) $$","58b0b45b":"Ahhh! :-) We can clearly see that it was easier to fool the model with natural fooling targets.","3aa6c298":"The images are 28x28 NumPy arrays, with pixel values ranging from 0 to 255. The labels are an array of integers, ranging from 0 to 9. These correspond to the class of clothing the image represents:\n\nLabel\tClass\n* 0       T-shirt\/top\n* 1\t    Trouser\n* 2\t    Pullover\n* 3\t    Dress\n* 4\t    Coat\n* 5\t    Sandal\n* 6\t    Shirt\n* 7\t    Sneaker\n* 8\t    Bag\n* 9\t    Ankle boot\n* Each image is mapped to a single label. Since the class names are not included with the dataset, store them here to use later when plotting the images:","dd4e845f":"Now this second part is a bit cumbersome (... hopefully I didn't make a mistake ...):\n\n$$ \\frac {\\partial y_{k}}{\\partial x} = \\frac{\\Sigma \\cdot \\partial_{x} \\exp(a_{k}) - \\exp(a_{k}) \\partial_{x} \\Sigma}{\\Sigma^{2}} = \\frac {\\sum_{c=1}^{K} \\exp(a_{c}) \\cdot \\vec{w}_{k} \\exp(a_{k}) - \\exp(a_{k}) \\sum_{c=1}^{K} \\vec{w}_{c} \\exp(a_{c})} {\\sum_{c=1}^{K}\\exp(a_{c}) \\cdot \\sum_{c=1}^{K}\\exp(a_{c})} $$\n\n$$ \\frac {\\partial y_{k}}{\\partial x} =  y_{k} \\vec{w}_{k} - y_{k} \\cdot \\sum_{c=1}^{K} y_{c} \\vec{w}_{c}  $$","bb1b9565":"Ohoh! Is this true? Sounds so nice, but... ","b035eb7b":"## Walkthrough contents\n\n* [Loading data and packages](#load)\n* [Attack methods and class](#methods)\n* [Training the model](#training)\n* [Non-Targeted-Attack](#nontargeted): \n    * Maximizing the output-target discrepance\n    * Natural fooling targets \n* [Targeted-Attack](#targeted): \n    * One image example\n    * Natural vs non-natural fooling targets & accuracy score breakdown for all images (Reworked)\n* [Comparison to Fast Gradient Method](#fastgradient): \n    * Thoughts: Maximizing discrepance vs. minimizing likelihood\n* [The Jacobian enters the door](#jacobian)\n* [Conclusion](#conclusion)","7baf7c24":" Now I want to see it in more detail: Which are the natural fooling targets (for successful foolings) for each digit?","160d1a2c":"Let's give it a try! :-)","8a919611":"To train a model we usually would maximize the likelihood with respect to the weight parameters whereas inputs are fixed. In our case we already have a trained model and fixed weights. But we can add tiny perturbations to our input images such that we maximize our discrepancy function. Let's do that and for making things simpler we use the log! :-)\n\n$$\\nabla_{x} \\hat{D} = \\nabla_{x} log D = \\nabla_{x} \\sum_{n=1}^{N} \\sum_{k=1}^{K} (1-t_{k}) \\log y_{k} $$\n\n$$\\hat{D} = \\sum_{n=1}^{N} \\sum_{k=1}^{K} (1-t_{n,k}) \\log y_{n,k}$$\n\n$$ \\partial_{x} \\hat{D} = \\sum_{n=1}^{N} \\sum_{k=1}^{K} \\frac {\\partial \\hat{D}}{\\partial y_{k}} \\frac {\\partial y_{k}}{\\partial x} $$","8fd8bdf4":"## Comparison to \"discrepance\" Gradient Sign Method","fe654c93":"And I will store results in a pandas dataframe such that we can easily find successful foolings:","4f19b3c4":"Some small experiment: Let's only take the sign of the gradient of our attack. I think we will not be as goog as we could be with the full gradient:","e923d9c9":"Yes, that's the same pattern as for the fooling targets. As this is caused by the difficulty of our model to draw good decision boundaries we should see this pattern as well for the true labels of those digits that were wrong predicted:","642b5d9b":"## Loading data \n\nNow we will use the digits of the digit-recognizer competition. Let's check: ","10f960c1":"Let's check if we have as much weight vectors as classes:","029c65a6":"Uii, the threshold is given by a max of 16 pixel that are allowed to be added as perturbation per pixel per image. Given this $\\epsilon$ we would end up with a model that still predicts around 40 % correctly. If we would use max $\\epsilon=30$ the model would fail with almoast 90 % digits in the test set :-) . Let's have a look at one example of successful fooling for a range of epsilons until max of $\\epsilon = 16$.","ac84f7c8":"Now, in each row we have one vector of $\\frac {\\partial y_{k}}{\\partial x}$. But that's not all! Have a closer look at the equations of $ \\nabla_{x} \\log L $ and $ \\nabla_{x} \\log D$. What are $t_{n,k}$ and $f_{n,k}$ doing here? ...\n\nThey are working like a mask. :-)\n\n...\n\nNot every class contributes to the gradient! As we use one-hot-encoding, only the class with the $1$ yields a summand which is not 0. Looking at the Jacobian this means that all rows are 0 except for the class that is represented by 1 in the encoding of $t_{n,k}$ or $f_{n,k}$. Let's call this specific class $\\hat{k}$, then we have:\n\n$$ \\nabla_{x} \\log D =  \\vec{w}_{\\hat{k}} - \\sum_{c=1}^{K} y_{c} \\vec{w}_{c} $$\n\n$$ \\nabla_{x} \\log L =  \\vec{w}_{\\hat{k}} - \\sum_{c=1}^{K} y_{c} \\vec{w}_{c} $$\n\nBut take in mind: $\\hat{k}$ is different for the both equations: one is the fooling class and the other the true label class and of course they are not the same! :-)\n\n### Why could this be important?\n\n","19c10966":"### Attacking the model\n\nI will force the attack to success by allowing an epsilon high enough to yield all targets. This way we can still find out, if we can see the true label or the fooling target."}}