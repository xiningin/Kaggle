{"cell_type":{"8adf3253":"code","c436a3d1":"code","013c6a89":"code","07fc72ec":"code","5c7b07cb":"code","86d19d4b":"code","0856f0cb":"markdown","47876933":"markdown","74bb12d8":"markdown","69317224":"markdown"},"source":{"8adf3253":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport torch\nimport torch.nn as nn\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c436a3d1":"class GaussianDropout(nn.Module):\n\n    def __init__(self, p: float):\n        \"\"\"\n        Multiplicative Gaussian Noise dropout with N(1, p\/(1-p))\n        It is NOT (1-p)\/p like in the paper, because here the\n        noise actually increases with p. (It can create the same\n        noise as the paper, but with reversed p values)\n\n        Source:\n        Dropout: A Simple Way to Prevent Neural Networks from Overfitting\n        https:\/\/www.cs.toronto.edu\/~rsalakhu\/papers\/srivastava14a.pdf\n\n        :param p: float - determines the the standard deviation of the\n        gaussian noise, where sigma = p\/(1-p).\n        \"\"\"\n        super().__init__()\n        assert 0 <= p < 1\n        self.t_mean = torch.ones((0,))\n        self.shape = ()\n        self.p = p\n        self.t_std = self.compute_std()\n\n    def compute_std(self):\n        return self.p \/ (1 - self.p)\n\n    def forward(self, t_hidden):\n        if self.training and self.p > 0.:\n            if self.t_mean.shape != t_hidden.shape:\n                self.t_mean = torch.ones_like(input=t_hidden\n                                              , dtype=t_hidden.dtype\n                                              , device=t_hidden.device)\n            elif self.t_mean.device != t_hidden.device:\n                self.t_mean = self.t_mean.to(device=t_hidden.device, dtype=t_hidden.dtype)\n\n            t_gaussian_noise = torch.normal(self.t_mean, self.t_std)\n            t_hidden = t_hidden.mul(t_gaussian_noise)\n        return t_hidden","013c6a89":"class TestNetwork(nn.Module):\n    \n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(8, 16)\n        self.dropout = GaussianDropout(p=0.2)\n        self.linear2 = nn.Linear(16, 8)\n        \n    def forward(self, t_input):\n        t_hidden = self.linear(t_input)\n        t_hidden = self.dropout(t_hidden)\n        t_hidden = self.linear2(t_hidden)\n        return t_hidden","07fc72ec":"network = TestNetwork()","5c7b07cb":"t_input = torch.randn((64, 80, 8))\nt_output = network(t_input)","86d19d4b":"t_output.shape, t_output","0856f0cb":"Gaussian dropout multiplies a normal distributed noise with **all** your hidden embeddings dimensions instead of setting some dimensions to 0. So hidden_embedding*normal_noise.\nThis means, that the mean of your outputs will stay the same, although the standard deviation and variance changes. It might make train\/inference performance more stable for regression task than normal dropout.\nThe noise used here, is similar to the [paper proposing it](https:\/\/www.cs.toronto.edu\/~rsalakhu\/papers\/srivastava14a.pdf) N(1, (1\/(1-p)). We use (1\/(1-p)) here instead the reverse, because the noise \nwill grow with p, instead the other way around.\nSee also [discussion here](https:\/\/www.kaggle.com\/c\/ventilator-pressure-prediction\/discussion\/277155).\nThanks to [Allohvk](https:\/\/www.kaggle.com\/allohvk) for suggesting it in the first place.","47876933":"# Gaussian Dropout for Pytorch","74bb12d8":"# Test-Network","69317224":"**Maybe it helps you to improve your score :)**"}}