{"cell_type":{"fcdcf87b":"code","cc969fe1":"code","b01e2a09":"code","1924651d":"code","260837de":"code","89c02457":"code","61f3b391":"code","3d3e2dff":"code","f454017f":"code","e659cdc1":"code","de5716ff":"code","53030e92":"code","77972fd6":"code","db608d1d":"code","ec58d570":"code","e0bfb70a":"code","7acdfa4d":"code","cdf95a95":"code","1ccc1f9e":"code","f340aa41":"code","8e7ced7c":"code","2f2a3093":"code","27b528f0":"code","650731be":"code","2f9729cc":"code","4b5e7d4e":"code","8f78810f":"code","0f9efc00":"code","6c5ff7ee":"code","d9bb39bc":"code","650b015c":"code","18ec5932":"code","f67a3ba9":"code","5e435a95":"code","d8cf3e5b":"code","0e8bedff":"code","a788b425":"code","f458b7f6":"code","f2103670":"code","a104d184":"code","35713473":"code","ec391d85":"code","b2998134":"code","97b93f35":"code","bfafbb07":"code","bbfcd946":"code","e81118fd":"code","8b67e6ad":"code","e848a565":"code","fc61f432":"code","c26e886c":"code","da6eea36":"code","1ea58f96":"code","61e3c57a":"code","ef72b204":"code","615ef2de":"code","ef5c01b3":"code","f386435e":"code","9a686c21":"code","fbff9f63":"code","f400487e":"code","41ddd976":"code","b7b5781e":"code","49d517ea":"code","37b9d9dc":"code","23d17c40":"code","3364443e":"code","5144ae78":"code","b1231d01":"code","50489a2d":"code","6cce88cc":"code","b2a9a397":"code","0bc12a9c":"code","2b07f01a":"code","1f32c9b5":"code","fd2ed907":"code","6355f8da":"code","3f0ad787":"code","f0ad3a04":"code","60c320a9":"code","db00ff15":"code","2a043d72":"markdown","77542cd0":"markdown","d46e9217":"markdown","5906f583":"markdown","eca25863":"markdown","28b1591b":"markdown","22e3a949":"markdown","d865a751":"markdown","dedec53c":"markdown","c5a2ba4f":"markdown","36adf83f":"markdown","74f9fc37":"markdown","3f0929b8":"markdown","7d0b8677":"markdown","13e04c7f":"markdown","258dd9d3":"markdown","e3504c8e":"markdown","507865c7":"markdown","fc56de45":"markdown","a680f4b4":"markdown","88bc3524":"markdown","b2ae54c8":"markdown","e2fe0a34":"markdown","40929170":"markdown","64040122":"markdown","1d17c728":"markdown","a5dc079f":"markdown","7dc7dd55":"markdown","242e36f4":"markdown","0497568b":"markdown","651180c4":"markdown","684bf78a":"markdown","b0028e57":"markdown","af1a160c":"markdown","5a0e4f48":"markdown","5b336f94":"markdown","c624ec9e":"markdown","60500170":"markdown","23f7900c":"markdown","c7f7e8ac":"markdown","b3dedbb6":"markdown","130b5ce7":"markdown","49355796":"markdown","bb590f50":"markdown","9ce6796c":"markdown","94b948d1":"markdown","7ed316c6":"markdown","58c6571b":"markdown","c68f16be":"markdown"},"source":{"fcdcf87b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n%matplotlib inline\nsns.set()\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cc969fe1":"nr_cv = 5\n\n# switch for using log values for SalePrice and features     \nuse_logvals = 1    \n# target used for correlation \ntarget = 'SalePrice_Log'\n    \n# will use only columns with correlation above this threshold value  \nmin_val_corr = 0.4    \n    \n# switch for dropping columns that are similar to others already used and show a high correlation to these     \ndrop_similar = 1","b01e2a09":"def get_best_score(grid):\n    \n    best_score = np.sqrt(-grid.best_score_)\n    print(best_score)    \n    print(grid.best_params_)\n    print(grid.best_estimator_)\n    \n    return best_score\n\n\ndef print_cols_large_corr(df, nr_c, targ) :            #nr_c=number of variables for heatmap\n    corr = df.corr()\n    corr_abs = corr.abs()                              #abs is for taking absolute i.e only magnitude\n    print (corr_abs.nlargest(nr_c, targ)[targ])\n    \n    \ndef plot_corr_matrix(df, nr_c, targ) :\n    \n    corr = df.corr()\n    corr_abs = corr.abs()\n    cols = corr_abs.nlargest(nr_c, targ)[targ].index\n    cm = np.corrcoef(df[cols].values.T)\n\n    plt.figure(figsize=(nr_c\/1.5, nr_c\/1.5))\n    sns.set(font_scale=1.25)\n    sns.heatmap(cm, linewidths=1.5, annot=True, square=True, \n                fmt='.2f', annot_kws={'size': 10}, \n                yticklabels=cols.values, xticklabels=cols.values\n               )\n    plt.show()    ","1924651d":"df_train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")","260837de":"print(df_train.shape)\nprint(\"*\"*50)\nprint(df_test.shape)","89c02457":"print(df_train.info())\nprint(\"*\"*50)\nprint(df_test.info())  ","61f3b391":"df_train.head()","3d3e2dff":"df_train.describe()","f454017f":"df_test.head()","e659cdc1":"df_test.describe()","de5716ff":"sns.distplot(df_train['SalePrice']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","53030e92":"df_train['SalePrice_Log'] = np.log(df_train['SalePrice'])\n\nsns.distplot(df_train['SalePrice_Log']);\n# skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice_Log'].kurt())\n# dropping old column\ndf_train.drop('SalePrice', axis= 1, inplace=True)","77972fd6":"numerical_feats = df_train.dtypes[df_train.dtypes != \"object\"].index\nprint(\"Number of Numerical features: \", len(numerical_feats))\n\ncategorical_feats = df_train.dtypes[df_train.dtypes == \"object\"].index\nprint(\"Number of Categorical features: \", len(categorical_feats))","db608d1d":"print(df_train[numerical_feats].columns)\nprint(\"*\"*100)\nprint(df_train[categorical_feats].columns)","ec58d570":"df_train[numerical_feats].head()","e0bfb70a":"df_train[categorical_feats].head()","7acdfa4d":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","cdf95a95":"# columns where NaN values have meaning e.g. no pool etc.\ncols_fillna = ['PoolQC','MiscFeature','Alley','Fence','MasVnrType','FireplaceQu',\n               'GarageQual','GarageCond','GarageFinish','GarageType', 'Electrical',\n               'KitchenQual', 'SaleType', 'Functional', 'Exterior2nd', 'Exterior1st',\n               'BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2',\n               'MSZoning', 'Utilities']\n\n# replace 'NaN' with 'None' in these columns\nfor col in cols_fillna:\n    df_train[col].fillna('None',inplace=True)\n    df_test[col].fillna('None',inplace=True)","1ccc1f9e":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(6)","f340aa41":"# fillna with mean for the remaining columns: LotFrontage, GarageYrBlt, MasVnrArea\ndf_train.fillna(df_train.mean(), inplace=True)\ndf_test.fillna(df_test.mean(), inplace=True)","8e7ced7c":"total = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(5)","2f2a3093":"df_train.isnull().sum().sum()","27b528f0":"df_test.isnull().sum().sum()","650731be":"for col in numerical_feats:\n    print('{:15}'.format(col), \n          'Skewness: {:05.2f}'.format(df_train[col].skew()) , \n          '   ' ,\n          'Kurtosis: {:06.2f}'.format(df_train[col].kurt())  \n         )","2f9729cc":"sns.distplot(df_train['GrLivArea']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['GrLivArea'].skew())\nprint(\"Kurtosis: %f\" % df_train['GrLivArea'].kurt())","4b5e7d4e":"sns.distplot(df_train['LotArea']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['LotArea'].skew())\nprint(\"Kurtosis: %f\" % df_train['LotArea'].kurt())","8f78810f":"for df in [df_train, df_test]:\n    df['GrLivArea_Log'] = np.log(df['GrLivArea'])\n    df.drop('GrLivArea', inplace= True, axis = 1)\n    df['LotArea_Log'] = np.log(df['LotArea'])\n    df.drop('LotArea', inplace= True, axis = 1)\n\n    \n    \n    \nnumerical_feats = df_train.dtypes[df_train.dtypes != \"object\"].index","0f9efc00":"sns.distplot(df_train['GrLivArea_Log']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['GrLivArea_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['GrLivArea_Log'].kurt())","6c5ff7ee":"sns.distplot(df_train['LotArea_Log']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['LotArea_Log'].skew())\nprint(\"Kurtosis: %f\" % df_train['LotArea_Log'].kurt())","d9bb39bc":"nr_rows = 12\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nli_num_feats = list(numerical_feats)\nli_not_plot = ['Id', 'SalePrice', 'SalePrice_Log']\nli_plot_num_feats = [c for c in list(numerical_feats) if c not in li_not_plot]\n\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_plot_num_feats):\n            sns.regplot(df_train[li_plot_num_feats[i]], df_train[target], ax = axs[r][c])\n            stp = stats.pearsonr(df_train[li_plot_num_feats[i]], df_train[target])\n            \n            str_title = \"r = \" + \"{0:.2f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.2f}\".format(stp[1])\n            axs[r][c].set_title(str_title,fontsize=11)\n            \nplt.tight_layout()    \nplt.show()  ","650b015c":"df_train = df_train.drop(\n    df_train[(df_train['OverallQual']==10) & (df_train['SalePrice_Log']<12.3)].index)","18ec5932":"df_train = df_train.drop(\n    df_train[(df_train['GrLivArea_Log']>8.3) & (df_train['SalePrice_Log']<12.5)].index)","f67a3ba9":"corr = df_train.corr()\ncorr_abs = corr.abs()\n\nnr_num_cols = len(numerical_feats)\nser_corr = corr_abs.nlargest(nr_num_cols, target)[target]\n\ncols_abv_corr_limit = list(ser_corr[ser_corr.values > min_val_corr].index)\ncols_bel_corr_limit = list(ser_corr[ser_corr.values <= min_val_corr].index)","5e435a95":"print(ser_corr)\nprint(\"*\"*30)\nprint(\"List of numerical features with r above min_val_corr :\")\nprint(cols_abv_corr_limit)\nprint(\"*\"*30)\nprint(\"List of numerical features with r below min_val_corr :\")\nprint(cols_bel_corr_limit)","d8cf3e5b":"for catg in list(categorical_feats) :\n    print(df_train[catg].value_counts())\n    print('#'*50)","0e8bedff":"li_cat_feats = list(categorical_feats)\nnr_rows = 15\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_cat_feats):\n            sns.boxplot(x=li_cat_feats[i], y=target, data=df_train, ax = axs[r][c])\n    \nplt.tight_layout()    \nplt.show()  ","a788b425":"catg_strong_corr = [ 'MSZoning', 'Neighborhood', 'Condition2', 'MasVnrType', 'ExterQual', \n                     'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType']\n\ncatg_weak_corr = ['Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', \n                  'LandSlope', 'Condition1',  'BldgType', 'HouseStyle', 'RoofStyle', \n                  'RoofMatl', 'Exterior1st', 'Exterior2nd', 'ExterCond', 'Foundation', \n                  'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', \n                  'HeatingQC', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', \n                  'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', \n                  'SaleCondition' ]","f458b7f6":"nr_feats = len(cols_abv_corr_limit)","f2103670":"plot_corr_matrix(df_train, nr_feats, target)","a104d184":"id_test = df_test['Id']\n\nto_drop_num  = cols_bel_corr_limit\nto_drop_catg = catg_weak_corr\n\ncols_to_drop = ['Id'] + to_drop_num + to_drop_catg \n\nfor df in [df_train, df_test]:\n    df.drop(cols_to_drop, inplace= True, axis = 1)","35713473":"catg_list = catg_strong_corr.copy()\ncatg_list.remove('Neighborhood')\n\nfor catg in catg_list :\n    #sns.catplot(x=catg, y=target, data=df_train, kind='boxen')\n    sns.violinplot(x=catg, y=target, data=df_train)\n    plt.show()\n    #sns.boxenplot(x=catg, y=target, data=df_train)\n    #bp = df_train.boxplot(column=[target], by=catg)","ec391d85":"fig, ax = plt.subplots()\nfig.set_size_inches(16, 5)\nsns.violinplot(x='Neighborhood', y=target, data=df_train, ax=ax)\nplt.xticks(rotation=45)\nplt.show()","b2998134":"for catg in catg_list :\n    g = df_train.groupby(catg)[target].mean()\n    print(g)","97b93f35":"# 'MSZoning'\nmsz_catg2 = ['RM', 'RH']\nmsz_catg3 = ['RL', 'FV'] \n\n\n# Neighborhood\nnbhd_catg2 = ['Blmngtn', 'ClearCr', 'CollgCr', 'Crawfor', 'Gilbert', 'NWAmes', 'Somerst', 'Timber', 'Veenker']\nnbhd_catg3 = ['NoRidge', 'NridgHt', 'StoneBr']\n\n# Condition2\ncond2_catg2 = ['Norm', 'RRAe']\ncond2_catg3 = ['PosA', 'PosN'] \n\n# SaleType\nSlTy_catg1 = ['Oth']\nSlTy_catg3 = ['CWD']\nSlTy_catg4 = ['New', 'Con']\n\n\n#[]","bfafbb07":"for df in [df_train, df_test]:\n    \n    df['MSZ_num'] = 1  \n    df.loc[(df['MSZoning'].isin(msz_catg2) ), 'MSZ_num'] = 2    \n    df.loc[(df['MSZoning'].isin(msz_catg3) ), 'MSZ_num'] = 3        \n    \n    df['NbHd_num'] = 1       \n    df.loc[(df['Neighborhood'].isin(nbhd_catg2) ), 'NbHd_num'] = 2    \n    df.loc[(df['Neighborhood'].isin(nbhd_catg3) ), 'NbHd_num'] = 3    \n\n    df['Cond2_num'] = 1       \n    df.loc[(df['Condition2'].isin(cond2_catg2) ), 'Cond2_num'] = 2    \n    df.loc[(df['Condition2'].isin(cond2_catg3) ), 'Cond2_num'] = 3    \n    \n    df['Mas_num'] = 1       \n    df.loc[(df['MasVnrType'] == 'Stone' ), 'Mas_num'] = 2 \n    \n    df['ExtQ_num'] = 1       \n    df.loc[(df['ExterQual'] == 'TA' ), 'ExtQ_num'] = 2     \n    df.loc[(df['ExterQual'] == 'Gd' ), 'ExtQ_num'] = 3     \n    df.loc[(df['ExterQual'] == 'Ex' ), 'ExtQ_num'] = 4     \n   \n    df['BsQ_num'] = 1          \n    df.loc[(df['BsmtQual'] == 'Gd' ), 'BsQ_num'] = 2     \n    df.loc[(df['BsmtQual'] == 'Ex' ), 'BsQ_num'] = 3     \n \n    df['CA_num'] = 0          \n    df.loc[(df['CentralAir'] == 'Y' ), 'CA_num'] = 1    \n\n    df['Elc_num'] = 1       \n    df.loc[(df['Electrical'] == 'SBrkr' ), 'Elc_num'] = 2 \n\n\n    df['KiQ_num'] = 1       \n    df.loc[(df['KitchenQual'] == 'TA' ), 'KiQ_num'] = 2     \n    df.loc[(df['KitchenQual'] == 'Gd' ), 'KiQ_num'] = 3     \n    df.loc[(df['KitchenQual'] == 'Ex' ), 'KiQ_num'] = 4      \n    \n    df['SlTy_num'] = 2       \n    df.loc[(df['SaleType'].isin(SlTy_catg1) ), 'SlTy_num'] = 1  \n    df.loc[(df['SaleType'].isin(SlTy_catg3) ), 'SlTy_num'] = 3  \n    df.loc[(df['SaleType'].isin(SlTy_catg4) ), 'SlTy_num'] = 4  \n  ","bbfcd946":"new_col_num = ['MSZ_num', 'NbHd_num', 'Cond2_num', 'Mas_num', 'ExtQ_num', 'BsQ_num', 'CA_num', 'Elc_num', 'KiQ_num', 'SlTy_num']\n\nnr_rows = 4\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(new_col_num):\n            sns.regplot(df_train[new_col_num[i]], df_train[target], ax = axs[r][c])\n            stp = stats.pearsonr(df_train[new_col_num[i]], df_train[target])\n            str_title = \"r = \" + \"{0:.2f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.2f}\".format(stp[1])\n            axs[r][c].set_title(str_title,fontsize=11)\n            \nplt.tight_layout()    \nplt.show()","e81118fd":"catg_cols_to_drop = ['Neighborhood' , 'Condition2', 'MasVnrType', 'ExterQual', 'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType']\n\ncorr1 = df_train.corr()\ncorr_abs_1 = corr1.abs()\n\nnr_all_cols = len(df_train)\nser_corr_1 = corr_abs_1.nlargest(nr_all_cols, target)[target]\n\nprint(ser_corr_1)\ncols_bel_corr_limit_1 = list(ser_corr_1[ser_corr_1.values <= min_val_corr].index)\n\n\nfor df in [df_train, df_test] :\n    df.drop(catg_cols_to_drop, inplace= True, axis = 1)\n    df.drop(cols_bel_corr_limit_1, inplace= True, axis = 1) ","8b67e6ad":"corr2 = df_train.corr()\ncorr_abs_2 = corr2.abs()\n\nnr_all_cols = len(df_train)\nser_corr_2 = corr_abs_2.nlargest(nr_all_cols, target)[target]\n\nprint(ser_corr_2)","e848a565":"df_train.head()","fc61f432":"df_test.head()","c26e886c":"corr = df_train.corr()\ncorr_abs = corr.abs()\n\nnr_all_cols = len(df_train)\nprint (corr_abs.nlargest(nr_all_cols, target)[target])","da6eea36":"nr_feats=len(df_train.columns)\nplot_corr_matrix(df_train, nr_feats, target)","1ea58f96":"cols = corr_abs.nlargest(nr_all_cols, target)[target].index\ncols = list(cols)\n\nif drop_similar == 1 :\n    for col in ['GarageArea','1stFlrSF','TotRmsAbvGrd','GarageYrBlt'] :\n        if col in cols: \n            cols.remove(col)","61e3c57a":"cols = list(cols)\nprint(cols)","ef72b204":"feats = cols.copy()\nfeats.remove('SalePrice_Log')\n\nprint(feats)","615ef2de":"df_train_ml = df_train[feats].copy()\ndf_test_ml  = df_test[feats].copy()\n\ny = df_train[target]","ef5c01b3":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\ndf_train_ml_sc = sc.fit_transform(df_train_ml)\ndf_test_ml_sc = sc.transform(df_test_ml)","f386435e":"df_train_ml_sc = pd.DataFrame(df_train_ml_sc)\ndf_train_ml_sc.head()","9a686c21":"X = df_train_ml.copy()\ny = df_train[target]\nX_test = df_test_ml.copy()\n\nX_sc = df_train_ml_sc.copy()\ny_sc = df_train[target]\nX_test_sc = df_test_ml_sc.copy()\n\nX.info()\nX_test.info()\n","fbff9f63":"X.head()","f400487e":"X_sc.head()","41ddd976":"X_test.head()","b7b5781e":"from sklearn.model_selection import GridSearchCV\nscore_calc = 'neg_mean_squared_error'","49d517ea":"from sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nparameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\ngrid_linear = GridSearchCV(linreg, parameters, cv=nr_cv, verbose=1 , scoring = score_calc)\ngrid_linear.fit(X, y)\n\nsc_linear = get_best_score(grid_linear)","37b9d9dc":"linreg_sc = LinearRegression()\nparameters = {'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True, False]}\ngrid_linear_sc = GridSearchCV(linreg_sc, parameters, cv=nr_cv, verbose=1 , scoring = score_calc)\ngrid_linear_sc.fit(X_sc, y)\n\nsc_linear_sc = get_best_score(grid_linear_sc)","23d17c40":"linregr_all = LinearRegression()\n#linregr_all.fit(X_train_all, y_train_all)\nlinregr_all.fit(X, y)\npred_linreg_all = linregr_all.predict(X_test)\npred_linreg_all[pred_linreg_all < 0] = pred_linreg_all.mean()","3364443e":"sub_linreg = pd.DataFrame()\nsub_linreg['Id'] = id_test\nsub_linreg['SalePrice'] = pred_linreg_all\n#sub_linreg.to_csv('linreg.csv',index=False)","5144ae78":"from sklearn.linear_model import Ridge\n\nridge = Ridge()\nparameters = {'alpha':[0.001,0.005,0.01,0.1,0.5,1], 'normalize':[True,False], 'tol':[1e-06,5e-06,1e-05,5e-05]}\ngrid_ridge = GridSearchCV(ridge, parameters, cv=nr_cv, verbose=1, scoring = score_calc)\ngrid_ridge.fit(X, y)\n\nsc_ridge = get_best_score(grid_ridge)","b1231d01":"ridge_sc = Ridge()\nparameters = {'alpha':[0.001,0.005,0.01,0.1,0.5,1], 'normalize':[True,False], 'tol':[1e-06,5e-06,1e-05,5e-05]}\ngrid_ridge_sc = GridSearchCV(ridge_sc, parameters, cv=nr_cv, verbose=1, scoring = score_calc)\ngrid_ridge_sc.fit(X_sc, y)\n\nsc_ridge_sc = get_best_score(grid_ridge_sc)","50489a2d":"pred_ridge_all = grid_ridge.predict(X_test)","6cce88cc":"from sklearn.linear_model import Lasso\n\nlasso = Lasso()\nparameters = {'alpha':[1e-03,0.01,0.1,0.5,0.8,1], 'normalize':[True,False], 'tol':[1e-06,1e-05,5e-05,1e-04,5e-04,1e-03]}\ngrid_lasso = GridSearchCV(lasso, parameters, cv=nr_cv, verbose=1, scoring = score_calc)\ngrid_lasso.fit(X, y)\n\nsc_lasso = get_best_score(grid_lasso)\n\npred_lasso = grid_lasso.predict(X_test)","b2a9a397":"from sklearn.tree import DecisionTreeRegressor\n\nparam_grid = { 'max_depth' : [7,8,9,10] , 'max_features' : [11,12,13,14] ,\n               'max_leaf_nodes' : [None, 12,15,18,20] ,'min_samples_split' : [20,25,30],\n                'presort': [False,True] , 'random_state': [5] }\n            \ngrid_dtree = GridSearchCV(DecisionTreeRegressor(), param_grid, cv=nr_cv, refit=True, verbose=1, scoring = score_calc)\ngrid_dtree.fit(X, y)\n\nsc_dtree = get_best_score(grid_dtree)\n\npred_dtree = grid_dtree.predict(X_test)","0bc12a9c":"dtree_pred = grid_dtree.predict(X_test)\nsub_dtree = pd.DataFrame()\nsub_dtree['Id'] = id_test\nsub_dtree['SalePrice'] = dtree_pred\n#sub_dtree.to_csv('dtreeregr.csv',index=False)","2b07f01a":"from sklearn.ensemble import RandomForestRegressor\n\nparam_grid = {'min_samples_split' : [3,4,6,10], 'n_estimators' : [70,100], 'random_state': [5] }\ngrid_rf = GridSearchCV(RandomForestRegressor(), param_grid, cv=nr_cv, refit=True, verbose=1, scoring = score_calc)\ngrid_rf.fit(X, y)\n\nsc_rf = get_best_score(grid_rf)","1f32c9b5":"pred_rf = grid_rf.predict(X_test)\n\nsub_rf = pd.DataFrame()\nsub_rf['Id'] = id_test\nsub_rf['SalePrice'] = pred_rf \n\nif use_logvals == 1:\n    sub_rf['SalePrice'] = np.exp(sub_rf['SalePrice']) \n\n#sub_rf.to_csv('rf.csv',index=False)","fd2ed907":"#sub_rf.head(10)","6355f8da":"list_scores = [sc_linear, sc_ridge, sc_lasso, sc_dtree, sc_rf]\nlist_regressors = ['Linear','Ridge','Lasso','DTr','RF']","3f0ad787":"fig, ax = plt.subplots()\nfig.set_size_inches(10,7)\nsns.barplot(x=list_regressors, y=list_scores, ax=ax)\nplt.ylabel('RMSE')\nplt.show()","f0ad3a04":"predictions = {'Linear': pred_linreg_all, 'Ridge': pred_ridge_all, 'Lasso': pred_lasso, 'DTr': pred_dtree, 'RF': pred_rf}\ndf_predictions = pd.DataFrame(data=predictions) \ndf_predictions.corr()","60c320a9":"plt.figure(figsize=(7, 7))\nsns.set(font_scale=1.25)\nsns.heatmap(df_predictions.corr(), linewidths=1.5, annot=True, square=True, \n                fmt='.2f', annot_kws={'size': 10}, \n                yticklabels=df_predictions.columns , xticklabels=df_predictions.columns\n            )\nplt.show()","db00ff15":"sub_mean = pd.DataFrame()\nsub_mean['Id'] = id_test\nsub_mean['SalePrice'] = np.round( (pred_lasso + pred_linreg_all  + pred_ridge_all + pred_rf) \/ 4.0 ) \nsub_mean['SalePrice'] = sub_mean['SalePrice'].astype(float)\nsub_mean.to_csv('mean.csv',index=False)","2a043d72":"**DecisionTreeRegressor**","77542cd0":"**Convert categorical columns to numerical**\n\nFor those categorcial features where the EDA with boxplots seem to show a strong dependence of the SalePrice on the category, we transform the columns to numerical. To investigate the relation of the categories to SalePrice in more detail, we make violinplots for these features Also, we look at the mean of SalePrice as function of category.","d46e9217":"There are few columns with quite large correlation to SalePrice (NbHd_num, ExtQ_num, BsQ_num, KiQ_num).\n\nThese will probably be useful for our model performance.","5906f583":"**The target variable : Distribution of SalePrice**","eca25863":"**Conclusion from EDA on numerical columns:**\n\nWe see that for some features like 'OverallQual' there is a strong linear correlation to the target.\nFor other features like 'MSSubClass' the correlation is very weak.\n\nWe have taken the default threshold for min_val_corr = 0.4,therefore the features to be dropped are:\n'Id', 'MSSubClass', 'LotArea', 'OverallCond', 'BsmtFinSF2', 'BsmtUnfSF', 'LowQualFinSF', 'BsmtFullBath', 'BsmtHalfBath', 'HalfBath',\n'BedroomAbvGr', 'KitchenAbvGr', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold'","28b1591b":"**mean of best models**","22e3a949":"**creating functions to use**","d865a751":"**Linear Regression**","dedec53c":"shape, info, head and describe","c5a2ba4f":"df train has 81 columns (79 features + id and target SalePrice) and 1460 entries (number of rows or house sales)\ndf test has 80 columns (79 features + id) and 1459 entries","36adf83f":"Of those features with the largest correlation to SalePrice, some also are correlated strongly to each other.\n\nTo avoid failures of the ML regression models due to multicollinearity, these are dropped later.\n\nThis is  controlled by the switch drop_similar (global settings)","74f9fc37":"**Numerical and Categorical features**","3f0929b8":"**Relation of features to target (SalePrice_log)**\n\n\n\nPlots of relation to target for all numerical features","7d0b8677":"**List of features used for the Regressors in Step 3 **","13e04c7f":"As we see, the target variable SalePrice is not normally distributed.\nThis can reduce the performance of the ML regression models because some assume normal distribution.\nTherfore we make a log transformation, the resulting distribution looks much better.","258dd9d3":"**Model tuning and selection with GridSearchCV**","e3504c8e":"**List of numerical features and their correlation coefficient to target**","507865c7":"**Find columns with strong correlation to target**\n\nOnly those with r > min_val_corr will be used in the ML Regressors later\n\n\nThe value for min_val_corr is  chosen in global settings","fc56de45":"**Ridge**","a680f4b4":"**Lasso**","88bc3524":"**Conclusion from EDA on categorical columns:**\n\n\nFor many of the categorical there is no strong relation to the target.\nHowever, for some fetaures it is easy to find a strong relation.\nFrom the figures above these are : 'MSZoning', 'Neighborhood', 'Condition2', 'MasVnrType', 'ExterQual', 'BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType' \n\nAlso for the categorical features, we will use only those that show a strong relation to SalePrice. So the other columns will be dropped later --\n'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Condition1', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'ExterCond', 'Foundation', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleCondition'","b2ae54c8":"*Correlation matrix 1*\n\n**Features with largest correlation to SalePrice_Log**\n\nall numerical features with correlation coefficient above threshold","e2fe0a34":"**StandardScaler**","40929170":"**List of all features with strong correlation to SalePrice_Log**\n\nafter dropping all coumns with weak correlation","64040122":"For the first three models, the predictions show a very high correlation to each other (very close to 1.00).\nOnly for Random Forest and Decision Tree, the results are less correlated with the other Regressors.","1d17c728":"**Filling missing values**\n\n\nFor a few columns there is lots of NaN entries.\nHowever, reading the data description we find this is not missing data:\nFor PoolQC, NaN is not missing data but means no pool, likewise for Fence, FireplaceQu etc.","a5dc079f":"*Correlation Matrix 2 : All features with strong correlation to SalePrice*","7dc7dd55":"**Checking correlation to SalePrice for the new numerical columns**","242e36f4":"**Dropping the converted categorical columns and the new numerical columns with weak correlation**\n\n\n**columns and correlation before dropping**","0497568b":"**columns and correlation after dropping**","651180c4":"**List of categorical features and their unique values**","684bf78a":"**Correlation of model results**","b0028e57":"**new dataframes**","af1a160c":"**Relation to SalePrice for all categorical features**","5a0e4f48":"**Missing values in train data ?**","5b336f94":"*Dropping all columns with weak correlation to SalePrice*","c624ec9e":"1-Drop all columns with only small correlation to SalePrice\n\n2-Transform Categorical to numerical\n\n3-Handling columns with missing data\n\n4-Log values\n\n5-Drop all columns with strong correlation to similar features","60500170":"**log transform**\n\n\nLike the target variable, also some of the feature values are not normally distributed and it is therefore better to use log values in df_train and df_test. Checking for skewness and kurtosis:","23f7900c":"**taking DATA into dataframes**","c7f7e8ac":"**EDA**","b3dedbb6":"The performance of all applied Regressors is very similar, except for Decision Tree which has larger RMSE than the other models.","130b5ce7":"**STEP 3 **\n\n*We test the following Regressors from scikit-learn:*\n* LinearRegression\n* Ridge\n* Lasso\n* DecisionTreeRegressor\n* RandomForestRegressor\n","49355796":"**Outliers**","bb590f50":"**Check for Multicollinearity**\n\n*Strong correlation of these features to other, similar features:*\n\n'GrLivArea_Log' and 'TotRmsAbvGrd'\n\n'GarageCars' and 'GarageArea'\n\n'TotalBsmtSF' and '1stFlrSF'\n\n'YearBuilt' and 'GarageYrBlt'\n\n**Of those features we drop the one that has smaller correlation coeffiecient to Target.**","9ce6796c":" **STEP 2 **","94b948d1":"**Creating Datasets for ML algorithms**","7ed316c6":"**RandomForestRegressor**","58c6571b":"**List of features with missing values**","c68f16be":"**Comparison plot: RMSE of all models**"}}