{"cell_type":{"69c77934":"code","112be13e":"code","ae76f07f":"code","ee963b4b":"code","9d909698":"code","b57a2382":"code","e81ecc68":"code","811e9729":"code","32500379":"code","3d8eadf4":"code","be33f349":"code","96b631de":"code","ba50914b":"code","be8216a4":"code","e335c92e":"code","60bf6034":"code","f2ac4af3":"code","e6c95600":"code","594f4c62":"code","89de64ea":"code","971759ca":"code","40d5b412":"code","f68caeb5":"code","b798d05c":"code","4eac9c9b":"code","3ae15d67":"code","5ee2ad85":"code","54a4411d":"code","cf07d03e":"code","3ff767ea":"markdown","94e0bc76":"markdown","a282c716":"markdown","0d4cc837":"markdown","f6314a95":"markdown","62aa622a":"markdown","0d5d2ddc":"markdown","b7bca2ee":"markdown","99911090":"markdown","06f79aa3":"markdown","12584484":"markdown","7200d1fb":"markdown","782f57b7":"markdown","0f8ad700":"markdown","51385065":"markdown","55ebde96":"markdown","571649f6":"markdown","d4ce9f0c":"markdown"},"source":{"69c77934":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","112be13e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","ae76f07f":"data=pd.read_csv(\"..\/input\/biomechanical-features-of-orthopedic-patients\/column_2C_weka.csv\")\ndata.head(20)","ee963b4b":"data.columns\ndata.describe()","9d909698":"data.info()","b57a2382":"colors = ['purple' if i == 'Normal' else 'cyan' for i in data.loc[:,'class']]\npd.plotting.scatter_matrix(data.loc[:, data.columns !='class'],\n                                       c = colors,\n                                       figsize =[30,30],\n                                       diagonal ='hist',    \n                                       alpha = 0.9,\n                                       s = 300,\n                                       marker = '.',\n                                       edgecolor=\"black\"\n                          )\nplt.savefig('graph4.png')\nplt.show()","e81ecc68":"sns.countplot(x=\"class\",data=data)","811e9729":"data.head(20)","32500379":"plt.scatter(data.lumbar_lordosis_angle,data.sacral_slope)\nplt.xlabel(\"lumbar_lordosis_angle\")\nplt.ylabel(\"sacral_slope\")\nplt.show()","3d8eadf4":"from sklearn.linear_model import LinearRegression\nlinear_reg=LinearRegression()\nx=data.lumbar_lordosis_angle.values.reshape(-1,1)\ny=data.sacral_slope.values.reshape(-1,1)\nlinear_reg.fit(x,y)","be33f349":"# PREDICTION\nprediction=np.linspace(min(x),max(x)).reshape(-1,1)\ny_head= linear_reg.predict(prediction)","96b631de":"# VISUALIZATION\nplt.plot(prediction,y_head,color='red',linewidth=3,alpha=0.9)\nplt.scatter(x=x,y=y)\nplt.xlabel(\"lumbar_lordosis_angle\")\nplt.ylabel(\"sacral_slope\")\nplt.show()","ba50914b":"N=data[data['class']==\"Normal\"]\nA=data[data['class']==\"Abnormal\"]\n","be8216a4":"# Visualization\nplt.scatter(N.pelvic_incidence,N.pelvic_radius,color=\"green\",label=\"GOOD\")\nplt.scatter(A.pelvic_incidence,A.pelvic_radius,color=\"red\",label=\"BAD\")\nplt.xlabel(\"pelvic_incidence\")\nplt.ylabel(\"pelvic_radius\")\nplt.legend()\nplt.show()","e335c92e":"data['class']=[1 if each==\"Normal\" else 0 for each in data['class']]\ndata.info() ","60bf6034":"x_data=data.drop([\"class\"],axis=1) # Drop the class feature.\ndata_class=data['class']\ny=data_class.values # Only class feature","f2ac4af3":"x=(x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data))","e6c95600":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3,random_state=1)","594f4c62":"from sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors=22)\nknn.fit(x_train,y_train)\nprediction=knn.predict(x_test)\n","89de64ea":"print(\"K-NN Accuracy : {}\".format(knn.score(x_test,y_test)))","971759ca":"score_list=[]\nfor each in range(1,50):\n    knn2=KNeighborsClassifier(n_neighbors=each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\nplt.plot(range(1,50),score_list)\nplt.xlabel(\"K Values\")\nplt.ylabel(\"Accuracy\")\nplt.show()","40d5b412":"from sklearn.svm import SVC\nsvm=SVC(random_state=1)\nsvm.fit(x_train,y_train)","f68caeb5":"# TEST\nprint(\"Accuracy of SVM Algorithm: \",svm.score(x_test,y_test))","b798d05c":"from sklearn.naive_bayes import GaussianNB\nnb=GaussianNB()\nnb.fit(x_train,y_train)\n","4eac9c9b":"#TEST\nprint(\"Accuracy of Navie Bayes Algorithm: \",nb.score(x_test,y_test))","3ae15d67":"from sklearn.tree import DecisionTreeClassifier\ndecision_class=DecisionTreeClassifier()\ndecision_class.fit(x_train,y_train)","5ee2ad85":"# TEST\nprint(\"Accuracy of Decision Tree Classification: \",decision_class.score(x_test,y_test))","54a4411d":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(n_estimators=100,random_state=1)\n# n_estimators is number of trees\nrf.fit(x_train,y_train)","cf07d03e":"# TEST\nprint(\"Accuracy of Random Forst Classification Algorithm: \",rf.score(x_test,y_test))","3ff767ea":"## NORMALIZATION\n\nWe should normalize the variables in data set. Because some features has high value. This situation can cause big mistakes.  \n\n\n\n\n![image.png](attachment:image.png)","94e0bc76":"<a id=\"6\"><\/a>\n# Support Vector Machine(SVM)\n","a282c716":"We see that k=22 is giving best result :)","0d4cc837":"In classification we cannot use object. So we must convert object to integer or categorical","f6314a95":"# INTRODUCTION\n\nWe will working on Biomechanical Features of Orthopedic Patients data set. \n\n<font color='purple'>\n\nContetnt:\n    \n  \n    \n1. [Load and Check Data](#1)\n2. [Basic Data Analysis (EDA)](#2)\n3. [Regression](#3)\n    \n    * [Linear Regression](#4)\n    \n    \n    \n4. [K-Nearest Neighbor (k-NN)](#5)\n5. [Support Vector Machine(SVM)](#6)\n6. [Naive Bayes Classification](#7)   \n7. [Decision Tree Classification](#8)\n8. [Random Forest Classification](#9)","62aa622a":"<a id=\"3\"><\/a>\n# Regression","0d5d2ddc":"<a id=\"2\"><\/a>\n# Basic Data Analysis (EDA)","b7bca2ee":"### 1. Float64(6):\n\n\n* pelvic_incidence \n* pelvic_tilt numeric\n* lumbar_lordosis_angle\n* sacral_slope\n* pelvic_radius\n* degree_spondylolisthesis\n\n\n### 2. Object(1):\n\n\n* class","99911090":"## KNN","06f79aa3":"<a id=\"8\"><\/a>\n# Decision Tree Classification","12584484":"<a id=\"4\"><\/a>\n# Linear Regression\n","7200d1fb":"## Lets Find Best K Value","782f57b7":"## TRAIN and TEST SPLIT","0f8ad700":"<a id=\"5\"><\/a>\n\n# K-Nearest Neighbor (k-NN)\n","51385065":"<a id=\"1\"><\/a>\n## Load and Check Data","55ebde96":"<a id=\"9\"><\/a>\n# Random Forest Classification\n\n","571649f6":"<a id=\"7\"><\/a>\n# Navie Bayes Classification","d4ce9f0c":"Calculate distances between all data and new data.  The input consists of the k closest training examples in data set. \n\n"}}