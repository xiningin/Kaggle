{"cell_type":{"a13647b4":"code","2db3a8fd":"code","9de6dc9e":"code","042a1f0a":"code","66bd0aaf":"code","b6c73896":"code","151e5b84":"code","78885f25":"code","67ecfbaf":"code","2014cc05":"code","e8ae002b":"code","bbe5b937":"code","1256dd4a":"code","62815e0e":"code","53c50e13":"code","924177ed":"code","26b89f3a":"code","d9940cff":"code","542ca308":"code","b96ccaa2":"code","3a47c5dd":"code","a0dd0307":"code","d5475df9":"code","ccb0e2bb":"markdown","38eaa7c4":"markdown","b7f8f8b9":"markdown","55ab904e":"markdown","b2f0fec6":"markdown","021955d7":"markdown","95649707":"markdown","32391021":"markdown","8d251dd0":"markdown","e016c50a":"markdown","289682e3":"markdown","cd39d65b":"markdown","4f53d5e1":"markdown","363f4316":"markdown","316f9cea":"markdown","4febf18e":"markdown","70fd158f":"markdown"},"source":{"a13647b4":"#imports \nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nimport sklearn.datasets\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport optuna \nimport os\n#models\nimport lightgbm as lgb\nimport xgboost as xgb\nimport torch\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data\nfrom torchvision import datasets\nfrom torchvision import transforms\n\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt","2db3a8fd":"#ignoring warnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\")","9de6dc9e":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv')\nsub_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/sample_submission.csv')","042a1f0a":"train_df.head()","66bd0aaf":"cat_features = [f'cat{x}'for x in range(0,19)]\ncont_features = [f'cont{x}'for x in range(0,11)]\nfeatures = cat_features + cont_features\n\ndata = train_df[features]\nX_test = test_df[features]\ntarget = train_df['target']\n\ntrain_data = train_df[features]\ntrain_data.head()","b6c73896":"#install sweetviz library for automated eda\n!pip install sweetviz","151e5b84":"%%time\n#analyzing data using sweetviz (automated EDA)\nimport sweetviz as sv\ndata_report = sv.analyze(train_df)\ndata_report.show_html('Analysis.html')\n","78885f25":"from IPython.display import IFrame\nIFrame(src = 'Analysis.html',width=1000,height=600)","67ecfbaf":"#setting up the platform\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n\nsns.countplot(target, color=\"g\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Values\")\nax.set(xlabel=\"Target\")\nax.set(title=\"Target values distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","2014cc05":"\n# we will look into the features distribution now, to get insight into the data\nplt.figure()\nfig, ax = plt.subplots(4, 3,figsize=(14, 20))\nfor i,feature in enumerate(cont_features):\n    plt.subplot(4, 3,i+1)\n    sns.distplot(train_data[feature],color=\"blue\", kde=True,bins=120, label='train')\n    sns.distplot(test_df[feature],color=\"red\", kde=True,bins=120, label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\nplt.show()\n\n","e8ae002b":"#we will take a look at the correlation of features\ncorr = train_data.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corr, vmax=0.9, cmap=\"inferno\", square=True)","bbe5b937":"#The next portion of code is adapted from this great kernal by andreshg\n#https:\/\/www.kaggle.com\/andreshg\/tps-march-a-complete-study\n\n#FE for cat2 var\ntrain_data['cat2'] = train_data['cat2'].apply(lambda x: x if x not in['A', 'C', 'D', 'G', 'F', 'J', 'I', 'M', 'Q', 'L', 'O'] else 'CAT2')\nX_test['cat2'] = X_test['cat2'].apply(lambda x: x if x not in['A', 'C', 'D', 'G', 'F', 'J', 'I', 'M', 'Q', 'L', 'O'] else 'CAT2')\n\n# Fix cat5 variable\ntrain_data['cat5'] = train_data['cat5'].apply(lambda x: x if x not in ['N', 'BS', 'BG', 'CA', 'BK', 'T', 'BO', 'CI', 'AY', 'BV'] else 'CAT50')\nX_test['cat5'] = X_test['cat5'].apply(lambda x: x if x not in ['BL', 'BJ', 'CK', 'AH', 'I', 'F', 'C', 'AQ', 'AM', 'X'] else 'CAT51')\n\nX_test['cat5'] = X_test['cat5'].apply(lambda x: x if x not in ['N', 'BS', 'BG', 'CA', 'BK', 'T', 'BO', 'CI', 'AY', 'BV'] else 'CAT50')\nX_test['cat5'] = X_test['cat5'].apply(lambda x: x if x not in ['BL', 'BJ', 'CK', 'AH', 'I', 'F', 'C', 'AQ', 'AM', 'X'] else 'CAT51')\n\n# Fix cat7 variable\ntrain_df['cat7'] = train_df['cat7'].apply(lambda x: x if x not in ['R', 'AA', 'AY', 'AP', 'O', 'Y'] else 'CAT70')\ntrain_df['cat7'] = train_df['cat7'].apply(lambda x: x if x not in ['AL', 'AD', 'L', 'AC', 'V', 'BA'] else 'CAT71')\n\nX_test['cat7'] = X_test['cat7'].apply(lambda x: x if x not in ['R', 'AA', 'AY', 'AP', 'O', 'Y'] else 'CAT70')\nX_test['cat7'] = X_test['cat7'].apply(lambda x: x if x not in ['AL', 'AD', 'L', 'AC', 'V', 'BA'] else 'CAT71')\n\n# Fix cat8 variable\ntrain_df['cat8'] = train_df['cat8'].apply(lambda x: x if x not in ['BC', 'BJ', 'I', 'AW', 'AS'] else 'CAT80')\ntrain_df['cat8'] = train_df['cat8'].apply(lambda x: x if x not in ['AO', 'AG', 'F', 'BD', 'A'] else 'CAT81')\n\nX_test['cat8'] = X_test['cat8'].apply(lambda x: x if x not in ['BC', 'BJ', 'I', 'AW', 'AS'] else 'CAT80')\nX_test['cat8'] = X_test['cat8'].apply(lambda x: x if x not in ['AO', 'AG', 'F', 'BD', 'A'] else 'CAT81')\n\n# Fix cat10 variable\ntrain_df['cat10'] = train_df['cat10'].apply(lambda x: x if x not in ['DF', 'IG', 'HJ', 'EK', 'GE', 'LN', 'HB'] else 'CAT10')\ntrain_df['cat10'] = train_df['cat10'].apply(lambda x: x if x not in ['FR', 'JR', 'GI', 'GK', 'MC', 'HC', 'CD'] else 'CAT11')\n\nX_test['cat10'] = X_test['cat10'].apply(lambda x: x if x not in ['DF', 'IG', 'HJ', 'EK', 'GE', 'LN', 'HB'] else 'CAT10')\nX_test['cat10'] = X_test['cat10'].apply(lambda x: x if x not in ['FR', 'JR', 'GI', 'GK', 'MC', 'HC', 'CD'] else 'CAT11')","1256dd4a":"#label encoding for the categorical values\ndef label_encoder(train_data, test_data, column):\n    le = LabelEncoder()\n    new_feature = f\"{column}_le\"\n    le.fit(train_data[column].unique().tolist() + test_data[column].unique().tolist())\n    \n    train_data[new_feature] = le.transform(train_data[column])\n    test_data[new_feature] = le.transform(test_data[column])\n    \n    return new_feature","62815e0e":"%%time\n#encoding \nEncodedFeatures = [label_encoder(train_data, X_test, feature) for feature in cat_features]\nEncodedFeatures","53c50e13":"Train_features = EncodedFeatures + cont_features\nX_train = train_data[Train_features]\nX_train.head()","924177ed":"#make a class to make dataset\nclass Dataset(torch.utils.data.Dataset):\n  'Characterizes a dataset for PyTorch'\n  def __init__(self, list_IDs, labels):\n        'Initialization'\n        self.labels = labels\n        self.list_IDs = list_IDs\n\n  def __len__(self):\n        'Denotes the total number of samples'\n        return len(self.list_IDs)\n\n  def __getitem__(self, index):\n        'Generates one sample of data'\n        ID = self.list_IDs[index]\n        # Load data and get label\n        X = X_train.loc[ID].to_numpy()\n        y = self.labels[ID]\n\n        return X, y","26b89f3a":"target = train_df['target']\ntraining_set = Dataset(X_train.index, target.values)\n\ndef get_data_generators(params):    \n    # Load dataset and return train validation split.\n    train_size = int(0.95 * len(training_set))\n    test_size = len(training_set) - train_size\n    train_dataset, val_dataset = torch.utils.data.random_split(training_set, [train_size, test_size])\n\n    train_generator = torch.utils.data.DataLoader(train_dataset, **params)\n    val_generator = torch.utils.data.DataLoader(val_dataset, **params)\n    \n    return train_generator, val_generator\n","d9940cff":"#define constants \nuse_cuda = torch.cuda.is_available()\n# DEVICE = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n#currently we only use CPU\nDEVICE = 'cpu'\ntorch.backends.cudnn.benchmark = True\n\nBATCHSIZE = 128\nCLASSES = 10\nDIR = os.getcwd()\nLOG_INTERVAL = 10\nN_TRAIN_EXAMPLES = BATCHSIZE * 30\nN_VALID_EXAMPLES = BATCHSIZE * 10\n\nparams = {'batch_size': BATCHSIZE,\n          'shuffle': True,\n          'num_workers': 1}\nEPOCHS = 10\n\n#make a model Neural Network\ndef define_model(trial): \n    #Suggest number of layers for our model.\n    n_layers = trial.suggest_int('n_layers', 1, 3)\n    layers = []\n    #number of input neurons\/features\n    in_features = 30\n    for i in range(n_layers):\n        out_features = trial.suggest_int('n_units_l{}'.format(i), 4, 128)\n        layers.append(torch.nn.Linear(in_features, out_features))\n        layers.append(torch.nn.ReLU())\n        #set dropout for regularization\n        p = trial.suggest_float(\"dropout_l{}\".format(i), 0.2, 0.5)\n        layers.append(nn.Dropout(p))\n        in_features = out_features\n    \n    classes = 1\n    layers.append(torch.nn.Linear(in_features, classes))\n    layers.append(torch.nn.Sigmoid())\n    return torch.nn.Sequential(*layers)\n\n\n#We will define objective function for actual training during study.\ndef objective(trial):\n    \n    # Generate the model.\n    model = define_model(trial).to(DEVICE)\n     # Generate the optimizers.\n    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n    optimizer = getattr(optim, optimizer_name)(model.parameters(), lr=lr)\n    \n    #Splitting into 95-5 train validation data.\n    train_loader, valid_loader = get_data_generators(params)\n    \n    # Training of the model.\n    for epoch in range(EPOCHS):\n        model.train()\n        for batch_idx, (data, target) in enumerate(train_loader):\n            # Limiting training data for faster epochs.\n            if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\n                break\n\n            data, target = data.to(DEVICE), target.to(DEVICE)\n            \n            #Actual training, propogating loss using optimizer.\n            optimizer.zero_grad()\n            output = model(data.float())\n            msloss = nn.MSELoss()\n            loss = msloss(output.float(),target.float())\n            loss.backward()\n            optimizer.step()\n    \n    \n        # Validation of the model.\n        model.eval()\n        preds = []\n        y_test = []\n        with torch.no_grad():\n            for batch_idx, (data, target) in enumerate(valid_loader):\n                # Limiting validation data.\n                if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n                    break\n                data, target = data.to(DEVICE), target.to(DEVICE)\n                output = model(data.float())\n                preds.extend(output.numpy())\n                y_test.extend(target.numpy())\n        \n        preds = np.array(np.array(preds) > 0.5,dtype=int)\n        # print(\"acc\", accuracy_score(y_test,preds))\n        if len(np.unique(preds)) != 2:\n            auc_score = 0\n        else:\n            auc_score = roc_auc_score(y_test,preds)\n        trial.report(auc_score, epoch)\n\n        # Handle pruning based on the intermediate value.\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n    #calculate auc loss\n    return auc_score","542ca308":"#ignore debug code \n# Validation of the model.\n# import os\n# os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n# model.eval()\n# preds = []\n# y_test = []\n# with torch.no_grad():\n#     for batch_idx, (data, target) in enumerate(valid_loader):\n#         # Limiting validation data.\n#         if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n#             break\n#         data, target = data.to(DEVICE), target.to(DEVICE)\n#         output = model(data.float())\n#         preds.append(output.cpu().numpy().reshape((BATCHSIZE,)))\n#         y_test.append(target.cpu().numpy())\n# print(\"acc\", accuracy_score(y_test,preds))\n# auc_score = roc_auc_score(y_test,preds)\n# np.array(preds.cpu().numpy(),dtype='float')\n# preds = []\n# y_test = []\n# with torch.no_grad():\n#     for batch_idx, (data, target) in enumerate(valid_loader):\n#         # Limiting validation data.\n#         if batch_idx * BATCHSIZE >= N_VALID_EXAMPLES:\n#             break\n#         data, target = data.to(DEVICE), target.to(DEVICE)\n#         output = model(data.float())\n#         preds.extend(output.numpy())\n#         y_test.extend(target.numpy())\n# np.array(np.array(preds) > 0.3,dtype=int).reshape(-1,BATCHSIZE)\n# for k,l in valid_loader:\n#     data, target = data.to(DEVICE), target.to(DEVICE)\n#     output = model(data.float())\n    \n#     print(output.reshape((BATCHSIZE,)).shape, target.shape)\n#     loss = nn.MSELoss()\n#     loss_val = loss(output.float(),target.float())\n#     loss_val.backward()\n#     break\n# model(torch.from_numpy(X_train.loc[0].to_numpy()).float())","b96ccaa2":"#Create a study object and optimize the objective function.\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=40, timeout=1000)","3a47c5dd":"print(f\"Value of Best trial is: {study.best_trial.value}\")\nprint(\"Best value Params: \")\nfor key, value in study.best_trial.params.items():\n    print(\"    {}: {}\".format(key, value))","a0dd0307":"#define constants \nuse_cuda = torch.cuda.is_available()\n# DEVICE = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\nDEVICE = 'cpu'\ntorch.backends.cudnn.benchmark = True\n\nBATCHSIZE = 128\nCLASSES = 10\nDIR = os.getcwd()\nLOG_INTERVAL = 10\nN_TRAIN_EXAMPLES = BATCHSIZE * 30\nN_VALID_EXAMPLES = BATCHSIZE * 10\n\nparams = {'batch_size': BATCHSIZE,\n          'shuffle': True,\n          'num_workers': 4}\nEPOCHS = 10\nlayers = []\n#number of input neurons\/features\nin_features = 30\nfor i in range(2):\n    out_features = 50\n    layers.append(torch.nn.Linear(in_features, out_features))\n    layers.append(torch.nn.ReLU())\n    #set dropout for regularization\n    p = 0.4928102110636615\n    layers.append(nn.Dropout(p))\n    in_features = out_features\n\nclasses = 1\nlayers.append(torch.nn.Linear(in_features, classes))\nlayers.append(torch.nn.Sigmoid())\nmodel = torch.nn.Sequential(*layers).to(DEVICE)\nlr = 0.001879813193722479\noptimizer = getattr(optim, 'Adam')(model.parameters(), lr=lr)\n    \ntrain_generator = torch.utils.data.DataLoader(training_set, **params)\n\n\n# Training of the model.\nfor epoch in range(50):\n    print(f\"Epoch no: {epoch}\")\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_generator):\n        # Limiting training data for faster epochs.\n        if batch_idx * BATCHSIZE >= N_TRAIN_EXAMPLES:\n            break\n\n        data, target = data.to(DEVICE), target.to(DEVICE)\n\n        #Actual training, propogating loss using optimizer.\n        optimizer.zero_grad()\n        output = model(data.float())\n        msloss = nn.MSELoss()\n        loss = msloss(output.float(),target.float())\n#         print(loss)\n        loss.backward()\n        optimizer.step()\n\n#we won't be validating this model as we already know this is best from above and use this to train on full data","d5475df9":"#predictions\n\nwith torch.no_grad():\n    test_data = X_test[Train_features].to_numpy()\n    predictions = model(torch.from_numpy(test_data).float())\n    final_preds = (np.concatenate(predictions.numpy()) > 0.3).astype(int)\n    sub_df['target'] = predictions.numpy()\n    sub_df.to_csv('FinalSubmission.csv')","ccb0e2bb":"Creating a study for trials of hyperparameters, we are maximizing the area under ROC curve which is the evaluation measure used in this competition","38eaa7c4":"# Introduction \n\n### About the competition:\nThe Tabular Playground Series March 2021 is relatively simpler, beginner-friendly competition which includes primary tabular data to play with and getting a good grasp on fundamentals.\n<\/br> \nThis notebook demonstrates a basic pipeline using a neural network with hyper-parameter tuning using Optuna framework. We will also taking a look at visualizaion and some EDA. Finally the notebook concludes the important learnings for the competition\/training.","b7f8f8b9":"Splitting train-validation data for further evaluation, we will use 5% data as validation data because our training dataset consist of 300,000 values and around 15000 would be enough to evaluate performance.The first step is to create dataset class which further called to make a dataloader(which can make better use of multiprocessing)","55ab904e":"## Part 6 - Conclusion\n\nAlthough Neural Networks are very powerful when it comes to retaining complexity of data and learning non-linear functions from the internal patterns of training data, it is prone to overfitting and we cann't see much deeper inside of it. It's like blackbox which is very powerful yet unknown from inside. But it doesn't prevent us to make better use of it. In this notebook we have demonstrated the concept of hyperparameter optimization using optuna which can be further used to find best model for training data given the evaluation metric. If you find this notebook interesting, don't forget to upvote and give your feedback down below. Thanks for reading and your valuable time!!","b2f0fec6":"First we will remove redundant values in categorical features by applying and filtering others function from values, then we will label encode them to use as numeric features for our training dataset.","021955d7":"It seems that the distribution for train and test set matches, meaning the data is sampled from the same distribution which is good for us.","95649707":"Comparing train and test data distribution for continuous variables ","32391021":"We will now make traing dataset from label encoded categorical features + previous continous features, then using train, validation split to train our model and finally evaluate our NN based on validation dataset","8d251dd0":"## Part 5 - Final Model and Final Predictions","e016c50a":"## Part 3 - Feature Engineering","289682e3":"Seperating training data with features from train_df","cd39d65b":"We will use best trial parameters from above and train a model on full training data, which we will use for final predictions","4f53d5e1":"## Part 1 - Automated EDA","363f4316":"## Part 2 - Training Data Visualization\n\nLet's take a quick look at training data distribution and what can we learn more about them.","316f9cea":"There are a few conclusions we can make from this EDA:\n* No feature contains missing\/Null values\n* There are 19 categorical features, 11 categorical features and 1 target which takes only two values (true\/false or 0\/1)\n* cat2, cat3, cat4, cat5, cat6 and cat9 has small portion of distinct values(Hypothesis: - which can be encoded as \"others\" without affecting significant performace)\n* The range of continuous variable are relative same which means we don't need to change the distribution of them.","4febf18e":"### Part 4 - Hyper-parameter Tuning with Optuna\nIn this section, we will tune our hyperparameters using a optimization framework called Optuna. It works by first creating an objective function which includes defining our model, then another part creates a study where our objective model is sampled using different parameters. Interally, the parameters are first sampled using different samplers to find optimum value from previous studies, then pruners are used to efficiently find parameters that work by prunning non-optimal values. The key to the optimization is providing efficient and fast hyperparameter search with distributed processing, in this notebook we will be using gpu to make search faster for our model parameters.","70fd158f":"**Note**:\nThis is only proof of concept for finding optimized hyperparameters of a neural network, your values of best trial may differ from those of here due to inherent non-determinism. Trying for more number of trials is one of the best way to find best_trial which are optimized. "}}