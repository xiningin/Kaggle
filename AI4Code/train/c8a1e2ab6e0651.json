{"cell_type":{"62dedf23":"code","5ab748e8":"code","0a1ea8f3":"code","1649afea":"code","f07a118e":"code","b9ddbe2b":"code","e370183c":"code","a7efc543":"code","2b6fa1d9":"code","aac9ade8":"code","ff449006":"code","9fd9f60a":"code","3842a334":"code","d5e450f7":"code","78820ad9":"code","62ccbe13":"code","d6325499":"code","7fcfd6ea":"code","51e26862":"code","3d9801b7":"code","344f87b8":"code","29e32f6f":"code","ee838d9a":"code","76167446":"code","5c8e21d8":"markdown","ef852724":"markdown","5bd19769":"markdown","924ee4b3":"markdown","ae0ee3b7":"markdown","95f2a18d":"markdown","7b7f03ed":"markdown","934f519b":"markdown","44b21d6e":"markdown","d186a49c":"markdown","e2ed389a":"markdown","2bf27cec":"markdown","a4a453ad":"markdown","2657e3d9":"markdown","e5914df2":"markdown","9e8d9cad":"markdown","a4781d9b":"markdown","48883065":"markdown","9bdd3daf":"markdown"},"source":{"62dedf23":"from google.colab import drive\ndrive.mount('\/content\/drive')","5ab748e8":"## install some extra libraries\n!pip install --no-deps ftfy regex tqdm\n!pip install omegaconf==2.0.0 pytorch-lightning==1.0.8\n!pip uninstall torchtext --yes\n!pip install einops","0a1ea8f3":"!git clone https:\/\/github.com\/openai\/CLIP.git \/content\/drive\/MyDrive\/Image_From_Text_GAN\/CLIP\n!git clone https:\/\/github.com\/CompVis\/taming-transformers \/content\/drive\/MyDrive\/Image_From_Text_GAN\/taming-transformers","1649afea":"import numpy as np\nimport torch\nimport os\nimport imageio\nimport pdb\nimport math\nimport torchvision\nimport torchvision.transforms as T\nimport torchvision.transforms.functional as TF\n\nimport PIL\nimport matplotlib.pyplot as plt\nimport yaml\nfrom omegaconf import OmegaConf\n\nfrom drive.MyDrive.Image_From_Text_GAN.CLIP import clip\n\nfrom IPython.display import HTML\nfrom base64 import b64encode","f07a118e":"def show_from_tensor(tensor):#show image from input tensor\n    img = tensor.clone()\n    img = img.mul(255).byte()\n    img = img.cpu().numpy().transpose((1,2,0))\n    \n    plt.figure(figsize=(10,7))\n    plt.axis('off')\n    plt.imshow(img)\n    plt.show()\n    \ndef norm_data(data):\n    return (data.clip(-1,1)+1)\/2","b9ddbe2b":"learning_rate = 0.5\nbatch_size = 1\nwd = 0.1 #weight decay\nnoise_factor = .1 #0.22\ntotal_iterations = 100# 400\nimg_shape = [225,400,3] #[450, 450, 3] #H W Channel\nsize1, size2, channels = img_shape#size1 - height, size2- width","e370183c":"clip_model, _ = clip.load('ViT-B\/32', jit=False)\nclip_model.eval() #not needed to train again\nprint(clip.available_models())\nprint('Clip Model Visual Input Resolution :', clip_model.visual.input_resolution)\n\ndevice= torch.device(\"cuda:0\")\ntorch.cuda.empty_cache()","a7efc543":"%cd drive\/MyDrive\/Image_From_Text_GAN\/taming-transformers\/\n\n!mkdir -p models\/vqgan_imagenet_f16_16384\/checkpoints\n!mkdir -p models\/vqgan_imagenet_f16_16384\/configs\n\nif len(os.listdir('models\/vqgan_imagenet_f16_16384\/checkpoints\/')) == 0:\n   !wget 'https:\/\/heibox.uni-heidelberg.de\/f\/867b05fc8c4841768640\/?dl=1' -O 'models\/vqgan_imagenet_f16_16384\/checkpoints\/last.ckpt' \n   !wget 'https:\/\/heibox.uni-heidelberg.de\/f\/274fb24ed38341bfa753\/?dl=1' -O 'models\/vqgan_imagenet_f16_16384\/configs\/model.yaml' ","2b6fa1d9":"from taming.models.vqgan import VQModel\n\ndef load_config(config_path, display=False):\n    config_data = OmegaConf.load(config_path)\n    if(display):\n        print(yaml.dump(OmegaConf.to_container(config_data)))\n    return config_data\n\ndef load_vqgan(config, chk_path=None):\n    model = VQModel(**config.model.params)\n    if(chk_path is not None):\n        state_dict = torch.load(chk_path, map_location='cpu')['state_dict']\n        missing, unexpected = model.load_state_dict(state_dict, strict=False)\n    return model.eval() #not going to train\n\ndef generator(x):\n    x = taming_model.post_quant_conv(x)\n    x = taming_model.decoder(x)\n    return x\n\ntaming_config = load_config(\".\/models\/vqgan_imagenet_f16_16384\/configs\/model.yaml\", display=True)\ntaming_model = load_vqgan(taming_config, chk_path=\".\/models\/vqgan_imagenet_f16_16384\/checkpoints\/last.ckpt\").to(device)","aac9ade8":"class Parameters(torch.nn.Module):\n    def __init__(self):\n        super(Parameters, self).__init__()\n        #batch_size=1, 256 channels(needed by VQGAN) size-size to generate225x400\n        self.data = 0.5*torch.randn(batch_size, 256, size1\/\/16, size2\/\/16).cuda() # 1(batch_size)x256(channels)x14(225\/\/16)x25(400\/\/16)\n        # divide by 16 for compatibility for transformer\n        self.data = torch.nn.Parameter(torch.sin(self.data))\n    def forward(self):\n        return self.data","ff449006":"def init_params():#re-initialize parameters again\n    params=Parameters().cuda()\n    optimizer = torch.optim.AdamW([{'params':[params.data], 'lr':learning_rate}], weight_decay=wd)\n    return params, optimizer","9fd9f60a":"params, optimizer = init_params()","3842a334":"def encodeText(text):#encode text using CLIP\n    t=clip.tokenize(text).cuda()\n    t=clip_model.encode_text(t).detach().clone()\n    return t\n\ndef create_encodings(include, exclude, extras):#what to include, exclude; extras-some extras to apply for includes\n    include_enc =[]\n    for text in include:\n        include_enc.append(encodeText(text))\n    exclude_enc = encodeText(exclude) if exclude != '' else 0\n    extras_enc = encodeText(extras) if extras != '' else 0\n    \n    return include_enc, exclude_enc, extras_enc\n\n#augmentation of images which is needed by CLIP (CLIP needs crop images. the more the variation the better the CLIP understanding)\naug_Transform = torch.nn.Sequential(\n    torchvision.transforms.RandomHorizontalFlip(),\n    torchvision.transforms.RandomAffine(30, (.2, .2))\n).cuda()","d5e450f7":"# test to generate image with random parameters\nwith torch.no_grad():\n    print(params().shape)\n    img = norm_data(generator(params()).cpu()) # 1 x 3(channels) x 224(224 rounded to 225) x 400\n    print('image dimensions :',img.shape)\n    show_from_tensor(img[0])","78820ad9":"def create_crops(img, num_crops=32):\n    p = size1\/\/2\n    #pad before transformations\n    img = torch.nn.functional.pad(img,(p,p,p,p), mode='constant', value=0)#zero padding on all sides of image 1x3x448x624(since 112*2 is added on all sides to 224x400)\n    img = aug_Transform(img)\n    crop_set = [] #cropping\n    for ch in range(num_crops):\n        gap1 = int(torch.normal(1.0, .5, ()).clip(.2, 1.5)*size1)\n        #gap1 = int(torch.normal(1.2, .3, ()).clip(.43, 1.9)*size1)\n        \n        offset_x = torch.randint(0, int(size1*2-gap1), ())#offset in x axis\n        offset_y = torch.randint(0, int(size1*2-gap1), ())#offset in y axis\n        \n        crop = img[:,:,offset_x:offset_x+gap1, offset_y:offset_y+gap1]\n        crop = torch.nn.functional.interpolate(crop,(224,224), mode='bilinear', align_corners=True)#interpolate - resize\n        crop_set.append(crop)\n    \n    img_crops = torch.cat(crop_set, 0) # 30(30 crops) x 3(channels) x 224(height) x 224(width) cat-concat\n    randnormal = torch.randn_like(img_crops, requires_grad=False)\n    num_rands=0\n    randstotal=torch.rand((img_crops.shape[0],1,1,1)).cuda()\n    \n    for ns in range(num_rands):\n        randstotal*=torch.rand((img_crops.shape[0],1,1,1)).cuda()\n        \n    img_crops = img_crops + noise_factor*randstotal*randnormal # add some noise to crops\n    \n    return img_crops    ","62ccbe13":"def display(params, show_crop):\n    with torch.no_grad():\n        generated = generator(params())\n        if(show_crop):#optional display augmented cropped image\n            print('Augmented Crop Example')\n            aug_gen = generated.float() #1 x 3 x 224(not 225) x 400\n            aug_gen = create_crops(aug_gen, num_crops=1)#ask for one crop\n            aug_gen_norm = norm_data(aug_gen[0]) #normalize\n            show_from_tensor(aug_gen_norm)\n        print('Generated Image')\n        latest_gen = norm_data(generated.cpu()) # 1 x 3 x 224 x 400\n        show_from_tensor(latest_gen[0])\n    return latest_gen[0]","d6325499":"normalize = torchvision.transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n#already prepared parameters while training CLIP (they are just mean and variance)","7fcfd6ea":"def optimize_result(params, prompt):\n    alpha = 1 #importance of include encoding\n    beta = .5 #importance of exclude encoding\n    \n    #image encoding\n    out = generator(params())\n    out = norm_data(out)\n    out = create_crops(out)\n    out = normalize(out) #extra normalization for CLIP compatibility 30 x 3 x 224 x 224\n    img_encode = clip_model.encode_image(out)    #encode image  30 x 512 (for each crop encoded value)\n    \n    #text encoding w1-include encoding weight; w2-extra encoding weight\n    final_enc = w1*prompt + w1*extras_enc #prompt & extras_enc : 1 x 512\n    final_text_include_enc = final_enc \/ final_enc.norm(dim=-1, keepdim=True) # 1 x 512\n    final_text_exclude_enc = exclude_enc\n    \n    #calc loss\n    main_loss = torch.cosine_similarity(final_text_include_enc, img_encode, -1) # 30\n    penalize_loss = torch.cosine_similarity(final_text_exclude_enc, img_encode, -1) #30\n    \n    final_loss = -alpha*main_loss + beta*penalize_loss\n    return final_loss\n\ndef loss_fn(params, optimizer, prompt): #returns loss\n    loss = optimize_result(params, prompt).mean() #mean loss\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    return loss","51e26862":"def training(params, optimizer, show_crop=False):\n    res_img = [] #images\n    res_z = [] #parameters\n    \n    for prompt in include_enc:\n        iteration = 0\n        params, optimizer = init_params() # 1 x 256 x 14(225\/16) x 25(400\/16)\n        \n        for i in range(total_iterations):\n            loss = loss_fn(params, optimizer, prompt)\n            if((iteration>80) and iteration%(total_iterations-1) == 0):\n                new_img = display(params, show_crop)\n                res_img.append(new_img)\n                res_z.append(params()) # 1 x 256 x 14 x 25\n                print('Loss :', loss.item(), '\\nIteration :', iteration)\n            \n            iteration+=1\n        torch.cuda.empty_cache()\n    return res_img, res_z","3d9801b7":"torch.cuda.empty_cache()\n#include = ['sketch of a lady', 'sketch of a man on horse']\n#exclude = 'watermark, cropped, confusing, incoherent, cut, blurry'\n#extras = 'watercolor paper texture'\n\ninclude = ['A painting of a pineapple in a bowl']\nexclude='watermark'\nextras=''\nnoise_factor= .22\ntotal_iter=400\n\nw1=1\nw2=1\ninclude_enc, exclude_enc, extras_enc = create_encodings(include, exclude, extras)\nres_img, res_z = training(params, optimizer, show_crop=True)","344f87b8":"torch.cuda.empty_cache()\n#include = ['A forest with purple trees', 'two boxers fighting in a boxing ring', 'a boy at the top of a mountain, looking at the stars', 'a boy at the top of a mountain, looking at the stars', 'one hundred people with blue jackets', 'one hundred people with blue jackets']\ninclude = ['a elephant at the top of a mountain, looking at the stars', 'one hundred people with green jackets']\nexclude = 'watermark, cropped, confusing, incoherent, cut, blurry'\nextras = ''\nw1=1\nw2=1\n#noise_factor= .22\n#total_iter=110\n#show_step=10 # set this to see the result every 10 interations beyond iteration 80\ninclude_enc, exclude_enc, extras_enc = create_encodings(include, exclude, extras)\nres_img, res_z = training(params, optimizer, show_crop=True)","29e32f6f":"def interpolate(res_z_list, duration_list):\n  gen_img_list = []\n  fps = 25\n\n  for idx, (z,duration) in enumerate(zip(res_z_list, duration_list)):\n    num_steps = int(duration*fps)\n    z1=z\n    z2=res_z_list[(idx+1)%len(res_z_list)] # 1 x 256 x 14 x 25\n\n    for step in range(num_steps):\n      alpha = math.sin(1.5*step\/num_steps)**6\n      z_new = alpha * z2 + (1-alpha)*z1\n\n      new_gen = norm_data(generator(z_new).cpu())[0] # 3 x 224 400\n      new_img = T.ToPILImage(mode='RGB')(new_gen)\n      gen_img_list.append(new_img)\n  return gen_img_list\n\ndurations=[5,5,5,5,5,5]\ninterpulation_res_img_list = interpolate(res_z, durations)","ee838d9a":"out_video_path = '\/content\/drive\/MyDrive\/Image_From_Text_GAN\/Video_1.mp4'\nwriter = imageio.get_writer(out_video_path, fps=25)\nfor pil_img in interpulation_res_img_list:\n  img = np.array(pil_img, dtype=np.uint8)\n  writer.append_data(img)\nwriter.close()","76167446":"mp4 = open('..\/Video_1.mp4', 'rb').read()\ndata = 'data:video\/mp4;base64'+b64encode(mp4).decode()\nHTML(\"\"\"<video width=800 controls><source src=\"%s\" type=\"video\/mp4\"><\/video>\"\"\"%data)","5c8e21d8":"# Display Video","ef852724":"# Install Packages","5bd19769":"# Helper Functions","924ee4b3":"# Optimization","ae0ee3b7":"# Interpolation","95f2a18d":"# Test Case 1","7b7f03ed":"# Instantiating Taming Transformer","934f519b":"# Training","44b21d6e":"# Display Generated Images (Crops also)","d186a49c":"# Parameters & Hyper-Parameters","e2ed389a":"# Encoding Text Prompts (Using CLIP)","2bf27cec":"# Mount Drive","a4a453ad":"# Create Crops To Pass to Encode the image\nClip Needs variations of images not one single image","2657e3d9":"# Instantiating CLIP Model","e5914df2":"# Create Video","9e8d9cad":"# Import Libraries","a4781d9b":"# Declare parameters","48883065":"We'll use 'ViT-B\/32' in ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'ViT-B\/32', 'ViT-B\/16']<br>\nInput images are to be encoded with 224 pixels","9bdd3daf":"# Test Case 2"}}