{"cell_type":{"4e9bce98":"code","4cff7e4c":"code","151a7f16":"code","101bc274":"code","7ff95773":"code","15d510da":"code","b2c62ebd":"code","95e4b4fe":"code","db32449d":"code","ee179dae":"code","51f9da54":"code","78f845c3":"code","f2c23552":"code","245efceb":"code","bf5a58c5":"code","187f1974":"code","175c5fc2":"markdown","f7dc5596":"markdown","56d2bc9f":"markdown","f83b889b":"markdown"},"source":{"4e9bce98":"#Setup Pandas and seaborn\nimport pandas as pd\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score, roc_auc_score\nprint(\"Setup Complete\")","4cff7e4c":"#Using Pandas to load the data \ndata_h = pd.read_csv(\"..\/input\/home-ex\/home_exercise_data.csv\") \ndata_h.describe()","151a7f16":"#Leave only the features and clean y_test nan as observed in the describe\ndata_h = data_h.dropna(axis=0, subset=[\"y_test\"])\nfig, ax = plt.subplots(ncols=4, figsize =(25,10))\nsns.boxplot(x=\"y_test\", y=\"x0\",data=data_h, ax=ax[0])\nsns.boxplot(x=\"y_test\", y=\"x1\" ,data=data_h, ax=ax[1])\nsns.boxplot(x=\"y_test\", y=\"x2\", data=data_h, ax=ax[2])\nsns.boxplot(x=\"y_test\", y=\"x3\", data=data_h, ax=ax[3])\n\nfig2, ax2 = plt.subplots(ncols=4, figsize =(25,10))\nsns.boxplot(x=\"y_test\", y=\"x4\" ,data=data_h, ax=ax2[0])\nsns.boxplot(x=\"y_test\", y=\"x5\", data=data_h, ax=ax2[1])\nsns.boxplot(x=\"y_test\", y=\"x6\", data=data_h, ax=ax2[2])\nsns.boxplot(x=\"y_test\", y=\"x7\", data=data_h, ax=ax2[3])\nplt.show()","101bc274":"# After looking at the plots from above we can tell about which feature effects whice y_test result\n#checking for correlation\ncorr = data_h.drop([\"y_test\",\"y_conf\", \"y_pred\",\"Unnamed: 0\"], axis=1).corr()\nprint(corr)\nsns.heatmap(corr,cmap=\"YlGnBu\")","7ff95773":"# because of the large corrolation (in abs) between x4, x5 and x6 we will want to ignore at least one of these variables to save time and memory.\n# in order to detrmine which viriable is best to ignore - we will sum up all corralations (in abs) and ignore the one with the largest number\ncorr_sum = (corr.abs()).sum()\ncorr_sum[['x4','x5','x6']]","15d510da":"only_features = data_h.drop([\"y_test\",\"y_conf\", \"y_pred\",\"Unnamed: 0\",\"x5\"], axis=1)\nmean = only_features.mean(axis=1)\nstd = only_features.std(axis=1)\nonly_features.head()","b2c62ebd":"#centering the data\nonly_features_centered = only_features.apply(lambda x: (x - mean)\/std)\nonly_features_centered.head()","95e4b4fe":"#using PCA to get more data on the model\n#Reducing 8 dimensions to 3\npca = PCA(n_components = 3)\nprincipalComponents = pca.fit_transform(only_features_centered)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2','principal component 3'])\nprincipalDf.head()","db32449d":"finalDf = pd.concat([principalDf, data_h[['y_test']]], axis = 1)\nfinalDf.head()","ee179dae":"ax = plt.axes(projection='3d')\nimport numpy as np\n\n\n# Data for three-dimensional scattered points\nfig = plt.figure(figsize=(8,8))\nzdata = principalDf['principal component 1']\nxdata = principalDf['principal component 2']\nydata = principalDf['principal component 3']\nax.scatter3D(xdata, ydata, zdata, c=data_h[\"y_test\"])","51f9da54":"# we can see that the yellow points(y_test = 3) are with a smaller amount than the rest and they are not next to the same gravity point\nprint(pca.explained_variance_ratio_)\n# We lost some information here but we are close to the ideal 95%.\nprint(sum(pca.explained_variance_ratio_)) ","78f845c3":"#y_conf in a graph\nplt.hist(data_h['y_conf'])\nplt.show()","f2c23552":"# Creating Accuracy, Coverage and Threshold Arrays\n# I am creating a 3d graph with the 3 axis being: threshold, accuracy and coverage\n\nt_df = data_h[[\"y_pred\",\"y_test\",\"y_conf\"]]\nt_df['hit'] = np.where(t_df['y_pred']== t_df[\"y_test\"], 1, 0)\n\n\ndef cov(threshold, df):\n    count_cov = 0\n    total_rows = 12250.0\n    coverage_series = df.apply(lambda x: True if x['y_conf'] >= threshold else False , axis=1)\n    for case in coverage_series:\n        if case is True:\n            count_cov += 1\n    return float(count_cov)\/total_rows\n            \n\ndef acc(threshold, df):\n    count_acc = 0\n    count_cov = 0\n    accurate_series_correct = df.apply(lambda x: True if  x['y_conf'] >= threshold and x[\"y_test\"] == x[\"y_pred\"] else False, axis=1)\n    coverage_series = df.apply(lambda x: True if x['y_conf'] >= threshold else False , axis=1)\n    \n    for case in coverage_series:\n        if case is True:\n            count_cov += 1\n            \n    for case in accurate_series_correct:\n        if case is True:\n            count_acc += 1\n    \n    return float(count_acc)\/float(count_cov)\n\ncov_array = []\nacc_array = []\nthreshold_array = []\n\nfor t in np.linspace(0,1,151):\n    cov_array.append(cov(t,t_df))\n    acc_array.append(acc(t,t_df))\n    threshold_array.append(t)","245efceb":"#3d graph that has accuracy, coverage and the threshold. \n#this graph can give us nice insight regarding the wanted threshold\nax = plt.axes(projection='3d')\nax.scatter(acc_array, cov_array, threshold_array, c=threshold_array, cmap='viridis');","bf5a58c5":"# coverage as a function of threshold\nsns.scatterplot(x=cov_array, y=threshold_array)","187f1974":"# accuracy as a function of threshold\nsns.scatterplot(x=acc_array, y=threshold_array)","175c5fc2":"***Question: WHAT WOULD YOU RECOMMEND THE DS TEAM TO DO?**\n\n**Answer**: I would suggest getting rid of feature x5, this feature is correlated\nwith x4 and x6. Furthermore, I would be aware of the class y_test = 3, because of the low amount of cases. \n","f7dc5596":"***Question : Please offer a new metric that will combine the coverage and the accuracy together to one score.***\n\n**Answer:**  Coverage * Accuracy * Correlation(Coverage,Accuracy)\nI would use this metric because I would like to take into consideration both the coverage and the accuracy into the calculation and also \"punish\" for bad prediction\nThe correlation would be positive if both move to the same direction and negative otherwise. That way I can take into consideration the mistake that was made with the prediction\nand show it in the calculation.","56d2bc9f":"***Final Conclusions***: \n\nI started off with data.describe to get a general feel about the data set. \nAfter that I visualized the data and got very interesting insights. I understood that feature x5 is correleted with x6 and x4. \nI continued with doing a PCA on the model. In the beginning I tried doing a 2d PCA but after seeing that I am losing much valuebal information, I moved into doing it 3d. \nI finished by creating another 3d graph that his z axis is the threshold and his x & y axis are the accuracy and coverage of the graph. \nThis graph has a very distinct inflection point which has a z value, the wanted threshold.\nThis was a very intelligent and fun assignment, thank you!","f83b889b":"The threshold is the z value(threshold array) of the inflection point of the curve \nThis threshold gives us the \"sweet spot\" which combines maximal coverage and maximal accuracy \nAnother option that because of lack of time I didn't pursue is to create a multi-class ROC graph \nfind the inflection point and then do a weighted mean calculating on all the 4 means, based on the appearance of the class # in the model."}}