{"cell_type":{"ffab9972":"code","d6a7630c":"code","a25355bc":"code","6f08be40":"code","f0018ad0":"code","42c3560b":"code","5beb5053":"code","35db26cb":"code","92e728c1":"code","2a38cc3e":"code","4d0bb8c1":"code","125f4a99":"code","40889abd":"code","abc091db":"code","b11a0b84":"code","90c4ceb5":"code","4ba60954":"code","8c6f2818":"code","5417779d":"code","0e892346":"code","c49eb598":"code","34b7e2a1":"code","839a1ec5":"code","aa3937d2":"code","758c9067":"code","1cca3d02":"markdown","a9dbde8a":"markdown","47175b04":"markdown","28326754":"markdown","4c055cab":"markdown","8502304e":"markdown","516aedda":"markdown","1e12a78b":"markdown","3583f61a":"markdown","4dc80b44":"markdown","8572892f":"markdown","7b343c17":"markdown","81644e83":"markdown","b02d7c05":"markdown","64ca3574":"markdown","e5407a71":"markdown","c7cd8fb9":"markdown","34704955":"markdown","4bc8a1e4":"markdown","c7567719":"markdown","772e1ab7":"markdown"},"source":{"ffab9972":"import os\nimport pandas as pd\nimport numpy as np\n\nos.chdir('\/kaggle\/input')\nos.getcwd()","d6a7630c":"df=pd.read_csv('boston-house-prices\/housing.csv')\ndf.head() #all values are in the first column and header is missing","a25355bc":"names=['CRIM','ZN','INDUS','CHAS','NOX','RM','AGE','DIS','RAD','TAX','PTRATIO','B','LSTAT','MEDV'] \ndf=pd.read_csv('boston-house-prices\/housing.csv',delim_whitespace=True,names=names) \n\n#df.head()\n#df.columns\n#df.shape #506*14\ndf.info()  #no missing value ","6f08be40":"df.describe()","f0018ad0":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style('whitegrid')\nscatterplots=sns.PairGrid(df)\nscatterplots.map_offdiag(plt.scatter) \nplt.show() ","42c3560b":"#correlation matrix\n#df.corr()\n\nplt.figure(figsize=(25, 12))\nsns.heatmap(df.corr(), vmin = -1, vmax = 1, center = 0, cmap = 'coolwarm', annot = True)\nplt.show()","5beb5053":"#split the data into predictors X and Y \n#df.info() #X:0-12; Y:13\nX=df.iloc[:,:12]\ny=df.iloc[:,13]","35db26cb":"#Splitting to training and testing data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3, random_state=1)","92e728c1":"from sklearn.linear_model import LinearRegression\n\nlr_all=LinearRegression()  \nlr_all.fit(X_train, y_train) \n\ny_pred1=lr_all.predict(X_test)","2a38cc3e":"# coefficient of intercept\nlr_all.intercept_","4d0bb8c1":"#Converting the coefficient values to a dataframe\nlr_all_coeffcients = pd.DataFrame([X_train.columns,lr_all.coef_]).T\nlr_all_coeffcients = lr_all_coeffcients.rename(columns={0: 'Attribute', 1: 'Coefficients'}) #put into dataframe\nlr_all_coeffcients #print out","125f4a99":"#accuracy score \nlr_all.score(X_test, y_test)","40889abd":"# other evaluation metrics\nfrom sklearn import metrics\nprint('R^2:',metrics.r2_score(y_test, y_pred1))\nprint('Adjusted R^2:',1 - (1-metrics.r2_score(y_test, y_pred1))*(len(y_test)-1)\/(len(y_test)-X_train.shape[1]-1))\nprint('MAE:',metrics.mean_absolute_error(y_test, y_pred1))\nprint('MSE:',metrics.mean_squared_error(y_test, y_pred1))\nprint('RMSE:',np.sqrt(metrics.mean_squared_error(y_test, y_pred1)))","abc091db":"from sklearn.linear_model import Ridge\nridge=Ridge(alpha=100)\nridge.fit(X_train, y_train)\ny_pred2=ridge.predict(X_test)","b11a0b84":"ridge.score(X_test, y_test)","90c4ceb5":"from sklearn.linear_model import Ridge\nrr1=Ridge(alpha=0.01)\nrr1.fit(X_train,y_train)\n\nrr2=Ridge(alpha=100)\nrr2.fit(X_train,y_train)\n\nprint('Linear regression test score:',lr_all.score(X_test,y_test))\nprint('Ridge regression test score with low alpha(0.1):',rr1.score(X_test,y_test))\nprint('Ridge regression test score with high alpha(100):',rr2.score(X_test,y_test)) #high alpha\u5bf9score\u7684penalty\u5f88\u9ad8","4ba60954":"import matplotlib.pyplot as plt\nplt.plot(names[0:12],lr_all.coef_,alpha=0.4,linestyle='none',marker='o',markersize=7,color='green',label='Linear Regression')\nplt.plot(names[0:12],rr1.coef_,alpha=0.4,linestyle='none',marker='*',markersize=7,color='red',label=r'Ridge;$\\alpha=0.01$')\nplt.plot(names[0:12],rr2.coef_,alpha=0.4,linestyle='none',marker='d',markersize=7,color='blue',label=r'Ridge;$\\alpha=100$')\nplt.xlabel('Coefficient Index',fontsize=16)\nplt.ylabel('Coefficient Magnitude',fontsize=16)\nplt.legend(fontsize=13,loc=4)\nplt.show()","8c6f2818":"from sklearn.linear_model import Lasso\nlasso=Lasso(alpha=0.8)\nlasso.fit(X_train, y_train)\ny_pred3=lasso.predict(X_test)\n\nlasso.score(X_test, y_test)","5417779d":"#print(lasso.coef_) \n\n#Converting the coefficient values to a dataframe\nlasso_coeffcients = pd.DataFrame([X_train.columns,lasso.coef_]).T\nlasso_coeffcients = lasso_coeffcients.rename(columns={0: 'Attribute', 1: 'Coefficients'}) #put into dataframe\nlasso_coeffcients #print out","0e892346":"#Viewing by comparing linear and lasso regression coefficient plots \nimport matplotlib.pyplot as plt\nplt.plot(names[0:12],lasso.coef_,alpha=0.4,linestyle='none',marker='o',markersize=7,color='green',label='Lasso Regression')\nplt.plot(names[0:12],lr_all.coef_,alpha=0.4,linestyle='none',marker='d',markersize=7,color='blue',label='Linear Regression')\nplt.xlabel('Coefficient Index',fontsize=16)\nplt.ylabel('Coefficient Magnitude',fontsize=16)\nplt.legend(fontsize=13,loc=4)\nplt.show()","c49eb598":"#find best alpha for Ridge Regression\nfrom sklearn.model_selection import GridSearchCV\nparam_grid={'alpha':np.arange(1,10,500)} #range from 1-500 with equal interval of 10 \nridge=Ridge() \nridge_best_alpha=GridSearchCV(ridge, param_grid)\nridge_best_alpha.fit(X_train,y_train)","34b7e2a1":"print(\"Best alpha for Ridge Regression:\",ridge_best_alpha.best_params_)\nprint(\"Best score for Ridge Regression with best alpha:\",ridge_best_alpha.best_score_)","839a1ec5":"#find best alpha for Lasso Regression\nfrom sklearn.model_selection import GridSearchCV\nparam_grid={'alpha':np.arange(0,0.1,1)} #range from 0-1 with equal interval of 0.1 \nlasso=Lasso() \nlasso_best_alpha=GridSearchCV(lasso, param_grid) \nlasso_best_alpha.fit(X_train,y_train)","aa3937d2":"print(\"Best alpha for Lasso Regression:\",lasso_best_alpha.best_params_)\nprint(\"Best score for Lasso Regression with best alpha:\",lasso_best_alpha.best_score_)","758c9067":"#Preprocessin data \n\n#1. handling with missing value (fill up by mean value)\nfrom sklearn.impute import SimpleImputer \nimport numpy as np\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n\n\n#2. Normalizing raw data\nfrom sklearn.preprocessing import StandardScaler\n\n#3. Select a prediction model \nfrom sklearn.linear_model import LinearRegression\n\n#4. Set up pipeline \nfrom sklearn.pipeline import Pipeline\nsteps=[('imputation',imputer),('scaler',StandardScaler()),('predict',LinearRegression())]\npipeline=Pipeline(steps) \n\n#5. Fit data into pipeline \nreg=pipeline.fit(X_train, y_train)\ny_pred4=reg.predict(X_test)\nreg.score(X_test,y_test)","1cca3d02":"## 1. Import data for analysis","a9dbde8a":"* \ud835\udc45^2 : It is a measure of the linear relationship between X and Y. It is interpreted as the proportion of the variance in the dependent variable that is predictable from the independent variable.\n\n* Adjusted \ud835\udc45^2 :The adjusted R-squared compares the explanatory power of regression models that contain different numbers of predictors.\n\n* MAE : It is the mean of the absolute value of the errors. It measures the difference between two continuous variables, here actual and predicted values of y. \n\n* MSE: The mean square error (MSE) is just like the MAE, but squares the difference before summing them all instead of using the absolute value. \n\n* RMSE: The mean square error (MSE) is just like the MAE, but squares the difference before summing them all instead of using the absolute value. ","47175b04":"## 3.Build Regression Model from Scikit-learn\n* Train the model: .fit()\n* Predit of new data: .predit()","28326754":"**Model Evaluation**","4c055cab":"* 1. in terms of test score: Ridge regression with high alpha has lowest test score","8502304e":"* **2.correlation matrix \/ Heatmap**","516aedda":"* ***compare Linear regression vs Ridge(alpha=0.1) vs Ridge(alpha=100)*** ","1e12a78b":"## 4.Overfitting, Regularization \n* Default Performance Metrics: accuracy=correct prediction\/ total # of prediction\n* The loss fuction: OLS:minimize sum of squares of residuals\n* :) the smaller the loss function, the better the model","3583f61a":"#### 2.2 Check relationship between predictors and outcome variable\n* **1.scatter plot**\n* According to the plots on the last row, we can observe moderate to strong relationship between each predictor and median house price, suggesting these predictors could explain the house prices to some extent. ","4dc80b44":"### 4.1 Ridge Regression\n* Ridge regression is one of the simple techniques to reduce model complexity and prevent over-fitting which may result from linear regression\n* The loss function is altered by adding a penalty equivalent to square of the magnitude of the coefficients \n* **One parameter: Alpha (also called 'lambda')**\n* **higher the alpha value --> more restriction on the coeffs**\n* **lower alpha --> more generalization**\n* **Normal pratice: alpha>1** (e.g. 150;230)","8572892f":"# Regression predictive modelling on Boston House Prices (Linear\/Lasso\/Ridge Regression)\n[Boston House Prices Dataset on Kaggle](https:\/\/www.kaggle.com\/vikrishnan\/boston-house-prices)","7b343c17":"### 4.3 Hyperparameter tunning \n* Ridge and Lasso regression: Choosing alpha\n* Hyperparameters cannot be learned by fitting he model\n* **Solution: GridSearch\/RandomizedSearch**","81644e83":"### 3.1 Linear regression\n* Y=aX+b\n* Y=target, X=features\n* a,b=paremeters of model\n* best line of fit: minimize the error function (SSE) --> best a,b","b02d7c05":"**X: Predictors** \n* CRIM: per capita crime rate by town\n* ZN: proportion of residential land zoned for lots over 25,000 sq.ft.\n* INDUS: proportion of non-retail business acres per town\n* CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n* NOX: nitric oxides concentration (parts per 10 million)\n* RM: average number of rooms per dwelling\n* AGE: proportion of owner-occupied units built prior to 1940\n* DIS: weighted distances to \ufb01ve Boston employment centers\n* RAD: index of accessibility to radial highways\n* TAX: full-value property-tax rate per 10k\n* PTRATIO: pupil-teacher ratio by town 12. \n* B: 1000(Bk\u22120.63)2 where Bk is the proportion of blacks by town 13. \n* LSTAT:%lower status of the population\n\n**Y: Outcome** \n* MEDV: Median value of owner-occupied homes in $1000s","64ca3574":"## 2. Data Cleaning & Wrangling ","e5407a71":"* Regularization: Penalizing large coefficients","c7cd8fb9":"## 5.Preprocessing Data + Pipeline \n* 1. Handling missing value: dropna; fillna; Imputer\n* 2. Normalizing(Centering and scaling): Features on larger scales can unduly in uence the model\n* 3. pipeline\uff1amissing value+normalizaiton+fit model+predict+score","34704955":"#### 2.1 Descriptive Analysis\n**View decriptive statistics for all variables**","4bc8a1e4":"* 2. in terms of magnitude of coefficients: Rigde regression with high alpha penalizes the coefficients on CHAS, NOX, and RM a lot","c7567719":"#### :) Feature Selection \n* Removing the predictors with zero coefficients: **CHAS and NOX**","772e1ab7":"### 4.2 Lasso Regression\n* Lasso regression is another simple technique to reduce model complexity and prevent over-fitting which result from lienar regression\n* Lasso regression not only helps in **reducing over-fitting** but it can help us in **feature selection** \n* **Normal practice: alpha<1** (e.g. 0.1, 0.03) "}}