{"cell_type":{"c665bd80":"code","818e5be9":"code","3eb5b488":"code","c4675efe":"code","779398fe":"code","5c1f83da":"code","ae26149f":"code","4f534c6c":"code","20cfa1e0":"code","f9f3c2f6":"code","0a6e4f8c":"code","2a125808":"code","dfb4b931":"code","2670d1e5":"code","75290944":"code","52345775":"code","13c9686b":"code","a8692059":"code","adda0335":"code","ac1e73ca":"code","f49522b6":"code","74c4ba9f":"code","2a6add48":"code","22f661a3":"code","ef4c3140":"code","6c08e9ff":"code","ae3601b7":"code","ffecf8be":"code","dba24b2e":"code","a2b64ffd":"code","629be28c":"code","78eed3a8":"code","8046a021":"code","02458df0":"code","6c6dd725":"code","21c6cfdc":"code","26bdac1d":"code","dcda1cd9":"code","11e500fd":"code","5f6a1a58":"code","69f5c306":"code","cebca329":"code","7d592999":"code","e19bfb88":"code","090eb45d":"markdown","a1670bb9":"markdown","1f05b98d":"markdown","fbefe01e":"markdown","7ded1a1a":"markdown","5da834df":"markdown","619f806d":"markdown","59668123":"markdown","daece0ba":"markdown","3c07c89b":"markdown","fe10bcff":"markdown","e302ad7d":"markdown","58dcf941":"markdown","9f0ed4f1":"markdown","a5ae3615":"markdown","b4ebc465":"markdown","8d4dfeb5":"markdown","7ae5a7f3":"markdown","5aa85f14":"markdown","81cd5e93":"markdown","720714fe":"markdown","3496f560":"markdown","272cb0b0":"markdown","077fc14f":"markdown","b0510213":"markdown","2d52c0cc":"markdown","ced162c6":"markdown","2babef7d":"markdown","a89d7725":"markdown","748128f4":"markdown","c62e4cbc":"markdown","c6112d1a":"markdown","0741de1c":"markdown","0e31bf61":"markdown","cc5aac16":"markdown","541de980":"markdown","7c0139dd":"markdown","d207a677":"markdown","7020aebd":"markdown","fa4aa164":"markdown","6ddfc0c3":"markdown","c029ad7a":"markdown","026165f5":"markdown","30d14f1e":"markdown","48169ee9":"markdown","ff0e1054":"markdown","d7e04b8b":"markdown","a151995e":"markdown","2d8c7ce3":"markdown","437f51f2":"markdown","ac4091c6":"markdown","4ad54af1":"markdown"},"source":{"c665bd80":"import os, random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.image import ImageDataGenerator, load_img\n\nprint(os.listdir(\"..\/input\/dogs-vs-cats-redux-kernels-edition\/\"))","818e5be9":"PATH = '\/kaggle\/input\/dogs-vs-cats-redux-kernels-edition\/'\n\nnum_classes  = 2\nsample_size  = 25000\nIMG_size     = 224\nbatch_size   = 50\nepoch_num    = 50","3eb5b488":"train_img_path = os.path.join(PATH, \"train.zip\")\ntest_img_path  = os.path.join(PATH, \"test.zip\")\n\nimport zipfile\nwith zipfile.ZipFile(train_img_path, \"r\") as z:\n   z.extractall(\".\")\nwith zipfile.ZipFile(test_img_path, \"r\") as z:\n   z.extractall(\".\")","c4675efe":"filenames  = os.listdir(\".\/train\/\")\ncategories = []\nfor filename in filenames:\n    category = filename.split('.')[0]\n    if category == 'dog':\n        categories.append(1)\n    else:\n        categories.append(0)\n\ndf = pd.DataFrame({\n    'filename': filenames,\n    'category': categories\n})","779398fe":"df.head()","5c1f83da":"df.tail()","ae26149f":"df[\"category\"] = df[\"category\"].replace({0: 'cat', 1: 'dog'}) \n\ntrain_df, val_df = train_test_split(df, test_size=0.4, random_state=2020)\n\ntrain_df  = train_df.reset_index(drop=True)\nval_df    = val_df.reset_index(drop=True)\ntrain_num = train_df.shape[0]\nval_num   = val_df.shape[0]","4f534c6c":"train_df['category'].value_counts().plot.bar()","20cfa1e0":"datagen = ImageDataGenerator(rescale=1.\/255.)\n\ntrain_generator = datagen.flow_from_dataframe(\nx_col = \"filename\",\ny_col = \"category\",\ndataframe = train_df,\ndirectory = \".\/train\/\",\nbatch_size = batch_size,\nshuffle    = True,\nclass_mode = \"categorical\",\ntarget_size = (IMG_size, IMG_size))","f9f3c2f6":"val_generator = datagen.flow_from_dataframe(\nx_col = \"filename\",\ny_col = \"category\",\ndataframe = val_df,\ndirectory = \".\/train\/\",\nbatch_size = batch_size,\nshuffle    = True,\nclass_mode = \"categorical\",\ntarget_size = (IMG_size, IMG_size))","0a6e4f8c":"import keras,os\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPool2D , Flatten\nfrom keras.optimizers import Adam\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping","2a125808":"model_vgg16 = Sequential()\n\n# CONV3-64 + POOL2\nmodel_vgg16.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# CONV3-128 + POOL2\nmodel_vgg16.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# CONV3-256 + POOL2\nmodel_vgg16.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# CONV3-512 + POOL2\nmodel_vgg16.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# CONV3-512 + POOL2\nmodel_vgg16.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg16.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# DENSE\nmodel_vgg16.add(Flatten())\nmodel_vgg16.add(Dense(units=4096,activation=\"relu\"))\nmodel_vgg16.add(Dense(units=4096,activation=\"relu\"))\nmodel_vgg16.add(Dense(units=2, activation=\"softmax\"))","dfb4b931":"opt = Adam(lr = 0.00001)\n\nmodel_vgg16.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\nmodel_vgg16.summary()","2670d1e5":"checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n\nearly = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')","75290944":"hist_vgg16 = model_vgg16.fit_generator(\n    generator = train_generator, \n    epochs = epoch_num,\n    validation_data  = val_generator,\n    validation_steps = val_num\/\/batch_size,\n    steps_per_epoch  = train_num\/\/batch_size,\n    callbacks = [checkpoint,early])","52345775":"def plot_acc_los(model_history):\n    hist = model_history.history\n    acc = hist['accuracy']\n    los = hist['loss']\n    val_acc = hist['val_accuracy']\n    val_los = hist['val_loss']\n    epochs = range(len(acc))\n    f,  ax = plt.subplots(1,2, figsize=(14,6))\n    ax[0].plot(epochs, acc, label='Training accuracy')\n    ax[0].plot(epochs, val_acc, label='Validation accuracy')\n    ax[0].set_title('Training and validation accuracy')\n    ax[0].legend()\n    ax[1].plot(epochs, los, label='Training loss')\n    ax[1].plot(epochs, val_los, label='Validation loss')\n    ax[1].set_title('Training and validation loss')\n    ax[1].legend()\n    plt.show()","13c9686b":"plot_acc_los(hist_vgg16)","a8692059":"test_filenames = os.listdir(\".\/test\/\")\n\ntest_df  = pd.DataFrame({'filename': test_filenames})\ntest_num = test_df.shape[0]","adda0335":"test_generator = datagen.flow_from_dataframe(\nx_col = \"filename\",\ny_col = None,\ndataframe = test_df,\ndirectory = \".\/test\/\",\nbatch_size = batch_size,\nshuffle    = False,\nclass_mode = None,\ntarget_size = (IMG_size, IMG_size))","ac1e73ca":"predict = model_vgg16.predict_generator(test_generator, steps = np.ceil(test_num\/batch_size))","f49522b6":"test_df['category'] = np.argmax(predict, axis=-1)\ntest_df['category'] = test_df['category'].replace({ 1: 'dog', 0: 'cat' })\n\ntest_df['category'].value_counts().plot.bar()","74c4ba9f":"sample_test = test_df.head(9)\nsample_test.head()\n\nplt.figure(figsize=(12, 24))\nfor index, row in sample_test.iterrows():\n    filename = row['filename']\n    category = row['category']\n    img = load_img(\".\/test\/\" + filename, target_size = (IMG_size,IMG_size))\n    plt.subplot(6, 3, index+1)\n    plt.imshow(img)\n    plt.title(\"Predicted:\" + format(category))\n    plt.axis('off')\nplt.tight_layout()\n\nplt.show()","2a6add48":"model_vgg19 = Sequential()\n\n# CONV3-64 + POOL2\nmodel_vgg19.add(Conv2D(input_shape=(224,224,3),filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(Conv2D(filters=64,kernel_size=(3,3),padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# CONV3-128 + POOL2\nmodel_vgg19.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(Conv2D(filters=128, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# CONV3-256 + POOL2\nmodel_vgg19.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(Conv2D(filters=256, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# CONV3-512 + POOL2\nmodel_vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# CONV3-512 + POOL2\nmodel_vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# CONV3-512 + POOL2\nmodel_vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(Conv2D(filters=512, kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_vgg19.add(MaxPool2D(pool_size=(2,2),strides=(2,2)))\n\n# DENSE\nmodel_vgg19.add(Flatten())\nmodel_vgg19.add(Dense(units=4096,activation=\"relu\"))\nmodel_vgg19.add(Dense(units=4096,activation=\"relu\"))\nmodel_vgg19.add(Dense(units=2, activation=\"softmax\"))","22f661a3":"model_vgg19.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])\nmodel_vgg19.summary()","ef4c3140":"checkpoint = ModelCheckpoint(\"vgg19_300.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\nearly      = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n\nhist_vgg19 = model_vgg19.fit_generator(\n    generator = train_generator, \n    epochs = epoch_num,\n    validation_data  = val_generator,\n    validation_steps = val_num\/\/batch_size,\n    steps_per_epoch  = train_num\/\/batch_size,\n    callbacks = [checkpoint,early])    ","6c08e9ff":"plot_acc_los(hist_vgg19)","ae3601b7":"from keras import layers, models, optimizers\nfrom keras.applications import VGG16\n\nconv_base = VGG16(weights='imagenet',\n                  include_top=False,\n                  input_shape=(IMG_size, IMG_size, 3))\n\nmodel_pre_vgg16 = models.Sequential()\nmodel_pre_vgg16.add(conv_base)\n\nmodel_pre_vgg16.add(Flatten())\nmodel_pre_vgg16.add(Dense(units=4096,activation=\"relu\"))\nmodel_pre_vgg16.add(Dense(units=4096,activation=\"relu\"))\nmodel_pre_vgg16.add(Dense(units=2, activation=\"softmax\"))\n\nmodel_pre_vgg16.compile(optimizer=opt, loss=keras.losses.categorical_crossentropy, metrics=['accuracy'])","ffecf8be":"hist_pre_vgg16 = model_pre_vgg16.fit_generator(\n    generator = train_generator, \n    epochs = 10,\n    validation_data  = val_generator,\n    validation_steps = val_num\/\/batch_size,\n    steps_per_epoch  = train_num\/\/batch_size)","dba24b2e":"plot_acc_los(hist_pre_vgg16)  ","a2b64ffd":"import os, cv2, random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom random import shuffle \n\nPATH = '\/kaggle\/input\/dogs-vs-cats-redux-kernels-edition\/'\nFOLDER_TRAIN = '.\/train\/'\nFOLDER_TEST  = '.\/test\/'\nIMG_SIZE     = 224\nNUM_CLASSES  = 2\nSAMPLE_SIZE  = 25000","629be28c":"train_img_path = os.path.join(PATH, \"train.zip\")\ntest_img_path  = os.path.join(PATH, \"test.zip\")\n\nimport zipfile\nwith zipfile.ZipFile(train_img_path, \"r\") as z:\n   z.extractall(\".\")\nwith zipfile.ZipFile(test_img_path, \"r\") as z:\n   z.extractall(\".\")\n\ntrain_img_list = os.listdir(\".\/train\/\")[0: SAMPLE_SIZE]\ntest_img_list  = os.listdir(\".\/test\/\")","78eed3a8":"def label_pet(img):\n    pet = img.split('.')[-3]\n    if pet == 'cat': return [1,0]\n    elif pet == 'dog': return [0,1]\n    \ndef process_data(data_img_list, DATA_FOLDER, isTrain=True):\n    data_df = []\n    for img in tqdm(data_img_list):\n        path = os.path.join(DATA_FOLDER,img)\n        if(isTrain):\n            label = label_pet(img)\n        else:\n            label = img.split('.')[0]\n        img = cv2.imread(path,cv2.IMREAD_COLOR)\n        img = cv2.resize(img, (IMG_SIZE,IMG_SIZE))\n        data_df.append([np.array(img),np.array(label)])\n    shuffle(data_df)\n    return data_df\n\ndef plot_image_list_count(data_image_list):\n    labels = []\n    for img in data_image_list:\n        labels.append(img.split('.')[-3])\n    sns.countplot(labels)\n    plt.title('Cats vs Dogs')","8046a021":"plot_image_list_count(train_img_list)    \n\ntrain = process_data(train_img_list, FOLDER_TRAIN)","02458df0":"X = np.array([i[0] for i in train]).reshape(-1,IMG_SIZE,IMG_SIZE,3)\ny = np.array([i[1] for i in train])\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.4, random_state = 2020)","6c6dd725":"from tensorflow.keras.applications import ResNet50\nfrom tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, GlobalAveragePooling2D,Dropout\n\nBATCH_SIZE = 50\nEPOCH_NUM  = 10","21c6cfdc":"model_RN50 = Sequential()\n\nmodel_RN50.add(ResNet50(include_top=False, pooling='max', weights='imagenet'))\nmodel_RN50.add(Dense(NUM_CLASSES, activation='softmax'))\nmodel_RN50.layers[0].trainable = True\n\nmodel_RN50.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel_RN50.summary()","26bdac1d":"hist_RN50 = model_RN50.fit(X_train, y_train,\n                  batch_size = BATCH_SIZE,\n                  epochs  = EPOCH_NUM,\n                  verbose = 1,\n                  validation_data = (X_val, y_val))","dcda1cd9":"plot_acc_loss(hist_RN50)    ","11e500fd":"score = model_RN50.evaluate(X_val, y_val, verbose=0)\nprint('Validation loss:', score[0])\nprint('Validation accuracy:', score[1])","5f6a1a58":"test = process_data(test_img_list, FOLDER_TEST, False)\n\nf, ax = plt.subplots(5,5, figsize=(15,15))\nfor i,data in enumerate(test[:25]):\n    img_data = data[0]\n    orig = img_data\n    data = img_data.reshape(-1,IMG_SIZE,IMG_SIZE,3)\n    model_out = model_RN50.predict([data])[0]\n    \n    if np.argmax(model_out) == 1: \n        str_predicted='Dog'\n    else: \n        str_predicted='Cat'\n    ax[i\/\/5, i%5].imshow(orig)\n    ax[i\/\/5, i%5].axis('off')\n    ax[i\/\/5, i%5].set_title(\"Predicted:{}\".format(str_predicted))    \nplt.show()","69f5c306":"from tensorflow.keras.applications import InceptionV3\n\nIncep = InceptionV3(weights=INCEP_PATH, include_top=False)\nx     = Incep.output\nx_pool  = GlobalAveragePooling2D()(x)\nx_dense = Dense(1024,activation='relu')(x_pool)\nfinal_pred  = Dense(NUM_CLASSES,activation='softmax')(x_dense)\n\nmodel_Incep = Model(inputs=Incep.input,outputs=final_pred)\n\nmodel_Incep.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel_Incep.summary","cebca329":"hist_Incep = model_Incep.fit(X_train, y_train,\n                  batch_size = BATCH_SIZE,\n                  epochs  = EPOCH_NUM,\n                  verbose = 1,\n                  validation_data = (X_val, y_val))","7d592999":"plot_acc_loss(hist_Incep)","e19bfb88":"score = model_Incep.evaluate(X_val, y_val, verbose=0)\nprint('Validation loss:', score[0])\nprint('Validation accuracy:', score[1])","090eb45d":"Training ResNet50 model on our data set:","a1670bb9":"Visualize validation acc and loss:","1f05b98d":"## Loading and data preparing","fbefe01e":"Visualize validation acc and loss:","7ded1a1a":"Packages:","5da834df":"Test generator:","619f806d":"Callbacks: ModelCheckpoint and EarlyStopping method.\n\n* monitoring validation accuracy by passing **val_acc** to ModelCheckpoint.\n* The model will only be saved to disk if val_acc in current epoch is greater than the last epoch\n* passing val_acc to EarlyStopping, set patience = 10 means the model will stop if there is no rise in val_acc in 10 epochs.","59668123":"Packages and parameters:","daece0ba":"Packages and parameters:","3c07c89b":"Prepare data frame:","fe10bcff":"# <a id=\"3\">References<\/a>\n\n[1] Dogs vs. Cats Redux: Kernels Edition, https:\/\/www.kaggle.com\/c\/dogs-vs-cats-redux-kernels-edition  \n[2] CNN Arthitectures: VGG, Resnet, InceptionNet, XceptionNet, https:\/\/www.kaggle.com\/shivamb\/cnn-architectures-vgg-resnet-inception-tl\n[3] Cats or Dogs - Using CNN with Transfer Learning, https:\/\/www.kaggle.com\/gpreda\/cats-or-dogs-using-cnn-with-transfer-learning","e302ad7d":"Training this model on our data set:","58dcf941":"Training VGG-16 model:","9f0ed4f1":"Compile VGG-16 model: \n* using Adam optimiser to reach global minimum, with learning rate = 0.00001.\n* loss function set as categorical_crossentropy, metrics = accuracy.\n* total 134,268,738 parameters, with trainable parameters 134,268,738.","a5ae3615":"Visualiz validation acc and loss:","b4ebc465":"Define functions for processing image data:","8d4dfeb5":"Creat labels:","7ae5a7f3":"# <a id=\"2\">ResNet \/ InceptionNet<\/a>","5aa85f14":"* Difference between VGG-16 and VGG-19: extra CONV3-512 layer.","81cd5e93":"Build and compile InceptionNet model:\n\n![](https:\/\/hackathonprojects.files.wordpress.com\/2016\/09\/inception_implement.png?w=649&h=337)\n\n* Inception modules performed as local network topology in InceptionNets.\n* Inception modules acts as the multi-level feature extractor in which convolutions of different sizes are obtained to create a diversified feature map.\n\n![](https:\/\/cdn-images-1.medium.com\/max\/2000\/1*uXfC5fcbDsL0TJG4T8PsVw.png)\n\n* Total 22 layers in pretrained model.\n* Importing pre-trained InceptionV3 model from Keras, weights from ImageNet.\n* Adding 1 global avg pooling layer, 1 ReLU dense layer with 1024 units and softmax dense layer with 2 units.\n* Using sigmoid optimizer to compile InceptionNet model.","720714fe":"## <a id=\"16\">Pre-trained VGG-16 model<\/a>","3496f560":"## Prepare train and validation data generator","272cb0b0":"## <a id=\"24\">InceptionNet model<\/a>","077fc14f":"## <a id=\"22\">ResNet50 model<\/a>","b0510213":"## <a id=\"13\">VGG-16 model<\/a>","2d52c0cc":"Visualize validation acc and loss:","ced162c6":"Pretrained model: \n* Importing from Keras and weights from ImagNet.\n* Adding 2 dense layers with 4096 units each and 1 softmax layer with 2 units.","2babef7d":"Build and complie ResNet50 model:\n\n* Importing pre-trained ResNet50 model from Keras, weights from ImageNet.\n* Adding additional layer of Dense with softmax activation function.\n* The first layer is not trainable, used pre-trained model.\n* Compling this model with using sigmoid optimizer.\n* Total 23,591,810 parameters, 23,538,690 trainable.\n\n![](https:\/\/i.stack.imgur.com\/gI4zT.png)","a89d7725":"Data exploration:","748128f4":"Samples with predicted labels:","c62e4cbc":"Preparing training and validation data:","c6112d1a":"Numeric validation acc and loss:","0741de1c":"Training generator:","0e31bf61":"Using the same callbacks and train VGG-19 model:","cc5aac16":"Get predictions:","541de980":"Extract pictures from zip:","7c0139dd":"Training Inception model on our data set:","d207a677":"Compiling VGG-19 model:","7020aebd":"Visualize validation acc and loss:","fa4aa164":"# <a id=\"1\">VGG-16\/19<\/a>","6ddfc0c3":"Validation generator:","c029ad7a":"## Predictions","026165f5":"Loading images:","30d14f1e":"## Predictions:","48169ee9":"## <a id=\"15\">VGG-19 model<\/a>","ff0e1054":"Parameters:","d7e04b8b":"Build VGG-16 model:\n\n* CNN model contains 16 layers in which weights and bias parameters are learned.\n* 13 convolutional layers are stacked one by one and 3 dense layers for classification.\n* the dense layers comprises of 4096, 4096, 2 nodes each.\n* dense layers action = ReLU + ReLU + Softmax.\n\n![](https:\/\/tech.showmax.com\/2017\/10\/convnet-architectures\/image_0-8fa3b810.png)","a151995e":"Prepare test data:","2d8c7ce3":"Getting predictions and show sample images:","437f51f2":"Numeric validation acc and loss:","ac4091c6":"## Loading images","4ad54af1":"Packages:"}}