{"cell_type":{"750cbaf6":"code","04382512":"code","38632313":"code","15548385":"code","9248ffed":"code","cb659048":"code","a1fb7fb9":"code","fde430e7":"markdown","8d37ed5f":"markdown","698f6592":"markdown","0011adb8":"markdown","09f3ab6d":"markdown","760ddc15":"markdown","ae01ae5b":"markdown","73796a3e":"markdown","d7528f00":"markdown"},"source":{"750cbaf6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas as pd\n\nfrom keras.layers import Dense, Dropout, Activation, Reshape, Conv2D, AveragePooling2D, Flatten\nfrom keras.layers import MaxPooling2D\nfrom keras.models import Sequential\nfrom keras.utils import np_utils\nfrom keras.optimizers import adam\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","04382512":"ds = pd.read_csv('..\/input\/train.csv')\ndata = ds.values","38632313":"split = int(0.8 * data.shape[0])\n\nX_train = data[:split, 1:]\/255.0\ny_train = np_utils.to_categorical(data[:split, 0])\n\nX_val = data[split:, 1:]\/255.0\ny_val = np_utils.to_categorical(data[split:, 0])\n\nprint (X_train.shape, y_train.shape)\nprint (X_val.shape, y_val.shape)","15548385":"model = Sequential()\nmodel.add(Reshape(target_shape=(1, 28, 28), input_shape = (784,)))\n\nmodel.add(Conv2D(kernel_size = (5, 5), filters = 4, padding = \"same\"))\n\nmodel.add(Conv2D(kernel_size = (5, 5), filters = 8, padding = \"same\"))\nmodel.add(MaxPooling2D(pool_size = (2, 2), data_format=\"channels_first\"))\n\nmodel.add(Conv2D(kernel_size = (5, 5), filters = 16, padding = \"same\"))\n\nmodel.add(Conv2D(kernel_size = (5, 5), filters = 32, padding = \"same\"))\nmodel.add(MaxPooling2D(pool_size=(2, 2), data_format=\"channels_first\"))\n\nmodel.add(Conv2D(kernel_size = (5, 5), filters = 128, padding = \"same\"))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(output_dim = 128, activation = 'relu'))\n\nmodel.add(Dense(output_dim = 100, activation = 'relu'))\n# Need an output of probabilities of all the 10 digits\nmodel.add(Dense(output_dim = 10, activation= 'softmax'))","9248ffed":"# adam = keras.optimizers.Adam(lr = 0.0005, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08)\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n\nX_train = data[:, 1:]\/255.0\ny_train = np_utils.to_categorical(data[:, 0])\n\nmodel.fit(X_train, y_train, epochs = 30, batch_size=64)","cb659048":"test = pd.read_csv('..\/input\/test.csv')\nresult = model.predict(test)","a1fb7fb9":"result = np.argmax(result,axis = 1)\nresult = pd.Series(result,name=\"Label\")\nsubmission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),result],axis = 1)\nsubmission.to_csv(\"submission_ANN.csv\",index=False)","fde430e7":"model = Sequential()\n\nmodel.add(Dense(512, input_shape=(784, )))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(180))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(10))\nmodel.add(Activation('softmax'))\nmodel.summary()","8d37ed5f":"This model secured in the top 58%.\n\nNow I will try implementing Convolution Neural Networks and see how my rank increases with the power of CNN's.\n\n\n> **ConvNet**","698f6592":"\n**Feed Forward Model Using Keras**","0011adb8":"I ran it for 30 epochs because the value starts to plateau after the 30th epoch giving out the maximum accuracy. \n\nI would recommend to try and test it yourself the number of epochs you want to train, and the number of layers (and nodes in each layer of your model).\nThis is intuitive and there is no specific rule for it.\n\nAnd running the model on train and validation set and saw that the model is not getting overfitted since the validation_accuracy is very similar to Accuracy. So now I will train the model on the whole 'train.csv' dataset.\n","09f3ab6d":"Now that I know that there is not much of a difference b\/w Accuracy and Validation_Accuracy, I can now fit the model to the whole data i.e. 'train.csv' and make predictions on the 'test.csv'","760ddc15":"**Split and Normalize the data**\n\nSplit 'train.csv' into 2 sets: Training Set consists of 80% rows Validation Set consists of 20%\n\nNormalize the data by dividing it by maximum value of a pixel i.e. 255.0 (255.0 such that the answer is stored as a float)","ae01ae5b":"test = pd.read_csv('..\/input\/test.csv')\nsub = model.predict(test)\nresult = np.argmax(sub,axis = 1)","73796a3e":"model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])","d7528f00":"X_train = data[:, 1:]\/255.0\ny_train = np_utils.to_categorical(data[:, 0])\nhist = model.fit(X_train, y_train,\n                 epochs = 30,\n                 shuffle = True,\n                 batch_size=128\n                 ,validation_data=(X_val, y_val)\n                 )"}}