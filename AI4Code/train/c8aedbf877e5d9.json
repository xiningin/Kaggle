{"cell_type":{"6e53c54f":"code","638d3e57":"code","d692b7f0":"code","6b73fd21":"code","edf8896f":"code","f2347b24":"code","7f5538a2":"code","7845f9b3":"code","8261bc8d":"code","0ffc68d5":"code","093c1e73":"code","7d8ef66f":"code","41f91127":"code","89af0979":"code","c56b51c4":"code","6c4fd4ad":"code","f85414dd":"code","8ba056fa":"code","1a9b47dc":"code","ee1cd79f":"code","1a9d4233":"code","2040c2bb":"code","c5475fb2":"code","d4bdd152":"code","4bdff7e3":"code","c2423b91":"code","5b3af88e":"markdown","3b7203a1":"markdown","c1efd53c":"markdown","6e8e38b4":"markdown","ebdb2a0e":"markdown","5af34bf8":"markdown","f021c136":"markdown","5caad67b":"markdown","71711c1d":"markdown","63b8e653":"markdown","99cd927c":"markdown","42b39dc6":"markdown","5b77918c":"markdown","04e234c1":"markdown","b9146898":"markdown","b695a541":"markdown","3cef23b3":"markdown","68280e69":"markdown","9dc6a2bd":"markdown","75a3afd9":"markdown","79544ca7":"markdown"},"source":{"6e53c54f":"import torch\nfrom torchtext import data","638d3e57":"TEXT = data.Field(tokenize = 'spacy', lower = True)\nLABEL = data.LabelField()","d692b7f0":"news = data.TabularDataset(\n    path='..\/input\/News_Category_Dataset_v2.json', format='json',\n    fields={'headline': ('headline', TEXT),\n            'short_description' : ('desc', TEXT),\n             'category': ('category', LABEL)})","6b73fd21":"import random\nSEED = 1234\n\ntrn, vld, tst = news.split(split_ratio=[0.7, 0.2, 0.1], random_state = random.seed(SEED))","edf8896f":"vars(trn[0])","f2347b24":"TEXT.build_vocab(trn, \n                 vectors = \"glove.6B.100d\", \n                 unk_init = torch.Tensor.normal_)\n\nLABEL.build_vocab(trn)","7f5538a2":"print(len(TEXT.vocab))\nprint(len(LABEL.vocab))","7845f9b3":"print(LABEL.vocab.stoi)","8261bc8d":"BATCH_SIZE = 64\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ntrain_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n    (trn, vld, tst), \n    batch_size = BATCH_SIZE, \n    device = device,\n    sort_key= lambda x: len(x.headline), \n    sort_within_batch= False\n    )","0ffc68d5":"import torch.nn as nn\n\nclass RNN(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout):\n        \n        super().__init__()\n                \n        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n        \n        self.lstm_head = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers, bidirectional = bidirectional, dropout = dropout)\n        \n        self.lstm_desc = nn.LSTM(embedding_dim, hidden_dim, num_layers = n_layers, bidirectional = bidirectional, dropout = dropout)\n        \n        self.fc_head = nn.Linear(hidden_dim * 2, 100)\n        \n        self.fc_desc = nn.Linear(hidden_dim * 2, 100)\n        \n        self.fc_total = nn.Linear(200, output_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n                \n    def forward(self, headline, description):\n                        \n        embedded_head = self.dropout(self.embedding(headline))\n        \n        embedded_desc = self.dropout(self.embedding(description))\n                                    \n        output_head, (hidden_head, cell_head) = self.lstm_head(embedded_head)\n        \n        output_desc, (hidden_desc, cell_desc) = self.lstm_desc(embedded_desc)\n        \n        hidden_head = self.dropout(torch.cat((hidden_head[-2, :, :], hidden_head[-1, :, :]), dim = 1))\n        \n        hidden_desc = self.dropout(torch.cat((hidden_desc[-2, :, :], hidden_desc[-1, :, :]), dim = 1))\n        \n        full_head = self.fc_head(hidden_head)\n        \n        full_desc = self.fc_desc(hidden_desc)\n        \n        hidden_total = torch.cat((full_head, full_desc), 1)\n        \n        return self.fc_total(hidden_total)","093c1e73":"INPUT_DIM = len(TEXT.vocab)\nEMBEDDING_DIM = 100\nHIDDEN_DIM = 256\nOUTPUT_DIM = len(LABEL.vocab)\nN_LAYERS = 2\nBIDIRECTIONAL = True\nDROPOUT = 0.2\n\nmodel = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, N_LAYERS, BIDIRECTIONAL, DROPOUT)","7d8ef66f":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","41f91127":"pretrained_embeddings = TEXT.vocab.vectors\n\nprint(pretrained_embeddings.shape)","89af0979":"model.embedding.weight.data.copy_(pretrained_embeddings)","c56b51c4":"import torch.optim as optim\n\noptimizer = optim.Adam(model.parameters())","6c4fd4ad":"criterion = nn.CrossEntropyLoss()\n\nmodel = model.to(device)\ncriterion = criterion.to(device)","f85414dd":"def categorical_accuracy(preds, y):\n    max_preds = preds.argmax(dim = 1, keepdim = True)\n    correct = max_preds.squeeze(1).eq(y)\n    return correct.sum() \/ torch.FloatTensor([y.shape[0]])","8ba056fa":"def train(model, iterator, optimizer, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.train()\n    \n    for batch in iterator:\n        \n        optimizer.zero_grad()\n                        \n        predictions = model(batch.headline, batch.desc).squeeze(1)\n        \n        loss = criterion(predictions, batch.category)\n        \n        acc = categorical_accuracy(predictions, batch.category)\n        \n        loss.backward()\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n        \n    return epoch_loss \/ len(iterator), epoch_acc \/ len(iterator)","1a9b47dc":"def evaluate(model, iterator, criterion):\n    \n    epoch_loss = 0\n    epoch_acc = 0\n    \n    model.eval()\n    \n    with torch.no_grad():\n    \n        for batch in iterator:\n            \n            predictions = model(batch.headline, batch.desc).squeeze(1)\n            \n            loss = criterion(predictions, batch.category)\n            \n            acc = categorical_accuracy(predictions, batch.category)\n\n            epoch_loss += loss.item()\n            epoch_acc += acc.item()\n        \n    return epoch_loss \/ len(iterator), epoch_acc \/ len(iterator)\n","ee1cd79f":"import time\n\ndef epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time \/ 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n","1a9d4233":"N_EPOCHS = 5\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n\n    start_time = time.time()\n    \n    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n    \n    end_time = time.time()\n\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'news_classification_model.pt')\n    \n    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","2040c2bb":"test_loss, test_acc = evaluate(model, test_iterator, criterion)\n\nprint(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')","c5475fb2":"import spacy\nnlp = spacy.load('en')\n\ndef predict_category(model, head, desc):\n    model.eval()\n    head = head.lower()\n    desc = desc.lower()\n    tokenized_head = [tok.text for tok in nlp.tokenizer(head)]\n    tokenized_desc = [tok.text for tok in nlp.tokenizer(desc)]\n    indexed_head = [TEXT.vocab.stoi[t] for t in tokenized_head]\n    indexed_desc = [TEXT.vocab.stoi[t] for t in tokenized_desc]\n    tensor_head = torch.LongTensor(indexed_head).to(device)\n    tensor_desc = torch.LongTensor(indexed_desc).to(device)\n    tensor_head = tensor_head.unsqueeze(1)\n    tensor_desc = tensor_desc.unsqueeze(1)\n    prediction = model(tensor_head, tensor_desc)\n    max_pred = prediction.argmax(dim=1)\n    return max_pred.item()","d4bdd152":"pred = predict_category(model, \"Trump\u2019s Art Of Distraction\", \"The conversation surrounding Trump\u2019s latest racist rants has provoked us to revisit author Toni Morrison\u2019s 1975 keynote address at Portland State University on the true purpose of racism..\")\nprint(f'Predicted category is: {pred} = {LABEL.vocab.itos[pred]}')","4bdff7e3":"pred = predict_category(model, \"Indiana Cop Apologizes After Accusing McDonald\u2019s Worker Of Eating His Sandwich\", \"The Marion County sheriff\u2019s deputy forgot he had taken a bite out of his McChicken earlier that day, authorities said.\")\nprint(f'Predicted category is: {pred} = {LABEL.vocab.itos[pred]}')","c2423b91":"pred = predict_category(model, \"Kyle \u2018Bugha\u2019 Giersdorf, 16, Wins Fortnite World Cup And Takes Home $ 3 Million Prize\", \"Fortnite has nearly 250 million registered players and raked in an estimated $2.4 billion last year.\")\nprint(f'Predicted category is: {pred} = {LABEL.vocab.itos[pred]}')","5b3af88e":"News headline: Kyle \u2018Bugha\u2019 Giersdorf, 16, Wins Fortnite World Cup And Takes Home $ 3 Million Prize\n\nNews short description: Fortnite has nearly 250 million registered players and raked in an estimated $2.4 billion last year.\n\nCorrect category: Sports","3b7203a1":"News headline: Trump\u2019s Art Of Distraction\n\nNews short description: The conversation surrounding Trump\u2019s latest racist rants has provoked us to revisit author Toni Morrison\u2019s 1975 keynote address at Portland State University on the true purpose of racism.\n\nCorrect category: Politics","c1efd53c":"**Building the Model**\n\nIn this section, we define our model. Since we are trying to classify the news based on its headline and short description that are in the form of sentences or paragraphs, we are going to use sequential model that is LSTM (Long Short Term Memory). More specifically, we use bidirectional and two-layered LSTM layer hopefully to get better accuracy for our prediction. We also implement regularization by using dropout during our forward pass. In this model, we specifically split the processing for the headline and short description and concatenate them before final processing to get the prediction of our news' category. The detail can be seen in the diagram below:\n\n![](https:\/\/i.imgur.com\/6nXjqx8.png)","6e8e38b4":"News headline: Indiana Cop Apologizes After Accusing McDonald\u2019s Worker Of Eating His Sandwich\n\nNews short description: The Marion County sheriff\u2019s deputy forgot he had taken a bite out of his McChicken earlier that day, authorities said.\n\nCorrect category: U.S. News","ebdb2a0e":"Here we define the training and evaluate part of our model.","5af34bf8":"**User Input**\n\nIn this section, we let ourself to put our own input and let the model predict the news' categories beyond the dataset. For consistencies, we will use news from Huffington Post and try to get its category predicted. Make sure that the first input is the headline and the second input is the short description of the article.\n\nNews can be obtained from [here](https:\/\/www.huffpost.com\/).","f021c136":"And we test it with our best model.","5caad67b":"Further split our dataset into training set trn, validation set vld, and test set tst using seed for reproducible result.","71711c1d":"We choose Adam algorithm as our optimizer, as well as cross entropy loss for our loss function since we are doing classification problem with multiple categories. We also define the function to calculate accuracy of our prediction.","63b8e653":"So first we specify what our data comprises of. We decide that our data comprises of TEXT which are the news' headlines and short descriptions, as well as LABEL which is the news' category. Here we tokenize the text using [spacy](https:\/\/spacy.io\/?source=post_page---------------------------) tokenizer and to make all the words use lower case. While we keep the entire LABEL as it is.","99cd927c":"Here, let's wrap out data to get the relevant iterator for our training, validation, as well as test sets.","42b39dc6":"We will check an example of our data. It should comprises parsed headline, description, and the associated category.","5b77918c":"We build our vocabulary from our datasets and convert it into vectors from glove. From there we check how many vocabularies we have from our text and how many categories we have.","04e234c1":"Now we create our model and check how many parameters we are training.","b9146898":"**News Classification by LSTM**\n\nIn this notebook, we will try to classify news merely from the language associated with it. We just use its headline and short description to classify the news's category. One thing we intentionally avoid is the author's name due to tendency of certain author to write articles on particular topics.\n\nIn general this notebook is comprised of some sections which are:\n1. Preparing data\n2. Building the model\n3. Training the model\n4. User input\n\nWe use some components here to name a few:\n* Torchtext library\n* Pre-trained word embedding\n* LSTM network architecture\n* Bidirectional LSTM\n* Multi-layered LSTM\n* Regularization\n* Adam optimizer\n* Cross-entropy loss function for classification problem","b695a541":"**Preparing Data**\n\nWe use Torchtext library to pre-process our data. Torchtext simplifies text data pre-processing that includes reading data, tokenizing, converting into tensors, and building vocabulary to be easier.","3cef23b3":"Next, replace the initial weights of the embedding layers with the pre-trained embeddings.","68280e69":"Let's use TabularDataset for json type file here. We extract the entirety of our data into something like dictionary with three keys, 'headline', 'desc', and 'category' that corresponds to each news' headline, short description, and category.","9dc6a2bd":"**Training the Model**","75a3afd9":"**References**\n\nThis notebook was created thanks to the two references below.\n* http:\/\/mlexplained.com\/2018\/02\/08\/a-comprehensive-tutorial-to-torchtext\/\n* https:\/\/github.com\/bentrevett\/pytorch-sentiment-analysis\/blob\/master\/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb","79544ca7":"Now we are ready to train our model. We will train it for five epochs."}}