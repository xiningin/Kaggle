{"cell_type":{"717065b2":"code","2075ba9e":"code","8f6281ac":"code","2751dc41":"code","e1858c18":"code","f10dcfd1":"code","f4f66d7b":"code","d9d3fa84":"code","5e10c064":"code","e12ae1ca":"code","2db2b8ab":"code","225febff":"code","d40823d3":"code","e5d2d7c5":"code","73af79cd":"code","7b6f67f7":"code","c24530fe":"code","3857fd8f":"markdown","9089f96d":"markdown","da7717e1":"markdown","554477f8":"markdown","28d7ef01":"markdown","36610937":"markdown","6ffa86d8":"markdown","f42ab983":"markdown","e1331b3a":"markdown","ab43bf7b":"markdown"},"source":{"717065b2":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","2075ba9e":"df = pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')\ndf.head()","8f6281ac":"df.describe()","2751dc41":"#Let's divide wine into good and bad\nbins = (2, 6.5, 8)\nlabels = ['0', '1'] #bad == 0, good == 1\ndf['quality'] = pd.cut(df['quality'], bins = bins, labels = labels)","e1858c18":"from sklearn.model_selection import train_test_split\nX = df.iloc[: , df.columns!='quality']\ny = df['quality']\nX_train, X_test, y_train, y_test = train_test_split(X, y , test_size= 0.3, random_state=42)","f10dcfd1":"from sklearn.preprocessing import StandardScaler, LabelEncoder\nX_train = StandardScaler().fit_transform(X_train)\nX_test = StandardScaler().fit_transform(X_test)","f4f66d7b":"sns.countplot(x=y, data=df)","d9d3fa84":"from imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=2)\nX_train_s, y_train_s = smote.fit_sample(X_train, y_train.ravel())","5e10c064":"#Balanced data\nsns.countplot(x=y_train_s, data=df)","e12ae1ca":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nparametrs = {'n_estimators':[10, 20 ,30], 'max_depth':[2, 5, 7 , 10]}\ngrid_search_cv_clf = GridSearchCV(rf, parametrs, cv = 5)\ngrid_search_cv_clf.fit(X_train, y_train)\n\nrf_pred = grid_search_cv_clf.predict(X_test)","2db2b8ab":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\nresult = confusion_matrix(y_test, rf_pred)\nprint(\"Confusion Matrix:\")\nprint(result)\nresult1 = classification_report(y_test, rf_pred)\nprint(\"Classification Report:\",)\nprint (result1)\nresult2 = accuracy_score(y_test, rf_pred)\nprint(\"Accuracy:\",result2)","225febff":"grid_search_cv_clf_s = GridSearchCV(rf, parametrs, cv = 5)\ngrid_search_cv_clf_s.fit(X_train_s, y_train_s)\ngrid_search_cv_clf_s.best_params_\n\nrf_pred_s = grid_search_cv_clf_s.predict(X_test)","d40823d3":"result = confusion_matrix(y_test, rf_pred_s)\nprint(\"Confusion Matrix:\")\nprint(result)\nresult1 = classification_report(y_test, rf_pred_s)\nprint(\"Classification Report:\",)\nprint (result1)\nresult2 = accuracy_score(y_test, rf_pred_s)\nprint(\"Accuracy:\",result2)","e5d2d7c5":"from catboost import CatBoostClassifier\n# Initialize CatBoostRegressor\nmodel = CatBoostClassifier(iterations=10,\n                        learning_rate=1,\n                        depth=5)\n# Fit model\nmodel.fit(X_train_s, y_train_s)\n# Get predictions\npred_cb = model.predict(X_test)","73af79cd":"result = confusion_matrix(y_test, pred_cb)\nprint(\"Confusion Matrix:\")\nprint(result)\nresult1 = classification_report(y_test, pred_cb)\nprint(\"Classification Report:\",)\nprint (result1)\nresult2 = accuracy_score(y_test, pred_cb)\nprint(\"Accuracy:\",result2)","7b6f67f7":"from sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf.fit(X_train, y_train)\npred_gs = clf.predict(X_test) ","c24530fe":"result = confusion_matrix(y_test, pred_gs)\nprint(\"Confusion Matrix:\")\nprint(result)\nresult1 = classification_report(y_test, pred_gs)\nprint(\"Classification Report:\",)\nprint (result1)\nresult2 = accuracy_score(y_test, pred_gs)\nprint(\"Accuracy:\",result2)","3857fd8f":"# *CatBoostClassifier*","9089f96d":"## Based on all the training we can conclude that the best prediction was made by RandomForestClassifier\n","da7717e1":"> *low f1-score*","554477f8":"### *You may notice that the data is very unbalanced, so we balance it using the SMOTE method* ","28d7ef01":"# *RandomForestClassifier*","36610937":" Training on balanced data","6ffa86d8":" # *GaussianNB*","f42ab983":"   Training on unbalanced data","e1331b3a":"> *much better on balanced data*","ab43bf7b":"# Hello, world!\n### In this notebook we will distinguish good wine from bad wine using machine learning algorithms:\n1. RandomForestClassifier\n2. CatBoostClassifier\n3. GaussianNB\n\n### At the end, we'll find out which algorithm did better."}}