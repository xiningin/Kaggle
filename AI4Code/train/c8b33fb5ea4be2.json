{"cell_type":{"56db8d84":"code","1f847aa7":"code","340b0e88":"code","bda140d4":"code","1f97f04b":"code","1ebf6057":"code","47cfbfbe":"code","92fbea54":"code","3574e53f":"code","a55eaf0f":"code","966b3331":"code","4b4fbf1a":"code","74e74793":"code","6dd4939c":"code","b4ec5659":"code","cb5294fe":"code","fd19883a":"markdown","e88e7e5f":"markdown","4170aa1b":"markdown","2137bd75":"markdown","4dfcf957":"markdown","fd0dc0db":"markdown","2f08332d":"markdown"},"source":{"56db8d84":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import RegexpTokenizer\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras import layers\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping","1f847aa7":"# Loading the dataset\ndata = pd.read_json('\/kaggle\/input\/quotes-dataset\/quotes.json')\nprint(data.shape)\ndata.head()","340b0e88":"# Dropping duplicates and creating a list containing all the quotes\nquotes = data['Quote'].drop_duplicates()\nprint(f\"Total Unique Quotes: {quotes.shape}\")\n\n# Considering only top 5000 quotes\nquotes_filt = quotes[:4000]\nprint(f\"Filtered Quotes: {quotes_filt.shape}\")\nall_quotes = list(quotes_filt)\nall_quotes[:2]","bda140d4":"# Converting the list of quotes into a single string\nprocessed_quotes = \" \".join(all_quotes)\nprocessed_quotes = processed_quotes.lower()\nprocessed_quotes[:100]","1f97f04b":"# Tokeinization\ntokenizer = Tokenizer(char_level=True, oov_token='<UNK>')\nseq_len = 100  #(Length of the training sequence)\nsentences = []\nnext_char = []\n\n# Function to create the sequences\ndef generate_sequences(corpus):\n    tokenizer.fit_on_texts(corpus)\n    total_vocab = len(tokenizer.word_index) + 1\n    print(f\"Total Length of characters in the Text: {len(corpus)}\")\n    print(f\"Total unique characters in the text corpus: {total_vocab}\")\n    \n    # Loop through the entire corpus to create input sentences of fixed length 100 and the next character which comes next with a step of 1\n    for i in range(0, len(corpus) - seq_len, 1):\n        sentences.append(corpus[i:i+seq_len])\n        next_char.append(corpus[i+seq_len])\n        \n            \n    return sentences, next_char, total_vocab\n\n# Generating sequences\nsentences, next_char, total_vocab = generate_sequences(processed_quotes)\n\nprint(len(sentences))\nprint(len(next_char))\nprint(sentences[:1])\nprint(next_char[:1])","1ebf6057":"# Create a matrix of required shape\nX_t = np.zeros((len(sentences), seq_len, total_vocab), dtype=np.bool)\ny_t = np.zeros((len(sentences), total_vocab), dtype=np.bool)\n\n# Loop through each sentences and each character in the sentence and replace the respective position with value 1.\nfor i, sentence in enumerate(sentences):\n    for t, char in enumerate(sentence):\n        X_t[i, t, tokenizer.word_index[char]] = 1\n        y_t[i, tokenizer.word_index[next_char[i]]] = 1","47cfbfbe":"print(f\"The shape of X: {X_t.shape}\")\nprint(f\"The shape of Y: {y_t.shape}\")","92fbea54":"print(f\"Corpus Length: {len(processed_quotes)}\")\nprint(f\"Vocab Length: {total_vocab}\")\nprint(f\"Total Sequences: {len(sentences)}\")","3574e53f":"# Building the model\ndef create_model():\n    model = Sequential()\n    model.add(layers.LSTM(256, dropout=0.5, input_shape = (X_t.shape[1], X_t.shape[2])))\n    model.add(layers.Dense(total_vocab, activation='softmax'))\n    \n    # compiling the model\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    return model\n\nmodel = create_model()\nmodel.summary()","a55eaf0f":"import gc\ngc.collect()","966b3331":"# Training the model\ncallback = EarlyStopping(monitor='loss', patience=2)\nmodel.fit(X_t, y_t, epochs=100, batch_size=64, callbacks=[callback])","4b4fbf1a":"model.save(\"char_text_gen_quotesV2.h5\")","74e74793":"# # Loading the model\n# from keras.models import load_model\n\n# char_model = load_model(\"..\/input\/char-trained-model\/char_text_gen_quotesV2.h5\")","6dd4939c":"# char_model.summary()","b4ec5659":"# Prediction sampling\ndef sample(preds, temperature = 0.5):\n    preds = np.asarray(preds).astype(\"float64\")\n    scaled_pred = np.log(preds)\/temperature\n    scaled_pred = np.exp(scaled_pred)\n    scaled_pred = scaled_pred\/np.sum(scaled_pred)\n    scaled_pred = np.random.multinomial(1, scaled_pred, 1)\n    return np.argmax(scaled_pred)","cb5294fe":"start_index = np.random.randint(0, len(processed_quotes) - seq_len - 1)\ngenerated = \"\"\nsentence = processed_quotes[start_index : start_index + seq_len].lower()\n\nfor i in range(1000):\n    x_pred = np.zeros((1, seq_len, total_vocab))\n    for t, char in enumerate(sentence):\n        x_pred[0, t, tokenizer.word_index[char]] = 1.0\n    preds = model.predict(x_pred, verbose=0)[0]\n    next_index = sample(preds)\n    next_char = tokenizer.index_word[next_index]\n    sentence = sentence[1:] + next_char\n    generated += next_char\n    \nprint(generated)","fd19883a":"The model generates a decent understandable text. Try using different text corpus to get exciting results.\n\nThanks for reading. Happy Learning!!","e88e7e5f":"In my previous kernel, I have explained the process of text generation at word level. Some of the limitations of that method is, the generated text makes sense only if you have huge amount of text to train with and also the problem of increase in dimensionality of the label due to the increase in number of words in the vocabulary.\n\nTo overcome these problems, you can create a Character lever text generation model, which considers only the unique characters present in your text. If you are considering English language, this would be A-Z(26) and some other puncutations. So, hardly your vocabulary limit will not exceed at max 100-150 (if your text has different symbols and punctuations) campared to the tens of thousands of words in a word level model.\n\nHere, I will walk you through the process of building character level text generation using LSTM. The main task here is to predict the next character given all the previous characters in a sequence of data i.e., generates text character by character.\n\nFirst, let's import the necessary libraries.","4170aa1b":"Thus, finally the data is now ready to feed into the model. The network architecture is:\n* Single LSTM layer with 256 units and dropout\n* One Dense hidden layer with 64 units\n* Final output Dense layer with units equal to our total vocabulary and a **softmax** activation\n* Since it is a multi-class classification we will use **categorial crossentropy** as loss and **adam** as optimizer.","2137bd75":"Now, we will create a prediction sampling function. What this function does is, it takes the prediction for the next character and scales it by the temperature value. Then recomputes the probability and samples the final prediction from a multimomial distribution simulation.\n\nNOTE: A temperature < 1, makes the model to have high confident for the next character whereas temperature > 1 allows the model to be creative (also this leads to higher mistakes). A temperature value of 1, has no scaling and hence it is same as the prediction from the network.","4dfcf957":"# Character-level Text Generation using LSTM","fd0dc0db":"For text generation with character, we want our model to learn probabilities about what character will come next given the seed text\/character. We then join these to our required length to form a sentence. \n\nWe need to know what are all the different characters present in our text data. Since the Neural networks understand only numbers, we need to create a dictionary to store the character with an integer index, so that we can use the index as the proxy for the character (commonly known as Vocabulary).","2f08332d":"Now, we have the data in a more structured format, but still we cannot input as it is. We need convert the text to numbers and reshape the data in **[samples, timesteps, features]** format as expected by the LSTM.\n* First we will create a matrix of zeros for the required shape\n* Then we will create a one-hot encoding of the input sequence by looping over throug each character in the sequence."}}