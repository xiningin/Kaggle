{"cell_type":{"69321cd7":"code","59c7845b":"code","35bc2d07":"code","8793aa66":"code","3e45f667":"code","3e1bdf3b":"code","3c7dd8f4":"code","4cc8ebd8":"code","c79317f4":"code","6b43422a":"code","e6ba17ee":"code","fae1aa31":"code","a5fa566a":"code","0c5a1b5f":"code","a49423b6":"code","f3016ce9":"code","dc844d43":"code","e39330a9":"code","3194502a":"code","8f28fcce":"code","55fc594e":"code","fbdcaf31":"markdown","2bdb1da1":"markdown","67d321f7":"markdown","57b3a844":"markdown","a24aba1b":"markdown","d591acbc":"markdown","25ad036d":"markdown","b8dfbb3b":"markdown","8dc586a3":"markdown","fad889c0":"markdown","a9b861ed":"markdown","bd7cda30":"markdown"},"source":{"69321cd7":"!pip install transformers==3.0.2\n!pip install nlp","59c7845b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom nlp import load_dataset\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nnp.random.seed(1234) \n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","35bc2d07":"mnli = load_dataset(path='glue', name='mnli') # loading more data from the Huggin face dataset\n#snli   =  load_dataset(\"snli\") # loading more data from the Huggin face dataset","8793aa66":"xnli = pd.read_csv('..\/input\/xnli-organized\/xnli_df.csv') # loading pre organized XNLI dataset\nxnli = xnli.rename(columns = {'Unnamed: 0': 'lang_abv', '0' : 'premise', '1': 'hypothesis', '0.1': 'label' }) # renaming the columns\nxnli.head()","3e45f667":"mnli_premise = pd.Series(mnli['train']['premise'])\nmnli_hypothesis = pd.Series(mnli['train']['hypothesis'])\nmnli_label = pd.Series(mnli['train']['label'])\n\n#snli_premise = pd.Series(snli['train']['premise'])\n#snli_hypothesis = pd.Series(snli['train']['hypothesis'])\n#snli_label = pd.Series(snli['train']['label'])\n\n\n#snli = None # cleaning memory \nmnli = None # cleaning memory\n\n\n#snli_df = pd.DataFrame(pd.concat([snli_premise, snli_hypothesis, pd.Series(['en'] * len(snli_label)), snli_label], axis = 1))\nmnli_df = pd.DataFrame(pd.concat([mnli_premise, mnli_hypothesis, pd.Series(['en'] * len(mnli_label)), mnli_label], axis = 1))\n\n\nmnli_premise = None #more memory cleaning\nmnli_hypothesis = None # more memory clenaning\nmnli_label = None #more memory cleaning\n\n#snli_premise = None #more memory cleaning\n#snli_hypothesis = None # more memory clenaning\n#snli_label = None #more memory cleaning","3e1bdf3b":"mnli_df = mnli_df.rename(columns = {0 : 'premise', 1: 'hypothesis', 2: 'lang_abv', 3: 'label' })\nprint(mnli_df.shape)\ndisplay(mnli_df.head())\n\n#snli_df = snli_df.rename(columns = {0 : 'premise', 1: 'hypothesis', 2: 'lang_abv', 3: 'label' })\n#print(snli_df.shape)\n#display(snli_df.head())\n","3c7dd8f4":"train_df = pd.read_csv('..\/input\/contradictory-my-dear-watson\/train.csv')\nprint('Traning Data, the size of the dataset is: {} \\n'.format(train_df.shape))\n\ntest_df = pd.read_csv('..\/input\/contradictory-my-dear-watson\/test.csv')","4cc8ebd8":"train_df = pd.concat([train_df, xnli, mnli_df]) #appending the original dataset to the additional datasets\ntrain_df = train_df[train_df['label'] != -1] #cleaning values with the wrong label\nmnli_df = None\nsnli_df = None\n\nprint('the shape of the whole DF to be used is: ' + str(train_df.shape))","c79317f4":"# searching for duplicates\n\ntrain_df = train_df[train_df.duplicated() == False]\nprint('the shape of the whole DF to be used is: ' + str(train_df.shape))","6b43422a":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize = (15,5))\n\nplt.subplot(1,2,1)\nplt.title('Traning data language distribution')\nsns.countplot(data = train_df, x = 'lang_abv', order = train_df['lang_abv'].value_counts().index)\n\nplt.subplot(1,2,2)\nplt.title('Test data laguage distribution')\nsns.countplot(data = test_df, x = 'lang_abv', order = test_df['lang_abv'].value_counts().index)","e6ba17ee":"# word count\n\ndef word_count(dataset, column):\n    len_vector = []\n    for text in dataset[column]:\n        len_vector.append(len(text.split()))\n    \n    return len_vector\n\ntrain_premise = word_count(train_df, 'premise')\ntrain_hypothesis = word_count(train_df, 'hypothesis')\n\ntest_premise = word_count(test_df, 'premise')\ntest_hypothesis = word_count(test_df, 'hypothesis')\n\nfig = plt.figure(figsize = (15,10))\n\nplt.subplot(2,2,1)\nplt.title('word count for train dataset premise')\nsns.distplot(train_premise)\n\nplt.subplot(2,2,2)\nplt.title('word count for train dataset hypothesis')\nsns.distplot(train_hypothesis)\n\nplt.subplot(2,2,3)\nplt.title('word count for test dataset premise')\nsns.distplot(test_premise)\n\nplt.subplot(2,2,4)\nplt.title('word count for test dataset hypothesis')\nsns.distplot(test_hypothesis)        ","fae1aa31":"# looking at the countplot of the labels of the traning data set\n\nplt.title('Label column countplot')\nsns.countplot(data = train_df, x = 'label')","a5fa566a":"from transformers import BertTokenizer, TFAutoModel, AutoTokenizer\nimport tensorflow as tf\nimport keras\nfrom tensorflow.math import softplus, tanh\nfrom tensorflow.keras.utils import get_custom_objects\nfrom tensorflow.keras import Input, Model, Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, LSTM, Embedding, GlobalAveragePooling1D\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.optimizers import Adam\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras import regularizers\n\n\nnp.random.seed(123)\nmax_len = 50\n\n# this is the model used BERT huggin face\n\nBert_model = \"bert-large-uncased\"\n\n# tokenizer\n\nBert_tokenizer = BertTokenizer.from_pretrained(Bert_model)\n\ndef tokeniZer(dataset,tokenizer):\n    encoded_list = [] # word id array\n    type_id_list = np.zeros((dataset.shape[0], max_len)) #type id array\n    mask_list = np.zeros((dataset.shape[0], max_len)) #masks array\n    \n    for i in range(dataset.shape[0]):\n        datapoint = '[CLS] ' + dataset['premise'][i] + ' [SEP]' + dataset['hypothesis'][i] + ' [SEP]' # putting the two sentences together along with special characters\n        datapoint = tokenizer.tokenize(datapoint)\n        datapoint = tokenizer.convert_tokens_to_ids(datapoint)\n        encoded_list.append(datapoint) \n    \n    encoded_list = pad_sequences(encoded_list, maxlen = max_len, padding = 'post')\n    \n    for i in range(encoded_list.shape[0]):\n        flag = 0\n        a = encoded_list[i]\n        for j in range(len(a)):\n            \n            #building the type_id matrix\n            \n            if flag == 0:\n                type_id_list[i,j] = 0\n            else:\n                type_id_list[i,j] = 1\n                \n            #flag for the type_id matrix\n            \n            if encoded_list[i,j] == 102:\n                flag = 1\n            \n    \n            #building the mask matrix \n            \n            if encoded_list[i,j] == 0:\n                mask_list[i,j] = 0\n            else:\n                mask_list[i,j] = 1\n                \n    return encoded_list,mask_list,type_id_list\n        \n        \n        ","0c5a1b5f":"# softplus - log(exp(x)+1), function that can be used for extra layers in the models\ndef mish(x):\n    return x*tanh(softplus(x))\nget_custom_objects()[\"mish\"] = Activation(mish)","a49423b6":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)","f3016ce9":"# model creator\n\ndef create_BERT(random_seed):\n    \n    tf.random.set_seed(random_seed)\n    \n    with tpu_strategy.scope():\n    \n        transformer_encoder = TFAutoModel.from_pretrained(Bert_model)\n\n        input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_layer\")\n        input_masks = Input(shape = (max_len,), dtype = tf.int32, name = 'input_mask')\n        input_type_id = Input(shape = (max_len,), dtype = tf.int32, name = 'input_type_id')\n\n        sequence_output = transformer_encoder([input_ids, input_masks, input_type_id])[0]\n\n        cls_token = sequence_output[:, 0, :]\n\n        #cls_token = Dense(512, activation = 'mish')(cls_token) # this layer improves the accuracy by several points about 5%\n\n        #cls_token = Dropout(0.2)(cls_token)\n\n        #cls_token = Dense(256, activation  = 'mish')(cls_token)\n\n        #cls_token = Dropout(0.3)(cls_token)\n\n        output_layer = Dense(3, activation='softmax')(cls_token)\n\n\n        model = Model(inputs=[input_ids, input_masks, input_type_id], outputs = output_layer)\n\n        model.summary()\n\n        model.compile(Adam(lr=1e-5), \n                loss='sparse_categorical_crossentropy', \n                metrics=['accuracy']\n            )\n    return model","dc844d43":"#ensemble creation and prediction\n\nfrom sklearn.utils import shuffle # shuffle dataframes\n#import random\n#random.seed(123) # random seed to generate random list of numbers\n\n#number_of_models = 4 #number of BERT models to be used in the ensemble \n\n#randomlist = random.sample(range(10, 1000), number_of_models) #creates a list of random integers that will be used for seeding the BERT models\n\n#history_list = [0] * number_of_models #list to save the training history of the models\n\ncallbacks = [tf.keras.callbacks.EarlyStopping(patience = 2, monitor = 'val_loss', \\\n                                           restore_best_weights = True, mode = 'min')]\n#predictions_list = [] #list to store the predictions of each model in the ensemble\n\n#for i in range(number_of_models):\n#tf.keras.backend.clear_session() #clear session to save memory \n#BertTokenizer = AutoTokenizer.from_pretrained(Bert_model)\n\nshuffled_data = shuffle(train_df).reset_index(drop = True)#shuffle the data to add more variance\n\n\ntrain_df = None #clearing more memory\n\n#input_ids_train, input_masks_train, type_id = tokeniZer(shuffled_data, Bert_tokenizer) #encode shuffled data\n\nbatch_size = 128\n\n#Bert = create_BERT(1234) #creates a single BERT model with a random seed\n#history_bert = Bert.fit([input_ids_train, input_masks_train, type_id], shuffled_data['label'],\n#                         validation_split = 0.2,\n#                         epochs = 30, batch_size = batch_size, callbacks = callbacks)","e39330a9":"XLM_model = \"jplu\/tf-xlm-roberta-large\"\nxlm_tokenizer = AutoTokenizer.from_pretrained(XLM_model) #Xlm tokenizer\n\n\nX_train_ids, X_train_masks, _ = tokeniZer(shuffled_data,xlm_tokenizer) #encoding input","3194502a":"# creating the XLM model \n\ndef create_xlm(transformer_layer,  random_seed, learning_rate = 1e-5):\n    \n    tf.keras.backend.clear_session()\n\n    tf.random.set_seed(random_seed)\n    \n    with tpu_strategy.scope():\n    \n        input_ids = Input(shape = (max_len,), dtype = tf.int32)\n        input_masks = Input(shape = (max_len,), dtype = tf.int32)\n        #input_type_id = Input(shape = (max_len,), dtype = tf.int32)\n\n            #insert roberta layer\n        roberta = TFAutoModel.from_pretrained(transformer_layer)\n        roberta = roberta([input_ids, input_masks])[0]\n\n        #only need <s> token here, so we extract it now\n        #out = roberta[:, 0, :]\n        \n        # using Avg pooling instead of the CLS token only\n        \n        out = GlobalAveragePooling1D()(roberta)\n\n\n        #two layers with mish activation\n        #out = tf.keras.layers.Dense(512, activation='mish')(out)\n        \n        #add optional Dense layer with dropout\n        #out = tf.keras.layers.Dropout(0.2)(out)\n        \n        #out = tf.keras.layers.Dense(256, activation='mish')(out)\n        \n        #out = Dropout(0.3)(out)\n                \n\n                #add our softmax layer\n        out = Dense(3, activation = 'softmax')(out)\n\n        #assemble model and compile\n\n\n        model = Model(inputs = [input_ids, input_masks], outputs = out)\n        model.compile(\n                                optimizer = Adam(lr = learning_rate), \n                                loss = 'sparse_categorical_crossentropy', \n                                metrics = ['accuracy'])\n    model.summary()\n        \n    return model  \n\n\nXlm = create_xlm(XLM_model ,123443334, 1e-5)","8f28fcce":"#STEPS_PER_EPOCH = int(train_df.shape[0] \/\/ batch_size)\n\nhistory_xlm = Xlm.fit([X_train_ids, X_train_masks], shuffled_data['label'],\n          batch_size = batch_size,\n        validation_split = 0.2,\n         epochs = 39, callbacks = callbacks)","55fc594e":"# preprocessing test data\n\ninput_ids_test_xml, input_masks_test_xml, _ = tokeniZer(test_df, xlm_tokenizer)\n#input_ids_test_bert, input_masks_test_bert, input_type_id_test = tokeniZer(test_df, Bert_tokenizer)\n#input_ids_test_xlm1, input_masks_test_xlm1, input_type_ids_test_xlm1 = tokeniZer(TTA1, xlm_tokenizer)\n#input_ids_test_xlm2, input_masks_test_xlm2, input_type_ids_test_xlm2 = tokeniZer(TTA2, xlm_tokenizer)\n#input_ids_test_xlm3, input_masks_test_xlm3, input_type_ids_test_xlm3 = tokeniZer(TTA3, xlm_tokenizer)\n#input_ids_test_xlm4, input_masks_test_xlm4, input_type_ids_test_xlm4 = tokeniZer(TTA4, xlm_tokenizer)\n\n#model predictions\n\npredictions_xlm = Xlm.predict([input_ids_test_xml, input_masks_test_xml])\n#predictions_bert = Bert.predict([input_ids_test_bert, input_masks_test_bert,input_type_id_test])\n#predictions_xlm1 = Xlm.predict([input_ids_test_xlm1, input_masks_test_xlm1, input_type_ids_test_xlm1])\n#predictions_xlm2 = Xlm.predict([input_ids_test_xlm2, input_masks_test_xlm2, input_type_ids_test_xlm2])\n#predictions_xlm3 = Xlm.predict([input_ids_test_xlm3, input_masks_test_xlm3, input_type_ids_test_xlm3])\n#predictions_xlm4 = Xlm.predict([input_ids_test_xlm4, input_masks_test_xlm4, input_type_ids_test_xlm4])\n\npredictions = predictions_xlm\n\n#final = np.zeros(predictions.shape[0])\n#for i in range(predictions.shape[0]):\nfinal = np.argmax(predictions, axis = 1)    \n\nsubmission = pd.DataFrame()    \n\nsubmission['id'] = test_df['id']\nsubmission['prediction'] = final.astype(np.int32)\n\nsubmission.to_csv('submission.csv', index = False)","fbdcaf31":"In this Notebook The XNLI dataset is used and the data comes from a CSV file that was prepared by the same author of this notebook, the prediction will be made with an ensemble XLM model  ","2bdb1da1":"Conclusions: the ensemble model shows clear improvement from the single bert model, also the extra Dense layer adds some improvement as well, a bigger ensemble might yield better results remains to be tested, also it is believed that more preporcessing might be keystone in improving the model's accuracy, any suggestions specially in the preprocessing stage are greatly welcomed and I hope it helps others like the author of this notebook who are begginers in this area","67d321f7":"The target column seems to be balanced","57b3a844":"The language column is clearly unbalanced being English the most frequent language","a24aba1b":"Turning the MNLI, XNLI and the SNLI datasets into a dataframe to be used in this notebook as part of the original data","d591acbc":"This is the function to create a single BERT model that takes as single input  random seed which is used in this case to create an esemble model using several BERT models with different seeds and shuffling the data","25ad036d":"Loading a pre-arranged version of the XNLI dataset in form a dataframe","b8dfbb3b":"Loading the Original train datset","8dc586a3":"From the notebook Watson :: XML-R & NLI :: train by novichok, the idea of using average pooling instead of just taking the CLS token from the XLM model ","fad889c0":"The following code is used to tokenize and preprocess the data for the Hugginface model, creating an array of ids, maks and type_id","a9b861ed":"premises are observed to be longer than the hypothesis","bd7cda30":"The following code will preprocess and create a XLM-RoBERTa Model "}}