{"cell_type":{"2f540c6b":"code","9232a891":"code","8fe0c91e":"code","c533b51a":"code","cceea9b4":"code","968ca7c5":"code","31cf2bc9":"code","f8155946":"code","27ce73bc":"code","690367b7":"code","30f8447d":"code","b03b4b1f":"code","d63a4b3b":"code","765c5823":"code","f5ee2311":"code","02467976":"code","9f381484":"code","4ce0a7ee":"code","91b39a48":"code","03633d43":"code","e96ed577":"code","964ebc06":"code","4d3f0d6d":"code","1d51dcb8":"code","842b7fa3":"code","5c265287":"code","15a6f66a":"code","3b406006":"code","ca71d2a5":"code","6e21be2d":"code","d9e3a0a6":"code","22317dd9":"code","2f2beb0f":"code","579fc6bc":"code","981303af":"code","c351d56a":"code","09a65a16":"code","524ec247":"code","636ba7e9":"markdown","0a70fea9":"markdown","542482ed":"markdown","5c301963":"markdown","7b85740e":"markdown","ce90a44b":"markdown","270da5f1":"markdown","4f8fda7d":"markdown","c0bb44ba":"markdown","ac16023b":"markdown","e305021b":"markdown","b39347e7":"markdown","858f443a":"markdown","5556835b":"markdown","35e62edf":"markdown","21b6a8af":"markdown"},"source":{"2f540c6b":"import zipfile\nwith zipfile.ZipFile(\"..\/input\/aerial-cactus-identification\/train.zip\",\"r\") as z:\n    z.extractall()","9232a891":"import pandas as pd\ntrain=pd.read_csv(\"\/kaggle\/input\/aerial-cactus-identification\/train.csv\")","8fe0c91e":"train.head()","c533b51a":"cd train","cceea9b4":"has_cactus=train['has_cactus'].value_counts()\nprint(has_cactus)","968ca7c5":"import tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Flatten,Dense,Conv2D,MaxPooling2D,ZeroPadding2D,Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","31cf2bc9":"datagen=ImageDataGenerator(rescale=1\/255,validation_split=0.30)","f8155946":"train[\"has_cactus\"]= train[\"has_cactus\"].apply(str)","27ce73bc":"bs = 64\ntrain_generator = datagen.flow_from_dataframe(\n    dataframe = train,\n    x_col = \"id\",\n    y_col = \"has_cactus\",\n    subset = \"training\",\n    batch_size = bs,\n    target_size=(32,32),\n    #seed = 1,\n    shuffle = True,\n    class_mode = \"categorical\")\n\nvalid_generator = datagen.flow_from_dataframe(\n    dataframe = train,\n    x_col = \"id\",\n    y_col = \"has_cactus\",\n    subset = \"validation\",\n    target_size=(32,32),\n    batch_size = bs,\n    shuffle = True,\n    class_mode = \"categorical\")","690367b7":"#mini-van\n!wget https:\/\/upload.wikimedia.org\/wikipedia\/commons\/b\/b4\/Bloody_Bridge_car_park%2C_May_2010_%2806%29.JPG\n#Taxi\n!wget https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/ef\/NYC_Taxi_Ford_Crown_Victoria.jpg\n#pug dog\n!wget https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/f0\/Mops_oct09_cropped2.jpg","30f8447d":"from keras.applications.vgg19 import VGG19\n# load the model\nmodel = VGG19(weights= 'imagenet',include_top=True,input_shape=(224,224,3))\nmodel.summary()","b03b4b1f":"from keras.preprocessing.image import img_to_array\nfrom keras.preprocessing.image import load_img\nimport numpy as np\nfrom keras.applications.vgg16 import preprocess_input\nfrom tensorflow.keras.applications.mobilenet import decode_predictions\ndef make_classification(image):\n    pred = model.predict(image)\n    pred_classes = decode_predictions(pred, top=5)\n    for i in pred_classes[0]:\n        print(i)\n\n\ndef preprocess(image_name):\n    image = load_img(image_name, target_size=(224, 224))\n    image = img_to_array(image)\n    image = np.expand_dims(image, axis=0)\n    image = preprocess_input(image)\n    return image","d63a4b3b":"mini_bus='Bloody_Bridge_car_park,_May_2010_(06).JPG'\npubg_dogs='Mops_oct09_cropped2.jpg'\ntaxi='NYC_Taxi_Ford_Crown_Victoria.jpg'\n\n#classifing mini_van \nprint(\"for mini bus\")\nimage=preprocess(mini_bus)\nmake_classification(image)\nprint(\"\\n\")\nprint(\"\\n\")\n#classifing pubg_dogs \nprint(\"for pubg dogs\")\nimage=preprocess(pubg_dogs)\nmake_classification(image)\nprint(\"\\n\")\nprint(\"\\n\")\n#classifing taxi \nprint(\"for taxi\")\nimage=preprocess(taxi)\nmake_classification(image)\nprint(\"\\n\")\nprint(\"\\n\")","765c5823":"from tensorflow.keras.applications.vgg19 import VGG19","f5ee2311":"model = VGG19(weights= 'imagenet',include_top=False,input_shape=(32,32,3))","02467976":"#to freeze some layer\nfor layer in model.layers[:]:\n    layer.trainable = False","9f381484":"for layer in model.layers:\n    sp=' '[len(layer.name):]\n    print(layer.name,sp,layer.trainable)","4ce0a7ee":"from tensorflow.keras.models import Model\nx = Flatten()(model.output)\n\nprediction = Dense(2, activation='softmax')(x)\n\n# create a model object\nmodel = Model(inputs=model.input, outputs=prediction)","91b39a48":"model.summary()","03633d43":"# compiling the model\nmodel.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])","e96ed577":"history = model.fit_generator(train_generator,\n                         steps_per_epoch = len(train_generator),\n                         epochs = 5,\n                         validation_data = valid_generator,\n                         validation_steps = len(valid_generator),\n                         verbose=1)","964ebc06":"import matplotlib.pyplot as plt\ndef plot_learningCurve(history, epoch):\n    # Plot training & validation accuracy values\n    epoch_range = range(1, epoch+1)\n    plt.plot(epoch_range, history.history['accuracy'])\n    plt.plot(epoch_range, history.history['val_accuracy'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Val'], loc='upper left')\n    plt.show()\n\n    # Plot training & validation loss values\n    plt.plot(epoch_range, history.history['loss'])\n    plt.plot(epoch_range, history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Val'], loc='upper left')\n    plt.show()\n\nplot_learningCurve(history, 5)","4d3f0d6d":"from tensorflow.keras.applications.vgg19 import VGG19","1d51dcb8":"model = VGG19(weights= 'imagenet',include_top=False,input_shape=(32,32,3))","842b7fa3":"#to freeze some layer\nfor layer in model.layers[:8]:\n    layer.trainable = False","5c265287":"for layer in model.layers:\n    sp=' '[len(layer.name):]\n    print(layer.name,sp,layer.trainable)","15a6f66a":"\nfrom tensorflow.keras.models import Model\nx = Flatten()(model.output)\n\nprediction = Dense(2, activation='softmax')(x)\n\n# create a model object\nmodel = Model(inputs=model.input, outputs=prediction)","3b406006":"model.summary()","ca71d2a5":"# compiling the model\nmodel.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])","6e21be2d":"history = model.fit_generator(train_generator,\n                         steps_per_epoch = len(train_generator),\n                         epochs = 5,\n                         validation_data = valid_generator,\n                         validation_steps = len(valid_generator),\n                         verbose=1)","d9e3a0a6":"import matplotlib.pyplot as plt\ndef plot_learningCurve(history, epoch):\n    # Plot training & validation accuracy values\n    epoch_range = range(1, epoch+1)\n    plt.plot(epoch_range, history.history['accuracy'])\n    plt.plot(epoch_range, history.history['val_accuracy'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Val'], loc='upper left')\n    plt.show()\n\n    # Plot training & validation loss values\n    plt.plot(epoch_range, history.history['loss'])\n    plt.plot(epoch_range, history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Val'], loc='upper left')\n    plt.show()\n\nplot_learningCurve(history, 5)","22317dd9":"from tensorflow.keras.applications.vgg19 import VGG19","2f2beb0f":"model = VGG19(weights= None,include_top=True,input_shape=(32,32,3),classes=2,classifier_activation=\"softmax\")","579fc6bc":"for layer in model.layers:\n    sp=' '[len(layer.name):]\n    print(layer.name,sp,layer.trainable)","981303af":"model.summary()","c351d56a":"# compiling the model\nmodel.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])","09a65a16":"history = model.fit_generator(train_generator,\n                         steps_per_epoch = len(train_generator),\n                         epochs = 5,\n                         validation_data = valid_generator,\n                         validation_steps = len(valid_generator),\n                         verbose=1)","524ec247":"import matplotlib.pyplot as plt\ndef plot_learningCurve(history, epoch):\n    # Plot training & validation accuracy values\n    epoch_range = range(1, epoch+1)\n    plt.plot(epoch_range, history.history['accuracy'])\n    plt.plot(epoch_range, history.history['val_accuracy'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Val'], loc='upper left')\n    plt.show()\n\n    # Plot training & validation loss values\n    plt.plot(epoch_range, history.history['loss'])\n    plt.plot(epoch_range, history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Val'], loc='upper left')\n    plt.show()\n\nplot_learningCurve(history, 5)","636ba7e9":"## making layer.trainable = False or Freeze some layers\n<p style=\"font-size:120%;\">\nHere we include_top=False so we only have convolution layer and we are freezing the starting 8 layers by making making layer.trainable = 'False'<br>\n<b>Code:<\/b>\n<\/p>    \n\n![Screenshot 2021-07-30 130005.png](attachment:f421297c-1b0f-4c21-beb7-73b63c20b5ce.png)\n","0a70fea9":"### Data Preparation","542482ed":"<p style=\"font-size:120%;\">\nHere layer_n='False' mean you don't want to train that layer you can also say we have freeze that layer <br>\nwhen layer_n='True' mean that you want to train that layer <br>\nAs include_top= 'False' then we only have convolution layer and we have freeze those layer<br>\n<\/p>\n\n![Screenshot 2021-07-28 231409.png](attachment:97964c61-7f64-419b-ae2b-1b4b1fb2a091.png)","5c301963":"# 2.Using Pretrained Model\n\n<p style=\"font-size:120%;\">\nIn pretrained model we use imagenet weight completely without any fine tuning of model.<b>Problem<\/b> with this approach is that we can only classify or perform task based upon the classes in which pretrained model weights are train <br>\n    imagenet 1000 classes-<a href=\"https:\/\/gist.github.com\/yrevar\/942d3a0ac09ec9e5eb3a\">https:\/\/gist.github.com\/yrevar\/942d3a0ac09ec9e5eb3a<\/a>\n<\/p>","7b85740e":"# 1.Introduction\n\n## 1.1 What is Transfer Learning?\n\n<p style=\"font-size:120%;\">\n<i>Transfer Learning is like Standing on the shoulders of giants<\/i>\n<\/p>\n\n<p style=\"font-size:120%;\">\nIn Transfer Learning we use the pretrained knowledge or weights and we tweak or fine tuning model on our dataset<br>\n<\/p>\n\n<p style=\"font-size:120%;\">\nTransfer Learning is a way of actually taking what work has been done and transferring that into a different domain <br>\nFor example-<b>Imagenet<\/b> contain regular images of cats ,dogs,buses and lot of everday objects.We train the model on these images and use that knowledge in malaria parasite detection or any other task<br>\n<\/p>\n<p style=\"font-size:120%;\">\n<i>Transfer Learning is quite popular in Computer vision and NLP task.In NLP we can use transformer based model Like BERT<br>\nIn this tutorial we will focus on computer vision<\/i><br>\n<\/p>\n<p style=\"font-size:120%;\">\n<b>ImageNet<\/b>-ImageNet is a image classification problem which contain  More than 14 million images and 1000 classes<br><br>\n  <\/p>\n\n## 1.2. Need of Transfer Learning?\n <p style=\"font-size:120%;\">\n1.If our dataset is really small<br>\n2.Low Computation Power<br>\n3.If our dataset is similar to pretrained data then we have to only fine tuning our model it would save lot of time<br>\n<\/p>\n\n## 1.3. Limitations\n\n <p style=\"font-size:120%;\">\n 1.Dataset is completely different from pretrained data\n  <\/p>","ce90a44b":"# 5. Use only model architecture and training from Scratch\n\n## Important model Arguments\n\n<p style=\"font-size:120%;\">\n1.<b>weights<\/b>-As we are training our model from scratch so we will set weights='None'.<br><br>\n2.<b>include_top<\/b>-Whether to include the 3 fully-connected layers at the top of the network or not.\nhere we have set include_top=True it will add fully-connected layers.if you want you can set include_top=False then you have define output layer and dense layer\n<br><br>\n3.<b>input_shape<\/b>-Input size of images as we set include_top='False' or weights= 'None' then we train on any image size for vgg19 image size should not be less than 32x32.<br><br>\n4.<b>classes<\/b>=2<br><br>\n4.<b>classifier_activation<\/b>=softmax<br><br>\n<\/p>\n","270da5f1":"![0_xNjEPIZmPvKeqss6.png](attachment:5743838b-44cd-42f3-b31a-86db43f3526f.png)","4f8fda7d":"## making layer trainable = False or Freezing convolution layers\n","c0bb44ba":"<p style=\"font-size:120%;\">\nHere layer_n='False' mean you don't want to train that layer you can also say we have freeze that layer <br>\nwhen layer_n='True' mean that you want to train that layer <br>\nAs include_top= 'False' then we only have convolution layer and we have freeze starting 8 layer<br>\n<\/p>\n\n![Screenshot 2021-07-28 002915.png](attachment:b4691e3f-2824-4743-8e75-077752f423b1.png)","ac16023b":"# Table of content\n\n<ul style=\"font-size:120%;\">\n    <li style=\"font-size:120%;\"><a href=\"https:\/\/www.kaggle.com\/rahulanand0070\/all-you-need-to-know-about-transfer-learning-cnn?scriptVersionId=69464942&cellId=4\">1. Introduction<\/a><\/li>\n    <ul>\n   <li style=\"font-size:120%;\">1.1. What is Transfer Learning? <\/li>\n    <li style=\"font-size:120%;\"> 1.2. Need of Transfer Learning <\/li>\n    <li style=\"font-size:120%;\"> 1.3. Limitations <\/li>\n    <\/ul>\n<li style=\"font-size:120%;\">2. Use Pretrained Model<\/li>\n<li style=\"font-size:120%;\">3. Fine tuning Last layer<\/li>\n<li style=\"font-size:120%;\">4. Freezing some layer and Fine tuning rest of model's layer<\/li>\n<li style=\"font-size:120%;\">5. Use only model architecture and learn weight from scratch<\/li>\n<\/ul>","e305021b":"## Important model Arguments\n\n<p style=\"font-size:120%;\">\n1.<b>weights<\/b>-As we are using pretrained model here, we have set weight='imagenet' if we set weight='None' then we cannot use pretrained weight.<br><br>\n2.<b>include_top<\/b>-whether to include the 3 fully-connected layers at the top of the network or not.As we want to do classification without training so we have to set include_top='True'.If we set include_top='False' then we have to add dense layer and output layer for pretrained model this is not required in pretrained we will see include_top='False' in Fine tuning Last layer & Freeze some layer and  Fine tuning the rest.<br><br>\n3.<b>input_shape<\/b>-input size of images as we have set include_top='True' and weights= 'imagenet' then input_shape must be '(224,244,3)'.if weights='imagenet' and include_top='True' then we cannot train or classify on custom images shape.If you want to train on custom image size then make sure weights is not equal to 'imagenet' and include_top is not equal to 'True'.Either weight has to be None or include_top='False' <br><br>\n4.As we set include_top='True' then classes='1000' and classifier_activation='softmax' <br><br>\n<\/p>","b39347e7":"### downloading images from wikipedia for classification\n <p style=\"font-size:120%;\">\n1.Mini Van <br>\n2.Taxi <br>\n3.Pug dog\n<\/p>","858f443a":"<p style=\"font-size:120%;\">\n<b>Code:<\/b>\n<\/p>\n\n![Screenshot 2021-07-30 143334.png](attachment:bed33612-fcf8-47cb-9558-6b7cc651a947.png)\n\nthis will make every layer non trainable","5556835b":"# 4.Freezing some layer and Fine tuning rest of model's layer\n\n<p style=\"font-size:120%;\">\nIn this we will freeze some of layer and fine tune rest of layers.In starting layer model learn about edges and roundness. \n<\/p>\n\n## Important model Arguments\n<p style=\"font-size:120%;\">\n1.<b>weights<\/b>-As we are fine tuning our model here we will set weight='imagenet' if we set weight='None' then we cannot use weight which is trained on imagenet.<br><br>\n2.<b>include_top<\/b>-Whether to include the 3 fully-connected layers at the top of the network not.here we are Fine tuning our model's last layer on our custom dataset so we need to add output layer based upon number of classes in our dataset here we will set include_top='False'\n<br><br>\n3.<b>input_shape<\/b>-As we have set include_top='False' then we train on any image size for vgg19 image size should not be less than 32x32.<br><br>\n4.As we have set include_top='False'so no need to pass number of classes here we will pass number classes in dense layer and same for classifier_activation<br><br>\n<\/p>","35e62edf":"# End Notes\n<p style=\"font-size:120%;\">\n<b>1.<\/b>In this notebook we have use VGG19 but you can use any model procedure is same for very model <a href=\"https:\/\/keras.io\/api\/applications\/\">https:\/\/keras.io\/api\/applications\/<\/a><br>\n<b>2.input_shape<\/b>-input size of images as we set include_top='True' and weights= 'imagenet' and then input_shape must be '(224,244,3)'.if weights= 'imagenet' and include_top='True' then we cannot train custom images shape.If you want to train on custom image size then make sure weights is not equal to 'imagenet' and include_top is not equal to 'True'.weight has to be None or include_top='False<br> \n![Screenshot 2021-07-28 235536.png](attachment:28c628d3-e251-4468-8d04-e4084e49a1ce.png)\n<b>3.<\/b>If layer_n='False' mean you don't want to train that layer you can also say we have freeze these layers <br>\nwhen layer_n='True' mean you we want the train that layer <br>\n    <b>Code:<\/b>\n\n![Screenshot 2021-07-30 130005.png](attachment:38ead782-a6ef-475e-99f2-282312c36df8.png)\n<\/p>\nAfter making some freezing some layer and making some layer trainable\n\n![Screenshot 2021-07-28 002915.png](attachment:334669c4-cfd8-4d2e-976e-ce5f948ab9ea.png)\n","21b6a8af":"# 3.Fine tuning Last layer\n<p style=\"font-size:120%;\">\nIn this we will use imagenet weight for all convolution layer and add output layer then we will train the last layer\n<\/p>\n\n## Important model Arguments\n<p style=\"font-size:120%;\">\n1.<b>weights<\/b>-As we are fine tuning our model here we will set weight='imagenet' if we set weight='None' then we cannot use weight which is trained on imagenet.<br><br>\n2.<b>include_top<\/b>-Whether to include the 3 fully-connected layers at the top of the network not.here we are Fine tuning our model's last layer on our custom dataset so we need to add output layer based upon number of classes in our dataset here we will set include_top='False'\n<br><br>\n3.<b>input_shape<\/b>-As we have set include_top='False' then we train on any image size for vgg19 image size should not be less than 32x32.<br><br>\n4.As we have set include_top='False'so no need to pass number of classes here we will pass number classes in dense layer and same for classifier_activation<br><br>\n<\/p>"}}