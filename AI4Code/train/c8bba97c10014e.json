{"cell_type":{"808cc2a5":"code","55d4623c":"code","8b5ce1af":"code","e344cd1a":"code","f248ca7a":"code","54441085":"code","ca92baa4":"code","ab09f5c4":"code","067766b6":"code","0f04f3ba":"code","84c24818":"code","4662bef7":"code","763859ea":"code","3a2f9086":"code","e4f0c86d":"code","fd26838b":"code","dbe14a13":"code","58de5dfc":"code","254473a3":"code","477271d6":"code","dcb4b096":"code","2e1f3f2a":"code","56505dba":"code","d2791d58":"code","80e0c041":"code","72bd442a":"code","309883ea":"code","34fcbe16":"code","ed3fde39":"code","18b3fcef":"code","25ef4602":"code","f6f6caee":"code","94e047fa":"code","6f252cb8":"code","dfc092df":"code","bbafbee1":"markdown","ba4865f4":"markdown","f0f38b9e":"markdown","345571c7":"markdown","38120204":"markdown","925e9141":"markdown","c4e12305":"markdown","537fbc27":"markdown","038c182f":"markdown","6f3d1e4b":"markdown","41a3bb82":"markdown","fe204080":"markdown","239dcc6d":"markdown","c456cd3f":"markdown"},"source":{"808cc2a5":"#Importing essential packages and modules\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\nimport warnings\nwarnings.simplefilter('ignore')","55d4623c":"#Load dataset using pandas\nbrstcancer_data = pd.read_csv('..\/input\/breast-cancer-dataset\/Breast Cancer Data.csv', index_col=False,)","8b5ce1af":"#To print first few values\nbrstcancer_data.drop('Unnamed: 0', axis=1, inplace=True)\nbrstcancer_data.head()","e344cd1a":"#Print shape of the dataset\nbrstcancer_data.shape","f248ca7a":"#Check for any missing values\nbrstcancer_data.isnull().values.any() ","54441085":"#Checking for missing value counts for each variable\nbrstcancer_data.isnull().sum()","ca92baa4":"#Checking overall for missing values\nbrstcancer_data.isnull().sum().sum() #returns overall sum","ab09f5c4":"#looking at summary using describe\nbrstcancer_data.describe()","067766b6":"#frequency table\npd.crosstab(index = brstcancer_data['diagnosis'], columns = 'count')","0f04f3ba":"type(brstcancer_data)","84c24818":"brstcancer_data.describe().unstack()","4662bef7":"# Using Violin Plot to check Malignant and Benign cancer Data\nimport seaborn as sns\ndata_dia = brstcancer_data['diagnosis']\ndata = brstcancer_data.drop('diagnosis',axis=1)\ndata_n_2 = (data - data.mean()) \/ (data.std())              # standardization\ndata = pd.concat([brstcancer_data['diagnosis'],data_n_2.iloc[:,0:15]],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(14,5))\nsns.violinplot(x=\"features\", y=\"value\", hue=\"diagnosis\", data=data,split=True, inner=\"quart\")\nplt.xticks(rotation=45,fontsize=13)","763859ea":"from sklearn.model_selection import train_test_split","3a2f9086":"X = brstcancer_data.values[:, 1:31]\nY = brstcancer_data.values[:,0]\n\n#transform the class labels from their original string representation (M and B) into integers\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nY = le.fit_transform(Y)\n#we had to do this because strings won't work","e4f0c86d":"x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.3, random_state = 12)\naccuracy = []\n#names=[]","fd26838b":"#Initializing the model\nmodel1 = RandomForestClassifier()","dbe14a13":"#Fitting the model for the train data\nmodel1.fit(x_train, y_train)","58de5dfc":"#Predicting for the unseen\/test data\npredict1 = model1.predict(x_test)","254473a3":"#Calculating the accuracy of the model\nacc1=accuracy_score(y_test, predict1)\naccuracy.append(int(acc1*100))\nacc1","477271d6":"#Initializing the model\nmodel2 = KNeighborsClassifier()","dcb4b096":"#Fitting the model for the train data\nmodel2.fit(x_train, y_train)","2e1f3f2a":"#Predicting for the unseen\/test data\npredict2 = model2.predict(x_test)","56505dba":"#Calculating the accuracy of the model\nacc2=accuracy_score(y_test, predict2)\naccuracy.append(int(acc2*100))\nacc2","d2791d58":"#Initializing the model\nmodel3 = SVC()","80e0c041":"#Fitting the model for the train data\nmodel3.fit(x_train, y_train)","72bd442a":"#Predicting for the unseen\/test data\npredict3 = model3.predict(x_test)","309883ea":"#Calculating the accuracy of the model\nacc3=accuracy_score(y_test, predict3)\naccuracy.append(int(acc3*100))\nacc3","34fcbe16":"from sklearn.model_selection import cross_val_score, KFold\n# Spot-Check Algorithms\nmodels = []\nmodels.append(( 'LR' , LogisticRegression()))\nmodels.append(( 'KNN' , KNeighborsClassifier()))\nmodels.append(( 'RF' , RandomForestClassifier()))\nmodels.append(( 'SVM' , SVC()))\n\n# Test options and evaluation metric\nnum_folds = 10\nnum_instances = len(x_train)\nseed = 7 \nscoring =  'accuracy'\n\n# Test options and evaluation metric\nnum_folds = 10\nnum_instances = len(x_train)\nseed = 7 \nscoring =  'accuracy'\nresults = []\nnames = []\nfor name, model in models:\n #kfold = KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n kfold = KFold(n_splits=4, random_state=seed, shuffle=False)\n cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring=scoring)\n print(\"accuracies for\",name)\n print(cv_results)\n results.append(cv_results)\n names.append(name)\n msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n print(msg)\nprint('-> 10-Fold cross-validation accurcay score for the training data for four classifiers') ","ed3fde39":"import matplotlib.pyplot as pylt \nIndex = [1,2,3,4]\nname=[\"Iteration 0\",\"Iteration 1\",\"Interation 2\",\"Iteration 3\"]\n#accuracy = [10,88,96,66]\npylt.plot(Index,results[0])\npylt.xticks(Index, name, rotation=45)\n#plt.yticks([0.0,0.5,0.10,0.15,0.20,0.25,0.30,0.35,0.40,0.45,0.50,0.55,0.60,0.65,0.70,0.75,0.80,0.85,0.90,0.95,1])\npylt.yticks(np.arange(0.9, 1.2, 0.1)) \npylt.ylabel('Accuracy')\npylt.xlabel('Model')\npylt.title('Comparison of accuracy for each Iteration in LR')","18b3fcef":"import matplotlib.pyplot as pylt \nIndex = [1,2,3,4]\nname=[\"Iteration 0\",\"Iteration 1\",\"Interation 2\",\"Iteration 3\"]\n#accuracy = [10,88,96,66]\npylt.plot(Index,results[1])\npylt.xticks(Index, name, rotation=45)\n#plt.yticks([0.0,0.5,0.10,0.15,0.20,0.25,0.30,0.35,0.40,0.45,0.50,0.55,0.60,0.65,0.70,0.75,0.80,0.85,0.90,0.95,1])\npylt.yticks(np.arange(0.8, 1.2, 0.1)) \npylt.ylabel('Accuracy')\npylt.xlabel('Model')\npylt.title('Comparison of accuracy for each Iteration inKNN')","25ef4602":"import matplotlib.pyplot as pylt \nIndex = [1,2,3,4]\nname=[\"Iteration 0\",\"Iteration 1\",\"Interation 2\",\"Iteration 3\"]\n#accuracy = [10,88,96,66]\npylt.plot(Index,results[2])\npylt.xticks(Index, name, rotation=45)\n#plt.yticks([0.0,0.5,0.10,0.15,0.20,0.25,0.30,0.35,0.40,0.45,0.50,0.55,0.60,0.65,0.70,0.75,0.80,0.85,0.90,0.95,1])\npylt.yticks(np.arange(0.9, 1.2, 0.1)) \npylt.ylabel('Accuracy')\npylt.xlabel('Model')\npylt.title('Comparison of accuracy for each Iteration in RF')","f6f6caee":"import matplotlib.pyplot as pylt \nIndex = [1,2,3,4]\nname=[\"Iteration 0\",\"Iteration 1\",\"Interation 2\",\"Iteration 3\"]\n#accuracy = [10,88,96,66]\npylt.plot(Index,results[3])\npylt.xticks(Index, name, rotation=45)\n#plt.yticks([0.0,0.5,0.10,0.15,0.20,0.25,0.30,0.35,0.40,0.45,0.50,0.55,0.60,0.65,0.70,0.75,0.80,0.85,0.90,0.95,1])\npylt.yticks(np.arange(0.5, 0.9, 0.1)) \npylt.ylabel('Accuracy')\npylt.xlabel('Model')\npylt.title('Comparison of accuracy for each Iteration in SVM')","94e047fa":"%matplotlib inline\nimport matplotlib.pyplot as plt\n# Compare Algorithms\nfig = plt.figure()\nfig.suptitle( 'Algorithm Comparison' )\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","6f252cb8":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n# Standardize the dataset\npipelines = []\npipelines.append(( 'ScaledLR' , Pipeline([( 'Scaler' , StandardScaler()),( 'LR' ,\n    LogisticRegression())])))\n\npipelines.append(( 'ScaledKNN' , Pipeline([( 'Scaler' , StandardScaler()),( 'KNN' ,\n    KNeighborsClassifier())])))\n\npipelines.append(( 'ScaledRF' , Pipeline([( 'Scaler' , StandardScaler()),( 'RF' ,\n    RandomForestClassifier())])))\npipelines.append(( 'ScaledSVM' , Pipeline([( 'Scaler' , StandardScaler()),( 'SVM' , SVC())])))\n\nresults = []\nnames = []\nfor name, model in pipelines:\n  #kfold = KFold(n=num_instances, n_folds=num_folds, random_state=seed)\n  kfold = KFold(n_splits=4, random_state=seed, shuffle=False)\n  cv_results = cross_val_score(model, x_train, y_train, cv=kfold,\n      scoring=scoring)\n  results.append(cv_results)\n  names.append(name)\n  msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n  print(msg)","dfc092df":"# Compare Algorithms\nfig = plt.figure()\nfig.suptitle( 'Scaled Algorithm Comparison' )\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","bbafbee1":"# Data Cleaning and preprocessing ","ba4865f4":"Observation\nThe results show a similar distribution for all classifiers except SVM which is suggesting low variance.\n\nIt is possible the varied distribution of the attributes may have an effect on the accuracy of algorithms such as SVM. We will repeat the above code with a standardized copy of the training dataset.","f0f38b9e":"# Comparison","345571c7":"## Project summary:\n- The objective of the project was to build various models and compare their prediction performance based on accuracy.\n\n","38120204":"# Using preloaded dataset in sklearn","925e9141":"# SVM \n","c4e12305":"# standardised data","537fbc27":"# Conclusion","038c182f":"# Random Forest","6f3d1e4b":"# KNN","41a3bb82":"## Splitting the dataset into train and test","fe204080":"The results show that standardization of the data has lifted the skill of SVM to be the most accurate algorithm tested so far.","239dcc6d":"# Violin Plot of features","c456cd3f":"Model trained on Logistic Regression performed the best in classifying the dataset. The accuracy was found to be 94.15 %"}}