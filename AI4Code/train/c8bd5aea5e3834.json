{"cell_type":{"145dbbfb":"code","4a1fd441":"code","3889ae78":"code","03a33407":"code","204c9a28":"code","e2928341":"code","1ade7a5f":"code","c2702999":"code","15df980d":"code","c8f275bf":"code","350cf30c":"code","4e4fc963":"code","dc4180bf":"code","22489576":"code","c8d73d2d":"code","4efd4000":"markdown"},"source":{"145dbbfb":"!pip install -q -U albumentations","4a1fd441":"import os\nimport numpy as np\nimport cv2\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport random\nfrom tensorflow.keras.layers import Input,Reshape, Lambda, Conv2D, DepthwiseConv2D, BatchNormalization, Activation, MaxPool2D, UpSampling2D,Dropout, Concatenate, Conv2DTranspose, dot, add,GlobalAveragePooling2D, Multiply, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\nfrom keras import backend as K\nfrom keras.regularizers import l2\nfrom albumentations import (\n    Compose, RandomBrightness, JpegCompression, HueSaturationValue, RandomContrast, HorizontalFlip, VerticalFlip,\n    Rotate,GridDistortion,ElasticTransform,ShiftScaleRotate\n)\n","3889ae78":"# helper function for data visualization\ndef visualize(**images):\n    \"\"\"PLot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(9, 9))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image, cmap = 'bone')\n    plt.show()","03a33407":"transforms = Compose([\n        ElasticTransform(p=0.3, alpha=120, sigma=120 * 0.05, alpha_affine=120 * 0.03),\n        GridDistortion(p=0.3),\n        HorizontalFlip(p=0.15),\n        VerticalFlip(p=0.15),\n        ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.1),\n#         OpticalDistortion(distort_limit=1, shift_limit=0.5, p=1),\n#         VerticalFlip(p=0.5),\n        \n])","204c9a28":"def load_data(x_path, y_path):\n    images = os.listdir(x_path)\n    masks = os.listdir(y_path)\n    \n    train_x = [os.path.join(x_path, image) for image in images]\n#     train_y = [os.path.join(y_path, mask) for mask in masks]\n    \n    train_y = list(map(lambda x : x.replace('.jpg', '.tiff'), train_x))\n    train_y = list(map(lambda x : x.replace('train_images\/train_images', 'train_masks\/train_masks'), train_y))\n    \n    \n    train_x, valid_x = train_test_split(train_x, test_size=0.15, random_state=42)\n    train_y, valid_y = train_test_split(train_y, test_size=0.15, random_state=42)\n\n    return (train_x, train_y), (valid_x, valid_y)\n\n\ndef read_image(x):\n    x = cv2.imread(x, cv2.IMREAD_COLOR)\n    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n    x = x \/ 255.0\n    x = x.astype(np.float32)\n    return x\n\n\ndef read_mask(x):\n    x = cv2.imread(x, cv2.IMREAD_GRAYSCALE)\n    x = x.astype(np.int32)\n    return x\n\n\ndef tf_dataset(x, y, batch=8):\n    dataset = tf.data.Dataset.from_tensor_slices((x,y))\n    dataset = dataset.shuffle(buffer_size= 5000)\n    dataset = dataset.map(preprocess)\n    dataset = dataset.batch(batch)\n    dataset = dataset.repeat()\n    dataset = dataset.prefetch(2)\n    return dataset\n\n\ndef preprocess(x,y):\n    \n    def aug_fn(image, mask):\n        img_data = {\"image\":image}\n        mask_data = {\"mask\":mask}\n        img_data = transforms(**image)\n        mask_data = transforms(**mask)\n        aug_img = img_data[\"image\"]\n        aug_mask = mask_data[\"mask\"]\n        return aug_img, aug_mask\n    \n    def f(x,y):\n        x = x.decode()\n        y = y.decode()\n        \n        image = read_image(x)\n        mask = read_mask(y)\n        augmented = transforms(image=image,mask=mask)\n        aug_img=augmented['image']\n        aug_mask = augmented['mask']\n        \n        return aug_img, aug_mask\n    \n    image, mask = tf.numpy_function(f, [x,y], [tf.float32, tf.int32])\n#     image, mask = tf.numpy_function(aug_fn,[image, mask],[tf.float32, tf.int32])\n    mask = tf.one_hot(mask, 3, dtype = tf.int32)\n    image.set_shape([256,256,3])\n    mask.set_shape([256,256,3])\n    \n    return image, mask","e2928341":"\ndef SqueezeExcite(x, ratio=16):\n    nb_chan = K.int_shape(x)[-1]\n\n    y = GlobalAveragePooling2D()(x)\n    y = Dense(nb_chan \/\/ ratio, activation='relu')(y)\n    y = Dense(nb_chan, activation='sigmoid')(y)\n\n    y = Multiply()([x, y])\n    return y\n\n\ndef res_block(inputs, filters, pool=True):\n    x = Conv2D(filters, 3, padding=\"same\", activation='relu')(inputs)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    res = x\n    x = Conv2D(filters, 3, padding=\"same\")(x)\n    x = BatchNormalization()(x)\n    x = add([res, x])\n    x = Activation(\"relu\")(x)\n\n    if pool == True:\n        p = MaxPool2D((2, 2))(x)\n        return x, p\n    else:\n        return x\n\n    \ndef single_conv_block(inputs, filters, kernel_size, strides=(1,1), padding='same', dilation_rate=(1,1),\n                   kernel_initializer='he_normal', kernel_regularizer=l2(1e-5)):\n    \n    x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, dilation_rate=dilation_rate,\n               kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer)(inputs)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    return x\n\n\ndef se_single_conv_block(inputs, filters, kernel_size, strides=(1,1), padding='same', dilation_rate=(1,1),\n                   kernel_initializer='he_normal', kernel_regularizer=l2(1e-5)):\n    \n    x = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding=padding, dilation_rate=dilation_rate,\n               kernel_initializer=kernel_initializer, kernel_regularizer=kernel_regularizer)(inputs)\n    x = SqueezeExcite(x, ratio=16)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    return x\n\ndef double_conv_block(inputs, filters, pool=True):\n\n    x = single_conv_block(inputs, filters=filters, kernel_size=(3,3))\n    x = single_conv_block(x, filters, kernel_size=(3,3))\n    \n    if pool == True:\n        p = MaxPool2D((2, 2))(x)\n        return x, p\n    else:\n        return x\n\ndef se_double_conv_block(inputs, filters, pool=True):\n\n    x = se_single_conv_block(inputs, filters=filters, kernel_size=(3,3))\n    x = se_single_conv_block(x, filters, kernel_size=(3,3))\n    \n    if pool == True:\n        p = MaxPool2D((2, 2))(x)\n        return x, p\n    else:\n        return x\n\n\ndef depth_conv_bn_relu(inputs, filters, kernel_size=(3,3), strides=(1, 1), padding='same', dilation_rate=(1, 1),\n                   initializer='he_normal', regularizer=l2(1e-5), pool=True):\n    x = DepthwiseConv2D(kernel_size=kernel_size, strides=strides, dilation_rate=dilation_rate, padding=padding,\n                        depthwise_initializer=initializer, use_bias=False, depthwise_regularizer=regularizer)(inputs)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    x = Conv2D(filters=filters, kernel_size=(1, 1), strides=(1, 1), dilation_rate=(1, 1), padding=padding,\n               kernel_initializer=initializer, kernel_regularizer=regularizer)(x)\n    x = BatchNormalization()(x)\n    x = Activation(\"relu\")(x)\n    return x\n\n\ndef res_triple_depth_conv_bn_relu(x, channels, pool):\n    res = single_conv_block(x, filters=channels, kernel_size=(1,1))\n    x = depth_conv_bn_relu(x, filters=channels, kernel_size=(3, 3))\n    x = depth_conv_bn_relu(x, filters=channels, kernel_size=(3, 3))\n    x = depth_conv_bn_relu(x, filters=channels, kernel_size=(3, 3))\n    x = add([x, res])\n    \n    if pool == True:\n        p = MaxPool2D((2, 2))(x)\n        return x, p\n    \n    else:\n        return x\n\n    \n    \ndef fsm(inputs):\n    channel_num = inputs.shape[-1]\n\n    res = inputs\n\n    inputs = single_conv_block(inputs, filters=int(channel_num \/\/ 2), kernel_size=(3,3)) #previously\n\n#     inputs = single_conv_block(inputs, filters=int(channel_num))\n\n    # x = non_local_block(x, compression=2, mode='dot')\n\n    ip = inputs\n    ip_shape = K.int_shape(ip)\n    batchsize, dim1, dim2, channels = ip_shape\n    intermediate_dim = channels \/\/ 2\n    rank = 4\n    if intermediate_dim < 1:\n        intermediate_dim = 1\n\n    # theta path\n    theta = Conv2D(intermediate_dim, (1, 1), padding='same', use_bias=False, kernel_initializer='he_normal',\n                   kernel_regularizer=l2(1e-5))(ip)\n    theta = Reshape((-1, intermediate_dim))(theta)\n\n    # phi path\n    phi = Conv2D(intermediate_dim, (1, 1), padding='same', use_bias=False, kernel_initializer='he_normal',\n                   kernel_regularizer=l2(1e-5))(ip)\n    phi = Reshape((-1, intermediate_dim))(phi)\n\n    # dot\n    f = dot([theta, phi], axes=2)\n    size = K.int_shape(f)\n    # scale the values to make it size invariant\n    f = Lambda(lambda z: (1. \/ float(size[-1])) * z)(f)\n\n    # g path\n    g = Conv2D(intermediate_dim, (1, 1), padding='same', use_bias=False, kernel_initializer='he_normal',\n                   kernel_regularizer=l2(1e-5))(ip)\n    g = Reshape((-1, intermediate_dim))(g)\n\n    # compute output path\n    y = dot([f, g], axes=[2, 1])\n    y = Reshape((dim1, dim2, intermediate_dim))(y)\n    y = Conv2D(channels, (1, 1), padding='same', use_bias=False, kernel_initializer='he_normal',\n               kernel_regularizer=l2(1e-5))(y)\n    y = add([ip, y])\n\n    x = y\n    x = single_conv_block(x, filters=int(channel_num), kernel_size = (3,3))\n    print(x)\n\n    x = add([x, res])\n    return x\n\n\n","1ade7a5f":"def build_unet(shape, num_classes):\n    inputs = Input(shape)\n\n#     \"\"\" Encoder \"\"\"\n    x1, p1 = res_triple_depth_conv_bn_relu(inputs, 32, pool=True)\n    x2, p2 = res_triple_depth_conv_bn_relu(p1, 64, pool=True)\n    x3, p3 = res_triple_depth_conv_bn_relu(p2, 128, pool=True)\n    x4, p4 = res_triple_depth_conv_bn_relu(p3, 256, pool=True)\n    #x5     = double_depth_conv_bn_relu(p4, 256, pool=False) \n    \n#     \"\"\" Bridge \"\"\"\n    fsm_out = fsm(p4)\n\n#     \"\"\" Decoder \"\"\"\n    u0 = Conv2DTranspose(256, (3,3), strides = (2,2), padding=\"same\")(fsm_out)\n    c0 = Concatenate()([u0, x4])\n    x6 = se_double_conv_block(c0, 128, pool=False)\n\n    u1 = Conv2DTranspose(128, (3,3), strides = (2,2), padding=\"same\")(x6)\n    c1 = Concatenate()([u1, x3])\n    x7 = se_double_conv_block(c1, 64, pool=False)\n\n    u2 = Conv2DTranspose(64, (3,3), strides = (2,2), padding=\"same\")(x7)\n    c2 = Concatenate()([u2, x2])\n    x8 = se_double_conv_block(c2, 32, pool=False)\n\n    u3 = Conv2DTranspose(32, (3,3), strides = (2,2), padding=\"same\")(x8)\n    c3 = Concatenate()([u3, x1])\n    x9 = se_double_conv_block(c3, 16, pool=False)\n\n# #     u4 = Conv2DTranspose(16, (3,3), strides = (2,2), padding=\"same\")(x9)\n# #     c4 = Concatenate()([u4, x1])\n# #     x10 = double_depth_conv_bn_relu(c4, 16, pool=False)\n\n#     #b2, x5, x6, x7, x8\n#                                                      #input size\n#     fsm_out_new = depth_conv_bn_relu(fsm_out, 16, pool=False) #8,8,256 \n\n    x6_new = double_conv_block(x6, 32, pool=False)           #16,16,256 \n    x7_new = double_conv_block(x7, 32, pool=False)           #32,32,128   \n    x8_new = double_conv_block(x8, 32, pool=False)           #64,64,64 \n    x9_new = double_conv_block(x9, 32, pool=False)           #128,128,32 \n\n# #     x10_new = double_depth_conv_bn_relu(x10, 16, pool=False)         #256,256,16\n    \n#     #all layers except x8_new need to transposed to dimension 256,256,16\n    \n#     fsm_out_new_transposed = Conv2DTranspose(16, (3,3), strides = (16,16))(fsm_out_new)\n    \n    x6_transposed = Conv2DTranspose(64, (3,3), strides = (8,8))(x6_new)  \n    x7_transposed = Conv2DTranspose(64, (3,3), strides = (4,4))(x7_new)     \n    x8_transposed = Conv2DTranspose(64, (3,3), strides = (2,2), padding='same')(x8_new)  \n    x9_transposed = (x9_new)\n#     x10_transposed = x10_new\n    \n    concat_output = Concatenate()([x6_transposed, x7_transposed, x8_transposed, x9_transposed])\n#                     #o\/p shape = 256,256,96\n\n       \n#     \"\"\" Output layer \"\"\"\n    output = Conv2D(num_classes, 1, padding=\"same\", activation=\"softmax\")(concat_output)\n        \n    return Model(inputs, output)\n","c2702999":"def dice_coef(y_true, y_pred, smooth=1):\n    \"\"\"\n    Dice = (2*|X & Y|)\/ (|X|+ |Y|)\n         =  2*sum(|A*B|)\/(sum(A^2)+sum(B^2))\n    ref: https:\/\/arxiv.org\/pdf\/1606.04797v1.pdf\n    \"\"\"\n    y_true = tf.cast(y_true, tf.float32)\n    y_pred = tf.cast(y_pred, tf.float32)\n    \n    intersection = K.sum(K.abs(y_true * y_pred), axis=-1)\n    return (2. * intersection + smooth) \/ (K.sum(K.square(y_true),-1) + K.sum(K.square(y_pred),-1) + smooth)\n\ndef dice_coef_loss(y_true, y_pred):\n    return 1-dice_coef(y_true, y_pred)","15df980d":"if __name__ == \"__main__\":\n    \n    \"\"\" Seeding \"\"\"\n    np.random.seed(42)\n    tf.random.set_seed(42)\n    \n    i = random.randint(0,49000)\n    \n    \"\"\" Dataset \"\"\"\n    train_frame_path = '..\/input\/lits-256x256\/train_images\/train_images'\n    train_mask_path = '..\/input\/lits-256x256\/train_masks\/train_masks'\n    \n    (train_x, train_y), (valid_x, valid_y) = load_data(train_frame_path, train_mask_path)\n    \n    visualize(image = read_image(train_x[i]), mask = read_mask(train_y[i]))\n    \n    \n    #hyperparameters\n    shape = (256,256,3)\n    classes = 3\n    lr = 1e-4\n    batch_size = 16\n    epochs = 6\n    \n    \"\"\" Model \"\"\"\n    model = build_unet(shape, classes)\n    model.compile(loss=dice_coef_loss , optimizer=tf.keras.optimizers.Adam(lr), metrics = [dice_coef] )\n    model.summary()\n    \n    train_dataset = tf_dataset(train_x, train_y, batch = batch_size)\n    valid_dataset = tf_dataset(valid_x, valid_y, batch = batch_size)\n    \n    train_steps = len(train_x)\/\/batch_size\n    valid_steps = len(valid_x)\/\/batch_size\n        \n    callbacks = [\n        ModelCheckpoint(\"best_model.h5\", verbose=1, save_best_model=True),\n        ReduceLROnPlateau(monitor=\"val_loss\", patience=3, factor=0.1, verbose=1, min_lr=1e-6),\n        EarlyStopping(monitor=\"val_loss\", patience=3, verbose=1,min_delta=0.001)\n    ]\n    \n    history = model.fit(train_dataset,\n        steps_per_epoch=train_steps,\n        validation_data=valid_dataset,\n        validation_steps=valid_steps,\n        epochs=epochs,\n        callbacks=callbacks,\n        verbose = 1\n    )\n    \n    model.save('.\/final_model.h5')","c8f275bf":"history.history.keys()","350cf30c":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')","4e4fc963":"plt.plot(history.history['dice_coef'])\nplt.plot(history.history['val_dice_coef'])\nplt.title('model dice accuracy')\nplt.ylabel('dice score')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper left')\nplt.show()","dc4180bf":"plt.plot(history.history['lr'])\nplt.title('learning rate')\nplt.ylabel('lr')\nplt.xlabel('epoch')\nplt.show()","22489576":"train_steps","c8d73d2d":"import gc\ngc.collect()","4efd4000":"Multiple Feature Pyramind Network Unet -- MFP unet\n\nhttps:\/\/arxiv.org\/ftp\/arxiv\/papers\/1906\/1906.10486.pdf\n\nImplementation of "}}