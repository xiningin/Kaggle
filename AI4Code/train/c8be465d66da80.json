{"cell_type":{"d8dbcf4f":"code","ce791707":"code","41bf6bf0":"code","29bc92dc":"code","243d1010":"code","8ccc2d55":"code","d2d6caac":"code","1c8f556c":"code","289680aa":"code","9fc6de22":"code","7393e024":"code","c2107a39":"code","309a5893":"code","e973452c":"code","0bac9191":"markdown"},"source":{"d8dbcf4f":"# IMPORT MODULES\n# TURN ON the GPU !!!\n# If importing dataset from outside - like this IMDB - Internet must be \"connected\"\n\nimport os\nfrom operator import itemgetter    \nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\nget_ipython().magic(u'matplotlib inline')\nplt.style.use('ggplot')\n\nimport tensorflow as tf\n\nfrom keras import models, regularizers, layers, optimizers, losses, metrics\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import np_utils, to_categorical\n \nfrom keras.datasets import reuters\n\nprint(os.getcwd())\nprint(\"Modules imported \\n\")\nprint(\"Files in current directory:\")\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\")) #check the files available in the directory\n","ce791707":"#(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n\nimport numpy as np\n# save np.load\nnp_load_old = np.load\n\n# modify the default parameters of np.load\nnp.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n\n# call load_data with allow_pickle implicitly set to true\n(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)\n\n# restore np.load for future normal usage\nnp.load = np_load_old","41bf6bf0":"print(\"train_data \", train_data.shape)\nprint(\"train_labels \", train_labels.shape)\n\nprint(\"test_data \", test_data.shape)\nprint(\"test_labels \", test_labels.shape)","29bc92dc":"# Reverse dictionary to see words instead of integers\n# Note that the indices are offset by 3 because 0, 1, and 2 are reserved indices for \u201cpadding,\u201d \u201cstart of sequence,\u201d and \u201cunknown.\u201d\n\nword_index = reuters.get_word_index()\nreverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\ndecoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in\ntrain_data[0]])\n\nprint(decoded_newswire)\nprint(train_labels[0])","243d1010":"# VECTORIZE function\n\ndef vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))\n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.\n    return results","8ccc2d55":"# Vectorize and Normalize train and test to tensors with 10k columns\n\nx_train = vectorize_sequences(train_data)\nx_test = vectorize_sequences(test_data)\n\nprint(\"x_train \", x_train.shape)\nprint(\"x_test \", x_test.shape)","d2d6caac":"# ONE HOT ENCODER of the labels\n\none_hot_train_labels = to_categorical(train_labels)\none_hot_test_labels = to_categorical(test_labels)\n\nprint(\"one_hot_train_labels \", one_hot_train_labels.shape)\nprint(\"one_hot_test_labels \", one_hot_test_labels.shape)","1c8f556c":"# Setting aside a VALIDATION set\n\nx_val = x_train[:1000]\npartial_x_train = x_train[1000:]\ny_val = one_hot_train_labels[:1000]\npartial_y_train = one_hot_train_labels[1000:]\n\nprint(\"x_val \", x_val.shape)\nprint(\"y_val \", y_val.shape)\n\nprint(\"partial_x_train \", partial_x_train.shape)\nprint(\"partial_y_train \", partial_y_train.shape)","289680aa":"# MODEL\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(256, kernel_regularizer=regularizers.l1(0.001), activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(256, kernel_regularizer=regularizers.l1(0.001), activation='relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(46, activation='softmax'))\n\n# REGULARIZERS L1 L2\n#regularizers.l1(0.001)\n#regularizers.l2(0.001)\n#regularizers.l1_l2(l1=0.001, l2=0.001)\n\n# Best results I got with HU=128\/128\/128 or 256\/256 and L1=0.001 and Dropout=0.5 = 77.02%\n# Without Regularizer 72.92%\n# Reg L1 = 76.04, L2 = 76.2, L1_L2 = 76.0\n# Only DropOut (0.5) = 76.85%","9fc6de22":"# FIT \/ TRAIN model\n\nNumEpochs = 50\nBatchSize = 512\n\nmodel.compile(optimizer='rmsprop',loss='categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(partial_x_train, partial_y_train, epochs=NumEpochs, batch_size=BatchSize, validation_data=(x_val, y_val))\n\nresults = model.evaluate(x_val, y_val)\nprint(\"_\"*100)\nprint(\"Test Loss and Accuracy\")\nprint(\"results \", results)\n\nhistory_dict = history.history\nhistory_dict.keys()","7393e024":"# VALIDATION LOSS curves\n\nplt.clf()\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(loss) + 1)\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","c2107a39":"## VALIDATION ACCURACY curves\n\nplt.clf()\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","309a5893":"# Retrain from scratch for # of epochs per LEARNING curves above - and evaluate with TEST (which was set aside above)\n\nmodel = models.Sequential()\nmodel.add(layers.Dense(64, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(46, activation='softmax'))\n\nmodel.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])\nmodel.fit(partial_x_train,partial_y_train,epochs= 20, batch_size=512,\nvalidation_data=(x_val, y_val))\n\nresults = model.evaluate(x_test, one_hot_test_labels)\n\nprint(\"_\"*100)\nprint(results)","e973452c":"# PREDICT\n\npredictions = model.predict(x_test)\n# Each entry in predictions is a vector of length 46\nprint(predictions[123].shape)\n\n# The coefficients in this vector sum to 1:\nprint(np.sum(predictions[123]))\n\n# The largest entry is the predicted class \u2014 the class with the highest probability:\nprint(np.argmax(predictions[123]))","0bac9191":"* **This kernel is based on one of the exercises in the excellent book: Deep Learning with Python by Francois Chollet**\n* It is a single-label (mutually exclusive), multi class classification of text problem \n* Solved using Keras - the Deep Learning framework I much appreciate (with TensorFlow as its backend)\n* The kernel imports the Reuters dataset from Keras"}}