{"cell_type":{"dfcd3d37":"code","33f85e04":"code","cf5dff4c":"code","c043485c":"code","6fe596a9":"code","8b371269":"code","267ea61b":"code","91c8e346":"code","a0f70071":"code","38cfd82f":"code","a1890ee8":"code","b1dda041":"code","c78417cd":"code","6ce2ba73":"code","ecdc993c":"code","bc88dc04":"code","c405df46":"code","18fc08bb":"code","4470c9a6":"code","91e48b51":"code","396b84f9":"code","14547551":"code","7efa8dc3":"code","461206fd":"code","cb669fe4":"code","9fc5a4ff":"code","c2bdcf06":"markdown","8eb4869b":"markdown","6c4d8b66":"markdown","53e35044":"markdown","f5aad8d7":"markdown","882f455a":"markdown","eda1783f":"markdown","910b5a44":"markdown","834d25d3":"markdown","c58b2935":"markdown"},"source":{"dfcd3d37":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn import decomposition\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nfrom sklearn.metrics import accuracy_score,confusion_matrix","33f85e04":"cancer = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ncancer.head()","cf5dff4c":"cancer.drop('Unnamed: 32', axis=1)\ncancer.diagnosis.unique()","c043485c":"cancer.columns","6fe596a9":"#drop 'id' column - godd practice to drop columns such as id, name, etc as they bear no fruit in model building.\nX = cancer.loc[:, ['radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']]\n\ny = cancer.loc[:, 'diagnosis']","8b371269":"#scaling of variables\nsc = StandardScaler()\nscaled_X = sc.fit_transform(X.values)\npd.DataFrame(scaled_X, columns=X.columns).head()","267ea61b":"#encode target variable y\nle = LabelEncoder()\ny = le.fit_transform(y)\npd.DataFrame(y, columns=['diagnosis']).head()","91c8e346":"pca = decomposition.PCA()\npca.fit_transform(scaled_X)","a0f70071":"print(X.shape)","38cfd82f":"#Information content by all new indep variables\npca.explained_variance_ratio_","a1890ee8":"#do a PCA plot and find the correct number of componenets from Elbo\ndf1 = pd.DataFrame({'Information':pca.explained_variance_ratio_,\n                    'PCs':['PC1','PC2','PC3','PC4','PC5','PC6','PC7','PC8','PC9','PC10',\n                          'PC11','PC12','PC13','PC14','PC15','PC16','PC17','PC18','PC19','PC20',\n                          'PC21','PC22','PC23','PC24','PC25','PC26','PC27','PC28','PC29','PC30']})\n\nplt.figure(figsize = (25,6))\nsns.barplot(x = 'PCs',y = 'Information',data = df1)","b1dda041":"#we will do PCA with only 5 components now as they seem to provide 80% of the information.\npca1 = decomposition.PCA(n_components=5)\npca_5var = pca1.fit_transform(scaled_X)","c78417cd":"pca1.explained_variance_ratio_","6ce2ba73":"np.sum(pca1.explained_variance_ratio_)*100","ecdc993c":"new_X = pd.DataFrame(pca_5var,columns=['PC1','PC2','PC3','PC4','PC5'])\nnew_X.head()","bc88dc04":"new_X.corr()","c405df46":"fig, ax = plt.subplots(figsize=(10, 5))\nsns.heatmap(new_X.corr(), annot= True, fmt='.10f', ax=ax)","18fc08bb":"#let's compare Logistic Regession with PCA\nnew_X_train, new_X_test, y_train, y_test = train_test_split(new_X, y, test_size = 0.2, random_state = 42)\n\nlogreg = LogisticRegression(solver='lbfgs')\nlogreg.fit(new_X_train, y_train)\n\ny_pred_test = logreg.predict(new_X_test)\nprint(confusion_matrix(y_test,y_pred_test))\nprint(accuracy_score(y_test,y_pred_test))","4470c9a6":"#let's compare Logistic Regession without PCA when we have all of the original features\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n\nlogreg2 = LogisticRegression(solver='lbfgs')\nlogreg2.fit(X_train, y_train)\n\ny_pred_test2 = logreg2.predict(X_test)\nprint(confusion_matrix(y_test, y_pred_test2))\nprint(accuracy_score(y_test, y_pred_test2))","91e48b51":"# factor loading = PC loadings\npca1.components_","396b84f9":"lda = LinearDiscriminantAnalysis()\nnew_X_train_lda = lda.fit_transform(X_train, y_train)","14547551":"lda.explained_variance_ratio_","7efa8dc3":"new_X_train_lda_df = pd.DataFrame(new_X_train_lda,columns=['LDA1'])\nnew_X_train_lda_df.head()","461206fd":"lda.coef_","cb669fe4":"lg2 = LogisticRegression()\nlg2.fit(new_X_train_lda, y_train)\n\nnew_x_test_lda = lda.transform(X_test)\ny_test_pred_lda = lg2.predict(new_x_test_lda)\n\nprint(confusion_matrix(y_test, y_test_pred_lda))\nprint(accuracy_score(y_test, y_test_pred_lda))","9fc5a4ff":"cancer['diagnosis'].value_counts()","c2bdcf06":"We can see that 82% information is obtained in first 5 components.","8eb4869b":"# Dimension Reduction Methods","6c4d8b66":"75% - 80% of information  - How many principal components is required for the same? We need to look for the elbow the curve.\nWe can clearly see that almost more than 80% of inforfmation is available with first four components. So, let's choose n_components=5 and apply PCA.","53e35044":"We can see above that the dimensions after PCA has not reduced. We've got entire set of new features also called principal componenets.\nWe have to find which all components are actualy important. We can consider ony those components that give about 80% of information.","f5aad8d7":"\nWe can see that there is no correlation among the principal components which is very important. We will now train LogisticRegression with these 5 components.","882f455a":"We can clearly see that accuracy is more when PCA is used compared to when it is not used.","eda1783f":"### Principal Component Analysis\nThis is a unsupervised technique.","910b5a44":"## LDA ( Linear Discriminant Analysis)\nThis is a supervised technique.","834d25d3":"### References\n* https:\/\/www.analyticsvidhya.com\/blog\/2018\/08\/dimensionality-reduction-techniques-python\/\n* https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/practical-guide-principal-component-analysis-python\/\n* https:\/\/www.analyticsvidhya.com\/blog\/2015\/07\/dimension-reduction-methods\/","c58b2935":"We can see abobe that accuracy has come down. LDA will do wonder when the target is uniformly distributed. \n\n1 = 50% and 0 = 50% --- lda will do wonder\n\n1 = 90% and 0 = 10% --- lda will poorly perform****"}}