{"cell_type":{"4ed2eab1":"code","5e9e5eca":"code","17429cda":"code","bf2af486":"code","a1c5a130":"code","bc84f7e5":"code","15b1adae":"code","2541c675":"code","ce24b9b8":"code","5e7b3a8b":"code","fd47a372":"code","70b6a882":"code","70cdd072":"code","7eb67417":"code","e1637b38":"code","4a84a596":"code","1ad5f655":"code","93fbed89":"code","4f012a74":"code","8b39a216":"code","b71f5592":"code","3b2d64f8":"code","6995fa46":"code","c6e38d28":"code","35000cf1":"code","131afcc2":"markdown","d3529677":"markdown","731e7b24":"markdown","c2a67477":"markdown","f5745b38":"markdown","1d319456":"markdown","9ae3e0d9":"markdown","50403845":"markdown","c48b31c4":"markdown","3b9efdcd":"markdown","c93472af":"markdown","b508ed49":"markdown","85dcab5e":"markdown","dd5a74c9":"markdown","296e9598":"markdown","1a7aef74":"markdown","ef98e18a":"markdown","72f467eb":"markdown","09a8a2b8":"markdown","f6ee1623":"markdown"},"source":{"4ed2eab1":"# Importing the libraries\n\nfrom __future__ import print_function\nfrom keras.preprocessing.image import load_img, img_to_array\nfrom keras.applications import vgg19\nfrom keras import backend as K\nimport numpy as np\nfrom scipy.optimize import fmin_l_bfgs_b\nimport time\nimport argparse\nimport cv2\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","5e9e5eca":"# loading the content image and style image\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\ncontent_image_path = (\"..\/input\/rabbit-and-a-style-for-style-transfer\/rabbit.jpg\")\nstyle_reference_image_path = (\"..\/input\/rabbit-and-a-style-for-style-transfer\/style.jpg\")\n\n# prefix for saving the generated image \nresult_prefix = 'gen'\n\n# Number of iterations \niterations = 10\n\n# these are the weights of the different loss components\ncontent_weight = 0.025\nstyle_weight = 1.0\n\nfrom IPython.display import Image\nImage(\"..\/input\/rabbit-and-a-style-for-style-transfer\/rabbit.jpg\")","17429cda":"# defining dimensions of the generated image\nwidth, height = load_img(content_image_path).size\n\ngen_height = 400\n\n# Resizing width according to the height\ngen_width = int( (width\/ height) * gen_height)\n\nprint(\"Ratio of width is to height of content image is \", (width\/height))\nprint(\"Ratio of width is to height of generated image is \", (gen_width\/gen_height))","bf2af486":"# Reading the content image\nfig=plt.figure(figsize=(8, 8))\ncontent_img = cv2.imread(content_image_path)\nplt.imshow(cv2.cvtColor(content_img, cv2.COLOR_BGR2RGB))","a1c5a130":"# Loading the style image\nfig=plt.figure(figsize=(8, 8))\nstyle_img = cv2.imread(style_reference_image_path)\nplt.imshow(cv2.cvtColor(style_img, cv2.COLOR_BGR2RGB))","bc84f7e5":"# preprocessing the image\n# resizing the image to the size of generated image\n# first we are expanding the dimension so as to include the batch size so it can be given to the network\n# Then we are preprocessing the standard way of vgg19 as we are using its trained model\ndef preprocess_image(image_path):\n    img = load_img(image_path, target_size=(gen_height, gen_width))\n    img = img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = vgg19.preprocess_input(img)\n    return img","15b1adae":"def deprocess_image(x):\n    x = x.reshape((gen_height, gen_width, 3))\n    # Remove zero-center by mean pixel\n    x[:, :, 0] += 103.939\n    x[:, :, 1] += 116.779\n    x[:, :, 2] += 123.68\n    # 'BGR'->'RGB'\n    x = x[:, :, ::-1]\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x","2541c675":"# Get tensor representations of our images and store them in tensorflow variable\n# These will go as input to the model\ncontent_image = K.variable(preprocess_image(content_image_path))\nstyle_reference_image = K.variable(preprocess_image(style_reference_image_path))\n\n# placeholder will contain our generated image\ngenerated_image = K.placeholder((1, gen_height, gen_width, 3))\n\n# combine the 3 images into a single Keras tensor along the axis of batch size(axis = 0)\n# batch size = 3 (content_image,style_reference_image, generated_image)\ninput_tensor = K.concatenate([content_image,\n                              style_reference_image,\n                              generated_image], axis=0)","ce24b9b8":"# build the VGG19 network with our 3 images as input\n# the model will be loaded with pre-trained ImageNet weights\nmodel = vgg19.VGG19(input_tensor=input_tensor,\n                    weights='imagenet', include_top=False)\nprint('Model loaded.')","5e7b3a8b":"# Getting the model layers\nprint(\"Summary of the model\", model.summary())","fd47a372":"# get the symbolic outputs of each \"key\" layer (we gave them unique names).\noutputs_dict = dict([(layer.name, layer.output) for layer in model.layers])\nprint(\" The output layers are \", outputs_dict)","70b6a882":"# Selecting the layer  for content loss\nlayer_features = outputs_dict['block5_conv2']\n\n# Along the dimension of batch size, the first is content image, second is style image and third the generated image\n# get the content image feature \ncontent_image_features = layer_features[0, :, :, :]\nprint(\"Shape of content_image_featues is \", content_image_features.shape )\n\n# get the generated image feature\ngenerated_features = layer_features[2, :, :, :]\nprint(\"Shape of generated_features is \", generated_features.shape )","70cdd072":"# compute the neural style loss\n# the gram matrix of an image tensor (feature-wise outer product)\n# In the output of size (25, 47, 512) there are 512 feature vectors, each of size 25 x 47\n\ndef gram_matrix(x):\n    # Since the features are along the 3rd axis, we permute them to bring them to the first axis.\n    # The tensor will become a tensor of size (512, 25, 47)\n    # Then we flatten the tensor. It becomes (512, 1175)\n    features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))\n    \n    # We take dot product with itself - (512 x 1175  x  1175 x 512) - which results in (512 x 512) matrix\n    gram = K.dot(features, K.transpose(features))\n    return gram","7eb67417":"def style_loss(style, generation):\n    \n    '''\n    Input: \n    style      -  the style feature vector that we get using the VGG19\n    generation -  the generated feature vector that we get using the VGG19\n    \n    Returns: style loss\n    '''\n    \n    assert K.ndim(style) == 3\n    assert K.ndim(generation) == 3\n    \n    S = gram_matrix(style)\n    C = gram_matrix(generation)\n    \n    channels = 3\n    size = gen_width * gen_height\n    return K.sum(K.square(S - C)) \/ (4. * (channels ** 2) * (size ** 2))\n","e1637b38":"# an auxiliary loss function\n# designed to maintain the \"content\" of the\n# base image in the generated image\n\ndef content_loss(content, generation):\n    \n    '''\n    Input: \n    content    -  the content feature vector that we get using the VGG19\n    generation -  the generated feature vector that we get using the VGG19\n    \n    Returns: content loss\n    '''\n    return K.sum(K.square(generation - content))","4a84a596":"# make variable to store total loss and initialise it with '0.'\nloss = K.variable(0.)\n\n# calculating the content loss by multiplying content loss with its weight\nlossy = content_weight * content_loss(content_image_features,\n                                      generated_features)\nloss = loss + lossy\n","1ad5f655":"# defining the layers for which we want to calculate style loss\nfeature_layers = ['block1_conv1', 'block2_conv1',\n                  'block3_conv1', 'block4_conv1',\n                  'block5_conv1']\n\n# calculating the style loss for all the feature layers\nfor layer_name in feature_layers:\n    layer_features = outputs_dict[layer_name]\n    \n    # style feature is the second layer along the batch dimension and generated features is along the third \n    style_reference_features = layer_features[1, :, :, :]\n    generated_features = layer_features[2, :, :, :]\n    \n    # calculating the style loss for each feature layer \n    sl = style_loss(style_reference_features, generated_features)\n    \n    # add the style loss to previous loss to calculate total loss\n    loss += (style_weight \/ len(feature_layers)) * sl","93fbed89":"# get the gradients of the generated image w.r.t. the loss\ngrads = K.gradients(loss, generated_image)\n\n# get the loss and gradient in the output variable\noutputs = [loss]\n\n# add gradient to the output\nif isinstance(grads, (list, tuple)):\n    outputs += grads\nelse:\n    outputs.append(grads)\n    \n# K.function(inputs, outputs, updates=None), like how we make model, where\n# inputs: List of placeholder tensors, here it is generated_image which we have defined above\n# outputs: List of output tensors, here it is loss with which we want to update the inputs\n# returns outputs(loss here) after evaluating inputs(generated image) on model\n# https:\/\/keras.io\/getting-started\/faq\/\n\nf_outputs = K.function([generated_image], outputs)","4f012a74":"def eval_loss_and_grads(x):\n    \n    '''\n    x : x is the generated image on which we do iteration. So, here we feed generated image to the model which\n        we have defined using K.function(input, output). It will return output(outs) which is combination of loss and grad.\n        We segregate the loss value and grad value.\n    \n    Returns: loss_value, grad_values\n    \n    '''\n    \n    x = x.reshape((1, gen_height, gen_width, 3))\n    \n    # here we feed generated input to the model to calulate the output\n    outs = f_outputs([x])\n    \n    # Rembember the first value of outs is loss value and the second value is gradient\n    loss_value = outs[0]\n    if len(outs[1:]) == 1:\n        grad_values = outs[1].flatten().astype('float64')\n    else:\n        grad_values = np.array(outs[1:]).flatten().astype('float64')\n    return loss_value, grad_values","8b39a216":"class Evaluator(object):\n    \n    '''\n    Input: Input to the fucntion is generated image. Here to pass it to eval_loss_and_grads function\n           to calculate the loss and grad on it. \n    \n    Return: loss function will return loss value \n            grad fucntion will return grad value\n    '''\n\n    def __init__(self):\n        self.loss_value = None\n        self.grads_values = None\n\n    # Return loss value     \n    def loss(self, x):\n        assert self.loss_value is None\n        # call the eval_loss_and_grads function, using input as generated image\n        loss_value, grad_values = eval_loss_and_grads(x)\n        self.loss_value = loss_value\n        self.grad_values = grad_values\n        # Return loss value\n        return self.loss_value\n\n    # Return grad values\n    def grads(self, x):\n        assert self.loss_value is not None\n        # Make copy of grad value\n        grad_values = np.copy(self.grad_values)\n        # set the loss value and grad values none for next iteration\n        self.loss_value = None\n        self.grad_values = None\n        # Return grad value for current iteration\n        return grad_values","b71f5592":"# Initialize the class\nevaluator = Evaluator()","3b2d64f8":"# run scipy-based optimization (L-BFGS) over the pixels of the generated image\n# so as to minimize the neural style loss\n\n# Initialize the generated image with content image (x) and pass to  fmin_l_bfgs_b()\nx = preprocess_image(content_image_path)\niterations = 10\nfor i in range(iterations):\n    print('Start of iteration', i)\n    start_time = time.time()\n    \n    # optimizing the neural style loss\n    x, min_val, info = fmin_l_bfgs_b(evaluator.loss, x.flatten(),\n                                     fprime=evaluator.grads, maxfun=20)\n    print('Current loss value:', min_val)\n    \n    # save current generated image\n    img = deprocess_image(x.copy())\n    fname = result_prefix + '_at_iteration_%d.png' % i\n    cv2.imwrite(fname, img)\n    end_time = time.time()\n    print('Image saved as', fname)\n    print('Iteration %d completed in %ds' % (i, end_time - start_time))","6995fa46":"# Generated imaage at first iteration\nfig=plt.figure(figsize=(8, 8))\ngen_image_final = cv2.imread('gen_at_iteration_1.png')    \nplt.imshow(cv2.cvtColor(gen_image_final, cv2.COLOR_BGR2RGB))","c6e38d28":"# Generated imaage at fifth iteration\nfig=plt.figure(figsize=(8, 8))\ngen_image_final = cv2.imread('gen_at_iteration_5.png')    \nplt.imshow(cv2.cvtColor(gen_image_final, cv2.COLOR_BGR2RGB))","35000cf1":"# Generated imaage at 9th iteration\nfig=plt.figure(figsize=(8, 8))\ngen_image_final = cv2.imread('gen_at_iteration_9.png')    \nplt.imshow(cv2.cvtColor(gen_image_final, cv2.COLOR_BGR2RGB))","131afcc2":"#### Preprocess the image. It will be used to preprocess the style and the content image, that we will give to the network. ","d3529677":"### Define Style Loss\nThe style loss is sum of L2 distances between the Gram matrices of the representations of the generated image and the style image, extracted from different layers of CNN(here, VGGNet). The general idea is to capture style (color\/texture\/shapes\/edges etc) at different layers.\n\nLet $A^l$ and $G^l$ style representations of generated(artistic) and style image in layer $l$. The contribution of that layer to the total loss is:\n![sLoss.PNG](attachment:sLoss.PNG)\n\nand the total style loss is:\n![lLoss.PNG](attachment:lLoss.PNG)","731e7b24":"### Define Class Evaluator\nIn the above function 'eval_loss_and_grads', it returned the combined loss and grad value. Here, we define a class which has separate 'loss' function and 'grads' function, that will seperate the loss and grad values. They will return those values separately.","c2a67477":"Once the candidate image is generated, we will plot it to see the result after deprocessing it using the following function.","f5745b38":"### Calculate Style Loss and add to Content Loss\nHere, we will calculate style loss for a particular set of layers and add the style loss calculated at each of these layers to the content loss to find the total loss. \n![lLoss.PNG](attachment:lLoss.PNG)","1d319456":"### Define Content Loss\nThe content loss is a L2 distance between the features of the generated image and the features of the content image. Let $P^l$ and $F^l$ be feature representation of content image and generated image in layer $l$. We then define the squared-error loss between the two feature representations as:\n\n![content.PNG](attachment:content.PNG)","9ae3e0d9":"## Define a function to feed the generated image and get the output - loss and gradient.\nWe define function, that takes input as generated image, and return combined loss and grad values using the function f_outputs that we have defined above. ","50403845":"## Train the model\nWe will use the L-BFGS optimiser to update the generated image. L-BFGS has empirically shown to be much more suitable for this task.\n\n### Steps:\n#### 1. Initialize the generated image with content image (rather than initialising it with white noise)\n#### 2. Pass the image along with grad and loss values to the optimiser 'fmin_l_bfgs_b()'. The optimiser will return the updated image. \n#### 3. Deprocess the image and save it.\n#### 4. Repeat steps 3 and 4 for a fix number of iterations.","c48b31c4":"### Accessing layer by their name\nWe already know that we can access any layer by their name. We are using layer 'block5_conv2', which is fairly deep, for capturing the content of the image. \n\n### Capturing content \nWe already know that deeper layers are able to capture most of the features of an image. And layer 'block5_conv2' is deep enough to capture the features. Also, <b>to capture more abstract features, we are using deeper VGG-19 instead of VGG-16<\/b>. Note that the content loss is calculated at only one layer which is generally one of the final layers. We'll calculate the content loss in the 'block5_conv2' layer. To calculate the style loss, we'll use multiple layers since style loss is calculated at multiple layers as you're already aware. \n\n#### Note that along the dimension of batch size, the first is content image, second is style image and third the generated image get the content image feature. We will seperate those to calculate the loss. ","3b9efdcd":"#### Plot the content image and style image","c93472af":"### Giving name to the layers\nWe can give each layer a particular name while defining the model. <b> Later, you can access a particular layer with its name <\/b>. Let's look at the layers with their names. ","b508ed49":"Note that, style transfer is different than training a CNN where we feed the input images during training process and the weights get updated in each iteration. Here, we are training the image itself. There is no native functionality in Keras to do style transfer. Therefore, we need to create a custom network where the image gets updated. Keras, however, does provide functionality to create customer networks by directly manipulating the tensorflow backend. You're already familiar with the steps involved in style transfer. In this notebook, you'll look at how to implement that in code. Before moving, there are certain tensorflow functions that you need to know about before proceeding. Till now, you didn't need to use these functions as these were taken care of by Keras. But you need to directly use these in order to create your custom network tailored according to the network.\n\n\n### K.variable()\nK.variable(), as the name suggest, can be used to store a variable. \n\n\n### K.placeholder()\nWith placeholders, we can insert values into the model in runtime. These can be used for training input(X) and target(Y) values.  The difference is that with <b>K.Variable() you have to provide an initial value when you declare it. With K.placeholder() we don't have to provide an initial value and can provide it at runtime.<\/b> \n\nSince, content and style image are contant and does not require to feed during runtime, we are using K.variable() to store that. Generated image is fed during runtime after each iteration, so we use placeholder for that. Note that when using backend as tensorflow(tf) for Keras, K.variable is same as tf.variable and K.placeholder is same as tf.placeholder. \n\nWe combine the 3 images into a single Keras tensor along the axis of batch size(axis = 0) which we feed to the model. So the batch size will become 3. Along the dimension of batch size, the first is content image, second is style image and third the generated image get the content image feature\n\nAdditional reading: \n1. https:\/\/keras.io\/backend\/\n2. https:\/\/learningtensorflow.com\/lesson4\/","85dcab5e":"### Calculate gradient \n#### Calculate gradient of loss with respect to the pixels of the generated image. Here, we do not change the pre-trained weights of the VGG19. We use the pre-trained weights just to capture the different style and content aspect of the images and then we calculate the loss. \n\n### Output of the model\nWe will use scipy-based optimization (L-BFGS) over the pixels of the generated image as loss function(instead of SGD or other loss function). The L-BFGS optimiser takes two parameters: the loss and the gradient of the image w.r.t. the loss. To generate the gradient and the loss, we'll use Keras' k.function().\n\n### K.function()\nWe need to define a function, which takes in the generated image, spits out the loss and the gradient. The output of the function will be used for optimisation. Till now, Keras used to do all this for you in the beacked. But in this problem, you need . In Keras, we define such a custom function (with a custom input and custom output), using K.function().\n\nAdditional reading: https:\/\/keras.io\/getting-started\/faq\/","dd5a74c9":"Let's understand the dimension of (25, 47, 512). It denotes <b>512 feature vector, each of height 25 and width 47<\/b>. Let's see the width\/height ratio: 49\/25=1.96. This is almost equal to 1.90, which is the width\/height ratio of the original image. This shows that both width and height are reduced by same factor during convolution and pooling operation","296e9598":"## Loss Function\nNow we will define the loss functions for the style loss and the content loss. For the style loss, we will define the Gram matrix.\n\n### Define Gram matrix\nHow do you capture relationship between 2 vector? By correlation! Correlation is a statistical measure that captures the relationship between 2 varibles. Correlation indicates the extent to which change in one variable increase or decrease the other variable.\n\n\nWe may see an image as distribution over different feature vectors. Each feature vector in a layer captures some aspect of image. For example, in the initial layer, feature vectors captures edges. Gram matrix captures correlation between different feature vectors, not the presence or absence of specific features. Gram matrix is the product between a matrix and its transpose.\n\n![gram.PNG](attachment:gram.PNG)\n","1a7aef74":"### Input to the model\nUsing image of rabbit as content image and splash of light as style image. You can use any other images as these two images. Here, train it for 10 iterations. Remember that we are not training any weights, but generating the candidate image using training. Therefore, not many iterations will be required. At the end of notebook, you will be able to see the results. \n\nIn this notebook, 'candidate image' is referred as 'generated image'.","ef98e18a":"## Total Loss\n\nNow we will combine style loss and content loss to find total loss. Let's define variable to store the the loss.\n\n### Calculate Content Loss\nCalculate content loss by multiplying content loss with its weight","72f467eb":"Import the libraries","09a8a2b8":"#### Defining the shape of the generated image. Let's make height as 400, and width according to the width:height ratio of the content image. ","f6ee1623":"### Load VGG19 model "}}