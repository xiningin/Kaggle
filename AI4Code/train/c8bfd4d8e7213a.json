{"cell_type":{"1769f979":"code","35d616a1":"code","4875077f":"code","3ede52da":"code","202483c7":"code","56239b21":"code","bb2293f4":"code","c03c7a5b":"code","382e5d6d":"code","170378f7":"code","c6f61580":"code","b3ea494a":"code","381f3dc1":"code","dc534d86":"code","3187da97":"code","62fc4f90":"code","00cb9749":"code","fb8faf50":"code","f618e371":"code","21ae326b":"code","522f9abc":"code","92a13538":"code","b2910304":"code","3c1a1073":"code","aaeb7def":"code","2c5ef13c":"code","1b2f4002":"code","5acc91d3":"code","9a0b24e4":"code","3044d800":"code","0f4c5eca":"code","ce9b6fad":"code","7b6b1e57":"code","27110dd5":"code","6fb37c0a":"code","6b2705f7":"code","b22a7764":"code","38290208":"code","d95ba1ac":"code","c3d59c7f":"code","f6296318":"code","3470c5cf":"code","e2671886":"code","20148bef":"code","424571a2":"code","a9945dad":"code","4442fae6":"code","94bd0797":"code","3fa3c812":"code","15253d0d":"code","fc6a3ac9":"code","c5b1bc31":"code","d39c006a":"code","4695cdd8":"code","0ca8f15c":"code","419b23e1":"code","4fe4f9de":"code","b513cfd1":"code","50615480":"code","777f9f78":"code","66884633":"code","2d3300e6":"code","7a9acdbb":"code","b9b17b01":"code","cce56f19":"code","4d447b4d":"code","53b5d86d":"code","abfb3fc8":"code","aba3625f":"code","126714e8":"code","e4e889cb":"code","f9855293":"code","f3103b9c":"code","f9acc330":"code","1b077c56":"code","67d90fc5":"code","88b682d3":"code","8366112d":"code","17f8a052":"code","688b2405":"code","b50c681a":"code","ef26e3ec":"code","66345439":"code","0ae100cd":"code","a407038b":"code","ab373ae6":"code","f189c641":"code","1c001632":"code","174718c8":"code","865456e0":"code","14856ac7":"code","5c57d004":"code","e077f679":"code","533e18d5":"code","c102eb47":"code","a2fc3895":"code","c24e60d4":"code","442614e0":"code","33807be6":"code","12d9d84e":"code","39e3e68a":"code","59050150":"code","c9a2f2c0":"code","8e5c09af":"code","208147cd":"code","aee655bf":"code","b185884a":"code","4a4dde8a":"markdown","6df5defe":"markdown","03eb26be":"markdown","284b1c6a":"markdown","5b94fcd7":"markdown","171bdacf":"markdown","f9aecfa4":"markdown","d85c1ec0":"markdown","d4b1a5d0":"markdown","a39c840e":"markdown","7b7654ce":"markdown","de3d6b07":"markdown","0daa5c5d":"markdown","8463d40f":"markdown","959fbf87":"markdown","0d34d92c":"markdown","ce5bf859":"markdown","0d05f23f":"markdown","1e04b5de":"markdown","4ae2a7d9":"markdown","c9fbc868":"markdown","d3c4a505":"markdown","093185b6":"markdown","0f6b7904":"markdown","2c964ddf":"markdown","210b7620":"markdown","a997053b":"markdown","755fbd5b":"markdown","24000dc9":"markdown","0dee9ad3":"markdown","6c265b15":"markdown","5008c605":"markdown"},"source":{"1769f979":"#Importing important libriries\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nimport os #Provides functions for interacting with the operating system \n%matplotlib inline","35d616a1":"#We check the current working directory\nos.getcwd()","4875077f":"#We check the parent working directory name.\nprint(os.listdir(\"..\/input\"))","3ede52da":"merc_train = pd.read_csv(\"..\/input\/mercedesbenz-greener-manufacturing\/train.csv\")\nmerc_test = pd.read_csv(\"..\/input\/mercedesbenz-greener-manufacturing\/test.csv\")","202483c7":"#We check the shape of each Dataset\n#Each Dataset comprises of 4209 attributes, 378 indexes for train dataset and 377 indexes for test dataset\nmerc_train.shape, merc_test.shape","56239b21":"#We check the nature of our columns by viewing the first five rows\nmerc_train.head()","bb2293f4":"#We check the data type. We have the pandas DataFrame\ntype(merc_train)","c03c7a5b":"#Summary statistics. The summary statistics for the 8 columns could not be displayed since it is categorical.\n#From X0 to X9 its look like we have the categorical indexes as they were noot included in the summary statistics.\nmerc_train.describe()","382e5d6d":"#The nature of our columns\n#We have one float which is our target variable y, It seems like we have 8 categorical variables, and from X10 to X385\n#We have integers.\nmerc_train.info()","170378f7":"merc_train.dtypes","c6f61580":"#We observe the nature of our indexes by printing the numerical and categorical features size of the columns.\n#From X0 to X8 are categorical variables. The target variable is continuous. The other indexes are binary nominal data type. \n#The numeric(binary) - presence\/absence of the car feature.\n\nnumerical_features = merc_train.select_dtypes(include=[np.number]).columns\ncategorical_features = merc_test.select_dtypes(include=[np.object]).columns\n\nprint('Numerical feature size: {}'.format(len(numerical_features)))\nprint('Categorical feature size: {}'.format(len(categorical_features)))","b3ea494a":"#We see what are those columns after column 10\nmerc_train.columns[10:]","381f3dc1":"#We check the columns with unique values\nnp.unique(merc_train.columns[10:])","dc534d86":"#We check if they are real binary\nnp.unique(merc_train[merc_train.columns[10:]])","3187da97":"merc_train.loc[:, (merc_train !=0).any(axis=0)]","62fc4f90":"(merc_train !=0).any(axis=0)","00cb9749":"#Checking for null values in each column\n#There is no null values in either training or testing datasets\n\nmerc_train.isna().any()","fb8faf50":"merc_test.isna().any()","f618e371":"#Lets get an overview and some statistics of a datasets especially on the variable y.\nprint(merc_train['y'].describe())","21ae326b":"#Distribution plot. We draw distribution plot as sns. \nimport seaborn as sns\nplt.figure(figsize=(12,6))\nplt.hist(merc_train['y'], bins=50, color='b')\nplt.xlabel('testing time in secs')","522f9abc":"plt.figure(figsize=(12,8))\n\nsns.distplot(merc_train.y.values, bins=50, kde=True)\nplt.xlabel('y value', fontsize=12)","92a13538":"#Simple plotting y visualize some outlier. One of the cars is taking 270s for testing(which is an outlier).\n#The majority of the cars are taking around 75 to 150 seconds for testing.\nplt.figure(figsize=(15,6))\nplt.plot(merc_train['y'])","b2910304":"#pdf and cdf\n\ncounts, bin_edges = np.histogram(merc_train['y'], bins=10, density=True)\nplt.xlabel('y')\npdf = counts\/(sum(counts))\nprint(\"pdf=\",pdf);\nprint(\"bin_edges=\",bin_edges);\ncdf = np.cumsum(pdf)\nprint(\"cdf=\",cdf)\nplt.plot(bin_edges[1:],pdf);\nplt.plot(bin_edges[1:], cdf)","3c1a1073":"#boxplot it view some statistical values. The minimum value is around 73 seconds, \n#the median is 100s(most cars test around this time). The maximun is approximately 140s.\n#we have one outlier at above 260s.\nsns.boxplot(y='y', data=merc_train)\nplt.figure(figsize=(15,9))\nplt.show()","aaeb7def":"#Violin plot gives pdf(probabilty density plot) along with boxplot in it.\n#We can observe that most of the cars are tested around 73s to 150s looking at the blue shade. \n#After that we can expect some extreme values and probably some outliers.\nsns.violinplot(y='y', data=merc_train, size=8)\nplt.show()","2c5ef13c":"numerics = ['int16','int32','int64','float16','float32','float64']\nobjects = ['object']","1b2f4002":"merc_train_num = merc_train.select_dtypes(include=numerics)\nmerc_train_cat = merc_train.select_dtypes(include=objects)","5acc91d3":"print(merc_train_num.shape, merc_train_cat.shape)\n\nprint('*****************************************************************')\nprint(merc_train_cat.columns)\nprint('*****************************************************************')\nprint(merc_train_num.columns)","9a0b24e4":"merc_test_num = merc_test.select_dtypes(include=numerics)\nmerc_test_cat = merc_test.select_dtypes(include=objects)","3044d800":"print(merc_test_num.shape, merc_test_cat.shape)\n\nprint('******************************************************************')\nprint(merc_test_cat.columns)\nprint('******************************************************************')\nprint(merc_test_num.columns)","0f4c5eca":"for col_name in merc_train_cat.columns:\n    print('The unique values in '+col_name+' are:', merc_train_cat[col_name].nunique())\n    print(merc_train_cat[col_name].unique())\n    print('*********************************************')","ce9b6fad":"for col_name in merc_test_cat.columns:\n    print('The unique values in '+col_name+' are:', merc_test_cat[col_name].nunique())\n    print(merc_test_cat[col_name].unique())\n    print('*********************************************')","7b6b1e57":"#We look at the change of y fro each of this category index X0, X2, X3...\n#We do this to analyze the outliers and drop them to make a match of our train and test datasets\ncols=['X0','X1','X2','X3','X4','X5','X6','X8']\nfor col in cols:\n    \n    plt.figure(figsize=(16,6))\n    \n    sns.boxplot(x=col, y='y', data=merc_train)\n    \n    plt.xlabel(col, fontsize=10)\n    plt.title('Distibution of y variable')\n    plt.ylabel('y', fontsize=10)\n    plt.xticks(fontsize=8)\n    plt.yticks(fontsize=8) ","27110dd5":"#As we observed the boxplot with categorical distribution of y. There might be some outliers because of small single dots above\n#150.\n\nplt.figure(figsize=(12,6))\nsns.violinplot(merc_train['y'].values)","6fb37c0a":"cols=['X0','X1','X2','X3','X4','X5','X6','X8']\n\nfor col in cols:\n    plt.figure(figsize=(16,6))\n    sns.violinplot(x=col, y='y', data=merc_train, height=15)\n    plt.show()","6b2705f7":"from numpy import percentile","b22a7764":"# calculate interquartile range\nq25, q75 = percentile(merc_train.loc[:,'y'], 25), percentile(merc_train.loc[:,'y'], 75)\niqr = q75 - q25","38290208":"print(q25,q75)","d95ba1ac":"iqr","c3d59c7f":"# calculate the outlier cutoff\ncut_off = iqr * 1.5\nlower, upper = q25 - cut_off, q75 + cut_off","f6296318":"print(lower,upper)","3470c5cf":"# identify outliers\noutliers = [x for x in merc_train.loc[:,'y'] if x < lower or x > upper]","e2671886":"outliers","20148bef":"print('Identified outliers: %d' % len(outliers))","424571a2":"outliers_removed = [x for x in merc_train.loc[:,'y'] if x >= lower or x <= upper]","a9945dad":"outliers_removed","4442fae6":"#We firstly append the two datasets (train and test)\n\n#With `ignore_index` set to True:\n\n#>>> df.append(df2, ignore_index=True)\nmerc_train.append(merc_test, ignore_index=True)","94bd0797":"#We assign the DataFrame as 'merc'\nmerc = merc_train.append(merc_test, ignore_index=True)","3fa3c812":"merc=pd.get_dummies(merc)","15253d0d":"#Checking the indexes of the converted dataset\nmerc.index","fc6a3ac9":"train, test = merc[0:len(merc_train)], merc[len(merc_train):]","c5b1bc31":"train.shape, test.shape","d39c006a":"merc.head()","4695cdd8":"#Seperate the features and response column\nX_train_1 = train.drop(['y','ID'], axis=1)\ny_train_1 = train['y']\n\nX_test_1 = test.drop(['y','ID'], axis=1)","0ca8f15c":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_squared_error\nX_train, X_test, y_train, y_test = train_test_split(X_train_1,y_train_1, test_size=0.30, random_state=101)","419b23e1":"from sklearn.tree import DecisionTreeRegressor\ndtree = DecisionTreeRegressor()","4fe4f9de":"%%time\ndtree.fit(X_train, y_train)","b513cfd1":"y_pred = dtree.predict(X_train)","50615480":"y_pred","777f9f78":"score = r2_score(y_train, y_pred)","66884633":"error = mean_squared_error(y_train,y_pred)","2d3300e6":"error","7a9acdbb":"score","b9b17b01":"y_pred = dtree.predict(X_test)","cce56f19":"score = r2_score(y_test, y_pred)","4d447b4d":"score","53b5d86d":"#instatiate the Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor \nrf_reg = RandomForestRegressor(n_estimators=50) ","abfb3fc8":"X_train, X_test, y_train, y_test = train_test_split(X_train_1,y_train_1, test_size=0.25, random_state=4)","aba3625f":"%%time\n#fit the data\nrf_reg.fit(X_train,y_train)","126714e8":"#from sklearn.metrics import r2_score, mean_squared_error\n\n\n#predict (training samples)\ny_pred = rf_reg.predict(X_train)\n\nprint('\\nTraining score :')\nprint('Mean square error : %2.f'% mean_squared_error(y_train,y_pred))\nprint('R2 score: %2.f' %r2_score(y_train,y_pred))\n\n#predict (testing samples)\ny_pred = rf_reg.predict(X_test)\n\nprint('\\nTesting score :')\nprint('Mean square error : %2.f'% mean_squared_error(y_test,y_pred))\nprint('R2 score: %2.f' %r2_score(y_test,y_pred))","e4e889cb":"# Use the forest's predict method on the test data\npredictions = rf_reg.predict(X_test)","f9855293":"# Calculate the absolute errors\nerrors = abs(predictions - y_test)","f3103b9c":"# Print out the mean absolute error (mae)\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')","f9acc330":"# Calculate mean absolute percentage error (MAPE)\nmape = 100 * (errors \/ y_test)","1b077c56":"# Calculate and display accuracy\naccuracy = 100 - np.mean(mape)","67d90fc5":"print('Accuracy:', round(accuracy, 2), '%.')","88b682d3":"#From sklearn we load Gradient Boosting\nfrom sklearn.ensemble import GradientBoostingRegressor","8366112d":"%%time\nGB_regressor = GradientBoostingRegressor()\nGB_regressor.fit(X_train,y_train)","17f8a052":"y_predictions = GB_regressor.predict(X_train)","688b2405":"y_predictions","b50c681a":"print('Score of Model:', r2_score(y_train,y_predictions))\nprint('Mean square error:', mean_squared_error(y_train, y_predictions))","ef26e3ec":"y_predictions = GB_regressor.predict(X_test)","66345439":"y_predictions","0ae100cd":"print('Score of Model:', r2_score(y_test,y_predictions))\nprint('Mean square error:', mean_squared_error(y_test,y_predictions))","a407038b":"from xgboost import XGBRegressor","ab373ae6":"%%time\nxgb_regressor = XGBRegressor()\nxgb_regressor.fit(X_train, y_train)","f189c641":"y_prediction2 = xgb_regressor.predict(X_train)","1c001632":"y_prediction2","174718c8":"print('Score of Model :', r2_score(y_train,y_prediction2))\nprint('Mean square error :', mean_squared_error(y_train,y_prediction2))","865456e0":"y_prediction3 = xgb_regressor.predict(X_test)","14856ac7":"y_prediction3","5c57d004":"print('Score of Model :', r2_score(y_test,y_prediction3))\nprint('Mean square error :', mean_squared_error(y_test,y_prediction3))","e077f679":"# Hyper Parametezation using RandomizedSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n# We evaluate the params using \"mae\"\nfrom sklearn.metrics import mean_absolute_error","533e18d5":"# Prepare the dict of parameters to use\nxgb_params = { \"max_depth\" : [3,4,5,6,7,8,9],\n                \"learning_rate\" : [0.05,0.10,0.15,0.20,0.25,0.30],\n                \"verbosity\" : [0,1,3],\n                \"min_child_weights\" : [1,3,5,7],\n                \"gamma\" : [0.0,0.1,0.2,0.3,0.4],\n                \"colsample_bytree\" : [0.3,0.4,0.5,0.7],\n                \"n_estimators\" : [100]\n               }","c102eb47":"def timer(start_time = None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod ((datetime.now()-start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('n\\ Time taken: %i hours %i modules and %s seconds.' %(thour, tmin, round(tsec,2)))","a2fc3895":"class Timer:\n    def _init_(self):\n        self.start = time.time()\n        def restart(self):\n            self.start = time.time()\n            m,s = divmod(end_self.start, 60)\n            h,m = divmod(m,60)\n            time_str = \"%02d:%02d\" %(h,m,s)\n            return time_str","c24e60d4":"#Instantiate the xgboost regressor model\nregressor=XGBRegressor()","442614e0":"#Call the RandomizdSearchCV\n\nrandom_search = RandomizedSearchCV(regressor,param_distributions=xgb_params,n_iter=5, scoring='neg_mean_squared_error', \n                                   n_jobs=-1,cv=5,verbose=3)","33807be6":"# Fitting the model into a timer\n\nfrom datetime import datetime\nstart_time = timer(None)\nrandom_search.fit(X_train,y_train)\ntimer(start_time)","12d9d84e":"# Parameter setting that gave the best results on the hold out data.\nrandom_search.best_estimator_","39e3e68a":"# Gives the parameter setting for the best model, that gives the highest mean score.\nrandom_search.best_params_","59050150":"# Alternate code for the above task.\nrandom_search.cv_results_['params'][random_search.best_index_]","c9a2f2c0":"regressor=XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.5, gamma=0.3,\n             importance_type='gain', learning_rate=0.05, max_delta_step=0,\n             max_depth=4, min_child_weight=1, min_child_weights=5, missing=None,\n             n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n             seed=None, silent=None, subsample=1, verbosity=1)","8e5c09af":"from sklearn.model_selection import cross_val_score","208147cd":"score = cross_val_score(regressor, X_train, y_train, cv=10)","aee655bf":"score","b185884a":"score.mean()","4a4dde8a":"#Our model has learned to predict the maximum time it takes for the cars to pass the testing with an accuracy of 94.4%","6df5defe":"#Not the good score in our testing dataset, we might as well forget or try en Ensemble learning methods and see which model\n#will give us a better accurary.","03eb26be":"#### XGBoost Modelling","284b1c6a":"#Analyze the columns types -test","5b94fcd7":"Ensemble techniques combine individual models together to improve the stabilty and predictive power of the model.\nOne of the ensemble methods are Boosting and Gradient Boosting model.\n\nBoosting technique reduce bias by adding all the weak learners sequantially, each classifier trying to correct its predecessor.\n\nGradient Boosting minimizes the loss function(MSE)of a models by adding weak learners using a gradient descesnt procedure.\n","171bdacf":"#### Observations\n+ As we apply above four regression models we come to observe that if we use model in productionization\n  Decision tree is the best among above as of time complexity,and with r2_score.\n+ If we also only care about the erros, Decision tree and Random Forest are  the best. However Random forest is the slowest \n  model with 26.1s.\n+ We can improve our ensembe learning models by doing more feature engineering:\n    + Removing outliers\n    + Dropping columns with low variance of about 0.01 threshold value\n    + Checking some correlation of the indexes and remove those columns with same correlation as it denotes duplicates\n    + Implementing the models using hyperparameter tuning.","f9aecfa4":"#Observations:\n\n#It appears that some of the categorical value conts in the training and testing are not the same. Some of the categorical\n#values in training set are not in the testing set. This would be a problem in running the model.\n\n\n#The unique values in Xo are: 47(merc_train)\n#The unique values in Xo are: 47(merc_test)\n\n    \n#The unique values in X1 are: 44(merc_train)\n#The unique values in X1 are: 45(merc_test)\n\n    \n#The unique values in X5 are: 29(merc_train)\n#The unique values in X5 are: 32(merc_test)    \n    ","d85c1ec0":"# Basic Statistics","d4b1a5d0":"# Looking into each categorical feature - train","a39c840e":"# Exploratory Data Analysis","7b7654ce":"# The distribution of the test time for each of the category variable","de3d6b07":"we draw this using univariable \u201cy\u201d, and drawn cummulative distribution function and probability density funtion.\nif we draw a straight line from y value at 150s, then it intersects the curve \nCummulative distribution funtion(yellow) at value approximately equal to \n0.98 i.e there are 98% cars from cummulative sum of 50 to 150 seconds","0daa5c5d":"Test predictions","8463d40f":"# CDF(cummulative distribution function) and PDF(probability density function):","959fbf87":"# Load the two Datasets","0d34d92c":"#We found out how far away our average prediction is from the actual value by taking an absolute error.","ce5bf859":"#Our average estimate is off by 3.01 degrees which is not bad","0d05f23f":"Outlier detection\/removal","1e04b5de":"# Encoding categorical features to numerical values","4ae2a7d9":"# Null values","c9fbc868":"# Detecting outliers using z score","d3c4a505":"Test predictions","093185b6":"#OBSERVATIONS:\n\n#Looking at the summary statistics based on y variable, 75% of the cars are tested less than 109s, 50% cars less than 100s.\n#Only 25% of the cars takes more testing time between 109s and 265s.\n#The minimum testing time is 72s, maximum testing time is 265s, mean with an average of 100s, deviation of 12.","0f6b7904":"# Distribution of y with category variables - Univariate, histogram, outliers","2c964ddf":"# Ensemble Learning (The performance of boosting models)","210b7620":"# Dertermine Performance Metrics","a997053b":"# Decision tree and Random Forest","755fbd5b":"#### Hyperparameter tuning","24000dc9":"# Compare the values in the columns between the train and test dataset","0dee9ad3":"#This is pretty! good our model is doing well with the train dataset prediction of 97% accuracy.","6c265b15":"#### Building Models\n+ Gradient BoostingRegressor\n+ XGBoost","5008c605":"Looking into each categorical feature - test"}}