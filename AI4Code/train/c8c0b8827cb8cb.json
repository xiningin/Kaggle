{"cell_type":{"0a2c5d66":"code","83d9edf3":"code","ccfb8b31":"code","62731647":"code","83139c92":"code","1ae44235":"code","85e785e7":"code","e439eb3f":"code","3092108c":"code","e06205de":"code","d5b265e2":"code","c4209d4a":"code","961a75ef":"code","ca58c1e0":"code","8becaed6":"code","fa8cd577":"code","700ed9c4":"code","98c5dc67":"code","0d5df7c0":"code","f6873fd6":"code","3978921f":"code","fbbf780c":"code","c070b472":"code","e5c8aa53":"code","8ec681fd":"code","e33fa136":"code","384540f1":"code","522f008b":"code","94cb1b9d":"code","0486f1ef":"code","589b6075":"code","bb621671":"code","dafd8fb0":"code","07476fcc":"code","978c7234":"code","fe62e01c":"code","485aaf7b":"code","3406a1c5":"code","5a41c6df":"code","6b8cf01e":"code","479044fc":"code","c82b634b":"code","862e230e":"code","7bd86fde":"code","0ea5af85":"code","4c446072":"code","8d05f547":"code","8e9e18e6":"code","0d8c7b52":"code","b9f6d684":"code","39bc42f7":"code","0f2cf46e":"code","09a1db91":"code","c14f5576":"code","0899437c":"code","ed189868":"code","db6fc860":"code","7533da8e":"code","cb36b600":"code","e32b88ab":"code","de02ccc7":"code","027c3ebf":"code","8e62b160":"code","81705255":"code","dee14a94":"code","95f136b9":"code","c1a97dcc":"code","7c10e103":"code","4e6ae516":"code","3ded3218":"code","649b17b0":"code","bd1094db":"code","1ebffd0d":"code","ca97a038":"code","64bc56b3":"code","5e208f11":"code","91887c08":"code","3cc677e6":"code","49341620":"code","5005e6a1":"code","d29782f1":"code","820b02ad":"code","c4dce19b":"code","852bb134":"code","37dabe91":"code","0f5e5682":"code","966cd785":"code","f47d9836":"code","65f25159":"code","ebfcd55e":"code","204071ab":"code","09db0f08":"code","01ffd167":"code","14bf1250":"code","37eea177":"code","28af6308":"code","53d00420":"code","383b3518":"code","33da98a9":"code","a7f03bb4":"code","c2703bd9":"code","5dccfdb0":"code","7c24a76b":"code","b9499419":"code","a37b6b89":"code","6fa9cc57":"code","3b981cc4":"code","afa6b0c9":"code","3fe317fd":"code","45a52022":"code","188b03fc":"code","77b7ea4e":"code","6830191c":"code","4d769971":"code","a27f692e":"code","c43a7207":"code","8f6d001b":"code","012f0e75":"code","63d1a827":"code","0f8ea428":"code","3f0e9986":"code","1271c574":"code","a392f0ab":"code","56ca53af":"code","5f195486":"code","c781ac29":"code","4276916c":"code","b261da24":"code","c547e105":"markdown","fae897b0":"markdown","e3a67a09":"markdown","c4ccbb0a":"markdown","99c99d34":"markdown","08921ed6":"markdown","de01fdee":"markdown","f11aac37":"markdown","794e4370":"markdown","1d6ee7b4":"markdown","ed28805f":"markdown","796630a7":"markdown","6600cec3":"markdown","de80d6a8":"markdown","1e069f44":"markdown","fba04783":"markdown","183b0d2e":"markdown","b59720d9":"markdown","4d6c5ca3":"markdown","2d374a1c":"markdown","ffc0971f":"markdown","da5de59a":"markdown","0e941cc9":"markdown","a4369186":"markdown","27ed7189":"markdown","492bc498":"markdown","28b83cbd":"markdown","68b23ad6":"markdown","5570cac3":"markdown","7d4dcce2":"markdown","778d2cb7":"markdown","6d13956f":"markdown","4913586f":"markdown","7510adaf":"markdown","fec7363b":"markdown","b689c21f":"markdown","abdd363b":"markdown","256fdf66":"markdown","223c6f18":"markdown","b4303634":"markdown","9fea61a5":"markdown","f6e29d11":"markdown","0b5ce8ca":"markdown","3f0666e4":"markdown","71e176fa":"markdown","cddf9d74":"markdown","286c354b":"markdown","ffe09d96":"markdown","5e9cd33b":"markdown","bf260679":"markdown","ad008daf":"markdown","7605cd4a":"markdown","ec9d5d1c":"markdown","7ae7e34b":"markdown","561555f3":"markdown","02482bf1":"markdown","c6100d32":"markdown","b8c0dacb":"markdown","f7651d3d":"markdown","748739d8":"markdown","0f865770":"markdown","7f7a8171":"markdown","2a22cacf":"markdown","767abad2":"markdown","9bbc8805":"markdown","9f3d3e48":"markdown","3019ea96":"markdown","cedcdd40":"markdown","8eaadb35":"markdown","d482a092":"markdown","de00082d":"markdown","c854890a":"markdown","219a0a04":"markdown","d6dc2de1":"markdown","71d6bd57":"markdown","0ce67b77":"markdown","c8cf0481":"markdown","701d6ac9":"markdown","efe4ff89":"markdown","855430af":"markdown","076387fd":"markdown","c55b6172":"markdown","d2662a71":"markdown","dfa6d97f":"markdown","7e4509ae":"markdown","d8d99072":"markdown","d01621bc":"markdown","a68fd3f5":"markdown"},"source":{"0a2c5d66":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')","83d9edf3":"from subprocess import check_output\n\nprint(check_output([\"ls\", \"..\/input\/wine-quality\"]).decode(\"utf8\"))","ccfb8b31":"df = pd.read_csv('..\/input\/wine-quality\/winequalityN.csv')","62731647":"df.info()","83139c92":"print(*df.columns, sep='\\n')","1ae44235":"df.columns = ('type', 'fixed_acidity', 'volatile_acidity', 'citric_acid',\n       'residual_sugar', 'chlorides', 'free_sulfur_dioxide',\n       'total_sulfur_dioxide', 'density', 'pH', 'sulphates', 'alcohol',\n       'quality')","85e785e7":"df.head()","e439eb3f":"import seaborn as sns","3092108c":"sns.heatmap(df.isnull(),yticklabels=False,cbar=False,cmap='viridis')","e06205de":"Sum = df.isnull().sum()\nPercentage = ( df.isnull().sum()\/df.isnull().count())\n\npd.concat([Sum,Percentage], axis =1, keys= ['Sum', 'Percentage'])","d5b265e2":"def null_cell(df): \n    total_missing_values = df.isnull().sum() \n    missing_values_per = df.isnull().sum()\/df.isnull().count() \n    null_values = pd.concat([total_missing_values, missing_values_per], axis=1, keys=['total_null', 'total_null_perc']) \n    null_values = null_values.sort_values('total_null', ascending=False) \n    return null_values[null_values['total_null'] > 0] ","c4209d4a":"fill_list = (null_cell(df)).index","961a75ef":"df_mean = df.copy()\n\nfor col in fill_list:\n    df_mean.loc[:, col].fillna(df_mean.loc[:, col].mean(), inplace=True)","ca58c1e0":"sns.heatmap(df_mean.isnull(),yticklabels=False,cbar=False,cmap='viridis')","8becaed6":"corr_matrix = df_mean.corr()\ncorr_list = corr_matrix.quality.abs().sort_values(ascending=False).index[0:]","fa8cd577":"corr_list","700ed9c4":"plt.figure(figsize=(11,9))\ndropSelf = np.zeros_like(corr_matrix)\ndropSelf[np.triu_indices_from(dropSelf)] = True\n\nsns.heatmap(corr_matrix, cmap=sns.diverging_palette(220, 10, as_cmap=True), annot=True, fmt=\".2f\", mask=dropSelf)\n\nsns.set(font_scale=1.5)","98c5dc67":"from scipy.stats import norm ","0d5df7c0":"plt.figure(figsize = (20,22))\n\nfor i in range(1,13):\n    plt.subplot(5,4,i)\n    sns.distplot(df_mean[df_mean.columns[i]], fit=norm)\n    ","f6873fd6":"df_bins= df_mean.copy()","3978921f":"bins = [0,5,10]\n\n\nlabels = [0, 1] # 'low'=0, 'high'=1\ndf_bins['quality_range']= pd.cut(x=df_bins['quality'], bins=bins, labels=labels)\n\nprint(df_bins[['quality_range','quality']].head(5))\n\ndf_bins = df_bins.drop('quality', axis=1) ","fbbf780c":"plt.figure(figsize=(8,5))\n\nsns.countplot(x = 'type', hue = 'quality_range', data = df_bins)\nplt.show()\n# 'low'=0, 'high'=1","c070b472":"plt.figure(figsize=(8,7))\nsns.scatterplot(x='quality_range', \n                y='alcohol', \n                hue='type',\n                data=df_bins);\nplt.xlabel('Quality',size=15)\nplt.ylabel('Alcohol', size =15)\nplt.show()","e5c8aa53":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 8))\nf.suptitle('Wine Types by Quality & Acidity', fontsize=14)\n\nsns.violinplot(x='quality_range', y='volatile_acidity', hue='type', data=df_bins, split=True, inner='quart', linewidth=1.3,\n               palette={'red': 'red', 'white': 'white'}, ax=ax1)\nax1.set_xlabel(\"Wine Quality Class \",size = 15,alpha=0.8)\nax1.set_ylabel(\"Wine Fixed Acidity\",size = 15,alpha=0.8)\n\nsns.violinplot(x='quality_range', y='alcohol', hue='type', data=df_bins, split=True, inner='quart', linewidth=1.3,\n               palette={'red': 'darkred', 'white': 'white'}, ax=ax2)\nax2.set_xlabel(\"Wine Quality Class\",size = 15,alpha=0.8)\nax2.set_ylabel(\"Wine Fixed Alcohol\",size = 15,alpha=0.8)\nplt.show()","8ec681fd":"plt.figure(figsize= (6,4))\n\nlow_quality = df_bins [df_bins['quality_range']== 0]['chlorides']\nhigh_quality   = df_bins [df_bins['quality_range']== 1][ 'chlorides']\nax = sns.kdeplot(data= low_quality, label= 'low_quality', shade=True, color=None)\nax = sns.kdeplot(data= high_quality,label= 'high_quality',shade=True, color= \"r\")\n\nplt.title(\"Chloride Level in Wine Classes\")\nplt.xlim(0.0,0.3)\nplt.legend()\nplt.show()","e33fa136":"f, (ax1, ax2, ax3) = plt.subplots(3, figsize = (10,10))\n\nf.suptitle('Wine Quality - Acidity Levels', fontsize=14)\n\n\nfixed_acidity_low_quality    = df_bins [df_bins['quality_range']== 0]['fixed_acidity']\nfixed_acidity_high_quality   = df_bins [df_bins['quality_range']== 1]['fixed_acidity']\n\n\nvolatile_acidity_low_quality = df_bins [df_bins['quality_range']== 0]['volatile_acidity']\nvolatile_acidity_high_quality= df_bins [df_bins['quality_range']== 1]['volatile_acidity']\n\ncitric_acid_low_quality      = df_bins [df_bins['quality_range']== 0]['citric_acid']\ncitric_acid_high_quality     = df_bins [df_bins['quality_range']== 1]['citric_acid']\n\n\nsns.kdeplot(data=fixed_acidity_low_quality, label=\"low_quality\", shade=True,ax=ax1)\nsns.kdeplot(data=fixed_acidity_high_quality, label=\"high_quality\", shade=True, ax=ax1)\nax1.set_xlabel(\"fixed_acidity\",size = 15,alpha=0.8)\nax1.set_ylabel(\"Wine Quality\",size = 15,alpha=0.8)\n\n\nsns.kdeplot(data=volatile_acidity_low_quality, label=\"low_quality\", shade=True,ax=ax2)\nsns.kdeplot(data=volatile_acidity_high_quality, label=\"high_quality\", shade=True, ax=ax2)\nax2.set_xlabel(\"volatile_acidity\",size = 15,alpha=0.8)\nax2.set_ylabel(\"Wine Quality\",size = 15,alpha=0.8)\n\n\nsns.kdeplot(data=citric_acid_low_quality, label=\"low_quality\", shade=True,ax=ax3)\nsns.kdeplot(data=citric_acid_high_quality, label=\"high_quality\", shade=True, ax=ax3)\nax3.set_xlabel(\"citric_acid\",size = 15,alpha=0.8)\nax3.set_ylabel(\"Wine Quality\",size = 15,alpha=0.8)\n\n\nplt.legend()\nplt.show()","384540f1":"plt.figure(figsize=(8,5))\n\nresidual_sugar_low   = df_bins [df_bins['quality_range']== 0]['residual_sugar']\nresidual_sugar_high  = df_bins [df_bins['quality_range']== 1]['residual_sugar'] \nax = sns.kdeplot(data= residual_sugar_low, label= 'low quality', shade=True)\nax = sns.kdeplot(data= residual_sugar_high,   label= 'high quality',   shade=True)\n\nplt.title(\"Distributions of Residual Sugar by Wine Qualities\")\nplt.legend()\nplt.show()","522f008b":"plt.figure(figsize=(12,8))\nsns.scatterplot(x='total_sulfur_dioxide', y='free_sulfur_dioxide', hue='quality_range',data=df_bins);\nplt.xlabel('total_sulfur_dioxide',size=15)\nplt.ylabel('free_sulfur_dioxide', size =15)","94cb1b9d":"plt.figure(figsize=(8,7))\n\npH_low_quality  = df_bins [df_bins['quality_range']== 0]['pH']\npH_high_quality = df_bins [df_bins['quality_range']== 1][ 'pH']\nax = sns.kdeplot(data= pH_low_quality, label= 'low_quality', shade=True) \nax = sns.kdeplot(data= pH_high_quality,label= 'high_quality',   shade=True)\n\nplt.title(\"pH Levels in Low\/High Quality Wines\")\nplt.xlabel('pH')\nplt.legend()\nplt.show()","0486f1ef":"plt.figure(figsize=(8,5))\n\ndensity_low_quality  = df_bins [df_bins['quality_range']== 0]['density']\ndensity_high_quality = df_bins [df_bins['quality_range']== 1][ 'density']\nax = sns.kdeplot(data= density_low_quality, label= 'low_quality', shade=True) \nax = sns.kdeplot(data= density_high_quality,label= 'high_quality', shade=True)\n\nplt.title(\"Density Levels in Low\/High Quality of Wines\")\nplt.xlabel('density')\nplt.legend()\nplt.show()","589b6075":"plt.figure(figsize=(8,5))\n\nsulphates_low_quality    = df_mean [df_bins['quality_range']== 0]['sulphates']\nsulphates_high_quality   = df_mean [df_bins['quality_range']== 1][ 'sulphates']\nax = sns.kdeplot(data= sulphates_low_quality, label= 'low_quality',  shade=True) \nax = sns.kdeplot(data= sulphates_high_quality,label= 'high_quality', shade=True)\n\nplt.title(\"Sulphates Levels in Low\/High Quality of Wines\")\nplt.xlabel('sulphates')\nplt.legend()\nplt.show()","bb621671":"outliers_by_12_variables = ['fixed_acidity', 'volatile_acidity', 'citric_acid',\n                            'residual_sugar', 'chlorides', 'free_sulfur_dioxide',\n                            'total_sulfur_dioxide', 'density', 'pH', 'sulphates', 'alcohol'] \nplt.figure(figsize=(22,20))\n\nfor i in range(0,11):\n    plt.subplot(5, 4, i+1)\n    plt.boxplot(df_bins[outliers_by_12_variables[i]])\n    plt.title(outliers_by_12_variables[i])","dafd8fb0":"def winsor(x, multiplier=3): \n    upper= x.median() + x.std()*multiplier\n    for limit in np.arange(0.001, 0.20, 0.001):\n        if np.max(winsorize(x,(0,limit))) < upper:\n            return limit\n    return None ","07476fcc":"from scipy.stats.mstats import winsorize\n\nkolon_isimleri = ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar', 'chlorides', 'free_sulfur_dioxide',\n                                  'total_sulfur_dioxide', 'density', 'pH', 'sulphates', 'alcohol']\n\nfor i in range(1,len(kolon_isimleri)):\n\n    df_bins[kolon_isimleri[i]] = winsorize(df_bins[kolon_isimleri[i]], (0, winsor(df_bins[kolon_isimleri[i]])))","978c7234":"df_bins.type = df_bins.type.map({'white':0, 'red':1})","fe62e01c":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score ","485aaf7b":"X = df_bins[['type', 'alcohol', 'density', 'volatile_acidity', 'chlorides',\n       'citric_acid', 'fixed_acidity', 'free_sulfur_dioxide',\n       'total_sulfur_dioxide', 'sulphates', 'residual_sugar', 'pH']] \ny = df_bins.quality_range\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=40)\n","3406a1c5":"lr = LogisticRegression(random_state=40)\nlr.fit(X_train, y_train)","5a41c6df":"train_accuracy = lr.score(X_train, y_train)\ntest_accuracy = lr.score(X_test, y_test)\nprint('One-vs-rest', '-'*35, \n      'Accuracy in Train Group   : {:.2f}'.format(train_accuracy), \n      'Accuracy in Test  Group   : {:.2f}'.format(test_accuracy), sep='\\n')","6b8cf01e":"from sklearn.metrics import confusion_matrix as cm\n\npredictions = lr.predict(X_test)\nscore = round(accuracy_score(y_test, predictions), 3)\ncm1 = cm(y_test, predictions)\nsns.heatmap(cm1, annot=True, fmt=\".0f\")\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Accuracy Score: {0}'.format(score), size = 15)\nplt.show()","479044fc":"pred_test  = lr.predict(X_test)\npred_train = lr.predict(X_train)","c82b634b":"from sklearn.metrics import confusion_matrix \n\n\ncm = confusion_matrix(y_test,pred_test)\ncm","862e230e":"quality_pred = LogisticRegression(random_state=40)\nquality_pred.fit(X_train,y_train)","7bd86fde":"confusion_matrix_train = confusion_matrix(y_train,pred_train)\nconfusion_matrix_test = confusion_matrix(y_test,pred_test)\n\nprint('Confusion Matrix Train Data', '--'*20, confusion_matrix_train, sep='\\n')\nprint('Confusion Matrix Test Data', '--'*20, confusion_matrix_test, sep='\\n')","0ea5af85":"TN = confusion_matrix_test[0][0]\nTP = confusion_matrix_test[1][1]\nFP = confusion_matrix_test[0][1]\nFN = confusion_matrix_test[1][0]\n\nprint(\"(Total) True Negative       :\", TN)\nprint(\"(Total) True Positive       :\", TP)\nprint(\"(Total) Negative Positive   :\", FP)\nprint(\"(Total) Negative Negative   :\", FN)","4c446072":"FP+FN ","8d05f547":"from sklearn.metrics import accuracy_score\n\nprint(\"Accuracy Score of Our Model     : \",  quality_pred.score(X_test, y_test))\n#print(\"Accuracy Score of Our Model     : \",  accuracy_score(y_test, pred_test)) # same ","8e9e18e6":"Error_Rate = 1- (accuracy_score(y_test, pred_test))  \nError_Rate","0d8c7b52":"from sklearn.metrics import precision_score\n\nprint(\"precision_score()         : \",  precision_score(y_test, pred_test, average='micro'))","b9f6d684":"from sklearn.metrics import recall_score\n\nprint(\"recall_score()            : \",  recall_score(y_test, pred_test, average='micro'))","39bc42f7":"print(\" Specificity Score   : \",  (TN)\/(TN + FP)) ","0f2cf46e":"from sklearn.metrics import f1_score\n\nprecision_s = precision_score(y_test, pred_test,average='micro')\nrecall_s    = recall_score(y_test, pred_test, average='micro')\n\n\nprint(\"F1_score     : \",  2*((precision_s*recall_s)\/(precision_s + recall_s)))\n#print(\"F1_score     : \",  f1_score(y_test, pred_test,average='micro')) #By formula","09a1db91":"from sklearn.metrics import classification_report, precision_recall_fscore_support\n\nprint(classification_report(y_test,pred_test))\n\nprint(\"f1_score        : {:.2f}\".format(f1_score(y_test, pred_test, average='micro')))\nprint(\"recall_score    : {:.2f}\".format(recall_score(y_test, pred_test, average='micro')))\nprint(\"precision_score : {:.2f}\".format(precision_score(y_test, pred_test, average='micro')))\n\nprint('\\n')\nmetrics =  precision_recall_fscore_support(y_test, pred_test)\nprint(\"Precision       :\" , metrics[0]) \n#print(\"Recall          :\" , metrics[1]) \nprint(\"F1 Score        :\" , metrics[2]) ","c14f5576":"probs = quality_pred.predict_proba(X_test)[:,1]  #Predict probabilities for the test data\n\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\nfpr, tpr, thresholds  = roc_curve(y_test, probs) #Get the ROC Curve\n\n\nimport matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(8,5))\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate = 1 - Specificity Score')\nplt.ylabel('True Positive Rate  = Recall Score')\nplt.title('ROC Curve')\nplt.show()","0899437c":"print('AUC De\u011feri : ', roc_auc_score(y_test.values, probs))","ed189868":"from sklearn.metrics import precision_recall_curve\nprecision, recall, _ = precision_recall_curve(y_test, pred_test)\n\nplt.plot(recall, precision)\nplt.show()","db6fc860":"from sklearn.metrics import log_loss\n\nprint(\"Log-Loss)    : \" , log_loss(y_test.values, probs))\nprint(\"Error Rate   : \" , 1- accuracy_score(y_test.values, pred_test))","7533da8e":"C_values = [0.001,0.01,0.1,1,10,100, 1000]\naccuracy_df = pd.DataFrame(columns = ['C_values','Accuracy'])\n\naccuracy_values = pd.DataFrame(columns=['C Value', 'Accuracy Train', 'Accuracy Test'])\n\nfor c in C_values:\n    \n    # Apply logistic regression model to training data\n    lr = LogisticRegression(penalty = 'l2', C = c, random_state = 0)\n    lr.fit(X_train,y_train)\n    accuracy_values = accuracy_values.append({'C Value': c,\n                                                    'Accuracy Train' : lr.score(X_train, y_train),\n                                                    'Accuracy Test': lr.score(X_test, y_test)\n                                                    }, ignore_index=True)\ndisplay(accuracy_values)","cb36b600":"df_mean.head(1)","e32b88ab":"df_bins3= df_mean.copy()","de02ccc7":"df_bins3.type = df_bins3.type.map({'white':0, 'red':1})","027c3ebf":"bins = [0,4,7,10]\n\nlabels = [0,1,2] # 'low'=0,'average'=1, 'high'=2\n\ndf_bins3['quality_range']= pd.cut(x=df_bins3['quality'], bins=bins, labels=labels)\n\n#df_bins3.type = df_bins3.type.map({'white':0, 'red':1})\n\nprint(df_bins3[['quality_range','quality']].head(5))\n","8e62b160":"X = df_bins3[['type', 'alcohol', 'density', 'volatile_acidity', 'chlorides',\n       'citric_acid', 'fixed_acidity', 'free_sulfur_dioxide',\n       'total_sulfur_dioxide', 'sulphates', 'residual_sugar', 'pH']]\ny = df_bins3.quality_range\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=40)\n","81705255":"X_test.head()","dee14a94":"lr    = LogisticRegression(random_state=40)\nlr.fit(X_train, y_train)","95f136b9":"train_accuracy = lr.score(X_train, y_train)\ntest_accuracy = lr.score(X_test, y_test)\nprint('One-vs-rest', '-'*35, \n      'Accuracy Score of Train Model : {:.2f}'.format(train_accuracy), \n      'Accuracy Score of Test  Model : {:.2f}'.format(test_accuracy), sep='\\n')","c1a97dcc":"from sklearn.metrics import confusion_matrix as cm\n\npredictions = lr.predict(X_test)\nscore = round(accuracy_score(y_test, predictions), 3)\ncm1 = cm(y_test, predictions)\nsns.heatmap(cm1, annot=True, fmt=\".0f\")\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Accuracy Score: {0}'.format(score), size = 15)\nplt.show()","7c10e103":"y_pred = lr.predict(X_test)\ny_pred[y_pred == 2]","4e6ae516":"cm = confusion_matrix(y_test,y_pred)\ncm","3ded3218":"quality_pred = LogisticRegression(random_state=40)\nquality_pred.fit(X_train,y_train)","649b17b0":"pred_train = lr.predict(X_train)\npred_test  = lr.predict(X_test)","bd1094db":"confusion_matrix_train = confusion_matrix(y_train,pred_train)\nconfusion_matrix_test = confusion_matrix(y_test,pred_test)\n\nprint('Confusion Matrix Train Data', '--'*20, confusion_matrix_train, sep='\\n')\nprint('Confusion Matrix Test  Data ', '--'*20, confusion_matrix_test, sep='\\n')","1ebffd0d":"#TN = confusion_matrix_test[0][0]\n#TP = confusion_matrix_test[1][1]\n#FP = confusion_matrix_test[0][1]\n#FN = confusion_matrix_test[1][0]\n\nprint(\"(Total) True Negative       :\", TN)\nprint(\"(Total) True Positive       :\", TP)\nprint(\"(Total) Negative Positive   :\", FP)\nprint(\"(Total) Negative Negative   :\", FN)","ca97a038":"from sklearn.metrics import accuracy_score\n\nprint(\"Accuracy Score of Test Model : \",  quality_pred.score(X_test, y_test))","64bc56b3":"Error_Rate = 1 - (accuracy_score(y_test, pred_test))\nError_Rate","5e208f11":"from sklearn.metrics import precision_score\n\nprint(\"precision_score        : \",  precision_score(y_test, pred_test, average='micro'))","91887c08":"from sklearn.metrics import recall_score\n\nprint(\"recall_score        : \",  recall_score(y_test, pred_test, average='micro'))","3cc677e6":"from sklearn.metrics import f1_score\n\nprecision_s = precision_score(y_test, pred_test,average='micro')\nrecall_s    = recall_score(y_test, pred_test, average='micro')\n\n\nprint(\"F1_score     : \",  2*((precision_s*recall_s)\/(precision_s + recall_s)))# by mathematical formula\nprint(\"f1_score()   : \",  f1_score(y_test, pred_test,average='micro'))  #By formula","49341620":"from sklearn.metrics import classification_report, precision_recall_fscore_support\n\nprint(classification_report(y_test,pred_test) )\n\nprint(\"f1_score()         : {:.2f}\".format(f1_score(y_test, pred_test, average='micro')))\nprint(\"recall_score()     : {:.2f}\".format(recall_score(y_test, pred_test, average='micro')))\nprint(\"precision_score()  : {:.2f}\".format(precision_score(y_test, pred_test, average='micro')))\n\nprint('\\n')\nmetrikler =  precision_recall_fscore_support(y_test, pred_test)\nprint(\"Precision   :\" , metrics[0]) \nprint(\"Recall      :\" , metrics[1]) \nprint(\"F1 Score    :\" , metrics[2]) \n\nwarnings.filterwarnings('ignore')","5005e6a1":"from sklearn.preprocessing import LabelBinarizer","d29782f1":"def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n    lb = LabelBinarizer()\n    lb.fit(y_test)\n    y_test = lb.transform(y_test)\n    y_pred = lb.transform(y_pred)\n    return roc_auc_score(y_test, y_pred, average=average)","820b02ad":"print('AUC De\u011feri : ', multiclass_roc_auc_score(y_test.values, y_pred))","c4dce19b":"probs = quality_pred.predict_proba(X_test)[:,1]\n\nfrom sklearn.metrics import roc_curve, roc_auc_score\n\nfpr, tpr, thresholds  = roc_curve(y_test, probs, pos_label=1)\n\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()","852bb134":"from sklearn.metrics import precision_recall_curve\nprecision, recall, _ = precision_recall_curve(y_test, probs, pos_label=1)\n\nplt.plot(precision, recall)\nplt.show()","37dabe91":"C_values = [0.001,0.01,0.1,1,10,100, 1000]\naccuracy_df = pd.DataFrame(columns = ['C_values','Accuracy'])\n\naccuracy_values = pd.DataFrame(columns=['C Value', 'Accuracy Train', 'Accuracy Test'])\n\nfor c in C_values: \n    \n    # Apply logistic regression model to training data\n    lr = LogisticRegression(penalty = 'l2', C = c, random_state = 0)\n    lr.fit(X_train,y_train)\n    accuracy_values = accuracy_values.append({'C Value': c,\n                                                    'Accuracy Train' : lr.score(X_train, y_train),\n                                                    'Accuracy Test': lr.score(X_test, y_test)\n                                                    }, ignore_index=True)\ndisplay(accuracy_values)","0f5e5682":"df_mean_imb = df_mean.copy() ","966cd785":"bins = [0,4,10] \n\n\nlabels = [0, 1] # 'low'=0, 'high'=1 \ndf_mean_imb['quality_range']= pd.cut(x=df_mean_imb['quality'], bins=bins, labels=labels) \n\nprint(df_mean_imb[['quality_range','quality']].head(5)) \n\ndf_mean_imb = df_mean_imb.drop('quality', axis=1) #","f47d9836":"sns.countplot(df_mean_imb.quality_range)\n #'low'=0, 'high'=1\n    \nprint(\"Low Quality  0   : %{:.2f}\".format(sum(df_mean_imb.quality_range)\/len(df_mean_imb.quality_range)*100))\nprint(\"High Quality 1   : %{:.2f}\".format((len(df_mean_imb.quality_range)-sum(df_mean_imb.quality_range))\/len(df_mean_imb.quality_range)*100))","65f25159":"balance = (df_mean_imb.quality_range.value_counts()[1]\/df_mean_imb.quality_range.shape[0])*100\nprint('Data Quality Percentage:\\n', balance,'%')","ebfcd55e":"from sklearn.utils import resample \nfrom imblearn.over_sampling import SMOTE \nsmote = SMOTE() ","204071ab":"df_mean_imb.type = df_mean_imb.type.map({'white':0, 'red':1}) ","09db0f08":"X =  df_mean_imb.drop(['quality_range'], axis=1) \ny =  df_mean_imb.quality_range \n\nX_sm, y_sm =smote.fit_resample(X,y) \n\nprint(X.shape, y.shape) \nprint(X_sm.shape, y_sm.shape) \nsns.countplot(y_sm) ","01ffd167":"def create_model(X, y): \n    X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.20, random_state=40, stratify = y) \n    logreg_model = LogisticRegression() \n    logreg_model.fit(X_train, y_train) \n\n    pred_train = logreg_model.predict(X_train) \n    pred_test = logreg_model.predict(X_test) \n    confusion_matrix_train = confusion_matrix(y_train, pred_train) \n    confusion_matrix_test = confusion_matrix(y_test, pred_test) \n    print(\"Accuracy of Test Model : \",  logreg_model.score(X_test, y_test)) \n    print(\"Train Data Set\") \n    print(classification_report(y_train,pred_train) ) \n    print(\"Test Data Set \") \n    print(classification_report(y_test,pred_test) ) \n    return  None ","14bf1250":"create_model(X_sm,y_sm) \nwarnings.filterwarnings('ignore')","37eea177":"df_bins.head()","28af6308":"X = df_bins.drop(['quality_range'], axis=1)\ny = df_bins.quality_range\ny = np.array(y)","53d00420":"plt.style.use('fivethirtyeight')\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=40)\nprint(\"Number of Rows in    Training dataset :  {} \".format(len(X_train)))\nprint(\"Number of Targets in Training dataset :  {} \".format(len(y_train)))\nprint(\"Number of Rows in    Test dataset :  {} \".format(len(X_test)))\nprint(\"Number of Targets in Test dataset :  {} \".format(len(y_test)))","383b3518":"sns.countplot(y_test)\nplt.ylim((0,1000))","33da98a9":"plt.figure(figsize=(15,9))\ny_list = [y, y_train, y_test]\ntitles = ['All Data','Train Data', 'Test Data']\n\nfor i in range(1,4):\n    plt.subplot(1,3,i)\n    sns.countplot(y_list[i-1])\n    plt.title(titles[i-1])\n    \n","a7f03bb4":"print(\"T\u00fcm veri k\u00fcmesi '0' y\u00fczdesi : %{:.0f} \".format(len(y[y==0])\/len(y)*100))\nprint(\"Test verisi '0' y\u00fczdesi     : %{:.0f} \".format(len(y_test[y_test==0])\/len(y_test)*100))\nprint(\"E\u011fitim verisi '0' y\u00fczdesi   : %{:.0f} \".format(len(y_train[y_train==0])\/len(y_train)*100))","c2703bd9":"LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\ntahmin_e\u011fitim = model.predict(X_train)\ntahmin_test = model.predict(X_test)\nmodel.score(X_test, y_test)","5dccfdb0":"from sklearn.model_selection import KFold \nkf = KFold(n_splits=5, shuffle=True, random_state=40) ","7c24a76b":"X.loc[[3,5]] \n","b9499419":"parcalar = kf.split(X)\nfor num, (train_index, test_index) in enumerate(parcalar): \n    print(\"{}.Training Set Size : {}\".format(num+1,len(train_index)))  \n    print(\"{}.Test Set Size     : {}\".format(num+1,len(test_index))) \n    print('-'*26)","a37b6b89":"from sklearn.metrics import mean_squared_error \n\nmodel2 = LogisticRegression()\npieces = kf.split(X)\naccuracy_list = []\n\nfor i, (egitim_indeks, test_indeks) in enumerate(pieces):\n    \n    X_train, y_train = X.loc[train_index], y[train_index]\n    X_test, y_test = X.loc[test_indeks], y[test_indeks]\n    \n    model2.fit(X_train, y_train)\n    tahmin = model2.predict(X_test)\n    accuracy_value = model2.score(X_test, y_test)  \n    \n    accuracy_list.append(accuracy_value)\n    \n    print(\"{}.Accuracy Value of Pieces: {:.3f}\".format(i+1, accuracy_value))\n    print(\"-\"*30)","6fa9cc57":"print(\"Avarage Accuracy Value : {:.2f}\".format(np.mean(accuracy_list)))","3b981cc4":"from sklearn.model_selection import cross_validate, cross_val_score","afa6b0c9":"lrm = LogisticRegression()\ncv = cross_validate(estimator=lrm,\n                     X=X,\n                     y=y,\n                     cv=10,return_train_score=True\n                    )\nprint('Test Scores            : ', cv['test_score'], sep = '\\n')\nprint(\"-\"*50)\nprint('Train Scores           : ', cv['train_score'], sep = '\\n')","3fe317fd":"print('Mean of Test Set  : ', cv['test_score'].mean())\nprint('Mean of Train Set : ', cv['train_score'].mean())","45a52022":"cv = cross_validate(estimator=lrm, \n                     X=X,\n                     y=y,\n                     cv=10,return_train_score=True,\n                     scoring = ['accuracy', 'r2', 'precision']\n                    )","188b03fc":"print('Test Set Accuracy   Mean      : {:.2f}'.format(cv['test_accuracy'].mean()))\nprint('Test Set R Square   Mean      : {:.2f}'.format(cv['test_r2'].mean()))\nprint('Test Set Precision  Mean      : {:.2f}'.format(cv['test_precision'].mean()))\nprint('Train Set Accuracy  Mean      : {:.2f}'.format(cv['train_accuracy'].mean()))\nprint('Train Set R Square  Mean      : {:.2f}'.format(cv['train_r2'].mean()))\nprint('Train Set Precision Mean      : {:.2f}'.format(cv['train_precision'].mean()))","77b7ea4e":"cv = cross_val_score(estimator=lrm,\n                     X=X,\n                     y=y,\n                     cv=10                    \n                    )\nprint('Model Scores           : ', cv, sep = '\\n')","6830191c":"from sklearn.model_selection import cross_val_predict ","4d769971":"y_pred = cross_val_predict(estimator=lrm, X=X, y=y, cv=10)\nprint(y_pred[0:10])","a27f692e":"logreg = LogisticRegression()\nprint(logreg.get_params())","c43a7207":"parameters = {\"C\": [10 ** x for x in range (-5, 5, 1)],\n                \"penalty\": ['l1', 'l2']\n                }","8f6d001b":"parameters","012f0e75":"from sklearn.model_selection import GridSearchCV\n\n\ngrid_cv = GridSearchCV(estimator=logreg,\n                       param_grid = parameters,\n                       cv = 10\n                      )\ngrid_cv.fit(X, y)","63d1a827":"print(\"The Best Parametre : \", grid_cv.best_params_)\nprint(\"The Best Score     : \", grid_cv.best_score_)","0f8ea428":"results = grid_cv.cv_results_\ndf = pd.DataFrame(results)\ndf.head()","3f0e9986":"df = df[['param_penalty','param_C', 'mean_test_score']]\ndf = df.sort_values(by='mean_test_score', ascending = False)\ndf","1271c574":"#The most successful 10 parametres on a chart.\nplt.style.use('fivethirtyeight')\n\nplt.figure(figsize=(12,12))\n\nsns.scatterplot(x = 'param_C', y = 'mean_test_score', hue = 'param_penalty', data = df[0:10], s=150)\n\nplt.xscale('symlog')\n#plt.ylim((0.9,1))\nplt.show()","a392f0ab":"parametres = {\"C\": [10 ** x for x in range (-5, 5, 1)],\n                \"penalty\": ['l1', 'l2']\n                }","56ca53af":"from sklearn.model_selection import RandomizedSearchCV\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nrs_cv = RandomizedSearchCV(estimator=logreg,\n                           param_distributions = parametres,\n                           cv = 10,\n                           n_iter = 10,\n                           random_state = 111,\n                           scoring = 'precision'\n                      )\nrs_cv.fit(X, y)","5f195486":"print(\"The Best Parametres        : \", rs_cv.best_params_)\nprint(\"All Precisions Values      : \", rs_cv.cv_results_['mean_test_score'])\nprint(\"The Best Precision Value   : \", rs_cv.best_score_)","c781ac29":"results_rs = rs_cv.cv_results_\ndf_rs = pd.DataFrame(results_rs)","4276916c":"results_rs = rs_cv.cv_results_\ndf_rs = pd.DataFrame(results_rs)\ndf_rs = df_rs[['param_penalty','param_C', 'mean_test_score']]\ndf_rs = df_rs.sort_values(by='mean_test_score', ascending = False)\ndf_rs","b261da24":"plt.style.use('fivethirtyeight')\nplt.figure(figsize=(12,12))\nsns.scatterplot(x = 'param_C', y = 'mean_test_score', hue = 'param_penalty', data = df_rs, s=200)\nplt.xscale('symlog')\nplt.ylim((0.0,1))\nplt.show()\n","c547e105":"## General Looking at Results","fae897b0":"# <div align=\"center\"> **2. The Aim of Analysis**","e3a67a09":"It is better to check FP and FN values for another deep study to focus on false predictions for a better target of accurancy and results.  \nA new data set can be created with predictions, X_test and y_test data, than we can check for prediction value of this seperate data set. ","c4ccbb0a":"## Sulphate Values in Wine Quality Classes","99c99d34":"Chloride Level is a bit higher in red wine in contrats with white wine. ","08921ed6":"I will make 10 combination with 'n_iter' parameter.","de01fdee":"### <font color='dark pink'> Precision: Out of all the predicted positive instances, how many were predicted correctly = TP \/ (TP + FP) ) \n","f11aac37":"# <div align=\"center\"> **6. Filling of the Row Data**","794e4370":"## <font color=\"darkblue\"> 7. 1 Creating 2 Bins Model of Two Types of Wine Quality Classes","1d6ee7b4":"# <div align=\"center\"> **7. General Looking at Wine Quality Classes","ed28805f":"### <font color='dark pink'> Accuracy","796630a7":"cross_val_score and cross_validate functions used only test set. In order to have model predictions we can also check cross_val_predict  function.","6600cec3":"##  <font color=\"darkblue\">10.4   Cross Validation Score & Cross Validate","de80d6a8":"#### ***Looking NAN values with heatmap***","1e069f44":"# <div align=\"center\">**8.  Overview about Outliers**","fba04783":"The average accuracy score is calculated from 10 different accuracy scores from the model.\n\nWe still have similiar accuracy scores (.96-.97) by different methods applied previously. ","183b0d2e":"# <div align=\"center\">  **1. Introduction**","b59720d9":"# <div align=\"center\">  **3. General Information of the Data**","4d6c5ca3":"## <font color=\"darkblue\">9.2.a  LogisticRegression","2d374a1c":"# <div align=\"center\"> **4. Data Exploration**","ffc0971f":"#### ***Getting Data***","da5de59a":"Apart from using appropriate function for our model, using the suitable parameter is also an important detail to have accurate predictions. I will use Grid Search and Random Search for this aim. ","0e941cc9":"# <div align=\"center\">  **11. Hyperparameter Tuning**","a4369186":"\n### <font color=\"black\">Type:<font color=\"gray\"> Two types of wines such as red wine and white wine.\n    \n### <font color=\"black\">Fixed acidity:<font color=\"gray\"> Fixed acids include tartaric, malic, citric, and succinic acids which are found in grapes (except succinic)\n\n### <font color=\"gray\">Acids are one of the fundamental properties of wine and contribute greatly to the taste of the wine, Acidity in food and drink tastes tart and zesty. Tasting acidity is also sometimes confused with alcohol. Wines with higher acidity feel lighter-bodied because they come across as \u201cspritzy\u201d. Reducing acids significantly might lead to wines tasting flat. If you prefer a wine that is richer and rounder, you enjoy slightly less acidity.\n\n### <font color=\"black\">Volatile acidity:<font color=\"gray\"> These acids are to be distilled out from the wine before completing the production process. It is primarily constituted of acetic acid though other acids like lactic, formic and butyric acids might also be present. Excess of volatile acids are undesirable and lead to unpleasant flavour.\n\n### <font color=\"black\">Citric acid:<font color=\"gray\"> This is one of the fixed acids which gives a wine its freshness. Usually most of it is consumed during the fermentation process and sometimes it is added separately to give the wine more freshness.\n\n### <font color=\"black\">Residual sugar: <font color=\"gray\">This typically refers to the natural sugar from grapes which remains after the fermentation process stops, or is stopped.\n\n### <font color=\"black\">Chlorides: <font color=\"gray\">Chloride concentration in the wine is influenced by terroir and its highest levels are found in wines coming from countries where irrigation is carried out using salty water or in areas with brackish terrains.\n\n### <font color=\"black\">Free sulfur dioxide:<font color=\"gray\"> This is the part of the sulphur dioxide that when added to a wine is said to be free after the remaining part binds. Winemakers will always try to get the highest proportion of free sulphur to bind. They are also known as sulfites and too much of it is undesirable and gives a pungent odour.\n\n### <font color=\"black\">Total sulfur dioxide:<font color=\"gray\"> This is the sum total of the bound and the free sulfur dioxide. This is mainly added to kill harmful bacteria and preserve quality and freshness. There are usually legal limits for sulfur levels in wines and excess of it can even kill good yeast and give out undesirable odour.\n\n### <font color=\"black\">Density: <font color=\"gray\">This can be represented as a comparison of the weight of a specific volume of wine to an equivalent volume of water. It is generally used as a measure of the conversion of sugar to alcohol. \n\n### <font color=\"black\">pH: <font color=\"gray\"> Also known as the potential of hydrogen, this is a numeric scale to specify the acidity or basicity the wine. Fixed acidity contributes the most towards the pH of wines. You might know, solutions with a pH less than 7 are acidic, while solutions with a pH greater than 7 are basic. With a pH of 7, pure water is neutral. Most wines have a pH between 2.9 and 3.9 and are therefore acidic.\n\n### <font color=\"black\">Sulphates: <font color=\"gray\">These are mineral salts containing sulfur. Sulphates are to wine as gluten is to food. They are a regular part of the winemaking around the world and are considered essential. They are connected to the fermentation process and affects the wine aroma and flavour. \n\n### <font color=\"black\">Alcohol: <font color=\"gray\"> It's usually measured in % vol or alcohol by volume (ABV).\n\n### <font color=\"black\">Quality:<font color=\"gray\"> Wine experts graded the wine quality between 0 (very bad) and 10 (very excellent). The eventual quality score is the median of at least three evaluations made by the same wine experts.\n","27ed7189":"We splitted y values equally and trained our model.However, in order to see X values distribution we need following Cross Validation Measurement.","492bc498":"# <div align=\"center\">  **10. Imbalanced Data**","28b83cbd":"## Density by Wine Quality Classes","68b23ad6":"#### Confusion Matrix in array format","5570cac3":"## Sulfur Dioxide Distribution in Wine Quality Classes","7d4dcce2":"\nIn the beginning of this study, I checked general characteristic of the data set. Data has some NULL values. Even though, dropping missing values is still an option due to low percentage of missing values in data, I preferred to filled them by the mean of data. \n\nData set shows that red wine is very reach in wine quality with a high correlation with alcohol. \n\nI also looked at quality levels in each variable by using suitable charts for a general understanding.  \n\nFollowing sections, I searched for 2 different types of models with different bins. Behind this study I created many models for a better accuracy and recall scores. This study only shows the best model with good scores and predictions.\n\nThe first model was included 2 bins with all variables in a quality range of 0-5,5-10. This model gives %0.74 accuracy score on train and test samples. \n\ndf_bins3 data frame was split in 3 different bins to check accuracy levels. First bin was between 0-5,5-6,6-10 range. This model gives score of %0.58.  \n\nOn the other hand, when bins are arranged by following 0-4,4-7,7-10; score reached 0.93%. I continued with this model for the further steps on other performance measurements. \n\nA general note: These results for imbalance data, thus I would like to see scores after balancing data set. Due to this reason, and to check the difference between logistic regression model, I resampled imbalance data. In previous steps, I added bins in low and high ranges on quality variable, this section will show the results by using resampling method.\nHowever, having very high scores and a negative R square show that data set needs another approach at the end. For further studies, more suitable data set can be chosen. \n\nA Quick Note for Resampling Data: Splitting data in 2 parts from 0 to 5 gives a balance distribution. However, when we split data  from 0 to 4 in the first bin, I got an imbalance data. \nAfter resampling our data, I needed to switch y values in array format to have Cross Validation scores. \n\nGenerally, our measurements and model scores worked well to show the aim of the study. This study can be completed in a shorter way as well without repeating similar functions; however, this study also aims to use different methods to have accurate scores from variable sources. \n\nI focused on classification methods on this study. However, I agree that other algorithms can be more successful such as Random forest and Boosting algorithms give better results. I will use these methods in my next kernel.\n","778d2cb7":"### <font color='dark pink'> Error Rate","6d13956f":"## <font color=\"darkblue\">10.2 Cross Validation with 2  Bins Model","4913586f":"##  <font color=\"darkblue\">10.3 K-Fold Cross Validation","7510adaf":"### <font color='dark pink'> Error Rate","fec7363b":"## <font color=\"darkblue\">9.2.b   Performance Measurements","b689c21f":"## <font color=\"darkblue\"> 9.1 Creating Train \/ Test Groups with 2 Bins Model ","abdd363b":"### <font color='dark pink'> Hassasiyet (Precision)","256fdf66":"## Quality in  Different Wine Types","223c6f18":"As we see on the chart, Low quality red wine has the highest numerical value in data set as well as low quality white wine. \nHigh quality white and red wines have little place in data. ","b4303634":"We splited our function in 5 pieces and trained them with Kfold method. In the following section, Cross Validate and Cross Validation Score tools will do everything itself.  ","9fea61a5":"#### ***About data***","f6e29d11":"### <font color='dark pink'> PRECISION RECALL CURVE","0b5ce8ca":"## Contents\n1.  Introduction\n2.  The Aim of Analysis\n3.  General Information of the Data\n4.  Data Exploration\n5.  Checking for NULL Values \n6.  Filling of the Row Data \n7.  General Looking at Wine Quality Classes\n     * 7. 1 Creating 2 Bins Model of Two Types of Wine Quality Classes\n8.  Overview about Outliers \n     * 8.1    Winsorization\n9.  LOGISTIC REGRESSION CLASSIFIER\n     * 9.1    Creating Train \/ Test Groups with 2 Bins Model \n     * 9.1.a  LogisticRegression\n     * 9.1.b  Performance Measurements\n     * 9.2    Creating Train \/ Test Groups with 3 Bins Model\n     * 9.2.a  LogisticRegression\n     * 9.2.b  Performance Measurements\n10.  Imbalanced Data\n     * 10.1   Resampling Imbalance Data\n     * 10.2   Cross Validation\n     * 10.3   K-Fold Cross Validation\n     * 10.4   Cross_val_score & Cross_validate \n11. Hyperparameter Tuning\n     * 11.1   Grid Search\n     * 11.2   RandomizedSearchCV","3f0666e4":"Wine quality has the highest correlation with alcohol. Other relation degrees are very low with each other,such as citric acid,free_sulfur_dioxide, sulphates and pH.\nQuality also has a low negative correlation with density,volatile acidity, chlorides, total_sulfur_dioxide and residual_sugar. ","71e176fa":"# <font color=\"red\"> <div align=\"center\"> CLASSIFICATION PROJECT  \n    \n## <div align=\"center\"> WINE QUALITY\n","cddf9d74":"## <font color=\"darkblue\"> 8.1 Winsorization","286c354b":"## Residual Sugar Levels by Wine Quality Classes","ffe09d96":"#### ***First 5 rows***","5e9cd33b":"There are some extreme values in low quality wine class. Total sulfur dioxide level is getting higher in some low quality wine class while general disturubution is standing up to 100 level of free sulfur dioxide.   ","bf260679":"#### ***Switching Column Names into a suitable format***","ad008daf":"## <font color=\"darkblue\"> 9.1.b   Performance Measurements","7605cd4a":"# <div align=\"center\">**9. LOGISTIC REGRESSION CLASSIFIER**","ec9d5d1c":"## pH Level in Wine Quality","7ae7e34b":"When splitting data in two parts starting from four, it gives an imbalanced data. ","561555f3":"### <font color='dark pink'> ROC\/AUC(Area Under Curve)","02482bf1":"### <font color='dark pink'> Specificity :(TN)\/(TN + FP)) ","c6100d32":"### <font color='dark pink'> Log Loss (calculating the difference between ground truth and predicted score for every observation and average those errors over all observations. )","b8c0dacb":"While we checked all combinations of our parameters with Grid Search method, we can also use this function with desired number of conbinations of parameters. ","f7651d3d":"## Quality & Volatile Acidity by Types","748739d8":"## Chlorides Level in Quality Classes ","0f865770":"Red and White wines has similar results on the chart. High quality wines are mostly red wines and have more alcohol level.","7f7a8171":"In order to have suitable parametres I used get_params() function. ","2a22cacf":"There is more low quality wine in between 0.4 and 0.6 levels of sulphate levels. Both quality classes have similar values.","767abad2":"### <font color='dark pink'> Recall ( Out of all the positive classes, how many instances were identified correctly = TP \/ (TP + FN)) ","9bbc8805":"### <font color='dark pink'>  Duyarl\u0131l\u0131k (Recall\/Sensitivity)","9f3d3e48":"In order to have all variables in numeric data, I mapped wine types as following by using the previous data frame 'df_bins':","3019ea96":"#### ***Checking for NULL Values***","cedcdd40":"### 0. Importing Packages","8eaadb35":"In order to see the differency between logistic regression model, I also would like to check resampling imblance data. In previous steps, I added bins in low and high ranges on quality variable, this section will show the results by using resampling method.  ","d482a092":"### <font color='dark pink'>PRECISION RECALL CURVE \n(The precision is the ratio tp \/ (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative.\n\nThe recall is the ratio tp \/ (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples.)","de00082d":"### <font color='dark pink'> Accuracy","c854890a":"# <div align=\"center\"> **5. Checking for NULL Values**","219a0a04":"### General Looking at Results ","d6dc2de1":" #### ***Distribution  of Variables***","71d6bd57":"# <font color=\"darkblue\">9.2 Creating 3 Bins Models by a large Margin","0ce67b77":"### <font color='dark pink'> ROC\/AUC(Area Under Curve)","c8cf0481":"Fixed acidity level is low in both wine classes, especially in white wine while red wine has more in low quality class up to 1.70.  Fixed alcohol level is again high in red wine class comparing white wine in low quality. High quality class has the highest fixed alcohol level in booth wine classes. ","701d6ac9":"### <font color=\"gray\">The dataset was downloaded from the UCI Machine Learning Repository.\n\n### <font color=\"gray\">The two datasets are related to red and white variants of the Portuguese \"Vinho Verde\" wine","efe4ff89":"## Quality & Alcohol Relation ","855430af":"## <font color=\"darkblue\">10.1 Resampling Imbalance Data","076387fd":"### <font color='dark pink'> F1-Score: From Precision and Recall, F-Measure is computed and used as metrics sometimes. F \u2013 Measure is nothing but the harmonic mean of Precision and Recall =(2 * Recall * Precision) \/ (Recall + Precision) )","c55b6172":"### <font color='dark pink'>  F1 (F1 Score)","d2662a71":"#### Confusion Matrix in Chart","dfa6d97f":"## <font color=\"darkblue\"> 9.1.a  LogisticRegression","7e4509ae":"### <font color=\"gray\"> This study aims to search for the elements which effects WINE QUALITY by using multiclass  decision classification methods such as Support Vector Machines, K-NN, Logistic Regression, Softmax, Confusion Matrix, Accuracy, Precision, Specificity, F1 Score, ROC\/AUC, Logarithmic Loss, Cross Validation, K-Fold Cross Validation, Grid Search, SMOTE ","d8d99072":"##  <font color=\"darkblue\">11.2 RandomizedSearchCV","d01621bc":"## Fixed Acidity & Volatile Acidity & Citric Acid Density in Quality Classes","a68fd3f5":"##  <font color=\"darkblue\">11.1 Grid Search"}}