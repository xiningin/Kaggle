{"cell_type":{"eade7429":"code","b605c17f":"code","2f249841":"code","c3f02968":"code","bf462a89":"code","b620b615":"code","60b85ed7":"code","2eebba6a":"code","a3ef6d9e":"code","54498dc3":"code","4c05c529":"code","b3b2d946":"code","ffcd0f8c":"code","4b0f1494":"code","145aad11":"code","ca4f52cb":"code","ceaf0bce":"code","a75fa1a3":"code","29949fbb":"code","685cb29a":"code","d635a8b4":"code","3bc404f8":"code","083627e8":"code","a7e1fa4c":"code","b5aaffa1":"code","3ec235f7":"code","ed640e58":"code","a4b93c3a":"code","a7f5d7ba":"code","f31859e5":"code","e40cacc7":"markdown","80da3092":"markdown","c324dc83":"markdown","1e414344":"markdown","d5a333aa":"markdown","4656c572":"markdown","2fbd0a39":"markdown","a7e72b5c":"markdown","2c4a8a7b":"markdown","43fe182b":"markdown","1faece72":"markdown","82d7904f":"markdown","fbcd1a55":"markdown","e2482406":"markdown","fe888cc8":"markdown","6c439122":"markdown"},"source":{"eade7429":"import pandas as pd\npd.options.display.max_columns = 100\n\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nimport seaborn as sns\nsns.set()\nimport pylab as plot","b605c17f":"train = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2022\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2022\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/tabular-playground-series-feb-2022\/sample_submission.csv\")","2f249841":"train.head()","c3f02968":"print(f'Number of rows: {train.shape[0]};  Number of columns: {train.shape[1]}; No of missing values: {sum(train.isna().sum())}')","bf462a89":"train.info()\n","b620b615":"train.describe().style.background_gradient(cmap=\"Pastel1\")","60b85ed7":"target_count = train['target'].value_counts().sort_index()\ntarget_count_df = pd.DataFrame(target_count)\n#pd.options.display.float_format = '{:,.2f}%'.format\ntarget_count_df['target(%)'] = (target_count_df\/target_count.sum()*100)\ntarget_count_df.sort_values('target(%)', ascending=False, inplace=True)\ndisplay(target_count_df)","2eebba6a":"fig, ax = plt.subplots(1, 1, figsize=(17, 8))\n\ntarget_count = train['target'].value_counts().sort_index()\n\nax.bar(target_count.index, target_count, color=['#1520E6' if i%2==0 else '#93D1FF' for i in range(9)],\n       width=0.55, \n       edgecolor='black', \n       linewidth=0.7)\n\nax.margins(0.02, 0.05)\n\nfor i in range(1,2):\n    ax.annotate(f'{target_count[i]\/len(train)*100:.3}', xy=(i, target_count[i]+1000),\n                   va='center', ha='center',\n               )\n#Annotate the point xy with text text.\n\n#In the simplest form, the text is placed at xy.\n\nax.set_title('target Distribution', weight='bold', fontsize=15)\nax.grid(axis='y', linestyle='-', alpha=0.4)\n\nfig.tight_layout()\nplt.show()","a3ef6d9e":"colors_4 = ['magenta','yellow','orange','red','maroon','blue','purple','lime','chocolate','silver']\ntarget_count.plot.pie(subplots=True, figsize=(20,10), labels=target_count.index,autopct='%1.1f%%', colors=colors_4)\nplt.show()","54498dc3":"from sklearn.preprocessing import StandardScaler, LabelEncoder\nlabelencoder=LabelEncoder()\ntrain['target']     = labelencoder.fit_transform(train['target'])\nsubmission['target']     = labelencoder.fit_transform(submission['target'])\n","4c05c529":"train.drop([\"row_id\"] , axis = 1 , inplace = True)\ny=train['target']\nX=train.drop(labels=['target'], axis=1)","b3b2d946":"### It will zero variance features\nfrom sklearn.feature_selection import VarianceThreshold\nvar_thres=VarianceThreshold(threshold=0)\nvar_thres.fit(X)","ffcd0f8c":"var_thres.get_support()","4b0f1494":"### Finding non constant features\nsum(var_thres.get_support())","145aad11":"# Lets Find non-constant features \nlen(X.columns[var_thres.get_support()])","ca4f52cb":"constant_columns = [column for column in X.columns\n                    if column not in X.columns[var_thres.get_support()]]\n\nprint(len(constant_columns))","ceaf0bce":"# with the following function we can select highly correlated features\n# it will remove the first feature that is correlated with anything other feature\n\ndef correlation(dataset, threshold):\n    col_corr = set()  # Set of all the names of correlated columns\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: # we are interested in absolute coeff value\n                colname = corr_matrix.columns[i]  # getting the name of column\n                col_corr.add(colname)\n    return col_corr","a75fa1a3":"corr_features = correlation(train, 0.7)\nlen(set(corr_features))","29949fbb":"from sklearn.datasets import load_boston\nX.corr()","685cb29a":"test.head()","d635a8b4":"print(f'Number of rows: {test.shape[0]};  Number of columns: {test.shape[1]}; No of missing values: {sum(test.isna().sum())}')","3bc404f8":"test.describe().style.background_gradient(cmap=\"Pastel1\")","083627e8":"test.drop([\"row_id\"] , axis = 1 , inplace = True)\nx_test=test","a7e1fa4c":"submission.head()","b5aaffa1":"submission.drop([\"row_id\"] , axis = 1 , inplace = True)\ny_test=submission.target","3ec235f7":"# calculate manually\ndef my_function(y,y_preds):\n  \n  d = y - y_preds\n  mse_f = np.mean(d**2)\n  mae_f = np.mean(abs(d))\n  rmse_f = np.sqrt(mse_f)\n\n\n  print(\"Results by manual calculation:\")\n  print(\"MAE:\",mae_f)\n  print(\"MSE:\", mse_f)\n  print(\"RMSE:\", rmse_f)","ed640e58":"from numpy import mean\nfrom numpy import std\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom lightgbm import LGBMClassifier\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","a4b93c3a":"import lightgbm\nfrom sklearn.metrics import roc_auc_score\n#Step2: Create a simple Light GBM Model and evaluate performance\n#LightGBM has function Dataset to read the data. This is required for using LightGBM\ntrain_data = lightgbm.Dataset(X_train, label=y_train)\nvalid_data = lightgbm.Dataset(X_valid, label=y_valid)\n\nparameters = {'objective': 'binary',\n              'metric': 'auc',\n              'is_unbalance': 'true',\n              'boosting': 'gbdt',\n              'num_leaves': 63,\n              'feature_fraction': 0.5,\n              'bagging_fraction': 0.5,\n              'bagging_freq': 20,\n              'learning_rate': 0.01,\n              'verbose': -1\n             }\n\nmodel_lgbm = lightgbm.train(parameters,\n                            train_data,\n                            valid_sets=valid_data,\n                            num_boost_round=5000,\n                            early_stopping_rounds=50)\ny_train_pred = model_lgbm.predict(X_train)\ny_valid_pred = model_lgbm.predict(X_valid)\n\nprint(\"AUC Train: {:.4f}\\nAUC Valid: {:.4f}\".format(roc_auc_score(y_train, y_train_pred),\n                                                    roc_auc_score(y_valid, y_valid_pred)))","a7f5d7ba":"# castboost\n#importing library and building model\nfrom catboost import CatBoostRegressor\nmodel=CatBoostRegressor(iterations=50, depth=5, learning_rate=0.1, loss_function='RMSE')\nmodel.fit(X_train, y_train,eval_set=(X_valid, y_valid), verbose=True,plot=True)","f31859e5":"print('lightgbm model')\ny_preds_lgbm = model_lgbm.predict(x_test )\nmy_function(y_test,y_preds_lgbm)\nprint('*********************************')\n\n\nprint('*********************************')\nprint(\" castboost   model\")\ny_preds_catboost = model.predict(x_test )\nmy_function(y_test,y_preds_catboost)","e40cacc7":"#  Introduction to LightGBM ","80da3092":"Feature Selection- Dropping constant features In this step we will be removing the features which have constant features which are actually not important for solving the problem statement","c324dc83":"Summarie and statistics\u00b6","1e414344":"Feature Selection- With Correlation\u00b6 In this step we will be removing the features which are highly correlated","d5a333aa":"Load and check data","4656c572":"Infos","2fbd0a39":"**LightGBM Classifier in Python\u00b6**\n\n\nIn this kernel, I will discuss one of the most successful ML algorithm LightGBM Classifier. LightGBM is a fast, distributed, high performance gradient boosting framework based on decision tree algorithms, used for ranking, classification and many other machine learning tasks. It has helped Kagglers win data science competitions.\n\nSo, let's get started.","a7e72b5c":"The dimension and number of missing values in the train dataset is as below:","2c4a8a7b":"# * * LightGBM vs. CatBoost: Which is better?\nAll of LightGBM, and CatBoost have the ability to execute on either CPUs or GPUs for accelerated learning, but their comparisons are more nuanced in practice. Each framework has an extensive list of tunable hyperparameters that affect learning and eventual performance.\n\nFirst off, CatBoost is designed for categorical data and is known to have the best performance on it, showing the state-of-the-art performance over XGBoost and LightGBM in eight datasets in its official journal article. As of CatBoost version 0.6, a trained CatBoost tree can predict extraordinarily faster than either XGBoost or LightGBM.\n\nOn the flip side, some of CatBoost\u2019s internal identification of categorical data slows its training time significantly in comparison to XGBoost, but it is still reported much faster than XGBoost. LightGBM also boasts accuracy and training speed increases over XGBoost in five of the benchmarks examined in its original publication. \n\nBut to XGBoost\u2019s credit, XGBoost has been around the block longer than either LightGBM and CatBoost, so it has better learning resources and a more active developer community. It also doesn\u2019t hurt that XGBoost is substantially faster and more accurate than its predecessors and other competitors such as Scikit-learn.\n\nEach boosting technique and framework has a time and a place\u2014and it is often not clear which will perform best until testing them all. Fortunately, prior work has done a decent amount of benchmarking the three choices, but ultimately it\u2019s up to you, the engineer, to determine the best tool for the job.","43fe182b":"# * Catboost\nCatboost uses a combination of one-hot encoding and an advanced mean encoding. For features with low number of categories, it uses one-hot encoding. The maximum number of categories for one-hot encoding can be controlled by the one_hot_max_size parameter. For the remaining categorical columns, CatBoost uses an efficient method of encoding, which is similar to mean encoding but with an additional mechanism aimed at reducing overfitting. Using CatBoost\u2019s categorical encoding comes with a downside of a slower model.","1faece72":"Below is the first 5 rows of test dataset:","82d7904f":"![tree.png](https:\/\/miro.medium.com\/max\/700\/1*i0CA9ho0WArOj-0UdpuKGQ.png)","fbcd1a55":"\nCatBoost has the flexibility of giving indices of categorical columns so that it can be encoded as one-hot encoding using one_hot_max_size (Use one-hot encoding for all features with number of different values less than or equal to the given parameter value).\nIf you don\u2019t pass any anything in cat_features argument, CatBoost will treat all the columns as numerical variables","e2482406":"LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n\nFaster training speed and higher efficiency.\nLower memory usage.\nBetter accuracy.\nSupport of parallel and GPU learning.\nCapable of handling large-scale data.\nAt present, decision tree based machine learning algorithms dominate Kaggle competitions. The winning solutions in these competitions have adopted an alogorithm called XGBoost.\n\nA couple of years ago, Microsoft announced its gradient boosting framework LightGBM. Nowadays, it steals the spotlight in gradient boosting machines. Kagglers start to use LightGBM more than XGBoost. LightGBM is 6 times faster than XGBoost.\n\nLight GBM is a relatively new algorithm and have long list of parameters given in the LightGBM documentation,\n\nThe size of dataset is increasing rapidly. It is become very difficult for traditional data science algorithms to give accurate results. Light GBM is prefixed as Light because of its high speed. Light GBM can handle the large size of data and takes lower memory to run.\n\nAnother reason why Light GBM is so popular is because it focuses on accuracy of results. LGBM also supports GPU learning and thus data scientists are widely using LGBM for data science application development.\n\nIt is not advisable to use LGBM on small datasets. Light GBM is sensitive to overfitting and can easily overfit small data.","fe888cc8":"Below is the first 5 rows of test dataset:","6c439122":"Summarie and statistics\u00b6"}}