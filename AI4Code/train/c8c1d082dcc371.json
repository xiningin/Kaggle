{"cell_type":{"809b2dc4":"code","a8c7e235":"code","d942a60c":"code","a3a07168":"code","88c243e4":"code","451b1d8c":"code","da1e32b1":"code","9b313322":"code","ba5ccc92":"code","4e00d3e7":"code","b0122299":"markdown","4a067aee":"markdown","c5cb4d72":"markdown","a1410d76":"markdown","838565f0":"markdown","5039904d":"markdown","528fa45e":"markdown","5f710552":"markdown"},"source":{"809b2dc4":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\ncheckpoint = \"bert-base-uncased\"","a8c7e235":"train_file = '..\/input\/nlp-getting-started\/train.csv'\ntest_file = '..\/input\/nlp-getting-started\/test.csv'\n\ntrain_df = pd.read_csv(train_file).fillna('unknown')\ntest_df = pd.read_csv(test_file).fillna('unknown')\ntrain_df.sample(3)","d942a60c":"from sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(train_df.text, train_df.target, train_size=0.66, random_state=0, shuffle=True)\n\nassert len(X_train) == len(y_train)\nassert len(X_valid) == len(y_valid)\n\ntrain_df.nunique()","a3a07168":"from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\n\ndef tokenize_dataset(dataset, return_tensors='tf'):\n    if (pd.Series== type(dataset)):\n        dataset = dataset.to_list()\n    encoded = tokenizer(dataset,\n        padding=True,\n        truncation=True,\n        return_tensors=return_tensors,\n    )\n    return encoded.data\n\ntokenized_datasets = {}\ntokenized_datasets['train'] = tokenize_dataset(X_train, 'np')\ntokenized_datasets['labels_train'] = tf.convert_to_tensor(y_train)\nassert len(tokenized_datasets['train']['input_ids']) == len(tokenized_datasets['labels_train']), \"Training data and labels don't have the same size\"\n\ntokenized_datasets['valid'] = tokenize_dataset(X_valid, 'np')\ntokenized_datasets['labels_valid'] = tf.convert_to_tensor(y_valid)\nassert len(tokenized_datasets['valid']['input_ids']) == len(tokenized_datasets['labels_valid']), \"Validation data and labels don't have the same size\"\n\n# For submission\ntokenized_datasets['test'] = tokenize_dataset(test_df.text, return_tensors='tf')","88c243e4":"from transformers import TFAutoModelForSequenceClassification\n\nmodel = TFAutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)","451b1d8c":"from tensorflow.keras.optimizers.schedules import PolynomialDecay\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\n\nbatch_size = 8\nnum_epochs = 10\n\n# The number of training steps is the number of samples in the dataset, divided by the batch size then multiplied\n# by the total number of epochs\nnum_train_steps = (len(tokenized_datasets['train']['input_ids']) \/\/ batch_size) * num_epochs\nlr_scheduler = PolynomialDecay(\n    initial_learning_rate=5e-5,\n    end_learning_rate=0.,\n    decay_steps=num_train_steps\n    )\nopt = Adam(learning_rate=lr_scheduler)\n\n# Loss\nloss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n# Model\nmodel.compile(optimizer=opt,\n              loss=loss,\n              metrics=['accuracy'])","da1e32b1":"model.fit(\n    tokenized_datasets['train']['input_ids'],\n    tokenized_datasets['labels_train'],\n    validation_data=(\n        tokenized_datasets['valid']['input_ids'],\n        tokenized_datasets['labels_valid'],\n    )\n    ,batch_size = batch_size\n    ,epochs = num_epochs\n)","9b313322":"preds = model.predict(tokenized_datasets['test'])\npreds_df = pd.DataFrame(data=[int(x[0]>x[1]) for x in preds[0]],\n                        index=test_df.id,\n                        columns=['target'])\npreds_df.sample(9)","ba5ccc92":"preds_df.value_counts()","4e00d3e7":"working_directory='\/kaggle\/working'\npreds_df.to_csv(f'{working_directory}\/outputs.csv')\nmodel.save_pretrained(f\"{working_directory}\/model\")\ntokenizer.save_pretrained(f\"{working_directory}\/tokenizer\")","b0122299":"# The base model\nThis kernel is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n```python\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n```\nInput data files are available in the read-only `..\/input\/` directory.<br>\nYou can write up to 20GB to the current directory `\/kaggle\/working\/` that gets preserved as output when you create a version using \"Save & Run All\"   \n   You can also write temporary files to `\/kaggle\/temp\/`, but they won't be saved outside of the current session","4a067aee":"# Saving outputs","c5cb4d72":"# Training","a1410d76":"## Tokenization","838565f0":"## Set up","5039904d":"## Optimizer","528fa45e":"## Splits","5f710552":"# Loading data\n"}}