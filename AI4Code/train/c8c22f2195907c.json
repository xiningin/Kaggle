{"cell_type":{"22ff7bb3":"code","6d139fbf":"code","4a1d5e1f":"code","f3e08c8a":"code","eb2612b9":"code","7f5be2e3":"code","2bc53683":"code","9c2138d0":"code","fc9e3796":"code","ae464f1e":"code","d76ca529":"code","b0c532f9":"code","acff9fd1":"code","21a35d72":"code","7b49acf8":"code","ba52919f":"code","7f6f1338":"code","f7a6b4d2":"code","c9813f9e":"code","6888f51e":"code","4bdaea02":"code","b6388530":"code","7086717b":"code","e2c33f4f":"code","9cfadcab":"code","01259a2f":"code","617100e2":"markdown","ec59a5b6":"markdown","3880467f":"markdown","6dad2dd9":"markdown","c8cfea8c":"markdown","c0494f33":"markdown","91be4486":"markdown","9492c02e":"markdown","0244f82d":"markdown","c506c3c1":"markdown"},"source":{"22ff7bb3":"import numpy as np\nimport pandas as pd\n\nimport scipy\nimport librosa #library for working with audio files\nimport matplotlib.pyplot as plt\n\nimport IPython.display as ipd\n\n\nfrom tqdm.notebook import tqdm\n\nimport seaborn as sns\n\nimport librosa.display\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nsns.set()","6d139fbf":"data = pd.read_csv(\"..\/input\/musical-emotions-classification\/MetaData.csv\")\nprint(data.shape)\ndata['Paths'] = \"..\/input\/musical-emotions-classification\/Audio_Files\/Audio_Files\/\"+data.Target+\"\/\"+data.ImageID\ndata.head()","4a1d5e1f":"print(data.Target.value_counts(),len(data.GroupID.unique())) #114 tunes have been chunked into 2126 tunes","f3e08c8a":"s1,sr1 = librosa.load(data.Paths.iloc[0])\nipd.Audio(s1,rate = sr1) #Happy Tune","eb2612b9":"s2,sr2 = librosa.load(data.Paths.iloc[1189])\nipd.Audio(s2,rate=sr2) #Sad Tune","7f5be2e3":"fig,ax = plt.subplots(1,2,figsize=(20,5))\nlibrosa.display.waveplot(s1,sr1,ax=ax[0])\nlibrosa.display.waveplot(s2,sr2,ax=ax[1])\nax[0].set_title(\"Happy Tune\")\nax[1].set_title(\"Sad Tune\")\nfor i in range(2):\n    ax[i].set_ylabel(\"Amplitude\")","2bc53683":"sns.distplot(data.Shapes.value_counts(),kde=False)","9c2138d0":"main_data = data[data.Shapes>=220032]\nprint(main_data.Shapes.value_counts())","fc9e3796":"max_shape = max(main_data.Shapes.values)\nprint(f\"The maximum values is {max_shape}\")","ae464f1e":"main_data.Paths.iloc[0].split(\"\/\")","d76ca529":"def compute_spec_features(paths):\n    #we are going to compute the spectral centroids\n    spec_centr = [] #list to append the spectral centroids of the audio files\n    zero_crss=[]\n    for i in tqdm(range(len(paths))):\n        song,samp_rate = librosa.load(paths[i]) #Path to load the music\n        song = librosa.util.fix_length(song,max_shape,axis=0)\n        target = paths[i].split(\"\/\")[6] # returns the class name to the variable\n        spec_centr.insert(i,[librosa.feature.spectral_centroid(song,samp_rate),librosa.feature.zero_crossing_rate(song,samp_rate),target])#inserts according to the indices\n    return spec_centr","b0c532f9":"spec_features = compute_spec_features(main_data.Paths.values)","acff9fd1":"spec_cntr_df = pd.DataFrame(index=np.arange(2012),columns=[f\"Spec_Centr{x}\" for x in range(432)])\nzero_crs_df = pd.DataFrame(index=np.arange(2012),columns=[f\"Zero_Cross{y}\" for y in range(432)])","21a35d72":"for i in range(2012):\n    spec_cntr_df.loc[i] = spec_features[i][0]\n    zero_crs_df.loc[i] = spec_features[i][1]","7b49acf8":"main_df = pd.concat([spec_cntr_df,zero_crs_df],axis=1)","ba52919f":"spec_features[0][2].split(\".\")[0][0:-5]","7f6f1338":"main_df['class'] = [\"NaN\"]*2012","f7a6b4d2":"main_df['class']","c9813f9e":"for z in range(2012):\n    main_df['class'].loc[z] = spec_features[z][2].split(\".\")[0][0:-5]","6888f51e":"main_df.to_csv(\"spec_features.csv\")","4bdaea02":"spec_data = pd.read_csv(\"..\/input\/musical-emotions-classification\/spec_features.csv\")\nspec_data = spec_data.drop([\"Unnamed: 0\"],axis=1)","b6388530":"happy_tunes = spec_data[spec_data['class']=='Happy']\nsad_tunes = spec_data[spec_data['class']=='Sad']","7086717b":"happy_tunes=happy_tunes.reset_index()\nsad_tunes=sad_tunes.reset_index()","e2c33f4f":"indices_happy = happy_tunes.loc[0].index[:432]\nvalues_happy = happy_tunes.loc[0][indices_happy]\nindices_sad = sad_tunes.loc[0].index[:432]\nvalues_sad = sad_tunes.loc[0][indices_sad]\n","9cfadcab":"fig,ax = plt.subplots(1,2,figsize=(20,5))\nax[0].semilogy((values_happy.to_numpy().astype(\"Float32\")))\nax[1].semilogy((values_sad.to_numpy().astype(\"Float32\")))\nax[0].set_title(\"Happy Tune\")\nax[1].set_title(\"Sad Tune\")","01259a2f":"print(np.max(values_happy),np.min(values_happy))\nprint(np.max(values_sad),np.min(values_sad))","617100e2":"### Features\n**Features that we gonna extract from those audio files in order to do our analysis**\n1. <a href=\"https:\/\/en.wikipedia.org\/wiki\/Zero-crossing_rate\">ZeroCrossingRate - The rate at which the signal changes from postive to zero to negative and vice-versa<\/a> (*Zero Crossing Rate really helps in classifying the percussive sounds which is nothing but sounds produced from instruments like drums,xylophones,rattles etc;*)\n2. <a href=\"\">Spectral-Centroid - Gives the brightness of the time-series signal<\/a>(*Brightness Increases - Tune becomes Sad*)","ec59a5b6":"## Analyzing the different shapes","3880467f":"**This notebook talks about the differences in kinds of music like happy,sad,motivation,etc;**\n*Do you guys think that happy music cannot be converted into a sad one, Then check this interesting piece of art <a>https:\/\/www.youtube.com\/watch?v=En1BApnx3Co<\/a>*\n1. So what's the thing that gives this transition?\n2. Why happy musics build some good feeling in us and sad music makes us to think about the sad side of our life.Why they do that?\n3. Can these features spotted using a spectrogram?\n4. Why do we get motivated while listening to BELIEVER (or) WHATEVER_IT_TAKES ?\n5. Do these sounds have some special acoustic behaviors that makes us feel it?!:think!!!\n","6dad2dd9":"## Analysis of Spectral_Features","c8cfea8c":"### Needed Imports","c0494f33":"**Reading the MetaData**","91be4486":"Let's visualize its wave form","9492c02e":"let's listen to some samples of happy and sad tunes\n* Afer listening to samples, let's group the data according to their shapes. since, we have chunked those audio files. Not all audio files will be in the same shape and there must be some messed up splits. we have to get rid out of it.","0244f82d":"* Tempo is one of the reasons to make a music sad or happy.\n\n**There are many types of Tempo which is listed here:<a>https:\/\/en.wikipedia.org\/wiki\/Tempo<\/a>**\nYou can also give a treat your eardrums with this <a>https:\/\/www.youtube.com\/watch?v=qm90euRP1c0<\/a> (Music in different types of tempo)","c506c3c1":"1. What we gonna do here?\n    * analyze the different types of shapes present in the dataset and omit those which are outliers\n    * pick top 4 shapes with higher population count and using`librosa.util.fix_length`, we can pad those arrays and make them equal in shapes.\n2. Why shapes matter to us?\n    * we can group the tunes with shapes. If we don't consider these shapes, problem would arise in model training and make them highly biased"}}