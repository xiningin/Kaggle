{"cell_type":{"def4ba08":"code","84c14a7e":"code","25202aa5":"code","0e87d037":"code","050906fe":"code","bbe1e645":"code","61fd9bc9":"code","97177b7f":"code","6d415d3d":"code","d5268632":"code","cd8a6491":"code","5dfad864":"code","77e8b52d":"code","9f65d728":"code","66391139":"code","38670e8a":"code","29ddb699":"code","e4b68c09":"code","313f8a1f":"code","8ed5049f":"code","44e77f50":"code","1d88f6f1":"code","b12df587":"code","5feb052e":"code","574d9131":"code","632c8567":"code","54798cf3":"code","4c8d8807":"code","c9490bae":"code","1dabb7d9":"code","abfaa57f":"code","98090c5e":"code","9f19a755":"code","c28d8210":"code","17011417":"code","54d0e435":"code","7c5a26fe":"code","6338d8c2":"code","03fa131f":"code","f39aae0f":"code","8ea3062b":"code","c103655a":"code","4cf79698":"code","d22d730a":"code","eb653eca":"code","be309313":"code","ee5f9c0d":"code","b97232cc":"code","b300d7e6":"code","d4013400":"code","74218ccb":"code","dd83f890":"code","8bff490b":"code","03ad6d8e":"code","65fbb863":"code","537e2207":"code","f54e3bfe":"code","4087e0b6":"code","8d3a6435":"code","d31a186c":"code","26405bb5":"markdown","281ccc87":"markdown","1a5ec7ee":"markdown","c4166410":"markdown","6da42fa1":"markdown","e36d1820":"markdown","faa256b6":"markdown","deab0a49":"markdown","e50d86b5":"markdown","59b8963d":"markdown","7cedc42e":"markdown","3f190459":"markdown","28ec242b":"markdown","0edea229":"markdown","80dcfce7":"markdown","06d750b3":"markdown","832f0e6b":"markdown","62a4bea0":"markdown","4db7eeb8":"markdown","2231cff2":"markdown","7679732a":"markdown","e5720a07":"markdown","43a7b9a7":"markdown","f533937c":"markdown","fa8a1f83":"markdown","9f55f64d":"markdown","d7b08e39":"markdown","0e457c2b":"markdown","a3b8bb44":"markdown","19a72a6e":"markdown","4f41df2d":"markdown","9d090678":"markdown","60f7ea71":"markdown","db01670b":"markdown","3ac541ff":"markdown","303a6823":"markdown","6e7af1a7":"markdown","e068e07a":"markdown","c526d71c":"markdown","a4d28cae":"markdown","a5efc9e0":"markdown","9e066805":"markdown","28ae306a":"markdown"},"source":{"def4ba08":"!pip install chart_studio","84c14a7e":"import pandas as pd\nimport os\nimport numpy as np \nimport pandas as pd \nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nfrom getpass import getpass\nimport plotly.figure_factory as ff\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.io as pio\npio.renderers.default = 'colab'\nfrom plotly import tools\nimport chart_studio.plotly as py\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.express as px\nimport re\n\n###Imbalanced datasets\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import TomekLinks\n\n\n###models------\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score, silhouette_samples, accuracy_score, roc_auc_score, classification_report, f1_score,confusion_matrix,roc_curve, auc\nfrom yellowbrick.cluster import SilhouetteVisualizer\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.model_selection import train_test_split, RepeatedKFold, cross_validate, StratifiedKFold, RandomizedSearchCV\n\n#---------------------------------------------------------------\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nplotly.offline.init_notebook_mode (connected = True)\nimport joblib\n\n#Configurations\n\npd.set_option('max_columns', 50)\nplt.style.use('bmh')\ncolor_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\npd.options.display.max_columns = 400\nplt.figure(figsize=(25,10))","25202aa5":"df = pd.read_csv(\"..\/input\/default-of-credit-card-clients-dataset\/UCI_Credit_Card.csv\")\n\ndf = df.rename(columns={ \"default.payment.next.month\":\"default payment next month\"})","0e87d037":"df.head()","050906fe":"solver = \"liblinear\"\npenalty = \"l1\"\nC=0.1\nclass_weight=\"balanced\"\n\n#---------------------------------------------------------------------------------------\n\ndef trend(x, y, s=\"up\"):\n\n  \"\"\" This functions help us to calculate trends in each variable,\n   we cam follow if the variable go up or down\n\n  Input:\n      x (float): lag_n for each variable\n      y (float): lag_n+1 for each variable\n  Output:\n      x-y or 0 (float) regarding the conditions\n  \"\"\"\n\n  if s==\"up\":\n\n    if (x-y>=0):\n\n      return x-y\n\n    else:\n\n      return 0\n  else:\n    if (x-y<0):\n\n      return x-y\n\n    else:\n\n      return 0\n\n\n#------------------------------------------------------------------------------\n\ndef make_mi_scores(X, y, discrete_features=None):\n\n    \"\"\" Function to calculate the entropy of each variable throught mutual\n     information score.\n\n    Inpus X (num): (Covariables)\n    Output mic (float)\"\"\"\n\n    mi_scores = mutual_info_classif(X, y)\n    mi_scores = pd.DataFrame(mi_scores, index=X.columns, columns=[\"MIC\"])\n    mi_scores = mi_scores.sort_values(by=\"MIC\",ascending=False)\n\n    return mi_scores\n\n\n#------------------------------------------------------------------------------\n\n\ndef select_k_variables(mi_scores_1, X_train, y_train, X_test, y_test, solver, penalty, C, class_weight):\n\n      \"\"\" This function runs a simple Logistic regresion for select the\n       most importance variables regarding to AUC score.\n\n      Input:\n            mi_scores_1 (df): Dataframe with MIC\n            X (df): Covariates\n            y (df): Target variables, this is binary\n            solver (str): solver for LGR\n            penalty (str): type of penalty of LGR\n            C (float): for penalty\n            class_weight (str)\n\n      Output:\n            scores (List): result of AUC for each simple model\n\n      \"\"\"\n\n      scores = []\n      importance = list(mi_scores_1.sort_values(by=\"MIC\", ascending=True).index)\n      y_train = y_train.values\n      y_test = y_test.values\n\n      for feat in importance[0:-1]:\n\n          X_train= X_train.drop(str(feat), axis=1)\n          X_test = X_test.drop(str(feat), axis=1)\n\n          X_train_ = X_train.values\n\n          X_test_ = X_test.values\n      \n\n          lr = LogisticRegression(solver=solver, C=C, penalty=penalty,\n                                  class_weight=class_weight, random_state=120)\n          lr.fit(X_train, y_train)\n          proba = lr.predict_proba(X_test)\n          scores.append(roc_auc_score(y_test, proba[:,1]))\n\n      return scores, importance\n\n\n#------------------------------------------------------------------------------\n\ndef train_baseline_models( models, models_text,X_train, y_train, n_splits_=10):\n\n    \"\"\" This function help us to train baseline models and perform kfold validation\n    for select the best model\n    INPUTS:\n      models : Basellines models to train\n      X_train (df): Covariates\n      y_train (df): Target variable\n      n_splits (int): number of splits for Kfold\n      n_rep (int): number of repetitions\n\n    OUTPUTS:\n\n    f1_scores (dict): values of f1 score\n    acuraccy_scores (dict): values of acuraccy score\n    auc_scores (dict): values of auc score\n    models_fit (dict): models\n    \"\"\"\n\n    f1_scores = {}\n    acuraccy_scores = {}\n    auc_scores = {}\n    models_fit = {}\n    \n  \n    skf = StratifiedKFold(n_splits=n_splits_, shuffle=True,  random_state=10)\n\n    for i,model in enumerate(models):\n\n      f1_scores_list =[]\n      acuraccy_scores_list = []\n      auc_scores_list = []\n      \n      for train_idx, test_idx in skf.split(X_train, y_train):\n\n            print(model)\n            x_train, x_test = X_train[train_idx-1], X_train[test_idx-1]\n            y_train_, y_test_ = y_train[train_idx-1], y_train[test_idx-1]\n            \n            model.fit(x_train, y_train_)\n\n            models_fit[models_text[i]]= model\n\n            proba = model.predict_proba(x_test)\n\n            proba_bin = model.predict(x_test)\n\n            f1_scores_list.append(f1_score(y_test_, proba_bin))\n\n            acuraccy_scores_list.append(accuracy_score(y_test_, proba_bin))\n\n            auc_scores_list.append(roc_auc_score(y_test_, proba[:,1]))\n\n            auc_scores[models_text[i]] =  auc_scores_list\n\n            f1_scores[models_text[i]] = f1_scores_list\n\n            acuraccy_scores[models_text[i]] =  acuraccy_scores_list\n\n    return f1_scores, acuraccy_scores,  auc_scores,  models_fit \n\n\n\n#-----------------------------------------------------------------------------------------------------\n\n\n\n\ndef evalBinaryClassifier(clf, x, y, labels=['Positives','Negatives']):\n\n   \n    #model predicts probabilities of positive class\n    p = clf.predict_proba(x)\n    pos_p = p[:,1]\n    pos_p0 = p[:,0]\n    \n    #FIGURE\n    plt.figure(figsize=[21,5])\n    \n    #1 -- Confusion matrix\n    cm = confusion_matrix(y,clf.predict(x))\n    plt.subplot(131)\n    ax = sns.heatmap(cm, annot=True, cmap='Blues', cbar=False, \n                annot_kws={\"size\": 14}, fmt='g')\n    cmlabels = ['True Negatives', 'False Positives',\n              'False Negatives', 'True Positives']\n    for i,t in enumerate(ax.texts):\n        t.set_text(t.get_text() + \"\\n\" + cmlabels[i])\n    plt.title('Confusion Matrix', size=15)\n    plt.xlabel('Predicted Values', size=13)\n    plt.ylabel('True Values', size=13)\n      \n    #2 -- Distributions of Predicted Probabilities of both classes\n\n    df = pd.DataFrame({'probPos':pos_p, 'target': y.values.flatten()})\n    plt.subplot(132)\n    plt.hist(df[df.target==1].probPos, density=True, bins=25,\n             alpha=.5, color='c',  label=labels[0])\n    plt.hist(df[df.target==0].probPos, density=True, bins=25,\n             alpha=.5, color='gray', label=labels[1])\n    plt.axvline(.5, color='c', linestyle='--', label='Boundary')\n    plt.xlim([df.probPos.min(),df.probPos.max()])\n    plt.title('Distributions of Predictions', size=15)\n    plt.xlabel('Positive Probability (predicted)', size=13)\n    plt.ylabel('Samples (normalized scale)', size=13)\n    plt.legend(loc=\"upper right\")\n\n\n\n    #3 -- ROC curve with annotated decision point\n\n    fp_rates, tp_rates, _ = roc_curve(y,p[:,1])\n    roc_auc = auc(fp_rates, tp_rates)\n    plt.subplot(133)\n    plt.plot(fp_rates, tp_rates, color='c',\n             lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], lw=1, linestyle='--', color='grey')\n    #plot current decision point:\n    tn, fp, fn, tp = [i for i in cm.ravel()]\n    plt.plot(fp\/(fp+tn), tp\/(tp+fn), 'bo', markersize=8, label='Decision Point')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate', size=13)\n    plt.ylabel('True Positive Rate', size=13)\n    plt.title('ROC Curve', size=15)\n    plt.legend(loc=\"lower right\")\n    plt.subplots_adjust(wspace=.3)\n    plt.show()\n\n\n#------------------------------------------------------------------------------\n\n\n\ndef random_search_xgb(X_train, y_train, X_val, y_val, folds_):\n\n      \"\"\"Optiization of XGB model.\n      Inputs:\n          X_train (vec): covariates\n          y_train (vec): target variable\n          X_test (vec): val vector\n          y_val (vec): target val vector\n\n      Outputs:\n          random_search (model)\n      \"\"\"\n      params = {\n              'eta':[0.001, 0.01,0.05, 0.2,0.3],\n              'max_delta_step': [2,5,7,10],\n              'min_child_weight': [1, 5, 10],\n              'gamma': [0.5, 1, 1.5, 2, 5],\n              'subsample': [0.6, 0.8, 1.0],\n              'colsample_bytree': [0.6, 0.8, 1.0],\n              'n_estimators':[70,150,250,350, 450],\n              'max_depth': [3, 4, 5,6]\n              }\n\n      xgb = XGBClassifier( objective='binary:logistic', nthread=1)\n\n\n      folds = folds_\n  \n      skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\n      random_search = RandomizedSearchCV(xgb, param_distributions=params,  scoring='roc_auc', n_jobs=4, cv=skf.split(X_train,y_train), verbose=3, random_state=1001 )\n\n      random_search.fit(X_train, y_train,  early_stopping_rounds=12, eval_metric=\"auc\", eval_set=[(X_train, y_train), (X_val, y_val)], verbose=1)\n\n      return random_search","bbe1e645":"print(\"The dimension of this Dataset is:\", df.shape)","61fd9bc9":"df.describe()","97177b7f":"plt.figure(figsize=(25,10))\nsns.displot(\n    data=df.isna().melt(value_name=\"missing\"),\n    y=\"variable\",\n    hue=\"missing\",\n    multiple=\"fill\",\n    aspect=1.25\n)\n\n\n#plt.savefig(\"missing_data.png\", dpi=100)\nplt.show()\n\n","6d415d3d":"var_cats = [\"SEX\", \t\"EDUCATION\",\t\"MARRIAGE\",\t\"AGE\", \"default payment next month\"]\n\nfor var in var_cats:\n  df_aux= df.groupby(var).agg({\"ID\":\"count\"}).rename(columns={\"ID\":\"Count\"}).reset_index()\n  fig = px.bar(df_aux, x=var, y=\"Count\", title = \"Basic analysis of:\"+\" \"+ var)\n  fig.update_layout(template='seaborn')\n  fig.update_traces(marker_color=\"cadetblue\", marker_line_color='rgb(8,48,107)', marker_line_width=0.3, opacity=0.5)\n  #plotly.offline.plot(fig, filename = \"exploracion_cat_basic_\"+var+\".html\", auto_open=False)\n  fig.show()\n","d5268632":"df[\"EDUCATION\"] = df[\"EDUCATION\"].apply(lambda x: 0 if ((x!=1) & (x!=2) & (x!=3) & (x!=4)) else x )\ndf[\"MARRIAGE\"] = df[\"MARRIAGE\"].apply(lambda x: 0 if ((x!=1) & (x!=2) & (x!=3)) else x )\n\nbins = [20,30, 40, 52,  72, 91] #bin for each generation\ndf[\"AGE_GROUP\"] = pd.cut(df['AGE'], bins=bins)\ndf.AGE_GROUP = df.AGE_GROUP.astype(str)\n\nprint(df.AGE_GROUP.value_counts())\n#df.drop(\"AGE\", inplace=True, axis=1)","cd8a6491":"limit_bal = df[\"LIMIT_BAL\"].values.tolist()\neducation = list(df.EDUCATION.unique())\n\neducation_limit = {}\n\nfor edu in education:\n\n  education_limit[edu] = df[\"LIMIT_BAL\"].loc[df['EDUCATION'] == edu].values.tolist()\n\n\ntrace0 = go.Histogram(\n    x=education_limit[0],\n    histnorm='probability',\n    name=\"Education: N\/A\",\n    marker = dict(color = 'rgba(100, 149, 237, 0.6)',)\n)\ntrace1 = go.Histogram(\n    x= education_limit[0],\n    histnorm='probability',\n    name=\"Education: Graduate school\",\n    marker = dict(\n        color = 'rgba(255, 182, 193, 0.6)',\n    )\n)\ntrace2 = go.Histogram(\n    x=education_limit[2],\n    histnorm='probability',\n    name=\"Education: University\",\n     marker = dict(\n        color = 'rgba(169, 169, 169, 0.6)',\n    )\n)\n\ntrace3 = go.Histogram(\n    x=education_limit[3],\n    histnorm='probability',\n    name=\"Education: High school\",\n     marker = dict(\n        color = 'rgba(222,184,135, 0.6)',\n    )\n)\n\ntrace4 = go.Histogram(\n    x=education_limit[4],\n    histnorm='probability',\n    name=\"Education: Others\",\n     marker = dict(\n        color = 'rgba(143,188,143,0.6)',\n    )\n)\n\n\ntrace5 = go.Histogram(\n    x= limit_bal,\n    histnorm='probability',\n    name=\"Education: All\",\n    marker = dict(\n    color = 'rgba(48,48,48, 0.6)',\n    )\n)\n\n\nfig = tools.make_subplots(rows=3, cols=3,  specs= [[{}, {}, {}],[{ \"rowspan\": 1,\"colspan\": 2},{}, {}], [{\"rowspan\": 1, \"colspan\": 3}, None, None]],\n                          subplot_titles=('N\/A','Graduate School', 'University', \"High School\", \" \", \"Others\", \"All\"), )\n\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 1, 3)\nfig.append_trace(trace3, 2, 1)\nfig.append_trace(trace4, 2, 3)\nfig.append_trace(trace5, 3, 1)\n\nfig.update_layout(template='seaborn')\nfig['layout'].update(showlegend=True, title='Amount of the given credit per education', bargap=0.05, height=500, width=800)\niplot(fig, filename='custom-sized-subplot-with-subplot-titles')\n\n#plotly.offline.plot(fig, filename = \"bill_amount_edu.html\", auto_open=False)\n","5dfad864":"df['limit_bins'] = pd.cut(df['LIMIT_BAL'],[5000, 50000, 100000, 150000, 200000, 300000, 400000, 500000, 1100000])\n\n\ndf['limit_bins'] = df['limit_bins'].astype('str')\nlimit_bin_order = ['(5000, 50000]', '(50000, 100000]', '(100000, 150000]', '(150000, 200000]',\n                '(200000, 300000]', '(300000, 400000]', '(400000, 500000]', '(500000, 1100000]']\n\na =df.groupby([\"limit_bins\", \"default payment next month\"]).agg({\"ID\":\"count\"}).rename(columns={\"ID\":\"Count\"}).reset_index()\nfig = px.bar(a, x=\"limit_bins\",y=\"Count\", color=\"default payment next month\",color_discrete_sequence= px.colors.sequential.YlGn, category_orders= {\"limit_bins\":limit_bin_order},\n            width=800, height=400, title= \"Amount of clients in each bin of limit credit\")\n\nfig.update_layout(template='seaborn')\nfig.update_xaxes(title_text='Bins of Amount of credit')\n\n\nfig.update_traces( marker_line_color='rgb(8,48,107)', marker_line_width=0.5, opacity=0.5)\n#plotly.offline.plot(fig, filename = \"bins_limit_default.html\", auto_open=False)\nfig.show()\n","77e8b52d":"age_group_order = [\"(20, 30]\", \"(30,40]\", \"(40,52]\",\"(52,72]\",\"(72,91]\"]\nfig = px.violin(df, y=\"LIMIT_BAL\", x=\"AGE_GROUP\", color=\"MARRIAGE\", box=True, color_discrete_sequence=[\"cadetblue\", \"burlywood\", \"coral\", \"goldenrod\"], \n                title=\"Distribution: Amount of the given credit per Age group\",  )\n\nfig.update_layout(template='seaborn')\nfig.update_xaxes(title_text='Age group (Generation)')\nfig.update_yaxes(title_text= 'Limit Amount($)')\nfig.update_traces( marker_line_color='rgb(8,48,107)', marker_line_width=0.3, opacity=0.75)\n#plotly.offline.plot(fig, filename = \"bill_amount_age_marriage.html\", auto_open=False)\nfig.show()","9f65d728":"fig = px.violin(df, y=\"LIMIT_BAL\", x=\"EDUCATION\", color=\"default payment next month\",facet_col=\"SEX\", box=True, color_discrete_sequence=[\"cadetblue\", \"burlywood\", \"coral\", \"goldenrod\"]\n                ,title=\"Distribution: Amount of the given credit per default of payment\",width=1100, facet_col_spacing=0 )\nfig.update_layout(template='seaborn')\nfig.update_xaxes(title_text='Education')\n#fig.update_yaxes(title_text= 'Limit A($)',)\nfig.update_traces( marker_line_color='rgb(8,48,107)', marker_line_width=0.3, opacity=0.75)\n#plotly.offline.plot(fig, filename = \"bill_amount_default_educa_sex.html\", auto_open=False)\nfig.show()\n","66391139":"a =df.groupby([\"EDUCATION\", \"MARRIAGE\",\"default payment next month\"]).agg({\"ID\":\"count\"}).rename(columns={\"ID\":\"Count\"}).reset_index()\nfig = px.bar(a, x=\"EDUCATION\", y=\"Count\", color=\"default payment next month\",color_discrete_sequence= px.colors.sequential.YlGn,\n            facet_col=\"MARRIAGE\", width=800, height=400, title=\"Default of payment for Eduaction and marital status\")\nfig.update_layout(template='seaborn')\nfig.update_xaxes(title_text='Education')\nfig.update_traces( marker_line_color='rgb(8,48,107)', marker_line_width=0.5, opacity=0.5)\n#plotly.offline.plot(fig, filename = \"pay_defaults_marriage_edu.html\", auto_open=False)\nfig.show()\n\n","38670e8a":"a = pd.DataFrame(df['default payment next month'].groupby(df[ \"AGE_GROUP\"]).value_counts(normalize = True)).rename(columns={\"default payment next month\": \"% of Default\"}).reset_index()\na = a[a[\"default payment next month\"]==1]\n\nfig = px.bar(a, x=\"AGE_GROUP\", y=\"% of Default\", width=800, height=400, title = \"% of Default per Age group\")\n\nfig.update_layout(template='seaborn')\nfig.update_xaxes(title_text='Age Group')\nfig.update_traces(marker_color=\"cadetblue\", marker_line_color='rgb(8,48,107)', marker_line_width=0.3, opacity=0.5)\n#plotly.offline.plot(fig, filename = ruta+\"\/plots\/pay_defaults_age_group.html\", auto_open=False)\nfig.show()\n","29ddb699":"repays = ['PAY_0', 'PAY_2', 'PAY_3', 'PAY_4', 'PAY_5', 'PAY_6']\n\nfigs = {}\ndfs_aux = {}\nfor repay in repays:\n\n  dfs_aux[repay] =df.groupby([repay,\"default payment next month\"]).agg({\"SEX\":\"count\"}).rename(columns={\"SEX\":\"Count\"}).reset_index()\n  figs[repay] = px.bar(dfs_aux[repay], x=repay, y=\"Count\", color=\"default payment next month\",color_discrete_sequence= px.colors.sequential.YlGn,\n              width=800, height=400, title=\"Default for each Repayments in: \"+ repay)\n  figs[repay].update_layout(template='seaborn')\n  figs[repay].update_xaxes(title_text=repay)\n  figs[repay].update_traces( marker_line_color='rgb(8,48,107)', marker_line_width=0.5, opacity=0.5)\n  #plotly.offline.plot(figs[repay], filename = ruta+\"\/plots\/default_\"+repay+\".html\", auto_open=False)\n  figs[repay].show()\n","e4b68c09":"df.drop([\"ID\"], axis=1,inplace=True)\nf = plt.figure(figsize= (15,15))\nsns.set_context(\"paper\", font_scale=1.)\nsns.heatmap(df.corr(),  annot=True,  linewidths=.3, fmt= '.1f',cmap=\"YlGnBu\")\nplt.title(\"Corration values for feaures\")\n#plt.savefig(ruta+\"\/plots\/correlations.png\", dpi=100); ","313f8a1f":"var_trends = [ \"BILL_AMT\", \"PAY_AMT\"]\n\nfor var in var_trends:\n\n    df[var+\"_A_1\"] = df.apply(lambda x: trend(x[var+\"1\"], x[var+\"2\"],s=\"up\"), axis=1)\n    df[var+\"_A_2\"] = df.apply(lambda x: trend(x[var+\"2\"], x[var+\"3\"], s=\"up\"), axis=1)\n    df[var+\"_A_3\"] = df.apply(lambda x: trend(x[var+\"3\"], x[var+\"4\"],s=\"up\"), axis=1)\n    df[var+\"_A_4\"] = df.apply(lambda x: trend(x[var+\"4\"], x[var+\"5\"],s=\"up\"), axis=1)\n    df[var+\"_A_5\"] = df.apply(lambda x: trend(x[var+\"5\"], x[var+\"6\"],s=\"up\"), axis=1)\n\n    df[var+\"_B_1\"] = df.apply(lambda x: trend(x[var+\"1\"], x[var+\"2\"],s=\"down\"), axis=1)\n    df[var+\"_B_2\"] = df.apply(lambda x: trend(x[var+\"2\"], x[var+\"3\"], s=\"down\"), axis=1)\n    df[var+\"_B_3\"] = df.apply(lambda x: trend(x[var+\"3\"], x[var+\"4\"],s=\"down\"), axis=1)\n    df[var+\"_B_4\"] = df.apply(lambda x: trend(x[var+\"4\"], x[var+\"5\"],s=\"down\"), axis=1)\n    df[var+\"_B_5\"] = df.apply(lambda x: trend(x[var+\"5\"], x[var+\"6\"],s=\"down\"), axis=1)\n","8ed5049f":"df[\"PAY_AMT_LAST_3M\"] =  df[\"PAY_AMT1\"] + df[\"PAY_AMT2\"] + df[\"PAY_AMT3\"]\ndf[\"PAY_AMT_FIRTS_3M\"] = df[\"PAY_AMT4\"] +df[\"PAY_AMT5\"]+df[\"PAY_AMT6\"]\ndf[\"PAY_AMT_LAST_6M\"] =  df[\"PAY_AMT1\"] + df[\"PAY_AMT2\"] + df[\"PAY_AMT3\"] + df[\"PAY_AMT4\"] +df[\"PAY_AMT5\"]+df[\"PAY_AMT6\"]\ndf[\"ratio_limit_pay\"] =  df[\"LIMIT_BAL\"]\/(df[\"PAY_AMT_LAST_6M\"]+1)\n\nnums = [1,2,3,4,5]\n\nfor i in nums:\n\n  df[\"BILL_AMT\/PAY_AMT\"+\"_\"+str(i)] = df[\"BILL_AMT\"+str(i)]\/(df[\"PAY_AMT\"+str(i)]+1) \n  df[\"BILL_AMT_BIN_AT_\"+str(i)] =  df[\"BILL_AMT_A_\"+str(i)].apply(lambda x: 1 if x!=0 else 0)\n  df[\"PAY_AMT_BIN_AT_\"+str(i)] =  df[\"PAY_AMT_A_\"+str(i)].apply(lambda x: 1 if x!=0 else 0)\n  df[\"BILL_AMT_BIN_BT_\"+str(i)] =  df[\"BILL_AMT_B_\"+str(i)].apply(lambda x: 1 if x!=0 else 0)\n  df[\"PAY_AMT_BIN_BT_\"+str(i)] =  df[\"PAY_AMT_B_\"+str(i)].apply(lambda x: 1 if x!=0 else 0)\n\n\n\ndf[\"BILL_AMT_AT\"] = df[\"BILL_AMT_A_1\"]+df[\"BILL_AMT_A_2\"]+ df[\"BILL_AMT_A_3\"]+df[\"BILL_AMT_A_4\"]+df[\"BILL_AMT_A_5\"] \n\ndf[\"BILL_AMT_BT\"] = df[\"BILL_AMT_B_1\"]+df[\"BILL_AMT_B_2\"]+ df[\"BILL_AMT_B_3\"]+df[\"BILL_AMT_B_4\"]+df[\"BILL_AMT_B_5\"] \n\ndf[\"PAY_AMT_AT\"] = df[\"PAY_AMT_A_1\"]+df[\"PAY_AMT_A_2\"]+ df[\"PAY_AMT_A_3\"]+df[\"PAY_AMT_A_4\"]+df[\"PAY_AMT_A_5\"] \n\ndf[\"PAY_AMT_BT\"] = df[\"PAY_AMT_B_1\"]+df[\"PAY_AMT_B_2\"]+ df[\"PAY_AMT_B_3\"]+df[\"PAY_AMT_B_4\"]+df[\"PAY_AMT_B_5\"] \n\n#-----------------------------------------Trends bin\n\ndf[\"BILL_AMT_BIN_AT_2\"] =df[\"BILL_AMT_BIN_AT_1\"] +df[\"BILL_AMT_BIN_AT_2\"]\ndf[\"BILL_AMT_BIN_AT_3\"] =df[\"BILL_AMT_BIN_AT_1\"] +df[\"BILL_AMT_BIN_AT_2\"]+df[\"BILL_AMT_BIN_AT_3\"]\ndf[\"BILL_AMT_BIN_AT_4\"] =df[\"BILL_AMT_BIN_AT_1\"] +df[\"BILL_AMT_BIN_AT_2\"]+df[\"BILL_AMT_BIN_AT_3\"]+df[\"BILL_AMT_BIN_AT_4\"]\ndf[\"BILL_AMT_BIN_AT_5\"] =df[\"BILL_AMT_BIN_AT_1\"] +df[\"BILL_AMT_BIN_AT_2\"]+df[\"BILL_AMT_BIN_AT_3\"]+df[\"BILL_AMT_BIN_AT_4\"]+df[\"BILL_AMT_BIN_AT_5\"]\n\n\ndf[\"PAY_AMT_BIN_AT_2\"] =df[\"PAY_AMT_BIN_AT_1\"] +df[\"PAY_AMT_BIN_AT_2\"]\ndf[\"PAY_AMT_BIN_AT_3\"] =df[\"PAY_AMT_BIN_AT_1\"] +df[\"PAY_AMT_BIN_AT_2\"]+df[\"PAY_AMT_BIN_AT_3\"]\ndf[\"PAY_AMT_BIN_AT_4\"] =df[\"PAY_AMT_BIN_AT_1\"] +df[\"PAY_AMT_BIN_AT_2\"]+df[\"PAY_AMT_BIN_AT_3\"]+df[\"PAY_AMT_BIN_AT_4\"]\ndf[\"PAY_AMT_BIN_AT_5\"] =df[\"PAY_AMT_BIN_AT_1\"] +df[\"PAY_AMT_BIN_AT_2\"]+df[\"PAY_AMT_BIN_AT_3\"]+df[\"PAY_AMT_BIN_AT_4\"]+df[\"PAY_AMT_BIN_AT_5\"]\n\n\ndf[\"BILL_AMT_BIN_BT_2\"] =df[\"BILL_AMT_BIN_BT_1\"] +df[\"BILL_AMT_BIN_BT_2\"]\ndf[\"BILL_AMT_BIN_BT_3\"] =df[\"BILL_AMT_BIN_BT_1\"] +df[\"BILL_AMT_BIN_BT_2\"]+df[\"BILL_AMT_BIN_BT_3\"]\ndf[\"BILL_AMT_BIN_BT_4\"] =df[\"BILL_AMT_BIN_BT_1\"] +df[\"BILL_AMT_BIN_BT_2\"]+df[\"BILL_AMT_BIN_BT_3\"]+df[\"BILL_AMT_BIN_BT_4\"]\ndf[\"BILL_AMT_BIN_BT_5\"] =df[\"BILL_AMT_BIN_BT_1\"] +df[\"BILL_AMT_BIN_BT_2\"]+df[\"BILL_AMT_BIN_BT_3\"]+df[\"BILL_AMT_BIN_BT_4\"]+df[\"BILL_AMT_BIN_AT_5\"]\n\n\ndf[\"PAY_AMT_BIN_BT_2\"] =df[\"PAY_AMT_BIN_BT_1\"] +df[\"PAY_AMT_BIN_BT_2\"]\ndf[\"PAY_AMT_BIN_BT_3\"] =df[\"PAY_AMT_BIN_BT_1\"] +df[\"PAY_AMT_BIN_BT_2\"]+df[\"PAY_AMT_BIN_BT_3\"]\ndf[\"PAY_AMT_BIN_BT_4\"] =df[\"PAY_AMT_BIN_BT_1\"] +df[\"PAY_AMT_BIN_BT_2\"]+df[\"PAY_AMT_BIN_BT_3\"]+df[\"PAY_AMT_BIN_BT_4\"]\ndf[\"PAY_AMT_BIN_BT_5\"] =df[\"PAY_AMT_BIN_BT_1\"] +df[\"PAY_AMT_BIN_BT_2\"]+df[\"PAY_AMT_BIN_BT_3\"]+df[\"PAY_AMT_BIN_BT_4\"]+df[\"PAY_AMT_BIN_BT_5\"]\n\n\n\ndf[\"ratio_bill_pay_bin_AT\"] = df[\"BILL_AMT_BIN_AT_5\"]\/(df[\"PAY_AMT_BIN_AT_5\"]+1) \n\ndf[\"ratio_bill_pay_bin_BT\"] = df[\"BILL_AMT_BIN_BT_5\"]\/(df[\"PAY_AMT_BIN_BT_5\"]+1) \n\n\n#-------------------------------------------------------------Basic Statistics variables--------------------------------------------------------\n\ndf[\"PAY_AMT_STD\"] = df[[\"PAY_AMT1\",\t\"PAY_AMT2\",\t\"PAY_AMT3\",\t\"PAY_AMT4\",\t\"PAY_AMT5\",\t\"PAY_AMT6\"]].std(axis=1)\ndf[\"BILL_AMT_STD\"] = df[[\"BILL_AMT1\",\t\"BILL_AMT2\",\t\"BILL_AMT3\",\t\"BILL_AMT4\",\t\"BILL_AMT5\",\t\"BILL_AMT6\"]].std(axis=1)\n\ndf[\"BILL_AMT_MAX\"] = df[[\"BILL_AMT1\",\t\"BILL_AMT2\",\t\"BILL_AMT3\",\t\"BILL_AMT4\",\t\"BILL_AMT5\",\t\"BILL_AMT6\"]].max(axis=1)\ndf[\"PAY_AMT_MAX\"] = df[[\"PAY_AMT1\",\t\"PAY_AMT2\",\t\"PAY_AMT3\",\t\"PAY_AMT4\",\t\"PAY_AMT5\",\t\"PAY_AMT6\"]].max(axis=1)\ndf[\"REPAYS_MEAN\"] = df[[\"PAY_0\",\t\"PAY_2\",\t\"PAY_3\",\t\"PAY_4\",\t\"PAY_5\",\t\"PAY_6\"]].mean(axis=1)\n\n\ndf = pd.get_dummies(df, drop_first=True)\n","44e77f50":"sc = StandardScaler()\ndf_prep = pd.DataFrame(sc.fit_transform(df.drop(\"default payment next month\", axis=1)), columns=df.drop(\"default payment next month\",axis=1).columns)\ndf_prep = pd.concat([df_prep, df[[\"default payment next month\"]]], axis=1)","1d88f6f1":"df_prep.head()","b12df587":"df_cluster = df_prep[[\"LIMIT_BAL\", \"AGE\", \"BILL_AMT1\",\t\"BILL_AMT2\",\t\"BILL_AMT3\",\t\"BILL_AMT4\",\t\"BILL_AMT5\",\n                         \"BILL_AMT6\", \"PAY_AMT_AT\", \"PAY_AMT_BT\", \"ratio_bill_pay_bin_AT\",\"ratio_bill_pay_bin_BT\",  \"PAY_AMT_STD\",\t\"BILL_AMT_STD\"]]\n\nn_clusters = [2,3,4,5,6,7,8]\n\nlabels_kmeans = {}\ncentroids_kmeans = {}\nscores_kmeans = {}\nk_means_models = {}\nfor cluster in n_clusters:\n  \n  k_means= (KMeans(n_clusters = cluster ,init='k-means++' ,max_iter=300, \n                        tol=0.0001,  random_state= 111  , algorithm='full') )\n  \n  k_means.fit_predict(df_cluster)\n  k_means_models[str(cluster)] =k_means\n  labels_kmeans[str(cluster)] = k_means.labels_\n  centroids_kmeans[str(cluster)] = k_means.cluster_centers_\n  scores_kmeans[str(cluster)] = silhouette_score( df_cluster,k_means.labels_, metric='euclidean')\n  \n  #joblib.dump(k_means, ruta+\"\/models\/k_means\/k_means_models_\"+str(cluster)+ \".hdf5\")  \n                \n","5feb052e":"si_scores = pd.DataFrame(list(scores_kmeans.values()), columns = [\"silhouette_score\"], index=n_clusters)\n\nfig = px.bar(si_scores, x=si_scores.index, y=\"silhouette_score\", width=800, height=400, title = \"Silhouette score for each cluster\")\n\nfig.update_layout(template='seaborn')\nfig.update_xaxes(title_text='Cluster')\nfig.update_yaxes(title_text='Silhouette Score')\nfig.update_traces(marker_color=\"cadetblue\", marker_line_color='rgb(8,48,107)', marker_line_width=0.3, opacity=0.5)\n#plotly.offline.plot(fig, filename = ruta+\"\/plots\/Silhoute_score.html\", auto_open=False)\nfig.show()","574d9131":"df_cluster_ = df[[\"LIMIT_BAL\", \"AGE\", \"BILL_AMT1\",\t\"BILL_AMT2\",\t\"BILL_AMT3\",\t\"BILL_AMT4\",\t\"BILL_AMT5\",\n                         \"BILL_AMT6\", \"PAY_AMT_AT\", \"PAY_AMT_BT\", \"ratio_bill_pay_bin_AT\", \"ratio_bill_pay_bin_BT\",  \"PAY_AMT_STD\",\t\"BILL_AMT_STD\", \"default payment next month\"]]\n\ndf_cluster_ = pd.DataFrame(sc.fit_transform(df_cluster_), columns=df_cluster_.columns)\ndf_cluster_[\"n_2\"] = labels_kmeans[\"2\"]\ndf_cluster_[\"n_3\"] = labels_kmeans[\"3\"]","632c8567":"a =df_cluster_.groupby([\"n_2\", \"default payment next month\"]).agg({\"AGE\":\"count\"}).rename(columns={\"AGE\":\"Count\"}).reset_index()\nfig = px.bar(a, x=\"n_2\", y=\"Count\",color=\"default payment next month\", color_discrete_sequence= px.colors.sequential.YlGn, width=800, height=400, title=\"Number of clients per cluster\")\n\nfig.update_layout(template='seaborn')\nfig.update_xaxes(title_text='Cluster')\n\n\nfig.update_traces( marker_line_color='rgb(8,48,107)', marker_line_width=0.5, opacity=0.5)\n#plotly.offline.plot(fig, filename = ruta+\"\/plots\/cant_clients_n2.html\", auto_open=False)\nfig.show()","54798cf3":"a =df_cluster_.groupby([\"n_3\", \"default payment next month\"]).agg({\"AGE\":\"count\"}).rename(columns={\"AGE\":\"Count\"}).reset_index()\nfig = px.bar(a, x=\"n_3\", y=\"Count\",color=\"default payment next month\", color_discrete_sequence= px.colors.sequential.YlGn, width=800, height=400, title=\"Number of clients per cluster\")\n\nfig.update_layout(template='seaborn')\nfig.update_xaxes(title_text='Cluster')\nfig.update_traces( marker_line_color='rgb(8,48,107)', marker_line_width=0.5, opacity=0.5)\n#plotly.offline.plot(fig, filename = ruta+\"\/plots\/cant_clients_n3.html\", auto_open=False)\nfig.show()","4c8d8807":"a =df_cluster_.groupby([\"n_2\"]).mean().reset_index().drop([\"n_3\",\"n_2\"], axis=1).T.rename(columns={0:\"0\", 1:\"1\"})\nfig = px.scatter(a, x=a.index, y=\"0\", title= \"Distribution of mean for each variable\", labels=[\"cluster_0\"])\n\nfig.update_layout(template='seaborn')\nfig.update_xaxes(title_text='Variable')\nfig.update_yaxes(title_text='Mean')\n\nfig.add_trace(go.Scatter(\n        x=a.index,\n        y=a[\"1\"],\n        marker = plotly.graph_objects.scatter.Marker(color=\"salmon\"),\n        mode=\"markers\",\n        name=\"cluster_1\",\n        showlegend=True)\n)\n\n\n#plotly.offline.plot(fig, filename = ruta+\"\/plots\/mean_var_cluster_2.html\", auto_open=False)\n\n\nfig.show()","c9490bae":"a =df_cluster_.groupby([\"n_3\"]).mean().reset_index().drop([\"n_3\",\"n_2\"], axis=1).T.rename(columns={0:\"0\", 1:\"1\", 2:\"2\"})\nfig = px.scatter(a, x=a.index, y=\"0\", title= \"Distribution of mean for each variable\", labels=[\"cluster_0\"])\n\nfig.update_layout(template='seaborn')\nfig.update_xaxes(title_text='Variable')\nfig.update_yaxes(title_text='Mean')\n\nfig.add_trace(go.Scatter(\n        x=a.index,\n        y=a[\"1\"],\n        marker = plotly.graph_objects.scatter.Marker(color=\"salmon\"),\n        mode=\"markers\",\n        name=\"cluster_1\",\n        showlegend=True)\n)\n\nfig.add_trace(go.Scatter(\n        x=a.index,\n        y=a[\"2\"],\n        marker = plotly.graph_objects.scatter.Marker(color=\"orange\"),\n        mode=\"markers\",\n        name=\"cluster_2\",\n        showlegend=True)\n)\n\n#plotly.offline.plot(fig, filename = ruta+\"\/plots\/mean_var_cluster_3.html\", auto_open=False)\n\n\nfig.show()","1dabb7d9":"fig, ax = plt.subplots(1,2, figsize=(20,8))\nvisualizer = SilhouetteVisualizer(k_means_models[\"2\"], colors='yellowbrick', ax=ax[0])\nviz_2 = visualizer.fit(df_cluster)      \nviz_2.finalize()\nvisualizer_ = SilhouetteVisualizer(k_means_models[\"3\"], colors='yellowbrick', ax=ax[1])\nviz_2 = visualizer_.fit(df_cluster)      \nviz_2.finalize()\n\n#plt.savefig( ruta+\"\/plots\/sihol_plot_2.png\",bbox_inches='tight')\nplt.show()","abfaa57f":"df_prep[\"clusters\"] = labels_kmeans[\"2\"]\ndf_prep.drop(\"AGE\", axis=1, inplace=True)\n\ndf_cluster_1 = df_prep[df_prep.clusters==0].drop(\"clusters\", axis=1)\n\ndf_cluster_2 = df_prep[df_prep.clusters==1].drop(\"clusters\", axis=1)","98090c5e":"X_1 = df_cluster_1.drop(\"default payment next month\", axis=1)\ny_1 = df_cluster_1[[\"default payment next month\"]]\n\n#------------------------------------------------------------------\n\nmi_scores_1 = make_mi_scores(X_1, y_1)\nmi_scores_1.head(20)","9f19a755":"var= list(mi_scores_1.iloc[0:15,:].index)\nscores = list(mi_scores_1.iloc[0:15,:].MIC.values)\n\nfig = go.Figure()\nfig.add_trace(go.Bar(\n    y= var,\n    x= scores,\n    orientation='h',\n    marker=dict(\n        color=\"cadetblue\", opacity=0.5\n        \n    )))\nfig.update_layout(template='seaborn')\nfig.update_xaxes(title_text='MIC')\nfig.update_yaxes(title_text='Varibale')\n\nfig['layout'].update(showlegend=True, title='Mutual information score for 15 variables in cluster 1', bargap=0.05)\niplot(fig, filename='custom-sized-subplot-with-subplot-titles')\n\n#plotly.offline.plot(fig, filename = ruta+\"\/plots\/mic_cluster_1.html\", auto_open=False)","c28d8210":"X_1 = X_1[list(mi_scores_1.index)]\nX_train, X_test, y_train, y_test = train_test_split(X_1, y_1, shuffle=True, stratify=y_1, random_state=0, test_size=0.2)\n\nscores, importance = select_k_variables(mi_scores_1, X_train, y_train, X_test, y_test, solver, penalty, C, class_weight )[0], select_k_variables(mi_scores_1, X_train, y_train, X_test, y_test, solver, penalty, C, class_weight )[1]\n","17011417":"max_value = max(scores)\nmax_index = scores.index(max_value) \nX_1.drop(importance[:max_index], axis=1, inplace=True)\n\nprint(\"Number of feautures to drop:\", max_index)","54d0e435":"X_2 = df_cluster_2.drop(\"default payment next month\", axis=1)\ny_2 = df_cluster_2[[\"default payment next month\"]]\n\nmi_scores_2 = make_mi_scores(X_2, y_2)\nmi_scores_2.head(20)","7c5a26fe":"var= list(mi_scores_2.iloc[0:15,:].index)\nscores = list(mi_scores_2.iloc[0:15,:].MIC.values)\n\nfig = go.Figure()\nfig.add_trace(go.Bar(\n    y= var,\n    x= scores,\n    orientation='h',\n    marker=dict(\n        color=\"cadetblue\",opacity=0.5\n        \n    )))\nfig.update_layout(template='seaborn')\nfig.update_xaxes(title_text='MIC')\nfig.update_yaxes(title_text='Varibale')\n\nfig['layout'].update(showlegend=True, title='Mutual information score for 15 variables in cluster 2', bargap=0.05)\niplot(fig, filename='custom-sized-subplot-with-subplot-titles')\n\n#plotly.offline.plot(fig, filename = ruta+\"\/plots\/mic_cluster_2.html\", auto_open=False)","6338d8c2":"X_2 = X_2[list(mi_scores_2.index)]\nX_train, X_test, y_train, y_test = train_test_split(X_2, y_2, shuffle=True, stratify=y_2, random_state=0, test_size=0.2)\n\n\nscores, importance = select_k_variables(mi_scores_2, X_train, y_train, X_test, y_test, solver, penalty, C, class_weight )[0], select_k_variables(mi_scores_2, X_train, y_train, X_test, y_test, solver, penalty, C, class_weight )[1]\n","03fa131f":"max_value = max(scores)\nmax_index = scores.index(max_value) \nX_2.drop(importance[:max_index], axis=1, inplace=True)\n\n\nprint(\"Number of feautures to drop:\", max_index)","f39aae0f":"df_train = pd.concat([X_1, y_1], axis=1)\ndf_test = pd.concat([X_2, y_2], axis=1)\ndf_train.to_csv(\"df_train.csv\", index=False)\n\ndf_test.to_csv(\"df_tes.csv\", index=False)","8ea3062b":"X_1 =df_train.drop(\"default payment next month\", axis=1)\nX_2 =df_test.drop(\"default payment next month\", axis=1)\ny_1 = df_train[[\"default payment next month\"]]\ny_2 = df_test[[\"default payment next month\"]]","c103655a":"X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(X_1, y_1, shuffle=True, stratify=y_1, random_state=0, test_size=0.3)\n\nX_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2, y_2, shuffle=True, stratify=y_2, random_state=0, test_size=0.2)\n\n\n#------------------------------------------------------------------\n\nregex = re.compile(r\"\\[|\\]|,<\", re.IGNORECASE)\n\nX_train_1.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train_1.columns.values]\nX_train_2.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_train_2.columns.values]\n\nX_test_2.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_test_2.columns.values]\nX_test_1.columns = [regex.sub(\"_\", col) if any(x in str(col) for x in set(('[', ']', '<'))) else col for col in X_test_1.columns.values]","4cf79698":"print(\"Dimension before resample for cluster 1:\", X_train_1.shape,)\nprint(\"Dimension before resample for cluster 2:\", X_train_2.shape)","d22d730a":"##Fit SMOTE TOMEK  for each cluster of clients\n\nresample = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\n\nX_train_1, y_train_1 = resample.fit_resample(X_train_1, y_train_1)\n\n\nX_train_2, y_train_2 = resample.fit_resample(X_train_2, y_train_2)\n\nprint(\"Dimension after resample for cluster 1:\", X_train_1.shape,)\nprint(\"Dimension after resample for cluster 2:\", X_train_2.shape)","eb653eca":"models = [  KNeighborsClassifier(n_neighbors=5),SVC(probability=True), XGBClassifier(), GaussianNB()]\nmodels_text = [  \"KNN\",\"SVC\", \"XGB\", \"GNB\"]\n\nfor vec in [X_train_1, y_train_1, X_test_1, y_test_1, X_train_2, y_train_2, X_test_2, y_test_2]:\n  vec.reset_index(drop=True, inplace=True)","be309313":"f1_scores_1, acuraccy_scores_1,  auc_scores_1,  models_fit_1 =train_baseline_models( models,models_text, X_train_1.values, y_train_1.values, n_splits_=10)","ee5f9c0d":"scores_names = [\"f1_score\", \"auc_score\", \"acuraccy_score\", ]\n\nscore_list = [f1_scores_1, auc_scores_1,acuraccy_scores_1]\nscores_1_dfs = {}\ni=0\nfor score_values in score_list:\n  for name, score in score_values.items():\n\n      scores_1_dfs[name+\"_\"+scores_names[i]] = pd.DataFrame(score_values[name], index=range(0,len(score_values[name]))).rename(columns={0:\"score\"})\n      scores_1_dfs[name+\"_\"+scores_names[i]][\"model\"] = name\n      scores_1_dfs[name+\"_\"+scores_names[i]][\"score_name\"] = scores_names[i]\n  i=i+1   \n\nresultados_modelo_1 = pd.concat(scores_1_dfs.values(), axis=0)","b97232cc":"fig = px.box(resultados_modelo_1 , x=\"model\", y=\"score\", color=\"score_name\", color_discrete_sequence=[\"cadetblue\", \"burlywood\", \"coral\", \"goldenrod\"],\n             title=\"Comparation between the scores for each model\")\nfig.update_layout(template='seaborn')\nfig.update_traces( marker_line_color='rgb(8,48,107)', marker_line_width=0.5, opacity=0.75)\n#plotly.offline.plot(fig, filename = ruta+\"\/plots\/comparacion_scores_4_modelos.html\", auto_open=False)\nfig.show()\n","b300d7e6":"#for name, model in models_fit_1.items():\n#  joblib.dump(model, ruta+\"\/models\/models_comparation_cluster_1\/model_\"+str(name)+ \".hdf5\")  ","d4013400":"f1_scores_2, acuraccy_scores_2,  auc_scores_2,  models_fit_2 =train_baseline_models( models,models_text, X_train_2.values, y_train_2.values, n_splits_=10)","74218ccb":"scores_names = [\"f1_score\", \"auc_score\", \"acuraccy_score\", ]\n\nscore_list = [f1_scores_2, auc_scores_2,acuraccy_scores_2]\nscores_2_dfs = {}\ni=0\nfor score_values in score_list:\n  for name, score in score_values.items():\n\n      scores_2_dfs[name+\"_\"+scores_names[i]] = pd.DataFrame(score_values[name], index=range(0,len(score_values[name]))).rename(columns={0:\"score\"})\n      scores_2_dfs[name+\"_\"+scores_names[i]][\"model\"] = name\n      scores_2_dfs[name+\"_\"+scores_names[i]][\"score_name\"] = scores_names[i]\n  i=i+1   \n\nresultados_modelo_2 = pd.concat(scores_2_dfs.values(), axis=0)","dd83f890":"fig = px.box(resultados_modelo_2 , x=\"model\", y=\"score\", color=\"score_name\", color_discrete_sequence=[\"cadetblue\", \"burlywood\", \"coral\", \"goldenrod\"],\n             title=\"Comparation between the scores for each model\")\n\nfig.update_layout(template='seaborn')\nfig.update_traces( marker_line_color='rgb(8,48,107)', marker_line_width=0.5, opacity=0.75)\n#plotly.offline.plot(fig, filename = ruta+\"\/plots\/comparacion_scores_4_modelos_cluster_22.html\", auto_open=False)\nfig.show()","8bff490b":"#for name, model in models_fit_2.items():\n#  joblib.dump(model, ruta+\"\/models\/models_comparation_cluster_2\/model_\"+str(name)+ \".hdf5\")  ","03ad6d8e":"model_1 = random_search_xgb(X_train_1, y_train_1, X_test_1, y_test_1, folds_ = 5)","65fbb863":"model_2 = random_search_xgb(X_train_2, y_train_2, X_test_2, y_test_2, folds_ = 3)","537e2207":"model_2 = model_2.best_estimator_\nmodel_1 = model_1.best_estimator_","f54e3bfe":"F1 = evalBinaryClassifier(model_1, X_test_1, y_test_1)","4087e0b6":"F1 = evalBinaryClassifier(model_2, X_test_2, y_test_2)","8d3a6435":"fig, ax = plt.subplots(figsize=(10,7))\nxgb.plot_importance(model_1, ax=ax, max_num_features=30 )\n#plt.savefig(ruta+\"\/plots\/featue_impt_model_1.png\",bbox_inches='tight')","d31a186c":"fig, ax = plt.subplots(figsize=(10,7))\nxgb.plot_importance(model_2, ax=ax, max_num_features=30 )\n#plt.savefig(ruta+\"\/plots\/featue_impt_model_2.png\",bbox_inches='tight')","26405bb5":"**Load Data**","281ccc87":"Exploration for: **Repayments** \n\n\nRemember:\n\n\n\n\n2: No consumption; -1: Paid in full; 0: The use of revolving credit; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.","1a5ec7ee":"## **Correlations**","c4166410":"# \ud83d\udd04 Customers Segmentation","6da42fa1":"**Exploring null values**","e36d1820":"Now to determine the number of variables to leave we are going to train a simple model like Logistic Regression varying the number of variables","faa256b6":"**WE HAVE A IMBALANCED DATASET, WE NEED TO FIX IT**\n\n* We have that there are categories of the variables **Marriage and Education** that are quite unbalanced, we can create a new category by putting it together.","deab0a49":"**Baseline models**","e50d86b5":"**SMOTE TOMEK: CREATING A BALANCED DATASET**","59b8963d":"<h4> Hi, it's me again.\ud83d\udc83\n    \n    \n    \nToday I want to share with you this great Kernel about credit card payment analysis. I've been working in the finance sector for a long time, so its time to transfer my knowledge to you(:\n\n    \n    \nPlease leave your comments and ideas to improve my Kernel. I hope you like it and enjoy the reading and your **UPVOTES would be highly appreciated!!!!**.\n\n\nPlease check my Dashboard for this Project here: https:\/\/defaultappnataliacastilla.herokuapp.com\/<\/h4>\n","7cedc42e":"To select the model, for each cluster, we trained 4 different baselines binary classification models: SVM, XGB, KNN, NB (each of a different nature). To guarantee the performance of each one we used K-fold in which we partition the data 10 times. In this way we obtain a list of scores from which we can obtain some basic statistics: standard deviation, mean, average; The metrics used for the evaluation in each Fold were: AUC, F1-Score, and Accuracy. To balance the data we used SMOTE-TOMEK which creates an artificial sample according to the nearest neighbors of each point.","3f190459":"# Feature Engineering\ud83c\udfa8","28ec242b":"## General Functions","0edea229":"**Feature selection for model 1: Cluster 1**","80dcfce7":"This is a great result we can see that as the number of segment increases the score deteriorates. So let's analyze the results for two and three clusters.\n\n*Number of clients per cluster*","06d750b3":"<center><img src=\"https:\/\/c.tenor.com\/aNOhj9NYx40AAAAi\/credit-card.gif\"><\/center>\n<center><h1>Payment default analysis: Credit cards \ud83d\udcb0<\/h1><\/center>","832f0e6b":"**OK, we don't have null values in any variable**","62a4bea0":"**We have many highly correlated variables, however remember that this type of behavior is normal if we have lag-type variables, as here.**","4db7eeb8":"**Training algorithms for cluster 1**","2231cff2":"To select the most important variables in each cluster I have decided to use the concept of entropy, which is widely used in information theory. The definition of this concept in physics represents the amount of information I need to be able to fully determine the state of a system and eliminate its uncertainty. Thus, when we do not know the state of a system, we have maximum entropy. In statistics entropy is used to determine the degree of dependence between two random variables, the mathematical form of this concept is similar to the concept of the union of sets, in which we add each part separately and subtract its intercept; in this case, we add separately the Shanon entropy of each variable.","7679732a":"As we observed in our data exploration, we found that the credit quota has a high standard deviation; this may make the performance of our model not stable. Therefore, I propose doing segmentation on the clients and thus grouping them according to their similarities. Therefore at the end, we will have a model for each segment.\n\n\nFor this segmentation we will employ the following variables:\n\n* LIMIT_BAL\n* Age\n* BILL_AMT1 - BILL_AMT6\n* PAY_AMT_AT\n* PAY_AMT_BT\n* ratio_bill_pay_bin_AT, ratio_bill_pay_nbin_BT\n* PAY_AMT_STD\n* BILL_AMT_STD","e5720a07":"Welcome to the section where the magic happens, here we will create some variables that will allow us to have a video of the behavior of each of our customers through variables:\n\n\nTrends: Vectires of payment behavior and quota.\n\nStatistics: Maximum and minimum values of balance variables, day in arrears. Standard deviations, averages by moving windows.","43a7b9a7":"**Exploring categorical features**","f533937c":"**Basic segmentation with: K-Means**","fa8a1f83":"**Random Grid Search Cluster 1**","9f55f64d":"We can see that for those who have professional studies the amount of money in the loan is higher than for other categories. In addition, all distributions seem to be skewed.","d7b08e39":"## Some other analysis","0e457c2b":"# \ud83d\udcd1 Business Context\n\nPreserving the financial health of customers is of great relevance for companies involved with the financial system.  But you may wonder how to preserve the financial health of each client? well, the solution for this problem consists of calculating the probability of payment of each client according to some variables and doing some strategies to anticipate customer needs.\n\n>  For this reason the objective of this project is to predict the probability of default on a given obligation, in this case, credit cards. This will allow the generation of strategies that minimize the risk of deterioration of the client's financial health. Additionally, to facilitate the development of collection strategies, it is proposed to use clustering algorithms to find homogeneous segments within the population and thus provide differential treatment to each customer.\n\n","a3b8bb44":"\n* The mean for our target variable is 0.221, which means that there are 22.1% of credit card that will default next month.\n\n* The average value for the amount of credit card limit is 167,484.32\nWe have that the standard deviation is 129,747.6 (**Is very Large**).\n\n\n* Regarding Eduaction level we have that the mean is 1.85 with standard deviation 0.79, so almost all the clients have university or graduate school in their education.\n\n* The average age is 35.5 years, with a standard deviation of 9.2.\n","19a72a6e":"**Training algorithms for cluster 2**","4f41df2d":"# Tunning a Xgboost Classifier","9d090678":"**For two clusters**","60f7ea71":"#  Modeling","db01670b":"# \ud83d\udd0d Data Description\n\n>  This research aimed at the case of customers default payments in Taiwan and compares the predictive accuracy of probability of default among six data mining methods. From the perspective of risk management, the result of predictive accuracy of the estimated probability of default will be more valuable than the binary result of classification - credible or not credible clients. Because the real probability of default is unknown, this study presented the novel sorting Smoothing Method to estimate the real probability of default. With the real probability of default as the response variable (Y), and the predictive probability of default as the independent variable (X).\n\n\n\n>**Variables:**\n\n>*LIMIT_BAL*: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his\/her family (supplementary) credit.\n>SEX: Gender (1 = male; 2 = female).\n\n>*EDUCATION*: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others).\n\n>*MARRIAGE*: Marital status (1 = married; 2 = single; 3 = divorse, 0= others).\n\n>*AGE*: Age (year).\n\n>*PAY0 - PAY6*: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: PAY0 = the repayment status in September, 2005; PAY1 = the repayment status in August, 2005; . . .;PAY6 = the repayment status in April, 2005. The measurement scale for the repayment status is: -2: No consumption; -1: Paid in full; 0: The use of revolving credit; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above.\n\n>*BILL_AMT1- BILL_AMT6*: Amount of bill statement (NT dollar). BILL_AMT1 = amount of bill statement in September, 2005; BILL_AMT2 = amount of bill statement in August, 2005; . . .; BILL_AMT6 = amount of bill statement in April, 2005.\n\n>*PAY_AMT1-PAY_AMT2*: Amount of previous payment (NT dollar). PAY_AMT1 = amount paid in September, 2005; PAY_AMT2 = amount paid in August, 2005; . . .; PAY_AMNT6 = amount paid in April, 2005.\n\n\n\n","3ac541ff":"# Feature selection for each model (2 clusters)","303a6823":"**It is found that for men there is a clear relationship between non-payment and level of education 1. This could be provided as a suggestion to the business experts to improve this aspect**","6e7af1a7":"Exploration for our target variable: **Default payment next month**","e068e07a":"**The data is speaking to us and telling us: it is better not to lend money to people over 72 years of age (:**","c526d71c":"# Evaluation","a4d28cae":"**Features selection for cluster 2**","a5efc9e0":"**Ok, this result is very interesting, note that for very small loans we have the highest amount of default and that this decreases as the credit quota increases.**","9e066805":"# EDA \ud83d\udd0e","28ae306a":"**From the above graph we can see that in general the credit quota is also related to marital status. In almost all age groups, marital status=1 is the one with the highest quota, the only group that differs from this behavior is the youngest group.**"}}