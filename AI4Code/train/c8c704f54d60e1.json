{"cell_type":{"5b571238":"code","1bd13940":"code","bbda1840":"code","23404400":"code","efc48ae2":"code","7c71765d":"code","0378e48c":"code","362d238b":"code","a5dc857d":"code","b4e1ee8d":"code","a3b9ab91":"code","bb70a3eb":"code","b9be3833":"code","081e7a4f":"code","820e18c5":"code","2e7220be":"code","f7251e59":"code","a2ba55ff":"code","8054e632":"code","e84dfaf0":"code","9ac5e8c3":"code","1e28b610":"code","4f6b69ab":"code","0412a865":"code","64f579a6":"code","46bc754b":"code","439f4c0d":"code","69847a64":"code","9e186fdf":"code","9b4fe0b7":"code","f24a185f":"code","3b93e0f0":"code","9f2cd6bc":"code","211e4641":"code","3de0ca72":"code","a4710f6e":"code","5a563ffe":"code","c6df1fbd":"code","11138246":"code","416238bb":"code","aab5df02":"code","b40399b1":"code","132b5e85":"code","ea67e800":"code","4462855a":"code","48d4c391":"code","fbcc8f8c":"code","3d98d0fd":"code","035dce28":"code","24b86f57":"markdown","e8a5f140":"markdown","7e0cddd1":"markdown","4e96fac8":"markdown","f0851fe6":"markdown","e31fb740":"markdown","33336174":"markdown","b19348b7":"markdown","463f5907":"markdown","c392d74a":"markdown","912a712c":"markdown","175425ca":"markdown","7370394b":"markdown","57d9b610":"markdown","21d6cf6d":"markdown","796a6b2f":"markdown","c9c5074c":"markdown","3a1dab79":"markdown","d60df964":"markdown","4ac3dc63":"markdown","e806265c":"markdown","ecc11716":"markdown","0b78d207":"markdown","9fd77d51":"markdown","efef834c":"markdown"},"source":{"5b571238":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1bd13940":"train=pd.read_csv(\"..\/input\/housingprices.txt\")\ntrain\n#Each row represents one district in California and there 10 columns or features\n#We have only one non-numerical column, ocean_proximitt column:","bbda1840":"train[\"ocean_proximity\"].value_counts()\n#df.value_counts() is very useful method because we can see how many categories each column has","23404400":"train.describe(include=\"all\") # we get overal statistical information about cumerical columns of the data","efc48ae2":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nsns.pairplot(train)\n# we get pair relations of the columns","7c71765d":"train.hist(bins=50,figsize=(20,15))","0378e48c":"housing = train.copy() # here we create a copy so that we can play with it without harming the training set\nhousing","362d238b":"#Since there is geographical information(latitude and longitude of each district),\n# it is better to create a scatterplot of all districts\nhousing.plot(x=\"longitude\",y=\"latitude\",kind=\"scatter\")","a5dc857d":"#This looks like California all right, but other than that it is hard to see any particular pattern. \n#Setting the alpha option to 0.1 makes it much easier to visualize the places where there is a high density of data points\nhousing.plot(x=\"longitude\",y=\"latitude\",kind=\"scatter\",alpha=0.1,figsize=(20,15))\n#Now that\u2019s much better: you can clearly see the high-density areas, namely the Bay Area and around Los Angeles and San Diego","b4e1ee8d":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\ns=housing[\"population\"]\/100, label=\"population\", figsize=(20,15),\nc=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,)\nplt.legend()\n#The radius of each circle represents the district\u2019s population (option s), and the color represents the price (option c). \n#We will use a predefined color map (option cmap) called jet, which ranges from blue (low values) to red (high prices):\n#c value are represented in the colorbar \n#The size of the circle represent the amounth of the population and the color inside it is the median house value\n","a3b9ab91":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\ns=housing[\"population\"]\/10, label=\"population\", figsize=(20,15),\nc=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,title=\"California housing prices: red is expensive, blue is cheap, larger circles indicate areas with a larger population\")\nplt.legend()\n# we divide population by 100 because otherwise the circles will be tto big in the map\n#This image tells you that the housing prices are very much related to the location (e.g., close to the ocean) \n#and to the population density, as you probably knew already.","bb70a3eb":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\ns=housing[\"population\"]\/20, label=\"population\", figsize=(20,15),\nc=\"median_income\", cmap=plt.get_cmap(\"jet\"), colorbar=True,)\nplt.legend()\n#The size of the circle represent the amounth of the population and the color inside it is the median house value\n# we can change it with another column name like median income","b9be3833":"#we can easily compute the standard correlation coefficient (also called Pearson\u2019s r) between every pair of\n# by attributes using the corr() method:\ncorrelation=housing.corr()\ncorrelation","081e7a4f":"#we can specifically see the correlation of a specific column\ncorrelation[\"median_house_value\"]","820e18c5":"correlation[\"median_house_value\"].sort_values(ascending=False) #now we list them from the highest to the lowest","2e7220be":"from pandas.plotting import scatter_matrix\nimp_features=[\"median_house_value\", \"median_income\", \"total_rooms\",\"housing_median_age\"]\nscatter_matrix(housing[imp_features],figsize=(15,10))\n#Median_income seems the most correlated feature among others with respect to house prices","f7251e59":"sns.pairplot(housing) #sns.pairplot() gives us all binary relation between a\u00f8\u00f8 variables","a2ba55ff":"#Here we create three new columns which are more useful for our algorithm:\nhousing[\"rooms_per_household\"]=housing[\"total_rooms\"]\/housing[\"households\"]\nhousing[\"bedrooms_per_room\"]=housing[\"total_bedrooms\"]\/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]\/housing[\"households\"]\nhousing.head()","8054e632":"#Lets check new correlations:\ncorr_matrix=housing.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)\n#Below we see that bedroom per room is negatively correlated with the value of house \n#While number of bedrooms in the district  does not make sense or not crrelate with the value of house\n#Secondly rooms per household effects also a positively the value of the house while households column does not make sense","e84dfaf0":"new_features=[\"median_house_value\", \"rooms_per_household\", \"population_per_household\",\"bedrooms_per_room\"]\nscatter_matrix(housing[new_features],figsize=(15,10))\n#Below we visualize the new features we added with the value of house","9ac5e8c3":"from sklearn.impute import SimpleImputer\nimputer=SimpleImputer(strategy=\"median\")\n#Since the median can only be computed on numerical attributes,we create a copy of the data without the text attribute ocean_proximity:\nhousing_new=housing.drop(\"ocean_proximity\",axis=1)\nimputer.fit(housing_new)\nimputer.statistics_#The imputer has simply computed the median of each attribute and stored theresult in its statistics_ variable\n","1e28b610":"X = imputer.transform(housing_new)\nX\n","4f6b69ab":"housing_ready=pd.DataFrame(X,columns=housing_new.columns,index=housing_new.index)\nhousing_ready.head()","0412a865":"plt.figure(figsize=(15,10))\nsns.heatmap(housing_ready.isnull()) # we can visualize it to see whether there are missing values or not","64f579a6":"housing_cat=housing[[\"ocean_proximity\"]]\nhousing_cat","46bc754b":"#we need to convert these categories from text to numbers\nfrom sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\n","439f4c0d":"housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat) #This returns a numpy array\nhousing_cat_encoded[:10]","69847a64":"ordinal_encoder.categories_","9e186fdf":"housing[\"ocean_proximity\"]=pd.DataFrame(housing_cat_encoded)\nhousing[\"ocean_proximity\"]=housing[\"ocean_proximity\"].fillna(housing[\"ocean_proximity\"].median())\nhousing[\"ocean_proximity\"]","9b4fe0b7":"data = pd.concat([housing_ready, housing[\"ocean_proximity\"]], axis = 1)\ny=data[\"median_house_value\"] # Here we assign our target variable\ndata.drop(\"median_house_value\",axis=1,inplace=True)\nX=data\ny\n","f24a185f":"X","3b93e0f0":"from sklearn.preprocessing import OneHotEncoder\ncat_encoder = OneHotEncoder()\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot","9f2cd6bc":"#First of all, we split data into training and test set before we apply the algorithm\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)","211e4641":"from sklearn.linear_model import LinearRegression\nlm=LinearRegression()\nlm.fit(X_train,y_train)","3de0ca72":"#In order to evaluate the performance we need to predict the test data and compare the predictions with the actual test data values\npredictions=lm.predict(X_test)\n#here we predict the y_test values from X_test data according to the our trained Linear Regression data\npredictions","a4710f6e":"# here I will visualize the real test values(y_test) versus the predicted values.\nsns.scatterplot(y_test,predictions)\n#It seems that our linear regression model predict ver well","5a563ffe":"# We will evaluate our model performance by calculating the residual sum of squares and the explained variance score\nfrom sklearn import metrics\nprint(\"MAE:\",metrics.mean_absolute_error(y_test,predictions))\nprint (\"MSE:\",metrics.mean_squared_error(y_test,predictions))\nprint(\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test,predictions)))","c6df1fbd":"#Evaluation of  the explained variance score (R^2)\nmetrics.explained_variance_score(y_test,predictions) #This shows our model predict %62 of the variance","11138246":"sns.distplot(y_test-predictions,bins=50) #this figure also proves that our model fits very good\n#There is no huge differences between our predictions and actual y data","416238bb":"cdf=pd.DataFrame(lm.coef_,X.columns,columns=[\"Coefficients\"])\ncdf # This represents one unit increase in every independent variable or columns cause how many increase or decrease in the target","aab5df02":"#Now we train the Decision Tree Regression Model\nfrom sklearn.tree import DecisionTreeRegressor\ntree=DecisionTreeRegressor()\ntree.fit(X_train,y_train)","b40399b1":"predictions=tree.predict(X_test)\npredictions","132b5e85":"# Now we will evaluate how good this model is: here I will visualize the real test values(y_test) versus the predicted values.\nsns.scatterplot(y_test,predictions)","ea67e800":"#Evaluation of  the explained variance score (R^2)\nmetrics.explained_variance_score(y_test,predictions) #This shows our model predict %62 of the variance\n","4462855a":"from sklearn.ensemble import RandomForestRegressor\nforest=RandomForestRegressor()\nforest.fit(X_train,y_train)\n# As we can see trying different machine learning model are not difficult after reparing data for machine learning","48d4c391":"predictions=forest.predict(X_test)\npredictions","fbcc8f8c":"# Now we will evaluate how good this model is: here I will visualize the real test values(y_test) versus the predicted values.\nsns.scatterplot(y_test,predictions)","3d98d0fd":"print(\"MAE:\",metrics.mean_absolute_error(y_test,predictions))\nprint (\"MSE:\",metrics.mean_squared_error(y_test,predictions))\nprint(\"RMSE:\",np.sqrt(metrics.mean_squared_error(y_test,predictions)))","035dce28":"#Evaluation of  the explained variance score (R^2)\nmetrics.explained_variance_score(y_test,predictions) #This shows our model predict %80 of the variance\n\n","24b86f57":"But the result is a plain NumPy array containing the transformed features. we need to make it back into a pandas DataFrame","e8a5f140":"Scikit-Learn provides a handy class to take care of missing values:\nSimpleImputer. Here is how to use it. First, you need to create a\nSimpleImputer instance, specifying that you want to replace each attribute\u2019s\nmissing values with the median of that attribute:\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"median\")","7e0cddd1":"The predictions of this model is far better than the other although we have not done aby changes with the data","4e96fac8":"*Most Machine Learning algorithms cannot work with missing features, so we need to fix them:\nWe can accomplish these easily using DataFrame\u2019s dropna(), drop(), and\nfillna() methods:\nhousing.dropna(subset=[\"total_bedrooms\"]) # option 1\nhousing.drop(\"total_bedrooms\", axis=1) # option 2\nmedian = housing[\"total_bedrooms\"].median() # option 3\nhousing[\"total_bedrooms\"].fillna(median, inplace=True)","f0851fe6":"The correlation coefficient only measures linear correlations (\u201cif x goes up, then y generally goes up\/down\u201d). It may completely miss out on nonlinear relationships (e.g., \u201cif x is close to 0, then y generally goes up\u201d). Note how all the plots of the bottom row have a correlation coefficient equal to 0, despite the fact that their axes are clearly not independent: these are examples of nonlinear relationships.","e31fb740":"Now we can use this \u201ctrained\u201d imputer to transform the training set by replacing missing values with the learned medians:","33336174":"we can get the list of categories using the categories_ instance variable. \nIt is a list containing a 1D array of categories for each categorical attribute","b19348b7":"Another way to check for correlation between attributes is to use the pandas\nscatter_matrix() function, which plots every numerical attribute against\nevery other numerical attribute. Since there are now 11 numerical attributes,\nyou would get 11 = 121 plots, which would not fit on a page\u2014so let\u2019s just\nfocus on a few promising attributes that seem most correlated with the\nmedian housing value","463f5907":"4.1. Handling Numerical Variables","c392d74a":"5.Feature Scaling:","912a712c":"However One issue with this representation is that ML algorithms will assume that two\nnearby values are more similar than two distant values. This may be fine in\nsome cases (e.g., for ordered categories such as \u201cbad,\u201d \u201caverage,\u201d \u201cgood,\u201d and\n19\n\u201cexcellent\u201d), but it is obviously not the case for the ocean_proximity column\n(for example, categories 0 and 4 are clearly more similar than categories 0\nand 1). To fix this issue, a common solution is to create one binary attribute\nper category: one attribute equal to 1 when the category is \u201c<1H OCEAN\u201d\n(and 0 otherwise), another attribute equal to 1 when the category is\n\u201cINLAND\u201d (and 0 otherwise), and so on. This is called one-hot encoding,\nbecause only one attribute will be equal to 1 (hot), while the others will be 0\n(cold). The new attributes are sometimes called dummy attributes. Scikit-\nLearn provides a OneHotEncoder class to convert categorical values into onehot\nvectors","175425ca":"4.2. Handling Text and Categorical Variables:","7370394b":"1. Getting Data and First Insights About the Data","57d9b610":"It seems decision tree model is not good for this data compared to linear regression, so we need try another model:\nThe RandomForestRegressor works by training many Decision Trees on random subsets of the features, then averaging out their predictions","21d6cf6d":"SimpleImputer(*, missing_values=nan, strategy='mean', fill_value=None, verbose=0, copy=True, add_indicator=False)\n |  \n |  Imputation transformer for completing missing values.\n Parameters\n |  ----------\n |  missing_values : number, string, np.nan (default) or None\n |      The placeholder for the missing values. All occurrences of\n |      `missing_values` will be imputed.\n \n strategy : string, default='mean'\n |      The imputation strategy.\n |  \n |      - If \"mean\", then replace missing values using the mean along\n |        each column. Can only be used with numeric data.\n |      - If \"median\", then replace missing values using the median along\n |        each column. Can only be used with numeric data.\n |      - If \"most_frequent\", then replace missing using the most frequent\n |        value along each column. Can be used with strings or numeric data.\n |      - If \"constant\", then replace missing values with fill_value. Can be\n |        used with strings or numeric data.\n ","796a6b2f":"4.Preparing Data for the Algorithm","c9c5074c":"6.Training and Evaluating the Trainig Set ","3a1dab79":"The total number of rooms in a district is not very useful if you don\u2019t know how many households there are. What you really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful: you probably want to compare it to the number of rooms. And the population per household also seems like an interesting attribute combination to look at","d60df964":"4.Cleaning the Data and Adding New Features:","4ac3dc63":"OrdinalEncoder(*, categories='auto', dtype=<class 'numpy.float64'>)\n |  \n |  Encode categorical features as an integer array.\n |  \n |  The input to this transformer should be an array-like of integers or\n |  strings, denoting the values taken on by categorical (discrete) features.\n |  The features are converted to ordinal integers. This results in\n |  a single column of integers (0 to n_categories - 1) per feature.","e806265c":"One of the most important transformations you need to apply to your data is\nfeature scaling. With few exceptions, Machine Learning algorithms don\u2019t\nperform well when the input numerical attributes have very different scales.\nThis is the case for the housing data: the total number of rooms ranges from\nabout 6 to 39,320, while the median incomes only range from 0 to 15.\n\n\n*There are two common ways to get all attributes to have the same scale: minmax\nscaling and standardization:\n\n-Min-max scaling (many people call this normalization) is the simplest: values\nare shifted and rescaled so that they end up ranging from 0 to 1. We do this by\nsubtracting the min value and dividing by the max minus the min. Scikit-\nLearn provides a transformer called MinMaxScaler for this.\n\n-Standardization is different: first it subtracts the mean value (so standardized\nvalues always have a zero mean), and then it divides by the standard deviation\nso that the resulting distribution has unit variance. Unlike min-max scaling,\nstandardization does not bound values to a specific range, which may be a\nproblem for some algorithms (e.g., neural networks often expect an input\nvalue ranging from 0 to 1). However, standardization is much less affected by\noutliers. For example, suppose a district had a median income equal to 100\n(by mistake). Min-max scaling would then crush all the other values from 0\u2013\n15 down to 0\u20130.15, whereas standardization would not be much affected.\nScikit-Learn provides a transformer called StandardScaler for\nstandardization.","ecc11716":"3.Visualization and Deeper Insights of the Data","0b78d207":"The values of MAE,MSE and RMSE was as follows:\nMAE: 50510.92379463419\nMSE: 4885570662.763187\nRMSE: 69896.85731678634\nWhen we compare the differences between the predictions of two models and the actual data,\nRandomForestRegressor performs better than the Linear Regression Model","9fd77d51":"The correlation coefficient ranges from \u20131 to 1. When it is close to 1, it means that there is a strong positive correlation; for example, the median house value tends to go up when the median income goes up.When the coefficient is close to \u20131, it means that there is a strong negative correlation; you can see a small negative correlation between the latitude and the median house value (i.e., prices have a slight tendency to go down when you go north). Finally, coefficients close to 0 mean that there is no linear correlation","efef834c":"All of these signs show that Linear Regression is not a good option to predict. We need more complex algorithms or models.\nWe will try DecisionTreeRegressor model which is a powerful model, capable of finding complex nonlinear relationships in the data"}}