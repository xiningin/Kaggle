{"cell_type":{"7ab71564":"code","ed5737f0":"code","ea1f664c":"code","86a376c3":"code","86b79338":"code","de1a10b4":"code","7d6d75c0":"code","e70f1cf6":"code","6b33bd65":"code","ed26f5b1":"code","faa98168":"code","4d2324e8":"code","31270045":"code","74876f8f":"code","b9455d99":"code","20d0fe33":"code","8ef31250":"code","2018ebe6":"code","b0d62c94":"code","0c9eca93":"code","b6da7587":"code","da8af015":"code","c2687c5c":"code","baaa04f8":"code","cd982f9f":"code","c4e347f1":"code","797e0c16":"code","2339497c":"code","5e658b37":"code","efd49c85":"code","29a38585":"code","ec7b6677":"code","02d7813c":"code","09484f18":"code","3ff859d3":"code","a4e6389f":"code","cc046c4b":"code","af34d5e3":"code","483cf9f2":"code","e5e8957a":"code","c699c578":"code","e2e70bdd":"code","d89b6a58":"code","3f46db9f":"code","d455ecc3":"code","69e2c7dc":"code","09e83554":"code","aec28946":"code","5b28265d":"code","17e02cb2":"code","e1f6025c":"code","ed7f0f97":"code","122dc09b":"code","16f10a77":"code","57ad60e7":"code","12bde452":"code","2fc8f4e2":"code","e6bcde33":"code","b68b83d5":"code","568cbbf9":"code","95215ec1":"code","d65f5c5d":"code","bea61f75":"markdown","b67985b0":"markdown","81a2dd73":"markdown","73b9a4e9":"markdown","be08b13b":"markdown","714ef31d":"markdown","cb81c234":"markdown","cffaadb2":"markdown","2ced09cd":"markdown","f6347b56":"markdown","bc1d72b7":"markdown","f8bad997":"markdown","4e63c19b":"markdown","3a03aa8b":"markdown","9f0a3b46":"markdown","a9bc10e2":"markdown","483b40a2":"markdown","0d4c6dd5":"markdown","566ba84a":"markdown","13ee53d8":"markdown","46975024":"markdown","8c0bfebd":"markdown","300a92c3":"markdown","4ed869fa":"markdown","b44a5b1e":"markdown","b4d3e475":"markdown","b8415143":"markdown","47b5be7f":"markdown","840dbff0":"markdown","d382eb00":"markdown","36c7b4af":"markdown","cd331145":"markdown","50f3f546":"markdown","6317ef35":"markdown","43e6296b":"markdown","49729882":"markdown","a8771ca7":"markdown","694c38be":"markdown","e2de19f4":"markdown","83c4405f":"markdown","e4177a5e":"markdown","9fd872e6":"markdown","45962e93":"markdown","8e43e0fd":"markdown","5684664d":"markdown","912f6ecc":"markdown","2f7aa243":"markdown","19fe1535":"markdown","8e864d75":"markdown","c279abf7":"markdown","d378e772":"markdown","ba911a78":"markdown","251f5552":"markdown","a22a2b1a":"markdown","fdd31d2b":"markdown","f2a461e2":"markdown","54fa7157":"markdown","13ebd1bf":"markdown","591c1053":"markdown","49672ab2":"markdown"},"source":{"7ab71564":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ed5737f0":"#import H2O and other libraries that will be sued in this tutorial\n\nimport h2o\nimport matplotlib as plt\n%matplotlib inline\n\n#import the Estimators\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator\nfrom h2o.estimators import H2ORandomForestEstimator\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\n\n#import h2o grid search\nimport h2o.grid\nfrom h2o.grid.grid_search import H2OGridSearch\nprint('Setup Complete')\n","ea1f664c":"h2o.init()","86a376c3":"loan_level = h2o.import_file(\"https:\/\/s3.amazonaws.com\/data.h2o.ai\/DAI-Tutorials\/loan_level_500k.csv\")\n","86b79338":"loan_level.head()","de1a10b4":"loan_level.describe()","7d6d75c0":"loan_level['DELINQUENT'].table()","e70f1cf6":"train, valid, test = loan_level.split_frame([0.7, 0.15], seed = 42)","6b33bd65":"print('train:%d, valid:%d, test:%d' % (train.nrows, valid.nrows, test.nrows))","ed26f5b1":"y = 'DELINQUENT'\nignore = ['DELINQUENT', 'PREPAID', 'PREPAYMENT_PENALTY_MORTGAGE_FLAG', 'PRODUCT_TYPE']\nx = list(set(train.names) - set(ignore))","faa98168":"glm = H2OGeneralizedLinearEstimator(family = 'binomial', seed = 42)","4d2324e8":"%time \nglm.train(x = x, y = y, training_frame = train, validation_frame = valid)","31270045":"glm","74876f8f":"glm.plot(metric='negative_log_likelihood')","b9455d99":"glm.varimp_plot()","20d0fe33":"glm.predict(valid).head(10)","8ef31250":"default_glm_perf=glm.model_performance(valid)","2018ebe6":"print(default_glm_perf.auc())","b0d62c94":"rf = H2ORandomForestEstimator (seed = 42, model_id='default_rf')\n%time rf.train(x = x, y = y, training_frame=train, validation_frame=valid)\n                               ","0c9eca93":"rf","b6da7587":"rf.plot(metric='auc')","da8af015":"rf.varimp_plot(20)","c2687c5c":"rf.accuracy()","baaa04f8":"rf.F1()","cd982f9f":"rf.predict(valid)","c4e347f1":"rf_default_per = rf.model_performance(valid)","797e0c16":"gbm = H2OGradientBoostingEstimator(seed = 42, model_id='default_gbm')\n%time gbm.train(x = x, y = y, training_frame=train, validation_frame=valid)","2339497c":"gbm","5e658b37":"gbm.varimp_plot(20)","efd49c85":"gbm.plot(metric='auc')","29a38585":"gbm.predict(valid)","ec7b6677":"default_gbm_per = gbm.model_performance(valid)","02d7813c":"glm_grid = h2o.grid.H2OGridSearch (\n    H2OGeneralizedLinearEstimator(family= 'binomial', lambda_search = True),\n\n    hyper_params = {\n        'alpha': [x*0.01 for x in range(0, 100)],\n        'lambda': [x*1e-8 for x in range(0, 1000)],\n        'missing_values_handling': ['Skip', 'MeanImputation']\n    },\n\n    grid_id = 'glm_random_grid',\n\n    search_criteria = {\n        'strategy': 'RandomDiscrete',\n        'max_models': 200,\n        'max_runtime_secs': 300,\n        'seed': 42}\n)\n\n%time glm_grid.train(x = x, y = y, training_frame=train, validation_frame=valid)","09484f18":"sorted_glm_grid = glm_grid.get_grid(sort_by='auc', decreasing=True)\nsorted_glm_grid.sorted_metric_table()","3ff859d3":"tuned_glm = glm_grid.models[0]\ntuned_glm.summary()","a4e6389f":"tuned_glm_perf = tuned_glm.model_performance(valid)","cc046c4b":"print('Default GLM AUC: %.4f \\nTuned GLM AUC: %.4f' % (default_glm_perf.auc(), tuned_glm_perf.auc()))","af34d5e3":"print('Default GLM F1 Score', default_glm_perf.F1())\nprint('Tuned FLm F1 Score', tuned_glm_perf.F1())","483cf9f2":"print('Default GLM: ', default_glm_perf.confusion_matrix())\nprint('Tuned GLM: ', tuned_glm_perf.confusion_matrix())","e5e8957a":"hyper_parameters = {'max_depth': [1,3,5,6,7,8,9,10,12,13,15,20,25,35]}\n\nrf = H2ORandomForestEstimator(\n        seed= 42,\n        stopping_rounds = 5,\n        stopping_tolerance = 1e-4,\n        stopping_metric = 'auc',\n        model_id= 'rf')\n\ngrid_id = 'depth_grid'\n\nsearch_criteria = {'strategy': \"Cartesian\"}\n\n#Grid Search\n\nrf_grid = H2OGridSearch(model = rf,\n                       hyper_params = hyper_parameters,\n                       grid_id = grid_id,\n                       search_criteria = search_criteria)\n\n%time rf_grid.train(x=x, y=y, training_frame=train, validation_frame=valid)","c699c578":"sorted_rf_depth = rf_grid.get_grid(sort_by='auc',decreasing=True)\nsorted_rf_depth.sorted_metric_table()","e2e70bdd":"hyper_parameters = {'categorical_encoding': ['auto','enum',\n                                             'one_hot_explicit', 'binary',\n                                             'label_encoder', 'sort_by_response',\n                                             'enum_limited'],\n                   'histogram_type': ['uniform_adaptive', 'random',\n                                      'quantiles_global', 'round_robin'],\n                   'nbins': [10,12,15,18,20,25,30,40,50] #Default = 20\n                   }\n\nrf = H2ORandomForestEstimator(max_depth = 10,\n                             ntrees = 50,\n                             seed = 42,\n                             stopping_rounds = 5,\n                             stopping_tolerance = 1e-5,\n                             stopping_metric = 'auc',\n                              model_id = 'rf'\n                             )\n\ngrid_id =  'rf_random_grid_'\n\nsearch_criteria = {'strategy': 'RandomDiscrete',\n                  'max_models': 100,\n                  'max_runtime_secs': 900,\n                  'seed': 42\n                  }\n\nrf_grid = H2OGridSearch(model = rf,\n                       hyper_params = hyper_parameters,\n                       grid_id = grid_id,\n                       search_criteria = search_criteria)\n\n%time rf_grid.train(x=x, y=y, training_frame = train, validation_frame = valid)\n    \n","d89b6a58":"sorted_rf = rf_grid.get_grid(sort_by='auc', decreasing=True)\nsorted_rf.sorted_metric_table()","3f46db9f":"tuned_rf = H2ORandomForestEstimator (max_depth = 10,\n                                    seed = 42,\n                                    model_id = 'tuned_rf',\n                                    categorical_encoding = 'auto',\n                                    histogram_type ='quantiles_global',\n                                    mtries = 4,\n                                    nbins = 10,\n                                    ntrees = 500,\n                                     \n                                    stopping_rounds = 3,\n                                    stopping_tolerance = 1e-5,\n                                    stopping_metric = 'auc'\n                                    )\n%time tuned_rf.train(x=x, y=y, training_frame=train, validation_frame=valid)","d455ecc3":"tuned_rf.plot(metric='auc')","69e2c7dc":"tuned_rf_per = tuned_rf.model_performance(valid)\ntuned_rf_per.auc()","09e83554":"tuned_rf_per.F1()","aec28946":"print('Default RF AUC: %.4f \\nTuned RF AUC: %.4f' % (rf_default_per.auc(), tuned_rf_per.auc()))","5b28265d":"print('Default RF F1 Score:', rf_default_per.F1())\nprint('Tuned RF F1 Score:', tuned_rf_per.F1())","17e02cb2":"print('Default RF: ', rf_default_per.confusion_matrix())\nprint('Tuned RF: ', tuned_rf_per.confusion_matrix())","e1f6025c":"hyper_params = {'max_depth': [3,4,5,6,7,8,9,10,11,12,13,15],\n               }\n\ngbm = H2OGradientBoostingEstimator(model_id = 'grid_gbm', ntrees = 50,\n                                  seed = 42\n                                  )\ngbm_grid = H2OGridSearch(gbm, hyper_params,\n                        grid_id = 'depth_gbm_grid',\n                        search_criteria = {'strategy': \"Cartesian\"})\n\n%time gbm_grid.train(x=x, y=y, training_frame=train, validation_frame=valid)","ed7f0f97":"sorted_gbm_depth = gbm_grid.get_grid(sort_by='auc', decreasing=True)\nsorted_gbm_depth.sorted_metric_table()","122dc09b":"gbm = H2OGradientBoostingEstimator(\n    max_depth = 6,\n    ntrees = 50,\n    seed = 42,\n    model_id = 'grid_gbm'\n)\n\nhyper_params_tune = {\n    'sample_rate': [x\/100. for x in range(20,101)],\n    'col_sample_rate': [x\/100. for x in range(20,101)],\n    'col_sample_rate_per_tree': [x\/100. for x in range(20,101)],\n    'col_sample_rate_change_per_level': [x\/100. for x in range(90,111)],\n    'learn_rate': [.5, .25, 0.1, 0.07, 0.05, 0.01, 0.001],\n    'nbins': [2**x for x in range(4,11)],\n    'nbins_cats': [2**x for x in range(4,13)],\n    'min_split_improvement': [0,1e-8,1e-6,1e-4],\n    'histogram_type': ['UniformAdaptive', 'QuantilesGlobal', 'RoundRobin']}\n\nsearch_criteria_tune = {'strategy': 'RandomDiscrete',\n                       'max_runtime_secs': 1200,\n                       'max_models': 100, ##build no more than 100 models\n                       'seed': 42 }\n\nrandom_grid = H2OGridSearch(model=gbm, \n                            hyper_params=hyper_params_tune,\n                           grid_id= 'random_grid',\n                           search_criteria = search_criteria_tune)\n\n%time random_grid.train(x=x, y=y, training_frame=train, validation_frame=valid)","16f10a77":"sorted_random_search = random_grid.get_grid(sort_by='auc',decreasing=True)\nsorted_random_search.sorted_metric_table()","57ad60e7":"tuned_gbm = H2OGradientBoostingEstimator(max_depth = 6,\n                                        ntrees = 200,\n                                        sample_rate = 0.7,\n                                        col_sample_rate = 0.68,\n                                        col_sample_rate_per_tree = 0.59,\n                                        col_sample_rate_change_per_level = 0.92,\n                                        learn_rate = 0.1,\n                                        nbins = 16,\n                                        nbins_cats = 128,\n                                        min_split_improvement = 1e-6,\n                                        histogram_type = 'UniformAdaptive',\n                                         \n                                        seed = 42,\n                                        model_id = 'tuned_gbm',\n                                        stopping_rounds = 3,\n                                        stopping_tolerance = 1e-5,\n                                        stopping_metric = 'auc'\n                                         \n                                    )\n%time tuned_gbm.train(x=x, y=y, training_frame=train, validation_frame=valid)","12bde452":"tuned_gbm_per = tuned_gbm.model_performance(valid)\nprint(tuned_gbm_per.auc())\nprint(tuned_gbm_per.F1())","2fc8f4e2":"tuned_gbm_per.confusion_matrix()","e6bcde33":"print('Default GBM AUC: %.4f \\nTuned GBM AUC: %.4f' % (default_gbm_per.auc(), tuned_gbm_per.auc()))","b68b83d5":"glm_test_per = tuned_glm.model_performance(test)\nrf_test_per = tuned_rf.model_performance(test)\ngbm_test_per = tuned_gbm.model_performance(test)","568cbbf9":"print('GLM Test AUC: %.4f \\nRF Test AUC: %.4f \\nGBM Test AUC: %.4f'\n     % (glm_test_per.auc(), rf_test_per.auc(), gbm_test_per.auc()))","95215ec1":"print('GLM Test F1 Score: ', glm_test_per.F1())\nprint('RF Test F1 Score: ', rf_test_per.F1())\nprint('GBM Test F1 Score: ', gbm_test_per.F1())","d65f5c5d":"print('GLM Confusion Matrix: ', glm_test_per.confusion_matrix())\nprint('RF Confusion Matrix: ', rf_test_per.confusion_matrix())\nprint('GBM Confusion Matrix: ', gbm_test_per.confusion_matrix())","bea61f75":"Gradient Boosting Machine (for Regression and Classification) is a forward learning ensemble method. H2O's GBM sequentially builds classification trees on all the features of the dataset in a fully distributed way - each tree is built in parallel. H2O's GBM fits consecutive trees where each solves for the net loss of the prior trees.\nSometimes GBMs tend to be the best possible models because they are robust and directly optimize the cost function. On the other hand, they tend to overfit, so you need to find the proper stopping point; they are sensitive to noise, and they have several hyper-parameters.\n\nDefining a GBM model is as simple as the other models we have been working with.","b67985b0":"From the variable importance plot, we can see that the most significant feature is SERVICER_NAME. In the most important feature, we have different banks or \"servicers,\" and in our linear model, each one makes a difference; for that reason, we see that the first four variables in the plot above are 4 of the servicers in the dataset. These services are the most influential to our model in making predictions of whether someone will default or not. Please keep in mind that it does not necessarily mean that if someone gets a loan from Wells Fargo, they have a high probability of default.\n\nWe will take a look at the first ten predictions of our model with the following command:","81a2dd73":"- lets take a look if the model is not overfitting:","73b9a4e9":"### print the AUC for default and the tuned model:","be08b13b":"## Tune the GBM model with H2O GridSearch","714ef31d":"- The AUC value for our RF model had a decent improvement by changing the max_depth, doing a quick random search for the parameters categorical_encoding,histogram_type,mtries, and nbins. Also by increasing the number of trees. Let's see if the F1 Score improved :","cb81c234":"- shutdown cluster once done with:\n\nh2o.cluster().shutdown()","cffaadb2":"#### By looking at the plot above, we can see that if we were to use less than 500 trees, we would get a similar score. Even if you use more than 500 trees, the training AUC might keep increasing, but the validation AUC will remain the same. For that reason, one way to find a good number of trees is just to build a model with a large number of trees, and from the scoring plot, identify a good cut-off or just use more aggressive early stopping settings. Please keep in mind that you need to be doing cross-validation.\n\n#### From the scoring history plot, we can see that the Validation AUC starts plateauing around 200 trees, but keeps slightly increasing. H2O models are, by default, optimized to give a good performance; therefore, sometimes, there is not much tuning to be done. We will see that with the GBM model as well.","2ced09cd":"Print the AUC and F1 scores to see how the model performed:","f6347b56":"#### The AUC for our tuned model actually improved, as well as the F1 Score. However, the misclassification error slightly increased. The new model is predicting fewer FALSE labels that are actually FALSE; this means the model is classifying more FALSE labels incorrectly. On the bright side, the model is predicting more TRUE labels correctly, and thus, we have a smaller misclassification error for the TRUE label. It is good to see that the model now predicts more TRUE labels as TRUE because we saw that the default model, as well as the GLM, were also having a hard time making those predictions.\n\nNow, we will see if we can improve our GBM model.","bc1d72b7":"H2O supports two types of grid search \u2013 traditional (or \"cartesian\") grid search and random grid search. In a cartesian grid search, you specify a set of values for each hyperparameter that you want to search over, and H2O will train a model for every combination of the hyperparameter values. This means that if you have three hyperparameters and you specify 5, 10, and 2 values for each, your grid will contain a total of 5*10*2 = 100 models.\n\nIn a random grid search, you specify the hyperparameter space in the exact same way, except H2O will sample uniformly from the set of all possible hyperparameter value combinations. In the random grid search, you also specify a stopping criterion, which controls when the random grid search is completed. You can tell the random grid search to stop by specifying a maximum number of models or the maximum number of seconds allowed for the search. You can also specify a performance-metric-based stopping criterion, which will stop the random grid search when the performance stops improving by a specified amount.\nOnce the grid search is complete, you can query the grid object and sort the models by a particular performance metric (for example, \"AUC\"). All models are stored in the H2O cluster and are accessible by model id.\n\nTo save some time, we will do a random grid search for our GLM model instead of the cartesian search. The H2OGridSearch has 4 parameters, and in order to use it, you need at least three of them. The first parameter for the grid search is the model that you want to tune. Next are your hyperparameters, which needs to be a string of parameters, and a list of values to be explored by grid search. The third one is optional, which is the grid id, and if you do not specify one, an id will automatically be generated. Lastly, the fourth parameter is the search criteria, where you can specify if you want to do a cartesian or random search.\n\nWe will explore two ways of defining your grid search, and you can use the way you prefer. One way is to define all at once in the grid search (as we will do it for the GLM). The second way is to define every parameter separately. For example, define your model, your hyper-parameters, and your search criteria, and just add that to your grid search once you are ready.\n\nFor our GLM, we will tune alpha,lambda, and missing_values_handling. The other parameters that you could change, such as solver,max_active_predictors, and nlambdas, to mention a few, are not supported by H2OGridSearch.\n\n1. alpha is the distribution of regularization between the L1 (Lasso) and L2 (Ridge) penalties. A value of 1 for alpha represents Lasso regression, a value of 0 produces Ridge regression, and anything in between specifies the amount of mixing between the two.\n\n2. lambda, on the other hand, is the regularization strength. For alpha, we can explore the range from 0 to 1 in steps of 0.01. For lambda, you could start just doing your own random searches, but that might take a lot of time. Instead, we can base our value for lambda on the original value of lambda, which was 6.626e-5. We can choose our starting point to be 1e-6 and go from there.\n\n3. missing_values_handling This parameter allows us to specify how we want to specify any missing data (Options are skip and MeanImputation)","f8bad997":"The model used by H2O for this classification problem is a Logistic Regression model, and the predictions are based on the threshold for each probability[1]. For a binary classifier, H2O predicts the labels based on the maximum F1 threshold. From the report, the threshold for max F1 is 0.1224. So, any time the probability for TRUE is greater than the 0.1224, the predicted label will be TRUE, as is in the case of the sixth prediction. To learn more about predictions, you can visit the Prediction Section from the H2O documentation.\n\nLastly, save the default performance of the model, as we will use this for comparison purposes later on.","4e63c19b":"With the code sample above, you will get the models that were created with their respective parameters, model id, and AUC. As you can see, the grid search did not take that long to complete, and it trained a total of 200 models, which was our second constraint. The AUC did improve, and we will compare it to the AUC from our default model.","3a03aa8b":"We are going to obtain the test performance of each of the best models. Notice that we are just taking the best models and checking the model performance with the test set.","9f0a3b46":"- F1 Score:","a9bc10e2":"- The F1 score also improved. Although the F1 score is still low, we will look at the confusion matrix, and let's see how this improvement reflects on the confusion matrix","483b40a2":"All three models made the same ten predictions, and this gives us an indication of why all three scores are close to each other. Although the sixth prediction is TRUE for all three models, the probability is not exactly the same, but since the thresholds for all three models were low, the predictions were still TRUE.\nAs we did with the other two models, save the model performance.","0d4c6dd5":"In this case, we see that the RF model is far from overfitting because the training error is still lower than the validation error, and that means that we can probably do some tuning to improve our model.\n\nWe can also generate the variable importance plot:\n\n","566ba84a":"### Print F1 score to see if it improved or not:","13ee53d8":"- The max F1 Score did not have a significant improvement. Although the threshold slightly increased, it did not improve the overall F1 Score by much. Let's take a look at the confusion matrix to see if the values changed.","46975024":"- We will do the grid search a bit differently this time. We are going to define each parameter of the grid search separately, and then pass the variables to the grid search function.\n\n- We will first find one of the most important parameters for an RF, which is the maximum depth.\n\n- max_depth defines the number of nodes along the longest path from the start of the tree to the farthest leaf node. Higher values will make the model more complex and can lead to overfitting. Setting this value to 0 specifies no limit. This value defaults to 20. We will first look for the best value for the max_depth; this would save us some computational time when we tune the other parameters. As we mentioned before, we will use a slightly different approach for the grid search. We are going to instantiate each parameter for the grid search, and then pass each one into it.","8c0bfebd":"We will build a default Distributed Random Forest (DRF) model and see how it performs on our validation set. DRF generates a forest of classification or regression trees, rather than a single classification or regression tree. Each of these trees is a weak learner built on a subset of rows and columns. More trees will reduce the variance. Both classification and regression take the average prediction over all of their trees to make a final prediction, whether predicting for a class or numeric value.\n\nTo build and train our Random Forest or RF(as we will be referring to from this point on) model, simply run the following two lines of code:","300a92c3":"We can see from the plot above that after four iterations, the score no longer improves; therefore, if we needed to set a number of iterations as a future parameter, we can choose 4, as the scores don't really improve after that point. We can also use the default number of iterations and use early stopping; that way, the model will stop training when it is no longer improving. We will use early stopping when we start tuning our models.\n\nWe can also generate a variable importance plot to see how each of our features contribute to the linear model.","4ed869fa":"We are doing a random search of values for max_depth to see if the default value is good, or if we need to adjust the value. After it is done training, print the models sorted by AUC","b44a5b1e":"Again, all three scores are very close to each other, but the best one is the GBM, second the RF, and lastly, our GLM. For the misclassification error, we see the opposite pattern to the F1 Score, the test misclassification error for both RF and GBM increased, and it slightly decreased for the GLM. However, it is important to note that for both RF and GBM, the error for the TRUE predicted label decreased, and for the GLM increased. The high misclassification error for the TRUE class, along with a relatively low F1 Score, is due to the highly imbalanced dataset.\n\nFor this dataset, we obtained a good AUC for all three models. We obtained an okay F1 Score, given that our dataset is highly imbalanced, and we also obtained a good overall misclassification error, although due to the given imbalanced data, the error for the TRUE label was not so low. Overall, The best model trained on our dataset was the GBM, followed by the RF, and lastly, the GLM.","b4d3e475":"- The first parameter shown in the list above is the threshold, and the second value is the accuracy.","b8415143":"Note that we defined the random seed and the model id. You do not need to do this; the model can be built without defining these parameters. The reason for choosing the random seed is for reproducibility purposes, and the model id is to recognize the model in Flow easily.\n\nAgain, print the summary of your model as we did with the GLM model. You will see the summary of the model with the default settings, and the metrics score on the training and validation data.\n\nBelow you will see some of the details from the model we just built.\n\nThe AUC and F1 Score reported on the training data are 0.8033 and 0.2620, respectively, and you can see them in the image below.","47b5be7f":"## Build Generalized Linear Model (GLM)","840dbff0":"The F1 Score for the RF and GBM slightly increased compared to the default value; however, the GLM F1 Score slightly decreased compared to both the default and the validation results. Even though the AUC for the GLM improved, the F1 did not, and we will see shortly how that is reflected in the misclassification error. On the other hand, by tuning some parameters, we were able to get better AUC and better F1 scores for both the RF and the GBM models.\n\nLastly, we will take a look at the confusion matrix for each model:","d382eb00":"Now we will train our GLM model. To do so, we just use the .train() function. In the train function, we need to specify the predictors (x), the response (y), the training set (train), and a validation frame, if you have one. In our case, we have our valid set, which we will use.","36c7b4af":"Both models, GLM and RF, made the same predictions in the first ten predictions. For e.g., the TRUE prediction for the sixth row is the same; there is a different probability, but the prediction is the same.\n\nAgain, save the model performance on the validation data","cd331145":"Notice how the overall error slightly decreased, as well as the error for the FALSE class. While the error for the TRUE class had minimal improvement, meaning the model is classifying a few more samples that are actually TRUE correctly. We see that our model has a hard time classifying the TRUE labels, and this is due to the highly imbalanced dataset that we are working on.\n\nWe will do the test evaluation after we tune our other two models.","50f3f546":"# Test Set Performance","6317ef35":"# Model tuning\n### Tune GLM with H2O GridSearch","43e6296b":"The random grid search slightly improved the results. Note that since the combination of possible models is so large, we will need to run the grid search for much longer to see if there are models that further improve the AUC. For now, we can leave it at that, or you can try it on your own to see if you get better results! The random search yielded an AUC of 0.8576.\n\nWe are going to build another GBM model, but we will update the parameters we found, and we are going to increase ntrees to 200 and see if we can further improve our model.","49729882":"We were able to improve the AUC of all three models with the quick grid search that we did for all three models. We saw the greatest improvement with the RF model, as the default parameters were a little off from what we found to be good. All three AUC test scores are slightly higher than the validation scores but close enough to trust the validation score to tune all our models. And as it could be expected, the GBM had the best AUC, followed by the RF and, lastly, the GLM.\n\nNow print the F1 Score for each model :","a8771ca7":"- Print validation AUC","694c38be":"### Save the best model and print the model summary","e2de19f4":"## Build Gradient Boosting Machine","83c4405f":"From the report, we can also see the max F1 score as well as all the metrics for our model with their respective thresholds. For the default GLM, we obtained a training F1 score of 0.2881 and a validation F1 score of 0.2827.","e4177a5e":"Based on the grid search that we just did, the best max_depth is 6.\nWe will do a random grid search with a few parameters to see if we can get a better model. We will use 50 trees, and after we find some values from the random grid search, we will increase the number of trees.\n\nNote: You don't have to run the following line of code unless you want to see it for yourself. The search criteria will only allow the grid search to run for 15 minutes, if you would like to see the results of running it for longer, just increase the max_runtime_secs to a higher value and wait for the results.\n\nHere is the list of parameters that we are going to try to tune\n\n1. sample_rate: Specify the row sampling rate (x-axis). (Note that this method is sample without replacement.) The range is 0.0 to 1.0, and this value defaults to 1. Higher values may improve training accuracy. Test accuracy improves when either columns or rows are sampled.\n\n2. col_sample_rate: Specify the column sampling rate (y-axis). (Note that this method is sampling without replacement.) The range is 0.0 to 1.0.\n\n3. col_sample_rate_per_tree: Specify the column sample rate per tree. This can be a value from 0.0 to 1.0 and defaults to 1. Note that it is multiplicative with col_sample_rate, so setting both parameters to 0.8, for example, results in 64% of columns being considered at any given node to split.\n\n4. col_sample_rate_change_per_level: This option specifies to change the column sampling rate as a function of the depth in the tree.\n\n5. learn_rate: Specify the learning rate. The range is 0.0 to 1.0.\n\nnbins: Specify the number of bins for the histogram to build, then split at the best point.\n\n6. nbins_cats: Specify the maximum number of bins for the histogram to build, then split at the best point. Higher values can lead to more overfitting.\n\n7. min_split_improvement: The value of this option specifies the minimum relative improvement in squared error reduction in order for a split to happen.\n\n8. histogram_type: Random split points or quantile-based split points can be selected as well. RoundRobin can be specified to cycle through all histogram types (one per tree). Use this option to specify the type of histogram to use for finding optimal split points.\n\nFind more parameters and more information about them at the Documentation - GBM Section and also the Python Module\n\n","9fd872e6":"#### Now that we have the proper depth for our RF, we will do a random grid search to try to find the next four parameters, categorical_encoding,histogram_type,mtries, and nbins.","45962e93":"From the summary results, we can see the GLM performance. We will focus on the Area Under the Curve (AUC), and since we have a very imbalanced dataset, we will be looking at the F1 score. Additionally, we will also take a quick look at the misclassification error and logloss.\n\nFrom the report, we can look at the metrics on the training and validation data, and we see that the training AUC was 0.8502 while the validation AUC was 0.8450\n\n","8e43e0fd":"### Tune the RF model with H2O GridSearch","5684664d":"We can plot the Scoring history for any of our models, as shown below:","912f6ecc":"### save the model:","2f7aa243":"You can easily see all four parameters of our grid search in the code sample above. We defined our GLM model the same way we did before. Then, we take care of the hyper-parameters and notice that we have used a for loop for the ranges of both alpha and lambda in order to cover more possible values. Because the number of possible models in our search criteria is very, we specify that we want a maximum number of 200 models, or that the grid search runs for only 300 seconds.\n\nPrint the models in descending order, sorted by the AUC. By default, the grid search will return the best models based on the logloss. Therefore, in order to get the best model based on the AUC, we will specify that we want to sort the models by AUC. You can change this to other metrics, depending on what you are looking for.","19fe1535":"- You will see the output in a list format. First, you will see the threshold, and then the actual value, the same as in the accuracy. You could also specify the threshold inside the parenthesis, that way you use the threshold that you want.\n\nLet's take a look at the first ten predictions in our validation set, and compare it to our first model.","8e864d75":"## Make a comparison between the performance of default glm model and the best model from grid search.\n- First, evaluate the model performane on the validation set:","c279abf7":"The default GBM model had a slightly better performance than the default RF.\nWe will make the predictions with the GBM model as well.","d378e772":"You can now print any performance metric that you would like. Right now, we will just focus on the AUC, F1 Score, and the misclassification error from the confusion matrix.","ba911a78":"## Build a Random Forest","251f5552":"Next, we need to choose our predictors, or x variable, and our response or y variable. For the H2O-3 estimators, we do not use the actual data frame; instead, we use strings containing the name of the columns in our dataset.\n\nReturn to your Jupyter Notebook. For our y variable, we will choose DELINQUENT because we want to predict whether or not a loan will default. For the x variable, we will choose all but four features. One is the feature that we will predict, and then PREPAID and PREPAYMENT_PENALTY_MORTGAGE_FLAG because they are clear indicators if a loan is or is not delinquent and we will not have the information at the time deciding whether to give a loan or not. In machine learning terms, introducing these types of features is called leakage. And lastly, PRODUCT_TYPE because that's a constant value for every row, meaning all samples have the same value; therefore, this feature will not have any predictive value.\n\nThere are several ways to choose your predictors, but for this tutorial, we will subtract the list in the variable ignore from all the names in our training set.","a22a2b1a":"We were able to get the highest validation AUC among the three models with our GBM. The model reached a 0.8606 AUC, while also improving the F1 to 0.3163.\n\nLet's take a look at the confusion matrix and see how are the misclassification errors from this model:","fdd31d2b":"Since we have a large enough dataset, we will split our dataset into three sets, and we will call them train, valid, and test. We will treat the test set as if it were some unseen data in which we want to make predictions, and we will use the valid set for validation purposes and to tune all our models. We will not use the test set until the end of the tutorial to check the final scores of our models.\n\nReturn to your Jupyter Notebook to split our dataset into three sets. We will use the .split_frame() function. Note that we can do this in one line of code. Inside the split function, we declare the ratio of the data that we want in our first set, in this case, the train set. We will assign 70% to the training set, and 15% for the validation, as well as for the test set. The random seed is set to 42 just for reproducibility purposes. You can choose any random seed that you want, but if you want to see consistent results, you will have to use the same random seed anytime you re-run your code.","f2a461e2":"- The AUC from the validation data was 0.8516, and the F1 Score 0.2993.\n\n- Compare the tuned model with the dafault one:","54fa7157":"- The AUC improved using max_depth=10 and doing a random grid search for the parameters mentioned above.\n\n- Another important parameter that we can tune is the number of trees (ntrees).\n\n- ntrees specifies the number of trees that you want your RF to have. When tuning the number of trees, you need to be careful because when you have too many trees, your model will tend to overfit. That's why it's always advised to use cross-validation, and never tune models based on training scores. Again, you can also use early stopping; that way, your model stops training once the validation score is no longer improving.\n\n- We won't do a grid search for a maximum number of trees. We are going to update the parameters of our model with the values we found in the previous two grid searches that we did. We will use 500 trees while using early stopping; that way, if the model doesn't improve, it will automatically stop.","13ebd1bf":"- We will take a similar approach to the tuning of the RF model. We could do the grid search for a list of a number of trees, but since the scoring history will show us the validation score based on the number of trees, we will obtain that number from the plot. We will be using 50 trees, which is the default. For a GBM model, conceptually speaking, the max_depth and ntrees is the same as the RF model. However, we will see that the values are smaller than the ones used for the RF.","591c1053":"Even though the misclassification error for the TRUE class improved, the error for the FALSE class and the overall error did not improve by much. However, with the tuning that we did, our GBM model was able to make more correct predictions for the TRUE class, which is good since we are dealing with a highly imbalanced dataset.\n\nHere is how you can compare the AUC from the default model with the tuned model:","49672ab2":"It is interesting to see that for our RF model, PROPERTY_STATE Is the most important variable, implying that the prediction of whether a loan could be delinquent or not depends on the state where someone is trying to buy that property. The second most important is a more intuitive one, which is the CREDIT_SCORE, as one could expect someone with really good credit to pay their loans fully.\n\nIf you want to check the options of what you can print from your model, just type the name of your model along with a dot (.) and press tab. You should see a drop-down menu like the one shown in the image below."}}