{"cell_type":{"f849762c":"code","2508becd":"code","6540496a":"code","bef0dc56":"code","cebfb648":"code","0b8a3b7a":"code","a6aedfae":"code","58a4c420":"code","dc53f0b9":"code","a74dc492":"code","4ccdf780":"code","03d2c88e":"code","8da7d612":"code","3dca460c":"code","a647cac7":"code","750eec38":"code","ac3ffe4a":"code","50ff3608":"code","a0eb4070":"code","85956bcc":"code","61cbbc85":"code","17b29088":"code","b7538970":"code","91688b13":"code","f11cc5b4":"markdown","4cc96f04":"markdown","b18f438a":"markdown","93224591":"markdown","fa18f4a6":"markdown","6c355d7b":"markdown","62e57291":"markdown","4cb3756f":"markdown","1f58dc5c":"markdown","b60674a6":"markdown","5e650610":"markdown","0b2f2a89":"markdown","37ad3d7f":"markdown","315723de":"markdown","b2803003":"markdown","67e167c9":"markdown","2ac39466":"markdown","669bb476":"markdown","4e265de5":"markdown"},"source":{"f849762c":"from IPython.utils import io\nwith io.capture_output() as captured:\n    !pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz\nimport spacy\nimport string\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nfrom pprint import pprint\nfrom IPython.utils import io\nfrom tqdm.notebook import tqdm\nfrom gensim.models import Word2Vec\nfrom IPython.core.display import HTML, display\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nwarnings.filterwarnings('ignore')","2508becd":"root_path = '\/kaggle\/input\/CORD-19-research-challenge'\nmetadata_path = f'{root_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str,\n    'abstract': str\n})\nmeta_df.head()","6540496a":"df_cord = pd.DataFrame(columns=['paper_id', 'title','abstract', 'doi'])\ndf_cord['paper_id'] = meta_df.sha\ndf_cord['title'] = meta_df.title\ndf_cord['abstract'] = meta_df.abstract\ndf_cord['doi'] = meta_df.doi\n\ndf_cord.head()","bef0dc56":"df_cord.info()","cebfb648":"df_cord.drop_duplicates(['abstract'], inplace=True)\ndf_cord.dropna(inplace=True)\ndf_cord.info()","0b8a3b7a":"import en_core_sci_lg\nnlp = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])# disabling Named Entity Recognition for speed\nnlp.max_length = 3000000\ndef spacy_tokenizer(sentence):\n    return ' '.join([word.lemma_ for word in nlp(sentence) if not (word.like_num or word.is_stop or word.is_punct or word.is_space or len(word)==1)])\n","a6aedfae":"customized_stop_words = [\n    'rights', 'reserved', 'permission', 'use', 'used', 'using', 'biorxiv', 'medrxiv', 'license',\n    'doi', 'preprint', 'copyright', 'org', 'https', 'et', 'al', 'author', 'figure', 'table',\n     'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI',\n    '-PRON-', 'usually'\n]\npunctuations = string.punctuation\nstopwords = list(STOP_WORDS)\n# Mark them as stop words\nstopwords.extend(customized_stop_words)","58a4c420":"tqdm.pandas()\ndf_cord[\"tokenized_abstract\"] = df_cord[\"abstract\"].progress_apply(spacy_tokenizer)\ndf_cord.head()\n","dc53f0b9":"abstracts = df_cord['abstract'].values\n\nnlp.add_pipe(nlp.create_pipe('sentencizer'), before=\"parser\")\nword2vec_corpus = []\n\nfor i in tqdm(range(0, len(abstracts))):\n    doc = nlp(abstracts[i])\n    word2vec_corpus.extend([spacy_tokenizer(sentence.string.strip()).split(\" \") for sentence in doc.sents])","a74dc492":"word2vec_corpus[:5]","4ccdf780":"# Train the gensim word2vec model with our corpus\nimport multiprocessing\ncores = multiprocessing.cpu_count() # Count the number of cores in a computer\nsize = 50\nmodel = Word2Vec(word2vec_corpus, min_count=5,size= size,workers=cores-1, window =5, sg = 1)","03d2c88e":"df_cord[\"centroid\"] = [[0.0]*size]*df_cord.shape[0]\nfor index, row in df_cord.iterrows():\n    abstract = row['tokenized_abstract']\n    centroid = np.array([0.0]*size)\n    for word in abstract.split(\" \"):\n        try:\n            word_vector = model[word]\n        except:\n            continue\n        centroid = np.add(centroid, word_vector)\n\n    df_cord.at[index,'centroid'] = centroid.tolist()\n\ndf_cord.head()","8da7d612":"def get_top_k_docs(model, query, df_cord, k) :\n    cosine_distance = []\n    \n    vectorized_query = []\n    for word in spacy_tokenizer(query).split(\" \"):\n        try:\n            vectorized_query.append(model[word])\n        except:\n            continue\n    \n    for _, row in df_cord.iterrows():\n        centroid = row['centroid']\n        total_simalirity = 0\n        for word_vec in vectorized_query:\n            word_simalirity = np.dot(word_vec, centroid)\/(np.linalg.norm(word_vec)*np.linalg.norm(centroid))\n            total_simalirity += word_simalirity\n        cosine_distance.append((row['title'], row['doi'],row['abstract'], total_simalirity)) \n    \n    \n    cosine_distance.sort(key=lambda x:x[3], reverse=True) #Sort according to cosine simalirity in descending order\n    return cosine_distance[:k]","3dca460c":"get_top_k_docs(model=model,query='origin of coronavirus',df_cord=df_cord,k=10)","a647cac7":"def search(search_query,n_docs=5):\n    html = \"\"\"\n        <html>\n            <body>\n                <ol>\n            \"\"\"\n    results = get_top_k_docs(model=model,query=search_query,df_cord=df_cord,k=n_docs)\n    for result in results:\n        paper_name = result[0]\n        paper_doi = result[1]\n        paper_abstract = result[2]\n        paper_link = \"https:\/\/doi.org\/\" + str(paper_doi)\n        html += f\"\"\"            \n                <li id=\"result-1\">\n                    <article>\n                        <header>\n                            <a href=\"{paper_link}\">\n                                <h2>{paper_name}<\/h2>\n                            <\/a>\n                        <\/header>\n                        <p>{paper_abstract}<\/p>\n                    <\/article>\n                  <\/li>\n                \"\"\"\n    html += \"<\/body><\/html>\"\n    display(HTML(html))","750eec38":"search('transmission and incubation of coronavirus')","ac3ffe4a":"search('coronavirus risk factors')","50ff3608":"search('genetics origin and evolution of coronavirus')","a0eb4070":"search('coronavirus vaccines and therapeutics')","85956bcc":"search('non-pharamceutical interventions of coronavirus')","61cbbc85":"search('ethical and social science considerations of coronavirus')","17b29088":"search('coronavirus medical care')","b7538970":"search('coronavirus diagnostics and surveillance')","91688b13":"search('coronavirus information sharing and colaboration')","f11cc5b4":"# What do we know about diagnostics and surveillance?\n","4cc96f04":"\n# What do we know about vaccines and therapeutics?\n","b18f438a":"# What do we know about non-pharmaceutical interventions?\n","93224591":"# What has been published about ethical and social science considerations?\n","fa18f4a6":"Dropping null and duplicate values.","6c355d7b":"## Text Tokenizing\nFor preprocessing we use scispaCy, which is a Python package containing spaCy models for processing biomedical, scientific or clinical text.","62e57291":"## Corpus Sentencization\nWe apply spacy's sentencizer to split all the abstracts into separate sentences so we can use them as our word2vec corpus.","4cb3756f":"## Applying the tokenizer on the abstarcts and creating a new column `tokenized_abstract`","1f58dc5c":"# Data loading and preprocessing\nWe consider the paper abstract only, but the approach could also be applied to the whole text body.\n\n","b60674a6":"# Semantic Matching:\n# Word Embedding for Prioritizing Research Papers\n\n![CORD19](https:\/\/pages.semanticscholar.org\/hs-fs\/hubfs\/covid-image.png?width=300&name=covid-image.png)\n\n## Introduction\nCOVID-19 Open Research Dataset (CORD-19) is a free dataset of academic papers, aggregated by a coalition of leading research groups, about COVID-19 and the coronavirus family of viruses. The dataset can be found on Semantic Scholar and there is a research challenge on Kaggle. This dataset is being given to the global research community to apply recent developments in natural language processing and other AI techniques to generate new insights in support of the ongoing battle against this infectious disease. These approaches are becoming increasingly urgent due to the rapid acceleration in the new literature on coronavirus, which makes it difficult for medical research community to keep up. In this notebook we try to prioterize research papers based on semantic search instead of classical search. methods\n\n## Approach\nIn this notebook, we use gensim's word2vec in order to generate word embeddings for the research papers' abstracts to use as our corpus. \n\n#### Semantic Matching\nWord embeddings represent words as d-dimensional dense vectors. The similarity or the distance between the vectors of words in the embedding space measure the relatedness between them. \n\n#### IWCS \nThe IDF re-weighted word centroid similarity (IWCS) model used word embeddings to construct a d-dimensional vector representing a passage (abstract in our case). In this model, the word vectors of the given text are aggregated into a single vector using a linear weighted combination of its word vectors.\nThe centroid vector of the query can also be computed in the same manner. Finally, to rank a text according to a query we use the cosine distnace between them.\n\nInspired from: [The Semantic Web: 16th International Conference](https:\/\/books.google.com.eg\/books?id=PxaaDwAAQBAJ&dq=The+Semantic+Web:+16th+International+Conference,+ESWC&source=gbs_navlinks_s)\n\n","5e650610":"# What has been published about medical care?\n","0b2f2a89":"# Text Centroid Calculation\nwe will calculate the centroid for each abstract using the vectors of all the words in the abstract.","37ad3d7f":"# Training the Model (word2vec)\nWe will use gensim's word2vec and train it on the processed abstract.\n\n## Important parameters:\n* `min_count` Ignores all words with total absolute frequency lower than this - (2, 100)\n* `window` The maximum distance between the current and predicted word within a sentence. E.g. window words on the left and window words on the left of our target - (2, 10)\n* `size` Dimensionality of the feature vectors. - (50, 300)\n* `workers` = Use these many worker threads to train the model (=faster training with multicore machines)\n* `sg`enable skipgram model","315723de":"# Installing\/Loading packagaes","b2803003":"# What is known about transmission, incubation, and environmental stability?","67e167c9":"# What do we know about COVID-19 risk factors?\n","2ac39466":"# What has been published about information sharing and inter-sectoral collaboration?","669bb476":"# What do we know about virus genetics, origin, and evolution?\n","4e265de5":"# Ranking research papers\nGiven a query, we compute its centroid and then determine the top `k` semantically similar papers to the query according to the cosine similarity score."}}