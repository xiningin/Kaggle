{"cell_type":{"6ebd2460":"code","1a2767fb":"code","40e1a302":"code","b3578716":"code","5175c96d":"code","d3d1e075":"code","d28aca03":"code","3e599569":"code","27d21a15":"code","9009418d":"code","5678391d":"code","e3248fe5":"code","f20cd7ee":"code","b6dc227d":"code","64f5f6ab":"code","1113250c":"code","3fdd1e85":"code","7178140f":"code","31772153":"code","6452c822":"code","04f95749":"code","ec4a2931":"code","f6cf6993":"code","72bd3d23":"code","cdeb93d6":"code","6b5f93e6":"code","128af53e":"code","2ca7d6a3":"code","cfb56bd7":"code","61d018f6":"code","548c4ddc":"code","979cdbb2":"code","e687dd8a":"code","548b7d6e":"code","5644d85f":"code","ce933b0f":"code","a0b5dfac":"markdown","b557d7ea":"markdown","3505e46d":"markdown","246868ee":"markdown","133a20b6":"markdown","868f9238":"markdown","6571654f":"markdown","e7ca68bd":"markdown","833ee8ee":"markdown","f2f82beb":"markdown","273e645f":"markdown","c8943556":"markdown","3c4602cd":"markdown","a8af896c":"markdown","bef64250":"markdown","008991bd":"markdown","a88d981b":"markdown","7d35dd38":"markdown","e653b05d":"markdown","46d0b69d":"markdown","999b9661":"markdown","0edeb736":"markdown","de911d1d":"markdown","00954f38":"markdown","9a3e43a3":"markdown","11de7477":"markdown","b4a76b76":"markdown","18739b04":"markdown","30a287c1":"markdown","b0765aa8":"markdown","fd1b613f":"markdown","c8fd3c39":"markdown","dafe176a":"markdown","ffaaaa3d":"markdown","15bff849":"markdown","0095fc19":"markdown"},"source":{"6ebd2460":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\nimport os\n!pip install imutils\n!pip install opencv-contrib-python\nimport cv2\nimport imutils\nfrom skimage.util import random_noise","1a2767fb":"lena = r\"\/kaggle\/input\/opencv-samples-images\/data\/lena.jpg\"\nchessboard = r\"\/kaggle\/input\/opencv-samples-images\/data\/chessboard.png\"\nminnions = r\"\/kaggle\/input\/opencv-samples-images\/minions.jpg\"\nbutterfly = r\"\/kaggle\/input\/opencv-samples-images\/data\/butterfly.jpg\"\nhome = r\"\/kaggle\/input\/opencv-samples-images\/data\/home.jpg\"\nbasketball = r\"\/kaggle\/input\/opencv-samples-images\/data\/basketball2.png\"\ncoffee = r\"\/kaggle\/input\/operations-with-opencv\/1coffee.jpg\"\nbuildings = r\"\/kaggle\/input\/opencv-samples-images\/data\/building.jpg\"\nmotion= r\"\/kaggle\/input\/opencv-samples-images\/data\/text_motion.jpg\"\ncoridor = r\"\/kaggle\/input\/computer-vision-course\/imagenes\/corridor.jpg\"\n\ndic = [ lena,coridor, minnions, butterfly, home, basketball ]","40e1a302":"image_ = cv2.imread(butterfly)\nimage_ = cv2.cvtColor(image_, cv2.COLOR_BGR2RGB)\nprint(\"The type of read image is: \", type(image_))\nplt.imshow(image_)","b3578716":"print(\"Image shape is : \",image_.shape)\nprint(\"Total pixel number is :\", image_.shape[0]*image_.shape[1]*image_.shape[2])","5175c96d":"lena_rgb = cv2.imread(lena)\nlena_rgb = cv2.cvtColor(lena_rgb, cv2.COLOR_BGR2RGB) \nprint(\"orginal lena shape :\", lena_rgb.shape)\n\nlena_gray = cv2.imread(lena)\nlena_gray = cv2.cvtColor(lena_gray, cv2.COLOR_BGR2GRAY) \nprint(\"gray scale lena shape :\", lena_gray.shape)\n\ntitles= [ \"lena original\", \" lena Gray\"]\nimages = [lena_rgb,lena_gray]\n\nfor i in range(2):\n    plt.subplot(1,2,i+1,title=\"line-up pictures\")\n    plt.imshow(images[i], \"gray\")   \n    plt.title(titles[i])\n    plt.xticks([])\n    plt.yticks([])\nplt.show()   ","d3d1e075":"titles = [\"lena\", \"corridor\", \"minnions\",\"butterfly\",\"home\",\"basketball\"]\n\nprint(\"For RGB color images by matplotlib\")\nfor i in range(6):    \n    image = cv2.imread(dic[i])\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)     \n    plt.subplot(2,3,i+1,title=\"line-up pictures\")\n    plt.imshow(image, \"gray\")   \n    plt.title(titles[i])\n    plt.xticks([])\n    plt.yticks([])\nplt.show() \n\n\nprint(\"For GRAY color images by matplotlib\")\nfor i in range(6):    \n    image = cv2.imread(dic[i])\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)     \n    plt.subplot(2,3,i+1,title=\"line-up pictures\")\n    plt.imshow(image, \"gray\")   \n    plt.title(titles[i])\n    plt.xticks([])\n    plt.yticks([])\nplt.show() ","d28aca03":"lena_rgb = cv2.imread(lena, cv2.IMREAD_UNCHANGED)\nlena_rgb = cv2.cvtColor(lena_rgb, cv2.COLOR_BGR2RGB)  \nprint('Original Dimensions : ',lena_rgb.shape)\n \nscale_percent = 0.50 #50% Rescale\nwidth = int(lena_rgb.shape[1] * scale_percent )\nheight = int(lena_rgb.shape[0] * scale_percent )\ndim = (width, height)\n  \n# resize image\nresized = cv2.resize(lena_rgb, dim, interpolation = cv2.INTER_AREA) \nprint('Resized Dimensions : ',resized.shape)\nplt.imshow(resized)","3e599569":"lena_rgb = cv2.imread(lena, cv2.IMREAD_UNCHANGED)\nlena_rgb = cv2.cvtColor(lena_rgb, cv2.COLOR_BGR2RGB)  \nprint('Original Dimensions : ',lena_rgb.shape)\n\nwidth = 620\nheight =290\ndim = (width, height)  \nresized = cv2.resize(lena_rgb, dim, interpolation = cv2.INTER_AREA) \nprint('Resized Dimensions : ',resized.shape)\nplt.imshow(resized)","27d21a15":"lena_flip_hor = cv2.flip(lena_rgb, 0)\nlena_flip_ver = cv2.flip(lena_rgb, 1)\ntitles = [\"original\",\"horizontal flip\",\"vertical flip\" ]\nimdic = [lena_rgb, lena_flip_hor,lena_flip_ver]\nprint(\"original shape is: \", lena_rgb.shape)\nprint(\"horizontal flip shape is: \", lena_flip_hor.shape)\nprint(\"vertical flip shape is: \", lena_flip_ver.shape)\n\n\nfor i in range(3):    \n    plt.subplot(1,3,i+1)\n    plt.imshow(imdic[i], \"gray\")   \n    plt.title(titles[i])\n    plt.xticks([])\n    plt.yticks([])\nplt.show() ","9009418d":"lena_90cw = cv2.rotate(lena_rgb, cv2.cv2.ROTATE_90_CLOCKWISE)\nlena_180cw = cv2.rotate(lena_rgb, cv2.ROTATE_180)\nlena_90ccw = cv2.rotate(lena_rgb, cv2.ROTATE_90_COUNTERCLOCKWISE)\ntitles=[\"original\", \"90 CW\", \"180 Rotate\", \"90 CCW\"]\nimdic = [ lena_rgb, lena_90cw , lena_180cw  ,lena_90ccw]\nfor i in range(4):    \n    plt.subplot(2,2,i+1)\n    plt.imshow(imdic[i], \"gray\")   \n    plt.title(titles[i])\n    plt.xticks([])\n    plt.yticks([])\nplt.show() \n","5678391d":"angle = 27\nrotated = imutils.rotate_bound(lena_rgb, angle)\nplt.imshow(rotated)\nprint(\"Look it's done and amazing!\")","e3248fe5":"font = cv2.FONT_HERSHEY_SIMPLEX\nfontScale = 1 \ncolor = (0, 38, 255)\nthickness = 3\n\norg = (40, 65)\ntext = \"Who does not love Lena ?\"\nimage_text = cv2.putText(lena_rgb, text, org, font, fontScale,  color, thickness, cv2.LINE_AA, False)\nplt.imshow(image_text)","f20cd7ee":"start_point = (0, 0)\nend_point = (500, 500)\ncolor = (10, 105, 220)\nthickness = 5\nimage_ = cv2.line(image_, start_point, end_point, color, thickness)\nplt.imshow(image_)","b6dc227d":"start_point = (52, 52)\nend_point = (220, 220)\ncolor = (255, 20,230)\nthickness = 4\nimage_ = cv2.rectangle(image_, start_point, end_point, color, thickness)\nplt.imshow(image_)","64f5f6ab":"center_coordinates = (400, 170)\ncolor = (25, 200,39)\nradius = 30  \nthickness = -1 # -1 is fullfil, 1,2 is empty circle\nimage_ = cv2.circle(image_, center_coordinates, radius, color, thickness)\nplt.imshow(image_)","1113250c":"pts = np.array([[35, 70], [35, 160], [110, 200], [200, 160], [200, 70], [110, 20]],np.int32)  \npts = pts.reshape((-1, 1, 2))  \nisClosed = True\ncolor = (255, 0, 10)\nthickness = 9\nimage_ = cv2.polylines(image_, [pts], isClosed, color, thickness)\nplt.imshow(image_)","3fdd1e85":"basketball_rgb = cv2.imread(basketball, cv2.IMREAD_UNCHANGED)\nbasketball_rgb = cv2.cvtColor(basketball_rgb, cv2.COLOR_BGR2RGB)\nprint(\"original shape: \", basketball_rgb.shape)\nroi = basketball_rgb[60:150,120:220]\n\n\ntitles=[\"original\", \"ROI the Ball\"]\nimdic = [ basketball_rgb, roi]\nfigure(figsize=(12,8), dpi=70)\nfor i in range(2):    \n    plt.subplot(2,1,i+1)\n    plt.imshow(imdic[i], \"gray\")   \n    plt.title(titles[i])\n    plt.xticks([])\n    plt.yticks([])\nplt.show() \nprint(\"roi shape: \",roi.shape)","7178140f":"image = cv2.imread(home)\nhome_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.imshow(home_rgb)","31772153":"histogram = cv2.calcHist([home_rgb], [0], None, [256], [0, 256])\nplt.hist(home_rgb.ravel(), 256, [0, 256]); plt.show()","6452c822":"color = ('r', 'g', 'b')\nfor i, col in enumerate(color):\n    histogram_rgb = cv2.calcHist([home_rgb], [i], None, [256], [0, 256])\n    plt.plot(histogram_rgb, color = col)\n    plt.xlim([0,256])  ","04f95749":"image = cv2.imread(lena)\nimage_ = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.imshow(image_)\nprint(\"shape is: \",image_.shape)","ec4a2931":"# 3x3 Low Pass Filter In image processing these sorts of filters are often used for blurring\nlow_pass_kernel=np.array([[1\/9,1\/9,1\/9], [1\/9,1\/9,1\/9], [1\/9,1\/9,1\/9]])\n# 5x5 Low Pass Filter In image processing these sorts of filters are often used for blurring\nlow_pass_kernel2 = np.ones((5,5),np.float32)\/25\n\nlow_pass = cv2.filter2D(image_, -1, low_pass_kernel2)\nfigure(figsize=(12,8), dpi=70)\nplt.subplot(121),plt.imshow(image_),plt.title('Original')\nplt.xticks([]), plt.yticks([])\nplt.subplot(122),plt.imshow(low_pass),plt.title('Low Pass Filtered')\nplt.xticks([]), plt.yticks([])\nplt.show()","f6cf6993":"figure(figsize=(14,8), dpi=90)\nlena_noisy = random_noise(image_, mode='s&p',amount=0.2)\nlow_pass = cv2.filter2D(lena_noisy, -1, low_pass_kernel2)\n\ntitles = [\"Original\", \"Salt & Pepper Noise\", \"Averaging Blurred\"]\nimdic = [ image_, lena_noisy, low_pass]\n\nfor i in range(3):    \n    plt.subplot(1,3,i+1)\n    plt.imshow(imdic[i], \"gray\")   \n    plt.title(titles[i])\n    plt.xticks([])\n    plt.yticks([])\nplt.show() \n","72bd3d23":"blur = cv2.GaussianBlur(lena_noisy,(5,5),1)\n\ntitles = [\"Original\", \"Salt & Pepper Noise\", \"Gaussian Blurred\"]\nimdic = [ image_, lena_noisy, blur]\nfigure(figsize=(14,8), dpi=90)\nfor i in range(3):    \n    plt.subplot(1,3,i+1)\n    plt.imshow(imdic[i], \"gray\")   \n    plt.title(titles[i])\n    plt.xticks([])\n    plt.yticks([])\nplt.show() \n","cdeb93d6":"bilateral_blur = cv2.bilateralFilter(lena_noisy.astype(np.float32),10,75,75)\n\ntitles = [\"Original\", \"Salt & Pepper Noise\", \"Bilateral Blurred\"]\nimdic = [ image_, lena_noisy, bilateral_blur]\nfigure(figsize=(14,8), dpi=90)\nfor i in range(3):    \n    plt.subplot(1,3,i+1)\n    plt.imshow(imdic[i], \"gray\")   \n    plt.title(titles[i])\n    plt.xticks([])\n    plt.yticks([])\nplt.show() ","6b5f93e6":"median_blur = cv2.medianBlur(lena_noisy.astype(np.float32),3)\n\ntitles = [\"Original\", \"Salt & Pepper Noise\", \"Median Blurred\"]\nimdic = [ image_, lena_noisy, median_blur]\nfigure(figsize=(14,8), dpi=90)\nfor i in range(3):    \n    plt.subplot(1,3,i+1)\n    plt.imshow(imdic[i], \"gray\")   \n    plt.title(titles[i])\n    plt.xticks([])\n    plt.yticks([])\nplt.show() ","128af53e":"titles = [\"Original\", \"Salt & Pepper Noise\",  \"Averaging Blurred\", \"Gaussian Blurred\",\"Bilateral Blurred\", \"Median Blurred\"]\nimdic = [ image_, lena_noisy, low_pass,blur,bilateral_blur, median_blur]\nfigure(figsize=(15,10), dpi=90)\nfor i in range(6):    \n    plt.subplot(3,2,i+1)\n    plt.imshow(imdic[i], \"gray\")   \n    plt.title(titles[i])\n    plt.xticks([])\n    plt.yticks([])\nplt.show() ","2ca7d6a3":"# 3x3 High Pass Filter In image processing these sorts of filters are often called edge-detectors\nSharp_kernel  = np.array([[-1, -1, -1], [-1,  9, -1], [-1, -1, -1]])\nsharpen = cv2.filter2D(image_, -1, Sharp_kernel )\n\nfigure(figsize=(12,8), dpi=70)\nplt.subplot(121),plt.imshow(image_),plt.title('Original')\nplt.xticks([]), plt.yticks([])\nplt.subplot(122),plt.imshow(sharpen),plt.title('Sharpen')\nplt.xticks([]), plt.yticks([])\nplt.show()","cfb56bd7":"# 3x3 Laplacian\nlaplacian= np.array([[0, -1, 0], [-1,  5, -1], [0, -1, 0]])\n\nlaplacian= cv2.filter2D(image_, -1, laplacian)\n\nfigure(figsize=(12,8), dpi=70)\nplt.subplot(121),plt.imshow(image_),plt.title('Original')\nplt.xticks([]), plt.yticks([])\nplt.subplot(122),plt.imshow(laplacian),plt.title('laplacian')\nplt.xticks([]), plt.yticks([])\nplt.show()","61d018f6":"# 5x5 Mexican Hat filter\nmexican_hat = np.array([[0,0,-1,0,0],[0,-1,-2,-1,0],[-1,-2,17,-2,-1],[0,-1,-2,-1,0],[0,0,-1,0,0]])\n\nmexican_hat= cv2.filter2D(image_, -1, mexican_hat)\n\nfigure(figsize=(12,8), dpi=70)\nplt.subplot(121),plt.imshow(image_),plt.title('Original')\nplt.xticks([]), plt.yticks([])\nplt.subplot(122),plt.imshow(mexican_hat),plt.title('mexican_hat')\nplt.xticks([]), plt.yticks([])\nplt.show()","548c4ddc":"# 3x3 linear smoothing filter In image processing these sorts of filters are often used for blurring\nsharpen_filter = np.array([[-1, -2, -1], [-1, 11, -1], [-1, -2, -1]])\nlow_pass_kernel = np.ones((5,5),np.float32)\/25\nlena_blurred= cv2.filter2D(image_, -1, low_pass_kernel)\nlena_blurred= cv2.filter2D(lena_blurred, -1, low_pass_kernel)\nlena_blurred= cv2.filter2D(lena_blurred, -1, low_pass_kernel)\n\nsharpen1 = cv2.filter2D(lena_blurred, -1, Sharp_kernel )\nsharpen2 =  cv2.filter2D(lena_blurred, -1, sharpen_filter  )\n\n\ntitles = [\"Original\", \"Blurred\",  \"Sharpen 1\",\"Sharpen 2\"]\nimdic = [ image_, lena_blurred, sharpen1 , sharpen2]\nfigure(figsize=(16,8), dpi=120)\nfor i in range(4):   \n    \n    plt.subplot(2,2,i+1)\n    plt.imshow(imdic[i], \"gray\")   \n    plt.title(titles[i])\n    plt.xticks([])\n    plt.yticks([])\nplt.show() \n","979cdbb2":"# 3x3 High Pass Filter In image processing these sorts of filters are often called edge-detectors\nSharp_kernel  = np.array([[-1, -1, -1], [-1,  8, -1], [-1, -1, -1]])\nsharpen = cv2.filter2D(image_, -1, Sharp_kernel )\n\nfigure(figsize=(12,8), dpi=70)\nplt.subplot(121),plt.imshow(image_),plt.title('Original')\nplt.xticks([]), plt.yticks([])\nplt.subplot(122),plt.imshow(sharpen),plt.title('Sharpen')\nplt.xticks([]), plt.yticks([])\nplt.show()","e687dd8a":"# 3x3 Laplacian\nlaplacian= np.array([[0, -1, 0], [-1,  4, -1], [0, -1, 0]])\n\nlaplacian= cv2.filter2D(image_, -1, laplacian)\n\nfigure(figsize=(12,8), dpi=70)\nplt.subplot(121),plt.imshow(image_),plt.title('Original')\nplt.xticks([]), plt.yticks([])\nplt.subplot(122),plt.imshow(laplacian),plt.title('laplacian')\nplt.xticks([]), plt.yticks([])\nplt.show()","548b7d6e":"# 5x5 Mexican Hat filter\nmexican_hat = np.array([[0,0,-1,0,0],[0,-1,-2,-1,0],[-1,-2,16,-2,-1],[0,-1,-2,-1,0],[0,0,-1,0,0]])\n\nmexican_hat= cv2.filter2D(image_, -1, mexican_hat)\n\nfigure(figsize=(12,8), dpi=70)\nplt.subplot(121),plt.imshow(image_),plt.title('Original')\nplt.xticks([]), plt.yticks([])\nplt.subplot(122),plt.imshow(mexican_hat),plt.title('mexican_hat')\nplt.xticks([]), plt.yticks([])\nplt.show()","5644d85f":"# 3x3 linear smoothing filter In image processing these sorts of filters are often used for blurring\nsharpen_filter = np.array([[0,0,-1,0,0],[0,-1,-2,-1,0],[-1,-2,16,-2,-1],[0,-1,-2,-1,0],[0,0,-1,0,0]])\nlow_pass_kernel = np.ones((5,5),np.float32)\/25\nlena_blurred= cv2.filter2D(image_, -1, low_pass_kernel)\n\n\nsharpen1 = cv2.filter2D(lena_blurred, -1, Sharp_kernel )\nsharpen2 =  cv2.filter2D(lena_blurred, -1, sharpen_filter  )\n\n\ntitles = [\"Original\", \"Blurred\",  \"Edge Enhancement\",\"Mexican Hat Enhancement\"]\nimdic = [ image_, lena_blurred, sharpen1 , sharpen2]\nfigure(figsize=(16,8), dpi=120)\nfor i in range(4):   \n    \n    plt.subplot(2,2,i+1)\n    plt.imshow(imdic[i], \"gray\")   \n    plt.title(titles[i])\n    plt.xticks([])\n    plt.yticks([])\nplt.show() \n","ce933b0f":"image_ = cv2.imread(minnions)\nimage_ = cv2.cvtColor(image_, cv2.COLOR_BGR2RGB)\n\n\nkernel_pink = np.array ([[0.2, 0.7,0.1],[0.02,0.8,0.02],[0.2, 0.7,0.1] ])\nsepia = np.array([[0.272, 0.534, 0.131],   [0.349, 0.686, 0.168],     [0.393, 0.769, 0.189]])\ns     = np.array([[0,-1,-1],   [1,0,-1],    [1,1,0]])\n\npink_m=cv2.transform(image_,kernel_pink)\nsep_m = cv2.transform(image_,sepia)\ns = cv2.transform(image_,s)\nplt.imshow(pink_m)","a0b5dfac":"## Section 1.3 Resize, Flip and Rotate <a class=\"anchor\"  id=\"section1_3\"><\/a>","b557d7ea":"## Section 1.1 Reading Images by OpenCV <a class=\"anchor\"  id=\"section1_1\"><\/a>\n\n\n1. To read an image by opencv, we will use the command of:\n   \n >image = cv2.imread(path)\n \n    * Where the path is the direct path of the image where it is stored. For our kaggle case it is stored as \"\/kaggle\/input\/opencv-samples-images\/data\/butterfly.jpg\". As you can see it is in .jpg format which is i will explain soon. We always add letter of r before the path to ensure that it is a raw string.\n\n\n2. Opencv reads the image by the channel order of Blue Green Red (BGR), however our systems anticipate the order as Red Green Blue (RGB). So that, we have to translate it as: \n\n>cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\nthe type of the read image is np.arry, thats important because we can play on it with numpy.","3505e46d":"##### ****$\\color{pink}{\\text{Conclusion of Bluring}}$****\n\nMedian Blur almost eliminate the salt and pepper noise whereas other filters do not. Still, other filters can be used for other operations for the specific problems you desire to solve.","246868ee":"Let say we have a blurred image and we would like to find the continous lines or edges. So first create a blurred image and then applied the kernels and discuss.","133a20b6":"The Gaussian kernel is the function that has a peak in the center and the curve flattens out as you move towards the edges. Imagine that this distribution is superimposed over a group of pixels in an image.The kernel is the approximate of the gaussian function.\n\n![Gaussian Kernel](https:\/\/homepages.inf.ed.ac.uk\/rbf\/HIPR2\/figs\/gausmask.gif)","868f9238":"## Section 2.2.4 Bonus: Create Your Own Instagram Filter <a class=\"anchor\"  id=\"section2_2_4\"><\/a>\n\n\nU can use transform function to try new kernels :)\n\n>cv2.transform(image_,kernel)","6571654f":"##### Resizing by not preserving the aspect ratio:","e7ca68bd":"##### Drawing polylines\nwe need to define all point carefully by using numpy array\n>cv2.polylines(image, [pts], isClosed, color, thickness)","833ee8ee":"One of the basic filtering operations is called \"low-pass\". A low-pass filter, also called a \"blurring\" or \"smoothing\" filter, averages out rapid changes in intensity. The simplest low-pass filter just calculates the average of a pixel and all of its eight immediate neighbors. The result replaces the original value of the pixel. The process is repeated for every pixel in the image by convolution operation.\n\n","f2f82beb":"To understand better why we use smoothing\/blurring or low pass filter, we will add same salt&pepper noise to our lena by using scikit-image's random_noise command. After that we will appply 1 pass 5x5 low pass kernel. This kernel also called \"averaging\" too. After we will check the results after.","273e645f":"##### ****$\\color{pink}{\\text{Conclusion of Sharpening}}$****\n\nSharpening helps to restore blurred images. As you can see the kernels has a property of that all elements of kernel must be equal to one. This creates a shift on the each pixels of the image by differentiating the neighbours of the corresponding pixel.","c8943556":"## Section 2.2.3 Edge Enhancement <a class=\"anchor\"  id=\"section2_2_3\"><\/a>\n\n\nEdge enhancement in vital for the images processing because the human visual system uses edges as a key factor in the understanding of the image. The comprehension of the contents of an image is not only important for the human system but also important for the computer systems. Edge detection is one of the important technique since it reveals the neccessary boundiries of an image.","3c4602cd":"\n# Chapter 1. Introduction to OpenCV <a class=\"anchor\" id=\"chapter1\"><\/a>\n\nOpen Source Computer Vision Library (OpenCV) is an open source computer vision and machine learning software library which contains more than 2500 optimized algorithm [1]. \n\nComputer vision is a rapidly growing field devoted to analyzing, modifying, and high-level understanding of images. Its objective is to determine what is happening in front of a camera and use that understanding to control a computer or robotic system, or to provide people with new images that are more informative [2].\n\n[1]  OpenCV library; https:\/\/opencv.org\/about\/. \n\n[2] Pulli, K., Baksheev, A., Kornyakov, K., & Eruhimov, V. (2012). Real-time computer vision with OpenCV. Communications of the ACM, 55(6), 61-69.\n\n\n*To import opencv:*\n\n* cv2 : Opencv library for image processing. To load to your local machine use the command line \"pip install opencv-python\" to your environment","a8af896c":"##### Drawing a line\n>cv2.line(image, start_point, end_point, color, thickness)","bef64250":"### Sub-section 1.3.2 Flipping an Image <a class=\"anchor\"  id=\"subsection1_3_2\"><\/a>\n\n##### Horizontal Flip\n\n> cv2.flip(image, 0)\n\n##### Vertical Flip\n\n > cv2.flip(image, 1)","008991bd":"## Section 2.2.1 Blur <a class=\"anchor\"  id=\"section2_2_1\"><\/a>","a88d981b":"##### Drawing a circle\n>cv2.circle(image, center_coordinates, radius, color, thickness)","7d35dd38":"In image processing color histogram is a representation of the distribution of colors in an image. For images, a color histogram represents the number of pixels that have colors in each of a fixed list of color ranges, that span the image's color space, the set of all possible colors.","e653b05d":"# Chapter 2. Basic Image Processing <a class=\"anchor\" id=\"chapter2\"><\/a>\n\nDigital image processing means obtaining the desired information on digital images or making the image desired by using computer algorithms. Since digital images consist of numbers, these desired properties or states are obtained by mathematical matrix operations. Digital image processing, which is considered a sub-topic of digital signal processing, has many advantages over analog image processing.\n\n\nThe opencv library enables us to perform basic operations through the many algorithms it contains. In this chapter, we will see operations such as blurring and sharpening on the image along with filter operations. In this way, we will be prepared for OpenCV for Computer Vision Part 2.","46d0b69d":"## Section 1.2 RGB to GRAY Scale <a class=\"anchor\"  id=\"section1_2\"><\/a>\n\nColor images are captured in a format that have 3 channels. That's why butterfly.jpg has dimentions as 356 x 493 x 3. Which means that image has 3 channels (RGB) and every channels consist of 8-bit ( values between 0-255 ) 356x493 matrice shape pixels.\n\n>cv2.cvtColor(image, cv2COLOR_BGR2GRAY) \n\n>cv2.cvtColor(image, cv2COLOR_RGB2GRAY) ","999b9661":"##### ****$\\color{pink}{\\text{Conclusion of Edge Enhancement}}$****\n\nEdge enhancement helps to create the boundries of the edges in an image","0edeb736":"### ****$\\color{orange}{\\text{If You want to be an AI Expert, Knowing Deep learning is never enough alone!}}$****\n\n### $\\color{Pink}{\\text{Table of Contents}}$\n\n* [Chapter 1. Introduction to OpenCV](#chapter1)\n    * [Section 1.1 Reading Images by OpenCV ](#section1_1)\n    * [Section 1.2 RGB to GRAY Scale](#section1_2)\n    * [Section 1.3 Resize, Flip and Rotate](#section1_3)\n        * [Sub-section 1.3.1 Resize](#subsection1_3_1)\n        * [Sub-section 1.3.3 Flip](#subsection1_3_2)\n        * [Sub-section 1.3.3 Rotate](#subsection1_3_3)\n    * [Section 1.4 Adding Text](#section1_4)\n    * [Section 1.5 Drawing shapes](#section1_5)\n    * [Section 1.6 Cropping & Region of Interest](#section1_6)\n    \n    \n        \n* [Chapter 2 Basic Image Processing ](#chapter2)\n    * [Section 2.1 Histograms](#section_2_1)\n    * [Section 2.2 Linear Filters](#section_2_2)\n        * [Section 2.2.1 Blur](#section2_2_1)\n        * [Section 2.2.2 Sharpening](#section2_2_2)\n        * [Section 2.2.3 Edge Enhancement](#section2_2_3)\n        * [Section 2.2.4 Bonus: Crete Your Own Instagram Filter](#section2_2_4)\n\n    \n\n****$\\color{pink}{\\text{If You like my work, Please upvote  }}$****\n","de911d1d":"Cropping is an important utility operation. Sometimes we do not want to consider all of the image. Rather than including all details, cropping the image and taking the region we are interested is so much beneficial for image processing. The part we take of is called Region of Interest \"ROI\". Having told that, we just process the image as a dataframe and gather the pixels we want to crop like below.","00954f38":"For rotating any arbitrary angle rather than 90 and its factors, you need to paddle the image matrix which means you have to fill the rest to contain the all image in it.Imutils enables this operation.\n\n>imutils.rotate_bound(image, int(angle))\n\nImutils is a image operation library developed by Adrian Rosebrock, founder of Pyimagesearch. I suggest you to check.","9a3e43a3":"## Section 1.5 Drawing shapes  <a class=\"anchor\"  id=\"section1_5\"><\/a>\n\n By using OpenCV, you can draw lines, shapes on an image.","11de7477":"## Section 1.4 Adding Text  <a class=\"anchor\"  id=\"section1_4\"><\/a>\n\nputText method is used to draw a text string on any image.\n\n>cv2.putText(image, text, org, font, fontScale,  color, thickness, cv2.LINE_AA, False)","b4a76b76":"We got the paths of the images we will use in the notebook, and create a dictionary for some printing purposes.","18739b04":"### Sub-section 1.3.1 Resizing an Image <a class=\"anchor\"  id=\"subsection1_3_1\"><\/a>","30a287c1":"##### Drawing a rectangular\n>cv2.rectangle(image, start_point, end_point, color, thickness)","b0765aa8":"## Section 1.6 Cropping & Region of Interest <a class=\"anchor\"  id=\"section1_6\"><\/a>\n","fd1b613f":"![kaggle.png](attachment:ea94d8f9-115a-4426-8856-847360867c6f.png)","c8fd3c39":"## Section 2.1 Histograms <a class=\"anchor\"  id=\"section2_1\"><\/a>\n\n\nAn image histogram is a type of histogram that acts as a graphical representation of the tonal distribution in a digital image [3]. By the help of histogram we could have an idea about the distribution of the pixel values in an image. If the image is a color image (RGB), then we could divide the channels as Red-Green-Blue and investigating it seperately. We could use matplotlib command to see the histogram distribution of an given image.\n\n\n>plt.hist(image.ravel(),256,[0,256])\n\n>plt.show()\n\n[3] Ed Sutton. \"Histograms and the Zone System\". Illustrated Photography. Archived from the original on 2015-02-23. Retrieved 2015-08-31.","dafe176a":"## Section 2.2 Linear Filters <a class=\"anchor\"  id=\"section2_2\"><\/a>\n\n**Convolution**\n\nConvoulution is a math operation which is used commonly in signal processing. It also very similar to convolution which are used in Convolutional Neural Networks. The idea behind is to multiplation of a filter matrix (let say 3x3) by the image by sliding manner which changes the value of a pixel according to the values of its surrounding pixels. We can say the filters to kernels as well.\n\n![cnv](https:\/\/ai.stanford.edu\/~syyeung\/cvweb\/gifs\/moving%20average.gif)\n\n\n\nFiltering is a technique for modifying or enhancing an image. For example, you can filter an image to emphasize certain features or remove other features. Image processing operations implemented with filtering include:\n\n* smoothing or blur, \n* sharpening,\n* edge enhancement.","ffaaaa3d":"### Sub-section 1.3.3 Rotating an Image <a class=\"anchor\"  id=\"subsection1_3_3\"><\/a>\n\n##### Rotating the image\n\nopencv enables to rotate the image in a factor of 90 degrees, if you want to rotate the pic in a specific manner you can use imutils lib like i will show you below. However, first check opencv 90 degrees rotating.","15bff849":"##### Resizing by preserving the aspect ratio\n\n>cv2.resize(image, (width,height), interpolation = cv2.INTER_AREA) ","0095fc19":"## Section 2.2.2 Sharpening <a class=\"anchor\"  id=\"section2_2_2\"><\/a>\n\n\nSharpening is an inverse process accordinng to bluring,to find the differenceby the neighborhood, done by spatial differentiation. spatial differentiation. Sharpening is used to find the difference by the neighborhood and enhancing by differentiation.\n\n\n"}}