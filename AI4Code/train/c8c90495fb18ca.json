{"cell_type":{"a4927dfa":"code","214d67e0":"code","1f93d7ca":"code","d19b580c":"code","ea86a367":"code","371fb18d":"code","cacd5a57":"code","d606a1d0":"code","f1ef3a6b":"code","f793fad3":"code","e097c34e":"code","5225a879":"code","fe14798d":"code","76836c48":"code","c4e825d6":"code","a2d4483f":"code","8ccc530a":"code","043ba521":"code","4304b7e7":"code","bfc53332":"code","ee12b4f2":"code","35ffbc5a":"markdown","808bede7":"markdown","8ae1c9bf":"markdown","bc7679f3":"markdown","d208bbb2":"markdown","46afb04a":"markdown","78601241":"markdown","2b92a1c4":"markdown","9752d064":"markdown","b69cf5e5":"markdown","342a458b":"markdown","71ec69ce":"markdown","786ff503":"markdown","218ef6ae":"markdown","3c5be2ae":"markdown","3977cac4":"markdown","004b06e1":"markdown","61b86dc9":"markdown","f1e06644":"markdown","493ee837":"markdown","67353b36":"markdown","b4b3ebdd":"markdown","9ee075ac":"markdown","620ccaff":"markdown","7ea00f41":"markdown","1683fb40":"markdown"},"source":{"a4927dfa":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom langdetect import detect_langs\nfrom textblob import TextBlob\n\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem.snowball import SnowballStemmer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,RandomizedSearchCV\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, log_loss, confusion_matrix\n\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","214d67e0":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/nlp-getting-started\/sample_submission.csv\")","1f93d7ca":"display(train.head(10))\ndisplay(test.sample(5))\nsubmission","d19b580c":"tr_rows,tr_cols=train.shape\nte_rows,te_cols=test.shape\nprint(\"Rows in train data is\",tr_rows,\"\\nRows in train data\",te_rows)","ea86a367":"train.id.nunique()","371fb18d":"train=train.set_index('id')\ntrain.sample(5)","cacd5a57":"train.isnull().sum()","d606a1d0":"train.info()","f1ef3a6b":"train.target.value_counts(normalize=True)","f793fad3":"disaster_df=train[train.target==1]\nnon_disaster_df=train[train.target==0]\ndisaster_df_kw=set(disaster_df.keyword)\nnon_disaster_df_kw=set(non_disaster_df.keyword)","e097c34e":"disaster_df","5225a879":"dis_kw=len(disaster_df_kw)\nndis_kw=len(non_disaster_df_kw)\nprint(dis_kw,ndis_kw)","fe14798d":"common_kw=disaster_df_kw.intersection(non_disaster_df_kw)\ncommon_kw_no=len(common_kw)\nprint(\"Number of common keywords are{}\".format(common_kw_no))\nprint(common_kw)","76836c48":"disaster_tweet=''\nfor x in disaster_df.text:\n    disaster_tweet=disaster_tweet+str(x)\ndisaster_tweet_wc=WordCloud(background_color='white',stopwords=STOPWORDS).generate(disaster_tweet)\nplt.imshow(disaster_tweet_wc)\nplt.axis('off')\nplt.show()","c4e825d6":"non_disaster_tweet=''\nfor x in non_disaster_df.text:\n    non_disaster_tweet=non_disaster_tweet+str(x)\nnon_disaster_tweet_wc=WordCloud(background_color='white').generate(non_disaster_tweet)\nplt.imshow(non_disaster_tweet_wc)\nplt.axis('off')\nplt.show()","a2d4483f":"X_train,X_test,y_train,y_test=train_test_split(train.text,train.target,test_size=0.20,random_state=3)\nx_sh=X_train.shape[0]\ny_sh=X_test.shape[0]\nprint('Number of data in training set:',x_sh,'Number of data in CV set:',y_sh)","8ccc530a":"count_vect=CountVectorizer()\nc_X_train=count_vect.fit_transform(X_train)\nc_X_test=count_vect.transform(X_test)","043ba521":"mult_nb=MultinomialNB()\nmult_nb.fit(c_X_train,y_train)\ny_predict=mult_nb.predict(c_X_test)","4304b7e7":"acc_score=accuracy_score(y_test,y_predict)\nconf_score=confusion_matrix(y_test,y_predict)\nprint('Accuracy Score:',acc_score,'\\nconfusion Matrix\\n',conf_score)","bfc53332":"c_X_t=count_vect.transform(test['text'])\ny_predict1=mult_nb.predict(c_X_t)\ntest['target']=y_predict1","ee12b4f2":"test['target'].to_csv('submission.csv')","35ffbc5a":"**7.WordCloud**","808bede7":"Keyword column has 61 missing values. location column missing 2533 missing values.","8ae1c9bf":"**2. Setting Index**","bc7679f3":"**4. Datatype**","d208bbb2":"# EDA","46afb04a":"# Building Model - 1st try","78601241":"I planned to build a multinomialNB model with only text column of the train set.","2b92a1c4":"# Importing Libraries","9752d064":"**5.Number of Disaster Tweets**","b69cf5e5":"unique keywords in disaster tweets are 221 and non-disaster keywords are 219. Lets check how many keywords are common between two sets.","342a458b":"Creating separate Dataframe for disaster and non-disaster tweets","71ec69ce":"Training data has 7613 rows.\nTest data has 3263 rows.","786ff503":"**4.Evaluating Model**","218ef6ae":"# Reading Data","3c5be2ae":"When we inspected the train and test data intialy, we found few NAN values. So we need to find the number of missing values in each column.","3977cac4":"3271 disaster tweets are there. Around 42.9% of train set are Disaster Tweets","004b06e1":"I am planning to set \"id\" as Index. So I am going to check unique values in 'id' column.","61b86dc9":"**2.Countvectorizer**","f1e06644":"**1. Shape of Train and Test Data**","493ee837":"Number of unique value matches with number of rows.So We are going to set id column as index","67353b36":"Our model able to achieve the accuracy score 80%","b4b3ebdd":"**3.Building MultinomialNB**","9ee075ac":"**6.Keyword seperation**","620ccaff":"**3. Number of missing values**","7ea00f41":"**1.Splitting the data**","1683fb40":"Intially I thought keywords between Disaster and Non-Disaster tweet vary a lot. But examining almost common keywords are same. So I planned to omit Keyword column while fit the model."}}