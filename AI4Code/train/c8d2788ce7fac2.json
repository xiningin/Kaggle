{"cell_type":{"0c9c14ac":"code","a2fece42":"code","3d8a8fc8":"code","8bee83dd":"code","9db6de1c":"code","aa7cd750":"code","6597c223":"code","98bdf5af":"code","d78bb0ba":"code","a357f85b":"code","de41221a":"code","cb4b57e9":"code","eb8fb2d8":"code","2d9d6d55":"code","e6a88a40":"code","60a95a03":"code","f6ff9426":"code","83203036":"code","cf8ee5e9":"code","0ddff188":"code","27951489":"code","20246d0c":"code","c572f5f8":"code","0d6f1dea":"markdown","0136e2e4":"markdown","c57ff880":"markdown","9198a0ee":"markdown","c85cc89b":"markdown","b174ddde":"markdown","fb946bb9":"markdown","476bf2af":"markdown"},"source":{"0c9c14ac":"import pandas as pd \r\nimport numpy as np \r\nimport matplotlib.pyplot as plt","a2fece42":"# Columns we will be extracting\r\ncols = [\r\n    \"crkever\", # Ever used crack\r\n    \"cocever\", # Ever used cocaine\r\n    \"iralcfy\", # Number of days used alcohol in past year\r\n    \"catag3\", # Age group\r\n    \"health\", # Health condition\r\n    \"irwrkstat\", # Work status\r\n    \"ireduhighst2\", # Highest completed education\r\n    \"newrace2\", # Race\/Ethnicity\r\n    \"irsex\", # Sex\r\n    \"irpinc3\", # Income range\r\n    \"irki17_2\", # Number of kids <18 y\/o\r\n    \"irmjfy\", # Number of days used marijuana in past year\r\n    \"wrkdhrswk2\", # Number of hours worked in past week\r\n    \"irhhsiz2\", # Number of people in household\r\n    \"cig30use\", # Number of days smoked cigarettes in past month\r\n    \"irherfy\", # Number of days used heroine in past year\r\n    \"irmethamyfq\", # Number of days used methamphetamine in past year\r\n    \"year\"\r\n]","3d8a8fc8":"# Because our dataset is large, we will read our data in chunks. Each chunk is stored in chunk_list\r\nchunk_list = []\r\n\r\n# Read the csv we exported from our R script into a pandas dataframe\r\nfor chunk in pd.read_csv(\"..\/input\/national-survey-of-drug-use-and-health-20152019\/NSDUH_2015-2019.csv\", index_col=False, usecols=cols, chunksize=1000):\r\n    chunk_list.append(chunk)\r\n\r\n# Concatenate all chunks, then delete our chunk_list to preserve memory\r\ndf = pd.concat(chunk_list, axis=0)\r\ndel chunk_list","8bee83dd":"# Take a brief look at our data and its shape\r\ndf","9db6de1c":"# Check if there is currently any NaN values in our dataset\r\ndf.isna().sum()","aa7cd750":"# Create a copy df that we will modify for cleaning\r\ndf2 = df.copy(deep=True)\r\n\r\n# Look at the unique values in each column, confirming that special codes exist in our data\r\nfor i, col in zip(range(len(cols)), cols):\r\n    print(col)\r\n    print(df2[cols[i]].unique())\r\n    print(\"========================\")","6597c223":"cont_cols = [\r\n    \"iralcfy\",\r\n    \"irmjfy\",\r\n    \"wrkdhrswk2\",\r\n    \"irherfy\",\r\n    \"irmethamyfq\"\r\n]\r\n\r\n# The columns without categorical data are all ordinal\r\nord_cols = [x for x in cols if x not in cont_cols]\r\n\r\n# Function that cleans continuous numerical data counting by year\r\ndef cont_clean_data(x):\r\n\r\n    # Survey codes for \"Bad Data\", Don't Know, Skip, Refused, or Blank\r\n    if ((x == -9) |\r\n    (x == 985) |\r\n    (x == 989) |\r\n    ((x >= 994) & (x < 1000))):\r\n        return np.nan\r\n\r\n    # Codes for \"Have never done...\" or \"Have not done in the past X days\"\r\n    # Equivalent to 0 for numbered questions\r\n    if ((x == 991) | \r\n    (x == 993)):\r\n        return 0\r\n\r\n    # Ignore value if conditions don't match\r\n    return x \r\n\r\n# Function that cleans all other special data codes\r\ndef ord_clean_data(x):\r\n\r\n    # Survey codes for \"Bad Data\", Don't Know, Skip, Refused, or Blank\r\n    if ((x == -9) |\r\n    ((x >= 94) & (x < 100)) |\r\n    (x == 85) |\r\n    (x == 89)):\r\n        return np.nan\r\n\r\n    # Codes for \"Have never done...\" or \"Have not done in the past X days\"\r\n    # Equivalent to 0 for numbered questions\r\n    if ((x == 91) |\r\n    (x == 93)):\r\n        return 0\r\n\r\n    # Ignore value if conditions don't match\r\n    return x ","98bdf5af":"# Changes for wrkdhrsw2\r\ndf2.loc[(df2.irwrkstat == 3) | (df2.irwrkstat == 4), \"wrkdhrswk2\"] = 0\r\ndf2.loc[df2.wrkdhrswk2 == 61, \"wrkdhrswk2\"] = np.nan\r\n\r\n# Changes for binary categorical variables\r\ndf2.loc[(df2.cocever == 2), \"cocever\"] = 0\r\ndf2.loc[(df2.crkever == 2), \"crkever\"] = 0\r\ndf2.loc[(df2.irsex == 2), \"irsex\"] = 0\r\n\r\n# Apply clean_data functions\r\ndf2[cont_cols] = df2[cont_cols].applymap(cont_clean_data)\r\ndf2[ord_cols] = df2[ord_cols].applymap(ord_clean_data)\r\n\r\ndf2","d78bb0ba":"# Observe unique values in each column to ensure our changes\r\n# are correct\r\nfor i, col in zip(range(len(cols)), cols):\r\n    print(col)\r\n    print(df2[cols[i]].unique())\r\n    print(\"========================\")","a357f85b":"df2.isna().sum()","de41221a":"df2['coccrkever'] = np.zeros(df.shape[0])\r\ndf2.loc[(df2.cocever == 1) | (df2.crkever == 1), \"coccrkever\"] = 1\r\n\r\ndf2.loc[(df2.cocever == np.nan) & (df2.crkever == np.nan), \"coccrkever\"] = np.nan\r\ndf2","cb4b57e9":"df2[['cocever','crkever','coccrkever']].sum()","eb8fb2d8":"# Pickle our data\r\ndf2.to_pickle(\".\/NSDUH_cleaned_2016-2019.pkl\")\r\n# df2 = pd.read_pickle(\".\/\/NSDUH_cleaned_2016-2019.pkl\")","2d9d6d55":"from scipy.stats import chi2_contingency, chi2","e6a88a40":"# df2_clean will be df2 with NaN values removed\r\ndf2_clean = df2.copy(deep=True)\r\ndf2_clean = df2_clean.drop(['irwrkstat'], axis=1).dropna()\r\ndf2_clean.isna().sum()","60a95a03":"import seaborn as sns\r\nsns.set(font_scale=1.2)","f6ff9426":"plot = sns.catplot(data=df2_clean, x='year', y='coccrkever', kind='bar', estimator=(lambda x: sum(x)\/len(x)), legend=True)","83203036":"# Observe the total \"Yes\" and \"No\" answers for crack\/cocaine users by year\r\ndf2_clean.groupby('coccrkever').year.value_counts()","cf8ee5e9":"no_values = []\r\nyes_values = []\r\n\r\n# Append lists with yes\/no values, where indices correspond to a given year\r\nfor year in range(2015, 2020):\r\n    no_values.append(df2_clean.groupby('coccrkever').year.value_counts()[0][year])\r\n    yes_values.append(df2_clean.groupby('coccrkever').year.value_counts()[1][year])\r\n\r\n# Create 2D matrix of values\r\nchi_matrix = [no_values, yes_values]\r\nchi_matrix","0ddff188":"# Use alpha of 0.05, but in reality our test is two-tailed\r\nsignificance = 0.05\r\nstat, p, dof, expected = chi2_contingency(chi_matrix)\r\ncritical = chi2.ppf(significance, dof)\r\nprint(\"P-value = %f\\nChi-Squared Stat = %f\\nCritical Value = %f\" %(p, stat, critical))","27951489":"# Function for easily plotting sns barplots on a grid\r\ndef plot_bar(data, grid, x, y, xlabel, ylabel, title, xticklabels, rotation=0):\r\n    ax = fig.add_subplot(grid[0], grid[1], grid[2])\r\n    sns.barplot(data=data, x=x, y=y, \r\n    estimator=(lambda x: sum(x)\/len(x)), ax=ax).set_title(title)\r\n    ax.set(xlabel=xlabel, ylabel=ylabel)\r\n    ax.set_xticklabels(xticklabels, rotation=rotation)","20246d0c":"# Set figure parameters\r\nplt.rcParams['figure.figsize'] = [16, 12]\r\nplt.rcParams['figure.subplot.wspace'] = 0.3\r\nplt.rcParams['figure.subplot.hspace'] = 0.7\r\nfig = plt.figure()\r\n\r\n# Call plot_bar to plot bar graphs for various variables\r\nplot_bar(df2_clean,[2, 2, 1], 'coccrkever', 'iralcfy', 'Crk\/Coc Usage', 'Days Consumed Alcohol', \r\n\"Average # Days Consumed Alcohol\\nin a Year vs Crk\/Coc Usage\", [\"Never Used\", \"Used\"])\r\n\r\nplot_bar(df2_clean,[2, 2, 2], 'ireduhighst2', 'coccrkever', 'Highest Completed Education', 'Proportion of People Who\\nHave Used Crk\/Cocaine', \r\n\"Proportion of People who have\\nUsed Crk\/Coc by Highest Completed Education\",\r\n[\"5th or less\", \"6th\", \"7th\", \"8th\", \"9th\", \"10th\", \"11th\/12th,\\nno diploma,\", \r\n\"High school\\ndiploma\/GED\", \"Some college,\\nno degree\", \"Associate's Deg.\", \"College Grad\\nor Higher\"], 90)\r\n\r\nplot_bar(df2_clean,[2, 2, 3], 'newrace2', 'coccrkever', 'Race\/Ethnicity', 'Proportion of People Who\\nHave Used Crk\/Cocaine', \r\n\"Average # Days Consumed Alcohol\\nin a Year vs Crk\/Coc Usage\", \r\n[\"White\", \"Black\/Afr Am\", \"Native Am\/\\nAK Native\", \"Pacific Isl\/\\nNative HI\", \"Asian\", \"Multiracial\", \"Hispanic\"], 90)\r\n\r\nplot_bar(df2_clean,[2, 2, 4], 'coccrkever', 'irmethamyfq', 'Crk\/Coc Usage', 'Days Used Meth', \r\n\"Average # Days Consumed Alcohol\\nin a Year vs Crk\/Coc Usage\", [\"Never Used\", \"Used\"])\r\n\r\nplt.show()","c572f5f8":"# Pickle our data\r\ndf2_clean.to_pickle(\".\/NSDUH_cleaned_dropna_2016-2019.pkl\")\r\n# df2_clean = pd.read_pickle(\".\/NSDUH_cleaned_dropna_2016-2019.pkl\")","0d6f1dea":"Failed to reject the null hypothesis. So, we can assume the years 2015-2019 have the same distributions\r\n\r\n## Creating Figures\r\n\r\nNow, we can explore how our features interact with our target through graphs. NaN values interfere with our graph estimators, so we will continue using df2_clean for graphs.","0136e2e4":"# Exploratory Data Analysis\r\n\r\n### Are the years significantly different from eachother?\r\n\r\nWe can test this with a Chi-Squared test for homogeneity. To train our model, we want data that does not significantly differ between the years. First, we need to state our hypotheses:\r\n\r\n**Null:** There is no difference in distribution of people who have\/have not used cocaine in the years 2015-2019\r\n\r\n**Alternative:** There is a difference in the distribution of people who have\/have not used cocaine in the years 2015=2019\r\n\r\nWe will be using an **$\\alpha$ = 0.10**, however, the value we pass into chi2.ppf() will be 0.05, as it looks at one tail (the lower tail probability)\r\n\r\nAdditionally, we will drop all rows with NaN so that we only do our analysis with completely valid observations.","c57ff880":"# Importing our Data\r\n\r\nWe have a large dataset, so we're going to choose specific columns to extract. Each column is a code in the NSDUH Survey Documentation, which can be found here:\r\n\r\nhttps:\/\/www.datafiles.samhsa.gov\/sites\/default\/files\/field-uploads-protected\/studies\/NSDUH-2002-2018\/NSDUH-2002-2018-datasets\/NSDUH-2002-2018-DS0001\/NSDUH-2002-2018-DS0001-info\/NSDUH-2002-2018-DS0001-info-codebook.pdf\r\n\r\nA variety of continuous, ordinal, and categorical data was chosen, such as sex, highest completed education, total days consumed alcohol, etc. The results of each survey question is represented as a code. For example, \"newrace2\" contains the following codes to represent categorical data:\r\n\r\n1. 1 = NonHisp White\r\n2. 2 = NonHisp Black\/Afr Am\r\n3. 3 = NonHisp Native Am\/AK Native\r\n4. 4 = NonHisp Native HI\/Other Pac Isl\r\n5. 5 = NonHisp Asian\r\n6. 6 = NonHisp more than one race\r\n7. 7 = Hispanic","9198a0ee":"# Creating our Target Column","c85cc89b":"# Predicting if People Have Tried Crack\/Cocaine \r\n## Exploratory Data Analysis\r\n\r\nYou can also check out my Crack\/Cocaine Usage Prediction repository on my Github!\r\n\r\nhttps:\/\/github.com\/bgallamoza\/Cocaine_Usage_Classification","b174ddde":"There are some special situations that need to be accounted for before applying our clean_data function. These situations are as follows:\r\n\r\n### **wrkdhrsw2**\r\n\r\nDoes not specify \"0 hours worked\", but we have irwrkstat, which specifies \"Unemployed\" or \"Not in work force\" for irwrkstat = 3 or 4. We can use irwrkstat to create 0 values for wrkhrsw2, as logically, someone who is unemployed or not in the work force would work 0 hours per week.\r\n\r\nAdditionally, people who work 61 hours or more are all pooled into wrkdhrsw2 = 61. If we want to maintain the continuous structure of wrkhrsw2, we cannot include this, so we'll remove them by converting 61 into np.nan values.\r\n\r\n### **impweeks**\r\n\r\nWe will assume that if a respondent has skipped this question, it's because it does not apply to them. Therefore, skip codes 89 and 99 should be converted to 0, for 0 weeks having difficulties with mental health.\r\n\r\n### **Binary Categorical Variables**\r\n\r\nWe have a two binary categorical variables that need editing:\r\n\r\n1. **cocever**: \"Have you ever used cocaine before?\"\r\n\r\n2. **irsex** Sex of respondent\r\n\r\nRight now, the codes are such that 1 = Yes, 2 = No. We want to change it to 1 = Yes and 0 = No, as this will be identical to categorical variables we will dummify later.\r\n\r\nWe will do this by matching these situations using df2.loc","fb946bb9":"The data codes for certain columns differ from others. Continuous data columns tend to use three-digit special codes, whereas ordinal and categorical variables use two-digit codes. We will create two separate functions to avoid cleaning meaningful data from the continuous columns.","476bf2af":"# Data Cleaning\n\nNo dataset is perfect. Our NSDUH documentation shows that several of our columns contain special codes equivalent to NaN (such as the 85 == BAD DATA codes in \"cocever\") or essentially equate to 0 (such as 991 == \"Have never drank alcohol\" for \"iralcfy). So we will have to clean our data according to the NSDUH documentation codes if we want to explore trends in the data."}}