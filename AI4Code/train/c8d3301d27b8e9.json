{"cell_type":{"0be4d629":"code","93129e1e":"code","8ecd56ef":"code","e8f702a7":"code","b635c5c9":"code","81886f79":"code","b7d356a6":"code","c18f3140":"code","d98277e2":"code","2370e56e":"code","37fad955":"code","cdde7d5f":"code","1fe670b9":"code","ccd965ae":"code","4d49e76e":"code","cd70b2ec":"code","5aeedd86":"code","98122365":"code","591f5660":"code","f50f806b":"code","6f58d9a7":"code","9e98b6eb":"code","14d2ae57":"code","b6728f36":"code","5702bdfe":"code","a9151158":"code","0b2141fe":"code","fad43f98":"code","6fc1bf45":"code","fd3ee873":"code","d1435e77":"code","3c2e6ec2":"code","8117f358":"code","79ed7dd3":"code","d8aff7eb":"code","cf04c400":"code","77fcafd0":"code","ed69d6b9":"code","d5588705":"code","0a0cc0de":"code","26f749d7":"code","82d59d67":"code","2537f12b":"code","1b448ed2":"code","30dc9fe3":"code","20af123f":"code","4537700e":"code","994cf7f0":"code","85ca3d2c":"code","b2060d87":"code","412ebf57":"code","0a1f1cec":"code","5d873304":"code","0d21567a":"code","cac7f456":"code","f473dfa8":"code","2de4d210":"code","aa372998":"code","67791712":"markdown","1b863c74":"markdown","a64456cb":"markdown","06d5aa12":"markdown","9cc80859":"markdown","e207c359":"markdown","7e951b9f":"markdown","955e87de":"markdown","1f2c83cb":"markdown","093a087c":"markdown","38dcc902":"markdown","993d3eb0":"markdown","1120be5f":"markdown","1c490d1d":"markdown","a32e4239":"markdown","157303bd":"markdown","bdae345e":"markdown","004b5a4d":"markdown","0a04e948":"markdown","a8437d08":"markdown","89e75431":"markdown","d208bfd9":"markdown","13252317":"markdown","d8a9c24a":"markdown","208d8d2e":"markdown","906dd7fd":"markdown"},"source":{"0be4d629":"import pandas as pd\nimport sklearn #rt sklearn\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np\nimport random\n\n# take fraction of the top features regarding xgboost\nTAKE_TOP_FRAC =  .85\n# do SVD decomposition based on how many components, None means no SVD decomposition\nSVD_COMPONENTS = 400\n# standardize the data, not really needed when doing Box-Cox transform\nSTANDARDIZE = True\n# num cross-validation folds\nNUM_FOLDS = 5\n# number of different weights to try for the ensemble\nNUM_WEIGHT_WIGGLES = 40\n# random state\nRANDOM_STATE = random.randint(0,int(2**16))\n# calculate cross-validation scores\nCROSSVAL_SCORES = False\n# calculate training set weights based on difference between training and test distributions\nDENSITY_RATIO_ESTIMATION = False\n# use XGB importances for polynomial and cubic features\nPOLY_USE_XGB = True","93129e1e":"# read in the dataset\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_test = df_test_orig = pd.read_csv('..\/input\/test.csv')\n# we also want to analyze the whole distribution of the data\ndf_all = df_all_orig = pd.concat([df_train.drop(columns=['SalePrice']), df_test])","8ecd56ef":"# take a look at general columns values\ndf_train.describe()","e8f702a7":"#to see how the data values are distributed in the training set\ndf_train.hist(bins=20, figsize=(20,15))\nplt.show()","b635c5c9":"# how the data is distributed in the test set\ndf_test.hist(bins=20, figsize=(20,15))\nplt.show()","81886f79":"# Compute the correlation matrix\ncorr = df_train.corr()\n# list the features with small correlation with sale price\ncutoff = 0.05\nprint(f\"Features with abs correlation lower than {cutoff}:\")\nprint(corr.SalePrice[np.abs(corr.SalePrice) < cutoff])","b7d356a6":"from string import ascii_letters\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set(style=\"white\")\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(20, 15))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","c18f3140":"#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","d98277e2":"#skewness and kurtosis\nprint(\"Skewness: %f\" % np.log(df_train['SalePrice']).skew())\nprint(\"Kurtosis: %f\" % np.log(df_train['SalePrice']).kurt())\n\nsns.distplot(np.log(df_train['SalePrice']));","2370e56e":"#Visualize how your important variables are distributed \nfrom pandas.plotting import scatter_matrix\ncolumns = ['SalePrice', 'OverallQual', 'YearBuilt', 'YearRemodAdd', 'TotalBsmtSF',\n             '1stFlrSF', 'GrLivArea', 'FullBath', 'TotRmsAbvGrd', 'GarageCars', 'GarageArea']\n\nscatter_matrix(df_train[columns], figsize=(20, 20));","37fad955":"rows_to_drop = []\nsns.scatterplot(x='GarageArea', y='SalePrice', data=df_train)","cdde7d5f":"rows_to_drop += list(df_train[df_train.SalePrice>700000 ][df_train.GarageArea > 0].index.values)","1fe670b9":"sns.scatterplot(y='SalePrice', x='TotalBsmtSF', data=df_train)","ccd965ae":"rows_to_drop+= list(df_train[df_train.TotalBsmtSF>6000 ].index.values)","4d49e76e":"sns.scatterplot(y='SalePrice', x='GrLivArea', data=df_train)","cd70b2ec":"rows_to_drop+= list(df_train[df_train.GrLivArea>4000][df_train.SalePrice < 300000].index.values)","5aeedd86":"# sale price is less skewed when we take the logarithm\n#histogram\nsns.distplot(np.log(df_train['SalePrice']));","98122365":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\n#drop outlier rows\n#df_train = df_train.drop(rows_to_drop)\ndf_train_orig = df_train\ny = np.log(df_train.SalePrice)\ndf_all = all_data = pd.concat([df_train.drop(columns=['SalePrice']), df_test])","591f5660":"# list missing data\ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","f50f806b":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")\n\nall_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")\n\nall_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")\n\nall_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")\n\nall_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")\n\n#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))\n\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')\n\nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)\n    \nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')\n    \nall_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)\n\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])\n\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")\n\nall_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])\n\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])\n\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])\n\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])\n\nall_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")\n\n#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\n\nmissing_data.head()","6f58d9a7":"# transforming numerical that are categorical\n#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\nall_data = all_data.drop(['Utilities'], axis=1)\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","9e98b6eb":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(all_data[c].values) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))\n","14d2ae57":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","b6728f36":"# standardize data\n\n# if STANDARDIZE:\n#     standardize_columns = [\n#         \"1stFlrSF\",\n#         \"2ndFlrSF\",\n#         \"GrLivArea\",\n#         \"BedroomAbvGr\",\n#         \"KitchenAbvGr\",\n#         \"TotalSF\"\n#     ]\n#     def standardize(df):\n#         df = df.copy()\n#         numerics = ['float16', 'float32', 'float64']\n#         for c in standardize_columns:\n#             if c == 'SalePrice' or c == 'Id' or c not in df or str(df[c].dtype) in ['category', 'object']:\n#                 continue\n#             # standardize\n#             df[c] = (df[c]-df[c].mean())\/df[c].std()\n#         return df\n\n#     all_data = standardize(all_data)\n","5702bdfe":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\n\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","a9151158":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","0b2141fe":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","fad43f98":"all_data","6fc1bf45":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","fd3ee873":"df_train = all_data.iloc[:len(df_train)]\ndf_test = all_data.iloc[len(df_train):]\nX_train = df_train.values\nX_test = df_test.values\nX = all_data.values\ndf_all = all_data\n\ny = y.astype(np.float64)","d1435e77":"df_all","3c2e6ec2":"import xgboost\nxgb = xgboost.XGBRegressor()\n\nxgb.fit(X_train, y)\nimportances = xgb.feature_importances_\n# Plot the feature importances of the forest\nindices = np.argsort(importances)[::-1]\n\nindices = indices[:int(len(X_train[0])*TAKE_TOP_FRAC)]\n\nX_train = X_train[:,indices]\nX_test  = X_test[:, indices]\nX = X[:, indices]","8117f358":"df_train['SalePrice'] = y\ncorr = df_train.select_dtypes(np.float64).corr()\n# low-correlated columns\n\nindices = np.argsort(corr.SalePrice.values)\ncorr.SalePrice.iloc[indices]\n# take top ten\nmost_correlated = corr.SalePrice.iloc[indices].index.values[-11:-1]","79ed7dd3":"# most correlated columns\nprint(\"Most correlated columns:\")\nprint(most_correlated)\nprint(\"XGB most importaint indices:\")\nprint(indices[:20])","d8aff7eb":"from sklearn import preprocessing\n# create polynomial features\npfeatures = preprocessing.PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\ncfeatures = preprocessing.PolynomialFeatures(degree=3, interaction_only=False, include_bias=False)\n\n\n# most correlated columns\ndf_corr = df_all[most_correlated]\nX_corr = df_corr.values\nX_important = X[:, indices[:20]]\n\nX_trans = X_corr\nif POLY_USE_XGB:\n    X_trans = X_important\n\npfeatures.fit(X_trans)\ncfeatures.fit(X_trans)\nX_poly= pfeatures.transform(X_trans)\nX_cubic = cfeatures.transform(X_trans)\n\n# concatenate 2nd order and 3rd order polynomial features\nX = np.hstack([X,X_poly, X_cubic])\n\nX_train, X_test = X[:len(df_train)], X[len(df_train):]\n\nX_train.shape","cf04c400":"if DENSITY_RATIO_ESTIMATION:\n    from scipy.stats import norm\n    from densratio import densratio\n\n    result = densratio(X, X_test)\n    weights = result.compute_density_ratio(X)","77fcafd0":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC, LassoCV\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split, cross_validate\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')","ed69d6b9":"\nfrom sklearn.decomposition import TruncatedSVD\n\nif not SVD_COMPONENTS is None:\n\n    svd = TruncatedSVD(n_components=SVD_COMPONENTS, n_iter=80, random_state=RANDOM_STATE)\n    svd.fit(X_train, y)\n\n    decomposition = svd\n\n    X = svd.transform(X)\n    X_train = svd.transform(X_train)\n    X_test = svd.transform(X_test)\n\n","d5588705":"# standardise data\nif STANDARDIZE:\n    from sklearn.preprocessing import StandardScaler\n    sc=StandardScaler()\n\n    sc.fit(X)\n    X_train = sc.transform(X_train)\n    X_test = sc.transform(X_test)","0a0cc0de":"#Validation function\ndef rmsle_cv(model):\n    kf = KFold(NUM_FOLDS, shuffle=True, random_state=RANDOM_STATE).get_n_splits()\n    rmse= np.sqrt(-cross_val_score(model,X_train, y, scoring=\"neg_mean_squared_error\", cv = kf))\n    return rmse\n\ndef rmsle(model):\n    rmse= np.sqrt(np.mean((model.predict(X_train)-y)**2))\n    return rmse\n\ndef rmsle_score(y, y_):\n    rmse= np.sqrt(np.mean((y-y_)**2))\n    return rmse","26f749d7":"sorted(sklearn.metrics.SCORERS.keys())","82d59d67":"# fit SVM regression\nmodel_svm = sklearn.svm.SVR(kernel='rbf', C=4)\nmodel_svm = make_pipeline(RobustScaler(), model_svm)\nif CROSSVAL_SCORES:\n    print(f'Mean CV score:{np.mean(rmsle_cv(model_svm))}')","2537f12b":"# fit XGBOOST\nmodel_xgb = xgboost.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.04, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =RANDOM_STATE, nthread = -1)\nmodel_xgb.fit(X_train,y, sample_weight=None)\nif CROSSVAL_SCORES:\n    print(f'Mean CV score:{np.mean(rmsle_cv(model_xgb))}')","1b448ed2":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=RANDOM_STATE))\nif CROSSVAL_SCORES:\n    print(f'Mean CV score:{np.mean(rmsle_cv(lasso))}')","30dc9fe3":"# fit kernel ridge regression\ntunable_params = dict(alpha=[0.2, 0.3, 0.6, 0.7], degree=[1,2,3])\nfixed_params = dict(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nKRR = KernelRidge(**fixed_params)\nif CROSSVAL_SCORES:\n    print(f'Mean CV score:{np.mean(rmsle_cv(KRR))}')","20af123f":"# fit LGBM\nmodel_lgb = lightgbm.LGBMRegressor(objective='regression',num_leaves=11,\n                              learning_rate=0.04, n_estimators=1000,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.4619,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11,\n                              sample_weight=None)\nif CROSSVAL_SCORES:\n    print(f'Mean CV score:{np.mean(rmsle_cv(model_lgb))}')","4537700e":"from sklearn.ensemble import RandomForestRegressor\n# fit random forest\nforest = RandomForestRegressor(n_estimators=200)\nif CROSSVAL_SCORES:\n    print(f'Score:{np.mean(rmsle_cv(forest))}')","994cf7f0":"import keras\nfrom keras.wrappers.scikit_learn import KerasRegressor, BaseWrapper\nfrom keras.layers import Dense, Input\nfrom keras.optimizers import Adam\n\nclass NeuralNet(BaseEstimator, KerasRegressor):\n    def __init__(self, num_features, epochs, batch_size, model=None, verbose=0):\n        self._model = keras.Sequential([\n            Dense(256, activation='relu', input_shape=(num_features,)),\n            Dense(128, activation='relu'),\n            Dense(64),\n            Dense(1)\n        ]) if model is None else model\n        optimizer = Adam(lr=4e-3)\n        \n        self._model.compile(optimizer, loss='mse')\n        self._epochs = epochs\n        self._batch_size = batch_size\n        self._num_features = num_features\n        self._verbose = verbose\n        \n    def fit(self, X, y):\n        self._model.fit(X, y, epochs=self._epochs, batch_size=self._batch_size, verbose=self._verbose)\n\n    \n    def predict(self, X):\n        return self._model.predict(X).flatten()\n    \n    def get_params(self, deep=True):\n        return {'num_features' : self._num_features, \n                'batch_size': self._batch_size,\n               'epochs': self._epochs,\n               'verbose' : self._verbose}\n\n    def set_params(self, **parameters):\n        for parameter, value in parameters.items():\n            setattr(self, parameter, value)\n        return self\n\n# fit neural net\nneural_net = NeuralNet(len(X[0]), 100, 64)\nif CROSSVAL_SCORES:\n    print(f'Score:{np.mean(rmsle_cv(neural_net))}')","85ca3d2c":"from joblib import Parallel, delayed\nimport numpy as np\n\nclass MetaModel(BaseEstimator, RegressorMixin):\n    def __init__(self, models, fit_sub=False):\n        self._models = models\n        self._fitted = []\n        self._fit_sub = fit_sub\n    def fit(self, X, y):\n        \n        if self._fit_sub:\n            for m in self._models:\n                m.fit(X,y)\n        \n        predictions = np.vstack([model.predict(X) for model in self._models]).T              \n        self._meta = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=1.0)\n        self._meta.fit(predictions, y)\n        \n        \n    def predict(self, X):\n        predictions = np.vstack([model.predict(X) for model in self._models]).T\n        #return self._meta.predict(X)+ np.mean(predictions, axis=1)\n        return  self._meta.predict(predictions)\n\n    def get_params(self, deep=True):\n        # suppose this estimator has parameters \"alpha\" and \"recursive\"\n        return {\"models\": self._models, \"fit_sub\" : self._fit_sub}\n\n    def set_params(self, **parameters):\n        for parameter, value in parameters.items():\n            setattr(self, parameter, value)\n        return self\n\n    \nmmodel =  MetaModel([KRR, model_lgb, lasso, neural_net, model_xgb], fit_sub=True)\n\nif CROSSVAL_SCORES: \n    score = np.mean(rmsle_cv(mmodel))\n    print(f'Mean CV score:{score}')\nmmodel.fit(X_train,y)\nmodel = mmodel","b2060d87":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=RANDOM_STATE)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)\n\nmodels = [clone(model_lgb), clone(lasso), clone(model_xgb), clone(KRR) ,MetaModel([KRR, model_lgb, clone(lasso)], fit_sub=True)]\nsam = StackingAveragedModels(models, clone(KRR), NUM_FOLDS)\nif CROSSVAL_SCORES:\n    print(f'Score: {rmsle_cv(sam)}')","412ebf57":"# fit final stacking model\nsam.fit(X_train,y)","0a1f1cec":"def rmsle_score(y, y_):\n    return np.sqrt(np.mean((y-y_)**2))","5d873304":"model_lgb = lightgbm.LGBMRegressor(objective='regression',num_leaves=11,\n                              learning_rate=0.04, n_estimators=2000,\n                              max_bin = 65, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2619,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf =11)\n\nmodel_lgb.fit(X_train, y)\ny_ = model_lgb.predict(X_test)\n\n# generate submission for tis model\ndf_eval = pd.DataFrame({\n    'Id' : df_test_orig['Id'],\n    'SalePrice': np.exp(y_)\n    })\ndf_eval.to_csv(f'submission_lgbm.csv',header=True, columns=['Id', 'SalePrice'], index=False)\n    ","0d21567a":"import xgboost as xgb\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.04, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =RANDOM_STATE, nthread = -1)\nmodel_xgb.fit(X_train, y)\n\n# generate submission for tis model\ny_ = model_xgb.predict(X_test)\ndf_eval = pd.DataFrame({\n    'Id' : df_test_orig['Id'],\n    'SalePrice': np.exp(y_)\n    })\ndf_eval.to_csv(f'submission_xgb.csv',header=True, columns=['Id', 'SalePrice'], index=False)\n    ","cac7f456":"class Ensemble(BaseEstimator, RegressorMixin):\n    def __init__(self, base_models, coefs):\n        self.base_models = base_models\n        self._coefs = coefs\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        pass\n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        prediction = np.vstack([m.predict(X)*c for m, c in zip(self.base_models, self._coefs)]).T.sum(axis=1)\n        \n        return prediction\n    ","f473dfa8":"predictions = model.predict(X_train)\nscore = rmsle_score(predictions, y)\nprint(f'Training set score: {score}')","2de4d210":"from tqdm import tqdm\nw = np.asarray([.6, .2, .2])\n# truncated normal\ndef dist(w):\n    return w\/np.sum(w)\n\ndef truncated_nromal(w=w, std=0.1):\n    return dist(np.clip(np.random.normal(loc=w, scale=0.1), std, np.inf))\n\nweight_vectors = [truncated_nromal() for i in range(NUM_WEIGHT_WIGGLES)]\nw_with_scores = []\n\nfor w in tqdm(weight_vectors):\n    e = Ensemble([sam, model_xgb, model_lgb], w)\n    y_ = e.predict(X_train)\n    score = rmsle_score(y, y_)\n    y_ = e.predict(X_test)\n    print(f'Score: {score} Weights: {w}')\n    w_with_scores.append((score, w, y_))\n    \n    \nw_with_scores = list(sorted(w_with_scores, key=lambda x: x[0]))\n","aa372998":"# output predictions based on top 60% of wiggled weights\nfor score, w, y_ in tqdm(w_with_scores[:int(0.6*NUM_WEIGHT_WIGGLES)]):\n    df_eval = pd.DataFrame({\n    'Id' : df_test_orig['Id'],\n    'SalePrice': np.exp(y_)\n    })\n    df_eval.to_csv(f'submission_{score}_{w[0]}_{w[1]}_{w[2]}.csv',header=True, columns=['Id', 'SalePrice'], index=False)\n    ","67791712":"## Box-Cox Transform\nIn this part we use the Box-Cox transform to reduce the skeweness of the features, this should make fitting the models to the sale price easier.","1b863c74":"## Decomposition\n\nIt may be beneficial to extract the essential directions in the data and project to them. This is why we use SVD, alternatives would be PCA or ICA.","a64456cb":"# Model Fitting","06d5aa12":"## Standardize Data\n\nStandardized data tends to work better with models.","9cc80859":"Take a look also at the test data distribution to see if there are significant differences between the distributions.","e207c359":"In this part we fill the NaN values of the columns","7e951b9f":"## Stacking Models\n\nWe use a stacking technique to further improve generalization capabilities. ","955e87de":"## Density Ratio Estimation for Dataset Balancing\nSince the training data and the test data distributions can vary significantly, it may be beneficial to calculate how much the samples in the training set are odd with regards to the test set distribution. This is just a way to squeeze out additional performance of the model.","1f2c83cb":"# Ensembling and Submissions\n\nAs a last step, we make an ensemble of models that consists of LGBM, XGBoost and the Stacked model.","093a087c":"Encoding the string labels to numbers.","38dcc902":"## Polynomial and Cubic Features\nGenerate polynomial and cubic features from the most important features based on XGBoost or mot correlated features.","993d3eb0":"We can see that there are outliers in the GarageArea, TotalBsmtSF, 1stFlrSF, GrLivArea, GarageArea, we can delete those","1120be5f":"# Data Loading\n\nAnalyzing the data, finding outliers","1c490d1d":"# Meta-Model\n\nWe train a meta-model that basically takes predictions of some fitted base models and makes those predictions a new dataset which is again fitted with a meta-regressor. This should reduce the overfitting that any individual models would have on the dataset.","a32e4239":"## Feature Importance XGBoost\n\nXGBoost is a gradient boosting algorithm. To get a general idea, we can take a look at the following image to see why we can extract feature importances. XGBoost fits many decision trees to the data, basically decision trees work such that they minimize the entropy of the output variable by splitting the dataset when going in depth. This means that the most important features are going to be near the top of the tree, because they reduce the entropy of the dataset significantly.\n\n<img src=\"https:\/\/i2.wp.com\/blog.kaggle.com\/wp-content\/uploads\/2017\/01\/gb_tree1B_4.png?w=1024\" width=\"1000px\">\n\nThis means that XGBoost is a good measure of feature importances.","157303bd":"## Ensemble Weight Estimate\n\nWe just wiggle the weights, i.e. draw from a normal distribution since fitting the weights may lead to overfitting on the data. This is again just to get a better score perhaps, but in general one can also fit the weights of the ensemble.","bdae345e":"## Get Rid of Outliers","004b5a4d":"By visualizing distributions of each column in the data we can see in which cases we have certain outliers. It is often benefficial for the model if outliers with > 3$\\sigma$  are removed. Looking at a scatter_matrix plot and its SalePrice row we can see that some of these columns have those outliers.","0a04e948":"## Skewness\n\nTest for skewness and kurtosis in the output variable SalePrice. \n\nSkewness is an indicator of how the data distribution is assymetric  around its mean. Tipically models favour normally distributed data, which means that it should be symmetric around the mean.  \n\nKurtosis shows us how heavy are the tails of the data distribution, a data distribution with heavy tails tends to have many outliers.","a8437d08":"Now transform numerical features that are actually categorical to categorical features.","89e75431":"## Cross-Validation Parameter Search\nIn principle I would do cross-val parameter search on the models, to find the best training parameters. Since this takes a considerable amount of time - I didn't do it here but used the best parameters that I saw from others.","d208bfd9":"Add an additional feature total square footage.","13252317":"# Data Cleaning and Feature Engineering\n\nIn this section we get rid of NaN values in the columns depending on the type of column. Also, certain columns that are numerical are actually categorical by nature such as year sold and month sold. These columns don't have a linear correlation with the output.","d8a9c24a":"We can see that the data is slightly skewed, but the kurtosis values are fine. After applying the log-transform to the sales price we reduce the skeweness of the output variable significantly, but kurtosis rises a bit.","208d8d2e":"## Data Training Set Distribution\nNex we look at how the data is distributed in the training set to get a quick idea of outliers, perhaps useless features and so on.","906dd7fd":"## Correlation Plot\nWe plot a triangular correlation matrix, then we can see already which features are correlated with the sales price. The uncorrelated ones we can drop since they probably don't have any predictive value."}}