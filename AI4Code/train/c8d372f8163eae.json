{"cell_type":{"e1af2c19":"code","47497ab6":"code","8e9e75bd":"code","1084b9fc":"code","b61e76fe":"code","544281d6":"code","2d451133":"code","efe70aff":"code","e5972555":"code","615b7c49":"code","3e3273a2":"code","8310c39b":"code","9499bd73":"code","0ae89592":"code","b8963e23":"code","67d1f1a4":"code","74368669":"code","f99c7d2a":"code","bbf6a1f7":"code","3431be72":"code","bda68cdc":"code","86feb17f":"code","505fa911":"code","6c66c22b":"code","b24e2e76":"code","cfcfe3a5":"code","d05e23da":"code","75fad6d9":"code","4c8c96ff":"code","4f5b5294":"code","21702db8":"code","4c0987f4":"code","b336e844":"code","13f34e1d":"code","421ce27d":"code","93eb85ed":"code","987c6da7":"code","1484efef":"code","3749567e":"code","c89baab8":"code","3eb59af3":"code","34bc1fec":"code","b4133e51":"code","bb8623d0":"code","56d467e1":"code","258814ea":"code","f9752a00":"code","6b8dd8c1":"code","5bc862b5":"code","0bc9e9a3":"code","675ac777":"code","ea1707b9":"code","7ec0cd9c":"code","f9137020":"code","8478bf3c":"code","0f36bd67":"code","e2172da1":"code","856ab837":"code","6357c47e":"markdown","3a023088":"markdown","9d0de4c1":"markdown","a434e3ad":"markdown","a38e536a":"markdown","2e440e3d":"markdown","8cee2847":"markdown","a3fc095c":"markdown","8465af32":"markdown","32d152df":"markdown","c07108e8":"markdown","480455ac":"markdown","30a48557":"markdown","89954bd6":"markdown","ab924d9f":"markdown","43b6b703":"markdown","4f4f917e":"markdown","47f86cbc":"markdown","b6c3ceb1":"markdown","fbe92048":"markdown","5a1efda0":"markdown","aa23fc1c":"markdown","8d3c66aa":"markdown","3b9907af":"markdown","d8b3712b":"markdown","6c9cffae":"markdown","bbd71e3d":"markdown","3318ef0c":"markdown","c4f93c83":"markdown","afc9f2c1":"markdown","6e07b66c":"markdown","645ff3e8":"markdown","a7d460a0":"markdown","1e6373b6":"markdown","0dcb2d39":"markdown","77f35932":"markdown","40272576":"markdown","5fb14c43":"markdown","e8e68587":"markdown","cc4a34a1":"markdown","cd52523f":"markdown","50803d66":"markdown","732db136":"markdown","cd7c3d1f":"markdown","2e6bb5b4":"markdown","6389dc98":"markdown","f1891650":"markdown","5c82276b":"markdown","5cb56753":"markdown","42c1d4e6":"markdown"},"source":{"e1af2c19":"import warnings\nwarnings.filterwarnings(\"ignore\")","47497ab6":"import pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport pandas as pd\nimport numpy as np\nimport math \nimport xgboost as xgb\nnp.random.seed(2019)\nfrom scipy.stats import skew\nfrom scipy import stats\n\nimport statsmodels\nfrom sklearn.metrics import accuracy_score\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nprint(\"done\")","8e9e75bd":"def read_and_concat_dataset(training_path, test_path):\n    train = pd.read_csv(training_path)\n    train['train'] = 1\n    test = pd.read_csv(test_path)\n    test['train'] = 0\n    data = train.append(test, ignore_index=True)\n    return train, test, data\n\ntrain, test, data = read_and_concat_dataset('..\/input\/train.csv', '..\/input\/test.csv')\ndata = data.set_index('PassengerId')","1084b9fc":"data.head(5)","b61e76fe":"data.describe()","544281d6":" g = sns.heatmap(data[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(),annot=True, cmap = \"coolwarm\")","2d451133":"def comparing(data,variable1, variable2):\n    print(data[[variable1, variable2]][data[variable2].isnull()==False].groupby([variable1], as_index=False).mean().sort_values(by=variable2, ascending=False))\n    g = sns.FacetGrid(data, col=variable2).map(sns.distplot, variable1)","efe70aff":"def counting_values(data, variable1, variable2):\n    return data[[variable1, variable2]][data[variable2].isnull()==False].groupby([variable1], as_index=False).mean().sort_values(by=variable2, ascending=False)","e5972555":"comparing(data, 'Parch','Survived')","615b7c49":"comparing(data, 'SibSp','Survived')","3e3273a2":"comparing(data, 'Fare','Survived')","8310c39b":"comparing(data, 'Age','Survived')","9499bd73":"counting_values(data, 'Sex','Survived')","0ae89592":"data['Women'] = np.where(data.Sex=='female',1,0)\ncomparing(data, 'Women','Survived')","b8963e23":"comparing(data, 'Pclass','Survived')","67d1f1a4":"grid = sns.FacetGrid(data, col='Survived', row='Pclass', size=2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)","74368669":"grid = sns.FacetGrid(data, row='Embarked', col='Survived', size=2, aspect=1.6)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)","f99c7d2a":"data.isnull().sum()","bbf6a1f7":"data.groupby('Pclass').Fare.mean()","3431be72":"data.Fare = data.Fare.fillna(0)","bda68cdc":"print(data.Embarked.value_counts())\ndata.Embarked = data.Embarked.fillna('S')","86feb17f":"data.Cabin = data.Cabin.fillna('Unknown_Cabin')\ndata['Cabin'] = data['Cabin'].str[0]","505fa911":"data.groupby('Pclass').Cabin.value_counts()","6c66c22b":"data['Cabin'] = np.where((data.Pclass==1) & (data.Cabin=='U'),'C',\n                                            np.where((data.Pclass==2) & (data.Cabin=='U'),'D',\n                                                                        np.where((data.Pclass==3) & (data.Cabin=='U'),'G',\n                                                                                                    np.where(data.Cabin=='T','C',data.Cabin))))","b24e2e76":"data['Title'] = data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(data['Title'], data['Sex'])\ndata = data.drop('Name',axis=1)","cfcfe3a5":"#let's replace a few titles -> \"other\" and fix a few titles\ndata['Title'] = np.where((data.Title=='Capt') | (data.Title=='Countess') | (data.Title=='Don') | (data.Title=='Dona')\n                        | (data.Title=='Jonkheer') | (data.Title=='Lady') | (data.Title=='Sir') | (data.Title=='Major') | (data.Title=='Rev') | (data.Title=='Col'),'Other',data.Title)\n\ndata['Title'] = data['Title'].replace('Ms','Miss')\ndata['Title'] = data['Title'].replace('Mlle','Miss')\ndata['Title'] = data['Title'].replace('Mme','Mrs')","d05e23da":"data[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\nfacet = sns.FacetGrid(data = data, hue = \"Title\", legend_out=True, size = 4.5)\nfacet = facet.map(sns.kdeplot, \"Age\")\nfacet.add_legend();","75fad6d9":"sns.boxplot(data = data, x = \"Title\", y = \"Age\")","4c8c96ff":"facet = sns.FacetGrid(data, hue=\"Survived\",aspect=3)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, data['Age'].max()))\nfacet.add_legend()","4f5b5294":"data.groupby('Title').Age.mean()","21702db8":"data['Age'] = np.where((data.Age.isnull()) & (data.Title=='Master'),5,\n                        np.where((data.Age.isnull()) & (data.Title=='Miss'),22,\n                                 np.where((data.Age.isnull()) & (data.Title=='Mr'),32,\n                                          np.where((data.Age.isnull()) & (data.Title=='Mrs'),37,\n                                                  np.where((data.Age.isnull()) & (data.Title=='Other'),45,\n                                                           np.where((data.Age.isnull()) & (data.Title=='Dr'),44,data.Age))))))                   ","4c0987f4":"data['FamilySize'] = data.SibSp + data.Parch + 1\ndata['Mother'] = np.where((data.Title=='Mrs') & (data.Parch >0),1,0)\ndata['Free'] = np.where(data['Fare']==0, 1,0)\ndata = data.drop(['SibSp','Parch','Sex'],axis=1)","b336e844":"import string\nTypeOfTicket = []\nfor i in range(len(data.Ticket)):\n    ticket = data.Ticket.iloc[i]\n    for c in string.punctuation:\n                ticket = ticket.replace(c,\"\")\n                splited_ticket = ticket.split(\" \")   \n    if len(splited_ticket) == 1:\n                TypeOfTicket.append('NO')\n    else: \n                TypeOfTicket.append(splited_ticket[0])\n            \ndata['TypeOfTicket'] = TypeOfTicket\n\ndata.TypeOfTicket.value_counts()\ndata['TypeOfTicket'] = np.where((data.TypeOfTicket!='NO') & (data.TypeOfTicket!='PC') & (data.TypeOfTicket!='CA') & \n                                (data.TypeOfTicket!='A5') & (data.TypeOfTicket!='SOTONOQ'),'other',data.TypeOfTicket)\ndata = data.drop('Ticket',axis=1)","13f34e1d":"comparing(data, 'FamilySize','Survived')","421ce27d":"counting_values(data, 'Title','Survived')","93eb85ed":"counting_values(data, 'TypeOfTicket','Survived')","987c6da7":"counting_values(data, 'Cabin','Survived')","1484efef":"comparing(data, 'Mother','Survived')","3749567e":"comparing(data, 'Free','Survived')","c89baab8":"bins = [0,12,24,45,60,data.Age.max()]\nlabels = ['Child', 'Young Adult', 'Adult','Older Adult','Senior']\ndata[\"Age\"] = pd.cut(data[\"Age\"], bins, labels = labels)","3eb59af3":"data = pd.get_dummies(data)","34bc1fec":"from sklearn.model_selection import train_test_split\ntrainX, testX, trainY, testY = train_test_split(data[data.Survived.isnull()==False].drop('Survived',axis=1),data.Survived[data.Survived.isnull()==False],test_size=0.30, random_state=2019)","b4133e51":"Results = pd.DataFrame({'Model': [],'Accuracy Score': [], 'Recall':[], 'F1score':[]})","bb8623d0":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import accuracy_score","56d467e1":"from sklearn.tree import DecisionTreeClassifier\nmodel = DecisionTreeClassifier(max_depth=4)\nmodel.fit(trainX, trainY)\ny_pred = model.predict(testX)\nres = pd.DataFrame({\"Model\":['DecisionTreeClassifier'],\n                    \"Accuracy Score\": [accuracy_score(y_pred,testY)],\n                   \"Recall\": [recall_score(testY, y_pred)],\n                   \"F1score\": [f1_score(testY, y_pred)]})\nResults = Results.append(res)","258814ea":"pd.crosstab(testY, y_pred, rownames=['Real data'], colnames=['Predicted'])","f9752a00":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(n_estimators=2500, max_depth=4)\nmodel.fit(trainX, trainY)\ny_pred = model.predict(testX)\nfrom sklearn.metrics import accuracy_score\nres = pd.DataFrame({\"Model\":['RandomForestClassifier'],\n                    \"Accuracy Score\": [accuracy_score(y_pred,testY)],\n                   \"Recall\": [recall_score(testY, y_pred)],\n                   \"F1score\": [f1_score(testY, y_pred)]})\nResults = Results.append(res)","6b8dd8c1":"pd.crosstab(testY, y_pred, rownames=['Real data'], colnames=['Predicted'])","5bc862b5":"from sklearn.neighbors import KNeighborsClassifier\nmodel = KNeighborsClassifier()\nmodel.fit(trainX, trainY)\ny_pred = model.predict(testX)\nfrom sklearn.metrics import accuracy_score\nres = pd.DataFrame({\"Model\":['KNeighborsClassifier'],\n                    \"Accuracy Score\": [accuracy_score(y_pred,testY)],\n                   \"Recall\": [recall_score(testY, y_pred)],\n                   \"F1score\": [f1_score(testY, y_pred)]})\nResults = Results.append(res)","0bc9e9a3":"pd.crosstab(testY, y_pred, rownames=['Real data'], colnames=['Predicted'])","675ac777":"from sklearn.svm import SVC\nmodel = SVC()\nmodel.fit(trainX, trainY)\ny_pred = model.predict(testX)\nfrom sklearn.metrics import accuracy_score\nres = pd.DataFrame({\"Model\":['SVC'],\n                    \"Accuracy Score\": [accuracy_score(y_pred,testY)],\n                   \"Recall\": [recall_score(testY, y_pred)],\n                   \"F1score\": [f1_score(testY, y_pred)]})\nResults = Results.append(res)","ea1707b9":"pd.crosstab(testY, y_pred, rownames=['Real data'], colnames=['Predicted'])","7ec0cd9c":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nmodel.fit(trainX, trainY)\ny_pred = model.predict(testX)\nfrom sklearn.metrics import accuracy_score\nres = pd.DataFrame({\"Model\":['LogisticRegression'],\n                    \"Accuracy Score\": [accuracy_score(y_pred,testY)],\n                   \"Recall\": [recall_score(testY, y_pred)],\n                   \"F1score\": [f1_score(testY, y_pred)]})\nResults = Results.append(res)","f9137020":"pd.crosstab(testY, y_pred, rownames=['Real data'], colnames=['Predicted'])","8478bf3c":"from xgboost.sklearn import XGBClassifier\nmodel = XGBClassifier(learning_rate=0.001,n_estimators=2500,\n                                max_depth=4, min_child_weight=0,\n                                gamma=0, subsample=0.7,\n                                colsample_bytree=0.7,\n                                scale_pos_weight=1, seed=27,\n                                reg_alpha=0.00006)\nmodel.fit(trainX, trainY)\ny_pred = model.predict(testX)\nfrom sklearn.metrics import accuracy_score\nres = pd.DataFrame({\"Model\":['XGBClassifier'],\n                    \"Accuracy Score\": [accuracy_score(y_pred,testY)],\n                   \"Recall\": [recall_score(testY, y_pred)],\n                   \"F1score\": [f1_score(testY, y_pred)]})\nResults = Results.append(res)","0f36bd67":"pd.crosstab(testY, y_pred, rownames=['Real data'], colnames=['Predicted'])","e2172da1":"Results","856ab837":"from xgboost.sklearn import XGBClassifier\nfrom sklearn.model_selection import train_test_split\ntrainX = data[data.Survived.isnull()==False].drop(['Survived','train'],axis=1)\ntrainY = data.Survived[data.Survived.isnull()==False]\ntestX = data[data.Survived.isnull()==True].drop(['Survived','train'],axis=1)\nmodel = XGBClassifier(learning_rate=0.001,n_estimators=2500,\n                                max_depth=4, min_child_weight=0,\n                                gamma=0, subsample=0.7,\n                                colsample_bytree=0.7,\n                                scale_pos_weight=1, seed=27,\n                                reg_alpha=0.00006)\nmodel.fit(trainX, trainY)\ntest = data[data.train==0]\ntest['Survived'] = model.predict(testX).astype(int)\ntest = test.reset_index()\ntest[['PassengerId','Survived']].to_csv(\"submissionXGB.csv\",index=False)\nprint(\"done1\")","6357c47e":"I put here some hyper-parameters tuning with n_estmators, max_depth and learning_rate parameters.","3a023088":"**Decision Tree Classifier**","9d0de4c1":"Fare vs Survived","a434e3ad":"Sex vs Survived","a38e536a":"Titanic competition is a very good way to introduce feature engineering and classification models. I'm gonna explore the data and make something with them and also imput missing values. Feature engineering is an important part of machine learning process so I want to spend more time for this part. I'm gonna try I few models and tell you which work the best with train dataset from this competition. Please consider upvoting if this is useful to you :)","2e440e3d":"There are 263 missing ages, 1014 missing cabins. Age is very important variable, so it's worth spending time to imput them. If it comes to imputing cabins - it's too hard to do because dataset has only 1309 observations so 77% cabins are missing.","8cee2847":"People with 'Master' have the highest survival rate. Maybe because people with the master are mainly boys under 13 years old.","a3fc095c":"Correlation matrix between numerical values:","8465af32":"**KNeighbors Classifier**","32d152df":"To check how good each model is I'm gonna split dataset to train (70%) and test (30%) dataset (excluding missing values in Survived variable) and use Accuracy Score from sklearn.metrics. I set random_state to 2019 in order to compare the results between the models.","c07108e8":"**Import the Libraries**","480455ac":"Now I'm gonna get title from each Name in dataset. This variable will be very useful and it can help to imput missing value in Age. People's titles can represent their age, earnings and life status and all these three properties can be associated with the possibility of survival on a ship.","30a48557":"TypeOfTicket vs Survived","89954bd6":"I'm adding here 'train' variable in order to check in the easiest way later which observations are from train and test dataset because I'm gonna join train and test datasets.","ab924d9f":"Cabin vs Survived","43b6b703":"##**Results**","4f4f917e":"Age has a very large impact on the survival rate, but when this variable has missing values - it is useless. I'm gonna impute the missing values using the average age values in particular groups due to the titles.","47f86cbc":"Correlations between numerical variables and Survived aren't so high but it doesn't mean that the other features are not useful.","b6c3ceb1":"SibSp vs Survived","fbe92048":"Let's check the distribution of the cabins in individual passenger classes.","5a1efda0":"##**Explore the Data**","aa23fc1c":"**PassengerId** - the unique id of the row, it doesn't have any effect on Survived value.\n\n**Survived** - binary:\n* 1 -> Survived\n* 0 -> Not survived\n\n**Pclass** (Passenger Class) - economic status of the passenger, this variable has 3 values;\n* 1 -> Upper Class\n* 2 -> Middle Class\n* 3 -> Lower Class\n\n**Name**, **Sex** and **Age** - are self-explanatory.\n\n**SibSp** - the total number of the passengers' siblings and spouse.\n\n**Parch** - the total number of the passengers' parents and children.\n\n**Ticket** - the ticket number.\n\n**Fare** - the passenger fare.\n\n**Cabin** - the cabin number.\n\n**Embarked** is port of embarkation, 3 values:\n* C -> Cherbourg\n* Q -> Queenstown\n* S -> Southampton","8d3c66aa":"Let's see distributions on box plots.","3b9907af":"Missing values in Embarked and Fare variables are very easy to imput because we can use the most popular value or something like that.\n\nI'm gonna replace missing value in Fare with 0 and in Embarked with the most popular value ('S').","d8b3712b":"Embarked vs Survived","6c9cffae":"**SVM**","bbd71e3d":"I need to replace a few titles with 'other' values because these titles are not as popular and have a low frequency of occurrence in this dataset.","3318ef0c":"**Logistic Regression**","c4f93c83":"I'm gonna to put result of each model in Data Frame 'Results'","afc9f2c1":"FamilySize vs Survived","6e07b66c":"The Cabin 'Unknown' will be set to C for the first class, D for the second class and G for the third class. One observation with Cabin 'T' and first class I'll fix with C.","645ff3e8":"I'm cutting Age variable to 5 equal intervals.","a7d460a0":"If it comes to Cabin variable, I'm gonna fill up NaN values with 'Unknown' and get first letter from every Cabin in dataset.","1e6373b6":"Title vs Survived","0dcb2d39":"Pclass vs Survived","77f35932":"**Random Forest Classifier**","40272576":"##**Modeling**\n* Decision Tree Classifier\n* Random Forest Classifier\n* KNeighbors Classifier\n* SVM\n* Logistic Regression\n* XGB Classifier","5fb14c43":"Let's check how the distribution of survival variable  depending on the title.","e8e68587":"Age vs Survived","cc4a34a1":"A few new variables:","cd52523f":"Mother vs Survived","50803d66":"How we see - XGB Classifier gives the best results. This model helps me to get 0.81339 on competition test dataset and it gives me place in 5% best results on Leaderboard.","732db136":"**XGB Classifier**","cd7c3d1f":"Parch vs Survived","2e6bb5b4":"I create dummy variables for all variables with categories using the function get_dummies from pandas.","6389dc98":"##**Feature engineering**","f1891650":"**Import Data**","5c82276b":"Free vs Survived","5cb56753":"* FamilySize - number of family members, people travelling alone will have a value of 1\n* Women - it depends on Sex variable but I'm making it in binary way\n* Mother - women with Mrs title and at least 1 parch, women, children and mothers probably have a survival factor\n* Free - people who don't need to pay fare, these people could win tickets or something like that, they can have a similar survival rate\n* TypeOfTicket - prefixes of ticket, tickets with same prefixes may have a similar class and survival.\n\nIf it comes to TypeOfTicket variable I'm gonna also replace a few values of this variable with 'other' values, relying on the same as in the case of titles.","42c1d4e6":"##**Missing values**"}}