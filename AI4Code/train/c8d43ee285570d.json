{"cell_type":{"51d5896e":"code","9595a87a":"code","2e824093":"code","3d7f6f26":"code","8135d81e":"code","6778904a":"code","3cbf1942":"code","245153f3":"code","0b789623":"code","039500e7":"code","b4305c56":"code","bdf96c93":"code","baddc9fa":"code","4b1a1f24":"code","4cfaaa60":"code","3d917279":"code","73e9f126":"code","234affed":"code","d7189935":"code","6a876c81":"code","2e8b5343":"code","1413c189":"code","71c39c58":"code","6274c434":"code","36da86fe":"code","85aadc9c":"code","0df583fa":"code","c2684d19":"code","9cf33c79":"code","71ec7adf":"code","fa39420d":"code","e8e795f0":"code","6579e6f3":"code","2d3b0a3f":"code","0590e4c7":"code","57d7507f":"markdown","b2eca36b":"markdown","09d7ae81":"markdown"},"source":{"51d5896e":"!pip install pytorch-pretrained-bert pytorch-nlp","9595a87a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n        \nimport xml.etree.ElementTree as ET\nfrom sklearn.metrics import f1_score\nimport torch\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom pytorch_pretrained_bert import BertTokenizer, BertConfig\nfrom pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\nfrom tqdm import tqdm, trange\nimport pandas as pd\nimport io\nimport numpy as np\nimport matplotlib.pyplot as plt\n% matplotlib inline\nimport torch\nfrom pytorch_pretrained_bert import BertTokenizer, BertModel, BertForMaskedLM, BertForSequenceClassification\n\n# Any results you write to the current directory are saved as output.","2e824093":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nn_gpu = torch.cuda.device_count()\ntorch.cuda.get_device_name(0)","3d7f6f26":"Training_XML=[]\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/patent-classification-train'):\n    for filename in filenames:\n        Training_XML.append(os.path.join(dirname, filename))","8135d81e":"Testing_XML=[]\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/pattent-classification-test'):\n    for filename in filenames:\n        Testing_XML.append(os.path.join(dirname, filename))","6778904a":"label_training=[]\nab_training=[]\ndictionnary_training={}\n\nfor i in Training_XML:\n    try:\n        tree = ET.parse(i)\n        root = tree.getroot()\n        for abstract in root.iter('ab'):\n            if(type(abstract.text)==type(\"text\")):\n                ab_training.append(abstract.text)\n                label_training.append(i.split(\"\/kaggle\/input\/patent-classification-train\/alphatrain-\")[1][0])\n                dictionnary_training[abstract.text]=i.split(\"\/kaggle\/input\/patent-classification-train\/alphatrain-\")[1][0]\n    except:\n        print(\"hachem\")\n","3cbf1942":"len(Training_XML)","245153f3":"len(Testing_XML)","0b789623":"label_testing=[]\nab_testing=[]\ndictionnary_testing={}\n\nfor i in Testing_XML:\n    try:\n        tree = ET.parse(i)\n        root = tree.getroot()\n        for abstract in root.iter('ab'):\n            if(type(abstract.text)==type(\"text\")):\n                ab_testing.append(abstract.text)\n                label_testing.append(i.split(\"\/kaggle\/input\/pattent-classification-test\/alphatest-\")[1][0])\n                dictionnary_testing[abstract.text]=i.split(\"\/kaggle\/input\/pattent-classification-test\/alphatest-\")[1][0]\n    except:\n        print(\"hachem\")\n","039500e7":"print(len(ab_testing))\nprint(len(label_testing))\nprint(set(label_testing))","b4305c56":"print(len(ab_training))\nprint(len(label_training))\nprint(set(label_training))","bdf96c93":"label=label_training+label_testing\nab=ab_testing+ab_training","baddc9fa":"print(label.count('A'))\nprint(label.count('B'))\nprint(label.count('C'))\nprint(label.count('D'))\nprint(label.count('E'))\nprint(label.count('F'))\nprint(label.count('G'))\nprint(label.count('H'))\n","4b1a1f24":"sentences = [\"[CLS] \" + str(sentence) + \" [SEP]\" for sentence in ab]\nlabels = label","4cfaaa60":"labels_new=[]\ntokenized_texts=[]","3d917279":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\ntokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\nprint (\"Tokenize the first sentence:\")\nprint (tokenized_texts[0])","73e9f126":"# Set the maximum sequence length. but we'll leave room on the end anyway. \n# In the original paper, the authors used a length of 512.\nMAX_LEN = 128","234affed":"# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]","d7189935":"# Pad our input tokens\ninput_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")","6a876c81":"# Create attention masks\nattention_masks = []\n\n# Create a mask of 1s for each token followed by 0s for padding\nfor seq in input_ids:\n    seq_mask = [float(i>0) for i in seq]\n    attention_masks.append(seq_mask)","2e8b5343":"labels[0]","1413c189":"for i in range(len(labels)):\n    if label[i]=='A':\n        labels_new.append(0)\n    if label[i]=='B':\n        labels_new.append(1)\n    if label[i]=='C':\n        labels_new.append(2)\n    if label[i]=='D':\n        labels_new.append(3)\n    if label[i]=='E':\n        labels_new.append(4)\n    if label[i]=='F':\n        labels_new.append(5)\n    if label[i]=='G':\n        labels_new.append(6)\n    if label[i]=='H':\n        labels_new.append(7)","71c39c58":"# Use train_test_split to split our data into train and validation sets for training\n\n\n\ntrain_inputs, test_inputs, train_labels, test_labels = train_test_split(input_ids, labels_new, \n                                                            random_state=2018, test_size=0.1)\ntrain_masks, test_masks, _, _ = train_test_split(attention_masks, input_ids,\n                                             random_state=2018, test_size=0.1)\n\ntrain_masks, validation_masks, _, _ = train_test_split(train_masks, train_inputs,\n                                             random_state=2018, test_size=0.1)\n\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(train_inputs, train_labels, \n                                                            random_state=2018, test_size=0.1)","6274c434":"# Convert all of our data into torch tensors, the required datatype for our model\n\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntest_inputs = torch.tensor(test_inputs)\n\ntrain_labels = torch.tensor(train_labels)\nvalidation_labels = torch.tensor(validation_labels)\ntest_labels = torch.tensor(test_labels)\n\ntrain_masks = torch.tensor(train_masks)\nvalidation_masks = torch.tensor(validation_masks)\ntest_masks = torch.tensor(test_masks)","36da86fe":"# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\nbatch_size = 32\n\n# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n# with an iterator the entire dataset does not need to be loaded into memory\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n\ntesting_data = TensorDataset(test_inputs, test_masks, test_labels)\ntesting_sampler = SequentialSampler(testing_data)\ntesting_dataloader = DataLoader(testing_data, sampler=testing_sampler, batch_size=batch_size)","85aadc9c":"# Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n\nmodel = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=8)\nmodel.cuda()","0df583fa":"param_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.01},\n    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n     'weight_decay_rate': 0.0}\n]","c2684d19":"# This variable contains all of the hyperparemeter information our training loop needs\noptimizer = BertAdam(optimizer_grouped_parameters,\n                     lr=2e-5,\n                     warmup=.1)","9cf33c79":"# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) \/ len(labels_flat)","71ec7adf":"from sklearn.metrics import f1_score","fa39420d":"# Function to calculate the accuracy of our predictions vs labels\ndef f1_macro(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(pred_flat, labels_flat, average='macro')","e8e795f0":"# Function to calculate the accuracy of our predictions vs labels\ndef f1_weighted(preds, labels):\n    pred_flat = np.argmax(preds, axis=1).flatten()\n    labels_flat = labels.flatten()\n    return f1_score(pred_flat, labels_flat, average='weighted')","6579e6f3":"# Store our loss and accuracy for plotting\ntrain_loss_set = []\n\n# Number of training epochs (authors recommend between 2 and 4)\nepochs = 9\n\n# trange is a tqdm wrapper around the normal python range\nfor _ in trange(epochs, desc=\"Epoch\"):\n    # Training\n    # Set our model to training mode (as opposed to evaluation mode)\n    model.train()\n  \n    # Tracking variables\n    tr_loss = 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n  \n    # Train the data for one epoch\n    for step, batch in enumerate(train_dataloader):\n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        # Clear out the gradients (by default they accumulate)\n        optimizer.zero_grad()\n        # Forward pass\n        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n        train_loss_set.append(loss.item())    \n        # Backward pass\n        loss.backward()\n        # Update parameters and take a step using the computed gradient\n        optimizer.step()\n    \n    \n        # Update tracking variables\n        tr_loss += loss.item()\n        nb_tr_examples += b_input_ids.size(0)\n        nb_tr_steps += 1\n\n    print(\"Train loss: {}\".format(tr_loss\/nb_tr_steps))\n    \n    \n    # Validation\n\n    # Put model in evaluation mode to evaluate loss on the validation set\n    model.eval()\n\n    # Tracking variables \n    eval_loss, eval_accuracy,F1_score, F2_score = 0, 0 , 0.0 , 0.0\n    nb_eval_steps, nb_eval_examples = 0, 0\n\n    # Evaluate data for one epoch\n    for batch in validation_dataloader:\n        # Add batch to GPU\n        batch = tuple(t.to(device) for t in batch)\n        # Unpack the inputs from our dataloader\n        b_input_ids, b_input_mask, b_labels = batch\n        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n        with torch.no_grad():\n          # Forward pass, calculate logit predictions\n          logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n\n        # Move logits and labels to CPU\n        logits = logits.detach().cpu().numpy()\n        label_ids = b_labels.to('cpu').numpy()\n\n        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n        \n        f1 = f1_macro(logits, label_ids)\n        \n        f2 = f1_weighted(logits, label_ids)\n        \n        F1_score+=f1\n        F2_score+=f2\n        eval_accuracy += tmp_eval_accuracy\n        nb_eval_steps += 1\n        \n    print(\"F1 score(macro): {}\".format(F1_score\/nb_eval_steps))\n    print(\"F1 score(weighted): {}\".format(F2_score\/nb_eval_steps))\n    print(\"Validation Accuracy: {}\".format(eval_accuracy\/nb_eval_steps))","2d3b0a3f":"plt.figure(figsize=(15,8))\nplt.title(\"Training loss\")\nplt.xlabel(\"Batch\")\nplt.ylabel(\"Loss\")\nplt.plot(train_loss_set)\nplt.show()\n","0590e4c7":"test_loss, test_accuracy,F1_score_test, F2_score_test = 0, 0,0,0\nnb_eval_steps, nb_test_examples = 0, 0\n\nmodel.eval()\n# Tracking variables \npredictions , true_labels = [], []\n# Predict \nfor batch in testing_dataloader:\n    # Add batch to GPU\n    batch = tuple(t.to(device) for t in batch)\n    # Unpack the inputs from our dataloader\n    b_input_ids, b_input_mask, b_labels = batch\n    # Telling the model not to compute or store gradients, saving memory and speeding up prediction\n    with torch.no_grad():\n        # Forward pass, calculate logit predictions\n        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n    # Move logits and labels to CPU\n    logits = logits.detach().cpu().numpy()\n    label_ids = b_labels.to('cpu').numpy()  \n    # Store predictions and true labels\n    tmp_test_accuracy = flat_accuracy(logits, label_ids)\n    \n    f1 = f1_macro(logits, label_ids)\n\n    f2 = f1_weighted(logits, label_ids)\n\n    F1_score_test+=f1\n    F2_score_test+=f2\n    test_accuracy += tmp_test_accuracy\n    nb_eval_steps += 1\n\nprint(\"F1 score(macro): {}\".format(F1_score_test\/nb_eval_steps))\nprint(\"F1 score(weighted): {}\".format(F2_score_test\/nb_eval_steps))\nprint(\"Test Accuracy: {}\".format(test_accuracy\/nb_eval_steps))","57d7507f":"***TESTING PHASE***","b2eca36b":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nfor i in range(len(sentences)):\n    try:\n        tokenized_texts.append([tokenizer.tokenize(sentences[i])])\n        labels_new.append(labels[i])\n    except:\n        print(type(sent))\n\nprint (\"Tokenize the first sentence:\")\nprint (tokenized_texts[0])","09d7ae81":"**NOTEBOOK:**\n* https:\/\/www.kaggle.com\/hachemsfar\/deep-learning-project\/\n\n**DATASET**\n* https:\/\/www.kaggle.com\/hachemsfar\/patent-classification-train\n* https:\/\/www.kaggle.com\/hachemsfar\/pattent-classification-test"}}