{"cell_type":{"713413e8":"code","5cf950f8":"code","b5428193":"code","9fdb5028":"code","8dffcedd":"code","4c0b0a90":"code","fbeb1748":"code","ec595468":"code","da5599ee":"code","e4c0a25b":"code","bf3a4a42":"code","ee9a0701":"code","edd404cc":"code","6fb6ddcb":"code","9bee5845":"code","a224a396":"code","f98e6498":"code","cc8a8792":"code","4d63ed1e":"code","963b0cba":"code","796e3700":"code","c16cfc21":"code","3772d440":"code","93842a40":"code","64c66995":"code","8cb721f2":"code","5ed1dd10":"code","cb64a919":"code","950daf32":"code","8f13e740":"code","8f53694a":"code","c93fae8c":"code","a2133772":"code","15ac9fb5":"code","d8642ba7":"code","f0174700":"code","ab75dd21":"code","ed07c4e6":"code","93b015b2":"code","ab2c55b5":"code","5ac3fb2c":"code","fe400aae":"code","55742d6d":"code","8f7b2b4e":"markdown","a69adbc1":"markdown","3636d482":"markdown","4099a2f4":"markdown","6f60f795":"markdown","d461973d":"markdown","48bcd26a":"markdown","2c208b21":"markdown","2a9b158a":"markdown","9e870d75":"markdown","2cd9acbf":"markdown","66c2ab67":"markdown","da6a576c":"markdown","78385ac7":"markdown","caf6fbb1":"markdown","19d8798b":"markdown","d00ab56f":"markdown","f9c5291e":"markdown","7d02268d":"markdown","dcb1e311":"markdown","8d1d3a60":"markdown","86d77336":"markdown","fc13f381":"markdown","4fe4626e":"markdown","f2461bda":"markdown"},"source":{"713413e8":"%%capture\n!conda install -y -c conda-forge jax jaxlib flax optax datasets transformers\n!conda install -y importlib-metadata","5cf950f8":"import os\nif 'TPU_NAME' in os.environ:\n    import requests\n    if 'TPU_DRIVER_MODE' not in globals():\n        url = 'http:' + os.environ['TPU_NAME'].split(':')[1] + ':8475\/requestversion\/tpu_driver_nightly'\n        resp = requests.post(url)\n        TPU_DRIVER_MODE = 1\n\n\n    from jax.config import config\n    config.FLAGS.jax_xla_backend = \"tpu_driver\"\n    config.FLAGS.jax_backend_target = os.environ['TPU_NAME']\n    print('Registered TPU:', config.FLAGS.jax_backend_target)\nelse:\n    print('No TPU detected. Can be changed under \"Runtime\/Change runtime type\".')","b5428193":"import jax\njax.local_devices()","9fdb5028":"model_checkpoint = \"bert-base-uncased\" # 'roberta-base' has an error remaining are working.\nper_device_batch_size = 32","8dffcedd":"import numpy as np\nimport datasets\n\ndef simple_rmse(preds, labels):\n    rmse = np.sqrt(np.sum(np.square(preds-labels))\/preds.shape[0])\n    return rmse\n\n\nclass RMSE(datasets.Metric):\n    def _info(self):\n        return datasets.MetricInfo(\n            description=\"Calculates Root Mean Squared Error (RMSE) metric.\",\n            citation=\"TODO: _CITATION\",\n            inputs_description=\"_KWARGS_DESCRIPTION\",\n            features=datasets.Features({\n                'predictions': datasets.Value('float32'),\n                'references': datasets.Value('float32'),\n            }),\n            codebase_urls=[],\n            reference_urls=[],\n            format='numpy'\n        )\n\n    def _compute(self, predictions, references):\n        return {\"RMSE\": simple_rmse(predictions, references)}\n","4c0b0a90":"from datasets import load_dataset, load_metric\nraw_train = load_dataset(\"csv\", data_files={'train': ['..\/input\/commonlitreadabilityprize\/train.csv']})\nraw_test = load_dataset('csv', data_files={'test': ['..\/input\/commonlitreadabilityprize\/test.csv']})","fbeb1748":"# Split the train set into train and valid sets\nraw_train = raw_train[\"train\"].train_test_split(0.1)","ec595468":"metric = RMSE()","da5599ee":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","e4c0a25b":"def preprocess_function(examples):\n    texts = (examples[\"excerpt\"],)\n    processed = tokenizer(*texts, padding=\"max_length\", max_length=128, truncation=True)\n    \n    processed[\"labels\"] = examples[\"target\"]\n    return processed","bf3a4a42":"tokenized_dataset = raw_train.map(preprocess_function, batched=True, remove_columns=raw_train[\"train\"].column_names)","ee9a0701":"tokenized_dataset","edd404cc":"# The test was created by the 0.1 split of the data which is our validation\/evaluation dataset.\ntrain_dataset = tokenized_dataset[\"train\"]\neval_dataset = tokenized_dataset[\"test\"]","6fb6ddcb":"from transformers import FlaxAutoModelForSequenceClassification, AutoConfig\n\nnum_labels = 1\nseed = 0\n\nconfig = AutoConfig.from_pretrained(model_checkpoint, num_labels=num_labels)\nmodel = FlaxAutoModelForSequenceClassification.from_pretrained(model_checkpoint, config=config, seed=seed)","9bee5845":"import flax\nimport jax\nimport optax\n\nfrom itertools import chain\nfrom tqdm.notebook import tqdm\nfrom typing import Callable\n\nimport jax.numpy as jnp\n\nfrom flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\nfrom flax.training import train_state\nfrom flax import traverse_util","a224a396":"num_train_epochs = 10\nlearning_rate = 2e-5","f98e6498":"total_batch_size = per_device_batch_size * jax.local_device_count()\nprint(\"The overall batch size (both for training and eval) is\", total_batch_size)","cc8a8792":"num_train_steps = len(train_dataset) \/\/ total_batch_size * num_train_epochs\n\nlearning_rate_function = optax.cosine_onecycle_schedule(transition_steps=num_train_steps, peak_value=learning_rate, pct_start=0.1, )\nprint(\"The number of train steps (all the epochs) is\", num_train_steps)","4d63ed1e":"class TrainState(train_state.TrainState):\n    logits_function: Callable = flax.struct.field(pytree_node=False)\n    loss_function: Callable = flax.struct.field(pytree_node=False)","963b0cba":"def decay_mask_fn(params):\n    flat_params = traverse_util.flatten_dict(params)\n    flat_mask = {path: (path[-1] != \"bias\" and path[-2:] != (\"LayerNorm\", \"scale\")) for path in flat_params}\n    return traverse_util.unflatten_dict(flat_mask)","796e3700":"def adamw(weight_decay):\n    return optax.adamw(learning_rate=learning_rate_function, b1=0.9, b2=0.999, eps=1e-6, weight_decay=weight_decay, mask=decay_mask_fn)","c16cfc21":"adamw = adamw(1e-2)","3772d440":"@jax.jit\ndef loss_function(logits, labels):\n    return jnp.mean((logits[..., 0] - labels) ** 2)\n\n@jax.jit    \ndef eval_function(logits):\n    return logits[..., 0]","93842a40":"state = TrainState.create(\n    apply_fn=model.__call__,\n    params=model.params,\n    tx=adamw,\n    logits_function=eval_function,\n    loss_function=loss_function,\n)","64c66995":"def train_step(state, batch, dropout_rng):\n    targets = batch.pop(\"labels\")\n    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n\n    def loss_function(params):\n        logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n        loss = state.loss_function(logits, targets)\n        return loss\n\n    grad_function = jax.value_and_grad(loss_function)\n    loss, grad = grad_function(state.params)\n    grad = jax.lax.pmean(grad, \"batch\")\n    new_state = state.apply_gradients(grads=grad)\n    metrics = jax.lax.pmean({\"loss\": loss, \"learning_rate\": learning_rate_function(state.step)}, axis_name=\"batch\")\n    return new_state, metrics, new_dropout_rng","8cb721f2":"parallel_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,))","5ed1dd10":"def eval_step(state, batch):\n    logits = state.apply_fn(**batch, params=state.params, train=False)[0]\n    return state.logits_function(logits)","cb64a919":"parallel_eval_step = jax.pmap(eval_step, axis_name=\"batch\")","950daf32":"def train_data_loader(rng, dataset, batch_size):\n    steps_per_epoch = len(dataset) \/\/ batch_size\n    perms = jax.random.permutation(rng, len(dataset))\n    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n    perms = perms.reshape((steps_per_epoch, batch_size))\n\n    for perm in perms:\n        batch = dataset[perm]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        batch = shard(batch)\n\n        yield batch","8f13e740":"def eval_data_loader(dataset, batch_size):\n    for i in range(len(dataset) \/\/ batch_size):\n        batch = dataset[i * batch_size : (i + 1) * batch_size]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        batch = shard(batch)\n\n        yield batch","8f53694a":"state = flax.jax_utils.replicate(state)","c93fae8c":"rng = jax.random.PRNGKey(seed)\ndropout_rngs = jax.random.split(rng, jax.local_device_count())","a2133772":"for i, epoch in enumerate(tqdm(range(1, num_train_epochs + 1), desc=f\"Epoch ...\", position=0, leave=True)):\n    rng, input_rng = jax.random.split(rng)\n\n    # train\n    with tqdm(total=len(train_dataset) \/\/ total_batch_size, desc=\"Training...\", leave=False) as progress_bar_train:\n        for batch in train_data_loader(input_rng, train_dataset, total_batch_size):\n            state, train_metrics, dropout_rngs = parallel_train_step(state, batch, dropout_rngs)\n            progress_bar_train.update(1)\n\n    # evaluate\n    with tqdm(total=len(eval_dataset) \/\/ total_batch_size, desc=\"Evaluating...\", leave=False) as progress_bar_eval:\n        for batch in eval_data_loader(eval_dataset, total_batch_size):\n            labels = batch.pop(\"labels\")\n            predictions = parallel_eval_step(state, batch)\n            metric.add_batch(predictions=chain(*predictions), references=chain(*labels))\n            progress_bar_eval.update(1)\n\n    eval_metric = metric.compute()\n\n    loss = round(flax.jax_utils.unreplicate(train_metrics)['loss'].item(), 3)\n    eval_score = round(list(eval_metric.values())[0], 3)\n    metric_name = list(eval_metric.keys())[0]\n\n    print(f\"{i+1}\/{num_train_epochs} | Train loss: {loss} | Eval {metric_name}: {eval_score}\")","15ac9fb5":"def preprocess_test_set_function(examples):\n    texts = (examples[\"excerpt\"],)\n    processed = tokenizer(*texts, padding=\"max_length\", max_length=128, truncation=True)\n    \n    return processed","d8642ba7":"tokenized_test_dataset = raw_test.map(preprocess_test_set_function, batched=True, remove_columns=raw_test[\"test\"].column_names)","f0174700":"test_dataset = tokenized_test_dataset[\"test\"]\ntest_dataset","ab75dd21":"def test_data_loader(dataset, batch_size):\n    if len(dataset)<batch_size:\n        batch = dataset[:]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        yield batch\n    else:\n        for i in range(len(dataset) \/\/ batch_size):\n            batch = dataset[i * batch_size : (i + 1) * batch_size]\n            batch = {k: jnp.array(v) for k, v in batch.items()}\n\n            yield batch\n        batch = dataset[(i+1) * batch_size:]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        yield batch","ed07c4e6":"from flax.jax_utils import unreplicate\n\nunrep_state = unreplicate(state)","93b015b2":"def generate_results():\n    preds = []\n    for batch in test_data_loader(test_dataset, total_batch_size):\n        if jax.process_index()==0:\n            predictions = unrep_state.apply_fn(**batch, train=False, return_dict=False)\n            preds.append(predictions[0])\n    return preds","ab2c55b5":"preds = generate_results()","5ac3fb2c":"import numpy as np\npreds = np.vstack([np.asarray(x) for x in preds])\npreds","fe400aae":"import pandas as pd\nsample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\nsample.target = preds\nsample","55742d6d":"sample.to_csv('submission.csv',index=False)","8f7b2b4e":"## Create the initial train state\nFinally!!","a69adbc1":"## AdamW Optimizer\nWe will be using the standard Adam optimizer with weight decay. For more information on AdamW (Adam + weight decay), one can take a look at [this](https:\/\/www.fast.ai\/2018\/07\/02\/adam-weight-decay\/) blog post. `weight_decay` value of 0.01 is a good starting point, you can tweak this hyper-parameter and experiment with how it influences the final trained model.\n\nRegularizing the *bias* and\/or *LayerNorm* has not shown to improve performance and can even be disadvantageous, which is why we disable it here. For more information on this, please check out the following [blog post](https:\/\/medium.com\/@shrutijadon10104776\/why-we-dont-use-bias-in-regularization-5a86905dfcd6) or [paper](https:\/\/arxiv.org\/abs\/1711.05101).\n\nHence we create a `decay_mask_fn` which makes sure that weight decay is not applied to any *bias* or *LayerNorm* weights. This can easily be done by passing a `mask_fn` to `optax.adamw`.\n\n**NOTE**: Beginners can **ignore** the `decay_mask_fn`, the changes are minimal if you leave out doing this step.","3636d482":"# Model\nWe have a regression problem at hand so the model just needs to output 1 number.","4099a2f4":"## Loss and eval functions\nThe standard loss function for regression problems is the MSE loss. The book by Bishop has an additional 0.5 term, but we're skipping in that without loss of generality. That term just scales the loss by a constant factor and doesn't have an impact on the gradients (other than scaling).","6f60f795":"# Generating Results\nOur test dataset has slightly different pre-processing step because we do not have a label in the dataset. So, we should handle accordingly.","d461973d":"# Training\nNow we define the full training loop. For each batch in each epoch, we run a training step. Here, we also need to make sure that the PRNGKey is sharded\/split over each device. Having completed an epoch, we report the training metrics and can run the evaluation.\n\nThe first batch takes a bit longer to process but nothing to worry because during the first batch, XLA compiler is working hard to make everything super fast. The first takes close to 5 mins for processing and then entire epochs take ~5 sec to process. Aren't TPUs amazing!!\n\n**5 seconds for an entire EPOCH!!**\n\nNote: The times mentioned above are an average estimate over 8 different runs on several different TPU machines and several model architectures.","48bcd26a":"I used the One-Cycle LR Scheduler with Cosine Annealing. It is super easy to create this LR Schedule with the [Optax](https:\/\/github.com\/deepmind\/optax) library, it is the recommended library while using any JAX based NN libraries. Optax is being developed by **DeepMind** has several amazing features, definitely give it a try!\n\nTODO: Add citations to the original One-Cycle and Cosine Annealing papers.","2c208b21":"There are 8 cores in TPUv3-8, so the effective `batch_size = 8 * per_device_batch_size`","2a9b158a":"# Loading dataset and metric\nI personally prefer HugginFace datasets because they are very well designed and makes it easy to pre-process all the samples very easily and it has several features like easily loading from the CSV file without using any Pandas data frame objects as intermediates.\n\nFull documentation of the HuggingFace dataset can be found [here](https:\/\/huggingface.co\/docs\/datasets\/package_reference\/main_classes.html#dataset)","9e870d75":"We won't shard our data anymore because usually the test sets are very small and can be done entirely on one-core without having the additional overheads. So, we also have to \"un-shard\" our model and run entirely on the single device of the device slice. So we use the `unreplicate` method in the flax library, [here is the documentation](https:\/\/flax.readthedocs.io\/en\/latest\/flax.jax_utils.html#flax.jax_utils.unreplicate)","2cd9acbf":"## Define Data Loaders\nIn a final step before we can start training, we need to define the data collators. The data collator is important to shuffle the training data before each epoch and to prepare the batch for each training and evaluation step.\n\nFirst, a random permutation of the whole dataset is defined. \nThen, every time the training data collator is called the next batch of the randomized dataset is extracted, converted to a JAX array and sharded over all local TPU devices.","66c2ab67":"# Setup TPU\nPrepare and setup our TPU so that it can be used with JAX. This snip is adapted from [this notebook](https:\/\/www.kaggle.com\/narainp\/jax-tpu-demo-wip)","da6a576c":"# Generation\nFinal step. We have successfully fine-tuned a BERT model to the Lit-Readability task. That's amazing! It took us less than 10 mins to reach a very good score! Now it is time to get our model predictions on our test set.","78385ac7":"Now we clean-up and make our results \"Submission ready\". First we convert all JAX **DeviceArray** objects to Numpy arrays, then we create a submission file.","caf6fbb1":"# Define the RMSE Metric\nThe contest **Common Lit Readability** evaluates all the submission with the **RMSE** metric. Please check the Evaluation tab of the contest to confirm. I implemented the formula given in the Evaluation tab in normal numpy (not JAX). \n\nI created a Metric wrapper from HuggingFace which handles the device and several other device related issues.\n\nRefer to [this link](https:\/\/huggingface.co\/docs\/datasets\/add_metric.html) for creating and defining your own metric using HuggingFace library.","19d8798b":"Next, we replicate\/copy the weight parameters on each device, so that we can pass them to our pmapped functions.\n","d00ab56f":"# Training and evaluation loop","f9c5291e":"# Pre-process the dataset\nThis is a very generic pre-processing nothing special. Just tokenized the sentence and padded it appropriately.","7d02268d":"## Create a Train State\nNext, we will create the *training state* that includes the optimizer, the loss function, and is responsible for updating the model's parameters during training.\n\nMost JAX transformations (notably [jax.jit](https:\/\/jax.readthedocs.io\/en\/latest\/jax-101\/02-jitting.html)) require functions that are transformed to have no side-effects as it follows a functional programming type paradigm at its core. This is because any such side-effects will only be executed once, when the Python version of the function is run during compilation (see [Stateful Computations in JAX](https:\/\/jax.readthedocs.io\/en\/latest\/jax-101\/07-state.html)). As a consequence, Flax models (which can be transformed by JAX transformations) are **immutable**, and the state of the model (i.e., its weight parameters) are stored *outside* of the model instance.\n\nFlax provides a convenience class [`flax.training.train_state.TrainState`](https:\/\/github.com\/google\/flax\/blob\/9da95cdd12591f42d2cd4c17089861bff7e43cc5\/flax\/training\/train_state.py#L22), which stores things such as the model parameters, the loss function, the optimizer, and exposes an `apply_gradients` function to update the model's weight parameters.\n\nWe create a derived `TrainState` class that additionally stores the model's forward pass as `eval_function` as well as a `loss_function`.","dcb1e311":"### Defining the training and evaluation step\n\nDuring fine-tuning, we want to update the model parameters and evaluate the performance after each epoch. \n\nLet's write the functions `train_step` and `eval_step` accordingly. During training the weight parameters should be updated as follows:\n\n1. Define a loss function `loss_function` that first runs a forward pass of the model given data input. Remember that Flax models are immutable, and we explicitly pass it the state (in this case the model parameters and the RNG). `loss_function` returns a scalar loss (using the previously defined `state.loss_function`) between the model output and input targets.\n2. Differentiate this loss function using [`jax.value_and_grad`](https:\/\/jax.readthedocs.io\/en\/latest\/notebooks\/autodiff_cookbook.html#evaluate-a-function-and-its-gradient-using-value-and-grad). This is a JAX transformation called [automatic differentiation](https:\/\/en.wikipedia.org\/wiki\/Automatic_differentiation), which computes the gradient of `loss_function` given the input to the function (i.e., the parameters of the model), and returns the value and the gradient in a pair `(loss, gradients)`.\n3. Compute the mean gradient over all devices using the collective operation [lax.pmean](https:\/\/jax.readthedocs.io\/en\/latest\/_autosummary\/jax.lax.pmean.html). As we will see below, each device runs `train_step` on a different batch of data, but by taking the mean here we ensure the model parameters are the same on all devices.\n4. Use `state.apply_gradients`, which applies the gradients to the weights.\n\nBelow, you can see how each of the described steps above is put into practice.\n\n**NOTE: Taken from HuggingFace examples** ","8d1d3a60":"Export our results to a CSV File.","86d77336":"# JAX and TPUs\nThis is the best possible combination of hardware and software to run the training loops as fast as possible. JAX uses the incredible XLA compiler which makes the code efficient to run on TPUs. HuggingFace is now porting all its models to Flax library which is a JAX based Neural Network library developed by Google Brain. \n\nThis is the best news to try out our NLP based tasks using the Pre-trained models powered by HuggingFace and using the raw power of TPUs. Once the compilation is done, a TPU-v3 can finish an entire epoch of training and evaluating on this dataset in less than 5 seconds (tested on Colab, Kaggle and TPU-VM), this is amazing!\n\nBest part of all, Kaggle gives you 30 free hours of TPUv3-8 usage every week! That is 240$ of free compute every week. (Price estimated as per costs at us-central1 on [Google Cloud TPU pricing](https:\/\/cloud.google.com\/tpu\/pricing))\n\nWithout further ado... Let's GO!\n\nThis notebook is inspired by Flax examples provided by the HuggingFace official library [here](https:\/\/github.com\/huggingface\/transformers\/tree\/master\/examples\/flax)","fc13f381":"# Install dependencies","4fe4626e":"# Model Name and Batch Size\n`model_checkpoint` is the name of the pre-trained model as per HuggingFace nomenclature. I had checked on several choices of this, but for the sake of simplicity for this tutorial let's just consider **BERT-BASE-UNCASED**. In my tests as on 28th June 2021, **ROBERTA-BASE** is giving an error related to an implementation error.\n\n`per_device_batch_size` here I'm giving a good starting point with 32. TPUv3-8 has a massive computational capability. It can handle a lot more than 32 per device\/core of TPU. But as the dataset we have is very tiny with just 2834 samples, I chose the smaller batch size. General rule of thumb, we want to do mini-batch training for better generalization purposes (TODO: Add Citation to the original paper concluded this.)\n\nFeel free to experiment with various models and batch sizes.","f2461bda":"Now, we want to do parallelized training over all TPU devices. To do so, we use [`jax.pmap`](https:\/\/jax.readthedocs.io\/en\/latest\/jax.html?highlight=pmap#parallelization-pmap). This will compile the function once and run the same program on each device (it is an [SPMD program](https:\/\/en.wikipedia.org\/wiki\/SPMD)). When calling this pmapped function, all inputs (`\"state\"`, `\"batch\"`, `\"dropout_rng\"`) should be replicated for all devices, which means that the first axis of each argument is used to map over all TPU devices.\n\nThe argument `donate_argnums` is used to tell JAX that the first argument `\"state\"` is \"donated\" to the computation, because it is not needed anymore afterwards. XLA can make use of donated buffers to reduce the memory needed."}}