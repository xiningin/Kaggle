{"cell_type":{"b6038756":"code","a7b33c6c":"code","cc1eaab0":"code","82ff8069":"code","56aca213":"code","ace7e4b3":"code","1404fa2e":"code","be286267":"code","afa51b33":"code","d8b2eea9":"code","0c523418":"code","e6870c25":"code","67dd3536":"code","a41aab07":"code","8b62a8dd":"code","9d6523c2":"code","a94f1ce5":"code","3b1b1b2d":"code","441195e7":"code","722f3176":"code","648b0946":"code","8c29e336":"code","b74fffd1":"code","74236450":"code","c65d7332":"code","ebe372a4":"markdown","f8809908":"markdown","1fa6cabf":"markdown","ed31456a":"markdown","8d3d2b00":"markdown"},"source":{"b6038756":"import pandas as pd\nimport numpy as np\nimport itertools\nimport matplotlib.pyplot as plt\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nimport re\nfrom collections import Counter\nfrom wordcloud import WordCloud\nimport nltk\nimport string\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nnltk.download('popular')","a7b33c6c":"df = pd.read_csv('..\/input\/one-million-reddit-jokes\/one-million-reddit-jokes.csv')","cc1eaab0":"df.head()","82ff8069":"df.info()","56aca213":"df_txt = df.query(\"selftext not in ['[deleted]','[removed]']\")","ace7e4b3":"df_txt.dropna(subset=['selftext'], inplace=True)","1404fa2e":"df_txt.drop_duplicates(subset='selftext', inplace=True)","be286267":"df_txt.info()","afa51b33":"df_txt = df_txt[['selftext','title']].reset_index(drop=True)","d8b2eea9":"# Remove Punc and Stopword\nstop_punct = list(punctuation)\nstop_nltk = stopwords.words(\"english\")\nstop_final = stop_punct + stop_nltk\n\ndef drop_stop(input_tokens):\n    return ' '.join([word for word in input_tokens.split() if word not in stop_final])\n\nClean = []\nfor text in df_txt['selftext']:\n  kecil = text.lower()\n  tab = re.sub(r\"\\n\",'',kecil)\n  punc = re.sub(r'[^\\w\\s]','',tab)\n  Clean.append(punc)\n\ndf_txt['clean_text'] = Clean\n\ndf_txt['clean_text'] = df_txt['clean_text'].apply(drop_stop)","0c523418":"# Count Number of Words\ndf_txt['word_count'] = df_txt['clean_text'].apply(lambda x: len(str(x).split(\" \")))\ndf_txt.sort_values('word_count', ascending=False).head()","e6870c25":"# Count Number Of Character\ndf_txt['char_count'] = df_txt['clean_text'].str.len() ## this also includes spaces\ndf_txt.sort_values('char_count', ascending=False).head(10)","67dd3536":"# Drop clean_text that duplicate in same title\ndf_txt.drop_duplicates(subset=['clean_text','title'], inplace=True)","a41aab07":"# Count Average Word Length\ndef avg_word(sentence):\n  words = sentence.split()\n  return (sum(len(word) for word in words)\/len(words) if len(words)!=0 else 1)\n\ndf_txt['avg_word'] = df_txt['clean_text'].apply(lambda x: avg_word(x))\ndf_txt.sort_values('avg_word', ascending=False).head()","8b62a8dd":"# Count Number Of Numeric\ndf_txt['numerics'] = df_txt['clean_text'].apply(lambda x: len([x for x in x.split() if x.isdigit()]))\ndf_txt.sort_values('numerics', ascending=False).head()","9d6523c2":"# TF-IDF\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(max_features=1000)\ntfidf_vect = tfidf.fit_transform(df_txt['clean_text'])","a94f1ce5":"tfidf_vect.toarray()","3b1b1b2d":"# Bag Of Word\nfrom sklearn.feature_extraction.text import CountVectorizer\nbow = CountVectorizer(max_features=1000)\nbow_vect = bow.fit_transform(df_txt['clean_text'])","441195e7":"bow_vect.toarray()","722f3176":"# Join all text in clean_text\njoin_text = ' '.join([text for text in df_txt['clean_text']])","648b0946":"join_text[:500]","8c29e336":"# Count Word Freq\nword_count = pd.Series(' '.join(df_txt['clean_text']).split()).value_counts()\nword_count","b74fffd1":"# Plot Top 20\nword_count[:20].plot(kind='bar')\nplt.title('Top 20 Common Word')\nplt.xlabel('Word')\nplt.ylabel('Count')","74236450":"# Plot Rare Word\nword_count[-20:].plot(kind='bar')\nplt.title('Top 20 Rare Word')\nplt.xlabel('Word')\nplt.ylabel('Count')","c65d7332":"# WordCloud Visualization\nwordcloud = WordCloud(width=1600, height=800, max_font_size=200,max_words=20).generate(join_text)\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","ebe372a4":"<h1 style='background:#CCE2CB; border:0; color:black'><center> Importing Libraries <\/center><\/h1> ","f8809908":"<h1 style='background:#CCE2CB; border:0; color:black'><center> Load Data <\/center><\/h1> ","1fa6cabf":"<h1 style='background:#CCE2CB; border:0; color:black'><center> Visualization <\/center><\/h1> ","ed31456a":"<h1 style='background:#CCE2CB; border:0; color:black'><center> Simple EDA & First Cleaning <\/center><\/h1> ","8d3d2b00":"<h1 style='background:#CCE2CB; border:0; color:black'><center> Feature Engineering <\/center><\/h1> "}}