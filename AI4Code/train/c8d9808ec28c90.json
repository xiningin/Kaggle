{"cell_type":{"3cc28650":"code","4fc0cdb3":"code","45cfb1dc":"code","1a5c62c4":"code","4024f0ea":"code","bba9f157":"code","278a5265":"code","4a004a9b":"code","b229eb0c":"code","049bdc14":"code","ff1a4b10":"code","a50c5107":"code","c212c9b4":"code","d8277b5c":"code","c330a134":"code","69aa25d3":"markdown","77a9b12d":"markdown","94306465":"markdown","ace4df31":"markdown","32c38426":"markdown","9c34b28c":"markdown","b570b6b7":"markdown","fcd25f23":"markdown","df3fa29e":"markdown","40123047":"markdown","bd5e6568":"markdown","443f71d9":"markdown","219e5366":"markdown","44e6afae":"markdown","5b12a006":"markdown","727fb3e2":"markdown","d2772c0c":"markdown","e5bef666":"markdown"},"source":{"3cc28650":"import time\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ntrain = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ndisplay(train)\n\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\ndisplay(test)","4fc0cdb3":"from gensim.parsing.preprocessing import stem_text\n\ndef clean_location_keyword(df):\n    df[\"location\"] = df[\"location\"].astype(\"string\").str.lower()\n    df[\"location\"].fillna(\"\", inplace=True)\n    df[\"keyword\"] = df[\"keyword\"].astype(\"string\").str.lower()\n    df[\"keyword\"].replace(regex=r\"\\%20\", value=\" \", inplace=True)\n    df[\"keyword\"].fillna(\"\", inplace=True)\n    df[\"keyword\"] = df[\"keyword\"].apply(stem_text)\n\nclean_location_keyword(train)\nclean_location_keyword(test)\ntrain","45cfb1dc":"counts = pd.DataFrame(train[\"target\"].value_counts())\ncounts.rename(columns={\"target\": \"Samples\"}, index={0: \"Not Real\", 1: \"Real\"}, inplace=True)\nax = sns.barplot(x=counts.index, y=counts.Samples)\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(\n        x=p.get_x()+(p.get_width()\/2),\n        y=height,\n        s=round(height),\n        ha=\"center\"\n    )","1a5c62c4":"with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n    display(pd.DataFrame(data=train[[\"id\", \"keyword\", \"target\"]].groupby([\"keyword\", \"target\"]).count()).rename(columns={\"id\": \"count\"}).head(50))","4024f0ea":"train[\"keyword\"] = train[\"keyword\"].astype(\"category\")\ntrain[\"keyword_cat\"] = train[\"keyword\"].cat.codes\ntest[\"keyword_cat\"] = test[\"keyword\"].apply(lambda x: train[\"keyword\"].cat.categories.get_loc(x) if x in train[\"keyword\"].cat.categories.to_list() else 0)\n\ntrain","bba9f157":"print([location for location in train[\"location\"]][:100])","278a5265":"from pycountry import subdivisions\n\ndef clean_state_country(df):\n    subs = [subdivision.name.lower() for subdivision in subdivisions]\n    countries = [subdivision.country_code for subdivision in subdivisions]\n    country = []\n    state = []\n    for _, row in df.iterrows():\n        match_found = False\n        location_lower = row[\"location\"].lower()\n        for index, subdivision in enumerate(subs):\n            if subdivision in location_lower:\n                country.append(countries[index])\n                state.append(subdivision)\n                match_found = True\n                break\n        if not match_found:\n            country.append(\"\")\n            state.append(\"\")\n    df[\"country\"] = country\n    df[\"state\"] = state\n    \nclean_state_country(train)\nclean_state_country(test)\n\ntrain[\"country\"] = train[\"country\"].astype(\"category\")\ntrain[\"country_cat\"] = train[\"country\"].cat.codes\ntrain[\"state\"] = train[\"state\"].astype(\"category\")\ntrain[\"state_cat\"] = train[\"state\"].cat.codes\n\n# Apply the same category codes to the testing dataset\ntest[\"state_cat\"] = test[\"state\"].apply(lambda x: train[\"state\"].cat.categories.get_loc(x) if x in train[\"state\"].cat.categories.to_list() else 0)\ntest[\"country_cat\"] = test[\"country\"].apply(lambda x: train[\"country\"].cat.categories.get_loc(x) if x in train[\"country\"].cat.categories.to_list() else 0)","4a004a9b":"with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n    display(pd.DataFrame(data=train[[\"id\", \"country\", \"target\"]].groupby([\"country\", \"target\"]).count()).rename(columns={\"id\": \"count\"}).head(50))","b229eb0c":"with pd.option_context(\"display.max_rows\", None, \"display.max_columns\", None):\n    display(pd.DataFrame(data=train[[\"id\", \"state\", \"target\"]].groupby([\"state\", \"target\"]).count()).rename(columns={\"id\": \"count\"}).head(50))","049bdc14":"import re\n\ndef engineer_features(df):\n    df[\"total_length\"] = df[\"text\"].apply(len)\n    df[\"avg_word_length\"] = df[\"text\"].apply(lambda x: round(sum(len(word) for word in x.split()) \/ len(x.split())))\n    df[\"num_ats\"] = df[\"text\"].apply(lambda x: x.count(\"@\"))\n    df[\"num_hashtags\"] = df[\"text\"].apply(lambda x: x.count(\"#\"))\n    df[\"num_numeric\"] = df[\"text\"].apply(lambda x: len(re.findall(r\"\\w[0-9,]+\\w\", x)))\n    df[\"num_urls\"] = df[\"text\"].apply(lambda x: x.count(\"http\"))\n    df[\"num_timestamps\"] = df[\"text\"].apply(lambda x: len(re.findall(r\"[0-9]+:[0-9]+\", x)))\n\nengineer_features(train)\nengineer_features(test)\ntrain","ff1a4b10":"from gensim.parsing.preprocessing import remove_stopwords, strip_punctuation, strip_multiple_whitespaces, strip_numeric\n\ndef normalize_text(df):\n    normalized_text = []\n\n    for _, row in df.iterrows():\n        new_text = remove_stopwords(row[\"text\"].lower())\n        new_text = re.sub(r\"http\\S+\", \"\", new_text)\n        new_text = strip_punctuation(new_text)\n        new_text = strip_numeric(new_text)\n        new_text = strip_multiple_whitespaces(new_text)\n        normalized_text.append(new_text.strip())\n\n    df[\"normalized_text\"] = normalized_text\n\nnormalize_text(train)\nnormalize_text(test)\ntrain","a50c5107":"import gc\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import SGDRegressor\n\nvectorizer = TfidfVectorizer()\nskf = StratifiedKFold(n_splits=5, random_state=2020, shuffle=True)\n\nfeatures = [\n    \"keyword_cat\", \"country_cat\", \"state_cat\", \"total_length\", \"avg_word_length\", \n    \"num_ats\", \"num_hashtags\", \"num_numeric\", \"num_urls\", \"num_timestamps\"\n]\n\nparams = {\n    \"learning_rate\": 0.01, \n    \"verbose\": 100, \n    \"random_state\": 2020, \n    \"metric\": \"average_precision\", \n    \"verbose\": -1,\n    \"boosting_type\": \"dart\",\n    \"num_leaves\": 20,\n    \"num_iterations\": 200,\n    \"cat_column\": \"0,1,2\", # array features 0, 1, and 2 are categorical\n}\n\nimportances = pd.DataFrame()\nbest_score = 0.0\nbest_model = None\nbest_sgd = None\n\nfor fold, (train_index, test_index) in enumerate(skf.split(train, train[\"target\"])):\n    print(\"-------> fold {} <--------\".format(fold + 1))\n    print(\": Build regression model\")\n    x_train, x_valid = pd.DataFrame(train.iloc[train_index]), pd.DataFrame(train.iloc[test_index])\n    y_train, y_valid = train[\"target\"].iloc[train_index], train[\"target\"].iloc[test_index]\n    \n    x_train_tfidf = vectorizer.fit_transform(x_train[\"normalized_text\"]).astype(np.float32)\n    x_valid_tfidf = vectorizer.transform(x_valid[\"normalized_text\"]).astype(np.float32)\n    \n    sgd_classifier = SGDRegressor(alpha=0.001, penalty=\"l2\", loss=\"squared_epsilon_insensitive\", random_state=2020)\n    sgd_classifier.fit(x_train_tfidf, y_train)\n    \n    x_train_features = pd.DataFrame(x_train[features])\n    x_train_features[\"sgd_score\"] = sgd_classifier.predict(x_train_tfidf)\n    x_valid_features = pd.DataFrame(x_valid[features])\n    x_valid_features[\"sgd_score\"] = sgd_classifier.predict(x_valid_tfidf)\n    \n    print(\": Build GBM model\")\n    model = lgb.LGBMClassifier(\n        **params,\n    )\n    model.fit(\n        x_train_features, \n        y_train,\n        eval_set=[(x_valid_features, y_valid)],\n        verbose=100,\n    )\n\n    train_predictions = model.predict(x_valid_features)\n    \n    imp_df = pd.DataFrame()\n    imp_df['feature'] = x_valid_features.columns\n    imp_df['gain'] = model.booster_.feature_importance(importance_type='gain')\n    imp_df['fold'] = fold + 1\n    importances = pd.concat([importances, imp_df], axis=0, sort=False)\n    print(classification_report(y_valid, train_predictions, target_names=[\"Not Real\", \"Real\"]))\n    score = model.score(x_valid_features, y_valid)\n    if score > best_score:\n        print(\"--> This model is the best so far {:0.5}\".format(score))\n        best_model = model\n        best_score = score\n        best_sgd = sgd_classifier\n        # Best is 0.79448 - alpha=0.001, loss=squared_epsilon_insensitive, penalty=l2 (lgbm boosting=dart, learning_rate=0.01, num_leaves=20, num_iterations=200)","c212c9b4":"import warnings\nwarnings.simplefilter('ignore', FutureWarning)\n\nimportances['gain_log'] = np.log1p(importances['gain'])\nmean_gain = importances[['gain', 'feature']].groupby('feature').mean()\nimportances['mean_gain'] = importances['feature'].map(mean_gain['gain'])\n\nplt.figure(figsize=(14, 35))\n_ = sns.barplot(x='gain_log', y='feature', data=importances.sort_values('mean_gain', ascending=False))","d8277b5c":"vectorizer = TfidfVectorizer(binary=True)\n\nfeatures = [\n    \"keyword_cat\", \"country_cat\", \"state_cat\", \"total_length\", \"avg_word_length\", \n    \"num_ats\", \"num_hashtags\", \"num_numeric\", \"num_urls\", \"num_timestamps\"\n]\n\nparams = {\n    \"learning_rate\": 0.01, \n    \"verbose\": 100, \n    \"random_state\": 2020, \n    \"metric\": \"average_precision\", \n    \"verbose\": -1,\n    \"boosting_type\": \"dart\",\n    \"num_leaves\": 20,\n    \"num_iterations\": 200,\n    \"cat_column\": \"0,1,2\", # array features 0, 1, and 2 are categorical\n}\n\nprint(\": Build regression model\")\ntrain_tfidf = vectorizer.fit_transform(train[\"normalized_text\"]).astype(np.float32)\n\nsgd_classifier = SGDRegressor(alpha=0.001, penalty=\"l2\", loss=\"squared_epsilon_insensitive\", random_state=2020)\nsgd_classifier.fit(train_tfidf, train[\"target\"])\n\ntrain_features = pd.DataFrame(train[features])\ntrain_features[\"sgd_score\"] = sgd_classifier.predict(train_tfidf)\n\nprint(\": Build GBM model\")\nmodel = lgb.LGBMClassifier(\n    **params,\n)\nmodel.fit(\n    train_features, \n    train[\"target\"],\n    verbose=100,\n)","c330a134":"test_tfidf = vectorizer.transform(test[\"normalized_text\"]).astype(np.float32)\ntest_features = pd.DataFrame(test[features])\ntest_features[\"sgd_score\"] = sgd_classifier.predict(test_tfidf)\npredictions = model.predict(test_features)\nsubmission = pd.DataFrame({\"id\": test[\"id\"], \"target\": predictions})\nsubmission.to_csv(\"submission.csv\", index=False)","69aa25d3":"We can see that there are certain keywords that are strongly tied to one class. For example, `airplan accid` is very strongly associated with the real disaster target - we see it appear 35 times, and 30 of those times it is a real disaster, while only 5 times it is not. This is good news, since it suggests there are likely keywords here that will provide separation between classes.\n\nOnce last thing we will have to do with the keywords is categorize them. Most machine learning algorithms expect categorical data to be in the form of an integer. We'll convert the keyword into a category type, and save the category code as a new column, and use that for our machine learning technique.","77a9b12d":"There are quite a number of entries for which we have no country information. The first row shows us the empty string, which is the number of rows without country information. The total is 5,353 entries, which is more than half of our available training data. For entries with countries, we're seeing somewhat equal splits between real and not real disasters. This is to be expected, as people geotag tweets from all countries whether or not they are actually disasters. It's unlikely that real disasters would exclusively be geotagged. This is probably the same for state information. Let's take a look.","94306465":"In some instances we have countries, others include states, and some include cities. Yet others include junk data such as `global` as well as `Twitter Lockout in progress`. We'll need a way to clean and normalize this data so that it's a little more useful to us. Normalizing this data may turn out to be beneficial, but based on how messy the field is, it may not be worthwhile to spend huge amounts of time trying to clean it. As it stands, let's see if we can use the Python package `pycountry` to help us sort out some of this data.","ace4df31":"For this particular set of data, it looks like we have a slightly skewed distribution between the two classes. In this instance, we'll have to be careful with any machine learning algorithm we use, since we have more tweets that do not pertain to disasters than we do that contain real disasters. ","32c38426":"# 9. Building and Submitting the Final Model\n\nLet's go ahead and build a model that uses all of the data, and takes all the features we've examined so far. Once the model is built, we can submit the result.","9c34b28c":"# 5. Simple Feature Engineering\n\nBefore we look directly at the text as a feature, let's think about some of the other first-order information we can extract from it. Here are a few features that may be informative:\n\n* Total length of the text\n* Average word length\n* Number of `@` mentions\n* Number of hashtags\n* Number of numeric values in the text (excluding timestamps)\n* Number of URLs in the text\n* Number of timestamps in the text\n\nLet's go ahead and extract these fields.","b570b6b7":"As predicted, we're missing the same amount of state information. Looking at the low counts for each state, we're probably not going to get very useful information from this field, but we'll keep it intact for now.","fcd25f23":"# 1. Importing the Data\n\nThe first step in the process is to import our training data so we can see what kinds of information we have to work with. For this project, we'll start by importing the entire training dataset into a single Pandas dataframe.","df3fa29e":"Now that we have country and state information, we should be able to look at those fields the same way we examined keywords. Let's take a look what happens.","40123047":"# 6. Pre-processing Text\n\nFor our textual analysis to be useful, we'll have to perform some pre-processing on the text first to make it easier to work with. Here's what we're going to do:\n\n* Remove words that don't have any value, such as `the`, `of`, `and` (stopword removal)\n* We're going to strip out any links since they are `tco` encoded for Twitter\n* We'll remove all punctuation\n* We'll remove the numerics\n* We'll remove multiple whitespaces\n\nFor now we're not going to stem the text. Stemming is meant to collapse multiple morphological variants of the same word into a single stem (for example `dance`, `dancing`, `dances` could collapse to `danc`). We're not going to do that yet.","bd5e6568":"Here is the code to run the predictions on the test data, and build the submission file.","443f71d9":"# 2. Looking at Class Imbalance\n\nIt looks like we have 7,613 training samples. Let's see how many tweets we have that are examples of disaster versus those that are not. What we're looking at is whether or not we have a balance between samples that are both real examples of disasters, and those that are not.","219e5366":"# 8. Feature Performance\n\nWe can take a look and see how our various features are performing.","44e6afae":"# 7. Training and Validating the Classifier\n\nNow it's time to acutally build a classifier and see how well it does. To evaluate how well our features are working, we're going to split up our training data into a training set and a validation set. We'll do this 5 times for a 5-fold cross validation. Our difference between sets gives us an idea how robust our model actually is to variations in training data. To handle our textual data, we're going to treat it as a bag-of-words and use a regression model. To represent the words, we'll use a TF\/IDF vector (term frequency \/ inverse document frequency). Once we build the regressor, we'll use that to make predictions, and use the predicted outputs, along with our other features in a light GBM model to make actual predictions on the test data. ","5b12a006":"# 4. Looking at Location\n\nLet's take look at the first 100 entries in the location field and see what we're working with.","727fb3e2":"It looks like we need to do a little cleanup here. Both `keyword` and `location` fields are meant to be interpreted as strings. While we're at it, we should probably convert them all to lowercase for ease of processing. We'll also fill all missing values (`<NA>` values) in `keyword` and `location` with the empty string. If we look at the `keyword` strings, we find that some entries have `%20` instead of a space. We should also stem the `keyword` field so we can collapse similar keywords into a single keyword (for example, `death` and `deaths` would become `death`). Let's go ahead and make those changes to the dataframe. ","d2772c0c":"# Introduction\n\nThe *Real or Not? NLP with Disaster Tweets* competitions offers a neat opportunity to see how different approaches to natural language processing work when compared to one another. In this notebook, we'll look at how to start examining NLP data and performing some rudimentary second-order feature engineering. Here's a breakdown of what this notebook covers:\n\n1. Perform an initial exploration of some simple fields.\n2. Clean and normalize the data set.\n3. Extract first-order features and examine how useful they are.\n4. Perform rudimentary natural language processing on the text field.\n5. Evaluate our natural language model.\n6. Use the model and make predictions that we can submit to the competition.","e5bef666":"# 3. Looking at Keywords\n\nLet's take a closer look at what kind of information we have in the `keyword` field, grouped by whether their target is real `1` or not real `0`. Let's just take a look at the first 50 rows or so."}}