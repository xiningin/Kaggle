{"cell_type":{"d401ad16":"code","90fcd6a8":"code","cfc1652a":"code","f443f6c1":"code","9a4f71eb":"code","31fdba09":"code","f8ad904e":"code","9ce50af9":"code","b1379fbf":"code","ceeebf4b":"code","3aa56ce5":"code","ac8358ad":"code","c7a69b73":"code","61cfd932":"code","641a239c":"code","6fb9a5ca":"code","c51dbd71":"code","0f0320ee":"code","4d017ec2":"code","03a4d2fd":"code","a7d9962b":"code","6203c0cd":"code","26be27de":"markdown","c58dcb92":"markdown","597113b9":"markdown","fbb9cb29":"markdown","4a5e355a":"markdown","6c9a56d8":"markdown","74976616":"markdown","60fcd43f":"markdown","0727dc86":"markdown","637b62bc":"markdown","3c7110e2":"markdown","ea0aeafc":"markdown","fc10cc4f":"markdown","42991c5c":"markdown","f2dd24b4":"markdown"},"source":{"d401ad16":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, cross_val_predict\nfrom imblearn.over_sampling import SMOTENC \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, confusion_matrix, roc_curve, roc_auc_score,plot_confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nimport imblearn\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')","90fcd6a8":"data = pd.read_csv('..\/input\/diabetes-dataset-2019\/diabetes_dataset__2019.csv')\ndata.info()","cfc1652a":"for col in data.columns:\n    print(col)\n    print(data[col].value_counts())\n    print(f'-'*20)","f443f6c1":"data['RegularMedicine'].replace('o','no', inplace=True)\ndata['BPLevel'] = data['BPLevel'].str.lower().str.strip()\ndata['Pdiabetes'].replace('0', 'no', inplace=True)\ndata['Diabetic'] = data['Diabetic'].str.strip()\n# there is nan value at pregancies column where gender is male \n# if these values are replaced with 0, there's only 26 values, so all nan values will be replaced with 0. \ndata[data['Gender']=='Male']['Pregancies'].isna().sum()","9a4f71eb":"data['Pregancies'].replace(np.nan, 0, inplace=True)\n# will drop all na's \ndata.dropna(inplace=True)\ndata.info()","31fdba09":"num_cols = ['BMI', 'Sleep', 'SoundSleep', 'Pregancies']\ncategory_cols = list(set(data.columns).difference(set(num_cols)))\n\ndata_clean = pd.DataFrame()\nfor col in num_cols: \n    data_clean[col] = data[col].astype('int')\n\nfor col in category_cols: \n    data_clean[col] = data[col].astype('category')\n\n# categorical variables in pandas is a little tricky. \n# I want to order the categorical variables according to the risks. \ndata_clean['Age'] = pd.Categorical(data['Age'], ordered=True, \n                                   categories=['less than 40', '40-49', '50-59', '60 or older'])\ndata_clean['PhysicallyActive'] = pd.Categorical(data['PhysicallyActive'], ordered=True, \n                                                categories=['one hr or more', 'more than half an hr', 'less than half an hr', 'none'])\ndata_clean['JunkFood'] = pd.Categorical(data['JunkFood'], ordered=True, categories=['occasionally', 'often', 'very often', 'always'])\ndata_clean['BPLevel'] = pd.Categorical(data['BPLevel'], ordered=True, \n                                       categories=['low', 'normal', 'high'])\ndata_clean['Stress'] = pd.Categorical(data['Stress'], ordered=True, \n                                      categories=['not at all', 'sometimes', 'very often', 'always'])\n\n# sklearn cannot map according to this order and so do it manually. \ncategory_mapping = {\n    'Age':{'less than 40':0, '40-49':1, '50-59':2, '60 or older':3},\n    'Family_Diabetes':{'no':0, 'yes':1},\n    'Gender':{'Female':0, 'Male':1},\n    'Smoking':{'no':0, 'yes':1},\n    'Pdiabetes':{'no':0, 'yes':1},\n    'RegularMedicine':{'no':0, 'yes':1},\n    'PhysicallyActive':{'one hr or more':0, 'more than half an hr':1, 'less than half an hr':2, 'none':3},\n    'JunkFood':{'occasionally':0, 'often':1, 'very often':2, 'always':3},\n    'BPLevel':{'low':0, 'normal':1, 'high':2},\n    'highBP':{'no':0, 'yes':1},\n    'Alcohol':{'no':0, 'yes':1},\n    'UriationFreq':{'not much':0, 'quite often':1},\n    'Stress':{'not at all':0, 'sometimes':1, 'very often':2, 'always':3},\n    'Diabetic':{'no':0, 'yes':1},\n}\n\nfor col in category_cols:\n    data_clean[col] = data_clean[col].map(category_mapping[col])","f8ad904e":"data_clean.head()","9ce50af9":"data_clean['Diabetic'].value_counts()","b1379fbf":"# split the data \nX = data_clean.drop('Diabetic', axis=1)\ny = data_clean['Diabetic']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y)","ceeebf4b":"print(y_train.value_counts())\nprint(y_test.value_counts())","3aa56ce5":"# The data is imbalanced and I will use smote\ndef smote_data(X, y):\n    smotenc = SMOTENC(random_state = 123, categorical_features = list(range(4, 17)), n_jobs=-1)\n    X_smote, y_smote = smotenc.fit_resample(X, y)\n    return X_smote, y_smote \n\nX_train_smote, y_train_smote = smote_data(X_train, y_train)\nX_test_smote, y_test_smote = smote_data(X_test, y_test)","ac8358ad":"print(y_train_smote.value_counts())\nprint(y_test_smote.value_counts())","c7a69b73":"def grid_search(X_tr, X_te, y_tr, y_te, model, params, scoring='recall'):\n    gs = GridSearchCV(estimator = model, param_grid = params, scoring = scoring, n_jobs=-1, cv=3)\n    gs.fit(X_tr, y_tr)\n    y_pred = gs.predict(X_te)\n    print(f\"{model}\")\n    print(f\"Best parameter      : {gs.best_params_}\")\n    print(f\"Test Accuracy Score : {accuracy_score(y_te, y_pred)}\")\n    print(f\"Train Accuracy Score: {accuracy_score(y_tr, gs.predict(X_tr))}\")\n    print(f\"Recall score        : {recall_score(y_te, y_pred)}\")\n    print(f\"Classification Report \\n{'-'*30}\\n {classification_report(y_te, y_pred)}\")\n    return gs.best_params_","61cfd932":"params = {\n    'C' : [0.00001, 0.0001, 0.001, 0.01, 0.1, 1, 10],\n    'penalty' : ['l1', 'l2', 'elasticnet', 'none']\n}\nmodel = LogisticRegression(max_iter=500)\nlr_best = grid_search(X_train_smote, X_test, y_train_smote, y_test, model, params, scoring='accuracy')","641a239c":"ada = AdaBoostClassifier()\nparams = {\n    'n_estimators' : [100,200, 300, 400],\n    'learning_rate' : [0.0001, 0.001,0.1,1,10]\n}\nada_best = grid_search(X_train_smote, X_test, y_train_smote, y_test, ada, params, scoring='accuracy')","6fb9a5ca":"params = {\n    'n_estimators':[100, 200, 300, 400, 500, 600],\n    'criterion' : ['gini', 'entropy'],\n    'max_depth' : [5, 10, 15, 20],\n}\nmodel = RandomForestClassifier(random_state=123)\nrf_best = grid_search(X_train_smote, X_test, y_train_smote, y_test, model, params, scoring='accuracy')","c51dbd71":"model = SVC(random_state=123)\nparams = {\n    'C' : [0.001, 0.01, 0.1, 1, 10],\n    'kernel' : ['linear', 'poly', 'rbf', 'sigmoid'],\n    'degree' : [2, 3, 4, 5]\n}\nsvc_best = grid_search(X_train_smote, X_test, y_train_smote, y_test, model, params, scoring='accuracy')","0f0320ee":"# model_lr = LogisticRegression(C=lr_best['C'], penalty=lr_best['penalty'], random_state=123)\n# y_scores_lr = cross_val_predict(model_lr, X_train_smote, y_train_smote, cv=5, method='predict_proba')\n\nmodel_ada = AdaBoostClassifier(n_estimators=ada_best['n_estimators'], learning_rate=ada_best['learning_rate'], random_state=123)\ny_scores_ada = cross_val_predict(model_ada, X_train_smote, y_train_smote, cv=5, method='predict_proba')\n\nmodel_rf = RandomForestClassifier(n_estimators=rf_best['n_estimators'], criterion=rf_best['criterion'], max_depth=rf_best['max_depth'], \n                                  random_state=123)\ny_scores_rf = cross_val_predict(model_rf, X_train_smote, y_train_smote, cv=5, method='predict_proba')\n\n\nmodel_svc = SVC(C=svc_best['C'], degree=svc_best['degree'], kernel=svc_best['kernel'], probability=True)\ny_scores_svc = cross_val_predict(model_svc, X_train_smote, y_train_smote, cv=5, method='predict_proba')\n\ny_scores = [y_scores_ada, y_scores_rf, y_scores_svc]\nmodel_names = ['AdaBoost Classification', 'Random Forest Classification', 'Support Vector Classifier']","4d017ec2":"plt.figure(figsize=(7,7))\nfor score, name in zip(y_scores, model_names):\n    roc_score = roc_auc_score(y_train_smote, score[:, 1])\n    fpr, tpr, threshold = roc_curve(y_train_smote, score[:, 1])\n    plt.plot(fpr, tpr, label=f\"{name} : {round(roc_score, 3)}\")\nplt.legend()\nplt.plot([0,1],[0,1], '--', linewidth=2)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Comparing roc_auc in 4 models');","03a4d2fd":"rf = RandomForestClassifier(n_estimators=rf_best['n_estimators'], criterion=rf_best['criterion'], max_depth=rf_best['max_depth'], \n                                  random_state=123)\nrf.fit(X_train_smote, y_train_smote)","a7d9962b":"plot_confusion_matrix(rf, X_test_smote, y_test_smote)\nplt.show()","6203c0cd":"features = pd.Series(rf.feature_importances_, index=X_train_smote.columns).sort_values(ascending=False)\nplt.figure(figsize=(6,8))\ng = sns.barplot(features, features.index)\nplt.title(\"Important features according to Random Forest\");","26be27de":"### Support Vector Classifier","c58dcb92":"Random Forest Classifier is the best model for this dataset. ","597113b9":"## Importing and Data Preprocessing","fbb9cb29":"## Model Comparing","4a5e355a":"I will correct the typo-errors manually. ","6c9a56d8":"# About this file\n## Context\nThis dataset was collected by Neha Prerna Tigga and Dr. Shruti Garg of the Department of Computer Science and Engineering, BIT Mesra, Ranchi-835215, for research, non-commercial purposes only. This diabetes database was collected as a part of a research project.\n\n## Content\nThere are 17 independent predictor variables and one binary dependent or target variable, Diabetes.","74976616":"## Model Training","60fcd43f":"I don't like the most important feature is `RegularMedicine` as if a person has diabetes, he will surely take medicine and there is a strong relation between regular medicine and diabetes. The next important features are age, BMI, BP level, stress and sound sleep. We cannot avoid the older age. We can adjust the BMI by taking exercises not to gain weight as if weight increases, BMI will increase. \nThanks a lot for your time. I am a begineer and this is my first notebook in Kaggle platform. ","0727dc86":"### Logistic Regression","637b62bc":"### AdaBoost Classifier","3c7110e2":"The data is imbalanced and so, let's balance with `SMOTE`. I think if the data is imbalanced, it effects the recall score. For this target variable, `Diabetes`, I think recall score is important as it is dangerous if it misclassified the actual diabetes as not having diabetes.","ea0aeafc":"Please suggest the categorical mapping from above, is there any function that operates like that?","fc10cc4f":"Define the function to avoid repetitions. ","42991c5c":"### Random Forest Classifer","f2dd24b4":"In this data, the variables `Age, Gender, Family-diabetes, highBP, PhysicallyActive, Smoking, Alcohol, RegularMedicine, JunkFood, Stress, BPLevel, Pdiabetes, UriationFreq` and the target variable `Diabetic` will be treated as categorical variables.  "}}