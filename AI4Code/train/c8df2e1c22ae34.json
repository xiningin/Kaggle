{"cell_type":{"e80869c6":"code","8db3a619":"code","cbd0eb35":"code","d4db2db6":"code","aac4b30d":"code","3d006161":"code","4743d356":"code","a95e0a58":"code","11fc214c":"code","3d582595":"code","399a8145":"code","b9351590":"code","ccfa8316":"code","aca47cf8":"code","88ac2101":"code","a902f2f1":"code","07bc8cdb":"code","022a4baa":"code","b0e0f3b0":"code","4b5b4904":"code","9e367a43":"code","6f6fc47a":"code","afdb44e8":"code","ee184eb8":"code","5a6cb61d":"code","7eb0849b":"code","9aadebd6":"code","7d3fc26c":"code","1a8bc427":"code","f6f83268":"code","c54290b3":"code","0a3bd5e6":"code","ed059c26":"code","d514e744":"code","8876d407":"code","c86b877f":"code","3a384ef9":"code","f0e71df4":"code","cf1709da":"code","daf10aed":"code","3ac167aa":"code","6cfeaf99":"code","4c3f7f95":"code","61b06203":"code","a4cbf77a":"markdown","dd962deb":"markdown","28d2a8bb":"markdown","81216a44":"markdown","95676f1f":"markdown","1482897f":"markdown","9584cee6":"markdown","23c49fb5":"markdown","12268fd7":"markdown","19168178":"markdown","ce825972":"markdown","2da321f6":"markdown","289576c7":"markdown","6db72bce":"markdown","bff74b58":"markdown","5f647d83":"markdown","06c57fcf":"markdown","5b7bbbab":"markdown","aaf5e7cc":"markdown","065ae1dd":"markdown","3dc073ae":"markdown","b9f33678":"markdown","53138f73":"markdown","f0014444":"markdown","f07c2851":"markdown","49704299":"markdown","7adad5c6":"markdown","93fc3af2":"markdown","ca22df31":"markdown","ba3fadb6":"markdown","d0934108":"markdown","4f438b51":"markdown","ff03f4a4":"markdown","6c5ea339":"markdown","c728b892":"markdown","6957beea":"markdown","50d57ba0":"markdown","407c886f":"markdown","573a14d3":"markdown","d99f8486":"markdown"},"source":{"e80869c6":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.preprocessing import MinMaxScaler\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\nfrom numpy import savetxt\nfrom scipy import stats\nfrom keras import Sequential\nfrom keras.layers.core import Dense, Dropout\nimport tensorflow as tf\nfrom keras.wrappers.scikit_learn import KerasClassifier","8db3a619":"df_train = pd.read_csv(\"\/kaggle\/input\/les-variables-des-images-de-publicit\/data_train.csv\", sep='\\t') #dataframe object","cbd0eb35":"print(df_train.shape)","d4db2db6":"# Select duplicate rows except first occurrence based on all columns:\ndf_duplicateRows = df_train[df_train.duplicated()]\nprint(\"Duplicate rows:\", df_duplicateRows)\nprint(df_duplicateRows.shape[0]) # 562 rows \/ 2459 rows\n\n# Remove duplicate rows:\ndf_train = df_train.drop_duplicates() # 2459 rows - 562 rows = 1897 rows\n# print(df_train.shape)","aac4b30d":"df_train_tr= df_train.transpose()\ndf_tr_duplicateCols = df_train_tr[df_train_tr.duplicated()]\n# print(\"Duplicate columns:\", df_duplicateCols)\nprint(df_tr_duplicateCols.shape[0]) # 820 cols \/ 1559 cols","3d006161":"# Remove duplicate columns:\ndf_train_tr = df_train_tr.drop_duplicates() # 1559 cols - 820 rows = 739 cols\ndf_train = df_train_tr.transpose()\nprint(df_train.shape)","4743d356":"# get the number of missing data points per column\nmissing_values_count = df_train.isna().sum()\nprint(\"missing values count:\\n\", missing_values_count)\n\n# how many total missing values do we have?\ntotal_cells = np.product(df_train.shape)\ntotal_missing = missing_values_count.sum()\n\nprint(\"total_cells:\", total_cells)\nprint(\"total_missing:\",total_missing)","a95e0a58":"median_x1 = df_train['X1'].median()\nmedian_x2 = df_train['X2'].median()\nmedian_x3 = df_train['X3'].median()\nmedian_x4 = df_train['X4'].median()\n\n# print(\"median_x1:\", median_x1) # 60.0\n# print(\"median_x2:\", median_x2) # 114.0\n# print(\"median_x3:\", median_x3) # 2.2708\n# print(\"median_x4:\", median_x4) # 1.0\n\ndf_train['X1'].fillna(median_x1, inplace=True)\ndf_train['X2'].fillna(median_x2, inplace=True)\ndf_train['X3'].fillna(median_x3, inplace=True)\ndf_train['X4'].fillna(median_x4, inplace=True)","11fc214c":"df_test = pd.read_csv(\"\/kaggle\/input\/les-variables-des-images-de-publicit\/data_test.csv\", sep='\\t') ","3d582595":"df_nans = df_test[df_test.isnull().any(axis=1)] # 394 rows have one or more NaN values\nprint(df_nans)\n\nmedian_x1_t = df_test['X1'].median()\nmedian_x2_t = df_test['X2'].median()\nmedian_x3_t = df_test['X3'].median()\nmedian_x4_t = df_test['X4'].median()\n\n# print(\"median_x1:\", median_x1_t) \n# print(\"median_x2:\", median_x2_t) \n# print(\"median_x3:\", median_x3_t) \n# print(\"median_x4:\", median_x4_t) \n\ndf_test['X1'].fillna(median_x1_t, inplace=True)\ndf_test['X2'].fillna(median_x2_t, inplace=True)\ndf_test['X3'].fillna(median_x3_t, inplace=True)\ndf_test['X4'].fillna(median_x4_t, inplace=True)","399a8145":"df_test.head() ","b9351590":"df_train_fl = df_train.iloc[: , :4]\n# print(\"df_pubData_Train_fl:\\n\", df_train_fl.head())\n\ndf_train_bool = df_train.iloc[: , 4:]\n# print(\"boolean dataframe:\\n\", df_train_bool.head())","ccfa8316":"df_test_fl = df_test.iloc[: , :4]\n# print(\"df_pubData_Train_fl:\\n\", df_test_fl.head())\n\ndf_test_bool = df_test.iloc[: , 4:]\n# print(\"boolean dataframe:\\n\",df_test_bool.head())","aca47cf8":"# X1_column = df_test_fl[\"X1\"]\n# X2_column = df_test_fl[\"X2\"]\n# X3_column = df_test_fl[\"X3\"]\n\n# train_fl_columns = [X1_column, X2_column, X3_column]\n\n# fig, ax = plt.subplots()\n# ax.boxplot(train_fl_columns)\n# plt.show()","88ac2101":"''' we better NOT remove the outliers of the test dataset '''\n# print(df_test_fl.shape)\n# df_test_fl_filtered = df_test_fl[(np.abs(stats.zscore(df_test_fl)) < 3).all(axis=1)] # \"all(axis=1)\" ensures that all column satisfy the constraint.\n# print(df_test_fl_filtered.shape)","a902f2f1":"print(df_train_fl.shape)\ndf_train_fl_filtered = df_train_fl[(np.abs(stats.zscore(df_train_fl)) < 3).all(axis=1)]\nprint(df_train_fl_filtered.shape)","07bc8cdb":"# https:\/\/stackoverflow.com\/questions\/23199796\/detect-and-exclude-outliers-in-pandas-data-frame\n    \n# X1_filtered = X1_column[X1_column.between(X1_column.quantile(.05), X1_column.quantile(.95))]\n# print(X1_filtered.shape)\n# X1_filtered.plot.box()\n\n# X2_filtered = X2_column[X2_column.between(X2_column.quantile(.05), X2_column.quantile(.95))]\n# print(X2_filtered.shape)\n# X2_filtered.plot.box()\n\n# X3_filtered = X3_column[X3_column.between(X3_column.quantile(.05), X3_column.quantile(.95))]\n# print(X3_filtered.shape)\n# X3_filtered.plot.box()","022a4baa":"# define min max scaler\nscaler = MinMaxScaler(feature_range=(0,1))\n\ncols = [\"X1\", \"X2\", \"X3\", \"X4\"]\n# transform train data\ndf_train_fl = pd.DataFrame(scaler.fit_transform(df_train_fl), columns = cols)\n# transform test data\ndf_test_fl = pd.DataFrame(scaler.fit_transform(df_test_fl), columns = cols)","b0e0f3b0":"label_train = df_train_bool.iloc[: , -1:] \n# label_train.shape # (1895, 1)\nprint(label_train.value_counts()) # 0: 1585, 1: 312\n\n# drop the \"outcome\" column (binary label column):\ndf_train_bool = df_train_bool.iloc[: , :-1] \ndf_train_bool.head()\n# df_pubData_bool.shape # (1895, 1556)","4b5b4904":"# Create and run an TSVD with one less than number of features\ntsvd = TruncatedSVD(n_components=df_train_bool.shape[1]-1)\nX_tsvd = tsvd.fit(df_train_bool)\n\n# List of explained variances\ntsvd_var_ratios = tsvd.explained_variance_ratio_\n\n# Calculating number of components required to pass threshold\ndef select_n_components(var_ratio, goal_var: float) -> int:\n\n    total_variance = 0.0\n    n_components = 0\n    \n    for explained_variance in var_ratio:\n        # Add the explained variance to the total\n        total_variance += explained_variance\n        # Add one to the number of components\n        n_components += 1\n        \n        if total_variance >= goal_var:\n            break\n            \n    return n_components\n\nselect_n_components(tsvd_var_ratios, 0.95) # 290","9e367a43":"# Create and run an TSVD with one less than number of features\ntsvd = TruncatedSVD(n_components=df_test_bool.shape[1]-1)\nX_tsvd = tsvd.fit(df_test_bool)\n\n# List of explained variances\ntsvd_var_ratios = tsvd.explained_variance_ratio_\n\n# Calculating number of components required to pass threshold\ndef select_n_components(var_ratio, goal_var: float) -> int:\n\n    total_variance = 0.0\n    n_components = 0\n    \n    for explained_variance in var_ratio:\n        # Add the explained variance to the total\n        total_variance += explained_variance\n        # Add one to the number of components\n        n_components += 1\n        \n        if total_variance >= goal_var:\n            break\n            \n    return n_components\n\nselect_n_components(tsvd_var_ratios, 0.95) # 206","6f6fc47a":"SVD_model = TruncatedSVD(n_components=290).fit(df_train_bool)\nprint(df_train_bool.shape)\n\ndf_train_bool_reduced = pd.DataFrame(SVD_model.transform(df_train_bool)) # pd.DataFrame is used to retain as data frame object\nprint(df_train_bool_reduced.shape)\n\n# apply same transformation to df_test to boolean:\n\nSVD_model = TruncatedSVD(n_components=206).fit(df_test_bool)\nprint(df_test_bool.shape)\n\ndf_test_bool_reduced = pd.DataFrame(SVD_model.transform(df_test_bool))\nprint(df_test_bool_reduced.shape)","afdb44e8":"# df_test_bool_reduced.head()","ee184eb8":"# indexes are not matching after preprocessing so we need to drop:\ndf_test_bool_reduced.reset_index(drop=True, inplace=True)\ndf_test_fl.reset_index(drop=True, inplace=True)\n\n# concatenation of boolean with numeric test dataframes:\nx_test = pd.concat( [df_test_fl, df_test_bool_reduced], axis=1 )\nprint((x_test.shape)) # (820, 27)\nx_test.head()","5a6cb61d":"target_count = df_train.outcome.value_counts()\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ntarget_count.plot(kind='bar', title='Count (target)')","7eb0849b":"# oversample = RandomOverSampler(sampling_strategy='minority')\n# x_over, y_over = oversample.fit_resample(df_train_bool, label_train)\n\n# sm = SMOTE(random_state = 42)\n# x_sm, y_sm = sm.fit_resample(df_train_bool, label_train)\n\n# sm = ADASYN()\n# x_syn, y_syn = sm.fit_resample(df_train_bool, label_train)\n\n# # print(\"sizes before:\", df_train_bool.shape, label_train.shape)\n# # print(\"sizes after RandomOverSampler resampling:\",x_over.shape, y_over.shape)\n# # print(\"sizes after SMOTE resampling:\",x_sm.shape, y_sm.shape)\n# # print(\"sizes after ADASYN resampling:\",x_syn.shape, y_syn.shape)","9aadebd6":"# # define the pipeline\n# steps = [('svd', TruncatedSVD(n_components=272)), ('m', LogisticRegression())] \n# model = Pipeline(steps=steps)\n\n# # evaluate model\n# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)\n\n# y_pred_ROS = cross_val_predict(model, x_over, y_over, cv=cv, n_jobs=-1) # 0.97 recall for nonad\n# print(\"classification report after RandomOverSampler:\\n\", classification_report(y_over, y_pred_ROS))\n\n# y_pred_sm = cross_val_predict(model, x_sm, y_sm, cv=cv, n_jobs=-1)\n# print(\"classification report after SMOTE:\\n\", classification_report(y_sm, y_pred_sm))\n\n# y_pred_syn = cross_val_predict(model, x_syn, y_syn, cv=cv, n_jobs=-1)\n# print(\"classification report after ADASYN:\\n\", classification_report(y_syn, y_pred_syn))","7d3fc26c":"oversample = RandomOverSampler(sampling_strategy='minority')\n# oversampling of boolean dataframe:\nx_over_bool, y_over = oversample.fit_resample(df_train_bool_reduced, label_train) # df_train_bool_reduced before oversampling: (1897, 273), label_train: (1897, 1)\nprint((x_over_bool.shape), (y_over.shape))","1a8bc427":"# oversampling of floating dataframe:\nx_over_fl, y_over = oversample.fit_resample(df_train_fl, label_train)\nprint((x_over_fl.shape), (y_over.shape))","f6f83268":"# concatenation of boolean with numeric training dataframes:\nx_over = pd.concat( [x_over_fl, x_over_bool], axis=1 )\n# print((x_over.shape))\nx_over.head()","c54290b3":"cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)\n\nmodel = LogisticRegression()\ny_pred_logr = cross_val_predict(model, x_over, y_over, cv=cv, n_jobs=-1) \nprint(\"classification report for logistic regression classifier:\\n\", classification_report(y_over, y_pred_logr))","0a3bd5e6":"model = DecisionTreeClassifier()\ny_pred_dt = cross_val_predict(model, x_over, y_over, cv=cv, n_jobs=-1) \nprint(\"classification report for decision tree classifier:\\n\", classification_report(y_over, y_pred_dt))","ed059c26":"print(x_over.shape)\nprint(y_over.shape)","d514e744":"logreg_model = LogisticRegression()\nlogreg_model.fit(x_over, y_over)","8876d407":"y_pred = logreg_model.predict(x_test) # output: numpy.ndarray","c86b877f":"# # save to csv file\n# savetxt('y_pred.csv', y_pred, delimiter=',', fmt=('%s'))","3a384ef9":"# training dataset: x_over, y_over\n# testing dataset: x_test\n\n# transform label dataset from string to integer https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelBinarizer.html \nfrom sklearn import preprocessing\nlb = preprocessing.LabelBinarizer()\ny_over = lb.fit_transform(y_over) # 'numpy.ndarray' object ","f0e71df4":"# Split the data to get validation dataset:\nx_train, x_valid, y_train, y_valid = train_test_split(x_over, y_over, test_size=0.33, shuffle= True)\n# print(x_train.shape)\n# print(y_train.shape)\n# print(x_valid.shape)\n# print(y_valid.shape)","cf1709da":"# how to choose between sigmoid or relu activation function?\n#\"Relu is less susceptible to vanishing gradients that prevent deep models from being trained, although it can suffer from other problems like saturated or \u201cdead\u201d units.\"\n# https:\/\/machinelearningmastery.com\/choose-an-activation-function-for-deep-learning\/\n\n# Dropout regularization to prevent overfitting https:\/\/machinelearningmastery.com\/dropout-regularization-deep-learning-models-keras\/\n# \"You are likely to get better performance when dropout is used on a larger network, giving the model more of an opportunity to learn independent representations.\"\n\n# create a multilayer perceptron model with number of layers 7:\ndef create_model():\n    model = Sequential()\n    model.add(Dense(128, input_dim = x_train.shape[1], activation = 'sigmoid'))\n    model.add(Dropout(.2))\n    model.add(Dense(64, activation = 'relu'))\n    model.add(Dropout(.2))\n    model.add(Dense(32, activation = 'relu'))\n    model.add(Dropout(.2))\n    model.add(Dense(16, activation = 'relu'))\n    model.add(Dropout(.1))\n    model.add(Dense(8, activation = 'relu'))\n    model.add(Dense(2, activation = 'relu'))\n    model.add(Dense(1, activation = 'sigmoid'))\n\n# compile model:\n    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = [tf.keras.metrics.Recall(), tf.keras.metrics.Precision(), tf.keras.metrics.Accuracy()])\n    # Binary cross-entropy loss is used for binary (0 or 1) classification applications. \n    return model","daf10aed":"# evaluate model with standardized dataset\nkeras_model = KerasClassifier(build_fn = create_model, verbose=1) # \"verbose=2\" to see the training progress for each epoch.\n\n# fit the keras model on the dataset\nhistory = keras_model.fit(x_train, y_train, \n                    epochs=300, \n                    batch_size=50, \n                    validation_data=(x_valid, y_valid), \n                    shuffle=True)","3ac167aa":"# history_dict = history.history\n# print(history_dict.keys())","6cfeaf99":"# summarize history for accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","4c3f7f95":"y_pred = keras_model.predict(x_test) # output: numpy.ndarray\n# print(y_pred)","61b06203":"# reverse transform the prediction to string value:\ny_pred = lb.inverse_transform(y_pred, threshold=None)\n# print(y_pred)","a4cbf77a":"### **1.1. Preprocessing of training data**","dd962deb":"### **3. 1. Fitting a logistic regression model**","28d2a8bb":"#### Using the box plot below, we can easiliy visualize the outliers of the numerical columns as individual points.","81216a44":"#### Putting dataframes of train data together:","95676f1f":"#### The script below belongs to Chris Albon: https:\/\/chrisalbon.com\/machine_learning\/feature_engineering\/select_best_number_of_components_in_tsvd\/","1482897f":"#### Separating label column from training boolean dataframe:","9584cee6":"#### **1.1.2. Filling the missing values**","23c49fb5":"#### Fill the missing values in first three columns and with the median of these columns:","12268fd7":"#### **1.1.1. Removing the duplicate values**","19168178":"#### While applying dimensionality reduction, first we fit the model using training data, and then we can use to transform the training and validation data.\n#### (We can't apply oversampling to validation data.)","ce825972":"#### **1. 4. 1. SVD for the dimensionality reduction of sparse boolean features** \n\n#### \"Singular Value Decomposition, or SVD, is one of the most popular techniques for dimensionality reduction for sparse data (data with many zero values).\"\n#### https:\/\/machinelearningmastery.com\/dimensionality-reduction-algorithms-with-python\/\n#### https:\/\/machinelearningmastery.com\/singular-value-decomposition-for-dimensionality-reduction-in-python\/","2da321f6":"### **3. 2. Fitting a deep neural network model**","289576c7":"## **1. Data Preprocessing**","6db72bce":"#### **1. 4. 3. Applying SVD encoding to boolean dataframes:**","bff74b58":"#### \u00c0 chaque it\u00e9ration, cross_val_predict g\u00e9n\u00e9rera un score m\u00e9trique individuel pour ce lot. En fin de compte, il renverra k score pour chaque it\u00e9ration. ","5f647d83":"#### Ratio of the missing values:","06c57fcf":"#### The script below computes the Z-score of each value in the column, relative to the column mean and standard deviation.\n#### While calculating the Z-score, we re-scale and center the data and look for data points which are too far from zero. If the Z-score value is greater than or less than 3 or -3 respectively, that data point will be identified as outliers. ","5b7bbbab":"#### **1.2.1. Filling the missing values**","aaf5e7cc":"#### **1. 5. 2. Applying several oversampling methods and comparing with cross validation**","065ae1dd":"## **2. Evaluating classification models with 10-fold cross validation**","3dc073ae":"#### RandomOverSampler gives the highest recall and f1-score on boolean datasets, so it will be the sampling method to apply:","b9f33678":"### **1. 3. Normalization of numeric dataframes of training and test data**","53138f73":"#### **1. 5. 1. Visualizing the ratio of classes**","f0014444":"#### Dataframe sizes before oversampling: (1897, 290), (1897, 1) ; after oversampling: (3170, 290), (3170, 1)","f07c2851":"#### **1.3.1. Separating the numeric and boolean dataframes**","49704299":"#### In the above confusion matrices, both methods are resulting with same *average accuracy* score. Since our data has a bias for minority class \"nonad\", we need to compare *recall* and *precision* values to be sure that class is correctly classified. For the non-ad class, *recall* value returns 0.93 with the decision tree classifier, and 0.98 with the logistic regression classifier. Therefore, the logistic regression classifier outperforms the decision tree classifier for this dataset.","7adad5c6":"#### **For test boolean data:** ","93fc3af2":"#### **1. 3. 2. Removing outliers of numeric training dataframe**","ca22df31":"#### Putting dataframes of test data together:","ba3fadb6":"### **1. 5. Balancing the highly imbalanced dataset**","d0934108":"## **3. Predictions based on several models**","4f438b51":"#### **For training boolean data:**","ff03f4a4":"#### **To try in future:** We can model 2 different networks by using 2 different input sets we have (boolean and numeric) and then concatenate these at the end: \nhttps:\/\/www.pyimagesearch.com\/2019\/02\/04\/keras-multiple-inputs-and-mixed-data\/ ","6c5ea339":"### **1.2. Preprocessing of test data**","c728b892":"#### Creating two dataframes, as the first part is continuous variables (floating) and the second part is boolean variables :","6957beea":"#### **1. 3. 3. Scaling of numeric dataframes of training and test data**","50d57ba0":"#### Same division of dataframes for the test data:","407c886f":"#### Predicting the test set results:","573a14d3":"#### **1. 4. 2. Optimizing \"n_components\" parameter in TruncatedSVD**","d99f8486":"### **1. 4. Dimensionality reduction of training and test data**"}}