{"cell_type":{"05e73849":"code","1637321f":"code","aa7561a6":"code","89a7df86":"code","29c5eca1":"code","26276cb6":"code","4120e459":"code","9219f53c":"code","566c6a3b":"code","6cdda296":"code","82c624b8":"code","7d6755a8":"code","26684dfb":"code","3c391294":"code","00507cf4":"code","4ccc218c":"code","0bc5f07e":"markdown","54badf9e":"markdown","2a798578":"markdown","279c6a8a":"markdown","354fa258":"markdown","b92ef314":"markdown","3c96166d":"markdown","21971c89":"markdown","aaa5246e":"markdown","b6ba9044":"markdown","4e0353a4":"markdown","34c9e358":"markdown","dd61deeb":"markdown","1526d5f9":"markdown"},"source":{"05e73849":"from pathlib import Path #\u6587\u4ef6\u8def\u5f84\nimport pandas as pd\nimport torch\nfrom fastprogress import progress_bar #\u663e\u793a\u8fdb\u5ea6\u6761\nimport numpy as np\nimport warnings\nfrom collections import defaultdict\nfrom collections import Counter#\u8ba1\u6570\u5668","1637321f":"torch.manual_seed(42)##\u4e3aCPU\u8bbe\u7f6e\u79cd\u5b50\u7528\u4e8e\u751f\u6210\u968f\u673a\u6570\uff0c\u4ee5\u4f7f\u5f97\u7ed3\u679c\u662f\u786e\u5b9a\u7684\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nnp.random.seed(42)","aa7561a6":"ROOT = Path.cwd().parent #\u83b7\u53d6\u4e0a\u5c42\u76ee\u5f55\nINPUT_ROOT = ROOT \/ \"input\"\nRAW_DATA = INPUT_ROOT \/ \"birdsong-recognition\"\nTRAIN_AUDIO_DIR = RAW_DATA \/ \"train_audio\"\nTEST_AUDIO_DIR = RAW_DATA \/ \"test_audio\"\n\nif not TEST_AUDIO_DIR.exists():\n    TEST_AUDIO_DIR = INPUT_ROOT \/ \"birdcall-check\" \/ \"test_audio\"\n    test = pd.read_csv(INPUT_ROOT \/ \"birdcall-check\" \/ \"test.csv\")\nelse:\n    test = pd.read_csv(RAW_DATA \/ \"test.csv\")\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","89a7df86":"BIRD_CODE = {\n    'aldfly': 0, 'ameavo': 1, 'amebit': 2, 'amecro': 3, 'amegfi': 4,\n    'amekes': 5, 'amepip': 6, 'amered': 7, 'amerob': 8, 'amewig': 9,\n    'amewoo': 10, 'amtspa': 11, 'annhum': 12, 'astfly': 13, 'baisan': 14,\n    'baleag': 15, 'balori': 16, 'banswa': 17, 'barswa': 18, 'bawwar': 19,\n    'belkin1': 20, 'belspa2': 21, 'bewwre': 22, 'bkbcuc': 23, 'bkbmag1': 24,\n    'bkbwar': 25, 'bkcchi': 26, 'bkchum': 27, 'bkhgro': 28, 'bkpwar': 29,\n    'bktspa': 30, 'blkpho': 31, 'blugrb1': 32, 'blujay': 33, 'bnhcow': 34,\n    'boboli': 35, 'bongul': 36, 'brdowl': 37, 'brebla': 38, 'brespa': 39,\n    'brncre': 40, 'brnthr': 41, 'brthum': 42, 'brwhaw': 43, 'btbwar': 44,\n    'btnwar': 45, 'btywar': 46, 'buffle': 47, 'buggna': 48, 'buhvir': 49,\n    'bulori': 50, 'bushti': 51, 'buwtea': 52, 'buwwar': 53, 'cacwre': 54,\n    'calgul': 55, 'calqua': 56, 'camwar': 57, 'cangoo': 58, 'canwar': 59,\n    'canwre': 60, 'carwre': 61, 'casfin': 62, 'caster1': 63, 'casvir': 64,\n    'cedwax': 65, 'chispa': 66, 'chiswi': 67, 'chswar': 68, 'chukar': 69,\n    'clanut': 70, 'cliswa': 71, 'comgol': 72, 'comgra': 73, 'comloo': 74,\n    'commer': 75, 'comnig': 76, 'comrav': 77, 'comred': 78, 'comter': 79,\n    'comyel': 80, 'coohaw': 81, 'coshum': 82, 'cowscj1': 83, 'daejun': 84,\n    'doccor': 85, 'dowwoo': 86, 'dusfly': 87, 'eargre': 88, 'easblu': 89,\n    'easkin': 90, 'easmea': 91, 'easpho': 92, 'eastow': 93, 'eawpew': 94,\n    'eucdov': 95, 'eursta': 96, 'evegro': 97, 'fiespa': 98, 'fiscro': 99,\n    'foxspa': 100, 'gadwal': 101, 'gcrfin': 102, 'gnttow': 103, 'gnwtea': 104,\n    'gockin': 105, 'gocspa': 106, 'goleag': 107, 'grbher3': 108, 'grcfly': 109,\n    'greegr': 110, 'greroa': 111, 'greyel': 112, 'grhowl': 113, 'grnher': 114,\n    'grtgra': 115, 'grycat': 116, 'gryfly': 117, 'haiwoo': 118, 'hamfly': 119,\n    'hergul': 120, 'herthr': 121, 'hoomer': 122, 'hoowar': 123, 'horgre': 124,\n    'horlar': 125, 'houfin': 126, 'houspa': 127, 'houwre': 128, 'indbun': 129,\n    'juntit1': 130, 'killde': 131, 'labwoo': 132, 'larspa': 133, 'lazbun': 134,\n    'leabit': 135, 'leafly': 136, 'leasan': 137, 'lecthr': 138, 'lesgol': 139,\n    'lesnig': 140, 'lesyel': 141, 'lewwoo': 142, 'linspa': 143, 'lobcur': 144,\n    'lobdow': 145, 'logshr': 146, 'lotduc': 147, 'louwat': 148, 'macwar': 149,\n    'magwar': 150, 'mallar3': 151, 'marwre': 152, 'merlin': 153, 'moublu': 154,\n    'mouchi': 155, 'moudov': 156, 'norcar': 157, 'norfli': 158, 'norhar2': 159,\n    'normoc': 160, 'norpar': 161, 'norpin': 162, 'norsho': 163, 'norwat': 164,\n    'nrwswa': 165, 'nutwoo': 166, 'olsfly': 167, 'orcwar': 168, 'osprey': 169,\n    'ovenbi1': 170, 'palwar': 171, 'pasfly': 172, 'pecsan': 173, 'perfal': 174,\n    'phaino': 175, 'pibgre': 176, 'pilwoo': 177, 'pingro': 178, 'pinjay': 179,\n    'pinsis': 180, 'pinwar': 181, 'plsvir': 182, 'prawar': 183, 'purfin': 184,\n    'pygnut': 185, 'rebmer': 186, 'rebnut': 187, 'rebsap': 188, 'rebwoo': 189,\n    'redcro': 190, 'redhea': 191, 'reevir1': 192, 'renpha': 193, 'reshaw': 194,\n    'rethaw': 195, 'rewbla': 196, 'ribgul': 197, 'rinduc': 198, 'robgro': 199,\n    'rocpig': 200, 'rocwre': 201, 'rthhum': 202, 'ruckin': 203, 'rudduc': 204,\n    'rufgro': 205, 'rufhum': 206, 'rusbla': 207, 'sagspa1': 208, 'sagthr': 209,\n    'savspa': 210, 'saypho': 211, 'scatan': 212, 'scoori': 213, 'semplo': 214,\n    'semsan': 215, 'sheowl': 216, 'shshaw': 217, 'snobun': 218, 'snogoo': 219,\n    'solsan': 220, 'sonspa': 221, 'sora': 222, 'sposan': 223, 'spotow': 224,\n    'stejay': 225, 'swahaw': 226, 'swaspa': 227, 'swathr': 228, 'treswa': 229,\n    'truswa': 230, 'tuftit': 231, 'tunswa': 232, 'veery': 233, 'vesspa': 234,\n    'vigswa': 235, 'warvir': 236, 'wesblu': 237, 'wesgre': 238, 'weskin': 239,\n    'wesmea': 240, 'wessan': 241, 'westan': 242, 'wewpew': 243, 'whbnut': 244,\n    'whcspa': 245, 'whfibi': 246, 'whtspa': 247, 'whtswi': 248, 'wilfly': 249,\n    'wilsni1': 250, 'wiltur': 251, 'winwre3': 252, 'wlswar': 253, 'wooduc': 254,\n    'wooscj2': 255, 'woothr': 256, 'y00475': 257, 'yebfly': 258, 'yebsap': 259,\n    'yehbla': 260, 'yelwar': 261, 'yerwar': 262, 'yetvir': 263\n}\n\nINV_BIRD_CODE = {v: k for k, v in BIRD_CODE.items()}#\u904d\u5386bird_code\u5217\u8868","29c5eca1":"\nimport torch.nn as nn #\u5305\u542b\u5927\u91cf\u7684\u635f\u5931\u51fd\u6570\u4e0e\u6fc0\u6d3b\u51fd\u6570\u548c\u7528\u4e8e\u6784\u5efa\u7f51\u7edc\u7684\u51fd\u6570\u3002\nimport numpy as np\nimport torch \nimport librosa # \u97f3\u9891\u5904\u7406\u5e93\nimport torch.nn.functional as F#\u7528 torch.nn.functional\u4e2d\u5e26\u6709\u7684\u635f\u5931\u51fd\u6570\u6765\u4ee3\u66ff\u6211\u4eec\u81ea\u5df1\u7f16\u5199\u7684\u51fd\u6570\uff0c\u4f7f\u5f97\u4ee3\u7801\u53d8\u5f97\u66f4\u7b80\u77ed\nclass DFTBase(nn.Module):\n    def __init__(self):\n        \"\"\"Base class for DFT and IDFT matrix\"\"\"\n        super(DFTBase, self).__init__()\n\n    def dft_matrix(self, n):#DFT\u6b63\u53d8\u6362\uff0c\u53ef\u4f7f\u7528\u5185\u7f6e\u51fd\u6570dftmtx(n)\n        (x, y) = np.meshgrid(np.arange(n), np.arange(n))#\u751f\u6210\u7f51\u683c\u70b9\u5750\u6807\u77e9\u9635\n        omega = np.exp(-2 * np.pi * 1j \/ n)\n        W = np.power(omega, x * y)\n        return W\n\n    def idft_matrix(self, n):\n        (x, y) = np.meshgrid(np.arange(n), np.arange(n))\n        omega = np.exp(2 * np.pi * 1j \/ n)\n        W = np.power(omega, x * y)\n        return W\n    \n    \nclass STFT(DFTBase):\n    def __init__(self, n_fft=2048, hop_length=None, win_length=None, \n        window='hann', center=True, pad_mode='reflect', freeze_parameters=True):\n        \"\"\"\u7528Conv1d\uff08\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff09\u5b9e\u73b0STFT\u3002\u8be5\u51fd\u6570\u5177\u6709librosa.core.stft\u7684\u76f8\u540c\u8f93\u51fa\n        Conv1D (batch, steps, channels)\uff0csteps\u8868\u793a1\u7bc7\u6587\u672c\u4e2d\u542b\u6709\u7684\u5355\u8bcd\u6570\u91cf\uff0cchannels\u8868\u793a1\u4e2a\u5355\u8bcd\u7684\u7ef4\u5ea6\n        STFT\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\uff0c\u5bf9\u4e00\u7cfb\u5217\u52a0\u7a97\u6570\u636e\u505aFFT(\u79bb\u6563\u5085\u6c0f\u53d8\u6362\uff08DFT\uff09\u7684\u5feb\u901f\u7b97\u6cd5)\u3002\n        \"\"\"\n        super(STFT, self).__init__()#\u5bf9\u7ee7\u627f\u81ea\u7236\u7c7b\u7684\u5c5e\u6027\u8fdb\u884c\u521d\u59cb\u5316\n\n        assert pad_mode in ['constant', 'reflect'] #pad_mode\uff1a\u586b\u5145\u6a21\u5f0f\n\n        self.n_fft = n_fft #n_fft\uff1a\u7a97\u53e3\u5927\u5c0f\n        self.center = center\n        self.pad_mode = pad_mode\n\n        # win_length \u6bcf\u4e00\u5e27\u97f3\u9891\u90fd\u7531window\uff08\uff09\u52a0\u7a97\u3002\u7a97\u957fwin_length\uff0c\u7136\u540e\u7528\u96f6\u586b\u5145\u4ee5\u5339\u914dN_FFT\u3002\u9ed8\u8ba4win_length=n_fft\u3002\n        #window\uff1a\u5b57\u7b26\u4e32\uff0c\u5143\u7ec4\uff0c\u6570\u5b57\uff0c\u51fd\u6570 shape =\uff08n_fft, )\n        if win_length is None:\n            win_length = n_fft\n\n        #hop_length\uff1a\u5e27\u79fb\uff08\u97f3\u9891\u6837\u672c\u6570\uff09 \u8bbe\u7f6e\u9ed8\u8ba4\u8dc3\u70b9\uff08\u5982\u679c\u5c1a\u672a\u6307\u5b9a\uff09\n        if hop_length is None:\n            hop_length = int(win_length \/\/ 4)\n\n        fft_window = librosa.filters.get_window(window, win_length, fftbins=True)#\u7a97\u51fd\u6570\u7684\u8c03\u7528\n\n        #\u5c06\u7a97\u53e3\u586b\u5145\u5230n_fft\u5927\u5c0f\n        fft_window = librosa.util.pad_center(fft_window, n_fft)\n\n        # DFT & IDFT matrix\n        self.W = self.dft_matrix(n_fft)\n\n        out_channels = n_fft \/\/ 2 + 1\n\n        self.conv_real = nn.Conv1d(in_channels=1, out_channels=out_channels, \n            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1, \n            groups=1, bias=False)\n\n        self.conv_imag = nn.Conv1d(in_channels=1, out_channels=out_channels, \n            kernel_size=n_fft, stride=hop_length, padding=0, dilation=1, \n            groups=1, bias=False)\n\n        self.conv_real.weight.data = torch.Tensor(\n            np.real(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n        # (n_fft \/\/ 2 + 1, 1, n_fft)\n\n        self.conv_imag.weight.data = torch.Tensor(\n            np.imag(self.W[:, 0 : out_channels] * fft_window[:, None]).T)[:, None, :]\n        # (n_fft \/\/ 2 + 1, 1, n_fft)\n\n        if freeze_parameters:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, input):\n        \"\"\"input: (batch_size, data_length)\n        Returns:\n          real: (batch_size, n_fft \/\/ 2 + 1, time_steps)\n          imag: (batch_size, n_fft \/\/ 2 + 1, time_steps)\n        \"\"\"\n\n        x = input[:, None, :]   # (batch_size, channels_num, data_length)\n\n        if self.center:\n            x = F.pad(x, pad=(self.n_fft \/\/ 2, self.n_fft \/\/ 2), mode=self.pad_mode)\n\n        real = self.conv_real(x)\n        imag = self.conv_imag(x)\n        # (batch_size, n_fft \/\/ 2 + 1, time_steps)\n\n        real = real[:, None, :, :].transpose(2, 3)#\u4ea4\u6362\u7ef4\u5ea6\n        imag = imag[:, None, :, :].transpose(2, 3)\n        # (batch_size, 1, time_steps, n_fft \/\/ 2 + 1)\n\n        return real, imag#\u51fd\u6570\u7684\u5b9e\u90e8\u3001\u865a\u90e8\n    \n    #  spectrogram\u662f\u4e00\u4e2aMATLAB\u51fd\u6570\uff0c\u4f7f\u7528\u77ed\u65f6\u5085\u91cc\u53f6\u53d8\u6362\u5f97\u5230\u4fe1\u53f7\u7684\u9891\u8c31\u56fe\u3002\nclass Spectrogram(nn.Module):\n    def __init__(self, n_fft=2048, hop_length=None, win_length=None, \n        window='hann', center=True, pad_mode='reflect', power=2.0, \n        freeze_parameters=True):\n        \"\"\"\u4f7f\u7528pytorch\u8ba1\u7b97\u9891\u8c31\u56fe\u3002STFT\u901a\u8fc7Conv1d\u5b9e\u73b0\n        \"\"\"\n        super(Spectrogram, self).__init__()\n\n        self.power = power\n\n        self.stft = STFT(n_fft=n_fft, hop_length=hop_length, \n            win_length=win_length, window=window, center=center, \n            pad_mode=pad_mode, freeze_parameters=True)\n\n    def forward(self, input):\n        \"\"\"input: (batch_size, 1, time_steps, n_fft \/\/ 2 + 1)\n        Returns:\n          spectrogram: (batch_size, 1, time_steps, n_fft \/\/ 2 + 1)\n        \"\"\"\n\n        (real, imag) = self.stft.forward(input)\n        # (batch_size, n_fft \/\/ 2 + 1, time_steps)\n\n        spectrogram = real ** 2 + imag ** 2\n\n        if self.power == 2.0:\n            pass\n        else:\n            spectrogram = spectrogram ** (power \/ 2.0)\n\n        return spectrogram\n\n    #\u63d0\u53d6Log-Mel Spectrogram \u7279\u5f81 Log-Mel Spectrogram\u7279\u5f81\u662f\u76ee\u524d\u5728\u8bed\u97f3\u8bc6\u522b\u548c\u73af\u5883\u58f0\u97f3\u8bc6\u522b\u4e2d\u5f88\u5e38\u7528\u7684\u4e00\u4e2a\u7279\u5f81\nclass LogmelFilterBank(nn.Module):\n    def __init__(self, sr=32000, n_fft=2048, n_mels=64, fmin=50, fmax=14000, is_log=True, \n        ref=1.0, amin=1e-10, top_db=80.0, freeze_parameters=True):\n        \"\"\"\u4f7f\u7528pytorch\u8ba1\u7b97logmel\u9891\u8c31\u56fe\u3002mel\u8fc7\u6ee4\u5668\u5e93\u662f\u57fa\u4e8e librosa.filters.mel\u7684pytorch\u5b9e\u73b0\n        self\uff1a\u8981\u663e\u793a\u7684\u77e9\u9635\n        sr\uff1a\u91c7\u6837\u7387\n        n_fft \uff1aFFT\u7ec4\u4ef6\u6570\n        \u9891\u7387\u7c7b\u578b\uff1a'is_log'\uff1a\u9891\u8c31\u4ee5\u5bf9\u6570\u523b\u5ea6\u663e\u793a  'mel'\uff1a\u9891\u7387\u7531mel\u6807\u5ea6\u51b3\u5b9a\uff0cn_mels \uff1a\u4ea7\u751f\u7684\u6885\u5c14\u5e26\u6570\n        fmin \uff1a\u6700\u4f4e\u9891\u7387\uff08Hz\uff09\n        fmax\uff1a\u6700\u9ad8\u9891\u7387\uff08\u4ee5Hz\u4e3a\u5355\u4f4d\uff09\n        ref \uff1a\u53c2\u8003\u503c\n        \"\"\"\n        super(LogmelFilterBank, self).__init__()\n\n        self.is_log = is_log\n        self.ref = ref\n        self.amin = amin\n        self.top_db = top_db\n\n        self.melW = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels,\n            fmin=fmin, fmax=fmax).T\n        # (n_fft \/\/ 2 + 1, mel_bins)\n\n        self.melW = nn.Parameter(torch.Tensor(self.melW))\n\n        if freeze_parameters:\n            for param in self.parameters():\n                param.requires_grad = False\n\n    def forward(self, input):\n        \"\"\"input: (batch_size, channels, time_steps)\n        \n        Output: (batch_size, time_steps, mel_bins)\n        \"\"\"\n\n        # Mel spectrogram\n        mel_spectrogram = torch.matmul(input, self.melW)\n\n        # Logmel spectrogram\n        if self.is_log:\n            output = self.power_to_db(mel_spectrogram)\n        else:\n            output = mel_spectrogram\n\n        return output\n\n         #\u529f\u7387\u8f6cdB\n    def power_to_db(self, input):\n        \"\"\"\n        \u57fa\u4e8elibrosa.core.power_to_lb\u5b9e\u73b0 \u5c06\u529f\u7387\u8c31\uff08\u5e45\u5ea6\u5e73\u65b9\uff09\u8f6c\u6362\u4e3a\u5206\u8d1d\uff08dB\uff09\u5355\u4f4d\n        \"\"\"\n        ref_value = self.ref\n        log_spec = 10.0 * torch.log10(torch.clamp(input, min=self.amin, max=np.inf))\n        log_spec -= 10.0 * np.log10(np.maximum(self.amin, ref_value))\n\n        if self.top_db is not None:\n            if self.top_db < 0:\n                raise ParameterError('top_db must be non-negative')\n            log_spec = torch.clamp(log_spec, min=log_spec.max().item() - self.top_db, max=np.inf)\n\n        return log_spec\n\n\nclass DropStripes(nn.Module):\n    def __init__(self, dim, drop_width, stripes_num):\n        \"\"\"\u4e0b\u964d\u6761\u7eb9. \n        Args:\n          dim: int, \u6cbf\u5176\u4e0b\u964d\u7684\u5c3a\u5bf8\n          drop_width: int, \u4e0b\u964d\u7684\u6700\u5927\u6761\u7eb9\u5bbd\u5ea6\n          stripes_num: int, how many stripes to drop\n        \"\"\"\n        super(DropStripes, self).__init__()\n\n        assert dim in [2, 3]    # dim 2: time; dim 3: frequency\n\n        self.dim = dim\n        self.drop_width = drop_width\n        self.stripes_num = stripes_num\n\n    def forward(self, input):\n        \"\"\"input: (batch_size, channels, time_steps, freq_bins)\"\"\"\n\n        assert input.ndimension() == 4\n\n        if self.training is False:\n            return input\n\n        else:\n            batch_size = input.shape[0]\n            total_width = input.shape[self.dim]\n\n            for n in range(batch_size):\n                self.transform_slice(input[n], total_width)\n\n            return input\n\n\n    def transform_slice(self, e, total_width):\n        \"\"\"e: (channels, time_steps, freq_bins)\"\"\"\n\n        for _ in range(self.stripes_num):\n            distance = torch.randint(low=0, high=self.drop_width, size=(1,))[0]#\u8fd4\u56de\u5747\u5300\u5206\u5e03\u7684[low,high]\u4e4b\u95f4\u7684\u6574\u6570\u968f\u673a\u503c\n            bgn = torch.randint(low=0, high=total_width - distance, size=(1,))[0]\n\n            if self.dim == 2:\n                e[:, bgn : bgn + distance, :] = 0\n            elif self.dim == 3:\n                e[:, :, bgn : bgn + distance] = 0\n\n\nclass SpecAugmentation(nn.Module):\n    def __init__(self, time_drop_width, time_stripes_num, freq_drop_width, \n        freq_stripes_num):\n        \"\"\"Spec augmetation. \u4e00\u79cd\u65b0\u6570\u636e\u589e\u5f3a\u65b9\u6cd5\n          time_drop_width: int\n          time_stripes_num: int\n          freq_drop_width: int\n          freq_stripes_num: int\n        \"\"\"\n\n        super(SpecAugmentation, self).__init__()\n\n        self.time_dropper = DropStripes(dim=2, drop_width=time_drop_width, \n            stripes_num=time_stripes_num)#\u65f6\u95f4\u53d8\u5f62\n\n        self.freq_dropper = DropStripes(dim=3, drop_width=freq_drop_width, \n            stripes_num=freq_stripes_num)#\u9891\u7387\u53d8\u5f62\n\n    def forward(self, input):\n        x = self.time_dropper(input)\n        x = self.freq_dropper(x)\n        return x","26276cb6":"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.models as models#torchvision\u56fe\u50cf\u89c6\u9891\u5904\u7406\n\ndef init_layer(layer):\n    nn.init.xavier_uniform_(layer.weight)#\u670d\u4ece\u5747\u5300\u5206\u5e03\n\n    if hasattr(layer, \"bias\"):\n        if layer.bias is not None:\n            layer.bias.data.fill_(0.)\n\n\ndef init_bn(bn):\n    bn.bias.data.fill_(0.)\n    bn.weight.data.fill_(1.0)\n\n\ndef interpolate(x: torch.Tensor, ratio: int):\n    \"\"\"\u5728\u65f6\u57df\u5185\u63d2\u6570\u636e\u3002\u8fd9\u7528\u4e8e\u8865\u507fCNN\u4e0b\u91c7\u6837\u65f6\u5206\u8fa8\u7387\u7684\u964d\u4f4e\u3002\n\n    Args:\n      x: (batch_size, time_steps, classes_num)\n      ratio: int, \u63d2\u503c\u6bd4\u4f8b\n    Returns:\n      upsampled: (batch_size, time_steps * ratio, classes_num)\n    \"\"\"\n    (batch_size, time_steps, classes_num) = x.shape\n    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n    return upsampled\n\n\ndef pad_framewise_output(framewise_output: torch.Tensor, frames_num: int):\n    \"\"\"\u5c06framewise_output\u586b\u5145\u5230\u4e0e\u8f93\u5165\u5e27\u76f8\u540c\u7684\u957f\u5ea6\u3002\u586b\u5145\u503c\u4e0e\u6700\u540e\u4e00\u5e27\u7684\u503c\u76f8\u540c\u3002\n    Args:\n      framewise_output: (batch_size, frames_num, classes_num)\n      frames_num: int, number of frames to pad\n    Outputs:\n      output: (batch_size, frames_num, classes_num)\n    \"\"\"\n    pad = framewise_output[:, -1:, :].repeat(\n        1, frames_num - framewise_output.shape[1], 1)\n    \"\"\"tensor for padding\"\"\"\n\n    output = torch.cat((framewise_output, pad), dim=1)\n    \"\"\"(batch_size, frames_num, classes_num)\"\"\"\n\n    return output\n\n    #\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels: int, out_channels: int):\n        super().__init__()\n\n        self.conv1 = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=(3, 3),\n            stride=(1, 1),\n            padding=(1, 1),\n            bias=False)\n\n        self.conv2 = nn.Conv2d(\n            in_channels=out_channels,\n            out_channels=out_channels,\n            kernel_size=(3, 3),\n            stride=(1, 1),\n            padding=(1, 1),\n            bias=False)\n\n        self.bn1 = nn.BatchNorm2d(out_channels)\n        self.bn2 = nn.BatchNorm2d(out_channels)\n\n        self.init_weight()\n\n    def init_weight(self):\n        init_layer(self.conv1)\n        init_layer(self.conv2)\n        init_bn(self.bn1)\n        init_bn(self.bn2)\n\n    def forward(self, input, pool_size=(2, 2), pool_type='avg'):\n\n        x = input\n        x = F.relu_(self.bn1(self.conv1(x)))\n        x = F.relu_(self.bn2(self.conv2(x)))\n        if pool_type == 'max':\n            x = F.max_pool2d(x, kernel_size=pool_size)\n        elif pool_type == 'avg':\n            x = F.avg_pool2d(x, kernel_size=pool_size)\n        elif pool_type == 'avg+max':\n            x1 = F.avg_pool2d(x, kernel_size=pool_size)\n            x2 = F.max_pool2d(x, kernel_size=pool_size)\n            x = x1 + x2\n        else:\n            raise Exception('Incorrect argument!')\n\n        return x\n\n      #\u5377\u79ef\u5757\u6ce8\u610f\u6a21\u5757\uff0c\u4e00\u4e2a\u7b80\u5355\u800c\u6709\u6548\u7684\u6ce8\u610f\u6a21\u5757\u7684\u524d\u9988\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\nclass AttBlock(nn.Module):\n    def __init__(self,\n                 in_features: int,\n                 out_features: int,\n                 activation=\"linear\",\n                 temperature=1.0):\n        super().__init__()\n\n        self.activation = activation\n        self.temperature = temperature\n        self.att = nn.Conv1d(\n            in_channels=in_features,\n            out_channels=out_features,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True)\n        self.cla = nn.Conv1d(\n            in_channels=in_features,\n            out_channels=out_features,\n            kernel_size=1,\n            stride=1,\n            padding=0,\n            bias=True)\n\n        self.bn_att = nn.BatchNorm1d(out_features)\n        self.init_weights()\n\n    def init_weights(self):\n        init_layer(self.att)\n        init_layer(self.cla)\n        init_bn(self.bn_att)\n\n    def forward(self, x):\n        # x: (n_samples, n_in, n_time)\n        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n        cla = self.nonlinear_transform(self.cla(x))\n        x = torch.sum(norm_att * cla, dim=2)\n        return x, norm_att, cla\n\n    def nonlinear_transform(self, x):\n        if self.activation == 'linear':\n            return x\n        elif self.activation == 'sigmoid':\n            return torch.sigmoid(x)\n        #\u6a21\u578b\u62df\u5408\nclass PANNsDense121Att(nn.Module):\n    def __init__(self, sample_rate: int, window_size: int, hop_size: int,\n                 mel_bins: int, fmin: int, fmax: int, classes_num: int, apply_aug: bool, top_db=None):\n        super().__init__()\n        \n        window = 'hann'\n        center = True\n        pad_mode = 'reflect'\n        ref = 1.0\n        amin = 1e-10\n        self.interpolate_ratio = 32  # Downsampled ratio\n        self.apply_aug = apply_aug\n\n        #\u9891\u8c31\u56fe\u63d0\u53d6\u5668\n        self.spectrogram_extractor = Spectrogram(\n            n_fft=window_size,\n            hop_length=hop_size,\n            win_length=window_size,\n            window=window,\n            center=center,\n            pad_mode=pad_mode,\n            freeze_parameters=True)\n\n        # Logmel\u7279\u5f81\u63d0\u53d6\u5668\n        self.logmel_extractor = LogmelFilterBank(\n            sr=sample_rate,\n            n_fft=window_size,\n            n_mels=mel_bins,\n            fmin=fmin,\n            fmax=fmax,\n            ref=ref,\n            amin=amin,\n            top_db=top_db,\n            freeze_parameters=True)\n\n        # Spec augmenter \u589e\u5f3a\u6570\u636e\n        self.spec_augmenter = SpecAugmentation(\n            time_drop_width=64,\n            time_stripes_num=2,\n            freq_drop_width=8,\n            freq_stripes_num=2)\n\n        self.bn0 = nn.BatchNorm2d(mel_bins)\n\n        self.fc1 = nn.Linear(1024, 1024, bias=True)\n        self.att_block = AttBlock(1024, classes_num, activation='sigmoid')\n\n\n        self.densenet_features = models.densenet121(pretrained=False).features\n\n        self.init_weight()\n\n    def init_weight(self):\n        init_bn(self.bn0)\n        init_layer(self.fc1)\n        #cnn\u7279\u5f81\u63d0\u53d6\n    def cnn_feature_extractor(self, x):\n        x = self.densenet_features(x)\n        return x\n        #\u9884\u5904\u7406\n    def preprocess(self, input_x, mixup_lambda=None):\n\n        x = self.spectrogram_extractor(input_x)  # (batch_size, 1, time_steps, freq_bins)\n        x = self.logmel_extractor(x)  # (batch_size, 1, time_steps, mel_bins)\n\n        frames_num = x.shape[2]\n\n        x = x.transpose(1, 3)\n        x = self.bn0(x)\n        x = x.transpose(1, 3)\n\n        if self.apply_aug:\n            x = self.spec_augmenter(x)\n\n        return x, frames_num\n        \n        \n    def forward(self, input_data):\n        input_x, mixup_lambda = input_data\n        \"\"\"\n        Input: (batch_size, data_length)\"\"\"\n        b, c, s = input_x.shape\n        input_x = input_x.reshape(b*c, s)\n        x, frames_num = self.preprocess(input_x, mixup_lambda=mixup_lambda)\n        if mixup_lambda is not None:\n            b = (b*c)\/\/2\n            c = 1\n        # Output shape (batch size, channels, time, frequency)\n        x = x.expand(x.shape[0], 3, x.shape[2], x.shape[3])\n        x = self.cnn_feature_extractor(x)\n        \n        #\u6c47\u603b\u5728\u9891\u7387\u8f74\u4e0a\n        x = torch.mean(x, dim=3)\n\n        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n        x = x1 + x2\n\n        x = F.dropout(x, p=0.5, training=self.training)\n        x = x.transpose(1, 2)\n        x = F.relu_(self.fc1(x))\n        x = x.transpose(1, 2)\n        x = F.dropout(x, p=0.5, training=self.training)\n\n        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n        segmentwise_output = segmentwise_output.transpose(1, 2)\n\n        # \u83b7\u53d6\u9010\u5e27\u8f93\u51fa\n        framewise_output = interpolate(segmentwise_output,\n                                       self.interpolate_ratio)\n        framewise_output = pad_framewise_output(framewise_output, frames_num)\n        frame_shape =  framewise_output.shape\n        clip_shape = clipwise_output.shape\n        output_dict = {\n            'framewise_output': framewise_output.reshape(b, c, frame_shape[1],frame_shape[2]),\n            'clipwise_output': clipwise_output.reshape(b, c, clip_shape[1]),\n        }\n\n        return output_dict","4120e459":"def get_model(ModelClass: object, config: dict, weights_path: str):\n    model = ModelClass(**config)\n    checkpoint = torch.load(weights_path, map_location='cpu')\n    model.load_state_dict(checkpoint[\"model\"])\n    model.to(device)\n    model.eval()\n    return model","9219f53c":"list_of_models = [\n    {\n        \"model_class\": PANNsDense121Att,\n        \"config\": {\n            \"sample_rate\": 32000,\n            \"window_size\": 1024,\n            \"hop_size\": 320,\n            \"mel_bins\": 64,\n            \"fmin\": 50,\n            \"fmax\": 14000,\n            \"classes_num\": 264,\n            \"apply_aug\": True,\n            \"top_db\": None\n        },\n        \"weights_path\": \"..\/input\/birdsongdetectionfinalsubmission1\/final_sed_dense121_nomix_fold0_checkpoint_50_score0.7057.pt\",\n        \"clip_threshold\": 0.3,\n        \"threshold\": 0.3\n    },\n    {\n        \"model_class\": PANNsDense121Att,\n        \"config\": {\n            \"sample_rate\": 32000,\n            \"window_size\": 1024,\n            \"hop_size\": 320,\n            \"mel_bins\": 64,\n            \"fmin\": 50,\n            \"fmax\": 14000,\n            \"classes_num\": 264,\n            \"apply_aug\": True,\n            \"top_db\": None\n        },\n        \"weights_path\": \"..\/input\/birdsongdetectionfinalsubmission1\/final_sed_dense121_nomix_fold1_checkpoint_48_score0.6943.pt\",\n        \"clip_threshold\": 0.3,\n        \"threshold\": 0.3\n    },\n    {\n        \"model_class\": PANNsDense121Att,\n        \"config\": {\n            \"sample_rate\": 32000,\n            \"window_size\": 1024,\n            \"hop_size\": 320,\n            \"mel_bins\": 64,\n            \"fmin\": 50,\n            \"fmax\": 14000,\n            \"classes_num\": 264,\n            \"apply_aug\": True,\n            \"top_db\": None\n        },\n        \"weights_path\": \"..\/input\/birdsongdetectionfinalsubmission1\/final_sed_dense121_nomix_fold2_augd_checkpoint_50_score0.6666.pt\",\n        \"clip_threshold\": 0.3,\n        \"threshold\": 0.3\n    },\n    {\n        \"model_class\": PANNsDense121Att,\n        \"config\": {\n            \"sample_rate\": 32000,\n            \"window_size\": 1024,\n            \"hop_size\": 320,\n            \"mel_bins\": 64,\n            \"fmin\": 50,\n            \"fmax\": 14000,\n            \"classes_num\": 264,\n            \"apply_aug\": True,\n            \"top_db\": None\n        },\n        \"weights_path\": \"..\/input\/birdsongdetectionfinalsubmission1\/final_sed_dense121_nomix_fold3_augd_checkpoint_50_score0.6713.pt\",\n        \"clip_threshold\": 0.3,\n        \"threshold\": 0.3\n    },\n    {\n        \"model_class\": PANNsDense121Att,\n        \"config\": {\n            \"sample_rate\": 32000,\n            \"window_size\": 1024,\n            \"hop_size\": 320,\n            \"mel_bins\": 64,\n            \"fmin\": 50,\n            \"fmax\": 14000,\n            \"classes_num\": 264,\n            \"apply_aug\": True,\n            \"top_db\": None\n        },\n        \"weights_path\": \"..\/input\/birdsongdetectionfinalsubmission1\/final_5fold_sed_dense121_nomix_fold0_checkpoint_50_score0.7219.pt\",\n        \"clip_threshold\": 0.3,\n        \"threshold\": 0.3\n    },\n    {\n        \"model_class\": PANNsDense121Att,\n        \"config\": {\n            \"sample_rate\": 32000,\n            \"window_size\": 1024,\n            \"hop_size\": 320,\n            \"mel_bins\": 64,\n            \"fmin\": 50,\n            \"fmax\": 14000,\n            \"classes_num\": 264,\n            \"apply_aug\": True,\n            \"top_db\": None\n        },\n        \"weights_path\": \"..\/input\/birdsongdetectionfinalsubmission1\/final_5fold_sed_dense121_nomix_fold1_checkpoint_44_score0.7645.pt\",\n        \"clip_threshold\": 0.3,\n        \"threshold\": 0.3\n    },\n    {\n        \"model_class\": PANNsDense121Att,\n        \"config\": {\n            \"sample_rate\": 32000,\n            \"window_size\": 1024,\n            \"hop_size\": 320,\n            \"mel_bins\": 64,\n            \"fmin\": 50,\n            \"fmax\": 14000,\n            \"classes_num\": 264,\n            \"apply_aug\": True,\n            \"top_db\": None\n        },\n        \"weights_path\": \"..\/input\/birdsongdetectionfinalsubmission1\/final_5fold_sed_dense121_nomix_fold2_checkpoint_50_score0.7737.pt\",\n        \"clip_threshold\": 0.3,\n        \"threshold\": 0.3\n    },\n    {\n        \"model_class\": PANNsDense121Att,\n        \"config\": {\n            \"sample_rate\": 32000,\n            \"window_size\": 1024,\n            \"hop_size\": 320,\n            \"mel_bins\": 64,\n            \"fmin\": 50,\n            \"fmax\": 14000,\n            \"classes_num\": 264,\n            \"apply_aug\": True,\n            \"top_db\": None\n        },\n        \"weights_path\": \"..\/input\/birdsongdetectionfinalsubmission1\/final_5fold_sed_dense121_nomix_fold3_checkpoint_48_score0.7746.pt\",\n        \"clip_threshold\": 0.3,\n        \"threshold\": 0.3\n    },\n    {\n        \"model_class\": PANNsDense121Att,\n        \"config\": {\n            \"sample_rate\": 32000,\n            \"window_size\": 1024,\n            \"hop_size\": 320,\n            \"mel_bins\": 64,\n            \"fmin\": 50,\n            \"fmax\": 14000,\n            \"classes_num\": 264,\n            \"apply_aug\": True,\n            \"top_db\": None\n        },\n        \"weights_path\": \"..\/input\/birdsongdetectionfinalsubmission1\/final_5fold_sed_dense121_nomix_fold4_checkpoint_50_score0.7728.pt\",\n        \"clip_threshold\": 0.3,\n        \"threshold\": 0.3\n    },\n    {\n        \"model_class\": PANNsDense121Att,\n        \"config\": {\n            \"sample_rate\": 32000,\n            \"window_size\": 1024,\n            \"hop_size\": 320,\n            \"mel_bins\": 64,\n            \"fmin\": 50,\n            \"fmax\": 14000,\n            \"classes_num\": 264,\n            \"apply_aug\": True,\n            \"top_db\": None\n        },\n        \"weights_path\": \"..\/input\/birdsongdetectionfinalsubmission1\/final_sed_dense121_mix_fold0_2_checkpoint_50_score0.6842.pt\",\n        \"clip_threshold\": 0.3,\n        \"threshold\": 0.3\n    },\n    {\n        \"model_class\": PANNsDense121Att,\n        \"config\": {\n            \"sample_rate\": 32000,\n            \"window_size\": 1024,\n            \"hop_size\": 320,\n            \"mel_bins\": 64,\n            \"fmin\": 50,\n            \"fmax\": 14000,\n            \"classes_num\": 264,\n            \"apply_aug\": True,\n            \"top_db\": None\n        },\n        \"weights_path\": \"..\/input\/birdsongdetectionfinalsubmission1\/final_sed_dense121_mix_fold1_2_checkpoint_50_score0.6629.pt\",\n        \"clip_threshold\": 0.3,\n        \"threshold\": 0.3\n    },\n    {\n        \"model_class\": PANNsDense121Att,\n        \"config\": {\n            \"sample_rate\": 32000,\n            \"window_size\": 1024,\n            \"hop_size\": 320,\n            \"mel_bins\": 64,\n            \"fmin\": 50,\n            \"fmax\": 14000,\n            \"classes_num\": 264,\n            \"apply_aug\": True,\n            \"top_db\": None\n        },\n        \"weights_path\": \"..\/input\/birdsongdetectionfinalsubmission1\/final_sed_dense121_mix_fold2_2_checkpoint_50_score0.6884.pt\",\n        \"clip_threshold\": 0.3,\n        \"threshold\": 0.3\n    },\n    {\n        \"model_class\": PANNsDense121Att,\n        \"config\": {\n            \"sample_rate\": 32000,\n            \"window_size\": 1024,\n            \"hop_size\": 320,\n            \"mel_bins\": 64,\n            \"fmin\": 50,\n            \"fmax\": 14000,\n            \"classes_num\": 264,\n            \"apply_aug\": True,\n            \"top_db\": None\n        },\n        \"weights_path\": \"..\/input\/birdsongdetectionfinalsubmission1\/final_sed_dense121_mix_fold3_2_checkpoint_50_score0.6870.pt\",\n        \"clip_threshold\": 0.3,\n        \"threshold\": 0.3\n    }\n]\nPERIOD = 30\nSR = 32000\nvote_lim = 4\nTTA = 10","566c6a3b":"for lm in list_of_models:\n    lm[\"model\"] = get_model(lm[\"model_class\"], lm[\"config\"], lm[\"weights_path\"])","6cdda296":"def prediction_for_clip(test_df: pd.DataFrame,\n                        clip: np.ndarray, \n                        model,\n                        threshold,\n                       clip_threshold):\n\n    audios = []\n    y = clip.astype(np.float32)\n    len_y = len(y)\n    start = 0\n    end = PERIOD * SR\n    while True:\n        y_batch = y[start:end].astype(np.float32)\n        if len(y_batch) != PERIOD * SR:\n            y_pad = np.zeros(PERIOD * SR, dtype=np.float32)\n            y_pad[:len(y_batch)] = y_batch\n            audios.append(y_pad)\n            break\n        start = end\n        end += PERIOD * SR\n        audios.append(y_batch)\n        \n    array = np.asarray(audios)\n    tensors = torch.from_numpy(array)\n    \n    model.eval()\n    estimated_event_list = []\n    global_time = 0.0\n    site = test_df[\"site\"].values[0]\n    audio_id = test_df[\"audio_id\"].values[0]\n    for image in tensors:\n        image = image.unsqueeze(0).unsqueeze(0)\n        image = image.expand(image.shape[0], TTA, image.shape[2])\n        image = image.to(device)\n        \n        with torch.no_grad():\n            prediction = model((image, None))\n            framewise_outputs = prediction[\"framewise_output\"].detach(\n                ).cpu().numpy()[0].mean(axis=0)\n            clipwise_outputs = prediction[\"clipwise_output\"].detach(\n                ).cpu().numpy()[0].mean(axis=0)\n                \n        thresholded = framewise_outputs >= threshold\n        \n        clip_thresholded = clipwise_outputs >= clip_threshold\n        clip_indices = np.argwhere(clip_thresholded).reshape(-1)\n        clip_codes = []\n        for ci in clip_indices:\n            clip_codes.append(INV_BIRD_CODE[ci])\n            \n        for target_idx in range(thresholded.shape[1]):\n            if thresholded[:, target_idx].mean() == 0:\n                pass\n            else:\n                detected = np.argwhere(thresholded[:, target_idx]).reshape(-1)\n                head_idx = 0\n                tail_idx = 0\n                while True:\n                    if (tail_idx + 1 == len(detected)) or (\n                            detected[tail_idx + 1] - \n                            detected[tail_idx] != 1):\n                        onset = 0.01 * detected[\n                            head_idx] + global_time\n                        offset = 0.01 * detected[\n                            tail_idx] + global_time\n                        onset_idx = detected[head_idx]\n                        offset_idx = detected[tail_idx]\n                        max_confidence = framewise_outputs[\n                            onset_idx:offset_idx, target_idx].max()\n                        mean_confidence = framewise_outputs[\n                            onset_idx:offset_idx, target_idx].mean()\n                        if INV_BIRD_CODE[target_idx] in clip_codes:\n                            estimated_event = {\n                                \"site\": site,\n                                \"audio_id\": audio_id,\n                                \"ebird_code\": INV_BIRD_CODE[target_idx],\n                                \"clip_codes\": clip_codes,\n                                \"onset\": onset,\n                                \"offset\": offset,\n                                \"max_confidence\": max_confidence,\n                                \"mean_confidence\": mean_confidence\n                            }\n                            estimated_event_list.append(estimated_event)\n                        head_idx = tail_idx + 1\n                        tail_idx = tail_idx + 1\n                        if head_idx >= len(detected):\n                            break\n                    else:\n                        tail_idx += 1\n        global_time += PERIOD\n        \n    prediction_df = pd.DataFrame(estimated_event_list)\n    return prediction_df\n\ndef prediction(test_df: pd.DataFrame,\n               test_audio: Path,\n               list_of_model_details):\n    unique_audio_id = test_df.audio_id.unique()\n\n    warnings.filterwarnings(\"ignore\")\n    prediction_dfs_dict = defaultdict(list)\n    for audio_id in progress_bar(unique_audio_id):\n        clip, _ = librosa.load(test_audio \/ (audio_id + \".mp3\"),\n                               sr=SR,\n                               mono=True,\n                               res_type=\"kaiser_fast\")\n        \n        test_df_for_audio_id = test_df.query(\n            f\"audio_id == '{audio_id}'\").reset_index(drop=True)\n        for i, model_details in enumerate(list_of_model_details):\n            prediction_df = prediction_for_clip(test_df_for_audio_id,\n                                                clip=clip,\n                                                model=model_details[\"model\"],\n                                                threshold=model_details[\"threshold\"],\n                                               clip_threshold=model_details[\"clip_threshold\"])\n\n            prediction_dfs_dict[i].append(prediction_df)\n    list_of_prediction_df = []\n    for key, prediction_dfs in prediction_dfs_dict.items():\n        prediction_df = pd.concat(prediction_dfs, axis=0, sort=False).reset_index(drop=True)\n        list_of_prediction_df.append(prediction_df)\n    return list_of_prediction_df","82c624b8":"list_of_prediction_df = prediction(test_df=test,\n                           test_audio=TEST_AUDIO_DIR,\n                           list_of_model_details=list_of_models)","7d6755a8":"def get_post_post_process_predictions(prediction_df):\n    labels = {}\n\n    for audio_id, sub_df in progress_bar(prediction_df.groupby(\"audio_id\")):\n        events = sub_df[[\"ebird_code\", \"onset\", \"offset\", \"max_confidence\", \"site\"]].values\n        n_events = len(events)\n\n        site = events[0][4]\n        for i in range(n_events):\n            event = events[i][0]\n            onset = events[i][1]\n            offset = events[i][2]\n            \n            start_section = int((onset \/\/ 5) * 5) + 5\n            end_section = int((offset \/\/ 5) * 5) + 5\n            cur_section = start_section\n\n            row_id = f\"{site}_{audio_id}_{start_section}\"\n            if labels.get(row_id) is not None:\n                labels[row_id].add(event)\n            else:\n                labels[row_id] = set()\n                labels[row_id].add(event)\n\n            while cur_section != end_section:\n                cur_section += 5\n                row_id = f\"{site}_{audio_id}_{cur_section}\"\n                if labels.get(row_id) is not None:\n                    labels[row_id].add(event)\n                else:\n                    labels[row_id] = set()\n                    labels[row_id].add(event)\n\n\n    for key in labels:\n        labels[key] = \" \".join(sorted(list(labels[key])))\n\n\n    row_ids = list(labels.keys())\n    birds = list(labels.values())\n    post_processed = pd.DataFrame({\n        \"row_id\": row_ids,\n        \"birds\": birds\n    })\n    return post_processed","26684dfb":"all_row_id = test[[\"row_id\"]]\nlist_of_submissions = []\nfor prediction_df in list_of_prediction_df:\n    post_processed = get_post_post_process_predictions(prediction_df)\n    submission = post_processed.fillna(\"nocall\")\n    submission = submission.set_index('row_id')\n    list_of_submissions.append(submission)","3c391294":"list_all_of_row_ids = []\nfor sub_x in list_of_submissions:\n    list_all_of_row_ids+= list(sub_x.index.values)\nlist_all_of_row_ids = list(set(list_all_of_row_ids))","00507cf4":"final_submission = []\nfor row_id in list_all_of_row_ids:\n    birds = []\n    for sub in list_of_submissions:\n        if row_id in sub.index:\n            birds.extend(sub.loc[row_id].birds.split(\" \"))\n    birds = [x for x in birds if \"nocall\" != x and \"\" != x]\n    count_birds = Counter(birds)\n    final_birds = []\n    for key, value in count_birds.items():\n        if value >= vote_lim:\n            final_birds.append(key)\n    if len(final_birds)>0:\n        row_data = {\n            \"row_id\": row_id,\n            \"birds\": \" \".join(sorted(final_birds))\n        }\n    else:\n        row_data = {\n            \"row_id\": row_id,\n            \"birds\": \"nocall\"\n        }\n    final_submission.append(row_data)\n    \nsite_3_data = defaultdict(list)\nfor row in final_submission:\n    if \"site_3\" in row[\"row_id\"]:\n        final_row_id = \"_\".join(row[\"row_id\"].split(\"_\")[0:-1])\n        birds = row[\"birds\"].split(\" \")\n        birds = [x for x in birds if \"nocall\" != x and \"\" != x]\n        site_3_data[final_row_id].extend(birds)\n        \nfor key, value in site_3_data.items():\n    count_birds = Counter(value)\n    final_birds = []\n    for k, v in count_birds.items():\n        if v >= vote_lim:\n            final_birds.append(k)\n    if len(final_birds)>0:\n        row_data = {\n            \"row_id\": key,\n            \"birds\": \" \".join(sorted(final_birds))\n        }\n    else:\n        row_data = {\n            \"row_id\": key,\n            \"birds\": \"nocall\"\n        }\n    final_submission.append(row_data)\n\nfinal_submission = pd.DataFrame(final_submission)\nfinal_submission = all_row_id.merge(final_submission, on=\"row_id\", how=\"left\")\nfinal_submission = final_submission.fillna(\"nocall\")","4ccc218c":"final_submission.to_csv(\"submission.csv\", index=False)\nfinal_submission.head(50)","0bc5f07e":"## 2.2 PANN Models","54badf9e":"Torch\u76ee\u6807\u662f\u8ba9\u4f60\u901a\u8fc7\u6781\u5176\u7b80\u5355\u8fc7\u7a0b\u3001\u6700\u5927\u7684\u7075\u6d3b\u6027\u548c\u901f\u5ea6\u5efa\u7acb\u81ea\u5df1\u7684\u79d1\u5b66\u7b97\u6cd5\u3002Torch\u6709\u4e00\u4e2a\u5728\u673a\u5668\u5b66\u4e60\u9886\u57df\u5927\u578b\u751f\u6001\u793e\u533a\u9a71\u52a8\u5e93\u5305\uff0c\u5305\u62ec\u8ba1\u7b97\u673a\u89c6\u89c9\u8f6f\u4ef6\u5305\uff0c\u4fe1\u53f7\u5904\u7406\uff0c\u5e76\u884c\u5904\u7406\uff0c\u56fe\u50cf\uff0c\u89c6\u9891\uff0c\u97f3\u9891\u548c\u7f51\u7edc\u7b49\uff0c\u57fa\u4e8eLua\u793e\u533a\u5efa\u7acb\u3002\n\nTorch \u7684\u6838\u5fc3\u662f\u6d41\u884c\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u5b83\u4f7f\u7528\u7b80\u5355\u7684\u4f18\u5316\u5e93\uff0c\u540c\u65f6\u5177\u6709\u6700\u5927\u7684\u7075\u6d3b\u6027\uff0c\u5b9e\u73b0\u590d\u6742\u7684\u795e\u7ecf\u7f51\u7edc\u7684\u62d3\u6251\u7ed3\u6784\u3002\u4f60\u53ef\u4ee5\u5efa\u7acb\u795e\u7ecf\u7f51\u7edc\u548c\u5e76\u884c\u4efb\u610f\u56fe\uff0c\u901a\u8fc7CPU\u548cGPU\u7b49\u6709\u6548\u65b9\u5f0f\u3002\n\ndict\u548cdefaultdict\u7684\u533a\u522b\uff1a\u666e\u901a\u7684\u5b57\u5178\u65f6\uff0c\u7528\u6cd5\u4e00\u822c\u662fdict={},\u6dfb\u52a0\u5143\u7d20\u7684\u53ea\u9700\u8981dict[element] =value\u5373\uff0c\u8c03\u7528\u7684\u65f6\u5019\u4e5f\u662f\u5982\u6b64\uff0cdict[element] = xxx,\u4f46\u524d\u63d0\u662felement\u5b57\u5178\u91cc\uff0c\u5982\u679c\u4e0d\u5728\u5b57\u5178\u91cc\u5c31\u4f1a\u62a5\u9519\ndefaultdict\u4f5c\u7528\u662f\u5728\u4e8e\uff0c\u5f53\u5b57\u5178\u91cc\u7684key\u4e0d\u5b58\u5728\u4f46\u88ab\u67e5\u627e\u65f6\uff0c\u8fd4\u56de\u7684\u4e0d\u662fkeyError\u800c\u662f\u5bf9\u5e94\u7684\u9ed8\u8ba4\u503c\uff0c\u6bd4\u5982list\u5bf9\u5e94[ ]\uff0cstr\u5bf9\u5e94\u7684\u662f\u7a7a\u5b57\u7b26\u4e32\uff0cset\u5bf9\u5e94set( )\uff0cint\u5bf9\u5e940\u7b49","2a798578":"## 2.3 Model Utils","279c6a8a":"# 2. Models","354fa258":"# 3. Model Parameters","b92ef314":"# 1. Prelim","3c96166d":"# 6. Ensemble","21971c89":"DFT\uff1a\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\n\u79bb\u6563\u5085\u91cc\u53f6\u53d8\u6362\u53ef\u4ee5\u5c06\u8fde\u7eed\u7684\u9891\u8c31\u8f6c\u5316\u6210\u79bb\u6563\u7684\u9891\u8c31\u53bb\u8ba1\u7b97\uff0c\u8fd9\u6837\u5c31\u6613\u4e8e\u8ba1\u7b97\u673a\u7f16\u7a0b\u5b9e\u73b0\u5085\u91cc\u53f6\u53d8\u6362\u7684\u8ba1\u7b97\n\nIDFT\uff1a\u79bb\u6563\u5085\u91cc\u53f6\u9006\u53d8\u6362\n\nclass torch.nn.Conv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)\n\nin_channels(int) \u2013 \u8f93\u5165\u4fe1\u53f7\u7684\u901a\u9053\u3002\nout_channels(int) \u2013 \u5377\u79ef\u4ea7\u751f\u7684\u901a\u9053\u3002\u6709\u591a\u5c11\u4e2aout_channels\uff0c\u5c31\u9700\u8981\u591a\u5c11\u4e2a1\u7ef4\u5377\u79ef\nkernel_size(int\u00a0or\u00a0tuple) - \u5377\u79ef\u6838\u7684\u5c3a\u5bf8\uff0c\u5377\u79ef\u6838\u7684\u5927\u5c0f\u4e3a(k,)\uff0c\u7b2c\u4e8c\u4e2a\u7ef4\u5ea6\u662f\u7531in_channels\u6765\u51b3\u5b9a\u7684\uff0c\u6240\u4ee5\u5b9e\u9645\u4e0a\u5377\u79ef\u5927\u5c0f\u4e3akernel_size*in_channels\n\nstride(int or tuple, optional) - \u5377\u79ef\u6b65\u957f\u3002\npadding (int or tuple, optional)- \u8f93\u5165\u7684\u6bcf\u4e00\u6761\u8fb9\u8865\u51450\u7684\u5c42\u6570\u3002\ndilation(int or tuple, `optional``) \u2013 \u5377\u79ef\u6838\u5143\u7d20\u4e4b\u95f4\u7684\u95f4\u8ddd\u3002\ngroups(int, optional) \u2013 \u4ece\u8f93\u5165\u901a\u9053\u5230\u8f93\u51fa\u901a\u9053\u7684\u963b\u585e\u8fde\u63a5\u6570\u3002\nbias(bool, optional) - \u5982\u679cbias=True\uff0c\u6dfb\u52a0\u504f\u7f6e\u3002","aaa5246e":"# 4. Predictions","b6ba9044":"cuDNN \u662f\u82f1\u4f1f\u8fbe\u4e13\u95e8\u4e3a\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u6240\u5f00\u53d1\u51fa\u6765\u7684 GPU \u52a0\u901f\u5e93\uff0c\u9488\u5bf9\u5377\u79ef\u3001\u6c60\u5316\u7b49\u7b49\u5e38\u89c1\u64cd\u4f5c\u505a\u4e86\u975e\u5e38\u591a\u7684\u5e95\u5c42\u4f18\u5316\uff0c\u6bd4\u4e00\u822c\u7684 GPU \u7a0b\u5e8f\u8981\u5feb\u5f88\u591a\ntorch.backends.cudnn.deterministic\u662f\u5565\uff1f\u987e\u540d\u601d\u4e49\uff0c\u5c06\u8fd9\u4e2a flag \u7f6e\u4e3aTrue\u7684\u8bdd\uff0c\u6bcf\u6b21\u8fd4\u56de\u7684\u5377\u79ef\u7b97\u6cd5\u5c06\u662f\u786e\u5b9a\u7684\uff0c\u5373\u9ed8\u8ba4\u7b97\u6cd5\u3002\u5982\u679c\u914d\u5408\u4e0a\u8bbe\u7f6e Torch \u7684\u968f\u673a\u79cd\u5b50\u4e3a\u56fa\u5b9a\u503c\u7684\u8bdd\uff0c\u53ef\u4ee5\u4fdd\u8bc1\u6bcf\u6b21\u8fd0\u884c\u7f51\u7edc\u7684\u65f6\u5019\u76f8\u540c\u8f93\u5165\u7684\u8f93\u51fa\u662f\u56fa\u5b9a\u7684\n\u4f7f\u7528 cuDNN \u7684\u65f6\u5019\uff0ctorch.backends.cudnn.benchmark \u6a21\u5f0f\u662f\u4e3a False","4e0353a4":"# 5. Post Process","34c9e358":"cuda \u7edf\u4e00\u8ba1\u7b97\u8bbe\u5907\u67b6\u6784\uff08Compute Unified Device Architecture, CUDA\uff09\uff0c\u662f\u7531NVIDIA\u63a8\u51fa\u7684\u901a\u7528\u5e76\u884c\u8ba1\u7b97\u67b6\u6784\u3002\u89e3\u51b3\u7684\u662f\u7528\u66f4\u52a0\u5ec9\u4ef7\u7684\u8bbe\u5907\u8d44\u6e90\uff0c\u5b9e\u73b0\u66f4\u9ad8\u6548\u7684\u5e76\u884c\u8ba1\u7b97","dd61deeb":"## 2.1 Audio Utils\n","1526d5f9":"\u540e\u5904\u7406\u6307\u4e00\u822c\u8bad\u7ec3\u6a21\u578b\u6807\u7b7e\u97f3\u9891\u7c7b\u578b\u540e\uff0c\u8fd8\u8981\u4f5c\u540e\u5904\u7406\u53bb\u6539\u5584\u9884\u6d4b\u7ed3\u679c\uff0c\u9884\u6d4b\u7ed3\u679c\u9700\u8981\u6309\u573a\u666f\u4f5c\u8c03\u6574"}}