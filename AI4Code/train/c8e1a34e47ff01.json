{"cell_type":{"e78fa388":"code","e1ddec77":"code","1e4eda57":"code","7b4e031e":"code","af666838":"code","cb1f4a7d":"code","f7f693b1":"code","65cc37e9":"code","e833a8b0":"code","1c0c668c":"code","2d8c3ac5":"code","16d16b41":"code","b9f31592":"code","3456a8d8":"code","0db9a1ec":"code","77bf044a":"code","11c9dda5":"code","5f6b8442":"code","e51f5b8b":"code","c081b460":"code","cfa1c822":"code","4439d3e0":"code","baf6435a":"code","c1ac1b8d":"code","e1e1d858":"code","b8e30713":"code","ca5b701f":"code","497b7086":"code","edc7b8d3":"code","04f898e5":"code","80739f12":"code","fa0a01da":"code","a8c17b15":"markdown","92d40fab":"markdown","88e80572":"markdown","f1586b66":"markdown","7293c159":"markdown","32f431f8":"markdown","2aab729a":"markdown","4181487a":"markdown","063c706c":"markdown","00cbf49f":"markdown","73e9568d":"markdown","823f0931":"markdown","39b71c5b":"markdown","8581323b":"markdown","b07b3436":"markdown","59b7c1cf":"markdown","e5d0991c":"markdown","ef9b063f":"markdown","07d2113b":"markdown","0527d045":"markdown","3a284c1b":"markdown","1566d0f4":"markdown","efa8744d":"markdown","f19bafcf":"markdown","1be9e388":"markdown"},"source":{"e78fa388":"from IPython.display import Image\nImage(\"..\/input\/Zebra_Test_Image.jpg\")","e1ddec77":"PART_1_SKIP = True\nPART_2_SKIP = True\nPART_3_SKIP = True","1e4eda57":"import numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\n\nimport skimage\nfrom PIL import Image\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.util import crop, pad\nfrom skimage.morphology import label\nfrom skimage.color import rgb2gray, gray2rgb\n\nimport os\n\nimport zipfile\nz = zipfile.ZipFile(\"Zebra_Color_Images.zip\", \"w\")","7b4e031e":"cap = cv2.VideoCapture('..\/input\/Dancing_Zebra_Footage.mp4')\nprint(cap.get(cv2.CAP_PROP_FPS))","af666838":"%%time\n\nif PART_1_SKIP == False:\n    try:\n        if not os.path.exists('data'):\n            os.makedirs('data')\n    except OSError:\n        print ('Error: Creating directory of data')\n\n    currentFrame = 0\n    count = 0\n    TRAIN_SIZE = 21000\n    FRAME_SKIP = 2\n    IMG_WIDTH = 96\n    IMG_HEIGHT = 64\n    IMG_CHANNELS = 3\n    X_train = np.zeros((TRAIN_SIZE, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype='float32')\n    \n    video = cv2.VideoWriter('Simple_Zebra_Dancer_Video_C.avi',cv2.VideoWriter_fourcc(*\"MJPG\"), 30, (IMG_WIDTH, IMG_HEIGHT), True)\n\n    while(count < TRAIN_SIZE):\n        try:\n            ret, frame = cap.read()\n\n            if currentFrame % FRAME_SKIP == 0:\n                count += 1\n                if count % int(TRAIN_SIZE\/10) == 0:\n                    print(str((count\/TRAIN_SIZE)*100)+\"% done\")\n                # preprocess frames\n                img = frame\n                img = resize(img, (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), mode='constant', preserve_range=True)\n                img = (img).astype('uint8')\n                # save frame to zip and new video sample\n                name = '.\/data\/frame' + str(count) + '.jpg'\n                cv2.imwrite(name, img)\n                video.write(img)\n                z.write(name)\n                os.remove(name)\n                # save image to training set if training directly to part 2\n                img = img.astype('float32') \/ 255.\n                X_train[count] = img\n        except:\n            print('Frame error')\n            break\n        currentFrame += 1\n\n    print(str(count)+\" Frames collected\")\n    cap.release()\n    z.close()\n    video.release()","cb1f4a7d":"import os\nimport sys\nimport random\nimport warnings\nfrom pylab import imshow, show, get_cmap\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom itertools import chain\nimport skimage\nfrom PIL import Image\nfrom skimage.io import imread, imshow, imread_collection, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.util import crop, pad\nfrom skimage.morphology import label\n\nfrom keras.models import Model, load_model, Sequential\nfrom keras.layers import Input, Dense, UpSampling2D, Flatten, Reshape\nfrom keras.layers.core import Dropout, Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, Callback\nfrom keras import backend as K\nimport tensorflow as tf\n\nwarnings.filterwarnings('ignore', category=UserWarning, module='skimage')\nseed = 42\nrandom.seed = seed\nnp.random.seed = seed","f7f693b1":"%%time\nif PART_1_SKIP:\n    IMG_WIDTH = 96\n    IMG_HEIGHT = 64\n    IMG_CHANNELS = 3\n    INPUT_SHAPE=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n    TRAIN_PATH = '..\/input\/zebra_color_images\/data\/'\n#     train_ids = next(os.walk(TRAIN_PATH))[2]\n    X_train = np.zeros((21000, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype='float32')\n    missing_count = 0\n    print('Getting training images ... ')\n#     sys.stdout.flush()\n    for n, id_ in tqdm(enumerate(X_train), total=21000):\n        path = TRAIN_PATH +'frame'+ str(n+1) + '.jpg'\n        try:\n            img = imread(path)\n            img = img.astype('float32') \/ 255.\n#             img = resize(img, (IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), mode='constant', preserve_range=True)\n            X_train[n-missing_count] = img\n        except:\n            print(\" Problem with: \"+path)\n            missing_count += 1\n\n    print(\"Done! total missing: \"+ str(missing_count))\n    X_train = X_train[75:]\nelse:\n    INPUT_SHAPE=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n    X_train = X_train[75:]","65cc37e9":"for n in range(0,5):\n    imshow(X_train[n])\n    plt.show()","e833a8b0":"def Encoder():\n    inp = Input(shape=INPUT_SHAPE)\n    x = Conv2D(128, (4, 4), activation='elu', padding='same',name='encode1')(inp)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode2')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode3')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same',name='encode4')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode5')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same',name='encode6')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode7')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same',name='encode8')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(32, (3, 3), activation='elu', padding='same',name='encode9')(x)\n    x = Flatten()(x)\n    x = Dense(256, activation='elu',name='encode10')(x)\n    encoded = Dense(128, activation='sigmoid',name='encode11')(x)\n    return Model(inp, encoded)\n\nencoder = Encoder()\nencoder.summary()","1c0c668c":"D_INPUT_SHAPE=[128]\ndef Decoder():\n    inp = Input(shape=D_INPUT_SHAPE, name='decoder')\n    x = Dense(256, activation='elu', name='decode1')(inp)\n    x = Dense(768, activation='elu', name='decode2')(x)\n    x = Reshape((4, 6, 32))(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same', name='decode3')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same', name='decode4')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same', name='decode5')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same', name='decode6')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(64, (2, 2), activation='elu', padding='same', name='decode7')(x)\n    x = Conv2D(128, (3, 3), activation='elu', padding='same', name='decode8')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(64, (2, 2), activation='elu', padding='same', name='decode9')(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same', name='decode10')(x)\n    x = Conv2D(128, (3, 3), activation='elu', padding='same', name='decode11')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same', name='decode12')(x)\n    x = Conv2D(32, (3, 3), activation='elu', padding='same', name='decode13')(x)\n    x = Conv2D(16, (2, 2), activation='elu', padding='same', name='decode14')(x)\n    decoded = Conv2D(IMG_CHANNELS, (2, 2), activation='sigmoid', padding='same', name='decode15')(x)\n    return Model(inp, decoded)\n\ndecoder = Decoder()\ndecoder.summary()","2d8c3ac5":"def Autoencoder():\n    inp = Input(shape=INPUT_SHAPE)\n    x = Conv2D(128, (4, 4), activation='elu', padding='same',name='encode1')(inp)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode2')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode3')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same',name='encode4')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode5')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same',name='encode6')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same',name='encode7')(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same',name='encode8')(x)\n    x = MaxPooling2D((2, 2), padding='same')(x)\n    x = Conv2D(32, (3, 3), activation='elu', padding='same',name='encode9')(x)\n    x = Flatten()(x)\n    x = Dense(256, activation='elu',name='encode10')(x)\n    encoded = Dense(128, activation='sigmoid',name='encode11')(x)\n    x = Dense(256, activation='elu', name='decode1')(encoded)\n    x = Dense(768, activation='elu', name='decode2')(x)\n    x = Reshape((4, 6, 32))(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same', name='decode3')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same', name='decode4')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(32, (2, 2), activation='elu', padding='same', name='decode5')(x)\n    x = Conv2D(64, (3, 3), activation='elu', padding='same', name='decode6')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(64, (2, 2), activation='elu', padding='same', name='decode7')(x)\n    x = Conv2D(128, (3, 3), activation='elu', padding='same', name='decode8')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(64, (2, 2), activation='elu', padding='same', name='decode9')(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same', name='decode10')(x)\n    x = Conv2D(128, (3, 3), activation='elu', padding='same', name='decode11')(x)\n    x = UpSampling2D((2, 2))(x)\n    x = Conv2D(64, (4, 4), activation='elu', padding='same', name='decode12')(x)\n    x = Conv2D(32, (3, 3), activation='elu', padding='same', name='decode13')(x)\n    x = Conv2D(16, (2, 2), activation='elu', padding='same', name='decode14')(x)\n    decoded = Conv2D(IMG_CHANNELS, (2, 2), activation='sigmoid', padding='same', name='decode15')(x)\n    return Model(inp, decoded)\n\nmodel = Autoencoder()\nmodel.compile(optimizer='adam', loss='mean_squared_error')\nmodel.summary()","16d16b41":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n                                            patience=4, \n                                            verbose=1, \n                                            factor=0.5,\n                                            min_lr=0.00001)\n\ncheckpoint = ModelCheckpoint(\"Zebra_Auto_Model_C.hdf5\",\n                             save_best_only=True,\n                             monitor='val_loss',\n                             mode='min')\n\nearly_stopping = EarlyStopping(monitor='val_loss',\n                              patience=8,\n                              verbose=1,\n                              mode='min',\n                              restore_best_weights=True)","b9f31592":"class ImgSample(Callback):\n\n    def __init__(self):\n       super(Callback, self).__init__() \n\n    def on_epoch_end(self, epoch, logs={}):\n        sample_img = X_train[50]\n        sample_img = sample_img.reshape(1, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)\n        sample_img = self.model.predict(sample_img)[0]\n        imshow(sample_img)\n        plt.show()\n\n\nimgsample = ImgSample()\nmodel_callbacks = [learning_rate_reduction, checkpoint, early_stopping, imgsample]\nimshow(X_train[50])","3456a8d8":"%%time\nif PART_2_SKIP == False:\n    model.fit(X_train, X_train,\n              epochs=40, \n              batch_size=32,\n              verbose=2,\n              validation_split=0.05,\n            callbacks=model_callbacks)\nelse:\n    model = load_model('..\/input\/Zebra_Auto_Model_C.hdf5')\n    model.load_weights(\"..\/input\/Zebra_Auto_Weights_C.hdf5\")","0db9a1ec":"decoded_imgs = model.predict(X_train)","77bf044a":"plt.figure(figsize=(20, 4))\nfor i in range(5,10):\n    # original\n    plt.subplot(2, 10, i + 1)\n    plt.imshow(X_train[i])\n    plt.axis('off')\n \n    # reconstruction\n    plt.subplot(2, 10, i + 1 + 10)\n    plt.imshow(decoded_imgs[i])\n    plt.axis('off')\n \nplt.tight_layout()\nplt.show()","11c9dda5":"model.save('Zebra_Auto_Model_C.hdf5')\nmodel.save_weights(\"Zebra_Auto_Weights_C.hdf5\")","5f6b8442":"encoder = Encoder()\ndecoder = Decoder()\n\nencoder.load_weights(\"Zebra_Auto_Weights_C.hdf5\", by_name=True)\ndecoder.load_weights(\"Zebra_Auto_Weights_C.hdf5\", by_name=True)\n\ndecoder.save('Zebra_Decoder_Model_C.hdf5') \nencoder.save('Zebra_Encoder_Model_C.hdf5')\n\ndecoder.save_weights(\"Zebra_Decoder_Weights_C.hdf5\")\nencoder.save_weights(\"Zebra_Encoder_Weights_C.hdf5\")","e51f5b8b":"encoder_imgs = encoder.predict(X_train)\nprint(encoder_imgs.shape)\nnp.save('Encoded_Zebra_C.npy',encoder_imgs)","c081b460":"decoded_imgs = decoder.predict(encoder_imgs[0:11])\n\nplt.figure(figsize=(20, 4))\nfor i in range(5,10):\n    # reconstruction\n    plt.subplot(1, 10, i + 1)\n    plt.imshow(decoded_imgs[i])\n    plt.axis('off')\n \nplt.tight_layout()\nplt.show()","cfa1c822":"import numpy as np\nimport pandas as pd\nimport keras as K\nimport random\nimport sqlite3\nimport cv2\nimport os\n\nfrom skimage.color import rgb2gray, gray2rgb\nfrom skimage.transform import resize\nfrom skimage.io import imread, imshow\nimport matplotlib.pyplot as plt\n\nfrom keras.layers import Input, Dropout, Dense, concatenate, Embedding\nfrom keras.layers import Flatten, Activation\nfrom keras.optimizers import Adam\nfrom keras.models import Model\nfrom keras.utils import np_utils\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.models import load_model\nfrom keras.layers import LSTM, CuDNNGRU, CuDNNLSTM\nfrom keras.layers import MaxPooling1D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, Callback\n\nimport warnings\nwarnings.filterwarnings('ignore')","4439d3e0":"if PART_2_SKIP:\n    Dance_Data = np.load('..\/input\/Encoded_Zebra_C.npy')\nelse:\n    Dance_Data = encoder_imgs\n\nDance_Data.shape","baf6435a":"TRAIN_SIZE = Dance_Data.shape[0]\nINPUT_SIZE = Dance_Data.shape[1]\nSEQUENCE_LENGTH = 70\n\nX_train = np.zeros((TRAIN_SIZE-SEQUENCE_LENGTH, SEQUENCE_LENGTH, INPUT_SIZE), dtype='float32')\nY_train = np.zeros((TRAIN_SIZE-SEQUENCE_LENGTH, INPUT_SIZE), dtype='float32')\nfor i in range(0, TRAIN_SIZE-SEQUENCE_LENGTH, 1 ): \n    X_train[i] = Dance_Data[i:i + SEQUENCE_LENGTH]\n    Y_train[i] = Dance_Data[i + SEQUENCE_LENGTH]\n\nprint(X_train.shape)\nprint(Y_train.shape)","c1ac1b8d":"def get_model():\n    inp = Input(shape=(SEQUENCE_LENGTH, INPUT_SIZE))\n    x = CuDNNLSTM(512, return_sequences=True,)(inp)\n    x = CuDNNLSTM(256, return_sequences=True,)(x)\n    x = CuDNNLSTM(512, return_sequences=True,)(x)\n    x = CuDNNLSTM(256, return_sequences=True,)(x)\n    x = CuDNNLSTM(512, return_sequences=True,)(x)\n    x = CuDNNLSTM(1024,)(x)\n    x = Dense(512, activation=\"elu\")(x)\n    x = Dense(256, activation=\"elu\")(x)\n    outp = Dense(INPUT_SIZE, activation='sigmoid')(x)\n    \n    model = Model(inputs=inp, outputs=outp)\n    model.compile(loss='mse',\n                  optimizer=Adam(lr=0.0002),\n                  metrics=['accuracy'],\n                 )\n\n    return model\n\nmodel = get_model()\n\nmodel.summary()","e1e1d858":"checkpoint = ModelCheckpoint(\"AI_Zebra_RNN_Model_C.hdf5\",\n                             monitor='loss',\n                             verbose=1,\n                             save_best_only=True,\n                             mode='min')\n\nearly = EarlyStopping(monitor=\"loss\",\n                      mode=\"min\",\n                      patience=3,\n                     restore_best_weights=True)\n\nmodel_callbacks = [checkpoint, early]","b8e30713":"%%time\nif PART_3_SKIP == False:\n    model.fit(X_train, Y_train,\n              batch_size=64,\n              epochs=40,\n              verbose=2,\n              callbacks = model_callbacks)\nelse:\n    model = load_model('..\/input\/AI_Zebra_RNN_Model_C.hdf5')\n    model.load_weights('..\/input\/AI_Zebra_RNN_Weights_C.hdf5')","ca5b701f":"model.save(\"AI_Zebra_RNN_Model_C.hdf5\")\nmodel.save_weights('AI_Zebra_RNN_Weights_C.hdf5')","497b7086":"%%time\nVIDEO_LENGTH = 6000\nLOOPBREAKER = 10\n\nx = np.random.randint(0, X_train.shape[0]-1)\npattern = X_train[x]\noutp = np.zeros((VIDEO_LENGTH, INPUT_SIZE), dtype='float32')\nfor t in range(VIDEO_LENGTH):\n    x = np.reshape(pattern, (1, pattern.shape[0], pattern.shape[1]))\n    pred = model.predict(x)\n    result = pred[0]\n    outp[t] = result\n    new_pattern = np.zeros((SEQUENCE_LENGTH, INPUT_SIZE), dtype='float32') \n    new_pattern[0:SEQUENCE_LENGTH-1] = pattern[1:SEQUENCE_LENGTH]\n    new_pattern[-1] = result\n    pattern = np.copy(new_pattern)\n    ####loopbreaker####\n    if t % LOOPBREAKER == 0:\n        pattern[np.random.randint(0, SEQUENCE_LENGTH-10)] = Y_train[np.random.randint(0, Y_train.shape[0]-1)]","edc7b8d3":"if PART_2_SKIP:\n    Decoder = load_model('..\/input\/Zebra_Decoder_Model_C.hdf5')\n    Decoder.load_weights('..\/input\/Zebra_Decoder_Weights_C.hdf5')\nelse:\n    Decoder = load_model('Zebra_Decoder_Model_C.hdf5')\n    Decoder.load_weights('Zebra_Decoder_Weights_C.hdf5')\n\nAI_Output = Decoder.predict(outp)\nAI_Output.shape","04f898e5":"IMG_HEIGHT = AI_Output[0].shape[0]\nIMG_WIDTH = AI_Output[0].shape[1]\n\nfor row in AI_Output[0:10]:\n    imshow(row)\n    plt.show()","80739f12":"video = cv2.VideoWriter('AI_Zebra_Video.avi', cv2.VideoWriter_fourcc(*\"XVID\"), 20.0, (IMG_WIDTH, IMG_HEIGHT),True)\n\nfor img in AI_Output:\n    img = img * 255\n    img = img.astype('uint8')\n    video.write(img)\n    cv2.waitKey(50)\n    \nvideo.release()","fa0a01da":"from IPython.display import Image\nImage(\"..\/input\/Zebra_Test_Image.jpg\")","a8c17b15":"## Train RNN Model\n\nOne change I am making to this model over the black and white version is to cut back the epochs to 40. Not only will this save time but since the zebra was so overfit the last 40 epochs was just excessive. I hope that cutting back the epochs might yield more interesting results in the color version.","92d40fab":"# Conclusion\n\nThe models still did a good job even with color channels. The results are boring like with the black and white data. I am a bit disappointed that there was not any \"rainbow glitches\" like with gans and other color image predictions. This was likely due to the video still being predominantly  tri-colored( black, white, and green). Using more complex color videos I hope will make some interesting \"rainbow glitches\" and larger training set to take care of overfitting.\n\nIf you enjoyed and learned something from this notebook, please like, comment, and check out some of my other coding projects on Kaggle and Github.","88e80572":"## Save Models and Create Encoded Dataset","f1586b66":"## Save Video","7293c159":"### Custom Image Sample Callback\n\nHere is a custom callback I made named ImgSample. It tests the result of the autoencoder after every epoch by desplaying an sample image. The goal is to have the dancer come into focus as clearly as possible.","32f431f8":"## Decode a Sample to Double Check Results\n\nIf the encoder and decoder models are working correctly, the zebra should appear like in the reconstruction of the autoencoder above.","2aab729a":"## Sample the Autoencoder Results\n\nIf the reconstructions look pretty close to the originals, then the autoencoder works.","4181487a":"# AI Dance Part 2: Autoencoder Compression\n\nNow that we have the preprocessed frames from the zebra video, we will still need to compress them much further to fit them into our RNN model. Among the many uses of autoencoders is making specialized compression models. In this section, we will train an autoencoder on our zebra images and use it to compress the images into a much smaller numpy array, saving the model so that we can decode the images later.","063c706c":"# AI Zebra Part 1: Video Preprocessing (In Color!)\n\nThe video is ~23 minutes of a guy in a zebra outfit preforming a veriety of actions. This is ideal as most other green screen dancing videos are too short, loops, and\/or messy for easy preprocessing. While it could be easily done, I will not take a binary shadow of the zebra (unlike the dancing silloette video I used in the first notebook). I am just curious how the AI handles the texture (and color). There is also little need to cut out the green screen but the simple continous background will help the computer focus on the zebra's movements and make it easier to compress. (The dancing video in the first notebook had a constanly changing background that needed to be removed for simple processing)\n\nThe original Last Week Tonight zebra video is here: https:\/\/youtu.be\/-Z668Qc0P4Q","00cbf49f":"## Read in Images","73e9568d":"## Create the Models\n\nIn addition to the Autoencoder model, we will also prepare an encoder and decoder for later. It is important to give the layers the same unique names and shapes in all 3 as we will be using the keras load_weights by_name option to copy our trained Autoencoder weights to each respective layer later. I'm leaving these models the exact same shape as in the original AI dance notebook to see how it does in comparison.","823f0931":"## Callbacks","39b71c5b":"## Train the Autoencoder","8581323b":"## Callbacks","b07b3436":"## Part 1 Results\n\nThe zebra looks a bit too low res at 64 by 96. The shadow dancer did not look quite as bad due to less detail. \n\n### Possible Improvments and Thoughts\n\n- There are also title frames at the bigining that need to be cut out\n\n- Taking every one of the 42,000 frames ends up being a problem because changes frame to frame are too small and the RNN will need a much longer sequence length to avoid the AI getting frozen. The zebra often moves slower or has subtle movements so taking every 3 frames might be a good idea.\n\n- Zebra might need a larger size to really look good.\n","59b7c1cf":"## Preprocess the Video\n\nIn this step we will take each frame in the video and add them to a zip file in sequence. We will also preprocess the frames in the following way to save space and make it easier for are models to process them later:\n\n- Only take every other frame: We don't need every frame and will mean that we won't need to use as many frames to look further back in time during with the RNN model later. \n\n- Resize the image to 64 by 96 pixels: Much smaller file size and 64 by 96 is easily divided which makes it easier to structure the autoencoder without data loss later.\n\n- Round down the total frames in the video to 21,000 frames. we could get more but the title screen at the end is worthless to us.\n","e5d0991c":"# Part 3 Results\n\nThe zebra comes out pretty accurately. It's movements are slower and less interesting than the dance video. The arms disappear a bit when the zebra moves them quickly. You can see some freezing when the zebra stops moving for a bit, probably because the movements during those frames are so minute that the computer treats them as the same.\n\n### Possible Improvements and Thoughts\n\n- The video is pretty overfit and could use more training data. However, the zebra video is pretty much tapped out at this point. With the often slow movements in the video, taking more frames would just lead to a much heavier demand on the models without any real gain.\n\n- The autoencoder is showing it's limits with this color data. The arms are getting too blurry and the stopsign is a mild red blur. I think that now is the time to start encoding to a larger numpy array.\n","ef9b063f":"## Read in Data\n\nWhen processing this type of model on text data, each character is expressed in one hot arrays between ~50-100, (depending on the unique characters in the text to consider). Our data is in 128 numpy arrays, so it is not that much more load on our model to consider our compressed images over single characters of a text document.","07d2113b":"## Create Compressed Dance Sequences\n\nOur model will look at the last 70 frames and attemp to predict the 71st. As such, sur X variable will be an array of 70 (compressed) frames in sequence and our Y variable will be the 71st frame. This block chops our Dance_Data into such sequences of frames.","0527d045":"## Generate New Computer Generated Dances\n\nThis block generates new video sequences in the style of the video of VIDEO_LENGTH size in frames. It takes a random seed pattern from the training set, predicts the next frame, adds it to the end of the pattern and drops the first frame of the pattern and predicts on the new pattern and so forth. The default VIDEO_LENGTH of 6000 frames is 5 minutes of video at 20 FPS.\n\nPretty much the AI will try to accurately duplicate the video but inevitably makes errors, and those errors compound, but is still trained well enough that it ends up making similar, but not quite the same, dances.\n\nThe LOOPBREAKER is used to add noise to the prediction pattern, replacing a random frame in the pattern with a random frame in the Dance_Data after every LOOPBREAKER frames. This noise can be used to force the AI to change up what it is doing. This can stop undertrained models from looping or overtrained models from duplication the training data too closely. Setting it too low, on the other hand, can cause the results to distort more. It is worth playing around with this setting and is a quick and dirty way to adjust the dance output post training.","3a284c1b":"# AI Dance Part 3: Train AI w\/ RNNs\n\nIf you have read any of my text generating notebooks or know text generating AIs this next part will be familiar with you. If not, here is one of my related notebooks: \n\nThe Pythonic Python Script for Making Monty Python Scripts: https:\/\/www.kaggle.com\/valkling\/pythonicpythonscript4makingmontypythonscripts\n\nFor the zebra AI, the technique is pretty much the same. We will use our compressed pictures to make n length sequences as input that the model will use to predict the n+1 frame in the sequence. The differences are:\n\n- The input\/outputs will not be in one-hot encoding but rather an array of floats between 0 and 1.\n\n- We will need a larger brain for our model to make it work.\n\n- We will need to decode the results after to turn them into a usable video.\n\nOnce again, I will be sticking with the same structure as in the dance AI notebook. Keeping with it until it hits something it cannot handle.\n","1566d0f4":"## Output the Dance\n\nBefore we can save the video, we need to decode the frames back into images using the decoder we made in part 2.","efa8744d":"## Create the RNN Model\n\nThe model is simply 6 LSTM layers stacked on top of each other. While text data only needs around 2-4 LSTM layers to work, the dance data benifits from a few more as the result is not categorical this time and a large brain allows for more \"creativity\"(variation) on the AIs part. (Note: CuDNNLSTM layers are just LSTM layers that automatically optimize for the GPU. They run a lot faster than standard LSTM layers at the cost of customization options)","f19bafcf":"## Part 2 Results\n\nThe results, like with the dancer video, are really good. Again there is only really a bit of blurriness around the hands after decoding.\n\n### Possible Improvements and Thoughts\n\n- The autoencoder still works alright but is showing it's limits with color data.\n\n- The Autoencoder could be used to make a much much larger training set. Even if the uncompressed images get to big for the memory limit, it is possible to just train the autoencoder on a subset of the images then compress the whole set after. A 128 array is not that big, I don't foresee resource exhaustion errors being an major issue, even for much larger datasets.\n","1be9e388":"# Teaching an AI to \"Zebra\" (In Color!)\n\n(Since the zebra AI worked so well in black and white, here is an attempt using color. The models are kept the same, except where it necessary, like the input\/output shapes of the autoencoder)\n\nLast Week Tonight released a green screen video of a zebra dancing and doing various other activities for viewers to edit into their own videos. This video is actually pretty good for video processing algorithms and AI video training. Let's try using this to create our own AI generated zebra dancing video.\n\nThis is a variation of my original notebook How to Teach an AI to Dance with new data: https:\/\/www.kaggle.com\/valkling\/how-to-teach-an-ai-to-dance\n\nWatch a sample output from this notebook here: https:\/\/youtu.be\/_Eq-u67ZJRI\n\nNLP and image CNNs are all the rage right now, here we combine techniques from both to have a computer learn to make it's own dance videos. This notebook is a consolidation of 3 smaller notebooks I made for this project: \n\nPart 1-Video Preprocessing: We will take the frames from the zebra video, preprocess them to smaller and simpler, and add them to a zip file in sequence.\n\nPart 2-Autoencoder Compression: To save even more memory for our model, we will compress these frames with an Autoencoder into a much smaller numpy array.\n\nPart 3-Train AI w\/ RNNs: We will put these compressed frames into sequences and train a model to create more.\n\n\nI based the original How to Teach an AI to Dance notebook off the project in this youtube video: https:\/\/www.youtube.com\/watch?v=Sc7RiNgHHaE While he does not share his code, the steps expressed in the video were clear enough to piece together this project. Thanks to Kaggle's kernals GPU and some alterations, we can achieve even better results in less time than what is shown in the video. While still pretty computationally expensive for modern computing power, using these techniques for a dancing AI opens up the groundwork for AI to predict on and create all types of different videos.\n\n\n### Skip Training\n\nThe results of the 3 parts are already recorded in the dataset and each part can work independently from each other by loading the pretrained data. Setting the following variables to *True* will skip the training for that part and just use the pretrained data instead. *False* will train through that step.\n\nTop to bottom, this whole notebook takes around ~2 hours to train with all 3 skips set to *False*. \n"}}