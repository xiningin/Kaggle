{"cell_type":{"07cca1d1":"code","f18bd48f":"code","d4442fec":"code","f142e427":"code","81fe3957":"code","d9effc04":"code","1bc0dab2":"code","505d424d":"code","94bbbd8b":"code","63722d00":"code","108b3036":"code","fc1d85c7":"code","476c3909":"code","a78f8d47":"code","c89fafde":"code","9e4aa8a3":"code","fa29efaf":"code","b29f5ceb":"code","ba2d91fc":"code","ec64391c":"code","c7442be3":"code","ee0fc428":"code","abd5c32a":"code","1819cb70":"code","f356148f":"code","d555f9e9":"code","c50a5f58":"code","b142bd0f":"code","26aaecee":"code","7b582e9b":"code","33eaddfb":"code","31567fca":"code","1dbc7980":"code","0f6c4b04":"code","f3808b71":"code","f4fb9eaa":"code","ef9daefe":"code","61df70dc":"code","19ebacfc":"code","c72c5e5d":"code","3409345c":"code","315877ea":"code","adb3d5bd":"code","7f7fc46c":"code","8628a58a":"code","f91be72e":"code","41ea8dce":"code","b9a27911":"markdown","bc5ff304":"markdown","0ae58379":"markdown","2ef8f89e":"markdown","5a77bc84":"markdown","d6b91d40":"markdown","b1a45903":"markdown","6a6eab2c":"markdown","ae66a190":"markdown","da43ae07":"markdown","0bf3deab":"markdown","f7df4ad7":"markdown","503a45c8":"markdown","f51ee4e6":"markdown","fcde1fd3":"markdown","2ea3854a":"markdown","e470e1d4":"markdown","bd5827c0":"markdown","dcf0b1a0":"markdown","1a8b1fe6":"markdown","8ea193e9":"markdown","f1e85db8":"markdown","0909fe5f":"markdown","88e38a61":"markdown"},"source":{"07cca1d1":"%%HTML\n<style type=\"text\/css\">\ndiv.h1 {\n    background-color:#e17b34; \n    color: white; \n    padding: 8px; \n    padding-right: 300px; \n    font-size: 35px; \n    max-width: 1500px; \n    margin: auto; \n    margin-top: 50px;\n}\n\ndiv.h2 {\n    background-color:#83ccd2; \n    color: white; \n    padding: 8px; \n    padding-right: 300px; \n    font-size: 35px; \n    max-width: 1500px; \n    margin: auto; \n    margin-top: 50px;\n}\n<\/style>","f18bd48f":"import sys\nsys.path.append('..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","d4442fec":"import numpy as np\nimport random\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport copy\nimport seaborn as sns\n\nfrom sklearn import preprocessing\nfrom sklearn.metrics import log_loss\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nimport warnings\nwarnings.filterwarnings('ignore')","f142e427":"from sklearn.preprocessing import QuantileTransformer","81fe3957":"os.listdir('..\/input\/lish-moa')","d9effc04":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","1bc0dab2":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","505d424d":"gnum = train_features[GENES].shape[1]\ngraphs = []\n\nfor i in range(0, gnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=771:\n            break\n        graph = sns.distplot(train_features[GENES].values[:,item], ax=axs[k])\n        graph.set_title(f\"g-{item}\")\n        graphs.append(graph)","94bbbd8b":"cnum = train_features[CELLS].shape[1]\ngraphs = []\n\nfor i in range(0, cnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=100:\n            break\n        graph = sns.distplot(train_features[CELLS].values[:,item], ax=axs[k])\n        graph.set_title(f\"c-{item}\")\n        graphs.append(graph)","63722d00":"gnum = test_features[GENES].shape[1]\ngraphs = []\n\nfor i in range(0, gnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=771:\n            break\n        graph = sns.distplot(test_features[GENES].values[:,item], ax=axs[k])\n        graph.set_title(f\"g-{item}\")\n        graphs.append(graph)","108b3036":"cnum = test_features[CELLS].shape[1]\ngraphs = []\n\nfor i in range(0, cnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=100:\n            break\n        graph = sns.distplot(test_features[CELLS].values[:,item], ax=axs[k])\n        graph.set_title(f\"c-{item}\")\n        graphs.append(graph)","fc1d85c7":"#RankGauss\n\nfor col in (GENES + CELLS):\n\n    transformer = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","476c3909":"gnum = train_features[GENES].shape[1]\ngraphs = []\n\nfor i in range(0, gnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=771:\n            break\n        graph = sns.distplot(train_features[GENES].values[:,item], ax=axs[k])\n        graph.set_title(f\"g-{item}\")\n        graphs.append(graph)","a78f8d47":"cnum = train_features[CELLS].shape[1]\ngraphs = []\n\nfor i in range(0, cnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=100:\n            break\n        graph = sns.distplot(train_features[CELLS].values[:,item], ax=axs[k])\n        graph.set_title(f\"c-{item}\")\n        graphs.append(graph)","c89fafde":"gnum = test_features[GENES].shape[1]\ngraphs = []\n\nfor i in range(0, gnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=771:\n            break\n        graph = sns.distplot(test_features[GENES].values[:,item], ax=axs[k])\n        graph.set_title(f\"g-{item}\")\n        graphs.append(graph)","9e4aa8a3":"cnum = test_features[CELLS].shape[1]\ngraphs = []\n\nfor i in range(0, cnum -1 , 7):\n    #for least display.... \n    if i >= 3:\n        break\n    idxs = list(np.array([0, 1, 2, 3, 4, 5, 6]) + i)\n    \n\n    fig, axs = plt.subplots(1, 7, sharey=True)\n    for k, item in enumerate(idxs):\n        if item >=100:\n            break\n        graph = sns.distplot(test_features[CELLS].values[:,item], ax=axs[k])\n        graph.set_title(f\"c-{item}\")\n        graphs.append(graph)","fa29efaf":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","b29f5ceb":"train_targets_scored.sum()[1:].sort_values()","ba2d91fc":"train_features['cp_type'].unique()","ec64391c":"# GENES\nn_comp = 50\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","c7442be3":"#CELLS\nn_comp = 15\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","ee0fc428":"from sklearn.feature_selection import VarianceThreshold\n\n\nvar_thresh = VarianceThreshold(threshold=0.5)\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)\n\ntrain_features\n","abd5c32a":"train = train_features.merge(train_targets_scored, on='sig_id')\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntest = test_features[test_features['cp_type']!='ctl_vehicle'].reset_index(drop=True)\n\ntarget = train[train_targets_scored.columns]","1819cb70":"train = train.drop('cp_type', axis=1)\ntest = test.drop('cp_type', axis=1)","f356148f":"train","d555f9e9":"# for col in GENES:\n#     train.loc[:, f'{col}_bin'] = pd.cut(train[col], bins=3, labels=False)\n#     test.loc[:, f'{col}_bin'] = pd.cut(test[col], bins=3, labels=False)\n","c50a5f58":"# plt.figure(figsize=(16,16))\n# sns.set_style(\"whitegrid\")\n\n# gene_choice = np.random.choice(len(GENES), 16)\n# for i, col in enumerate(gene_choice):\n#     plt.subplot(4, 4, i+1)\n#     plt.hist(train_features.loc[:, GENES[col]],bins=100, color='orange')\n#     plt.title(GENES[col])","b142bd0f":"\n# train_ = train.copy() [Didn't wanted to actually normalize, so created a copy and normalized that for further calculation]\n# for col in GENES:\n    \n# #     train_[col] = (train[col]-np.mean(train[col])) \/ (np.std(train[col]))\n    \n#     mean = train_[col].mean()\n#     std = train_[col].std()\n\n#     std_r = mean + 4*std\n#     std_l = mean - 4*std\n\n#     drop = train_[col][(train_[col]>std_r) | (train_[col]<std_l)].index.values\n\n# train = train.drop(drop).reset_index(drop=True)\n# # folds = folds.drop(drop).reset_index(drop=True)\n# target = target.drop(drop).reset_index(drop=True)\n","26aaecee":"# n_comp = 50\n\n# data = pd.concat([pd.DataFrame(train[CELLS]), pd.DataFrame(test[CELLS])])\n# data2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\n# train2 = data2[:train.shape[0]]; test2 = data2[train.shape[0]:]\n\n# train2 = pd.DataFrame(train2, columns=[f'c-{i}' for i in range(n_comp)])\n# test2 = pd.DataFrame(test2, columns=[f'c-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\n# train = train.drop(columns=drop_cols)\n# test = test.drop(columns=drop_cols)","7b582e9b":"target_cols = target.drop('sig_id', axis=1).columns.values.tolist()","33eaddfb":"folds = train.copy()\n\nmskf = MultilabelStratifiedKFold(n_splits=5)\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train, y=target)):\n    folds.loc[v_idx, 'kfold'] = int(f)\n\nfolds['kfold'] = folds['kfold'].astype(int)\nfolds","31567fca":"print(train.shape)\nprint(folds.shape)\nprint(test.shape)\nprint(target.shape)\nprint(sample_submission.shape)","1dbc7980":"class MoADataset:\n    def __init__(self, features, targets):\n        self.features = features\n        self.targets = targets\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n        }\n        return dct\n    \nclass TestDataset:\n    def __init__(self, features):\n        self.features = features\n        \n    def __len__(self):\n        return (self.features.shape[0])\n    \n    def __getitem__(self, idx):\n        dct = {\n            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n        }\n        return dct\n    ","0f6c4b04":"def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n    model.train()\n    final_loss = 0\n    \n    for data in dataloader:\n        optimizer.zero_grad()\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n#         print(inputs.shape)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        \n        final_loss += loss.item()\n        \n    final_loss \/= len(dataloader)\n    \n    return final_loss\n\n\ndef valid_fn(model, loss_fn, dataloader, device):\n    model.eval()\n    final_loss = 0\n    valid_preds = []\n    \n    for data in dataloader:\n        inputs, targets = data['x'].to(device), data['y'].to(device)\n        outputs = model(inputs)\n        loss = loss_fn(outputs, targets)\n        \n        final_loss += loss.item()\n        valid_preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    final_loss \/= len(dataloader)\n    valid_preds = np.concatenate(valid_preds)\n    \n    return final_loss, valid_preds\n\ndef inference_fn(model, dataloader, device):\n    model.eval()\n    preds = []\n    \n    for data in dataloader:\n        inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(inputs)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    \n    return preds\n   \n    ","f3808b71":"class Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size):\n        super(Model, self).__init__()\n        self.batch_norm1 = nn.BatchNorm1d(num_features)\n        self.dropout1 = nn.Dropout(0.2)\n        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n        \n        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n        self.dropout2 = nn.Dropout(0.5)\n        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n        \n        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n        self.dropout3 = nn.Dropout(0.5)\n        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n    \n    def forward(self, x):\n        x = self.batch_norm1(x)\n        x = self.dropout1(x)\n        x = F.relu(self.dense1(x))\n        \n        x = self.batch_norm2(x)\n        x = self.dropout2(x)\n        x = F.relu(self.dense2(x))\n        \n        x = self.batch_norm3(x)\n        x = self.dropout3(x)\n        x = self.dense3(x)\n        \n        return x","f4fb9eaa":"def process_data(data):\n    \n    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n#     data.loc[:, 'cp_time'] = data.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})\n#     data.loc[:, 'cp_dose'] = data.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n\n# --------------------- Normalize ---------------------\n#     for col in GENES:\n#         data[col] = (data[col]-np.mean(data[col])) \/ (np.std(data[col]))\n    \n#     for col in CELLS:\n#         data[col] = (data[col]-np.mean(data[col])) \/ (np.std(data[col]))\n    \n#--------------------- Removing Skewness ---------------------\n#     for col in GENES + CELLS:\n#         if(abs(data[col].skew()) > 0.75):\n            \n#             if(data[col].skew() < 0): # neg-skewness\n#                 data[col] = data[col].max() - data[col] + 1\n#                 data[col] = np.sqrt(data[col])\n            \n#             else:\n#                 data[col] = np.sqrt(data[col])\n    \n    return data","ef9daefe":"feature_cols = [c for c in process_data(folds).columns if c not in target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","61df70dc":"# HyperParameters\n\nDEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 5\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size=1024\n","19ebacfc":"def run_training(fold, seed):\n    \n    seed_everything(seed)\n    \n    train = process_data(folds)\n    test_ = process_data(test)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n    \n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n    \n    x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n    \n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.to(DEVICE)\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n    \n    loss_fn = nn.BCEWithLogitsLoss()\n    \n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n    \n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n    \n    for epoch in range(EPOCHS):\n        \n        train_loss = train_fn(model, optimizer,scheduler, loss_fn, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n        \n        if valid_loss < best_loss:\n            \n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_.pth\")\n        \n        elif(EARLY_STOP == True):\n            \n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n            \n    \n    #--------------------- PREDICTION---------------------\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = Model(\n        num_features=num_features,\n        num_targets=num_targets,\n        hidden_size=hidden_size,\n    )\n    \n    model.load_state_dict(torch.load(f\"FOLD{fold}_.pth\"))\n    model.to(DEVICE)\n    \n    predictions = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n    predictions = inference_fn(model, testloader, DEVICE)\n    \n    return oof, predictions\n","c72c5e5d":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n    \n    for fold in range(NFOLDS):\n        oof_, pred_ = run_training(fold, seed)\n        \n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n    return oof, predictions","3409345c":"# Averaging on multiple SEEDS\n\nSEED = [0, 1, 2, 3 ,4, 5]\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\n\nfor seed in SEED:\n    \n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ \/ len(SEED)\n    predictions += predictions_ \/ len(SEED)\n\ntrain[target_cols] = oof\ntest[target_cols] = predictions\n","315877ea":"# test['atp-sensitive_potassium_channel_antagonist'] = 0.0\n# test['erbb2_inhibitor'] = 0.0\n\n# train['atp-sensitive_potassium_channel_antagonist'] = 0.0\n# train['erbb2_inhibitor'] = 0.0","adb3d5bd":"train_targets_scored","7f7fc46c":"len(target_cols)\n","8628a58a":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ \/ target.shape[1]\n    \nprint(\"CV log_loss: \", score)\n    ","f91be72e":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","41ea8dce":"sub.shape","b9a27911":"### In this kernel I add RankGauss here. \u2193\u2193\u2193","bc5ff304":"**train set before using RankGauss**","0ae58379":"# Binning","2ef8f89e":"It appears that we were able to transform the distribution of each data to resemble a normal distribution, as intended.\n\nSo, let's enter the data into the benchmarking method to see the improvement.","5a77bc84":"# CV folds","d6b91d40":"I use the great notebook(refer [1]https:\/\/www.kaggle.com\/namanj27\/new-baseline-pytorch-moa?scriptVersionId=42580548) for benchmark.","b1a45903":"Some kagglers try to use PCA for dimensionality reduction and found good effect.\n\nI won't explain it now, but some people say it is best that the data for PCA has a multivariate normal distribution.\n\nIn this notebook, by RankGauss, I processed the data to normal distribute-like data, and see it's effect to scores.","6a6eab2c":"# <div class=\"h2\">Evaluation of preprocessed data<\/div>\n\n","ae66a190":"# <div class=\"h1\">About this notebook<\/div>\n\nI use RankGauss to g-* and c-* columns for preprocessing. \n\nI found score improvement from baseline[1] (CV: 0.014650792660668535, LB:0.01873).\n\n- **CV**: 0.01463430775798657 (check ver1 of this notebook.)\n\n- **LB**: 0.01867\n","da43ae07":"I'll check distributions of g-* and c-* of train and test set. They are spiky distribution rather than normal distribution. Regardless of the train and test, they look be in the same shape.\n\n","0bf3deab":"It may be a too simple idea, it appears that the gene expression data and cell viability data can be controlled by the experimenter, so it is safe to assume that these data are independent of each other.\n\nAlso, since the shape of the distribution is close to normal distribution to begin with, I don't think there is much of a problem if it is forced to be transformed into a Gaussian distribution.","f7df4ad7":"**test set before using RankGauss**","503a45c8":"# Model","f51ee4e6":"# [Naive] Outlier Removal","fcde1fd3":"# PCA","2ea3854a":"**test set after using RankGauss**","e470e1d4":"# Dataset Classes","bd5827c0":"# feature Selection using Variance Encoding","dcf0b1a0":"We can confirme that the shapes of data got close to the normal distribution.\n\n**train set after using RankGauss**","1a8b1fe6":"# Single fold training","8ea193e9":"\n### References :\n\n[1] Baseline method\n\nhttps:\/\/www.kaggle.com\/namanj27\/new-baseline-pytorch-moa?scriptVersionId=42580548\n\n[2] About RankGauss\n\nhttps:\/\/www.kaggle.com\/c\/porto-seguro-safe-driver-prediction\/discussion\/44629","f1e85db8":"# PCA features + Existing features","0909fe5f":"# Distribution plots","88e38a61":"# Preprocessing steps"}}