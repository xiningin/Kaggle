{"cell_type":{"1a013b11":"code","8b0e553a":"code","7e8bc543":"code","8ce0d126":"code","6b5906f7":"code","5f6fc419":"code","0d088bd9":"code","5bb4c057":"code","b867370e":"code","bbb84737":"code","7ce82a6b":"code","66ae28a5":"code","922cd69c":"code","8cc7bdef":"code","7afcfd73":"code","2a16e4ab":"code","c9b56009":"code","1be770a4":"code","ee38bd14":"code","038fa97e":"code","fbfa3930":"code","547b1bc0":"code","bbcd5bcf":"code","d27044a4":"code","aa86f7f9":"code","bca9194d":"code","139b8b5b":"code","e5c0e4f2":"code","df3a8e4b":"code","d65ad4e4":"code","37008912":"code","1848e859":"code","57d5d288":"code","552fca16":"code","9c1fb50b":"code","f6fdafd5":"code","fb394023":"code","8bfeebbe":"code","1d9e9767":"code","6de1bd53":"code","4e6dde8c":"code","890d729c":"code","a73dca02":"code","34e69d93":"code","843ec713":"code","6c7dda6e":"code","7d855a0c":"code","e86fc662":"code","3141c374":"code","d5b1bccb":"code","b497d0c8":"code","0e2e02a9":"code","269dd106":"code","66f21def":"code","c457659e":"code","bcf5eaa7":"code","e660fb20":"code","a133bcdb":"code","b147be59":"code","77b5004e":"code","5a45248b":"code","1f480930":"code","db4cf9e2":"code","c585a12a":"code","ed0fa1ea":"code","c88dc385":"code","b41fa3ff":"code","14c41a91":"code","f907925b":"code","a08d0416":"code","4df9ce67":"code","2cd1c3f4":"code","fe0a792c":"code","58f6c0fd":"code","a0ee418b":"code","4a13751f":"code","22f18f22":"code","1b15bedf":"code","7cbb5d88":"code","a9e505e4":"markdown","0075696e":"markdown","f42c9bd6":"markdown","b587db5f":"markdown","3c7af1f0":"markdown","47ef66d5":"markdown","ef80e8ed":"markdown","8169d5f0":"markdown","54094c42":"markdown","221a63d6":"markdown","116a0d94":"markdown","6cef0881":"markdown","9dfa2643":"markdown","9af5ad3c":"markdown","da2815e0":"markdown","3fffe4ff":"markdown","bbd9ea7f":"markdown","a00f18c4":"markdown","52dec01b":"markdown","a6249fcb":"markdown","cfce7798":"markdown","9c0e1393":"markdown","2201258b":"markdown","07bc46cb":"markdown","47478130":"markdown","46f9ae16":"markdown","aeb69990":"markdown","88f5c474":"markdown","408f1eff":"markdown","abbd02c4":"markdown","c22b6fb0":"markdown","781dd80f":"markdown","44aaca1a":"markdown","834dcbd7":"markdown","21a2643b":"markdown","2e09496d":"markdown","eaf7504d":"markdown","5b868c84":"markdown","ae75e5fd":"markdown","6a2bf263":"markdown","74a40611":"markdown","9c2229c9":"markdown","f9542cf9":"markdown","5586ee20":"markdown","de57d962":"markdown","bddc1cae":"markdown","814f1d22":"markdown","ce483ae0":"markdown","9349709f":"markdown","816b996b":"markdown","99303443":"markdown","db665f38":"markdown","ac289765":"markdown","253d4ca1":"markdown","fa66d74e":"markdown","18943fe5":"markdown","60007031":"markdown","7a0ca040":"markdown","814be7a6":"markdown","046ae2d0":"markdown","48fe2816":"markdown","27334c0d":"markdown","3641f563":"markdown","ad9c1c4f":"markdown","dabb8c1f":"markdown","4eabb987":"markdown","d452543c":"markdown"},"source":{"1a013b11":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8b0e553a":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","7e8bc543":"data = pd.read_csv(\"..\/input\/us-accidents\/US_Accidents_Dec20_Updated.csv\")\ndf = data.copy()\ndf.head(3)","8ce0d126":"df.info()","6b5906f7":"df.isna().sum().sort_values(ascending = False)","5f6fc419":"null_values = df.isna().sum().reset_index()\nnull_values.columns = [\"Columns\", \"Null_count\"]\nnull_values[\"% Null_values\"] = (null_values[\"Null_count\"]*100)\/len(df)\nnull_values.sort_values(by = \"Null_count\",ascending = False, inplace = True)\nMissing_values = null_values[null_values[\"Null_count\"] != 0]\nMissing_values","0d088bd9":"sns.set_style(\"darkgrid\")","5bb4c057":"plt.figure(figsize=(15,10))\nplt.xticks(rotation = 90)\nplt.title(\"Percentage Of Null Values\",fontsize= 20)\nsns.barplot(x = \"Columns\", y = \"% Null_values\", data = Missing_values )","b867370e":"df.drop(axis = 1, columns = ['Number','Precipitation(in)','Wind_Chill(F)','Wind_Speed(mph)','End_Lat','End_Lng'], inplace = True)","bbb84737":"df.drop(axis = 1, columns = ['Wind_Direction','Pressure(in)','Weather_Timestamp','Airport_Code','Timezone','Zipcode','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight'], inplace = True)","7ce82a6b":"df[\"Temperature(F)\"].fillna(df[\"Temperature(F)\"].median(), inplace = True)\ndf[\"Humidity(%)\"].fillna(df[\"Humidity(%)\"].median(), inplace = True)\ndf[\"Visibility(mi)\"].fillna(df[\"Visibility(mi)\"].median(), inplace = True)","66ae28a5":"df[\"Weather_Condition\"].fillna(df[\"Weather_Condition\"].mode()[0], inplace = True)\ndf[\"Sunrise_Sunset\"].fillna(df[\"Sunrise_Sunset\"].mode()[0], inplace = True)","922cd69c":"df[\"City\"].fillna(value = \"None\", inplace = True)","8cc7bdef":"df.isna().sum().sort_values(ascending = False)","7afcfd73":"df.head(3).transpose()","2a16e4ab":"for col in df.columns:\n    if df[col].nunique() < 2100 and df[col].nunique() > 10 and df[col].dtype== \"object\": \n        df[col] = df[col].astype(\"category\")","c9b56009":"df[\"Country\"] = df[\"Country\"].astype(\"category\")","1be770a4":"df[\"Side\"] = df[\"Side\"].astype(\"category\")","ee38bd14":"convert_columns1 = [\"Start_Time\",\"End_Time\"]\ndf[convert_columns1] = df[convert_columns1].astype(\"datetime64[ns]\")\ndf.info()","038fa97e":"df.describe().T","fbfa3930":"plt.figure(figsize = (25,15))\nVar_Corr = df.corr()\nsns.heatmap(Var_Corr, cmap = \"coolwarm\", xticklabels=Var_Corr.columns, yticklabels=Var_Corr.columns, annot=True)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.show()","547b1bc0":"plt.figure(figsize = (25,15))\ndf[\"State\"].value_counts(ascending = True).plot(kind = \"barh\", color = \"b\")\nplt.title(\"Accidents by States\", fontsize = 20)","bbcd5bcf":"plt.figure(figsize = (25,10))\n(df[\"State\"].value_counts(ascending = False)*100\/len(df)).plot(kind = \"bar\", color = \"r\")\nplt.title(\"Percentage of Accidents by States\", fontsize = 20)","d27044a4":"cities = df[\"City\"].value_counts(ascending = False).reset_index()\ncities.columns = [\"City\", \"Number_of_Accidents\"]\ncities[\"% of_Accidents\"] =(cities[\"Number_of_Accidents\"]*100)\/len(df)\ncities.sort_values(by = \"Number_of_Accidents\",ascending = False, inplace = True)\ncities_accidents = cities.head(50)\ncities_accidents.head()","aa86f7f9":"plt.figure(figsize=(20,20))\nplt.xticks(rotation = 90)\nplt.title(\"Accident by Cities (Top 50)\",fontsize= 20)\nsns.barplot(y = \"City\", x = \"Number_of_Accidents\", data = cities_accidents )","bca9194d":"len(cities)","139b8b5b":"cities[\"% of_Accidents\"].head(1000).sum()","e5c0e4f2":"cities[\"% of_Accidents\"].head(100).sum()","df3a8e4b":"cities[\"% of_Accidents\"].head(10).sum()","d65ad4e4":"streets = df[\"Street\"].value_counts(ascending = False).reset_index()\nstreets.columns = [\"Street\", \"Number_of_Accidents\"]\nstreets[\"% of_Accidents\"] =(streets[\"Number_of_Accidents\"]*100)\/len(df)\nstreets.sort_values(by = \"Number_of_Accidents\",ascending = False, inplace = True)\nstreets_accidents = streets.head(50)\nstreets_accidents.head()","37008912":"plt.figure(figsize=(20,20))\nplt.xticks(rotation = 90)\nplt.title(\"Accident by Streets (Top 50)\",fontsize= 20)\nsns.barplot(y = \"Street\", x = \"Number_of_Accidents\", data = streets_accidents )","1848e859":"len(streets)","57d5d288":"streets[\"% of_Accidents\"].head(10000).sum()","552fca16":"streets[\"% of_Accidents\"].head(1000).sum()","9c1fb50b":"streets[\"% of_Accidents\"].head(100).sum()","f6fdafd5":"df[\"Year\"] = df[\"Start_Time\"].dt.year","fb394023":"plt.figure(figsize = (15,10))\ndf[\"Start_Time\"].dt.year.value_counts().plot(kind = \"line\")\nplt.title(\"Yearly Accidents Trend\", fontsize = 15)","8bfeebbe":"plt.figure(figsize = (25,10))\nexplode = (0, 0.1, 0, 0, 0)\ncolors = ['#c2c2f0','#ffb3e6', '#99ff99', '#66b3ff', '#ffcc99']\n(df[\"Start_Time\"].dt.year.value_counts(ascending = True)*100\/len(df)).plot(kind = \"pie\", autopct = \"%1.1f%%\", colors = colors, explode = explode, shadow = True)\nplt.title(\"Percentage of yearly Accidents\", fontsize = 20)","1d9e9767":"plt.figure(figsize = (15,30))\nsns.countplot(y = \"State\", hue=\"Year\", data=df, order = df[\"State\"].value_counts().index)\nplt.title(\"Percentage of yearly Accidents by States\", fontsize = 15)","6de1bd53":"df[\"Month\"] = df[\"Start_Time\"].dt.month_name()","4e6dde8c":"plt.figure(figsize = (20,10))\n(df[\"Start_Time\"].dt.month_name().value_counts(ascending = True)*100\/len(df)).plot(kind = \"bar\", color = \"m\")\nplt.title(\"Percentage of Monthly Accidents\", fontsize = 20)","890d729c":"plt.figure(figsize = (20,10))\nsns.countplot(x = \"Month\", hue=\"Year\", data=df, order = df[\"Month\"].value_counts().index)\nplt.title(\"Percentage of yearly Accidents by Months\", fontsize = 20)\nplt.show()","a73dca02":"df[\"Day_of_Week\"] = df[\"Start_Time\"].dt.day_name()","34e69d93":"plt.figure(figsize = (20,10))\n(df[\"Start_Time\"].dt.day_name().value_counts(ascending = True)*100\/len(df)).plot(kind = \"bar\", color = \"purple\")\nplt.title(\"Percentage of Accidents by Days\", fontsize = 20)","843ec713":"df[\"Hour\"] = df[\"Start_Time\"].dt.hour","6c7dda6e":"plt.figure(figsize = (15,10))\nsns.histplot(data = df, x = \"Hour\", bins = 24)\nplt.title(\"Frequency of Accidents throughout the Day\", fontsize = 15)","7d855a0c":"plt.figure(figsize = (25,10))\nexplode = (0, 0.1, 0, 0)\ndf[\"Severity\"].value_counts().plot(kind = \"pie\", autopct = \"%1.1f%%\", colors = ('#c2c2f0','#ffb3e6', '#99ff99', '#66b3ff' ), explode = explode, shadow = True)\nplt.title(\"Percentage of Severity of Accidents\", fontsize = 20)","e86fc662":"plt.figure(figsize = (25,10))\nsns.countplot(x = \"Severity\", hue = \"Year\", data = df)","3141c374":"weather = df[\"Weather_Condition\"].value_counts().reset_index()\nweather.columns = [\"Weather\", \"Number_of_Accidents\"]\nweather[\"% of_Accidents\"] =(weather[\"Number_of_Accidents\"]*100)\/len(df)\nweather.sort_values(by = \"Number_of_Accidents\",ascending = False, inplace = True)\nweather_condition = weather.head(30)\nweather_condition.head()","d5b1bccb":"plt.rcParams[\"figure.figsize\"] = (20,15)\nweather_condition.plot(x = \"Weather\", y = \"% of_Accidents\", kind = \"bar\")\nplt.title(\"Accidents by Weather Condition (Top 30)\", fontsize = 20)\nplt.xticks(rotation = 90)\nplt.show()","b497d0c8":"weather_condition[\"% of_Accidents\"].head(6).sum()","0e2e02a9":"group = df.groupby([\"Weather_Condition\", \"Severity\"])[\"Severity\"].count().sort_values(ascending = False).unstack(\"Weather_Condition\")","269dd106":"weather_severity = group[[\"Fair\",\"Clear\", \"Mostly Cloudy\", \"Partly Cloudy\", \"Cloudy\", \"Overcast\"]].unstack()","66f21def":"plt.figure(figsize = (30,30))\ncolors = ('lightblue', \"beige\", \"cyan\", 'lightsteelblue')\nexplode = (0, 0, 0.1, 0)\nplt.subplot(2,3,1)\nweather_severity.loc[\"Fair\"].sort_values().plot(kind = \"pie\",autopct = \"%1.1f%%\", textprops={'fontsize': 20}, colors =colors, explode = explode, shadow = True)\nplt.title(\"Fair\", fontsize = 30)\nplt.ylabel(\"\")\nplt.subplot(2,3,2)\nweather_severity.loc[\"Clear\"].sort_values().plot(kind = \"pie\",autopct = \"%1.1f%%\", textprops={'fontsize': 20}, colors = colors, explode = explode, shadow = True)\nplt.title(\"Clear\", fontsize = 30)\nplt.ylabel(\"\")\nplt.subplot(2,3,3)\nweather_severity.loc[\"Mostly Cloudy\"].sort_values().plot(kind = \"pie\",autopct = \"%1.1f%%\", textprops={'fontsize': 20}, colors = colors, explode = explode, shadow = True)\nplt.title(\"Mostly Cloudy\", fontsize = 30)\nplt.ylabel(\"\")\nplt.subplot(2,3,4)\nweather_severity.loc[\"Partly Cloudy\"].sort_values().plot(kind = \"pie\",autopct = \"%1.1f%%\", textprops={'fontsize': 20}, colors = colors, explode = explode, shadow = True)\nplt.title(\"Partly Cloudy\", fontsize = 30)\nplt.ylabel(\"\")\nplt.subplot(2,3,5)\nweather_severity.loc[\"Cloudy\"].sort_values().plot(kind = \"pie\",autopct = \"%1.1f%%\", textprops={'fontsize': 20}, colors = colors, explode = explode, shadow = True)\nplt.title(\"Cloudy\", fontsize = 30)\nplt.ylabel(\"\")\nplt.subplot(2,3,6)\nweather_severity.loc[\"Overcast\"].sort_values().plot(kind = \"pie\",autopct = \"%1.1f%%\", textprops={'fontsize': 20}, colors = colors, explode = explode, shadow = True)\nplt.title(\"Overcast\", fontsize = 30)\nplt.ylabel(\"\")","c457659e":"group1 = df.groupby([\"Weather_Condition\", \"Year\"])[\"Year\"].count().sort_values(ascending = False).unstack(\"Weather_Condition\")","bcf5eaa7":"weather_year = group1[[\"Fair\",\"Clear\", \"Mostly Cloudy\", \"Partly Cloudy\", \"Cloudy\", \"Overcast\"]].unstack()","e660fb20":"plt.figure(figsize = (30,30))\nexplode = (0,0, 0, 0.1, 0)\ncolors = ['#c2c2f0', '#ffcc99', '#99ff99', '#66b3ff','#ff6666']\nplt.subplot(2,3,1)\nweather_year.loc[\"Fair\"].sort_values().plot(kind = \"pie\",autopct = \"%1.1f%%\", textprops={'fontsize': 20}, colors = colors, explode = explode, shadow = True)\nplt.title(\"Fair\", fontsize = 30)\nplt.ylabel(\"\")\nplt.subplot(2,3,2)\nweather_year.loc[\"Clear\"].sort_values().plot(kind = \"pie\",autopct = \"%1.1f%%\", textprops={'fontsize': 20}, colors = colors, explode = explode, shadow = True)\nplt.title(\"Clear\", fontsize = 30)\nplt.ylabel(\"\")\nplt.subplot(2,3,3)\nweather_year.loc[\"Mostly Cloudy\"].sort_values().plot(kind = \"pie\",autopct = \"%1.1f%%\", textprops={'fontsize': 20}, colors = colors, explode = explode, shadow = True)\nplt.title(\"Mostly Cloudy\", fontsize = 30)\nplt.ylabel(\"\")\nplt.subplot(2,3,4)\nweather_year.loc[\"Partly Cloudy\"].sort_values().plot(kind = \"pie\",autopct = \"%1.1f%%\", textprops={'fontsize': 20}, colors = colors, explode = explode, shadow = True)\nplt.title(\"Partly Cloudy\", fontsize = 30)\nplt.ylabel(\"\")\nplt.subplot(2,3,5)\nweather_year.loc[\"Cloudy\"].sort_values().plot(kind = \"pie\",autopct = \"%1.1f%%\", textprops={'fontsize': 20}, colors = colors, explode = explode, shadow = True)\nplt.title(\"Cloudy\", fontsize = 30)\nplt.ylabel(\"\")\nplt.subplot(2,3,6)\nweather_year.loc[\"Overcast\"].sort_values().plot(kind = \"pie\",autopct = \"%1.1f%%\", textprops={'fontsize': 20}, colors = colors, explode = explode, shadow = True)\nplt.title(\"Overcast\", fontsize = 30)\nplt.ylabel(\"\")","a133bcdb":"df[\"Temp Range\"] = pd.cut(df[\"Temperature(F)\"], [-100, -50, 0, 50,100, 150, 200, 250], labels = [\"-100 - -50\", \"-50 - 0\", \"0 - 50\", \"50 - 100\", \"100 - 150\", \"150 - 200\", \"200 - 250\"])","b147be59":"plt.figure(figsize = (25,10))\nexplode = (0, 0.1, 0, 0, 0, 0, 0)\ndf[\"Temp Range\"].value_counts().plot(kind = \"pie\", autopct = \"%1.1f%%\", textprops={'fontsize': 15}, explode = explode, shadow = True)\nplt.title(\"Percentage of Accidents in Temp Range\", fontsize = 20)","77b5004e":"group2 = df.groupby([\"Temp Range\", \"Visibility(mi)\"])[\"Temp Range\"].count().unstack().stack().sort_values(ascending = False).head(30)","5a45248b":"plt.figure(figsize = (20,10))\n(group2*100\/len(df)).plot(kind = \"bar\", color = \"y\")\nplt.title(\"Accidents by Temp Range and Visibility (Top 30)\", fontsize = 20)","1f480930":"plt.figure(figsize = (20,10))\n(df[\"Visibility(mi)\"].value_counts().head(30)*100\/len(df)).plot(kind = \"bar\")\nplt.title(\"Accidents by Visibility (Top 30)\", fontsize = 20)","db4cf9e2":"df[\"Dist Range\"] = pd.cut(df[\"Distance(mi)\"], [-1,0,1,2,3,4,350], labels = [ \"-1-0\",\"0-1\",\"1-2\", \"2-3\", \"3-4\", \"4+\"])","c585a12a":"plt.figure(figsize = (25,10))\nexplode = (0, 0.1, 0, 0, 0, 0)\ndf[\"Dist Range\"].value_counts().plot(kind = \"pie\", autopct = \"%1.1f%%\", textprops={'fontsize': 15}, colors = ['#c2c2f0','#ffb3e6', '#99ff99', '#66b3ff','#ff6666', '#ffcc99'], explode = explode, shadow = True)\nplt.title(\"Percentage of Accidents in Dist Range\", fontsize = 20)","ed0fa1ea":"group3 = df.groupby([\"Dist Range\", \"Severity\"])[\"Dist Range\"].count().unstack().stack().sort_values(ascending = False).head(30)","c88dc385":"plt.figure(figsize = (20,10))\n(group3*100\/len(df)).plot(kind = \"bar\", color = \"y\")\nplt.title(\"Accidents by Dist Range and Severity (Top 30)\", fontsize = 20)","b41fa3ff":"plt.figure(figsize = (30,30))\nexplode = (0, 0, 0.1, 0)\ncolors = ['#ff9999','#66b3ff','#99ff99','#ffcc99']\nplt.subplot(2,3,1)\ngroup3.loc[\"-1-0\"].sort_values().plot(kind = \"pie\",autopct = \"%1.1f%%\", textprops={'fontsize': 20}, colors = colors, explode = explode, shadow = True)\nplt.title(\"0 (mi)\", fontsize = 30)\nplt.ylabel(\"\")\nplt.subplot(2,3,2)\ngroup3.loc[\"0-1\"].sort_values().plot(kind = \"pie\",autopct = \"%1.1f%%\", textprops={'fontsize': 20}, colors = colors, explode = explode, shadow = True)\nplt.title(\"0 - 1 (mi)\", fontsize = 30)\nplt.ylabel(\"\")\nplt.subplot(2,3,3)\ngroup3.loc[\"1-2\"].sort_values().plot(kind = \"pie\",autopct = \"%1.1f%%\", textprops={'fontsize': 20}, colors = colors, explode = explode, shadow = True)\nplt.title(\"1 - 2 (mi)\", fontsize = 30)\nplt.ylabel(\"\")\nplt.subplot(2,3,4)\ngroup3.loc[\"2-3\"].sort_values().plot(kind = \"pie\",autopct = \"%1.1f%%\", textprops={'fontsize': 20}, colors = colors, explode = explode, shadow = True)\nplt.title(\"2 - 3 (mi)\", fontsize = 30)\nplt.ylabel(\"\")\nplt.subplot(2,3,5)\ngroup3.loc[\"3-4\"].sort_values().plot(kind = \"pie\",autopct = \"%1.1f%%\", textprops={'fontsize': 20}, colors = colors, explode = explode, shadow = True)\nplt.title(\"3 - 4 (mi)\", fontsize = 30)\nplt.ylabel(\"\")\nplt.subplot(2,3,6)\ngroup3.loc[\"4+\"].sort_values().plot(kind = \"pie\",autopct = \"%1.1f%%\", textprops={'fontsize': 20}, colors = colors, explode = explode, shadow = True)\nplt.title(\"4+ (mi)\", fontsize = 30)\nplt.ylabel(\"\")","14c41a91":"new_columns = ['Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', 'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop']","f907925b":"new_df = pd.DataFrame(columns = [\"Columns\", \"% of Accidents\"])","a08d0416":"new_df[\"Columns\"] = new_columns","4df9ce67":"for i in range(len(new_df)):\n    new_df[\"% of Accidents\"].loc[i] = len(df[df[new_df[\"Columns\"].loc[i]] == True])*100\/len(df)","2cd1c3f4":"new_df.sort_values(\"% of Accidents\", ascending = False, inplace = True)\nnew_df","fe0a792c":"sns.barplot(x = \"Columns\", y = \"% of Accidents\", data = new_df)\nplt.title(\"Accidents in presence of different factors\", fontsize = 20)","58f6c0fd":"ca_df = pd.DataFrame(columns = [\"Columns\", \"% of Accidents\"])","a0ee418b":"ca_df[\"Columns\"] = new_columns","4a13751f":"for i in range(len(ca_df)):\n    mask1 = df[ca_df[\"Columns\"].loc[i]] == True\n    mask2 = df[\"State\"]  == \"CA\"\n    ca_df[\"% of Accidents\"].loc[i] = len(df[mask1 & mask2])*100\/len(df[df[\"State\"] == \"CA\"])","22f18f22":"ca_df.sort_values(\"% of Accidents\", ascending = False, inplace = True)\nca_df","1b15bedf":"sns.barplot(x = \"Columns\", y = \"% of Accidents\", data = ca_df)\nplt.title(\"Accidents in California presence of different factors\", fontsize = 20)","7cbb5d88":"plt.figure(figsize = (25,10))\ndf[\"Sunrise_Sunset\"].value_counts().plot(kind = \"pie\", autopct = \"%1.1f%%\", textprops={'fontsize': 15}, explode = (0.05,0), colors = ['lightblue','lightsteelblue'], shadow = True)\nplt.title(\"Accidents in Day\/Night\", fontsize = 15)\n","a9e505e4":"> #### Junction could be one of the cause of Accidents in California which amounts for **14%** of the total accidents in the State. And Traffic Signal with 8%.","0075696e":"> #### We can clearly see that California (CA) has the highest number of Accidents by a large margine with Florida (FL) which has the second most accident records. Lets plot a percentage graph for a clearer picture.","f42c9bd6":"> #### Let's add an hour column.","b587db5f":"#### i. Impute the missing values of numerical columns:","3c7af1f0":"> #### 60% of the accidents have happened in 50-100F Temp Range with 10.0 mi Visibility and 19% of the accidents have happened in 0-50F with 10.0 mi Visibility","47ef66d5":"### a) State","ef80e8ed":"> #### Los Angeles has the highest number of accidents with houston being second with almost similar number of accidents. Top 10 cities account for the most of the accidents.","8169d5f0":"##  1. Import Data and Libraries","54094c42":"> #### 72.5% of accidents have happened in Temperature Range of 50-100 F and 26.9% of accidents have happened in 0 - 50 F Temp Range.","221a63d6":"> #### We have **11790** cities. Of that Top 1000 cities account for **81%** of the Accidents, Top 100 cities account for **45%** of the Accidents and Top 10 cities account for **16%** of the Accidents.","116a0d94":"> #### Let's plot a bar plot to understand which state has the most accidents records.","6cef0881":"> #### Now that we've taken taken care of all the columns with null values that we don't require, We'll now fill the remaining coulmns with appropriate values.","9dfa2643":"> #### 2020 have had most of the accidents for all the States. For Florida it seems the number has increased **3 times** than the previous year. California also had **1.6 times** increase over the previous year. PA, VA also had significant increase in 2020. ","9af5ad3c":"> #### **2\/3rd** of accidents happen during the day.","da2815e0":"> #### I-5N had the most number of accidents, following that I-95N, I-95S. Steets follow the same trend as cities with top streets having the most number of accidents.","3fffe4ff":"> #### \"Fair\" weather condition has the large number of accidents i.e 26% of the accidents. Clear and Mostly Cloudy also have 17% and 13% respectively. Also for Partly Cloudy has 9%, Cloudy has 8%, Overcast has 8% accidents. These top 6 Weather conditions amounts to **82.6%** of total accidents.","bbd9ea7f":"> #### As we can see we've dealt with all the null values. We've a clean data for our analysis. Now let's do some memory optimization","a00f18c4":"### i) Other Columns","52dec01b":"> #### Let's add a month column","a6249fcb":" > #### **73%** reported accidents have Severity 2 which could mean that there are a lot of accidents which caused some injuries and had little impact.","cfce7798":"> #### Overcast and Clear weather has no accident records for 2020. This could be an error while collecting data since 2020 recorded most accidents overall. Fair and Cloudy weather conditions had more than 65% accidents happen in 2020.","9c0e1393":"### f) Weather Condition","2201258b":"### c) Street","07bc46cb":"> #### We'll first see how many null values are there in the dataset. We'll drop the columns containing large number of null values since they won't be much useful. We'll also get rid of few of the columns which aren't too important.","47478130":"> #### We've reduced the memory usage by approximately 55% i.e 790 MB to 343.8 MB after removing all the null values and converting few columns to categorical datatype.","46f9ae16":"> #### We're all done with the data preparation. Now let's explore our data.","aeb69990":"### h) Distance(mi)","88f5c474":"### b) City","408f1eff":"> #### There are many boolean datatypes as well, we'll have to be careful while selecting the columns.","abbd02c4":"> #### There are lots of column with a string datatype which could be converted into categorial datatype columns for performance improvement. Let\u2019s take a look at which columns might be good candidates for a categorical datatype. ","c22b6fb0":"#### ii. Impute the missing values of categorical columns:","781dd80f":"### d) Start_Time","44aaca1a":"> #### **39%** of the accidents have happened on the spot with severity 2 accidents. **28%** accidents have happened with 0 - 1 (mi) Dist Range with Severity 2 accidents.","834dcbd7":"> #### The US have 50 states in the total. We have the records of 49 states.","21a2643b":"> #### Saturday and Sundays have least number of Accidents. Weekends are off days for most of the working people, that could be the reason behind less accidents. We can get clear understanding about this when we plot a graph for timeline of every hour.","2e09496d":"> #### In these past 5 years **25%** of accidents have happened in the state of California which is signifiant. Following that **9%** in Florida and **8%** in Texas.","eaf7504d":"> #### There's an increasing trend in 2020. The accidents are growing rapidly in numbers.","5b868c84":"> #### **54%** of the accidents happened on the spot. **37%** of accidents have happened in 0 -1 mi Dist Range.","ae75e5fd":"## 3. Explore The Data","6a2bf263":"### f) Visibility(mi)","74a40611":"# Exploratory Data Analysis - US Accidents","9c2229c9":"#### Let's create a barplot for the State of California since it has the highest number of accidents with the same columns.","f9542cf9":"> #### As the Distance increases the percentage of Severity 3 & 4 in that Dist Range also increases. Though all Dist Ranges has mostly Severity 2 Accidents among them.","5586ee20":"> #### For December and November, 2020 had more accidents approx **3 times** that of the previous year. For July and August, it seems the number has decreased by **half**. It's interesting to note that except 2020, all months had balanced number of accidents throughout all years.","de57d962":"> #### We have **175527** Streets. Of that Top 10000 Streets account for **77%** of the Accidents, Top 1000 Streets account for **49%** of the Accidents and Top 100 Streets account for **25%** of the Accidents.","bddc1cae":"> #### For **15%** of Accidents Traffic Signal is nearby. **10%** for Junction and **8%** for Crossing.","814f1d22":"## 2. Memory Optimization","ce483ae0":"> #### There are a lot of null values. Some columns have definitely way more null columns, They should be discarded completely. But first let's visualize the null values of the data for better understanding. ","9349709f":">    ### Before We get to exploring the data, first and foremost we should prepare the data for the analysis. We'll first do data cleaning. we'll check for the null values and remove all the columns with a lot of null values. Also we'll imput appropriate values for the required columns for our analysis then We'll do memory optimzation since our data is too large.","816b996b":"### g) Temperature(F)","99303443":">  ### In this Project, We are going to Explore the countrywide car accident dataset of the US. The accident data are collected from February 2016 to Dec 2020, there are about **3 million** accident records in this dataset. We are going to analyse the data to explore various questions like Hotspot locations of the Accidents, What time of the day is the frequency higher? and the impact of environmental stimuli on accident occurrence. ","db665f38":"> #### It seems all six Weather Conditions has most accidents happened in Severity 2 i.e **above 65%**. Clear and Overcast Weather had no Severity 1 accidents.","ac289765":"> #### Most of the accidents happen during last quarter of the year with December having the most accidents. July registered least number of accidents.","253d4ca1":"> #### We still have null values in our dataset. But we don't require all the  columns for our analysis. We'll remove all the unncessary columns containing null values.","fa66d74e":"> #### 80% of the accident records have 10.0 mi Visibility","18943fe5":"> ### In this, We'll analyse each column of our dataset excluding some which don't impact or have any meaningful insights whatsoever. There are many columns worth exploring like State, City, Street, County, Start_Time, Temperature(F), Weather_Condition, Visibility(mi). We'll gain many insights and will try to answer a lot of questions about the dataset.\n\n","60007031":"> #### Let's plot a pie chart for the above six Weather Conditions with Severity.","7a0ca040":"> #### Out of all the accident records **35.6%** of accidents have happened in 2020. Accidents are increasing at an alarming rate every year.","814be7a6":"### e) Severity","046ae2d0":"> #### The top 3 columns with most null values have more than 40% of null values so they're useless. There's also  a big jump from 7th to 6th column, since the top 6 columns containing most null values are not that important for our analysis, We'll drop them completely.","48fe2816":"# 2. Data Preparation","27334c0d":"> #### Most accidents happen between 6AM-9AM and between 3PM-6PM. People commute to work and from work in those time gaps respectively. This might be the reason that Saturday and Sunday has least number of Accidents.","3641f563":"> #### Lets' add a year column","ad9c1c4f":"## 1. Data Cleaning","dabb8c1f":"> #### Now let's convert the datatypes of columns Start_Time and End_Time as these should be in datetime datatypes for our analysis.\n","4eabb987":"> #### We can see that we only have 46 columns, taking 790 MB of memory, We'll try to reduce this as much we can after we deal with the null values.","d452543c":"> #### It seems there are little to no records of Severity 1. Year 2020 had the most number of Severity 2 Accidents though it doesn't seem to be the case in Severity 3 and 4 which is an interesting find."}}