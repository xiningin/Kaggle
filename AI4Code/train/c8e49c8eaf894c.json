{"cell_type":{"c818b4ce":"code","c01adf73":"code","cff1b210":"code","941e5ed3":"code","1b8438bb":"code","b48e8ba9":"code","12d18319":"code","dfa7dd30":"code","0518165f":"code","b738f3d1":"code","b8b5ac3b":"code","b324c87f":"code","5275c7b4":"code","c4995973":"code","4188ddb5":"code","e4e2dac3":"code","67d8070c":"code","556adbae":"code","2bbb391f":"code","043a1f8c":"code","8fca0765":"code","ad458559":"code","1fea52d3":"code","2a4ea245":"code","3c149470":"code","8d611d6a":"code","567549bc":"code","158845c0":"code","a8217cfd":"code","dcf4fc19":"code","ef8c35bf":"code","969c8a84":"code","9d7acecf":"code","e007ca3b":"code","10cc03aa":"code","1343a85d":"code","38280e7c":"code","79346cb0":"code","081889e7":"code","dad705f0":"code","2e9a39dd":"code","9fcef9f3":"code","29d10a2b":"code","6b51319b":"code","dc87228a":"code","80ab8dd2":"code","1b48d611":"code","ec5b8944":"code","61268306":"code","c38c6622":"code","94d9003e":"code","31651508":"code","01cfd7bf":"code","310b18a1":"code","71185aeb":"code","363d296f":"code","9e87c857":"code","23cc4112":"markdown","7793056f":"markdown","b42c7246":"markdown","fed489c7":"markdown","6815d0d4":"markdown","a0b35804":"markdown","6d0f8a51":"markdown","bbe4141a":"markdown","9eae280b":"markdown","d2091faa":"markdown","09fe18b5":"markdown","15bc0254":"markdown","f2d1a3a6":"markdown","6ea14a61":"markdown","5dee0368":"markdown","6f220d37":"markdown","251d7f34":"markdown","40e0956a":"markdown","dc47a9bc":"markdown","0536c66b":"markdown","a512be7e":"markdown","9dd739de":"markdown","242873a8":"markdown","e070abe3":"markdown","3afc4c7c":"markdown","38ae3eba":"markdown"},"source":{"c818b4ce":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n%matplotlib inline","c01adf73":"# Read in Train Data\ntrain = pd.read_csv(\"..\/input\/train.csv\")","cff1b210":"# Read in Test Data\ntest = pd.read_csv(\"..\/input\/test.csv\")","941e5ed3":"# Number of rows and columns of training and test data\ntrain.shape, test.shape","1b8438bb":"train.head()","b48e8ba9":"train.info()","12d18319":"train.select_dtypes(include=\"object\").columns","dfa7dd30":"# Checking if ID_code is unique\ntrain.ID_code.nunique() == train.shape[0]","0518165f":"sns.countplot(train.target)","b738f3d1":"train.target.value_counts() *100 \/ train.target.count()","b8b5ac3b":"train.groupby(\"target\").mean()","b324c87f":"train.groupby(\"target\").median()","5275c7b4":"np.mean(train.groupby(\"target\").mean().iloc[1] >= train.groupby(\"target\").mean().iloc[0])","c4995973":"np.mean(train.groupby(\"target\").median().iloc[1] >= train.groupby(\"target\").mean().iloc[0])","4188ddb5":"features = train.columns.values[2:203]","e4e2dac3":"from scipy.stats import normaltest","67d8070c":"# # D\u2019Agostino\u2019s K^2 Test on TRAIN DATA\n# non_normal_features = []\n# for feature in features:\n#     stat, p = normaltest(train[feature])\n#     if p <= 0.01:\n#         print(feature,\"not normal\")\n#         non_normal_features.append(feature)","556adbae":"# # D\u2019Agostino\u2019s K^2 Test on TEST DATA\n# non_normal_features_test_data = []\n# for feature in test.columns.values[1:202]:\n#     stat, p = normaltest(test[feature])\n#     if p <= 0.05:\n#         print(feature,\"not normal\")\n#         non_normal_features_test_data.append(feature)","2bbb391f":"train.isnull().sum().sum()","043a1f8c":"correlations = train[features].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]\ncorrelations.tail(10)","8fca0765":"from sklearn.preprocessing import StandardScaler\nstandardized_train = StandardScaler().fit_transform(train.set_index(['ID_code','target']))","ad458559":"standardized_train = pd.DataFrame(standardized_train, columns=train.set_index(['ID_code','target']).columns)\nstandardized_train = standardized_train.join(train[['ID_code','target']])","1fea52d3":"from sklearn.decomposition import PCA\nk=80\npca = PCA(n_components=k, random_state=42, whiten=True)\npca.fit(standardized_train.set_index(['ID_code','target']))","2a4ea245":"plt.figure(figsize=(25,5))\nplt.plot(pca.explained_variance_ratio_)\nplt.xticks(range(k))\nplt.xlabel(\"Number of Features\")\nplt.ylabel(\"Proportion of variance explained by additional feature\")","3c149470":"sum(pca.explained_variance_ratio_)","8d611d6a":"sum(PCA(n_components=120, random_state=42, whiten=True).fit(standardized_train.set_index(['ID_code','target'])).\\\nexplained_variance_ratio_)","567549bc":"sum(PCA(n_components=160, random_state=42, whiten=True).fit(standardized_train.set_index(['ID_code','target'])).\\\nexplained_variance_ratio_)","158845c0":"pca = PCA(n_components=160).fit_transform(standardized_train.set_index(['ID_code','target']))","a8217cfd":"pca_col_names = []\nfor i in range(160):\n    pca_col_names.append(\"pca_var_\" + str(i))\npca_col_names","dcf4fc19":"# Save PCA transformed train dataset just in case\npca_train = pd.DataFrame(pca, columns=pca_col_names).join(train[['ID_code','target']])\npca_train.to_csv(\"pca_train.csv\")","ef8c35bf":"# Standardize the test data as well\nstandardized_test = StandardScaler().fit_transform(test.set_index(['ID_code']))\nstandardized_test = pd.DataFrame(standardized_test, columns=test.set_index(['ID_code']).columns)\nstandardized_test = standardized_test.join(test[['ID_code']])","969c8a84":"pca = PCA(n_components=160).fit_transform(standardized_test.set_index(['ID_code']))","9d7acecf":"pca_col_name_for_test = []\nfor i in range(160):\n    pca_col_name_for_test.append(\"pca_var_\" + str(i))","e007ca3b":"# Save PCA transformed test dataset just in case\npca_test = pd.DataFrame(pca, columns=pca_col_name_for_test).join(train[['ID_code']])\npca_test.to_csv(\"pca_test.csv\")","10cc03aa":"# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import cross_val_score","1343a85d":"# X = standardized_train.drop('target',axis=1).set_index('ID_code')\n# y = standardized_train[['target']]","38280e7c":"# # Split training dataset to train and validation set\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","79346cb0":"# Split Train Dataset into Predictor variables Matrix and Target variable Matrix\nX_train = standardized_train.set_index(['ID_code','target']).values.astype('float64')\ny_train = standardized_train['target'].values","081889e7":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nlogit_clf = LogisticRegression(random_state=42).fit(X_train,y_train)","dad705f0":"plt.figure(figsize=(10, 10))\nfpr, tpr, thr = roc_curve(y_train, logit_clf.predict_proba(X_train)[:,1])\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operator Characteristic Plot', fontsize=20, y=1.05)\nauc(fpr, tpr)","2e9a39dd":"cross_val_score(logit_clf, X_train, y_train, scoring='roc_auc', cv=10).mean()","9fcef9f3":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n\nlda_clf = LinearDiscriminantAnalysis()\nlda_clf.fit(X_train, y_train)","29d10a2b":"plt.figure(figsize=(6, 6))\nfpr, tpr, thr = roc_curve(y_train, lda_clf.predict_proba(X_train)[:,1])\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operator Characteristic Plot', fontsize=20, y=1.05)\nauc(fpr, tpr)","6b51319b":"cross_val_score(lda_clf, X_train, y_train, scoring='roc_auc', cv=10).mean()","dc87228a":"qda_clf = QuadraticDiscriminantAnalysis()\nqda_clf.fit(X_train, y_train)","80ab8dd2":"plt.figure(figsize=(6, 6))\nfpr, tpr, thr = roc_curve(y_train, qda_clf.predict_proba(X_train)[:,1])\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operator Characteristic Plot', fontsize=20, y=1.05)\nauc(fpr, tpr)","1b48d611":"cross_val_score(qda_clf, X_train, y_train, scoring='roc_auc', cv=10).mean()","ec5b8944":"X_test = standardized_test.set_index('ID_code').values.astype('float64')\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = logit_clf.predict_proba(X_test)[:,1]\nsubmission.to_csv('LR.csv', index=False)","61268306":"X_test = standardized_test.set_index('ID_code').values.astype('float64')\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = lda_clf.predict_proba(X_test)[:,1]\nsubmission.to_csv('lda.csv', index=False)","c38c6622":"X_test = standardized_test.set_index('ID_code').values.astype('float64')\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = qda_clf.predict_proba(X_test)[:,1]\nsubmission.to_csv('lda.csv', index=False)","94d9003e":"X_test = standardized_test.set_index('ID_code').values.astype('float64')\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\n\nlogit_pred = logit_clf.predict_proba(X_test)[:,1]\nlda_pred = lda_clf.predict_proba(X_test)[:,1]\nqda_pred = qda_clf.predict_proba(X_test)[:,1]","31651508":"submission = \\\nsubmission.join(pd.DataFrame(qda_pred, columns=['target1'])).join(pd.DataFrame(logit_pred, columns=['target2'])).\\\njoin(pd.DataFrame(lda_pred, columns=['target3']))","01cfd7bf":"submission['target'] = (submission.target1 + submission.target2 + submission.target3) \/ 3","310b18a1":"submission.head()","71185aeb":"del submission['target1']\ndel submission['target2']\ndel submission['target3']","363d296f":"submission.head()","9e87c857":"submission.to_csv('logit_lda_qda_mean_ensemble.csv', index=False)","23cc4112":"52% and 51% of the variables have higher mean and median values respectively for observations with target==1.","7793056f":"You may want to log or squared transform these non-normal features!","b42c7246":"We have to determine the number of features we are going to extract with PCA! We use the cumulative variance explained and find the number of features where the variance doesn't increase as much.","fed489c7":"> ### **Missing Data** ","6815d0d4":"80% of the total variance is explained if we use 160 principal components. 80% is not bad! Let's reduce 200 features to 160 by setting k=160 for PCA.","a0b35804":"The only categorical feature is the ID_code variable and all other variables are numerical!","6d0f8a51":"#### Quadratic Discriminant Analysis\n- A classifier with a quadratic decision boundary, generated by fitting class conditional densities to the data and using Bayes\u2019 rule.\n- The model fits a Gaussian density to each class.\n- QDA is a better option for large data sets (compared to LDA), as it tends to have a lower bias and a higher variance.","bbe4141a":"So every observation is an unique customer record!","9eae280b":"### **Correlations amongst Variables** (Credits to Gebriel Preda's Kernel \"Santander EDA and Prediction\")","d2091faa":"#### Linear Discriminant Analysis (LDA)\n- LDA aims to find the directions that maximize the separation (or discrimination) between different classes\n- LDA tries to determine a suitable feature (sub)space in order to distinguish between patterns that belong to different classes\n- Estimate parameters with maximum likelihood (those parameters minimize Squared Mahalanobis Distance)\n- Models the distribution of predictors separately in each of the response classes, and then it uses Bayes\u2019 theorem to estimate the probability\n- Both LDA and QDA assume the the predictor variables X are drawn from a multivariate Gaussian (aka normal) distribution.\n- (Compared to QDA) LDA is more suitable for smaller data sets, and it has a higher bias, and a lower variance.\n- If n is small and the distribution of the predictors X is approximately normal in each of the classes, the LDA model is more stable than logistic.\n- When the classes are well-separated, the parameter estimates for the logistic model are surprisingly unstable. LDA does not suffer from this.\n- LDA and QDA are attractive because they have closed-form solutions that can be easily computed, are inherently multiclass, have proven to work well in practice, and have no hyper-parameters to tune.\n","09fe18b5":"# **Exploratory Data Analysis (EDA)**","15bc0254":"Normally, if there is a elbow looking point in the graph above, the x value(number of features) of that point is usually the ideal number of components for PCA. However in this case, each principal component explains very little of the total variance (e.g. first principal component only explains abou 0.6% of the total variance). Even when we sum up all the variance explained by the 80 principal components, it only amounts to 40%. Let's increase the k and see what happens.","f2d1a3a6":"89% for target equal to 0 and 10% for target equal to 1. Pretty unbalanced! We might want to take this unbalance into consideration! (Some algorithms don't perform well with class unbalance). Algorithms like KNN, Boosting, Random forest might work better than others. But the model evaluation metric here is \"AUC\" which is less sensitive to class imbalance (other recommended metrics for unbalanced classes are f1-score and logloss)","6ea14a61":"Just by first glance, observations with target==1 seem to have higher mean & median  values for each variable in general than those with target==0. Let's see if that is the case.","5dee0368":"I am interested in which variables are not linear not likely from a Gaussian Distribution because some ML algorithms work better if each feature is normally distributed (and if not, we might want to log transform it!) There are multiple normality tests but Shapiro test is appropriate for small dataset(N<5000), so I will use the D\u2019Agostino\u2019s K^2 Test!","6f220d37":"> ### **Target Variable** (What we want to predict)","251d7f34":"# **Modelling**","40e0956a":"# **Simple Ensemble Method**","dc47a9bc":"LDA has the highest AUC for cross validation among the three ML algorithms (Logis****tic regression, LDA, QDA) I tried so far! ","0536c66b":"PCA is a dimensionality reduction technique that reduces noise and extracts features that are independent(orthogonal). But PCA is sensitive to variance and different scales, so standardizing will help PCA perform better! HOWEVER, we found that the correlation between different features in the training dataset is not that significant, so using PCA might not be meaningful (because PCA is best when the dimension p is very large and a lot of features are correlated to one another a lot)","a512be7e":"#### Logistic Regression","9dd739de":"There are various Ensemble methods and one way is to use the mean probability of all the models. That is, for each observation, different ML algorithms predict the probability of that observation being part of class 1 and we calculate the mean of all those probabilities.","242873a8":"There is no missing data!","e070abe3":"### **PCA**","3afc4c7c":"### **Distributions of variables**","38ae3eba":"Even the top 10 pairs with highest correlation have absolute values of 0.008. This is a very weak correlation. Multicollinearity issue doesn't seem to be a problem here!"}}