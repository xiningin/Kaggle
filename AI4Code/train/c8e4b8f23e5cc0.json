{"cell_type":{"935cce5b":"code","3eaf2948":"code","8d4aefbe":"code","18bb1987":"code","f8543ce5":"code","cc16a860":"code","e20457fa":"code","2b5a8da5":"code","cef6b40f":"code","e9033016":"code","1da4412f":"code","7533050f":"code","990dd4f1":"code","fc5504cd":"code","3ae1acac":"code","d5750835":"code","e80a886b":"code","0c344950":"code","b0bc4fa1":"code","32049add":"code","3071504b":"markdown","b8e1cbc3":"markdown","626f32a8":"markdown","1c4e2349":"markdown","ae8ca870":"markdown","d9229d25":"markdown","5717649f":"markdown","8aa41edb":"markdown","34b79a03":"markdown","79a6f420":"markdown","2a90cfad":"markdown"},"source":{"935cce5b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3eaf2948":"data = pd.read_csv('..\/input\/data.csv')\ndata.info()","8d4aefbe":"import seaborn as sns\nfrom pandas.plotting import scatter_matrix\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\ndata['diagnosis'] = data['diagnosis'].map({'M':1, 'B':0})\ny = pd.DataFrame(data = data['diagnosis'])   \nlist = ['Unnamed: 32','id','diagnosis']\nx = data.drop(list,axis = 1 )\nx.head()","18bb1987":"#correlation map\nf,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(x.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","f8543ce5":"rem = ['perimeter_mean','radius_mean','compactness_mean','concave points_mean','radius_se','perimeter_se','radius_worst','perimeter_worst','compactness_worst','concave points_worst','compactness_se','concave points_se','texture_worst','area_worst']\ndata_new = x.drop(rem, axis=1)\nf,ax = plt.subplots(figsize=(14, 14))\nsns.heatmap(data_new.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","cc16a860":"data_new['diagnosis'] = y\ncorrmat = data_new.corr().abs()\ntop_corr_features = corrmat.index[abs(corrmat[\"diagnosis\"])>0.1]\nplt.figure(figsize=(10,10))\ng = sns.heatmap(data_new[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","e20457fa":"req = ['texture_mean', 'area_mean', 'smoothness_mean', 'concavity_mean', 'symmetry_mean', 'area_se', 'concavity_se',\n      'smoothness_worst', 'concavity_worst', 'symmetry_worst', 'fractal_dimension_worst']\ndata_new2 = data_new[req]\ndata_new = data_new.drop(['diagnosis'], axis = 1)","2b5a8da5":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score,confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\n# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(data_new2, y, test_size=0.3, random_state=42)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf = RandomForestClassifier(random_state=43)      \nclr_rf = clf_rf.fit(x_train,y_train)\n\nac = accuracy_score(y_test,clf_rf.predict(x_test))\nprint('Accuracy is: ',ac)\ncm = confusion_matrix(y_test,clf_rf.predict(x_test))\nsns.heatmap(cm,annot=True,fmt=\"d\")","cef6b40f":"data_final = pd.read_csv('..\/input\/data.csv')\ny_final = data_final['diagnosis'].map({'M':0, 'B':1})\ndata_final = data_final[req]\ndata_final.info()","e9033016":"from sklearn.model_selection import train_test_split\nx_final = data_final\nx_train, x_test, y_train, y_test = train_test_split(x_final, y_final, test_size=0.2, random_state=42)\nx_train.shape","1da4412f":"from sklearn.model_selection import RandomizedSearchCV\n\ncriterion =['mse', 'friedman_mse', 'mae']\nsplitter =['best', 'random']\nmax_depth = [None, 1, 11, 21, 31]\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nmax_features = ['auto', 'sqrt', 'log2']\n\nrandom_grid = {'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'criterion' :criterion,\n              'splitter' : splitter}\n\nprint(random_grid)\n\nmodel_test = DecisionTreeRegressor(random_state=43)    \nmodel_random = RandomizedSearchCV(estimator = model_test, param_distributions = random_grid, n_iter = 100, cv = 3, random_state = 42, n_jobs = -1)\nmodel_random.fit(x_train, y_train)\nmodel_random.best_params_","7533050f":"clf_rf = RandomForestClassifier(random_state=43, n_estimators = 20, min_samples_split=2, min_samples_leaf=2,\n                               max_features='sqrt', max_depth=21,bootstrap=False)      \nclr_rf = clf_rf.fit(x_train,y_train)\n\nprint(r2_score(y_test,clf_rf.predict(x_test)))\nprint(mean_squared_error(y_test, clf_rf.predict(x_test)))\n","990dd4f1":"from sklearn.ensemble import ExtraTreesClassifier\n\netc_test = ExtraTreesClassifier(random_state=43, n_estimators=300, min_samples_split=2,min_samples_leaf=1,\n                               max_features='sqrt',max_depth=31, bootstrap=True)\netc_test = etc_test.fit(x_train,y_train)\n\nprint(r2_score(y_test,etc_test.predict(x_test)))\nprint(mean_squared_error(y_test, etc_test.predict(x_test)))","fc5504cd":"from sklearn.ensemble import ExtraTreesRegressor\n\netr_test = ExtraTreesRegressor(random_state=43,n_estimators=100, min_samples_split=2, min_samples_leaf=1,\n                               max_features='auto',max_depth = None,bootstrap=False)\netr_test = etr_test.fit(x_train,y_train)\n\nprint(r2_score(y_test,etr_test.predict(x_test)))\nprint(mean_squared_error(y_test, etr_test.predict(x_test)))","3ae1acac":"from sklearn.ensemble import RandomForestRegressor\n\nrfr_test = RandomForestRegressor(random_state=43,n_estimators=200,min_samples_split=2,min_samples_leaf=1,\n                                max_features='log2',max_depth=11, bootstrap=False)\nrfr_test = rfr_test.fit(x_train,y_train)\n\nprint(r2_score(y_test,rfr_test.predict(x_test)))\nprint(mean_squared_error(y_test, rfr_test.predict(x_test)))","d5750835":"from sklearn.tree import DecisionTreeClassifier\n\ndtc_test = DecisionTreeClassifier(random_state=43,min_samples_leaf=8)\ndtc_test = dtc_test.fit(x_train,y_train)\n\nprint(r2_score(y_test,dtc_test.predict(x_test)))\nprint(mean_squared_error(y_test, dtc_test.predict(x_test)))","e80a886b":"from sklearn.tree import DecisionTreeRegressor\n\ndtr_test = DecisionTreeRegressor(random_state=43,splitter='best',min_samples_split=2,min_samples_leaf=8,\n                                max_features='auto', max_depth=11, criterion='mse')\ndtr_test = dtr_test.fit(x_train,y_train)\n\nprint(r2_score(y_test,dtr_test.predict(x_test)))\nprint(mean_squared_error(y_test, dtr_test.predict(x_test)))","0c344950":"from sklearn import ensemble\ngbr = ensemble.GradientBoostingRegressor(n_estimators = 400, max_depth = 7, min_samples_split = 8,\n          learning_rate = 0.1, loss = 'ls')\ngbr = gbr.fit(x_train,y_train)\n\nprint(r2_score(y_test,gbr.predict(x_test)))\nprint(mean_squared_error(y_test, gbr.predict(x_test)))","b0bc4fa1":"from mlxtend.regressor import StackingCVRegressor\nrfc = RandomForestClassifier(random_state=43, n_estimators = 20, min_samples_split=2, min_samples_leaf=2,\n                               max_features='sqrt', max_depth=21,bootstrap=False)   \netc = ExtraTreesClassifier(random_state=43, n_estimators=300, min_samples_split=2,min_samples_leaf=1,\n                               max_features='sqrt',max_depth=31, bootstrap=True)\netr = ExtraTreesRegressor(random_state=43,n_estimators=100, min_samples_split=2, min_samples_leaf=1,\n                               max_features='auto',max_depth = None,bootstrap=False)\nrfr = RandomForestRegressor(random_state=43,n_estimators=200,min_samples_split=2,min_samples_leaf=1,\n                                max_features='log2',max_depth=11, bootstrap=False)\ndtc = DecisionTreeClassifier(random_state=43,min_samples_leaf=8)\ndtr = DecisionTreeRegressor(random_state=43,splitter='best',min_samples_split=2,min_samples_leaf=8,\n                                max_features='auto', max_depth=11, criterion='mse')\ngbr = ensemble.GradientBoostingRegressor(n_estimators = 400, max_depth = 7, min_samples_split = 8,\n          learning_rate = 0.1, loss = 'ls')\nstack_gen = StackingCVRegressor(regressors=(rfc, etr, rfr, dtc, dtr),\n                                meta_regressor=dtr,\n                                use_features_in_secondary=True)\n\nstack_gen_model = stack_gen.fit(x_train,y_train)\n\nprint(r2_score(y_test,stack_gen_model.predict(x_test)))\nprint(mean_squared_error(y_test, stack_gen_model.predict(x_test)))","32049add":"def blend_models_predict(X):\n    return ((0.05 * etc.predict(X)) + \\\n            (0.05 * gbr.predict(X)) + \\\n            (0.1 * rfc.predict(X)) + \\\n            (0.1 * rfr.predict(X)) + \\\n            (0.1 * dtc.predict(X)) + \\\n            (0.2 * dtr.predict(X)) + \\\n            (0.1 * etr.predict(X)) + \\\n            (0.3 * stack_gen_model.predict(np.array(X))))\n\netc_model = etc.fit(x_train,y_train)\ngbr_model = gbr.fit(x_train,y_train)\nrfc_model = rfc.fit(x_train,y_train)\nrfr_model = rfr.fit(x_train,y_train)\ndtc_model = dtc.fit(x_train,y_train)\ndtr_model = dtr.fit(x_train,y_train)\netr_model = etr.fit(x_train,y_train)\n\nprint(r2_score(y_test,blend_models_predict(x_test)))\nprint(mean_squared_error(y_test, blend_models_predict(x_test)))","3071504b":"To begin, we shall look at the correlation of the features (i.e. exclduing that of Diagnosis)","b8e1cbc3":"To optimize the code, we will use the Randomized Search CV to help.","626f32a8":"Now we can start modelling. We will be stacking and mixing the models for this test run.","1c4e2349":"Our objective is to evaluate the diagnosis based on data. ","ae8ca870":"Once again, we further filter out the features, leaving only those that have sufficiently strong correlation with that of diagnosis.","d9229d25":"The idea is that we should remove features that are correlated, as it will lower the number of features to work with. Once we have done so, we can then compare the correlation of the remaining features against that of diagnosis.","5717649f":"With that in mind, we can recreate the new data based on the required columns.","8aa41edb":"Notice that the data is very clean, with no Null values, and all being floats (except diagnosis)","34b79a03":"Lastly, we can blend the models together to gain a prediction.","79a6f420":"After testing and confirming the different models, we can join them all together using Stacking CV Regressor.","2a90cfad":"To check if the data agrees with what was selected, we can do a simple test."}}