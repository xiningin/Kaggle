{"cell_type":{"dd630ac6":"code","faa6823e":"code","198440f1":"code","30f72c19":"code","ef53cf9c":"code","5ed23892":"code","ff29bbb2":"code","a5fb87b7":"code","1fe76d93":"code","f26ebda4":"code","bfebd216":"code","dbcd86c4":"code","3c2a673f":"code","acbd5540":"code","6f2e0794":"code","5be27c4b":"code","7b96ef01":"code","14eb3a0b":"code","7b125656":"code","79028125":"code","aa13fd0f":"code","4f395d64":"code","00d3e82b":"code","03e6e959":"code","18d70aea":"code","5bb87bc9":"code","c286f3b1":"code","df98718c":"code","d9ef8311":"code","19d73d2e":"code","51ced96c":"code","d2e87ca8":"code","4da86b77":"code","9e58129b":"code","5fb499d1":"code","6535da12":"markdown","e7ee5156":"markdown","0b72df9f":"markdown"},"source":{"dd630ac6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","faa6823e":"!pip install tensorflow --upgrade","198440f1":"import numpy as np\nnp.random.seed(42)\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nimport matplotlib.pyplot as plt \n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nos.environ['OMP_NUM_THREADS'] = '4'","30f72c19":"train = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv')\ntest = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv')\nlocal_sumb = pd.read_csv('..\/input\/saved-relations\/gru_we_submission.csv')\nsubm = pd.read_csv('..\/input\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv')\n","ef53cf9c":"test.head()","5ed23892":"subm.head()","ff29bbb2":"train.head()","a5fb87b7":"text = train['comment_text']","1fe76d93":"text[0]","f26ebda4":"train['comment_text'][0]","bfebd216":"# for train\nlens = train.comment_text.str.len()\nlens.mean(), lens.std(), lens.max()","dbcd86c4":"# for test\nlens = test.comment_text.str.len()\nlens.mean(), lens.std(), lens.max()","3c2a673f":"lens.hist();","acbd5540":"label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ntrain['none'] = 1-train[label_cols].max(axis=1) ## each colum may have the value of one ( Labled ) . 1- calc the max # if has no lable max = 0 then col = 1 -0 = 0\ntrain.describe()","6f2e0794":"len(train),len(test)","5be27c4b":"## deal with nulls \nCOMMENT = 'comment_text'\ntrain[COMMENT].fillna(\"unknown\", inplace=True)\ntest[COMMENT].fillna(\"unknown\", inplace=True)","7b96ef01":"import re, string\nre_tok = re.compile(f'([{string.punctuation}\u201c\u201d\u00a8\u00ab\u00bb\u00ae\u00b4\u00b7\u00ba\u00bd\u00be\u00bf\u00a1\u00a7\u00a3\u20a4\u2018\u2019])')\ndef tokenize(s): return re_tok.sub(r' \\1 ', s).split()\ndef clean(s): return re_tok.sub(r' \\1 ', s)","14eb3a0b":"## decide vocab size \nwords = []\nfor t in text:\n    words.extend(tokenize(t))\nprint(words[:100])\nvocab = list(set(words))\nprint(len(words), len(vocab))","7b125656":"train['comment_text'][0]","79028125":"clean(train['comment_text'][0])","aa13fd0f":"def full_one_hot_word_embedding(vtrain_data,vtest_data):\n    # switch data back to text \n    train_labels = vtrain_data[label_cols]\n    txt_train_data = [clean(txt) for txt in train['comment_text']]\n    txt_test_data = [clean(txt) for txt in test['comment_text']]\n    \n    # integer encode the documents\n    vocab_size = 10000\n    encoded_txt_train_data = [keras.preprocessing.text.one_hot(d, vocab_size) for d in txt_train_data]\n    encoded_txt_test_data = [keras.preprocessing.text.one_hot(d, vocab_size) for d in txt_test_data]\n    #print(encoded_txt_train_data)\n\n    ptxt_train_data = keras.preprocessing.sequence.pad_sequences(encoded_txt_train_data,\n                                                            padding='post',\n                                                            maxlen=5000)\n\n    ptxt_test_data = keras.preprocessing.sequence.pad_sequences(encoded_txt_test_data,\n                                                           padding='post',\n                                                           maxlen=5000)\n    partial_x_train = ptxt_train_data[:100]\n    partial_y_train = train_labels[:100]\n    return (partial_x_train,partial_y_train,ptxt_test_data)","4f395d64":"max_features = 30000\nmaxlen = 5000\nembed_size = 300\nvocab_size = 10000\n\ndef get_model():\n    inp = keras.layers.Input(shape=(maxlen, ))\n    x = keras.layers.Embedding(vocab_size, 16)(inp)\n    x = keras.layers.SpatialDropout1D(0.2)(x)\n    x = keras.layers.GRU(80, return_sequences=True)(x)\n    # x = keras.layers.Bidirectional(keras.layers.LSTM(80, return_sequences=True))(x)\n    avg_pool = keras.layers.GlobalAveragePooling1D()(x)\n    max_pool = keras.layers.GlobalMaxPooling1D()(x)\n    conc = keras.layers.concatenate([avg_pool, max_pool])\n    outp = keras.layers.Dense(6, activation=\"sigmoid\")(conc)\n    \n    model = keras.models.Model(inputs=inp, outputs=outp)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n    return model","00d3e82b":"x_train,y_train,test_data = full_one_hot_word_embedding(train[:100],test[:100]) ","03e6e959":"x_train.shape,y_train.shape","18d70aea":"model1 = get_model()","5bb87bc9":"batch_size = 32\nepochs = 1\n\nX_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n\nX_tra = X_tra.astype(np.float32)\nX_val = X_val.astype(np.float32)\ny_tra = y_tra.values.astype(np.float32)\ny_val = y_val.values.astype(np.float32)","c286f3b1":"hist = model1.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n                  verbose=1)","df98718c":"# serialize model to JSON\nmodel_json = model1.to_json()\nwith open(\"my_model1.json\", \"w\") as json_file:\n    json_file.write(model_json)\n    \nmodel1.save_weights('my_model1_weights.h5')","d9ef8311":"### load the model 1 \n# load json and create model\njson_file = open('my_model1.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nmodel1 = tf.keras.models.model_from_json(loaded_model_json)\n# load weights into new model\nmodel1.load_weights(\"my_model1_weights.h5\")\nprint(\"Loaded model from disk\")","19d73d2e":"def column(matrix, i):\n    return [row[i] for row in matrix]","51ced96c":"##y_pred = model.predict(test_data, batch_size=1024)","d2e87ca8":"submission = local_sumb","4da86b77":"#submission = pd.DataFrame()\n#submission['id'] = test['id']\n#submission['toxic'] = column(y_pred, 0)\n#submission['severe_toxic'] = column(y_pred, 1)\n#submission['obscene'] = column(y_pred, 2)\n#submission['threat'] = column(y_pred, 3)\n#submission['insult'] = column(y_pred, 4)\n#submission['identity_hate'] = column(y_pred, 5)","9e58129b":"submission.to_csv('submission.csv', index=False)","5fb499d1":"submission","6535da12":"### Data Exploration","e7ee5156":"### Building the model\u00b6\n","0b72df9f":"## Kaggle score 0.95 ---> > > result uploaded from local machine \n## Kaggle refused to accelerate GPU for this notebook\n## Proof of concept that Single Direction GRU can also make good results "}}