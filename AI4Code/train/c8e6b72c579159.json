{"cell_type":{"541af4d1":"code","bc5b52ef":"code","cf7061a6":"markdown"},"source":{"541af4d1":"import pandas as pd; pd.set_option('mode.chained_assignment','raise');\nimport numpy as np\nimport scipy\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n\ndef run_normality_test(y = None):\n    if(y is None):\n        # our sample data\n        y= pd.Series([1.16,1.3,1.16,1.15,1.2,1.19,1.13,1.11,1.22,1.08,1.14,1.41,1.09,1.11,1.44,1.33,1.24,1.06,1.11,1.11,1.09,1.26])\n    elif(type(y) != pd.Series):\n        y = pd.Series(y)\n    ## Test for similarity to distribution \/ normality\n    # \n    # Always remember: If the checks are not giving good results you are allowed\n    # and encouraged to transform the data\n    \n    \n    \n    # Version 0: The histogram first look\n    y.hist()\n    \n    \n    # Version 1: QQPlot. Should follow 45 degrees line. Can check any distribution    \n    # however for real data, you must shift the comparing normal distribution\n    sm.qqplot(y, loc=np.mean(y), scale=np.std(y), dist=scipy.stats.norm, line=\"45\")\n    plt.show()\n    \n    \n    ## Version 2 KS.Test\n    from scipy.stats import kstest\n        \n    kstest(y, y) # test with another drawn distribution   \n    statistics, p = kstest(y, scipy.stats.norm.rvs(loc=np.mean(y), scale=np.std(y), size=10000)) # Test against normal distribution\n    if p > 0.05: \n        print(\"According to KS-Test sample looks gaussian, alpha 0.05\")\n    else:\n        print(\"According to KS-Test sample does NOT looks gaussian, alpha 0.05\")\n    \n    # Downsides: The test has a low \"Sensitivit\u00e4t\". Meaning that it will quite seldom tell, that the distributions are different. Even if they are.\n    # However, when the test tells, that they are diffrent (In R through a p-value below 5% or 1% etc.) than it is highly likely not the same distribution.\n    # So the test is a first indicator! And if we have two samples we might suggest, that they are really different.\n    \n    ## Version 3: Shapiro Test\n    stat, p = scipy.stats.shapiro(np.random.normal(0,1,10)) # this will be normal\n    stat, p = scipy.stats.shapiro(y) # this is our demo sample\n    print(stat, p)\n    if p > 0.05: \n        print(\"According to Shapiro sample looks gaussian, alpha 0.05\")\n    else:\n        print(\"According to Shapiro sample does NOT looks gaussian, alpha 0.05\")\n    \n    \n    # Version 4: D'Agostino K^2 Test\n    from scipy.stats import normaltest\n    stat, p = normaltest(y)\n    alpha = 0.05\n    if p > alpha:\n    \tprint('According to D\\'Agostino sample looks Gaussian (fail to reject H0), alpha 0.05')\n    else:\n    \tprint('According to D\\'Agostino sample not look Gaussian (reject H0), alpha 0.05')\n        \n  \n\n\n\n        \n        \n        \n        \n        ","bc5b52ef":"run_normality_test()","cf7061a6":"# Normality Tests\n\nThis notebooks shows three very common normality tests combined into one function.\n\nNormality checks are usually done when using parametric statstical methods. The assumption\nof normality should hold to make further claims of the data.\n\nThere are also many more test: https:\/\/machinelearningmastery.com\/a-gentle-introduction-to-normality-tests-in-python\/T\n\nGenerally one should to as many tests as possible.\nIf you MUST be extremely normal, then if one tests fails, you cannot assume gaussian style\n\nHowever often in machine learning and regression you do not have to be 100% gaussian. \nA gaussian-like is often ok. So having only some tests fail (less than 50% of the tests!) may be sufficient"}}