{"cell_type":{"d2c96e0f":"code","c06083b0":"code","25fa5820":"code","55970229":"code","e18517de":"code","25a16765":"code","4e056cff":"code","e1ba87fd":"code","c9d1313c":"code","904bd289":"code","76dff967":"code","070ea4fe":"code","4f77096c":"code","4f95c3ec":"code","9383f3cc":"code","df72db24":"code","d4b53267":"code","94763a33":"code","cc572d40":"code","2d3d46d2":"code","9918deba":"code","719ad991":"code","3167bdca":"code","1da9845a":"code","bafdde03":"code","c68a8c5f":"code","9c89ddd4":"code","75df3e07":"code","d0eba98c":"markdown","e0a7062e":"markdown","f9b00d7d":"markdown","4f9cf8f1":"markdown","c2ca89ba":"markdown"},"source":{"d2c96e0f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c06083b0":"!pip install sentencepiece","25fa5820":"import random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\nimport tensorflow as tf\nimport transformers\nfrom transformers import AutoTokenizer, TFAutoModelForSequenceClassification, TFAutoModel\nfrom transformers import AdamW\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport os\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\nos.environ[\"WANDB_API_KEY\"] = \"0\"","55970229":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy() # for CPU and single GPU\n\nprint('Number of replicas:', strategy.num_replicas_in_sync)    ","e18517de":"BATCH_SIZE= 16 * strategy.num_replicas_in_sync","25a16765":"train = pd.read_csv(\"\/kaggle\/input\/contradictory-my-dear-watson\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/contradictory-my-dear-watson\/test.csv\")","4e056cff":"train['label'].value_counts()\ntrain.shape\ntrain.head()","e1ba87fd":"print(train['premise'].str.len().describe())\nprint(train['hypothesis'].str.len().describe())","c9d1313c":"print(test['premise'].str.len().describe())\nprint(test['hypothesis'].str.len().describe())","904bd289":"wrong_labels = train.groupby(['premise', 'hypothesis']).nunique().sort_values(by='label', ascending=False)\nwrong_labels","76dff967":"train_labels, train_frequencies = np.unique(train.language.values, return_counts = True)\ntest_labels, test_frequencies = np.unique(test.language.values, return_counts = True)\n\nplt.figure(figsize = (10,10))\nplt.pie(train_frequencies,labels = train_labels, autopct = '%1.2f%%')\nplt.show()","070ea4fe":"plt.figure(figsize = (10,10))\nplt.pie(test_frequencies,labels = test_labels, autopct = '%1.2f%%')\nplt.show()","4f77096c":"language_list = ['English', 'French', 'Spanish', 'German', 'Greek', 'Bulgarian', 'Russian', 'Turkish', 'Arabic' ,'Vietnamese', 'Thai', 'Chinese', 'Hindi', 'Swahili', 'Urdu']","4f95c3ec":"for langs in train.language.unique():\n    if langs not in language_list:\n        print(langs)","9383f3cc":"for langs in test.language.unique():\n    if langs not in language_list:\n        print(langs)","df72db24":"train_premise_texts, val_premise_texts, train_hypothesis_texts, val_hypothesis_texts, train_labels, val_labels = train_test_split(train['premise'].tolist(), train['hypothesis'].tolist(), train['label'].tolist())","d4b53267":"tokenizer = AutoTokenizer.from_pretrained('joeddav\/xlm-roberta-large-xnli')","94763a33":"train_encodings = tokenizer(train_premise_texts, train_hypothesis_texts,  padding=\"max_length\", truncation=True, return_tensors='tf')\nval_encodings = tokenizer(val_premise_texts, val_hypothesis_texts,  padding=\"max_length\", truncation=True, return_tensors='tf')\n\ntest_encodings = tokenizer(test['premise'].tolist(), test['hypothesis'].tolist(),  padding=\"max_length\", truncation=True, return_tensors='tf')","cc572d40":"train_tf_dataset = tf.data.Dataset.from_tensor_slices((dict(train_encodings), train_labels))\ntrain_tf_dataset = train_tf_dataset.shuffle(len(train_encodings)).batch(BATCH_SIZE)\n\neval_tf_dataset = tf.data.Dataset.from_tensor_slices((dict(val_encodings), val_labels))\neval_tf_dataset = eval_tf_dataset.batch(BATCH_SIZE)","2d3d46d2":"try_encodings = tokenizer(train['premise'].tolist()[:3], train['hypothesis'].tolist()[:3],  padding=\"max_length\", truncation=True, return_tensors='tf')","9918deba":"MAX_LEN = train_encodings['input_ids'].shape[1]\n\nwith strategy.scope():\n    input_ids = tf.keras.Input(shape =(512,), dtype=tf.int32, name='input_ids') \n    attention_mask = tf.keras.Input(shape=(512,),dtype=tf.int32, name='attention_mask')  \n    \n    roberta = TFAutoModel.from_pretrained('joeddav\/xlm-roberta-large-xnli')\n    \n    for layers in roberta.layers:\n        layers.trainable = False\n        \n    tf_model_output = roberta([input_ids, attention_mask])[1]\n    \n    tf_model_output = tf.keras.layers.Dense(512, activation = 'relu')(tf_model_output)\n    tf_model_output = tf.keras.layers.Dropout(0.2)(tf_model_output)\n    tf_model_output = tf.keras.layers.Dense(256, activation = 'relu')(tf_model_output)\n    tf_model_output = tf.keras.layers.Dropout(0.1)(tf_model_output)\n    tf_model_output = tf.keras.layers.Dense(64, activation = 'relu')(tf_model_output)\n    output = tf.keras.layers.Dense(3, activation = 'softmax')(tf_model_output)\n        \n    model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=output)\n    \n    model.compile(optimizer = tf.keras.optimizers.Adam(lr=1e-3), \n                  loss='sparse_categorical_crossentropy', \n                  metrics=['accuracy']) \n    \n    model.summary()","719ad991":"early_stop = tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n\nmodel.fit(train_tf_dataset, \n        validation_data=eval_tf_dataset,\n        callbacks = [early_stop],\n        epochs=20,\n        verbose=1)\n","3167bdca":"for layers in roberta.layers:\n    layers.trainable = True\nmodel.compile(optimizer = tf.keras.optimizers.Adam(lr=3e-5), \n                  loss='sparse_categorical_crossentropy',  \n                  metrics=['accuracy'])","1da9845a":"early_stop = tf.keras.callbacks.EarlyStopping(patience=2, restore_best_weights=True)\n\nmodel.fit(train_tf_dataset, \n        validation_data=eval_tf_dataset,\n        callbacks = [early_stop],\n        epochs=8,\n        verbose=1)","bafdde03":"test = pd.read_csv('..\/input\/contradictory-my-dear-watson\/test.csv')","c68a8c5f":"test_encoded = tokenizer(test['premise'].tolist(), test['hypothesis'].tolist(), padding='max_length', return_tensors='tf')\npredictions = [np.argmax(i) for i in model.predict(test_encoded.data)]","9c89ddd4":"submission = test.id.copy().to_frame()\nsubmission['prediction'] = predictions\nprint(submission['prediction'].value_counts())\nprint(submission.head())","75df3e07":"submission.to_csv(\"submission.csv\", index=False)","d0eba98c":"There are only 15 languages handled by by this model","e0a7062e":"Lets check for Any wrong labels","f9b00d7d":"# Lets do a basic EDA","4f9cf8f1":"Look at Test Data","c2ca89ba":"# Lets fine tune the whole model now"}}