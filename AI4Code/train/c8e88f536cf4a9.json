{"cell_type":{"a5a66d64":"code","26f8cd61":"code","9f9a9be4":"code","306492c6":"code","78422eec":"code","9ee1e554":"code","9afa9b76":"code","5579392e":"code","bb14e503":"code","0c8b5960":"code","71e6c91d":"code","a236dff3":"code","f74e71cb":"code","f1d389df":"code","2fc418a0":"code","6fbc1921":"code","9ea1dc66":"code","6e4197c4":"code","9794e4d1":"markdown","c29c4194":"markdown","95dbcc65":"markdown","79d05209":"markdown","b489f09f":"markdown","8d662541":"markdown","cbde07c4":"markdown","02d04360":"markdown","0dcdc51f":"markdown","0d27b56a":"markdown","32ff9e1d":"markdown","689645dc":"markdown","0ce97782":"markdown","673db13b":"markdown","9222058f":"markdown"},"source":{"a5a66d64":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pydicom\nimport pandas as pd\nfrom glob import glob\nimport os\nfrom matplotlib.patches import Rectangle\ndet_class_path = '..\/input\/stage_1_detailed_class_info.csv'\nbbox_path = '..\/input\/stage_1_train_labels.csv'\ndicom_dir = '..\/input\/stage_1_train_images\/'\ntest_dicom_dir = '..\/input\/stage_1_test_images\/'","26f8cd61":"det_class_df = pd.read_csv(det_class_path)\nprint(det_class_df.shape[0], 'class infos loaded')\nprint(det_class_df['patientId'].value_counts().shape[0], 'patient cases')\ndet_class_df.groupby('class').size().plot.bar()\ndet_class_df.sample(3)","9f9a9be4":"bbox_df = pd.read_csv(bbox_path)\nprint(bbox_df.shape[0], 'boxes loaded')\nprint(bbox_df['patientId'].value_counts().shape[0], 'patient cases')\nbbox_df.sample(3)","306492c6":"# we first try a join and see that it doesn't work (we end up with too many boxes)\ncomb_bbox_df = pd.merge(bbox_df, det_class_df, how='inner', on='patientId')\nprint(comb_bbox_df.shape[0], 'combined cases')","78422eec":"comb_bbox_df = pd.concat([bbox_df, \n                        det_class_df.drop('patientId',1)], 1)\nprint(comb_bbox_df.shape[0], 'combined cases')\ncomb_bbox_df.sample(3)","9ee1e554":"box_df = comb_bbox_df.groupby('patientId').\\\n    size().\\\n    reset_index(name='boxes')\ncomb_box_df = pd.merge(comb_bbox_df, box_df, on='patientId')\nbox_df.\\\n    groupby('boxes').\\\n    size().\\\n    reset_index(name='patients')","9afa9b76":"comb_bbox_df.groupby(['class', 'Target']).size().reset_index(name='Patient Count')","5579392e":"image_df = pd.DataFrame({'path': glob(os.path.join(dicom_dir, '*.dcm'))})\nimage_df['patientId'] = image_df['path'].map(lambda x: os.path.splitext(os.path.basename(x))[0])\nprint(image_df.shape[0], 'images found')\nimg_pat_ids = set(image_df['patientId'].values.tolist())\nbox_pat_ids = set(comb_box_df['patientId'].values.tolist())\n# check to make sure there is no funny business\nassert img_pat_ids.union(box_pat_ids)==img_pat_ids, \"Patient IDs should be the same\"","bb14e503":"DCM_TAG_LIST = ['PatientAge', 'BodyPartExamined', 'ViewPosition', 'PatientSex']\ndef get_tags(in_path):\n    c_dicom = pydicom.read_file(in_path, stop_before_pixels=True)\n    tag_dict = {c_tag: getattr(c_dicom, c_tag, '') \n         for c_tag in DCM_TAG_LIST}\n    tag_dict['path'] = in_path\n    return pd.Series(tag_dict)\nimage_meta_df = image_df.apply(lambda x: get_tags(x['path']), 1)\n# show the summary\nimage_meta_df['PatientAge'] = image_meta_df['PatientAge'].map(int)\nimage_meta_df['PatientAge'].hist()\nimage_meta_df.drop('path',1).describe(exclude=np.number)","0c8b5960":"test_image_df = pd.DataFrame({'path': glob(os.path.join(test_dicom_dir, '*.dcm'))})\ntest_image_meta_df = test_image_df.apply(lambda x: get_tags(x['path']), 1)\n# show the summary\ntest_image_meta_df['PatientAge'] = test_image_meta_df['PatientAge'].map(int)\ntest_image_meta_df['PatientAge'].hist()\ntest_image_meta_df.to_csv('test_stats.csv')\ntest_image_meta_df.drop('path',1).describe(exclude=np.number)","71e6c91d":"image_full_df = pd.merge(image_df,\n                         image_meta_df,\n                         on='path')\nimage_bbox_df = pd.merge(comb_box_df, \n                         image_full_df, \n                         on='patientId',\n                        how='left')\nprint(image_bbox_df.shape[0], 'image bounding boxes')\nimage_bbox_df.sample(3)","a236dff3":"image_bbox_df = image_bbox_df[image_bbox_df['class'].isin(['Normal', 'Lung Opacity'])]","f74e71cb":"sample_df = image_bbox_df.\\\n    groupby(['Target','class', 'boxes']).\\\n    apply(lambda x: x[x['patientId']==x.sample(1)['patientId'].values[0]]).\\\n    reset_index(drop=True)\nsample_df","f1d389df":"fig, m_axs = plt.subplots(2, 3, figsize = (20, 10))\nfor c_ax, (c_path, c_rows) in zip(m_axs.flatten(),\n                    sample_df.groupby(['path'])):\n    c_dicom = pydicom.read_file(c_path)\n    c_ax.imshow(c_dicom.pixel_array, cmap='bone')\n    c_ax.set_title('{class}'.format(**c_rows.iloc[0,:]))\n    for i, (_, c_row) in enumerate(c_rows.dropna().iterrows()):\n        c_ax.plot(c_row['x'], c_row['y'], 's', label='{class}'.format(**c_row))\n        c_ax.add_patch(Rectangle(xy=(c_row['x'], c_row['y']),\n                                width=c_row['width'],\n                                height=c_row['height'], \n                                 alpha = 0.5))\n        if i==0: c_ax.legend()","2fc418a0":"import zipfile as zf\nfrom skimage.io import imread, imsave\nfrom skimage.transform import resize\nfrom tqdm import tqdm_notebook\nimport warnings\nwarnings.simplefilter('default')\n\nall_rows = []\nDS_SIZE = 512\nGROUP_SIZE = 25\nAGE_SPLITS = 5\nwith zf.ZipFile('high_res.zip', 'w') as hrz, zf.ZipFile('low_res.zip', 'w') as lrz:\n    for finding, rows_df in tqdm_notebook(image_bbox_df.groupby(['class'])):\n        # only 1 per patient\n        clean_rows_df = rows_df.groupby('patientId').apply(lambda x: x.sample(1)).reset_index(drop=True)\n        # group by age and gender\n        clean_rows_df['age_group'] = pd.qcut(\n            clean_rows_df['PatientAge'], AGE_SPLITS)\n        ag_groups = clean_rows_df.groupby(['age_group', \n                                           'PatientSex'])\n        \n        if len(ag_groups)<GROUP_SIZE:\n            # for some patients the count is very low\n            # and so we cant really stratify\n            out_rows = clean_rows_df.sample(min(GROUP_SIZE, clean_rows_df.shape[0]))\n        else:\n            # if we have enough stratify by age and gender\n            out_rows = ag_groups.apply(lambda x: \n                                       x.sample(GROUP_SIZE\/\/AGE_SPLITS)).reset_index(drop=True)\n        \n        print(finding, out_rows.shape[0],'\/',clean_rows_df.shape[0], 'cases')\n        arc_path = lambda x: os.path.basename(x)\n        for _, c_row in out_rows.iterrows():\n            hrz.write(c_row['path'],\n                arcname = arc_path(c_row['path']),\n                compress_type = zf.ZIP_STORED)\n            full_image = pydicom.read_file(c_row['path']).pixel_array\n            full_image = full_image\/full_image.max()\n            rs_img = resize(full_image, (DS_SIZE, DS_SIZE))\n            rgb_rs = plt.cm.gray(rs_img)[:, :, :3]\n            imsave('test.png', rgb_rs)\n            lrz.write('test.png',\n                arcname = arc_path(c_row['path']),\n                compress_type = zf.ZIP_STORED)\n        out_rows['Image Index'] = out_rows['path'].map(arc_path)\n        all_rows += [out_rows]","6fbc1921":"dataset_df = pd.concat(all_rows)\ndataset_df.to_csv('dataset_overview.csv', index=False)\ndataset_df.sample(3)","9ea1dc66":"import seaborn as sns\nsns.swarmplot(y='Target', \n               x = 'PatientAge', \n               hue = 'PatientSex',\n               data = dataset_df)\ndataset_df.groupby('Target').size().reset_index(name='counts')","6e4197c4":"import json\nannotation_task = {\n    'google_forms': {'form_url': 'https:\/\/docs.google.com\/forms\/d\/e\/1FAIpQLSemGag9uitnPnV6OBHDNgrvr2nh-jArJZhVco0Kfjkx4eRkYA\/viewform', \n    'sheet_url': 'https:\/\/docs.google.com\/spreadsheets\/d\/1JUCLX_17JIGit0Nk4wphgTHlmji9u9PYPmyf_9Wscvg\/edit?usp=sharing'\n            },\n    'dataset': {\n        'image_path': 'Image Index', # column name\n        'output_labels': 'class', # column name\n        'dataframe': dataset_df.drop('age_group', 1).to_dict(),\n        'base_image_directory': 'sample_data', # path\n        'questions': {'Normal': 'Is there no opacity present in this image?', \n                      'Lung Opacity': 'Is there an opacity present in this image?'} \n    }\n}\n\nwith open('task.json', 'w') as f:\n    json.dump(annotation_task, f, indent=4, sort_keys=True)","9794e4d1":"# Detailed Class Info\nHere we show the image-level labels for the scans. The most interesting group here is the `No Lung Opacity \/ Not Normal` since they are cases that look like opacity but are not. So the first step might be to divide the test images into clear groups and then only perform the bounding box prediction on the suspicious images.","c29c4194":"# How are class and target related?\nI assume that all the `Target=1` values fall in the `Lung Opacity` class, but it doesn't hurt to check.","95dbcc65":"# Show the Stage1\/Test Dataset","79d05209":"## Create Sample Data Set\nWe create a sample dataset covering different cases, and number of boxes","b489f09f":"# Images\nNow that we have the boxes and labels loaded we can examine a few images.","8d662541":"# Load the Bounding Box Data\nHere we show the bounding boxes","cbde07c4":"# Enrich the image fields\nWe have quite a bit of additional data in the DICOM header we can easily extract to help learn more about the patient like their age, view position and gender which can make the model much more precise","02d04360":"# Combine Boxes and Labels\nHere we bring the labels and the boxes together and now we can focus on how the boxes look on the images","0dcdc51f":"## Only keep clear cases of opacity and normality","0d27b56a":"# Distribution of Boxes and Labels\nThe values below show the number of boxes and the patients that have that number. ","32ff9e1d":"## Concatenate\nWe have to concatenate the two datasets and then we get class and target information on each region","689645dc":"## Show the position and bounding box\nHere we can see the position (point) and the bounding box for each of the different image types","0ce97782":"# Combine the Training Data with Labels","673db13b":"# Overview\nThe notebook makes a quick overview of the training and test data for the Lung Opacity competition and makes a small training set for using the jupyanno tool","9222058f":"# Create Task File"}}