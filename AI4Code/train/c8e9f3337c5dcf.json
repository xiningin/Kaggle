{"cell_type":{"c55e84c1":"code","ee4596ec":"code","67555ba6":"code","329edaa9":"code","3debc24f":"code","c1bea2ec":"code","75440dda":"code","09c4d250":"code","a4260e95":"code","3034b9c6":"code","a3b97e9a":"code","e66981d6":"code","2c13f1a2":"code","2499dc5e":"code","335b2d5a":"code","5eace9e1":"code","858cf16d":"code","fdaa090a":"code","9e92016a":"code","fd43d7e0":"code","04e352c1":"code","a0db566f":"code","06a7df28":"code","d02c6ffd":"code","cdf9de89":"code","186f6e81":"code","a20507c7":"code","c14cd2b5":"code","f0841a1c":"code","6d4b0867":"code","6c4117ad":"code","58f67da8":"code","3f6a5f41":"code","d38c322f":"code","e8c05cb0":"code","3f6c6268":"code","5c8f5a0a":"code","d29b7a64":"code","31c47d63":"code","19db8423":"code","d018256f":"code","0fed6bf9":"code","a9df82d7":"code","32696697":"code","03eace09":"code","ff1f71d1":"code","9fe1a3d2":"code","7bdb7891":"code","1b84f322":"code","62283413":"code","1bc53b39":"code","5670c297":"code","2637b4d2":"code","74412d36":"code","af9b83d0":"code","54047db0":"markdown","03658570":"markdown","a62aac66":"markdown","c65295f6":"markdown","78550ecd":"markdown","02e047c8":"markdown","1797b493":"markdown","44bdf9cb":"markdown","ba54e1f5":"markdown","37e6d853":"markdown","4028388d":"markdown","cc973fcf":"markdown","3e4fd093":"markdown","edda55ae":"markdown","1f2fc209":"markdown","e7747015":"markdown"},"source":{"c55e84c1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Flatten, Dense, MaxPool2D, Conv2D, Dropout, BatchNormalization\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.datasets import mnist\nfrom keras.utils.np_utils import to_categorical\n\nsns.set(style='white', context='notebook', palette='deep')\n\n%matplotlib inline","ee4596ec":"df = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\ndf.head()","67555ba6":"df.shape","329edaa9":"df2 = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ndf2.head()","3debc24f":"df2.shape","c1bea2ec":"y_train = df['label']\nX_train = df.drop(labels = ['label'], axis=1)","75440dda":"hist = y_train.value_counts()","09c4d250":"hist.plot.bar()","a4260e95":"df.isnull().sum()","3034b9c6":"#normalization\nX_train = X_train \/ 255.0\ndf2 = df2 \/ 255.0","a3b97e9a":"#reshape\nX_train = X_train.values.reshape(-1, 28,28,1)\ndf2 = df2.values.reshape(-1, 28,28,1)","e66981d6":"#label encoding\ny_train = to_categorical(y_train, num_classes=10)","2c13f1a2":"#split data into training ang testing\nX_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.1, random_state = 2)","2499dc5e":"print(\"X_train shape: \", X_train.shape)\nprint(\"X_test shape: \", X_test.shape)\nprint(\"y_train shape: \", y_train.shape)\nprint(\"y_test shape: \", y_test.shape)","335b2d5a":"nrows = 2\nncols = 2\n\npic_indexes = np.random.randint(X_train.shape[0], size=(nrows*ncols))\n\nf, axs = plt.subplots(nrows, ncols, figsize=(12, 8))\naxs = axs.flatten()\n\npicture_index  = 0\n\nfor picture_index, ax in zip(pic_indexes, axs):\n        ax.imshow(X_train[picture_index].reshape(28, 28), cmap='gray')\n        ax.set_title(f'Label: {y_train[picture_index]}')\n\nplt.show()","5eace9e1":"g = plt.imshow(X_train[0][:,:,0])","858cf16d":"y_train[0]","fdaa090a":"model = Sequential()\n\nmodel.add(Conv2D(filters=32, kernel_size = (5,5), padding = 'Same', activation= 'relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters=32, kernel_size = (5,5), padding = 'Same', activation= 'relu'))\nmodel.add(MaxPool2D(pool_size = (2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters=64, kernel_size = (3,3), padding = 'Same', activation= 'relu'))\nmodel.add(Conv2D(filters=64, kernel_size = (3,3), padding = 'Same', activation= 'relu'))\nmodel.add(MaxPool2D(pool_size = (2,2), strides = (2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = 'softmax'))","9e92016a":"optimizer = RMSprop(lr = 0.001, rho = 0.9, epsilon=1e-08, decay =0.0)","fd43d7e0":"model.summary()","04e352c1":"model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])","a0db566f":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","06a7df28":"epochs = 50 #model1 => 30\nbatch_size = 86","d02c6ffd":"#Data Augmentation\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(X_train)","cdf9de89":"# Fit the model\nhistory = model.fit(datagen.flow(X_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (X_test,y_test),\n                              verbose = 2, steps_per_epoch=X_train.shape[0] \/\/ batch_size\n                              , callbacks=[learning_rate_reduction])","186f6e81":"# Confusion matrix \n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nY_pred = model.predict(X_test)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_test,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","a20507c7":"score = model.evaluate(X_test, y_test, verbose = 0)\nprint(\"Test Accuracy: \",score[1])","c14cd2b5":"def get_newtriplecnn():\n    return Sequential([\n        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape = (28,28,1)),\n        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.25),\n        \n        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same' ),\n        BatchNormalization(),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.25),\n        \n        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same' ),\n        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same' ),\n        BatchNormalization(),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.25),\n        \n        \n        Flatten(),\n          \n        Dense(512, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.5),\n        \n        Dense(256, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.4),\n        \n        Dense(64, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.3),\n        \n        Dense(10, activation = \"softmax\")\n        \n    ])","f0841a1c":"model = get_newtriplecnn()\nmodel.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","6d4b0867":"#adding callbacks\ncallbacks1 = [ \n    EarlyStopping(monitor = 'loss', patience = 6), \n    ReduceLROnPlateau(monitor = 'loss', patience = 3), \n    ModelCheckpoint('model.best.hdf5', save_best_only=True) # saving the best model\n]","6c4117ad":"history = model.fit(datagen.flow(X_train,y_train, batch_size=batch_size), epochs = 50, \n                               steps_per_epoch = X_train.shape[0] \/\/ batch_size,\n                               validation_data = (X_test, y_test),\n                               callbacks = callbacks1,\n                             )","58f67da8":"def load_data(path):\n    with np.load(path) as f:\n        x_train, y_train = f['x_train'], f['y_train']\n        x_test, y_test = f['x_test'], f['y_test']\n        return (x_train, y_train), (x_test, y_test)\n\n(x_train1, y_train1), (x_test1, y_test1) = load_data(r'Dataset\/mnist.npz')","3f6a5f41":"x_train1 = x_train1 \/ 255.0\nx_test1 = x_test1 \/ 255.0\n\nx_train1 = x_train1.reshape(-1, 28, 28, 1)\nx_test1 = x_test1.reshape(-1, 28, 28, 1)\n\ny_train1 = y_train1.reshape(y_train1.shape[0], 1)\ny_test1 = y_test1.reshape(y_test1.shape[0], 1)","d38c322f":"print(len(y_train1))\nprint(len(y_test1))","e8c05cb0":"train = pd.read_csv(r\"Dataset\/train.csv\")\n\nX_train = train.drop(labels = [\"label\"], axis = 1)\ny_train = train['label']\n#y_train = to_categorical(y_train)\n\nX_train \/= 255.0\nX_train = X_train.values.reshape(-1, 28, 28, 1)\n\n#y_train = y_train.reshape(y_train.shape[0], 1)","3f6c6268":"y_train = y_train.to_numpy()","5c8f5a0a":"Add_X = np.vstack((x_train1, x_test1))\n\nAdd_y = np.vstack((y_train1, y_test1))","d29b7a64":"len(Add_y)","31c47d63":"len(Add_X)","19db8423":"y_train = y_train.reshape(y_train.shape[0], 1)","d018256f":"len(y_train)","0fed6bf9":"Add_X = np.vstack((Add_X, X_train))\n\nAdd_y = np.vstack((Add_y, y_train))","a9df82d7":"Add_y = to_categorical(Add_y)","32696697":"len(Add_y)","03eace09":"len(Add_X)","ff1f71d1":"#split data into training ang testing\nAdd_X_train, Add_X_test, Add_y_train, Add_y_test = train_test_split(Add_X, Add_y, test_size = 0.1, random_state = 2)","9fe1a3d2":"add_train_aug = ImageDataGenerator(\n        featurewise_center = False,\n        samplewise_center = False,\n        featurewise_std_normalization = False, \n        samplewise_std_normalization = False,\n        zca_whitening = False,\n        horizontal_flip = False,\n        vertical_flip = False,\n        fill_mode = 'nearest',\n        rotation_range = 10,  \n        zoom_range = 0.1, \n        width_shift_range = 0.1, \n        height_shift_range = 0.1)\n        \n\nadd_train_aug.fit(Add_X_train)\nadd_train_gen = add_train_aug.flow(Add_X_train, Add_y_train, batch_size=batch_size)","7bdb7891":"add_callbacks = [ \n    EarlyStopping(monitor = 'loss', patience = 6), \n    ReduceLROnPlateau(monitor = 'loss', patience = 3), \n    ModelCheckpoint('additional_model.best.hdf5', save_best_only=True) # saving the best model\n]","1b84f322":"def get_addcnn():\n    return Sequential([\n        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same', input_shape = (28,28,1)),\n        Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same'),\n        BatchNormalization(),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.25),\n        \n        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same' ),\n        BatchNormalization(),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.25),\n        \n        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same' ),\n        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same' ),\n        BatchNormalization(),\n        MaxPool2D(pool_size=(2, 2)),\n        Dropout(0.25),\n        \n        \n        Flatten(),\n          \n        Dense(512, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.5),\n        \n        Dense(256, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.4),\n        \n        Dense(64, activation='relu'),\n        BatchNormalization(),\n        Dropout(0.3),\n        \n        Dense(10, activation = \"softmax\")\n        \n    ])","62283413":"model = get_addcnn()\nmodel.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])\nmodel.summary()","1bc53b39":"history = model.fit((add_train_gen), epochs = 50, \n                               steps_per_epoch = Add_X_train.shape[0] \/\/ batch_size,\n                               validation_data = (Add_X_test, Add_y_test),\n                               callbacks = add_callbacks,\n                             )","5670c297":"model = load_model('additional_model.best.hdf5')","2637b4d2":"score = model.evaluate(Add_X_test, Add_y_test, verbose = 0)\nprint(\"Test Accuracy: \",score[1])","74412d36":"result = model.predict(df2)\n\nresult = np.argmax(result, axis=1)\n\nresult = pd.Series(result, name='Label')","af9b83d0":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),result],axis = 1)\n\nsubmission.to_csv(\"submission_cnn.csv\",index=False)","54047db0":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Import-Libraries\" data-toc-modified-id=\"Import-Libraries-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Import Libraries<\/a><\/span><\/li><li><span><a href=\"#Load-Data\" data-toc-modified-id=\"Load-Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Load Data<\/a><\/span><\/li><li><span><a href=\"#EDA-and-Processing\" data-toc-modified-id=\"EDA-and-Processing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>EDA and Processing<\/a><\/span><\/li><li><span><a href=\"#Double-CNN\" data-toc-modified-id=\"Double-CNN-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Double CNN<\/a><\/span><\/li><li><span><a href=\"#Triple-CNN\" data-toc-modified-id=\"Triple-CNN-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Triple CNN<\/a><\/span><\/li><li><span><a href=\"#Additional-mnist-data\" data-toc-modified-id=\"Additional-mnist-data-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Additional mnist data<\/a><\/span><\/li><li><span><a href=\"#Submission\" data-toc-modified-id=\"Submission-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Submission<\/a><\/span><\/li><\/ul><\/div>","03658570":"* Hello guys, in this notebook I have specifically used 3 approaches to show Digit Recognition on MNIST dataset.","a62aac66":"# Load Data","c65295f6":"# Additional mnist data","78550ecd":"* My submissions log:<br>\n1) Basic ML algo - 99.021% acc <br>\n2) CNN with Data Augmentation - 99.41% acc<br>\n3) Double Layer CNN - 97.842% acc<br>\n4) Triple Layer CNN with BatchNormalization - 99.621% acc<br>\n5) Triple Layer CNN with BatchNormalization plus additonal data - 99.835% acc<br>","02e047c8":"# Submission","1797b493":"Adding original training data to the additional data","44bdf9cb":"* The three approaches are - Double Layer CN Network, Triple Layer CN Network and Last one being Triple Layer CN Network but with additonal mnist data.","ba54e1f5":"Run the below cells to get the respective model trained on the data.","37e6d853":"# Double CNN","4028388d":"Using additional data to increase the accuracy","cc973fcf":"# Triple CNN","3e4fd093":"# EDA and Processing","edda55ae":"* I hope you learn something from this notebook. ","1f2fc209":"# Import Libraries","e7747015":"* The MNIST database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems. The database is also widely used for training and testing in the field of machine learning. The MNIST database contains 60,000 training images and 10,000 testing images. Half of the training set and half of the test set were taken from NIST's training dataset, while the other half of the training set and the other half of the test set were taken from NIST's testing dataset."}}