{"cell_type":{"d18a0940":"code","1dccfffd":"code","df56404b":"code","380e43d8":"code","c2e77dd3":"code","46f62639":"code","43932051":"code","6ad4bcde":"code","a9fa3dac":"code","3189d1b8":"code","3ddd0c53":"code","12786d9f":"code","1a247455":"code","aee304dc":"code","545b7e08":"code","a37a55b3":"code","dc3e9616":"code","58755bf2":"code","ca02561d":"code","8598b4d3":"code","7448c29e":"code","c8a5bb9b":"code","2172b815":"code","51818a1f":"code","e9210188":"code","8d5c673c":"code","14363f4e":"code","12b252f1":"code","5401d436":"code","a34c66ac":"code","e58c2d5a":"code","a8a6a5e3":"code","381fa731":"code","5d6bb34d":"code","23e2e887":"code","577529ce":"code","a02d6b31":"code","cac9c7b2":"markdown","a4865b27":"markdown","a7bbbee1":"markdown","72a69ba2":"markdown","5e42a9aa":"markdown","6268153a":"markdown","1123dabd":"markdown","327240a7":"markdown","d0f9d512":"markdown","779fb9f1":"markdown","0136dd79":"markdown","1e87d25d":"markdown","5b3dac96":"markdown","6d1b6681":"markdown","5b2c9304":"markdown","0aeffd53":"markdown","d44bee18":"markdown","7eaf1362":"markdown","3eaf2bcd":"markdown","9098de6b":"markdown","ae4ba984":"markdown","11d51a42":"markdown","a023ef99":"markdown","b2dfaf0c":"markdown","3700f8fe":"markdown","bd6ac542":"markdown","59548006":"markdown","5518d351":"markdown","b92abdaa":"markdown","262d72b3":"markdown","f1cc24b5":"markdown","4bc40bbc":"markdown","91c2f496":"markdown"},"source":{"d18a0940":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.set_style('darkgrid')\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1dccfffd":"df = pd.read_csv('\/kaggle\/input\/predicting-a-pulsar-star\/pulsar_stars.csv')\ndf = df.sample(frac=1).reset_index(drop=True) # shuffle data\nprint(df.shape)\ndf.head()","df56404b":"df.describe()","380e43d8":"df.isna().sum()","c2e77dd3":"fig, axes = plt.subplots(ncols=len(df.columns) \/\/ 2, nrows=2, figsize=(30,16))\n\nfor i, col in enumerate(df.columns[:len(df.columns) \/\/ 2]):\n    sns.distplot(df[col], bins=10, rug=True, ax=axes[0][i])\n    \nfor i, col in enumerate(df.columns[len(df.columns) \/\/ 2:-1]):\n    sns.distplot(df[col], bins=10, rug=True, ax=axes[1][i])","46f62639":"sns.countplot(x=\"target_class\", data=df)","43932051":"plt.figure(figsize=(10,10))\nsns.heatmap(df.corr(), annot=True)","6ad4bcde":"fig, axes = plt.subplots(ncols=8, figsize=(30,8))\n\nfor i, col in enumerate(df.columns.drop('target_class')):\n    sns.boxplot(x='target_class', y=col, data=df, ax=axes[i])\n    \nplt.tight_layout()","a9fa3dac":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Lasso\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error\n\n## not doing any features here, just finding features to remove. So no train test split ##\n\n# set the seed to output consistent results\nnp.random.seed(0)\n\nX = df.drop('target_class', axis=1)\ny = df['target_class']","3189d1b8":"from scipy.stats import normaltest\n# H0: normally distributed\n# H1: not normally distributed\n\nk2, p = normaltest(X)\nk2, p","3ddd0c53":"scaler = StandardScaler()\nX_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n\nk2, p = normaltest(X_scaled)\nk2, p","12786d9f":"# another test of normality\n# https:\/\/docs.scipy.org\/doc\/scipy\/reference\/generated\/scipy.stats.shapiro.html\nfrom scipy.stats import shapiro\n\nfor col in X.columns:\n    stat, p = shapiro(X[col])\n    print('%s\\nstatistics=%.3f, p=%.3f' % (col, stat, p))\n    # interpret\n    alpha = 0.05\n    if p > alpha:\n        print('Sample looks Gaussian (fail to reject H0)\\n')\n    else:\n        print('Sample does not look Gaussian (reject H0)\\n')","1a247455":"from statsmodels.graphics.gofplots import qqplot\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import FunctionTransformer\n\nX_scaled = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)\nX_scaled_mm = pd.DataFrame(MinMaxScaler().fit_transform(X), columns=X.columns)\nX_scaled_rs = pd.DataFrame(RobustScaler().fit_transform(X), columns=X.columns)\nX_scaled_qt = pd.DataFrame(QuantileTransformer(n_quantiles=5, random_state=0).fit_transform(X), columns=X.columns)\nX_scaled_log = pd.DataFrame(FunctionTransformer(np.log1p, validate=True).fit_transform(X), columns=X.columns)\n\nfig, axes = plt.subplots(ncols=len(X.columns), nrows=6, figsize=(30,32))\nfor i, col in enumerate(X.columns):\n    axes[0][i].set_title(col)\n    axes[1][i].set_title('Scaled ' + col)\n    axes[2][i].set_title('MM Scaled ' + col)\n    axes[3][i].set_title('RS Scaled ' + col)\n    axes[4][i].set_title('QT Scaled ' + col)\n    axes[5][i].set_title('Log Scaled ' + col)\n    qqplot(X[col], line='s', ax=axes[0][i])\n    qqplot(X_scaled[col], line='s', ax=axes[1][i])\n    qqplot(X_scaled_mm[col], line='s', ax=axes[2][i])\n    qqplot(X_scaled_rs[col], line='s', ax=axes[3][i])\n    qqplot(X_scaled_qt[col], line='s', ax=axes[4][i])\n    qqplot(X_scaled_log[col], line='s', ax=axes[5][i])","aee304dc":"# all distribution still look non-normal\n# only one that looks possibly normal for a few of the variables is the log-transformation\nfrom scipy.stats import shapiro\n\nfor col in X_scaled_log.columns:\n    stat, p = shapiro(X_scaled_log[col])\n    print('Log Transformation of%s\\nstatistics=%.3f, p=%.3f' % (col, stat, p))\n    # interpret\n    alpha = 0.05\n    if p > alpha:\n        print('Sample looks Gaussian (fail to reject H0)\\n')\n    else:\n        print('Sample does not look Gaussian (reject H0)\\n')","545b7e08":"from scipy.stats import zscore\n\nX_scaled = pd.DataFrame(StandardScaler().fit_transform(X), columns=X.columns)\nprint(X_scaled.shape)\n\nocols = []\nzcols = []\nfor col in X_scaled.columns:\n    ocol = 'outlier ' + col\n    ocols.append(ocol)\n    X_scaled[ocol] = np.where(zscore(X_scaled[col]) <= -2.5, 1,\n                              np.where(zscore(X_scaled[col]) >= 2.5, 1, 0))\n\nX_scaled['outlier'] = X_scaled[ocols].sum(axis=1)\nX_scaled.drop(ocols, axis=1, inplace=True)\nX_scaled = X_scaled[X_scaled['outlier'] == 0]\nX_scaled.drop('outlier', axis=1, inplace=True)\n\nprint(X_scaled.shape)\n\nX_scaled.head()","a37a55b3":"from scipy.stats import shapiro\n\nfor col in X_scaled.columns:\n    stat, p = shapiro(X_scaled[col])\n    print('Log Transformation of%s\\nstatistics=%.3f, p=%.3f' % (col, stat, p))\n    # interpret\n    alpha = 0.05\n    if p > alpha:\n        print('Sample looks Gaussian (fail to reject H0)\\n')\n    else:\n        print('Sample does not look Gaussian (reject H0)\\n')\n        \nfig, axes = plt.subplots(ncols=len(X.columns), figsize=(30,8))\nfor i, col in enumerate(X.columns):\n    axes[i].set_title(col)\n    qqplot(X_scaled[col], line='s', ax=axes[i])","dc3e9616":"scaler = StandardScaler()\n\n# have to icnlude identifier in front of param followed by two underscores\nparams={'lasso__alpha': [1e-6, 1e-4, 1e-2, 1e-1, 1, 10, 100]}\nlasso = Lasso(random_state=0)\n\n# scale the data then apply the lasso regression\npipe = Pipeline(steps=[('scaler', scaler),\n                       ('lasso', lasso)\n                        ])\n\ngs_cv = GridSearchCV(pipe, params, cv=5, scoring='neg_mean_squared_error')\ngs_cv.fit(X, y)\n\nlasso_best = gs_cv.best_estimator_\n\nprint(lasso_best.named_steps['lasso'].coef_)","58755bf2":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n# from sklearn.linear_model import SGDClassifier  ## Stochastic Gradient Descent ##\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score  ## use StratifiedKfold to preserve distribution of samples from each fold ##\n\nscaler = StandardScaler()\n\nparams={'gb': {\n            'gb__learning_rate': [1e-4, 1e-2, 1e-1, 1],\n            'gb__max_depth': [2, 3, 4],\n            'gb__n_estimators': [500], ## Gradient boosting is fairly robust to over-fitting so a large number usually results in better performance ##\n            'gb__random_state': [0],\n            'gb__tol': [1e-3], ## lower tolerance to speed up algo ##\n            },\n        'knn': {\n            'knn__n_neighbors': [2, 4, 6, 8],            \n            },\n        'lr': {\n            'lr__solver': ['lbfgs'],\n            'lr__random_state': [0],\n            },\n       }\n\nmodels = {'knn': KNeighborsClassifier(),\n          'lr': LogisticRegression(),\n          'gb': GradientBoostingClassifier(),\n         }\n\nX = df.drop('target_class', axis=1)\ny = df['target_class']\n\nkfold = StratifiedKFold(n_splits=3, random_state=0)\n\nfor name, model in models.items():\n    # scale the data then apply each model\n    pipe = Pipeline(steps=[('scaler', scaler),\n                           (name, model)\n                            ])\n\n    gs_cv = GridSearchCV(pipe, \n                         params[name], \n                         cv=5, \n                         scoring='accuracy',\n                         n_jobs=-1)\n    \n    results = cross_val_score(gs_cv, X, y, cv=kfold)\n    results *= 100\n    \n    print(\"Results for {}: {:.3f}% ({:.3f}%) [{:.3f}%, {:.3f}%] accuracy\".format(name, \n                                                                                results.mean(),\n                                                                                results.std(),\n                                                                                results.mean() - results.std(),\n                                                                                results.mean() + results.std()\n                                                                                ))","ca02561d":"## Implement Neural Network too ##\n## https:\/\/machinelearningmastery.com\/regression-tutorial-keras-deep-learning-library-python\/ ##\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense as Dense2\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras import regularizers\nfrom keras import callbacks\n\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D\nfrom tensorflow.keras import Model\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport tensorflow as tf\n\nfrom sklearn.model_selection import cross_val_score, KFold, train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import StratifiedKFold, cross_val_score  ## use StratifiedKfold to preserve distribution of samples from each fold ##","8598b4d3":"def baseline_model():\n    ## create NN model ##\n    model = Sequential()\n    ## add first hidden layer - same as # of features##\n    model.add(Dense2(X.shape[1], \n                    input_dim=X.shape[1], \n                    kernel_initializer='normal', \n                    activation='relu'))\n    ## single output layer ##\n    model.add(Dense2(1, kernel_initializer='normal'))\n    ## compile model - adam is fastest optimizer ##\n    model.compile(loss='mean_squared_error', optimizer='adam', metrics=[\"accuracy\"])\n    return model\n\ndef larger_model():\n    model = Sequential()\n    ## 1st hidden layer ##\n    model.add(Dense2(X.shape[1], \n                    input_dim=X.shape[1], \n                    kernel_initializer='normal', \n                    activation='relu'))\n    ## 2nd hidden layer ##\n    model.add(Dense2(max(X.shape[1]-3, 2), \n                    kernel_initializer='normal', \n                    activation='relu'))\n    ## output layer ##\n    model.add(Dense2(1, kernel_initializer='normal'))\n    ## Compile model ##\n    model.compile(loss='mean_squared_error', optimizer='adam', metrics=[\"accuracy\"])\n    return model","7448c29e":"num_rows = 1000 # don't use full dataset as it takes too long to train\n\n## baseline model ##\nmodel = KerasClassifier(build_fn=baseline_model, \n                        epochs=50, \n                        batch_size=5, \n                        verbose=0)\n\n## NN need input values b\/w [-1, +1] ##\n## the sigmoid activation function is rather flat for values who's modulus is large so we often normalise inputs to say between [-1, +1] ##\n## Should use `sklearn.preprocessing.MinMaxScaler` instead of `StandardScaler`, however, `StandardScaler` gave better results ##\n# scaler = MinMaxScaler()\nscaler = StandardScaler()\n\npipe = Pipeline(steps=[('scaler', scaler),\n                       ('model', model)\n                      ])\n\nX = df.drop('target_class', axis=1)\ny = df['target_class']\n\nkfold = StratifiedKFold(n_splits=5, random_state=0)\nresults = cross_val_score(pipe, X[:num_rows], y[:num_rows], scoring='accuracy', cv=kfold)\nresults *= 100\n\nprint(\"Results for {}: {:.3f}% ({:.3f}%) [{:.3f}%, {:.3f}%] accuracy\".format('`KerasClassifier` baseline model', \n                                                                            results.mean(),\n                                                                            results.std(),\n                                                                            results.mean() - results.std(),\n                                                                            results.mean() + results.std()\n                                                                            ))","c8a5bb9b":"num_rows = 1000 # don't use full dataset as it takes too long to train\n\n## larger model ##\nmodel = KerasClassifier(build_fn=larger_model, \n                        epochs=50, \n                        batch_size=5, \n                        verbose=0)\n\n## NN need input values b\/w [-1, +1] ##\n## the sigmoid activation function is rather flat for values who's modulus is large so we often normalise inputs to say between [-1, +1] ##\n## Should use `sklearn.preprocessing.MinMaxScaler` instead of `StandardScaler`, however, `StandardScaler` gave better results ##\n# scaler = MinMaxScaler()\nscaler = StandardScaler()\n\npipe = Pipeline(steps=[('scaler', scaler),\n                       ('model', model)\n                      ])\n\nX = df.drop('target_class', axis=1)\ny = df['target_class']\n\nkfold = StratifiedKFold(n_splits=5, random_state=0)\nresults = cross_val_score(pipe, X[:num_rows], y[:num_rows], scoring='accuracy', cv=kfold)\nresults *= 100\n\nprint(\"Results for {}: {:.3f}% ({:.3f}%) [{:.3f}%, {:.3f}%] accuracy\".format('`KerasClassifier` baseline model', \n                                                                            results.mean(),\n                                                                            results.std(),\n                                                                            results.mean() - results.std(),\n                                                                            results.mean() + results.std()\n                                                                            ))","2172b815":"X = df.drop('target_class', axis=1)\ny = df['target_class']\n\nX_scaled = scaler.fit_transform(X)","51818a1f":"def lrelu_01(x): \n    return tf.nn.leaky_relu(x, alpha=0.01)\n\ndef build_nn_model():\n    '''\n    activation function rankings: elu > leaky relu > relu > tanh > sigmoid\n    regularization techniques (avoid overfitting): l1-, l2-norm\n                                                   Dropout: at every training step, every neuron has a prob p of being temporarily \u201cdropped out\u201d\n                                                            prevents overreliance on small subset of neurons (prevent overfitting)\n                                                            (Most popular, more than l1, l2)\n    BatchNormalization: address the vanishing\/exploding gradients problems. \n                        simply zero-centering and normalizing the inputs, \n                        then scaling and shifting the result using two newparameters per layer (one for scaling, the other for shifting)\n    '''\n    model = keras.Sequential([\n    #         layers.Dense(64, activation=lrelu_01, input_shape=[len(X.keys())]),\n    #         layers.Dense(64, activation=lrelu_01),\n            layers.Dense(64, \n                         activation='elu', \n                         input_shape=[len(X.keys())],),\n    #                      kernel_regularizer=regularizers.l2(0.01),\n    #                      activity_regularizer=regularizers.l1(0.01)),\n            layers.BatchNormalization(),\n            layers.Dropout(0.2),\n            layers.Dense(64, activation='elu'),\n            layers.BatchNormalization(),\n            layers.Dropout(0.2),\n            layers.Dense(1)\n    ])\n\n    optimizer = tf.keras.optimizers.RMSprop(0.001)\n\n    model.compile(loss='mse',\n                optimizer=optimizer,\n                metrics=['mae', 'mse', 'accuracy'])\n    return model","e9210188":"model = build_nn_model()\nmodel.summary()","8d5c673c":"EPOCHS = 10\n\n'''\nTo avoid overfitting the training set, interrupt training when its performance on the validation set starts dropping.\nPatience - number of epochs that produced the monitored quantity with no improvement after which training will be stopped.\nReference: https:\/\/keras.io\/callbacks\/\n'''\nes = callbacks.EarlyStopping(monitor='acc', min_delta=0.001, patience=5,\n                             verbose=1, mode='max', baseline=None, restore_best_weights=True)\n\n''' \nLearning Rate -  set too high, training diverges \n              -  set too low, training eventually converges to the optimum, but takes long time\n              -  solution: start with high LR, then reduce\nReference: https:\/\/keras.io\/callbacks\/\n'''\nrlr = callbacks.ReduceLROnPlateau(monitor='acc', factor=0.5,\n                                  patience=3, min_lr=1e-4, mode='max', verbose=1)\n\nhistory = model.fit(X_scaled, \n                    y.values,\n                    epochs=EPOCHS, \n                    callbacks=[es],\n                    validation_split = 0.2,\n                    verbose=1)","14363f4e":"hist = pd.DataFrame(history.history)\nhist['epoch'] = history.epoch\nhist.tail()","12b252f1":"hist[['acc', 'val_acc']].plot(title='NN Acuuracy Score')","5401d436":"from sklearn.decomposition import PCA\nfrom sklearn.cross_decomposition import PLSRegression\n\n## must normalize data before PCA otherwise first factor will appear to explain majority of variance ##\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\npca = PCA(n_components=X_scaled.shape[1], random_state=0)\npca.fit(X_scaled)\nvar = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=3)*100)\nvar = np.append([0], var)\nvar","a34c66ac":"plt.ylabel('% Variance Explained')\nplt.xlabel('# of Features')\nplt.title('PCA Analysis')\nplt.ylim(0, 110)\nplt.xticks(np.arange(1, X.shape[1]+2, 1))\nplt.style.context('seaborn-whitegrid')\n\n\nplt.plot(var)","e58c2d5a":"scaler = StandardScaler()\nmodel = PLSRegression(n_components=4)\n\npipe = Pipeline(steps=[('scaler', scaler),\n                       ('model', model)])\n\nkfold = StratifiedKFold(n_splits=5, random_state=0)\nresults = cross_val_score(pipe, X, y, cv=kfold)\nresults *= 100\n\nprint(\"Results for {}: {:.3f}% ({:.3f}%) [{:.3f}%, {:.3f}%] accuracy\".format('PLSRegression',\n                                                                            results.mean(),\n                                                                            results.std(),\n                                                                            results.mean() - results.std(),\n                                                                            results.mean() + results.std()\n                                                                            ))","a8a6a5e3":"for i in range(1, X.shape[1]):\n    scaler = StandardScaler()\n    model = PLSRegression(n_components=i)\n\n    pipe = Pipeline(steps=[('scaler', scaler),\n                           ('model', model)])\n\n    kfold = StratifiedKFold(n_splits=5, random_state=0)\n    results = cross_val_score(pipe, X, y, cv=kfold)\n    results *= 100\n    \n    print(\"Results for {}: {:.3f}% ({:.3f}%) [{:.3f}%, {:.3f}%] accuracy\".format(i,\n                                                                                results.mean(),\n                                                                                results.std(),\n                                                                                results.mean() - results.std(),\n                                                                                results.mean() + results.std()\n                                                                                ))","381fa731":"from sklearn.ensemble import VotingClassifier\n\n# voting = 'soft' --> If \u2018hard\u2019, uses predicted class labels for majority rule voting. \n# Else if \u2018soft\u2019, predicts the class label based on the argmax of the sums of the predicted probabilities\n# This gave the best results, so wrap in a function to use in the resampling methods later\ndef voting_classifier(X, y):\n    v_clf = VotingClassifier(estimators=[\n                ('knn', KNeighborsClassifier()),\n                ('lr', LogisticRegression()),\n                ('gb', GradientBoostingClassifier()),\n                ], voting='soft')\n\n    params={\n            'model__gb__learning_rate': [1e-4, 1e-2, 1e-1, 1],\n            'model__gb__n_estimators': [10, 100, 1000],\n            'model__gb__random_state': [0],\n            'model__knn__n_neighbors': [2, 4, 6, 8],            \n            'model__lr__solver': ['lbfgs'],\n            'model__lr__random_state': [0],\n           }\n\n    pipe = Pipeline(steps=[('scaler', scaler),\n                           ('model', v_clf)\n                            ])\n\n    gs_cv = GridSearchCV(pipe, \n                         params, \n                         cv=5, \n                         scoring='accuracy',\n                         n_jobs=-1,\n                         iid=True)\n\n    kfold = StratifiedKFold(n_splits=3, random_state=0)\n    results = cross_val_score(gs_cv, X, y, cv=kfold, scoring='accuracy', n_jobs=-1)\n    results *= 100\n    \n    print(\"Results for {}: {:.3f}% ({:.3f}%) [{:.3f}%, {:.3f}%] accuracy\".format('VotingClassifier',\n                                                                                results.mean(),\n                                                                                results.std(),\n                                                                                results.mean() - results.std(),\n                                                                                results.mean() + results.std()\n                                                                                ))","5d6bb34d":"X = df.drop('target_class', axis=1)\ny = df['target_class']\nvoting_classifier(X,y)","23e2e887":"## Running the voting classifier with `gradient boost` takes too long, so leave it out for the resampling methods. ##\ndef voting_classifier_small(X, y):\n    v_clf = VotingClassifier(estimators=[\n                ('knn', KNeighborsClassifier()),\n                ('lr', LogisticRegression()),\n                ], voting='soft')\n\n    params={\n            'model__knn__n_neighbors': [2, 4, 6, 8],            \n            'model__lr__solver': ['lbfgs'],\n            'model__lr__random_state': [0],\n           }\n\n    pipe = Pipeline(steps=[('scaler', scaler),\n                           ('model', v_clf)\n                            ])\n\n    gs_cv = GridSearchCV(pipe, \n                         params, \n                         cv=5, \n                         scoring='accuracy',\n                         n_jobs=-1,\n                         iid=True)\n\n    kfold = StratifiedKFold(n_splits=3, random_state=0)\n    results = cross_val_score(gs_cv, X, y, cv=kfold, scoring='accuracy', n_jobs=-1)\n    results *= 100\n    \n    print(\"Results for {}: {:.3f}% ({:.3f}%) [{:.3f}%, {:.3f}%] accuracy\".format('VotingClassifier',\n                                                                                results.mean(),\n                                                                                results.std(),\n                                                                                results.mean() - results.std(),\n                                                                                results.mean() + results.std()\n                                                                                ))","577529ce":"from imblearn.over_sampling import RandomOverSampler\n\nX = df.drop('target_class', axis=1)\ny = df['target_class']\n\nros = RandomOverSampler(random_state=0)\nX_resampled, y_resampled = ros.fit_resample(X, y)\n\nvoting_classifier_small(X_resampled,y_resampled)","a02d6b31":"from imblearn.over_sampling import SMOTE\n\nX = df.drop('target_class', axis=1)\ny = df['target_class']\n\nsmote = SMOTE(random_state=0)\nX_resampled, y_resampled = smote.fit_resample(X, y)\n\nvoting_classifier_small(X_resampled,y_resampled)","cac9c7b2":"### Method 3. VotingClassifier\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier\n\nIn majority voting, the predicted class label for a particular sample is the class label that represents the majority (mode) of the class labels predicted by each individual classifier.","a4865b27":"## Data Visualization","a7bbbee1":"Lot of variation here. The box plot lines represent the min, first quartile, median, third quartile, and max. The `whiskers` above the min and max are present when \"the dataset is too large and the range of value is too big then it shows some values as outliers based on an inter-quartile function.\"\n\nEvery features has whiskers for `target_class` = 0 and 5\/8 features have whiskers for `target_class` = 1. So we're going to have to either standardize the features or remove outliers. Let's opt for standardizing.","72a69ba2":"## Data Processing","5e42a9aa":"## Classification Methods (No Random Sampling)","6268153a":"## Data Stats","1123dabd":"Even with the data scaled, all p-values are very low, meaning we reject the null assumption of normality for any of the distributions.","327240a7":"#### Normality Testing","d0f9d512":"The qqplot further reinforces the non-normality of the data. If the data was normally distributed, the blue dots would fall along the red line.\n\nFor all five transformations of the data, it's still not normally distributed. From the measures I've tried I can conclude there isn't a scaling possible to transform the data to normally distributed.","779fb9f1":"With just four features 94% of the variance in X is explained. To hepl prevent overfitting, I'll use four features for `PLSCanonical`.","0136dd79":"`Skewness of the integrated profile`, `Excess kurtosis of the integrated profile`, and `Mean of the integrated profile` are the strongest predictors of the `target_class`. \n\nHowever, `Skewness of the integrated profile` and `Excess kurtosis of the integrated profile` have a strong correlation of 0.95. I'll try two approaches later:\n1. Remove one of the correlated variables.\n2. Use PCA with both variables included.\n\nLet's do some boxplots of the data to see how many outliers there are, as well as use StandardScaler to scale the features and Lasso to remove unnecessary ones.","1e87d25d":"No feature coefficient of the Lasso regression equal 0, so can't exclude any features based on this. Since we can't exclude features manually, we'll try three different methods:\n\n1. Include all variables regardless of correlation.\n2. Use PCA and to determine optimal number of new features to keep. The run PLS regression to predict.\n3. Run sklearn.ensemble.VotingClassifier to combine the predictions of the classifiers used in method 1.\n\nAs an added approach to handle the uneven distribution of the `target_class` variable we're trying to predict, we'll use two methods (https:\/\/imbalanced-learn.readthedocs.io\/en\/stable\/over_sampling.html):\n1. `imblearn.over_sampling.RandomOverSampler`\n2. `imblearn.over_sampling.SMOTE`\n\n\n\nFor all three methods we'll user K-folds cross-validation due to the unequal binary distribution of the `target_class` output. We'll see which one performs best later.","5b3dac96":"### NN Classifier (scikit-learn wrapper)","6d1b6681":"The output `target_class` is severely imbalanced. To offset this, I'll do two things separately.\n\n1. `sklearn.model_selection.StratifiedKFold` to get an average score instead of simply splitting into train and test sets.\n2. `imblearn.over_sampling.RandomOverSampler` to even out the distribution.","5b2c9304":"### Method 1. `imblearn.over_sampling.RandomOverSampler` ##","0aeffd53":"Better than KNN and LR, but not GB or either NN model. The accuracy was already so high it's hard to see a noticeable improvement, but there's an improvement none-the-less. ","d44bee18":"### NN (no wrapper)","7eaf1362":"#### Feature Selection","3eaf2bcd":"Still can't get normality. I'll have to proceed without. \n\nI'll still standard scale the data to prevent a subset of the features controlling the variation.","9098de6b":"## Handling Uneven Distribution of Target Class ##","ae4ba984":"There's improvement as new features are added, but still doesn't reach the simple classifiers from method #1. Going all the way to seven features is likely to cause overfitting as well.","11d51a42":"### Method 1. Include all variables (regardless of correlation)","a023ef99":"Neither neural network models perform better than any of the simpler classification models. In addition, they each have larger standard deviations.\n\nWe do see an improvment between the two NN models by adding an additional hidden layer.","b2dfaf0c":"Neural networks and PLS didn't perform as well as simpler classifiers. Using `KNN`, `LogisticRegression`, and `GradientBoostingClassifier` along with `GridSearchCV`, higher results were obtained. The best results without random sampling were obtained from `GradientBoostingClassifier` with the `VotingClassifier` as a close second (aggregated the predictions from `KNN`, `LogisticRegression`, and `GradientBoostingClassifier`).\n\nHowever, by applying `RandomOverSampler` using the `VotingClassifier` (without `GradientBoostingClassifier`) it performed better than using the `GradientBoostingClassifier`. It's not entirely unexpected given the large imbalance in the `target_class` variable that is being predicted. In addition, the standard deviation was so low the results will consistently outperform the other classifiers. In comparison, the `KerasClassifers` models both had worse results than the three classifiers without random sampling, and relatively high standard deviations.\n\nAll accuracy scores (disregarding PLS) obtained were greater than 97% though, so it's hard to say any classifier did bad.","3700f8fe":"Around the same performance as the base NN model used. Doesn't beat out the simpler classification models from method #1.","bd6ac542":"Appears to be overfitting the data at the higher epochs. The accuracy scores for the training and validation set are already above 98% after just one epoch. Probably wouldn't go beyond 5 epochs to fully train the model.\n\nThe NN without using the `scikit-learn` wrapper performed much faster as well. With the wrapper, I had to run the code on a fraction of the dataset given time constraints, but I am able to make 5 full passes of the data without the wrapper in less than a minute.","59548006":"So 2,246 rows were removed as outliers in at least one of the columns. Now I'll do my normality tests again.","5518d351":"Using `RandomOverSampler` with the `VotingClassifier` (only `KNN` and `LogisticRegression` and not `GradientBoostingClassifier` like previously) the accuracy improves from 98.01% to 98.75% with a lower standard deviation too. \n\nInterestingly, the `SMOTE` sampler performs worse. \n\nIf we allow the program to run to completion with the `RandomOverSampler` and with the `VotingClassifier` including `GradientBoostingClassifier` I assume the results would be even better.","b92abdaa":"As a final approach at normality, I'll remove outliers from the scaled data, using `StandardScaler` and Z-score as a criteria. ","262d72b3":"### Method 2. PLS (PCA w\/ output variable)","f1cc24b5":"All the models perform very well with a high accuracy of predictions evaluated with cross validation using StratefiedKFolds. \n\n`GradientBoostingClassifier` is the best classifier but has a a relatively large standard deviation\n\nNow let's try applying a neural network classifier.","4bc40bbc":"## Conclusion","91c2f496":"### Method 2. `imblearn.over_sampling.SMOTE` ##"}}