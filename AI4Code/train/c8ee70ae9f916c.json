{"cell_type":{"02e98090":"code","bfb68f54":"code","e7385761":"code","0705cad5":"code","058e1b91":"code","9cf5d3d2":"code","8feb4579":"code","e9b54ebd":"code","85655024":"code","f52f5764":"code","5709b017":"code","6068dc42":"code","64c7fae5":"code","8b00eaeb":"code","11e91b4a":"code","c1f088f8":"code","79fc75e0":"code","4521d57c":"code","a3be3cab":"code","a4d3dc79":"code","f75ea71c":"code","ac40f5b0":"code","3a426e38":"code","674e8023":"code","49ec6765":"code","28729bd5":"code","833ed9df":"code","039d6b6d":"code","4e32b13f":"code","12d797df":"code","09a80954":"code","e5f67c9d":"code","0932b702":"code","51ccba80":"code","bd4771a0":"code","47ab243a":"code","3a7ff391":"code","10db1ebb":"code","cce4b194":"code","4abd058c":"code","ef02c8f4":"code","caa5a330":"code","c6b240b5":"markdown","4d0fbe75":"markdown","894bcad4":"markdown","5447f474":"markdown","c8171344":"markdown","7a660d5d":"markdown"},"source":{"02e98090":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib.pylab import rcParams\n","bfb68f54":"#Read the data\ndf_train = pd.read_csv(\"..\/input\/iowa-house-prices\/train.csv\", index_col = 'Id')\ndf_test = pd.read_csv(\"..\/input\/iowa-house-prices\/test.csv\", index_col ='Id')","e7385761":"df_train.head()","0705cad5":"df_train.isna().sum()","058e1b91":"df_train.dropna(axis = 0, subset = ['SalePrice'], inplace = True)","9cf5d3d2":"y = df_train['SalePrice']","8feb4579":"y ","e9b54ebd":"df_train.drop(axis = 1, labels = ['SalePrice'], inplace = True)","85655024":"# Shape\nprint(df_train.shape)\nprint(df_test.shape)","f52f5764":"df_train.columns","5709b017":"#dropping columns which are not required for prediction of new house\ncol_drop = ['MoSold', 'YrSold', 'SaleType', 'SaleCondition']\ndf_train.drop(labels = col_drop, axis = 1, inplace = True)\n","6068dc42":"df_test.drop(labels = col_drop,axis=1,inplace=True)","64c7fae5":"print(df_train.shape)\nprint(df_test.shape)","8b00eaeb":"num_col = [col for col in df_train.columns if df_train[col].dtype in ['int64','float64']]","11e91b4a":"num_col","c1f088f8":"cols_nulls = df_train[num_col].isnull().sum()","79fc75e0":"cols_nulls[cols_nulls >0]","4521d57c":"#we can drop LotFrontage is have many null values\ndf = df_train[['LotFrontage', 'MasVnrArea', 'GarageYrBlt']].dropna(axis=0)\n","a3be3cab":"plt.figure(figsize=(15,5))\nsns.distplot(df['LotFrontage'],bins=30,kde=True)","a4d3dc79":"\nplt.figure(figsize = (12,4))\nsns.distplot(a = df['MasVnrArea'], bins = 30, norm_hist=False, kde=True, color = 'purple')","f75ea71c":"\nplt.figure(figsize = (12,4))\nsns.distplot(a = df['GarageYrBlt'], bins = 30, norm_hist=False, kde=True, color = 'orange')","ac40f5b0":"from sklearn.impute import SimpleImputer\n\nnumerical_cols_median = ['LotFrontage']\nnumerical_transformer_median = SimpleImputer(strategy = 'median')\n\nnumerical_cols_mod = ['MasVnrArea']\nnumerical_transformer_mod = SimpleImputer(strategy = 'most_frequent')\n\nnumerical_cols_mean = ['GarageYrBlt']\nnumerical_transformer_mean = SimpleImputer(strategy = 'mean')\n\nnumerical_cols_remain = set(num_col) - set(numerical_cols_mean) - set(numerical_cols_median) - set(numerical_cols_mod)\nnumerical_cols_remain = list(numerical_cols_remain)","3a426e38":"cat_data = [col for col in df_train.columns if df_train[col].dtype == 'object']","674e8023":"cat_data ","49ec6765":"cat_null = df_train[cat_data].isnull().sum()\n","28729bd5":"cat_null[cat_null > 0]","833ed9df":"#Because there are a few columns with too many missing values, we'll filter them out from the data\ndf_train.drop(labels = ['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis = 1, inplace = True)","039d6b6d":"#Because there are a few columns with too many missing values, we'll filter them out from the data\ndf_test.drop(labels = ['Alley', 'FireplaceQu', 'PoolQC', 'Fence', 'MiscFeature'], axis = 1, inplace = True)","4e32b13f":"#Redo categorical_cols variable\ncategorical_cols = [col for col in df_train.columns if df_train[col].dtype == 'object']","12d797df":"#performing imputer and OHE to encode the features\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline","09a80954":"cat_tranform = Pipeline(steps=[('imputer',SimpleImputer(strategy='most_frequent')),\n                              ('onehot',OneHotEncoder(handle_unknown='ignore'))])","e5f67c9d":"from sklearn.compose import ColumnTransformer","0932b702":"preprocessor = ColumnTransformer(transformers=[('num_median', numerical_transformer_median, numerical_cols_median),\n                                               ('num_mod', numerical_transformer_mod, numerical_cols_mod),\n                                               ('num_mean', numerical_transformer_mean, numerical_cols_mean),\n                                               ('num_rest', numerical_transformer_mean, numerical_cols_remain),\n                                              ('cat', cat_tranform, categorical_cols)])","51ccba80":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom sklearn import model_selection, metrics\nfrom sklearn.model_selection import GridSearchCV","bd4771a0":"X_train, X_valid, y_train, y_valid = train_test_split(df_train, y, test_size = 0.3, random_state = 0)\n","47ab243a":"def modelfit(model):\n    pipeline = Pipeline(steps=[('preprocessing',preprocessor),\n                              ('model',model)])\n    pipeline.fit(X_train,y_train)\n    pred = pipeline.predict(X_valid)\n    mae = mean_absolute_error(y_valid,pred)\n    print('MAE',mae)","3a7ff391":"xgb1 = XGBRegressor( learning_rate =0.1, n_estimators=1000, max_depth=5, min_child_weight=1, gamma=0, subsample=0.8,\n                     colsample_bytree=0.8, nthread=4, scale_pos_weight=1, seed=27, objective='reg:squarederror')\nmodelfit(xgb1)","10db1ebb":"from sklearn.model_selection import RandomizedSearchCV\n","cce4b194":"# param_1= {'max_depth' : range(3, 10, 2),\n#           'min_child_weight' : range(1, 6, 2),\n#           'reg_alpha' : [1e-5, 1e-2, 0.1, 1, 100],\n#           'gamma' : [i\/10.0 for i in range(0,5)],\n#           'subsample' : [i\/10.0 for i in range(6,10)],\n#           'colsample_bytree' : [i\/10.0 for i in range(6,10)]}\n#Use parameters and apply tunning\nxgb_final = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.8, gamma=0.0,\n             importance_type='gain', learning_rate=0.005, max_delta_step=0,\n             max_depth=4, min_child_weight=1, missing=None, n_estimators=10000,\n             n_jobs=1, nthread=4, objective='reg:squarederror', random_state=0,\n             reg_alpha=1, reg_lambda=1, scale_pos_weight=1, seed=27,\n             silent=None, subsample=0.8, verbosity=1)\nmodelfit(xgb_final)\n","4abd058c":"#my model with all tunings\nmy_model = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.8, gamma=0.0,\n             importance_type='gain', learning_rate=0.005, max_delta_step=0,\n             max_depth=4, min_child_weight=1, missing=None, n_estimators=5000,\n             n_jobs=1, nthread=4, objective='reg:squarederror', random_state=0,\n             reg_alpha=1, reg_lambda=1, scale_pos_weight=1, seed=27,\n             silent=None, subsample=0.8, verbosity=1)\npipeline_final = Pipeline(steps=[('preprocessor', preprocessor),\n                                ('model', my_model)])\n\npipeline_final.fit(df_train, y)\n\npreds_test = pipeline_final.predict(df_test)\n","ef02c8f4":"output = pd.DataFrame({'Id': df_test.index, 'SalePrice' : preds_test})","caa5a330":"output","c6b240b5":"# Intro:\nThis kernel will get anyone started that already finished the ML Course Package on Kaggle, but doesn't know what to do next.","4d0fbe75":"## Categorical Data","894bcad4":"# Tunning Model","5447f474":"# Numerical Data","c8171344":"# Training\n","7a660d5d":"## PipeLine and One Hot Encoding "}}