{"cell_type":{"2a16c580":"code","46ed38ef":"code","ef665d8d":"code","019e0eff":"code","dc268851":"code","314c646a":"code","036abece":"code","92ba570e":"code","ffaa2900":"code","e67b856f":"code","fbb7d483":"code","6a601f18":"code","e1e5b6bf":"code","86d4a437":"code","c1083741":"code","cf355839":"code","0bbf1686":"code","82ea6622":"code","8589d58a":"code","6d1f4fe3":"code","c442a5de":"code","ba8e9406":"code","d454b55a":"code","85857305":"code","43c8e031":"code","7ee7d73f":"code","c6a4d258":"code","dd79909c":"code","e993e521":"code","0bf9c63f":"code","1bf8c11e":"code","9fb684be":"code","1345a169":"code","ed183c8b":"code","e5c3e0cf":"code","6936bb18":"code","1a6c3057":"code","cb43ee5e":"code","02f06cfd":"code","312e1334":"code","979a0b32":"code","94ab4ee5":"code","bfbc2f40":"code","a396596a":"code","03565b51":"code","54f40e6e":"code","d7705fa6":"code","79087ed4":"code","392ebac3":"code","61155eed":"code","5965ce65":"code","445f7f07":"code","fb8fead2":"code","c9aeb2bb":"code","17eafabd":"code","8b28904f":"markdown","3d9c2902":"markdown","db21114e":"markdown","aa4f5630":"markdown","25c3f57b":"markdown"},"source":{"2a16c580":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","46ed38ef":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom tqdm import tqdm","ef665d8d":"PATH=\"\/kaggle\/input\/\"","019e0eff":"fileList = []\nimport glob\nprint(fileList.append(glob.glob(PATH+\"\/*\/*.csv\")))\nfileList","dc268851":"def createDataFrame(path):\n    df = pd.read_csv(path)\n    return df","314c646a":"true_df = createDataFrame(fileList[0][0])\nfalse_df = createDataFrame(fileList[0][1])","036abece":"true_df.head(3)","92ba570e":"false_df.head(3)","ffaa2900":"true_df['target'] = 0","e67b856f":"true_df.head(3)","fbb7d483":"false_df['target'] = 1","6a601f18":"print(true_df.shape)\nprint(false_df.shape)","e1e5b6bf":"df = pd.concat([true_df,false_df],axis=0)","86d4a437":"df.shape","c1083741":"axis=sns.countplot(x=\"target\",palette='tab10',data=df)\nfor patch in axis.patches:\n    axis.text(x = patch.get_x() + patch.get_width()\/2, y = patch.get_height()\/2,\n    s = f\"{np.round(patch.get_height()\/len(df)*100, 1)}%\",\n    ha = 'center', size = 20, rotation = 0, weight = 'bold',color = 'black')\nplt.xlabel(\"Target Distribution\", size=14)\nplt.ylabel('Count', size=14)\nplt.xticks(rotation=90)\nplt.show()","cf355839":"fig, (ax1,ax2)=plt.subplots(1,2, figsize=(10,5))\nnews_len = df[df['target']==1]['text'].str.len()\nax1.hist(news_len,color='red')\nax1.set_title(\"Fake News\")\nnews_len = df[df['target']==0]['text'].str.len()\nax2.hist(news_len,color='green')\nax2.set_title(\"Real News\")\nfig.suptitle(\"Charcter in News\")\nplt.show()","0bbf1686":"fig, (ax1,ax2) = plt.subplots(1, 2, figsize=(10,5))\nnews_length = df[df['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(news_length, color='red')\nax1.set_title(\"Fale News\")\nnews_length = df[df['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(news_length, color='green')\nax2.set_title(\"Real News\")\nfig.suptitle(\"Words in a News\")\nplt.show()","82ea6622":"fig,(ax1,ax2)=plt.subplots(1,2, figsize=(10,5)) \nword_length = df[df['target']==1]['text'].str.split().apply(lambda x: [len(i) for i in x])\nsns.distplot(word_length.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('Fake News Word Length')\nword_length = df[df['target']==0]['text'].str.split().apply(lambda x: [len(i) for i in x])\nsns.distplot(word_length.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Real News Word Length')\nfig.suptitle(\"Average Words Length in News\")\nplt.show()","8589d58a":"def create_corpus(target):\n    corpus = []\n    \n    for x in df[df['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n        return corpus","6d1f4fe3":"corpus = create_corpus(0)\n\ndic = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1], reverse=True)[:10]","c442a5de":"x,y=zip(*top)\nplt.bar(x,y)","ba8e9406":"corpus = create_corpus(1)\n\ndic = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \ntop=sorted(dic.items(), key=lambda x:x[1], reverse=True)[:10]","d454b55a":"x,y=zip(*top)\nplt.bar(x,y)","85857305":"plt.figure(figsize=(10,5))\ncorpus = create_corpus(1)\n\ndic = defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y)","43c8e031":"plt.figure(figsize=(10,5))\ncorpus = create_corpus(0)\n\ndic = defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color='green')","7ee7d73f":"corpus = create_corpus(1)\ncounter = Counter(corpus)\nmost = counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop):\n        x.append(word)\n        y.append(count)","c6a4d258":"sns.barplot(x=y,y=x)","dd79909c":"corpus = create_corpus(0)\ncounter = Counter(corpus)\nmost = counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:40]:\n    if (word not in stop):\n        x.append(word)\n        y.append(count)","e993e521":"sns.barplot(x=y,y=x)","0bf9c63f":"def get_top_news_biagram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2,2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key=lambda x:x[1], reverse=True)\n    return words_freq[:n]","1bf8c11e":"plt.figure(figsize=(10,5))\ntop_news_biagrams = get_top_news_biagram(df['text'])[:10]\nx,y = map(list,zip(*top_news_biagrams))\nsns.barplot(x=y,y=x)","9fb684be":"df.head(2)","1345a169":"df['title'][0]","ed183c8b":"df['title'] = df['title'].apply(lambda x: x.lower())\ndf['text'] = df['text'].apply(lambda x: x.lower())","e5c3e0cf":"contractions = { \n\"ain\u2019t\": \"am not \/ are not \/ is not \/ has not \/ have not\",\n\"aren\u2019t\": \"are not \/ am not\",\n\"can\u2019t\": \"cannot\",\n\"can\u2019t\u2019ve\": \"cannot have\",\n\"\u2019cause\": \"because\",\n\"could\u2019ve\": \"could have\",\n\"couldn\u2019t\": \"could not\",\n\"couldn\u2019t\u2019ve\": \"could not have\",\n\"didn\u2019t\": \"did not\",\n\"doesn\u2019t\": \"does not\",\n\"don\u2019t\": \"do not\",\n\"hadn\u2019t\": \"had not\",\n\"hadn\u2019t\u2019ve\": \"had not have\",\n\"hasn\u2019t\": \"has not\",\n\"haven\u2019t\": \"have not\",\n\"he\u2019d\": \"he had \/ he would\",\n\"he\u2019d\u2019ve\": \"he would have\",\n\"he\u2019ll\": \"he shall \/ he will\",\n\"he\u2019ll\u2019ve\": \"he shall have \/ he will have\",\n\"he\u2019s\": \"he has \/ he is\",\n\"how\u2019d\": \"how did\",\n\"how\u2019d\u2019y\": \"how do you\",\n\"how\u2019ll\": \"how will\",\n\"how\u2019s\": \"how has \/ how is \/ how does\",\n\"I\u2019d\": \"I had \/ I would\",\n\"I\u2019d\u2019ve\": \"I would have\",\n\"I\u2019ll\": \"I shall \/ I will\",\n\"I\u2019ll\u2019ve\": \"I shall have \/ I will have\",\n\"I\u2019m\": \"I am\",\n\"I\u2019ve\": \"I have\",\n\"isn\u2019t\": \"is not\",\n\"it\u2019d\": \"it had \/ it would\",\n\"it\u2019d\u2019ve\": \"it would have\",\n\"it\u2019ll\": \"it shall \/ it will\",\n\"it\u2019ll\u2019ve\": \"it shall have \/ it will have\",\n\"it\u2019s\": \"it has \/ it is\",\n\"let\u2019s\": \"let us\",\n\"ma\u2019am\": \"madam\",\n\"mayn\u2019t\": \"may not\",\n\"might\u2019ve\": \"might have\",\n\"mightn\u2019t\": \"might not\",\n\"mightn\u2019t\u2019ve\": \"might not have\",\n\"must\u2019ve\": \"must have\",\n\"mustn\u2019t\": \"must not\",\n\"mustn\u2019t\u2019ve\": \"must not have\",\n\"needn\u2019t\": \"need not\",\n\"needn\u2019t\u2019ve\": \"need not have\",\n\"o\u2019clock\": \"of the clock\",\n\"oughtn\u2019t\": \"ought not\",\n\"oughtn\u2019t\u2019ve\": \"ought not have\",\n\"shan\u2019t\": \"shall not\",\n\"sha\u2019n\u2019t\": \"shall not\",\n\"shan\u2019t\u2019ve\": \"shall not have\",\n\"she\u2019d\": \"she had \/ she would\",\n\"she\u2019d\u2019ve\": \"she would have\",\n\"she\u2019ll\": \"she shall \/ she will\",\n\"she\u2019ll\u2019ve\": \"she shall have \/ she will have\",\n\"she\u2019s\": \"she has \/ she is\",\n\"should\u2019ve\": \"should have\",\n\"shouldn\u2019t\": \"should not\",\n\"shouldn\u2019t\u2019ve\": \"should not have\",\n\"so\u2019ve\": \"so have\",\n\"so\u2019s\": \"so as \/ so is\",\n\"that\u2019d\": \"that would \/ that had\",\n\"that\u2019d\u2019ve\": \"that would have\",\n\"that\u2019s\": \"that has \/ that is\",\n\"there\u2019d\": \"there had \/ there would\",\n\"there\u2019d\u2019ve\": \"there would have\",\n\"there\u2019s\": \"there has \/ there is\",\n\"they\u2019d\": \"they had \/ they would\",\n\"they\u2019d\u2019ve\": \"they would have\",\n\"they\u2019ll\": \"they shall \/ they will\",\n\"they\u2019ll\u2019ve\": \"they shall have \/ they will have\",\n\"they\u2019re\": \"they are\",\n\"they\u2019ve\": \"they have\",\n\"to\u2019ve\": \"to have\",\n\"wasn\u2019t\": \"was not\",\n\"we\u2019d\": \"we had \/ we would\",\n\"we\u2019d\u2019ve\": \"we would have\",\n\"we\u2019ll\": \"we will\",\n\"we\u2019ll\u2019ve\": \"we will have\",\n\"we\u2019re\": \"we are\",\n\"we\u2019ve\": \"we have\",\n\"weren\u2019t\": \"were not\",\n\"what\u2019ll\": \"what shall \/ what will\",\n\"what\u2019ll\u2019ve\": \"what shall have \/ what will have\",\n\"what\u2019re\": \"what are\",\n\"what\u2019s\": \"what has \/ what is\",\n\"what\u2019ve\": \"what have\",\n\"when\u2019s\": \"when has \/ when is\",\n\"when\u2019ve\": \"when have\",\n\"where\u2019d\": \"where did\",\n\"where\u2019s\": \"where has \/ where is\",\n\"where\u2019ve\": \"where have\",\n\"who\u2019ll\": \"who shall \/ who will\",\n\"who\u2019ll\u2019ve\": \"who shall have \/ who will have\",\n\"who\u2019s\": \"who has \/ who is\",\n\"who\u2019ve\": \"who have\",\n\"why\u2019s\": \"why has \/ why is\",\n\"why\u2019ve\": \"why have\",\n\"will\u2019ve\": \"will have\",\n\"won\u2019t\": \"will not\",\n\"won\u2019t\u2019ve\": \"will not have\",\n\"would\u2019ve\": \"would have\",\n\"wouldn\u2019t\": \"would not\",\n\"wouldn\u2019t\u2019ve\": \"would not have\",\n\"y\u2019all\": \"you all\",\n\"y\u2019all\u2019d\": \"you all would\",\n\"y\u2019all\u2019d\u2019ve\": \"you all would have\",\n\"y\u2019all\u2019re\": \"you all are\",\n\"y\u2019all\u2019ve\": \"you all have\",\n\"you\u2019d\": \"you had \/ you would\",\n\"you\u2019d\u2019ve\": \"you would have\",\n\"you\u2019ll\": \"you shall \/ you will\",\n\"you\u2019ll\u2019ve\": \"you shall have \/ you will have\",\n\"you\u2019re\": \"you are\",\n\"you\u2019ve\": \"you have\",\n}","6936bb18":"def remove_contractions(text):\n    str1 = \"\"\n    for word in text.split():\n        if word in (contractions.keys()):\n            word = word.replace(word,contractions[word])\n        else:\n            pass\n        str1+=word+\" \"\n    return str1","1a6c3057":"df['title'] = df['title'].apply(remove_contractions)\ndf['text'] = df['text'].apply(remove_contractions)","cb43ee5e":"df['text'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))\ndf['title'] = df['title'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop]))","02f06cfd":"df['new_text'] = df['text']+\" \"+df['title']","312e1334":"new_df = df.copy()","979a0b32":"new_df.drop(['title','text','subject','date'],axis=1,inplace=True)","94ab4ee5":"new_df.head(3)","bfbc2f40":"def remove_special_characters(text):\n    result = re.sub('[^a-zA-Z]',' ',text)\n    return result","a396596a":"new_df['new_text'] = new_df['new_text'].apply(remove_special_characters)","03565b51":"from nltk.stem.porter import PorterStemmer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer","54f40e6e":"def do_stemming(text):\n    corpus = []\n    sentense=\"\"\n    ps = PorterStemmer()\n    for word in text.split():\n        word = ps.stem(word)\n        sentense+=word+\" \"\n    #print(sentense)\n    return sentense","d7705fa6":"new_df['cleaned_text'] = new_df['new_text'].apply(do_stemming)","79087ed4":"new_df.head(10)","392ebac3":"cv = CountVectorizer(max_features = 2000)\nX = cv.fit_transform(new_df['cleaned_text']).toarray()\ny = new_df['target']","61155eed":"X_train, X_test, y_train, y_test = train_test_split(X, y ,test_size=0.3,random_state=101,stratify=new_df['target'])","5965ce65":"def display_performance():\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.svm import SVC\n    from sklearn.tree import DecisionTreeClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.ensemble import GradientBoostingClassifier\n    from xgboost import XGBClassifier\n    from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n    from sklearn.metrics import f1_score, recall_score,precision_score,cohen_kappa_score\n    import matplotlib\n    \n    lgc = LogisticRegression()\n    knnc = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p=2)\n    nbc = GaussianNB()\n    svc = SVC(kernel = 'linear')\n    dtc = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n    rfc = RandomForestClassifier(n_estimators=100, criterion = 'entropy')\n    xgb = XGBClassifier()\n    algo = ['Logistic Regression','K Nearest Neibhour','Naive Bayes','Support Vector Machines','Decision trees','Random Forest','Gradient Boosting']\n    performance_measure = ['Recall','Precision','Overall Accuracy','F1 Score','Cohen Kappa Score']\n    models = [lgc,knnc,nbc,svc,dtc,rfc,xgb]\n    accuracy_list = []\n    f1_score_list = []\n    recall_list = []\n    precision_list = []\n    cohen_kappa_list = []\n\n    for i in range(0,7):\n        models[i].fit(X_train,y_train)\n        y_pred = models[i].predict(X_test)\n        recall = recall_score(y_test,y_pred)\n        recall_list.append(recall)\n        precision = precision_score(y_test,y_pred)\n        precision_list.append(precision)\n        accuracy = accuracy_score(y_test,y_pred)\n        accuracy_list.append(accuracy)\n        f_score = f1_score(y_test,y_pred)\n        f1_score_list.append(f_score)\n        cohen = cohen_kappa_score(y_test,y_pred)\n        cohen_kappa_list.append(cohen)\n    \n    df = pd.DataFrame([recall_list,precision_list,accuracy_list,f1_score_list,cohen_kappa_list])\n    new_df = df.transpose()\n    new_df.columns=performance_measure\n    new_df.index=algo\n    colors = [\"bisque\",\"ivory\",\"sandybrown\",\"steelblue\",\"lightsalmon\"]\n    colormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n    background_color = \"white\"\n    fig = plt.figure(figsize=(18,26)) # create figure\n    gs = fig.add_gridspec(4, 2)\n    gs.update(wspace=0.1, hspace=0.5)\n    ax0 = fig.add_subplot(gs[0, :])\n    sns.heatmap(new_df, cmap=colormap,annot=True,fmt=\".1%\",vmin=0,vmax=0.95, linewidths=2.5,cbar=False,ax=ax0,annot_kws={\"fontsize\":16})\n    fig.patch.set_facecolor(background_color) # figure background color\n    ax0.set_facecolor(background_color) \n    ax0.text(0,-0.5,'Model Comparison',fontsize=20,fontweight='bold',fontfamily='serif')\n    plt.show()","445f7f07":"#display_performance()","fb8fead2":"from xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(X_train,y_train)\ny_pred = xgb.predict(X_test)","c9aeb2bb":"from sklearn.metrics import classification_report,confusion_matrix,cohen_kappa_score\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncohen = cohen_kappa_score(y_test,y_pred)\nprint('Cohen Kappa Score',cohen)","17eafabd":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(n_estimators=100, criterion = 'entropy')\nrfc.fit(X_train,y_train)\ny_pred = rfc.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\ncohen = cohen_kappa_score(y_test,y_pred)\nprint('Cohen Kappa Score',cohen)","8b28904f":"### Explarotary Data Analysis","3d9c2902":"### *Now Our Data seems to be clean Lets Create Bag of words and check our model accuracy.*","db21114e":"### Data Cleaning","aa4f5630":"### *Analyzing Punctuations*","25c3f57b":"### *NGram Analysis*"}}