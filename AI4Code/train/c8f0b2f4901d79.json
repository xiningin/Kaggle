{"cell_type":{"2e2ddd28":"code","1eb5e2d0":"code","b8b86f68":"code","324bdbc8":"code","08c5e443":"code","f70728e8":"code","9b4750ca":"code","1554879a":"code","1ac05101":"code","79d6bde2":"code","7c46835b":"code","9e782ca5":"code","2d35dc12":"code","963833aa":"code","f7de6f45":"code","bb2e1849":"code","423c7e57":"code","72b2b2cf":"code","b5d5f9e5":"code","9d53a85c":"code","87d6f4ed":"code","f66692cc":"code","c578210c":"code","c571b6b6":"code","0aec3c56":"code","7753178c":"code","7a344510":"code","17c47fb2":"code","d8cb0ecd":"code","6f225588":"code","7947894f":"code","998071f0":"code","7cb85010":"code","68eb032b":"code","c4590736":"code","3a67a388":"code","a08f9128":"code","725846bb":"code","99c9cbe7":"markdown","923f57f7":"markdown","ed13d1e1":"markdown","480a288a":"markdown","c6fd2a14":"markdown","db6d6c5d":"markdown","d6a3d1e3":"markdown","d521344f":"markdown","8864310d":"markdown","afa5dc15":"markdown","8291e197":"markdown"},"source":{"2e2ddd28":"import numpy as np \nimport pandas as pd \nfrom glob import glob \nimport os\nimport tensorflow as tf\nimport cv2\nimport matplotlib.pyplot as plt\n%matplotlib inline","1eb5e2d0":"images_path = \"..\/input\/flickr8k-sau\/Flickr_Data\/Images\/\"\nimages = glob(images_path+\"*.jpg\")\nprint(len(images))","b8b86f68":"images[0]","324bdbc8":"from  tensorflow.keras.applications import ResNet50","08c5e443":"incept_model = ResNet50(include_top=True)","f70728e8":"incept_model.summary()","9b4750ca":"from tensorflow.keras.models import Model \nlast_layer = incept_model.layers[-2].output\nres_net_model = Model(inputs=incept_model.input,outputs=last_layer)","1554879a":"incept_model.input\n","1ac05101":"res_net_model.summary()","79d6bde2":"images_features = {}\ncount= 0\nfor image in images : \n    img = cv2.imread(image)\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img,(224,224))\n    img = img.reshape(1,224,224,3)\n    predict = res_net_model.predict(img).reshape(2048,)\n    image_name = image.split(\"\/\")[-1]\n    images_features[image_name] = predict\n    count +=1\n    if count>1700: \n        break\n    elif count % 50 == 0:\n        print(count)\n            \n    ","7c46835b":"captions_path = \"..\/input\/flickr8k-sau\/Flickr_Data\/Flickr_TextData\/Flickr8k.token.txt\"","9e782ca5":"captions = pd.read_csv(captions_path,sep=\"\\n\",header=None)","2d35dc12":"captions = open(captions_path,\"rb\").read().decode('utf8').split(\"\\n\")","963833aa":"captions[0]","f7de6f45":"captions_dict = {}\nfor i in captions : \n    try : \n        img_name = i.split(\"\\t\")[0][:-2]\n        caption  = i.split(\"\\t\")[1]\n        if img_name in images_features: \n            if img_name not in captions_dict:\n                # the case when the image name is not created yet\n                captions_dict[img_name] = [caption]\n            else : \n                # the case where the image name is created so we will juste append the new value knowing that there is 5 captions for every image\n                captions_dict[img_name].append(caption)\n    except:\n        pass","bb2e1849":"def preprocessed(txt): \n    modified = txt.lower()\n    modified = \"startofseq\" + modified + \"endofseq\"\n    return modified","423c7e57":"for k , v in captions_dict.items() : \n    for caption in v: \n        captions_dict[k][v.index(caption)] = preprocessed(caption)\n        ","72b2b2cf":"from tensorflow.keras.preprocessing.text import Tokenizer","b5d5f9e5":"tokenizer = Tokenizer(oov_token=\"Other\")","9d53a85c":"sentences = []\nfor k,v in captions_dict.items(): \n    for sentence in v: \n        sentences.append(sentence)","87d6f4ed":"tokenizer.fit_on_texts(sentences)","f66692cc":"count_words = tokenizer.word_index","c578210c":"sequences =  tokenizer.texts_to_sequences(sentences)","c571b6b6":"sequences_images = {}\nfor k,v in captions_dict.items(): \n    sequences_images[k] = sequences[:5]\n    sequences = sequences[5:]\n    ","0aec3c56":"from tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","7753178c":"sequences =  tokenizer.texts_to_sequences(sentences)","7a344510":"MAX_LEN = np.max([len(x) for x in sequences])\nMAX_LEN","17c47fb2":"VOCAB_SIZE = len(count_words)\ndef generator (photo , caption ): \n    X = []\n    y_in = []\n    y_out = []\n    for k , v in caption.items(): \n        for sentence in v : \n            for i in range(1,len(sentence)): \n#                 this for iterate every word in the sentence\n                X.append(photo[k])\n                in_seq = [sentence[:i]]\n                out_seq = [sentence[i]]    \n                in_seq = pad_sequences(in_seq,maxlen=MAX_LEN,padding =\"post\" ,truncating=\"post\")[0]\n                out_seq = to_categorical([out_seq],num_classes=VOCAB_SIZE+1)[0]\n                y_in.append(in_seq)\n                y_out.append(out_seq)\n        \n    return X, y_in,y_out","d8cb0ecd":"X,y_in,y_out = generator(images_features,sequences_images)","6f225588":"X,y_in,y_out = np.array(X),np.array(y_in),np.array(y_out)","7947894f":"X.shape,y_in.shape,y_out.shape","998071f0":"from tensorflow.keras.utils import to_categorical , plot_model \nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Input \nfrom tensorflow.keras.layers import Dense \nfrom tensorflow.keras.layers import LSTM \nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Convolution2D\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras.layers import Bidirectional\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras.layers import RepeatVector\nfrom tensorflow.keras.layers import Concatenate\nfrom tensorflow.keras.callbacks import ModelCheckpoint\n\n","7cb85010":"MAX_LEN","68eb032b":"embeding_size = 128\nmax_len = MAX_LEN\nvocab_size = len(count_words) +1\nimage_model = Sequential()\nimage_model.add(Dense(embeding_size,input_shape = (2048,),activation=\"relu\"))\nimage_model.add(RepeatVector(max_len))\nimage_model.summary()\n\nlanguage_model = Sequential()\nlanguage_model.add(Embedding(input_dim=vocab_size,output_dim = embeding_size,input_length=max_len))\nlanguage_model.add( LSTM(256,return_sequences = True))\nlanguage_model.add(TimeDistributed(Dense(embeding_size)))\nlanguage_model.summary()\n\nconca = Concatenate()([image_model.output, language_model.output])\nx = LSTM(128,return_sequences = True)(conca)\nx = LSTM(512,return_sequences = False)(x)\nx = Dense(vocab_size)(x)\nout = Activation(\"softmax\")(x)\nmodel = Model([image_model.input,language_model.input],outputs = out)\nmodel.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics = [\"accuracy\"])","c4590736":"model.summary()","3a67a388":"model.fit([X, y_in], y_out, batch_size=256, epochs=50)\n","a08f9128":"def getImage(x): \n    test_img_path = images[x]\n    test_img = cv2.imread(test_img_path)\n    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n    test_img = cv2.resize(test_img, (224,224))\n    test_img = np.reshape(test_img, (1,224,224,3))\n    \n    return test_img","725846bb":"for i in range(5):\n    # getting a random image\n    no = np.random.randint(1500,7000,(1,1))[0,0]\n    # extracting the features from the ResNet model\n    test_feature = res_net_model.predict(getImage(no)).reshape(1,2048)\n    \n    test_img_path = images[no]\n    test_img = cv2.imread(test_img_path)\n    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n\n\n    text_inp = ['startofseq']\n\n    count = 0\n    caption = ''\n    while count < 25:\n        count += 1\n        encoded = tokenizer.texts_to_sequences(text_inp)\n        encoded = pad_sequences(encoded, padding='post', truncating='post', maxlen=max_len)\n        prediction = np.argmax(model.predict([test_feature, encoded]))\n        for k,v in tokenizer.word_index.items():\n            if v == prediction:\n                sampled_word = k\n                break\n                \n                \n        if sampled_word == 'endofseq':\n            break\n        caption = caption + ' ' + sampled_word\n        \n        text_inp[0] += ' ' + sampled_word\n    plt.figure()\n    plt.imshow(test_img)\n    plt.xlabel(caption)","99c9cbe7":"Now will create our vocabulary, and for this we will the Tensorflow tokenizer","923f57f7":"Data Preprocess","ed13d1e1":"Now we will choose a random image and predict it's caption","480a288a":"# Creating the model\n","c6fd2a14":"> Note : we are not able to work with more data because we had a limited resources.","db6d6c5d":"# Testing the model ","d6a3d1e3":"we are going to read the data (images) from the files and for every image we will resize it to (224,224) to being able to fiting it to the ResNet50 model ","d521344f":"# Pre processing the Data","8864310d":"We will use the Resnet Model from Keras which is a pre-trained model to extract the features (objects) from the images","afa5dc15":"As we can see, after 100 epochs we had an accuracy of 88.76% on the training set. Not bed for juste a 100 of epochs and 1700 images","8291e197":"Now we will add a StartOfStrinng and EndOfString Tokens to the sentences"}}