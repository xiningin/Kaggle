{"cell_type":{"28f04a49":"code","60a0006a":"code","cdbb8dfd":"code","ebc00b7b":"code","b03d20e9":"code","345432c6":"code","9c327737":"markdown","8d2ea47d":"markdown","dd365963":"markdown","6b43e43e":"markdown","3bbc4323":"markdown","3e12f599":"markdown"},"source":{"28f04a49":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport time\nimport gc\nstart_time = time.time()","60a0006a":"random_seed = 17025\nnp.random.seed(random_seed)\nfrom tensorflow.keras.models import load_model\nfrom sklearn.metrics import confusion_matrix\nfrom skimage.transform import resize\nimport matplotlib.pyplot as plt","cdbb8dfd":"#Load train and test data\ntrain = pd.read_csv('..\/input\/bengaliai-cv19\/train.csv')\ntest = pd.read_csv('..\/input\/bengaliai-cv19\/test.csv')\ngmap = pd.read_csv('..\/input\/bengaliai-cv19\/class_map.csv')\n\npq_paths = {'Train_0':'..\/input\/bengaliai-cv19\/train_image_data_0.parquet',\n            'Train_1':'..\/input\/bengaliai-cv19\/train_image_data_1.parquet',\n            'Train_2':'..\/input\/bengaliai-cv19\/train_image_data_2.parquet',\n            'Train_3':'..\/input\/bengaliai-cv19\/train_image_data_3.parquet',\n            'Test_0':'..\/input\/bengaliai-cv19\/test_image_data_0.parquet',\n            'Test_1':'..\/input\/bengaliai-cv19\/test_image_data_1.parquet',\n            'Test_2':'..\/input\/bengaliai-cv19\/test_image_data_2.parquet',\n            'Test_3':'..\/input\/bengaliai-cv19\/test_image_data_3.parquet'}\n\ntarget_cols = ['grapheme_root','vowel_diacritic','consonant_diacritic']","ebc00b7b":"def values_from_pqt(name, batch_size):\n    \n    pqt = pd.read_parquet(pq_paths[name])\n    \n    PQT_COLUMNS = pqt.columns[1:]\n    concurrent_n = batch_size\/\/4\n    \n    for i in range(0, pqt.shape[0], batch_size):\n        \n        accumulator_x = np.array([], dtype='float16')\n        \n        for n in range(i, i+batch_size, concurrent_n):\n            print('PQT %s begin %i end %i' %(name, i, n+concurrent_n))\n            try:\n                value = 1 - (pqt[PQT_COLUMNS][n:n+concurrent_n].values.reshape(concurrent_n, 137, 236, 1))\/255\n                value = resize(value, (concurrent_n, 80, 80, 1))\n            except:\n                value = pqt[PQT_COLUMNS][n:].values\n                if value.shape[0]:\n                    value = 1 - (value.reshape(value.shape[0], 137, 236, 1))\/255\n                    value = resize(value, (value.shape[0], 80, 80, 1))\n                else:\n                    break\n            try:\n                accumulator_x = np.concatenate([accumulator_x, value.astype('float16')], axis=0)\n            except:\n                accumulator_x = value.astype('float16')\n        try:\n            yield (np.array(accumulator_x).astype('float16'), pqt.image_id[i:i+batch_size].values.tolist())\n        except:\n            yield (np.array(accumulator_x).astype('float16'), pqt.image_id[i:].values.tolist())\n    del pqt\n    gc.collect()","b03d20e9":"prefix = \"..\/input\/three-part-model-training-kernel-only-part-2-3\/\" #Viable to change\n\ntargets = {'grapheme_root': load_model(prefix+'multi_out_cnn_model_root.h5'),\n           'vowel_diacritic': load_model(prefix+'multi_out_cnn_model_vowel.h5'),\n           'consonant_diacritic': load_model(prefix+'multi_out_cnn_model_consonant.h5')\n          }\n\ntarget_cols = ['consonant_diacritic','grapheme_root','vowel_diacritic'] #Arrange in this order in sample_submission\n\npredictions = {'row_id':[], 'target':{\n    'consonant_diacritic':[],\n    'grapheme_root':[],\n    'vowel_diacritic':[]}\n              }\n\nprint(\"Start reading data:\")\n\nfor i in range(4):\n    for test_x, ids in values_from_pqt(\"Test_{}\".format(i), 10000):\n        print(\"Currently Predicting: Test_{}\".format(i))\n        #For X values\n        for component in target_cols:\n            print(\"For %s:\" %component)\n            preds = np.argmax(targets[component].predict(test_x), axis=1)\n            try:\n                predictions['target'][component] = np.concatenate([\n                    predictions['target'][component], preds],\n                    axis=0\n                )\n            except:\n                predictions['target'][component] = preds\n        #For Y values\n        try:\n            predictions['row_id'] = np.concatenate([\n                predictions['row_id'], ids\n            ])\n        except:\n            predictions['row_id'] = ids\n        print(\"#\"*72)\nids = []\ntgt = []\nfor i in range(len(predictions['row_id'])):\n    for col in target_cols:\n        ids.append(predictions['row_id'][i]+'_'+col)\n        tgt.append(int(predictions['target'][col][i]))\n\nsubmission = pd.DataFrame()\nsubmission['row_id'] = ids\nsubmission['target'] = tgt\nsubmission.to_csv('submission.csv', index=False)\nprint(submission)","345432c6":"end_time = time.time()\ntotal_time = end_time - start_time\nhours = total_time\/\/3600\nminutes = (total_time%3600)\/\/60\nseconds = (total_time%60)\nprint(\"Total Time spent is: %i hours, %i minutes, and %i seconds\" %((hours, minutes, seconds)))","9c327737":"# End","8d2ea47d":"# Import Modules","dd365963":"# Read Data","6b43e43e":"# Part 3: Prediction","3bbc4323":"# Peek","3e12f599":"# Pre-processing"}}