{"cell_type":{"f72768a1":"code","4d2bc339":"code","7ba1c85a":"code","5e9ed5ff":"code","a40fea6c":"code","0222f323":"code","b51ec383":"code","b3b39a2e":"code","e2a13381":"code","34a0a1a1":"code","918b8196":"code","2b058806":"code","21823ba9":"code","2f358402":"code","11230262":"code","da4d9902":"code","c8377452":"code","76523871":"code","abf9815d":"code","cf93f574":"code","ce80ba23":"code","8edf62fd":"code","1d0c3d67":"code","b27593d6":"code","6094311c":"code","efd99102":"code","e2859fa6":"code","696d1c6d":"code","799eeffc":"code","6e33e243":"code","fb2b6e23":"code","839ecd25":"code","4f198f0d":"code","d31140bd":"code","60a21961":"code","2672faaa":"markdown","f567859d":"markdown"},"source":{"f72768a1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4d2bc339":"import plotly.express as px\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.model_selection import train_test_split\nfrom pylab import plot, show, subplot, specgram, imshow, savefig\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score,f1_score, confusion_matrix\n%matplotlib inline","7ba1c85a":"train = pd.read_csv(\"\/kaggle\/input\/janatahack-crosssell-prediction\/train.csv\")\ntrain.head()","5e9ed5ff":"test = pd.read_csv(\"\/kaggle\/input\/janatahack-crosssell-prediction\/test.csv\")\ntest.head()","a40fea6c":"train.info()","0222f323":"train.isna().sum()","b51ec383":"train.dtypes","b3b39a2e":"train.shape","e2a13381":"def plot_feature_importance(importance,names,model_type):\n\n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n\n    #Define size of bar plot\n    plt.figure(figsize=(10,8))\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + ' FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')","34a0a1a1":"le = LabelEncoder()\ntrain[\"Gender\"] = le.fit_transform(train[\"Gender\"])\ntrain[\"Vehicle_Age\"] = le.fit_transform(train[\"Vehicle_Age\"])\ntrain[\"Vehicle_Damage\"] = le.fit_transform(train[\"Vehicle_Damage\"])\n\ntest[\"Gender\"] = le.fit_transform(test[\"Gender\"])\ntest[\"Vehicle_Age\"] = le.fit_transform(test[\"Vehicle_Age\"])\ntest[\"Vehicle_Damage\"] = le.fit_transform(test[\"Vehicle_Damage\"])","918b8196":"rf_model = RandomForestClassifier().fit(train.drop([\"id\", \"Response\"],axis=1),train[\"Response\"])\nplot_feature_importance(rf_model.feature_importances_,train.drop([\"id\", \"Response\"],axis=1).columns,'RANDOM FOREST')","2b058806":"train.dtypes","21823ba9":"train.head()","2f358402":"sns.set(rc={'figure.figsize':(11.7,8.27)})\nsns.heatmap(data=train.drop(\"id\", axis=1).corr().round(2), annot = True)\nplt.show()","11230262":"sns.set(rc={'figure.figsize':(15,8)})\nsns.lineplot(x='Age', y='Vintage', data=train)\nplt.show()","da4d9902":"sns.set(rc={'figure.figsize':(19,8)})\nsns.distplot(train['Age'], kde=True)\nplt.show()","c8377452":"def show_donut_plot(col): #donut plot function\n    \n    rating_data =train.groupby(col)[['id']].count().head(10)\n    plt.figure(figsize = (12, 8))\n    plt.pie(rating_data[['id']], autopct = '%1.0f%%', startangle = 140, pctdistance = 1.1, shadow = True)\n\n    # create a center circle for more aesthetics to make it better\n    gap = plt.Circle((0, 0), 0.5, fc = 'white')\n    fig = plt.gcf()\n    fig.gca().add_artist(gap)\n    \n    plt.axis('equal')\n    \n    cols = []\n    for index, row in rating_data.iterrows():\n        cols.append(index)\n    plt.legend(cols)\n    \n    plt.title('Donut Plot: Response Proportion for Cross-Sale ', loc='center')\n    plt.show()","76523871":"show_donut_plot('Response')","abf9815d":"print(train[train.Response == 1].shape)\nprint(train[train.Response == 0].shape)","cf93f574":"shuffled_train = train.sample(frac=1,random_state=4)\n\ntrain_1 = shuffled_train.loc[shuffled_train['Response'] == 1]\n\ntrain_0 = shuffled_train.loc[shuffled_train['Response'] == 0].sample(n = 46710,random_state=42)\n\nnew_train = pd.concat([train_1, train_0])\n\nnew_train = new_train.sample(frac=1,random_state=4)\n\nplt.figure(figsize=(8, 8))\nsns.countplot('Response', data=new_train)\nplt.title('Balanced Data')\nplt.show()\n","ce80ba23":"new_train.drop('id', axis=1, inplace=True)\nId = test['id'].tolist()\ntest.drop('id', axis=1, inplace=True)","8edf62fd":"print(new_train.columns)\nprint(test.columns)","1d0c3d67":"train_ = new_train.drop(['Gender', 'Driving_License'], axis=1)\ntest_ = test.drop(['Gender', 'Driving_License'], axis=1)","b27593d6":"#List of classifiers\n\nclfs = {\n    'mnb': MultinomialNB(),\n    'gnb': GaussianNB(),\n    'dtc': DecisionTreeClassifier(),\n    'rfc': RandomForestClassifier(),\n    'lr': LogisticRegression(),\n    'gbc': GradientBoostingClassifier()\n}","6094311c":"#accuracy for the list of classifiers\n\naccuracy_scores = dict()\ntrain_x, test_x, train_y, test_y = train_test_split(train_.drop(\"Response\", axis=1), train_[\"Response\"], test_size= 0.3)\nfor clf_name in clfs:\n    \n    clf = clfs[clf_name]\n    clf.fit(train_x, train_y)\n    y_pred = clf.predict(test_x)\n    accuracy_scores[clf_name] = accuracy_score(y_pred, test_y)\n    print(clf, '-' , accuracy_scores[clf_name])","efd99102":"accuracy_scores = dict(sorted(accuracy_scores.items(), key = lambda kv:(kv[1], kv[0]), reverse= True))\nvilli = list(accuracy_scores.keys())[0]\nprint(\"Classifier with high accuracy --> \",clfs[villi])\nprint(\"With the accuracy of\",accuracy_scores[villi])","e2859fa6":"clfs[villi].fit(train_.drop(\"Response\", axis=1), train_[\"Response\"])","696d1c6d":"print(train_.columns)\nprint(test_.columns)","799eeffc":"pred = clfs[villi].predict(test_)\npred = pred.tolist()","6e33e243":"sub = pd.DataFrame({\"id\" : Id, \"Response\" : pred})","fb2b6e23":"s = pd.read_csv(\"\/kaggle\/input\/janatahack-crosssell-prediction\/sample_submission.csv\")\ns.head()","839ecd25":"s.shape","4f198f0d":"test.shape","d31140bd":"s.to_csv(\"Submission_CSP.csv\", index=False)","60a21961":"s.Response.value_counts()","2672faaa":"We can infer from the dataset that nearly 85% of the customers are interested in Vehicle insurance.","f567859d":"***Top 5 features are,***\n* Vintage\n* Annual_Premium\n* Age\n* Region_Code\n* Vehicle_Damage"}}