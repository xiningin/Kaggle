{"cell_type":{"f21dc637":"code","933c799d":"code","5007b565":"code","0ff9086d":"code","684376c0":"code","feccfbee":"code","a01a0053":"code","4a96a6b7":"code","ccceea4a":"code","b03f39c2":"code","64d07b04":"code","190b804c":"markdown","1993ab3f":"markdown","bbd4a51a":"markdown","9b70173d":"markdown","1a76f1dc":"markdown","518308bf":"markdown","d472d311":"markdown"},"source":{"f21dc637":"# Loading the packages\nimport numpy as np\nimport pandas as pd \nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import make_scorer \nfrom sklearn.model_selection import GridSearchCV\n\nimport warnings\nwarnings.filterwarnings('ignore')","933c799d":"# Loading the training dataset\ndf_train = pd.read_csv(\"..\/input\/train.csv\")","5007b565":"y = df_train[\"target\"]\n# We exclude the target and id columns from the training dataset\ndf_train.pop(\"target\");\ndf_train.pop(\"id\")\ncolnames1 = df_train.columns","0ff9086d":"\n\nscaler = StandardScaler()\nscaler.fit(df_train)\nX = scaler.transform(df_train)\ndf_train = pd.DataFrame(data = X, columns=colnames1)   # df_train is standardized ","684376c0":"random_forest_predictors = [\"33\", \"279\", \"272\", \n                           \"83\", \"237\", \"241\", \n                           \"91\", \"199\", \"216\", \n                           \"19\", \"65\", \"141\", \"70\", \"243\", \"137\", \"26\", \"90\"]\n\npredictors = random_forest_predictors\n\ndf_train = df_train[predictors]\n","feccfbee":"# We adapt code from this kernel: \n# https:\/\/www.kaggle.com\/vincentlugat\/logistic-regression-rfe\n\n# Find best hyperparameters (roc_auc)\nrandom_state = 0\nclf = LogisticRegression(random_state = random_state)\nparam_grid = {'class_weight' : ['balanced'], \n              'penalty' : ['l1'],  \n              'C' : [0.0001, 0.0005, 0.001, \n                     0.005, 0.01, 0.05, 0.1, 0.5, 1, \n                     10, 100, 1000, 1500, 2000, 2500, \n                     2600, 2700, 2800, 2900, 3000, 3100, 3200  \n                     ], \n              'max_iter' : [100, 1000, 2000, 5000, 10000] }\n\n# Make an roc_auc scoring object using make_scorer()\nscorer = make_scorer(roc_auc_score)\n\ngrid = GridSearchCV(estimator = clf, param_grid = param_grid , \n                    scoring = scorer, verbose = 1, cv=20,\n                    n_jobs = -1)\n\nX = df_train.values\n\ngrid.fit(X,y)\n\nprint(\"Best Score:\" + str(grid.best_score_))\nprint(\"Best Parameters: \" + str(grid.best_params_))\n\nbest_parameters = grid.best_params_","a01a0053":"# We get the best model \nbest_clf = grid.best_estimator_\nprint(best_clf)","4a96a6b7":"model = LogisticRegression(C=0.1, class_weight='balanced', dual=False,\n          fit_intercept=True, intercept_scaling=1, max_iter=100,\n          multi_class='warn', n_jobs=None, penalty='l1', random_state=0,\n          solver='warn', tol=0.0001, verbose=0, warm_start=False);\n\nmodel.fit(X, y);\n","ccceea4a":"print(model.coef_)","b03f39c2":"df_test = pd.read_csv(\"..\/input\/test.csv\")\ndf_test.pop(\"id\");\nX = df_test \nX = scaler.transform(X)\ndf_test = pd.DataFrame(data = X, columns=colnames1)   # df_train is standardized \ndf_test = df_test[predictors]\n\n\nX = df_test.values\ny_pred = model.predict_proba(X)\ny_pred = y_pred[:,1]    ","64d07b04":"# submit prediction\nsmpsb_df = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsmpsb_df[\"target\"] = y_pred\nsmpsb_df.to_csv(\"logistic_regression_l2_v1.csv\", index=None)","190b804c":"In this kernel:\n\nhttps:\/\/www.kaggle.com\/ricardorios\/random-forests-don-t-overfit\n\nWe have found the following variables that are related with the target variable: 33, 279, 272, 83, 237, 241, 91, 199, 216, 19, 65, 141, 70, 243, 137, 26, 90. We are going to use these variables to fit the model.","1993ab3f":"There are only 7 variables with coefficients different than zero, the resulting model is more parsimonious and, in consequence less prone to overfitting. Finally, we will generate the file submission.  ","bbd4a51a":"We are going to standardize the explanatory variables by removing the mean and scaling to unit variance, this is mandatory for logistic regression. The standard score for the variable X is calculated as follows:\n\n$$ z= \\frac{X\u2212\\mu}{s} $$\n \nWhere  \u03bc  is the mean and s is the standard deviation.","9b70173d":"The coefficients of the model are shown as follows. ","1a76f1dc":"First of all, I have used ideas from this website:\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2016\/01\/complete-tutorial-ridge-lasso-regression-python\/\n\nIn this kernel we are going to use lasso regression.\n\nhttps:\/\/en.wikipedia.org\/wiki\/Lasso_(statistics)","518308bf":"The best model is obtained with C=0.1, next we are going to fit the model with the whole training dataset.","d472d311":"In order to regularize the model fitted in: \n\nhttps:\/\/www.kaggle.com\/ricardorios\/logistic-regression-model-don-t-overfit\n\nWe are going to use [Lasso Regression](https:\/\/www.statisticshowto.datasciencecentral.com\/lasso-regression\/), one of the advantages of using this approach is that the model is sparse and we get the best predictors. Next, we are going to perform a grid search over the parameter C.  "}}