{"cell_type":{"43136429":"code","f9f32205":"code","4ec1da59":"code","ee9ee1b2":"code","bb50db0f":"code","acfa1680":"code","3ade963b":"code","fae278a7":"code","9f28778f":"code","8b1afe25":"code","52eb2144":"code","e82d9f11":"code","20fd74c7":"code","5865bf26":"code","224639ba":"code","4271a355":"code","a017b659":"code","422774ab":"code","2a4ff28b":"code","5309a72d":"code","01b5138c":"code","e3211b3c":"code","4650a600":"code","b55e945f":"code","a9c2784c":"code","30d8cf01":"code","5cecfce9":"code","b5d9d5c5":"code","afb4e1de":"code","ffd91648":"code","5e704398":"code","364301ad":"code","7c3ea2af":"code","75427b4c":"code","d44bb6dc":"code","10821bbd":"code","6668d7f5":"code","1bf8a196":"code","eee0e377":"code","e05a4552":"code","2eea4028":"code","f4ab1884":"code","20656198":"code","e9eb12a2":"markdown","0019077a":"markdown","c1940264":"markdown","77b3822c":"markdown","4bfdcfa7":"markdown","c7dd5370":"markdown","e2e1ecb3":"markdown","40f35446":"markdown","66f3a792":"markdown","3cb28a0d":"markdown","99a55d14":"markdown","fa205100":"markdown","4768fa8b":"markdown","df242130":"markdown","9c716e0e":"markdown","ee648419":"markdown","2ec54ecd":"markdown","84929cee":"markdown","2e6c3e5a":"markdown","5a1f403f":"markdown","9c32736f":"markdown","83a1f708":"markdown","a1dcda9c":"markdown","0b4fe133":"markdown","42af7df0":"markdown","3f840de5":"markdown","475ed889":"markdown","9d11e84d":"markdown","291fd423":"markdown","7e84105c":"markdown","a396ebc9":"markdown","d5e8b9c7":"markdown","49e77e7e":"markdown","c6121267":"markdown","f3cbb1ad":"markdown","7cb201ed":"markdown","ff777d68":"markdown","d0670c26":"markdown","faab04dd":"markdown","375f3012":"markdown","1198a75a":"markdown","fbd11fc3":"markdown","c54deba0":"markdown"},"source":{"43136429":"import numpy as np  # Scientific computing\nimport pandas as pd # Data analysis and manipulation\n\nfrom datetime import datetime # Dates and times manipulations\n\n# Visualization modules\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n# Plotly is a graphing library for interactive, publication-quality graphs\n# pip install plotly==4.5.4\nimport plotly.graph_objects as go                   \nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings(\"ignore\")","f9f32205":"# Loading the PJM East energy consumption raw data\ndf_raw = pd.read_csv('\/kaggle\/input\/hourly-energy-consumption\/PJME_hourly.csv', index_col = 'Datetime', parse_dates = ['Datetime'])","4ec1da59":"# Creating a data cleaning function\ndef data_clean(df):\n    \n    # Sorting the datetime index\n    df.sort_index(inplace = True)\n    \n    # Dropping datetime duplicates\n    df = df[~df.index.duplicated()]\n    \n    # Setting the frequence to be Hourly\n    df = df.asfreq('H')\n    \n    # Renaming the PJME_MW column to energy\n    df.rename(columns = {'PJME_MW' : 'energy'}, inplace = True)\n    \n    # Filling the Missing values using the preceding values\n    df.energy = df.energy.fillna(method = 'ffill')\n    \n    return df","ee9ee1b2":"# Creating a clean dataframe from the raw data\ndf_clean = data_clean(df_raw)\ndf_clean.head()","bb50db0f":"df_clean.info()","acfa1680":"df_clean.describe()","3ade963b":"# Creating a function to extract some features\ndef data_prep(df):\n    df['year'] = df.index.year\n    df['month'] = df.index.month\n    df['month_name'] = df.index.month_name()\n    df['week_of_year'] = df.index.weekofyear\n    df['quarter'] = df.index.quarter\n    df['day_of_week'] = df.index.dayofweek\n    df['day_of_week_name'] = df.index.day_name()\n    df['day_of_month'] = df.index.day\n    df['day_of_year'] = df.index.dayofyear\n    df['hour'] = df.index.hour\n    \n    return df","fae278a7":"# Adding useful features to the cleaned dataframe\ndf = data_prep(df_clean)\ndf.head()","9f28778f":"# Creating times grouped dataframes in order to analyse them\ndf_year = df.groupby('year')['energy'].sum()\ndf_month = df.groupby('month_name', sort = False)['energy'].sum()\ndf_week_of_year = df.groupby('week_of_year')['energy'].sum()\ndf_quarter = df.groupby('quarter')['energy'].sum()\ndf_day_of_week = df.groupby('day_of_week_name', sort = False)['energy'].sum()\ndf_day_of_month = df.groupby('day_of_month')['energy'].sum()\ndf_day_of_year = df.groupby('day_of_year')['energy'].sum()\ndf_hour = df.groupby('hour')['energy'].sum()","8b1afe25":"# Creating a plotly subplot\nfig = make_subplots(rows=4, cols=2, vertical_spacing = 0.175,\n                    subplot_titles=(['Year', 'Month', 'Day of Week', 'Day of Month', 'Week of Year', 'Day of year', 'Hour', 'Quarter']))\n\nfig.add_trace(go.Scatter(x=df_year.index, y = df_year), row=1, col=1)\nfig.add_trace(go.Scatter(x=df_month.index, y=df_month), row=1, col=2)\nfig.add_trace(go.Scatter(x=df_week_of_year.index, y=df_week_of_year), row=3, col=1)\nfig.add_trace(go.Scatter(x=df_day_of_week.index, y=df_day_of_week), row=2, col=1)\nfig.add_trace(go.Scatter(x=df_day_of_month.index, y=df_day_of_month), row=2, col=2)\nfig.add_trace(go.Scatter(x=df_day_of_year.index, y=df_day_of_year), row=3, col=2)\nfig.add_trace(go.Scatter(x=df_hour.index, y=df_hour), row=4, col=1)\nfig.add_trace(go.Scatter(x=df_quarter.index, y=df_quarter), row=4, col=2)\n\nfig.update_layout(title = 'Energy Consumption of PJME per', height = 700, showlegend = False)\n\nfig.show()","52eb2144":"# Plotting the time series\nfig = px.line(df, x=df.index, y = df.energy)\n\nfig.update_layout(title = \"Energy Consumption of PJME from 2002 to 2018\", \n                  yaxis_title=\"Energy (MW)\", \n                  xaxis_title=\"Date\", \n                  xaxis_rangeslider_visible=True)\nfig.show()","e82d9f11":"from statsmodels.tsa.seasonal import seasonal_decompose","20fd74c7":"# Since our time series has an hourly frequence, we can set the seasonal decomposition frequence to\n# 24 * 365 = 8760 to capture any Yearly seasonality\n# 24 * 30.5 = 732 to capture any Montly seasonality\n# And so on ....\nyear_period = int(24 * 365)\nmonth_period = int(24 * 30.5)\n\ns_dec_additive = seasonal_decompose(df.energy, freq = year_period, model = 'additive')\ns_dec_multiplicative = seasonal_decompose(df.energy, freq = year_period, model = 'multiplicative')\n\ns_dec_additive_monthly = seasonal_decompose(df[:year_period].energy, freq = month_period, model = 'additive')\ns_dec_multiplicative_monthly = seasonal_decompose(df[:year_period].energy, freq = month_period, model = 'multiplicative')","5865bf26":"# Plotting the components\ndef plot_seasonal(res, axes, model):\n    axes[0].set_title(model)\n    res.observed.plot(ax=axes[0], legend=False)\n    axes[0].set_ylabel('Observed')\n    res.trend.plot(ax=axes[1], legend=False)\n    axes[1].set_ylabel('Trend')\n    res.seasonal.plot(ax=axes[2], legend=False)\n    axes[2].set_ylabel('Seasonal')\n    res.resid.plot(ax=axes[3], legend=False)\n    axes[3].set_ylabel('Residual')","224639ba":"# Plotting the yearly seasonal decompose\nfig, axes = plt.subplots(ncols=2, nrows=4, sharex=True, figsize=(12,5))\n\nplot_seasonal(s_dec_additive, axes[:,0], 'Additive')\nplot_seasonal(s_dec_multiplicative, axes[:,1], 'Multiplicative')\n\nplt.tight_layout()\nplt.show()","4271a355":"# Plotting the monthly seasonal decompose\nfig, axes = plt.subplots(ncols=2, nrows=4, sharex=True, figsize=(12,5))\n\nplot_seasonal(s_dec_additive_monthly, axes[:,0], 'Additive')\nplot_seasonal(s_dec_multiplicative_monthly, axes[:,1], 'Multiplicative')\n\nplt.tight_layout()\nplt.show()","a017b659":"import statsmodels.tsa.stattools as sts","422774ab":"# Running the Adfuller test\nadfuller = sts.adfuller(df.energy)\n\n# Extracting the test and critical values\ntest_value, critical_values = adfuller[0], adfuller[4]","2a4ff28b":"print(f'The test value : \\t{test_value}\\nThe critical values : \\t{critical_values}')","5309a72d":"# Creating a function to split the data into train and test based on a given size\ndef data_split(df, size):\n    return df[:size], df[size:]","01b5138c":"# Setting up the size\nsize = int(0.8 * len(df))\n\n# Creating the train and test dataframes\ntrain, test = data_split(df_clean, size)\n\ntrain_hours = len(train)\ntest_hours = len(test)\n\nprint(f'{str(train.energy.tail())} \\n\\n {str(test.energy.head())}')","e3211b3c":"# Plotting the dataframes\nfig = make_subplots()\n\nfig.add_trace(go.Line(x = train.index, y = train.energy, name = \"Train\") )\n\nfig.add_trace(go.Line(x = test.index, y = test.energy, name = 'Test'))\n\nfig.update_layout(title = \"Energy Consumption of PJME \/ Train and Test\", \n                  yaxis_title=\"Energy (MW)\", \n                  xaxis_title=\"Date\", xaxis_rangeslider_visible=True)\n\nfig.show()","4650a600":"from fbprophet import Prophet","b55e945f":"# Creating a function to prepare our data\ndef data_prep_prophet(df):\n    return df.reset_index().copy().rename(columns = {'Datetime' : 'ds', 'energy':'y'})[['ds', 'y']]","a9c2784c":"train_p = data_prep_prophet(train)\ntrain_p.head()","30d8cf01":"# Instantiating a new Prophet object with Multiple Seasonalities\nmodel_p = Prophet(yearly_seasonality=True)\nmodel_p.add_seasonality(name='monthly', period = month_period, fourier_order=5)\n\n# Fitting the model to the train dataframe\nmodel_p.fit(train_p);","5cecfce9":"# Setting up the period of prediction to be the length of the test Hours in the future\nfuture_p = model_p.make_future_dataframe(periods = test_hours, freq = 'H')\n\n# Creating a forecast dataframe that contains the forecasted values - yhat \nforecast_p = model_p.predict(future_p)\n\nforecast_p[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()","b5d9d5c5":"# Saving the forcast into a csv file\nforecast_p.to_csv('forecast_p.csv')","afb4e1de":"fig, ax = plt.subplots(1, figsize=(15,5))\nmodel_p.plot(forecast_p, ax = ax);","ffd91648":"model_p.plot_components(forecast_p);","5e704398":"# Creating a funtion to Plot the test with the forecast results\ndef plot_forecast_test(test, forecast, model):\n    fig = make_subplots()\n\n    fig.add_trace(go.Line(x=test.index, y=test.energy, name = 'Test'))\n    fig.add_trace(go.Line(x=forecast.ds, y = forecast.yhat, name = \"Forecast\") )\n\n    fig.update_layout(title = \"Energy Consumption of PJME \/ Forecast VS Test \/ {}\".format(model), \n                      yaxis_title=\"Energy (MW)\", \n                      xaxis_title=\"Date\", \n                      xaxis=dict(\n                          rangeselector=dict(\n                              buttons=list([\n                                  dict(count=1, label=\"1m\", step =\"month\", stepmode=\"backward\"),\n                                  dict(count=6, label=\"6m\", step =\"month\", stepmode=\"backward\"),\n                                  dict(count=1, label=\"1y\", step =\"year\", stepmode=\"backward\"),\n                                  dict(step=\"all\")\n                              ])\n                          ),\n                          rangeslider=dict(visible=True),\n                          type=\"date\")\n                     )\n    return fig.show()","364301ad":"plot_forecast_test(test, forecast_p[train_hours:], 'Prophet')","7c3ea2af":"import xgboost as xgb","75427b4c":"# Defining the features\nfeatures = ['year', 'month', 'week_of_year', 'quarter', 'day_of_week', 'day_of_month', 'day_of_year', 'hour']\n\nX_train, y_train = train[features], train['energy']\nX_test, y_test = test[features], test['energy']","d44bb6dc":"# Fitting XGB regressor \nmodel_xgb = xgb.XGBRegressor()\nmodel_xgb.fit(X_train, y_train)\nprint(model_xgb)","10821bbd":"# Making the prediction on the X_test set \nfuture_xgb = model_xgb.predict(data = X_test)\n\n# Creating the forecast dataframe\nforecast_xgb = pd.DataFrame()\nforecast_xgb['ds'] = X_test.index\nforecast_xgb[\"yhat\"] = future_xgb\n\nforecast_xgb.tail()","6668d7f5":"# Saving the forcast into a csv file\nforecast_xgb.to_csv(\"forecast_xgb.csv\")","1bf8a196":"plot_forecast_test(test, forecast_xgb, 'XGBoost')","eee0e377":"from sklearn.metrics import mean_squared_error, mean_absolute_error","e05a4552":"def mean_absolute_percentage_error(y_true, y_pred):\n    \"\"\"Calculates MAPE given y_true and y_pred\"\"\"\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100\n\n# Creating an error metrics printing function\ndef print_err(y_true, y_pred):\n    print('MSE   :\\t', round(mean_squared_error(y_true, y_pred), 3))\n    print('RMSE  :\\t', round(np.sqrt(mean_squared_error(y_true, y_pred)), 3))\n    print('MAE   :\\t', round(mean_absolute_error(y_true, y_pred), 3))\n    print('MAPE  :\\t', round(mean_absolute_percentage_error(y_true, y_pred), 3), '%')","2eea4028":"y_true = test.energy\ny_pred_p = forecast_p[train_hours:].yhat\ny_pred_xgb = forecast_xgb.yhat","f4ab1884":"print_err(y_true, y_pred_p)","20656198":"print_err(y_true, y_pred_xgb)","e9eb12a2":"# <center>Exploratory Data Analysis","0019077a":"## Loading Modules","c1940264":"**Analysing the results :** we observe a concrete cyclical pattern from the seasonal graph of each model.\n\n$\\Longrightarrow$ Our data is Yearly Seasonal.","77b3822c":"## <center>Error Metrics\n\nAn **Error Metric** is a type of Metric used to measure the error of a forecasting model. They can provide a way for forecasters to quantitatively compare the performance of competing models. \n\nSome common error metrics are:\n- Mean Squared Error (MSE)\n- Root Mean Square Error (RMSE)\n- Mean Absolute Error (MAE)\n- Mean Absolute Percentage Error (MAPE)","4bfdcfa7":"### Data Preparation \n\nThe input to Prophet is always a dataframe with two columns: ds and y.\n- The ds (datestamp) column should be a date or timestamp format.\n- The y column must be numeric, and represents the measurement we wish to forecast.","c7dd5370":"Some metrics are note available on the sklearn librairy let's define them.","e2e1ecb3":"# <center>2 Time Series Forecasting\n\nTime seriesforecasting is the use of a model to predict future values based on previously observed values.","40f35446":"The decomposition will lead to three components -- trend, seasonal and residual.\n- The trend component captures the slowly-moving overall level of the series.\n- The seasonal component captures patterns that repeat every season.\n- The residual is what is left. It may or may not be autocorrelated.","66f3a792":"### Plotting the grouped dataframes","3cb28a0d":"## <center>1.2 Seasonality\n\n**seasonality** is the presence of variations that occur at specific regular intervals such as weekly, monthly, quarterly or yearly.\n\nWe are gonna use the Seasonal decomposition with the moving averages from the Statsmodel librairy, and test multiple seasonalities.","99a55d14":"**Analysing the results :** Our test value is lower than the critical values for 1%, 5% and 10 %.\n\n$\\Longrightarrow$ We rejet the null hypothesis\n$\\Longrightarrow$ Our data is Stationary\n<br><br><br>","fa205100":"<img src = 'https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/e1\/Stationarycomparison.png'>","4768fa8b":"## <center>2.2 XGBoost","df242130":"### Plotting the Forecast and Components","9c716e0e":"# <center>PJM Time Series Analysis and Forecasting\n\n**PJM** Interconnection LLC (PJM) is a regional transmission organization (RTO) in the United States.\n\nThis notebook is a study of the **PJM** energy consumption dataset and focus on the **East** coast of the US, a friend of mine \"Souames Annis\" worked on a similar notebook targeting the West coast. \n\nGithub repo : https:\/\/github.com\/BrahimMebrek\/PJM-Time-Series-Analysis-and-Forecasting\n\nCheck out Anis work on : https:\/\/github.com\/anis-coding\/PJM-Time-Series-Analysis\n\nIt contains :\n- An Exploratory Data Analysis.\n- A Time Series Analysis through the concepts of **Seasonality** and **Stationarity**.\n- A Time Series Forecasting using **Facebook Prophet** and **XGBoost** Models in order to predict future energy consumption.\n\n\n**Data :** The dataset contains hourly data from 2002 upto 2018 on the power consumption from several companies including PJM.","ee648419":"Two types of seasonal model can be used **Additive** and **Multiplicative**.","2ec54ecd":"### Modeling and Forecasting","84929cee":"<br>\n\n### Created By MEBREK Brahim.","2e6c3e5a":"### Error metrics of the Prophet model ","5a1f403f":"### Analysing the results\n\n- The yearly energy consumption of PJME is constant from 2002 to 2015, a slight decreasing trend is observed from 2015 to 2017, probably due to electrical energy reduction awarness these last 5 years. \n\n    The data for the year 2018 is incomplete which explains the abrupt reduction.\n<br>\n\n- The monthly energy consumption is as expected, a high demande in the summer with a peak in the month of July, probably due to to excessive use of the air conditioners. These months should be target for electrical energy reduction awarness campain. \n\n    A Second pulse is however observed Between December and January probably due to the events, especially Christmas and New Year's Eve.\n<br>\n\n- The weekly energy consumption can be devided as, a high constant demand during the working days due to the to factories, and a relatively smaller demand during weekends.\n<br>\n\n- The hourly energy consumption is predictible, an increase from 6AM to 8PM with a peak at 6PM due to households consumption. Then a decrease until 5AM.\n\n\n- The week of year, day of year and quarter energy consumption demands graph are similar to the monthly destibution.\n\n\n<br>","9c32736f":"XGBoost is a high performance implementation of the gradient boosting framework, in this case if we have $k$ regression trees, where each tree optimize the previous one, we can define the predicted target $\\hat{y}$ as:\n\n$$\\hat{y}_i = \\sum_{k=1}^K f_k(x_i), f_k \\in \\mathcal{F}$$\n\nWe wish to minimize the following objective function: (loss + regularization (either $L^1$ or $L^2$))\n\n$$ \\text{obj}(\\theta) = \\sum_i^n l(y_i, \\hat{y}_i) + \\sum_{k=1}^K \\Omega(f_k) $$","83a1f708":"**Analysing the results :** We can see that our XGBoost model fits our test data pretty well, even better than the Prophet model. He's reaching for high values to predict high peaks demands.\n\nBut we can't visually decide which one is better, let's run some error metrics measurements to to make up our mind.\n\n\n<br>","a1dcda9c":"Let's define the true values from the test dataset and predicted values from the forecasted datasets.","0b4fe133":"The **Augmented Dickey\u2013Fuller (ADF)** statistic, used in our test, is a negative number. The more negative it is, the stronger the rejection of the hypothesis that there is a unit root at some level of confidence.","42af7df0":"### How do we test Stationarity ?\n\nIn statistics and econometrics, an **Augmented Dickey\u2013Fuller test (ADF)** tests the null hypothesis that a unit root is present in a time series sample. \n\nThe alternative hypothesis is different depending on which version of the test is used, in our case we are testing **stationarity**.","3f840de5":"Prophet is a framework from facebook to forecast time series by decomposing the time series into components: trend, seasonality, Holidays and residuals.\n\n$$ x(t) = T(t) + S(t) + H(t) + \\epsilon(t) $$\n\n$S(t)$ is approximated using a fourier series with a period $P = 365$ by default, we can choose the fourier order $n$ of each seasonality:\n\n$$ \\sum_{i=1}^n a_n cos( \\frac{n\\pi}{P} t ) + b_n sin( \\frac{n\\pi}{P} t  ) $$","475ed889":"**Analysing the results :** As we can see the model performs well, but has trouble predicting peak demands. Most of the time he doesn't go for the high values.\n\n<br>","9d11e84d":"<br>\nThe results are quite similar for both models:\n\n- A high **MSE**, **RMSE** due to the natural dispersion of our intial data (Standard deviation $\\sigma = 6464$). With a slight advantage for Prophet.\n    \n- A 10% **MAPE** for both models, which is very precise and accurate.  With a slight advantage for XGBoost.\n\n<br>","291fd423":"### Data Preparation","7e84105c":"**Note :** \n\n- We see that the energy consumption on the end of october 2012 is relatively low, compared to the rest of the distribution. This is due to **Hurricane Sandy** who was the deadliest and most destructive, as well as the strongest, hurricane of the 2012 Atlantic hurricane season. Inflicting nearly 70 billion (2012 USD) in damage, it was the second-costliest hurricane on record in the United States until surpassed by Hurricanes Harvey and Maria in 2017.\n\n<br>","a396ebc9":"### Error metrics of the XGBoost model ","d5e8b9c7":"### Data Splitting\n\nThe Train-set will be 80% and the Test-set 20% of the complete dataframe.","49e77e7e":"## <center>2.1 Facebook Prophet","c6121267":"# Conclusion\n\nThe **EDA** allowed us to extract informations about the general patterns of the energy consumption in the East region of America. These patterns are most of the time Seasonal (cyclical) patterns.\n\nHow can we improve the basic EDA ?\n- Calculting some ratios and percentages, for exemple, what is the percentage of energy consumption of households in comparison to society in general?\n- Gathering recent energy consumption data, in order to see if the slight decreasing trend continues continue over the year.\n- Using cross analysis to extract more insights, for exemple, how daily trends change depending of the time of year ?\n\n\nThe **Time Series** study allowed us to predict energy consumption using two powerful models with similar and precise results.\n\n\n","f3cbb1ad":"### Plotting the test with the forecast results","7cb201ed":"**Analysing the results :** we observe a cyclical pattern too.\n\n$\\Longrightarrow$ Our data is also Monthly Seasonal.\n\n**Note :** We are going to use a Facebook Prophet model and an XGBoost model, so we don't need to specify every seasonality, since the models can recognise most of the cyclical patterns.","ff777d68":"# <center> Time Series\n\nA **time series** is a series of data points indexed in time order. \n\nMost commonly, the data points are equally spaced points in time, and the observations can be taken on any variable that changes over time.\n\nIn our case we are studying an **Hourly energy consumption time series**.","d0670c26":"### Modeling and Forecasting","faab04dd":"### Plotting the test with the forecast results","375f3012":"# <center>1 Time Series Analysis\n\n**Time series analysis** comprises methods for analyzing data in order to extract meaningful statistics and other characteristics of the data.\n\nLet's analyse 2 main aspects of our time series : Seasonality and Stationarity.","1198a75a":"**Analysing the results :** As we can see the prophet model has been able to recognize multiple seasonalities and extract the constant trend.\n\nLet's plot the forecasted and test values, to visually see how our model performed.","fbd11fc3":"## <center>1.2 Stationarity\n\nIn the most intuitive sense, **stationarity** means that the statistical properties of a process generating a time series do not change over time. \n\nIt does not mean that the series does not change over time, just that the way it changes does not itself change over time.\n\nFor example :","c54deba0":"## Data Loading and Cleaning"}}