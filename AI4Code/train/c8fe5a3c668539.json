{"cell_type":{"367d7700":"code","d895baa4":"code","6d582e2d":"code","ffc7ec9d":"code","29eec791":"code","53d37e7d":"code","1c604aa2":"code","93cfdef4":"code","71fb8dbd":"code","7a944bfc":"code","c598979a":"code","bc5c058a":"code","651d9c02":"code","a540ef07":"code","717a44d3":"code","b9278009":"code","23c909a9":"code","bf66fcc9":"code","429766da":"code","513d22f8":"code","2961fa02":"code","70d47e70":"code","aee25299":"code","67923b05":"code","4bbe80fd":"code","b5273c0f":"code","c22b7831":"code","bb9f2558":"code","850bd8d3":"code","45133916":"code","8d3b08f7":"code","d6869d44":"code","78c18f1c":"code","2293065e":"code","36c2ecb3":"markdown","70084f8d":"markdown","eaeb75e0":"markdown","f59f4e31":"markdown","affebf1e":"markdown","3bcf3672":"markdown","f9936380":"markdown","f2165300":"markdown","80ee5258":"markdown"},"source":{"367d7700":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d895baa4":"\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout, Bidirectional\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom wordcloud import WordCloud\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.metrics import classification_report, accuracy_score","6d582e2d":"#Reading the train and test data\n\n#Exploratory Data Analysis on the data\n\n#Text preprocessing in order to clean the data and making it more consistent \n\n#Developing a word embedding layer to convert the words into vectors of lower dimensions\n\n#Model Building using LSTM and Bi-LSTM \n\n#Model Evaulation and Predicting the test output","ffc7ec9d":"# Reading the training and test data set\n\ntrain = pd.read_csv(\"..\/input\/fake-news\/train.csv\")\ntrain.shape","29eec791":"train.head()\n# Labels are 0 and 1 \n# 0 : Its not a fake news article (real news)\n# 1 : Its a fake news article","53d37e7d":"# test dataset\ntest = pd.read_csv('..\/input\/fake-news\/test.csv')\ntest.shape","1c604aa2":"print(train.isnull().sum())\nprint(test.isnull().sum())","93cfdef4":"\n#filling NULL values with empty string\ntrain=train.fillna('')\ntest=test.fillna('')","71fb8dbd":"train['label'].value_counts()\n\n# This dataset is a balanced one","7a944bfc":"# Preparing the word cloud for the real news article to order to understand type of words in real news\n# (column : title of the news)\n\nreal = train[train['label']==0]\nreal.shape\n\ntext = ''\nfor news in real.title.values:\n    text += f\" {news}\"  # f will help to retain the text as a string\nwordcloud = WordCloud(width = 3000,height = 2000, background_color = 'black',\n                      stopwords = set(nltk.corpus.stopwords.words(\"english\"))).generate(text)\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()\ndel text\n\n# In this real news articles there is mention of publisher : (eg : Newyork times) in some articles","c598979a":"# Preparing the word cloud for fake news article to order to understand type of words in fake news\n# (column : title)\n\nfake = train[train['label']==1]\nfake.shape\n\ntext = ''\nfor news in fake.title.values:\n    text += f\" {news}\"  # f will help to retain the text as a string\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = set(nltk.corpus.stopwords.words(\"english\"))).generate(text)\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()\ndel text\n\n# In this articles there is no mention of any publisher in the fake news ","bc5c058a":"# Preparing the word cloud for real news article to check distribution of words in real author names\n# column : Author\n\nreal = train[train['label']==0]\nreal.shape\n\ntext = ''\nfor news in real.author.values:\n    text += f\" {news}\"  # f will help to retain the text as a string\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = set(nltk.corpus.stopwords.words(\"english\"))).generate(text)\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()\ndel text\n\n# We can see mention of real author names in the real news articles","651d9c02":"# Preparing the word cloud for fake news article to check distribution of words in fake author names\n# column : author\n\nfake = train[train['label']==1]\nfake.shape\n\ntext = ''\nfor news in fake.author.values:\n    text += f\" {news}\"  # f will help to retain the text as a string\nwordcloud = WordCloud(\n    width = 3000,\n    height = 2000,\n    background_color = 'black',\n    stopwords = set(nltk.corpus.stopwords.words(\"english\"))).generate(text)\nfig = plt.figure(\n    figsize = (40, 30),\n    facecolor = 'k',\n    edgecolor = 'k')\nplt.imshow(wordcloud, interpolation = 'bilinear')\nplt.axis('off')\nplt.tight_layout(pad=0)\nplt.show()\ndel text\n\n# In this fake news article, the author names are hidden in the form of blogger, editor, anonymous, no reply, activist, post, admin, editor...","a540ef07":"# Creating a new independent variable  : concatenating author and title column\n# This new independent variable (creating using author and title column) would be perform better in classifying the real and fake news instead of taking an individual variable\n# Will use this new independent variable in building the model\n\ntrain['ti_auth'] = train['title']+' '+ train['author']\ntest['ti_auth']= test['title']+' '+test['author']","717a44d3":"# Dropping the label column from train set #\n\nX = train.drop(['label'],axis=1)\nprint(X.shape)\n\nY = train['label']\nprint(Y.head())","b9278009":"from nltk.stem.porter import PorterStemmer\nps = PorterStemmer()","23c909a9":"X.columns","bf66fcc9":"# Training data\n\ncorpus_train=[]\n\nfor i in range(len(X)):\n    input = re.sub('[^a-zA-Z]',' ',X['ti_auth'][i]) # except a-z and A-Z, substitute all other characters with ' '\n    input = input.lower() # Lower case \n    input  = input.split() # tokenize the text\n    input = [ps.stem(word) for word in input if word not in stopwords.words('english')] # ignoring stopwords and stemming rest of the words\n    text = ' '.join(input)  # concatenating all words into a single text (list is created)#\n    corpus_train.append(text) # appending text into a single corpus #\nlen(corpus_train)","429766da":"# Preprocessed text \ncorpus_train[0]","513d22f8":"# Test Data\n\n\ncorpus_test=[]\n\nfor i in range(len(test)):\n    input = re.sub('[^a-zA-Z]',' ',test['ti_auth'][i]) # except a-z and A-Z, substitute all other characters with ' '\n    input = input.lower()\n    input  = input.split() # tokenize the text\n    input = [ps.stem(word) for word in input if word not in stopwords.words('english')] # ignoring stopwords and stemming rest of the words\n    text = ' '.join(input)  # concatenating all words into a single text (list is created)#\n    corpus_test.append(text) # appending text into a single corpus #\nlen(corpus_test)","2961fa02":"# Converting this text into One hot representation #\n# Provide an index for each and every word in the text w.r.t the voc_size\n\nvoc_size =5000\nonehot_rep_train = [one_hot(words,voc_size)for words in corpus_train]\nonehot_rep_test = [one_hot(words,voc_size)for words in corpus_test]","70d47e70":"onehot_rep_train[0]","aee25299":"# Train dataset #\n# Find out the maximum length of the sentence in the corpus so that we will pad all sentences to that length\n\nc=[]\nfor i in range(len(train)):\n    m=len(train['ti_auth'][i].split())\n    c.append(m)\n\nprint('Maximum length of a sentence is : ',max(c))\nprint('Average length of a sentence is : ',sum(c)\/len(c))\n\n# In train dataset, max length is 74","67923b05":"# Test dataset #\n# Find out the maximum length of the sentence in the corpus so that we will pad all sentences to that length\n\nc=[]\nfor i in range(len(test)):\n    m=len(test['ti_auth'][i].split())\n    c.append(m)\n\nprint('Maximum length of a sentence is : ',max(c))\nprint('Average length of a sentence is : ',sum(c)\/len(c))\n\n# In train dataset, max length is 65\n\n# So we will sentence length is 74 for train and test as it is maxium of the two","4bbe80fd":"# Pad_sequences in order to make all sentences of equal lenghts of 74\n\nembedded_docs_train = pad_sequences(onehot_rep_train,padding='pre',maxlen=74)\nembedded_docs_test = pad_sequences(onehot_rep_test,padding='pre',maxlen=74)","b5273c0f":"# Building an LSTM Model with an embedding layer #\nmodel = Sequential()\nmodel.add(Embedding(voc_size,40,input_length=74))  # 40 : dimension of each vector, input_length : sentence length\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(100))  # 100 memory cells \/ Neurons \nmodel.add(Dropout(0.3))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation='sigmoid'))  # sigmoid will help in classifying news as fake or real\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())","c22b7831":"#Converting into numpy array\ntrain_final = np.array(embedded_docs_train)\ny_final = np.array(Y)\ntest_final = np.array(embedded_docs_test)\nprint(train_final.shape,y_final.shape,test_final.shape)","bb9f2558":"#training model\nmodel.fit(train_final,y_final,epochs=5,batch_size=100)","850bd8d3":"y_pred = model.predict_classes(test_final)","45133916":"final_sub = pd.DataFrame()\nfinal_sub['id']=test['id']\nfinal_sub['label'] = y_pred\nfinal_sub.to_csv('final_sub1.csv',index=False)\n\n#Score of 99.2 for the test data submission\n# Link to evaluate the score : https:\/\/www.kaggle.com\/c\/fake-news\/submit","8d3b08f7":"# Building an LSTM Model with an embedding layer #\nmodel1 = Sequential()\nmodel1.add(Embedding(voc_size,40,input_length=74))  # 40 : dimension of each vector, input_length : sentence length\nmodel1.add(Dropout(0.3))\nmodel1.add(Bidirectional(LSTM(100)))  # 100 memory cells \/ Neurons \nmodel1.add(Dropout(0.3))\nmodel1.add(Dense(64,activation='relu'))\nmodel1.add(Dropout(0.3))\nmodel1.add(Dense(1,activation='sigmoid'))  # sigmoid will help in classifying news as fake or real\nmodel1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model1.summary())","d6869d44":"# training model\nmodel1.fit(train_final,y_final,epochs=10,batch_size=100)","78c18f1c":"y_pred = model1.predict_classes(test_final)","2293065e":"final_sub = pd.DataFrame()\nfinal_sub['id']=test['id']\nfinal_sub['label'] = y_pred\nfinal_sub.to_csv('final_sub4.csv',index=False)\n\n#Score of 99.0 for the test data submission","36c2ecb3":"# Building the Bi-LSTM Model #","70084f8d":"## Steps to build the fake news classifier model #","eaeb75e0":"# Building the word embedding layer with help of keras embedding #","f59f4e31":"# Importing required libraries #","affebf1e":"# Exploratory Data Analysis on the data","3bcf3672":"# Hope you like this notebook.. Do upvote this notebook..\n# Happy Learning !","f9936380":"# Building the LSTM Model","f2165300":"# Reading the training and test dataset #","80ee5258":"# Text Preprocessing #\nSteps as below :\n\n1.Removing special characters and numbers\n\n2.Converting into lowercase to develop a consistent data \n\n3.Tokenize the text, ignoring stopwords and performing stemming on the words"}}