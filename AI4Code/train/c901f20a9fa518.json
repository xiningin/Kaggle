{"cell_type":{"aa488370":"code","bcf482a7":"code","fa52857e":"code","3b78880f":"code","2478195a":"code","72df0702":"code","a9a35c3d":"code","b8b65ddb":"code","7bb15e29":"code","5acb55f3":"code","e1fa1cc2":"code","63431277":"code","69b3260e":"code","2bc39b32":"code","03899307":"code","5d440055":"code","872cfb48":"code","65f844be":"code","6f0a605b":"code","504ff921":"code","f95335e1":"code","e97e3901":"code","57b5a04b":"code","0fdcc87d":"code","7160d9aa":"code","b82c3286":"code","c4aea312":"code","f0fb45c4":"code","b5f5b9bc":"code","679ec622":"code","671990b9":"code","176d630e":"code","9ab32d4f":"code","2cf64998":"code","ca57886e":"code","bc1609ab":"code","d9fb7ac0":"code","eb6cd7ee":"code","06c12283":"code","86757736":"markdown","e71cd042":"markdown","847a3e64":"markdown","32d027a3":"markdown","ee33cc23":"markdown","92238272":"markdown","a9a12e0b":"markdown","77ea8c74":"markdown","6fec56f1":"markdown","cdb5bda5":"markdown","6a70e329":"markdown","1410dc1c":"markdown","5f430b0c":"markdown"},"source":{"aa488370":"!cd ..\n!mkdir \/kaggle\/tmp\/\n%cd \/kaggle\/tmp\/","bcf482a7":"!pwd","fa52857e":"# to deal with file system\nimport os\n# for reading images\nimport cv2\n# to read and process xml files\nfrom bs4 import BeautifulSoup\n\n\n# for various operations (you know why these 3 are being used)\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n\n\n# preprocessing images\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.utils import to_categorical\n\n# for train and validation split\nfrom sklearn.model_selection import train_test_split\n\n\n\n# modelling with VGG19\nfrom tensorflow.keras.applications import  VGG19\nmodel_name = 'vgg_19'\nfrom tensorflow.keras.applications.vgg19  import preprocess_input as vgg_preprocess_input\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.layers import MaxPooling2D, AveragePooling2D, Dropout, BatchNormalization, Flatten, Dense, Input\nfrom tensorflow.keras.optimizers import Adam\n\n\n# for callbacks\nimport time\nfrom tensorflow.keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\nfrom datetime import datetime\n\n\n# model evaluation\nfrom tensorflow.keras.metrics import Recall, Precision\nfrom tensorflow_addons.metrics import F1Score\nfrom sklearn.metrics import classification_report","3b78880f":"# setting some parameters\nimage_size = 224 # this is the size that gave me better results than default size of 224\nvalidation_split_size = 0.20 # 20% will be used for validation\n\n# define the hyperparamets for training the neural network\nbatch_size = 32\ninit_lr = 0.0_001\nnum_epochs = 100\n\n# directories\nlabels_path = '..\/input\/face-mask-detection\/annotations\/'\nimages_path = '..\/input\/face-mask-detection\/images\/'\n\n!mkdir '.\/VGG19'\nsave_vgg19 = '.\/VGG19\/'","2478195a":"images = sorted(os.listdir(\"..\/input\/face-mask-detection\/images\/\"))\nlabels = sorted(os.listdir(\"..\/input\/face-mask-detection\/annotations\/\"))\n\n\n\nlen(images) == len(labels), len(images), len(labels)","72df0702":"def generate_label_dictionary(xml_loc): \n    \"\"\"\n    takes location to image and xml files on file system and return the image as numpy array and extracted bounding boxes\n    \"\"\"\n    with open(xml_loc) as xml_file:\n        # read the input file\n        soup = BeautifulSoup(xml_file.read(), 'xml')\n        objects = soup.find_all('object')\n\n        # extract the number of persons in an image\n        num_persons = len(objects)\n\n        # to store all the points for boundary boxes and target labels\n        boxes = []\n        labels = []\n        # doing it now\n        for obj in objects:\n            # extract output class and append it to 'boxes' list\n            if obj.find('name').text == \"without_mask\":\n                labels.append(0)\n            elif obj.find('name').text == \"mask_weared_incorrect\":\n                labels.append(1)\n            elif obj.find('name').text == \"with_mask\":\n                labels.append(2)\n            else:\n                break\n            \n            # extract bounding box and append it to 'labels' list\n            xmin = int(obj.find('xmin').text)\n            ymin = int(obj.find('ymin').text)\n            xmax = int(obj.find('xmax').text)\n            ymax = int(obj.find('ymax').text)\n            boxes.append([xmin, ymin, xmax, ymax])\n        \n\n        # converting them to numpy arrays\n        boxes = np.array(boxes)\n        labels = np.array(labels)\n\n        # save them to dictionary\n        target = {}\n        target[\"labels\"] = labels\n        target[\"boxes\"] = boxes\n\n        return target, num_persons","a9a35c3d":"targets=[] # store coordinates of bounding boxes\nnum_persons=[] # stores number of faces in each image\n\n#run the loop for number of images we have\nfor label_path in labels:\n    # generate label\n    target_image, num_persons_image = generate_label_dictionary(labels_path+label_path)\n    targets.append(target_image)\n    num_persons.append(num_persons_image)","b8b65ddb":"print(len(targets))\nprint()\nprint(targets[0: 100: 10])\nprint()\nprint(num_persons[0: 100: 10])","7bb15e29":"face_images = []\nface_labels = []\n\n# read each image from the file system and extract only the faces using the boundaries extracted in previous step\nfor i, image_path in enumerate(images):\n    image_read = cv2.imread(images_path+image_path, cv2.IMREAD_COLOR)\n    # get co-ordinates of the image\n    for j in range(0, num_persons[i]):\n        # get the locations of boundary box now\n        face_locs = targets[i]['boxes'][j]\n        # extract the face now using those co-ordinates\n        temp_face = image_read[face_locs[1]:face_locs[3], face_locs[0]:face_locs[2]]\n        temp_face = cv2.resize(temp_face, (image_size, image_size))\n        temp_face = vgg_preprocess_input(temp_face)\n        \n        # store this processed image to list now\n        face_images.append(temp_face)\n        # store it's respective label too\n        face_labels.append(targets[i]['labels'][j])\n\n# convert them to numpy arrays\nface_images = np.array(face_images, dtype=np.float32)\nface_labels = np.array(face_labels)\nprint(face_images.shape, face_labels.shape)","5acb55f3":"np.unique(face_labels, return_counts=True)","e1fa1cc2":"def show_face_and_label(index):\n    plt.imshow(face_images[index])\n    plt.show()\n\n    face_label_num = face_labels[index]\n\n    if face_label_num == 0:\n        face_label_text = \"doesn't have a mask on.\"\n    elif face_label_num == 1:\n        face_label_text = \"wore mask improperly.\"\n    elif face_label_num == 2:\n        face_label_text = \"has a mask on.\"\n    else:\n        face_label_text = \"error\"\n    return 'person {}'.format(face_label_text)","63431277":"show_face_and_label(2)","69b3260e":"show_face_and_label(46)","2bc39b32":"show_face_and_label(47)","03899307":"# since one-hot encoding need to be done for \nface_labels_enc = to_categorical(face_labels)\nface_labels_enc","5d440055":"pd.DataFrame(face_labels_enc).apply(pd.Series.value_counts, normalize=False).to_dict()","872cfb48":"pd.DataFrame(face_labels_enc).apply(pd.Series.value_counts, normalize=True).to_dict()","65f844be":"train_imgs, val_imgs, train_targets, val_targets = train_test_split(face_images, face_labels_enc,\n                                                                    stratify=face_labels_enc,\n                                                                    test_size=validation_split_size, random_state=100, shuffle=True)\n\ntrain_imgs.shape, val_imgs.shape, train_targets.shape, val_targets.shape","6f0a605b":"# ensuring that the samples are stratified between train and test splits to validate the model right way\nprint(pd.DataFrame(train_targets).apply(pd.Series.value_counts, normalize=True))\nprint()\nprint(pd.DataFrame(val_targets).apply(pd.Series.value_counts, normalize=True))","504ff921":"face_images, face_labels, face_labels_enc, face_locs, num_persons, targets, images, labels = None, None, None, None, None, None, None, None\ndel face_images, face_labels, face_labels_enc, face_locs, num_persons, targets, images, labels\n# RAM usage after this = ~3.7GB (reduction of more than 3.5 GB)","f95335e1":"train_image_generator = ImageDataGenerator(zoom_range=0.1, width_shift_range=0.1, height_shift_range=0.1,\n                                           shear_range=0.15,fill_mode=\"nearest\")","e97e3901":"vgg19_base = VGG19(include_top=False, pooling=None,\n                   input_shape=(image_size, image_size, 3)) # with max pooling (None, 2048)\n\n\ninner = vgg19_base.output\n\n\n## only the followeing layers will be trained or weights updated will only be of below layers\ninner = AveragePooling2D(pool_size=(7, 7))(inner)\ninner = Flatten()(inner)\ninner = Dense(units=256, activation='relu')(inner)\ninner = Dropout(rate=0.25)(inner)\ninner = Dense(units=3, activation='softmax')(inner)\n\n\nmodel_1 = Model(inputs=vgg19_base.input, outputs=inner)\n\n\nmodel_1.summary()","57b5a04b":"model_1.compile(loss = 'categorical_crossentropy',                             # \"multi log-loss\"  as loss\n                optimizer = Adam(lr=init_lr, decay=init_lr \/ num_epochs),      # \"adam\"            as optimiser\n                metrics = [Recall(name='recall'), 'accuracy',\n                           F1Score(average='macro', name='macro_f1', num_classes=3), # weighted_f1,\n                           F1Score(average='weighted', name='weighted_f1', num_classes=3),\n                           Precision(name='precision')])","0fdcc87d":"model_save_cb = ModelCheckpoint(filepath= save_vgg19+model_name+'-epoch{epoch:03d}-recall-{val_recall:.5f}-acc-{val_accuracy:.5f}.h5',\n                                monitor='val_recall', mode='max', \n                                verbose=1, save_best_only=False, save_weights_only=True)\n# I will be storing the complete model to be able to resume training should something happens and also to load the model with best fbeta score on validation set for evaluation\n\n\n# since recall is my primary metric of choice, i want the training to be stopped, when recall doesn't increase even after 15 epochs.\nearly_stop_cb = EarlyStopping(monitor='val_recall', min_delta=0, patience=15, verbose=1, mode='max')","7160d9aa":"history_vgg19 = model_1.fit(train_image_generator.flow(x=train_imgs, y=train_targets, batch_size=batch_size, seed=100),\n                            steps_per_epoch=len(train_imgs) \/\/ batch_size,\n                            \n                            validation_data = (val_imgs, val_targets),\n                            validation_steps=len(val_imgs) \/\/ batch_size,\n                            \n                            epochs=num_epochs,\n                            \n                            class_weight={0:5, 1:13, 2:1}, # got these weights after a lot of eperimenting\n                            \n                            callbacks=[model_save_cb, early_stop_cb],\n                            \n                            verbose=2\n                            )","b82c3286":"# printing all the maximum scores\nmax(history_vgg19.history['val_recall']), max(history_vgg19.history['val_macro_f1']), max(history_vgg19.history['val_weighted_f1']), max(history_vgg19.history['val_accuracy'])","c4aea312":"train_stats = pd.DataFrame(history_vgg19.history)\n\n# looking at the epochs that had best recall and macro-f1 scores for validaiton set\ntrain_stats.sort_values(by=['val_recall'], inplace=False, ascending=False).head()","f0fb45c4":"train_stats.plot(y=['val_recall', 'recall'], kind=\"line\")","b5f5b9bc":"train_stats.plot(y=['val_macro_f1', 'macro_f1'], kind=\"line\")","679ec622":"train_stats.plot(y=['val_weighted_f1', 'weighted_f1'], kind=\"line\")","671990b9":"train_stats.plot(y=['val_accuracy', 'accuracy'], kind=\"line\")","176d630e":"train_stats.plot(y=['val_loss', 'loss'], kind=\"line\")","9ab32d4f":"train_stats.plot(y=['val_precision', 'precision'], kind=\"line\")","2cf64998":"very_good_epochs = []\nfor col in ['val_recall', 'val_accuracy','val_macro_f1', 'val_precision', 'val_weighted_f1']:\n    epoch = train_stats.loc[:,col].argmax()\n    very_good_epochs.append(epoch)\n    print(train_stats.loc[epoch, ['val_recall', 'val_accuracy','val_macro_f1', 'val_weighted_f1']])\n    print()","ca57886e":"# looking at all the rows with highest results for respective metric\ngood_results = train_stats.loc[set(very_good_epochs),\n                               ['val_recall', 'val_accuracy','val_macro_f1', 'val_weighted_f1', 'val_precision']]\n\n# since recall is my primary metric\ngood_results.sort_values(by=['val_recall', 'val_accuracy', 'val_macro_f1'], ascending=False, inplace=True)\ngood_results","bc1609ab":"models_not_to_delete = []\nfor epoch in list(np.array(good_results.index)):\n    good_vals = good_results.loc[epoch, ['val_recall', 'val_accuracy']].values\n    best_model_loc = f'{save_vgg19}vgg_19-epoch{epoch+1:03d}-recall-{good_vals[0]:.5f}-acc-{good_vals[1]:.5f}.h5'\n    print(best_model_loc)\n    models_not_to_delete.append(best_model_loc)\n    model_2 = None\n    del model_2\n    model_2 = None\n    try:\n        model_2 = Model(inputs=vgg19_base.input, outputs=inner)\n        model_2.load_weights(filepath=best_model_loc)\n        val_preds = model_2.predict(val_imgs, batch_size=32)\n        val_preds = np.argmax(val_preds, axis=1)\n        print(classification_report(y_true=val_targets.argmax(axis=1), y_pred=val_preds, target_names=['without mask', 'incorrectly worn', 'with mask']))\n    except OSError:\n        print('file not found')","d9fb7ac0":"models_not_to_delete","eb6cd7ee":"!cp .\/VGG19\/vgg_19-epoch039-recall-0.95706-acc-0.95828.h5 ..\/working\/","06c12283":"!cp .\/VGG19\/vgg_19-epoch046-recall-0.95337-acc-0.95460.h5 ..\/working\/","86757736":"In key `labels`, information about presence or absence of mask is being stored, while the key `boxes` tell you where(where meaning location to the face) to locate in the image.","e71cd042":"### processing xml files","847a3e64":"# Modelling\n","32d027a3":"### making train and test splits","ee33cc23":"## read locations to images and xml files","92238272":"### defining and training model","a9a12e0b":"Summary:\n1. Individual faces has been extracted using the boundaries provided in annotation files and model has trained on those faces.\n2. If you've been wondering how an image with face(s) could be used to predict the presence of mask; faces have to be extracted from the images first and then fed to model. There are many ways to accomplish this; but here are [some trusted and open source algorithms](https:\/\/github.com\/opencv\/opencv\/tree\/master\/data\/haarcascades) by people at openCV.\n3. After having extracted the boundaries of faces from images, each face with or without a mask on can then be fed to these models for prediction. check my github repo to learn how this can be done and see the results for yourself.","77ea8c74":"Training and validation:\n1. My primary evaluation metric is 'Recall', which tells the proportion of positive examples that have been predicted correctly. But, I have also used and printed results of other metrics(Accuracy, Macro averaged F1 Score, Weighted F1 score and Precision) for you to be able to assess this.\n2. VGG19 with pretrained 'imagenet' weights with some layers added at the end has been adopted.\n3. The lack of many examples for 'improperly worn mask', despite being trained with additional weight for this class and image augmentation(so model will get to see images from different angles and of various contrast levels;), final model still doesn't perform acceptable enough on this images(of myself from my webcam) I fed to the model. Or this may even be a case of test data coming from a distribution foreign to data the model was trained on.","6fec56f1":"### making image generator\nnow to ensure that the model have seen multiple variations of each type of image class, I am using `ImageDataGenerator` for augmentation.","cdb5bda5":"# Pre-Processing","6a70e329":"### model stats","1410dc1c":"### processing images","5f430b0c":"<h2>Check <a href='https:\/\/github.com\/naveen-9697\/Face-Mask-Identification'>my github repository<\/a> for more information.<\/h2>"}}