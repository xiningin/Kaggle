{"cell_type":{"2ce3aae9":"code","e6df91ee":"code","d39d8d1c":"code","d828edab":"code","8018a31f":"code","af729f08":"code","98f7914b":"code","fcd0226d":"code","eee85ed6":"code","b81b1194":"code","43814fd8":"code","5c3a509d":"markdown","55d6ce17":"markdown","d802e48e":"markdown","d71ee41a":"markdown","16f05fbe":"markdown","b2c219f2":"markdown","1ae39e5c":"markdown","8f104aea":"markdown","64fcbfdc":"markdown","8ac36440":"markdown","a4cd76ce":"markdown","12ea8518":"markdown"},"source":{"2ce3aae9":"import cv2\nimport numpy as np\nimport tensorflow as tf\nimport keras\nfrom keras import layers\nimport matplotlib.pyplot as plt","e6df91ee":"SIZE = 512\n\nimage1= cv2.imread('..\/input\/human-faces\/Humans\/1 (10).jpeg')\nimage2= cv2.imread('..\/input\/human-faces\/Humans\/1 (1009).jpg')\n\nimage1= cv2.resize(image1,(SIZE,SIZE))\nimage1 = cv2.cvtColor(image1,cv2.COLOR_BGR2RGB)\nimage2= cv2.resize(image2,(SIZE,SIZE))\nimage2 = cv2.cvtColor(image2,cv2.COLOR_BGR2RGB)\n","d39d8d1c":"plt.subplot(1,2,1)\nplt.imshow(image1)\nplt.subplot(1,2,2)\nplt.imshow(image2)","d828edab":"model = tf.keras.models.Sequential()\nmodel.add(layers.Conv2D(filters = 64, kernel_size = (3,3), strides = 1, padding = 'same', input_shape = (SIZE,SIZE,3)))\nmodel.add(layers.MaxPool2D(pool_size = (3,3)))\nmodel.add(layers.Conv2D(filters = 128,kernel_size = (3,3), strides = 1, padding = 'same'))\nmodel.add(layers.MaxPool2D(pool_size = (3,3)))\nmodel.add(layers.Conv2D(filters = 256, kernel_size = (3,3), strides = 1, padding = 'same'))\nmodel.add(layers.MaxPool2D(pool_size = (3,3)))\nmodel.add(layers.Conv2D(filters = 512, kernel_size = (3,3), strides = 1, padding = 'same'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(1,activation = 'sigmoid'))","8018a31f":"model.summary()","af729f08":"def ShowImage(Image, layer, MyModel):\n    img = tf.keras.preprocessing.image.img_to_array(Image)\n    img = np.expand_dims(img, 0)\n    ### preprocessing for img for vgg16\n    img = tf.keras.applications.vgg16.preprocess_input(img)\n    \n    ## Now lets define a model which will help us\n    ## see what vgg16 sees \n    inputs = MyModel.inputs\n    outputs = MyModel.layers[layer].output\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    model.summary()\n    \n    ## let make predictions to see what the Cnn sees\n    featureMaps = model.predict(img)\n    \n    ## Plotting Features\n    for maps in featureMaps:\n        plt.figure(figsize=(20,20))\n        pltNum = 1\n        \n        for a in range(8):\n            for b in range(8):\n                plt.subplot(8, 8, pltNum)\n                plt.imshow(maps[: ,: ,pltNum - 1], cmap='gray')\n                pltNum += 1\n        \n        plt.show()","98f7914b":"ShowImage(image1, 0, model)","fcd0226d":"ShowImage(image2, 0, model)","eee85ed6":"ShowImage(image1,4,model)","b81b1194":"ShowImage(image1, 6, model)","43814fd8":"ShowImage(image2,6, model)","5c3a509d":"## Let's start coding part","55d6ce17":"# Ploting images","d802e48e":"## Visualize data","d71ee41a":"## Loading data","16f05fbe":"## Pooling layers\n\nConvolutional layers are followed by pooling layers which are generally used for downsampling ie reducing dimension of convolutional output.Furthermore, it is useful for extracting dominant features which are rotational and positional invariant, thus maintaining the process of effectively training of the model.\n\n<img src = 'https:\/\/miro.medium.com\/max\/396\/1*uoWYsCV5vBU8SHFPAPao-w.gif'>\n\n\nThere are two types of Pooling: Max Pooling and Average Pooling. Max Pooling returns the maximum value from the portion of the image covered by the Kernel. On the other hand, Average Pooling returns the average of all the values from the portion of the image covered by the Kernel.\n\n\n<img src = 'https:\/\/miro.medium.com\/max\/500\/1*KQIEqhxzICU7thjaQBfPBQ.png'>\n\n\nPooling layers are followed by anoter convolution layers and is finally flattened is fed to feed-forward networks for classification purpose.","b2c219f2":"## Defining model","1ae39e5c":"# Thank You","8f104aea":"# Introduction\nThe most fascinating things about deep learning is capacity of deep learning models to classify images.Recent deep learning models can detect different objects from videos and images, all this has been possible due to rapid progress in field of computer vision.Convolutional Neural Networks(ConvNets) have been major breakthrough in the field of deep learning and computer vision.\n<img src = 'https:\/\/www.researchgate.net\/publication\/326152216\/figure\/fig2\/AS:644307941879809@1530626392505\/Deep-convolutional-neural-network-DCNN-architecture-A-schematic-diagram-of-AlexNet.png'><br>\n\nThe name \u201cconvolutional neural network\u201d indicates that the network employs a mathematical operation called <b>convolution<\/b>. Convolutional networks are a specialized type of neural networks that use convolution in place of general matrix multiplication in at least one of their layers.A convolutional neural network consists of an input layer, hidden layers and an output layer. \n\n\n","64fcbfdc":"## Defining function to plot images","8ac36440":"## Convolutional Layers\n\n\nIn a CNN, the input is a tensor with shape (number of images) x (image height) x (image width) x (input channels). After passing through a convolutional layer, the image becomes abstracted to a feature map, with shape (number of images) x (feature map height) x (feature map width) x (feature map channels). A convolutional layer within a neural network should have the following attributes:\n\n   1.Convolutional filters\/kernels defined by a width and height (hyper-parameters). The value for filters are learned during training.\n   \n   2.Covolutional filters number is also hyperparamer.\n   \n   3.The hyperparameters of the convolution operation, like padding size and stride.Stride refers to number of cell to skip during convolution process. padding is done inorder to preserve features specially at corners.\n   \n   \n   ### Understanding padding\n<i>When we augment the mXmX1 image into a (m+1)*(m+1)*1 image and then apply the 3x3x1 kernel over it,  the convolved matrix will be of dimensions mXmX1, so it is called Same Padding.\n\nOn the other hand, if we perform the same operation without padding,it is called  Valid Padding.<\/i>\n   \n   The dimension of image after convolution can be calculated by using this formula \n   ### [(W\u2212K+2P)\/S]+1.\n\n    W is the input size (ie. height * width)\n    K is the Kernel size \n    P is the padding (padding can be same or valid)\n    S is the stride \n\n\nIn the figure below: Input size is 5X5, kernel size is 3X3, there is not padding and stride is 1 so, output dimension is 3X3\n  \n   \n   \n #### Convolution process on image having channel one by 3X3 filter is show below:\n <img src = 'https:\/\/miro.medium.com\/max\/500\/1*GcI7G-JLAQiEoCON7xFbhg.gif'>","a4cd76ce":"## Import necessary libraries","12ea8518":"ref:: <a href=\"https:\/\/towardsdatascience.com\/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53\">here <\/a><br>\nref:: <a href = \"https:\/\/www.kaggle.com\/vanvalkenberg\/what-a-cnn-sees-plotting-feature-maps-of-cnn\"> here <\/a>"}}