{"cell_type":{"1440acdb":"code","6cf781cf":"code","3d838fcb":"code","277666b1":"code","1250988d":"code","597d4892":"code","bff097b6":"code","8ce0a68e":"code","6cca9797":"code","4cccd196":"code","1a6e0edf":"code","8fd61169":"code","4558c983":"code","e7416d8b":"code","a02f8c48":"code","a7cdd268":"code","b12674cd":"code","69741049":"code","2ab3f345":"code","0dd43334":"code","3d128f64":"code","0995e0a9":"markdown","5042c2b3":"markdown","fc36f146":"markdown","35d21947":"markdown","b8ea8592":"markdown","59241cc2":"markdown","ebe2428e":"markdown","f5a9d029":"markdown","916febf6":"markdown","7bb3f3c3":"markdown","3ceb6733":"markdown","41ebd56f":"markdown","4f8a4716":"markdown","94118e14":"markdown","0587db84":"markdown","73d4c700":"markdown","6a9a699f":"markdown","0adb2062":"markdown","59998dc2":"markdown"},"source":{"1440acdb":"# as usual, let us load all the necessary libraries\nimport numpy as np  # numerical computation with arrays\nimport pandas as pd # library to manipulate datasets using dataframes\nimport scipy as sp  # statistical library\n\n\n# below sklearn libraries for different models\nfrom sklearn.tree import DecisionTreeClassifier as DecisionTree\nfrom sklearn.ensemble import RandomForestClassifier as RandomForest\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.impute import KNNImputer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\n\n#import libraries for implementing neural networks\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import SGD, Adam\nfrom keras.regularizers import l2\n\n# plot \nimport matplotlib\nimport matplotlib.pyplot as plt\n%matplotlib inline","6cf781cf":"#Additional Libraries\n\nimport seaborn as sns\nfrom scipy.stats import zscore\nfrom sklearn.svm import SVR\nfrom sklearn.svm import LinearSVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.model_selection import train_test_split","3d838fcb":"# Read training data\ntrain_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/onefishy\/Rwanda-course-2020\/master\/Competition_data\/train.csv',na_values='?') # read in the data as a DataFrame\ntrain_data.head(3) # show the first 3 rows of the dataset","277666b1":"# Read test data \ntest_data = pd.read_csv('https:\/\/raw.githubusercontent.com\/onefishy\/Rwanda-course-2020\/master\/Competition_data\/test.csv') # read in the data as a DataFrame\ntest_data.head(3) # show the first 3 rows of the dataset","1250988d":"# Start building a model here","597d4892":"train_data['weight']=train_data['weight'].abs()#Change negative values in the weight column\ntrain_data['horsepower'] = pd.to_numeric(train_data['horsepower'], errors='coerce')#Chnage the horsepower column to numerical from object\ntrain_data=train_data.drop(['car name'],axis=1)#Drop the car name column in train data since we don't need it for prediction due to its Uniqueness\ntest_data=test_data.drop(['car name'],axis=1)#Drop the car name column in test data since we don't need it for prediction due to its Uniqueness","bff097b6":"train_data.isna().sum() #check for null values in the dataset","8ce0a68e":"#Select column with null values\nnull_data = train_data[train_data.isnull().any(axis=1)]#check for null values in the training set and assign to local variable\ntrain_data2 = train_data.dropna()#select all non-null columns by dropping the null values\n\nx_imp = train_data2.drop([\"horsepower\"], axis = 1)#We have seen the horsepower column has null values and we have to split for further learning\ny_imp = train_data2.horsepower #Select horsepower as label to be predicted \n\nX_train, X_test, Y_train, Y_test = train_test_split(x_imp,y_imp, test_size  = 0.3,random_state = 42)#appply split on non-null and null column\n\nlr = LinearRegression()#instantiate liner regression model\nlr.fit(X_train,Y_train)#fit the training date to our model\n\nnull_data2 = null_data.copy()#copy the null data to null_data2 for saving the original\nnull_data2 = null_data2.drop(\"horsepower\", axis = 1)##copy the horsepower data to null_data2 for saving the original\n\n#start loop iteration for each null cell and append the prediction to null cells\npredictions = []\nfor i in range(null_data2.shape[0]):\n    predictions.append(null_data2.iloc[i,:])\n\n#append all null values to the original data\nvalues = []\nfor i in range(len(predictions)):\n    for j in range(null_data2.shape[1]):\n        values.append(predictions[i][j])\n\n#start looping fora each cell with null value\n#instantaite some local variables for the loop      \ni = 0\nj = null_data2.shape[1]\nlr_predictions =[]\n\nfor a in range(0,null_data2.shape[0]):\n    print(\"Prediction {}\".format(a+1))\n    print(lr.predict((np.array([values[i:j]]))))\n    lr_predictions.append(lr.predict((np.array([values[i:j]])))[0])\n    print(\"---------------\")\n    i = i+(int(len(values) \/ len(predictions)))\n    j = j+(int(len(values) \/ len(predictions)))\n\nnull_index = train_data[train_data[\"horsepower\"].isna()].index\n\n#Append the predicted null values to our original dataset\nfor i in range(len(null_index)):\n    train_data[\"horsepower\"][null_index[i]] = lr_predictions[i]\n\n#print the number of missing values after imputation\nprint(\"Missing Values: {}\".format(train_data.isnull().sum().sum()))","6cca9797":"low = .05 #lower quntile\nhigh = .95 #Upper quantile \n\n# Step 1: compute 5% percentile and the 95% percentile of each column in the dataset\nquantile_df = train_data.quantile([low, high])\n\n# Step 2: perform outlier removal fornumerical columns\n# COMPLETE\nfeatures=['displacement', 'horsepower', 'weight', 'acceleration','fuel (L\/100km)']\n          \nfor i in features:\n  train_data_rm = train_data[(train_data[i] > quantile_df.loc[0.05, i]) & (train_data[i] < quantile_df.loc[0.95, i])]\n  print('Number of rows after outlier removal: {}'.format(train_data_rm.shape[0]))\n   \ntrain_data=train_data_rm ","4cccd196":"# Divide the column 'x = training covariates and 'y=label as output\n\nx = train_data.drop(columns=['fuel (L\/100km)'])#remove output label \n\ny = train_data['fuel (L\/100km)']# assign output label for y","1a6e0edf":"#Encode all catagorical columns in the training set \nx[\"origin\"] = x[\"origin\"].astype(str)\nx[\"cylinders\"] = x[\"cylinders\"].astype(str)\nx = pd.get_dummies(x)\n\n\n#Encode all catagorical columns in the testing set \ntest_data[\"origin\"] = test_data[\"origin\"].astype(str) \ntest_data[\"cylinders\"] = test_data[\"cylinders\"].astype(str)\ntest_data = pd.get_dummies(test_data)\n\n#Add cylinders with value 5 in testing set with 0 values, since there is no cylinder value with 5\n#we have to apply the code to make our training and testing columns equlal size\ntest_data.insert(loc=7, column='cylinders_5', value = 0)","8fd61169":"#Split our data for training and testing \n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=0)\n","4558c983":"#Checking the shapes for training and testing of our datas\nprint(\"Shape of Training data is:\",x_train.shape)\nprint(\"Shape of Testing data is:\",x_test.shape)","e7416d8b":"#Support Vector Machine\n\n# Step 1: Instantiate the Model for Support Vector Regressor`\nsvm_regr = make_pipeline(StandardScaler(),SVR(C=8,epsilon=2e-1))\nsvm_regr.fit(x_train,y_train)\n  \n# Step 2: Predict label on training set\ny_train_pred = svm_regr.predict(x_train)\n# Step 3: Compute RMSE on training set \nprint('RMSE on Training Data on :', np.sqrt(mean_squared_error(y_train, y_train_pred)))\n# Step 4: Predict label on test set\ny_test_pred = (svm_regr.predict(x_test))\n# Step 5: Compute RMSE on test set \nprint('RMSE on Testing Data  :', np.sqrt(mean_squared_error(y_test, y_test_pred)),'\\n')\n\n# Save your predictions to a DataFrame\nprediction = svm_regr.predict(test_data)\nmy_submission = pd.DataFrame(prediction)\nmy_submission=my_submission.rename(columns={0: \"predictions\"})\nmy_submission.to_csv('my_submission_svm.csv',index=True,index_label='id')\n#files.download('my_submission_svm.csv')\n","a02f8c48":"# Step 1: Instantiate the Liner regression Model \nliner_model=LinearRegression()\nliner_model.fit(x,y)\n\n# Step 2: Predict label on training set\ny_train_pred = liner_model.predict(x_train)\n\n# Step 3: Compute RMSE on training set \nprint('RMSE on Training Data:', np.sqrt(mean_squared_error(y_train, y_train_pred)))\n\n# Step 4: Predict label on test set\ny_test_pred = liner_model.predict(x_test)\n\n# Step 5: Compute RMSE on test set \nprint('RMSE on Testing Data: ', np.sqrt(mean_squared_error(y_test, y_test_pred)))\n\n# Save your predictions to a DataFrame\nliner_pred=liner_model.predict(test_data)\nmy_submission = pd.DataFrame(liner_pred)\nmy_submission=my_submission.rename(columns={0: \"predictions\"})\nmy_submission.to_csv('my_submission_liner.csv',index=True,index_label='id')\n#files.download('my_submission_liner.csv')","a7cdd268":"# Step 1: Instantiate the Random Forest Model \nx, y = make_regression(n_features=8, n_informative=3,random_state=0, shuffle=False)\nrand_regr = RandomForestRegressor(max_depth=3, random_state=0)\nrand_regr.fit(x_train,y_train)\n\n# Step 2: Predict label on training set\ny_train_pred = rand_regr.predict(x_train)\n# Step 3: Compute RMSE on training set \nprint('RMSE on Training Data:', np.sqrt(mean_squared_error(y_train, y_train_pred)))\n\n# Step 4: Predict label on test set\ny_test_pred = (rand_regr.predict(x_test))\n# Step 5: Compute RMSE on test set \nprint('RMSE on Testing Data: ', np.sqrt(mean_squared_error(y_test, y_test_pred)))\n\n# Save your predictions to a DataFrame\nmy_submission = pd.DataFrame(rand_regr.predict(test_data))\nmy_submission=my_submission.rename(columns={0: \"predictions\"})\nmy_submission.to_csv('my_submission_random.csv',index=True,index_label='id')\n#files.download('my_submission_random.csv')","b12674cd":"# Step1: Instantiate Decision Tree Model\n\ndec_regr = DecisionTreeRegressor(max_depth=6)\ndec_regr.fit(x_train,y_train)\n\n# Step 2: Predict label on training set\ny_tran_pred = dec_regr.predict(x_train)\n# Step 3: Compute RMSE on training set \nprint('RMSE on Training Data:', np.sqrt(mean_squared_error(y_train, y_train_pred)))\n# Step 4: Predict label on test set\ny_test_pred = dec_regr.predict(x_test)\n# Step 5: Compute RMSE on test set \nprint('RMSE on Testing Data: ', np.sqrt(mean_squared_error(y_test, y_test_pred)))\n\n# Save your predictions to a DataFrame\nmy_submission = pd.DataFrame(dec_regr.predict(test_data))\nmy_submission=my_submission.rename(columns={0: \"predictions\"})\nmy_submission.to_csv('my_submission_decison.csv',index=True,index_label='id')\n#files.download('my_submission_decison.csv')","69741049":"# Here's a sample submission file\nsample_submission = pd.read_csv('https:\/\/raw.githubusercontent.com\/onefishy\/Rwanda-course-2020\/master\/Competition_data\/sampleSubmission.csv') # read in the data as a DataFrame\nsample_submission.head() # show the first 3 rows of the dataset","2ab3f345":"# Save your predictions to a DataFrame\nmy_submission = pd.DataFrame(svm_regr.predict(test_data))\nmy_submission=my_submission.rename(columns={0: \"predictions\"})\nmy_submission.to_csv('my_submission.csv',index=True,index_label='id')","0dd43334":"# Check that your submission looks the same as the samples submission\npd.read_csv('my_submission.csv')","3d128f64":"#files.download('my_submission.csv')","0995e0a9":"#Model 2\n># Liner Regression Model","5042c2b3":"# Model 4\n# Decision Tree Model\n","fc36f146":"## Imputing Null values with Liner Regression Model\n Since our data has liner pattern we will apply liner regression imputer to fill the null values by learning the non-null datapoints","35d21947":"# Once you have generated predictions on the holdout and test data, create a submission file\nYour submission file should follow the Kaggle submission template.\n","b8ea8592":"# Save and download the submission file and upload to the Kaggle website\n\nThen download `my_submission.csv` by running the following line and submit to the [Kaggle](https:\/\/www.kaggle.com\/t\/b9bc778c9e8842d28c5526f578e6c348) compeition website.","59241cc2":"## Data Cleaning","ebe2428e":"## Split the dataset","f5a9d029":"# Loading Competition Data\nRun all the steps below to obtain the data","916febf6":"#Remove Outliers","7bb3f3c3":"### Assign X and Y","3ceb6733":"---\n\n\n## From all the given Models we have selected Support Vector Regression for final result\n\n---\n\n","41ebd56f":"#Model 1\n># Support Vector Regression (SVR)","4f8a4716":"# Towards Reducing Pollution in Kigali\n### Predict Vehicles' consumption in liters\/100km for each car in the city","94118e14":"## Check for Null values","0587db84":"\n\n---\n\n\n#MODELING\n\n---\n\n","73d4c700":"Introduction\n\nPollution is a big issue in the city of Kigali. Policy makers in the city want to take action and deploy some measures to address this problem. You have been hired as a machine learning expert to analyze some data and help them make good decisions.\n\nCars that consume more fuel pollute more. As a first step, we want to estimate how much fuel each individual car consumes every 100 km. The provided dataset concerns city-cycle fuel consumption in liters per 100 kilometers (target).\n\nThe aim of this homework is to help you apply the skills that you have learned so far to a real dataset. This involves learning what data means, how to handle and visualize data, training, cross validation, prediction, testing your model, etc.\n\nDescription of covariates\nThis dataset has 3 multi-valued discrete and 5 continuous covariates\n\n\n    1. cylinders:     multi-valued discrete\n    2. displacement:  continuous\n    3. horsepower:    continuous\n    4. weight:        continuous\n    5. acceleration:  continuous\n    6. model year:    multi-valued discrete\n    7. origin:        multi-valued discrete\n    8. car name:      string (unique for each instance)","6a9a699f":"#Model 3\n> # Random Forest","0adb2062":"# One-hot Encoding","59998dc2":"# Now we are done with downloading data! \n* Try building a model inside this notebook by create additional cells below with code to specify and fit the model\n* If you are fitting large neural nets, make sure this google colab notebook is running on GPUs\n* Check Edit --> Notebook settings --> Hardware accelerator: GPU"}}