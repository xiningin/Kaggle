{"cell_type":{"c90368f6":"code","c165c0e5":"code","98c41cbd":"code","d88a24f4":"code","48c85558":"code","a3122de2":"code","82a62889":"code","5f076be3":"code","bd889908":"code","e6046be1":"code","86ffe4a4":"code","5065e8cc":"code","37d8b83c":"code","4614255b":"code","1df8d8cd":"code","cbab7274":"code","ecdd29b1":"code","a3fa276d":"code","3130de47":"code","585bd565":"code","cd8a953f":"code","38775f49":"code","4ed6716d":"code","d58c584b":"code","1d27e479":"code","c8853691":"code","4df2dbce":"code","9cc47c9b":"code","599429e8":"code","a550c0c1":"code","033fcc97":"markdown","72c6b366":"markdown","8b451961":"markdown","2f3b0dbd":"markdown","c121205b":"markdown","0266b0e0":"markdown","3e3a1c71":"markdown","3656c627":"markdown","0ccebf6e":"markdown","b7718f87":"markdown","26519544":"markdown","2e59ec62":"markdown","0c2d144e":"markdown"},"source":{"c90368f6":"from fastai import *\nfrom fastai.vision import *","c165c0e5":"classes = ['peppa','others']","98c41cbd":"folder = 'peppa'\nfile = 'urls_peppa.txt'","d88a24f4":"path = Path('..\/working\/')\ndest = path\/folder\ndest.mkdir(parents=True, exist_ok=True)","48c85558":"# resetting the path\n!cp ..\/input\/* {path}\/","a3122de2":"download_images(path\/file, dest, max_pics=200)","82a62889":"folder = 'others'\nfile = 'urls_others.txt'","5f076be3":"path = Path('..\/working\/')\ndest = path\/folder\ndest.mkdir(parents=True, exist_ok=True)","bd889908":"!cp ..\/input\/* {path}\/","e6046be1":"download_images(path\/file, dest, max_pics=200)","86ffe4a4":"for c in classes:\n    print(c)\n    verify_images(path\/c, delete=True, max_size=500)","5065e8cc":"# creating a data bunch\n# training set is in the current path\n# since we don't have a validation set we set valid_pct = 0.2\n# to use 20% data as validation\nnp.random.seed(42)\ndata = ImageDataBunch.from_folder(\".\", train=\".\", valid_pct=0.2,\n        ds_tfms=get_transforms(), size=224, num_workers=0).normalize(imagenet_stats)","37d8b83c":"# verifying the classes\ndata.classes","4614255b":"# viewing some of the images\ndata.show_batch(rows=3, figsize=(8,7))","1df8d8cd":"# creating a resnet34\nlearn = create_cnn(data, models.resnet34, metrics=error_rate)","cbab7274":"learn.fit_one_cycle(4)","ecdd29b1":"# saving the weights so we don't have to retrain it each time\nlearn.save('stage-1')","a3fa276d":"learn.unfreeze()","3130de47":"learn.lr_find()","585bd565":"learn.recorder.plot()","cd8a953f":"learn.fit_one_cycle(1, max_lr=slice(1e-5,1e-3))","38775f49":"learn.save('stage-2')","4ed6716d":"interp = ClassificationInterpretation.from_learner(learn)","d58c584b":"# we see that our model is 100% accurate\ninterp.plot_confusion_matrix()","1d27e479":"# from fastai.widgets import *","c8853691":"# ds, idxs = DatasetFormatter().from_toplosses(learn, n_imgs=100)","4df2dbce":"# ImageCleaner(ds, idxs, path)","9cc47c9b":"# ds, idxs = DatasetFormatter().from_similars(learn)","599429e8":"# opening a random image and making a prediction\nimg = open_image(path\/'peppa\/00000041.jpg')\nimg","a550c0c1":"pred_class,pred_idx,outputs = learn.predict(img)\nprint(pred_class)","033fcc97":"**selecting a slice**\n\nWe select the slice with the maximum downward slope (which is not a bump)","72c6b366":"\n## Creating your own dataset from Google Images\n\nby: Francisco Ingham and Jeremy Howard. Inspired by [Adrian Rosebrock](https:\/\/www.pyimagesearch.com\/2017\/12\/04\/how-to-create-a-deep-learning-dataset-using-google-images\/)\n","8b451961":"First we need to get the file paths from our top_losses. We can do this with <code>.from_toplosses<\/code> We then feed the top losses indexes and corresponding dataset to <code>ImageCleaner<\/code>.\n\nNotice that the widget will not delete images directly from disk but it will create a new csv file <code>cleaned.csv<\/code> from where you can create a new ImageDataBunch with the corrected labels to continue training your model.\n\nNote: Please Set the Number of images to a number that you'd like to view: ex:  <code>n_imgs=100<\/code>","2f3b0dbd":"# Peppa pig soft toy classifier using custom data set from google images","c121205b":"** Import the libraries**","0266b0e0":"**Making directories for the classes and downloading images**","3e3a1c71":"## Get a list of URLs\n\n** Search and scroll**\n\nGo to Google Images and search for the images you are interested in. The more specific you are in your Google Search, the better the results and the less manual pruning you will have to do.\n\nScroll down until you've seen all the images you want to download, or until you see a button that says 'Show more results'. All the images you scrolled past are now available to download. To get more, click on the button, and continue scrolling. The maximum number of images Google Images shows is 700.\n\nIt is a good idea to put things you want to exclude into the search query, for instance if you are searching for the Eurasian wolf, \"canis lupus lupus\", it might be a good idea to exclude other variants:\n\n\"canis lupus lupus\" -dog -arctos -familiaris -baileyi -occidentalis\n\nYou can also limit your results to show only photos by clicking on Tools and selecting Photos from the Type dropdown.\n\n**Download into file**\n\nNow you must run some Javascript code in your browser which will save the URLs of all the images you want for you dataset.\n\nPress Ctrl + Shift + J in Windows\/Linux and CmdOptJ in Mac, and a small window the javascript 'Console' will appear. That is where you will paste the JavaScript commands.\n\nYou will need to get the urls of each of the images. You can do this by running the following commands:\n\n<pre>\nurls = Array.from(document.querySelectorAll('.rg_di .rg_meta')).map(el=>JSON.parse(el.textContent).ou);window.open('data:text\/csv;charset=utf-8,' + escape(urls.join('\\n')));\n<\/pre>\n\n\n\nI have downloaded the urls locally, saved them in text files and uploaded them on Kaggle!","3656c627":"## Cleaning up\n\nSome of the losses \/ missclassifications might not be due to the performance of the model but because of the noise in the data. Hence, it is a good practice to clean the data. In this case, we have achieved 100% accurarcy but that won't be the case in every application. Uncomment the following 4 code cells to delete noisy images","0ccebf6e":"** define the classes**","b7718f87":"**deleting images that cannot be used**","26519544":"**Creating folders for the urls and downloading images**\n\nNote: Since input is a read-only directory we store the images in the working directory.","2e59ec62":"## Learning rate\n\nFinding and plotting the learning rate","0c2d144e":"\n\nFlag photos for deletion by clicking 'Delete'. Then click 'Next Batch' to delete flagged photos and keep the rest in that row. ImageCleaner will show you a new row of images until there are no more to show. In this case, the widget will show you images until there are none left from top_losses.ImageCleaner(ds, idxs)\n\nYou can also find duplicates in your dataset and delete them! To do this, you need to run .from_similars to get the potential duplicates' ids and then run ImageCleaner with duplicates=True. The API works in a similar way as with misclassified images: just choose the ones you want to delete and click 'Next Batch' until there are no more images left.\n"}}