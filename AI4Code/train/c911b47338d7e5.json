{"cell_type":{"6a19f63b":"code","b58c9bd0":"code","8e49bae5":"code","6b8501f7":"code","9dd63f12":"code","175d429d":"code","bcc5c72b":"code","423dd257":"code","86343f80":"code","d63684f6":"code","db30604a":"code","32946699":"code","b008aec0":"code","44bbe8e8":"code","1cd6a4be":"code","b153c2ef":"code","4067bbc9":"code","eeec6065":"code","ddf96f1b":"code","3200363f":"code","e9697f5b":"code","c078bb14":"code","115332c7":"code","6c3995bb":"code","c283c92e":"code","4d2a8a43":"code","eb05da52":"code","cbcff37b":"code","51000703":"code","a1243f1c":"code","e8c77c8d":"code","fe370680":"code","675aea79":"code","88689e16":"code","3d2d88f0":"code","699b4c9b":"code","a3ce4250":"code","0415bda3":"code","a2938568":"code","9d130512":"code","0aa4a594":"code","e7c00159":"code","42d0a98b":"code","630a09cd":"code","9bcfc103":"code","e056fd87":"code","d6f626d6":"code","e628833c":"code","fd39a2d4":"code","e1916ca0":"code","ff602209":"code","ab2caf30":"code","050de0e4":"code","079e61a1":"code","c28b2545":"code","a8b6097f":"code","0d0a1252":"code","c39baaeb":"code","8a66114c":"code","30235132":"markdown","97b44f2b":"markdown","776424a1":"markdown","b892cf8c":"markdown","bb4c6513":"markdown","0b04cb76":"markdown","543c575f":"markdown","70e0ef9f":"markdown","768bc07e":"markdown","0d834fb4":"markdown","71dcefac":"markdown","0fe56111":"markdown","d2e03a23":"markdown","d897dd0a":"markdown","2215a62e":"markdown","ce5419dc":"markdown","eea66758":"markdown","0f7c241b":"markdown","ae0008a5":"markdown","f1cd33f5":"markdown","22df39d0":"markdown","10e497d2":"markdown","99231708":"markdown","cd810daa":"markdown","29067c62":"markdown","3d25b724":"markdown","50828c32":"markdown","66d7b084":"markdown","bc030046":"markdown","b1c39f2a":"markdown","5b2ca22e":"markdown","08d5a28d":"markdown","b2dc80ca":"markdown","c4dee8e7":"markdown","333605dc":"markdown","01f98635":"markdown","311088a1":"markdown","8c7d39aa":"markdown","80fe3f24":"markdown","01d82c75":"markdown","1a3782b5":"markdown","4576f2ca":"markdown","d54c4441":"markdown","176aa9bc":"markdown","3de2a10e":"markdown","4f7d8417":"markdown","22f08fcf":"markdown","64d967de":"markdown","55634c42":"markdown","d7882c2d":"markdown","40766739":"markdown","28aeb9f0":"markdown","fca0e694":"markdown","b588c9b8":"markdown","3c2bf4dd":"markdown","746e1fc1":"markdown","3175b2de":"markdown","3a8d96fa":"markdown","ca43c1a3":"markdown","789320c4":"markdown","eab3f453":"markdown","92ae6000":"markdown","99a26d7d":"markdown","ea8816f1":"markdown","80e829fb":"markdown","f820fd6c":"markdown","25b4d7a9":"markdown","5acc8f4d":"markdown","caf3dbc3":"markdown","e4db4584":"markdown","8ef7d9ec":"markdown","9b7a7b5f":"markdown","288ab793":"markdown","48b4af0a":"markdown","7efa6576":"markdown","3023ff8b":"markdown"},"source":{"6a19f63b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #biblioteca para tra\u00e7ar gr\u00e1ficos\nimport sklearn #model evaluation\nimport seaborn as sns","b58c9bd0":"df = pd.read_csv(\"..\/input\/adult-pmr3508\/train_data.csv\", na_values = \"missing\") #importando dados\ndf.head()","8e49bae5":"df.info()","6b8501f7":"df.describe() #estat\u00edsticas dos dados","9dd63f12":"sns.countplot(x='income', data=df)","175d429d":"sns.boxplot(x='income', y='age', data=df)","bcc5c72b":"sns.catplot(x='income', y='age', data=df, kind='violin')","423dd257":"sns.boxplot(x='income', y='fnlwgt', data=df)","86343f80":"sns.catplot(x='income', y='fnlwgt', data=df, kind='violin')","d63684f6":"sns.boxplot(x='income', y='education.num', data=df)","db30604a":"sns.catplot(x='income', y='education.num', data=df, kind='violin')","32946699":"sns.boxplot(x='income', y='capital.gain', data=df)\n","b008aec0":"sns.catplot(x='income', y='capital.gain', data=df, kind = 'violin')\n","44bbe8e8":"sns.boxplot(x='income', y='capital.loss', data=df)\n","1cd6a4be":"sns.catplot(x='income', y='capital.gain', data=df, kind = 'violin')","b153c2ef":"sns.boxplot(x='income', y='hours.per.week', data=df)\n","4067bbc9":"sns.catplot(x='income', y='hours.per.week', data=df, kind = 'violin')","eeec6065":"from sklearn.preprocessing import LabelEncoder\ndf_aux = df.copy()\nle = LabelEncoder()\n\n# Modificando o nosso dataframe para mapear 'income'\ndf_aux['income'] = le.fit_transform(df_aux['income'])\ndf_aux['income']","ddf96f1b":"plt.figure(figsize=(10,10))\n\nsns.heatmap(df.corr(), square = True, annot=True, vmin=-1, vmax=1, cmap='autumn')\nplt.show()","3200363f":"sns.catplot(x=\"sex\", y=\"income\", kind=\"bar\", data=df_aux);\n","e9697f5b":"sns.catplot(x=\"race\", y=\"income\", kind=\"bar\", data=df_aux, aspect = 2);\n","c078bb14":"sns.catplot(x=\"workclass\", y=\"income\", kind=\"bar\", data=df_aux, aspect = 2);\n","115332c7":"sns.catplot(x=\"marital.status\", y=\"income\", kind=\"bar\", data=df_aux, aspect = 4);\n","6c3995bb":"df_aux['marital.status'].value_counts()","c283c92e":"sns.catplot(x=\"occupation\", y=\"income\", kind=\"bar\", data=df_aux, aspect = 5);\n","4d2a8a43":"sns.catplot( x = \"education\", hue = \"income\", kind = \"count\", data = df, height=5, aspect=5)\n","eb05da52":"sns.catplot( x = \"relationship\", hue = \"income\", kind = \"count\", data = df,height=5, aspect=5)\n","cbcff37b":"sns.catplot(y=\"native.country\", x=\"income\", kind=\"bar\", data=df_aux);","51000703":"df['native.country'].value_counts()","a1243f1c":"df.drop_duplicates(keep = 'first', inplace = True)","e8c77c8d":"df=df.drop(['fnlwgt', 'native.country'], axis = 1)","fe370680":"df.head()","675aea79":"df.isnull().sum()\n","88689e16":"from sklearn.model_selection import train_test_split\ny = df.pop('income') #target\nX = df\nX_train, X_valid, Y_train, Y_valid = train_test_split(X, y, test_size=0.25, random_state=42)","3d2d88f0":"# Seleciona as vari\u00e1veis num\u00e9ricas\nnumerical_cols = list(X_train.select_dtypes(include=[np.number]).columns.values)\n\n# Remove as vari\u00e1veis num\u00e9ricas esparsas\nnumerical_cols.remove('capital.gain')\nnumerical_cols.remove('capital.loss')\n\n# Seleciona as vari\u00e1veis num\u00e9ricas esparsas\nsparse_cols = ['capital.gain', 'capital.loss']\n\n# Seleciona as vari\u00e1veis categ\u00f3ricas\ncategorical_cols = list(X_train.select_dtypes(exclude=[np.number]).columns.values)","699b4c9b":"from sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(strategy='most_frequent')","a3ce4250":"from sklearn.preprocessing import OneHotEncoder\n\none_hot = OneHotEncoder(sparse=False) ","0415bda3":"from sklearn.pipeline import Pipeline\n\ncategorical_pipeline = Pipeline(steps = [\n    ('imputer', SimpleImputer(strategy = 'most_frequent')),\n    ('onehot', OneHotEncoder(drop='if_binary'))\n])","a2938568":"from sklearn.preprocessing import RobustScaler,StandardScaler\n\nsparse_pipeline = Pipeline(steps = [\n    ('scaler', RobustScaler())])\nnumerical_pipeline = Pipeline(steps = [\n    ('scaler', StandardScaler())])","9d130512":"from sklearn.compose import ColumnTransformer\npreprocessor = ColumnTransformer(transformers = [\n    ('num', numerical_pipeline, numerical_cols),\n    ('spr', sparse_pipeline, sparse_cols),\n    ('cat', categorical_pipeline, categorical_cols)\n])\n\nX_train = preprocessor.fit_transform(X_train)","0aa4a594":"from sklearn.neighbors import KNeighborsClassifier\n","e7c00159":"from sklearn.model_selection import cross_val_score\n# Quantidades e vizinhos a serem testados\nneighbors = np.arange(10, 30, 1)\n\n# Dicion\u00e1rio que guarda o desempenho de cada k\nneighbors_scores = {}\n\nfor k in neighbors:\n    # Calcula a m\u00e9dia de acur\u00e1cia de cada classificador\n    score = cross_val_score(KNeighborsClassifier(n_neighbors=k), X_train, Y_train, cv = 5, scoring=\"accuracy\").mean()\n\n    # Guarda essa acur\u00e1cia\n    neighbors_scores[k] = score\n\n# Obt\u00e9m a quantidade de vizinhos com o melhor desempenho\nbest_k = max(neighbors_scores, key=neighbors_scores.get)\n\nprint(\"Hiperpar\u00e2metro com melhor resultado: \", best_k)\nprint(\"Acur\u00e1cia obtida: \", neighbors_scores[best_k])","42d0a98b":"knn = KNeighborsClassifier(n_neighbors = best_k)\nknn.fit(X_train, Y_train)","630a09cd":"from sklearn.svm import SVC\nfrom sklearn.model_selection import cross_val_score\n\nsvc = SVC(random_state=42, probability=True)","9bcfc103":"score_svc = cross_val_score(svc, X_train, Y_train, cv = 4, scoring=\"accuracy\")\nprint(\"Acur\u00e1cia com cross validation:\", score.mean())","e056fd87":"# Importa o Bayes Search:\nfrom skopt import BayesSearchCV\n\n# Importa o espa\u00e7o de busca inteiro\nfrom skopt.space import Integer, Real\n\n# Cria o Bayes Search:\nsvc_search_cv = BayesSearchCV(estimator = svc,\n                              search_spaces = {'C': Real(1e-2, 20),\n                                               'gamma': ['scale', 'auto'],},\n                              cv = 2,\n                              n_iter = 15, n_jobs=-1, random_state=42, refit = True)\n\n# Realizando a otimiza\u00e7\u00e3o por BayesSearch:\n%timeit -n 1 -r 1 svc_search_cv.fit(X_train, Y_train)\n\nprint('Melhores hiperpar\u00e2metros: {}'.format(svc_search_cv.best_params_))\nsvc_score = round(svc_search_cv.best_score_,5)\nprint('Desempenho do melhor modelo: {}'.format(svc_score))","d6f626d6":"from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier(random_state = 42)\n\nprint(dtc.get_params().keys())\nprint(dtc.get_params()['max_depth'])","e628833c":"dtc_search_cv = BayesSearchCV(estimator = dtc,\n                              search_spaces = {'max_depth':Integer(1,100), \n                                              'min_samples_leaf':Integer(1,100),}, \n                              scoring='accuracy', \n                              cv = 3,\n                              n_iter = 20, random_state=42)\n\n# Realizando a otimiza\u00e7\u00e3o por BayesSearch:\n%timeit -n 1 -r 1 dtc_search_cv.fit(X_train, Y_train)\n\nprint('Melhores hiperpar\u00e2metros: {}'.format(dtc_search_cv.best_params_))\ndtc_score = round(dtc_search_cv.best_score_,5)\nprint('Desempenho do melhor modelo: {}'.format(dtc_score))","fd39a2d4":"from sklearn.neural_network import MLPClassifier\n\n# Instancia nosso classificador\nmlp = MLPClassifier(random_state=42, early_stopping=True)","e1916ca0":"# Como a dimens\u00e3o do Bayes Search n\u00e3o \u00e9 uma lista, devemos utilizar a RandomSearch ou GridSearch para otimizar\n#Escolhi a Random pois \u00e9 mais r\u00e1pida\nfrom scipy.stats import loguniform as sp_loguniform\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Hiperpar\u00e2metros a serem otimizados\nhyperparams = {'hidden_layer_sizes': [(2 ** i, 2 ** j) for j in np.arange(5, 8) for i in np.arange(4, 7)],\n               'alpha': sp_loguniform(1e-10, 1e-1),\n               'learning_rate': ['constant','adaptive']}\n\n# Busca de Hiperpar\u00e2metros\nmlp_search_cv = RandomizedSearchCV(mlp, hyperparams, scoring='accuracy', n_iter=25, cv=3, n_jobs=-1, random_state=42)\n%timeit -n 1 -r 1 mlp_search_cv.fit(X_train, Y_train)\n\nprint('Melhores hiperpar\u00e2metros: {}'.format(mlp_search_cv.best_params_))\nmlp_score = round(mlp_search_cv.best_score_,5)\nprint('Desempenho do melhor modelo: {}'.format(mlp_score))","ff602209":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression(solver = 'liblinear', C = 1.0, penalty = 'l2', warm_start =  True)\nlr.get_params().keys()","ab2caf30":"lr_search_cv = BayesSearchCV(estimator = svc,\n                              search_spaces = {'C': Real(1e-2, 20),\n                                               'gamma': ['scale', 'auto'],},\n                              cv = 2,\n                              n_iter = 15, n_jobs=-1, random_state=42)\n%timeit -n 1 -r 1 lr_search_cv.fit(X_train, Y_train)\n\nprint('Melhores hiperpar\u00e2metros: {}'.format(lr_search_cv.best_params_))\nlr_score = round(lr_search_cv.best_score_,5)\nprint('Desempenho do melhor modelo: {}'.format(lr_score))","050de0e4":"print('Score SVM: {}'.format(svc_score))\nprint('Score Decison Tree: {}'.format(dtc_score))\nprint('Score Rede Neural: {}'.format(mlp_score))\nprint('Score Logistic Regression: {}'.format(lr_score))","079e61a1":"scores = [svc_score, dtc_score, mlp_score, lr_score]\nprint('Maior score: ', max(scores), ' No \u00edndice: ', scores.index(max(scores)))","c28b2545":"test_data = pd.read_csv(\"..\/input\/adult-pmr3508\/test_data.csv\", na_values=\"missing\") #importanto os arquivos teste\n\nX_test = test_data.drop(['fnlwgt', 'native.country'], axis=1) #removendo as vari\u00e1veis que designamos como irrelevantes","a8b6097f":"#svc_search_cv.fit(X_train, Y_train)\n\nX_test = preprocessor.transform(X_test)\nY_pred = svc_search_cv.predict(X_test)","0d0a1252":"results = pd.DataFrame()\nincomes = []\nfor income in Y_pred:\n    if income == \"<=50K\":\n        incomes.append('<=50K')\n    else:\n        incomes.append('>50K')\nresults[0] = test_data.index\nresults[1] = incomes\nresults.columns = ['Id', 'income']\nresults.head()","c39baaeb":"results.tail()","8a66114c":"results.to_csv('submission.csv',index = False)","30235132":"## 1.2) Visualiza\u00e7\u00e3o dos dados num\u00e9ricos\nCerto, agora vamos visualizar o comportamento das vari\u00e1veis num\u00e9ricas que se relacionam com a nossa 'income'. Para isso, utilizarei as bibliotecas matplotlib e seaborn para plotar os gr\u00e1ficos.","97b44f2b":"Vamos separar nosso dataset em tr\u00eas tipos de categorias:\n* Dados num\u00e9ricos\n* Dados categ\u00f3ricos\n* Dados esparsos","776424a1":"O heatmap nos mostra uma baix\u00edssima correla\u00e7\u00e3o entre fnlwgt e income, dessa forma n\u00f3s descartaremos esta vari\u00e1vel para nosso modelo de predi\u00e7\u00e3o. As outras nos mostram uma certa rela\u00e7\u00e3o com a nossa classe 'income', portanto seguem inalteradas.\n\nO interessante \u00e9 que a nossa an\u00e1lise corrobora com os dados fornecidos pelo heatmap at\u00e9 agora.","b892cf8c":"* ### occupation","bb4c6513":"## 1.3) Visualiza\u00e7\u00e3o dos dados categ\u00f3ricos","0b04cb76":"* ### Race","543c575f":"Similarmente ao de sexo, o gr\u00e1fico revela uma forte desigualdade de g\u00eanero, onde os maridos recebem bem mais que as esposas.","70e0ef9f":"## Outros modelos)","768bc07e":"Agora que o nossos dados est\u00e3o limpos e pr\u00e9-processados, devemos fazer o nosso modelo de previs\u00e3o. Iremos utilizar o K-Nearest Neighbors (KNN).","0d834fb4":"Essa barra escura representa a imprecis\u00e3o nos dados, ou seja, h\u00e1 uma vari\u00e2ncia muito alta. Assim n\u00e3o pode-se afirmar muito sobre pessoas casadas com a esposa nas for\u00e7as armadas. Ainda assim, podemos considerar uma tend\u00eancia de pessoas casadas ganharem mais de 50 K de renda.","71dcefac":"## Decision Tree\n\nIremos utilizar o classificador \"Decision Tree\"","0fe56111":"# Sergio Magalh\u00e3es Contente\n##  Planejamento do projeto (2a Entrega):\n* ### Importa\u00e7\u00e3o e visualiza\u00e7\u00e3o de dados:\n   Consiste em importar os dados do dataset fornecido, al\u00e9m de observar eventuais dados faltantes para serem trabalhados depois.\n   \n* ### Pr\u00e9-processamento:\n    Nesta etapa, n\u00f3s limpamos nossos dados a fim de facilitar a an\u00e1lise.\n* ### Predi\u00e7\u00e3o:\n    Testamos alguns modelos, al\u00e9m do KNN, para prever os resultados e selecionamos o que pontuou mais na valida\u00e7\u00e3o cruzada.\n* ### Conclus\u00e3o:\n    Por fim, n\u00f3s testamos a acur\u00e1cia do nosso modelo e verificamos se ele conseguiu prever os resultados adequadamente.\n","d2e03a23":"## Os melhores modelos s\u00e3o: o SVM e a Regress\u00e3o Log\u00edstica (mesmo score)!","d897dd0a":"Nosso algoritmo nos retornou que o melhor hiperpar\u00e2metro **k** \u00e9 o 14 e sua acur\u00e1cia foi de: 86,22%. Portanto, utilizaremos este valor como KNN.","2215a62e":"De fato, temos pouco dados a respeito de Married-AF-Spouse, como dito anteriormente, isso cria uma grande varia\u00e7\u00e3o de dados.","ce5419dc":"Agora trabalharemos com os dados faltantes e para isso usaremos a estrat\u00e9gias de substitu\u00ed-los pela moda. Para tal usaremos um Simple Imputer do scikit-learn. O SimpleImputer \u00e9 um transformador que preenche os dados faltantes de cada vari\u00e1vel de acordo com uma estrat\u00e9gia que podemos escolher: m\u00e9dia, mediana ou moda.\n","eea66758":"Ap\u00f3s isso, remove-se as colunas 'fnlwgt', 'native.country' pois demostraram ter baix\u00edssima relev\u00e2ncia na nossa an\u00e1lise com o \u00edndice de correla\u00e7\u00e3o baixo ou varia\u00e7\u00e3o alta de dados internos","0f7c241b":"Concluimos que h\u00e1 um total de **15 vari\u00e1veis diferentes** (Id n\u00e3o conta). Dentra as quais 9 s\u00e3o do tipo _object_ e 6 s\u00e3o do tipo _int64_. Tamb\u00e9m devemos nos atentar ao fato de que possu\u00edmos vari\u00e1veis categ\u00f3ricas e dados quantitativas. As categ\u00f3ricas s\u00e3o aquelas cujo valor n\u00e3o \u00e9 expresso por n\u00fameros, como por exemplo \"education\", sendo portanto mais dif\u00edceis do computador interpretar. J\u00e1 as quantitativas s\u00e3o num\u00e9ricas, o que leva a uma facilidade na an\u00e1lise, como \u00e9 o caso de \"age\".","ae0008a5":"Concluimos que os dados extra\u00eddos foram desproporcionais, ou seja, a maioria foi retirada dos Estados Unidos. Assim, por conta dessa falta de amostragem, os demais pa\u00edses geram esta enorma varia\u00e7\u00e3o na an\u00e1lise com o 'income'. Iremos descartar essa vari\u00e1vel.","f1cd33f5":"* ### workclass","22df39d0":"* ### Capital.gain\n\n","10e497d2":"Para os dados categ\u00f3ricos, iremos utilizar o One Hot Encoder a fim de mape\u00e1-los em n\u00fameros","99231708":"Importamos os nossos dados de teste e avaliamos o melhor modelo com base neles","cd810daa":"* ### Education\n\n","29067c62":"* ### Capital.loss\n","3d25b724":"Vamos primeiro dividir nosso dafaframe em treino e valida\u00e7\u00e3o:","50828c32":"### Constru\u00e7\u00e3o dos modelos:\n### Nessa se\u00e7\u00e3o, n\u00f3s iremos construir modelos que ser\u00e3o comparados entre si com base em suas acur\u00e1cias e, por fim, ser\u00e1 decidido o melhor entre os quatro.","66d7b084":"# ENTREGA 2","bc030046":"O gr\u00e1fico nos mostra que pessoas com um n\u00edvel de escolaridade maior tendem tamb\u00e9m a receberem mais renda que os demais, como observado em bacharels, mestres e doutores.","b1c39f2a":"A observa\u00e7\u00e3o \u00e9 a mesma realizada no 'capital.gain'","5b2ca22e":"### Analisando a correla\u00e7\u00e3o de vari\u00e1veis num\u00e9ricas","08d5a28d":"Agora vamos ver a acur\u00e1cia do modelo sem a otimiza\u00e7\u00e3o de hiperpar\u00e2metros","b2dc80ca":"Informa\u00e7\u00f5es \u00fateis retiradas da tabela:\n\nDesvio padr\u00e3o de 'capital.gain' e 'capital.loss' bem alto, o que implica em uma poss\u00edvel dispers\u00e3o (dados esparsos). Tamb\u00e9m h\u00e1 o fato de ambos possu\u00edrem um valor m\u00e1ximo alto, o que pode ser sinal de outliers.\n","c4dee8e7":"Observa-se que pessoas que trabalham por mais de 40h semanais ganham mais de 50K.","333605dc":"Para essa vari\u00e1vel, as classifica\u00e7\u00f5es s\u00e3o bem parecidas, o que nos leva a descart\u00e1-lo no nosso modelo de previs\u00e3o.","01f98635":"Agora iremos estimar a acur\u00e1cia do nosso modelo atrav\u00e9s da valida\u00e7\u00e3o cruzada com hiperpar\u00e2metro \"k\":","311088a1":"Agora vamos criar nossa pipeline categ\u00f3rica. Uma pipeline \u00e9 uma sequ\u00eancia de transformadores.","8c7d39aa":"Agora veremos como ficou o nosso dataframe:","80fe3f24":"## Rede Neural\n\nO terceiro classificador que n\u00f3s utilizaremos ser\u00e1 o uso de uma rede neural do scikit-learn","01d82c75":"Tra\u00e7ando o mapa de calor das vari\u00e1veis:","1a3782b5":"* ### hours.per.week","4576f2ca":"## 2.1) Dados duplicados e irrelevantes","d54c4441":"## K-Nearest Neighbors","176aa9bc":"Dado que todos os modelos fizeram um score atrav\u00e9s da valida\u00e7\u00e3o cruzada, basta atentar-se ao que pontuou mais","3de2a10e":"Para os dados num\u00e9ricos, iremos utilizar dois transformadores: StandardScaler e RobustScaler.\n\nO StandardScaler consiste em normalizar o conjunto de dados, transformando os dados de tal forma que sua distribui\u00e7\u00e3o ter\u00e1 um valor m\u00e9dio 0 e um desvio padr\u00e3o de 1.\n\nAgora o Robust Scaler \u00e9 um transformador que possui o prop\u00f3sito de retirar eventuais outliers do dado.","4f7d8417":"Essa vari\u00e1vel nos diz que h\u00e1 maior densidade de pessoas com maior tempo gasto em educa\u00e7\u00e3o com a classifica\u00e7\u00e3o > 50K. Entretanto, observa-se que a distribui\u00e7\u00e3o dos gr\u00e1ficos tem uma enorma diferen\u00e7a para cada uma das duas classifica\u00e7\u00f5es:\n\nPara <= 50k, h\u00e1 muitas pessoas com tempo entre 7.5 e 10 anos. Nota-se que h\u00e1 uma quantidade relevante para quem tem entre 12.5 e 15 anos. Sendo que a maior parte encontra-se com baixos valores de tempo de estudo.\n\nPara > 50k, h\u00e1 uma grande quantidade de pessoas distribu\u00eddas entre 12.5 e 15 anos, mas tamb\u00e9m entre 7.5 e 10 anos, sendo que a maior parte encontra-se para altos valores de tempo de estudo.","22f08fcf":" Como previsto em \n> df.describe()\n\nnossos dados est\u00e3o muito esparsos e indicam a presen\u00e7a de outliers. Os gr\u00e1ficos refor\u00e7am essa ideia, portanto, devemos levar isso em conta no pr\u00e9-processamento.","64d967de":"## Support Vector Machine","55634c42":"Agora vamos nos atentar ao tipo de vari\u00e1vel do nosso data frame e suas quantidades (dados faltantes n\u00e3o inclusos)","d7882c2d":"O gr\u00e1fico de pa\u00edses nativos possui muitas barras escuras, isso indica que nossos dados est\u00e3o com uma varia\u00e7\u00e3o alta dentro de cada grupo. Dessa forma, vamos analisar a quantidade de indiv\u00edduos analisados de cada pa\u00eds:","40766739":"Agora, vamos otimizar os hiperpar\u00e2metros da \u00e1rvore de decis\u00e3o:","28aeb9f0":"Para podermos aplicar nosso Pipeline num\u00e9rico em uma coluna devemos importar o Colum Transformer","fca0e694":"* ### Relationship\n\n\n","b588c9b8":"Temos uma ideia de que h\u00e1 mais adultos com renda m\u00e9dia <= 50 K do que > 50 K","3c2bf4dd":"## Compara\u00e7\u00e3o entre os modelos","746e1fc1":"Agora vamos criar o arquivo de submiss\u00e3o ao judge","3175b2de":"## 2) Pr\u00e9-processamento","3a8d96fa":"Primeiramente, vamos retirar os dados duplicados do nosso dataframe\n","ca43c1a3":"Observamos que pessoas que trabalham por conta pr\u00f3pria recebem mais que 50k em m\u00e9dia","789320c4":"* ### fnlwgt","eab3f453":"## 3) Predi\u00e7\u00e3o","92ae6000":"## 1) Importa\u00e7\u00e3o e Visualiza\u00e7\u00e3o de dados\n## 1.1) Importa\u00e7\u00e3o e primeiras an\u00e1lises","99a26d7d":"Os dois gr\u00e1ficos j\u00e1 nos mostram que quanto mais velha uma pessoa, maior a chance dela receber >50 K. Sendo que a maioria das que possuem esta quantia encontra-se por valta dos 40 anos. J\u00e1 para quem possui <= 50 K de renda, a maior parte encontra-se nos 20\/30 anos de idade para nosso dataset","ea8816f1":"## Regress\u00e3o Log\u00edstica\n\nO \u00faltimo modelo ser\u00e1 a regress\u00e3o log\u00edstica","80e829fb":"### 2.2) Dados faltantes","f820fd6c":"* ### Conclus\u00e3o\n","25b4d7a9":"Vamos utilizar a classe LabelEncoder do scikit-learn para transformar nossa vari\u00e1vel'income' em num\u00e9rica, mapeando todos os valores \"<=50K\" e \">50K\" em 0s e 1s.","5acc8f4d":"* ### marital.status","caf3dbc3":"Al\u00e9m disso, percebmos tam\u00e9m um forte desigualdade de ganhos \u00e9tnica. Sendo que as etnias branca e asi\u00e1tica das ilhas do Pac\u00edfico s\u00e3o as que possuem maiores probabilidade de ganhar mais de 50 K","e4db4584":"* ### Sex","8ef7d9ec":"Nota-se que as profiss\u00f5es de \"prof-specialty\" e \"exec-managerial\" s\u00e3o as que mais geram uma grande renda (> 50K), ao passo que \"Farming-fishing\" e \"Handlers-cleaners\" s\u00e3o umas das que menos trazem esse retorno financeiro.","9b7a7b5f":"Para otimizar nossos hiperpar\u00e2metros, vamos utilizar a Bayes Search:","288ab793":"* ### Native.country","48b4af0a":"* ### Age","7efa6576":"* ### education.num","3023ff8b":"Lembrando que conseguimos utilizar esses gr\u00e1ficos porque nosso income foi mapeado para 0s e 1s.\n\nNele percebemos que h\u00e1 uma diferen\u00e7a significativa entre os ganhos de homens e mulheres. Enquanto 30% dos homens ganham mais que 50 K, apenas 10% das mulheres realizam o mesmo feito. Isso \u00e9 um reflexo do machismo e da desigualdade de g\u00eanero na nossa sociedade."}}