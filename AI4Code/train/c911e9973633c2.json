{"cell_type":{"c4163779":"code","89326e9b":"code","bce3a5cb":"code","00f5f72f":"code","509b4a93":"code","f46219b9":"code","3d55e2dc":"code","a3268557":"code","6fa60d7e":"code","caea1ead":"code","182bd66f":"code","50bb52f2":"code","4a3f571a":"code","03685cf3":"code","18ecc114":"code","b07b3c0f":"code","62a9c5a0":"code","63642db9":"code","e76fe153":"code","afb16245":"code","6eca6e0b":"markdown","6d15baaa":"markdown","85c00648":"markdown","eabf37d5":"markdown","811916b9":"markdown","2ee93b47":"markdown","d6e7eca3":"markdown","3be586e5":"markdown","62e51be6":"markdown","7353e6ec":"markdown"},"source":{"c4163779":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","89326e9b":"import numpy as np \nimport pandas as pd \nimport math, random\n\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\n\npd.set_option('display.max_columns', 100)\n\nfrom lightgbm import LGBMRegressor\n\nSEED = 47","bce3a5cb":"PATH = '..\/input\/30-days-of-ml\/'\n\ndf_train = pd.read_csv(PATH + '\/train.csv')\ndf_test = pd.read_csv(PATH + '\/test.csv')\ndf_sub = pd.read_csv(PATH + '\/sample_submission.csv')","00f5f72f":"target = df_train['target']\nfeatures = df_train.drop(['id','target'], axis=1)","509b4a93":"features.head()","f46219b9":"import featuretools as ft","3d55e2dc":"es = ft.EntitySet(id = 'data')\nes.entity_from_dataframe(entity_id = 'january', \n                         dataframe = features, \n                         index='id')\n                         #make_index = True, index = 'index') \n\n\nfeature_matrix, feature_defs = ft.dfs(entityset = es,                                          \n                                      target_entity = 'january',                               \n                                      trans_primitives = ['add_numeric', 'multiply_numeric'], \n                                      verbose=1) ","a3268557":"feature_matrix.head()\ncat_features = ['cat0','cat1','cat2','cat3','cat4','cat5','cat6','cat7','cat8','cat9']\ndf_cat = feature_matrix[cat_features]\n\n\n\nfeature_matrix = feature_matrix.drop(cat_features,axis=1)","6fa60d7e":"feature_matrix.head()","caea1ead":"import optuna\nfrom optuna.visualization import plot_optimization_history, plot_param_importances","182bd66f":"def objective(trial, data=feature_matrix, target=target):\n    \n    X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=42)\n    \n    params = {\n        'metric': 'rmse', \n        'random_state': SEED,\n        'n_estimators': 10000,\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-3, 10.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-3, 10.0),\n        #'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.35,0.4,0.45,0.5,0.6,0.7,0.75,0.8,0.85]),\n        'subsample': trial.suggest_categorical('subsample', [0.6,0.65,0.7,0.75,0.8,0.85]),\n        'learning_rate': trial.suggest_categorical('learning_rate', \n                                                   [0.005,0.006,0.008,0.01,0.015,0.02,0.03]),\n        #'max_depth': trial.suggest_categorical('max_depth', [-1,10,20]),\n        'num_leaves' : trial.suggest_int('num_leaves', 1, 300),\n        #'min_child_samples': trial.suggest_int('min_child_samples', 1, 300),\n        #'cat_smooth' : trial.suggest_int('min_data_per_groups', 1, 100)\n    }\n    \n \n    model = LGBMRegressor(**params)      \n    model.fit(X_train, y_train, eval_set=[(X_test,y_test)], early_stopping_rounds=300, verbose=False)\n    preds = model.predict(X_test)\n    \n\n    rmse = mean_squared_error(y_test, preds, squared=False)\n    \n    return rmse","50bb52f2":"%%time\n\n\nstudy = optuna.create_study(direction='minimize')  \nstudy.optimize(objective, n_trials=5)              \n\n\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best score:', study.best_trial.value)","4a3f571a":"plot_optimization_history(study)","03685cf3":"plot_param_importances(study)","18ecc114":"\nstudy.best_params","b07b3c0f":"optuna_params = study.best_params\n\noptuna_params['metric'] = 'rmse'\noptuna_params['random_state'] = SEED\noptuna_params['n_estimators'] = 10000","62a9c5a0":"\nX_train, X_test, y_train, y_test = train_test_split(feature_matrix, target, test_size=0.2, random_state=SEED)\n\n\nmodel_optuna = LGBMRegressor(**optuna_params)\nmodel_optuna.fit(X_train, y_train, eval_set=[(X_test, y_test)], early_stopping_rounds=300, verbose=300)","63642db9":"features= df_test.drop(cat_features,axis=1)\n\nes = ft.EntitySet(id = 'data')\nes.entity_from_dataframe(entity_id = 'target', \n                         dataframe = features, \n                         index='id')\n                         #make_index = True, index = 'index') \n\ntest_feature_matrix, test_feature_defs = ft.dfs(entityset = es,                                          \n                                      target_entity = 'target',                               \n                                      trans_primitives = ['add_numeric', 'multiply_numeric'],  \n                                      verbose=1) ","e76fe153":"test_feature_matrix.head()","afb16245":"\npred_optuna = model_optuna.predict(test_feature_matrix)\n\ndf_sub['target'] = pred_optuna\ndf_sub.to_csv('submission.csv', index=False)","6eca6e0b":"# Model Training after HyperParameter Optimization","6d15baaa":"# Importing Necessary Libraries","85c00648":"# Check Optimization History","eabf37d5":"# Using Optuna for Hyperparameter Optimization for LightGBM Regressor","811916b9":"# Final Predictions","2ee93b47":"# New Feature Matrix","d6e7eca3":"# Feature Importance of HyperParameters","3be586e5":"# Check target and features values as being stated in the Tutorial","62e51be6":"# Using Featuretools to create synthetic features","7353e6ec":"# Creating the Test set features similar to training set using featuretools"}}