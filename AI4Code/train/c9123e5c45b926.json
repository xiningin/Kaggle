{"cell_type":{"f4c42cd6":"code","ccdce120":"code","7d1b34fc":"code","e0bbdfa9":"code","905629d3":"code","b1a19333":"code","51670a39":"code","a2a36d86":"code","b0e5f8ac":"code","4abecb75":"code","105127d6":"code","aa38666e":"code","091f5ba9":"code","4746a099":"code","ff55487e":"code","7946bfb3":"code","cce6fd05":"code","c1404da6":"code","bb20b87a":"code","0c7e5651":"code","39c3796e":"code","8067c0da":"code","29e42e7c":"code","638eb175":"code","d72a6fa3":"code","b70cdcc5":"code","2ec6bac2":"code","e2b24296":"code","be450db3":"code","0dc599cc":"code","9e26e8c7":"code","765c2652":"code","cff5b783":"code","b3b15380":"code","ae680eac":"code","128b38ae":"code","b63f92cc":"code","68826357":"code","67a362e3":"code","d9b52d78":"code","5d1be5d8":"code","754cfd9c":"code","8ab82d57":"code","1af5ada6":"code","23c605bc":"code","86150d25":"code","964a5c9d":"markdown","cf6502ec":"markdown","de8cbf39":"markdown","0b2c8c27":"markdown","5dc83c5c":"markdown","43e1d02b":"markdown","fded7ead":"markdown","8b3968e7":"markdown","941c0779":"markdown","306b7c86":"markdown","817f30ab":"markdown","2bae03e1":"markdown","0711eb5d":"markdown","6318c58b":"markdown","575684b9":"markdown","529ddd65":"markdown","9a27caac":"markdown","7aab009d":"markdown","737ff4f0":"markdown","c7d938ad":"markdown","48d4b687":"markdown","c4ed428e":"markdown","80274a5b":"markdown","bdf7311c":"markdown","26341ba2":"markdown","de89fd94":"markdown","f4496c97":"markdown"},"source":{"f4c42cd6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ccdce120":"import warnings\nwarnings.filterwarnings('ignore')","7d1b34fc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","e0bbdfa9":"grad= pd.read_csv(\"\/kaggle\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv\")","905629d3":"grad.head()","b1a19333":"grad.info()","51670a39":"grad.describe()","a2a36d86":"grad.shape","b0e5f8ac":"# percentage of missing values in each column\nround((100*(grad.isnull().sum())\/len(grad)),2).sort_values(ascending=False)","4abecb75":"# percentage of missing values in each rows\nround((100*(grad.isnull().sum(axis=1))\/len(grad)),2).sort_values(ascending=False)","105127d6":"grad_dub=grad.copy()","aa38666e":"# Checking for duplicates and dropping the entire duplicate row if any\ngrad_dub.drop_duplicates(subset=None, inplace=True)","091f5ba9":"grad_dub.shape","4746a099":"grad.shape","ff55487e":"# Check the datatypes before Removing\ngrad.info()","7946bfb3":"grad.drop('Serial No.',axis=1, inplace=True)","cce6fd05":"grad.shape","c1404da6":"grad.info()","bb20b87a":"for col in grad:\n    print(grad[col].value_counts(ascending=False))","0c7e5651":"# Check the shape before spliting\n\ngrad.shape","39c3796e":"from sklearn.model_selection import train_test_split\n\n# We should specify 'random_state' so that the train and test data set always have the same rows, respectively\n\nnp.random.seed(0)\ndf_train, df_test = train_test_split(grad, train_size = 0.70, test_size = 0.30, random_state = 100)","8067c0da":"df_train.info()","29e42e7c":"df_train.shape","638eb175":"df_test.info()","d72a6fa3":"df_test.shape","b70cdcc5":"df_train.info()","2ec6bac2":"df_train.columns","e2b24296":"# Create a new dataframe of only numeric variables:\n\ngrad_num=df_train[[ 'GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR ', 'CGPA',\n       'Research', 'Chance of Admit ']]\n\nsns.pairplot(grad_num, diag_kind='kde')\nplt.show()","be450db3":"# Let's check the correlation coefficients to see which variables are highly correlated. Note:\n# here we are considering only those variables (dataframe: grad) that were chosen for analysis\n\nplt.figure(figsize = (25,20))\nsns.heatmap(grad.corr(), annot = True, cmap=\"RdBu\")\nplt.show()","0dc599cc":"from sklearn.preprocessing import MinMaxScaler","9e26e8c7":"scaler= MinMaxScaler()","765c2652":"df_train.head()","cff5b783":"df_train.columns","b3b15380":"# Apply scaler() to all the numeric variables\n\nnum_vars = ['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR ', 'CGPA',\n        'Chance of Admit ']\n\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])","ae680eac":"# Checking values after scaling\ndf_train.head()","128b38ae":"df_train.describe()","b63f92cc":"y = df_train.pop('Chance of Admit ')\nX= df_train","68826357":"X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=80)","67a362e3":"from sklearn.metrics import mean_squared_error, r2_score \nfrom sklearn.linear_model import LinearRegression\nlin_reg = LinearRegression()\nlin_reg.fit(X_train, y_train)\ny_pred = lin_reg.predict(X_test)\n\nprint('Error', np.sqrt(mean_squared_error(y_test, y_pred)))","d9b52d78":"# feature selection\ndef select_features(X_train, y_train, X_test):\n    # configure to select all features\n    fs = SelectKBest(score_func=f_regression, k='all')\n    # learn relationship from training data\n    fs.fit(X_train, y_train)\n    # transform train input data\n    X_train_fs = fs.transform(X_train)\n    # transform test input data\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs","5d1be5d8":"# feature selection\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_regression\nX_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)","754cfd9c":"# what are scores for the features\nfrom matplotlib import pyplot\nfor i in range(len(fs.scores_)):\n    print('Feature %d: %f' % (i, fs.scores_[i]))\n# plot the scores\npyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\npyplot.show()","8ab82d57":"# print the coefficients\nlist(zip(X_train.columns, lin_reg.coef_))","1af5ada6":"print(lin_reg.score(X_test, y_test))\nprint(r2_score(y_test, y_pred))","23c605bc":"coef = pd.Series(lin_reg.coef_, X.columns).sort_values()\ncoef.plot(kind='bar', title =  'Model Coeff\\'s');","86150d25":"fig = plt.figure()\nsns.regplot(y_test, y_pred)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_pred', fontsize = 16) \nplt.show()","964a5c9d":"Note:\n- The shape after running the drop duplicate command is same as the original dataframe.\n\n- Hence we can conclude that there were zero duplicate values in the dataset.\n","cf6502ec":"# BUILDING A LINEAR MODEL","de8cbf39":"# Correlation Matrix","0b2c8c27":"### Note\n- As expected CGPA explains most of the relationship between CGPA+GPA+Toefl scores","5dc83c5c":"# RESCALING THE FEATURES","43e1d02b":"# Data Cleaning","fded7ead":"- Verify the info and shape of the dataframes after split:","8b3968e7":"# DATA QUALITY CHECK","941c0779":"### Note\n- A simple linear regression gave us an accuracy of  84.3%","306b7c86":"# Remove unrequired Columns","817f30ab":"## Dividing into X and Y sets for the model building","2bae03e1":"# EXPLORATORY DATA ANALYSIS","0711eb5d":"- We need to perform the EDA on TRAINING (df_train) Dataset.","6318c58b":"### Note:\n- It has 500 rows & 9 columns","575684b9":"As per our final Model, the top predictor variables that influences the Chance of Admit are:\n\n- **CGPA:** A coefficient value of \u20180.630901226823657\u2019 indicated that a unit increase in CGPA variable, increases the Chance of Admit  by 0.630901226823657 units.\n- **TOEFL Score:** A coefficient value of \u20180.1562165519192439\u2019 indicated that a unit increase in tOEFL Score variable, increases the Chance of Admit  by 0.1562165519192439 units.\n- **LOR:** A coefficient value of \u20180.11410688658266605\u2019 indicated that a unit increase in LOR variable, increases the Chance of Admit  by 0.11410688658266605 units.\n- **GRE Score:** A coefficient value of \u20180.07818771419694856\u2019 indicated that a unit increase in GRE Score variable, increases the Chance of Admit  by 0.07818771419694856 units.\n","529ddd65":"## Visualising Numeric Variables\n- Let's make a pairplot of all the numeric variables.","9a27caac":"## Check for NULL\/MISSING values","7aab009d":"# Note\n- There seems to be no Junk\/Unknown values in the entire dataset.","737ff4f0":"Note: \n- There are no missing \/ Null values either in columns or rows","c7d938ad":"- splitting the data to Train and Test: - We will now split the data into TRAIN and TEST (70:30 ratio)\n- We will use train_test_split method from sklearn package for this","48d4b687":"# Duplicate Check","c4ed428e":"# FINAL REPORT","80274a5b":"- Checking value_counts() for entire dataframe.\n\n- This will help to identify any Unknow\/Junk values present in the dataset.","bdf7311c":"Note:\n\n-The above Pair-Plot tells us that there is a LINEAR RELATION between ''GRE Score', 'TOEFL Score', 'CGPA' &  'Chance of Admit '","26341ba2":"# SPLITTING THE DATA","de89fd94":"Note:\n\n- The heatmap clearly shows which all variable are multicollinear in nature, and which variable have high collinearity with the target variable.\n- We will refer this map back-and-forth while building the linear model so as to validate different correlated values along with VIF & p-value, for identifying the correct variable to select\/eliminate from the model.","f4496c97":"# Building Linear Model "}}