{"cell_type":{"2ae6ac21":"code","1707b504":"code","d3b7012f":"code","e4d70bfd":"code","adc926af":"code","d8015509":"code","a0906be3":"code","18f9919d":"code","74120fab":"code","0c698b77":"code","8a2ba079":"code","8ff298f0":"code","ae9cfeeb":"code","3600c324":"code","0025cb7f":"code","185c1044":"code","26ecb702":"code","f0a9fedb":"code","3b2f6b30":"code","838f8941":"code","f93955a6":"code","fae21a88":"code","eceb9c37":"code","1478b86f":"code","40c84695":"code","e6131e65":"code","7681423e":"code","a83ad64f":"code","b737bf3e":"code","ca7a766d":"code","ce89d643":"code","de973670":"code","717b093a":"markdown","4c5b31be":"markdown","8d46bc37":"markdown","4c17ea4f":"markdown","6e1aab2a":"markdown","a79b16e9":"markdown","fc337b71":"markdown","b1156626":"markdown","747cd62b":"markdown","55c50599":"markdown","399f1814":"markdown","bbec6d86":"markdown","9750f0e5":"markdown","0ace2629":"markdown","afea0413":"markdown","681f8f84":"markdown","3ebdaf38":"markdown"},"source":{"2ae6ac21":"import re\nimport sys\n\nimport time\nimport datetime\n\nimport numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\n\n# Loading the data\ndf = pd.read_csv('..\/input\/googleplaystore.csv')","1707b504":"%matplotlib inline","d3b7012f":"sns.set(style='darkgrid')\nsns.set_palette('PuBuGn_d')","e4d70bfd":"df.head()\n# Executing the above script will display the first five rows of the dataset as shown below","adc926af":"# Checking the data type of the columns\ndf.info()","d8015509":"# Exploring missing data and checking if any has NaN values\nplt.figure(figsize=(7, 5))\nsns.heatmap(df.isnull(), cmap='viridis')\ndf.isnull().any()","a0906be3":"df.isnull().sum()","18f9919d":"# The best way to fill missing values might be using the median instead of mean.\ndf['Rating'] = df['Rating'].fillna(df['Rating'].median())\n\n# Before filling null values we have to clean all non numerical values & unicode charachters \nreplaces = [u'\\u00AE', u'\\u2013', u'\\u00C3', u'\\u00E3', u'\\u00B3', '[', ']', \"'\"]\nfor i in replaces:\n\tdf['Current Ver'] = df['Current Ver'].astype(str).apply(lambda x : x.replace(i, ''))\n\nregex = [r'[-+|\/:\/;(_)@]', r'\\s+', r'[A-Za-z]+']\nfor j in regex:\n\tdf['Current Ver'] = df['Current Ver'].astype(str).apply(lambda x : re.sub(j, '0', x))\n\ndf['Current Ver'] = df['Current Ver'].astype(str).apply(lambda x : x.replace('.', ',',1).replace('.', '').replace(',', '.',1)).astype(float)\ndf['Current Ver'] = df['Current Ver'].fillna(df['Current Ver'].median())","74120fab":"# Count the number of unique values in category column \ndf['Category'].unique()","0c698b77":"# Check the record  of unreasonable value which is 1.9\ni = df[df['Category'] == '1.9'].index\ndf.loc[i]","8a2ba079":"# Drop this bad column\ndf = df.drop(i)","8ff298f0":"# Removing NaN values\ndf = df[pd.notnull(df['Last Updated'])]\ndf = df[pd.notnull(df['Content Rating'])]","ae9cfeeb":"# App values encoding\nle = preprocessing.LabelEncoder()\ndf['App'] = le.fit_transform(df['App'])\n# This encoder converts the values into numeric values","3600c324":"# Category features encoding\ncategory_list = df['Category'].unique().tolist() \ncategory_list = ['cat_' + word for word in category_list]\ndf = pd.concat([df, pd.get_dummies(df['Category'], prefix='cat')], axis=1)","0025cb7f":"# Genres features encoding\nle = preprocessing.LabelEncoder()\ndf['Genres'] = le.fit_transform(df['Genres'])","185c1044":"# Encode Content Rating features\nle = preprocessing.LabelEncoder()\ndf['Content Rating'] = le.fit_transform(df['Content Rating'])","26ecb702":"# Price cealning\ndf['Price'] = df['Price'].apply(lambda x : x.strip('$'))","f0a9fedb":"# Installs cealning\ndf['Installs'] = df['Installs'].apply(lambda x : x.strip('+').replace(',', ''))","3b2f6b30":"# Type encoding\ndf['Type'] = pd.get_dummies(df['Type'])","838f8941":"# Last Updated encoding\ndf['Last Updated'] = df['Last Updated'].apply(lambda x : time.mktime(datetime.datetime.strptime(x, '%B %d, %Y').timetuple()))","f93955a6":"# Convert kbytes to Mbytes \nk_indices = df['Size'].loc[df['Size'].str.contains('k')].index.tolist()\nconverter = pd.DataFrame(df.loc[k_indices, 'Size'].apply(lambda x: x.strip('k')).astype(float).apply(lambda x: x \/ 1024).apply(lambda x: round(x, 3)).astype(str))\ndf.loc[k_indices,'Size'] = converter","fae21a88":"# Size cleaning\ndf['Size'] = df['Size'].apply(lambda x: x.strip('M'))\ndf[df['Size'] == 'Varies with device'] = 0\ndf['Size'] = df['Size'].astype(float)","eceb9c37":"# Split data into training and testing sets\nfeatures = ['App', 'Reviews', 'Size', 'Installs', 'Type', 'Price', 'Content Rating', 'Genres', 'Last Updated', 'Current Ver']\nfeatures.extend(category_list)\nX = df[features]\ny = df['Rating']","1478b86f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 10)","40c84695":"# Look at the 15 closest neighbors\nmodel = KNeighborsRegressor(n_neighbors=15)","e6131e65":"# Find the mean accuracy of knn regression using X_test and y_test\nmodel.fit(X_train, y_train)","7681423e":"# Calculate the mean accuracy of the KNN model\naccuracy = model.score(X_test,y_test)\n'Accuracy: ' + str(np.round(accuracy*100, 2)) + '%'","a83ad64f":"# Try different numbers of n_estimators - this will take a minute or so\nn_neighbors = np.arange(1, 20, 1)\nscores = []\nfor n in n_neighbors:\n    model.set_params(n_neighbors=n)\n    model.fit(X_train, y_train)\n    scores.append(model.score(X_test, y_test))\nplt.figure(figsize=(7, 5))\nplt.title(\"Effect of Estimators\")\nplt.xlabel(\"Number of Neighbors K\")\nplt.ylabel(\"Score\")\nplt.plot(n_neighbors, scores)","b737bf3e":"model = RandomForestRegressor(n_jobs=-1)\n# Try different numbers of n_estimators - this will take a minute or so\nestimators = np.arange(10, 200, 10)\nscores = []\nfor n in estimators:\n    model.set_params(n_estimators=n)\n    model.fit(X_train, y_train)\n    scores.append(model.score(X_test, y_test))\nplt.figure(figsize=(7, 5))\nplt.title(\"Effect of Estimators\")\nplt.xlabel(\"no. estimator\")\nplt.ylabel(\"score\")\nplt.plot(estimators, scores)\nresults = list(zip(estimators,scores))\nresults","ca7a766d":"predictions = model.predict(X_test)\n'Mean Absolute Error:', metrics.mean_absolute_error(y_test, predictions)","ce89d643":"'Mean Squared Error:', metrics.mean_squared_error(y_test, predictions)","de973670":"'Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, predictions))","717b093a":"The dataset has 10,841 records and 13 columns, all of them are object types except the target column (Rating) which is float","4c5b31be":"This can be done by selecting all k values from the \"Size\" column and replace those values by their corresponding M values, and since k indices belong to a list of non-consecutive numbers, a new dataframe (converter) will be created with these k indices to perform the conversion, then the final values will be assigned back to the \"Size\" column.","8d46bc37":"# K-Nearest Neighbors Model","4c17ea4f":"Looks like there are missing values in \"Rating\", \"Type\", \"Content Rating\" and \" Android Ver\". But most of these missing values in Rating column.","6e1aab2a":"# Data Exploration and Cleaning","a79b16e9":"# Random Forest Model","fc337b71":"The k-nearest neighbors algorithm is based around the simple idea of predicting unknown values by matching them with the most similar known values. Building the model consists only of storing the training dataset. To make a prediction for a new data point, the algorithm finds the closest data points in the training dataset \u2014 its \"*nearest neighbors*\".","b1156626":"In this section shows how k-nearest neighbors and random forests can be used to predict app ratings based on the other matrices. First, the dataset has to separate into dependent and independent variables (or features and labels). Then those variables have to split into a training and test set.\n\nDuring training stage we give the model both the features and the labels so it can learn to classify points based on the features.","747cd62b":"Many machine learning algorithms can support categorical values without further manipulation but there are many more algorithms that do not. We need to make all data ready for the model, so we will convert categorical variables (variables that stored as text values) into numircal variables.","55c50599":"# Categorical Data Encoding\u00b6","399f1814":"There are two strategies to handle missing data, either removing records with these missing values or replacing missing values with a specific value like (mean, median or mode) value of the column.","bbec6d86":"The above script splits the dataset into 85% train data and 25% test data.","9750f0e5":"# Evaluation Procedure","0ace2629":"It's obvious that the first value of this record is missing (App name) and all other values are respectively propagated backward starting from \"Category\" towards the \"Current Ver\"; and the last column which is \"Android Ver\" is left null. It's better to drop the entire recored instead of consider these unreasonable values while cleaning each column!\n","afea0413":"In this Python notebook, K-nearest neighbors and random forest algorithms will be applied to predict app ratings on Google play store.","681f8f84":"The above line drops the reference column and just keeps only one of the two columns as retaining this extra column does not add any new information for the modeling process, this line is exactly the same as setting drop_first parameter to True.","3ebdaf38":"The RandomForestRegressor class of the sklearn.ensemble library is used to solve regression problems via random forest. The most important parameter of the RandomForestRegressor class is the n_estimators parameter. This parameter defines the number of trees in the random forest."}}