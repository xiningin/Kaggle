{"cell_type":{"5039a4f2":"code","293e1b91":"code","66520074":"code","086864e7":"code","3b24007c":"code","6d47c829":"code","8809f3e1":"code","024e31ea":"code","9b30fce1":"code","7d6a7636":"markdown","6310fa18":"markdown","efea3744":"markdown","2d180bc7":"markdown"},"source":{"5039a4f2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport random\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom collections import defaultdict\nimport itertools\n\nimport torch\nfrom torch import nn, optim\nfrom torch.nn import functional as F\n\ncuda = torch.cuda.is_available()\nif cuda:\n    print(\"CUDA ON\")\nelse:\n    print('NO CUDA')","293e1b91":"text = None\nfor i in range(1,6):\n    with open(\"\/kaggle\/input\/game-of-thrones-book-files\/got{}.txt\".format(i), 'r') as f:\n        if text is None:\n            text = f.read()\n        else:\n            text += f.read()\ntext = text.replace('\\n', '')\nlen(text)","66520074":"chars = tuple(set(text)) # drop duplicates\nint2char = dict(enumerate(chars))\nchar2int = {ch: i for i, ch in int2char.items()}\nencoded = np.array([char2int[ch] for ch in text[2002:]]) # Number Encoding of Characters\nencoded[:100]","086864e7":"def one_hot(arr, n_labels):\n    res = np.zeros((arr.size, n_labels), dtype=np.float32)\n    res[np.arange(res.shape[0]), arr.flatten()]=1\n    res = res.reshape((*arr.shape, n_labels))\n    return res\n\ndef get_batch(arr, batch_size, seq_length):\n    n_batches = len(arr) \/\/ (batch_size*seq_length)\n    arr = arr[:n_batches*batch_size*seq_length] # So that it divides to zero\n    arr = arr.reshape((batch_size, -1))\n    for i in range(0, arr.shape[1], seq_length):\n        x = arr[:, i:i+seq_length]\n        y = np.zeros_like(x) # Labels: the next character\n        try:\n            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, i+seq_length]\n        except IndexError:\n            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n        yield x, y","3b24007c":"batches = get_batch(encoded, 8, 50)\nx, y = next(batches)\nx[0], y[0] # Note that y values are shifted","6d47c829":"class LSTM(nn.Module):\n    def __init__(self, tokens, n_hiddens, n_layers, drop, lr):\n        super(LSTM, self).__init__()\n        self.char = tokens\n        self.n_hiddens = n_hiddens\n        self.n_layers = n_layers\n        self.drop = drop\n        self.lr = lr\n        self.int2char = dict(enumerate(self.char))\n        self.char2int = {ch: i for i, ch in self.int2char.items()}\n        \n        self.LSTM = nn.LSTM(len(tokens), n_hiddens, n_layers, dropout=drop, batch_first=True)\n        self.dropout = nn.Dropout(drop)\n        self.fc = nn.Linear(n_hiddens, len(self.char))\n        \n    def forward(self, x, hidden):\n        output, hidden = self.LSTM(x, hidden)\n        output = self.dropout(output)\n        output = output.contiguous().view(-1, self.n_hiddens)\n        output = self.fc(output)\n        \n        return output, hidden\n    \n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        if cuda:\n            hidden = (weight.new(self.n_layers, batch_size, self.n_hiddens).zero_().cuda(),\n                     weight.new(self.n_layers, batch_size, self.n_hiddens).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.n_hiddens).zero_(),\n                     weight.new(self.n_layers, batch_size, self.n_hiddens).zero_())\n        return hidden","8809f3e1":"# ---------------\n# Hyperparams\n# ---------------\n\nn_hiddens = 512\nn_layers = 2\nbatch_size = 128\nseq_length = 100\nn_epochs = 30\ndrop = 0.5\nlr = 0.001\nclip = 5\n\nmodel = LSTM(chars, n_hiddens, n_layers, drop, lr)\n\noptimizer = optim.Adam(model.parameters(), lr=lr)\ncriterion = nn.CrossEntropyLoss()\n\nvalidation = 0.3\nval_idx = int(len(encoded)*(1-validation))\ntrain, valid = encoded[:val_idx], encoded[val_idx:]\n\nval_loss_def = np.Inf\ntra_losses=[]\nval_losses=[]\n\nif cuda:\n    model.cuda()\n    \nfor epoch in range(n_epochs):\n    \n    tra_loss, val_loss = 0,0\n    \n    h = model.init_hidden(batch_size)\n    \n    for x, y in get_batch(train, batch_size, seq_length):\n        x = one_hot(x, len(chars))\n        x, y = torch.from_numpy(x), torch.from_numpy(y)\n        if cuda:\n            x, y = x.cuda(), y.cuda()\n        h = tuple([_.data for _ in h])\n        model.zero_grad()\n        output, h = model(x, h)\n        loss = criterion(output, y.view(-1).long())\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        tra_loss += loss.item()\n        \n    with torch.no_grad():\n        h = model.init_hidden(batch_size)\n        for x,y in get_batch(valid, batch_size, seq_length):\n            x = one_hot(x, len(chars))\n            x, y = torch.from_numpy(x), torch.from_numpy(y)\n            if cuda:\n                x, y = x.cuda(), y.cuda()\n            h = tuple([_.data for _ in h])\n            model.zero_grad()\n            output, h = model(x, h)\n            loss = criterion(output, y.view(-1).long())\n            val_loss += loss.item()\n            \n    tra_losses.append(tra_loss)\n    val_losses.append(val_loss)\n            \n    print(\"Epoch: {}\/{}\".format(epoch+1, n_epochs))\n    print(\"Training Loss: {:.3f}\".format(tra_loss))\n    print(\"Validation Loss: {:.3f}\".format(val_loss))\n    if val_loss < val_loss_def:\n        torch.save(model.state_dict(), 'best_model.pt')\n        print(\"Validation Loss dropped from {:.3f} ---> {:.3f}. Model Saved.\".format(val_loss_def, val_loss))\n        val_loss_def = val_loss","024e31ea":"model.load_state_dict(torch.load('best_model.pt'))\n\ndef predict(model, chars, h=None, topk=None):\n    \n    x = np.array([[model.char2int[chars]]])\n    x = one_hot(x, len(model.char))\n    x = torch.from_numpy(x)\n    if cuda:\n        x = x.cuda()\n    h = tuple([_.data for _ in h])\n    x, h = model(x, h)\n    p = F.softmax(x, dim=1).data\n    if cuda:\n        p = p.cpu()\n    if topk is None:\n        top_ch = np.arange(len(model.char))\n    else:\n        p, top_ch = p.topk(topk)\n        top_ch = top_ch.numpy().squeeze()\n    p = p.numpy().squeeze()\n    char = np.random.choice(top_ch, p=p\/p.sum())\n    return model.int2char[char], h\n\ndef sample(model, size, prime, topk):\n    \n    if cuda:\n        model.cuda()\n        \n    with torch.no_grad():\n        chars = [c for c in prime]\n        h = model.init_hidden(1)\n        \n        for c in prime:\n            r, h = predict(model, c, h, topk)\n        chars.append(r)\n        \n        for i in range(size):\n            r, h = predict(model, chars[-1], h, topk)\n            chars.append(r)\n\n    return ''.join(chars)","9b30fce1":"sample(model, size=1500, prime='The', topk=3)","7d6a7636":"# 4. Train","6310fa18":"# 3. Model","efea3744":"# 2. Tokenization","2d180bc7":"# 1. Import"}}