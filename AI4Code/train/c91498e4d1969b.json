{"cell_type":{"2bed075a":"code","104664c9":"code","4804eace":"code","d39c467b":"code","4f4f30a8":"code","c18502fd":"code","dada2fc4":"code","2a81fafe":"code","302a6ecb":"code","0d25ebee":"code","1ee0bcb9":"code","c3ed055a":"code","815e263b":"markdown","5976daa4":"markdown","45441bac":"markdown","a2bfcf99":"markdown","66cd8996":"markdown","b8a75def":"markdown"},"source":{"2bed075a":"###################\n# Imports\n###################\n\nimport json\nimport numpy as np # Linear aljebra\n\nfrom os import listdir #Navigate in pc\nfrom os.path import isfile, join\n\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import scale\nfrom mpl_toolkits.mplot3d import Axes3D # For 3d scatterplot visualization","104664c9":"####################\n# Load data\n####################\n\npath_in = '..\/input\/abstraction-and-reasoning-challenge\/'\npath_train = path_in + \"training\/\"\npath_test = path_in + \"test\/\"\npath_evaluation = path_in + \"evaluation\/\"\npath_out = '..\/working\/kaggle\/working\\\\'\n\n# Function to open json\ndef open_json(path,name):\n    with open('%s%s'%(path,name)) as f:\n          data = json.load(f)\n    return(data)\n\n\n# File name lists\ntrain_file_list = [f for f in sorted(listdir(path_train)) if isfile(join(path_train, f))]\ntest_file_list = [f for f in sorted(listdir(path_test)) if isfile(join(path_test, f))]\nevaluation_file_list = [f for f in sorted(listdir(path_evaluation)) if isfile(join(path_evaluation, f))]\n\n# Tasks to list\ntrain_task_list = [open_json(path_train,name) for name in train_file_list]\ntest_task_list = [open_json(path_test,name) for name in test_file_list]\nevaluation_task_list = [open_json(path_evaluation,name) for name in evaluation_file_list]","4804eace":"# ATTRIBUTION to\n# BY T88 and Bo in kaggle: \n# > https:\/\/www.kaggle.com\/t88take\/check-the-purpose\n# > https:\/\/www.kaggle.com\/boliu0\/visualizing-all-task-pairs-with-gridlines#evaluation-set\n#\n# ----------------------------------------------------------------------------------------\n#\n# Some changes have been made to avoid showing the images\n#  two optional variables are added to *plot_task*\n#  for avoiding showing the plot\n#  and for saving the image\n\ndef plot_one(ax, i,train_or_test,input_or_output,task):\n    cmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n    norm = colors.Normalize(vmin=0, vmax=9)\n    \n    input_matrix = task[train_or_test][i][input_or_output]\n    ax.imshow(input_matrix, cmap=cmap, norm=norm)\n    ax.grid(True,which='both',color='lightgrey', linewidth=0.5)    \n    ax.set_yticks([x-0.5 for x in range(1+len(input_matrix))])\n    ax.set_xticks([x-0.5 for x in range(1+len(input_matrix[0]))])     \n    ax.set_xticklabels([])\n    ax.set_yticklabels([])\n    ax.set_title(train_or_test + ' '+input_or_output)\n    \n\ndef plot_task(task,show=True,savedict=None):\n    \"\"\"\n    Plots the first train and test pairs of a specified task,\n    using same color scheme as the ARC app\n    \"\"\"    \n    num_train = len(task['train'])\n    fig, axs = plt.subplots(2, num_train, figsize=(3*num_train,3*2))\n    for i in range(num_train):     \n        plot_one(axs[0,i],i,'train','input',task)\n        plot_one(axs[1,i],i,'train','output',task)        \n    plt.tight_layout()\n    if show:\n        plt.show()  \n    \n    if savedict!=None:\n        plt.savefig(savedict['path']+savedict[\"name\"]+\"_train\"+savedict['fmt'])\n        plt.close()\n        \n    num_test = len(task['test'])\n    fig, axs = plt.subplots(2, num_test, figsize=(3*num_test,3*2))\n    if num_test==1: \n        plot_one(axs[0],0,'test','input',task)\n        plot_one(axs[1],0,'test','output',task)     \n    else:\n        for i in range(num_test):      \n            plot_one(axs[0,i],i,'test','input',task)\n            plot_one(axs[1,i],i,'test','output',task)  \n    plt.tight_layout()\n    if show:\n        plt.show() \n        \n    if savedict!=None:\n        plt.savefig(savedict['path']+\"\\\\\"+savedict[\"name\"]+\"_test\"+savedict['fmt'])\n        plt.close()\n       \n    \n# This is mine\ndef plot_export(task,subtask_number,train_test):\n    fig, axs = plt.subplots(2,1, figsize=(3,3*2))\n    plot_one(axs[0],subtask_number-1,train_test,'input',task)\n    plot_one(axs[1],subtask_number-1,train_test,'output',task)        \n    \n    return(fig)","d39c467b":"# ----------------------------\n# Save all tasks in train\n# ----------------------------\n\"\"\"\ntask_counter = 1\nfor task,task_file_code in zip(train_task_list,train_file_list):\n    plot_task(task,show=False,savedict={'path': path_out,\n                                        'name': str(task_counter)+\"_\"+task_file_code,\n                                        'fmt': \".png\"})\n    if(task_counter\/5==float(task_counter\/\/5)): print(task_counter)\n    task_counter+=1\n\"\"\"","4f4f30a8":"#################################\n# Single feature extractors\n#############################\n\n# Different *task* feature extractor functions\ndef n_subtask_train(task):\n    return(len(task['train']))\n\n\n# Different *subtask* feature extractor\n\ndef check_is_subtask(subtask):\n    assert(isinstance(subtask,dict))\n    assert('input' in subtask.keys())\n    assert('output' in subtask.keys())\n\ndef n_shape(subtask,origin='input'):\n    # Make sure we have a subtask structure\n    check_is_subtask(subtask)     \n    # Constraint for origin\n    assert(origin in ['input','output'])\n\n    # return shape of origin\n    return(np.array(subtask[origin]).shape)\n\ndef n_colors(subtask,origin='input'):\n    # Make sure we have a subtask structure\n    check_is_subtask(subtask)\n    # Constraint for origin\n    assert(origin in ['input','output'])    \n\n    # return shape of origin\n    return(len(np.unique(np.array(subtask[origin]))))             \n\n\n##############################\n#  All features of a task\n###########################\n\ndef get_features_task(task,train_test='train'):\n\n    features_list = []\n    for subtask in task[train_test]:\n            aux_feature_list = []\n            # SHAPES of INPUT\/OUTPUT\n            input_shape = n_shape(subtask, origin='input')\n            # number of ROWS of subtask INPUT\n            aux_feature_list.append(input_shape[0])\n            # number of COLUMNS of subtask INPUT\n            aux_feature_list.append(input_shape[1])\n            \n            output_shape = n_shape(subtask, origin='output')\n            # number of ROWS of subtask OUTPUT\n            aux_feature_list.append(output_shape[0])\n            # number of ROWS of subtask OUTPUT\n            aux_feature_list.append(output_shape[1])\n            \n               \n            # NUMBER OF COLORS\n            # Input\n            aux_feature_list.append(n_colors(subtask,origin='input'))\n            # Output\n            aux_feature_list.append(n_colors(subtask,origin='output'))\n    \n            features_list.append(aux_feature_list)\n    return(features_list)","c18502fd":"#########################################\n#  Extract features of TRAIN tasks\n#########################################\n\nfeature_rows = []\n\ntask_idx = 1 # Counter of task, will also be added for reference\nfor task in train_task_list:\n    \n    # Add all TRAIN subtask features\n    subtask_count=1\n    for subtask_feature_row in get_features_task(task,train_test='train'):\n        feature_rows.append(['train',task_idx,subtask_count,train_file_list[task_idx-1]]+subtask_feature_row)\n        subtask_count+=1\n        \n    # Add TEST subtask features\n    feature_rows.append(['test',task_idx,1,train_file_list[task_idx-1]]+\n                       get_features_task(task,train_test='test')[0])\n\n    \n    task_idx+=1 # Add for next task number\n\n    \n# Header following the added features\ncolumns = [\"type\",\"#task\",\"#subtask\",\"task_id\",\"n_row_IN\",\"n_col_IN\",\"n_row_OUT\",\"n_col_OUT\",\"#colors_IN\",\"#colors_OUT\"]\n\n# Create a pandas dataframe to work on it\nfeatures_train = pd.DataFrame(feature_rows,columns=columns)\n\n# Add additional features\nfeatures_train['cells_IN'] = features_train['n_row_IN']*features_train['n_col_IN']\nfeatures_train['cells_OUT'] = features_train['n_row_OUT']*features_train['n_col_OUT']\nfeatures_train['ratio_cells'] = features_train['cells_OUT']\/features_train['cells_IN']\nfeatures_train['ratio_color'] = features_train['#colors_OUT']\/features_train['#colors_IN']","dada2fc4":"################################\n# Explore the features\n###########################\n\nfeatures_train.describe()","2a81fafe":"\n# Take just columns we are interested in\ncolumn_numerics = ['n_row_IN', 'n_col_IN', 'n_row_OUT', 'n_col_OUT', '#colors_IN',\n       '#colors_OUT', 'cells_IN', 'cells_OUT', 'ratio_cells', 'ratio_color']\n\n# SCALING \n# Scale data to avoid just getting principal components too similar to \n#  features in data\nX = scale(features_train[column_numerics])\nfeatures_train_scaled =pd.DataFrame(X,columns=column_numerics)\n\n###########################\n# PCA\n###########################\n\n\nprint(\"%s\\n#    EXPLAINED VARIANCE\\n%s\"%(\"#\"+\"-\"*60,\"#\"+\"-\"*60))\n\npca = PCA().fit(features_train_scaled)\nplt.plot(list(range(1,len(pca.explained_variance_ratio_)+1)),   \n    np.cumsum(pca.explained_variance_ratio_), marker=\".\")\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.title(\"Explained variance vs. Number of components\")\nplt.show()\nplt.close()\n\n\nprint(\"%s\\n#    COMPONENT CORRELATIONS\\n%s\"%(\"#\"+\"-\"*60,\"#\"+\"-\"*60))\n# PC explained variance \npca3 = PCA(n_components=3)\npca3.fit(features_train_scaled)\n# Component meaning\nplt.matshow(pca3.components_,cmap=\"bwr\",vmin=-1,vmax=1)\nplt.yticks([0,1,2],['1st Comp','2nd Comp', '3rd Comp'],fontsize=10)\nplt.colorbar()\nplt.xticks(range(1,len(column_numerics)+1),column_numerics,rotation=5,ha='left')\nplt.show()\nplt.close()\n\n\nprint(\"%s\\n#    3D PLOT\\n%s\"%(\"#\"+\"-\"*60,\"#\"+\"-\"*60))\n# 3PC plot\nX3= pca3.transform(features_train_scaled)\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\n# >> Colors depending on OUT\/IN number of cells\ncolor_list= features_train['ratio_cells']<1\nplt.scatter(X3[:, 0], X3[:, 1], X3[:,2], alpha=0.8, marker=\".\",c=color_list)\nax.set_xlabel('PC1')\nax.set_ylabel('PC2')\nax.set_zlabel('PC3')\nplt.title(\"3PC scatterplot\")\nplt.show()\nplt.close()\n\n\nprint(\"%s\\n#    2D PLOT\\n%s\"%(\"#\"+\"-\"*60,\"#\"+\"-\"*60))\n\n# 2PC plot\n# > Performe PCA\npca2 = PCA(n_components=2)\npca2.fit(features_train_scaled)\nX2 = pca2.transform(features_train_scaled)\n# > Plot dimensionality reduction to 2PCs\n# >> Colors depending on OUT\/IN number of cells\ncolor_list= features_train['ratio_cells']<1\n# >> Scatter all points, with the corresponding color. We set \n#      transparency to 20%\nplt.scatter(X2[:, 0], X2[:, 1], alpha=0.2, marker=\".\",c=color_list)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title(\"2PC Scatterplot\")\nplt.show()\nplt.close()\n\n\n","302a6ecb":"###################\n# Imports\n###################\n\nfrom bokeh.io import output_notebook, show\nfrom bokeh.layouts import column, layout\nfrom bokeh.models import ColumnDataSource, Div, Select, Slider, TextInput\nfrom bokeh.plotting import figure","0d25ebee":"# Create a pandas dataframne with the 2PC DIMENSIONALITY REDUCTION\nX2_pd = pd.DataFrame(X2,columns=[\"PC1\",\"PC2\"])\n# Add the features of the subtaks\nX2_pd['task_id']=features_train['task_id']\nX2_pd['type']=features_train['type']\nX2_pd['#task']=features_train['#task']\nX2_pd['#subtask']=features_train['#subtask']","1ee0bcb9":"# We will use the visualizations attributed to bo (https:\/\/www.kaggle.com\/boliu0)\n# \"https:\/\/www.kaggle.com\/boliu0\/visualizing-all-task-pairs-with-gridlines#training-set\"\n# in the bokeh hoover.\n\n# You should go to the previous link and copy the url link of one of the images \n#  Take all except the last digit and \".png\".\n\n# Paste that to the following variable:\nkaggle_bo_visuals_html = \"https:\/\/www.kaggleusercontent.com\/kf\/28659886\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..fc-KLYaYs_niFe6hIBdOJg.5tv8EGf6GCHzIBRwdkJiIflkyyM5_YPKkbZ7cIWNrictbL27VWHnfOgZvYg2BW0sazthBATNnRiQzE4c_k4fAaesAuHAuuwkjrAUC5C-GY-F_ur-rCl1h1Brv4GrA6gGsRoWfMebUh8bz2zk0jCf5JWufr8FYEydFknD175ODao.XlVrPiYKwoh2MwqNUiK1BQ\/__results___files\/__results___5_\"\n\n# The number of the link is calculated to match the task\nX2_pd['img_number'] = [str(3*(task_number)-2+int(task_type!=\"train\"))+\".png\" for task_number,task_type in zip(X2_pd['#task'],X2_pd['type'])]\n# The url for the task image\nX2_pd['img'] = kaggle_bo_visuals_html+X2_pd['img_number']\n\n\npath_aux = \"..\/working\/..\\\\working\\\\kaggle\\\\working\\\\\"\npath_aux =\"https:\/\/strath-my.sharepoint.com\/personal\/clb19159_uni_strath_ac_uk\/Documents\/task_plots\/\"\nX2_pd['img'] = [path_aux + str(task_number)+\"_\"+task_id+\"_\"+type_str+\".png\" for task_number,task_id,type_str in\n               zip(X2_pd['#task'],X2_pd['task_id'],X2_pd['type'])]\n\n\n\n# FILTER TASKS\nX2_filtered = X2_pd.copy()\n\nX2_filtered[X2_filtered['task_id']==\"94f9d214.json\"]['img'].unique()","c3ed055a":"######################################################  \n# BOKEH figure\n################################################\n\n# Data pandas dataframe\ntask_pd = X2_filtered\n\n# Add color for TEST TRAIN and different transparency since there are more TRAIN than TEST\ntask_pd[\"color\"] = np.where(task_pd[\"type\"] == \"test\", \"green\",\"red\")\ntask_pd[\"alpha\"] = np.where(task_pd[\"type\"] == \"test\", 0.5, 0.3)\n\n# Create Column Data Source that will be used by the plot\nsource = ColumnDataSource(data=dict(x=[], y=[], color=[], type_task=[],task_id=[],task_number=[],subtask_number=[],alpha=[],img=[]))\n\n# Hoover: \n#   This information will be show when the cursor is on a task on the Principal Component plane\n#  We add:\n#    -> an image with all the train substaks, \n#    -> the subtask number that tells which task\n#         starting from the left the point refers to, and\n#    -> the json file name\n\nTOOLTIPS = \"\"\"\n    <div>\n        <div>\n            <span style=\"font-size: 13px;\">Subtask<\/span>\n            <span style=\"font-size: 13px; color: #696;\">@subtask_number<\/span>\n            <span style=\"font-size: 13px;\">  &#160&#160&#160     <\/span>\n            <span style=\"font-size: 10px; font-weight: bold;\">@task_id<\/span>\n        <\/div>\n        <div>\n            <img\n                src=\"@img\" height=\"100\" alt=\"@img\" width=\"200\"\n                style=\"float: left; margin: 0px 15px 15px 0px;\"\n                border=\"2\"\n            ><\/img>\n        <\/div>\n    <\/div>\n\"\"\"\n\n# Create the figure\np = figure(plot_height=500, plot_width=650, title=\"\", tooltips=TOOLTIPS, sizing_mode=\"fixed\")\n# Add the tasks for the PC1, PC2 components, specify the color and the tranparency\np.circle(x=\"x\", y=\"y\", source=source, size=7, color=\"color\", line_color=None, fill_alpha=\"alpha\")\n# Add axis labels, we describe which overall feature of the tasks\n#  the Principal Components are related to.\np.xaxis.axis_label = \"PC1: Shape-color complexity\"\np.yaxis.axis_label = \"PC2: Cell_ratio\"\n# Add title\np.title.text = \"Tasks over feature principal components\"\n\n# This function is not compulsory, we can edit it to add updates if needed\ndef update():\n    df = task_pd\n    \n    source.data = dict(\n        x=df['PC1'],\n        y=df['PC2'],\n        color=df[\"color\"],\n        type_task=df[\"type\"],\n        task_id=df[\"task_id\"],\n        task_number=df['#task'],\n        subtask_number=df['#subtask'],\n        alpha=df[\"alpha\"],\n        img=df['img']\n    )\n    \n# Load data\nupdate()  # initial load of the data\n# Load notebook output\noutput_notebook()\n# Show the figure p (if we show before updating it will lead a blank plot)\nshow(p)","815e263b":"__*Observations*__\n\n\nDue to COMPONENT CORRELATIONS plot\n- We expect more complex shape-color configurations as the PC1 increases.\n\n- We expect the ratio number of cells OUT\/IN to increase as the PC2 increases, and decrease as PC2 decreases.","5976daa4":"## PCA over subtask FEATURES\nPrincipal Component Analysis","45441bac":"# Subtask feature extraction and PC visualization","a2bfcf99":"## Subtask features extraction","66cd8996":"__*Observations*__\n\nAs we expected:\n    - More complex shape-color configurations as the PC1 increases.\n\n    - The ratio number of cells OUT\/IN to increase as the PC2 increases, and decrease as PC2 decreases.\n","b8a75def":"# BOKEH for task visualization over Feature PC-plane"}}