{"cell_type":{"a3a8630a":"code","8f5bb6a1":"code","c811f099":"code","07741f26":"code","b0b7cd59":"code","bd1cc43f":"code","0250af89":"code","ea25db96":"code","fb81aec2":"code","ca0aa4cf":"code","329377ae":"code","23849114":"code","21e0aa5f":"code","42c5b697":"code","f0ab91fe":"code","00500a6e":"code","576c31d1":"code","4f31afae":"code","52748ec1":"code","4a7023dc":"code","52f6b47b":"code","6b48c33c":"code","60d94e14":"code","a13cc7c7":"code","50333025":"code","0705a4f6":"code","783d54b5":"code","ab8596e8":"code","63e2fba8":"code","cfd02b5e":"code","6a68cb35":"code","a3f833f6":"code","90de5a15":"code","57508c05":"code","c68d0747":"code","9461f981":"code","150c7d23":"code","111caa13":"code","30d9f17b":"code","f5f28727":"code","1bb8ad6c":"code","fe9326b4":"code","e68e3221":"code","04633075":"code","00d4aaaa":"code","57cd9e78":"code","ba939b48":"code","63a2f0b9":"code","707ba814":"code","7d1c72d7":"code","98c39166":"code","6db26d71":"code","529db875":"code","5f9922ad":"code","c244ea0d":"code","0ca09536":"code","84e5b2b3":"code","c344b7f0":"code","428beddc":"code","4cd6b2ee":"code","4cd11865":"code","dcbe7bb1":"code","2a58bec1":"code","47233838":"code","31e085f7":"code","20057fe6":"code","30fcb866":"code","39982ff3":"code","a97a5a82":"markdown","37db0f4e":"markdown","c4c4c492":"markdown","7cfdc4f1":"markdown","e0fdd499":"markdown","fb59bd61":"markdown","82cfbc39":"markdown","fd869e09":"markdown"},"source":{"a3a8630a":"# Asthetics\nimport warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Basic\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport json\nimport os\nimport random\nfrom tqdm.autonotebook import tqdm\nimport string\nimport re\nfrom functools import partial\nfrom pprint import pprint\n\n# Visualizations\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom wordcloud import WordCloud, STOPWORDS\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os, glob, pickle, time, gc, copy, sys\nimport yaml\nimport pandas_profiling as pdp\nimport warnings\nfrom tqdm import tqdm\nimport re\nimport json\nfrom transformers import RobertaTokenizer, RobertaModel\nimport nltk\n\ntqdm.pandas()   #tqdm is used to show any code running with a progress bar. \npd.set_option('display.max_columns', 100) # \u8868\u793a\u3067\u304d\u308b\u5217\u6570","8f5bb6a1":"!nvidia-smi","c811f099":"!ls -la \/kaggle\/input\/download-spacy","07741f26":"!pip install -U spacy[cuda110] --no-index --find-links \/kaggle\/input\/download-spacy\/spacy_cuda110\/ # gpu\n# !pip install -U spacy --no-index --find-links \/kaggle\/input\/download-spacy\/spacy\/ # cpu","b0b7cd59":"!pip install \/kaggle\/input\/download-spacy\/en_core_web_lg-3.0.0-py3-none-any.whl\/en_core_web_lg-3.0.0-py3-none-any.whl\n# roberta\n!pip install spacy-transformers --no-index --find-links \/kaggle\/input\/download-spacy\/spacy-transformers\n!pip install \/kaggle\/input\/download-spacy\/en_core_web_trf-3.0.0-py3-none-any.whl\/en_core_web_trf-3.0.0-py3-none-any.whl","bd1cc43f":"import spacy\nprint(spacy.prefer_gpu())\n# nlp = spacy.load('en_core_web_lg') # CPU optimized, not transformer\n# nlp = spacy.load('en_core_web_trf') # RoBERTa\n# finetuned_ner_nlp = spacy.load(\"..\/input\/spacy-ner-aug-lexeme\/model-best\") # lexeme table + data augmentation using the gov data\nfinetuned_ner_nlp = spacy.load(\"..\/input\/spacynerlexeme0620\/model-best\") # added lexeme table","0250af89":"def read_append_return(filename, train_files_path, output='text'):\n    \"\"\"\n    Function to read json file and then return the text data from them and append to the dataframe\n    \"\"\"\n    json_path = os.path.join(train_files_path, (filename+'.json'))\n    headings = []\n    contents = []\n    combined = []\n    with open(json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            headings.append(data.get('section_title'))\n            contents.append(data.get('text'))\n            combined.append(data.get('section_title'))\n            combined.append(data.get('text'))\n    \n    all_headings = ' '.join(headings)\n    all_contents = ' '.join(contents)\n    all_data = '. '.join(combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data\n    \ndef clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower()).strip()\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\n\ndef calc_score(y_true, y_pred, beta=0.5):\n    y_true = y_true.split('|')\n    y_pred = y_pred.split('|')\n#     print(y_true)\n#     print(y_pred)\n    TP = 0\n    FP = 0\n    FN = 0\n    for i in range(len(y_pred)):\n        FP += 1\n        for j in range(len(y_true)):\n            if jaccard(y_true[j], y_pred[i])>=0.5:\n                FP -= 1\n                break\n    for i in range(len(y_true)):\n        FN += 1\n        for j in range(len(y_pred)):\n            if jaccard(y_true[i], y_pred[j])>=0.5:\n                FN -= 1\n                TP += 1\n                break\n    return TP, FP, FN\n\ndef detect_duplicated(x):\n    for i in range(len(df_train_reduced)):\n        if x==df_train_reduced['text'][i]:\n            return df_train_reduced['Id'][i]\n    return 'no dup'\n\ndef pickle_save(path, df):\n    with open(path, 'wb') as f:\n        pickle.dump(df, f)\n\ndef pickle_load(path):\n    with open(path, 'rb') as f:\n        df = pickle.load(f)\n    return df\n\ndef ri(df):\n    return df.reset_index(drop=True)\n","ea25db96":"def det_acronym_ver1(text, keywords, TH_LEN_CHAR = 3):\n    ans = []\n    # text = re.sub(\"-\", \" \", text)\n    words = text.split()\n    for i, word in enumerate(words):\n        if word[0]!='(' or word[-1]!=')': continue # (XXX)\u306e\u5f62\u3067\u306a\u3051\u308c\u3070\u30b9\u30eb\u30fc\n        acronym_cand = word[1:-1]\n        if acronym_cand.lower()==acronym_cand: continue # \u5927\u6587\u5b57\u304c\u4e00\u3064\u3082\u306a\u3044\u306a\u3089\u30b9\u30eb\u30fc\n        len_acronym_cand = len(acronym_cand)\n        if len_acronym_cand<TH_LEN_CHAR: continue # 3\u6587\u5b57\u4ee5\u4e0b\u306a\u3089\u30b9\u30eb\u30fc\n        acronym_cand_lower = acronym_cand.lower()\n        acronym_cand_reverse = acronym_cand_lower[::-1]\n        words_cand = words[np.clip(i-len_acronym_cand*3, 0, i):i] # (XXX)\u306e\u524d\u306e\u6570\u5358\u8a9e\u3092\u62bd\u51fa\n        words_cand = ' '.join(words_cand).strip()\n        words_cand = re.sub('[^A-Za-z0-9]+', ' ', words_cand).strip()\n        words_cand = words_cand.split(\" \")\n        words_cand_reverse = words_cand[::-1]\n        idx = 0 # acronym_cand\u306e\u6587\u5b57index\n#         print(\"words_cand_reverse[0]\", words_cand_reverse[0])\n        for j, word in enumerate(words_cand_reverse):\n            if idx==len_acronym_cand:\n                break\n            if len(word)==0:\n                continue\n            if word[0].lower()==acronym_cand_reverse[idx]:\n                if idx==len_acronym_cand-1: # 1\u6587\u5b57\u76ee\u3092\u691c\u51fa\n                    idx_start = j # dataset\u540d\u306e1\u5358\u8a9e\u76ee\u306eindex\n                idx += 1\n        if idx==len_acronym_cand:\n            words_reverse = words_cand_reverse[:idx_start+1]\n            dataset = ' '.join(words_reverse[::-1]).strip().lower()\n            if detect_keywords(dataset, keywords):\n                acronym = acronym_cand\n#                 print(\"Acronym: {}\".format(acronym))\n#                 print(\"Dataset: {}\".format(dataset))\n                ans.append([acronym_cand, dataset])\n        return ans","fb81aec2":"def chunk_text0(text, CHUNK_SIZE = 512): # text\u3092\u6587\u306b\u5206\u3051keyword\u3092\u542b\u3080\u3082\u306e\u3060\u3051\u3092\u62bd\u51fa\n    sentences_i = nltk.tokenize.sent_tokenize(text)\n    sentences_i2 = []\n    for j, sentence in enumerate(sentences_i):\n        for keyword in keywords:\n            if keyword in sentence.lower():\n                sentences_i2.append(sentence)\n    token_chunks = []\n    for j, sentence in enumerate(sentences_i2):\n        token_j = tokenizer(sentence)['input_ids'][1:-1]\n        num_chunk = int(np.ceil(len(token_j)\/CHUNK_SIZE))\n        for k in range(num_chunk):\n            token_chunks.append(token_j[k*CHUNK_SIZE:(k+1)*CHUNK_SIZE])\n    text_chunks = [tokenizer.decode(chunk) for chunk in token_chunks]\n    return text_chunks\n\ndef chunk_text(text, CHUNK_SIZE = 512, keywords=[]): # text\u3092\u6587\u306b\u5206\u3051keyword\u3092\u542b\u3080\u3082\u306e\u3060\u3051\u3092\u62bd\u51fa\n    sentences_i = nltk.tokenize.sent_tokenize(text)\n    sentences_i2 = []\n    for j, sentence in enumerate(sentences_i):\n        for keyword in keywords:\n            if keyword in sentence.lower():\n                sentences_i2.append(sentence)\n    token_chunks = []\n    for j, sentence in enumerate(sentences_i2):\n        token_j = tokenizer(sentence)['input_ids'][1:-1]\n        num_chunk = int(np.ceil(len(token_j)\/CHUNK_SIZE))\n        for k in range(num_chunk):\n            token_chunks.append(token_j[k*CHUNK_SIZE:(k+1)*CHUNK_SIZE])\n    text_chunks = [tokenizer.decode(chunk) for chunk in token_chunks]\n    text_chunks2 = []\n    for j, chunk in enumerate(text_chunks):\n        for keyword in keywords:\n            if keyword in chunk.lower():\n                text_chunks2.append(chunk)\n    return text_chunks2\n\ndef get_df2(sentence_list):\n    df_ner = []\n    for i, sent in enumerate(sentence_list):\n        doc_tmp = nlp(sent) # sentence\u3067\u533a\u5207\u3063\u3066\u30d0\u30c3\u30c1\u5316\u3057\u3066\u51e6\u7406\u3055\u308c\u308b\n    #     print(doc_tmp)\n        df_tmps = [str(chunk) for chunk in doc_tmp.noun_chunks]\n        if len(df_tmps)>0:\n            df_tmps = pd.DataFrame(df_tmps, columns=['chunk'])\n            df_tmps['sentence_idx'] = i\n            df_ner.append(df_tmps)\n    if len(df_ner)>0:\n        df_ner = pd.concat(df_ner).reset_index(drop=True)\n    else:\n        df_ner = pd.DataFrame(columns=['chunk'])\n    return df_ner\n\ndef monitor_loop(i, len_data, start, verbose):\n    if (i+1)%verbose==0:\n        print(\"{}\/{}, sec: {:.1f}\".format(i+1, len_data, time.time()-starttime))","ca0aa4cf":"def check_no_capital(x):\n    try:\n        if x[1:]==x[1:].lower():\n            return True\n        return False\n    except:\n        return False\n\ndef delete_the(x):\n    try:\n        x_split = x.split()\n        if x_split[0]=='the':\n            return ' '.join(x_split[1:])\n        return x\n    except:\n        return x\n\ndef check_including_acronym(x):\n    try:\n        x_split = re.split(\"[ ']\" , x)\n        for word in x_split:\n            if word.upper()==word and word.lower()!=word: # \u5927\u6587\u5b57\u3060\u3051\u306e\u5358\u8a9e\u304c\u3042\u308b\u304b\n                return True\n        return False\n    except:\n        return True    \n    \ndef get_match(x, ref_labels, threshold=0.5):\n    for label in ref_labels:\n        if jaccard(x, label)>=threshold:\n            return label\n    return 'no_match'\n\ndef jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    if len(a)==0 or len(b)==0:\n        return 0\n    return float(len(c)) \/ (len(a) + len(b) - len(c))\n\ndef delete_keywords(x, ref):\n    try:\n        x_split = x.split()\n        x_new = []\n        for item in x_split:\n            if item.lower() not in ref:\n                x_new.append(item)\n        x_new = ' '.join(x_new)\n        return x_new\n    except:\n        return ''\n\n\ndef check_head_capital(x):\n    try:\n        if len(x)==0:\n            return False\n        return x[0]==x[0].upper() and x[0]!=x[0].lower() \n    except:\n        return False\n\ndef detect_black(x, ref):\n    try:\n        for item in ref:\n            if item in x:\n                return True\n        return False\n    except:\n        return True\n    \ndef detect_label(x, ref_label, ref_target):\n    try:\n        predict = []\n        for i in range(len(ref_label)):\n            if ref_label[i] in x:\n                predict.append(ref_target[i])\n        return np.array(predict, np.int64)\n    except:\n        return np.array([], np.int64)\n    \ndef get_label(x, ref_label, ref_target):\n    try:\n        predict = []\n        for i in range(len(x)):\n            predict.append(ref_label[ref_target==x[i]][0])\n        predict = np.unique(predict).tolist()\n        predict = '|'.join(predict)\n        return predict\n    except:\n        return ''\n    \ndef detect_acronym(x, ref_label, ref_acronym, ref_target, th=[1,2]):\n    try:\n        ans = []\n        for i in range(len(ref_label)):\n            if x.count(ref_label[i])>=th[0] and x.count(ref_acronym[i])>=th[1]:\n                ans.append(ref_target[i])\n    #             print(i)\n        return ans\n    except:\n        return []\n    \ndef detect_keywords(x, ref):\n    try:\n        for keyword in ref:\n            if keyword in x:\n                return True\n        return False\n    except:\n        return False","329377ae":"keywords = [\n    'study',\n    'studies',\n    'data',\n    'survey',\n    'panel',\n    'census',\n    'cohort',\n    'longitudinal',\n    'registry',   \n]\n\nkeywords2 = [\n    'study',\n    'studies',\n    'data',\n    'survey',\n    'panel',\n    'census',\n    'cohort',\n    'longitudinal',\n    'registry',\n    'the',\n]\nkeywords3 = [\n    'study',\n    'studies',\n    'dataset',\n    'database',\n    'survey',\n    'panel',\n    'census',\n    'cohort',\n    'longitudinal',\n    'registry',\n]\nkeywords4 = [\n    'system',\n    'center',\n    'centre',\n    'committee',\n    'documentation',\n    'entry',\n    'assimilation',\n    'explorer',\n    'regulation',\n    'portal',\n    'format',\n    'data science',\n    'analysis',\n    'management',\n    'agreement',\n    'branch',\n    'acquisition',\n    'request',\n    'task force',\n    'program',\n    'operator',\n    'office',\n    'data view',\n    'data language',\n    'mission',\n    'alliance',\n    'data model',\n    'data structure',\n    'corporation',\n]\n\nwhite_list = [\n    'ipeds',\n]\n\nkeywords5 = keywords + ['of', 'the', 'national', 'education']\n\nng_list = [\n    'national longitudinal survey',\n    'education longitudinal survey',\n    'census bureau',\n    'data appendix',\n    'data file user',\n    'supplementary data',\n    'data supplement',\n    'major field of study'\n]\nblack_list = [\n    'USGS',\n    'GWAS',\n    'ECLS',\n    'DAS',\n    'NCDC',\n    'NDBC',\n    'UDS',\n    'GTD',\n    'ISC',\n    'DGP',\n    'EDC',\n    'FDA',\n    'TSE',\n    'DEA',\n    'CDA',\n    'IDB',\n    'NGDC',\n    'JODC',\n    'EDM',\n    'FADN',\n    'LRD',\n    'DBDM',\n    'DMC',\n    'WSC',\n    ###count4##\n]","23849114":"df_train = pd.read_csv(\"..\/input\/coleridgeinitiative-show-us-the-data\/train.csv\")\nprint(df_train.shape)\ndf_train.head()","21e0aa5f":"df_train_reduced = pd.read_csv(\"..\/input\/coleridge-ext\/df_train_reduced.csv\")\nprint(df_train_reduced.shape)\ndf_train_reduced.head()","42c5b697":"df_test = pd.read_csv(\"..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv\")\nprint(df_test.shape)\ndf_test.head()","f0ab91fe":"test_files_path = \"..\/input\/coleridgeinitiative-show-us-the-data\/test\"\ndf_test['text'] = df_test['Id'].progress_apply(lambda x: read_append_return(x, train_files_path=test_files_path))\ndf_test['clean_text'] = df_test['text'].progress_apply(clean_text)\ndf_test.head()","00500a6e":"# find train data in test data\ndf_test['dup_id'] = df_test['text'].progress_apply(lambda x: detect_duplicated(x))\ndf_test['dup'] = df_test['dup_id']!='no dup'\nif len(df_test)==4:\n    df_test['dup'][0] = False\ndf_test.head()","576c31d1":"df_traintest = pd.concat([df_train_reduced, df_test[df_test['dup']==False]]).reset_index(drop=True)\nprint(df_traintest.shape)\ndf_traintest.head()","4f31afae":"if len(df_test)==4:\n    df_test_reduced = df_train_reduced.iloc[:1000][['Id', 'text', 'clean_text']].copy()\n    df_test_reduced['text'].iloc[-1] = \" \"\n    df_test_reduced['clean_text'].iloc[-1] = \" \"\nelse:\n    df_test_reduced = df_test[df_test['dup']==False].reset_index(drop=True).iloc[:][['Id', 'text', 'clean_text']]\n\nprint(df_test_reduced.shape)\ndf_test_reduced.head()","52748ec1":"if len(df_test)==4:\n    df_traintest = df_train_reduced.iloc[:100].copy()\n#     df_traintest['text'].iloc[-1] = \"\"\n#     df_traintest['clean_text'].iloc[-1] = \"\"\n#     df_traintest = df_train_reduced\nprint(df_traintest.shape)\ndf_traintest.head()","4a7023dc":"df_label = df_train['cleaned_label'].value_counts().reset_index()\ndf_label.columns = ['cleaned_label', 'count']\ndf_label['-count'] = -df_label['count']\ndf_label = df_label.sort_values(['-count', 'cleaned_label']).reset_index(drop=True)\ndf_label['target'] = np.arange(len(df_label))\ndf_label['cleaned_label'] = df_label['cleaned_label'].apply(clean_text)\n\ndf_tmp1 = df_train[['dataset_label', 'cleaned_label', 'dataset_title']]\ndf_tmp2 = df_train[['dataset_title', 'cleaned_label', 'dataset_title']]\ndf_tmp3 = df_train[['cleaned_label', 'cleaned_label', 'dataset_title']]\ndf_tmp1.columns = ['label', 'cleaned_label', 'dataset_title']\ndf_tmp2.columns = ['label', 'cleaned_label', 'dataset_title']\ndf_tmp3.columns = ['label', 'cleaned_label', 'dataset_title']\ndf_label2 = pd.concat([df_tmp1, df_tmp2, df_tmp3])\n\ndf_label2['label'] = df_label2['label'].apply(lambda x: x.lower())\ndf_label2['cleaned_label_my'] = df_label2['label'].apply(clean_text)\ndf_label2['cleaned_label'] = df_label2['cleaned_label'].apply(clean_text)\ndf_label2 = df_label2[df_label2['label'].duplicated()==False].reset_index(drop=True)\ndf_label2 = pd.merge(df_label2, df_label, on='cleaned_label', how='left')\nprint(df_label2.shape)\nprint(np.sum(pd.isna(df_label2['target'])))\n# print(df_label2[df_label2['target']==1])\ndf_label2 = df_label2.sort_values(['-count', 'label']).reset_index(drop=True)\ndf_label2['target_relabeled'] = df_label2['target']\ndf_label2['target_relabeled'][df_label2['cleaned_label']!=df_label2['cleaned_label_my']] =\\\n    np.arange(np.sum(df_label2['cleaned_label']!=df_label2['cleaned_label_my'])) + df_label2['target'].max()+1\nprint(\"not match: \", np.sum(df_label2['cleaned_label']!=df_label2['cleaned_label_my']))\nprint(\"unique cleaned_label_my: \", len(df_label2['cleaned_label_my'].unique()))\nprint(df_label2['target'].max())\nprint(df_label2['target_relabeled'].max())\ndf_label_train = df_label2\ndf_label_train.head()","52f6b47b":"df_label_train['label-keywords'] = df_label_train['cleaned_label_my'].apply(\n    lambda x: delete_keywords(x, keywords5))\ndf_label_train.head()","6b48c33c":"df_train['target'] = df_train['cleaned_label'].progress_apply(lambda x: df_label2['target'][df_label2['cleaned_label']==x.strip()].values[0])\ndf_tmp = df_train.groupby('Id')['target'].agg(lambda x: list(x)).reset_index()\ndf_tmp.columns = ['Id', 'targets']\ndf_tmp['targets'] = df_tmp['targets'].apply(lambda x: np.array([x]).flatten())\ndf_train_reduced2 = pd.merge(df_train_reduced, df_tmp, on='Id', how='left')\ndf_train_reduced2.head()","60d94e14":"acronym_list = [\n    ['National Education Longitudinal Study', 'nels'],\n#     ['NOAA Tide Gauge', 'nan'],\n    ['Sea, Lake, and Overland Surges from Hurricanes', 'slosh'],\n    ['Coastal Change Analysis Program', 'c-cap'],\n    ['Aging Integrated Database (AGID)', 'agid'],\n    [\"Alzheimer's Disease Neuroimaging Initiative (ADNI)\", 'adni'],\n    ['Baltimore Longitudinal Study of Aging (BLSA)', 'blsa'],\n    ['Agricultural Resource Management Survey', 'arms'],\n    ['Beginning Postsecondary Student', 'bps'],\n    [\"The National Institute on Aging Genetics of Alzheimer's Disease Data Storage Site (NIAGADS)\", 'niagads'],\n    ['Common Core of Data', 'ccd'],\n#     ['Survey of Industrial Research and Development', 'nan'],\n    ['Baccalaureate and Beyond', 'b&b'],\n    ['International Best Track Archive for Climate Stewardship', 'IBTrACS'],\n    ['National Teacher and Principal Survey', 'ntps'],\n    ['Higher Education Research and Development Survey', 'herd'],\n    ['Survey of Earned Doctorates', 'sed'],\n    ['School Survey on Crime and Safety', 'ssocs'],\n    ['World Ocean Database', 'wod'],\n    ['Program for the International Assessment of Adult Competencies', 'piaac'],\n    ['Early Childhood Longitudinal Study', 'ecls'],\n#     ['Survey of Graduate Students and Postdoctorates in Science and Engineering', 'nan'],\n    ['Trends in International Mathematics and Science Study', 'timss'],\n    ['Education Longitudinal Study', 'els'],\n    ['Optimum Interpolation Sea Surface Temperature', 'oisst'],\n    ['National Assessment of Education Progress', 'naep'],\n    ['High School Longitudinal Study', 'hsls'],\n    ['Survey of Doctorate Recipients', 'sdr'],\n    ['Rural-Urban Continuum Codes', 'rucc'],\n#     ['Survey of Science and Engineering Research Facilities', 'nan'],\n#     ['FFRDC Research and Development Survey', 'nan'],\n#     ['Survey of State Government Research and Development', 'nan'],\n    ['Advanced National Seismic System (ANSS) Comprehensive Catalog (ComCat)', 'comcat'],\n#     ['Census of Agriculture', 'nan'],\n    ['North American Breeding Bird Survey (BBS)', 'bbs'],\n    ['COVID-19 Open Research Dataset (CORD-19)', 'cord-19'],\n    ['Complexity Science Hub COVID-19 Control Strategies List (CCCSL)', 'cccsl'],\n#     ['Our World in Data COVID-19 dataset', 'nan'],\n    ['COVID-19 Precision Medicine Analytics Platform Registry (JH-CROWN)', 'jh-crown'],\n    ['Characterizing Health Associated Risks, and Your Baseline Disease In SARS-COV-2 (CHARYBDIS)', 'charybdis'],\n#     ['COVID-19 Deaths data', 'nan'],\n#     ['SARS-CoV-2 genome sequence', 'nan'],\n#     ['COVID-19 Image Data Collection', 'nan'],\n    ['RSNA International COVID-19 Open Radiology Database (RICORD)', 'ricord'],\n#     ['CAS COVID-19 antiviral candidate compounds dataset', 'nan'],\n]\ndf_label_train_acronym = pd.DataFrame(acronym_list, columns=['dataset_title', 'acronym'])\ndf_label_train_acronym['acronym_clean'] = df_label_train_acronym['acronym'].apply(clean_text)\ndf_label_train_acronym['target'] = np.arange(len(df_label_train_acronym))+df_label_train['target'].max()+1\ndf_label_train_acronym","a13cc7c7":"TH_LEN_SELF = 4\ndf_label_train_acronym2 = pd.merge(df_label_train_acronym, df_label_train[['cleaned_label_my', 'dataset_title']], on='dataset_title', how='left')\nprint(df_label_train_acronym2.shape)\ndf_label_train_acronym2['base'] = df_label_train_acronym2['cleaned_label_my']\ndf_label_train_acronym2['label'] = df_label_train_acronym2['acronym_clean']\ndf_label_train_acronym2['len'] = df_label_train_acronym2['acronym_clean'].apply(lambda x: len(x))\ndf_label_train_acronym2 = ri(df_label_train_acronym2[df_label_train_acronym2['len']>=TH_LEN_SELF])\nprint(df_label_train_acronym2.shape)\ndf_label_train_acronym2.head()","50333025":"df_label_train_ref = df_label_train.copy()\nprint(df_label_train_ref['dataset_title'].unique().shape)\ndf_agg = df_label_train_ref.groupby('dataset_title')['target_relabeled'].agg(min).reset_index()\ndf_agg.columns = ['dataset_title', 'target_group']\ndf_label_train_ref = pd.merge(df_label_train_ref, df_agg, on='dataset_title', how='left')\ndf_label_train_ref = df_label_train_ref[df_label_train_ref['dataset_title']!='National Education Longitudinal Study']\ndf_label_train_ref = df_label_train_ref[df_label_train_ref['dataset_title']!='Education Longitudinal Study']\ndf_label_train_ref = ri(df_label_train_ref)\nprint(df_label_train_ref['target_group'].unique().shape)\nprint(df_label_train_ref.shape)\ndf_label_train_ref.head()","0705a4f6":"df_test_reduced['det_acronym'] = df_test_reduced['text'].progress_apply(lambda x: det_acronym_ver1(x, keywords))\ndf_test_reduced.head()","783d54b5":"tmp = []\nfor i in range(len(df_test_reduced)):\n    if df_test_reduced['det_acronym'][i] is not None:\n        tmp += df_test_reduced['det_acronym'][i]\ndf_label_acronym_ver1_test = pd.DataFrame(tmp)\ndf_label_acronym_ver1_test.columns = ['acronym', 'base']\ndf_label_acronym_ver1_test['base_acronym'] = df_label_acronym_ver1_test['base']+\"|\"+df_label_acronym_ver1_test['acronym']\n\ndf_label_acronym_ver1_test_tmp = df_label_acronym_ver1_test[df_label_acronym_ver1_test['base_acronym'].duplicated()==False]\ndf_agg = df_label_acronym_ver1_test.groupby('base_acronym')['base'].agg(len).reset_index()\ncols_tmp = df_agg.columns.tolist()\ncols_tmp[-1] = 'count_BA'\ndf_agg.columns = cols_tmp\ndf_label_acronym_ver1_test = pd.merge(df_label_acronym_ver1_test_tmp, df_agg, on='base_acronym', how='left')\n\nprint(df_label_acronym_ver1_test.shape)\n# BA unique 325\ndf_label_acronym_ver1_test.head(30)","ab8596e8":"df_label_acronym_ver1_train = pd.read_csv(\"..\/input\/coleridge-ext\/df_label_acronym_ver1_all_210619_02.csv\")\nprint(df_label_acronym_ver1_train.shape)\ndf_label_acronym_ver1_train.head()","63e2fba8":"if len(df_test)==4:\n    df_label_acronym_ver1_test['acronym'].iloc[-1] = 'DUMMY'\n    df_label_acronym_ver1_test['base'].iloc[-1] = 'dummy'\n    df_label_acronym_ver1_test['base_acronym'].iloc[-1] = 'dummy|DUMMY'\ndf_label_acronym_ver1_test['test'] = True\ndf_tmp = df_label_acronym_ver1_train.copy()\ndf_tmp['count_BA_train'] = df_tmp['count_BA']\ndf_label_acronym_ver1_test_tmp = pd.merge(df_label_acronym_ver1_test, df_tmp[['base_acronym', 'count_BA_train']], on='base_acronym', how='left')\nprint(df_label_acronym_ver1_test_tmp.shape)\ndf_label_acronym_ver1_train_tmp = pd.merge(df_label_acronym_ver1_train, df_label_acronym_ver1_test[['base_acronym', 'test']], on='base_acronym', how='left')\ndf_label_acronym_ver1_train_tmp = ri(df_label_acronym_ver1_train_tmp[df_label_acronym_ver1_train_tmp['test']!=True])\nprint(df_label_acronym_ver1_train_tmp.shape)\ndf_label_acronym_ver1 = ri(pd.concat([df_label_acronym_ver1_test_tmp, df_label_acronym_ver1_train_tmp], axis=0))\nprint(df_label_acronym_ver1.shape)\ndf_label_acronym_ver1['cleaned_label'] = df_label_acronym_ver1['base'].apply(clean_text)\ndf_label_acronym_ver1['target'] = np.arange(len(df_label_acronym_ver1))\\\n    +df_label_train_acronym['target'].max()+1\ndf_label_acronym_ver1.head()","cfd02b5e":"df_label_acronym_ver1['match_train'] = df_label_acronym_ver1['base'].apply(\n    get_match, ref_labels=df_label_train['cleaned_label_my'])\ndf_label_acronym_ver1['label-keywords'] = df_label_acronym_ver1['base'].apply(\n    lambda x: delete_keywords(x, keywords5))\ndf_label_acronym_ver1['len_label-keywords'] = df_label_acronym_ver1['label-keywords'].apply(\n    lambda x: len(x.split()))\ndf_label_acronym_ver1['match_train-keywords'] = df_label_acronym_ver1['label-keywords'].apply(lambda x:\n    get_match(x, ref_labels=df_label_train['label-keywords'], threshold=0.75))\ndf_label_acronym_ver1['acronym_clean'] = df_label_acronym_ver1['acronym'].apply(clean_text)\ndf_label_acronym_ver1['match_train_acronym'] = df_label_acronym_ver1['acronym_clean'].apply(\n    lambda x: x in df_label_train_acronym['acronym_clean'].tolist()\n)\ndf_label_acronym_ver1['keyword3'] = df_label_acronym_ver1['base'].apply(lambda x: detect_keywords(x.lower(), keywords3))\ndf_label_acronym_ver1['keyword4'] = df_label_acronym_ver1['base'].apply(lambda x: detect_keywords(x.lower(), keywords4))\ndf_label_acronym_ver1['black'] = df_label_acronym_ver1['acronym'].apply(lambda x: x.upper() in black_list)\ndf_label_acronym_ver1['white'] = df_label_acronym_ver1['acronym_clean'].apply(lambda x: x in white_list)\n\ndf_label_acronym_ver1.head()","6a68cb35":"idx_tmp = (df_label_acronym_ver1['len_label-keywords']>0)\nidx_tmp = idx_tmp & (df_label_acronym_ver1['match_train-keywords']=='no_match')\nidx_tmp = idx_tmp & (df_label_acronym_ver1['match_train_acronym']==False)\n# idx_tmp = idx_tmp & (df_label_acronym_ver1['black']==False)\nidx_tmp = idx_tmp & ((df_label_acronym_ver1['keyword3'])|(df_label_acronym_ver1['keyword4']==False))\nidx_tmp = idx_tmp | (df_label_acronym_ver1['white'])\nidx_tmp = idx_tmp & (df_label_acronym_ver1['cleaned_label'].duplicated()==False)\ndf_label_acronym_ver1_2 =ri(df_label_acronym_ver1[idx_tmp])\nprint(df_label_acronym_ver1.shape)\nprint(df_label_acronym_ver1_2.shape)\nprint(df_label_acronym_ver1_2[df_label_acronym_ver1_2['cleaned_label'].duplicated()==False].shape)\ndf_label_acronym_ver1_2.head(30)","a3f833f6":"df_traintest['det_acronym'] = df_traintest['clean_text'].progress_apply(lambda x: \n    detect_acronym(x, df_label_acronym_ver1_2['cleaned_label'], \n                   df_label_acronym_ver1_2['acronym_clean'], df_label_acronym_ver1_2['target'])\n)\ndf_traintest.head()","90de5a15":"tmp = df_traintest['det_acronym'].values\ntmp = np.concatenate(tmp)\n\ndf_label_acronym_ver1_2['count'] = df_label_acronym_ver1_2['target'].apply(lambda x: np.sum(tmp==x))\ndf_label_acronym_ver1_2 = df_label_acronym_ver1_2.sort_values('count', ascending=False)\ndf_label_acronym_ver1_2","57508c05":"if len(df_test)==4:\n    TH_COUNT = 0 # 1 is best for private\nelse:\n    TH_COUNT = 1\nidx_tmp =  (df_label_acronym_ver1_2['count']>=TH_COUNT)\ndf_label_acronym_ver1_black =ri(df_label_acronym_ver1_2[idx_tmp])\nidx_tmp =  (df_label_acronym_ver1_2['count']>=TH_COUNT)\nidx_tmp = idx_tmp & (df_label_acronym_ver1_2['black']==False)\ndf_label_acronym_ver1_3 =ri(df_label_acronym_ver1_2[idx_tmp])\nprint(df_label_acronym_ver1.shape)\nprint(df_label_acronym_ver1_2.shape)\nprint(df_label_acronym_ver1_black.shape)\nprint(df_label_acronym_ver1_3.shape)\ndf_label_acronym_ver1_3.head(30)","c68d0747":"TH_COUNT = 1 # 1 is best for private\ndf_label_acronym_ver1_self = df_label_acronym_ver1_3.drop('target', axis=1)\ndf_tmp = ri(df_label_acronym_ver1_self[df_label_acronym_ver1_self['acronym_clean'].duplicated()==False])\ndf_tmp['target'] = np.arange(len(df_tmp)) + df_label_acronym_ver1_3['target'].max()+1\ndf_label_acronym_ver1_self = pd.merge(\n    df_label_acronym_ver1_self, df_tmp[['acronym_clean', 'target']], on='acronym_clean', how='left')\nprint(df_label_acronym_ver1.shape)\nprint(df_label_acronym_ver1_self.shape)\nprint(df_label_acronym_ver1_self[df_label_acronym_ver1_self['cleaned_label'].duplicated()==False].shape)\ndf_label_acronym_ver1_self.head(30)","9461f981":"df_govt = pd.read_csv(\"..\/input\/coleridge-ext\/datasets_govt.csv\")\nprint(df_govt.shape)\ndf_govt.head()","150c7d23":"df_govt['keyword'] = df_govt['title'].progress_apply(lambda x: detect_keywords(x, keywords))\nidx_tmp = df_govt['keyword']\ndf_govt2 = ri(df_govt[df_govt['keyword']])\ndf_govt2['cleaned_label'] = df_govt2['title'].progress_apply(clean_text)\ndf_govt2['match_train'] = df_govt2['cleaned_label'].progress_apply(\n    get_match, ref_labels=df_label_train['cleaned_label_my'])\n\nidx_tmp = df_govt2['keyword']\nprint(df_govt2[idx_tmp].shape)\ndf_govt2[idx_tmp].head()","111caa13":"df_govt2['len'] = df_govt2['cleaned_label'].progress_apply(lambda x: len(x.split()))\nidx_tmp = df_govt2['match_train']=='no_match'\nidx_tmp = df_govt2['len']>=3\ndf_govt3 = ri(df_govt2[idx_tmp])\nprint(df_govt2[idx_tmp].shape)\ndf_govt2[idx_tmp].head()","30d9f17b":"df_govt3['match_ver1'] = df_govt2['cleaned_label'].progress_apply(\n    get_match, ref_labels=df_label_acronym_ver1_3['cleaned_label'])\nidx_tmp = df_govt3['match_ver1']=='no_match'\nidx_tmp = idx_tmp & (df_govt3['match_train']=='no_match')\ndf_govt4 = ri(df_govt3[idx_tmp])\nprint(df_govt4.shape)\ndf_govt4.head()","f5f28727":"df_govt4['target'] = np.arange(len(df_govt4)) + df_label_acronym_ver1_self['target'].max()+1\ndf_govt4.head()","1bb8ad6c":"idx_tmp = df_govt4['len']<=10\ndf_govt5 = df_govt4[idx_tmp]\ndf_govt5 = ri(df_govt5.sort_values('len'))\nprint(df_govt5.shape)\ndf_govt5.head()","fe9326b4":"def func(x, ref):\n    index_tmp = x['index']\n    if index_tmp==0:\n        return 'no_match'\n    ref = ref[:index_tmp-1]\n    for item in ref:\n        if jaccard(x['cleaned_label'], item)>=0.5:\n            return item\n    return 'no_match'\nref_list = df_govt5['cleaned_label'].tolist()\ndf_govt5['index'] = np.arange(len(df_govt5))\nif len(df_test)==4:\n    df_govt5 = df_govt5.iloc[:1000]\ndf_govt5['match_in_self'] = df_govt5.progress_apply(lambda x: func(x, ref_list), axis=1)","e68e3221":"idx_tmp = df_govt5['match_in_self']=='no_match'\ndf_govt6 = ri(df_govt5[idx_tmp])\nprint(df_govt6.shape)\ndf_govt6.head()","04633075":"df_traintest['det_govt'] = df_traintest['clean_text'].progress_apply(lambda x: \n    detect_label(x.lower(), ref_label=df_govt6['cleaned_label'].values, \n                 ref_target=df_govt6['target'].values))","00d4aaaa":"tmp = df_traintest['det_govt'].values\ntmp = np.concatenate(tmp)\n\ndf_govt6['count'] = df_govt6['target'].apply(lambda x: np.sum(tmp==x))\ndf_govt6 = ri(df_govt6.sort_values('count', ascending=False))\ndf_govt6","57cd9e78":"TH_COUNT = 2\nidx_tmp = df_govt6['count']>=TH_COUNT\nidx_tmp = idx_tmp & (df_govt6['match_train']=='no_match')\ndf_govt7 = ri(df_govt6[idx_tmp])\nprint(df_govt7.shape)\ndf_govt7.head(30)","ba939b48":"tokenizer = pickle_load(\"..\/input\/coleridge-ext\/tokenizer_210613_01.pkl\")","63a2f0b9":"df_label_ref = df_label_acronym_ver1_3[['cleaned_label', 'target']].copy()\ndf_tmp = df_label_train_ref[['cleaned_label_my', 'target_group']]\ndf_tmp.columns = ['cleaned_label', 'target']\ndf_label_ref = ri(pd.concat([df_label_ref, df_tmp], axis=0))\nprint(df_label_ref.shape)\ndf_label_ref.head()","707ba814":"df_test_reduced['det_ref'] = df_test_reduced['clean_text'].progress_apply(lambda x: \n    detect_label(x, df_label_ref['cleaned_label'], df_label_ref['target'].values)\n)\ndf_test_reduced['len_det'] = df_test_reduced['det_ref'].apply(len)\nprint(df_test_reduced[df_test_reduced['len_det']>0].shape) # 340,6\ndf_test_reduced[df_test_reduced['len_det']>0].head(30)","7d1c72d7":"def get_include(x, ref):\n    try:\n        for item in ref:\n            if item in x:\n                return item\n        return 'no_include'\n    except:\n        return 'no_include'\n    \ndef get_include2(x, ref):\n    try:\n        for item in ref:\n            if x in item:\n                return item\n        return 'no_include'\n    except:\n        return 'no_include'\n    \nnels_els_list = [\n    'national education longitudinal study',\n    'education longitudinal study',\n]","98c39166":"%%time\ni = 41\nBATCH_SIZE = 64\ndef get_ner_pred(x):\n    try:\n        text = x['text']\n        sentence_list = chunk_text(text, keywords=keywords) # keyword\u542b\u3080\u6587\u306e\u307f\u306echunk\u3092\u4f5c\u6210\n        det_ref = x['det_ref']\n        labels_det_ref = []\n        for target in det_ref:\n            labels_det_ref.append(df_label_ref['cleaned_label'][df_label_ref['target']==target].values[0])\n        labels_det_ref = np.unique(labels_det_ref).tolist()\n\n        num_batch = int(np.ceil(len(sentence_list)\/BATCH_SIZE))\n        naun_chunks = []\n        for j in range(num_batch):\n            batch = sentence_list[j*BATCH_SIZE:(j+1)*BATCH_SIZE]\n            for k, sent in enumerate(batch):\n                naun_chunks_k = list(finetuned_ner_nlp(sent).ents) # sentence\u3067\u533a\u5207\u3063\u3066\u30d0\u30c3\u30c1\u5316\u3057\u3066\u51e6\u7406\u3055\u308c\u308b\n                naun_chunks += naun_chunks_k\n        naun_chunks = [str(chunk) for chunk in naun_chunks]\n        naun_chunks2 = np.unique(naun_chunks)\n        df_chunks = pd.DataFrame(naun_chunks2, columns=['chunk'])\n        df_chunks['chunk-the'] = df_chunks['chunk'].apply(delete_the)\n        df_chunks['cleaned_label'] = df_chunks['chunk-the'].apply(clean_text)\n        df_chunks['len'] = df_chunks['cleaned_label'].apply(lambda x: len(x.split()))\n        df_chunks['det_already'] = df_chunks['cleaned_label'].apply(lambda x: x in labels_det_ref)\n        df_chunks['match_ref'] = df_chunks['cleaned_label'].apply(\n            lambda x: get_match(x, ref_labels=df_label_ref['cleaned_label'], threshold=0.5)\n        )\n        df_chunks['match_nels_els'] = df_chunks['cleaned_label'].apply(\n            lambda x: get_match(x, ref_labels=nels_els_list, threshold=0.5)\n        )\n        df_chunks['include_ref'] = df_chunks['cleaned_label'].apply(\n            lambda x: get_include(x, ref=df_label_ref['cleaned_label'])\n        )\n        \n        df_chunks['include_ref2'] = df_chunks['cleaned_label'].apply(\n            lambda x: get_include2(x, ref=df_label_ref['cleaned_label'])\n        )\n        idx_tmp = df_chunks['cleaned_label'].duplicated()==False\n        idx_tmp = idx_tmp & (df_chunks['len']>=3)\n        idx_tmp = idx_tmp & (df_chunks['det_already']==False)\n        idx_tmp = idx_tmp & (df_chunks['match_ref']!='no_match')\n        idx_tmp = idx_tmp & (df_chunks['match_nels_els']=='no_match')\n        idx_tmp = idx_tmp & (df_chunks['include_ref']=='no_include')\n        idx_tmp = idx_tmp & (df_chunks['include_ref2']=='no_include')\n        df_chunks2 = ri(df_chunks[idx_tmp])\n\n        pred_ner = \"|\".join(df_chunks2['cleaned_label'].values.tolist())\n#         pred_ner = 'A'\n        return pred_ner\n    except:\n        return ''\n\ndf_test_reduced['pred_ner'] = \"\"\ndf_test_reduced['pred_ner'].iloc[:20] = df_test_reduced.iloc[:20].progress_apply(get_ner_pred, axis=1)\ndf_test_reduced.iloc[10:20]","6db26d71":"\"\"\"\nver3\n10 joint center for political studies\n14 national study of postgraduate faculty\n\n\"\"\"\ndf_test_reduced[df_test_reduced['pred_ner']!='']","529db875":"df_label_train_ref['cleaned_label'][df_label_train_ref['target_group']==3]","5f9922ad":"label_tmp = 'baltimore longitudinal study on aging'\nfor i in range(len(df_label_ref)):\n    label_i = df_label_ref['cleaned_label'][i]\n    if jaccard(label_i, label_tmp)>=0.5:\n        print(label_i)","c244ea0d":"def pred_dup(x):\n    df_tmp = df_train_reduced2[df_train_reduced2['text']==x]\n    if len(df_tmp)>0:\n        label_list = df_tmp['targets'].values[0]\n    else:\n        label_list = np.zeros(0, np.int64)\n    return label_list\n\ndf_test['det_dup'] = df_test['text'].apply(lambda x: pred_dup(x))\ndf_test.head()","0ca09536":"df_test['det_train'] = df_test['clean_text'].progress_apply(lambda x: \n    detect_label(x.lower(), ref_label=df_label_train['label'].values, \n                 ref_target=df_label_train['target_relabeled'].values))","84e5b2b3":"df_test['det_train_acronym'] = df_test['clean_text'].progress_apply(lambda x: \n    detect_acronym(x, df_label_train_acronym2['base'], \n                   df_label_train_acronym2['acronym_clean'], \n                   df_label_train_acronym2['target'], th=[1,2])\n)\ndf_test.head()","c344b7f0":"df_test['det_govt'] = df_test['clean_text'].progress_apply(lambda x: \n    detect_label(x.lower(), ref_label=df_govt7['cleaned_label'].values, \n                 ref_target=df_govt7['target'].values))","428beddc":"df_test['det_acronym'] = df_test['clean_text'].progress_apply(lambda x: \n    detect_acronym(x, df_label_acronym_ver1_3['cleaned_label'], \n                   df_label_acronym_ver1_3['acronym_clean'], df_label_acronym_ver1_3['target'], th=[1,0])\n)\ndf_test.head()","4cd6b2ee":"df_test['det_acronym_self'] = df_test['clean_text'].progress_apply(lambda x: \n    detect_acronym(x, df_label_acronym_ver1_self['cleaned_label'], \n                   df_label_acronym_ver1_self['acronym_clean'], df_label_acronym_ver1_self['target'], th=[1,1])\n)\ndf_test.head()","4cd11865":"df_test['det_ref'] = df_test['clean_text'].progress_apply(lambda x: \n    detect_label(x, df_label_ref['cleaned_label'], df_label_ref['target'].values)\n)","dcbe7bb1":"df_test['pred_ner'] = df_test.progress_apply(get_ner_pred, axis=1)\ndf_test.head()","2a58bec1":"df_test['pred_det'] = df_test.progress_apply(lambda x: \n    np.sort(np.unique(np.concatenate([\n        x['det_acronym_self'],\n        x['det_acronym'],\n        x['det_govt'],\n#         x['det_dup'],\n#         x['det_train'],\n#         x['det_train_acronym'],\n    ]))).astype(np.int64), axis=1\n)\ndf_test.head()","47233838":"df_tmp1 = df_label_acronym_ver1_self[['acronym_clean', 'target']]\ndf_tmp1.columns = ['label', 'target']\ndf_tmp2 = df_label_acronym_ver1_3[['cleaned_label', 'target']]\ndf_tmp2.columns = ['label', 'target']\ndf_tmp3 = df_govt7[['cleaned_label', 'target']]\ndf_tmp3.columns = ['label', 'target']\ndf_tmp4 = df_label_train[['cleaned_label_my', 'target']]\ndf_tmp4.columns = ['label', 'target']\ndf_tmp5 = df_label_train_acronym2[['acronym_clean', 'target']]\ndf_tmp5.columns = ['label', 'target']\ndf_label_all = pd.concat([df_tmp1, df_tmp2, df_tmp3, df_tmp4, df_tmp5]).reset_index(drop=True)\ndf_label_all = ri(df_label_all[df_label_all['target'].duplicated()==False])\ndf_label_all.tail()","31e085f7":"df_test['PredictionString'] = df_test['pred_det'].progress_apply(lambda x: \n    get_label(x, df_label_all['label'].values, df_label_all['target'].values)\n)\ndf_test['PredictionString'] = df_test['PredictionString'] + '|' + df_test['pred_ner']\ndf_test['PredictionString'] = df_test['PredictionString'].apply(lambda x: x.strip('|'))\ndf_test.head()","20057fe6":"df_sub = df_test[['Id', 'PredictionString']]\ndf_sub.to_csv(\"submission.csv\", index=None)\ndf_sub.head()","30fcb866":"for i in range(len(df_sub.iloc[:10])):\n    print(df_sub['PredictionString'][i])","39982ff3":"for i in range(len(df_sub.iloc[:10])):\n    pred_list = df_sub['PredictionString'][i].split('|')\n#     print(pred_list, len(pred_list))\n    if pred_list[0]!='':\n        for j in range(len(pred_list)):\n            print(\"{:4d}: {}\".format(i, pred_list[j]))","a97a5a82":"# train label acronym","37db0f4e":"# Detect Acronym ver1 self","c4c4c492":"# Govt","7cfdc4f1":"# Test prediction","e0fdd499":"# Detect Acronym ver1","fb59bd61":"# Data loading","82cfbc39":"# NER\ndet1, train\u306e\u8868\u8a18\u3086\u308c\u691c\u51fa  \ntrain\u306fdataset\u540d\u3067\u30b0\u30eb\u30fc\u30d4\u30f3\u30b0\u3059\u308b\nels, nels\u306f\u691c\u51fa\u3057\u306a\u3044\u3002","fd869e09":"# train label"}}