{"cell_type":{"4fe96fa5":"code","bac245db":"code","c035f8dd":"code","69348bb1":"code","1ba50ae8":"code","7a92e875":"code","f9609476":"code","83403113":"code","84c62a53":"code","89f217e0":"code","171f0778":"code","d501fb7d":"code","1ea935f1":"code","1b7b5492":"code","fb2d20e3":"code","0f56a404":"code","3eb5b285":"code","dd8142cc":"code","7786806d":"code","73c3ebf0":"code","c43fd081":"code","464fb37f":"code","c76607fa":"code","0899e2f7":"code","6c5fab68":"code","99f3d8cc":"code","95712d38":"code","452bbeb2":"code","83737260":"code","46576748":"code","61b789b7":"code","4a8e79b7":"code","b1b58188":"code","702bef11":"code","7d10ddc8":"code","afcc7f96":"code","f99df622":"code","a628c3de":"markdown","e7d7f468":"markdown","6510758c":"markdown","5df6155f":"markdown","f29a2084":"markdown","21b526c9":"markdown","2d154d99":"markdown","39114349":"markdown","c216d6ce":"markdown","e2f1c7dd":"markdown","3e8ba613":"markdown","dba2f45e":"markdown","1cda000e":"markdown","604f47ad":"markdown","1ffc6875":"markdown","f1f76de4":"markdown","ac47a472":"markdown","c69df363":"markdown"},"source":{"4fe96fa5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nimport os\n\nfrom sklearn.model_selection import train_test_split\n\npath_list = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        path_list.append(os.path.join(dirname, filename))\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bac245db":"!pip install --upgrade tensorflow -q\nimport tensorflow as tf","c035f8dd":"tf.__version__","69348bb1":"traincsv = pd.read_csv(path_list[1])\ntraincsv.head(1)","1ba50ae8":"def dfmaker(dataframe, train=True):\n    df = dataframe.copy()\n    \n    age = df['Age']\n    df.loc[(age<20),['Age']] = 10\n    df.loc[(20<=age)&(age<30), ['Age']] = 20\n    df.loc[(30<=age)&(age<40), ['Age']] = 30\n    df.loc[(40<=age)&(age<50), ['Age']] = 40\n    df.loc[(50<=age)&(age<60), ['Age']] = 50\n    df.loc[(60<=age)&(age<70), ['Age']] = 60\n    df.loc[(70<=age)&(age<80), ['Age']] = 70\n    df.loc[(80<=age), ['Age']] = 80\n    \n    if train:\n        df.drop(['Id'], axis=1, inplace=True)\n        \n        return df\n    else:\n        Id = df.pop('ID')\n        \n        return df, Id","7a92e875":"dftrain = dfmaker(traincsv)\ndftrain.head(1)","f9609476":"income_label = []\n\nincome_label.append(dftrain['Income'].quantile(q=0.1))\nincome_label.append(dftrain['Income'].quantile(q=0.25))\nincome_label.append(dftrain['Income'].quantile(q=0.5))\nincome_label.append(dftrain['Income'].quantile(q=0.75))\nincome_label.append(dftrain['Income'].quantile(q=0.9))\nincome_label.append(dftrain['Income'].max())\nincome_label.sort()","83403113":"print(income_label)","84c62a53":"def make_income_label(df, label_list):\n    x = df['Income']\n    df.loc[x<=label_list[0], ['Income']] = int(label_list[0])\n    for num in range(1,len(label_list)-1):\n        df.loc[(label_list[num-1]<x)&(x<=label_list[num]), ['Income']] = int(label_list[num])\n    df.loc[(label_list[-2]<x), ['Income']] = int(label_list[-1])","89f217e0":"make_income_label(dftrain, income_label)","171f0778":"dftrain['Income'].unique()","d501fb7d":"for column in dftrain.columns:\n    if column != 'Risk_Flag':\n        sns.barplot(data=dftrain, x=column, y='Risk_Flag')\n        plt.show()","1ea935f1":"dftrain.drop(['CURRENT_HOUSE_YRS'], axis=1, inplace=True)","1b7b5492":"train, val = train_test_split(dftrain, test_size=0.2)","fb2d20e3":"dftrain_target = train.pop('Risk_Flag')\ndftrain_inputs = train\n\ndfval_target = val.pop('Risk_Flag')\ndfval_inputs = val","0f56a404":"def set_layer(df): \n    inputs = {}\n    encoded_features = []\n    numeric_col = ['Age','Income']\n\n    for col in df.columns:\n        if df[col].dtype == 'object':\n            vocab = sorted(list(df[col].unique()))\n            inputs[col] = tf.keras.Input(name=col, shape=(), dtype=tf.string)\n            lookup = tf.keras.layers.StringLookup(vocabulary=vocab)\n            value_index = lookup(inputs[col])\n            \n            embedding_dims = int(math.sqrt(lookup.vocabulary_size()))\n            embedding = tf.keras.layers.Embedding(input_dim=lookup.vocabulary_size(), output_dim=embedding_dims)\n            encoded_feature = embedding(value_index)\n            encoded_features.append(encoded_feature)\n        else:\n            inputs[col] = tf.keras.Input(name=col, shape=(), dtype=tf.float32)\n            if col in numeric_col:\n                bound = sorted(list(df[col].unique()))\n                lookup = tf.keras.layers.IntegerLookup(vocabulary=bound)\n                value_index = lookup(inputs[col])\n                \n                embedding_dims = int(math.sqrt(lookup.vocabulary_size()))\n                embedding = tf.keras.layers.Embedding(input_dim=lookup.vocabulary_size(), output_dim=embedding_dims) \n                encoded_feature = embedding(value_index)\n                encoded_features.append(encoded_feature)\n            else:\n                encoded_feature = inputs[col]\n                if inputs[col].shape[-1] is None:\n                    encoded_feature = tf.expand_dims(encoded_feature, -1)\n                encoded_features.append(encoded_feature)\n                \n    all_features = tf.keras.layers.concatenate(encoded_features)\n                \n            \n    return inputs, all_features","3eb5b285":"class DecisionTree(tf.keras.Model):\n    def __init__(self, depth, num_features, used_features_rate, num_classes):\n        super(DecisionTree, self).__init__()\n        self.depth = depth\n        self.num_leaves = 2**depth\n        self.num_classes = num_classes\n        \n        num_used_features = int(num_features*used_features_rate)\n        one_hot = np.eye(num_features)\n        sampled_feature_indicies = np.random.choice(np.arange(num_features), num_used_features, replace=False)\n        self.used_feature_mask = one_hot[sampled_feature_indicies]\n        \n        self.pi = tf.Variable(\n                    initial_value=tf.random_normal_initializer()(shape=[self.num_leaves, self.num_classes]),\n                    dtype='float32', trainable=True)\n        \n        self.decision_fn = tf.keras.layers.Dense(units=self.num_leaves, activation='sigmoid', name='decision')\n    \n    @tf.function\n    def call(self, features):\n        batch_size = tf.shape(features)[0]\n        \n        features = tf.matmul(features, self.used_feature_mask, transpose_b=True)\n        decisions = tf.expand_dims(self.decision_fn(features), axis=2)\n        decisions = tf.keras.layers.concatenate([decisions, 1-decisions], axis=2)\n        \n        mu = tf.ones([batch_size,1,1])\n        \n        begin_idx = 1\n        end_idx = 2\n        \n        \n        for level in range(self.depth):\n            mu = tf.reshape(mu, [batch_size, -1, 1])\n            mu = tf.tile(mu,(1,1,2))\n            \n            level_decisions = decisions[:, begin_idx:end_idx ,:]\n            mu = mu * level_decisions\n            end_idx = begin_idx + 2**(level+1)\n            \n        mu = tf.reshape(mu, [batch_size, self.num_leaves])\n        prob = tf.keras.activations.softmax(self.pi)\n        \n        outputs = tf.matmul(mu, prob)\n        return outputs","dd8142cc":"def make_tree_model(depth=None, used_features_rate=None, num_classes=None, input_df=None):\n    \n    inputs, features = set_layer(input_df)\n    \n    features = tf.keras.layers.BatchNormalization()(features)\n    num_features = features.shape[1]\n    \n    tree = DecisionTree(depth, num_features, used_features_rate, num_classes)\n    \n    outputs = tree(features)\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model","7786806d":"risk_tree = make_tree_model(depth=10, used_features_rate=1, num_classes=2, input_df=dftrain_inputs)","73c3ebf0":"# rankdir='LR' is used to make the graph horizontal.\ntf.keras.utils.plot_model(risk_tree, show_shapes=True, rankdir=\"LR\")","c43fd081":"initial_learning_rate = 0.01\nlr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate, decay_steps=10000,decay_rate=0.96,\n                                                             staircase=True)\n\nloss_object = tf.keras.losses.CategoricalCrossentropy(from_logits=False, reduction=tf.keras.losses.Reduction.AUTO)\noptimizer = tf.keras.optimizers.Adam(learning_rate = lr_schedule)\ntrain_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\ntrain_loss = tf.keras.metrics.Mean(name='train_loss')\n\n@tf.function\ndef train_step(x, y):\n    with tf.GradientTape() as tape:\n        y_pred = risk_tree(x)\n        loss = loss_object(y, y_pred)\n    gradients = tape.gradient(loss, risk_tree.trainable_variables)\n    optimizer.apply_gradients(zip(gradients, risk_tree.trainable_variables))\n    \n    train_loss(loss)\n    train_accuracy(y, y_pred)\n    \nrisk_tree.compile(optimizer=optimizer,\n                 loss=loss_object,\n                 metrics=['accuracy'])","464fb37f":"def df_to_ds(dfinput, dfoutput=None, shuffle=True):\n    if dfoutput is None:\n        ds = tf.data.Dataset.from_tensor_slices((dict(dfinput)))\n    else:\n        ds = tf.data.Dataset.from_tensor_slices((dict(dfinput), dfoutput))\n    if shuffle:\n        ds = ds.shuffle(len(dfinput))\n    ds = ds.batch(512)\n    ds = ds.prefetch(1)\n    \n    return ds   ","c76607fa":"def onehot_label(target_df, need_label_list=False):\n    label_list = sorted(list(target_df.unique()))\n    onehot = None\n    \n    for data in target_df.to_list():\n        zeros = np.zeros((len(label_list)), dtype=np.int64)\n        for num, label in enumerate(label_list):\n            if data == label:\n                zeros[num] = 1\n                if onehot is None:\n                    onehot = zeros\n                else:\n                    onehot = np.vstack((onehot, zeros))\n                    \n    if need_label_list:\n        return onehot, label_list\n    else:\n        return onehot","0899e2f7":"train_target = onehot_label(dftrain_target)\nval_target = onehot_label(dfval_target)","6c5fab68":"trainds = df_to_ds(dftrain_inputs, dfoutput=train_target)\nvalds = df_to_ds(dfval_inputs, dfoutput=val_target)","99f3d8cc":"EPOCHS = 30\nfor epoch in range(EPOCHS):\n    train_accuracy.reset_states()\n    train_loss.reset_states()\n    for x, y in trainds:\n        loss = train_step(x,y)\n        \n    if (epoch+1)%10 == 0:\n        print(f'Epochs : {epoch+1}  Loss : {train_loss.result():0.3f}  Accuracy : {train_accuracy.result():0.3f}')","95712d38":"forecast = risk_tree.predict(trainds)","452bbeb2":"expect_risk = []\nfor ex in forecast:\n    order = np.argmax(ex)\n    expect_risk.append(int(order))\n    \nprint(expect_risk[:300])","83737260":"risk_tree.evaluate(valds)","46576748":"testcsv = pd.read_csv('\/kaggle\/input\/loan-prediction-based-on-customer-behavior\/Test Data.csv')\ndftest, Id = dfmaker(testcsv, train=False)\ndftest.head(1)","61b789b7":"make_income_label(dftest, income_label)\ndftest['Income'].unique()","4a8e79b7":"dftest.drop(['CURRENT_HOUSE_YRS'], axis=1, inplace=True)","b1b58188":"testds = df_to_ds(dftest, shuffle=False)","702bef11":"testfore = risk_tree.predict(testds)\n\ntest_risk_exp = []\nfor ex in testfore:\n    order = np.argmax(ex)\n    test_risk_exp.append(int(order))\n    \nprint(test_risk_exp[:300])","7d10ddc8":"sub = pd.DataFrame(data=test_risk_exp, index=Id, columns=['Risk_Flag'])\nsub.head(5)","afcc7f96":"sub.groupby('Risk_Flag')['Risk_Flag'].count().plot.bar()","f99df622":"tf.keras.backend.clear_session()","a628c3de":"#### Convert Income column to categorical\n- I'm using qunatile for making categori\n- q=0.1 means that get data is loacted at 10% of data, started from minimum value.\n- making list for using this categori values when I convert test set","e7d7f468":"#### Making Model's Input and encoding layer","6510758c":"#### Making Model","5df6155f":"## USING Decision Tree for Prediction\n- ### TensorFlow 2.6.0 is required.","f29a2084":"#### Let's Train!","21b526c9":"#### Splist Train and Val","2d154d99":"#### Define Loss and Optimizer\n- I'm going to use one-hot for label, so Loss should be 'CategoricalCrossentropy'","39114349":"#### Divide features and target","c216d6ce":"#### Define DecisionTree","e2f1c7dd":"#### LET's load dataset","3e8ba613":"#### Drop Current_House_Yrs","dba2f45e":"#### Convert target to have one_hot","1cda000e":"#### Check the forecast values\n- I Used Shuffle at df_to_ds function. If you want to check those predicted values are equal to original, using shuffle = False or split your trainds, train and val.","604f47ad":"#### Convert dataframe to dataset(tensor)","1ffc6875":"#### Check the model works well at Unseen data\n- about 88% accuracy at val","f1f76de4":"#### Let's predict Test Set","ac47a472":"#### Define dfmaker function\n- First I'm going to convert numeric age column to categorical column. \n- Aftert that, if train set is a input dataframe, drop Id. else pop 'Id column'","c69df363":"#### Let's see Connections between Risk_Flag and the others"}}