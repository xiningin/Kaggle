{"cell_type":{"0aad00f3":"code","6fe5d2d2":"code","0bda91fe":"code","fd275c0c":"code","5d243ba3":"code","60b89845":"code","ef33c87e":"code","72b564f6":"code","1b62497f":"code","4eb0333c":"code","acab261c":"code","3564e817":"code","3e8cb541":"code","f8b56d03":"code","042b1492":"code","e34132b4":"code","ce8a819e":"code","a2b20045":"code","76df0bf0":"code","3b3a3a3e":"code","9f5a4f03":"code","add720fd":"code","af97598b":"code","7a0d6752":"code","d108736b":"code","4ad3b931":"code","34bd4a3e":"code","00cbd6f9":"code","f405ace6":"code","287444d8":"code","07c68462":"code","4a23e2a0":"code","26d733df":"code","c3d87366":"code","3fe8bfdd":"code","58aca047":"code","e5b18449":"code","c6afc61d":"code","a41789d2":"code","cbb8b96d":"code","5324fe9f":"code","268442a4":"code","6e381f9d":"code","e0196d6e":"code","070d2fc6":"code","66a9efa4":"code","49b0b926":"code","09c399ee":"code","5ba1bf6b":"code","ff9c39ba":"code","7bfa250d":"code","f9edc9a7":"code","5e633c50":"code","2fabb51b":"code","d14dfd41":"code","0d27f144":"code","e7009cc0":"code","a813a553":"markdown","d1a99b5d":"markdown","a4988415":"markdown","0588d0b3":"markdown"},"source":{"0aad00f3":"!pip install spacy\n!python -m spacy download en_core_web_sm\n!pip install afinn\n!pip install xgboost","6fe5d2d2":"import numpy as np # linear algebra\nimport pandas as pd\nimport nltk\nimport string\nimport re\nimport spacy\nfrom nltk.corpus import stopwords\nfrom nltk import sent_tokenize, word_tokenize, TweetTokenizer \nfrom nltk import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\nnlp = spacy.load('en_core_web_sm')\nfrom afinn import Afinn\naf = Afinn()\nfrom sklearn import preprocessing\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier","0bda91fe":"nltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')","fd275c0c":"path=\"..\/input\/biased-text\/\"","5d243ba3":"train_data = pd.read_csv(path+\"biased.word.test\",delimiter='\\t',encoding='latin-1',header=None)","60b89845":"train_data=train_data.iloc[:,3:5]\ntrain_data","ef33c87e":"train_data.columns=['before','after']","72b564f6":"train_data","1b62497f":"def preprocess(text):\n    text = re.sub('[()!@#$.,;\"]', '', text)\n    text = re.sub('[(]', '', text)\n    text = re.sub('[)]', '', text)\n    text = re.sub(r'[0-9]+', '', text)\n    text = text.lower()\n    text = text.split()\n    text = ' '.join(text)\n    text =text.strip()\n    return text","4eb0333c":"X=[]\nY=[]","acab261c":"for i in range(len(train_data)):\n    X.append(preprocess(train_data['before'][i]))\n    Y.append(preprocess(train_data['after'][i]))\n","3564e817":"text_processed=pd.DataFrame(X,columns=[\"before\"])\ntext_processed[\"after\"]=Y","3e8cb541":"X=[]\nY=[]\ntrain_data=[]","f8b56d03":"biased=[]\ncount=-1\nc=[]\nfor i in range(len(text_processed)):\n    b=text_processed['before'][i].split()\n    a=text_processed['after'][i].split()\n    l1=len(b) if(len(b)<=len(a)) else len(a)\n    for j in range(l1):\n        if (b[j]!=a[j]):\n            biased.append(b[j])\n            count=i\n            break\n    if(count!=i):\n        biased.append(b[len(b)-1])\n            ","042b1492":"truth=pd.DataFrame(biased)\ntruth.columns=['biased']","e34132b4":"df = pd.DataFrame()\ndata=pd.DataFrame()","ce8a819e":"# Extract position of word in the sentence start\/mid\/end\nposition=[]\nfor i in range(len(text_processed)):\n    t=text_processed['before'][i].split()\n    l=len(t)\n    text = ['start' if i<int(l\/3) else 'end' if i>=int(2*l\/3) else 'mid' for i in range(l)]\n    text=' '.join(text)\n    text =text.strip()\n    position.append(text)\ndata['position']=position","a2b20045":"# Extract grammatical relation of word in the sentence \n\ngrammatical=[]\nfor i in range(len(text_processed)):\n    t=nlp(text_processed['before'][i])\n    text = [word.dep_ for word in t]\n    text=' '.join(text)\n    text =text.strip()\n    grammatical.append(text)\ndata['grammatical']=grammatical","76df0bf0":"data","3b3a3a3e":"data2=[]\ncolumns=['id','text','word_around','position','grammatical']\nfor index, row in text_processed.iterrows():\n    text=row['before'].split()\n    position=data['position'][index].split()\n    grammer=data['grammatical'][index].split()\n    for t in range(len(text)):\n        value=[]\n        value.append(index)\n        value.append(text[t])\n        around=[\"None\" if((i<0)|(i>=len(text))) else text[i] for i in [x for x in range(t-2,t+3) if x != t] ]\n        around=' '.join(around)\n        value.append(around)\n        value.append(position[t])\n        value.append(grammer[t])\n        zipped = zip(columns, value)\n        a_dictionary = dict(zipped)\n        data2.append(a_dictionary)\ndf = pd.DataFrame.from_dict(data2)","9f5a4f03":"data=[]\ncolumns=['POS-2','POS-1','POS+1','POS+2']\nfor index, row in df.iterrows():\n    text=nltk.pos_tag(nltk.word_tokenize(row['word_around']))\n    value=[]\n    for i in text:\n        if(i[0]==\"None\"):\n            value.append(\"None\")\n        else:\n            value.append(i[1])\n    zipped = zip(columns, value)\n    a_dictionary = dict(zipped)\n    data.append(a_dictionary)\nPOS = pd.DataFrame.from_dict(data)","add720fd":"l=['positive','strong_subjectives','implicatives','npov_word','factives','assertives',\n   'weak_subjectives','hedges','entailment','negative','report_verbs']","af97598b":"for k in l:\n    data=pd.read_csv(path+k+'.csv')\n    p=list(data.iloc[:,0])\n    lexi=[]\n    for i in range(len(df)):\n        t=df['text'][i]\n        text=\"True\" if t in p else \"False\"\n        lexi.append(text)   \n    df[k]=lexi\n","7a0d6752":"columns=['positive', 'strong_subjectives', 'implicatives','factives', 'assertives',\n         'weak_subjectives', 'hedges', 'entailment','negative', 'report_verbs']\ncol=[i+\"_around\" for i in columns]\nfor k in columns:\n    data=pd.read_csv(path+k+'.csv')\n    p=list(data.iloc[:,0])\n    lexi=[]\n    for index, row in df.iterrows():\n        text=nltk.word_tokenize(row['word_around'])\n        value=[\"True\" if i in p else \"False\" for i in text]\n        lexi.append(\"True\" if \"True\" in value else \"False\")        \n    df[k+\"_around\"]=lexi\n","d108736b":"data=[]\nfor index, row in df.iterrows():\n    text=row['text']\n    value = 1 if (truth['biased'][row['id']]==text) else 0      \n    data.append(value)\n\nlabel = pd.DataFrame.from_dict(data)\nlabel.columns=['label']","4ad3b931":"pos_before=[]\nfor i in range(len(df)):\n    t=nltk.word_tokenize(df['text'][i])\n    for j in nltk.pos_tag(t):\n        tagged = j[1] \n    pos_before.append(tagged)\ndf['POS']=pos_before","34bd4a3e":"lemma=[]\nfor i in range(len(df)):\n    t=df['text'][i]\n    text = wordnet_lemmatizer.lemmatize(t) \n    lemma.append(text)\ndf['lemma']=lemma\n","00cbd6f9":"polarity=[]\nfor i in range(len(df)):\n    t=df['text'][i]\n    sentiment_scores = af.score(t)\n    text = 'negative' if sentiment_scores<0 else 'positive' if sentiment_scores>0 else 'neutral'\n    polarity.append(text)\ndf['polarity']=polarity","f405ace6":"df = pd.concat([df,POS,label], axis=1, sort=False)","287444d8":"df=df.drop('word_around',axis=1)\ndf","07c68462":"word=df['text']\nlemma=df['lemma']\ntest= df.drop(['text','lemma'],axis=1)","4a23e2a0":"col=['id','positive','strong_subjectives','implicatives','npov_word','factives','assertives','weak_subjectives',\n     'hedges','entailment','negative','report_verbs','position','grammatical','polarity','POS-2','POS-1','POS',\n     'POS+1','POS+2','positive_around','strong_subjectives_around','implicatives_around','factives_around',\n     'assertives_around','weak_subjectives_around','hedges_around','entailment_around','negative_around','report_verbs_around','label']","26d733df":"test = test[col]","c3d87366":"!pip install gensim","3fe8bfdd":"# import gensim.downloader as api\n\n# wv = api.load('word2vec-google-news-300')\n# model = wv","58aca047":"import gensim\npath = \"..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin\"\nmodel = gensim.models.KeyedVectors.load_word2vec_format(path,binary=True)","e5b18449":"word_vec=[]\ncol=[\"word_vec_\"+str(i) for i in range(300)]\n\nfor i in range(len(word)):\n    \n    if word[i] in model.vocab:\n        w = model[word[i]]\n    else:\n        w=np.zeros(300)\n    \n    zipped = zip(col, w)\n    a_dictionary = dict(zipped)\n    word_vec.append(a_dictionary)\n","c6afc61d":"word_lemma=[]\ncol=[\"lemma_vec_\"+str(i) for i in range(300)]\n\nfor i in range(len(lemma)):\n    if lemma[i] in model.vocab:\n        w = model[lemma[i]]\n    else:\n        w=np.zeros(300)\n\n    zipped = zip(col, w)\n    a_dictionary = dict(zipped)\n    word_lemma.append(a_dictionary)","a41789d2":"word_vec=pd.DataFrame(word_vec)\nword_lemma=pd.DataFrame(word_lemma)\nword=pd.DataFrame(word)","cbb8b96d":"model=[]","5324fe9f":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.classes_ = np.load('..\/input\/biased-text\/'+\"classes.npy\")\ntest_encoded = test.apply(le.fit_transform)","268442a4":"test=[]","6e381f9d":"testset =pd.concat([word,word_vec,word_lemma,test_encoded],axis=1)","e0196d6e":"word=testset['text']\ny_test=testset['label']\nID=testset['id']","070d2fc6":"testset = testset.drop(['id','label','text'],axis=1)","66a9efa4":"col=ID.unique()","49b0b926":"!pip install scikit-learn --upgrade","09c399ee":"import pickle\nfilename = '..\/input\/biased-text\/'+'finalized_model_logistic_40000.sav'\nmodel = pickle.load(open(filename, \"rb\"))","5ba1bf6b":"y_pred = pd.DataFrame(model.predict_proba(testset)[:,1],index=y_test.index,columns=['y_pred'])","ff9c39ba":"analyze =pd.concat([ID,word,y_test,y_pred],axis=1)\nanalyze = analyze.set_index('id')\nanalyze","7bfa250d":"top1=[]\ntop2=[]\ntop3=[]\nfor i in col:\n    l=list(analyze.loc[i].sort_values(by=['y_pred'], ascending=False)[:3]['text'])\n    top1.append(l[0])\n    top2.append(l[:2])\n    top3.append(l)","f9edc9a7":"result1=0\nresult2=0\nresult3=0\n\nfor i,row in truth.iterrows():\n    result1+=1 if row['biased'] in top1[i] else 0\n    result2+=1 if row['biased'] in top2[i] else 0\n    result3+=1 if row['biased'] in top3[i] else 0","5e633c50":"result1=result1\/len(truth)\nresult2=result2\/len(truth)\nresult3=result3\/len(truth)","2fabb51b":"result1","d14dfd41":"result2","0d27f144":"result3","e7009cc0":"top3=pd.DataFrame(top3)\nresult =pd.concat([text_processed,truth,top3],axis=1)\nresult.columns=['before','after','biased','top1','top2','top3']\nresult.to_csv('result.csv',index=None)","a813a553":"# Test file upload with Before and After edits","d1a99b5d":"# Bias Word extraction By Comparing Sentence.","a4988415":"# Bias Word detection from Wikipedia edits.","0588d0b3":"# Text Precprocessing "}}