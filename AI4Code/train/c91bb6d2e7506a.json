{"cell_type":{"921bf3ab":"code","76558f1a":"code","3f584a82":"code","9a80af2a":"code","4df0df86":"code","20a55726":"code","fadd6ab0":"code","3da04de0":"code","b7c7f563":"code","69512d18":"code","537db3e7":"code","e6b4156b":"code","18ca769f":"code","007fa3ae":"code","51f3d01f":"code","0f3aa854":"code","884596ae":"code","fc2de52c":"code","fc808584":"code","17632653":"code","da5b622d":"code","acf8cebb":"code","7eac539d":"code","cda25fd4":"code","58fae11f":"code","514cf51f":"code","87a76b15":"code","aba7b278":"code","ac461565":"code","f7bedeca":"code","5f4d5b07":"code","b64df5da":"code","ca6a1827":"code","3b7f8134":"code","740ccc21":"code","4c7c4f16":"code","7eab02d4":"code","6b78b5bd":"code","18037bae":"code","8c6a02aa":"code","2f10726a":"code","293097c9":"code","2d6765ed":"code","fc752911":"code","0b0275c4":"code","ee1b7e3b":"code","b41b4807":"code","b6c29408":"code","ad3e873b":"code","127f923f":"code","3a497028":"code","c7b1669b":"code","b1079948":"code","13f3925d":"code","608bac7d":"code","eae1a4ee":"code","8ac1a0bc":"code","18e725e3":"code","5138a3fa":"code","0f4ee66f":"code","34e898a3":"code","8482a7f2":"code","38734014":"code","3283b28c":"code","497ed9be":"code","fe8c1a71":"code","38837069":"code","a15704d6":"code","b2ea648b":"code","571cce97":"code","1b60a697":"code","2886c536":"code","2ae1da59":"code","128dc223":"code","8efe78a1":"code","88e6e4bc":"code","a06eb891":"code","48bd2596":"code","e2687320":"code","68ff525d":"code","e82bc1cf":"code","8531e55e":"code","f33cba93":"code","0070f9e7":"code","0eb87875":"code","0b3a3804":"code","b7febd3a":"code","83ada187":"code","fdeb71e4":"code","8ebc1f46":"code","e4489f4c":"code","35e0d3e5":"code","773c9366":"code","11e143ca":"code","711ea896":"code","ad71fc96":"code","65b31062":"code","526f2392":"code","b30d85c3":"code","047e3857":"code","635af221":"code","2236f602":"code","5bd1c16a":"code","644bf326":"code","f5446d0d":"markdown","85adac3d":"markdown","4f56e36b":"markdown"},"source":{"921bf3ab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\n#import tensorflow as tf\n#from tensorflow import keras\n#from tensorflow.keras import layers\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\ndataset = pd.read_csv('..\/input\/train.csv')\n","76558f1a":"dataset.drop('Id',axis=1,inplace=True)","3f584a82":"#tf.logging.set_verbosity(tf.logging.INFO)","9a80af2a":"dataset.info()","4df0df86":"#Lets view the corr matrix between columns\nplt.figure(figsize=(20,20))\nsns.heatmap(dataset.corr(),annot=True,cmap='coolwarm')","20a55726":"datasetcorr = dataset.corr()\ncrit1 = datasetcorr['SalePrice'] > 0.5\ncrit2 = datasetcorr['SalePrice'] < -0.5\ncrit = crit1 | crit2\ndatasetcorr[crit]['SalePrice']","fadd6ab0":"# Based on the above corr below continous columns are the important \ncolumn_names = ['OverallQual','YearBuilt','YearRemodAdd','TotalBsmtSF','1stFlrSF','GrLivArea','FullBath','TotRmsAbvGrd',\n                'GarageCars','GarageArea','SalePrice']","3da04de0":"contdataset = dataset[column_names]","b7c7f563":"contdataset.head()","69512d18":"#YearBuilt,YearRemodAdd - Lets Bucketize these columns later as they are years \n# Discrete columns : FullBath,TotRmsAbvGrd,GarageCars","537db3e7":"catdataset = dataset.select_dtypes(include=np.object)","e6b4156b":"catdataset.loc[catdataset['MSZoning'] == 'C (all)','MSZoning'] = 'C' # Chaning the C (all) -> C as thats more comfirtable for any processing","18ca769f":"# Removing NULL values in Categorical variables","007fa3ae":"# Prints and displays \ndef chkcatcolsdet(ds):\n    nullcatcols = []\n    for col in ds.columns:\n            print(\"%s count:%d, nullvalues: %s, values: %s \"%(col,len(ds[col].unique()),ds[col].isnull().sum(),ds[col].unique()))\n            if ds[col].isnull().sum(): nullcatcols.append(col) \n    return nullcatcols","51f3d01f":"nullcatcols1 = chkcatcolsdet(catdataset)","0f3aa854":"print(nullcatcols1)","884596ae":"def repmincntvals(ds,nullcatcols):\n    for col in nullcatcols:\n        columnval = pd.DataFrame(ds.groupby([col])[col].count()).transpose().min().index[0]\n        ds.groupby(by=col)[col].count()\n        ds.loc[ds[col].isnull(),col] = columnval\n        ds.groupby(by=col)[col].count()","fc2de52c":"repmincntvals(catdataset,nullcatcols1)","fc808584":"chkcatcolsdet(catdataset) # no more Null columns","17632653":"# Lets drop Alley,FireplaceQu,PoolQC,Fence,MiscFeature as most of the values are NULL\n#Alley count:3, nullvalues: 1369, values: [nan 'Grvl' 'Pave'] \n#FireplaceQu count:6, nullvalues: 690, values: [nan 'TA' 'Gd' 'Fa' 'Ex' 'Po'] \n#PoolQC count:4, nullvalues: 1453, values: [nan 'Ex' 'Fa' 'Gd'] \n#Fence count:5, nullvalues: 1179, values: [nan 'MnPrv' 'GdWo' 'GdPrv' 'MnWw'] \n#MiscFeature count:5, nullvalues: 1406, values: [nan 'Shed' 'Gar2' 'Othr' 'TenC'] \ncatdataset.drop(['Alley','FireplaceQu','PoolQC','Fence','MiscFeature'],axis=1,inplace=True)","da5b622d":"len(catdataset.columns)","acf8cebb":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ndef labelencoderdataset(ds,catcols):\n    labencdataset = pd.DataFrame()\n    for i in range(0,len(catcols)):\n        if (int(ds[str(catcols[i])].isna().sum())):\n            next\n        else:\n            print (\"label encoding column : %s\"%str(catcols[i]))\n            labencdataset[catcols[i]] = labelencoder.fit_transform(ds[catcols[i]])\n    return labencdataset","7eac539d":"catlabencdataset = labelencoderdataset(catdataset,catdataset.columns)","cda25fd4":"# Lets Scale the Continous data\ncontdataset1 = contdataset","58fae11f":"contdataset.columns","514cf51f":"# - Lets Bucketize these columns later as they are years \n# Discrete columns : OverallQual,FullBath,TotRmsAbvGrd,GarageCars,YearBuilt,YearRemodAdd\n#Continous Variables : 'TotalBsmtSF', '1stFlrSF','GrLivArea','GarageArea'\n\ncontcontdataset = contdataset[['TotalBsmtSF', '1stFlrSF','GrLivArea','GarageArea']]\ncontdiscdataset = contdataset[['OverallQual','FullBath','TotRmsAbvGrd','GarageCars']]\ncontbuckdataset = contdataset[['YearBuilt','YearRemodAdd']]","87a76b15":"contbuckdataset.min(),contbuckdataset.max()","aba7b278":"year_ranges = [\"[{0} - {1})\".format(year, year + 10) for year in range(1870, 2010, 10)]\n\ncontbuckdataset['YearBuiltrange'] = pd.cut(contbuckdataset['YearBuilt'],labels=year_ranges,bins=len(year_ranges))\ncontbuckdataset['YearRemodAddrange'] = pd.cut(contbuckdataset['YearRemodAdd'],labels=year_ranges,bins=len(year_ranges))","ac461565":"contbuckdataset['YearBuiltrange'].value_counts().plot(kind='bar')","f7bedeca":"contbuckdataset['YearRemodAddrange'].value_counts().plot(kind='bar')","5f4d5b07":"contbuckdataset.drop(['YearBuilt','YearRemodAdd'],axis=1,inplace=True)","b64df5da":"contbuckdataset.info()","ca6a1827":"contbucklabdataset = labelencoderdataset(contbuckdataset,contbuckdataset.columns)","3b7f8134":"nullcatcols1 = chkcatcolsdet(contdiscdataset)","740ccc21":"labeldataset = pd.DataFrame(contdataset['SalePrice'])","4c7c4f16":"contdataset1.drop(['SalePrice'],axis=1,inplace=True)","7eab02d4":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ncontcontdataset2 = pd.DataFrame(scaler.fit_transform(contcontdataset),columns=contcontdataset.columns)","6b78b5bd":"finaldataset = pd.concat([contcontdataset2,contdiscdataset,contbucklabdataset,catlabencdataset,labeldataset],axis=1)","18037bae":"finaldataset.head()","8c6a02aa":"# Lets Try Z-Score to detect outliers\nfrom scipy import stats\nimport numpy as np\nz = np.abs(stats.zscore(finaldataset))\nprint(z)","2f10726a":"threshold = 10\nlen(np.where(z > 3.9)[0])","293097c9":"# Lets take backup of existing dataset - just in case and remove the above 100 outliers\nfinaldataset1 = finaldataset","2d6765ed":"finaldataset = finaldataset[(z < 3.9).all(axis=1)]","fc752911":"X = finaldataset.drop(['SalePrice'],axis=1)\nY = finaldataset['SalePrice'] # np.log(finaldataset['SalePrice']) - lets try the skewed data later\nY = np.log(finaldataset['SalePrice']) # Metric to calculate is log of the price","0b0275c4":"X_train, X_test, y_train, y_test = train_test_split(X,Y,test_size=0.33, random_state=101)","ee1b7e3b":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor","b41b4807":"Decstreemodel = DecisionTreeRegressor()","b6c29408":"Decstreemodel.fit(X_train,y_train)","ad3e873b":"from sklearn.metrics import mean_squared_error","127f923f":"y_pred_dectree = Decstreemodel.predict(X_test)","3a497028":"mean_squared_error(y_test,y_pred_dectree)**0.5","c7b1669b":"y_pred_dectree[:2],y_test[:2]","b1079948":"from sklearn.model_selection import RandomizedSearchCV\nparam_dist = {\n 'n_estimators': [50,100,150,200,250,350,450,550,650,750,1000],\n 'learning_rate' : [0.001,0.01,0.05,0.1,0.3,1],\n 'loss' : ['linear', 'square', 'exponential']\n }\nAdaBoostRgr = RandomizedSearchCV(AdaBoostRegressor(DecisionTreeRegressor(max_depth=50)),\n param_distributions = param_dist,\n cv=4,\n n_iter = 25,\n n_jobs=-1)\n","13f3925d":"AdaBoostRgr.fit(X_train,y_train)","608bac7d":"AdaBoostRgr.best_params_","eae1a4ee":"AdaBoostRgr.best_params_['n_estimators']","8ac1a0bc":"# Create the dataset\nAdaBoostfinalrgr = AdaBoostRegressor(DecisionTreeRegressor(max_depth=40),\n                          n_estimators=AdaBoostRgr.best_params_['n_estimators'], random_state=33,learning_rate=1,loss='square')","18e725e3":"AdaBoostfinalrgr.fit(X_train,y_train)","5138a3fa":"y_pred_adaboost = AdaBoostfinalrgr.predict(X_test)","0f4ee66f":"(y_pred_adaboost[:5],y_test[:5])","34e898a3":"from sklearn.metrics import mean_squared_error,median_absolute_error","8482a7f2":"mean_squared_error(y_test,y_pred_adaboost)**0.5","38734014":"from sklearn import ensemble","3283b28c":"param_dist = {\n 'n_estimators': [50,100,150,200,250,350,450,550,650,750,1000],\n 'learning_rate' : [0.001,0.01,0.05,0.1,0.3,0.7,1],\n 'loss' : ['ls', 'lad', 'huber', 'quantile'],\n }\nGradBoostRgrs = RandomizedSearchCV(ensemble.GradientBoostingRegressor(max_depth=10),param_dist)","497ed9be":"GradBoostRgrs.fit(X_train,y_train)","fe8c1a71":"GradBoostRgrs.best_params_","38837069":"gradboost = ensemble.GradientBoostingRegressor(max_depth=10,n_estimators=GradBoostRgrs.best_params_['n_estimators'],loss=GradBoostRgrs.best_params_['loss'],learning_rate=GradBoostRgrs.best_params_['learning_rate'])","a15704d6":"gradboost.fit(X_train,y_train)","b2ea648b":"gradboost_pred = gradboost.predict(X_test)","571cce97":"mse_gradboost = mean_squared_error(y_test, gradboost_pred)\nmse_adaboost = mean_squared_error(y_test,y_pred_adaboost)","1b60a697":"mse_gradboost**0.5,mse_adaboost**0.5","2886c536":"from xgboost import XGBRegressor","2ae1da59":"param_dist = {\n 'n_estimators': [100,200,300,400,500,600,700,800,900,1000],\n 'learning_rate' : [0.001,0.01,0.02,0.05,0.1,0.3,0.5,0.7,1],\n }\nXGBRgrs = RandomizedSearchCV(XGBRegressor(max_depth=10),param_dist)","128dc223":"XGBRgrs.fit(X_train,y_train)","8efe78a1":"XGBRgrs.best_params_","88e6e4bc":"xgb = XGBRegressor(max_depth=10,n_estimators=XGBRgrs.best_params_['n_estimators'],learning_rate=XGBRgrs.best_params_['learning_rate'])","a06eb891":"xgb.fit(X_train,y_train)","48bd2596":"xgb_pred = xgb.predict(X_test)","e2687320":"mse_gradboost = mean_squared_error(y_test, gradboost_pred)\nmse_adaboost = mean_squared_error(y_test,y_pred_adaboost)\nmse_xgboost = mean_squared_error(y_test,xgb_pred)","68ff525d":"print(mse_gradboost**0.5,mse_adaboost**0.5,mse_xgboost**0.5)","e82bc1cf":"testdataset = pd.read_csv('..\/input\/test.csv')","8531e55e":"# Based on the above corr below continous columns are the important \ncolumn_names = ['OverallQual','YearBuilt','YearRemodAdd','TotalBsmtSF','1stFlrSF','GrLivArea','FullBath','TotRmsAbvGrd',\n                'GarageCars','GarageArea']\nresultid = testdataset['Id']\ntestdataset.drop(['Id'],axis=1,inplace=True)\nconttestdataset = testdataset[column_names]\ncattestdataset = testdataset.select_dtypes(include=np.object)","f33cba93":"cattestdataset.loc[cattestdataset['MSZoning'] == 'C (all)','MSZoning'] = 'C'\nnullcatcols1 = chkcatcolsdet(cattestdataset)\nrepmincntvals(cattestdataset,nullcatcols1)\nnullcatcols1 = chkcatcolsdet(cattestdataset)\ncattestdataset.drop(['Alley','FireplaceQu','PoolQC','Fence','MiscFeature'],axis=1,inplace=True)","0070f9e7":"catlabenctestdataset = labelencoderdataset(cattestdataset,cattestdataset.columns)","0eb87875":"contconttestdataset = conttestdataset[['TotalBsmtSF', '1stFlrSF','GrLivArea','GarageArea']]\ncontdisctestdataset = conttestdataset[['OverallQual','FullBath','TotRmsAbvGrd','GarageCars']]\ncontbucktestdataset = conttestdataset[['YearBuilt','YearRemodAdd']]","0b3a3804":"nullcatcols3 = chkcatcolsdet(contdisctestdataset)","b7febd3a":"repmincntvals(contdisctestdataset,nullcatcols3)","83ada187":"# Test Dataset has GarageArea\/TotalBsmtSF as NULL values - so Lets replace with mean values.\nTotalBsmtSF_array = contconttestdataset[contconttestdataset[\"TotalBsmtSF\"]!=np.nan][\"TotalBsmtSF\"]\ncontconttestdataset[\"TotalBsmtSF\"].replace(np.nan,TotalBsmtSF_array.mean())\nGarageArea_array = contconttestdataset[contconttestdataset[\"GarageArea\"]!=np.nan][\"GarageArea\"]\ncontconttestdataset[\"GarageArea\"].replace(np.nan,GarageArea_array.mean())\n","fdeb71e4":"# Replace 660 column value with the mean value of TotalBDmtSF\ncontconttestdataset.loc[660,'TotalBsmtSF'] = np.mean(contconttestdataset['TotalBsmtSF'])","8ebc1f46":"contconttestdataset[contconttestdataset['TotalBsmtSF'].isnull()]","e4489f4c":"contbucktestdataset['YearBuiltrange'] = pd.cut(contbucktestdataset['YearBuilt'],labels=year_ranges,bins=len(year_ranges))\ncontbucktestdataset['YearRemodAddrange'] = pd.cut(contbucktestdataset['YearRemodAdd'],labels=year_ranges,bins=len(year_ranges))\ncontbucktestdataset.drop(['YearBuilt','YearRemodAdd'],axis=1,inplace=True)\ncontbucklabtestdataset = labelencoderdataset(contbucktestdataset,contbucktestdataset.columns)","35e0d3e5":"contconttestdataset2 = pd.DataFrame(scaler.fit_transform(contconttestdataset),columns=contconttestdataset.columns)","773c9366":"contconttestdataset2.isna().sum()","11e143ca":"contconttestdataset2[contconttestdataset2['GarageArea'].isnull()]\ncontconttestdataset2.loc[1116,'GarageArea'] = np.mean(contconttestdataset2['GarageArea'])","711ea896":"contconttestdataset2[contconttestdataset2['GarageArea'].isnull()]","ad71fc96":"finaldataset = pd.concat([contconttestdataset2,contdisctestdataset,contbucklabtestdataset,catlabenctestdataset],axis=1)","65b31062":"finaldataset.dropna()","526f2392":"X_test_csv = finaldataset","b30d85c3":"X_test_csv[X_test_csv['TotalBsmtSF'].isnull() == True]","047e3857":"Y_test_adaboostcsv = AdaBoostfinalrgr.predict(X_test_csv)\nY_test_xgboostcsv = xgb.predict(X_test_csv)","635af221":"Y_test_csv = np.exp(Y_test_xgboostcsv)","2236f602":"finalpredresult = pd.concat([resultid,pd.DataFrame(Y_test_csv,columns=['SalePrice'])],axis=1,names=['Id','SalePrice'])","5bd1c16a":"finalpredresult","644bf326":"finalpredresult.to_csv('sasanka_submission2.csv', index=False)","f5446d0d":"### XGB REGRESSOR","85adac3d":"### FINAL EVALUATION","4f56e36b":"### Lets Try XGBoost"}}