{"cell_type":{"5fadb97d":"code","5242c8aa":"code","a8240a8e":"code","c214b326":"code","b262efe5":"code","a6b4b9ce":"code","645499b7":"code","362dab80":"code","ce6ff256":"code","c94d5411":"code","e070e9e0":"code","9313d857":"code","c59cfd7d":"code","3dbb8382":"code","94d3d1c5":"code","e7379012":"code","82d54194":"code","7782b41b":"code","638957f6":"code","5e52ff3b":"code","be7859a6":"code","6290dd95":"code","9d181430":"code","2e99c9a8":"code","8730023c":"code","ae40ab34":"code","6bf706a4":"code","dfcac4a3":"code","688af0f1":"code","324ee6cf":"code","d9df8d42":"code","2b24f17d":"code","9ed2aa73":"code","71a8f45f":"code","cfab20e1":"code","f6f50a05":"markdown","54912188":"markdown","b049217d":"markdown","56c05ace":"markdown","d6992389":"markdown","b42fdc23":"markdown","36fcbc60":"markdown","9ef6321c":"markdown","ba9daae7":"markdown","04165fe2":"markdown","166c6756":"markdown","d7fa1122":"markdown","c0d27c71":"markdown","44529e37":"markdown","d9e5543d":"markdown","cf9877f2":"markdown","072b64e6":"markdown","47db96c1":"markdown","60b26e9a":"markdown","05a15dd8":"markdown","df074b10":"markdown","52beca02":"markdown","8433b425":"markdown","6b88a1f6":"markdown","a8f1a8f7":"markdown"},"source":{"5fadb97d":"import pandas as pd \nimport numpy as np \nimport seaborn as sns\nfrom itertools import cycle\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection  import train_test_split,GridSearchCV\nfrom sklearn.decomposition import PCA\nfrom collections import defaultdict\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn import svm\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom sklearn.preprocessing import scale\nfrom scipy import stats","5242c8aa":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest= pd.read_csv(\"..\/input\/test.csv\")","a8240a8e":"dftrain=train.copy()","c214b326":"dftrain.head()","b262efe5":"dftrain.info()","a6b4b9ce":"plt.figure(figsize=(15,15))\nsns.heatmap(dftrain.corr(),fmt=\".2f\",cmap=\"YlGnBu\")","645499b7":"dftrain.describe()","362dab80":"dftrain.columns","ce6ff256":"for i in dftrain.columns:\n    sns.FacetGrid(dftrain,hue=\"Cover_Type\",size=8)\\\n        .map(sns.distplot ,i)\nplt.legend()    ","c94d5411":"import scipy.stats as stats\nfrom scipy.stats import chi2_contingency\n\nclass ChiSquare:\n    def __init__(self, dataframe):\n        self.df = dataframe\n        self.p = None #P-Value\n        self.chi2 = None #Chi Test Statistic\n        self.dof = None\n        \n        self.dfObserved = None\n        self.dfExpected = None\n        \n    def _print_chisquare_result(self, colX, alpha):\n        result = \"\"\n        if self.p<alpha:\n            result=\"{0} is IMPORTANT for Prediction\".format(colX)\n        else:\n            result=\"{0} is NOT an important predictor. (Discard {0} from model)\".format(colX)\n\n        print(result)\n        \n    def TestIndependence(self,colX,colY, alpha=0.05):\n        X = self.df[colX].astype(str)\n        Y = self.df[colY].astype(str)\n        self.dfObserved = pd.crosstab(Y,X) \n        chi2, p, dof, expected = stats.chi2_contingency(self.dfObserved.values)\n        self.p = p\n        self.chi2 = chi2\n        self.dof = dof \n        \n        self.dfExpected = pd.DataFrame(expected, columns=self.dfObserved.columns, index = self.dfObserved.index)\n        \n        self._print_chisquare_result(colX,alpha)\n\n#Initialize ChiSquare Class\ncT = ChiSquare(dftrain)\n\n#Feature Selection\ntestColumns = ['Id', 'Elevation', 'Aspect', 'Slope',\n       'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology',\n       'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon',\n       'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points',\n       'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3',\n       'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3',\n       'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8',\n       'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12',\n       'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16',\n       'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20',\n       'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24',\n       'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28',\n       'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32',\n       'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36',\n       'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\nfor var in testColumns:\n    cT.TestIndependence(colX=var,colY=\"Cover_Type\" )  \n","e070e9e0":"dftrain.columns","9313d857":"import itertools\n#removing outliers\noutlier_list=[]\nfor i in dftrain.columns:\n    q1=np.percentile(dftrain.loc[:,i],25)\n    q3=np.percentile(dftrain.loc[:,i],75)\n    step=1.5*(q3-q1)\n    print (\"Data points considered outliers for the feature '{}':\".format(i))\n\n    outliers_rows = dftrain.loc[~((dftrain[i] >= q1 - step) & (dftrain[i] <= q3 + step)), :]\n    outlier_list.append(list(outliers_rows.index))\n    outliers = list(itertools.chain.from_iterable(outlier_list))\n\nuniq_outlier=list(set(outliers))\n\n#duplicate enteries removal\n\ndup_outliers = list(set([x for x in outliers if outliers.count(x) > 1]))\nprint ('Outliers list:\\n', uniq_outlier)\nprint( 'Length of outliers list:\\n', len(uniq_outlier))\n\nprint ('Duplicate list:\\n', dup_outliers)\nprint ('Length of duplicates list:\\n', len(dup_outliers))\n\nreal_data=dftrain.drop(dftrain.index[dup_outliers]).reset_index(drop=True)\n\nprint (real_data.shape)","c59cfd7d":"soil_list = []\nsoil_not=[7,8,15,25]\nfor i in range(1, 41):\n    if i not in soil_not:\n       soil_list.append('Soil_Type' + str(i))\n\nwilderness_area_list = ['Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4']\n\nprint(soil_list, \"\\n\")\nprint(wilderness_area_list)","3dbb8382":"def wilderness_compress(df):\n    \n    df[wilderness_area_list] = df[wilderness_area_list].multiply([1, 2, 3, 4], axis=1)\n    df['Wilderness_Area'] = df[wilderness_area_list].sum(axis=1)\n    return df","94d3d1c5":"def soil_compress(df):\n    \n    df[soil_list] = df[soil_list].multiply([i for i in range(1, 37)], axis=1)\n    df['Soil_Type'] = df[soil_list].sum(axis=1)\n    return df","e7379012":"dftrain = wilderness_compress(dftrain)\ndftrain = soil_compress(dftrain)\n\ndftrain[['Wilderness_Area', 'Soil_Type']].head()","82d54194":"cols = dftrain.columns.tolist()\ncolumns = cols[1:11] + cols[56:]\n\nprint(\"Useful columns: \", columns)\n\nvalues = dftrain[columns]\nlabels = dftrain['Cover_Type']\n\nprint(\"Values: \", values.shape)\nprint(\"Labels: \", labels.shape)","7782b41b":"import seaborn as sns\nsns.set_style('whitegrid')\nsns.set(rc={'figure.figsize':(11.7,8.27)})","638957f6":"ax = sns.countplot(labels, alpha=0.75)\nax.set(xlabel='Cover Type', ylabel='Number of labels')\nplt.show()","5e52ff3b":"ax = sns.distplot(dftrain['Elevation'], color='pink')\nplt.show()","be7859a6":"ax=sns.factorplot(x=\"Cover_Type\",col=\"Wilderness_Area\", data=dftrain , kind=\"count\",size=6, aspect=.7,palette=['crimson','lightblue','yellow','green','orange','purple','black'])","6290dd95":"ax = sns.violinplot(x=\"Cover_Type\", y=\"Wilderness_Area\", data=dftrain, inner=None)\n","9d181430":"ax = plt.scatter(x=dftrain['Horizontal_Distance_To_Hydrology'], y=dftrain['Vertical_Distance_To_Hydrology'], c=dftrain['Elevation'], cmap='jet')\nplt.xlabel('Horizontal Distance')\nplt.ylabel('Vertical Distance')\nplt.title(\"Distance to Hydrology with Elevation\")\nplt.show()","2e99c9a8":"ax = sns.countplot(dftrain['Soil_Type'], alpha=0.75)\nax.set(xlabel='Soil Type', ylabel='Count', title='Soil types - Count')\nplt.show()\n","8730023c":"ax = sns.jointplot(x='Soil_Type', y='Cover_Type', data=dftrain, kind='kde', color='purple')\nplt.show()","ae40ab34":"clean = dftrain[['Id', 'Cover_Type'] + columns]\nclean.head()\n","6bf706a4":"y=clean.iloc[:,0]\nclean=clean.drop(['Id','Cover_Type'],axis=1)\nclean.info()\n","dfcac4a3":"pca=PCA()\npca.fit_transform(clean)\nratio=pca.explained_variance_ratio_\nplt.figure(figsize=(10,10))    \nplt.plot(range(1,13),ratio,'-o',color='blue',alpha=0.75)\nplt.ylim(0,1)\nplt.xlim(0,10)\nplt.grid(axis=\"both\")\nplt.xlabel('Principal Component')\nplt.ylabel('percentages of variance in data set')\nplt.show()","688af0f1":"pca=PCA(n_components=2)\n\nprincipal_comp=pd.DataFrame(pca.fit_transform(clean),index=clean.index,columns=['PC1','PC2'])\npca_load=pd.DataFrame(pca.fit(clean).components_.T,index=clean.columns,columns=['v1','v2'])\n#plotting all states\nfor i in principal_comp.index:\n    plt.annotate(i,(principal_comp.PC1.loc[i],principal_comp.PC2.loc[i]),ha='center')\n#plotting reference lines\nplt.figure(figsize=(15,15))\nplt.hlines(0,-3.5,3.5,linestyle='dotted',color='green')\nplt.vlines(0,-3.5,3.5,linestyle='dotted',color='green')\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt1=plt.twinx().twiny()\n#setting limits\nplt.xlim(-1,1)\nplt.ylim(-1,1)\nplt.xlabel('eigen vectors',color='red')\n#setting annotation\na=1.07\nfor i in pca_load.index:\n    plt.annotate(i,(pca_load.v1.loc[i]*a,pca_load.v2.loc[i]*a),color='blue',ha='center')\n    \n#setting arrow    \nfor i,j in zip(range(1,13),['black', 'blue', 'purple', 'yellow', 'white', 'red', 'lime', 'cyan', 'orange', 'gray']):    \n    plt.arrow(0,0, pca_load.v1[i],pca_load.v2[i],color=j,shape='full',length_includes_head=True,head_width=.009,lw=3)\n\nplt.show()","324ee6cf":"from itertools import combinations\nfrom sklearn.preprocessing import PolynomialFeatures\n\ndef add_interactions(df):\n    # Get feature names\n    combos = list(combinations(list(df.columns), 2))\n    colnames = list(df.columns) + ['_'.join(x) for x in combos]\n    \n    # Find interactions\n    poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n    df = poly.fit_transform(df)\n    df = pd.DataFrame(df)\n    df.columns = colnames\n    \n    # Remove interaction terms with all 0 values            \n    noint_indicies = [i for i, x in enumerate(list((df == 0).all())) if x]\n    df = df.drop(df.columns[noint_indicies], axis=1)\n    \n    return df","d9df8d42":"from sklearn.preprocessing import OneHotEncoder\n\nA = add_interactions(clean)","2b24f17d":"from sklearn.model_selection  import train_test_split,GridSearchCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import classification_report,accuracy_score\nfrom scipy import stats","9ed2aa73":"\nx_train,x_test,y_train,y_test=train_test_split(A,y,test_size=.30)","71a8f45f":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nxtrain = sc.fit_transform(x_train)\nxtest = sc.transform(x_test)","cfab20e1":"pca=PCA(n_components=.95)\nx_train=pca.fit_transform(xtrain)\nx_test=pca.transform(xtest)\npca.explained_variance_","f6f50a05":"# as we can see that most of the foret that is from 3 to 6 are more like to have soil type from 0 to 150 times \n# forest cover 7 is with soil types of 1400 times , and 1 and 2 is soil types 250 to 800 times ","54912188":"<h1> Chi Square Formula  <h1>\n<br>\n<img src=\"https:\/\/latex.codecogs.com\/gif.latex?\\sum&space;=&space;(O-E)^2\/E\" title=\"\\sum = (O-E)^2\/E\" \/>\n<br>\n<b> O = Observed Value and E= Expected Value<b>","b049217d":"# first baseline model is ready got accuracy of 83 % percent max","56c05ace":"# id , Soil Type 7,8,15,25 has to be discarded from the final model","d6992389":"# Now removing outliers from the dftrain and duplicate outliers ","b42fdc23":"# Now I will compress the features so that i can do visualization and can drawout some inferences","36fcbc60":"# No class imbalance issues ","9ef6321c":"# Removed some soil features which were not useful in prediction of forest cover ","ba9daae7":"# So we can say that forest cover 1,2,5 and 7 are Area 1 \n# very less forest are in Areas 2\n# and Area 3 is more densely all types of cover except 4 \n# and Area 4 is more likely dense for 3,4 and 6 forest cover","04165fe2":"# So here are we with Forest Cover Analysis , that on various features we have to predict at least seven types of covers given to us ,\n# Feature engg , polynomial features with pca and xgboost apart from lgbm we can get best result since being a handling various dataset, this problem is merely on few featured ensembel models \n# Decision tree , base line model\n# random forest , stackers, \n# gbm , ada, xgboost and lgbm \n# with taste of PCA too . \n# evauation metrics will be accuracy , AUC , logloss and checking , variance and bias in final model","166c6756":"# as we can see slope is increasing and elevation to we are getting various forest covers so this variable is nececssary to know which type of cover","d7fa1122":"# Let's apply Chi square test to find the relation of prediction with target variable \n## for those who don't who what is chi square here is description to understand what it is exactly\nIntroduction\nFeature selection is an important part of building machine learning models. As the saying goes, garbage in garbage out. Training your algorithms with irrelevant features will affect the performance of your model. Also known as variable selection or attribute selection, choosing or engineering new features is often what separates the best performing models from the rest. \n\nFeatures selection can be both an art and science and it\u2019s a very broad topic. In this blog we will focus on one of the methods you can use to identify the relevant features for your machine learning algorithm and implementing it in python using the scipy library. We will be using the chi square test of independence to identify the important features in the titanic dataset.\n\nAfter reading this blog post you will be able to:\n\nGain an understanding of the chi-square test of independence\nImplement the chi-square test in python using scipy\nUtilize the chi-square test for feature selection\n\n\n\nChi Square Test\nThe Chi-Square test of independence is a statistical test to determine if there is a significant relationship between 2 categorical variables.  In simple words, the Chi-Square statistic will test whether there is a significant difference in the observed vs the expected frequencies of both variables. \n\n\nThe Null hypothesis is that there is NO association between both variables. \n\nThe Alternate hypothesis says there is evidence to suggest there is an association between the two variables. \n\nIn our case, we will use the Chi-Square test to find which variables have an association with the Survived variable. If we reject the null hypothesis, it's an important variable to use in your model.\n\nTo reject the null hypothesis, the calculated P-Value needs to be below a defined threshold. Say, if we use an alpha of .05, if the p-value < 0.05 we reject the null hypothesis. If that\u2019s the case, you should consider using the variable in your model.\n\nRules to use the Chi-Square Test:\n\n1. Variables are Categorical\n\n2. Frequency is at least 5\n\n3. Variables are sampled independently\n\nChi-Square Test in Python\nWe will now be implementing this test in an easy to use python class we will call ChiSquare. Our class initialization requires a panda\u2019s data frame which will contain the dataset to be used for testing. The Chi-Square test provides important variables such as the P-Value mentioned previously, the Chi-Square statistic and the degrees of freedom. Luckily you won\u2019t have to implement the show functions as we will use the scipy implementation for this.","c0d27c71":"# As we can see that we are provided with non null dataset so we can more further by undertsanding outlier , feature scaling etc \n","44529e37":"# almost we are left with 58% percent of data very since we have to predict the lakh no records \n# this no is very low for now ,  i will check my result in both cases as well ","d9e5543d":"## So stay tuned we are with removed feature which were not necessary \n## what left boxcox tranform , depending upon what lamda will value will come , standard , log , sq , cubic , sqrt    transformation will be applied \n## 1.  After  that Some feature engg will be done to create more features within features like we polynomial but \n## before that we have move PCA , then poly , then KBEST after that DT , RF , GBT, ADA, XG\n\n# So stay tuned more are coming some interesting visual and codes which DS family will like \n# Pls upvote if u like \n","cf9877f2":"# One more feature we can think of having euclidean distance of horizontal distance of water , fire and roadways since , they can bbe combined to form one by visualzing the PCA ","072b64e6":"# AS we can see that mean  vs max values lot of gap , outliers are here lot's of \n# three types of elevation cover we can see with the help of quartiles ","47db96c1":"# As far of now we are dealing with these much columns since we saw that from chi-square predictions that some features are not required to predict cover type","60b26e9a":"# now we have clean data , now we will go for PCA And other Arrows annotations visualization too ","05a15dd8":"# AS if for now we have compressed the features and now we can go visualization using bokeh visualization , better than seaborn","df074b10":"# from correlation analysis we can see that very few featues are depicting relation with one another a l like graph can been from top to bottom on LHS , VERY FEW soil type are showing some corrleation with another and other var as we can see some white lines has been drawn up this due tp heatmap scaling of points","52beca02":"# now some interesting visualization by means of using correlation analysis","8433b425":"# dftrain=dftrain.drop(['Soil_Type7','Soil_Type8','Soil_Type15','Soil_Type25'],axis=1)\n# we already dropping them  while not taking these featues when i was compressing the features , see code below ","6b88a1f6":"# analysis bring up like forest cover 1 is on 1 and 3 wilder areas , forest cover 2 so same\n# 1,2,5,7 forest cover are closey related to one another and 3 ,6 also to one another forest cover is on 4 areas , so we cans say that to predcit forest cover 4 areas 4 , for 1,2,5,7 we need 1,3 areas and for 3,6 we need 3, 4 areas ","a8f1a8f7":"# elevation is from 2260 to 3250 maximum values "}}