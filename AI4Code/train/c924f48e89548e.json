{"cell_type":{"f2713892":"code","f6f47950":"code","f5f837bf":"code","26c47d4a":"code","06dad248":"code","7f5db5b5":"code","6b1269ef":"code","aed54f19":"code","ba4e054d":"code","6ecbbf01":"code","5567edf1":"code","7a4abacc":"code","8ee50ef2":"code","d7e7b788":"code","88cbba26":"code","6e599321":"code","b7d52607":"code","9e995c7a":"code","8a70abd6":"code","7ae7e162":"code","5a2ff13a":"code","737f9823":"code","7752b3ad":"code","f00a5542":"code","c6defe99":"code","43e4b39a":"code","40405bb3":"code","22ecbe03":"code","4f014867":"code","5939e9e8":"code","26980289":"code","8732079a":"code","9422be58":"code","c48123c0":"code","37adafae":"code","aed79f58":"code","62c97362":"code","b9053fbf":"code","59d207a9":"markdown","1b948ea3":"markdown","c24d6524":"markdown","bfc9ba8a":"markdown","2ed02270":"markdown","2e5bb94f":"markdown","201b6aff":"markdown","3ab634bc":"markdown","468d49ff":"markdown","be0ca1e5":"markdown","f39e270e":"markdown","3c34db73":"markdown","d22402bf":"markdown"},"source":{"f2713892":"import os\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport string\nimport re\nfrom collections import Counter\nfrom sklearn import feature_extraction, linear_model, model_selection, preprocessing, decomposition\nimport xgboost as xgb \n!pip install contractions\nimport IPython\nimport contractions\nfrom datetime import datetime","f6f47950":"train_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nprint(len(train_df), len(test_df))","f5f837bf":"twt = nltk.tokenize.TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\nstop = nltk.corpus.stopwords.words(\"english\") + list(string.punctuation)\nstemmer = nltk.stem.PorterStemmer()\nlemmatizer = nltk.stem.WordNetLemmatizer()\n# print(stop)\n\ndef clean_text(df, col='text', normalize='lemmatize', stopwords=True, add_keyword=False, fill_empty='NULL', drop_dupe=False, shuffle=False):\n    cleaned_text, pos, neg = [], [], []\n    text_df = df[col]\n    if drop_dupe:\n        df = df.drop_duplicates(col)\n    \n    try: \n        targets = df.target\n    except:\n        targets = -np.ones(len(df))\n        \n    if add_keyword:\n        df.keyword = df.keyword.str.replace(\"%20\", \" \").fillna(\"\")\n        text_df = df.text + \" \" + df.keyword\n    \n    for (target, text) in zip(targets, text_df):\n#         print(text)\n        text = text.lower().split(\" \")\n        text = [word for word in text if \"http\" not in word]\n        text = [contractions.fix(word) for word in text]\n        text = \" \".join(text).lower()\n        text = re.sub(r'\\d+|#', '', text)\n        text = twt.tokenize(text)\n        if stopwords:\n            text = [word for word in text if word not in stop]\n        text = [word for word in text if word not in [\"rt\", \"\u00fb_\", \"amp\", \"\u00fb\u00aa\", \"\u00fb\u00aas\", \"\u00fb\u00f2\", \"\u00fb\u00ef\", \"\u00fb\u00f3\", \"\u00e5\u00e8\", \"\u00ec\u00f11\", \"\\x89\", \"...\", \"..\", \"via\"]]\n        if normalize == 'lemmatize':\n            text = [lemmatizer.lemmatize(word) for word in text]\n        if normalize == 'stem':\n            text = [stemmer.stem(word) for word in text]\n            \n        if target == 1: \n            pos.append(text)\n        if target == 0: \n            neg.append(text)\n        text = \" \".join(text)\n        cleaned_text.append(text)\n#         print(text)\n        \n    df[\"clean_text\"] = cleaned_text\n    df.clean_text = df.clean_text.replace(\"%20\", \" \")\n    if fill_empty != False:\n        df.loc[df.clean_text.str.len() == 0, 'clean_text'] = fill_empty\n    if shuffle:\n        df = df.sample(frac=1)\n    \n    return pos, neg, df\n        \npos_text, neg_text, train_df = clean_text(train_df, add_keyword=False, drop_dupe=True, shuffle=True)\n_, _, test_df = clean_text(test_df, add_keyword=False)\npos_text = [item for sublist in pos_text for item in sublist]\nneg_text = [item for sublist in neg_text for item in sublist]","26c47d4a":"pos_common = pd.DataFrame(Counter(pos_text).most_common(20))\nneg_common = pd.DataFrame(Counter(neg_text).most_common(20))\npd.concat([pos_common, neg_common], axis=1)","06dad248":"display(train_df.loc[(train_df.clean_text == \"NULL\"), :])\ndisplay(test_df.loc[(test_df.clean_text == \"NULL\"), :])","7f5db5b5":"display(train_df.sample(frac=1).head(10))\ndisplay(test_df.sample(frac=1).head(10))","6b1269ef":"feature_col = \"clean_text\"\n\ncount_vectorizer = feature_extraction.text.CountVectorizer()\ncount_vectorizer_sw = feature_extraction.text.CountVectorizer()\ntfidf = feature_extraction.text.TfidfVectorizer()\nLSA = decomposition.TruncatedSVD(n_components=100)\n\n## let's get counts for the first 5 tweets in the data\nexample_train_vectors = count_vectorizer.fit_transform(train_df[\"text\"][0:50])\nexample_train_vectors_sw = count_vectorizer_sw.fit_transform(train_df[feature_col][0:50])\nexample_tfidf = tfidf.fit_transform(train_df[feature_col][0:50])\nexample_tfidf_lsa = LSA.fit_transform(example_tfidf)","aed54f19":"## we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\nprint('No cleaning')\nprint(example_train_vectors[0].todense().shape)\n# print(example_train_vectors[0].todense())\nprint('Cleaned')\nprint(example_train_vectors_sw[0].todense().shape)\n# print(example_train_vectors_sw[0].todense())\nprint('TF-IDF cleaned')\nprint(example_tfidf[0].todense().shape)\n# print(example_tfidf[0].todense())\nprint('TF-IDF + LSA cleaned')\nprint(example_tfidf_lsa[0].shape)\n# print(example_tfidf_lsa[0])","ba4e054d":"train_vectors = count_vectorizer.fit_transform(train_df[feature_col])\ntrain_vectors_sw = count_vectorizer_sw.fit_transform(train_df[feature_col])\ntrain_tfidf = tfidf.fit_transform(train_df[feature_col])\n\n## note that we're NOT using .fit_transform() here. Using just .transform() makes sure\n# that the tokens in the train vectors are the only ones mapped to the test vectors - \n# i.e. that the train and test vectors use the same set of tokens.\ntest_vectors = count_vectorizer.transform(test_df[feature_col])\ntest_vectors_sw = count_vectorizer_sw.transform(test_df[feature_col])\ntest_tfidf = tfidf.transform(test_df[feature_col])","6ecbbf01":"train_tfidf_lsa = LSA.fit_transform(train_tfidf)\ntest_tfidf_lsa = LSA.transform(test_tfidf)","5567edf1":"# clf = linear_model.RidgeClassifier(class_weight='balanced')\n# ridge_params = {\n#     \"alpha\": np.linspace(0, 2, 100),\n#     \"tol\": np.linspace(1e-5, 1e-1, 2000)\n# }\n# ridge_rscv = model_selection.RandomizedSearchCV(clf, ridge_params, scoring=[\"f1\", \"precision\", \"recall\"], refit=\"f1\", cv=5, n_iter=100)\n# ridge_rscv_lsa = model_selection.RandomizedSearchCV(clf, ridge_params, scoring=[\"f1\", \"precision\", \"recall\"], refit=\"f1\", cv=5, n_iter=100)","7a4abacc":"# search = ridge_rscv.fit(train_tfidf, train_df[\"target\"])\n# search_lsa = ridge_rscv_lsa.fit(train_tfidf_lsa, train_df[\"target\"])\n# IPython.display.clear_output()\n# print(\"Best RidgeClassifier TF-IDF\")\n# print(search.best_score_)\n# print(search.best_params_)\n# print(\"Best RidgeClassifier TF-IDF LSA\")\n# print(search_lsa.best_score_)\n# print(search_lsa.best_params_)","8ee50ef2":"# scores_tfidf = model_selection.cross_validate(clf, train_tfidf, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n\n# scores_tfidf_lsa = model_selection.cross_validate(clf, train_tfidf_lsa, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n\n# print(\"RidgeClassifier TF-IDF F1:              \", scores_tfidf['test_f1'])\n# print('RidgeClassifier TF-IDF & LSA F1:        ', scores_tfidf_lsa['test_f1'])\n# print(\"RidgeClassifier TF-IDF Precision:       \", scores_tfidf['test_precision'])\n# print('RidgeClassifier TF-IDF & LSA Precision: ', scores_tfidf_lsa['test_precision'])\n# print('RidgeClassifier TF-IDF Recall:          ',  scores_tfidf['test_recall'])\n# print('RidgeClassifier TF-IDF & LSA Recall:    ', scores_tfidf_lsa['test_recall'])","d7e7b788":"# scores_tfidf = model_selection.cross_validate(ridge_rscv.best_estimator_, train_tfidf, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n\n# scores_tfidf_lsa = model_selection.cross_validate(ridge_rscv_lsa.best_estimator_, train_tfidf_lsa, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n\n# print(\"Best RidgeClassifier TF-IDF F1:              \", scores_tfidf['test_f1'])\n# print('Best RidgeClassifier TF-IDF & LSA F1:        ', scores_tfidf_lsa['test_f1'])\n# print(\"Best RidgeClassifier TF-IDF Precision:       \", scores_tfidf['test_precision'])\n# print('Best RidgeClassifier TF-IDF & LSA Precision: ', scores_tfidf_lsa['test_precision'])\n# print('Best RidgeClassifier TF-IDF Recall:          ',  scores_tfidf['test_recall'])\n# print('Best RidgeClassifier TF-IDF & LSA Recall:    ', scores_tfidf_lsa['test_recall'])","88cbba26":"# xgb_clf = xgb.XGBClassifier(random_state=765, tree_method='gpu_hist', predictor='gpu_predictor')\n# xgb_params = {\n#     \"max_depth\": [i for i in range(4, 14)],\n#     \"min_child_weight\": np.linspace(0.25, 0.45, 100),\n#     \"gamma\": np.linspace(0, 0.015, 1000),\n#     \"learning_rate\": np.linspace(0.2, 0.5, 100),\n# }\n# xgb_rscv = model_selection.RandomizedSearchCV(xgb_clf, xgb_params, scoring=[\"f1\", \"precision\", \"recall\"], refit=\"f1\", cv=5, verbose=2)\n# xgb_rscv_lsa = model_selection.RandomizedSearchCV(xgb_clf, xgb_params, scoring=[\"f1\", \"precision\", \"recall\"], refit=\"f1\", cv=5, verbose=2)","6e599321":"# search = xgb_rscv.fit(train_tfidf, train_df[\"target\"])\n# search_lsa = xgb_rscv_lsa.fit(train_tfidf_lsa, train_df[\"target\"])\n# IPython.display.clear_output()\n# print(\"Best XGBClassifier TF-IDF\")\n# print(search.best_score_)\n# print(search.best_params_)\n# print(\"Best XGBClassifier TF-IDF LSA\")\n# print(search_lsa.best_score_)\n# print(search_lsa.best_params_)","b7d52607":"# scores_tfidf = model_selection.cross_validate(xgb_clf, train_tfidf, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n# scores_tfidf_lsa = model_selection.cross_validate(xgb_clf, train_tfidf_lsa, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n\n# print(\"XGBClassifier TF-IDF F1:              \", scores_tfidf['test_f1'])\n# print('XGBClassifier TF-IDF & LSA F1:        ', scores_tfidf_lsa['test_f1'])\n# print(\"XGBClassifier TF-IDF Precision:       \", scores_tfidf['test_precision'])\n# print('XGBClassifier TF-IDF & LSA Precision: ', scores_tfidf_lsa['test_precision'])\n# print('XGBClassifier TF-IDF Recall:          ',  scores_tfidf['test_recall'])\n# print('XGBClassifier TF-IDF & LSA Recall:    ', scores_tfidf_lsa['test_recall'])","9e995c7a":"# scores_tfidf = model_selection.cross_validate(xgb_rscv.best_estimator_, train_tfidf, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n# scores_tfidf_lsa = model_selection.cross_validate(xgb_rscv_lsa.best_estimator_, train_tfidf_lsa, train_df[\"target\"], cv=5, scoring=[\"f1\", \"precision\", \"recall\"])\n\n# print(\"Best XGBClassifier TF-IDF F1:              \", scores_tfidf['test_f1'])\n# print('Best XGBClassifier TF-IDF & LSA F1:        ', scores_tfidf_lsa['test_f1'])\n# print(\"Best XGBClassifier TF-IDF Precision:       \", scores_tfidf['test_precision'])\n# print('Best XGBClassifier TF-IDF & LSA Precision: ', scores_tfidf_lsa['test_precision'])\n# print('Best XGBClassifier TF-IDF Recall:          ',  scores_tfidf['test_recall'])\n# print('Best XGBClassifier TF-IDF & LSA Recall:    ', scores_tfidf_lsa['test_recall'])","8a70abd6":"import torch\nimport torch.nn.functional as F\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import AdamW, get_linear_schedule_with_warmup","7ae7e162":"bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')","5a2ff13a":"bert_tokenizer.add_special_tokens({'additional_special_tokens': ['[LINK]', '[USER]']})\nbert_tokenizer","737f9823":"def bert_tokenize(df, tokenizer=bert_tokenizer, max_seq_len = 100):\n    input_sequences = []\n    # The attention mask is an optional argument used when batching sequences together.\n    # The attention mask is a binary tensor indicating the position of the padded indices so that the model does not attend to them.\n    attention_masks = []\n    bert_text = []\n    \n    # some very minor text processing, try to keep the text as close as original\n    for i, text in enumerate(df['text']):\n#         print(i, text)\n        text = text.replace(\"\\n\", \" \").split(\" \")\n        text = [word if \"http\" not in word else \"[LINK]\" for word in text]\n        text = [word if \"@\" not in word else \"[USER]\" for word in text]\n        text = \" \".join(text)\n        text = re.sub(r'#', '', text)\n        bert_text.append(text)\n        \n#         print(i, text)\n        sequence_dict = tokenizer.encode_plus(text, max_length=max_seq_len, pad_to_max_length=True)\n        input_ids = sequence_dict['input_ids']\n        att_mask = sequence_dict['attention_mask']\n#         print(i, tokenizer.tokenize(text))\n        input_sequences.append(input_ids)\n        attention_masks.append(att_mask)\n    \n    df['bert_text'] = bert_text\n    return input_sequences, attention_masks, df\n\ntrain_X, train_att, train_df = bert_tokenize(train_df)\ntrain_y = train_df['target'].values\ntest_X, test_att, test_df = bert_tokenize(test_df)","7752b3ad":"# Checking the tokenized format\nprint(train_X[0])\nprint(train_att[0])\nprint(test_X[0])\nprint(test_att[0])","f00a5542":"train_X = torch.tensor(train_X)\ntrain_y = torch.tensor(train_y)\ntrain_att = torch.tensor(train_att)\ntest_X = torch.tensor(test_X)\ntest_att = torch.tensor(test_att)","c6defe99":"batch_size = 32\ntrain_data = torch.utils.data.TensorDataset(train_X, train_att, train_y)\ntrain_sampler = torch.utils.data.RandomSampler(train_data)\ntrain_dataloader = torch.utils.data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\ntest_data = torch.utils.data.TensorDataset(test_X, test_att)\ntest_sampler = torch.utils.data.SequentialSampler(test_data)\ntest_dataloader = torch.utils.data.DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)","43e4b39a":"model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\nmodel.resize_token_embeddings(len(bert_tokenizer))","40405bb3":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\nmodel.to(device)\nIPython.display.clear_output()","22ecbe03":"optimizer = AdamW(model.parameters(), lr=2e-5, eps=1e-8)\nloss_fct = torch.nn.NLLLoss()","4f014867":"def train(epoch):\n    t0 = datetime.now()\n    model.train()\n    for i, batch in enumerate(train_dataloader, start=1):\n        batch = tuple(t.to(device) for t in batch)\n        inputs, att_masks, labels = batch\n        model.zero_grad()  \n        \n        logits = model(inputs, attention_mask=att_masks)\n        outputs = F.log_softmax(logits[0], dim=1)\n        \n        loss = loss_fct(outputs.view(-1, 2), labels.view(-1))\n        loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n        \n        if i % 20 == 0:\n            print('Train Epoch: {} [{}\/{} ({:.0%})] - Elapsed: {}  |  Loss: {:.4f}'.format(\n                epoch, i * len(inputs), len(train_dataloader.dataset),\n                 i \/ len(train_dataloader), datetime.now() - t0, loss.item()\n            ))","5939e9e8":"def test():\n    t0 = datetime.now()\n    model.eval()\n    test_loss, test_acc = 0, 0\n    for batch in test_dataloader:\n        batch = tuple(t.to(device) for t in batch)\n        inputs, att_masks, labels = batch\n        with torch.no_grad():\n            logits = model(inputs, attention_mask=att_masks)\n            outputs = F.log_softmax(logits[0], dim=1)\n            \n            loss = loss_fct(outputs.view(-1, 2), labels.view(-1))\n\n        test_loss += loss.item()\n        outputs = outputs.detach().cpu().numpy()\n\n        pred = np.argmax(outputs, axis=1)\n        labels = labels.cpu().numpy()\n        \n        test_acc += accuracy_score(pred, labels)\n\n    test_loss \/= len(test_dataloader)\n    test_acc \/= len(test_dataloader)\n    print('\\nTest set: Loss: {:.4f}, Accuracy: {:.1%} - Elapsed: {}\\n'.format(\n        test_loss, test_acc, datetime.now() - t0\n    ))","26980289":"nb_epoch = 2\nfor epoch in range(1, nb_epoch+1):\n    train(epoch)\n#     test()","8732079a":"def predict(text):\n    # pre-process text\n    input_ = torch.tensor(bert_tokenizer.encode(text)).unsqueeze(0).to(device)\n    logits = model.eval()(input_ids=input_)[0]\n    pred = F.softmax(logits, dim=1)[0]\n    return pred","9422be58":"predictions = []\nfor text in test_df.text:\n    prob = predict(text)\n    pred = np.argmax(prob.cpu().detach().numpy())\n    predictions.append(pred)","c48123c0":"sample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","37adafae":"# train_prediction = ridge_rscv.best_estimator_.predict(train_tfidf)\n# train_df['pred_target'] = train_prediction\n\n# ridge with rscv\n# sample_submission[\"target\"] = ridge_rscv.best_estimator_.predict(test_tfidf)\n\n# bert\nsample_submission[\"target\"] = predictions","aed79f58":"# clean_text_wc = train_df.clean_text.str.count(' ').add(1)\n# short_text_incorrect = train_df.loc[(clean_text_wc < 5) & (train_df.target != train_df.pred_target), :]\n# (short_text_incorrect.target == 1).sum(), (short_text_incorrect.target == 0).sum()","62c97362":"# display(sample_submission.head(30))\n# display(test_df['text'].head(30))\npd.merge(sample_submission, test_df, on=['id']).sample(frac=1).head(10)","b9053fbf":"sample_submission.to_csv(\"submission.csv\", index=False)","59d207a9":"# Introduction\n\n**This notebook linear model part is based on the tutorial notebook**\n\nhttps:\/\/www.kaggle.com\/philculliton\/nlp-getting-started-tutorial\n\n**The sections for RidgeClassifier and XGBClassifier do not contribute to the final score but they are alternative models for this problem**\n\n**The BERT section contributes to the final score**","1b948ea3":"# Count and Vectorize approach (1-gram)","c24d6524":"# **MAIN CONTENT: BERT**\n\n**Note that this is not the best version, fine tunings and validation may help obtain a better score**","bfc9ba8a":"As BERT is able to read complete passages and learn from the context, too much text preprocessing may not be beneficial.\n\nSome minor preprocessing with URLs, @usernames, and #hashtag, as they may be tokenized weirdly and the token make no sense\n\n*Note: The BERT model still did pretty good without the above processing*\n\nThen tokenize the data","2ed02270":"**Pretrained model from bert-base-uncased**\n\nresize_token_embeddings is required as we have added new special tokens","2e5bb94f":"# **Linear Model: Ridge Classifier**","201b6aff":"# **GBDT: XGB Classifier**\nTurns out not as good as ridge","3ab634bc":"**Generating predictions for test data**","468d49ff":"# Submission","be0ca1e5":"Define train and test functions","f39e270e":"# Text preprocessing for linear models","3c34db73":"Forming dataset","d22402bf":"Adding additional tokens for masking URLs and usernames in tweets"}}