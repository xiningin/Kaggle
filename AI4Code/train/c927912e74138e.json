{"cell_type":{"2de750ca":"code","5f9ed2d0":"code","ca237164":"code","ca170c91":"code","fdaa5dad":"code","68624e05":"code","2321d59c":"code","d7f9585b":"code","d3455c4f":"code","135da968":"code","ca44aa20":"code","fa141135":"code","bb1c2c35":"code","cd908c92":"code","d3089e57":"code","4d686747":"code","7fb04c25":"code","961b8f9c":"code","ff18ce59":"code","6f8abd04":"code","cb1f6aab":"code","08ed0efe":"code","68dc5631":"code","efab63b9":"code","87770716":"code","a54c21e6":"code","1ce75ba2":"code","a904a1ba":"code","cccd5360":"code","6d9d3992":"code","9a0f99f9":"code","54075a3a":"code","f7a6b6dc":"code","cdc83df5":"code","2462f54a":"code","9485eb6a":"markdown","59319c09":"markdown","42ee26c2":"markdown","274c009b":"markdown","97128155":"markdown"},"source":{"2de750ca":"import pandas as pd\nimport numpy as np\nimport random\nimport re\nimport nltk\nimport string \nfrom nltk.corpus import stopwords  \nfrom nltk.stem import PorterStemmer  \nfrom nltk.tokenize import TweetTokenizer\nimport emoji\nfrom nltk.stem import WordNetLemmatizer\n\nnltk.download('stopwords')\n\n# dictionary for lemmatization\nnltk.download('wordnet')","5f9ed2d0":"real = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/True.csv\",encoding_errors='ignore')","ca237164":"real.shape","ca170c91":"real.head(5)","fdaa5dad":"real.drop(['title', 'subject','date'], axis = 1,inplace=True)","68624e05":"fake = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/Fake.csv\", encoding_errors='ignore')","2321d59c":"fake.head(4)","d7f9585b":"fake.drop(['title', 'subject','date'], axis = 1,inplace=True)","d3455c4f":"#Tweet Dataset\ndf = pd.read_csv(\"..\/input\/large-random-tweets-from-pakistan\/Random Tweets from Pakistan- Cleaned- Anonymous.csv\",encoding_errors='ignore')","135da968":"df.shape","ca44aa20":"df.head(4)","fa141135":"df = df['full_text']","bb1c2c35":"# Removing empty values\ndf= df.dropna()","cd908c92":"# removing urdu\nreg = re.compile(r'[\\-\\u06ff]+', re.UNICODE)\ndf = df.apply(lambda x: re.sub(reg, \"\", x))","d3089e57":"# remove hyperlinks and #\ndf = df.apply(lambda x: re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', x))\ndf = df.apply(lambda x: re.sub(r'#', '', x))\ndf = df.apply(lambda x: re.sub(r'rt : ', '', x))","4d686747":"df = df.drop_duplicates()","7fb04c25":"# Tokenizing tweets\ntokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                           reduce_len=True)\ndf = df.apply(tokenizer.tokenize)","961b8f9c":"# Removing stop words\nstopwords_english = stopwords.words('english')\n\ndef remove_stop(x):\n    return [y for y in x if y not in stopwords_english and y not in string.punctuation\n          and (len(y) > 1 or emoji.is_emoji(y)) ]\n\ndf = df.apply(remove_stop)","ff18ce59":"# Stemming\nstemmer = PorterStemmer()\ndef stem(x):\n    return [stemmer.stem(y) for y in x]\n\nstemmed_tweets = df.apply(stem)","6f8abd04":"tweet = pd.DataFrame({'text':df})\ntweet.head(5)","cb1f6aab":"tweet_list = tweet['text'].values","08ed0efe":"tweet_list","68dc5631":"sample_tweet = [*tweet_list[0:35]]","efab63b9":"sample_tweet = \" \".join(sample_tweet)","87770716":"sample_tweet[:2]","a54c21e6":"real_list = real['text'].to_list()","1ce75ba2":"sample_real = real_list[0:35]","a904a1ba":"ss2 = sample_real","cccd5360":"fake_list = fake['text'].to_list()","6d9d3992":"sample_fake = fake_list[0:35]","9a0f99f9":"ss3 = sample_fake","54075a3a":"ss1 = [\" \".join(x) for x in ss1 ]","f7a6b6dc":"ss1 =\" \".join(ss1)","cdc83df5":"def cosine_distance_countvectorizer_method(ss1,ss3):\n    \n    # sentences to list\n    allsentences = [ss1,*ss3]\n    \n    # packages\n    from sklearn.feature_extraction.text import CountVectorizer\n    from scipy.spatial import distance\n    # text to vector\n    vectorizer = CountVectorizer()\n    all_sentences_to_vector = vectorizer.fit_transform(allsentences)\n    text_to_vector_v1 = all_sentences_to_vector.toarray()[0].tolist()\n    text_to_vector_v2 = all_sentences_to_vector.toarray()[1].tolist()\n    \n    # distance of similarity\n    cosine = distance.cosine(text_to_vector_v1, text_to_vector_v2)\n    print('Similarity of two dataset are equal to ',round((1-cosine)*100,2),'%')\n    return cosine","2462f54a":"cosine_distance_countvectorizer_method(ss1,ss3)","9485eb6a":"### Real Dataset","59319c09":"### Exploratory Data Analysis","42ee26c2":"### Cosine Similarity between Tweet and Fake","274c009b":"### Fake dataset","97128155":"### Filtering & Cleaning"}}