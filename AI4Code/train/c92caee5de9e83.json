{"cell_type":{"dce935bc":"code","aab3ea64":"code","ceee20ae":"code","122a2b53":"code","cf21c66c":"code","00deff08":"code","9c08f90f":"code","18c84c99":"code","2ad925c6":"code","699a9d8f":"code","57053247":"code","01c3b7e0":"code","e26091be":"code","14715343":"code","a0a9ac2f":"code","5372fdb3":"code","8b429093":"code","d2d1ce2d":"code","ceb7b903":"code","5c8d0f5c":"code","411b35f6":"code","5ae8d41d":"code","7b328e00":"code","6ad77575":"code","2721b119":"code","e56920ba":"code","198d2c23":"code","65a18f78":"code","9558fe3e":"code","3d09fd61":"code","5800788c":"code","aa7bdbbb":"code","832592ce":"code","02bc5deb":"code","7e5d983c":"code","e1cccaa0":"code","b80d3588":"code","11d71983":"code","1ca9980b":"code","51e8a732":"code","7ca2ee0a":"code","39282468":"code","a98d638a":"code","046d6968":"markdown","cb1ef754":"markdown","6d25abdf":"markdown","2a8e91bf":"markdown","9da87b99":"markdown","2a4cd3f7":"markdown","3f992f1a":"markdown","8dfdb7c5":"markdown","25e0fb46":"markdown","9ef985cd":"markdown","e58e16ae":"markdown","76907889":"markdown","83b58421":"markdown","afc76b5f":"markdown"},"source":{"dce935bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","aab3ea64":"from bs4 import BeautifulSoup as bs\nimport requests\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering, MeanShift, DBSCAN\nfrom sklearn.preprocessing import normalize\nimport seaborn as sns\nfrom datetime import datetime, timedelta, date\nimport csv\nimport seaborn as sns","ceee20ae":"def get_test_range(years = 3): \n    \n    \"\"\"Takes a range of year, returns two integers used by the \n    Yahoo Finance stock urls to select the range of data to display.\n    The higher number will represent today, the lower number represents \n    the day x number of years ago.\"\"\"\n    \n    # Assign initial date and upper range from url\n    initial_date = datetime(2020, 10, 25)\n    initial_secs = 1603584000\n\n    # Check \"today's\" date, compute total seconds between dates\n    today = datetime.today()\n\n\n    elapsed_time = (today.date() - initial_date.date()).total_seconds()\n\n    # Add total seconds to inital upper range number\n    updated_upper_range = int(initial_secs + elapsed_time)\n\n    # Subtract number of chosen years in seconds for lower range\n\n    years_to_days = years*365\n\n    test_duration = timedelta(days = years_to_days).total_seconds()\n\n    lower_range = int(updated_upper_range - test_duration)\n    \n    # Create dictionary\n    \n    daterange = {'period1': lower_range, 'period2': updated_upper_range}\n    \n    return daterange","122a2b53":"def get_urls_names(main_url_list, testrange = 3) :\n    \n    \"\"\"Takes list of stock-collection urls ('Trending', 'Active', or specific indexes),\n    collects the urls of specific stocks, transforms the urls to show data from the \n    desired range of years, then returns a dataframe with the each stock and its url.\"\"\"\n    \n    # Get integers to place in url\n    period = get_test_range(years = testrange)\n    \n    # For each stock collection url\n    list_of_dfs = []\n    for main_url in main_url_list:\n        \n        #Grab html\n        r = requests.get(main_url)\n        soup = bs(r.content)\n        \n        #Parse out the table of different stocks\n        table = soup.find('table')\n        elements = table.select('a')\n        \n        # Parse out the url for stock, add test range integers\n        urls = []\n        names = []\n        \n        for item in elements:\n            href_piece = item['href'].split('=')\n            url = 'https:\/\/query1.finance.yahoo.com\/v7\/finance\/download\/' + href_piece[1] + '?period1=' + str(period['period1']) + '&period2=' + str(period['period2']) + '&interval=1d&events=history&includeAdjustedClose=true'\n            urls.append(url)\n            names.append(item['href'].split('=')[1])\n    \n            \n        # Create dict with stock name and its url\n        url_name = {'urls':urls, 'names': names} \n        \n        # Add dict to list\n        list_of_dfs.append(pd.DataFrame(url_name))\n        \n    # Create and return dataframe\n    return pd.concat(list_of_dfs, axis = 0).drop_duplicates(subset=['names']) ","cf21c66c":"### NOT BEING USED\n\ndef get_table(urls, stock_names):\n    \n    \"\"\"Takes a list of stock urls and a list of the names of those stocks, \n    grabs the html table data and returns a pandas dataframe of that stock data.\"\"\"\n    \n    list_of_dfs = []\n    for url, name in zip(urls, stock_names):\n        column_names = []\n        list_of_rows = []\n\n        #Get HTML\n        r = requests.get(url)\n        #time.sleep(3) \n\n        #Convert HTML to BeautifulSoup object\n        soup = bs(r.content)\n\n        #Isolates row of table\n        info_rows = soup.find_all('tr')\n\n\n        for index, row in enumerate(info_rows):\n            entries = []\n\n            #Gets column names\n            if index == 0:\n                for x in info_rows[0].find_all('span'):\n                    column_names.append(x.get_text().strip('*'))\n\n            #Grabs values from each row, saves as a list, adds list to master list       \n            else:\n                for entry in row.find_all('span'):\n                    entries.append(entry.get_text().replace(',', ''))\n                #Skips some headers\n                if len(entries) < 7:\n                    continue\n\n                else:\n                    list_of_rows.append(entries)\n\n        #Create df, cleans the data, adds it to list_of_dfs\n        stock_df = pd.DataFrame(list_of_rows, columns = column_names)\n        stock_df['Volume'] = stock_df['Volume'].astype('int64')   \n        stock_df['Date'] = pd.to_datetime(stock_df['Date'])\n        stock_df.iloc[:, 1:-1] = stock_df.iloc[:, 1:-1].astype('float')        \n        stock_df['Stock'] = name\n\n        list_of_dfs.append(stock_df)\n\n    #Concatenates list of dfs into master dataframe    \n    return pd.concat(list_of_dfs).reset_index(drop=True)    ","00deff08":"#blah\ndef get_csv_table(urls, stock_names):\n    \n    \"\"\"Takes a list of stock urls and a list of the names of those stocks, \n    grabs the .csv file data and returns a pandas dataframe of that stock data.\"\"\"\n    \n    # For each stock in the list of stock urls\n    list_of_dfs = []\n    for url, name in zip(urls, stock_names):\n        column_names = []\n        list_of_rows = []\n\n        #Get HTML for .csv download link from specific stock and transform to a list\n        r = requests.get(url)\n        \n        soup = bs(r.content)\n        table = soup.find('p').get_text()\n        lines = table.splitlines()\n        reader = csv.reader(lines)\n\n\n        lis = list(reader)\n        \n\n        # Create dataframe from .csv file data\n        col_list = lis[0]\n        stock_df = pd.DataFrame(lis[1:], columns = col_list)  \n        \n        \n        for col in col_list[1:]:\n            stock_df[col] = pd.to_numeric(stock_df[col], errors= 'coerce')\n        \n        \n        test_df = stock_df.dropna()\n        \n   \n        #Create df, cleans the data, adds it to list_of_dfs\n    \n        stock_df.iloc[:,0] = pd.to_datetime(stock_df.iloc[:,0])\n        \n        stock_df['Stock'] = name\n        \n        if (len(test_df) == len(stock_df)):\n            list_of_dfs.append(stock_df)\n        else: continue\n        \n    #Concatenates list of dfs into master dataframe    \n    fullest_df = pd.concat(list_of_dfs).reset_index(drop=True)\n    return fullest_df.iloc[:, :8]     ","9c08f90f":"def find_distribution_center(stock_df, statistic):\n    \"\"\"Takes the dataframe of a single stock, determines the mean, median, or mode\"\"\"\n                \n    if statistic == 'mean':\n        center = np.mean(stock_df['Close%Change'])\n\n    elif statistic == 'median':\n        center = np.median(stock_df['Close%Change'])\n\n    elif statistic == 'mode':\n        test_column = stock_df['Close%Change']\n        y, x, _ = plt.hist(test_column, bins = 10*int(np.sqrt(len(test_column))))\n\n        md =  x[np.where(y == y.max())]\n        center = md[0]\n        \n    return center","18c84c99":"def add_resampled_increase(full_data, previous_days = [2, 3, 4], statistic = 'mean' ):\n    \n    \"\"\"Takes dataframe generated by 'collect_data' function, \n    and list of desired ranges of days, \n    and statistic to use for central tendancy (mean, median or mode),\n    then creates new columns that give the 'x' day average. \"\"\"\n    \n    # Sort by stock, then dates\n    resample_df = full_data.sort_values(['Stock', 'Date'], ascending=False)\n    \n    # For each time period in previous days\n    new_columns_dict = {}\n    for index, days in enumerate(previous_days):\n        \n        new_column = []\n        normalized_volume = []\n        \n        avg_change_value = []\n         \n        # For each stock    \n        for stock in resample_df['Stock'].unique():\n            stock_df = resample_df[resample_df[\"Stock\"] == stock].copy()\n            \n            # Find mean, median, or mode\n            center = find_distribution_center(stock_df, statistic)\n            stock_df['AvgChange'] = center\n            \n            # Cleans up column for 'Normal Vol'\n            if stock_df['Volume'].max() > 0:\n                stock_df['Normal Vol'] = (stock_df['Volume'] - stock_df['Volume'].min())\/(stock_df['Volume'].max() - stock_df['Volume'].min())\n            else:\n                stock_df['Normal Vol'] = 0\n            \n            # Count the number of days in each stock dataframe\n            df_length = len(stock_df)\n                        \n            # For each date in the stock df, find the average of the previous 'x' days\n            # If at the end of the list, use the mean of all the entries collected so far, \n            # or use the average of a smaller time period. \n            changes = []\n            \n            for i in range(df_length):\n                \n                # Add the 'x' day average if not at the end of the stock's dataframe\n                if (df_length-2) > (i + days): \n\n                    sample_change = (stock_df.iloc[i, 1] - stock_df.iloc[i+days, 1]) \/ stock_df.iloc[i+days, 1]\n                    changes.append(sample_change)\n                    new_column.append(sample_change)\n                    \n                # Take average of smaller period size   \n                elif len(changes) == 0:\n                    sample_change2 = (stock_df.iloc[i, 1] - stock_df.iloc[-1, 1]) \/ stock_df.iloc[-1, 1]\n                    new_column.append(sample_change2)\n                    \n                # Take average of the values already collected    \n                else:\n                    new_column.append(np.mean(changes))\n                    \n            # Take values for new columns\n            if index == (len(previous_days)-1):\n                avg_change_value.append(stock_df['AvgChange'])\n                normalized_volume.append(stock_df['Normal Vol'])\n            else: continue\n              \n        # Add values to dictionary\n        column_name = \"%change in previous \" + str(days) + \" days\"\n        new_columns_dict[column_name] = new_column\n        \n        # Add new columns \n        if index == (len(previous_days)-1):\n            new_columns_dict[\"Volume (normalized)\"] = pd.concat(normalized_volume)\n            new_columns_dict[\"Avg daily % change\"] = pd.concat(avg_change_value)\n        else: continue\n                \n    return pd.DataFrame(new_columns_dict)","2ad925c6":"def get_cluster_labels(full_data, k, random_state = 138, model_type = 'KMeans'):\n    \n    \"\"\"Takes dataframe generated by 'collect_data' function, number of desired clutering groups \n    and the type of clustering algorithm to be used, organized data by Dates and Stocks, clusters stocks for each day,\n    and returns a dataframe with the group label of each stock for each day. \"\"\"\n        \n    #Groups data to be indexed by date, stock name\n    grouped_df= full_data.sort_values(['Date','Stock'], ascending=False).set_index(['Date', 'Stock'])\n    \n    \n    # Parameters\n    min_cluster_number = int(len(full_data.Stock.unique())\/k)\n        \n    # List of possible clustering algorithms\n    models = {'KMeans': KMeans(n_clusters = k, random_state=random_state), \n            \n            'Agglomerative': AgglomerativeClustering(n_clusters=k, affinity='euclidean', memory=None, connectivity=None, compute_full_tree='auto', linkage='ward', distance_threshold=None),\n                      \n            'DBSCAN' : DBSCAN(eps=0.34, min_samples=k, metric='euclidean', metric_params=None, algorithm='auto', leaf_size=30, p=None, n_jobs=None),\n              \n            'MeanShift': MeanShift(bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=300),\n              \n            'Spectral' : SpectralClustering(n_clusters=k, eigen_solver=None, n_components=None, random_state=None, n_init=10, gamma=1.0, affinity='rbf', n_neighbors=10, eigen_tol=0.0, assign_labels='kmeans', degree=3, coef0=1, kernel_params=None, n_jobs=None)\n   \n           }   \n    \n    #Choose training data from full dataset\n    list_of_dates = full_data.Date.unique()\n\n    cluster_df = grouped_df.iloc[:, 6:-1]\n    cluster_df.sort_index(inplace=True)\n    dates = list_of_dates[:len(list_of_dates)]\n    \n    # Cluster stocks together everyday\n    \n    list_of_dfs = []\n    \n    for date in dates:\n        \n        if len(cluster_df.loc[date, ].index.values) > k:\n            #Training set\n            train = cluster_df.loc[date,   ]\n            \n            train_normal = normalize(train, norm='l2')\n            \n            #Pick model from list\n            model = models[model_type]\n            #Fit and predict\n            \n            if model_type == 'DBSCAN':\n                pred = model.fit(train_normal)\n                y_pred = model.labels_\n            else:\n                y_pred = model.fit_predict(train_normal)\n                \n            y_pred = y_pred.reshape(1, len(y_pred))\n            \n            # Create a dataframe with the stock and its predicted group for that day\n            df_results = pd.DataFrame(y_pred, columns = train.index.values, index=[date]).astype('float64')\n\n            list_of_dfs.append(df_results)\n        \n            \n        else:\n            continue\n            \n    # Data Frame of the clustering results for each day\n    return pd.concat(list_of_dfs)    ","699a9d8f":"def get_probability_vector(labeled_df):\n    \n    \"\"\"Takes dataframe generated by 'get_cluster_labels' function, and the number of k-neighbors,\n    returns dataframe with stock names for rows, and groups for columns. \n    The values indicate the probability that the stock will be in a specific group.\"\"\"\n    \n    # Number of clustered groups\n    group_range = int(max(labeled_df.max(axis=1))) + 1\n    \n    probs_df = []\n    column_names = []\n    index_names = []\n    \n    #For each stock \n    for col in labeled_df.columns:\n        \n        probs = []\n        \n        # Count number of times stock was put in each group, normalized to give percentages\n        counts  = labeled_df[col].value_counts(normalize = True)\n        index_names.append(col)\n        \n        # Adds counts for each group to a list, or zero if stock was never in a group\n        # Create column label\n        for i in range(group_range):\n            if counts.index.sort_values().isin([i]).any():\n                probs.append(counts[i])\n\n            else :   \n                probs.append(0)\n                \n            if len(column_names) < group_range:\n                column_names.append(\"% in Group \" + str(i))\n            else: \n                continue\n\n        probs_df.append(probs)\n    \n    # Dataframe of each stock, and the probablity of being in each group\n    return pd.DataFrame(probs_df, columns = column_names, index=index_names)","57053247":"def collect_data(urls, testrange = 3):\n    \n    \"\"\"Takes list of stock-collection urls, grabs urls of each stock, scrapes stock data, adds features,\n    returns dataframe with data for all stocks going back as many years as specified by 'testrange'.\"\"\"\n    \n    \n    #take list of urls, returns stock-specific urls and names\n    all_urls = get_urls_names(urls, testrange)\n\n    #scrapes table data from each stock webpage, concatenates together\n    full_data = get_csv_table(all_urls['urls'], all_urls['names'])\n    \n    \n    \n    #adds 'normalized' features\n    full_data[\"AdjClose%Change\"] = (full_data['Adj Close'] - full_data['Open'])\/full_data['Open']\n    full_data[\"High%Change\"] = (full_data['High'] - full_data['Open'])\/full_data['Open']\n    full_data[\"Low%Change\"] = (full_data['Low'] - full_data['Open'])\/full_data['Open']\n    full_data[\"Close%Change\"] = (full_data['Close'] - full_data['Open'])\/full_data['Open']\n    \n    \n    return full_data","01c3b7e0":"def get_final_dataframe(data, fingerprint):\n    \n    \"\"\"Takes main dataframe with new clustering results, and the probability vector 'fingerprint', \n    returns a dataframe with the Stock tickers and Average Daily Growth, arranged in columns by group. \n    Intended for displaying results in the form of a dataframe.\"\"\"\n    \n    groups_dict = {}\n    \n    # For each group in the probability vecor\n    for i in range(len(fingerprint['results'].unique())):\n        cluster_dict = {}\n        stock_list = []\n        rank_list = []\n        \n        # Selects a group then loops through stocks in that group\n        group = fingerprint[fingerprint['results'] == i]\n        stocks = group.index.values\n        for stock in stocks:\n            \n            # Adds stock to list\n            stock_list.append(stock)\n    \n            # Adds average growth to list\n            daily_change_df = data[data['Stock']==stock].round(4)\n            rank_list.append(daily_change_df['Avg daily % change'].values[0]*100 )\n        \n        \n        # Adds values to dictionary\n        cluster_dict['Ticker'] = stock_list\n        cluster_dict['Daily Growth'] = rank_list\n        name = 'Group ' + str(i) \n        groups_dict[name] = cluster_dict\n    \n    \n    #Final dataframe\n \n    final_dict = {}\n    for group in groups_dict:\n        lis_df = pd.DataFrame(groups_dict[group]).sort_values('Daily Growth', ascending=False).reset_index(drop=True)\n        final_dict[group] = lis_df\n        \n    groups_df = pd.concat(final_dict, axis = 1, names = ['Groups: ', 'Details: ']).fillna('')\n    \n    return groups_df\n    ","e26091be":"def get_final_chart(data, fingerprint):\n    \n    \"\"\"Takes main dataframe with new clustering results, and the probability vector 'fingerprint', \n    returns a chart that is formated for creating seaborn barplots.\"\"\"\n    \n    df_list = []\n    \n    # For each group in the probability vecor\n    for i in range(len(fingerprint['results'].unique())):\n        cluster_dict = {}\n        stock_list = []\n        rank_list = []\n        groups_list = []\n        \n        # Selects a group then loops through stocks in that group\n        group = fingerprint[fingerprint['results'] == i]\n        stocks = group.index.values\n        for stock in stocks:\n            # Adds stock to list\n            stock_list.append(stock)\n            \n            # Adds average growth to list\n            daily_change_df = data[data['Stock']==stock].round(4)\n            rank_list.append(daily_change_df['Avg daily % change'].values[0]*100 )\n            \n            # Adds group label to list\n            groups_list.append(i)\n        \n        # Adds lists to dictionary\n        cluster_dict['Ticker'] = stock_list\n        cluster_dict['Avg Dly % Chg'] = rank_list\n        cluster_dict['Group'] = groups_list\n        \n        df = pd.DataFrame(cluster_dict)\n        df_list.append(df)\n    \n    \n    #Final dataframe\n     \n    chart_df = pd.concat(df_list) \n    chart_df.sort_values(['Avg Dly % Chg'], ascending = False, inplace = True)\n\n    \n    return chart_df\n    ","14715343":"def drop_new_stocks(scraped_df):\n\n    \"\"\"Takes dataframe from 'collect_data' function, \n    drops the stocks with less recorded data than a quater of a year,\n    returns a dataframe without those stocks. \"\"\"\n    \n    # Count the instances of each stock\n    entry_count = scraped_df.Stock.value_counts()\n\n    drop_stock = []\n    \n    # Check if stock count is less than a quarter of year\n    for i, count in enumerate(entry_count):\n        if count < 90:\n            drop_stock.append(entry_count.index[i])\n            \n    # Filter out the stocks in drop_stocks\n    return scraped_df[~scraped_df.Stock.isin(drop_stock)]\n","a0a9ac2f":"def clustered_groups(full_data, groups=7, prob_groups=5, previous_days = [2, 3, 4], cluster_type = 'DBSCAN', model_type= 'KMeans', random_state = 138, agg_statistic = 'mean'):\n    \n    \"\"\"Takes dataframe generated by 'collect_data' function, performs two layers of clustering, \n    returns dictionary for results of double clustering. The dictionary contains a presentable dataframe, \n    or dataframe that can be used by 'show_barplot'function. \"\"\"\n    \n    full_data  = drop_new_stocks(full_data)\n    \n    #adds new columns showing % change of previous days\n    resampled = add_resampled_increase(full_data, previous_days, statistic = agg_statistic)\n    \n   \n    #Concatenates new columns\n    full_data = pd.concat([full_data, resampled], axis = 1)\n    \n    min_cluster_number = int(len(full_data.Stock.unique())\/groups)\n    \n    #Clusters stocks by changes of each day\n    daily_labels = get_cluster_labels(full_data, prob_groups, random_state, model_type = cluster_type)\n    \n    \n    #Creates probability vector, or \"fingerprint\"\n    probs = get_probability_vector(daily_labels)\n    \n    \n    #Various Model Options to cluster by \"fingerprint\"\n    \n\n    models = {'KMeans': KMeans(n_clusters = groups, random_state=random_state), \n\n                'Agglomerative': AgglomerativeClustering(n_clusters=groups, affinity='euclidean', memory=None, connectivity=None, compute_full_tree='auto', linkage='ward', distance_threshold=None),\n\n                'MeanShift': MeanShift(bandwidth=None, seeds=None, bin_seeding=False, min_bin_freq=1, cluster_all=True, n_jobs=None, max_iter=300), \n              \n                #'DBSCAN' : hdbscan.HDBSCAN(algorithm='best', alpha=1.0, approx_min_span_tree=True, gen_min_span_tree=False, leaf_size=40, metric='euclidean', min_cluster_size=min_cluster_number, min_samples=None, p=None),\n\n                'DBSCAN' : DBSCAN(eps=0.06, min_samples=7, metric='euclidean', metric_params=None, algorithm='auto', leaf_size=30, p=None, n_jobs=None),\n\n                'Spectral' : SpectralClustering(n_clusters=groups, eigen_solver=None, n_components=None, random_state=None, n_init=10, gamma=1.0, affinity='rbf', n_neighbors=10, eigen_tol=0.0, assign_labels='kmeans', degree=3, coef0=1, kernel_params=None, n_jobs=None)\n           }\n    \n\n    #Clusters stocks by \"fingerprint\"\n    model = models[model_type]\n    y_pred_test = model.fit_predict(probs)\n    \n    \n    #Adds clustering results to previous dataframe\n    probs['results'] = y_pred_test\n    \n    \n    #Creates final Dataframe\n    df = get_final_dataframe(full_data, probs)\n    chartdf = get_final_chart(full_data, probs)\n    \n    return {'DataFrame': df, 'ChartData': chartdf}\n","5372fdb3":"# Possible palettes that I think are cool\n#palette_list = ['gist_earth', 'dark', 'brg_r', 'gist_stern_r', 'nipy_spectral_r']\n\n\ndef show_barplot(chart_table, selection_size = 10, ax = None):\n    \n    \"\"\"Takes dataframe generated by \"clustered_groups\" function, \n    the desired selection size from each group, and \n    which axes to place chart on, if necessary, \n    returns a barplot of the highest performing stocks, \n    with colors representing their predicted group.\"\"\"\n    \n    \n    # Sort the results by the highest performing stocks\n    chart_table.sort_values(['Avg Dly % Chg', 'Group'], ascending = False, inplace = True)\n    \n    # Take the top stocks, then order them by group\n    topg = chart_table.head(selection_size)\n    top = topg.sort_values(['Group', 'Avg Dly % Chg'], ascending = False)\n    \n    # Create a barplot\n    sns.set_palette('dark') \n    sns.axes_style(\"darkgrid\")\n    plt.figure(figsize=(10, 7))\n    g = sns.barplot(x=\"Ticker\", y='Avg Dly % Chg', hue=\"Group\", data=top, dodge=False, ax = ax)\n    g.set_xticklabels(rotation=45, labels = top['Ticker'].values)\n    g.set_ylabel('Daily Growth (%)')\n    \n\n\n    # Fill out the bars so they all touch\n    for bar in g.patches:\n\n        newwidth = .9\n\n        x = bar.get_x()\n        width = bar.get_width()\n        centre = x+width\/2.\n\n        position = centre-newwidth\/2.\n\n\n        bar.set_x(position)\n        bar.set_width(newwidth)\n\n\n    return g","8b429093":"def metric_distances(train_data):\n    \n    \"\"\"Takes dataframe, computes the distances between \n    every pair of points in each feature, \n    returns dataframe of all the distances.\"\"\"\n    \n    distances_columns = []\n    \n    for col in train_data.columns:\n        #pick out series\n        series = train_data[col]\n        \n        distances_list = []\n        for i in range(len(series)):\n            \n            for j in range(len(series)):\n                #compute distance if points are different\n                if i == j:\n                    continue\n                    \n                else:\n                    \n                    distance = series[i] - series[j]\n                    distances_list.append(distance.round(3))\n        # take the absolute value of each distance\n        distances_columns.append(pd.Series(distances_list).abs())\n    #create dataframe and transpose    \n    df =pd.DataFrame(distances_columns, index = train_data.columns).T\n    \n    return df\n    ","d2d1ce2d":"# For scraping\nactive_one  = 'https:\/\/finance.yahoo.com\/most-active?count=100&offset=0'\nactive_two = 'https:\/\/finance.yahoo.com\/most-active?count=100&offset=100'\nactive_three = 'https:\/\/finance.yahoo.com\/most-active?count=100&offset=200'\ngainers = 'https:\/\/finance.yahoo.com\/gainers'\nnasdaq = 'https:\/\/finance.yahoo.com\/quote\/%5EIXIC\/components?p=%5EIXIC'\ndow = 'https:\/\/finance.yahoo.com\/quote\/%5EDJI\/components?p=%5EDJI'\nnyse = 'https:\/\/finance.yahoo.com\/quote\/%5ENYA\/components?p=%5ENYA'\ntrending = 'https:\/\/finance.yahoo.com\/trending-tickers'\n\n#main_urls = [active_one, active_two, trending, nasdaq, dow]\n\n\n#For tests\n\n       #'https:\/\/finance.yahoo.com\/quote\/AAPL?p=AAPL'\nAAPL = 'https:\/\/finance.yahoo.com\/quote\/AAPL\/history?p=AAPL'\nAAPL_five_1027 = 'https:\/\/finance.yahoo.com\/quote\/AAPL\/history?period1=1445904000&period2=1603756800&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true'\nAAPL_five_1025 = 'https:\/\/finance.yahoo.com\/quote\/AAPL\/history?period1=1445731200&period2=1603584000&interval=1d&filter=history&frequency=1d&includeAdjustedClose=true'\n\ntest_urls = [nasdaq, nyse]\n\n\n","ceb7b903":"#blah\n#Lists of htmls to pages with collections of different stocks\nactive_long = [active_one, active_two, active_three, trending, gainers]\nactive = [active_one, trending]\nindexes = [nasdaq, nyse]\n\n## Collect data\n# Goes to each page, grabs the links to the stocks on the page\n# then grabs table data from the stock page, combines to one big dataframe\n\ntrending_data = collect_data(active, testrange = 2)\nindex_data = collect_data(indexes, testrange = 2)","5c8d0f5c":"#Data from stock indexes\nindex_data","411b35f6":"trending_data","5ae8d41d":"get_cluster_labels(trending_data, 5)","7b328e00":"## EDA of all data\n\n# Add new features to scraped data\nresampled_eda = add_resampled_increase(index_data, statistic = 'mean')\nconcat = pd.concat([index_data, resampled_eda], axis = 1)\n\n# Select data that will be used for training the model\ntrain = concat.iloc[:, -9:]","6ad77575":"#No null values\ntrain.info()","2721b119":"# Helpful for determining clustering parameters\ntrain.describe()","e56920ba":"##Create pair plot of training data to see distribution of data\n\n# Will help to decide which model to use for clustering\nsns.set_context(\"poster\", font_scale=0.5) \nsns.pairplot(train, plot_kws={\"s\": 10})\n","198d2c23":"normal_train = normalize(train)\ndfNormal = pd.DataFrame(normal_train, columns = train.columns)\ndfNormal.describe()","65a18f78":"## Sort main dataframe by Date, then Stock to pick out one day of data\ngrouped_train= concat.sort_values(['Date','Stock'], ascending=False).set_index(['Date', 'Stock'])\ngrouped_train\n","9558fe3e":"#Pick out one day of data\ndate = concat.Date.unique()[0]\ndf = grouped_train.loc[date,   ]","3d09fd61":"# Grab training data and normalize\nday_train = df.iloc[:, -9:]\nday_normal_train = normalize(day_train)\ndfNormal_day = pd.DataFrame(day_normal_train, columns = day_train.columns)\n\ndfNormal_day.head()","5800788c":"# Compute the distance of every pair of points\ndistance_df = metric_distances(dfNormal_day)","aa7bdbbb":"# Show statistics \ndistance_df.describe()","832592ce":"# Create the proability vectors for each stock\ndaily_labels = get_cluster_labels(concat, k=int(np.sqrt(len(trending_data.Stock.unique()))), random_state=138, model_type = 'Spectral')\nprobs = get_probability_vector(daily_labels)\n","02bc5deb":"probs.head()","7e5d983c":"# Show statistics of probability vectors\nprobs.describe()","e1cccaa0":"# Get the distance between each point on each dimension of the vectors\nprob_distance_df = metric_distances(probs)","b80d3588":"# Show statistics of distances\nprob_distance_df.describe()","11d71983":"# Get dataframe for results\ntrending_chartData = clustered_groups(trending_data, prob_groups=int(np.sqrt(len(trending_data.Stock.unique()))), groups = 10, cluster_type = 'Spectral', model_type = 'Agglomerative', agg_statistic = 'mean')","1ca9980b":"# Results in the form of dataframe, showing all stocks\ntrending_chartData[\"DataFrame\"]","51e8a732":"# Results showing just the top 20 stocks, by daily growth\nshow_barplot(trending_chartData['ChartData'], 20).set_title('1st Cluster: DBSCAN, 2nd Cluster: Agglomerative')\nplt.show()","7ca2ee0a":"# Get dataframe for results\ntest_chartData = clustered_groups(index_data, prob_groups=int(np.sqrt(len(index_data.Stock.unique()))), groups = 10, cluster_type = 'Spectral', model_type = 'Agglomerative', agg_statistic = 'mean')","39282468":"# Results in the form of dataframe, showing all stocks\ntest_chartData[\"DataFrame\"]","a98d638a":"# Results showing just the top 20 stocks, by daily growth\nshow_barplot(test_chartData['ChartData'], 20).set_title('1st Cluster: DBSCAN, 2nd Cluster: Agglomerative')\nplt.show()","046d6968":"### Results for Index stocks","cb1ef754":"#### I focused mainly on finding statistics that would help determine good clustering parameters. Because many of the algorithms cluster by distance between points of each feature, I computed the distance between every pair of points in each feature of the scrapped stock data of one day, and then for the probability vectors. Knowing the general range of distances between these point may help determine paramters for density-based clustering, and for linkage thresholds in the agglomerative clustering algorithm. ","6d25abdf":"### Statistics of normalized training data","2a8e91bf":"### General stats for scrapped data","9da87b99":"### Statistics of distances between points for one day of stock data","2a4cd3f7":"### Import Libraries","3f992f1a":"## EDA","8dfdb7c5":"### Statistics for the distance between probability vectors","25e0fb46":"## Functions to be used","9ef985cd":"## Results of clustering","e58e16ae":"#### In this project, I scraped yahoo finance stock pages and clustered stocks based off of the similarity of their daily movements. The goal was to identify correlated stocks and help diversify my investment portfolio. \n\n#### I really don't know that much about doing well in the stock market. When I first started buying stocks, I basically picked the most popular ones. Over time I noticed that many of my investments followed similar trends, I had picked too many correlated stocks and needed to diversify things. This is an attempt to group correlated stocks together and pick the top performers from each group. \n\n#### To cluster stocks by similar trends, I employed two layers of clustering algorithms. The first layer groups the stocks for every day, generating as many grouping results as there are days in the dataframe. Since a stocks will change groups depending on the day, I counted each time a stock was in a certain group, then calculated the probability that it would be in a certain group. Depending on how many cluster groups I chose, this created a dataframe with the probablility of being in each group as a feature. This approach is similar to (or is?) a count vectorizer. Then I clustered together the dataframes of probabilites, which groups stocks by probability vectors that are near eachother in the vector space. \n\n#### The general plan is to buy about 20 stocks and have 2 or 3 stocks from each correlated group. I tested 2 years of data for each stock, scrapping a DataFrame of index stocks and a DataFrame for Trending stocks. I used Spectral Clustering for the vectorizer and Agglomertive Clustering to group the probability vectors. The results are displayed in a bar chart showing the average daily growth on the y-axis, with the color of the bars indicating the group.\n\n#### There may be, and probably is, a better way to cluster stocks like this, but I was curious to see how well I could do based off of my intuition, and without having to do hours of research in stock analysis (yet). In the future I plan on learning more about algorithmic trading and adjusting these ideas as necessary. \n\n\n#### Note: I'm pretty new to all of this and I'm looking for ways to improve. Comments, suggestions, and constructive criticisms will be very much appreciated!\n","76907889":"## Summary","83b58421":"## Scraping data ","afc76b5f":"### Results for Trending stocks"}}