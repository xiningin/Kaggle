{"cell_type":{"3b799a01":"code","cbaaedc9":"code","4777e5a7":"code","fafb5d76":"code","753c8fe4":"code","1a950103":"code","8fd1cec4":"code","90a29aab":"code","385c05df":"code","615e9eba":"code","6463af3b":"code","5e10b26a":"code","9d8299dc":"code","64c7c61c":"code","8db729d9":"code","bb3edda1":"code","39e11d5c":"code","f9d15901":"code","33c9bf72":"code","11ca49ea":"code","26beef5f":"code","d4527cca":"code","87cacf6b":"code","8ca5fdde":"code","d98e473f":"code","3cc85916":"code","b9509e98":"code","1fdbaac3":"code","fa68750d":"markdown"},"source":{"3b799a01":"!pip install \/kaggle\/input\/iterate\/iterative-stratification-master\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","cbaaedc9":"from __future__ import print_function, division\n\nimport torch\nimport torch.nn as nn\nfrom  torch.autograd import Variable\nfrom torch.optim import lr_scheduler\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nfrom torch.utils import data as torch_data \nfrom torch.utils.data import Dataset, DataLoader\n\nfrom torchvision import transforms, utils\nimport torchvision \nimport torchvision.models as models\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom skimage import io, transform\n\n# from iterate.iterativestratification.iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport itertools\nfrom PIL import Image\nimport random\nimport pandas as pd\nfrom tqdm import tqdm\nimport csv\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nimport logging\nimport warnings\nimport gc\nwarnings.filterwarnings(\"ignore\")\nimport cv2 as cv\nimport copy\nfrom random import shuffle\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import QuantileTransformer","4777e5a7":"seed = 2020\n\ncudnn.benchmark = False\ncudnn.deterministic = True\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n","fafb5d76":"train_features = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_nonscored.csv')\ntest_features = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ntrain_drug = pd.read_csv('\/kaggle\/input\/lish-moa\/train_drug.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')","753c8fe4":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","1a950103":"#RankGauss\nfor col in (GENES + CELLS):\n\n    transformer = QuantileTransformer(n_quantiles=100,random_state=seed, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","8fd1cec4":"# import seaborn as sns\n# sns.displot(train_features['g-3'].values)","90a29aab":"# GENES\nn_comp = 650  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=seed).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","385c05df":"#CELLS\nn_comp = 100  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=seed).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","615e9eba":"dic1 = {24:1, 72:2, 48:3}\ndic2 = {'D1':1, 'D2':2}\n\ntrain_features['cp_time'] = train_features.cp_time.map(dic1)\ntrain_features['cp_dose'] = train_features.cp_dose.map(dic2)\n\ntest_features['cp_time'] = test_features.cp_time.map(dic1)\ntest_features['cp_dose'] = test_features.cp_dose.map(dic2)","6463af3b":"# bkup_df = train_features.copy()\ncols = train_features.columns","5e10b26a":"def shuffle_cols(df,cols):\n    colnames     = list(cols)\n    colnames1    = colnames.copy()\n    colnames1    = np.random.RandomState(seed).permutation(colnames1[4:],)\n    colnames[4:] = colnames1\n    #pd.DataFrame({'columns':colnames}).to_csv('columns.csv',index=False)\n    df = df.reindex(columns=colnames)\n    return df","9d8299dc":"# train_features  = shuffle_cols(train_features,cols)\n# test_features   = shuffle_cols(test_features,cols)","64c7c61c":"import torch.nn.functional as F\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv1 = nn.Conv2d(in_channels=1,out_channels=32,kernel_size=3,stride=1)\n        self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size=3,stride=1)\n        self.conv3 = nn.Conv2d(in_channels=64,out_channels=128,kernel_size=1,stride=1)\n        self.relu  = nn.ReLU(inplace=True)\n        self.bn1   = nn.BatchNorm2d(num_features=32)\n        self.bn2   = nn.BatchNorm2d(num_features=64)\n        self.bn3   = nn.BatchNorm2d(num_features=128)\n        self.mp    = nn.AvgPool2d(kernel_size=2,stride=1)\n        self.do    = nn.Dropout2d(p=0.3)\n        self.l1    = nn.Linear(139394,700)\n        self.l2    = nn.Linear(700, 206)\n        self.feat1 = nn.Linear(2,4)\n        self.feat2 = nn.Linear(4,2)\n        \n    def forward(self, x , z):\n        x = self.conv_layers1(x)\n        x = self.conv_layers2(x)\n        x = self.conv_layers3(x)\n        N ,_,_,_ = x.size()\n        x = x.view(N,-1)\n        z = self.feat1(z)\n        x = self.do(x)\n        z = self.feat2(z)\n        \n        w = torch.cat((x,z),1)\n        w = w.view(N,-1)\n        print(w.size())\n        w = self.l1(w)\n#         x = self.do(x)\n        w = self.relu(w)\n        w = self.l2(w)\n        return w\n    \n    def conv_layers1(self, x):\n        x = self.conv1(x)\n        x = self.relu(x)\n#         x = self.bn1(x)\n        x = self.mp(x)\n#         x = self.do(x)\n        return x\n    \n    def conv_layers2(self, x):\n        x = self.conv2(x)\n        x = self.relu(x)\n        x = self.bn2(x)\n        x = self.mp(x)\n#         x = self.do(x)\n        return x\n    \n    def conv_layers3(self, x):\n        x = self.conv3(x)\n        x = self.relu(x)\n#         x = self.do(x)\n#         x = self.bn3(x)\n        x = self.mp(x)\n        return x\n","8db729d9":"use_gpu = torch.cuda.is_available()\nprint(use_gpu)","bb3edda1":"weight = train_targets_scored.iloc[:,1:].sum()\nwght = 1-(np.log(weight)\/np.mean(weight))\nwght = wght.values\nwght = torch.tensor(wght).cuda()\n# wght","39e11d5c":"import torch\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\n\nclass SmoothBCEwLogits(_WeightedLoss):\n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = wght\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","f9d15901":"class LabelSmoothingLoss(nn.Module):\n    def __init__(self, classes, smoothing=0.0, dim=-1):\n        super(LabelSmoothingLoss, self).__init__()\n        self.confidence = 1.0 - smoothing\n        self.smoothing = smoothing\n        self.cls = classes\n        self.dim = dim\n\n    def forward(self, pred, target):\n        pred = pred.log_softmax(dim=self.dim)\n        with torch.no_grad():\n            # true_dist = pred.data.clone()\n            true_dist = torch.zeros_like(pred)\n            true_dist.fill_(self.smoothing \/ (self.cls - 1))\n            true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n        return torch.mean(torch.sum(-true_dist * pred, dim=self.dim))    ","33c9bf72":"def convert_to_img(row):\n    image = np.array(row[2:].values,dtype=float)\n    ax = image[4:1604]\n    \n    ax= ax.reshape((40,40))\n#     ax = np.random.shuffle(ax)\n    ax  = (ax - np.min(ax))\/(np.max(ax)-np.min(ax))\n#     ax = ax * 255\n    ax = ax.reshape((-1, ax.shape[0], ax.shape[1]))\n    return ax\n\n\nclass myDataset(torch_data.Dataset):\n    def __init__(self, feats, labels):\n        self.features = feats\n        self.labels = labels\n        \n    def __len__(self):\n        return (self.features.shape[0])\n\n    def __getitem__(self, idx):\n        img_name = str(self.features.iloc[idx, 0])\n        img1 = convert_to_img(self.features.iloc[idx, :])\n        feats = self.features.iloc[idx, 2:4]\n        d = {\"img\": img_name}\n        d[\"label\"] = torch.tensor(self.labels.iloc[idx, 1:], dtype=torch.float) \n        d[\"img1\"] = torch.tensor(img1, dtype=torch.float)\n        d[\"feats\"] = torch.tensor(feats, dtype=torch.float)\n\n        \n        return d","11ca49ea":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","26beef5f":"def create_train(X_train,Y_train):\n    train_dataset = myDataset(feats = X_train, labels=Y_train)\n    trainloader = torch_data.DataLoader(train_dataset, batch_size= 64, shuffle=True)\n    return trainloader\n\ndef create_test(X_test,Y_test):\n    val_dataset   = myDataset(feats = X_test, labels=Y_test)\n    valloader = torch_data.DataLoader(val_dataset, batch_size= 64, shuffle=False)\n    return valloader\n    ","d4527cca":"def train(epoches, model,dataloaders,f, num_label=206):\n    since = time.time()\n    best_model_wts = copy.deepcopy(model.state_dict())\n    running_loss_train = []\n    running_loss_val = []\n    running_corrects_train = []\n    running_corrects_val = []\n    epoch_acc_hist = {\"train\": [], \"valc\": []}\n    epoch_loss_hist = {\"train\": [], \"val\": []}\n    \n    \n    LEARNING_RATE = 8e-2\n    WEIGHT_DECAY = 1e-3\n    loss_fn = nn.BCEWithLogitsLoss()\n    criterion = SmoothBCEwLogits(smoothing =0.001)\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n                                              pct_start=0.1,\n                                              div_factor=1e3,\n                                              max_lr=1e-2, \n                                              epochs=2, \n                                              steps_per_epoch=len(trainloader))\n    early_stopping_steps = 3\n    early_step = 1\n    \n    min_loss = 9999999\n    print(f\"Fold {f}\/{n_splt}\")\n    print(\"- \" * 15)\n    for epoch in range(epoches):\n        print(f\"Epoch {epoch}\/{epoches}\")\n\n        for phase in [\"train\", \"val\"]:\n            if phase == \"train\":\n                model.train()\n            else:\n                model.eval()\n\n            running_loss = 0.0\n            running_corrects = 0.0\n            \n            for batch_idx, batch_data in enumerate(tqdm(dataloaders[phase])):\n                img1  = Variable(batch_data[\"img1\"].float().cuda())\n                feats = Variable(batch_data[\"feats\"].float().cuda())\n                label = Variable(batch_data[\"label\"].float().cuda())\n                model.zero_grad()\n\n                with torch.set_grad_enabled(phase == \"train\"):\n                    out = model(img1,feats)\n                    batch_loss = 0.0\n                    batch_loss += criterion(out, label)\n\n                    if phase == \"train\":\n                        batch_loss.backward()\n                        optimizer.step()\n\n                running_loss += batch_loss * img1.size(0)\n                if phase == \"train\":\n                    running_loss_train.append(batch_loss.item())\n                elif phase == \"val\":\n                    running_loss_val.append(batch_loss.item())\n                batch_corrects = 0.0\n\n            epoch_loss = running_loss \/ len(dataloaders[phase].dataset)\n            epoch_loss_hist[phase].append(epoch_loss)\n\n            print('-------------{} Loss: {:.4f}'.format(phase, epoch_loss))\n            \n \n                \n            if phase == \"val\" and epoch_loss < min_loss:\n                min_loss = epoch_loss\n                best_model_wts = copy.deepcopy(model.state_dict())\n                torch.save({\"current_model_wts\": model.state_dict(),\n                            \"best_model_wts\": best_model_wts,\n                            \"current_epoch\": epoch,\n                            \"optimizer_wts\": optimizer.state_dict(),\n                            \"running_loss_train\": running_loss_train,\n                            \"running_loss_val\": running_loss_val,\n                            \"epoch_loss_hist\": epoch_loss_hist, },\n                            \"moa_model_fold_{}.pth\".format(str(f)))\n        if epoch_loss >= min_loss:\n            early_step = early_step + 1\n        if early_step == early_stopping_steps:\n            break\n                \n    gc.collect()\n    time_elapsed = time.time() - since\n    print('Fold Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Min val Loss: {:4f}'.format(min_loss))\n","87cacf6b":"n_splt = 6\nmskf = MultilabelStratifiedKFold(n_splits=n_splt)\nepoches = 5\nmodel = None\n\nfor f, (t_idx, v_idx) in enumerate(mskf.split(X=train_features, y=train_targets_scored)):\n    train_df = train_features.loc[t_idx,:].reset_index(drop=True)\n    train_y_df = train_targets_scored.loc[t_idx,:].reset_index(drop=True)\n    \n    valid_df = train_features.loc[v_idx,:].reset_index(drop=True)\n    valid_y_df = train_targets_scored.loc[v_idx,:].reset_index(drop=True)\n    \n    trainloader = create_train(train_df,train_y_df)\n    validloader = create_train(valid_df,valid_y_df)\n    \n    dataloaders = {\"train\": trainloader, \"val\": validloader}\n#     dataset_sizes = {\"train\":  len(train_dataset), \"val\": len(val_dataset)}\n    if model != None:\n        del model\n    torch.cuda.empty_cache()\n\n    model = Model()\n    if use_gpu:\n        model = model.cuda()\n        \n    train(epoches, model, dataloaders,f, num_label = 206)\n    \n    \n    ","8ca5fdde":"def inference_fn(model, dataloader):\n    model.eval()\n    preds  = []\n    labels = []\n    \n    for batch_idx, batch_data in enumerate(tqdm(dataloader)):\n        img1  = Variable(batch_data[\"img1\"].float().cuda())\n        feats = Variable(batch_data[\"feats\"].float().cuda())\n        label = Variable(batch_data[\"label\"].float().cuda())\n#         inputs = data['x'].to(device)\n\n        with torch.no_grad():\n            outputs = model(img1,feats)\n        \n        preds.append(outputs.sigmoid().detach().cpu().numpy())\n        labels.append(label.cpu().numpy())\n        \n    preds = np.concatenate(preds)\n    labels = np.concatenate(labels)\n    return preds,labels","d98e473f":"validloader = create_train(test_features,sample_submission)","3cc85916":"for i in range(n_splt):\n    if model != None:\n        del model\n    torch.cuda.empty_cache()\n    \n    model = Model()\n    model.eval()\n    model.load_state_dict(torch.load(f\"moa_model_fold_{i}.pth\")['current_model_wts'])\n    \n\n    model.to(device)\n\n    preds,labels = inference_fn(model, validloader)\n    preds += preds\nsubmission = preds\/n_splt","b9509e98":"preds","1fdbaac3":"# preds = preds - (np.mean(preds)\/3)\npreds[preds<0] = 0.000123\nsample_submission.iloc[:,1:] = preds\nsample_submission = sample_submission.fillna(0)\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission.head(100)","fa68750d":"This notebook uses real valued features and their pca transformed values to create a 40 * 40 image, then pass these images to a 3 convolutional layers and concanate its output vector with output of a linear layer which its inputs are cp_time & cp_dose.\nfull process of our work is shown in below image:\n\n<img src=\"https:\/\/i.ibb.co\/HnK8NDD\/cnn.png\" alt=\"cnn\" border=\"0\">"}}