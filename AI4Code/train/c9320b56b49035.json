{"cell_type":{"34cb8c12":"code","4a847d9a":"code","215db3f4":"code","4711408d":"code","d6cf3ad5":"code","fbeeb187":"code","1524f258":"code","ea86d0ed":"code","2ed0cf03":"code","605ccc2f":"code","b821836f":"code","054694b7":"code","35cdbc84":"code","d829cbc3":"code","ac94d022":"code","1fccc1ad":"code","31958724":"code","2fbca862":"code","59be8bcc":"code","d1939d02":"code","bb780b7a":"code","bc6ee494":"code","1a0a3592":"code","e787da5c":"code","4f13c25e":"markdown","d3b7d67d":"markdown","d92d3942":"markdown","240cb18a":"markdown","88fdda09":"markdown","43d36c4e":"markdown","c5c81415":"markdown","ad6025ac":"markdown","e0cf48dc":"markdown","fe17f82f":"markdown","0904eb43":"markdown","441bb69f":"markdown","e25e73fa":"markdown","4a7cfb0d":"markdown","8f06452c":"markdown"},"source":{"34cb8c12":"import pandas as pd\ndf=pd.read_csv(\"..\/input\/us-macroeconomic-data-19962020-source-fred\/macrodata.csv\",index_col=0,parse_dates=True)\ndf","4a847d9a":"import numpy as np\nimport matplotlib.pyplot as plt \n\nfrom statsmodels.tsa.stattools import adfuller #to check unit root in time series \nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\n\nimport seaborn as sns #for correlation heatmap\n\nimport warnings\nwarnings.filterwarnings('ignore')","215db3f4":"df.index.set_names(names='Date',inplace=True)","4711408d":"# add lags\nfor col in df.drop(['Regime'], axis=1):\n    for n in [3,6,9,12,18]:\n        df['{} {}M lag'.format(col, n)] = df[col].shift(n).ffill().values\n\n# 1 month ahead prediction\ndf[\"Regime\"]=df[\"Regime\"].shift(-1)","d6cf3ad5":"df=df.dropna(axis=0)","fbeeb187":"# Check stationarity in time series data\n# We will perform adfuller test to check unit roots 3 times. \n# First time for non-stationary series we will take first order difference\n# Second time we will take second order difference\n# Third time if there are still remaining non-stationary columns we will drop them from feature set\n\nfrom statsmodels.tsa.stattools import adfuller #to check unit root in time series \nthreshold=0.01 #significance level\nfor column in df.drop(['Regime'], axis=1):\n    result=adfuller(df[column])\n    if result[1]>threshold:\n        df[column]=df[column].diff()\ndf1=df.dropna(axis=0)\n\nfor column in df1.drop(['Regime'], axis=1):\n    result=adfuller(df1[column])\n    if result[1]>threshold:\n        df1[column]=df1[column].diff()\ndf1=df1.dropna(axis=0)\n\nnonstationary_col=[]\nfor column in df1.drop(['Regime'], axis=1):\n    result=adfuller(df1[column])\n    if result[1]>threshold:\n        nonstationary_col.append(column)\ndf1=df1.dropna(axis=0)\ndf1.drop(nonstationary_col,axis=1,inplace=True)","1524f258":"nonstationary_col","ea86d0ed":"from sklearn.preprocessing import StandardScaler\nfeatures=df1.drop(['Regime'],axis=1)\ncol_names=features.columns\n\nscaler=StandardScaler()\nscaler.fit(features)\nstandardized_features=scaler.transform(features)\n\ndf2=pd.DataFrame(data=standardized_features,columns=col_names,index=df1.index)\ndf2.insert(loc=1,column='Regime', value=df1['Regime'].values)\ndf2.shape","2ed0cf03":"df2","605ccc2f":"# import packages for modelling\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nfrom sklearn import metrics\n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, auc\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.feature_selection import SelectFromModel\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost as xgb\n\nfrom matplotlib import pyplot as mp\nimport seaborn as sns\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","b821836f":"Label = df2[\"Regime\"].apply(lambda regime: 1. if regime == 'Normal' else 0.)\ndf2.insert(loc=2, column=\"Label\", value=Label.values)","054694b7":"df_targets=df2['Label'].values\ndf_features=df2.drop(['Regime','Label'], axis=1)\n\ndf_training_features = df2[:'2014-09'].drop(['Regime','Label'], axis=1)\ndf_validation_features = df2['2014-10':].drop(['Regime','Label'], axis=1)\n\ndf_training_targets = df2[:'2014-09']['Label'].values\n\n\ndf_validation_targets = df2['2014-10':]['Label'].values","35cdbc84":"xtr=df_training_features[:'2006-03']\nytr=df2[:'2006-03']['Label'].values\nxdev=df_training_features['2006-04':]\nydev=df2['2006-04':'2014-09']['Label'].values","d829cbc3":"print(len(df_training_features),len(df_training_targets),len(df_targets))\nprint(len(df_validation_features),len(df_validation_targets),len(df_features))","ac94d022":"df_training_targets","1fccc1ad":"seed=8\nscoring='roc_auc' \nkfold = model_selection.TimeSeriesSplit(n_splits=2) \nmodels = []\n\nmodels.append(('LR', LogisticRegression(C=1e09)))\nmodels.append(('LR_L1', LogisticRegression(penalty = 'l1',solver='liblinear')))\nmodels.append(('LR_L2', LogisticRegression(penalty = 'l2')))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('GB', GradientBoostingClassifier()))\nmodels.append(('ABC', AdaBoostClassifier()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('XGB', xgb.XGBClassifier()))\n\nresults = []\nnames = []\nlb = preprocessing.LabelBinarizer()\n\nfor name, model in models:\n    cv_results = model_selection.cross_val_score(estimator = model, X = df_training_features, \n                                                 y = lb.fit_transform(df_training_targets), cv=kfold, scoring = scoring)\n    \n    model.fit(df_training_features, df_training_targets) # train the model\n    fpr, tpr, thresholds = metrics.roc_curve(df_training_targets, model.predict_proba(df_training_features)[:,1])\n    auc = metrics.roc_auc_score(df_training_targets,model.predict(df_training_features))\n    plt.plot(fpr, tpr, label='%s ROC (area = %0.2f)' % (name, auc))\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('1-Specificity(False Positive Rate)')\nplt.ylabel('Sensitivity(True Positive Rate)')\nplt.title('Receiver Operating Characteristic')\nplt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\nplt.show() \nwarnings.filterwarnings('ignore')","31958724":"fig = plt.figure()\nfig.suptitle('Algorithm Comparison based on Cross Validation Scores')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","2fbca862":"from sklearn.metrics import roc_auc_score\nC = np.reciprocal([0.00000001, 0.00000005, 0.0000001, 0.0000005, 0.000001, 0.000005, 0.00001, 0.00005, \n                         0.0001, 0.0005, 0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100, 500, 1000, 5000])\npenalty=['l1','l2']\n\npenals=pd.DataFrame(index=penalty)\ncs=[]\nscrs=[]\nfor p in penalty:\n        scores=[]\n        params=pd.DataFrame(index=C)\n        for c in C:\n            model=LogisticRegression(C=c,max_iter=10000,penalty=p,solver='liblinear')\n            lr1=model.fit(xtr,ytr)\n            ypreds=lr1.predict(xdev)\n            score=roc_auc_score(ydev,ypreds)\n            scores.append(score)\n        params['rocauc']=scores\n        maxc=params['rocauc'].idxmax()\n        maxsc=params['rocauc'].max()\n        scrs.append(maxsc)\n        cs.append(maxc)\npenals['C']=cs\npenals['score']=scrs\npenals","59be8bcc":"model=LogisticRegression(C=1000,penalty='l2',max_iter=10000,solver='liblinear')\nlr1=model.fit(df_training_features,df_training_targets)\nypreds=lr1.predict(df_validation_features)\nparam=lr1.get_params()\nscore=roc_auc_score(df_validation_targets,ypreds)\nscore","d1939d02":"seed=8\nscoring='roc_auc' \nkfold = model_selection.TimeSeriesSplit(n_splits=2) \nlb = preprocessing.LabelBinarizer()\nxgboost = model_selection.GridSearchCV(estimator=xgb.XGBClassifier(),\n                                       param_grid={'booster': ['gbtree'],\n                                                  'max_depth':[2,3,5,10],\n                                                  'learning_rate':[0.01,0.1,1]},\n                                       scoring=scoring, cv=kfold).fit(df_training_features, \n                                                                      lb.fit_transform(df_training_targets)).best_estimator_\nxgboost.fit(df_training_features, df_training_targets)\n\n","bb780b7a":"modelxg=xgb.XGBClassifier(learning_rate=0.001,n_estimators=1000,max_depth=100,booster='gbtree',n_jobs=-1).fit(df_training_features, df_training_targets)\nypredsxgb=modelxg.predict(df_validation_features)\n\nxgbscore=roc_auc_score(df_validation_targets,ypredsxgb)\nxgbscore","bc6ee494":"import datetime\n# define periods of recession\nrec_spans = []\n\nrec_spans.append([datetime.datetime(2001,3,1), datetime.datetime(2001,10,1)])\nrec_spans.append([datetime.datetime(2007,12,1), datetime.datetime(2009,5,1)])\nrec_spans.append([datetime.datetime(2020,3,1), datetime.datetime(2020,5,1)])","1a0a3592":"prob_predictions = lr1.predict_proba(df_training_features)\nprob_predictions = np.append(prob_predictions, lr1.predict_proba(df_validation_features), axis=0)\nsample_range = pd.date_range(start='10\/1\/1997', end='5\/1\/2020', freq='MS')\n\nplt.figure(figsize=(20,5))\nplt.plot(sample_range.to_series().values, prob_predictions[:,0])\nfor i in range(len(rec_spans)):\n    plt.axvspan(rec_spans[i][0], rec_spans[i][len(rec_spans[i]) - 1], alpha=0.25, color='grey')\nplt.axhline(y=0.5, color='r', ls='dashed', alpha = 0.5)\nplt.title('Recession Prediction Probabalities with Logistic Regression')\nplt.show()","e787da5c":"import shap\n# load your data here, e.g. X and y\n# create and fit your model here\n# load JS visualization code to notebook\nshap.initjs()\n\n# explain the model's predictions using SHAP\n# (same syntax works for LightGBM, CatBoost, scikit-learn and spark models)\nexplainer = shap.LinearExplainer(lr1,df_training_features)\nshap_values = explainer.shap_values(df_training_features.values)\n\n# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\n#shap.force_plot(explainer.expected_value, shap_values[0,:], X.iloc[0,:])\n\nshap.summary_plot(shap_values, df_training_features)","4f13c25e":"As the dataset is too small and recessions are in continuous in time series, we cannot use cross validation functions as folds with only one class will be formed.\n\nThus, loop and iteration of hyperparamets is done in hypertuning Logistic Regression.","d3b7d67d":"The Data Collection and Metadata notebook can be found as Part_1 : https:\/\/www.kaggle.com\/devarshraval\/1-data-collection-for-predicting-recession","d92d3942":"Thus, it is clear XGboost model overfits on train data and LG(l2) model performs better on test data.","240cb18a":"### Data preprocessing\n\n1) Add lags of the variables as additional features\n\n2) Test stationarity of time series\n\n3) Standardize the dataset","88fdda09":"## Limitations and Future Scope\nSome limitations for this model are described and can be regarded as future scope\/tasks for Kaggle and GitHub users:\n\n#### 1. Range of time periods:\n\nThe shorter time period was selected for two main reasons:\n\nTo consider more number of features such as exchange rates(EXUSUK,etc), Consumer goods demand(ACOGNO) and business inventories(BUSINV), data for which is not available in past.\n\nIn addition, it is discussed in the online course that recent data holds more importance to predict future trends.\n\nHowever, user can experiment with time period to obtain a optimal range with minimal noise.\n\n#### 2. Modern potential indicators are not used such as \n(a)cryptocurrency rates, \n\n(b)news sentiment analysis,\n\n(c)Research and innovation indices(especially important going forward owing to public health and climate crisis)\n\n(d)Political and geopolitical stability indices","43d36c4e":"### Hypertuning Logistic Regression ","c5c81415":"## Modelling","ad6025ac":"Thus, it is clear Logistic Regression with L2 norm and XGboost perform best in cross validation.","e0cf48dc":"Thus, the blue line graph past 2014 is the predicted probabilities of our model. One can see it does not predict recession for continuous two months until 2020 when the actual recession started. \nLimitation of this model is the period range of data selected is shorter, as in the online course data was selected for 1960-2018. \nThus, with more sophistication this model can be used to predict market recession in one month prior.","fe17f82f":"#### XGBoost model","0904eb43":"## Results","441bb69f":"# Predicting market recession using macroeconomic data","e25e73fa":"#### Train Test Split\nAssigning 75% of data as training set and 30 % as test set","4a7cfb0d":"In the most intuitive sense, stationarity means that the statistical properties of a process generating a time series do not change over time . It does not mean that the series does not change over time, just that the way it changes does not itself change over time. The mean and variance do not change over time. \n[towardsdatascience.com,Stationarity in time series analysis | by Shay Palachy]","8f06452c":"### Feature Importance\nFeature importance is obtained using the shap library. The results are somewhat intuitive with the Nasdaq lag feature, US exchange rate, federal funds rate and AAA bonnd rate being prominent features that influence the model results."}}