{"cell_type":{"634b774f":"code","fb15cf43":"code","6c9d06ab":"code","a8bf7f55":"code","56aa02c7":"code","e8debbde":"code","a6f5866c":"code","59e1c700":"code","13511bf8":"code","d6d747fd":"code","ef681f02":"code","8f1943f1":"code","0170f963":"code","3f497b0b":"code","c91e4415":"code","772f44fe":"code","ba606fcb":"code","78e5d7ba":"code","8f8f82e2":"code","20b846e6":"code","d72e9006":"markdown","6c8fda45":"markdown","1f0d0a4b":"markdown","117be0b6":"markdown","c0cb6722":"markdown","30fea054":"markdown","3cce3a83":"markdown","abfdcc70":"markdown","68d1cdd4":"markdown","bd8302c4":"markdown","01c0f152":"markdown","75f370bc":"markdown","8700120a":"markdown","1b1a72bf":"markdown","96cc7daa":"markdown","ffd2e1b7":"markdown","fbbbc184":"markdown","75099d79":"markdown","b3a02721":"markdown","9d2e6c72":"markdown","993a336e":"markdown","391ee42e":"markdown"},"source":{"634b774f":"# Let's first do the necessary imports.\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.\n\npd.options.mode.chained_assignment = None  # default='warn'","fb15cf43":"# Load the train and test datasets to create two dataframes\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","6c9d06ab":"train.describe() ","a8bf7f55":"train.shape","56aa02c7":"# absolute numbers\nprint(train[\"Survived\"].value_counts())\n\n# percentages\nprint(train[\"Survived\"].value_counts(normalize = True))","e8debbde":"# Males that survived vs males that passed away\nprint(train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts())\n\n# Females that survived vs Females that passed away\nprint(train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts())\n\n# Normalized male survival\nprint(train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize=True))\n\n# Normalized female survival\nprint(train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize=True))","a6f5866c":"# Create the column Child and assign to 'NaN'\ntrain[\"Child\"] = float('NaN')\n\n# Assign 1 to passengers under 18, 0 to those 18 or older. Print the new column\n\ntrain[\"Child\"][train[\"Age\"] < 18] = 1\ntrain[\"Child\"][train[\"Age\"] >= 18] = 0\n\n# Print normalized Survival Rates for passengers under 18\nprint(train[\"Survived\"][train[\"Child\"] == 1].value_counts(normalize = True))\n\n# Print normalized Survival Rates for passengers 18 or older\nprint(train[\"Survived\"][train[\"Child\"] == 0].value_counts(normalize = True))","59e1c700":"# Print the train data to see the available features\nprint(train.columns.values)","13511bf8":"# Import 'tree' from scikit-learn library\nfrom sklearn import tree","d6d747fd":"train.isnull().sum()","ef681f02":"train[\"Age\"] = train[\"Age\"].fillna(train[\"Age\"].median())","8f1943f1":"# Let us take another look at the values in dataset again.\ntrain.head()","0170f963":"# Checking the frequency of values in Embarked column. \n# Thank you @A.mannan.z for mentioning that this step was not added. \ntrain['Embarked'].value_counts()","3f497b0b":"# Convert the male and female groups to integer form\ntrain[\"Sex\"][train[\"Sex\"] == \"male\"] = 0\ntrain[\"Sex\"][train[\"Sex\"] == \"female\"] = 1\n\n# Impute the Embarked variable\ntrain[\"Embarked\"] = train[\"Embarked\"].fillna(\"S\")\n\n# Convert the Embarked classes to integer form\ntrain[\"Embarked\"][train[\"Embarked\"] == \"S\"] = 0\ntrain[\"Embarked\"][train[\"Embarked\"] == \"C\"] = 1\ntrain[\"Embarked\"][train[\"Embarked\"] == \"Q\"] = 2","c91e4415":"# Let's take a look at train.head() again. \ntrain.head()","772f44fe":"# Create the target and features numpy arrays: target, features_one\ntarget = train[\"Survived\"].values\nfeatures_one = train[[\"Pclass\", \"Sex\", \"Age\", \"Fare\"]].values\n\n# Fit your first decision tree: my_tree_one\nmy_tree_one = tree.DecisionTreeClassifier()\nmy_tree_one = my_tree_one.fit(features_one, target)\n\n# Look at the importance and score of the included features\nprint(my_tree_one.feature_importances_)\nprint(my_tree_one.score(features_one, target))","ba606fcb":"# Looking at test.info() we see that Fare has one null value. \ntest.isnull().sum()","78e5d7ba":"test[\"Fare\"] = test[\"Fare\"].fillna(test[\"Fare\"].median())\ntest[\"Age\"] = test[\"Age\"].fillna(test[\"Age\"].median())\n\ntest[\"Sex\"][test[\"Sex\"] == \"male\"] = 0\ntest[\"Sex\"][test[\"Sex\"] == \"female\"] = 1","8f8f82e2":"# Extract the features from the test set: Pclass, Sex, Age, and Fare.\ntest_features = test[['Pclass', 'Sex', 'Age', 'Fare']].values\n\n# Make your prediction using the test set\nmy_prediction = my_tree_one.predict(test_features)\nprint(my_prediction)\n\n# Create a data frame with two columns: PassengerId & Survived. Survived contains your predictions\nPassengerId =np.array(test[\"PassengerId\"]).astype(int)\nmy_solution = pd.DataFrame(my_prediction, PassengerId, columns = [\"Survived\"])\nprint(my_solution)\n\n# Check that your data frame has 418 entries\nprint(my_solution.shape)\n\n# Write your solution to a csv file with the name my_solution.csv\nmy_solution.to_csv(\"my_solution_one.csv\", index_label = [\"PassengerId\"])","20b846e6":"!ls","d72e9006":"For our feature array we are using Age column and it has 177 missing values. Let's first impute values on this column with the median of all present values.","6c8fda45":"It looks like it makes sense to predict that all females will survive, and all men will die. (A novice prediction!)","1f0d0a4b":"Let's check for missing values. ","117be0b6":"Before we proceed further with Decision Tree we need to do some cleanup. Let's first check for missing values in any of the columns that we will be using in our feature array. ","c0cb6722":"Age has 86 null values and Fare has 1 null value. Let's do imputation similar to what we did for train set. ","30fea054":"**Simple Decision Tree**\n\nWe will need the following to build a decision tree\n* target: A one-dimensional numpy array containing the target\/response from the train data. (Survival in this case)\n* features: A multidimensional numpy array containing the features\/predictors from the train data. (ex. Sex, Age)\n","3cce3a83":"As you can see from the survival proportions, age does certainly seem to play a role.","abfdcc70":"Let us choose our target and feature variables. \n* Target: Survived Column\n* Feature: Passenger, Class, Sex, Age, Fare\n* Build a decision tree tree_one to predict survival using features and target.\n* Look at the importance of features in your tree and compute the score ","68d1cdd4":"**Data Dictionary** (to understand more please visit: https:\/\/www.kaggle.com\/c\/titanic\/data)\n\n      Variable         Definition                          Key\n\n      survival         Survival                            0 = No, 1 = Yes\n      pclass           Ticket class                        1 = 1st, 2 = 2nd, 3 = 3rd\n      sex              Sex    \n      Age              Age in years    \n      sibsp            # of siblings \/ spouses aboard the Titanic    \n      parch            # of parents \/ children aboard the Titanic    \n      ticket           Ticket number    \n      fare             Passenger fare    \n      cabin            Cabin number    \n      embarked         Port of Embarkation                 C = Cherbourg, Q = Queenstown, S=  Southampton\n\n","bd8302c4":"You will see that 549 individuals died which is about 61.61 % and 342 survived which is about 38.38 %.  ","01c0f152":"You can upload my_solution_one.csv to kaggle. ","75f370bc":"Before we dive deeper in, let's see if we can answer some basic questions from data directly. \n\n* **How many people in your training set survived the disaster with the Titanic? **\n* **Number of males that survived vs number of males who did not survive**\n* **Number of femailes that survived vs number of females who did not survive**\n* **Does Age play a role?**","8700120a":"It looks good, so let's proceed with creating target and features array.","1b1a72bf":"Let's extract features from test set and make our prediction based on my_tree_one model.","96cc7daa":"I have written this code based on the following free tutorial on datacamp.  https:\/\/www.datacamp.com\/community\/open-courses\/kaggle-python-tutorial-on-machine-learning I hightly recommend you take this beginner friendly course from datacamp. It further goes onto introduce Random Forest which gives a much better prediction score.","ffd2e1b7":"Sex and Embarked are categorical values but they are in non-numeric format. We need to convert them to numeric format. Emarked also has some missing values and we need to impute them with the majority value that is 'S' before converting to numeric format. ","fbbbc184":"**How many people in your training set survived the disaster with the Titanic? **","75099d79":"**Titanic dataset** is more or less like the \"Hello World!\" of Data Science and Machine Learning. This is a beginner friendly tutorial for anyone who wants to explore Titanic Dataset. ","b3a02721":"**Time to understand the structure of your data**\n\n.describe() summarizes the columns\/features of the DataFrame, including the count of observations, mean, max and so on. \n\n.shape gives the dimensions of the dataframe","9d2e6c72":"Based on our tree, we can see that \"Fare\" contributed more than other features. That is an analysis we are driving out from this decision tree. \n\nLet's now take a look at our test dataset and see if there are any null values. ","993a336e":"**Does age play a role?**\n\nAnother variable that could influence survival is age; since it's probable that children were saved first. You can test this by creating a new column with a categorical variable Child. Child will take the value 1 in cases where age is less than 18, and a value of 0 in cases where age is greater than or equal to 18.","391ee42e":"**Number of males that survived vs number of males who did not survive**\n\n**Number of femailes that survived vs number of females who did not survive**"}}