{"cell_type":{"58d4b0b5":"code","36bdb807":"code","f7f78925":"code","1537adc9":"code","fd434d9b":"code","a719ef5f":"code","332f4cf1":"code","ac586731":"code","e1c548c7":"code","144f9b95":"code","a81e6e57":"code","be0dd0cd":"code","f25672f1":"code","f18e211a":"code","f4441813":"code","d23a5292":"code","e3c888b2":"code","39008076":"code","dac39752":"code","950d9282":"code","e6aacdd9":"code","90726973":"code","a8035f8f":"code","c6452f1f":"code","284943d2":"code","3404a403":"code","5decfdbd":"code","1fa5bdc5":"code","d10243a9":"code","a28d8b14":"code","2cf25078":"code","492f2631":"code","35842690":"code","22a66722":"code","767f13b6":"code","68286719":"code","b0a5f0c9":"code","8b389c36":"code","85b07330":"code","1e20788e":"code","120ecef7":"markdown","139aba95":"markdown","5f5525c7":"markdown","b8ec50b0":"markdown","3b5e6dab":"markdown","5f519f02":"markdown","31b0b7d1":"markdown","582d5b1b":"markdown","035f9d8d":"markdown","91db350f":"markdown","a344f67f":"markdown","400488a5":"markdown","a32e57ea":"markdown","978b5860":"markdown","2eec2280":"markdown","e1ae5ff1":"markdown","23256a4f":"markdown","cdc4fc51":"markdown","e7e0c70e":"markdown","29a6121d":"markdown","dc39ace1":"markdown","076971b3":"markdown","4d017044":"markdown","3952a380":"markdown","714d2c03":"markdown","7d145967":"markdown","c34bc8fe":"markdown","68ddb7d3":"markdown","bf4dcad6":"markdown","38e6b7b6":"markdown"},"source":{"58d4b0b5":"# Asthetics\nimport warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Basic\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport json\nimport os\nimport random\n    from tqdm.autonotebook import tqdm\nimport string\nimport re\nfrom functools import partial\n\n# Visualizations\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nfrom wordcloud import WordCloud, STOPWORDS\n\n# NLP\nimport spacy\nnlp = spacy.load('en_core_web_lg', disable=['parser', 'ner'])\nnlp.max_length = 40000000","36bdb807":"RANDOM_SEED = 42","f7f78925":"def seed_everything(seed = RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    ","1537adc9":"seed_everything","fd434d9b":"train_df = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/train.csv')\nsample_sub = pd.read_csv('..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv')\ntrain_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train'\ntest_files_path = '..\/input\/coleridgeinitiative-show-us-the-data\/test'","a719ef5f":"train_df.head(10)\n","332f4cf1":"def append_text(filename, train_files_path =train_files_path, output = 'text'):\n    json_path = os.path.join(train_files_path, (filename+ '.json'))\n    Heading = []\n    Content = []\n    Combined = []\n    with open (json_path, 'r') as f:\n        json_decode = json.load(f)\n        for data in json_decode:\n            Heading.append(data.get('section_title'))\n            Content.append(data.get('text'))\n            Combined.append(data.get('section_title'))\n            Combined.append(data.get('text'))\n            \n    all_headings = ' '.join(Heading)\n    all_contents = ' '.join(Content)\n    all_data = ' '.join(Combined)\n    \n    if output == 'text':\n        return all_contents\n    elif output == 'head':\n        return all_headings\n    else:\n        return all_data        ","ac586731":"tqdm.pandas()\ntrain_df['text'] = train_df['Id'].progress_apply(append_text)","e1c548c7":"train_df.head(10)","144f9b95":"train_df.dataset_title.unique()","a81e6e57":"tqdm.pandas()\nsample_sub['text'] = sample_sub['Id'].progress_apply(partial(append_text, train_files_path = test_files_path)) ","be0dd0cd":"sample_sub.head()","f25672f1":"for x in train_df.columns:\n    print(f\"Unique Values for {x} are : {len(train_df[x].unique())}\")","f18e211a":"train_df.info()","f4441813":"print(\"Showing below the Dataset Titles which have multiple Dataset Labels associated with them:\\n\")\ndataset_titles_unqiue = train_df[\"dataset_title\"].unique()\nfor title in dataset_titles_unqiue:\n    if len (train_df[train_df[\"dataset_title\"] == title][\"dataset_label\"].unique()) > 1:\n        print(f\"'{title}' : \", list(train_df[train_df[\"dataset_title\"] == title][\"dataset_label\"].unique()), \"\\n\")","d23a5292":"unique_titles = train_df['dataset_title'].unique()\ndup_title = []\ncount = []\ndup_list = []\nfor ut in unique_titles:\n    title_df = train_df[train_df['dataset_title'] == ut]\n    tdf = title_df[['Id', 'dataset_title', 'dataset_label']].drop_duplicates('dataset_label')\n    if tdf.shape[0] > 1:\n        #print(ut)\n        dup_title.append(ut)\n        count.append(tdf.shape[0])\n        dup_list.append(list(tdf['dataset_label']))\n        \ndup_df = pd.DataFrame({'dataset_title':dup_title, 'label_count':count, 'label_list':dup_list})","e3c888b2":"dup_df.set_index('dataset_title')['label_count'].sort_values(ascending=False).plot.barh(figsize=(12,18))\nplt.title(\"No of labels that a dataset have\")\nplt.xlabel('labels_count')\nplt.show()","39008076":"import nltk\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize\nnltk.download('stopwords')\n%matplotlib inline\ndef cleantext(df): \n    \n    train_df['cleaned_text'] = train_df['text'].replace(r'\\'|\\\"|\\,|\\.|\\?|\\+|\\-|\\\/|\\=|\\(|\\)|\\n|\"', '', regex=True)\n    train_df['cleaned_text'] = train_df['cleaned_text'].replace(\"  \", \" \")\n    \n    # convert tweets to lowercase\n    train_df['cleaned_text'] = train_df['cleaned_text'].str.lower()\n    \n     #remove_symbols\n    train_df['cleaned_text']  = train_df['cleaned_text'].replace(r'[^a-zA-Z0-9]', \" \", regex=True)\n    \n    #remove punctuations \n    train_df['cleaned_text'] = train_df['cleaned_text'].replace(r'[[]!\"#$%\\'()\\*+,-.\/:;<=>?^_`{|}]+',\"\", regex = True)\n    \n    #remove_URL(x):\n    train_df['cleaned_text']  = train_df['cleaned_text'].replace(r'https.*$', \"\", regex = True)\n    \n    #remove stopwords and words_to_remove\n    \n    mystopwords = set(stopwords.words('english'))\n    \n    train_df['fully_cleaned_text'] = train_df['cleaned_text'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in mystopwords]))\n    \n\n    return df\n\ntrain_df = cleantext(train_df)","dac39752":"train_df.head()","950d9282":"from collections import defaultdict\n\nwords_in_text_by_dataset = defaultdict(list)\n\nfor _, row in train_df.iterrows():\n    words_in_text_by_dataset[row['dataset_title']].extend(row['fully_cleaned_text'].split())\n\n# Defining our word cloud drawing function\ndef wordcloud_draw(data, color = 'white'):\n    wordcloud = WordCloud(stopwords = STOPWORDS,\n                          background_color = color,\n                          width = 3000,\n                          height = 2000\n                         ).generate(' '.join(data))\n    plt.figure(1, figsize = (12, 8))\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()\n\nfor dataset_title in train_df['dataset_title'].unique():\n    print(\"WordCloud for Publications Text mentioning\", dataset_title, \":\")\n    wordcloud_draw(words_in_text_by_dataset[dataset_title])","e6aacdd9":"from sklearn.model_selection import train_test_split\ndev, val = train_test_split(train_df, test_size=0.1, random_state=42)\nprint(\"Development Shape : \", dev.shape)\nprint(\"Validation Shape : \", val.shape)","90726973":"def clean_text(txt):\n    return re.sub('[^A-Za-z0-9]+', ' ', str(txt).lower())","a8035f8f":"def baseline_model(dev, val):\n    \n    print(\"Running...\")\n    datasets_titles = [x.lower() for x in set(dev['dataset_title'].unique()).union(set(dev['dataset_label'].unique()))]\n\n    print(\"Preparing Validation set...\")\n    val_id = val['Id'].unique()\n    ids = []\n    texts = []\n    gt = []\n\n    for ed in val_id:\n        tdf = val[val['Id'] == ed]\n        gt_label = \"|\".join(tdf['cleaned_label'].tolist())\n        text = tdf['text'].tolist()[0]\n        ids.append(ed)\n        gt.append(gt_label)\n        texts.append(text)\n\n    pval = pd.DataFrame({'Id':ids, 'text':texts, 'ground_truth':gt})\n\n    #print(pval.shape)\n\n    print(\"Generating predictions...\")\n    labels = []\n    for index in pval['Id']:\n        publication_text = pval[pval['Id'] == index].text.str.cat(sep='\\n').lower()\n        #print(publication_text)\n        label = []\n        for dataset_title in datasets_titles:\n            if dataset_title in publication_text:\n                label.append(clean_text(dataset_title))\n        labels.append('|'.join(label))\n\n    pval['prediction'] = labels\n    print(\"Done!\")\n    \n    return pval","c6452f1f":"output = baseline_model(dev, val)\noutput.head()","284943d2":"def jaccard(str1,str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","3404a403":"jaccard(\"This is a so nice Notebook\", \"This is so nice Codebook\")","5decfdbd":"jaccard(\"This is a so nice Notebook\", \"This is a so nice Notebook\")","1fa5bdc5":"def get_precision_recall(tp, fp, fn):\n    precision = tp \/ (tp+fp)\n    recall = tp \/ (tp+fn)\n    return precision,recall\n\ndef fbeta_score(precision, recall, beta):\n    fbeta = (1 + (beta*beta)) * ((precision *recall) \/ ( (beta*beta*precision) +recall))\n    return fbeta\n    ","d10243a9":"fbeta_score(0.7, 0.7, 0.5)","a28d8b14":"fbeta_score(0.7, 0.5, 0.5)\n","2cf25078":"fbeta_score(0.5, 0.7, 0.5)\n","492f2631":"def coleridge_iniative_jaccard_metric(ground_truth, prediction, verbose = True):\n    gts = ground_truth.split('|')\n    pds = sorted(prediction.split('|'))\n    if verbose:\n        print(\"Ground Truth : \" , gts)\n        print(\"Prediction : \" , pds)\n    \n    js_scores = []\n    cf_matrix = []\n    \n    #Counting True Positives (TP) and False Positives (FP)\"\"\"\n    \n    for pd in pds:\n        score = -1\n        for gt in gts:\n            js = jaccard(pd, gt)\n            if js > score:\n                score = js\n        if score >= 0.5:\n            js_scores.append(score)\n            cf_matrix.append(\"TP\")\n        else:\n            js_scores.append(score)\n            cf_matrix.append(\"FP\")\n            \n    #counting False negatives\"\"\"\n    \n    for gt in gts:\n        score = -1\n        for pd in pds:\n            js = jaccard(gt, pd)\n            if js > score:\n                score = js\n        if score == 0:\n            js_scores.append(score)\n            cf_matrix.append(\"FN\")\n            \n    return js_scores, \" \".join(cf_matrix)","35842690":"coleridge_iniative_jaccard_metric(\"this data|that dataset|xyz\", \"which data|no dataset|that dataset\")","22a66722":"output['evaluation'] = output.apply(lambda x: coleridge_iniative_jaccard_metric(x['ground_truth'], x['prediction'], verbose=False), axis=1)\noutput['js_scores'] = output['evaluation'].apply(lambda x : x[0])\noutput['pred_type'] = output['evaluation'].apply(lambda x : x[1])","767f13b6":"output.head()\n","68286719":"def get_count_tp_fp_fn(prediction, verbose=True):\n    preds = prediction.split(\" \")\n    if verbose:\n        print(preds)\n    tpc = 0\n    fpc = 0\n    fnc = 0\n    for pred in preds:\n        if pred == \"TP\":\n            tpc = tpc + 1\n        elif pred == \"FP\":\n            fpc = fpc + 1\n        elif pred == \"FN\":\n            fnc == fnc + 1\n    return [tpc, fpc, fnc]\n\ndef make_col_tp_fp_fn(df, col):\n    df['TP'] = df[col].apply(lambda x : x[0])\n    df['FP'] = df[col].apply(lambda x : x[1])\n    df['FN'] = df[col].apply(lambda x : x[2])\n    return df","b0a5f0c9":"output['tp_fp_fn'] = output['pred_type'].apply(lambda x : get_count_tp_fp_fn(x, verbose=False))\noutput = make_col_tp_fp_fn(output, 'tp_fp_fn')\noutput.head()","8b389c36":"tp = sum(output['TP'])\nfp = sum(output['FP'])\nfn = sum(output['FN'])\n\nprint(\"True Positives (TP) : \", tp)\nprint(\"False Positives (FP) : \", fp)\nprint(\"False Negatives (FN) : \", fn)","85b07330":"precision, recall = get_precision_recall(tp, fp, fn)\nprint(\"Precision : \", precision)\nprint(\"Recall : \", recall)","1e20788e":"fbeta = fbeta_score(precision, recall, 0.5)\nprint(\"FBeta Score : \", fbeta)","120ecef7":"**Context**\n\nData, evidence, and science are critical if government is to address so many imminent threats: pandemics, climate change and coastal inundation, Alzheimer\u2019s disease, child hunger, biodiversity, among others. Yet much of the information about data needed to inform evidence and science is locked inside publications.\n\nThe Coleridge Initiative is launching a new competition, Show Us the Data, to spur the use of artificial intelligence (AI) to unlock this information. Westat is among the sponsors of this exciting competition. Participants are challenged to use natural language processing (NLP) to identify critical datasets used in scientific publications. This work will help researchers quickly find the data they need to do their work. The effort will help improve access and use of critical datasets.\n\n**Challenge** - Much of the information about data necessary to inform evidence and science is locked inside publications.\n\n**Approach** - lets leverage natural language processing find the hidden-in-plain-sight data citations.\n\nBasically In this competition, we will use natural language processing (NLP) to automate the discovery of how scientific data are referenced in publications.","139aba95":"# Lets append Publication Text to Test Sample submission File as well","5f5525c7":"# Function to count TP, FP, FN","b8ec50b0":"**\"cleaned_label\" is our target variable.**\n\n","3b5e6dab":"# \ud83d\udce2 Jaccard similarity based FBeta metric \ud83d\udce2","5f519f02":"**When precision less than recall**","31b0b7d1":"**we can test above function like below:**","582d5b1b":"*The Jaccard similarity index measures the similarity between two sets of data. \n\nIt can range from 0 to 1. The higher the number, the more similar the two sets of data. The Jaccard similarity index is calculated as:*\n\n# Jaccard Similarity = (number of observations in both sets) \/ (number in either set)","035f9d8d":"# Lets calculate FBeta Score for Our Case-","91db350f":"# BaseLine Model","a344f67f":"Notice that F-Score is not FBeta score. Now, Let's look into FBeta\n\n**FBeta Score**: It a F-Score that uses a positive real factor \u03b2, where \u03b2 is chosen such that recall is considered \u03b2 times as important as precision. The formula for FBeta is give as\n\n![image.png](attachment:8a90af4d-0e23-499c-9230-95bc6208d3ed.png)","400488a5":"# **generating Word Cloud of Publication Text based on Dataset mentioned.**\n# \n**Note**: could only generate for few dataset_titles due to memory constraints.","a32e57ea":"# Lets append Publication Text from JSON File to this Table","978b5860":"**visualize No. of labels that a Dataset has**","2eec2280":"# Import Packages","e1ae5ff1":"**applying it on our dataset.**","23256a4f":"# beta = 0.5 gives more importance to Precision so we can infer that competition's evaluation is baised towards Precision.","cdc4fc51":"# splitting this datafeame into development set and validation set","e7e0c70e":"**when both strings are exact same.**","29a6121d":"**Dataset Titles which have multiple Dataset Labels associated.** ![image.png](attachment:06aeac49-40ac-448f-98e2-491caa6a8d26.png)","dc39ace1":"# \ud83d\udcda\ud83d\udcdaColeridge Initiative - Show US the Data \ud83d\udcda\ud83d\udcda","076971b3":"# Lets apply it on our DataSet","4d017044":"**When precision greater than recall**","3952a380":"# Data Wrangling","714d2c03":"**Let's check it on some examples (We are passing beta = 0.5 because this competition uses beta = 0.5)**","7d145967":"**Lets calculate Beta Score**","c34bc8fe":"![image.png](attachment:13351f14-973d-4366-b2b6-81ada7c8b347.png)","68ddb7d3":"**when Precision eauals to recall**","bf4dcad6":"**evaluating the function**","38e6b7b6":"# Text Cleaning and Preprocessing"}}