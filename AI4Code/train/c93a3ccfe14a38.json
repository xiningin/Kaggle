{"cell_type":{"d64496d9":"code","ca2594c0":"code","6ef30f9a":"code","cfa85e1f":"code","c1f60c12":"code","1660b01e":"code","6c19e7e3":"code","54d21f74":"code","0bba76da":"code","2de849a8":"code","5aa685d2":"code","675d5991":"code","508fe00e":"code","3817638f":"code","589d8928":"code","14939dc0":"code","e1f212ed":"code","a84549af":"code","922ba5cf":"code","9a18fc3a":"markdown","acb6376f":"markdown","edb43a8d":"markdown","ae06287e":"markdown","5bd0f8ab":"markdown","18b5c60e":"markdown","5d46447a":"markdown","ec39051b":"markdown","d3c1c066":"markdown","aaff2928":"markdown","8dd816a6":"markdown","90fc0a9d":"markdown"},"source":{"d64496d9":"# Importing dependencies\n\n# Surprise is a Python library for collaborative filtering recommendation algorithms\nfrom surprise import SVD\nfrom surprise import NMF\nfrom surprise.model_selection import cross_validate\nfrom surprise import Reader, Dataset\n\n# Sci-kit Learn is a popular Python library for machine learning and data science models\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel \nfrom sklearn.model_selection import train_test_split\n\nimport pandas as pd\nimport numpy as np","ca2594c0":"# Data pulled into a Pandas DataFrame\n\nwine_df = pd.read_csv('\/kaggle\/input\/wine-reviews\/winemag-data-130k-v2.csv')\nwine_df.head()","6ef30f9a":"# Investigate our numeric columns\n\nwine_df.describe()","cfa85e1f":"# Check the non-null count and data types for each column\n\nwine_df.info()","c1f60c12":"# Select the categories necessary for CF and assign them to categorical representations\n\nwine_cf_df = wine_df.loc[:, ['points', 'taster_name', 'title']]\nwine_cf_df.loc[:, 'tasterId'] = wine_cf_df.loc[:, 'taster_name'].astype('category').cat.codes\nwine_cf_df.loc[:, 'wineId'] = wine_cf_df.loc[:, 'title'].astype('category').cat.codes\n\nwine_cf_df.head()","1660b01e":"# We know that the minimum and maximum of the rating scale 'points' is 80 and 100\nreader = Reader(rating_scale=(80, 100))\n\n# We load the data into the Surprise dataset with the reader\ndata = Dataset.load_from_df(wine_cf_df[['tasterId', 'wineId', 'points']], reader)","6c19e7e3":"# Set the algorithm to Surprise's SVD and cross validate on our data\nalgo = SVD()\ncross_validate(algo, data, measures=['RMSE','MAE'], cv=5, verbose=True)","54d21f74":"# Building a data trainset and testset for building predictions of what new wines users will like\n\ntrain_set = data.build_full_trainset()\ntest_set = train_set.build_anti_testset()\npredictions = algo.fit(train_set).test(test_set)","0bba76da":"# We can now see the top 10 wines for a user, and make other data analysis possible with the recommendations\n\npredictions_df = pd.DataFrame(predictions)\n\n# Get the top 10 recommended wines for Taster #9 (Kerin O'Keefe)\nuid = 9\n\nuid_preds = predictions_df.loc[predictions_df['uid'] == 9, :].sort_values(['est'], ascending=False).iloc[:10]\nuid_preds['wineLabel'] = uid_preds.loc[:, 'iid'].apply(lambda i: wine_cf_df.loc[wine_cf_df['wineId'] == i, 'title'].values[0])\nuid_preds.loc[:, ['wineLabel', 'est']].reset_index(drop=True)","2de849a8":"# Check how Taster #9 (Kerin O'Keefe) will like Wine # 89368 (Quinta dos Avidagos 2011)\ntasterId_ = 9\nwineId_ = 103657\n\npred = predictions_df.loc[(predictions_df['uid'] == tasterId_) & (predictions_df['iid'] == wineId_), 'est'].values[0]\n\nprint(f\"{wine_cf_df.loc[wine_cf_df['tasterId'] == int(tasterId_),'taster_name'].values[0]} is predicted to rate {wine_cf_df.loc[wine_cf_df['wineId'] == int(wineId_), 'title'].values[0]} with a score of {pred:.2f}\"\n     f\" which is {pred - predictions_df.loc[0, 'r_ui']:.2f} off the global mean.\")","5aa685d2":"# Importing dependencies from sci-kit learn's features and metrics libraries\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel \nfrom sklearn.model_selection import train_test_split","675d5991":"# We once again need wineId codes, so we use the same categorization of wine titles\n\nwine_df.loc[:, 'wineId'] = wine_df.loc[:, 'title'].astype('category').cat.codes\n\nwine_df.loc[:, ['description', 'wineId', 'title']].head()","508fe00e":"# Split the wine dataframe into a train and test split. Although we are not evaluating the data, the dataframe is much too large for Kaggle's \n    # kernels and so I'm only training on 5% of the data. A full implementation would require more significant memory or distributed training.\n    \ntrain_wine, test_wine = train_test_split(wine_df, train_size=0.05)\n\ntrain_wine.reset_index(drop=True, inplace=True)\n\nprint(f\"Training on {len(train_wine)} samples.\")","3817638f":"# We're running the content-based recommender on TFIDF data from the wine descriptions. \n\ntf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df = 0, stop_words='english')\n\ntfidf_matrix = tf.fit_transform(train_wine['description'])\n\nprint(f\"The term-frequency inverse document frequency matrix is {tfidf_matrix.shape[0]} by {tfidf_matrix.shape[1]}\")","589d8928":"# First finding the cosine similarities for the tfidf matrix\ncosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)\n\n# Next, appending the results to a dictionary of the similar items to each wine\nresults = {}\nfor idx, row in train_wine.iterrows():\n    similar_indices = cosine_similarities[idx].argsort()[:100:-1]\n    similar_items = [(cosine_similarities[idx][i], train_wine['wineId'][i]) for i in similar_indices]\n    results[row['wineId']] = similar_items[1:]","14939dc0":"def item(id):\n    return train_wine.loc[train_wine['wineId'] == id]['title'].tolist()[0].split(' - ')[0]\n\ndef recommend(item_id, num):\n    print('Recommending ' + str(num) + ' products similar to ' + item(item_id) + ' ...')\n    print('-----')\n    recs = results[item_id][:num]\n    for rec in recs:\n        print('Recommended: ' + item(rec[1]) + '(score: ' + f\"{rec[0]:.2f}\" + ')')","e1f212ed":"# itemId (wineId) is grabbed from the trainset of wines\n\nitemId_ = train_wine.loc[:, 'wineId'].values[0]\nitemName_ = train_wine.loc[train_wine['wineId'] == itemId_, 'title'].values[0]\n\nprint(f\"Using itemId {itemId_} which is {itemName_} \\n\")\n\n# The recommend function is then run to find and return the top num matches (5 in this case)\n\nrecommend(item_id=itemId_, num=5)","a84549af":"results[itemId_][0][1]","922ba5cf":"# We can then compare the descriptions of the two wines to see how they match\n\ndescription_original = train_wine.loc[train_wine['title'] == itemName_, 'description'].values[0]\n\ndescription_matched = train_wine.loc[train_wine['wineId'] == results[itemId_][0][1], 'description'].values[0]\n\nprint(f\"First wine description: \\n{description_original} \\n\\nMatched wine description: \\n{description_matched}\")","9a18fc3a":"There are a number of different methods for content-based recommendations, but I'll be using NLP TF-IDF features for this recommendation. Otherwise, features could include categorical columns on the province \/ country of the wine, some variable key words from the description, its price, and other data available. But since the description is readily available, that will be my use case.","acb6376f":"## Collaborative Filter Recommendations\n\nI'll start with CF recommendations. \n\nThe required format for CF recommendations is ['userId', 'itemId', 'rating']. \n\nIn our case, that will be ['tasterId', 'wineId', 'points']. The collaborative filtering will use the Surprise library and import its Singular Value Decompisition model, made famous by Simon Funk during the Netflix competition. Essentially, the model will follow stochaistic gradient descent to minimalize the squared error of the predictions. ","edb43a8d":"Some simple functions below allow us to find the item by its id, and then grab the top num recommendations for a given item. ","ae06287e":"The linear_kernel from sklearn allows us to compute the linear kernel (the linear seperation of data) for the TFIDF matrices. We then find the similar indices for each item (wine), turn the similar index into a list of similar items (wines) and then append the similar items (other than the first which is the item itself) into the results. ","5bd0f8ab":"After our model is fitted to the data, we can use it to predict how a user will like a certain item.","18b5c60e":"SVD works decent with the data and returns a RMSE on average about 2.79 off. Perhaps there could be better algorithms in the Surprise library to use (such as KNN, Basic, etc.) but for our purposes we'll stick to SVD. \n\nNext I'll set up a train_set of the data using the Surprise Data class build_full_trainset method. Note that we can't simply fit the data, but first must prepare it for fitting with this method. Afterwords, we'll do the same to set up a test set using the build_anti_testset method from the train_set object. The anti_testset pulls all items and users where the rating is not known (they haven't tried it yet). This is important for exploration and recommending new items to the user.","5d46447a":"## Content-Based Recommendations\n\nThe other method of recommendation engines that we'll explore is content-based recommendations. Content-Based algorithms perform on a matrix of items and the items features to find similarities between data. In our example below, I use cosine similarities between term frequency inverse document frequency vectors for the description of the different wines. This allows us to find similar wines based solely on their term frequency (relative to document frequency). ","ec39051b":"Next, I initiate the Surprise reader and dataset classes.\n* The **reader** is used to parse the dataset \n* The **Dataset** is used to hold data as parsed by a Surprise reader\n\nI'll then initiate the SVD algorithm and run the preset Surprise cross validation method to check how the model handles the data","d3c1c066":"# Recommending Wine to Wine Reviewers\n\nThis notebook aims to use both Collaborative Filtering and Content-Based Recommendation Engines to recommend wines to wine reviewers based on the data set \"wine-reviews.\" \n\nThe two methods of recommendation machine learning that I'll use:\n* **Collaborative Filter (CF) Recommendations** - collaborative filter recommendations use the 'wisdom of the masses' to recommend items for users based on similarities between user ratings of the items. It uses the logic of Person A likes Item 1, 2 and 3 - Person B likes Item 1, and 2 - therefore Person B will likely like Item 3 as well. \n* **Content-Based Recommendations** - content based recommendations rely on matrix reductions to identify cosine similarity between feature vectors of items. It uses the logic of identifying similarity between items to understand what a user would like given their current preferences. \n\nThis notebook will investigate both methods of recommendation engines to produce results for wine reviewers.","aaff2928":"And now we can see the top num recommendations for any given item. ","8dd816a6":"Both of these recommendation models have practical uses for building a more hybrid and comprehensive recommendation engine. For instance, content-based models allow for recommendations in absence of past user decisions - useful for a first-time reviewer. As the user's actions and decision history is compiled over time, the recommendation engine could begin to use more collaborative filtering to recommend wines liked by similarly tasted reviewers.\n\nThe recommendation engine:\n* **First-time reviewers** - mostly using content-based models to recommend wines based on their past preferences. Similar to Netflix's select some movies you've previously enjoyed. \n* **Repeat reviewers** - as data is compiled on the user's tastes, the engine begins to shift from content-based to collaborative filtering. \n* **New wines** - as new items are added, without recommendations provided yet by reviewers, they can be recommended based on their description from the winery from the content-based model. Therefore it is important for exploration purposes that the engine never completely abandon content-based recommendations.","90fc0a9d":"## Reading and Analyzing the Data\n\nFirst we will read the wine review data and conduct some simple data exploration on it."}}