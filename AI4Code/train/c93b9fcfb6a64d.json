{"cell_type":{"e263e2f8":"code","89fb03b5":"code","b88d4f07":"code","9a028558":"code","e8e69a7a":"code","545348d3":"code","7d1007fb":"code","90f0dbc9":"code","d6a75f9a":"code","27aa489b":"code","f159f4b5":"code","5c9e9792":"code","9b8dbf55":"code","06abb84f":"code","563578d2":"code","984d83e2":"code","6ce5865e":"code","c4f53944":"markdown","8e869e47":"markdown","30da952d":"markdown"},"source":{"e263e2f8":"import pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OrdinalEncoder\n\nnp.random.seed(42)\nfrom keras.utils.np_utils import to_categorical # used for converting labels to one-hot-encoding\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nfrom sklearn.preprocessing import LabelEncoder\n# import autokeras as ak\n\nimport keras\nfrom keras.utils.np_utils import to_categorical # used for converting labels to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom sklearn.model_selection import train_test_split\nfrom scipy import stats\nfrom sklearn.preprocessing import LabelEncoder","89fb03b5":"folder_benign_train = '..\/input\/skin-cancer-malignant-vs-benign\/train\/benign'\nfolder_malignant_train = '..\/input\/skin-cancer-malignant-vs-benign\/train\/malignant'\n\nfolder_benign_test = '..\/input\/skin-cancer-malignant-vs-benign\/test\/benign'\nfolder_malignant_test = '..\/input\/skin-cancer-malignant-vs-benign\/test\/malignant'\n\nread = lambda imname: np.asarray(Image.open(imname).convert(\"RGB\"))\n\n# Load in training pictures \nims_benign = [read(os.path.join(folder_benign_train, filename)) for filename in os.listdir(folder_benign_train)]\nX_benign = np.array(ims_benign, dtype='uint8')\nims_malignant = [read(os.path.join(folder_malignant_train, filename)) for filename in os.listdir(folder_malignant_train)]\nX_malignant = np.array(ims_malignant, dtype='uint8')\n\n# Load in testing pictures\nims_benign = [read(os.path.join(folder_benign_test, filename)) for filename in os.listdir(folder_benign_test)]\nX_benign_test = np.array(ims_benign, dtype='uint8')\nims_malignant = [read(os.path.join(folder_malignant_test, filename)) for filename in os.listdir(folder_malignant_test)]\nX_malignant_test = np.array(ims_malignant, dtype='uint8')\n\n# Create labels\nY_benign = np.zeros(X_benign.shape[0])\nY_malignant = np.ones(X_malignant.shape[0])\n\nY_benign_test = np.zeros(X_benign_test.shape[0])\nY_malignant_test = np.ones(X_malignant_test.shape[0])\n\n\n# Merge data \nX_train = np.concatenate((X_benign, X_malignant), axis = 0)\nY_train = np.concatenate((Y_benign, Y_malignant), axis = 0)\n\nX_test = np.concatenate((X_benign_test, X_malignant_test), axis = 0)\nY_test = np.concatenate((Y_benign_test, Y_malignant_test), axis = 0)\n\n# Shuffle data\ns = np.arange(X_train.shape[0])\nnp.random.shuffle(s)\nX_train = X_train[s]\nY_train = Y_train[s]\n\ns = np.arange(X_test.shape[0])\nnp.random.shuffle(s)\nX_test = X_test[s]\nY_test = Y_test[s]\n","b88d4f07":"fig=plt.figure(figsize=(20, 50))\ncolumns = 5\nrows = 10\n\nfor i in range(1, columns*rows +1):\n    ax = fig.add_subplot(rows, columns, i)\n    if Y_train[i] == 0:\n        ax.title.set_text('Benign')\n    else:\n        ax.title.set_text('Malignant')\n    plt.imshow(X_train[i], interpolation='nearest')\nplt.show()","9a028558":"print('Shapes: \\n')\nprint('X_train: ',X_train.shape,' \\n')\nprint('Y_train: ',Y_train.shape,' \\n')\nprint('X_test : ',X_test.shape,' \\n')\nprint('Y_test : ',Y_test.shape,' \\n')\nprint('Y_train malginant Distrib: ',(Y_train.sum()\/len(Y_train)).round(4), '\\n')\nprint('Y_test malginant Distrib: ',(Y_test.sum()\/len(Y_test)).round(4), '\\n')","e8e69a7a":"#Subsampling to prior CNN model evaluation\nSample1 = np.random.choice(range(0,len(X_train)), size=1000, replace=False)\nSample1\nSample2 = np.random.choice(range(0,len(X_test)), size=600, replace=False)\nSample2","545348d3":"X_train_sample = X_train[Sample1]\nY_train_sample = Y_train[Sample1]\nX_test_sample = X_test[Sample2]\nY_test_sample = Y_test[Sample2]","7d1007fb":"X_train_sample.shape","90f0dbc9":"Y_train_sample.shape","d6a75f9a":"# Prior model for subsample\nnum_classes = 1\nSIZE = 224            # Image Sizes\nkern_size = 3\n\n# padding=\"same\", kernel_initializer=\"glorot_uniform\"\n\nmodel = Sequential()\nmodel.add(Conv2D(filters=20, kernel_size=(kern_size, kern_size), activation=\"relu\", input_shape=(SIZE, SIZE, 3)))\nmodel.add(MaxPool2D(pool_size=(2, 2)))  \nmodel.add(Dropout(0.3))\n\nmodel.add(Conv2D(filters=20, kernel_size=(kern_size, kern_size), activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2)))  \nmodel.add(Dropout(0.3))\n\n\nmodel.add(Conv2D(filters=20, kernel_size=(kern_size, kern_size),activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2, 2)))  \nmodel.add(Dropout(0.2))\n\n\nmodel.add(Flatten())\n\nmodel.add(Dense(128,activation='relu'))\nmodel.add(Dense(128,activation='relu'))\n#model.add(Dense(num_classes, activation='softmax'))      # For multiclassification\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()\n\n# model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])    # For multiclassification\n# model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['acc'])     # For multiclassification\n\nmodel.compile(loss='binary_crossentropy', optimizer='Adam', metrics=[\"BinaryAccuracy\"])\n","27aa489b":"# Model evaluation in subsample\nbatch_size = 20 \nepochs = 50\n\nhistory = model.fit(\n    X_train_sample, Y_train_sample,\n    epochs=epochs,\n    batch_size = batch_size,\n    validation_data=(X_test_sample, Y_test_sample),\n    verbose=2)","f159f4b5":"plt.plot(history.history['binary_accuracy'])\nplt.plot(history.history['val_binary_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","5c9e9792":"# Final\nnum_classes = 1\nSIZE = 224            # Image Sizes\nkern_size = 3\n\n# padding=\"same\", kernel_initializer=\"glorot_uniform\"\n\nmodel2 = Sequential()\nmodel2.add(Conv2D(filters=20, kernel_size=(kern_size, kern_size), activation=\"relu\", input_shape=(SIZE, SIZE, 3)))\nmodel2.add(MaxPool2D(pool_size=(2, 2)))  \nmodel2.add(Dropout(0.3))\n\nmodel2.add(Conv2D(filters=20, kernel_size=(kern_size, kern_size), activation='relu'))\nmodel2.add(MaxPool2D(pool_size=(2, 2)))  \nmodel2.add(Dropout(0.3))\n\n\nmodel2.add(Conv2D(filters=20, kernel_size=(kern_size, kern_size),activation='relu'))\nmodel2.add(MaxPool2D(pool_size=(2, 2)))  \nmodel2.add(Dropout(0.2))\n\nmodel2.add(Flatten())\n\nmodel2.add(Dense(128,activation='relu'))\nmodel2.add(Dense(128,activation='relu'))\n#model.add(Dense(num_classes, activation='softmax'))      # For multiclassification\nmodel2.add(Dense(1, activation='sigmoid'))\nmodel2.summary()\n\n# model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])    # For multiclassification\n# model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['acc'])     # For multiclassification\n\nmodel2.compile(loss='binary_crossentropy', optimizer='Adam', metrics=[\"BinaryAccuracy\"])\n","9b8dbf55":"# Fine tunning\n# Running model in all data\nbatch_size = 20 \nepochs = 19\n\nhistory2 = model2.fit(\n    X_train, Y_train,\n    epochs=epochs,\n    batch_size = batch_size,\n    validation_data=(X_test, Y_test),\n    verbose=2)","06abb84f":"plt.plot(history2.history['binary_accuracy'])\nplt.plot(history2.history['val_binary_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history2.history['loss'])\nplt.plot(history2.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","563578d2":"# Final\nnum_classes = 1\nSIZE = 224            # Image Sizes\nkern_size = 3\n\n# padding=\"same\", kernel_initializer=\"glorot_uniform\"\n\nmodel3 = Sequential()\nmodel3.add(Conv2D(filters=50, kernel_size=(kern_size, kern_size), activation=\"relu\", input_shape=(SIZE, SIZE, 3)))\nmodel3.add(MaxPool2D(pool_size=(2, 2)))  \nmodel3.add(Dropout(0.3))\n\nmodel3.add(Conv2D(filters=20, kernel_size=(kern_size, kern_size), activation='relu'))\nmodel3.add(MaxPool2D(pool_size=(2, 2)))  \nmodel3.add(Dropout(0.3))\n\nmodel3.add(Conv2D(filters=20, kernel_size=(kern_size, kern_size),activation='relu'))\nmodel3.add(MaxPool2D(pool_size=(2, 2)))  \nmodel3.add(Dropout(0.3))\n\nmodel3.add(Flatten())\n\nmodel3.add(Dense(100,activation='relu'))\nmodel3.add(Dropout(0.3))\nmodel3.add(Dense(50,activation='relu'))\nmodel3.add(Dropout(0.3))\n\n#model.add(Dense(num_classes, activation='softmax'))      # For multiclassification\nmodel3.add(Dense(1, activation='sigmoid'))\nmodel3.summary()\n\n# model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['acc'])    # For multiclassification\n# model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['acc'])     # For multiclassification\n\nmodel3.compile(loss='binary_crossentropy', optimizer='Adam', metrics=[\"BinaryAccuracy\"])\n","984d83e2":"# Fine tunning\n# Running model in all data\nbatch_size = 50 \nepochs = 20\n\nhistory3 = model3.fit(\n    X_train, Y_train,\n    epochs=epochs,\n    batch_size = batch_size,\n    validation_data=(X_test, Y_test),\n    verbose=2)","6ce5865e":"plt.plot(history3.history['binary_accuracy'])\nplt.plot(history3.history['val_binary_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history3.history['loss'])\nplt.plot(history3.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","c4f53944":"# Model 3","8e869e47":"# Skin Cancer Image Classification (benign vs malign) - Deep Learning CNN","30da952d":"# Model 2"}}