{"cell_type":{"5ba730bf":"code","752a2184":"code","544da4cb":"code","ed7f767d":"code","e3b1f18c":"code","3e114ad8":"code","21af62db":"code","ceafb457":"code","8fca9df3":"code","70236ca4":"code","e33373a0":"code","a76c63ea":"code","e2386f49":"code","2781d9c5":"code","1300443a":"code","127a2d17":"code","8cf34100":"code","66a0de57":"code","6475a547":"code","b019251f":"code","29ba98ea":"code","fbd3ad04":"code","47a83dea":"code","0ca274dc":"code","4b33466c":"markdown","14007286":"markdown","289748ec":"markdown","65762080":"markdown","3100b474":"markdown","9295c3f2":"markdown","01c7affb":"markdown","7621943d":"markdown"},"source":{"5ba730bf":"from collections import Counter\nimport string\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom torch.utils.data import Dataset, DataLoader\n\nimport pandas as pd\nimport numpy as np\nimport re\nimport spacy\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split","752a2184":"# Load the data\nreviews = pd.read_csv(\"\/kaggle\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv\")\nprint(reviews.shape)\nreviews.head()","544da4cb":"# Fill empty cells, and combine the 'Title' and 'Review Text' into a new column 'review'\nreviews['Title'] = reviews['Title'].fillna('')\nreviews['Review Text'] = reviews['Review Text'].fillna('')\nreviews['review'] = reviews['Title'] + ' ' + reviews['Review Text']\n\n# Keep only relevant columns and calculate sentence lengths\nreviews = reviews[['review', 'Rating']]\nreviews.columns = ['review', 'rating']\nreviews['review_length'] = reviews['review'].apply(lambda x: len(x.split()))\nreviews.head()","ed7f767d":"print(\"Mean sentence length: \", np.mean(reviews['review_length']))\nprint(\"Ratings distribution: \", Counter(reviews['rating']))","e3b1f18c":"# Change ratings to 0-numbering\nzero_numbering = {1:0, 2:1, 3:2, 4:3, 5:4}\nreviews['rating'] = reviews['rating'].apply(lambda x: zero_numbering[x])","3e114ad8":"# Tokenization\n# ( Tokenization is the task of splitting a text into meaningful segments, called tokens. See https:\/\/spacy.io\/usage\/linguistic-features#tokenization )\ntok = spacy.load('en')\ndef tokenize (text):\n    text = re.sub(r\"[^\\x00-\\x7F]+\", \" \", text)\n    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]') # Remove punctuation and numbers\n    nopunct = regex.sub(\" \", text.lower())\n    return [token.text for token in tok.tokenizer(nopunct)]","21af62db":"# Count number of occurences of each word\ncounts = Counter()\nfor index, row in reviews.iterrows():\n    counts.update(tokenize(row['review']))\n    \n# Delete infrequent words\nprint(\"num_words before:\",len(counts.keys()))\nfor word in list(counts):\n    if counts[word] < 2:\n        del counts[word]\nprint(\"num_words after:\",len(counts.keys()))","ceafb457":"# Create vocabulary\nvocab2index = {\"\":0, \"UNK\":1}\nwords = [\"\", \"UNK\"]\nfor word in counts:\n    vocab2index[word] = len(words)\n    words.append(word)\n    \ndef encode_sentence(text, vocab2index, N=70):\n    tokenized = tokenize(text)\n    encoded = np.zeros(N, dtype=int)\n    enc1 = np.array([vocab2index.get(word, vocab2index[\"UNK\"]) for word in tokenized])\n    length = min(N, len(enc1))\n    encoded[:length] = enc1[:length]\n    return encoded, length\n\nreviews['encoded'] = reviews['review'].apply(lambda x: np.array(encode_sentence(x,vocab2index )))\nreviews.head()","8fca9df3":"class ReviewsDataset(Dataset):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n        \n    def __len__(self):\n        return len(self.y)\n    \n    def __getitem__(self, idx):\n        return torch.from_numpy(self.x[idx][0].astype(np.int32)), self.y[idx], self.x[idx][1]","70236ca4":"x = list(reviews['encoded'])\ny = list(reviews['rating'])\nx_train, x_valid, y_train, y_valid = train_test_split(x, y, test_size=0.2)\n\ntrain_ds = ReviewsDataset(x_train, y_train)\nvalid_ds = ReviewsDataset(x_valid, y_valid)","e33373a0":"def train_model(model, epochs=10, lr=0.001):\n    parameters = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.Adam(parameters, lr=lr)\n    for i in range(epochs):\n        model.train()\n        sum_loss = 0.0\n        total = 0\n        for x, y, l in train_dl:\n            x = x.long()\n            y = y.long()\n            y_pred = model(x, l)\n            optimizer.zero_grad()\n            loss = F.cross_entropy(y_pred, y)\n            loss.backward()\n            optimizer.step()\n            sum_loss += loss.item()*y.shape[0]\n            total += y.shape[0]\n        val_loss, val_acc, val_rmse = validation_metrics(model, val_dl)\n        if i % 5 == 1:\n            print(\"train loss %.3f, val loss %.3f, val accuracy %.3f, and val rmse %.3f\" % (sum_loss\/total, val_loss, val_acc, val_rmse))\n\ndef validation_metrics (model, valid_dl):\n    model.eval()\n    correct = 0\n    total = 0\n    sum_loss = 0.0\n    sum_rmse = 0.0\n    for x, y, l in valid_dl:\n        x = x.long()\n        y = y.long()\n        y_hat = model(x, l)\n        loss = F.cross_entropy(y_hat, y)\n        pred = torch.max(y_hat, 1)[1]\n        correct += (pred == y).float().sum()\n        total += y.shape[0]\n        sum_loss += loss.item()*y.shape[0]\n        sum_rmse += np.sqrt(mean_squared_error(pred, y.unsqueeze(-1)))*y.shape[0]\n    return sum_loss\/total, correct\/total, sum_rmse\/total","a76c63ea":"batch_size = 5000\nvocab_size = len(words)\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\nval_dl = DataLoader(valid_ds, batch_size=batch_size)","e2386f49":"class LSTM_fixed_len(torch.nn.Module) :\n    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n        super().__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.linear = nn.Linear(hidden_dim, 5)\n        self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, x, l):\n        x = self.embeddings(x)\n        x = self.dropout(x)\n        lstm_out, (ht, ct) = self.lstm(x)\n        return self.linear(ht[-1])\n    \nmodel_fixed =  LSTM_fixed_len(vocab_size, 50, 50)","2781d9c5":"train_model(model_fixed, epochs=30, lr=0.01)","1300443a":"class LSTM_variable_input(torch.nn.Module) :\n    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.dropout = nn.Dropout(0.3)\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.linear = nn.Linear(hidden_dim, 5)\n        \n    def forward(self, x, s):\n        x = self.embeddings(x)\n        x = self.dropout(x)\n        x_pack = pack_padded_sequence(x, s, batch_first=True, enforce_sorted=False)\n        out_pack, (ht, ct) = self.lstm(x_pack)\n        out = self.linear(ht[-1])\n        return out\n    \nmodel = LSTM_variable_input(vocab_size, 50, 50)","127a2d17":"train_model(model, epochs=30, lr=0.1)","8cf34100":"def load_glove_vectors(glove_file=\"\/kaggle\/input\/glove6b50dtxt\/glove.6B.50d.txt\"):\n    \"\"\"Load the glove word vectors\"\"\"\n    word_vectors = {}\n    with open(glove_file) as f:\n        for line in f:\n            split = line.split()\n            word_vectors[split[0]] = np.array([float(x) for x in split[1:]])\n    return word_vectors","66a0de57":"def get_emb_matrix(pretrained, word_counts, emb_size = 50):\n    \"\"\" Creates embedding matrix from word vectors\"\"\"\n    vocab_size = len(word_counts) + 2\n    vocab_to_idx = {}\n    vocab = [\"\", \"UNK\"]\n    W = np.zeros((vocab_size, emb_size), dtype=\"float32\")\n    W[0] = np.zeros(emb_size, dtype='float32') # adding a vector for padding\n    W[1] = np.random.uniform(-0.25, 0.25, emb_size) # adding a vector for unknown words \n    vocab_to_idx[\"UNK\"] = 1\n    i = 2\n    for word in word_counts:\n        if word in word_vecs:\n            W[i] = word_vecs[word]\n        else:\n            W[i] = np.random.uniform(-0.25,0.25, emb_size)\n        vocab_to_idx[word] = i\n        vocab.append(word)\n        i += 1   \n    return W, np.array(vocab), vocab_to_idx","6475a547":"word_vecs = load_glove_vectors()\npretrained_weights, vocab, vocab2index = get_emb_matrix(word_vecs, counts)","b019251f":"class LSTM_glove_vecs(torch.nn.Module) :\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, glove_weights) :\n        super().__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.embeddings.weight.data.copy_(torch.from_numpy(glove_weights))\n        self.embeddings.weight.requires_grad = False ## freeze embeddings\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.linear = nn.Linear(hidden_dim, 5)\n        self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, x, l):\n        x = self.embeddings(x)\n        x = self.dropout(x)\n        lstm_out, (ht, ct) = self.lstm(x)\n        return self.linear(ht[-1])\n    \nmodel = LSTM_glove_vecs(vocab_size, 50, 50, pretrained_weights)","29ba98ea":"train_model(model, epochs=30, lr=0.1)","fbd3ad04":"def train_model_regr(model, epochs=10, lr=0.001):\n    parameters = filter(lambda p: p.requires_grad, model.parameters())\n    optimizer = torch.optim.Adam(parameters, lr=lr)\n    for i in range(epochs):\n        model.train()\n        sum_loss = 0.0\n        total = 0\n        for x, y, l in train_dl:\n            x = x.long()\n            y = y.float()\n            y_pred = model(x, l)\n            optimizer.zero_grad()\n            loss = F.mse_loss(y_pred, y.unsqueeze(-1))\n            loss.backward()\n            optimizer.step()\n            sum_loss += loss.item()*y.shape[0]\n            total += y.shape[0]\n        val_loss = validation_metrics_regr(model, val_dl)\n        if i % 5 == 1:\n            print(\"train mse %.3f val rmse %.3f\" % (sum_loss\/total, val_loss))\n\ndef validation_metrics_regr (model, valid_dl):\n    model.eval()\n    correct = 0\n    total = 0\n    sum_loss = 0.0\n    for x, y, l in valid_dl:\n        x = x.long()\n        y = y.float()\n        y_hat = model(x, l)\n        loss = np.sqrt(F.mse_loss(y_hat, y.unsqueeze(-1)).item())\n        total += y.shape[0]\n        sum_loss += loss.item()*y.shape[0]\n    return sum_loss\/total","47a83dea":"class LSTM_regr(torch.nn.Module) :\n    def __init__(self, vocab_size, embedding_dim, hidden_dim) :\n        super().__init__()\n        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n        self.linear = nn.Linear(hidden_dim, 1)\n        self.dropout = nn.Dropout(0.2)\n        \n    def forward(self, x, l):\n        x = self.embeddings(x)\n        x = self.dropout(x)\n        lstm_out, (ht, ct) = self.lstm(x)\n        return self.linear(ht[-1])\n    \nmodel =  LSTM_regr(vocab_size, 50, 50)","0ca274dc":"train_model_regr(model, epochs=30, lr=0.05)","4b33466c":"### 3. LSTM with fixed input size and fixed pre-trained Glove word-vectors:\nhttps:\/\/nlp.stanford.edu\/projects\/glove\/","14007286":"## Data Preprocessing","289748ec":"### 2. LSTM with variable input size:","65762080":"## PyTorch Training Loop","3100b474":"### Bonus: Predict ratings using regression instead of classification","9295c3f2":"# **Problem Statement:** Given an item\u2019s review comment, predict the rating\nWe'll use Womens Clothing E-Commerce Reviews as our dataset, where the rating takes integer values from 1 to 5 (1 being worst and 5 being best).\n\nReference: https:\/\/towardsdatascience.com\/multiclass-text-classification-using-lstm-in-pytorch-eac56baed8df","01c7affb":"## PyTorch Dataset","7621943d":"## LSTM Model\n\n### 1. LSTM with fixed input size"}}