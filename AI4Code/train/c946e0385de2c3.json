{"cell_type":{"63518005":"code","38672286":"code","bfea6550":"code","5a708820":"code","f7f28c7d":"code","5f4277c9":"code","6e207305":"code","635a32da":"code","048d050f":"code","73af4465":"code","fe779746":"code","e04a5377":"code","8666246c":"code","bb774643":"code","472b2511":"code","3afbe328":"code","2c86124a":"code","d6534999":"code","82fb2bf4":"code","a6a8705e":"code","cf078b06":"code","4ecaa77a":"code","fadc55e9":"code","158ae323":"code","b20fd6a6":"code","8fcf776d":"code","54928bf0":"code","a5b2cf00":"code","5a621039":"code","728ee091":"code","1debcba4":"code","768730eb":"code","374992c8":"code","d80d41c6":"code","91edd3b2":"code","f089a805":"code","21a9c6dc":"code","a164c961":"code","02f2d4e5":"code","14a3eeae":"code","53ac2d99":"code","a68f38ca":"code","d2b25c96":"code","3aa83a21":"code","39960264":"code","23bc2459":"code","9193e166":"code","583ec91a":"code","f442abcf":"code","6fd68a56":"code","20448ae6":"code","268df2d8":"code","66f72552":"code","2a494396":"code","01965ba7":"code","344227f8":"code","9c760a40":"code","ee0b6375":"code","93deb0ec":"code","90547b8e":"code","003af08d":"code","143907d0":"code","78d41417":"code","ea4ab8c6":"code","1cb0ef7f":"code","ad4d8963":"code","f2341fcf":"code","47988d2a":"code","0895be21":"code","df248db6":"code","578087cc":"code","ab49ef04":"markdown","d155d058":"markdown","cc6323d2":"markdown","5c9dedca":"markdown","4aaa0859":"markdown","908e708e":"markdown","13598090":"markdown","ed7cf94f":"markdown","fd9b56f5":"markdown","e470bae0":"markdown","e426c98f":"markdown","7b790e71":"markdown","1a2b5dff":"markdown","6a4ccd4a":"markdown","65c1df80":"markdown","6a049d20":"markdown","9c865541":"markdown","eeca32ac":"markdown","6a48c5da":"markdown","264a16dc":"markdown","07316788":"markdown","22698686":"markdown","44c87007":"markdown","95f5596f":"markdown","64a5077a":"markdown","7986264a":"markdown","cddd40ed":"markdown","883613a8":"markdown","a868f973":"markdown","b2392d93":"markdown","6c9c164c":"markdown","adecc790":"markdown","4c1e94b0":"markdown","da440924":"markdown","00f9347f":"markdown","d8e6aa28":"markdown","fe166e0b":"markdown","eeccb7fe":"markdown"},"source":{"63518005":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import feature_column\nfrom sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import model_selection\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","38672286":"def save_results(Survived, test, path):\n    submission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": Survived\n    })\n    submission.to_csv(path, index=False)","bfea6550":"def evaulate_and_save(\n    model, \n    validation_features, \n    validation_targets, \n    test_features, \n    save_path, \n    best_score, \n    best_path, \n    columns = None\n):\n    if columns is None:\n        feature_columns = validation_features.columns\n    else:\n        feature_columns = columns\n    y_pred = model.predict(validation_features[feature_columns])\n    if y_pred.dtype != int:\n        if y_pred.shape[-1] == 2:\n            y_pred = np.argmax(y_pred, axis=-1)\n        if y_pred.shape[-1] == 1:\n            y_pred = np.array(y_pred > 0.5, dtype=int)\n    y_pred = y_pred.reshape(-1)\n    score = sklearn.metrics.accuracy_score(validation_targets, y_pred)\n    f1 = sklearn.metrics.f1_score(validation_targets, y_pred)\n    print(\"Accuracy Score:\", score)\n    print(\"Classification Report:\")\n    print(sklearn.metrics.classification_report(validation_targets, y_pred))\n    Survived = model.predict(test_features[feature_columns])\n    if Survived.dtype != int:\n        if Survived.shape[-1] == 2:\n            Survived = np.argmax(Survived, axis=-1)\n        if Survived.shape[-1] == 1:\n            Survived = np.array(Survived > 0.5, dtype=int)\n    Survived = np.array(Survived, dtype=int).reshape(-1)\n    save_results(Survived, test_features, save_path)\n    if score > best_score:\n        best_score = score\n        best_path = save_path\n    return best_score, best_path","5a708820":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","f7f28c7d":"train.head()","5f4277c9":"test.head()","6e207305":"train.isnull().sum()","635a32da":"test.isnull().sum()","048d050f":"embark_label = train[\"Embarked\"].mode()[0]\ntrain[\"Cabin\"] = train[\"Cabin\"].replace(np.NAN,  \"Unknown\")\ntrain[\"Embarked\"] = train[\"Embarked\"].replace(np.NAN, embark_label)\ntrain[\"Age\"] = train[\"Age\"].replace(np.NAN, train[\"Age\"].median())\ntest[\"Cabin\"] = test[\"Cabin\"].replace(np.NAN, \"Unknown\")\ntest[\"Age\"] = test[\"Age\"].replace(np.NAN, test[\"Age\"].median())\ntest[\"Fare\"] = test[\"Fare\"].replace(np.NAN, test[\"Fare\"].mean())","73af4465":"cabin_labels = sorted(set(list(train[\"Cabin\"].unique()) + list(test[\"Cabin\"].unique())))\nprint(cabin_labels[:30])","fe779746":"train[\"Cabin_type\"] = train[\"Cabin\"].apply(lambda cabin: cabin[0])\ntest[\"Cabin_type\"] = test[\"Cabin\"].apply(lambda cabin: cabin[0])","e04a5377":"train[\"family_member_size\"] = 1 + train[\"SibSp\"] + train[\"Parch\"]\ntest[\"family_member_size\"] = 1 + test[\"SibSp\"] + test[\"Parch\"]","8666246c":"categorical_features = [\"Sex\", \"Cabin_type\", \"Embarked\"]\ncategorical_label_dictionary = dict()\nfor feature in categorical_features:\n    unique_labels = sorted(set(list(train[feature].unique()) + list(test[feature].unique())))\n    for data in [train, test]:\n        categorical_label_dictionary[feature] = unique_labels\n        data[feature + \"_value\"] = data[feature].apply(lambda item: unique_labels.index(item))","bb774643":"train.head(30)","472b2511":"train.info()","3afbe328":"train.describe()","2c86124a":"train.corr()[\"Survived\"].sort_values(key=lambda x: abs(x), ascending=False)","d6534999":"related_columns = list(train.corr()[train.corr()[\"Survived\"].abs() > 0.05].index)\nrelated_columns.remove(\"Survived\")\nprint(related_columns)","82fb2bf4":"sns.countplot(x=\"Sex\", hue=\"Survived\", data=train)\nplt.title(\"Survival of different Gender\")\nplt.show()","a6a8705e":"sns.histplot(x=\"Age\", hue=\"Survived\", data=train)\nplt.title(\"Survival of different Age\")\nplt.show()","cf078b06":"train.groupby(\"Pclass\")[\"Survived\"].mean()","4ecaa77a":"sns.countplot(x=\"Pclass\", hue=\"Survived\", data=train)\nplt.title(\"Survival of different Pclass\")\nplt.show()","fadc55e9":"sns.histplot(x=\"Fare\", hue=\"Survived\", data=train)\nplt.title(\"Survival of different Fare\")\nplt.show()","158ae323":"train.groupby(\"Cabin_type\")[\"Survived\"].mean()","b20fd6a6":"sns.countplot(x=\"Cabin_type\", hue=\"Survived\", data=train)\nplt.title(\"Survival of different Cabin\")\nplt.show()","8fcf776d":"train.groupby(\"Embarked\")[\"Survived\"].mean()","54928bf0":"sns.countplot(x=\"Embarked\", hue=\"Survived\", data=train)\nplt.title(\"Survival of different Embarked\")\nplt.show()","a5b2cf00":"train.groupby(\"SibSp\")[\"Survived\"].mean()","5a621039":"sns.countplot(x=\"SibSp\", hue=\"Survived\", data=train)\nplt.title(\"Survival of different SibSp\")\nplt.show()","728ee091":"train.groupby(\"Parch\")[\"Survived\"].mean()","1debcba4":"sns.countplot(x=\"Parch\", hue=\"Survived\", data=train)\nplt.title(\"Survival of different SibSp\")\nplt.show()","768730eb":"train_test = pd.concat([train, test])\ntrain_test.head()","374992c8":"for feature in [\"Sex\", \"Cabin_type\", \"Embarked\"]:\n    items = pd.get_dummies(train_test[feature + \"_value\"])\n    labels = categorical_label_dictionary[feature]\n    items.columns = [feature + \"_\" + labels[column] for column in list(items.columns)]\n    train_test[items.columns] = items\n    train_test.pop(feature + \"_value\")","d80d41c6":"train_test.head()","91edd3b2":"for column in [\"Name\", \"Sex\", \"Ticket\", \"Cabin\", \"Cabin_type\", \"Embarked\"]:\n    train_test.pop(column)","f089a805":"train_features = train_test.iloc[0: len(train)]\ntest_features = train_test.iloc[len(train):]","21a9c6dc":"_ = train_features.pop(\"PassengerId\")","a164c961":"_ = test_features.pop(\"Survived\")","02f2d4e5":"train_features.head()","14a3eeae":"test_features.head()","53ac2d99":"validation_split = 0.2","a68f38ca":"train_targets = train_features.pop(\"Survived\")\ntrain_features, validation_features, train_targets, validation_targets = model_selection.train_test_split(train_features, train_targets, test_size=validation_split, random_state=np.random.randint(1, 1000))\nprint(train_features.shape, validation_features.shape, train_targets.shape, validation_targets.shape)","d2b25c96":"best_score = 0\nbest_path = \"\"","3aa83a21":"model = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(train_features.shape[1])),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dropout(0.3),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n])\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=20)\nhistory = model.fit(\n    train_features, train_targets, \n    epochs=400, validation_data=(validation_features, validation_targets), \n    callbacks=[early_stop],\n    verbose=0\n)\npd.DataFrame(history.history).plot()","39960264":"best_score, best_path = evaulate_and_save(\n    model, \n    validation_features, \n    validation_targets, \n    test_features,\n    \"submission_dnn.csv\",\n    best_score, \n    best_path,\n    columns=validation_features.columns\n)","23bc2459":"categorical_feature_names = [\"Pclass\", \"Sex_value\", \"Embarked_value\", \"Cabin_type_value\"]\nnumerical_feature_names = [\"Age\", \"Fare\", \"SibSp\", \"Parch\", \"family_member_size\"]\ncategorical_features = [\n    feature_column.indicator_column(\n        feature_column.categorical_column_with_vocabulary_list(key, sorted(list(train[key].unique())))\n    ) for key in categorical_feature_names\n]\nnumerical_features = [feature_column.numeric_column(key) for key in numerical_feature_names]\ninput_dictionary = dict()\ninputs = dict()\nfor item in numerical_features:\n    inputs[item.key] = tf.keras.layers.Input(name=item.key, shape=())\nfor item in categorical_features:\n    inputs[item.categorical_column.key] = tf.keras.layers.Input(name=item.categorical_column.key, shape=(), dtype=\"int32\")","9193e166":"def features_and_labels(row_data):\n    label = row_data.pop(\"Survived\")\n    features = row_data\n    return features, label\n\ndef create_dataset(pattern, epochs=1, batch_size=32, mode='eval'):\n    dataset = tf.data.experimental.make_csv_dataset(\n        pattern, batch_size\n    )\n    dataset = dataset.map(features_and_labels)\n    if mode == 'train':\n        dataset = dataset.shuffle(buffer_size=128).repeat(epochs)\n    dataset = dataset.prefetch(1)\n    return dataset\n\ndef create_test_dataset(pattern, batch_size=32):\n    dataset = tf.data.experimental.make_csv_dataset(\n        pattern, batch_size\n    )\n    dataset = dataset.map(lambda features: features)\n    dataset = dataset.prefetch(1)\n    return dataset","583ec91a":"train_data, val_data = train_test_split(\n    train[categorical_feature_names + numerical_feature_names + [\"Survived\"]],\n    test_size=validation_split,\n    random_state=np.random.randint(0, 1000)\n)\ntrain_data.to_csv(\"train_data.csv\", index=False)\nval_data.to_csv(\"val_data.csv\", index=False)\ntest[categorical_feature_names + numerical_feature_names].to_csv(\"test_data.csv\", index=False)\nbatch_size = 32\ntrain_dataset = create_dataset(\"train_data.csv\", batch_size=batch_size, mode='train')\nval_dataset = create_dataset(\"val_data.csv\", batch_size=val_data.shape[0], mode='eval').take(1)\ntest_dataset = create_test_dataset(\"test_data.csv\", batch_size = test.shape[0]).take(1)","f442abcf":"def build_deep_and_wide_model():\n    deep = tf.keras.layers.DenseFeatures(numerical_features, name='deep')(inputs)\n    deep = tf.keras.layers.Dense(16, activation='relu')(deep)\n    deep = tf.keras.layers.Dropout(0.3)(deep)\n    deep = tf.keras.layers.Dense(16, activation='relu')(deep)\n    deep = tf.keras.layers.Dropout(0.3)(deep)\n    deep = tf.keras.layers.Dense(16, activation='relu')(deep)\n    deep = tf.keras.layers.Dropout(0.3)(deep)\n    deep = tf.keras.layers.Dense(16, activation='relu')(deep)\n    deep = tf.keras.layers.Dropout(0.3)(deep)\n    wide = tf.keras.layers.DenseFeatures(categorical_features, name='wide')(inputs)\n    wide = tf.keras.layers.Dense(32, activation='relu')(wide)\n    combined = tf.keras.layers.concatenate(inputs=[deep, wide], name='combined')\n    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(combined)\n    model = tf.keras.Model(inputs=list(inputs.values()), outputs=output)\n    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n    return model","6fd68a56":"deep_and_wide_model = build_deep_and_wide_model()\ntf.keras.utils.plot_model(deep_and_wide_model, show_shapes=False, rankdir='LR')","20448ae6":"epochs = 400\nearly_stop = tf.keras.callbacks.EarlyStopping(patience=10)\nsteps_per_epoch = train_data.shape[0] \/\/ batch_size\nhistory = deep_and_wide_model.fit(\n    train_dataset, \n    steps_per_epoch=steps_per_epoch,\n    validation_data=val_dataset,\n    epochs=epochs,\n    callbacks=[early_stop],\n    verbose=0\n)\npd.DataFrame(history.history).plot()","268df2d8":"y_pred =  np.array(deep_and_wide_model.predict(val_dataset) > 0.5, dtype=int).reshape(-1)\nscore = accuracy_score(val_data[\"Survived\"], y_pred)\nprint(\"Accuracy score:\", score)\nprint(sklearn.metrics.classification_report(val_data[\"Survived\"], y_pred))\nSurvived = np.argmax(deep_and_wide_model.predict(test_dataset), axis=-1).reshape(-1)\nprint(Survived.shape)\npath = \"submission_deep_and_wide_model.csv\"\nsave_results(Survived, test, path)\nif score > best_score:\n    best_score = score\n    best_path = path","66f72552":"logitistc_related_columns = list(train.corr()[train.corr()[\"Survived\"].abs() > 0.2].index)\nlogitistc_related_columns.remove(\"Survived\")\nlogitistc_related_columns","2a494396":"from sklearn.linear_model import LogisticRegression\nbest_logit = None\nbest_solver = \"\"\nbest_logit_score = 0\nlogit_train_features, logit_val_features = train_test_split(train[logitistc_related_columns +  [\"Survived\"]], test_size=0.2, random_state=48)\nlogit_train_targets = logit_train_features.pop(\"Survived\")\nlogit_val_targets = logit_val_features.pop(\"Survived\")\nfor solver in ['newton-cg', 'lbfgs', 'liblinear']:\n    logit = LogisticRegression(solver=solver)\n    logit.fit(logit_train_features, logit_train_targets)\n    score = logit.score(logit_val_features, logit_val_targets)\n    if score > best_logit_score:\n        best_solver = solver\n        best_logit_score = score\n        best_logit = logit\nprint(\"Best Solver:\", best_solver, \"Score:\", best_logit_score)","01965ba7":"test_features","344227f8":"best_score, best_path = evaulate_and_save(\n    best_logit, \n    logit_val_features, \n    logit_val_targets, \n    test, \n    \"submission_logit.csv\", \n    best_score, \n    best_path,\n    columns=logitistc_related_columns\n)","9c760a40":"best_algorithm = \"\"\nbest_knn_score = 0\nbest_knn = None\nfor algorithm in ['ball_tree', 'kd_tree', 'brute']:\n    knn = KNeighborsClassifier(2, algorithm=algorithm)\n    knn.fit(train_features, train_targets)\n    score = knn.score(validation_features, validation_targets) \n    if score > best_knn_score:\n        best_knn_score = score\n        best_knn = knn\nprint(\"Best KNN Score: \", best_knn_score, \"Model:\", best_knn)","ee0b6375":"best_score, best_path = evaulate_and_save(\n    best_knn, validation_features, validation_targets, test_features, \n    \"submission_knn.csv\", best_score, best_path\n)","93deb0ec":"from sklearn.tree import DecisionTreeClassifier\nbest_tree = None\nbest_tree_score = 0\nfor max_depth in range(6, 30):\n    tree = sklearn.tree.DecisionTreeClassifier(max_depth=max_depth)\n    tree.fit(train_features, train_targets)\n    score = tree.score(validation_features, validation_targets)\n    if score > best_tree_score:\n        best_tree_score = score\n        best_tree = tree\nprint(\"Best Decision Tree Score: \", best_tree_score, \"Model:\", best_tree)","90547b8e":"best_score, best_path = evaulate_and_save(\n    best_tree, validation_features, validation_targets, test_features, \n    \"submission_tree.csv\", best_score, best_path\n)","003af08d":"best_gbc_score = 0\nbest_depth = 5\nbest_n_estimators = 5\nbest_learning_rate = 0.1\nbest_gbc_model = None\nfor learning_rate in list(np.arange(0.05, 0.15, 0.01)):\n    gbc = GradientBoostingClassifier(\n            n_estimators=best_n_estimators, \n            learning_rate=learning_rate, \n            max_depth=best_depth, \n            random_state=np.random.randint(1, 1000)\n    )\n    gbc.fit(train_features, train_targets)\n    score = gbc.score(validation_features, validation_targets)\n    if score > best_gbc_score:\n        best_learning_rate = learning_rate\n        best_gbc_score = score\n        best_gbc_model = gbc\nprint(\"Best Learning Rate:\", best_learning_rate)\nfor depth in range(5, 20):\n    gbc = GradientBoostingClassifier(\n            n_estimators=best_n_estimators, \n            learning_rate=best_learning_rate, \n            max_depth=depth, \n            random_state=np.random.randint(1, 1000)\n    )\n    gbc.fit(train_features, train_targets)\n    score = gbc.score(validation_features, validation_targets)\n    if score > best_gbc_score:\n        best_depth = depth\n        best_gbc_score = score\n        best_gbc_model = gbc\nprint(\"Best Depth:\", best_depth)\nfor n_estimators in range(5, 15):\n    gbc = GradientBoostingClassifier(\n            n_estimators=n_estimators, \n            learning_rate=best_learning_rate, \n            max_depth=best_depth, \n            random_state=np.random.randint(1, 1000)\n    )\n    gbc.fit(train_features, train_targets)\n    score = gbc.score(validation_features, validation_targets)\n    if score > best_gbc_score:\n        best_n_estimators = n_estimators\n        best_gbc_score = score\n        best_gbc_model = gbc\nprint(\"Best Number of Estimator:\", best_depth)\nprint(\"Best Gradient Boosting Classifier Score:\", best_gbc_score, \" Model:\", best_gbc_model)","143907d0":"best_score, best_path = evaulate_and_save(\n    best_gbc_model, validation_features, validation_targets, test_features, \n    \"submission_gbc.csv\", best_score, best_path\n)","78d41417":"best_forest = None\nbest_max_depth = 8\nbest_n_estimators = 15\nbest_forest_score = 0\nprint(\"Find best number of estimators\")\nfor n_estimators in list(range(3, 40, 2)):\n    forest = RandomForestClassifier(\n        n_estimators=n_estimators, \n        max_depth=best_max_depth, \n        random_state=np.random.randint(1, 1000)\n    )\n    forest.fit(train_features, train_targets)\n    score = forest.score(validation_features, validation_targets)\n    print(\"Score: \", score)\n    if score > best_forest_score:\n        best_n_estimators = n_estimators\n        best_forest_score = score\n        best_forest = forest\nprint(\"Best Number of Estimator:\", best_n_estimators)\nfor max_depth in range(4, 15):\n    forest = RandomForestClassifier(\n        n_estimators=best_n_estimators, \n        max_depth=max_depth, \n        random_state=np.random.randint(1, 1000)\n    )\n    forest.fit(train_features, train_targets)\n    score = forest.score(validation_features, validation_targets)\n    print(\"Score: \", score)\n    if score > best_forest_score:\n        best_max_depth = max_depth\n        best_forest_score = best_score\n        best_forest = forest\nprint(\"Best Max Depth:\", best_max_depth,\"\\nBest score:\", best_forest_score)","ea4ab8c6":"best_score, best_path = evaulate_and_save(\n    best_forest, validation_features, validation_targets, test_features, \n    \"submission_forest.csv\", best_score, best_path\n)","1cb0ef7f":"kmeans = KMeans(n_clusters=2)\nkmeans.fit(train_features, train_targets)","ad4d8963":"best_score, best_path = evaulate_and_save(\n    kmeans, validation_features, validation_targets, test_features, \n    \"submission_kmeans.csv\", best_score, best_path\n)","f2341fcf":"def get_value(key1, key2, value, parameters, best_index):\n    return parameters[key1][best_index[key1]] if key1 != key2 else value\ndef find_best_model_with_xgboost(\n    train_features, \n    train_targets,\n    validation_features,\n    validation_targets,\n    parameters,\n    columns = None\n):\n    train_f = train_features\n    val_f = validation_features\n    if columns != None:\n        train_f = train_features[columns]\n        val_f = validation_features[columns]\n    else: \n        train_f = train_features\n        val_f = validation_features\n    all_keys = parameters.keys()\n    best_index = {key: 0 for key in all_keys}\n    best_xgb_score = 0\n    best_xgb_model = None\n    for key in all_keys:\n        values = parameters[key]\n        current_best_model = None\n        current_best_score = 0\n        for index, value in enumerate(values):\n            learning_rate = get_value(\"learning_rate\", key, value, parameters, best_index)\n            max_depth = get_value(\"max_depth\", key, value, parameters, best_index)\n            gamma = get_value(\"gamma\", key, value, parameters, best_index)\n            xgb = XGBClassifier(\n                max_depth=max_depth,\n                learning_rate=learning_rate,\n                gamma=gamma\n            )\n            xgb.fit(\n                train_f, \n                train_targets, \n                early_stopping_rounds=10, \n                eval_metric=\"logloss\", \n                eval_set=[(val_f, validation_targets)], \n                verbose=False\n            )\n            score = xgb.score(validation_features, validation_targets)\n            if score > current_best_score:\n                current_best_score = score\n                current_best_model = xgb\n                best_index[key] = index\n            if score > best_xgb_score: \n                best_xgb_score = score\n                best_xgb_model = xgb\n    return best_xgb_model, best_xgb_score","47988d2a":"from xgboost import XGBClassifier\nhyper_parameters = {\n    \"max_depth\": list(range(5, 15)),\n    \"learning_rate\": [0.1, 0.15, 0.2, 0.25, 0.3],\n    \"gamma\": [0.5, 1, 1.5, 2.0]\n}\nbest_xgb_score, best_xgb_model = find_best_model_with_xgboost(\n    train_features, \n    train_targets,\n    validation_features,\n    validation_targets,\n    hyper_parameters\n)\nprint(\"Best Model:\", best_xgb_model, \" Score: \", best_xgb_score)","0895be21":"best_score, best_path = evaulate_and_save(\n    best_xgb_score, validation_features, validation_targets, test_features, \n    \"submission_xgb.csv\", best_score, best_path\n)","df248db6":"print(\"Best path:\", best_path)\nprint(\"Best Score:\", best_score)","578087cc":"submission = pd.read_csv(best_path)\nprint(submission.head(10))\nsubmission.to_csv(\"submission.csv\", index=False)","ab49ef04":"### Basic Statistic infos","d155d058":"#### Survival of different Cabin\n- More than half Passengers from Cabin started with C, D, E, F, G survived;\n- Less than half Passengers from Cabin started with A,B survived;\n- 30% of Passengers with unknown Cabin survived;\n- Almost no Passengers from Cabin started with T survived.","cc6323d2":"Let's see the Cabin labels, there are so many of them. But I make an assmption that the First Alphabet matters, it indicated the location and class of the Passengers so it had an impact to survive.","5c9dedca":"## Import Packages","4aaa0859":"### Train Validation Split","908e708e":"**Evaluate Model, save and keep tract of best results**","13598090":"#### Survival of Different Age\n","ed7cf94f":"#### Survival of Different Parch (Number of parents or children)\n- Passengers without parents or children had 1 \/ 3 survival rate.\n- Passengers with 1 - 3 parents or children had 1 \/ 2 survival rate.\n- Passengers with more than 4 parents or children were less likely to survive.\n","fd9b56f5":"## Exploratory Data Analysis","e470bae0":"## Using KMeans","e426c98f":"#### Survival of Different SibSp (Number of siblings or spouses)\n- Passengers without siblings or spouses had 1 \/ 3 Survival Rate.\n- Passengers with one or two siblings or spouses had about 1 \/ 2 Survival Rate.\n- Passengers with more than two siblings were less likely to survive.","7b790e71":"## Import datasets","1a2b5dff":"### Using Logistic Regression","6a4ccd4a":"#### Survival of different Pclass\n- Passengers from Pclass 1 had 62% Survival Rate;\n- Passengers from Pclass 2 had 47% Survival Rate;\n- Passengers from Pclass 3 had 24% Survival Rate;","65c1df80":"# Classification with SKLearn and TensorFlow\n## Table of Contents\n- Summary\n- Import Packages\n- Common Functions\n- Import datasets\n- Data Wrangling\n- Handle Categorical Features\n- Exploratory Data Analysis\n    - Basic Statistic infos\n    - What's the factor to survive?\n        - Survival of different Gender\n        - Survival of Different Age\n        - Survival of different Pclass\n        - Survival of different Fare\n        - Survival of different Cabin\n        - Survival of different Embarked\n        - Survival of Different SibSp (Number of siblings or spouses)\n        - Survival of Different Parch (Number of parents or children)\n- More data Preprocessing\n    - Train Validation Split\n- Model Development and Evaluation\n    - Using Deep Neural Network\n    - Using Deep and Wide Model\n    - Using KNN\n    - Using Decision Tree Classifier\n    - Using Gradient Boosting Classifier\n    - Using Random Forest Classifier\n    - Using KMeans\n    - Using XGBoost Classifier\n- Submit best Model\n- Conclusions\n- Discussions\n    \n## Summary\nThis notebook aim to create Models to make predictions on Titanic - Machine Learning from Disaster dataset. And I will try to answer following questions:\n- Which Model can achieve a better performance? I will use different kinds of Models:\n    - Deep Neural Network\n    - Deep and Wide Neural Network using keras DenseFeatures\n    - Logistic Regression\n    - KNN\n    - Decision Tree Classifier\n    - Gradient Boosting Classifier\n    - Random Forest Classifier\n    - KMeans\n    - XGBoost \n    \n- How to preprocess the data to get the optimal results? To answer this question, I will try different techniques:\n    - Exploratory Data Analysis\n    - Missing Data Imputation\n    - HyperParameter Searching\n    - Train Validation Split\n    - Using Logistic Regression","6a049d20":"### Using Deep Neural Network","9c865541":"## Submit best Model\nThis result can be different from Kagggle LeaderBoard, so you may try different submission files.","eeca32ac":"## More data Preprocessing","6a48c5da":"#### Survival of different Fare\nMost of the tickets were less than 100 pounds. Only about 1 \/ 5 with fare around 10 pounds survived.","264a16dc":"As we can see Age, Cabin and Fare information contains missing values, so we need to apply Missing Value  Imputation to them. The most common way is to replace categorical missing values with most fequent category and repalce numerical missing values with average value of that feature.","07316788":"### Using Deep and Wide Model","22698686":"## Using XGBoost Classifier","44c87007":"#### Survival of different Embarked\n- About 1 \/ 3 passengers from Embarked Q, S survived;\n- About half passengers from Embarked C survived;","95f5596f":"### Using Gradient Boosting Classifier","64a5077a":"## Handle Categorical Features","7986264a":"**Save results**","cddd40ed":"### What's the factor to survive?\nAs we can see it's related to Gender, PClass, Status, Fare, Cabin and Embarked. ","883613a8":"## Common Functions","a868f973":"#### Survival of different Gender\nWomen have a higher Survival rate than Men.","b2392d93":"### Using Random Forest Classifier","6c9c164c":"### Using KNN","adecc790":"## Model Development and Evaluation\nI will try different Models and use results from best Model.","4c1e94b0":"Let's see after we preprocess, what does the data look like?","da440924":"## Discussions\n- How to find a better Feature Engineering Approach to this dataset?\n- Dataset is so small, what kind of Data Augmentation method can I apply to it?\n- There is a Label Imbalance problem which will affect outcome of Machile Learning Models expecially Deep Neural Network. If I make the Label balanced, can I get a better result?\n- If I apply Data Normalization to data, can I get a better result?\n- If I apply K-Fold alogorithm, can I get a better result?\n- How about providing a custom loss function to DNN model?","00f9347f":"## Data Wrangling","d8e6aa28":"We can indicate family member size by SibSp and Parch feature: ","fe166e0b":"## Conclusions\n- Although Deep Learning is very powerful. When handling this dataset, it's not easy to find a Model that outperforms some traditional Machine Learning algorithms. Maybe because the dataset is too small.\n- Gradient Boosting Classifier, Random Forest Classifier can also achieve a very good performance and it requires less computing power than Deep Neural Network. ","eeccb7fe":"### Using Decision Tree Classifier"}}