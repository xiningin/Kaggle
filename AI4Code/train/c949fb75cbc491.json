{"cell_type":{"45633a5c":"code","c80467c9":"code","b55b1645":"code","5bc94ac0":"code","72f8f605":"code","a6df03fd":"code","6d192c1f":"code","2e83df44":"code","0de979d2":"code","23789248":"code","555967d8":"code","5066c6d6":"code","7b781cab":"code","bf47c134":"code","08cc4250":"code","24bee906":"code","f9205d2d":"code","144d5176":"code","10230030":"code","4609acac":"code","4aec6b50":"code","1a3a702f":"code","bd9b3fb0":"code","c875ff96":"code","ac8e5b13":"code","842b8d08":"code","a7d5baa3":"code","957fdfc6":"markdown","f59bd489":"markdown","66f598d8":"markdown","ca8fe5cd":"markdown","c587f58e":"markdown","cf415f2f":"markdown","d6bdfbc4":"markdown","e0dc4ecc":"markdown","ca80e69d":"markdown","aec57639":"markdown","17551981":"markdown","aa6dc80c":"markdown","528329a9":"markdown","470ff6cb":"markdown","c3c7dd8c":"markdown","82da4087":"markdown"},"source":{"45633a5c":"\nimport pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport folium \n\nimport os\nimport gc\nimport joblib\n\nimport time\n\nfrom sklearn import metrics, linear_model\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm.notebook import tqdm\n\n\n\nimport tensorflow as tf\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics, preprocessing\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras import utils\n\nfrom sklearn import model_selection\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","c80467c9":"data = pd.read_csv('..\/input\/chicago-traffic\/chicago.csv')\ndata[\"TIME\"]=pd.to_datetime(data[\"TIME\"], format=\"%m\/%d\/%Y %H:%M:%S %p\")\ndata.drop(data[data['SPEED']==0].index,inplace =True)\ndata['day'] = data['TIME'].dt.day\ndata['MONTH'] = data['TIME'].dt.month\ndata['YEAR'] = data['TIME'].dt.year\ndata = data[data['YEAR']!=2020]\ndata = data[data['SPEED']<100]\n\ndata = data.groupby(['REGION_ID','HOUR','MONTH','day', 'WEST','EAST', 'SOUTH','NORTH','DAY_OF_WEEK','YEAR'])[['SPEED','BUS_COUNT','NUM_READS']].agg('mean').reset_index()\ndata['CENTER_LAT']=data['NORTH']*0.5+0.5*data['SOUTH']\ndata['CENTER_LON']=data['EAST']*0.5+0.5*data['WEST']","b55b1645":"data['MINUTE'] = '00'\ndata['Time'] = pd.to_datetime(data[['YEAR','MONTH','day','HOUR','MINUTE']].astype(str).agg('-'.join,axis=1),format='%Y-%m-%d-%H-%M')","5bc94ac0":"def haversine_array(lat1, lng1, lat2, lng2):\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    AVG_EARTH_RADIUS = 6371  # in km\n    lat = lat2 - lat1\n    lng = lng2 - lng1\n    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n    return h","72f8f605":"def width(x) : \n    return haversine_array(x['NORTH'],x['WEST'],x['NORTH'],x['EAST'])\ndef length(x) : \n    return haversine_array(x['NORTH'],x['EAST'],x['SOUTH'],x['EAST'])","a6df03fd":"tqdm.pandas()\ndata['length'] =  data[['WEST','EAST', 'SOUTH','NORTH']].progress_apply(length,axis=1)\ndata['width']  =  data[['WEST','EAST', 'SOUTH','NORTH']].progress_apply(width,axis=1)","6d192c1f":"data['area'] = data['length']*data['width']\ndata['reders_per_area'] = data['BUS_COUNT']\/data['area'] \ndata['READS_per_area'] = data['NUM_READS']\/data['area'] \ndata['BUS_ratio'] = data['BUS_COUNT']\/data['NUM_READS'] ","2e83df44":"categorical_features = ['REGION_ID','MONTH','HOUR','day','DAY_OF_WEEK','YEAR']\nNumerical_features = ['NUM_READS','BUS_COUNT','area','length','width','CENTER_LAT','CENTER_LON','reders_per_area','READS_per_area','BUS_ratio']","0de979d2":"for categorical_var in categorical_features:\n    \n    cat_emb_name= categorical_var.replace(\" \", \"\")+'_Embedding'\n  \n    no_of_unique_cat  = data[categorical_var].nunique()\n    embedding_size = int(min(np.ceil((no_of_unique_cat)\/2), 64))\n  \n    print('Categorica Variable:', categorical_var,\n        'Unique Categories:', no_of_unique_cat,\n        'Embedding Size:', embedding_size)","23789248":"def create_model(data, categorical_features ,  Numerical_features ):    \n    input_models=[]\n    output_embeddings=[]\n    for categorical_var in categorical_features  :\n        cat_emb_name= categorical_var.replace(\" \", \"\")+'_Embedding'\n        no_of_unique_cat  = data[categorical_var].nunique() +1\n        embedding_size = int(min(np.ceil((no_of_unique_cat)\/2), 24 ))\n        \n        input_model = layers.Input(shape=(1,),name=cat_emb_name)\n        output_model = layers.Embedding(no_of_unique_cat, embedding_size, name=cat_emb_name+'emblayer')(input_model)\n        output_model = layers.Reshape(target_shape=(embedding_size,))(output_model)    \n        input_models.append(input_model)\n        output_embeddings.append(output_model)\n\n    for c in Numerical_features :\n        num_name= c.replace(\" \", \"\")+'_num'\n        input_numeric = layers.Input(shape=(1,),name= num_name)\n        embedding_numeric = layers.Dense(16, kernel_initializer=\"uniform\")(input_numeric) \n        input_models.append(input_numeric)\n        output_embeddings.append(embedding_numeric)\n\n  \n\n    #At the end we concatenate altogther and add other Dense layers\n    output = layers.Concatenate()(output_embeddings)\n\n    output = layers.Dense(1024, kernel_initializer=\"uniform\")(output)\n    output = layers.Activation('relu')(output)\n    output= layers.Dropout(0.5)(output)\n    output = layers.Dense(512, kernel_initializer=\"uniform\")(output)\n    output = layers.Activation('relu')(output)\n    output= layers.Dropout(0.3)(output)\n    output = layers.Dense(256, kernel_initializer=\"uniform\")(output)\n    output = layers.Activation('relu')(output)\n    output= layers.Dropout(0.1)(output)\n    output = layers.Dense(1)(output)\n\n    model = Model(inputs=input_models, outputs=output)\n    return model ","555967d8":"model = create_model(data , categorical_features ,Numerical_features)\nmodel.summary()","5066c6d6":"from sklearn.preprocessing import StandardScaler\nfor num in Numerical_features :\n    scalar=StandardScaler()\n    scalar.fit(data[num].values.reshape(-1, 1))\n    data[num]=scalar.transform(data[num].values.reshape(-1, 1)) \n    data[num]=scalar.transform(data[num].values.reshape(-1, 1)) ","7b781cab":"#converting data to list format to match the network structure\ndef preproc(X_train, X_val, X_test):\n\n    input_list_train = dict()\n    input_list_val = dict()\n    input_list_test = dict()\n    \n    #the cols to be embedded: rescaling to range [0, # values)\n    for c in categorical_features :\n        cat_emb_name= c.replace(\" \", \"\")+'_Embedding'\n        raw_vals = X_train[c].unique()\n        val_map = {}\n        for i in range(len(raw_vals)):\n            val_map[raw_vals[i]] = i       \n        input_list_train[cat_emb_name]=X_train[c].map(val_map).values\n        input_list_val[cat_emb_name]=X_val[c].map(val_map).fillna(0).values\n        input_list_test[cat_emb_name]=X_test[c].map(val_map).fillna(0).values\n    for c in Numerical_features :\n        num_name= c.replace(\" \", \"\")+'_num'\n               \n        input_list_train[num_name]=X_train[c].values\n        input_list_val[num_name]=X_val[c].values\n        input_list_test[num_name]=X_test[c].values\n\n    \n    return input_list_train, input_list_val, input_list_test","bf47c134":"X_train , X_test  = model_selection.train_test_split(data , test_size = 0.1 , random_state=44 , shuffle =True)\nX_train , X_vaild = model_selection.train_test_split(X_train , test_size = 0.2 , random_state=44 , shuffle =True)","08cc4250":"y_train,y_valid,y_test = X_train.SPEED,X_vaild.SPEED,X_test.SPEED\nX_train,X_vaild,X_test = preproc(X_train,X_vaild,X_test)","24bee906":"gpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n  # Restrict TensorFlow to only use the first GPU\n  try:\n    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPU\")\n  except RuntimeError as e:\n    # Visible devices must be set before GPUs have been initialized\n    print(e)","f9205d2d":"EPOCHS = 35\nBATCH_SIZE =1048\nAUTO = tf.data.experimental.AUTOTUNE\n","144d5176":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_train, y_train))\n    .repeat() \n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((X_vaild, y_valid))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(AUTO)\n)","10230030":"start_time = time.time()\nmodel = create_model(data , categorical_features ,Numerical_features)\nes = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001,\n                                 verbose=5, baseline=None, restore_best_weights=True)\nrlr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5,\n                                      patience=3, min_lr=1e-6, mode='max', verbose=1)\nmodel.compile(optimizer = Adam(lr=5e-5), loss = 'mean_squared_error', metrics =[tf.keras.metrics.RootMeanSquaredError()])\nn_steps = sum( [x.shape[0] for x in X_train.values()] ) \/\/ BATCH_SIZE\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    validation_data=valid_dataset,\n    epochs=EPOCHS\n)\n\n\n              ","4609acac":"print(\"total training time is %s Minute \" %((time.time() - start_time)\/60))\nprint(\"hardware : NVidia K80 GPUs \")","4aec6b50":"\n# summarize history for accuracy\nplt.plot(train_history.history['loss'])\nplt.plot(train_history.history['val_loss'])\nplt.title('loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper right')\nplt.show()\n","1a3a702f":"!mkdir infrastructure_project\nmodel.save_weights('infrastructure_project\/Model_1.ckpt')","bd9b3fb0":"def rmse(predictions, targets): \n    return sqrt(mean_squared_error(predictions, targets))","c875ff96":"valid_fold_preds = model.predict(X_test)\nprint('TEST RMSE = :' , rmse(y_test.values, valid_fold_preds  ))","ac8e5b13":"def plot(region , year , month , day ) : \n    sub_plot = data[(data['REGION_ID']==region)&(data['YEAR']==year)&(data['MONTH']==month)& (data['day']<day)]\n    sub_plot = sub_plot.sort_values('Time')\n    y=  sub_plot.SPEED\n    X,_,_ = preproc(sub_plot,sub_plot,sub_plot)\n    predictions  = model.predict(X)\n    plt.figure(figsize=(24, 8))\n    plt.plot(sub_plot['Time'].values , sub_plot['SPEED'].values, '--',label = 'real values')\n    plt.plot(sub_plot['Time'].values , predictions,label = 'predicted values')\n    plt.ylabel(f'SPEED in region id {region}')\n    plt.title(f' predicted vs real values ')\n    plt.xlabel('Time')\n    plt.grid(True)\n    plt.legend()\n\n    plt.show()","842b8d08":"plot(1,2019,1,10)","a7d5baa3":"plot(5,2018,4,14)","957fdfc6":"# Analyzing categorical variable","f59bd489":"##  how does Entity Embeddings work ?\n\nWe map categorical variables in a function approximation problem into Euclidean spaces, which are the entity embeddings of the categorical variables. The mapping is learned by a neural network during the standard supervised training process. Entity embedding not only reduces memory usage and speeds up neural networks compared with one-hot encoding, but more importantly by mapping similar values close to each other in the embedding space it reveals the intrinsic properties of the categorical variables.","66f598d8":"\nOne need to convert data to list format to match the network structure\n\nThe following function takes the list of categorical  and numerical features , and prepare such lists for the NN input.","ca8fe5cd":"## The Network Architecture","c587f58e":"## Model evalutation \n","cf415f2f":"##  Embedding Dimension - Hyperparamter ","d6bdfbc4":"![image.png](attachment:image.png)","e0dc4ecc":"# Importations ","ca80e69d":"The haversine formula determines the great-cercle distance between two on a sphere given their longitudes an laitudes .","aec57639":"# create a model","17551981":"### Using Haversine Distance to calculate the region area ","aa6dc80c":"# Model Training ","528329a9":"# Loading Data : ","470ff6cb":"# Handling Longitude Latitude features ","c3c7dd8c":"Categorical variable are known to hide and mask lots of intersting information in a data set and in our poroblem they are the most important variables. so handiling them effectively and efficiently can help us to make a good model . so let's start analyzing this variables ","82da4087":"# Data preprocessing "}}