{"cell_type":{"abd57217":"code","9fb818f0":"code","fd86dd67":"code","b6c4bb5d":"code","0f0bb081":"code","6797e6f6":"code","1756ec2d":"code","83fd521f":"code","df9a7085":"code","bd025213":"code","20aa77d9":"code","ead2d39b":"code","1f00b4a6":"code","b6c3c307":"code","7d6cf2fb":"code","766288ee":"code","32c36397":"code","c5144e27":"code","60474f4c":"code","079571a5":"code","b408fefc":"code","f92077bb":"code","7a9771e4":"code","f7c21b4b":"code","e21666c4":"code","e8a5d257":"code","e01a3b77":"code","2475c9cd":"code","69f8bb25":"code","9462d002":"code","4a1fc3cc":"markdown","02e7654e":"markdown","ed172b03":"markdown","6bbe6190":"markdown"},"source":{"abd57217":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport h2o\nimport seaborn as sns\nimport json\n%matplotlib inline\nfrom pandas.io.json import json_normalize","9fb818f0":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, KFold","fd86dd67":"json_cols = ['device', 'geoNetwork', 'totals', 'trafficSource']","b6c4bb5d":"def load_df(filename):\n    path = \"..\/input\/\" + filename\n    df = pd.read_csv(path, converters={column: json.loads for column in json_cols}, \n                     dtype={'fullVisitorId': 'str'})\n    \n    for column in json_cols:\n        column_as_df = json_normalize(df[column])\n        column_as_df.columns = [f\"{column}.{subcolumn}\" for subcolumn in column_as_df.columns]\n        df = df.drop(column, axis=1).merge(column_as_df, right_index=True, left_index=True)\n    return df","0f0bb081":"%%time\ntrain_df = load_df('train.csv')\ntest_df = load_df('test.csv')\ntest_ind = test_df['fullVisitorId'].copy()","6797e6f6":"train_df.head()","1756ec2d":"target = train_df['totals.transactionRevenue'].fillna(0).astype(float)\ntarget = target.apply(lambda x: np.log(x) if x > 0 else x)\ndel train_df['totals.transactionRevenue']","83fd521f":"columns = [col for col in train_df.columns if train_df[col].nunique() > 1]","df9a7085":"train_df = train_df[columns].copy()\ntest_df = test_df[columns].copy()","bd025213":"# some data processing\ntrain_df['date'] = pd.to_datetime(train_df['date'].apply(lambda x: str(x)[:4] + '-' + str(x)[4:6] + '-' + str(x)[6:]))\ntest_df['date'] = pd.to_datetime(test_df['date'].apply(lambda x: str(x)[:4] + '-' + str(x)[4:6] + '-' + str(x)[6:]))","20aa77d9":"useless_columns = ['sessionId', 'visitId']\ntrain_df = train_df.drop(useless_columns, axis = 1)\ntest_df = test_df.drop(useless_columns, axis = 1)","ead2d39b":"for col in ['visitNumber', 'totals.hits', 'totals.pageviews']:\n    train_df[col] = train_df[col].astype(float)\n    test_df[col] = test_df[col].astype(float)","1f00b4a6":"sns.distplot(target[target!=0])","b6c3c307":"def feature_design(df):\n    df['month'] = df['date'].dt.month\n    df['day'] = df['date'].dt.day\n    df['weekday'] = df['date'].dt.weekday\n    df['weekofyear'] = df['date'].dt.weekofyear\n    \n    df['month_unique_user_count'] = df.groupby('month')['fullVisitorId'].transform('nunique')\n    df['day_unique_user_count'] = df.groupby('day')['fullVisitorId'].transform('nunique')\n    df['weekday_unique_user_count'] = df.groupby('weekday')['fullVisitorId'].transform('nunique')\n    df['weekofyear_unique_user_count'] = df.groupby('weekofyear')['fullVisitorId'].transform('nunique')\n    \n    df['browser_category'] = df['device.browser'] + '_' + df['device.deviceCategory']\n    df['browser_operatingSystem'] = df['device.browser'] + '_' + df['device.operatingSystem']\n    df['source_country'] = df['trafficSource.source'] + '_' + df['geoNetwork.country']\n    \n    df['visitNumber'] = np.log1p(df['visitNumber'])\n    df['totals.hits'] = np.log1p(df['totals.hits'])\n    df['totals.pageviews'] = np.log1p(df['totals.pageviews'].fillna(0))\n    \n    df['sum_pageviews_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('sum')\n    df['count_pageviews_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('count')\n    df['mean_pageviews_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('mean')\n    df['sum_hits_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('sum')\n    df['count_hits_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('count')\n    df['mean_hits_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('mean')\n    \n    df['mean_hits_per_day'] = df.groupby(['day'])['totals.hits'].transform('mean')\n    df['sum_hits_per_day'] = df.groupby(['day'])['totals.hits'].transform('sum')\n\n    df['sum_pageviews_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('sum')\n    df['count_pageviews_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('count')\n    df['mean_pageviews_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.pageviews'].transform('mean')\n\n    df['sum_pageviews_per_region'] = df.groupby('geoNetwork.region')['totals.pageviews'].transform('sum')\n    df['count_pageviews_per_region'] = df.groupby('geoNetwork.region')['totals.pageviews'].transform('count')\n    df['mean_pageviews_per_region'] = df.groupby('geoNetwork.region')['totals.pageviews'].transform('mean')\n\n    df['sum_hits_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('sum')\n    df['count_hits_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('count')\n    df['mean_hits_per_network_domain'] = df.groupby('geoNetwork.networkDomain')['totals.hits'].transform('mean')\n\n    df['sum_hits_per_region'] = df.groupby('geoNetwork.region')['totals.hits'].transform('sum')\n    df['count_hits_per_region'] = df.groupby('geoNetwork.region')['totals.hits'].transform('count')\n    df['mean_hits_per_region'] = df.groupby('geoNetwork.region')['totals.hits'].transform('mean')\n\n    df['sum_hits_per_country'] = df.groupby('geoNetwork.country')['totals.hits'].transform('sum')\n    df['count_hits_per_country'] = df.groupby('geoNetwork.country')['totals.hits'].transform('count')\n    df['mean_hits_per_country'] = df.groupby('geoNetwork.country')['totals.hits'].transform('mean')\n\n    df['user_pageviews_sum'] = df.groupby('fullVisitorId')['totals.pageviews'].transform('sum')\n    df['user_hits_sum'] = df.groupby('fullVisitorId')['totals.hits'].transform('sum')\n\n    df['user_pageviews_count'] = df.groupby('fullVisitorId')['totals.pageviews'].transform('count')\n    df['user_hits_count'] = df.groupby('fullVisitorId')['totals.hits'].transform('count')\n\n    df['user_pageviews_sum_to_mean'] = df['user_pageviews_sum'] \/ df['user_pageviews_sum'].mean()\n    df['user_hits_sum_to_mean'] = df['user_hits_sum'] \/ df['user_hits_sum'].mean()\n\n    df['user_pageviews_to_region'] = df['user_pageviews_sum'] \/ df['mean_pageviews_per_region']\n    df['user_hits_to_region'] = df['user_hits_sum'] \/ df['mean_hits_per_region']\n    \n    return df","7d6cf2fb":"%%time\ntrain_df =  feature_design(train_df)","766288ee":"%%time\ntest_df =  feature_design(test_df)","32c36397":"train_df.head(10)","c5144e27":"train_df = train_df.drop(['date', 'visitStartTime', 'fullVisitorId'], axis = 1)\ntest_df = test_df.drop(['date', 'visitStartTime', 'fullVisitorId'], axis = 1)","60474f4c":"cat_cols = train_df.select_dtypes(exclude=['float64', 'int64']).columns","079571a5":"for col in cat_cols:\n    lbl = LabelEncoder()\n    lbl.fit(list(train_df[col].values.astype('str')) + list(test_df[col].values.astype('str')))\n    train_df[col] = lbl.transform(list(train_df[col].values.astype('str')))\n    test_df[col] = lbl.transform(list(test_df[col].values.astype('str')))","b408fefc":"train_df.head()","f92077bb":"import lightgbm\nfrom bayes_opt import BayesianOptimization","7a9771e4":"def rmsle(y, pred):\n    assert len(y) == len(pred)\n    return np.sqrt(np.mean(np.power(y-pred, 2)))","f7c21b4b":"def lgb_eval(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples,bagging_fraction,feature_fraction):\n    params = {\n    \"objective\" : \"regression\",\n    \"metric\" : \"rmse\", \n    \"num_leaves\" : int(num_leaves),\n    \"max_depth\" : int(max_depth),\n    \"lambda_l2\" : lambda_l2,\n    \"lambda_l1\" : lambda_l1,\n    \"num_threads\" : 4,\n    \"min_child_samples\" : int(min_child_samples),\n    \"learning_rate\" : 0.03,\n    \"bagging_fraction\" : bagging_fraction,\n    \"feature_fraction\" : feature_fraction,\n    \"subsample_freq\" : 5,\n    \"bagging_seed\" : 42,\n    \"verbosity\" : -1\n    }\n    lgtrain = lightgbm.Dataset(train_df, target,categorical_feature=categorical_features)\n    cv_result = lightgbm.cv(params,\n                       lgtrain,\n                       10000,\n                       categorical_feature=categorical_features,\n                       early_stopping_rounds=100,\n                       stratified=False,\n                       nfold=5)\n    return -cv_result['rmse-mean'][-1]\n\ndef lgb_train(num_leaves,max_depth,lambda_l2,lambda_l1,min_child_samples,bagging_fraction,feature_fraction):\n    params = {\n    \"objective\" : \"regression\",\n    \"metric\" : \"rmse\", \n    \"num_leaves\" : int(num_leaves),\n    \"max_depth\" : int(max_depth),\n    \"lambda_l2\" : lambda_l2,\n    \"lambda_l1\" : lambda_l1,\n    \"num_threads\" : 4,\n    \"min_child_samples\" : int(min_child_samples),\n    \"learning_rate\" : 0.01,\n    \"bagging_fraction\" : bagging_fraction,\n    \"feature_fraction\" : feature_fraction,\n    \"subsample_freq\" : 5,\n    \"bagging_seed\" : 42,\n    \"verbosity\" : -1\n    }\n    t_x,v_x,t_y,v_y = train_test_split(train_df, target,test_size=0.2)\n    lgtrain = lightgbm.Dataset(t_x, t_y,categorical_feature=categorical_features)\n    lgvalid = lightgbm.Dataset(v_x, v_y,categorical_feature=categorical_features)\n    model = lightgbm.train(params, lgtrain, 2000, valid_sets=[lgvalid], early_stopping_rounds=100, verbose_eval=100)\n    pred_test_y = model.predict(test_df, num_iteration=model.best_iteration)\n    return pred_test_y, model\n    \ndef param_tuning(init_points,num_iter,**args):\n    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (25, 50),\n                                                'max_depth': (5, 15),\n                                                'lambda_l2': (0.0, 0.05),\n                                                'lambda_l1': (0.0, 0.05),\n                                                'bagging_fraction': (0.5, 0.8),\n                                                'feature_fraction': (0.5, 0.8),\n                                                'min_child_samples': (20, 50),\n                                                })\n\n    lgbBO.maximize(init_points=init_points, n_iter=num_iter,**args)\n    return lgbBO","e21666c4":"categorical_features = list(cat_cols)\nresult = param_tuning(5,20)","e8a5d257":"params = result.res['max']['max_params']\nparams","e01a3b77":"categorical_features = list(cat_cols)\nprediction1,model1 = lgb_train(**params)\nprediction2,model2 = lgb_train(**params)\nprediction3,model3 = lgb_train(**params)","2475c9cd":"prediction_lgb = (np.expm1(prediction1)+np.expm1(prediction2)+np.expm1(prediction3))\/3\nprediction_lgb = [0 if x < 0 else x for x in prediction_lgb]","69f8bb25":"submission = pd.DataFrame()\nsubmission['fullVisitorId'] = test_ind\nsubmission['PredictedLogRevenue'] = prediction_lgb\nsubmission = submission.groupby('fullVisitorId').sum()['PredictedLogRevenue'].apply(np.log1p).fillna(0).reset_index()","9462d002":"submission.to_csv('submit_lgbm.csv', index=False)","4a1fc3cc":"## Let's start - LGBM\nFrom this kernel: https:\/\/www.kaggle.com\/qwe1398775315\/eda-lgbm-bayesianoptimization","02e7654e":"**Revenue**","ed172b03":"Categorical columns","6bbe6190":"Thanks to this kernels!<br>\nhttps:\/\/www.kaggle.com\/ashishpatel26\/1-67-pb-first-try-to-think<br>\nFrom this kernel: https:\/\/www.kaggle.com\/qwe1398775315\/eda-lgbm-bayesianoptimization<br>\nhttps:\/\/www.kaggle.com\/artgor\/eda-on-basic-data-and-lgb-in-progress"}}