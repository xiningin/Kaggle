{"cell_type":{"382672ea":"code","f12af84d":"code","9f9590b8":"code","3fb00ac4":"code","147bbdd1":"code","bd640c37":"code","56aabed0":"code","1bbc60d9":"code","1dad83c2":"code","8f804a0f":"code","956df463":"code","f3057833":"code","f4d0c476":"code","eacfe180":"code","fc20483f":"code","12139376":"markdown","ddc27464":"markdown","2e7abb61":"markdown","78c7fb0f":"markdown","8a39abd8":"markdown","e365ecc6":"markdown","0846fd13":"markdown","fd0a46cb":"markdown","c1a8e4ea":"markdown","943b7655":"markdown","f26534d0":"markdown","af1f98d6":"markdown","455cd67d":"markdown","6cda68a0":"markdown","c72551bb":"markdown"},"source":{"382672ea":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport warnings\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom plotly.tools import FigureFactory as ff\nfrom time import time\nfrom IPython.display import display\n\nwarnings.filterwarnings('ignore')","f12af84d":"data = pd.read_csv('..\/input\/train.csv')","9f9590b8":"data.head()","3fb00ac4":"data = data.drop(columns=['Name','Ticket','Cabin'])\nlen(data)","147bbdd1":"data = data.dropna()\nlen(data)","bd640c37":"def entropy(data, y_col):\n    # Get all the values for the Y column\n    y_val = data[y_col].value_counts().index.values\n    # Get the vector with the number of element for each Y class\n    tmp = np.array([len(data[data[y_col] == y_val[i]]) for i in range(0, len(y_val))])\n    return sum(-tmp \/ len(data) * np.log2(tmp \/ len(data)))","56aabed0":"def gini_impurity(data, y_col):\n    # Get all the values for the Y column\n    y_val = data[y_col].value_counts().index.values\n    # Get the vector with the number of element for each Y class\n    tmp = np.array([len(data[data[y_col] == y_val[i]]) for i in range(0, len(y_val))])\n    return 1 - sum((tmp \/ len(data))**2)","1bbc60d9":"test = pd.DataFrame(data=[['A',1],['A',1],['A',1],['B',1],['B',0],['B',0]]\n                    , columns=['letter','bit'])\ntest","1dad83c2":"display(test[test['letter'] == 'A'])\ndisplay(test[test['letter'] == 'B'])","8f804a0f":"def info_gain(data, feature_col, y_col, criterion='entropy'):\n    # Get all the values for this feature\n    feature_val = data[feature_col].value_counts().index.values\n    # Get the vector of the number of element for each class\n    len_feat = np.array([len(data[data[feature_col] == feature_val[i]]) for i in range(0, len(feature_val))])\n    # Get the vector of the criterion for each class\n    if criterion == 'entropy':\n        crit_feat = np.array([entropy(data[data[feature_col] == feature_val[i]], y_col) for i in range(0, len(feature_val))])\n        gain = entropy(data, y_col) - sum((len_feat \/ len(data)) * crit_feat)\n    elif criterion == 'gini':\n        crit_feat = np.array([gini_impurity(data[data[feature_col] == feature_val[i]], y_col) for i in range(0, len(feature_val))])\n        gain = gini_impurity(data, y_col) - sum((len_feat \/ len(data)) * crit_feat)\n    return gain","956df463":"class DecisionTree:\n    def __init__(self, data, y_col, cat_cols=[], cont_cols=[], criterion='entropy', max_depth=5):\n        self.data = data\n        self.y_col = y_col\n        self.cat_cols = cat_cols\n        self.cont_cols = cont_cols\n        self.criterion = criterion\n        self.leaves = list()\n        self.max_depth = max_depth if len(cat_cols) > max_depth or len(cont_cols) > 0 else len(cat_cols)\n        \n    def get_best_split_continuous(self, feature_col, data):\n        # Init best gain and best split\n        best_gain, best_split = -1, -1\n        # Get all the values for this feature\n        feat_val = data[feature_col].drop_duplicates().sort_values().reset_index(drop=True).dropna()\n        # Get the information gain for each feature and keep the best\n        for i in range(1, len(feat_val)):\n            split = (feat_val[i - 1] + feat_val[i]) \/ 2\n            data[feature_col + '_tmp'] = data[feature_col] <= split\n            gain = info_gain(data, feature_col + '_tmp', self.y_col, criterion=self.criterion)\n            best_gain, best_split = (gain, split) if best_gain < gain else (best_gain, best_split)\n        return best_split, best_gain\n    \n    def get_best_feat_leaf(self, data, leaf=None):\n        cat_cols = [c for c in self.cat_cols if c not in leaf.get_feat_parent()] if leaf is not None else self.cat_cols\n        all_gains = [info_gain(data, c, self.y_col, criterion=self.criterion) for c in cat_cols]\n        continuous = [(c, self.get_best_split_continuous(c, data)) for c in self.cont_cols]\n        cont_gains = [c[1][1] for c in continuous]\n\n        all_gains = all_gains + cont_gains if len(continuous) > 0 and len(all_gains) > 0 else all_gains if len(\n            all_gains) > 0 else cont_gains\n        all_cols = cat_cols + self.cont_cols if len(cat_cols) > 0 and len(self.cont_cols) > 0 else cat_cols if len(\n            cat_cols) > 0 else cont_cols\n        \n        best_feat = pd.Series(data=all_gains, index=all_cols).idxmax()\n        \n        return best_feat if best_feat not in cont_cols else [c for c in continuous if c[0] == best_feat][0]\n        \n    def learn(self):\n        t0 = time()\n        print('----- START LEARNING -----')\n        # Get the first feature where to split\n        feat = self.get_best_feat_leaf(self.data)\n        split = None\n        \n        # If the type is not a string then it's a continuous feature \n        # and we get the best value to split\n        if (type(feat) != type(str())):\n            split = feat[1][0]\n            feat = feat[0]    \n        \n        # Add it to the Tree\n        self.leaves.append(Leaf(None\n                                , None\n                                , self.data\n                                , feat\n                                , self.data[self.y_col]\n                                , split))\n        \n        for i in range (1, self.max_depth):\n            print('----- BEGIN DEPTH '+str(i)+' at %0.4f s -----' % (time() - t0))\n            # Get all the leaves that are in the upper depth\n            leaves_parent = [l for l in self.leaves if l.depth == i-1]\n            \n            # If there is 0 parent we can stop the learning algorithm\n            if(len(leaves_parent) == 0):\n                break\n            else:\n                for leaf in leaves_parent:\n                    # If there is only one value that means it's useless to split\n                    # because we already have our prediction\n                    if(len(leaf.values) == 1):\n                        continue\n                    # Get all values for the current feature\n                    feature_val = leaf.data[leaf.feature] <= leaf.split if leaf.split is not None else leaf.data[leaf.feature]\n                    feature_val = feature_val.value_counts().index.values\n                    \n                    # Add all possibilities to the Tree\n                    for k in range(0, len(feature_val)): \n                        if leaf.split is None:\n                            data = leaf.data[leaf.data[leaf.feature] == feature_val[k]]\n                        else:\n                            split_cond = leaf.data[leaf.feature] <= leaf.split\n                            data = leaf.data[split_cond == feature_val[k]]\n                        \n                        if len(data) > 0:\n                            # Get the best feature for the split\n                            next_feat = self.get_best_feat_leaf(data, leaf)\n\n                            split = None\n\n                            # If the type is not a string then it's a continuous feature \n                            # and we get the best value to split\n                            if (type(next_feat) != type(str())):\n                                split = next_feat[1][0]\n                                next_feat = next_feat[0]\n                            \n                            self.leaves.append(Leaf(prev_leaf=leaf\n                                                , condition=feature_val[k]\n                                                , data=data\n                                                , feature=next_feat\n                                                , values=data[self.y_col]\n                                                , split=split))\n        print('Number of leaves : '+str(len(self.leaves)))\n        print('----- END LEARNING : %0.4f s-----' % (time() - t0))\n        print()\n        \n    def display_final_leaves(self):\n        leaves = [l for l in self.leaves if len(l.values) == 1 or l.depth == self.max_depth]\n        for l in leaves:\n            l.display()\n                        \n    def predict(self, data):\n        pred = list()\n        for i in range(0, len(data)):\n            row = data.iloc[i,:]\n            leaf = self.leaves[0]\n            while(len(leaf.values) > 1 and leaf.depth < self.max_depth):\n                if leaf.split is None:\n                    tmp_leaf = [l for l in self.leaves if (l.prev_leaf == leaf and l.condition == row[leaf.feature])]\n                else:\n                    tmp_leaf = [l for l in self.leaves if (l.prev_leaf == leaf and l.condition == (row[leaf.feature] <= leaf.split))]\n                if (len(tmp_leaf) > 0):\n                    leaf = tmp_leaf[0]                    \n                else:\n                    break\n            pred.append(leaf.pred_class)\n        return pred\n    \nclass Leaf:\n    def __init__(self, prev_leaf, condition, data, feature, values, split=None):\n        self.prev_leaf = prev_leaf\n        self.depth = 0 if prev_leaf is None else prev_leaf.depth+1\n        self.condition = condition\n        self.data = data\n        self.feature = feature\n        self.values = values.value_counts(sort=False)\n        self.pred_class = self.set_predict_class()\n        self.split = split\n        \n    def set_predict_class(self):\n        return self.values.idxmax()\n    \n    def get_feat_parent(self):\n        cols = [self.feature]\n        leaf = self\n        while(leaf.prev_leaf is not None):\n            cols.append(leaf.prev_leaf.feature)\n            leaf = leaf.prev_leaf\n        return cols\n\n    def display(self):\n        cond = ''\n        leaf = self\n        while(leaf.prev_leaf is not None):\n            if leaf.prev_leaf.split is None:\n                cond = str(leaf.prev_leaf.feature)+' : '+str(leaf.condition)+' --> '+cond\n            else:\n                cond = str(leaf.prev_leaf.feature)+' <= '+str(round(leaf.prev_leaf.split,2))+' : '+str(leaf.condition)+' --> '+cond\n            leaf = leaf.prev_leaf\n        print(cond+' prediction : '+str(self.pred_class))","f3057833":"cat_cols = ['Sex', 'Embarked', 'SibSp', 'Parch']\ncont_cols = ['Age', 'Fare']\n\ntree_gini = DecisionTree(data, 'Survived', cat_cols=cat_cols, cont_cols=cont_cols, criterion='gini', max_depth=5)\ntree_entr = DecisionTree(data, 'Survived', cat_cols=cat_cols, cont_cols=cont_cols, criterion='entropy', max_depth=5)\ntree_gini.learn()\ntree_entr.learn()","f4d0c476":"print('----- GINI TREE -----')\ntree_gini.display_final_leaves()\nprint()\nprint('----- ENTROPY TREE -----')\ntree_entr.display_final_leaves()","eacfe180":"def plot_confusion_matrix(y_true, y_pred, name):\n    trace = go.Heatmap(z=confusion_matrix(y_true, y_pred),\n                       x=['Died', 'Survived'],\n                       y=['Died', 'Survived'],\n                       colorscale='Reds')\n    \n    layout = go.Layout(title='Confusion Matrix '+name,\n                            xaxis=dict(\n                                title='Prediction'\n                            ),\n                            yaxis=dict(\n                                title='Real'\n                            )\n                        )\n    fig = go.Figure(data=[trace], layout=layout)\n    \n    py.iplot(fig)","fc20483f":"from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\npred_gini = tree_gini.predict(data)\npred_entr = tree_entr.predict(data)\n\nprint('----- GINI TREE -----')\nprint('F1 Score : '+str(f1_score(data['Survived'], pred_gini)))\nprint('Accuracy : '+str(accuracy_score(data['Survived'], pred_gini)))\nprint('----- ENTROPY TREE -----')\nprint('F1 Score : '+str(f1_score(data['Survived'], pred_entr)))\nprint('Accuracy : '+str(accuracy_score(data['Survived'], pred_entr)))\n\nplot_confusion_matrix(data['Survived'], pred_gini, 'Gini impurity')\nplot_confusion_matrix(data['Survived'], pred_entr, 'Entropy')","12139376":"## <a id=\"2\"> 2. Decision Tree <\/a>\n\nNow let's dive into the math that make the **Decision Tree model** !\n\nTo be honest at the beginning I had some difficulties to really understand how it works because it's really different from linear regression for example. Nevertheless this model has a big **advantage** it's really easy for a human to understand how it gets the predicted result. So to start I search \"Decision Tree\" on Google Image and just took a look on the image to understand. \n\nHere is an example of a Tree :\n\n![](https:\/\/annalyzin.files.wordpress.com\/2016\/07\/decision-trees-titanic-tutorial.png)\n\nSo now the question is : **How to know where to split ?** \n\nOn many ressources that I checked they just choose a feature and a value just by visualizing on a plot. But it's not the math that automate the model ! And then I find different notions that define the model : \n* Entropy \n* Gini impurity\n* Information gain\n\nAnd just with that we can build our model !! First let's define those terms.\n\n### <a id=\"2.1\">Entropy<\/a>\n\nThe entropy is define by this formula : \n$$ H(T) = I_E(p_1,p_2, ... , p_J) = - \\sum_{i=1}^J p_i log_2 p_i$$\n\nwhere $p_1, p_2, ...$ are fractions that add up to 1 and represent the percentage of each class present in the child node (node for a specific value of the feature). And $J$ is the number of classes for this feature.\n\nLet's take an example : we have 10 rows of a feature $X$. Here is the data : $X = [A, A, A, A, B, B, B, C, C, C]$\n\nTo obtain $p$ for each class we just count the number of value and divise it by the number of elements in $X$ :\n\n$$p_A = \\frac{(number\\space of\\space A)}{(length\\space of\\space X)} = \\frac{4}{10} = 0.4 $$\n\n$$p_B = \\frac{(number\\space of\\space B)}{(length\\space of\\space X)} = \\frac{3}{10} = 0.3 $$\n\n$$p_C = \\frac{(number\\space of\\space C)}{(length\\space of\\space X)} = \\frac{3}{10} = 0.3 $$\n\nAnd then we can calculate the entropy for this feature :\n\n$= -(p_A log_2 p_A + p_B log_2 p_B + p_C log_2 p_C )\\\\\n= -(0.4 \\times (-1.3219) + 0.3 \\times  (-1.737) + 0.3 \\times  (-1.737) )\\\\\n=-( -0.529 -0.5211 -0.5211 )\\\\\n= 1.571$\n\nLet's code it !","ddc27464":"\n\nSo the main function of the Decision tree is the *learn* function that execute the building algorithm. I also added a function that display all the path that give us a prediction.\n","2e7abb61":"# The mathematics of Decision Tree\n*November 2018*\n\n***\n\nHello world !\n\nSince last month I began a series of Kernel that aim to understand the math behind the models in Machine Learning. \n\nIf you want more informations I invite you to take a look on this topic : [Understand the math behind models](https:\/\/www.kaggle.com\/general\/69885)\n\nToday, it's the turn of the **Decision Tree**.\n\nThe goals of this kernel for me is :\n* to learn how to write a model in python just with numpy linear algebra library\n* to improve my writing skills (in English and with Markdown)\n* to have a better sense of the notebooks\n\nI hope you will enjoy it and if you have any suggestions : I will be glad to hear them to improve my skills !\n\n![](https:\/\/www.sciencemag.org\/sites\/default\/files\/styles\/article_main_large\/public\/cc_iStock-478639870_16x9.jpg?itok=1-jMc4Xv)\n\n## Table of content\n\n* [1. Load libraries and read data](#1)\n    * [1.1 Load libraries](#1.1)\n    * [1.2 Read the data](#1.2)\n    * [1.3 A quick look on the data](#1.3)\n    \n* [2. Decision Tree](#2)\n    * [2.1 Entropy](#2.1)\n    * [2.2 Gini impurity](#2.2)\n    * [2.3 Information gain](#2.3)\n    * [2.4 Decision Tree building - algorithm](#2.4)\n    * [2.5 F1 Score \/ Accuracy](#2.5)\n    \n## <a id=\"1\">1. Load libraries and read data<\/a>\n\n### <a id=\"1.1\">1.1 Load libraries<\/a>","78c7fb0f":"### <a id=\"1.3\">1.3 A quick look on the data<\/a>","8a39abd8":"So I hope that you enjoyed this kernel, please feel free to say what you though about my work !\n\nThanks for reading,\n\n$Nathan.$\n\nSources :\n* [Decision Trees in Machine Learning](https:\/\/towardsdatascience.com\/decision-trees-in-machine-learning-641b9c4e8052)\n* [Wikipedia : Decision Tree learning](https:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning)\n* [Introduction to Decision Tree Learning](https:\/\/heartbeat.fritz.ai\/introduction-to-decision-tree-learning-cd604f85e236)\n* [Coursera : Picking the best threshold to split on](https:\/\/www.coursera.org\/lecture\/ml-classification\/optional-picking-the-best-threshold-to-split-on-sKrGp)","e365ecc6":"### <a id=\"2.4\">2.4 Decision Tree building - algorithm<\/a>\n\nHere is the most difficult part for the Decision tree : build the algorithm to predict the output. \n\nTo start this we need to go step by step : first the question we need to ask ourselves is **how to know where to split ? **\n\nWe have the information gain formula which gives us the ability to quantify the gain of a feature. Now the logic is just to compare the gain of each feature and then keep the best ! It's easy on the paper but for categorical feature because we have a limited number of classes. For example for the *Sex* feature it's just *male* and *female*. When it's a continuous feature it's really different for juste a dataset with 1 000 rows we can have like 800 differents values so if we split like it's a categorical value the model is overfitting and it's not really usefull.. \n\nSo how can we deal with continuous feature ? The answer is just to transform it in a categorical value more precisely into a boolean feature (True or False) we can take an example : the *Fare* feature we can choose to split it at the value 50 --> *Fare <= 50*\n\nWe still have a problem for continuous feature : how to choose where to split this feature ? I used an algorithm for that, it has a advantage : you are sure to take the best split and a BIG inconvinient : more data you have slower is your model. For this kernel I will use it because my goal is to understand the model not necessary to optimize it (but if you have suggestion about it do not hesitate).\n\nThe algorithm is simple :\n\n**Step 1 :** Sort the values of the feature : $[v_1, v_2, ..., v_n]$\n\n**Step 2 :** For i = 1 ... n- 1 : get the entropy \/ gini impurity for a new feature that is : \n$$ data <= \\frac{(v_i + v_{i+1})}{2} $$\nThen we choose the best split.\n\n**Okay, now we have our best feature to split !**\n\nThen we create n leaves where n is the number of class of the feature after that we repeat the operation on the children leaves.\n\nHere is our algorithm to build the Decision Tree model ! It's Great now let's code it (I choose to do it with a DecisionTree class and Leaf class)\n\nTo start we need some informations :\n* The data\n* The name of the column that we want to predict\n* The name of all the categorical feature\n* The name of all the continuous feature\n* Which metric to evaluate our model : gini or entropy\n* The maximum depth of the tree","0846fd13":"   ### <a id=\"2.5\">2.5 F1 Score \/ Accuracy<\/a>\n   \n   Now that we have two trees one with entropy and one with gini impurity we can see if they have a great performance !","fd0a46cb":"### <a id=\"2.2\">2.2 Gini impurity<\/a>\n\nThis formula is very similar to the entropy formula :\n\n$$ H(T) =  I_G(p_1,p_2, ... , p_J) = 1 - \\sum_{i=1}^J p_i^2$$\n\nSo let's code it \ud83d\udc4d","c1a8e4ea":"Then since the goal of this kernel is to understand the math I don't handle NA values.","943b7655":"### <a id=\"1.2\">1.2 Read the data<\/a>","f26534d0":"So the entropy or gini impurity of the parent is based on all the rows of this dataset but if we want the children based on the feature *letter* here is what our children will be :","af1f98d6":"For this model I choose to handle 2 types of columns : categorical columns and continuous columns.\n\nIn this dataset, I prefer to remove 3 columns because I think it's Useless in this kernel to keep them : Name, Ticket and Cabin.","455cd67d":"I'm sorry that the visualisation of the tree is not really good but I didn't code something too complicated \ud83d\ude09","6cda68a0":"### <a id=\"2.3\">2.3 Information gain<\/a>\n\nThe information gain is really important because it allows us to get the gain for a specific feature on the split. For that it's necessary to introduce the notion of parent and children. The parent is where the data is full and children represent the data filtered by the value of the feature (if we sum the length of each children it gives us the length of the parent).\n\nLet's see an example to understand. Here is our dataset : ","c72551bb":"In general information gain is used with entropy but here I choosed to use it with both entropy and gini impurity.\n\nSo here is the formula :\n\n$$\n  \\overbrace{IG(T, a)}^\\text{Information Gain} = \\overbrace{H(T)}^\\text{Entropy \/ Gini (parent)} - \\overbrace{H(T\\space|\\space a)}^\\text{Weighted Sum of Entropy \/ Gini (Children)}\n $$\n \n So if we take the entropy metric we get this :\n $$\n IG(T, a) = Entropy(T) - \\sum_a{p(a) Entropy(T\\space | \\space a)}\n $$\n \n And now we can do it in python ! \ud83d\ude09"}}