{"cell_type":{"d1b24afa":"code","816f49a6":"code","afb82c90":"code","fdd5e47e":"code","57fb2fdf":"code","8bda40a0":"code","6514c0c2":"code","58c5e1b0":"code","a1218f8c":"code","3590ce66":"code","0c127e91":"code","b6dcf2e1":"code","4f6d1cce":"code","34de7504":"code","d4c62d6a":"code","d43648d8":"code","6b9885ee":"code","c6016768":"code","947e088d":"code","b5809ee9":"code","ed92668b":"code","43771dc1":"code","a1b12d72":"markdown","0920cace":"markdown","5a765c49":"markdown","decbc12a":"markdown","879eb76b":"markdown","67685cae":"markdown","9cbbb53b":"markdown","910c1e9a":"markdown","db070cf3":"markdown","aa8e09d5":"markdown","028e884c":"markdown","2a86a027":"markdown","2d0b5623":"markdown","73ed1a37":"markdown","cf553fd7":"markdown","dedf681e":"markdown","6d18fdd7":"markdown","8f85edbb":"markdown","acdf8ded":"markdown","2da160bd":"markdown","4e248c9a":"markdown","82703d5d":"markdown","b89b4cf4":"markdown"},"source":{"d1b24afa":"import pandas as pd\nfrom PIL import Image\nimport pydicom as dicom\nfrom skimage import exposure\nfrom skimage.transform import resize\nimport os\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nimport time\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import LeakyReLU\nimport seaborn as sns\nimport keras","816f49a6":"# This block of code will read the img and convert them to array of size of (250,250,3)\nstart_time = time.time()\ndata = []\nfor filename in os.listdir('..\/input\/weather-images\/dataset2'):\n    img = Image.open('..\/input\/weather-images\/dataset2\/' + filename)\n    img_array = np.array(img)\n    img_comp = resize(img_array,(250,250,3))\n    \n    class_1 = lambda class_dsc,class_id:('sunrise',0) if 'sunrise' in filename else (('shine',1) if 'shine' in filename else ('cloudy',2) if 'cloudy' in filename else ('rain',3))\n    \n    data.append({ \"img\":img_comp,\"class_des\":class_1(filename,filename)[0],\"class_id\":class_1(filename,filename)[1]})\n    \nimg_df = pd.DataFrame(data)\nprint('Excution time: ', time.time() - start_time)","afb82c90":"# ploting the numbers of images for each class\n#sns.set_theme(style=\"darkgrid\")\nax = sns.countplot(x=img_df.class_des, data=img_df)","fdd5e47e":"np.random.shuffle(data)\ndata","57fb2fdf":"for i in range(9):\n    ax = plt.subplot(3,3,i+1)\n    plt.imshow(data[i]['img'])\n    plt.axis(\"off\")\n    plt.title(data[i]['class_des'])","8bda40a0":"x_data = []\ny_data = []\nfor i,item in enumerate(data):\n    x_data.append(item['img'])\n    y_data.append(item['class_id'])\n    \nx_data = np.array(x_data)\ny_data = np.array(y_data)\nx_train = x_data[:900]\ny_train = y_data[:900]\nx_test = x_data[900:]\ny_test = y_data[900:]","6514c0c2":"num_classes = 4\n\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nprint(f'shape of x_train: {x_train.shape}')\nprint(f'shape of y_train: {y_train.shape}')\nprint(f'shape of x_test: {x_test.shape}')\nprint(f'shape of y_test: {y_test.shape}')","58c5e1b0":"x_train.shape[1:]","a1218f8c":"# Let's build a CNN using Keras' Sequential capabilities\n\nmodel_1 = Sequential()\n\n\n## 5x5 convolution with 2x2 stride and 250 filters\nmodel_1.add(Conv2D(250, (5, 5), strides = (2,2), padding='same',\n                 input_shape=x_train.shape[1:]))\nmodel_1.add(Activation('relu'))\n\n## Another 5x5 convolution with 2x2 stride and 250 filters\nmodel_1.add(Conv2D(250, (5, 5), strides = (2,2)))\nmodel_1.add(Activation('relu'))\n\n## 2x2 max pooling reduces to 3 x 3 x 250\nmodel_1.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_1.add(Dropout(0.25))\n\n## Flatten turns 3x3x250 into 225000X1\nmodel_1.add(Flatten())\nmodel_1.add(Dense(512))\nmodel_1.add(Activation('relu'))\nmodel_1.add(Dropout(0.5))\nmodel_1.add(Dense(num_classes))\nmodel_1.add(Activation('softmax'))\n\nmodel_1.summary()","3590ce66":"batch_size = 32\n\n# initiate RMSprop optimizer\nopt = keras.optimizers.RMSprop(lr=0.0005, decay=1e-6)\n\n# Let's train the model using RMSprop\nmodel_1.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n\nmodel1_history = model_1.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=10,\n              validation_data=(x_test, y_test),\n              shuffle=True)","0c127e91":"# adding one more layer (model_1 + one more Conv2D layer)\nmodel_2 = Sequential()\n\n\n## 5x5 convolution with 2x2 stride and 250 filters\nmodel_2.add(Conv2D(250, (5, 5), strides = (2,2), padding='same',\n                 input_shape=x_train.shape[1:]))\nmodel_2.add(Activation('relu'))\n\n## Another 5x5 convolution with 2x2 stride and 250 filters\nmodel_2.add(Conv2D(250, (5, 5), strides = (2,2)))\nmodel_2.add(Activation('relu'))\n\n## Another 5x5 convolution with 2x2 stride and 250 filters\nmodel_2.add(Conv2D(250, (5, 5), strides = (2,2)))\nmodel_2.add(Activation('relu'))\n\n## 2x2 max pooling reduces to 3 x 3 x 250\nmodel_2.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_2.add(Dropout(0.25))\n\n## Flatten turns 3x3x250 into 49000\nmodel_2.add(Flatten())\nmodel_2.add(Dense(512))\nmodel_2.add(Activation('relu'))\nmodel_2.add(Dropout(0.5))\nmodel_2.add(Dense(num_classes))\nmodel_2.add(Activation('softmax'))\n\nmodel_2.summary()","b6dcf2e1":"batch_size = 32\n\n# initiate RMSprop optimizer\nopt = keras.optimizers.RMSprop(lr=0.0005, decay=1e-6)\n\n# Let's train the model using RMSprop\nmodel_2.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n\nmodel2_hist = model_2.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=10,\n              validation_data=(x_test, y_test),\n              shuffle=True)","4f6d1cce":"# Changing the activation function in model_1 to leaky Relu\n\n\nmodel_3 = Sequential()\n\n\n## 5x5 convolution with 2x2 stride and 250 filters\nmodel_3.add(Conv2D(250, (5, 5), strides = (2,2), padding='same',\n                 input_shape=x_train.shape[1:]))\nmodel_3.add(LeakyReLU(alpha=0.1))\n\n\n## Another 5x5 convolution with 2x2 stride and 250 filters\nmodel_3.add(Conv2D(250, (5, 5), strides = (2,2)))\nmodel_3.add(LeakyReLU(alpha=0.1))\n\n\n\n## 2x2 max pooling reduces to 3 x 3 x 250\nmodel_3.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_3.add(Dropout(0.25))\n\n## Flatten turns 3x3x250 into 225000\nmodel_3.add(Flatten())\nmodel_3.add(Dense(512))\nmodel_3.add(LeakyReLU(alpha=0.1))\nmodel_3.add(Dropout(0.5))\nmodel_3.add(Dense(num_classes))\nmodel_3.add(Activation('softmax'))\n\nmodel_3.summary()","34de7504":"# Run it for 7 epochs\nbatch_size = 32\n\n# initiate RMSprop optimizer\nopt = keras.optimizers.RMSprop(lr=0.0005, decay=1e-6)\n\n# Let's train the model using RMSprop\nmodel_3.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n\nmodel3_hist = model_3.fit(x_train, y_train,\n              batch_size=batch_size,\n                          \n              epochs=7,\n              validation_data=(x_test, y_test),\n              shuffle=True)","d4c62d6a":"# Reducing the kernal size to (3,3)\nmodel_4 = Sequential()\n\n\n## 5x5 convolution with 2x2 stride and 250 filters\nmodel_4.add(Conv2D(250, (3, 3), strides = (2,2), padding='same',\n                 input_shape=x_train.shape[1:]))\nmodel_4.add(Activation('relu'))\n\n## Another 5x5 convolution with 2x2 stride and 250 filters\nmodel_4.add(Conv2D(250, (3, 3), strides = (2,2)))\nmodel_4.add(Activation('relu'))\n\n## 2x2 max pooling reduces to 3 x 3 x 250\nmodel_4.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_4.add(Dropout(0.25))\n\n## Flatten turns 3x3x250 into 240250X1\nmodel_4.add(Flatten())\nmodel_4.add(Dense(512))\nmodel_4.add(Activation('relu'))\nmodel_4.add(Dropout(0.5))\nmodel_4.add(Dense(num_classes))\nmodel_4.add(Activation('softmax'))\n\nmodel_4.summary()","d43648d8":"# Run it for 7 epochs\nbatch_size = 32\n\n# initiate RMSprop optimizer\nopt = keras.optimizers.RMSprop(lr=0.0005, decay=1e-6)\n\n# Let's train the model using RMSprop\nmodel_4.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n\nmodel4_history = model_4.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=10,\n              validation_data=(x_test, y_test),\n              shuffle=True)","6b9885ee":"# Using Adam optimizer instead of RMSprop\nmodel_5 = Sequential()\n\n\n## 5x5 convolution with 2x2 stride and 250 filters\nmodel_5.add(Conv2D(250, (3, 3), strides = (2,2), padding='same',\n                 input_shape=x_train.shape[1:]))\nmodel_5.add(Activation('relu'))\n\n## Another 5x5 convolution with 2x2 stride and 250 filters\nmodel_5.add(Conv2D(250, (3, 3), strides = (2,2)))\nmodel_5.add(Activation('relu'))\n\n## 2x2 max pooling reduces to 3 x 3 x 250\nmodel_5.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_5.add(Dropout(0.25))\n\n## Flatten turns 3x3x250 into 240250X1\nmodel_5.add(Flatten())\nmodel_5.add(Dense(512))\nmodel_5.add(Activation('relu'))\nmodel_5.add(Dropout(0.5))\nmodel_5.add(Dense(num_classes))\nmodel_5.add(Activation('softmax'))\n\nmodel_5.summary()","c6016768":"# Run it for 10 epochs\nbatch_size = 32\n\n# initiate Adam optimizer\nopt = keras.optimizers.Adam(lr=0.0005, decay=1e-6)\n\n# Let's train the model using RMSprop\nmodel_5.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])\n\nmodel5_history = model_5.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=10,\n              validation_data=(x_test, y_test),\n              shuffle=True)","947e088d":"yhat_model5 = model_5.predict_classes(x_test)","b5809ee9":"yhat_model5_prob = model_5.predict(x_test)","ed92668b":"for i in range(9):\n    ax = plt.subplot(3, 3,i + 1)\n    plt.subplots_adjust(hspace=0.9)\n    plt.imshow(x_test[i])\n    title = f'(T:{np.argmax(y_test[i])})' + ' Vs ' + f'(P:{yhat_model5[i]})'\n    plt.title(title)\n    plt.axis('off')","43771dc1":"fig, ax = plt.subplots()\nax.plot(model1_history.history[\"val_accuracy\"],'r', marker='.', label=\"model1 accuracy\")\nax.plot(model2_hist.history[\"val_accuracy\"],'y', marker='.', label=\"model2 accuracy\")\nax.plot(model3_hist.history[\"val_accuracy\"],'o', marker='.', label=\"model3 accuracy\")\nax.plot(model4_history.history[\"val_accuracy\"],'b', marker='.', label=\"model4 accuracy\")\nax.plot(model5_history.history[\"val_accuracy\"],'g', marker='.', label=\"model5 accuracy\")\n#ax.plot(model2_hist.history[\"loss\"],'g', marker='.', label=\"Train loss\")\n#ax.plot(model1_history.history[\"val_loss\"],'y', marker='.', label=\"Validation loss\")\nax.set_xlabel('No.Epoch')\nax.set_xlim(1,10)\nax.set_title('Model Acuracy Vs No.epochs')\nax.legend(fontsize='small',loc=(0,1))","a1b12d72":"<p style=\"font-size:14px\">\n    This report focuses on using CNN algorithm to classify different images by using machine equipped by CPU processor.\n<br>The aims of this report are:<br>\n1-Classifing the images by using different sequential CNN models.<br>\n2- Studying the effect of tuning different hyperparameter on models performance in order to determine which set of parameters have noticeable impact on model behavior.<br>\nThis report can benefit People who are interested in image classification domain as it an attempt to quantify the relationship between different sets of hyperparameters and models performance.<\/p>","0920cace":"<b>Third Model:<\/b>\nChanging the activation function in model_1 to leaky Relu\n","5a765c49":"1-\tRead the image file from image directory<br>\n2-\tConvert the image file to array<br>\n3-\tResize the images to (250,250,3)<br>\n4-\tExtract the class description and class number for each image<br>\n","decbc12a":"5-\tRearrange randomly the samples to make sure samples are not ordered by class (When images are loaded from net they are sorted by name so either to resort them in the hosted directly by using other features (For example size), Or to resort them randomly by using np.random.shuffle() method.","879eb76b":"<b>Second Model<\/b>\nOne more conv2D layer has been added to first model\n","67685cae":"Ploting sample images","9cbbb53b":"<br>\n<p style=\"font-size:30px\">Suggestions<\/p> <br>\n1-\tReconducting this study by using more variety and number of images.<br>\n2-\tUsing Function type model to evaluate its performance and weather significant achievement can be gained.<br>\n3-\tUsing GPU instead of CPU to leverage machine hardware capabilities.<br>\n4-\tExperience different models from Pytorch.<br>\n5-\tWorking in building classification image models which covers high number of objects.<br>\n","910c1e9a":"This dataset consists of 1125 outdoor colorful images for different weather conditions (Shine, Cloudy, Rain and sunrise), their size varies from one image to other and they are proper balanced meaning that number of images for different weather classes are somehow similar.\n\n\nDataset Attribute:\nImage: numpy array (shape (250,250,3)\nClass_des: object (values: Shine, Sunrise, Cloudy, Rain)\nClass_id: integer (values:0,1,2,3)\nData comes in form of images file (JPG) and each image name represent the corresponding weather status (screenshot for images files) as part of data features engineering class description and class_id will be extracted.\n","db070cf3":"\n7-\tConvert classes to categorical  by using keras.utils.to_categorical() method.\n","aa8e09d5":"<br>\n<p style=\"font-size:30px\">Selection Model<\/p> <br>","028e884c":"<br>\n<p style=\"font-size:30px\">Model Training<\/p> <br>\nDifferent models have been created by tuning different hyperparameters like ( number of epochs, number of conv2d layers, type of activation function, type of optimizers, size of kernel..etc)","2a86a027":"<b>First Model:<\/b>\nThis model served as a bassline for other models, it consists from (conv2d layers , relu activation function, RMSprop optimizer, kernel size of (5,5) , MaxPooling 2D size(2,2)\n","2d0b5623":"<br>\n<p style=\"font-size:30px\">Key Findings<\/p> <br>\n1-\tIncreases the number of layers impact badly the performance of model.<br>\n2-\tRunning the models for high number of epochs does not leverage model performance, in contrast it impacts badly its performance.<br>\n3-\tUsing leaky Relu activation function instead of Relu has impacted badly model performance.<br>\n4-\tAdam optimizer has shown significant distribution toward model performance.<br>\n5-\tReducing Kernal size has shown significant distribution toward model performance.<br>\n","73ed1a37":"<b>Fifth Model:<\/b>\nSince we achived good enhacenent in model number 4 we have decided to keep model  4 and change its optimizer to Adam.\n","cf553fd7":"<br>\n<p style=\"font-size:30px\">Main Objectives:<\/p> \n<br>","dedf681e":"Following results vary slitely from one run to another:<br>\n1-The best accuracy achieved by first model is 82.67% at epoch 10 <br>\n2-The best accuracy achieved by second model is 75.56% at epoch 10<br>\n3-The best accuracy achieved by third model is 78.22% at epoch 6<br>\n4-The best accuracy achieved by fourth model is 85.33% at epoch 10 (it was run for only 7 epochs)<br>\n5-The best accuracy achieved by fifth model is 87.56% at epoch 10 <br><br>\nModel number 5 has been selected because its accuracy is the highest one, we reached to approximately 88% score by using this model and its execution time was in average, below figure illustrates the performance of model\u2019s vs number of epochs, Its Cleary noticeable that model 5 (Green ones) is the best one as shown in below chart","6d18fdd7":"Ploting some images from validations dataset shwing true class Vs predicted class","8f85edbb":"6-\tDistribute the data between training and validation sets (900 samples have been dedicated for trainng and 225 for validation).","acdf8ded":"<html>\n<head>\n<title>Page Title<\/title>\n<\/head>\n<body>\n\n<h1>\t&nbsp;\t&nbsp;\t&nbsp;\t&nbsp;\t&nbsp;\t&nbsp;\t&nbsp;\t&nbsp;\t&nbsp;\t&nbsp;\t&nbsp;\t&nbsp;\t&nbsp;\t&nbsp;\t&nbsp;Image Classification By Using CNN<\/h1>\n\n\n<\/body>\n<\/html>","2da160bd":"<b>Fourth model:<\/b>\nReducing the kernal size to (3,3)\n","4e248c9a":"<br>\n<p style=\"font-size:30px\">Data Processing:<\/p> <br>","82703d5d":"\n<table style=\"width:40%\">\n  <tr>\n    <th>Class Description <\/th>\n    <th>Class_id<\/th>\n  <\/tr>\n  <tr>\n    <td>Sunrise<\/td>\n    <td>0<\/td>\n  <\/tr>\n  <tr>\n    <td>Shine<\/td>\n    <td>1<\/td>\n  <\/tr>\n  <tr>\n    <td>Cloudy<\/td>\n    <td>2<\/td>\n  <\/tr>\n  <tr>\n    <td>Rain<\/td>\n    <td>3<\/td>\n  <\/tr>\n\n<\/table>","b89b4cf4":"<br>\n<p style=\"font-size:30px\">DataSet:<\/p> <br>"}}