{"cell_type":{"b71a9f20":"code","11391304":"code","ed222ab4":"code","dd597b01":"code","7d44f6cc":"code","efb8155c":"code","bd5690a9":"code","588ee689":"code","6f971d08":"code","2054542b":"code","9ad538c2":"code","7f11be1f":"code","48a513be":"code","7030009b":"code","38e53bff":"code","69eb989a":"code","f3dc182a":"code","a15ed9c5":"code","32745764":"code","850a94ca":"code","f1448c52":"code","a3411ddd":"code","68da0f5b":"code","d4121cd1":"code","4a73fc9a":"code","5812287d":"code","ebcaff19":"code","482adaf7":"code","a2ae206e":"code","309b214f":"code","5d343090":"code","74a162d0":"code","2594aaae":"code","1673d671":"code","2eec4cf7":"code","dfdb7755":"code","a60070e7":"code","aed1579c":"code","4d97da4b":"code","db018f3a":"code","b56027d6":"code","eec845e7":"code","4e911cc3":"code","ec1c830c":"code","b1714f2f":"code","1904937c":"code","984798f6":"code","12933e62":"code","5165cc50":"code","d1b3015f":"code","5d38da12":"code","c8bea5ad":"code","0bdf75aa":"code","8b91111d":"code","6a351f96":"code","ced86550":"code","70405eb6":"code","9c13c79b":"code","67d074f8":"code","092b3451":"code","14ac78ef":"code","d30ca3d9":"code","276d2b6d":"code","af176999":"code","c208c8b4":"code","593666ac":"code","6a113ad5":"code","502b999b":"code","d631e833":"code","6461da3b":"code","3c8da98f":"code","d5426b2c":"code","caa87a2b":"code","0a7a4c52":"code","f9dc1cff":"code","68e32109":"code","5379e860":"code","b3307fa5":"code","1b77c61e":"code","a99582d2":"code","b7a66f2b":"code","8a8159aa":"code","1fc05742":"code","3c3a849e":"code","eaf26e0d":"code","b71bc88f":"code","10620a12":"code","84fa3669":"code","436dd31f":"code","38fae7ba":"code","24156429":"code","01ae20e4":"code","6886913c":"code","cec0f9fa":"code","17ae2380":"code","de7b066d":"code","0382dca0":"code","94cfe092":"code","97bf6448":"code","6dc83d0d":"code","f4652d68":"code","1732b713":"code","1448c616":"code","678955d8":"code","99cbd3c7":"code","360534e2":"code","df3297b1":"code","f9acf6f3":"code","6564d703":"code","14be0a1c":"code","b58cb8bb":"code","3c92b33c":"code","ef06e607":"code","f5972e51":"code","6b0b4f19":"code","573577e3":"code","21978293":"code","75efa228":"code","ccf35320":"code","b66b28df":"code","45bba906":"code","972dbc20":"code","3ae4ec07":"code","7009565a":"code","32e41ea2":"code","dba33290":"code","9c96f1c5":"code","190fe351":"code","c0b51f88":"code","8c3ecf3b":"code","60b8c39e":"code","3b1becc7":"code","90dee1b7":"code","73306a31":"code","a0313d69":"code","2663c2c7":"code","150d5aa6":"code","c624e2dc":"code","33b422a0":"markdown","b44c3d9d":"markdown","b98115b5":"markdown","21b75922":"markdown","9c5e777a":"markdown","bfd9577f":"markdown","dfd806fc":"markdown","41e6134a":"markdown","3d332ccf":"markdown","ae4df688":"markdown","47ba19b9":"markdown","e5b27ed4":"markdown","b320d296":"markdown","83111608":"markdown","806fce49":"markdown","a5247c10":"markdown","bc3d25a4":"markdown","56ea5da7":"markdown","4cddabe6":"markdown","a365ee38":"markdown","fc368c72":"markdown","8b7363d6":"markdown","5e96e1df":"markdown","efc68597":"markdown","5800deb4":"markdown","01712eba":"markdown","3bc169eb":"markdown","8bcc3949":"markdown","23b63c3a":"markdown","e8dddd3e":"markdown"},"source":{"b71a9f20":"from __future__ import division \nimport nltk, re, pprint\nfrom nltk import word_tokenize\nfrom urllib import request\nfrom nltk.collocations import *","11391304":"url = \"http:\/\/www.gutenberg.org\/files\/2554\/2554-0.txt\"\nresponse = request.urlopen(url)\nraw = response.read().decode('utf8')\ntype(raw)\nlen(raw)\nraw[:75]","ed222ab4":">>> import nltk\n>>> nltk.download('punkt')\ntokens = word_tokenize(raw)\ntype(tokens)\nlen(tokens)\ntokens[:10]","dd597b01":"text = nltk.Text(tokens)\ntype(text)\ntext[1024:1050]","7d44f6cc":">>> raw.find(\"PART I\")","efb8155c":">>> raw.rfind(\"End of Project Gutenberg's Crime\")","bd5690a9":">>> raw = raw[5338:1157743] #[1]\n>>> raw.find(\"PART I\")","588ee689":"url = \"http:\/\/news.bbc.co.uk\/2\/hi\/health\/2284783.stm\"\nhtml = request.urlopen(url).read().decode('utf8')\nhtml[:60]","6f971d08":"from bs4 import BeautifulSoup\nraw = BeautifulSoup(html, 'html.parser').get_text()\ntokens = word_tokenize(raw)\ntokens","2054542b":"tokens = tokens[110:390]\ntext = nltk.Text(tokens)\ntext.concordance('gene')","9ad538c2":">>> import nltk \n>>> nltk.download('gutenberg')\npath = nltk.data.find('corpora\/gutenberg\/melville-moby_dick.txt')\nraw = open(path, 'r').read()","7f11be1f":"s = input(\"Enter some text: \")\nprint(\"You typed\", len(word_tokenize(s)), \"words.\")","48a513be":"tokens = word_tokenize(raw)\ntype(tokens)","7030009b":"words = [w.lower() for w in tokens]\ntype(words)","38e53bff":"vocab = sorted(set(words))\ntype(vocab)","69eb989a":"query = ['Who knows?']\nbeatles = ['john', 'paul', 'george', 'ringo']\nquery + beatles","f3dc182a":"Siti = 'Siti Python' \nSiti","a15ed9c5":"now = \"Siti learn Python's now\"\nnow","32745764":"now = 'Siti learn Python\\'s now' \nnow","850a94ca":"now = 'Siti learn Python\\'s now' ","f1448c52":"couplet = \"Shall I compare thee to a Summer's day?\"\\\n           \"Thou are more lovely and more temperate:\" \nprint(couplet)","a3411ddd":"couplet = (\"Rough winds do shake the darling buds of May,\"\n           \"And Summer's lease hath all too short a date:\")\nprint(couplet)","68da0f5b":"couplet = \"\"\"Shall I compare thee to a Summer's day?\nThou are more lovely and more temperate:\"\"\"\nprint(couplet)","d4121cd1":"couplet = '''Rough winds do shake the darling buds of May,\nAnd Summer's lease hath all too short a date:'''\nprint(couplet)","4a73fc9a":"'siti' + 'siti' + 'siti'","5812287d":"'siti' * 3","ebcaff19":"a = [1, 2, 3, 4, 5, 6, 7, 6, 5, 4, 3, 2, 1]\nb = [' ' * 2 * (7 - i) + 'siti' * i for i in a]\nfor line in b:\n     print(line)","482adaf7":"sent = 'the moss is on the wall'\nfor char in sent:\n     print(char, end=' ')","a2ae206e":">>> import nltk\n>>> nltk.download('gutenberg')\nfrom nltk.corpus import gutenberg\nraw = gutenberg.raw('melville-moby_dick.txt')\nfdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())\nfdist.most_common(5)","309b214f":"[char for (char, count) in fdist.most_common()]","5d343090":"phrase = 'Whenever I don\\'t have any homework'\nif 'have' in phrase:\n     print('found \"have\"')","74a162d0":"query = 'Who knows?'\nbeatles = ['John', 'Paul', 'George', 'Ringo']\nquery[4]","2594aaae":"beatles[2]","1673d671":"query[:2]","2eec4cf7":"beatles[:2]","dfdb7755":"query + \" I don't\"","a60070e7":"beatles + ['Brian']","aed1579c":"beatles + ['Brian']","4d97da4b":"beatles[0] = \"John Lennon\"\ndel beatles[-1]\nbeatles","db018f3a":">>> import nltk\n>>> nltk.download('unicode_samples')\npath = nltk.data.find('corpora\/unicode_samples\/polish-lat2.txt')","b56027d6":"f = open(path, encoding='latin2')\nfor line in f:\n    line = line.strip()\n    print(line)","eec845e7":"f = open(path, encoding='latin2')\nfor line in f:\n     line = line.strip()\n     print(line.encode('unicode_escape'))","4e911cc3":"ord('\u0144')","ec1c830c":"nacute = '\\u0144'\nnacute","b1714f2f":"nacute.encode('utf8')","1904937c":"import unicodedata\nlines = open(path, encoding='latin2').readlines()\nline = lines[2]\nprint(line.encode('unicode_escape'))","984798f6":"for c in line: \n    if ord(c) > 127:\n        print('{} U+{:04x} {}'.format(c.encode('utf8'), ord(c), unicodedata.name(c)))","12933e62":"line.find('zosta\\u0142y')","5165cc50":"line = line.lower()\nline","d1b3015f":"import re\nm = re.search('\\u015b\\w*', line)\nm.group()","5d38da12":"word_tokenize(line)","c8bea5ad":">>> import nltk\n>>> nltk.download('words')\nimport re\nwordlist = [w for w in nltk.corpus.words.words('en') if w.islower()]","0bdf75aa":"[w for w in wordlist if re.search('ed$', w)]","8b91111d":"[w for w in wordlist if re.search('^..j..t..$', w)]","6a351f96":"[w for w in wordlist if re.search('^[ghi][mno][jlk][def]$', w)]","ced86550":">>> import nltk\n>>> nltk.download('nps_chat')\nchat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\n[w for w in chat_words if re.search('^m+i+n+e+$', w)]","70405eb6":"[w for w in chat_words if re.search('^[ha]+$', w)]","9c13c79b":">>> import nltk\n>>> nltk.download('treebank')\nwsj = sorted(set(nltk.corpus.treebank.words()))\n[w for w in wsj if re.search('^[0-9]+\\.[0-9]+$', w)]","67d074f8":"[w for w in wsj if re.search('^[A-Z]+\\$$', w)]","092b3451":"[w for w in wsj if re.search('^[0-9]{4}$', w)]","14ac78ef":"[w for w in wsj if re.search('^[0-9]+-[a-z]{3,5}$', w)]","d30ca3d9":"[w for w in wsj if re.search('^[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$', w)]","276d2b6d":"[w for w in wsj if re.search('(ed|ing)$', w)]","af176999":"word = 'supercalifragilisticexpialidocious'\nre.findall(r'[aeiou]', word)","c208c8b4":"len(re.findall(r'[aeiou]', word))","593666ac":"wsj = sorted(set(nltk.corpus.treebank.words()))\nfd = nltk.FreqDist(vs for word in wsj\n                       for vs in re.findall(r'[aeiou]{2,}', word))\nfd.most_common(12)","6a113ad5":">>> import nltk\n>>> nltk.download('udhr')\nregexp = r'^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]'\ndef compress(word):\n     pieces = re.findall(regexp, word)\n     return ''.join(pieces)\n\nenglish_udhr = nltk.corpus.udhr.words('English-Latin1')\nprint(nltk.tokenwrap(compress(w) for w in english_udhr[:75]))","502b999b":">>> import nltk\n>>> nltk.download('toolbox')\nrotokas_words = nltk.corpus.toolbox.words('rotokas.dic')\ncvs = [cv for w in rotokas_words for cv in re.findall(r'[ptksvr][aeiou]', w)]\ncfd = nltk.ConditionalFreqDist(cvs)\ncfd.tabulate()","d631e833":"cv_word_pairs = [(cv, w) for w in rotokas_words\n                          for cv in re.findall(r'[ptksvr][aeiou]', w)]\ncv_index = nltk.Index(cv_word_pairs)\ncv_index['su']","6461da3b":"cv_index['po']","3c8da98f":"def stem(word):\n     for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n         if word.endswith(suffix):\n             return word[:-len(suffix)]\n     return word","d5426b2c":"re.findall(r'^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')","caa87a2b":"re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processing')","0a7a4c52":"re.findall(r'^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')","f9dc1cff":"re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$', 'processes')","68e32109":"re.findall(r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$', 'language')","5379e860":"def stem(word):\n     regexp = r'^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$'\n     stem, suffix = re.findall(regexp, word)[0]\n     return stem\n\nraw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\nis no basis for a system of government.  Supreme executive power derives from\na mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\ntokens = word_tokenize(raw)\n[stem(t) for t in tokens]","b3307fa5":"from nltk.corpus import gutenberg, nps_chat\nmoby = nltk.Text(gutenberg.words('melville-moby_dick.txt'))\nmoby.findall(r\"<a> (<.*>) <man>\")","1b77c61e":"chat = nltk.Text(nps_chat.words())\nchat.findall(r\"<.*> <.*> <bro>\")","a99582d2":"chat.findall(r\"<l.*>{3,}\")","b7a66f2b":">>> import nltk\n>>> nltk.download('brown')\nfrom nltk.corpus import brown\nhobbies_learned = nltk.Text(brown.words(categories=['hobbies', 'learned']))\nhobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")","8a8159aa":"raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\nis no basis for a system of government.  Supreme executive power derives from\na mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\ntokens = word_tokenize(raw)","1fc05742":"porter = nltk.PorterStemmer()\nlancaster = nltk.LancasterStemmer()\n[porter.stem(t) for t in tokens]","3c3a849e":"[lancaster.stem(t) for t in tokens]","eaf26e0d":"class IndexedText(object):\n\n    def __init__(self, stemmer, text):\n        self._text = text\n        self._stemmer = stemmer\n        self._index = nltk.Index((self._stem(word), i)\n                                 for (i, word) in enumerate(text))\n\n    def concordance(self, word, width=40):\n        key = self._stem(word)\n        wc = int(width\/4)                # words of context\n        for i in self._index[key]:\n            lcontext = ' '.join(self._text[i-wc:i])\n            rcontext = ' '.join(self._text[i:i+wc])\n            ldisplay = '{:>{width}}'.format(lcontext[-width:], width=width)\n            rdisplay = '{:{width}}'.format(rcontext[:width], width=width)\n            print(ldisplay, rdisplay)\n\n    def _stem(self, word):\n        return self._stemmer.stem(word).lower()","b71bc88f":">>> import nltk\n>>> nltk.download('webtext')\nporter = nltk.PorterStemmer()\ngrail = nltk.corpus.webtext.words('grail.txt')\ntext = IndexedText(porter, grail)\ntext.concordance('lie')","10620a12":">>> import nltk\n>>> nltk.download('wordnet')\nwnl = nltk.WordNetLemmatizer()\n[wnl.lemmatize(t) for t in tokens]","84fa3669":"raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone\nthough), 'I won't have any pepper in my kitchen AT ALL. Soup does very\nwell without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\"","436dd31f":"re.split(r' ', raw)","38fae7ba":"re.split(r'[ \\t\\n]+', raw)","24156429":"re.split(r'\\W+', raw)","01ae20e4":"re.findall(r'\\w+|\\S\\w*', raw)","6886913c":"print(re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw))","cec0f9fa":"text = 'That U.S.A. poster-print costs $12.40...'\npattern = r'''(?x)     # set flag to allow verbose regexps\n    (?:[A-Z]\\.)+       # abbreviations, e.g. U.S.A.\n   | \\w+(?:-\\w+)*       # words with optional internal hyphens\n   | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n   | \\.\\.\\.             # ellipsis\n   | [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n '''\nnltk.regexp_tokenize(text, pattern)","17ae2380":"len(nltk.corpus.brown.words()) \/ len(nltk.corpus.brown.sents())","de7b066d":"text = nltk.corpus.gutenberg.raw('chesterton-thursday.txt')\nsents = nltk.sent_tokenize(text)\npprint.pprint(sents[79:89])","0382dca0":"text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\nseg1 = \"0000000000000001000000000010000000000000000100000000000\"\nseg2 = \"0100100100100001001001000010100100010010000100010010000\"","94cfe092":"def segment(text, segs):\n    words = []\n    last = 0\n    for i in range(len(segs)):\n        if segs[i] == '1':\n            words.append(text[last:i+1])\n            last = i+1\n    words.append(text[last:])\n    return words","97bf6448":"text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\nseg1 = \"0000000000000001000000000010000000000000000100000000000\"\nseg2 = \"0100100100100001001001000010100100010010000100010010000\"\nsegment(text, seg1)","6dc83d0d":"segment(text, seg2)","f4652d68":"def evaluate(text, segs):\n    words = segment(text, segs)\n    text_size = len(words)\n    lexicon_size = sum(len(word) + 1 for word in set(words))\n    return text_size + lexicon_size","1732b713":"text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\nseg1 = \"0000000000000001000000000010000000000000000100000000000\"\nseg2 = \"0100100100100001001001000010100100010010000100010010000\"\nseg3 = \"0000100100000011001000000110000100010000001100010000001\"\nsegment(text, seg3)","1448c616":"evaluate(text, seg3)","678955d8":"evaluate(text, seg2)","99cbd3c7":"evaluate(text, seg1)","360534e2":"from random import randint\n\ndef flip(segs, pos):\n    return segs[:pos] + str(1-int(segs[pos])) + segs[pos+1:]\n\ndef flip_n(segs, n):\n    for i in range(n):\n        segs = flip(segs, randint(0, len(segs)-1))\n    return segs\n\ndef anneal(text, segs, iterations, cooling_rate):\n    temperature = float(len(segs))\n    while temperature > 0.5:\n        best_segs, best = segs, evaluate(text, segs)\n        for i in range(iterations):\n            guess = flip_n(segs, round(temperature))\n            score = evaluate(text, guess)\n            if score < best:\n                best, best_segs = score, guess\n        score, segs = best, best_segs\n        temperature = temperature \/ cooling_rate\n        print(evaluate(text, segs), segment(text, segs))\n    print()\n    return segs","df3297b1":"text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\nseg1 = \"0000000000000001000000000010000000000000000100000000000\"\nanneal(text, seg1, 5000, 1.2)","f9acf6f3":"silly = ['We', 'called', 'him', 'Tortoise', 'because', 'he', 'taught', 'us', '.']\n' '.join(silly)","6564d703":"';'.join(silly)","14be0a1c":"''.join(silly)","b58cb8bb":"word = 'cat'\nsentence = \"\"\"hello\nworld\"\"\"\nprint(word)","3c92b33c":"print(sentence)","ef06e607":"word","f5972e51":"sentence","6b0b4f19":"for word in sorted(fdist):\n   print('{}->{};'.format(word, fdist[word]), end=' ')","573577e3":"'{}->{};'.format ('cat', 3)","21978293":"'{}->'.format('cat')","75efa228":"'{}'.format(3)","ccf35320":"'I want a {} right now'.format('coffee')","b66b28df":"'{} wants a {} {}'.format ('Lee', 'sandwich', 'for lunch')","45bba906":"'{} wants a {} {}'.format ('sandwich', 'for lunch')","972dbc20":"'{} wants a {}'.format ('Lee', 'sandwich', 'for lunch')","3ae4ec07":"'from {1} to {0}'.format('A', 'B')","7009565a":"template = 'Lee wants a {} right now'\nmenu = ['sandwich', 'spam fritter', 'pancake']\nfor snack in menu:\n    print(template.format(snack))","32e41ea2":"'{:6}'.format(41)","dba33290":"'{:<6}' .format(41)","9c96f1c5":"'{:6}'.format('dog')","190fe351":"'{:>6}'.format('dog')","c0b51f88":"import math\n'{:.4f}'.format(math.pi)","8c3ecf3b":"count, total = 3205, 9375\n\"accuracy for {} words: {:.4%}\".format(total, count \/ total)","60b8c39e":"def tabulate(cfdist, words, categories):\n    print('{:16}'.format('Category'), end=' ')                    # column headings\n    for word in words:\n        print('{:>6}'.format(word), end=' ')\n    print()\n    for category in categories:\n        print('{:16}'.format(category), end=' ')                  # row heading\n        for word in words:                                        # for each word\n            print('{:6}'.format(cfdist[category][word]), end=' ') # print table cell\n        print()                                                   # end the row\n\nfrom nltk.corpus import brown\ncfd = nltk.ConditionalFreqDist(\n          (genre, word)\n          for genre in brown.categories()\n          for word in brown.words(categories=genre))\ngenres = ['news', 'religion', 'hobbies', 'science_fiction', 'romance', 'humor']\nmodals = ['can', 'could', 'may', 'might', 'must', 'will']\ntabulate(cfd, modals, genres)\n#Category            can  could    may  might   must   will\n#news                 93     86     66     38     50    389\n#religion             82     59     78     12     54     71\n#hobbies             268     58    131     22     83    264\n#science_fiction      16     49      4     12      8     16\n#romance              74    193     11     51     45     43\n#humor                16     30      8      8      9     13","3b1becc7":"'{:{width}}'.format('Monty Python', width=15)","90dee1b7":">>> import nltk\n>>> nltk.download('genesis')\noutput_file = open('output.txt', 'w')\nwords = set(nltk.corpus.genesis.words('english-kjv.txt'))\nfor word in sorted(words):\n    print(word, file=output_file)","73306a31":"len(words)","a0313d69":"str(len(words))","2663c2c7":"print(str(len(words)), file=output_file)","150d5aa6":"saying = ['After', 'all', 'is', 'said', 'and', 'done', ',',\n           'more', 'is', 'said', 'than', 'done', '.']\nfor word in saying:\n    print(word, '(' + str(len(word)) + '),', end=' ')","c624e2dc":"from textwrap import fill\npieces = [\"{} {}\".format(word, len(word)) for word in saying]\noutput = ' '.join(pieces)\nwrapped = fill(output)\nprint(wrapped)","33b422a0":"3.7   Regular Expressions for Tokenizing Text","b44c3d9d":"Simple Approaches to Tokenization","b98115b5":"Doing More with Word Pieces","21b75922":"from bs4 import BeautifulSoup\nraw = BeautifulSoup(html, 'html.parser').get_text()\ntokens = word_tokenize(raw)\ntokens","9c5e777a":"Text Wrapping","bfd9577f":"Lemmatization","dfd806fc":"3.5   Useful Applications of Regular Expressions","41e6134a":"Capturing user input","3d332ccf":"text.collocations()","ae4df688":"Sentence Segmentation","47ba19b9":"Writing Results to a File","e5b27ed4":"**Dealing with HTML**","b320d296":"3.9   Formatting: From Lists to Strings","83111608":"3.8   Segmentation","806fce49":"NLTK's Regular Expression Tokenizer","a5247c10":"**Processing Search Engine Results**\n","bc3d25a4":"Finding Word Stems","56ea5da7":"Searching Tokenized Text","4cddabe6":"Lining Things Up","a365ee38":"Stemmers","fc368c72":"3.4   Regular Expressions for Detecting Word Patterns","8b7363d6":"Strings and Formats","5e96e1df":"The web can be thought of as a huge corpus of unannotated text. Web search engines provide an efficient means of searching this large quantity of text for relevant linguistic examples. The main advantage of search engines is size: since you are searching such a large set of documents, you are more likely to find any linguistic pattern you are interested in. Furthermore, you can make use of very specific patterns, which would only match one or two examples on a smaller example, but which might match tens of thousands of examples when run on the web. A second advantage of web search engines is that they are very easy to use. Thus, they provide a very convenient tool for quickly checking a theory, to see if it is reasonable.","efc68597":"Extracting Word Pieces","5800deb4":"The Difference between Lists and Strings","01712eba":"3.6   Normalizing Text","3bc169eb":"Word Segmentation","8bcc3949":"3.3   Text Processing with Unicode","23b63c3a":"From Lists to Strings","e8dddd3e":"modified example  from the book https:\/\/www.nltk.org\/book\/ch03.html"}}