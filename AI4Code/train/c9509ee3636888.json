{"cell_type":{"098262ea":"code","d8e724cd":"code","9c98a15b":"code","431abc51":"code","2a39d64a":"code","28707d3b":"code","139700df":"code","a07f2ae0":"code","2e43eea5":"code","ffe2a656":"code","ffdc5e9d":"code","259363f2":"code","bcafe7c8":"code","51a20785":"code","7a7e2b93":"code","2058bde6":"code","509a66f1":"markdown"},"source":{"098262ea":"!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-development.tsv -q\n!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-test.tsv -q\n!wget https:\/\/github.com\/google-research-datasets\/gap-coreference\/raw\/master\/gap-validation.tsv -q","d8e724cd":"!ls ..\/input","9c98a15b":"!pip install pytorch-pretrained-bert","431abc51":"import time\nimport os\nimport random\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nfrom torch.optim import Adam\nfrom torch.utils.data import Dataset\nfrom torch.nn import Module, Linear, Dropout\nimport torch.nn.functional as F\nfrom pytorch_pretrained_bert.modeling import BertModel, BertLayer\nfrom pytorch_pretrained_bert import BertTokenizer\nfrom pytorch_pretrained_bert.optimization import BertAdam\nfrom pytorch_pretrained_bert.optimization import warmup_linear\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data import RandomSampler\n\nfrom sklearn.metrics import log_loss\nimport matplotlib.pyplot as plt","2a39d64a":"seed = 9876\n\nrandom.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True","28707d3b":"# Model\nbert_model = \"bert-large-cased\"\nn_bertlayers = 22\ndropout = 0.1\n\n# Preprocessing\ndo_lower_case = False\n\n# Training\ntrain_batch_size = 4\ngradient_accumulation_steps = 5\nlr = 1e-5\nnum_train_epochs = 2\nwarmup_proportion = 0.1\noptim = \"bertadam\"\nweight_decay = False\n\n\n# Others\nn_models = 10\neval_batch_size = 32\n\ndevice = torch.device(\"cuda\")\ndata_dir = \"\"","139700df":"def insert_tag(row):\n    \"\"\"Insert custom tags to help us find the position of A, B, and the pronoun after tokenization.\"\"\"\n    to_be_inserted = sorted([\n        (row[\"A-offset\"], \" [A] \"),\n        (row[\"B-offset\"], \" [B] \"),\n        (row[\"Pronoun-offset\"], \" [P] \")\n    ], key=lambda x: x[0], reverse=True)\n    text = row[\"Text\"]\n    for offset, tag in to_be_inserted:\n        text = text[:offset] + tag + text[offset:]\n    return text\n\n\ndef tokenize(text, tokenizer):\n    \"\"\"Returns a list of tokens and the positions of A, B, and the pronoun.\"\"\"\n    entries = {}\n    final_tokens = []\n    for token in tokenizer.tokenize(text):\n        if token in (\"[A]\", \"[B]\", \"[P]\"):\n            entries[token] = len(final_tokens)\n            continue\n        final_tokens.append(token)\n    return final_tokens, (entries[\"[A]\"], entries[\"[B]\"], entries[\"[P]\"])\n\n\nclass GAPDataset(Dataset):\n    \"\"\"Custom GAP Dataset class\"\"\"\n    def __init__(self, df, tokenizer, labeled=True):\n        self.labeled = labeled\n        if labeled:\n            tmp = df[[\"A-coref\", \"B-coref\"]].copy()\n            tmp[\"Neither\"] = ~(df[\"A-coref\"] | df[\"B-coref\"])\n            self.y = tmp.values.astype(\"bool\")\n        # Extracts the tokens and offsets(positions of A, B, and P)\n        self.offsets, self.tokens = [], []\n        self.seq_len = []\n        for _, row in df.iterrows():\n            text = insert_tag(row)\n            tokens, offsets = tokenize(text, tokenizer)\n            self.offsets.append(offsets)\n            self.tokens.append(tokenizer.convert_tokens_to_ids(\n                [\"[CLS]\"] + tokens + [\"[SEP]\"]))\n            self.seq_len.append(len(self.tokens[-1]))\n\n    def __len__(self):\n        return len(self.tokens)\n\n    def __getitem__(self, idx):\n        if self.labeled:\n            return self.tokens[idx], self.offsets[idx], self.y[idx]\n        return self.tokens[idx], self.offsets[idx]\n\n    def get_seq_len(self):\n        return self.seq_len\n\n\ndef collate_examples(batch, truncate_len=500):\n    \"\"\"Batch preparation.\n\n    1. Pad the sequences\n    2. Transform the target.\n    \"\"\"\n    transposed = list(zip(*batch))\n    max_len = min(\n        max((len(x) for x in transposed[0])),\n        truncate_len\n    )\n    tokens = np.zeros((len(batch), max_len), dtype=np.int64)\n    for i, row in enumerate(transposed[0]):\n        row = np.array(row[:truncate_len])\n        tokens[i, :len(row)] = row\n    token_tensor = torch.from_numpy(tokens)\n    # Offsets\n    offsets = torch.stack([\n        torch.LongTensor(x) for x in transposed[1]\n    ], dim=0) + 1 # Account for the [CLS] token\n    # Labels\n    if len(transposed) == 2:\n        return token_tensor, offsets, None\n    one_hot_labels = torch.stack([\n        torch.from_numpy(x.astype(\"uint8\")) for x in transposed[2]\n    ], dim=0)\n    _, labels = one_hot_labels.max(dim=1)\n    return token_tensor, offsets, labels","a07f2ae0":"def get_pretrained_bert(modelname, num_hidden_layers=None):\n    bert = BertModel.from_pretrained(modelname)\n    if num_hidden_layers is None:\n        return bert\n    old_num_hidden_layers = bert.config.num_hidden_layers\n    if num_hidden_layers < old_num_hidden_layers:\n        # Only use the bottom n layers\n        del bert.encoder.layer[num_hidden_layers:]\n    elif num_hidden_layers > old_num_hidden_layers:\n        # Add BertLayer(s)\n        for i in range(old_num_hidden_layers, num_hidden_layers):\n            bert.encoder.layer.add_module(str(i), BertLayer(bert.config))\n    if num_hidden_layers != old_num_hidden_layers:\n        bert.config.num_hidden_layers = num_hidden_layers\n        bert.init_bert_weights(bert.pooler.dense)\n    return bert\n\n\nclass BertCl_GAP(Module):\n    \"\"\"The main model.\"\"\"\n    def __init__(self, bert, dropout, n_offsets=3):\n        super().__init__()\n        self.bert = bert\n        self.bert_hidden_size = self.bert.config.hidden_size\n        self.dropout = Dropout(dropout)\n        self.classifier = Linear(self.bert.config.hidden_size * n_offsets, n_offsets)\n\n    def forward(self, token_tensor, offsets, label_id=None):\n        bert_outputs, _ = self.bert(\n            token_tensor, attention_mask=(token_tensor > 0).long(),\n            token_type_ids=None, output_all_encoded_layers=False)\n        extracted_outputs = bert_outputs.gather(\n            1, offsets.unsqueeze(2).expand(-1, -1, bert_outputs.size(2))\n        ).view(bert_outputs.size(0), -1)\n        outputs = self.classifier(self.dropout(extracted_outputs))\n        return outputs\n\n\ndef get_param_size(model):\n    trainable_psize = np.sum([np.prod(p.size()) for p in model.parameters() if p.requires_grad])\n    total_psize = np.sum([np.prod(p.size()) for p in model.parameters()])\n    return total_psize, trainable_psize\n\n\ndef run_epoch(model, dataloader, optimizer, criterion, device,\n              verbose_step=10000):\n    model.train()\n    t1 = time.time()\n    tr_loss = 0\n    for step, batch in enumerate(dataloader):\n        batch = tuple(t.to(device) for t in batch)\n        label_ids = batch[-1]\n        outputs = model(*batch[:-1])\n        if criterion._get_name() == \"BCEWithLogitsLoss\":\n            outputs = outputs[:, 0]\n            label_ids = label_ids.float()\n        loss = criterion(outputs, label_ids)\n        if gradient_accumulation_steps > 1:\n            loss = loss \/ gradient_accumulation_steps\n        loss.backward()\n        tr_loss += loss.item()\n        if (step + 1) % verbose_step == 0:\n            loss_now = gradient_accumulation_steps * tr_loss \/ (step + 1)\n            print(f'step:{step+1} loss:{loss_now:.7f} time:{time.time() - t1:.1f}s')\n        if (step + 1) % gradient_accumulation_steps == 0:\n            optimizer.step()\n            model.zero_grad()\n    return gradient_accumulation_steps * tr_loss \/ (step + 1)\n\n\ndef predict(model, data_loader, device, proba=True, to_numpy=True):\n    model.eval()\n    preds = []\n    for step, batch in enumerate(data_loader):\n        batch = batch[:2]\n        batch = tuple(t.to(device) for t in batch)\n        # input_ids, offsets, label_ids = batch\n        # label_ids = batch[-1]\n        with torch.no_grad():\n            logits = model(*batch)\n            preds.append(logits.detach().cpu())\n    preds = torch.cat(preds) if len(preds) > 1 else preds[0]\n    if proba:\n        if preds.size(-1) > 1:\n            preds = F.softmax(preds, dim=1)\n        else:\n            preds = torch.sigmoid(preds)\n    if to_numpy:\n        preds = preds.numpy()\n    return preds","2e43eea5":"def get_gap_model(bert_model, n_bertlayers, dropout,\n                  steps_per_epoch, device):\n    bert = get_pretrained_bert(bert_model, n_bertlayers)\n    model = BertCl_GAP(bert, dropout)\n\n    model.to(device)\n\n    param_optimizer = list(model.named_parameters())\n\n    if weight_decay:\n        no_decay = [\"bias\", \"gamma\", \"beta\", \"head\"]\n        optimizer_grouped_parameters = [\n            {\"params\": [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n             \"weight_decay\": 0.01},\n            {\"params\": [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n             \"weight_decay\": 0.0}\n        ]\n    else:\n        optimizer_grouped_parameters = [\n            {\"params\": [p for n, p in param_optimizer], \"weight_decay\": 0.0}\n\n        ]\n\n    t_total = int(\n        steps_per_epoch \/ gradient_accumulation_steps * num_train_epochs)\n    if optim == 'bertadam':\n        optimizer = BertAdam(optimizer_grouped_parameters,\n                             lr=lr,\n                             warmup=warmup_proportion,\n                             t_total=t_total)\n    elif optim == 'adam':\n        optimizer = Adam(optimizer_grouped_parameters, lr=lr)\n    return model, optimizer\n\n\ndef get_loader(train_df, val_df, test_df):\n    tokenizer = BertTokenizer.from_pretrained(\n        bert_model,\n        do_lower_case=do_lower_case,\n        never_split=(\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\", \"[A]\", \"[B]\", \"[P]\")\n    )\n    # These tokens are not actually used, so we can assign arbitrary values.\n    tokenizer.vocab['[A]'] = -1\n    tokenizer.vocab['[B]'] = -1\n    tokenizer.vocab['[P]'] = -1\n\n    train_ds = GAPDataset(train_df, tokenizer)\n    val_ds = GAPDataset(val_df, tokenizer)\n    test_ds = GAPDataset(test_df, tokenizer, labeled=False)\n        \n    train_loader = DataLoader(\n        train_ds,\n        collate_fn=collate_examples,\n        batch_size=train_batch_size,\n        shuffle=True,\n        drop_last=True\n    )\n    val_loader = DataLoader(\n        val_ds,\n        collate_fn=collate_examples,\n        batch_size=eval_batch_size,\n        shuffle=False\n    )\n    test_loader = DataLoader(\n        test_ds,\n        collate_fn=collate_examples,\n        batch_size=eval_batch_size,\n        shuffle=False\n    )\n    return train_loader, val_loader, test_loader","ffe2a656":"train_df = pd.concat([pd.read_csv(data_dir + \"gap-test.tsv\", delimiter=\"\\t\"),\n                      pd.read_csv(data_dir + \"gap-development.tsv\", delimiter=\"\\t\")])\nval_df = pd.read_csv(data_dir + \"gap-validation.tsv\", delimiter=\"\\t\")\nval_y = val_df[['A-coref', 'B-coref']].astype(int)\nval_y['None'] = 1 - val_y.sum(1)\n\ntest_df = pd.read_csv(\"..\/input\/test_stage_2.tsv\", delimiter=\"\\t\")\n\nprint(f\"Train:{train_df.shape[0]}, Valid:{val_df.shape[0]}, Test:{test_df.shape[0]}\")","ffdc5e9d":"train_loader, val_loader, test_loader = get_loader(train_df, val_df, test_df)","259363f2":"steps_per_epoch = len(train_loader)\nsteps_per_epoch","bcafe7c8":"scores = []\n\ncriterion = torch.nn.CrossEntropyLoss()\nval_pr_avg = [np.zeros(val_y.shape) for _ in range(num_train_epochs)]\ntest_pr_avg = np.zeros((test_df.shape[0], 3))","51a20785":"for model_id in range(n_models):\n    model, optimizer = get_gap_model(bert_model, n_bertlayers, dropout,\n                                     steps_per_epoch, device)\n    total_psize, trainalbe_psize = get_param_size(model)\n    print(f\"Total params: {total_psize}\\nTrainable params: {trainalbe_psize}\")\n    for e in range(num_train_epochs):\n        t1 = time.time()\n        tr_loss = run_epoch(model, train_loader, optimizer, criterion, device)\n        val_pr = predict(model, val_loader, device)\n        val_pr_avg[e] += val_pr\n        val_loss = log_loss(val_y, val_pr)\n        val_avg_loss = log_loss(val_y, val_pr_avg[e] \/ (model_id + 1))\n        elapsed = time.time() - t1\n        print(f\"Epoch:{e + 1} tr_loss:{tr_loss:.4f} val_loss:{val_loss:.4f}\"\n              f\" val_avg_loss: {val_avg_loss:.4f} time:{elapsed:.1f}s\")\n        scores.append({\"model_id\": model_id, \"epoch\": e + 1, \"time\": elapsed,\n                       \"tr_loss\": tr_loss, \"val_loss\": val_loss, \"val_avg_loss\": val_avg_loss})\n    test_pr = predict(model, test_loader, device)\n    test_pr_avg += test_pr\n    del model, optimizer\n    torch.cuda.empty_cache()","7a7e2b93":"df = pd.DataFrame(scores)\n\npd.set_option(\"precision\", 5)\nprint(\"\\nSingle model\")\nprint(df.groupby(\"epoch\")[['tr_loss', 'val_loss']].mean())\n\nif n_models > 1:\n    print(f\"\\nAvg of {n_models} models\")\n    print(df[df.model_id == n_models - 1][['epoch', 'val_avg_loss']].set_index('epoch'))","2058bde6":"test_pr_avg \/= n_models\ndf_sub = pd.DataFrame(test_pr_avg, columns=[\"A\", \"B\", \"NEITHER\"])\ndf_sub[\"ID\"] = test_df.ID\ndf_sub.to_csv(\"submission.csv\", index=False)\n","509a66f1":"I borrow some code from [pytorch-pretrained-BERT\/example](https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT\/tree\/master\/examples) and [[PyTorch] BERT Baseline (Public Score ~ 0.54)](https:\/\/www.kaggle.com\/ceshine\/pytorch-bert-baseline-public-score-0-54)."}}