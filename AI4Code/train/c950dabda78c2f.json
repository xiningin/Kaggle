{"cell_type":{"700dc2e3":"code","98c17536":"code","289ea374":"code","74ffb9ae":"code","ebf4de56":"code","e9e47ce9":"code","34a475da":"code","ab9feba8":"code","6ff7917c":"code","b7fe4473":"code","02a1fbed":"code","147f325e":"code","27a98765":"code","bfb1e71a":"code","906d728d":"code","b0372180":"code","c1ddf18b":"code","0d722433":"code","a2658ef8":"code","5362a94b":"code","8049d4ed":"code","b5f295f5":"code","f7479706":"code","ab5c9399":"code","cff29017":"code","7a1ea559":"code","f6093de3":"code","c919eea1":"code","b219a1c1":"code","ab10f49e":"code","a7c8e900":"code","42a3f082":"code","17fe21e4":"code","f13769bb":"code","21cf592a":"code","551fd85d":"code","35d10c1a":"code","44fa91e8":"code","a1f07c3f":"code","1eca9742":"code","5507a1ed":"code","477ac987":"code","526e8f0e":"code","bdcbaca7":"code","a71c3c5b":"code","9a3b2e77":"code","2ac9d2e5":"code","8f2109f7":"code","8d6a030b":"code","e82dfe9c":"code","c33c072f":"code","43772acd":"code","94aabf3f":"code","9272521e":"code","949331cc":"code","13e48990":"code","7c8333e2":"code","3e4ef112":"code","b547d0a2":"code","bd6659d5":"code","5caa6f05":"code","27a51b5a":"code","4636933d":"code","c25f7cfb":"code","3d8e6ea2":"code","0bda46c2":"code","9e39fb5a":"code","ddb53d2e":"code","118c2000":"code","835c982a":"code","be85a201":"code","23fd708a":"code","4c11c573":"code","7726ac2e":"code","177982a8":"code","b1871032":"code","2656e6da":"code","b64a990e":"code","b9ed5ba1":"code","9625c97b":"code","8286b7ca":"code","4b3fa1d0":"code","b7bb39c4":"code","73371393":"code","f6510600":"code","ac0fd7a7":"code","5dfcf03a":"code","b7aa2050":"code","891c81c3":"code","39b1171b":"code","a202e51f":"code","2bab1ff9":"code","875aa0f9":"code","3517507e":"code","8a6f076f":"code","4cd50ba5":"code","70cfc3e2":"markdown","b9740500":"markdown","c38fde45":"markdown","fa8efb63":"markdown","0c4600e9":"markdown","e67ec458":"markdown","034e0931":"markdown","0349cddc":"markdown","292084f0":"markdown","593cd841":"markdown","985aab6c":"markdown","2397c90d":"markdown","2aa8200c":"markdown","062a6da4":"markdown","38de23cf":"markdown","8ade27cc":"markdown","c7829f3a":"markdown","8646e64f":"markdown","a7e2faed":"markdown","83631ebb":"markdown","39b8ecbe":"markdown","1c760f1e":"markdown","26fcd948":"markdown","82cebb84":"markdown","75dd5650":"markdown","8fe1b14f":"markdown","dd0839d4":"markdown","ca1850df":"markdown","2eb8828b":"markdown","e5669f3d":"markdown","3b67b96a":"markdown","5ab9fcdf":"markdown","42dcc5f1":"markdown","5ec60a93":"markdown","dee029a7":"markdown","1f203da2":"markdown","65851c3f":"markdown","3329a45e":"markdown","190d6d22":"markdown","345a000f":"markdown","8dea91b9":"markdown","eba51a94":"markdown","279de71f":"markdown"},"source":{"700dc2e3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","98c17536":"data = pd.read_csv(\"..\/input\/diabetes-data-set\/diabetes.csv\")","289ea374":"data.head()","74ffb9ae":"print(\"Data has\\033[1m\" , data.shape[0] , \"\\033[0mrows and\\033[1m\", data.shape[1] , \"\\033[0mcolumns\\n\\n\")\nprint(\"Column Name \\t\\t Data Type\\n\")\nprint(data.dtypes) ","ebf4de56":"data.isnull().sum() # no missing values at first glance","e9e47ce9":"data.duplicated().sum() #no duplicated values","34a475da":"data.describe()","ab9feba8":"corrupted_data = data.loc[:,'Glucose':'BMI']","6ff7917c":"corrupted_data.replace(0,np.nan,inplace=True) # replace 0 with NaN","b7fe4473":"data.loc[:,'Glucose':'BMI'] = corrupted_data","02a1fbed":"data.describe()","147f325e":"data.isnull().sum()","27a98765":"import missingno as msno","bfb1e71a":"msno.bar(data)","906d728d":"msno.matrix(data)","b0372180":"msno.matrix(data.sort_values(by=\"Insulin\"))","c1ddf18b":"msno.heatmap(data)","0d722433":"data.dropna(subset=['Glucose','BMI'],inplace=True) # drop rows having missing values for Glucose or BMI","a2658ef8":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline","5362a94b":"from sklearn.impute import KNNImputer","8049d4ed":"data_knn = data.copy()","b5f295f5":"knn_imputer = KNNImputer(n_neighbors=5)\ndata_knn.iloc[:,:] = knn_imputer.fit_transform(data_knn)","f7479706":"data_knn.shape","ab5c9399":"data_knn.describe()","cff29017":"cleaned_data = data_knn.copy()","7a1ea559":"sns.boxplot(data=data_knn,x='Pregnancies')","f6093de3":"cleaned_data = cleaned_data[cleaned_data['Pregnancies']<=13]","c919eea1":"sns.boxplot(data=cleaned_data,x='Pregnancies')","b219a1c1":"sns.boxplot(data=data_knn,x='Glucose')","ab10f49e":"sns.boxplot(data=data_knn,x='BloodPressure')","a7c8e900":"cleaned_data = cleaned_data[cleaned_data['BloodPressure']>=30]","42a3f082":"sns.boxplot(data=cleaned_data,x='BloodPressure')","17fe21e4":"sns.boxplot(data=data_knn,x='Insulin')","f13769bb":"cleaned_data = cleaned_data[cleaned_data['Insulin']<=500]","21cf592a":"sns.boxplot(data=data_knn,x='SkinThickness')","551fd85d":"cleaned_data = cleaned_data[cleaned_data['SkinThickness']<=80]","35d10c1a":"sns.boxplot(data=data_knn,x='BMI')","44fa91e8":"cleaned_data = cleaned_data[cleaned_data['BMI']<=60]","a1f07c3f":"sns.boxplot(data=data_knn,x='Age')","1eca9742":"cleaned_data = cleaned_data[cleaned_data['Age']<=70]","5507a1ed":"print(\"We removed\",data_knn.shape[0]-cleaned_data.shape[0],\"outliers\")","477ac987":"plt.figure(figsize=(12,8))\nsns.heatmap(cleaned_data.corr(),annot=True,cmap='viridis')","526e8f0e":"sns.set_style('white')\nsns.histplot(data=cleaned_data,x='Age',y='Pregnancies',color='darkblue')","bdcbaca7":"sns.displot(data=cleaned_data,x='Pregnancies',hue='Outcome',kde=True)","a71c3c5b":"sns.displot(data=cleaned_data,x='Glucose',hue='Outcome',kind='kde')","9a3b2e77":"sns.scatterplot(data=cleaned_data,x='Glucose',y='Insulin',hue='Outcome')","2ac9d2e5":"sns.displot(data=cleaned_data,x='BloodPressure',hue='Outcome',kind='kde')","8f2109f7":"sns.scatterplot(data=cleaned_data,y='SkinThickness',x='BMI',hue='Outcome')","8d6a030b":"cleaned_data['Outcome'].value_counts(normalize=True)*100","e82dfe9c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE","c33c072f":"X = cleaned_data.drop('Outcome',axis=1)  # predictor variables\ny = cleaned_data['Outcome']  # outcome variable","43772acd":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,stratify=y,random_state=42) #with stratify=True we get same proportion in our train test split as in our original dataset","94aabf3f":"sm = SMOTE(random_state=42) # initalize SMOTE\nx_res,y_res=sm.fit_resample(X_train,y_train) # Oversample our data.","9272521e":"#select best params\nfrom sklearn.model_selection import GridSearchCV\nestimator = Pipeline([(\"scaler\", StandardScaler()),\n        (\"logistic_regression\", LogisticRegression(solver='liblinear'))])\n\nparams = {'logistic_regression__penalty' : ['l1','l2'],\n    'logistic_regression__C': np.linspace(0.001,100)\n}\ngrid = GridSearchCV(estimator, params, cv=5,verbose=True,n_jobs=-1,scoring='recall')","949331cc":"grid.fit(x_res,y_res) #fit the model","13e48990":"grid.best_params_ #best params","7c8333e2":"y_pred = grid.predict(X_test)  #predict ","3e4ef112":"from sklearn.metrics import confusion_matrix, classification_report","b547d0a2":"#plot the confusion matrix\n_, ax = plt.subplots(figsize=(12,12))\nax = sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='viridis', annot_kws={\"size\": 40, \"weight\": \"bold\"})  \nlabels = ['True', 'False']\nax.set_xticklabels(labels, fontsize=25);\nax.set_yticklabels(labels, fontsize=25);\nax.set_ylabel('Prediction', fontsize=30);\nax.set_xlabel('Ground Truth', fontsize=30)","bd6659d5":"print(classification_report(y_test,y_pred))","5caa6f05":"from sklearn.neighbors import KNeighborsClassifier","27a51b5a":"se = StandardScaler()  # we need to scale our data for knn\nX_scaled = se.fit_transform(X)","4636933d":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33,stratify=y,random_state=42)","c25f7cfb":"sm = SMOTE(random_state=42)\nx_res,y_res=sm.fit_resample(X_train,y_train)","3d8e6ea2":"#to optimize the value of k \nfrom sklearn.metrics import recall_score,f1_score\nmax_k = 40\nrecall_scores = list()\nf1_scores = list()\nfor k in range(1, max_k):\n    \n    knn = KNeighborsClassifier(n_neighbors=k, weights='distance')\n    knn = knn.fit(x_res, y_res)\n    \n    y_pred = knn.predict(X_test)\n    r = recall_score(y_pred, y_test)\n    recall_scores.append((k, round(recall_score(y_test, y_pred), 4)))\n    f1_scores.append((k, round(f1_score(y_test, y_pred), 4)))\n    \nr_results = pd.DataFrame(recall_scores, columns=['K', 'Recall'])\nf1_results = pd.DataFrame(f1_scores, columns=['K', 'F1'])","0bda46c2":"sns.set_context('talk')\nsns.set_style('ticks')\n\nplt.figure(dpi=300)\nax = r_results.set_index('K').plot(figsize=(12, 8), linewidth=6)\nax.set(xlabel='K', ylabel='Recall')\nax.set_xticks(range(1, max_k, 2));\nplt.title('KNN recall')\nplt.legend(labels='R')","9e39fb5a":"sns.set_context('talk')\nsns.set_style('ticks')\n\nplt.figure(dpi=300)\nax = f1_results.set_index('K').plot(figsize=(12, 8), linewidth=6)\nax.set(xlabel='K', ylabel='f1_score')\nax.set_xticks(range(1, max_k, 2));\nplt.title('KNN F1')\nplt.legend(labels='f1_score')","ddb53d2e":"_, ax = plt.subplots(figsize=(12,12))\nax = sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='viridis', annot_kws={\"size\": 40, \"weight\": \"bold\"})  \nlabels = ['True','False']\nax.set_xticklabels(labels, fontsize=25);\nax.set_yticklabels(labels, fontsize=25);\nax.set_ylabel('Prediction', fontsize=30);\nax.set_xlabel('Ground Truth', fontsize=30)\n### END SOLUTION","118c2000":"knn = KNeighborsClassifier(n_neighbors=13, weights='distance')\nknn = knn.fit(x_res, y_res)\ny_pred = knn.predict(X_test)\nprint(classification_report(y_test, y_pred))","835c982a":"from sklearn.svm import LinearSVC","be85a201":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,stratify=y,random_state=42)","23fd708a":"sm = SMOTE(random_state=42)\nx_res,y_res=sm.fit_resample(X_train,y_train)","4c11c573":"from sklearn.model_selection import GridSearchCV\nestimator = Pipeline([(\"scaler\", StandardScaler()),\n        (\"linear_svc\", LinearSVC(max_iter=100000))])\n\nparams = {\n    'linear_svc__C': np.linspace(0.001,100)\n}\ngrid = GridSearchCV(estimator, params, cv=5,verbose=True,n_jobs=-1,scoring='f1')","7726ac2e":"grid.fit(x_res,y_res)","177982a8":"grid.best_params_","b1871032":"y_pred = grid.predict(X_test)","2656e6da":"_, ax = plt.subplots(figsize=(12,12))\nax = sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='viridis', annot_kws={\"size\": 40, \"weight\": \"bold\"})  \nlabels = ['True','False']\nax.set_xticklabels(labels, fontsize=25);\nax.set_yticklabels(labels, fontsize=25);\nax.set_ylabel('Prediction', fontsize=30);\nax.set_xlabel('Ground Truth', fontsize=30)\n### END SOLUTION","b64a990e":"print(classification_report(y_test, y_pred))","b9ed5ba1":"from sklearn.ensemble import RandomForestClassifier","9625c97b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33,stratify=y,random_state=42)","8286b7ca":"sm = SMOTE(random_state=42)\nx_res,y_res=sm.fit_resample(X_train,y_train)","4b3fa1d0":"from sklearn.model_selection import GridSearchCV\nestimator = RandomForestClassifier(n_estimators=300)\n\nparams = { 'max_depth' : range(5,20),\n          'min_samples_split' : np.arange(2,10)\n}\ngrid = GridSearchCV(estimator, params, cv=5,verbose=True,n_jobs=-1,scoring='f1')","b7bb39c4":"grid.fit(x_res,y_res)","73371393":"y_pred = grid.predict(X_test)","f6510600":"_, ax = plt.subplots(figsize=(12,12))\nax = sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='viridis', annot_kws={\"size\": 40, \"weight\": \"bold\"})  \nlabels = ['True','False']\nax.set_xticklabels(labels, fontsize=25);\nax.set_yticklabels(labels, fontsize=25);\nax.set_ylabel('Prediction', fontsize=30);\nax.set_xlabel('Ground Truth', fontsize=30)\n### END SOLUTION","ac0fd7a7":"grid.best_params_","5dfcf03a":"print(classification_report(y_test, y_pred))","b7aa2050":"from sklearn.ensemble import VotingClassifier","891c81c3":"lr = LogisticRegression(solver='liblinear',penalty='l1',C=2.0417959183673466)\nknn = KNeighborsClassifier(n_neighbors=13, weights='distance')\nsvc = LinearSVC(penalty='l2',C=0.001)\nrf = RandomForestClassifier(max_depth=8, min_samples_split=2,n_estimators=300)","39b1171b":"eclf = VotingClassifier(estimators=[('clf1',lr),('clf2',knn),('clf3',svc),('clf4',rf)])","a202e51f":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.33, random_state=42,stratify=y)","2bab1ff9":"sm = SMOTE(random_state=42)\nx_res,y_res=sm.fit_resample(X_train,y_train)","875aa0f9":"eclf.fit(x_res,y_res)","3517507e":"y_pred = eclf.predict(X_test)","8a6f076f":"_, ax = plt.subplots(figsize=(12,12))\nax = sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='viridis', annot_kws={\"size\": 40, \"weight\": \"bold\"})  \nlabels = ['False', 'True']\nax.set_xticklabels(labels, fontsize=25);\nax.set_yticklabels(labels[::-1], fontsize=25);\nax.set_ylabel('Prediction', fontsize=30);\nax.set_xlabel('Ground Truth', fontsize=30)\n### END SOLUTION","4cd50ba5":"print(classification_report(y_test, y_pred))","70cfc3e2":"Lets observe these missing values more closely and find out if their is any relation","b9740500":"Type 1 diabeties is caused when pancreas produces very little to no insulin and thus the level of glucose increases , from the scatterplot we can clearly see that diabetic patients have high glucose and low insulin \n\n##### Note: Data is collected after 2 hour of giving an insulin injection which explains the high values of insulin.","c38fde45":"BloodPressure seems to be normally distributed for both positive and negative cases, indicating little corr as we have seen earlier. The mean BloodPressure seems to be a bit higher but is still within the normal diastolic range of 60-80 mm Hg","fa8efb63":"The dataset seems to be imbalanced with 65% of input as non diabetic , we will use SMOTE to deal with this later as we train our model","0c4600e9":"We can see heavy correlation between SkinThickness and Insulin","e67ec458":"The following conclusions can be drawn from the heatmap :\n- There is not much correlation among different predictors\n- Age and Pregnancy have a positive corr indicating that adults have more children\n- There is high corr between BMI and SkinThickness, generally people with very high BMI are considered obese thus explaining thicker skin\n- There is positive correlation among Insulin and Glucose as well which could be explained by the fact that perhaps the type 1 diabetic patients who generally have high Glucose , were given Insulin injections.\n- Glucose also has corr with our Outcome , i.e. , diabetic patients have higher level of glucose in their blood","034e0931":"# KNN","0349cddc":"- Clean the data and deal with missing values\n- Perform EDA to get a better understanding of underlying trends\n- Fit different models and tune their hyperparameter for best performance","292084f0":"# Combining models \nLets try and combine all our models and see how well it works then","593cd841":"Similar results to that of Logistic Regression ","985aab6c":"Again similar results compared to the other models, with very little to no improvement in our scores","2397c90d":"Females between age of 40 and 50 have more children while those around the age of 21 generally have 0 children","2aa8200c":"Using knnimputer we fill the missing values without impacting the variability of the dataset","062a6da4":"Looking at the above matrix we can figure out that glucose and BMI are MCAR(Missing completely at random) and insulin , SkinThickness are MNAR(Missing not at random)","38de23cf":"People who don't have diabeties have glucose normally distributed around 100 as mean , while those with diabeties have much higher level of glucose ranging between 100 and 200","8ade27cc":"# Exploratory Data Analysis","c7829f3a":"# SVC\n\nFor our next two models let's try and optimize for f1 score and see the results","8646e64f":"Verify outlier removal effect","a7e2faed":"BMI obove 60 removed ","83631ebb":"Majority of females taking part have 0 preganancies , also the number of pregnancies doesn't seem to have impact on wether someone is diabetic or not","39b8ecbe":"Still the same results , which is not surprising considering that all the models had similar predictions","1c760f1e":"Insulin level above 500 could skew our model and hence removed","26fcd948":"Age of 81 is removed as outlier","82cebb84":"Values above 13 are outliers and removed ","75dd5650":"SkinThickness of 99 removed as outlier","8fe1b14f":"KNN focused more heavily on identifying the False negatives","dd0839d4":"All the models had similar performance with recall around 0.75, which is not bad considering the size of the dataset and the imbalance.\n\nHowever perhaps employing unsupervised methods such as clustering could reveal more information and help us improve our scores.\nMore powerful ensemble methods such as XGboost could also yield better result and I would like to try that perhaps some other day.\n\nAlso other sampling methods might have yielded better results.\n\nIn the end I would like your feedback on how i could improve the performance, thank you.","ca1850df":"# Metric\nIt is important to identify the correct metric for a classification problem with unbalanced dataset as accuracy would fail \nSince we are trying to build a model to corretly identify patients with Diabeted we should prioritize our recall or f1 score.","2eb8828b":"BMI range of 18.5-24.9 is generally considered as normal while those above 30 are considerd as obsese ans BMI above 55 is considerd as Morbid Obsese\nWith increase in BMI skinthickness also increases and it seems that obses people are generally more prone to diabeties","e5669f3d":"Verify the outlier removal","3b67b96a":"# Conclusion","5ab9fcdf":"# Dataset Description","42dcc5f1":"# Random Forest","5ec60a93":"A recall of 0.75 is not that bad , lets see if we can improve upon in other models","dee029a7":"The data is taken from \"https:\/\/www.kaggle.com\/mathchi\/diabetes-data-set\"\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases.All the patients here are female 21 years or older.It contains the following columns:\n \n  - Pregnancies: Number of times pregnant\n  - Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n  - BloodPressure: Diastolic blood pressure (mm Hg)\n  - SkinThickness: Triceps skin fold thickness (mm)\n  - Insulin: 2-Hour serum insulin (mu U\/ml)\n  - BMI: Body mass index (weight in kg\/(height in m)^2)\n  - DiabetesPedigreeFunction: Diabetes pedigree function\n  - Age: Age (years)\n  - Outcome: Class variable (0 or 1)","1f203da2":"We note that minimum value for columns such as Glucose,BP,Insulin,BMI are 0 which is not possible , hence these must be missing values which are encoded as 0\nWe can use imputation techniques to deal with these","65851c3f":"BP below 30 mm Hg can be considered as outliers and hence removed","3329a45e":"# Outlier Removal","190d6d22":"# Logistic Regression","345a000f":"# Data Cleaning","8dea91b9":"# Objectives","eba51a94":"From both the recall and F1 score we can see that k=13 can be considered as an optimal value","279de71f":"No outliers detected"}}