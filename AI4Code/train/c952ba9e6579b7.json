{"cell_type":{"f068d65e":"code","56ea32d2":"code","04335951":"code","1e95bcbc":"code","63311398":"code","c118292b":"code","5079809c":"code","e122c3a8":"code","95a7524b":"code","66d2d881":"code","0882c341":"code","43cb28b4":"code","d66d1970":"code","72500a5c":"code","9b6972ab":"code","40d12166":"code","ebc78c01":"code","f599669f":"code","937c9f7d":"code","2fd21701":"code","21fa3467":"code","76e8cceb":"code","939ce65e":"code","8d3a6362":"code","227d15a6":"code","234e6484":"code","562b0048":"code","c1ecfec4":"code","1875896e":"code","5d11e512":"code","1ea44910":"code","f68fb514":"code","5b2f5729":"code","f36a3977":"code","76b20e82":"code","591f5da6":"code","7447de71":"code","66d471b9":"code","bcbf6d9f":"code","096c8c0f":"code","dfc98a24":"code","e59db8a2":"code","344ae037":"code","5a7de3a3":"code","429a67a9":"code","1742e82f":"markdown","7addcc78":"markdown","b5d0c4af":"markdown","1e3ee872":"markdown"},"source":{"f068d65e":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score , average_precision_score \nfrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve ,auc , log_loss ,  classification_report \nfrom sklearn.preprocessing import StandardScaler , Binarizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nimport time\nimport os, sys, gc, warnings, random, datetime\nimport math\nimport shap\nimport joblib\nwarnings.filterwarnings('ignore')\n\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold , cross_val_score\nfrom sklearn.metrics import roc_auc_score","56ea32d2":"df = pd.read_pickle('..\/input\/searching-for-bad-loan-data-preprocessing\/df_pp.pkl')","04335951":"# df = pd.read_pickle('..\/input\/loan-include-chargeoff\/df_pp.pkl')","1e95bcbc":"f,ax=plt.subplots(1,2,figsize=(18,8))\ndf['Loan_status'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Loan_status')\nax[0].set_ylabel('')\nsns.countplot('Loan_status',data=df,ax=ax[1])\nax[1].set_title('Loan_status')\nplt.show()","63311398":"X = df.drop('Loan_status', axis=1)\ny = df['Loan_status']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 2020, stratify = y)","c118292b":"params_xGB = {\n    'nthread':16, \n    'gamma': 0, \n    'max_depth': 6, \n    'min_child_weight': 1, \n    'max_delta_step': 0, \n    'subsample': 1.0,\n        \n    'colsample_bytree': 1.0, \n       \n    'objective':'binary:logistic',\n    'num_class':1,\n    'eval_metric':'logloss',\n    'seed':2020,\n#     'tree_method' : 'gpu_hist',\n}","5079809c":"trainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],\n                                    index=y_train.index,columns=['prediction'])\nk_fold = StratifiedKFold(n_splits=5,shuffle=True,random_state=2020)\nstart = time.time() \nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train)),\n                                          y_train.ravel()):\n    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n        X_train.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n        y_train.iloc[cv_index]\n    \n    dtrain = xgb.DMatrix(data=X_train_fold, label=y_train_fold)\n    dCV = xgb.DMatrix(data=X_cv_fold)\n    \n    bst = xgb.cv(params_xGB, dtrain, num_boost_round=2000, \n                 nfold=5, early_stopping_rounds=200, verbose_eval=100)\n    \n    best_rounds = np.argmin(np.array(bst['test-logloss-mean']))\n    bst = xgb.train(params_xGB, dtrain, best_rounds)\n    \n    loglossTraining = log_loss(y_train_fold, bst.predict(dtrain))\n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \\\n        bst.predict(dCV)\n    loglossCV = log_loss(y_cv_fold, \\\n        predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n    cvScores.append(loglossCV)\n    \n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n    \nxgb_runtime = time.time() - start    \nloglossXGBoostGradientBoosting = \\\n    log_loss(y_train, predictionsBasedOnKFolds.loc[:,'prediction'])\n\nprint( 'XGBoost Gradient Boosting Log Loss : {0:.4f} ,  XGBoost Runtime : {1:.4f}'.format(loglossXGBoostGradientBoosting ,xgb_runtime ))","e122c3a8":"preds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,'prediction']], axis=1)\npreds.columns = ['trueLabel','prediction']\npredictionsBasedOnKFoldsXGBoostGradientBoosting = preds.copy()\n\nprecision, recall, thresholds = \\\n    precision_recall_curve(preds['trueLabel'],preds['prediction'])\naverage_precision = \\\n    average_precision_score(preds['trueLabel'],preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\n        Area under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","95a7524b":"n_estimators = 10\nmax_features = 'auto'\nmax_depth = None\nmin_samples_split = 2\nmin_samples_leaf = 1\nmin_weight_fraction_leaf = 0.0\nmax_leaf_nodes = None\nbootstrap = True\noob_score = False\nn_jobs = -1\nrandom_state = 2018\nclass_weight = 'balanced'\n\nRFC = RandomForestClassifier(n_estimators=n_estimators, \n        max_features=max_features, max_depth=max_depth,\n        min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n        min_weight_fraction_leaf=min_weight_fraction_leaf, \n        max_leaf_nodes=max_leaf_nodes, bootstrap=bootstrap, \n        oob_score=oob_score, n_jobs=n_jobs, random_state=random_state, \n        class_weight=class_weight)","66d2d881":"##RandomForest with stratified 5 Fold\n\ntrainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],\n                                        index=y_train.index,columns=[0,1])\n\nstart = time.time() \nclf = RandomForestClassifier(n_estimators=30, min_samples_leaf=20, max_features=0.7, n_jobs=-1, random_state = 2020, oob_score=True)\ncv = StratifiedKFold(n_splits=5,random_state = 2020)\ny_preds_rf = np.zeros(X_test.shape[0])\nn_iter = 0 \nfor train_index,test_index in cv.split(X_train,y_train):\n    trx , tsx = X_train.iloc[train_index] , X_train.iloc[test_index]\n    vly , vlt = y_train.iloc[train_index] , y_train.iloc[test_index]\n    RFC = RFC.fit(trx,vly)   \n    loglossTraining = log_loss(vly, \\\n                                RFC.predict_proba(trx))\n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFolds.loc[tsx.index,:] = \\\n        RFC.predict_proba(tsx)  \n    loglossCV = log_loss(vlt, \\\n        predictionsBasedOnKFolds.loc[tsx.index,1])\n    cvScores.append(loglossCV)\n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\n    \n    n_iter += 1\n    cv_roc_score = roc_auc_score(y_test, RFC.predict_proba(X_test)[:,1], average = 'macro')\n    cv_precision, cv_recall, _ = precision_recall_curve(y_test,RFC.predict_proba(X_test)[:,1])\n    cv_pr_auc = auc(cv_recall, cv_precision)\n    print( '\\n#{0}, CV_ROC_AUC : {1} , RF_CV_PR_AUC : {2} '.format(n_iter ,cv_roc_score, cv_pr_auc))\n    y_preds_rf += RFC.predict_proba(X_test)[:,1]\/ cv.n_splits\nrf_runtime = time.time() - start \nrf_cv_roc_score = roc_auc_score(y_test, y_preds_rf, average = 'macro')\nrf_cv_precision, rf_cv_recall, _ = precision_recall_curve(y_test,y_preds_rf)\nrf_cv_pr_auc = auc(rf_cv_recall, rf_cv_precision)    \nloglossRandomForestsClassifier = log_loss(y_train, \n                                          predictionsBasedOnKFolds.loc[:,1])\nprint( 'Random Forest Log Loss : {0:.4f} ,  Random Forest Runtime : {1:.4f}'.format(loglossRandomForestsClassifier ,rf_runtime ))\n    ","0882c341":"preds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,1]], axis=1)\npreds.columns = ['trueLabel','prediction']\npredictionsBasedOnKFoldsRandomForests = preds.copy()\n\nprecision, recall, thresholds = precision_recall_curve(preds['trueLabel'],\n                                                       preds['prediction'])\naverage_precision = average_precision_score(preds['trueLabel'],\n                                            preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\n          Area under the curve = {0:0.2f}'.format(\n          areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","43cb28b4":"params_lightGB = {\n    'task': 'train',\n    'application':'binary',\n    'num_class':1,\n    'boosting': 'gbdt',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'metric_freq':50,\n    'is_training_metric':False,\n    'max_depth':4,\n    'num_leaves': 31,\n#     'learning_rate': 0.01,\n    'feature_fraction': 1.0,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 0,\n    'bagging_seed': 2020,\n    'verbose': 50,\n    'num_threads':16,\n    'random_state ' : 2020\n}","d66d1970":"trainingScores = []\ncvScores = []\npredictionsBasedOnKFolds = pd.DataFrame(data=[],\n                                index=y_train.index,columns=['prediction'])\nstart = time.time() \nfor train_index, cv_index in k_fold.split(np.zeros(len(X_train)),\n                                          y_train.ravel()):\n    X_train_fold, X_cv_fold = X_train.iloc[train_index,:], \\\n        X_train.iloc[cv_index,:]\n    y_train_fold, y_cv_fold = y_train.iloc[train_index], \\\n        y_train.iloc[cv_index]\n    \n    lgb_train = lgb.Dataset(X_train_fold, y_train_fold)\n    lgb_eval = lgb.Dataset(X_cv_fold, y_cv_fold, reference=lgb_train)\n    gbm = lgb.train(params_lightGB, lgb_train, num_boost_round=10000,\n                   valid_sets=lgb_eval, early_stopping_rounds=200 , verbose_eval = 500)\n    \n    loglossTraining = log_loss(y_train_fold, \\\n                gbm.predict(X_train_fold, num_iteration=gbm.best_iteration))\n    trainingScores.append(loglossTraining)\n    \n    predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'] = \\\n        gbm.predict(X_cv_fold, num_iteration=gbm.best_iteration) \n    loglossCV = log_loss(y_cv_fold, \\\n        predictionsBasedOnKFolds.loc[X_cv_fold.index,'prediction'])\n    cvScores.append(loglossCV)\n    \n    print('Training Log Loss: ', loglossTraining)\n    print('CV Log Loss: ', loglossCV)\nlgbm_runtime = time.time() - start     \nloglossLightGBMGradientBoosting = \\\n    log_loss(y_train, predictionsBasedOnKFolds.loc[:,'prediction'])\nprint( 'LightGBM Log Loss : {0:.4f} ,  LightGBM Runtime : {1:.4f}'.format(loglossLightGBMGradientBoosting ,lgbm_runtime ))","72500a5c":"preds = pd.concat([y_train,predictionsBasedOnKFolds.loc[:,'prediction']], axis=1)\npreds.columns = ['trueLabel','prediction']\npredictionsBasedOnKFoldsLightGBMGradientBoosting = preds.copy()\n\nprecision, recall, thresholds = \\\n    precision_recall_curve(preds['trueLabel'],preds['prediction'])\naverage_precision = \\\n    average_precision_score(preds['trueLabel'],preds['prediction'])\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(preds['trueLabel'],preds['prediction'])\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","9b6972ab":"predictionsTestSetRandomForests = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\npredictionsTestSetRandomForests.loc[:,'prediction'] = \\\n    RFC.predict_proba(X_test)[:,1]\nlogLossTestSetRandomForests = \\\n    log_loss(y_test, predictionsTestSetRandomForests)","40d12166":"predictionsTestSetXGBoostGradientBoosting = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\ndtest = xgb.DMatrix(data=X_test)\npredictionsTestSetXGBoostGradientBoosting.loc[:,'prediction'] = \\\n    bst.predict(dtest)\nlogLossTestSetXGBoostGradientBoosting = \\\n    log_loss(y_test, predictionsTestSetXGBoostGradientBoosting)","ebc78c01":"predictionsTestSetLightGBMGradientBoosting = \\\n    pd.DataFrame(data=[],index=y_test.index,columns=['prediction'])\npredictionsTestSetLightGBMGradientBoosting.loc[:,'prediction'] = \\\n    gbm.predict(X_test, num_iteration=gbm.best_iteration)\nlogLossTestSetLightGBMGradientBoosting = \\\n    log_loss(y_test, predictionsTestSetLightGBMGradientBoosting)","f599669f":"print(\"Log Loss of Random Forests on Test Set: \", \\\n          logLossTestSetRandomForests)\nprint(\"Log Loss of XGBoost Gradient Boosting on Test Set: \", \\\n          logLossTestSetXGBoostGradientBoosting)\nprint(\"Log Loss of LightGBM Gradient Boosting on Test Set: \", \\\n          logLossTestSetLightGBMGradientBoosting)","937c9f7d":"#RF\nprecision, recall, thresholds = \\\n    precision_recall_curve(y_test,predictionsTestSetRandomForests)\naverage_precision = \\\n    average_precision_score(y_test,predictionsTestSetRandomForests)\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = roc_curve(y_test,predictionsTestSetRandomForests)\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","2fd21701":"#XGB\nprecision, recall, thresholds = \\\n    precision_recall_curve(y_test,predictionsTestSetXGBoostGradientBoosting)\naverage_precision = \\\n    average_precision_score(y_test,predictionsTestSetXGBoostGradientBoosting)\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = \\\n    roc_curve(y_test,predictionsTestSetXGBoostGradientBoosting)\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","21fa3467":"#LGB\nprecision, recall, thresholds = \\\n    precision_recall_curve(y_test,predictionsTestSetLightGBMGradientBoosting)\naverage_precision = \\\n    average_precision_score(y_test,predictionsTestSetLightGBMGradientBoosting)\n\nplt.step(recall, precision, color='k', alpha=0.7, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='k')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\n\nplt.title('Precision-Recall curve: Average Precision = {0:0.2f}'.format(\n          average_precision))\n\nfpr, tpr, thresholds = \\\n    roc_curve(y_test,predictionsTestSetLightGBMGradientBoosting)\nareaUnderROC = auc(fpr, tpr)\n\nplt.figure()\nplt.plot(fpr, tpr, color='r', lw=2, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='k', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic: \\\nArea under the curve = {0:0.2f}'.format(areaUnderROC))\nplt.legend(loc=\"lower right\")\nplt.show()","76e8cceb":"def get_clf_eval(y_test, pred):\n    confusion = confusion_matrix(y_test, pred)\n    accuracy = accuracy_score(y_test , pred)\n    precision = precision_score(y_test, pred)\n    recall = recall_score(y_test,pred)\n    f1 = f1_score(y_test, pred)\n    print('Confusion Matrix')\n    print(confusion)\n    print('Auccuracy : {0:.4f}, Precision : {1:.4f} , Recall : {2:.4f} , F1_Score : {3:.4f}'.format(accuracy , precision, recall, f1))\n    print('------------------------------------------------------------------------------')","939ce65e":"thresholds = {0.1,0.15, 0.2,0.25, 0.3,0.35, 0.4 , 0.45 , 0.5}\n\ndef get_eval_by_threshold(y_test, pred_proba_c1, thresholds):\n    for custom_threshold in thresholds:\n        binarizer = Binarizer(threshold = custom_threshold).fit(pred_proba_c1)\n        custom_predict = binarizer.transform(pred_proba_c1)\n        print('threshold:', custom_threshold)\n        get_clf_eval(y_test, custom_predict)\n\n## get_eval_by_threshold(y_test, pred_proba[:,1].reshape(-1,1), thresholds)","8d3a6362":"### Using CPU only\n\nstart = time.time()\n\nparams_lgb={'boosting_type':'gbdt',\n           'objective': 'binary',\n           'random_state':2020,\n           'metric':'binary_logloss',\n            'metric_freq' : 50,\n            'max_depth' :4, \n            'num_leaves' : 31,\n            'learning_rate' : 0.01,\n            'feature_fraction' : 1.0,\n            'bagging_fraction' : 1.0,\n            'bagging_freq' : 0,\n            'bagging_seed' : 2020,\n            'num_threads' : 16\n           }\n\n\nlgbm_clf = LGBMClassifier(boosting_type = 'gbdt',\n           objective= 'binary',\n           metric='auc',\n#             metric_freq = 50,\n#             max_depth =4, \n#             num_leaves = 31,\n#             learning_rate = 0.01,\n#             feature_fraction = 1.0,\n#             bagging_fraction = 1.0,\n#             bagging_freq = 0,\n# #             bagging_seed = 2020,\n#             num_threads = 16,\n                          random_state = 2020)\n\nevals = [(X_test, y_test)]\nlgbm_clf.fit(X_train, y_train,  verbose = 50)\n\n\nlgbm_cpu_runtime = time.time() - start\n\nget_eval_by_threshold(y_test, lgbm_clf.predict_proba(X_test)[:,1].reshape(-1,1), thresholds)\nlgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1], average = 'macro')\nlgbm_precision, lgbm_recall, _ = precision_recall_curve(y_test,lgbm_clf.predict_proba(X_test)[:,1])\nlgbm_pr_auc = auc(lgbm_recall, lgbm_precision)\n\n\n\nprint( 'LightGBM_ROC_AUC : {0:.4f} , LightGBM_PR_AUC : {1:.4f} ,Runtime : {2:.4f}'.format(lgbm_roc_score ,lgbm_pr_auc, lgbm_cpu_runtime))","227d15a6":"start = time.time()\n\nxgb_clf = XGBClassifier(random_state = 2020)\nxgb_clf.fit(X_train, y_train, verbose = 50)\n\nxgb_gpu_runtime = time.time() - start\n\npred = xgb_clf.predict(X_test)\n\nget_eval_by_threshold(y_test, xgb_clf.predict_proba(X_test)[:,1].reshape(-1,1), thresholds)\n\nxgb_gpu_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1], average = 'macro')\n\nxgb_precision, xgb_recall, _ = precision_recall_curve(y_test,xgb_clf.predict_proba(X_test)[:,1])\nxgb_gpu_pr_auc = auc(xgb_recall, xgb_precision)\n\n\n\nprint( 'XGboost_gpu_ROC_AUC : {0:.4f} , XGboost_gpu_PR_AUC : {1:.4f} , Runtime : {2:.4f}'.format(xgb_gpu_roc_score ,xgb_gpu_pr_auc, xgb_gpu_runtime ))","234e6484":"import eli5\nfrom eli5.sklearn import PermutationImportance\nwarnings.filterwarnings('ignore')","562b0048":"perm_xgb = PermutationImportance(xgb_clf, random_state=2020).fit(X_test, y_test)\neli5.show_weights(perm_xgb, feature_names = X_test.columns.tolist())","c1ecfec4":"import shap","1875896e":"X = df.drop('Loan_status', axis=1)\ny = df['Loan_status']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 2020, stratify = y)","5d11e512":"y_trainin = y_train.to_frame()\nfor_sample_train_df = pd.concat([X_train, y_trainin], axis=1)\ny_testet = y_test.to_frame()\nfor_sample_test_df = pd.concat([X_test, y_testet], axis=1)","1ea44910":"X_ = for_sample_train_df.drop('Loan_status', axis=1)\ny_ = for_sample_train_df['Loan_status']\n\nsample_train_x, sample_test_x, sample_train_y, sample_test_y = train_test_split(X_, y_, test_size = 0.8 , random_state = 2020, stratify = y_)\n\ndel X_train, X_test, y_train, y_test , y_trainin, y_testet,","f68fb514":"gc.collect()","5b2f5729":"## Make sample for faster computation\n\nX_ = for_sample_train_df.drop('Loan_status', axis=1)\ny_ = for_sample_train_df['Loan_status']\n\nsample_train_x, sample_test_x, sample_train_y, sample_test_y = train_test_split(X_, y_, test_size = 0.80 , random_state = 2020, stratify = y_)","f36a3977":"sample_train_yin = sample_train_y.to_frame()\nfor__sample_train_df = pd.concat([sample_train_x, sample_train_yin], axis=1)\nsample_test_yin = sample_test_y.to_frame()\nfor__sample_test_df = pd.concat([sample_test_x, sample_test_yin], axis=1)","76b20e82":"for__sample_train_df.head(50)","591f5da6":"X_sampled = sample_train_x.copy()","7447de71":"#LightGBM\nimport shap\nshap.initjs()\n\n# (same syntax works for LightGBM, CatBoost, and scikit-learn models)\n\nexplainer = shap.TreeExplainer(lgbm_clf)\nshap_values = explainer.shap_values(X_sampled)","66d471b9":"shap.force_plot(explainer.expected_value[1], shap_values[1][0,:], X_sampled.iloc[0,:])","bcbf6d9f":"shap.force_plot(explainer.expected_value[1], shap_values[1][1,:], X_sampled.iloc[1,:])","096c8c0f":"shap.force_plot(explainer.expected_value[1], shap_values[1][15,:], X_sampled.iloc[15,:])","dfc98a24":"shap.force_plot(explainer.expected_value[1], shap_values[1][3,:], X_sampled.iloc[3,:])","e59db8a2":"shap.force_plot(explainer.expected_value[1], shap_values[1][3,:], X_sampled.iloc[3,:])","344ae037":"shap.force_plot(explainer.expected_value[1], shap_values[1][4,:], X_sampled.iloc[4,:])","5a7de3a3":"# # summarize the effects of all the features\n# shap.summary_plot(shap_values, X_sampled, plot_type=\"bar\")","429a67a9":"# shap.force_plot(base_value=explainer.expected_value[1], shap_values=shap_values[1], features=X_sampled.columns)","1742e82f":"Apply to Test Set","7addcc78":"### Xgboost","b5d0c4af":"### LightGBM","1e3ee872":"### Random Forest"}}