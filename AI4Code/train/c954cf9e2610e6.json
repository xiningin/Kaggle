{"cell_type":{"a53db98f":"code","b402b908":"code","cb8977e5":"code","22d4495d":"code","cd1b2e92":"code","4142f019":"code","322fd763":"code","4ee0165a":"code","b988b0ef":"code","563f9da9":"code","59360517":"code","5bcd57e3":"code","54ea3713":"code","592b604c":"code","517a4e4c":"code","46e030b9":"code","5b55bf01":"code","e819b374":"code","c614d70d":"code","a2d3e716":"code","19f58ebd":"code","7936b3fe":"code","8430be0e":"code","0d6c4d05":"code","2f3bbfc8":"code","063e5073":"code","4c2dc23b":"code","170d5bd7":"code","2a291d79":"code","965df20d":"code","17210455":"code","5a1939cb":"code","99e8cb12":"markdown","31a4115b":"markdown","8fed525e":"markdown","1e139b8d":"markdown","32ea258b":"markdown","f9752a2d":"markdown","6a3ead0b":"markdown","b4aea2b2":"markdown","95613a35":"markdown","1532555b":"markdown","93e72eac":"markdown","bc7a848f":"markdown","e8421730":"markdown","0e844d6a":"markdown","7dd270c0":"markdown","0afaccfc":"markdown","5eb81c8e":"markdown","8a57c395":"markdown","4547ce04":"markdown","c48e8c7a":"markdown","e0f2c3ea":"markdown","0062eb8c":"markdown","b4ad4592":"markdown","e8a1e3ca":"markdown","d3c60287":"markdown","8dd1fff2":"markdown","6f816ae8":"markdown","a9e5b2ee":"markdown","d643a475":"markdown"},"source":{"a53db98f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#to scale the data using z-score \nfrom sklearn.preprocessing import StandardScaler\n\n#importing PCA and TSNE\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE","b402b908":"data = pd.read_csv(\"..\/input\/auto-mpg\/auto-mpg.csv\")","cb8977e5":"data.head()","22d4495d":"data.info()","cd1b2e92":"data[\"car name\"].nunique()","4142f019":"# dropping car_name\ndata1 = data.copy()\ndata = data.drop(['car name'], axis=1)","322fd763":"# checking if there are values other than digits in the column 'horsepower' \nhpIsDigit = pd.DataFrame(data.horsepower.str.isdigit())  # if the string is made of digits store True else False\n\n# print isDigit = False!\ndata[hpIsDigit['horsepower'] == False]   # from temp take only those rows where hp has false","4ee0165a":"#Relacing ? with np.nan\ndata = data.replace('?', np.nan)\ndata[hpIsDigit['horsepower'] == False]","b988b0ef":"# Imputing the missing values with median value\ndata.horsepower.fillna(data.horsepower.median(), inplace=True)\ndata['horsepower'] = data['horsepower'].astype('float64')  # converting the hp column from object data type to float","563f9da9":"#Write your code here\ndata.describe().T","59360517":"# Uncomment and complete the code by filling the blanks \n\nfor col in data.columns:\n    print(col)\n    print('Skew :',round(data[col].skew(),2))\n    plt.figure(figsize=(15,4))\n    plt.subplot(1,2,1)\n    data[col].hist(bins=10, grid=False)\n    plt.ylabel('count')\n    plt.subplot(1,2,2)\n    sns.boxplot(x=data[col])\n    plt.show()","5bcd57e3":"plt.figure(figsize=(8,8))\nsns.heatmap(data.corr(), annot=True)\nplt.show()","54ea3713":"# scaling the data\nscaler=StandardScaler()\ndata_scaled=pd.DataFrame(scaler.fit_transform(data), columns=data.columns)","592b604c":"data_scaled.head()","517a4e4c":"#Defining the number of principal components to generate \nn=data_scaled.shape[1]\n\n#Finding principal components for the data\npca = PCA(n_components=n, random_state=1) #Apply the PCA algorithm with random state = 1\ndata_pca1 = pd.DataFrame(pca.fit_transform(data_scaled)) #Fit and transform the pca function on scaled data\n\n#The percentage of variance explained by each principal component\nexp_var = pca.explained_variance_ratio_\nexp_var","46e030b9":"# visulaize the explained variance by individual components\nplt.figure(figsize = (10,10))\nplt.plot(range(1,8), exp_var.cumsum(), marker = 'o', linestyle = '--')\nplt.title(\"Explained Variances by Components\")\nplt.xlabel(\"Number of Components\")\nplt.ylabel(\"Cumulative Explained Variance\")","5b55bf01":"# find the least number of components that can explain more than 90% variance\nsum = 0\nfor ix, i in enumerate(exp_var):\n  sum = sum + i\n  if(sum>0.90):\n    print(\"Number of PCs that explain at least 90% variance: \", ix+1)\n    break","e819b374":"pc_comps = ['PC1','PC2','PC3']\ndata_pca = pd.DataFrame(np.round(pca.components_[:3,:],2),index=pc_comps,columns=data_scaled.columns)\ndata_pca.T","c614d70d":"def color_high(val):\n    if val <= -0.40: # you can decide any value as per your understanding\n        return 'background: pink'\n    elif val >= 0.40:\n        return 'background: skyblue'   \n    \ndata_pca.T.style.applymap(color_high)","a2d3e716":"plt.figure(figsize = (7,7))\nsns.scatterplot(x=data_pca1[0],y=data_pca1[1])\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\nplt.show()","19f58ebd":"df_concat = pd.concat([data_pca1, data], axis=1)\n\nplt.figure(figsize = (7,7))\n#Create a scatter plot with x=0 and y=1 using df_concat dataframe\n#_________________________________________\nsns.scatterplot(x = 0, y=1, data = df_concat, hue='cylinders')\nplt.xlabel(\"PC1\")\nplt.ylabel(\"PC2\")\npca.explained_variance_ratio_","7936b3fe":"tsne = TSNE(n_components = 2, random_state=1)  #Apply the TSNE algorithm with random state = 1\ndata_tsne = tsne.fit_transform(data_scaled) #Fit and transform tsne function on the scaled data","8430be0e":"data_tsne.shape","0d6c4d05":"data_tsne = pd.DataFrame(data = data_tsne, columns = ['Component 1', 'Component 2'])","2f3bbfc8":"data_tsne.head()","063e5073":"sns.scatterplot(x=data_tsne.iloc[:,0],y=data_tsne.iloc[:,1])","4c2dc23b":"# Lets see scatter plot of the data w.r.t number of cyinders\nsns.scatterplot(x=data_tsne.iloc[:,0],y=data_tsne.iloc[:,1],hue=data.cylinders)","170d5bd7":"# Lets assign points to the 3 different groups\ndef grouping(x):\n    first_component = x['Component 1']\n    second_component = x['Component 2']\n    if (first_component> 0) and (second_component >0): \n        return 'group_1'\n    if (first_component >-20 ) and (first_component < 5):\n        return 'group_2'\n    else: \n        return 'group_3'","2a291d79":"data_tsne['groups'] = data_tsne.apply(grouping,axis=1)","965df20d":"sns.scatterplot(x=data_tsne.iloc[:,0],y=data_tsne.iloc[:,1],hue=data_tsne.iloc[:,2])","17210455":"data['groups'] = data_tsne['groups'] ","5a1939cb":"all_col = data.columns.tolist()\nplt.figure(figsize=(20, 20))\n\nfor i, variable in enumerate(all_col):\n    if i==7:\n        break\n    plt.subplot(4, 2, i + 1)\n    #Create boxplot with groups on the x-axis\n    sns.boxplot(y=data[variable], x=data['groups'])\n    plt.tight_layout()\n    plt.title(variable)\nplt.show()","99e8cb12":"**Observations:\n- The first principal component, PC1, is related to high values of number of cylinders, engine size, horsepower and weight and lower value of milage per galoon. This principal component seams to capture attributes that generally define lower performance cars. Bigger cars or the cars with very low performance. \n- The second principal component, PC2, is related to low values of model year. This seems like capture older cars.\n- The third principal component, PC3, is related to time taken to accelerate. This group takes higher time to accelerate which means this might define defective cars.**","31a4115b":"## t-SNE","8fed525e":"**Observations:\n- The first 2 components contributes to about 73% of the total variance. So it's ok enough to chose 2 components to measure variance when hue is number of cylinders. 5 classes of cylynders does not look properly seperated with 2 components but is good enough. It looks like 3 seperate classes instead of 5. **","1e139b8d":"#### Check the info of the data","32ea258b":"#### **Applying the TSNE embedding with 2 components for the dataframe data_scaled (random_state=1)**","f9752a2d":"#### Summary Statistics","6a3ead0b":"#### Checking values in horsepower column","b4aea2b2":"- The variable mpg has strong negative correlation with cylinders, displacement, horsepower, and weight.\n- horsepower and acceleration are negatively correlated.\n- The variable weight has strong positively correlation with horsepower, displacement and cylinders\n- model year is positively correlated with mpg.","95613a35":"**Observations:\n-  We can see that out of the 8 original features, we reduced the number of features through principal components to 3, these components explain 90% of the original variance.\n\n- So that is about 90% reduction in the dimensionality with a loss of 10% in variance.","1532555b":"#### Let's check the distribution and outliers for each column in the data","93e72eac":"**Observations:\n- Average miles per galoon for a typical car is 23.5 and 2\/3rd of data has (~23.5 + or - 7.8). Also mpg is spreded between 9 and 46.6 and over 50% of the cars has mor ethan 23 mpg..\n- Average number of cylinders is ~5 and it resides between 3 and 8. Standard deviation is ~1.8 and over 75% of the cars has more than 8.\n- Average engine displacement of a car is ~193 and minimum dispacement from given data is 68 and maximum is 455. Standard deviation is ~104.3 and over 75% of the cars has more than 262.\n- Average horsepower of a car is around 104 and it ranges between 46 and 230. Standard deviation is ~38 and over 50% of the cars has more than 93.5.\n- Average weigt of a car is around 2970 and it ranges between 1613 and 5140. Standard deviation is ~847 and over 25% of the cars has more than 2224.\n- Average time takes to accelerate is ~15.6 and it ranges between 8 and 24.8. Standard deviation is ~2.8 and over 75% of the cars has more than 17.\n- Average model year of the cars of the given data set is 76 and it ranges between 70 and 82. Standard deviation is ~4 and over 75% of the cars model year is greatee than 79.**","bc7a848f":"## Dimensional Reduction using PCA and tSNE\n\n## Objective: \n-----------------------------\nExplore the data and reduce the number of features by using dimensionality reduction techniques like PCA and TSNE and generate meaningful insights. \n\n-----------------------------\n## Dataset: \n-----------------------------\nThere are 8 variables in the data: \n\n- mpg: miles per gallon\n- cyl: number of cylinders\n- disp: engine displacement (cu. inches) or engine size\n- hp: horsepower\n- wt: vehicle weight (lbs.)\n- acc: time taken to accelerate from O to 60 mph (sec.)\n- yr: model year\n- car name: car model name","e8421730":"- There are 6 observations where horsepower is ?.\n- We can consider these values as missing values.\n- Let's impute these missing values and change the data type of horsepower column.\n- First we need to replace the ? with np.nan.","0e844d6a":"#### Checking correlation","7dd270c0":"## Principal Component Analysis","0afaccfc":"- The column 'car name' is of object data type containing a lot of unique entries and would not add values to our analysis. We can drop this column.","5eb81c8e":"**Apply the PCA algorithm with number of components equal to the total number of columns in the data with random_state=1 and check observations on the variance explained by components**","8a57c395":"#### Loading data","4547ce04":"## Importing necessary libraries and overview of the dataset","c48e8c7a":"**Let's try adding hue to the scatter plot**","e0f2c3ea":"#### **Interpret the coefficients of three principal components from the below dataframe **","0062eb8c":"**Observations:\n\n- The distribution plots show that mpg, Accept, Cylinders, displacement, weight and acceleration variables has moderate right skew. \n- Acceleration and horsepower variables has some outliers. \n- Horsepower has higher skewed destribution than all other variables. It has skewed right. Also it has outliers as well.\n- Model year has very little skewness and data destribution looks almost symmetric and acceleration also seems to be symmetric but more skewer than Model year variable.\n- It is evident from boxplots that all these variables have outliers.\n- Cylinders, displacement, weight, model year variables does not possess outliers.**","b4ad4592":"#### Scaling the data","e8a1e3ca":"**Observations**:\n\n- There are three groups in the data. Each group has different set of characteristics.\n\n- Group 1 represents **Good performance cars**. As there is High MPG, comparatively high model year (which means new) and lower # of cylinders, horsepower, engine size, weight, the car performance is higher. \n\n- Group 2 represents cars with moderate mpg with comparatively higher (compared to Group 1) # of cylinders, moderate acceleration, horsepower, displacement and weight. The model year is average and some model years overlaps with both group 1 and rest overlaps with Group 3 which means this group consist with both older and latest cars.\n\n- Group 3 represents **Weak\/Defective cars**. As there is very low MPG, comparatively low model year (which means old) and higher # of cylinders, horsepower, engine size, weight, the car performance is lower or it can be defective.**","d3c60287":"#### Scatter plot for first two principal components with hue = 'cylinders'","8dd1fff2":"**Observation:**\n\n- There are 398 observations and 8 columns in the data.\n- All variables except horsepower and car name are of numeric data type.\n- The horsepower must be a numeric data type. We will explore this further.","6f816ae8":"## Data Preprocessing and Exploratory Data Analysis","a9e5b2ee":"#### We can also visualize the data in 2 dimensions using first two principal components ","d643a475":"**Observations:\n- We see 3 clear clusters of data as (3-4), (5-6) and 8 with few outliers which are distant from each other. Cluster distribution is more clear in here than PCA.**"}}