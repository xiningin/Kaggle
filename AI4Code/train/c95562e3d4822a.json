{"cell_type":{"963416f7":"code","4d46bb2c":"code","80574076":"code","53fe4989":"code","4c33f887":"code","45789355":"code","fbfb4a0c":"code","3021cd11":"code","08d4769e":"code","490a6802":"code","c5c330fc":"code","bdca0433":"code","c3ee8998":"code","8f183c92":"code","a159933a":"code","7906dfb5":"code","027850be":"code","a5698a49":"code","7e91bce9":"code","c6417a4a":"code","3a427ee8":"code","d2aabb5c":"code","c833b98e":"code","8ffea7f0":"code","67d8e8f9":"code","211e654e":"markdown","74f1d35e":"markdown","9754e8b9":"markdown","1fa8ff0c":"markdown","84003024":"markdown","87f8bbb9":"markdown","fba3ddb2":"markdown","b956d231":"markdown","78f618a8":"markdown","37344132":"markdown","fbb4a9b9":"markdown","37150539":"markdown","6f2b1279":"markdown","acd1fe42":"markdown","dbc3d79f":"markdown","b10c950b":"markdown","5cc819f1":"markdown","47608e0b":"markdown","6e1ac4cf":"markdown"},"source":{"963416f7":"import os\nimport time\nimport math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import GroupKFold\nimport lightgbm as lgb\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nsns.set()\n\nprint(\"Files in the input folder:\")\nprint(os.listdir(\"..\/input\"))\ntrain = pd.read_csv('..\/input\/X_train.csv')\ntest = pd.read_csv('..\/input\/X_test.csv')\ny = pd.read_csv('..\/input\/y_train.csv')\nsub = pd.read_csv('..\/input\/sample_submission.csv')\nprint(\"\\nX_train shape: {}, X_test shape: {}\".format(train.shape, test.shape))\nprint(\"y_train shape: {}, submission shape: {}\".format(y.shape, sub.shape))","4d46bb2c":"train.head()","80574076":"y.head()","53fe4989":"plt.figure(figsize=(10,6))\nplt.title(\"Training labels\")\nax = sns.countplot(y='surface', data=y)","4c33f887":"y.groupby('group_id').surface.nunique().max()","45789355":"plt.figure(figsize=(22,6)) \nsns.countplot(x='group_id', data=y, order=y.group_id.value_counts().index)\nplt.show()","fbfb4a0c":"data = train.merge(y, on='series_id', how='left')\n\n# Some utility functions here\n\ndef plot_box_and_kde(num_axis, feature_group, catplot='boxplot'):\n    plt.figure()\n    fig, ax = plt.subplots(num_axis, 2,figsize=(12, 5 * num_axis))\n    j = 0\n    for i in range(num_axis):\n        axis_list = ['_X', '_Y', '_Z', '_W']\n        col = feature_group + axis_list[i]\n        j += 1\n        plt.subplot(num_axis, 2, j)\n        if catplot == 'boxplot':\n            sns.boxplot(y='surface', x=col, data=data, fliersize=0.4)\n        else:\n            sns.violinplot(y='surface', x=col, data=data)\n        j += 1\n        plt.subplot(num_axis, 2, j)\n        sns.kdeplot(train[col], label='train')\n        sns.kdeplot(test[col], label='test')\n    plt.show()\n\n\ndef quaternion_to_euler(x, y, z, w):\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n    return X, Y, Z\n    \nplot_box_and_kde(3, 'linear_acceleration')","3021cd11":"plot_box_and_kde(3, 'angular_velocity')","08d4769e":"def convert_to_euler(df):\n    euler = df.apply(lambda r: quaternion_to_euler(r['orientation_X'], r['orientation_Y'],\n                                                   r['orientation_Z'], r['orientation_W']), axis=1)\n    df['euler_X'] = np.array([value[0] for value in euler])\n    df['euler_Y'] = np.array([value[1] for value in euler])\n    df['euler_Z'] = np.array([value[2] for value in euler])\n    return df\n\ndata = convert_to_euler(data)\ntrain = convert_to_euler(train)\ntest = convert_to_euler(test)","490a6802":"plot_box_and_kde(3, 'euler')","c5c330fc":"def change1(x):\n    return np.mean(np.abs(np.diff(x)))\n\ndef change2(x):\n    return np.mean(np.diff(np.abs(np.diff(x))))\n\ndef feature_extraction(df):\n    feat = pd.DataFrame()\n    df['linear_acceleration'] = np.sqrt(df['linear_acceleration_X']**2 + df['linear_acceleration_Y']**2 + df['linear_acceleration_Z']**2)\n    df['linear_acceleration_XZ'] = np.sqrt(df['linear_acceleration_X']**2 + df['linear_acceleration_Z']**2)\n    \n    df['acceleration_X_cumsum'] = df['linear_acceleration_X'].cumsum().fillna(0)\n    df['acceleration_Y_cumsum'] = df['linear_acceleration_Y'].cumsum().fillna(0)\n    df['acceleration_Z_cumsum'] = df['linear_acceleration_Z'].cumsum().fillna(0)\n    \n    for col in df.columns[3:]:\n        feat[col + '_mean'] = df.groupby(['series_id'])[col].mean()\n        feat[col + '_std'] = df.groupby(['series_id'])[col].std()\n        feat[col + '_max'] = df.groupby(['series_id'])[col].max()\n        feat[col + '_min'] = df.groupby(['series_id'])[col].min()\n        feat[col + '_max_to_min'] = feat[col + '_max'] \/ feat[col + '_min']\n        \n        # Change 1st order\n        feat[col + '_mean_abs_change'] = df.groupby('series_id')[col].apply(change1)\n        # Change 2nd order\n        feat[col + '_mean_abs_change2'] = df.groupby('series_id')[col].apply(change2)\n        feat[col + '_abs_max'] = df.groupby('series_id')[col].apply(lambda x: np.max(np.abs(x)))\n    return feat","bdca0433":"train_df = feature_extraction(train)\ntest_df = feature_extraction(test)\ntrain_df.head()","c3ee8998":"train_df.shape","8f183c92":"le = LabelEncoder()\ntarget = le.fit_transform(y['surface'])","a159933a":"params = {\n    'num_leaves': 18,\n    'min_data_in_leaf': 40,\n    'objective': 'multiclass',\n    'metric': 'multi_error',\n    'max_depth': 8,\n    'learning_rate': 0.01,\n    \"boosting\": \"gbdt\",\n    \"bagging_freq\": 5,\n    \"bagging_fraction\": 0.812667,\n    \"bagging_seed\": 11,\n    \"verbosity\": -1,\n    'reg_alpha': 0.3,\n    'reg_lambda': 0.1,\n    \"num_class\": 9,\n    'nthread': -1\n}\n\nt0 = time.time()\ntrain_set = lgb.Dataset(train_df, label=target)\neval_hist = lgb.cv(params, train_set, nfold=10, num_boost_round=9999,\n                   early_stopping_rounds=100, seed=19)\nnum_rounds = len(eval_hist['multi_error-mean'])\n# retrain the model and make predictions for test set\nclf = lgb.train(params, train_set, num_boost_round=num_rounds)\npredictions = clf.predict(test_df, num_iteration=None)\nprint(\"Timer: {:.1f}s\".format(time.time() - t0))","7906dfb5":"v = eval_hist['multi_error-mean'][-1]\nprint(\"Validation error: {:.4f}, accuracy: {:.4f}\".format(v, 1 - v))\nplt.figure(figsize=(10, 4))\nplt.title(\"CV multiclass error\")\nnum_rounds = len(eval_hist['multi_error-mean'])\nax = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_error-mean'])\nax2 = ax.twinx()\np = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_error-stdv'], ax=ax2, color='r')","027850be":"importance = pd.DataFrame({'gain': clf.feature_importance(importance_type='gain'),\n                           'feature': clf.feature_name()})\nimportance.sort_values(by='gain', ascending=False, inplace=True)\nplt.figure(figsize=(10, 28))\nax = sns.barplot(x='gain', y='feature', data=importance)","a5698a49":"sub['surface'] = le.inverse_transform(predictions.argmax(axis=1))\nsub.to_csv('submission_kfold.csv', index=False)\nsub.surface.value_counts()","7e91bce9":"group_info = pd.DataFrame()\ngroup_info['num_groups'] = y.groupby('surface').group_id.nunique()\ngroup_info['num_samples'] = y.groupby('surface').size()\ngroup_info","c6417a4a":"num_folds = 5\n\ndef group_kfold():\n    \"\"\"Generator that yiels train and test indexes.\"\"\"\n    folds = GroupKFold(n_splits=num_folds)\n    for train_idx, test_idx in folds.split(train_df, groups=y['group_id'].values):\n        yield train_idx, test_idx\n\n\nt0 = time.time()\ntrain_set = lgb.Dataset(train_df, label=target)\neval_hist = lgb.cv(params, train_set, nfold=num_folds, num_boost_round=9999, folds=group_kfold(),\n                   early_stopping_rounds=100, seed=19)\nnum_rounds = len(eval_hist['multi_error-mean'])\nclf = lgb.train(params, train_set, num_boost_round=num_rounds)\npredictions = clf.predict(test_df, num_iteration=None)\nprint(\"Timer: {:.1f}s\".format(time.time() - t0))\nv = eval_hist['multi_error-mean'][-1]\nprint(\"Validation error: {:.4f}, accuracy: {:.4f}\".format(v, 1 - v))","3a427ee8":"plt.figure(figsize=(10, 4))\nplt.title(\"CV multiclass error\")\nnum_rounds = len(eval_hist['multi_error-mean'])\nax = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_error-mean'])\nax2 = ax.twinx()\np = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_error-stdv'], ax=ax2, color='r')","d2aabb5c":"sub['surface'] = le.inverse_transform(predictions.argmax(axis=1))\nsub.to_csv('submission_group.csv', index=False)\nsub.surface.value_counts()","c833b98e":"num_folds = 72\n\ndef logo_cv():\n    \"\"\"Generator that yiels train and test indexes.\"\"\"\n    for group_id in range(73):\n        if group_id == 27: continue\n        test_idx = list(y[y.group_id == group_id].index)\n        train_idx = [i for i in range(3810) if i not in test_idx]\n        yield train_idx, test_idx\n\nt0 = time.time()\ntrain_set = lgb.Dataset(train_df, label=target)\neval_hist = lgb.cv(params, train_set, nfold=num_folds, num_boost_round=9999, folds=logo_cv(),\n                   early_stopping_rounds=100, seed=19)\nnum_rounds = len(eval_hist['multi_error-mean'])\nclf = lgb.train(params, train_set, num_boost_round=num_rounds)\npredictions = clf.predict(test_df, num_iteration=None)\nprint(\"Timer: {:.1f}s\".format(time.time() - t0))\nv = eval_hist['multi_error-mean'][-1]\nprint(\"Validation error: {:.4f}, accuracy: {:.4f}\".format(v, 1 - v))","8ffea7f0":"plt.figure(figsize=(10, 4))\nplt.title(\"CV multiclass error\")\nnum_rounds = len(eval_hist['multi_error-mean'])\nax = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_error-mean'])\nax2 = ax.twinx()\np = sns.lineplot(x=range(num_rounds), y=eval_hist['multi_error-stdv'], ax=ax2, color='r')","67d8e8f9":"sub['surface'] = le.inverse_transform(predictions.argmax(axis=1))\nsub.to_csv('submission_logo.csv', index=False)\nsub.surface.value_counts()","211e654e":"<h2>Leave One Group Out<\/h2>\n\nIn this cross-validation scheme, each training set is constituted by all the samples except the ones related to a specific group. So we have one split for each group_id, except for the group 27, which is the only group for the hard-tiles surface.","74f1d35e":"<h3>Submission<\/h3>","9754e8b9":"<h2>Group KFold<\/h2>\n\nThe cross-validation accuracy is much higher than the LB, so using stratified KFold is probably a bad idea. Another approach is using series that are in the same recording session (group_id) for training and series from different sessions for validation. ","1fa8ff0c":"The number of series in each group can be quite different:","84003024":"<h3>Orientation<\/h3>\n\n\nFirst let's try to convert the quartenions to euler angles:","87f8bbb9":"<h2>Data Analysis<\/h2>\n\nIn this experiment, robots are using an [Inertial Measurement Unit](https:\/\/en.wikipedia.org\/wiki\/Inertial_measurement_unit), which is a a combination of accelerometers, gyroscopes and magnetometers (optional) to collect data. There is a good guide explaining how IMU devices works in this [link](http:\/\/www.starlino.com\/imu_guide.html).\n\nA few ideas about this dataset:","fba3ddb2":"<h3>Feature importance<\/h3>","b956d231":"<h3>Data structure<\/h3>\n\nEach series has 128 measurements, that's why there are almost half million rows at x_train, but only 3810 outputs (y_train). For each measurement we have ten features, which are basically the orientation, angular velocity and acceleration in three dimensions. The orientation channel has a fourth dimension since it's using [quaternions](https:\/\/en.wikipedia.org\/wiki\/Conversion_between_quaternions_and_Euler_angles).\n\n<h3>Target<\/h3>\n\nThis is a classification problem with nine possible classes (floor surfaces):","78f618a8":"<h2>Introduction<\/h2>\n\nIn this competition, participants must help robots recognize the floor surface they\u2019re standing on using data collected from IMU sensors.","37344132":"Values are distributed in a small range, except for the Z axis, where the orientation is quite different depending on the surface type.","fbb4a9b9":"<h3>Linear Acceleration<\/h3>\n\n* Linear acceleration is probably measured in m\/s\u00b2, however there are some very high values (almost 12g in y-axis).\n\n* The distribution for acceleration_X is centered at zero, while acceleration_Z is at g (-9.8 m\/s\u00b2).\n\n* While most values are the same for all surfaces (mean and quantiles in boxplot), the ranges are quite different.","37150539":"<h3>Submission<\/h3>","6f2b1279":"<h3>Angular Velocity<\/h3>\n\n* Looking at the range of values, the angular velocity might be in radians per second (rad\/s), which is also the standard unit (SI).\n* The mean here is also close for all surfaces, but ranges are different.","acd1fe42":"The following plot shows the mean error at each iteration (blue line). The red line is the standard deviation between folds:","dbc3d79f":"<h3>Submission<\/h3>","b10c950b":"<h2>Feature Engineering<\/h2>","5cc819f1":"<h2>Gradient Boosting<\/h2>\n\nI will be using lightgbm with **stratified kfold** for cross-validation. The *multi_error* metric is the ratio of misclassified samples, so the multiclass accuracy (competition metric) is just 1 - multierror.","47608e0b":"The scores are far from LB and there is a huge deviation between folds for group kfold and leave one out. Validation is a problem in this competition and we need to try more strategies.","6e1ac4cf":"<h3>Group id<\/h3>\n\nEach group_id is a unique **recording session** and has only one surface type:"}}