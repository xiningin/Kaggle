{"cell_type":{"234036b0":"code","dc994248":"code","cb5c2749":"code","28366a5c":"code","b2895853":"code","c9a463d5":"code","87f91b12":"code","1ac18c56":"code","8751305a":"code","4c0f130a":"code","01a9caf7":"code","b360c7e2":"code","ebf873dc":"code","36caa989":"code","2d126cfb":"code","df819a37":"markdown","3f40eb3e":"markdown","2c59a051":"markdown","3a66d979":"markdown","643edd5f":"markdown","615fbc68":"markdown","00c18ffe":"markdown","a45bd613":"markdown","f4a5ec03":"markdown","bbef70ff":"markdown","9fd8de5c":"markdown","1fb93bdd":"markdown"},"source":{"234036b0":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgbm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom skopt import gp_minimize\nfrom skopt.space import Real, Integer\nfrom skopt.plots import plot_convergence\nimport json\nimport string\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport lightgbm as lgb\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes,linear_model\nfrom sklearn.ensemble import RandomForestClassifier\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\nrand_seed = 13579\nnp.random.seed(rand_seed)\n\nsns.set(style=\"darkgrid\", context=\"notebook\")\n\nimport os\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/train\"))\nprint(os.listdir(\"..\/input\/test\"))","dc994248":"train_df = pd.read_csv(\"..\/input\/train\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test\/test.csv\")","cb5c2749":"train_df.head()","28366a5c":"test_df.head()","b2895853":"## Taken from SRKs Kernel\ncnt_srs = train_df['AdoptionSpeed'].value_counts()\ntrace = go.Bar(\n    x=cnt_srs.index,\n    y=cnt_srs.values,\n    marker=dict(\n        color=cnt_srs.values,\n        colorscale = 'Picnic',\n        reversescale = True\n    ),\n)\n\nlayout = go.Layout(\n    title='Target Count',\n    font=dict(size=18)\n)\n\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"TargetCount\")\n\n## target distribution ##\nlabels = (np.array(cnt_srs.index))\nsizes = (np.array((cnt_srs \/ cnt_srs.sum())*100))\n\ntrace = go.Pie(labels=labels, values=sizes)\nlayout = go.Layout(\n    title='Target distribution',\n    font=dict(size=18),\n    width=600,\n    height=600,\n)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"usertype\")","c9a463d5":"## Taken from SRKs Kernel\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    \n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train_df[\"Description\"], title=\"Word Cloud of Train\")","87f91b12":"## Taken from SRKs Kernel\nfrom wordcloud import WordCloud, STOPWORDS\n\n\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    \n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(test_df[\"Description\"], title=\"Word Cloud of Test\")","1ac18c56":"train_df[\"Description\"].fillna(\"#\",inplace=True)","8751305a":"## Taken from SRKs Kernel\nfrom collections import defaultdict\ntrain1_df = train_df[train_df[\"AdoptionSpeed\"]==0]\ntrain2_df = train_df[train_df[\"AdoptionSpeed\"]==1]\ntrain3_df = train_df[train_df[\"AdoptionSpeed\"]==2]\ntrain4_df = train_df[train_df[\"AdoptionSpeed\"]==3]\ntrain5_df = train_df[train_df[\"AdoptionSpeed\"]==4]\n\n\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"Description\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train2_df[\"Description\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\nfreq_dict = defaultdict(int)\nfor sent in train3_df[\"Description\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\nfreq_dict = defaultdict(int)\nfor sent in train4_df[\"Description\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace3 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train5_df[\"Description\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace4 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of Adoption Speed==0\", \n                                          \"Frequent words of Adoption Speed==1\",\n                                         \"Frequent words of Adoption Speed==2\",\n                                         \"Frequent words of Adoption Speed==3\",\"Frequent words of Adoption Speed==4\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\nfig.append_trace(trace3, 2, 2)\nfig.append_trace(trace4, 3, 1)\n\n\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')","4c0f130a":"## Taken from SRKs Kernel\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"Description\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train2_df[\"Description\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train3_df[\"Description\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace2 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train4_df[\"Description\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace3 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\nfreq_dict = defaultdict(int)\nfor sent in train5_df[\"Description\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace4 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=3, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent bigrams of Adoption Speed==0\", \n                                          \"Frequent bigrams of Adoption Speed==1\",\n                                         \"Frequent bigrams of Adoption Speed==2\",\n                                         \"Frequent bigrams of Adoption Speed==3\",\"Frequent bigrams of Adoption Speed==4\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig.append_trace(trace2, 2, 1)\nfig.append_trace(trace3, 2, 2)\nfig.append_trace(trace4, 3, 1)\n\n\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')","01a9caf7":"## Taken from SRKs Kernel\n## Number of words in the text ##\ntrain_df[\"num_words\"] = train_df[\"Description\"].apply(lambda x: len(str(x).split()))\ntest_df[\"num_words\"] = test_df[\"Description\"].apply(lambda x: len(str(x).split()))\n\n## Number of unique words in the text ##\ntrain_df[\"num_unique_words\"] = train_df[\"Description\"].apply(lambda x: len(set(str(x).split())))\ntest_df[\"num_unique_words\"] = test_df[\"Description\"].apply(lambda x: len(set(str(x).split())))\n\n## Number of characters in the text ##\ntrain_df[\"num_chars\"] = train_df[\"Description\"].apply(lambda x: len(str(x)))\ntest_df[\"num_chars\"] = test_df[\"Description\"].apply(lambda x: len(str(x)))\n\n## Number of stopwords in the text ##\ntrain_df[\"num_stopwords\"] = train_df[\"Description\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\ntest_df[\"num_stopwords\"] = test_df[\"Description\"].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n## Number of punctuations in the text ##\ntrain_df[\"num_punctuations\"] =train_df['Description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\ntest_df[\"num_punctuations\"] =test_df['Description'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\n\n## Number of title case words in the text ##\ntrain_df[\"num_words_upper\"] = train_df[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\ntest_df[\"num_words_upper\"] = test_df[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n## Number of title case words in the text ##\ntrain_df[\"num_words_title\"] = train_df[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\ntest_df[\"num_words_title\"] = test_df[\"Description\"].apply(lambda x: len([w for w in str(x).split() if w.istitle()]))\n\n## Average length of the words in the text ##\ntrain_df[\"mean_word_len\"] = train_df[\"Description\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\ntest_df[\"mean_word_len\"] = test_df[\"Description\"].apply(lambda x: np.mean([len(w) for w in str(x).split()]))","b360c7e2":"f, axes = plt.subplots(3, 1, figsize=(10,20))\nsns.boxplot(x='AdoptionSpeed', y='num_words', data=train_df, ax=axes[0])\naxes[0].set_xlabel('AdoptionSpeed', fontsize=12)\naxes[0].set_title(\"Number of words in each class\", fontsize=15)\n\nsns.boxplot(x='AdoptionSpeed', y='num_chars', data=train_df, ax=axes[1])\naxes[1].set_xlabel('AdoptionSpeed', fontsize=12)\naxes[1].set_title(\"Number of characters in each class\", fontsize=15)\n\nsns.boxplot(x='AdoptionSpeed', y='num_punctuations', data=train_df, ax=axes[2])\naxes[2].set_xlabel('AdoptionSpeed', fontsize=12)\naxes[2].set_title(\"Number of punctuations in each class\", fontsize=15)\nplt.show()","ebf873dc":"# Get the tfidf vectors #\ntfidf_vec = TfidfVectorizer(stop_words='english',ngram_range=(1,3),encoding='utf-8')\ntfidf_vec.fit_transform(train_df['Description'].values.astype(\"U\").tolist() + test_df['Description'].values.astype(\"U\").tolist())\ntrain_tfidf = tfidf_vec.transform(train_df['Description'].astype(\"U\").values.tolist())\ntest_tfidf = tfidf_vec.transform(test_df['Description'].astype(\"U\").values.tolist())","36caa989":"train_y = train_df[\"AdoptionSpeed\"].values\nmodel = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=2019)\nmodel.fit(train_tfidf, train_y)","2d126cfb":"import eli5\neli5.show_weights(model, vec=tfidf_vec, top=100, feature_filter=lambda x: x != '<BIAS>')","df819a37":"Baseline Model","3f40eb3e":"**Exploration of some features**","2c59a051":"**Filling up nan values**","3a66d979":"**Wordcloud for Train**\n\n***Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes*** \n","643edd5f":"**Word Frequency Plot**","615fbc68":"**Importing libraries,train and test**","00c18ffe":"**Inspired from SRKs great kernel:**\n\nhttps:\/\/www.kaggle.com\/sudalairajkumar\/simple-exploration-notebook-qiqc\n(Most code snippets used in the visualizations are directly taken from his kernel)\n\n**References to other kernels given below.**","a45bd613":"**Bigram Frequency Plot**","f4a5ec03":"**Tfidf Vectorization**","bbef70ff":"**Target Distribution**","9fd8de5c":"**Word Cloud for Test**","1fb93bdd":"**Now we will look at the important words . We will use eli5 library for the same. Thanks to this excellent kernel by   @lopuhin. **"}}