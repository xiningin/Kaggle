{"cell_type":{"ebaa6a95":"code","51f7f75e":"code","9da464b2":"code","9245c89a":"code","d191e5f3":"code","61fb5535":"code","a3576713":"code","67e596ff":"code","ae156499":"code","f7413332":"code","ec086bdf":"code","01e63f59":"code","1148a621":"code","ecfbe2f9":"code","3d6c1006":"code","172630eb":"code","ea0adb79":"code","f315cf16":"code","d8aa02d7":"code","ed9dfc21":"code","d8bff50b":"code","44269ccd":"code","c1734bd5":"code","b81417d8":"code","79e884f3":"code","c734a53c":"code","0ea4cc4d":"code","3f6b2d1e":"code","c9ac3d0c":"code","6ec83344":"code","290461ff":"code","a269cdf1":"code","32fa5305":"code","8a9b13b7":"code","ad89e285":"code","4f51f19d":"code","2fa60ec2":"code","c2e8ae4b":"code","b68f0393":"code","6fb18fc8":"code","deb54468":"code","3855184c":"code","477b6c85":"code","24b00ec7":"code","eb983b5c":"code","66e7543d":"markdown","351d375a":"markdown","834bd1ba":"markdown","798b0414":"markdown","69604da2":"markdown","e4fdca00":"markdown"},"source":{"ebaa6a95":"import pandas as pd # python dataframes\nimport numpy as np # python numerics\nimport matplotlib.pyplot as plt # python plotting\nimport seaborn as sns\n\n# Keras imports\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM, GRU, SpatialDropout1D, GlobalMaxPool1D\nfrom keras.layers.embeddings import Embedding\n\nfrom kaggle.competitions import twosigmanews # Needed to obtain training\/test data\n\nfrom tqdm import tqdm\nimport gc","51f7f75e":"# Change DEBUG to False when you're ready, Corey.\nDEBUG = False\n\n# Change YEARMIN to change the cutoff point for your data\n# All data must be greater than YEARMIN\nYEARMIN = 2011","9da464b2":"#random seeds for stochastic parts of neural network \nnp.random.seed(100)\nfrom tensorflow import set_random_seed\nset_random_seed(150)","9245c89a":"env = twosigmanews.make_env()","d191e5f3":"# Load in market data, garbage collect news data\n(market_train, _) = env.get_training_data()","61fb5535":"# Require all data to be more recent than YEARMIN\nmarket_train = market_train.loc[market_train['time'].dt.year > YEARMIN]","a3576713":"market_train.columns","67e596ff":"# # Require all TARGETS be in range (-1, 1)\n# market_train['returnsOpenNextMktres10'] = market_train['returnsOpenNextMktres10'].clip(-1,1)","ae156499":"# Are there any columns that have NA's?\n# Recall Neural Networks requires all values imputed\nprint('MARKET TRAIN:')\nfor col in market_train.columns:\n    print(col+' has '+str(market_train[col].isna().sum())+' NAs')","f7413332":"# If DEBUG, then don't read in all of the data.\nif DEBUG:\n    market_train = market_train.sample(50000, random_state=4)","ec086bdf":"# Attempt to impute by group by's median\nmarket_train['returnsClosePrevMktres1'] = market_train.groupby(['assetCode'])['returnsClosePrevMktres1'].transform(lambda x: x.fillna(x.median()))\nmarket_train['returnsOpenPrevMktres1'] = market_train.groupby(['assetCode'])['returnsOpenPrevMktres1'].transform(lambda x: x.fillna(x.median()))\nmarket_train['returnsClosePrevMktres10'] = market_train.groupby(['assetCode'])['returnsClosePrevMktres10'].transform(lambda x: x.fillna(x.median()))\nmarket_train['returnsOpenPrevMktres10'] = market_train.groupby(['assetCode'])['returnsOpenPrevMktres10'].transform(lambda x: x.fillna(x.median()))\n\n# If the assetCode has no non-null values, then impute with column median\nmarket_train = market_train.fillna(market_train.median())","01e63f59":"market_train = market_train.sort_values(['assetCode','time']) # Sort it by time for use in LSTM later\nmarket_train.reset_index(drop=True,inplace=True)\nmarket_train.head()","1148a621":"market_train.columns","ecfbe2f9":"market_train['time'] = pd.to_datetime(market_train['time'].dt.date) # Change from datetime to date for less memory and easier merge with news","3d6c1006":"# Feature Engineering\nmarket_train['margin1'] = market_train['open'] \/ market_train['close']\nmarket_train['TARGET'] = np.sign(market_train['returnsOpenNextMktres10'])","172630eb":"# # Keep last 30 of each asset\ntotal_market_obs_df = [market_train.loc[(market_train['time'].dt.year >= 2016) & (market_train['time'].dt.month >= 9)].groupby('assetCode').tail(30).drop(['universe','returnsOpenNextMktres10'], axis=1)]","ea0adb79":"LSTM_COLUMNS_TO_USE = ['time', # Time variable is necessary\n                       'assetCode', # AssetCode is necessary to perform merges\/historical analysis\n                       'universe', # binary variable indicating if entry will be used in metric\n                       'returnsOpenNextMktres10',\n                       'TARGET',\n                       'volume',\n                       'returnsClosePrevRaw1', 'returnsOpenPrevRaw1',\n                       'returnsClosePrevMktres1', 'returnsOpenPrevMktres1',\n                       'returnsClosePrevRaw10', 'returnsOpenPrevRaw10',\n                       'returnsClosePrevMktres10', 'returnsOpenPrevMktres10',\n                       'margin1',\n                     ]","f315cf16":"# Drop columns not in use\nmarket_train = market_train[LSTM_COLUMNS_TO_USE]","d8aa02d7":"gc.collect()","ed9dfc21":"# If the assetCode has no non-null values, then impute with column median\nmarket_train = market_train.fillna(market_train.median())","d8bff50b":"INFORMATION_COLS = ['time','assetCode','universe','returnsOpenNextMktres10','TARGET']\nINPUT_COLS = [f for f in market_train.columns if f not in INFORMATION_COLS]","44269ccd":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nmarket_train[INPUT_COLS] = scaler.fit_transform(market_train[INPUT_COLS])","c1734bd5":"market_train.head(20)","b81417d8":"# Adapted from: https:\/\/machinelearningmastery.com\/multivariate-time-series-forecasting-lstms-keras\/\n\ndef series_to_supervised(dataset, n_in=1):\n    data_X = []\n    data_time = []\n    data_assetCode = []\n    data_universe = []\n    data_returns = []\n    data_TARGET = []\n    \n    # input sequence (t-n, ... t-1, t)\n    for i in range(0, len(dataset)):\n        data_time.append(dataset[i][0])\n        data_assetCode.append(dataset[i][1])\n        data_universe.append(dataset[i][2])\n        data_returns.append(dataset[i][3])\n        data_TARGET.append(dataset[i][4])\n        to_append = np.append(np.zeros(shape=(max(0,n_in - 1 - i), 10)),(dataset[max(0, i - n_in+1):i+1, len(INFORMATION_COLS):]), axis=0)\n        data_X.append( to_append)\n        \n    return data_X, data_time, data_assetCode, data_universe, data_returns, data_TARGET","79e884f3":"LOOK_BACK = 15","c734a53c":"# Create LSTM input for each assetCode individually and store in a huge list\nlstm_df_list = np.empty(shape=(market_train.shape[0],LOOK_BACK,10))\n#lstm_df_list = []\nthe_time = []\nthe_assetCode = []\nthe_universe = []\nthe_returns = []\nthe_TARGET = []\n\nrow_at = 0\n\n#for assetCode in ['AA.N','ABAX.O']:#tqdm(market_train['assetCode'].unique()[1:3]):\nfor i in tqdm(market_train.groupby('assetCode')['time'].count().reset_index().values):\n    res = series_to_supervised(market_train.loc[market_train['assetCode']==i[0]].values, n_in=LOOK_BACK)\n    #lstm_df_list = np.append(lstm_df_list, np.array(res[0]), axis=0)\n    #lstm_df_list.append(np.array(res[0]))\n    lstm_df_list[row_at:row_at+i[1]] = np.array(res[0])\n    row_at = row_at + i[1]\n    the_time.append(res[1])\n    the_assetCode.append(res[2])\n    the_universe.append(res[3])\n    the_returns.append(res[4])\n    the_TARGET.append(res[5])","0ea4cc4d":"# FLATTEN LISTS\nimport itertools\n\nthe_time = list(itertools.chain.from_iterable(the_time))\nthe_assetCode = list(itertools.chain.from_iterable(the_assetCode))\nthe_universe = list(itertools.chain.from_iterable(the_universe))\nthe_returns = list(itertools.chain.from_iterable(the_returns))\nthe_TARGET = list(itertools.chain.from_iterable(the_TARGET))","3f6b2d1e":"the_TARGET[-5:]","c9ac3d0c":"del market_train\ngc.collect()","6ec83344":"print(lstm_df_list.shape)","290461ff":"from keras import callbacks","a269cdf1":"# https:\/\/medium.com\/@thongonary\/how-to-compute-f1-score-for-each-epoch-in-keras-a1acd17715a2\n\nclass Metrics(callbacks.Callback):\n    def on_train_begin(self, logs={}):\n        self._data = []\n\n    def on_epoch_end(self, batch, logs={}):\n        X_val, y_val = self.validation_data[0], self.validation_data[1]\n        y_predict = (pd.DataFrame(model.predict(X_val)) * 2) - 1 # Need to convert it back to [-1, 1] instead of [0, 1]\n        _sigmascore = sigma_scorelstm(y_val, y_predict)\n        print(\" \u2014 sigmascore: %f\" % (_sigmascore))\n\n        self._data.append({\n            'val_sigmascore': _sigmascore\n        })\n        return\n\n    def get_data(self):\n        return self._data\n\nmetrics = Metrics()","32fa5305":"train_index = [i for i in range(len(lstm_df_list))]","8a9b13b7":"def sigma_scorelstm(y_true, y_pred):\n        x_t_i = y_pred * pd.DataFrame([the_returns[i] for i in train_index]) * pd.DataFrame([the_universe[i] for i in train_index]) # Multiply my confidence by return multiplied by universe\n        data = pd.concat([pd.DataFrame([the_time[i] for i in train_index]), x_t_i], axis=1)\n        data.columns = ['day','x_t_i']\n        x_t = data.groupby('day').sum().values.flatten()\n        mean = np.mean(x_t)\n        std = np.std(x_t)\n        score_valid = mean \/ std\n        return score_valid","ad89e285":"trainX = np.array([lstm_df_list[i] for i in train_index])\ntrainY = (np.array([the_TARGET[i] for i in train_index]) + 1) \/ 2","4f51f19d":"trainY[-5:]","2fa60ec2":"model = Sequential()\nmodel.add(GRU(50, return_sequences=True, input_shape=(LOOK_BACK, trainX.shape[2])))\nmodel.add(SpatialDropout1D(0.5))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dropout(0.50))\n\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.25))\n\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy', optimizer='rmsprop') # RMS prop is supposed to be better for recurrent neural networks.\n\nhistory = model.fit(trainX, trainY, epochs=2, batch_size=1028, validation_data=(trainX, trainY), verbose=2, shuffle=True, callbacks=[metrics])","c2e8ae4b":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'valid'], loc='upper right')\nplt.show()","b68f0393":"# Gain memory\ndel trainX, trainY\ngc.collect()","6fb18fc8":"# You can only iterate through a result from `get_prediction_days()` once\n# so be careful not to lose it once you start iterating.\ndays = env.get_prediction_days()","deb54468":"# Correct LSTM columns to use\nLSTM_COLUMNS_TO_USE = [col for col in LSTM_COLUMNS_TO_USE if (col!='universe' and col!='returnsOpenNextMktres10' and col!='TARGET')]","3855184c":"# Drop this\ntotal_market_obs_df[0].drop('TARGET', axis=1, inplace=True)","477b6c85":"# Adapted from: https:\/\/machinelearningmastery.com\/multivariate-time-series-forecasting-lstms-keras\/\n\ndef series_to_supervised(dataset, curdate, n_in=1):\n    data_X = []\n    \n    # input sequence (t-n, ... t-1, t)\n    for i in range(0, len(dataset)):\n        # Only create with the prediction date\n        if dataset[i][0]==curdate:\n            to_append = np.append(np.zeros(shape=(max(0,n_in - 1 - i), 10)),(dataset[max(0, i - n_in+1):i+1, 2:]), axis=0)\n            data_X.append( to_append)\n        \n    return data_X\n","24b00ec7":"for (market_obs_df, _, predictions_template_df) in days:\n    #######################\n    # LGBM modeling:\n    \n    market_obs_df = market_obs_df.fillna(market_obs_df.median())\n    market_obs_df = market_obs_df.sort_values('assetCode')\n    \n    market_obs_df['time'] = pd.to_datetime(market_obs_df['time'].dt.date)\n        \n    # Feature Engineering\n    market_obs_df['margin1'] = market_obs_df['open'] \/ market_obs_df['close']\n    \n    # Save to history df\n    total_market_obs_df.append(market_obs_df)\n    history_df = pd.concat(total_market_obs_df[-(np.max(30)+1):]) # Store last 30 for assetCodes\n    \n    ###################################\n    # LSTM modeling:\n    \n    tmp = history_df[LSTM_COLUMNS_TO_USE]\n    \n    # If the assetCode has no non-null values, then impute with column median\n    tmp = tmp.fillna(tmp.median())\n    \n    # Scale\n    tmp[INPUT_COLS] = scaler.fit_transform(tmp[INPUT_COLS])\n    \n    # Create LSTM input for each assetCode individually and store in a huge list\n    lstm_df_list = np.empty(shape=(predictions_template_df.shape[0],LOOK_BACK,10))\n\n    row_at = 0\n\n    for asset in market_obs_df['assetCode'].unique():\n        res = series_to_supervised(tmp.loc[tmp['assetCode']==asset].values, curdate=tmp['time'].max(), n_in=LOOK_BACK)\n\n        lstm_df_list[row_at] = np.array(res)\n        row_at = row_at + 1\n        \n    trainX = np.array([lstm_df_list[i] for i in range(len(lstm_df_list))])\n    \n    yhat_lstm = model.predict(trainX)\n    yhat_lstm = yhat_lstm.flatten() # Flatten it\n    \n    yhat_lstm = pd.DataFrame(yhat_lstm)\n    preds = (yhat_lstm * 2) - 1\n    \n#     # Predict on Ensemble now\n#     ensemble = pd.concat([yhat_lgbm, yhat_goss, yhat_dart, yhat_lstm], axis=1)\n#     ensemble.columns = ['lgbm','goss','dart','lstm']\n\n#     preds = logreg.predict_proba(ensemble)[:,1]\n#     preds = (preds * 2) - 1 # Convert from [0,1] to [-1,1]\n    \n    predictions_template_df['confidenceValue'] = preds\n    env.predict(predictions_template_df)","eb983b5c":"env.write_submission_file()","66e7543d":"# Created by Corey Levinson","351d375a":"# Four columns have NA's. Let's impute them with the median of the group","834bd1ba":"# Light preprocessing:","798b0414":"# Predictions:","69604da2":"# Train LSTM model now","e4fdca00":"# Hypothesis: I think i dont have enough RAM to construct the list. So I am reducing amount of information being fed."}}