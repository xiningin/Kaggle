{"cell_type":{"de4db9e5":"code","dd2d0f6b":"code","dd6ad79d":"code","f818bbe6":"code","1d9275ae":"code","ac4b793d":"code","06df0c39":"code","2a8d1b90":"code","a6c147be":"code","c2b653dd":"code","9b1b06ee":"code","2aa9397f":"code","5bb11626":"code","7a491d85":"code","71d097c1":"code","ec815912":"code","c8ddb7a7":"code","e7072490":"code","20c65540":"code","be97f98b":"code","0b2d961e":"code","53708f00":"code","935b92de":"code","78befe59":"code","644401c9":"code","59b13d5a":"code","e5db696f":"code","a35e2a78":"code","afe72249":"code","72f01cf6":"code","93906faf":"code","c252340b":"code","4ab4f526":"code","53eb077d":"code","776d204f":"code","1c783048":"code","2f85ee96":"code","e9d14a4a":"code","19fe7c45":"code","0aaf3932":"code","0f0d0bb1":"code","0b3f5041":"code","8bd24efa":"code","0649c2d9":"code","de46457c":"code","33941eff":"code","05439105":"code","a990c220":"code","6a86622d":"code","f9128117":"code","e15515c5":"code","8215e83b":"code","f2fceb0c":"code","a1852fc2":"code","be4bf609":"code","dcf07917":"code","22a17cb7":"code","f5e4b5ec":"markdown","6575b059":"markdown","dda32b05":"markdown","a17cf53d":"markdown","c4252548":"markdown","dc4e5ad4":"markdown","74cc58ed":"markdown","eeb82809":"markdown","43699c30":"markdown","d6c55906":"markdown","5731ca97":"markdown","30bd0a75":"markdown","e1bc08dd":"markdown","2c9e623d":"markdown","0f1ace1e":"markdown","ce7386ac":"markdown","b851e6aa":"markdown","18bae43c":"markdown","248c77e5":"markdown","58397c3f":"markdown","52705f81":"markdown","2cf4e1c4":"markdown","bb19ec1d":"markdown","e4973a71":"markdown","e64491eb":"markdown","a7659704":"markdown","3ab86aa6":"markdown","6321bf92":"markdown","69d127ca":"markdown","baaf3813":"markdown","5abec726":"markdown","94f83773":"markdown","ced78e3e":"markdown","e4643ccb":"markdown","756218ae":"markdown","abcb7cd3":"markdown","ebdc5905":"markdown","b555458d":"markdown","689e4c1c":"markdown","bb9b394d":"markdown","7e5d82a5":"markdown"},"source":{"de4db9e5":"import os, sys\n\nimport numpy as np\nimport pandas as pd\n\n# %% deep learning related \nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\n\n# upload the data from local drive\n# from google.colab import files\nimport io\nimport torchvision.transforms as transforms\nfrom sklearn.utils import shuffle\nfrom torch.autograd import Variable\nimport re\nimport seaborn as sns","dd2d0f6b":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","dd6ad79d":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest_passenger = test_data['PassengerId']","f818bbe6":"train_data.head()","1d9275ae":"train_data.info()","ac4b793d":"dataset = pd.concat(objs = [train_data, test_data], axis = 0).reset_index(drop = True)","06df0c39":"dataset.isnull().sum()","2a8d1b90":"sns.heatmap(train_data[['Survived', 'Age', 'SibSp', 'Parch', 'Fare']].corr(),annot=True, fmt = \".2f\",cmap = 'coolwarm')","a6c147be":"sns.catplot(x = 'Survived', y = 'Age', data = train_data, aspect=2, kind = 'violin')","c2b653dd":"dataset['Age'].fillna(dataset['Age'].median(), inplace = True)","9b1b06ee":"dataset['Children'] = dataset['Age'].map(lambda i:1 if i < 12 else 0)\ndataset['Teenage'] = dataset['Age'].map(lambda i:1 if 12 <= i < 20 else 0)\ndataset['Young Adult'] = dataset['Age'].map(lambda i:1 if 20 <= i < 30 else 0)\ndataset['Adult'] = dataset['Age'].map(lambda i:1 if 30 <= i < 50 else 0)\ndataset['Old Adult'] = dataset['Age'].map(lambda i:1 if 50 <= i < 60 else 0)\ndataset['Elder'] = dataset['Age'].map(lambda i:1 if 60 <= i  else 0)","2aa9397f":"dataset.drop(labels = ['Age'], axis = 1, inplace = True)","5bb11626":"sns.distplot(dataset['Fare'])","7a491d85":"dataset['Fare'] = dataset['Fare'].map(lambda i: np.log(i) if i >0 else 0)","71d097c1":"sns.distplot(dataset['Fare'])","ec815912":"dataset['FareLow'] = dataset['Fare'].map(lambda i:1 if i <= 2 else 0)\ndataset['FareMedian'] = dataset['Fare'].map(lambda i:1 if 2 < i <= 3 else 0)\ndataset['FareHigh'] = dataset['Fare'].map(lambda i:1 if 3 <i <= 5 else 0)\ndataset['FareExpensive'] = dataset['Fare'].map(lambda i:1 if i > 5 else 0)","c8ddb7a7":"dataset.drop(labels = ['Fare'], axis = 1, inplace = True)","e7072490":"sns.catplot(x = 'SibSp', y = 'Survived', data = train_data, kind=\"bar\", ci=None, aspect=3, palette= 'Set3')","20c65540":"sns.catplot(x = 'Parch', y = 'Survived', data = train_data, kind=\"bar\", ci=None, aspect=3, palette= 'Set2')","be97f98b":"dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","0b2d961e":"sns.catplot(x = 'FamilySize', y = 'Survived', data = dataset, kind=\"bar\", ci=None, aspect=3, palette='Set1')","53708f00":"dataset['Single'] = dataset['FamilySize'].map(lambda i:1 if i == 0 else 0)\ndataset['SmallFam'] = dataset['FamilySize'].map(lambda i:1 if i == 2 else 0)\ndataset['MedianFam'] = dataset['FamilySize'].map(lambda i: 1 if 3<= i <=4 else 0)\ndataset['LargeFam'] = dataset['FamilySize'].map(lambda i:1 if i>5 else 0)","935b92de":"dataset.drop(['SibSp', 'Parch','FamilySize'], axis = 1, inplace = True)","78befe59":"sns.catplot(x = 'Embarked', y = 'Survived', data = train_data, kind=\"bar\", ci=None, aspect=2, palette= 'Set3')","644401c9":"  dataset['Embarked'].replace('S', 1, inplace = True)\n  dataset['Embarked'].replace('C', 2, inplace = True)\n  dataset['Embarked'].replace('Q', 3, inplace = True)","59b13d5a":"dataset['Embarked'].fillna(1, inplace = True)","e5db696f":"dataset = pd.get_dummies(dataset, columns = ['Embarked'], prefix = ['Em_Type'])","a35e2a78":"dataset['Sex'].replace('female', 1, inplace = True)\ndataset['Sex'].replace('male', 0, inplace = True)","afe72249":"NameTitle = [i.split(',')[1].split('.')[0].strip() for i in dataset['Name']]\n\n# the axis labels are collectively referred to as the index with pd.Series\ndataset['Title'] = pd.Series(NameTitle)\n\nfig = sns.catplot(x = 'Title', data = dataset, kind = 'count', aspect = 3, palette = 'Set2')","72f01cf6":"dataset['Title'] = dataset['Title'].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndataset['Title'] = dataset['Title'].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})","93906faf":"sns.catplot(x = 'Title', y = 'Survived', data = dataset,  kind=\"bar\", ci=None, aspect=2, palette= 'Set1')","c252340b":"dataset.drop(labels = ['Name'], axis = 1, inplace = True)","4ab4f526":"dataset = pd.get_dummies(dataset, columns = ['Title'], prefix = ['Title_Type'])","53eb077d":"sns.catplot(x = 'Pclass', y = 'Survived', data = train_data, kind=\"bar\", ci=None, aspect=2, palette= 'Set3')","776d204f":"dataset['Pclass'] = dataset['Pclass'].astype('category')\ndataset = pd.get_dummies(dataset, columns = ['Pclass'], prefix = 'Pc')","1c783048":"dataset['Cabin'].fillna('X', inplace = True)","2f85ee96":"dataset['Cabin'] = dataset['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())","e9d14a4a":"sns.catplot(x = 'Cabin', y = 'Survived', data = dataset, kind = 'bar', ci = None, aspect=2, palette= 'Set3')","19fe7c45":"dataset = pd.get_dummies(dataset, columns = ['Cabin'], prefix = 'Ca')","0aaf3932":"drop_column = ['PassengerId','Ticket']\ndataset.drop(drop_column, axis = 1, inplace = True)","0f0d0bb1":"dataset.head()","0b3f5041":"norm_train = dataset[:round(0.8*len(train_data))]\nnorm_val = dataset[round(0.8*len(train_data)):len(train_data)]\nnorm_test = dataset[len(train_data):]","8bd24efa":"x_train = norm_train.iloc[:,1:].values\ny_train = norm_train.iloc[:,0].values\nprint(x_train.shape)\nprint(y_train.shape)","0649c2d9":"x_val = norm_val.iloc[:,1:].values\ny_val = norm_val.iloc[:,0].values","de46457c":"x_test = norm_test.iloc[:,1:].values\nx_test.shape","33941eff":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(len(x_train[0]), 30)\n        self.fc2 = nn.Linear(30, 40)\n        self.fc3 = nn.Linear(40, 20)\n        self.fc4 = nn.Linear(20, 2)\n        self.dropout = nn.Dropout(0.5)\n        \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc2(x))\n        x = self.dropout(x)\n        x = F.relu(self.fc3(x))\n        x = self.dropout(x)\n        x = F.softmax(self.fc4(x),dim = -1) \n        return x\nmodel = Net()","05439105":"x_train_Py = x_train.copy()\ny_train_Py = y_train.copy()\nx_val_Py = x_val.copy()\ny_val_Py = y_val.copy()\nx_test_Py = x_test.copy()","a990c220":"batch_size = 60\nbatch_num = len(x_train_Py) \/\/ batch_size\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n\nnum_epoch = 201\ntrain_loss = 0\n\nfor epoch in range(num_epoch):\n    # model.train()\n    x_train_Py, y_train_Py = shuffle(x_train_Py, y_train_Py)\n    for i in range(batch_num):\n      start = i * batch_size\n      end = start + batch_size\n      x_var = Variable(torch.FloatTensor(x_train_Py[start:end]))\n      y_var = Variable(torch.LongTensor(y_train_Py[start:end]))\n      optimizer.zero_grad()\n      out = model(x_var)\n      loss = criterion(out, y_var)\n      loss.backward()\n      optimizer.step()\n\n      values, labels = torch.max(out, 1) \n      # returns the maximum value and its position\n      train_correct = np.sum(labels.data.numpy() == y_train_Py[start:end])\n      train_loss += loss.item()*batch_size\n\n    train_loss = train_loss\/len(x_train_Py)\n\n    if epoch % 10 == 0:\n      print(\"Epoch: {} \\tTrain AVG Loss: {} \\tTrain Accuracy: {}\".format(epoch, train_loss, train_correct \/ len(y_train_Py[start:end]) ))","6a86622d":"x_val_var = Variable(torch.FloatTensor(x_val_Py), requires_grad=True)\nwith torch.no_grad():\n    result = model(x_val_var)\nvalues, labels = torch.max(result, 1)\nnum_correct = np.sum(labels.data.numpy() == y_val_Py)\nprint('Accuracy {:.2f}'.format(num_correct \/ len(y_val_Py)))","f9128117":"x_test_var = Variable(torch.FloatTensor(x_test_Py), requires_grad=True)\nwith torch.no_grad():\n  test_result = model(x_test_var)\nvalues, labels = torch.max(test_result,1)\nSurvived = labels.data.numpy()","e15515c5":"#import csv\n\n#test_passenger = test_data['PassengerId']\n\n#test_data.head()\n\n# submission = [['PassengerId', 'Survived']]\n\n# for i in range(len(Survived)):\n#   submission.append([test_passenger[i], Survived[i]])","8215e83b":"# with open('submission.csv', 'w') as submissionFile:\n#     writer = csv.writer(submissionFile)\n#     writer.writerows(submission)\n\n# files.download(\"submission.csv\")","f2fceb0c":"from keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten\nfrom keras.optimizers import SGD, Adam\nfrom keras.utils import np_utils\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold","a1852fc2":"x_train_ka = x_train.copy()\ny_train_ka = y_train.copy()\nx_val_ka = x_val.copy()\ny_val_ka = y_val.copy()\nx_test_ka = x_test.copy()","be4bf609":"from keras.utils import to_categorical\n\ny_train_ka = to_categorical(y_train_ka)\ny_val_ka = to_categorical(y_val_ka)","dcf07917":"print(y_train_ka.shape)\nprint(y_val_ka.shape)","22a17cb7":"if __name__ == '__main__':\n    \n    model = Sequential() \n\n    model.add(Dense(input_dim = len(x_train_ka[0]),units=100, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(units = 50, activation = 'relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(units = 10, activation = 'relu'))\n    model.add(Dense(units = 2, activation='softmax'))\n    model.summary()\n\n    model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n\n    model.fit(x_train_ka, y_train_ka, batch_size = 60, epochs = 100)\n\n    result_train = model.evaluate(x_train_ka, y_train_ka)\n    result_val = model.evaluate(x_val_ka,y_val_ka)\n\n    pred_result = np.argmax(model.predict(x_test_ka,verbose=0),axis = -1)\n\n    # submission = pd.DataFrame({'PassengerId': test_passenger, 'Survived': pred_result}).to_csv('submissionKeras.csv', index=False, header=True)\n    print('Training Accuracy:',result_train[1])\n    print('Validation Accuracy:',result_val[1])\n\n","f5e4b5ec":"- Fare distribution is very skewed.   \n\n- This can lead to overweight very high values in the model, even if it is scaled.  \n- In this case, it is better to transform it with the log function to reduce this skew.  ","6575b059":"## 3.2 Model training (Pytorch)","dda32b05":"## 2.2 Categorical values","a17cf53d":"- Only `Fare` feature seems to have a significant correlation with the `Survived`.","c4252548":"`PassengerId` & `Ticket` (drop off)","dc4e5ad4":"# 2. Feature analysis","74cc58ed":"- Get Title from Name","eeb82809":"- Take the prefix","43699c30":"- Drop off the Name column","d6c55906":"# 1. Load and check the data","5731ca97":"`Cabin`","30bd0a75":"- We can see that median size of family (SibSp + Parch) has more chance to survive.  \n- So I choose to add a family size feature as the sum of SibSp and Parch.","e1bc08dd":"## 3.4 Testing & exporting","2c9e623d":"- Drop off the fare column","0f1ace1e":"# 3. Build the model","ce7386ac":"- Group the title and convert to categorical values","b851e6aa":"## 2.1 Numerical values","18bae43c":"- Drop off SibSp, Parch, FamilySize","248c77e5":"`Sex`\n- Convert to numerical values","58397c3f":"- Convert to categorical values","52705f81":"- Group the Age and convert to the categorical values","2cf4e1c4":"Family size","bb19ec1d":"## 1.3 Check missing values","e4973a71":"## 1.2 Join train and test datasets  \n- To obtain the same number of features for following feature analysis.","e64491eb":"## 3.1 split the dataset back into training and testing set","a7659704":"- Fill the missing value of age with meadian value.","3ab86aa6":"## 1.1 Load data","6321bf92":"`Embarked`","69d127ca":"`Fare` distribution","baaf3813":"# 0. References  \nhttps:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling by Yassine Ghouzam\nhttps:\/\/www.kaggle.com\/frtgnn\/introduction-to-pytorch-a-very-gentle-start by Firat Gonen  \nThank you :)","5abec726":"- Group the data in family size","94f83773":"- copy the data for Pytorch","ced78e3e":"`Name`","e4643ccb":"`Pclass`","756218ae":"## 3.3 Validate","abcb7cd3":"- Group the fare and convert to categorical values","ebdc5905":"- Fill the missing values of Embarked with most frequent class 'C'.","b555458d":"- Passengers at very young age have higher survival rate.  \n- No significant difference between median value of age on survived and non-survived subpopulation.","689e4c1c":"- Drop off the Age column","bb9b394d":"`Age` distribution","7e5d82a5":"# 4. Keras"}}