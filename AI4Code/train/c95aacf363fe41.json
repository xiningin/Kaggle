{"cell_type":{"7a140f8c":"code","4e651bc0":"code","170a3c61":"code","0ee2262e":"code","348da68e":"code","5cd78ce9":"code","aa6f3428":"code","803833af":"code","b7958c8e":"code","986e0b1d":"code","4644225c":"code","b8e93279":"code","5466deb7":"code","5ff54d52":"code","4a3c0bd1":"code","567c30ed":"code","5c4d6b31":"code","f3ed1567":"code","0a5fd0c9":"code","e30cf6eb":"code","38e5c9bd":"code","95fbff28":"code","2bd04481":"code","1948d97f":"code","b32765d6":"code","55aa6e64":"code","a7e73bea":"code","2f883ef0":"code","6ff4b9eb":"code","2e19d509":"code","4220bfcc":"code","36694176":"code","bd007be1":"code","c6c83086":"code","9e7521e5":"code","66b8f7ab":"code","dce15375":"code","25c48f93":"code","40eef7b2":"code","04c43c1e":"code","fc47293d":"code","8bb7db7e":"code","0f07df71":"code","6d9f7902":"code","fc36bcf3":"code","9da8cda8":"code","59643392":"code","2ffde0c5":"code","fce14d83":"code","96b7875d":"code","78473dcb":"code","f59e5093":"code","c55b1697":"code","e95c9507":"markdown","dbd4a67e":"markdown","8b61fea9":"markdown","f5de3dfd":"markdown","86c9885a":"markdown","03ba7dec":"markdown","9941cb0d":"markdown","181dbdbf":"markdown","61fd8a6c":"markdown","bc707cd7":"markdown","91e3e4d4":"markdown","c03710b1":"markdown","4d7cbbfa":"markdown","3c1eeff0":"markdown","c3d23178":"markdown","77153003":"markdown","0da97a7e":"markdown","f8fb4f34":"markdown","1199d278":"markdown","36cce0cb":"markdown","d440a5c3":"markdown","1dae93e3":"markdown","772682f5":"markdown","9047b4d5":"markdown","4b0da1ce":"markdown","6bdadab8":"markdown","87b05943":"markdown","790f076b":"markdown","1c603aeb":"markdown","2174eb0d":"markdown","a8b8802f":"markdown","2658c22a":"markdown","1637c9c5":"markdown","9397113d":"markdown","ba7bc3fd":"markdown","fb8e4a02":"markdown","1460afa9":"markdown","0d0bd748":"markdown","1de072fc":"markdown","5fa08f7d":"markdown","8917ec30":"markdown","3a18a9b3":"markdown","539e3e05":"markdown","4bcd2953":"markdown","c0f70472":"markdown","0d21de79":"markdown","2d98807f":"markdown","0181242a":"markdown","8098ee42":"markdown","4d18e85d":"markdown","3cdfa6e4":"markdown","9e901943":"markdown","067b2af2":"markdown","27d02595":"markdown","c902dddc":"markdown","ca5cd86c":"markdown","3a5b09d6":"markdown","ccd004ce":"markdown","c3597b5d":"markdown","055a8dc5":"markdown","c22621a9":"markdown","79174972":"markdown","a51d593a":"markdown","ddcb5557":"markdown","8eea3053":"markdown","ad15380d":"markdown","629cd00e":"markdown","9ac3f9ae":"markdown","d4442cb2":"markdown","e144b12f":"markdown","9e499a47":"markdown","b1eabdfa":"markdown","aada2ed4":"markdown","a5f590b8":"markdown","94675631":"markdown","971efd0a":"markdown","f329f30e":"markdown","1da8ac0e":"markdown","fc80f326":"markdown","3f1d4dde":"markdown","6c920624":"markdown","ababc62b":"markdown","34b95957":"markdown","d70fa67f":"markdown","caab3a02":"markdown","0424d9ca":"markdown","0e8b4c4d":"markdown"},"source":{"7a140f8c":"import numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#override the matplotlib style of graphs wiht seaborn.\nsns.set()","4e651bc0":"raw_data = pd.read_csv(\"..\/input\/housesalesprediction\/kc_house_data.csv\")\nraw_data.head()","170a3c61":"raw_data.info()","0ee2262e":"raw_data.describe(include='all')\n#We observe no missing values at first which we confirm later. \n#There seem to be a lot of house with some that are exceptionally priced.","348da68e":"raw_data.isnull().sum()","5cd78ce9":"data = raw_data.drop(['id','date'],axis = 1)\ndata.head()","aa6f3428":"#change the size of the figure using this matplotlib me.thod\nplt.subplots(figsize=(15,10))\n\n#plot a correalation heatmap using seaborn. Border the squares with black color, show the correaltion index and round it up.\nsns.heatmap(data.corr(), annot = True,linewidths=.5,linecolor='black',fmt = '1.1f')\n\n#give a title to the map and display it.\nplt.title('correlation heatmap',size = 18)\nplt.show()","803833af":"#We use this in-built seaborn method to plot the specified variables and display regression lines to summarize the trends.\nsns.pairplot(data,vars = [\"price\",\"bedrooms\",\"bathrooms\",\"floors\"], kind =\"reg\")","b7958c8e":"#pandas mehtod to obtain the unique values of this variable to understand which values have been taken.\ndata['bedrooms'].unique()","986e0b1d":"plt.subplots(figsize=(12,10))\n\n#seaborn method to plot a boxplot using the specified variables from the dataset.\nsns.boxplot(x=\"bedrooms\", y = \"price\",data= data)\n\nplt.title('price vs no of bedrooms',size = 18)\nplt.show()","4644225c":"data['bathrooms'].unique()","b8e93279":"plt.subplots(figsize=(12,10))\n\n#The underscore is a dummy variable used for making it 2D.\n_=plt.hist(data['bathrooms'],color='salmon')\n_=plt.xlabel('no of bathrooms')\n_=plt.ylabel('price')\n\nplt.title('price vs no of bathrooms',size = 18)\nplt.show()","5466deb7":"data['floors'].unique()","5ff54d52":"plt.subplots(figsize=(15,10))\n\n#seaborn method for plotting a bargraph.\nsns.barplot(x=\"floors\",y=\"price\",data=data,palette=\"Blues_d\")\n\nplt.title('price vs no of floors',size = 18)\nplt.show()","4a3c0bd1":"\n#seaborn method for a 'relpot':view acts as a further breakdown dimension. We change the look of the graph by using another color pallate.\nsns.relplot(x=\"sqft_living\",y=\"price\",hue=\"waterfront\",col=\"view\",palette=[\"g\", \"r\"],data=data)\n\n","567c30ed":"sns.pairplot(data, vars = [\"sqft_living\",\"sqft_lot\",\"sqft_basement\",\"sqft_living15\",\"sqft_lot15\"], kind =\"reg\")","5c4d6b31":"data['grade'].unique()","f3ed1567":"plt.subplots(figsize=(12,10))\n\nsns.boxplot(x=\"grade\", y = \"price\",data= data,palette=\"Set3\")\n\nplt.title('price vs grade',size = 18)\nplt.show()","0a5fd0c9":"data['condition'].unique()","e30cf6eb":"plt.subplots(figsize=(12,10))\n\nsns.boxplot(x=\"condition\", y = \"price\",data= data,palette=\"Set1\")\n\nplt.title('price vs condition',size = 18)\nplt.show()","38e5c9bd":"plt.subplots(figsize=(15,10))\n\n#we use a scatterplot to analyze the relationship between price and grade and further break it down using condiiton.\nsns.scatterplot(x=\"grade\",y=\"price\",hue=\"condition\",size=\"condition\",sizes=(20, 200),data=data)\n\nplt.title('relationship between price,grade and condition',size = 18)\nplt.show()","95fbff28":"plt.subplots(figsize=(15,15))\n\n#this matplotlib method gives us the distribution of the counts of the first 50 observations and the year in which they were built. \n#the argument passed in displays the percentage upto the first decimal place.\ndata.yr_built.value_counts().head(50).plot.pie(autopct='%1.1f%%')\n\nplt.title('year built pie chart',size = 18)\nplt.show()","2bd04481":"data.yr_built.value_counts()","1948d97f":"plt.subplots(figsize=(12,10))\n\nplt.scatter(data['long'],data['lat'],color=\"purple\")\n\n#we set the limits according to the cartographical convention.\nplt.xlim(-180,180)\nplt.ylim(-180,180)\n\nplt.xlabel('longitude')\nplt.ylabel('latitude')\n\nplt.title('distribution of houses',size = 18)\nplt.show()","b32765d6":"#we zoom in for a better picture.\n\nplt.subplots(figsize=(12,10))\n\nplt.scatter(data['long'],data['lat'],color=\"purple\")\n\n#note that the coordinates have been selected based on the output of the previous scatterplot and hence won't be 100% accurate.\nplt.xlim(-121.2,-122.6)\nplt.ylim(47,47.9)\n\nplt.xlabel('longitude')\nplt.ylabel('latitude')\n\nplt.title('distribution of houses closeup',size = 18)\nplt.show()","55aa6e64":"plt.subplots(figsize=(12,10))\n\ny=data['price']\nx=data['sqft_living']\n\nplt.scatter(x,y,color='green')\n\nplt.title('price vs living area',size = 18)\nplt.show()\nplt.show()","a7e73bea":"plt.subplots(figsize=(8,8))\n\n#this in-built seaborn method plot the necessary graph.\nsns.distplot(data['price'],color='crimson')\n\nplt.title('pdf of price',size = 18)\nplt.show()","2f883ef0":"plt.subplots(figsize=(8,8))\n\n#we create a new variable to contain the observations in the 99th percentile, that is the most dramatic outliers.\nq = data['price'].quantile(0.99)\n\n#we store it in a new data fram that contains all the observations except for the top 1 percentile. They would normally represent some luxury houses.\ndata_1 = data[data['price']<q]\ndata_1.describe(include = \"all\")\n\nsns.distplot(data_1['price'],color='crimson')\n\nplt.title('pdf of price less than the 99th percentile',size = 18)\nplt.show()","6ff4b9eb":"plt.subplots(figsize=(8,8))\n\nsns.distplot(data_1['bedrooms'],color='m')\n\nplt.title('pdf of no of bedrooms',size = 18)\nplt.show()","2e19d509":"plt.subplots(figsize=(8,8))\n\np=data_1['bedrooms'].quantile(0.99)\ndata_2 = data_1[data_1['bedrooms']<p]\n\nsns.distplot(data_2['bedrooms'],color='m')\n\nplt.title('pdf of no of bedrooms less than 99th percentile',size = 18)\nplt.show()","4220bfcc":"plt.subplots(figsize=(12,10))\n\nsns.distplot(data_2['sqft_living'],color='pink')\n\nplt.title('pdf of living area',size = 18)\nplt.show()","36694176":"plt.subplots(figsize=(8,8))\np = data_2['sqft_living'].quantile(0.99)\n\ndata_3 = data_2[data_2['sqft_living']<p]\n\nsns.distplot(data_3['sqft_living'],color='pink')\n\nplt.title('pdf of price less than 99th percentile',size = 18)\nplt.show()","bd007be1":"plt.subplots(figsize=(8,8))\n\nsns.distplot(data_2['sqft_basement'],color='cyan')\n\nplt.title('pdf of basement',size = 18)\n\nplt.show()","c6c83086":"plt.subplots(figsize=(8,8))\n\np = data_3['sqft_basement'].quantile(0.99)\ndata_4 = data_3[data_3['sqft_basement']<p]\n\nsns.distplot(data_4['sqft_basement'],color='cyan')\n\nplt.title('pdf of basement with 99th percentile',size = 18)\nplt.show()","9e7521e5":"data_model=data_3.reset_index(drop=True)\ndata_model.describe(include=\"all\")","66b8f7ab":"#we can directly use the numpy method to convert all the price datapoints and store it a new variable.\nlog_price=np.log(data_model['price'])\n\n#we store the new variable in a new column in the existing dataframe.\ndata_model['log_price'] = log_price\n\ndata_model.head()","dce15375":"plt.subplots(figsize=(8,8))\n\ny=data_model['log_price']\nx=data_model['sqft_living']\n\nplt.scatter(x,y,color='green')\n\nplt.title('log price vs living area',size = 18)\nplt.show()","25c48f93":"data_model.columns.values","40eef7b2":"#please note that this method has been directly used as per the statsmodels documentation. There is no inbuilt method to calculate the vif but the algorithm is cited and cab be found in the accompanying documentation.\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvariables = data_model[[ 'bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot','floors', 'waterfront', 'view', 'condition', 'grade', 'sqft_above','sqft_basement', 'yr_built', 'yr_renovated', 'zipcode', 'lat','long', 'sqft_living15', 'sqft_lot15']]\n\nvif = pd.DataFrame()\n\nvif[\"VIF\"] = [variance_inflation_factor(variables.values, i) for i in range(variables.shape[1])]\nvif[\"Features\"] = variables.columns","04c43c1e":"vif","fc47293d":"data_cleaned = data_model.drop(['sqft_living15','sqft_lot15','sqft_above','sqft_lot','sqft_basement',],axis = 1)\ndata_cleaned.head()","8bb7db7e":"data_cleaned.columns.values","0f07df71":"#In the above equation, x1 is x or the values taken by the x variable and y is the values taken by the y variable.\n\nx1=data_cleaned[[ 'bedrooms', 'bathrooms', 'sqft_living',\n       'floors', 'waterfront', 'view', 'condition', 'grade',\n        'yr_built', 'yr_renovated', 'zipcode', 'lat',\n       'long']]\n\ny=data_cleaned[['log_price']]\n","6d9f7902":"#again, this method is pre-existing and can be directly used. The citation can be found in the documentation.\n\n#this is b0. We are essentially adding a coulmn consisting of only 1s that is equal in length to the y variable.\nx= sm.add_constant(x1)\n\n#we fit the regression model on x and y using the appropriate method and store it in a variable.\nresults = sm.OLS(y,x).fit()\n\n#we summarize our findings.\nresults.summary()\n\n#note that there is a variable to represent the error in the image. In statistical terms it is the SSE. In easier words, we are trying to minimize the error. The lower the error, the better our model is.","fc36bcf3":"data_reg=data_model.drop(['long','yr_renovated'],axis=1)\ndata_reg.head()","9da8cda8":"x1=data_cleaned[[ 'bedrooms', 'bathrooms', 'sqft_living',\n       'floors', 'waterfront', 'view', 'condition', 'grade',\n        'yr_built', 'lat','zipcode']]\ny=data_cleaned[['log_price']]","59643392":"x= sm.add_constant(x1)\nresults = sm.OLS(y,x).fit()\nresults.summary()","2ffde0c5":"#the first column displays the constant that we added earlier.\nx\n","fce14d83":"#we create a new dataframe with some observations.\ndata_with_predictions = pd.DataFrame({'const':1,'bedrooms':[3,3],'bathrooms':[1,2.25],'sqft_living':[1180,2570],'floors':[1,2],'waterfront':[0,0],'view':[0,0],'condition':[3,3], 'grade':[7,7],'yr_built':[1955,1951],'lat':[47.5112,47.7210],'zipcode':[98103,98002]})\n\n#we name the columns and display it.\ndata_with_predictions=data_with_predictions[['const','bedrooms','bathrooms','sqft_living','floors','waterfront','view','condition','grade','yr_built','lat','zipcode']]\n\ndata_with_predictions","96b7875d":"predictions = results.predict(data_with_predictions)\npredictions","78473dcb":"#we store the predictions in a new variable and attach it to the dataframe\ndata_with_predictions['predictions'] = predictions\ndata_with_predictions","f59e5093":"#using the inbuilt numpy method we take the exponent(inverse of logarthim)of the logarithmic price to get the original prices that we are interested in.\npred_price=np.exp(data_with_predictions['predictions'])\n\npred_price","c55b1697":"#again we store it in a new variable and attach it to the dataset.\ndata_with_predictions['predicted_price']=pred_price\ndata_with_predictions","e95c9507":"# 4.4.Is the Model Significant?","dbd4a67e":"The **most important relationship** would perhaps be this one. This forms the basis of our next few sections.","8b61fea9":"We see an **exponential type of relationship**. We will analyze it after dealing with outliers.","f5de3dfd":"* As evident from the graphs above, **a house with a good view of the waterfront definitely costs more on an average.** There is one house which doesn't have as good as a view but still costs more.\n* As expected, the presence of a waterfront is not game-changing because a lot of houses with no such views still continue to priced similarly.\n* But for houses with a similar living area, **a very good view of the waterfront shoots the price up **(See graph no 4 ).\n","86c9885a":"Since the **no of bedrooms is not continuous, we can plot it using a boxplot.** A box plot (or box-and-whisker plot) shows the distribution of quantitative data in a way that facilitates comparisons between variables or across levels of a categorical variable. The box shows the quartiles of the dataset while the whiskers extend to show the rest of the distribution, except for points that are determined to be \u201coutliers\u201d using a method that is a function of the inter-quartile range.","03ba7dec":"# 2.12.Living Area and Price","9941cb0d":"We use a **heatmap** in order to find the correaltion between the variables of this dataset. The further section,Data Visualization, uses this heatmap primarily.","181dbdbf":"# 5.1.Predicting Prices","61fd8a6c":"The following method checks for any missing values and returns their sum. It is important to get rid of missing values to create an accurate model.","bc707cd7":"As evident, the relationship is now much clearer and easier to interpret.","91e3e4d4":"# 4.1.New DataFrame","c03710b1":"In this section, we surround all the visualizations and their analyses around the dependent variable, price. \n\n**This data being from a mix of urban and suburban locations, has a lot of outliers or exceptions. These cannot be fully explained because of the disparity in the price of houses across various neighborhoods. \n**\n\nWe can try to reason the existence of such exceptions looking at just the correaltion but we **can never fully explain the causation.**","4d7cbbfa":"#  1.3. Correaltion Matrix","3c1eeff0":"We identify some variables with the most significant number of outlier from our previous analyses and drop the 99th percentile.\nEvery time we remove the outliers of a particular variable, we create a new dataframe with updated values.","c3d23178":"* Most of the houses have an average of **3 to 4 bedrooms**.\n* The horzintal line across the boxes denotes the median price, the lower half and the upper half represent the 25th and the 75th quartile respectively. The vertical line is the typical range and outliers above the range are represented as individual points.\n* There are **a lot of outliers** interestingly. A house with no bedrooms might be a studio apartment and there is one with 33 bedrooms. Surprizingly, it is modestly priced in comparision to some other houses.\n* For any given number of bedrooms, there are a lot of outliers which donn't fall under the range. This can be because c**ertain houses in the Greater Seattle Area might naturally be more expensive than a suburban house with more no of bedrooms.**\n* **Thus, this is not the only variable that explains price. We have to take into account the zipcode, size and other variables as well.**","77153003":"# 6.Conclusion","0da97a7e":"# 5.3.Exponential Transformation","f8fb4f34":"# 5.4.Final Prediction","1199d278":"Get some **descriptive statistics** like mean, standard deviation, minimum and maximum values,etc. The \"include ='all'\" method is used to get data on both categorical and numerical data.","36cce0cb":"# 4.Regression Model","d440a5c3":"# 3.Dealing with Outliers","1dae93e3":"# 2.6. Living Area,Lot Size, Living Area and Lot Size in the Proximity and Price","772682f5":"# 2.3. Number of Bathrooms and Price","9047b4d5":"This gives us an overview of the years in which the first 50 houses were built.","4b0da1ce":"* **A lot of houses were built in the 2000s and 2010s.**\n* A quick representation of the count of unique values confirms the fact.","6bdadab8":"# 4.2.Checking the Assumptings for the Ordinary Least Squares Method(For final project, see documentation).","87b05943":"* It is pretty interesting to note that **although better grade of materials used demands a higher price, it doesn't necessarily mean that the house is any better condition.**\n* On an average, **houses that were constructed using quality materials as are in a similar condition as their counterparts built with mediocre materials.**\n* However is that houses in an excellent condition(condition=6) were mainly built using the best grade materials(12 or 13).\n* Again, we have to keep in mind that other variables like the year in which the house was built and the fact that it was renovated or not might play a role.\n* A quick look at the heatmap does indicate **a negative correalation between condition of the house and the year in which it was built.** There is however, no such correlation between condtion and renovation.\n* Some other variables like maintenance and deterioration due to weather conditions might have a hand. These are out of our scope though for the given dataset.\n","790f076b":"**The vif for some values is extremely high**, more some being infinity!\n\nWe recall that **some variables related to area were extremely correlated. We drop those variables.**","1c603aeb":"**We fit the model using the appropriate Ordinary Least Squares method that comes with the statsmodels.api package.**","2174eb0d":"In our previous analysis, we couldn't spot an obvious linear relationship between the living area and price. **We convert the price using logarithms in order to linearize it.**","a8b8802f":"* We observe that **a better condition doesn't necessarily imply a higher price.**\n* Again it would be to broad to generalize anything given the number of outliers.\n* It is interesting to note that houses in a mediocre condition(grade=3) have a lot of outliers. It is an intersting point for further exploration by the concerned users.","2658c22a":"While still there are many outliers, as against the previous case, they are far fewer.Getting rid of all the outliers at the same time will make our model incapable of explaining exceptions altogether and paint an inaccurate and biased image of housing prices.","1637c9c5":"I have imported relevant libraries in order to carry out the analysis.**For the final project,they have been documneted in the final project proposal.**","9397113d":"# 2.4. Number of Floors and Price","ba7bc3fd":"We drop the columns 'id' and 'date' since they don't give us much information about the price.","fb8e4a02":"This variable explores how good or bad the condition of the house is on a scale of 1 to 5.","1460afa9":"For the OLS Method, we had added a constant,'x' which was equal in size to all the other x1 variables.","0d0bd748":"* The new dataframe is actually the first two observations of the orginal dataset.\n* The first prediction is within 31% of the observed value.\n* The second prediction is at around 20% from the observed value.\n* The** combined prediciton is within 27%** of the the observed values, our R-squared it at around 25%.","1de072fc":"# 1.Data Exploration     ","5fa08f7d":"**We make sure that the assumptions of normality and multicollinearity aren't violated.**","8917ec30":"**The price is going to be the dependent variable ****which is influence by other variables like number of bedrooms,bathrooms, condition,grade, waterfront,etc.** \n\nFor the final project, the variables have been explored in more detail in the documentation file supporting the project.","3a18a9b3":"Given the kind of distribution, using a boxplot makes sense.","539e3e05":"* Again, as in the case of the number of bedrooms, the number of bathrooms and the price varies a lot. Some houses with 2 bathrooms are priced way more than some wiht 4 or 5.\n* We might be tempted to think that the number of bathrooms are the price show an increasing trend. We have to keep in mind that this dataset primarily comes from an urban and a suburban region. Exceptions are therfore bound to occur.","4bcd2953":"# 4.2.1.Normality by Logarithmic Transformation","c0f70472":"**We take the exponent of the prices which is still in logarithms and store it in the dataframe.**","0d21de79":"This tells us how the grade, that is the quality of construction materials used might affet price.","2d98807f":"**We declare the independent and dependent variables.**\n![](http:miro.medium.com\/max\/2872\/1*k2bLmeYIG7z7dCyxADedhQ.png)","0181242a":"We drop the 99th percentile since most of them are exceptions.","8098ee42":"# 2.10. Year Built","4d18e85d":"We obtain the unique entries in this following column.","3cdfa6e4":"5.5.Remarks","9e901943":"# 2.5. Living Area, Waterfront, View and Price","067b2af2":"# 4.5.Regression Model ","27d02595":"This tells us how much the behavior of a variable is influenced by the other variables. We will use it in tandem with the heatmap to identify **multicollinearity**.","c902dddc":"We reset the indices of the previous dataframe in order to make the process easier.","ca5cd86c":"**The regression lies in the 'results' method.** We will fit the dataframe using this method and add it to our existing dataframe.","3a5b09d6":"We create **a new dataframe in order to predict the prices** using some other variables.","ccd004ce":"Use the pandas method to create a dataframe and print the first 5 rows of it. ","c3597b5d":"Given the number of variables involved,a **relplot** is the best option to explore these varibles.\n\nThis function provides access to several different axes-level functions that show the relationship between two variables with semantic mappings of subsets.The relationship between x and y can be shown for different subsets of the data using the hue, size, and style parameters.\n\nWe note that the **waterfront has already been converted into a dummy variable by mapping no waterfront and a waterfront with 0 and 1 respectively.\n**\nTHe view just tells us how good the view of the waterfront is, if any, on a scale of 0 to 4.","055a8dc5":"The number of bathrooms seems to follow a **continuous distribution.** A **histogram** would be an ideal option to denote it.[](http:\/\/)","c22621a9":"![](http:\/\/www.seattle.gov\/Images\/Clerk\/DistrictsMap.jpg)\n* This yield a very interesting result. **Majority of the houses are located in the Greater Seattle Region.**\n* More location details can be found out by plugging in the values of the longitude and the latitude in online calculators such as this one:\n[https:\/\/www.latlong.net\/](http:\/\/)\n","79174972":"# 2.7. Grade and Price","a51d593a":"We plot the latitude and longitude coordinates given in the dataset. \n\nThe axes range from negative to positive values because of how directions are plotted on a graph.","ddcb5557":"# 2.2. Number of Bedrooms and Price","8eea3053":"* We first identified the independent variables and the dependent variable, price.\n* After visualizing the dataset using graphics,we inferred:\n  1. Most of the **houses are located in the Seattle Metropolis.**\n  \n  2. **Most of the houses were built in the 21st century.**\n  3. We observed that the **housing in King County is primarily influenced by the living area and the grade of the construction materials** used to build the house,followed by the number of bedrooms and the number of bathrooms.\n  4. The **neighborhoods are distinctly demarcated** because of the similarity in the sizes and prices of houses in close proximity.\n  5. **When it comes to prices and the number of bedrooms,bathrooms and floors, there are a lot of exceptions because of the unique locations of the houses merged into one large dataset.**\n  6. **Houses built with better construction materials cost more on an average though they do not guarantee a better condition of the house.**\n  7. For houses that have an excellent view of the waterfront, their prices are signifiantly higher than both those houses that totally lack a view or those with a compromised view.\n* We dealt with outliers by plotting the probability density function of some variables. We also linearized the model and identified the regressors keeping in mind the assumptions of the Ordinary Least Squares Method.\n* We built a **regression model using the OLS Method with around 74% of accuracy.**\n* We also predicted the prices of two houses and compared it to their actual values. We found that their **combined accuracy was 73% overall.**","ad15380d":"# 4.3.Building the Model","629cd00e":"Get more information on the type of variable.","9ac3f9ae":"# 5.2.New Dataframe","d4442cb2":"* It is easy to see an obvious **upward trend** in the graph.\n* Better construction materials increase and the cost of labor and raw materials which is refected by the price.\n* Although we can't deny some exceptions, they definitely aren't as deviant as some of the variables that we have seen earlier.","e144b12f":"* For a quick overview, we use the pairplot method of seaborn. It will pairwise relationships of different variable subsets of a dataset on rows and columns.\n* It seems that the **number of bedrooms, bathrooms and floors have a positive correaltion with price and with each other.**\n* This is pretty intuitive because the more the number of these variables, the bigger the house and the costlier it would be.\n* We shouldn't jump to conclusions though since there a lot of outliers that we will deal with on a case-by-case basis.","9e499a47":"Given the distribution of the variable, a **bargraph** would be the simplest way to visualize this variable.","b1eabdfa":"#  1.1.Importing the relevant modules and the dataset","aada2ed4":"* There is a **lot of correalation between the living area**('sqft_living') and the **other variables **like the size of the lot('sqft_lot'), basement('sqft_basement') and the living area and plot size of the nearest 15 houses ('sqft_living15' and 'sqft_lot15' respectively).\n* **A large living area would definitely mean more room for basement and a larger lot**. As evident from the correlation heatmap, a larger living area is also strongly correlated with the number of bedrooms, bathrooms and floors.\n* Houses that are close to each other are similar in area and similarly priced(refer the heatmap).This can be attributed to the clustering of similar houses into a neighborhood. **More affluent neighborhoods will also have similar prices.**\n* There aren't as many exceptions in this case.","a5f590b8":"# 2.8. Condition and Price","94675631":"# 2.Data Visualization","971efd0a":"* **This model is more statistically significant because without dropping the R-squared much and keeping the adjusted R-squared the same, the F-statistic has increased significantly and the p-value is 0 now.**\n* We still have a warning about strong multicollinearity. This is because certain variables like the number of bedrooms and bathrooms are although not very strongly correlated with the livng area, still have a non-negligible correaltion. Despite this, dropping them would significantly reduce the R-squared or the explanatory power.\n* We have to keep in mind that **no model can fully capture the dataset**(If it does so, we might have an overfitting model).\n* **We can therefore adopt the model at the accuracy level of 74%.**","f329f30e":"# 2.11. Location","1da8ac0e":"**We draw the probability density functions of some variables to understand the concentration and distribution of the variable. We can also see the distribuiton of outliers and remove them.**","fc80f326":"* Just creating a model is not enough. We have to gauge **the significance of the model by looking at summary table.(For the final project, see documentation for more inforamtion)**\n  \n  1. **R-squared**: It tells us how close the data is to the fitted line.It is **pretty close at around 74%**.\n  \n  2. **Adjusted R-squared:** It penalizes us for adding variables that have no explanatory power. Looks like it has **passed this test** too.\n  \n  3.**F-statistic:** It tells us if the F-distribution is followed. The higher the value, the better.\n  \n  4.**p-value:**It tells us the lowest value at which the null hypothesis can be rejected. Here the null hypothesis is that the dependent variables don't have any       explanatory power.The lowest value is 0.05. This test is also passed.","3f1d4dde":"# 5.Predictions","6c920624":"# 4.2.2.Multicollinearity Using Variable Influence Factor","ababc62b":"# 2.9. Grade, Condition and Price","34b95957":"# Introduction\nWelcome to my Kernel! This is my first Kaggle project that I am creating for my class final project this semester. \n\nThe dataset is about the **housing prices in King County, WA,** whose county seat is Seattle. It is a predominantly urban and suburban county. \n\nMore information on the place : \n\n[https:\/\/en.wikipedia.org\/wiki\/King_County,_Washington](http:\/\/)\n\n![](http:\/\/upload.wikimedia.org\/wikipedia\/commons\/b\/bc\/Seattle_-_King_County_Courthouse_and_King_County_Administration_Building_01.jpg)\n\nI have first tried to **visualize the data, clean it and ultimately build a linear regression model to analyze it**.\n\nI have used some **python packages** to carryout the analyses and build the regression model.\n\nThis project helps to solve the problem a lot of the potential users face: with the housing market being so diverse and with so many factors influencing the prices coming into play, one can easily be overwhelmed. By the menas of this project, I want to highlight the important components of such a market in a purely unbiased,data-driven way. After looking at this project, one might get a good enough idea about the housing market in King County, WA in 2014-15.\n\nThis kernel will be extremely helpful for **potential buyers, realtors and builders** who will get more insights into what influences prices and help them take more educated data-driven decisions.\n\nI would greatly appreciate any further comments\/questions.\n\nSince it is created for academic purposes, detailed inferences\/documentation has been provided. Feel free to jump to the conclusion for a snapshot of the findings.\n\n","d70fa67f":"#  2.1. Relationship between price and the number of bedrooms, bathrooms and floors:","caab3a02":"#  1.2.Describing the preliminary data ","0424d9ca":"Although our last model was pretty significant, we can still achieve a better result by trying to eliminate more correlation. On repeated trials, we can drop the following variables:","0e8b4c4d":"* As expected, although there is a postive correaltion between these two variables, **there isn't an obvious trend.**\n* Penthouses and loft apartments in downtown Seattle might definitely be more expensive than a three-story suburban colonel."}}