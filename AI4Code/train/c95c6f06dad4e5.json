{"cell_type":{"9069d4fc":"code","8db69c4d":"code","9d77cdf6":"code","0e76e843":"code","528c1fb9":"code","b1d0317c":"code","3b43e026":"code","129c0810":"code","98d0e5e4":"code","06217515":"code","006d8dbc":"code","f2bbd713":"code","79d87a8f":"code","aea8233d":"code","f4df7fb1":"code","45bbdb83":"code","7342313a":"code","bc7ddb6c":"code","ceb12ee3":"code","76a353b0":"code","571ec0a0":"code","2cbb614f":"code","614b58c1":"code","8147319d":"code","59ab6c77":"code","861dc771":"code","d4afd7d1":"code","70709935":"code","86e23d23":"code","aa0e6b3c":"code","2edf201b":"code","702d9673":"markdown","7f2a94da":"markdown","b8e49359":"markdown","2664e7cd":"markdown","ce1f9e28":"markdown","fdf502b6":"markdown","05f43f79":"markdown","cb1117b1":"markdown","f1599733":"markdown","040825ee":"markdown","c56ec663":"markdown","5125bfd9":"markdown","3af71040":"markdown","6a699364":"markdown"},"source":{"9069d4fc":"import os\n# DECLARE HOW MANY GPUS YOU WISH TO USE. \n# KAGGLE ONLY HAS 1, BUT OFFLINE, YOU CAN USE MORE\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #0,1,2,3 for four gpu\n\n# VERSION FOR SAVING\/LOADING MODEL WEIGHTS\n# THIS SHOULD MATCH THE MODEL IN LOAD_MODEL_FROM\nVER=14 \n\n# IF VARIABLE IS NONE, THEN NOTEBOOK COMPUTES TOKENS\n# OTHERWISE NOTEBOOK LOADS TOKENS FROM PATH\nLOAD_TOKENS_FROM = '..\/input\/tf-longformer-v12'\n\n# IF VARIABLE IS NONE, THEN NOTEBOOK TRAINS A NEW MODEL\n# OTHERWISE IT LOADS YOUR PREVIOUSLY TRAINED MODEL\nLOAD_MODEL_FROM = '..\/input\/tflongformerv14'\n\n# IF FOLLOWING IS NONE, THEN NOTEBOOK \n# USES INTERNET AND DOWNLOADS HUGGINGFACE \n# CONFIG, TOKENIZER, AND MODEL\nDOWNLOADED_MODEL_PATH = '..\/input\/tf-longformer-v12'\n\nif DOWNLOADED_MODEL_PATH is None:\n    DOWNLOADED_MODEL_PATH = 'model'    \nMODEL_NAME = 'allenai\/longformer-base-4096'","8db69c4d":"if DOWNLOADED_MODEL_PATH == 'model':\n    os.mkdir('model')\n    \n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    tokenizer.save_pretrained('model')\n\n    config = AutoConfig.from_pretrained(MODEL_NAME) \n    config.save_pretrained('model')\n\n    backbone = TFAutoModel.from_pretrained(MODEL_NAME, config=config)\n    backbone.save_pretrained('model')","9d77cdf6":"import pandas as pd, numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\nfrom transformers import *\nprint('TF version',tf.__version__)","0e76e843":"# USE MULTIPLE GPUS\nif os.environ[\"CUDA_VISIBLE_DEVICES\"].count(',') == 0:\n    strategy = tf.distribute.get_strategy()\n    print('single strategy')\nelse:\n    strategy = tf.distribute.MirroredStrategy()\n    print('multiple strategy')","528c1fb9":"tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\nprint('Mixed precision enabled')","b1d0317c":"train = pd.read_csv('..\/input\/feedback-prize-2021\/train.csv')\nprint( train.shape )\ntrain.head()","3b43e026":"print('The train labels are:')\ntrain.discourse_type.unique()","129c0810":"IDS = train.id.unique()\nprint('There are',len(IDS),'train texts.')","98d0e5e4":"MAX_LEN = 1024\n\n# THE TOKENS AND ATTENTION ARRAYS\ntokenizer = AutoTokenizer.from_pretrained(DOWNLOADED_MODEL_PATH)\ntrain_tokens = np.zeros((len(IDS),MAX_LEN), dtype='int32')\ntrain_attention = np.zeros((len(IDS),MAX_LEN), dtype='int32')\n\n# THE 14 CLASSES FOR NER\nlead_b = np.zeros((len(IDS),MAX_LEN))\nlead_i = np.zeros((len(IDS),MAX_LEN))\n\nposition_b = np.zeros((len(IDS),MAX_LEN))\nposition_i = np.zeros((len(IDS),MAX_LEN))\n\nevidence_b = np.zeros((len(IDS),MAX_LEN))\nevidence_i = np.zeros((len(IDS),MAX_LEN))\n\nclaim_b = np.zeros((len(IDS),MAX_LEN))\nclaim_i = np.zeros((len(IDS),MAX_LEN))\n\nconclusion_b = np.zeros((len(IDS),MAX_LEN))\nconclusion_i = np.zeros((len(IDS),MAX_LEN))\n\ncounterclaim_b = np.zeros((len(IDS),MAX_LEN))\ncounterclaim_i = np.zeros((len(IDS),MAX_LEN))\n\nrebuttal_b = np.zeros((len(IDS),MAX_LEN))\nrebuttal_i = np.zeros((len(IDS),MAX_LEN))\n\n# HELPER VARIABLES\ntrain_lens = []\ntargets_b = [lead_b, position_b, evidence_b, claim_b, conclusion_b, counterclaim_b, rebuttal_b]\ntargets_i = [lead_i, position_i, evidence_i, claim_i, conclusion_i, counterclaim_i, rebuttal_i]\ntarget_map = {'Lead':0, 'Position':1, 'Evidence':2, 'Claim':3, 'Concluding Statement':4,\n             'Counterclaim':5, 'Rebuttal':6}","06217515":"# WE ASSUME DATAFRAME IS ASCENDING WHICH IT IS\nassert( np.sum(train.groupby('id')['discourse_start'].diff()<=0)==0 )\n\n# FOR LOOP THROUGH EACH TRAIN TEXT\nfor id_num in range(len(IDS)):\n    if LOAD_TOKENS_FROM: break\n    if id_num%100==0: print(id_num,', ',end='')\n        \n    # READ TRAIN TEXT, TOKENIZE, AND SAVE IN TOKEN ARRAYS    \n    n = IDS[id_num]\n    name = f'..\/input\/feedback-prize-2021\/train\/{n}.txt'\n    txt = open(name, 'r').read()\n    train_lens.append( len(txt.split()))\n    tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n                                   truncation=True, return_offsets_mapping=True)\n    train_tokens[id_num,] = tokens['input_ids']\n    train_attention[id_num,] = tokens['attention_mask']\n    \n    # FIND TARGETS IN TEXT AND SAVE IN TARGET ARRAYS\n    offsets = tokens['offset_mapping']\n    offset_index = 0\n    df = train.loc[train.id==n]\n    for index,row in df.iterrows():\n        a = row.discourse_start\n        b = row.discourse_end\n        if offset_index>len(offsets)-1:\n            break\n        c = offsets[offset_index][0]\n        d = offsets[offset_index][1]\n        beginning = True\n        while b>c:\n            if (c>=a)&(b>=d):\n                k = target_map[row.discourse_type]\n                if beginning:\n                    targets_b[k][id_num][offset_index] = 1\n                    beginning = False\n                else:\n                    targets_i[k][id_num][offset_index] = 1\n            offset_index += 1\n            if offset_index>len(offsets)-1:\n                break\n            c = offsets[offset_index][0]\n            d = offsets[offset_index][1]","006d8dbc":"if LOAD_TOKENS_FROM is None:\n    plt.hist(train_lens,bins=100)\n    plt.title('Histogram of Train Word Counts',size=16)\n    plt.xlabel('Train Word Count',size=14)\n    plt.show()","f2bbd713":"if LOAD_TOKENS_FROM is None:\n    targets = np.zeros((len(IDS),MAX_LEN,15), dtype='int32')\n    for k in range(7):\n        targets[:,:,2*k] = targets_b[k]\n        targets[:,:,2*k+1] = targets_i[k]\n    targets[:,:,14] = 1-np.max(targets,axis=-1)","79d87a8f":"if LOAD_TOKENS_FROM is None:\n    np.save(f'targets_{MAX_LEN}', targets)\n    np.save(f'tokens_{MAX_LEN}', train_tokens)\n    np.save(f'attention_{MAX_LEN}', train_attention)\n    print('Saved NER tokens')\nelse:\n    targets = np.load(f'{LOAD_TOKENS_FROM}\/targets_{MAX_LEN}.npy')\n    train_tokens = np.load(f'{LOAD_TOKENS_FROM}\/tokens_{MAX_LEN}.npy')\n    train_attention = np.load(f'{LOAD_TOKENS_FROM}\/attention_{MAX_LEN}.npy')\n    print('Loaded NER tokens')","aea8233d":"def build_model():\n    \n    tokens = tf.keras.layers.Input(shape=(MAX_LEN,), name = 'tokens', dtype=tf.int32)\n    attention = tf.keras.layers.Input(shape=(MAX_LEN,), name = 'attention', dtype=tf.int32)\n    \n    config = AutoConfig.from_pretrained(DOWNLOADED_MODEL_PATH+'\/config.json') \n    backbone = TFAutoModel.from_pretrained(DOWNLOADED_MODEL_PATH+'\/tf_model.h5', config=config)\n    \n    x = backbone(tokens, attention_mask=attention)\n    x = tf.keras.layers.Dense(256, activation='relu')(x[0])\n    x = tf.keras.layers.Dense(15, activation='softmax', dtype='float32')(x)\n    \n    model = tf.keras.Model(inputs=[tokens,attention], outputs=x)\n    model.compile(optimizer = tf.keras.optimizers.Adam(lr = 1e-4),\n                  loss = [tf.keras.losses.CategoricalCrossentropy()],\n                  metrics = [tf.keras.metrics.CategoricalAccuracy()])\n    \n    return model","f4df7fb1":"with strategy.scope():\n    model = build_model()","45bbdb83":"# LEARNING RATE SCHEDULE AND MODEL CHECKPOINT\nEPOCHS = 5\nBATCH_SIZE = 4 \nLRS = [0.25e-4, 0.25e-4, 0.25e-4, 0.25e-4, 0.25e-5] \ndef lrfn(epoch):\n    return LRS[epoch]\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)","7342313a":"# TRAIN VALID SPLIT 90% 10%\nnp.random.seed(42)\ntrain_idx = np.random.choice(np.arange(len(IDS)),int(0.9*len(IDS)),replace=False)\nvalid_idx = np.setdiff1d(np.arange(len(IDS)),train_idx)\nnp.random.seed(None)\nprint('Train size',len(train_idx),', Valid size',len(valid_idx))","bc7ddb6c":"# LOAD MODEL\nif LOAD_MODEL_FROM:\n    model.load_weights(f'{LOAD_MODEL_FROM}\/long_v{VER}.h5')\n    \n# OR TRAIN MODEL\nelse:\n    model.fit(x = [train_tokens[train_idx,], train_attention[train_idx,]],\n          y = targets[train_idx,],\n          validation_data = ([train_tokens[valid_idx,], train_attention[valid_idx,]],\n                             targets[valid_idx,]),\n          callbacks = [lr_callback],\n          epochs = EPOCHS,\n          batch_size = BATCH_SIZE,\n          verbose = 2)\n\n    # SAVE MODEL WEIGHTS\n    model.save_weights(f'long_v{VER}.h5')","ceb12ee3":"p = model.predict([train_tokens[valid_idx,], train_attention[valid_idx,]], \n                  batch_size=16, verbose=2)\nprint('OOF predictions shape:',p.shape)\noof_preds = np.argmax(p,axis=-1)","76a353b0":"target_map_rev = {0:'Lead', 1:'Position', 2:'Evidence', 3:'Claim', 4:'Concluding Statement',\n             5:'Counterclaim', 6:'Rebuttal', 7:'blank'}","571ec0a0":"def get_preds(dataset='train', verbose=True, text_ids=IDS[valid_idx], preds=oof_preds):\n    all_predictions = []\n\n    for id_num in range(len(preds)):\n    \n        # GET ID\n        if (id_num%100==0)&(verbose): \n            print(id_num,', ',end='')\n        n = text_ids[id_num]\n    \n        # GET TOKEN POSITIONS IN CHARS\n        name = f'..\/input\/feedback-prize-2021\/{dataset}\/{n}.txt'\n        txt = open(name, 'r').read()\n        tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n                                   truncation=True, return_offsets_mapping=True)\n        off = tokens['offset_mapping']\n    \n        # GET WORD POSITIONS IN CHARS\n        w = []\n        blank = True\n        for i in range(len(txt)):\n            if (txt[i]!=' ')&(txt[i]!='\\n')&(txt[i]!='\\xa0')&(txt[i]!='\\x85')&(blank==True):\n                w.append(i)\n                blank=False\n            elif (txt[i]==' ')|(txt[i]=='\\n')|(txt[i]=='\\xa0')|(txt[i]=='\\x85'):\n                blank=True\n        w.append(1e6)\n            \n        # MAPPING FROM TOKENS TO WORDS\n        word_map = -1 * np.ones(MAX_LEN,dtype='int32')\n        w_i = 0\n        for i in range(len(off)):\n            if off[i][1]==0: continue\n            while off[i][0]>=w[w_i+1]: w_i += 1\n            word_map[i] = int(w_i)\n        \n        # CONVERT TOKEN PREDICTIONS INTO WORD LABELS\n        ### KEY: ###\n        # 0: LEAD_B, 1: LEAD_I\n        # 2: POSITION_B, 3: POSITION_I\n        # 4: EVIDENCE_B, 5: EVIDENCE_I\n        # 6: CLAIM_B, 7: CLAIM_I\n        # 8: CONCLUSION_B, 9: CONCLUSION_I\n        # 10: COUNTERCLAIM_B, 11: COUNTERCLAIM_I\n        # 12: REBUTTAL_B, 13: REBUTTAL_I\n        # 14: NOTHING i.e. O\n        ### NOTE THESE VALUES ARE DIVIDED BY 2 IN NEXT CODE LINE\n        pred = preds[id_num,]\/2.0\n    \n        i = 0\n        while i<MAX_LEN:\n            prediction = []\n            start = pred[i]\n            if start in [0,1,2,3,4,5,6,7]:\n                prediction.append(word_map[i])\n                i += 1\n                if i>=MAX_LEN: break\n                while pred[i]==start+0.5:\n                    if not word_map[i] in prediction:\n                        prediction.append(word_map[i])\n                    i += 1\n                    if i>=MAX_LEN: break\n            else:\n                i += 1\n            prediction = [x for x in prediction if x!=-1]\n            if len(prediction)>4:\n                all_predictions.append( (n, target_map_rev[int(start)], \n                                ' '.join([str(x) for x in prediction]) ) )\n                \n    # MAKE DATAFRAME\n    df = pd.DataFrame(all_predictions)\n    df.columns = ['id','class','predictionstring']\n    \n    return df","2cbb614f":"oof = get_preds( dataset='train', verbose=True, text_ids=IDS[valid_idx] )\noof.head()","614b58c1":"print('The following classes are present in oof preds:')\noof['class'].unique()","8147319d":"# CODE FROM : Rob Mulla @robikscube\n# https:\/\/www.kaggle.com\/robikscube\/student-writing-competition-twitch\ndef calc_overlap(row):\n    \"\"\"\n    Calculates the overlap between prediction and\n    ground truth and overlap percentages used for determining\n    true positives.\n    \"\"\"\n    set_pred = set(row.predictionstring_pred.split(' '))\n    set_gt = set(row.predictionstring_gt.split(' '))\n    # Length of each and intersection\n    len_gt = len(set_gt)\n    len_pred = len(set_pred)\n    inter = len(set_gt.intersection(set_pred))\n    overlap_1 = inter \/ len_gt\n    overlap_2 = inter\/ len_pred\n    return [overlap_1, overlap_2]\n\n\ndef score_feedback_comp(pred_df, gt_df):\n    \"\"\"\n    A function that scores for the kaggle\n        Student Writing Competition\n        \n    Uses the steps in the evaluation page here:\n        https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/overview\/evaluation\n    \"\"\"\n    gt_df = gt_df[['id','discourse_type','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df = pred_df[['id','class','predictionstring']] \\\n        .reset_index(drop=True).copy()\n    pred_df['pred_id'] = pred_df.index\n    gt_df['gt_id'] = gt_df.index\n    # Step 1. all ground truths and predictions for a given class are compared.\n    joined = pred_df.merge(gt_df,\n                           left_on=['id','class'],\n                           right_on=['id','discourse_type'],\n                           how='outer',\n                           suffixes=('_pred','_gt')\n                          )\n    joined['predictionstring_gt'] = joined['predictionstring_gt'].fillna(' ')\n    joined['predictionstring_pred'] = joined['predictionstring_pred'].fillna(' ')\n\n    joined['overlaps'] = joined.apply(calc_overlap, axis=1)\n\n    # 2. If the overlap between the ground truth and prediction is >= 0.5, \n    # and the overlap between the prediction and the ground truth >= 0.5,\n    # the prediction is a match and considered a true positive.\n    # If multiple matches exist, the match with the highest pair of overlaps is taken.\n    joined['overlap1'] = joined['overlaps'].apply(lambda x: eval(str(x))[0])\n    joined['overlap2'] = joined['overlaps'].apply(lambda x: eval(str(x))[1])\n\n\n    joined['potential_TP'] = (joined['overlap1'] >= 0.5) & (joined['overlap2'] >= 0.5)\n    joined['max_overlap'] = joined[['overlap1','overlap2']].max(axis=1)\n    tp_pred_ids = joined.query('potential_TP') \\\n        .sort_values('max_overlap', ascending=False) \\\n        .groupby(['id','predictionstring_gt']).first()['pred_id'].values\n\n    # 3. Any unmatched ground truths are false negatives\n    # and any unmatched predictions are false positives.\n    fp_pred_ids = [p for p in joined['pred_id'].unique() if p not in tp_pred_ids]\n\n    matched_gt_ids = joined.query('potential_TP')['gt_id'].unique()\n    unmatched_gt_ids = [c for c in joined['gt_id'].unique() if c not in matched_gt_ids]\n\n    # Get numbers of each type\n    TP = len(tp_pred_ids)\n    FP = len(fp_pred_ids)\n    FN = len(unmatched_gt_ids)\n    #calc microf1\n    my_f1_score = TP \/ (TP + 0.5*(FP+FN))\n    return my_f1_score","59ab6c77":"# VALID DATAFRAME\nvalid = train.loc[train['id'].isin(IDS[valid_idx])]","861dc771":"f1s = []\nCLASSES = oof['class'].unique()\nfor c in CLASSES:\n    pred_df = oof.loc[oof['class']==c].copy()\n    gt_df = valid.loc[valid['discourse_type']==c].copy()\n    f1 = score_feedback_comp(pred_df, gt_df)\n    print(c,f1)\n    f1s.append(f1)\nprint()\nprint('Overall',np.mean(f1s))","d4afd7d1":"# GET TEST TEXT IDS\nfiles = os.listdir('..\/input\/feedback-prize-2021\/test')\nTEST_IDS = [f.replace('.txt','') for f in files if 'txt' in f]\nprint('There are',len(TEST_IDS),'test texts.')","70709935":"# CONVERT TEST TEXT TO TOKENS\ntest_tokens = np.zeros((len(TEST_IDS),MAX_LEN), dtype='int32')\ntest_attention = np.zeros((len(TEST_IDS),MAX_LEN), dtype='int32')\n\nfor id_num in range(len(TEST_IDS)):\n        \n    # READ TRAIN TEXT, TOKENIZE, AND SAVE IN TOKEN ARRAYS    \n    n = TEST_IDS[id_num]\n    name = f'..\/input\/feedback-prize-2021\/test\/{n}.txt'\n    txt = open(name, 'r').read()\n    tokens = tokenizer.encode_plus(txt, max_length=MAX_LEN, padding='max_length',\n                                   truncation=True, return_offsets_mapping=True)\n    test_tokens[id_num,] = tokens['input_ids']\n    test_attention[id_num,] = tokens['attention_mask']","86e23d23":"# INFER TEST TEXTS\np = model.predict([test_tokens, test_attention], \n                  batch_size=16, verbose=2)\nprint('Test predictions shape:',p.shape)\ntest_preds = np.argmax(p,axis=-1)","aa0e6b3c":"# GET TEST PREDICIONS\nsub = get_preds( dataset='test', verbose=False, text_ids=TEST_IDS, preds=test_preds )\nsub.head()","2edf201b":"# WRITE SUBMISSION CSV\nsub.to_csv('submission.csv',index=False)","702d9673":"# \u30e2\u30c7\u30eb\u306e\u69cb\u7bc9\n\nLongFormer\u30d0\u30c3\u30af\u30dc\u30fc\u30f3\u3092\u4f7f\u7528\u3057\u3001\u30b5\u30a4\u30ba256\u306e1\u3064\u306e\u96a0\u308c\u5c64\u3068softmax\u306e\u6700\u5f8c\u306e1\u3064\u306e\u5c64\u3092\u4f7f\u7528\u3057\u3066\u72ec\u81ea\u306eNER\u30d8\u30c3\u30c9\u3092\u8ffd\u52a0\u3057\u307e\u3059\u3002 7\u3064\u306e\u30e9\u30d9\u30eb\u306e\u305d\u308c\u305e\u308c\u306bB\u30af\u30e9\u30b9\u3068I\u30af\u30e9\u30b9\u304c\u3042\u308b\u305f\u3081\u300115\u306e\u30af\u30e9\u30b9\u3092\u4f7f\u7528\u3057\u307e\u3059\u3002 \u307e\u305f\u300114\u306e\u30af\u30e9\u30b9\u306e\u3044\u305a\u308c\u306b\u3082\u5c5e\u3055\u306a\u3044\u30c8\u30fc\u30af\u30f3\u7528\u306e\u8ffd\u52a0\u306e\u30af\u30e9\u30b9\uff08O\u30af\u30e9\u30b9\u3068\u547c\u3070\u308c\u308b\uff09\u304c\u3042\u308a\u307e\u3059\u3002","7f2a94da":"# \u691c\u8a3c\u30e1\u30c8\u30ea\u30c3\u30af\u306e\u8a08\u7b97\n\n\u6b21\u306e\u30b3\u30fc\u30c9\u306f\u3001RobMulla\u306e\u512a\u308c\u305f[\u30ce\u30fc\u30c8\u30d6\u30c3\u30af][2]\u304b\u3089\u306e\u3082\u306e\u3067\u3059\u3002 LongFormer\u30b7\u30f3\u30b0\u30eb\u30d5\u30a9\u30fc\u30eb\u30c9\u30e2\u30c7\u30eb\u306fCV\u30b9\u30b3\u30a20.633\u3092\u9054\u6210\u3057\u3066\u3044\u307e\u3059\u3002\n\n[2]: https:\/\/www.kaggle.com\/robikscube\/student-writing-competition-twitch","b8e49359":"\u4e0a\u8a18\u306f\u30d5\u30a1\u30a4\u30eb\u3092\u4fdd\u5b58\u3057\u307e\u3059\n* \u30c8\u30fc\u30af\u30f3\u5316\u30d5\u30a1\u30a4\u30eb-merges.txt\u3001tokenizer_config.json\u3001special_tokens_map.json\u3001tokenizer.json\u3001vocab.json\n* \u8a2d\u5b9a\u30d5\u30a1\u30a4\u30eb-config.json\n* \u30e2\u30c7\u30eb\u91cd\u91cf\u30d5\u30a1\u30a4\u30eb-tf_model.h5\n\n\u6b21\u306b\u3001[\u3053\u3053] [1]\u3067\u884c\u3063\u305f\u3088\u3046\u306b\u3001\u3053\u308c\u3089\u3059\u3079\u3066\u306e\u30d5\u30a1\u30a4\u30eb\u3092Kaggle\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002 \u6b21\u306b\u3001\u8aad\u3093\u3067\u3044\u308b\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3068\u540c\u3058\u3088\u3046\u306b\u3001\u305d\u308c\u3089\u3092\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306b\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002 \u305d\u3057\u3066\u3001\u30a4\u30f3\u30bf\u30fc\u30cd\u30c3\u30c8\u3092\u30aa\u30d5\u306b\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\uff01\n\n[1]: https:\/\/www.kaggle.com\/cdeotte\/tf-longformer-v12","2664e7cd":"# \u30e2\u30c7\u30eb\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u307e\u305f\u306f\u30ed\u30fc\u30c9\n\n\u4e0a\u8a18\u306e\u5909\u6570`LOAD_MODEL_FROM`\u306b\u30d1\u30b9\u3092\u6307\u5b9a\u3059\u308b\u3068\u3001\u4ee5\u524d\u306b\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3055\u308c\u305f\u30e2\u30c7\u30eb\u304c\u30ed\u30fc\u30c9\u3055\u308c\u307e\u3059\u3002 \u305d\u308c\u4ee5\u5916\u306e\u5834\u5408\u306f\u3001\u4eca\u3059\u3050\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3057\u307e\u3059\u3002\n\n\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306e\u30d0\u30fc\u30b8\u30e7\u30f31\u304b\u30894\u306b\u30ed\u30fc\u30c9\u3055\u308c\u305f\u30e2\u30c7\u30eb\u306f\u3001\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba32\u306e5\u3064\u306e\u30a8\u30dd\u30c3\u30af\u3092\u4f7f\u7528\u3057\u3066\u30aa\u30d5\u30e9\u30a4\u30f3\u3067\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3055\u308c\u3001\u6700\u521d\u306e4\u3064\u306e\u30a8\u30dd\u30c3\u30af\u3067\u5b66\u7fd2\u7387`1e4`\u3001\u6700\u5f8c\u306e\u30a8\u30dd\u30c3\u30af\u3067`1e5`\u3092\u5b66\u7fd2\u3057\u307e\u3057\u305f\u3002 \u305d\u306e\u30e2\u30c7\u30eb\u306f\u30014xV100GPU\u3092\u4f7f\u7528\u3057\u3066\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3055\u308c\u307e\u3057\u305f\u3002\n\n\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306e\u30d0\u30fc\u30b8\u30e7\u30f35\u306f\u3001\u30e6\u30fc\u30b6\u30fc@kaggleqrdl\u304c\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u30d0\u30fc\u30b8\u30e7\u30f320\u3067Kaggle\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3057\u305f\u30e2\u30c7\u30eb\u3092\u3053\u3053\u306b\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002 Qrdl\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306b\u8cdb\u6210\u3057\u3066\u304f\u3060\u3055\u3044:-) Qrdl\u306f\u5b9f\u9a13\u3092\u884c\u3063\u3066\u304a\u308a\u3001\u5c0f\u3055\u306a\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3067\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u3068\u304d\u306b\u4f7f\u7528\u3067\u304d\u308b\u512a\u308c\u305f\u5b66\u7fd2\u7387\u3092\u898b\u3064\u3051\u307e\u3057\u305f\u3002\n\nKaggle\u306e1xP100GPU\u3067\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u5834\u5408\u3001\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u30924\u306b\u6e1b\u3089\u3059\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u307e\u305f\u3001\u5b66\u7fd2\u7387\u3092`0.25e-4`\u3068`0.25e-5`\u306b\u6e1b\u3089\u3057\u307e\u3059\u3002 \u4ee5\u4e0b\u306e\u30d0\u30c3\u30c1\u30b5\u30a4\u30ba\u3068\u5b66\u7fd2\u7387\u3092\u66f4\u65b0\u3057\u3066\u3001Kaggle\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3067\u304d\u308b\u3088\u3046\u306b\u3057\u307e\u3057\u305f\u3002 \uff08Kaggle\u306e\u5404\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u30a8\u30dd\u30c3\u30af\u306b\u306f1\u6642\u95938\u5206\u304b\u304b\u308a\u307e\u3059\uff09\u3002 \n\n[1]: https:\/\/www.kaggle.com\/kaggleqrdl\/v4expmt-tensorflow-longformer-ner-cv-0-634?scriptVersionId=83341823","ce1f9e28":"# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u63a8\u6e2c\u3059\u308b\n\n\u6b21\u306b\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u63a8\u6e2c\u3057\u3001\u63d0\u51fa\u7269\u3092\u4f5c\u6210\u3057\u307e\u3059\u3002 \u79c1\u305f\u3061\u306eCV\u306f0.633\u3067\u3059\u3001\u79c1\u305f\u3061\u306eLB\u304c\u4f55\u3067\u3042\u308b\u304b\u898b\u3066\u307f\u307e\u3057\u3087\u3046\u3002","fdf502b6":"# \u30c8\u30ec\u30a4\u30f3\u306e\u8aad\u307f\u8fbc\u307f","05f43f79":"# \u3010\u65e5\u672c\u8a9e\u89e3\u8aac\u3011 TensorFlow LongFormer NER Baseline - CV 0.633!\n\nThis notebook is a Japanese explanation of [TensorFlow LongFormer NER Baseline][3] by CHRIS DEOTTE.\n\n\u3053\u308c\u306f\u4e0a\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3092\u81ea\u7fd2\u3059\u308b\u305f\u3081\u306b\u4f5c\u308a\u307e\u3057\u305f\u304c\u3001\u53c2\u8003\u306b\u306a\u308b\u65b9\u3044\u305f\u3089\u6295\u7968\u3044\u305f\u3060\u3051\u308b\u3068\u5e78\u3044\u3067\u3059\u3002\n\u30b3\u30fc\u30c9\u306e\u4e3b\u306a\u5185\u5bb9\u306f\u6b21\u306e\u70b9\u3067\u3059\u3002\n\n* LongFormer\n* NER\u306e\u5b9a\u5f0f\u5316\n* 1-fold\n\n\u7c21\u5358\u306a\u5909\u66f4\u3067\u3001\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3092\u8cea\u554f\u3068\u56de\u7b54\u306e\u5b9a\u5f0f\u5316\u306b\u5909\u63db\u3057\u3001\u3055\u307e\u3056\u307e\u306a\u30d0\u30c3\u30af\u30dc\u30fc\u30f3\u3092\u8a66\u3059\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u3055\u3089\u306b\u3001\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306f1\u3064\u6298\u308a\u3067\u3059\u3002 90\uff05\u306e\u30c7\u30fc\u30bf\u3067\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3057\u300110\uff05\u306e\u30c7\u30fc\u30bf\u3067\u691c\u8a3c\u3057\u307e\u3059\u3002 \u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3092K\u30d5\u30a9\u30fc\u30eb\u30c9\u306b\u5909\u63db\u3059\u308b\u304b\u3001LB\u3092\u30d6\u30fc\u30b9\u30c8\u3059\u308b\u305f\u3081\u306b100\uff05\u306e\u30c7\u30fc\u30bf\u3067\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002\n\nTransformer\u30e2\u30c7\u30eb\u306eLongFormer\u306b\u3064\u3044\u3066\u3001\u65e5\u672c\u8a9e\u306e\u89e3\u8aac\u306f[\u3053\u3053][1]\u304c\u826f\u3044\u3068\u601d\u308f\u308c\u307e\u3059\u3002 Roberta\u306b\u4f3c\u3066\u3044\u307e\u3059\u304c\u3001\u6700\u59274096\u30c8\u30fc\u30af\u30f3\u306e\u5165\u529b\u3092\u53d7\u3051\u5165\u308c\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u3067\u306f\u30011024\u5e45\u306e\u30c8\u30fc\u30af\u30f3\u3092Transformer\u306b\u4f9b\u7d66\u3057\u307e\u3059\u3002 HuggingFace\u30e6\u30fc\u30b6\u30fc\u306eAllenAI\u306f\u3001\u4e8b\u524d\u306b\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3055\u308c\u305f\u30a6\u30a7\u30a4\u30c8\u3092\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3057\u3066\u304f\u308c\u307e\u3057\u305f[\u3053\u3053][2]\u3002\n\n[1]: https:\/\/data-analytics.fun\/2020\/12\/14\/understanding-longformer\/\n[2]: https:\/\/huggingface.co\/allenai\/longformer-base-4096\n[3]: https:\/\/www.kaggle.com\/cdeotte\/tensorflow-longformer-ner-cv-0-633","cb1117b1":"# \u30e2\u30c7\u30eb\u306e\u691c\u8a3c-OOF\u3092\u63a8\u6e2c\u3059\u308b\n\n\u6b21\u306b\u3001\u691c\u8a3c\u30c6\u30ad\u30b9\u30c8\u306e\u4e88\u6e2c\u3092\u884c\u3044\u307e\u3059\u3002 \u3053\u306e\u30e2\u30c7\u30eb\u306f\u3001\u30c8\u30fc\u30af\u30f3\u3054\u3068\u306b\u30e9\u30d9\u30eb\u4e88\u6e2c\u3092\u884c\u3044\u307e\u3059\u3002\u3053\u308c\u3092\u5404\u30e9\u30d9\u30eb\u306e\u5358\u8a9e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306e\u30ea\u30b9\u30c8\u306b\u5909\u63db\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002 \u30c8\u30fc\u30af\u30f3\u3068\u5358\u8a9e\u306f\u540c\u3058\u3067\u306f\u306a\u3044\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u3002 1\u3064\u306e\u5358\u8a9e\u304c\u8907\u6570\u306e\u30c8\u30fc\u30af\u30f3\u306b\u5206\u5272\u3055\u308c\u308b\u5834\u5408\u304c\u3042\u308a\u307e\u3059\u3002 \u3057\u305f\u304c\u3063\u3066\u3001\u6700\u521d\u306b\u30de\u30c3\u30d7\u3092\u4f5c\u6210\u3057\u3066\u3001\u30c8\u30fc\u30af\u30f3\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u5358\u8a9e\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306b\u5909\u66f4\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002","f1599733":"# \u30c8\u30ec\u30a4\u30f3\u3092\u30c8\u30fc\u30af\u30f3\u5316\u3059\u308b\n\u6b21\u306e\u30b3\u30fc\u30c9\u306f\u3001Kaggle\u306e\u30c8\u30ec\u30a4\u30f3\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u3092\u3001NER\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306b\u4f7f\u7528\u3067\u304d\u308bNER\u30c8\u30fc\u30af\u30f3\u914d\u5217\u306b\u5909\u63db\u3057\u307e\u3059\u3002 \u3069\u306e\u30bf\u30fc\u30b2\u30c3\u30c8\u304c\u3069\u306e\u30af\u30e9\u30b9\u306b\u5c5e\u3057\u3066\u3044\u308b\u304b\u3092\u660e\u78ba\u306b\u3057\u307e\u3057\u305f\u3002 \u3053\u308c\u306b\u3088\u308a\u3001\u5fc5\u8981\u306b\u5fdc\u3058\u3066\u3001\u3053\u306e\u30b3\u30fc\u30c9\u3092\u975e\u5e38\u306b\u7c21\u5358\u306b\u8cea\u554f\u56de\u7b54\u306e\u5b9a\u5f0f\u5316\u306b\u5909\u63db\u3067\u304d\u307e\u3059\u3002 14\u500b\u306eNER\u914d\u5217\u3092\u30017\u3064\u306e\u30af\u30e9\u30b9\u305d\u308c\u305e\u308c\u306e\u958b\u59cb\u4f4d\u7f6e\u3068\u7d42\u4e86\u4f4d\u7f6e\u306e14\u500b\u306e\u914d\u5217\u306b\u5909\u66f4\u3059\u308b\u3060\u3051\u3067\u3059\u3002 \uff081\u3064\u306e\u30c6\u30ad\u30b9\u30c8\u306b1\u3064\u306e\u30af\u30e9\u30b9\u304c\u8907\u6570\u3042\u308b\u5834\u5408\u306f\u3001\u3069\u3046\u3059\u308c\u3070\u3088\u3044\u304b\u3092\u5275\u9020\u7684\u306b\u8003\u3048\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\uff09\u3002","040825ee":"# \u69cb\u6210\u8aac\u660e\nLongFormer\u306f\u3001Hugging Face\u793e\u304c\u63d0\u4f9b\u3057\u3066\u3044\u308bTransformer\u7cfb\u306e\u30e2\u30c7\u30eb\u306b\u7279\u5316\u3057\u305fDeep Learning\u306e\u30d5\u30ec\u30fc\u30e0\u30ef\u30fc\u30af\u304b\u3089\u4f7f\u7528\u3057\u307e\u3059\u3002\nTransformer\u7cfb\u306e\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u3059\u308b\u306e\u306b\u5fc5\u8981\u306aTokenizer\u3084Pretrain\u30e2\u30c7\u30eb\u3092\u3001\u4e0b\u306e\u3088\u3046\u306bHugging Face\u793e\u306eHP\u4e0a\u306b\u516c\u958b\u3055\u308c\u305f\u3082\u306e\u304b\u3089\u7c21\u5358\u306b\u30ed\u30fc\u30c9\u3059\u308b\u3053\u3068\u304c\u51fa\u6765\u307e\u3059\u3002\n\n\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306f\u3001\u65b0\u3057\u3044\u30e2\u30c7\u30eb\u3092\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3059\u308b\u3053\u3068\u3082\u3001\u4ee5\u524d\u306b\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u3057\u305f\u30e2\u30c7\u30eb\uff08\u4ee5\u524d\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u30d0\u30fc\u30b8\u30e7\u30f3\u304b\u3089\u4f5c\u6210\uff09\u3092\u30ed\u30fc\u30c9\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\u3002 \u3055\u3089\u306b\u3001\u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u306f\u3001\u65b0\u3057\u3044NER(Named Entity Recognition; \u56fa\u6709\u8868\u73fe\u62bd\u51fa)\u30c8\u30fc\u30af\u30f3\u3092\u4f5c\u6210\u3059\u308b\u304b\u3001\u65e2\u5b58\u306e\u30c8\u30fc\u30af\u30f3\uff08\u4ee5\u524d\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u30d0\u30fc\u30b8\u30e7\u30f3\u304b\u3089\u4f5c\u6210\uff09\u3092\u30ed\u30fc\u30c9\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u3053\u306e\u30ce\u30fc\u30c8\u30d6\u30c3\u30af\u30d0\u30fc\u30b8\u30e7\u30f3\u3067\u306f\u3001\u30e2\u30c7\u30eb\u3092\u30ed\u30fc\u30c9\u3057\u3001NER\u30c8\u30fc\u30af\u30f3\u3092\u30ed\u30fc\u30c9\u3057\u307e\u3059\u3002","c56ec663":"![](https:\/\/raw.githubusercontent.com\/cdeotte\/Kaggle_Images\/main\/Dec-2021\/lengths4.png)\n\n\u4e0a\u8a18\u306e\u30c8\u30ec\u30a4\u30f3\u30ef\u30fc\u30c9\u30ab\u30a6\u30f3\u30c8\u306e\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u304b\u3089\u30011024\u306e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u5e45\u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u306f\u3001\u30c7\u30fc\u30bf\u306e\u4fe1\u53f7\u306e\u5927\u90e8\u5206\u3092\u30ad\u30e3\u30d7\u30c1\u30e3\u3059\u308b\u3053\u3068\u3067\u69cb\u6210\u3055\u308c\u307e\u3059\u304c\u3001\u30e2\u30c7\u30eb\u304c\u5927\u304d\u3059\u304e\u306a\u3044\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002 \uff08\u30c8\u30ec\u30a4\u30f3**\u30c8\u30fc\u30af\u30f3**\u30ab\u30a6\u30f3\u30c8\u306e\u30d2\u30b9\u30c8\u30b0\u30e9\u30e0\u3092\u5206\u6790\u3059\u308b\u65b9\u304c\u826f\u3044\u3053\u3068\u306b\u6ce8\u610f\u3057\u3066\u304f\u3060\u3055\u3044\u304c\u3001\u3053\u3053\u3067\u306f\u305d\u308c\u3092\u884c\u3044\u307e\u305b\u3093\uff09\u3002 \u304a\u305d\u3089\u304f\u3001512\u304b\u30891024\u306e\u9593\u306e\u4ed6\u306e\u5e45\u3082\u8abf\u3079\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u3002 \u307e\u305f\u306f\u3001\u30b5\u30a4\u30ba\u304c512\u4ee5\u4e0b\u306e\u5e45\u3092\u4f7f\u7528\u3057\u30011\u3064\u306e\u30c6\u30ad\u30b9\u30c8\u3092\u8907\u6570\u306e\u30c1\u30e3\u30f3\u30af\u306b\u5206\u5272\u3059\u308b\u30b9\u30c8\u30e9\u30a4\u30c9\u3092\u4f7f\u7528\u3059\u308b\u3053\u3068\u3082\u3067\u304d\u307e\u3059\uff08\u91cd\u8907\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\uff09\u3002","5125bfd9":"# \u30e9\u30a4\u30d6\u30e9\u30ea\u306e\u8aad\u307f\u8fbc\u307f","3af71040":"# \u30a4\u30f3\u30bf\u30fc\u30cd\u30c3\u30c8\u306a\u3057\u3067TensorFlow\u3092\u9001\u4fe1\u3059\u308b\u65b9\u6cd5\nHuggingFace Transformer\u3092\u4f7f\u7528\u3059\u308b\u3068\u3001\u7c21\u5358\u3067\u3059\u3002 \u6b21\u306e3\u3064\uff081\uff09\u30e2\u30c7\u30eb\u306e\u91cd\u307f\u3001\uff082\uff09\u30c8\u30fc\u30af\u30ca\u30a4\u30b6\u30fc\u30d5\u30a1\u30a4\u30eb\u3001\uff083\uff09\u69cb\u6210\u30d5\u30a1\u30a4\u30eb\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3057\u3001Kaggle\u30c7\u30fc\u30bf\u30bb\u30c3\u30c8\u306b\u30a2\u30c3\u30d7\u30ed\u30fc\u30c9\u3059\u308b\u3060\u3051\u3067\u3059\u3002 \u4ee5\u4e0b\u306b\u3001AllenAI\u306e\u30e2\u30c7\u30eblongformer-base\u306eHuggingFace\u304b\u3089\u30d5\u30a1\u30a4\u30eb\u3092\u53d6\u5f97\u3059\u308b\u65b9\u6cd5\u306e\u30b3\u30fc\u30c9\u3092\u793a\u3057\u307e\u3059\u3002 \u3057\u304b\u3057\u3001\u3053\u308c\u3068\u540c\u3058\u30b3\u30fc\u30c9\u3067\u3001roberta-base\u306a\u3069\u306e\u4efb\u610f\u306e\u30c8\u30e9\u30f3\u30b9\u30d5\u30a9\u30fc\u30de\u30fc\u3092\u30c0\u30a6\u30f3\u30ed\u30fc\u30c9\u3067\u304d\u307e\u3059\u3002","6a699364":"# submission.csv\u306e\u66f8\u304d\u8fbc\u307f"}}