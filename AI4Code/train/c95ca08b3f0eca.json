{"cell_type":{"7e26fc76":"code","593f097d":"code","9b7c791d":"code","e31446ad":"code","e489a7d6":"code","f0520a29":"code","e0038a61":"code","c1313b7b":"code","f99733c7":"code","bf0f583f":"code","cbe4ab76":"code","692648dd":"code","53edda23":"code","c6c66115":"code","64ac7b17":"code","bcd7f61f":"code","5191398c":"code","b8639b05":"code","7a0dcf0c":"code","e0ebd651":"code","2e3d3c84":"code","c4762105":"code","9bdc881f":"code","dd4ff883":"code","67d215d9":"code","63db0e4c":"code","43074c51":"code","5f4db283":"code","e20293cf":"code","ec30f1c1":"code","b7a6edad":"code","856f8e57":"code","74054b1f":"code","ac3b6f4f":"code","4a286522":"code","bd3d7b16":"code","3ba23d3f":"code","7c44d1f5":"code","5821a3b9":"code","edd80658":"code","b01e6a9c":"code","e0a49f46":"code","10cb5b18":"code","382041b5":"code","8817c652":"code","61fcec0b":"code","5afaee7a":"code","2c8083ef":"code","0390fc78":"code","eb35d054":"code","3a1fa939":"code","5b91d36d":"code","42d5a566":"code","f00add28":"code","7f5de1dc":"code","02a6fbe0":"code","a919206e":"code","adc18a83":"code","57f63326":"code","cd7e83cf":"code","544a16d8":"code","2547af3c":"code","ed0de69c":"code","f9c75b27":"code","c7eafc8f":"code","d051feb5":"code","dea93686":"code","63c102d4":"code","eb4018b0":"code","f8f00937":"markdown","8ac44276":"markdown","32c2d0f4":"markdown","e7992a14":"markdown","668adf63":"markdown","055b2293":"markdown","d523339a":"markdown","65ba43a1":"markdown","69687d9e":"markdown","8f8b7a56":"markdown","1effd91c":"markdown","b4a2a550":"markdown","efb35389":"markdown","4cdb9847":"markdown","8787c9bf":"markdown","2807f40d":"markdown","dd67e1c6":"markdown","c84afe17":"markdown","25a94a61":"markdown","2bebe30e":"markdown","baa9c8f9":"markdown","7e70faf0":"markdown","7e9bedfe":"markdown","0dcdb9fd":"markdown","2b89baf8":"markdown","f263c809":"markdown","1a019e8d":"markdown","e75767ea":"markdown","5ccd8bfc":"markdown","b3f7a88c":"markdown","649aa410":"markdown","a6cc22a6":"markdown","01eaded1":"markdown","a51dc09b":"markdown","44ae9b61":"markdown","f1d3eb41":"markdown","d0b6e63b":"markdown","1bdb41ff":"markdown","c0acaf53":"markdown","ee3f164d":"markdown","9e8a02f0":"markdown","a6a5fb6c":"markdown","d90b59b7":"markdown","945d6606":"markdown","7d247b44":"markdown","bf46873c":"markdown","53acc65e":"markdown","ab9e0998":"markdown","d693519a":"markdown","86cc66bb":"markdown","4a3824a8":"markdown","0c699d72":"markdown","ada1a70b":"markdown","0551dbcc":"markdown","2e3dff13":"markdown","579c0261":"markdown","f8b6159a":"markdown","f6549509":"markdown","204e2ebb":"markdown","79e83a9c":"markdown","3f9fe445":"markdown","cd5b9067":"markdown","fffa67ed":"markdown","0b615f69":"markdown","7da22e53":"markdown","513d00ea":"markdown","ea36a364":"markdown","20abc355":"markdown","b60582cb":"markdown","fd0ba962":"markdown","99aa7bed":"markdown","78d8cfa2":"markdown","49ac7bc7":"markdown"},"source":{"7e26fc76":"import numpy as np \nimport pandas as pd \nimport os\n       \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold","593f097d":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt \n\nimport transformers\nimport random\n\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\n\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nscaler = torch.cuda.amp.GradScaler() # GPU\u3067\u306e\u9ad8\u901f\u5316\u3002\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # cpu\u304cgpu\u304b\u3092\u81ea\u52d5\u5224\u65ad\ndevice","9b7c791d":"def random_seed(SEED):\n    \n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    np.random.seed(SEED)\n    torch.manual_seed(SEED)\n    torch.cuda.manual_seed(SEED)\n    torch.cuda.manual_seed_all(SEED)\n    torch.backends.cudnn.deterministic = True\n","e31446ad":"class CFG():\n    \n    epochs = 20\n    \n    train_batch = 16\n    valid_batch = 32\n    \n    kfold = 5\n    \n    LR = 2e-5\n    \n    num_steps = 0.1\n    \n    endepoch = 10\n    \n    sentence_len = 314   \n   \n    ","e489a7d6":"CFG = CFG()","f0520a29":"train = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntrain.head(3)","e0038a61":"train.excerpt.iloc[0]","c1313b7b":"test = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ntest.head(3)","f99733c7":"sample = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")\nsample","bf0f583f":"train = train.sort_values(\"target\").reset_index(drop=True)\ntrain","cbe4ab76":"train[\"kfold\"] = train.index % 5","692648dd":"train","53edda23":"BERT_tokenizer = transformers.BertTokenizer.from_pretrained(\"..\/input\/bert-base-uncased\")","c6c66115":"Roberta_tokenizer =  transformers.RobertaTokenizer.from_pretrained(\"..\/input\/roberta-base\")","64ac7b17":"class MyDataSet(Dataset):\n    \n    def __init__(self,sentences,targets,tokenizer):\n        \n        self.sentences = sentences\n        self.targets = targets\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        \n        return len(self.sentences)\n    \n    def __getitem__(self,idx):\n        \n        sentence = self.sentences[idx]\n        \n       \n        bert_sens = self.tokenizer.encode_plus(\n                                sentence,\n                                add_special_tokens = True, \n                                max_length = CFG.sentence_len, \n                                pad_to_max_length = True, \n                                return_attention_mask = True,\n                                truncation=True)\n        \n        \n\n        ids = torch.tensor(bert_sens['input_ids'], dtype=torch.long)\n        mask = torch.tensor(bert_sens['attention_mask'], dtype=torch.long)\n        #token_type_ids = torch.tensor(bert_sens['token_type_ids'], dtype=torch.long)\n     \n            \n        target = torch.tensor(self.targets[idx],dtype=torch.float)\n        \n        return {\n                'ids': ids,\n                'mask': mask,\n                #'token_type_ids': token_type_ids,\n                'targets': target\n            }","bcd7f61f":"def loss_fn(output,target):\n    return torch.sqrt(nn.MSELoss()(output,target))","5191398c":"def training(\n    train_dataloader,\n    model,\n    optimizer,\n    scheduler\n):\n    \n    model.train()\n    torch.backends.cudnn.benchmark = True\n\n    allpreds = []\n    alltargets = []\n\n    for num,a in enumerate(train_dataloader):\n\n        losses = []\n\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast():\n\n            ids = a[\"ids\"].to(device,non_blocking=True)\n            mask = a[\"mask\"].to(device,non_blocking=True)\n      #      tokentype = a[\"token_type_ids\"].to(device,non_blocking=True)\n    \n            output = model(ids,mask)\n            output = output[\"logits\"].squeeze(-1)\n\n            target = a[\"targets\"].to(device,non_blocking=True)\n\n            loss = loss_fn(output,target)\n\n\n            # For scoring\n            losses.append(loss.item())\n\n            allpreds.append(output.detach().cpu().numpy())\n            alltargets.append(target.detach().squeeze(-1).cpu().numpy())\n\n        scaler.scale(loss).backward() # backwards of loss\n        \n        \n        scaler.step(optimizer) # Update optimizer\n        scaler.update() # scaler update\n            \n\n\n        scheduler.step() # Update learning rate schedule\n        del loss\n        \n        # Combine dataloader minutes\n\n    allpreds = np.concatenate(allpreds)\n    alltargets = np.concatenate(alltargets)\n\n    # I don't use loss, but I collect it\n\n    losses = np.mean(losses)\n\n    # Score with rmse\n    train_rme_loss = np.sqrt(mean_squared_error(alltargets,allpreds))\n    \n    \n\n    return losses,train_rme_loss","b8639b05":"def validating(\n    valid_dataloader,\n    model\n):\n    \n    model.eval()\n\n    allpreds = []\n    alltargets = []\n\n    for a in valid_dataloader:\n\n        losses = []\n\n        with torch.no_grad():\n\n            ids = a[\"ids\"].to(device,non_blocking=True)\n            mask = a[\"mask\"].to(device,non_blocking=True)\n            #tokentype = a[\"token_type_ids\"].to(device,non_blocking=True)\n\n            output = model(ids,mask)\n            output = output[\"logits\"].squeeze(-1)\n\n            target = a[\"targets\"].to(device,non_blocking=True)\n\n            loss = loss_fn(output,target)\n\n\n            # For scoring\n            losses.append(loss.item())\n            allpreds.append(output.detach().cpu().numpy())\n            alltargets.append(target.detach().squeeze(-1).cpu().numpy())\n            \n            del loss\n\n\n    # Combine dataloader minutes\n\n    allpreds = np.concatenate(allpreds)\n    alltargets = np.concatenate(alltargets)\n\n    # I don't use loss, but I collect it\n\n    losses = np.mean(losses)\n\n    # Score with rmse\n    valid_rme_loss = np.sqrt(mean_squared_error(alltargets,allpreds))\n\n    return allpreds,losses,valid_rme_loss","7a0dcf0c":"def initialize(fold,SEED,tokenizer,mode):\n    random_seed(SEED)\n\n    p_train = train[train[\"kfold\"]!=fold].reset_index(drop=True)\n    p_valid = train[train[\"kfold\"]==fold].reset_index(drop=True)\n\n\n    train_dataset = MyDataSet(p_train[\"excerpt\"],p_train[\"target\"],tokenizer)\n    valid_dataset = MyDataSet(p_valid[\"excerpt\"],p_valid[\"target\"],tokenizer)\n\n    train_dataloader = DataLoader(train_dataset,batch_size=CFG.train_batch,shuffle = True,num_workers=4,pin_memory=True)\n    valid_dataloader = DataLoader(valid_dataset,batch_size=CFG.valid_batch,shuffle = False,num_workers=4,pin_memory=True)\n\n    \n    if mode == \"BERT\":\n        model = transformers.BertForSequenceClassification.from_pretrained(\"..\/input\/bert-base-uncased\",num_labels=1)\n    else:\n        model = transformers.RobertaForSequenceClassification.from_pretrained(\"..\/input\/roberta-base\",num_labels=1)\n    model.to(device)\n\n    optimizer = AdamW(model.parameters(), CFG.LR,betas=(0.9, 0.999), weight_decay=1e-2) # AdamW optimizer\n\n    train_steps = int(len(p_train)\/CFG.train_batch*CFG.epochs)\n\n    num_steps = int(train_steps*CFG.num_steps)\n\n    scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)\n    \n    scaler = torch.cuda.amp.GradScaler() # GPU\u3067\u306e\u9ad8\u901f\u5316\u3002\n    \n    return train_dataloader,valid_dataloader,model,optimizer,scheduler,scaler","e0ebd651":"SEEDS = [100,101,102,103,104]","2e3d3c84":"train_exe = False # I strongly recommend setting it to false.","c4762105":"if train_exe:\n    \n    trainlosses = []\n    vallosses = []\n    \n    trainscores = []\n    validscores = []\n\n    BERTresult = []\n\n    for fold in range(CFG.kfold):\n\n        bestscore = 100\n\n        for SEED in SEEDS:\n\n            train_dataloader,valid_dataloader,model,optimizer,scheduler,scaler = initialize(fold,SEED,BERT_tokenizer,\"BERT\")    \n\n            for epoch in tqdm(range(CFG.epochs)):\n\n                print(\"-{}{}--{}{}----{}{}----{}----\".format(\"fold:\",str(fold),\"seed:\",str(SEED),\"epoch:\",str(epoch),\"start\"))\n\n                trainloss,trainscore = training(train_dataloader,model,optimizer,scheduler)\n\n                trainlosses.append(trainloss)\n                trainscores.append(trainscore)\n\n                print(\"trainscore is \" + str(trainscore))\n\n                preds,validloss,valscore=validating(valid_dataloader,model)\n\n                vallosses.append(validloss)\n                validscores.append(valscore)\n\n\n                print(\"valscore is \" + str(valscore))\n\n                if bestscore > valscore:\n\n                    bestscore = valscore\n\n                    print(\"found better point\")\n\n                    state = {\n                                    'state_dict': model.state_dict(),\n                                    'optimizer_dict': optimizer.state_dict(),\n                                    \"bestscore\":bestscore\n                                }\n\n\n                    torch.save(state, \"BERTmodel_fold\" + str(fold) + \".pth\")\n\n                else:\n                    pass\n\n                BERTresult.append([fold,SEED,epoch,trainloss,trainscore,validloss,valscore,bestscore])\n\n                if epoch == CFG.endepoch:\n                    break\n            \n    BERTdf = pd.DataFrame(BERTresult)\n    BERTdf.columns=[\"fold\",\"SEED\",\"epoch\",\"trainloss\",\"trainscore\",\"validloss\",\"valscore\",\"bestscore\"]\n    BERTdf.to_csv(\"BERTdf.csv\",index=False)","9bdc881f":"if train_exe:\n   \n\n    trainlosses = []\n    vallosses = []\n    bestscore = None\n\n    trainscores = []\n    validscores = []\n\n    Robertaresult = []\n\n    for fold in range(CFG.kfold):\n\n\n        bestscore = 100\n\n        for SEED in SEEDS:\n\n            train_dataloader,valid_dataloader,model,optimizer,scheduler,scaler = initialize(fold,SEED,Roberta_tokenizer,\"Roberta\") \n\n\n            for epoch in tqdm(range(CFG.epochs)):\n\n                print(\"-{}{}--{}{}----{}{}----{}----\".format(\"fold:\",str(fold),\"seed:\",str(SEED),\"epoch:\",str(epoch),\"start\"))\n\n                trainloss,trainscore = training(train_dataloader,model,optimizer,scheduler)\n\n                trainlosses.append(trainloss)\n                trainscores.append(trainscore)\n\n                print(\"trainscore is \" + str(trainscore))\n\n                preds,validloss,valscore=validating(valid_dataloader,model)\n\n                vallosses.append(validloss)\n                validscores.append(valscore)\n\n\n                print(\"valscore is \" + str(valscore))\n\n\n\n                if bestscore > valscore:\n\n                    bestscore = valscore\n\n                    print(\"found better point\")\n\n                    state = {\n                                    'state_dict': model.state_dict(),\n                                    'optimizer_dict': optimizer.state_dict(),\n                                    \"bestscore\":bestscore\n                                }\n\n\n                    torch.save(state, \"Robertamodel_fold\" + str(fold) + \".pth\")\n\n                else:\n                    pass\n\n                Robertaresult.append([fold,SEED,epoch,trainloss,trainscore,validloss,valscore,bestscore])\n\n                if epoch == CFG.endepoch:\n                    break\n\n        Robertadf = pd.DataFrame(Robertaresult)\n        Robertadf.columns=[\"fold\",\"SEED\",\"epoch\",\"trainloss\",\"trainscore\",\"validloss\",\"valscore\",\"bestscore\"]\n        Robertadf.to_csv(\"Robertadf.csv\",index=False)","dd4ff883":"BERTres = pd.read_csv(\"..\/input\/allresbertvsrobert\/BERT_allres.csv\")\nBERTres","67d215d9":"BERTres[BERTres[\"fold\"]==0]","63db0e4c":"RoBERTa_res = pd.read_csv(\"..\/input\/allresbertvsrobert\/Roberta_allres.csv\")\nRoBERTa_res","43074c51":"BERT_fold0 = BERTres[BERTres[\"fold\"]==0]\nRoberta_fold0 = RoBERTa_res[RoBERTa_res[\"fold\"]==0]","5f4db283":"BERT_fold0.head(3)","e20293cf":"plt.figure(figsize = (20,5))\nplt.subplot(1,2,1)\nfor a in BERT_fold0[\"SEED\"].unique():\n    tmpdf = BERT_fold0[BERT_fold0[\"SEED\"]==a]\n    plt.scatter(tmpdf[\"epoch\"],tmpdf[\"valscore\"],label = \"seed=\"+str(a))\n    plt.plot(tmpdf[\"epoch\"],tmpdf[\"valscore\"],)\nplt.title(\"BERT-fold0\",fontsize = 23)\nplt.xlabel(\"epoch\",fontsize = 15)\nplt.ylabel(\"validation score\",fontsize = 15)\n\nplt.ylim(0.45,0.9)\nplt.legend()\nplt.grid()\n\n\nplt.subplot(1,2,2)\nfor a in Roberta_fold0[\"SEED\"].unique():\n    tmpdf = Roberta_fold0[Roberta_fold0[\"SEED\"]==a]\n    plt.scatter(tmpdf[\"epoch\"],tmpdf[\"valscore\"],label = \"seed=\"+str(a))\n    plt.plot(tmpdf[\"epoch\"],tmpdf[\"valscore\"])\n\nplt.title(\"RoBERTa-fold0\",fontsize = 23)\nplt.xlabel(\"epoch\",fontsize = 15)\nplt.ylabel(\"validation score\",fontsize = 15)\n\nplt.ylim(0.45,0.9)\nplt.legend()\nplt.grid()\n","ec30f1c1":"BERTdf_mean = BERT_fold0.groupby(\"epoch\")[\"valscore\"].mean().reset_index()\nBERTdf_mean.columns = [\"epoch\",\"BERT_scores_mean\"]\nRobertadf_mean = Roberta_fold0.groupby(\"epoch\")[\"valscore\"].mean().reset_index()\nRobertadf_mean.columns = [\"epoch\",\"RoBERTa_scores_mean\"]\nResdf_mean = pd.merge(BERTdf_mean ,Robertadf_mean,on=\"epoch\")\nResdf_mean","b7a6edad":"x = np.arange(Resdf_mean[\"epoch\"].max()+1)\nplt.plot(x,Resdf_mean[\"BERT_scores_mean\"],label=\"BERT_mean\")\nplt.plot(x,Resdf_mean[\"RoBERTa_scores_mean\"],label=\"RoBERTa_mean\")\n\nplt.ylim(0.45,0.85)\nplt.legend()\n\nplt.title(\"Mean value comparison of BERT and RoBERTa at fold0\",fontsize = 15)\nplt.xlabel(\"epoch\",fontsize = 15)\nplt.ylabel(\"validation score\",fontsize = 15)\n\nplt.grid()\n","856f8e57":"BERTres.head(3)","74054b1f":"tmp = BERTres.groupby([\"fold\",\"SEED\"])[\"valscore\"].min().reset_index()\ntmp.head(12)","ac3b6f4f":"import seaborn as sns\n\nsns.boxplot(x=\"fold\",y=\"valscore\",data=tmp)\n","4a286522":"tmp2 = tmp.groupby(\"fold\")[\"valscore\"].idxmin().reset_index()\ntmp2","bd3d7b16":"BERT_bestscore = tmp.iloc[tmp2[\"valscore\"],:]\nBERT_bestscore.columns = [\"fold\",\"BERT_SEED\",\"BERT_val_bestscore\"]\nBERT_bestscore","3ba23d3f":"tmp = RoBERTa_res.groupby([\"fold\",\"SEED\"])[\"valscore\"].min().reset_index()\nsns.boxplot(x=\"fold\",y=\"valscore\",data=tmp)\n","7c44d1f5":"tmp2 = tmp.groupby(\"fold\")[\"valscore\"].idxmin().reset_index()\nRoBERTa_bestscore = tmp.iloc[tmp2[\"valscore\"],:]\nRoBERTa_bestscore.columns = [\"fold\",\"RoBERTa_SEED\",\"RoBERTa_val_bestscore\"]\nRoBERTa_bestscore","5821a3b9":"Mergeres = pd.merge(BERT_bestscore,RoBERTa_bestscore,on=\"fold\")\nMergeres","edd80658":"Mergeres[\"valscore_better_model\"] = np.where(Mergeres[\"BERT_val_bestscore\"]<Mergeres[\"RoBERTa_val_bestscore\"],\"BERT\",\"RoBERTa\")\nMergeres","b01e6a9c":"BERT_bestscore","e0a49f46":"BERT_publicLB = [0.544,0.554,0.534,0.548,0.572]","10cb5b18":"BERT_bestscore[\"BERT_publicLB\"] = BERT_publicLB","382041b5":"BERT_bestscore","8817c652":"RoBERTa_bestscore","61fcec0b":"RoBERTa_publicLB = [0.532,0.530,0.511,0.536,0.535]","5afaee7a":"RoBERTa_bestscore[\"RoBERTa_publicLB\"] = RoBERTa_publicLB\nRoBERTa_bestscore","2c8083ef":"Mergeres = pd.merge(BERT_bestscore,RoBERTa_bestscore,on=\"fold\")\nMergeres","0390fc78":"Mergeres[\"valscore_better_model\"] = np.where(Mergeres[\"BERT_val_bestscore\"]<Mergeres[\"RoBERTa_val_bestscore\"],\"BERT\",\"RoBERTa\")\nMergeres[\"publicLB_better_model\"] = np.where(Mergeres[\"BERT_publicLB\"]<Mergeres[\"RoBERTa_publicLB\"],\"BERT\",\"RoBERTa\")\nMergeres","eb35d054":"plt.figure(figsize=(5,5))\n\nplt.scatter(Mergeres[\"BERT_val_bestscore\"],Mergeres[\"BERT_publicLB\"],label=\"BERT\")\n\n\nplt.scatter(Mergeres[\"RoBERTa_val_bestscore\"],Mergeres[\"RoBERTa_publicLB\"],label=\"RoBERTa\")\n\nfor num,a in enumerate(Mergeres[\"fold\"]):\n    plt.annotate(\"fold \" + str(a),xy=(Mergeres[\"BERT_val_bestscore\"].iloc[num]+0.001,Mergeres[\"BERT_publicLB\"].iloc[num]+0.001),c=\"blue\")\n    plt.annotate(\"fold \" + str(a),xy=(Mergeres[\"RoBERTa_val_bestscore\"].iloc[num]+0.001,Mergeres[\"RoBERTa_publicLB\"].iloc[num]+0.001),c=\"orange\")\n    plt.annotate(\"y=x\",xy=(0.50,0.51),c=\"black\")\n\n\n\nx = y = np.arange(0.49,0.56,0.01)\nplt.plot(x,y,c=\"black\")\n\nplt.title(\"Comparison of my validation score and Public LB score\")\n\nplt.xlabel(\"My validation score\")\nplt.ylabel(\"Public LB score\")\n\nplt.legend()\nplt.grid()","3a1fa939":"Mergeres","5b91d36d":"BERTres = [Mergeres[\"BERT_val_bestscore\"].mean(),Mergeres[\"BERT_publicLB\"].mean(),0.522]\nRoBERTares = [Mergeres[\"RoBERTa_val_bestscore\"].mean(),Mergeres[\"RoBERTa_publicLB\"].mean(),0.505]","42d5a566":"foldmeans = pd.DataFrame()\nfoldmeans[\"BERTres\"] = BERTres\nfoldmeans[\"RoBERTares\"] = RoBERTares\n\nIndexname = [\"Mean validation_score\",\"Mean public LB with single model\",\"5 k-fold public LB result\"]\nfoldmeans[\"Metric\"] = Indexname\nfoldmeans = foldmeans.set_index(\"Metric\")\nfoldmeans","f00add28":"x1 = [1, 5, 9]\ny1 = foldmeans.BERTres\n\nx2 = [1.3, 5.3, 9.3]\ny2 = foldmeans.RoBERTares\n\nlabel_x = foldmeans.index\n\n# 1\u3064\u76ee\u306e\u68d2\u30b0\u30e9\u30d5\nplt.bar(x1, y1, color='b', width=0.3, label='BERT', align=\"center\")\n\n# 2\u3064\u76ee\u306e\u68d2\u30b0\u30e9\u30d5\nplt.bar(x2, y2, color='g', width=0.3, label='RoBERTa', align=\"center\")\n\n# \u51e1\u4f8b\nplt.legend(loc=4)\n\n# X\u8ef8\u306e\u76ee\u76db\u308a\u3092\u7f6e\u63db\nplt.xticks([1.15, 5.15, 9.15], label_x)\nplt.xticks(rotation=45)\nplt.show()","7f5de1dc":"class Inf_DataSet(Dataset):\n    \n    def __init__(self,sentences,tokenizer):\n        \n        self.sentences = sentences\n        self.tokenizer = tokenizer\n       \n        \n    def __len__(self):\n        \n        return len(self.sentences)\n    \n    def __getitem__(self,idx):\n        \n        sentence = self.sentences[idx]\n        \n        sentence = str(sentence) # adding in ver 16\n        sentence = \" \".join(sentence.split()) # adding in ver 16\n        \n#        sentence = sentence.replace('\\n', '') # adding in ver 15.\n       \n        \n        \n        \n        bert_sens = self.tokenizer.encode_plus(\n                                sentence,\n                                add_special_tokens = True, # [CLS],[SEP]\n                                max_length = CFG.sentence_len,\n                                pad_to_max_length = True, # add padding to blank\n                                truncation=True)\n\n        ids = torch.tensor(bert_sens['input_ids'], dtype=torch.long)\n        mask = torch.tensor(bert_sens['attention_mask'], dtype=torch.long)\n#        token_type_ids = torch.tensor(bert_sens['token_type_ids'], dtype=torch.long)\n     \n        \n    \n        \n        return {\n                'ids': ids,\n                'mask': mask,\n #               'token_type_ids': token_type_ids,\n                \n            }","02a6fbe0":"test_dataset = Inf_DataSet(test[\"excerpt\"],Roberta_tokenizer)","a919206e":"test_batch = 32","adc18a83":"test_dataloader = DataLoader(test_dataset,batch_size=test_batch,shuffle = False,num_workers=4,pin_memory=True)","57f63326":"model = transformers.RobertaForSequenceClassification.from_pretrained(\"..\/input\/roberta-base\",num_labels=1)","cd7e83cf":"#pthes = [os.path.join(\"..\/input\/roberta-res\",s) for s in os.listdir(\"..\/input\/roberta-res\")]\npthes = [\"..\/input\/roberta-res\/Robertamodel_fold0_seed102.pth\",\"..\/input\/roberta-res\/Robertamodel_fold1_seed103.pth\",\"..\/input\/roberta-res\/Robertamodel_fold2_seed100.pth\"\n        ,\"..\/input\/roberta-res\/Robertamodel_fold3_seed104.pth\"]\npthes","544a16d8":"def predicting(\n    test_dataloader,\n    model,\n    pthes\n    \n):\n\n    allpreds = []\n    \n    for pth in pthes:\n        \n        state = torch.load(pth)\n        model.load_state_dict(state[\"state_dict\"])\n        model.to(device)\n        model.eval()\n    \n    \n        preds = []\n    \n        with torch.no_grad():\n\n\n            for a in test_dataloader:\n\n\n\n                ids = a[\"ids\"].to(device)\n                mask = a[\"mask\"].to(device)\n                #tokentype = a[\"token_type_ids\"].to(device)\n\n                output = model(ids,mask)\n                output = output[\"logits\"].squeeze(-1)\n\n\n                preds.append(output.cpu().numpy())\n\n            preds = np.concatenate(preds)\n            \n            allpreds.append(preds)\n\n    return allpreds","2547af3c":"allpreds = predicting(test_dataloader,model,pthes)","ed0de69c":"findf = pd.DataFrame(allpreds)\nfindf = findf.T","f9c75b27":"findf","c7eafc8f":"finpred = findf.mean(axis=1)\nfinpred","d051feb5":"sample","dea93686":"sample[\"target\"] = finpred","63c102d4":"sample","eb4018b0":"sample.to_csv(\"submission.csv\",index = False)","f8f00937":"Enter the submitted result.\n\n\nsubmit\u3057\u305f\u7d50\u679c\u3092\u5165\u529b\u3057\u307e\u3059\u3002","8ac44276":"------------------------------","32c2d0f4":"\u30e9\u30f3\u30c0\u30e0\u30b7\u30fc\u30c9\u3054\u3068\u306b\u3001\u6700\u5c0f\u3068\u306a\u308bvalidation score\u306f\u3051\u3063\u3053\u3046\u3070\u3089\u3064\u3044\u3066\u3044\u308b\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3059\u3002","e7992a14":"-----------------------------------------------------","668adf63":"# 7. initialize function \n\n## ref : How to initialize the code correctly (English&\u65e5\u672c\u8a9e) \n\nhttps:\/\/www.kaggle.com\/chumajin\/how-to-initialize-the-code-correctly-english\/edit\/run\/64377180","055b2293":"##### \u521d\u671f\u5316\u3092\u6b63\u78ba\u306b\u884c\u3046\u30b3\u30fc\u30c9\u3092\u7528\u3044\u3066\u3001BERT\u3068RoBERTa\u3092\u30e9\u30f3\u30c0\u30e0\u30b7\u30fc\u30c9\u3092\u632f\u3063\u3066\u79c1\u306e\u30e2\u30c7\u30eb\u3067\u6bd4\u8f03\u3057\u307e\u3057\u305f\u3002\n\n##### \u7d50\u8ad6\u3068\u3057\u3066\u306f\u3001**BERT\u3088\u308a\u3082RoBERTa\u306e\u65b9\u304c\u826f\u3044\u3053\u3068\u3092\u78ba\u8a8d\u3057\u307e\u3057\u305f\u3002**\n\n##### \u305d\u306e\u904e\u7a0b\u3067\u3001\u79c1\u306e\u30e2\u30c7\u30eb\u306e\u5834\u5408\u3001\u4ee5\u4e0b\u306e\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3057\u305f\u3002\n\n* epoch\u3054\u3068\u306e\u3070\u3089\u3064\u304d\u304c\u5927\u304d\u3044\u3002\uff08\u3082\u3046\u5c11\u3057epoch\u5897\u3084\u3055\u306a\u3044\u3068\u5b89\u5b9a\u3057\u306a\u3044\u304b\u3082\uff09\n* seed\u3054\u3068\u306e\u3070\u3089\u3064\u304d\u304c\u5927\u304d\u3044\u3002 (seed\u3092\u4f55\u500b\u304b\u632f\u3063\u3066\u3001\u4e00\u756a\u826f\u3044\u30b9\u30b3\u30a2\u3092\u53d6\u308b\u3084\u308a\u65b9\u306f\u826f\u3044\u304b\u3082)\n* \u7279\u5b9a\u306efold\u3067\u306evalidation score\u306eLeaders board\u306b\u5bfe\u3059\u308b\u4fe1\u983c\u6027\u304c\u4f4e\u3044\u3002\uff08k-fold\u306e\u3084\u308a\u65b9\u3092\u5909\u3048\u305f\u65b9\u304c\u3044\u3044\u304b\u3082.\u7d50\u679c\u3068\u3057\u3066https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds\u3000\u306e\u5206\u3051\u65b9\u306e\u65b9\u304c\u826f\u304b\u3063\u305f\u3067\u3059\u3002)\n\n* inference\u3067\u6539\u884c\u3092\u629c\u304f\u3068\u30b9\u30b3\u30a2\u304c\u5c11\u3057\u4e0a\u304c\u308b(training\u3067\u306f\u672a\u691c\u8a3c)","d523339a":"\u5168\u90e8Roberta\u306e\u307b\u3046\u304c\u826f\u3044\u3068\u601d\u3063\u3066\u3044\u305f\u304c\u3001validation score\u3067\u6bd4\u8f03\u3059\u308b\u3068\u3001fold3\u3067bert\u306e\u307b\u3046\u304c\u826f\u3044\u7d50\u679c\u3082\u898b\u3089\u308c\u307e\u3057\u305f\u3002\n\n\n\u305d\u306e\u305f\u3081\u3001\u3053\u308c\u3089\u306esingle\u30e2\u30c7\u30eb\u3092\u5168\u90e8submit\u3057\u3066Public Leaders Board\u306e\u30b9\u30b3\u30a2\u3092\u6bd4\u8f03\u3057\u307e\u3057\u305f\u3002","65ba43a1":"#### To explain the meaning, in fold 0, when SEED was 101, the validation score was the lowest at 0.552.","69687d9e":"# 3. Tokenizer","8f8b7a56":"\u3053\u308c\u3089\u306e\u30e2\u30c7\u30eb\u306f\u3001ver 8(fold 0), ver 9(fold 1), ver 7(fold 2), ver 10(fold 3), ver 4(fold 4)\u3067\u8a13\u7df4\u3055\u308c\u305f\u3082\u306e\u3067\u3059\u3002\u305d\u308c\u3092\u307e\u3068\u3081\u3066dataset\u306b\u3057\u307e\u3057\u305f\u3002","1effd91c":"------------------------------","b4a2a550":"BERT\u3068Roberta\u306e\u3069\u3061\u3089\u304c\u826f\u304b\u3063\u305f\u304b\u3092fold\u3054\u3068\u306b\u6bd4\u8f03\u3057\u307e\u3059\u3002","efb35389":"## \u3053\u306efold\u3060\u3068\u3001RoBERTa\u306e\u307b\u3046\u304c\u7d50\u679c\u304c\u826f\u304f\u898b\u3048\u307e\u3059\u3002\n\u203b\u3000\u5b9f\u969b\u306f\u3001\u4ed6\u306efold\u3060\u3068\u5224\u65ad\u3057\u3065\u3089\u3044\u3082\u306e\u3082\u3042\u308a\u307e\u3057\u305f\u3002","4cdb9847":"#### Comparing the mean value.","8787c9bf":"--------------------------------------","2807f40d":"#### Next, for each fold, output the index that minimizes the validation score.\n\u6b21\u306b\u5404fold\u3054\u3068\u306bvalidation score\u304c\u6700\u5c0f\u3068\u306a\u308b\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u3092\u51fa\u3057\u307e\u3059\u3002","dd67e1c6":"\u3010\u6ce8\u610f\u3011\u53c2\u8003\u3068\u3057\u3066training\u30b3\u30fc\u30c9\u3092\u8f09\u305b\u307e\u3057\u305f\u304c\u3001\u5b9f\u969b\u306b\u6d41\u3059\u306815\u6642\u9593\u304b\u304b\u308b\u898b\u7a4d\u3082\u308a\u3067\u3001kaggle notebook\u3067\u306f\u6d41\u305b\u307e\u305b\u3093\u3002\u305d\u306e\u305f\u3081\u3001\u4ee5\u4e0b\u306etrain_exe=False\u306b\u3057\u3066\u3044\u307e\u3059\u3002\u4f7f\u7528\u3059\u308b\u5834\u5408\u306f\u30a2\u30ec\u30f3\u30b8\u3057\u3066\u3054\u4f7f\u7528\u304f\u3060\u3055\u3044(\u5b9f\u969b\u306f\u79c1\u306f\u5404fold\u3054\u3068\u306b3\u6642\u9593\u304b\u3051\u3066\u56de\u3057\u3066\u3044\u307e\u3059\u3002\u904e\u53bb\u306eversion\u3092\u3054\u53c2\u8003\u304f\u3060\u3055\u3044\u3002)\u3002\u5b9f\u969b\u306e\u8a13\u7df4\u306f\u3001ver 8\u3067fold 0, ver 9\u3067fold 1, ver 7\u3067fold 2, ver 10\u3067fold 3, ver 4\u3067fold 4\u3092\u3057\u307e\u3057\u305f\u3002","c84afe17":"#### I compared BERT and RoBERTa in my model with random seeds, using code that does the initialization correctly.\n\n#### In conclusion, **RoBERTa was better than BERT.**\n\n#### In the process, for my model, I found the following:\n\n* There is a large variation for each epoch. (It may not be stable unless you increase epoch a little more)\n* There is a large variation among seeds. (It may be better to shake some seeds and get the best score)\n* The validation score for a particular fold is less reliable for the Leaders board. (Maybe I should change the k-fold method. As a result, https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds is better.)\n\nver15\n* If you remove the \\n in inference, the score will increase a little (not verified in training)\n","25a94a61":"#### If you want to see only the result, please click the link below and jump from here.\u3000Chapters 0 to 10 are training codes as reference.(No training is performed in this version.)\n\n\u7d50\u679c\u3060\u3051\u898b\u305f\u3044\u65b9\u306f\u3053\u3053\u304b\u3089\u30b8\u30e3\u30f3\u30d7\u3057\u3066\u304f\u3060\u3055\u3044\u3002\u30c1\u30e3\u30d7\u30bf\u30fc0\uff5e10\u306f\u8a13\u7df4\u306e\u30b3\u30fc\u30c9\u3092\u53c2\u8003\u307e\u3067\u306b\u66f8\u3044\u305f\u3082\u306e\u3067\u3059\u3002(\u3053\u306eversion\u3067\u306f\u8a13\u7df4\u306f\u3057\u307e\u305b\u3093\u3002)\n\n## [Jump to 11. Comparison results of BERT and RoBERTa](#section-one)","2bebe30e":"# **About this notebook** \n## Using the method of accurate initialization\u203b1 as a base,\n## BERT and RoBERTa were compared with various random seeds in my model\u203b2.\n\n\u203bref1) How to initialize the code correctly (English&\u65e5\u672c\u8a9e)\u3000(I would be grateful if you could also upvote this.) :\nhttps:\/\/www.kaggle.com\/chumajin\/how-to-initialize-the-code-correctly-english\n\n\u203bref2) Pytorch BERT Biginner's room : \nhttps:\/\/www.kaggle.com\/chumajin\/pytorch-bert-beginner-s-room.\n\n### **Thank you for visiting this page. I'm looking forward to helping you even a little. I'm glad if you upvote!**\n### **Also, thank you for those who always upvote.**","baa9c8f9":"-----------------------------------------------","7e70faf0":"---------------\u65e5\u672c\u8a9e--------------\n\n## **\u6b63\u78ba\u306b\u521d\u671f\u5316\u3059\u308b\u65b9\u6cd5\u203b1\u3092\u30d9\u30fc\u30b9\u306b\u7528\u3044\u3066\u3001**\n## **\u30e9\u30f3\u30c0\u30e0\u30b7\u30fc\u30c9\u3092\u632f\u3063\u3066BERT\u3068RoBERTa \u3092\u79c1\u306e\u30e2\u30c7\u30eb\u203b2\u3067\u6bd4\u8f03\u3057\u307e\u3057\u305f\u3002**\n \n\u203b1) \u6b63\u78ba\u306b\u521d\u671f\u5316\u3059\u308b\u65b9\u6cd5(\u3053\u3061\u3089\u3082upvote\u3057\u3066\u3082\u3089\u3048\u308b\u3068\u5b09\u3057\u3044\u3067\u3059\u3002)\nHow to initialize data accurately(English&\u65e5\u672c\u8a9e) https:\/\/www.kaggle.com\/chumajin\/how-to-initialize-the-code-correctly-english\n \n\u203b2) \u79c1\u306e\u30e2\u30c7\u30eb Pytorch BERT Biginner's room https:\/\/www.kaggle.com\/chumajin\/pytorch-bert-beginner-s-room-version\u3000\n \n### **\u898b\u3066\u9802\u3044\u3066\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\u3002\u5c11\u3057\u3067\u3082\u304a\u5f79\u306b\u7acb\u3066\u305f\u3089\u5e78\u3044\u3067\u3059\u3002upvote\/follow\u9802\u3051\u305f\u3089\u5b09\u3057\u3044\u3067\u3059\uff01**\n### **\u904e\u53bb\u306bupvote\u3057\u3066\u304f\u308c\u305f\u65b9\u3001\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059!**","7e9bedfe":"Judge the comparison result of BERT and RoBERTa not only the validation score but also the Public LB score.\n\nBERT\u3068RoBERTa\u306e\u6bd4\u8f03\u7d50\u679c\u3092\u5148\u307b\u3069\u306evalidation score\u3060\u3051\u3067\u306a\u304f\u3001Public LB score\u3082judge\u3057\u307e\u3059\u3002","0dcdb9fd":"## 11.3 The result of comparing all folds\n\n\u3059\u3079\u3066\u306efold\u3067\u306e\u6bd4\u8f03","2b89baf8":"## 11.2 Example : Visualizing the validation score of fold 0 every SEED.\n\u4e8b\u4f8b\u3068\u3057\u3066\u3001fold 0\u3092randam seed\u3092\u632f\u3063\u305fvalidation score\u306e\u7d50\u679c\u3092\u793a\u3057\u307e\u3059\u3002","f263c809":"## fold 4\u306e\u7d50\u679c\u306f\u660e\u3089\u304b\u306b\u304a\u304b\u3057\u3044\u3053\u3068\u304c\u78ba\u8a8d\u3055\u308c\u305f\u3002fold3\u306eBERT\u306e\u7d50\u679c\u3082\u5c11\u3057\u305a\u308c\u3066\u3044\u308b\u3088\u3046\u306b\u601d\u3048\u307e\u3059\u3002\n## \u3053\u306e\u3053\u3068\u304b\u3089\u3001\u79c1\u306ek-fold\u306e\u5206\u3051\u65b9\u306f\u5c11\u3057\u826f\u304f\u306a\u3044\u3053\u3068\u304c\u308f\u304b\u308a\u307e\u3057\u305f\u3002","1a019e8d":"------------------------------------","e75767ea":"------------------------------","5ccd8bfc":"## 2.1 K-fold","b3f7a88c":"\n\n\n#### \u4e00\u898b\u3059\u308b\u3068\u3001RoBERTa\u306e\u65b9\u304cvalidation score\u304c\u5c0f\u3055\u304f\u3066\u826f\u3044\u3002\n#### \u307e\u305f\u3001\u79c1\u306e\u30e2\u30c7\u30eb\u3060\u3068\u3001random seed\u3054\u3068\u3001epoch\u3054\u3068\u306b\u304b\u306a\u308a\u3070\u3089\u3064\u304d\u304c\u5927\u304d\u3044\u3053\u3068\u304c\u78ba\u8a8d\u3055\u308c\u307e\u3059\u3002","649aa410":"# 8. Define evaluating SEED. In this notebook, the random seeds are set to from 100 to 105.\n\u4eca\u56de\u306f\u3001random seed\u3092100-105\u3067\u632f\u308a\u307e\u3059\u3002","a6cc22a6":"# 4. DataSet","01eaded1":"# --------------------------From here, I will show the all of results. ----------------------------------------------\n## Training was done in ver 8(fold 0), ver 9(fold 1), ver 7(fold 2), ver 10(fold 3), ver 4(fold 4). Each takes about 3 hours.\n \u8a13\u7df4\u306f\u3001ver 8\u3067fold 0, ver 9\u3067fold 1, ver 7\u3067fold 2, ver 10\u3067fold 3, ver 4\u3067fold 4\u3092\u3057\u307e\u3057\u305f\u3002\u305d\u308c\u305e\u308c3\u6642\u9593\u305a\u3064\u304b\u304b\u308a\u307e\u3059\u3002","a51dc09b":"#### Compare each fold to see which was better, BERT or Roberta.","44ae9b61":"# 2. Sample preparation","f1d3eb41":"The results of RoBERTa are the same.\n\nRoBERTa\u306e\u7d50\u679c\u3082\u540c\u69d8\u3067\u3059\u3002","d0b6e63b":"#### model path : these models were trained in ver 8(fold 0), ver 9(fold 1), ver 7(fold 2), ver 10(fold 3), ver 4(fold 4).I made the merged one into a dataset.","1bdb41ff":"Merge as you would for a validation score comparison.\n\nvalidation score\u306e\u6bd4\u8f03\u3068\u540c\u3058\u304f\u3001\u30de\u30fc\u30b8\u3057\u307e\u3059\u3002","c0acaf53":"### Extract the bestscore(min validation value) in each SEED and fold.\n\u5404fold\u3068seed\u3054\u3068\u306bvalidation\u306e\u6700\u5c0f\u5024(best score)\u3092\u62bd\u51fa\u3057\u307e\u3059\u3002","ee3f164d":"#### Next, Roberta's results are processed in the same way and merged.\n\u6b21\u306bRoberta\u306e\u7d50\u679c\u3082\u540c\u69d8\u306b\u51e6\u7406\u3057\u3066\u30de\u30fc\u30b8\u3057\u307e\u3059\u3002","9e8a02f0":"#### By extracting the index part, you can extract the minimum value and which seed was the minimum value in each fold.\n\u30a4\u30f3\u30c7\u30c3\u30af\u30b9\u306e\u7b87\u6240\u3092\u62bd\u51fa\u3059\u308b\u3068\u3001\u5404fold\u3067\u3069\u306eseed\u3067\u6700\u5c0f\u5024\u306b\u306a\u3063\u305f\u304b\u3068\u305d\u306e\u6700\u5c0f\u5024\u3092\u62bd\u51fa\u3067\u304d\u307e\u3059\u3002","a6a5fb6c":"# 0. Preparation","d90b59b7":"## 3.2 Roberta","945d6606":"---------------------------------------","7d247b44":"## **Please note that this result is for my model.**\n\n\n# Thank you for watching so far!\n\n# If it is helpful to you, I would appreciate it if you could upvote it.","bf46873c":"## \u30b9\u30b3\u30a2\u304c\u4e00\u756a\u826f\u304b\u3063\u305fRoBERTa\u306ek-fold\u30e2\u30c7\u30eb\u3092\u63a8\u8ad6\u3057\u3066\u63d0\u51fa\u3057\u307e\u3059\u3002fold4\u306f\u304a\u304b\u3057\u306a\u7d50\u679c\u304c\u51fa\u3066\u3044\u305f\u306e\u3067\u3001\u629c\u304d\u307e\u3057\u305f\u3002\n\n## ver15 : \u6587\u7ae0\u306e\u6539\u884c\u3092\u629c\u304f\u3068\u30b9\u30b3\u30a2\u304c\u4e0a\u304c\u308b\u3053\u3068\u304c\u308f\u304b\u3063\u305f\u306e\u3067\u3001\u629c\u3044\u3066\u307f\u307e\u3057\u305f(\u8a13\u7df4\u3067\u3082\u3084\u308c\u3070\u3088\u304b\u3063\u305f\u3067\u3059\u3002\u3002)","53acc65e":"## With this fold, RoBERTa looks better.\u3000\n#### \u203b\u3000Actually, there was the case that is difficult to judge as other folds.\n","ab9e0998":"## 11.1 Loading the all results ","d693519a":"# 13.Summary","86cc66bb":"## BERT\u3082RoBERTa\u3082\u660e\u3089\u304b\u306bk-fold\u306e\u52b9\u679c\u304c\u51fa\u3066\u3044\u308b\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u307e\u3057\u305f\u3002","4a3824a8":"## 3.1 BERT","0c699d72":"### It was confirmed that the result of fold 4 was clearly strange. The BERT result for fold3 also seems to be a little off.\n### From this, I found that my k-fold division was a little bad.","ada1a70b":"# 12. inference of best models with RoBERTa k-fold except fold4 which is strange about validation score and LB score.\n#### ver15 : I found that removing the \\n in the text would get better for the score, so I tried to remove it (I should have done it in training as well.)","0551dbcc":"<a id=\"section-one\"><\/a>\n# **11. Comparison results of BERT and RoBERTa**","2e3dff13":"### Evaluation Condition (\u8a55\u4fa1\u6761\u4ef6)\n* model : BERT or RoBERTa\n* fold : 5\n* epoch : 10\n* metric : valscore(validation score), Public LeadersBoard score(actually, I submit these models.Total 12 submits(Single 5 models with BERT and RoBERTa and 2 models by k-fold.) \n\nvalidation\u30b9\u30b3\u30a2\u3068\u3001\u5b9f\u969b\u306bsubmit\u3057\u3066Public LeadersBoard\u306e\u30b9\u30b3\u30a2\u3092\u6307\u6a19\u306b\u3057\u307e\u3057\u305f\uff08BERT\u3068RoBERTa\u306e\u5404fold\u306esingle model 5\u00d72\u3068\u3001\u305d\u308c\u305e\u308c\u306emodel\u306ek-fold 2\u3064\u3067\u8a0812\u500bsubmit\u3057\u307e\u3057\u305f\u3002)\n\n* random seed : 101,102,103,104,105\n* how to adopt the best model : The model created with randam seed, which had the smallest validation score in each fold. \u203b\u3000Bestscore means the smallest validation score in each fold.\n \n     \u4e00\u756a\u826f\u3044\u30e2\u30c7\u30eb\u306f\u3001\u5404fold\u3067\u6700\u3082validation score\u304c\u5c0f\u3055\u304f\u306a\u308brandam seed\u3092\u63a1\u7528\u3057\u307e\u3057\u305f\u3002\n     bestscore\u306e\u610f\u5473\u306f\u3001\u5404fold\u3067\u6700\u3082\u5c0f\u3055\u304b\u3063\u305fvalidation score\u3067\u3059\u3002\n","579c0261":"# 6. Training & validation function","f8b6159a":"## A comparison of validation scores showed that BERT was good for fold3, \n\n## but a comparison with public LB confirmed that RoBERTa was good for all.","f6549509":"# 10. RoBERTa Training","204e2ebb":"## 12.1 prediction function","79e83a9c":"## validation score\u306e\u6bd4\u8f03\u3060\u3068\u3001fold3\u3067BERT\u304c\u826f\u3044\u3068\u3044\u3046\u7d50\u679c\u304c\u51fa\u305f\u304c\u3001\n\n## public LB\u3067\u6bd4\u8f03\u3059\u308b\u3068\u3001\u3059\u3079\u3066\u306b\u304a\u3044\u3066\u3001RoBERTa\u304c\u826f\u304b\u3063\u305f\u3053\u3068\u304c\u78ba\u8a8d\u3055\u308c\u305f\u3002","3f9fe445":"## \u3053\u306e\u7d50\u679c\u306f\u79c1\u306e\u30e2\u30c7\u30eb\u306e\u5834\u5408\u306a\u306e\u3067\u3001\u3054\u6ce8\u610f\u304f\u3060\u3055\u3044\uff01\n\n## \u6700\u5f8c\u307e\u3067\u898b\u3066\u9802\u3044\u3066\u3042\u308a\u304c\u3068\u3046\u3054\u3056\u3044\u307e\u3059\uff01\n\n## \u3082\u3057\u3001\u5c11\u3057\u3067\u3082\u304a\u5f79\u306b\u7acb\u3066\u3070\u3001**upvote**\u3044\u305f\u3060\u3051\u305f\u3089\u5b09\u3057\u3044\u3067\u3059\uff01","cd5b9067":"#### BERT case","fffa67ed":"## It was confirmed that both BERT and RoBERTa clearly had the good effect of k-fold.","0b615f69":"## 11.4 The result of comparing Public Leaders board score","7da22e53":"## 11.5 Comparison of my validation score and Public LB score\n\n## validation score\u3068Public LB score\u306e\u6bd4\u8f03\u3092\u3057\u307e\u3059\u3002","513d00ea":"## You can see that the minimum validation score varies considerably for each random seed.","ea36a364":"#### I thought RoBERTa was better for all, but when I compared it with the validation score, I confirmed better results for BERT in fold3.\n#### So I then submitted all of these models as a single model and compared the Leaders Board scores.\n\n\n","20abc355":"# 1. CFG","b60582cb":"# 5. loss function","fd0ba962":"#### At first glance, RoBERTa may have a smaller validation score.\n#### And, with my model, it was confirmed that there was considerable variation every random seed and every epoch.\n\n","99aa7bed":"# 9. BERT training \n### \u3010Attention\u3011I've showed the training code for reference, but it takes 15 hours to run and it can't run on kaggle notebook. Therefore, I set the train_exe = False.If you want to use it, please arrange it. (Actually, I spent 3 hours for each fold. Refer to the past version.)\u3000Actual training was done in ver 8(fold 0), ver 9(fold 1), ver 7(fold 2), ver 10(fold 3), ver 4(fold 4). \n ","78d8cfa2":"## 11.6 Score when submitting the average of 5 folds inference\n## \u901a\u5e38\u306e5\u500b\u306ek-fold\u306e\u5e73\u5747\u3092submit\u3057\u305f\u3068\u304d\u306e\u30b9\u30b3\u30a2","49ac7bc7":"\u610f\u5473\u3092\u5c11\u3057\u89e3\u8aac\u3059\u308b\u3068\u3001fold 0\u3067\u306f\u3001SEED\u304c101\u306e\u3068\u304d\u30010.552\u3068\u6700\u3082validation score\u304c\u4f4e\u304f\u306a\u308a\u307e\u3057\u305f\u3002"}}