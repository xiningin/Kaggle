{"cell_type":{"10e58e81":"code","545639b3":"code","b2d7b5d5":"code","5b7232f7":"code","e06846b8":"code","716b9b28":"code","ae4dd0f8":"code","a93a72ac":"code","9af9427c":"code","aa69fd07":"code","1513a848":"code","8979ee8e":"code","4d52fd35":"code","901c648a":"code","4d5602da":"code","afc47039":"code","de9b4045":"code","d77a3ce1":"code","cd4de61b":"code","c2510c3b":"code","9f001c9d":"code","c4fb7395":"code","ecc1bcce":"code","92545b00":"code","2b121d75":"code","9fc9524f":"code","5b092223":"code","04b3382b":"code","661230b6":"code","4e8c80db":"markdown","1f2f6601":"markdown","b93652a9":"markdown","bb9acedc":"markdown","54b86699":"markdown","c3a1147b":"markdown","3fa05b5a":"markdown","ab4f0a00":"markdown","b507a785":"markdown","7bf2b24f":"markdown","ec3eed0f":"markdown","694fbc82":"markdown","f2fc725c":"markdown","27fda714":"markdown","a8be70f3":"markdown","0cdd6b23":"markdown","68ff6a6c":"markdown","d03bf8c0":"markdown","e623872b":"markdown","7ec5aca2":"markdown","b8a7ee08":"markdown","4edc81b2":"markdown","e703a4a4":"markdown","53fad86c":"markdown","bc03415d":"markdown","be1e1ee2":"markdown","10b82c8c":"markdown","d608775f":"markdown","e117e2f5":"markdown","c3ab629c":"markdown"},"source":{"10e58e81":"import numpy as np # linear algebra\nimport pandas as pd # data processing, \n\n# data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# data preprocessing\nimport sklearn\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Machine Learning Algorithms\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor \nfrom sklearn.neighbors import KNeighborsRegressor\n\nimport xgboost as xgb\nfrom xgboost.sklearn import XGBRegressor\n\n# utils\nimport os\nimport warnings\nimport pickle\nfrom math import sqrt\n\n\n# To ignore warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n# standard scaler object\nstdscaler = StandardScaler()\n\n# check the data files\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","545639b3":"# create Training Dataframe\ntrain_df = pd.read_csv(\"\/kaggle\/input\/black-friday\/train.csv\")","b2d7b5d5":"train_df.head()","5b7232f7":"train_df.info()","e06846b8":"train_df.describe(include='all')","716b9b28":"print(\"All columns -> {}\".format(list(train_df.columns)))\nprint()\nprint(\"==============================================\")\nprint(\"Total Transactions -> {}\".format(train_df.shape[0]))\n\nunique_users = len(train_df.User_ID.unique())\nprint(\"Total unique users -> {}\".format(unique_users))\n\nunique_products = len(train_df.Product_ID.unique())\nprint(\"Total unique products -> {}\".format(unique_products))","ae4dd0f8":"# Creating Count plots for Important categorical fields \nfig,axis = plt.subplots(nrows=2,ncols=3,figsize=(17,10))\n\nsns.countplot(train_df[\"Age\"],ax=axis[0,0])\nsns.countplot(train_df[\"Gender\"],ax=axis[0,1])\nsns.countplot(train_df[\"Occupation\"],ax=axis[0,2])\nsns.countplot(train_df[\"City_Category\"],ax=axis[1,1])\nsns.countplot(train_df[\"Stay_In_Current_City_Years\"],ax=axis[1,0])\nsns.countplot(train_df[\"Marital_Status\"],ax=axis[1,2])","a93a72ac":"# Creating Count plots for Important categorical fields \n\nfig,axis = plt.subplots(nrows=2,ncols=3,figsize=(17,10))\n\ntrain_df.groupby([\"Age\"])[\"Purchase\"].sum().plot(kind='pie',ax=axis[0,0])\ntrain_df.groupby([\"Gender\"])[\"Purchase\"].sum().plot(kind='bar',ax=axis[0,1])\ntrain_df.groupby([\"Occupation\"])[\"Purchase\"].sum().plot(kind='pie',ax=axis[0,2])\ntrain_df.groupby([\"City_Category\"])[\"Purchase\"].sum().plot(kind='bar',ax=axis[1,1])\ntrain_df.groupby([\"Stay_In_Current_City_Years\"])[\"Purchase\"].sum().plot(kind='pie',ax=axis[1,0])\ntrain_df.groupby([\"Marital_Status\"])[\"Purchase\"].sum().plot(kind='bar',ax=axis[1,2])","9af9427c":"fig,axis = plt.subplots(nrows=2,ncols=3,figsize=(17,10))\n\ntrain_df.groupby([\"Age\"])[\"Purchase\"].mean().plot(kind='bar',ax=axis[0,0])\ntrain_df.groupby([\"Gender\"])[\"Purchase\"].mean().plot(kind='bar',ax=axis[0,1])\ntrain_df.groupby([\"Occupation\"])[\"Purchase\"].mean().plot(kind='bar',ax=axis[0,2])\ntrain_df.groupby([\"City_Category\"])[\"Purchase\"].mean().plot(kind='bar',ax=axis[1,1])\ntrain_df.groupby([\"Stay_In_Current_City_Years\"])[\"Purchase\"].mean().plot(kind='bar',ax=axis[1,0])\ntrain_df.groupby([\"Marital_Status\"])[\"Purchase\"].mean().plot(kind='bar',ax=axis[1,2])","aa69fd07":"fig,axis = plt.subplots(nrows=2,ncols=2,figsize=(17,10))\n\ntrain_df.groupby([\"Age\",\"Gender\"])[[\"Purchase\"]].mean().unstack().plot(kind='bar',rot=0, ax = axis[0,0])\ntrain_df.groupby([\"Occupation\",\"Gender\"])[[\"Purchase\"]].mean().unstack().plot(kind='bar',rot=0, ax = axis[0,1])\ntrain_df.groupby([\"Marital_Status\",\"Gender\"])[[\"Purchase\"]].mean().unstack().plot(kind='bar',rot=0, ax = axis[1,0])\ntrain_df.groupby([\"Stay_In_Current_City_Years\",\"Gender\"])[[\"Purchase\"]].mean().unstack().plot(kind='bar',rot=0, ax = axis[1,1])","1513a848":"sns.pairplot(train_df,diag_kind=\"kde\",corner=True,\n             markers=\"+\",\n             plot_kws=dict(s=1, edgecolor=\"b\", linewidth=1),\n             diag_kws=dict(shade=True) )","8979ee8e":"(train_df.isna().sum()*100\/train_df.shape[0]).sort_values(ascending=False).to_frame().rename(columns={0:\"Percentage of missing values\"})","4d52fd35":"train_df.loc[train_df.Product_ID==\"P00265242\",[\"User_ID\",\"Product_ID\",\"Product_Category_1\" ,\"Product_Category_2\",\"Product_Category_3\"]]","901c648a":"fig,axis = plt.subplots(nrows=1,ncols=3,figsize=(20,8))\n\nsns.countplot(train_df[\"Product_Category_1\"],ax=axis[0])\nsns.countplot(train_df[\"Product_Category_2\"],ax=axis[1])\nsns.countplot(train_df[\"Product_Category_3\"],ax=axis[2])","4d5602da":"train_df[[\"Product_Category_2\",\"Product_Category_3\"]] = train_df[[\"Product_Category_2\",\"Product_Category_3\"]].fillna(0)","afc47039":"(train_df.isna().sum()*100\/train_df.shape[0]).sort_values(ascending=False).to_frame().rename(columns={0:\"Percentage of missing values\"})","de9b4045":"# Label encoder object\nle = LabelEncoder()\n\ntrain_df[\"Age\"] = le.fit_transform(train_df[\"Age\"])\ntrain_df[\"Stay_In_Current_City_Years\"] = le.fit_transform(train_df[\"Stay_In_Current_City_Years\"])\ntrain_df[\"City_Category\"] = le.fit_transform(train_df[\"City_Category\"])\n\n# dropped unnecessary fields\ntrain_dropped_df = train_df.drop(['User_ID', 'Product_ID'],axis=1)\nprint(\"Dropped the user and product id field\")\n\ntrain_dropped_df = pd.get_dummies(train_dropped_df)\n\nX = train_dropped_df.drop(columns= [\"Purchase\"])\n# separate dataframes one is for independant fields and another for dependant field (Target Field)\ny = train_dropped_df['Purchase'].values","d77a3ce1":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,9))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train_dropped_df.corr(),\n            vmin=-1,\n            vmax=1,\n            cmap='RdBu',\n            annot=True)","cd4de61b":"print(\"Input shape -> {}\".format(X.shape))\nprint(\"Output shape -> {}\".format(y.shape))","c2510c3b":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\nprint(\"Input train shape -> {}\".format(X_train.shape))\nprint(\"Input test shape -> {}\".format(X_test.shape))","9f001c9d":"def train_and_evaluate(model,X_train,y_train,X_test,y_test):\n    '''\n    This function is to fit the machine learning model and evaluate the R2 score for train and test data\n    \n    INPUT:\n    model - Machine Learning model\n    X_train - Training data \n    y_train - Training output values\n    X_test - Testing data\n    y_test - Testing output values\n    \n    OUTPUT:\n    model - Trained Machine Learning  model\n    '''\n    model.fit(X_train,y_train)\n    y_pred_train = model.predict(X_train)\n    y_pred_test = model.predict(X_test)\n    \n    # Score method gives the R2 score actually, so we can directly check the R2 score\n    print(\"Train R-2 Score -> {}\".format(r2_score(y_train, y_pred_train)))\n    print(\"Test R-2 Score -> {}\".format(r2_score(y_test,y_pred_test)))\n    print()\n    print(\"=============================================\")\n    print()\n    print(\"Train RMSE  -> {}\".format(sqrt(mean_squared_error(y_train, y_pred_train))))\n    print(\"Test RMSE  -> {}\".format(sqrt(mean_squared_error(y_test,y_pred_test))))\n    return model","c4fb7395":"lr = LinearRegression(n_jobs=-1)\ntrain_and_evaluate(lr,X_train,y_train,X_test,y_test)","ecc1bcce":"dtr = DecisionTreeRegressor(max_depth=8,\n                            min_samples_split=5,\n                           max_leaf_nodes=10,\n                            min_samples_leaf=2,\n                            random_state=142)\ntrain_and_evaluate(dtr,X_train,y_train,X_test,y_test)","92545b00":"rf = RandomForestRegressor(max_depth=8,\n    min_samples_split=4,\n    min_samples_leaf=2,\n    random_state=142)\ntrain_and_evaluate(rf,X_train,y_train,X_test,y_test)","2b121d75":"knnreg = KNeighborsRegressor(n_neighbors = 6)\ntrain_and_evaluate(knnreg,X_train,y_train,X_test,y_test)","9fc9524f":"# Various hyper-parameters to tune\nxgbr = XGBRegressor()\nparameters = {\n              'objective':['reg:squarederror'],\n              'learning_rate': [.03, 0.05, .07], #so called `eta` value\n              'max_depth': [5, 6, 7],\n              'min_child_weight': [4],\n              'subsample': [0.7],\n              'colsample_bytree': [0.7],\n              'n_estimators': [500]}\n\nxgb_grid = GridSearchCV(xgbr,\n                        parameters,\n                        cv = 5,\n                        n_jobs = -1,\n                        verbose=True)\n\nfitted_xgb = train_and_evaluate(xgb_grid,X_train,y_train,X_test,y_test)","5b092223":"print(fitted_xgb.best_score_)\nprint(fitted_xgb.best_params_)","04b3382b":"!mkdir \/kaggle\/model","661230b6":"# Save the model to file in the current working directory\n\nPkl_Filename = \"\/kaggle\/model\/BlackFriday_XGB_Model_V3.pkl\"  \n\nwith open(Pkl_Filename, 'wb') as file:  \n    pickle.dump(fitted_xgb, file)","4e8c80db":"## Thanks","1f2f6601":"this is what I understood from the data , so to fill the missing values we just need to fill those as zero.","b93652a9":"## 3. Prepare Data\n#### Handle Missing Values ","bb9acedc":"## =====================================================================\n\n#### Let's see Average Purchased Amount by Age, Gender, Occupation, City , Years Stay In current City  and marital status\n\n##### Questions :- \n1. Does average values have different story to tell than with the total values?","54b86699":"#### Let's check again ","c3a1147b":"## 2. Data Understanding\n\nNow we have the dataframe, so we can start doing **EDA (Exploratory Data Analysis)** , **DA (Descriptive Analysis)**.\n\nLet's have a look at first 5 rows","3fa05b5a":"So, For particular product ID you will have a same values for all the product categories","ab4f0a00":"#### 4. KNN regressor ","b507a785":"# Black Friday\n\n <img src=\"images\/blackfridaycanceled.jpg\" \/>\n\n### What is Black Friday?\nBlack Friday is the day after Thanksgiving. Retailers typically offer steep discounts on Black Friday to kick off the holiday season.Black friday is the one of the most profitable days for all the retailers and for the buyers too. It\u2019s common for retailers to offer special promotions and open their doors during the pre-dawn hours on Black Friday to attract customers.\n\nTo keep up with the competition, retailers need some insights and cluster of customers to segmantize and to target them. So, retailers can use different marketing strategies to attract different types of customers.\n\nData scientist can help retailers with proper insights based on the historical data of black friday and also give proper data driven solutions by using Data science methods and by following the **CRISP-DM Process (Cross Industry Process for Data Mining)**. Thus, they can achieve thier target\/goal.This project is all about this.\n\nIn this Notebook, we are going to follow stages,\n\n1. Business Understanding\n2. Data Understanding\n3. Prepare Data\n4. Data Modeling\n5. Evaluate the Results\n6. Deploy\n\n## 1. Business Understanding\nAs mentioned above, All the retailers' goal is to attract more and more customers for black friday deals. This is like a competition for them to get the customers to thiers online platforms or physical retail stores. So let's start exploring the dataset.\n\nFirst We will import libraries and load the data","7bf2b24f":"####  Train Different Models and evaluate the results","ec3eed0f":"#### From Above Figures we can conclude that,\n\n##### Conclusions :- \n* In figure 1, 26-35 age range have done more transactions.\n* In figure 2, Male users have done more transactions.\n* In figure 3, Users whose occupations are 0 and 4 have done more transactions.\n* In figure 4, Users who stayed in current city for 1 year have done more transactions.\n* In figure 5, Users from city category B have done more transactions.\n* In figure 6, Users who are not married yet have done more transactions.","694fbc82":"#### Data Shape and unique values of  users, products ","f2fc725c":"#### From Above Figures we can conclude that,\n\n##### Conclusions :- \n* In figure 1, 26-35 age range have spent more money.\n* In figure 2, Male users have spent more money.\n* In figure 3, Users whose occupations are 0 and 4 have spent more money.\n* In figure 4, Users who stayed in current city for 1 year have spent more money.\n* In figure 5, Users from city category B have spent more money.\n* In figure 6, Users who are not married yet have spent more money.\n\n#### Btw the insights look same for both purchase total Count and Amount. So let's chceck out the average Amount","27fda714":"## 4. Data Modeling  and 5. Evaluate the Results\n#### Split the Data into train and test data set by 70%-30% ","a8be70f3":"#### 5. XGBoost Regressor","0cdd6b23":"## =====================================================================\n\n#### Total Transactions by Age, Gender, Occupation, City and Years Stay In current City  \n\n##### Questions :- \n1. Which age group have done more transactions?\n2. Have males done more transactions  than females\u00a0?\n3. Do users' occupation have any relation to number of transactions?\n4. Do users who are living in the city more than 1 year have done more transactions?\n5. Does category of city have any impact on purchase quantity?\u00a0\n6. Does marital status is related to Black Friday purchase?","68ff6a6c":"### Product Categories\n\nThe product categories are categories of that particular product, if product belongs to more than one category , then there will be some values available for other product categories. So here we are dealing with very strange categorical feature.\n\nlet me show you one example of particular product_ID = \"**P00265242**\" ,","d03bf8c0":"### Data Transformation\n\n#### Now we don't have any Missing values, So we can transform our data and feed into the Machine learning model for training.\n* For Transformation :- \n    1. **Categorical Fields** :- We will use the label encoder to labelize all the categories.\n    2. **Numeric Fields** :- We can standardize the values based on standard scaler , and transform the data into  mean = 0, std = 1, Btw it seems actually there is no need , as all the fields are somewhat categorical fields only.\n* Get two dataframes containing only Independent fields and only dependent field into new variables","e623872b":"## 6. Deploy \nIn this notebook the meaning of deployment is to save the best model in pickle format and then using that saved pickle we can load this model in to the API service which will be deployed in the cloud plateform. This API then will be used to predict the purchase amount of individual customer for black friday.","7ec5aca2":"## =====================================================================\n\n####  Let's see the total Purchased Amount by gender by Age, Occupation, Years Stay In current City  and marital status","b8a7ee08":"#### What is the correlation between all the fields ?","4edc81b2":"#### 3. Random Forest Regressor","e703a4a4":"After seeing above figures, we can conclude that, according to the data , for all the categories of the categorical fields, the average purchased amounts are almost same.","53fad86c":"* Above Pairplot is not giving more good insights about the correlation between numeric values.\n* Also marital status and occupation fields are also being considered as those are in Numeric form.We need to change those fields to categorical type.","bc03415d":"## =====================================================================\n\n#### Total Purchasing Amount Paid group by Age, Gender, Occupation, City , Years Stay In current City  and marital status \n\n##### Questions :- \n1. Which age group have spent more in black friday?\n2. Have females spent more than males ?\n3. Do users' occupation have any relation on spending money at black friday?\n4. Do users who are living in the city more than 1 year have spent more money?\n5. Does category of city have any imapact on total purchase amount? \n6. Does marital status is related to the black friday?","be1e1ee2":"#### Descriptive Analysis of all the fields' which includes count of non Null values, data types , mean, median, min, max , etc.","10b82c8c":"## =====================================================================\n\n#### Let's see Pairplot with all Numeric columns","d608775f":"#### 2. Decision Tree Regressor","e117e2f5":"#### 1. Linear Regression","c3ab629c":"## Conclusion\n\nWe went through each and every stages of **CRISP_DM**, did different types of analysis on the data, visualized the data with different types of charts and trained the model with different types of machine learning models to predict the purchase amount that user might spend on next black friday. \n\nSo, now the question is what retailers can get from all of this?\nRetailers now have analysis reports and a trained model, using those they can create some different clusters of customers and give the different types of offers. Different types marketing strategis should be applied for each and every clusters after that they can do outstanding in the black friday 'competition'."}}