{"cell_type":{"023c9e4a":"code","96f83be5":"code","fcd62550":"code","1142c87c":"code","aa84da64":"code","61403fd1":"code","91a6659b":"code","8288b522":"code","b8b93186":"code","3bbb3d2d":"code","d6a2a917":"code","3f91da44":"code","2874390d":"code","087bf986":"code","ee49330b":"code","f724c541":"code","8a5f519e":"code","29fe7363":"code","55608eef":"code","2597e370":"code","e22d90e9":"code","4931d5e2":"code","062b25d9":"code","8cde1a84":"code","e01126f7":"code","d21fa7b0":"code","469a4b36":"code","22d5c733":"code","5a7e3651":"code","0f253ebf":"code","cd85efc6":"code","f3fcc815":"code","917ff6f8":"code","bfccf5d0":"code","5f7c2630":"code","fb12981c":"code","e3a7156a":"code","66efcf7a":"code","b7ced833":"code","f085adba":"code","cf755ad4":"code","7ba05188":"code","c7f7d153":"code","0eaed96a":"code","cee336b6":"code","04a23b88":"code","ce9053ed":"code","e8797444":"code","d58923ff":"code","03e3726a":"code","d1e9a15f":"code","a98edeb1":"code","9298034d":"code","5593560c":"code","10f9f1f4":"code","093e11ed":"code","85ab50fa":"code","96f9a13c":"markdown","62ee709d":"markdown","56097dd7":"markdown","ee32852b":"markdown","62685a1e":"markdown","1c09e990":"markdown"},"source":{"023c9e4a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","96f83be5":"# Import all the required libraries for data analysis and model building\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport datetime\nfrom sklearn.preprocessing import LabelEncoder\n\npd.set_option('display.max_columns', 0)\npd.set_option('display.max_rows', 500)\n\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score, classification_report","fcd62550":"# Import the dataset\ndf = pd.read_csv('..\/input\/loan.csv')","1142c87c":"# Check dimension\ndf.shape","aa84da64":"# Take a random sample of 40% data from the original dataset (due to pc memory problem)\ndf= df.sample(frac=0.4, random_state= 1)","61403fd1":"# Check new dimension after sampling\ndf.shape","91a6659b":"# Have a look at the data type\ndf.info()","8288b522":"# Check missing values count and percent\ntotal= df.isnull().sum().sort_values(ascending=False)\npercent= (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)*100\nmissing_data= pd.concat([total, percent],axis=1, keys=[\"Total\", \"Percent\"])\nmissing_data.head(25)","b8b93186":"# Plot missing values\nplt.figure(figsize=(15,5))\nmissing= df.isnull().sum()\nmissing= missing[missing > 0]\nmissing.sort_values(ascending= False, inplace=True)\nplt.xlabel(\"Bar Plot of Missing Values\", fontsize=15)\nplt.ylabel(\"Number of missing values\", fontsize=15)\nplt.title(\"Number of missing data by feature\", fontsize=15)\nmissing.plot(kind=\"bar\")\n\nplt.show()","3bbb3d2d":"# Any variable having missing values more than 50 percent are removed\ndf.drop([\"dti_joint\", \"verification_status_joint\", \"annual_inc_joint\", \"il_util\", \"mths_since_rcnt_il\",\n            \"all_util\", \"max_bal_bc\", \"open_rv_24m\", \"open_rv_12m\", \"total_cu_tl\", \"total_bal_il\", \"open_il_24m\",\n            \"open_il_12m\", \"total_cu_tl\", \"total_bal_il\", \"open_il_24m\", \"open_il_12m\", \"open_il_6m\", \"open_acc_6m\",\n            \"inq_fi\", \"inq_last_12m\", \"desc\", \"mths_since_last_record\", \"mths_since_last_major_derog\",\n            \"mths_since_last_delinq\", \"next_pymnt_d\", \"total_rev_hi_lim\", \"tot_cur_bal\", \"tot_coll_amt\"], axis=1,\n           inplace=True)\n\n# Delete unwanted columns\ndf.drop([\"id\", \"url\", \"member_id\"], axis=1, inplace=True)\n\n# Payment plan has all the values \"n\" and only 3 values \"y\" so it is not important\ndf.drop([\"pymnt_plan\"], axis=1, inplace=True)\n\n# Since we have both address state and zip code let's drop zip code and use address state only\ndf.drop([\"zip_code\"], axis=1, inplace=True)\n\n# Title is not important instead we will use \"purpose\" variable\ndf.drop([\"title\"], axis=1, inplace=True)\n\n# The grade is implied by the subgrade, so let's drop the grade column.\ndf.drop([\"grade\"], axis=1, inplace=True)","d6a2a917":"# remove \"months\" from \"36 months\" and convert it to int type\ndf[\"term\"]= df['term'].map(lambda x: x.rstrip('months'))\ndf[\"term\"]= df[\"term\"].astype(\"int\")","3f91da44":"# Again Check missing values count and percent in remaining columns\ntotal= df.isnull().sum().sort_values(ascending=False)\npercent= (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)*100\nmissing_data= pd.concat([total, percent],axis=1, keys=[\"Total\", \"Percent\"])\nmissing_data.head(15)","2874390d":"# Loan Status\nplt.figure(figsize = (12,8))\ng = sns.countplot(x=\"loan_status\",data=df,\n                  palette='hls')\ng.set_xticklabels(g.get_xticklabels(),rotation=45)\ng.set_title(\"Loan Status\", fontsize=20)\ng.set_xlabel(\"Loan Status\", fontsize=15)\ng.set_ylabel(\"Loan Amount\", fontsize=20)","087bf986":"# Lets fill mode value in place of the missing value\ndf[\"emp_title\"].value_counts(ascending=False).head()\ndf[\"emp_title\"]= df[\"emp_title\"].fillna(df[\"emp_title\"].mode()[0])","ee49330b":"# Lets fill mode value in place of the missing value\ndf[\"emp_length\"].value_counts(ascending=False).head()\ndf[\"emp_length\"]= df[\"emp_length\"].fillna(df[\"emp_length\"].mode()[0])","f724c541":"# Convert to date time\ndf[\"last_pymnt_d\"]= pd.to_datetime(df[\"last_pymnt_d\"])\ndf[\"last_pymnt_d\"].value_counts(ascending=False).head()\n# Lets fill mode value in place of the missing value\ndf[\"last_pymnt_d\"]= df[\"last_pymnt_d\"].fillna(df[\"last_pymnt_d\"].mode()[0])","8a5f519e":"# Let's fill the Median in place of the missing values\ndf[\"revol_util\"].value_counts(ascending=False).head()\ndf[\"revol_util\"]= df[\"revol_util\"].fillna(df[\"revol_util\"].median())","29fe7363":"# Lets fill mode value in place of the missing value\ndf[\"purpose\"].value_counts(ascending=False).head()\ndf[\"purpose\"]= df[\"purpose\"].fillna(df[\"purpose\"].mode()[0])","55608eef":"# Lets fill mode value in place of the missing value\ndf[\"collections_12_mths_ex_med\"].value_counts()\ndf[\"collections_12_mths_ex_med\"]= df[\"collections_12_mths_ex_med\"].fillna(df[\"collections_12_mths_ex_med\"].mode()[0])","2597e370":"# Convert to date time\ndf[\"last_credit_pull_d\"]= pd.to_datetime(df[\"last_credit_pull_d\"])\ndf[\"last_credit_pull_d\"].value_counts(ascending=False).head()\n\n# Lets fill mode value in place of the missing value\ndf[\"last_credit_pull_d\"]= df[\"last_credit_pull_d\"].fillna(df[\"last_credit_pull_d\"].mode()[0])","e22d90e9":"# Lets fill mode value in place of the missing value\ndf[\"acc_now_delinq\"].value_counts(ascending=False)\ndf[\"acc_now_delinq\"]= df[\"acc_now_delinq\"].fillna(df[\"acc_now_delinq\"].mode()[0])","4931d5e2":"# Let's fill the Median in place of the missing values\ndf[\"total_acc\"].value_counts(ascending=False)\ndf[\"total_acc\"]= df[\"total_acc\"].fillna(df[\"total_acc\"].median())","062b25d9":"# Let's fill the Median in place of the missing values\ndf[\"open_acc\"].value_counts(ascending=False)\ndf[\"open_acc\"]= df[\"open_acc\"].fillna(df[\"open_acc\"].median())","8cde1a84":"# Convert to date time\ndf[\"earliest_cr_line\"]= pd.to_datetime(df[\"earliest_cr_line\"])\ndf[\"earliest_cr_line\"].value_counts(ascending=False).head()\n\n# Lets fill mode value in place of the missing value\ndf[\"earliest_cr_line\"]= df[\"earliest_cr_line\"].fillna(df[\"earliest_cr_line\"].mode()[0])","e01126f7":"# Lets fill mode value in place of the missing value\ndf[\"inq_last_6mths\"].value_counts(ascending=False)\ndf[\"inq_last_6mths\"]= df[\"inq_last_6mths\"].fillna(df[\"inq_last_6mths\"].mode()[0])","d21fa7b0":"# Lets fill mode value in place of the missing value\ndf[\"pub_rec\"].value_counts(ascending=False)\ndf[\"pub_rec\"]= df[\"pub_rec\"].fillna(df[\"pub_rec\"].mode()[0])","469a4b36":"# Lets fill mode value in place of the missing value\ndf[\"delinq_2yrs\"].value_counts(ascending=False)\ndf[\"delinq_2yrs\"]= df[\"delinq_2yrs\"].fillna(df[\"delinq_2yrs\"].mode()[0])","22d5c733":"# Let's fill the Median in place of the missing values\ndf[\"annual_inc\"].value_counts(ascending=False)\ndf[\"annual_inc\"]= df[\"annual_inc\"].fillna(df[\"annual_inc\"].median())","5a7e3651":"# Extract only year from the variable\ndf[\"earliest_cr_line\"] = pd.DatetimeIndex(df[\"earliest_cr_line\"]).month\ndf[\"last_pymnt_d\"] = pd.DatetimeIndex(df[\"last_pymnt_d\"]).month\ndf[\"last_credit_pull_d\"] = pd.DatetimeIndex(df[\"last_credit_pull_d\"]).month","0f253ebf":"df[\"loan_status\"].value_counts()","cd85efc6":"# Create a target variable \"Loan Status\" with two categories Good and Bad Loan\ndf[\"Loan_Class\"] = np.where((df.loan_status == 'Current') |\n                        (df.loan_status == 'Fully Paid') |\n                        (df.loan_status== \"Issued\") |\n                        (df.loan_status == 'Does not meet the credit policy. Status:Fully Paid'), 1, 0)","f3fcc815":"# Bar plot of Term (Loan taken for number of months)\nplt.figure(figsize=(10,6))\nsns.barplot(\"term\", \"loan_amnt\", data=df, palette='spring')\nplt.title(\"Term of Loan\", fontsize=16)\nplt.xlabel(\"Months\", fontsize=14)\nplt.ylabel(\"Loan Amount\", fontsize=14)","917ff6f8":"# Frequency distribution of Loan Amount\nplt.figure(figsize=(12,6))\ng = sns.distplot(df[\"loan_amnt\"])\ng.set_xlabel(\"Loan Amount\", fontsize=12)\ng.set_ylabel(\"Frequency\", fontsize=12)\ng.set_title(\"Frequency Distribuition- Loan Amount\", fontsize=20)","bfccf5d0":"# Frequency distribution of Interest Rate\nplt.figure(figsize=(12,6))\ng = sns.distplot(df[\"int_rate\"])\ng.set_xlabel(\"Interest Rate\", fontsize=12)\ng.set_ylabel(\"Frequency\", fontsize=12)\ng.set_title(\"Int Rate Distribuition\", fontsize=20)","5f7c2630":"# Loan Status vs Loan Amount\nplt.figure(figsize = (12,8))\ng = sns.countplot(x=\"loan_status\",data=df,\n                  palette='hls')\ng.set_xticklabels(g.get_xticklabels(),rotation=45)\ng.set_title(\"Loan Status\", fontsize=20)\ng.set_xlabel(\"Loan Status\", fontsize=15)\ng.set_ylabel(\"Loan Amount\", fontsize=20)","fb12981c":"# Application Type and Loan Amount\nplt.figure(figsize = (12,8))\ng = sns.countplot(x=\"purpose\",data=df,\n                  palette='hls')\ng.set_xticklabels(g.get_xticklabels(),rotation=45)\ng.set_title(\"Application Type\", fontsize=20)\ng.set_xlabel(\"Application Type\", fontsize=15)\ng.set_ylabel(\"Loan Amount\", fontsize=15)","e3a7156a":"# Boxplot of Employment Length and Inter\nplt.figure(figsize = (12,8))\nax = sns.boxplot(x=\"emp_length\" ,y= \"int_rate\", data=df, linewidth=2.5)\n\nplt.show()","66efcf7a":"# Employment Length and Number of Loans\ndf['emp_length'].value_counts().sort_values().plot(kind='barh',figsize=(18,8))\nplt.title('Number of loans distributed by Employment Years',fontsize=20)\nplt.xlabel('Number of loans',fontsize=15)\nplt.ylabel('Years worked',fontsize=15);","b7ced833":"# No of Defaulted Loans per State\nfig = plt.figure(figsize=(18,10))\ndf[df['Loan_Class']==1].groupby('addr_state')['Loan_Class'].count().sort_values().plot(kind='barh')\nplt.ylabel('State',fontsize=15)\nplt.xlabel('Number Of Loans',fontsize=15)\nplt.title('Number Of Defaulted Loans Per State',fontsize=20);","f085adba":"# Boxplot of Term and Loan Amount\nprint(\"Loan Amount Distribution BoxPlot\")\nplt.figure(figsize=(8,5))\nsns.boxplot(x=df.term, y=df.loan_amnt)","cf755ad4":"# Boxplot of Verification Status and Loan Amount\nplt.figure(figsize=(10,8))\nsns.boxplot(x=df.verification_status, y=df.loan_amnt)\nplt.xlabel(\"Verification Status\")\nplt.ylabel(\"Loan Amount\")","7ba05188":"# Crosstabulation of Purpose and Loan Status\npurp_loan= ['purpose', 'loan_status']\ncm = sns.light_palette(\"green\", as_cmap=True)\npd.crosstab(df[purp_loan[0]], df[purp_loan[1]]).style.background_gradient(cmap = cm)","c7f7d153":"# Convert into One Hot Encoding\n# categorical_features.columns\n\ncols= ['sub_grade', 'emp_title', 'emp_length', 'home_ownership',\n       'verification_status', 'issue_d', 'loan_status', 'purpose', 'addr_state',\n       'initial_list_status', 'application_type']\n\nfor i in cols:\n    lbl= LabelEncoder()\n    lbl.fit(list(df[i].values))\n    df[i]= lbl.transform(list(df[i].values))","0eaed96a":"# Check correlation between Loan_Class and all other independent variables\ncorrelation_m = df.corr()\ncorrelation_m[\"Loan_Class\"].sort_values(ascending=False)","cee336b6":"# Countplot of Good Loans and Bad Loans\ng= sns.countplot(df[\"Loan_Class\"])\ng.set_xticklabels(g.get_xticklabels(),rotation=45)\ng.set_title(\"Good Loan and Bad Loan\", fontsize=20)\ng.set_xlabel(\"Loan Type\", fontsize=15)\ng.set_ylabel(\"Frequency\", fontsize=15)","04a23b88":"# split the data into training and testing\nfrom sklearn.model_selection import train_test_split\nX = df.ix[:, df.columns != \"Loan_Class\"]\ny = df[\"Loan_Class\"]\n\nX_train, X_test, y_train, y_test = train_test_split (X, y, test_size=0.2, random_state=44)","ce9053ed":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test=sc.transform(X_test)","e8797444":"# Logistic Regression\nlog= LogisticRegression()\nlog.fit(X_train, y_train)\n\ny_pred= log.predict(X_test)\n\n# Summary of the prediction\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n\n# Accuracy\nprint(\"Accuracy of the model is: \", accuracy_score(y_pred,y_test))","d58923ff":"# Naive Bayes\nnaive= GaussianNB()\nnaive.fit(X_train, y_train)\n\ny_pred= naive.predict(X_test)\n\n# Summary of prediction\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n\n# Accuracy score\nprint(\"Accuracy of the model is: \", accuracy_score(y_pred,y_test))","03e3726a":"# from imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\n# Over Sampling\nros = RandomOverSampler(random_state=0)\nX_train_res, y_train_res = ros.fit_sample(X_train, y_train.ravel())\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))","d1e9a15f":"# Naive Bayes\nnaive= GaussianNB()\nnaive.fit(X_train, y_train)\n\ny_pred= naive.predict(X_test)\n\n# Summary of prediction\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n\n# Accuracy score\nprint(\"Accuracy of the model is: \", accuracy_score(y_pred,y_test))","a98edeb1":"# Logistic Regression\nlog= LogisticRegression()\nlog.fit(X_train, y_train)\n\ny_pred= log.predict(X_test)\n\n# Summary of the prediction\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n\n# Accuracy\nprint(\"Accuracy of the model is: \", accuracy_score(y_pred,y_test))","9298034d":"tmp = log.fit(X_train_res, y_train_res.ravel())\ny_pred_sample_score = tmp.decision_function(X_test)\n\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_sample_score)\n\nroc_auc = auc(fpr,tpr)\n\n# Plot ROC\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.0])\nplt.ylim([-0.1,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n","5593560c":"# from imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\n# Under Sampling\nrus = RandomUnderSampler(random_state=0)\nX_train_res, y_train_res = rus.fit_sample(X_train, y_train.ravel())\n\nprint(\"After UnderSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\nprint(\"After UnderSampling, counts of label '0': {}\".format(sum(y_train_res==0)))","10f9f1f4":"# Naive Bayes\nnaive= GaussianNB()\nnaive.fit(X_train, y_train)\n\ny_pred= naive.predict(X_test)\n\n# Summary of prediction\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n\n# Accuracy score\nprint(\"Accuracy of the model is: \", accuracy_score(y_pred,y_test))","093e11ed":"# Logistic Regression\nlog= LogisticRegression()\nlog.fit(X_train, y_train)\n\ny_pred= log.predict(X_test)\n\n# Summary of the prediction\nprint(classification_report(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n\n# Accuracy\nprint(\"Accuracy of the model is: \", accuracy_score(y_pred,y_test))","85ab50fa":"tmp = log.fit(X_train_res, y_train_res.ravel())\ny_pred_sample_score = tmp.decision_function(X_test)\n\n\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_sample_score)\n\nroc_auc = auc(fpr,tpr)\n\n# Plot ROC\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b',label='AUC = %0.3f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.1,1.0])\nplt.ylim([-0.1,1.01])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n","96f9a13c":"# SMOTE : OverSampling","62ee709d":"# Apply Algorithms\n\n- Let's apply different alogrithm to our dataset and build a machine learning model\n- Here we are applying alogrithms Logisitc Regression, Naive Bayes, SVM, KNN and Decision Tree\n\n# Conclusion\n\n- We can clearly see that due to dominace of one class that is Good Loans all are alogrithms are predicting good loans\n  with high accuracy but Bad Loans with a very low accuracy\n- There is class imbalance problem\n- We have to use SMOTE package and try over sampling and undersampling to see if there is any improvement","56097dd7":"# Correlation Matrix\n\n- Before that we need to convert our categorical variables level into numbers\n- For that we will do One Hot Encoding","ee32852b":"# SMOTE: UnderSampling","62685a1e":"# Create Target Variable\n\n- Loan_Class is our Target variable with two categories Good Loan and Bad Loan\n- For Good Loan we are considering categories from loan_status 1- Current 2- Fully Paid 3- Issued and 4- Does not meet\n  the credit policy. Status:Fully Paid\n- Since Current loan could be a Good Loan or a Bad Loan in future which as for now we are considering to be a Good Loan\n\n- Our **Target Varible** is **Loan_Class** where **1 = Good Loan** and **0 = Bad Loan**","1c09e990":"# Impute Missing Values"}}