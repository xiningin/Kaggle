{"cell_type":{"b0e2eda2":"code","945bc487":"code","37b17f69":"code","8a625740":"code","ff3d37b0":"code","c39b1ada":"code","d3896c1c":"code","0990ffb8":"code","b4911079":"code","32d87b6f":"code","b53a2ebe":"code","1c70b2e0":"code","056b211d":"code","2091f9e6":"code","ef063507":"code","6d3e3913":"code","413a6c21":"code","27a866bb":"code","61b8dacb":"code","4e68e9d1":"code","188c5163":"code","fe33d6e0":"code","faf11bea":"code","a010d3e0":"code","9e2edc03":"code","d3ea2e7e":"code","8c77351d":"code","a179cd37":"code","c6a93591":"code","a06ae8ca":"code","922fb62c":"code","3407e5c2":"code","e6f1df38":"code","64f362da":"code","4ac7e08d":"code","f8d56ece":"code","c3c85d91":"code","c8594169":"code","89da12c0":"code","09a53354":"code","faf11b79":"code","645623bd":"code","22ac28b4":"code","4739a06b":"code","02b54be6":"code","37d1679f":"code","0dbf5e74":"code","af81abfd":"code","3c916e3f":"code","7a75daa3":"code","245c174b":"code","c0debc87":"code","4386ed5a":"code","a26a8b6f":"code","59511dc4":"code","bd154e2f":"code","fe56a3c5":"code","4cebc243":"code","51b915f7":"code","61a1cfe2":"code","8eb69503":"code","af4247d5":"markdown","4619181d":"markdown","09ea0b49":"markdown","3b7b76f6":"markdown","efdaa39d":"markdown","0479c634":"markdown","e32a4d2e":"markdown","2d9fbc20":"markdown","8d84c6d5":"markdown","b761aec9":"markdown","51dafc65":"markdown","cc9b3a21":"markdown","506b6f6d":"markdown","4cbec82d":"markdown","4cdb9939":"markdown","250dff18":"markdown","46f8b566":"markdown","bd2b345d":"markdown","2c5d4701":"markdown","d5ade165":"markdown","ed691757":"markdown","463c3f58":"markdown","080e5618":"markdown","250e093f":"markdown","0661be7a":"markdown","cf3990a1":"markdown","1f60d394":"markdown","4739d838":"markdown","28db3398":"markdown","8f506f80":"markdown","744773e6":"markdown","ce8c8c5f":"markdown","d1778184":"markdown","d6659adf":"markdown","400adb74":"markdown","2c8edeb8":"markdown","ae6f8e80":"markdown","5a86b200":"markdown","5d0ff30e":"markdown","eddfe391":"markdown","34d4f78b":"markdown","e110b5b7":"markdown","659e2e73":"markdown","4e5afdc0":"markdown","39216273":"markdown","fb603e91":"markdown","14dfdc0d":"markdown","ada1416c":"markdown","accd3f60":"markdown","72e541d8":"markdown","be6545fd":"markdown","1107643f":"markdown","35a095c0":"markdown","e96468a5":"markdown","ded91858":"markdown","5ec8e349":"markdown"},"source":{"b0e2eda2":"!pip install pyspark","945bc487":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nimport pyspark # only run after findspark.init()\nfrom pyspark.sql import SparkSession\n# May take awhile locally\nspark = SparkSession.builder.appName(\"Pyspark_1\").getOrCreate()\n\ncores = spark._jsc.sc().getExecutorMemoryStatus().keySet().size()\nprint(\"You are working with\", cores, \"core(s)\")\nspark","37b17f69":"values = [('Pasta',100),('Pizza',200),('Noodles',50),('Burger',199),('Dosa',10000),('Bread',1)]\ndf = spark.createDataFrame(values,['food','my_hunger_index'])\ndf.show()","8a625740":"path =\"\"\n\n# Some csv data\nwine_quality = spark.read.csv(path+'\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv',\n                          inferSchema=True,header=True)","ff3d37b0":"wine_quality","c39b1ada":"wine_quality.show()","d3896c1c":"wine_quality.limit(5).show()","0990ffb8":"wine_quality.limit(5).toPandas()","b4911079":"print(type(wine_quality))\n\nwine_quality_pandas_df = wine_quality.toPandas()\nprint(type(wine_quality_pandas_df))","32d87b6f":"wine_quality","b53a2ebe":"wine_quality.printSchema()\n","1c70b2e0":"print(wine_quality.columns)","056b211d":"wine_quality.schema['fixed acidity'].dataType","2091f9e6":"wine_quality.describe().toPandas()","ef063507":"wine_quality_pandas_df.describe()","6d3e3913":"wine_quality.summary().toPandas()","413a6c21":"wine_quality.describe(['fixed acidity']).toPandas()","27a866bb":"wine_quality.select(\"fixed acidity\").summary().toPandas()","61b8dacb":"wine_quality.select(\"fixed acidity\",\"chlorides\").summary().toPandas()","4e68e9d1":"from pyspark.sql.types import StructField,StringType,IntegerType,StructType,DateType, DoubleType","188c5163":"data_schema = [StructField(\"fixed acidity\", DoubleType(), True),\n               StructField(\"citric acid\", DoubleType(), True),\n               StructField(\"chlorides\", DoubleType(), True),\n               StructField(\"pH\", DoubleType(), True),\n               StructField(\"sulphates\", DoubleType(), True),\n               StructField(\"density\", DoubleType(), True)]","fe33d6e0":"final_struc = StructType(fields=data_schema)","faf11bea":"wine_quality_partial = spark.read.csv(path+'\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv',\n                              schema=final_struc)","a010d3e0":"wine_quality_partial.printSchema()","9e2edc03":"wine_quality.write.mode(\"overwrite\").csv('wine_quality.csv')","d3ea2e7e":"wine_quality.toPandas().to_csv('wine_quality_pandas.csv')","8c77351d":"wine_quality.write.mode(\"overwrite\").partitionBy(\"quality\").csv('partitioned_by_quality_csv\/')","a179cd37":"from pyspark.sql.functions import *","c6a93591":"wine_quality.select(['fixed acidity','sulphates','pH']).show(5)","a06ae8ca":"wine_quality.select(['fixed acidity','sulphates','pH', 'quality']).orderBy(\"quality\").show(5)","922fb62c":"wine_quality.select(['fixed acidity','sulphates','pH', 'quality']).orderBy(wine_quality[\"quality\"].desc()).show(5)","3407e5c2":"#isin\nwine_quality[wine_quality.quality.isin(5, 6)].limit(4).toPandas()","e6f1df38":"\n# Starting\nprint('Starting row count:',wine_quality.count())\nprint('Starting column count:',len(wine_quality.columns))\n\n# Slice rows\ndf2 = wine_quality.limit(100)\nprint('Sliced row count:',df2.count())\n\n# Slice columns\ncols_list = wine_quality.columns[0:3]\ndf3 = wine_quality.select(cols_list)\nprint('Sliced column count:',len(df3.columns))","64f362da":"wine_quality.select(\"pH\",\"quality\").where(wine_quality.pH.startswith(\"3\")) \\\n                                  .where(wine_quality.pH.endswith(\"1\")).limit(4).toPandas()","4ac7e08d":"wine_quality.select('fixed acidity','sulphates','pH', 'quality').where(wine_quality.pH.like(\"%5%\")).show(10, False)","f8d56ece":"wine_quality.filter(\"pH>3\").limit(4).toPandas()","c3c85d91":"wine_quality.filter(\"pH>3 and sulphates<0.6\").limit(4).toPandas()","c8594169":"wine_quality.filter(\"pH>3 or sulphates<0.6\").limit(4).toPandas()","89da12c0":"wine_quality.filter(\"pH>3 and sulphates<0.6 and density like '%0.997%'\").limit(4).toPandas()","09a53354":"wine_quality.filter(\"pH>3 and sulphates<0.6 and density not like '%0.997%'\").limit(4).toPandas()","faf11b79":"## or condition\nwine_quality.filter( (wine_quality.pH  == 3.0) | (wine_quality.quality  == 5) ).limit(5).toPandas()","645623bd":"## and condition\nwine_quality.filter( (wine_quality.pH  == 3.0) & (wine_quality.quality  == 5) ).limit(5).toPandas()","22ac28b4":"## and and not equal to\nwine_quality.filter( (wine_quality.pH  == 3.0) & (wine_quality.quality  != 5) ).limit(5).toPandas()","4739a06b":"wine_quality.filter(wine_quality.pH.startswith(\"2\")).limit(5).toPandas()","02b54be6":"wine_quality.filter(wine_quality.pH.endswith(\"9\")).limit(5).toPandas()","37d1679f":"wine_quality.filter(wine_quality.pH.contains(\"4\")).limit(5).toPandas()","0dbf5e74":"## null check\nwine_quality.filter(wine_quality['fixed acidity'].isNull() | wine_quality.alcohol.isNull()).limit(4).toPandas()","af81abfd":"## Not null check\nwine_quality.filter(wine_quality['fixed acidity'].isNotNull() | wine_quality.alcohol.isNotNull()).limit(4).toPandas()","3c916e3f":"wine_quality.filter(\"'fixed acidity' is NULL\").limit(4).toPandas()","7a75daa3":"wine_quality.filter(\"'fixed acidity' is not NULL\").limit(4).toPandas()","245c174b":"wine_quality.filter(\"pH>4\").select(['fixed acidity','sulphates','pH', 'quality']).limit(4).toPandas()","c0debc87":"wine_quality.select(['fixed acidity','sulphates','pH', 'quality']).filter(\"pH>3\").orderBy(wine_quality[\"quality\"].desc()).limit(4).toPandas()","4386ed5a":"from pyspark.sql import Row\n\nPerson = Row(\"name\", \"age\")\nPerson","a26a8b6f":"'name' in Person","59511dc4":"Person(\"Alice\", 11)","bd154e2f":"result = wine_quality.select(['fixed acidity','sulphates','pH', 'quality']).filter(\"pH>3\").orderBy(wine_quality[\"pH\"].desc()).collect()","fe56a3c5":"result[:5]","4cebc243":"type(result[0])","51b915f7":"result[0].asDict()","61a1cfe2":"for item in result[0]:\n    print(item)","8eb69503":"backto_df = spark.createDataFrame(result)\nbackto_df.limit(5).toPandas()","af4247d5":"# Multiple filters","4619181d":"# A brief Walkthrough of the Technologies\n\nApache Spark is one of the hottest new trends in the technology domain. It is the framework with probably the highest potential to realize the fruit of the marriage between Big Data and Machine Learning.\n\nSpark is implemented on Hadoop\/HDFS and written mostly in Scala, a functional programming language, similar to Java. \nIn fact, Scala needs the latest Java installation on your system and runs on JVM. However, for most beginners, Scala is not a language that they learn first to venture into the world of data science. \n\nFortunately, Spark provides a wonderful Python integration, called PySpark, which lets Python programmers to interface with the Spark framework and learn how to manipulate data at scale and work with objects and algorithms over a distributed file system.\n\n\n\n# The Power of Spark\n\n\nOne thing to remember is that Spark is not a programming language like Python or Java. It is a general-purpose distributed data processing engine, suitable for use in a wide range of circumstances. It is particularly useful for big data processing both at scale and with high speed.\n\nApplication developers and data scientists generally incorporate Spark into their applications to rapidly query, analyze, and transform data at scale. \n\nSome of the tasks that are most frequently associated with Spark, include, \u2013 ETL and SQL batch jobs across large data sets (often of terabytes of size), \u2013 processing of streaming data from IoT devices and nodes, data from various sensors, financial and transactional systems of all kinds, and \u2013 machine learning tasks for e-commerce or IT applications.\n\nAt its core, Spark builds on top of the Hadoop\/HDFS framework for handling distributed files. It is mostly implemented with Scala, a functional language variant of Java. \n\nThere is a core Spark data processing engine, but on top of that, there are many libraries developed for SQL-type query analysis, distributed machine learning, large-scale graph computation, and streaming data processing. \n\nMultiple programming languages are supported by Spark in the form of easy interface libraries: Java, Python, Scala, and R.\n\n\n# How Spark Works\n\nThe basic idea of distributed processing is to divide the data chunks into small manageable pieces (including some filtering and sorting), bring the computation close to the data i.e. use small nodes of a large cluster for specific jobs and then re-combine them back. \n\nThe dividing portion is called the \u2018Map\u2019 action and the recombination is called the \u2018Reduce\u2019 action. Together, they make the famous \u2018MapReduce\u2019 paradigm, which was introduced by Google around 2004.\n\nFor example, if a file has 100 records to be processed, 100 mappers can run together to process one record each. Or maybe 50 mappers can run together to process two records each. After all the mappers complete processing, the framework shuffles and sorts the results before passing them on to the reducers. A reducer cannot start while a mapper is still in progress. \n\nAll the map output values that have the same key are assigned to a single reducer, which then aggregates the values for that key.\n\n","09ea0b49":"### OrderBy","3b7b76f6":"#### and now welcome on stage the summary() function in PySpark.","efdaa39d":"#### If the object is printed out it tells about the column names and data types of those columns separated by colon.","0479c634":"#### Filter and select combination","e32a4d2e":"#### Let's try getting some standard stats of the data we have","2d9fbc20":"#### Better but not eye pleasing though.","8d84c6d5":"# Slicing a DataFrame","b761aec9":"#### The same using summary() function. It is done using select() function which we'll cover right in this notebook in just a while.","51dafc65":"# Select functionality on a DataFrame","cc9b3a21":"#### way too many rows, lets use the limit method.","506b6f6d":"#### Filter, select and orderby combination","4cbec82d":"# SQL based null filters","4cdb9939":"# Collecting Results as Objects\n\nThe collect() function on DataFrame returns an array\/list of Rows which is a flattened version of the DataFrame. It shows the metadata of the DataFrame.","250dff18":"Spark DataFrames are built on top of the Spark SQL platform, if you already know SQL, you can easily use SQL commands to get similar operations done in Spark also.","46f8b566":"Reference: https:\/\/spark.apache.org\/docs\/latest\/api\/python\/reference\/api\/pyspark.sql.Row.html#pyspark.sql.Row","bd2b345d":"# DataFrame based null filters","2c5d4701":"#### We have seen the functions show(), limit() and toPandas() already in action for data printing.","d5ade165":"# Filtering a DataFrame","ed691757":"#### It can be made much better though with the help of a new function.","463c3f58":"## Thanks!\n\n### More in the next notebook.","080e5618":"# Using DataFrame style filters","250e093f":"# String based filters","0661be7a":"#### The quartiles are additional in Pandas describe() function","cf3990a1":"#### Note how only the columns we defined the schema for have been read.","1f60d394":"#### Back to collect()","4739d838":"#### This is pretty smooth on the eyes but what is happening inside the toPandas() function call?\n* First of all this method is only going to work if Pandas library is installed and available in the Python execution environment. Yes you guessed it right, this function returns a Pandas DF.\n* Second, do not use this method of displaying content when the content is too heavy because all the content is loaded into memory and it would be similar to using Pandas without Spark's ability to deal with large data. \n* In case the data is heavy beyond a manageable limit then the processes would most probability end up either crashing or executing at snail's pace.","28db3398":"#### As good as the Pandas describe(), notice the index of the two DataFrames.","8f506f80":"#### Accessing a row as a dictionary","744773e6":"#### List of rows to DataFrame","ce8c8c5f":"##### The name of the file in the output seems to make less sense. The folder name is indicative but not the file name. \n","d1778184":"#### The same for multiple columns","d6659adf":"Reference\n* https:\/\/sparkbyexamples.com\/pyspark\/pyspark-where-filter\/\n* https:\/\/sparkbyexamples.com\/pyspark\/pyspark-filter-rows-with-null-values\/\n* https:\/\/stackoverflow.com\/questions\/41000273\/spark-difference-between-collect-take-and-show-outputs-after-conversion\n* https:\/\/www.kdnuggets.com\/2020\/04\/benefits-apache-spark-pyspark.html","400adb74":"#### In case one needs to get the standard stats of a single column","2c8edeb8":"Reference: https:\/\/www.kdnuggets.com\/2020\/04\/benefits-apache-spark-pyspark.html","ae6f8e80":"### Saving file in Partitions by Categorical Column","5a86b200":"### PySpark describe() VS Pandas describe()","5d0ff30e":"#### When one needs to know the data type of a single column ","eddfe391":"#### Note the types here:","34d4f78b":"A row in DataFrame. The fields in it can be accessed:\n\n* like attributes (row.key)\n* like dictionary values (row[key])\n\nkey in row will search through row keys.\n\nRow can be used to create a row object by using named arguments. It is not allowed to omit a named argument to represent that the value is None or missing. This should be explicitly set to None in this case.\n\n","e110b5b7":"#### Using filter construct","659e2e73":"# What are we planning to do?","4e5afdc0":"# Reading a DataFrame","39216273":"#### Filtering a DataFrame with SQL 'Like' Operation","fb603e91":"# Data Summary","14dfdc0d":"### Schema Modifications \n\nIt can be possible that PySpark is not reading the columns in the very specific data type that you'd like the columns to be read in.\n\nIn that case data type can be assigned when reading the file.","ada1416c":"# Creating a DataFrame manually","accd3f60":"1. Creating a DataFrame manually\n2. Read DataFrame\n3. Write DataFrame\n4. Validate DataFrame\n5. Search DataFrame\n6. Select functionality on a DataFrame\n7. Filter DataFrame\n8. Slicing a DataFrame\n9. Introducing SQL in PySpark","72e541d8":"#### Notice how in Pyspark the isin() function doesn't require a list but the values directly as arguments","be6545fd":"# Writing a DataFrame","1107643f":"#### We have also seen how the schema of a DataFrame can be structured by printing the DataFrame object like below","35a095c0":"#### Using where construct","e96468a5":"Row also can be used to create another Row like class, then it could be used to create Row objects, such as","ded91858":"#### Pretty slick!\n\n#### Now printing just the columns as a list of strings.","5ec8e349":"### Descending Order"}}