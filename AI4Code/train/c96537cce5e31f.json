{"cell_type":{"1a5822ce":"code","80410316":"code","9b6ece7b":"code","3a5e266e":"code","879263f2":"code","b5dfa31c":"code","b02ba905":"code","567f243f":"code","836d5498":"code","059635df":"code","42206402":"code","bd0b8462":"code","67988216":"code","5c7ec5ec":"code","b37549b6":"code","bda4eb17":"code","ba6da4b2":"code","a8316908":"code","30fa19c9":"code","496f19a9":"code","80949a35":"code","0104dbc1":"code","b7455b06":"code","45347297":"code","5971e89a":"code","e36a8aa9":"code","d0b0c1c0":"code","300045a8":"code","66f2a08b":"code","b471c7d1":"code","a4b56f76":"code","69ecabea":"code","a97213df":"code","395d2c8a":"code","92a0f709":"code","8caa8902":"code","4506104c":"code","d7b112e9":"code","6c217f88":"code","a601357a":"code","fdd08b36":"code","b26d6029":"code","93dbd5bf":"code","a734f386":"markdown","99c85378":"markdown","9497acc2":"markdown","6a28342f":"markdown","47741809":"markdown","b950b346":"markdown","d1e3cd7a":"markdown","6290e1ef":"markdown","459e3570":"markdown","33bd10dc":"markdown","9cc6081a":"markdown","e9b30f41":"markdown","e1cbbcef":"markdown","9e4121c0":"markdown"},"source":{"1a5822ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","80410316":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nfrom scipy.stats import norm, skew\nimport pylab\n%matplotlib inline","9b6ece7b":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')\ntest_df = test['Id']","3a5e266e":"print(train.shape)\nprint(test.shape)","879263f2":"train.head()","b5dfa31c":"train.info()","b02ba905":"train.describe()","567f243f":"# count the number of missing values in each column\nmissing = train.isnull().sum()\nmissing = missing[missing > 0]\nmissing.plot.bar(rot=75)","836d5498":"# get a list of quantative columns(numeric) \nquantitative = [f for f in train.columns if train.dtypes[f] != 'object']\n# get a list of qulitative features\nqualitative = [f for f in train.columns if train.dtypes[f] == 'object']","059635df":"#get the output and visualize it\ny = train['SalePrice']\ny.hist(bins=50)\nplt.show()","42206402":"quantitative_data = train[quantitative]\nquantitative_data.hist(bins=50, figsize=(20,20))\nplt.show()","bd0b8462":"quantitative_data = train[quantitative]\ncorr_matrix = quantitative_data.corr()\ncorr_matrix[\"SalePrice\"].sort_values(ascending=False)","67988216":"plt.subplots(figsize=(20,15))\nax = sns.heatmap(quantitative_data.corr(),\n                vmin=-1, vmax=1, center=0,\n                cmap=sns.diverging_palette(20, 220, n=200),\n                square=True)","5c7ec5ec":"train_to_plot = train.copy()\nfor c in qualitative:\n    train_to_plot[c] = train_to_plot[c].astype('category')\n    if train_to_plot[c].isnull().any():\n        train_to_plot[c] = train_to_plot[c].cat.add_categories(['MISSING'])\n        train_to_plot[c] = train_to_plot[c].fillna('MISSING')\ndef boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\n#create a box plot to show how the output changes with qualitative features\nf = pd.melt(train_to_plot, id_vars=['SalePrice'], value_vars=qualitative)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, height=5)\ng = g.map(boxplot, \"value\", \"SalePrice\")","b37549b6":"train[quantitative].isnull().sum()","bda4eb17":"# fill missing values in train quantitative features\ntrain[\"LotFrontage\"] = train.groupby(\"Neighborhood\")[\"LotFrontage\"]\\\n                            .transform(lambda x: x.fillna(x.median()))\ntrain['MasVnrArea'] = train['MasVnrArea'].fillna(0)\ntrain['GarageYrBlt'] = train['GarageYrBlt'].fillna(0)\n\n\n# fill missing values in test quantitative features\n\ntest[\"LotFrontage\"] = test.groupby(\"Neighborhood\")[\"LotFrontage\"]\\\n                          .transform(lambda x: x.fillna(x.median()))\ntest['MasVnrArea'] = test['MasVnrArea'].fillna(0)\ntest['GarageYrBlt'] = test['GarageYrBlt'].fillna(0)","ba6da4b2":"train[quantitative].isnull().sum()","a8316908":"qualitative","30fa19c9":"train[qualitative].isnull().sum()","496f19a9":"train[qualitative] = train[qualitative].fillna('unknown')\ntest[qualitative] = test[qualitative].fillna('unknown')","80949a35":"train[qualitative].isnull().sum()\ntest[qualitative].isnull().sum()","0104dbc1":"train['Alley'].unique()","b7455b06":"# drop features with many missing values \ntrain = train.drop(columns=[\"Id\", \"Fence\", \"CentralAir\", \"FireplaceQu\", \"PoolArea\", \"LowQualFinSF\", \"3SsnPorch\", \"MiscVal\", 'RoofMatl','Street','Condition2','Utilities','Heating'])\n\ntest = test.drop(columns=[\"Id\", \"Fence\", \"CentralAir\", \"FireplaceQu\", \"PoolArea\", \"LowQualFinSF\", \"3SsnPorch\", \"MiscVal\", 'RoofMatl','Street','Condition2','Utilities','Heating'])","45347297":"train.drop(['GarageYrBlt','TotRmsAbvGrd','BsmtFinSF1','BsmtFinSF2', 'BsmtUnfSF'], axis=1, inplace=True)\n\ntest.drop(['GarageYrBlt','TotRmsAbvGrd','BsmtFinSF1','BsmtFinSF2', 'BsmtUnfSF'], axis=1, inplace=True)","5971e89a":"print(train.shape)\nprint(test.shape)\ntrain.isnull().sum()","e36a8aa9":"ordinal_ranking = {'Ex':5, 'Gd':4, 'TA':3, 'Fa':2, 'Po':1, 'NA':0, 'GdPrv':4, 'MnPrv':3, 'GdWo':2, 'MnWw':1, 'Reg':3, 'IR1':2, 'IR2':1,'IR3':0,'AllPub':4, 'NoSewr':3, 'NoSeWa':2, \n            'ELO':1, 'Gtl':3, 'Mod':2, 'Sev':1, 'Av':3, 'Mn':2, 'No':1, 'GLQ':5, 'ALQ':4, 'BLQ':3, 'Rec':2, 'LwQ':1, 'Unf':-1, 'Typ':8, 'Min1':7, 'Min2':6, 'Mod':5,\n            'Maj1':4, 'Maj2':3, 'Sev':2, 'Sal':1, 'Fin':2, 'RFn':1, 'unknown':0}","d0b0c1c0":"train = train.replace(ordinal_ranking)\ntest = test.replace(ordinal_ranking)","300045a8":"nominal_features = ['Alley','LandContour', 'LotConfig','Neighborhood','Condition1', 'BldgType','RoofStyle',\n                    'MasVnrType','Foundation','GarageType','PavedDrive',\n                   'SaleCondition']","66f2a08b":"train[nominal_features].head()","b471c7d1":"# one_hot_encoding\ntrain = pd.get_dummies(data = train , columns = nominal_features, drop_first=True)\n\ntest = pd.get_dummies(data = test , columns = nominal_features, drop_first=True)","a4b56f76":"print(train.shape)\nprint(test.shape)\n","69ecabea":"col_to_drop = ['MiscFeature' , 'Electrical' , 'SaleType' , 'Exterior2nd' , 'Exterior1st' , 'HouseStyle' , 'MSZoning']","a97213df":"train = train.drop(columns = col_to_drop)\n\ntest = test.drop(columns = col_to_drop)","395d2c8a":"print(train.shape)\nprint(test.shape)","92a0f709":"# from sklearn.preprocessing import OrdinalEncoder\n# col_to_encode = ['MiscFeature' , 'Electrical' , 'SaleType' , 'HouseStyle' , 'MSZoning']\n# cols = ['Exterior2nd', 'Exterior1st', ]\n# train.drop(columns = cols)\n# test.drop(columns=cols)\n# ordinal_encoder = OrdinalEncoder()\n# # Assigning numerical values and storing in another column\n# train[col_to_encode] = ordinal_encoder.fit_transform(train[col_to_encode])\n# test[col_to_encode] = ordinal_encoder.transform(test[col_to_encode])\n# for col in col_to_encode:\n#     print(train[col].unique())","8caa8902":"y = train['SalePrice']\nx = train.drop(columns = ['SalePrice'])","4506104c":"# split data\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_val, y_train, y_val = train_test_split(x, y, train_size = 0.8, test_size=0.2, random_state=0)","d7b112e9":"from sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\n\nmodel_1=RandomForestRegressor(n_estimators=500, random_state=0)\nxg_model = XGBRegressor(n_estimators=700, learning_rate=0.01)\nmodel3 = LinearRegression()\nmodel4 = Ridge(alpha=1.0, normalize=True, tol=0.001, solver='svd', random_state=0)\n\nxg_model.fit(x_train, y_train)\nmodel_1.fit(x_train, y_train)\nmodel3.fit(x_train, y_train)\nmodel4.fit(x_train, y_train)","6c217f88":"predicted=xg_model.predict(x_val)\n\npredicted1 = model_1.predict(x_val)\n\npreds = model3.predict(x_val)\npreds1 = model4.predict(x_val)","a601357a":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nimport numpy as np\n\n\nprint('mean_squared_error using XGBRegressor : {}'.format(np.sqrt(mean_squared_error(y_val,predicted))))\n\nprint('mean_squared_error using RandomForestRegressor : {}'.format(np.sqrt(mean_squared_error(y_val,predicted1))))\nprint('*' * 100)\nprint('mean_absolute_error using XGBRegressor : {}'.format(mean_absolute_error(y_val,predicted)))\n\nprint('mean_absolute_error using RandomForestRegressor : {}'.format(mean_absolute_error(y_val,predicted1)))\nprint('*' * 100)\nprint('Accuracy using XGBRegressor : {}'.format(r2_score(y_val,predicted)))\n\nprint('Accuracy using RandomForestRegressor : {}'.format(r2_score(y_val,predicted1)))","fdd08b36":"print('mean_squared_error using Ridge : {}'.format(np.sqrt(mean_squared_error(y_val,preds1))))\n\nprint('*' * 100)\nprint('mean_absolute_error using Ridge : {}'.format(mean_absolute_error(y_val,preds1)))\n\nprint('*' * 100)\nprint('Accuracy using Ridge : {}'.format(r2_score(y_val,preds1)))\n\n","b26d6029":"# test = test.fillna(0)\n# np.where(np.isnan(test))\n# # x_val.info()\n# y_test_pred = model_1.predict(test)","93dbd5bf":"# test['SalePrice']= y_test_pred\n# test['Id']= test_df\n# test[['Id','SalePrice']].to_csv('\/kaggle\/working\/submission.csv', index=False)","a734f386":"### import packages\n","99c85378":"### plot the correlation of features with the target","9497acc2":"### handling missing values","6a28342f":"### train the model","47741809":"### calculate error and accuracy","b950b346":"### make a boxplot to show how the output changes with qualitative features","d1e3cd7a":"### Exploratory Data Analysis","6290e1ef":"### feature selection","459e3570":"### handle missing values in categorical data","33bd10dc":"### feature encoding","9cc6081a":"### read data","e9b30f41":"### make predictions","e1cbbcef":"### model","9e4121c0":"### visualize quantitative features"}}