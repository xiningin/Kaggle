{"cell_type":{"44130095":"code","931e2f30":"code","6f3ac0ad":"code","d8cde754":"code","89f88c0e":"code","587261b7":"code","c29bc31d":"code","905a89af":"code","4c2b2025":"code","c0f9b35e":"code","44b599c4":"code","b207317b":"code","07d41f41":"code","534a708f":"code","e601b451":"code","677d590e":"code","3c2495b4":"code","103cc03a":"code","d5707eaa":"code","af1b63d0":"code","dc25d6de":"code","5f4c50b1":"code","f18842ec":"code","511daed8":"code","f2a0ef16":"code","fa14b045":"code","7f8d99a5":"code","d3a92b02":"code","b93dbd25":"code","cdb6559b":"code","5f1a2b92":"code","5cd80447":"markdown","a5cb475d":"markdown","411411cd":"markdown","e6a6780c":"markdown","5bb4f3e3":"markdown","2300d93b":"markdown","409d3e57":"markdown","b9c3fa74":"markdown","a4dc66fb":"markdown","f531498c":"markdown","b3a55e4c":"markdown","4625e660":"markdown","48a35203":"markdown","16e44deb":"markdown","f803d192":"markdown","447ce258":"markdown","08d8c440":"markdown","f89c2e2b":"markdown","ae9f47ff":"markdown","d59297ad":"markdown","30dc26a0":"markdown","d7e78c56":"markdown","b0e9e0cc":"markdown","0bfd2ac3":"markdown","e9756962":"markdown"},"source":{"44130095":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","931e2f30":"import cv2\nimport os\nimport re\nimport torch\nimport torchvision\nfrom torchvision import transforms \nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\nfrom matplotlib import pyplot as plt","6f3ac0ad":"WEIGHTS_FILE = '\/kaggle\/input\/fasterrcnn\/fasterrcnn_resnet50_fpn_best.pth'","d8cde754":"train_df = pd.read_csv(\"\/kaggle\/input\/global-wheat-detection\/train.csv\")\nsubmit = pd.read_csv(\"\/kaggle\/input\/global-wheat-detection\/sample_submission.csv\")","89f88c0e":"train_df.head()","587261b7":"train_df=train_df.drop(columns=['width','height','source']) #Drop unwanted columns","c29bc31d":"train_df['image_id'].nunique() # There are total 3373 unique image in training dataset","905a89af":"(train_df['image_id'].value_counts()).max()  # maximum number of boxes in a single image are 116","4c2b2025":"(train_df['image_id'].value_counts()). min() # Minimum number of box in a single image is 1","c0f9b35e":"train_df['x'] = -1\ntrain_df['y'] = -1\ntrain_df['w'] = -1\ntrain_df['h'] = -1\n\ndef expand_bbox(x):\n    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n    if len(r) == 0:\n        r = [-1, -1, -1, -1]\n    return r","44b599c4":"train_df[['x', 'y', 'w', 'h']] = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x))) ##Lets convert the Box in \ntrain_df['x'] = train_df['x'].astype(np.float)                                        #in our desired formate    \ntrain_df['y'] = train_df['y'].astype(np.float)\ntrain_df['w'] = train_df['w'].astype(np.float)\ntrain_df['h'] = train_df['h'].astype(np.float)","b207317b":"train_df.head() ","07d41f41":"image_ids = train_df['image_id'].unique()\nvalid_ids = image_ids[-665:]\ntrain_ids = image_ids[:-665]\n\nvalid_df = train_df[train_df['image_id'].isin(valid_ids)]\ntrain_df = train_df[train_df['image_id'].isin(train_ids)]","534a708f":"trans = transforms.Compose([transforms.ToTensor()])   #Apply transform to image ","e601b451":"class WheatDataset(Dataset):\n\n    def __init__(self, dataframe, image_dir, transforms=None,train=True):\n        super().__init__()\n\n        self.image_ids = dataframe['image_id'].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n        self.train=train\n\n    def __len__(self) -> int:\n        return self.image_ids.shape[0]\n\n    def __getitem__(self, index: int):\n\n        image_id = self.image_ids[index]\n        image = cv2.imread(f'{self.image_dir}\/{image_id}.jpg', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        if self.transforms is not None:  #Apply transformation\n            image = self.transforms(image)\n        if(self.train==False):  # For test data\n            return image, image_id\n        #Else for train and validation data\n        records = self.df[self.df['image_id'] == image_id]   \n        boxes = records[['x', 'y', 'w', 'h']].values\n        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n        \n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        area = torch.as_tensor(area, dtype=torch.float32)\n\n        # there is only one class\n        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n        \n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        target['image_id'] = torch.tensor([index])\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n\n        return image, target,image_id  ","677d590e":"train_dir = '\/kaggle\/input\/global-wheat-detection\/train'\ntest_dir = '\/kaggle\/input\/global-wheat-detection\/test'","3c2495b4":"class Averager:      ##Return the average loss \n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total \/ self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n        \n        \ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_dataset = WheatDataset(train_df, train_dir, trans,True)\nvalid_dataset = WheatDataset(valid_df, train_dir, trans,True)\n\n\n# split the dataset in train and test set\nindices = torch.randperm(len(train_dataset)).tolist()\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=16,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=8,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\n#device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","103cc03a":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","d5707eaa":"images, targets, image_ids = next(iter(train_data_loader))\nimages = list(image.to(device) for image in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\nboxes = targets[4]['boxes'].cpu().numpy().astype(np.int32)\nsample = images[4].permute(1,2,0).cpu().numpy()\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 3)\n    \nax.set_axis_off()\nax.imshow(sample)","af1b63d0":"# load a model; pre-trained on COCO\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)","dc25d6de":"num_classes = 2  # 1 class (wheat) + background\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\n# Load the trained weights\nmodel.load_state_dict(torch.load(WEIGHTS_FILE))  ##Load pre trained weights\n#model.eval()\n\n#x = model.to(device)","5f4c50b1":"model.train()\nmodel.to(device)\nparams = [p for p in model.parameters() if p.requires_grad]\noptimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, weight_decay=0.00001)\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.5)\n#lr_scheduler = None\n\nnum_epochs = 5\n\nloss_hist = Averager()\nitr = 1\n\nfor epoch in range(num_epochs):\n    loss_hist.reset()\n    \n    for images, targets, image_ids in train_data_loader:\n        \n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)   ##Return the loss\n\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        loss_hist.send(loss_value)  #Average out the loss\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        if itr % 50 == 0:\n            print(f\"Iteration #{itr} loss: {loss_value}\")\n\n        itr += 1\n    \n    # update the learning rate\n    if lr_scheduler is not None:\n        lr_scheduler.step()\n\n    print(f\"Epoch #{epoch} loss: {loss_hist.value}\")","f18842ec":"test_dataset = WheatDataset(submit,test_dir, trans,False)","511daed8":"test_data_loader = DataLoader( test_dataset, batch_size=8, shuffle=False)  ##Test dataloader","f2a0ef16":"detection_threshold = 0.45","fa14b045":"def format_prediction_string(boxes, scores): ## Define the formate for storing prediction results\n    pred_strings = []\n    for j in zip(scores, boxes):\n        pred_strings.append(\"{0:.4f} {1} {2} {3} {4}\".format(j[0], j[1][0], j[1][1], j[1][2], j[1][3]))\n\n    return \" \".join(pred_strings)","7f8d99a5":"## Lets make the prediction\nresults=[]\nmodel.eval()\n\nfor images, image_ids in test_data_loader:    \n\n    images = list(image.to(device) for image in images)\n    outputs = model(images)\n\n    for i, image in enumerate(images):\n\n        boxes = outputs[i]['boxes'].data.cpu().numpy()    ##Formate of the output's box is [Xmin,Ymin,Xmax,Ymax]\n        scores = outputs[i]['scores'].data.cpu().numpy()\n        \n        boxes = boxes[scores >= detection_threshold].astype(np.int32) #Compare the score of output with the threshold and\n        scores = scores[scores >= detection_threshold]                    #slelect only those boxes whose score is greater\n                                                                          # than threshold value\n        image_id = image_ids[i]\n        \n        boxes[:, 2] = boxes[:, 2] - boxes[:, 0]         \n        boxes[:, 3] = boxes[:, 3] - boxes[:, 1]         #Convert the box formate to [Xmin,Ymin,W,H]\n        \n        \n            \n        result = {                                     #Store the image id and boxes and scores in result dict.\n            'image_id': image_id,\n            'PredictionString': format_prediction_string(boxes, scores)\n        }\n\n        \n        results.append(result)              #Append the result dict to Results list","d3a92b02":"test_df = pd.DataFrame(results, columns=['image_id', 'PredictionString'])\ntest_df.head()","b93dbd25":"sample = images[1].permute(1,2,0).cpu().numpy()\nboxes = outputs[1]['boxes'].data.cpu().numpy()\nscores = outputs[1]['scores'].data.cpu().numpy()\n\nboxes = boxes[scores >= detection_threshold].astype(np.int32)","cdb6559b":"fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 2)\n    \nax.set_axis_off()\nax.imshow(sample)","5f1a2b92":"test_df.to_csv('submission.csv', index=False)","5cd80447":"## If you like this kernel, Please upvote it ","a5cb475d":"### Lets load the test data","411411cd":"Welcome to the \"Global Wheat Detection\" competition! In this competition, contestent will detect wheat heads from outdoor images of wheat plants, including wheat datasets from around the globe. Using worldwide data, contestent will focus on a generalized solution to estimate the number and size of wheat heads. To better gauge the performance for unseen genotypes, environments, and observational conditions, the training dataset covers multiple regions. contestent will use more than 3,000 images from Europe (France, UK, Switzerland) and North America (Canada). The test data includes about 1,000 images from Australia, Japan, and China.","e6a6780c":"# 3.Finetuuning the model","5bb4f3e3":"###  Load train and test file","2300d93b":"### Defining the model\n\nFaster R-CNN is a model that predicts both bounding boxes and class scores for potential objects in the image.\n![image.png](attachment:image.png)\n\n\nLet\u2019s explain how this architecture works, Faster RCNN is composed from 3 parts\n\n1. Part 1 : Convolution layers : A CNN architecture is formed by a stack of distinct layers that transform the input volume into an output volume (e.g. holding the class scores) through a differentiable function.Convolutional networks were inspired by biological processes in that the connectivity pattern between neurons resembles the organization of the animal visual cortex. Individual cortical neurons respond to stimuli only in a restricted region of the visual field known as the receptive field. The receptive fields of different neurons partially overlap such that they cover the entire visual field.\n\n2. Part 2 : Region Proposel Network (RPN) : RPN is small neural network sliding on the last feature map of the convolution layers and predict wether there is an object or not and also predict the bounding box of those objects.\n\n3. Part 3 : Classes and Bounding Boxes prediction : Now we use another Fully connected neural networks that takes as an inpt the regions proposed by the RPN and predict object class ( classification) and Bounding boxes (Regression).","409d3e57":"# 2.Writing a custom dataset for our work","b9c3fa74":"### Lets train our model","a4dc66fb":"### Lets visualize some of the images with bounding box","f531498c":"### Splitting the dimension of box in the formate [xmin, ymin, w, h]\n#### Latter on we will convert the deminsion of box into [xmin, ymin, xmax, ymax] in ","b3a55e4c":"## Background\nSince this is my first object detection competition in Kaggle, I am mainly using it for learning. There are great kernel here, mostly using the FastRCNN model And I have learned how to prepare the data and use it with this popular model.I have decided to build a learning kernel, that at least at the beginning will explain the topic and the concepts, the definition and the basics. I will explain every important code line.\n\n## IF you like this kernel, Please upvote","4625e660":"One note on the labels. The model considers class 0 as background. If your dataset does not contain the background class, you should not have 0 in your labels. For example, assuming you have just two classes, cat and dog, you can define 1 (not 0) to represent cats and 2 to represent dogs. So, for instance, if one of the images has booth classes, your labels tensor should look like [1,2].\n\nIn our case number of classes are 2, class 0 for background and class 1 for wheat.","48a35203":"# Content\n### 1.EDA\n### 2. Writing a custom dataset for our work\n### 3. Define the model\n### 4. Finetuning from a pretrained model\n### 5. Prediction\n### 6. End Note","16e44deb":"# Introduction","f803d192":"1. PYTORCH TORCHVISION OBJECT DETECTION FINETUNING TUTORIAL [https:\/\/pytorch.org\/tutorials\/intermediate\/torchvision_tutorial.html](http:\/\/)\n2. GREAT KERNEL BY nvnn [https:\/\/www.kaggle.com\/nvnnghia\/fasterrcnn-pseudo-labeling](http:\/\/)\n3. Pretrained weight [https:\/\/www.kaggle.com\/mathurinache\/fasterrcnn](http:\/\/)\n4. Fast RCNN : https:\/\/towardsdatascience.com\/faster-rcnn-object-detection-f865e5ed7fc4","447ce258":"# 4. Prediction","08d8c440":"### Set the threshold value for predicting bounding box.","f89c2e2b":"### 2.1 Writing a custom dataset for train and validation images","ae9f47ff":"### Splitting the data into train and validation set","d59297ad":"## Submission","30dc26a0":"# 1.EDA","d7e78c56":"# Acknowledgements","b0e9e0cc":"### Install and import necessary libraries","0bfd2ac3":"### Lets plot some of our prediction","e9756962":"*There are two common situations where one might want to modify one of the available models in torchvision modelzoo. The first is when we want to start from a pre-trained model, and just finetune the last layer. The other is when we want to replace the backbone of the model with a different one . In our case, we want to fine-tune from a pre-trained model, given that our dataset is very small, so we will be following approach number first.*"}}