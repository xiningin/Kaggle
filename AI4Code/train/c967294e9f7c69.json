{"cell_type":{"b4d694a9":"code","82493f8e":"code","fa04b97e":"code","23b91432":"code","270835c8":"code","cb8f83ac":"code","93a3f5aa":"code","6afeec91":"code","1cf1e361":"code","0ac19625":"code","c808df32":"code","6e361ef9":"code","8f9bedfc":"code","6620b43b":"code","42e39bdb":"code","53eadb5d":"code","1d4afff5":"code","8331c728":"code","95e1dbd2":"code","cacce361":"code","e13042ec":"code","387d8525":"code","0e4c371f":"code","a86ffe2d":"code","498307b3":"code","e70eb8c4":"code","ede20d2d":"code","28ccc249":"code","86160176":"code","9e44293e":"code","12b41496":"code","072c0b99":"code","bc463c31":"code","7d617294":"code","877edb7b":"code","5cdda735":"code","5af956ca":"code","49adf372":"code","5e6ace35":"code","a4edc3af":"code","b21cedeb":"code","3a848e17":"code","59a058e3":"code","3147b63f":"code","428730b9":"code","a9e2bc67":"code","ac25f2bb":"code","013267ae":"code","8e6da9e8":"code","72551070":"code","50a131e7":"code","417e6202":"code","8d39f7d8":"code","40ee29b5":"code","a7752e1b":"code","c9c57e9d":"code","f1c309ce":"code","fb40938d":"code","28df9c9f":"code","4503ed48":"code","27d59959":"code","8cd5dd94":"code","ca4e0b5b":"code","0d97bb0b":"code","22486c62":"code","a021ec93":"code","32c214cd":"code","0dd79551":"code","c6a08550":"code","a025e828":"code","a6770675":"code","ef60b698":"code","2e255391":"code","2ceaa441":"code","e503ef7f":"code","20a94e11":"code","314576b8":"code","90fe0212":"code","5082c07d":"code","e38d611b":"markdown","67f7abf4":"markdown","4f87abd0":"markdown","e75bdea3":"markdown","f7b0531a":"markdown","ff5b8674":"markdown","e6da69af":"markdown","d5e7beae":"markdown","a58e514e":"markdown","d1a1c74a":"markdown","997b3430":"markdown","d56c9575":"markdown","66f7ed35":"markdown","38696f79":"markdown","7f553176":"markdown","99ed6ff5":"markdown","8b081065":"markdown","9367069f":"markdown","7fdd74b8":"markdown","98ddd712":"markdown","3c5365b4":"markdown"},"source":{"b4d694a9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier","82493f8e":"train_data = pd.read_csv('..\/input\/train.csv')","fa04b97e":"train_data.info()","23b91432":"train_data.head()","270835c8":"train_data.Sex.value_counts()","cb8f83ac":"train_data['is_male'] = np.where(train_data.Sex == 'male', 1,0)","93a3f5aa":"train_data.Embarked.value_counts()","6afeec91":"train_data['Embarked_S'] = np.where(train_data.Embarked == 'S', 1,0)\ntrain_data['Embarked_C'] = np.where(train_data.Embarked == 'C', 1,0)","1cf1e361":"train_data['FamilySize'] = train_data['SibSp'] + train_data['Parch'] + 1","0ac19625":"train_data[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c808df32":"train_data.isnull().sum()","6e361ef9":"train_data.isnull().mean()","8f9bedfc":"train_data.isnull().mean().plot.bar(figsize=(12,6))\nplt.ylabel('Percentage of missing values')\nplt.xlabel('Variables')\nplt.title('Quantifying missing data')","6620b43b":"train_data.nunique()","42e39bdb":"train_data.nunique().plot.bar(figsize=(12,6))\nplt.ylabel('Number of unique categories')\nplt.xlabel('Variables')\nplt.title('Cardinality')","53eadb5d":"label_freq = train_data['Pclass'].value_counts() \/ len(train_data)\nfig = label_freq.sort_values(ascending=False).plot.bar()\nfig.axhline(y=0.30, color='red')\nfig.set_ylabel('percentage within each category')\nfig.set_xlabel('Variable: Pclass')\nfig.set_title('Identifying Rare Categories')\nplt.show()","1d4afff5":"label_freq = train_data['Sex'].value_counts() \/ len(train_data)\nfig = label_freq.sort_values(ascending=False).plot.bar()\nfig.axhline(y=0.35, color='red')\nfig.set_ylabel('percentage within each category')\nfig.set_xlabel('Variable: Sex')\nfig.set_title('Identifying Rare Categories')\nplt.show()","8331c728":"label_freq = train_data['SibSp'].value_counts() \/ len(train_data)\nfig = label_freq.sort_values(ascending=False).plot.bar()\nfig.axhline(y=0.10, color='red')\nfig.set_ylabel('percentage within each category')\nfig.set_xlabel('Variable: SibSp')\nfig.set_title('Identifying Rare Categories')\nplt.show()","95e1dbd2":"label_freq = train_data['Embarked'].value_counts() \/ len(train_data)\nfig = label_freq.sort_values(ascending=False).plot.bar()\nfig.axhline(y=0.20, color='red')\nfig.set_ylabel('percentage within each category')\nfig.set_xlabel('Variable: Embarked')\nfig.set_title('Identifying Rare Categories')\nplt.show()","cacce361":"label_freq = train_data['Parch'].value_counts() \/ len(train_data)\nfig = label_freq.sort_values(ascending=False).plot.bar()\nfig.axhline(y=0.15, color='red')\nfig.set_ylabel('percentage within each category')\nfig.set_xlabel('Variable: Parch')\nfig.set_title('Identifying Rare Categories')\nplt.show()","e13042ec":"train_data['Fare'].hist(bins=50)\n\nplt.title('Histogram Column Fare')","387d8525":"import scipy.stats as stats\nstats.probplot(train_data['Fare'], dist=\"norm\", plot=plt)\nplt.title('Q-Q Plot for column Fare')\nplt.show()","0e4c371f":"train_data['Age'].hist(bins=50)\nplt.title('Histogram Column Age')","a86ffe2d":"stats.probplot(train_data['Age'], dist=\"norm\", plot=plt)\nplt.title('Q-Q Plot for column Age')\nplt.show()","498307b3":"_ = train_data.hist(bins=30, figsize=(12,12), density=True)","e70eb8c4":"import seaborn as sns\nsns.lmplot(x=\"Age\", y=\"Survived\", data=train_data, order=1)\nplt.ylabel('Target')\nplt.xlabel('Independent variable')","ede20d2d":"import seaborn as sns\nsns.lmplot(x=\"Fare\", y=\"Survived\", data=train_data, order=1)\nplt.ylabel('Target')\nplt.xlabel('Independent variable')","28ccc249":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(train_data, col='Survived', row='Pclass', size=2.5, aspect=1.5)\ngrid.map(plt.hist, 'Age', alpha=1, bins=20)\ngrid.add_legend();","86160176":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(train_data, col='Survived', row='Pclass', size=2.5, aspect=1.5)\ngrid.map(plt.hist, 'Fare', alpha=1, bins=20)\ngrid.add_legend();","9e44293e":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(train_data, col='Survived', row='Embarked', size=2.5, aspect=1.5)\ngrid.map(plt.hist, 'Age', alpha=1, bins=20)\ngrid.add_legend();","12b41496":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(train_data, col='Survived', row='Embarked', size=2.5, aspect=1.5)\ngrid.map(plt.hist, 'Fare', alpha=1, bins=20)\ngrid.add_legend();","072c0b99":"grid = sns.FacetGrid(train_data, row='Embarked', col='Survived', size=2.5, aspect=1.5)\ngrid.map(sns.barplot, 'Sex', 'Age', alpha=1, ci=None)\ngrid.add_legend()","bc463c31":"grid = sns.FacetGrid(train_data, row='Embarked', col='Survived', size=2.5, aspect=1.5)\ngrid.map(sns.barplot, 'Sex', 'Fare', alpha=1, ci=None)\ngrid.add_legend()","7d617294":"sns.boxplot(y=train_data['Fare'])\nplt.title('Boxplot')","877edb7b":"sns.boxplot(y=train_data['Age'])\nplt.title('Boxplot')","5cdda735":"def find_boundaries(df, variable, distance):\n    IQR = df[variable].quantile(0.75) - df[variable].quantile(0.25)\n    lower_boundary = df[variable].quantile(0.25) - (IQR * distance)\n    upper_boundary = df[variable].quantile(0.75) + (IQR * distance)\n    return upper_boundary, lower_boundary","5af956ca":"upper_boundary, lower_boundary = find_boundaries(train_data, 'Age', 1.5)\nupper_boundary, lower_boundary","49adf372":"train_data.describe()","5e6ace35":"list1 =[\"SibSp\", \"Parch\", \"Age\", \"Fare\", \"Survived\"]\nsns.heatmap(train_data[list1].corr(), annot = True, fmt = \".2f\")\nplt.show()","a4edc3af":"g = sns.factorplot(x = \"Pclass\", y = \"Survived\", data = train_data, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","b21cedeb":"g = sns.factorplot(x = \"Parch\", y = \"Survived\", data = train_data, kind = \"bar\", size = 6)\ng.set_ylabels(\"Survived Probability\")\nplt.show()","3a848e17":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import BayesianRidge\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer","59a058e3":"X_train, X_test, y_train, y_test = train_test_split(\ntrain_data.drop(['Name','Sex', 'Embarked', 'Ticket', 'PassengerId', 'Cabin','Survived'], axis=1), train_data['Survived'], test_size=0.3,\nrandom_state=0, stratify = train_data['Survived'])","3147b63f":"y_train.mean()","428730b9":"y_test.mean()","a9e2bc67":"imputer_bayes = IterativeImputer(\nestimator=BayesianRidge(),\nmax_iter=10,\nrandom_state=0)\n\nimputer_knn = IterativeImputer(\nestimator=KNeighborsRegressor(n_neighbors=5),\nmax_iter=10,\nrandom_state=0)\n\nimputer_DT = IterativeImputer(\nestimator=DecisionTreeRegressor(\nmax_features='sqrt', random_state=0),\nmax_iter=10,\nrandom_state=0)\n\nimputer_missForest = IterativeImputer(\nestimator=ExtraTreesRegressor(\nn_estimators=10, random_state=0),\nmax_iter=10,\nrandom_state=0)","ac25f2bb":"imputer_bayes.fit(X_train)\nimputer_knn.fit(X_train)\nimputer_DT.fit(X_train)\nimputer_missForest.fit(X_train)","013267ae":"X_train_bayes = imputer_bayes.transform(X_train)\nX_train_knn = imputer_knn.transform(X_train)\nX_train_DT = imputer_DT.transform(X_train)\nX_train_missForest = imputer_missForest.transform(X_train)","8e6da9e8":"variables = train_data.columns\npredictors = [var for var in variables if var not in ['Name','Sex', 'Embarked', 'Ticket', 'PassengerId', 'Cabin', 'Survived']]\nX_train_bayes = pd.DataFrame(X_train_bayes, columns = predictors)\nX_train_knn = pd.DataFrame(X_train_knn, columns = predictors)\nX_train_DT = pd.DataFrame(X_train_DT, columns = predictors)\nX_train_missForest = pd.DataFrame(X_train_missForest, columns = predictors)","72551070":"vis_column = 'Fare'\nfig = plt.figure()\nax = fig.add_subplot(111)\nX_train[vis_column].plot(kind='kde', ax=ax, color='blue')\nX_train_bayes[vis_column].plot(kind='kde', ax=ax, color='green')\nX_train_knn[vis_column].plot(kind='kde', ax=ax, color='red')\nX_train_DT[vis_column].plot(kind='kde', ax=ax, color='black')\nX_train_missForest[vis_column].plot(kind='kde', ax=ax, color='orange')\n# add legends\nlines, labels = ax.get_legend_handles_labels()\nlabels = [vis_column+' original', vis_column+' bayes', vis_column+' knn', vis_column+' Trees', vis_column+' missForest']\nax.legend(lines, labels, loc='best')\nplt.show()","50a131e7":"vis_column = 'Age'\nfig = plt.figure()\nax = fig.add_subplot(111)\nX_train[vis_column].plot(kind='kde', ax=ax, color='blue')\nX_train_bayes[vis_column].plot(kind='kde', ax=ax, color='green')\nX_train_knn[vis_column].plot(kind='kde', ax=ax, color='red')\nX_train_DT[vis_column].plot(kind='kde', ax=ax, color='black')\nX_train_missForest[vis_column].plot(kind='kde', ax=ax, color='orange')\n# add legends\nlines, labels = ax.get_legend_handles_labels()\nlabels = [vis_column+' original', vis_column+' bayes', vis_column+' knn', vis_column+' Trees', vis_column+' missForest']\nax.legend(lines, labels, loc='best')\nplt.show()","417e6202":"X_test_DT = imputer_DT.transform(X_test)\nX_test_DT = pd.DataFrame(X_test_DT, columns = predictors)","8d39f7d8":"from sklearn.preprocessing import RobustScaler\nscaler = RobustScaler()\nscaler.fit(X_train_DT[['Age','Fare']])\nX_train_DT[['Age_scaled','Fare_scaled']] = scaler.transform(X_train_DT[['Age','Fare']])\nX_test_DT[['Age_scaled','Fare_scaled']] = scaler.transform(X_test_DT[['Age','Fare']])\n#X_test_scaled = scaler.transform(X_test)","40ee29b5":"X_train_DT.head()","a7752e1b":"# removed scaled features from latest version as distance based algos are not being used anymore\nX_train_DT.drop(['Age_scaled','Fare_scaled'], axis = 1, inplace = True)\nX_test_DT.drop(['Age_scaled','Fare_scaled'], axis = 1, inplace = True)","c9c57e9d":"gaussian = GaussianNB()\ngaussian.fit(X_train_DT, y_train)\nY_pred = gaussian.predict(X_test_DT)\nacc_gaussian_train = round(gaussian.score(X_train_DT, y_train) * 100, 2)\nacc_gaussian_test = round(gaussian.score(X_test_DT, y_test) * 100, 2)\nacc_gaussian_test","f1c309ce":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier(random_state=0, max_depth = 3, min_samples_leaf= 50)\ndecision_tree.fit(X_train_DT, y_train)\nY_pred = decision_tree.predict(X_test_DT)\nacc_decision_tree = round(decision_tree.score(X_train_DT, y_train) * 100, 2)\nacc_decision_tree_train = round(decision_tree.score(X_train_DT, y_train) * 100, 2)\nacc_decision_tree_test = round(decision_tree.score(X_test_DT, y_test) * 100, 2)\nacc_decision_tree_test","fb40938d":"feature_imp = pd.DataFrame({'feature':X_train_DT.columns, 'importance':decision_tree.feature_importances_})\nfeature_imp.sort_values(by = 'importance', ascending = False)","28df9c9f":"# Random Forest\n\nrandom_for = RandomForestClassifier(random_state=0, max_depth= 4, min_weight_fraction_leaf= 0.001)\nrandom_for.fit(X_train_DT, y_train)\nY_pred = random_for.predict(X_test_DT)\nacc_random_for_train = round(random_for.score(X_train_DT, y_train) * 100, 2)\nacc_random_for_test = round(random_for.score(X_test_DT, y_test) * 100, 2)\nacc_random_for_test","4503ed48":"feature_imp = pd.DataFrame({'feature':X_train_DT.columns, 'importance':random_for.feature_importances_})\nfeature_imp.sort_values(by = 'importance', ascending = False)","27d59959":"## GradientBoostingClassifier\n\ngrad_boost = GradientBoostingClassifier(random_state=0, max_depth= 4, min_weight_fraction_leaf= 0.01, \n                                        learning_rate=0.01, n_estimators= 700)\ngrad_boost.fit(X_train_DT, y_train)\nY_pred = grad_boost.predict(X_test_DT)\nacc_grad_boost_train = round(grad_boost.score(X_train_DT, y_train) * 100, 2)\nacc_grad_boost_test = round(grad_boost.score(X_test_DT, y_test) * 100, 2)\nacc_grad_boost_test","8cd5dd94":"feature_imp = pd.DataFrame({'feature':X_train_DT.columns, 'importance':grad_boost.feature_importances_})\nfeature_imp.sort_values(by = 'importance', ascending = False)","ca4e0b5b":"models = pd.DataFrame({\n    'Model': [  \n               'Naive Bayes',  \n              'Decision Tree', 'Random Forest', 'GradientBoostingClassifier'],\n    'Train Score': [ \n               acc_gaussian_train,  acc_decision_tree_train,acc_random_for_train, acc_grad_boost_train],\n    'Test Score': [ \n               acc_gaussian_test,  acc_decision_tree_test,acc_random_for_test, acc_grad_boost_test]})\nmodels.sort_values(by='Test Score', ascending=False)","0d97bb0b":"test_data = pd.read_csv('..\/input\/test.csv')","22486c62":"test_data['is_male'] = np.where(test_data.Sex == 'male', 1,0)\ntest_data['Embarked_S'] = np.where(test_data.Embarked == 'S', 1,0)\ntest_data['Embarked_C'] = np.where(test_data.Embarked == 'C', 1,0)\n\ntest_data['FamilySize'] = test_data['SibSp'] + test_data['Parch'] + 1\n\ntest_data_DT = imputer_DT.transform(test_data[predictors])\ntest_data_DT = pd.DataFrame(test_data_DT, columns = predictors)\n\ntest_data_DT[['Age_scaled','Fare_scaled']] = scaler.transform(test_data_DT[['Age','Fare']])\n#test_data_DT[['Age_scaled','Fare_scaled']] = scaler.transform(test_data_DT[['Age','Fare']])\ntest_data_DT.drop(['Age_scaled','Fare_scaled'], axis = 1, inplace = True)","a021ec93":"x_train_full = pd.concat([X_train_DT,X_test_DT])\ny_train_full = pd.concat([y_train,y_test])\n\ngrad_boost.fit(x_train_full, y_train_full)","32c214cd":"Y_pred = grad_boost.predict(test_data_DT[X_test_DT.columns])","0dd79551":"Y_pred.mean()","c6a08550":"submission = pd.DataFrame({\n        \"PassengerId\": test_data[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\n#submission.to_csv('submission.csv', index=False)","a025e828":"sublist1 = ['Embarked_C',  'Parch', 'FamilySize']\n\ngrad_boost_submodel1 = GradientBoostingClassifier(random_state=0, max_depth= 4, min_weight_fraction_leaf= 0.001, \n                                        n_estimators= 300)\ngrad_boost_submodel1.fit(X_train_DT[sublist1], y_train)\nY_pred = grad_boost_submodel1.predict(X_test_DT[sublist1])\nacc_grad_boost_submodel1_train = round(grad_boost_submodel1.score(X_train_DT[sublist1], y_train) * 100, 2)\nacc_grad_boost_submodel1_test = round(grad_boost_submodel1.score(X_test_DT[sublist1], y_test) * 100, 2)\nacc_grad_boost_submodel1_test","a6770675":"print(acc_grad_boost_submodel1_train)\nprint(acc_grad_boost_submodel1_test)","ef60b698":"X_train_DT['pred_submodel1'] = grad_boost_submodel1.predict(X_train_DT[sublist1])\nX_test_DT['pred_submodel1'] = grad_boost_submodel1.predict(X_test_DT[sublist1])","2e255391":"grad_boost_stacked = GradientBoostingClassifier(random_state=0, max_depth= 4, min_weight_fraction_leaf= 0.01, \n                                        learning_rate=0.01, n_estimators= 700)\n\nfinal_list = ['Pclass', 'Age', 'SibSp', 'Embarked_S', 'Fare', 'is_male', 'FamilySize', 'pred_submodel1']\ngrad_boost_stacked.fit(X_train_DT[final_list], y_train)\nY_pred = grad_boost_stacked.predict(X_test_DT[final_list])\nacc_grad_boost_stacked_train = round(grad_boost_stacked.score(X_train_DT[final_list], y_train) * 100, 2)\nacc_grad_boost_stacked_test = round(grad_boost_stacked.score(X_test_DT[final_list], y_test) * 100, 2)","2ceaa441":"print(acc_grad_boost_stacked_train)\nprint(acc_grad_boost_stacked_test)","e503ef7f":"feature_imp = pd.DataFrame({'feature':X_train_DT[final_list].columns, 'importance':grad_boost_stacked.feature_importances_})\nfeature_imp.sort_values(by = 'importance', ascending = False)","20a94e11":"X_train_DT.columns","314576b8":"test_data_DT['pred_submodel1'] = grad_boost_submodel1.predict(test_data_DT[sublist1])","90fe0212":"Y_pred = grad_boost_stacked.predict(test_data_DT[final_list])","5082c07d":"submission = pd.DataFrame({\n        \"PassengerId\": test_data[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\nsubmission.to_csv('submission.csv', index=False)","e38d611b":"##### -With great help from the book Python Feature Engineering Cookbook by Soledad Galli","67f7abf4":"##### It is kind of difficult to have columns with more than 40% missing data to be a part of the final features but it is too early to decide on this one here","4f87abd0":"### Individual Column Wise Analysis","e75bdea3":"### Scaling","f7b0531a":"#### Checking Distribution of continuous variables since many algorithms assume normal distribution","ff5b8674":"## Missing Value Imputation","e6da69af":"#### Missing Rate","d5e7beae":"#### If a variable is normally distributed, its quantiles follow the quantiles of the theoretical distribution such that the dots of the variable values fall along the 45-degree line.","a58e514e":"## Stacking\n###### This is the first time I am using this technique called stacking where predictions from another model are taken in a features for the final model. I am using a few randomly selected but less significant features for it.","d1a1c74a":"## Data Exploration","997b3430":"### Identify Outliers\n\n#### boxplots and the inter-quartile range (IQR) proximity rule.\n##### According to the IQR proximity rule, a value is an outlier if it falls outside these boundaries:\n    Upper boundary = 75th quantile + (IQR * 1.5)\n    Lower boundary = 25th quantile - (IQR * 1.5)\nHere, IQR is given by the following equation:\nIQR = 75th quantile - 25th quantile","d56c9575":"#### Missing Count","66f7ed35":"#### These two didn't do better than the individual GBM model that I used.","38696f79":"### Basic Categorical Transformations","7f553176":"#### We have seen univariate imputation using mean, median, mode etc. Let's take a look at Multivariate imputation by chained equations(MICE) is a multiple imputation technique that models each variable with missing values as a function of the remaining variables and uses that estimate for imputation. MICE has the following basic steps:\n1. A simple univariate imputation is performed for every variable with missing data, for example, median imputation.\n2. One specific variable is selected, say, var_1, and the missing values are set back to missing.\n3. A model that\u2019s used to predict var_1 is built based on the remaining variables in the dataset.\n4. The missing values of var_1 are replaced with the new estimates.\n5. Repeat step 2 to step 4 for each of the remaining variables.\n\n#### Once all the variables have been modeled based on the rest, a cycle of imputation is concluded. Step 2 to step 4 are performed multiple times, typically 10 times, and the imputation values after each round are retained. The idea is that, by the end of the cycles, the distribution of the imputation parameters should have converged.","99ed6ff5":"##### Identifying Rare Categories","8b081065":"### Multivariate EDA","9367069f":"### When scaling variables to the median and quantiles, the median value is removed from the observations and the result is divided by the inter-quartile range (IQR). The IQR is the range between the 1st quartile and the 3rd quartile, or, in other words, the range between the 25th quantile and the 75th quantile.\n\n### This method is known as robust scaling because it produces more robust estimates for the center and value range of the variable, and is recommended if the data contains outliers.","7fdd74b8":"##### The Decision Tree imputer seemed to be working because I think KNN is getting close to an overfit which it is prone to a lot of times.","98ddd712":"#### Unique Count","3c5365b4":"### Correlation"}}