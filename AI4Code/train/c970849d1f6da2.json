{"cell_type":{"989de46c":"code","ebca438c":"code","3bda5f4a":"code","04f6cb5c":"code","3889b878":"code","76c63db2":"code","21dc7d8a":"markdown","47ebf34c":"markdown","5dd9b18e":"markdown","60fad92b":"markdown","37fa99b9":"markdown","6de60f04":"markdown","55dd6383":"markdown"},"source":{"989de46c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport fasttext\nimport re\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n\n# Any results you write to the current directory are saved as output.","ebca438c":"from pathlib import Path\n# metadata\ndata_dir = Path('\/kaggle\/input\/')\n\ndtypes = {'title': str, 'abstract': str, ' text': str}\ndf1 = pd.read_csv(data_dir\/'cord-19-eda-parse-json-and-generate-clean-csv\/biorxiv_clean.csv', dtype=dtypes)\ndf2 = pd.read_csv(data_dir\/'cord-19-eda-parse-json-and-generate-clean-csv\/clean_comm_use.csv',dtype=dtypes)\ndf3 = pd.read_csv(data_dir\/'cord-19-eda-parse-json-and-generate-clean-csv\/clean_noncomm_use.csv', dtype=dtypes)\ndf4 = pd.read_csv(data_dir\/'cord-19-eda-parse-json-and-generate-clean-csv\/clean_pmc.csv', dtype=dtypes)\n\nimport gc \n\nall_data = pd.concat([df1, df2, df3, df4])\ndel df1, df2, df3, df4\ngc.collect()\nall_data.tail()\n\n","3bda5f4a":"%%time\nfrom collections import Counter    \nfrom nltk.corpus import stopwords\n\n# remove special characters \ndef remove_special(x):\n    # new line at sentence end for training purposes\n    x = x.replace('.', '\\n')\n    return re.sub('[^A-Za-z \\n]+', ' ', x).lower()\n    \nstop_words = stopwords.words('english')\nstopwords_dict = Counter(stop_words)\ndef remove_stopwords(x):\n    return ' '.join([word for word in x.split() if word not in stopwords_dict])\n    \ntitle = all_data['title'][~all_data['title'].isna()].apply(remove_stopwords).apply(remove_special).values\ntext_body = all_data['text'][~all_data['text'].isna()].apply(remove_stopwords).apply(remove_special).values\nabstract = all_data['abstract'][~all_data['abstract'].isna()].apply(remove_stopwords).apply(remove_special).values\n\ncorpus = np.concatenate([title, text_body, abstract], axis=0)\n\nprint('Number of rows in corpus list: ', len(corpus))\n\n# convert to string for model training\ncorpus = \"\\n\".join(corpus)","04f6cb5c":"%%time\n\ntrain_file = 'covid_corpus.txt'\n\nwith open(train_file, 'w') as f:\n    f.write(corpus)\n    \n\"\"\"\nUncomment below to train. Training can take a 2-8 hours based on number of epochs\n\"\"\"\n# model_train = fasttext.train_unsupervised(input=newpath, epoch=5, model=\"skipgram\")\n# model_train.save_model(\"model_1.bin\")\n\n# using previously trained model \nmodel = fasttext.load_model(str(data_dir\/'trainedfasttext\/model_1.bin'))\n","3889b878":"\"\"\"\nUtility visualization functions \n\"\"\"\n\ndef get_words_from_indices(model, indices: list):\n    \"\"\"\n    Gets words from model vocab given the indices\n    \"\"\"\n    labels = np.array(model.get_labels())\n    \n    return labels[indices]\n\ndef get_embedding_from_indices(model, indices: list):\n    \"\"\"\n    Gets word embeddings of model given the indices\n    \"\"\"\n\n    return model.get_output_matrix()[indices, :]\n  \n\ndef find_kneighbours(embeddings_matrix, vector, n_neighbors=20):\n    \"\"\"\n    Finds k nearest neighbors of vector (1xN) in the (MxN) matrix. \n    \"\"\"\n    from sklearn.neighbors import NearestNeighbors\n\n    if len(vector.shape) == 1:\n        vector = vector.reshape(-1,1).T\n\n    assert vector.shape[1] == embeddings_matrix.shape[1]\n\n    nn = NearestNeighbors(n_neighbors=n_neighbors)\n    nn.fit(embeddings_matrix)\n    indices = nn.kneighbors(vector, return_distance=False)\n    \n    return indices.squeeze(0)\n\ndef get_tnse_embeddings(output_matrix):\n    from sklearn.manifold import TSNE \n\n    tnse = TSNE(n_components=3, perplexity=30.0, early_exaggeration=12.0, \n                            learning_rate=200.0, n_iter=500, n_iter_without_progress=100, \n                            min_grad_norm=1e-07, metric='euclidean', init='random', verbose=0, \n                            random_state=None, \n                            method='barnes_hut', angle=0.5)\n    embedding_matrix = tnse.fit_transform(output_matrix)\n    return embedding_matrix\n\ndef plot_tnse_embeddings(embedding_matrix, labels, color=None):\n    \"\"\"\n    input: (M x 3) and labels\n    output: plotly figure \n    \"\"\"\n    import plotly.io as pio\n    import plotly.graph_objs as go\n    import math\n    plot_mode = 'text+markers'\n    if not color:\n        color = \"#3266c1\"\n\n    if embedding_matrix.shape[1] == 3:\n        scatter = go.Scatter3d(\n            name='None',\n            x=embedding_matrix[:,0],\n            y=embedding_matrix[:,1],\n            z=embedding_matrix[:,2],\n            text=labels,\n            textposition=\"top center\",\n            showlegend=False,\n            mode=plot_mode,\n            marker=dict(size=5, color=color, symbol=\"circle\"),\n        )\n        figure = go.Figure(data=[scatter])\n        return figure\n    else:\n        raise ValueError('Unsupported dimensions: must be 3, not {}'.format(embedding_matrix.shape[1])) \n        \n    \ndef plot_tnse_aroud_word(model, word: str, n_neighbors=20):\n\n    \"\"\"\n    creates visualization of tnse showing the input word. \n    \"\"\"\n    if len(word.strip().split()) == 1:\n        vector = model.get_word_vector(word)\n    else:\n        vector = model.get_sentence_vector(word)\n\n    indices = find_kneighbours(model.get_output_matrix(), vector, n_neighbors=n_neighbors)\n    embedding_matrix = get_embedding_from_indices(model, indices)\n    print('Size of embedding matrix:', embedding_matrix.shape)\n    labels = get_words_from_indices(model, indices)\n    embeddings_with_target = np.concatenate((vector.reshape(-1,1).T, embedding_matrix),axis=0)\n    labels_with_target = [word] + list(labels) # appending to first as embedding also appened to first\n    tnse_embeddings_with_target = get_tnse_embeddings(embeddings_with_target)\n    \n    colors = np.ones(tnse_embeddings_with_target.shape[0])\n    colors[0] = 5\n\n    return plot_tnse_embeddings(tnse_embeddings_with_target, labels_with_target, color=colors.tolist())\n\n","76c63db2":"word= 'virus'\nplot_tnse_aroud_word(model, word, n_neighbors=20)              ","21dc7d8a":"# Next Steps: \nExplore: \n* Different data cleaning strategies\n* Training for more epochs\n* Different models eg. CBOW, skipgram\n* Start Mining ","47ebf34c":"# Language Model on COVID Research Papers\n* This notebook explores COVID related research papers through a custom skipgram language model trained on the corpus. \n* Analysing the word embeddings could help draw connections between words and the model can be used for information retrieval, to pull out certain topics, meanings from the documents\n\n","5dd9b18e":"# Loading Data","60fad92b":"# Creating text training file\n* Clean data - Remove special characters and stopwords\n* Can consider keeping special characters - Will be useful for retrieving percentages, dollars etc \n","37fa99b9":"# Util functions","6de60f04":"# Visualize embeddings\n* We can attempt to understand the language model by visualizing its embeddings that are around input words\n* Modify the input word to explore the model further\n* Eg. for the input word \"virus\", the closest embeddings within the language mdoel include * coronavirus, human, viral, viruses, infection * etc. However, there are many adverbs such as *finally, however, additionally, and similarily*, which makes sense, because these words occur frequently in the same context, given this paper is research oriented. One explore further cleaning up the text of these kinds words prior to training","55dd6383":"# Training fasttext model\n* This uses skipgram model, but you can choose between skipgram, CBOW etc. Since the skip-gram model is designed to predict the context, instead of predicting word by context, I went with this because we can then match a single\/few words to a document more effectively, because the embedding of the single word will be taking context into account. "}}