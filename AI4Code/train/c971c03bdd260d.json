{"cell_type":{"03e4ae3c":"code","46d1ddc1":"code","b6f22923":"code","dc601176":"code","8a7ede98":"code","e047a0b5":"code","1e61b876":"code","98ef7141":"markdown","53ca2b00":"markdown","053d8a8f":"markdown","54511950":"markdown","8f1e9267":"markdown","ad01522e":"markdown","3202c328":"markdown","10c57481":"markdown","7778010c":"markdown"},"source":{"03e4ae3c":"import numpy as np\nfrom sklearn import preprocessing\nimport tensorflow as tf","46d1ddc1":"# Extract\nraw_csv_data = np.loadtxt('..\/input\/audiobook-store-customer\/Audiobooks_data.csv',delimiter=',')\n\n\n# Shuffle\nshuffled_indices = np.arange(raw_csv_data.shape[0])\nnp.random.shuffle(shuffled_indices)\nshuffled_inputs = raw_csv_data[shuffled_indices]\n\n\n# Separate Input and Target\ninputs_all = shuffled_inputs[:,1:-1]\ntargets_all = shuffled_inputs[:,-1]\n\n\n# Balance data set by removing the excessive 0 targets as it will bias the learnig of model\none_targets_count = int(np.sum(targets_all))\nzero_targets_count = 0\nindices_to_remove = []\n\nfor i in range(targets_all.shape[0]):\n    if targets_all[i] == 0:\n        zero_targets_count += 1\n        if zero_targets_count > one_targets_count:\n            indices_to_remove.append(i)\n            \nbalanced_inputs_all = np.delete(inputs_all, indices_to_remove, axis=0)\nbalanced_targets_all = np.delete(targets_all, indices_to_remove, axis=0)\n\n\n# Standardize the input values\nstd_inputs = preprocessing.scale(balanced_inputs_all)\n\n\n# Shuffle again\nshuffled_indices = np.arange(inputs.shape[0])\nnp.random.shuffle(shuffled_indices)\n\ninputs = std_inputs[shuffled_indices]\ntargets = balanced_targets_all[shuffled_indices]","b6f22923":"# Total samples\nsamples = inputs.shape[0]\n\n# Split count\ntrain_samples_count = int(0.8 * samples)\nvalidation_samples_count = int(0.1 * samples)\ntest_samples_count = samples - train_samples_count - validation_samples_count\n\n# Creating train set\ntrain_inputs = inputs[:train_samples_count]\ntrain_targets = targets[:train_samples_count]\n\n# Creating validation set\nvalidation_inputs = inputs[train_samples_count:train_samples_count+validation_samples_count]\nvalidation_targets = targets[train_samples_count:train_samples_count+validation_samples_count]\n\n# Creating test set\ntest_inputs = inputs[train_samples_count+validation_samples_count:]\ntest_targets = targets[train_samples_count+validation_samples_count:]\n\n# Print the number of targets that are 1s, the total number of samples, and the proportion for training, validation, and test.\nprint(\"---Train---\")\nprint(np.sum(train_targets), train_samples_count, np.sum(train_targets) \/ train_samples_count)\nprint(\"---Validation---\")\nprint(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) \/ validation_samples_count)\nprint(\"---Test---\")\nprint(np.sum(test_targets), test_samples_count, np.sum(test_targets) \/ test_samples_count)","dc601176":"np.savez('.\/Audiobooks_data_train', inputs=train_inputs, targets=train_targets)\nnp.savez('.\/Audiobooks_data_validation', inputs=validation_inputs, targets=validation_targets)\nnp.savez('.\/Audiobooks_data_test', inputs=test_inputs, targets=test_targets)","8a7ede98":"# Train set\nnpz = np.load('Audiobooks_data_train.npz')\ntrain_inputs, train_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n\n# Validation set\nnpz = np.load('Audiobooks_data_validation.npz')\nvalidation_inputs, validation_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)\n\n# Test set\nnpz = np.load('Audiobooks_data_test.npz')\ntest_inputs, test_targets = npz['inputs'].astype(np.float), npz['targets'].astype(np.int)","e047a0b5":"input_size = 10\noutput_size = 2\n\n# Configuring the NN values. Hidden layers = 2\nhidden_layer_size = 50\nbatch_size = 120\nmax_epochs = 100\n\n# Early stopping mechanism with 2 patience level. Which means the model will continue learing until the error has been minimized \n# and cannot minimize further. The model will stop learning after 2 (patience level) such instances\nearly_stopping = tf.keras.callbacks.EarlyStopping(patience=2)\n        \nmodel = tf.keras.Sequential([\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer    \n    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n])\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nmodel.fit(train_inputs, \n          train_targets, \n          batch_size=batch_size, \n          epochs=max_epochs, \n          callbacks=[early_stopping], \n          validation_data=(validation_inputs, validation_targets), \n          verbose = 2 \n          )  ","1e61b876":"test_loss, test_accuracy = model.evaluate(test_inputs, test_targets,verbose=0)\nprint('\\nTest loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))","98ef7141":"# Model ","53ca2b00":"# Audiobooks Customer Return Prediction","053d8a8f":"# Testing the model","54511950":"## Preprocessing\n\n1. Shuffling the data to remove any time bias.\n2. Balancing the target data as 50\/50 to remove the learning bias.\n3. Standardizing the inputs\n4. Shuffling again to make sure train, validation and test data set are unbiased\n\nThere are 10 input fields except ID column and 1 Target field.","8f1e9267":"# Create *.npz for model","ad01522e":"# Information about Input data\n\nThe data in CSV is without headers and some level of pre-processing is already done. \nFor instance, the missing values of rating field are filled with the mean value (8.91)\n\nThe columns are as follows (in order)\n\n1. Customer ID\n2. Total minutes of all the audio books purchased\n3. Average minutes of all the audio books purchased\n4. Total price\n5. Average price\n6. Review ( 0 = Did not submit review, 1 = Submitted review )\n7. Review out of 10\n8. Minutes listened\n9. Completion percent\n10. Support requests made\n11. ( Last visited date - Purchase date )\n12. Customer returned to buy new audiobook (Target value) ( 1 = Customer returned, 0 = Customer did not return )","3202c328":"# Load npz files","10c57481":"## Imports","7778010c":"# Split the dataset into Train, Validation, and Test sets\n\nDataset is split in 80\/10\/10 parts. "}}