{"cell_type":{"8d85ab14":"code","3cd7e9cd":"code","185d27b0":"code","326df1c4":"code","becad84c":"code","fed18d1e":"code","ebfe4d1c":"code","faf1ce5e":"code","314ce9b3":"code","86cea646":"code","511bfffb":"code","7eff1a26":"code","4ac31ec0":"code","b16cc7e5":"code","ebe31d93":"code","90526498":"code","8b6e2c7f":"code","28983519":"code","80407fb7":"code","f4803439":"code","bd0471eb":"code","5e785dee":"code","a3160e8a":"code","e9414149":"code","4bb649ba":"code","0844948c":"code","8e46690e":"code","c36b1df3":"code","646810a4":"code","110e2521":"code","fe85270c":"code","68847627":"code","28da9176":"code","c5f00453":"code","61d54c00":"code","d80c8376":"code","2d26e7e8":"code","9cb55cba":"code","636fb46a":"code","d9b70673":"code","156d5188":"code","41b56394":"code","8bbb4a33":"markdown","8f579ba4":"markdown","e22cd8ee":"markdown","1986a6fe":"markdown","3ffa8f41":"markdown","0c27a733":"markdown","ece5afdf":"markdown","58c6474b":"markdown","89b96a6e":"markdown","9896d206":"markdown","c44f462e":"markdown","bbe10be8":"markdown","24ddd91c":"markdown"},"source":{"8d85ab14":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom tsfresh import extract_features\n","3cd7e9cd":"dataPath = \"\/kaggle\/input\/m5-forecasting-accuracy\/\"\ntimesteps = 14\nstartDay = 350","185d27b0":"# To run the evaluation set the parameter to \"\/sales_train_evaluation.csv\"\ndt = pd.read_csv(dataPath + \"\/sales_train_validation.csv\")\ndt.head(3)\n","326df1c4":"print(dt.info())","becad84c":"#To reduce memory usage\ndef downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df","fed18d1e":"#Reduce memory usage and compare with the previous one to be sure\ndt = downcast_dtypes(dt)","ebfe4d1c":"# print(dt.info())","faf1ce5e":"#Take the transpose so that we have one day for each row, and 30490 items' sales as columns\ndt= dt.T    \ndt.head(8)","314ce9b3":"#Remove id, item_id, dept_id, cat_id, store_id, state_id columns\ndt= dt[6 + startDay:]\ndt.head(5)","86cea646":"#Reshape the dataframe in order to be usable by tsfresh\ndf_new = pd.melt(dt.reset_index(), id_vars='index')\ndf_new.columns = ['day', 'product', 'sales']\n\n#Delete dt to save some memory\n#del dt","511bfffb":"# dt.head(5)","7eff1a26":"#remove \"d_\" from days\ndf_new['day'] = df_new['day'].str.lstrip('d_')\n\n#data frame Object to Int\ndf_new = df_new.astype('int32')\ndf_new.dtypes\n","4ac31ec0":"#Extract features for each product and put results in data frame extracted_features\n#fc_parameters contains all the features+parameters we want to extract from the sales data\n\nfc_parameters = {\n#     \"count_below\": [{\"t\":1.0}], # zero counts\n#     \"linear_trend\": [{\"attr\": \"pvalue\"}], # linear least-square regression (?)\n    \"cid_ce\": [{\"normalize\": True }],# Complexity of time series\n#     \"autocorrelation\": [{\"lag\": 1}],\n#     \"partial_autocorrelation\": [{\"lag\": 1}],\n}\n\nextracted_features = extract_features(df_new, column_id=\"product\", column_sort=\"day\", default_fc_parameters=fc_parameters)\nextracted_features = extracted_features.fillna(0)\n","b16cc7e5":"#creater clusters based on features\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters=3, random_state=0).fit(extracted_features)\n\n","ebe31d93":"labels = kmeans.labels_\nresults = pd.DataFrame(labels)\nresults.columns = ['cluster']\nresults.head(100000)\nresults['cluster'].value_counts()","90526498":"#Create dataframes containing only products for one cluster\ndroplist_cluster0 = results.index[results['cluster'] == 0].tolist()\ndroplist_cluster1 = results.index[results['cluster'] == 1].tolist()\ndroplist_cluster2 = results.index[results['cluster'] == 2].tolist()\n\ndt2 = dt.copy()\ndt2.head(100)\ndt_cluster0 = dt2.drop(dt2.columns.difference(droplist_cluster0),axis=1,inplace=False)\ndt_cluster1 = dt2.drop(dt2.columns.difference(droplist_cluster1),axis=1,inplace=False)\ndt_cluster2 = dt2.drop(dt2.columns.difference(droplist_cluster2),axis=1,inplace=False)\ndt_cluster1.head(100)","8b6e2c7f":"#Load train and calendar data\ncalendar = pd.read_csv(dataPath + \"\/calendar.csv\")\n","28983519":"#Create dataframe with zeros for 1969 days in the calendar\ndaysBeforeEvent = pd.DataFrame(np.zeros((1969,1)))\nisWeekend = pd.DataFrame(np.zeros((1969,1)))\nisWinter = pd.DataFrame(np.zeros((1969,1)))\nisSpring = pd.DataFrame(np.zeros((1969,1)))\nisSummer = pd.DataFrame(np.zeros((1969,1)))\nisAutumn = pd.DataFrame(np.zeros((1969,1)))\n","80407fb7":"# \"1\" is assigned to the days before the event_name_1. Since \"event_name_2\" is rare, it was not added.\nfor x,y in calendar.iterrows():\n    if((pd.isnull(calendar[\"event_name_1\"][x])) == False):\n        daysBeforeEvent[0][x-1] = 1 \n    if(calendar[\"weekday\"][x] == \"Friday\" or calendar[\"weekday\"][x] == \"Saturday\" or calendar[\"weekday\"][x] == \"Sunday\"):\n        isWeekend[0][x] = 1\n    if(calendar[\"month\"][x] in [12, 1, 2]):\n        isWinter[0][x] = 1\n    if(calendar[\"month\"][x] in [3, 4, 5]):\n        isSpring[0][x] = 1\n    if(calendar[\"month\"][x] in [6, 7, 8]):\n        isSummer[0][x] = 1\n    if(calendar[\"month\"][x] in [9, 10, 11]):\n        isAutumn[0][x] = 1\n    \n        #if first day was an event this row will cause an exception because \"x-1\".\n        #Since it is not i did not consider for now.\n\n   ","f4803439":"#\"calendar\" won't be used anymore. \ndel calendar","bd0471eb":"# To be able to run the evaluation set, different timeperiods need to be specified. 1913:1941 needs to be changed to 1941:1969 and startDay:1913 needs to be changed to startDay:1941.\n\n#\"daysBeforeEventTest\" will be used as input for predicting (We will forecast the days 1913-1941).\ndaysBeforeEventTest = daysBeforeEvent[1913:1941]\n#\"daysBeforeEvent\" will be used for training as a feature.\ndaysBeforeEvent = daysBeforeEvent[startDay:1913]\n\n#\"isWeekendTest\" will be used as input for predicting (We will forecast the days 1913-1941).\nisWeekendTest = isWeekend[1913:1941]\n#\"isWeekend\" will be used for training as a feature.\nisWeekend = isWeekend[startDay:1913]\n\n#'isWinterTest'will be used as input for predicting (We will forecast the days 1913-1941).\nisWinterTest = isWinter[1913:1941]\n#\"isWinter\" will be used for training as a feature.\nisWinter = isWinter[startDay:1913]\n\n\n#'iSpringTest' will be used as input for predicting (We will forecast the days 1913-1941).\nisSpringTest = isSpring[1913:1941]\n#\"isSpring\" will be used for training as a feature.\nisSpring = isSpring[startDay:1913]\n\n\n#'isSummerTest' will be used as input for predicting (We will forecast the days 1913-1941).\nisSummerTest = isSummer[1913:1941]\n#\"isSummer\" will be used for training as a feature.\nisSummer = isSummer[startDay:1913]\n\n\n# 'isAutumnTest' will be used as input for predicting (We will forecast the days 1913-1941).\nisAutumnTest = isAutumn[1913:1941]\n#\"isAutumn\" will be used for training as a feature.\nisAutumn = isAutumn[startDay:1913]","5e785dee":"#Before concatanation with our main data \"dt\", indexes are made the same.\ndaysBeforeEvent.columns = [\"oneDayBeforeEvent\"]\ndaysBeforeEvent.index = dt.index\n\nisWeekend.columns = ['Weekend']\nisWeekend.index = dt.index\n\nisWinter.columns = ['Winter']\nisWinter.index = dt.index\n\nisSpring.columns = ['Spring']\nisSpring.index = dt.index\n\nisSummer.columns = ['Summer']\nisSummer.index = dt.index\n\nisAutumn.columns = ['Autumn']\nisAutumn.index = dt.index\n","a3160e8a":"dt_cluster0 = pd.concat([dt_cluster0, daysBeforeEvent, isWeekend, isWinter, isSpring, isSummer, isAutumn], axis = 1)\ndt_cluster1 = pd.concat([dt_cluster1, daysBeforeEvent, isWeekend, isWinter, isSpring, isSummer, isAutumn], axis = 1)\ndt_cluster2 = pd.concat([dt_cluster2, daysBeforeEvent, isWeekend, isWinter, isSpring, isSummer, isAutumn], axis = 1)","e9414149":"#Feature Scaling\n#Scale the features using min-max scaler in range 0-1\nfrom sklearn.preprocessing import MinMaxScaler\nsc1 = MinMaxScaler(feature_range = (0, 1))\nsc2 = MinMaxScaler(feature_range = (0, 1))\nsc3 = MinMaxScaler(feature_range = (0, 1))\ndt_scaled_0 = sc1.fit_transform(dt_cluster0)\ndt_scaled_1 = sc2.fit_transform(dt_cluster1)\ndt_scaled_2 = sc3.fit_transform(dt_cluster2)","4bb649ba":"x_train_0 = []\nx_train_1 = []\nx_train_2 = []\n\ny_train_0 = []\ny_train_1 = []\ny_train_2 = []\n\n# To be able to run evaluation 1913 need to be set to 1941.\nfor i in range(timesteps, 1913 - startDay):\n    x_train_0.append(dt_scaled_0[i-timesteps:i])\n    y_train_0.append(dt_scaled_0[i][0:len(dt_cluster0.columns)-6]) \n    \n    x_train_1.append(dt_scaled_1[i-timesteps:i])\n    y_train_1.append(dt_scaled_1[i][0:len(dt_cluster1.columns)-6])\n    \n    x_train_2.append(dt_scaled_2[i-timesteps:i])\n    y_train_2.append(dt_scaled_2[i][0:len(dt_cluster2.columns)-6])","0844948c":"#Convert to np array to be able to feed the LSTM model\nX_train_0 = np.array(x_train_0)\nX_train_1 = np.array(x_train_1)\nX_train_2 = np.array(x_train_2)\ny_train_0 = np.array(y_train_0)\ny_train_1 = np.array(y_train_1)\ny_train_2 = np.array(y_train_2)\n\nprint(X_train_0.shape)\nprint(y_train_0.shape)","8e46690e":"# Model 1 for products being classified as 0.\n\n# Importing the Keras libraries and packages\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\n\n# Initialising the RNN\nregressor0 = Sequential()\n\n# Adding the first LSTM layer and some Dropout regularisation\nlayer_1_units=50\nregressor0.add(LSTM(units = layer_1_units, return_sequences = True, input_shape = (X_train_0.shape[1], X_train_0.shape[2])))\nregressor0.add(Dropout(0.2))\n\n# Adding a second LSTM layer and some Dropout regularisation\nlayer_2_units=400\nregressor0.add(LSTM(units = layer_2_units, return_sequences = True))\nregressor0.add(Dropout(0.2))\n\n# Adding a third LSTM layer and some Dropout regularisation\nlayer_3_units=400\nregressor0.add(LSTM(units = layer_3_units))\nregressor0.add(Dropout(0.2))\n\n# Adding the output layer\nregressor0.add(Dense(units = len(dt_cluster0.columns)-6))\n\n# Compiling the RNN\nregressor0.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n# Fitting the RNN to the Training set\nepoch_no=32\nbatch_size_RNN=44\nregressor0.fit(X_train_0, y_train_0, epochs = epoch_no, batch_size = batch_size_RNN)\n\n","c36b1df3":"from keras.utils.vis_utils import plot_model\n\n# Make image of model\nplot_model(regressor0, to_file='regressor0_plot.png', show_shapes=True, show_layer_names=True)","646810a4":"# Model 2 for product classified as 1.\n\n# Initialising the RNN\nregressor1 = Sequential()\n\n# Adding the first LSTM layer and some Dropout regularisation\nlayer_1_units=50\nregressor1.add(LSTM(units = layer_1_units, return_sequences = True, input_shape = (X_train_1.shape[1], X_train_1.shape[2])))\nregressor1.add(Dropout(0.2))\n\n# Adding a second LSTM layer and some Dropout regularisation\nlayer_2_units=400\nregressor1.add(LSTM(units = layer_2_units, return_sequences = True))\nregressor1.add(Dropout(0.2))\n\n# Adding a third LSTM layer and some Dropout regularisation\nlayer_3_units=400\nregressor1.add(LSTM(units = layer_3_units))\nregressor1.add(Dropout(0.2))\n\n# Adding the output layer\nregressor1.add(Dense(units = len(dt_cluster1.columns)-6))\n\n# Compiling the RNN\nregressor1.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n# Fitting the RNN to the Training set\nepoch_no=32\nbatch_size_RNN=44\nregressor1.fit(X_train_1, y_train_1, epochs = epoch_no, batch_size = batch_size_RNN)","110e2521":"# Model 3 for products classified as 2.\n\n# Initialising the RNN\nregressor2 = Sequential()\n\n# Adding the first LSTM layer and some Dropout regularisation\nlayer_1_units=50\nregressor2.add(LSTM(units = layer_1_units, return_sequences = True, input_shape = (X_train_2.shape[1], X_train_2.shape[2])))\nregressor2.add(Dropout(0.2))\n\n# Adding a second LSTM layer and some Dropout regularisation\nlayer_2_units=400\nregressor2.add(LSTM(units = layer_2_units, return_sequences = True))\nregressor2.add(Dropout(0.2))\n\n# Adding a third LSTM layer and some Dropout regularisation\nlayer_3_units=400\nregressor2.add(LSTM(units = layer_3_units))\nregressor2.add(Dropout(0.2))\n\n# Adding the output layer\nregressor2.add(Dense(units = len(dt_cluster2.columns)-6))\n\n# Compiling the RNN\nregressor2.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n# Fitting the RNN to the Training set\nepoch_no=32\nbatch_size_RNN=44\nregressor2.fit(X_train_2, y_train_2, epochs = epoch_no, batch_size = batch_size_RNN)","fe85270c":"inputs0= dt_cluster0[-timesteps:]\ninputs0 = sc1.transform(inputs0)\n\ninputs1= dt_cluster1[-timesteps:]\ninputs1 = sc2.transform(inputs1)\n\ninputs2= dt_cluster2[-timesteps:]\ninputs2 = sc3.transform(inputs2)","68847627":"# Make predictions for the first cluster\n# To be able to run the evaluation data set, 1913 needs to be changed to 1941.\n\nX_test0 = []\nX_test0.append(inputs0[0:timesteps])\nX_test0 = np.array(X_test0)\npredictions0 = []\n\nfor j in range(timesteps,timesteps + 28):\n    predicted_stock_price = regressor0.predict(X_test0[0,j - timesteps:j].reshape(1, timesteps, len(dt_cluster0.columns)))\n    \n    testInputEvent = np.column_stack((np.array(predicted_stock_price), daysBeforeEventTest[0][1913 + j - timesteps]))\n    testInputWeekend = np.column_stack((testInputEvent, isWeekendTest[0][1913 + j - timesteps]))\n    testInputWinter = np.column_stack((testInputWeekend, isWinterTest[0][1913 + j - timesteps]))\n    testInputSpring = np.column_stack((testInputWinter, isSpringTest[0][1913 + j - timesteps]))\n    testInputSummer = np.column_stack((testInputSpring, isSummerTest[0][1913 + j - timesteps]))\n    testInput = np.column_stack((testInputSummer, isAutumnTest[0][1913 + j - timesteps]))\n    \n    X_test0 = np.append(X_test0, testInput).reshape(1,j + 1,len(dt_cluster0.columns))\n    predicted_stock_price = sc1.inverse_transform(testInput)[:,0:len(dt_cluster0.columns)-6]\n    predictions0.append(predicted_stock_price)","28da9176":"# Make predictions for the second cluster\n# To be able to run the evaluation data set, 1913 needs to be changed to 1941.\n\nX_test1 = []\nX_test1.append(inputs1[0:timesteps])\nX_test1 = np.array(X_test1)\npredictions1 = []\n\nfor j in range(timesteps,timesteps + 28):\n    predicted_stock_price = regressor1.predict(X_test1[0,j - timesteps:j].reshape(1, timesteps, len(dt_cluster1.columns)))\n    \n    testInputEvent = np.column_stack((np.array(predicted_stock_price), daysBeforeEventTest[0][1913 + j - timesteps]))\n    testInputWeekend = np.column_stack((testInputEvent, isWeekendTest[0][1913 + j - timesteps]))\n    testInputWinter = np.column_stack((testInputWeekend, isWinterTest[0][1913 + j - timesteps]))\n    testInputSpring = np.column_stack((testInputWinter, isSpringTest[0][1913 + j - timesteps]))\n    testInputSummer = np.column_stack((testInputSpring, isSummerTest[0][1913 + j - timesteps]))\n    testInput = np.column_stack((testInputSummer, isAutumnTest[0][1913 + j - timesteps]))\n    \n    X_test1 = np.append(X_test1, testInput).reshape(1,j + 1,len(dt_cluster1.columns))\n    predicted_stock_price = sc2.inverse_transform(testInput)[:,0:len(dt_cluster1.columns)-6]\n    predictions1.append(predicted_stock_price)","c5f00453":"# Make predictions for the third cluster\n# To be able to run the evaluation data set, 1913 needs to be changed to 1941.\n\nX_test2 = []\nX_test2.append(inputs2[0:timesteps])\nX_test2 = np.array(X_test2)\npredictions2 = []\n\nfor j in range(timesteps,timesteps + 28):\n    predicted_stock_price = regressor2.predict(X_test2[0,j - timesteps:j].reshape(1, timesteps, len(dt_cluster2.columns)))\n    \n    testInputEvent = np.column_stack((np.array(predicted_stock_price), daysBeforeEventTest[0][1913 + j - timesteps]))\n    testInputWeekend = np.column_stack((testInputEvent, isWeekendTest[0][1913 + j - timesteps]))\n    testInputWinter = np.column_stack((testInputWeekend, isWinterTest[0][1913 + j - timesteps]))\n    testInputSpring = np.column_stack((testInputWinter, isSpringTest[0][1913 + j - timesteps]))\n    testInputSummer = np.column_stack((testInputSpring, isSummerTest[0][1913 + j - timesteps]))\n    testInput = np.column_stack((testInputSummer, isAutumnTest[0][1913 + j - timesteps]))\n    X_test2 = np.append(X_test2, testInput).reshape(1,j + 1,len(dt_cluster2.columns))\n    \n    predicted_stock_price = sc3.inverse_transform(testInput)[:,0:len(dt_cluster2.columns)-6]\n    predictions2.append(predicted_stock_price)","61d54c00":"# Transform lists of predictions to dataframes\npred0 = pd.DataFrame(np.array(predictions0).reshape(28, len(predictions0[0][0])), columns= [dt_cluster0.columns[0:len(dt_cluster0.columns)-6]])\npred1 = pd.DataFrame(np.array(predictions1).reshape(28, len(predictions1[0][0])), columns= [dt_cluster1.columns[0:len(dt_cluster1.columns)-6]])\npred2 = pd.DataFrame(np.array(predictions2).reshape(28, len(predictions2[0][0])), columns=[dt_cluster2.columns[0:len(dt_cluster2.columns)-6]])\n\n# Concatenate the dataframes together and place them in the right order\nsubmission = pd.concat([pred0,pred1,pred2], axis=1).sort_index(axis=1, level=None, ascending=True)","d80c8376":"submission.head(10)","2d26e7e8":"# When we run the evaluation data set, the manually computed score cannot be computer, therefore this cell needs to be commented out.\n\n# Read in the correct sales\ntrue_sales = pd.read_csv(dataPath + \"\/sales_train_evaluation.csv\")\n\n# Transform data\ntrue_sales = true_sales.T[6:][-28:]\ntrue_sales = true_sales.astype(float)\nsubmission.index = true_sales.index\nsubmission.columns = true_sales.columns\n\n# Get mean sales for each day\ntrue_sales_day = true_sales.mean(axis=1)\nsubmission_day = submission.mean(axis=1)\n","9cb55cba":"# When we run the evaluation data set, the manually computed score cannot be computer, therefore this cell needs to be commented out.\n\n# Make column\ntrue_sales_day.columns = ['mean']\nsubmission_day.columns = ['mean']","636fb46a":"# When we run the evaluation data set, the manually computed score cannot be computer, therefore this cell needs to be commented out.\n\nfrom sklearn.metrics import mean_squared_error\n\nscore_mse = np.sqrt(mean_squared_error(true_sales, submission))\nscore_mse_day = np.sqrt(mean_squared_error(true_sales_day, submission_day))\n\nprint(score_mse)\nprint(score_mse_day)","d9b70673":"# # When we run the evaluation data set, we need to save eval_submission.pk1\n# submission.to_pickle('val_submission.pkl')","156d5188":"# # When both validation and evaluation are run, we can download the pickle files and give them as input for this notebook in order to make the final prediction. \n\n# dataPath = \"\/kaggle\/input\/eval-submission\/\"\n\n\n# eval_submission = pd.read_pickle(dataPath + 'eval_submission.pkl')\n# val_submission = pd.read_pickle('val_submission.pkl')","41b56394":"# # Use for submission\n\n# import time\n\n# dataPath = \"\/kaggle\/input\/m5-forecasting-accuracy\/\"\n\n# eval_submission = eval_submission.T\n# val_submission = val_submission.T\n\n# submission = pd.concat((val_submission, eval_submission), ignore_index=True)\n\n# sample_submission = pd.read_csv(dataPath + \"\/sample_submission.csv\")\n    \n# idColumn = sample_submission[[\"id\"]]\n    \n# submission[[\"id\"]] = idColumn  \n\n# cols = list(submission.columns)\n# cols = cols[-1:] + cols[:-1]\n# submission = submission[cols]\n\n# colsdeneme = [\"id\"] + [f\"F{i}\" for i in range (1,29)]\n\n# submission.columns = colsdeneme\n\n# currentDateTime = time.strftime(\"%d%m%Y_%H%M%S\")\n\n# submission.to_csv(\"submission.csv\", index=False)","8bbb4a33":"* Since, the \"daysBeforeEvent\" feature is used for predicting after the model trained as input, we seperate the 28 days as \"daysBeforeEventTest\"\n* For training the first 1914 days (if \"startDay\" is zero otherwise \"1913-startDay\") will be used.","8f579ba4":"# LSTM Models with Keras","e22cd8ee":"* Concatenate \"daysBeforeEvent\", \"isWeekend\",\"isWinter\",\"isSpring\", \"isSummer\" and \"isAutumn\" features with our main dataframe \"dt\".","1986a6fe":"* Take last days, 14 for this notebook (\"timestep\" parameter) in order to predict firts unknown day's sales.\n* Before using values for prediction, again use min-max transformation","3ffa8f41":"* Here a dataframe is created to store the knowledge if an event exist in the next day\n* Firstly, fill with zeros the dataframe","0c27a733":"* The shape of the data is not exactly what we want.\n* We want to have each day as row and 30490 items' sales as columns (features)\n* Therefore take the transpose of \"dt\"","ece5afdf":"* Here is the important part. \"X_train\" and \"y_train\" data is created. For each X_train item, 14 past days' sales and 14 daysBeforeEvent feature are included. So one element of X_train's shape is (14, 30491). For y_train we are predicting one day sales of 30490 items therefore one element of y_train's shape is (1, 30490)","58c6474b":"# Data Prepration for LSTM\n\n* In this notebook LSTM is used for forecasting future sales. \n* The type of the LSTM can be regarded as Multivariate and Multiple Input Series (Multi-step is not used).\n* The model is trained using past sales values for each 30490 item and a feature which represents if there exists an event at the following day.  \n* For this notebook \"sales of past 14 days and event feature\" are used for predicting 15th day sales. Past days is a parameter which can be set to see the effect. ","89b96a6e":"![resim.png](attachment:resim.png)","9896d206":"* Below timesteps is set to 14 to use past 14 days' sales.\n* Since there are lots of zero values in first days, \"startDay\" parameter can be used ignore unwanted days from the beginning.","c44f462e":"* Remove the first six  colums id, item_id, dept_id, cat_id, store_id, state_id columns, to end up only days as rows","bbe10be8":"* Now, \"1\" is assigned the day before an event exist. Other days will remain as \"0\".","24ddd91c":"# Submission File Creation\n* Here, the submission file creation is done."}}