{"cell_type":{"f5dae34b":"code","5a03cd4c":"code","b6f0b07d":"code","5e8d4bcb":"code","18cc1a46":"code","8f2a3c50":"code","7fb555aa":"code","e5b79fca":"code","125a761d":"code","35b8cd50":"code","6db7eff8":"code","99b008c9":"code","eb1d3d29":"code","07c7994b":"code","67e48b8d":"code","bd0bc6f7":"code","7972ceb5":"code","18a13091":"code","71aaebe4":"code","adccc240":"code","e1ed999c":"code","0286393c":"code","897b8846":"code","fd8acbfc":"code","c58f2fb6":"code","5f656d84":"code","e8238ae9":"code","582f5dd2":"code","465b5182":"code","502024c5":"code","a307a436":"code","a87fa500":"code","41ea94b8":"code","35321349":"markdown","98470723":"markdown","3545f7bb":"markdown","eeb9b942":"markdown","2cb77ff1":"markdown","01063e7a":"markdown","041707e1":"markdown","9944e0ca":"markdown","cdd51e48":"markdown","f6accd1e":"markdown","720dd202":"markdown","07d96119":"markdown","82a43a10":"markdown","53f66f07":"markdown","3abcfa76":"markdown","46beff7b":"markdown","6f684e9a":"markdown","e6d6dd5c":"markdown"},"source":{"f5dae34b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nimport tensorflow as tf\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom gensim.models import word2vec\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.linear_model import LogisticRegression","5a03cd4c":"dd = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',encoding = \"ISO-8859-1\")\ndd.columns = ['sentiment','id','date','query','special','text']\ndd.head()","b6f0b07d":"dd.drop(['id','date','query','special'],axis = 1,inplace = True)","5e8d4bcb":"df = dd.sample(100000)\n","18cc1a46":"df['Cleaned'] = df['text'].str.replace('@','')\ndf['Cleaned'] = df['Cleaned'].str.replace(r'http\\S+','')\ndf['Cleaned'] = df['Cleaned'].str.replace('[^a-zA-Z]',' ')","8f2a3c50":"stopwords = stopwords.words('english')","7fb555aa":"def remove_stopwords(text):\n    clean_text=' '.join([word for word in text.split() if word not in stopwords])\n    return clean_text","e5b79fca":"df['Cleaned'] = df['Cleaned'].apply(lambda text : remove_stopwords(text.lower()))\ndf['Cleaned'] = df['Cleaned'].apply(lambda x : x.split())","125a761d":"df.head()","35b8cd50":"sns.countplot(df.sentiment)","6db7eff8":"wordnet=WordNetLemmatizer()\ndf['Cleaned'] = df['Cleaned'].apply(lambda x : [wordnet.lemmatize(i) for i in x])","99b008c9":"df['Cleaned'] = df['Cleaned'].apply(lambda x : ' '.join([w for w in x]))","eb1d3d29":"df['Cleaned'] = df['Cleaned'].apply(lambda x : ' '.join([w for w in x.split()]))","07c7994b":"df.head()","67e48b8d":"cv = CountVectorizer(max_features = 2500)\nx = cv.fit_transform(df['Cleaned']).toarray()\nx.shape","bd0bc6f7":"x_train,x_test,y_train,y_test = train_test_split(x,df['sentiment'],test_size = 0.2,random_state = 42)","7972ceb5":"%%time\nmodel = RandomForestClassifier()\nmodel.fit(x_train,y_train)","18a13091":"model.score(x_train,y_train)","71aaebe4":"model.score(x_test,y_test)","adccc240":"%%time\nreg = LogisticRegression()\nreg.fit(x_train,y_train)","e1ed999c":"reg.score(x_train,y_train)","0286393c":"reg.score(x_test,y_test)","897b8846":"tf = TfidfVectorizer(max_features = 2500)\nz = tf.fit_transform(df['Cleaned']).toarray()\nz.shape","fd8acbfc":"z_train,z_test,y_train,y_test = train_test_split(z,df['sentiment'],test_size = 0.2,random_state = 42)","c58f2fb6":"%%time\nmodel1 = RandomForestClassifier()\nmodel1.fit(z_train,y_train)","5f656d84":"model1.score(z_train,y_train)","e8238ae9":"model1.score(z_test,y_test)","582f5dd2":"%%time\nreg1 = LogisticRegression()\nreg1.fit(z_train,y_train)","465b5182":"reg1.score(z_train,y_train)","502024c5":"reg1.score(z_test,y_test)","a307a436":"scores = pd.DataFrame({'Bow(RF)': model.score(x_test,y_test),\n                       'Bow(LR)': reg.score(x_test,y_test),\n                       'TF(RF)': model1.score(z_test,y_test),\n                       'TF(LR)': reg1.score(z_test,y_test)},\n                      index = [0])","a87fa500":"scores","41ea94b8":"scores.T.plot(kind = 'bar')","35321349":"* Taking a sample from whole data set coz original data set is very big","98470723":"# In This Notebook I Will Try To Create 2 Basic Models\n* Tf-Idf\n* Bag Of Words\n","3545f7bb":"# Plotting all the scores","eeb9b942":"\n# Bag of Words\nBag of Words (BOW) is a method to extract features from text documents. ... It creates a vocabulary of all the unique words occurring in all        the documents in the training set. In simple terms, it's a collection of words to represent a sentence with word count and mostly disregarding      the order in which they appea","2cb77ff1":"# Loading The Data Set\n* Assigning column names","01063e7a":"# Log Regression\n","041707e1":"# Plz Upvote if you like this notebook.","9944e0ca":"# Random Forest","cdd51e48":"# Model Building\n* Random Forest\n* Logistic Regression","f6accd1e":"# Lemmatization\n* Lemmatization is the process of grouping together the different inflected forms of a word so they can be analysed as a single item.\n* Using Lemmatization instead of Stemming bcoz when we use stemming it changes the words in a way where it losses its original meaning (eg. Studies   => studi). On the Other hand in Lemmatization original meaning of the word does not changes.","720dd202":"# Libraries","07d96119":"# Tf-Idf","82a43a10":"# Random Forest","53f66f07":"# Bag of Words","3abcfa76":"# Log Regression","46beff7b":"# Tf-Idf\nTF-IDF is a statistical measure that evaluates how relevant a word is to a document in a collection of documents. This is done by multiplying      two metrics: how many times a word appears in a document, and the inverse document frequency of the word across a set of documents.\n","6f684e9a":"# Dropping Few Columns\n* As they are not required","e6d6dd5c":"# Basic Preprocessing\n* Removing unwanted elements like '@' , ',' as they dont add any value.\n* (Stopwords)Removing the words which does not add any meaning."}}