{"cell_type":{"a0e51e8b":"code","30f9dc11":"code","dd523b2a":"code","d845fc99":"code","306790c2":"code","65f6c42d":"code","4f08247b":"code","09e4b81e":"code","d2fb1156":"code","8ba0b54d":"code","d00f0528":"code","6b394e92":"code","2308f401":"markdown"},"source":{"a0e51e8b":"import pandas as pd\nfrom pathlib import Path\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import roc_auc_score\nfrom catboost import Pool, CatBoostClassifier\nfrom tqdm import tqdm\nroot = Path(\"..\/input\")","30f9dc11":"train = pd.read_csv(root.joinpath(\"train.csv\"))\ntest = pd.read_csv(root.joinpath(\"test.csv\"))","dd523b2a":"train_df = train.copy()\ntest_df = test.copy()\ntrain_df.drop(columns=[\"ID_code\", \"target\"], inplace=True)\ntest_df.drop(columns=[\"ID_code\"], inplace=True)\ntarget = train.target","d845fc99":" def augment_train(df_train, y_train):   \n    t0 = df_train[y_train == 0].copy()\n    t1 = df_train[y_train == 1].copy()\n    i = 0\n    N = 3\n    for I in range(0):  # augment data into 2x\n        for col in df_train.columns:\n            i = i + 1000\n            np.random.seed(i)\n            np.random.shuffle(t0[col].values)\n            np.random.shuffle(t1[col].values)\n        df_train = pd.concat([df_train, t0.copy()])\n        df_train = pd.concat([df_train, t1.copy()])\n        y_train = pd.concat([y_train, pd.Series([0] * t0.shape[0]), pd.Series([1] * t1.shape[0])])\n    return df_train, y_train","306790c2":"model = CatBoostClassifier(subsample=0.36, #rawdata 0.5  \u00d72 0.45 \u00d73 0.36\n                            custom_loss='Logloss',\n                           random_strength = 0,\n                           max_depth=3,\n                           eval_metric=\"AUC\",\n                           learning_rate=0.02,\n                           iterations=60000,\n                           #class_weights=[1,2],\n                           bootstrap_type='Bernoulli',\n                           #rsm=0.045,\n                            l2_leaf_reg=0.3,\n                           task_type=\"GPU\",\n                           random_seed=432013,\n                           od_type=\"Iter\",\n                           border_count=128\n                           #has_time= True \n                          )","65f6c42d":" def run_cat(model,  trt, tst, tar,n_splits=5, plot=False):   \n    kf = KFold(n_splits=n_splits, random_state=432013, shuffle=True)\n    oof = np.zeros(len(trt))\n    feature_importance_df = pd.DataFrame()\n    y_valid_pred = 0 * tar\n    y_test_pred = 0\n    for n_fold, (train_index, valid_index) in enumerate(kf.split(trt, tar)):\n        y_train, y_valid = tar.iloc[train_index], tar.iloc[valid_index]\n        X_train, X_valid = trt.iloc[train_index,:], trt.iloc[valid_index,:]\n        X_train, y_train = augment_train(X_train, y_train)\n        X_train,X_valid = generate_fe(trn=X_train,tst=X_valid)\n        _train = Pool(X_train, label=y_train)\n        _valid = Pool(X_valid, label=y_valid)\n        print( \"Fold \", n_fold)\n        fit_model = model.fit(_train,\n                              verbose_eval=1000, \n                              early_stopping_rounds=1000,\n                              eval_set=[_valid],\n                              use_best_model=True,\n                              plot=False,\n                                            \n                             )\n        pred = fit_model.predict_proba(X_valid)[:,1]\n        oof[valid_index] = pred\n        print( \"auc = \", roc_auc_score(y_valid, pred) )\n        y_valid_pred.iloc[valid_index] = pred\n        y_test_pred += fit_model.predict_proba(test_fe)[:,1]\n    y_test_pred \/= n_splits\n    print(\"average auc:\", roc_auc_score(tar, oof))\n    return y_test_pred, oof","4f08247b":"def generate_fe(trn, tst):\n    #tst,target=augment_train(tst,y_train=target)\n    real,syn = detect_test(test_df[features])\n    al = pd.concat([trn,tst,test_df.iloc[real]],axis=0)\n    for c in features:\n        trn[c+\"_test\"]=trn[c].map(al[c].value_counts())\n        trn[c+\"_test\"] = trn[c+\"_test\"]*trn[c]\n        tst[c+\"_test\"]=tst[c].map(al[c].value_counts())\n        tst[c+\"_test\"] = tst[c+\"_test\"]*tst[c]\n    return trn, tst","09e4b81e":"features = [c for c in train_df.columns if c not in [\"ID_code\",\"target\"]]","d2fb1156":"def detect_test(test_df):\n    df_test=test_df.values\n    unique_count = np.zeros_like(df_test)\n    for feature in tqdm(range(df_test.shape[1])):\n        _, index_, count_ = np.unique(df_test[:, feature], return_counts=True, return_index=True)\n        unique_count[index_[count_ == 1], feature] += 1\n\n    # Samples which have unique values are real the others are fake\n    real_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\n    synthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n    return real_samples_indexes,synthetic_samples_indexes","8ba0b54d":"def generate_fe_test(tst):\n    re,sy =  detect_test(tst[features])\n    al = pd.concat([train_df,test_df.iloc[re]],axis=0)\n    for c in features:\n        tst[c+\"_test\"]=tst[c].map(al[c].value_counts())\n        tst[c+\"_test\"] = tst[c+\"_test\"]*tst[c]\n    return tst\ntest_fe = generate_fe_test(test_df[features])","d00f0528":"y_test_pred, oof = run_cat(model,train_df, test_df, target)#0.5619","6b394e92":"submission = pd.read_csv(root.joinpath(\"sample_submission.csv\"))\nsubmission['target'] = y_test_pred\npd.Series(oof).to_csv(\"Cat_oof.csv\", index = False)\nsubmission.to_csv('submission_cb_light_0.8999.csv', index=False)","2308f401":"0.1\uff1a "}}