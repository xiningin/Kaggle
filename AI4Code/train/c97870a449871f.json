{"cell_type":{"97b7e7e4":"code","159de184":"code","fd9e885d":"code","0082bede":"code","836dac1b":"code","b639b1fb":"code","b978c04b":"code","71acf204":"code","9e248925":"code","b3c6e6ab":"code","5e7ff6e6":"code","e6c42007":"code","67b506f9":"code","874c59bc":"code","67420199":"code","488b71fe":"code","782ed1ec":"code","6779b81f":"code","07246d5f":"code","c453fa30":"code","e0c03289":"code","5dc1b91f":"markdown","780012bc":"markdown","0fb09951":"markdown","182a62da":"markdown","0d25784e":"markdown","7242c9d5":"markdown","8088c7be":"markdown","6154fe21":"markdown","447cb697":"markdown","6a283415":"markdown","e93a5bdb":"markdown","9894b540":"markdown","ad79ecf2":"markdown","32d15643":"markdown","0b4fa417":"markdown","f9a0f14c":"markdown","40b11987":"markdown","7e5c997f":"markdown","947254b8":"markdown","95bc093d":"markdown","703f13f6":"markdown"},"source":{"97b7e7e4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom torchvision.datasets import ImageFolder\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport tensorflow as tf\n\n\nif torch.cuda.is_available():  \n    dev = \"cuda:0\" \nelse:  \n    dev = \"cpu\"  \ndevice = torch.device(dev)\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","159de184":"class ColorizationDataset(Dataset):\n    def __init__(self, image_size, cuda=False):\n        self.image_size = image_size\n        self.cuda = cuda\n    def __len__(self):\n        return len(os.listdir('\/kaggle\/input\/anime-faces\/data\/data\/'))\n\n    def __getitem__(self, idx):\n        with Image.open(f'\/kaggle\/input\/anime-faces\/data\/data\/{str(idx+1)}.png') as image:\n            img = image.copy()\n            \n        compose_transforms = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n        ])\n\n        Y = compose_transforms(img)\n        X = (Y[0] * 0.299 + Y[1] * 0.587 + Y[2] * 0.114).reshape(1, 64, 64)\n        if self.cuda:\n            X = X.to(device)\n            Y = Y.to(device)\n        return X, Y","fd9e885d":"dset = ColorizationDataset(64)\nbatch_size = 32\ndataset, _ = torch.utils.data.random_split(dset, [int(len(dset)\/4), len(dset)-int(len(dset)\/4)])\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)","0082bede":"def to_numpy_image(img):\n    img = img.cpu()\n    return img.detach().transpose(0, 1).transpose(1, 2).numpy()","836dac1b":"x,y = dset[1]\n\nplt.imshow(x.reshape(64, 64,1), cmap='Greys_r')\nplt.title(\"Input\")\nplt.axis('off')\nplt.show()\nplt.title(\"Expected output\")\nplt.axis('off')\nplt.imshow(to_numpy_image(y))\nplt.show()","b639b1fb":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Input(shape=(64,64,1)))\n\nmodel.add(tf.keras.layers.Convolution2D(64, (3, 3), activation='relu',padding='same'))\nmodel.add(tf.keras.layers.Convolution2D(64, (3, 3), activation='relu',padding='same'))\nmodel.add(tf.keras.layers.MaxPooling2D((2,2), strides=None,padding='same'))\n\nmodel.add(tf.keras.layers.Convolution2D(32, (3, 3), activation='relu',padding='same'))\nmodel.add(tf.keras.layers.Convolution2D(32, (3, 3), activation='relu',padding='same'))\nmodel.add(tf.keras.layers.MaxPooling2D((2,2), strides=None,padding='same'))\n\nmodel.add(tf.keras.layers.Convolution2D(16, (3, 3), activation='relu',padding='same'))\nmodel.add(tf.keras.layers.Convolution2D(16, (3, 3), activation='relu',padding='same'))\nmodel.add(tf.keras.layers.MaxPooling2D((2,2), strides=None,padding='same'))\n\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(12288, activation='relu')) #64*64*3=12288\n\nmodel.summary()\n\nmodel.compile(loss='MSE',optimizer='adam',metrics=[\"accuracy\"])","b978c04b":"X_train = [dset[i][0].numpy().reshape(64,64,1) for i in range(int(len(dset)\/2))]\nY_train = [dset[i][1].numpy().reshape(12288) for i in range(int(len(dset)\/2))]\nX_valid = [dset[i][0].numpy().reshape(64,64,1) for i in range(int(len(dset)\/2)+1,len(dset)-4)]\nY_valid = [dset[i][1].numpy().reshape(12288) for i in range(int(len(dset)\/2)+1,len(dset)-4)]","71acf204":"X_train = np.array(X_train)\nY_train = np.array(Y_train)\nX_valid = np.array(X_valid)\nY_valid = np.array(Y_valid)\n\nhistory = model.fit(X_train,Y_train,batch_size=64,epochs=10, validation_data=(X_valid,Y_valid))","9e248925":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\n\nplt.figure()\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","b3c6e6ab":"for i in range(1,4):\n    x,y = dset.__getitem__(len(dset)-i)\n    x = x.numpy().reshape(-1,64,64,1)\n    plt.figure(figsize=(10,10))\n    plt.subplot(141)\n    plt.axis('off')\n    if i==1:\n        plt.title('Input')\n    plt.imshow(x.reshape(64, 64,1), cmap='Greys_r')\n    y_pred = torch.from_numpy(model.predict(x))\n    aff=(y_pred.reshape(3, 64, 64)).transpose(0, 1).transpose(1, 2).numpy()\n    plt.subplot(142)\n    plt.imshow(aff)\n    if i==1:\n        plt.title('Output')\n    plt.axis('off')\n    plt.subplot(143)\n    if i==1:\n        plt.title('Expected output')\n    plt.axis('off')\n    plt.imshow((y.reshape(3, 64, 64)).transpose(0, 1).transpose(1, 2).numpy())\n    plt.show()","5e7ff6e6":"model = tf.keras.Sequential()\nmodel.add(tf.keras.layers.Input(shape=(64,64,1)))\n\n#contracting path\nmodel.add(tf.keras.layers.Convolution2D(64, (3, 3), activation='relu'))\nmodel.add(tf.keras.layers.Convolution2D(64, (3, 3), activation='relu'))\nmodel.add(tf.keras.layers.MaxPooling2D((2,2), padding='same'))\n\nmodel.add(tf.keras.layers.Convolution2D(128, (3, 3), activation='relu'))\nmodel.add(tf.keras.layers.Convolution2D(128, (3, 3), activation='relu'))\nmodel.add(tf.keras.layers.MaxPooling2D((2,2), padding='same'))\n\n#expansive path\nmodel.add(tf.keras.layers.Convolution2D(256, (3, 3), activation='relu'))\nmodel.add(tf.keras.layers.Convolution2D(256, (3, 3), activation='relu'))\nmodel.add(tf.keras.layers.Conv2DTranspose(256, (2, 2), strides=(2,2), padding='valid'))\n\nmodel.add(tf.keras.layers.Convolution2D(128, (3, 3), padding='same'))\nmodel.add(tf.keras.layers.Convolution2D(128, (3, 3), padding='same'))\nmodel.add(tf.keras.layers.Conv2DTranspose(128, (2, 2), strides=(2,2), padding='valid'))\n\nmodel.add(tf.keras.layers.Convolution2D(64, (3, 3)))\nmodel.add(tf.keras.layers.Convolution2D(64, (3, 3)))\nmodel.add(tf.keras.layers.Conv2DTranspose(64, (2, 2), strides=(2,2), padding='valid'))\n\nmodel.add(tf.keras.layers.Convolution2D(3, (1, 1), padding='same'))\n\n\nmodel.summary()\n\nmodel.compile(loss='MSE',optimizer='adam',metrics=[\"accuracy\"])","e6c42007":"X_train = [dset[i][0].numpy().reshape(64,64,1) for i in range(int(3*len(dset)\/4))]\nY_train = [dset[i][1].numpy().reshape(64,64,3) for i in range(int(3*len(dset)\/4))]\nX_valid = [dset[i][0].numpy().reshape(64,64,1) for i in range(int(3*len(dset)\/4)+1,len(dset)-4)]\nY_valid = [dset[i][1].numpy().reshape(64,64,3) for i in range(int(3*len(dset)\/4)+1,len(dset)-4)]","67b506f9":"X_train = np.array(X_train)\nY_train= np.array(Y_train)\nX_valid = np.array(X_valid)\nY_valid = np.array(Y_valid)\n\nhistory = model.fit(X_train,Y_train,batch_size=64,epochs=30, validation_data=(X_valid,Y_valid))","874c59bc":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\nplt.plot(epochs, acc, 'r', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\n\n\nplt.figure()\nplt.plot(epochs, loss, 'r', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","67420199":"for i in range(1,4):\n    x,y = dset.__getitem__(len(dset)-i)\n    x = x.numpy().reshape(-1,64,64,1)\n    plt.figure(figsize=(10,10))\n    plt.subplot(141)\n    plt.axis('off')\n    if i==1:\n        plt.title('Input')\n    plt.imshow(x.reshape(64, 64,1), cmap='Greys_r')\n    y_pred = torch.from_numpy(model.predict(x))\n    aff=(y_pred.reshape(3, 64, 64)).transpose(0, 1).transpose(1, 2).numpy()\n    plt.subplot(142)\n    plt.imshow(aff)\n    if i==1:\n        plt.title('Output')\n    plt.axis('off')\n    plt.subplot(143)\n    if i==1:\n        plt.title('Expected Output')\n    plt.axis('off')\n    plt.imshow((y.reshape(3, 64, 64)).transpose(0, 1).transpose(1, 2).numpy())\n    plt.show()","488b71fe":"dset = ColorizationDataset(64, cuda=True) #USE THE GPU\ndataset, _ = torch.utils.data.random_split(dset, [int(len(dset)\/4), len(dset)-int(len(dset)\/4)])\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)","782ed1ec":"class Colorizer(nn.Module):\n\n    def __init__(self):\n        super(Colorizer, self).__init__()\n        self.conv1 = nn.Conv2d(1, 16, 3, 2, 1)\n        self.conv2 = nn.Conv2d(16, 64, 3, 2, 1)\n        self.conv3 = nn.Conv2d(64, 256, 3, 2, 1)\n        self.conv4 = nn.Conv2d(256, 256, 3, 1, 2, dilation=2)\n        self.conv5 = nn.Conv2d(256, 256, 3, 1, 2, dilation=2)\n        self.conv6 = nn.ConvTranspose2d(256, 128, 3, 2, 1, 1)\n        self.conv7 = nn.ConvTranspose2d(128, 64, 3, 2, 1, 1)\n        self.conv8 = nn.ConvTranspose2d(64, 3, 3, 2, 1, 1)\n        self.bn1 = nn.BatchNorm2d(16)\n        self.bn2 = nn.BatchNorm2d(64)\n        self.bn3 = nn.BatchNorm2d(256)\n        self.bn4 = nn.BatchNorm2d(256)\n        self.bn5 = nn.BatchNorm2d(256)\n        self.bn6 = nn.BatchNorm2d(128)\n        self.bn7 = nn.BatchNorm2d(64)\n        self.leakyrelu = nn.LeakyReLU(0.2)\n        self.tanh = nn.Tanh()\n    \n    def forward(self, x):\n        y = self.bn1(self.leakyrelu(self.conv1(x)))\n        y = self.bn2(self.leakyrelu(self.conv2(y)))\n        y = self.bn3(self.leakyrelu(self.conv3(y)))\n        y = self.bn4(self.leakyrelu(self.conv4(y)))\n        y = self.bn5(self.leakyrelu(self.conv5(y)))\n        y = self.bn6(self.leakyrelu(self.conv6(y)))\n        y = self.bn7(self.leakyrelu(self.conv7(y)))\n        y = self.tanh(self.conv8(y))\n        return y","6779b81f":"model = Colorizer()\n\nif torch.cuda.is_available(): \n    model.to(device)","07246d5f":"num_epochs = 5 #RAM eater\nlr = 1e-3\n\n\noptimizer = torch.optim.Adam(model.parameters(), lr=lr)\ncriterion = nn.MSELoss()\n\nhistory = []\nfor epoch in range(num_epochs):\n    for x, y in tqdm(dataloader):\n        optimizer.zero_grad()\n        output = model(x)\n        loss = criterion(output, y)\n        history.append(loss)\n        loss.backward()\n        optimizer.step()","c453fa30":"plt.plot(history)\nplt.title('Loss')\nplt.show()","e0c03289":"for i in range(3):\n    rng = np.random.randint(len(dset))\n    img_gray, img_true = dset[rng]\n    img_pred = model(img_gray.reshape(1, 1, 64, 64))\n    img_pred = to_numpy_image(img_pred.reshape(3, 64, 64))\n    plt.figure(figsize=(10,10))\n    plt.subplot(141)\n    if i==0:\n        plt.title('Input')\n    plt.axis('off')\n    plt.imshow(1-to_numpy_image(img_gray).reshape((64, 64)), cmap= 'Greys')\n    plt.subplot(142)\n    if i==0:\n        plt.title('Output')\n    plt.axis('off')\n    plt.imshow(img_pred)\n    plt.subplot(143)\n    if i==0:\n        plt.title('Expected Output')\n    plt.axis('off')\n    plt.imshow(to_numpy_image(img_true))\n    plt.show()","5dc1b91f":"## 2.2. Simple CNN results","780012bc":"## 2.1. Training the model","0fb09951":"# 1. Introduction","182a62da":"## 3.1. Training the model","0d25784e":"Also, it seems that the network has better chance to reduce its loss by fully coloring eyes in black.","7242c9d5":"## 3.2. Results","8088c7be":"## 4.1. Training the model","6154fe21":"As you can see, the up-con 2x2 are supposed to *double* the size of the images. Taking the formula of the relation between input and output shape for an upconv layer : <br\/> \n$out = (in \u2212 1)s - 2p + d(k-1) + padd + 1\n$with $d=1$ being the dilation ; $padd=0$ being the padding; $k=2$ being the kernel size <br\/> \nWe get $s=2$. That's why we have strides=(2,2).","447cb697":"The purpose of this notebook is to create Convolutionnal Neural Network in Pytorch to colorize anime faces from the dataset Anime Faces by Mckinsey666. In order to this, we will use a similar model than the U-Net by Ronneberger and al : https:\/\/arxiv.org\/abs\/1505.04597.","6a283415":"## 4.2. Pytorch model results","e93a5bdb":"The particularity of the U-Net is that it concatenates the data from the first path into the second symmetrically. We will not do such things.  \n![image.png](attachment:image.png)","9894b540":"It seems that the colorization isn't really convincing, according to the plot of the loss more epochs couldn't do the trick. Furthermore, we can see artefacts on some of the generated images. To me, the main reason fot this not working properly is that I'm not using the good architecture. In addition, the manga characters have a wide variety of hair colors. So the program has a better chance of reducing its loss by taking an \"average\" color that appears to be brown.","ad79ecf2":"*Extended work direction :* Using a subclass of Model rather than using Sequential would have allowed to use the concatenation layers in order to avoid the blurry images in the tensorflow model.","32d15643":"*Note : I recommend you to use a GPU for faster results as we work on images.*","0b4fa417":"# 3. U-NET Architecture with TensorFlow","f9a0f14c":"# 4. Different U-NET Architecture Pytorch","40b11987":"# 5. Conclusion","7e5c997f":"## 1.1. Imports","947254b8":"## 1.2. Classes and look at the dataset\n","95bc093d":"This network has two symmetrical parts. The *contracting path* which is a typical convolutionnal neural network reducing the size of images and increasing the number of feature maps with convolutions and max pooling. This is followed by the *expansive path* which uses up convolutionnal layers (transposed convolutionnal layers) and upsampling to do the opposite.\n","703f13f6":"\n# 2. Typical CNN"}}