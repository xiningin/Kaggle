{"cell_type":{"89193fcd":"code","73bcf5fc":"code","70d9d623":"code","92e5dadc":"code","52f860d2":"code","146afb9c":"code","2b380a43":"code","74ca2981":"code","36ea08e8":"code","39ffc316":"code","1209c541":"code","e7a927ba":"markdown"},"source":{"89193fcd":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport re\nfrom tqdm.notebook import tqdm\nimport math\nimport sklearn\ntqdm.pandas()","73bcf5fc":"from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\n\ndef lemmatize_sentence(sentence):\n    return \" \".join([lemmatizer.lemmatize(t) for t in sentence.split()])\n\ndef simple_clean(text):\n    text = text.lower()\n    text = re.sub(r\"[^a-z ]+\", \" \", text)\n    return lemmatize_sentence(text)\ndef encode_sentiment(text):\n    if text == \"positive\":\n        return 1\n    elif text == \"negative\":\n        return 0\n    else:\n        print(\"error\")\ndef decode_sentiment(number):\n    if number == 1:\n        return \"positive\"\n    elif number == 0:\n        return \"negative\"","70d9d623":"df = pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")","92e5dadc":"df[\"sentiment_value\"] = df[\"sentiment\"].apply(encode_sentiment)\ndf[\"review_clean\"] = df[\"review\"].progress_apply(simple_clean)","52f860d2":"def tokenize(text):\n    tokens = []\n    one_gram = text.split(\" \")\n    for i in range(len(one_gram)-1):\n        tokens.append(one_gram[i]+\" \"+one_gram[i+1])\n    tokens += one_gram\n    return tokens\n\ndef hash_trick(tokens,dim=512):\n    arr = np.zeros((dim))\n    for t in tokens:\n        h = sklearn.utils.murmurhash3_32(t)\n        if h >= 0:\n            arr[h%dim] += 1\n        else:\n            arr[abs(h)%dim] -= 1\n    return arr\n\ndef preprocess(batch_text,dim=512):\n    return np.concatenate([np.expand_dims(hash_trick(tokenize(t),dim),0) for t in batch_text],axis=0)","146afb9c":"X_train,X_test,y_train,y_test = train_test_split(df[\"review_clean\"].values,df[\"sentiment_value\"].values, test_size=0.2, random_state=42)","2b380a43":"def sigmoid(x):\n    x = np.where(x >= 0, \n                    1 \/ (1 + np.exp(-x)), \n                    np.exp(x) \/ (1 + np.exp(x)))\n    return np.clip(x,0,1)\n\ndef loss(y_prob,y_true):\n    y_prob = np.clip(y_prob,0.0000001,0.9999999)\n    return -np.mean(y_true*(np.log(y_prob)) + (1-y_true)*np.log(1-y_prob))\n\nclass LogRegModel:\n    def __init__(self,size_w):\n        self.w = np.random.randn(size_w)\n        self.b = 0\n        self.mw = np.zeros_like(self.w)\n        self.mb = 0\n    \n    def forward(self,x):\n        return sigmoid(x.dot(self.w) + self.b)\n    \n    def update(self,x,y_true,lr,reg=0.1):\n        \"\"\"no autograd here so need to pass in true and predicted values\"\"\"\n        bs = y_true.shape[0]\n        y_prob = self.forward(x)\n        batch_loss = loss(y_prob,y_true)\n        dw = 1\/bs * (x.T.dot(y_prob-y_true) + reg*self.w)\n        db = 1\/bs * np.sum(y_prob-y_true)\n        # momentum\n        self.mw = 0.9*self.mw + 0.1*dw\n        self.mb = 0.9*self.mb + 0.1*db\n        \n        self.w -= lr*self.mw\n        self.b -= lr*self.mb\n        return batch_loss","74ca2981":"class AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","36ea08e8":"epochs = 20\nbatch_size = 100\ndim = 16384\nmodel = LogRegModel(dim)\nlr = 1\navg_meter = AverageMeter()\n\nnum_batches = math.ceil(X_train.shape[0]\/batch_size)\nfor epoch in range(epochs):    \n    tk = tqdm(range(num_batches))\n    total_loss = 0\n    perm = np.arange(X_train.shape[0])\n    np.random.shuffle(perm)\n    shuffled_x = X_train[perm]\n    shuffled_y = y_train[perm]\n    avg_meter.reset()\n    # Training\n    for i in tk:\n        x_batch = shuffled_x[i*batch_size:(i+1)*batch_size]\n        x_batch = preprocess(x_batch,dim)\n        y_batch = shuffled_y[i*batch_size:(i+1)*batch_size]\n        \n        batch_loss = model.update(x_batch,y_batch,lr)\n        avg_meter.update(batch_loss,x_batch.shape[0])\n        total_loss += batch_loss\n        tk.set_postfix({'loss':avg_meter.avg})\n    # validation\n    y_prob = model.forward(preprocess(X_test,dim))\n    val_loss = loss(y_prob,y_test)\n    val_acc = ((y_prob>0.5) == y_test).mean().item()\n    print(f\"Epoch: {epoch} Training Loss:{total_loss\/num_batches} validation_loss:{val_loss} validation accuracy:{val_acc}\")\n    \n    if epoch%5==0:\n        lr *= 0.5","39ffc316":"y_pred = model.forward(preprocess(X_test,dim)) > 0.5","1209c541":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred))","e7a927ba":"# Logistic Regression from scratch with SGD + Hashing trick"}}