{"cell_type":{"9a7468a7":"code","faabb545":"code","ef5a0978":"code","c5dc4f0a":"code","86cefc58":"code","e7589595":"code","51d5579b":"code","0029727a":"code","7a48118a":"code","6d8b31c8":"code","57828738":"code","8408f507":"code","c82c818f":"code","9b2af1c3":"code","f95d291d":"code","263a4bd7":"code","b273ce63":"markdown","a0352140":"markdown","78b2a829":"markdown"},"source":{"9a7468a7":"import gc\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_absolute_error\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.layers import Bidirectional, LSTM\nfrom tensorflow.keras.layers import Dense, Dropout, Input\nfrom tensorflow.keras.layers import Concatenate, Add, GRU\n","faabb545":"#Modify according to the seeds you set\nnp.random.seed(42)\ntf.random.set_seed(42)","ef5a0978":"train_df = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')\nprint(f\"train_df: {train_df.shape}\")\ntrain_df.head()\n\ntest_df = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')\nprint(f\"test_df: {test_df.shape}\")\ntest_df.head()","c5dc4f0a":"all_pressure = sorted(train_df.pressure.unique())\nPRESSURE_MIN = all_pressure[0].item()\nPRESSURE_MAX = all_pressure[-1].item()\nPRESSURE_STEP = ( all_pressure[1] - all_pressure[0] ).item()","86cefc58":"def prepare_set(df): #CV 0.1579\n    \n    np.random.seed(42)\n    df['noise'] = np.random.choice([0, PRESSURE_STEP, -PRESSURE_STEP], len(df))\n    \n    df['flow'] = np.sqrt(2*df['u_in'])\n    \n    df['u_in_log'] = np.log(df['u_in']).replace(-np.inf,0)\n        \n    #df['area'] = df['time_step'] * df['u_in']\n    #df['area'] = df.groupby('breath_id')['area'].cumsum()\n    df['state'] = np.array([1 if x>0 else 0 for x in df['u_in']]) - df['u_out']\n    \n    #df['exhale'] = df.groupby('breath_id')['u_out'].cumsum()\n        \n    df['delta_time'] = df.groupby('breath_id')['time_step'].diff().fillna(0)\n    df['delta_u_in'] = df.groupby(df['breath_id'])['u_in'].diff().fillna(0).reset_index(level=0,drop=True)     \n    df['delta_flow'] = df.groupby(df['breath_id'])['flow'].diff().fillna(0).reset_index(level=0,drop=True)          \n    \n    df['inhale_time'] = df['state'] *  df['delta_time'] * (1 - df['u_out'])\n    df['inhale_time'] = (df.groupby(df['breath_id'])['inhale_time']).cumsum()  * (1 - df['u_out'])\n\n    df['flow_1st_der'] = (df['delta_flow'] \/df['delta_time']).fillna(0)\n    df['u_in_cumsum'] = (df['u_in']).groupby(df['breath_id']).cumsum()\n    df['flow_cumsum'] = (df['flow']).groupby(df['breath_id']).cumsum()\n    \n    df['u_in_lag2'] = df.groupby('breath_id')['u_in'].shift(2).fillna(0).reset_index(level=0,drop=True)\n    df['u_in_lag4'] = df.groupby('breath_id')['u_in'].shift(4).fillna(0).reset_index(level=0,drop=True)\n\n    df['u_in_lag-1'] = df.groupby('breath_id')['u_in'].shift(-1).fillna(0).reset_index(level=0,drop=True)\n    \n    df['u_in_dif2'] = df['u_in'] - df['u_in_lag2']\n    df['u_in_dif4'] = df['u_in'] - df['u_in_lag4']\n    df['u_in_dif-1'] = df['u_in'] - df['u_in_lag-1']\n    \n    df['volume_mean']= df['flow'] * df['delta_time']\n\n       \n    df['volume_in_cumsum']=df.groupby('breath_id')['volume_mean'].cumsum()     \n    df['volume_in_cumsum_reverse']=df.groupby(df['breath_id'])['volume_in_cumsum'].transform('max')  - df['volume_in_cumsum']\n        \n    df['_volume'] = df['volume_in_cumsum'] * (1 - df['u_out'])\n    df['tidal_volume']=df.groupby(df['breath_id'])['_volume'].transform('max')\n    \n    df['volume_part'] = (df['volume_mean']\/df['volume_in_cumsum']).fillna(0)\n    df['volume_part'] = df.groupby('breath_id')['volume_part'].shift(-1).fillna(0).reset_index(level=0,drop=True)\n    \n    df['time_constant'] = df.groupby(df['breath_id'])['inhale_time'].transform('max')\n    df['V_dot'] = df['tidal_volume'] \/ df['time_constant']\n    \n\n    df['u_in_rol_q0.25'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1, center=True).quantile(0.25).reset_index(level=0,drop=True)\n    df['u_in_rol_q0.75'] = df.groupby(df['breath_id'])['u_in'].rolling(window=10, min_periods=1, center=True).quantile(0.75).reset_index(level=0,drop=True)\n\n    \n    df['dP_on_R'] = df['flow'] * df['R']\/1000 \n    df['lung_expand'] = df['dP_on_R'] * df['C'] \n    df['dP_on_C'] = df['flow'] * df['delta_time'] * 1000 \/ df['C'] \n    df['dP_on_C_cumsum'] = df.groupby('breath_id')['dP_on_C'].cumsum()   \n    \n    df['RC'] = df['R'].astype(str) + df['C'].astype(str)\n    df['R'] = df['R'].astype(str)\n    df['C'] = df['C'].astype(str)\n    df = pd.get_dummies(df)\n\n    df['u_in_1st_order_grad'] = np.stack(df.groupby(df['breath_id'])['u_in'].apply(np.gradient).values).reshape(len(df),)\n    \n\n    df = df.fillna(0)\n    \n    return df\n\ntrain = prepare_set(train_df)\ntest = prepare_set(test_df)\n\ndel train_df, test_df\ngc.collect()","e7589595":"targets = train[['pressure']].to_numpy().reshape(-1, 80)\nu_outs = train[['u_out']].to_numpy().reshape(-1, 80)\n\ntrain.drop(['pressure', 'id', 'breath_id'], axis = 1, inplace = True)\ntest = test.drop(['id', 'breath_id'], axis = 1)\n\nprint(f\"train: {train.shape}\")","51d5579b":"scaler = RobustScaler()\ntrain = scaler.fit_transform(train)\ntest = scaler.transform(test)\n\ntrain = train.reshape(-1, 80, train.shape[-1])\ntest = test.reshape(-1, 80, train.shape[-1])\n\nprint(f\"train: {train.shape} \\n targets: {targets.shape}\")","0029727a":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    BATCH_SIZE = strategy.num_replicas_in_sync * 64\n    print(\"Running on TPU:\", tpu.master())\n    print(f\"Batch Size: {BATCH_SIZE}\")\n    \nexcept ValueError:\n    strategy = tf.distribute.get_strategy()\n    BATCH_SIZE = 512\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    print(f\"Batch Size: {BATCH_SIZE}\")","7a48118a":"def GBVPP_loss(y_true, y_pred, cols = 80):\n    u_out = y_true[:, cols: ]\n    y = y_true[:, :cols ]\n\n    w = 1 - u_out\n    mae = w * tf.abs(y - y_pred)\n    return tf.reduce_sum(mae, axis=-1) \/ tf.reduce_sum(w, axis=-1)\n   \ndef get_model():  \n    x_input = keras.Input(shape=(train.shape[-2:]))\n    \n    x1 = layers.Bidirectional(layers.LSTM(units=768, return_sequences=True))(x_input)\n    x2 = layers.Bidirectional(layers.LSTM(units=512, return_sequences=True))(x1)\n    x3 = layers.Bidirectional(layers.LSTM(units=384, return_sequences=True))(x2)\n    x4 = layers.Bidirectional(layers.LSTM(units=256, return_sequences=True))(x3)\n    x5 = layers.Bidirectional(layers.LSTM(units=128, return_sequences=True))(x4)\n    \n    z2 = layers.Bidirectional(layers.GRU(units=384, return_sequences=True))(x2)\n    \n    z31 = layers.Multiply()([x3, z2])\n    z31 = layers.BatchNormalization()(z31)\n    z3 = layers.Bidirectional(layers.GRU(units=256, return_sequences=True))(z31)\n    \n    z41 = layers.Multiply()([x4, z3])\n    z41 = layers.BatchNormalization()(z41)\n    z4 = layers.Bidirectional(layers.GRU(units=128, return_sequences=True))(z41)\n    \n    z51 = layers.Multiply()([x5, z4])\n    z51 = layers.BatchNormalization()(z51)\n    z5 = layers.Bidirectional(layers.GRU(units=64, return_sequences=True))(z51)\n    \n    x = layers.Concatenate(axis=2)([x5, z2, z3, z4, z5])\n    \n    x = layers.Dense(units=128, activation='selu')(x)\n    \n    x_output = layers.Dense(units=1)(x)\n    \n    model = keras.Model(inputs=x_input, outputs=x_output)\n    \n    model.compile(optimizer = \"adam\", \n                  #loss = \"mae\",\n                  loss=GBVPP_loss,\n                 #sample_weight_mode=\"temporal\",\n                 )\n    \n    return model  ","6d8b31c8":"gc.collect()","57828738":"#keras.backend.clear_session()","8408f507":"train_df = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')\ntrain_preds = train_df[['id', 'breath_id', 'pressure']]\ntrain_preds.loc[:, 'modified_breath_id'] = [i for i in range(len(train)) for _ in range(80)]","c82c818f":"NUM_FOLD = 10\nEPOCH = 40\nBATCH_SIZE = 256\n\n\nwith strategy.scope():\n    \n    VERBOSE = 0\n    test_preds = []\n    \n    kf = KFold(n_splits=NUM_FOLD, shuffle=True, random_state=100)\n    \n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        X_train, X_valid = train[train_idx], train[test_idx]\n        y_train, y_valid = targets[train_idx], targets[test_idx]\n        u_out_train, u_out_valid = u_outs[train_idx], u_outs[test_idx]  \n        \n        model = get_model()\n        model_path = f'..\/input\/gb-vpp-yet-another-lstm-colab\/best_valid_fold_{fold+1}.hdf5' \n        model.load_weights(model_path)\n        \n        #keras.backend.set_value(model.optimizer.lr, 0.00007)\n        model.compile(optimizer = tf.keras.optimizers.SGD(learning_rate=0.0007), loss=GBVPP_loss)\n        \n        plateau = keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.7, patience=3, verbose=1, min_lr=1e-08)\n        estop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min',restore_best_weights=False)\n\n        checkpoint_filepath = f\"best_valid_fold_{fold+1}.hdf5\"\n        sv = keras.callbacks.ModelCheckpoint(\n            checkpoint_filepath, monitor='val_loss', verbose=1, save_best_only=True,\n            save_weights_only=False, mode='auto', save_freq='epoch',\n            options=None\n        )\n        \n        \n        model.fit(X_train, np.append(y_train, u_out_train, axis =1),\n                  validation_data = (X_valid, np.append(y_valid, u_out_valid, axis =1)), epochs = EPOCH, \n                  batch_size = BATCH_SIZE, callbacks = [estop, plateau, sv],\n                  shuffle=True,\n                 )\n        \n        model.save(f\"end_of_fold_{fold+1}.hdf5\")\n            \n        y_true = y_valid.squeeze().reshape(-1, 1)\n        y_pred = model.predict(X_valid, batch_size=BATCH_SIZE).squeeze().reshape(-1, 1)\n        score = mean_absolute_error(y_true, y_pred)\n        train_preds.loc[train_preds.loc[:, 'modified_breath_id'].isin(test_idx), 'pressure'] = y_pred\n        print(f\"Fold-{fold+1} | OOF Score: {score}\")\n        \n        test_preds.append(model.predict(test, batch_size=BATCH_SIZE).squeeze().reshape(-1, 1).squeeze())\n        \n        gc.collect()","9b2af1c3":"fea_names = ['id', 'pressure']\ntrain_preds[fea_names].to_csv('oof.csv', index=False)","f95d291d":"#sub = pd.read_csv('..\/input\/pressure-speed-and-weights-ltsm\/submission.csv')\n#sub.to_csv('submission.csv', index=False) ","263a4bd7":"ss = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')\n\nss['pressure'] = np.median(np.vstack(test_preds),axis=0)\nss[\"pressure\"] =\\\n    np.round( (ss.pressure - PRESSURE_MIN)\/PRESSURE_STEP ) * PRESSURE_STEP + PRESSURE_MIN\nss.pressure = np.clip(ss.pressure, PRESSURE_MIN, PRESSURE_MAX)\nss.to_csv('submission.csv', index=False)","b273ce63":"# Data Load","a0352140":"# Feature Engineering","78b2a829":"# IMPORT LIB"}}