{"cell_type":{"09179a34":"code","c9d1495c":"code","7e685a30":"code","c046ae19":"code","7b7805fe":"code","0befa7a5":"code","fa3c464b":"code","cc9a2642":"code","c68b53c9":"code","e32c828a":"code","19dfab90":"code","cf56c18f":"code","056132f5":"code","f57bb6d5":"code","26e3a6b6":"code","2d344087":"code","a9742e9e":"code","e54f0845":"code","f373da0c":"code","ce5beb9b":"code","622c3439":"code","b26982d1":"code","6e4a9c06":"code","cf35e8c3":"code","03a9c094":"code","964012ac":"code","2a9bdc22":"code","71ce2697":"code","d2140c98":"code","8fdbb5e2":"code","8d2ac81b":"code","a5482eee":"code","45085c7d":"code","22fad00d":"code","47cb0efa":"code","592882b1":"code","61a1c9b2":"code","3bace064":"code","063b7a19":"code","93abc3d7":"code","8b582fd1":"code","0c8e4f1a":"code","34a510d9":"code","4ffb2435":"code","2bb56309":"code","00764178":"code","13330fd7":"code","4149c048":"code","7a88a9fd":"code","3ac08de8":"code","c1acefe1":"code","a004d0b5":"code","d9d6af12":"code","84a8917e":"code","be6cb834":"code","7a36040c":"code","ff9f34d4":"code","9223725f":"code","f680d93b":"code","2e3a2726":"code","260738a1":"code","8cb03702":"code","84635358":"code","57d157b2":"code","7a776d40":"code","99a7b1e8":"code","e6f113dc":"code","fcd4794f":"code","b1f6f5ba":"code","0271b806":"code","beb4bf6c":"code","231a7658":"code","64a5bc12":"code","347c8032":"code","32f82121":"code","73f1da71":"code","963a9207":"code","67d47d8c":"code","ee439dbc":"code","a35be712":"code","685a4407":"code","c2bb2f48":"code","0ea1abbc":"code","585ea90c":"code","5e46adb2":"code","e8848ed3":"code","a2795b36":"code","7bc85a2b":"code","9960ee09":"code","a75f50b8":"code","2b6d8e48":"code","162626a3":"code","48e54135":"code","092c774c":"code","92f3f28f":"code","f67fa546":"code","43b5d46d":"code","e15edd2b":"code","bf2ce1fe":"code","4e234e64":"code","0d230de9":"code","57863d2d":"code","5e91010e":"code","bf8c5500":"code","f3ed907f":"code","47f4c001":"markdown","73380198":"markdown","e6a98112":"markdown","13782117":"markdown","e2522472":"markdown","ff7424f0":"markdown","00934ebf":"markdown","19b28acf":"markdown","e9984698":"markdown","2500b94b":"markdown","14ecbb71":"markdown","35ec6fe0":"markdown","49af8d96":"markdown","76356a56":"markdown","17004021":"markdown","988992d9":"markdown","d93ab87a":"markdown","009b7b15":"markdown","7252c3ea":"markdown","9f624dff":"markdown","a9c23b4c":"markdown","b1dfaaf9":"markdown","30a6d1ee":"markdown","cd1b4305":"markdown","942abda3":"markdown","12c831ec":"markdown","980cf292":"markdown","2d65c53f":"markdown","d8cbeb0a":"markdown","f4c6142b":"markdown","a975d49c":"markdown","45c7cf5c":"markdown","704a1f32":"markdown","577eebfa":"markdown","da5bedf6":"markdown","d75dc7ae":"markdown","f2cb24e9":"markdown","4c0c622b":"markdown","6e8662f6":"markdown","8657d1b9":"markdown","08f664c2":"markdown","2b6e0861":"markdown","8595ad79":"markdown","8680ad53":"markdown","79fc1fee":"markdown","bde15136":"markdown","711b8247":"markdown","45579133":"markdown","56cbd9bf":"markdown","94950fd3":"markdown","2d01484b":"markdown","a3cf7440":"markdown","542b6099":"markdown","9a16bdf6":"markdown","48766e27":"markdown","f9f065bc":"markdown","c255572c":"markdown","78e40df6":"markdown","0c49b8a2":"markdown","d9f4b3c4":"markdown","e73986e7":"markdown","adb133d1":"markdown","47a9b2c4":"markdown","cc297d64":"markdown","fd3a4370":"markdown","056c5bb3":"markdown","0d5b6c0d":"markdown","a27df2b6":"markdown","c4c932f4":"markdown","bd405476":"markdown","75c4299a":"markdown","0a96e97c":"markdown","bb39bd4e":"markdown","f18a9d51":"markdown","1e343a57":"markdown","c9672175":"markdown","2eeb4159":"markdown","211bc4f0":"markdown","3fa83f68":"markdown","16748ba4":"markdown","a696006f":"markdown","8ef41b6c":"markdown","ec949916":"markdown","8f4d31f1":"markdown","151be83a":"markdown"},"source":{"09179a34":"import numpy as np\nimport pandas as pd\npd.set_option('max_columns', 200)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n%matplotlib inline\nsns.set()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nimport os","c9d1495c":"# Importing train.csv\n# Please make sure that the csv file is in the same folder as the python notebook otherwise this command wont work\ndf_train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")","7e685a30":"# Check the head of the dataset\ndf_train.head()","c046ae19":"df_train.shape","7b7805fe":"# Prining all the columns of the dataframe\ndf_train.columns.values","0befa7a5":"print(\"{} Numerical columns, {} Categorial columns are part of the original dataset.\".format(\n    list(df_train.select_dtypes(include=[np.number]).shape)[1],\n    list(df_train.select_dtypes(include = ['object']).shape)[1]))","fa3c464b":"df_train.info()","cc9a2642":"df_train.describe()","c68b53c9":"#Checking for any duplicates in the data frame\ndf_train.loc[df_train.duplicated()]","e32c828a":"df_train = df_train.drop('Id',axis=1)","19dfab90":"# finding all the missing data and summing them based on each column and storing it in a dataframe\ntotal = df_train.isnull().sum().sort_values(ascending = False)\n# Finding the percentage of the missing data by diving the number of missing values with total and  storing it in a dataframe\npercent = (df_train.isnull().sum()\/df_train.isnull().count()*100).sort_values(ascending = False)\n# Concatinating both the above df's\ndf_train_missing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n# Printing the data\ndf_train_missing_data.head(20)","cf56c18f":"# setting a grid for the pot\nsns.set_style(\"whitegrid\")\n# finding the no of missing values \nmissing = df_train.isnull().sum()\n# filtering the columns with just missing values\nmissing = missing[missing > 0]\n# sorting the values\nmissing.sort_values(inplace=True)\n# plotting the bar chart\nmissing.plot.bar()\n# setting the title of the plot\nplt.title('Columns with Missing values', fontsize=15)\n# setting the x label\nplt.xlabel('Columns')\n# setting the y label\nplt.ylabel('No of missing values')\n\n","056132f5":"# removing any column which has more than 90% null values\ndf_train = df_train.loc[:,df_train.isnull().sum()\/df_train.shape[0]*100<80]\n# printing the df\nprint(df_train.shape)","f57bb6d5":"# finding all the missing data and summing them based on each column and storing it in a dataframe\ntotal = df_train.isnull().sum().sort_values(ascending = False)\n# Finding the percentage of the missing data by diving the number of missing values with total and  storing it in a dataframe\npercent = (df_train.isnull().sum()\/df_train.isnull().count()*100).sort_values(ascending = False)\n# Concatinating both the above df's\ndf_train_missing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n# Printing the data\ndf_train_missing_data.head(16)","26e3a6b6":"NA=df_train[['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','GarageYrBlt','BsmtFinType2',\n'BsmtFinType1','BsmtCond', 'BsmtQual','BsmtExposure', 'MasVnrArea','MasVnrType','Electrical','FireplaceQu',\n             'LotFrontage']]","2d344087":"NAcat=NA.select_dtypes(include='object')\nNAnum=NA.select_dtypes(exclude='object')\nprint('We have :',NAcat.shape[1],'categorical features with missing values')\nprint('We have :',NAnum.shape[1],'numerical features with missing values')","a9742e9e":"# columns where NA values have meaning e.g. no garage etc.\ncols_fillna = ['MasVnrType','FireplaceQu',\n               'GarageQual','GarageCond','GarageFinish','GarageType',\n               'BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2']\n\n# replace 'NA' with 'No' in these columns\nfor col in cols_fillna:\n    df_train[col].fillna('No',inplace=True)","e54f0845":"# finding all the missing data and summing them based on each column and storing it in a dataframe\ntotal = df_train.isnull().sum().sort_values(ascending = False)\n# Finding the percentage of the missing data by diving the number of missing values with total and  storing it in a dataframe\npercent = (df_train.isnull().sum()\/df_train.isnull().count()*100).sort_values(ascending = False)\n# Concatinating both the above df's\ndf_train_missing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n# Printing the data\ndf_train_missing_data.head(5)","f373da0c":"# checking the count of different values within the column\ndf_train['LotFrontage'].value_counts()","ce5beb9b":"# pulling the length of number of unique values in the column\nnum_unique_values = len(df_train['LotFrontage'].unique())\n# Plotting a histogram for visualizing the data\ndf_train['LotFrontage'].plot.hist(bins = num_unique_values)","622c3439":"# checking the mean of the column\nprint(\"Mean is \",df_train['LotFrontage'].mean())\n# checking the mode of the column\nprint(\"Mode is \",df_train['LotFrontage'].mode())\n# checking the median of the column\nprint(\"Median is \",df_train['LotFrontage'].median())","b26982d1":"# imputing the value of median to the null values\ndf_train.loc[pd.isnull(df_train['LotFrontage']),['LotFrontage']]=69","6e4a9c06":"# pulling the length of number of unique values in the column\nnum_unique_values =  len(df_train['LotFrontage'].unique())\n# Plotting a histogram for visualizing the data\ndf_train['LotFrontage'].plot.hist(bins = num_unique_values)","cf35e8c3":"# checking the count of different values within the column\ndf_train['GarageYrBlt'].value_counts()","03a9c094":"# pulling the length of number of unique values in the column\nnum_unique_values =  len(df_train['GarageYrBlt'].unique())\n# Plotting a histogram for visualizing the data\ndf_train['GarageYrBlt'].plot.hist(bins = num_unique_values)","964012ac":"# checking the mean of the column\nprint(\"Mean is \",df_train['GarageYrBlt'].mean())\n# checking the mode of the column\nprint(\"Mode is \",df_train['GarageYrBlt'].mode())\n# checking the median of the column\nprint(\"Median is \",df_train['GarageYrBlt'].median())","2a9bdc22":"# imputing the value of mode to the null values\ndf_train.loc[pd.isnull(df_train['GarageYrBlt']),['GarageYrBlt']]=1980","71ce2697":"# pulling the length of number of unique values in the column\nnum_unique_values =  len(df_train['GarageYrBlt'].unique())\n# Plotting a histogram for visualizing the data\ndf_train['GarageYrBlt'].plot.hist(bins = num_unique_values)","d2140c98":"# finding all the missing data and summing them based on each column and storing it in a dataframe\ntotal = df_train.isnull().sum().sort_values(ascending = False)\n# Finding the percentage of the missing data by diving the number of missing values with total and  storing it in a dataframe\npercent = (df_train.isnull().sum()\/df_train.isnull().count()*100).sort_values(ascending = False)\n# Concatinating both the above df's\ndf_train_missing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n# Printing the data\ndf_train_missing_data.head(5)","8fdbb5e2":"# checking the count of different values within the column\ndf_train['MasVnrArea'].value_counts()","8d2ac81b":"# checking the mean of the column\nprint(\"Mean is \",df_train['MasVnrArea'].mean())\n# checking the mode of the column\nprint(\"Mode is \",df_train['MasVnrArea'].mode())\n# checking the median of the column\nprint(\"Median is \",df_train['MasVnrArea'].median())","a5482eee":"# imputing the value of median to the null values\ndf_train.loc[pd.isnull(df_train['MasVnrArea']),['MasVnrArea']]=0","45085c7d":"# checking the count of different values within the column\ndf_train['Electrical'].value_counts()","22fad00d":"df_train['Electrical'] = df_train['Electrical'].fillna(\"SBrkr\")","47cb0efa":"# finding all the missing data and summing them based on each column and storing it in a dataframe\ntotal = df_train.isnull().sum().sort_values(ascending = False)\n# Finding the percentage of the missing data by diving the number of missing values with total and  storing it in a dataframe\npercent = (df_train.isnull().sum()\/df_train.isnull().count()*100).sort_values(ascending = False)\n# Concatinating both the above df's\ndf_train_missing_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n# Printing the data\ndf_train_missing_data[df_train_missing_data.sum(axis=1)>0]","592882b1":"# before we move forward, lets create a copy of the existing df\ndf_train1=df_train.copy()","61a1c9b2":"# Initializing the figure\nfig = plt.figure(figsize = (12,8))\n# prining the boxplot\n# df_train = df_train.drop(['SalePrice'],axis=1)\n\nsns.boxplot(data=df_train1)\n# setting the title of the figure\nplt.title(\"PC Distribution\", fontsize = 12)\n# setting the y-label\nplt.ylabel(\"Range\")\n# setting the x-label\nplt.xlabel(\"Columns\")\nplt.xticks(rotation=90)\n\n# printing the plot\nplt.show()","3bace064":"df_train1.describe(percentiles=[.05,.25, .5, .75, .90, .95, .99])","063b7a19":"# Finding the columns on which the outlier treatment will be performed\nAllCols = df_train1.select_dtypes(exclude='object')\n# Sorting the columns\nAllCols = AllCols[sorted(AllCols.columns)]\n# printing the columns\nprint(AllCols.columns)","93abc3d7":"# running a for loop to remove the outliers from each column\nfor i in AllCols.columns:\n    # setting the lower whisker\n    Q1 = df_train[i].quantile(0.05)\n    # setting the upper whisker\n    Q3 = df_train[i].quantile(0.95)\n    # setting the IQR by dividing the upper with lower quantile\n    IQR = Q3 - Q1\n    # performing the outlier analysis\n    df_train = df_train[(df_train[i] >= Q1-1.5*IQR) & (df_train[i] <= Q3+1.5*IQR)]","8b582fd1":"# Checking the shape of the df now\ndf_train.shape","0c8e4f1a":"# checking the different percentiles now\ndf_train.describe(percentiles=[.05,.25, .5, .75, .90, .95, .99])","34a510d9":"# Initializing the figure\nfig = plt.figure(figsize = (12,8))\n# prining the boxplot\n# df_train1 = df_train.drop(['SalePrice','LotArea'],axis=1)\n\nsns.boxplot(data=df_train)\n# setting the title of the figure\nplt.title(\"PC Distribution\", fontsize = 12)\n# setting the y-label\nplt.ylabel(\"Range\")\n# setting the x-label\nplt.xlabel(\"Columns\")\nplt.xticks(rotation=90)\n\n# printing the plot\nplt.show()","4ffb2435":"# Let's look at the scarifice\nprint(\"Shape before outlier treatment: \",df_train1.shape)\nprint(\"Shape after outlier treatment: \",df_train.shape)\n\nprint(\"Percentage data removal is around {}%\".format(round(100*(df_train1.shape[0]-df_train.shape[0])\/df_train1.shape[0]),2))","2bb56309":"# Initializing a figure\nplt.figure(figsize=(20,8))\n\n# Initializing a subplot\nplt.subplot(1,2,1)\n# setting the title of the plot\nplt.title('House Price Histogram')\n# Plotting a Histogram for price column\nsns.distplot(df_train.SalePrice, kde=False, fit=stats.lognorm)\nplt.ylabel('Frequency')\n\n# Initializing another subplot\nplt.subplot(1,2,2)\n# setting the title of the plot\nplt.title('House Price Box Plot')\n# Plotting a boxplot for price column\nsns.boxplot(y=df_train.SalePrice)\n\nplt.show()","00764178":"# Checking the various percentile values for the price column\nprint(df_train.SalePrice.describe(percentiles = [0.25,0.50,0.75,0.85,0.90,1]))","13330fd7":"#skewness\nprint(\"Skewness: \" + str(df_train['SalePrice'].skew()))","4149c048":"# Finding all the numerical columns in the dataset. \nnumCols = df_train.select_dtypes(include=['int64','float'])\n\n# Sorting the columns\nnumCols = numCols[sorted(numCols.columns)]\n\n# Printing the columns\nprint(numCols.columns)\nprint(\"Numerical features : \" + str(len(numCols.columns)))\n","7a88a9fd":"# Initializing a figure\nplt.figure(figsize=(30,200))\n\n# Dropping the price column from the plot since we dont need to plot a scatter plot for price\nnumCols = numCols.drop('SalePrice',axis=1)\n\n# running a for-loop to print the scatter plots for all numerical columns\nfor i in range(len(numCols.columns)):\n    # Creating a sub plot\n    plt.subplot(len(numCols.columns),2,i+1)\n    # Creating a scatter plot\n    plt.scatter(df_train[numCols.columns[i]],df_train['SalePrice'])\n    # Assigning a title to the plot\n    plt.title(numCols.columns[i]+' vs Price')\n    # Setting the y label\n    plt.ylabel('Price')\n    # setting the x label\n    plt.xlabel(numCols.columns[i])\n\n\n# printing all the plots\nplt.tight_layout()","3ac08de8":"# Finding the categorical columns and printing the same.\ncategCols = df_train.select_dtypes(exclude=['int64','float64'])\n# Sorting the columns\ncategCols = categCols[sorted(categCols.columns)]\n# printing the columns\nprint(categCols.columns)","c1acefe1":"# Initializing a figure\nplt.figure(figsize=(15,100))\n\n# Initializing a variable for plotting multiple sub plots\nn=0\n\n# running a for-loop to print the histogram and boxplots for all categorical columns\nfor i in range(len(categCols.columns)):\n    # Increasing the count of the variable n\n    n+=1\n    # Creating a 1st sub plot\n    plt.subplot(len(categCols.columns),2,n)\n    # Creating a Histogram as the 1st plot for the column\n    sns.countplot(df_train[categCols.columns[i]])\n    # assigning x label rotation for carName column for proper visibility\n    if categCols.columns[i]=='Exterior1st' or categCols.columns[i]=='Exterior2nd' or categCols.columns[i]=='Neighborhood':        plt.xticks(rotation=75)\n    else:\n        plt.xticks(rotation=0)\n    # Assigning a title to the plot\n    plt.title(categCols.columns[i]+' Histogram')\n    \n    # Increasing the count of the variable n to plot the box plot for the same column\n    n+=1\n    \n    # Creating a 2nd sub plot\n    plt.subplot(len(categCols.columns),2,n)\n    # Creating a Boxplot as the 2nd plot for the column\n    sns.boxplot(x=df_train[categCols.columns[i]], y=df_train1.SalePrice)\n    # Assigning a title to the plot\n    plt.title(categCols.columns[i]+' vs Price')\n    # assigning x label rotation for carName column for proper visibility\n    if categCols.columns[i]=='Exterior1st' or categCols.columns[i]=='Exterior2nd' or categCols.columns[i]=='Neighborhood':\n        plt.xticks(rotation=75)\n    else:\n        plt.xticks(rotation=0)\n        \n\n# printing all the plots\nplt.tight_layout()","a004d0b5":"# pulling all the columns which can be deleted based on skewness\ncols_to_drop = ['Utilities','3SsnPorch','LowQualFinSF','MiscVal','PoolArea','ScreenPorch','KitchenAbvGr','GarageQual'\n               ,'GarageCond','Functional','Heating','LandContour','LandSlope','LotConfig','MSZoning','PavedDrive',\n                'RoofMatl','RoofStyle','SaleCondition','SaleType','Street']\n\n# running the for loop to print the value counts\nfor i in cols_to_drop:\n    print(df_train[i].value_counts(normalize=True) * 100)","d9d6af12":"# dropping the columns \ndf_train = df_train.drop(['Utilities','3SsnPorch','LowQualFinSF','MiscVal','PoolArea','ScreenPorch','KitchenAbvGr','GarageQual'\n               ,'GarageCond','Functional','Heating','LandContour','MSZoning','PavedDrive',\n                'RoofStyle','SaleCondition','SaleType','Street','BedroomAbvGr','MoSold'],axis=1)","84a8917e":"# checking the shape of the df now\ndf_train.shape","be6cb834":"# saleprice correlation matrix\ncorr_num = 15 #number of variables for heatmap\ncorrmat = df_train.corr()\ncols_corr = corrmat.nlargest(corr_num, 'SalePrice')['SalePrice'].index\ncorr_mat_sales = np.corrcoef(df_train[cols_corr].values.T)\nsns.set(font_scale=1.25)\nf, ax = plt.subplots(figsize=(12, 9))\nhm = sns.heatmap(corr_mat_sales, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 7}, yticklabels=cols_corr.values, xticklabels=cols_corr.values)\nplt.show()","7a36040c":"# Initializing a figure\nplt.figure(figsize=(20,8))\n\n# Initializing a subplot\nplt.subplot(1,2,1)\n# setting the title of the plot\nplt.title('House Price Histogram')\n# Plotting a Histogram for price column\nsns.distplot(df_train.SalePrice, kde=False, fit=stats.lognorm)\nplt.ylabel('Frequency')","ff9f34d4":"# Checking if the log transformation normalizes the target variable\nsns.distplot(np.log(df_train[\"SalePrice\"]))","9223725f":"# Applying the log transformation to the target variable\ndf_train[\"SalePrice\"] = np.log(df_train[\"SalePrice\"])","f680d93b":"# importing the skew library to check the skewness\nfrom scipy.stats import skew  ","2e3a2726":"# pulling the numeric columns from the dataset \nnumeric_feats = df_train.dtypes[df_train.dtypes != \"object\"].index\n\nskewed_feats = df_train[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\n\nskewed_feats","260738a1":"# Lets combine the floors square feet and the basement square feet to create the total sq feet\ndf_train['Total_sq_feet'] = (df_train['BsmtFinSF1'] + df_train['BsmtFinSF2'] +\n                                     df_train['1stFlrSF'] + df_train['2ndFlrSF'])\n\n# Lets combine all the bathrooms square feet to create the total bathroom sq feet\ndf_train['Total_Bathrooms_sq_feet'] = (df_train['FullBath'] + (0.5 * df_train['HalfBath']) +\n                                   df_train['BsmtFullBath'] + (0.5 * df_train['BsmtHalfBath']))\n\n# Lets combine all the porch square feet to create the total porch sq feet\ndf_train['Total_porch_sq_feet'] = (df_train['OpenPorchSF'] + df_train['EnclosedPorch'] + df_train['WoodDeckSF'])","8cb03702":"# checking the shape of the df now\ndf_train.shape","84635358":"# lets drop the columns which we used to create new features\ndf_train= df_train.drop(['BsmtFinSF1','BsmtFinSF2','1stFlrSF','2ndFlrSF','FullBath','HalfBath',\n                           'BsmtFullBath','BsmtHalfBath','OpenPorchSF','EnclosedPorch','WoodDeckSF'],axis=1)","57d157b2":"# checking the df now\ndf_train.head()","7a776d40":"# checking the shape now\ndf_train.shape","99a7b1e8":"# pulling the list of all the year columns from the dataset\nYear_cols = df_train.filter(regex='Yr|Year').columns\n# running a for loop to find the max year of each column \nfor i in Year_cols:\n    i = df_train[i].max()\n    print(i)","e6f113dc":"# running a for loop to subtract the max year with all values\nfor i in Year_cols:\n    df_train[i] = df_train[i].apply(lambda x: 2010 - x)","fcd4794f":"# Checking the dataset now\ndf_train.head()","b1f6f5ba":"# pulling all the categorical columns from the dataset.\ncategCols = df_train.select_dtypes(exclude=['int64','float64'])\n# Sorting the columns\ncategCols = categCols[sorted(categCols.columns)]\n# printing the categorical columns\nprint(categCols.columns)","0271b806":"# Defining the map function\ndef dummies(x,df):\n    # Get the dummy variables for the categorical feature and store it in a new variable - 'dummy'\n    dummy = pd.get_dummies(df[x], drop_first = True)\n    for i in dummy.columns:\n        dummy = dummy.rename(columns={i: x+\"_\"+i})\n    # Add the results to the original dataframe\n    df = pd.concat([df, dummy], axis = 1)\n    # Drop the original category variables as dummy are already created\n    df.drop([x], axis = 1, inplace = True)\n    # return the df\n    return df\n\n#Applying the function to the df_train categorical columns\nfor i in categCols:\n    df_train = dummies(i,df_train)","beb4bf6c":"# checking the dataset now\ndf_train.head()","231a7658":"# Checking the shape of the new dataset which will be used for model building\ndf_train.shape","64a5bc12":"# importing the libraries\nfrom sklearn.preprocessing import StandardScaler\n# dropping the target variable and stroing the remaining in a new df\nX = df_train.drop(['SalePrice'],axis=1)\n# storing the target column in a new df\ny = df_train['SalePrice']\n# initializing the standard scalar\nscaler = StandardScaler()\n# scale the X df\nscaler.fit(X)","347c8032":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif.loc[vif['VIF'] > 3000, :]","32f82121":"# Dropping the above columns\nX=X.drop(['Exterior2nd_CBlock','BsmtUnfSF','BsmtQual_No','Total_sq_feet','Exterior1st_CBlock','BsmtCond_No'\n         ,'TotalBsmtSF','GrLivArea','GarageFinish_No','GarageType_No','BsmtFinType1_No'],axis=1)","73f1da71":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\n# X = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif.loc[vif['VIF'] > 100, :]","963a9207":"X=X.drop(['ExterCond_TA','Condition2_Norm','Exterior1st_VinylSd','Exterior2nd_VinylSd','Exterior1st_MetalSd','Exterior2nd_MetalSd'\n         ,'Exterior1st_HdBoard'],axis=1)","67d47d8c":"# Calculate the VIFs for the new model\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\nvif = pd.DataFrame()\n# X = X_train_new\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","ee439dbc":"# importing the required libraries\nfrom sklearn.model_selection import train_test_split\n\n# splitting the dataset into train and test\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 100)","a35be712":"# Checking the number of columns and rows in the train dataset\nX_train.shape","685a4407":"# Checking the number of columns and rows in the test dataset\nX_test.shape","c2bb2f48":"# linear regression\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\n# predict\ny_train_pred = lm.predict(X_train)\n\n# print(metrics.r2_score(y_true=y_train, y_pred=y_train_pred))\nprint(\"RMSE Train {}\".format(np.sqrt(mean_squared_error(y_train, y_train_pred))))\nprint(\"R2 Score Train {}\".format(r2_score(y_train, y_train_pred)))\ny_test_pred = lm.predict(X_test)\n# print(metrics.r2_score(y_true=y_test, y_pred=y_test_pred))\nprint(\"RMSE Test {}\".format(np.sqrt(mean_squared_error(y_test, y_test_pred))))\nprint(\"R2 Score Test {}\".format(r2_score(y_test, y_test_pred)))","0ea1abbc":"# model coefficients\n# liner regression model parameters\nmodel_parameters = list(lm.coef_)\nmodel_parameters.insert(0, lm.intercept_)\nmodel_parameters = [round(x, 3) for x in model_parameters]\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nlist(zip(cols, model_parameters))","585ea90c":"# list of alphas to tune\nparams = {'alpha': [0.0001, 0.001, 0.01, 0.05, 0.1, \n 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 2.0, 3.0, \n 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 20, 50, 100, 500, 1000 ]}\n\n# Initializing the Ridge regression\nridge = Ridge()\n\n# cross validation\n# Setting the number of folds\nfolds = 5\n# performing GridSearchCV on the ridge regression using the list of params\nmodel_cv = GridSearchCV(estimator = ridge, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n# Fitting the model on our Train sets\nmodel_cv.fit(X_train, y_train) ","5e46adb2":"# Storing the results in a new df\ncv_results = pd.DataFrame(model_cv.cv_results_)\n# filtering out the alpha parameters which are less than 200\ncv_results = cv_results[cv_results['param_alpha']<=200]\n# checking the results\ncv_results.head()","e8848ed3":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n\n# plotting\n# plotting the mean train scores\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\n# plotting the mean test scores\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\n# setting the x label\nplt.xlabel('alpha')\n# setting the y label\nplt.ylabel('Negative Mean Absolute Error')\n# setting the title\nplt.title(\"Negative Mean Absolute Error and alpha\")\n# setting the legend\nplt.legend(['train score', 'test score'], loc='upper left')\n# showing the plot\nplt.show()","a2795b36":"# finding the best Alpha value\nprint ('The best value of Alpha for Ridge Regression is: ',model_cv.best_params_)","7bc85a2b":"# setting the value of alpha as 7\nalpha = 7\n# initializing the ridge regression with the optimized alpha value\nridge = Ridge(alpha=alpha)\n\n# running the ridge algo on the train datasets\nridge.fit(X_train, y_train)\n\n# Lets predict\ny_train_pred = ridge.predict(X_train)\nprint(\"RMSE Train {}\".format(np.sqrt(mean_squared_error(y_train, y_train_pred))))\nprint(\"R2 Score Train {}\".format(r2_score(y_train, y_train_pred)))\ny_test_pred = ridge.predict(X_test)\nprint(\"RMSE Test {}\".format(np.sqrt(mean_squared_error(y_test, y_test_pred))))\nprint(\"R2 Score Test {}\".format(r2_score(y_test, y_test_pred)))\n","9960ee09":"# checking the coefficient values of all the features.\nridge.coef_","a75f50b8":"# Assigning the columns to the respective coefficient values\n# ridge model parameters\nmodel_parameters = list(ridge.coef_)\nmodel_parameters.insert(0, ridge.intercept_)\nmodel_parameters = [round(x, 3) for x in model_parameters]\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nlist(zip(cols, model_parameters))","2b6d8e48":"# pulling the coefficients and index and creating a new df\ncoef = pd.Series(ridge.coef_, index = X.columns).sort_values()\n# filtering the top 5 positive and negative features \nridge_imp_coef = pd.concat([coef.head(10), coef.tail(10)])\n# plotting the graph\nridge_imp_coef.plot(kind = \"barh\")\n# setting the title of the plot\nplt.title(\"Model Coefficients\")","162626a3":"# Converting the important feature list into a df for better understanding\nridge_imp_coef = ridge_imp_coef.to_frame('Coeff_val').reset_index()\nridge_imp_coef.columns = ['Features', 'Coeff_val']\nridge_imp_coef['Coeff_val'] = ridge_imp_coef['Coeff_val'].abs()\nridge_imp_coef = ridge_imp_coef.sort_values(by=['Coeff_val'], ascending=False)\nridge_imp_coef.head(10)","48e54135":"p_pred = np.expm1(ridge.predict(X))\nplt.scatter(p_pred, np.expm1(y))\nplt.plot([min(p_pred),max(p_pred)], [min(p_pred),max(p_pred)], c=\"red\")","092c774c":"# initializing the Lasso regression\nlasso = Lasso()\n\n# cross validation\n# performing GridSearchCV on the lasso regression using the list of params\nmodel_cv = GridSearchCV(estimator = lasso, \n                        param_grid = params, \n                        scoring= 'neg_mean_absolute_error', \n                        cv = folds, \n                        return_train_score=True,\n                        verbose = 1)            \n# Fitting the model on our Train sets\nmodel_cv.fit(X_train, y_train) ","92f3f28f":"# Storing the results in a new df\ncv_results = pd.DataFrame(model_cv.cv_results_)\n\n# checking the results\ncv_results.head()","f67fa546":"# plotting mean test and train scoes with alpha \ncv_results['param_alpha'] = cv_results['param_alpha'].astype('int32')\n\n# plotting\n# plotting the mean train scores\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\n# plotting the mean test scores\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\n# setting the x label\nplt.xlabel('alpha')\n# setting the y label\nplt.ylabel('Negative Mean Absolute Error')\n# setting the title\nplt.title(\"Negative Mean Absolute Error and alpha\")\n# setting the legend\nplt.legend(['train score', 'test score'], loc='upper left')\n# showing the plot\nplt.show()","43b5d46d":"cv_results['param_alpha'] = cv_results['param_alpha'].astype('float32')\n# plotting the mean train scores\nplt.plot(cv_results['param_alpha'], cv_results['mean_train_score'])\n# plotting the mean test scores\nplt.plot(cv_results['param_alpha'], cv_results['mean_test_score'])\n# setting the x label\nplt.xlabel('alpha')\n# setting the y label\nplt.ylabel('Negative Mean Absolute Error')\n# setting the xscale into log\nplt.xscale('log')\n# setting the title\nplt.title(\"Negative Mean Absolute Error and alpha\")\n# setting the legend\nplt.legend(['train score', 'test score'], loc='upper left')\n# showing the plot\nplt.show()","e15edd2b":"print('The best value of Alpha for Lasso Regression is: ',model_cv.best_params_)","bf2ce1fe":"# initializing the ridge regression with the optimized alpha value\nlm = Lasso(alpha=0.001)\n# fitting the model on the train datasets\nlm.fit(X_train, y_train)\n\n# predict\ny_train_pred = lm.predict(X_train)\nprint(\"RMSE Train {}\".format(np.sqrt(mean_squared_error(y_train, y_train_pred))))\nprint(\"R2 Score Train {}\".format(r2_score(y_train, y_train_pred)))\ny_test_pred = lm.predict(X_test)\nprint(\"RMSE Test {}\".format(np.sqrt(mean_squared_error(y_test, y_test_pred))))\nprint(\"R2 Score Test {}\".format(r2_score(y_test, y_test_pred)))","4e234e64":"# checking the coefficient values of all the features.\nlm.coef_","0d230de9":"# Assigning the columns to the respective coefficient values\n# lasso model parameters\nmodel_parameters = list(lm.coef_)\nmodel_parameters.insert(0, lm.intercept_)\nmodel_parameters = [round(x, 3) for x in model_parameters]\ncols = X.columns\ncols = cols.insert(0, \"constant\")\nlist(zip(cols, model_parameters))","57863d2d":"# pulling the coefficients and index and creating a new df\ncoef = pd.Series(lm.coef_, index = X.columns).sort_values()\n# filtering the top 5 positive and negative features \nlasso_imp_coef = pd.concat([coef.head(10), coef.tail(10)])\n# plotting the graph\nlasso_imp_coef.plot(kind = \"barh\")\n# setting the title of the plot\nplt.title(\"Model Coefficients\")","5e91010e":"# Converting the important feature list into a df for better understanding\nlasso_imp_coef = lasso_imp_coef.to_frame('Coeff_val').reset_index()\nlasso_imp_coef.columns = ['Features', 'Coeff_val']\nlasso_imp_coef['Coeff_val'] = lasso_imp_coef['Coeff_val'].abs()\nlasso_imp_coef = lasso_imp_coef.sort_values(by=['Coeff_val'], ascending=False)\nlasso_imp_coef.head(10)","bf8c5500":"p_pred = np.expm1(lm.predict(X))\nplt.scatter(p_pred, np.expm1(y))\nplt.plot([min(p_pred),max(p_pred)], [min(p_pred),max(p_pred)], c=\"red\")","f3ed907f":"# checking how many features were dropped by lasso during modelling\nprint(\"Lasso kept\",sum(coef != 0), \"important features and dropped the other\", sum(coef == 0),\"features\")","47f4c001":"Now lets run Lasso Regression on the dataset.","73380198":"## 4.1 Check for skewness","e6a98112":"First, let's take a look at our target.","13782117":"### 5.3 Splitting the Data into Training and Testing sets","e2522472":"#### Inference :\n\n1. The house price's looks to be `right-skewed` as majority of the house prices are `low` (Below 250,000).\n2. There is a significant difference between the `mean` and the `median` of the price distribution.\n3. `Large Standard deviation` indicates `high variance` in the house prices (85% of the prices are below 250,000, whereas the remaining 15% are between 250,000 and 755,000).\n\n**`Note:`** There are some outliers in the Price as well but we will not remove them for now. Also the target variable is highly skewed. We will treat this later on. ","ff7424f0":"### 3.1 Univariate Analysis","00934ebf":"Inspect the various aspects of the df_train dataframe","19b28acf":"Now before we move forward, we should also remove multicolinearity. \n\nWe will do this by checking VIF and removing all the highly correlated columns since they would be redundent in our model.","e9984698":"Now lets check for skewness for all columns","2500b94b":"### 3.1.2 Visualising Numeric Variables\n\nFor Visualization, we will 1st find all the numerical columns and then make `scatterplots` for all of them.","14ecbb71":"## Inference:\n- Majority of the values in BldgType are 1Fam i.e Single-family Detached.\n- Majority of the values in BsmtCond are TA i.e typical condition.\n- Majority of the values in BsmtExposure are No i.e No Exposure.\n- Majority of the values in BsmtFinType1 are GLQ (Good Living Quarters) and Unf (Unfinshed). The GLQ are highly priced as compared to other Ratings.\n- Majority of the values in BsmtFinType2 are Unf which means that the second basement is mostly unfinished. The ALQ rating basement has the highest quantile range.\n- Majority of the values in BsmtQual are Gd and TA which means that the average height of the basement is above 80 inches. Ex(Excellent) has the highest price range.\n- Majority of the houses have Central air conditioning and hence have the highest price range as well.\n- Conditon1 and Condition2 of the houses in majority are Normal. Artery has a good price range as compared to others except Normal condition.\n- Majority of the houses have SBrkr Electrical system.\n- Majority of the external conditions of the materials are Average\/Typical. These and Good external conditions have the highest price ranges.\n- The external quality of the materials on an average is Typical. Good and Excellent condition have the highest price ranges which is as expected.\n- Exterior covering on majority of the houses are Vinyl Siding followed by Metal Siding and Hard Board. Hard Board and Vinyl Siding have the highest price range.\n- Majority of the houses have No Fireplace. If they have then they are in Good and Typical condition and hence these houses attract more prices.\n- The type of foundation is more of Poured Contrete\tand Cinder Block. Poured Contrete have the highest price range.\n- The home functionality, Garage Condition, GarageQual is typical in majority of the cases. We can think of deleting these columns if they are heavily skewed.\n- The Garage Finish is majorly Unfinished but furnished garages have a higher price.\n- Majority of the GarageType are attached to the home and attract the highest house price as well.\n- Majority of the houses are Gas forced warm air furnace heated. We can think of deleting this column if it is heavily skewed.\n- Majority of the houses have Excellent Heating quality and hance have the highest price range.\n- Majority of the houses are 1 story tall followed by 2 Stories. 2Stories houses are most priced followed by 1 story houses.\n- Majority of the houses have typical Kitchen quality but the excellent quality attract the highest prices.\n- Majority of the houses are on a leveled or flat land and have a Gentle slope. We can think of deleting this column if it is heavily skewed.\n- Majority of the houses have an inside lot and are regular in shape. We can think of deleting this column if it is heavily skewed.\n- Majority of the houses are from Residential Low Density zone and these attract the highest prices as well.\n- Majority of the houses do not have Masonry veneer done and the ones which have it's Brick Common. These 2 have the highest price range as well.\n- Majority of the houese have been bought in North Ames area followed by College Creek. Northridge and Northridge Heights neighbourhood have the highest price ranges.\n- Majority of the houses have a Paved Driveway and a Paved Street. We can think of deleting this column if it is heavily skewed.\n- Majority of the houses have the roof Material as Standard Shingle and the type as Gable. We can think of deleting this column if it is heavily skewed.\n- Majority of the houses has had a Normal with Warranty Deed - Conventional Sale. We can think of deleting this column if it is heavily skewed.\n- Majority of the houses have All public utilities available (E,G,W,& S). We can think of deleting this column if it is heavily skewed.","35ec6fe0":"#### 2.2.2 GarageYrBlt column check","49af8d96":"There are 2 types of outliers and we will treat the outliers since they can skew our dataset.\n\n- Statistical\n- Domain specific","76356a56":"#### 2.2.1 LotFrontage column check","17004021":"Let's check the VIF values again","988992d9":"### Inference:\nSince the distribution has not changed much before and after the null value imputation, we should be good here.","d93ab87a":"Lets visualize the fit after the modelling","009b7b15":"## Step 1: Reading and Understanding the Data\n\nLet us first import NumPy and Pandas and read the automobile dataset","7252c3ea":"### Inference:  ","9f624dff":"## 4.2 Feature Engineering","a9c23b4c":"#### Inference:\nAs we can see from the above our `train R2 score is 92.68 and test R2 is 87.61`.","b1dfaaf9":"## Inference:\n- `1stFlrSF, 2ndFlrSF, GarageArea, GrLivArea, GarageYrBlt,LotFrontage, LotArea,OverallQual,TotalBsmtSF,WoodDeckSF` seem to be positively correlated to price\n- Majority of the values in `3SsnPorch, LowQualFinSF, MiscVal, PoolArea,ScreenPorch` are 0 hence we can take a call to delete these columns if the columns are heavily skewed.\n- `BedroomAbvGr,MoSold` seems to have less correlation with price.\n- `BsmtFinSF1, BsmtFinSF2,BsmtFullBath, BsmtHalfBath, BsmtUnfSF,Enclosed Porch, HalfBath, Fireplaces, FullBath, GarageCars, MSSubClass, MasVnrArea, OpenPorchSF,OverallCond,TotRmsAbvGrd, YearBuilt, YearRemodAdd,YrSold` seems to have some correlation with price.\n- Majority of the values in `kitchenAbvGr` are 1 hence we can take a call to delete this columns if the column is heavily skewed.","30a6d1ee":"First thing to do is get rid of the features with more than 90% missing values. For example the PoolQC's missing values are probably due to the lack of pools in some buildings, which is very logical. But replacing those (more than 90%) missing values with \"no pool\" will leave us with a feature with low variance, and low variance features are uniformative for machine learning models. So we drop the features with more than 80% missing values.","cd1b4305":"#### Inference:\nDropping the columns which have inf VIF value.","942abda3":"Now we have decided to `impute` the values of either the `mean\/ median or mode` with the null values. Hence lets check what these 3 metrics provide us.","12c831ec":"## 4.3 Creating dummies","980cf292":"#### Inference:\nWe can see that there are columns which have skewness in them but we will leave these for now.","2d65c53f":"As we can see all the categorical values have been expanded and representated as 0's & 1's. This step is crucial to build a robust linear regression model.","d8cbeb0a":"## Step 6: Ridge and Lasso Regression\n\nSince we have multiple features and a big difference between the R2 score of Train and Test set, we will try to make this better by performing Advanced Regression Techniques.\n\nThe 2 that we will use here are:\n- Ridge Regression\n- Lasso Regression","f4c6142b":"Need to check if there is any missing data in the dataset.","a975d49c":"#### Inference:\nDropping the columns which have VIF value>100.","45c7cf5c":"Now that we have imputed the NA columns with the value of `No`, we are still left with columns which have a high percentage of null values. \n\nLet's check these individually and see how to treat such columns.","704a1f32":"## Step 5: Model Building","577eebfa":"Lets plot a box plot to check the outliers","da5bedf6":"#### Inference:\n- We can see that OverAllQual, GrLivArea and GarageCars are highly correlated with Sales Price.\n","d75dc7ae":"We will now create dummy variables for all the categorical variables in order to conveert them to numerical so that the model could be built for the same.","f2cb24e9":"As we can see from the above, the R2 score of `Train set is 0.90 and test is 0.88` for Lasso Regression.","4c0c622b":"#### 2.2.4 Electrical column check","6e8662f6":"## Step 3: Visualising the Data using EDA\n\n- Here's where we'll identify if some predictors directly have a strong association with the outcome variable i.e `Sales Price`.\n\n- We'll visualise our data using `matplotlib` and `seaborn`.","8657d1b9":"#### 2.2.3 MasVnrArea column check","08f664c2":"### 3.1.1 Plotting the Price of all the houses in the dataset","2b6e0861":"Lets plot the above values so that we can better visualize the results.","8595ad79":"#### Let's check the correlation coefficients to see which variables are highly correlated","8680ad53":"### 5.1 Rescaling the Features  \n\nIt is extremely important to rescale the variables so that they have a comparable scale. If we don't have comparable scales, then some of the coefficients as obtained by fitting the regression model might be very large or very small as compared to the other coefficients.\n\nNow, there are two common ways of rescaling:\n\n1. Min-Max scaling \n2. Standardisation (mean-0, sigma-1) \n\nWe will use `Standardisation scaling`. In this, for all the columns the mean will be 0.","79fc1fee":"Looking at the distribution of the values in the above mentioned columns, we have taken a decision of `deleting all these columns which have a single value of >80%` since they will not help us in our model building.","bde15136":"### Inference:\nSince the mode and median are the same, and majority of the values are 0, we will impute the missing values as 0.","711b8247":"### 3.1.3 Visualising Categorical Variables\n\nIn order to visualize the Categorical Variables, we will make `Histograms and Boxplots`.","45579133":"### Inference:\nAs all 3 metrics are comparable. But since we want missing value to be imputed with an integer, taking **median** i.e. `69`.","56cbd9bf":"Lets check the different features and their respective coefficients value","94950fd3":"Lets check the Distribution now","2d01484b":"Lets check the Distribution now","a3cf7440":"Ok, now that we have dealt with all the missing values and the uncorrelated columns, it looks like it's time for some feature engineering, the second part of our data preprocessing. We need to create feature vectors in order to get the data ready to be fed into our model as training data. This requires us to convert the categorical values into representative numbers..","542b6099":"Since the data is skewed, we will try to fix this with a `log transformation`.","9a16bdf6":"Let's check the VIF values again","48766e27":"## 6.2 Lasso Regression","f9f065bc":"#### Model 1:\n\nLets first run linear regression on the dataset and check what kind of results we get.","c255572c":"### 2.1 Drop un-needed variables","78e40df6":"Since we are unable to read the above graph properly, we will convert the x-scale into log","0c49b8a2":"Now we can also see that there are 4 `YEAR columns` in the dataset. In order to handle them we will convert them as well by finding the number of years.","d9f4b3c4":"### 5.2 Remove Multicolinearity","e73986e7":"## 6.1 Ridge Regression","adb133d1":"Now looking at the data dictionary, we can see that there are some columns which can be merged to create new features.\n\nLet's do that now.","47a9b2c4":"#### Inference:\nAs we can see that our predicted line is passing through almost the entire dataset.","cc297d64":"### HyperParameter tuning","fd3a4370":"As we can see from the above, the R2 score of `Train set is 0.92 and test is 0.88` for Ridge Regression.","056c5bb3":"### Inference:\nSince this column tells about in what year the garage was built, we cannot apply the mean or the median here. Hence we will impute the values with mode.","0d5b6c0d":"### Inference:\nSince the distribution has not changed much before and after the null value imputation, we should be good here.","a27df2b6":"### HyperParameter tuning","c4c932f4":"Now, upon further checking the data, we can see that there are columns having null values but as per the data description these are not values which were not captured. These basically mean that those features were not available as part of the property. Hence we will have to impute them appropriately.\n\n- We have decided to impute such columns with a value of `No`.","bd405476":"Dropping Id column since its a unique identifier in the dataset and does not help in the analysis.","75c4299a":"Since we now know that the best Alpha (Regularization term) value is 7, we will now build our model with the same.","0a96e97c":"Let's Plot the above and find the top 10 features of Ridge regression","bb39bd4e":"# Step 4: Data Preparation","f18a9d51":"## Step 2: Cleaning the Data","1e343a57":"### Inference:\nIt appears that the target, SalePrice, is very skewed and a transformation like a logarithm would make it more normally distributed. Machine Learning models tend to work much better with normally distributed targets, rather than greatly skewed targets. By transforming the prices, we can boost model performance.","c9672175":"As we can see from the graph and table above, there are some outliers in the dataset. Lets treat these outliers. We will keep the lower quantile at 0.05 and higher quantile at 0.95.","2eeb4159":"### 2.2 Checking for Missing Values and Treating Them","211bc4f0":"Now we will build our model with the optimized value of alpha for Lasso regression i.e 0.001","3fa83f68":"## 2.3 Outlier Analysis","16748ba4":"Since the max values for all the year columns are the same, we will now convert the year columns by `subtracting the max year date with all the values in the 4 columns`.","a696006f":"We now need to split our variable into training and testing sets. We'll perform this by importing `train_test_split` from the `sklearn.model_selection` library. It is usually a good practice to keep 70% of the data in the train dataset and the rest 30% in the test dataset which is what we will follow as well.","8ef41b6c":"Since the Majority is SBrkr, we will impute it with the same.","ec949916":"## Conclusion:\n\nBased on the EDA, we can easily drop some columns which are highly skewed since they will not help us in our model building.","8f4d31f1":"Let's visualize the above as well","151be83a":"# Final Conclusion:\n\nBased on our regression results, below are the top 10 features which drive the Sales prices of the Houses in Australia.\n\n- **Ridge Regression**:\n    - We can see that the Train and Test R2 value was `0.92 and 0.88` respectively.\n    - The top 10 features that drive the house prices as per Ridge regression are :\n        - Neighborhood_Crawfor\n        - Exterior1st_BrkFace\n        - Neighborhood_StoneBr\n        - BsmtQual_Fa\n        - Neighborhood_MeadowV\n        - OverallQual\n        - Neighborhood_BrDale\n        - Neighborhood_NoRidge\n        - Neighborhood_Gilbert\n        - KitchenQual_TA\n        \n        \n- **Lasso Regression**:\n    - We can see that the Train and Test R2 value was `0.90 and 0.88` respectively.\n    - The top 10 features that drive the house prices as per Lasso regression are:\n        - Neighborhood_Crawfor\n        - Exterior1st_BrkFace\n        - OverallQual\n        - Neighborhood_StoneBr\n        - Total_Bathrooms_sq_feet\n        - KitchenQual_TA\n        - TotRmsAbvGrd\n        - Foundation_PConc\n        - Fireplaces\n        - Neighborhood_NoRidge"}}