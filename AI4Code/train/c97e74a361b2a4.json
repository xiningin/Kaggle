{"cell_type":{"5e170f4c":"code","50798063":"code","01e39c90":"code","38b91900":"code","4794419b":"code","75baafd8":"code","1ac20a33":"code","a323bfe4":"code","66f3ad21":"code","43d878ae":"code","7235d101":"code","bd5e9b7e":"code","902fc065":"code","8f9118a3":"code","0ccb0ab5":"code","aff68c1a":"markdown","b5b1cbc1":"markdown","5f315bdf":"markdown","323e2ee6":"markdown","845c5499":"markdown","b34db41a":"markdown","947bad14":"markdown","647a0944":"markdown","1e224c77":"markdown","b1eae0e7":"markdown","500ee805":"markdown","3883e8ba":"markdown","36ae1e74":"markdown"},"source":{"5e170f4c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os","50798063":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics","01e39c90":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","38b91900":"train.isna().sum()","4794419b":"test.isna().sum()","75baafd8":"train.fillna('missing', inplace=True)\ntest.fillna('missing', inplace=True)","1ac20a33":"train.iloc[0]","a323bfe4":"X_train, X_test, y_train, y_test = train_test_split(train[['keyword', 'location', 'text']], train['target'],\n                                                   test_size=0.2, random_state=42)","66f3ad21":"bow_key = CountVectorizer()\n\nX_train_key = bow_key.fit_transform(X_train['keyword']).toarray()\nX_test_key = bow_key.transform(X_test['keyword']).toarray()\n\ntest_key = bow_key.transform(test['keyword']).toarray()","43d878ae":"bow_loc = CountVectorizer()\n\nX_train_loc = bow_loc.fit_transform(X_train['location']).toarray()\nX_test_loc = bow_loc.transform(X_test['location']).toarray()\n\ntest_loc = bow_loc.transform(test['location']).toarray()","7235d101":"tfidf = TfidfVectorizer(ngram_range=(1,2))\n\nX_train_text = tfidf.fit_transform(X_train['text']).toarray()\nX_test_text = tfidf.transform(X_test['text']).toarray()\n\ntest_text = tfidf.transform(test['text']).toarray()","bd5e9b7e":"len(tfidf.get_feature_names())","902fc065":"X_train_vec = np.concatenate((X_train_key, X_train_loc, X_train_text), axis=1)\nX_test_vec = np.concatenate((X_test_key, X_test_loc, X_test_text), axis=1)\ntest_vec = np.concatenate((test_key, test_loc, test_text), axis=1)","8f9118a3":"X_train_vec.shape","0ccb0ab5":"clf = MultinomialNB().fit(X_train_vec, y_train)\nclf.score(X_test_vec, y_test)","aff68c1a":"If you have any idea that I should try, please comment below.","b5b1cbc1":"In a [previous notebook](https:\/\/www.kaggle.com\/riyadmorshedshoeb\/using-tfidf-model-comparison), I only used the text feature and Tf-Idf vectorization on that to classify the tweets.\n\nIn this notebook and future versions of this, I intend to use all three features and use different feature extraction methods on them on every version. If there is any improvement of performance for a method, then I will use that method on my other notebook for the actual classification task and submission to the competition.","5f315bdf":"### Splitting the data","323e2ee6":"### Creating the vectors and Transforming the features","845c5499":"# Versions","b34db41a":"## Reading the data","947bad14":"# Data","647a0944":"## Handling the missing values","1e224c77":"## Version 2\nTf-Idf with unigrams and bi-grams on text feature and BOW on the rest.","b1eae0e7":"## Version 1\nTf-Idf on text feature and Bag-Of-Words on the rest.","500ee805":"### Combining the vectors together","3883e8ba":"# Imports","36ae1e74":"# Test it on a model"}}