{"cell_type":{"e6550965":"code","2c27ccbb":"code","4e350346":"code","ce8d1453":"code","4ae95c39":"code","f9e4f007":"code","99b6b3d9":"code","81f245ed":"code","3b776407":"code","fb9d87b7":"code","7b2bd5cb":"code","26d417d1":"code","5230ede0":"code","fab7acc6":"code","cb001a9a":"code","9ceb118c":"code","6434ee99":"code","9bd7f811":"code","b331fc5c":"code","76f2f94a":"code","8d4a4fce":"code","41940856":"code","8540087c":"code","899c314a":"code","019642df":"code","70764128":"code","c1eba2ca":"code","b7502a94":"code","f3536c03":"code","c526055f":"markdown","accbd581":"markdown","d706c55e":"markdown","9325b3b7":"markdown","026eb763":"markdown","097c2fb8":"markdown","aaae85c2":"markdown","521eb583":"markdown","1e1c3602":"markdown","130c1389":"markdown","a0feaf37":"markdown","c7e9a16d":"markdown","ac6c7516":"markdown","e9ad26d2":"markdown","af0e9320":"markdown","5ec9d00d":"markdown","9049aa76":"markdown","dff42b2f":"markdown","719215a2":"markdown","908df2b2":"markdown","6ac86eb0":"markdown","e031d580":"markdown"},"source":{"e6550965":"!pip install git+https:\/\/www.github.com\/keras-team\/keras-contrib.git","2c27ccbb":"from random import random\nfrom numpy import load\nfrom numpy import zeros\nfrom numpy import ones\nfrom numpy import asarray\nfrom numpy.random import randint\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.initializers import RandomNormal\nfrom keras.models import Model\nfrom keras.models import Input\nfrom keras.layers import Conv2D\nfrom keras.layers import Conv2DTranspose\nfrom keras.layers import LeakyReLU\nfrom keras.layers import Activation\nfrom keras.layers import Concatenate\nfrom keras_contrib.layers.normalization.instancenormalization import InstanceNormalization\nfrom matplotlib import pyplot\n\n# example of using saved cyclegan models for image translation\nfrom keras.models import load_model","4e350346":"# example of preparing the horses and zebra dataset\nfrom os import listdir\nfrom numpy import asarray\nfrom numpy import vstack\nfrom keras.preprocessing.image import img_to_array\nfrom keras.preprocessing.image import load_img\nfrom numpy import savez_compressed\nimport pandas as pd\nfrom PIL import Image, ImageDraw\nimport random as rd\nimport numpy as np","ce8d1453":"# load all images in a directory into memory\ndef load_images(path, size=(256,256)):\n\tdata_list = list()\n\t# enumerate filenames in directory, assume all are images\n\tfor filename in listdir(path):\n\t\t# load and resize the image\n\t\tpixels = load_img(path + filename, target_size=size)\n\t\t# convert to numpy array\n\t\tpixels = img_to_array(pixels)\n\t\t# store\n\t\tdata_list.append(pixels)\n\treturn asarray(data_list)\n\n# dataset path\npath = '..\/input\/massachusetts-buildings-dataset\/png\/'\n\n# load dataset A\ndataA1 = load_images(path + 'train\/')\ndataAB = load_images(path + 'test\/')\ndataA = vstack((dataA1, dataAB))\nprint('Loaded dataA: ', dataA.shape)\n\n# load dataset B\n# dataB1 = load_images(path + 'trainB\/')\n# dataB2 = load_images(path + 'testB\/')\n# dataB = vstack((dataB1, dataB2))\n# print('Loaded dataB: ', dataB.shape)\n\n# save as compressed numpy array (Both domains; train and test)\n# filename = '\/content\/drive\/MyDrive\/noise2text\/noise2text_256.npz'\n# savez_compressed(filename, dataA, dataB)\n# print('Saved dataset: ', filename)","4ae95c39":"dataset = pd.read_csv(\"..\/input\/text-trail\/tamil_thirukkural_train.csv\")\nsub_df= dataset.iloc[0]","f9e4f007":"text_img =[]\nRand_number_for_color = randint(0, 255)\nfor i in range(0,137):\n  img = Image.new('RGB', (256, 256), color = (Rand_number_for_color, Rand_number_for_color, Rand_number_for_color))\n  d = ImageDraw.Draw(img)\n  d.text((5,5), dataset.at[i,'explanation'], fill=(255,255,0))\n  n = np.array(img)\n  text_img.append(n)\n\n","99b6b3d9":"text_img_test =[]\nfor i in range(137,147):\n  img = Image.new('RGB', (256, 256), color = (Rand_number_for_color, Rand_number_for_color, Rand_number_for_color))\n  d = ImageDraw.Draw(img)\n  d.text((5,5), dataset.at[i,'explanation'], fill=(255,255,0))\n  n = np.array(img)\n  text_img_test.append(n)\n","81f245ed":"list_of_gen_img_train = np.array(text_img)\nlist_of_gen_img_test = np.array(text_img_test)","3b776407":"dataA = vstack((dataA1, dataAB))\ndataB = vstack((list_of_gen_img_train, list_of_gen_img_test))","fb9d87b7":"filename = 'CS_text.npz'\nsavez_compressed(filename, dataA, dataB)\nprint('Saved dataset: ', filename)","7b2bd5cb":"# load and plot the prepared dataset\nfrom numpy import load\nfrom matplotlib import pyplot\n\n# load the dataset\ndata = load('CS_text.npz')\ndataA, dataB = data['arr_0'], data['arr_1']\nprint('Loaded: ', dataA.shape, dataB.shape)\n\n# plot source images\nn_samples = 3\nfor i in range(n_samples):\n\tpyplot.subplot(3, n_samples, 1 + i)\n\tpyplot.axis('off')\n\tpyplot.imshow(dataA[i].astype('uint8'))\n \n# plot target image\nfor i in range(n_samples):\n\tpyplot.subplot(3, n_samples, 1 + n_samples + i)\n\tpyplot.axis('off')\n\tpyplot.imshow(dataB[i].astype('uint8'))\npyplot.show()","26d417d1":"# define the discriminator model\ndef define_discriminator(image_shape):\n\n\t# weight initialization\n\tinit = RandomNormal(stddev=0.02)\n\t# source image input\n\tin_image = Input(shape=image_shape)\n\t# C64\n\td = Conv2D(64, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(in_image)\n\td = LeakyReLU(alpha=0.2)(d)\n\t# C128\n\td = Conv2D(128, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n  # \u201caxis\u201d argument is set to -1 to ensure that features are normalized per feature map.\n\td = InstanceNormalization(axis=-1)(d)\n\td = LeakyReLU(alpha=0.2)(d)\n\t# C256\n\td = Conv2D(256, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n\td = InstanceNormalization(axis=-1)(d)\n\td = LeakyReLU(alpha=0.2)(d)\n\t# C512\n\td = Conv2D(512, (4,4), strides=(2,2), padding='same', kernel_initializer=init)(d)\n\td = InstanceNormalization(axis=-1)(d)\n\td = LeakyReLU(alpha=0.2)(d)\n\t# second last output layer\n\td = Conv2D(512, (4,4), padding='same', kernel_initializer=init)(d)\n\td = InstanceNormalization(axis=-1)(d)\n\td = LeakyReLU(alpha=0.2)(d)\n\t# patch output\n\tpatch_out = Conv2D(1, (4,4), padding='same', kernel_initializer=init)(d)\n\t# define model\n\tmodel = Model(in_image, patch_out)\n\t# compile model\n\tmodel.compile(loss='mse', optimizer=Adam(lr=0.0002, beta_1=0.5), loss_weights=[0.5])\n \n\treturn model","5230ede0":"# Generator: ResNet block\n#### Probably we can ignore this\ndef resnet_block(n_filters, input_layer):\n\t# weight initialization\n\tinit = RandomNormal(stddev=0.02)\n\t# first layer convolutional layer\n\tg = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(input_layer)\n\tg = InstanceNormalization(axis=-1)(g)\n\tg = Activation('relu')(g)\n\t# second convolutional layer\n\tg = Conv2D(n_filters, (3,3), padding='same', kernel_initializer=init)(g)\n\tg = InstanceNormalization(axis=-1)(g)\n\t# concatenate merge channel-wise with input layer\n\tg = Concatenate()([g, input_layer])\n\treturn g","fab7acc6":"### 9-ResNet blocks for 256x256 images for generator\n\n# define the standalone generator model\ndef define_generator(image_shape, n_resnet=9):\n\t# weight initialization\n\tinit = RandomNormal(stddev=0.02)\n\t# image input\n\tin_image = Input(shape=image_shape)\n\t# c7s1-64\n\tg = Conv2D(64, (7,7), padding='same', kernel_initializer=init)(in_image)\n\tg = InstanceNormalization(axis=-1)(g)\n\tg = Activation('relu')(g)\n\t# d128\n\tg = Conv2D(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n\tg = InstanceNormalization(axis=-1)(g)\n\tg = Activation('relu')(g)\n\t# d256\n\tg = Conv2D(256, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n\tg = InstanceNormalization(axis=-1)(g)\n\tg = Activation('relu')(g)\n\t# R256\n\tfor _ in range(n_resnet):\n\t\tg = resnet_block(256, g)\n\t# u128\n\tg = Conv2DTranspose(128, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n\tg = InstanceNormalization(axis=-1)(g)\n\tg = Activation('relu')(g)\n\t# u64\n\tg = Conv2DTranspose(64, (3,3), strides=(2,2), padding='same', kernel_initializer=init)(g)\n\tg = InstanceNormalization(axis=-1)(g)\n\tg = Activation('relu')(g)\n\t# c7s1-3\n\tg = Conv2D(3, (7,7), padding='same', kernel_initializer=init)(g)\n\tg = InstanceNormalization(axis=-1)(g)\n\tout_image = Activation('tanh')(g)\n\t# define model\n\tmodel = Model(in_image, out_image)\n\treturn model","cb001a9a":"# define a composite model for updating generators by adversarial and cycle loss\ndef define_composite_model(g_model_1, d_model, g_model_2, image_shape):\n\t# ensure the model we're updating is trainable\n\tg_model_1.trainable = True\n\t# mark discriminator as not trainable\n\td_model.trainable = False\n\t# mark other generator model as not trainable\n\tg_model_2.trainable = False\n\t# discriminator element\n\tinput_gen = Input(shape=image_shape)\n\tgen1_out = g_model_1(input_gen)\n\toutput_d = d_model(gen1_out)\n\t# identity element\n\tinput_id = Input(shape=image_shape)\n\toutput_id = g_model_1(input_id)\n\t# forward cycle\n\toutput_f = g_model_2(gen1_out)\n\t# backward cycle\n\tgen2_out = g_model_2(input_id)\n\toutput_b = g_model_1(gen2_out)\n\t# define model graph\n\tmodel = Model([input_gen, input_id], [output_d, output_id, output_f, output_b])\n\t# define optimization algorithm configuration\n\topt = Adam(lr=0.0002, beta_1=0.5)\n\t# compile model with weighting of least squares loss and L1 loss\n\tmodel.compile(loss=['mse', 'mae', 'mae', 'mae'], loss_weights=[1, 5, 10, 10], optimizer=opt)\n\treturn model","9ceb118c":"# load and prepare training images\ndef load_real_samples(filename):\n\t# load the dataset\n\tdata = load(filename)\n\t# unpack arrays\n\tX1, X2 = data['arr_0'], data['arr_1']\n\t# scale from [0,255] to [-1,1]\n\tX1 = (X1 - 127.5) \/ 127.5\n\tX2 = (X2 - 127.5) \/ 127.5\n\treturn [X1, X2]","6434ee99":"# select a batch of random samples, returns images and target\ndef generate_real_samples(dataset, n_samples, patch_shape):\n\t# choose random instances\n\tix = randint(0, dataset.shape[0], n_samples)\n\t# retrieve selected images\n\tX = dataset[ix]\n\t# generate 'real' class labels (1)\n\ty = ones((n_samples, patch_shape, patch_shape, 1))\n\treturn X, y","9bd7f811":"# generate a batch of images, returns images and targets\ndef generate_fake_samples(g_model, dataset, patch_shape):\n\t# generate fake instance\n\tX = g_model.predict(dataset)\n\t# create 'fake' class labels (0)\n\ty = zeros((len(X), patch_shape, patch_shape, 1))\n\treturn X, y","b331fc5c":"# save the generator models to file\ndef save_models(step, g_model_AtoB, g_model_BtoA):\n\t# save the first generator model\n\tfilename1 = 'g_model_AtoB_%06d.h5' % (step+1)\n\tg_model_AtoB.save(filename1)\n\t# save the second generator model\n\tfilename2 = 'g_model_BtoA_%06d.h5' % (step+1)\n\tg_model_BtoA.save(filename2)\n\tprint('>Saved: %s and %s' % (filename1, filename2))","76f2f94a":"# generate samples and save as a plot and save the model\ndef summarize_performance(step, g_model, trainX, name, n_samples=5):\n\t# select a sample of input images\n\tX_in, _ = generate_real_samples(trainX, n_samples, 0)\n\t# generate translated images\n\tX_out, _ = generate_fake_samples(g_model, X_in, 0)\n\t# scale all pixels from [-1,1] to [0,1]\n\tX_in = (X_in + 1) \/ 2.0\n\tX_out = (X_out + 1) \/ 2.0\n\t# plot real images\n\tfor i in range(n_samples):\n\t\tpyplot.subplot(2, n_samples, 1 + i)\n\t\tpyplot.axis('off')\n\t\tpyplot.imshow(X_in[i])\n\t# plot translated image\n\tfor i in range(n_samples):\n\t\tpyplot.subplot(2, n_samples, 1 + n_samples + i)\n\t\tpyplot.axis('off')\n\t\tpyplot.imshow(X_out[i])\n\t# save plot to file\n\tfilename1 = '%s_generated_plot_%06d.png' % (name, (step+1))\n\tpyplot.savefig(filename1)\n\tpyplot.close()","8d4a4fce":"# update image pool for fake images\ndef update_image_pool(pool, images, max_size=50):\n\tselected = list()\n\tfor image in images:\n\t\tif len(pool) < max_size:\n\t\t\t# stock the pool\n\t\t\tpool.append(image)\n\t\t\tselected.append(image)\n\t\telif random() < 0.5:\n\t\t\t# use image, but don't add it to the pool\n\t\t\tselected.append(image)\n\t\telse:\n\t\t\t# replace an existing image and use replaced image\n\t\t\tix = randint(0, len(pool))\n\t\t\tselected.append(pool[ix])\n\t\t\tpool[ix] = image\n\treturn asarray(selected)","41940856":"# train cyclegan models\n### 2 generator, 2 discriminator, 2 composite\ndef train(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset):\n\t# define properties of the training run\n  #### epochs: changed to 10 to fasten the work a bit\n\tn_epochs, n_batch, = 100, 1\n\t# determine the output square shape of the discriminator\n\tn_patch = d_model_A.output_shape[1]\n\t# unpack dataset\n\ttrainA, trainB = dataset\n\t# prepare image pool for fakes\n\tpoolA, poolB = list(), list()\n\t# calculate the number of batches per training epoch\n\tbat_per_epo = int(len(trainA) \/ n_batch)\n\t# calculate the number of training iterations\n\tn_steps = bat_per_epo * n_epochs\n\t# manually enumerate epochs\n\tfor i in range(n_steps):\n\t\t# select a batch of real samples\n\t\tX_realA, y_realA = generate_real_samples(trainA, n_batch, n_patch)\n\t\tX_realB, y_realB = generate_real_samples(trainB, n_batch, n_patch)\n\t\t# generate a batch of fake samples\n\t\tX_fakeA, y_fakeA = generate_fake_samples(g_model_BtoA, X_realB, n_patch)\n\t\tX_fakeB, y_fakeB = generate_fake_samples(g_model_AtoB, X_realA, n_patch)\n\t\t# update fakes from pool\n\t\tX_fakeA = update_image_pool(poolA, X_fakeA)\n\t\tX_fakeB = update_image_pool(poolB, X_fakeB)\n\t\t# update generator B->A via adversarial and cycle loss\n\t\tg_loss2, _, _, _, _  = c_model_BtoA.train_on_batch([X_realB, X_realA], [y_realA, X_realA, X_realB, X_realA])\n\t\t# update discriminator for A -> [real\/fake]\n\t\tdA_loss1 = d_model_A.train_on_batch(X_realA, y_realA)\n\t\tdA_loss2 = d_model_A.train_on_batch(X_fakeA, y_fakeA)\n\t\t# update generator A->B via adversarial and cycle loss\n\t\tg_loss1, _, _, _, _ = c_model_AtoB.train_on_batch([X_realA, X_realB], [y_realB, X_realB, X_realA, X_realB])\n\t\t# update discriminator for B -> [real\/fake]\n\t\tdB_loss1 = d_model_B.train_on_batch(X_realB, y_realB)\n\t\tdB_loss2 = d_model_B.train_on_batch(X_fakeB, y_fakeB)\n\t\t# summarize performance\n\t\tprint('>%d, dA[%.3f,%.3f] dB[%.3f,%.3f] g[%.3f,%.3f]' % (i+1, dA_loss1,dA_loss2, dB_loss1,dB_loss2, g_loss1,g_loss2))\n\t\t# evaluate the model performance every so often\n\t\tif (i+1) % (bat_per_epo * 1) == 0:\n\t\t\t# plot A->B translation\n\t\t\tsummarize_performance(i, g_model_AtoB, trainA, 'AtoB')\n\t\t\t# plot B->A translation\n\t\t\tsummarize_performance(i, g_model_BtoA, trainB, 'BtoA')\n\t\tif (i+1) % (bat_per_epo * 5) == 0:\n\t\t\t# save the models\n\t\t\tsave_models(i, g_model_AtoB, g_model_BtoA)","8540087c":"# load image data\ndataset = load_real_samples('CS_text.npz')\n\nprint('Loaded', dataset[0].shape, dataset[1].shape)\n\n# define input shape based on the loaded dataset\nimage_shape = dataset[0].shape[1:]\n\n# generator: A -> B\ng_model_AtoB = define_generator(image_shape)\n# generator: B -> A\ng_model_BtoA = define_generator(image_shape)\n\n# discriminator: A -> [real\/fake]\nd_model_A = define_discriminator(image_shape)\n# discriminator: B -> [real\/fake]\nd_model_B = define_discriminator(image_shape)\n\n# composite: A -> B -> [real\/fake, A]\nc_model_AtoB = define_composite_model(g_model_AtoB, d_model_B, g_model_BtoA, image_shape)\n# composite: B -> A -> [real\/fake, B]\nc_model_BtoA = define_composite_model(g_model_BtoA, d_model_A, g_model_AtoB, image_shape)\n\n# train models\ntrain(d_model_A, d_model_B, g_model_AtoB, g_model_BtoA, c_model_AtoB, c_model_BtoA, dataset)\n\n","899c314a":"# select a random sample of images from the dataset\ndef select_sample(dataset, n_samples):\n\t# choose random instances\n\tix = randint(0, dataset.shape[0], n_samples)\n\t# retrieve selected images\n\tX = dataset[ix]\n\treturn X","019642df":"# plot the image, the translation, and the reconstruction\ndef show_plot(imagesX, imagesY1, imagesY2):\n\timages = vstack((imagesX, imagesY1, imagesY2))\n\ttitles = ['Real', 'Generated', 'Reconstructed']\n\t# scale from [-1,1] to [0,1]\n\timages = (images + 1) \/ 2.0\n\t# plot images row by row\n\tfor i in range(len(images)):\n\t\t# define subplot\n\t\tpyplot.subplot(1, len(images), 1 + i)\n\t\t# turn off axis\n\t\tpyplot.axis('off')\n\t\t# plot raw pixel data\n\t\tpyplot.imshow(images[i])\n\t\t# title\n\t\tpyplot.title(titles[i])\n\tpyplot.show()","70764128":"# load dataset\nA_data, B_data = load_real_samples('.\/CS_text.npz')\nprint('Loaded', A_data.shape, B_data.shape)","c1eba2ca":"#### Specify how to load the InstanceNormalization layer while loading the genenrator model\n# load the models\n## Take the generators at epoch 7 \ncust = {'InstanceNormalization': InstanceNormalization}\nmodel_AtoB = load_model('.\/g_model_AtoB_014700.h5', cust)\nmodel_BtoA = load_model('.\/g_model_BtoA_014700.h5', cust)","b7502a94":"## Select image from Domain-A, convert it to B by generator AtoB\n## then reconstruct it to A again by Generator BtoA\n\n# plot A->B->A\nA_real = select_sample(A_data, 1)\nB_generated  = model_AtoB.predict(A_real)\nA_reconstructed = model_BtoA.predict(B_generated)\nshow_plot(A_real, B_generated, A_reconstructed)","f3536c03":"# plot B->A->B\nB_real = select_sample(B_data, 1)\nA_generated  = model_BtoA.predict(B_real)\nB_reconstructed = model_AtoB.predict(A_generated)\nshow_plot(B_real, A_generated, B_reconstructed)\n","c526055f":"## Compress together","accbd581":"# 2. Develop Cycle GAN\n(Use keras; output -> Color Images of size 256x256) \\\n- The architecture is comprised of **four** models, **two discriminator** models, and **two generator** models.\n\n","d706c55e":"- **Discriminator** models are trained directly on **real and generated** images.\n- Generator models are trained via their discriminator models to **minimize** the **adversarial loss** [For discriminator decibt]. (**L2 distance** between model out and target values '1: real', '0: fake')\n- Also, generators have \"**cycle loss**\" [For regeneration]. (2 losses: Fwd + Bwd)\n- Finally, **Identity loss** [Maintain same image if it's originally from the target domain] (1 Loss)\n\n== Both **cycle and Identity loss** are **L1 distance ** between input and output image for each translation \\\n== A composite model is built to allow sharing the weights of the generator with the related discriminator. \\\n- The composite model has 2 inputs (images from A and B) and 4 outputs (discriminator out, identity generated image, forward-cycle generated img, bwd cycle img)\n\n\n","9325b3b7":"## 2.2. Generator Model\n- Generator is an **encoder-decoder** model.\n- Model takes **source image** (domain A) and generates **target image** (domain B).\n1. Downsample (encode) input (by bottleNeck)\n2. Interpret this by **ResNet**, then series of **upsample** or decode to return back to original size.\n\n### ResNet: Used to solve the problem of the DL models having exploding or vanishing gradients.","026eb763":"# 4. Model performance\n## 4.1. Save generator models each epoch","097c2fb8":"We have 2 generators and 2 discriminators corresponding to each domain of images (could be the real images and the stegano one; re-check the paper to know what are your 2 domains of images)\n\n# 1.\\\nDomain-A -> Discriminator-A -> [Real\/Fake] \\\nDomain-B -> Generator-A -> Discriminator-A -> [Real\/Fake]\n\nDomain-B -> Discriminator-B -> [Real\/Fake]\\\nDomain-A -> Generator-B -> Discriminator-B -> [Real\/Fake]\n\n#2.\\\nCycle Consistency \\\n((Genertor A: means it generate sth as domain A, but takes input as anything except domain A)) \\\n**Domain-B** -> **Generator-A** -> **Domain-A** -> Generator-B -> Domain-B\n#3.\\\nIdentity Mapping (result in better color profile)","aaae85c2":"## 3.1. Selecting Random batches of images from each domain for the discriminator and composite generator","521eb583":"# 0. Import Libraries","1e1c3602":"Note that GANs don't converge rather they reach an **equilibrium** state, thus we can decide when to **stop** (epochs) based on the **generated image quality** (save generator each n epochs and generate with it)","130c1389":"**Trials**\n1. For 5 epochs and (37, 38 train images) => no change in the images just bluring or little color distortion.\n2. For 100 epochs and same number of images => just makes distortions in the images but not generate anything (we need large number of dataset)\n\n===> If this didn't work too then we must provide some sort of large dataset","a0feaf37":"## 2.1. Discriminator Model\n\n- **Discriminator** is CNN for **image classification**; one for each domain.\n\n- CycleGAN **discriminator**: `Convolutional-BatchNorm-LeakyReLU` but uses **InstanceNormalization** instead of BatchNormalization; standardizing the values on **each output feature map**, rather than across features in a batch. Instance norm has implementation in [keras contrib project](https:\/\/github.com\/keras-team\/keras-contrib).\n\n- The discriminator model takes a 256\u00d7256 sized image as input and **outputs** a **patch of predictions 70x70**","c7e9a16d":"## 1.1. Load text files and convert to images","ac6c7516":"#### Note: Image Pool for discriminator","e9ad26d2":"# 3. Load the images \n### As a list of 2 numpy array (coming from the compressed NPZ) [1st source image, 2nd target image]","af0e9320":"We need to create a composite model for each generator: Generator-A (input: B, out: A) and Generator B. \\\n- Here: A -> Horse \\\n\n**Generator-A Composite Model** (BtoA or Zebra to Horse)\n\nThe inputs, transformations, and outputs of the model are as follows:\n\n**Adversarial Loss**: Domain-B -> Generator-A -> Domain-A -> Discriminator-A -> [real\/fake]\n**Identity Loss**: Domain-A -> Generator-A -> Domain-A\n**Forward Cycle Loss**: Domain-B -> Generator-A -> Domain-A -> Generator-B -> Domain-B\n**Backward Cycle Loss**: Domain-A -> Generator-B -> Domain-B -> Generator-A -> Domain-A\nWe can summarize the inputs and outputs as:\n\nInputs: Domain-B, Domain-A\nOutputs: Real\/Fake, Domain-A, Domain-B, Domain-A\n","5ec9d00d":"## 4.2. Plot Sample of source images (1st row) against the generated images (2nd row)","9049aa76":"# 5. Train Models\nThe batch size is 1, so the number of iterations in each epoch. Models are saved every 5 epochs.\n\nImages are generated using both generators each epoch.\n\nBatch of real images from each domain is selected then a batch of fake images is generated to update each discriminator's fake image pool.\n\nGenerator-A is updated by the composite model then by discriminator-A.","dff42b2f":"## 3.2. Select Sample of generated images to update the discriminator","719215a2":"# 7. Beneficial Extensions (General)\n\n- Smaller Image Size. Update the example to use a smaller image size, such as 128\u00d7128\n- Adjust the size of the generator model to use 6 ResNet layers as is used in the cycleGAN paper.\n- Different Dataset. Update the example to use the apples to oranges dataset.\n- Without Identity Mapping. Update the example to train the generator models without the identity mapping and compare results.","908df2b2":"# 1. Load the images [train+test; 2 domains) into two arrays then compress the 2 arrays in one compressed array ","6ac86eb0":"# 6. Image Translation with Cycle-GAN (Example)","e031d580":"## 1.2. Load the compressed images and plot"}}