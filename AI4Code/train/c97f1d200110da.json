{"cell_type":{"fbdf83c5":"code","d0315701":"code","daf8bad1":"code","89ba4681":"code","c1bd8dcf":"code","c9ec8713":"code","962654ec":"code","e2c80907":"code","e0e60ae1":"code","58170e20":"code","c9a62658":"code","10105d60":"code","134b58d9":"code","09ac8aa1":"code","bce8d7e9":"code","7601b9a8":"code","71cab106":"code","92dbe928":"code","438bbc72":"code","eee43e81":"code","7f4d9d93":"code","7d9c42ce":"code","68ce6889":"code","f174fee7":"code","69b1d2d0":"code","ca112216":"code","7bc05689":"code","9b835d56":"code","ccc7da30":"code","bf4fe107":"code","758c4a8c":"code","7ed1c185":"code","c5393171":"code","0ab0c841":"code","ca96488f":"code","4a0a949f":"code","3542ea2a":"code","2644be6a":"code","233dce8d":"markdown","bacb0bfc":"markdown","cb3b1147":"markdown","236f7465":"markdown","97b323fd":"markdown","7fc5b171":"markdown","fbb46f20":"markdown"},"source":{"fbdf83c5":"# First of all, we need to load the CSV file that contains all the data\n\nimport os\nimport pandas as pd\n\n# Uncomment the following lines for loading the dataset in a Kaggle notebook\nroot = '\/kaggle\/input'\nds_name = 'facial-expression-recognitionferchallenge\/fer2013\/fer2013\/fer2013.csv'\nfp = os.path.join(root, ds_name) # Dataset file path\n\n# Uncomment the following lines for loading the dataset in a local environment\n# root = os.getcwd()\n# ds_name = \"facial-expression-recognitionferchallenge\/fer2013\/fer2013.csv\"\n# fp = os.path.join(root, ds_name)\n\ndf = pd.read_csv(fp)\ndf.head()","d0315701":"# Images' pixels are stored as strings (listing all the pixel values), so we need to parse\n# and transform them into lists of floats\n\nimport numpy as np\n\nimg_pixels =  [np.asarray(x, dtype=float) for x in df.pixels.str.split(' ').tolist()]\nimg_pixels[:5] # Print some samples to check","daf8bad1":"# Moreover, we proceed to encode emotion numbers in a one-hot fashion\n\ndef value_to_onehot(x):\n    return np.eye(7, dtype=float)[x]\n\nemotions_onehot = [value_to_onehot(x) for x in df.emotion]\nemotions_onehot[:5]","89ba4681":"# We also add a column with text labels corresponding to emotion numbers\n# (taken from https:\/\/www.kaggle.com\/msambare\/fer2013)\n\nemotions = ['anger', 'disgust', 'fear', 'happiness', 'sadness', 'surprise', 'neutral']\nemotion_names = [emotions[x] for x in df.emotion]\nemotion_names[:5]","c1bd8dcf":"# After that, we create a new DataFrame with the cleaned data\n\nd = {'usage': df.Usage.values, 'image': img_pixels, 'emotion_idx': df.emotion, 'emotion_onehot': emotions_onehot, 'emotion_name': emotion_names}\ndf2 = pd.DataFrame(data=d) # Create a new dataset with the correct format\ndf2.head()","c9ec8713":"# Start by checking how the dataset is split\n\nimport plotly.express as px\n\npx.pie(df2.groupby(\"usage\").size().reset_index(), names=\"usage\", values=0, title=\"Number of samples per dataset\")","962654ec":"# Then, check if the various classes are balanced (number of samples per class)\n# Notice that the class 1 (\"disgust\") is a bit under-represented (we will tackle this problem later)\n\npx.bar(df2.groupby([\"emotion_name\", \"usage\"]).size().reset_index(), x=\"emotion_name\", y=0, color=\"usage\" ,title=\"Class distribution of samples\")","e2c80907":"# Check if there are some empty (black) images in the dataset\n\nblack = df2.image.apply(np.max) == 0\ndf2[black]","e0e60ae1":"# Proceed to exclude black images from the dataset\n\ndf3 = df2[np.logical_not(black)]\ndf3.head()","58170e20":"# After that, we proceed to visualize some of the images in the dataset, just to\n# visualize better what we are working with\n\nimport matplotlib.pyplot as plt\n\nrow, col = 2, 5\nimg_size = 14 # Size of each picture in the plot\nfig, axes = plt.subplots(row, col, figsize=(img_size,img_size*row\/col))\nfig.tight_layout()\nfig.subplots_adjust(hspace=.25)\n\n# Print row*col random images\nfor i in range(row):\n    for j in range(col):\n        ax = axes[i,j]\n        k = np.random.randint(len(df3.image)) # Pick a random image in the dataset\n        img = df3.image[k].reshape(48,48)\n        lbl = df3.emotion_name[k].capitalize() # Label for the image\n        ax.set_title(f'{lbl}')\n        ax.imshow(img, cmap='gray')","c9a62658":"# Perform PCA to check whether the images are clustered in some way\n\nfrom sklearn.decomposition import PCA\n\npca = PCA(3)\nt = df3.image.values.copy()\nt = np.array([(x-x.min()) \/ (x.max() - x.min()) for x in t])\nt = pca.fit_transform(t)\npx.scatter_3d(x=t[:,0], y=t[:,1], z=t[:,2], color=df3.emotion_idx)","10105d60":"# Split the dataset (training, public testing, and private testing)\n\ntrain = df3[df3.usage == 'Training'].reset_index(drop=True)\npub_test = df3[df3.usage == 'PublicTest'].reset_index(drop=True)\npriv_test = df3[df3.usage == 'PrivateTest'].reset_index(drop=True)","134b58d9":"train.head()","09ac8aa1":"pub_test.head()","bce8d7e9":"priv_test.head()","7601b9a8":"# Random oversampling to balance the dataset\n# Applied only on the training set\n\nfrom imblearn.over_sampling import RandomOverSampler\n\noversampler = RandomOverSampler()\ntrain, _ = oversampler.fit_resample(train, train.emotion_idx)\ntrain = train.sample(frac=1).reset_index(drop=True) # Shuffle the training set after oversampling in order to avoid blocks with same label\npx.bar(train.groupby([\"emotion_name\"]).size().reset_index(), x=\"emotion_name\", y=0,title=\"Class distribution of samples\")","71cab106":"# Create the class for representing an image dataset, which will be used\n# to feed into the dataloaders\n# Notice that we implement the possibility of applying transformations\n# to the data before being fed into the dataloader\n# It is possible to compose many transformations with the following command:\n# t = transforms.Compose([transform_1(), transform_2(), ...])\n\nimport torch\nfrom torch.utils.data import Dataset\n\nclass ImgDataset(Dataset):\n    \n    def __init__(self, df, transform=None):\n        self.df = df\n        self.transform = transform\n    \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx.tolist()\n        img = self.df.image[idx]\n        lbl = self.df.emotion_idx[idx] # Label is index (i.e. argmax of one-hot representation)\n        sample = (img, lbl) # Sample is the pair (features, target)\n        if self.transform:\n            sample = self.transform(sample)\n        return sample","92dbe928":"# Implement transformation that performs data augmentation\n# Transformations applied to images are: rotate, warp, flip (left\/right), gaussian blur, random noise\n# https:\/\/www.analyticsvidhya.com\/blog\/2019\/12\/image-augmentation-deep-learning-pytorch\/\n\nfrom skimage.transform import rotate, warp, AffineTransform\nfrom skimage.filters import gaussian\nfrom skimage.util import random_noise\nfrom numpy.random import *\n\nclass Augment(object):\n    \n    def __init__(self, rotate=100, warp=100, flip_lr=100, gaussian=100, random_noise=100): # Probability of each certain transformation (over 100)\n        self.rotate = rotate\n        self.warp = warp\n        self.flip_lr = flip_lr\n        self.gaussian = gaussian\n        self.random_noise = random_noise\n    \n    def __call__(self, sample):\n        img, lbl = sample\n        if randint(0,100) < self.rotate: img = rotate(img, angle=uniform(-30,30), mode='edge')\n        if randint(0,100) < self.warp: img = warp(img, AffineTransform(translation=(uniform(-7,7), uniform(-7,7))), mode='edge')\n        if randint(0,100) < self.flip_lr: img = [img, np.fliplr(img)][choice([0,1])]\n        if randint(0,100) < self.gaussian: img = gaussian(img, sigma=uniform(.0,.8), multichannel=True)\n        # if randint(0,100) < self.gaussian: img = gaussian(img, sigma=uniform(.0,.8), channel_axis=0) # Newer version of gaussian blur library, not available in Kaggle\n        if randint(0,100) < self.random_noise: img = random_noise(img\/255, var=uniform(.0,.0008))*255\n        return (img, lbl)","438bbc72":"# Implement transformation that reshapes images from 1D array to 2D matrix (48x48)\n\nclass Reshape(object):\n    \n    def __init__(self, one_channel=False):\n        self.one_channel = one_channel\n    \n    def __call__(self, sample):\n        img, lbl = sample\n        r = None\n        if self.one_channel:\n            r = img.reshape(1, 48, 48)\n        else:\n            r = img.reshape(48, 48)\n        return (r, lbl)","eee43e81":"# Implement transformation that rescales values form [0,255] to [0,1]\n\nclass Rescale(object):\n    \n    def __call__(self, sample):\n        img, lbl = sample\n        return (img\/255.0, lbl)","7f4d9d93":"# Set up dataloaders for train, public test, and private test sets\n\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\n\nbatch_size = 128\n\ntrain_transform = transforms.Compose([Reshape(one_channel=False), Augment(), Rescale(), Reshape(one_channel=True)])\ntrain_ds = ImgDataset(train, train_transform)\n\ntest_transform = transforms.Compose([Reshape(one_channel=False), Rescale(), Reshape(one_channel=True)]) # No augmentation performed on test sets\npub_test_ds = ImgDataset(pub_test, test_transform)\npriv_test_ds = ImgDataset(priv_test, test_transform)\n\ntrain_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=0)\npub_test_dl = DataLoader(pub_test_ds, batch_size=batch_size, shuffle=False, num_workers=0) # No need to shuffle test sets\npriv_test_dl = DataLoader(priv_test_ds, batch_size=batch_size, shuffle=False, num_workers=0)","7d9c42ce":"# Next, define the model that will be trained on the analyzed data\n# In PyTorhc, it is sufficient to define a class with a forward() method\n# Notice that we implemented some custom methods that compute partial runs\n# on the network, in order to be able to visualize what convolutional filters\n# do learn (we will mention this later in this notebook)\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BasicConvNet(nn.Module):\n    \n    def __init__(self):\n        super(BasicConvNet, self).__init__()\n        # self.b0 = nn.BatchNorm2d(1)\n        self.c10 = nn.Conv2d(1, 64, 1, padding='same')\n        self.c11 = nn.Conv2d(1, 64, 3, padding='same')\n        self.c12 = nn.Conv2d(64, 64, 3, padding='same')\n        self.b11 = nn.BatchNorm2d(64)\n        self.b12 = nn.BatchNorm2d(64)\n        self.c20 = nn.Conv2d(64, 128, 1, padding='same')\n        self.c21 = nn.Conv2d(64, 128, 3, padding='same')\n        self.c22 = nn.Conv2d(128, 128, 3, padding='same')\n        self.b21 = nn.BatchNorm2d(128)\n        self.b22 = nn.BatchNorm2d(128)\n        self.c30 = nn.Conv2d(128, 256, 1, padding='same')\n        self.c31 = nn.Conv2d(128, 256, 3, padding='same')\n        self.c32 = nn.Conv2d(256, 256, 3, padding='same')\n        self.b31 = nn.BatchNorm2d(256)\n        self.b32 = nn.BatchNorm2d(256)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.dropout = nn.Dropout(.2)\n        self.top1 = nn.Linear(256*6*6, 2048)\n        self.top2 = nn.Linear(2048, 256)\n        self.top3 = nn.Linear(256, 32)\n        self.top4 = nn.Linear(32, 7)\n    \n    def forward(self, x, training=False):\n        # x = self.b0(x)\n        # Residual block 1\n        y = F.leaky_relu(self.b11(self.c11(x)), negative_slope=.005)\n        y = self.b12(self.c12(y))\n        x = F.leaky_relu(self.c10(x) + y, negative_slope=.005)\n        x = self.pool(x)\n        # Residual block 2\n        y = F.leaky_relu(self.b21(self.c21(x)), negative_slope=.005)\n        y = self.b22(self.c22(y))\n        x = F.leaky_relu(self.c20(x) + y, negative_slope=.005)\n        x = self.pool(x)\n        # Residual block 3\n        y = F.leaky_relu(self.b31(self.c31(x)), negative_slope=.005)\n        y = self.b32(self.c32(y))\n        x = F.leaky_relu(self.c30(x) + y, negative_slope=.005)\n        x = self.pool(x)\n        # Top\n        x = torch.flatten(x, start_dim=1)\n        x = self.dropout(x)\n        x = F.leaky_relu(self.top1(x), negative_slope=.005)\n        x = F.leaky_relu(self.top2(x), negative_slope=.005)\n        x = F.leaky_relu(self.top3(x), negative_slope=.005)\n        x = F.softmax(self.top4(x), dim=1)\n        return x\n    \n    def getLayer1(self, x, training=False):\n        # x = self.b0(x)\n        # Residual block 1\n        y = F.leaky_relu(self.b11(self.c11(x)), negative_slope=.005)\n        y = self.b12(self.c12(y))\n        x = F.leaky_relu(self.c10(x) + y, negative_slope=.005)\n        x = self.pool(x)\n        return x\n    \n    def getLayer2(self, x, training=False):\n        # x = self.b0(x)\n        # Residual block 1\n        y = F.leaky_relu(self.b11(self.c11(x)), negative_slope=.005)\n        y = self.b12(self.c12(y))\n        x = F.leaky_relu(self.c10(x) + y, negative_slope=.005)\n        x = self.pool(x)\n        # Residual block 2\n        y = F.leaky_relu(self.b21(self.c21(x)), negative_slope=.005)\n        y = self.b22(self.c22(y))\n        x = F.leaky_relu(self.c20(x) + y, negative_slope=.005)\n        x = self.pool(x)\n        return x\n    \n    def getLayer3(self, x, training=False):\n        # x = self.b0(x)\n        # Residual block 1\n        y = F.leaky_relu(self.b11(self.c11(x)), negative_slope=.005)\n        y = self.b12(self.c12(y))\n        x = F.leaky_relu(self.c10(x) + y, negative_slope=.005)\n        x = self.pool(x)\n        # Residual block 2\n        y = F.leaky_relu(self.b21(self.c21(x)), negative_slope=.005)\n        y = self.b22(self.c22(y))\n        x = F.leaky_relu(self.c20(x) + y, negative_slope=.005)\n        x = self.pool(x)\n        # Residual block 3\n        y = F.leaky_relu(self.b31(self.c31(x)), negative_slope=.005)\n        y = self.b32(self.c32(y))\n        x = F.leaky_relu(self.c30(x) + y, negative_slope=.005)\n        x = self.pool(x)\n        return x","68ce6889":"# Select GPU if available\n# (Kaggle TPU not working!)\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)","f174fee7":"# Perform training of the defined model\n\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom sklearn.metrics import *\nfrom tqdm import tqdm\n\nnet = BasicConvNet().to(device)\nloss_func = nn.CrossEntropyLoss() # Because of multiclass classification task\noptimizer = optim.Adam(net.parameters(), lr=1e-4) # State-of-the-art optimizer\nscheduler = ExponentialLR(optimizer, gamma=0.93, verbose=True)\n\nepochs = 30\n\nhistory_train, history_val, history_loss = [], [], []\n\nfor epoch in range(epochs):\n    running_loss = 0.0\n    print(f'Epoch {epoch+1}')\n    \n    # Training\n    train_metrics = {'labels': [], 'outputs': [], 'n_samples': len(train)}\n\n    with tqdm(enumerate(train_dl, 0), total=len(train_dl), mininterval=1) as pbar:\n        for i, data in pbar:\n            inputs, labels = data # Labels are already index, not one-hot\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n            outputs = net(inputs.float(), training=True)\n            outputs_idx = outputs.argmax(axis=1)\n\n            train_metrics['labels'] += labels.tolist()\n            train_metrics['outputs'] += outputs_idx.tolist()\n\n            loss = loss_func(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n            if i % 100 == 99: # Update loss every 100 batches of data\n                history_loss.append(running_loss \/ 100)\n                pbar.set_description('loss: %.3f' % (running_loss \/ 100))\n                running_loss = 0.0\n                \n    scheduler.step() # Apply exponential decay on learning rate\n\n    acc = accuracy_score(train_metrics['labels'], train_metrics['outputs'])\n    print('Training accuracy: {acc:.2f}%'.format(acc=acc*100))\n    history_train.append(train_metrics)\n\n    # Validation (performed on pub_test set)\n    val_metrics = {'labels': [], 'outputs': [], 'n_samples': len(pub_test)}\n\n    for i, data in enumerate(pub_test_dl, 0):\n        inputs, labels = data\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        outputs = net(inputs.float())\n        outputs_idx = outputs.argmax(axis=1)\n\n        val_metrics['labels'] += labels.tolist()\n        val_metrics['outputs'] += outputs_idx.tolist()\n\n    acc = accuracy_score(val_metrics['labels'], val_metrics['outputs'])\n    print('Validation accuracy: {acc:.2f}%'.format(acc=acc*100))\n    history_val.append(val_metrics)","69b1d2d0":"# Training and validation metrics\n\nhistory_train_metrics = {\n    'accuracy_score': [accuracy_score(t['labels'], t['outputs']) for t in history_train],\n    'balanced_accuracy_score': [balanced_accuracy_score(t['labels'], t['outputs']) for t in history_train],\n    'precision_score': [precision_score(t['labels'], t['outputs'], average='macro') for t in history_train],\n    'recall_score': [recall_score(t['labels'], t['outputs'], average='macro') for t in history_train],\n    'f1_score': [f1_score(t['labels'], t['outputs'], average='macro') for t in history_train],\n    'balanced_accuracy_score': [balanced_accuracy_score(t['labels'], t['outputs']) for t in history_train],\n    'type': [\"Training\" for t in history_val],\n    'epoch': list(range(len(history_val)))\n}\n\nhistory_val_metrics = {\n    'accuracy_score': [accuracy_score(t['labels'], t['outputs']) for t in history_val],\n    'balanced_accuracy_score': [balanced_accuracy_score(t['labels'], t['outputs']) for t in history_val],\n    'precision_score': [precision_score(t['labels'], t['outputs'], average='macro') for t in history_val],\n    'recall_score': [recall_score(t['labels'], t['outputs'], average='macro') for t in history_val],\n    'f1_score': [f1_score(t['labels'], t['outputs'], average='macro') for t in history_val],\n    'balanced_accuracy_score': [balanced_accuracy_score(t['labels'], t['outputs']) for t in history_val],\n    'type': [\"Validation\" for t in history_val],\n    'epoch': list(range(len(history_val)))\n}","ca112216":"history_train_metrics = pd.DataFrame(history_train_metrics)\nhistory_val_metrics = pd.DataFrame(history_val_metrics)\nhistory_metrics = pd.concat([history_train_metrics, history_val_metrics]).reset_index(drop=True)\nhistory_metrics","7bc05689":"iter_per_epoch = len(train_dl) \/\/ 100\nhistory_loss = pd.DataFrame({'loss': history_loss, 'epoch': [t \/ iter_per_epoch for t in range(len(history_loss))]})\nhistory_loss","9b835d56":"# Accuracy plot\n\npx.line(history_metrics, x=\"epoch\", y=\"accuracy_score\", color=\"type\").show()","ccc7da30":"# Loss plot\n\npx.line(history_loss, x='epoch', y='loss')","bf4fe107":"# Confusion matrix\n\nConfusionMatrixDisplay(confusion_matrix(history_train[-1]['labels'], history_train[-1]['outputs']), display_labels=emotions).plot() # Training\nConfusionMatrixDisplay(confusion_matrix(history_val[-1]['labels'], history_val[-1]['outputs']), display_labels=emotions).plot() # Validation","758c4a8c":"# Scoring and confusion matrices on priv_test set\n\n    \n# Test (performed on priv_test set)\ntest_metrics = {'labels': [], 'outputs': [], 'n_samples': len(pub_test)}\n\nfor i, data in enumerate(priv_test_dl, 0):\n    inputs, labels = data\n    inputs = inputs.to(device)\n    labels = labels.to(device)\n\n    outputs = net(inputs.float())\n    outputs_idx = outputs.argmax(axis=1)\n\n    test_metrics['labels'] += labels.tolist()\n    test_metrics['outputs'] += outputs_idx.tolist()\n\nprint('Test accuracy: {acc:.2f}%'.format(acc=accuracy_score(test_metrics['labels'], test_metrics['outputs'])*100))\nprint('Test precision (macro): {acc:.2f}%'.format(acc=precision_score(test_metrics['labels'], test_metrics['outputs'], average=\"macro\")*100))\nprint('Test recall (macro): {acc:.2f}%'.format(acc=recall_score(test_metrics['labels'], test_metrics['outputs'], average=\"macro\")*100))\nprint('Test f1: {acc:.2f}%'.format(acc=f1_score(test_metrics['labels'], test_metrics['outputs'], average=\"macro\")*100))\nConfusionMatrixDisplay(confusion_matrix(history_val[-1]['labels'], history_val[-1]['outputs']), display_labels=emotions).plot() # Test","7ed1c185":"import matplotlib.pyplot as plt\n\nrow, col = 5, 5\nimg_size = 20 # Size of each picture in the plot\nfig, axes = plt.subplots(row, col, figsize=(img_size,img_size*row\/col))\nfig.tight_layout()\nfig.subplots_adjust(hspace=.25)\n\nimgs, lbls = next(iter(priv_test_dl))\n\npred = net(imgs.to(device).float()).argmax(axis=1).cpu()\n\n# Print row*col random images\nfor i in range(row):\n    for j in range(col):\n        ax = axes[i,j]\n        k = i * row + j\n        img = imgs[k].reshape(48,48)\n        lbl = \"Predicted: \" + emotions[pred[k]] + \" \\nActual: \" + emotions[lbls[k]] # Label for the image\n        ax.set_title(f'{lbl}')\n        ax.imshow(img, cmap='gray')","c5393171":"# Metrics save\n\nexp_path = '.\/out\/'\nos.makedirs(exp_path)\nhistory_metrics.to_csv(exp_path + 'history_metrics.csv')\nhistory_loss.to_csv(exp_path + 'history_loss.csv')\n\nhistory_full = pd.concat([pd.DataFrame(history_val).assign(type='Validation'), pd.DataFrame(history_train).assign(type='Training')])\nhistory_full.to_csv(exp_path + 'history_full.csv', header=True)","0ab0c841":"# Visualization of the learnt convolutional filters\n\ninputs, labels = next(iter(train_dl))\n\ninput_img = inputs[0][0].numpy()\n\ninputs = inputs.to(device)\nlabels = labels.to(device)\n\noutputs1 = net.getLayer1(inputs.float(), training=False)\noutputs1 = outputs1[0].cpu().detach().numpy()\n\noutputs2 = net.getLayer2(inputs.float(), training=False)\noutputs2 = outputs2[0].cpu().detach().numpy()\n\noutputs3 = net.getLayer3(inputs.float(), training=False)\noutputs3 = outputs3[0].cpu().detach().numpy()","ca96488f":"# Input image sample\n\nplt.imshow(input_img, cmap='gray')","4a0a949f":"# Some filters of the first layer\n\nrow, col = 2, 5\nimg_size = 14\nfig, axes = plt.subplots(row, col, figsize=(img_size,img_size*row\/col))\nfig.tight_layout()\nfig.subplots_adjust(hspace=.25)\n\nfor i in range(row):\n    for j in range(col):\n        ax = axes[i,j]\n        img = outputs1[i*col+j]\n        ax.imshow(img, cmap='gray')\n        ","3542ea2a":"# Some filters of the second layer\n\nrow, col = 2, 5\nimg_size = 14\nfig, axes = plt.subplots(row, col, figsize=(img_size,img_size*row\/col))\nfig.tight_layout()\nfig.subplots_adjust(hspace=.25)\n\nfor i in range(row):\n    for j in range(col):\n        ax = axes[i,j]\n        img = outputs2[i*col+j]\n        ax.imshow(img, cmap='gray')","2644be6a":"# Some filters of the third layer\n\nrow, col = 2, 5\nimg_size = 14\nfig, axes = plt.subplots(row, col, figsize=(img_size,img_size*row\/col))\nfig.tight_layout()\nfig.subplots_adjust(hspace=.25)\n\nfor i in range(row):\n    for j in range(col):\n        ax = axes[i,j]\n        img = outputs3[i*col+j]\n        ax.imshow(img, cmap='gray')","233dce8d":"# Dataset cleaning","bacb0bfc":"# Metrics and evaluation","cb3b1147":"# Dataset analysis and visualization","236f7465":"# Training","97b323fd":"# Prepare data for model","7fc5b171":"# Model definition","fbb46f20":"# Dataset loading"}}