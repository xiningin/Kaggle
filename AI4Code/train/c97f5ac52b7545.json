{"cell_type":{"72090090":"code","3db5ffa2":"code","48bd50be":"code","e3089163":"code","3507cff5":"code","28b3cfb8":"code","8e20b615":"code","7f4068b2":"code","fc48e7a0":"code","674d096a":"code","04164941":"code","9f43e5c3":"code","e836eb85":"code","7f238aef":"code","d4e03a91":"code","3e28b8d7":"code","7fb16634":"code","4aa68262":"code","fad65783":"code","fda70c4d":"code","5ae704b5":"code","f53b4420":"markdown","88fd954c":"markdown","7713a271":"markdown","fbdf08ca":"markdown","a2bc5075":"markdown","0f071217":"markdown","986587c9":"markdown","8f87e89a":"markdown","0e9f5b18":"markdown","a3a8c282":"markdown","d2939a0c":"markdown","91efc603":"markdown","ef5f9476":"markdown"},"source":{"72090090":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, classification_report, recall_score, roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import LogisticRegression\nsns.set()","3db5ffa2":"df_test = pd.read_csv(\"..\/input\/carinsurance\/carInsurance_test.csv\")\ndf_train = pd.read_csv(\"..\/input\/carinsurance\/carInsurance_train.csv\")","48bd50be":"print(\"The training data has {0} samples and {1} features.\".format(df_train.shape[0], df_train.shape[1]-1))\nprint(\"The testing data has {0} samples and {1} features.\".format(df_test.shape[0], df_test.shape[1]-1))","e3089163":"df_train.isnull().sum()","3507cff5":"df_test.isnull().sum()","28b3cfb8":"def hist_matrix(data):\n    numeric_cols = [col for col in data if data[col].dtype!= \"O\"]\n    fig, ax = plt.subplots(nrows = 4, ncols = 3,figsize = (16,10))\n    fig.subplots_adjust(hspace = 0.5)\n    x=0\n    y=0\n    for i in numeric_cols:\n        ax[y,x].hist(data[i])\n        ax[y,x].set_title(\"{}\".format(i))\n        x+=1\n        if x == 3:\n            x-=3\n            y+=1\n    return","8e20b615":"hist_matrix(df_train)","7f4068b2":"def preprocessing(data):\n    data = data.drop('Id', axis = 1)\n    data['Education'] = data['Education'].fillna(data['Education'].mode()[0])\n    data['Job'] = data['Job'].fillna(data['Job'].mode()[0])\n    for i in ['CallStart', 'CallEnd']:\n        data[i] = pd.to_datetime(data[i])\n    data['CallDur'] = ((data['CallEnd']-data['CallStart']).dt.seconds)\/60\n    data['CallHour'] = data['CallStart'].dt.hour\n    data = data.drop(['CallStart', 'CallEnd'], axis = 1)\n    for i in ['Balance', 'NoOfContacts', 'PrevAttempts', 'DaysPassed']:\n        val = data[i].quantile(.99).astype(int)\n        data.loc[data[i]>val, i] = val\n    data.loc[data['DaysPassed']<0, 'DaysPassed'] = 0\n    data['Communication'] = data['Communication'].fillna('Missing')\n    data['Outcome'] = data['Outcome'].fillna('Mising')\n    return data","fc48e7a0":"df_train = preprocessing(df_train)","674d096a":"df_train.isnull().sum()","04164941":"hist_matrix(df_train)","9f43e5c3":"y = df_train['CarInsurance'].copy()\nx_cols = [col for col in df_train.columns if col != \"CarInsurance\"]\nx = df_train[x_cols].copy()","e836eb85":"numeric_cols = [col for col in x if x[col].dtype != \"O\"]\nnumeric_transformer = Pipeline(steps = [(\n                        'scaler', MinMaxScaler())])\n\ncategorical_cols = [col for col in x if col not in numeric_cols]\ncategorical_transformer = Pipeline(steps=[(\n                            'ohe', OneHotEncoder(drop = 'first'))])\n\npreprocessor = ColumnTransformer(transformers = [\n                ('num', numeric_transformer, numeric_cols),\n                ('cat', categorical_transformer, categorical_cols)])","7f238aef":"models = ['Random Forest: ', 'Logistic Regression: ', 'XGBoost: ']\nsuffixes = ['_rf', '_logit', '_xgb']\nnames = ['fpr', 'tpr', 'thresholds', 'auc']\nj = 0\nfor i in [RandomForestClassifier(n_estimators = 100, random_state=0), LogisticRegression(solver = 'lbfgs'), XGBClassifier(random_state=0)]:\n    clf = Pipeline(steps = [\n            ('preprocessor', preprocessor),\n            ('classifier', i)])\n    x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 0)\n    clf.fit(x_train, y_train)\n    preds = clf.predict(x_test)\n    print(models[j])\n    print(\"Model Accuracy: {}\".format(round(accuracy_score(y_test, preds),4)*100))\n    print(classification_report(y_test, preds))\n    print(\"*****************************************************\")   \n    exec(\"{0}, {1}, {2} = roc_curve(y_test, clf.predict_proba(x_test)[:,1])\".format(names[0]+suffixes[j], names[1]+suffixes[j], names[2]+suffixes[j]))\n    exec(\"{} = roc_auc_score(y_test, clf.predict_proba(x_test)[:,1])\".format(names[3]+suffixes[j]))\n    j+=1","d4e03a91":"fig, ax = plt.subplots(nrows = 1, ncols = 3, figsize = (18,6))\nax[0].plot(fpr_rf,tpr_rf, label = \"Random Forest\")\nax[0].plot([0,1], [0,1], label = 'Base Rate')\nax[0].set_ylabel(\"True Positive Rate\")\nax[0].set_title(\"Random Forest\")\nax[0].text(0.3, 0.7, \"AUC = {}\".format(round(auc_rf, 2)))\nax[1].plot(fpr_logit,tpr_logit, label = \"Random Forest\")\nax[1].plot([0,1], [0,1], label = 'Base Rate')\nax[1].set_xlabel(\"False Positive Rate\")\nax[1].set_title(\"Logistic Regression\")\nax[1].text(0.3, 0.7, \"AUC = {}\".format(round(auc_logit, 2)))\nax[2].plot(fpr_xgb,tpr_xgb, label = \"Random Forest\")\nax[2].plot([0,1], [0,1], label = 'Base Rate')\nax[2].set_title(\"XGBoost\")\nax[2].text(0.3, 0.7, \"AUC = {}\".format(round(auc_xgb, 2)))\nfig.suptitle(\"ROC Graph\")","3e28b8d7":"clf = Pipeline(steps = [\n        ('preprocessor', preprocessor),\n        ('classifier', RandomForestClassifier(random_state=0))])\n\nparam_grid = {'classifier__n_estimators' : [10, 50, 75, 100, 150, 200, 250, 300],\n             'classifier__criterion' : ['gini', 'entropy']}\n\nsearch = GridSearchCV(clf, param_grid, n_jobs = -1, cv = 7)\nsearch.fit(x_train, y_train)\nprint(search.best_params_)","7fb16634":"clf = Pipeline(steps = [\n        ('preprocessor', preprocessor),\n        ('classifier', RandomForestClassifier(n_estimators = 200, criterion = 'gini', random_state=0))])\nclf.fit(x_train, y_train)\npreds = clf.predict(x_test)\nprint(\"Tuned Random Forest: \")\nprint(\"Model Accuracy: {}\".format(round(accuracy_score(y_test, preds),4)*100))\nprint(classification_report(y_test, preds))","4aa68262":"cm = confusion_matrix(y_test, preds)\nplt.figure(figsize=(8,6))\nsns.heatmap(cm, annot=True, fmt = \".0f\")\nplt.yticks([1.5,0.5], ['Did not Buy', 'Did Buy'])\nplt.xticks([1.5,0.5], ['Did Buy', 'Did not Buy'])\nplt.ylabel(\"Actual\")\nplt.xlabel(\"Predicted\")\nplt.title(\"Confusion Matrix\")","fad65783":"names = {\"feature\" : numeric_cols + list(clf['preprocessor'].transformers_[1][1]['ohe'].get_feature_names(categorical_cols))}\nimp = {'importances' : list(clf.steps[1][1].feature_importances_)}\nfeature_importances = {**names, **imp}\nfeature_importances_df = pd.DataFrame(feature_importances) ","fda70c4d":"feature_importances_df = feature_importances_df.sort_values(by = 'importances', ascending = True)\nfeature_importances_df","5ae704b5":"plt.figure(figsize = (16,10))\nplt.title(\"Feature Importances from Random Forest Classifier Model\")\nplt.barh(feature_importances_df['feature'], feature_importances_df['importances'])","f53b4420":"# Model Building\nNow I will split the training data into the targets and features. Then, I will create a pipeline that will (1) scale our numeric features, (2) one-hot-encode our categorical features, and (3) train a model on the data.","88fd954c":"It seems here that the Random Forest is the best performing model across all metrics. Now, I will tune some hyperparameters of the model using `GridSearchCV` to see if I can improve it.","7713a271":"We can now see that there are no missing values in the data and the distributions of our numeric variables are less extreme.","fbdf08ca":"We can see below that there are a number of features with missing information, namely `Job`, `Education`, `Communication`, and `Outcome`. We can also see that the same features have missing information in the testing set. As I mentioned, all of the labels for the testing set are missing, but we won't focus on those here, as I won't be testing the model on that set.","a2bc5075":"Now I will write a brief function to visualize the distributions of our numeric variables.","0f071217":"Now let's take a look at the feature importances of our model to get an idea of what is predicting sales.","986587c9":"# Introduction\nThis notbook uses the Car Insurance Cold Calls data from Kaggle. I put this work together in order to practice my machine learning classification skills as well as to focus on incorporating pipelines into my model building.    \n\nFirst, we will import the relevant libraries and load in the training and testing data. In reality, the testing data has no labels, so this notebook will focus on building a model and testing it on a validation set. In the process, I will also clean up the testing data to make it ready for implementing the model.","8f87e89a":"## Creating the Pipeline\nFirst, I will create one pipeline that will do the preprocessing on our features. This will basically scale the numeric variables using a MinMaxScaler and one-hot encode the categorical variables. Then, I will create the model pipeline that will preprocess the data, split the data, and estimate different types of models using a `for` loop. In this step I will also calculate model metrics for comparison purposes.","0e9f5b18":"Now, let's apply the function to our training data. and then check the distributions of the variables and the number of missing values.","a3a8c282":"# Going Forward\nIn the future, I would like to experiment more with the pipeline here. It could be interesting to do a bit more feature engineering, especially with regard to the functional forms of the continuous features in the model.  One could easily imagine, for instance, that the relationship between purchasing car insurance and age is nonlinear. After all, young people may not buy their own insurance and older people may no longer drive, so we would expect that middle-aged individuals would have a higher propensity to purchase car insurance.\n","d2939a0c":"We can see that a few features are playing a prominent role in our model. `Outcome_success` indicates if a previous marketing campaign was successful or not. The documentation doesnt really explain what this means. Next, an individual's having household insurance is predictive of purchasing car insurance, which is intuitive. We would expect that risk-averse individuals would be more likely to buy both, and riskier individuals to have neither. Interestingly, our constructed feature `CallDur`, which was the length of the sales call in minutes, strongly influenced the predictive capacity of the model. There may be two reasons for this. First, longer call durations could be correlated with a salesman's ability, thereby leading to higher sales. Second, individuals who were already likely to buy insurance for other reasons may need to have longer calls in order to fill out documentation, etc. So, this is not necessarilly a causal relationship, but the two are correlated nonetheless. Communication via phone or cellphone were also importance determinants, but one has to keep in mind that the reference category was 'missing', so it is really not clear what the interpretation of these importances are without figuring out what the missing communication types were.","91efc603":"By tuning the number of estimators we improved the accuracy and F1 scores of the model by a little bit. There is more tuning that can be done, but I will save that for another time. Now, lets plot the confusion matrix of our model.","ef5f9476":"# Preprocessing and Feature Engineering\nRight away we can see that a number of our features have significant outliers. These include `Balance`, `NoOfContacts`, `PrevAttempts`, and `DaysPassed`. To address these and to deal with the missing values in the categorical features, I will write another function that will do the bulk of our preprocessing. This will make it very simple to then apply the same transformations to the testing data as well. Specifically, the function will: (1) drop the ID column, (2) fill the categorical data with the modal value (when there are not a significant number of missings), (3) replace outliers with a top-coded value (here, the value of the feature at the 99th percentile), and (4) create new features for the duration of a salesperson's phone call and the hour of the day the call was initiated."}}