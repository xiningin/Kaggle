{"cell_type":{"349cb371":"code","1529c8f1":"code","0398b695":"code","1d34aad5":"code","b7fc41f1":"code","2977a3b4":"code","a71359e6":"code","8d7dd20b":"code","7121ad3f":"code","2ce7214b":"code","83e4b927":"code","a257ea19":"code","8525bc87":"code","9ff144bb":"code","c1b583d2":"code","0e521971":"code","002ab922":"code","0abcb375":"code","ecf50b03":"code","7d55d793":"code","3908e1ab":"code","272faa62":"code","d6eb54b4":"code","05d398b1":"code","e3ec27f4":"code","ec99764d":"code","65656f74":"code","7d419503":"code","84135373":"code","9f409971":"code","4f4a64d4":"code","a754468f":"code","abbaa562":"code","28844f99":"code","9c4da4ed":"code","78b04206":"code","3ac1bbd7":"code","f73a4a0a":"code","bcb1c799":"code","0afa2fed":"code","a6e0571c":"code","9fd61d34":"code","a7433a4d":"code","8bf57fa6":"code","2dbb2bc0":"code","f200ed64":"code","3f2e25ea":"code","67267235":"markdown","9e62a27c":"markdown","c59132f7":"markdown","206e9d5c":"markdown","78fe59c3":"markdown","bbc09286":"markdown","af05c164":"markdown","5a12b348":"markdown","c777dc67":"markdown","ba3c3efa":"markdown","4548ed20":"markdown","1b2abada":"markdown","27476730":"markdown","10ab5c26":"markdown","33c16f6a":"markdown","fc30a83a":"markdown","ec79043e":"markdown","9ae569e8":"markdown","56c5a308":"markdown","1a803d1d":"markdown","2b73c72e":"markdown","bb95d1e7":"markdown","84ae2d7d":"markdown","0ee5ba8b":"markdown","f96cc68f":"markdown","cb58dd86":"markdown","de2d9d88":"markdown","a638da64":"markdown","e20ce1ec":"markdown","6fb0e071":"markdown","2d96c32b":"markdown","73665698":"markdown","0470cc49":"markdown","af0bf6f1":"markdown","f03dcc5f":"markdown","62d20379":"markdown","7d7314ec":"markdown","c6629c2a":"markdown","e60d2f21":"markdown","a727c9d2":"markdown","0597fab7":"markdown"},"source":{"349cb371":"import os\nimport pandas_profiling as pp\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nplt.style.use('ggplot')\n\nimport datetime\nimport pandas_profiling as pp\n\nfrom scipy import stats\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import train_test_split\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nstop = set(stopwords.words('russian'))\n\nimport lightgbm as lgb\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import KFold\n\nkf = KFold(n_splits=5)","1529c8f1":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test  = pd.read_csv(\"..\/input\/test.csv\")\ndf_sub   = pd.read_csv(\"..\/input\/sample_submission.csv\")\ndf_periods_train = pd.read_csv(\"..\/input\/periods_train.csv\")","0398b695":"df_train.head(5)","1d34aad5":"df_train.info()","b7fc41f1":"df_train.describe(include='all')","2977a3b4":"df_periods_train.head(5)","a71359e6":"df_periods_train.info()","8d7dd20b":"df_periods_train.describe(include='all')","7121ad3f":"pp.ProfileReport(df_train)","2ce7214b":"df_train[\"activation_date\"] = pd.to_datetime(df_train[\"activation_date\"])\ndf_train[\"date\"]            = df_train[\"activation_date\"].dt.date\ndf_train[\"weekday\"]         = df_train[\"activation_date\"].dt.weekday\ndf_train[\"day\"]             = df_train[\"activation_date\"].dt.day\n\ncount_by_date_train         = df_train.groupby(\"date\")[\"deal_probability\"].count()\nmean_by_date_train          = df_train.groupby(\"date\")[\"deal_probability\"].mean()\n\ndf_test[\"activation_date\"]  = pd.to_datetime(df_test[\"activation_date\"])\ndf_test[\"date\"]             = df_test[\"activation_date\"].dt.date\ndf_test[\"weekday\"]          = df_test[\"activation_date\"].dt.weekday\ndf_test[\"day\"]              = df_test[\"activation_date\"].dt.day\n\ncount_by_date_test          = df_test.groupby('date')['item_id'].count()","83e4b927":"fig, (ax1, ax3) = plt.subplots(figsize=(26, 8), \n                              ncols=2,\n                              sharey=True)\ncount_by_date_train.plot(ax=ax1, \n                        legend=False,\n                        label=\"Ads Count\")\n\nax2 = ax1.twinx()\n\nmean_by_date_train.plot(ax=ax2,\n                       color=\"g\",\n                       legend=False,\n                       label=\"Mean deal_probability\")\n\nax2.set_ylabel(\"Mean deal_probabilit\", color=\"g\")\n\ncount_by_date_test.plot(ax=ax3,\n                       color=\"r\",\n                       legend=False,\n                       label=\"Ads count test\")\n\nplt.grid(False)\n\nax1.title.set_text(\"Trends of deal_probability and number of ads\")\nax3.title.set_text(\"Trends of number of ads for test data\")\nax1.legend(loc=(0.8, 0.35))\nax2.legend(loc=(0.8, 0.2))\nax3.legend(loc=\"upper right\")","a257ea19":"fig, ax1 = plt.subplots(figsize=(16, 8))\n\nplt.title(\"Ads Count and Deal Probability by day of the week\")\n\nsns.countplot(x   = \"weekday\",\n             data = df_train,\n             ax   = ax1)\n\nax1.set_ylabel(\"Ads Count\", color=\"b\")\n\nplt.legend([\"Projects Count\"])\n\nax2 = ax1.twinx()\n\nsns.pointplot(x   = \"weekday\",\n             y    = \"deal_probability\",\n             data = df_train,\n             ci   = 99,\n             ax   = ax2,\n             color = \"black\")\n\nax2.set_ylabel(\"deal_probability\", color=\"g\")\nplt.legend([\"deal_probability\"], loc=(0.875, 0.9))\nplt.grid(False)\n","8525bc87":"a = df_train.groupby([\"parent_category_name\", \"category_name\"]).agg({\"deal_probability\": [\"mean\", \"count\"]}).reset_index().sort_values([(\"deal_probability\", \"mean\")], ascending=False).reset_index(drop=True)\n\na","9ff144bb":"city_ads = df_train.groupby(\"city\").agg({\"deal_probability\": [\"mean\", \"count\"]}).reset_index().sort_values([(\"deal_probability\", \"mean\")], ascending=False).reset_index(drop=True)\n\nprint(\"There are {0} cities in total\".format(len(df_train.city.unique())))\n\nprint(\"There are {1} cities with more than {0} ads\".format(100, city_ads[city_ads[\"deal_probability\"][\"count\"] > 100].shape[0]))\n\nprint('There are {1} cities with more that {0} ads.'.format(1000, city_ads[city_ads['deal_probability']['count'] > 1000].shape[0]))\n\nprint('There are {1} cities with more that {0} ads.'.format(10000, city_ads[city_ads['deal_probability']['count'] > 10000].shape[0]))","c1b583d2":"city_ads[city_ads[\"deal_probability\"][\"count\"] > 1000].head()","0e521971":"city_ads[city_ads['deal_probability']['count'] > 1000].tail()","002ab922":"print(\"\u041b\u0430\u0431\u0438\u043d\u0441\u043a\")\n\ndf_train.loc[df_train.city == \"\u041b\u0430\u0431\u0438\u043d\u0441\u043a\"].groupby('category_name').agg({'deal_probability': ['mean', 'count']}).reset_index().sort_values([('deal_probability', 'count')], ascending=False).reset_index(drop=True).head(5)","0abcb375":"print('\u041c\u0438\u043b\u043b\u0435\u0440\u043e\u0432\u043e')\ndf_train.loc[df_train.city == '\u041c\u0438\u043b\u043b\u0435\u0440\u043e\u0432\u043e'].groupby('category_name').agg({'deal_probability': ['mean', 'count']}).reset_index().sort_values([('deal_probability', 'count')], ascending=False).reset_index(drop=True).head()","ecf50b03":"plt.hist(df_train[\"deal_probability\"]);\nplt.title(\"deal_probability\");","7d55d793":"text = ' '.join(df_train[\"title\"].values)\nwordCloud = WordCloud(max_font_size = None,\n                      stopwords = stop,\n                      background_color = \"white\",\n                      width = 1200,\n                      height = 1000).generate(text)\n\nplt.figure(figsize=(12, 8))\nplt.imshow(wordCloud)\nplt.title('Top words for title')\nplt.axis(\"off\")\nplt.show()","3908e1ab":"df_train[\"description\"] = df_train[\"description\"].apply(\n    lambda x: str(x).replace('\/\\n', ' ').replace('\\xa0', ' ')\n)","272faa62":"text = ' '.join(df_train['description'].values)\ntext = [i for i in ngrams(text.lower().split(), 3)]\nprint('Common trigrams.')\nCounter(text).most_common(40)","d6eb54b4":"df_train[df_train.description.str.contains('\u2193')]['description'].head(10).values","05d398b1":"df_train['has_image'] = 1\ndf_train.loc[df_train['image'].isnull(),'has_image'] = 0\n\nprint('There are {} ads with images. Mean deal_probability is {:.3}.'.format(len(df_train.loc[df_train['has_image'] == 1]), df_train.loc[df_train['has_image'] == 1, 'deal_probability'].mean()))","e3ec27f4":"print('There are {} ads without images. Mean deal_probability is {:.3}.'.format(len(df_train.loc[df_train['has_image'] == 0]), df_train.loc[df_train['has_image'] == 0, 'deal_probability'].mean()))","ec99764d":"plt.scatter(df_train.item_seq_number, df_train.deal_probability, label=\"item_seq_number vs deal_probability\");\n\nplt.xlabel(\"item_seq_number\");\nplt.ylabel(\"deal_probability\");","65656f74":"df_train[\"params\"] = df_train[\"param_1\"].fillna('') + ' ' + df_train[\"param_2\"].fillna('') + ' ' + df_train[\"param_3\"].fillna('')\n\ndf_train[\"params\"] = df_train[\"params\"].str.strip()\n\ntext = ' '.join(df_train[\"params\"].values)\ntext = [i for i in ngrams(text.lower().split(), 3)]\n\nprint(\"common trigrams\")\n\nCounter(text).most_common(40)","7d419503":"sns.set(rc = {'figure.figsize': (15, 8)})\n\ndf_train_ = df_train[df_train.price.isnull() == False]\ndf_train_ = df_train.loc[df_train.price < 100000.0]\n\nsns.boxplot(x = \"parent_category_name\",\n           y = \"price\",\n           hue = \"user_type\",\n           data = df_train_)\n\nplt.title(\"Price by parent gategory and user type\")\nplt.xticks(rotation = \"vertical\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.show()","84135373":"df_train[\"price\"] = df_train.groupby([\"city\", \"category_name\"])[\"price\"].apply(\n    lambda x: x.fillna(x.median())\n)\n\ndf_train[\"price\"] = df_train.groupby([\"region\", \"category_name\"])[\"price\"].apply(\n    lambda x: x.fillna(x.median())\n)\n\ndf_train[\"price\"] = df_train.groupby([\"category_name\"])[\"price\"].apply(\n    lambda x: x.fillna(x.median())\n)\n\nplt.hist(df_train[\"price\"]);","9f409971":"plt.hist(stats.boxcox(df_train[\"price\"] + 1)[0]);","4f4a64d4":"# lets transform the test in the same way as train\n\ndf_test[\"params\"] = df_test[\"param_1\"].fillna('') + ' ' + df_test[\"param_2\"].fillna('') + ' ' + df_test[\"param_3\"].fillna('')\ndf_test[\"params\"] = df_test[\"params\"].str.strip()\n\ndf_test[\"description\"] = df_test[\"description\"].apply( lambda x: str(x).replace(\"\/\\n\", ' ').replace(\"\\xa0\", \" \"))\n\ndf_test[\"has_image\"] = 1\ndf_test.loc[df_test[\"image\"].isnull(), \"has_image\"] = 0\n\ndf_test[\"price\"] = df_test.groupby([\"city\", \"category_name\"])[\"price\"].apply(lambda x: x.fillna(x.median()))\ndf_test[\"price\"] = df_test.groupby([\"region\", \"category_name\"])[\"price\"].apply(lambda x: x.fillna(x.median()))\ndf_test[\"price\"] = df_test.groupby([\"category_name\"])[\"price\"].apply(lambda x: x.fillna(x.median()))\n\ndf_train[\"price\"] = stats.boxcox(df_train.price + 1)[0]\ndf_test[\"price\"]  = stats.boxcox(df_test.price + 1)[0]","a754468f":"df_train[\"user_price_mean\"] = df_train.groupby(\"user_id\")[\"price\"].transform(\"mean\")\ndf_train[\"user_ad_count\"]   = df_train.groupby(\"user_id\")[\"price\"].transform(\"sum\")\n\ndf_train[\"region_price_mean\"]   = df_train.groupby(\"region\")[\"price\"].transform(\"mean\")\ndf_train[\"region_price_median\"] = df_train.groupby(\"region\")[\"price\"].transform(\"median\")\ndf_train[\"region_price_max\"]    = df_train.groupby(\"region\")[\"price\"].transform(\"max\")\n\ndf_train[\"city_price_mean\"]   = df_train.groupby(\"region\")[\"price\"].transform(\"mean\")\ndf_train[\"city_price_median\"] = df_train.groupby(\"region\")[\"price\"].transform(\"median\")\ndf_train[\"city_price_max\"]    = df_train.groupby(\"region\")[\"price\"].transform(\"max\")\n\ndf_train[\"parent_category_name_price_mean\"]   = df_train.groupby(\"parent_category_name\")[\"price\"].transform(\"mean\")\ndf_train[\"parent_category_name_price_median\"] = df_train.groupby(\"parent_category_name\")[\"price\"].transform(\"median\")\ndf_train[\"parent_category_name_price_max\"]    = df_train.groupby(\"parent_category_name\")[\"price\"].transform(\"max\")\n","abbaa562":"df_train[\"category_name_price_mean\"]   = df_train.groupby(\"category_name\")[\"price\"].transform(\"mean\")\ndf_train[\"category_name_price_median\"] = df_train.groupby(\"category_name\")[\"price\"].transform(\"median\")\ndf_train[\"category_name_price_max\"]    = df_train.groupby(\"category_name\")[\"price\"].transform(\"max\")\n\ndf_train[\"user_type_category_price_mean\"]   = df_train.groupby([\"user_type\", \"parent_category_name\"])[\"price\"].transform(\"mean\")\ndf_train[\"user_type_category_price_median\"] = df_train.groupby([\"user_type\", \"parent_category_name\"])[\"price\"].transform(\"mean\")\ndf_train[\"user_type_category_price_mean\"]   = df_train.groupby([\"user_type\", \"parent_category_name\"])[\"price\"].transform(\"mean\")","28844f99":"df_test[\"user_price_mean\"] = df_test.groupby(\"user_id\")[\"price\"].transform(\"mean\")\ndf_test[\"user_ad_count\"]   = df_test.groupby(\"user_id\")[\"price\"].transform(\"sum\")\n\ndf_test[\"region_price_mean\"]   = df_test.groupby(\"region\")[\"price\"].transform(\"mean\")\ndf_test[\"region_price_median\"] = df_test.groupby(\"region\")[\"price\"].transform(\"median\")\ndf_test[\"region_price_max\"]    = df_test.groupby(\"region\")[\"price\"].transform(\"max\")\n\ndf_test[\"city_price_mean\"]   = df_test.groupby(\"region\")[\"price\"].transform(\"mean\")\ndf_test[\"city_price_median\"] = df_test.groupby(\"region\")[\"price\"].transform(\"median\")\ndf_test[\"city_price_max\"]    = df_test.groupby(\"region\")[\"price\"].transform(\"max\")\n\ndf_test[\"parent_category_name_price_mean\"]   = df_test.groupby(\"parent_category_name\")[\"price\"].transform(\"mean\")\ndf_test[\"parent_category_name_price_median\"] = df_test.groupby(\"parent_category_name\")[\"price\"].transform(\"median\")\ndf_test[\"parent_category_name_price_max\"]    = df_test.groupby(\"parent_category_name\")[\"price\"].transform(\"max\")\n","9c4da4ed":"df_test[\"category_name_price_mean\"]   = df_test.groupby(\"category_name\")[\"price\"].transform(\"mean\")\ndf_test[\"category_name_price_median\"] = df_test.groupby(\"category_name\")[\"price\"].transform(\"median\")\ndf_test[\"category_name_price_max\"]    = df_test.groupby(\"category_name\")[\"price\"].transform(\"max\")\n\ndf_test[\"user_type_category_price_mean\"]   = df_test.groupby([\"user_type\", \"parent_category_name\"])[\"price\"].transform(\"mean\")\ndf_test[\"user_type_category_price_median\"] = df_test.groupby([\"user_type\", \"parent_category_name\"])[\"price\"].transform(\"mean\")\ndf_test[\"user_type_category_price_mean\"]   = df_test.groupby([\"user_type\", \"parent_category_name\"])[\"price\"].transform(\"mean\")","78b04206":"def target_encode(trn_series = None,\n                 tst_series  = None, \n                 target      = None,\n                 min_samples_leaf = 1,\n                 smoothing   = 1,\n                 noise_level = 0):\n    \"\"\"\n    \n    https:\/\/www.kaggle.com\/ogrellier\/python-target-encoding-for-categorical-features\n    Smoothing is computed like in the following paper by Daniele Micci-Barreca\n    https:\/\/kaggle2.blob.core.windows.net\/forum-message-attachments\/225952\/7441\/high%20cardinality%20categoricals.pdf\n    trn_series : training categorical feature as a pd.Series\n    tst_series : test categorical feature as a pd.Series\n    target : target data as a pd.Series\n    min_samples_leaf (int) : minimum samples to take category average into account\n    smoothing (int) : smoothing effect to balance categorical average vs prior  \n    \"\"\" \n    \n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean \n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    \n    # Compute smoothing\n    smoothing = 1 \/ (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) \/ smoothing))\n    \n    # Apply average function to all target data\n    prior = target.mean()\n    \n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    \n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    \n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index \n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    \n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return ft_trn_series, ft_tst_series\n    ","3ac1bbd7":"df_train['parent_category_name'], df_test['parent_category_name'] = target_encode(df_train['parent_category_name'], df_test['parent_category_name'], df_train['deal_probability'])\n\ndf_train['category_name'], df_test['category_name'] = target_encode(df_train['category_name'], df_test['category_name'], df_train['deal_probability'])\n\ndf_train['region'], df_test['region'] = target_encode(df_train['region'], df_test['region'], df_train['deal_probability'])\ndf_train['image_top_1'], df_test['image_top_1'] = target_encode(df_train['image_top_1'], df_test['image_top_1'], df_train['deal_probability'])\n\ndf_train['city'], df_test['city'] = target_encode(df_train['city'], df_test['city'], df_train['deal_probability'])\n\ndf_train['param_1'], df_test['param_1'] = target_encode(df_train['param_1'], df_test['param_1'], df_train['deal_probability'])\ndf_train['param_2'], df_test['param_2'] = target_encode(df_train['param_2'], df_test['param_2'], df_train['deal_probability'])\ndf_train['param_3'], df_test['param_3'] = target_encode(df_train['param_3'], df_test['param_3'], df_train['deal_probability'])","f73a4a0a":"df_train.drop(['date', 'day', 'user_id'], axis=1, inplace=True)\ndf_test.drop(['date', 'day', 'user_id'], axis=1, inplace=True)","bcb1c799":"df_train[\"len_title\"] = df_train[\"title\"].apply(lambda x: len(x))\ndf_train[\"words_title\"] = df_train[\"title\"].apply(lambda x: len(x.split()))\n\ndf_train[\"len_description\"] = df_train[\"description\"].apply(lambda x: len(x))\ndf_train[\"words_description\"] = df_train[\"description\"].apply(lambda x: len(x.split()))\n\ndf_train[\"len_params\"] = df_train[\"params\"].apply(lambda x: len(x))\ndf_train[\"words_params\"] = df_train[\"params\"].apply(lambda x: len(x.split()))\n\ndf_train['symbol1_count'] = df_train['description'].str.count('\u2193')\ndf_train['symbol2_count'] = df_train['description'].str.count('\\*')\ndf_train['symbol3_count'] = df_train['description'].str.count('\u2714')\ndf_train['symbol4_count'] = df_train['description'].str.count('\u2740')\ndf_train['symbol5_count'] = df_train['description'].str.count('\u279a')\ndf_train['symbol6_count'] = df_train['description'].str.count('\u0b9c')\ndf_train['symbol7_count'] = df_train['description'].str.count('.')\ndf_train['symbol8_count'] = df_train['description'].str.count('!')\ndf_train['symbol9_count'] = df_train['description'].str.count('\\?')\ndf_train['symbol10_count'] = df_train['description'].str.count('  ')\ndf_train['symbol11_count'] = df_train['description'].str.count('-')\ndf_train['symbol12_count'] = df_train['description'].str.count(',')\n\ndf_test['len_title']         = df_test['title'].apply(lambda x: len(x))\ndf_test['words_title']       = df_test['title'].apply(lambda x: len(x.split()))\ndf_test['len_description']   = df_test['description'].apply(lambda x: len(x))\ndf_test['words_description'] = df_test['description'].apply(lambda x: len(x.split()))\ndf_test['len_params']        = df_test['params'].apply(lambda x: len(x))\ndf_test['words_params']      = df_test['params'].apply(lambda x: len(x.split()))\n\ndf_test['symbol1_count'] = df_test['description'].str.count('\u2193')\ndf_test['symbol2_count'] = df_test['description'].str.count('\\*')\ndf_test['symbol3_count'] = df_test['description'].str.count('\u2714')\ndf_test['symbol4_count'] = df_test['description'].str.count('\u2740')\ndf_test['symbol5_count'] = df_test['description'].str.count('\u279a')\ndf_test['symbol6_count'] = df_test['description'].str.count('\u0b9c')\ndf_test['symbol7_count'] = df_test['description'].str.count('.')\ndf_test['symbol8_count'] = df_test['description'].str.count('!')\ndf_test['symbol9_count'] = df_test['description'].str.count('\\?')\ndf_test['symbol10_count'] = df_test['description'].str.count('  ')\ndf_test['symbol11_count'] = df_test['description'].str.count('-')\ndf_test['symbol12_count'] = df_test['description'].str.count(',')","0afa2fed":"vectorizer = TfidfVectorizer(stop_words = stop, max_features = 6000)\nvectorizer.fit(df_train[\"title\"])\n\ndf_train_title = vectorizer.transform(df_train[\"title\"])\ndf_test_title  = vectorizer.transform(df_test[\"title\"])\n","a6e0571c":"df_train.drop([\"title\", \"params\", \"description\", \"user_type\", \"activation_date\"], axis=1, inplace=True)\ndf_test.drop([\"title\", \"params\", \"description\", \"user_type\", \"activation_date\"], axis=1, inplace=True)","9fd61d34":"pd.set_option('max_columns', 60)\ndf_train.head()","a7433a4d":"%%time\n\nX_meta = np.zeros((df_train_title.shape[0], 1))\nX_test_meta = []\n\nfor fold_i, (train_i, test_i) in enumerate(kf.split(df_train_title)):\n    print(fold_i)\n    model = Ridge()\n    model.fit(df_train_title.tocsr()[train_i], df_train[\"deal_probability\"][train_i])\n    X_meta[test_i, :] = model.predict(df_train_title.tocsr()[test_i]).reshape(-1, 1)\n    X_test_meta.append(model.predict(df_test_title))\n    \nX_test_meta = np.stack(X_test_meta)\nX_test_meta_mean = np.mean(X_test_meta, axis=0)","8bf57fa6":"X_full = csr_matrix(hstack([df_train.drop(['item_id', 'deal_probability', 'image'], axis=1), X_meta]))\nX_test_full = csr_matrix(hstack([df_test.drop(['item_id', 'image'], axis=1), X_test_meta_mean.reshape(-1, 1)]))\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_full, df_train[\"deal_probability\"], test_size=0.20, random_state=42)","2dbb2bc0":"def rmse(predictions, targets):\n    return np.sqrt( ( (predictions - targets) ** 2).mean() )","f200ed64":"# took parameters from this kernel:  https:\/\/www.kaggle.com\/the1owl\/beep-beep\n\nparams = {\"learning_rate\": 0.08,\n          \"max_depth\": 8,\n          \"boosting\": \"gbdt\",\n          \"objective\": \"regression\",\n          \"metric\": [\"auc\", \"rmse\"],\n          \"is_training_metric\": True,\n          \"seed\": 19,\n          \"num_leaves\": 63,\n          \"feature_fraction\": 0.9,\n          \"bagging_fraction\": 0.8,\n          \"bagging_freq\": 5\n         }\n\nmodel = lgb.train(params,\n                 lgb.Dataset(X_train, label=y_train),\n                 2000,\n                 lgb.Dataset(X_valid, label=y_valid),\n                 verbose_eval=50,\n                 early_stopping_rounds=20)","3f2e25ea":"pred = model.predict(X_test_full)\n\n#clipping is necessary.\ndf_sub['deal_probability'] = np.clip(pred, 0, 1)\ndf_sub.to_csv('sub.csv', index=False)","67267235":"**Meta-features:**\n\nOne of the features is used to build a model A and the prediction of model A is used as a feature in building model B.\n\nOne of possible ideas is creating meta-features. It means that we use some features to build a model and use the predictions in another model. I'll use ridge regression to create a new feature based on tokenized title and then I'll combine it with other features.","9e62a27c":"**Params:**\n\nThere are three fields with additional information, let's combine it into one. Technically it is possible to treat these features as categorical, but there would be too many of them.","c59132f7":"**Categorical features:**\n\nI'll use the target encoding to deal with categorical features.","206e9d5c":"**description:**","78fe59c3":"Let's try to understand the output of the pandas profiling:\n\n- Types of features: we have different types of features i.e. numerical, categorical, text, date\n- Missing values: some columns have missing values like some times description text is missing for a particular ad.\n- Users: We have over 51% unique users, most of them don't post lot of ads. \n- Categories: There are 9 categories in parent_category_name\n- price is skewed and hence will require careful processing. ","bbc09286":"We can see that shops usually have higher prices than companies and private sellers usually have the lowest price - maybe because they are usually second-hand.\n\n","af05c164":"**Pandas profiling:**\n\nGenerates profile reports from a pandas DataFrame. The pandas df.describe() function is great but a little basic for serious exploratory data analysis.\n\nFor each column the following statistics - if relevant for the column type - are presented in an interactive HTML report:\n\n- **Essentials**: type, unique values, missing values\n- **Quantile statistics** like minimum value, Q1, median, Q3, maximum, range, interquartile range\n- **Descriptive statistics** like mean, mode, standard deviation, sum, median absolute deviation, coefficient of variation, kurtosis, skewness\n- **Most frequent values**\n- **Histogram**\n- **Correlations** highlighting of highly correlated variables, Spearman and Pearson matrixes\n","5a12b348":"Some of the authors are using emoticons for their ads. ","c777dc67":"Most popular categories are \"\u0410\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u0438\" (cars) and \"\u0422\u0435\u043b\u0435\u0444\u043e\u043d\u044b\" (telephones).\n\n","ba3c3efa":"Most of params belong to clothes or cars.\n\n","4548ed20":"**City: **\n\nLets take a look at the city feature in depth. ","1b2abada":"Lets chart out the Counts and Means for deal probability and try to find relation between number of ads and mean of deal_probability to find any patterns. ","27476730":"**Feature Analysis:**\n\nLets analyze some of the features in more details. \n\n**activation_date:**\nLets create new features based on activation_date: date, weekday, day of month.\n\n","10ab5c26":"**Building a simple model:**","33c16f6a":"Most popular categories are \"\u0410\u0432\u0442\u043e\u043c\u043e\u0431\u0438\u043b\u0438\" (cars). And it seems that second-hand wares are least popular.\n\n","fc30a83a":"**Text Features:**\n\nWe have several features with text data and they need to be processed in different ways. But at first let's create new features based on texts:\n- length of text (symbols)\n- number of words\n- counts of punctuation\n- counts of strange symbols ( emoticons)","ec79043e":"**Feature Engineering:**\n","9ae569e8":"**title:**","56c5a308":"We can see that \"\u0423\u0441\u043b\u0443\u0433\u0438\" (services) is the category with the highest deal_probability. Other \"good\" categories are about animals or electronics\/cars. Least successful are various accessories or expensive things.","1a803d1d":"Imagine that you are watching a race and that you are located close to the finish line. When the first and fastest runners complete the race, the differences in times between them will probably be quite small.\n\nNow wait until the last runners arrive and consider their finishing times. For these slowest runners, the differences in completion times will be extremely large. This is due to the fact that for longer racing times a small difference in speed will have a significant impact on completion times, whereas for the fastest runners, small differences in speed will have a small (but decisive) impact on arrival times.\n\nThis phenomenon is called \u201cheteroscedasticity\u201d (non-constant variance). In this example, the amount of Variation depends on the average value (small variations for shorter completion times, large variations for longer times).\n\nThis distribution of running times data will probably not follow the familiar bell-shaped curve (a.k.a. the normal distribution). The resulting distribution will be asymmetrical with a longer tail on the right side. This is because there's small variability on the left side with a short tail for smaller running times, and larger variability for longer running times on the right side, hence the longer tail.\n\n\nWhy does this matter?\n\nModel bias and spurious interactions: If you are performing a regression or a design of experiments (any statistical modelling), this asymmetrical behavior may lead to a bias in the model. If a factor has a significant effect on the average speed, because the variability is much larger for a larger average running time, many factors will seem to have a stronger effect when the mean is larger. This is not due, however, to a true factor effect but rather to an increased amount of variability that affects all factor effect estimates when the mean gets larger. This will probably generate spurious interactions due to a non-constant variation, resulting in a very complex model with many (spurious and unrealistic) interactions.\n\nIf you are performing a standard capability analysis, this analysis is based on the normality assumption. A substantial departure from normality will bias your capability estimates.\n\nhttp:\/\/blog.minitab.com\/blog\/applying-statistics-in-quality-projects\/how-could-you-benefit-from-a-box-cox-transformation","2b73c72e":"As majority of the columns are categorical variables, we have very few numerical variables. ","bb95d1e7":"On the one hand the distribution of the target value is highly skewered towards zero, on the other hand, there is a spike at about 0.8.","84ae2d7d":"It seems that most of the cities have little ads posted and only in 33 of them there are a lot of ads. Let's see the best and the worst cities by mean deal_probability.","0ee5ba8b":"Now let's start transforming texts. Titles have little number of unique words, so we can use default values for TfidfVectorizer (only add stopwords). I have to limit max_features due to memory constraints. I won't use descriptions and parameters due to kernel limits. ","f96cc68f":"**user_type:**\n\nthere are three main user_Types. Let's see prices of their wares, where prices are below 100,000. ","cb58dd86":"**Aggregate features:**\n\nI'll create a number of aggregate features. \n\n- user_price_mean\n- user_ad_count\n- region_price_mean\n- region_price_median\n- region_price_max\n- city_price_mean\n- city_price_median\n- city_price_max\n- parent_category_name_price_mean\n- parent_category_name_price_median\n- parent_category_name_price_max\n- category_name_price_mean\n- category_name_price_median\n- category_name_price_max\n- user_type_category_price_mean\n- user_type_category_price_median\n- user_type_category_price_nax\n\n","de2d9d88":"As we can see, we have few weeks of data in training set and one week of data in the test dataset. \n\n- For most of the data in training dataset, the number of ads is quite high ( 100,000 or more) and mean deal_probability is low around 0.14, but after March 28, the number of ads drop off and so the deal_probability fluctuates.\n- In test data set, we have few ads going up to April 18th and then it falls off. \n\nFor training dataset, lets remove the data after March 29 because number of ads fall  to 0.","a638da64":"**deal_probability**\n\n","e20ce1ec":"I think that it could be interesting to see what is sold in \u041b\u0430\u0431\u0438\u043d\u0441\u043a and \u041c\u0438\u043b\u043b\u0435\u0440\u043e\u0432\u043e","6fb0e071":"Let's use boxcox transformation to get rid of skewness\n\n","2d96c32b":"We can see that sellers try to tell buyers that their wares are great and also tell about possibilities of delivery. But there are some strange values, let's have a look...\n\n","73665698":"**Data Overview:**\n\nLets understand the data that we have for this kernel.","0470cc49":"**Avito Demand Prediction Challenge:**\n\nDescription of the challenge taken from the competition page:\n\nWhen selling used goods online, a combination of tiny, nuanced details in a product description can make a big difference in drumming up interest. \n\nAnd, even with an optimized product listing, demand for a product may simply not exist\u2013frustrating sellers who may have over-invested in marketing.\n\nAvito, Russia\u2019s largest classified advertisements website, is deeply familiar with this problem. Sellers on their platform sometimes feel frustrated with both too little demand (indicating something is wrong with the product or the product listing) or too much demand (indicating a hot item with a good description was underpriced).\n\nIn their fourth Kaggle competition, Avito is challenging you to predict demand for an online advertisement based on its full description (title, description, images, etc.), its context (geographically where it was posted, similar ads already posted) and historical demand for similar ads in similar contexts. With this information, Avito can inform sellers on how to best optimize their listing and provide some indication of how much interest they should realistically expect to receive.\n\n**What are we doing in this kernel?**\n\nThe aim of this kernel is to perform EDA of Avito demand prediction challenge competition as well add new features which would be used by the LightGBM library for performing prediction. ","af0bf6f1":"**price:**\n\nThe first question is how to deal with missing values. I have decided to do the following:\n- at first fill missing values with median by city and category.\n- then missing values which are left are filled with region by region and category\n- the remaining missing values are filled with median by category\n","f03dcc5f":"Lets create a box plot between Ads Count and Deal probability by day of the week to find some patterns. ","62d20379":"**item_seq_number:**","7d7314ec":"Learnings from above plots:\n- Deal probability increases as the ads count fall and deal probability reaches a maximum on day 6th of the week. \n- Ads count are highest on weekends (assuming day 0 and day 6 are the weekend).\n- deal probability fall off during mid-week, potentially due to mid of the week effect \n\n","c6629c2a":"It seems like there are many users who post a lot of ads and number of ads posted isn't really correlated with deal_probability. ","e60d2f21":"Columns:\n- item_id\n- user_id\n- region\n- city\n- parent_category_name\n- category_name\n- param_1 \n- param_2\n- param_3\n- title\n- description\n- price\n- item_seq_number\n- activation_date\n- user_type\n- image\n- image_top_1\n- deal_probability\n","a727c9d2":"**image:**\n\nIn this kernel I won't use the images themselves, but I'll create a feature showing wheather there is an image or not","0597fab7":"**Categories:**\n\nLets spend some time looking at the categories. "}}