{"cell_type":{"74e74bdb":"code","0a3c5709":"code","d7f08afd":"code","d0472c17":"code","4e18d9fa":"code","9b17342f":"code","ddbff49e":"code","6e42884c":"code","d32710b0":"code","83abc189":"code","5f8707ff":"code","729b6dcd":"code","f8f10bcb":"code","3ae524a2":"code","a891b56d":"code","de9c129c":"code","b5810779":"code","a8e7d5db":"code","9a42ae47":"code","33bfb5c0":"code","b42cba3b":"code","e52982cf":"code","56f67fa3":"code","7aa930b4":"code","ed8c3c3a":"code","9af47f9d":"code","811bfa2b":"code","569aaafe":"code","f7a2f10e":"code","06e97b29":"code","1c47c980":"code","8f4fe3c4":"code","f7373c90":"code","3e747d9d":"code","862fb13f":"markdown","59c4ff5c":"markdown","12555eee":"markdown","6c1658b0":"markdown","1a6375d3":"markdown","ba92d31a":"markdown","70379baf":"markdown","7d272c62":"markdown","39984787":"markdown","30e06374":"markdown","fbcadbc5":"markdown","1336e6c2":"markdown","fa9e19cf":"markdown","2f5d44e5":"markdown","dde95baa":"markdown","7b5d277f":"markdown","67d7e43b":"markdown","d785a64f":"markdown","5751ba0a":"markdown","284a1a04":"markdown","f8dcc2bb":"markdown","15d4c9c0":"markdown","0095a2ac":"markdown","4ab029e7":"markdown","c656417a":"markdown","ef72bc06":"markdown"},"source":{"74e74bdb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom tensorflow import keras\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport tensorflow.keras.backend as K\nfrom sklearn.manifold import TSNE\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0a3c5709":"train  =  pd.read_csv(r'\/kaggle\/input\/mnist-in-csv\/mnist_train.csv')\ntest  =  pd.read_csv(r'\/kaggle\/input\/mnist-in-csv\/mnist_test.csv')","d7f08afd":"train.head()","d0472c17":"y_train = train.label\nX_train =train.drop('label', axis=1)\/255\n\nX_test = test.drop('label', axis = 1)\/255\ny_test = test['label']\n\nX_train = X_train.values.reshape(X_train.shape[0], 28, 28)\nX_test = X_test.values.reshape(X_test.shape[0], 28, 28)\n","4e18d9fa":"stacked_encoder = keras.models.Sequential([\n    keras.layers.Flatten(input_shape = (28, 28)),\n    keras.layers.Dense(128, activation = 'selu'),\n    keras.layers.Dense(32, activation = 'selu'),\n])\n\nstacked_decoder = keras.models.Sequential([\n    keras.layers.Dense(128, activation = 'selu', input_shape = (32, )),\n    keras.layers.Dense(28*28, activation = 'sigmoid'),\n    keras.layers.Reshape([28, 28])\n])\n\nstacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])\n\nstacked_ae.compile(loss = 'binary_crossentropy', optimizer = keras.optimizers.SGD(lr=.1))\n\nhistory  =  stacked_ae.fit(X_train, X_train, epochs = 25, validation_data = (X_test, X_test))","9b17342f":"to_predict = X_test[:6]\nprediction = stacked_ae.predict(to_predict)\n\ndef visualize_predictions(predictions, data):\n    fig, axes = plt.subplots(2, predictions.shape[0], figsize = (predictions.shape[0]*5, 5))\n    for i, ax in zip(range(predictions.shape[0]), axes[0, :]):\n        ax.imshow(predictions[i], cmap = 'Greys')\n        ax.set_title(y_test[i])\n    for i, ax in zip(range(predictions.shape[0]), axes[1, :]):\n        ax.imshow(X_test[i], cmap = 'Greys')\n        ax.set_title(y_test[i])    \n    return plt\n\nvisualize_predictions(prediction, to_predict)","ddbff49e":"# Select 1000 samples from data\nX_test_sampled = X_test[:1000, :]\ny_test_sampled = y_test.iloc[:1000]\n\n# Visualizing the Latent representation - Taking the average of the volume\nx_compressed = stacked_encoder.predict(X_test_sampled).mean(axis = -1).reshape(1000, -1)\n\n# Use TSNE\ntsne = TSNE(n_jobs=-1)\nX_compressed_2d  =  tsne.fit_transform(x_compressed)","6e42884c":"# Plot in scatterplot and color by digit name\nplt.figure(figsize = (12, 7))\ncmap = plt.get_cmap('RdBu', 10)\nsc = plt.scatter(X_compressed_2d[:, 0], X_compressed_2d[:, 1], c = y_test_sampled, alpha = .85, cmap = cmap)\ncax = plt.colorbar(sc, ticks=np.arange(0,10))\nplt.xlabel('tsne 1')\nplt.ylabel('tsne 2')\n#plt.colorbar(sc)","d32710b0":"stacked_encoder = keras.models.Sequential([\n    keras.layers.Flatten(input_shape = (28, 28)),\n    keras.layers.Dense(128, activation = 'selu'),\n    keras.layers.Dense(32, activation = 'selu'),\n    keras.layers.ActivityRegularization(l1=1e-3)\n])\n\nstacked_decoder = keras.models.Sequential([\n    keras.layers.Dense(128, activation = 'selu', input_shape = (32, )),\n    keras.layers.Dense(28*28, activation = 'sigmoid'),\n    keras.layers.Reshape([28, 28])\n])\n\nstacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])\n\nstacked_ae.compile(loss = 'binary_crossentropy', optimizer = keras.optimizers.SGD(lr=.1))\n\nhistory  =  stacked_ae.fit(X_train, X_train, epochs = 30, validation_data = (X_test, X_test))","83abc189":"to_predict = X_test[:6]\nprediction = stacked_ae.predict(to_predict)\n\ndef visualize_predictions(predictions, data):\n    fig, axes = plt.subplots(2, predictions.shape[0], figsize = (predictions.shape[0]*5, 5))\n    for i, ax in zip(range(predictions.shape[0]), axes[0, :]):\n        ax.imshow(predictions[i], cmap = 'Greys')\n        ax.set_title(y_test[i])\n    for i, ax in zip(range(predictions.shape[0]), axes[1, :]):\n        ax.imshow(X_test[i], cmap = 'Greys')\n        ax.set_title(y_test[i])    \n    return plt\n\nvisualize_predictions(prediction, to_predict)","5f8707ff":"# Select 1000 samples from data\nX_test_sampled = X_test[:1000, :]\ny_test_sampled = y_test.iloc[:1000]\n\n# Visualizing the Latent representation - Taking the average of the volume\nx_compressed = stacked_encoder.predict(X_test_sampled).mean(axis = -1).reshape(1000, -1)\n\n# Use TSNE\ntsne = TSNE(n_jobs=-1)\nX_compressed_2d  =  tsne.fit_transform(x_compressed)","729b6dcd":"# Plot in scatterplot and color by digit name\nplt.figure(figsize = (12, 7))\ncmap = plt.get_cmap('RdBu', 10)\nsc = plt.scatter(X_compressed_2d[:, 0], X_compressed_2d[:, 1], c = y_test_sampled, alpha = .85, cmap = cmap)\ncax = plt.colorbar(sc, ticks=np.arange(0,10))\nplt.xlabel('tsne 1')\nplt.ylabel('tsne 2')\n#plt.colorbar(sc)","f8f10bcb":"y_train = train.label\nX_train =train.drop('label', axis=1)\/255\n\nX_test = test.drop('label', axis = 1)\/255\ny_test = test['label']\n\nX_train = X_train.values.reshape(X_train.shape[0], 28, 28, 1)\nX_test = X_test.values.reshape(X_test.shape[0], 28, 28, 1)\n","3ae524a2":"stacked_encoder = keras.models.Sequential([\n    keras.layers.Conv2D(32, kernel_size = (3,3), activation = 'selu', input_shape = (28, 28, 1), padding = 'same'),\n    keras.layers.MaxPooling2D(2, 2, padding = 'same'),\n    keras.layers.Conv2D(8, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n    keras.layers.MaxPooling2D(2, 2, padding = 'same'),\n    keras.layers.Conv2D(8, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n\n])\n\nstacked_decoder = keras.models.Sequential([\n    keras.layers.Conv2D(8, kernel_size = (3,3), activation = 'selu', input_shape = (7,7,8), padding = 'same'),\n    keras.layers.UpSampling2D((2,2)),\n    keras.layers.Conv2D(8, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n    keras.layers.UpSampling2D((2,2)),\n    keras.layers.Conv2D(32, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n    keras.layers.Conv2D(1, kernel_size = (3,3), activation = 'sigmoid', padding = 'same'),\n\n    \n])\n\nstacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])\n\nstacked_ae.compile(loss = 'binary_crossentropy', optimizer = keras.optimizers.SGD(lr=.1))\n\nhistory  =  stacked_ae.fit(X_train, X_train, epochs = 10, validation_data = (X_test, X_test))","a891b56d":"to_predict = X_test[:6]\nprediction = stacked_ae.predict(to_predict)\n\ndef visualize_predictions(predictions, data):\n    fig, axes = plt.subplots(2, predictions.shape[0], figsize = (predictions.shape[0]*5, 5))\n    for i, ax in zip(range(predictions.shape[0]), axes[0, :]):\n        ax.imshow(predictions[i], cmap = 'Greys')\n        ax.set_title(y_test[i])\n    for i, ax in zip(range(predictions.shape[0]), axes[1, :]):\n        ax.imshow(X_test[i], cmap = 'Greys')\n        ax.set_title(y_test[i])    \n    return plt\n\nvisualize_predictions(prediction, to_predict)","de9c129c":"# Select 1000 samples from data\nX_test_sampled = X_test[:1000, :]\ny_test_sampled = y_test.iloc[:1000]\n\n# Visualizing the Latent representation - Taking the average of the volume\nx_compressed = stacked_encoder.predict(X_test_sampled).mean(axis = -1).reshape(1000, -1)\n\n# Use TSNE\ntsne = TSNE(n_jobs=-1)\nX_compressed_2d  =  tsne.fit_transform(x_compressed)","b5810779":"# Plot in scatterplot and color by digit name\nplt.figure(figsize = (12, 7))\ncmap = plt.get_cmap('RdBu', 10)\nsc = plt.scatter(X_compressed_2d[:, 0], X_compressed_2d[:, 1], c = y_test_sampled, alpha = .85, cmap = cmap)\ncax = plt.colorbar(sc, ticks=np.arange(0,10))\nplt.xlabel('tsne 1')\nplt.ylabel('tsne 2')\n#plt.colorbar(sc)","a8e7d5db":"stacked_encoder = keras.models.Sequential([\n    keras.layers.Conv2D(16, kernel_size = (3,3), activation = 'selu', input_shape = (28, 28, 1), padding = 'same'),\n    keras.layers.MaxPooling2D(2, 2, padding = 'same'),\n    keras.layers.Conv2D(32, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n    keras.layers.MaxPooling2D(2, 2, padding = 'same'),\n    keras.layers.Conv2D(64, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n    keras.layers.MaxPooling2D(2, 2, padding = 'same'),\n])\n\nstacked_decoder = keras.models.Sequential([\n    keras.layers.Conv2D(64, kernel_size = (3,3), activation = 'selu', input_shape = (7,7,64), padding = 'same'),\n    keras.layers.UpSampling2D((2,2)),\n    keras.layers.Conv2D(32, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n    keras.layers.UpSampling2D((2,2)),\n    keras.layers.Conv2D(16, kernel_size = (3,3), activation = 'selu', padding = 'valid'),\n    keras.layers.UpSampling2D((2,2)),\n    keras.layers.Conv2D(1, kernel_size = (3,3), activation = 'sigmoid', padding = 'same'),    \n])\n\nstacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])\noptimizer = keras.optimizers.SGD(lr=.1)\noptimizer = 'adam'\nstacked_ae.compile(loss = 'binary_crossentropy', optimizer = optimizer)\n\nhistory  =  stacked_ae.fit(X_train, X_train, epochs = 10, validation_data = (X_test, X_test))","9a42ae47":"# Select 1000 samples from data\nX_test_sampled = X_test[:1000, :]\ny_test_sampled = y_test.iloc[:1000]\n\n# Visualizing the Latent representation - Taking the average of the volume\nx_compressed = stacked_encoder.predict(X_test_sampled).mean(axis = -1).reshape(1000, -1)\n\n# Use TSNE\ntsne = TSNE(n_jobs=-1)\nX_compressed_2d  =  tsne.fit_transform(x_compressed)\n\n\n# Plot in scatterplot and color by digit name\nplt.figure(figsize = (12, 7))\ncmap = plt.get_cmap('RdBu', 10)\nsc = plt.scatter(X_compressed_2d[:, 0], X_compressed_2d[:, 1], c = y_test_sampled, alpha = .85, cmap = cmap)\ncax = plt.colorbar(sc, ticks=np.arange(0,10))\nplt.xlabel('tsne 1')\nplt.ylabel('tsne 2')\n#plt.colorbar(sc)","33bfb5c0":"stacked_encoder = keras.models.Sequential([\n    keras.layers.Conv2D(16, kernel_size = (3,3), activation = 'selu', input_shape = (28, 28, 1), padding = 'same'),\n    keras.layers.MaxPooling2D(2, 2, padding = 'same'),\n    keras.layers.Conv2D(32, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n    keras.layers.MaxPooling2D(2, 2, padding = 'same'),\n    keras.layers.Conv2D(64, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n    keras.layers.MaxPooling2D(2, 2, padding = 'same'),\n    keras.layers.ActivityRegularization(l1=1e-1)\n])\n\nstacked_decoder = keras.models.Sequential([\n    keras.layers.Conv2D(64, kernel_size = (3,3), activation = 'selu', input_shape = (7,7,64), padding = 'same'),\n    keras.layers.UpSampling2D((2,2)),\n    keras.layers.Conv2D(32, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n    keras.layers.UpSampling2D((2,2)),\n    keras.layers.Conv2D(16, kernel_size = (3,3), activation = 'selu', padding = 'valid'),\n    keras.layers.UpSampling2D((2,2)),\n    keras.layers.Conv2D(1, kernel_size = (3,3), activation = 'sigmoid', padding = 'same'),    \n])\n\nstacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])\noptimizer = keras.optimizers.SGD(lr=.1)\noptimizer = 'adam'\nstacked_ae.compile(loss = 'binary_crossentropy', optimizer = optimizer)\n\nhistory  =  stacked_ae.fit(X_train, X_train, epochs = 10, validation_data = (X_test, X_test))","b42cba3b":"# Select 1000 samples from data\nX_test_sampled = X_test[:1000, :]\ny_test_sampled = y_test.iloc[:1000]\n\n# Visualizing the Latent representation - Taking the average of the volume\nx_compressed = stacked_encoder.predict(X_test_sampled).mean(axis = -1).reshape(1000, -1)\n\n# Use TSNE\ntsne = TSNE(n_jobs=-1)\nX_compressed_2d  =  tsne.fit_transform(x_compressed)\n\n\n# Plot in scatterplot and color by digit name\nplt.figure(figsize = (12, 7))\ncmap = plt.get_cmap('RdBu', 10)\nsc = plt.scatter(X_compressed_2d[:, 0], X_compressed_2d[:, 1], c = y_test_sampled, alpha = .85, cmap = cmap)\ncax = plt.colorbar(sc, ticks=np.arange(0,10))\nplt.xlabel('tsne 1')\nplt.ylabel('tsne 2')\n#plt.colorbar(sc)","e52982cf":"X_train_noisy = X_train + np.random.normal(0, .05, (X_train.shape[0], 28, 28, 1))\nX_test_noisy = X_test + np.random.normal(0, .05, (X_test.shape[0], 28, 28, 1))\n","56f67fa3":"plt.imshow(X_train_noisy[10].reshape(28, 28), cmap = 'Greys')","7aa930b4":"stacked_encoder = keras.models.Sequential([\n    keras.layers.Conv2D(32, kernel_size = (3,3), activation = 'selu', input_shape = (28, 28, 1), padding = 'same'),\n    keras.layers.MaxPooling2D(2, 2, padding = 'same'),\n    keras.layers.Conv2D(8, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n    keras.layers.MaxPooling2D(2, 2, padding = 'same'),\n    keras.layers.Conv2D(8, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n\n])\n\nstacked_decoder = keras.models.Sequential([\n    keras.layers.Conv2D(8, kernel_size = (3,3), activation = 'selu', input_shape = (7,7,8), padding = 'same'),\n    keras.layers.UpSampling2D((2,2)),\n    keras.layers.Conv2D(8, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n    keras.layers.UpSampling2D((2,2)),\n    keras.layers.Conv2D(32, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n    keras.layers.Conv2D(1, kernel_size = (3,3), activation = 'sigmoid', padding = 'same'),\n\n    \n])\n\nstacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])\n\nstacked_ae.compile(loss = 'binary_crossentropy', optimizer = keras.optimizers.SGD(lr=.01))\n\nhistory  =  stacked_ae.fit(X_train_noisy, X_train, batch_size = 128, epochs = 10,\n                           validation_data = (X_test_noisy, X_test))","ed8c3c3a":"# Select 1000 samples from data\nX_test_sampled = X_test[:1000, :]\ny_test_sampled = y_test.iloc[:1000]\n\n# Visualizing the Latent representation - Taking the average of the volume\nx_compressed = stacked_encoder.predict(X_test_sampled).mean(axis = -1).reshape(1000, -1)\n\n# Use TSNE\ntsne = TSNE(n_jobs=-1)\nX_compressed_2d  =  tsne.fit_transform(x_compressed)\n\n\n# Plot in scatterplot and color by digit name\nplt.figure(figsize = (12, 7))\ncmap = plt.get_cmap('RdBu', 10)\nsc = plt.scatter(X_compressed_2d[:, 0], X_compressed_2d[:, 1], c = y_test_sampled, alpha = .85, cmap = cmap)\ncax = plt.colorbar(sc, ticks=np.arange(0,10))\nplt.xlabel('tsne 1')\nplt.ylabel('tsne 2')\n#plt.colorbar(sc)","9af47f9d":"stacked_encoder = keras.models.Sequential([\n    keras.layers.Dropout(.1),\n    keras.layers.Conv2D(32, kernel_size = (3,3), activation = 'selu', input_shape = (28, 28, 1), padding = 'same'),\n    keras.layers.MaxPooling2D(2, 2, padding = 'same'),\n    keras.layers.Conv2D(8, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n    keras.layers.MaxPooling2D(2, 2, padding = 'same'),\n    keras.layers.Conv2D(8, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n\n])\n\nstacked_decoder = keras.models.Sequential([\n    keras.layers.Conv2D(8, kernel_size = (3,3), activation = 'selu', input_shape = (7,7,8), padding = 'same'),\n    keras.layers.UpSampling2D((2,2)),\n    keras.layers.Conv2D(8, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n    keras.layers.UpSampling2D((2,2)),\n    keras.layers.Conv2D(32, kernel_size = (3,3), activation = 'selu', padding = 'same'),\n    keras.layers.Conv2D(1, kernel_size = (3,3), activation = 'sigmoid', padding = 'same'),\n\n    \n])\n\nstacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])\n\nstacked_ae.compile(loss = 'binary_crossentropy', optimizer = keras.optimizers.SGD(lr=.01))\n\nhistory  =  stacked_ae.fit(X_train, X_train, batch_size = 128, epochs = 10,\n                           validation_data = (X_test, X_test))","811bfa2b":"# Select 1000 samples from data\nX_test_sampled = X_test[:1000, :]\ny_test_sampled = y_test.iloc[:1000]\n\n# Visualizing the Latent representation - Taking the average of the volume\nx_compressed = stacked_encoder.predict(X_test_sampled).mean(axis = -1).reshape(1000, -1)\n\n# Use TSNE\ntsne = TSNE(n_jobs=-1)\nX_compressed_2d  =  tsne.fit_transform(x_compressed)\n\n\n# Plot in scatterplot and color by digit name\nplt.figure(figsize = (12, 7))\ncmap = plt.get_cmap('RdBu', 10)\nsc = plt.scatter(X_compressed_2d[:, 0], X_compressed_2d[:, 1], c = y_test_sampled, alpha = .85, cmap = cmap)\ncax = plt.colorbar(sc, ticks=np.arange(0,10))\nplt.xlabel('tsne 1')\nplt.ylabel('tsne 2')\n#plt.colorbar(sc)","569aaafe":"stacked_encoder = keras.models.Sequential([\n    keras.layers.Flatten(input_shape = (28, 28)),\n    keras.layers.Dense(128, activation = 'selu'),\n    keras.layers.Dense(32, activation = 'selu'),\n])\n\nstacked_decoder = keras.models.Sequential([\n    keras.layers.Dense(128, activation = 'selu', input_shape = (32, )),\n    keras.layers.Dense(28*28, activation = 'sigmoid'),\n    keras.layers.Reshape([28, 28])\n])\n\nstacked_ae = keras.models.Sequential([stacked_encoder, stacked_decoder])\n\nstacked_ae.compile(loss = 'binary_crossentropy', optimizer = keras.optimizers.SGD(lr=.1))\n\nhistory  =  stacked_ae.fit(X_train, X_train, epochs = 25, validation_data = (X_test, X_test))","f7a2f10e":"# Extract the minimum and maximum values of the latent space. \nlatent = pd.DataFrame(stacked_encoder.predict(X_train), columns = [str(i) for i in range(32)])\nmins = latent.min(axis = 0).values\nmaxs = latent.max(axis = 0).values\nmean = latent.mean(axis = 0).values\nstddev = latent.std(axis = 0).values\n#print(latent.describe())","06e97b29":"# Create some data by sampling at random from the latent space - from a normal distribution\ncodings = tf.random.normal(mean = mean, stddev = stddev, shape = [12, 32])\n#codings = tf.random.uniform(minval = mins, maxval = maxs, shape = [12, 32])\nimages = stacked_decoder(codings).numpy()\ndef plot(mat):\n    n = mat.shape[0]\n    fig, axes = plt.subplots(1, n, figsize = (20, 4))\n    for image, ax in zip(mat, axes):\n        ax.imshow(image, cmap = 'Greys')\n        \nplot(images)","1c47c980":"y_train = train.label\nX_train =train.drop('label', axis=1)\/255\n\nX_test = test.drop('label', axis = 1)\/255\ny_test = test['label']\n\nX_train = X_train.values.reshape(X_train.shape[0], 28, 28, 1)\nX_test = X_test.values.reshape(X_test.shape[0], 28, 28, 1)\n","8f4fe3c4":"class sampling(keras.layers.Layer):\n    def call(self, inputs):\n        mean, log_var = inputs\n        return K.random_normal(tf.shape(log_var))*K.exp(log_var\/2)+mean","f7373c90":"########### Encoder Part ###########\ncodings_size = 10\ninputs = keras.layers.Input(shape = [28, 28])\nz = keras.layers.Flatten()(inputs)\nz = keras.layers.Dense(150, activation = 'selu')(z)\nz = keras.layers.Dense(100, activation = 'selu')(z)\ncodings_mean = keras.layers.Dense(codings_size)(z) #Mean Encoding \ncodings_log_var = keras.layers.Dense(codings_size)(z) #LogVar Encoding\ncodings = sampling()([codings_mean,  codings_log_var])\nvariational_encoder = keras.Model(inputs = [inputs], \n                                  outputs = [codings_mean, codings_log_var, codings])\n\n########### Decoder Part ###########\ndecoder_inputs = keras.layers.Input(shape=[codings_size])\nx = keras.layers.Dense(100, activation='selu')(decoder_inputs)\nx = keras.layers.Dense(150, activation='selu')(x)\nx = keras.layers.Dense(28*28, activation='sigmoid')(x)\noutputs = keras.layers.Reshape([28, 28])(x)\nvariational_decoder = keras.Model(inputs = [decoder_inputs], outputs = [outputs])\n\n########### Formalize autoencoder Model ##########\n_, _, codings = variational_encoder(inputs)\nreconstructions = variational_decoder(codings)\nvariational_ae = keras.Model(inputs = [inputs], outputs = [reconstructions])\n\n########## Define Latent Loss ##############\nlatent_loss = -0.5 *K.sum(1 + codings_log_var - K.exp(codings_log_var) - K.square(codings_mean), \n                          axis = -1)\nvariational_ae.add_loss(K.mean(latent_loss)\/784.)\nvariational_ae.compile(loss='binary_crossentropy', optimizer='adam')\n\n########## Fit Model ###########\nhistory = variational_ae.fit(X_train, X_train, epochs=70, batch_size=256,\n                            validation_data=(X_test, X_test))","3e747d9d":"codings = tf.random.normal(shape = [12, codings_size])\nimages = variational_decoder(codings).numpy()\ndef plot(mat):\n    n = mat.shape[0]\n    fig, axes = plt.subplots(1, n, figsize = (20, 4))\n    for image, ax in zip(mat, axes):\n        ax.imshow(image, cmap = 'Greys')\n        \nplot(images)","862fb13f":"It Reconstructs well but let us add convolutions to make the quality better.\nFirst, let us add regularization by building sparse autoencoders. This regularization can be applied to the coding layer.","59c4ff5c":"As we can see, the autoencoder did not work well as a generative model - as the artificial images generated do not look real. However, we can see it did capture the broad strokes.","12555eee":"## Generative Modeling - Variational Autoencoders(VAE)\nVariational Autoencoders are better for Generative modelling. We can use them to construct new data. As we shall see, data generated by VAE will look more real.","6c1658b0":"##### Generative model from a Regular Autoencoder","1a6375d3":"### Let us train another convolutional autoencoder - with different numbers of filters","ba92d31a":"### Autoencoders\nWe will explore:\n1. **Stacked Autoencoders**\n2. **Sparse Autoencoders**\n3. **Variational Autoencoders**\n\n**We will then visualize the Latent encodings provided by the autoencoders - by using TSNE to map the latent representations to 2 dimensions. It will allow us to see distinct clusters in the data.**\n\n**We will also explore the idea of generative modeling - which allows us to generate data that never existed.**\n\nWe will also explore:\n1. The difference between using a Dense Autoencoder vs a convolutional Autoencoder for Image data\n2. Comparing the results of using Regular Autoencoder vs Variational Autoencoder for Generative Modeling\n\n","70379baf":"#### Let us map the latent representations to 2 dimensions","7d272c62":"The clusters look very clear. The 4s occupy a place very close to 9s - which make sense because the tops of 4s and 9s look kindof similar. Similarly, 9 and 7 seem close - which makes sense as they have similar structure. So, our latent representation makes sense.","39984787":"#### Denoising using Dropout\nHere, we apply dropout to the input pixels - and allow the network to reconstruct them. This is another variation of denoising autoencoders. We hope, at the end of this, the network will learn to reconstruct the missing pixels.","30e06374":"#### Denoising after adding Random Noise to the inputs","fbcadbc5":"Plot one noisy data","1336e6c2":"We see the overlap between 9, 7 and 4 which we discussed earlier. We also see slightoverlap between 2, 7 which also makes sense. Denoising allowed the neural network to only reliably encode the most important features of the inputs. The Unimportant features could not be relied upon due to the noisiness of the data.  \n\n**Application**: Denoising Autoencoders not only lead to better dimensionality reduction which focuses more on the important features of the data, but can also be used for data denoising - as it has been trained on noisy data to recover the original data. ","fa9e19cf":"### Sparse Autoencoder - using Convolutions","2f5d44e5":"As we can see, these images generated by the Variational Autoencoder look better than the ones generated through a regular autoencoder.","dde95baa":"Reconstruct From Test Data","7b5d277f":"**Denoising Autoencoders gave a better distinction between clusters. It shows overlap between the clusters when it looks reasonable - like between 4, 9 and 3, 8.**","67d7e43b":"# Generative Modelling\n#### Let us use the autoencoder as a generative model\nThe original autoencoder we built, does not perform well as a generative model. So, if we sampled random vectors from the latent space, we will most likely get an image that does not look like any digit from 0 - 9. Regular Autoencoders are good for performing Anomaly detection but do not work well as generative models. \n\nTo make a good generative model, we need a Variational Autoencoder. \n\nHowever, let us compare and see what happens if we build a generative model from:\n* A regular Autoencoder\n* A Variational Autoencoder","d785a64f":"#### Visualize a Lower dimensional representation of the data - using the encoder output and applying TSNE","5751ba0a":"## Stacked Autoecoder","284a1a04":"## Denoising Autoencoder\n1. Using Random normal Noise\n2. Using Dropout\n\nThe idea is to corrupt the training data by noise, and use the autoencoder to reconstruct the training data. This way, the autoencoder learns to denoise the data - which once again, allows it to learn the essential features of the data.","f8dcc2bb":"In a sparse autoencoder, we restrict the activations of the middle layer to be sparse by adding an L1 Penalty to the activations of the middle layer. So, this means - that a lot of the activations of the middle layer will be zero - and the autoencoder will be forced to assign non-zero values only to the most important attributes of the data. ","15d4c9c0":"## Use CNNs to build the AutoEncoder","0095a2ac":"The 4s occupy a place very close to 9s - which make sense because the tops of 4s and 9s look kindof similar. Similarly, 9 and 7 seem close - which makes sense as they have similar structure. So, our latent representation makes sense.","4ab029e7":"View the Latent Representation coded by the autoencoder by applying TSNE on the latent representation","c656417a":"## Sparse Autoencoder","ef72bc06":"Let us store the stastical properties of the latent space present in the training data. We will then sample 'Artificial Data' from this."}}