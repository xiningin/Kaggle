{"cell_type":{"62977be8":"code","8ba182f6":"code","bde25e7f":"code","6586c23a":"code","e1acdd41":"code","f3bc5205":"code","45e496d1":"code","25dafde2":"code","68e41e2d":"code","8a966c9a":"code","a32179c5":"code","1d7280df":"code","a7bae1d0":"code","e57d2db7":"code","704816e3":"code","1693c884":"code","6f062e77":"code","0cbaf9db":"code","b6c7a888":"code","9f9d1394":"code","a97d5dcf":"code","1df82449":"code","d505ba9c":"code","0ab78dfc":"code","c0065458":"code","8c1224bc":"code","20966beb":"code","fca949c8":"code","2d9cb2e6":"code","31cc5d4f":"code","3d1e1818":"code","5cf42356":"code","e6cdecaf":"code","bdc62dd9":"code","43a5d123":"code","0eb7d280":"code","6281e27c":"code","eb45f5a6":"code","0923e924":"code","660eb13d":"code","aef5a16b":"code","1f065805":"code","46c5c713":"code","cc6c99f5":"code","f24f86f2":"code","8c89b0c3":"code","08b09f1c":"code","320cd6af":"code","90c720a3":"code","c7922707":"code","5b140ba7":"code","da38f298":"code","8e04b51e":"code","34e55f97":"code","d0bc3fdb":"code","7775f91c":"code","3dd13b20":"code","7c56ccf7":"code","1b6cb424":"code","6b8d1e0f":"code","22250ed0":"code","d4426293":"code","30d4ddd4":"code","d4eecbfa":"code","cee60c2b":"code","6894ffeb":"code","41937c38":"code","37ba7ea2":"code","98e62317":"code","3582c2ab":"code","0206d347":"code","13981207":"code","c4b1ba82":"code","23868d4b":"code","27aa7011":"code","7e587f03":"code","3bb4ca2b":"code","1372e00a":"code","5b605653":"code","e048e7de":"code","7d9c55d3":"code","59d2cb0b":"code","c0e5c9aa":"code","a8c7872d":"code","fac92b6b":"code","ac868c88":"code","33cefad2":"code","21d00343":"code","316b77bc":"code","a48ac871":"code","77ee71da":"code","2b470b64":"code","c0b0435f":"code","0b8b85aa":"code","7c7ad6ad":"code","cb872ef4":"code","ccfa17ae":"code","53ad73e9":"code","fa9a51b5":"code","e8a740e3":"code","de906fe3":"code","aa562ea1":"code","9c6ca026":"code","1e593207":"code","29bd2169":"code","66a11401":"code","70508a99":"code","67f5a670":"code","bb498305":"code","6e60a1d9":"code","e85632d3":"code","98806d56":"code","ffa1bef2":"code","f889fa9c":"code","f4feeb1c":"code","465a4465":"markdown","4b166066":"markdown","405a7680":"markdown","3bd8c92f":"markdown","2bf2107a":"markdown","b2ae06f4":"markdown","cd92f3e8":"markdown","7b2e0ba4":"markdown","d7cdc7cd":"markdown","052736f2":"markdown","7768ac3b":"markdown","d1d8b9d8":"markdown","28b3e3ea":"markdown","38b2b1f2":"markdown","0635f070":"markdown","d9cb9a6c":"markdown","714b0691":"markdown","6233c38d":"markdown","5601c122":"markdown","d83db39f":"markdown","9dbd8d44":"markdown","1058e9d4":"markdown","bf25024e":"markdown","64b3062a":"markdown","f8609836":"markdown","315bc51a":"markdown","f16789d8":"markdown","8bfb54fe":"markdown","311c25be":"markdown","b04b669a":"markdown","86bdde15":"markdown","72d3785f":"markdown","6689fcfe":"markdown","33270b83":"markdown","4a9d5ebc":"markdown","6cdaa75f":"markdown","55ef3032":"markdown","c062d6c2":"markdown","90f3451e":"markdown","252d29a0":"markdown","0947e867":"markdown","8fc453ec":"markdown"},"source":{"62977be8":"# https:\/\/www.kaggle.com\/cdc\/national-health-and-nutrition-examination-survey\/home\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport missingno as msno\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nprint(\"Input Directory:\")\nos.listdir(\"..\/input\")","8ba182f6":"\"\"\"\nDiseases of the rich -> Heart disease\nLinked to high cholesteral\nUse demog and habbtis to see who's at risk of high cholesteral\n\nhttps:\/\/www.webmd.com\/heart-disease\/guide\/heart-disease-lower-cholesterol-risk#1\nhttps:\/\/www.medicalnewstoday.com\/articles\/315900.php\nhttps:\/\/www.who.int\/nutrition\/topics\/2_background\/en\/\n\"\"\"","bde25e7f":"\"\"\"\nJust going to use Demographic, questionnaire (habbits) & cholestoral level to help us answer the question:\n\nCOULD WE IDENTIFY PEOPLE AT RISK OF HIGH CHOLESTEROL LEVELS FROM A SHORT BEHAVIOURAL SURVEY\n\"\"\"\n\ndemog = pd.read_csv(\"..\/input\/national-health-and-nutrition-examination-survey\/demographic.csv\")\nques = pd.read_csv(\"..\/input\/national-health-and-nutrition-examination-survey\/questionnaire.csv\")\nlabs = pd.read_csv(\"..\/input\/national-health-and-nutrition-examination-survey\/labs.csv\")","6586c23a":"# check to see if there are shared column names in our two tables, before combining\ndemog_cols = demog.columns\nques_cols = ques.columns\nprint(\"The demog and question datasets share the {} column\".format(set(demog_cols).intersection(ques_cols)))\nprint(\"So will join them on this column\")\n\n# check shaped of these columne\nprint(\"\\nShape of Demographics dataset: {}\".format(demog.shape))\nprint(\"Shape of Questionnaire dataset: {}\".format(ques.shape))\n\n# Join the two datasets on the unique number for respondants\ndataset = demog.join(ques.set_index('SEQN'), on='SEQN')\nprint(\"\\nShape of COMBINED dataset: {}**\".format(dataset.shape))\nprint(\"\\n** one less column since SEQN is shared\")","e1acdd41":"# Get cholesteral levels info, first see how many answers we have\nlabs['LBXTC'].describe()","f3bc5205":"cholesterol = labs[['SEQN','LBXTC']]\ndataset = dataset.join(cholesterol.set_index('SEQN'), on='SEQN')\n\nprint(\"Finally, add the cholesterol level (target) column.\")\nprint(\"New Shape: {}\".format(dataset.shape))\n\n# only keep the rows where we have cholesteral info\ndataset = dataset[dataset['LBXTC'].isna() == False]\nprint(\"\/nFiltering for records where we have Cholesterol info gives final shape: {}\".format(dataset.shape))","45e496d1":"## define function to categorise the cholesteral level\ndef cholesterol_level(mg):\n    \"\"\"\n    1 - OK\n    2 - Borderline High (at risk)\n    3 - High\n    \"\"\"\n    if mg < 200.0:\n        return 1\n    elif mg < 240:\n        return 2\n    else:\n        return 3","25dafde2":"# categorise cholesterol\ndataset['Target'] = dataset['LBXTC'].apply(lambda x: cholesterol_level(x))\n\nprint(\"Group cholesterol levels in to 3 categories:\")\nprint(\"1 - OK\")\nprint(\"\\n2 - Borderline High (at risk)\")\nprint(\"\\n3 - High\")\n\nprint(\"\\nWith the following totals:\")\nprint(dataset['Target'].value_counts())","68e41e2d":"### first check for the Demographics\nmsno.matrix(dataset.iloc[:, 0:47])","8a966c9a":"#print(dataset.columns[0:47])\n#dataset.iloc[:,0:47].isna().sum()\ncols_missing_vals = []\ntot_missing_vals = []\nfor col,missing in zip(dataset.columns[0:47], dataset.iloc[:,0:47].isna().sum()):\n    if missing > 0:\n        cols_missing_vals.append(col)\n        tot_missing_vals.append(missing)\n\ncols_missing_vals_df = pd.DataFrame({'Variable Name': cols_missing_vals,'Records Mising': tot_missing_vals})\n","a32179c5":"## using data from https:\/\/wwwn.cdc.gov\/Nchs\/Nhanes\/Search\/variablelist.aspx?Component=Demographics&CycleBeginYear=2013\n## get decription of the questions asked\n\ndemog_qs = pd.read_csv(\"..\/input\/goodhealth\/demog_qs.csv\")\nprint(\"Total number of cols in description file (should be 47): {}\".format(len(demog_cols)))\nprint(\"Display descriptions with missing records where relevant\")\n\n# filter out duplicate Qs by only using those Qs with no use constraints\ndemog_description  = demog_qs[demog_qs['Variable Name'].isin(demog_cols)]\ndemog_description  = demog_description[demog_description['Use Constraints']=='None']\n\n# View descriptions of columns with missing data\npd.set_option('display.max_colwidth', -1)\npd.merge(demog_description, cols_missing_vals_df, how='outer', on=['Variable Name'])","1d7280df":"##TRY TO COMBINE DMDEDUC2 & DMDEDUC3\nDEMO_KEEP_COLS = ['SEQN','RIAGENDR','WTINT2YR','RIDAGEYR','INDFMIN2','DMDHHSIZ','DMDHHSZA','DMDHHSZB','DMDHHSZE']","a7bae1d0":"msno.matrix(dataset[['DMDEDUC2','DMDEDUC3']])","e57d2db7":"dataset['YearsOfEduc'] = dataset.fillna(0)['DMDEDUC3'] + dataset.fillna(0)['DMDEDUC2']\nprint(\"Number of missing records in combined colum: {}\".format(dataset['YearsOfEduc'].isna().sum()))","704816e3":"DEMO_KEEP_COLS = DEMO_KEEP_COLS + ['YearsOfEduc']","1693c884":"# see if missing values in these columns\ncount_missing =[]\nname_ques_missing =[]\nfor col, missing in zip(dataset.columns[48:],dataset.iloc[:,48:].isna().sum()):\n    if missing > 380:\n        count_missing.append(1)\n        name_ques_missing.append(col)\nprint(\"{} columns with 5% or more missing records\".format(len(count_missing)))\n      \nmsno.matrix(dataset.iloc[:, 48:])","6f062e77":"### our list of columns in the questionnaire\n### dropping those with so many missing\nques_cols_new = ques_cols.drop(name_ques_missing)","0cbaf9db":"ques_qs = pd.read_csv(\"..\/input\/goodhealth\/questionnaire_qs.csv\")\nprint(\"Total number of cols in description file (should be 47): {}\".format(len(ques_cols)))\nprint(\"Display descriptions with missing records where relevant\")\n\n# filter out duplicate Qs by only using those Qs with no use constraints and where >5% of records are NaN\nques_description  = ques_qs[ques_qs['Variable Name'].isin(ques_cols.drop(name_ques_missing))]\nprint(ques_description.shape)\nques_description  = ques_description[(ques_description['Use Constraints']=='None') & (ques_description['Variable Name'] != 'SEQN')]\nprint(ques_description.shape)\nlen(ques_description['Variable Name'].unique())\nques_description\n","b6c7a888":"QUES_KEEP_COLS = ['DLQ010','DLQ020','DLQ040','DLQ050','DLQ060','MCQ010','MCQ053','MCQ082','MCQ086','MCQ092','MCQ203','HIQ011','HUQ051',\n                  'HUQ071','HUQ090','PAQ710','PAQ715','DIQ010','SMD460','HOD050','HOQ065','INQ060','INQ080','INQ090','INQ132','INQ140',\n                  'INQ150','CBD120','CBD130','FSD032A','FSD032B','FSD032C','FSD151','FSQ165','OHQ030']","9f9d1394":"KEEP_COLS = DEMO_KEEP_COLS + QUES_KEEP_COLS + ['Target']\nprint(\"A total of {} columns to keep\".format(len(KEEP_COLS)))","a97d5dcf":"dataset = dataset[KEEP_COLS]\nmsno.matrix(dataset)","1df82449":"### Not to many NaNs. Lets just take them all out.\ndataset = dataset.dropna(axis='index')\nprint(\"Final Dataset has {} unique records\".format(len(dataset['SEQN'].unique())))\nprint(\"{} missing data points\".format(dataset.isna().sum().sum()))","d505ba9c":"dataset.head()","0ab78dfc":"### CATEGORIZE THE COLUMNS\nBINARY_COLS = ['RIAGENDR','DLQ010','DLQ020','DLQ040','DLQ050','DLQ060','MCQ010','MCQ053','MCQ082','MCQ086','MCQ092','MCQ203','HIQ011','HUQ071','HUQ090','INQ060','INQ080','INQ090','INQ132','INQ140','INQ150']\n# INQ060 7,9\nCAT_COLS = ['INDFMIN2','PAQ710','PAQ715','DIQ010','SMD460','HOQ065','FSD032A','FSD032B','FSD032C','FSD151','FSQ165'] #remove don't know\/refused as 7,9,77,99,777,999\nCAT_COLS_SPEC = ['HUQ051','OHQ030'] # these have 7 & 9 as vals so treat seperate\nNUM_COLS = ['WTINT2YR','RIDAGEYR','DMDHHSIZ','DMDHHSZA','DMDHHSZB','DMDHHSZE','YearsOfEduc','HOD050','CBD120','CBD130']\n# HOD050 remove 777,999; CBD120\/CBD130 remove 777777, 999999\n","c0065458":"### Define a function to remove the rows of \"don't know\" and \"refused\"\ndef remove_refused_dontknow(dataframe, cols_list, removal_list):\n    for col in cols_list:\n        dataframe = dataframe[~dataframe[col].isin(removal_list)]\n    return dataframe","8c1224bc":"print(\"Original number of records: {}\".format(dataset.shape))\n\ndataset = remove_refused_dontknow(dataset, BINARY_COLS, [7,9])\nprint(\"Records after BINARY processed: {}\".format(dataset.shape))\n\ndataset = remove_refused_dontknow(dataset, CAT_COLS, [7,9,77,99,777,999])\nprint(\"Records after CAT processed: {}\".format(dataset.shape))\n\ndataset = remove_refused_dontknow(dataset, CAT_COLS_SPEC, [77,99,777,999])\nprint(\"Records after CAT_SPEC processed: {}\".format(dataset.shape))\n\ndataset = remove_refused_dontknow(dataset, NUM_COLS, [777,999,777777,999999])\nprint(\"Records after NUM processed: {}\".format(dataset.shape))\n\ndataset = remove_refused_dontknow(dataset, ['YearsOfEduc'], [55,66,99])\nprint(\"Records after  Years of Education processed: {}\".format(dataset.shape))","20966beb":"### DROP THE SEQN COLUMN\ndataset = dataset.drop(columns=['SEQN'], axis=1)","fca949c8":"sns.set(rc={'figure.figsize':(9, 5)})\nsns.countplot(dataset['Target'])\nplt.title(\"Target Variables\")\nplt.show()","2d9cb2e6":"def at_risk(score):\n    if score == 1:\n        return 0 # no risk\n    else:\n        return 1 # at risk","31cc5d4f":"dataset['Target'] = dataset['Target'].apply(lambda x: at_risk(x))\nprint(dataset.Target.value_counts() \/ dataset.Target.count()) #not balanced yet, but we'll come back to it\nprint(dataset.Target.value_counts())\n# look at response variable\nsns.set(rc={'figure.figsize':(9, 5)})\nsns.countplot(dataset['Target'])\nplt.title(\"Target Variables\")\nplt.show()","3d1e1818":"def plot_corr_matrix(dataset):\n    # Correlation analasys\n    corrMatt = dataset.corr()\n    mask = np.array(corrMatt)\n    mask[np.tril_indices_from(mask)] = False\n    plt.figure(figsize = (20,10))\n    sns.heatmap(corrMatt, mask = mask, annot = True)","5cf42356":"plot_corr_matrix(dataset[NUM_COLS])","e6cdecaf":"# see correlatiosn with the RESPONSE variable\ndataset.iloc[:,:-1].corrwith(dataset.Target).plot.bar(figsize= (20,10),\n                                            title = 'Correlations with Response Variable',\n                                            fontsize = 15, rot = 45, grid = True) \nplt.show()","bdc62dd9":"NUM_COLS = [e for e in NUM_COLS if e not in ('DMDHHSIZ','DMDHHSZA','DMDHHSZB','DMDHHSZE')]\ndataset = dataset.drop(columns = ['DMDHHSIZ','DMDHHSZA','DMDHHSZB','DMDHHSZE'], axis=1)","43a5d123":"sns.pairplot(dataset, hue='Target', vars=NUM_COLS)","0eb7d280":"## Box plots of features and response\nsns.set(style=\"whitegrid\")\nfig, axes = plt.subplots(nrows=1, ncols=2)\nfig.set_size_inches(18,10)\na = sns.boxplot(data=dataset[NUM_COLS[1:]], orient='v', ax=axes[0]) # numerical features except salary\nb = sns.boxplot(data=dataset['WTINT2YR'], orient='v', ax=axes[1])  #salary\na.set_xticklabels(labels = NUM_COLS[1:], rotation=90)\nb.set_xticklabels(labels=['Household Income'])\na.set_title('Box plot of numerical features (not salary)')\nb.set_title('Box plot of salary')\nplt.show()\n","6281e27c":"def remove_outliers(dataframe, column, num_std_dev):\n    \"\"\"\n    dataframe -- dataframe to trim\n    column -- column in the datarame to trim\n    num_std -- the number of Standard dev above which to remove (for normal disribution > 2 std is 2.2%)\n    \"\"\"\n    mean = np.mean(dataset[column])\n    std = np.std(dataset[column])\n    \n    dataframe = dataframe[dataframe[column] < mean + num_std_dev*std]\n    return dataframe","eb45f5a6":"## REMOVE OUTLIERS\ndataset = remove_outliers(dataset, 'CBD120', 2)\ndataset = remove_outliers(dataset, 'CBD130', 2)","0923e924":"## CHECK HOW IT LOOKS\nsns.set(rc={'figure.figsize':(18, 10)})\nax = sns.boxplot(data=dataset[NUM_COLS[1:]], orient='v') # numerical features except salary\nax.set_xticklabels(labels = NUM_COLS[1:], rotation=90)\nax.set_title('Box plot of numerical features (not salary) - OUTLIERS REMOVED')\n","660eb13d":"(dataset[BINARY_COLS].shape[1])","aef5a16b":"fig = plt.figure(figsize=(18,10))\n\nplt.suptitle('Pie Chart Distributions - BINARY', fontsize = 20)\n\nfor i in range(1, dataset[BINARY_COLS].shape[1]+1):\n    plt.subplot(5, 5, i)\n    f = plt.gca()\n    f.axes.get_yaxis().set_visible(False)\n    f.set_title(dataset[BINARY_COLS].columns.values[i-1])\n    \n    values = dataset[BINARY_COLS].iloc[:, i-1].value_counts(normalize = True).values\n    index = dataset[BINARY_COLS].iloc[:, i-1].value_counts(normalize = True).index\n    plt.pie(values, labels = index, autopct='%1.1f%%')\n    plt.axis('equal')\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])","1f065805":"dataset[dataset['DLQ060'] == 1].Target.value_counts()","46c5c713":"dataset[dataset['MCQ053'] == 1].Target.value_counts()","cc6c99f5":"dataset[dataset['MCQ082'] == 1].Target.value_counts()","f24f86f2":"dataset[dataset['MCQ086'] == 1].Target.value_counts()","8c89b0c3":"dataset[dataset['MCQ203'] == 1].Target.value_counts()","08b09f1c":"CAT_COLS = CAT_COLS + CAT_COLS_SPEC","320cd6af":"plt.suptitle('Pie Chart Distributions - CATEGORICAL', fontsize = 20)\n\nfor i in range(1, dataset[CAT_COLS].shape[1]+1):\n    plt.subplot(5, 5, i)\n    f = plt.gca()\n    f.axes.get_yaxis().set_visible(False)\n    f.set_title(dataset[CAT_COLS].columns.values[i-1])\n    \n    values = dataset[CAT_COLS].iloc[:, i-1].value_counts(normalize = True).values\n    index = dataset[CAT_COLS].iloc[:, i-1].value_counts(normalize = True).index\n    plt.pie(values, labels = index, autopct='%1.1f%%')\n    plt.axis('equal')\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])","90c720a3":"dataset[dataset['INDFMIN2'] == 13].Target.value_counts()","c7922707":"dataset[dataset['PAQ710'] == 8].Target.value_counts()","5b140ba7":"dataset[dataset['DIQ010'] == 3].Target.value_counts()","da38f298":"dataset[dataset['SMD460'] == 3].Target.value_counts()","8e04b51e":"dataset[dataset['HUQ051'] == 7].Target.value_counts()","34e55f97":"dataset[dataset['OHQ030'] == 7].Target.value_counts()","d0bc3fdb":"cat_str = dataset[CAT_COLS].astype('category')\ncat_ohe = pd.get_dummies(cat_str)\n\n## since these \"binary\" columns are currently stored as 1 or 2\nbin_str = dataset[BINARY_COLS].astype('category')\nbin_ohe = pd.get_dummies(bin_str)\n\ndataset = dataset.drop(columns=CAT_COLS, axis=1)\ndataset = dataset.drop(columns=BINARY_COLS, axis=1)","7775f91c":"print(cat_ohe.shape)\nprint(bin_ohe.shape)\nprint(dataset.shape)","3dd13b20":"## drop the extra columns to remove dependency\ncat_ohe = cat_ohe.drop(columns=['INDFMIN2_1.0','PAQ710_0.0','PAQ715_0.0','DIQ010_1.0','SMD460_0.0','HOQ065_1.0','FSD032A_1.0','FSD032B_1.0',\n                          'FSD032C_1.0','FSD151_1.0','FSQ165_1.0','HUQ051_0','OHQ030_1.0'], axis=1)\n\nbin_ohe = bin_ohe.drop(columns = ['RIAGENDR_1','DLQ010_1.0','DLQ020_1.0','DLQ040_1.0','DLQ050_1.0','DLQ060_1.0','MCQ010_1.0','MCQ053_1.0',\n                        'MCQ082_1.0','MCQ086_1.0','MCQ092_1.0','MCQ203_1.0','HIQ011_1','HUQ071_1','HUQ090_1.0','INQ060_1.0','INQ080_1.0',\n                        'INQ090_1.0','INQ132_1.0','INQ140_1.0','INQ150_1.0'], axis=1)\n\nprint(cat_ohe.shape)\nprint(bin_ohe.shape)\n","7c56ccf7":"dataset = dataset.join(cat_ohe).join(bin_ohe)","1b6cb424":"dataset.shape","6b8d1e0f":"X = dataset.drop(columns=['Target'], axis=1)\ny = dataset['Target']","22250ed0":"X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.3, random_state=42)","d4426293":"X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)","30d4ddd4":"print(\"Size of Training set is {} records\".format(X_train.shape[0]))\nprint(\"Size of Dev set is {} records\".format(X_dev.shape[0]))\nprint(\"Size of Test set is {} records\".format(X_test.shape[0]))","d4eecbfa":"X_Sc = StandardScaler()","cee60c2b":"## Only Scaling the Numerical Columns not Binary\nX_train_bin = X_train.drop(NUM_COLS, axis = 1) \nX_dev_bin = X_dev.drop(NUM_COLS, axis = 1) \nX_test_bin = X_test.drop(NUM_COLS, axis = 1) \n\n\nX_train = X_train[NUM_COLS]\nX_dev = X_dev[NUM_COLS] \nX_test = X_test[NUM_COLS]","6894ffeb":"X_train2 = pd.DataFrame(X_Sc.fit_transform(X_train))\nX_dev2 = pd.DataFrame(X_Sc.transform(X_dev))\nX_test2 = pd.DataFrame(X_Sc.transform(X_test))\n\n#scaler returns numpy array and lose index and columns names which we don't want!\nX_train2.columns = X_train.columns.values\nX_dev2.columns = X_dev.columns.values\nX_test2.columns = X_test.columns.values\n\nX_train2.index = X_train.index.values\nX_dev2.index = X_dev.index.values\nX_test2.index = X_test.index.values\n\n# combine the numerical and categorical values\nX_train = pd.concat([X_train2, X_train_bin],axis=1, sort=False)\nX_dev = pd.concat([X_dev2, X_dev_bin],axis=1, sort=False)\nX_test = pd.concat([X_test2, X_test_bin],axis=1, sort=False)\n\n# check shape\nprint(X_train.shape)\nprint(X_dev.shape)\nprint(X_test.shape)\n\nprint(y_train.shape)\nprint(y_dev.shape)\nprint(y_test.shape)","41937c38":"X_train.head(10)","37ba7ea2":"## First lets have make a table to store our results\nresults_df = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])","98e62317":"random_forest = RandomForestClassifier(n_estimators=400)\n\ny_train = pd.DataFrame(y_train)\nrandom_forest.fit(X_train, y_train.values.ravel())","3582c2ab":"### function for plotting scores\n\ndef how_did_it_do(ml_model, X_dev, y_dev, use_model_dot_score=True, cf_matrix=True):\n    ## predict from dev set\n    y_pred = ml_model.predict(X_dev)\n    print(\"performance on X_dev:\")\n    \n    if use_model_dot_score:\n        # Accuracy\n        print(\"\\nAccuracy:\")\n        acc = round(ml_model.score(X_dev, y_dev), 3)\n        print(acc)\n    else:\n        print(\"\\nAccuracy score:\")\n        acc = round(accuracy_score(y_dev, y_pred), 3)\n        print(acc)    \n    \n\n\n    # of predicted +ve, how many correct\n    print(\"Precision score:\")\n    prec = round(precision_score(y_dev, y_pred, average='macro'), 3)\n    print(prec)\n\n\n    # of all actual +ve how many did we get\n    print(\"Recall score:\")\n    rec = round(recall_score(y_dev, y_pred, average='macro'), 3)\n    print(rec)\n\n    # f1 combines\n    print(\"Global F1 score:\")\n    f1 = round(f1_score(y_dev, y_pred, average='macro'), 3)\n    print(f1)\n    \n    ### plot confusion matrix if needed\n    if cf_matrix:\n        cm = confusion_matrix(y_dev, y_pred.round())\n        df_cm = pd.DataFrame(cm, index = (0,1), columns=(0,1))\n        plt.figure(figsize = (10,7))\n        sns.set(font_scale=1.4)\n        sns.heatmap(df_cm, annot = True, fmt='g')\n        plt.ylabel('True label')\n        plt.xlabel('Predicted label')\n        plt.show()\n        \n    ","0206d347":"how_did_it_do(random_forest, X_dev, y_dev, cf_matrix=True)","13981207":"model_results = pd.DataFrame([['RandomForest_1 (n=400)', 0.753, 0.663, 0.571, 0.57]], columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\nresults_df = results_df.append(model_results, ignore_index=True)\nresults_df","c4b1ba82":"from imblearn.over_sampling import SMOTE\nX_train_nonBal = X_train\ny_train_nonBal = y_train","23868d4b":"train_cols = X_train.columns\nsm = SMOTE(random_state=2)\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train)\nX_train_res = pd.DataFrame(X_train_res, columns=train_cols)","27aa7011":"X_train = X_train_res\ny_train = pd.DataFrame(y_train_res, columns=[\"Target\"])","7e587f03":"y_train.Target.value_counts()","3bb4ca2b":"random_forest_2 = RandomForestClassifier(n_estimators=400)\n\ny_train = pd.DataFrame(y_train)\nrandom_forest_2.fit(X_train, y_train.values.ravel())","1372e00a":"how_did_it_do(random_forest_2, X_dev, y_dev)","5b605653":"model_results = pd.DataFrame([['RandomForest_2 (Balanced Data, n=400)', 0.74, 0.639, 0.595, 0.603]], columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\nresults_df = results_df.append(model_results, ignore_index=True)\nresults_df","e048e7de":"## Set the params we're gonna try\nparameters = {'max_depth': [100, None],\n             \"max_features\" : [\"auto\", \"log2\", \"sqrt\"],\n             'n_estimators': [400, 500],\n             'min_samples_split': [2, 5, 10],\n             'min_samples_leaf': [1, 5, 10],\n             'bootstrap': [True, False],\n             'criterion': ['gini','entropy']}\n\nfrom sklearn.model_selection import GridSearchCV\n\n# n_job = -1tells it to use all cores on your computer\ngrid_search = GridSearchCV(estimator = random_forest_2,\n                           param_grid = parameters,\n                           scoring = 'f1_macro',\n                           verbose=1,\n                           cv = 2,\n                           n_jobs = 8)","7d9c55d3":"\"\"\"import time\nt0 = time.time()\ngrid_search = grid_search.fit(X_train, y_train.values.ravel())\nt1 = time.time()\n\nprint(\"Took %0.2f seconds\" % (t1 - t0))\"\"\"","59d2cb0b":"\"\"\"rf_best_accuracy = grid_search.best_score_\nfr_best_paramaters = grid_search.best_params_\n\nprint(rf_best_accuracy)\nfr_best_paramaters\"\"\"","c0e5c9aa":"random_forest_3 = RandomForestClassifier(n_estimators=400, bootstrap=False, criterion='gini', max_depth=100, max_features='log2',\n                                        min_samples_leaf=1, min_samples_split=2)\n\ny_train = pd.DataFrame(y_train)\nrandom_forest_3.fit(X_train, y_train.values.ravel())","a8c7872d":"how_did_it_do(random_forest_3, X_dev, y_dev)","fac92b6b":"model_results = pd.DataFrame([['RandomForest_3 (Grid Search x1)', 0.738, 0.628, 0.573, 0.577]], columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\nresults_df = results_df.append(model_results, ignore_index=True)\nresults_df","ac868c88":"from sklearn.linear_model import LogisticRegression\n\nlogistic_regression = LogisticRegression(random_state = 0)\n\nlogistic_regression.fit(X_train, y_train.values.ravel())","33cefad2":"how_did_it_do(logistic_regression, X_dev, y_dev)","21d00343":"model_results = pd.DataFrame([['Logistic Regression', 0.638, 0.601, 0.628, 0.595]], columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\nresults_df = results_df.append(model_results, ignore_index=True)\nresults_df","316b77bc":"### K-Fold Cross Val \nfrom sklearn.model_selection import cross_val_score\n\naccuracies = cross_val_score(estimator = logistic_regression,\n                            X = X_train, \n                            y = y_train.values.ravel(),\n                            scoring = 'f1',\n                            cv = 5)\nprint(accuracies)\nprint(accuracies.mean())","a48ac871":"from sklearn.svm import SVC\n\nsvc_linear = SVC(random_state = 0, kernel = 'linear')\n\nsvc_linear.fit(X_train, y_train.values.ravel())","77ee71da":"how_did_it_do(svc_linear, X_dev, y_dev)","2b470b64":"model_results = pd.DataFrame([['SVM (linear)', 0.636, 0.61, 0.642, 0.6]], columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\nresults_df = results_df.append(model_results, ignore_index=True)\nresults_df","c0b0435f":"from sklearn.svm import SVC\n\nsvc_rbf = SVC(random_state = 0, kernel = 'rbf')\n\nsvc_rbf.fit(X_train, y_train.values.ravel())","0b8b85aa":"how_did_it_do(svc_rbf, X_dev, y_dev)","7c7ad6ad":"model_results = pd.DataFrame([['SVM (RBF)', 0.63, 0.624, 0.662, 0.604]], columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\nresults_df = results_df.append(model_results, ignore_index=True)\nresults_df","cb872ef4":"from sklearn import svm\ndef svc_param_selection(X, y, nfolds):\n    Cs = [0.001, 0.01, 0.1, 1, 10]\n    gammas = [0.001, 0.01, 0.1, 1]\n    param_grid = {'C': Cs, 'gamma' : gammas}\n    grid_search = GridSearchCV(svm.SVC(kernel='rbf'), param_grid, cv=nfolds, verbose=1)\n    grid_search.fit(X, y)\n    grid_search.best_params_\n    return grid_search.best_params_","ccfa17ae":"svc_param_selection(X_train, y_train.values.ravel(), 3)","53ad73e9":"svc_rbf_2 = svm.SVC(kernel = 'rbf', C=10, gamma=1, random_state=42)","fa9a51b5":"svc_rbf_2.fit(X_train, y_train.values.ravel())","e8a740e3":"how_did_it_do(svc_rbf_2, X_dev, y_dev)","de906fe3":"model_results = pd.DataFrame([['SVM (RBF 2) C=10, gamma=1', 0.743, 0.371, 0.5, 0.426]], columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\nresults_df = results_df.append(model_results, ignore_index=True)\nresults_df","aa562ea1":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout","9c6ca026":"model = Sequential([\n    Dense(units = 16, input_dim = 79, activation = 'relu'),\n    Dense(units = 24, activation = 'relu'),\n    Dropout(0.7),\n    Dense(20, activation = 'relu'),\n    Dropout(0.7),\n    Dense(20, activation = 'relu'),\n    Dropout(0.7),\n    Dense(24, activation = 'relu'),\n    Dense(1, activation = 'sigmoid')\n])\n\nmodel.summary()","1e593207":"model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","29bd2169":"model.fit(X_train, y_train, batch_size=32, epochs=50)","66a11401":"y_pred = model.predict(X_dev)\ny_pred = y_pred.round()","70508a99":"    print(\"\\nAccuracy score:\")\n    print(round(accuracy_score(y_dev, y_pred), 3))    \n\n    # of predicted +ve, how many correct\n    print(\"Precision score:\")\n    print(round(precision_score(y_dev, y_pred, average='macro'), 3))\n\n\n    # of all actual +ve how many did we get\n    print(\"Recall score:\")\n    print(round(recall_score(y_dev, y_pred, average='macro'), 3))\n\n    # f1 combines\n    print(\"Global F1 score:\")\n    print(round(f1_score(y_dev, y_pred, average='macro'), 3))\n    \n    ### plot confusion matrix \n\n    cm = confusion_matrix(y_dev, y_pred.round())\n    df_cm = pd.DataFrame(cm, index = (0,1), columns=(0,1))\n    plt.figure(figsize = (10,7))\n    sns.set(font_scale=1.4)\n    sns.heatmap(df_cm, annot = True, fmt='g')\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","67f5a670":"model_results = pd.DataFrame([['DNN', 0.637, 0.601, 0.629, 0.595]], columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\nresults_df = results_df.append(model_results, ignore_index=True)\nresults_df","bb498305":"model_2 = Sequential([\n    Dense(units = 16, input_dim = 79, activation = 'relu'),\n    Dense(units = 24, activation = 'relu'),\n    Dropout(0.7),\n    Dense(20, activation = 'relu'),\n    Dropout(0.7),\n    Dense(20, activation = 'relu'),\n    Dropout(0.7),\n    Dense(24, activation = 'relu'),\n    Dense(1, activation = 'sigmoid')\n])\n\nmodel_2.summary()","6e60a1d9":"model_2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nmodel_2.fit(X_train_nonBal, y_train_nonBal, batch_size=32, epochs=50)","e85632d3":"y_pred = model_2.predict(X_dev)\ny_pred = y_pred.round()","98806d56":"    print(\"\\nAccuracy score:\")\n    print(round(accuracy_score(y_dev, y_pred), 3))    \n\n    # of predicted +ve, how many correct\n    print(\"Precision score:\")\n    print(round(precision_score(y_dev, y_pred, average='macro'), 3))\n\n\n    # of all actual +ve how many did we get\n    print(\"Recall score:\")\n    print(round(recall_score(y_dev, y_pred, average='macro'), 3))\n\n    # f1 combines\n    print(\"Global F1 score:\")\n    print(round(f1_score(y_dev, y_pred, average='macro'), 3))\n    \n    ### plot confusion matrix \n\n    cm = confusion_matrix(y_dev, y_pred.round())\n    df_cm = pd.DataFrame(cm, index = (0,1), columns=(0,1))\n    plt.figure(figsize = (10,7))\n    sns.set(font_scale=1.4)\n    sns.heatmap(df_cm, annot = True, fmt='g')\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","ffa1bef2":"model_results = pd.DataFrame([['DNN 2 (Original Data)', 0.643, 0.371, 0.5, 0.426]], columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])\nresults_df = results_df.append(model_results, ignore_index=True)\nresults_df","f889fa9c":"how_did_it_do(svc_rbf, X_test, y_test)","f4feeb1c":"### re-run first DNN but plot learning curve\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, validation_data=(X_dev, y_dev), epochs=50, batch_size=16, verbose=1)\n\n# list all data in history\nprint(history.history.keys())\n\n# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","465a4465":"# EDA\n## Correlation","4b166066":"## QUESTIONNAIRE Feature Selection","405a7680":"# SPLIT dataset","3bd8c92f":"### predicting OK when at risk... could be due to unbalnced data set. Lets balance training set","2bf2107a":"* 999 features is quite a lot to start with\n* lets check for missing values to see if we should drop some of these\n\n## Demographic feature selection","b2ae06f4":"* OK these are also good","cd92f3e8":"* Some sizable correlations amongst the household size figures and age\n* in retrospect was a bit silly to include all\n* will remove the HH size variables since AGE is important for sure\n\nFirst lets correlation with Response variable","7b2e0ba4":"#### Get a list of these columns with missing values","d7cdc7cd":"# Scale dataset","052736f2":"### Same for CATEGORICAL features","7768ac3b":"### Improve with Grid Search","d1d8b9d8":"## Keras training graphs (for fun)","28b3e3ea":"## Great. FINALLY have our dataset. Now we can explore it a bit","38b2b1f2":"## Check out spread of the data first","0635f070":"### **Seems like the SVM RBF (1) is best for us since it minimizes false negatives i.e. catches most people at risk**\n### Let's test on the test set. Then let's plot some trainnig graphs on the DNN for fun","d9cb9a6c":"## 6. SVM (RBF)","714b0691":"## 2. RANDOM FOREST","6233c38d":"#### Based on this we can:\n* Remove those columsn with many missing values\n* Remove many of the other columns as well\n* Goal is to have a survey someone could fill out online or their mobile\n* Keep:\n    * RIDAGEYR - Age\n    * 'WTINT2YR' -  Weight\n    * RIAGENDR - Gender\n    * INDFMIN2- Total FAMIL  income (need to remove 84 records?)\n    * DMDFMSIZ\tTotal number of people in the Family\t\n    * DMDHHSIZ\tTotal number of people in the Household\t\n    * DMDHHSZA\tNumber of children aged 5 years or younger in the household\t\n    * DMDHHSZB\tNumber of children aged 6-17 years old in the household\t\n    * DMDHHSZE\tNumber of adults aged 60 years or older in the household","5601c122":"## 5. SVM (Linear)","d83db39f":"## 4. Logistic Regression","9dbd8d44":"* This is A LOT of missing data. 822 have > 1000 missing! \n* NOT all might be relevant (could be subquestions)\n* Let's remove the ones with >5% missing - kepp in mind could be eliminating **useful** gender specific data here\n* However, let's hand pick some features.","1058e9d4":"* Amounts spent on foodout and takeaways have some outliers, lets remove","bf25024e":"###  Some more data cleaning\n*  These data are BINAY, CATEGORICAL, NUMERICAL\n*  From the data scource we can see that there are several options for \"don't know\" or \"refused to answer\"\n* We'll clean these out aswell","64b3062a":"* Both values are represented so we can leave these features","f8609836":"# OHE","315bc51a":"### **DATA IS BALANCED - MODELLING AGAIN!!**","f16789d8":"## How about Binary and Categorical columns","8bfb54fe":"* Analyze coefficinets\n*  RFE","311c25be":"### AWESOME!","b04b669a":"# 6. DNN","86bdde15":"* HH size variable do correlate, but not as strongly as age.\n* let's still take them out","72d3785f":"# Balance Training Data","6689fcfe":"* Take a look at those small values","33270b83":"# **FINALLY** Lets build some models!","4a9d5ebc":"## 1.Random Forest","6cdaa75f":"# Test set","55ef3032":"* Let's fix the education","c062d6c2":"# 7. DNN (On original Data - no balance)","90f3451e":"# Target Column\n* Above we created 3 options for Cholesterol - OK, borderline-high, High\n* Since the values for high are relvatively low we are going to re-categorize the problem as OK vs At Risk\n* Makes sense since if we identified someone as having Borderline-high or High we'd advise them to see doctor and get tested","252d29a0":"### As suspected there missing values complement each other","0947e867":"## 3. Random Forest","8fc453ec":"### Based on the info we've selected the below columns to start"}}