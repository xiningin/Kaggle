{"cell_type":{"56760bdf":"code","bcf6a7a8":"code","d1a80e62":"code","f6168c78":"code","9eb1e1fe":"code","4cd16c14":"code","c6020cd4":"code","b51d718a":"code","6c912812":"code","d7f4c225":"code","0c3126e2":"code","f8da557e":"code","4f4cdd62":"code","91aae74c":"code","f1cdf732":"code","0ccb1b12":"code","72ee0339":"markdown","39feb873":"markdown","abbc0d10":"markdown","cfae1d2f":"markdown","16ebe09d":"markdown"},"source":{"56760bdf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np\nfrom torch.autograd import Variable\nfrom sklearn.preprocessing import MinMaxScaler\nimport random\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bcf6a7a8":"data = pd.read_csv('\/kaggle\/input\/binance-coin-data\/Binance Coin - Historic data.csv')","d1a80e62":"data.head()","f6168c78":"##Checking NaN values\ndata.isnull().values.any()","9eb1e1fe":"data_price = data['Price(in dollars)']\ndata_change = data['Change%']","4cd16c14":"plt.plot(data_price, label = 'Price Evolution')\nplt.show()","c6020cd4":"data_change = data['Change%']\nplt.plot(data_change, label = 'Changing Evolution')\nplt.show()","b51d718a":"data_price = np.array([[i] for i in data_price])\ndata_change = np.array([[i] for i in data_change])","6c912812":"#This function separate the data into two grups: X and Y. \ndef sliding_windows(data, seq_length):\n    x = []\n    y = []\n\n    for i in range(len(data)-seq_length-1):\n        _x = data[i:(i+seq_length)]\n        _y = data[i+seq_length]\n        x.append(_x)\n        y.append(_y)\n\n    return np.array(x),np.array(y)","d7f4c225":"sc = MinMaxScaler()\ntraining_data = sc.fit_transform(data_price)  #Data Normalization\n\n\nseq_length = 6 ##Hiperparamter that determinates how many data goes into the LSTM model. \n#For example: if we have a seq_length = 2 the X data will be (X1,X2) where X1 is the price in the day one and X2 the price in the day two. The Y value for this sample will be the third day or X3. \n#That separation of data in X and Y samples take place in the sliding_windows function\nx, y = sliding_windows(training_data, seq_length)\n\ntrain_size = int(len(y) * 0.70) #70% train\ntest_size = len(y) - train_size\n\ndataX = Variable(torch.Tensor(np.array(x)))\ndataY = Variable(torch.Tensor(np.array(y)))\n\nrandom_indexs_train = random.sample(range(0,len(x)),train_size)\nrandom_indexs_test = [i for i in range(len(x)) if i not in random_indexs_train]\n\ntrainX = Variable(torch.Tensor(np.array(x[random_indexs_train])))\ntrainY = Variable(torch.Tensor(np.array(y[random_indexs_train])))\n\ntestX = Variable(torch.Tensor(np.array(x[random_indexs_test])))\ntestY = Variable(torch.Tensor(np.array(y[random_indexs_test])))\n","0c3126e2":"class LSTM(nn.Module):\n\n    def __init__(self, num_classes, input_size, hidden_size, num_layers):\n        super(LSTM, self).__init__()\n        \n        self.num_classes = num_classes\n        self.num_layers = num_layers\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.seq_length = seq_length\n        \n        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n                            num_layers=num_layers, batch_first=True)\n        \n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        h_0 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size))\n        \n        c_0 = Variable(torch.zeros(\n            self.num_layers, x.size(0), self.hidden_size))\n        \n        # Propagate input through LSTM\n        ula, (h_out, _) = self.lstm(x, (h_0, c_0))\n        \n        h_out = h_out.view(-1, self.hidden_size)\n        \n        out = self.fc(h_out)\n        \n        return out","f8da557e":"num_epochs = 1500\nlearning_rate = 0.0075\n\ninput_size = 1\nhidden_size = 8\nnum_layers = 1\nnum_classes = 1\nlstm = LSTM(num_classes, input_size, hidden_size, num_layers)\n#Criterion default:  MSE\n#Optimizer default: Adam\ncriterion = torch.nn.MSELoss()    \noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\nfor epoch in range(num_epochs):\n  outputs = lstm(trainX)\n  optimizer.zero_grad()\n  \n  # obtain the loss function\n  loss = criterion(outputs, trainY)\n  \n  loss.backward()\n  \n  optimizer.step()\n  if epoch % 100 == 0:\n    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))","4f4cdd62":"lstm.eval()\ntrain_predict = lstm(dataX)\n\ndata_predict = train_predict.data.numpy()\ndataY_plot = dataY.data.numpy()\n\ndata_predict = sc.inverse_transform(data_predict)\ndataY_plot = sc.inverse_transform(dataY_plot)\n\n\n\nplt.plot(dataY_plot)\nplt.plot(data_predict)\nplt.suptitle('Price prediction')\nplt.show()","91aae74c":"sc = MinMaxScaler()\ntraining_data = sc.fit_transform(data_change)\n\n\nseq_length = 6 ##Hiperparamter that determinates how many data goes into the LSTM model. \n#For example: if we have a seq_length = 2 the X data will be (X1,X2) where X1 is the price in the day one and X2 the price in the day two. The Y value for this sample will be the third day or X3. \n#That separation of data in X and Y samples take place in the sliding_windows function\nx, y = sliding_windows(training_data, seq_length)\n\ntrain_size = int(len(y) * 0.70) #70% train\ntest_size = len(y) - train_size\n\ndataX = Variable(torch.Tensor(np.array(x)))\ndataY = Variable(torch.Tensor(np.array(y)))\n\nrandom_indexs_train = random.sample(range(0,len(x)),train_size)\nrandom_indexs_test = [i for i in range(len(x)) if i not in random_indexs_train]\n\ntrainX = Variable(torch.Tensor(np.array(x[random_indexs_train])))\ntrainY = Variable(torch.Tensor(np.array(y[random_indexs_train])))\n\ntestX = Variable(torch.Tensor(np.array(x[random_indexs_test])))\ntestY = Variable(torch.Tensor(np.array(y[random_indexs_test])))","f1cdf732":"num_epochs = 2500\nlearning_rate = 0.01\n\ninput_size = 1\nhidden_size = 10\nnum_layers = 1\nnum_classes = 1\nlstm = LSTM(num_classes, input_size, hidden_size, num_layers)\n#Criterion default:  MSE\n#Optimizer default: Adam\ncriterion = torch.nn.MSELoss()    \noptimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\nfor epoch in range(num_epochs):\n  outputs = lstm(trainX)\n  optimizer.zero_grad()\n  \n  # obtain the loss function\n  loss = criterion(outputs, trainY)\n  \n  loss.backward()\n  \n  optimizer.step()\n  if epoch % 100 == 0:\n    print(\"Epoch: %d, loss: %1.5f\" % (epoch, loss.item()))","0ccb1b12":"lstm.eval()\ntrain_predict = lstm(dataX)\n\ndata_predict = train_predict.data.numpy()\ndataY_plot = dataY.data.numpy()\n\ndata_predict = sc.inverse_transform(data_predict)\ndataY_plot = sc.inverse_transform(dataY_plot)\n\n\n\nplt.plot(dataY_plot)\nplt.plot(data_predict)\nplt.suptitle('Change prediction')\nplt.show()","72ee0339":"Training","39feb873":"Training the model to predict the change","abbc0d10":"The objective for this notebook is to provide a LSTM model to predict the Binance price and the change of Binance price. \nThe model needs the X and Y data.\nX: [X1,X2,X3] where X1 is the Binance Price in day one, X2 is the Binance price in two day ,...\nY: [X4] is the Binance Price in the fourth day. \nThe idea is that the LSTM model recibe a sample of X and predict the price for de next day. For example, following the previous examples recibe the data of the \nfirst three days and predict the price for the fourth day.\n","cfae1d2f":"Evalutation","16ebe09d":"Model\n"}}