{"cell_type":{"80c664bd":"code","2c3fcbb4":"code","49c2d126":"code","34d6de00":"code","a90f2cff":"code","352bf2ce":"code","769cbeb7":"code","a1aaed69":"code","974ad797":"code","4555fca9":"code","63ac2a04":"code","57670ffb":"code","9aada2fe":"code","029b10c0":"code","8fbb7f0c":"code","7e4be1cd":"code","4a69d265":"code","7750abf0":"code","a1e1cde3":"code","e9c9cdd2":"code","3e97496c":"code","96bf0873":"code","d219c924":"code","e70f9165":"code","d29ea198":"code","b6a0d511":"code","4585289c":"code","cede8961":"code","75534471":"code","28908405":"code","e5183f5a":"code","76912726":"code","f3a789e6":"code","c76f421a":"code","c5faae9a":"code","a9d8d6f0":"code","a8bdc94f":"code","1d3fb48d":"code","dd5e8724":"code","75e1bce0":"code","406df664":"code","4190fdbc":"code","16216e8a":"code","adbe04f0":"code","034f8019":"code","e8e18e95":"code","47de6770":"code","665eab1f":"code","31542635":"code","afb95c36":"code","f68fb5ad":"code","93ad3058":"code","a726f828":"code","a9d769b2":"code","bd1d9c01":"code","6c1d2700":"code","4a5caa48":"code","685485ae":"code","c2d3010a":"code","eeda847d":"code","407b1453":"code","763a57f1":"code","533bfad5":"code","abf3bfc4":"code","159d89ea":"code","00a71b69":"markdown","99de1e70":"markdown","22266d4b":"markdown","32c77658":"markdown","110d879c":"markdown"},"source":{"80c664bd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2c3fcbb4":"# importing libraries\n\nimport warnings\nimport scipy\nimport datetime\nimport argparse\nimport math\nimport timeit\n\nfrom scipy import misc\nfrom skimage import io\nfrom PIL import Image\n\nwarnings.filterwarnings(\"ignore\")","49c2d126":"# importing keras libraries\n\nimport keras\nfrom keras import utils\nfrom keras import models\nfrom keras import backend as K\nfrom keras import optimizers\nfrom keras.layers.core import (Dense, Dropout, Activation, Flatten, Reshape)\nfrom keras.layers.convolutional import (Conv2D, MaxPooling2D)\n\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.convolutional import UpSampling2D","34d6de00":"# loading the mnist data\n\ndf = pd.read_csv(\"..\/input\/train.csv\")","a90f2cff":"df.head()","352bf2ce":"df.info()","769cbeb7":"df.describe()","a1aaed69":"# loading the testing data\n\ntest = pd.read_csv(\"..\/input\/test.csv\")","974ad797":"test.head()","4555fca9":"test.info()","63ac2a04":"test.describe()","57670ffb":"# misc properties\n\nBATCH_SIZE = 64 # number of images per batch\nNP_EPOCHS = 10 # number of times the data has been projected towards the model\nNP_CLASSES = 10 # number of output classes has been passed to the network\nVERBOSE = 1 # verbose logs\nVALIDATION_SPLIT = 0.20 # splitting the data into train and validation\nOPTIMIZER = optimizers.RMSprop() # optimizer has been passed to the model","9aada2fe":"# dividing the data into train and target \n\ntrain = df.drop(['label'], axis=1)\ntarget = df['label']","029b10c0":"train.head()","8fbb7f0c":"target.head()","7e4be1cd":"# import seaborn and matplotlib\n\n%matplotlib inline\nimport seaborn as sns\nimport matplotlib.pyplot as plt","4a69d265":"sns.distplot(target)","7750abf0":"sns.countplot(target)","a1e1cde3":"# normalizing the train data\ntrain \/= 255\n\n# normalizing the test data\ntest \/= 255","e9c9cdd2":"# reshaping the train data\n\ntrain = train.values.reshape(-1, 28, 28, 1)","3e97496c":"# reshaping the test data\n\ntest = test.values.reshape(-1, 28, 28, 1)","96bf0873":"# Label Encoding\nfrom keras.utils import np_utils\n\ntarget = np_utils.to_categorical(target, NP_CLASSES)","d219c924":"target[:5]","e70f9165":"# some examples\n\nplt.imshow(train[42][:, :, 0])","d29ea198":"from keras.preprocessing import image\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Flatten, BatchNormalization, Activation, Dropout, LeakyReLU\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras import optimizers\n\nNB_OUTPUT_FUNC = \"softmax\"\nNB_DESCRIMINATOR_FUNC=\"sigmoid\"\nDROPOUT_FIRST = 0.25\nDROPOUT_SECOND = 0.20","b6a0d511":"model = Sequential()\nmodel.add(Conv2D(32, (5, 5), padding='same', input_shape=(28, 28, 1)))\nmodel.add(LeakyReLU(alpha=0.02))\nmodel.add(Conv2D(32, (5, 5)))\nmodel.add(LeakyReLU(alpha=0.02))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(DROPOUT_FIRST))\n\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(LeakyReLU(alpha=0.02))\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(LeakyReLU(alpha=0.02))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(DROPOUT_FIRST))\n\nmodel.add(Flatten())\nmodel.add(Dense(128))\nmodel.add(LeakyReLU(alpha=0.02))\nmodel.add(Dropout(DROPOUT_SECOND))\n\nmodel.add(Dense(128))\nmodel.add(LeakyReLU(alpha=0.02))\nmodel.add(Dropout(DROPOUT_SECOND))\n\nmodel.add(Dense(NP_CLASSES))\nmodel.add(Activation(NB_OUTPUT_FUNC))","4585289c":"model.summary()","cede8961":"current_dt_time = datetime.datetime.now()\nmodel_name = 'model_init' + '_' + str(current_dt_time).replace(' ', '').replace(':', '_') + '\/'\n\nif not os.path.exists(model_name):\n    os.mkdir(model_name)\n    \nfile_path = model_name + \"model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5\"\ncheckpoint = ModelCheckpoint(filepath=file_path, monitor='val_categorical_accuracy', verbose=1, save_best_only=True,\n                             save_weights_only=False, mode='auto', period=1)\nLR = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=2, min_lr=0.000001, verbose=1, cooldown=1)\ncallbacks = [checkpoint, LR]","75534471":"# creating optimizer\n\n# optimizer = optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)\noptimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06, decay=0.0)","28908405":"# compiling the model\n\nmodel.compile(optimizer=optimizer, \n              loss='categorical_crossentropy', \n              metrics=['categorical_accuracy'])","e5183f5a":"# splitting the data for training and validation\nimport sklearn\nfrom sklearn import model_selection\n\nX_train, X_val, y_train, y_val = model_selection.train_test_split(train, target, test_size=0.20, random_state=123456789)","76912726":"print(\"X training data shape: {}\".format(X_train.shape))\nprint(\"Y training data shape: {}\".format(y_train.shape))","f3a789e6":"print(\"X validation data shape: {}\".format(X_val.shape))\nprint(\"Y validation data shape: {}\".format(y_val.shape))","c76f421a":"# creating a generator to pull the data as batches in a lazy format - Data augumentation\n\ngenerator = image.ImageDataGenerator(\n    featurewise_center=True,\n    featurewise_std_normalization=True,\n    rotation_range=10,\n    zoom_range=0.1,\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    vertical_flip=False,\n    horizontal_flip=False)","c5faae9a":"num_train_sequences = len(X_train)\nnum_val_sequences = len(X_val)\n\nprint(\"# training sequences: {}\".format(num_train_sequences))\nprint(\"# validation sequences: {}\".format(num_val_sequences))","a9d8d6f0":"# calculating number of training and validation steps per epoch\n# for training\nif (num_train_sequences % BATCH_SIZE) == 0:\n    steps_per_epoch = int(num_train_sequences \/ BATCH_SIZE)\nelse:\n    steps_per_epoch = int(num_train_sequences \/ BATCH_SIZE) + 1\n    \n# for validation    \nif (num_val_sequences % BATCH_SIZE) == 0:\n    validation_steps = int(num_val_sequences \/ BATCH_SIZE)\nelse:\n    validation_steps = int(num_val_sequences \/ BATCH_SIZE) + 1    \n    \nprint(\"# number of steps required for training: {}\".format(steps_per_epoch))\nprint(\"# number of steps required for validation: {}\".format(validation_steps))","a8bdc94f":"# fitting the model\n\nhistory = model.fit_generator(generator.flow(X_train, y_train, batch_size=BATCH_SIZE), \n                             validation_data=generator.flow(X_val, y_val, batch_size=BATCH_SIZE),\n                             epochs=NP_EPOCHS,\n                             verbose=VERBOSE,\n                             steps_per_epoch=steps_per_epoch,\n                             validation_steps=validation_steps,\n                             class_weight=None,\n                             initial_epoch=0,\n                             callbacks=callbacks)","1d3fb48d":"# best accuracy found\n# best model: saving model to model_init_2019-07-0108_08_42.188814\/model-00009-0.21558-0.93310-0.12834-0.96155.h5\n\nvalues = {}\nmodels = os.listdir(model_name)\n\nfor model in models:\n    converted = model.replace(\".h5\", \"\")\n    accuracy = float(converted.split(\"-\")[-1])\n    values.update({accuracy: model})\n    \nkey = max(values, key = values.get)\nbest = values.get(key)\n\nprint(\"Best model found: {}\".format(best))","dd5e8724":"# all data in history\n\nhistory.history.keys()","75e1bce0":"plt.plot(history.history['categorical_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","406df664":"# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","4190fdbc":"# loading the best model\nfrom keras.models import load_model\n\nbest_model_path = model_name + best\nprint(\"Full path found: {}\".format(best_model_path))\n\nbest_model = load_model(best_model_path)","16216e8a":"best_model.summary()","adbe04f0":"results = best_model.predict(test)","034f8019":"results[:5]","e8e18e95":"convert = np.argmax(results, axis=1)","47de6770":"results = pd.Series(convert, name='Label')","665eab1f":"results.head()","31542635":"submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"), results],axis = 1)","afb95c36":"submission.head()","f68fb5ad":"submission.to_csv(\"submissions-10epochs.csv\", index=False)","93ad3058":"from keras import backend as K\n\nK.set_image_data_format(\"channels_first\")","a726f828":"## creating the DCGAN - generator model\n\ndef generator():\n    \n    gen = Sequential()\n    gen.add(Dense(input_dim=100, output_dim=1024))\n    gen.add(LeakyReLU(alpha=0.02))\n    gen.add(Dense(128 * 7 * 7))\n    gen.add(BatchNormalization())\n    gen.add(LeakyReLU(alpha=0.02))\n    gen.add(Reshape((128, 7, 7), input_shape=(128 * 7 * 7,)))\n    gen.add(UpSampling2D(size=(2,2)))\n    gen.add(Conv2D(64, 5, 5, border_mode=\"same\"))\n    gen.add(LeakyReLU(alpha=0.02))\n    gen.add(UpSampling2D(size=(2,2)))\n    gen.add(Conv2D(1, 5, 5, border_mode=\"same\"))\n    gen.add(LeakyReLU(alpha=0.02))\n    return gen","a9d769b2":"generator().summary()","bd1d9c01":"## creating the DCGAN - descriminator model\n\ndef descriminator():\n    \n    des = Sequential()    \n    des.add(Conv2D(64, (5, 5), padding='same', input_shape=(1, 28, 28)))\n    des.add(LeakyReLU(alpha=0.02))\n    des.add(MaxPooling2D(pool_size=(2, 2)))\n    des.add(Conv2D(128, (5, 5)))\n    des.add(LeakyReLU(alpha=0.02))\n    des.add(MaxPooling2D(pool_size=(2, 2)))\n    des.add(Flatten())\n    des.add(Dense(1024))\n    des.add(LeakyReLU(alpha=0.02))\n    des.add(Dense(1))\n    des.add(Activation(NB_DESCRIMINATOR_FUNC))\n        \n    return des","6c1d2700":"descriminator().summary()","4a5caa48":"# changing the number of epochs\n\nNP_EPOCHS = 100\nBATCH_SIZE=128","685485ae":"# combining the images\n\ndef combine(generated_images):\n    number = generated_images.shape[0]\n    width = int(math.sqrt(number))\n    height = int(math.ceil(float(number) \/ width))\n    shape = generated_images.shape[2:]\n    image = np.zeros((height * shape[0], width * shape[1]), dtype=generated_images.dtype)\n    \n    for index, img in enumerate(generated_images):\n        i = int(index \/ width)\n        j = index % width\n        \n        image[i * shape[0]:(i+1) * shape[0], j * shape[1]:(j+1) * shape[1]] = img[0, :, :]\n    \n    return image","c2d3010a":"# generator containing descriminator\n\ndef generator_containing_desciminator(generator, descriminator):\n    model = Sequential()\n    model.add(generator)\n    descriminator.trainable = False\n    model.add(descriminator)\n    return model","eeda847d":"# training the network\n\ndef train_network():\n    \n    # Reshaping the data\n    X_train, y_train, X_test, y_test = model_selection.train_test_split(train, target, test_size=0.20, random_state=123456789)\n    print(\"X training data shape: {}\".format(X_train.shape))\n    print(\"Y training data shape: {}\".format(y_train.shape))\n    print(\"X validation data shape: {}\".format(X_test.shape))\n    print(\"Y validation data shape: {}\".format(y_test.shape))\n    \n    X_train = X_train.reshape((X_train.shape[0], 1) + X_train.shape[1 : 3])\n    print(\"After reshaping X training data shape: {}\".format(X_train.shape))\n    \n    # descriminator model\n    descriminator_model = descriminator()   \n    # generator model\n    generator_model = generator()\n    \n    # descriminator on generator\n    descriminator_on_generator = generator_containing_desciminator(generator_model, descriminator_model)\n    \n    # descriminator optimizer\n    descriminator_optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06, decay=0.0)\n    # generator optimizer\n    generator_optimizer =  optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06, decay=0.0)\n    \n    # compiling the model\n    generator_model.compile(loss=\"binary_crossentropy\", optimizer=generator_optimizer)\n    descriminator_on_generator.compile(loss=\"binary_crossentropy\", optimizer=generator_optimizer)\n    \n    descriminator.trainable = True    \n    descriminator_model.compile(loss=\"binary_crossentropy\", optimizer=descriminator_optimizer)\n        \n    noise = np.zeros((BATCH_SIZE, NP_EPOCHS))\n    \n    for epoch in range(NP_EPOCHS):\n        print(\"Training started for Epoch: {}\".format(epoch))\n        print(\"Number of batches to be trained for each epoch: {}\".format(X_train.shape[0] \/ BATCH_SIZE))\n        \n        for index in range(int(X_train.shape[0] \/ BATCH_SIZE)):\n            for i in range(BATCH_SIZE):\n                noise[i, :] = np.random.uniform(-1, 1, 100)\n                \n            image_batch = X_train[index * BATCH_SIZE: (index + 1) * BATCH_SIZE]\n            generated_images = generator_model.predict(noise, verbose=0)\n            \n            if index % 20 == 0:\n                image = combine(generated_images)\n                image = image * 127.5 + 127.5\n                Image.fromarray(image.astype(np.uint8)).save(str(epoch) + \"_\" + str(index) + \".png\")\n                \n            X = np.concatenate((image_batch, generated_images))\n            y = [1] * BATCH_SIZE + [0] * BATCH_SIZE\n            descriminator_loss = descriminator_model.train_on_batch(X, y)\n            # print(\"Batch: {} found loss: {}\".format(index, descriminator_loss))\n            \n            for i in range(BATCH_SIZE):\n                noise[i, :] = np.random.uniform(-1, 1, 100)\n            descriminator_model.trainable = False\n            \n            generator_loss = descriminator_on_generator.train_on_batch(noise, [1] * BATCH_SIZE)\n            # print(\"Batch: {} found loss: {}\".format(index, generator_loss))\n            \n            if index % 10 == 9:\n                generator_model.save_weights(\"generator\", True)\n                descriminator_model.save_weights(\"descriminator\", True)    ","407b1453":"# calling the training method\n\ntrain_network()","763a57f1":"# generating the images from the network\n\ndef generate(BATCH_SIZE, nice: False):\n    \n    # getting the generator model\n    generator_model = generator()\n    \n    # creating the optimizer for the model\n    generator_optimizer =  optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06, decay=0.0)\n    \n    # compiling the model\n    generator_model.compile(loss=\"binary_crossentropy\", optimizer=generator_optimizer)\n    \n    if nice:\n        # descriminator model\n        descriminator_model = descriminator()\n        \n        # creating the optimizer for the model\n        descriminator_optimizer = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-06, decay=0.0)\n        \n        # compiling the model\n        descriminator_model.compile(loss=\"binary_crossentropy\", optimizer=descriminator_optimizer)\n        \n        # loading the weights from the training\n        descriminator_model.load_weights(\"descriminator\")\n        \n        noise = np.zeros((BATCH_SIZE * 20, 100))\n        \n        for i in range(BATCH_SIZE * 20):\n            noise[i, :] = np.random.uniform(-1, 1, 100)\n            \n        generated_images = generator_model.predict(noise, verbose=1)\n        descriminator_predicted = descriminator_model.predict(generated_images, verbose=1)\n        index = np.arange(0, BATCH_SIZE * 20)\n        index.resize((BATCH_SIZE * 20, 1))\n        predicted_with_index = list(np.append(descriminator_predicted, index, axis=1))\n        nice_images = np.zeros((BATCH_SIZE, 1) + (generated_images.shape[2:]), dtype=np.float32)\n        \n        for i in range(int(BATCH_SIZE)):\n            idx = int(predicted_with_index[i][1])\n            nice_images[i, 0, :, :] = generated_images[idx, 0, :, :]\n        image = combine(nice_images)\n    else:\n        \n        noise = np.zeros((BATCH_SIZE, 100))\n        for i in range(BATCH_SIZE):\n            noise[i, :] = np.random.uniform(-1, 1, 100)\n        generated_images = generator_model.predict(noise, verbose=1)\n        image = combine(generated_images)\n        \n    \n    image = image * 127.5 + 127.5\n    Image.fromarray(image.astype(np.uint8)).save(\"generated_image.png\")","533bfad5":"generate(BATCH_SIZE, True)","abf3bfc4":"!ls -lrt","159d89ea":"# displaying the image\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimg = mpimg.imread('generated_image.png')\nimgplot = plt.imshow(img)\nplt.show()","00a71b69":"### Creating the Generative adversarial network","99de1e70":"### Importing the data","22266d4b":"#### Normalizing the train and test data","32c77658":"### Model Building using Keras","110d879c":"### Construction of model"}}