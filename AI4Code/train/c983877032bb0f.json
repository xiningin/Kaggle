{"cell_type":{"7078ed05":"code","de62ae59":"code","afd77b27":"code","32b5ed2b":"code","5cd121c1":"code","c1b5ef70":"code","02ab215d":"code","dd047f55":"code","aa83fa51":"code","f490ec10":"code","c00b7853":"code","6bd708b0":"code","891f9398":"code","cf711c0e":"code","d2dbe3e5":"code","62467598":"code","f20e0682":"code","fcaa3e34":"code","288527c7":"code","8d587510":"code","00d7776c":"code","8ed6c94f":"code","eebd3a4c":"code","5dac44da":"code","6895dbec":"code","0d7a4f23":"code","e19ae105":"code","d590ed95":"code","a36aef9c":"code","2de2d6b7":"code","83ac9ea1":"code","6e0554bd":"code","b7854047":"code","53e8294a":"code","6bf8c6fa":"code","75b3c47a":"code","8a83c146":"code","051dfa45":"code","270f0566":"code","e02fb83a":"code","333af431":"code","254115f4":"code","0f46dfde":"code","13966cc7":"code","5319774e":"code","7f4a71ec":"code","850e2c21":"code","a7ebf384":"code","09f8d939":"markdown","b26db0d5":"markdown","3a50751d":"markdown","c8b8f6ca":"markdown","4614f774":"markdown","23681236":"markdown","1f2e0626":"markdown","260d6baf":"markdown","5cd7ef26":"markdown","8f253371":"markdown","87e15255":"markdown","93d96c30":"markdown","da339f92":"markdown","a6084765":"markdown","41f6388b":"markdown","c8a84a9f":"markdown","acd0a515":"markdown","27562f86":"markdown","bdf64f86":"markdown"},"source":{"7078ed05":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","de62ae59":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\n%config InlineBackend.figure_format = 'retina'","afd77b27":"df = pd.read_csv('..\/input\/productivity-prediction-of-garment-employees\/garments_worker_productivity.csv')\ndf['date'] = pd.to_datetime(df['date'])\n\ndf","32b5ed2b":"df.info()","5cd121c1":"# Count NaN\ndf.isnull().sum()","c1b5ef70":"# Count zeros. NaN is NOT considered zero\ndf.isin([0]).astype(int).sum(axis=0)\n\n# # Count zeros. NaN is considered zero\n# df.fillna(0).isin([0]).astype(int).sum(axis=0)\n\n# # Count non zeros. NaN is NOT considered zero\n# df.astype(bool).sum(axis=0)","02ab215d":"sns.lmplot(x='no_of_style_change', y='actual_productivity', data=df)\n# Quarter 2 and 5 has no idle time, no interruption\n# January has no idle time","dd047f55":"df.department.unique()","aa83fa51":"# Remove whitespace in department column\ndf['department'] = [string.strip() for string in df.department]","f490ec10":"# Change 'sweing' to 'sewing'\ndf.loc[df.department=='sweing', 'department'] = 'sewing'","c00b7853":"df.department.value_counts().plot.pie(autopct='%.2f %%')","6bd708b0":"df['quarter'] = [int(q[7:]) for q in df.quarter]\n\ndf['quarter']","891f9398":"dates = df.date\nmonths = [date.month for date in dates]\n\ndf['month'] = months\n\ndf","cf711c0e":"df.smv.plot.hist()","d2dbe3e5":"df[df.smv>30]","62467598":"df['over_time'] = df['over_time'] \/ 60\n\ndf['over_time']","f20e0682":"df['wip'] = df['wip'].replace(np.nan, 0)\n\ndf['wip']","fcaa3e34":"df['margin'] = df['actual_productivity'] - df['targeted_productivity']\n\ndf['margin']","288527c7":"c = list(np.full(12, 'grey'))\nc[5], c[11] = 'orange', 'orange'\n\nf, ax = plt.subplots(2, 2, figsize=(9,6))\n\nsns.barplot(data=df, x='team', y='margin', palette=c, ax=ax[0][0])\nax[0][0].set_title('Productivity', size=15)\nsns.barplot(data=df, x='team', y='no_of_workers', palette=c, ax=ax[0][1])\nax[0][1].set_title('Manpower', size=15)\nsns.barplot(data=df, x='team', y='over_time', palette=c, ax=ax[1][0])\nax[1][0].set_title('Overtime [hours]', size=15)\nsns.barplot(data=df, x='team', y='incentive', palette=c, ax=ax[1][1])\nax[1][1].set_title('Incentive [BDT]', size=15)\n\nplt.tight_layout()","8d587510":"f, ax = plt.subplots(figsize=(10,6))\nsns.barplot(data=df, x='team', y='margin', hue='department', ax=ax)","00d7776c":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4))\n\nc = ['green', 'green', 'red', 'green', 'green', 'green']\nsns.barplot(data=df, x='day', y='margin', ax=ax1, palette=c)\nsns.barplot(data=df, x='day', y='margin', hue='department', ax=ax2)\nplt.tight_layout()","8ed6c94f":"f, ax = plt.subplots(2, 2, figsize=(8,6))\n\nsns.barplot(data=df, x='department', y='smv', ax=ax[0][0])\nsns.barplot(data=df, x='department', y='incentive', ax=ax[0][1], estimator=np.sum)\nsns.barplot(data=df, x='department', y='over_time', ax=ax[1][0])\nsns.barplot(data=df, x='department', y='wip', ax=ax[1][1])\n\nplt.tight_layout()","eebd3a4c":"f, ax = plt.subplots(1, 1, figsize=(12,6))\nsns.barplot(data=df, x='team', y='smv', hue='department',ax=ax)","5dac44da":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,4))\n\nc = ['darkgrey', 'darkgrey', 'red']\nsns.barplot(data=df, x='month', y='no_of_workers', estimator=np.sum, palette=c, ax=ax1)\nsns.barplot(data=df, x='month', y='incentive', estimator=np.sum, palette=c, ax=ax2)\nplt.tight_layout()","6895dbec":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,4))\n\nsns.barplot(data=df, x='day', y='incentive', ax=ax1)\nsns.barplot(data=df, x='day', y='wip', ax=ax2)","0d7a4f23":"sns.catplot(data=df, x='quarter', y='margin', kind='bar', col='month')","e19ae105":"# sns.catplot(data=df, x='quarter', y='margin', kind='violin', hue='department', strip=False, col='month')","d590ed95":"def corr_heatmap(df):    \n    plt.figure(figsize=(8,8))\n\n    mask = np.triu(np.ones_like(df.corr(), dtype=np.bool))\n    sns.heatmap(df.corr(), mask=mask, vmin=-1, vmax=1, cmap='icefire')","a36aef9c":"corr_heatmap(df)","2de2d6b7":"df.corr()","83ac9ea1":"# Change incentive with values of 0 with very small value 0.0001\ndf.loc[df.incentive==0, 'incentive'] = 0.0001","6e0554bd":"df.corr()","b7854047":"# Feature synthesis\nfeatures = ['smv', 'wip', 'incentive', 'no_of_workers', 'over_time', 'no_of_style_change']\ndf_t = df.copy()\n\nfor f in features:\n    # Log\n    df_t[f+'_log'] = df_t[f].apply(lambda x: np.log10(x) if x>0 else np.log10(x+0.0001))\n    # Sqrt\n    df_t[f+'_sqrt'] = df_t[f].apply(lambda x: np.sqrt(x))\n    # Cube root\n    df_t[f+'_cbrt'] = df_t[f].apply(lambda x: np.cbrt(x))\n    # Square\n    df_t[f+'^2'] = df_t[f].apply(lambda x: x**2)\n    # Cube\n    df_t[f+'^3'] = df_t[f].apply(lambda x: x**3)\n    for f2 in features:\n        if f==f2:\n            pass\n        else:\n            x, y = df_t[f], df_t[f2]\n            # Multiply\n            df_t[f+'*'+f2] = x*y\n            # Divide\n            y = y.apply(lambda c: c if c>0 else c+0.0001)\n            df_t[f+'\/'+f2] = x\/y\n            # Take log of multiplication\n            xx = df_t[f+'*'+f2]\n            df_t[f+'*'+f2+'_log'] = xx.apply(lambda x: np.log10(x) if x>0 else np.log10(x+0.0001))\n\nwith pd.option_context('display.max_rows', None): \n    print(df_t.corr().actual_productivity.apply(lambda x: np.abs(x)).sort_values(ascending=False).head(50))\n\n# incentive*over_time (0.249), no_of_workers\/smv (0.157)\n# incentive*over_time (0.249), no_of_style_change\/no_of_workers (0.21), smv^3 (0.156), smv*no_of_style_change_log (0.233)","53e8294a":"\"\"\"\nBEST FEATURE COMBINATIONS:\n\nwip\/incentive 0.34\nno_of_style_change*over_time 0.20\nno_of_workers\/smv 0.157\n\nincentive*over_time (0.249)\nno_of_workers\/smv (0.157)\nwip*no_of_style_change 0.197892\n\"\"\"","6bf8c6fa":"# Feature engineer smv, manpower, and incentive \ndf['smv_manpower'] = np.log(df['smv'] \/ df['no_of_workers'])\ndf['log_incentive'] = np.log(df['incentive'])\n# df['wip_over_time'] = (df['wip'])","75b3c47a":"# New correlation\ndf.corr()","8a83c146":"df2 = df.drop(columns=['smv', 'no_of_workers', 'incentive', 'idle_time', 'idle_men',\n                       'targeted_productivity', 'margin', 'team', 'date'])\n\ndf2","051dfa45":"from sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor as GBRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.pipeline import make_pipeline\n\n# Feature and target\nX = df2.drop(columns=['actual_productivity'])\ny = df2.actual_productivity\n\n# Encoding\nle = LabelEncoder()\ncateg = ['department', 'day']\nX[categ] = X[categ].apply(le.fit_transform)\n\n# Train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Pipeline\npipe = make_pipeline(StandardScaler(), XGBRegressor())\n# pipe = make_pipeline(StandardScaler(), GBRegressor(n_estimators=200, min_samples_split=5, min_samples_leaf=5,\n#                                                    max_features=7, max_leaf_nodes=10))\n\npipe = make_pipeline(StandardScaler(), XGBRegressor(max_depth=5, eta=0.06, n_estimators=90, subsample=0.9, reg_lambda=1.2, \n                                                    booster='gbtree', colsample_bynode=0.8, colsample_bytree=0.8))\n\ncv_r2 = cross_val_score(pipe, X_train, y_train, cv=10)\nprint(np.mean(cv_r2))\nprint(cv_r2)","270f0566":"pipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)","e02fb83a":"from sklearn.metrics import mean_absolute_error as mae\n\nmae(y_test, y_pred)","333af431":"# Feature and target\nX = df_t[['incentive*over_time', 'no_of_workers\/smv', 'wip*no_of_style_change', 'department', 'day', 'month']]\ny = df_t.actual_productivity\n\n# Encoding\nle = LabelEncoder()\ncateg = ['department', 'day']\nX[categ] = X[categ].apply(le.fit_transform)\n\n# Train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Pipeline\npipe = make_pipeline(StandardScaler(), XGBRegressor())\n# pipe = make_pipeline(StandardScaler(), GBRegressor(n_estimators=200, min_samples_split=5, min_samples_leaf=5,\n#                                                    max_features=7, max_leaf_nodes=10))\n\npipe = make_pipeline(StandardScaler(), XGBRegressor(max_depth=5, eta=0.06, n_estimators=90, subsample=0.9, reg_lambda=1.2, \n                                                    booster='gbtree', colsample_bynode=0.8, colsample_bytree=0.8))\n\ncv_r2 = cross_val_score(pipe, X_train, y_train, cv=10)\nprint(np.mean(cv_r2))\nprint(cv_r2)","254115f4":"pipe.steps[1][1]","0f46dfde":"from sklearn.model_selection import GridSearchCV\n\n# Define parameter search grid\nparam_grid = {'lgbmregressor__n_estimators': [180, 200, 250], \n              'lgbmregressor__num_leaves': [10, 12, 14],\n              'lgbmregressor__reg_alpha' : [1, 1.5, 2],\n              'lgbmregressor__reg_lambda': [1, 1.5, 2],\n              'lgbmregressor__boosting_type': ['gbdt'] # Gradient Boosting Decision Tree\n             }\n\n# Grid search CV\ngrid = GridSearchCV(pipe, param_grid, verbose=1, cv=5, n_jobs=-1)\n\n# Fit grid on train set\ngrid.fit(X_train, y_train)\n\n# Best model from tuning\nprint(grid.best_params_)\nprint(grid.best_score_)","13966cc7":"# Best model from tuning\nprint(grid.best_params_)\nprint(grid.best_score_)","5319774e":"\"\"\"\n1. Dropping 'idle_men', 'idle_time' and 'no_of_style_change' columns because high frequency in 0 values\n2. There are 506 null values in wip column. It appears that these null is from finishing dept. The finishing department needs to get a work from the sewing department. This result could mean that the finishing department has no work in progress while waiting for work from the sewing department. So we can replace the null values with zero\n\"\"\"","7f4a71ec":"# sns.scatterplot(data=df, x=\"no_of_workers\", y=\"smv\", hue=\"department\")\n# # For the finising department smv doees not change with no_of_workers","850e2c21":"# df.day.value_counts() ","a7ebf384":"# df['wip'].interpolate(method='time',inplace=True)","09f8d939":"Now let's break down the productivity of every team with respect to different departments where they work. It was thought before that sewing will have less productivity than finishing because sewing clothes is harder and takes more time than finishing it. This happens in team 1, 3, 4, 5, 7, 8, and 12. However, for the rest of teams, the productivity of sewing is larger than that of finishing. My interpretation is that each team has different capabilities; some better on sewing, others better on finishing.","b26db0d5":"It's interesting to see the SMV varies across the teams. This could mean different teams assigned to work on different kind of clothes. Team 6, 11, and 12 work on easier clothes than the rest others.\n\n**NOTE.** Team 6, 11, and 12 work with SMV of 15-17 minutes. This SMV is typically for **making hoodies**. Team 7 has the highest SMV of 27. **Men's long sleeve shirt** has SMV of 21. Check this [website](https:\/\/ordnur.com\/apparel\/standard-minute-value-smv-garments-calculation-importance\/) to learn more about SMV for different clothes.","3a50751d":"The incentive paid to workers on Monday is the largest, while on average the incentive is similar on the other days. This also represents the WIP which Monday has the most unfinished bundles.","c8b8f6ca":"# Feature engineering","4614f774":"# EDA","23681236":"Instead of evaluating the actual productivity, I'll take the difference of actual and targeted productivity then take the percentage to create new feature `margin`. This will determine how many percent the actual productivity exceeds the targeted productivity. If actual productivity does not meet the target, `margin` will be negative.\n\n**Positive margin: actual productivity > targeted productivity**\n\n**Negative margin: actual productivity > targeted productivity**","1f2e0626":"Let's compare the SMV (standard minute value), incentive, over-time, and WIP (work-in-progress) for different departments. We could see that all of these in the sewing department are larger than finishing department. This simply means that sewing clothes is harder than finishing it. Sewing takes more time than finishing (speaking in terms of SMV). To sew clothes people work over-time and have more unfinished bundles, therefore receives more incentive.\n\n**NOTE.** We can use `estimator=np.sum` to compound all values in the data. The default estimator is mean. For example in incentive. This is because incentive has outliers (look the black bar), therefore mean to measure central tendency can be misleading. Therefore, we can use sum or median.","260d6baf":"Since smv and no_of_workers are correlated, I will transform both features as follows; $\\log (\\frac{smv}{workers})$. For incentive, I will take the log; $\\log(incentive)$","5cd7ef26":"`idle_time`, `idle_men`, and `no_of_style_change` have zeros >80% of data.","8f253371":"Finally, we assess the productivity over the quarters.  ","87e15255":"Strongly correlated features are:\n\n* `smv` and `over_time`: The more time took to work on a bundle, the more overtime an employee will have to work\n* `smv` and `no_of_workers`: The more time took to work on a bundle, the more workers needed to finish the work\n* `over_time` and `no_of_workers`: More workers on the line, more chance to work overtime\n\nsmv to actual productivity correlation is -0.12, no_of_workers to actual productivity correlation -0.05, and incentive to actual productivity correlation 0.07. \n\nI will increase the correlation with some feature engineering. \n","93d96c30":"We have improved correlation of incentive to actual productivity from 0.07 to 0.15 with $\\log(incentive)$, and improved each correlation of smv -0.12 and no_of_workers -0.05 to correlation of -0.16 with $\\log(\\frac{smv}{workers})$. \n\nNow let's drop some of the features.","da339f92":"Incentive is one of features I would like to feature-engineer. Incentive has some values of 0. When taking the log, values of zeros will result -inf. So, I will change 0 to a very small number such as 0.0001.","a6084765":"Comparing number of workers and incentive for every months, we could see that on March, the number of workers is the least of all months. However, the incentive is the largest on March. The factory pays more incentive when the workers are very few.","41f6388b":"There are 12 teams in the factory, each team works on both sewing and finishing works. Looking at the productivity, team 1-5 and 12 have positive margin, meaning their productivity exceeds the targeted productivity. Team 1 is the most productive. However, the productivity of team 6-11 do not meet the target (they have negative margin). Team 6 is the least productive.\n\nNow let's focus on team 6 and 12. Although both teams have the same number of workers and the same over-time, team 12 has better productivity than team 6. It turns out that team 12 receives more incentive than team 6.","c8a84a9f":"`wip` has 506 NaNs.","acd0a515":"Comparing the productivity over days, we could see that Saturday and Tuesday are the most productive. **Where is Friday??** Friday is not recorded in this data, which simply means that workers do not work on Friday. Saturday has a significantly larger productivity because Saturday comes after Friday, and usually the productivity after a day-off is better.\n\nWe see that Sunday has negative productivity. It was thought before that workers do not work on Sunday. However, it is recorded in the data, meaning that people work on this day. If we break down the productivity w.r.t. departments, only on Sunday, the sewing productivity is very negative while finishing is positive. This could mean people typically work on finishing and they must finish all the clothes before Monday. ","27562f86":"# Using engineered features","bdf64f86":"# Data analysis"}}