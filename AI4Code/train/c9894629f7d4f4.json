{"cell_type":{"97e59771":"code","914148c8":"code","b0cc6a9b":"code","0451c6b4":"code","38f1d44f":"code","5b4e95ce":"code","0465cecb":"code","35bdfaba":"code","cc3b7a53":"code","b0b96abe":"code","2fec268e":"code","dde9e54c":"code","44942b3c":"code","bfe6dd33":"markdown","980a4740":"markdown","0cfa531f":"markdown","f467b777":"markdown","ebacea0e":"markdown","fea6ee1a":"markdown","c8f785e4":"markdown","95378772":"markdown","81672278":"markdown","97c7c015":"markdown","fcdbbefd":"markdown","b2d1dcac":"markdown","29437e33":"markdown"},"source":{"97e59771":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom skimage import io,segmentation,future,color,data,feature,measure,morphology,filters,util,restoration\nfrom scipy import ndimage\nimport matplotlib.patches as mpatches\nfrom keras.datasets import mnist\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D\nfrom keras.models import Model\nfrom keras import backend as K\nfrom sklearn.decomposition import DictionaryLearning\n\nPATH = '\/kaggle\/input\/image-processing\/'","914148c8":"img = io.imread(PATH+'vibrant.jpeg')\n\nlabel1 = segmentation.slic(img) #default 100 segments, 10.0 overlay\nlabel2 = segmentation.slic(img,n_segments=333,compactness=33) \n#higher compactness gives more shape to segment in the form of a square as it describes spatial closeness\n\noverlay1_1 = color.label2rgb(label1,img,kind='overlay') # overlay the segments over image\noverlay1_2 = color.label2rgb(label1,img,kind='avg') # depict average color value of that segment\noverlay2_1 = color.label2rgb(label2,img,kind='overlay')\noverlay2_2 = color.label2rgb(label2,img,kind='avg')\n\nf,ax = plt.subplots(3,2,figsize=(14,14))\nax = ax.ravel()\n\nax[0].imshow(img)\nax[0].set_title('Original')\n\nax[1].imshow(overlay1_1)\nax[1].set_title('100 Seg, 10 compact, overlay')\n\nax[2].imshow(overlay1_2)\nax[2].set_title('100 Seg, 10 compact, average')\n\nax[3].imshow(overlay2_1)\nax[3].set_title('333 Seg, 3.3 compact, overlay')\n\nax[4].imshow(overlay2_2)\nax[4].set_title('333 Seg, 3.3 compact, average')\n\nf.delaxes(ax[-1])\nplt.show()","b0cc6a9b":"# 333 segments will be our favourite now as it seems to extract better segments\nrag = future.graph.rag_mean_color(img,label2,connectivity=3,mode='distance',sigma=321)\nnew_labels = future.graph.cut_threshold(label2,rag,thresh=27) \n# merge each and every two labels who have distance less than the thresh\n\nrag_img_avg = color.label2rgb(new_labels,img,kind='avg')\n\nf,ax = plt.subplots(1,2,figsize=(14,5))\nax = ax.ravel()\n\nax[0].imshow(img)\nax[0].set_title('Original')\n\nax[1].imshow(rag_img_avg)\nax[1].set_title('Average')\nplt.show()","0451c6b4":"# Import Image and Convert to Grayscale\nimage = data.coins()\nimage = color.rgb2gray(image)\n\n# make a figure to plot our steps\nfig, axes = plt.subplots(ncols=3, nrows=3,figsize=(16,16))\nax0, ax1, ax2, ax3, ax4, ax5, ax6, ax7, ax8 = axes.flat\n\n# plot original image\nax0.imshow(image, cmap='gray')\nax0.set_title('Original',)\nax0.axis('off')\n\n# plot an Histogram of image\nax1.hist(image.flatten(),bins=256, lw=2,)\nax1.set_xlabel('Intensity')\nax1.set_ylabel('No of Pixels')\nax1.set_title('Histogram')\nax1.grid()\n\n# Denoise Image using one or both or combination based on level of noise **we have no noise\n# image = restoration.denoise_bilateral(image) # bilateral\n# image = filters.gaussian(image,0.01) # gaussian\nax2.imshow(image, cmap='gray')\nax2.set_title('Filtering: Remove Noise')\nax2.axis('off')\n\n# apply any threshold local,median,otsu etc\nthresh = filters.threshold_otsu(image) # Otsu Threshold\nbw = image.copy() > thresh # get Binary Image\n# bw = filters.threshold_local(image, 95, offset=-15) # Local threshold to get binary image\n\nax3.imshow(bw, cmap='gray')\nax3.set_title('Thresholding: Binary\/B&W Image')\nax3.axis('off')\n\n# Perform Opening which removes any bright spots less than the threshold size in a B&W Image\n# bw = morphology.area_opening(bw) # Producing errors in the newer version of skimage\nax4.imshow(bw, cmap='gray')\nax4.set_title('Opening: Small Bright Spot Removal')\nax4.axis('off')\n\n# Dilation to remove the blemishes in the coins\ninverted = util.invert(bw) # black coins white background\ndilation_seed = inverted.copy() \ndilation_seed[1:-1,1:-1] = inverted.min()\nmask = inverted\ndilated_img = morphology.reconstruction(dilation_seed,mask,method='dilation')\nbw = util.invert(dilated_img) # convert to original White Coins and Black Background\n\nax5.imshow(bw, cmap='gray')\nax5.set_title('Dilation: Filling holes inside Objects',)\nax5.axis('off')\n\n# Apply Local Maxima to find the Peak  Gradient intensities of the local objects\ncoordinates = feature.peak_local_max(image, min_distance=20)\n\nax6.imshow(image, cmap=plt.cm.gray)\nax6.autoscale(False)\nax6.plot(coordinates[:, 1],coordinates[:, 0], c='r',ls='none',marker='*')\nax6.set_title('Peak local maxima',)\nax6.axis('off')\n\n# Find Edges from using Canny Filter\nedges = feature.canny(image, sigma=3,low_threshold=10,high_threshold=80)\n\nax7.imshow(edges, cmap='gray')\nax7.set_title('Edges',)\nax7.axis('off')\n\n#  Give label to each object in the image and draw a rectangle over that label\nlabel_image = morphology.label(edges)\n\nax8.imshow(image, cmap=plt.cm.gray)\nax8.set_title('Labeled Coins')\nax8.axis('off')\n # Draw rectangle around segmented coins.\nfor region in measure.regionprops(label_image):\n    minr, minc, maxr, maxc = region.bbox\n    rect = mpatches.Rectangle((minc, minr), maxc - minc, maxr - minr, fill=False, edgecolor='red',lw=2)\n    ax8.add_patch(rect)\n    \nplt.show()","38f1d44f":"coins = data.coins()\n# convert to grayscale\n\nf,ax = plt.subplots(2,3,figsize=(15,8))\nax = ax.ravel()\nax[0].imshow(coins,cmap='gray')\nax[0].set_title('Original')\n\n# define markers as objects or not objects\nmarkers = np.zeros_like(coins)\nmarkers[coins < 30] = 1\nmarkers[coins > 150] = 2\n\nax[1].imshow(markers,cmap='gray')\nax[1].set_title('Markers')\n\n# apply sobel filter\nelevation_map = filters.sobel(coins)\n\nax[2].imshow(elevation_map,cmap='gray')\nax[2].set_title('Elevation Map')\n\n# apply watershed\nseg = segmentation.watershed(elevation_map, markers)\nax[3].imshow(seg,cmap='gray')\nax[3].set_title('Segmentation')\n\n# Fill holes or dilation\nseg = ndimage.binary_fill_holes(seg - 1)\nax[4].imshow(seg,cmap='gray')\nax[4].set_title('Filled Holes')\n\n# label the objects\nlabeled_coins, _ = ndimage.label(seg)\noverlaid_img = color.label2rgb(labeled_coins,image=coins,bg_label=0)\nax[5].imshow(overlaid_img)\nax[5].set_title('Labelled Coins')\n\nfor region in measure.regionprops(labeled_coins):\n    # take regions with large enough areas\n    if region.area >= 100:\n        # draw rectangle around segmented coins\n        minr, minc, maxr, maxc = region.bbox\n        rect = mpatches.Rectangle((minc, minr), maxc - minc, maxr - minr,\n                                  fill=False, edgecolor='red', linewidth=2)\n        ax[5].add_patch(rect)\n\nplt.show()","5b4e95ce":"input_img = Input(shape=(28, 28, 1))  # adapt this if using `channels_first` image data format\n\n#Encoder\nx = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = MaxPooling2D((2, 2), padding='same')(x)\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n\nencoded = MaxPooling2D((2, 2), padding='same')(x) # This is the latent layer\n\n# at this point the representation is (4, 4, 8) i.e. 128-dimensional\n\n# Decoder\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded) # Mirror of 3rd convolution layer\nx = UpSampling2D((2, 2))(x) # opposite of Pooling\nx = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\nx = UpSampling2D((2, 2))(x)\nx = Conv2D(16, (3, 3), activation='relu')(x)\nx = UpSampling2D((2, 2))(x)\ndecoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x) # Because We need 1 image only\n\nautoencoder = Model(input_img, decoded)\nautoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n","0465cecb":"(x_train, _), (x_test, _) = mnist.load_data()\n\nx_train = x_train.astype('float32') \/ 255. # Normalize the range from 0-1\nx_test = x_test.astype('float32') \/ 255.\nx_train = np.reshape(x_train, (len(x_train), 28, 28, 1))  # adapt this if using `channels_first` image data format\nx_test = np.reshape(x_test, (len(x_test), 28, 28, 1))  # adapt this if using `channels_first` image data format","35bdfaba":"autoencoder.fit(x_train, x_train,epochs=1,batch_size=128,shuffle=True,validation_data=(x_test, x_test))\n# y is the image itself","cc3b7a53":"decoded_imgs = autoencoder.predict(x_test)\n\nn = 10\nfig = plt.figure(figsize=(20, 4))\nfor i in range(1,n):\n    # display original\n    ax = plt.subplot(2, n, i)\n    plt.imshow(x_test[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # display reconstruction\n    ax = plt.subplot(2, n, i + n)\n    plt.imshow(decoded_imgs[i].reshape(28, 28))\n    plt.gray()\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    \nfig.suptitle('Original (Above) vs Recreated (Below)')\nplt.show()","b0b96abe":"sample = x_train[:50,:] # Get first 50 images\nprint(f'Our images are of size: {sample.shape}')\n# Flatten the images\nf_sample = sample.reshape(50,-1)\/sample.max() # normalize\nprint(f'Each of our image has now {f_sample.shape[1]} Features')","2fec268e":"dict_l = DictionaryLearning(n_components=36,fit_algorithm='lars',transform_algorithm='lasso_lars')\nX_dict = dict_l.fit_transform(f_sample)","dde9e54c":"print(f'We have found {X_dict.shape[1]} features from the old {f_sample.shape[1]} features')","44942b3c":"n = 10\nfig = plt.figure(figsize=(20, 4))\nfor i in range(1,n):\n    # display original\n    ax = plt.subplot(2, n, i)\n    ax.imshow(sample[i].reshape(28, 28),cmap='Blues_r')\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    # display dictonary\n    ax = plt.subplot(2, n, i + n)\n    ax.imshow(X_dict[i].reshape(6,6),cmap='Blues_r',interpolation='nearest') # 6*6 = 36\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    \nfig.suptitle('Original (Above) vs Atoms (Below)')\nplt.show()","bfe6dd33":"## Prepare Data","980a4740":"## Train Model\nFor 1 Epoch , just for demonstration","0cfa531f":"## Build Simple Model\nWe Will feed the image itself in place of y-labels because this what we have to re create and see how different it is from the original.","f467b777":"# Autoencoding\nLet us start our analogy of autoencoders. Imagine a child, a little one with a toy car which he\/she opens up apart piece to piece. Guess we all have done this with our toys out of curiosity and tried to build it again. This child does the same but instead of crying like us, he has lots and lots of toy cars of same model so when he fails completely, he opens up another and try to build this and he keeps doing it until one fine day, he rebuilds the car from scratch. This child is named autoencoder. **But how did he learn**? He learned it by using lots of data (toy cars), neurons weights (trying out different combinations) and by minimizing the errors (how different the re-assembled car was from the original).\n\nAutoencoders breaks down the image to form hidden features and re assemnle the image. They minimize the loss or difference in the actual and re created image and in the process they know some hidden features.\n\n**USE**\n1. **Anomaly Detection**: You can train autoencoders on images of `normal` events such as traffic lights where all cars are moving according to rules. By doing this, system will know what `normal` is for that particular scenario and when an acident happens, it'll give you a high loss value which means there is an anomaly.\n2. **Noise Reduction**: You can feed the inputs as noisy images and output as the Actual images of same thing. In the process, the autoencoders will learn how to change the parametrs so that the noisy image looks like perfect. We can denoise any image like that.\n3. **Deep Fakes**: This is my favourite. You use the latent features of the images and exchange them with latent features of other images. In this way, you can have the costume of Superman with Face of Joker. For this you need to have lots and lots of images of both to learn the latent features.\n4. **Dimensionality Reduction**: You can keep the latent features of images and use them as the primary input for some task because these features are the important features of an image which makes them unique.\n\n**Structure**:\nAutoencoders are Hourglass means they have Encoders and Decoders as exactly mirrors of each other and aligned to the both sides of `latent layer`. So you can think of an encoder as an odd layered model with the moddle layer as `latent layer`. **`latent layer`** is always having dimensions less than the original image so that it can learn **important** features.\nCheck out this awesome [Keras Blog about encoders](https:\/\/blog.keras.io\/building-autoencoders-in-keras.html) for more information on autoencoders and [my post on CNN and Keras](https:\/\/www.kaggle.com\/deshwalmahesh\/bengali-ai-complete-beginner-tutorial-95-acc).\nLet us build a CNN Encoder using the MNIST Dtaset.","ebacea0e":"## Test Model","fea6ee1a":"# Image Segmentation \nGrouping of Pixels which form a *meangning* together. Such as some have the same intensity, or texture or brightness or just the contours; just like two different shirts in an image will have two different *types* of pixels so we can group those to separate the shirts from the background.\n**HOW DO WE FIND IT**?\nBefore we answer it, we'll go about **How do we store or collect same pixels together**??\nWe'll be using:\n## Region Adjacency Graph (RAG Thresholding)\nRAG is a special kind of data of data structure to store the same kind of pixels together in form of blocks. Each block or region is defined as a node and edges have weights which represent the **difference between the `average color` of pixels between two regions**. Weights define that how a block or region is different from the neighbouring region. You can define a threshold value that if weight of two regions are smaller than thi`th`, those two regions are similar and you can keep merging until there are no 2 regions left who are below the threshold.\n### Create Segments and Generate Labels\nWe'll be generating the desired numbers of segments using the `SLIC` which uses `K-Means Clustering` under the hood to find and cluster the segments and generate Labels","c8f785e4":"# Object Detection\nThere are many ways to do the same using one technique or a blend of multiple techniques. We'll be discussing a few of those techniques.\n\n**Some of the techniques are image specific like `gaussian\/bilateral filter to remove noise`, `opening\/closing to remove small spots` or `dilation\/erosion to fill the holes and patches` and you may or may not need them with your image**.\n\nVery detailed discussions about the thresholding, Canny, filters, histogram is given in my previous notebooks of this Image Processing series.\n\n**Steps are given as the comments to the code**\n## Threshold + Local Maxima + Canny","95378772":"For finding the markers and more insights about the working of whole Watershed algorithm, visit [this stackoverflow answer](https:\/\/stackoverflow.com\/questions\/11294859\/how-to-define-the-markers-for-watershed-in-opencv) to have some better understanding of the algorithm.","81672278":"## Create RAG","97c7c015":"# Thank You","fcdbbefd":"# Hi!!\nThis is 2nd of the 3 parts tutorial about the Image Processing Techniques. If you are not familiar with the image processing techniques, Please Check out:\n\n1. [Part 1](https:\/\/www.kaggle.com\/deshwalmahesh\/image-processing-1-basic-ocr-feature-pooling-conv) where I have briefly explained mant things for image processing such as Basic operations on images like reading, cropping, resizing, flipping, color space conversion etc.. And the nsome intermediate steps like Convolution, Pooling, Custom Filters, Reconstruction using Erosion, Dilation , Finding Convex Hull, Noise Reduction, Histogram, Feature Detection, OCR to read itext from data and how to compare 2 images using Structural Similarity.\n\n2. [Part 2](https:\/\/www.kaggle.com\/deshwalmahesh\/image-processing-2-contour-edge-threshold) is all about Object Boundry segmentation using Canny Filters, Contour Detection, Sobel Filters, Corner Detection, Thresholding (Local and Global) to seperate the Image in 2 Parts as Object and Background using Otsu, Yen, Mean, Adaptive etc.\n\n\nIn this Part, We'll be looking at:\n1. Image Segmentation using Rag or Region Adjecency Graph\n2. Object Detection Using Canny + Thresholding\n3. Watershed Algorithm as Marker Based Algorithm\n4. Autoencoders for Latent Features Detection For Noise Removal, Image Reconstruction, Deep Fake, Anomaly Detector\n5. Dictonary Learning for Dimentionality Reduction","b2d1dcac":"# Dictonary Learning\nApart from Isomap, PCA, LDA, SVD etc, you can use `DictonaryLearning` to get the low dimensionality Representation of images. `Used in Denoising`\n\n**Atom extraction and dictionary learning is a technique that allows you to rebuild a sample starting from a sparse dictionary of atoms (similar to principal components, but without constraints about the independence).**\n\nConventionally, when the dictionary contains a number of elements less than the dimensionality of the samples m, it is called under-complete, and on the other hand it's called over-complete when the number of atoms is larger (sometimes much larger) than m.\n\n**Working is just like Autoencoders** to learn hidden Sparse encodings from images to denoise images but it uses  Machine Learning techniques such as OMP, LARS or thresholding for finding the representation or atoms, then it stores the atoms as well as combinations of atoms and performs denoising using those combinations for new images.","29437e33":"### Watershed + Sobel + Manual Markers\nAny grayscale image can be viewed as a topographic surface where high intensity denotes peaks and hills while low intensity denotes valleys. You start filling every isolated valleys (local minima) with different colored water (labels). As the water rises, depending on the peaks (gradients) nearby, water from different valleys, obviously with different colors will start to merge. To avoid that, you build barriers in the locations where water merges. You continue the work of filling water and building barriers until all the peaks are under water. Then the barriers you created gives you the segmentation result. This is the \"philosophy\" behind the watershed. You can visit the [CMM webpage on watershed](http:\/\/www.cmm.mines-paristech.fr\/~beucher\/wtshed.html) to understand it with the help of some animations.\n\n**Best thing about Watershed Algorithm is that unlike other segmentation, edge, contour and object detection algorithms, `it can find the overlapping and the touching boundries too`**"}}