{"cell_type":{"008567ae":"code","dbb3287c":"code","74f5cc6e":"code","de7f00fe":"code","2f06fb77":"code","1f67069b":"code","bea2ba1a":"code","c392f833":"code","485d589c":"code","bca61eda":"code","0eab138b":"code","e5f8653e":"code","6741baf3":"code","438a6210":"code","d60d7435":"code","2b4b8289":"code","c04a3c36":"code","3ec66adc":"code","bffaec5a":"code","1ac82571":"code","7c321327":"code","cb2621ac":"code","3e434095":"code","6c189f96":"code","aa53f73d":"code","7ed9962b":"code","566c3bc1":"code","1e9ddf28":"code","5379638d":"code","b0c174cd":"code","68ba3b41":"code","fd8e6f9a":"code","b65f299b":"code","61709e99":"code","a9d0853c":"code","346d2de2":"code","9cf7b251":"code","9685bb73":"code","db806ea9":"code","70b98909":"code","3fcf33e8":"code","37024f11":"code","6d312288":"markdown","045a4930":"markdown","8b7e0b35":"markdown","d5f090e8":"markdown","f3d38046":"markdown","15265349":"markdown","80b8b269":"markdown","c379032b":"markdown","1c88fa24":"markdown","39e10b27":"markdown","affa8eeb":"markdown","1b71801a":"markdown","16960b7c":"markdown","14b70e5d":"markdown","9f489013":"markdown","95f5eb01":"markdown","fab2d06a":"markdown","330aa0bb":"markdown","dd18cb95":"markdown","7db093fb":"markdown","e189d5ae":"markdown","183019c8":"markdown","ef96ce8f":"markdown","814f1d4d":"markdown","83962603":"markdown","b4508a19":"markdown","e88f3b8a":"markdown","00a9294d":"markdown","3db8461b":"markdown","f45030cd":"markdown","f827c154":"markdown","63a3b9f3":"markdown","acb0d0b2":"markdown","11d88ea5":"markdown","06cf29de":"markdown"},"source":{"008567ae":"#N-grams\nimport nltk               # NLP toolkit\nimport re                 # Library for Regular expression operations\n\nnltk.download('punkt')    # Download the Punkt sentence tokenizer ","dbb3287c":"#Install nltk\n!pip install -U nltk==3.4","74f5cc6e":"import pandas as pd\nimport numpy as np\nimport regex as re\nimport nltk\nimport random\nfrom nltk.lm.preprocessing import padded_everygram_pipeline\nfrom nltk.tokenize import TweetTokenizer\nimport emoji\nfrom nltk.lm import MLE","de7f00fe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2f06fb77":"data = pd.read_csv(\"\/kaggle\/input\/large-random-tweets-from-pakistan\/Random Tweets from Pakistan- Cleaned- Anonymous.csv\",encoding_errors = 'ignore')\ndata.head(10)","1f67069b":"print('There are {} rows and {} columns in data'.format(data.shape[0],data.shape[1]))","bea2ba1a":"data.isnull().values.any()","c392f833":"data.isnull().sum()","485d589c":"data.fillna(0, inplace = True)\ndata.head()","bca61eda":"data['full_text']=data['full_text'].apply(str)\ndef cleantxt(text):\n    text = re.sub(r'@[A-Za-z0-9]+', '',text)\n    text = re.sub(r'#', '',text)\n    text = re.sub(r'RT[\\s]+', '',text)\n    text = re.sub(r'https?:\\\/\\\/\\S+', '',text)\n    \n    text = re.sub(r\"that's\",\"that is\",text)\n    text = re.sub(r\"there's\",\"there is\",text)\n    text = re.sub(r\"what's\",\"what is\",text)\n    text = re.sub(r\"where's\",\"where is\",text)\n    text = re.sub(r\"it's\",\"it is\",text)\n    text = re.sub(r\"who's\",\"who is\",text)\n    text = re.sub(r\"i'm\",\"i am\",text)\n    text = re.sub(r\"she's\",\"she is\",text)\n    text = re.sub(r\"he's\",\"he is\",text)\n    text = re.sub(r\"they're\",\"they are\",text)\n    text = re.sub(r\"who're\",\"who are\",text)\n    text = re.sub(r\"ain't\",\"am not\",text)\n    text = re.sub(r\"wouldn't\",\"would not\",text)\n    text = re.sub(r\"shouldn't\",\"should not\",text)\n    text = re.sub(r\"can't\",\"can not\",text)\n    text = re.sub(r\"couldn't\",\"could not\",text)\n    text = re.sub(r\"won't\",\"will not\",text)\n\n    return text\n\ndata['full_text'] = data['full_text'].apply(cleantxt)\n\ndata['full_text']","0eab138b":"data = data['full_text']\ndata = data.dropna()","e5f8653e":"print('Tweet before preprocessing and tokenization: \\n', data[9])\n\n# Removing Urdu language\nreg = re.compile(r'[\\u0600-\\u06ff]+', re.UNICODE)\ndata = data.apply(lambda x: re.sub(reg, \"\", x))\n\n# removing extra spaces\ndata = data.apply(lambda x: re.sub(r'[  ]+', \" \", x))\n\n# converting to lowercase letters\ndata = data.apply(lambda x: x.strip().lower())\n\n# remove hyperlinks\ndata = data.apply(lambda x: re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', x))\n\n# removing @Mentions\ndata = data.apply(lambda x:re.sub(r'@.+?\\s', '', x))\n\n# removing extra symbols\ndata = data.apply(lambda x: re.sub(r'#', '', x))\ndata = data.apply(lambda x: re.sub(r'rt : ', '', x))\ndata = data.apply(lambda x: re.sub(r'\\n', ' ', x))\n\n# Dropping duplicates\ndata = data.drop_duplicates()\n\n# Tokenizing text\ntokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                           reduce_len=True)\ndata = data.apply(tokenizer.tokenize)\n\n# removing emoji\ndef clean(x):\n    return [y for y in x if not emoji.is_emoji(y)]\n\n# removing tweets with less than 3 words\ndata = data.apply(clean)\ndata = data.apply(lambda x:np.nan if len(x)<3 else x)\ndata.dropna(inplace = True)\n\nprint('Tweet After preprocessing and tokenization: \\n', data[9])","6741baf3":"# Preprocess the tokenized text for 3-grams language modelling\nn_gram_size = 3\ntrain_data, padded_sents = padded_everygram_pipeline(n_gram_size, data)","438a6210":"# tranining a probablistic model using maximum liklihood estimation\nmodel = MLE(n_gram_size)\nmodel.fit(train_data, padded_sents)\nprint(model.vocab)\nlen(model.vocab)","d60d7435":"# Testing model on Out Of Vocabulary words, unknown tokens\nprint(model.vocab.lookup(['here', 'is', 'a', 'problem', 'in','it'])) \nprint(model.vocab.lookup(['cant','find','solution','buahah','and', 'buahah']))","2b4b8289":"print(\"\",model.counts)","c04a3c36":"print(\"Count of unigram 'pakistan': \",model.counts['pakistan'])\nprint(\"Count of bigram 'is\/pakistan': \",model.counts[['pakistan']]['is'])\nprint(\"Count of trigram 'a\/pakistan is': \",model.counts[['pakistan', 'is']]['a'])","3ec66adc":"# probablities of occurences\nprint(\"probability of 'pakistan' given <s>: \",model.score('pakistan'))\nprint(\"probability of 'is' given 'pakistan': \",model.score('is',['pakistan']))\nprint(\"probability of 'a' given 'pakistan is': \",model.score('a',['pakistan','is']))","bffaec5a":"# Using our 2-gram language to generate text\n# max number of tokens\nlength = 15\nrand_seed=random.randint(0,100)\n\ngen_tweet =' '.join(model.generate(length,random_seed=rand_seed ))\n\n# removing extra ending tokens generated by model\nre.sub('[<\/s>]+','',gen_tweet)","1ac82571":"# Perplexity of our model on sample text\nmodel.perplexity('pakistan')","7c321327":"corpus = \"Learning% makes 'me' happy. I am happy be-cause I am learning! :)\"\ncorpus = corpus.lower()\n\n# note that word \"learning\" will now be the same regardless of its position in the sentence\nprint(corpus)","cb2621ac":"# remove special characters\ncorpus = \"learning% makes 'me' happy. i am happy be-cause i am learning! :)\"\ncorpus = re.sub(r\"[^a-zA-Z0-9.?! ]+\", \"\", corpus)\nprint(corpus)","3e434095":"# split text by a delimiter to array\ninput_date=\"Sat May  9 07:33:35 CEST 2020\"\n\n# get the date parts in array\ndate_parts = input_date.split(\" \")\nprint(f\"date parts = {date_parts}\")\n\n#get the time parts in array\ntime_parts = date_parts[4].split(\":\")\nprint(f\"time parts = {time_parts}\")","6c189f96":"# tokenize the sentence into an array of words\n\nsentence = 'i am happy because i am learning.'\ntokenized_sentence = nltk.word_tokenize(sentence)\nprint(f'{sentence} -> {tokenized_sentence}')","aa53f73d":"# find length of each word in the tokenized sentence\nsentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\nword_lengths = [(word, len(word)) for word in sentence] # Create a list with the word lengths using a list comprehension\nprint(f' Lengths of the words: \\n{word_lengths}')","7ed9962b":"def sentence_to_trigram(tokenized_sentence):\n    \"\"\"\n    Prints all trigrams in the given tokenized sentence.\n    \n    Args:\n        tokenized_sentence: The words list.\n    \n    Returns:\n        No output\n    \"\"\"\n    # note that the last position of i is 3rd to the end\n    for i in range(len(tokenized_sentence) - 3 + 1):\n        # the sliding window starts at position i and contains 3 words\n        trigram = tokenized_sentence[i : i + 3]\n        print(trigram)\n\ntokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n\nprint(f'List all trigrams of sentence: {tokenized_sentence}\\n')\nsentence_to_trigram(tokenized_sentence)\n","566c3bc1":"# get trigram prefix from a 4-gram\nfourgram = ['i', 'am', 'happy','because']\ntrigram = fourgram[0:-1] # Get the elements from 0, included, up to the last element, not included.\nprint(trigram)","1e9ddf28":"# when working with trigrams, you need to prepend 2 <s> and append one <\/s>\nn = 3\ntokenized_sentence = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\ntokenized_sentence = [\"<s>\"] * (n - 1) + tokenized_sentence + [\"<e>\"]\nprint(tokenized_sentence)","5379638d":"# manipulate n_gram count dictionary\n\nn_gram_counts = {\n    ('i', 'am', 'happy'): 2,\n    ('am', 'happy', 'because'): 1}\n\n# get count for an n-gram tuple\nprint(f\"count of n-gram {('i', 'am', 'happy')}: {n_gram_counts[('i', 'am', 'happy')]}\")\n\n# check if n-gram is present in the dictionary\nif ('i', 'am', 'learning') in n_gram_counts:\n    print(f\"n-gram {('i', 'am', 'learning')} found\")\nelse:\n    print(f\"n-gram {('i', 'am', 'learning')} missing\")\n\n# update the count in the word count dictionary\nn_gram_counts[('i', 'am', 'learning')] = 1\nif ('i', 'am', 'learning') in n_gram_counts:\n    print(f\"n-gram {('i', 'am', 'learning')} found\")\nelse:\n    print(f\"n-gram {('i', 'am', 'learning')} missing\")","b0c174cd":"# concatenate tuple for prefix and tuple with the last word to create the n_gram\nprefix = ('i', 'am', 'happy')\nword = 'because'\n\n# note here the syntax for creating a tuple for a single word\nn_gram = prefix + (word,)\nprint(n_gram)","68ba3b41":"import numpy as np\nimport pandas as pd\nfrom collections import defaultdict\ndef single_pass_trigram_count_matrix(corpus):\n    \"\"\"\n    Creates the trigram count matrix from the input corpus in a single pass through the corpus.\n    \n    Args:\n        corpus: Pre-processed and tokenized corpus. \n    \n    Returns:\n        bigrams: list of all bigram prefixes, row index\n        vocabulary: list of all found words, the column index\n        count_matrix: pandas dataframe with bigram prefixes as rows, \n                      vocabulary words as columns \n                      and the counts of the bigram\/word combinations (i.e. trigrams) as values\n    \"\"\"\n    bigrams = []\n    vocabulary = []\n    count_matrix_dict = defaultdict(dict)\n    \n    # go through the corpus once with a sliding window\n    for i in range(len(corpus) - 3 + 1):\n        # the sliding window starts at position i and contains 3 words\n        trigram = tuple(corpus[i : i + 3])\n        \n        bigram = trigram[0 : -1]\n        if not bigram in bigrams:\n            bigrams.append(bigram)        \n        \n        last_word = trigram[-1]\n        if not last_word in vocabulary:\n            vocabulary.append(last_word)\n        \n        if (bigram,last_word) not in count_matrix_dict:\n            count_matrix_dict[bigram,last_word] = 0\n            \n        count_matrix_dict[bigram,last_word] += 1\n    \n    # convert the count_matrix to np.array to fill in the blanks\n    count_matrix = np.zeros((len(bigrams), len(vocabulary)))\n    for trigram_key, trigam_count in count_matrix_dict.items():\n        count_matrix[bigrams.index(trigram_key[0]), \\\n                     vocabulary.index(trigram_key[1])]\\\n        = trigam_count\n    \n    # np.array to pandas dataframe conversion\n    count_matrix = pd.DataFrame(count_matrix, index=bigrams, columns=vocabulary)\n    return bigrams, vocabulary, count_matrix\n\ncorpus = ['i', 'am', 'happy', 'because', 'i', 'am', 'learning', '.']\n\nbigrams, vocabulary, count_matrix = single_pass_trigram_count_matrix(corpus)\n\nprint(count_matrix)","fd8e6f9a":"# create the probability matrix from the count matrix\nrow_sums = count_matrix.sum(axis=1)\n# divide each row by its sum\nprob_matrix = count_matrix.div(row_sums, axis=0)\n\nprint(prob_matrix)","b65f299b":"# find the probability of a trigram in the probability matrix\ntrigram = ('i', 'am', 'happy')\n\n# find the prefix bigram \nbigram = trigram[:-1]\nprint(f'bigram: {bigram}')\n\n# find the last word of the trigram\nword = trigram[-1]\nprint(f'word: {word}')\n\n# we are using the pandas dataframes here, column with vocabulary word comes first, row with the prefix bigram second\ntrigram_probability = prob_matrix[word][bigram]\nprint(f'trigram_probability: {trigram_probability}')","61709e99":"# lists all words in vocabulary starting with a given prefix\nvocabulary = ['i', 'am', 'happy', 'because', 'learning', '.', 'have', 'you', 'seen','it', '?']\nstarts_with = 'ha'\n\nprint(f'words in vocabulary starting with prefix: {starts_with}\\n')\nfor word in vocabulary:\n    if word.startswith(starts_with):\n        print(word)","a9d0853c":"# we only need train and validation %, test is the remainder\nimport random\ndef train_validation_test_split(data, train_percent, validation_percent):\n    \"\"\"\n    Splits the input data to  train\/validation\/test according to the percentage provided\n    \n    Args:\n        data: Pre-processed and tokenized corpus, i.e. list of sentences.\n        train_percent: integer 0-100, defines the portion of input corpus allocated for training\n        validation_percent: integer 0-100, defines the portion of input corpus allocated for validation\n        \n        Note: train_percent + validation_percent need to be <=100\n              the reminder to 100 is allocated for the test set\n    \n    Returns:\n        train_data: list of sentences, the training part of the corpus\n        validation_data: list of sentences, the validation part of the corpus\n        test_data: list of sentences, the test part of the corpus\n    \"\"\"\n    # fixed seed here for reproducibility\n    random.seed(87)\n    \n    # reshuffle all input sentences\n    random.shuffle(data)\n\n    train_size = int(len(data) * train_percent \/ 100)\n    train_data = data[0:train_size]\n    \n    validation_size = int(len(data) * validation_percent \/ 100)\n    validation_data = data[train_size:train_size + validation_size]\n    \n    test_data = data[train_size + validation_size:]\n    \n    return train_data, validation_data, test_data\n\ndata = [x for x in range (0, 100)]\n\ntrain_data, validation_data, test_data = train_validation_test_split(data, 80, 10)\nprint(\"split 80\/10\/10:\\n\",f\"train data:{train_data}\\n\", f\"validation data:{validation_data}\\n\", \n      f\"test data:{test_data}\\n\")\n\ntrain_data, validation_data, test_data = train_validation_test_split(data, 98, 1)\nprint(\"split 98\/1\/1:\\n\",f\"train data:{train_data}\\n\", f\"validation data:{validation_data}\\n\", \n      f\"test data:{test_data}\\n\")","346d2de2":"# to calculate the exponent, use the following syntax\np = 10 ** (-250)\nM = 100\nperplexity = p ** (-1 \/ M)\nprint(perplexity)","9cf7b251":"# build the vocabulary from M most frequent words\n# use Counter object from the collections library to find M most common words\nfrom collections import Counter\n\n# the target size of the vocabulary\nM = 3\n\n# pre-calculated word counts\n# Counter could be used to build this dictionary from the source corpus\nword_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning': 3, '.': 1}\n\nvocabulary = Counter(word_counts).most_common(M)\n\n# remove the frequencies and leave just the words\nvocabulary = [w[0] for w in vocabulary]\n\nprint(f\"the new vocabulary containing {M} most frequent words: {vocabulary}\\n\") ","9685bb73":"# test if words in the input sentences are in the vocabulary, if OOV, print <UNK>\nsentence = ['am', 'i', 'learning']\noutput_sentence = []\nprint(f\"input sentence: {sentence}\")\n\nfor w in sentence:\n    # test if word w is in vocabulary\n    if w in vocabulary:\n        output_sentence.append(w)\n    else:\n        output_sentence.append('<UNK>')\n        \nprint(f\"output sentence: {output_sentence}\")","db806ea9":"# iterate through all word counts and print words with given frequency f\nf = 3\n\nword_counts = {'happy': 5, 'because': 3, 'i': 2, 'am': 2, 'learning':3, '.': 1}\n\nfor word, freq in word_counts.items():\n    if freq == f:\n        print(word)","70b98909":"# many <unk> low perplexity \ntraining_set = ['i', 'am', 'happy', 'because','i', 'am', 'learning', '.']\ntraining_set_unk = ['i', 'am', '<UNK>', '<UNK>','i', 'am', '<UNK>', '<UNK>']\n\ntest_set = ['i', 'am', 'learning']\ntest_set_unk = ['i', 'am', '<UNK>']\n\nM = len(test_set)\nprobability = 1\nprobability_unk = 1\n\n# pre-calculated probabilities\nbigram_probabilities = {('i', 'am'): 1.0, ('am', 'happy'): 0.5, ('happy', 'because'): 1.0, ('because', 'i'): 1.0, ('am', 'learning'): 0.5, ('learning', '.'): 1.0}\nbigram_probabilities_unk = {('i', 'am'): 1.0, ('am', '<UNK>'): 1.0, ('<UNK>', '<UNK>'): 0.5, ('<UNK>', 'i'): 0.25}\n\n# got through the test set and calculate its bigram probability\nfor i in range(len(test_set) - 2 + 1):\n    bigram = tuple(test_set[i: i + 2])\n    probability = probability * bigram_probabilities[bigram]\n        \n    bigram_unk = tuple(test_set_unk[i: i + 2])\n    probability_unk = probability_unk * bigram_probabilities_unk[bigram_unk]\n\n# calculate perplexity for both original test set and test set with <UNK>\nperplexity = probability ** (-1 \/ M)\nperplexity_unk = probability_unk ** (-1 \/ M)\n\nprint(f\"perplexity for the training set: {perplexity}\")\nprint(f\"perplexity for the training set with <UNK>: {perplexity_unk}\")\n","3fcf33e8":"def add_k_smooting_probability(k, vocabulary_size, n_gram_count, n_gram_prefix_count):\n    numerator = n_gram_count + k\n    denominator = n_gram_prefix_count + k * vocabulary_size\n    return numerator \/ denominator\n\ntrigram_probabilities = {('i', 'am', 'happy') : 2}\nbigram_probabilities = {( 'am', 'happy') : 10}\nvocabulary_size = 5\nk = 1\n\nprobability_known_trigram = add_k_smooting_probability(k, vocabulary_size, trigram_probabilities[('i', 'am', 'happy')], \n                           bigram_probabilities[( 'am', 'happy')])\n\nprobability_unknown_trigram = add_k_smooting_probability(k, vocabulary_size, 0, 0)\n\nprint(f\"probability_known_trigram: {probability_known_trigram}\")\nprint(f\"probability_unknown_trigram: {probability_unknown_trigram}\")","37024f11":"# pre-calculated probabilities of all types of n-grams\ntrigram_probabilities = {('i', 'am', 'happy'): 0.15}\nbigram_probabilities = {( 'am', 'happy'): 0.3}\nunigram_probabilities = {'happy': 0.4}\n\n# the weights come from optimization on a validation set\nlambda_1 = 0.8\nlambda_2 = 0.15\nlambda_3 = 0.05\n\n# this is the input trigram we need to estimate\ntrigram = ('i', 'am', 'happy')\n\n# find the last bigram and unigram of the input\nbigram = trigram[1: 3]\nunigram = trigram[2]\nprint(f\"besides the trigram {trigram} we also use bigram {bigram} and unigram ({unigram})\\n\")\n\n# in the production code, you would need to check if the probability n-gram dictionary contains the n-gram\nprobability_hat_trigram = lambda_1 * trigram_probabilities[trigram] \n+ lambda_2 * bigram_probabilities[bigram]\n+ lambda_3 * unigram_probabilities[unigram]\n\nprint(f\"estimated probability of the input trigram {trigram} is {probability_hat_trigram}\")","6d312288":"# Tweet before preprocessing and tokenization and Tweet after preprocessing and tokenization","045a4930":"# New Corpus","8b7e0b35":"# Dataset Size","d5f090e8":"# Probablities of Occurences","f3d38046":"# Tokenization","15265349":"# 3-grams language modelling","80b8b269":"# N-grams","c379032b":"# Finding Length","1c88fa24":"# New Tweets Generation using Language Model","39e10b27":"# Create the n_gram\n for creating n-gram we need to concatenate tuple for prefix and tuple with the last word to create the n_gram","affa8eeb":"# pre-calculated probabilities of all types of n-grams","1b71801a":"# Testing if words in the input sentences are in the vocabulary, if OOV, print UNK>","16960b7c":"# Train and validation %, test","14b70e5d":"# Count of unigram , bigram and trigram. ","9f489013":"#  Manipulate n_gram count dictionary","95f5eb01":"# **Assignment Description:**\n**Task 1:**\n* Create a language model using our tweets dataset\n\n**Task 2:**\n\n* Generate new tweets using the language model and calculate perplexity of the model\n\n**Code Description**\n* Task 1 and task 2 is applied on both methods.\n1.**we need to create a language model on random twitter dataset and then calculate perplexity of whole model.** \n*    Loading data\n*    EDA and Preprocessing \n*    Tweet before preprocessing and tokenization and Tweet after preprocessing and tokenization\n*    3-gram language modeling \n*    Tranining a probablistic model\n*    Testing model on Out Of Vocabulary words\n*    Count of Unigram Bigram ,Trigram \n*    Probablities of Occurences\n*    New Tweets Generation using Language Model\n*    Perplexity of model\n \n \n \n \n\n2. **i have also tried it on simple corpus and again created whole language model and also checked proabability of matrices by another method.**\n*      New Corpus\n*      Remove Special Characters\n*      Split text by a delimiter to array\n*      Tokenization \n*      Finding Length\n*      sentence to trigram Function\n*      Manipulate n_gram count dictionary\n*      Create the n_gram\n*      Probability matrix\n*      Probability of a trigram\n*      Train and validation %, test\n*      Calculating Exponent\n*      unk> low perplexit\n*      K-Smoothing Probabaility \n*      pre-calculated probabilities of all types of n-grams\n\n3. **Check code for A to Z guide on Language modeling.**","fab2d06a":"# Loading Dataset","330aa0bb":"# k_smooting_probability","dd18cb95":"# Trigram prefix from a 4-gram","7db093fb":"# Testing model on Out Of Vocabulary words","e189d5ae":"# sentence to trigram Function","183019c8":"# Probability matrix from the count matrix","ef96ce8f":"# calculating the exponent","814f1d4d":"# Split text by a delimiter to array","83962603":"# Tranining a probablistic model","b4508a19":"# Probability of a trigram in the probability matrix","e88f3b8a":"#  Function of single_pass_trigram_count_matrix","00a9294d":"# unk> low perplexity ","3db8461b":"# Remove special characters","f45030cd":"# Importing Required Libraries","f827c154":"# EDA And Preprocessing","63a3b9f3":"# lists all words in vocabulary starting with a given prefix","acb0d0b2":"# Iterate through all word counts and print words with given frequency f","11d88ea5":"# Perplexity of model","06cf29de":"# Importing counter"}}