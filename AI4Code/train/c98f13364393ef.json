{"cell_type":{"cc709902":"code","1c4f339d":"code","dab36bf2":"code","c9918e66":"code","53e05f65":"code","0959476e":"code","9d0b76a3":"code","351cf138":"code","5f761098":"code","bd9ca3fd":"code","a943a648":"code","0cc43ee6":"code","e5a57136":"code","ec418576":"code","44e70125":"code","eb1f66a4":"code","493cb44c":"code","1383428b":"code","ee1a6893":"code","3ca89af5":"code","86cf1e49":"code","2caae77f":"code","5bd67115":"code","e9881ef4":"code","a6b15516":"markdown","cd6731db":"markdown","e0a824bf":"markdown","0bbcff5d":"markdown","32e09e9e":"markdown","9fbad006":"markdown"},"source":{"cc709902":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1c4f339d":"import pandas as pd\ntrain = pd.read_csv(\"..\/input\/chinese-text-multi-classification\/nCoV_100k_train.labled.csv\")\ntest = pd.read_csv(\"..\/input\/chinese-text-multi-classification\/nCov_10k_test.csv\")\ntrain.head(2)","dab36bf2":"labled_train=pd.read_csv(\"\/kaggle\/input\/unlabled-train-data-sample\/train_unlable_sample.csv\")\n#labled_train=pd.read_csv(\"\/kaggle\/input\/fake-data-8w\/train_unlable_sample_8w.csv\")\nlabled_train.drop(['id','date','user','image','vedio'],axis = 1,inplace = True)\n#labled_train=labled_train.sample(n=40000,random_state=2020)\nlabled_train.head(2)","c9918e66":"columns=['id','date','user','content','image','vedio','target']\ntrain.columns=columns\ntest.columns=columns[:-1]\ntrain.drop(['id','date','user','image','vedio'],axis = 1,inplace = True)\ntrain.head()","53e05f65":"train['target'].value_counts()","0959476e":"train_1=train.loc[(train['target']=='-1')] \ntrain0=train.loc[(train['target']=='0')] \ntrain1=train.loc[(train['target']=='1')] \n\ntrain_1.loc[:,'target']=-1\ntrain0.loc[:,'target']=0\ntrain1.loc[:,'target']=1\n\ntrain = pd.concat([train_1,train0,train1])\nprint(train['target'].value_counts())\ntrain.head()","9d0b76a3":"train = pd.concat([train,labled_train])\nprint(train['target'].value_counts())","351cf138":"train.reset_index(drop=True,inplace=True)\ntrain.head()","5f761098":"train=train.sample(frac=1,random_state=2020)\ntrain.head()","bd9ca3fd":"EPOCHS=10\nMAX_SEQUENCE_LENGTH=220","a943a648":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\nimport tensorflow_hub as hub\nimport numpy as np\nprint(tf.__version__)","0cc43ee6":"from transformers import *","e5a57136":"#\u52a0\u8f7dtokenizer\u548cmodel\n#pretrained = \"hfl\/chinese-roberta-wwm-ext\"\npretrained = 'bert-base-chinese'\ntokenizer = BertTokenizer.from_pretrained(pretrained)\npretrained_model = TFBertModel.from_pretrained(pretrained)","ec418576":"tokenizer.tokenize(\"\u5b66\u4e60\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684BERT\u6a21\u578b\")","44e70125":"tokenizer.encode_plus(\"\u5b66\u4e60\u81ea\u7136\u8bed\u8a00\u5904\u7406\u7684BERT\u6a21\u578b\",max_length=220)","eb1f66a4":"#train_input = text_encode(train['content'].astype(str), tokenizer, max_len=MAX_SEQUENCE_LENGTH)\n#test_input = text_encode(test['content'].astype(str), tokenizer, max_len=MAX_SEQUENCE_LENGTH)\n\ntrain_input = tokenizer.batch_encode_plus(train['content'].astype(str), max_length=MAX_SEQUENCE_LENGTH, pad_to_max_length=True, return_tensors='tf')\ntest_input = tokenizer.batch_encode_plus(test['content'].astype(str), max_length=MAX_SEQUENCE_LENGTH, pad_to_max_length=True, return_tensors='tf')\n\ntrain_labels = train['target'].astype(int)+1","493cb44c":"def create_model(pretrained_model):\n    input_ids = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_ids')\n    token_type_ids = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='token_type_ids')\n    attention_mask = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='attention_mask')\n    \n    # Use pooled_output(hidden states of [CLS]) as sentence level embedding\n    pooled_output = pretrained_model({'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids})[1]\n    \n    output = Dense(3, activation='sigmoid')(pooled_output)\n    model = Model(inputs={'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids}, outputs=output)\n    model.compile(Adam(lr=2e-6), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model","1383428b":"def create_model_multi_dropout(pretrained_model):\n    input_ids = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_ids')\n    token_type_ids = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='token_type_ids')\n    attention_mask = Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='attention_mask')\n    \n    # Use pooled_output(hidden states of [CLS]) as sentence level embedding\n    pooled_output = pretrained_model({'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids})[1]\n    \n    # multi dropout\n    dropouts = [Dropout(0.1) for _ in range(3)]\n    for  i, dropout in enumerate(dropouts) :\n        if i==0:\n            out = dropout( pooled_output)\n            output = Dense(3, activation='sigmoid')(out)\n        else:\n            temp_out = dropout ( pooled_output )\n            output = output + Dense(3, activation='sigmoid')(temp_out)\n    output = output \/ len( dropouts)        \n\n    model = Model(inputs={'input_ids': input_ids, 'attention_mask': attention_mask, 'token_type_ids': token_type_ids}, outputs=output)\n    #model.compile(Adam(lr=2e-6), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    model.compile(Adam(lr=1e-6), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model","ee1a6893":"#model=create_model(pretrained_model)\nmodel = create_model_multi_dropout(pretrained_model)\nmodel.summary()","3ca89af5":"train_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=EPOCHS,\n    batch_size=16,\n    callbacks=[EarlyStopping(monitor='val_loss',patience=3,restore_best_weights=True)]\n)","86cf1e49":"model.save_weights('bert_hf_model_epoch10_addlabeled_data_multi_dropout.h5')","2caae77f":"test_pred = model.predict(test_input)\npredictions = np.argmax(test_pred, axis=-1)-1\nprint(predictions)","5bd67115":"submission=pd.read_csv(\"\/kaggle\/input\/chinese-text-multi-classification\/submit_example.csv\")\nsubmission['y']=predictions\nsubmission.to_csv('submission_bert_hf_addlabeled_data_multi_dropout.csv', index=False)","e9881ef4":"res=[26.58379066, 26.9272789 , 24.38326198]\ntest_pred_op=test_pred*res\npredictions_op = np.argmax(test_pred_op, axis=-1)-1\nsubmission=pd.read_csv(\"\/kaggle\/input\/chinese-text-multi-classification\/submit_example.csv\")\nsubmission['y']=predictions_op\nsubmission.to_csv('submission_bert_hf_addlabeled_data_multi_dropout_optimize.csv', index=False)","a6b15516":"# \u6a21\u578b","cd6731db":"# \u6570\u636e","e0a824bf":"\u9608\u503c\u8c03\u6574","0bbcff5d":"Bert base \uff1a  0.72745\n\nBert base + self labeled data 5w\uff1a0.7346\n\nBert base + self labeled data 4w\uff1a0.72911257\n\nBert base + self labeled data 8w 0.73291934\n\nBert base + self labeled data 5w + multi-dropout \uff1a0.72623032","32e09e9e":"\u52a0\u5165\u81ea\u5df1\u6253\u6807\u7b7e\u7684\u6570\u636e","9fbad006":"\u81ea\u5df1\u6253\u6807\u7b7e\u7684\u6570\u636e"}}