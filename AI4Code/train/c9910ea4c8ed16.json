{"cell_type":{"c91f1ae7":"code","3eab127b":"code","70ccfde0":"code","4c7fc3c3":"code","1891d14c":"code","f2fa98ea":"code","417bf983":"code","fc76037b":"code","96ab158e":"code","9ca96a54":"code","9ce80e2c":"code","671d92bc":"code","7c311913":"code","d602063c":"code","daac5c81":"code","a8d6dd2a":"code","712c37bd":"code","c182416a":"code","5ed11bf4":"code","c163314f":"code","16bb1c84":"code","e30da9b1":"code","eec47bbf":"code","3e20e281":"code","691e111d":"code","8d6efcc9":"markdown","9339ef63":"markdown","98eb6dfc":"markdown","68bf4acc":"markdown","4baa7be0":"markdown","d990a7ab":"markdown","8037b5a3":"markdown","7efa2bf4":"markdown","984265a6":"markdown","5b90bad5":"markdown","4272c5f1":"markdown","17230793":"markdown","81dc4932":"markdown","0d2d82c4":"markdown","148bfcc6":"markdown","3b9d5d0c":"markdown"},"source":{"c91f1ae7":"!pip3 install -U scidownl\n!scidownl -c 3","3eab127b":"from scidownl.scihub import *\n\nwith open('..\/input\/dois-batch-1\/dois_1.txt',\"r\") as f:\n    dois = f.readlines()\n    for doi in dois:\n        SciHub(doi, \"out\").download(choose_scihub_url_index=3)\n        ","70ccfde0":"from bs4 import BeautifulSoup\nimport os.path\nimport glob","4c7fc3c3":"tei_doc = '..\/input\/tei-xml-files\/tei_xml_files\/0001.tei.xml'\n#with open(tei_doc, 'r') as tei:\n#    soup = BeautifulSoup(tei, 'lxml')","1891d14c":"def read_tei(tei_doc):\n    with open(tei_doc, 'r') as tei:\n        soup = BeautifulSoup(tei, 'lxml')\n        return soup\n    raise RuntimeError('Cannot generate a soup from the input') ","f2fa98ea":"def elem_to_text(elem, default=''):\n    if elem:\n        return elem.getText()\n    else:\n        return default","417bf983":"elem_to_text(soup.foobarelem, default=\"NA\")","fc76037b":"idno_elem = soup.find('idno', type='doi')\nprint(f\"The doi is {idno_elem.getText()}\")","96ab158e":"from dataclasses import dataclass\n\n@dataclass\nclass Person:\n    firstname: str\n    middlename: str\n    surname: str\n\nturing_author = Person(firstname='Alan', middlename='M', surname='Turing')\n\nf\"{turing_author.firstname} {turing_author.surname} authored many influential publications in computer science.\"","9ca96a54":"class TEIFile(object):\n    def __init__(self, filename):\n        self.filename = filename\n        self.soup = read_tei(filename)\n        self._text = None\n        self._title = ''\n        self._abstract = ''\n\n    @property\n    def doi(self):\n        idno_elem = self.soup.find('idno', type='DOI')\n        if not idno_elem:\n            return ''\n        else:\n            return idno_elem.getText()\n\n    @property\n    def title(self):\n        if not self._title:\n            self._title = self.soup.title.getText()\n        return self._title\n\n    @property\n    def abstract(self):\n        if not self._abstract:\n            abstract = self.soup.abstract.getText(separator=' ', strip=True)\n            self._abstract = abstract\n        return self._abstract\n\n    @property\n    def authors(self):\n        authors_in_header = self.soup.analytic.find_all('author')\n\n        result = []\n        for author in authors_in_header:\n            persname = author.persname\n            if not persname:\n                continue\n            firstname = elem_to_text(persname.find(\"forename\", type=\"first\"))\n            middlename = elem_to_text(persname.find(\"forename\", type=\"middle\"))\n            surname = elem_to_text(persname.surname)\n            person = Person(firstname, middlename, surname)\n            result.append(person)\n        return result\n    \n    @property\n    def text(self):\n        if not self._text:\n            divs_text = []\n            for div in self.soup.body.find_all(\"div\"):\n                # div is neither an appendix nor references, just plain text.\n                if not div.get(\"type\"):\n                    div_text = div.get_text(separator=' ', strip=True)\n                    divs_text.append(div_text)\n\n            plain_text = \" \".join(divs_text)\n            self._text = plain_text\n        return self._text","9ce80e2c":"tei = TEIFile(\"..\/input\/tei-xml-files\/tei_xml_files\/0001.tei.xml\")\nf\"The authors of the paper entitled '{tei.title}' are {tei.authors}\"","671d92bc":"tei.abstract","7c311913":"from os.path import basename, splitext\n\ndef basename_without_ext(path):\n    base_name = basename(path)\n    stem, ext = splitext(base_name)\n    if stem.endswith('.tei'):\n        # Return base name without tei file\n        return stem[0:-4]\n    else:\n        return stem\n    \nbasename_without_ext(tei_doc)","d602063c":"def tei_to_csv_entry(tei_file):\n    tei = TEIFile(tei_file)\n    print(f\"Handled {tei_file}\")\n    base_name = basename_without_ext(tei_file)\n    return base_name, tei.doi, tei.title, tei.abstract, tei.authors, tei.text","daac5c81":"tei_to_csv_entry(tei_doc)","a8d6dd2a":"import glob\nfrom pathlib import Path\n\npapers = sorted(Path(\"..\/input\/tei-xml-files\/tei_xml_files\").glob('*.tei.xml'))","712c37bd":"import multiprocessing\nprint(f\"My machine has {multiprocessing.cpu_count()} cores.\")\n\nfrom multiprocessing.pool import Pool\npool = Pool()","c182416a":"csv_entries = pool.map(tei_to_csv_entry, papers)\ncsv_entries","5ed11bf4":"import pandas as pd\n\nresult_csv = pd.DataFrame(csv_entries, columns=['paper_id', 'doi','title', 'abstract','authors','full_text'])\nresult_csv","c163314f":"result_csv.to_csv(\"metadata.csv\", index=False)","16bb1c84":"import asyncio\nimport json\nimport os.path\nimport sys\nfrom pathlib import Path\n\nfrom bs4 import BeautifulSoup\n\n\n# create tag list\ntag2attr = {\n    \"addrline\": None,\n    \"author\": None,\n    \"biblscope\": \"unit\",\n    \"date\": \"when\",\n    \"editor\": None,\n    \"idno\": \"type\",\n    \"note\": None,\n    \"ptr\": \"type\",\n    \"publisher\": None,\n    \"pubplace\": None,\n    \"p\": None,\n    \"title\": \"level\"\n}\n\ntag_list = list(tag2attr.keys())","e30da9b1":"async def fetch_author(auth):\n    \"\"\"\n    Return author attributes in dict\n    :param auth: bs4.BeautifulSoup\n    :return: dict\n    \"\"\"\n    auth_ = {}\n    for name_part in [\"first\", \"middle\"]:\n        if auth.find(\"forename\", {\"type\": name_part}):\n            name = auth.find(\"forename\", {\"type\": name_part}).get_text()\n            auth_.update({name_part: name})\n    for name_part in [\"surname\", \"genname\"]:\n        if auth.find(name_part):\n            name = auth.find(name_part).get_text()\n            auth_.update({name_part: name})\n    return auth_\n\n\n","eec47bbf":"async def fetch_authors(soup):\n    \"\"\"\n    Return the list of author dict\n    :param soup: bs4.element.Tag\n    :return: List[dict]\n    \"\"\"\n    if soup.find_all(\"author\"):\n        authors = [fetch_author(auth) for auth in soup.find_all(\"author\")]\n        return await asyncio.gather(*authors)","3e20e281":"file= \"..\/input\/tei-xml-files\/tei_xml_files\/0001.tei.xml\"\nfetch_author(file)","691e111d":"async def fetch_keywords(soup):\n    \"\"\"Get the keywords\"\"\"\n    if soup.find_all(\"keywords\"):\n        keywords = [term.string.rstrip() for keywords in soup.find_all(\"keywords\") for term in keywords if\n                    (term.string.rstrip())]\n        return keywords\n    else:\n        return []\n\n\nasync def fetch_tag(tag):\n    \"\"\"\n    Return a dict for each tag with k the name of the tag and v the string of the tag\n    :param tag: bs4.element.Tag\n    :return: dict\n    \"\"\"\n    assert tag.name in tag_list\n    attr = tag2attr[tag.name]\n    if attr and tag.string:\n        if tag.name == \"title\":\n            try:\n                return {tag.name + \"_\" + tag[\"type\"] + \"_\" + tag[attr]: tag.string}\n            except KeyError:\n                return {tag.name + \"_\" + tag[attr]: tag.string}\n        elif tag.name == \"date\":\n            try:\n                return {tag.name: tag[attr]}\n            except KeyError:\n                return {tag.name: tag.string}\n        else:\n            try:\n                return {tag[attr]: tag.string}\n            except KeyError:  # handles cases like <idno>Pages 15 - 20<\/idno>\n                return {tag.name: tag.string}\n    else:\n        return {tag.name: tag.text}\n\n\nasync def fetch_all_tags(id_, soup, id_name=\"npl_publn_id\", output_file=None):\n    \"\"\"\n    Return grobid ouptut as a json\n    :param id_: str\n    :param soup: 'bs4.BeautifulSoup'\n    :param id_name: str\n    :return: dict\n    \"\"\"\n    tasks = []\n    for tag_ in tag_list:\n        for tag in soup.find_all(tag_):\n            task = asyncio.create_task(fetch_tag(tag))\n            tasks.append(task)\n    task_authors = asyncio.create_task(fetch_authors(soup))\n    task_keywords = asyncio.create_task(fetch_keywords(soup))\n\n    tasks = await asyncio.gather(*tasks)\n    await task_authors\n    await task_keywords\n\n    cit = {}\n\n    for task in tasks:\n        cit.update(task)\n    cit.update({\"authors\": task_authors.result()})\n    cit.update({\"keywords\": task_keywords.result()})\n    cit.update({id_name: id_})\n    with open(output_file, 'w') as out:\n        json.dump(cit, out)\n\n\ndef get_schema(primary_key=\"npl_publn_id\", pk_type=\"number\"):\n    schema = {\n        \"type\": \"object\",\n        \"properties\": {\n            primary_key: {\"type\": pk_type},\n            \"DOI\": {\"type\": \"string\"},\n            \"ISSN\": {\"type\": \"string\"},\n            \"ISSNe\": {\"type\": \"string\"},\n            \"PMCID\": {\"type\": \"string\"},\n            \"PMID\": {\"type\": \"string\"},\n            \"authors\": {\n                \"type\": \"array\",\n                \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"first\": {\"type\": \"string\"},\n                        \"middle\": {\"type\": \"string\"},\n                        \"surname\": {\"type\": \"string\"},\n                        \"genname\": {\"type\": \"string\"},\n                    },\n                },\n            },\n            \"target\": {\"type\": \"string\"},\n            \"title_j\": {\"type\": \"string\"},\n            \"title_abbrev_j\": {\"type\": \"string\"},\n            \"title_m\": {\"type\": \"string\"},\n            \"title_main_m\": {\"type\": \"string\"},\n            \"title_main_a\": {\"type\": \"string\"},\n            \"year\": {\"type\": \"number\"},\n            \"issue\": {\"type\": \"number\"},\n            \"volume\": {\"type\": \"number\"},\n            \"from\": {\"type\": \"number\"},\n            \"to\": {\"type\": \"number\"},\n            \"issues\": {\"type\": \"array\"}\n            # 'page': {\"type\": \"string\"},\n            # 'type': {\"type\": \"string\"},\n            # 'unit': {\"type\":},\n            # 'when': {\"type\": \"string\"}\n            # 'idno': {\"type\":},\n        },\n        \"required\": [primary_key],\n    }\n    return schema\n\n\ndef process_directory(source_directory, output_directory=None):\n    for root, dirs, files in os.walk(source_directory):\n        for file_ in files:\n            if not file_.lower().endswith(\"tei.xml\"):\n                continue\n\n            try:\n                process_file(root, file_, output_directory)\n            except Exception as e:\n                print(\"Something went wrong. Skipping. \" + str(e))\n                continue\n\n\ndef process_file(root, file_, output_directory=None):\n    abs_path = os.path.join(root, file_)\n    output_file = abs_path.replace(\".tei.xml\", \".json\")\n\n    if output_directory is not None:\n        output_file = os.path.join(output_directory, file_.replace(\".tei.xml\", \".json\"))\n\n    with open(abs_path, 'r') as tei:\n        soup = BeautifulSoup(tei, 'lxml')\n        # event_loop = asyncio.get_event_loop()\n        asyncio.run(fetch_all_tags(str(file_), soup, output_file=output_file))\n\n\nif __name__ == '__main__':\n    if len(sys.argv) < 2 or len(sys.argv) > 3:\n        print(\"Invalid parameters. Usage: python parse_grobid.py source output_directory. \"\n              \"The source can be either a directory or a file. \")\n        sys.exit(-1)\n\ninput = sys.argv[1]\noutput = None\nif len(sys.argv) == 3:\n    output = sys.argv[2]\n\nif os.path.isdir(input):\n    input_path = Path(input)\n    process_directory(input_path)\n\nelif os.path.isfile(input):\n    input_path = Path(input)\n    file = input_path.name\n    root = str(input_path.parent)\n    process_file(root, file)\n","8d6efcc9":"Next, we can apply tei_to_csv_entry() to multiple papers by, firstly, selecting all TEI XML documents:","9339ef63":"**Interactive XML parsing with Beautiful Soup** First, we interactively parse a TEI XML document using Beautiful Soup. Then, we map a TEI document to a Python object allowing us to systemically retrieve publication\u2019s information with just two lines of Python code. We use this implementation to generate a pandas data frame and to store the extracted metadata as a CSV.\n\nhttps:\/\/komax.github.io\/blog\/text\/python\/xml\/parsing_tei_xml_python\/","98eb6dfc":"**Data mapping TEI to python objects**\nIn the previous section, we manually extracted information from a publication. We reuse this code to programmatically extract textual information in a TEI XML document. First of all, we define a function to transform a TEI document into a BeautitfulSoup:","68bf4acc":"***Extract metadata from PDFs usinf the S2ORC pipeline***\nThis approach will extract words from pdfs so that we can place information such as the title of research, authors and institution, doi #, PubMedID#, abstract body, into a csv file.\n\n1.Process PDFs and LATEX sources to derive\nmetadata, clean full text, inline citations and\nreferences, and bibliography entries\n2. Select the best metadata and full text parses\nfor each paper cluster,\n4971\n3. Filter paper clusters with insufficient metadata or content\n4. Resolve bibliography links between paper\n","4baa7be0":"# ***Dataset description***\n\n\nThere are 1331 articles on response to increased temperature in tropical reef fishes, 2205 articles on acclimation in tropical fish, 43 articles on molecular response to microplastic in fishes, 103 articles on microplastic exposure in fishes, and 369 on transgenerational acclimation in fish, and 384 papers gene expression associated with acclimation reef fish on PMC; 23 articles on response to \"increased temperature\" \"tropical reef fish\", 451 articles on \"acclimation\" in \"tropical fish\", 18 articles on \"molecular response\" to microplastic in \"fish\", 104 articles on microplastic exposure in tropical reef fishes, 33 papers on \"gene expression\" associated with \"acclimation\" \"reef fish\", and 34 on \"transgenerational acclimation\" \"fish\" on Science Direct by Elsevier; and 12 under \"mircoplastics\" on the Austrialian Insitute of Marine Science publication repository. \n\n\nThis dataset is provided to the global research community to apply recent advances in natural lanuage processing and other AI techniques to generate new insights in support of the ongoing threat posed by marine pollution. There is growing urgency for these approaches because the rapid acceleration of microplastics concentration in marine systems threatens fishery species, and if unresolved, could contaminate fisheries and pose a serious threat to human health.","d990a7ab":"We store the results as comma separated values (CSV) and we are done!","8037b5a3":"**Converted PDFs to text**\nI converted PDFs to text by download and running grobid and grobid- client. This Python client can be used to process in an efficient concurrent manner a set of PDFs in a given directory by the GROBID service. Results are written in a given output directory and include the resulting XML TEI representation of the PDF.\n\ngit clone https:\/\/github.com\/kermitt2\/grobid-client-python\n\nrun from \/grobid-installation","7efa2bf4":"***Table of contents***\n\n1. Get pdfs from PubMed Central and Science Direct by Elsevier \n    - PMC-Batch-Download from DOIs\n2. Convert PDFs to tei xml \n3. Map TEI to python objects \n    - obtain metadata\n4. Parse tei into JSON format\n    https:\/\/github.com\/allenai\/s2orc-doc2json\n5. Dataset description\n","984265a6":"Then we build a pandas DataFrame from these entries.","5b90bad5":"   ***Pubmed Batch Download***\nI downloaded all PMCIDs that arose from the search terms then used the scidownl package to get artciles from their doi numbers.","4272c5f1":"Secondly, we setup a thread pool which can handle multiple papers simultaneously (limited by the number of CPUs in your machine):","17230793":"**Parse TEI to JSON**\n\nFirst, read the xml tree with bs4, then find the elements (title\/authors\/etc) with bs4.find \/ bs4.find_all, and manually construct a dictionary with the desired structure\/content. \n\n\nThe two approaches I am testing here are adapted from https:\/\/github.com\/kermitt2\/grobid-client-python\/blob\/master\/grobid-client.py and https:\/\/github.com\/cverluise\/PatCit\n","81dc4932":"***These steps are from Lo et al. 2020***\n\n***Extracting structured data from PDFs*** \n- from TEI XML output of GROBID extract \n    - title and authors from each PDF (metadata) \n    - paragraphs from the body text organized under section headings, \n    - figure and table captions, \n    - equations, table content, headers, and footers, which we remove from the body text, \n    - inline citations in the abstract and body text, \n    - parsed bibliography entries with title, authors, year, and venue identified, and \n    - links between inline citation mentions and their corresponding bibliography entries.","0d2d82c4":" # Constructing a data frame\nA TEIFile enables us to handle multiple files at a time and build a table- like structure. For this purpose, we apply our TEIFile implementation to a handful TEI XML. First, we will write a function tei_to_csv_entry() which captures the paper\u2019s title, doi and abstract. Then, we we will use this function to build a data frame wherein each row represents an output from tei_to_csv_entry(). Eventually, we dump the results as CSV.\n\nIn addition to the extracted text, it\u2019s essential and good practice to have a unique identifier for each paper. To this end, we define a helper function base_name_without_ext() outputting the paper\u2019s filename without path and file type extension:","148bfcc6":"These pdf_parses\/ are supplementary to the metadata\/ entries. PDF parses are represented as JSONlines file (one line per paper) with the following keys:\n\npaper_id: a str-valued field which is the same S2ORC paper ID in metadata\/\n\n_pdf_hash: a str-valued field. Internal usage only. We use this for debugging.\n\nabstract and body_text are List[Dict]-valued fields representing parsed text from the PDF. Each Dict corresponds to a paragraph. List preserves their original ordering.\n\nbib_entries and ref_entries are Dict-valued fields representing extracted entities that can be referenced (inline) within the text.","3b9d5d0c":"Output the paper\u2019s title, doi and abstract as a tuple."}}