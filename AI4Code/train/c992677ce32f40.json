{"cell_type":{"cf97701f":"code","c74df2d3":"code","ac6997c0":"code","8304cf25":"code","27fb2062":"code","2e057589":"code","0e4db704":"code","b12d73b6":"code","8fb36ade":"code","abbba9b0":"code","9083b6bd":"code","329c333c":"code","ce961404":"code","b469b39e":"code","86a5ad89":"code","5ee91acf":"code","85b59129":"code","fff43241":"code","ed409144":"code","abfd6abb":"code","acb84eea":"code","21d221da":"code","e0bb2806":"markdown","e697d9f7":"markdown","3d221b1b":"markdown","8dfa4fa1":"markdown"},"source":{"cf97701f":"# Import basic libraries \nimport pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt        \n%matplotlib inline\nimport seaborn as sn\nfrom scipy import stats\n\nimport sklearn.metrics as metrics\nimport sklearn.preprocessing as skp\nimport sklearn.model_selection as skm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n#import classification modules\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.model_selection import RandomizedSearchCV as rs\n\n# first neural network with keras tutorial\nfrom sklearn.metrics import accuracy_score,precision_score, recall_score, roc_auc_score,roc_curve ","c74df2d3":"final = pd.read_csv(\"\/home\/amir\/DataScience\/Module 3\/Final Exam\/hepatitis.csv\")","ac6997c0":"final","8304cf25":"print(\"Shape of The Data:\",final.shape)\nprint(\"Null Values:\\n\",final.isnull().sum())\nprint(\"Data Types:\\n\",final.dtypes)\nprint(\"Unique Values:\\n\",final.nunique())","27fb2062":"col = ['class','sex','steroid','antivirals','fatigue','malaise','anorexia','liver_big','liver_firm','spleen_palable','spiders','ascites','varices','histology']\n\nfor i in col:\n    final[i]= final[i].astype(str)\nfinal.dtypes","2e057589":"BOLD = '\\033[1m'\nEND = '\\033[0m'\nnumcols = final.select_dtypes(include=np.number)\n    \nfor col in numcols:\n\n    fig, (ax1, ax2) = plt.subplots(1,2,figsize=(12,3))\n    \n    sn.boxplot(final[col], linewidth=1, ax = ax1)\n    \n    final[col].hist(ax = ax2)\n\n    plt.tight_layout()\n    \n    plt.show()\n    \n    print(BOLD+col.center(115)+END)","0e4db704":"stringcols = final.select_dtypes(exclude=np.number)\n\nfig = plt.figure(figsize = (8,10))\n\nfor i,col in enumerate(stringcols):\n    \n    fig.add_subplot(6,3,i+1)\n    \n    final[col].value_counts().plot(kind = 'barh' ,fontsize=10)\n    \n    plt.tight_layout()\n    \n    plt.title(col)     \n","b12d73b6":"plt.figure(figsize=(12, 10))\n\ncorr = final.select_dtypes(include=np.number).corr()\n\nax = sn.heatmap(corr,vmin=-1, vmax=1, center=0,square=True, annot = True)\n\nax.set_xticklabels(ax.get_xticklabels(),rotation=45,horizontalalignment='right');\n\nax.set_yticklabels(ax.get_yticklabels(),rotation=0,horizontalalignment='right');","8fb36ade":"colz = ['protime']\ncol_zscore = np.abs(stats.zscore(final[colz]))\ncol_zscore\n","abbba9b0":"out_z = np.where(col_zscore>3)\nout_z","9083b6bd":"final = final[(col_zscore < 3).all(axis =1)]\nfinal","329c333c":"final = pd.get_dummies(final,drop_first = True)\nfinal.dtypes","ce961404":"#Separating the label\ny = final['class_2'].copy()\n\n#Separating the features\nX = final.drop('class_2',axis=1)\nprint(\"Shape of X: \", X.shape)\nprint(\"Shape of y: \", y.shape)\n\nfeatures = X.columns\nfeatures","b469b39e":"X = skp.StandardScaler().fit(X).transform(X)  \nX","86a5ad89":"#create train-test split parts for manual split\ntrainX, testX, trainy, testy= skm.train_test_split(X, y, test_size=0.25, random_state=99)\nprint(\"\\n Shape of train split: \")\nprint(trainX.shape, trainy.shape)\nprint(\"\\n Shape of test split: \")\nprint(testX.shape, testy.shape)","5ee91acf":"#Random Forest  \n  \nclf = RandomForestClassifier(n_estimators = 200)\nclf.fit(trainX,trainy)\n    \nfeature_imp = clf.feature_importances_\nfeature_imp","85b59129":"feature_array = np.array(features)\nfeature_array","fff43241":"zipfeatureimp = pd.DataFrame(list(zip(feature_array,feature_imp)))\n\nprint(zipfeatureimp.sort_values(by = 1,ascending = False))","ed409144":"#AdaBoost with Randomize Search\n\nparameters = {'n_estimators':(10,200,10),'learning_rate':range(1,20,1)}\n\nclf_tree = AdaBoostClassifier()\nclf =rs(clf_tree,parameters,cv =5, scoring = 'precision')\nclf.fit(trainX,trainy)\n\npredictions = clf.predict(testX)\nfpr1 , tpr1, _ = roc_curve(testy,predictions)\n    \nprint(\" accuracy of AdaBoost Classifier (%)\", accuracy_score(testy,predictions)*100)\n\nprint(\"Precision of AdaBoost Classifierr (%)\",precision_score(testy,predictions)*100)\n\nprint(\"Recall of AdaBoost Classifierr (%)\",recall_score(testy,predictions)*100)\n\nprint(\"Best score (%)\",clf.best_score_*100)\n\nprint(\"Best Parameters (%)\",clf.best_params_)\n\nprint(\"Best Estimator (%)\",clf.best_estimator_)\n","abfd6abb":"#Decision Tree with Randomize Search\n\nparameters = {'min_samples_split' : range(10,200,10), 'max_depth' : range(1,20,1),'random_state':(1,20,1)}\n\nclf_tree = DecisionTreeClassifier()\nclf =rs(clf_tree,parameters,cv =5, scoring = 'precision')\nclf.fit(trainX,trainy)\n\npredictions = clf.predict(testX)\nfpr2 , tpr2, _ = roc_curve(testy,predictions)\n    \nprint(\" accuracy of Decision Tree Classifier (%)\", accuracy_score(testy,predictions)*100)\n\nprint(\"Precision of Decision Tree Classifierr (%)\",precision_score(testy,predictions)*100)\n\nprint(\"Recall of Decision Tree Classifierr (%)\",recall_score(testy,predictions)*100)\n\nprint(\"Best score (%)\",clf.best_score_*100)\n\nprint(\"Best Parameters (%)\",clf.best_params_)\n\nprint(\"Best Estimator (%)\",clf.best_estimator_)\n","acb84eea":"#GradientBoostingClassifier with Randomize Search\n\nparameters = {'n_estimators':(10,200,10), 'max_depth':(1,20,1), 'random_state':(1,20,1)}\n\nclf_tree = GradientBoostingClassifier()\nclf =rs(clf_tree,parameters, scoring = 'precision')\nclf.fit(trainX,trainy)\n    \npredictions = clf.predict(testX)\nfpr3 , tpr3, _ = roc_curve(testy,predictions)\n\nprint(\" accuracy of Gradient Boosting Classifier (%)\", accuracy_score(testy,predictions)*100)\n\nprint(\"Precision of Gradient Boosting Classifierr (%)\",precision_score(testy,predictions)*100)\n\nprint(\"Recall of Gradient Boosting Classifierr (%)\",recall_score(testy,predictions)*100)\n\nprint(\"Best score (%)\",clf.best_score_*100)\n\nprint(\"Best Parameters (%)\",clf.best_params_)\n\nprint(\"Best Estimator (%)\",clf.best_estimator_)\n","21d221da":"roc_auc1 = metrics.auc(fpr1 ,tpr1)\nroc_auc2 = metrics.auc(fpr2 ,tpr2)\nroc_auc3 = metrics.auc(fpr3 ,tpr3)\n\nplt.figure()\nplt.title('Recevier Operating Chatactersticks')\nplt.plot(fpr1 , tpr1 , 'b', label ='roc default auc = %0.2f' % roc_auc1)\nplt.plot(fpr2 , tpr2 , 'r', label ='roc grid search auc = %0.2f' % roc_auc2)\nplt.plot(fpr3 , tpr3 , 'g', label ='roc random search auc = %0.2f' % roc_auc3)\nplt.legend(loc = 'lower right')\nplt.plot([0 , 1],[0,1],'r--')\nplt.xlim([0 , 1])\nplt.ylim([0 , 1])\nplt.ylabel('True Positive rate')\nplt.xlabel('False Positive rate')\nplt.show()","e0bb2806":"# Many Columns Have Value Yes And No \n\nColumns have change into Strings, it is easy to make dummies","e697d9f7":"# Problem: To detect patient will live or die ","3d221b1b":"# Numerical And Categorical Columns Analysis","8dfa4fa1":"# Removing Outliers using zscore"}}