{"cell_type":{"df1515d7":"code","e3db5805":"code","78f08893":"code","b1ea0efe":"code","a833a0c9":"code","b3471055":"code","75c06580":"code","9907a18d":"code","8ad5e6e0":"code","3c8bf375":"code","7f7da40f":"code","630ee724":"code","f0cfd6f4":"code","75de6741":"code","dd8e0a91":"code","78712f38":"code","5f0c67f8":"code","2dcbcdae":"code","083dd8ff":"code","e3305d9b":"code","bd763836":"code","263af506":"code","75fe48e9":"code","e6428b2a":"code","79459f2d":"code","3172c410":"code","fcdf479b":"code","ecb65d7b":"code","ff9a2c46":"code","54fecd81":"code","accc17f2":"code","16a971ad":"code","113f620e":"code","0c206ae8":"code","da98e255":"code","6cc5d8cc":"code","e20343f4":"code","19f9a0c0":"markdown","968c32dc":"markdown","4b656818":"markdown","97bcc711":"markdown","eed04c22":"markdown","ebe9f40f":"markdown","fa767388":"markdown","3dd3b52f":"markdown","100847e9":"markdown","bc403433":"markdown","3feafd4a":"markdown","bd26841a":"markdown","38617621":"markdown","0c54d9ec":"markdown","6b8f86f2":"markdown","cb762c00":"markdown"},"source":{"df1515d7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport random \n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.metrics import classification_report\nimport os\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n        \n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e3db5805":"data=pd.read_csv(\"\/kaggle\/input\/entity-annotated-corpus\/ner_dataset.csv\",encoding=\"latin1\")\ndata.head()","78f08893":"data['Sentence #']=data['Sentence #'].ffill(axis = 0) \ndata.head()","b1ea0efe":"agg_func = lambda s: [(w,p, t) for w,p, t in zip(s[\"Word\"].values.tolist(),\n                                                       s['POS'].values.tolist(),\n                                                        s[\"Tag\"].values.tolist())]","a833a0c9":"agg_data=data.groupby(['Sentence #']).apply(agg_func).reset_index().rename(columns={0:'Sentence_POS_Tag_Pair'})\nagg_data.head()","b3471055":"agg_data['Sentence']=agg_data['Sentence_POS_Tag_Pair'].apply(lambda sentence:\" \".join([s[0] for s in sentence]))\nagg_data['POS']=agg_data['Sentence_POS_Tag_Pair'].apply(lambda sentence:\" \".join([s[1] for s in sentence]))\nagg_data['Tag']=agg_data['Sentence_POS_Tag_Pair'].apply(lambda sentence:\" \".join([s[2] for s in sentence]))","75c06580":"agg_data.shape","9907a18d":"agg_data.head()","8ad5e6e0":"agg_data['tokenised_sentences']=agg_data['Sentence'].apply(lambda x:x.split())\nagg_data['tag_list']=agg_data['Tag'].apply(lambda x:x.split())\nagg_data.head()","3c8bf375":"agg_data['len_sentence']=agg_data['tokenised_sentences'].apply(lambda x:len(x))\nagg_data['len_tag']=agg_data['tag_list'].apply(lambda x:len(x))\nagg_data['is_equal']=agg_data.apply(lambda row:1 if row['len_sentence']==row['len_tag'] else 0,axis=1)\nagg_data['is_equal'].value_counts()","7f7da40f":"agg_data=agg_data[agg_data['is_equal']!=0]","630ee724":"agg_data.shape","f0cfd6f4":"sentences_list=agg_data['Sentence'].tolist()\ntags_list=agg_data['tag_list'].tolist()\n\nprint(\"Number of Sentences in the Data \",len(sentences_list))\nprint(\"Are number of Sentences and Tag list equal \",len(sentences_list)==len(tags_list))","75de6741":"tags_list[0]","dd8e0a91":"tokeniser= tf.keras.preprocessing.text.Tokenizer(lower=False,filters='')\n\ntokeniser.fit_on_texts(sentences_list)\n","78712f38":"print(\"Vocab size of Tokeniser \",len(tokeniser.word_index)+1) ## Adding one since 0 is reserved for padding","5f0c67f8":"tokeniser.index_word[326]","2dcbcdae":"encoded_sentence=tokeniser.texts_to_sequences(sentences_list)\nprint(\"First Original Sentence \",sentences_list[0])\nprint(\"First Encoded Sentence \",encoded_sentence[0])\nprint(\"Is Length of Original Sentence Same as Encoded Sentence \",len(sentences_list[0].split())==len(encoded_sentence[0]))\nprint(\"Length of First Sentence \",len(encoded_sentence[0]))\n","083dd8ff":"tags=list(set(data['Tag'].values))\nprint(tags)\nnum_tags=len(tags)\nprint(\"Number of Tags \",num_tags)\n\ntags_map={tag:i for i,tag in enumerate(tags)}\nprint(\"Tags Map \",tags_map)","e3305d9b":"reverse_tag_map={v: k for k, v in tags_map.items()}","bd763836":"encoded_tags=[[tags_map[w] for w in tag] for tag in tags_list]\nprint(\"First Sentence \",sentences_list[0])\nprint('First Sentence Original Tags ',tags_list[0])\nprint(\"First Sentence Encoded Tags \",encoded_tags[0])\nprint(\"Is length of Original Tags and Encoded Tags same \",len(tags_list[0])==len(encoded_tags[0]))\nprint(\"Length of Tags for First Sentence \",len(encoded_tags[0]))","263af506":"max_sentence_length=max([len(s.split()) for s in sentences_list])\nprint(max_sentence_length)","75fe48e9":"max_len=128\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\n\npadded_encoded_sentences=pad_sequences(maxlen=max_len,sequences=encoded_sentence,padding=\"post\",value=0)\npadded_encoded_tags=pad_sequences(maxlen=max_len,sequences=encoded_tags,padding=\"post\",value=tags_map['O'])\n\nprint(\"Shape of Encoded Sentence \",padded_encoded_sentences.shape)\nprint(\"Shape of Encoded Labels \",padded_encoded_tags.shape)\n\nprint(\"First Encoded Sentence Without Padding \",encoded_sentence[0])\nprint(\"First Encoded Sentence with padding \",padded_encoded_sentences[0])\nprint(\"First Sentence Encoded Label without Padding \",encoded_tags[0])\nprint(\"First Sentence Encoded Label with Padding \",padded_encoded_tags[0])","e6428b2a":"target= [to_categorical(i,num_classes = num_tags) for i in  padded_encoded_tags]\nprint(\"Shape of Labels  after converting to Categorical for first sentence \",target[0].shape)","79459f2d":"from sklearn.model_selection import train_test_split\nX_train,X_val_test,y_train,y_val_test = train_test_split(padded_encoded_sentences,target,test_size = 0.3,random_state=42)\nX_val,X_test,y_val,y_test = train_test_split(X_val_test,y_val_test,test_size = 0.2,random_state=42)\nprint(\"Input Train Data Shape \",X_train.shape)\nprint(\"Train Labels Length \",len(y_train))\nprint(\"Input Test Data Shape \",X_test.shape)\nprint(\"Test Labels Length \",len(y_test))\n\nprint(\"Input Validation Data Shape \",X_val.shape)\nprint(\"Validation Labels Length \",len(y_val))","3172c410":"#print(\"First Sentence in Training Data \",X_train[0])\n#print(\"First sentence Label \",y_train[0])\nprint(\"Shape of First Sentence -Train\",X_train[0].shape)\nprint(\"Shape of First Sentence Label  -Train\",y_train[0].shape)","fcdf479b":"from tensorflow.keras import Model,Input\nfrom tensorflow.keras.layers import LSTM,Embedding,Dense\nfrom tensorflow.keras.layers import TimeDistributed, SpatialDropout1D,Bidirectional","ecb65d7b":"embedding_dim=128\nvocab_size=len(tokeniser.word_index)+1\nlstm_units=128\nmax_len=128\n\ninput_word = Input(shape = (max_len,))\nmodel = Embedding(input_dim = vocab_size+1,output_dim = embedding_dim,input_length = max_len)(input_word)\n\nmodel = LSTM(units=embedding_dim,return_sequences=True)(model)\nout = TimeDistributed(Dense(num_tags,activation = 'softmax'))(model)\nmodel = Model(input_word,out)\nmodel.summary()","ff9a2c46":"model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics = ['accuracy'])","54fecd81":"history = model.fit(X_train,np.array(y_train),validation_data=(X_val,np.array(y_val)),batch_size = 32,epochs = 3)\n","accc17f2":"preds=model.predict(X_test) ## Predict using model on Test Data","16a971ad":"\ndef evaluatePredictions(test_data,preds,actual_preds):\n    print(\"Shape of Test Data Array\",test_data.shape)\n    y_actual=np.argmax(np.array(actual_preds),axis=2)\n    y_pred=np.argmax(preds,axis=2)\n    num_test_data=test_data.shape[0]\n    print(\"Number of Test Data Points \",num_test_data)\n    data=pd.DataFrame()\n    df_list=[]\n    for i in range(num_test_data):\n        test_str=list(test_data[i])\n        df=pd.DataFrame()\n        df['test_tokens']=test_str\n        df['tokens']=df['test_tokens'].apply(lambda x:tokeniser.index_word[x] if x!=0 else '<PAD>')\n        df['actual_target_index']=list(y_actual[i])\n        df['pred_target_index']=list(y_pred[i])\n        df['actual_target_tag']=df['actual_target_index'].apply(lambda x:reverse_tag_map[x])\n        df['pred_target_tag']=df['pred_target_index'].apply(lambda x:reverse_tag_map[x])\n        df['id']=i+1\n        df_list.append(df)\n    data=pd.concat(df_list)\n    pred_data=data[data['tokens']!='<PAD>']\n    accuracy=pred_data[pred_data['actual_target_tag']==pred_data['pred_target_tag']].shape[0]\/pred_data.shape[0]\n    \n    \n    return pred_data,accuracy\n        ","113f620e":"pred_data,accuracy=evaluatePredictions(X_test,preds,y_test)","0c206ae8":"y_pred=pred_data['pred_target_tag'].tolist()\ny_actual=pred_data['actual_target_tag'].tolist()","da98e255":"print(classification_report(y_actual,y_pred))","6cc5d8cc":"pred_data[pred_data['actual_target_tag']==\"B-art\"]","e20343f4":"pred_data[pred_data['actual_target_tag']==\"B-nat\"]","19f9a0c0":"<a name=\"Introduction\"><\/a>\n# Introduction\n\nNER is a subtask of information extraction that locates and classifies named entities in a text. The named entities could be organizations, persons, locations, times, etc. \n\n**For example:**\n\nCOVID-19 has ensured low key August  15    celebrations in India\n O        O   O       O   O   B-time I-time  O.          O. B-geo\nIs labeled as follows: \n\n- August 15 - time indicator\n- India - geographic indicator\n\nEverything else that is labeled with an `O` is not considered to be a named entity. ","968c32dc":"### Let us calculate the precision and recall of each Class - and also the F-Score","4b656818":"As we can see in the tags map above, every entity has a prefix B- or I-. B- denotes the **beginning** and I- **inside** of an entity. The prefixes are used to detect multiword entities.","97bcc711":"<a name=\"Conclusion and Future Steps\"><\/a>\n\n## How to Improve the Model\n\n- Currently the model looks only at the previous words to predict the tag and not the words after it. This also can be one reason why I-GPE Tags are not classified properly.To avoid this we can use an Bidirectional LSTM\n\n- Using pre-trained word Embeddings can also help improve the model. \n\n- Using BERT like models for Fine-Tuning\n\n- We can also make changes to the model hyperparameters like epochs, number of LSTM Units, Activation Function used etc\n\n\n","eed04c22":"Also, while the model has learnt to classify words as B-GPE, it has failed to understand I-GPE Words. For example in African American => it classifies African as B-GPE but classifies American as B-ORG instead of I-GPE","ebe9f40f":"<a name=\"Model Evaluation\"><\/a>\n\n## Evaluating the Model\n\nTo evaluate the model, we will have to remove the padded portion and identify the accuracy. For this, for every test data, let us create a dataframe wuth the tokens and the actual and predicted value - and use it to calculate the metrics","fa767388":"<a name=\"Tokenising the Sentence\"><\/a>\n\n### Tokenising the sentences\n\n- We have to create a vocab of words and then each word should be assigned an unique identifier. Keras provides an Tokeniser API, which can be used for this purpose. It will both tokenise and encode sentences. \n\n- Also, since different sentences are of different length, we will have to pad the input sequence to the largest sentence length.Keras provides pad_sequences function to achieve the same","3dd3b52f":"<a name=\"Model Building Using LSTM\"><\/a>\n\n## Building the Model using LSTM","100847e9":"We have to create a list of sentences and list of NER Tags associated with each sentence","bc403433":"The model has failed to learn tags associated with Work of Art. For a Few cases where there is word like \"France\" or \"Washington\", it has classified it as a place.","3feafd4a":"<a name=\"Splitting Data into Train, Test and Validation\"><\/a>\n\n### Splitting the data into Train,Test and Validation Sets","bd26841a":"\n#### Using Tokeniser To Encode the Sentences","38617621":"#### Creating Tag Mapping and Encoding the Tags","0c54d9ec":"<a name=\"Exploring Data\"><\/a>\n# Exploring the Dataset\n\nIn the dataset provided on Kaggle consists of four columns, the sentence number, the word, the part of speech of the word, and the tag associated with each word.\nWe will have to preprocess the dataset, in such a way that every sentence is one row and Tokenise and Encode \n\n","6b8f86f2":"From the histogram, we can see that 50 will be a good number for sequence length and 104 is the maximum sequence length. Instead of using 104 as the sequence length, let us choose the nearest power of two as the sequence length - in this case it is 128\n\nWe now have to pad the encoded sentences and encoded tags . For the Encoded Sentences we can use 0 as the padding token and for Tags, the token index of 'O' can be used ","cb762c00":"#### Understanding the Length of Sentences - Identifying appropriate SEQ LENGTH"}}