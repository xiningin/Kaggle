{"cell_type":{"f2e50048":"code","343367a8":"code","4309a674":"code","738d4f1d":"code","ce40a46b":"code","9e9c8f15":"code","0e845b46":"code","0d667f65":"code","ac897512":"code","fb6985b1":"code","8b9ff76c":"code","8e5673e6":"code","e568fdab":"code","722a6d93":"code","674e076d":"code","4605745b":"code","20778bb8":"code","815c142c":"code","da907e33":"code","b369ea7d":"code","ce8e9711":"code","cf575417":"code","f2dc4faa":"code","36fbc687":"code","8b13a566":"code","bb87f371":"code","c4acc962":"markdown","e7739a19":"markdown","6721993e":"markdown","44e72a20":"markdown","c06920d6":"markdown","5d885660":"markdown","6e593d5f":"markdown","cf801984":"markdown","cd950850":"markdown","94e512fe":"markdown","6557da9b":"markdown","587f7d01":"markdown","3dad1609":"markdown"},"source":{"f2e50048":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport datetime\nimport gc\nimport time\nimport warnings\nfrom itertools import chain\n\nimport lightgbm as lgb\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom matplotlib import pyplot as plt\nfrom sklearn.ensemble import IsolationForest, VotingClassifier\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder\nwarnings.filterwarnings(\"ignore\")\nimport os","343367a8":"from kaggle.competitions import twosigmanews\nenv = twosigmanews.make_env()","4309a674":"(marketdata, news) = env.get_training_data()","738d4f1d":"marketdata.head(1)","ce40a46b":"marketdata[\"time\"] = marketdata[\"time\"].dt.date\nmarketdata.rename(columns={\"time\": \"date\"}, inplace=True)","9e9c8f15":"marketdata.shape","0e845b46":"len(marketdata[marketdata.assetCode=='A.N']) #Number of Days","0d667f65":"len(news.time) #number of news","ac897512":"marketdata.describe()","fb6985b1":"marketdata.isna().sum()","8b9ff76c":"marketdata[\"returnsClosePrevMktres1\"].fillna(marketdata[\"returnsClosePrevRaw1\"], inplace=True)\nmarketdata[\"returnsOpenPrevMktres1\"].fillna(marketdata[\"returnsOpenPrevRaw1\"], inplace=True)\nmarketdata[\"returnsClosePrevMktres10\"].fillna(marketdata[\"returnsClosePrevRaw10\"], inplace=True)\nmarketdata[\"returnsOpenPrevMktres10\"].fillna(marketdata[\"returnsOpenPrevRaw10\"], inplace=True)\nprint(marketdata.isna().sum())","8e5673e6":"returns = marketdata[\"close\"].values \/ marketdata[\"open\"].values\noutliers = ((returns > 1.5).astype(int) + (returns < 0.5).astype(int)).astype(bool)\nmarketdata = marketdata.loc[~outliers, :]","e568fdab":"marketdata.shape","722a6d93":"marketdata.describe()","674e076d":"marketdata.sort_values('returnsOpenPrevRaw1',ascending=False)[:5]","4605745b":"#We can check for the stocks above and the days above\nmarketdata[(marketdata.assetCode=='EXH.N') & (marketdata.date==pd.to_datetime(\"2007-8-23\").date())]","20778bb8":"return_columns = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1', 'returnsOpenPrevMktres1','returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\nfor i in return_columns:\n    returns = marketdata[i].values\n    outliers = ((returns > 1).astype(int) + (returns < -0.7).astype(int)).astype(bool)\n    marketdata = marketdata.loc[~outliers, :]\nmarketdata.shape","815c142c":"del returns\ndel outliers","da907e33":"marketdata.describe()","b369ea7d":"marketdata = marketdata.loc[marketdata[\"assetName\"] != \"Unknown\", :]","ce8e9711":"plt.figure(figsize=(10,5))\nplt.plot(marketdata.groupby('date').returnsOpenPrevRaw10.mean().index,marketdata.groupby('date').returnsOpenPrevRaw10.mean().values,color='green')\nplt.title(\"Mean 10 Day Returns\")\nplt.show()\n\nplt.figure(figsize=(10,5))\nplt.plot(marketdata.groupby('date').returnsOpenPrevRaw1.mean().index,marketdata.groupby('date').returnsOpenPrevRaw1.mean().values,color='brown')\nplt.title(\"Mean 1 Day Returns\")\nplt.show()","cf575417":"for asset in np.random.choice(marketdata['assetName'].unique(), 5):\n    asset_df = marketdata[(marketdata['assetName'] == asset)]\n    plt.figure(figsize=(10,5))\n    plt.plot(asset_df.date,asset_df.returnsOpenPrevRaw1,color=\"blue\")\n    plt.title(asset)\n    plt.show()","f2dc4faa":"del asset_df\ngc.collect()","36fbc687":"news.isna().sum() # no NA","8b13a566":"news.sample(2)","bb87f371":"news[\"sourceTimestamp\"] = news[\"sourceTimestamp\"].dt.date #Convert time to date\nnews.rename(columns={\"sourceTimestamp\": \"date\"}, inplace=True) #Rename accurately\n#Normalize the location of the mentioning in word and sentence counts\nnews[\"realfirstMentionPos\"] = news[\"firstMentionSentence\"].values \/ news[\"sentenceCount\"].values\nnews[\"realSentimentWordCount\"] = news[\"sentimentWordCount\"].values \/ news[\"wordCount\"].values\n#Normalization Continues\nnews[\"realSentenceCount\"] = news.groupby([\"date\"])[\"sentenceCount\"].transform(lambda x: (x - x.mean()) \/ x.std())\nnews[\"realWordCount\"] = news.groupby([\"date\"])[\"wordCount\"].transform(lambda x: (x - x.mean()) \/ x.std())\nnews[\"realBodySize\"] = news.groupby([\"date\"])[\"bodySize\"].transform(lambda x: (x - x.mean()) \/ x.std())\n","c4acc962":"By this way, we come to an end to exploratory data analysis. We cleaned the data, added necessary variables for analysis and looked at the insights from data. A more detailed analysis can be made easily, but this kernel aims to provide a simple EDA for everyone to understand!","e7739a19":"As we can see, there are NA's in the market raw return data,which can be replaced with adjusted return data. By applying this replacement, we will get rid of N\/A's, which will give freedom to us in EDA. (Exploratory Data Analysis)","6721993e":"Also, let's drop the rows which has unknown Asset Name, which is not related with the news data.","44e72a20":"To clean the news data, we have to convert time to date again. Also, each sentence and paragraph has different length. The position of an asset in the given news can not be analyzed by the given method. We have to normalize it to understand the exact position of the mention of asset in the news.","c06920d6":"Let's plot the data and see if it is clean, are there outliers or any other things to fix. Also, plotting for some individual assets will make us realize the market trends too. (PLOT THESE)","5d885660":"If we dig into data deeper, where the variable returnsOpenPrevRaw1 has extremely large values, yesterday's data is not available. So, we need to remove those data which is extremely large, which has excessive returns for 1 and 10 day returns. ","6e593d5f":"Let's investigate the data first. What are the variables, how does the data look at first glance, how many observations are there, etc. An essential thing we have to figure out before visually observing the data is N\/A values. ","cf801984":"Now, we can see that there are no null values in the marketdata. As we looked at the describe function above, there is one thing that was obvious about the dataset. The max returns for the data, was way more higher than expected. For instance, the max value for the variable \"returnsOpenPrevRaw1\", which is the 1 day raw return open-open, 9209. Clearly, there is some discrepancy there, and we have to understand the size of how many outliers are there, and remove them if it is necessary.","cd950850":"Let's fix the time format as date to perform operations easier.","94e512fe":"Let's move on to News Data.","6557da9b":"**EDA**\n\nThis kernel explores the data in Two Sigma Competition. There is no model provided for prediction. ","587f7d01":"This plot is inspired from Andrew Lukyanenko's \"EDA, feature engineering and everything\" notebook. In this plot, we can see the effect of crisis happened prior to 2010.\n","3dad1609":"First, we start with importing the data, as explained in the competition. There are two datasets, \n1. Market Data, stock prices and relative information per day.\n1. News Data, Reuters news data that we need to combine with stock prices for predictive analysis."}}