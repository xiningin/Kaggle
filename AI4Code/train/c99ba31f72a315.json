{"cell_type":{"bf072f53":"code","18872ff5":"code","e09bae74":"code","44efded4":"code","0a2a7d06":"code","300cf779":"code","9d3712c7":"code","ff7f7264":"code","028a142d":"code","7bdcefe2":"code","4fb6d8ac":"code","78be1228":"code","bf5f2e7c":"code","893a325a":"code","ee4fcf23":"code","7651a3a7":"code","e5914dac":"code","efc73fdc":"code","93e7ff82":"code","0e1737e7":"code","69523e68":"code","168d361b":"code","c90290ad":"code","375d6b09":"code","53fd069f":"code","cf34a063":"code","7133bde1":"code","085adcd7":"code","93411a42":"code","a025d536":"code","c1955987":"code","1e9b6570":"code","d2579c2f":"code","07976d19":"code","941f6c14":"code","1b43872a":"code","6622e376":"code","0a7bbd75":"code","2b033e4f":"code","dd1b21db":"code","02cf294e":"code","9542f576":"code","66f99af8":"code","5fab5b90":"code","dda27dc6":"markdown","2397fcc0":"markdown","a68cdc0e":"markdown","e4e2ccb5":"markdown","1755ff1d":"markdown","c39130be":"markdown","7cef43a6":"markdown","4a3772f8":"markdown","83cab0f6":"markdown","9c1408ea":"markdown","e449ecc7":"markdown","b3fc22f8":"markdown","e6f7d2bd":"markdown","14054b9a":"markdown","477d6055":"markdown","c734f62d":"markdown","767bb02b":"markdown","102ffeb0":"markdown","4edc150d":"markdown","19ef77d0":"markdown","02697dcc":"markdown","0bfc2e7f":"markdown","b156e7dd":"markdown","d4b51883":"markdown","6f4779fa":"markdown","5d39b55f":"markdown","e169f021":"markdown","a346660e":"markdown","11a36d2a":"markdown","02ae51ac":"markdown","c5340faf":"markdown","3e0cdfbf":"markdown","c9e63a7b":"markdown","a3f34033":"markdown","71f401ce":"markdown","23c45774":"markdown","0163581c":"markdown","0c6c389b":"markdown"},"source":{"bf072f53":"import numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn as skl\nimport pandas as pd","18872ff5":"adult = pd.read_csv('\/kaggle\/input\/adult-pmr3508\/train_data.csv',\n        names=[\n        \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n        \"Hours per week\", \"Country\", \"Target\"],\n        sep=r'\\s*,\\s*',\n        engine='python',\n        na_values=\"?\")","e09bae74":"adult.shape # nome_da_base.shape , shape == forma","44efded4":"adult.head() # nome_da_base.head() , head() == cabe\u00e7alho {Pequena demonstra\u00e7\u00e3o da base}","0a2a7d06":"nadult = adult.dropna() #Elimina-se observa\u00e7\u00f5es com \"missing data\"","300cf779":"nadult.shape","9d3712c7":"adult.shape","ff7f7264":"Observacoes_com_missing_data = 32561 - 30162\nprint(Observacoes_com_missing_data) #numero absoluto de missing data\nprint((Observacoes_com_missing_data\/32561)*100) #percentual de missing data","028a142d":"#importantdo a base de teste\ntestAdult = pd.read_csv('\/kaggle\/input\/adult-pmr3508\/train_data.csv',\n        names=[\n        \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n        \"Hours per week\", \"Country\", \"Target\"],\n        sep=r'\\s*,\\s*',\n        engine='python',\n        na_values=\"?\")","7bdcefe2":"nTestAdult = testAdult.dropna()\nprint(testAdult.shape)\nprint(nTestAdult.shape)","4fb6d8ac":"from sklearn.linear_model import LogisticRegression #Chamando a Regressao Logistica na biblioteca sklearn\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing","78be1228":"#Fun\u00e7\u00e3o Regress\u00e3o Logistica com cross validation\ndef LR_CV(do_predict):\n#Area de ajsustes de parametros da funcao:_______________________________________________________________\n    pcv = 10 #Numero de divisoes da base de dados para cross validation\n    \n#A funcao:_______________________________________________________________________________________________\n    from sklearn.linear_model import LogisticRegression #Chamando a Regressao Logistica na biblioteca sklearn\n    from sklearn.model_selection import cross_val_score\n    from sklearn.metrics import accuracy_score\n    from sklearn import preprocessing\n    #Instanciando e organizando a base de dados\n    numAdult = nadult.apply(preprocessing.LabelEncoder().fit_transform)\n    numTestAdult = nTestAdult.apply(preprocessing.LabelEncoder().fit_transform)\n    Xadult = numAdult.iloc[:,0:14]\n    Yadult = numAdult.Target\n    XtestAdult = numTestAdult.iloc[:,0:14]\n    YtestAdult = numTestAdult.Target\n    #Realizando a regress\u00e3o Logistica e corss validation\n    LR = LogisticRegression(random_state=0)\n    scores_cv = cross_val_score(LR, Xadult, Yadult, cv=pcv, scoring='accuracy')\n    \n    #Observacao das features relevantes por meio dos pesos calculados, \"betas\"\n    LR.fit(Xadult,Yadult)\n    coefs = pd.Series(LR.coef_[0], index=Xadult.columns)\n    coefs.sort_values(ascending = False)\n    \n    i=0\n    average = 0\n    while i<pcv :\n        average = average + scores_cv[i]\n        i = i + 1\n    average = average\/pcv\n    \n    if(do_predict == 1):\n        YtestPred = LR.predict(XtestAdult)\n        print(YtestPred)\n    return average #Retorna a media dos resultados do cross validation\n\n#COMO USAR: do_predict eh um booleano[0,1] que indica se\n#uma previsao deve ser feita sobre a base de testes ou nao.","bf5f2e7c":"#Fun\u00e7\u00e3o para mostrar os pesos da regressao logistica, mostra \"betas\"\ndef betas():\n    pcv = 10;\n    from sklearn.linear_model import LogisticRegression #Chamando a Regressao Logistica na biblioteca sklearn\n    \n    #Instanciando e organizando a base de dados\n    numAdult = nadult.apply(preprocessing.LabelEncoder().fit_transform)\n    numTestAdult = nTestAdult.apply(preprocessing.LabelEncoder().fit_transform)\n    Xadult = numAdult.iloc[:,0:14]\n    Yadult = numAdult.Target\n    XtestAdult = numTestAdult.iloc[:,0:14]\n    YtestAdult = numTestAdult.Target\n    #Realizando a regress\u00e3o Logistica e corss validation\n    LR = LogisticRegression(random_state=0)\n    scores_cv = cross_val_score(LR, Xadult, Yadult, cv=pcv, scoring='accuracy')\n    \n    #Observacao das features relevantes por meio dos pesos calculados, \"betas\"\n    LR.fit(Xadult,Yadult)\n    coefs = pd.Series(LR.coef_[0], index=Xadult.columns)\n    return coefs.sort_values(ascending = False)","893a325a":"#Fun\u00e7\u00e3o para mostrar os pesos da regressao logistica, mostra \"betas\"\n#com selecao de atributos\ndef betas_select(Atributos):\n    pcv = 10;\n    from sklearn.linear_model import LogisticRegression #Chamando a Regressao Logistica na biblioteca sklearn\n    \n    #Instanciando e organizando a base de dados\n    numAdult = nadult.apply(preprocessing.LabelEncoder().fit_transform)\n    numTestAdult = nTestAdult.apply(preprocessing.LabelEncoder().fit_transform)\n    Xadult = numAdult.iloc[:,Atributos]\n    Yadult = numAdult.Target\n    XtestAdult = numTestAdult.iloc[:,Atributos]\n    YtestAdult = numTestAdult.Target\n    \n    #Realizando a regress\u00e3o Logistica e corss validation\n    LR = LogisticRegression(random_state=0)\n    scores_cv = cross_val_score(LR, Xadult, Yadult, cv=pcv, scoring='accuracy')\n    \n    #Observacao das features relevantes por meio dos pesos calculados, \"betas\"\n    LR.fit(Xadult,Yadult)\n    coefs = pd.Series(LR.coef_[0], index=Xadult.columns)\n    return coefs.sort_values(ascending = False)\n","ee4fcf23":"#Fun\u00e7\u00e3o Regress\u00e3o Logistica com cross validation e selecao de atributos via \n# vetor numerico \ndef LR_cv_select(do_predict,Atributos):\n    pcv = 10;\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.model_selection import cross_val_score\n    from sklearn.metrics import accuracy_score\n    from sklearn import preprocessing\n    numAdult = nadult.apply(preprocessing.LabelEncoder().fit_transform)\n    numTestAdult = nTestAdult.apply(preprocessing.LabelEncoder().fit_transform)\n    Xadult = numAdult.iloc[:,Atributos]\n    Yadult = numAdult.Target\n    XtestAdult = numTestAdult.iloc[:,Atributos]\n    YtestAdult = numTestAdult.Target\n    #Realizando a regress\u00e3o Logistica e corss validation\n    LR = LogisticRegression(random_state=0)\n    scores_cv = cross_val_score(LR, Xadult, Yadult, cv=pcv, scoring='accuracy')\n    \n    \n    i=0\n    average = 0\n    while i<pcv :\n        average = average + scores_cv[i]\n        i = i + 1\n    average = average\/pcv\n    \n    if(do_predict == 1):\n        LR.fit(Xadult,Yadult)\n        YtestPred = LR.predict(XtestAdult)\n        accuracy_pred = accuracy_score(YtestAdult,YtestPred)\n        print(accuracy_pred)\n    return average #Retorna a media dos resultados do cross validation\n    ","7651a3a7":"LR_CV(1)","e5914dac":"betas()","efc73fdc":"betas_select([4,9,11,10,0,12])","93e7ff82":"LR_cv_select(1,[4,9,11,10,0,12]) #Com parametros selecionados via RL","0e1737e7":"LR_cv_select(0,[0,3,6,7,10,11]) #com parametros selecionados no data prep do EP1","69523e68":"#Funcao KNN com cross validation\ndef knn_cv_select(KN,Atributos):\n    pcv = 10;\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.model_selection import cross_val_score\n    from sklearn.metrics import accuracy_score\n    from sklearn import preprocessing\n    numAdult = nadult.apply(preprocessing.LabelEncoder().fit_transform)\n    numTestAdult = nTestAdult.apply(preprocessing.LabelEncoder().fit_transform)\n    Xadult = numAdult.iloc[:,Atributos]\n    Yadult = numAdult.Target\n    XtestAdult = numTestAdult.iloc[:,Atributos]\n    YtestAdult = numTestAdult.Target\n    knn = KNeighborsClassifier(n_neighbors=KN)\n    scores_cv = cross_val_score(knn, Xadult, Yadult, cv=pcv)\n    \n    i=0\n    average = 0\n    while i<pcv :\n        average = average + scores_cv[i]\n        i = i + 1\n    average = average\/pcv\n    \n    knn.fit(Xadult,Yadult)\n    YtestPred = knn.predict(XtestAdult)\n    accuracy_pred = accuracy_score(YtestAdult,YtestPred)\n    print(accuracy_pred)\n        \n    return average","168d361b":"knn_cv_select(16,[0,3,6,7,10,11]) #com parametros selecionados no EP1 pra KNN","c90290ad":"knn_cv_select(16,[4,9,11,10,0,12]) #com parametros selcionados via RegLogist","375d6b09":"LR_cv_select(0,[4,9,0]) #somente os mais importantes","53fd069f":"LR_cv_select(0,[4,9,11,10,0,12,2]) #adicionando o melhor dos piores (fnlwgt)","cf34a063":"LR_cv_select(0,[2,4,6,8,10,12]) #Alto grau de aleatoriedade em relacao ao problema\n","7133bde1":"LR_cv_select(1,[4,9,11,10,0,12])","085adcd7":"#Funcao Random Forest com cross validation\ndef RF_cv_select_pred(do_predict,rs,Atributos):\n    pcv = 10;\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.model_selection import cross_val_score\n    from sklearn.metrics import accuracy_score\n    from sklearn import preprocessing\n    #Data base\n    numAdult = nadult.apply(preprocessing.LabelEncoder().fit_transform)\n    numTestAdult = nTestAdult.apply(preprocessing.LabelEncoder().fit_transform)\n    Xadult = numAdult.iloc[:,Atributos]\n    Yadult = numAdult.Target\n    XtestAdult = numTestAdult.iloc[:,Atributos]\n    YtestAdult = numTestAdult.Target\n    #Processing Data\n    RF = RandomForestClassifier(random_state=rs)\n    scores_cv = cross_val_score(RF, Xadult, Yadult, cv=pcv)\n    \n    i=0\n    average = 0\n    while i<pcv :\n        average = average + scores_cv[i]\n        i = i + 1\n    average = average\/pcv\n    \n    if(do_predict == 1):\n        RF.fit(Xadult,Yadult)\n        YtestPred = RF.predict(XtestAdult)\n        accuracy_pred = accuracy_score(YtestAdult,YtestPred)\n        print(\"Teste\")\n        print(accuracy_pred)\n    print(\"Cross_Validation\")\n    return average #Retorna a media dos resultados do cross validation","93411a42":"RF_cv_select_pred(1,1,[1,2,3,4,5,6,7,8,9,10,11,12,13]) #1 arvore","a025d536":"RF_cv_select_pred(1,2,[1,2,3,4,5,6,7,8,9,10,11,12,13]) #2 arvores","c1955987":"RF_cv_select_pred(1,10,[1,2,3,4,5,6,7,8,9,10,11,12,13]) #10 arvores","1e9b6570":"RF_cv_select_pred(1,20,[1,2,3,4,5,6,7,8,9,10,11,12,13])","d2579c2f":"RF_cv_select_pred(1,50,[1,2,3,4,5,6,7,8,9,10,11,12,13])","07976d19":"RF_cv_select_pred(1,100,[1,2,3,4,5,6,7,8,9,10,11,12,13])","941f6c14":"RF_cv_select_pred(1,500,[1,2,3,4,5,6,7,8,9,10,11,12,13])","1b43872a":"i=0\nwhile i<21 :\n    print(i,RF_cv_select_pred(1,i,[1,2,3,4,5,6,7,8,9,10,11,12,13]))\n    i = i + 1","6622e376":"arr = np.array([0.8347255179535971,0.8339962288266571,0.8329685422307043,0.8336314633233639,0.8331009922850943,0.8338305562832211,0.8330681545952977,0.8343938319905859,0.8330680006639337,0.8347252431042806,0.8311115913310386,0.835256054931208,0.8337641112686788,0.833731405588787,0.8332335962343975,0.8344600570761023,0.8336314633671048,0.8341292175241787,0.8328359932818827,0.8345267877986287])\nIndex_Max_Accuracy = np.where(arr == np.amax(arr))\nprint('List of Indices of maximum element :', Index_Max_Accuracy[0])\n# Get the maximum element from a Numpy array\nprint('Max accuracy is :', np.amax(arr))","0a7bbd75":"i=1\narr_res = []\nwhile i<31 :\n    arr_res.insert(i, RF_cv_select_pred(1,i,[0,3,6,7,10,11]))\n    i = i + 1\narr = np.array(arr_res)\nIndex_Max_Accuracy = np.where(arr == np.amax(arr))\nprint('List of Indices of maximum element :', Index_Max_Accuracy[0])\n# Get the maximum element from a Numpy array\nprint('Max accuracy is :', np.amax(arr))","2b033e4f":"i=1\narr_res = []\nwhile i<31 :\n    rf = RF_cv_select_pred(1,i,[4,9,11,10,0,12])\n    arr_res.insert(i, rf)\n    print(i)\n    i = i + 1\narr = np.array(arr_res)\nIndex_Max_Accuracy = np.where(arr == np.amax(arr))\nprint('List of Indices of maximum element :', Index_Max_Accuracy[0])\n# Get the maximum element from a Numpy array\nprint('Max accuracy is :', np.amax(arr))","dd1b21db":"RF_cv_select_pred(1,19,[0,3,6,7,10,11])","02cf294e":"#Funcao SVM - Polinomial gamma auto\ndef SVM_cv_select_pred_pol(do_predict,Atributos):\n    pcv = 10;\n    from sklearn.svm import SVC\n    from sklearn.model_selection import cross_val_score\n    from sklearn.metrics import accuracy_score\n    from sklearn import preprocessing\n    #Data base\n    numAdult = nadult.apply(preprocessing.LabelEncoder().fit_transform)\n    numTestAdult = nTestAdult.apply(preprocessing.LabelEncoder().fit_transform)\n    Xadult = numAdult.iloc[:,Atributos]\n    Yadult = numAdult.Target\n    XtestAdult = numTestAdult.iloc[:,Atributos]\n    YtestAdult = numTestAdult.Target\n    #Processing Data\n    SVM = SVC(kernel='rbf')\n    scores_cv = cross_val_score(SVM, Xadult, Yadult, cv=pcv)\n    \n    i=0\n    average = 0\n    while i<pcv :\n        average = average + scores_cv[i]\n        i = i + 1\n    average = average\/pcv\n    \n    if(do_predict == 1):\n        SVM.fit(Xadult,Yadult)\n        YtestPred = SVM.predict(XtestAdult)\n        accuracy_pred = accuracy_score(YtestAdult,YtestPred)\n        print(\"Teste\")\n        print(accuracy_pred)\n    print(\"Cross_Validation\")\n    return average #Retorna a media dos resultados do cross validation","9542f576":"SVM_cv_select_pred_pol(1,[0])","66f99af8":"SVM_cv_select_pred_pol(1,[0,3,6,7,10,11])","5fab5b90":"SVM_cv_select_pred_pol(1,[4,9,11,10,0,12])","dda27dc6":"# EP2 - Compara\u00e7\u00e3o entre 3 diferentes classificadores de Machine Learning aplicados sobre a base Adult da UCI\n      \nAutor do Estudo: Fernando Zolubas Preto - Nusp: 10694192","2397fcc0":"Esses resultados mostram que um algoritimo mais ligado ao conheciemnto te\u00f3rico-estat\u00edstico como a Regress\u00e3o Log\u00edstica quando aplicado sem as devidas otimiza\u00e7\u00f5es pode apresentar, ao contrario do que se possa imaginar a priori, um resultado pior em termos de acuracia do que um algoritimo intuitivo como o KNN realizado com as devidas otimiza\u00e7\u00f5es. Em n\u00fameros isso se traduz pelos  81,7% de acuracia alcan\u00e7ados pela regress\u00e3o log\u00edstica frente aos 83,4% de acuracia alcan\u00e7ados pelo algoritimo KNN.\n\nLembrando que essas conclus\u00f5es podem n\u00e3o ser gerais caso o problema em analise seja muito distindo do proposto pela base Adult.","a68cdc0e":"* Aplica\u00e7\u00f5es, levando em conta o hiper-par\u00e2metro n\u00famero de \u00e1rvores","e4e2ccb5":"* Regressao Logistica com cross validation, predict[0,1], Toda a base","1755ff1d":"*Resultado* 1)\n* List of Indices of maximum element : [19]\n* Max accuracy is : 0.8416224107069674","c39130be":"Vemos que somente as features: Education-Num, Sex, Capital Loss, Capital Gain,\nAge e Hours per week s\u00e3o de fato relevantes. Assim, chamemos betas_select(Atributos) para olhar somente para esses atributos descartando os demais de modo que o algoritimo aqui utilizado fez uma especie de data prep autom\u00e1tico.","7cef43a6":"**Resultados SVM Classifier**\nO melhor resultado para as itera\u00e7\u00f5es do SMV Classifier com Kernel Gaussiano foi para a sele\u00e7\u00e3o de atributos do Data Preparation como esperado alcan\u00e7ando as acuraricas de:\n\nTeste:\n* 80.47808764940239%\n\nCross_Validation\n* 84.05280484618712%\n\nN\u00e3o Foram realizados mais testes devido a alto consumo computacional do algoritimo.","4a3772f8":"# Random Forest","83cab0f6":"**Tentativas de otimiza\u00e7\u00e3o da Regress\u00e3o Logistica via sele\u00e7\u00e3o de par\u00e2metros**","9c1408ea":"Tem-se na base adult 32561 amostras, 14 features (=atributos) e 1 lablel (=r\u00f3tulo). ","e449ecc7":"**Base de testes:**","b3fc22f8":"**Regulagem do Hiper-par\u00e2metro N\u00famero de \u00c1rvores para todos os atributos**\n\nDada a busca acima conclui-se que ao se considerar todas as features do data set temos que o n\u00famero \u00f3timo de \u00e1rvores nesse caso \u00e9 11.\n* Acuracia Cross Validation = 83.11115913310386%\n* Acuracia Teste            = 82.80876494023904%","e6f7d2bd":"Execu\u00e7\u00f5es","14054b9a":"# Logistic Regression\n\n* P(Y = 1|D) = (e^w)\/(1 + e^w)\n* w = somat\u00f3ria{i=1 to n}(beta_i * X_i)\n* logit(P)","477d6055":"Compara\u00e7\u00e3o: Data prep Intuitivo vs Betas da regress\u00e3o Log\u00edstica\n\nNo estudo anterior da base Adult (com o algoritimo KNN - EP1) numa estrat\u00e9gia\nde bom senso e observa\u00e7\u00f5es simples observou-se a otimiza\u00e7\u00e3o no KNN para\nos seguintes atributos:\n* Age, Education, Occupation, Relationship, Capital Gain, Capital Loss\n\nAo passo que para a regress\u00e3o log\u00edstica:\n* Age, Education-Num, Hours per week, Sex, Capital Gain, Capital Loss\n\nE de maneira divergente do esperado vemos que o atributo Relationship que apresenta a pior influencia na regress\u00e3o log\u00edstica se mostrou relevante para o KNN de acordo com esse data prep \"intuitivo\" do EP1 mostrando que algoritimos diferentes se baseiam em atributos eventualmente distintos e que a intui\u00e7\u00e3o n\u00e3o boa o bastante para guiar sozinha os metodos de aprendizado de m\u00e1quina.","c734f62d":"Curiosamente houve uma invers\u00e3o com rela\u00e7\u00e3o a relvancia das features dado que o atributo \"sex\" se mostrou muito mais relevante do que \"Education-num\" na aplica\u00e7\u00e3o do algoritimo em que se selecionou apenas os atributos relevantes.","767bb02b":"# Data check","102ffeb0":"*Resultado* 2)\n* List of Indices of maximum element : [19]\n* Max accuracy is : 0.8215634534964499","4edc150d":"***Suspeitas para o fato do KNN ter alcan\u00e7ado resultados melhores que a regress\u00e3o logistica:***\n\nO grau de linearidade do problema pode tal que de modo que a analise desse tipo de regress\u00e3o fica prejudicada.","19ef77d0":"TESTE 2) \nFeatures sugeridas pela Regressao Logistica","02697dcc":"Dado o alto consumo computacional desse algoritimo decidiu-se utilizar a vers\u00e3o\n\"Gaussian Kernel\" (rbf) numa tentativa de reduzir a procura de hiper-parametros em rela\u00e7\u00e3o a vers\u00e3o polinomial. ","0bfc2e7f":"**Resultados da Otimiza\u00e7\u00e3o**\n\nA estra\u00e9gia que mais funcionou foi a trataiva n\u00famero 1 em que considerou-se o data Prep realizado no EP1 de onde se conclui que a escolha dos par\u00e2metros:\nAge, Education, Occupation, Relationship, Capital Gain, Capital Loss.\n\u00c9 a que mais otimiza a Random Forest de **19 \u00e1rvores** com acuracia de \n* 84.16% Cross Validation\n* 83.97% Teste","b156e7dd":"Comparacao entre KNN e regressao Logistica com parametros selecionados\n* Metodo intuitivo vs Metodo estatistico\n","d4b51883":"Os melhores valores de acuraria foram para at\u00e9 20 \u00e1rvores. As medidas foram:\n* 01) 0.8347255179535971\n* 02) 0.8339962288266571\n* 10) 0.8347252431042806\n* 20) 0.8345267877986287\n","6f4779fa":"# Conclus\u00f5es\n\nMelhores Itera\u00e7\u00f5es:\n\n*Teste*\n\n* KNN:                     82.80 %\n* Logistic Regression:     81.73 %\n* Random Forest:           83.97 %\n* Support Vector Machine:  80.48 %\n\n*Cross Validation*\n\n* KNN:                    83.44 %\n* Logistic Regression:    81.75 %\n* Random Forest:          84.16 %\n* Support Vector Machine: 84.05 %\n\n\n**Tempo**\n\n* Os algoritimos Logistic Regression e Random Forest aprensentaram tempos similares de execu\u00e7\u00e3o na ordem de poucos segundos enquanto o algoritimo SVM gastou dezenas de minutos para entregar o pior resultado de predi\u00e7\u00e3o. \n\n* Isso pode estar relacionado a problemas de overfiting no SVM dado que o mesmo algoritimo que se ruim nos testes foi o melhor no processo de cross validation.\n\n**Data Prep Autom\u00e1tico ?**\nTestou-se Utilizar nos m\u00e9todos KNN, Random Forest e SVM as features de \"peso positvo no calculo\" no calulo dos \"betas\" da regress\u00e3o log\u00edstica. \nResultados:\n\n* KNN = Teste 0.816 Cross_Validation 0.825 pior_cv 0.727\n* RandomForest = Teste 0.794 Cross_Validation  0.821 pior_cv 0.832\n* SVM = Teste 0.79 Cross_Validation 0.82 pior_cv 0.751\n\nVemos que embora em nenhum desses tr\u00eas algoritimos o \"data prep autolamtico\" gerado pela regress\u00e3o log\u00edstica ganha de um data prep mais cuidadosamente elaborado como feito no EP1 para a base adult. Por\u00e9m, essa estratat\u00e9gia pode ser uma boa caso seja preciso elaborar um classificador muito rapidamente pois apresenta resultados de cross validation, e por tanto de teste, muito superiores do que os piores casos de cada algoritimo que seriam evitados usando essa proposta fornecida pela regress\u00e3o log\u00edstica.\n\n*OBS* O Data Prep \u00e9 o mesmo do EP1 dado que se trata da mesma base de dados, sendo assim ele n\u00e3o foi repetido nesse EP mas seus resultados foram utilizados e devidamente explicados.\n\n\n**Acuraria**\n    \nO melhor algoritimo na base de teste, objetivo final de um algoritimo de Machine Learning antes de ser utilizado na pr\u00e1tica da classifica\u00e7\u00e3o, foi o Random Forest de **19 \u00e1rvores** com incriveis 83,97 % de acuracia. Esse bom resultado se deve as propriedades do algoritimo que permite uma otima classifica\u00e7\u00e3o para problemas do tipo, supervised Leraning proposto pela Base Adult, bem como o trabalho de Data Preparation que tornou possivel esse resultado garantindo mais (83.97 - 82.81 = 1.16 % de acur\u00e1cia em rela\u00e7\u00e3o ao caso n\u00e3o otimizado com as 14 features consideradas, um percentual alto quando a competi\u00e7\u00e3o acima pela melhor eficacia esta na casa das unidades de percentual.","5d39b55f":"# SUPORT VECTOR MACHINE","e169f021":"* SVM com variaveis do Data Prep, 10 min aproximadamente","a346660e":"Classificadores e t\u00e9cnicas consideradas:\n* Knn (K-nearest neighbor)\n* Naive Bayes\n* Redes Neurais\n* Regress\u00e3o Linear\n* Regress\u00e3o N\u00e3o Linear\n* Ridge\n* La\u00e7o\n* Regress\u00e3o Logistica\n* LDA  (Linear discriminant analysis)\n* QDA\n* \u00c1rvores de Classifica\u00e7\u00e3o\n* Bagging\n* Random Forest\n* Boosting\n* Support Vector Machines SVMs\n\nEsse estudo tem como limita\u00e7\u00e3o os seguintes aspectos-chave para a escolha dos classificadores a serem comparados:\n\n* Tempo estimado para a realiza\u00e7\u00e3o do estudo: 2 dias (24h totais)\n* Facilidade de interpreta\u00e7\u00e3o dos resultados\n* Facilidade de implementa\u00e7\u00e3o dos classificadores \n\nDado esse cen\u00e1rio escolheu-se as seguintes t\u00e9cnicas com base nisso e nas aulas do curso:\n\n* Regress\u00e3o Logistica \n        Motiva\u00e7\u00e3o: Implementa\u00e7\u00e3o simples, Bom para classificac\u00e3o bin\u00e1ria, pouco sujeito a outliers.\n* Random Forest\n        Motiva\u00e7\u00e3o: Implementa\u00e7\u00e3o simples, Intendimento simples, pouco sujeito a outliers, recomendado pelo professor dados os bons resultados que costuma fornecer.\n* Support Vector Machines\n        Motiva\u00e7\u00e3o: Pouco sujeito a outiliers, N\u00e3o muito complexo, Relacionado a modelos matem\u00e1ticos estudados no ciclo b\u00e1sico da poli.\n        \n**OBS** \nO Data Preparation Mencionado Nesse EP \u00e9 o mesmo do EP1 Pois a base de dados \u00e9 a mesma.","11a36d2a":"Dadas as limita\u00e7\u00f5es de tempo n\u00e3o foi possivel encontrar uma otimiza\u00e7\u00e3o melhor do que a proposta pelo proprio algoritimo cuja acuracia nesse caso foi de: \n* 81.73201139272305% para o Corss_Validation e de \n* 81.75298804780876% quando aplicado na base de testes.\n\nObs: Fica claro aqui a eficacia da estrategia de cross_validation haja vista a precisao da acuracia aplicada a base de testes que \u00e9 a mesma daquela realcioanda ao cross validation.","02ae51ac":"TESTE 1) Resusltados do Data Prep do EP1","c5340faf":"**Tentaiva de otimiza\u00e7\u00e3o, variando as features consideradas da base de dados**\n\nAqui se prop\u00f5es duas estrat\u00e9gias simples de otimiza\u00e7\u00e3o\n* 1) Testar os resultados do Data preparation do EP1 \n* 2) Testar os atributos indicados pela regress\u00e3o Log\u00edstica, isto \u00e9, usar os atributos cujos  \"betas\" s\u00e3o positivos.\n","3e0cdfbf":"**Execu\u00e7\u00f5es do SVM**\n\nSer\u00e3o adotas as duas estrat\u00e9gias anteriores como tentativas de Otimiza\u00e7\u00e3o.\nAl\u00e9m disso ser\u00e1 realizada uma execu\u00e7\u00e3o aleat\u00f3ria de baixa intensidade computacional a fim de determinar um valor que tenda ao minimo da eficacia do algoritimo para que se tenha como m\u00e9trica geral.\n\n* 1) Execu\u00e7\u00e3o Minima\n* 2) Execu\u00e7\u00e3o do Data Prep\n* 3) Execu\u00e7\u00e3o Com features sugeridas pelos pesos da Regress\u00e3o Log\u00edstica","c9e63a7b":"* SVM com variaveis do Data Prep, 6 min aproximadamente","a3f34033":"* SVM com 1 variavel para testes, 3 min aproximadamente","71f401ce":"Fun\u00e7\u00f5es para regress\u00e3o logistica","23c45774":"Obtivemos os seguintes resultados:\n* RegLog(parametros via RegLog) = 0,817\n* RegLog(parametros via data prep) = 0,792\n* KNN( parametros via RegLog) = 0,825\n* KNN( parametros via data prep) = 0,834\n\n\n","0163581c":"* Regressao Logistica com cross validation, predict[0,1], Selecao da base","0c6c389b":"Numero dos atributos\n* Age________________0\n* Workclass__________1\n* fnlwgt_____________2\n* Education__________3\n* Education-Num______4\n* Martial Stauts_____5\n* Occupation_________6\n* Relationship_______7\n* Race_______________8\n* Sex________________9\n* Capital Gain______10\n* Capital Loss______11\n* Hours per week____12\n* Country___________13"}}