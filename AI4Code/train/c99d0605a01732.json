{"cell_type":{"26db8aa2":"code","24bb3290":"code","858c268b":"code","49b315c8":"code","291ef456":"code","9111fbae":"code","d69d3737":"code","d92fdd29":"code","f70c5062":"code","9970bfbc":"code","738cf5b0":"code","b69194c1":"code","97e49952":"code","31de2179":"code","2fcefd40":"code","66a227ac":"code","712b7b48":"markdown"},"source":{"26db8aa2":"import os\nimport numpy as np\nimport glob\nimport shutil\n\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt","24bb3290":"from tensorflow.keras.preprocessing.image import ImageDataGenerator","858c268b":"_URL = \"https:\/\/storage.googleapis.com\/download.tensorflow.org\/example_images\/flower_photos.tgz\"\n\nzip_file = tf.keras.utils.get_file(origin=_URL,\n                                   fname=\"flower_photos.tgz\",\n                                   extract=True)\n\nbase_dir = os.path.join(os.path.dirname(zip_file), 'flower_photos')\n","49b315c8":"classes = ['roses', 'daisy', 'dandelion', 'sunflowers', 'tulips']","291ef456":"for cl in classes:\n  img_path = os.path.join(base_dir, cl)\n  images = glob.glob(img_path + '\/*.jpg')\n  print(\"{}: {} Images\".format(cl, len(images)))\n  train, val = images[:round(len(images)*0.8)], images[round(len(images)*0.8):]\n\n  for t in train:\n    if not os.path.exists(os.path.join(base_dir, 'train', cl)):\n      os.makedirs(os.path.join(base_dir, 'train', cl))\n    shutil.move(t, os.path.join(base_dir, 'train', cl))\n\n  for v in val:\n    if not os.path.exists(os.path.join(base_dir, 'val', cl)):\n      os.makedirs(os.path.join(base_dir, 'val', cl))\n    shutil.move(v, os.path.join(base_dir, 'val', cl))","9111fbae":"train_dir = os.path.join(base_dir, 'train')\nval_dir = os.path.join(base_dir, 'val')","d69d3737":"batch_size = 100\nIMG_SHAPE = 150","d92fdd29":"image_gen = ImageDataGenerator(rescale=1.\/255, horizontal_flip=True)\ntrain_data_gen = image_gen.flow_from_directory(batch_size=batch_size,\n                                               directory=train_dir,\n                                               shuffle=True,\n                                               target_size=(IMG_SHAPE,IMG_SHAPE))","f70c5062":"image_gen = ImageDataGenerator(rescale=1.\/255,rotation_range=45)\n\ntrain_data_gen = image_gen.flow_from_directory(\n    batch_size=batch_size,\n    directory=train_dir,\n    shuffle=True,\n    target_size=(IMG_SHAPE,IMG_SHAPE)\n)","9970bfbc":"image_gen = ImageDataGenerator(rescale=1.\/255,zoom_range=0.5)\n\ntrain_data_gen = image_gen.flow_from_directory(\n    batch_size=batch_size,\n    directory=train_dir,\n    shuffle=True,\n    target_size=(IMG_SHAPE,IMG_SHAPE)\n)","738cf5b0":"image_gen_train = ImageDataGenerator(rescale=1.\/255,rotation_range=45,horizontal_flip=True,width_shift_range=0.15,height_shift_range=0.15) \n\n\ntrain_data_gen = image_gen_train.flow_from_directory(\n    batch_size=batch_size,\n    directory=train_dir,\n    shuffle=True,\n    target_size=(IMG_SHAPE,IMG_SHAPE),\n    class_mode='binary'\n)","b69194c1":"\nimage_gen_val = ImageDataGenerator(rescale=1.\/255)\nval_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size,\n                                                 directory=val_dir,\n                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\n                                                 class_mode='binary')","97e49952":"model = tf.keras.models.Sequential([\n                                    tf.keras.layers.Conv2D(16,(3,3),activation='relu',input_shape=(150,150,3)),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n\n                                    tf.keras.layers.Conv2D(32,(3,3),activation='relu'),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n\n                                    tf.keras.layers.Conv2D(64,(3,3),activation='relu'),\n                                    tf.keras.layers.MaxPooling2D(2,2),\n\n                                    tf.keras.layers.Dropout(0.2),\n                                    tf.keras.layers.Flatten(),\n                                    tf.keras.layers.Dense(512, activation='relu'),\n\n                                    tf.keras.layers.Dense(5,activation='softmax')\n])","31de2179":"# Compile the model\nmodel.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])","2fcefd40":"epochs = 30\n\nhistory = model.fit_generator(\n    train_data_gen,\n    epochs=epochs,\n    validation_data=val_data_gen\n)","66a227ac":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","712b7b48":"# Data Augmentation\nOverfitting generally occurs when we have small number of training examples. One way to fix this problem is to augment our dataset so that it has sufficient number of training examples. Data augmentation takes the approach of generating more training data from existing training samples, by augmenting the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, your model will never see the exact same picture twice. This helps expose the model to more aspects of the data and generalize better.\n\nIn tf.keras we can implement this using the same ImageDataGenerator class we used before. We can simply pass different transformations we would want to our dataset as a form of arguments and it will take care of applying it to the dataset during our training process."}}