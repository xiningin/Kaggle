{"cell_type":{"12b6752c":"code","64a7b11a":"code","e7ea7091":"code","2e15e1af":"code","07ec6fd1":"code","888db5dc":"code","5cf8db36":"code","c4bc80a9":"code","9a9940be":"code","91fff077":"code","edf95ced":"code","220e9b72":"code","05fc0976":"code","5ae0ee7e":"code","6ab00e23":"code","9de53c09":"code","36132d09":"code","68f016a2":"code","3e5041be":"code","ee4eeb82":"code","35f9483e":"code","0e868b01":"markdown","b20cbf6e":"markdown","f32315df":"markdown","f4d08a82":"markdown","a3a03518":"markdown","e998b331":"markdown","6aa3fcb1":"markdown","edac7546":"markdown","c243cc43":"markdown","ed1c57b3":"markdown","8657109b":"markdown","5a8ac044":"markdown","959c2866":"markdown","8ae356ab":"markdown","240d3ee7":"markdown","e234aa09":"markdown","da753f9f":"markdown","12640d7b":"markdown","8fa14bb7":"markdown","b043189f":"markdown","f23d0abb":"markdown","395435ec":"markdown","5898b7a1":"markdown","34deb9d3":"markdown","231c3c5e":"markdown"},"source":{"12b6752c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","64a7b11a":"!cp \/kaggle\/input\/advertising-dataset\/'Advertising Dataset'\/Images\/* .\n!ls","e7ea7091":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport warnings\nfrom sklearn.preprocessing import StandardScaler\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor \nimport statsmodels.api as sm\nfrom scipy.stats import kurtosis, skew\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline","2e15e1af":"df = pd.read_csv(\"\/kaggle\/input\/advertising-dataset\/Advertising Dataset\/Advertising Dataset.csv\")","07ec6fd1":"df.shape","888db5dc":"df.head()","5cf8db36":"sns.distplot(df.Sales,color='blue', hist=True,rug=False)","c4bc80a9":"skew(df.Sales),kurtosis(df.Sales)","9a9940be":"sns.scatterplot(x='TV', y='Sales', data=df)","91fff077":"# var = df.columns.values\n\nvar = df.loc[:, df.columns != 'Sales'].columns.values\n\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(3,1,figsize=(8,10))\n\ni= 0\nfor feature in var:\n    i += 1\n    plt.subplot(3,1,i)\n    sns.regplot(x=feature, y='Sales', data=df)\n    plt.xlabel(feature, fontsize=12)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='both', which='major', labelsize=12)\n    fig.tight_layout(pad=3.0)\nplt.show();","edf95ced":"np.std(df.TV),np.std(df.Radio),np.std(df.Newspaper)","220e9b72":"sns.heatmap(df.loc[:, df.columns != 'Sales'].corr(),annot=True)","05fc0976":"X = df.loc[:, df.columns != 'Sales']\nvif_data = pd.DataFrame() \nvif_data[\"feature\"] = df.loc[:, df.columns != 'Sales'].columns\n\n# calculating VIF for each feature \nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) \n                          for i in range(len(X.columns))] \n  \nvif_data.head()","5ae0ee7e":"X = df.loc[:, df.columns.isin(['TV','Radio','Newspaper'])]\nmodel = sm.OLS(df.Sales,X)\nresults = model.fit()\nprint(results.summary())","6ab00e23":"residual = results.resid\n\nsns.residplot(x = results.predict(), y = residual , \n              lowess=True, color=\"olivedrab\")\nplt.show()","9de53c09":"sns.distplot(residual,color='blue',label= 'Residuals', hist=True,rug=False)","36132d09":"skew(residual),kurtosis(residual)","68f016a2":"X = df.loc[:, df.columns.isin(['TV','Radio','Newspaper'])]\n#Bring Features to comparable scale\nscaler = StandardScaler(with_mean=False, with_std=True)\nscaler.fit(X)\nX_t = scaler.transform(X)\nmodel = sm.OLS(df.Sales,X_t)\nresults = model.fit()\nprint(results.summary())","3e5041be":"value = [results.params[0],results.params[1],results.params[2]]\nfeatures = ['TV','Radio','Newspaper']","ee4eeb82":"tmp = pd.DataFrame({'Feature': features, 'Feature importance': value})\ntmp = tmp.sort_values(by='Feature importance',ascending=False)\nplt.figure(figsize = (12,6))\nplt.title('Features importance',fontsize=14)\ns = sns.barplot(x='Feature',y='Feature importance',data=tmp)\ns.set_xticklabels(s.get_xticklabels(),rotation=90)\nplt.show()","35f9483e":"# f, ax = plt.subplots(figsize = (8,5))\n\n# ax = sns.barplot(x=value, y=features)\n# plt.ylabel('Features', fontsize=12)\n# plt.xlabel('Value', fontsize=12)\n# plt.show()","0e868b01":"<h1>Agenda: Fitting Linear Regression & verifying how well it's assumptions are satisfied<\/h1>\n\n<figure>\n          <img src= 'Regression1.png' style=\"width:75%\">\n<\/figure>","b20cbf6e":"<h3>Check for Multivariate Normality<\/h3>\n\n<font color='red'><i>\nLinear regression analysis requires all variables to be multivariate normal. (Follow Gaussian distribution ~ 68\u201395\u201399.7 rule)<br>\nAlternatively, the residuals should be normally distributed. (Suggests other variables also don\u2019t deviate much from normality as well)\n    <\/i><\/font>","f32315df":"<h3>EDA: Next we will do Exploratory Data Analysis (EDA) to understand our dataset better.<\/h3>\n\n<figure>\n  <img src=\"eda.jpg\" style=\"width:75%\">\n    <center><figcaption><b>Image Source:<\/b> <a href='https:\/\/www.freepik.com\/free-vector\/data-inform-illustration-concept_6195525.htm'>https:\/\/www.freepik.com\/free-vector\/data-inform-illustration-concept_6195525.htm<\/a><\/figcaption><\/center>\n<\/figure>\n","f4d08a82":"<h3>Test for Autocorrelation:  Durbin-Watson test<\/h3><br>\n<font color='red'><i>\nThe null hypothesis of the test is that there is no serial correlation. <br>\nValue close to 2 indicates No Autocorrelation, ~0: Positive Autocorrelation, ~4: Negative Autocorrelation\n\nDurbin-Watson Test returns a value of 1.949 (close to zero) indicating no-autocorrelation.\n    <\/i><\/font>\n","a3a03518":"<h3> Check for Multicollinearity <\/h3>\n\n\n<b>Correlation matrix: <\/b><font color='red'><i>Calculates Pearson\u2019s Bivariate Correlation among all independent variables. Remove predictors with high correlation.\n","e998b331":"<b>F-Statistics<\/b><br>\n    \nH0: <font color='red'>The null hypothesis states that the model with no independent variables fits the data as well as your model.<\/font><br>\nAlternate Hypothesis: <font color='red'>The alternative hypothesis says that your model fits the data better than the intercept-only model.\n    <\/i><font\/>","6aa3fcb1":"<h2> Summary<\/h2>\n<font color='Blue'><i><br>\n1. TV ads comes out to be the most important feature.<br>\n2. Linear Regression results are statistically significant as all the assumptions are satisfied.","edac7546":"<font color='red'><i>\nWe don't see high correlation between independent variables.<br>\n    VIF score is also < 5 for all the variables.<\/i><\/font>","c243cc43":"<h2><font color='brown'><i>Assumptions of Linear Regression: Linear Relationship statisfied\n    <\/i><\/font><\/h2>","ed1c57b3":"Problems due to Heteroscedasticity:<br>\n<font color='red'><i>\nIf there\u2019s patterns in the residuals, model has a problem and is not able to explain the data patterns completely. Hence, the coefficients values are unreliable.<br>\nHeteroscedasticity tends to produce p-values that are smaller than they should be reducing the statistical significance.\n    <\/i><\/font>","8657109b":"<font color='red'><i><b>Realisation: Definitely the Advertisement spent & Sales are in different scale<\/i><\/b><\/font>\n","5a8ac044":"<b>Regression Plot: <\/b><font color='red'><i>Draws a scatterplot of two variables, x and y, and then fit the regression model to visualise the linear relationship.<\/i><\/font>","959c2866":"<h3>Check for Homoscedasticity<\/h3><br>\n<font color='red'><i>\nHomoscedasticity(meaning \u201csame variance\u201d) assumes the residuals are randomly scattered along the regression line\/plane\/hyperplane and does not follow any pattern.<br>\n    \nIn contrast, Heteroscedasticity is a systematic change in the spread of the residuals. \n    <\/i><\/font>\n<br><br>\n<figure>\n  <img src=\"Homoscedasticity.png\" style=\"width:75%\">\n    <center><figcaption><b>Image Source:<\/b> <a href='https:\/\/miro.medium.com\/max\/1400\/1*Jan9oVOzNqQyhA4bSg_zwA.png'>https:\/\/miro.medium.com\/max\/1400\/1*Jan9oVOzNqQyhA4bSg_zwA.png<\/a><\/figcaption><\/center>\n<\/figure>","8ae356ab":"<font color='blue'><i>\n<li>\nAbsolute value of Skewness and Kurtosis when <= 0.05, denotes almost Normal distribution.\n<li>                                                \nEven in this case, Sales doesn't deviate a lot from normal distribution. Kurtosis is high and negative indicating the excess flatness in curve.\n   \n<\/i><\/font>","240d3ee7":"<b>EDA Insights: <\/b><font color='green'><i>\n<br>1. TV ads seems to have highest impact with the steepest slope followed by Radio and Newspaper\n<br>2. Spending on TV ads seems to have highest variance followed by Newspaper and Radio. One reason could be the cost-associated is high for TV ads.","e234aa09":"<h2><font color='brown'><i>Assumptions of Linear Regression: No or Little Multicollinearity satisfied\n    <\/i><\/font><\/h2>","da753f9f":"<h2><font color='brown'><i>Assumptions of Linear Regression: Multivariate Normality satisfied<\/i><\/font><\/h2>","12640d7b":"<b>Variance Inflation Factor (VIF): <\/b><font color='red'><i>Each feature is regressed against all the other features.<br>Variance Inflation Factor is defined as VIF = 1\/T. With VIF > 5 there is an indication that multicollinearity may be present; with VIF > 10 there is certainly multicollinearity among the variables. \n","8fa14bb7":"<h2><font color='brown'><i>Assumptions of Linear Regression: Homoscedasticity Satisfied<\/i><\/font><\/h2>","b043189f":"<h2><font color='brown'><i>Assumptions of Linear Regression: No or Little Autocorrelation satisfied\n    <\/i><\/font><\/h2>","f23d0abb":"<h2>Let's have a look at the Advertisement Dataset<\/h2>\n\n<figure>\n  <img src=\"look.jpg\" style=\"width:50%\">\n    <center><figcaption><b>Image Source:<\/b> <a href='https:\/\/image.freepik.com\/free-vector\/computer-concept_1308-35665.jpg'>https:\/\/image.freepik.com\/free-vector\/computer-concept_1308-35665.jpg<\/a><\/figcaption><\/center>\n<\/figure>\n\n","395435ec":"<h3><b>Problem Statement:<\/b><\/h3>\n\n<a href = 'https:\/\/en.wikipedia.org\/wiki\/Advertising'>Advertising<\/a> is a marketing communication that promotes the sell of a product, service or idea. \nThe dataset consists of money spent in TV, Radio and Newspaper ads and corresponding achieved sales in that period. Task is to analyse the dataset and create a predictive model to forecast the expected sales given the advertisement spent. <br>\nDataset Link: <a href = 'https:\/\/www.kaggle.com\/ashydv\/advertising-dataset'>https:\/\/www.kaggle.com\/ashydv\/advertising-dataset<\/a>\n\n<figure>\n  <img src=\"Adverstisement_.png\" style=\"width:75%\">\n<\/figure>","5898b7a1":"<h2> References<\/h2>\n<font color='Blue'><i><br>\n1. <a href='https:\/\/towardsdatascience.com\/assumptions-of-linear-regression-algorithm-ed9ea32224e1'>https:\/\/towardsdatascience.com\/assumptions-of-linear-regression-algorithm-ed9ea32224e1<\/a><br>\n2. <a href='https:\/\/towardsdatascience.com\/linear-regression-and-its-assumptions-ef6e8db4904d'>https:\/\/towardsdatascience.com\/linear-regression-and-its-assumptions-ef6e8db4904d<\/a>","34deb9d3":"\n<font color='red'><i>\nWeight can be in kilogram or grams similarly height can be in cm or meters, it's necessary to bring variables in a scale where the deviation is similar.\n    <\/i><\/font>","231c3c5e":"<h2>Feature Importance<\/h2>"}}