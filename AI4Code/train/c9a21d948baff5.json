{"cell_type":{"1b198c5a":"code","cb66d830":"code","a07a51f0":"code","a089f824":"code","22bcd224":"code","20c2ff6b":"code","090687f6":"code","b6b42eaa":"code","7ea21ad5":"code","de8f8b1b":"code","5a031b2e":"code","fd27de01":"code","cdf469d1":"code","1f0a35fd":"code","c45e53a5":"code","d5eac66e":"code","4c4edf14":"code","8a9f67cc":"code","7198aefd":"code","cd541d3e":"code","a2f27124":"code","8979982f":"code","be1e0e34":"code","0a29b926":"code","89b508cb":"code","442ee5f2":"code","d1ab1a30":"code","f90c1caa":"code","a5a24ee9":"code","44287b90":"code","708385eb":"code","3f5330e8":"code","171f788c":"code","d22bab8a":"code","dd6778a9":"code","34fd41fb":"code","e2803afd":"code","58708d9f":"code","edf84401":"markdown","5b357991":"markdown","c70623f9":"markdown","8908e71b":"markdown","8efb3955":"markdown","07bcc116":"markdown","75643827":"markdown","d405318b":"markdown","33a75390":"markdown","3121e302":"markdown","ef82a22b":"markdown","5b5cbd48":"markdown","bfaefad7":"markdown","52b9533c":"markdown","7a004664":"markdown","2fc3bdc7":"markdown","f5743b5b":"markdown","4c7fca5d":"markdown","fec9b716":"markdown","8512219f":"markdown","5d326e16":"markdown","6ff5fbe8":"markdown","4cb37b6a":"markdown","050f9cf7":"markdown","00addc1a":"markdown","f0c1d635":"markdown","48ce6a55":"markdown","c62643fb":"markdown"},"source":{"1b198c5a":"# import Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib as pyplot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# to see all the comands result in a single kernal \nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n# to increase no. of rows and column visibility in outputs\npd.set_option('display.max_rows', 2000)\npd.set_option('display.max_columns', 2000)\npd.set_option('display.width', 2000)\n\n#To ignore warnings\nimport warnings\nwarnings.simplefilter('ignore')","cb66d830":"#Import data\ntrain = pd.read_csv(r'..\/input\/house-price-prediction-challenge-machine-hack\/Train.csv')\ntest = pd.read_csv(r'..\/input\/house-price-prediction-challenge-machine-hack\/Test.csv')\nsample = pd.read_csv(r'..\/input\/house-price-prediction-challenge-machine-hack\/sample_submission.csv')","a07a51f0":"# Having a look at data and its shape \ntrain.head()\ntest.head()\ntrain.shape ,test.shape ,sample.shape","a089f824":"# Dublicacy Analyses\nduplicate_input_train = train[train.duplicated(subset=['POSTED_BY', 'UNDER_CONSTRUCTION', 'RERA', 'BHK_NO.', 'BHK_OR_RK', 'SQUARE_FT', 'READY_TO_MOVE', 'RESALE', 'ADDRESS', 'LONGITUDE', 'LATITUDE'])]\nduplicate_Rows_train = train[train.duplicated()]\nduplicate_Rows_test = test[test.duplicated()]\ndf=pd.concat([train,test],ignore_index=True )\nduplicate_input_DF = df[df.duplicated(subset=['POSTED_BY', 'UNDER_CONSTRUCTION', 'RERA', 'BHK_NO.', 'BHK_OR_RK', 'SQUARE_FT', 'READY_TO_MOVE', 'RESALE', 'ADDRESS', 'LONGITUDE', 'LATITUDE'])]\nduplicate_input_train.shape ,duplicate_Rows_train.shape ,duplicate_Rows_test.shape ,duplicate_input_DF.shape ","22bcd224":"# to check type of columns and identify whether missing values exist or not\ntrain.info()\ntest.info()","20c2ff6b":"# Genrate a new column substitude it with mean of log of target column, we will use this before submission\ntrain['is_train']=1\ntest['is_train']=0\ntrain['TARGET_log']=np.log1p(train['TARGET(PRICE_IN_LACS)'])\ndf=pd.concat([train,test],ignore_index=True)\ndf['T1_log_dub']=df.groupby(['POSTED_BY', 'UNDER_CONSTRUCTION', 'RERA','BHK_NO.', 'BHK_OR_RK', 'SQUARE_FT', 'READY_TO_MOVE', 'RESALE', 'ADDRESS', 'LONGITUDE', 'LATITUDE'])['TARGET_log'].transform('mean')\ndf['t1_dub']=np.exp(df['T1_log_dub'])-1\ntrain=df[df['is_train']==1]\ntest=df[df['is_train']==0]\ntest.shape\ntest.t1_dub.isna().sum()","090687f6":"t2=test[test['t1_dub'].isna() == True]\nt2[t2.duplicated()].shape","b6b42eaa":"#Conerting categorical variable to numeric\ntrain['BHK_OR_RK']=train['BHK_OR_RK'].replace({'BHK':0,'RK':1})\ntrain['POSTED_BY']=train['POSTED_BY'].replace({'Owner':0,'Dealer':1,'Builder':2})\ntest['BHK_OR_RK']=test['BHK_OR_RK'].replace({'BHK':0,'RK':1})\ntest['POSTED_BY']=test['POSTED_BY'].replace({'Owner':0,'Dealer':1,'Builder':2})","7ea21ad5":"# found out that there is no. missing value and only one address as object type variable\n# Target varibale distribution \ntrain['TARGET(PRICE_IN_LACS)'].plot(kind = 'density', title = 'Price Distribution')","de8f8b1b":"# Transforming target varible(log transformation), because target is to optimize Root mean square log error\n# and checking log transformed varibale distribution\n#train['TARGET_log']=np.log1p(train['TARGET(PRICE_IN_LACS)'])\ntrain['TARGET_log'].plot(kind = 'density', title = 'log of Price Distribution')","5a031b2e":"# Analysing distribution in Categorical varibales \ncat_cols = ['POSTED_BY', 'UNDER_CONSTRUCTION', 'RERA', 'BHK_NO.', 'BHK_OR_RK', 'RESALE']\nfig, axes = plt.subplots(1,6, figsize=(24, 10))\n\nfor i, c in enumerate(['POSTED_BY', 'UNDER_CONSTRUCTION', 'RERA', 'BHK_NO.', 'BHK_OR_RK', 'RESALE']):\n    _ = train[c].value_counts()[::-1].plot(kind = 'pie', ax=axes[i], title=c, autopct='%.0f', fontsize=18)\n    _ = axes[i].set_ylabel('')\n    \n_ = plt.tight_layout()","fd27de01":"\n# to get exact values of distribution \n# cat_col=['POSTED_BY', 'UNDER_CONSTRUCTION', 'RERA', 'BHK_NO.', 'BHK_OR_RK', 'RESALE']\n# for col in cat_col:\n#  train[col].value_counts()\/len(train)","cdf469d1":"sns.jointplot(x=np.log1p(train['SQUARE_FT']), y=train['TARGET_log'])","1f0a35fd":"sns.jointplot(x=(train['BHK_NO.']), y=train['TARGET_log'],kind='reg')","c45e53a5":" train.groupby('BHK_NO.')['TARGET_log'].mean().plot()","d5eac66e":"sns.violinplot(data=train, x=\"POSTED_BY\", y=\"TARGET_log\", hue=\"RESALE\",split=True, inner=\"quart\", linewidth=1)","4c4edf14":"sns.scatterplot(x=np.log1p(train['LATITUDE']), y=train['TARGET(PRICE_IN_LACS)'])","8a9f67cc":"sns.scatterplot(x=np.log1p(train['LONGITUDE']), y=train['TARGET(PRICE_IN_LACS)'])","7198aefd":"# Checking correlation\nplt.figure(figsize=(15, 8))\nsns.heatmap(train.corr(),annot=True)","cd541d3e":"train['sq_per_room']=train['SQUARE_FT']\/train['BHK_NO.']\ntest['sq_per_room']=test['SQUARE_FT']\/test['BHK_NO.']","a2f27124":"# Extracting name of city and locality of house\ntrain['City']=train['ADDRESS'].str.split(',').str.get(-1)\ntrain['locality']=train['ADDRESS'].str.split(',').str.get(-2)\ntest['City']=test['ADDRESS'].str.split(',').str.get(-1)\ntest['locality']=test['ADDRESS'].str.split(',').str.get(-2)","8979982f":"# train['City'].value_counts()\/len(train)\n# Maharashtra is not a city but coming 5% times, we can replace that for furthur improvement, can use above command to see","be1e0e34":"train['is_train']=1\ntest['is_train']=0\ndf=pd.concat([train,test])\ndf['SQUARE_FT']=np.log1p(df['SQUARE_FT'])\ndf['loc_'+'SQUARE_FT'+'mean'] = df.groupby(['locality'])['SQUARE_FT'].transform('mean')\ndf['loc_'+'SQUARE_FT'+'median'] = df.groupby(['locality'])['SQUARE_FT'].transform('median')\ndf['loc_'+'SQUARE_FT'+'_max'] = df.groupby(['locality'])['SQUARE_FT'].transform('max')\ndf['loc_'+'SQUARE_FT'+'_min'] = df.groupby(['locality'])['SQUARE_FT'].transform('min')\ndf['loc_'+'SQUARE_FT'+'_var'] = df.groupby(['locality'])['SQUARE_FT'].transform('std')\ndf['city_'+'SQUARE_FT'+'_mean'] = df.groupby(['City'])['SQUARE_FT'].transform('mean')\ndf['city_'+'SQUARE_FT'+'_median'] = df.groupby(['City'])['SQUARE_FT'].transform('median')\ndf['city_'+'SQUARE_FT'+'_max'] = df.groupby(['City'])['SQUARE_FT'].transform('max')\ndf['city_'+'SQUARE_FT'+'_min'] = df.groupby(['City'])['SQUARE_FT'].transform('min')\ndf['city_'+'SQUARE_FT'+'_var'] = df.groupby(['City'])['SQUARE_FT'].transform('std')\ndf['loc_'+'city_'+'SQUARE_FT'+'mean'] = df.groupby(['locality','City'])['SQUARE_FT'].transform('mean')\ndf['loc_'+'count'] = df.groupby(['locality'])['locality'].transform('count')\ndf['City_'+'count'] = df.groupby(['City'])['City'].transform('count')\ndf['city'+'loc_count'] = df.groupby(['City'])['locality'].transform('count')\ndf['city'+'loc_unique'] = df.groupby(['City'])['locality'].transform('nunique')\ndf['PB_UC']=train['POSTED_BY']*3+train['UNDER_CONSTRUCTION']\n#df['loc_'+'BHK_NO'+'_mean'] = df.groupby(['loc'])['SQUARE_FT'].transform('mean')\ntrain=df[df['is_train']==1]\ntest=df[df['is_train']==0]","0a29b926":"# to see same address with different city name can use below command\nencode=df.groupby(['locality'])['City'].unique()\ndf['unique']=df['locality'].map(encode)\ndf['count_city']=df.groupby(['locality'])['City'].transform('nunique')\ndf.sort_values(by='count_city',ascending=False).head(3)","89b508cb":"train.shape\ntrain.drop_duplicates(subset=['POSTED_BY', 'UNDER_CONSTRUCTION', 'RERA', 'BHK_NO.', 'BHK_OR_RK', 'SQUARE_FT', 'READY_TO_MOVE', 'RESALE', 'ADDRESS', 'LONGITUDE', 'LATITUDE','TARGET_log'],keep ='last', inplace = True,ignore_index=True)\ntrain.shape","442ee5f2":"#Importing Packages\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV,KFold\nfrom xgboost.sklearn import XGBRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor","d1ab1a30":"test.head()","f90c1caa":"def run_gradient_boosting(clf,k, fit_params, train, test, features):\n  N_SPLITS = k\n  oofs = np.zeros(len(train))\n  preds = np.zeros((len(test)))\n\n  folds = KFold(n_splits = N_SPLITS,random_state=2021)\n\n  for fold_, (trn_idx, val_idx) in enumerate(folds.split(train, train[TARGET_COL])):\n    print(f'\\n------------- Fold {fold_ + 1} -------------')\n    target=train[TARGET_COL]\n    X_trn, y_trn = train[features].iloc[trn_idx], target.iloc[trn_idx]\n\n    X_val, y_val = train[features].iloc[val_idx], target.iloc[val_idx]\n\n    X_test = test[features]\n\n    scaler = StandardScaler()\n    _ = scaler.fit(X_trn)\n\n    X_trn = scaler.transform(X_trn)\n    X_val = scaler.transform(X_val)\n    X_test = scaler.transform(X_test)\n    \n    _ = clf.fit(X_trn, y_trn, eval_set = [(X_val, y_val)], **fit_params)\n    preds_val = clf.predict(X_val)\n    preds_test = clf.predict(X_test)\n    preds_test1=(np.exp(preds_test)-1)\n    fold_score =  np.sqrt(mean_squared_error(y_val,preds_val)) \n    print(f'\\n RMLSE score for validation set is {fold_score}')\n\n    oofs[val_idx] = (np.exp(preds_val)-1)\n    preds += preds_test1 \/ N_SPLITS\n\n\n  oofs_score = np.sqrt(mean_squared_error(target, np.log1p(oofs)))\n  print(f'\\n\\n rmlse score for oofs is {oofs_score}')\n\n  return oofs, preds","a5a24ee9":"train.columns","44287b90":"# Selecting Columns to take in the model\nmodel_col=['POSTED_BY', 'UNDER_CONSTRUCTION', 'RERA','BHK_NO.', 'SQUARE_FT', 'RESALE','LONGITUDE', 'LATITUDE', 'BHK_OR_RK', 'loc_SQUARE_FTmean', 'loc_SQUARE_FT_max', 'loc_SQUARE_FT_min', 'loc_SQUARE_FT_var', 'city_SQUARE_FT_mean', 'city_SQUARE_FT_max', 'city_SQUARE_FT_min', 'city_SQUARE_FT_var','cityloc_count']\ncat_col=['POSTED_BY', 'UNDER_CONSTRUCTION', 'RERA', 'BHK_NO.', 'BHK_OR_RK', 'RESALE']\ntar_col=['TARGET(PRICE_IN_LACS)']","708385eb":"from sklearn.model_selection import GridSearchCV,StratifiedKFold,KFold\n\nxgb = XGBRegressor(n_estimators = 8000,\n#                        learning_rate = 0.05,\n#                       colsample_bytree = 0.76,\n                        )\nfit_params = {'verbose': False, 'early_stopping_rounds': 100}\nTARGET_COL= 'TARGET_log'\nfeatures =model_col\nxgb_oofs, xgb_preds = run_gradient_boosting(xgb,10, fit_params, train, test, features)","3f5330e8":"# Here You can even extract your training data and analyse on which outcomes you are getting most error\n# and take further step accordingly\ntrain['pred']=xgb_oofs\ntrain['error']=(train['TARGET_log']-np.log1p(train['pred']))**2\nvis=train[model_col]\nvis.to_csv('train_out.csv',index=False)","171f788c":"\nlgb = LGBMRegressor(n_estimators=5000, importance_type='gain',\n#                          learning_rate = 0.05,\n#                          colsample_bytree = 0.76,\n                        )\nfit_params = {'verbose': False, 'early_stopping_rounds': 100}\nTARGET_COL= 'TARGET_log'\nfeatures =model_col\nlgb_oofs, lgb_preds = run_gradient_boosting(lgb,10, fit_params, train, test, features)","d22bab8a":"# You can even see Plot of your error with iteration in catboost, by adding plot=True in fit_parametrs\nfrom catboost import CatBoostRegressor\ncb = CatBoostRegressor(iterations=10000,\n                       learning_rate = 0.1,\n#                         colsample_bytree = 0.76,\n                        )\nfit_params = {'verbose': False , 'early_stopping_rounds': 100}\nTARGET_COL= 'TARGET_log'\nfeatures =model_col\ncb_oofs, cb_preds = run_gradient_boosting(cb,10, fit_params, train, test, features)","dd6778a9":"# you can change predictions to xgb_pred and lgb_pred to extract their outputs\ntest['t2']=cb_preds\nsample['TARGET(PRICE_IN_LACS)']=np.where(test['t1_dub'].isna()==True,test['t2'],test['t1_dub'])\nsample.to_csv('10fold_ccccb_2mod_final.csv',index=False)","34fd41fb":"# Checking Feature Importance\n# you can also change cb to xgb or lgb to see their imporatance of variable\nfeat_importances = pd.Series(cb.feature_importances_, index=model_col)\nfeat_importances.nlargest(20).plot(kind='barh')\nplt.show()","e2803afd":"avg= 0.4*xgb_oofs +0.2* lgb_oofs + 0.4*cb_oofs\nnp.sqrt(mean_squared_error(train[TARGET_COL], np.log1p(avg)))","58708d9f":"#avg_test= 0.34*xgb_preds+ 0.33*lgb_preds+0.33*cb_preds\navg_test= 0.4*xgb_preds+ 0.2*lgb_preds+0.4*cb_preds\ntest['t2']=avg_test\nsample['TARGET(PRICE_IN_LACS)']=np.where(test['t1_dub'].isna()==True,test['t2'],test['t1_dub'])\nsample.to_csv('10_fold_3_424_equal_model_ensembeling.csv',index=False)","edf84401":"### Things which may work\n1. You can observe that city and locality turned out to very important variable in this competition, but there are few problem with these variables, same address are given in 2 or 3 different form multiple times, like 'Rajiv Gandhi Nagar,Maharashtra' and 'Rajiv Gandhi Nagar,Mumbai', 'Anand Nagar,Maharashtra'and 'Anand Nagar,Pune'. Correcting them can boost your score (prefer to always have city name instead of state. Maharashtra only come around 5% times in data)\n1. locality and City variables encoded with square_ft work very well, you can also try group of other variables like locality and POSTED_BY with SQUARE_FT or even grouped of other variable with locality and city variable. \n1. There is problem also with some variables like SQUARE_FT, it is less than 90 (even less tha 10 for few), you can improve that values(by replacig them to missing or by multipling by a factor of 10 or 100), You may also bin values of BHK_no. as with even higher BHK_NO. values mean price is decreasing. Same way  Longitude and latitude data is not in range of Indian latitude and longitude for some observation, whereas data is only of india. (Indian latitude range 8\u00b04' north to 37\u00b06' north and longitude range 68\u00b07' east to 97\u00b025'). But be careful there may be pattern in data like whenever latitude and longitude are out of range price are less compare to other. Additing to this here Latitudes as given as longitude and vice-versa.\n1. I have removed only those dublicate columns from train data which are having same output, but there are also few columns( around 100) which have same input but different output(you can find dublicacy by ignoring adress columns also). Additinally I have remove Dublicates just before training. Use them while genrating new features, you can try by removing before genrating new features.\n1. Genrating few more fetures, and better tuning of all parameters and even some better way of ensembeling or stacking can also help.","5b357991":"## Special Thanks to few people for clearning my doubts and sharing their insightful Notebooks (of some other competitions)\n### @NikhilMishra @gcspkmdr @Vetrivel-PS @Anil Betta @Piyush Raj Gupta","c70623f9":"Remove approax 1% dublicate data ","8908e71b":"Longitude and latitude values which are out of indian range are giving less values, that why i not try to thing of any method to improve these values, but still can and save our outcomes in a different column, that is the longitude latitude are corrct or not","8efb3955":"* we found out that 99% of houses are either 1,2,3,or 4 BHK \n* almost all the houses are BHK ","07bcc116":"0.2785 oof, 0.2660 public leaderboard, 0.2672 private leaderboard,\n* you can give categorical_column also as categorical_column in lgb it can improve your score","75643827":"### This  is to get Rough idea of how much each step matters, \n* I have not fixed random seed in kfold that time, so score may vary a bit for you\n\n* 0.2917 ,5 fold only 2 genrated features city and loc sqrt_mean\n* 0.2911     , 5 fold only 2 genrated features city and loc log_sqrt_mean\n* 0.2892     , 5 fold with 8 genrated fetures mean , max , min, var, of city and loc with log_sqrt \n* Not used ,0.2879   , 5 fold with 9 variables new variable city mean if count>2\n* Not used ,0.2891   , 5 fold with 9 variables new variable city mean if count>1\n* Not used ,0.2912  , added variable Loc count \n* Not used ,0.2892  , added city loc count \n* 0.2872  , added cityloc unique values \n* Not used  ,0.2923, added median of sqrt grouped with city and loc \n----------------------------------------------------\n### 0.2632 on public leader board # 0.2823 on validation set # Kfold =10 \n----------------------------------------------------","d405318b":"# Model Fitting","33a75390":"# Averaging of 3  models to get best score ","3121e302":"Most of very High price houses are which are not for RESALE.","ef82a22b":"* If you reach till here Hopefully you have learned or atleast reviced few concept, which may help you in your next upcoming competition\n### Don't forget to Upvote this Notebook and share your feedback in comment section\n* if i get positive responce, soon i will come up with few more notebooks and upgrade this one also with methods i mention in \"Things which may work\" section","5b5cbd48":"### Things That Work\n1. Features (First extracted city and locality features from address column)\n    1. SQUARE_FT variable mean,max, min, variance encoded with locality\n    1. SQUARE_FT variable mean,max, min, variance encoded with City\n    1. count of unique locality in each city\n1. K-fold, I have used K as 10 here\n1. Ensembeling of 3, 10 fold models- Light Gradient Bossting, Extreme Gradient Boosting, Catboost (with large no. of n_estimators(iterations in catboost) and early_stopping_rounds )\n1. Remove dublicate rows from train data while training and used mean of log target variable in test data which are exactly dublicate from train set.\n1. Fitting Log of target variable in training and then taking its inverse while submitting. because by default models minimize RMSE and by doing so it will minimize RMSLE, which is exact evaluation metric for this competition.","bfaefad7":"### Just by using above 5 steps you can achieve Rank 7, I myself has achieved rank 9 in this competition.","52b9533c":"You can increase the value under head to 100 or 1000 to see more locality values with distinct city values, but remember that, few of them are genuine, there is possiblity that in two different city same name of locality can exist, but sometimes some arbitary name is given or state name is given instead of City you can correct that. ","7a004664":"* In catboost learning rate decreases as iterations increase, but i have kep it constant for faster result\n* can also added categorical feature in fit_params to improve your model\n* 0.2757 on out of fold, 0.2644 on public leaderboard, 0.2673 on private leaderboard","2fc3bdc7":"-------------------------------------------------\n### 0.2724 on oof , 0.2610 on public leaderboard, 0.26311 on private leaderboard\n### You can Secured Rank 7 in this competiotion by this solution \n-------------------------------------------------\n* may be using stacking will boost score bit more.","f5743b5b":"### Things which not work\n* After Tuning model parameters oof (out of fold) score improves but but score on leaderboard is decreasing, because tuning is resulting in making all different fold models similar and the benifit which we are getting through ensembeling of multiple fold, we are losing that. ( may be tring with lesser fold while tuning can help)\n* Added SQUARE_FT variable median encoded with City and locality","4c7fca5d":"To decide weights by this way averaging oof is not totally correct, because on each test set point outcome is average of each fold model, but here we are using differnt fold outputs for different outcomes in oof set. ","fec9b716":"# Feature Engineering","8512219f":"We found that as we expect TARGET_log should increase with increasing BHK_NO. values, it is not following with that. One reason for that can also be there are very few houses(~1%) with BHK_NO. greater than 4. so we can also bin BHK_no. column here","5d326e16":"But still test data has lot of missing values","6ff5fbe8":"2286 test data values are filled","4cb37b6a":"find out that there are few houses with very less value of SQUARE_FT(which are ven not possible)","050f9cf7":"# This is a Machine Hack Competition\n## House Pricing Prediction Challenge\n* Now this competion is closed but you can still make submissions if you wish.\n* Link to Competition-https:\/\/www.machinehack.com\/hackathons\/house_price_prediction_beat_the_benchmark\/overview\n* Dataset for kaggle notebook- https:\/\/www.kaggle.com\/jassican\/house-price-prediction-challenge-machine-hack","00addc1a":"0.2821 on oof, 0.2632 on public, 0.2657 on private can get rank 11 with this","f0c1d635":"### INDEX\n* [Basic Visulization](#1)\n* [Feature Engineering](#2)\n* [Model Fitting](#3)","48ce6a55":" **Basic Visulization**","c62643fb":"READY_TO_MOVE and UNDER_CONSTRUCTION have correlation -1 so both giving same information, we can remove one of variable "}}