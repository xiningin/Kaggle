{"cell_type":{"11902abd":"code","81ecb108":"code","56aee381":"code","bb2e50f7":"code","8dbc55e7":"code","b1617b40":"code","362d9410":"code","7214be89":"code","57389d53":"code","5f1f5568":"code","121831a3":"code","12032d80":"code","09fae907":"markdown","7f654f5e":"markdown","6b310527":"markdown","3f41d7de":"markdown","13a3aee5":"markdown","ddbe9e9d":"markdown","7d7fc588":"markdown","76f74542":"markdown","7e2567e5":"markdown"},"source":{"11902abd":"import re\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport transformers\nimport tensorflow.keras as keras \nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npd.set_option('display.max_columns', None) \npd.set_option('display.max_rows', 20)  \npd.set_option('display.max_colwidth', -1)  ","81ecb108":"df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\nprint(len(df))\nprint(df.columns)\n\ndf","56aee381":"sns.countplot(x = 'target', data = df)\nplt.xlabel('Classes')\nplt.ylabel('Count')\nplt.show()","bb2e50f7":"def clean_text(text):\n    \n    # Remove http\/https links\n    text = re.sub(r'http\\S+', '', text)  \n    \n    # Remove mentions\n    text = re.sub(r\"(?:\\@)\\w+\", '', text)\n    \n    # Remove any characters that is not and alphabet or number\n    text = re.sub(r'[^a-zA-Z0-9\\'.,?$&\\s]', '', text)  \n    \n    # Lower case all the alphabets\n    text = text.lower()\n    \n    return text\n\n# Let's view some random tweets with their cleaned versions\nfor i in range(10):\n    index = np.random.randint(low=0, high=len(df))\n    print('Raw text:', df['text'][index])\n    print('Cleaned text:', clean_text(df['text'][index]))\n    print('Label: ', df['target'][index], '\\n')","8dbc55e7":"# Tokenizing the sentences \ndef convert_to_features(data, tokenizer, max_len=None):\n    data = data.replace('\\n', '')\n    \n    # Return a dictionary containing 'input_ids', 'attention_mask' & 'token_type_ids' each of shape (1, max_len)\n    if max_len is not None:\n        tokenized = tokenizer.encode_plus(\n            data, \n            padding ='max_length',\n            max_length=max_len, \n            truncation=True,\n            return_tensors='np',\n            return_attention_mask=True,\n            return_token_type_ids=True,\n        )\n        \n    else:\n        tokenized = tokenizer.encode_plus(\n            data,  \n            return_tensors='np',\n            return_attention_mask=True,\n            return_token_type_ids=True,\n        )\n    return tokenized\n\n# Create dataset for data with labels\ndef create_inputs_with_targets(x, y, tokenizer, max_len=128):\n    \n    dataset_dict = {\n        \"input_ids\": [],\n        \"attention_mask\": [],\n        'labels': []\n    }\n    \n    for sentence, label in tqdm(zip(x,y)):\n        cleaned_sentence = clean_text(sentence)\n        temp = convert_to_features(cleaned_sentence, tokenizer, max_len=max_len)\n        dataset_dict[\"input_ids\"].append(temp[\"input_ids\"][0])\n        dataset_dict[\"attention_mask\"].append(temp[\"attention_mask\"][0])\n        dataset_dict[\"labels\"].append(label)\n\n    x = [\n        np.array(dataset_dict[\"input_ids\"]),\n        np.array(dataset_dict[\"attention_mask\"]),\n    ]\n    \n    y = np.array(dataset_dict['labels'])\n    \n    return x, y\n\n# Create dataset for data without labels\ndef create_inputs_without_targets(x, tokenizer, max_len=128):\n    \n    dataset_dict = {\n        \"input_ids\": [],\n        \"attention_mask\": [],\n    }\n    \n    for sentence in tqdm(x):\n        cleaned_sentence = clean_text(sentence)\n        temp = convert_to_features(cleaned_sentence, tokenizer, max_len=max_len)\n        dataset_dict[\"input_ids\"].append(temp[\"input_ids\"][0])\n        dataset_dict[\"attention_mask\"].append(temp[\"attention_mask\"][0])\n\n    x = [\n        np.array(dataset_dict[\"input_ids\"]),\n        np.array(dataset_dict[\"attention_mask\"]),\n    ]\n    \n    return x","b1617b40":"# I will be using the BERT base model here\nbase_model = 'bert-base-uncased'\n\nbert_tokenizer = transformers.BertTokenizer.from_pretrained(base_model)\nmax_len = 80","362d9410":"# Splitting the dataframe into 80-20% training and validation dataframes\nvalidation_data_indices = df.sample(frac=0.2).index\nvalidation_df = df.loc[validation_data_indices, :].reset_index(drop=True)\ntrain_df = df.drop(validation_data_indices, axis=0).reset_index(drop=True)\ntest_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\n\nx_train, y_train = create_inputs_with_targets(list(train_df['text']), list(train_df['target']), tokenizer=bert_tokenizer, max_len=max_len)\nx_val, y_val = create_inputs_with_targets(list(validation_df['text']), list(validation_df['target']), tokenizer=bert_tokenizer, max_len=max_len)\nx_test = create_inputs_without_targets(list(test_df['text']), tokenizer=bert_tokenizer, max_len=max_len)\n\nprint('Training dataframe size: ', len(train_df))\nprint('Validation dataframe size: ', len(validation_df))\nprint('Test dataframe size: ', len(test_df))","7214be89":"def create_model(model_name, max_len=128):\n    \n    seed = 500\n    my_init = tf.keras.initializers.glorot_uniform(seed)\n    max_len = max_len\n    \n    # BERT encoder\n    encoder = transformers.TFAutoModel.from_pretrained(model_name)\n    \n    # UnFreeze the base model weights\n    encoder.trainable = True\n\n    # Define input shapes \n    input_ids = keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n    attention_mask = keras.layers.Input(shape=(max_len,), dtype=tf.int32)\n     \n    sequence_output = encoder(input_ids, attention_mask=attention_mask)['last_hidden_state']\n    \n    # Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n    bi_lstm = tf.keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True))(sequence_output)\n    \n    # Applying hybrid pooling approach \n    avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n    max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n    \n    concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n    dropout = tf.keras.layers.Dropout(0.3)(concat)\n    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dropout)\n    \n    model = tf.keras.models.Model(\n        inputs=[input_ids, attention_mask], outputs=[output]\n    )\n    \n    return model","57389d53":"epochs = 20\nlr = 2e-4\n\n# Initialize the model on tpu\nuse_tpu = True\nif use_tpu:\n    # Create distribution strategy\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n    # Create model\n    with strategy.scope():\n        model = create_model(base_model, max_len=max_len)\n        \n        optimizer = keras.optimizers.Adam(learning_rate=lr)\n    \n        model.compile(optimizer=optimizer,\n                      loss = keras.losses.BinaryCrossentropy(), \n                      metrics= [keras.metrics.BinaryAccuracy()])\n        \nelse:\n    model = create_model()\n\nmodel.summary()","5f1f5568":"# Train the model\nmy_callbacks = [keras.callbacks.EarlyStopping(monitor='val_binary_accuracy', patience=2, mode='max', restore_best_weights=True)]\n\nhist = model.fit(x_train, \n                 y_train,\n                 validation_data = (x_val, y_val),\n                 epochs= epochs, \n                 batch_size= 128,\n                 callbacks = my_callbacks,\n                 verbose= 1)","121831a3":"predictions = model.predict(x_test)\n\nids = list(test_df['id'])\ntarget = [round(i[0]) for i in predictions]\n\nsub = pd.DataFrame({'id':ids, 'target':target}, index=None)\nsub.to_csv('submission.csv', index=False)\nsub","12032d80":"# !git clone https:\/\/github.com\/mitramir55\/Kaggle_NLP_competition.git\n# perfect = pd.read_csv('Kaggle_NLP_competition\/perfect_submission.csv')\n# cheat = list(perfect['target'])","09fae907":"# The Dataset\n### 1-real disaster\n### 0-not a disaster","7f654f5e":"The model structure implemented in this notebook is similar to this:\n\n![](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2019\/09\/bert_pipeline2.png)","6b310527":"# Do not submit this\n\n### Here is the perfect submission file. This is only put here in case you want to test your model's performance.","3f41d7de":"# Building The Model","13a3aee5":"# Tokenizing the Inputs","ddbe9e9d":"# Text Cleaning and Preprocessing","7d7fc588":"# Initializing and Training the Model on TPU ","76f74542":"# Submission","7e2567e5":"# BERT\n\nBERT stands for Bidirectional Encoder Representations from Transformers. BERT is a \u201cdeeply bidirectional\u201d model. Bidirectional means that BERT learns information from both the left and the right side of a token\u2019s context during the training phase.\n\nThe bidirectionality of a model is important for truly understanding the meaning of a language. Let\u2019s see an example to illustrate this. There are two sentences in this example and both of them involve the word \u201cbank\u201d:\n![BERT captures both the left and right context](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2019\/09\/sent_context.png)\n\nIf we try to predict the nature of the word \u201cbank\u201d by only taking either the left or the right context, then we will be making an error in at least one of the two given examples.\n\nOne way to deal with this is to consider both the left and the right context before making a prediction. That\u2019s exactly what BERT does! Traditionally, we had language models either trained to predict the next word in a sentence (right-to-left context used in GPT) or language models that were trained on a left-to-right context or a shallow concatenation of these two (ElMo). This made the models susceptible to errors due to loss in information.\n\n![](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2019\/09\/bert-vs-openai-.jpg)\n\nIt\u2019s evident from the above image: BERT is bi-directional, GPT is unidirectional (information flows only from left-to-right), and ELMO is shallowly bidirectional.\n\nBERT is pre-trained on two NLP tasks:\n\n* Masked Language Modeling\n*  Next Sentence Prediction\n\n(This excerpt is taken from here:\nhttps:\/\/www.analyticsvidhya.com\/blog\/2019\/09\/demystifying-bert-groundbreaking-nlp-framework\/\nFor detailed explanation please visit the link.)"}}