{"cell_type":{"5065438d":"code","d8d517e7":"code","990f1f36":"code","df096ed8":"code","0e6c4321":"code","58c37bbc":"code","fd801b91":"code","9196f977":"code","f0bfb788":"code","7b628bfe":"code","19946e55":"code","99fd6d7d":"code","9c0e696e":"code","1b9370b3":"code","586a8f8b":"code","b120a904":"code","3625324c":"code","1ac0f943":"code","dd1b0931":"code","231cd1f1":"markdown","4072e82f":"markdown","502c41fa":"markdown","1f2d7bf7":"markdown","582427b0":"markdown","ceee3acc":"markdown","06638ced":"markdown","248228b6":"markdown","a8230e3d":"markdown"},"source":{"5065438d":"from fastai.tabular.all import *","d8d517e7":"train = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")","990f1f36":"train = train.drop(\"id\", axis=1)\ntrain","df096ed8":"# specify variables\ntarget_var = \"target\"\ncat_var = [col for col in train.columns if \"cat\" in col]\ncont_var = [col for col in train.columns if \"cont\" in col]\n\n# setting preprocessing\npreprocessing = [Categorify, Normalize]\n\n# split the dataset into train and validation set\nsplits = RandomSplitter()(train)","0e6c4321":"to = TabularPandas(train, preprocessing, cat_var, cont_var, y_names=target_var, splits=splits)\nto.show(3)","58c37bbc":"to.items.head(3)","fd801b91":"X_train, y_train = to.train.xs, to.train.y\nX_valid, y_valid = to.valid.xs, to.valid.y","9196f977":"X_train.shape, X_valid.shape","f0bfb788":"def get_val_error(model):\n    y_hat = model.predict(X_valid)\n    return mean_squared_error(y_valid, y_hat, squared=False)","7b628bfe":"from sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(\n    n_estimators=40, \n    max_samples=50_000, \n    max_features=0.5, \n    min_samples_leaf=5,\n    n_jobs=-1,\n    random_state=1\n).fit(X_train, y_train)","19946e55":"get_val_error(rf)","99fd6d7d":"feat_important = pd.DataFrame(\n    {'cols':train.columns[:-1], 'imp':rf.feature_importances_}\n).sort_values('imp', ascending=False)\nfeat_important","9c0e696e":"feat_important.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False);","1b9370b3":"col_keep = feat_important[feat_important.imp > 0.005].cols\nX_train = X_train[col_keep]\nX_valid = X_valid[col_keep]","586a8f8b":"test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\").drop('id',axis=1)\nto_tst = to.new(test)\nto_tst.process()","b120a904":"to_tst = to_tst[col_keep]","3625324c":"X = pd.concat([X_train, X_valid], axis=0).reset_index(drop=True)\ny = pd.concat([y_train, y_valid]).reset_index(drop=True)","1ac0f943":"from sklearn.model_selection import KFold\nfrom xgboost import XGBRegressor\n\nbest_params = {\n    'n_estimators': 9800,\n    'max_depth': 2,\n    'learning_rate': 0.07363768077193145,\n    'gamma': 0.4,\n    'min_child_weight': 1,\n    'subsample': 0.7912492436244456,\n    'colsample_bytree': 0.1613480080803224,\n    'reg_alpha': 12.65778876193281,\n    'reg_lambda': 50.25603582806218\n}\n\nfinal_predictions = []\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfor i, j in kfold.split(X, y):\n    xgb_reg_final = XGBRegressor(\n        **best_params,\n        n_jobs=-1,\n    )\n    \n    xgb_reg_final.fit(\n        X.iloc[i, :], y[i], \n        early_stopping_rounds=300, \n        eval_set=[(X.iloc[j,:], y[j])],\n        verbose=0\n    )\n    \n    final_predictions.append(xgb_reg_final.predict(to_tst))\n\npreds = np.mean(np.column_stack(final_predictions), axis=1)","dd1b0931":"sub = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")\nsub[\"target\"] = preds\nsub.to_csv(\"submissions.csv\", index=False)","231cd1f1":"# Eliminate features with Random Froest","4072e82f":"Create TabularPandas object","502c41fa":"# XGBoost tuned with optuna","1f2d7bf7":"These hyperparameters were obtrained with Optuna in this [notebook](https:\/\/www.kaggle.com\/hongpeiyi\/tuning-xgboost-with-optuna).","582427b0":"get training and validation set","ceee3acc":"eliminate variables with low feature importance","06638ced":"get the test set","248228b6":"# Date setup","a8230e3d":"## Feature importance"}}