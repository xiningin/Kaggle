{"cell_type":{"25aeb041":"code","dcad4c89":"code","d2a017d7":"code","840d02cf":"code","62d38ee6":"code","af800f35":"code","5a37767f":"code","c1dfec45":"code","fb35b679":"code","a2b5046f":"markdown","a7133c9a":"markdown","9196f1a7":"markdown","5e6083b1":"markdown","a3c613bb":"markdown"},"source":{"25aeb041":"import os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.preprocessing import Normalizer, StandardScaler, MinMaxScaler, MaxAbsScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.linear_model import ElasticNet, LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import accuracy_score\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","dcad4c89":"# Load in the data\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","d2a017d7":"# Let's visualize the distribution of the numerical features\ng = sns.pairplot(train[['Survived', 'Pclass', 'Age', 'Fare', 'Cabin']], hue='Survived')\ng.map_diag(sns.histplot)\ng.map_offdiag(sns.scatterplot)\ng.add_legend();","840d02cf":"def process_data(df, name='X', training_cols=None):\n    \n    ''' Takes as input the raw Titanic data and returns a dataframe prepared for a scikit-learn Pipeline process. '''\n    \n    temp = df.copy().drop(columns=['PassengerId'], axis=1)\n\n    # Convert Sex column to binary value\n    temp['Sex'] = temp['Sex'].apply(lambda x: 0 if x == 'male' else 1)\n    \n    # Impute missing age values based on Pclass mean. \n    temp['Age'] = temp[['Age', 'Pclass']].groupby('Pclass').transform(lambda x: x.fillna(x.mean()))\n    \n    # Impute missing Embarked stats by mode.\n    temp['Embarked'] = temp['Embarked'].fillna(value=temp['Embarked'].mode().iloc[0])\n\n    # Impute missing Fare values based on Pclass mean.\n    temp['Fare'] = temp[['Fare', 'Pclass']].groupby('Pclass').transform(lambda x: x.fillna(x.mean()))\n    \n    # Feature engineering\n    temp['Name Length'] = temp['Name'].str.len()\n    temp['Party Size'] = (temp['SibSp'] + temp['Parch'])\n    temp['Title'] = temp['Name'].apply(lambda s: s.split(',')[1].split('.')[0])\n    temp['Cabin Letter'] = temp['Cabin'].fillna(value='N').apply(lambda s: s[0])\n\n    # One-hot encode the Pclass, Embarked, Cabin, and Title columns\n    temp = pd.merge(temp.drop(columns=['Pclass'], axis=1), pd.get_dummies(temp['Pclass'], prefix='Pclass'),\n                    how='left', left_index=True, right_index=True)\n    temp = pd.merge(temp.drop(columns=['Embarked'], axis=1), pd.get_dummies(temp['Embarked'], prefix='Embarked'),\n                    how='left', left_index=True, right_index=True)\n    temp = pd.merge(temp.drop(columns=['Title'], axis=1), pd.get_dummies(temp['Title'], prefix='Title'),\n                    how='left', left_index=True, right_index=True)\n    temp = pd.merge(temp.drop(columns=['Cabin Letter'], axis=1), pd.get_dummies(temp['Cabin Letter'], prefix='CabinLetter'),\n                    how='left', left_index=True, right_index=True)\n    \n    # When processing testing data, ensure that all columns we trained on are available here:\n    if name == 'X' and training_cols is not None:\n        \n        for col in training_cols:\n            if col not in temp.columns:\n                temp[col] = 0\n                \n        # Refine temp to ensure we only return columns we trained on\n        temp = temp[training_cols]\n        \n        return temp\n\n    if name == 'X':\n        # Refine dataset for just those features to be used in model training.\n        return temp[['Sex', 'Age', 'Party Size', 'Name Length', 'SibSp', 'Parch', 'Fare'] + [c for c in temp.columns if '_' in c]]\n    else:\n        return temp[['Survived']]","62d38ee6":"# Generate training and testing data. \nX_train, X_test, y_train, y_test = train_test_split(process_data(train, name='X'), process_data(train, name='Y'),\n                                                    train_size=0.70, stratify=process_data(train, name='Y'), random_state=23)\n\ntraining_columns = X_train.columns","af800f35":"### Model 1 - SVC ###\n# Initialize parameter dictionary to tune, instantiate SVC Pipeline + CV classifier\nsvc_param_search = {'scaler': [MinMaxScaler(), StandardScaler(), Normalizer(), MaxAbsScaler()],\n                    'clf__C': [pow(2, -10 + i*2) for i in range(0, 16)], 'clf__gamma': [pow(2, -17 + i*2) for i in range(0, 12)]}\n\nsvc_pipeline = Pipeline(steps=[('scaler', StandardScaler()),\n                               ('feature_selector', SelectFromModel(estimator=LinearSVC(random_state=23, penalty='l1', dual=False, max_iter=15000), threshold='mean')),\n                               ('clf', SVC(probability=True, random_state=23))])\n\nsvc_clf = GridSearchCV(estimator=svc_pipeline, scoring='accuracy', param_grid=svc_param_search, verbose=0)\n\n\n### Model 2 - RandomForest ###\n# Initialize parameter dictionary to tune, instantiate RF Pipeline + CV classifier\nrf_param_search = {'scaler': [MinMaxScaler(), StandardScaler(), Normalizer(), MaxAbsScaler()],\n                   'clf__n_estimators': np.arange(100, 1500, 20), 'clf__max_depth': np.arange(2, 12),\n                   'clf__min_samples_split': np.arange(2, 15), 'clf__min_samples_leaf': np.arange(1, 12)}\n\nrf_pipeline = Pipeline(steps=[('scaler', StandardScaler()),\n                              ('feature_selector', SelectFromModel(estimator=LinearSVC(random_state=23, penalty='l1', dual=False, max_iter=15000), threshold='mean')),\n                              ('clf', RandomForestClassifier(random_state=23))])\n\nrf_clf = RandomizedSearchCV(estimator=rf_pipeline, scoring='accuracy', n_iter=30, param_distributions=rf_param_search, verbose=0, random_state=23)\n\n\n### Model 3 - KNN ###\n# Initialize parameter dictionary to tune, instantiate KNN Pipeline + CV classifier\nknn_param_search = {'scaler': [MinMaxScaler(), StandardScaler(), Normalizer(), MaxAbsScaler()],\n                    'clf__n_neighbors': np.arange(1, 20, 1), 'clf__leaf_size': np.arange(10, 45, 1), 'clf__p': np.arange(1, 3)}\n\nknn_pipeline = Pipeline(steps=[('scaler', StandardScaler()),\n                               ('feature_selector', SelectFromModel(estimator=LinearSVC(random_state=23, penalty='l1', dual=False, max_iter=15000), threshold='mean')),\n                               ('clf', KNeighborsClassifier())])\n\nknn_clf = RandomizedSearchCV(estimator=knn_pipeline, scoring='accuracy', n_iter=300, param_distributions=knn_param_search, verbose=0, random_state=23)","5a37767f":"# Instantiate VotingClassifier, pass in underlying estimators.\nensemble_clf = VotingClassifier(estimators=[('svc', svc_clf), ('rf', rf_clf), ('knn', knn_clf)], voting='soft')\n\n# Train it\nensemble_clf.fit(X_train, y_train.values.ravel())\n\n# Print the various final estimators, their parameters, and their scores:\nfor estimator in ensemble_clf.estimators_:\n    print('Est: {}, Score: {:.2%}\\n'.format(estimator.best_estimator_, estimator.best_score_))","c1dfec45":"predictions = pd.merge(test[['PassengerId']], pd.DataFrame(ensemble_clf.predict(process_data(test, name='X', training_cols=training_columns)), columns=['Survived']),\n                       how='left', left_index=True, right_index=True)\n\npredictions","fb35b679":"predictions.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","a2b5046f":"<b>Construct final VotingClassifier model based upon underlying estimators previously defined.<\/b>","a7133c9a":"<b>Define function to pre-process data.<\/b><br>\n\n<p>Note: we originally one-hot encoded the columns for Pclass and Embarked, but generally found the model's performance declined as a result. Seems like too much data sparsity weakens the model.<\/p>","9196f1a7":"<b>Define ensemble-based VotingClassifier with:<\/b><br>\n    <ol>\n        <li>Model #1 = SVM<\/li>\n        <li>Model #2 = RandomForest<\/li>\n        <li>Model #3 = KNN<\/li>\n    <\/ol>\n    We utilize a Pipeline process for cross-validation\/hyperparameter tuning.<\/p>","5e6083b1":"<b>Make predictions and submit them.<\/b>","a3c613bb":"<b>Feature exploration<\/b>"}}