{"cell_type":{"b53bf76e":"code","d7c8cdf7":"code","c187b230":"code","f946f841":"code","46c8ecd2":"code","01731c95":"code","6d9a7d9d":"code","05dd027c":"code","14fc7437":"code","e9b63787":"code","f53081cd":"code","43ee50c7":"code","8929cef1":"code","7c16a8ab":"code","d808be4c":"code","e2ffd835":"code","a8b84a08":"code","f7594167":"code","b617dd60":"code","53c472d9":"code","92026d5e":"code","ea74ed39":"code","5881790a":"code","9ba6a3f7":"code","34a6646c":"code","64fcfd57":"code","b2b24d22":"code","2149b6b1":"markdown","ea0a1765":"markdown","2c37edee":"markdown","5c090197":"markdown","5bda7db9":"markdown","3b460fc0":"markdown","bce9754d":"markdown","54c7874a":"markdown","01654931":"markdown","dc8a2f4c":"markdown","ba5276b8":"markdown","ec9326c6":"markdown","c7583442":"markdown","055f87ec":"markdown","d26bda92":"markdown","1846e7ce":"markdown","113dcaa8":"markdown","de00235e":"markdown","65bfe90e":"markdown"},"source":{"b53bf76e":"!pip install rouge_score","d7c8cdf7":"import pandas as pd \nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport transformers\nfrom transformers import BertTokenizerFast, RobertaTokenizerFast, TFEncoderDecoderModel, AdamWeightDecay\nfrom sklearn.model_selection import train_test_split\nimport datasets\nfrom tqdm.notebook import tqdm\nfrom tensorflow.python.ops.numpy_ops import np_config\nfrom pprint import pprint\n\n# enable model saving at eager mode\nnp_config.enable_numpy_behavior()\nprint('transformers version:', transformers.__version__)\nprint('tensorflow version:', tf.__version__)","c187b230":"df_reviews = pd.read_csv('..\/input\/amazon-fine-food-reviews\/Reviews.csv')\ndf_reviews","f946f841":"df_reviews.drop_duplicates(subset=['UserId','Text'], inplace=True)\n# print(df_reviews[df_reviews['Summary'].isnull() == True]['Text'].unique())\ndf_reviews.dropna(subset=['Summary'], inplace=True)\ndf_reviews.reset_index(inplace=True)","46c8ecd2":"# shuffled_df_reviews = df_reviews.sample(frac=1, ignore_index=True)\n# shuffled_df_reviews.to_pickle('Shuffled_Amz_Reviews.pickle')","01731c95":"df_reviews = pd.read_pickle('..\/input\/shuffledamazonfinefoodreviews\/Shuffled_Amz_Reviews.pickle')\ndf_reviews_sampled = df_reviews.copy()\n# sampling n pairs of reviews and summarizies on shuffled dataframe\ndf_reviews_sampled = df_reviews_sampled.iloc[:40000, :]\ndf_reviews_sampled['text_len'] = df_reviews_sampled['Text'].apply(lambda x: len(x.split()))\ndf_reviews_sampled['summary_len'] = df_reviews_sampled['Summary'].apply(lambda x: len(x.split()))\nfig, ax = plt.subplots(nrows=1, ncols=2, figsize=(16,5))\nax[0].hist(df_reviews_sampled['text_len'])\nax[0].set_xticks(np.arange(0, max(df_reviews_sampled['text_len'])+1,250))\nax[0].set_title('Histogram of length of tokenized text')\nax[1].hist(df_reviews_sampled['summary_len'], color='yellow')\nax[1].set_xticks(np.arange(0, max(df_reviews_sampled['summary_len'])+1,5))\nax[1].set_title('Histogram of length of tokenized summary')","6d9a7d9d":"# Configure the training parameters\nclass TrainingConfig:\n    val_split = 0.2\n    pretrained_checkpoint = 'bert-base-uncased'\n    encoder_checkpoint = 'bert-base-uncased'\n    decoder_checkpoint = 'bert-base-uncased'\n    pad_token_id = 0\n    shared_weight = False\n    encoder_max_len = 256 \n    decoder_max_len = 30 \n    nb_epoch = 3 \n    learning_rate = 3e-5 \n    batch_size = 8 \n    \n    def __init__(self, **kwargs):\n        for k,v in kwargs.items():\n            setattr(self, k, v)","05dd027c":"# load the train and validation dataset\nclass DataLoader:\n    def __init__(self, paragraphs, summaries, **kwargs):\n        self.paragraphs = paragraphs \n        self.summaries = summaries \n        self.tokenizer = kwargs.get('tokenizer')\n        self.val_split = kwargs.get('val_split')\n        self.encoder_max_len = kwargs.get('encoder_max_len')\n        self.decoder_max_len = kwargs.get('decoder_max_len')\n    \n    @property\n    def sample_size(self):\n        assert len(self.paragraphs)==len(self.summaries)\n        return len(self.paragraphs)\n    \n    def split_train_test(self):\n        train_idx, val_idx = train_test_split(\n            list(range(self.sample_size)), \n            test_size=self.val_split, \n            random_state=98\n        )\n        return train_idx, val_idx\n    \n    def convert_text_to_ids(self, input_paragraphs, input_summaries):\n        inputs = self.tokenizer(\n            list(input_paragraphs), \n            return_tensors='np', \n            padding='max_length', \n            truncation=True, \n            max_length=self.encoder_max_len\n        )\n        outputs = self.tokenizer(\n            list(input_summaries), \n            return_tensors='np', \n            padding='max_length', \n            truncation=True, \n            max_length=self.decoder_max_len\n        )\n        return inputs, outputs\n    \n    def list_to_tensor_dataset(self, input_paragraphs, input_summaries):\n        inputs, outputs = self.convert_text_to_ids(\n            input_paragraphs, \n            input_summaries\n        )\n        input_ids = tf.data.Dataset.from_tensor_slices(\n            inputs['input_ids']\n        )\n        attention_masks = tf.data.Dataset.from_tensor_slices(\n            inputs['attention_mask']\n        )\n        output_ids = tf.data.Dataset.from_tensor_slices(\n            outputs['input_ids']\n        )\n        output_attention_masks = tf.data.Dataset.from_tensor_slices(\n            outputs['attention_mask']\n        )                                                \n        tf_dataset = tf.data.Dataset.zip(\n            ({\n                'input_ids': input_ids, \n                'attention_mask': attention_masks,\n                'decoder_input_ids': output_ids, \n                'decoder_attention_mask': output_attention_masks\n            }, \n            output_ids)\n        )\n        return tf_dataset\n    \n    def __call__(self):\n        train_idx, val_idx = self.split_train_test()\n        train_paras, val_paras = self.paragraphs[train_idx], self.paragraphs[val_idx]\n        train_sums, val_sums = self.summaries[train_idx], self.summaries[val_idx]\n        train_dataset = self.list_to_tensor_dataset(train_paras, train_sums)\n        val_dataset = self.list_to_tensor_dataset(val_paras, val_sums)\n        return train_dataset, val_dataset","14fc7437":"# Customized loss function for seq2seq model\nclass Seq2SeqLoss(tf.keras.losses.Loss):\n    def __init__(self, pad_token_id, name=\"seq2seq_loss\"):\n        super().__init__(name=name)\n        self.pad_token_id = pad_token_id\n\n    def call(self, y_true, y_pred):\n        loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n            from_logits=True, \n            reduction=tf.keras.losses.Reduction.NONE\n        )\n        # shift the label and output sequences to match  \n        output_logits = y_pred[:,:-1,:]\n        input_labels = y_true[:,1:] \n        loss = loss_fn(input_labels, output_logits)\n        # calculate loss without the padding tokens in label sequence\n        mask = tf.cast((input_labels != self.pad_token_id), dtype=tf.float32)\n        loss = loss * mask\n        return tf.reduce_sum(loss) \/ tf.reduce_sum(mask)","e9b63787":"class Trainer:\n    def __init__(self, model, loss_fn, optimizer, metric):\n        self.model = model\n        self.loss_fn = loss_fn\n        self.optimizer = optimizer\n        self.metric = metric\n        # loss tracker will capture the mean of loss till now\n        self.loss_tracker = tf.keras.metrics.Mean(name='mean_loss')\n    \n    # Training Step\n    @tf.function \n    def train_step(self, inputs):\n        input_seqs, input_labels = inputs\n        with tf.GradientTape() as tape: \n            outputs = self.model(\n                input_seqs['input_ids'],\n                input_seqs['attention_mask'],\n                input_seqs['decoder_input_ids'],\n                input_seqs['decoder_attention_mask'],\n                training = True\n            )\n            logits = outputs.logits\n            loss = self.loss_fn(input_labels, logits)\n        gradients = tape.gradient(loss, self.model.trainable_weights)\n        self.optimizer.apply_gradients(\n            zip(gradients, self.model.trainable_weights)\n        )\n        self.loss_tracker.update_state(loss)\n#         self.metric.update_state(y, predictions)\n        return loss\n        \n    # Validation Step\n    @tf.function  \n    def val_step(self, inputs):\n        input_seqs, input_labels = inputs\n        outputs = self.model(                \n                input_seqs['input_ids'],\n                input_seqs['attention_mask'],\n                input_seqs['decoder_input_ids'],\n                input_seqs['decoder_attention_mask'],\n                training = False\n        )\n        logits = outputs.logits\n        loss = self.loss_fn(input_labels, logits)\n        self.loss_tracker.update_state(loss)\n#         self.metric.update_state(y,predictions)\n        return loss","f53081cd":"def batched_generate_summary(model, tokenizer, batched_input):\n    input_seqs, input_labels = batched_input\n    outputs = model.generate(\n        input_ids=input_seqs['input_ids'], \n        attention_mask=input_seqs['attention_mask']\n    )\n    output_strs = tokenizer.batch_decode(\n        outputs, \n        skip_special_tokens=True\n    )\n    output_gold = tokenizer.batch_decode(\n        input_seqs['decoder_input_ids'], \n        skip_special_tokens=True\n    )\n    input_strs = tokenizer.batch_decode(\n        input_seqs['input_ids'], \n        skip_special_tokens=True\n    )\n    return output_strs, output_gold, input_strs","43ee50c7":"reviews = df_reviews_sampled['Text'].values\nsummaries = df_reviews_sampled['Summary'].values\n\ntraining_config = TrainingConfig(nb_epoch=5)\ntokenizer = BertTokenizerFast.from_pretrained(training_config.encoder_checkpoint)\n\ndataloader_args = {\n    'tokenizer': tokenizer,\n    'val_split': training_config.val_split,\n    'encoder_max_len': training_config.encoder_max_len,\n    'decoder_max_len': training_config.decoder_max_len\n}\ndataloader = DataLoader(reviews, summaries, **dataloader_args)\ntrain_dataset, val_dataset = dataloader()\ntrain_dataset = (train_dataset\n                 .shuffle(int(dataloader.sample_size*(1-training_config.val_split)))\n                 .batch(training_config.batch_size))\nval_dataset = val_dataset.batch(training_config.batch_size)","8929cef1":"bert2bert = TFEncoderDecoderModel.from_encoder_decoder_pretrained(\n    training_config.encoder_checkpoint, \n    training_config.decoder_checkpoint,\n    # whether to share the encoder weight\n    tie_encoder_decoder=training_config.shared_weight\n)","7c16a8ab":"bert2bert.save_pretrained('bert2bert')\nbert2bert = TFEncoderDecoderModel.from_pretrained('bert2bert')","d808be4c":"# The special tokens for decoder should be aligned with the special tokens for encoder\n# Since we are using Bert checkpoint for both decoder and decoder, \n# the cls and sep tokens in the encoder could be used as the start and end token for the decoder\nbert2bert.config.decoder_start_token_id = tokenizer.cls_token_id # 101\nbert2bert.config.eos_token_id = tokenizer.sep_token_id # 102 \nbert2bert.config.pad_token_id = tokenizer.pad_token_id # 0\nbert2bert.config.vocab_size = bert2bert.config.encoder.vocab_size \n\n# These configurations are for the beam search in decoding process\nbert2bert.config.max_length = 30\nbert2bert.config.min_length = 3\nbert2bert.config.no_repeat_ngram_size = 2\nbert2bert.config.early_stopping = True\nbert2bert.config.length_penalty = 2.0\nbert2bert.config.num_beams = 4","e2ffd835":"rouge = datasets.load_metric('rouge')\ntf.keras.backend.clear_session()\ntrainer = Trainer(model=bert2bert,\n                  loss_fn=Seq2SeqLoss(training_config.pad_token_id),\n                  optimizer=AdamWeightDecay(\n                      learning_rate=training_config.learning_rate, \n                      weight_decay_rate=0.005\n                  ),\n                  metric=None)\n\n# Training Loop\nfor epoch in range(training_config.nb_epoch):\n    print(f'\\nEpoch {epoch+1}\\n')\n    print('Training....')\n    for step,batched_input in enumerate(tqdm(train_dataset)):\n        loss = trainer.train_step(batched_input)\n        till_now_loss = trainer.loss_tracker.result()\n        if step%200 == 0:\n            print(f'Training loss for one batch at step {step}: {round(till_now_loss,3)}') \n    trainer.loss_tracker.reset_states()\n    \n    print('Validating....')\n    val_measures = {'rouge precision':0, 'rouge recall':0, 'rouge f1': 0}\n    for step, batched_input in enumerate(tqdm(val_dataset)):\n        val_loss = trainer.val_step(batched_input)\n#         pred_str, gold_str = generate_summary(bert2bert, \n#                                               tokenizer, \n#                                               batched_input)\n#         rouge_output = rouge.compute(predictions=pred_str,\n#                                      references=gold_str,\n#                                      rouge_types=['rouge2'])['rouge2'].mid\n#         val_measures['rouge precision'] += rouge_output.precision \/ len(val_dataset)\n#         val_measures['rouge recall'] += rouge_output.recall \/ len(val_dataset)\n#         val_measures['rouge f1'] += rouge_output.fmeasure \/ len(val_dataset)\n    till_now_val_loss = trainer.loss_tracker.result()\n    print(f'Validation loss: {round(till_now_val_loss,3)}')\n    bert2bert.save_pretrained(\n        f'bert2bert-Checkpoint-epoch{epoch+1}-loss{round(till_now_val_loss,3)}'\n    )\n#     for name, value in val_measures.items():\n#         print(f'Validation {name}: {value}')\n    trainer.loss_tracker.reset_states()","a8b84a08":"# Load the best model checkpoint\ntrained_bert2bert = TFEncoderDecoderModel.from_pretrained(\n    '.\/bert2bert-Checkpoint-epoch2-loss3.552000045776367'\n)","f7594167":"# val0 = list(val_dataset.as_numpy_iterator())[0]\nfor step, batched_input in enumerate(tqdm(val_dataset)):\n    pred_str, gold_str, input_strs = batched_generate_summary(\n        trained_bert2bert, \n        tokenizer, \n        batched_input\n    )\n    rouge_output = rouge.compute(\n        predictions=pred_str,\n        references=gold_str,\n        rouge_types=[\"rouge1\"]\n    )\n    print('Rouge report: ')\n    print(rouge_output['rouge1'].mid)\n    for p_str,g_str,in_str in zip(pred_str, gold_str, input_strs):\n        print('='*100)\n        print('Review: ' + in_str)\n        print('Summary: ' + g_str)\n        print('Generated: ' + p_str)\n    \n    break","b617dd60":"df_reviews_test = df_reviews.copy()\ndf_reviews_test = df_reviews_test.iloc[40000:41000, :]\ntest_reviews = df_reviews_test['Text'].values\ntest_sums = df_reviews_test['Summary'].values\n\ntest_dataloader = DataLoader(test_reviews, test_sums, **dataloader_args)\ntest_dataset = test_dataloader.list_to_tensor_dataset(test_paras, test_sums)\ntest_dataset = test_dataset.batch(training_config.batch_size)\npred_strs = []\ngold_strs = []\n\nfor batched_input in tqdm(test_dataset):\n    pred_str, gold_str, _ = batched_generate_summary(\n        trained_bert2bert, \n        tokenizer, \n        batched_input\n    )\n    pred_strs.extend(pred_str)\n    gold_strs.extend(gold_str)\n    \nrouge_output = rouge.compute(\n    predictions=pred_strs,\n    references=gold_strs,\n    rouge_types=[\"rouge1\"]\n)\npprint(rouge_output)","53c472d9":"reviews = df_reviews_sampled['Text'].values\nsummaries = df_reviews_sampled['Summary'].values\n\n# learning rate is subject to change\ntraining_config = TrainingConfig(\n    encoder_checkpoint = 'roberta-base',\n    decoder_checkpoint = 'roberta-base',\n    shared_weight = False,\n    nb_epoch=4,\n    learning_rate=1e-5,\n    # the pad token id for roBERTa is 1\n    pad_token_id = 1\n)\ntokenizer = RobertaTokenizerFast.from_pretrained(training_config.encoder_checkpoint)\n\ndataloader_args = {\n    'tokenizer': tokenizer,\n    'val_split': training_config.val_split,\n    'encoder_max_len': training_config.encoder_max_len,\n    'decoder_max_len': training_config.decoder_max_len\n}\ndataloader = DataLoader(reviews, summaries, **dataloader_args)\ntrain_dataset, val_dataset = dataloader()\ntrain_dataset = (train_dataset\n                 .shuffle(int(dataloader.sample_size*(1-training_config.val_split)))\n                 .batch(training_config.batch_size))\nval_dataset = val_dataset.batch(training_config.batch_size)","92026d5e":"roberta_shared = TFEncoderDecoderModel.from_encoder_decoder_pretrained(\n    training_config.encoder_checkpoint,                           \n    training_config.decoder_checkpoint,\n    # whether to share the encoder weight\n    tie_encoder_decoder=training_config.shared_weight\n)","ea74ed39":"roberta_shared.save_pretrained('roberta_shared')\nroberta_shared = TFEncoderDecoderModel.from_pretrained('roberta_shared')","5881790a":"# the start and end tokens for roBERTa is different from Bert\nroberta_shared.config.decoder_start_token_id = tokenizer.bos_token_id # 0                                          \nroberta_shared.config.eos_token_id = tokenizer.eos_token_id # 2\nroberta_shared.config.pad_token_id = tokenizer.pad_token_id # 1\nroberta_shared.config.vocab_size = roberta_shared.config.encoder.vocab_size \n                            \nroberta_shared.config.max_length = 30\nroberta_shared.config.min_length = 3\nroberta_shared.config.early_stopping = True\nroberta_shared.config.no_repeat_ngram_size = 2\nroberta_shared.config.length_penalty = 2.0\nroberta_shared.config.num_beams = 4","9ba6a3f7":"rouge = datasets.load_metric('rouge')\ntf.keras.backend.clear_session()\ntrainer = Trainer(model=roberta_shared,\n                  loss_fn=Seq2SeqLoss(training_config.pad_token_id),\n                  optimizer=AdamWeightDecay(\n                      learning_rate=training_config.learning_rate, \n                      weight_decay_rate=1e-5,\n                  ),\n                  metric=None)\n\n# Training Loop\nfor epoch in range(training_config.nb_epoch):\n    print(f'\\nEpoch {epoch+1}\\n')\n    print('Training....')\n    for step,batched_input in enumerate(tqdm(train_dataset)):\n        loss = trainer.train_step(batched_input)\n        till_now_loss = trainer.loss_tracker.result()\n        if step%200 == 0:\n            print(f'Training loss for one batch at step {step}: {till_now_loss}') \n    trainer.loss_tracker.reset_states()\n    \n    print('Validating....')\n    val_measures = {'rouge precision':0, 'rouge recall':0, 'rouge f1': 0}\n    for step, batched_input in enumerate(tqdm(val_dataset)):\n        val_loss = trainer.val_step(batched_input)\n#         pred_str, gold_str = generate_summary(bert2bert, \n#                                               tokenizer, \n#                                               batched_input)\n#         rouge_output = rouge.compute(predictions=pred_str,\n#                                      references=gold_str,\n#                                      rouge_types=['rouge2'])['rouge2'].mid\n#         val_measures['rouge precision'] += rouge_output.precision \/ len(val_dataset)\n#         val_measures['rouge recall'] += rouge_output.recall \/ len(val_dataset)\n#         val_measures['rouge f1'] += rouge_output.fmeasure \/ len(val_dataset)\n    till_now_val_loss = trainer.loss_tracker.result()\n    print(f'Validation loss: {round(till_now_val_loss,3)}')\n    roberta_shared.save_pretrained(\n        f'robertashared-Checkpoint-epoch{epoch+1}-loss{till_now_val_loss}'\n    )\n#     for name, value in val_measures.items():\n#         print(f'Validation {name}: {value}')\n    trainer.loss_tracker.reset_states()","34a6646c":"trained_roberta_shared = TFEncoderDecoderModel.from_pretrained(\n    '.\/robertashared-Checkpoint-epoch4-loss4.075682640075684'\n)","64fcfd57":"for step, batched_input in enumerate(tqdm(val_dataset)):\n    pred_str, gold_str, input_strs = batched_generate_summary(\n        trained_roberta_shared, \n        tokenizer, \n        batched_input\n    )\n    rouge_output = rouge.compute(\n        predictions=pred_str,\n        references=gold_str,\n        rouge_types=[\"rouge1\"]\n    )\n    print('Rouge report: ')\n    print(rouge_output['rouge1'].mid)\n    for p_str,g_str,in_str in zip(pred_str, gold_str, input_strs):\n        print('='*100)\n        print('Review: ' + in_str)\n        print('Summary: ' + g_str)\n        print('Generated: ' + p_str)\n    break","b2b24d22":"df_reviews_test = df_reviews.copy()\ndf_reviews_test = df_reviews_test.iloc[40000:41000, :]\ntest_paras = df_reviews_test['Text'].values\ntest_sums = df_reviews_test['Summary'].values\n\ntest_dataloader = DataLoader(test_reviews, test_sums, **dataloader_args)\ntest_dataset = test_dataloader.list_to_tensor_dataset(test_paras, test_sums)\ntest_dataset = test_dataset.batch(training_config.batch_size)\npred_strs = []\ngold_strs = []\n\nfor batched_input in tqdm(test_dataset):\n    pred_str, gold_str, _ = batched_generate_summary(\n        trained_roberta_shared, \n        tokenizer, \n        batched_input\n    )\n    pred_strs.extend(pred_str)\n    gold_strs.extend(gold_str)\n    \nrouge_output = rouge.compute(\n    predictions=pred_strs,\n    references=gold_strs,\n    rouge_types=[\"rouge1\"]\n)\npprint(rouge_output)","2149b6b1":"# Bert2Bert","ea0a1765":"# Training Preparation","2c37edee":"Let's see what is our generated output look like!","5c090197":"Drop the duplicated reviews and reviews without summaries","5bda7db9":"# Model Setup","3b460fc0":"When I try to train the model with fit and compile method from keras API, the model output seems like the pooler output (None,768) of the Bert and I haven't figured out the reason why it happens. So I write the customized training loop instead. ","bce9754d":"In this notebook, according the paper 'Leveraging Pre-trained Checkpoints for Sequence Generation Tasks (2020)', I implement the warm-started encoder-decoder model which is proved to acheive similar results compared to widely used pre-trained encoder-decoder models like T5, Bart...","54c7874a":"More details about the model structure and the pytorch version of the code, please go to this awesome [blog](https:\/\/huggingface.co\/blog\/warm-starting-encoder-decoder) from Huggingface","01654931":"Similarly, let's see what the generation result of our model look likes.","dc8a2f4c":"Let's also report the roughe1 score for roBERTaShared on the test data","ba5276b8":"# Data Preparation","ec9326c6":"The generated summaries make sense. It great!","c7583442":"Rouge1 score is 13.5. Not Bad!","055f87ec":"Let's report the rouge score on the 1000 test samples. Since most of the summaries have very small length, I use rouge1 score instead of rouge2 score to evaluate the generated texts.","d26bda92":"According the report result on the paper, warm-starting the encoder and the decoder with RoBERTa checkpoints and using shared weight in decoder and ecoder module achieves the highest rouge score on two of the three Abstrative Summary task corpus (CNN dailymail\/BBC Xsum). So I am also curious about if it will outperforms Bert2Bert in this dataset.","1846e7ce":"It seems that the model is underfitting. I need to retrain the model with more epochs in the next version.","113dcaa8":"We can see that most of the sampled reviews have length < 256 and the sampled reviews have length < 30. It could give us an idea of deciding the input length of the encoder and decoder sequences.","de00235e":"Recently I got interest in the text summarization task. When I start this project, I find that there are very rare resouces online about fine tuning transformers models on abstractive summarization task, especially for Tensorflow code. I hope this notebook could be helpful for people who want to implement the transformer models on text generation by using Huggingface API with Tensorflow.","65bfe90e":"# RoBERTaShared"}}