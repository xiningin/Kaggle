{"cell_type":{"f64d9ef7":"code","0ba77210":"code","f9476a37":"code","23ba4d11":"code","6eb27d00":"code","3438438e":"code","83ab62ce":"code","6eece1f5":"code","a5b7f67b":"code","61e458fb":"code","f0699511":"code","cdb5170d":"code","730378bc":"code","7862d108":"code","4a9d33db":"code","494df4da":"code","d17d49fb":"code","219ae022":"code","9324dafa":"code","5c53c259":"code","b7cd4da2":"code","72e3a659":"code","79387559":"code","df8fd57c":"code","f5590061":"code","a08a04bb":"code","a8823773":"code","1a80c102":"markdown","2e275169":"markdown","bd5e9488":"markdown","d0eb1be1":"markdown","5dc946e8":"markdown","e6c92e68":"markdown","0cb36af7":"markdown","e04d02aa":"markdown","95eb87bb":"markdown","b59e1f1a":"markdown"},"source":{"f64d9ef7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        continue\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0ba77210":"import sys\nsys.path.append('..\/input\/moalib')\nsys.path.append('..\/input\/iterativestratification')\n\nfrom rankgauss import quantile_transform_dataframe\nfrom pca import add_PC_to_dataframe\nfrom featureselection import select_features\nfrom cvfolds import get_folds\nfrom main import run_training\nfrom main import run_predicition\nfrom preprocess import process_data\nfrom models import Model\nfrom seed import seed_everything\nfrom transferlearning import FineTuneScheduler\nimport torch\nimport torch.nn as nn\nfrom sklearn.metrics import log_loss\nfrom sklearn.feature_selection import VarianceThreshold\nimport torch.nn.functional as F\n#from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n#import_modules()","f9476a37":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ncontnames = train_features.columns[4:]\n\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\n\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsample_submission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","23ba4d11":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","6eb27d00":"train_features.head()","3438438e":"quantile_transform_dataframe(train_features, test_features, GENES + CELLS)","83ab62ce":"train_features.head()","6eece1f5":"n_comp_GENES = 463\nn_comp_CELLS = 60\ntrain_features, test_features = add_PC_to_dataframe(train_features, test_features, n_comp_GENES, n_comp_CELLS, GENES, CELLS)","a5b7f67b":"train_features.head(5)","61e458fb":"VarianceThreshold_for_FS = 0.9\ntrain, test, target, target_cols, slected_feats = select_features(train_features, test_features, train_targets_scored, VarianceThreshold_for_FS)\n#train, test, target, target_cols = select_original_or_PCA_features(train_features, test_features, train_targets_scored, False)","f0699511":"#target_cols = [x for x in train_targets_scored.columns if x != 'sig_id']\naux_target_cols = [x for x in train_targets_nonscored.columns if x != 'sig_id']\nall_target_cols = target_cols + aux_target_cols\n\nnum_targets = len(target_cols)\nnum_aux_targets = len(aux_target_cols)\nnum_all_targets = len(all_target_cols)","cdb5170d":"train = train.merge(train_targets_nonscored, on='sig_id')","730378bc":"[print(f) for f in train_features.columns[4:][slected_feats]]","7862d108":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfolds = get_folds(train, target)","4a9d33db":"print(train.shape)\nprint(folds.shape)\nprint(test.shape)\nprint(target.shape)\nprint(sample_submission.shape)","494df4da":"feature_cols = [c for c in process_data(folds).columns if c not in all_target_cols]\nfeature_cols = [c for c in feature_cols if c not in ['kfold','sig_id']]\nlen(feature_cols)","d17d49fb":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self, num_features, num_targets, hidden_size, Dropout_Model):\n        super(Model, self).__init__()\n        self.hidden_size = [hidden_size]\n        self.dropout_value = [Dropout_Model]\n        \n        self.batch_norm_0 = nn.BatchNorm1d(num_features)\n        self.dense_0 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n\n        self.batch_norm_1 = nn.BatchNorm1d(hidden_size)\n        self.dropout_1 = nn.Dropout(Dropout_Model)\n        self.dense_1 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n\n        self.batch_norm_last = nn.BatchNorm1d(hidden_size)\n        self.dropout_last = nn.Dropout(Dropout_Model)\n        self.dense_last = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n        \n    def recalibrate_layer(self, layer):\n\n        if(torch.isnan(layer.weight_v).sum() > 0):\n            print ('recalibrate layer.weight_v')\n            layer.weight_v = torch.nn.Parameter(torch.where(torch.isnan(layer.weight_v), torch.zeros_like(layer.weight_v), layer.weight_v))\n            layer.weight_v = torch.nn.Parameter(layer.weight_v + 1e-7)\n\n        if(torch.isnan(layer.weight).sum() > 0):\n            print ('recalibrate layer.weight')\n            layer.weight = torch.where(torch.isnan(layer.weight), torch.zeros_like(layer.weight), layer.weight)\n            layer.weight += 1e-7\n\n    def forward(self, x):\n        x = self.batch_norm_0(x)\n        self.recalibrate_layer(self.dense_0)\n        x = F.leaky_relu(self.dense_0(x))        \n\n        x = self.batch_norm_1(x)\n        x = self.dropout_1(x)\n        self.recalibrate_layer(self.dense_1)\n        x = F.leaky_relu(self.dense_1(x))\n\n        x = self.batch_norm_last(x)\n        x = self.dropout_last(x)\n        self.recalibrate_layer(self.dense_last)\n        x = self.dense_last(x)\n\n        return x","219ae022":"from seed import seed_everything\nfrom preprocess import process_data\nfrom datasets import MoADataset\nfrom datasets import TestDataset\nfrom labelsmoothing import SmoothBCEwLogits\nfrom training import train_fn\nfrom training import valid_fn\nfrom training import inference_fn\nimport numpy as np\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\n\ndef run_training(fold, seed, folds, model, feature_cols, target_cols, target, BATCH_SIZE, DEVICE, LEARNING_RATE, WEIGHT_DECAY, EPOCHS, EARLY_STOPPING_STEPS, EARLY_STOP):\n    seed_everything(seed)\n\n    train = process_data(folds)\n\n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n\n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n\n    x_train, y_train = train_df[feature_cols].values, train_df[target_cols].values\n    x_valid, y_valid = valid_df[feature_cols].values, valid_df[target_cols].values\n\n    train_dataset = MoADataset(x_train, y_train)\n    valid_dataset = MoADataset(x_valid, y_valid)\n    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    model.to(DEVICE)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3,\n                                              max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n\n    loss_fn = nn.BCEWithLogitsLoss()\n    loss_tr = SmoothBCEwLogits(smoothing=0.001)\n\n    early_stopping_steps = EARLY_STOPPING_STEPS\n    early_step = 0\n\n    oof = np.zeros((len(train), target.iloc[:, 1:].shape[1]))\n    best_loss = np.inf\n\n    for epoch in range(EPOCHS):\n\n        train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n        valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n        print(f\"FOLD: {fold}, EPOCH: {epoch}, valid_loss: {valid_loss}\")\n\n        if valid_loss < best_loss:\n            best_loss = valid_loss\n            oof[val_idx] = valid_preds\n            torch.save(model.state_dict(), f\"FOLD{fold}_{seed}.pth\")\n\n        elif (EARLY_STOP == True):\n            early_step += 1\n            if (early_step >= early_stopping_steps):\n                break\n\n    return oof\n\n\n\ndef run_predicition(fold, model, test, feature_cols, DEVICE, BATCH_SIZE):\n    test_ = process_data(test)\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    model.load_state_dict(torch.load(f\"FOLD{fold}_{seed}.pth\"))\n    model.to(DEVICE)\n\n    predictions = inference_fn(model, testloader, DEVICE)\n\n    return predictions","9324dafa":"def run_training(fold, seed):\n    seed_everything(seed)\n    \n    train = process_data(folds)\n    \n    trn_idx = train[train['kfold'] != fold].index\n    val_idx = train[train['kfold'] == fold].index\n\n    train_df = train[train['kfold'] != fold].reset_index(drop=True)\n    valid_df = train[train['kfold'] == fold].reset_index(drop=True)   \n    \n    def train_model(model, tag_name, target_cols_now, fine_tune_scheduler=None):       \n        x_train, y_train  = train_df[feature_cols].values, train_df[target_cols_now].values\n        x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols_now].values\n        \n        train_dataset = MoADataset(x_train, y_train)\n        valid_dataset = MoADataset(x_valid, y_valid)\n\n        trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n        validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n        \n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=WEIGHT_DECAY[tag_name])\n        scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer,\n                                                  steps_per_epoch=len(trainloader),\n                                                  pct_start=PCT_START,\n                                                  div_factor=DIV_FACTOR[tag_name], \n                                                  max_lr=MAX_LR[tag_name],\n                                                  epochs=EPOCHS)\n        \n        loss_fn = nn.BCEWithLogitsLoss()\n        loss_tr = SmoothBCEwLogits(smoothing=0.001)\n\n        oof = np.zeros((len(train), len(target_cols_now)))\n        best_loss = np.inf\n        \n        for epoch in range(EPOCHS):\n            if fine_tune_scheduler is not None:\n                fine_tune_scheduler.step(epoch, model)\n\n            train_loss = train_fn(model, optimizer, scheduler, loss_tr, trainloader, DEVICE)\n            valid_loss, valid_preds = valid_fn(model, loss_fn, validloader, DEVICE)\n            print(f\"SEED: {seed}, FOLD: {fold}, {tag_name}, EPOCH: {epoch}, train_loss: {train_loss:.6f}, valid_loss: {valid_loss:.6f}\")\n\n            if np.isnan(valid_loss):\n                break\n            \n            if valid_loss < best_loss:\n                best_loss = valid_loss\n                oof[val_idx] = valid_preds\n                torch.save(model.state_dict(), f\"{tag_name}_FOLD{fold}_{seed}.pth\")\n\n        return oof\n\n    fine_tune_scheduler = FineTuneScheduler(EPOCHS)\n\n    pretrained_model = Model(\n        num_features=num_features,        \n        num_targets=num_all_targets,\n        hidden_size=hidden_size,\n        Dropout_Model=Dropout_Model)  \n    #pretrained_model = Model(num_features, num_all_targets)\n    pretrained_model.to(DEVICE)\n\n    # Train on scored + nonscored targets\n    train_model(pretrained_model, 'ALL_TARGETS', all_target_cols)\n\n    # Load the pretrained model with the best loss\n    pretrained_model =Model(\n        num_features=num_features,        \n        num_targets=num_all_targets,\n        hidden_size=hidden_size,\n        Dropout_Model=Dropout_Model)  \n    pretrained_model.load_state_dict(torch.load(f\"ALL_TARGETS_FOLD{fold}_{seed}.pth\"))\n    pretrained_model.to(DEVICE)\n\n    model_new = Model(\n        num_features=num_features,        \n        num_targets=num_all_targets,\n        hidden_size=hidden_size,\n        Dropout_Model=Dropout_Model)  \n    model_new.load_state_dict(pretrained_model.state_dict())\n    # Copy model without the top layer\n    final_model = fine_tune_scheduler.copy_without_top(pretrained_model, model_new, num_targets, DEVICE)\n\n    # Fine-tune the model on scored targets only\n    oof = train_model(final_model, 'SCORED_ONLY', target_cols, fine_tune_scheduler)\n    \n    return oof   \n\n\ndef run_predicition(fold, model, test, feature_cols, DEVICE, BATCH_SIZE):\n    test_ = process_data(test)\n    x_test = test_[feature_cols].values\n    testdataset = TestDataset(x_test)\n    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    model.load_state_dict(torch.load(f\"SCORED_ONLY_FOLD{fold}_{seed}.pth\"))\n    model.to(DEVICE)\n\n    predictions = inference_fn(model, testloader, DEVICE)\n\n    return predictions","5c53c259":"DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 25\nBATCH_SIZE = 128\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 1e-5\nNFOLDS = 7\nEARLY_STOPPING_STEPS = 10\nEARLY_STOP = False\nDropout_Model = 0.25\n\nnum_features=len(feature_cols)\nnum_targets=len(target_cols)\nhidden_size = 1550","b7cd4da2":"DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\nEPOCHS = 28\nBATCH_SIZE = 128\n\nWEIGHT_DECAY = {'ALL_TARGETS': 1e-5, 'SCORED_ONLY': 3e-6}\nMAX_LR = {'ALL_TARGETS': 1e-2, 'SCORED_ONLY': 3e-3}\nDIV_FACTOR = {'ALL_TARGETS': 1e3, 'SCORED_ONLY': 1e2}\nPCT_START = 0.1","72e3a659":"import os.path\ndef has_model(fold): return os.path.isfile(f\"FOLD{fold}_.pth\")","79387559":"import gc","df8fd57c":"def run_k_fold(NFOLDS, seed):\n    oof = np.zeros((len(train), len(target_cols)))\n    predictions = np.zeros((len(test), len(target_cols)))\n   \n    for fold in range(NFOLDS):\n        gc.collect()\n        #model = Model(\n        #num_features=num_features,        \n        #num_targets=num_targets,\n        #hidden_size=hidden_size,\n        #Dropout_Model=Dropout_Model)     \n                      \n        \n        #oof_ = run_training(fold, seed, folds, model, feature_cols, \n        #                    target_cols, target, BATCH_SIZE, DEVICE, LEARNING_RATE,\n        #                    WEIGHT_DECAY, EPOCHS, EARLY_STOPPING_STEPS, EARLY_STOP)\n        \n        oof_ = run_training(fold, seed)\n        \n        modelPred = Model(\n        num_features=num_features,        \n        num_targets=num_targets,\n        hidden_size=hidden_size,\n        Dropout_Model=Dropout_Model)       \n        \n        pred_ = run_predicition(fold, modelPred, test, feature_cols, DEVICE, BATCH_SIZE)\n        \n        predictions += pred_ \/ NFOLDS\n        oof += oof_\n        \n    return oof, predictions\n# Averaging on multiple SEEDS\n\nSEED = [0,1]\n\n\noof = np.zeros((len(train), len(target_cols)))\npredictions = np.zeros((len(test), len(target_cols)))\nfor seed in SEED:  \n    #folds = get_folds(train, target)\n    oof_, predictions_ = run_k_fold(NFOLDS, seed)\n    oof += oof_ \/ len(SEED)\n    predictions += predictions_ \/ len(SEED)  \n    #del folds\n                \n                \n\ntrain[target_cols] = oof\ntest[target_cols] = predictions\n        \nvalid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ \/ target.shape[1]\n            \n\n            \n#print(\"num_layers: \", num_layers)\nprint(\"hidden_size: \", hidden_size)\nprint(\"CV log_loss: \", score)  ","f5590061":"#train[target_cols] = oof\n#test[target_cols] = predictions","a08a04bb":"valid_results = train_targets_scored.drop(columns=target_cols).merge(train[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\n\ny_true = train_targets_scored[target_cols].values\ny_pred = valid_results[target_cols].values\n\nscore = 0\nfor i in range(len(target_cols)):\n    score_ = log_loss(y_true[:, i], y_pred[:, i])\n    score += score_ \/ target.shape[1]\n    \nprint(\"CV log_loss: \", score)    ","a8823773":"sub = sample_submission.drop(columns=target_cols).merge(test[['sig_id']+target_cols], on='sig_id', how='left').fillna(0)\nsub.to_csv('submission.csv', index=False)","1a80c102":"# Model scaling","2e275169":"# Dataloading","bd5e9488":"# Description\n* This notebook started as an identical copy of @Vitalii Mokin's great notebook https:\/\/www.kaggle.com\/vbmokin\/moa-pytorch-rankgauss-pca-nn-upgrade-3d-visual\/notebook\n* In order to keep the notebook short all the functions are imported from moalib dataset","d0eb1be1":"# Submission","5dc946e8":"# Feature selection","e6c92e68":"# Evaluation","0cb36af7":"# Quantile regression","e04d02aa":"# Upgrade \n* Commit 9: Added features generated by cnn from https:\/\/www.kaggle.com\/martintosstorff\/moa-dimensionalityreduction-for-applying-cnn\n* Commit 11: Removed features generated by cnn as they lead to overfit","95eb87bb":"# PCA","b59e1f1a":"# Training"}}