{"cell_type":{"db225143":"code","7566b06f":"code","ae1d2ac1":"code","c605b153":"code","918ec217":"code","69b734a6":"code","8cfaf5d3":"code","961bc06f":"code","090f1c0b":"code","92d9c730":"code","4bb06bba":"code","d2066042":"code","935de6f8":"code","c4e06c83":"code","fe6b9221":"code","e298d531":"code","8f1dd968":"code","46c6439a":"code","573062b6":"code","848fb0ed":"code","90c1c5d5":"code","d60a33dd":"code","1f144594":"code","1157cfe8":"code","0a698626":"code","4554db2a":"code","045622e1":"code","8d1dc250":"code","1e563a82":"code","b040db86":"code","f5d65f72":"code","c7d2aa4a":"code","211c8aa0":"code","857457eb":"code","933ef4c8":"code","d032aab5":"code","dfbc1107":"code","6cd944ef":"code","0f5a3472":"code","0c223ec5":"code","b44f8a07":"code","ec13d4d7":"code","91249e75":"code","516ebe01":"code","be3533d8":"markdown","097eb78d":"markdown","a542863d":"markdown","0d15f762":"markdown","6ce2ac33":"markdown","43aca521":"markdown","9592af2a":"markdown","b240abbd":"markdown","96fdb482":"markdown","803fe317":"markdown","f44c27b2":"markdown","90a29273":"markdown","f551a893":"markdown","6ac67042":"markdown","3bfc5065":"markdown","8a111e1d":"markdown","639ec75b":"markdown","0c7a0a6d":"markdown","18608c2e":"markdown","597599aa":"markdown","3ef58e88":"markdown","dac2d979":"markdown","abd01e90":"markdown","c54511df":"markdown"},"source":{"db225143":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7566b06f":"#Four minutes here\n\n%%capture\n!conda install -y -c conda-forge jax jaxlib flax optax datasets transformers\n!conda install -y importlib-metadata","ae1d2ac1":"#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook\n\nimport os\nif 'TPU_NAME' in os.environ:\n    import requests\n    if 'TPU_DRIVER_MODE' not in globals():\n        url = 'http:' + os.environ['TPU_NAME'].split(':')[1] + ':8475\/requestversion\/tpu_driver_nightly'\n        resp = requests.post(url)\n        TPU_DRIVER_MODE = 1\n\n\n    from jax.config import config\n    config.FLAGS.jax_xla_backend = \"tpu_driver\"\n    config.FLAGS.jax_backend_target = os.environ['TPU_NAME']\n    print('Registered TPU:', config.FLAGS.jax_backend_target)\nelse:\n    print('No TPU detected. Can be changed under \"Runtime\/Change runtime type\".')","c605b153":"import jax\njax.local_devices()","918ec217":"model_checkpoint = \"bert-base-uncased\" # 'roberta-base' has an error remaining are working.\nper_device_batch_size = 32","69b734a6":"#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook\n\nimport numpy as np\nimport datasets\n\ndef simple_rmse(preds, labels):\n    rmse = np.sqrt(np.sum(np.square(preds-labels))\/preds.shape[0])\n    return rmse\n\n\nclass RMSE(datasets.Metric):\n    def _info(self):\n        return datasets.MetricInfo(\n            description=\"Calculates Root Mean Squared Error (RMSE) metric.\",\n            citation=\"TODO: _CITATION\",\n            inputs_description=\"_KWARGS_DESCRIPTION\",\n            features=datasets.Features({\n                'predictions': datasets.Value('float32'),\n                'references': datasets.Value('float32'),\n            }),\n            codebase_urls=[],\n            reference_urls=[],\n            format='numpy'\n        )\n\n    def _compute(self, predictions, references):\n        return {\"RMSE\": simple_rmse(predictions, references)}","8cfaf5d3":"#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook\n\nfrom datasets import load_dataset, load_metric\nraw_train = load_dataset(\"csv\", data_files={'train': ['..\/input\/nlp-with-disaster-tweets-cleaning-data\/train_data_cleaning.csv']})\nraw_test = load_dataset('csv', data_files={'test': ['..\/input\/nlp-with-disaster-tweets-cleaning-data\/test_data_cleaning.csv']})","961bc06f":"# Split the train set into train and valid sets\nraw_train = raw_train[\"train\"].train_test_split(0.1)","090f1c0b":"metric = RMSE()","92d9c730":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","4bb06bba":"#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook\n\ndef preprocess_function(examples):\n    texts = (examples[\"text\"],)\n    processed = tokenizer(*texts, padding=\"max_length\", max_length=128, truncation=True)\n    \n    processed[\"labels\"] = examples[\"target\"]\n    return processed","d2066042":"tokenized_dataset = raw_train.map(preprocess_function, batched=True, remove_columns=raw_train[\"train\"].column_names)","935de6f8":"tokenized_dataset","c4e06c83":"# The test was created by the 0.1 split of the data which is our validation\/evaluation dataset.\ntrain_dataset = tokenized_dataset[\"train\"]\neval_dataset = tokenized_dataset[\"test\"]","fe6b9221":"#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook\n\nfrom transformers import FlaxAutoModelForSequenceClassification, AutoConfig\n\nnum_labels = 1\nseed = 0\n\nconfig = AutoConfig.from_pretrained(model_checkpoint, num_labels=num_labels)\nmodel = FlaxAutoModelForSequenceClassification.from_pretrained(model_checkpoint, config=config, seed=seed)","e298d531":"import flax\nimport jax\nimport optax\n\nfrom itertools import chain\nfrom tqdm.notebook import tqdm\nfrom typing import Callable\n\nimport jax.numpy as jnp\n\nfrom flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\nfrom flax.training import train_state\nfrom flax import traverse_util","8f1dd968":"num_train_epochs = 10\nlearning_rate = 2e-5","46c6439a":"#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook\n\ntotal_batch_size = per_device_batch_size * jax.local_device_count()\nprint(\"The overall batch size (both for training and eval) is\", total_batch_size)","573062b6":"#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook\n\nnum_train_steps = len(train_dataset) \/\/ total_batch_size * num_train_epochs\n\nlearning_rate_function = optax.cosine_onecycle_schedule(transition_steps=num_train_steps, peak_value=learning_rate, pct_start=0.1, )\nprint(\"The number of train steps (all the epochs) is\", num_train_steps)","848fb0ed":"#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook\n\nclass TrainState(train_state.TrainState):\n    logits_function: Callable = flax.struct.field(pytree_node=False)\n    loss_function: Callable = flax.struct.field(pytree_node=False)","90c1c5d5":"#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook\n\ndef decay_mask_fn(params):\n    flat_params = traverse_util.flatten_dict(params)\n    flat_mask = {path: (path[-1] != \"bias\" and path[-2:] != (\"LayerNorm\", \"scale\")) for path in flat_params}\n    return traverse_util.unflatten_dict(flat_mask)","d60a33dd":"#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook\n\ndef adamw(weight_decay):\n    return optax.adamw(learning_rate=learning_rate_function, b1=0.9, b2=0.999, eps=1e-6, weight_decay=weight_decay, mask=decay_mask_fn)","1f144594":"adamw = adamw(1e-2)","1157cfe8":"@jax.jit\ndef loss_function(logits, labels):\n    return jnp.mean((logits[..., 0] - labels) ** 2)\n\n@jax.jit    \ndef eval_function(logits):\n    return logits[..., 0]","0a698626":"#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook\n\nstate = TrainState.create(\n    apply_fn=model.__call__,\n    params=model.params,\n    tx=adamw,\n    logits_function=eval_function,\n    loss_function=loss_function,\n)","4554db2a":"#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook\n\ndef train_step(state, batch, dropout_rng):\n    targets = batch.pop(\"labels\")\n    dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n\n    def loss_function(params):\n        logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n        loss = state.loss_function(logits, targets)\n        return loss\n\n    grad_function = jax.value_and_grad(loss_function)\n    loss, grad = grad_function(state.params)\n    grad = jax.lax.pmean(grad, \"batch\")\n    new_state = state.apply_gradients(grads=grad)\n    metrics = jax.lax.pmean({\"loss\": loss, \"learning_rate\": learning_rate_function(state.step)}, axis_name=\"batch\")\n    return new_state, metrics, new_dropout_rng","045622e1":"parallel_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,))","8d1dc250":"#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook\n\ndef eval_step(state, batch):\n    logits = state.apply_fn(**batch, params=state.params, train=False)[0]\n    return state.logits_function(logits)","1e563a82":"parallel_eval_step = jax.pmap(eval_step, axis_name=\"batch\")","b040db86":"#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook\n\ndef train_data_loader(rng, dataset, batch_size):\n    steps_per_epoch = len(dataset) \/\/ batch_size\n    perms = jax.random.permutation(rng, len(dataset))\n    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n    perms = perms.reshape((steps_per_epoch, batch_size))\n\n    for perm in perms:\n        batch = dataset[perm]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        batch = shard(batch)\n\n        yield batch","f5d65f72":"#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook\n\ndef eval_data_loader(dataset, batch_size):\n    for i in range(len(dataset) \/\/ batch_size):\n        batch = dataset[i * batch_size : (i + 1) * batch_size]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        batch = shard(batch)\n\n        yield batch","c7d2aa4a":"state = flax.jax_utils.replicate(state)","211c8aa0":"#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook\n\nrng = jax.random.PRNGKey(seed)\ndropout_rngs = jax.random.split(rng, jax.local_device_count())","857457eb":"#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook\n\n\nfor i, epoch in enumerate(tqdm(range(1, num_train_epochs + 1), desc=f\"Epoch ...\", position=0, leave=True)):\n    rng, input_rng = jax.random.split(rng)\n\n    # train\n    with tqdm(total=len(train_dataset) \/\/ total_batch_size, desc=\"Training...\", leave=False) as progress_bar_train:\n        for batch in train_data_loader(input_rng, train_dataset, total_batch_size):\n            state, train_metrics, dropout_rngs = parallel_train_step(state, batch, dropout_rngs)\n            progress_bar_train.update(1)\n\n    # evaluate\n    with tqdm(total=len(eval_dataset) \/\/ total_batch_size, desc=\"Evaluating...\", leave=False) as progress_bar_eval:\n        for batch in eval_data_loader(eval_dataset, total_batch_size):\n            labels = batch.pop(\"labels\")\n            predictions = parallel_eval_step(state, batch)\n            metric.add_batch(predictions=chain(*predictions), references=chain(*labels))\n            progress_bar_eval.update(1)\n\n    eval_metric = metric.compute()\n\n    loss = round(flax.jax_utils.unreplicate(train_metrics)['loss'].item(), 3)\n    eval_score = round(list(eval_metric.values())[0], 3)\n    metric_name = list(eval_metric.keys())[0]\n\n    print(f\"{i+1}\/{num_train_epochs} | Train loss: {loss} | Eval {metric_name}: {eval_score}\")","933ef4c8":"#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook\n\ndef preprocess_test_set_function(examples):\n    texts = (examples[\"text\"],)\n    processed = tokenizer(*texts, padding=\"max_length\", max_length=128, truncation=True)\n    \n    return processed","d032aab5":"tokenized_test_dataset = raw_test.map(preprocess_test_set_function, batched=True, remove_columns=raw_test[\"test\"].column_names)","dfbc1107":"test_dataset = tokenized_test_dataset[\"test\"]\ntest_dataset","6cd944ef":"#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook\n\n\ndef test_data_loader(dataset, batch_size):\n    if len(dataset)<batch_size:\n        batch = dataset[:]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        yield batch\n    else:\n        for i in range(len(dataset) \/\/ batch_size):\n            batch = dataset[i * batch_size : (i + 1) * batch_size]\n            batch = {k: jnp.array(v) for k, v in batch.items()}\n\n            yield batch\n        batch = dataset[(i+1) * batch_size:]\n        batch = {k: jnp.array(v) for k, v in batch.items()}\n        yield batch","0f5a3472":"from flax.jax_utils import unreplicate\n\nunrep_state = unreplicate(state)","0c223ec5":"#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook\n\ndef generate_results():\n    preds = []\n    for batch in test_data_loader(test_dataset, total_batch_size):\n        if jax.process_index()==0:\n            predictions = unrep_state.apply_fn(**batch, train=False, return_dict=False)\n            preds.append(predictions[0])\n    return preds","b44f8a07":"preds = generate_results()","ec13d4d7":"import numpy as np\npreds = np.vstack([np.asarray(x) for x in preds])\npreds","91249e75":"#import pandas as pd\n#sample = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n#sample.target = preds\n#sample","516ebe01":"#sample.to_csv('submission.csv',index=False)","be3533d8":"Next, we replicate\/copy the weight parameters on each device, so that we can pass them to our pmapped functions.","097eb78d":"#Create the initial train state","a542863d":"We won't shard our data anymore because usually the test sets are very small and can be done entirely on one-core without having the additional overheads. So, we also have to \"un-shard\" our model and run entirely on the single device of the device slice. So we use the unreplicate method in the flax library.","0d15f762":"#AdamW Optimizer\n\nWe will be using the standard Adam optimizer with weight decay. For more information on AdamW (Adam + weight decay), one can take a look at this blog post. weight_decay value of 0.01 is a good starting point, you can tweak this hyper-parameter and experiment with how it influences the final trained model.\n\nRegularizing the bias and\/or LayerNorm has not shown to improve performance and can even be disadvantageous, which is why we disable it here. For more information on this, please check out the following blog post or paper.\n\nHence we create a decay_mask_fn which makes sure that weight decay is not applied to any bias or LayerNorm weights. This can easily be done by passing a mask_fn to optax.adamw.\n\nNOTE: Beginners (myself) can ignore the decay_mask_fn, the changes are minimal if you leave out doing this step.","6ce2ac33":"I used the One-Cycle LR Scheduler with Cosine Annealing. It is super easy to create this LR Schedule with the Optax library, it is the recommended library while using any JAX based NN libraries. Optax is being developed by DeepMind has several amazing features, definitely give it a try!\n\nTODO: Add citations to the original One-Cycle and Cosine Annealing papers.","43aca521":"<h1 style=\"background-color:#DC143C; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">JAX for the Impatient<\/h1>\n\nJAX is NumPy on the CPU, GPU, and TPU, with great automatic differentiation for high-performance machine learning research.\n\nThey cover the basics of JAX so that you can get started with Flax, however we very much recommend that you go through JAX\u2019s documentation here after going over the basics there.\n\nhttps:\/\/flax.readthedocs.io\/en\/latest\/notebooks\/jax_for_the_impatient.html","9592af2a":"#Create a Train State\n\nNext, we will create the training state that includes the optimizer, the loss function, and is responsible for updating the model's parameters during training.\n\nMost JAX transformations (notably jax.jit) require functions that are transformed to have no side-effects as it follows a functional programming type paradigm at its core. This is because any such side-effects will only be executed once, when the Python version of the function is run during compilation (see Stateful Computations in JAX). As a consequence, Flax models (which can be transformed by JAX transformations) are immutable, and the state of the model (i.e., its weight parameters) are stored outside of the model instance.\n\nFlax provides a convenience class flax.training.train_state.TrainState, which stores things such as the model parameters, the loss function, the optimizer, and exposes an apply_gradients function to update the model's weight parameters.\n\nWe create a derived TrainState class that additionally stores the model's forward pass as eval_function as well as a loss_function.","b240abbd":"#Training and evaluation loop","96fdb482":"#Training\n\nNow we define the full training loop. For each batch in each epoch, we run a training step. Here, we also need to make sure that the PRNGKey is sharded\/split over each device. Having completed an epoch, we report the training metrics and can run the evaluation.\n\nThe first batch takes a bit longer to process but nothing to worry because during the first batch, XLA compiler is working hard to make everything super fast. The first takes close to 5 mins for processing and then entire epochs take ~5 sec to process. Aren't TPUs amazing!!\n\n5 seconds for an entire EPOCH!!\n\nNote: The times mentioned above are an average estimate over 8 different runs on several different TPU machines and several model architectures.","803fe317":"#Generating Results\n\nOur test dataset has slightly different pre-processing step because we do not have a label in the dataset. So, we should handle accordingly.","f44c27b2":"#Defining the training and evaluation step\n\nDuring fine-tuning, we want to update the model parameters and evaluate the performance after each epoch.\n\nLet's write the functions train_step and eval_step accordingly. During training the weight parameters should be updated as follows:\n\nDefine a loss function loss_function that first runs a forward pass of the model given data input. Remember that Flax models are immutable, and we explicitly pass it the state (in this case the model parameters and the RNG). loss_function returns a scalar loss (using the previously defined state.loss_function) between the model output and input targets.\nDifferentiate this loss function using jax.value_and_grad. This is a JAX transformation called automatic differentiation, which computes the gradient of loss_function given the input to the function (i.e., the parameters of the model), and returns the value and the gradient in a pair (loss, gradients).\nCompute the mean gradient over all devices using the collective operation lax.pmean. As we will see below, each device runs train_step on a different batch of data, but by taking the mean here we ensure the model parameters are the same on all devices.\nUse state.apply_gradients, which applies the gradients to the weights.\nBelow, you can see how each of the described steps above is put into practice.\n\nNOTE: Taken from HuggingFace examples","90a29273":"<h1 style=\"background-color:#DC143C; font-family:'Brush Script MT',cursive;color:white;font-size:200%; text-align:center;border-radius: 50% 20% \/ 10% 40%\">JAX is NumPy + autodiff + GPU\/TPU<\/h1>\n\nIt allows for fast scientific computing and machine learning with the normal NumPy API (+ additional APIs for special accelerator ops when needed)\n\nJAX comes with powerful primitives, which you can compose arbitrarily:\n\nAutodiff (jax.grad): Efficient any-order gradients w.r.t any variables\n\nJIT compilation (jax.jit): Trace any function \u27f6 fused accelerator ops\n\nVectorization (jax.vmap): Automatically batch code written for individual samples\n\nParallelization (jax.pmap): Automatically parallelize code across multiple accelerators (including across hosts, e.g. for TPU pods)\n\nIf you don\u2019t know JAX but just want to learn what you need to use Flax, you can check our JAX for the impatient notebook.\n\nhttps:\/\/flax.readthedocs.io\/en\/latest\/overview.html","f551a893":"#Pre-process the dataset\n\nThis is a very generic pre-processing nothing special. Just tokenized the sentence and padded it appropriately.","6ac67042":"#Started at 18:17 Finished 19:14","3bfc5065":"There are 8 cores in TPUv3-8, so the effective batch_size = 8 * per_device_batch_size","8a111e1d":"Now we clean-up and make our results \"Submission ready\". First we convert all JAX DeviceArray objects to Numpy arrays, then we create a submission file.","639ec75b":"#Background: JAX  https:\/\/flax.readthedocs.io\/en\/latest\/overview.html#background-jax\n\nTODO: Add an example for running on Google Cloud.\n\n#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook","0c7a0a6d":"#Generation\n\nFinal step. We have successfully fine-tuned a BERT model to the Lit-Readability task. That's amazing! It took us less than 10 mins to reach a very good score! Now it is time to get our model predictions on our test set.","18608c2e":"#Model","597599aa":"#Loading dataset and metric\n\nI personally prefer HugginFace datasets because they are very well designed and makes it easy to pre-process all the samples very easily and it has several features like easily loading from the CSV file without using any Pandas data frame objects as intermediates.","3ef58e88":"#Define Data Loaders\n\nIn a final step before we can start training, we need to define the data collators. The data collator is important to shuffle the training data before each epoch and to prepare the batch for each training and evaluation step.\n\nFirst, a random permutation of the whole dataset is defined. Then, every time the training data collator is called the next batch of the randomized dataset is extracted, converted to a JAX array and sharded over all local TPU devices.","dac2d979":"Now, we want to do parallelized training over all TPU devices. To do so, we use jax.pmap. This will compile the function once and run the same program on each device (it is an SPMD program). When calling this pmapped function, all inputs (\"state\", \"batch\", \"dropout_rng\") should be replicated for all devices, which means that the first axis of each argument is used to map over all TPU devices.\n\nThe argument donate_argnums is used to tell JAX that the first argument \"state\" is \"donated\" to the computation, because it is not needed anymore afterwards. XLA can make use of donated buffers to reduce the memory needed.","abd01e90":"#Loss and eval functions\n\nThe standard loss function for regression problems is the MSE loss. The book by Bishop has an additional 0.5 term, but we're skipping in that without loss of generality. That term just scales the loss by a constant factor and doesn't have an impact on the gradients (other than scaling).","c54511df":"#Code by Kartheek Akella  https:\/\/www.kaggle.com\/asvskartheek\/bert-tpus-jax-huggingface\/notebook"}}