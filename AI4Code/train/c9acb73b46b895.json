{"cell_type":{"bf73df56":"code","cace88de":"code","ca112e8c":"code","7a7e5791":"code","e2ec8d2a":"code","55f08c36":"code","797c46b3":"code","e109f290":"code","01ce2293":"code","00f8705e":"code","347ba57b":"code","6d6f4984":"code","e171f007":"markdown","9f6d567c":"markdown","a35a1439":"markdown","70040e1e":"markdown","82c342ca":"markdown","00165ebd":"markdown","42c6b37c":"markdown","481332d3":"markdown","1bec5337":"markdown","51b3147f":"markdown","17968e71":"markdown"},"source":{"bf73df56":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cace88de":"import random\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nfrom IPython.display import Image, display\nfrom tensorflow.keras.preprocessing.image import load_img\nimport PIL\nfrom PIL import ImageOps\n\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras import layers","ca112e8c":"input_dir = \"..\/input\/gravel-sand-wood\/Dataset\/train\/images\"\ntarget_dir = \"..\/input\/gravel-sand-wood\/Dataset\/valid\/images\"\nimg_size = (160, 160)\nnum_classes = 3\nbatch_size = 16\n\ninput_img_paths = sorted(\n    [\n        os.path.join(input_dir, fname)\n        for fname in os.listdir(input_dir)\n        if fname.endswith(\".jpg\")\n    ]\n)\ntarget_img_paths = sorted(\n    [\n        os.path.join(target_dir, fname)\n        for fname in os.listdir(target_dir)\n        if fname.endswith(\".jpg\") and not fname.startswith(\".\")\n    ]\n)\n\nprint(\"Number of samples:\", len(input_img_paths))","7a7e5791":"#Display sample of Image Dataset\ni = 7\nfigure, ax = plt.subplots(nrows=1,ncols=2,figsize=(8,8))\nax.ravel()[0].imshow(mpimg.imread(input_img_paths[i]))\nax.ravel()[0].set_title(\"Original image\")\nax.ravel()[0].set_axis_off()\nax.ravel()[1].imshow(mpimg.imread(target_img_paths[i]))\nax.ravel()[1].set_title(\"Mask\")\nax.ravel()[1].set_axis_off()\n#ax.ravel()[2].imshow(PIL.ImageOps.autocontrast(load_img(target_img_paths[i])))\n#ax.ravel()[2].set_title(\"Contrast of mask\")\n#ax.ravel()[2].set_axis_off()\nplt.tight_layout()","e2ec8d2a":"class PetsDataset(keras.utils.Sequence):\n    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n\n    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n        self.batch_size = batch_size\n        self.img_size = img_size\n        self.input_img_paths = input_img_paths\n        self.target_img_paths = target_img_paths\n\n    def __len__(self):\n        return len(self.target_img_paths) \/\/ self.batch_size\n\n    def __getitem__(self, idx):\n        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n        i = idx * self.batch_size\n        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n        for j, path in enumerate(batch_input_img_paths):\n            img = load_img(path, target_size=self.img_size)\n            x[j] = img\n        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n        for j, path in enumerate(batch_target_img_paths):\n            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n            y[j] = np.expand_dims(img, 2)\n            # Ground truth labels are 1, 2, 3. Subtract one to make them 0, 1, 2:\n            y[j] -= 1\n        return x, y","55f08c36":"def get_model(img_size, num_classes):\n    inputs = keras.Input(shape=img_size + (3,))\n\n    ### [First half of the network: downsampling inputs] ###\n\n    # Entry block\n    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n    x = layers.BatchNormalization()(x)\n    x = layers.Activation(\"relu\")(x)\n\n    previous_block_activation = x  # Set aside residual\n\n    # Blocks 1, 2, 3 are identical apart from the feature depth.\n    for filters in [64, 128, 256]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n\n        # Project residual\n        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n            previous_block_activation\n        )\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    ### [Second half of the network: upsampling inputs] ###\n\n    for filters in [256, 128, 64, 32]:\n        x = layers.Activation(\"relu\")(x)\n        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.Activation(\"relu\")(x)\n        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n        x = layers.BatchNormalization()(x)\n\n        x = layers.UpSampling2D(2)(x)\n\n        # Project residual\n        residual = layers.UpSampling2D(2)(previous_block_activation)\n        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n        x = layers.add([x, residual])  # Add back residual\n        previous_block_activation = x  # Set aside next residual\n\n    # Add a per-pixel classification layer\n    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n\n    # Define the model\n    model = keras.Model(inputs, outputs)\n    return model\n\n\n# Free up RAM in case the model definition cells were run multiple times\nkeras.backend.clear_session()\n\n# Build model\nmodel = get_model(img_size, num_classes)\nmodel.summary()","797c46b3":"#Split Dataset into a training and a validation set\n\nval_samples = 201 # 85% Training -- 15% Validation (1341 samples 15% is 201)\nrandom.Random(335).shuffle(input_img_paths)#Originalis here is 25%\nrandom.Random(335).shuffle(target_img_paths)\ntrain_input_img_paths = input_img_paths[:-val_samples]\ntrain_target_img_paths = target_img_paths[:-val_samples]\nval_input_img_paths = input_img_paths[-val_samples:]\nval_target_img_paths = target_img_paths[-val_samples:]\n\n# Instantiate data Sequences for each split\ntrain_gen = PetsDataset(\n    batch_size, img_size, train_input_img_paths, train_target_img_paths\n)\nval_gen = PetsDataset(batch_size, img_size, val_input_img_paths, val_target_img_paths)","e109f290":"#Training\n\n# We use the \"sparse\" version of categorical_crossentropy\n# because our target data is integers.\n#I changed to binary\nmodel.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=['accuracy'])\n\ncallbacks = [\n    keras.callbacks.ModelCheckpoint(\"pets_segmentation.h5\", save_best_only=True)\n]\n\nepochs = 30\nmodelunet=model.fit(train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks)","01ce2293":"# summarize history for accuracy\nplt.plot(modelunet.history['accuracy'])\nplt.plot(modelunet.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.grid(True)\nplt.show()\n# summarize history for loss\nplt.plot(modelunet.history['loss'])\nplt.plot(modelunet.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.grid(True)\nplt.show()","00f8705e":"#Inference\n\n# Generate predictions for all images in the validation set\nval_gen = PetsDataset(batch_size, img_size, val_input_img_paths, val_target_img_paths)\nval_preds = model.predict(val_gen)","347ba57b":"#Display results for validation image\n\ndef display_mask(i):\n    \"\"\"Quick utility to display a model's prediction.\"\"\"\n    mask = np.argmax(val_preds[i], axis=-1)\n    mask = np.expand_dims(mask, axis=-1)\n    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n    return img","6d6f4984":"# Display image #20\ni = 20\nfigure, ax = plt.subplots(nrows=1,ncols=3,figsize=(8,5))\nax.ravel()[0].imshow(mpimg.imread(val_input_img_paths[i]))\nax.ravel()[0].set_title(\"Original image\")\nax.ravel()[0].set_axis_off()\nax.ravel()[1].imshow(mpimg.imread(val_target_img_paths[i]))\nax.ravel()[1].set_title(\"Mask\")\nax.ravel()[1].set_axis_off()\nax.ravel()[2].imshow(display_mask(i))\nax.ravel()[2].set_title(\"Predicted mask \")\nax.ravel()[2].set_axis_off()\nplt.tight_layout()","e171f007":"Not a mask or even a predicted one.","9f6d567c":"#TRAINING\n\nI changed `sparse_categorical_crossentropy` to `binary_entropy`","a35a1439":"#Inference","70040e1e":"#Display sample of Image Dataset","82c342ca":"#Codes by Ammar Alhaj Ali https:\/\/www.kaggle.com\/ammarnassanalhajali\/image-segmentation-with-a-u-net-and-keras\/data","00165ebd":"#I don't have masks (only images from train and valid)","42c6b37c":"#Build the U-Net Model Architecture","481332d3":"#History for Accuracy","1bec5337":"U-Net is a convolutional neural network that was developed for biomedical image segmentation at the Computer Science Department of the University of Freiburg. The network is based on the fully convolutional network and its architecture was modified and extended to work with fewer training images and to yield more precise segmentations. Segmentation of a 512 \u00d7 512 image takes less than a second on a modern GPU.\n\nhttps:\/\/en.wikipedia.org\/wiki\/U-Net","51b3147f":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcReLhJVrbyn23sEmB2zsYxZenRXqZ3Ovi_q4A&usqp=CAU)youtube.com","17968e71":"#I kept the name Pet Dataset to avoid errors."}}