{"cell_type":{"6f765ee9":"code","e821384c":"code","fc2691b9":"code","9826bf73":"code","7a7eafcc":"code","98cb88a5":"code","db2518d1":"code","c978af74":"code","4e40339c":"code","80cc872d":"code","c0c192df":"code","09d10b59":"code","9bf6a049":"code","989b88d8":"code","a4259d98":"code","a011b765":"code","ebb99d89":"code","d847d32e":"code","b9022274":"code","3cbeeeae":"code","ed2871c0":"code","0e24529b":"code","0008ab35":"code","1bc2fa31":"code","c6337b35":"code","1c5df0db":"code","d3008770":"code","03fc2b5d":"markdown","0270fffd":"markdown","bbf71384":"markdown","0a5378ae":"markdown","6d42eaa0":"markdown","561c89c9":"markdown","fe63cdba":"markdown","11c06b47":"markdown","bf9659fd":"markdown","8bc59349":"markdown","6c7fb778":"markdown","d5b30255":"markdown","4c63a1ed":"markdown","57ab2c61":"markdown","bb1eb0fc":"markdown","fbc0394d":"markdown","f0b990c0":"markdown"},"source":{"6f765ee9":"!pip install keras-adabound","e821384c":"import numpy as np\nimport tensorflow as tf\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom keras import Sequential\nfrom keras import layers\nfrom keras import backend as K\nfrom keras.layers.core import Dense\nfrom keras import regularizers\nfrom keras.layers import Dropout\nfrom keras.constraints import max_norm\nfrom keras.callbacks import EarlyStopping\n\nfrom keras_adabound import AdaBound\n\npd.set_option('display.max_columns', 200)","fc2691b9":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","9826bf73":"train_df.head()","7a7eafcc":"test_df.head()","98cb88a5":"train_df.target.value_counts()","db2518d1":"X_train = train_df.drop([\"target\", \"ID_code\"], axis=1)\nX_test = test_df.drop([\"ID_code\"], axis=1)\ny_train = train_df[\"target\"]","c978af74":"X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=71)","4e40339c":"X_train.shape, X_valid.shape, y_train.shape, y_valid.shape, X_test.shape","80cc872d":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_valid = sc.transform(X_valid)\nX_test = sc.transform(X_test)","c0c192df":"# Add RUC metric to monitor NN\ndef auc(y_true, y_pred):\n    auc = tf.metrics.auc(y_true, y_pred)[1]\n    K.get_session().run(tf.local_variables_initializer())\n    return auc","09d10b59":"# callback = EarlyStopping(monitor='val_loss', min_delta=0, patience=50, verbose=1, mode='auto', baseline=None, restore_best_weights=True)","9bf6a049":"input_dim = X_train.shape[1]","989b88d8":"def create_model():\n    model = Sequential()\n    \n    # Input layer\n    model.add(Dense(units = 200, activation = \"relu\", input_dim = input_dim, kernel_initializer = \"normal\", kernel_regularizer=regularizers.l2(0.005), \n                    kernel_constraint = max_norm(5.)))\n    # Add dropout regularization\n    model.add(Dropout(rate=0.2))\n\n    # Hidden layer\n    model.add(Dense(units = 200, activation='relu', kernel_regularizer=regularizers.l2(0.005), kernel_constraint=max_norm(5)))\n    # Add dropout regularization\n    model.add(Dropout(rate=0.2))\n\n    # Stacking hidden layers\n    for _ in range(7):\n        # Hidden layer\n        model.add(Dense(200, activation='relu', kernel_regularizer=regularizers.l2(0.005), kernel_constraint=max_norm(5)))\n        # Add dropout regularization\n        model.add(Dropout(rate=0.3))\n\n    # Hidden layer\n    model.add(Dense(200, activation='tanh', kernel_regularizer=regularizers.l2(0.005), kernel_constraint=max_norm(5)))\n    # Add dropout regularization\n    model.add(Dropout(rate=0.4))\n\n    # Output layer\n    model.add(layers.Dense(units = 1, activation='sigmoid'))\n\n    return model","a4259d98":"model_adabound = create_model()\nmodel_adabound.summary()","a011b765":"model_adabound.compile(loss='binary_crossentropy', optimizer=AdaBound(lr=1e-3, final_lr=0.1), metrics=['accuracy', auc])","ebb99d89":"history_adabound = model_adabound.fit(X_train, y_train, batch_size=128, epochs=100, validation_data=(X_valid, y_valid)) #, callbacks=[callback])","d847d32e":"y_pred = model_adabound.predict_proba(X_valid)\nroc_auc_score(y_valid, y_pred)","b9022274":"pred = model_adabound.predict(X_test)\npred_ = pred[:,0]","3cbeeeae":"pred_","ed2871c0":"sub_df = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nsub_df[\"target\"] = pred_\nsub_df.head()","0e24529b":"sub_df.to_csv('Santander_submit_simple_DNN_AdaBound.csv', index=False)","0008ab35":"model_adam = create_model()\nmodel_adam.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', auc])\nhistory_adam = model_adam.fit(X_train, y_train, batch_size=128, epochs=100, validation_data=(X_valid, y_valid)) #, callbacks=[callback])","1bc2fa31":"y_pred_adam = model_adam.predict_proba(X_valid)\nroc_auc_score(y_valid, y_pred_adam)","c6337b35":"def plot_history(history1, history2, history1_label, history2_label, metric):\n    fig = plt.figure(figsize=(10, 10))\n    fig.patch.set_facecolor(\"white\")\n    \n    plt.plot(history1.history[metric], label='{} {}'.format(history1_label, metric))\n    plt.plot(history1.history['val_{}'.format(metric)], label='{} val_{}'.format(history1_label, metric))\n    plt.plot(history2.history[metric], label='{} {}'.format(history2_label, metric))\n    plt.plot(history2.history['val_{}'.format(metric)], label='{} val_{}'.format(history2_label, metric))\n    plt.title(metric)\n    plt.xlabel('epoch')\n    plt.ylabel(metric)\n    plt.legend()\n    plt.show()","1c5df0db":"plot_history(history_adam, history_adabound, 'Adam', 'AdaBound', 'loss')","d3008770":"plot_history(history_adam, history_adabound, 'Adam', 'AdaBound', 'auc')","03fc2b5d":"## Installing keras-adabound\n- Required : \"Internet connected\" Kernel setting","0270fffd":"## Visualizing transitions of loss & auc through training","bbf71384":"## **According to the graphs above, training with AdaBound provides better LOSS and AUC than that with Adam!**","0a5378ae":"## Making Predictions","6d42eaa0":"## **AdaBound's AUC is better than Adam's one.**","561c89c9":"## Reading Data Sets","fe63cdba":"## Simple Neural Network Model","11c06b47":"## Create Data Frames","bf9659fd":"# [For Your Reference] Experiment : AdaBoost v.s. Adam ","8bc59349":"## Feature Scaling","6c7fb778":"## Training with AdaBound Optimizer","d5b30255":"## Keras Callback","4c63a1ed":"## Checking AUC with Validation Data","57ab2c61":"# AdaBound Optimizer Trial with Simple DNN + v.s. Adam Experiment\n\n## Reference\n- Original Paper : https:\/\/openreview.net\/pdf?id=Bkg3g2R9FX\n- Website : https:\/\/www.luolc.com\/publications\/adabound\/\n\n<img src=\"https:\/\/www.luolc.com\/assets\/research\/adabound\/adabound-banner.png\">\n\n## Using AdaBound for Keras\n- GitHub : https:\/\/github.com\/CyberZHG\/keras-adabound","bb1eb0fc":"## Submission","fbc0394d":"## AUC metric","f0b990c0":"## Training with Adam"}}