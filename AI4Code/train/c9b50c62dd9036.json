{"cell_type":{"d685768d":"code","21fc2530":"code","2d857678":"code","04395695":"code","8e4fb022":"code","f3dbce8c":"code","6be545ab":"code","5cac0cf5":"code","18f919c8":"code","070abe95":"code","f9443bb3":"code","ae616796":"code","9587eb8b":"code","24b8559e":"markdown","e58c27d6":"markdown","a242572e":"markdown","96cf5872":"markdown"},"source":{"d685768d":"import pandas as pd\ndf = pd.read_csv('..\/input\/tabular-playground-series-feb-2021\/train.csv')\ndf.head()","21fc2530":"cat_cols = [x for x in df.columns if x.startswith('cat')]\ncat_cols","2d857678":"for cat in cat_cols:\n    df[cat] = df[cat].astype(\"category\")","04395695":"from sklearn.model_selection import train_test_split\ntrain, test = train_test_split(df, train_size = 0.5)","8e4fb022":"from lightgbm.sklearn import LGBMRegressor\nbt = LGBMRegressor(n_jobs=1)\nx_train = train.drop(columns=['target','id'])\ny_train = train['target']","f3dbce8c":"bt.fit(x_train, y_train)","6be545ab":"x_test = test.drop(columns=['target','id'])\ny_test = test['target']","5cac0cf5":"pred = bt.predict(x_test)","18f919c8":"from sklearn.metrics import mean_squared_error\nimport math\nmath.sqrt(mean_squared_error(y_test, pred))","070abe95":"from plotnine import ggplot, geom_point, aes\nfrom plotnine.geoms import geom_histogram, geom_bar, geom_boxplot, geom_bin2d, geom_density_2d, geom_segment\nfrom plotnine.labels import ggtitle","f9443bb3":"df_plot = pd.DataFrame({'pred':pred, 'y':y_test})","ae616796":"ggplot(df_plot, aes(x='y_test', y='pred'))+geom_bin2d(bins = 100)+geom_segment(aes(x = 2, y = 2, xend = 10, yend = 10))","9587eb8b":"df_plot_dist = pd.melt(df_plot)\nfrom plotnine.facets import facet_grid\nggplot(df_plot_dist, aes(x='value', ))+geom_histogram(bins=100)+facet_grid('variable ~ .')","24b8559e":"## Conclusion\nThe LightGBM model does a very poor job of capturing the bimodel distribution of the output. Is there any way we can do better?","e58c27d6":"## Plotting the model's response","a242572e":"# Load data","96cf5872":"# Training a simple lightgbm model"}}