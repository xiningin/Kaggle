{"cell_type":{"791b35ad":"code","92fab6fd":"code","5ed57f82":"code","ebff49db":"code","874c8022":"code","b1082a09":"code","634c3527":"code","7671d598":"code","d499d88b":"code","7f5c5376":"code","6a2181a9":"code","262b6dbe":"code","df3f6a8c":"code","94d453a1":"code","6e0889a3":"code","b777ecec":"code","e785ad17":"code","dcde1b68":"code","7f631706":"code","d77af787":"code","06fd6b16":"code","4beeadde":"code","5d6b8226":"code","7b2c2159":"code","6a12ed0c":"code","3c718be9":"code","df269a06":"code","e64032fb":"markdown","cac4ae02":"markdown","e2fb9afc":"markdown","e2b8c698":"markdown","3c940ba7":"markdown","6183721d":"markdown","25ac3ada":"markdown","3b845d1b":"markdown","8450ba25":"markdown","7c13dd80":"markdown","2059964d":"markdown"},"source":{"791b35ad":"import numpy as np\nimport matplotlib\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import KFold\n%matplotlib inline\nmatplotlib.style.use('ggplot')","92fab6fd":"np.random.seed(7777)","5ed57f82":"Theta = np.array([5,2])\nX = np.array([[1,1], [9,7]])\nY = np.dot(Theta, X)","ebff49db":"print(\"Theta =\", Theta)\nprint(\"X =\", X, sep=\"\\n\")\nprint(\"Y =\", Y)","874c8022":"x_start, x_stop = -15.0, 30.0\nTheta_true = np.array([50.0, 10.0])\nnum = 100 #number of examples\nmu, sigma = 0, 20.0","b1082a09":"def f(Theta, x):\n    return Theta[0] + Theta[1] * x","634c3527":"def getXVector(x_start, x_stop, num):\n    return np.linspace(x_start, x_stop, num)","7671d598":"def getX(X_vector):\n    return np.stack([np.ones(shape=(X_vector.shape[0],)), X_vector])","d499d88b":"def getNoise(mu, sigma, num):\n    return np.random.normal(mu, sigma, num)","7f5c5376":"def getY(Theta_true, X):\n    return np.dot(Theta_true, X)","6a2181a9":"def getYTrue(Y, noise):\n    return Y + noise","262b6dbe":"noise = getNoise(mu, sigma, num)\nX_vector = getXVector(x_start, x_stop, num)\nY_f = f(Theta_true, X_vector) + noise #get true Y from function f\nX = getX(X_vector)\nY = getY(Theta_true, X)\nY_true = getYTrue(Y, noise) #get true Y from matrix product\nprint(np.array_equal(Y_f, Y_true)) #Y from function f and matrix product is the same","df3f6a8c":"def showPlot(**kwargs):\n    X = kwargs['X']\n    Y = kwargs['Y']\n    if len(X.shape) == 2:\n        X = X[1] #matrix to vector\n    if not isinstance(Y, list):\n        Y = [Y]\n    labels = ['x', 'y'] if 'labels' not in kwargs else kwargs['labels']\n    legend = len(Y) * [\"\"] if 'legend' not in kwargs else kwargs['legend']\n    with_interval = False if 'with_interval' not in kwargs \\\n                                          else kwargs['with_interval']\n    if with_interval:\n        for i in range(len(Y)):\n            plt.fill_between(X, [y[0] for y in Y[i]], \n                                [y[1] for y in Y[i]], alpha=0.3) #show confidence intervals\n            Y[i] = [(y[0]+y[1])\/2.0 for y in Y[i]]\n\n    for i in range(len(Y)):\n        plt.plot(X, Y[i], label=legend[i])\n    plt.xlabel(labels[0])\n    plt.ylabel(labels[1])\n    plt.legend()\n    plt.show()","94d453a1":"showPlot(X=X, Y=Y_true)","6e0889a3":"def calcThetaProba(X, Y_true):\n    return np.dot(np.dot(np.linalg.inv(np.dot(X, X.T)), X), Y_true)","b777ecec":"Theta_proba = calcThetaProba(X, Y_true)\nTheta_proba","e785ad17":"def predictY(Theta, X):\n    return f(Theta, X[1])","dcde1b68":"Y_predicted = predictY(Theta_proba, X)","7f631706":"def error_f(Y_true, Y_predicted):\n    return (Y_true - Y_predicted) ** 2","d77af787":"def calcError(Y_true, Y_predicted):\n    return np.mean(error_f(Y_true, Y_predicted))","06fd6b16":"error = calcError(Y_true, Y_predicted)\nerror","4beeadde":"showPlot(X=X, Y=[Y_true, Y_predicted])","5d6b8226":"def crossVal(Theta_true, mu, sigma, num, n_splits):\n    noise = getNoise(mu, sigma, num)\n    X_vector = getXVector(x_start, x_stop, num)\n    X = getX(X_vector)\n    Y = getY(Theta_true, X)\n    Y_true = getYTrue(Y, noise)\n    splits = KFold(n_splits=n_splits, shuffle=True, random_state=7777)\n    errors_train = []\n    errors_test = []\n    for train_index, test_index in splits.split(X.T):\n        X_train, X_test = X.T[train_index].T, X.T[test_index].T\n        Y_train, Y_test = Y_true[train_index], Y_true[test_index]\n        Theta_proba = calcThetaProba(X_train, Y_train)\n        Y_train_predicted = predictY(Theta_proba, X_train)\n        Y_test_predicted = predictY(Theta_proba, X_test)\n        errors_train.append(calcError(Y_train, Y_train_predicted))\n        errors_test.append(calcError(Y_test, Y_test_predicted))\n    return errors_train, errors_test","7b2c2159":"Theta_true = [20.0, -3.0]\nmu = 0.0\nsigma = 5.0\nnum = 100\nn_splits = 10\nerrors_train, errors_test = crossVal(Theta_true, mu, sigma, num, n_splits)\nprint(\"Train error =\", np.mean(errors_train))\nprint(\"Test error =\", np.mean(errors_test))","6a12ed0c":"def calcError95Interval(errors): #95% confidence interval of errors\n    mean = np.mean(errors, axis = 1)\n    std = np.std(errors, axis = 1)\n    return np.dstack((mean - 2.0 * std, mean + 2.0 * std))[0]","3c718be9":"def multipleCrossVal(**kwargs):\n    for param_name in kwargs:\n        #default params\n        params = {\n            \"Theta_true\": [20.0, -3.0],\n            \"mu\": 0.0,\n            \"sigma\": 5.0,\n            \"num\": 100,\n            \"n_splits\": 10}\n        \n        errors_train = []\n        errors_test = []\n        for param in kwargs[param_name]:\n            params[param_name] = param\n            error_train, error_test = crossVal(**params)\n            errors_train.append(error_train)\n            errors_test.append(error_test)\n        train_intervals = calcError95Interval(errors_train)\n        test_intervals = calcError95Interval(errors_test)\n        showPlot(X=kwargs[param_name], Y=[train_intervals, test_intervals], labels=[param_name, \"errors\"], \\\n                 legend=[\"Train\", \"Test\"], with_interval=True)","df269a06":"mu = np.linspace(0.0, 5.0, 50)\nsigma = np.linspace(0.0, 10.0, 50)\nnum = np.linspace(20, 300, 100).astype(np.int32)\nmultipleCrossVal(mu=mu, sigma=sigma, num=num)","e64032fb":"## Matrix product in numpy\nDenote $\\Theta$ as a feature vector. Then we write the original function in the form $y = \\Theta_0 + \\Theta_1 x$. We can rewrite it in matrix form by adding in $X$ a zero row of features filled with 1: $Y=\\Theta X$.","cac4ae02":"## Dependence of errors on initial variables","e2fb9afc":"## Searching for theta","e2b8c698":"You can see that changing the $\\mu$ of noise does not affect the error. An increase in $\\sigma$ of noise increases both the mathematical expectation of errors and the error interval. An increase in the number of examples reduces the error interval and the difference between the training error and the test error, which indicates a decrease in overfitting.","3c940ba7":"### Show true function","6183721d":"## Cross validation\nCalculate expectations of train and test errors. To do this, we split the set into several folds, perform training on these sets, calculate the errors and calculate the average value of these errors","25ac3ada":"# Linear Regression\nHere I will show an example of an analytical solution to linear regression. The true function will be a simple linear function of the form $y = ax + b$, to which noise from the normal distribution is added.","3b845d1b":"## Import libraries","8450ba25":"## Generate true function","7c13dd80":"A vector can be written as a row and as a column. In numpy vector is written as a row. In matrix, column is an example, row is a feature, and zero row is for bias.","2059964d":"## Fix seed for noise generator"}}