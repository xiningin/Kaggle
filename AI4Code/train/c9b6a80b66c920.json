{"cell_type":{"919a19b1":"code","baf510a8":"code","4ae426b7":"code","2cd8b758":"code","f3eccda5":"code","f73aebaa":"code","129d32ee":"code","f8b62a87":"code","d62a2656":"code","988f19c9":"code","3924aa3c":"code","53c52fa9":"code","3ffd8352":"code","175e06b2":"code","7ebdf6d5":"code","070fe577":"code","2c4ffb02":"code","f8b20b58":"code","1878ea32":"code","369c2c8e":"code","93721b53":"code","5368db55":"code","f26d7b72":"code","5b2d91a9":"code","831b85d0":"code","c7225273":"code","a6d6c6fd":"code","726d2fd6":"code","fe32c056":"code","4bec1f99":"code","c3928c88":"code","843f7774":"code","b87d9ac7":"code","ceb261ec":"code","34dd896a":"code","f0411970":"code","4cd39d89":"code","0f14c4f9":"code","33591154":"code","6c5800ed":"code","1e08b876":"code","f579375b":"code","6f40d545":"code","be42db56":"code","9ce8f79f":"markdown","6ac34307":"markdown","248e3b34":"markdown","a05f8653":"markdown","69523db2":"markdown","9b2f3fe7":"markdown","63ba6157":"markdown","864d2800":"markdown","9016e210":"markdown","cbf7cefc":"markdown","1ac3f07b":"markdown","141c07ec":"markdown","520e076e":"markdown","2887cfa9":"markdown","6c509e09":"markdown","9f38bdff":"markdown","849b868a":"markdown","508359d1":"markdown","dc4b2374":"markdown","57956a2f":"markdown","f80ff80d":"markdown","0cee3d32":"markdown","4a565d64":"markdown","988bbc4a":"markdown"},"source":{"919a19b1":"sentence = \"\"\"Hello Reader! How are you doing. Glad to meet you today.\"\"\"\nsentence.split()","baf510a8":"import numpy as np\ntokens = sentence.split() # Tokenization\nvocab = sorted(set(tokens)) # sort(lexically) and extarct unique words\nprint(f\"Here is the sorted text: {' '.join(vocab)}\")","4ae426b7":"one_hot_vectors = np.zeros((len(tokens), len(vocab)), int) # Create a array of zero vectors \none_hot_vectors","2cd8b758":"for i, word in enumerate(tokens):\n    one_hot_vectors[i, vocab.index(word)] = 1\none_hot_vectors    ","f3eccda5":"import pandas as pd\npd.DataFrame(one_hot_vectors, columns = vocab)","f73aebaa":"import re\n\npattern = r'[-\\s.,;!?]+' # regex to find space, hypen, comma and more\n\ntokens = re.split(pattern, sentence)\ntokens","129d32ee":"!pip install nltk","f8b62a87":"from nltk.tokenize import RegexpTokenizer # Regex based tokenizer\n\ntokernizer = RegexpTokenizer(r'\\w+|$[0-9.]+|\\S+')\ntokernizer.tokenize(sentence)","d62a2656":"from nltk.tokenize.treebank import TreebankWordTokenizer","988f19c9":"sentense = \"I wasn't sure if simple tokernizer can split the sentence with punctuations.\"\ntokenizer = TreebankWordTokenizer()\ntokenizer.tokenize(sentense)","3924aa3c":"from nltk.tokenize.casual import casual_tokenize \n\nmessage = \"\"\"@Reader NLP is super awessssssssssome to learn. It's real time application is wideeeeee :) \"\"\"\n\ncasual_tokenize(message)","53c52fa9":"casual_tokenize(message, reduce_len=True, strip_handles=True)","3ffd8352":"from nltk.util import ngrams","175e06b2":"list(ngrams(tokens, 2))","7ebdf6d5":"import nltk\nnltk.download(\"stopwords\")","070fe577":"stop_words = nltk.corpus.stopwords.words('english')\nprint(f\"Number of stopwords {len(stop_words)}\")","2c4ffb02":"stop_words[:10]","f8b20b58":"from nltk.stem.porter import PorterStemmer\n\nstemmer = PorterStemmer()\n\nsentence = \"Dish washer's washed dishes\"\n\n' '.join([stemmer.stem(val).strip(\"'\") for val in sentence.split()])","1878ea32":"nltk.download(\"wordnet\")","369c2c8e":"from nltk.stem import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\nlemmatizer.lemmatize(\"good\")","93721b53":"lemmatizer.lemmatize(\"good\", pos='a') # Parts of speach adjectives ","5368db55":"lemmatizer.lemmatize(\"goods\", pos='n') # Noun","f26d7b72":"!pip install vaderSentiment","5b2d91a9":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer","831b85d0":"sa = SentimentIntensityAnalyzer()\nlen(sa.lexicon) # VADER has 7506 keywords which has been scored ","c7225273":"# few examples of the scores\nfor key, val in sa.lexicon.items():\n    if \" \" in key:\n        print(key, val)","a6d6c6fd":"sa.polarity_scores(text = \"Python is very readble and it's a great language for machine learning\")","726d2fd6":"sa.polarity_scores(text=\"Python is not bad to get started\")","fe32c056":"corpus = [\"Perfect fit. Love it!! :-) :-)\",\n         \"Horrible, waste of money\",\n         \"It was okay, some good and some bad things\"]\n\nfor val in corpus:\n    scores = sa.polarity_scores(val)\n    print(f\"{scores['compound']}: {val}\")","4bec1f99":"# Load in the data\n! pip install nlpia","c3928c88":"from nlpia.data.loaders import get_data\nmovies = get_data(\"hutto_movies\")","843f7774":"print(movies.shape)\nmovies.head().round(2)\n","b87d9ac7":"movies.describe().round()","ceb261ec":"# Naives bayes is a classifier hence converting the sentiment to binary output\nmovies['sentiment_ispositive'] = (movies['sentiment'] > 0).astype(int)","34dd896a":"from collections import Counter \nbags_of_words = []","f0411970":"for text in movies.text:\n    bags_of_words.append(Counter(casual_tokenize(text)))\n    ","4cd39d89":"df_movies = pd.DataFrame.from_records(bags_of_words)\ndf_movies.head()","0f14c4f9":"df_movies = df_movies.fillna(0).astype(int)\ndf_movies.head()","33591154":"df_movies.shape","6c5800ed":"from sklearn.naive_bayes import MultinomialNB","1e08b876":"nb_model = MultinomialNB()\nnb_model = nb_model.fit(df_movies, movies['sentiment_ispositive'])\nmovies['predicted_sentiment'] = nb_model.predict(df_movies)\nmovies['predicted_sentiment'] = movies['predicted_sentiment'].astype(int)","f579375b":"movies.head()","6f40d545":"from sklearn.metrics import confusion_matrix\npd.DataFrame(confusion_matrix(movies.predicted_sentiment, movies.sentiment_ispositive))","be42db56":"#Evaluate accuracy\n\n(movies.predicted_sentiment == movies.sentiment_ispositive).sum()\/len(movies)","9ce8f79f":"## Normalizing vocabulary \n\n**1. Case folding** -  Consolidating multiple spelling words which differs only by their capitalisation - This could result in loss of information modern NLP pipeline recommend turning OFF the case folding based on the application. \n\n**2. Stemming** - Elimination of small meaning pluralization and possessive endings of words. Stemming identifies the stems in various forms of words. E.g. \"Housing\" and \"Houses\" has stem \"house\". Stemming helps to generalise the search by retrieving searches related to the stem words. At the same time it can reduce the accuracy by retreiving the results which are irrelevant. ","6ac34307":"#### Simplest tokenizer using Python's split() ","248e3b34":"## Segmenting texts into token - Tokenization\n\nTokenization is the first step in NLP pipeline. Which breakes the unstructured natural language text into chunks of information that can be counted as discrete elements. The count of token occurence of a document is a vector representation of the documents which is called as **Bag-Of-Words** vector. The most common usecase of such vectors is document retrieval or search. ","a05f8653":"93.4 % of the ratings were correctly analysed as per their sentiment recorded\n\n# [Chapter 3: Math with words(TF-IDF vectors)](https:\/\/www.kaggle.com\/shilpagopal\/nlp-in-action-chapter-3-tf-idf-vectors)","69523db2":"This tokenizer is much better than the split() one as it separates the punctuations as separate tokens and additional white spaces are removed. There is a even better tokenizer that incorporate the common rules in english word tokenization. i.e. **TreebankWordTokenizer**","9b2f3fe7":"VADER doesn't considers n-grams hence the above sentence got a score more towards being neutral rather than being positive. Let's see few more examples ","63ba6157":"As I read the [book](https:\/\/www.amazon.in\/Natural-Language-Processing-Action-Understanding\/dp\/1617294632) **Natural Langauge Processing In Action** , will be summarizing the every chapters in the subsequent notebooks. This is the second notebook for Chapter 2: Word Tokenization\n\n![image.png](attachment:e1c5efb2-2183-4373-b353-6052062632bc.png)\n\nPC:InsideAIML","864d2800":"So far, so good the only drawback is that VADER can only look for 7506 keywords to score the sentiment of a document. If your document doesn't have these words then sentiment analysis may not work in that case. We may have to build our own sentiment score dictionary. What if we are not aware of the language to build the dictionary, that why the machine learning approach is for. \n\n#### Machine learning approach\n\nIt relies on labeled set of statements or documents to train a machine learning model to create those rules. These models takes texts as input and produces numerical scores for the sentiments like positivity, negativity, spamminess or trolliness. Let's use a Naive Bayes Model to analyse the movie reviews ","9016e210":"VADER provides sentiment polarity in three different scores - **Positive, Negative** and **Neutral** then combines those into a compond positve statement","cbf7cefc":"\"wasn't\" is tokenized to \"was\" and \"n't\". Contraction words like don't, wasn't weren\u2019t can be split to their original words. Some of the syntax tree needs the words to be split like this.\n\n#### How to tokenize informal text from twitter or facebook? NLTK has a api for that too.","1ac3f07b":"#### Building Simple tokenizer using NLTK","141c07ec":"**2. Lemmatization** - If there is access to the connections between the meanings of various words, they can be combined even though their spellings are different. E,g. \"Banked\", \"Banking\", \"Banker\" to a \"Bank\". Though it can help to generalize well at the cost of precision. \n\n* Lemmetization takes word meaning into account where as other two normalization techniques won't.   \n* It also Parts of Speach into account to combine words\n* Reduces dimentions and recall improvement  \n* Output of lemmmatization is a proper meaningful english words where as stemming results in stems which are not always meaningful \n\n>  Finding lemmas using NLTKs' wordnet","520e076e":"Yes, You're right this is our first numerical representation of words to building NLP models. There is no loss of information in this representation. This type of representation is typically used in  Neural nets, sequence-to-sequence language models and generative language models.  \n\nImagine you have thousands of words and encoding them to zeros and ones is not memory efficient. Though this approach is lossless but our aim is to capture the meaning of the documents but not all of it. ","2887cfa9":"> So far we have built the vocabulary with the help of tokenization techniques. The size of the vocabulary plays a very important role in the performance of NLP pipelines. Normalizing(like correcting spelling, case check) the tokens can combine the similar meaning words and reduce the vocabulary size. ","6c509e09":"### Question : When should you use a stemmer or a lemmatizer? \nLeave the answer in the comments","9f38bdff":"Tokenizers eventually become complex when we consider special cases like split by \".\" when the word is not followed by a number so that the decimal values are retained. Handling emoticons in Twitter messages and more. \n\n2. Several Python libraries implement Tokenizers. \n\n* [spaCy](https:\/\/spacy.io\/) -  Accurate, fast, flexible\n* [NLTK(Natural Language Toolkit)](https:\/\/www.nltk.org\/) - Popular","849b868a":"#### Treating punctuation as separate token\n\nPunchuation can be separated as tokens using pattern matching separators. \n\n1. Using Regular expression","508359d1":"ngrams() API, builds word pair by considering preceding and succeeding words. But not all combinations of words are meaningful. We will see in further chapters to handle this. These tokerzised words includes a lot of **stop words** which carries less meaning when there are not in phrase. \n\n* a, an\n* the, this\n* and or\n* of, on\n\nRemoving stop words will reduce computational complexity and memory but sometimes might result in loss of informations. \n\n    Sentence 1: Mark reported to the CEO \n\n    Sentence 2: Suzanne reported as the CEO to the board\n\n    If we perform 2-gram tokenize on these sentences \n\n    Token1 : [\"Mark\", \"reported\"] [\"reported\" \"CEO\"]\n\n    Token2 : [\"Suzanne\", \"reported\"] [\"reported\", \"CEO\"] [\"CEO\", \"board\"]\n\n    Here [\"reported\" \"CEO\"] gram, sounds like someone is reporting to the CEO where as in the second sentence Suzanne was reporting as CEO to the board. \n    \n    \nSo designing a stop word filter depends on the application, If we are extracting country names from the corpus of text removing stop words makes more sense than grammar check applications. Antways NLTK has list of stop words which are exposed as API\n","dc4b2374":"split() does a decent job of tokenizing the sentence except for extracting the punctuation as a separate token, which could be very useful for sentence segmenter or sentence boundary detector to find the end of a sentence. We will learn about how to handle that later for now let's move ahead to create the vector representation(one-hot-vector) for each words. ","57956a2f":"Without the normalization bag of words can grow quite large. Anyways let's use the model to predict the sentiment","f80ff80d":"# [ Chapter 1: Packets of Thought(NLP Overview)](https:\/\/www.kaggle.com\/shilpagopal\/nlp-in-action-chapter-1-overview)\n# Chapter 2: Build your Vocabulary(Word Tokenization)\n\nWhat makes the natural language speakers and writers powerful is their vocabulary. To build vocabulary for NLP systems we need to split the documents, any string to form discrete tokens of meaning. This chapter deals with extracting words from strings to build pairs, triplets, quadruplets and even quintuples. These are called **n-grams**.\n\npair words - 2-grams\n\ntriplets words - 3-grams and so on\n\nOnce tokens are extracted from documents\/string to build the vocabulary, there could be words with similar meaning hence we could combine those words with similar meaning in a process called **Stemming**\n\nAs we learnt from the previous chapter, what makes NLP hard is the extraction of meaning words from the text which is also known as feature extraction. \n\nE.g. \n\n* How to differentiate the words ending with verbs, \"ending\" possible stem is \"end\" but the word \"sing\" has its meaning removing \"ing\" would result in single letter \"s\"\n* The words \"running\" has verbs as well as an extra \"n\" how to we remove these additional letters\n* How to differentiate between plurals and words ending with \"s\", \"words\" and \"bus\"","0cee3d32":"#### n-gram tokenizer \n\nSome words pair in the sentence is more meaningful than the individual words, like \"ice cream\"","4a565d64":"## Sentiment\n\nSo far, we have learnt to break the document\/string into single word tokens, n-grams, stems and lemmas. We know that all these tokens carry some information, but it is also important to know the emotions behind these token of phrases.  This **Sentiment Analysis** -- measuring the sentiment of phrases -- is a common application of NLP. \n\nThere are two appraches to sentiment analysis. \n1. Human designed rule based algorithm \n2. Machine learning algorithm trained to data to learn the sentiments\n\n#### Rule based algorithm \n\nAs the name indicates in this approach sentiment analysis uses the human-designed rules sometimes called heuristics to measure the sentiment.  A common approach is to create a dictionary of sentiment scores. \n\nSay you want to analyse the reviews of the product. \n* Tokenize the reviews\n* Record the sentiment scores using the dictionary \n* If the score is close to 1 then it's a positive review\n* If it is close to 0 then the review is negative. \n\n[Huttonn and Gilbert](http:\/\/comp.social.gatech.edu\/papers\/icwsm14.vader.hutto.pdf) came up with the first successful rule based sentiment analysis algorithm called VADER -  **V**alence **A**ware **D**ictionary for s**E**ntiment **R**easoning","988bbc4a":"Movies are rated on the scale of -4 to +4. Lets tokenize the to create a bag of words using NLTK. "}}