{"cell_type":{"b1faf947":"code","5c00dd65":"code","f57b96e0":"code","a3f2837d":"code","f01d5fdd":"code","19cdc30d":"code","15c84ae8":"code","5adb1076":"code","53cfbbde":"code","102e9502":"code","7a73c407":"code","2745e4a3":"code","f7f71245":"code","56230f97":"code","5465a024":"code","572c7f02":"code","061783db":"code","bbf6d773":"code","ac252326":"code","00622d16":"code","ca335b89":"code","e3bfd32c":"code","36d41363":"code","f3fd04b0":"code","bef675eb":"code","3a56a08e":"code","dcba173d":"code","8b06eb02":"code","7932c458":"code","a4e88d0e":"code","691b5ff9":"code","d2844c04":"code","1990789f":"code","ae013d3f":"code","3ada460a":"code","6a8c5faa":"code","783bc773":"code","d23e8851":"code","e579de97":"code","f100803c":"code","ed4e875d":"code","baf4c2b5":"code","d2bc60da":"code","28d6be64":"code","236d8f3c":"code","a8fafe04":"code","30126d8f":"code","20f8d83f":"code","0ecdcf17":"code","15937521":"code","875f84e8":"code","fa5ca3b1":"markdown","812ff416":"markdown","ce3b801f":"markdown","dbe05739":"markdown","f993fb9c":"markdown","5d8517fd":"markdown","d5f70afc":"markdown","195f6e79":"markdown","050a0e4b":"markdown","a0763e00":"markdown","cf3547f7":"markdown","7316e1fc":"markdown","f544b001":"markdown","c7b4cffe":"markdown","4677ea5b":"markdown","42d19581":"markdown","a331800b":"markdown","a41e4b91":"markdown","55181cd3":"markdown","90a8fbb9":"markdown","ee134486":"markdown","22aec46c":"markdown","af0f96b3":"markdown","cbf9fb34":"markdown","9c977236":"markdown"},"source":{"b1faf947":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5c00dd65":"import warnings\nwarnings.filterwarnings(\"ignore\")","f57b96e0":"!pip install scikit-learn  -U","a3f2837d":"import random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nimport sklearn\nimport gc\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_selection import RFE\nfrom sklearn.pipeline import Pipeline\n\nfrom xgboost import XGBClassifier","f01d5fdd":"seed = 47","19cdc30d":"def evaluate_model(model, x, y):\n    y_pred = model.predict(x)\n    y_pred_prob = model.predict_proba(x)[:, 1]\n    auc_roc = roc_auc_score(y, y_pred_prob)\n    return {'auc_roc_curve' : auc_roc}","15c84ae8":"random.seed(seed)\nn = 1000000\ns = 100000\nskip = sorted(random.sample(range(n),n-s))\n\ntrain_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-oct-2021\/train.csv', sep=',', skiprows=skip)","5adb1076":"train_df.head()","53cfbbde":"print('Dataset shape: ', train_df.shape )","102e9502":"train_df.info()","7a73c407":"# Sanity check for balanced number of classes in the target variable\n\nsns.countplot(train_df['target'])\nplt.title('Distribution of classes in target variable (target) \\n')\nplt.xlabel('Target')\nplt.ylabel('Count')","2745e4a3":"def get_train_test_split(test_size=0.2):\n    x_train = train_df.drop(['id', 'target'], axis=1).values\n    y_train = train_df['target'].values \n    x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = test_size, random_state = seed)\n\n    print('x_train', x_train.shape, 'y_train', y_train.shape)\n    print('x_test', x_test.shape, 'y_test', y_test.shape)\n    return x_train, x_test, y_train, y_test","f7f71245":"x_train, x_test, y_train, y_test = get_train_test_split()","56230f97":"print(\"Fitting a simple Logistic Regression model\")\nmodel = LogisticRegression(random_state=seed, solver='liblinear')\nmodel.fit(x_train, y_train)\nmodel.score(x_test, y_test)\nresults = evaluate_model(model, x_test, y_test)\nprint(results)","5465a024":"print(\"Fitting XGBoost Classifier\")\nmodel = XGBClassifier(random_state=0, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\nmodel.fit(x_train, y_train)\nresults = evaluate_model(model, x_test, y_test)\nprint(results)","572c7f02":"# get modeling pipelines to evaluate\ndef get_pipelines(model):\n    pipelines = list()\n    # normalize\n    p = Pipeline([('s',MinMaxScaler()), ('m',model)])\n    pipelines.append(('norm', p))\n    # standardize\n    p = Pipeline([('s',StandardScaler()), ('m',model)])\n    pipelines.append(('std', p))\n    # quantile\n    p = Pipeline([('s',QuantileTransformer(n_quantiles=100, output_distribution='normal')), ('m',model)])\n    pipelines.append(('quan', p))\n    # pca\n    p = Pipeline([('s',PCA()), ('m',model)])\n    pipelines.append(('pca', p))\n    # svd\n    p = Pipeline([('s',TruncatedSVD()), ('m',model)])\n    pipelines.append(('svd', p))\n    \n    p = Pipeline([('s',StandardScaler()), ('p', PowerTransformer()), ('m',model)])\n    pipelines.append(('std-power', p))\n    # scale and power\n    p = Pipeline([('s',MinMaxScaler()), ('p', PowerTransformer()), ('m',model)])\n    pipelines.append(('min-max-power', p))\n    \n    p = Pipeline([('p', PowerTransformer()), ('m',model)])\n    pipelines.append(('power', p))\n    \n    return pipelines","061783db":"def score_model(x, y, model):\n    # define the cross-validation procedure\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    # evaluate model\n    scores = cross_val_score(model, x, y, scoring='roc_auc', cv=cv, n_jobs=-1)\n    return scores","bbf6d773":"model = LogisticRegression(random_state=seed, solver='liblinear')\npipelines = get_pipelines(model)\nx_train = train_df.drop(['id', 'target'], axis=1).values\ny_train = train_df['target'].values \n\n# evaluate each pipeline\nresults, names = list(), list()\nfor name, pipeline in pipelines:\n\t# evaluate\n\tscores = score_model(x_train, y_train, pipeline)\n\t# summarize\n\tprint('>%s: %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n\t# store\n\tresults.append(scores)\n\tnames.append(name)\n\n# No Transform\nscores = score_model(x_train, y_train, model)\nprint('>%s: %.3f (%.3f)' % ('No-transform', np.mean(scores), np.std(scores)))\nresults.append(scores)\nnames.append('No-transform')","ac252326":"plt.figure(figsize=(15,8))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","00622d16":"# model = XGBClassifier(random_state=seed, verbosity=0)\nmodel = XGBClassifier(random_state=seed, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\npipelines = get_pipelines(model)\nx_train = train_df.drop(['id', 'target'], axis=1).values\ny_train = train_df['target'].values \n\n# evaluate each pipeline\nresults, names = list(), list()\nfor name, pipeline in pipelines:\n\t# evaluate\n\tscores = score_model(x_train, y_train, pipeline)\n\t# summarize\n\tprint('>%s: %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n\t# store\n\tresults.append(scores)\n\tnames.append(name)\n\n# No Transform\nscores = score_model(x_train, y_train, model)\nprint('>%s: %.3f (%.3f)' % ('No-transform', np.mean(scores), np.std(scores)))\nresults.append(scores)\nnames.append('No-transform')","ca335b89":"r = dict(zip(names, np.mean(results, axis=1).tolist()))\nn = max(r, key=r.get)\nprint(n, r[n])\ndict(zip(names, np.mean(results, axis=1).tolist()))","e3bfd32c":"plt.figure(figsize=(15,8))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","36d41363":"geomean = lambda x, axis : np.exp(np.mean(np.log(x), axis=axis))\nharmonic_mean = lambda x, axis : len(x) \/ np.sum(1.0\/x, axis=axis) \n\nfuncs = {'mean' : np.mean, \n         'std' : np.std, \n         'var' : np.var, \n         'geo_mean' : geomean, \n         'harmonic_mean' : harmonic_mean, \n         'median' : np.median}","f3fd04b0":"results, names = list(), list()\np = PowerTransformer()\n\nfor key in funcs.keys():\n    x = train_df.drop(['id', 'target'], axis=1)\n    x[key] = funcs[key](x, axis=1)\n    y = train_df['target']\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = seed)\n    x_train = p.fit_transform(x_train)\n    x_test = p.transform(x_test)\n    model = LogisticRegression(random_state=seed, solver='liblinear')\n    \n    model.fit(x_train, y_train)\n    model.score(x_test, y_test)\n    result = evaluate_model(model, x_test, y_test)\n    names.append(key)\n    results.append(result['auc_roc_curve'])\n\nfor name, score in zip(names, results):\n    print('>%s: %f' % (name, score))","bef675eb":"results, names = list(), list()\n\nfor key in funcs.keys():\n    x = train_df.drop(['id', 'target'], axis=1)\n    x[key] = funcs[key](x, axis=1)\n    y = train_df['target']\n    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = seed)\n    model = XGBClassifier(random_state=0, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\n    \n    model.fit(x_train, y_train)\n    result = evaluate_model(model, x_test, y_test)\n    names.append(key)\n    results.append(result['auc_roc_curve'])\n\nfor name, score in zip(names, results):\n    print('>%s: %f' % (name, score))","3a56a08e":"del x, y\ngc.collect()","dcba173d":"p = PowerTransformer()\nx_train = train_df.drop(['id', 'target'], axis=1)\nx_train['std'] = np.std(x_train, axis=1)\ny_train = train_df['target']\nx_train = p.fit_transform(x_train)","8b06eb02":"space = dict()\nspace['solver'] = ['liblinear', 'newton-cg', 'lbfgs']\nspace['penalty'] = ['none', 'l1', 'l2', 'elasticnet']\nspace['C'] = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 10, 100]\nmodel = LogisticRegression(random_state=seed, verbose=0)\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=1)\nsearch = GridSearchCV(model, space, scoring='roc_auc', n_jobs=-1, cv=cv)","7932c458":"result = search.fit(x_train, y_train)\n# summarize result\nprint('Best Score: %s' % result.best_score_)\nprint('Best Hyperparameters: %s' % result.best_params_)","a4e88d0e":"result.best_estimator_","691b5ff9":"p = PowerTransformer()\nx_train = train_df.drop(['id', 'target'], axis=1)\nx_train['var'] = np.var(x_train, axis=1)\ny_train = train_df['target']\nx_train = p.fit_transform(x_train)","d2844c04":"params = {'n_estimators' : [1000, 1290, 1295, 1300, 1305, 1310, 1315, 1325],\n          'max_depth' : [3, 4],\n          'subsample' : [0.8, 0.9, 1.0],\n          'eta' : [0.12],\n          'colsample_bytree' : [0.3, 0.4],\n          'min_child_weight': [5],\n          'gamma': [5],\n         }\n\nmodel = XGBClassifier(random_state=seed, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)\nsearch = GridSearchCV(model, param_grid=params, scoring='roc_auc', refit='roc_auc', n_jobs=-1, cv=cv)","1990789f":"result = search.fit(x_train, y_train)\n# summarize result\nprint('Best Score: %s' % result.best_score_)\nprint('Best Hyperparameters: %s' % result.best_params_)","ae013d3f":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-oct-2021\/train.csv', sep=',')","3ada460a":"p = PowerTransformer()\nx_train = train_df.drop(['id', 'target'], axis=1)\nx_train['var'] = np.var(x_train, axis=1)\ny_train = train_df['target']\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.2, random_state = seed)\nx_train = p.fit_transform(x_train)\nx_test = p.transform(x_test)","6a8c5faa":"params = result.best_params_.copy()\nprint(params)","783bc773":"results_trees = {}\ntrees = [100, 150, 200, 250, 300, 350, 400, 450, 500, 600, 700, 800, 900, 950, 975, 1000, 1025, 1050, 1100, 1150, 1290, 1295, 1300, 1305, 1310, 1315, 1325, 2000]\nfor n in trees:\n    params['n_estimators'] = n\n    model = XGBClassifier(**params, random_state=seed, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\n    model.fit(x_train, y_train)\n    result = evaluate_model(model, x_test, y_test)\n    results_trees[n] = result['auc_roc_curve']\n    print('n_estimators:', n, 'auc_roc_curve:', results_trees[n])\n\nbest_nestimator = max(results_trees, key=results_trees.get)\nprint('\\nBest n_estimators:', best_nestimator, 'AUCROC score:', results_trees[best_nestimator])","d23e8851":"results_max_depths = {}\nparams['n_estimators'] = best_nestimator\nmax_depths = [i for i in range(1,5)]\n\nfor max_depth in max_depths:\n    params['max_depth'] = max_depth\n    model = XGBClassifier(**params, random_state=seed, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\n    model.fit(x_train, y_train)\n    result = evaluate_model(model, x_test, y_test)\n    results_max_depths[max_depth] = result['auc_roc_curve']\n    print('max_depth:', max_depth, 'auc_roc_curve:', results_max_depths[max_depth])\n\nbest_max_depth = max(results_max_depths, key=results_max_depths.get)\nprint('\\nBest max_depth:', best_max_depth, 'AUCROC score:', results_max_depths[best_max_depth])","e579de97":"results_subsamples = {}\nparams['max_depth'] = best_max_depth\nsubsamples = [i for i in np.arange(0.1, 1.1, 0.1)]\n\nfor subsample in subsamples:\n    params['subsample'] = subsample\n    model = XGBClassifier(**params, random_state=seed, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\n    model.fit(x_train, y_train)\n    result = evaluate_model(model, x_test, y_test)\n    results_subsamples[subsample] = result['auc_roc_curve']\n    print('subsample:', subsample, 'auc_roc_curve:', results_subsamples[subsample])\n\nbest_subsample = max(results_subsamples, key=results_subsamples.get)\nprint('\\nBest subsample:', best_subsample, 'AUCROC score:', results_subsamples[best_subsample])","f100803c":"results_etas = {}\nparams['subsample'] = best_subsample\netas = [0.0001, 0.001, 0.003, 0.005, 0.01, 0.03, 0.05, 0.1, 0.12, 0.13, 0.3, 0.5, 1.0]\n\nfor eta in etas:\n    params['eta'] = eta\n    model = XGBClassifier(**params, random_state=seed, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\n    model.fit(x_train, y_train)\n    result = evaluate_model(model, x_test, y_test)\n    results_etas[eta] = result['auc_roc_curve']\n    print('eta:', eta, 'auc_roc_curve:', results_etas[eta])\n\nbest_eta = max(results_etas, key=results_etas.get)\nprint('\\nBest eta:', best_eta, 'AUCROC score:', results_etas[best_eta])","ed4e875d":"results_colsample_bytrees = {}\nparams['eta'] = best_eta\ncolsample_bytrees = [i for i in np.arange(0.1, 1.1, 0.1)]\n\nfor colsample_bytree in colsample_bytrees:\n    params['colsample_bytree'] = colsample_bytree\n    model = XGBClassifier(**params, random_state=seed, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\n    model.fit(x_train, y_train)\n    result = evaluate_model(model, x_test, y_test)\n    results_colsample_bytrees[colsample_bytree] = result['auc_roc_curve']\n    print('colsample_bytree:', colsample_bytree, 'auc_roc_curve:', results_colsample_bytrees[colsample_bytree])\n\nbest_colsample_bytree = max(results_colsample_bytrees, key=results_colsample_bytrees.get)\nprint('\\nBest colsample_bytree:', best_colsample_bytree, 'AUCROC score:', results_colsample_bytrees[best_colsample_bytree])\n    \n","baf4c2b5":"results_min_child_weight = {}\nparams['colsample_bytree'] = best_colsample_bytree\nmin_child_weights = [i for i in range(1,10)]\n\nfor min_child_weight in min_child_weights:\n    params['min_child_weight'] = min_child_weight\n    model = XGBClassifier(**params, random_state=seed, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\n    model.fit(x_train, y_train)\n    result = evaluate_model(model, x_test, y_test)\n    results_min_child_weight[min_child_weight] = result['auc_roc_curve']\n    print('min_child_weight:', min_child_weight, 'auc_roc_curve:', results_min_child_weight[min_child_weight])\n\nbest_min_child_weight = max(results_min_child_weight, key=results_min_child_weight.get)\nprint('\\nBest min_child_weight:', best_min_child_weight, 'AUCROC score:', results_min_child_weight[best_min_child_weight])","d2bc60da":"results_gamma = {}\nparams['min_child_weight'] = best_min_child_weight\ngammas = [0.01, 0.02, 0.03, 0.1, 0.3, 0.5, 1, 1.1, 1.5, 2, 5, 7, 9, 10]\n\nfor gamma in gammas:\n    params['gamma'] = gamma\n    model = XGBClassifier(**params, random_state=seed, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\n    model.fit(x_train, y_train)\n    result = evaluate_model(model, x_test, y_test)\n    results_gamma[gamma] = result['auc_roc_curve']\n    print('gamma:', gamma, 'auc_roc_curve:', results_gamma[gamma])\n\nbest_gamma = max(results_gamma, key=results_gamma.get)\nprint('\\nBest gamma:', best_gamma, 'AUCROC score:', results_gamma[best_gamma])","28d6be64":"params['gamma'] = best_gamma\nprint('Best Score', results_gamma[best_gamma])\nprint('Best Hyperparameters:', params)","236d8f3c":"model = XGBClassifier(**params, random_state=seed, tree_method='gpu_hist', predictor='gpu_predictor', use_label_encoder=False, verbosity=0)\nmodel.fit(x_train, y_train)\nresults = evaluate_model(model, x_test, y_test)\nprint(results)","a8fafe04":"del x_train, x_test, y_train, y_test, train_df\ngc.collect()","30126d8f":"test_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-oct-2021\/test.csv', sep=',')","20f8d83f":"x_test = test_df.drop(['id'], axis=1)\nx_test['var'] = np.var(x_test, axis=1)\nx_test = p.transform(x_test)","0ecdcf17":"target = model.predict_proba(x_test)[:, 1]\nids = test_df['id'].values\nsubmission = pd.DataFrame({'id' : ids, 'target' : target})","15937521":"submission.head()","875f84e8":"submission.to_csv('submission.csv', index=False)","fa5ca3b1":"<h3> 6 - Testing different values for min_child_weight<\/h3>","812ff416":"# Individual Search on Full data set\n\nNow with best params, we will do an individual search on the full dataset since we were using only 10% in the grid search.","ce3b801f":"# Submission","dbe05739":"<h3>2 - Testing different max_depth<\/h3>","f993fb9c":"# XGBoost - Pipelines","5d8517fd":"# Logistic regression\n\nWe create synthetic features and use power transform as well","d5f70afc":"# Experiment - 1\n\nHere we just test the performance of logistic regression and an xgboost without in the dataset as it is. No preprocessing in the data is performed.","195f6e79":"# Grid Search\n\nReferences:\n\n    [1] https:\/\/machinelearningmastery.com\/hyperparameter-optimization-with-random-search-and-grid-search\/","050a0e4b":"Logistic regression showed the best results. It seems to work well with mean or std, however std was almost identical it was a little better than the mean. XGboost present the best result with the var as synthetic feature. \n\nLogistic - std\n\nXGboost - var","a0763e00":"# XGBoost Classifier","cf3547f7":"# GridCV on Data Preparation\n\nBefore doing anything else with the baseline models, we will perform a grid search on data preprocessing techniques.\n\nReferences:\n\n    [1]  https:\/\/machinelearningmastery.com\/grid-search-data-preparation-techniques\/","7316e1fc":"# Logistic Regression - Pipelines","f544b001":"# Feature engineering - Creating some synthetic features","c7b4cffe":"# Tabular Playground Series - Oct 2021\n\nThe tabular series on kaggle are meant to help novices in data science field like me get acquainted with kaggle competitions.\n\nThe dataset created for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the biological response of molecules given various chemical properties.\n\nThe first step in almost every data science project is to perfom some exploratory data analysis, which is already done in a previous notebook [1]. Here we will try to use some assumption based on that analysis to\nverify whether we can obtaion better performance. To begin with, we will use only 100k samples of the original data. Here we try some baseline models and also do some grid search on some data transformation methods.\n\n[1] https:\/\/www.kaggle.com\/peressim\/tabular-playground-series-oct-2021-eda","4677ea5b":"# Data splitting\n\nHere we split the data into train and test sets","42d19581":"# Results\n\n<h3>Best results<\/h3>\n\nLogistic Regression when used with PowerTransformer overcome all the XGBoost.\nXGBoost - No transform (any other transformation gives the same result, except when it is used with svd)\n","a331800b":"<h3>Testing different subsamples<\/h3>","a41e4b91":"# Next Steps\n\nThe next step is try to improve the results by training both Logistic Regression and XGBoost and then averaging their results. We are going to try it in a new notebook [1] using the best parameters found here so far.\n\n[1] https:\/\/www.kaggle.com\/peressim\/tabular-playground-series-oct-2021-final-models","55181cd3":"# XGBoost Classifier","90a8fbb9":"# Logistic Regression","ee134486":"<h3>1 - Testing different number of estimators<\/h3>","22aec46c":"<h3>4 - Testing different learning rates<\/h3>","af0f96b3":"# Baseline\n\nBaseline models to choose the methods that will allow the model to achieve a good performance.\n\n1 - Logistic Regression\n\n2 - XGBoost\n\nAll the tests will be based on a fraction of 10% of all available data","cbf9fb34":"<h3> 7 - Testing different values for gamma<\/h3>","9c977236":"<h3>5 - Testing different number of features<\/h3>"}}