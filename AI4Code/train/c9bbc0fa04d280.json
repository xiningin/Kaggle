{"cell_type":{"a71614cf":"code","15e55f17":"code","9504a58d":"code","9ab98c13":"code","5daa9692":"code","026368dd":"code","f2b9c6a3":"code","0a0d57d4":"code","a074a42f":"code","283f5445":"code","cab187f5":"code","d65e751d":"code","426606c1":"code","4a00b818":"code","9e645745":"code","a3de3408":"code","41c856f7":"code","ca5e1f28":"code","5989cfdd":"code","4081725c":"code","730a3f37":"code","3f236da1":"code","9f87fb96":"code","a882b908":"code","5e4ccacc":"code","20eeef2d":"code","b936ecb6":"code","d73014b1":"code","fd4f22ca":"code","66940c4c":"code","b1cea4b7":"code","394c6245":"code","cd353cc8":"code","60aaa2be":"code","997bc132":"code","d062b0d4":"code","d3a35ef1":"code","4e9d63bf":"code","04e1c7f0":"code","5096dc84":"code","8e4580bf":"code","4adee19d":"markdown","6c1c007c":"markdown","0c370cc5":"markdown","b605c86a":"markdown","ca01e3de":"markdown","a7b2199e":"markdown","0db3733f":"markdown","7519502b":"markdown","189775e0":"markdown","18c58eff":"markdown","726ea0bd":"markdown","1efddf65":"markdown","7647e9f4":"markdown","a20d007c":"markdown"},"source":{"a71614cf":"import spacy\nimport nltk\nfrom nltk.corpus import stopwords\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy import lemmatizer\nfrom nltk.tokenize import RegexpTokenizer\nimport pandas as pd\nimport gensim\nfrom gensim.models import Word2Vec\nfrom sklearn.model_selection import train_test_split\nimport torch \nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np","15e55f17":"nlp=spacy.load('en_core_web_sm', disable=['tagger', 'parser', 'ner'])","9504a58d":"tokenizer=RegexpTokenizer(\"(\\w+\\'?\\w+?)\")","9ab98c13":"# nltk.download('punkt')\n# nltk.download('stopwords')","5daa9692":"sw1=stopwords.words(\"english\")\nsw2=STOP_WORDS\nstop_words=set(sw1).union(sw2)","026368dd":"def tokenize(rev):\n    return(tokenizer.tokenize(str(rev).lower()))","f2b9c6a3":"def remove_stop_words(rev_tokens):\n    return([tok for tok in rev_tokens if tok not in stop_words])","0a0d57d4":"def lemmatize(rev_tokens):\n    result=[]\n    for tok in rev_tokens:\n        temp=nlp(tok)\n        for tok in temp:\n            result.append(tok.lemma_)\n    return result","a074a42f":"def preprocess_pipeline(review):\n    review=tokenize(review)\n    review=remove_stop_words(review)\n    review=lemmatize(review)\n    return review","283f5445":"df=pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\ndf.head(2)","cab187f5":"reviews=list(df['review'])\nsentiments=list(df['sentiment'])","d65e751d":"reviews=list(map(lambda x: preprocess_pipeline(x), reviews))","426606c1":"dimension=100 # will also be used in the RNN unit part \nmodel=Word2Vec(reviews, size=dimension, window=3, min_count=3, workers=4)","4a00b818":"model.sg#its CBOW not skipgram","9e645745":"# tip: as soon as we train our model, we have to delete the model unless there is further training or updation required in the model because it consumes lots of memory\n# but, to use it even after deleting the model, we can use key'd vector model that holds all the info about the embedding model\nword_vec=model.wv\ndel(model)","a3de3408":"#save the vocabulary of the model\nlen(word_vec.vocab)","41c856f7":"word_vec.similar_by_word(word=\"bad\", topn=10)","ca5e1f28":"word_vec.similarity(\"good\", \"be\")","5989cfdd":"#now apply contextual relation with the word\n#ex: king - man + woman = queen\n#example: \nword_vec.most_similar(negative=[\"bad\"], positive=[\"decent\"], topn=5)","4081725c":"# from gensim.models import KeyedVectors #to store the loaded pretrained model","730a3f37":"# model=KeyedVectors.load_word2vec_format('pretrained embedding path in .bin format', binary=True)","3f236da1":"#to ensure results are reproducable set these\nSEED=2031\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic=True","9f87fb96":"def convWordInd(embedding_model, review):\n    indice_rev=[]\n    for word in review:\n        try:\n            indice_rev.append(embedding_model.vocab[word].index)\n        except:\n            pass\n    return torch.tensor(indice_rev)","a882b908":"review_indexes=list(map(lambda x: convWordInd(word_vec, x), reviews))","5e4ccacc":"pad_value=len(word_vec.index2word) #used later during RNN param initialization\npad_value #this is the length of the longest review in the batch","20eeef2d":"embed_weights=torch.Tensor(word_vec.vectors)","b936ecb6":"class RNN(nn.Module):\n    def __init__(self, inp_dim, embed_dim, hidden_dim, out_dim, n_layers, bidirectional, dropout, embed_weights):\n        super().__init__()\n        self.embedding_layer=nn.Embedding.from_pretrained(embed_weights)\n        self.rnn=nn.LSTM(embed_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout)\n        self.dense=nn.Linear(hidden_dim*2, out_dim)#ip layer is hid_layer*2 because in case of bidirectional RNN, there are 2 hidden layer o\/p\n        self.dropout=nn.Dropout(dropout)#never use dropout in the ip or op layers but in the intermediate layers\n        \n    def forward(self, x, text_lens):\n        embedded=self.embedding_layer(x)\n        packed_embed=nn.utils.rnn.pack_padded_sequence(embedded, text_lens)\n        packed_out, (hidden, cell)=self.rnn(packed_embed) #output size=[text_len, batch_size, hiddendim*num of dim]\n        #output, output_lens=nn.utils.rnn.pad_packed_sequence(packed_out) commented as output is not used here \n        #bdirlstm consists [f0, b0, f1, b1, ..... fn, bn]\n        #so, concatinating the last two hidden state (forward and backward) from the last layer, it is passed to linear layer \n        hidden =self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))\n        return self.dense(hidden.squeeze(0))        ","d73014b1":"#updating the hyperparameters\ninp_dim=pad_value\nembed_dim=dimension\nhidden_dim=256\noutput_dim=1\nn_layers=2\nbidirectional=True\ndropout=0.5","fd4f22ca":"model=RNN(inp_dim, embed_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout, embed_weights)\nmodel","66940c4c":"optimizer=optim.Adam(model.parameters())\nloss_function=nn.BCEWithLogitsLoss()","b1cea4b7":"device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","394c6245":"#binary encoding the y vals\nsentiments=[0 if label == 'negative' else 1 for label in sentiments]\nsentiments[:5]","cd353cc8":"X_train, X_test, Y_train, Y_test=train_test_split(review_indexes, sentiments, test_size=0.25)\nX_train, X_val, Y_train, Y_val=train_test_split(X_train, Y_train, test_size=0.2)","60aaa2be":"#now batches out of entire dataset is prepared\nbatch_size=128\ndef iterate_func(x, y):\n    size=len(x)\n    permute=np.random.permutation(size) #for creating the random blocks from the list of reviews\n    iterator=[]\n    for i in range(0, size, batch_size): #from 0 to size stepping from batch size\n        indices=permute[i: i+batch_size]\n        batch={}\n        batch[\"text\"]=[x[i] for i in indices]\n        batch[\"label\"]=[y[i] for i in indices]\n        #sort the texts based on their lengths\n        batch[\"text\"], batch[\"label\"]=zip(*sorted(zip(batch[\"text\"], batch[\"label\"]), key=lambda x: len(x[0]), reverse=True))\n        batch[\"length\"]=[len(rev) for rev in batch[\"text\"]]\n        batch[\"length\"]=torch.IntTensor(batch[\"length\"])\n        #Now, reviews are padded in each batch and are passed into the model. \n        #For padding, pytorch offers the method within its nn module\n        batch[\"text\"]=torch.nn.utils.rnn.pad_sequence(batch[\"text\"], batch_first=True).t()#transpose is performed so that it is accepted into rnn\n        batch[\"label\"]=torch.Tensor(batch[\"label\"])\n        \n        #now pushing all to the gpu\n        batch[\"text\"]=batch[\"text\"].to(device)\n        batch[\"label\"]=batch[\"label\"].to(device)\n        batch[\"length\"]=batch[\"length\"].to(device)\n        \n        iterator.append(batch)\n        \n    return iterator","997bc132":"train_iter=iterate_func(X_train, Y_train)\nval_iter=iterate_func(X_val, Y_val)\ntest_iter=iterate_func(X_test, Y_test)","d062b0d4":"model=model.to(device)\ncriterion=loss_function.to(device)","d3a35ef1":"def binary_acc(preds, y):\n    round_preds=torch.round(torch.sigmoid(preds))\n    pos=(round_preds==y).float()\n    accuracy=pos.sum()\/len(pos)\n    return accuracy","4e9d63bf":"def train(model, iterator, optimizer, criterion):\n    epoch_loss=0\n    epoch_accuracy=0\n    model.train()\n    \n    for batch in iterator:\n        optimizer.zero_grad()\n        predictions=model(batch[\"text\"], batch[\"length\"]).squeeze(1)\n        loss=criterion(predictions, batch[\"label\"])\n        accuracy=binary_acc(predictions, batch[\"label\"])\n        loss.backward()\n        optimizer.step() #updates the weight for the model\n        epoch_loss+=loss.item()\n        epoch_accuracy+=accuracy.item()\n    \n    #return the average epoch loss and iterator\n    return(epoch_loss\/len(iterator), epoch_accuracy\/len(iterator))","04e1c7f0":"def evaluator(model, iterator, criterion):\n    epoch_loss=0\n    epoch_accuracy=0\n    model.eval()\n    #to prevent any gradient calculation with nograd is used\n    with torch.no_grad():\n        for batch in iterator:\n            predictions=model(batch[\"text\"], batch[\"length\"]).squeeze(1)\n            loss=criterion(predictions, batch[\"label\"])\n            accuracy=binary_acc(predictions, batch[\"label\"])\n            epoch_loss+=loss.item()\n            epoch_accuracy+=accuracy.item()\n\n    #return the average epoch loss and iterator\n    return(epoch_loss\/len(iterator), epoch_accuracy\/len(iterator))","5096dc84":"epochs=7\n\nfor epoch in range(epochs):\n    train_loss, train_accuracy=train(model, train_iter, optimizer, criterion)\n    valid_loss, valid_accuracy=evaluator(model, val_iter, criterion)\n    \n    print(\"Epoch number: \", epoch)\n    print(\"Train Loss = \", train_loss, \" Train Accuracy = \", train_accuracy)\n    print(\"Validation Loss = \", valid_loss, \" Validation Accuracy = \", valid_accuracy)","8e4580bf":"test_loss, test_accuracy=evaluator(model, test_iter, criterion)\nprint(\"Epoch number: \", epoch)\nprint(\"Test Loss = \", test_loss, \" Test Accuracy = \", test_accuracy)","4adee19d":"Define the parameters for the  RNN class and create the objects of the  RNN Class","6c1c007c":"#### defining the function to Test the model\nset the loss and accuracies as false from the previous use and set the model to evaluation mode","0c370cc5":"##### word embddings using word2vec from gensim","b605c86a":"### Build the model for sentiment analysis using RNN","ca01e3de":"Take in the text in the review and convert into corresponding index (dictionary) for this dictionary, key'd vector from gensim is used. Using that dictionary we'd convert all the words in our text corpus as indices and then we pass those indices through the RNN","a7b2199e":"#### defining a function to know the accuracy subsequently","0db3733f":"RNN Model-for training and evaluation.\nAfter that the  dense layer is declared\n\nforward function: for data when passed onto the model, for each batch x, with size (max(len(sentances)*len(batch)))\n\npytorch internally converts the indexed representation which we passed to one hot encoding.\n\n##### NOTE\nIn the forward pass function, batch of reviews and the len of each review (text length) is passed. But, in case of RNN, each and every review must have same length. However, it is not required that the model to read that padded values. Hence we use the pytorch inbuilt function called \"packpaddedsequence\". It packs the padded values automatically and internally handles for us. \n\nSo the packed sequence is passed through the RNN and after passing through the RNN, the packed sequence is unlocked. ","7519502b":"#### BUILD THE RNN MODEL\n\nlayers:\n1. Embedding layer - input - indexed reviews and convert to embedded format \n2. RNN unit - input - embedded representation\n3. Fully connected unit (dense)","189775e0":"Embedding layer :Word embeddings are already created using gensim and these pre-trained embeddings are used within our nn. For that first extract the weights that the gensim model has learned while training","18c58eff":"#### defining the function to train the model \nset the loss and accuracies as false from the previous use and set the model to training mode","726ea0bd":"### IMDB dataset having 50K movie reviews for natural language processing or Text analytics.\n### This is a dataset for binary sentiment classification containing substantially more data than previous benchmark datasets\n\nSOURCE: Kaggle\nlink: https:\/\/www.kaggle.com\/lakshmi25npathi\/imdb-dataset-of-50k-movie-reviews","1efddf65":"#### prepare the data for :\n\n    a. Training\n\n    b. Validation\n    \n    c. Testing","7647e9f4":"#### Running the model and evaluation ","a20d007c":"Passing the data to nn as batches. When it is done so, all the sentance should have the same length For that, all the statements are padded with the padding value (len of longest statement in the batch). So, the padding value is appended to the end of the shorter statements (that is the index value that's why it is done so)"}}