{"cell_type":{"29862f28":"code","88f95b67":"code","113f5be5":"code","ed06a984":"code","8a14d416":"code","48594c99":"code","153bea0e":"code","e63612e7":"code","cb93e43f":"code","720872e7":"code","eb4604c2":"code","7e35071c":"code","dfe3047a":"code","b6f57f07":"code","8e18bbef":"code","891125ae":"code","85bc496c":"code","4c0ababb":"markdown","1ec28335":"markdown","5954573d":"markdown","6028a3bd":"markdown","805d732c":"markdown","24bfff55":"markdown","6a786193":"markdown","d47dfde0":"markdown","7b562273":"markdown","4931afdd":"markdown","6c5e4487":"markdown","97227bbb":"markdown","7b5bec95":"markdown","51900d02":"markdown","0538fbc8":"markdown","c8017d7f":"markdown","2bd01eab":"markdown","72111179":"markdown"},"source":{"29862f28":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport spacy\nimport string\nimport gensim\nimport operator\nimport re","88f95b67":"df_movies = pd.read_csv('\/kaggle\/input\/movies-similarity\/movies.csv')\ndf_movies.head()","113f5be5":"from spacy.lang.en.stop_words import STOP_WORDS\n\nspacy_nlp = spacy.load('en_core_web_sm')\n\n#create list of punctuations and stopwords\npunctuations = string.punctuation\nstop_words = spacy.lang.en.stop_words.STOP_WORDS\n\n#function for data cleaning and processing\n#This can be further enhanced by adding \/ removing reg-exps as desired.\n\ndef spacy_tokenizer(sentence):\n \n    #remove distracting single quotes\n    sentence = re.sub('\\'','',sentence)\n\n    #remove digits adnd words containing digits\n    sentence = re.sub('\\w*\\d\\w*','',sentence)\n\n    #replace extra spaces with single space\n    sentence = re.sub(' +',' ',sentence)\n\n    #remove unwanted lines starting from special charcters\n    sentence = re.sub(r'\\n: \\'\\'.*','',sentence)\n    sentence = re.sub(r'\\n!.*','',sentence)\n    sentence = re.sub(r'^:\\'\\'.*','',sentence)\n    \n    #remove non-breaking new line characters\n    sentence = re.sub(r'\\n',' ',sentence)\n    \n    #remove punctunations\n    sentence = re.sub(r'[^\\w\\s]',' ',sentence)\n    \n    #creating token object\n    tokens = spacy_nlp(sentence)\n    \n    #lower, strip and lemmatize\n    tokens = [word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in tokens]\n    \n    #remove stopwords, and exclude words less than 2 characters\n    tokens = [word for word in tokens if word not in stop_words and word not in punctuations and len(word) > 2]\n    \n    #return tokens\n    return tokens\n","ed06a984":"print ('Cleaning and Tokenizing...')\n%time df_movies['wiki_plot_tokenized'] = df_movies['wiki_plot'].map(lambda x: spacy_tokenizer(x))\n\ndf_movies.head()","8a14d416":"movie_plot = df_movies['wiki_plot_tokenized']\nmovie_plot[0:5]","48594c99":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nseries = pd.Series(np.concatenate(movie_plot)).value_counts()[:100]\nwordcloud = WordCloud(background_color='white').generate_from_frequencies(series)\n\nplt.figure(figsize=(15,15), facecolor = None)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","153bea0e":"from gensim import corpora\n\n#creating term dictionary\n%time dictionary = corpora.Dictionary(movie_plot)\n\n#filter out terms which occurs in less than 4 documents and more than 20% of the documents.\n#NOTE: Since we have smaller dataset, we will keep this commented for now.\n\n#dictionary.filter_extremes(no_below=4, no_above=0.2)\n\n#list of few which which can be further removed\nstoplist = set('hello and if this can would should could tell ask stop come go')\nstop_ids = [dictionary.token2id[stopword] for stopword in stoplist if stopword in dictionary.token2id]\ndictionary.filter_tokens(stop_ids)\n","e63612e7":"#print top 50 items from the dictionary with their unique token-id\ndict_tokens = [[[dictionary[key], dictionary.token2id[dictionary[key]]] for key, value in dictionary.items() if key <= 50]]\nprint (dict_tokens)","cb93e43f":"corpus = [dictionary.doc2bow(desc) for desc in movie_plot]\n\nword_frequencies = [[(dictionary[id], frequency) for id, frequency in line] for line in corpus[0:3]]\n\nprint(word_frequencies)","720872e7":"%time movie_tfidf_model = gensim.models.TfidfModel(corpus, id2word=dictionary)\n%time movie_lsi_model = gensim.models.LsiModel(movie_tfidf_model[corpus], id2word=dictionary, num_topics=300)","eb4604c2":"%time gensim.corpora.MmCorpus.serialize('movie_tfidf_model_mm', movie_tfidf_model[corpus])\n%time gensim.corpora.MmCorpus.serialize('movie_lsi_model_mm',movie_lsi_model[movie_tfidf_model[corpus]])","7e35071c":"#Load the indexed corpus\nmovie_tfidf_corpus = gensim.corpora.MmCorpus('movie_tfidf_model_mm')\nmovie_lsi_corpus = gensim.corpora.MmCorpus('movie_lsi_model_mm')\n\nprint(movie_tfidf_corpus)\nprint(movie_lsi_corpus)\n","dfe3047a":"from gensim.similarities import MatrixSimilarity\n\n%time movie_index = MatrixSimilarity(movie_lsi_corpus, num_features = movie_lsi_corpus.num_terms)","b6f57f07":"from operator import itemgetter\n\ndef search_similar_movies(search_term):\n\n    query_bow = dictionary.doc2bow(spacy_tokenizer(search_term))\n    query_tfidf = movie_tfidf_model[query_bow]\n    query_lsi = movie_lsi_model[query_tfidf]\n\n    movie_index.num_best = 5\n\n    movies_list = movie_index[query_lsi]\n\n    movies_list.sort(key=itemgetter(1), reverse=True)\n    movie_names = []\n\n    for j, movie in enumerate(movies_list):\n\n        movie_names.append (\n            {\n                'Relevance': round((movie[1] * 100),2),\n                'Movie Title': df_movies['title'][movie[0]],\n                'Movie Plot': df_movies['wiki_plot'][movie[0]]\n            }\n\n        )\n        if j == (movie_index.num_best-1):\n            break\n\n    return pd.DataFrame(movie_names, columns=['Relevance','Movie Title','Movie Plot'])\n","8e18bbef":"# search for movie tiles that are related to below search parameters\nsearch_similar_movies('crime and drugs ')","891125ae":"# search for movie tiles that are related to below search parameters\nsearch_similar_movies('violence protest march')","85bc496c":"# search for movie tiles that are related to below search parameters\nsearch_similar_movies('love affair hate')","4c0ababb":"### Data Cleaning and Pre-processing\nData pre-processing is one of the most significant step in text analytics. The purpose is to remove any unwanted words or characters which are written for human readability, but won't contribute to topic modelling in anyway.\n\nThe following function applies regular expression for matching patterns of unwanted text and removing\/replacing them.\n","1ec28335":"Store the tokenized column into a sepearte variable for ease of operations in subsequent sections and have a quick peek into the values","5954573d":"# Introduction\n\nThe World Wide Web has become colossal and its growth is also dynamic. Most of the people rely on the search engines to retrieve and share information from various resources. All the results returned by search engines are not always relevant as it is retrieved from heterogeneous data sources. Moreover a naive user finds it difficult to confirm that the retrieved results are significant to the user query.Therefore semantic web plays a major role in interpreting the relevancy of search results. \n\nIn this work, we will retrieve relevant movie titles using semantic search based on the concept of Natural Language processing (NLP)","6028a3bd":"The above results shows vocabulary with their frequency.","805d732c":"Below is the helper function to search the index, sort and return the results","24bfff55":"## Keyword Search Vs Semantic Search\n\nAt first, search engines were lexical: the search engine looked for literal matches of the query words, without understanding of the query\u2019s meaning and only returning links that contained the exact query.By using regular keyword search, a document either contains the given word or not, and there is no middle ground\n\nOn the other hand, \"Semantic Search\" can simplify query building, becuase it is supported by automated natural language processing programs i.e. using Latent Semantic Indexing - a concept that search engines use to discover how a keyword and content work together to mean the same thing.\n\nLSI adds an important step to the document indexing process. LSI examines a collection of documents to see which documents contain some of those same words. LSI considers documents that have many words in common to be semantically close, and ones with less words in common to be less close.\n\nIn brief, LSI does not require an exact match to return useful results. Where a plain keyword search will fail if there is no exact match, LSI will often return relevant documents that don't contain the keyword at all.\n\n","6a786193":"Now let us apply the data-cleaning and pre-processing function to our movies \"wiki_plot\" column and store the cleaned, tokenized data into new column","d47dfde0":"## Time for Semantic Search\n\nNow comes the fun part. With the index of movies initialized and loaded, we can use it to find similar movies based\n\n\nWe will input a search query and model will return relevant movie titles with \"Relevance %\" which is the similarity score. The higher the similarity score, the more similar the query to the documetn at the given index","7b562273":"### Build Tf-Idf and LSI Model\n\nTf-Idf means, Term frequency-Inverse Document Frequency. it is a commonly used NLP model that helps you determine the most important words in each document in the corpus. Once the Tf-Idf is build, pass it to LSI model and specify the num of features to build","4931afdd":"### Building Word Dictionary\n\nIn the next step we will build the vocabulary of the corpus in which all the unique words are given IDs and their frequency counds are also stored. You may note that we are using gensim library for building the dictionary.   In gensim, the words are referred as \"tokens\" adn the index of each word in the dictionary is called ID","6c5e4487":"Serialize and Store the corpus locally for easy retrival whenver required.","97227bbb":"### Closing Notes\n\nIn general usage, computing semantic relationships between textual data enables to recommend articles or products related to given query, to follow trends, to explore a specific subject in more details. \n\nIn this article we saw the basic version of how semantic search can be implemented. There are many ways to further enhance it using deep learning models.\n\n###################################################################################################################################\n\n### This was just an attempt to showcase my learning journey into field of NLP and Machine Learning. \n### Your valuable feedback, comments and suggestions are welcomed ! \n### Hope you all like it and vote for it. \n\n","7b5bec95":"### Load the data\nWe will now load the movies data csv into dataframe and quickly peek into the columns and data provided","51900d02":"### Feature Extraction (Bag of Words)\n\nA bag of words model, or BoW for short is a way of extracting features from text for use in modelling, such as with machine learning algorithms. It is a representation of tet that describes teh occurence of words within a document. It involves two things\n\n1. A vocabulary of known words\n2. A measure of the presence of known words\n\nThe doc2bow method of dictionary, iterates through all the words in the text, if the word already exists in the corpus, it increments the frequency count, other wise it inserts the word into the corpus and sets it freqeuncy count to 1","0538fbc8":"Here the top most movie title \"Gandhi\" is surely related to non-violence protests","c8017d7f":"![mining-the-web.jpg](attachment:mining-the-web.jpg)","2bd01eab":"You can see that there are 2 additional steps performed after creating the dictionary.\n1. All the tokens in the dictionary which either have occurrred in less than 4 articles or have occurred in more than 20% of the articles are removed from the dictionary, as these words will not be contributing to the various themes or topics. \n2. Removing content neutral words from the dictionary and additional stopwords.","72111179":"The model returns movie titles with \"Relevance %\". Definitely, the top list movies are related to crimes and drugs."}}