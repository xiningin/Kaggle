{"cell_type":{"03142779":"code","4006dcce":"code","6015546f":"code","b56a1bb9":"code","6b8aafa1":"code","df9d246f":"code","959d2d96":"code","0e5c7c8a":"code","9854e330":"code","88a02473":"code","7ef00439":"code","3a27ca9b":"code","ef5d295c":"code","36b9c4f0":"code","68789e87":"code","01b90af5":"code","0bb60124":"code","eb386ea8":"code","2d1ab468":"code","802c2f73":"code","44d37eda":"code","456bffb6":"code","d39b8990":"code","6aa62ebf":"code","64e4f071":"code","063d73ed":"markdown","7200d439":"markdown","e671cd5a":"markdown","2682fab6":"markdown","ba95d338":"markdown","0671e265":"markdown","34e73447":"markdown","dc5b61a5":"markdown","c1e5fa03":"markdown","c3336a8b":"markdown","ba6e4893":"markdown","5c104a53":"markdown","235130d8":"markdown","6aa109a6":"markdown","fd17fe06":"markdown","2ffbb0fc":"markdown","e7192793":"markdown","166c6dad":"markdown","1cb52dec":"markdown","5d0b1dfd":"markdown","337e2b4d":"markdown","7599a258":"markdown","7671aadd":"markdown","75fe2659":"markdown"},"source":{"03142779":"import pandas as pd\nfrom numpy import nan\n\ntrain_data=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_data=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\nprint(\"Training shape \", train_data.shape)\nprint(\"Test shape \", test_data.shape)\n\n\n\n\n#print(train_data[\"Id\"].dtype)\n#print(train_data[\"MSZoning\"].dtype)\n\ntrain_data=train_data.drop(['Id'], axis=1)\ntest_data=test_data.drop(['Id'], axis=1)\n\nprint(train_data.shape)\nprint(test_data.shape)\n\ntrain_data.describe()\ntrain_data.info()","4006dcce":"total = train_data.isnull().sum().sort_values(ascending=False)\npercent = ((train_data.isnull().sum()\/train_data.isnull().count())*100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nprint(missing_data.head(50))\n\n\n\nfor x in missing_data.index:\n    a=missing_data.loc[x]\n    if a[1]>=16:\n        print(x)\n        train_data=train_data.drop(columns=[x])\n        test_data=test_data.drop(columns=[x])\n        \n\n\nprint(train_data.shape)\nprint(test_data.shape)\n","6015546f":"total = train_data.isnull().sum().sort_values(ascending=False)\nprint(total)","b56a1bb9":"train_data=train_data.fillna(method='ffill')\ntest_data=test_data.fillna(method='ffill')\ntotal_train = train_data.isnull().sum().sort_values(ascending=False)\nprint(total_train)\ntotal_test = test_data.isnull().sum().sort_values(ascending=False)\nprint(total_test)\n\n","6b8aafa1":"#Check How many are catagory and numerical variables\ncat_cols = [x for x in train_data.columns if train_data[x].dtype == 'object']\nnum_cols = [x for x in train_data.columns if train_data[x].dtype != 'object']\n\nprint(\"Number of column features in Training data set: \", train_data.shape[1])\nprint(\"Categorical Variables in 'train.csv' file: \", len(cat_cols))\nprint(\"Numerical Variables in 'train.csv' file: \", len(num_cols))\n\n#Copying categorical variables \ncat_df = train_data.select_dtypes(include=['object']).copy()\n\n\n#Copying numerical variables \nnum_df = train_data.select_dtypes(include=['int64','float64']).copy()\n\n#num_df.head()\ncat_df.head()\n","df9d246f":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\n\n#for i in cat_df.columns:\n#    cat_df[i]=le.fit_transform(cat_df[i])\n\n#cat_df.head(50)\n\nfor i in train_data.columns:\n    if i!=\"SalePrice\":\n        if train_data[i].dtype=='object':\n            train_data[i]=le.fit_transform(train_data[i])\n        elif test_data[i].dtype=='object':\n            test_data[i]=le.fit_transform(test_data[i])\n\n\n\nprint(train_data.shape)\nprint(test_data.shape)\n\ntrain_data.head()\n#test_data.head()\n","959d2d96":"#Feature Selection Technique 3: Correlation analysis\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nX = train_data.iloc[:,0:-1]  #independent columns\ny = train_data.iloc[:,-1]    #target column i.e price range\n#get correlations of each features in dataset\ncorrmat = train_data.corr()\ntop_corr_features = corrmat.index\nplt.figure(figsize=(81,81))\n#plot heat map\ng=sns.heatmap(train_data[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","0e5c7c8a":"k = 15 \n  \ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index \n  \ncm = np.corrcoef(train_data[cols].values.T) \nf, ax = plt.subplots(figsize =(12, 10)) \n  \nsns.heatmap(cm, ax = ax, cmap =\"YlGnBu\", \n            linewidths = 0.1, yticklabels = cols.values,  \n                              xticklabels = cols.values) ","9854e330":"#Feature Selection Technique 3: Correlation analysis\ncorrmat = train_data.corr()\ncorrmat.sort_values([\"SalePrice\"], ascending = False, inplace = True)\ncor_SalePrice=corrmat.SalePrice\n\n\nfor x in cor_SalePrice.index:\n    if cor_SalePrice[x] < 0.4:\n        cor_SalePrice=cor_SalePrice.drop([x])\n\nprint(cor_SalePrice)","88a02473":"import seaborn as sns\n#for a in cor_SalePrice.index:\n#train_set=train_data\n#test_set=test_data\n\ndf_train=pd.DataFrame()\ndf_test=pd.DataFrame()\n\nfor n in cor_SalePrice.index:\n    #rint(train_data[n])\n    df_train=df_train.append(train_data[n])\n    if n != \"SalePrice\":\n        df_test=df_test.append(test_data[n])\n#   df = pd.DataFrame(data, index =['rank1', 'rank2', 'rank3', 'rank4']) \n\ndf_train=df_train.T\ndf_test=df_test.T\n\ndf_train.head()\n#df_test.head()","7ef00439":"\n\n\n\nYearBuilt=df_train[\"YearBuilt\"]\nGarageYrBlt=df_train[\"GarageYrBlt\"]\nYearRemodAdd=df_train[\"YearRemodAdd\"]\n\nfor b in range(0,len(YearBuilt)):\n    if YearBuilt[b]>=2000:\n        YearBuilt[b]=1\n    elif YearBuilt[b]>=1950 and YearBuilt[b]<2000:\n        YearBuilt[b]=2\n    else:\n        YearBuilt[b]=3\n        \n    if GarageYrBlt[b]>=2000:\n        GarageYrBlt[b]=1\n    elif GarageYrBlt[b]>=1950 and GarageYrBlt[b]<2000:\n        GarageYrBlt[b]=2\n    else:\n        GarageYrBlt[b]=3\n    \n    if YearRemodAdd[b]>=2000:\n        YearRemodAdd[b]=1\n    elif YearRemodAdd[b]>=1950 and YearRemodAdd[b]<2000:\n        YearRemodAdd[b]=2\n    else:\n        YearRemodAdd[b]=3\n\n        \n        \nYearBuilt_test=df_test[\"YearBuilt\"]\nGarageYrBlt_test=df_test[\"GarageYrBlt\"]\nYearRemodAdd_test=df_test[\"YearRemodAdd\"]\n\nfor b in range(0,len(YearBuilt_test)):\n    if YearBuilt_test[b]>=2000:\n        YearBuilt_test[b]=1\n    elif YearBuilt_test[b]>=1950 and YearBuilt_test[b]<2000:\n        YearBuilt_test[b]=2\n    else:\n        YearBuilt_test[b]=3\n\n    if GarageYrBlt_test[b]>=2000:\n        GarageYrBlt_test[b]=1\n    elif GarageYrBlt_test[b]>=1950 and GarageYrBlt_test[b]<2000:\n        GarageYrBlt_test[b]=2\n    else:\n        GarageYrBlt_test[b]=3\n    \n    if YearRemodAdd_test[b]>=2000:\n        YearRemodAdd_test[b]=1\n    elif YearRemodAdd_test[b]>=1950 and YearRemodAdd_test[b]<2000:\n        YearRemodAdd_test[b]=2\n    else:\n        YearRemodAdd_test[b]=3\n\n        \n        \ndf_test.head()\ndf_train.head()\n        \n","3a27ca9b":"\n#####Basic Scatter Plot#########\nfig, ax = plt.subplots()\nplt.scatter(df_train[\"SalePrice\"],df_train[\"OverallQual\"])\nax.set_xlabel('Sale Price')\nax.set_ylabel('Overall Quality Rating')\nax.set_title('Sale Price vs Overall Qual')\nplt.show()\n\n\n############ Bubble Chart using Scatter Plot ##################\nfig1, ax1 = plt.subplots()\n\nscatter = ax1.scatter(df_train[\"SalePrice\"],df_train[\"OverallQual\"], c=df_train[\"YearBuilt\"], s=df_train[\"YearBuilt\"]*100)\n\n# produce a legend with the unique colors from the scatter\nlegend1 = ax1.legend(*scatter.legend_elements(),\n                    loc=\"lower right\", title=\"Built Year Classes\",frameon=True)\nax1.add_artist(legend1)\nax1.set_xlabel('Sale Price')\nax1.set_ylabel('Overall Quality Rating')\nax1.set_title('Sale Price vs Overall Qual vs YearBuilt')\n# produce a legend with a cross section of sizes from the scatter\n#handles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.6)\n#legend2 = ax.legend(handles, labels, loc=\"upper right\", title=\"Sizes\")\nplt.show()\n\n","ef5d295c":"# Basics in feature scaling\nsns.distplot(df_train['SalePrice'], kde = False, color ='red', bins = 30)\n\nfrom scipy.stats import kurtosis, skew\nprint(kurtosis(df_train['SalePrice']))\nprint(skew(df_train['SalePrice']))\n\nprint(df_train['SalePrice'].describe())","36b9c4f0":"#Inverse transform\n#inversed = scaler.inverse_transform(data_scaled)\n#print(inversed)","68789e87":"#MIN-MAX scaler\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler() \ndata_min_max_scaled = scaler.fit_transform(df_train)\n\nsns.distplot(data_min_max_scaled[:,2], kde = False, color ='red', bins = 30)\nsample=data_min_max_scaled[:,2]\nsample=pd.DataFrame(sample)\nsample.describe()","01b90af5":"# Log+1 function standardization\n\nnorm_log=np.log(df_train+1)\nsns.distplot(norm_log.iloc[:,0], kde = False, color ='red', bins = 30)\n#for n in norm_log.columns:\n#    sns.distplot(norm_log[n], kde = False, color ='red', bins = 30)\nprint(norm_log.iloc[:,0].describe())","0bb60124":"# standardScaler()\n#NOTE:  standardization (or Z-score normalization) means centering the variable at zero and standardizing the variance at 1.\n#print(df_train.describe())\nfrom sklearn.preprocessing import StandardScaler\nscaler_train = StandardScaler() \nscaler_test = StandardScaler() \n\ntrain_data_scaled = scaler_train.fit_transform(df_train) #Standardscaler on training_data\ntest_data_scaled = scaler_test.fit_transform(df_test)#Standardscaler on testing_data\n\nsaleprice_scaled=pd.DataFrame(train_data_scaled[:,0])\nprint(saleprice_scaled.describe())\n\nsns.distplot(train_data_scaled[:,0], kde = False, color ='red', bins = 30)\n\n\n#Inverse transform\n#inversed = scaler.inverse_transform(data_scaled)\n#print(inversed)\nprint(train_data_scaled.shape)\nprint(train_data_scaled[:,1::].shape)","eb386ea8":"from sklearn.model_selection import train_test_split\nimport statistics as st\n#X=df_train.iloc[:,1::]\n#Y=df_train.iloc[:,0]\n\n#print(X.shape)\n#print(Y.shape)\n\ntrain, test= train_test_split(df_train, test_size=0.3, shuffle=False,random_state=42)\n\nprint(train.shape)\nprint(test.shape)\n\ntrain_mean=train.mean()\ntrain_std=np.std(train)\n\nscaled_train =  (train - train.mean()) \/ np.std(train)\n\nscaled_test = (test - train.mean()) \/ np.std(train)\n\nx_train=scaled_train.iloc[:,1::]\ny_train=scaled_train.iloc[:,0]\n\nx_test=scaled_test.iloc[:,1::]\ny_test=scaled_test.iloc[:,0]\n\ntrain.head()\ntest.head()\n\n#print(scaled_train.min(), scaled_train.max())\n#df_test\n#train, test = split(df_train)\n# Gradient boost regressor\n\n#train_y=train_data_scaled[:,0]\n#train_x=train_data_scaled[:,1::]\n#print(train_x.shape)\n#train_y=train_y.reshape((len(train_y),1))\n#print(train_y.shape)\n\n","2d1ab468":"#SVR on Scaled Data\nfrom sklearn import svm\n\nfrom sklearn import metrics\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\n\n# Fit regression model\nregr_rbf = svm.SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\nregr_rbf.fit(x_train, y_train)\npredict_scaled_output_svr_rbf=regr_rbf.predict(x_test)\n# Fit regression model\n#svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n#svr_lin = SVR(kernel='linear', C=100, gamma='auto')\n#svr_poly = SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1,coef0=1)\n#Inverse transform\ntest_predict_svr_rbf=(predict_scaled_output_svr_rbf*train_std.iloc[0])+ train_mean.iloc[0] \nactual=(y_test*train_std.iloc[0])+ train_mean.iloc[0]\n\nprint(\"R2_Score\", r2_score(actual, test_predict_svr_rbf))\n\n#print(\"MAE\", metrics.mean_absolute_error(actual, test_predict_svr_rbf))\n#print(\"MSE\", metrics.mean_squared_error(actual, test_predict_svr_rbf))\nprint(\"RMSE\",np.sqrt(metrics.mean_squared_error(actual, test_predict_svr_rbf)))\n#plt.plot(actual, test_predict_svr_rbf)\n#plt.show()\n\n#print(actual.iloc[0], test_predict_svr_rbf[0])\nfrom yellowbrick.regressor import PredictionError\nvisualizer_regr_rbf = PredictionError(regr_rbf)\n\nvisualizer_regr_rbf.fit(x_train, y_train)  # Fit the training data to the visualizer\nvisualizer_regr_rbf.score(x_test, y_test)  # Evaluate the model on the test data\nvisualizer_regr_rbf.show()                 # Finalize and render the figure\n","802c2f73":"#SVR on Not scaled data\nfrom sklearn import svm\nx_train_raw=train.iloc[:,1::]\ny_train_raw=train.iloc[:,0]\n\nx_test_raw=test.iloc[:,1::]\ny_test_raw=test.iloc[:,0]\n\n# Fit regression model\nregr_rbf_raw = svm.SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\nregr_rbf_raw.fit(x_train_raw, y_train_raw)\npredict_raw_output_svr_rbf=regr_rbf_raw.predict(x_test_raw)\n# Fit regression model\n#svr_rbf = SVR(kernel='rbf', C=100, gamma=0.1, epsilon=.1)\n#svr_lin = SVR(kernel='linear', C=100, gamma='auto')\n#svr_poly = SVR(kernel='poly', C=100, gamma='auto', degree=3, epsilon=.1,coef0=1)\n\n#print(mean_squared_error(y_test_raw, predict_raw_output_svr_rbf))\nprint(\"R2_Score\", r2_score(y_test_raw, predict_raw_output_svr_rbf))\n\n#print(\"MAE\", metrics.mean_absolute_error(y_test_raw, predict_raw_output_svr_rbf))\n#print(\"MSE\", metrics.mean_squared_error(y_test_raw, predict_raw_output_svr_rbf))\nprint(\"RMSE\",np.sqrt(metrics.mean_squared_error(y_test_raw, predict_raw_output_svr_rbf)))\n\n#plt.plot(actual, test_predict_svr_rbf)\n#plt.show()\n\n#print(y_test_raw.iloc[0], predict_raw_output_svr_rbf[0])\n\nfrom yellowbrick.regressor import PredictionError\nvisualizer_rbf_raw = PredictionError(regr_rbf_raw)\n\nvisualizer_rbf_raw.fit(x_train_raw, y_train_raw)  # Fit the training data to the visualizer\nvisualizer_rbf_raw.score(x_test_raw, y_test_raw)  # Evaluate the model on the test data\nvisualizer_rbf_raw.show()                 # Finalize and render the figure\n\n\n","44d37eda":"# XGB on scaled data\nimport xgboost as xgb\n\nfrom sklearn import metrics\n\n\n#Fitting XGB regressor \nmodel = xgb.XGBRegressor(base_score=0.5, booster=None, colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints=None,\n             learning_rate=0.100000012, max_delta_step=0, max_depth=6,\n             min_child_weight=1, missing=nan, monotone_constraints=None,\n             n_estimators=100, n_jobs=0, num_parallel_tree=1,\n             objective='reg:squarederror', random_state=0, reg_alpha=0,\n             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method=None,\n             validate_parameters=False, verbosity=None)\n\n#model = xgb.XGBRegressor()#default - learning rate=0.3, base_score=0.5, booster=None\nmodel.fit(x_train,y_train)\nprint (model)\n\n\n\n\n#print(test_x.shape)\n#print(train_x.shape)\npredict_scaled_output = model.predict(data=x_test)\n#print(predict_scaled_output.shape)\n\n#print(train_mean.iloc[0])\n#print(train_std.iloc[0])\n#print(np.std(train).shape)\n#print(train.mean().shape)\n\n#Inverse transform\ntest_predict=(predict_scaled_output*train_std.iloc[0])+ train_mean.iloc[0] \nactual=(y_test*train_std.iloc[0])+ train_mean.iloc[0]\n\nprint(test_predict.shape)\nprint(actual.shape)\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nprint(\"R2_Score\", r2_score(actual, test_predict))\n\n#print(\"MAE\", metrics.mean_absolute_error(actual, test_predict))\n#print(\"MSE\", metrics.mean_squared_error(actual, test_predict))\nprint(\"RMSE\",np.sqrt(metrics.mean_squared_error(actual, test_predict)))\n\nprint(\"Mean squared logarithmic error regression loss: \", metrics.mean_squared_log_error(actual, test_predict))\n#print('RMSE in % is', (np.sqrt(metrics.mean_squared_error(actual, test_predict))\/max(15)*100)\n#actual=actual.reset_index()\n#plt.plot(test_predict)\n#plt.plot(actual)\n#plt.ylabel('Sale Price in $')\n#plt.show()\nfrom yellowbrick.regressor import PredictionError\nvisualizer = PredictionError(model)\n\nvisualizer.fit(x_train, y_train)  # Fit the training data to the visualizer\nvisualizer.score(x_test, y_test)  # Evaluate the model on the test data\nvisualizer.show()                 # Finalize and render the figure\n\n#print(x_train.shape)\n#print(y_train.shape)\n#print(x_test.shape)\n#print(y_test.shape)\n#variance_xgb= np.var(test_predict)\n#print(\"The Variance of the XGBoost model is: \", variance_xgb)","456bffb6":"import xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import metrics\n\n########Parameter tuning starts here\n# ROUND 1: Tuning max_depth and min_child_weight\nparam_grid1 = {\n    'max_depth':np.arange(1,11,2),\n    'min_child_weight':np.arange(1,11,2)\n}\n\n# First Search\ngsearch1 = GridSearchCV(xgb.XGBRegressor(),\n    param_grid1, \n    scoring='neg_mean_squared_error',\n    cv=4)\n\ngsearch1 = gsearch1.fit(x_train, y_train)\n\nbest_params = gsearch1.best_params_\n#best_params, -gsearch1.best_score_\n\n\n# ROUND 2: Tuning Gamma\n\nparam_grid2 = {'gamma':[i\/10.0 for i in range(0,6)]}\n\ngsearch2 = GridSearchCV(\n    xgb.XGBRegressor(**best_params),\n    param_grid2,\n    scoring='neg_mean_squared_error', \n    cv=5\n)\n\ngsearch2 = gsearch2.fit(x_train, y_train)\n\nbest_params.update(gsearch2.best_params_)\n#best_params, -gsearch2.best_score_\n\n# ROUND 3: Tuning subsamle and colsample_bytree\nparam_grid3 = {\n    'colsample_bytree':[i\/10.0 for i in range(0,11)],\n    'subsample':[i\/10.0 for i in range(0,11)]\n}\n\ngsearch3 = GridSearchCV(\n    xgb.XGBRegressor(**best_params),\n    param_grid3,\n    scoring='neg_mean_squared_error',\n    cv=3   \n)\n\ngsearch3 = gsearch3.fit(x_train, y_train)\nbest_params.update(gsearch3.best_params_)\n#best_params, -gsearch3.best_score_\n\n# TUNING Alpha & Lambda\nparam_grid4 = {\n    'alpha':[i\/10 for i in range(0,11)],\n    'lambda':[i\/10 for i in range(0,11)]\n}\n\ngsearch4 = GridSearchCV(\n    xgb.XGBRegressor(**best_params),\n    param_grid4, \n    scoring='neg_mean_squared_error',\n    cv=5\n)\n\ngsearch4 = gsearch4.fit(x_train, y_train)\n\nbest_params.update(gsearch4.best_params_)\n#best_params, -gsearch4.best_score_\n\n# Tuning: the number of trees and learning rate\nparam_grid5 = {\n    'n_estimators':np.arange(50, 450, 50),\n    'learning_rate':[0.01, 0.05, 0.1, .5, 1]\n}\n\ngsearch5 = GridSearchCV(\n    xgb.XGBRegressor(**best_params),\n    param_grid5, \n    scoring='neg_mean_squared_error',\n    cv=5\n)\n\ngsearch5 = gsearch5.fit(x_train, y_train)\n\nbest_params.update(gsearch5.best_params_)\n#best_params, -gsearch5.best_score_","d39b8990":"print(best_params)","6aa62ebf":"# XGB Tuned Model\nmodeltuned = xgb.XGBRegressor(**best_params)\n\n\n\nmodeltuned.fit(x_train,y_train)\nprint (modeltuned)\n\n\n\n\n#print(test_x.shape)\n#print(train_x.shape)\npredict_scaled_output = modeltuned.predict(data=x_test)\n#print(predict_scaled_output.shape)\n\n#print(train_mean.iloc[0])\n#print(train_std.iloc[0])\n#print(np.std(train).shape)\n#print(train.mean().shape)\n\n#Inverse transform\ntest_predict=(predict_scaled_output*train_std.iloc[0])+ train_mean.iloc[0] \nactual=(y_test*train_std.iloc[0])+ train_mean.iloc[0]\n\nprint(test_predict.shape)\nprint(actual.shape)\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\n\nprint(\"R2_Score\", r2_score(actual, test_predict))\n\n#print(\"MAE\", metrics.mean_absolute_error(actual, test_predict))\n#print(\"MSE\", metrics.mean_squared_error(actual, test_predict))\nprint(\"RMSE\",np.sqrt(metrics.mean_squared_error(actual, test_predict)))\n\nprint(\"Mean squared logarithmic error regression loss: \", metrics.mean_squared_log_error(actual, test_predict))\n#print('RMSE in % is', (np.sqrt(metrics.mean_squared_error(actual, test_predict))\/max(15)*100)\n#actual=actual.reset_index()\n#plt.plot(test_predict)\n#plt.plot(actual)\n#plt.ylabel('Sale Price in $')\n#plt.show()\nfrom yellowbrick.regressor import PredictionError\nvisualizer = PredictionError(modeltuned)\n\nvisualizer.fit(x_train, y_train)  # Fit the training data to the visualizer\nvisualizer.score(x_test, y_test)  # Evaluate the model on the test data\nvisualizer.show()                 # Finalize and render the figure\n\n#print(x_train.shape)\n#print(y_train.shape)\n#print(x_test.shape)\n#print(y_test.shape)\n#variance_xgb= np.var(test_predict)\n#print(\"The Variance of the XGBoost model is: \", variance_xgb)","64e4f071":"#Scale [df_test] for scaling\n\ntrain_mean=train.mean()\nSalePrice_train_mean=train_mean[0]\ntrain_mean=train_mean.drop(['SalePrice'])\n\ntrain_std=np.std(train)\nSalePrice_train_std=train_std[0]\ntrain_std=train_std.drop(['SalePrice'])\n\ndf_test_scaled=(df_test - train_mean)\/train_std\n\n\n\n#Prediction the scales 'SalePrice' \n# Use the loaded base XGBRegressor model to make predictions \npredict_output_scaled=model.predict(df_test_scaled) \n\n# Scaling back the scaled prediction value to normal value\npredicted_SalePrice=(predict_output_scaled*SalePrice_train_std)+ SalePrice_train_mean \n\nprint(predicted_SalePrice)\n#print(predicted_SalePrice.shape)\n","063d73ed":"# Scatter Plot & Bubble Plot for more insight\n\nInference from Bubble Plot,\n\n* House built after 2000 is of good quality and priced higher\n* Houses built before 1950 is mostly of low quality and priced higher \n\n# Careful on Outliers [Category House priced > 700000]\n","7200d439":"# Hyper parameter tuning on XGB","e671cd5a":" # - Careful on Outliers - Plot Distribution Plot on [SalePrice]\n \n # The [SalePrice] distribution plot is positive skewness \n \n # Note the Distribution of values in [25%, 50%, 75%, max]\n \n # Normally distributed plot with [skewness = 0] and [Kurtosis < 6] is good\n \n # Distribution with high [skewness and kurtosis] is not good\n \n # Meaning - there may be high chances of OUTLIERS","2682fab6":"# - Create new dataframe only having relevant features","ba95d338":"# Check for total number of missing values in training dataset ","0671e265":"# - Print Top correlated features with [SalePrice]\n\n# - Some important intepretations:\n     * Correlation values are between (-1 & 1)\n     * +ve values - +ve correlation\n     * -ve values - -ve correlation\n     * values closer to '0' - no correlation\n     * values closer to '1' - high +ve correlation | values closer to '-1' - high -ve correlation\n     \n     * \"+ve correlated features are considered for this example\"\n\n# Some Important inferences from 'HeatMap' & 'Correlation values'\n     * [SalePrice] is influnced by [Overall Quality], [Garage features = GarageCars, GarageArea, GarageYrBuilt]\n     * Customers give importance to [Garage features = GarageCars, GarageArea, GarageYrBuilt]-> Meaning -> if you have a with recent [Yrbuilt] & decent [OverallQuality] with all [garage features] -> you can \"price it higher\"\n     * \n     ","34e73447":"# - HeatMap for [SalePrice] vs ['Top 15' correlated features] ","dc5b61a5":"# Note: Perfom prediction on \"test.csv\" \/ [df_test] dataframe\n\nSince the R square score of Base XGBRegressor is more than the tuned model I choose to you the base regressor model to perform [SalePrice] prediction on \"test.csv\" \/ df_test\n","c1e5fa03":"# - Correlation Analysis on Testing data\n\n# - Correlation between the [SalePrice] vs [Other Features]\n\n# - Plot the correlation as HeatMap using 'Seaborn'","c3336a8b":"# - SVR on not scaled data","ba6e4893":"# - Use 'LabelEncoder()' to convert 'Categorical' features to 'Numerical' features in both training and testing data frame\n\n# - No. of features in training - 74\n# - No. of features in testing  - 73 ...because testing does not have 'SalePrice' feature","5c104a53":"# Support Vector Regressor on Scaled data","235130d8":"# Split [df_train] into [train, test]\n\n   - [train] to build the ML model\n   - [test] to check the model prediction accuracy\n   - [70% : 30%] = [train : test]\n   \n# - I use my own scaling technique (use train(mean, stDev) for scaling testing set)\n","6aa109a6":"# - Convert [YearBuilt], [GarageYrBlt], [YearRemodAdd] into categorical numeric variables.\n\nif year>2000         -> category 1;\n\nif year 1950 to 2000 -> category 2;\n\nif year <1950        -> category 3;","fd17fe06":"# I build the model using training dataset [df_train]\n\n# df_test does not have [SalePrice] feature so we cannot use this to train ML model\n\n# Note: \n   - I don't consider test.csv[df_test] as of now and I will use [df_test] only on the build model.\n   - I build the entire ML prediction model based on [df_train] or \"train.csv\" dataset","2ffbb0fc":"# - Check for  'Nan'\/'Null'\/'missing values' in 'Percentage'\n\n# - 'Drop' column features where missing values is more than 16%\n\n# - Print shapes of both taining and testing dataframes","e7192793":"# - Check for 'categorical' and 'numerical' features","166c6dad":"# - Fill missing\/NaN\/Null using 'fillna()'","1cb52dec":"# - Scaling should be applied to [all the features] or can be [user defined]\n\n   Procedure in Scaling\n   - Scale the data\n   - Train the model\n   - Predict the outcome\n   - Use Inverse scaling to arrive 'Exact Prediction'","5d0b1dfd":"# XGB boost on Scaled data","337e2b4d":"# Reference\n\nI learnt Hyper parameter tuning from \"wcneill\" and I wish to cite his git-hub profile\n\n\"wcneill\", https:\/\/github.com\/wcneill\/kaggle\/blob\/master\/house%20prices\/sales.ipynb ","7599a258":"# Methods to remove OUTLIERS,\n\n   - Identify and Remove the outlier data\n   - Scale\/Normalize the feature\n   - Methods of Scaling\n     1. Standard Scaler\n     2. Robust Scaler\n     3. Min-Max Scaler\n     4. Sigmoid function\n     5. Log function\n     6. Log+1 function\n     7. Cube root function\n     8. Log Max root function\n     9. Hyperbolic tangent (tanh) function\n    10. Percentile linearization\n    \n    - Feature Scaling also helps in normalize the dataset","7671aadd":"# Conclusion:\n\n- Base & Tuned XGBRegressor are used to predict the Sale Price of house.   \n- The \n- Base XGBRegressor produces R square score 0.859\n- Tuned XGBRegressor produces R square score 0.786\n- Tuning the regressor model degrades its performance\n\n- Maybe the effect of Outliers: I haven't removed outliers from the data which may show improvement in R2 score of tuned model.","75fe2659":"# - Import 'train.csv' and 'test.csv' file as pandas DataFrame and print it's shape\n\n# - Drop 'Id' feature from both the dataframe\n\n# - Use 'describe()' & 'info()' to know basic info of your data"}}