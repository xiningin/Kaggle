{"cell_type":{"09a241f0":"code","844721d5":"code","299580a4":"code","fe9b217b":"code","5204897a":"code","dd9d9e6f":"code","319854b9":"code","10018a89":"code","0743776d":"code","7e0b45fd":"code","d0491623":"code","a58bc088":"code","de3cdc66":"code","27d63588":"code","218bf456":"code","ee1a7862":"code","f3d75572":"code","a3731a7f":"markdown","649b0e9e":"markdown","685c0ed2":"markdown","0a985d63":"markdown","16071ca8":"markdown","dd6f59aa":"markdown"},"source":{"09a241f0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","844721d5":"!pip install sklearn_crfsuite","299580a4":"import numpy as np\nimport pandas as pd\nfrom pandas.core.groupby.base import OutputKey\nfrom sklearn.feature_extraction import DictVectorizer\nimport sklearn_crfsuite\nfrom sklearn_crfsuite import scorers\nfrom sklearn_crfsuite import metrics\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nimport nltk\nfrom tqdm import tqdm\nimport json\nimport eli5","fe9b217b":"ner_data = pd.read_csv(\"\/kaggle\/input\/entity-annotated-corpus\/ner_dataset.csv\", encoding = \"ISO-8859-1\")\nner_data = ner_data.fillna(method='ffill')","5204897a":"ner_data.head()","dd9d9e6f":"ner_data.isnull().sum()","319854b9":"ner_data['Sentence #'].nunique() ","10018a89":"ner_data['Word'].nunique()","0743776d":"ner_data.Tag.nunique()","7e0b45fd":"ner_data.groupby('Tag').size().reset_index(name='counts')","d0491623":"y = ner_data.Tag.values\n\nclasses = np.unique(y)\nclasses = classes.tolist()\n\nnew_classes = classes.copy()\nnew_classes.pop()\nnew_classes","a58bc088":"class SentenceGetter(object):\n    def __init__(self, data):\n        self.n_sent = 1\n        self.data = data\n        self.empty = False\n        agg_func = lambda s: [(w, p, t) for w, p, t in zip(s['Word'].values.tolist(), \n                                                           s['POS'].values.tolist(), \n                                                           s['Tag'].values.tolist())]\n        self.grouped = self.data.groupby('Sentence #').apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n        \ngetter = SentenceGetter(ner_data)\nsentences = getter.sentences","de3cdc66":"def word2features(sent, i):\n    word = sent[i][0]\n    postag = sent[i][1]\n    features = {\n        'bias': 1.0, \n        'word.lower()': word.lower(), \n        'word[-3:]': word[-3:],\n        'word[-2:]': word[-2:],\n        'word.isupper()': word.isupper(),\n        'word.istitle()': word.istitle(),\n        'word.isdigit()': word.isdigit(),\n        'postag': postag,\n        'postag[:2]': postag[:2],\n    }\n    if i > 0:\n        word1 = sent[i-1][0]\n        postag1 = sent[i-1][1]\n        features.update({\n            '-1:word.lower()': word1.lower(),\n            '-1:word.istitle()': word1.istitle(),\n            '-1:word.isupper()': word1.isupper(),\n            '-1:postag': postag1,\n            '-1:postag[:2]': postag1[:2],\n        })\n    else:\n        features['BOS'] = True\n    if i < len(sent)-1:\n        word1 = sent[i+1][0]\n        postag1 = sent[i+1][1]\n        features.update({\n            '+1:word.lower()': word1.lower(),\n            '+1:word.istitle()': word1.istitle(),\n            '+1:word.isupper()': word1.isupper(),\n            '+1:postag': postag1,\n            '+1:postag[:2]': postag1[:2],\n        })\n    else:\n        features['EOS'] = True\n    return features\n\ndef sent2features(sent):\n    return [word2features(sent, i) for i in range(len(sent))]\n\ndef sent2labels(sent):\n    return [label for token, postag, label in sent]\n\ndef sent2tokens(sent):\n    return [token for token, postag, label in sent]\n","27d63588":"X = [sent2features(s) for s in sentences]\ny = [sent2labels(s) for s in sentences]\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)","218bf456":"parameters = {'c1':[0.1, 0.2, 0.5, 1], 'c2':[0.1, 0.2, 0.5, 1], 'max_iterations': [100, 150, 200]}","ee1a7862":"# clf = GridSearchCV(crf, parameters, scoring='accuracy', verbose = 3, n_jobs=-1)","f3d75572":"for c1_val in parameters['c1']:\n    for c2_val in parameters['c2']:\n        for iters in parameters['max_iterations']:\n            print(f\"c1 value : {c1_val} , c2 value : {c2_val} , iter values : {iters}\")\n            crf = sklearn_crfsuite.CRF(\n                algorithm='lbfgs',\n                all_possible_transitions=True,\n                c1 = c1_val,\n                c2 = c2_val,\n                max_iterations = iters)\n            crf.fit(X_train, y_train)\n            y_pred = crf.predict(X_val)\n            print(metrics.flat_classification_report(y_val, y_pred, labels = new_classes))","a3731a7f":"Lets start training the model","649b0e9e":"Lets Define and build the CRF features","685c0ed2":"Lets create a Train and Eval set","0a985d63":"Can use Gridsearch Implementation too\n","16071ca8":"Lets Create Train_X and Train_Y","dd6f59aa":"#For visualizing the data\n"}}