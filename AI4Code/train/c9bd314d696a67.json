{"cell_type":{"45045a47":"code","f22543c0":"code","0680bf03":"code","0f0a3475":"code","961478d1":"code","ec93cece":"code","cf3403b8":"code","634b4e9c":"code","15438bee":"code","c3644488":"code","a490a8b3":"code","5d895972":"code","ad973730":"code","50e763b2":"code","a1d14c71":"code","12134f71":"code","9d1fda16":"code","5648382a":"code","d31253cc":"code","8e289108":"code","fdc95c37":"code","398324f4":"code","b7ae5fca":"code","905da51f":"code","810d1cbb":"code","7af772ff":"code","dc87c945":"code","5412bd0b":"markdown","5565b332":"markdown","68eaf4a2":"markdown","84685aab":"markdown","afd1c92f":"markdown","97a7b01b":"markdown","4e43ffa7":"markdown"},"source":{"45045a47":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport IPython.display as ipd\nimport librosa\nimport os\nimport librosa.display\n\nimport tensorflow as tf\nimport keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout,Activation,Flatten,BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom sklearn import metrics\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom datetime import datetime ","f22543c0":"df = pd.read_csv(\"..\/input\/urbansound8k\/UrbanSound8K.csv\")\ndf.head()","0680bf03":"df['class'].value_counts()","0f0a3475":"import seaborn as sns\nplt.figure(figsize=(15,8))\nsns.set_theme(style=\"darkgrid\")\nax = sns.countplot(x=\"class\", data=df)\nplt.show()","961478d1":"filename1 = \"..\/input\/urbansound8k\/fold1\/101415-3-0-2.wav\"\nplt.figure(figsize=(14,5))\ndata,sample_rate=librosa.load(filename1)\nprint(sample_rate)\nprint(type(data))\nprint(data.shape)\nlibrosa.display.waveplot(data,sr=sample_rate)\nipd.Audio(filename1)","ec93cece":"data","cf3403b8":"filename2 = \"..\/input\/urbansound8k\/fold5\/100032-3-0-0.wav\"\nplt.figure(figsize=(14,5))\ndata1,sample_rate1=librosa.load(filename2)\nlibrosa.display.waveplot(data1,sr=sample_rate1)\nipd.Audio(filename2)","634b4e9c":"def features_extractor(file):\n    audio, sample_rate = librosa.load(file_name, res_type='kaiser_fast') \n    mfccs_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n    #print(mfccs_features)\n    mfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\n    \n    return mfccs_scaled_features","15438bee":"extracted_features=[]\nfor i in range(8732):\n    file_name = '..\/input\/urbansound8k\/fold' + str(df[\"fold\"][i]) + '\/' + df[\"slice_file_name\"][i]\n    final_class_labels=df[\"class\"][i]\n    data=features_extractor(file_name)\n    extracted_features.append([data,final_class_labels])","c3644488":"extracted_features_df=pd.DataFrame(extracted_features,columns=['feature','class'])\nextracted_features_df.head()","a490a8b3":"X=np.array(extracted_features_df['feature'].tolist())\ny=np.array(extracted_features_df['class'].tolist())","5d895972":"X.shape","ad973730":"y=np.array(pd.get_dummies(y))","50e763b2":"y.shape","a1d14c71":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.15,random_state=0,stratify=y)","12134f71":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","9d1fda16":"X_train_scaled.shape","5648382a":"X_test_scaled.shape","d31253cc":"num_labels=y.shape[1]\nnum_labels","8e289108":"model=Sequential()\n###first layer\nmodel.add(Dense(100,input_shape=(40,)))\nmodel.add(Activation('relu'))\nBatchNormalization()\nmodel.add(Dropout(0.5))\n###second layer\nmodel.add(Dense(200))\nmodel.add(Activation('relu'))\nBatchNormalization()\nmodel.add(Dropout(0.5))\n###third layer\nmodel.add(Dense(100))\nmodel.add(Activation('relu'))\nBatchNormalization\nmodel.add(Dropout(0.5))\n\n###final layer\nmodel.add(Dense(num_labels))\nmodel.add(Activation('softmax'))","fdc95c37":"model.summary()\n","398324f4":"model.compile(loss='categorical_crossentropy',metrics=['CategoricalAccuracy'],optimizer='adam')","b7ae5fca":"num_epochs = 100\nnum_batch_size = 32\n\ncheckpointer = ModelCheckpoint(filepath='saved_models\/audio_classification.hdf5', \n                               verbose=1, save_best_only=True)\nstart = datetime.now()\n\nhistory = model.fit(X_train_scaled, y_train, batch_size=num_batch_size, epochs=num_epochs, validation_data=(X_test, y_test), callbacks=[checkpointer], verbose=1)\n\n\nduration = datetime.now() - start\nprint(\"Training completed in time: \", duration)","905da51f":"plt.plot(history.history['categorical_accuracy'])\nplt.plot(history.history['val_categorical_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'val'], loc='upper left')\nplt.show()","810d1cbb":"history.history.keys()","7af772ff":"test_accuracy=model.evaluate(X_test_scaled,y_test,verbose=0)\ntest_accuracy","dc87c945":"predictions = model.predict(X_test_scaled)\npreds = np.argmax(predictions, axis = 1)\nresult = pd.DataFrame(preds)\nresult.to_csv(\"UrbanSound8kResults.csv\")","5412bd0b":"# Extract Features\n\nHere we will be using Mel-Frequency Cepstral Coefficients(MFCC) from the audio samples. The MFCC summarises the frequency distribution across the window size, so it is possible to analyse both the frequency and time characteristics of the sound. These audio representations will allow us to identify features for classification.","5565b332":"# Label Encoding","68eaf4a2":"# Observation\n\nHere Librosa converts the signal to mono, meaning the channel will alays be 1","84685aab":"# Now we iterate through every audio file and extract features using Mel-Frequency Cepstral Coefficients\n","afd1c92f":"# Split the dataset into independent and dependent dataset","97a7b01b":"import pandas as pd\nimport numpy as np\nimport librosa\nimport numpy as np\n\nfile_name = '..\/input\/testing\/5002.csv'\ndf = pd.read_csv(file_name)\nprint(df.head())\nsr = int(len(df['FHR']))\/(90*60)\nsignal = df['FHR']\nprint(signal.shape)\nprint(type(signal.to_numpy()))\nmfccs_features = librosa.feature.mfcc(y=signal.to_numpy(), sr=sr, n_mfcc=13)\nmfccs_scaled_features = np.mean(mfccs_features.T,axis=0)\nprint(mfccs_scaled_features)","4e43ffa7":"# Check whether the dataset is imbalanced"}}