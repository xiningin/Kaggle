{"cell_type":{"68e4ca88":"code","d7aecaf1":"code","818de148":"code","32f55d73":"code","5256a97f":"code","a150bda1":"code","81828086":"code","793f2392":"code","6a45d39a":"code","a5425b24":"code","d323c48b":"code","f445d8c9":"code","b1c56916":"code","b38c4ff8":"code","30c2c166":"code","c0c268c2":"code","776b271c":"code","828d8ac1":"code","c91edd12":"code","98c2dbb5":"code","90919154":"code","125b9d07":"code","92141da7":"code","10140221":"code","ab6bea3e":"code","88628021":"code","bc66a3eb":"code","a77321e6":"code","3566324e":"code","38445055":"code","958485ed":"code","93adb3e5":"code","8d07e696":"code","9944b0f8":"code","63f736d9":"code","ab23bf27":"code","67590404":"code","79956468":"code","85e7542c":"code","c4391a53":"code","dac60bb3":"code","30c19d79":"code","a18cabb6":"code","0509b52a":"code","c199a556":"code","deb3d176":"code","3a9c7580":"code","50fad2fd":"code","590c16f7":"code","31ea57a4":"code","57442c71":"code","f2bf9116":"code","474d995d":"code","33d1d5eb":"code","8ccb78af":"code","3c63c722":"code","72fff26f":"code","f89320c0":"code","3d30b0db":"code","9447827d":"code","39e2171e":"code","af6ab1fa":"code","adfd8f26":"code","d5cb7e97":"code","73e3e7e8":"code","64cb04e6":"code","3cb5c54d":"code","77ea7d66":"code","ce20d5c4":"code","a341cd9c":"code","f16d6516":"code","1054296b":"code","90799508":"code","f7693e36":"code","b69908d4":"code","01558f75":"code","c2aadea9":"code","8d01372b":"code","cbef4f0f":"code","e9b180f1":"code","925a6e68":"code","3542a5cb":"code","2c034053":"code","e9630ab6":"code","82bb1c9f":"code","714d5dd6":"code","47799b51":"code","2e2aa65d":"code","55e4c3e0":"code","7de1a3d8":"code","eae1280a":"code","1e84cff1":"code","95c723df":"code","dd12a86d":"code","6c768659":"code","92f50026":"code","1f284377":"code","219a1b79":"code","e75e187f":"code","0d056bcd":"code","67adee8f":"code","caad669f":"code","10420cbf":"code","49713022":"code","a5879923":"code","89568a86":"code","58e05a55":"code","47061ccb":"code","1bcaa533":"code","0c395994":"code","95d18541":"code","c78fa187":"code","afee18d9":"code","17c948e7":"code","828348c8":"code","11376e6a":"code","f281b03f":"code","58a82fcc":"code","88599d10":"code","849e4edb":"code","b96bccdf":"code","7e9abf3c":"code","3fff36b3":"code","01b0eec0":"code","75c91413":"code","be88aec6":"code","c12aab1b":"code","e46f7bbe":"code","7e9d43a9":"code","c0b79dfc":"code","08495459":"code","70c025ce":"code","65182267":"code","70a3dccf":"code","6427fd10":"code","29218918":"code","21476b92":"code","93aa1717":"code","d2b76963":"code","ec2b6f82":"code","1792ad0e":"code","0f66161c":"code","072c5102":"code","2f8250da":"code","01a6a6e1":"code","ffdbe613":"code","0c5e465a":"code","b8461c7b":"markdown","b0392c1b":"markdown","50e8621a":"markdown","d03914c9":"markdown","6281d64e":"markdown","5787d32e":"markdown","004d60cb":"markdown","2d3fda88":"markdown","015e6acb":"markdown","e0499362":"markdown","4a8cafcd":"markdown","947ddb9e":"markdown","ceb997c1":"markdown","0e2e18c0":"markdown","f66793e3":"markdown","39c2a4f7":"markdown","fc8b310d":"markdown","6fb3fbe8":"markdown","29ab1086":"markdown","9010c513":"markdown","47cc8ead":"markdown","de3e1710":"markdown","ccf49169":"markdown","673c0d81":"markdown","c44093e4":"markdown","526fa6df":"markdown","73dbe675":"markdown","6fe0d5c8":"markdown","d65ea437":"markdown","7940206f":"markdown","5945f183":"markdown","9c2d4096":"markdown","b4866130":"markdown","05e7d578":"markdown","4bed3c75":"markdown","d6c654ed":"markdown","d0219811":"markdown","be0d48ed":"markdown","0cde9bfa":"markdown","0d8863bd":"markdown","32187c75":"markdown","2059c970":"markdown","f93b4381":"markdown","bd5eae73":"markdown","b89319e5":"markdown","8f3ec883":"markdown","c1995383":"markdown","be688156":"markdown"},"source":{"68e4ca88":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d7aecaf1":"!pip install bs4","818de148":"import re\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom collections import defaultdict, Counter\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import WordPunctTokenizer\nfrom bs4 import BeautifulSoup\n\n\nfrom wordcloud import WordCloud \n\nimport warnings\nwarnings.filterwarnings('ignore')","32f55d73":"sns.set(style=\"white\", font_scale=1.2)\nplt.rcParams[\"figure.figsize\"] = [10,8]\npd.set_option.display_max_columns = 0\npd.set_option.display_max_rows = 0\n\nnltk.download('stopwords', quiet=True)\nstopwords = stopwords.words('english')","5256a97f":"train_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntrain_df.head()","a150bda1":"test_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntest_df.head()","81828086":"#total data length\nprint('There are {} rows and {} columns in train'.format(train_df.shape[0],train_df.shape[1]))\nprint('There are {} rows and {} columns in test'.format(test_df.shape[0],test_df.shape[1]))\n\n# unique location and keyword size of data\nprint(\"Checking train location column values\",len(train_df.location.unique()))\nprint(\"Checking train keyword column values\",len(train_df.keyword.unique()))\nprint(\"Checking test location column values\",len(test_df.location.unique()))\nprint(\"Checking test keyword column values\",len(test_df.keyword.unique()))\n\n#number of disaster tweets\nprint(\"disaster tweets\", len(train_df[train_df[\"target\"]==1]) )\nprint(\"non-disaster tweets\", len(train_df[train_df[\"target\"]==0]) )","793f2392":"target_distribution = train_df[\"target\"].value_counts(normalize=True)\nprint(\"Not Disaster: {:.2%}, Disaster: {:.2%}\".format(target_distribution[0], target_distribution[1]))\n\nsns.barplot(x=target_distribution.index, y=target_distribution)\nplt.title(\"Histogram of Disaster vs. Non-Disaster\")\nplt.xlabel(\"0 = Non-Disaster, 1 = Disaster\")\nplt.show()","6a45d39a":"#take look at neutral tweets\ntrain_df[train_df.target == 0].head(2)","a5425b24":"#take look at disaster tweets\ntrain_df[train_df.target == 1].head(2)","d323c48b":"plt.subplots(1,2,figsize=(10,5))\n#visualize top 20 train unique keywords\nplt.subplot(1,2,1)\ntrain_df.keyword.value_counts()[:20].plot(kind=\"bar\",title=\"Unique Keywords\")\n\n#visualize top 20 train unique locations\nplt.subplot(1,2,2)\ntrain_df.location.value_counts()[:20].plot(kind=\"bar\",title=\"Unique Locations\")\n\nplt.show()","f445d8c9":"plt.subplots(1,2,figsize=(10,5))\n#visualize top 20 disaster tweets and their keywords bar graph\nplt.subplot(1,2,1)\ntrain_df[train_df[\"target\"]==1].keyword.value_counts()[:20].plot(kind=\"bar\",title=\"Disaster tweets keywords\")\n\n#visualize top 20 non disaster tweets and their keywords bar graph\nplt.subplot(1,2,2)\ntrain_df[train_df[\"target\"]==0].keyword.value_counts()[:20].plot(kind=\"bar\",title=\"Non-Disaster tweets keywords\")\n\nplt.show()","b1c56916":"plt.subplots(1,2,figsize=(10,5))\n#visualize top 20 disaster tweets and their locations bar graph\nplt.subplot(1,2,1)\ntrain_df[train_df[\"target\"]==1].location.value_counts()[:20].plot(kind=\"bar\",title=\"Disaster tweets Locations\")\n\n#visualize top 20 non disaster tweets and their locations bar graph\nplt.subplot(1,2,2)\ntrain_df[train_df[\"target\"]==0].location.value_counts()[:20].plot(kind=\"bar\",title=\"Non-Disaster tweets Locations\")\n\nplt.show()","b38c4ff8":"null_counts = pd.DataFrame({\"Number_Null\": train_df.isnull().sum()})\nnull_counts[\"Percent_Null\"] = null_counts[\"Number_Null\"] \/ train_df.count() * 100\nnull_counts","30c2c166":"null_counts = pd.DataFrame({\"Number_Null\": test_df.isnull().sum()})\nnull_counts[\"Percent_Null\"] = null_counts[\"Number_Null\"] \/ test_df.count() * 100\nnull_counts","c0c268c2":"# Let's get rid of `Location` and  `keywords` columns as they are unnecessary.\n# train_df.drop(['keyword','location'],axis=1,inplace=True)\n# test_df.drop(['keyword','location'],axis=1,inplace=True)","776b271c":"#take look at neutral tweets index distribution\ntrain_df[train_df.target == 0].index","828d8ac1":"#take look at disaster tweets index distribution\ntrain_df[train_df.target == 1].index","c91edd12":"dupli_sum = train_df.text.duplicated().sum()\n\nif(dupli_sum>0):\n    print(dupli_sum, \" duplicates found\\nremoving duplicates...\")\n    train_df = train_df.loc[False==train_df.text.duplicated(), :]\n    print('There are {} rows and {} columns in train after removing duplicates'\n          .format(train_df.shape[0],train_df.shape[1]))\nelse:\n    print(\"no duplicates found\")\n    \ntrain_df","98c2dbb5":"#extract hashtags\ntrain_df[\"hashtags\"]=train_df[\"text\"].apply(lambda x:re.findall(r\"#(\\w+)\",x.lower()))\ntest_df[\"hashtags\"]=test_df[\"text\"].apply(lambda x:re.findall(r\"#(\\w+)\",x.lower()))\n\n#convert tokens hashtags to text\ntrain_df[\"hashtags\"]=train_df[\"hashtags\"].apply(lambda x: ' '.join(x))\ntest_df[\"hashtags\"]=test_df[\"hashtags\"].apply(lambda x: ' '.join(x))","90919154":"train_df.head(2)","125b9d07":"test_df.head(2)","92141da7":"# add the characters length of tweets\ntrain_df['text_len'] = [len(t) for t in train_df.text]\ntrain_df.head(2)","10140221":"sns.distplot(train_df[\"text_len\"])\nplt.title(\"Histogram of Tweet Length\")\nplt.xlabel(\"Number of Characters\")\nplt.ylabel(\"Density\")\nplt.show()","ab6bea3e":"g = sns.FacetGrid(train_df, col=\"target\", height=5)\n\ng = g.map(sns.distplot, \"text_len\")\ng.fig.subplots_adjust(top=.8)\n\nplt.suptitle(\"Distribution Tweet Length\")\nplt.show()","88628021":"test_df['text_len'] = [len(t) for t in test_df.text]\ntest_df.head(2)","bc66a3eb":"sns.distplot(test_df[\"text_len\"])\n\nplt.title(\"Histogram of Tweet Length\")\nplt.xlabel(\"Number of Characters\")\nplt.ylabel(\"Density\")\nplt.show()","a77321e6":"def count_words(x):\n    '''\n        A function to count number of words in a tweet\n        inpit : tweet\n        output: (int) number of words\n    '''\n    return len(x.split())","3566324e":"train_df[\"num_words_text\"] = train_df[\"text\"].apply(count_words)\n\nsns.distplot(train_df[\"num_words_text\"], bins=10)\nplt.title(\"Histogram of Number of Words per Tweet\")\nplt.xlabel(\"Number of Words\")\nplt.ylabel(\"Density\")\nplt.show()","38445055":"g = sns.FacetGrid(train_df, col=\"target\", height=5)\n\ng = g.map(sns.distplot, \"num_words_text\")\ng.fig.subplots_adjust(top=.8)\n\nplt.suptitle(\"Distribution Number of Words\")\nplt.show()","958485ed":"test_df[\"num_words_text\"] = test_df[\"text\"].apply(count_words)\n\nsns.distplot(test_df[\"num_words_text\"], bins=10)\nplt.title(\"Histogram of Number of Words per Tweet\")\nplt.xlabel(\"Number of Words\")\nplt.ylabel(\"Density\")\nplt.show()","93adb3e5":"def avg_word_length(x):\n    return np.sum([len(w) for w in x.split()]) \/ len(x.split())","8d07e696":"train_df[\"avg_word_length\"] = train_df[\"text\"].apply(avg_word_length)\n\nsns.distplot(train_df[\"avg_word_length\"])\nplt.title(\"Histogram of Average Word Length\")\nplt.xlabel(\"Average Word Length\")\nplt.ylabel(\"Density\")\nplt.show()","9944b0f8":"g = sns.FacetGrid(train_df, col=\"target\", height=5)\ng = g.map(sns.distplot, \"avg_word_length\")\n\ng.fig.subplots_adjust(top=.8)\n\nplt.suptitle(\"Distribution Average Word Length\")\nplt.show()","63f736d9":"test_df[\"avg_word_length\"] = test_df[\"text\"].apply(avg_word_length)\n\nsns.distplot(test_df[\"avg_word_length\"])\nplt.title(\"Histogram of Average Word Length\")\nplt.xlabel(\"Average Word Length\")\nplt.ylabel(\"Density\")\nplt.show()","ab23bf27":"tok = WordPunctTokenizer()\n\npat1 = r'@[A-Za-z0-9_]+'\npat2 = r'https?:\/\/[^ ]+'\ncombined_pat = r'|'.join((pat1, pat2))\nwww_pat = r'www.[^ ]+'\nnegations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\", \"haven't\":\"have not\",\"hasn't\":\"has not\",\n                 \"hadn't\":\"had not\",\"won't\":\"will not\",\"wouldn't\":\"would not\",\"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n                 \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\", \"mightn't\":\"might not\", \"mustn't\":\"must not\",\n                 }\n\nneg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n\ndef tweet_cleaner(text):\n    \n    soup = BeautifulSoup(text, 'lxml')\n    souped = soup.get_text()\n    \n    try:\n        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n    except:\n        bom_removed = souped\n        \n    stripped = re.sub(combined_pat, '', bom_removed)\n    stripped = re.sub(www_pat, '', stripped)\n    lower_case = stripped.lower()\n    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)   \n    words = [x for x  in tok.tokenize(letters_only) if len(x) > 1]\n    \n    return (\" \".join(words)).strip()","67590404":"train_df['clean_text'] = train_df['text'].map(lambda x: tweet_cleaner(x))\ntest_df['clean_text'] = test_df['text'].map(lambda x: tweet_cleaner(x))","79956468":"train_df.head(2)","85e7542c":"test_df.head(2)","c4391a53":"# Length of tweets after cleaning\ntrain_df['clean_text_len'] = [len(t) for t in train_df.clean_text]\ntrain_df.head(2)","dac60bb3":"# Length of tweets after cleaning\ntest_df['clean_text_len'] = [len(t) for t in test_df.clean_text]\ntest_df.head(2)","30c19d79":"# numbet of words in tweets after cleaning\ntrain_df[\"clean_num_words\"] = train_df[\"clean_text\"].apply(count_words)\ntrain_df.head(2)","a18cabb6":"# numbet of words in tweets after cleaning\ntest_df[\"clean_num_words\"] = test_df[\"clean_text\"].apply(count_words)\ntest_df.head(2)","0509b52a":"# Average words length in tweets after cleaning\ntrain_df[\"clean_avg_word_length\"] = train_df[\"clean_text\"].apply(avg_word_length)\ntrain_df.head(2)","c199a556":"# Average words length in tweets after cleaning\ntest_df[\"clean_avg_word_length\"] = test_df[\"clean_text\"].apply(avg_word_length)\ntest_df.head(2)","deb3d176":"train_df['clean_text_stopword'] = train_df['clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\ntrain_df.head(2)","3a9c7580":"test_df['clean_text_stopword'] = test_df['clean_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\ntest_df.head(2)","50fad2fd":"#Dropping words whose length is less than 3\ntrain_df['clean_text_stopword'] = train_df['clean_text_stopword'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ntrain_df.head(2)","590c16f7":"test_df['clean_text_stopword'] = test_df['clean_text_stopword'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ntest_df.head(2)","31ea57a4":"disaster_tweets = train_df[train_df.target == 1]\ndisaster_string = []\n\nfor t in disaster_tweets.clean_text_stopword:\n    disaster_string.append(t)\n    \ndisaster_string = pd.Series(disaster_string).str.cat(sep=' ')","57442c71":"wordcloud_disaster = WordCloud(width=1600, height=800,max_font_size=200 ,colormap='magma').generate(disaster_string)\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud_disaster, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","f2bf9116":"neutral_tweets = train_df[train_df.target == 0]\nneutral_string = []\nfor t in neutral_tweets.clean_text_stopword:\n    neutral_string.append(t)\nneutral_string = pd.Series(neutral_string).str.cat(sep=' ')","474d995d":"wordcloud_neutral = WordCloud(width=1600, height=800,max_font_size=200).generate(neutral_string)\nplt.figure(figsize=(12,10))\nplt.imshow(wordcloud_neutral, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","33d1d5eb":"plt.subplots(1,2,figsize=(15,10))\n\nplt.subplot(1,2,1)\n#Bigram Frequency distribution for disaster tweets\n#convert disaster tweets into single string\ntxt=' '.join(train_df[train_df[\"target\"]==1][\"clean_text_stopword\"])\ndisaster_bigram=nltk.FreqDist(nltk.bigrams(nltk.word_tokenize(txt)))\ntmplst=disaster_bigram.most_common(30)\n\n#visualize Bigram frequency distribution for disaster tweets using bar graph\nwrd,cnt=zip(*tmplst)\nwrd=[ x+\",\"+y for (x,y) in wrd]\nplt.barh(wrd,cnt)\nplt.title(\"Disaster Bigram BarGraph\")\n\nplt.subplot(1,2,2)\n#Bigram Frequency distribution for non disaster tweets\n#convert non disaster tweets into single string\ntxt=' '.join(train_df[train_df[\"target\"]==0][\"clean_text_stopword\"])\nnondisaster_bigram=nltk.FreqDist(nltk.bigrams(nltk.word_tokenize(txt)))\ntmplst=nondisaster_bigram.most_common(30)\n\n#visualize Bigram frequency distribution for non disaster tweets using bar graph\nwrd,cnt=zip(*tmplst)\nwrd=[ x+\",\"+y for (x,y) in wrd]\nplt.barh(wrd,cnt)\nplt.title(\"Non Disaster Bigram BarGraph\")\nplt.tight_layout()\nplt.show()","8ccb78af":"plt.subplots(1,2,figsize=(15,15))\nplt.subplot(1,2,1)\n#Uigram Frequency distribution for disaster hashtags\n#convert disaster hashtags into single string\ntxt=' '.join(train_df[train_df[\"target\"]==1][\"hashtags\"])\ndisaster_unigram_hash=nltk.FreqDist(nltk.word_tokenize(txt))\n\n#visualize unigram frequency distribution for disaster hashtags using wordcloud\ndisaster_wc = WordCloud(width=800, height=400, max_words=100).generate_from_frequencies(disaster_unigram_hash)\nplt.title(\"Disaster Unigram Frequency Distribution hashtags\")\nplt.imshow(disaster_wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\n\nplt.subplot(1,2,2)\n#Uigram Frequency distribution for non disaster hashtags\n#convert non disaster hashtags into single string\ntxt=' '.join(train_df[train_df[\"target\"]==0][\"hashtags\"])\nnondisaster_unigram_hash=nltk.FreqDist(nltk.word_tokenize(txt))\n\n#visualize unigram frequency distribution for non disaster hashtags using wordcloud\nnondisaster_wc = WordCloud(width=800, height=400, max_words=100).generate_from_frequencies(nondisaster_unigram_hash)\nplt.title(\"Non Disaster Unigram Frequency Distribution hashtags\")\nplt.axis(\"off\")\nplt.imshow(nondisaster_wc, interpolation=\"bilinear\")\nplt.show()","3c63c722":"# Word Embedding\nfrom gensim.models import KeyedVectors\n# Keras\nfrom keras import optimizers\nfrom keras.models import Model, Sequential\nfrom keras.layers import Dense, Input, Embedding, Dropout\nfrom keras.layers import GlobalMaxPool1D, MaxPooling1D, GlobalMaxPooling1D\nfrom keras.layers import LSTM, Bidirectional\nfrom keras.layers.convolutional import Conv1D\nfrom keras.utils import plot_model\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# Visualization\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom keras.utils import plot_model\n# Measuring metrics\nfrom sklearn.metrics import f1_score","72fff26f":"# Downloading fastext\n!wget https:\/\/dl.fbaipublicfiles.com\/fasttext\/vectors-wiki\/wiki.en.vec","f89320c0":"train = train_df[['id','text','clean_text_stopword','target']]\ntrain.rename(columns = {'clean_text_stopword':'clean_text'},inplace = True)","3d30b0db":"train.head(2)","9447827d":"test = test_df[['id','text','clean_text_stopword']]\ntest.rename(columns = {'clean_text_stopword':'clean_text'},inplace = True)","39e2171e":"test.head(2)","af6ab1fa":"x_test = test['clean_text']\nprint('Number of testing sentence: ', x_test.shape)\nx_test = np.asarray(x_test)","adfd8f26":"x_train = train['clean_text']\ny_train = train['target']","d5cb7e97":"x_train = np.asarray(x_train)\ny_train = np.asarray(y_train)","73e3e7e8":"print('Number of training sentence: ', x_train.shape)\nprint('Number of training label: ', y_train.shape)","64cb04e6":"x_train = np.asarray(x_train)\ny_train = np.asarray(y_train)","3cb5c54d":"# See the data number of sentence in each category \nfrom collections import Counter\ncnt = Counter(y_train)\ncnt = dict(cnt)\nprint(cnt)","77ea7d66":"labels = list(cnt.keys())\nsizes = list(cnt.values())\ncolors = ['#3fba36', '#66b3ff']\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, labels=labels, colors=colors,\n        autopct='%1.1f%%', startangle=90)\n#draw circle\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nfig = plt.gcf()\nfig.gca().add_artist(centre_circle)\n# Equal aspect ratio ensures that pie is drawn as a circle\nax1.axis('equal')  \nplt.tight_layout()\n# Decomment following line if you want to save the figure\n# plt.savefig('distribution.png')\nplt.show()","ce20d5c4":"EMBEDDING_FILE = 'wiki.en.vec'\n\ndef import_with_gensim(file_address):\n    # Creating the model\n    ft_model = KeyedVectors.load_word2vec_format(file_address)\n    # Getting the tokens\n    ft_words = []\n    for ft_word in ft_model.index_to_key:\n        ft_words.append(ft_word)\n    return ft_model, ft_words\n  \nft_model, ft_words = import_with_gensim(EMBEDDING_FILE)","a341cd9c":"# FastText embedding dimensionality\nembed_size = 300","f16d6516":"# We get the mean and standard deviation of the embedding weights so that we could maintain the\n# same statistics for the rest of our own random generated weights.\n\nembedding_list = list()\n\nfor w in ft_words:\n    embedding_list.append(ft_model[w])\n\nall_embedding = np.stack(embedding_list)\nemb_mean, emb_std = all_embedding.mean(), all_embedding.std()","1054296b":"num_words = 2500\n\n# Create the tokenizer\ntokenizer = Tokenizer()\n\n# fFt the tokenizer on the training documents\ntokenizer.fit_on_texts(x_train)","90799508":"# Find maximum length of training sentences\nmax_length = max([len(s.split()) for s in x_train])","f7693e36":"# Embed training sequences\nencoded_docs = tokenizer.texts_to_sequences(x_train)\n\n# Pad embeded training sequences\nx_train_padded = pad_sequences(encoded_docs, maxlen=max_length, padding='post')","b69908d4":"# Define vocabulary size (largest integer value)\nvocab_size = len(tokenizer.word_index)+1","01558f75":"# We are going to set the embedding size to the pre-trained dimension as we are replicating it\nnb_words = len(tokenizer.word_index)+1\n\n# the size will be Number of Words in Vocab X Embedding Size\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n\n# With the newly created embedding matrix, we'll fill it up with the words that we have in both\n# our own dictionary and loaded pre-trained embedding.\nembeddedCount = 0\nfor word, i in tokenizer.word_index.items():\n    i -= 1\n    # then we see if this word is in glove's dictionary, if yes, get the corresponding weights\n    if word in ft_model.index_to_key:\n        embedding_vector = ft_model[word]\n        # and store inside the embedding matrix that we will train later on.\n        embedding_matrix[i] = embedding_vector\n        embeddedCount += 1\n    else:   # Unknown words\n        embedding_vector = ft_model['subdivision_name']\n        embedding_matrix[i] = embedding_vector\n        embeddedCount += 1\n\nprint('total embedded:', embeddedCount, 'common words')\nprint('Embedding matrix shape:', embedding_matrix.shape)","c2aadea9":"# Embed testing sequences\nencoded_docs = tokenizer.texts_to_sequences(x_test)\n# Pad testing sequences\nx_test_padded = pad_sequences(encoded_docs, maxlen=max_length, padding='post')","8d01372b":"from sklearn.model_selection import train_test_split\nX_train, X_val, Y_train, Y_val = train_test_split(x_train_padded, y_train, test_size=0.2, random_state=2)","cbef4f0f":"print(\"Train set has total {0} entries with {1:.2f}% disaster, {2:.2f}% neutral\".format(len(X_train),\n                                                                             (len(X_train[Y_train == 0]) \/ (len(X_train)*1.))*100,\n                                                                            (len(X_train[Y_train == 1]) \/ (len(X_train)*1.))*100))\nprint(\"Validation set has total {0} entries with {1:.2f}% disaster, {2:.2f}% neutral\".format(len(X_val),\n                                                                             (len(X_val[Y_val == 0]) \/ (len(X_val)*1.))*100,\n                                                                            (len(X_val[Y_val == 1]) \/ (len(X_val)*1.))*100))","e9b180f1":"model_blstm_fast = Sequential()\nmodel_blstm_fast.add(Embedding(vocab_size, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False))\nmodel_blstm_fast.add(Bidirectional(LSTM(300, return_sequences=True, name='lstm_layer')))\nmodel_blstm_fast.add(GlobalMaxPool1D())\nmodel_blstm_fast.add(Dropout(0.1))\nmodel_blstm_fast.add(Dense(300, activation=\"relu\"))\nmodel_blstm_fast.add(Dropout(0.1))\nmodel_blstm_fast.add(Dense(1, activation='sigmoid'))","925a6e68":"model_blstm_fast.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nmodel_blstm_fast.summary()\nbatch_size_blstm = 32\nepochs_blstm = 5","3542a5cb":"# Train model\nhist_blstm_fast = model_blstm_fast.fit(X_train, Y_train, batch_size=batch_size_blstm, epochs=epochs_blstm,\n                             validation_data = (X_val, Y_val))","2c034053":"# Evaluate model\nloss_blstm_fast, acc_blstm_fast = model_blstm_fast.evaluate(X_val, Y_val, verbose=0)\nprint('Test Accuracy: %f' % (acc_blstm_fast*100))","e9630ab6":"# Get prediction label\ny_pred_val_blstm_fast = model_blstm_fast.predict_classes(X_val)","82bb1c9f":"# Get prediction label\ny_pred_blstm_fast = model_blstm_fast.predict_classes(x_test_padded)","714d5dd6":"model_cnn_fast = Sequential()\nmodel_cnn_fast.add(Embedding(vocab_size, embedding_matrix.shape[1], weights=[embedding_matrix], trainable=False))\nmodel_cnn_fast.add(Conv1D(filters=64, kernel_size=4, activation='relu', padding='same'))\nmodel_cnn_fast.add(MaxPooling1D(pool_size=2))\nmodel_cnn_fast.add(Conv1D(filters=64, kernel_size=8, activation='relu', padding='same'))\nmodel_cnn_fast.add(MaxPooling1D(pool_size=2))\nmodel_cnn_fast.add(Conv1D(filters=64, kernel_size=16, activation='relu', padding='same'))\nmodel_cnn_fast.add(GlobalMaxPooling1D())\nmodel_cnn_fast.add(Dropout(0.1))\nmodel_cnn_fast.add(Dense(500, activation=\"sigmoid\"))\nmodel_cnn_fast.add(Dense(1, activation='sigmoid'))","47799b51":"model_cnn_fast.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nmodel_cnn_fast.summary()\nbatch_size_cnn = 64\nepochs_cnn = 10","2e2aa65d":"# Train model\nhist_cnn_fast = model_cnn_fast.fit(X_train, Y_train, batch_size=batch_size_cnn, epochs=epochs_cnn,\n                        validation_data = (X_val, Y_val))","55e4c3e0":"# Evaluate model\nloss_cnn_fast, acc_cnn_fast = model_cnn_fast.evaluate(X_val, Y_val, verbose=0)\nprint('Test Accuracy: %f' % (acc_cnn_fast*100))","7de1a3d8":"y_pred_val_cnn_fast = model_cnn_fast.predict_classes(X_val)","eae1280a":"# Get prediction label\ny_pred_cnn_fast = model_cnn_fast.predict_classes(x_test_padded)","1e84cff1":"def model_evaluation(model):\n    t_loss = model.history['loss']\n    t_acc  = model.history['accuracy']\n    v_loss = model.history['val_loss']\n    v_acc  = model.history['val_accuracy']\n    x_axis = len(t_loss)\n\n    fig,(ax1) = plt.subplots(1,1,figsize=(12,8))\n    ax1.plot(t_acc,color = 'blue',label = 'Train')\n    ax1.plot(v_acc,color = 'orange',label = 'Val')\n    ax1.set_title('Accuracy Plot')\n    ax1.set_xlabel(\"#Epochs\")\n    ax1.set_ylabel(\"Accuracy\")\n    ax1.legend()\n    \n    plt.show()\n","95c723df":"model_evaluation(hist_blstm_fast)","dd12a86d":"model_evaluation(hist_cnn_fast)","6c768659":"from sklearn.metrics import confusion_matrix\nfrom sklearn.utils.multiclass import unique_labels\n\ndef plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = classes[unique_labels(y_true, y_pred)]\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    \n    print(cm)\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    print(im)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\nclass_names = np.array([0, 1])\nnp.set_printoptions(precision=2)","92f50026":"# Plot non-normalized confusion matrix\nplot_confusion_matrix(Y_val, y_pred_val_blstm_fast, classes=class_names)\n# plt.savefig('cm-blstm.png')\n# Plot normalized confusion matrix\nplot_confusion_matrix(Y_val, y_pred_val_blstm_fast, classes=class_names, normalize=True)\n# Decomment following line if you want to save the figure\n# plt.savefig('cm-blstm-normalized.png')\nplt.show()","1f284377":"# Plot non-normalized confusion matrix\nplot_confusion_matrix(Y_val, y_pred_val_cnn_fast, classes=class_names)\n# plt.savefig('cm-cnn.png')\n# Plot normalized confusion matrix\nplot_confusion_matrix(Y_val, y_pred_val_cnn_fast, classes=class_names, normalize=True)\n# plt.savefig('cm-cnn-normalized.png')\nplt.show()","219a1b79":"print(\"(Weighted) F1 score of FasttextEmb B-LSTM model:\")\nf1_score(Y_val, y_pred_val_blstm_fast, average='weighted')","e75e187f":"print(\"(Weighted) F1 score of FasttextEmb B-LSTM model:\")\nf1_score(Y_val, y_pred_val_cnn_fast, average='weighted')","0d056bcd":"model_blstm_keras = Sequential()\nmodel_blstm_keras.add(Embedding(vocab_size, 300, input_length=max_length))\nmodel_blstm_keras.add(Bidirectional(LSTM(300, return_sequences=True, name='lstm_layer')))\nmodel_blstm_keras.add(GlobalMaxPool1D())\nmodel_blstm_keras.add(Dropout(0.3))\nmodel_blstm_keras.add(Dense(300, activation=\"relu\"))\nmodel_blstm_keras.add(Dropout(0.3))\nmodel_blstm_keras.add(Dense(300, activation=\"relu\"))\nmodel_blstm_keras.add(Dropout(0.3))\nmodel_blstm_keras.add(Dense(1, activation='sigmoid'))","67adee8f":"model_blstm_keras.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nmodel_blstm_keras.summary()\nbatch_size_blstm = 32\nepochs_blstm = 5","caad669f":"# Train model\nhist_blstm_keras = model_blstm_keras.fit(X_train, Y_train, batch_size=batch_size_blstm, epochs=epochs_blstm,\n                             validation_data = (X_val, Y_val))","10420cbf":"# Evaluate model\nloss_blstm_keras, acc_blstm_keras = model_blstm_keras.evaluate(X_val, Y_val, verbose=0)\nprint('Test Accuracy: %f' % (acc_blstm_keras*100))","49713022":"# Get prediction label\ny_pred_val_blstm_keras = model_blstm_keras.predict_classes(X_val)","a5879923":"# Get prediction label\ny_pred_blstm_keras = model_blstm_keras.predict_classes(x_test_padded)","89568a86":"model_evaluation(hist_blstm_keras)","58e05a55":"model_cnn_keras = Sequential()\nmodel_cnn_keras.add(Embedding(vocab_size, 300, input_length=max_length))\nmodel_cnn_keras.add(Conv1D(filters=64, kernel_size=4, activation='relu', padding='same'))\nmodel_cnn_keras.add(MaxPooling1D(pool_size=2))\nmodel_cnn_keras.add(Conv1D(filters=64, kernel_size=8, activation='relu', padding='same'))\nmodel_cnn_keras.add(MaxPooling1D(pool_size=2))\nmodel_cnn_keras.add(Conv1D(filters=64, kernel_size=16, activation='relu', padding='same'))\nmodel_cnn_keras.add(GlobalMaxPooling1D())\nmodel_cnn_keras.add(Dropout(0.1))\nmodel_cnn_keras.add(Dense(500, activation=\"sigmoid\"))\nmodel_cnn_keras.add(Dense(1, activation='sigmoid'))","47061ccb":"\nmodel_cnn_keras.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])\n\nmodel_cnn_keras.summary()\nbatch_size_cnn = 64\nepochs_cnn = 10","1bcaa533":"# Train model\nhist_cnn_keras = model_cnn_keras.fit(X_train, Y_train, batch_size=batch_size_cnn, epochs=epochs_cnn,\n                        validation_data = (X_val, Y_val))","0c395994":"# Evaluate model\nloss_cnn_keras, acc_cnn_keras = model_cnn_keras.evaluate(X_val, Y_val, verbose=0)\nprint('Test Accuracy: %f' % (acc_cnn_keras*100))","95d18541":"y_pred_val_cnn_keras= model_cnn_keras.predict_classes(X_val)","c78fa187":"# Get prediction label\ny_pred_cnn_keras = model_cnn_keras.predict_classes(x_test_padded)","afee18d9":"model_evaluation(hist_cnn_keras)","17c948e7":"# Plot non-normalized confusion matrix\nplot_confusion_matrix(Y_val, y_pred_val_blstm_keras, classes=class_names)\n# plt.savefig('cm-blstm.png')\n# Plot normalized confusion matrix\nplot_confusion_matrix(Y_val, y_pred_val_blstm_keras, classes=class_names, normalize=True)\n# Decomment following line if you want to save the figure\n# plt.savefig('cm-blstm-normalized.png')\nplt.show()","828348c8":"# Plot non-normalized confusion matrix\nplot_confusion_matrix(Y_val, y_pred_val_cnn_keras, classes=class_names)\n# plt.savefig('cm-cnn.png')\n# Plot normalized confusion matrix\nplot_confusion_matrix(Y_val, y_pred_val_cnn_keras, classes=class_names, normalize=True)\n# plt.savefig('cm-cnn-normalized.png')\nplt.show()","11376e6a":"print(\"(Weighted) F1 score of KerasEmb B-LSTM model:\")\nf1_score(Y_val, y_pred_val_blstm_keras, average='weighted')","f281b03f":"print(\"(Weighted) F1 score of KerasEmb CNN model:\")\nf1_score(Y_val, y_pred_val_cnn_keras, average='weighted')","58a82fcc":"# sklearn\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.pipeline import Pipeline\n\n# Measuring metrics\nfrom sklearn.metrics import f1_score\nfrom nltk.tokenize import word_tokenize","88599d10":"# When building the vocabulary ignore terms that have a document frequency strictly lower than\n# the given threshold. This value is also called cut-off in the literature.\nmin_df = 1\n\n# Tokenize function used in Vectorizer\ndef tokenize(text):\n    return word_tokenize(text)","849e4edb":"train.head(2)","b96bccdf":"test.head(2)","7e9abf3c":"x_test = test['clean_text']\nprint('Number of testing sentence: ', x_test.shape)\nx_test = np.asarray(x_test)","3fff36b3":"x_train = np.asarray(train['clean_text'])\ny_train = np.asarray(train['target'])\nprint('Number of training sentence: ', x_train.shape)\nprint('Number of training label: ', y_train.shape)","01b0eec0":"X_train, X_val, Y_train, Y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=2)","75c91413":"# Naive Bayes Model\nnaive_bayes = Pipeline([('vect', CountVectorizer(tokenizer=tokenize,\n                                              analyzer='word', ngram_range=(1, 2), min_df=min_df, lowercase=False)),\n                     ('tfidf', TfidfTransformer(sublinear_tf=True)),\n                     ('clf', MultinomialNB())])\nnaive_bayes = naive_bayes.fit(X_train, Y_train)\nnaive_score = naive_bayes.score(X_val, Y_val)\nprint('Naive Bayes Model: ', naive_score)\npredict_val_nb = naive_bayes.predict(X_val)","be88aec6":"predict_nb = naive_bayes.predict(x_test)","c12aab1b":"# Linear Support Vector Machine Model\nsvm = Pipeline([('vect', CountVectorizer(tokenizer=tokenize,\n                                                         analyzer='word', ngram_range=(1, 2),\n                                                         min_df=min_df, lowercase=False)),\n                                ('tfidf', TfidfTransformer(sublinear_tf=True)),\n                                ('clf-svm', LinearSVC(loss='hinge', penalty='l2',\n                                                      max_iter=5))])\n\nsvm = svm.fit(X_train, Y_train)\nlinear_svc_score = svm.score(X_val, Y_val)\nprint('Linear SVC Model: ', linear_svc_score)\npredict_val_svm = svm.predict(X_val)","e46f7bbe":"predict_svm = svm.predict(x_test)","7e9d43a9":"# SGD (Stochastic Gradient Descent) Model\nsgd = Pipeline([('vect', CountVectorizer(tokenizer=tokenize,\n                                                  analyzer='word', ngram_range=(1, 2), min_df=min_df, lowercase=False)),\n                         ('tfidf', TfidfTransformer(sublinear_tf=True)),\n                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2',\n                                                   alpha=1e-3, max_iter=5))])\nsgd = sgd.fit(X_train, Y_train)\nsgd_score = sgd.score(X_val, Y_val)\nprint('SGD Model: ', sgd_score)\npredict_val_sgd = sgd.predict(X_val)","c0b79dfc":"predict_sgd = sgd.predict(x_test)","08495459":"Y_val = Y_val.astype(int)\npredict_val_nb = predict_val_nb.astype(int)\npredict_val_svm = predict_val_svm.astype(int)\npredict_val_sgd = predict_val_sgd.astype(int)","70c025ce":"# Plot non-normalized confusion matrix\nplot_confusion_matrix(Y_val, predict_val_nb, classes=class_names)\n# plt.savefig('cm-nb.png')\n# Plot normalized confusion matrix\nplot_confusion_matrix(Y_val, predict_val_nb, classes=class_names, normalize=True)\n# plt.savefig('cm-nb-normalized.png')\nplt.show()","65182267":"# Plot non-normalized confusion matrix\nplot_confusion_matrix(Y_val, predict_val_svm, classes=class_names)\n# plt.savefig('cm-svm.png')\n# Plot normalized confusion matrix\nplot_confusion_matrix(Y_val, predict_val_svm, classes=class_names, normalize=True)\n# plt.savefig('cm-svm-normalized.png')\nplt.show()","70a3dccf":"# Plot non-normalized confusion matrix\nplot_confusion_matrix(Y_val, predict_val_sgd, classes=class_names)\n# plt.savefig('cm-sgd.png')\n# Plot normalized confusion matrix\nplot_confusion_matrix(Y_val, predict_val_sgd, classes=class_names, normalize=True)\n# plt.savefig('cm-sgd-normalized.png')\nplt.show()","6427fd10":"print(\"F1 score of NB model:\")\nf1_score(Y_val, predict_val_nb, average='weighted')","29218918":"print(\"F1 score of SVM model:\")\nf1_score(Y_val, predict_val_svm, average='weighted')","21476b92":"print(\"F1 score of SGD model:\")\nf1_score(Y_val, predict_val_sgd, average='weighted')","93aa1717":"train.head(2)","d2b76963":"test.head(2)","ec2b6f82":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport lightgbm as lgb\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import Perceptron\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport re\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.metrics import roc_auc_score #Compute Area Under the Curve (AUC) from prediction scores\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom warnings import filterwarnings\nfrom sklearn.model_selection import cross_validate\n\nfrom sklearn.pipeline import Pipeline","1792ad0e":"sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n\nX = train['clean_text'].to_numpy()\ny = train['target'].to_numpy()\n\nfor train_index, test_index in sss.split(X, y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    #X_train = X.loc[train_index]\n    X_train, X_test = X[train_index], X[test_index]\n\n    y_train, y_test = y[train_index], y[test_index]","0f66161c":"plt.figure(figsize=(2.5,5))\nplt.title(\"Distribution in Train dataset\")\np1 = sns.countplot(y_train, palette = 'plasma')\n\nfor p in p1.patches:\n        p1.annotate('{:6.2f}%'.format(p.get_height()\/len(y_train)*100), (p.get_x()+0.1, p.get_height()+50))\n        \nplt.show()","072c5102":"plt.figure(figsize=(2.5,5))\nplt.title(\"Distribution in Test dataset\")\np1 = sns.countplot(y_test, palette = 'plasma')\n\nfor p in p1.patches:\n        p1.annotate('{:6.2f}%'.format(p.get_height()\/len(y_test)*100), (p.get_x()+0.2, p.get_height()+12))\n        \nplt.show()","2f8250da":"tweets_pipeline = Pipeline([('CVec', CountVectorizer(stop_words='english')),\n                     ('Tfidf', TfidfTransformer())])\n\nX_train_tranformed = tweets_pipeline.fit_transform(X_train)\nX_test_tranformed = tweets_pipeline.transform(X_test)","01a6a6e1":"classifiers = {\n    \"Logistic Regression\": LogisticRegression(class_weight='balanced'),\n    \"Decision Tree\": DecisionTreeClassifier(class_weight='balanced'),\n    \"k-Nearest Neighbors\": KNeighborsClassifier(),\n    \"Linear SVM\": SVC(class_weight='balanced'),\n    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n    \"Random Forest\": RandomForestClassifier(),\n    'RidgeClassifier': RidgeClassifier(class_weight='balanced'),\n    'AdaBoost': AdaBoostClassifier(n_estimators=100),\n    'MNB': MultinomialNB(),\n    'Perceptron': Perceptron(class_weight='balanced'),\n    'xgboost': XGBClassifier(n_estimators=300),\n    'catboost': CatBoostClassifier(verbose=0)\n        \n}","ffdbe613":"no_classifiers = len(classifiers.keys())\n\nfrom time import process_time \n\n\ndef batch_classify(X_train_tranformed, y_train, X_test_tranformed, y_test, verbose = True):\n    df_results = pd.DataFrame(data=np.zeros(shape=(no_classifiers,3)), columns = ['Classifier', 'Area Under Curve', 'Training time'])\n    count = 0\n    for key, classifier in classifiers.items():\n        t_start = process_time()  \n        classifier.fit(X_train_tranformed, y_train)\n        t_stop = process_time() \n        t_elapsed = t_stop - t_start\n        y_predicted = classifier.predict(X_test_tranformed)\n        \n        df_results.loc[count,'Classifier'] = key\n        df_results.loc[count,'Area Under Curve'] = roc_auc_score(y_test, y_predicted)\n        df_results.loc[count,'Training time'] = t_elapsed\n        if verbose:\n            print(\"trained {c} in {f:.2f} s\".format(c=key, f=t_elapsed))\n        count+=1\n\n    return df_results","0c5e465a":"df_results = batch_classify(X_train_tranformed, y_train,X_test_tranformed, y_test)\nprint(df_results.sort_values(by='Area Under Curve', ascending=False))","b8461c7b":"## Neural Network","b0392c1b":"Location includes `1105` null values and`26` keywords are null in test dataset.","50e8621a":"# Prepare FastText Model","d03914c9":"## B-LSTM Model","6281d64e":"## Setting tokenizer up","5787d32e":"Adding length of the tweets before cleaning to the dataframe.","004d60cb":"### BLSTM","2d3fda88":"### Stochastic Gradient Descent Model","015e6acb":"### Naive Bayes","e0499362":"### Linear Support Vector Machine Model","4a8cafcd":"# CNN Model","947ddb9e":"As you can see there are more data points with the label 0 meaning tweets that are not disaster tweets and fewer data points with the label 1 which is tweets that are related to a disaster. Usually, for data that has some skewed labels, it is recommended to use an F-score instead of accuracy for model evaluation.","ceb997c1":"The order of the cleaning is\n1. Souping\n2. BOM removing\n3. Url address(\u2018http:\u2019pattern), twitter ID removing\n4. Url address(\u2018www.'pattern) removing\n5. Lower-case\n6. Negation handling\n7. Removing numbers and special characters\n8. Tokenizing and joining","0e2e18c0":"# Confusion Matrix","f66793e3":"Now we can take a look at the distribution of `Keywords` and `Locations`. We plot 20 most repeated values for each.","39c2a4f7":"#### Visualize unigram frequency distribution for disaster hashtags","fc8b310d":"#### Extracting hashtags from tweets","6fb3fbe8":"We do the same for test dataset.","29ab1086":"## Model Evaluation and Confusion Matrix","9010c513":"Taking care of Stopwords","47cc8ead":"# Try other models","de3e1710":"# Prepare data for Deep Learning model","ccf49169":"# fastText word embeddings","673c0d81":"# Machine Learning Algorithms","c44093e4":"## Word Cloud\nA word cloud represents word usage in a document by resizing individual words proportionally to its frequency and then presenting them in a random arrangement. ","526fa6df":"## Exploratory Data Analysis","73dbe675":"## CNN Model\n","6fe0d5c8":"## Keras embedding","d65ea437":"I do some further analysis on number of words and the length of the tweet by calculating the average word length of the tweets","7940206f":"Now we can examine if the neutral and disaster tweets, spread equally in our dataset","5945f183":"We can also plot Keywords and Location for different categories.","9c2d4096":"Now I plot the target value distribution","b4866130":"Location includes `2533` null values and `61` values for keywords are null in train dataset.","05e7d578":"#### Taking care of duplicate values.","4bed3c75":"We can delete the values for these two columns as they seem not neccessary","d6c654ed":"### Visualize Bigram frequency distribution of tweets","d0219811":"#### Plot Disaster tweets wordcloud","be0d48ed":"#### Taking care of null values","0cde9bfa":"## Confusion Matrix","0d8863bd":"# F1 Score","32187c75":"The test data set is also left skewed.","2059c970":"## Embed sentences","f93b4381":"#### Plot Neutral tweets wordcloud","bd5eae73":"## Preprocessing","b89319e5":"It can be seen in above plot that the character distribution is left skewed.\nwhat about the test dataset?","8f3ec883":"It seems that number of words follow a normal distribution.As we can see the majority of tweets are between 11 to 19 words.","c1995383":"### Train and Validation split","be688156":"This train dataset consists of the following features:\n- **Id:** a numerical identifier for the tweet. This will be important when we upload our predictions to the leaderboard.\n- **Keyword:** a keyword from the tweet which may in some cases be missing.\n- **Location:** the location the tweet was sent from. This may also not be present.\n- **Text:** the full text of the tweet.\n- **Target:** this is the label we are trying to predict. This will be 1 if the tweet is really about a disaster and 0 if not."}}