{"cell_type":{"5e2643ff":"code","a9fa14fa":"code","b723ece6":"code","cd1eaa03":"code","e4de2f74":"code","36d5b0fd":"code","13918298":"code","486e56e7":"code","f5d71d1f":"code","7620dd07":"code","30f3e6fb":"code","1668ab0c":"code","621cbafe":"code","b9e3dc1e":"code","bcddc563":"code","cf15d7d3":"code","848fd131":"code","bb2bb3df":"code","cc3f912f":"code","dcf8a473":"code","b31f6114":"code","5b3db9cb":"code","771f3c4f":"code","4862cf5e":"code","b77d2f0f":"code","585e3a40":"code","b66434d3":"code","a212c89f":"code","dff71ce2":"code","8d6d5d80":"code","831a0f74":"code","c797319d":"code","eda96d71":"code","d6c4d1e7":"code","881b5117":"code","ac282e44":"code","c9835b30":"markdown","02d93e27":"markdown","7056949d":"markdown","7b457a2b":"markdown","307e3bba":"markdown","984cb17e":"markdown","64ce9d31":"markdown","03adc48b":"markdown"},"source":{"5e2643ff":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport cv2\nfrom IPython.display import Image","a9fa14fa":"def show(img):\n    return (Image(cv2.imencode(\".png\",img)[1].tobytes()))","b723ece6":"base_path = '\/kaggle\/input\/houseroom\/House_Room_Dataset\/'\nfolders = os.listdir(base_path)\nprint(folders)\nfolders = ['seg_train', 'seg_pred', 'seg_test']","cd1eaa03":"base_path+folders[0]+'\/'","e4de2f74":"train_folders = os.listdir(os.path.join(base_path,folders[0],folders[0]))\nprint(f'train_folders: {train_folders}')\nprint('----Train Data Distribution-----')\nprint(f'Folder Name : No. of Images')\nfor folder in train_folders:\n    print(f'{folder:11} : {len(os.listdir(os.path.join(base_path,folders[0],folders[0],folder)))}')","36d5b0fd":"val_folders = os.listdir(os.path.join(base_path,folders[2],folders[2]))\nprint(f'val_folders: {val_folders}')\nprint('----Val Data Distribution-----')\nprint(f'Folder Name : No. of Images')\nfor folder in val_folders:\n    print(f'{folder:11} : {len(os.listdir(os.path.join(base_path,folders[2],folders[2],folder)))}')","13918298":"test_images = os.listdir(os.path.join(base_path,folders[1],folders[1]))\nprint(f'test_images: {len(test_images)}')\nprint('First ten Images')\nprint(test_images[:10])","486e56e7":"!pip install -U efficientnet","f5d71d1f":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n# from keras.applications.inception_v3 import InceptionV3\n# from tensorflow.keras.applications import EfficientNetB2\nimport efficientnet.keras as efn \n# model = efn.EfficientNetB0(weights='imagenet') \nfrom keras import Model, layers\nfrom keras.layers import GlobalAveragePooling2D, Dropout, Dense, Input\n\nprint(\"Libraries Imported!\")","7620dd07":"import tensorflow as tf\nprint(tf.__version__)","30f3e6fb":"train_DIR = \"\/kaggle\/input\/houseroom\/House_Room_Dataset\/seg_train\/seg_train\/\"\n\ntrain_datagen = ImageDataGenerator( rescale = 1.0\/255,\n                                          width_shift_range=0.2,\n                                          height_shift_range=0.2,\n                                          zoom_range=0.2,\n                                          vertical_flip=True,\n                                          fill_mode='nearest')\n\n\ntrain_generator = train_datagen.flow_from_directory(train_DIR,\n                                                    batch_size=32,\n                                                    class_mode='categorical',\n                                                    target_size=(250, 250))\n\ntest_DIR = \"\/kaggle\/input\/houseroom\/House_Room_Dataset\/seg_test\/seg_test\/\"\nvalidation_datagen = ImageDataGenerator(rescale = 1.0\/255)\n\n\nvalidation_generator = validation_datagen.flow_from_directory(test_DIR,\n                                                    batch_size=128,\n                                                    class_mode='categorical',\n                                                    target_size=(250, 250))","1668ab0c":"print(validation_generator.class_indices)\nclass2index = validation_generator.class_indices\n\nindex2class = {v: k for k, v in class2index.items()}\nprint(index2class)","621cbafe":"from tensorflow.keras.applications import VGG16, InceptionResNetV2\nfrom tensorflow.keras.optimizers import Adam,RMSprop,SGD,Adamax\nfrom tensorflow.keras.layers import (\n    Input, Dense, Conv2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D,\n    Flatten, Activation, GlobalAveragePooling2D, GlobalMaxPooling2D, add)\n# from keras.layers.normalization import BatchNormalization\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import initializers\n# from keras.engine import Layer, InputSpec\nfrom tensorflow.keras.layers import Layer, InputSpec\n# from keras.engine.topology import get_source_inputs\nfrom tensorflow.keras.utils import get_source_inputs\n\nIMG_SIZE=250\nepochs = 30\nbatch_size = 32","b9e3dc1e":"model = tf.keras.Sequential()\nmodel.add(VGG16(include_top = False,weights = 'imagenet',input_shape= (IMG_SIZE,IMG_SIZE,3)))\nmodel.add(Flatten())\nmodel.add(Dense(5,activation = 'softmax'))","bcddc563":"model.summary()","cf15d7d3":"optimizer = Adam(lr = 0.0001)\nmodel.compile(loss = \"categorical_crossentropy\",optimizer = optimizer,metrics = ['accuracy'])","848fd131":"history = model.fit(train_generator, epochs = epochs,validation_data = validation_generator, steps_per_epoch=len(train_generator),\n  validation_steps=len(validation_generator))","bb2bb3df":"fig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\nfig.set_size_inches(12,4)\n\nax[0].plot(history.history['accuracy'])\nax[0].plot(history.history['val_accuracy'])\nax[0].set_title('Training Accuracy vs Validation Accuracy')\nax[0].set_ylabel('Accuracy')\nax[0].set_xlabel('Epoch')\nax[0].legend(['Train', 'Validation'], loc='upper left')\n\nax[1].plot(history.history['loss'])\nax[1].plot(history.history['val_loss'])\nax[1].set_title('Training Loss vs Validation Loss')\nax[1].set_ylabel('Loss')\nax[1].set_xlabel('Epoch')\nax[1].legend(['Train', 'Validation'], loc='upper left')\n\nplt.show()","cc3f912f":"model.save('messy_clean_model.h5')","dcf8a473":"from keras.applications.vgg16 import preprocess_input","b31f6114":"from keras.models import load_model\nfrom keras.applications.vgg16 import preprocess_input","5b3db9cb":"import requests as r\nimport os\ndef loader(url):\n  response = r.get('https:\/\/daniilak.ru\/photoDown\/photos\/'+url+'.jpg')\n  file = open(url+'.jpg', \"wb\")\n  file.write(response.content)\n  file.close()\nfor i in range(0,5):\n  loader(str(i))","771f3c4f":"from keras.preprocessing import image","4862cf5e":"model = load_model('messy_clean_model.h5')\n","b77d2f0f":"img = image.load_img('0.jpg', target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nimg_data = preprocess_input(x)\nclasses = model.predict(img_data)\nprint(classes, index2class[np.argmax(classes)])","585e3a40":"img = image.load_img('1.jpg', target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nimg_data = preprocess_input(x)\nclasses = model.predict(img_data)\nprint(classes, index2class[np.argmax(classes)])","b66434d3":"img = image.load_img('2.jpg', target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nimg_data = preprocess_input(x)\nclasses = model.predict(img_data)\nprint(classes, index2class[np.argmax(classes)])","a212c89f":"img = image.load_img('3.jpg', target_size=(224, 224))\nx = image.img_to_array(img)\nx = np.expand_dims(x, axis=0)\nimg_data = preprocess_input(x)\nclasses = model.predict(img_data)\nprint(classes, index2class[np.argmax(classes)])","dff71ce2":"efficientNet = efn.EfficientNetB1(weights='imagenet')\n\n# for layer in efficientNet.layers:\n#     layer.trainable = False","8d6d5d80":"\nlast_output = efficientNet.layers[-1].output","831a0f74":"# x = tf.keras.layers.Flatten()(last_output)\nx = tf.keras.layers.Dense(units = 128, activation = tf.nn.relu)(last_output)\n# x = tf.keras.layers.Dropout(0.2)(x)\nx = tf.keras.layers.Dense  (5, activation = tf.nn.softmax)(x)\n\nmodel = tf.keras.Model( efficientNet.input, x)\n\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc',\n                                            patience=1,\n                                            verbose=1,\n                                            factor=0.25,\n                                            min_lr=0.000003)\n\nmodel.compile(loss = 'categorical_crossentropy', optimizer= tf.keras.optimizers.Adam(), metrics=['acc'])\n\n# model.summary()","c797319d":"history = model.fit(train_generator,\n                    epochs = 15,\n                    verbose = 1,\n                   validation_data = validation_generator,\n                   callbacks=[learning_rate_reduction])","eda96d71":"%matplotlib inline\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","d6c4d1e7":"print(validation_generator.class_indices)","881b5117":"import requests as r\nimport os\nresponse = r.get('https:\/\/daniilak.ru\/photoDown\/photos\/2.jpg')\nfile = open('2.jpg', \"wb\")\nfile.write(response.content)\nfile.close()\n# path = os.path.join(base_path,folders[1],folders[1],test_images[0])\n# path = '\/kaggle\/input\/houseroom\/House_Room_Dataset\/seg_pred\/seg_pred\/Livingroom'\nimg = cv2.resize(cv2.imread('2.jpg'),(250,250))\nimg.shape\nshow(img)\n\n","ac282e44":"image_prediction = np.argmax(model.predict(np.array([img])))\nprint(index2class[image_prediction])","c9835b30":"# Get Model","02d93e27":"# Import Libraries","7056949d":"**EfficientNet** is a lightweight convolutional neural network architecture achieving the state-of-the-art accuracy with an order of magnitude fewer parameters and FLOPS, on both ImageNet and five other commonly used transfer learning datasets.\n\n**EfficientNets** rely on AutoML and compound scaling to achieve superior performance without compromising resource efficiency. The AutoML Mobile framework has helped develop a mobile-size baseline network, EfficientNet-B0, which is then improved by the compound scaling method to obtain EfficientNet-B1 to B7.\n\n<img src='https:\/\/raw.githubusercontent.com\/tensorflow\/tpu\/master\/models\/official\/efficientnet\/g3doc\/flops.png' width=\"500\" height=\"600\">\n\nEfficientNets achieve state-of-the-art accuracy on ImageNet with an order of magnitude better efficiency:\n\n* In high-accuracy regime, EfficientNet-B7 achieves the state-of-the-art 84.4% top-1 \/ 97.1% top-5 accuracy on ImageNet with 66M parameters and 37B FLOPS. At the same time, the model is 8.4x smaller and 6.1x faster on CPU inference than the former leader, Gpipe.\n\n* In middle-accuracy regime, EfficientNet-B1 is 7.6x smaller and 5.7x faster on CPU inference than ResNet-152, with similar ImageNet accuracy.\n\n* Compared to the widely used ResNet-50, EfficientNet-B4 improves the top-1 accuracy from 76.3% of ResNet-50 to 82.6% (+6.3%), under similar FLOPS constraints.","7b457a2b":"# Visualize Model Perfomance","307e3bba":"# EDA","984cb17e":"# About EfficientNet","64ce9d31":"# Train","03adc48b":"# Read Data"}}