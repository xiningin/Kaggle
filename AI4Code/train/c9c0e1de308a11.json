{"cell_type":{"22b56d91":"code","00cf26c3":"code","19526061":"code","679fc516":"code","8978a6b2":"code","4590d52f":"code","f6b21b06":"code","76f6d6ea":"code","f52f61b0":"code","69fc3d1f":"code","44326c57":"code","c8f24503":"code","2ecb3d06":"code","3bbb246d":"code","f0b3e0c0":"code","1242a68e":"code","2c247fff":"code","4d95748c":"code","18af72ea":"code","0edff4f2":"code","81a5e7de":"code","c516c636":"code","ba93dd57":"code","f9203467":"code","0e552511":"code","ba378183":"code","5978263c":"code","110e8e87":"code","569a38e1":"code","194a77aa":"code","d986c7c0":"code","24529993":"code","b2991097":"code","305ecc34":"code","1e03a15d":"code","8bef1d5f":"code","1dd24706":"code","f3f7cea2":"code","8ca9c147":"code","35d7420b":"code","d38c748c":"code","bf4501af":"code","d1390864":"code","24576ecb":"code","dca9f443":"code","bb799c28":"code","f01c12c4":"code","a3c34450":"code","871c45c1":"code","2431380b":"code","b9a57969":"code","e093eb5b":"code","66404b74":"code","c2e26db0":"code","6e912e16":"code","eb6e2459":"code","a783b0c4":"code","2032f560":"code","3ca41d9e":"code","ae0f1c7a":"code","a7d83e71":"code","799128bf":"code","cbec5951":"code","6b9a2e2d":"markdown","50b8ed73":"markdown","201d2f8d":"markdown","782afbbd":"markdown","7469bd41":"markdown","6b67a661":"markdown","7e1d4e41":"markdown","4dfdb87a":"markdown","6ee9a356":"markdown","720824d7":"markdown","9f9e2cc6":"markdown","e185fa99":"markdown","9d37cb6c":"markdown","082a43e7":"markdown","4cbe16fc":"markdown","0c2c7ea0":"markdown","666dc236":"markdown","db94a1c9":"markdown","acf30e91":"markdown","9a7db5cf":"markdown","4df6db22":"markdown","4b11f78c":"markdown","07e85cf7":"markdown","7e5042db":"markdown","aeae492d":"markdown","b66e8de2":"markdown","56d47a22":"markdown","437cbb87":"markdown","2580ddfd":"markdown","583656cd":"markdown","e2212cee":"markdown","4495da7d":"markdown","5de109b3":"markdown","8f79d075":"markdown","d6361199":"markdown","f9924395":"markdown","ccba10f7":"markdown","2818e3ec":"markdown","b1c975c0":"markdown","0ec8c194":"markdown","b4b8867f":"markdown","47163769":"markdown","3d1e8f5e":"markdown","21cb49bb":"markdown","51debd44":"markdown","cd18191d":"markdown","fc1c05d6":"markdown"},"source":{"22b56d91":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","00cf26c3":"#pip install comet_ml","19526061":"# Import comet_ml in the top of your file\n#from comet_ml import Experiment\n\n#experiment = Experiment(api_key=\"1T5FVvyOGYzMIIYf6dyXqFLcJ\",\n                        #project_name=\"classification-predict\", workspace=\"juandreliebenberg\")","679fc516":"# Context manager allows logging og parameters\n\n#experiment.context_manager(\"validation\")","8978a6b2":"# URL where experiments can be found\n\n#experiment.url","4590d52f":"# Analysis Libraries\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\n\n# Visualisation Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image\n\n# Language Processsing Libraries\nimport nltk\n#nltk.download(['punkt','stopwords'])\n#nltk.download('vader_lexicon')\n#nltk.download('popular')\nfrom sklearn.utils import resample\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.tokenize import word_tokenize, TreebankWordTokenizer \nimport re\nimport string\nfrom nltk import SnowballStemmer\nimport spacy\n\n# ML Libraries\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,recall_score,precision_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import metrics\nfrom nltk import SnowballStemmer\n\n# Code for hiding seaborn warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f6b21b06":"X_df = pd.read_csv('\/kaggle\/input\/climate-change-belief-analysis\/train.csv')\nX_df.head(5)","76f6d6ea":"# Inspect structure of dataset\n\nX_df.info()","f52f61b0":"# Generate dataframe to indicate unique values\n\nnumber_of_unique=[X_df[i].nunique() for i in X_df.columns] # Number of unique values per column\ncolumn_names=[i for i in X_df.columns]\nunique_zip=list(zip(column_names,number_of_unique))\nunique_df=pd.DataFrame(unique_zip,columns=['Column_Feature','Unique_Values'])\nunique_df","69fc3d1f":"# A function to remove duplicate rows from the message column\n\ndef delete_dup(df):\n  df=df.copy()\n  df = df.drop_duplicates(subset='message') #messges specified as subset to evaluate\n  return df","44326c57":"X_df=delete_dup(X_df)","c8f24503":"# Recheck for duplicates\nnumber_of_unique=[X_df[i].nunique() for i in X_df.columns]\nunique_df=pd.DataFrame(unique_zip,columns=['Column_Feature','Unique_Values'])\nunique_df","2ecb3d06":"# A function to add the text version of 'sentiment'. This is just for graphing purposes\n# and should be droped.\ndef add_text_sent(df):\n\n  \n    # Copy the input DataFrame\n\n    out_df = df.copy()\n    \n    sentiment_text = []\n    \n    # Loop though the sentiments and assign the text version. \n    # Pro: 1, News: 2, Neutral: 0, Anti: -1\n    for sent in df['sentiment']:\n        \n        if sent == 1:\n            sentiment_text.append('Pro')\n            \n        elif sent == 2:\n            sentiment_text.append('News')\n            \n        elif sent == 0:\n            sentiment_text.append('Neutral')\n            \n        elif sent == -1:\n            sentiment_text.append('Anti')\n            \n    out_df['sentiment_text'] = sentiment_text\n    \n    out_df.drop(['message', 'tweetid'], axis = 1, inplace = True)\n        \n    return out_df","3bbb246d":"# Function to arrange the DataFrame to show percentage of classes\ndef class_table(df):\n    out_df = df.groupby(['sentiment_text']).count()\n    \n    class_perc = [round(100 * x \/ len(df), 1) for x in out_df['sentiment']]\n    \n    out_df['% of Total Classes'] = class_perc\n    \n    return out_df","f0b3e0c0":"# Create a new DataFrame for graphing purposes. Show the sentiment classes as a \n# percentage.\nnew_X_df = add_text_sent(X_df)\nnew_X_df_t = class_table(new_X_df)\nnew_X_df_t","1242a68e":"# Show the ditribution of the classes as a graph\n\nf, ax = plt.subplots(figsize=(10, 8))\nsns.set(style=\"whitegrid\")\nax = sns.countplot(x=\"sentiment_text\", data=new_X_df)\nplt.title('Message Count', fontsize =20)\nplt.show()","2c247fff":"# Add a column of length of tweets\n\nnew_X_df['message_length'] = X_df['message'].str.len()\nnew_X_df.head()","4d95748c":"# Display the boxplot of the length of tweets.\nplt.figure(figsize=(12.8,6))\nsns.boxplot(data=new_X_df, x='sentiment_text', y='message_length');","18af72ea":"# Plot of distribution of scores for building categories\nplt.figure(figsize=(12.8,6))\n    \n# Density plot of Energy Star scores\nsns.kdeplot(new_X_df[new_X_df['sentiment_text'] == 'Pro']['message_length'], label = 'Pro', shade = False, alpha = 0.8);\nsns.kdeplot(new_X_df[new_X_df['sentiment_text'] == 'News']['message_length'], label = 'News', shade = False, alpha = 0.8);\nsns.kdeplot(new_X_df[new_X_df['sentiment_text'] == 'Neutral']['message_length'], label = 'Neutral', shade = False, alpha = 0.8);\nsns.kdeplot(new_X_df[new_X_df['sentiment_text'] == 'Anti']['message_length'], label = 'Anti', shade = False, alpha = 0.8);\n\n# label the plot\nplt.xlabel('message length (char)', size = 15); plt.ylabel('Density', size = 15); \nplt.title('Density Plot of Message Length by Sentiment Class ', size = 20);","0edff4f2":"# Function to remove\/replace unwanted text such as characters,URLs etc\n\ndef clean(text):\n\n  text=text.replace(\"'\",'')\n  text=text.replace(\".\",' ')\n  text=text.replace(\"  \",'')\n  text=text.replace(\",\",' ')\n  text=text.replace(\"_\",' ')\n  text=text.replace(\"!\",' ')\n  text=text.replace(\"RT\",'retweet') #Replace RT(Retweet) with relay\n  text=text.replace(r'\\d+','')\n  text=re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+)|(https?\/\/[^\\s]+))','weblink',text)\n  text=re.sub('((co\/[^\\s]+)|(co?:\/\/[^\\s]+)|(co?\/\/[^\\s]+))','',text)\n  text=text.lower()  # Lowercase tweet\n  text =text.lstrip('\\'\"') # Remove extra white space\n  \n  return text","81a5e7de":"#Function 3\ndef rm_punc(text):\n  \n  clean_text=[]\n  for i in str(text).split():\n    rm=i.strip('\\'\"?,.:_\/<>!')\n    clean_text.append(rm)\n  return ' '.join(clean_text)","c516c636":"X_df['message']=X_df['message'].apply(clean)\nX_df['message']=X_df['message'].apply(rm_punc)\nX_df.head(5)","ba93dd57":"# Function replaces the @ symbol with the word at\n\ndef at(text):\n \n  return ' '.join(re.sub(\"(@+)\",\"at \",text).split())","f9203467":"# Function replaces the # symbol with the word tag\n\ndef hashtag(text):\n\n  return ' '.join(re.sub(\"(#+)\",\" tag \",text).split())","0e552511":"# Remove hashtags and replace @\n\nX_df['message']=X_df['message'].apply(at)\nX_df['message']=X_df['message'].apply(hashtag)\nX_df.head(5)","ba378183":"# Tokenise each tweet messge\n\ntokeniser = TreebankWordTokenizer()\nX_df['tokens'] = X_df['message'].apply(tokeniser.tokenize)\nX_df.head(5)","5978263c":"# Function performs lemmatization in the tokens column\n\ndef lemma(text):\n  lemma = WordNetLemmatizer() \n  return [lemma.lemmatize(i) for i in text]\n","110e8e87":"X_df['lemma'] =X_df['tokens'].apply(lemma)\nX_df.head(5)","569a38e1":"# Insert new clean message column\nX_df['clean message'] = X_df['lemma'].apply(lambda i: ' '.join(i))\nX_df.head(5)","194a77aa":"# Create copy of X_df to generate word cloud DataFrame\n\nword_df=X_df.copy()","d986c7c0":"# Remove small words that will clutter word cloud and have no significant meaning\n\ndef remove_small(text):\n  output=[]\n  for i in text.split():\n  \n    if len(i)>3:\n      output.append(i)\n    else:\n      pass\n  return ' '.join(output)\n\nword_df['clean message']=word_df['clean message'].apply(remove_small)","24529993":"# Create and generate a word cloud image:\n\n# Display the generated image:\n\nfig, axs = plt.subplots(2, 2, figsize=(18,10))\n\n\n# Anti class word cloud\n\nanti_wordcloud = WordCloud(width=1800, height = 1200,background_color=\"white\").generate(' '.join(i for i in word_df[word_df['sentiment']==-1]['clean message']))\naxs[0, 0].imshow(anti_wordcloud, interpolation='bilinear')\naxs[0, 0].set_title('Anti Tweets')\naxs[0, 0].axis('off')\n\n# Neutral cloud word cloud\n\nneutral_wordcloud = WordCloud(width=1800, height = 1200,background_color=\"white\").generate(' '.join(i for i in word_df[word_df['sentiment']==0]['clean message']))\naxs[0, 1].imshow(neutral_wordcloud, interpolation='bilinear')\naxs[0, 1].set_title('Neutral Tweets')\naxs[0, 1].axis('off')\n\n# Positive class word cloud\n\npositive_wordcloud = WordCloud(width=1800, height = 1200).generate(' '.join(i for i in word_df[word_df['sentiment']==1]['clean message']))\naxs[1, 0].imshow(positive_wordcloud, interpolation='bilinear')\naxs[1, 0].set_title('Positive Tweets')\naxs[1, 0].axis('off')\n\n# News class word cloud\n\nnews_wordcloud = WordCloud(width=1800, height = 1200).generate(' '.join(i for i in word_df[word_df['sentiment']==2]['clean message']))\naxs[1, 1].imshow(news_wordcloud, interpolation='bilinear')\naxs[1, 1].set_title('News Tweets')\naxs[1, 1].axis('off')\n\nplt.show()","b2991097":"# Spacy will be used to generate entities\nnlp = spacy.load('en_core_web_sm')","305ecc34":"# A new dataframe NER_df is created for the following visualisations\nNER_df=pd.DataFrame(X_df['clean message'])","1e03a15d":"# Function generates docs to get Name Entity Recognitions\n\ndef doc(text):\n  doc=nlp(text)\n  return doc","8bef1d5f":"# Create a new column containing the nlp transformed text\nNER_df['doc']=NER_df['clean message'].apply(doc)","1dd24706":"#Functions below extract persons and organisations from the input parameter text. If entity is not found 'None' is populated in cell\n\ndef person(doc):\n    if doc.ents:\n        for ent in doc.ents:\n          if ent.label_=='PERSON':\n            return (ent.text)\n    else:\n      return ('None')\n\ndef org(doc):\n    if doc.ents:\n        for ent in doc.ents:\n          if ent.label_=='ORG':\n            return (ent.text)\n    else:\n      return ('None')","f3f7cea2":"# Generate new columns 'persons' and 'organisation'\n\nNER_df['persons']=NER_df['doc'].apply(person)\nNER_df['organisation']=NER_df['doc'].apply(org)","8ca9c147":"# Retrive all the PERSON labels from the NER_df and generate a new dataframe person_df for analysis\npersons=[i for i in NER_df['persons']]\nperson_counts = Counter(persons).most_common(20)\nperson_df=pd.DataFrame(person_counts,columns=['persons name','count'])\nperson_df.drop([0,1,7,8,13,15,16],axis=0,inplace=True) # rows removed due to 'None' entries, incorrect classification or different entry of a same entity (repetition)\n\n# Plot top persons tweeted\nf, ax = plt.subplots(figsize=(30, 10))\nsns.set(style='white',font_scale=1.2)\nsns.barplot(x=person_df[person_df['count'] <1000].iloc[:,0],y=person_df[person_df['count'] <1000].iloc[:,1])\nplt.xlabel('Persons Name') \nplt.ylabel('Mentions')   \nplt.show()","35d7420b":"# Retrive all the ORG labels from the NER_df and generate a new dataframe org_df for analysis\norg=[i for i in NER_df['organisation']]\n\norg_counts = Counter(org).most_common(15)\norg_df=pd.DataFrame(org_counts,columns=['organisation name','count'])\norg_df.drop([0,1,3,8,12],axis=0,inplace=True) # rows removed due to 'None' entries, incorrect classification or different entry of a same entity (repetition)\n\n# Plot top organisations tweeted\nf, ax = plt.subplots(figsize=(30, 10))\nsns.set(style='white',font_scale=2)\norg_bar=sns.barplot(x=org_df[org_df['count'] <1000].iloc[:,0],y=org_df[org_df['count'] <1000].iloc[:,1])\nplt.xlabel('Organisation Name') \nplt.ylabel('Mentions')   \nplt.show()","d38c748c":"# Feature and label split \n\nX=X_df['clean message']\ny=X_df['sentiment']","bf4501af":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","d1390864":"#pipeline = Pipeline([('tfidf', TfidfVectorizer()),('clf', SVC())])\n\n#parameters = {\n #   'tfidf__max_df': (0.25, 0.5, 0.75),'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n   # 'tfidf__max_features':(500,2500,5000),'clf__C':(0.1,1,10),'clf__gamma':(1,0.1,0.001)}\n\n#svc = GridSearchCV(pipeline, parameters, cv=2, n_jobs=2, verbose=3)\n#svc.fit(X_train, y_train)","24576ecb":"#svc.best_params_","dca9f443":"#svc_predictions = svc.predict(X_test)","bb799c28":"#Pipeline \n\nsvc = Pipeline(\n    [('tfidf', TfidfVectorizer(analyzer='word', max_df=0.75,max_features=5000,ngram_range=(1,1)))\n    ,('clf', SVC(C=10,gamma=1))])\n\n# Train model\nmodel=svc.fit(X_train, y_train)\n\n# Form a prediction set\npredictions = model.predict(X_test)","f01c12c4":"# Print Results\n#Confusion matrix\nconfusion = 'Confusion Matrix'.center(100, '*')\nprint(confusion)\nmatrix=confusion_matrix(y_test,predictions)\nprint(confusion_matrix(y_test,predictions))\nprint('')\n\n#Classification report\nreport='Classification Report'.center(100,'*')\nprint(report)\nprint('')\nprint(classification_report(y_test,predictions))\nprint('')\n\n#Model Performance\nperformance='Performance Metrics'.center(100,'*')\nprint(performance)\nprint('The model accuracy is :',accuracy_score(y_test,predictions))\nprint('The model recall is :',recall_score(y_test, predictions,average='weighted'))\n\nF1 = 2 * (precision_score(y_test,predictions,average='weighted') * recall_score(y_test, predictions,average='weighted')) \/ (precision_score(y_test,predictions,average='weighted') + recall_score(y_test, predictions,average='weighted'))\n\nprint('The model F1score is : ',F1)","a3c34450":"import pickle\nmodel_save_path = \"SVC.pkl\"\nwith open(model_save_path,'wb') as file:\n    pickle.dump(svc,file)","871c45c1":"#import tes.csv\n\ntest=pd.read_csv('\/kaggle\/input\/climate-change-belief-analysis\/test.csv')","2431380b":"# Text cleaning\n\ntest['message']=test['message'].apply(clean) #clean data\ntest['message']=test['message'].apply(rm_punc) #remove punctuation\ntest['message']=test['message'].apply(at) #replace @\ntest['message']=test['message'].apply(hashtag) #remove #","b9a57969":"# Tokenize messages\n\ntokeniser = TreebankWordTokenizer()\ntest['tokens'] = test['message'].apply(tokeniser.tokenize)","e093eb5b":"# Lemmatize tokens column\n \ntest['lemma'] = test['tokens'].apply(lemma)","66404b74":"# Generate clean message column\n\ntest['clean message'] = test['lemma'].apply(lambda i: ' '.join(i))","c2e26db0":"test.head(5)","6e912e16":"#Drop columns not needed for predictions\ndrop_list=['message','tokens','lemma']\ntest.drop(drop_list,axis=1,inplace=True)","eb6e2459":"test.head(5)","a783b0c4":"model_load_path = \"SVC.pkl\"\nwith open(model_load_path,'rb') as file:\n    pickle_rick = pickle.load(file)","2032f560":"# Perfom predictions on test set\n\nkaggle_predictions = pickle_rick.predict(test['clean message'])\nkaggle_predictions = pd.DataFrame(kaggle_predictions)\nkaggle_predictions.rename(columns={0: \"sentiment\"}, inplace=True)\nkaggle_predictions[\"tweetid\"] = test['tweetid']\ncols = ['tweetid','sentiment']\nkaggle_predictions = kaggle_predictions[cols]","3ca41d9e":"kaggle_predictions.to_csv(path_or_buf='upload_kaggle_pred.csv',index=False)","ae0f1c7a":"#prediction output\n\npred_df=pd.read_csv('upload_kaggle_pred.csv')\npred_df","a7d83e71":"# Log metrics\n\n#experiment = Experiment(api_key=\"1T5FVvyOGYzMIIYf6dyXqFLcJ\")\n#with experiment.context_manager(\"validation\"):\n  #svc.fit(X_train, y_train)\n  #accuracy = accuracy_score(y_test,predictions)\n  #recall = recall_score(y_test, predictions,average='weighted')\n  #F1 = 2 * (precision_score(y_test,predictions,average='weighted') * recall_score(y_test, predictions,average='weighted')) \/ (precision_score(y_test,predictions,average='weighted') + recall_score(y_test, predictions,average='weighted'))\n  # returns the validation accuracy,recall and F1 score\n  #experiment.log_metric(\"accuracy\", accuracy)\n  #experiment.log_metric(\"recall\", recall)\n  #experiment.log_metric(\"F1\", F1)","799128bf":"# End the experiment\n\n#experiment.end()","cbec5951":"# Display comet experiment\n\n#experiment.display()","6b9a2e2d":"###Background\nMany companies are built around lessening one\u2019s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat.\n\n### Problem Statement\nCreate a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.","50b8ed73":"**Train\/Test Split**","201d2f8d":"## Exploratory Data Analysis","782afbbd":"Fitting model with optimized parameters","7469bd41":"**Named Entity Recognition**","6b67a661":"The Perparation of the data for modelling will require spliting of features and labels. In this instance, only the sentiment and clean message columns are required from X_df","7e1d4e41":"Perform data cleaning as done before on the test dataframe from test.csv","4dfdb87a":"The strings generated for each sentiment class will provide the information used to generate the wordclouds for each class","6ee9a356":"Generate Lemmatization column","720824d7":"**Clean Text Data**","9f9e2cc6":"Furthermore, the @ and # must be dealt with by replacing at with @ and removing #\n","e185fa99":"Futhermore, it can be seen which people and organisations are refered in tweets using NER","9d37cb6c":"##Problem statement","082a43e7":"Final Dataframe format for kaggle prediction","4cbe16fc":"Punctuation at the beginning and end of words can be removed","0c2c7ea0":"From the Gridsearch. The ideal parameters for SVC classification combined with TfidfVectoriztion is c=10 , gamma=1 , max_df=0.25 , max_features=5000 , ngram=(1,1)","666dc236":"Below are the plots for the top persons and organisations tweeted about","db94a1c9":"Perform Lemmatization of tokens to group together the different inflected forms of a word so they can be analysed as a single item","acf30e91":"The class labels of the training data are not balanced. As shown in the table above, most of the tweets (50.8%) are classified as `Pro`, which in the context of the `Problem Statement` means they believe in man-made climate change. The other classes (`News`, `Neutral` and `Anti`) account for `24.9%`, `15.8%` and `8.6%`respectively. This presents a challenge in that the models developed might have a bias in classifying tweets. To further illustrate this distribution, a bar graph of the count of each sentiment class is shown below: ","9a7db5cf":"**Save model as pickle**","4df6db22":"X_df DataFrame contains three columns; sentiment, message and tweetid","4b11f78c":"## Load Data","07e85cf7":"Lastly a new column derived from the lemma column can be generate in order to train the model\/s","7e5042db":"25% of data allocated to testing, with random state set to 42","aeae492d":"#Classification Predict","b66e8de2":"There are many organisations that are referenced in tweets but the epa (United States Environmental Protection Agency) is mentioned the most, followed by well known organisations such as CNN (news) , the UN (ntergovernmental organization) and DOE( Department of Energy)","56d47a22":"Write predictions to upload_kaggle_pred.csv","437cbb87":"Split tweets into individual words via tokenization in the tokens column.","2580ddfd":"#### Length of Tweets per Sentiment Class ","583656cd":"##Model Training and Testing","e2212cee":"Text data needs to be pre-processed before modelling. Steps taken include\n\n1. Removing\/Replacing of certain text and characters\n2. Tokenization and lemmatization of text","4495da7d":"X and y training features split from X_df\n","5de109b3":"As seen in the plot above, the people most tweeted about are prominent USA politicians such as Donald trump and Al Gore, Leonardo Dicaprio is also a popular figure tweeted about regarding climate change. Although there is a misclassifications of doe(Department of Energy) ","8f79d075":"## Importing the libraries","d6361199":"##Model Prediction on test.csv","f9924395":"Parameter Search performed using GridSearch.(Takes a while to run, output hardcopied in pipeline.Uncheck to run)","ccba10f7":"## Data Preprocessing","2818e3ec":"The words most popular amongst the sentiment groups can be represented visually. This provides insights into the nature of the Tweet messages for each class","b1c975c0":"**Word Clouds**","0ec8c194":"Load model pickle to make predictions","b4b8867f":"**Tokenize and Lemmatization**","47163769":"The Tweets seem to be around `125` characters long on average. This is shown by both the mean of the classes in the boxplots above and distribution density of the classes below. All the classes have a distribution density centred around `125` characters. \n\nThe length of `Neutral` Tweets seem to have a wider range and `Pro` Tweets have a shorter range. The `Pro` Tweets also seem to have a lot of `outliers` by this measurement.","3d1e8f5e":"A check for uniqueness indicates that the message column has over 1000 duplicates due to repeated tweetids. This should be removed as to not train the model on repeated data.\n","21cb49bb":"##Text Analysis","51debd44":"From the word clouds it is clear that all groups speak about climate change and global warming as expected. The word clouds of each group however contain information that echo their sentiment.\n\n1.   The Anti cloud contains words such as fake, man made and scam\n2.   The Positive cloud contains words such as real, believe,change and talk climate\n3.   The Neutral cloud contains a mixture of words that could lean either to anti or positive\n4.   The News cloud contains words and names such as Trump, Scott Pruitt (Administrator of the United States Environmental Protection Agency) and change policy. Typical of news reporting.\n","cd18191d":"In order to give our graphs more context, we have added the text version of the sentiment to the train DataFrame. These new columns will be deleted as they do not assist in the actual classification problem.","fc1c05d6":"**SVC Classification with TfidfVectorization**"}}