{"cell_type":{"beaec2b2":"code","ae8fb365":"code","609b0d24":"code","8816cb7b":"code","082f5a16":"code","1dcaa6b2":"code","ed77907c":"code","30c52895":"code","0d723443":"code","2f45409a":"code","13871e99":"code","aeb2a879":"code","cc5ea9b6":"code","d22132c2":"code","0214d9aa":"code","1c83bba5":"code","0ab437ac":"code","30157f22":"code","46adcf79":"code","a02051f1":"code","5fa8f85b":"code","937db132":"code","a21d34df":"code","20c7cf5e":"code","66f85e87":"code","59a2ba0c":"code","f514738d":"code","bfdf4c8b":"markdown","05e43126":"markdown","c5052f4f":"markdown","474e8087":"markdown","6ccd03cf":"markdown","fd20b08d":"markdown","450dac9e":"markdown","b54aca7c":"markdown","da5faf0a":"markdown","007be1db":"markdown","5a412127":"markdown","be2e036f":"markdown","5fac613d":"markdown"},"source":{"beaec2b2":"pip install beautifulsoup4","ae8fb365":"import requests\n\nfrom bs4 import BeautifulSoup as bs\n","609b0d24":"r= requests.get('https:\/\/keithgalli.github.io\/web-scraping\/example.html')","8816cb7b":"soup = bs(r.content)\nprint(soup.prettify())","082f5a16":"first_header = soup.find(\"h2\")\nfirst_header","1dcaa6b2":"headers = soup.find_all(\"h2\")\nprint(headers)","ed77907c":"first_header = soup.find_all([\"h1\" , \"h2\"])\nfirst_header","30c52895":"paragraph = soup.find_all(\"p\" , attrs= {\"id\" : \"paragraph-id\"})\nparagraph","0d723443":"body = soup.find('body')\n\ndiv = soup.find('div')\nheader = div.find('h1')\nheader","2f45409a":"import re","13871e99":"paragraphs =  soup.find_all(\"p\" , string= re.compile(\"Some\"))\nparagraphs\nheaders = soup.find_all(\"h2\" , string = re.compile(\"(H|h)eader\"))\nheaders","aeb2a879":"content = soup.select(\"p\")\ncontent","cc5ea9b6":"content = soup.select(\"div p\")\ncontent","d22132c2":"paragraphs = soup.select (\"h2 ~ p\")\nparagraphs","0214d9aa":"bold_text = soup.select(\"p#paragraph-id b\")\nbold_text","1c83bba5":"paragraphs = soup.select(\"body > p\")\nprint(paragraphs)\n\nfor paragraph in paragraphs:\n  print(paragraph.select(\"i\"))","0ab437ac":"soup.select(\"[align=middle]\")","30157f22":"## Load the webpage content\nr = requests.get(\"https:\/\/keithgalli.github.io\/web-scraping\/webpage.html\")\n# convert to a beautiful soup object\nwebpage = bs(r.content)\n#print out our html\nprint(webpage.prettify())","46adcf79":"links = webpage.select(\"a\")\nlinks","a02051f1":"links = webpage.select(\"ul.socials\")\nlinks","5fa8f85b":"links = webpage.select(\"ul.socials a\")\nlinks","937db132":"links = webpage.select(\"ul.socials a\")\nactual_links = [link['href'] for link in links]\nactual_links","a21d34df":"ulist = webpage.find(\"ul\" , attrs={\"class\": \"socials\"})\nlinks = ulist.find_all(\"a\")\nlinks","20c7cf5e":"import pandas as pd\ntable =  webpage.select(\"table.hockey-stats\")[0]\ncolumns = table.find(\"thead\").find_all(\"th\")\ncolumns_names = [c.string for c in columns]\ncolumns_names","66f85e87":"table_rows = []\nl = []\nfor tr in table_rows:\n    td = tr.find_all('td')\n    row = [tr.text for tr in td]\n    l.append(row)\npd.DataFrame(l, columns=[\"A\", \"B\", ...])","59a2ba0c":"import pandas as pd\n\ntable =  webpage.select(\"table.hockey-stats\")[0]\ncolumns = table.find(\"thead\").find_all(\"th\")\ncolumns_names = [c.string for c in columns]\n\ntable_rows = table.find(\"tbody\").find_all(\"tr\")\nl = []\nfor tr in table_rows:\n    td = tr.find_all('td')\n    row = [str(tr.get_text()).strip() for tr in td]\n    l.append(row)\n    \ndf = pd.DataFrame(l, columns = columns_names)\ndf.head()","f514738d":"facts = webpage.select(\"ul.fun-facts li\")\nfacts_with_is = [fact.find(string = re.compile(\"is\")) for fact in facts]\nfacts_with_is","bfdf4c8b":"## Convert to a beautiful soup page","05e43126":"## Do this in at least 3 different ways","c5052f4f":"Load in the necessary libraries","474e8087":"## select (css selection)","6ccd03cf":"### we can search specific strings in our find \\ find_all calls","fd20b08d":"## Grab all fun facts that use word \"is\"","450dac9e":"## You can nest find \/ find_all calls","b54aca7c":"## Look the webpage content","da5faf0a":"## Start using beatiful soup to scrape\n# Fina and find out","007be1db":"## you can pass in attributes to the find \/ find_all function","5a412127":"## pass in a list of elements to look for","be2e036f":"## Grap by element with specific property","5fac613d":"## Grab all the social links from the webpage"}}