{"cell_type":{"9944a4db":"code","94e85aa3":"code","1340e625":"code","e66806a8":"code","3489df00":"code","e5612d81":"code","6d6891b6":"code","c2a765d6":"code","5a0b96c7":"code","16a29427":"code","181a8fe8":"code","92645107":"code","b111e0e5":"code","8e72c111":"code","5bdbd043":"code","74c17ad7":"code","b2cd096c":"code","656ebecc":"code","099d5c81":"code","2e702121":"code","0f3b21da":"code","7b2af31d":"code","218c6aa3":"code","bbd6e9c3":"code","55517335":"code","eb046157":"code","0fa06f25":"code","d7ebd313":"code","6804b267":"code","e0c3df1c":"code","6f87c134":"markdown"},"source":{"9944a4db":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","94e85aa3":"store = pd.read_csv(\"..\/input\/store.csv\")\ntrain = pd.read_csv(\"..\/input\/train.csv\",parse_dates=[2])\ntest = pd.read_csv(\"..\/input\/test.csv\",parse_dates=[3])","1340e625":"store.head()","e66806a8":"# check store nan rows\nstore.isnull().sum()","3489df00":"store.PromoInterval.value_counts()","e5612d81":"# fillna in store with 0 has better result than median()\nstore.fillna(0, inplace=True)","6d6891b6":"train.head().append(train.tail())","c2a765d6":"train.isnull().sum()","5a0b96c7":"train.Open.value_counts()","16a29427":"np.sum([train['Sales'] == 0])","181a8fe8":"# draw store 1 and store 10 sales distribution plot\nimport matplotlib.pyplot as plt\nstore_1 = train.loc[(train[\"Store\"]==1)&(train['Sales']>0), ['Date',\"Sales\"]]\nstore_10 = train.loc[(train[\"Store\"]==10)&(train['Sales']>0), ['Date',\"Sales\"]]\nf = plt.figure(figsize=(18,10))\nax1 = f.add_subplot(211)\nax1.plot(store_1['Date'], store_1['Sales'], '-')\nax1.set_xlabel('Time')\nax1.set_ylabel('Sales')\nax1.set_title('Store 1 Sales Distribution')\n\nax2 = f.add_subplot(212)\nax2.plot(store_10['Date'], store_10['Sales'], '-')\nax2.set_xlabel('Time')\nax2.set_ylabel('Sales')\nax2.set_title('Store 10 Sales Distribution')","92645107":"test.head()","b111e0e5":"test.isnull().sum()","8e72c111":"# check stores open distribution on days of week\nimport seaborn as sns\nsns.countplot(x = 'DayOfWeek', hue = 'Open', data = test)\nplt.title('Store Daily Open Countplot')","5bdbd043":"# check missing values in test open column\ntest[test.Open.isnull()]","74c17ad7":"# fill missing values in test with 1\ntest.fillna(value = 1, inplace = True)","b2cd096c":"# merge data with store \ntrain = pd.merge(train, store, on='Store')\ntest = pd.merge(test, store, on='Store')\n\n# split the last 6 weeks data as hold-out set (idea from Gert https:\/\/www.kaggle.com\/c\/rossmann-store-sales\/discussion\/18024)\ntrain = train.sort_values(['Date'],ascending = False)\ntrain_total = train.copy()\n\nsplit_index = 6*7*1115\nvalid = train[:split_index] \ntrain = train[split_index:]\n\n# only use data of Sales>0 and Open is 1\nvalid = valid[(valid.Open != 0)&(valid.Sales >0)]\ntrain = train[(train.Open != 0)&(train.Sales >0)]\ntrain_total = train_total[(train_total.Open != 0)&(train_total.Sales >0)]","656ebecc":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# check distribution of sales in train set\nfig = plt.figure(figsize=(12,5))\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\ng1 = sns.distplot(train['Sales'],hist = True,label='skewness:{:.2f}'.format(train['Sales'].skew()),ax = ax1)\ng1.legend()\ng1.set(xlabel = 'Sales', ylabel = 'Density', title = 'Sales Distribution')\ng2 = sns.distplot(np.log1p(train['Sales']),hist = True,label='skewness:{:.2f}'.format(np.log1p(train['Sales']).skew()),ax=ax2)\ng2.legend()\ng2.set(xlabel = 'log(Sales+1)',ylabel = 'Density', title = 'log(Sales+1) Distribution')\nplt.show()","099d5c81":"# process train and test\ndef process(data, isTest = False):\n    # label encode some features\n    mappings = {'0':0, 'a':1, 'b':2, 'c':3, 'd':4}\n    data.StoreType.replace(mappings, inplace=True)\n    data.Assortment.replace(mappings, inplace=True)\n    data.StateHoliday.replace(mappings, inplace=True)\n    \n    # extract some features from date column  \n    data['Month'] = data.Date.dt.month\n    data['Year'] = data.Date.dt.year\n    data['Day'] = data.Date.dt.day\n    data['WeekOfYear'] = data.Date.dt.weekofyear\n    \n    # calculate competiter open time in months\n    data['CompetitionOpen'] = 12 * (data.Year - data.CompetitionOpenSinceYear) + \\\n        (data.Month - data.CompetitionOpenSinceMonth)\n    data['CompetitionOpen'] = data['CompetitionOpen'].apply(lambda x: x if x > 0 else 0)\n    \n    # calculate promo2 open time in months\n    data['PromoOpen'] = 12 * (data.Year - data.Promo2SinceYear) + \\\n        (data.WeekOfYear - data.Promo2SinceWeek) \/ 4.0\n    data['PromoOpen'] = data['PromoOpen'].apply(lambda x: x if x > 0 else 0)\n                                                 \n    # Indicate whether the month is in promo interval\n    month2str = {1:'Jan', 2:'Feb', 3:'Mar', 4:'Apr', 5:'May', 6:'Jun', \\\n             7:'Jul', 8:'Aug', 9:'Sept', 10:'Oct', 11:'Nov', 12:'Dec'}\n    data['month_str'] = data.Month.map(month2str)\n\n    def check(row):\n        if isinstance(row['PromoInterval'],str) and row['month_str'] in row['PromoInterval']:\n            return 1\n        else:\n            return 0\n        \n    data['IsPromoMonth'] =  data.apply(lambda row: check(row),axis=1)    \n    \n    # select the features we need\n    features = ['Store', 'DayOfWeek', 'Promo', 'StateHoliday', 'SchoolHoliday',\n       'StoreType', 'Assortment', 'CompetitionDistance',\n       'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', 'Promo2',\n       'Promo2SinceWeek', 'Promo2SinceYear', 'Year', 'Month', 'Day',\n       'WeekOfYear', 'CompetitionOpen', 'PromoOpen', 'IsPromoMonth']  \n    if not isTest:\n        features.append('Sales')\n        \n    data = data[features]\n    return data\n\ntrain = process(train)\nvalid = process(valid)\ntrain_total = process(train_total)\nx_test = process(test,isTest = True)    ","2e702121":"# sort by index\nvalid.sort_index(inplace = True)\ntrain.sort_index(inplace = True)\ntrain_total.sort_index(inplace = True)\n\n# split x and y\nx_train, y_train = train.drop(columns = ['Sales']), np.log1p(train['Sales'])\nx_valid, y_valid = valid.drop(columns = ['Sales']), np.log1p(valid['Sales'])\nx_train_total, y_train_total = train_total.drop(columns = ['Sales']), np.log1p(train_total['Sales'])","0f3b21da":"# define eval metrics\ndef rmspe(y, yhat):\n    return np.sqrt(np.mean((yhat\/y-1) ** 2))\n\ndef rmspe_xg(yhat, y):\n    y = np.expm1(y.get_label())\n    yhat = np.expm1(yhat)\n    return \"rmspe\", rmspe(y,yhat)","7b2af31d":"# try random forest\nfrom sklearn.ensemble import RandomForestRegressor\n\nclf = RandomForestRegressor(n_estimators = 15)\nclf.fit(x_train, y_train)\n# validation\ny_pred = clf.predict(x_valid)\nerror = rmspe(np.expm1(y_valid), np.expm1(y_pred))\nprint('RMSPE: {:.4f}'.format(error))","218c6aa3":"# plot feature importance for random forest model, show top 10 features\nfeat_importances = pd.Series(clf.feature_importances_, index=x_train.columns)\nfeat_importances.nlargest(10).sort_values(ascending = True).plot(kind='barh')\nplt.xlabel('importance')\nplt.title('Feature Importance')","bbd6e9c3":"import xgboost as xgb\n\nparams = {\"objective\": \"reg:linear\", # for linear regression\n          \"booster\" : \"gbtree\",   # use tree based models \n          \"eta\": 0.03,   # learning rate\n          \"max_depth\": 10,    # maximum depth of a tree\n          \"subsample\": 0.9,    # Subsample ratio of the training instances\n          \"colsample_bytree\": 0.7,   # Subsample ratio of columns when constructing each tree\n          \"silent\": 1,   # silent mode\n          \"seed\": 10   # Random number seed\n          }\nnum_boost_round = 4000\n\ndtrain = xgb.DMatrix(x_train, y_train)\ndvalid = xgb.DMatrix(x_valid, y_valid)\nwatchlist = [(dtrain, 'train'), (dvalid, 'eval')]\n# train the xgboost model\nmodel = xgb.train(params, dtrain, num_boost_round, evals=watchlist, \\\n  early_stopping_rounds= 100, feval=rmspe_xg, verbose_eval=True)","55517335":"# validation\ny_pred = model.predict(xgb.DMatrix(x_valid))\nerror = rmspe(np.expm1(y_valid), np.expm1(y_pred))\nprint('RMSPE: {:.4f}'.format(error))\n","eb046157":"# rmspe correction on the whole\ndef correction():\n    weights = np.arange(0.98, 1.02, 0.005)\n    errors = []\n    for w in weights:\n        error = rmspe(np.expm1(y_valid), np.expm1(y_pred*w))\n        errors.append(error)\n        \n    # make line plot\n    plt.plot(weights, errors)\n    plt.xlabel('weight')\n    plt.ylabel('RMSPE')\n    plt.title('RMSPE Curve')\n    # print min error\n    idx = errors.index(min(errors))\n    print('Best weight is {}, RMSPE is {:.4f}'.format(weights[idx], min(errors)))\n    \ncorrection()\n","0fa06f25":"x_train_total.head().append(x_train_total.tail())","d7ebd313":"print(x_train_total.shape)\nprint(y_train_total.shape)","6804b267":"dtrain = xgb.DMatrix(x_train_total, y_train_total)\ndtest = xgb.DMatrix(x_test)\n# specify parameters via map\nparams = {\"objective\": \"reg:linear\", # for linear regression\n          \"booster\" : \"gbtree\",   # use tree based models \n          \"eta\": 0.03,   # learning rate\n          \"max_depth\": 10,    # maximum depth of a tree\n          \"subsample\": 0.9,    # Subsample ratio of the training instances\n          \"colsample_bytree\": 0.7,   # Subsample ratio of columns when constructing each tree\n          \"silent\": 1,   # silent mode\n          \"seed\": 10   # Random number seed\n          }\nnum_round = 3000\nmodel = xgb.train(params, dtrain, num_round)\n# make prediction\npreds = model.predict(dtest)","e0c3df1c":"# make submission using best weight\nresult = pd.DataFrame({\"Id\": test[\"Id\"],'Sales': np.expm1(preds*0.995)})\nresult.to_csv(\"submission_xgb.csv\", index=False)\n\n# plot feature importance, show top 10 features\nfig, ax = plt.subplots(figsize=(8,8))\nxgb.plot_importance(model, max_num_features= 10, height=0.5, ax=ax)\nplt.show()","6f87c134":"Reference:  \n\n1. [XGBoost documentation](http:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html#)  \n2. [Model documentation 1st place](http:\/\/www.kaggle.com\/c\/rossmann-store-sales\/discussion\/18024)\n3. [XGBoost Feature Importance](https:\/\/www.kaggle.com\/cast42\/xgboost-in-python-with-rmspe-v2\/code)\n4. [Rossmann Sales Top1%](https:\/\/www.kaggle.com\/xwxw2929\/rossmann-sales-top1)\n\n\n    "}}