{"cell_type":{"f57916f7":"code","ab313cc8":"code","f62aee3e":"code","ad1b380c":"code","04782833":"code","47ee61c9":"code","2b4de04c":"code","4277646d":"code","abb7428e":"code","6008bef0":"code","0d2de657":"code","837005dd":"code","94c1afe7":"code","c91398df":"code","397c701f":"code","63c6f266":"code","ea3d29e9":"code","061b9d4c":"code","3aec5e2a":"code","538506c0":"code","025f27fd":"code","b52f6068":"code","81e9fc3e":"code","eb81290e":"code","15fc9c4d":"markdown","4e079e53":"markdown","48016213":"markdown","db20d973":"markdown","b93bde7f":"markdown","5602ed2b":"markdown","e6be2e42":"markdown","d337de89":"markdown","df3b19a9":"markdown","ad169312":"markdown","7768f732":"markdown","1f8c705a":"markdown","15a2e304":"markdown","98e01eeb":"markdown","aff3ed02":"markdown","a982cd6b":"markdown","48180606":"markdown","a1553d8e":"markdown","1e3f059a":"markdown","53549409":"markdown","90a58cbf":"markdown","b01fe676":"markdown","6b90a2ef":"markdown","ee32d90c":"markdown","574db38d":"markdown","f12490da":"markdown","8f20bf65":"markdown","e6482e27":"markdown","30b32cb3":"markdown"},"source":{"f57916f7":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\n#%matplotlib inline for static images in notebooks\nfrom IPython.core.pylabtools import figsize\nimport matplotlib.pyplot as plt\nfigsize(10, 10)\n\ndf_orginal= pd.read_csv('..\/input\/star-dataset\/6 class csv.csv')","ab313cc8":"df_orginal.head()","f62aee3e":"df_orginal.describe()","ad1b380c":"df_orginal['Star type'].value_counts()","04782833":"df_orginal.isnull().sum()","47ee61c9":"set(df_orginal['Spectral Class']) ","2b4de04c":"set(df_orginal['Star color']) ","4277646d":"df=df_orginal\ndf['Spectral Class'] = df['Spectral Class'].map({'O': 0, 'B': 1, 'A': 2, 'F': 3, 'G': 4, 'K': 5, 'M': 6})\ndf.head()","abb7428e":"df[['Blue','White','Orange','Red','Yellow']] = 0\ndf.loc[df['Star color'].str.contains(\"Blue\",case=False),'Blue']=1\ndf.loc[df['Star color'].str.contains(\"Whit\",case=False),'White']=1\ndf.loc[df['Star color'].str.contains(\"Red\",case=False),'Red']=1\ndf.loc[df['Star color'].str.contains(\"Yellow\",case=False),'Yellow']=1\ndf.loc[df['Star color'].str.contains(\"Orange\",case=False),'Orange']=1","6008bef0":"df[df['Star color']=='Yellowish White' ]","0d2de657":"df[df['Star color']=='Whitish' ]","837005dd":"df.drop(labels=['Star color'],axis=1, inplace=True)\ndf.head()","94c1afe7":"cols = list(df.columns.values)\ncols.pop(cols.index('Star type')) \ndf = df[cols+['Star type']]\ndf.head()","c91398df":"from sklearn.preprocessing import StandardScaler\ndf_n=df\ndf_n.iloc[:,:-1] = StandardScaler().fit_transform(df.iloc[:,:-1])\ndf_n.head()","397c701f":"sns.scatterplot(data=df_n, x='Temperature (K)', y='Absolute magnitude(Mv)', hue='Star type') ","63c6f266":"sns.scatterplot(data=df_n, x='Spectral Class', y='Absolute magnitude(Mv)', hue='Star type')","ea3d29e9":"from sklearn.model_selection import train_test_split\nX = df_n.iloc[:,:-1]\ny = df_n['Star type']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=1) # 0.25 x 0.8 = 0.2\nlen(X_train), len(X_test), len(X_val)","061b9d4c":"y_train.value_counts()","3aec5e2a":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\n\naccuracy_scores = []\nmax_depths = []\n\nfor max_depth in range(1, 16):\n    model = DecisionTreeClassifier(max_depth = max_depth)\n    model.fit(X_train, y_train)\n\n    test_prediction = model.predict(X_test)\n    test_accuracy = accuracy_score(y_test, test_prediction)\n\n    max_depths.append(max_depth)\n    accuracy_scores.append(test_accuracy)\n\nax = sns.lineplot(x = max_depths, y = accuracy_scores)\nax.set(xlabel='Decision Tree Max Depth', ylabel='Accuracy Score')\nprint(f'Best accuracy score |{max(accuracy_scores)}| achieved at max depth |{np.argmax(accuracy_scores) + 1}|')\n\nmodel1 = DecisionTreeClassifier(max_depth=max_depths[np.argmax(accuracy_scores)])\nmodel1.fit(X_train, y_train)","538506c0":"from sklearn.linear_model import LogisticRegression\n\nmodel2 = LogisticRegression()\nmodel2.fit(X_train, y_train)\n\ntest_prediction = model2.predict(X_test)\nmodel2_acc = accuracy_score(y_test, test_prediction)\n\nprint(f'Logistic Regression Accuracy on train: |{model2_acc}|')","025f27fd":"from sklearn.svm import SVC\naccuracy_scores = []\nparams = []\n\nfor C in np.linspace(0.01, 5, 20):\n    for degree in range(2, 10):\n        model = SVC(C = C, degree=degree, kernel='poly')\n        model.fit(X_train, y_train)\n\n        test_prediction = model.predict(X_test)\n        test_accuracy = accuracy_score(y_test, test_prediction)\n\n        params.append({'C': C, 'degree': degree})\n        accuracy_scores.append(test_accuracy)\n\nprint(f'Best accuracy score |{max(accuracy_scores)}| achieved with params|{params[np.argmax(accuracy_scores)]}|')\n\nmodel3 = SVC(C = params[np.argmax(accuracy_scores)]['C'], degree=params[np.argmax(accuracy_scores)]['degree'], kernel='poly')\nmodel3.fit(X_train, y_train)","b52f6068":"from tqdm import tqdm\n\naccuracy_scores = []\nparams = []\n\nfor C in tqdm(np.linspace(0.01, 5, 20)):\n    for gamma in np.linspace(0.001, 2, 40):\n        model = SVC(C = C, gamma=gamma, kernel='rbf')\n        model.fit(X_train, y_train)\n\n        test_prediction = model.predict(X_test)\n        test_accuracy = accuracy_score(y_test, test_prediction)\n\n        params.append({'C': C, 'gamma': gamma})\n        accuracy_scores.append(test_accuracy)\n\nprint(f'Best accuracy score |{max(accuracy_scores)}| achieved with params|{params[np.argmax(accuracy_scores)]}|')\n\nmodel4 = SVC(C = params[np.argmax(accuracy_scores)]['C'], gamma=params[np.argmax(accuracy_scores)]['gamma'], kernel='rbf')\nmodel4.fit(X_train, y_train)\n","81e9fc3e":"from sklearn.neighbors import KNeighborsClassifier\n\naccuracy_scores = []\nK_values = []\n\nfor K in range(1, 10):\n    model = KNeighborsClassifier(n_neighbors = K)\n    model.fit(X_train, y_train)\n\n    test_prediction = model.predict(X_test)\n    test_accuracy = accuracy_score(y_test, test_prediction)\n\n    K_values.append(K)\n    accuracy_scores.append(test_accuracy)\n\nax = sns.lineplot(x = K_values, y = accuracy_scores)\nax.set(xlabel='K Neigbours considered', ylabel='Accuracy Score')\nprint(f'Best accuracy score |{max(accuracy_scores)}| achieved with K |{K_values[np.argmax(accuracy_scores)]}|')\n\nmodel5 = KNeighborsClassifier(n_neighbors = K_values[np.argmax(accuracy_scores)])\nmodel5.fit(X_train, y_train)","eb81290e":"model1_predictions = model1.predict(X_val)\nmodel2_predictions = model2.predict(X_val)\nmodel3_predictions = model3.predict(X_val)\nmodel4_predictions = model4.predict(X_val)\nmodel5_predictions = model5.predict(X_val)\n\nmodel1_accuracy = accuracy_score(y_val, model1_predictions)\nmodel2_accuracy = accuracy_score(y_val, model2_predictions)\nmodel3_accuracy = accuracy_score(y_val, model3_predictions)\nmodel4_accuracy = accuracy_score(y_val, model4_predictions)\nmodel5_accuracy = accuracy_score(y_val, model5_predictions)\n\nprint(f'Accuracy Score is |{model1_accuracy}| for model |{model1}|')\nprint(f'Accuracy Score is |{model2_accuracy}| for model |{model2}|')\nprint(f'Accuracy Score is |{model3_accuracy}| for model |{model3}|')\nprint(f'Accuracy Score is |{model4_accuracy}| for model |{model4}|')\nprint(f'Accuracy Score is |{model5_accuracy}| for model |{model5}|')","15fc9c4d":"Now our data is ready.","4e079e53":"Regading the colors, we noticed that all rows consist of one or two combinations of the colors : ['Blue','White','Orange','Red','Yellow']\nSo we will create an attribut for each of them and put 1 where the color exist.","48016213":"# Data Preprocessing  \nWe will start by changing string columns.  \nIn the dataset link \"https:\/\/www.kaggle.com\/deepu1109\/star-dataset\", they said that Spectral Class (O,B,A,F,G,K,,M) and in this order represent info about spectral. like spectral O is close of spectral B.  \nSo we will represt this column by ordinal data as below:","db20d973":"Let's see how the columns affect the Star type.\n","b93bde7f":"Yeah 40 row for each type. We will check missing data.","5602ed2b":"## DecisionTreeClassifier","e6be2e42":"# Conclusion  \nAfter good preprocessing for the data, most of the classifier gives 100% accuracy. We figure out the best hyperparameter for each classifier.","d337de89":"Standardisation.","df3b19a9":"We got 100% accuracy for max depths 4.  \nWe will see the performance of other models also.","ad169312":"SVC with Radial basis function kernel we gor 0.96 accuaracy.","7768f732":"We have five main colors. and we have some modification to do regarding color.","1f8c705a":"# Model Evaluation ","15a2e304":"## KNeighborsClassifier","98e01eeb":"We will check by making some queries.","aff3ed02":"## SVC","a982cd6b":"# Read the data\nWe will import also the usual needed libraries.","48180606":"We have only 240 rows. and it seems that the target columns 'Star type' ditributed uniformally between rows. we will check.","a1553d8e":"Now we will drop the orginal 'Star color' attribute.","1e3f059a":"So these columns have high impact","53549409":"We have five spectral classes. ","90a58cbf":"Also Spectral Class have considerable impact.  \nWe will split the data into train, tes and validate data (0.6,0.2,0.2)","b01fe676":"Good news. no missing data.  \nWe will see the range of values for string data.","6b90a2ef":"# Basic Insight of Dataset","ee32d90c":"We will move the target column to the last.","574db38d":"We notice that we have two String columns that we have to move them to numerical.","f12490da":"# Model Development","8f20bf65":"## LogisticRegression","e6482e27":"Good the types are distrbuted uniformaly in train data","30b32cb3":"# Download the data\nWe followed the steps illustrated in the link:\nhttps:\/\/www.kaggle.com\/general\/74235. \nYou need to upload kaggle.json file."}}