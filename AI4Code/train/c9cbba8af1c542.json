{"cell_type":{"2d8414e2":"code","8345a9ae":"code","e93bbf18":"code","e4c0deb1":"code","b3954d43":"code","e1a3d091":"code","649cf51c":"code","7c9dbc27":"code","04e94c82":"code","746151a6":"code","2a4bfab3":"code","c6ed2dc5":"code","950b576d":"code","b9fcad2e":"code","aeaa0cf5":"code","301b3e20":"code","382482f7":"code","18cf4047":"code","18641c7e":"code","900657d4":"code","4cf489d5":"code","1727ac96":"code","5f0dcbc4":"code","2eb5b00b":"code","27769c7b":"code","0dfe8117":"code","e338a803":"code","db0f9d3d":"code","29e8d970":"code","f974ea24":"code","54393c98":"markdown","210d1085":"markdown","863cb1ed":"markdown"},"source":{"2d8414e2":"import pandas as pd","8345a9ae":"df = pd.read_csv('..\/input\/stock-sentiment-analysis\/Stock_Dataa.csv', encoding = \"ISO-8859-1\")","e93bbf18":"df.head()","e4c0deb1":"train = df[df['Date']< '20150101']\ntest = df[df['Date'] > '20141231']","b3954d43":"#removing punchuations\n\ndata = train.iloc[:,2:27]\ndata.replace(\"[^a-zA-Z]\",\" \", regex= True, inplace = True)","e1a3d091":"data.head()","649cf51c":"#renaming the column names by number 1-25 for easy access\nlist1 = [i for i in range(25)]\nnew_Index = [str(i) for i in list1]\ndata.columns = new_Index\ndata.head()","7c9dbc27":"#converting to lower case \nfor index in new_Index:\n    data[index] = data[index].str.lower()\ndata.head(1)    ","04e94c82":"#combine all 25 headlines to one paragraph\nheadlines = []\nfor row in range(0,len(data.index)):\n    headlines.append(' '.join(str(x) for x in data.iloc[row,0:25]))","746151a6":"headlines[0]","2a4bfab3":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.ensemble import RandomForestClassifier","c6ed2dc5":"## implement BAG OF WORDS\ncountvector=CountVectorizer(ngram_range=(2,2))\ntraindataset=countvector.fit_transform(headlines)","950b576d":"# implement RandomForest Classifier\nrandomclassifier=RandomForestClassifier(n_estimators=200,criterion='entropy')\nrandomclassifier.fit(traindataset,train['Label'])","b9fcad2e":"## Predict for the Test Dataset\ntest_transform= []\nfor row in range(0,len(test.index)):\n    test_transform.append(' '.join(str(x) for x in test.iloc[row,2:27]))\ntest_dataset = countvector.transform(test_transform)\npredictions = randomclassifier.predict(test_dataset)","aeaa0cf5":"## Import library to check accuracy\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n","301b3e20":"matrix=confusion_matrix(test['Label'],predictions)\nprint(matrix)\nscore=accuracy_score(test['Label'],predictions)\nprint(score)\nreport=classification_report(test['Label'],predictions)\nprint(report)","382482f7":"# USING RANDOM FOREST CLASSIFIER WITH TF-IDF VECTORIZER\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier","18cf4047":"## IMPLEMENTING TF-IDF VECTORIZER\ntfidf= TfidfVectorizer(ngram_range=(2,2))\ntraindataset= tfidf.fit_transform(headlines)","18641c7e":"# Implement RandomForestClassifier on traindataset\nrandom_classifier= RandomForestClassifier(n_estimators=200,criterion='entropy')\nrandom_classifier.fit(traindataset,train['Label'])","900657d4":"# WE WILL BE PERFORMING SAME STEPS FOR TEST DATA ALSO.\n\ntest_transform=[]\nfor row in range(0,len(test.index)):\n    test_transform.append(' '.join(str(x) for x in test.iloc[row,2:27]))\ntest_dataset= tfidf.transform(test_transform)\npredictions= random_classifier.predict(test_dataset)","4cf489d5":"# ACCURACY AFTER USING TF-IDF VECTORIZER\nmatrix= confusion_matrix(test[\"Label\"],predictions)\nprint(matrix)\nscore= accuracy_score(test[\"Label\"],predictions)\nprint(score)\nreport= classification_report(test['Label'],predictions)\nprint(report)","1727ac96":"from sklearn.naive_bayes import MultinomialNB\nnaive= MultinomialNB()","5f0dcbc4":"# WE WILL FIRST USE BAG OF WORDS MODEL FOR CONVERTING TEXT INTO VECTORS\n## IMPLEMENTING BAG OF WORDS MODEL\ncountvector= CountVectorizer(ngram_range=(2,2))\ntraindataset= countvector.fit_transform(headlines)","2eb5b00b":"# FITTING TRAIN DATA INTO  NAIVE BAYES CLASSIFIER \nnaive.fit(traindataset,train['Label'])","27769c7b":"# WE WILL BE PERFORMING SAME STEPS FOR TEST DATA ALSO.\n\ntest_transform=[]\nfor row in range(0,len(test.index)):\n    test_transform.append(' '.join(str(x) for x in test.iloc[row,2:27]))\ntest_dataset= countvector.transform(test_transform)\npredictions= naive.predict(test_dataset)","0dfe8117":"matrix= confusion_matrix(test[\"Label\"],predictions)\nprint(matrix)\nscore= accuracy_score(test[\"Label\"],predictions)\nprint(score)\nreport= classification_report(test['Label'],predictions)\nprint(report)","e338a803":"# NOW WE WILL USE TF-IDF VECTORIZER WITH NAIVE BAYES CLASSIFIER\ntraindataset= tfidf.fit_transform(headlines)","db0f9d3d":"naive.fit(traindataset,train['Label'])","29e8d970":"# WE WILL BE PERFORMING SAME STEPS FOR TEST DATA ALSO.\n\ntest_transform=[]\nfor row in range(0,len(test.index)):\n    test_transform.append(' '.join(str(x) for x in test.iloc[row,2:27]))\ntest_dataset= countvector.transform(test_transform)\npredictions= naive.predict(test_dataset)","f974ea24":"# ACCURACY AFTER USING TF-IDF VECTORIZER IN NAIVE BAYES CLASSIFIER\nmatrix= confusion_matrix(test[\"Label\"],predictions)\nprint(matrix)\nscore= accuracy_score(test[\"Label\"],predictions)\nprint(score)\nreport= classification_report(test['Label'],predictions)\nprint(report)","54393c98":"**USING NAIVE BAYES CLASSIFIER WITH TF-IDF VECTORIZER**","210d1085":"NAIVE BAYES CLASSIFIER****","863cb1ed":"**OVERALL ACCURACY= 85.185%\n\nWE CAN SEE THAT NAIVE BAYES CLASSIFIER WITH TF-IDF VECTORIZER IS GIVING THE HIGHEST ACCURACY i.e 85.185%**"}}