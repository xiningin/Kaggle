{"cell_type":{"323f5eed":"code","c6a0cc0c":"code","20bda610":"code","870c1701":"code","c761a917":"code","99e3859e":"code","127e25b6":"code","225113c5":"code","f862e7d7":"code","1d658ba1":"markdown","67ee60ef":"markdown","5742620a":"markdown","efa42cf1":"markdown","93bf522e":"markdown"},"source":{"323f5eed":"import numpy as np\nimport pandas as pd","c6a0cc0c":"df = pd.read_csv('\/kaggle\/input\/news-headlines-summary-from-select-12-sources\/full_data.csv')\ncolumns = ['source','author', 'description', 'url', 'requested_date', 'publishedAt',]\ndf.drop(columns, inplace=True, axis=1)\ndf.head()","20bda610":"# drop nan values\ndf = df.dropna()\n# resetting the index\ndf = df.reset_index(drop=True)\ndf.head()","870c1701":"# remove noise\nfor i in range(0,len(df['content'])):\n    if type(df['content'][i]) == str:\n        df['content'][i] = df['content'][i].replace('\\n','').replace('\\r','').replace('\/','')\n        df['title'][i] = df['title'][i].replace('\\n','').replace('\\r','').replace('\/','')\n    else:\n        print(str(df['content'][i]))\ndf.head()","c761a917":"#train_data_length = int(len(df['content'])*0.95)\nimport sklearn.model_selection as model_selection\ntrain_df, test_df = model_selection.train_test_split(df, train_size = 0.997)\ntrain_data = []\nfor i in range(0,len(train_df['content'])):\n    train_data.append((df['content'][i], df['title'][i]))\ntrain_data[2]","99e3859e":"!pip install headliner\n!pip install tensorflow_datasets","127e25b6":"from headliner.preprocessing.bert_preprocessor import BertPreprocessor\nfrom spacy.lang.en import English\n\n# use BERT-specific start and end token\npreprocessor = BertPreprocessor(nlp=English())\ntrain_prep = [preprocessor(t) for t in train_data]\ntargets_prep = [t[1] for t in train_prep]","225113c5":"from tensorflow_datasets.core.features.text import SubwordTextEncoder\nfrom transformers import BertTokenizer\nfrom headliner.model.bert_summarizer import BertSummarizer\nfrom headliner.preprocessing.bert_vectorizer import BertVectorizer\nfrom headliner.trainer import Trainer\n\n# Use a pre-trained BERT embedding and BERT tokenizer for the encoder \ntokenizer_input = BertTokenizer.from_pretrained('bert-base-uncased')\ntokenizer_target = SubwordTextEncoder.build_from_corpus(\n    targets_prep, target_vocab_size=2**13,  reserved_tokens=[preprocessor.start_token, preprocessor.end_token])\n\nvectorizer = BertVectorizer(tokenizer_input, tokenizer_target)\nsummarizer = BertSummarizer(num_heads=2,\n                            feed_forward_dim=512,\n                            num_layers_encoder=0,\n                            num_layers_decoder=4,\n                            bert_embedding_encoder='bert-base-uncased',\n                            embedding_size_encoder=768,\n                            embedding_size_decoder=768,\n                            dropout_rate=0.1,\n                            max_prediction_len=50)\nsummarizer.init_model(preprocessor, vectorizer)\n\ntrainer = Trainer(batch_size=2)\ntrainer.train(summarizer, train_data, num_epochs=10)","f862e7d7":"test_df = test_df.reset_index(drop=True)\nfor i in range(0, len(test_df['content'])):\n    print('t:',test_df['title'][i])\n    prediction = summarizer.predict(test_df['content'][i])\n    print('p:',prediction)","1d658ba1":"## 1.0 importing the data","67ee60ef":"## 1.3 splitting train data","5742620a":"## 1.1 cleaning the data","efa42cf1":"## 2.2 preprocessing","93bf522e":"## 2.3 training "}}