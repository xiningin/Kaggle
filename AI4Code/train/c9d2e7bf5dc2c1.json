{"cell_type":{"a5e01848":"code","0bf08459":"code","4276e0b3":"code","a6e717dc":"code","dbe7273d":"code","1e0d75d9":"code","11bdf673":"code","487aabdb":"code","8f6ce67e":"code","7b32533a":"code","d53cd054":"code","e4bd6db3":"code","0aa01924":"code","813a9527":"code","16574695":"code","5c986fba":"code","ef048e21":"code","866690cc":"code","5679bcf0":"code","85502911":"code","ca8a6a37":"code","797138bf":"code","a09b6b9f":"code","4b9f26c9":"code","1ddaa757":"code","a863353f":"code","9182b541":"code","a2f242f6":"markdown","cfa1d1dd":"markdown","5f263174":"markdown","2a7749a4":"markdown","f7066cc1":"markdown","a8fdf196":"markdown","5d313d31":"markdown","7f2a412c":"markdown","a64606e1":"markdown","1d1f4213":"markdown"},"source":{"a5e01848":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\nimport gc","0bf08459":"# Prohibited Variables (redlining regulations)\nprohibited = ['zip_code','addr_state']\n\n# Many Errors \/ Not Codified\nmany_errors = ['desc','emp_title','title']\n\n# Suspicious Data\n# next_pymnt_d: missing correlates suspiciously to default rate\n# issue_d and last_credit_pull_d are surpisingly and counterintuitively predictive\n# id: significant, maybe is proxy of tenure or time of Origination \/ drop however\n# member_id: same as id\nsuspect = ['next_pymnt_d', 'issue_d', 'last_credit_pull_d', 'id', 'member_id']\n\nunknown_at_origination = ['funded_amnt_inv', 'funded_amnt', 'int_rate', 'installment', 'grade', 'sub_grade',\n    'out_prncp', 'out_prncp_inv', 'total_pymnt', 'total_pymnt_inv',\n    'total_rec_prncp', 'total_rec_int', #'total_rec_late_fee', 'recoveries',\n    'collection_recovery_fee', 'last_pymnt_amnt', 'last_pymnt_d']\n\nzero_variance = ['policy_code','pymnt_plan'] # zero or near-zero variance\n\nquasi_separation = [\n    'open_il_6m', 'open_il_12m', 'open_il_24m', 'mths_since_rcnt_il', 'total_bal_il',\n    'il_util', 'open_rv_12m', 'open_rv_24m', 'max_bal_bc', 'all_util', 'inq_fi', 'inq_last_12m', \n    'open_acc_6m', 'total_cu_tl'\n]\n\nskipvars = prohibited + many_errors + suspect + unknown_at_origination + zero_variance + quasi_separation","4276e0b3":"dates = ['issue_d', 'last_pymnt_d', 'next_pymnt_d', 'last_credit_pull_d'] #,'earliest_cr_line']\n    # earliest_cr_line is not parsed correctly automatically\n    \ncred = pd.read_csv(\n    '..\/input\/Data File.csv',\n    usecols = lambda x: x not in skipvars,\n    parse_dates = list(set(dates) - set(skipvars)),\n    date_parser = lambda x: pd.to_datetime(x,format='%b-%y'),\n    low_memory=False\n)\nprint('Dataset Dimensions:',cred.shape)","a6e717dc":"# Remove Already Delinquent\n\ncred = cred[(cred.total_rec_late_fee == 0) & (cred.recoveries == 0)]","dbe7273d":"with pd.option_context('display.max_rows', 10, 'display.max_columns', None): \n    display(cred)","1e0d75d9":"y = cred['default_ind'].values\nprint(\"Mean Bad Rate = {:.2%}\".format(np.mean(y)))","11bdf673":"def read_earliest_cr_date(s):\n    year2 = int(s[4:])\n    y = year2 if year2 > 16 else year2 + 100\n    return 116 - y\n\ncred['years_since_first_credit'] = cred['earliest_cr_line'].apply(read_earliest_cr_date)\ncred.drop('earliest_cr_line', axis=1, inplace=True)","487aabdb":"# metadata\nmetadata = dict()\nfor v in cred.dtypes.index:\n    if v != \"default_ind\":\n        metadata.setdefault(str(cred.dtypes[v]), []).append(v)","8f6ce67e":"# Set missing to default values\ncred.fillna({col:'Missing' for col in metadata['object']}, inplace=True)\ncred.fillna({col:-99 for col in metadata['int64']}, inplace=True)\ncred.fillna({col:-999.0 for col in metadata['float64']}, inplace=True)","7b32533a":"# Convert Dates to Integers\nbase = pd.Timestamp('2000-01-01 00:00:00')\nyr = pd.to_timedelta(pd.Timestamp('2001-01-01 00:00:00')-base)\n\ntry:\n    for col in metadata['datetime64[ns]']:\n        cred[col+'_t'] = (cred[col]-base) \/ yr\n    cred.fillna({col+'_t':0 for col in metadata['datetime64[ns]']}, inplace=True)\n    metadata['dates_t'] = [col+'_t' for col in metadata['datetime64[ns]']]\nexcept KeyError:\n    metadata['dates_t'] = []\n    print('No datetime64[ns] Variables in the Model')","d53cd054":"predictors = [col for mgroup in ['int64','float64','dates_t'] for col in metadata[mgroup]]","e4bd6db3":"dum = pd.get_dummies(cred[metadata['object']])\ndum.shape","0aa01924":"cred = pd.concat([cred[predictors], dum],axis=1)\npredictors += list(dum.columns)\ndel dum\ngc.collect()","813a9527":"X_train, X_test, y_train, y_test = train_test_split(\n    cred.values,\n    y,\n    test_size=0.20,\n    random_state=19,\n    stratify = y\n)","16574695":"lgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n\n# specify your configurations as a dict\nparams = {\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': ['auc','binary_logloss'],\n    'max_depth':10,\n    'max_bin':255,\n    'learning_rate': 0.004,\n    'feature_fraction': 0.5,\n    'pos_bagging_fraction': 0.6,\n    'neg_bagging_fraction':0.05,\n    'random_seed':2019,\n    'min_data_in_leaf':400,\n    'bagging_freq': 1,\n    'verbosity': 0\n}","5c986fba":"out = dict()\ngbm = lgb.train(\n    params,\n    lgb_train,\n    num_boost_round=13000,\n    verbose_eval=500,\n    valid_sets=[lgb_train,lgb_eval],\n    feature_name=predictors,\n    evals_result = out\n)","ef048e21":"lgb.plot_metric(out, metric='auc');","866690cc":"lgb.plot_metric(out, metric='binary_logloss');","5679bcf0":"# Looks better in \"Edit Mode\"\n#lgb.create_tree_digraph(\n#    gbm, tree_index=0,\n#    show_info=('internal_value', 'internal_count', 'leaf_count'),\n#    precision = 3\n#)\n\n#You should be able to zoom in on this. Right-click on the graph.","85502911":"lgb.plot_tree(\n    gbm, tree_index=0,\n    show_info=('internal_value', 'internal_count', 'leaf_count'),\n    precision = 3,\n    figsize=(44,35)\n);","ca8a6a37":"lgb.plot_importance(gbm, importance_type=\"gain\", precision=0, max_num_features=30, figsize=(7,11));","797138bf":"#lst = sorted(zip(gbm.feature_importance(importance_type=\"gain\"),gbm.feature_name()))\n#[nm for imp, nm in lst if imp > 0]","a09b6b9f":"def sensitivity(on_column,column_names,X,model):\n    idx = [column_names.index(col) for col in on_column]\n    stats = [(i,np.mean(X[:,i]),np.std(X[:,i])) for i in idx]\n    res = []\n    for i, mean_, std_ in stats:\n        logdiff = []\n        print(\"{:20} Mean: {:9.2f} St.Dev.: {:9.2f}\".format(column_names[i], mean_, std_))\n        original_column = X[:,i]\n        X[:,i] = mean_ - 0.5 * std_\n        prob = model.predict(X)\n        logit0 = np.log( prob \/ (1-prob) )\n        X[:,i] = mean_ + 0.5 * std_\n        prob = model.predict(X)\n        logit1 = np.log( prob \/ (1-prob) )\n        srs = pd.Series(logit1-logit0, index = logit0, name = column_names[i])\n        res += [srs.sort_index()]\n        X[:,i] = original_column\n    return res\n\nsmp = np.random.choice(len(X_test),400)\nsens = sensitivity(['revol_util','total_rev_hi_lim'], predictors, X_test[smp], gbm)","4b9f26c9":"import matplotlib.pyplot as plt\n\nfor srs in sens:\n    avg = pd.Series(srs.mean(),index=srs.index)\n    plt.scatter(srs.index, srs, marker='.', label='Logit Diff')\n    plt.plot(avg.index,avg,'r--', label='Mean Effect')\n    plt.title(srs.name)\n    plt.legend()\n    plt.grid()\n    plt.show()","1ddaa757":"score_test = gbm.predict(X_test)\nscore = pd.DataFrame({\"true\":y_test,\"score\":score_test})\n\nscore.groupby(\"score\")[\"true\"].agg([\"sum\",\"size\"])[\"size\"].mean() # no score ties","a863353f":"buckets = 20\ngains_table = score.groupby(pd.qcut(score.score,buckets))[\"true\"].agg([\"sum\",\"size\"])\ngains_table['bad'] = gains_table['sum']\ngains_table['good'] = gains_table['size'] - gains_table['sum']\ngains_table['p_bad'] = gains_table['bad'] \/ gains_table['size']\ngains_table['p_good'] = gains_table['good'] \/ gains_table['size']\ngains_table = gains_table.reset_index(drop=True)\ngains_table = gains_table.iloc[::-1]","9182b541":"tmp = gains_table[[\"size\",\"bad\",\"good\"]].cumsum()\ntmp = tmp \/ tmp.iloc[-1]\ntmp.columns = [\"prop\",\"prop_bad\",\"prop_good\"]\ngains_table = pd.concat([gains_table,tmp],axis=1)\nplt.plot(gains_table.prop,gains_table.prop,'k:',label='prop')\nplt.plot(gains_table.prop,gains_table.prop_bad,'r-',label='prop_bad')\nplt.plot(gains_table.prop,gains_table.prop_good,'b-.',label='prop_good')\nplt.legend()\nplt.grid()\nplt.show()\ngains_table.index = gains_table[\"prop\"].apply(lambda x: \"{:.1%}\".format(x))\ngains_table = gains_table.drop([\"sum\",\"size\",\"bad\",\"good\",\"prop\"],axis=1)\ngains_table.style.format(\"{:.4f}\")","a2f242f6":"Refer to [Exploratory Data Analysis](https:\/\/www.kaggle.com\/yanpapadakis\/credit-default-risk-data-eda) for more information about the dataset we utilize.","cfa1d1dd":"## Sensitivity Analysis","5f263174":"## Gains Table","2a7749a4":"#### Below is a depiction of the first tree out of <num_boosting_round> in our ensemble.","f7066cc1":"---\n> END of Notebook","a8fdf196":"## Model Data Frame and Fitting","5d313d31":"validation AUC has reached maximum (we know this from previous runs with higher value for num_boosting_round)","7f2a412c":"<hr>\nAll Done! We have a model ready. ","a64606e1":"# Data Transformations","1d1f4213":"# CREDIT RISK DEFAULT DATA <br>Probability of Default (PD) Model\n## Gradient Boosting\n\n* *Warning: The model we develop is a \"black box\" model and should be treated as such for decision support purposes. The standard method for PD model development is Logistic Regression*"}}