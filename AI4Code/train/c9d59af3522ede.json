{"cell_type":{"471f885d":"code","36e76b6a":"code","951aaf14":"code","856a40be":"code","84a97bf2":"code","2b7dcdfb":"code","8bc9b2cf":"code","d23f1f6d":"code","613d7185":"code","82858670":"code","29e02516":"code","2690e58b":"code","1caa74d3":"code","67257ee7":"code","1d09a6eb":"code","136831b4":"code","9c91a960":"code","78567586":"code","7baa9486":"code","42ee4423":"code","d43d584c":"code","3a67158f":"code","44d68593":"code","44800f1b":"code","d2a6c853":"code","6c9e7037":"code","808011a2":"code","6d69ba25":"code","e7070858":"code","07438d67":"code","097df340":"code","76ca1b42":"code","451f813a":"code","35e3d891":"code","4c438144":"code","ae533a88":"code","e2cc9246":"code","33bebd6d":"code","0d5797ca":"code","012f1827":"code","cab76ee8":"code","e0025814":"code","7ec9bc5b":"code","ab6a082a":"code","5890653a":"code","d8864ecb":"code","928c255a":"code","0b575e5f":"code","569d80c9":"code","640d4220":"code","71af26a2":"code","d16bcc37":"code","2f604abd":"code","eb15f9fe":"code","031f8a86":"code","50daa2d0":"code","f8b36adb":"code","8a9b92c1":"code","37132acc":"code","c517c6c1":"code","81a6e6ab":"code","add9c78e":"code","a8d37413":"code","73b9e35e":"code","db8fadd2":"code","d152b64d":"code","6a8669ac":"code","ca42f177":"code","0b19cbc3":"code","5c5e3d29":"code","17506bae":"code","559a8d76":"code","5d3e3af7":"markdown","044c2fa9":"markdown","ca926f39":"markdown","e3a0a2b2":"markdown","230bb859":"markdown","41822b43":"markdown","a01dfa59":"markdown","a577630c":"markdown","0024f025":"markdown","c35bc595":"markdown","807b8fd0":"markdown","0d9cf6a0":"markdown","5f6646d9":"markdown","db4bceef":"markdown","c696a134":"markdown","deac8d61":"markdown","463df745":"markdown","8cfb8159":"markdown","ef958b6c":"markdown","850cdc8f":"markdown","50808c9d":"markdown","d4a78512":"markdown","52eb068c":"markdown","a793265f":"markdown","67e15c5d":"markdown","35d94d85":"markdown","513c6234":"markdown","4cc5271b":"markdown","cd177c50":"markdown","1fca8425":"markdown","872b5cbc":"markdown","02b8b251":"markdown","df28ae5c":"markdown","d63be39d":"markdown","da9b180e":"markdown","0899fe0f":"markdown","15d959d0":"markdown","41a2f9f3":"markdown","79ebe12f":"markdown","c4132ba0":"markdown","3ed1175f":"markdown","ec75dd7d":"markdown","cf5476a5":"markdown","b334b441":"markdown","372a71b6":"markdown","b981356f":"markdown","50a91a0a":"markdown","04fa398f":"markdown","814241b4":"markdown","003573fc":"markdown","7698351b":"markdown","9dc72a0e":"markdown","e6c0101b":"markdown","aebc8aa7":"markdown","3f2d91b5":"markdown","941ed976":"markdown","9617321e":"markdown","4c2b297e":"markdown"},"source":{"471f885d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","36e76b6a":"import pandas as pd\n\n# Plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline\n\n# Models\nfrom sklearn import tree\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# Over\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.multiclass import OneVsOneClassifier\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nfrom scipy.stats import norm\nfrom scipy import stats\n\nimport missingno as msno\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# To draw a tree\nfrom graphviz  import Source\nfrom IPython.display import SVG, display, HTML\nstyle = \"<style>svg{width: 40% !important; height: 50% !important;} <\/style>\"\nHTML( style )\nimport os\nos.environ[\"PATH\"] += os.pathsep + \"C:\\\\Program Files (x86)\\\\graphviz2.38\\\\bin\" + os.pathsep + \"C:\\\\Program Files (x86)\\\\graphviz2.38\"","951aaf14":"df = pd.read_csv('\/kaggle\/input\/mobile-price-classification\/train.csv')","856a40be":"pd.options.display.max_columns = df.shape[1] # let's show all columns\ndf.head()","84a97bf2":"msno.matrix(df)","2b7dcdfb":"df.isnull().sum()","8bc9b2cf":"df.describe()","d23f1f6d":"y = df['price_range']","613d7185":"sns.distplot(y, kde=False)","82858670":"df.info()","29e02516":"for i, j in enumerate(list(df)):\n    print(j, len(df[j].unique()))","2690e58b":"def categorical(df, a):\n    cat_col = []\n    for i in df.columns:\n        if len(df[i].unique()) <= a:\n            cat_col.append(i)\n    return(cat_col)","1caa74d3":"cat_columns = categorical(df, 8)","67257ee7":"df[cat_columns].hist(figsize=(15,10), bins=50)","1d09a6eb":"sns.heatmap(data=df[cat_columns].corr(), cbar=True, annot=True, square=True, annot_kws={'size': 10},)","136831b4":"df[df['three_g'] == 0]['four_g'].hist()","9c91a960":"num_columns = [x for x in df.columns if x not in cat_columns]\nfor i in num_columns:\n    sns.distplot(df[i])\n    plt.show()","78567586":"df_size = df[['sc_w', 'px_width', 'sc_h', 'px_height']]\ndf_size_not_zero = df_size.drop(df_size[(df_size['sc_w'] == 0) | (df_size['px_height'] == 0)].index)\ndf_size_not_zero.corr()\nsns.heatmap(data=df_size_not_zero.corr(), yticklabels=['sc_w           ', 'px_width      ', 'sc_h             ', 'px_height      '],\n           cbar=True, annot=True, square=True)","7baa9486":"df_size_not_zero['area_sc'] = df_size_not_zero['sc_w'] * df_size_not_zero['sc_h']\ndf_size_not_zero['area_px'] = df_size_not_zero['px_width'] * df_size_not_zero['px_height']\nsns.heatmap(data=df_size_not_zero.corr(), cbar=True, annot=True, square=True, annot_kws={'size': 10},)","42ee4423":"sns.jointplot(x='sc_w' , y='sc_h' , data=df_size_not_zero)","d43d584c":"df['sc_w'] = df[~df.apply(lambda x: x.eq(0))]['sc_w']\ndf['px_height'] = df[~df.apply(lambda x: x.eq(0))]['px_height']\ndf['sc_w'] = df.groupby('sc_h')['sc_w'].transform(lambda x: x.fillna(x.median()))\ndf['px_height'] = df.groupby('px_width')['px_height'].transform(lambda x: x.fillna(x.median()))","3a67158f":"fig = plt.figure(figsize=(15,12))\nr = sns.heatmap(df.corr(), cmap='Purples')\nr.set_title(\"Correlation \")","44d68593":"num_top10_corr = df.corr()['price_range'].sort_values(ascending=False).head(10).to_frame()\ncm = sns.light_palette(\"blue\", as_cmap=True)\ns = num_top10_corr.style.background_gradient(cmap=cm)\ns","44800f1b":"sns.boxplot(x='price_range', y='ram', data=df)","d2a6c853":"plt.figure(figsize=(10,6))\ndf['fc'].hist(alpha=0.5, color='blue', label='Front camera')\ndf['pc'].hist(alpha=0.5, color='red', label='Primary camera')\nplt.legend()\nplt.xlabel('MegaPixels')","6c9e7037":"g = sns.FacetGrid(df, col=\"dual_sim\", hue=\"price_range\", palette=\"Set1\", height=5)\ng = g.map(sns.distplot, \"ram\").add_legend()","808011a2":"g = sns.FacetGrid(df, col=\"touch_screen\", hue=\"price_range\", palette=\"Set1\", height=5)\ng = g.map(sns.distplot, \"ram\").add_legend()","6d69ba25":"g = sns.FacetGrid(df, col=\"wifi\", hue=\"price_range\", palette=\"Set1\", height=5)\ng = g.map(sns.distplot, \"ram\").add_legend()","e7070858":"g = sns.FacetGrid(df, col=\"blue\", hue=\"price_range\", palette=\"Set1\", height=5)\ng = g.map(sns.distplot, \"ram\").add_legend()","07438d67":"split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(df, df[\"price_range\"]):\n    df_train = df.loc[train_index]\n    df_test = df.loc[test_index]","097df340":"df_train[\"price_range\"].hist(figsize=(10,5), bins=10)\nprint('\u0420\u0430\u0437\u043c\u0435\u0440 train \u0432\u044b\u0431\u043e\u0440\u043a\u0438', df_train.size)","76ca1b42":"df_test[\"price_range\"].hist(figsize=(10,5), bins=10)\nprint('\u0420\u0430\u0437\u043c\u0435\u0440 test \u0432\u044b\u0431\u043e\u0440\u043a\u0438', df_test.size)","451f813a":"y_train = df_train['price_range']\nX_train = df_train.drop('price_range', axis=1)\ny_test = df_test['price_range']\nX_test = df_test.drop('price_range', axis=1)","35e3d891":"max_depth_values = range(1,100)\nscores_data = pd.DataFrame()\nbest_depth = 0\nbest_score = 0\nfor max_depth in max_depth_values:\n    clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=max_depth)\n    clf.fit(X_train, y_train)\n    train_score = clf.score(X_train, y_train)\n    test_score = clf.score(X_test, y_test)\n    if test_score > best_score:\n        best_score = test_score\n        best_depth = max_depth\n    temp_score_data = pd.DataFrame({'max_deph': [max_depth],\n                                   'train_score': [train_score],\n                                   'test_score': [test_score]})\n    scores_data = scores_data.append(temp_score_data)","4c438144":"scores_data_long = pd.melt(scores_data,\n                           id_vars=['max_deph'], \n                           value_vars=['train_score','test_score'],\n                           var_name=['set_type'], \n                           value_name='score')","ae533a88":"sns.lineplot(x='max_deph', y='score', hue='set_type', data=scores_data_long)","e2cc9246":"print(best_score, best_depth)","33bebd6d":"clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=best_depth)\nclf.fit(X_train, y_train)","0d5797ca":"graph = Source(tree.export_graphviz(clf,\n                                    out_file=None,\n                                    feature_names=list(X_train),\n                                    filled=True))\ndisplay(SVG(graph.pipe(format='svg')))","012f1827":"kf = KFold(n_splits=5, random_state=42, shuffle=True)","cab76ee8":"for k in range(1, 102, 10):\n    clf = RandomForestRegressor(n_estimators=k)\n    acc = cross_val_score(clf, X_train, y_train, cv=kf)\n    acc_mean = np.mean(acc)\n    print(acc_mean, k)","e0025814":"clf = RandomForestRegressor(n_estimators=50)\nclf.fit(X_train, y_train)\nclf.score(X_test, y_test)","7ec9bc5b":"n_list = []\nscore_list_knn = []\nfor n in range(1, 20):\n    clf = KNeighborsClassifier(n_neighbors=n)\n    a\u0441\u0441 = cross_val_score(clf, X_train, y_train, cv=kf)\n    score_list_knn.append(np.mean(a\u0441\u0441))\n    n_list.append(n)","ab6a082a":"knn = KNeighborsClassifier(n_neighbors=11)\nknn.fit(X_train,y_train)\nprint(knn.score(X_test,y_test))","5890653a":"for n in range(1, 102, 10):\n    clf = GradientBoostingClassifier(n_estimators=n)\n    acc = cross_val_score(clf, X_train, y_train, cv=kf)\n    print(np.mean(acc), n)","d8864ecb":"for n in range(151, 182, 10):\n    clf = GradientBoostingClassifier(n_estimators=n)\n    acc = cross_val_score(clf, X_train, y_train, cv=kf)\n    print(np.mean(acc), n)","928c255a":"skewness_data = []\nkurtosis_data = []\nfor i in num_columns:\n    print('-----', i ,'-----')\n    print(\"Skewness: %f\" % df_train[i].skew())\n    if abs(df_train[i].skew()) > 0.5:\n        print('Check skewness')\n        skewness_data.append(i)\n    print(\"Kurtosis: %f\" % df_train[i].kurt())\n    if abs(df_train[i].kurt()) > 3:\n        print('Check kurtosis')\n        kurtosis_data.append(i)\n    sns.distplot(df[i], fit=norm)\n    plt.show()","0b575e5f":"print('skewness_data', skewness_data)\nprint('kurtosis_data', kurtosis_data)","569d80c9":"df_num_sc = df_train[num_columns]","640d4220":"df_num_sc['px_height'] = np.sqrt(df_num_sc['px_height'])","71af26a2":"sns.distplot(df_num_sc['px_height'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_num_sc['px_height'], plot=plt)\nprint(df_num_sc['px_height'].skew())","d16bcc37":"df_num_sc['fc'] = np.sqrt(df_num_sc['fc'])\ndf_num_sc['sc_w'] = np.sqrt(df_num_sc['sc_w'])","2f604abd":"print(df_num_sc['fc'].skew())\nprint(df_num_sc['sc_w'].skew())","eb15f9fe":"scaler = StandardScaler()\ndf_temp = df_num_sc.copy()\ndf_num_sc[num_columns] = scaler.fit_transform(df_temp)","031f8a86":"sns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nax = sns.boxplot(data=df_num_sc[num_columns] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","50daa2d0":"df_train[num_columns] = df_num_sc","f8b36adb":"df_num_sc_t = df_test[num_columns]\ndf_num_sc_t['px_height'] = np.sqrt(df_num_sc_t['px_height'])\ndf_num_sc_t['fc'] = np.sqrt(df_num_sc_t['fc'])\ndf_num_sc_t['sc_w'] = np.sqrt(df_num_sc_t['sc_w'])\ndf_temp = df_num_sc_t.copy()\ndf_num_sc_t[num_columns] = scaler.fit_transform(df_temp)\ndf_test[num_columns] = df_num_sc_t","8a9b92c1":"y_train = df_train['price_range']\nX_train = df_train.drop('price_range', axis=1)\ny_test = df_test['price_range']\nX_test = df_test.drop('price_range', axis=1)","37132acc":"C_list = []\nScore_list = []\nfor C_ in np.arange(0.1, 11, 0.2):\n    clf = LogisticRegression(C=C_)\n    a\u0441\u0441 = cross_val_score(clf, X_train, y_train, cv=kf)\n    Score_list.append(np.mean(a\u0441\u0441))\n    C_list.append(C_)","c517c6c1":"plt.plot(C_list, Score_list)\nprint(max(Score_list))","81a6e6ab":"C_list = []\nScore_list = []\nfor C_ in np.arange(0.25, 10, 0.5):\n    clf = SVC(C=C_)\n    a\u0441\u0441 = cross_val_score(clf, X_train, y_train, cv=kf)\n    Score_list.append(np.mean(a\u0441\u0441))\n    C_list.append(C_)","add9c78e":"plt.plot(C_list, Score_list)\nprint(max(Score_list))","a8d37413":"C_list = []\nScore_list = []\nfor C_ in np.arange(0.1, 11, 0.2):\n    clf = OneVsOneClassifier(LogisticRegression(C=C_))\n    a\u0441\u0441 = cross_val_score(clf, X_train, y_train, cv=kf)\n    Score_list.append(np.mean(a\u0441\u0441))\n    C_list.append(C_)","73b9e35e":"plt.plot(C_list, Score_list)\nprint(max(Score_list))","db8fadd2":"clf = OneVsOneClassifier(LogisticRegression())\n\nparameters = {\n    'estimator__C': np.arange(0.1, 21, 0.2),\n    'estimator__penalty': ['l1','l2'],\n    'estimator__solver': ['saga']\n}\n\ngrid_logres = GridSearchCV(clf, param_grid=parameters, cv=kf)\ngrid_logres.fit(X_train, y_train)\nprint('best score: ', grid_logres.best_score_)\nprint('best param: ', grid_logres.best_params_)","d152b64d":"parameters = {\n    'C': np.arange(0.1, 6, 0.2),\n    'kernel': ['linear', 'rbf'],\n    'gamma': ['auto', 1, 0.1, 0.01, 0.001],\n    'decision_function_shape': ['ovo', 'ovr']\n}\n# Instead of the OneVsOneClassifier module, we use the SVC parameter - decision_function_shape","6a8669ac":"clf = SVC()\ngrid_svc = GridSearchCV(clf, param_grid=parameters)\ngrid_svc.fit(X_train, y_train)\nprint('best score: ', grid_svc.best_score_)\nprint('best param: ', grid_svc.best_params_)","ca42f177":"clf = grid_logres.best_estimator_\nclf.fit(X_train, y_train)","0b19cbc3":"clf.score(X_test, y_test)","5c5e3d29":"prediction = clf.predict(X_test)\nconf_mx = confusion_matrix(y_test, prediction)\nplt.matshow(conf_mx, cmap=plt.cm.gray)\nplt.show","17506bae":"row_sums = conf_mx.sum(axis=1, keepdims=True)\nnorm_conf_mx = conf_mx \/ row_sums # let's move from absolute indicators to relative\nnp.fill_diagonal(norm_conf_mx, 0)","559a8d76":"sns.heatmap(data=norm_conf_mx, cbar=True, annot=True, square=True)","5d3e3af7":"Let's pay our attention to the missing values ","044c2fa9":"Categorical features are mostly balanced, except is the presence of 3G. Most models have 3g.","ca926f39":"Distribution for binary traits are quite similar.","e3a0a2b2":"In the next version we will implement the pipeline.","230bb859":"Now apply to all data the standard transformation.","41822b43":"Most phones have a main camera. At the same time, about 700 models (35% of the sample) do not have a front camera.","a01dfa59":"Create a stratified test sample. And we can use accuracy as a metric.","a577630c":"I'm just beginner in this business. I look forward tp your questions, comments and feedback. It's very important for me!","0024f025":"Maybe the sc_w and px_widht correlate?","c35bc595":"Next, we will check the score for cross-validation.","807b8fd0":"It is expected that the larger the ram, the higher the cost of the phone.","0d9cf6a0":"It look like we were lucky. Make shure of that.","5f6646d9":"In this case, the optimal solution would be to replace zeros with the median value.","db4bceef":"Great. \nLet's see if there are any anomole values.","c696a134":"The best score was shown by the logistic regression model. Let's see what quality will be on the test sample.","deac8d61":"Although the forest of trees is almost not overfiting, we see that 50 trees will be enough for him. A further increase in this hyperparameter does not lead to an increase in accuracy.","463df745":"The price of a phone is most closely related to its RAM.","8cfb8159":"Repeat all steps for the test sample.","ef958b6c":"There is no explicit relationship between px_width and sc_w (","850cdc8f":"Since the skew is small, sqrt conversion will suffice","50808c9d":"Well, it looks like the screen resolution is clearly not related to its physical size. But in all the graphs, sc_w correlates with sc_h. Let's see our joint plot.","d4a78512":"**Before moving on to other classifiers, we transform our data.**\nFirst you need to make sure that the bevels and excesses are within normal limits, according to: https:\/\/codeburst.io\/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis- 388fef94eeaa","52eb068c":"Good, sample is perfectly balanced.\nSee on features.","a793265f":"This plot indicates that all phones have a higher resolution in height and a lower resolution in width","67e15c5d":"# Create train and test sets","35d94d85":"It is worth taking a closer look at the availability of 3g and 4g.","513c6234":"So, if the phone does not have 3G, then it does not have 4G. But in some models there is 3G, while 4G is not.","4cc5271b":"So, in our case, the rows are the actual classes, and the columns are the predicted classes. Most often, the classifier is mistaken in predicting the 1st and 2nd classes (Price_range 2, 3). The main conclusion that can be drawn is that the classifier can attribute the object in the worst case to a neighboring class. For example, an object of the 2nd class can be assigned to the 1st or 3rd, but it will never be assigned to the 1st class. This is important for our task because the target attribute is ranked.","cd177c50":"Well, the decisive rule was really the decisive factor. The graph shows that the hyperparameter of regularization can be further increased. Create a GridSearch, and compare the results with the l1 regularizer.","1fca8425":"Now we can try linear models.","872b5cbc":"Cross validation accuracy continues to increase.","02b8b251":"\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c GridSearch \u043d\u0430 SVM.","df28ae5c":"px_height parameter is important. Therefore, we will try to reduce its skew","d63be39d":"Alas, that was unnecessary.","da9b180e":"We will use another algorithm that does not require the transformation of features. Gradient Boost.","0899fe0f":"![image.png](attachment:image.png)","15d959d0":"Rate target variable.","41a2f9f3":"Maybe you should try to multiply the px_height and px_width","79ebe12f":"Yeah! There are anomalies. When analyzing the graphs, we noticed that the variable sc_w (screen width in pixels) has zero values, which is strange. Like px_height.","c4132ba0":"We focus on the mistakes.","3ed1175f":"although the screen height has only 15 unique values, it is hardly a categorical variable, I think that it is related to the size of the sample. Therefore, we take as a limitation the value of 8","ec75dd7d":"Find out if is there any correlation between these features.","cf5476a5":"On SVM, we did not get a high score. Before you abandon them, you should try changing the decision function. From the standard OneVsOne to OneVsRest. And also try polynomials.","b334b441":"Logistic Regression.","372a71b6":"Check what gives KNN, with a different number of neighbors.","b981356f":"features:\n* id:ID\n* battery_power:Total energy a battery can store in one time measured in mAh\n* blue:Has bluetooth or not\n* clock_speed:speed at which microprocessor executes instructions\n* dual_sim:Has dual sim support or not\n* fc:Front Camera mega pixels\n* four_g:Has 4G or not\n* int_memory:Internal Memory in Gigabytes\n* m_dep:Mobile Depth in cm\n* mobile_wt:Weight of mobile phone\n* n_cores:Number of cores of processor\n* pc:Primary Camera mega pixels\n* px_height:Pixel Resolution Height\n* px_width:Pixel Resolution Width\n* ram:Random Access Memory in Megabytes\n* sc_h:Screen Height of mobile in cm\n* sc_w:Screen Width of mobile in cm\n* talk_time:longest time that a single battery charge will last when you are\n* three_g:Has 3G or not\n* touch_screen:Has touch screen or not\n* wifi:Has wifi or not","50a91a0a":"Fine. Apply this transformation to all beveled features.","04fa398f":"Let's see confusion matrix.","814241b4":"![image.png](attachment:image.png)","003573fc":"Fine. Let's start with a model for which you do not need to transform attributes. Decision tree. Just see at what height it starts overfitting.","7698351b":"Overfitting comes after 6 leaves in depth.","9dc72a0e":"Unexpectedly, on a test sample, the result was better than on validation. Maybe we were lucky, but maybe due to the fact that we had 0.8 data on cross validation, and now that's it. For a dataset of our size, this can be significant.","e6c0101b":"Super)","aebc8aa7":"At first glance, values that clearly not true are not visible.","3f2d91b5":"Draw our tree and see which features are more important.","941ed976":"Well, we have prepared our features. Take a closer look at each of them. Maybe we'll see something interesting.","9617321e":"Take a look at what happens on the support vectors.","4c2b297e":"We haven't string features. So, we are going to find categorical features."}}