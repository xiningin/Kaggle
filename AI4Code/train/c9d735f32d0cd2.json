{"cell_type":{"e015b074":"code","c19a8564":"code","1a935b38":"code","6bc2e8b2":"code","8ce3d696":"code","74220e30":"code","cb960d93":"code","5be4800a":"code","f29f4cf1":"code","55853820":"code","8c0650b0":"code","2ee79ad6":"code","4c15b35b":"code","640a0b6b":"code","7614878a":"code","e8c3a342":"code","3df7b887":"code","2d8b1d38":"code","2c778a47":"code","a554e7da":"code","b2713e04":"code","5edcba18":"code","b4d38906":"code","c4b22282":"code","44c1da2d":"code","8b3ef413":"code","100507cb":"code","521b70a5":"code","d092f948":"code","20b4e082":"code","b86b0517":"code","52a57e19":"code","436a4087":"code","eaa5ba3e":"code","ac8d66ac":"code","a9729e70":"code","d467c12e":"code","7f4dd1c5":"code","02b6a6af":"code","3da3b68f":"code","6692695d":"code","f24610b6":"code","ff7b839b":"code","52d846b7":"code","1cd38747":"code","b8e49cfc":"code","f48e0ed8":"markdown","993ad204":"markdown","8578b374":"markdown","359ff916":"markdown","61b9b316":"markdown","70e045a9":"markdown","6c2a10f2":"markdown","c3e830f0":"markdown"},"source":{"e015b074":"import numpy as np\nimport pandas as pd\nimport cv2\nimport os\nfrom glob import glob","c19a8564":"images_path = '..\/input\/flickr8k-sau\/Flickr_Data\/Images\/'\nimages = glob(images_path+'*.jpg')\nlen(images)","1a935b38":"images[:5]","6bc2e8b2":"import matplotlib.pyplot as plt\n\nfor i in range(5):\n    plt.figure()\n    img = cv2.imread(images[i])\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.imshow(img)","8ce3d696":"from keras.applications import ResNet50\n\nincept_model = ResNet50(include_top=True)","74220e30":"from keras.models import Model\nlast = incept_model.layers[-2].output\nmodele = Model(inputs = incept_model.input,outputs = last)\nmodele.summary()","cb960d93":"images_features = {}\ncount = 0\nfor i in images:\n    img = cv2.imread(i)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    img = cv2.resize(img, (224,224))\n    \n    img = img.reshape(1,224,224,3)\n    pred = modele.predict(img).reshape(2048,)\n        \n    img_name = i.split('\/')[-1]\n    \n    images_features[img_name] = pred\n    \n    count += 1\n    \n    if count > 1499:\n        break\n        \n    elif count % 50 == 0:\n        print(count)\n    \n        \n    ","5be4800a":"len(images_features)","f29f4cf1":"caption_path = '..\/input\/flickr8k-sau\/Flickr_Data\/Flickr_TextData\/Flickr8k.token.txt'","55853820":"captions = open(caption_path, 'rb').read().decode('utf-8').split('\\n')","8c0650b0":"len(captions)","2ee79ad6":"captions_dict = {}\nfor i in captions:\n    try:\n        img_name = i.split('\\t')[0][:-2] \n        caption = i.split('\\t')[1]\n        if img_name in images_features:\n            if img_name not in captions_dict:\n                captions_dict[img_name] = [caption]\n                \n            else:\n                captions_dict[img_name].append(caption)\n            \n    except:\n        pass","4c15b35b":"len(captions_dict)","640a0b6b":"import matplotlib.pyplot as plt\n\nfor i in range(5):\n    plt.figure()\n    img_name = images[i]\n    \n    \n    img = cv2.imread(img_name)\n    \n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.xlabel(captions_dict[img_name.split('\/')[-1]])\n    plt.imshow(img)","7614878a":"import matplotlib.pyplot as plt\n\nfor k in images_features.keys():\n    plt.figure()\n    \n    img_name = '..\/input\/flickr8k-sau\/Flickr_Data\/Images\/' + k\n    \n    \n    img = cv2.imread(img_name)\n    \n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.xlabel(captions_dict[img_name.split('\/')[-1]])\n    plt.imshow(img)\n    \n    break","e8c3a342":"\ndef preprocessed(txt):\n    modified = txt.lower()\n    modified = 'startofseq ' + modified + ' endofseq'\n    return modified\n    ","3df7b887":"for k,v in captions_dict.items():\n    for vv in v:\n        captions_dict[k][v.index(vv)] = preprocessed(vv)","2d8b1d38":"count_words = {}\nfor k,vv in captions_dict.items():\n    for v in vv:\n        for word in v.split():\n            if word not in count_words:\n\n                count_words[word] = 0\n\n            else:\n                count_words[word] += 1","2c778a47":"len(count_words)","a554e7da":"THRESH = -1\ncount = 1\nnew_dict = {}\nfor k,v in count_words.items():\n    if count_words[k] > THRESH:\n        new_dict[k] = count\n        count += 1\n        ","b2713e04":"len(new_dict)","5edcba18":"new_dict['<OUT>'] = len(new_dict) ","b4d38906":"captions_backup = captions_dict.copy()","c4b22282":"captions_dict = captions_backup.copy()","44c1da2d":"for k, vv in captions_dict.items():\n    for v in vv:\n        encoded = []\n        for word in v.split():  \n            if word not in new_dict:\n                encoded.append(new_dict['<OUT>'])\n            else:\n                encoded.append(new_dict[word])\n\n\n        captions_dict[k][vv.index(v)] = encoded","8b3ef413":"captions_dict","100507cb":"from keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences","521b70a5":"MAX_LEN = 0\nfor k, vv in captions_dict.items():\n    for v in vv:\n        if len(v) > MAX_LEN:\n            MAX_LEN = len(v)\n            print(v)","d092f948":"MAX_LEN","20b4e082":"captions_dict","b86b0517":"Batch_size = 5000\nVOCAB_SIZE = len(new_dict)\n\ndef generator(photo, caption):\n    n_samples = 0\n    \n    X = []\n    y_in = []\n    y_out = []\n    \n    for k, vv in caption.items():\n        for v in vv:\n            for i in range(1, len(v)):\n                X.append(photo[k])\n\n                in_seq= [v[:i]]\n                out_seq = v[i]\n\n                in_seq = pad_sequences(in_seq, maxlen=MAX_LEN, padding='post', truncating='post')[0]\n                out_seq = to_categorical([out_seq], num_classes=VOCAB_SIZE)[0]\n\n                y_in.append(in_seq)\n                y_out.append(out_seq)\n            \n    return X, y_in, y_out\n    \n    ","52a57e19":"X, y_in, y_out = generator(images_features, captions_dict)","436a4087":"len(X), len(y_in), len(y_out)","eaa5ba3e":"X = np.array(X)\ny_in = np.array(y_in, dtype='float64')\ny_out = np.array(y_out, dtype='float64')\n\n","ac8d66ac":"X.shape, y_in.shape, y_out.shape","a9729e70":"X[1510]","d467c12e":"y_in[2]","7f4dd1c5":"\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.utils import plot_model\nfrom keras.models import Model, Sequential\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\nfrom keras.layers import Dropout\nfrom keras.layers.merge import add\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate\nfrom keras.models import Sequential, Model","02b6a6af":"embedding_size = 128\nmax_len = MAX_LEN\nvocab_size = len(new_dict)\n\nimage_model = Sequential()\n\nimage_model.add(Dense(embedding_size, input_shape=(2048,), activation='relu'))\nimage_model.add(RepeatVector(max_len))\n\nimage_model.summary()\n\nlanguage_model = Sequential()\n\nlanguage_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\nlanguage_model.add(LSTM(256, return_sequences=True))\nlanguage_model.add(TimeDistributed(Dense(embedding_size)))\n\nlanguage_model.summary()\n\nconca = Concatenate()([image_model.output, language_model.output])\nx = LSTM(128, return_sequences=True)(conca)\nx = LSTM(512, return_sequences=False)(x)\nx = Dense(vocab_size)(x)\nout = Activation('softmax')(x)\nmodel = Model(inputs=[image_model.input, language_model.input], outputs = out)\n\n# model.load_weights(\"..\/input\/model_weights.h5\")\nmodel.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\nmodel.summary()","3da3b68f":"model.fit([X, y_in], y_out, batch_size=512, epochs=50)","6692695d":"inv_dict = {v:k for k, v in new_dict.items()}","f24610b6":"model.save('model.h5')","ff7b839b":"model.save_weights('mine_model_weights.h5')","52d846b7":"np.save('vocab.npy', new_dict)","1cd38747":"def getImage(x):\n    \n    test_img_path = images[x]\n\n    test_img = cv2.imread(test_img_path)\n    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n\n    test_img = cv2.resize(test_img, (299,299))\n\n    test_img = np.reshape(test_img, (1,299,299,3))\n    \n    return test_img","b8e49cfc":"for i in range(5):\n    \n    no = np.random.randint(1500,7000,(1,1))[0,0]\n    test_feature = modele.predict(getImage(no)).reshape(1,2048)\n    \n    test_img_path = images[no]\n    test_img = cv2.imread(test_img_path)\n    test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n\n\n    text_inp = ['startofseq']\n\n    count = 0\n    caption = ''\n    while count < 25:\n        count += 1\n\n        encoded = []\n        for i in text_inp:\n            encoded.append(new_dict[i])\n\n        encoded = [encoded]\n\n        encoded = pad_sequences(encoded, padding='post', truncating='post', maxlen=MAX_LEN)\n\n\n        prediction = np.argmax(model.predict([test_feature, encoded]))\n\n        sampled_word = inv_dict[prediction]\n\n        caption = caption + ' ' + sampled_word\n            \n        if sampled_word == 'endofseq':\n            break\n\n        text_inp.append(sampled_word)\n        \n    plt.figure()\n    plt.imshow(test_img)\n    plt.xlabel(caption)","f48e0ed8":"# **Text Preprocess**","993ad204":"# **MODEL**","8578b374":"# **Visualize Images with captions**","359ff916":"# **Build Generator Function**","61b9b316":"# **Predictions**","70e045a9":"------------------------------------------------------------------------------------------------------","6c2a10f2":"# **Create Vocabulary**","c3e830f0":"# **image Preprocess**"}}