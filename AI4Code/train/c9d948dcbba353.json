{"cell_type":{"919a8190":"code","360d4754":"code","9201621e":"code","9ae59259":"code","be8a9fb9":"code","cee0eaa1":"code","7a43f335":"code","e2cff6b5":"code","524e0c95":"code","171b4f8d":"code","7d35234c":"code","59e05a98":"code","8213af99":"code","103f07dc":"code","d00fefa0":"code","da01dbe0":"code","5cbf5227":"code","5fc45e28":"code","f7beea8d":"code","05fcaea9":"code","1c21a928":"code","5a39ad23":"code","c4d7f994":"code","b10045f9":"code","9641fcfc":"code","92f0231d":"markdown","7aaefccd":"markdown","562f39a7":"markdown","442ca06c":"markdown","81077bd8":"markdown","86716f4d":"markdown","92ebcec4":"markdown","b0d0ea90":"markdown","d08ca5a8":"markdown","ca8fd76e":"markdown","f2adc0d3":"markdown","4e4efde2":"markdown","980967c2":"markdown","b274d560":"markdown","41c4c130":"markdown","5a64fd4d":"markdown","e43c2acc":"markdown","04b0395c":"markdown","0881ab24":"markdown","7e91c08c":"markdown","287e4309":"markdown","58cac1a6":"markdown","497c8f85":"markdown","16659788":"markdown","b3701f2b":"markdown","cfefd0a5":"markdown","8267c6b3":"markdown","62d8c9d3":"markdown","460a5f30":"markdown"},"source":{"919a8190":"from pathlib import Path\n\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport missingno as msno\n\nsns.set_style('whitegrid')\n%matplotlib inline\n\ntraining_dataset_path = Path(\"..\/input\/train.csv\")\ntesting_dataset_path= Path(\"..\/input\/test.csv\")\n\ntrain = pd.read_csv(training_dataset_path)\ntest = pd.read_csv(testing_dataset_path)\nprint(f\"Training dataset shape: {train.shape}\")\nprint(f\"Testing dataset shape: {test.shape}\")","360d4754":"categorical_features = train.select_dtypes(include=[np.object]).columns.values\nnumerical_features = train.select_dtypes(include=[np.number]).columns.values\n\nfeature_names = {\"Categorical\": categorical_features, \"Numerical\": numerical_features}\n# For readability sake, I transpose dataframe so categories are columns and features are rows\nfeature_types = pd.DataFrame.from_dict(feature_names, orient=\"index\").T\n\n# Fillna so last rows do not contain nans\ndisplay(feature_types.fillna(''))\ndisplay(feature_types.count())","9201621e":"ordinal_features = np.array([\n    \"ExterQual\",\n    \"ExterCond\",\n    \"BsmtQual\",\n    \"BsmtCond\",\n    \"BsmtExposure\",\n    \"HeatingQC\",\n    \"KitchenQual\",\n    \"Functional\",\n    \"FireplaceQu\",\n    \"GarageFinish\",\n    \"GarageQual\",\n    \"GarageCond\",\n    \"PavedDrive\",\n    \"PoolQC\",\n    \"Utilities\",\n    \"BsmtFinType1\",\n    \"BsmtFinType2\",\n    \"LandSlope\",\n    \"Electrical\",\n    \"Fence\"\n])","9ae59259":"nominal_features = np.setdiff1d(categorical_features, ordinal_features)","be8a9fb9":"feature_names = {\n    \"Ordinal\": ordinal_features,\n    \"Nominal\": nominal_features,\n    \"Numerical\": numerical_features,\n}\n\nfeature_types = pd.DataFrame.from_dict(feature_names, orient=\"index\").T\ndisplay(feature_types.fillna(''))\ndisplay(feature_types.count())","cee0eaa1":"full = pd.concat([train, test], keys=['train', 'test'], sort=False)\ndisplay(full.head())\ndisplay(full.tail())","7a43f335":"def nan_rows(dataset: pd.DataFrame):\n    nan_rows_count = dataset.isnull().any(axis=1).sum()\n    return nan_rows_count, nan_rows_count \/ len(dataset) * 100\n\n\ndef nan_features(dataset: pd.DataFrame):\n    nans_per_feature = dataset.isnull().sum().sort_values(ascending=False)\n    nan_features = nans_per_feature[nans_per_feature != 0].reset_index()\n    nan_features.columns = [\"Feature\", \"NaNs\"]\n    return nan_features\n\n\ndef nan_count(dataset: pd.DataFrame):\n    return dataset.isnull().sum().sum()\n\n\ndef display_nan_statistics(\n    dataset: pd.DataFrame, remove_target: bool = True, target_name: str = \"SalePrice\"\n):\n    if remove_target:\n        df = dataset.drop(target_name, axis=1)\n    else:\n        df = dataset\n    print(\"Dataset contains {} NaNs\".format(nan_count(df)))\n    print(\"NaN rows: {} | In percentage: {}\".format(*nan_rows(df)))\n    print(\"NaNs per feature:\")\n    display(nan_features(df))\n\n\ndisplay_nan_statistics(full)","e2cff6b5":"described_features = [\n  \"PoolQC\",\n  \"MiscFeature\",\n  \"Alley\",\n  \"Fence\",\n  \"FireplaceQu\",\n  \"GarageCond\",\n  \"GarageQual\",\n  \"GarageFinish\",\n  \"GarageType\",\n  \"BsmtCond\",\n  \"BsmtExposure\",\n  \"BsmtQual\",\n  \"BsmtFinType2\",\n  \"BsmtFinType1\",\n]\n\nfull.fillna(value={feature: \"ValueAbsent\" for feature in described_features}, inplace=True)\n\ndisplay_nan_statistics(full)","524e0c95":"full = full[full[\"Electrical\"].notnull()]","171b4f8d":"garage_data = full.filter(regex=\".*Garage.*\")\ngarage_data = garage_data.replace([\"ValueAbsent\", 0], np.NaN)\nmsno.heatmap(garage_data)","7d35234c":"def falsely_described_nans(dataset):\n  # If any feature in the dataset is NaN while other is not it will be returned.\n  indices = []\n  features = dataset.columns.tolist()\n  for index, nan_feature in enumerate(features):\n    for non_nan_feature in features[index:]:\n      df = dataset[\n                dataset[nan_feature].isnull() &\n                dataset[non_nan_feature].notnull()\n              ]\n      if not df.empty:\n        indices.extend(tuple(df.index.tolist()))\n\n  return dataset.loc[list(set(indices))].copy()","59e05a98":"false_nans = falsely_described_nans(garage_data)\ndisplay(false_nans)","8213af99":"# Change GarageYrBlt values to zero\nfull.fillna(value={\"GarageYrBlt\": 0}, inplace=True)\n# Get index and replace our imputed ValueAbsent with NaN \nfull.at[false_nans.index, [\"GarageYrBlt\", \"GarageFinish\", \"GarageQual\", \"GarageCond\"]] = np.NaN\n\ndisplay_nan_statistics(full)","103f07dc":"basement_data = full.filter(regex=\".*Bsmt.*\").copy()\nbasement_data.replace([\"ValueAbsent\"], np.NaN, inplace=True)\nmsno.heatmap(basement_data)","d00fefa0":"described_features = [\"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\"]\nbasement_described_data = basement_data[described_features]\n\nfalse_nans = falsely_described_nans(basement_described_data)\ndisplay(false_nans)\n\ndisplay(msno.heatmap(basement_described_data))","da01dbe0":"full.at[\n    false_nans.index,\n    [\"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\"],\n] = np.NaN\n\ndisplay_nan_statistics(full)","5cbf5227":"year_features=[ \"YearBuilt\", \"YearRemodAdd\", \"GarageYrBlt\" ]\nfull[year_features].describe()","5fc45e28":"fake_years = full[full[\"GarageYrBlt\"] > 2010]\ndisplay(fake_years)","f7beea8d":"full.at[fake_years.index, 'GarageYrBlt'] = 2007","05fcaea9":"fake_remodel = full[full[\"YearBuilt\"] > full[\"YearRemodAdd\"]][[\"YearBuilt\", \"YearRemodAdd\"]]\nfake_remodel","1c21a928":"full.at[fake_remodel.index, 'YearRemodAdd'] = 2002","5a39ad23":"# Go through all the possible contradictions to description:\ndef contradictive_area_quality(\n    dataset,\n    area_feature: str,\n    quality_feature: str,\n    absent_value_string: str = \"ValueAbsent\",\n):\n    area_no_quality = full[\n        (dataset[area_feature] != 0) & (full[quality_feature] == absent_value_string)\n    ].copy()\n\n    quality_no_area = full[\n        (dataset[area_feature] == 0) & (full[quality_feature] != absent_value_string)\n    ].copy()\n\n    return pd.concat([area_no_quality, quality_no_area])\n\nprint(\"Contradictive pools:\")\npools = contradictive_area_quality(full, \"PoolArea\", \"PoolQC\")\ndisplay(pools[[\"PoolArea\", \"PoolQC\"]])\n\nprint(\"Contradictive masonry veneer:\")\nveneers = contradictive_area_quality(full, \"MasVnrArea\", \"MasVnrType\", \"None\")\ndisplay(veneers[[\"MasVnrArea\", \"MasVnrType\"]])\n\nprint(\"Lot Area false variables\")\nzero_lot_data = full[full[\"LotArea\"] == 0].copy()\ndisplay(zero_lot_data)","c4d7f994":"# Pool Area looks legit, I suppose poll quality is simply missing\nfull.at[pools.index, \"PoolQC\"] = np.NaN\n\n# 1.0 veneer area is ridiculously small, maybe a simple mistakes, change to 0\nindices = pd.MultiIndex.from_tuples([(\"train\", 773), (\"train\", 1230), (\"test\", 992)])\nfull.at[indices, \"MasVnrArea\"] = 0\n\n# MasVnrType does not seem random, input NaN and leave it for imputation\nindices = pd.MultiIndex.from_tuples([(\"train\", 688), (\"train\", 1241), (\"test\", 859)])\nfull.at[indices, \"MasVnrArea\"] = np.NaN\n\n# MasVnrArea looks legit, the type seems to be simply missing\nindices = pd.MultiIndex.from_tuples(\n    [(\"train\", 624), (\"train\", 1300), (\"train\", 1334), (\"test\", 209)]\n)\nfull.at[indices, \"MasVnrType\"] = np.NaN","b10045f9":"ordinal_mapping = {\n  \"Ex\": 2,\n  \"Gd\": 1,\n  \"TA\": 0,\n  \"Fa\": -1,\n  \"Po\": -2,\n  \"ValueAbsent\": -3, # our designed NaN placeholder\n\n  \"Gd\":\t1,\n  \"Av\":\t0,\n  \"Mn\":\t-1,\n  \"No\":\t-2,\n\n  \"GLQ\": 3,\n  \"ALQ\": 2,\n  \"BLQ\": 1,\n  \"Rec\": 0,\n  \"LwQ\": -1,\n  \"Unf\": -2,\n\n  \"Typ\" : 3,\n  \"Min1\": 2,\n  \"Min2\": 1,\n  \"Mod\" : 0,\n  \"Maj1\": -1,\n  \"Maj2\": -2,\n  \"Sev\" : -3,\n  \"Sal\" : -4,\n\n  \"Fin\": 0,\n  \"RFn\": -1,\n  \"Unf\": -2,\n\n  \"Y\": 1,\n  \"P\": 0,\n  \"N\": -1,\n\n  \"AllPub\": 1,\n  \"NoSewr\": 0,\n  \"NoSeWa\": -1,\n  \"ELO\": -2,\n\n  \"Gtl\": 1,\n  \"Severe\": -1\n}\n\nfull.replace({\"LandSlope\": \"Sev\"}, \"Severe\")\nfull = full.replace(ordinal_mapping)\n\ndisplay_nan_statistics(full)","9641fcfc":"initially_preprocessed_train = \"..\/input\/initially_preprocessed_train.csv\"\ninitially_preprocessed_test = \"..\/input\/initially_preprocessed_test.csv\"\n\ntrain = full.loc[\"train\", :]\ntest = full.loc[\"test\", :]\n\n# Can't save here, you might do it locally though :)\n# train.to_csv(initially_preprocessed_train)\n# test.to_csv(initially_preprocessed_test)","92f0231d":"There are a few features I have decided to exclude from this group due to\nlack of necessary housing knowledge and\/or unclear descriptions, namely:\n\n- __Heating__\n- __RoofMatl__\n- __Exterior1st__\n\nIn case of reasonable doubt __I would rather NOT__ encode feature specific\nknowledge (ordinality in this case).\nFurthermore I think those three traits listed above vary  to infer any ordering.\n\nOur `nominal_features` will be a difference between `categorical` and it's `ordinal` subset, simply defined like:","7aaefccd":"Furthermore it should not be possible for __YearRemodAdd__ to be smaller than\n__YearBuilt__, cell below verifies it:","562f39a7":"Finally trivial recovery of nominal features by calculating the difference of\nsets, leaves us with the following separation:","442ca06c":"For faster analysis I will implement functions describing NaN values in our\ndataset (will be used extensively along the way).","81077bd8":"Data for this example seems to be flawed , so I am going to leave values of example `666` untouched. It cannot be deleted (as it's in the `test` set). Other data points will be left as they were (so imputed according to description). \n\nAdditionally I will impute `GarageYrBlt` with unique value `0` so models are able to learn it means garage missingness.\nOne could argue that missing garage is N times worse than garage built in a certain year. Usually, the later the garage was built, the worse for prices of the house. IMO this imputation is a good pragmatic choice for future predictive tasks.","86716f4d":"Features seem to be appropriately divided (later we will encode them appropriately). Now it's time for...","92ebcec4":"## 2.1 Imputting values according to description\n\nFor some features __NaN__ means absence of a trait, not a missing value. Usually you can read more about it in dataset's description. If you can't find such information, you should assume values are missing.\n\nI will imput absent values with `ValueAbsent`, accordingly to `data_description.txt` provided for this competition:","b0d0ea90":"Of course it did, let's fix it the same way it's been already done:","d08ca5a8":"Correlation is pretty high, hypothesis seems almost perfectly in-line with data.\n\nYou should spot something strange though; __GarageType's missingness__ does not\nperfectly correlate with other variables (although it should according to the description). All in all, if there is no garage (indicated by say `GarageType`) there should be no garage quality (`GarageQual`), right?\n\nYou will see this pattern more than once so I have created a function finding examples where one feature has `NaN` and other has `non-NaN` value even though both values should be equal:","ca8fd76e":"Once again, there are some records in which we should not trust as they are self-contradictory (according to\n`data_description.txt` at least). Same thing applies: we change all those feature values to `NaN`s for those specific examples.\n\nYou may also leave their data instead on imputting `NaN`s everywhere like I did, but I do not trust those records and would rather get them fully imputed (which might be the wrong move, it's your call).","f2adc0d3":"## 3. Re-input falsely described absent values\n\nAs you shall soon see there are some data points contradicting correctness of \ndata description given to us by the competition creators. I call those `falsely described absent values` as in reality those are __missing__, not absent as `data_description.txt` claims.\n\nI think you will understand the idea as I go through some examples:\n\n### 3.1 Garage\n\n__GarageYrBlt__ is not described in the data itself, but it may be that `NaNs` mean the garage is missing. \n\nTo verify this hypothesis I am going to take a subset of our dataset\nlinked to garage features, change imputed values to NaNs again and check missingness correlation.","4e4efde2":"# Error in data fixing and initial preprocessing\n\nIn this notebook I will focus solely on features and understanding them.\nIt is not strictly a beginner notebook, though you should be able to follow through. __Please at ;east read dataset description before checking this kernel!__\n\nIf you are looking for strictly beginners notebooks, check [this](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python), [this](https:\/\/www.kaggle.com\/bsivavenu\/house-price-calculation-methods-for-beginners) or [this](https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models).\n\n__Upvote if you found this notebook useful and want to pay kudos for this work, thanks!__\n\nLet's start by defining __what I will not do in this notebook__:\n\n- Finding importance of each feature in the context of the main task (predicting house price)\n- Transformations of targets and creative feature engineering\n- Finding outliers\n- Preprocessing of features to improve final score (though employing those steps __should be beneficial to the task__) like normalization or standardization\n\nThings listed above can be found in other great kernels (like the very popular [COMPREHENSIVE DATA EXPLORATION WITH PYTHON](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python)) and I wouldn't like to repeat points made over there.\n\n__Instead this kernel tries to focus on data from another perspective:__\n- Proper encoding of different features\n- Errors in data and contradictory deata description (some where missed by other publicly available kernels)\n- Explain reasoning and intuition standing behind each decision\n- Readable and easy to follow code following newest Python guidelines (many Kaggle solutions do not care about code quality unfortuantely)\n\nOkay, I hope you will have some fun and you will learn something as we go on this journey!\n\nLet's start by setting up some good old imports of libraries we'll need further down the line (you probably know most of them, don't you?):","980967c2":"It is hard to pinpoint the mistake here, my guess would be the house was not\nremodelled\nat all so I will change `YearRemodAdd` to reflect my assumption making it equal to `YearBuilt` value (you may decide to go the other wayy around and set both values to `2001`):","b274d560":"# 1. Divide dataset into nominal, ordinal and numerical features\n\nWe can move on to dividing our dataset based on features\ncategory, high level split would consist of:\n\n- __numerical features__ (containing numbers)\n- __categorical__ (other like objects of type `string`)\n\nLast four lines of code display names of features belonging to one of those categories:","41c4c130":"__Important:__ I am not going to `one-hot` dataset for now. It's a simple one-liner with `pandas` and I think analysis of missingness mechanisms should be done before tinkering with this representation. Furthermore, it would unnecessarily increase dataset size, hence I left it out of this kernel (and it's a pretty easy, boring task even).\n\nOh, and speaking of data imputation...","5a64fd4d":"**Remember: according to data description all `NaN`s above should mean absence of\ngarage, \nhence all of them should have `NA` in the same data points!** \n\nSee below to be sure it really isn't the case here:","e43c2acc":"Finally, let's split our dataset and save it separately to `train.csv` and `test.csv` after our initial analysis.","04b0395c":"__Categorical__ features can be further divided into:\n\n- __Ordinal__ - clear ordering of variable's values can be inferred. Intuitive example of such values\ncould be a triple: `['Bad', 'Average', 'Good']` with respective encodings `[-1, 0, 1]`. \n- __Nominal__ - variable's values cannot be reasonably ordered. Here hair colour would be a perfect example.\nIt would be hard to justify why `blonde` should be rated higher than `dark` hair. You should perform [One-Hot Encoding](https:\/\/en.wikipedia.org\/wiki\/One-hot) for those variables.\n\nBy employing this scheme, we can encode additional knowledge into the data. It often helps models as they don't have to infer it by themselves or sometimes they are even unable to do so. Yeah, you should take some time with encoding, trust me.\n\nTo create those distinctions one needs to manually explore each categorical trait. In my opinion here are\nthe ordinal variables:","0881ab24":"Matter seems more complicated than it was previously. I will focus on the described data\nfor now. Remember: __Either all variables should be missing or none for each example__. \n\nIf the case was different, it would mean the data is __trully missing__ and not __absent__ as description tells us:","7e91c08c":"## 4. Map ordinal features\n\nThe last thing I am going to do, for now at least,\nis changing ordinal variables to it's numeric counterparts. I have ordered each of the variables from best (higher score) to worst (lowest score). I have decided that absence of features __is worse__ than poor quality. IMO this assumption makes intuitive sense, though does not always hold...\n\nLet's imagine beautiful house with rotten pool, it would be better to have it absent in this case, wouldn't it? You might want to check out if this occurs in the dataset (like above, would be cool to see someone do this) and maybe you can find a better encoding scheme for those features than I did?","287e4309":"### 3.3 Fake year (great name indeed)\n\nYear is another type of feature where multiple predictors depend on each other.","58cac1a6":"Maximum value for `GarageYrBlt` is way above the possible one (2010, dataset\ncreation date), we should inspect anything above this threshold:","497c8f85":"### 3.2 Basement\n****\nThere are some predictors (variables) revolving around basement of the house. Their missing values should not cotradict each other either. \n\nLet's see how this goes...","16659788":"I agree with conclusions from [this\nkernel](https:\/\/www.kaggle.com\/laurenstc\/top-2-of-leaderboard-advanced-fe) \nand\nit probably is a typo and the year was meant to be 2007, I'm going to change it accordingly:","b3701f2b":"# 3.4 Fake pools, masonry and parking lots\n\nIf there is no pool quality there should be no pool (or so the description says). Same goes for masonry or parking lot.\n\nBelow is the code inspecting whether the description has hidden some `NaN`s from us once again:","cfefd0a5":"I have not found other features contradicting `description.txt` because of missingness. One might check for other contradictory data, though it would require much more work and might amount to manual inspection of many records.\n\nIf you did it, please post a link to your kernel below, would love to see!","8267c6b3":"# 5. Data imputation and final words\n\nNow, when we know what values are missing and where, we can move on to imputation.\n\nA great source of knowledge on the topic can be found in [Flexible Imputation of Missing Data](https:\/\/www.amazon.com\/Flexible-Imputation-Missing-Interdisciplinary-Statistics\/dp\/1439868247) by __Stef Van Buuren__. Inside it you can find why mean imputation is not good for your data (usually), what's MICE package is actually about, what's up with those MCAR, MAR, MNAR and what other SOTA approaches of handling those problems exist. __Highly recommended to at least skim through!__\n\n#### If you want me to make a kernel on data imputation this time using R language or you found this kernel useful, please upvote it. If you have some comments I would love to hear them, thanks for staying with me! :D","62d8c9d3":"One can see only __~17%__ of data is missing values right now.\n\n__A little cheat now__: example with missing __Electrical__ feature can be safely deleted, because:\n- It is in the training dataset (not required for making prediction)\n- Only one example will not affect predictive power of the algorithm one might later build\n- I cannot do any sensible analysis with __only one value__ missing and we should assume it's missing completely at random, hence can be safely deleted.\n\nYou should always be careful when deleting examples as it may change true distribution of underlying data. Usually it should be avoided (unless you have a good reason to do otherwise) as too large standard error might be introduced by doing so. \n\nFor each feature one should decide what's the mechanism standing behind missingnes (__Missing Completely At Random (MCAR)__ \/ __Missing At Random (MAR)__ or __Missing Not At Random__) and act approprietly (see [here](https:\/\/www.theanalysisfactor.com\/mar-and-mcar-missing-data\/) for some description). I will say a few words more about it at the end of this notebook.\n\n","460a5f30":"# 2. Locating missing values and fixing where possible\n\nI will join training and testing dataset for this section as it will be easier to work with it. "}}