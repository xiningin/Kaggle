{"cell_type":{"1204aff3":"code","ce64e903":"code","eb6ab6bc":"code","212966de":"code","ce7fb67d":"code","8da0d33a":"code","43b49591":"code","bf5defbc":"code","fb11a7bb":"code","7183b437":"code","b9f6f019":"code","ed65a181":"code","cefe8afb":"code","812ede80":"code","f4dcff80":"code","0944d2f0":"code","ea532f01":"code","d827f3be":"code","ee0d902a":"code","c5a53d16":"code","701d4a33":"code","3db60767":"code","0f99ae00":"code","c153f223":"code","f94e1175":"code","7517369f":"code","19daa933":"markdown","6a27e2a6":"markdown","610a5e1c":"markdown","a45c32bb":"markdown","ad00e48f":"markdown","3cdeedd4":"markdown","c6e03734":"markdown","44b87c00":"markdown","720dca9c":"markdown","6e5c9de3":"markdown","97c7fc42":"markdown"},"source":{"1204aff3":"# Asthetics\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nimport pandas as pd\npd.set_option('display.max_columns', None)\nimport numpy as np\nimport os\nfrom scipy.optimize import fmin as scip_fmin\n\n# Visialisation\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\n# Machine Learning\n\n# Utils\nfrom sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, cross_validate\nfrom sklearn.model_selection import cross_val_score, train_test_split, KFold\nfrom sklearn import preprocessing\nimport category_encoders as ce\n\n#Feature Selection\nfrom sklearn.feature_selection import chi2, f_classif, f_regression\nfrom sklearn.feature_selection import mutual_info_classif, mutual_info_regression\nfrom sklearn.feature_selection import SelectKBest, SelectPercentile, VarianceThreshold\n\n# Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nimport xgboost as xgb\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import StackingClassifier, VotingClassifier","ce64e903":"data_dir = '..\/input\/tabular-playground-series-may-2021'\n\ntrain_file_path = os.path.join(data_dir, 'train.csv')\ntest_file_path = os.path.join(data_dir, 'test.csv')\nsample_sub_file_path = os.path.join(data_dir, 'sample_submission.csv')\n\nprint(f'Train file: {train_file_path}')\nprint(f'Train file: {test_file_path}')\nprint(f'Train file: {sample_sub_file_path}')","eb6ab6bc":"RANDOM_SEED = 42","212966de":"def seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)","ce7fb67d":"seed_everything()","8da0d33a":"train_df = pd.read_csv(train_file_path)\ntest_df = pd.read_csv(test_file_path)\nsub_df = pd.read_csv(sample_sub_file_path)","43b49591":"train_df.shape","bf5defbc":"train_df.describe().T","fb11a7bb":"train_df.nunique()","7183b437":"ax = plt.subplots(figsize=(12, 6))\nsns.set_style(\"whitegrid\")\nsns.countplot(x='target', data=train_df);\nplt.ylabel(\"No. of Observations\", size=20);\nplt.xlabel(\"Target\", size=20);","b9f6f019":"mapping_dict = {'Class_1':1, 'Class_2':2, 'Class_3':3, 'Class_4':4}\ntrain_df['target'] = train_df['target'].map(mapping_dict)","ed65a181":"train_df.dtypes","cefe8afb":"not_features = ['id', 'target']\nfeatures = []\nfor feat in train_df.columns:\n    if feat not in not_features:\n        features.append(feat)\nprint(features)","812ede80":"scaler = preprocessing.StandardScaler()\nscaler.fit(train_df[features])\ntrain_df[features] = scaler.transform(train_df[features])\ntest_df[features] = scaler.transform(test_df[features])","f4dcff80":"NUM_SPLITS = 5\n\ntrain_df[\"kfold\"] = -1\ntrain_df = train_df.sample(frac=1).reset_index(drop=True)\ny = train_df.target.values\nkf = StratifiedKFold(n_splits=NUM_SPLITS)\nfor f, (t_, v_) in enumerate(kf.split(X=train_df, y=y)):\n    train_df.loc[v_, 'kfold'] = f\n    \ntrain_df.head()","0944d2f0":"# From https:\/\/github.com\/abhishekkrthakur\/approachingalmost\nclass UnivariateFeatureSelction:\n    def __init__(self, n_features, problem_type, scoring, return_cols=True):\n        \"\"\"\n        Custom univariate feature selection wrapper on\n        different univariate feature selection models from\n        scikit-learn.\n        :param n_features: SelectPercentile if float else SelectKBest\n        :param problem_type: classification or regression\n        :param scoring: scoring function, string\n        \"\"\"\n        self.n_features = n_features\n        \n        if problem_type == \"classification\":\n            valid_scoring = {\n                \"f_classif\": f_classif,\n                \"chi2\": chi2,\n                \"mutual_info_classif\": mutual_info_classif\n            }\n        else:\n            valid_scoring = {\n                \"f_regression\": f_regression,\n                \"mutual_info_regression\": mutual_info_regression\n            }\n        if scoring not in valid_scoring:\n            raise Exception(\"Invalid scoring function\")\n            \n        if isinstance(n_features, int):\n            self.selection = SelectKBest(\n                valid_scoring[scoring],\n                k=n_features\n            )\n        elif isinstance(n_features, float):\n            self.selection = SelectPercentile(\n                valid_scoring[scoring],\n                percentile=int(n_features * 100)\n            )\n        else:\n            raise Exception(\"Invalid type of feature\")\n    \n    def fit(self, X, y):\n        return self.selection.fit(X, y)\n    \n    def transform(self, X):\n        return self.selection.transform(X)\n    \n    def fit_transform(self, X, y):\n        return self.selection.fit_transform(X, y)\n    \n    def return_cols(self, X):\n        if isinstance(self.n_features, int):\n            mask = SelectKBest.get_support(self.selection)\n            selected_features = []\n            features = list(X.columns)\n            for bool, feature in zip(mask, features):\n                if bool:\n                    selected_features.append(feature)\n                    \n        elif isinstance(self.n_features, float):\n            mask = SelectPercentile.get_support(self.selection)\n            selected_features = []\n            features = list(X.columns)\n            for bool, feature in zip(mask, features):\n                if bool:\n                    selected_features.append(feature)\n        else:\n            raise Exception(\"Invalid type of feature\")\n        \n        return selected_features","ea532f01":"ufs = UnivariateFeatureSelction(\n    n_features=0.9,\n    problem_type=\"classification\",\n    scoring=\"f_classif\"\n)\n\nufs.fit(train_df[features], train_df['target'].values.ravel())\nselected_features = ufs.return_cols(train_df[features])","d827f3be":"def get_voting(vote_type='hard'):\n    models = list()\n    models.append(('Bernoulli', BernoulliNB(alpha=0.2)))\n    models.append(('rf', RandomForestClassifier(n_estimators = 500,\n                                                random_state=42)))\n    models.append(('xgb', xgb.XGBClassifier(max_depth=7,\n                                            n_estimators=1000,\n                                            colsample_bytree=0.8,\n                                            subsample=0.8,\n                                            learning_rate=0.1,\n                                            tree_method='gpu_hist',\n                                            gpu_id=0)))\n    models.append(('lgbm', LGBMClassifier(metric='multi_logloss',\n                                          objective='multiclass',\n                                          learning_rate=0.01,\n                                          seed=42,\n                                          n_estimators=1000)))\n    models.append(('cbc', CatBoostClassifier(verbose=0,\n                                             n_estimators=1000,\n                                             eval_metric='MultiClass',\n                                             task_type='GPU',\n                                             devices='0',\n                                             random_seed=42)))\n    models.append(('mlp', MLPClassifier()))\n    \n    ensemble = VotingClassifier(estimators=models, voting=vote_type)\n    return ensemble","ee0d902a":"def get_models():\n    models = dict()\n    models['Bernoulli'] = BernoulliNB(alpha=0.2)\n    models['rf'] = RandomForestClassifier(n_estimators = 500,\n                                          random_state=42,\n                                          n_jobs=-1)\n    models['xgb'] = xgb.XGBClassifier(max_depth=7,\n                                      n_estimators=1000,\n                                      colsample_bytree=0.8,\n                                      subsample=0.8,\n                                      nthread=-1,\n                                      learning_rate=0.1,\n                                      tree_method='gpu_hist',\n                                      gpu_id=0)\n    models['lgbm'] = LGBMClassifier(metric='multi_logloss',\n                                    objective='multiclass',\n                                    seed=42,\n                                    learning_rate=0.01,\n                                    n_estimators=1000)\n    models['cbc'] = CatBoostClassifier(verbose=0,\n                                       n_estimators=1000,\n                                       eval_metric='MultiClass',\n                                       task_type='GPU',\n                                       devices='0',\n                                       random_seed=42)\n    models['MLP'] = MLPClassifier()\n    models['voting'] = get_voting(vote_type='soft')\n    return models\n\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=1, random_state=42)\n    scores = cross_val_score(model, X, y, scoring='neg_log_loss', cv=cv, n_jobs=-1, error_score='raise')\n    return scores","c5a53d16":"%%time\n\nX = train_df[selected_features]\ny = train_df['target']\n\nmodels = get_models()\nresults = []\nnames = []\n\nfor name, model in models.items():\n    scores = -evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print(f'{name} : {round(np.mean(scores),3)} ({round(np.std(scores),3)})')","701d4a33":"ax = plt.subplots(figsize=(12, 6))\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","3db60767":"mean_scores = []\nfor score in results:\n    mean_scores.append(round(np.mean(score),3))\nmin_index = mean_scores.index(min(mean_scores))\nmodel_name = names[min_index]","0f99ae00":"print(f'Best Score: {mean_scores[min_index]}')\nprint(f'Best Model: {model_name}')","c153f223":"%%time\n\nmodels = get_models()\nclf = models[model_name]\nX = train_df[selected_features]\ny = train_df['target']\n\nclf.fit(X, y)\npreds = clf.predict_proba(test_df[selected_features])\nsubmission = pd.DataFrame()\nsubmission['id'] = test_df['id']\nsubmission['Class_1'] = preds[:, 0]\nsubmission['Class_2'] = preds[:, 1]\nsubmission['Class_3'] = preds[:, 2]\nsubmission['Class_4'] = preds[:, 3]","f94e1175":"submission.head()","7517369f":"submission.to_csv(\"ensemble_sub.csv\",index=False)","19daa933":"# Submission","6a27e2a6":"# Descriptive Analysis\n\nLet's do some descriptive analysis on the dataset...","610a5e1c":"Thus there are no missing values in any of the features... Good! Less work for us... \ud83d\ude04","a45c32bb":"# Models Benchmarking","ad00e48f":"# Target Encoding","3cdeedd4":"Since features are already in numric format, we do not need to convert them into numbers anymore...\n# Feature Scaling\nTo bring all features into a similar scale let's use simple scaler to scale all the features...","c6e03734":"# Features Selection\nWe need to select only the important features for better performance of the model. As unnecessary in best case scenario will not add to any productive calculation of the algorithm or in worst case scenario 'confuse' the model.\n\nTo do the same let's create a wrapper class that has all the built in statistical tests required to perform feature selection and takes some basic inputs from user and spits out the required features.","44b87c00":"# KFold Splits\nBefore we move on to feature engineering, it is always a good idea to perform cross validation splits. In that way, we will not risk any data leakage and would be more certain of the validation set being aptly represenative of the real world unknown data.","720dca9c":"Almost all features look like of the categorical nature with various cardinality values... Since the is no feature names we do not definitively know which ones are ordinal and which ones are nominal.  \nSo for sake of simplicity, let's assume all of them are nominal...","6e5c9de3":"**If you found this notebook useful and use parts of it in your work, please don't forget to show your appreciation by upvoting this kernel. That keeps me motivated and inspires me to write and share these public kernels.** \ud83d\ude0a","97c7fc42":"# Why this Competition?\nThis competition provides an unique oppertunity for Data Science beginners to participate in a Hackathon style challenge. It also provides the unique oppertunities for beginners to get their hands dirty and indulge is practical application of ML and do one of the basic tasks machine learning algorithms are capable of doing:- Classification.\n\nThis competition has the right mix to Catergorical and Numerical features we might expect in a practical problem and this helps us know how to leverage both of thhem in conjugation for a Classification task.\n\n# Problem Statement\nThe goal of this competition is to provide a fun, and approachable for anyone, tabular dataset. These competition will be great for people looking for something in between the Titanic Getting Started competition and a Featured competition.\n\nThe dataset is used for this competition is synthetic but based on a real dataset and generated using a CTGAN.\n\nSo we are sort of dealing with a variation of actual real-world data and here as Data Scientists are expected to predict the Multi-Class Classification based on these features.\n\n## Expected Outcome:-\n* Build a model to predict the category on an eCommerce product given various attributes about the listing.\n* Grading Metric: Multi-Class Log Loss\n\n## Problem Category:-\nFrom the data and objective its is evident that this is a Multi-Class Classification Problem in the Tabular Data format.\n\nSo without further ado, let's now start with some basic imports to take us through this journey:-"}}