{"cell_type":{"267cb782":"code","93760677":"code","8275fa31":"code","b5658939":"code","d9e49843":"code","411bb681":"code","b6ca065e":"code","d30b8a93":"code","265e6c61":"code","22a725a1":"code","0eb86bcb":"code","b858f107":"code","59769ebd":"code","ad726619":"code","d168f97f":"code","69c439a7":"code","7f27a30c":"code","7813d54f":"code","ca7a47ac":"code","3db42103":"code","6ab17430":"code","7eff08f1":"code","90315929":"code","c4ec9e29":"code","eec6aab8":"code","08c5455e":"code","a184f100":"code","7512c14f":"code","b3352a37":"code","96977fb8":"code","62c4c166":"code","2129cffb":"code","1cb47f77":"code","9a8228b2":"code","bbee25b0":"code","c475e8ec":"code","682ac248":"code","3e220a38":"code","79d8e194":"code","995162a1":"code","1fbeeefb":"code","f197d601":"code","d320cac2":"code","e7ef9a47":"code","00a00209":"markdown","d0e8cb16":"markdown","bd2f5e60":"markdown","97e7037f":"markdown","477a3826":"markdown","e3c9d58a":"markdown","4c73e7fc":"markdown","e349985f":"markdown","fbf9c792":"markdown","62c47a44":"markdown","03511c2a":"markdown","f8117b27":"markdown","fc0d910d":"markdown","6eab503d":"markdown","d28cba03":"markdown","e09602a0":"markdown","95a91423":"markdown","7d2bf688":"markdown","27481c08":"markdown","bf96a3e5":"markdown","5bfdcb1d":"markdown","7cf2ffdd":"markdown","6a374c2c":"markdown"},"source":{"267cb782":"import pandas as pd\nimport numpy as np\nimport json\nimport os\nimport glob\nfrom pathlib import Path, PurePath\nfrom ipywidgets import interact\nimport ipywidgets as widgets\nfrom IPython.display import FileLink\nfrom collections import defaultdict","93760677":"from sklearn.decomposition import LatentDirichletAllocation","8275fa31":"!pip install transformers -q #==2.2.2\n\nfrom transformers import *\nimport logging\nimport torch\nfrom tqdm.notebook import tqdm","b5658939":"!pip install bert-extractive-summarizer -q\nfrom summarizer import Summarizer\n\n!pip install --upgrade git+https:\/\/github.com\/zalandoresearch\/flair.git -q\nfrom flair.data import Sentence\nfrom flair.embeddings import BertEmbeddings,DocumentPoolEmbeddings","d9e49843":"!pip install whoosh -q\nimport whoosh\nfrom whoosh.qparser import *\nfrom whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED, NUMERIC, NGRAMWORDS\nfrom whoosh.analysis import StemmingAnalyzer,StandardAnalyzer, NgramFilter\nfrom whoosh import index","411bb681":"def setup_local_data():\n  input_dir = '\/kaggle\/input\/'\n  for item in list(Path(input_dir).glob('*')):\n    print(item)\n  return input_dir\n\n# this loads the precomupted dataframe that includes the original metadata along with features for bert summaries and scibert embeddings\ndef read_full_data_json(input_dir):\n  path = input_dir + 'covid19-corpus-data\/full_metadata_with_scibert_embeddings_v2.json'\n  data = pd.read_json(path)\n  return data\n\ndef read_metadata_csv(input_dir):\n    metadata_path = input_dir + '\/metadata.csv'\n    metadata = pd.read_csv(metadata_path,\n                           dtype={'cord_uid':str,\n                                  'sha':str,\n                                  'publish_time': str, \n                                  'authors':str,\n                                  'title': str,\n                                  'abstract':str,\n                                  'url': str},\n                           parse_dates = ['publish_time']\n                          )\n    #get publish year\n    metadata['publish_year'] = pd.DatetimeIndex(metadata['publish_time']).year\n    #set the abstract to the paper title if it is null\n    metadata['abstract'] = metadata['abstract'].fillna(metadata['title'])\n    #remove if abstract is empty or contains only one word\n    metadata = metadata.dropna(subset=['abstract'], axis = 0)\n    metadata['number_tokens'] = metadata['abstract'].apply(lambda x: len(x.split()))\n    metadata = metadata[metadata['number_tokens']>1].reset_index(drop=True)\n    return metadata","b6ca065e":"local_dir = setup_local_data()","d30b8a93":"# run the below code to load the metadata from the official data set\n\n# initial_metadata = read_metadata_csv(local_dir + '\/CORD-19-research-challenge')\n# print(initial_metadata.info())","265e6c61":"# Load model, model config and tokenizer via Transformers\ndef create_custom_model_and_tokenizer(pretrain_model):\n    custom_config = AutoConfig.from_pretrained(pretrain_model)\n    custom_config.output_hidden_states = True\n    custom_tokenizer = AutoTokenizer.from_pretrained(pretrain_model)\n    custom_model = AutoModel.from_pretrained(pretrain_model, config=custom_config)\n    return custom_model, custom_tokenizer\n\ndef extract_summary(text, custom_model=None, custom_tokenizer=None):\n    model = Summarizer(custom_model=custom_model, custom_tokenizer=custom_tokenizer)\n    return model(text)","22a725a1":"# instantiate custom model and tokenizer\n\n# sciBert, sciBert_tokenizer = create_custom_model_and_tokenizer('allenai\/scibert_scivocab_uncased')","0eb86bcb":"#extract summaries from the abstracts\n#chunk processing due to long processing time and possible notebook runtime shutdowns\ndef get_abstract_bert_summaries_and_save(num_chunks=20):\n  chunks = np.array_split(initial_metadata, num_chunks)\n  for index in range(len(chunks)):\n      chunk = chunks[index].reset_index()\n      summary_list = []\n      for i in tqdm(chunk.index):\n          summary = extract_summary(chunk.iloc[i]['abstract'], \n                                    custom_model=sciBert, \n                                    custom_tokenizer=sciBert_tokenizer)\n          summary_list.append(\n              {\n                  \"cord_uid\": chunk.iloc[i]['cord_uid'],\n                  \"sha\": chunk.iloc[i]['sha'],\n                  \"summary\": summary\n              }\n          )\n      summary_df = pd.DataFrame(data=summary_list)\n      summary_df.to_json(local_dir + 'abstract_summaries_part{}.json'.format(index))","b858f107":"# uncomment below if you need to rerun bert summaries\n# we've included the files for you already to save time since it takes >10 hours\n\n# get_abstract_bert_summaries_and_save()","59769ebd":"# get scibert embeddings of the abstract summaries\ndef get_embeddings(text, model):\n  sentence = Sentence(text)\n  document_embedding = DocumentPoolEmbeddings([model],\n                                             pooling= 'mean')\n  document_embedding.embed(sentence)\n  # now check out the embedded sentence.\n  return sentence.get_embedding().data.numpy()\n\ndef get_scibert_embeddings_and_save(emb_model, chunks=3):\n  files = glob.glob(local_dir+'\/covid19-corpus-data\/AbstractSummaries\/*.json')\n\n  for index in range(len(files)):\n      df = pd.read_json(files[index]).dropna(subset=['summary']).reset_index(drop=True)\n\n      chunks = np.array_split(df, CHUNKS_COUNT)\n      \n      for chunk_idx in range(len(chunks)):\n        chunk = chunks[chunk_idx].reset_index()\n        emb_list=[]\n        for i in tqdm(chunk.index):\n          try:\n            embedding = get_embeddings(chunk.iloc[i]['summary'], emb_model)\n            emb_list.append(\n            {\n              \"cord_uid\": chunk.iloc[i]['cord_uid'],\n              \"sha\": chunk.iloc[i]['sha'],\n              \"scibert_emb\": embedding\n            })\n          except RuntimeError:\n            emb_list.append(\n            {\n              \"cord_uid\": chunk.iloc[i]['cord_uid'],\n              \"sha\": chunk.iloc[i]['sha'],\n              \"scibert_emb\": np.nan\n            })\n        emb_df = pd.DataFrame(data=emb_list)\n        emb_df.to_json(local_dir+'covid19-corpus-data\/AbstractEmbeddings\/abstract_embeddings_part{}_{}.json'.format(index, chunk_idx),\n                      default_handler=str)\n        del emb_df","ad726619":"# uncomment below if you need to rerun scibert embeddings\n# we've included the files for you already to save time since it takes >3 hours\n\n# emb_model = BertEmbeddings(bert_model_or_path=\"allenai\/scibert_scivocab_uncased\", layers='-2')\n# get_scibert_embeddings_and_save(emb_model)","d168f97f":"#now that we have the summaries and embeddings, we will combine them into one main df\n\n# read preprocessed SciBERT embeddings\ndef read_summary_data(input_dir):\n  summary_path = input_dir+'covid19-corpus-data\/AbstractSummaries' \n  summaries = pd.concat([pd.read_json(f) for f in Path(summary_path).glob('*')]).reset_index(drop=True)\n  return summaries\n\ndef read_embeddings(input_dir):\n  vector_path = input_dir+'covid19-corpus-data\/AbstractEmbeddings' \n  embeddings = pd.concat([pd.read_json(f) for f in Path(vector_path).glob('*')]).reset_index(drop=True)\n  return embeddings\n\n# uncomment below to load if needed\n\n# summaries = read_summary_data(local_dir)\n# embeddings = read_embeddings(local_dir)\n# print(summaries.info())\n# print(embeddings.info())","69c439a7":"'''\nthis function will attempt to merge metadata with summaries\/embeddings data based\non uid or sha. It will also correct for any nulls\n'''\ndef merge_metadata_with_summaries_and_embeddings(metadata, summaries, embeddings):\n  merged = metadata.merge(summaries, on=['cord_uid','sha']).merge(embeddings, on=['cord_uid','sha'])\n  #for debugging\n  # print(merged.info())\n  # print(merged[pd.isnull(merged['scibert_emb'])]['summary'])\n  # print(merged[pd.isnull(merged['scibert_emb'])]['abstract'])\n  for row in merged.loc[merged['scibert_emb'].isnull(), 'scibert_emb'].index:\n    try:\n      merged.at[row, 'scibert_emb'] = get_embeddings(merged.iloc[row]['abstract'], emb_model)\n    except RuntimeError:\n      #truncate articles have very long abstracts that exceeds bert's sequence length limit\n      merged.at[row, 'scibert_emb'] = get_embeddings(merged.iloc[row]['abstract'][:512], emb_model)\n  return merged","7f27a30c":"# uncomment below to run code that will combine summaries, embeddings, and metadata csv into one json\n\n# merged = merge_metadata_with_summaries_and_embeddings(initial_metadata, summaries, embeddings)\n# merged.info()\n# # save this combined data so we can reuse it below in main application code\n# merged.to_json('\/kaggle\/working\/full_metadata_with_scibert_embeddings.json')\n# del merged","7813d54f":"#uncomment below to download the file\n\n# os.chdir(r'\/kaggle\/working')\n# merged.to_json(r'full_metadata_with_scibert_embeddings.json')\n# FileLink(r'full_metadata_with_scibert_embeddings.json')","ca7a47ac":"local_dir = setup_local_data()\nfull_data = read_full_data_json(local_dir)\nprint(full_data.info())\nprint(full_data.shape)\nprint(full_data.head(5))","3db42103":"#to view how long each scibert embedding is\nprint(len(full_data.iloc[0]['scibert_emb'])) ","6ab17430":"#now we need to create a new df of just n sample x scibert embedding features\ndf = pd.DataFrame(data=full_data['scibert_emb'].tolist(), index=full_data['title'])\nprint(df.shape)\ndf.head(5)","7eff08f1":"import pickle\n\ndef get_lda_model(num_topics):\n    lda_model = LatentDirichletAllocation(n_components=num_topics, max_iter=10, learning_method='online')\n    return lda_model\n\n# this loads an lda model that has already been run with 10 topics to save time re-running.\ndef get_lda_model_saved():\n    saved_lda_model = pickle.load(open('\/kaggle\/input\/covid19-corpus-data\/saved_lda_model.pk', 'rb'))\n    return saved_lda_model\n\ndef save_model(model, file_name):\n    pickle.dump(model, open(f\"\/kaggle\/working\/{file_name}.pk\", 'wb'))","90315929":"# Since LDA requires non-negative values, we perform unity-based normalization, essentially standardizes from 0 to 1, on the dataset\n# https:\/\/stats.stackexchange.com\/questions\/70801\/how-to-normalize-data-to-0-1-range\n# https:\/\/datascience.stackexchange.com\/questions\/5885\/how-to-scale-an-array-of-signed-integers-to-range-from-0-to-1\nnormalized_df = (df-df.min())\/(df.max()-df.min())\n\n# we use the pretained model to save time from rerunning the lda model's fit operation since that takes awhile in Kaggle.\n# lda = get_lda_model(10)\nlda = get_lda_model_saved()\nlda_output = lda.transform(normalized_df)\nprint(lda_output.shape)  # (NO_DOCUMENTS, NO_TOPICS)","c4ec9e29":"results = pd.DataFrame(lda_output, index=full_data['title'])\nprint(\"Printing top articles for each topic based on % comprised of that topic\")\nfor i in range(0, 10):\n  print(f\"Printing the top 10 articles for topic {i}\")\n  sorted_df = results.sort_values(i, ascending=False)\n  print(sorted_df[i].head(10))\n  print()","eec6aab8":"#get schema for the index\ndef get_search_schema():\n  schema = Schema(uid = TEXT(stored=True),\n                  sha = TEXT(stored=True),\n                  year = TEXT(stored=True),\n                  author = TEXT(stored=True),\n                  #here we set minisize = 1 to preserve numeric values\n                  #so we can differentiate between sars-cov and sars-cov-2\n                  title = TEXT(analyzer=StandardAnalyzer(minsize=1),stored=True),\n                  abstract = TEXT(analyzer=StandardAnalyzer(minsize=1),stored=True),\n                  url = TEXT(stored=True))\n  return schema\n\n# creates an index in a dictionary (only need to run once)\n# noop if it's already created\ndef create_search_index(drive_path, search_schema):\n  if not os.path.exists(drive_path + 'indexdir'):\n      os.mkdir(drive_path + 'indexdir')\n  ix = index.create_in(drive_path + 'indexdir', search_schema)\n  #open an existing index object\n  ix = index.open_dir(drive_path + 'indexdir')\n  return ix\n\ndef add_documents_to_index(ix, metadata):\n  # cancel writer in case re-indexing is needed\n  # if 'writer' in locals(): #doesn't work on Kaggle\n  # writer.cancel()\n\n  #create a writer object to add documents to the index\n  writer = ix.writer()\n\n  #now we can add documents to the index\n  uid = metadata['cord_uid']\n  sha = metadata['sha']\n  year = metadata['publish_year']\n  author = metadata['authors']\n  title = metadata['title']\n  abstract = metadata['abstract']\n  url = metadata['url']\n\n  for UID, SHA, YEAR, AUTHOR, TITLE, ABSTRACT, URL in zip(uid, sha, year, author, title, abstract, url):\n    writer.add_document(uid = str(UID),\n                        sha= str(SHA),\n                        year= str(YEAR),\n                        author=str(AUTHOR),\n                        title=str(TITLE),\n                        abstract=str(ABSTRACT),\n                        url=str(URL))\n\n  #close the writer and save the added documents in the index\n  #you should call the commit() function once you finish adding the documents otherwise you will cause an error-\n  #when you try to edit the index next time and open another writer. \n  writer.commit()\n\n  # need to cancel writer if error or need to reset\n  # writer.cancel()\n  return\n\n# get a multifield parser for the list of inptted fields\ndef get_multifield_parser(fields, search_schema):\n  parser = MultifieldParser(fields, schema=search_schema)\n  parser.add_plugin(SequencePlugin())\n  parser.add_plugin(PhrasePlugin())\n  return parser\n\n# this takes in a parser and query string to return the actual query that'll be sent to the searcher\ndef get_parser_query(parser, query):\n  result = parser.parse(query) # use boolean operators in quotation\n  print(result)\n  return result\n    \n# this method takes in a search index and query to return a dataframe of results\n# ix is the document index we created before\n# query is the string found from the parser\ndef get_search_results(ix, query):\n  #you can open the searcher using a with statement so the searcher is automatically closed when you\u2019re done with it\n  with ix.searcher() as searcher:\n      #The Results object acts like a list of the matched documents\n      results = searcher.search(query, limit=None)\n      print('Total Hits: {}\\n'.format(len(results)))\n      output_dict = defaultdict(list)\n\n      for result in results:\n        output_dict['cord_uid'].append(result['uid'])\n        output_dict['sha'].append(result['sha'])\n        output_dict['bm25_score'].append(result.score)\n        output_dict['title'].append(result['title'])\n        output_dict['abstract'].append(result['abstract'])\n        output_dict['publish_year'].append(result['year'])\n        output_dict['authors'].append(result['author'])\n        output_dict['url'].append(result['url'])\n        \n  output_df = pd.DataFrame(output_dict)\n  return output_df","08c5455e":"search_schema = get_search_schema()\n#we set this for Kaggle but can be any directory where you're working in. It takes a while to run.\nix = create_search_index('\/kaggle\/working\/', search_schema) \nadd_documents_to_index(ix, full_data)","a184f100":"#the code below creates an interactive query builder\nfrom ipywidgets import interact, Layout, HBox, VBox, Box\nfrom IPython.display import HTML, display, clear_output\nimport ipywidgets as widgets\nfrom IPython.display import update_display\n\ndef get_new_text_box():\n  textW = widgets.Textarea(\n        value='',\n        placeholder='Type something like \"covid\" or incubation',\n        description='',\n        disabled=False,\n        layout=Layout(width='100%', height='50px')\n    )\n  return textW\n\ndef get_new_plus_button():\n  button = widgets.Button(description=\"+\")\n  return button\n\ndef get_new_dropdown():\n  dropdown = widgets.Dropdown(\n      options=['AND', 'OR', 'NOT'],\n      value='AND',\n      description='Operator: ',\n      disabled=False,\n    )\n  return dropdown\n\ndef dynamic_search_query(parser, ix):\n  textW = widgets.Textarea(\n        value='',\n        placeholder='Type something like \"covid\" or incubation',\n        description='',\n        disabled=False,\n        layout=Layout(width='100%', height='50px')\n    )\n  \n  button = widgets.Button(description=\"+\")\n  search_rows_list = []\n  search_rows_list.append( HBox([textW, button], layout=Layout(align_items='center')) )\n  display_handle = display(VBox(search_rows_list, layout=Layout(align_items='center')), display_id='disp')\n\n  #search_rows_list is a list of HBox objects\n  # the first index will just be a text box and '+' button\n  # subsequent rows will have operator, text box, and '+' button\n  def on_button_clicked(b):\n    global STORED_SEARCH_QUERY\n    clear_output(wait=True)\n    new_text_box = get_new_text_box()\n    dropdown = get_new_dropdown()\n    search_rows_list.append( HBox([dropdown, new_text_box, button], layout=Layout(align_items='center')) )\n    display_handle.update(VBox(search_rows_list, layout=Layout(align_items='center')))\n\n    combined = ''\n    for i in range(0, len(search_rows_list)-1): #we do len - 1 since newet row has no values\n      row = search_rows_list[i]\n      if i == 0:\n        temp = combined + row.children[0].value\n        combined = temp\n      else:\n        temp = combined + ' ' + row.children[0].value + ' ' + row.children[1].value\n        combined = temp\n    \n    print(\"Current raw search query:\\n\" + combined)\n    print(\"Current query from parser:\")\n    query = get_parser_query(parser, combined) #already prints in method\n    STORED_SEARCH_QUERY = query\n\n  button.on_click(on_button_clicked)\n","7512c14f":"#read saved index  \nix = index.open_dir('\/kaggle\/working\/indexdir')\nfields = [\"title\", \"abstract\"] #set search fields\nparser = get_multifield_parser(fields, search_schema)\nSTORED_SEARCH_QUERY = '' #query is stored as a global so the last search query from the parser can be used in later cells\ndynamic_search_query(parser, ix)","b3352a37":"#see the stored query\nprint(STORED_SEARCH_QUERY)","96977fb8":"#get search engine output\nsearch_results = get_search_results(ix, STORED_SEARCH_QUERY)\nprint(search_results.shape)\nsearch_results.head(5)","62c4c166":"#for each of the output articles, find the closest 20 articles by comparing cosine scores\nfrom scipy.spatial import distance\n\ncord_uid_lda_df = pd.DataFrame(lda_output, index=full_data['cord_uid'])\ncord_uids = search_results['cord_uid'].tolist()\n\nlist_of_similar_articles = []\nduplicates = []\nfor cord_uid in cord_uids:\n  if isinstance(cord_uid_lda_df.loc[cord_uid], pd.DataFrame):\n    duplicates.append(cord_uid)\n    continue\n  topic_score = cord_uid_lda_df.loc[cord_uid].tolist()\n  distances = []\n  for entry in lda_output:\n    distances.append(distance.cosine(topic_score, entry))\n  full_data['cosine_distance'] = np.asarray(distances)\n  #ascending must be true since scipy does 1 - cosine(theta), so smaller values are closer\n  full_data_sorted = full_data.sort_values('cosine_distance', ascending=True)\n  list_of_similar_articles.append(full_data_sorted[:20]['cord_uid'].tolist())\n  del full_data_sorted\n\n# print(duplicates) #this is helpful for debugging purposes if duplicates are found after doing a search\nprint(len(list_of_similar_articles))","2129cffb":"# ideally we want to see if there are any documents that are included across all lists of top similar articles \n# we'll use set operations for this.\noverlapping_articles = list_of_similar_articles[0]\nfor articles in list_of_similar_articles:\n  overlapping_articles = set(overlapping_articles) & set(articles)\nprint(len(overlapping_articles))","1cb47f77":"import operator\nimport collections\n\narticle_counts = {}\nfor articles in list_of_similar_articles:\n  for article in articles:\n    curValue = article_counts.get(article, 0)\n    article_counts[article] = curValue + 1\n\narticle_counts = sorted(article_counts.items(), key=lambda kv: kv[1], reverse=True)\nsorted_articles = collections.OrderedDict(article_counts)\nrelevant_articles = []\nfor k,v in sorted_articles.items():\n    if v > 3:\n        row = full_data.loc[full_data['cord_uid'] == k]\n        title = row['title'].tolist()[0]\n        print(f\"{title}: {v}\")\n        relevant_articles.append(k)","9a8228b2":"full_data = full_data.drop('cosine_distance', axis=1)\nfull_data.info()","bbee25b0":"def read_query_dictionary(input_dir):\n  path = input_dir+'covid19-corpus-data\/'+'all_dz_query_dictionary_v5.json'\n  with open(path) as f:\n    query_dict = json.load(f)\n  return query_dict\n\n# Get a list of queries formatted for the whoosh search engine\nwhooshified_query_dict = {}\nquery_dictionary = read_query_dictionary(local_dir) #reads json query file \nfor key, value in query_dictionary.items():\n  query = get_parser_query(parser, value) #converts dict values to whoosh objects\n  whooshified_query_dict[key] = query","c475e8ec":"# Run search for each query and append results to dataframe\nall_search_results_dict = {}\nfor key, value in whooshified_query_dict.items():\n  #Stores results as dictionary with key as `query` and values as dataframes\n  search_results_df = get_search_results(ix, value)\n  if len(search_results_df) > 0:\n    search_results_df['source'] = 'search' #indicate origin of result (search vs. lda)\n    search_results_df['query'] = key\n    all_search_results_dict[key] = search_results_df","682ac248":"from scipy.spatial import distance\n\ndict_of_similar_articles = {}\ncord_uid_lda_df = pd.DataFrame(lda_output, index=full_data['cord_uid'])\n\nfor query, titles in all_search_results_dict.items():\n  cord_uids = titles['cord_uid'].tolist()\n  list_of_similar_articles = []\n  duplicates = []\n  for cord_uid in cord_uids:\n    if isinstance(cord_uid_lda_df.loc[cord_uid], pd.DataFrame): #indicates a duplicate\n      duplicates.append(cord_uid)\n      continue\n    topic_score = cord_uid_lda_df.loc[cord_uid].tolist()\n    distances = []\n    for entry in lda_output: #lda_output --> topic similarity results for each article in the corpus\n      distances.append(distance.cosine(topic_score, entry))\n    full_data['cosine_distance'] = np.asarray(distances)\n    full_data_sorted = full_data.sort_values('cosine_distance', ascending=True)\n    list_of_similar_articles.append(full_data_sorted[:20]['cord_uid'].tolist())\n    del full_data_sorted\n  dict_of_similar_articles[query] =  list_of_similar_articles\n  # print('Duplicates: ', duplicates) #this is helpful for debugging purposes if duplicates are found after doing a search\nprint('Length of similar articles dict: ', [len(v) for k, v in dict_of_similar_articles.items()])","3e220a38":"for query, list_of_similar_artilcles in dict_of_similar_articles.items():\n  overlapping_articles = list_of_similar_artilcles[0]\n  for articles in list_of_similar_artilcles:\n    overlapping_articles = set(overlapping_articles) & set(articles)\n  print(f\"Query: {query} and number of overlapping articles: {len(overlapping_articles)}\")","79d8e194":"from collections import OrderedDict\n\nall_relevant_articles = {}\nprint(\"Printing top recurring articles for each query.\")\nfor query, list_of_similar_artilcles in dict_of_similar_articles.items():\n  print(f\"\\nQuery: {query}\")\n  article_counts = {}\n  for articles in list_of_similar_artilcles:  \n    for article in articles:\n      curValue = article_counts.get(article, 0)\n      article_counts[article] = curValue + 1\n\n  article_counts = sorted(article_counts.items(), key=lambda kv: kv[1], reverse=True)\n  sorted_articles = OrderedDict(article_counts)\n\n  relevant_articles = []\n  for k, v in sorted_articles.items():\n    if v > 3:\n      row = full_data.loc[full_data['cord_uid'] == k]\n      title = row['title' ].tolist()[0]\n      print(f\"{title}: {v}\")\n      relevant_articles.append(k)\n  all_relevant_articles[query] = relevant_articles","995162a1":"#Number of relevant articles per precoded query\n[len(v) for k, v in all_relevant_articles.items()]","1fbeeefb":"temp = []\nfor k, v in all_search_results_dict.items():\n  search_output_df = full_data[full_data['cord_uid'].apply(lambda x: x in v['cord_uid'].tolist())]\n  search_output_df['source'] = 'search'\n  search_output_df['query'] = k\n  temp.append(search_output_df)\nall_search_output = pd.concat(temp)\nall_search_output.drop_duplicates(subset=['cord_uid', 'source', 'query'])\nprint('all_search len: ', len(all_search_output))\n\ntemp = []\nfor k, v in all_relevant_articles.items():\n  expanded_output = full_data[full_data['cord_uid'].apply(lambda x: x in v)]\n  expanded_output['source'] = 'lda'\n  expanded_output['query'] = k\n  temp.append(expanded_output)\nrelevant_search_results_lda_output = pd.concat(temp)\nrelevant_search_results_lda_output.drop_duplicates(subset=['cord_uid', 'source', 'query'])\nprint('relevant_search_results_lda_output len: ', len(relevant_search_results_lda_output))","f197d601":"combined_output_master = pd.concat([all_search_output,relevant_search_results_lda_output]).reset_index(drop=True)\ncombined_output_master = combined_output_master[['cord_uid','sha','title','abstract','publish_year','summary','url', 'source', 'query']]\nprint(combined_output_master.info())\n\n#this dedupes the entries by the unique cord_uid, but the source data will be lost as a result. This shouldn't be used for analyzing where source data comes from \ncombined_output_dedup = combined_output_master.drop_duplicates(subset=['cord_uid', 'query']).reset_index(drop=True)\ncombined_output_dedup = combined_output_dedup[['cord_uid','sha','title','abstract','publish_year','summary','url', 'source', 'query']]\nprint(combined_output_dedup.info())","d320cac2":"print(combined_output_master['query'].value_counts())","e7ef9a47":"combined_output_master.head()","00a00209":"#### Search Instructions\n\nThe search engine searches in both title and abstract by default. To search keywords in title or abstract separtely, see example below:\n\n**title:(\"covid-19\") AND abstract:(\"incubation period\" OR exposure)**\n\nNotes: \n* Enclose hyphenated words or phrases with quotation\n* Enclose a group of words to search within a field with parenthesis\")","d0e8cb16":"And here, we have our final result. This final output contains the original search engine output and additional relevant articles found via LDA for each of the 15 precoded queries.","bd2f5e60":"We run through our dictionary, get the search results for each precoded query, and store the results in the corresponding value of a new dictionary.","97e7037f":"Now that we have our LDA model and our initial topic distribution,we can extend the model we have to find similar articles based on topic probabilities. To do this, we'll create a search engine which uses boolean operators to find relevant articles.\n\nYou can think of this as a cheap\/naive way for us to label a small subset of articles that we think are relevant. Then we can find the closest articles to each result and ideally find articles that overlap multiple times across similar articles lists.\n","477a3826":"# Importing Dependencies","e3c9d58a":"# Findings\nOur resuts can be found in a separate notebook [here](https:\/\/www.kaggle.com\/crispyc\/coronawhy-task-ties) after running this notebook. Below is a brief description of what we looked at.\n\nOnce we have identified literature relevant to our task questions, we extract the following attributes to help us analyze our findings:\n* Target disease\n* Median\/mean age\n* Geographic location\n\nWe also selected outcomes that were reported in a more standardized way to demonstrate how we can use automated methods to synthesize evidence. These include:\n* Incubation periods\n* Prevalence of asymptomatic infection","4c73e7fc":"# Training LDA using SciBert Embeddings","e349985f":"Here, we have the number of articles with information pertinent to each precoded query.\n","fbf9c792":"# Running Precoded Queries","62c47a44":"And the final concatenation of all results...","03511c2a":"As done in the previous section, we're just checking to see if there are any articles that occur in all of the 12 topics (which is an indication of particular strength).","f8117b27":"Based on the above results, we can see that some of the topics start to show coherence such as the following groups:\n* Disease management in East Asian regions such as China, Taiwan, Hong Kong, and Singapore.\n* Specific cells or proteins involved with diseases.\n* Genomic approaches.\n* Animal studies with cattle, cats, and pigs.\n\nThis can be fine-tuned by trying different number of topics in the LDA model, different layers of output to pool in the scibert embedding model, etc. But as an experiment we can see with minimal configuration that there are already some defined topics that stand out. There is some overlap between topics so perhaps a lower number of topics could've been used in the LDA model.","fc0d910d":"We've defined 15 specific questions based on those outlined in the Kaggle competition.  We've defined each of the question in terms of boolean logic (regarding terms\/phrases that should or should not be referred to in the search), and assigned them to a json file (`all_dz_query_dictionary_v5.json`) available in this notebook's data stores.\n\nTo answer these task-specific questions, we use the search engine to find a small set of relevant articles, and use their topic probabilities to discover articles of similar topic. \n\nThis section produces results for each of these precoded queries (as was done in the previous section for a single query).","6eab503d":"Great! As we can see that there are some articles that do appear repeatedly. These are good candidates to start exploring as relevant articles.","d28cba03":"# Preprocessing","e09602a0":"First, we're reading in our set of precoded queries from the json file.  We then convert the values to whoosh objects (with keys being the precoded query) that can be utilized in our search.","95a91423":"Let's create a simple boolean search query that looks for abstracts\/titles that contain the keywords \"covid-19\" and incubation. We will run this query, get the results, and then find similar articles.","7d2bf688":"* This section contains optional code that loads the metadata.csv file from the CORD-19-research-challenge.\n* From the metadata, we use bert-extractive-summarizer to get summaries from the abstracts. [NEED TO CITE BERT SUMMARIZER]\n* Then, we use flairNLP to obtain scibert embeddings for the summaries above. [CITE flair]\n* Finally, this new data, which is saved separately in chunks, is combined with the original metadata to form the 'full_metadata_with_scibert_embeddings.json' file\n\nThe commands to run have been commented out since the results are available in our data section. The code is available if you'd like to fine tune anything.","27481c08":"Now, we'll collect the articles that are most relevant to each particular topic.  We're measuring relevance by the number of times an article appears in the results.  We've arbitrarily decided that we're only including articles mentioned at least 3 times.","bf96a3e5":"We can see from above that there isn't a single article that appears over all the lists of similar articles. That is ok, it's one way to see if there's a particular article that stands out as a strong match. Now let's see below if there's an article that appears at least > 3 times.","5bfdcb1d":"Now, we'll get the similarity (cosine) distances for each of our search results we just stored.  We return the `cord_uid` values of the 20 most similar articles.","7cf2ffdd":"# Searching for Similar Articles Based on Topic Distribution","6a374c2c":"We'll now combine the results from our general search and LDA efforts into the same dataframe.  We've identified whether the results origin in the `source` column, and the query to which the results pertain in the `query` column."}}