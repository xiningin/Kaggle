{"cell_type":{"803ba97c":"code","2c8ddebc":"code","3684f5a5":"code","308a0018":"code","dbceafe8":"code","8c199f1a":"code","07e6a142":"code","e196765c":"code","d7659be3":"code","24e157d0":"code","b5d9f9fe":"code","4c2bf6a4":"code","c839ac01":"code","7ff13d83":"code","cb4e6ad8":"code","05c6642b":"code","81d2d34e":"code","da3d6d9d":"code","260df147":"code","bb75028c":"code","4cbb2e0d":"markdown","ea481bdf":"markdown","63a7e311":"markdown","de75e8d0":"markdown","8641d165":"markdown","9871c51a":"markdown","8ca1e1b2":"markdown","0b7fd361":"markdown"},"source":{"803ba97c":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2c8ddebc":"df = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","3684f5a5":"df.describe()","308a0018":"df.dtypes","dbceafe8":"df.info()","8c199f1a":"plt.figure(figsize=(12,10)) # This is done to find the relation between the different parameters whether they are positively or negatively correlated\nsns.heatmap(df.corr(), annot = True,cmap='terrain')","07e6a142":"df.hist(figsize = (10, 10));","e196765c":"sns.set_style('whitegrid')\nsns.countplot(x='output',data=df)\n# sns.countplot(x = df['age'], saturation= 1)","d7659be3":"scaled_df = pd.get_dummies(df,columns=['sex','cp','fbs','restecg','exng','slp','caa','thall']) # this ae all categorical features","24e157d0":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nStandardScaler = StandardScaler()\ncolumn_scale = ['age','trtbps','chol','thalachh','oldpeak']\nscaled_df[column_scale] = StandardScaler.fit_transform(scaled_df[column_scale])","b5d9f9fe":"scaled_df.head()","4c2bf6a4":"X = scaled_df.drop('output', axis = 1)\nY = scaled_df['output']","c839ac01":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y,test_size = 0.3, random_state = 40)","7ff13d83":" def get_score(model, X_train, X_test, Y_train, Y_test):\n     model.fit(X_train, Y_train)\n     return model.score(X_test, Y_test)","cb4e6ad8":"# Logistic regression\nfrom sklearn.linear_model import LogisticRegression\nget_score (LogisticRegression(max_iter=10000),X_train, X_test, Y_train, Y_test)","05c6642b":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\nget_score(DecisionTreeClassifier(),X_train, X_test, Y_train, Y_test)","81d2d34e":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nget_score(RandomForestClassifier(),X_train, X_test, Y_train, Y_test)","da3d6d9d":"# Support Vector Machine\nfrom sklearn.svm import SVC\nget_score(SVC(),X_train, X_test, Y_train, Y_test)","260df147":"import sklearn.linear_model as sk\nlearn = sk.LogisticRegression();          #initializing linear regression model\nlearn.fit(X_train,Y_train);               #training the linear regression model\nY_predicted = learn.predict(X_test)\nscore = learn.score(X_test,Y_test);       #testing the linear regression model\nprint (score)","bb75028c":"print(classification_report(Y_test, Y_predicted))","4cbb2e0d":"### 7. Check K fold cross validatiion ","ea481bdf":"### 5. Data processing ","63a7e311":"### 6. Train and test data ","de75e8d0":"### 4. Feature selection technique","8641d165":"### Above graph shows that a data is balanced","9871c51a":"### 8. Build Logistic Regression Model ","8ca1e1b2":"##### Comment1: Above data shows that there are no any null values in the Data set","0b7fd361":"### Visualise data"}}