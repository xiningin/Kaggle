{"cell_type":{"330e16d4":"code","9cbc6be9":"code","a10e8cf5":"code","0d68a99c":"code","e2b3a3f3":"code","c4efff31":"code","c1fc6c43":"code","ce5b7059":"code","8d0968ae":"code","1117e3c8":"code","8614076a":"code","d58afecb":"code","d3a0c61b":"code","03c13817":"code","15b3a9cf":"code","0c5f2678":"code","51d43de9":"code","6a8aa95d":"code","ed6877a4":"code","166d49f7":"code","71a9c509":"code","4ac0fe8f":"code","bcf45eb3":"code","925546e0":"code","b87ae1f1":"code","6eadcba8":"code","04ec8c87":"code","fdb789f1":"code","87a1f7be":"code","c336ff6e":"code","16523430":"code","33f3267b":"code","e7787438":"markdown","441d994a":"markdown","24eb16c5":"markdown","32fd7113":"markdown","683d2d64":"markdown","0fe57fd5":"markdown","8029c209":"markdown","d6178987":"markdown","fca97d4a":"markdown","b08f425d":"markdown","c912ca5e":"markdown","5e30380e":"markdown","7ab90d6b":"markdown","45ba5d9e":"markdown","42769116":"markdown","b11e8b6f":"markdown","080e23b4":"markdown","94ba2040":"markdown","e9a36a39":"markdown","36e9eb68":"markdown","fe499c3e":"markdown","75638cca":"markdown","53e1ecf0":"markdown","71bba1b1":"markdown","effa2136":"markdown","6582e16a":"markdown","d1ab5430":"markdown","c6fd6454":"markdown","549b74aa":"markdown","2d709c07":"markdown","fb71b8df":"markdown","0da9cc6f":"markdown","789db80d":"markdown","70a59aad":"markdown","311d2b28":"markdown","9cb3e777":"markdown","34552acf":"markdown","15e00f87":"markdown","525d7788":"markdown","6d92ee5b":"markdown","ee8e945e":"markdown","d4a99961":"markdown","34ec582b":"markdown","87845457":"markdown","1b8af0db":"markdown","e08959d3":"markdown","43e859b6":"markdown","66d6051a":"markdown","3e010672":"markdown","e561b825":"markdown","6e24bbae":"markdown","40fa5869":"markdown","0bfa522c":"markdown","cfb51229":"markdown","eecbadfe":"markdown","8aa5bc30":"markdown","bf1dfd00":"markdown","3aef0d3b":"markdown","90ace719":"markdown","d80000ca":"markdown","3026c955":"markdown","4fb921ab":"markdown","a913c4e9":"markdown","f706783d":"markdown","e95860a2":"markdown","7fd81860":"markdown","8525ac79":"markdown","836f641c":"markdown","759ef7df":"markdown","9d204fa5":"markdown","e96b524b":"markdown","30bc1886":"markdown","92a1ac85":"markdown","6e6e2855":"markdown","10d20dfb":"markdown","def15584":"markdown","08d66c62":"markdown","73681932":"markdown","1e982fb5":"markdown","db536563":"markdown","4dd995ef":"markdown","e1a0355d":"markdown","7dc80533":"markdown","a4b65473":"markdown","45898ac0":"markdown","52f8b399":"markdown","c53d1bf8":"markdown","da4f3e5f":"markdown","7fb15ed5":"markdown","6d1a58f7":"markdown"},"source":{"330e16d4":"import pandas as pd\nimport numpy as np\n\n\nimport gensim\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom gensim.models import Word2Vec\nfrom gensim.parsing.preprocessing import preprocess_string\n\nfrom ipywidgets import interact\nfrom IPython.display import display, HTML\n\nfrom gensim.summarization.textcleaner import get_sentences\n\nimport re\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport time\n","9cbc6be9":"def get_data(dir_path):\n    \n    \"\"\"\n    Take as input a directory path containing json files from \n    biorxiv_medrxiv, comm_use_subset, noncomm_use_subset or custom_license.\n    Four dataframes are returned: papers_df, authors_df, affiliations_df, bib_entries_df\n    \"\"\"\n    \n    files = os.listdir(dir_path)\n    papers_df = pd.DataFrame(columns= [\"paper_id\", \"title\", \"authors\", \"abstract\", \"body_text\", \"bib_titles\", \"dataset\"])\n    authors_df = pd.DataFrame(columns= [\"author\", \"affiliation\"])\n    affiliations_df = pd.DataFrame(columns= [\"affiliation\", \"country\"])\n    bib_entries_df = pd.DataFrame(columns=[\"title\", \"authors\", \"year\", \"venue\"])\n    \n    line_author_df = 0\n    line_affiliations_df = 0\n    line_bib_entries_df = 0\n    \n    \n    \n    for line,file in enumerate(files):\n        \n        #loading information\n        n_files = len(files)\n        if (line%300==0)or(line==n_files-1):\n            print(\"Line\", line, \" \", round(100*line\/n_files, 2),\"%\")\n            \n        file_path = os.path.join(\"..\/data\/{}\".format(dir),file)\n    \n        with open(file_path) as f:\n            data = json.load(f)\n        \n        \n        #Papers\n        \n        paper_id = data[\"paper_id\"]\n        \n        title = data[\"metadata\"][\"title\"]\n        \n        authors, affiliations, countries = \"\", \"\", \"\"\n        for author in data[\"metadata\"][\"authors\"]:\n            first_last_name = author[\"first\"] + \" \" + author[\"last\"]\n            authors = authors + \" || \" + first_last_name\n            \n            if author[\"affiliation\"]=={}:\n                affiliation = \"NA\"  #Non Available\n                affiliations = affiliations + \" || \" + affiliation.strip()\n                \n                country = \"NA\"\n                countries = countries + \" || \" + country.strip()\n                continue\n                \n            affiliation = author[\"affiliation\"][\"laboratory\"] + \" \" + author[\"affiliation\"][\"institution\"]\n            affiliations = affiliations + \" || \" + affiliation.strip()\n            \n            if \"country\" not in author[\"affiliation\"][\"location\"].keys():\n                country = \"NA\"\n                countries = countries + \" || \" + country\n                continue\n            \n            country = author[\"affiliation\"][\"location\"][\"country\"]\n            countries = countries + \" || \" + country\n            \n        authors = authors[4:]   #extracting out the first separator \" || name1 || name2\" => \"name1 || name2\"\n        affiliations = affiliations[4:]\n        countries = countries[4:]\n        \n        abstract = \"\"\n        for info in data[\"abstract\"]:\n            abstract = abstract + \" \" + info[\"text\"]\n        abstract = abstract.strip()\n\n        body_text = \"\"\n        for info in data[\"body_text\"]:\n            body_text = body_text + \" \" + info[\"text\"]\n        body_text = body_text.strip()\n\n        bib_titles, bib_authors, years, venues = \"\", \"\", \"\", \"\"\n        for bib in data[\"bib_entries\"]:\n            bib_titles = bib_titles + \" || \" + data[\"bib_entries\"][bib][\"title\"]\n            \n            \n            year = data[\"bib_entries\"][bib][\"year\"]\n            \n            years = years + \" || \" + str(year)\n            \n            venue = data[\"bib_entries\"][bib][\"venue\"]\n            venues = venues + \" || \" + venue\n   \n            bib_author = [author[\"first\"] + \" \" + author[\"last\"] for author in data[\"bib_entries\"][bib][\"authors\"]]\n            bib_author = \" | \".join(bib_author)\n            bib_authors = bib_authors + \" || \" + bib_author\n            \n        bib_titles, bib_authors, years, venues = bib_titles[4:] , bib_authors[4:], years[4:], venues[4:] \n        \n        ##appending to paper_df\n        papers_df.loc[line, :] = [paper_id, title, authors, abstract, body_text, bib_titles, dir]\n        \n        \n        #Authors\n        authors_list = authors.split(\" || \")\n        affiliations_list = affiliations.split(\" || \")\n        \n        for i in range(len(authors_list)):\n            authors_df.loc[line_author_df, :] = authors_list[i],  affiliations_list[i]\n            \n            line_author_df+=1\n        \n        #Affiliations\n        countries_list = countries.split(\" || \")\n        \n        for i in range(len(affiliations_list)):\n            affiliations_df.loc[line_affiliations_df,:] = affiliations_list[i], countries_list[i]\n            \n            line_affiliations_df+=1\n        \n        \n        #Bibliograpphy\n        bib_titles_list = bib_titles.split(\" || \")\n        bib_authors_list = bib_authors.split(\" || \")\n        years_list = years.split(\" || \")\n        venues_list = venues.split(\" || \")\n        \n        \n        for i in range(len(bib_titles_list)):\n            \n            bib_entries_df.loc[line_bib_entries_df, :] = bib_titles_list[i], bib_authors_list[i], years_list[i], venues_list[i]\n            line_bib_entries_df+=1\n            \n    authors_df = authors_df.drop_duplicates().reset_index(drop=True)\n    affiliations_df = affiliations_df.drop_duplicates().reset_index(drop=True)\n    bib_entries_df = bib_entries_df.drop_duplicates().reset_index(drop=True)\n    \n    return papers_df, authors_df, affiliations_df, bib_entries_df","a10e8cf5":"df = pd.read_csv(\"\/kaggle\/input\/papers\/papers.csv\", sep=\";\", nrows=100)\ndf.head()","0d68a99c":"#dropping eventual duplicates\ndf = df.drop_duplicates()\ndf.shape","e2b3a3f3":"meta_df = pd.read_csv(\"..\/input\/CORD-19-research-challenge\/metadata.csv\")\nmeta_df.head()","c4efff31":"#dropping any paper not present in the meta dataset. NB: paper_id (df) == sha (meta_df)\npaper_in_meta = df.paper_id.isin(meta_df.sha)\ndf = df[paper_in_meta]\ndf = df.sort_values(by=\"paper_id\", axis=0).reset_index(drop=True)\n\n#Keeping meta data of papers present in the papers dataframe and sorting it acording to sha\nmeta_df = meta_df.loc[meta_df.sha.isin(df.paper_id.values)]\nmeta_df = meta_df.sort_values(by=\"sha\", axis=0).reset_index(drop=True)\n\n#We can now transfert information from meta to papers dataframe: both dataframe share now the same number of papers order in the same way\ndf[\"title\"] = meta_df[\"title\"]\ndf[\"abstract\"] = meta_df[\"abstract\"]\ndf[\"authors\"] = meta_df[\"authors\"]\ndf[\"publish_time\"] = meta_df[\"publish_time\"]\ndf[\"url\"] = meta_df[\"url\"]\n","c1fc6c43":"df = df.loc[(df.title.notna()&df.authors.notna()&df.abstract.notna()), :].reset_index(drop=True)\n\ndf[\"paper\"] = df.title + \" \" + df.abstract + \" \" + df.body_text\n","ce5b7059":"\ndf[\"paper_token\"] = df.paper.apply(preprocess_string)","8d0968ae":"n_papers = df.shape[0]\ndf[\"paper_tag\"] = [TaggedDocument(words=df.paper_token[i],tags=[i]) for i in range(n_papers)]","1117e3c8":"model_doc2vec = Doc2Vec(df.paper_tag.values, dm=1, vector_size=200, window=5, min_count=1, workers=8, epochs=5)\n","8614076a":"def TagInText(text, tag_list):\n    tagIn = False\n    for tag in tag_list:\n        match = re.match(\".* {}.*|.* {}.*|.* {}.*\".format(tag.lower(), tag.upper(), tag.capitalize()), text)\n        if match:\n            tagIn=True\n    \n    return tagIn","d58afecb":"model_word2vec = Word2Vec(sentences=df.paper_token, size=100,window=5,min_count=1, seed=1,  sg =0)\n\n#model_word2vec.save(\"..\/data\/embedding\/final\/word2vec_cbow.model\")","d3a0c61b":"\ndef SimilarWords(tags):\n    tag_list = preprocess_string(tags)\n    similar_words_list = []\n    for tag in tag_list:\n        #try is important here to avoid errors if a word is not in the vocabulary\n        try:\n            similar_words = model_word2vec.wv.most_similar(tag.lower())\n            similar_words = [word_sim[0] for word_sim in similar_words]\n            similar_words_list = similar_words_list + similar_words\n        except:\n            pass\n    \n    return list(set(similar_words_list)) #using set allows to filter out duplicates\n\n","03c13817":"#example of words closed to China\ntags = \"China Wuhan\"\nSimilarWords(tags)","15b3a9cf":"\ndef SearchIndex(search, data, column, model_doc2vec, pubdate_before=None, pubdate_after=None, pubdate_in=None, tags=None, topn=10, seed=0):\n    \n    \"\"\"\n    Take a search string and return list of line indexes of n closest papers\n    \n    search: string containing the query\n    data: papers dataframe in the right format\n    column: column in \"data\" containing papers texts\n    model_doc2vec: a Doc2Vec model trained on papers from \"data\"\n    pubdate_before: to get papers which publication dates are before \"pubdate_before\"; format date : year-month-day\n    pubdate_after: to get papers which publication dates are after pubdate_before. format date : year-month-day\n    pubdate_in: to get papers which publication dates are between date1 and date2. format date : year1-month1-day1 year2-month2-day2\n    tags: string with tag words\n    topn: maximum number of results to be returned,\n    seed: an integer for random seed; useful for the doc2vec model. \n    \"\"\"\n    \n    #filtering according to date\n    if pubdate_before:\n        data = data[data.publish_time<pubdate_before]\n    if pubdate_after:\n        data = data[data.publish_time>pubdate_after]\n    if pubdate_in:\n        date1 = pubdate_in.split()[0]\n        date2 = pubdate_in.split()[1]\n        data = data[(data.publish_time>=date1)&(data.publish_time<=date2)]\n        \n    n = data.shape[0]\n    model_doc2vec.random.seed(seed)\n    search = preprocess_string(search)\n    search_vec = model_doc2vec.infer_vector(search)\n    similar_text_index = model_doc2vec.docvecs.most_similar([search_vec], topn = n)\n    similar_text_index = [index_sim[0] for index_sim in similar_text_index  if index_sim[0] in data.index]\n    \n    if tags: \n        tag_list = preprocess_string(tags)\n        similar_words = SimilarWords(tags)\n        tag_list = tag_list + similar_words\n        \n        filter_papers_index = []\n\n        #finding articles containing tags\n        tagged = data[column].apply(lambda paper: TagInText(paper,tag_list))\n        tagged_papers_index = data[tagged].index\n        \n        similar_text_index = [index for index in similar_text_index if index in tagged_papers_index]\n\n    \n    similar_text_index = similar_text_index[:topn]\n\n    \n    \n    return similar_text_index","0c5f2678":"search = \"What is covid-19 risks factors?\"\ntags = \"risk covid China\"\n\nselect_indexes = SearchIndex(search=search, data=df, column=\"paper\", model_doc2vec=model_doc2vec, pubdate_before=\"2020-02-01\", topn=5)\nselect_indexes","51d43de9":"sentences_df = pd.DataFrame(columns=[\"paper_index\", \"sentence\"])\n\nfor paper_index in select_indexes:\n\n    sentences_list = list(get_sentences(df.loc[paper_index, \"body_text\"]))\n    n_sent = len(sentences_list)\n    new_sent_df = pd.DataFrame(list(zip([paper_index]*n_sent, sentences_list)) , columns=[\"paper_index\", \"sentence\"])\n    sentences_df = sentences_df.append(new_sent_df)\n\nsentences_df.head()","6a8aa95d":"#filtering out non relevant sentences \n\npattern = \".*doi.*|.*http.*|.*copyright.*|.*author.*|.*license.*|.*without permission.*|.*CC-BY-NC-ND.*|.*author.*|.*funder.*|.*medrxiv.*|.*fig.*|.*all rights.*\"\nsentences_df[\"filtered\"] = sentences_df[\"sentence\"].apply(lambda sent: True if re.match(pattern, sent.lower()) else False)\n\nsentences_df = sentences_df.loc[~sentences_df.filtered,:]\nsentences_df = sentences_df.reset_index(drop=True)\n\nsentences_df.head()\n\nsentences_df[\"sentence_token\"] = sentences_df.sentence.apply(preprocess_string)\n\n#### Filtering out sentences with less than 5 tokens\n\nsentences_df.sentence_token = sentences_df.sentence_token.apply(lambda sent: sent if len(sent)>5 else np.nan)\nsentences_df = sentences_df.dropna().reset_index(drop=True)\n\n","ed6877a4":"def RelevantSent(search, papers_indexes, topn_sent=5, tags=None):\n    \"\"\"\n    This is a function that look for more relevant sentences in a list of papers given a search.\n    It returns a dictionary which keys are papers indexes and values are list of top n relevant sentences\n    \"\"\"\n    model_doc2vec.random.seed(1)\n    search = preprocess_string(search)\n    search_vec = model_doc2vec.infer_vector(search)\n\n    sentences_df[\"sent_vec\"] = sentences_df.sentence_token.apply(model_doc2vec.infer_vector).values\n    sentences_df[\"cos_similarity\"] = sentences_df.sent_vec.apply(lambda sent_vec: cosine_similarity(search_vec.reshape(-1,1), sent_vec.reshape(-1,1))[0][0])\n\n    topn_sent_dict = {}\n    \n    if tags:\n        tags = preprocess_string(tags) + SimilarWords(tags)\n            \n    for paper_index in papers_indexes:\n        sent_paper_order = sentences_df.loc[sentences_df.paper_index==paper_index, :]\n        sent_paper_order = sent_paper_order.sort_values(by=\"cos_similarity\", axis=0, ascending=False).reset_index(drop=True)\n\n        \n        #keeping only sentences with at least one tag word\n        sent_list_tag = []\n        if tags:\n            sent_list = list(sent_paper_order.loc[:1000, \"sentence\"])\n            for sent in sent_list:\n                tagIn = TagInText(sent, tags)\n                if tagIn is True:\n                    sent_list_tag.append(sent)\n\n        else:\n            sent_list_tag = list(sent_paper_order.loc[:topn_sent-1, \"sentence\"])\n            \n        topn_sent_dict[paper_index] = sent_list_tag[:topn_sent]\n        \n    return topn_sent_dict","166d49f7":"\ntop_sent = RelevantSent(search, select_indexes, topn_sent=10, tags=tags)\npaper_index = select_indexes[0]\ntop_sent[paper_index]\n","71a9c509":"\ndef SearchDisplayPaper(search, model_doc2vec, pubdate_before=None, pubdate_after=None, pubdate_in=None, topn=10, topn_sent=10, tags=None):\n    \"\"\"\n    Take a request and return most relevant papers, with their titles, authors, abstracts and most relevant sentences\n    \n    search: string containing the query\n    model_doc2vec: a Doc2Vec model trained on papers from \"data\"\n    pubdate_before: to get papers which publication dates are before \"pubdate_before\"; format date : year-month-day\n    pubdate_after: to get papers which publication dates are after pubdate_before. format date : year-month-day\n    pubdate_in: to get papers which publication dates are between date1 and date2. format date : year1-month1-day1 year2-month2-day2\n    tags: string with tag words\n    topn: maximum number of results to be returned,\n    seed: an integer for random seed; useful for the doc2vec model. \n    \"\"\"\n    \n    select_indexes = SearchIndex(search=search, data=df, column=\"paper\", model_doc2vec=model_doc2vec, pubdate_before=pubdate_before, pubdate_after=pubdate_after, pubdate_in=pubdate_in, tags=tags, topn=topn)\n    top_sent = RelevantSent(search, select_indexes, topn_sent=topn_sent, tags=tags)\n\n    if tags:\n        tag_list = tags.split()\n        similar_words = SimilarWords(tags)\n            \n    summary = \"\"\n    \n    for i,index in enumerate(select_indexes):\n        title, authors, date, url, abstract = list(df.loc[index, [\"title\", \"authors\", \"publish_time\", \"url\", \"abstract\"]])\n        \n        sentences = top_sent[index]\n        top_sentences = [\n                \"\"\"\n                <p>&nbsp;<\/p>\n                <h3 style=\"text-align: justify;\"><span style=\"color: #800080;\"><strong>Sentence {nb}:<\/h3>\n                <h4 style=\"text-align: justify;\"><\/strong> <span style=\"color: #000000;\">{sent}<\/span><\/span><\/h4>\n                \"\"\".format(nb=i+1, sent=sent) for i, sent in enumerate(sentences) \n                ]\n\n        top_sentences = \" \".join(top_sentences)\n        \n        \n        if tags:\n            \n            #highlighting tags in magenta\n            for tag in similar_words:\n                abstract = re.sub(\" {}| {}| {}\".format(tag.lower(), tag.upper(), tag.capitalize()), \"\"\"<span style=\"color: #ff00ff;\"> {}<\/span>\"\"\".format(tag.capitalize()), abstract)\n                top_sentences = re.sub(\" {}| {}| {}\".format(tag.lower(), tag.upper(), tag.capitalize()), \"\"\"<span style=\"color: #ff00ff;\"> {}<\/span>\"\"\".format(tag.capitalize()), top_sentences)\n            #highlighting tags in red\n            \n            for tag in tag_list:\n                abstract = re.sub(\" {}| {}| {}\".format(tag.lower(), tag.upper(), tag.capitalize()), \"\"\"<span style=\"color: #ff0000;\"> {}<\/span>\"\"\".format(tag.capitalize()), abstract)\n                top_sentences = re.sub(\" {}| {}| {}\".format(tag.lower(), tag.upper(), tag.capitalize()), \"\"\"<span style=\"color: #ff0000;\"> {}<\/span>\"\"\".format(tag.capitalize()), top_sentences)\n             \n            \n        summary = summary + \"\"\"\n        <p>&nbsp;<\/p>\n        <p>&nbsp;<\/p>\n        <h1><span style=\"color: #333300; background-color: #ffffff;\">Paper {nb}.<\/span><\/h1>\n        <p>&nbsp;<\/p>\n        <h1><span style=\"color: #800000; background-color: #999999;\">Title:<span style=\"background-color: #ffffff;\"> {title} <\/span><\/span><\/h1>\n        <p>&nbsp;<\/p>\n        <p><strong><span style=\"color: #800000; background-color: #999999;\"><span style=\"background-color: #ffffff;\">Authors: {authors} <\/span><\/span><\/strong><\/p>\n        <p><span style=\"color: #003366;\"><strong>Publication date: {date} <\/strong><\/span><\/p>\n        <p><a href={url} ><span style=\"color: #3366ff;\"><strong>Link to full paper<\/strong><\/span> <\/a><\/p>\n        <p>&nbsp;<\/p>\n        <h2><span style=\"background-color: #999999;\">Abstract: <\/span><\/h2>\n        <p>&nbsp;<\/p>\n        <h4  style=\"text-align: justify;\"><span style=\"background-color: #ffffff;\"> {abstract} <\/span><\/h4>\n        \n        \"\"\".format(nb=i+1, title=title, authors=authors, date=date, url=url, abstract=abstract)\n        \n        \n        summary = summary + top_sentences\n        \n    summary = HTML(summary)\n    \n    return display(summary)","4ac0fe8f":"SearchDisplayPaper(search, model_doc2vec=model_doc2vec, pubdate_before=\"2020-04-01\", topn=10, topn_sent=5, tags=tags)","bcf45eb3":"class Search():\n    def __init__(self, papers_df, meta_df=None, papers_preprocessed=False, model_doc2vec=None, model_word2vec=None):\n        \"\"\"\n         papers_df: papers dataframe in the right fromat\n         meta_df: meta dataframe. if \"papers_preprocessed=True\", meta_df must be provided\n         papers_preprocessed: True if papers need to be preprocessed, False otherwise\n         model_doc2vec: A doc2vec model train on papers from \"papers_df[\"paper\"]\"\n         model_word2vec: A word3vec model preferably trained on \"papers_df[\"paper\"]\"\n        \"\"\"\n        \n        if papers_preprocessed:\n            self.papers_df = papers_df\n            self.model_doc2vec = model_doc2vec \n            self.model_word2vec = model_word2vec\n        else:   \n            self.papers_df = self.preprocessing_papers(papers_df, meta_df)\n        \n             \n        \n            \n        self.html_index = 0  #for html hidden links\n        \n    def preprocessing_papers(self, papers_df, meta_df):\n\n        print(\"Processing papers!\", time.strftime(\"%H:%M:%S\", time.localtime()))\n\n        \n        print(\"Keeping only papers with meta data!\", time.strftime(\"%H:%M:%S\", time.localtime()))\n\n        paper_in_meta = papers_df.paper_id.isin(meta_df.sha)\n        papers_df = papers_df[paper_in_meta]\n        papers_df = papers_df.sort_values(by=\"paper_id\", axis=0).reset_index(drop=True)\n\n        ##Keeping meta data of papers present in the papers dataframe and sorting it acording to sha\/paper_id\n        meta_df = meta_df.loc[meta_df.sha.isin(papers_df.paper_id.values)]\n        meta_df = meta_df.sort_values(by=\"sha\", axis=0).reset_index(drop=True)\n\n        ##We can now transfert information from meta to papers dataframe\n        papers_df[\"title\"] = meta_df[\"title\"]\n        papers_df[\"abstract\"] = meta_df[\"abstract\"]\n        papers_df[\"authors\"] = meta_df[\"authors\"]\n        papers_df[\"publish_time\"] = meta_df[\"publish_time\"]\n        papers_df[\"url\"] = meta_df[\"url\"]\n        \n        print(\"Concatenation of titles, abstracts and body_texts!\", time.strftime(\"%H:%M:%S\", time.localtime()))\n        papers_df[\"paper\"] = papers_df.title + \" \" + papers_df.abstract + \" \" + papers_df.body_text\n        \n        \n        print(\"Dropping papers without title, authors names or abstract!\", time.strftime(\"%H:%M:%S\", time.localtime()))\n        papers_df = papers_df.loc[(papers_df.title.notna()&papers_df.authors.notna()&papers_df.abstract.notna()), :].reset_index(drop=True)\n        \n        print(\"Tokenization!\", time.strftime(\"%H:%M:%S\", time.localtime()))\n        papers_df[\"paper_token\"] = papers_df.paper.apply(preprocess_string)\n        \n\n        print(\"Building a Doc2Vec model with preprocessed data\", time.strftime(\"%H:%M:%S\", time.localtime()))\n        n_papers = papers_df.shape[0]\n        papers_df[\"paper_tag\"] = [TaggedDocument(words=papers_df.paper_token[i],tags=[i]) for i in range(n_papers)]\n        model_doc2vec = Doc2Vec(papers_df.paper_tag.values, dm=1, vector_size=200, window=5, min_count=1, workers=8, epochs=5)\n        self.model_doc2vec = model_doc2vec \n        \n        print(\"Building a Word2Vec model with preprocessed data\", time.strftime(\"%H:%M:%S\", time.localtime()))\n        model_word2vec = Word2Vec(sentences=papers_df.paper_token, size=100,window=5,min_count=1, seed=1,  sg =0)\n        self.model_word2vec = model_word2vec \n\n        print(\"Processing over! Do not forget saving the preprocessed data (self.papers_df), the doc2vec and word2vec models (self.model_doc2vec and self.model_word2vec) to go faster next time\")\n\n        return papers_df\n    \n    \n        \n    def preprocessing_sentences(self, select_indexes):\n        \n        \"\"\"\n        Generating a dataframe with columns [\"paper_index\", \"sentence\"]: \n        each line with a given sentence and the paper_index of the paper containing the sentence\n        \"\"\"\n        sentences_df = pd.DataFrame(columns=[\"paper_index\", \"sentence\"])\n\n        for paper_index in select_indexes:\n\n            sentences_list = list(get_sentences(self.papers_df.loc[paper_index, \"body_text\"]))\n            n_sent = len(sentences_list)\n            new_sent_df = pd.DataFrame(list(zip([paper_index]*n_sent, sentences_list)) , columns=[\"paper_index\", \"sentence\"])\n            sentences_df = sentences_df.append(new_sent_df)\n\n    \n        #filtering out non relevant sentences \n\n        pattern = \".*doi.*|.*http.*|.*copyright.*|.*author.*|.*license.*|.*without permission.*|.*CC-BY-NC-ND.*|.*author.*|.*funder.*|.*medrxiv.*|.*fig.*|.*all rights.*\"\n        sentences_df[\"filtered\"] = sentences_df[\"sentence\"].apply(lambda sent: True if re.match(pattern, sent.lower()) else False)\n\n        sentences_df = sentences_df.loc[~sentences_df.filtered,:]\n\n        sentences_df[\"sentence_token\"] = sentences_df.sentence.apply(preprocess_string)\n\n        #### Filtering out sentences with less than 5 tokens\n\n        sentences_df.sentence_token = sentences_df.sentence_token.apply(lambda sent: sent if len(sent)>5 else np.nan)\n        sentences_df = sentences_df.dropna().reset_index(drop=True)\n        \n        return sentences_df\n    \n    \n    def TagInText(self, text, tag_list):\n        tagIn = False\n        for tag in tag_list:\n            match = re.match(\".* {}.*|.* {}.*|.* {}.*\".format(tag.lower(), tag.upper(), tag.capitalize()), text)\n            if match:\n                tagIn=True\n\n        return tagIn\n    \n    def SimilarWords(self,tags):\n        \"\"\"\n        Find 5 words similar to each tag. Return a list of words filtered out of duplicates.\n        \"\"\"\n        tag_list = preprocess_string(tags)\n        similar_words_list = []\n        for tag in tag_list:\n            #try is important here to avoid errors if a word is not in the vocabulary\n            try:\n                similar_words = model_word2vec.wv.most_similar(tag.lower())\n                similar_words = [word_sim[0] for word_sim in similar_words]\n                similar_words_list = similar_words_list + similar_words\n            except:\n                pass\n\n        return list(set(similar_words_list)) #using set allows to filter out duplicates\n\n\n    def RelevantSent(self, search, papers_indexes, topn_sent, tags=None):\n        \"\"\"\n        This is a function that look for more relevant sentences in a list of papers given a search.\n        It returns a dictionary which keys are papers indexes and values are list of top n relevant sentences\n        \"\"\"\n        \n        sentences_df = self.preprocessing_sentences(papers_indexes)\n        model_doc2vec = self.model_doc2vec\n        \n        model_doc2vec.random.seed(1)\n        search = preprocess_string(search)\n        search_vec = model_doc2vec.infer_vector(search)\n\n        sentences_df[\"sent_vec\"] = sentences_df.sentence_token.apply(model_doc2vec.infer_vector).values\n        sentences_df[\"cos_similarity\"] = sentences_df.sent_vec.apply(lambda sent_vec: cosine_similarity(search_vec.reshape(-1,1), sent_vec.reshape(-1,1))[0][0])\n\n        topn_sent_dict = {}\n        \n        if tags:\n            tag_list = preprocess_string(tags) + self.similar_words #including similar words to tags\n            \n        for paper_index in papers_indexes:\n            sent_paper_order = sentences_df.loc[sentences_df.paper_index==paper_index, :]\n            sent_paper_order = sent_paper_order.sort_values(by=\"cos_similarity\", axis=0, ascending=False).reset_index(drop=True)\n\n\n            #keeping only sentences with at least one tag word\n            sent_list_tag = []\n            if tags:\n                sent_list = list(sent_paper_order.loc[:1000, \"sentence\"])\n                for sent in sent_list:\n                    tagIn = self.TagInText(sent, tag_list)\n                    if tagIn:\n                        sent_list_tag.append(sent)\n\n            else:\n                sent_list_tag = list(sent_paper_order.loc[:topn_sent-1, \"sentence\"])\n\n            topn_sent_dict[paper_index] = sent_list_tag[:topn_sent]\n        \n\n        return topn_sent_dict\n\n        \n    \n    def SearchIndex(self, search, pubdate_before=None, pubdate_after=None, pubdate_in=None, tags=None, topn=10, seed=0):\n\n        \"\"\"\n        Take a search string and return list of indexes of n closest papers\n\n        search: string containing the query\n        pubdate_before: to get papers which publication dates are before \"pubdate_before\"; format date : year-month-day\n        pubdate_after: to get papers which publication dates are after pubdate_before. format date : year-month-day\n        pubdate_in: to get papers which publication dates are between date1 and date2. format date : year1-month1-day1 year2-month2-day2\n        tags: string with tag words\n        topn: maximum number of results to be returned,\n        seed: an integer for random seed; useful for the doc2vec model. \n        \"\"\"\n        data = self.papers_df\n        model_doc2vec=self.model_doc2vec\n        \n        #filtering according to date\n        if pubdate_before:\n            data = data[data.publish_time<pubdate_before]\n        if pubdate_after:\n            data = data[data.publish_time>pubdate_after]\n        if pubdate_in:\n            date1 = pubdate_in.split()[0]\n            date2 = pubdate_in.split()[1]\n            data = data[(data.publish_time>=date1)&(data.publish_time<=date2)]\n            \n        #Ranging papers indexes according to their similarity to the search query\n        n = data.shape[0]\n        model_doc2vec.random.seed(seed)\n        search = preprocess_string(search)\n        search_vec = self.model_doc2vec.infer_vector(search)\n        similar_text_index = self.model_doc2vec.docvecs.most_similar([search_vec], topn = n)\n        similar_text_index = [index_sim[0] for index_sim in similar_text_index  if index_sim[0] in data.index]\n        \n        if tags:\n            tag_list = tags.split()\n            self.similar_words = self.SimilarWords(tags)\n            tag_list = tag_list + self.similar_words\n            \n            filter_papers_index = []\n\n            #finding articles containing tags\n            \n            tagged = data[\"paper\"].apply(lambda paper: self.TagInText(paper,tag_list))\n            tagged_papers_index = data[tagged].index\n\n            similar_text_index = [index for index in similar_text_index if index in tagged_papers_index]\n\n        similar_text_index = similar_text_index[:topn]\n\n        return similar_text_index\n\n\n    def SearchDisplayPaper(self, search, pubdate_before=None, pubdate_after=None, pubdate_in=None, topn=10, topn_sent=5, tags=None):\n        \"\"\"\n        Take a request and return most relevant papers, with their titles, authors, abstracts and most relevant sentences\n\n        search: string containing the request\n        pubdate_before: Publication dates should be before pubdate_before. format date : year-month-day\n        pubdate_after: Publication dates should be after pubdate_before. format date : year-month-day\n        pubdate_in: Publication dates should be between pubdate_before. format date : year1-month1-day1 format date year2-month2-day2\n        tags: string with tag words\n        topn: maximum number of results to be returned,\n        topn_sent: maximum number of sentences closely linked to search in a given paper\n        seed: an integer for random seed\n        \"\"\"\n        \n         \n        select_indexes = self.SearchIndex(search=search, pubdate_before=pubdate_before, pubdate_after=pubdate_after, pubdate_in=pubdate_in, tags=tags, topn=topn)\n        top_sent = self.RelevantSent(search, select_indexes, topn_sent=topn_sent, tags=tags)\n\n        \n    \n        summary = \"\"\"\n                <script>\n                function visibilite(thingId)\n                {\n                var targetElement;\n                targetElement = document.getElementById(thingId) ;\n                if (targetElement.style.display == \"none\")\n                {\n                targetElement.style.display = \"\" ;\n                } else {\n                targetElement.style.display = \"none\" ;\n                }\n                }\n                <\/script>\n                <\/body>\n                \"\"\"\n        if tags!=None:   \n            tag_list = preprocess_string(tags)\n            \n        paper_df = self.papers_df\n        \n        for i,index in enumerate(select_indexes):\n            title, authors, date, url, abstract, body_text = list(paper_df.loc[index, [\"title\", \"authors\", \"publish_time\", \"url\", \"abstract\", \"body_text\"]])\n            sentences = top_sent[index]\n            #adding relevant sentences from the paper\n            top_sentences = [\n                \"\"\"\n                <p>&nbsp;<\/p>\n                <h3 style=\"text-align: justify;\"><span style=\"color: #800080;\"><strong>Sentence {nb}:<\/h3>\n                <h4 style=\"text-align: justify;\"><\/strong> <span style=\"color: #000000;\">{sent}<\/span><\/span><\/h4>\n                \"\"\".format(nb=i+1, sent=sent) for i, sent in enumerate(sentences) \n                ]\n\n            top_sentences = \" \".join(top_sentences)\n            \n            \n            if tags!=None:\n                \n                #highlighting similar words to tags in magenta color\n                for tag in self.similar_words:\n                    abstract = re.sub(\" {}| {}| {}\".format(tag.lower(), tag.upper(), tag.capitalize()), \"\"\"<span style=\"color: #ff00ff;\"> {}<\/span>\"\"\".format(tag.capitalize()), abstract)\n                    top_sentences = re.sub(\" {}| {}| {}\".format(tag.lower(), tag.upper(), tag.capitalize()), \"\"\"<span style=\"color: #ff00ff;\"> {}<\/span>\"\"\".format(tag.capitalize()), top_sentences)\n                    body_text = re.sub(\" {}| {}| {}\".format(tag.lower(), tag.upper(), tag.capitalize()), \"\"\"<span style=\"color: #ff00ff;\"> {}<\/span>\"\"\".format(tag.capitalize()), body_text)\n            \n                #highlighting tags in red color\n                for tag in tag_list:\n                    abstract = re.sub(\" {}| {}| {}\".format(tag.lower(), tag.upper(), tag.capitalize()), \"\"\"<span style=\"color: #ff0000;\"> {}<\/span>\"\"\".format(tag.capitalize()), abstract)\n                    top_sentences = re.sub(\" {}| {}| {}\".format(tag.lower(), tag.upper(), tag.capitalize()), \"\"\"<span style=\"color: #ff0000;\"> {}<\/span>\"\"\".format(tag.capitalize()), top_sentences)\n                    body_text = re.sub(\" {}| {}| {}\".format(tag.lower(), tag.upper(), tag.capitalize()), \"\"\"<span style=\"color: #ff0000;\"> {}<\/span>\"\"\".format(tag.capitalize()), body_text)\n                \n                \n            #adding paper's summary\n            summary = summary + \"\"\"\n            <p>&nbsp;<\/p>\n            <p>&nbsp;<\/p>\n            <h1><span style=\"color: #333300; background-color: #ffffff;\">Paper {nb}.<\/span><\/h1>\n            <p>&nbsp;<\/p>\n            <h1><span style=\"color: #000000\";>Title:<span style=\"color: #000080;\"> {title} <\/span><\/span><\/h1>\n            <p>&nbsp;<\/p>\n            <p><strong><span style=\"color: #800000; background-color: #999999;\"><span style=\"background-color: #ffffff;\">Authors: {authors} <\/span><\/span><\/strong><\/p>\n            <p><span style=\"color: #003366;\"><strong>Publication date: {date} <\/strong><\/span><\/p>\n            <p><a href={url} ><span style=\"color: #3366ff;\"><strong>Link to full paper<\/strong><\/span> <\/a><\/p>\n            <p>&nbsp;<\/p>\n            <h2>\n                <a href=\"javascript:visibilite('divid1{html_id}');\"><span style=\"background-color: ##ffffff;\">Abstract<\/span><\/a>\n                | <a href=\"javascript:visibilite('divid2{html_id}');\"><span style=\"background-color: ##ffffff;\">Relevant sentences<\/span><\/a> |\n                | <a href=\"javascript:visibilite('divid3{html_id}');\"><span style=\"background-color: ##ffffff;\">Read paper<\/span><\/a> |          \n            <\/h2>\n            <p>&nbsp;<\/p>\n            <div id=\"divid1{html_id}\" style=\"display:none;\"><\/p>\n                <h4  style=\"text-align: justify;\">{abstract}<\/strong><\/h4>\n            <\/div>\n            <div id=\"divid2{html_id}\" style=\"display:none;\"><\/p>\n                <p  style=\"text-align: justify;\">{top_sentences}<\/strong><\/p>\n            <\/div>\n            <div id=\"divid3{html_id}\" style=\"display:none;\"><\/p>\n                <p  style=\"text-align: justify;\">{body_text}<\/strong><\/p>\n            <\/div>\n            \"\"\".format(nb=i+1, html_id=self.html_index, title=title, authors=authors, date=date, url=url, abstract=abstract, top_sentences=top_sentences, body_text=body_text)\n            \n            self.html_index+=1 \n            \n            \n            \n        summary = HTML(summary)\n    \n        return display(summary)\n    \n    \n    \n","925546e0":"papers_preprocessed_df = pd.read_pickle(\"..\/input\/preprocessedcord-19\/papers_preprocessed.pkl\") \nmeta_df = pd.read_csv(\"..\/input\/CORD-19-research-challenge\/metadata.csv\")\nmodel_doc2vec =  Doc2Vec.load(\"..\/input\/preprocessedcord-19\/doc2vec_model_tag_papers\")\nmodel_word2vec =  Word2Vec.load(\"..\/input\/preprocessedcord-19\/word2vec_cbow.model\")","b87ae1f1":"#papers are preprocessed \npapers_preprocessed_df.head()","6eadcba8":"%%time\nsearching = Search(papers_df=papers_preprocessed_df, papers_preprocessed=True,\n                   model_doc2vec = model_doc2vec)\n","04ec8c87":"search_risk = \"What are covid-19 risk factors?\"\ntags_risk = \"\"\"risk smoking pulmonary disease co-infection respiratory viral infection \ntransmissible virulent co-morbidities co-morbidity Neonate pregnant Socio-economic behavior economic impact\"\"\"\n\nsearching.SearchDisplayPaper(search=search_risk, pubdate_in=\"2019-12-01 2020-04-01\", topn=5, topn_sent=10, tags=tags_risk)","fdb789f1":"search_transmission = \"\"\"\"Transmission dynamics of the virus, including the basic reproductive number, incubation period, serial interval, modes of transmission and environmental factors\"\"\"\ntags_transmission = \"\"\"transmission dynamics reproductive incubation serial interval environmental\"\"\"\n\nsearching.SearchDisplayPaper(search=search_transmission,  pubdate_in=\"2019-12-01 2020-04-01\", topn=5,  topn_sent=10, tags=tags_transmission)","87a1f7be":"search_severity = \"\"\"Severity of disease including risk of fatality \namong symptomatic hospitalized patients and high-risk patient groups about covid-19\"\"\"\n\ntags_severity  = \"\"\"severity disease fatality symptomatic hospitalized high-risk\"\"\"\n\nsearching.SearchDisplayPaper(search=search_severity, pubdate_in=\"2019-12-01 2020-04-01\", topn=5, topn_sent=10, tags=tags_severity)","c336ff6e":"search_susceptibility = \"\"\"Susceptibility of populations\"\"\"\n\ntags_susceptibility  = \"\"\"susceptibility population\"\"\"\n\nsearching.SearchDisplayPaper(search=search_susceptibility, pubdate_in=\"2019-12-01 2020-04-01\", topn=5, topn_sent=10, tags=tags_susceptibility)","16523430":"search_mitigation = \"\"\"Public health mitigation measures\"\"\"\n\ntags_mitigation  = \"\"\"public health mitigation measures government\"\"\"\n\nsearching.SearchDisplayPaper(search=search_mitigation, pubdate_in=\"2019-12-01 2020-04-01\", topn=5, topn_sent=10, tags=tags_mitigation)","33f3267b":"interact(searching.SearchDisplayPaper, search=\"What are covid-19 risks factors?\", \n         pubdate_before=\"\", pubdate_after=\"\", pubdate_in=\"2019-12-01 2020-04-01\",\n         topn=5, topn_sent=10, tags=\"risk covid China\")","e7787438":"But of course, our tool is applicable to any task of the  <a href =https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/tasks> CORD-19 tasks.<\/a>","441d994a":"From now on, we only use the papers dataframe.","24eb16c5":"![saving_preprocessed.PNG](attachment:saving_preprocessed.PNG)","32fd7113":"![doc2vec.PNG](attachment:doc2vec.PNG)","683d2d64":"# To conclude","0fe57fd5":"In the screenshot, the preprocessed papers dataframe is saved in csv format. But for efficiency, we strongly recommend formats such as [pickle](https:\/\/ianlondon.github.io\/blog\/pickling-basics\/).\n\nFrom now, we will no longer need this step of preprocessing before instantiating the **Search** class.\n\nIn the next cell code, we import the objects that have been generated in the preprocessing step.","8029c209":"# Practical utility and Limits of our tool","d6178987":"As you can notice, relevant sentences are not displayed in every results. The reason is we filter out every matched sentences without any tag or similar words. This is to increase relevance of displayed sentences.\n\nYou likely also noticed that some words are semi highlighted (in part). The reason is that in the data processing step, the preprocessing function **preprocess_string** that we used to tokenize papers' texts also includes a stemming function.\n\nThis is useful as using the steming of words allows including every word with the same stem. For instance, using the the stemming of **infection** that is **infect**, we implicity include at the same time **infection**, **infections**,  **infect**,  **infects**, **infected**.","fca97d4a":"### Loading papers data","b08f425d":"### <center> Other team members :","c912ca5e":"# Search engine","5e30380e":"## Q2: Transmission dynamics","7ab90d6b":"## Text engineering","45ba5d9e":"## Limits of our tool","42769116":" <div align=\"justify\"> \n    In this step we extract 4 different tables using the json files from repositories **biorxiv_medrxiv**, **comm_use_subset**, **noncomm_use_subset** and **custom_license**. \nBut in this work we only use the table \"papers\", combined with the meta dataset. \nHowever, the remaining tables can be used to build a more advance searching engine. \nThe task is performed by the following function that just need the directory path to extract the 4 tables. \nTo be efficient in extracting information from thousands of json files, we recommend this function to be used in a parallelized framework. \n <\/div> ","b11e8b6f":"Arguments to settle: \n\n- search: string containing the search query\n        \n- pubdate_before: to get papers with publication dates before pubdate_before. format date : year-month-day\n- pubdate_after: to get papers with publication dates after pubdate_before. format date : year-month-day\n- pubdate_in: to get papers with publication dates between two dates. format date : year1-month1-day1 year2-month2-day2\n- tags: string containing tag words\n- topn: maximum number of results to be returned,\n- topn_sent: maximum number of sentences closely linked to search in a given paper","080e23b4":"To have an idea about what we are doing, here are some screenshots of what we get at the end of this notebook.","94ba2040":"![abstract.PNG](attachment:abstract.PNG)","e9a36a39":"## Q5: Public health mitigation measures","36e9eb68":"#### <center> Results <center> ","fe499c3e":"#### Training a Word2vec model to find lexicons of tags","75638cca":"#### <center> Relevant sentences from a paper in the results <center>","53e1ecf0":"![body_text.PNG](attachment:body_text.PNG)","71bba1b1":"### Creating easy-to-use table from json files","effa2136":"# Creating a python class that integrates all the above steps (with some additional options)","6582e16a":"#### Concatenating title, abstract and body_text","d1ab5430":"\n\nWe want to build a search engine capable to take a request and find matching papers. We also look for most relevant sentences in the papers with regard to the request. \n\nOur methodology is simple. \n\n- We build a doc2vec model with papers dataset: doc2vec allows to vectorize documents.Vectors embed the meaning of documents and can be used for various tasks such as topic modelling. The doc2vec model built can also be used to infer new sentences. This feature will particularly be very useful for next steps. For more explanation about doc2vec, please take a look to at this tutorial: <a href='https:\/\/medium.com\/wisio\/a-gentle-introduction-to-doc2vec-db3e8c0cce5e'> Doc2vec <\/a>\n\n- The doc2vec model is then used to vectorize every papers (we call paper a concatenation of title, abstract and body text) in our dataset and is also used to vectorize search requests. Then cosine similarity is used to find papers and sentences matching the query.\n\n- We present results in a convenient way for user friendliness.\n","c6fd6454":"We want to include an option in the search tool allowing to tag words and highlight them in red.","549b74aa":"# <center> A doc2vec search engine to look over thousands of papers about Covid-19<center> ","2d709c07":"- Data on potential **risks factors** : \n    - **Smoking**, pre-existing **pulmonary disease**\n    - **Co-infections** (determine whether co-existing **respiratory\/viral infections** make the virus more **transmissible** or **virulent**) and other **co-morbidities**\n    - **Neonates** and **pregnant** women\n    - **Socio-economic** and **behavioral factors** to understand the **economic impact** of the virus and whether there were differences.\n\n- **Transmission dynamics** of the virus, including the basic **reproductive number**, **incubation period**, **serial interval**, **modes of transmission** and **environmental factors**\n\n- **Severity of disease**, including **risk of fatality** among **symptomatic hospitalized patients**, and **high-risk patient** groups\n\n- **Susceptibility** of populations\n\n- Public health **mitigation measures** that could be effective for control","fb71b8df":"To present results in a simple and practical way, we make use of html power. ","0da9cc6f":"### Doc2Vec : Representing each paper in a 200 dimensional vector","789db80d":"#### Generating a dataframe for sentences","70a59aad":"<div align=\"justify\"> \nHere we vectorize papers with a Doc2Vec model named \"Paragraph Vector-Distributed Memory\" (PV-DM) as describe in the above image. Each paper is represented by a 200-dimensional vector that embed its meaning.  $a_i \\space  \n\\forall \\space i = 1,..., 200$, are real numbers and coordinates of the 200-dimensional vector. \n\nHere 200 is an arbitrary choice for the vector dimension. In general, higher the dimensionality, better it is, but of course at the cost of computational power. \n\nA nice tutorial about Doc2Vec models is given <a href=https:\/\/medium.com\/wisio\/a-gentle-introduction-to-doc2vec-db3e8c0cce5e> here.<\/a>.  \nFor technical details you can read this <a href=https:\/\/arxiv.org\/pdf\/1405.4053v2.pdf> article.<\/a>. \n\n<\/div> ","311d2b28":"If you are not data scientist or simply do not have time to dive into the codes above, this tool is for you. You can simply write down your search question with related parameters and you will get below the result to your request. \n\n##### NB: You need to import packages at section \"Required packages\" and run the code cell at section \"Creating a python class...\" to create the Search class. ","9cb3e777":"Our search tool can be very useful for health scientists, research centers and more generally to the medical community to deal with the massive flow of information. \n\nWith this tool the community is able to have papers matching any search about Covid-19 and a concrete summary. \n\nThe search tool we built is done almost from scratch. We make this choice rather than using pretrained search tools as we want it to really be specific to Covid-19 litterature.We also wanted to avoid as much as possible the *black-box* side of some pre-existing packages while keeping it as simple as possible. \nHowever, this tool can be used in other contexts: \n- for instance to answer to all the tasks at <a href =https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge\/tasks> CORD-19 tasks,<\/a>\n- or to have summary about other litteratures whether in the health field or beyond as far as data of the same format can be provided.  \n\nWith the interactive tool that we built, no need to be expert in data science to use it. \nWhile with the Python class that we built, we make it simple to integrate this tool in any web or mobile applications. ","34552acf":"# Answering to questions","15e00f87":"A problem with tagging is that several words can be used to search the same thing. \nExample: \"population at risk\" can also be search with \"people at risk\" or even \"susceptible people\". \nTo account for that we build a [Word2Vec](https:\/\/en.wikipedia.org\/wiki\/Word2vec) model capable to find words used in similar contexts. Each word is vectorized in a 100 dimensional vector using [Continuous Bag of Words](https:\/\/towardsdatascience.com\/nlp-101-word2vec-skip-gram-and-cbow-93512ee24314) (CBOW) a particular Word2Vec model. 100 is arbitrarily chosen, but it is generally enough to embed the meaning of words.\n\nSimilar words to tags will be highlighted with the <span style=\"color: #ff00ff;\">magenta<\/span> color.\n","525d7788":"# Why this notebook?","6d92ee5b":"# Required packages","ee8e945e":"#### Training and saving the doc2vec model\n","d4a99961":"### For each relevant paper, find the top n sentences matching the search (in the body text)","34ec582b":"Here is how to save the generated objects:","87845457":"#### Cleaning and preprocessing (tokenization and filtering)","1b8af0db":"It is possible to not get any relevant sentence cause we only keep the ones that contain at least one tag or one similar word. ","e08959d3":"![word2vec.PNG](attachment:word2vec.PNG)","43e859b6":"#### A function to detect tags in texts","66d6051a":"![results.PNG](attachment:results.PNG)","3e010672":"#### Preparing inputs to the doc2vec model\n","e561b825":"We only load 100 papers to explain the different steps of our methodology. Of couse, results are poorer but it should be optimal for those who will run this notebook in order to have deeper insights of what is done around. At the end of the notebook, we make it simple to run every steps through the use of a *class*. For those who are unfamiliar with this notion, it is just a way to structure codes using  [object-oriented programming](https:\/\/en.wikipedia.org\/wiki\/Object-oriented_programming).","6e24bbae":"## Q3: Severity of disease","40fa5869":"You are aware of the current health situation on our planet. The whole world is paralyzed by a respiratory virus called COVID-19, which is a new type of highly contagious coronavirus originating from Wuhan in Hubei Province, China. \n\nThe objective of this notebook is to make a search engine to extract information from papers about this pandemic as simple as possible.  This tool allows bringing out summaries, relevant sentences and body texts of papers matching a search query and presenting the results in a user-friendly way. \n\nIn order to do so, we first vectorize the papers using Doc2Vec techniques. We train the Doc2Vec model on almost 20.000 papers (title+abstract+body text). We are then able to find useful information about any search on Covid-19 in just in a few seconds. \n\n\nThis work is done by a team of Data scientists settled by the association <a href='https:\/\/sites.google.com\/view\/upafro'> UpAfro <\/a>  to answer to the call of action from the <a href='https:\/\/www.kaggle.com\/allen-institute-for-ai\/CORD-19-research-challenge'> COVID-19 Open Research Dataset Challenge (CORD-19) <\/a>. \nThis is our way to contribute to the fight against Covid-19.\n","0bfa522c":"### <center> Team led by [Gilles HACHEME](https:\/\/fr.linkedin.com\/in\/gilles-q-hacheme-a0956ab7)<center>","cfb51229":"#### Instantiating the Search class","eecbadfe":"#####  Example","8aa5bc30":"#### Presenting results","bf1dfd00":"<div align=\"justify\"> \nThis step will enable to give more functionality to our search tool as we can have with meta data information such as : publication dates and url. \n\nWe also notice that sometimes some information such as the asbtract can be missing in the papers table but present in the meta dataset. So we keep titles, abstracts and authors from the meta dataset.\n<\/div> ","3aef0d3b":"# Steps to build the search engine","90ace719":"Arguments to set: \n\n- search: string containing the search query\n        \n- pubdate_before: to get papers with publication dates before pubdate_before. format date : year-month-day\n- pubdate_after: to get papers with publication dates after pubdate_before. format date : year-month-day\n- pubdate_in: to get papers with publication dates between two dates. format date : year1-month1-day1 year2-month2-day2\n- tags: string containing tag words\n- topn: maximum number of results to be returned,\n- topn_sent: maximum number of sentences closely linked to search in a given paper\n\n","d80000ca":"*Source: [Diabetesvoice](https:\/\/diabetesvoice.org\/fr\/nouvelles-en-bref\/le-covid-19-et-le-diabete\/)*","3026c955":"#### A function to get relevant sentences","4fb921ab":"### <center> [Nour\u00e9ini SAYOUTI](https:\/\/www.linkedin.com\/in\/nour%C3%A9ini-sayouti-souleymane-20295aa9\/?originalSubdomain=fr)  <center>\n\n### <center> [Divine Tulomba](https:\/\/www.linkedin.com\/in\/divine-tulomba-912422189\/)  <center>\n    \n### <center> [Gloria SOLOME](https:\/\/www.linkedin.com\/in\/sena-gloria-solome-789717159\/)  <center>","a913c4e9":"### Keeping only papers with metadata","f706783d":"Here we focus on one particular task: **What do we know about COVID-19 risk factors?**.","e95860a2":"## Searching articles ","7fd81860":"\nHere we use the power of Object-Oriented Programming (OOP) in python to synthesize and structure in a proper way all the steps above. This also include the preprocessing steps. But to be efficient, we create an option to directly pass preprocessed data to the class so that we can save lots of time. We add the option of reading the body text if available and a click to show\/hide option to read abstracts, relevant sentences and body texts in a convenient way.\nWith tags highlighting, it is easy to keep track of the essential parts of papers relevant for the search. \n\nIf you did not run the previous data processing steps, the doc2vec and word2vec model building, and you want to do it in one line, just instantiate the class in the following way: **searching = Search(papers_df, meta_df)**.\n\n**papers_df** and **meta_df** are respectively papers and meta dataframes.\n\nOnce the processing is done, save the processed data and the doc2vec model. You can retrieve them using the Search object:\n\n- processed papers : **searching.papers_df**    (saving example: **searching.papers_df.to_csv(path)** )\n- doc2vec model built with papers : **searching.model_doc2vec** (saving example: **searching.model_doc2vec.save(path)** )\n- word2vec model built with papers : **searching.model_word2vec** (saving example: **searching.model_word2vec.save(path)** )\n\nIf you already got preprocessed papers and the related doc2vec and word2vec models, you can directly go to next Section \n    \n##### NB: Do not forget to import packages at section \"Required packages\" and run next cell to create the Search class. ","8525ac79":"Due to our computational power limits we only use about 20.000 papers to learn 200 dimensional vectors to represent papers for our doc2vec search engine. Though this gives pretty good results, we know that with more papers and higher vector dimensionality, the results should even be better. \n\nSo, we strongly recommend to the research community to use more papers, and higher dimensionnality to improve the results from this search engine. \n\nDue to incomplete access to the \"body text\" of some papers, some results could be cut. ","836f641c":"#### <center> Interactive search bar <center> ","759ef7df":"#### Sentences preprocessing","9d204fa5":"We have to mention that this training would be poor as we only use for this tutorial 100 papers to train the doc2vec model. More we have papers, better it should be. \n\n**NB**: The tool we propose at the end of this notebook is trained on almost 20.000 papers. ","e96b524b":"## Q1: Risks factors","30bc1886":"#### <center> Body text from a paper in the results <center>","92a1ac85":"# Methodology","6e6e2855":"#### Loading the complete papers and meta datasets, doc2vec, and word2vec models","10d20dfb":"![COVID_19_image_RS.jpg](attachment:COVID_19_image_RS.jpg)","def15584":"#### A function that find indexes of papers matching to the search","08d66c62":"![sentences.PNG](attachment:sentences.PNG)","73681932":"#### Loading the meta dataset","1e982fb5":"##  Practical utility","db536563":"### Finding the n closest papers to a search","4dd995ef":"#### <center> Abstract from a paper in the results <center> ","e1a0355d":"![preprocessing.PNG](attachment:preprocessing.PNG)","7dc80533":"## Q4: Susceptibility of populations","a4b65473":"![databuilding.PNG](attachment:databuilding.PNG)","45898ac0":"#### Keeping only papers with meta information and transfering information from meta dataframe to papers dataframe","52f8b399":"We hope that this work will be helpful for the medical community and we are completly openned to suggestions to upgrade this work. Be free to get in touch with us. \n\nThis work is powered by [UpAfro](https:\/\/sites.google.com\/view\/upafro), a Pan-African association based in Marseille, France.  ","c53d1bf8":"## Data engineering","da4f3e5f":"We have to mention that the doc2vec and word2vec models have been trained on the complete papers dataset. \nHere is the preprocessing step (screenshot):","7fb15ed5":"# A simple widget for searching","6d1a58f7":"![search.PNG](attachment:search.PNG)"}}