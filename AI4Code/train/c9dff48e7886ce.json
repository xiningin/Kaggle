{"cell_type":{"2550e845":"code","f3f55a4e":"code","eb8fa28f":"code","19a64716":"code","2d317b42":"code","3112cc2d":"code","7b9f76e0":"code","48359215":"code","b890a301":"code","3405c5f3":"code","ec1cd4ab":"code","acda1f70":"code","adc127fb":"code","901d6d29":"markdown","7c92a111":"markdown","58b89cc3":"markdown","e5b43e61":"markdown"},"source":{"2550e845":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f3f55a4e":"filename = '\/kaggle\/input\/alice-and-wonderland-text-file\/wonderland.txt'\nraw_text = open(filename, 'r', encoding='utf-8').read()\nraw_text = raw_text.lower()","eb8fa28f":"#raw_text\n#to view our whole text","19a64716":"# create mapping of unique chars to integers\nchars = sorted(list(set(raw_text)))\nchar_to_int = dict((c, i) for i, c in enumerate(chars))","2d317b42":"n_chars = len(raw_text)\nn_vocab = len(chars)\nprint (\"Total Characters: \", n_chars)\nprint (\"Total Vocab: \", n_vocab)","3112cc2d":"# prepare the dataset of input to output pairs encoded as integers\nseq_length = 100\ndataX = []\ndataY = []\nfor i in range(0, n_chars - seq_length, 1):\n\tseq_in = raw_text[i:i + seq_length]\n\tseq_out = raw_text[i + seq_length]\n\tdataX.append([char_to_int[char] for char in seq_in])\n\tdataY.append(char_to_int[seq_out])\nn_patterns = len(dataX)\nprint (\"Total Patterns: \", n_patterns)","7b9f76e0":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import LSTM\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.utils import np_utils\n...","48359215":"# reshape X to be [samples, time steps, features]\nX = np.reshape(dataX, (n_patterns, seq_length, 1))\n# normalize\nX = X \/ float(n_vocab)\n# one hot encode the output variable\ny = np_utils.to_categorical(dataY)","b890a301":"\n\n# define the LSTM model\nmodel = Sequential()\nmodel.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(y.shape[1], activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')","3405c5f3":"model.summary()","ec1cd4ab":"model.fit(X, y, epochs=20, batch_size=128)","acda1f70":"int_to_char = dict((i, c) for i, c in enumerate(chars))","adc127fb":"# pick a random seed\nimport sys\nstart = np.random.randint(0, len(dataX)-1)\npattern = dataX[start]\nprint (\"Seed:\")\nprint (\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n# generate characters\nfor i in range(1000):\n\tx = np.reshape(pattern, (1, len(pattern), 1))\n\tx = x \/ float(n_vocab)\n\tprediction = model.predict(x, verbose=0)\n\tindex = np.argmax(prediction)\n\tresult = int_to_char[index]\n\tseq_in = [int_to_char[value] for value in pattern]\n\tsys.stdout.write(result)\n\tpattern.append(index)\n\tpattern = pattern[1:len(pattern)]\nprint (\"\\nDone.\")","901d6d29":" we will split the book text up into subsequences with a fixed length of 100 characters, an arbitrary length. We could just as easily split the data up by sentences and pad the shorter sequences and truncate the longer ones.\n\nEach training pattern of the network is comprised of 100 time steps of one character (X) followed by one character output (y). When creating these sequences, we slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it (except the first 100 characters of course).","7c92a111":"we must prepare the data for modeling by the neural network. We cannot model the characters directly, instead we must convert the characters to integers.\n\nWe can do this easily by first creating a set of all of the distinct characters in the book, then creating a map of each character to a unique integer.","58b89cc3":"First we must transform the list of input sequences into the form [samples, time steps, features] expected by an LSTM network.\n\nNext we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.","e5b43e61":" we need to load the ASCII text for the book into memory and convert all of the characters to lowercase to reduce the vocabulary that the network must learn."}}