{"cell_type":{"66669b87":"code","d353e24b":"code","93ca34c4":"code","c4ff71ac":"code","d7fe617c":"code","4e879eb1":"code","98f8c83c":"code","752f9c0b":"code","8781e0a9":"code","166803ff":"code","9bd29faa":"code","afd28e8d":"code","ca4de704":"code","a7a0f35e":"code","d4bbffa4":"code","4f210422":"code","12c61fc8":"code","757d8a98":"code","8829d344":"code","4ad23964":"code","03510b1e":"code","00fb5862":"code","104b0a51":"code","21f229cb":"code","b6636ed2":"code","2b601092":"code","74f3450c":"code","fbff748b":"code","e261b316":"markdown","5cb131a9":"markdown","62adc353":"markdown","26d686dd":"markdown","a2fdd118":"markdown","3b598251":"markdown","06c1f7b6":"markdown","3bf46419":"markdown","cb1e068c":"markdown","07cce362":"markdown","107d5561":"markdown","2ff62e0a":"markdown","53165661":"markdown","b2b509dc":"markdown","be894155":"markdown","a68dc8ae":"markdown","70467aff":"markdown","67fc3c00":"markdown"},"source":{"66669b87":"!pip install rake_nltk\n!pip install pytextrank","d353e24b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport pandas as pd\nimport re\nimport os\nimport spacy\n\n#https:\/\/pypi.org\/project\/rake-nltk\/\nimport rake_nltk\nfrom rake_nltk import Metric, Rake\nr = Rake()\n\n#https:\/\/github.com\/vi3k6i5\/flashtext\nfrom flashtext import KeywordProcessor\n\n#TextRank\n#https:\/\/towardsdatascience.com\/textrank-for-keyword-extraction-by-python-c0bae21bcec0\nimport pytextrank\n\n#Spacy\nimport spacy\nnlp = spacy.load('en')\nimport spacy\nnlp = spacy.load('en_core_web_sm')\n\n\n#Bar\nfrom tqdm import tqdm, tqdm_pandas\ntqdm(tqdm())\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","93ca34c4":"data_train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\nprint(\"Data shape = \",data_train.shape)\ndata_train.head(2)","c4ff71ac":"data_test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nprint(\"Data shape = \",data_train.shape)\ndata_train.head(2)","d7fe617c":"### Own Stop words\nown_stop_word = ['i','we','are','and']\n### Spacy Lemma \ndef spacy_lemma_text(text):\n    doc = nlp(text)\n    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n    tokens = [tok for tok in tokens if tok not in own_stop_word ]\n    tokens = ' '.join(tokens)\n    return tokens\n\n### Remove URL\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)","4e879eb1":"data_train['text_clean'] = data_train['text'].apply(remove_URL)\ndata_train['text_clean'] = data_train['text'].apply(spacy_lemma_text)\nprint(\"Train Cleaning - Done\")\ndata_test['text_clean'] = data_test['text'].apply(remove_URL)\ndata_test['text_clean'] = data_test['text'].apply(spacy_lemma_text)\nprint(\"Test Cleaning - Done\")","98f8c83c":"Rake_keywords = []\nr = Rake()\nr = Rake(min_length=2, max_length=4)\n\nfor text in data_train['text_clean']:\n      r.extract_keywords_from_text(text)\n      r.get_ranked_phrases()\n      Rake_keywords.append(r.get_ranked_phrases())  \n\ndata_train['Rake_Keyword'] = Rake_keywords        ","752f9c0b":"Rake_keywords = []\nr = Rake()\nr = Rake(min_length=2, max_length=4)\n\nfor text in data_test['text_clean']:\n      r.extract_keywords_from_text(text)\n      r.get_ranked_phrases()\n      Rake_keywords.append(r.get_ranked_phrases())  \n\ndata_test['Rake_Keyword'] = Rake_keywords     ","8781e0a9":"from flashtext import KeywordProcessor\nkeyword_processor = KeywordProcessor(case_sensitive=False)","166803ff":"keyword_dict = {\n    \"excaltor\": [\"exclators\", \"excaltors\"]}\nkeyword_processor.add_keywords_from_dict(keyword_dict)\n\n## You can add the important key word i.e prouduct name , features , payments","9bd29faa":"Flash_keywords = []\nfor i in data_train['text_clean']:\n    keyword_processor.extract_keywords(i)\n    Flash_keywords.append(keyword_processor.extract_keywords(i))\n    \ndata_train['Flash_Keyword'] = Flash_keywords    ","afd28e8d":"Flash_keywords = []\nfor i in data_test['text_clean']:\n    keyword_processor.extract_keywords(i)\n    Flash_keywords.append(keyword_processor.extract_keywords(i))\n    \ndata_test['Flash_Keyword'] = Flash_keywords        ","ca4de704":"aspect_terms = []\nfor review in nlp.pipe(data_train.text_clean):\n    chunks = [(chunk.root.text) for chunk in review.noun_chunks if chunk.root.pos_ == 'NOUN']\n    aspect_terms.append(' '.join(chunks))\n    \ndata_train['Aspect_Terms'] = aspect_terms    ","a7a0f35e":"aspect_terms = []\nfor review in nlp.pipe(data_test.text_clean):\n    chunks = [(chunk.root.text) for chunk in review.noun_chunks if chunk.root.pos_ == 'NOUN']\n    aspect_terms.append(' '.join(chunks))\n    \ndata_test['Aspect_Terms'] = aspect_terms    ","d4bbffa4":"sentiment_terms = []\n\nfor review in nlp.pipe(data_train['text_clean']):\n        if review.is_parsed:\n            sentiment_terms.append(' '.join([token.lemma_ for token in review if (not token.is_stop and not token.is_punct and (token.pos_ == \"ADJ\" or token.pos_ == \"VERB\"))]))\n        else:\n            sentiment_terms.append('')  \n            \ndata_train['Sentiment_terms'] = sentiment_terms            ","4f210422":"sentiment_terms = []\n\nfor review in nlp.pipe(data_test['text_clean']):\n        if review.is_parsed:\n            sentiment_terms.append(' '.join([token.lemma_ for token in review if (not token.is_stop and not token.is_punct and (token.pos_ == \"ADJ\" or token.pos_ == \"VERB\"))]))\n        else:\n            sentiment_terms.append('')  \n            \ndata_test['Sentiment_terms'] = sentiment_terms            ","12c61fc8":"import spacy\nimport pytextrank\nnlp = spacy.load('en_core_web_sm')\ntr = pytextrank.TextRank()\nnlp.add_pipe(tr.PipelineComponent, name='textrank', last=True)","757d8a98":"pytext_key = []\n\nfor text in data_train['text_clean']:\n    text = nlp(text)\n    t = text._.phrases\n    pytext_key.append(t)\n    \ndata_train['Pytextrank_keyword'] = pytext_key    ","8829d344":"pytext_key = []\n\nfor text in data_test['text_clean']:\n    text = nlp(text)\n    t = text._.phrases\n    pytext_key.append(t)\n    \ndata_test['Pytextrank_keyword'] = pytext_key    ","4ad23964":"data_train.head()","03510b1e":"data_test.head()","00fb5862":"from IPython.core.display import display, HTML\nimport plotly.graph_objects as go\ndf = data_train.copy()\ndf['length'] = df['text_clean'].apply(len)","104b0a51":"data = [\n    go.Box(\n        y=df[df['target']==1]['length'],\n        name='Real'\n    ),\n    go.Box(\n        y=df[df['target']==0]['length'],\n        name='Not'\n    ),\n\n]\nlayout = go.Layout(\n    title = 'Target Class Vs Comment Lenght (After Cleaning)'\n)\nfig = go.Figure(data=data, layout=layout)\nfig.show()","21f229cb":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.decomposition import TruncatedSVD\n\nimport seaborn as sns\n\ndef get_tf_idf(df, text_field ='text', sent_field=\"airline_sentiment\"):\n    # creating bag of words freq counts post tokenizing\n    count_vect = CountVectorizer()\n    X_bog = count_vect.fit_transform(df[text_field])\n\n    # creating the tf-idf vectors. \n    tf_transformer = TfidfTransformer(norm='l2')\n    X = tf_transformer.fit_transform(X_bog)\n    y = df[sent_field]\n    return X, y","b6636ed2":"X, y = get_tf_idf(df, \"text_clean\", \"target\")\npca = TruncatedSVD(n_components=2, n_iter=7, random_state=4)\npca.fit_transform(X.T)\nax1 = sns.scatterplot(x=pca.components_[0], y=pca.components_[1], hue=y)\nplt.title(\"Text Vs Target\")","2b601092":"X, y = get_tf_idf(df, \"Aspect_Terms\", \"target\")\npca = TruncatedSVD(n_components=2, n_iter=7, random_state=4)\npca.fit_transform(X.T)\nax2 = sns.scatterplot(x=pca.components_[0], y=pca.components_[1], hue=y)\nplt.title(\"Aspect Terms Vs Target\")","74f3450c":"X, y = get_tf_idf(df, \"Sentiment_terms\", \"target\")\npca = TruncatedSVD(n_components=2, n_iter=7, random_state=4)\npca.fit_transform(X.T)\nax3 = sns.scatterplot(x=pca.components_[0], y=pca.components_[1], hue=y)\nplt.title(\"Sentiment Terms Vs Target\")","fbff748b":"print (\"Working on the Aspect Based Extraction....:) \")","e261b316":"# Aspect-Based Opinion Mining","5cb131a9":"## PCA Analysis - Aspect Terms","62adc353":" This is used to replace keywords in sentences or extract keywords from sentences.Below are the Usage\n*  Extract keywords\n*  Replace keywords\n*  Case Sensitive \n*  Span of keywords extracted\n*  Get Extra information with keywords extracted\n*  No clean name for Keywords\n*  Add Multiple Keywords simultaneously\n*  To Remove keywords\n*  To check Number of terms in KeywordProcessor\n*  To check if term is present in KeywordProcessor\n*  Get all keywords in dictionary\n ","26d686dd":"## PCA Analysis - Sentiment Terms","a2fdd118":"# Sentiment Word Extract","3b598251":"PyTextRank is a Python implementation of TextRank as a spaCy extension, used to:\n\n*     Extract the top-ranked phrases from text documents\n*     Infer links from unstructured text into structured data\n*     Run extractive summarization of text documents\n","06c1f7b6":"# PCA Analysis ","3bf46419":"> # Flashtext ","cb1e068c":"# Basic Pre-Processing","07cce362":"# Sentence Lenght Analysis","107d5561":"# Here is the Output ","2ff62e0a":"Aspect-Based Opinion Mining is to extract product's aspects and the associated user opinions from the user text review","53165661":"In this notebook, we will see how to generate insights from text data using NLP techniques. This kernel will be divided into the following parts.\n\n1. Basic Pre-Processing\n2. Rapid Automatic Keyword Extraction\n3. Flashtext\n4. Sentiment Word Extract\n5. Text Rank\n\n** These techniques most useful in the Key topics extraction and contextual sentiment in the product review","b2b509dc":"# Text Rank","be894155":"**RAKE** short for Rapid Automatic Keyword Extraction algorithm, is a domain independent keyword extraction algorithm which tries to determine key phrases in a body of text by analyzing the frequency of word appearance and its co-occurance with other words in the text.\n* This is used for extracting and ranking the keywords\/phrases out of a document without any other context except for the document \nitself.","a68dc8ae":"Key topics extraction and contextual sentiment of users\u2019 reviews\/contents","70467aff":"## PCA Analysis - Pre Processed Text","67fc3c00":"# Rapid Automatic Keyword Extraction"}}