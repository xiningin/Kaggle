{"cell_type":{"cda40900":"code","3f938277":"code","a1218c08":"code","de055dd7":"code","5080e799":"code","74f98066":"code","2f7e06b9":"code","f93f9445":"code","493e791d":"code","5baf1ab0":"code","fea10ba8":"code","b5d3b84d":"code","93e3cb70":"code","d04638c9":"code","1e36e496":"code","c7bd868b":"code","21b15dc0":"code","ad78f1cb":"code","efd8f488":"code","226e42b3":"code","2035cd14":"code","392847d4":"code","557d647d":"code","dc483f36":"code","ae1ebab6":"code","c64bd227":"code","1ffc400e":"code","58226faa":"code","b2a10ca6":"code","632d772d":"markdown","f32be1d1":"markdown","58e063df":"markdown","d3429062":"markdown","afbfcd1f":"markdown","da705456":"markdown","29f7f6a2":"markdown","edd4e6f5":"markdown","84f5426c":"markdown","5697e3c8":"markdown","3f4d7566":"markdown","6c919a60":"markdown","79f97593":"markdown","a5ecf21a":"markdown"},"source":{"cda40900":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport pandas as pd\nimport numpy as np\nimport keras\nfrom keras import Sequential\nfrom keras.layers import Dense, Embedding, Input\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3f938277":"data_simple = pd.read_csv('..\/input\/datafiles\/RegularSeasonCompactResults.csv')\n","a1218c08":"data_simple_2017 = data_simple[data_simple['Season'] == 2017] ### look at 2017 season","de055dd7":"teams = list(set(data_simple_2017['WTeamID'].values.tolist() + data_simple_2017['LTeamID'].values.tolist())) ### get all team codes","5080e799":"team_dict = {val:index for index,val in enumerate(teams)} ### get mapping from team to code","74f98066":"data_simple_2017['WCode'] = data_simple_2017['WTeamID'].apply(lambda x: team_dict[x])\ndata_simple_2017['LCode'] = data_simple_2017['LTeamID'].apply(lambda x: team_dict[x])    #### replace the code with our code","2f7e06b9":"def model_builder(embedding_size,embedding_output_size):\n     \n    ### left input ###\n    left_input = Input(shape = (1,))\n    right_input = Input(shape = (1,))\n    \n    embedding_layer = Embedding(embedding_size,embedding_output_size,input_length=1)\n    \n    left_encode = embedding_layer(left_input)\n    right_encode = embedding_layer(right_input)\n    \n    \n    subtracted = keras.layers.Subtract()([left_encode,right_encode])\n#     lay  = Dense(4,activation = 'relu')(subtracted)\n    \n    out = keras.layers.Dense(2,activation='softmax')(subtracted)\n    return keras.models.Model(inputs=[left_input,right_input],outputs=out)\n","f93f9445":"keras_model = model_builder(351,2)","493e791d":"keras_model.summary()","5baf1ab0":"train = data_simple_2017.sample(frac = .8)\ntest = data_simple_2017.drop(train.index)","fea10ba8":"train_augment = np.zeros((train.shape[0]*2,2))\ntest_augment = np.zeros((test.shape[0],2))","b5d3b84d":"train_augment[0:train.shape[0],:] = train[[\"WCode\",\"LCode\"]].values\ntrain_augment[train.shape[0]:,:] = train[[\"LCode\",\"WCode\"]].values\nX_train = [train_augment[:,0],train_augment[:,1]] #### we augment to remove bias\ntest_augment[0:test.shape[0],:] = test[[\"WCode\",\"LCode\"]].values\n# test_augment[test.shape[0]:,:] = test[[\"LCode\",\"WCode\"]].values\nX_test = [test_augment[:,0],test_augment[:,1]]\ny_train = np.zeros((train.shape[0]*2,1))\ny_train[0:train.shape[0],:] = 1\ny_test = np.ones((test.shape[0],1))\n# y_test[0:test.shape[0],:] = 1\ny_train = keras.utils.to_categorical(y_train,2)\ny_test = keras.utils.to_categorical(y_test,2)   ","93e3cb70":"keras_model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy']) ## compile model","d04638c9":"history = keras_model.fit(X_train,y_train[:,np.newaxis,:],validation_data=(X_test,y_test[:,np.newaxis,:]),\n               epochs=10)","1e36e496":"accuracy = history.history['acc']\nval_accuracy = history.history['val_acc']","c7bd868b":"# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","21b15dc0":"teams  = pd.read_csv(\"..\/input\/datafiles\/Teams.csv\")\n","ad78f1cb":"name_dict = { row['TeamID']:row['TeamName'] for _,row in teams.iterrows()}\nteam_name_map = { team_dict[ID]:name for ID,name in name_dict.items() if ID in team_dict}\npredictions_whole = keras_model.predict(X_test)\npred_whole = np.squeeze(predictions_whole)\ntest['WName'] = test['WCode'].apply(lambda x: team_name_map[x])\ntest['LName'] = test['LCode'].apply(lambda x: team_name_map[x])\ntest['W Win Prob'] = pred_whole[:,1]\ntest[['WName',\"LName\",\"W Win Prob\",\"WScore\",\"LScore\"]]","efd8f488":"def model_builder_joint(embedding_size,embedding_output_size):\n     \n    ### left input ###\n    left_input = Input(shape = (1,))\n    right_input = Input(shape = (1,))\n    \n    embedding_layer = Embedding(embedding_size,embedding_output_size,input_length=1)\n    \n    left_encode = embedding_layer(left_input)\n    right_encode = embedding_layer(right_input)\n    \n    \n    subtracted = keras.layers.Concatenate()([left_encode,right_encode])\n    \n    predict_points = Dense(2,activation = 'relu')(subtracted)\n    concat = keras.layers.Concatenate()([predict_points,subtracted])\n#     lay  = Dense(4,activation = 'relu')(subtracted)\n    \n    out = Dense(2,activation='softmax')(concat)\n    \n    \n    return keras.models.Model(inputs=[left_input,right_input],outputs=[out,predict_points])","226e42b3":"joint_model = model_builder_joint(351,5)\njoint_model.summary()","2035cd14":"y_train_joint = np.zeros_like(y_train)\ny_train_joint[0:train.shape[0],:] = train[['WScore','LScore']].values\ny_train_joint[train.shape[0]:,:] = train[['LScore','WScore']].values\ny_test_joint = test[['WScore','LScore']].values\n","392847d4":"joint_model.compile(optimizer='adam',metrics=['accuracy','mae'],loss=['categorical_crossentropy','mean_squared_error'],\n                   loss_weights=[1,1])","557d647d":"history_joint = joint_model.fit(X_train,\n                [y_train[:,np.newaxis,:],\n                 y_train_joint[:,np.newaxis,:]],\n                validation_data=(X_test,[y_test[:,np.newaxis,:],y_test_joint[:,np.newaxis,:]]),\n               epochs=100,\n                               verbose=False)","dc483f36":"# summarize history for accuracy\nplt.plot(history_joint.history['dense_3_acc'])\nplt.plot(history_joint.history['val_dense_3_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history_joint.history['dense_3_loss'])\nplt.plot(history_joint.history['val_dense_3_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","ae1ebab6":"probs,points = joint_model.predict(X_test)\ntest['W Win Prob Joint'] = np.squeeze(probs)[:,1]\ntest['W Score'] = np.squeeze(points)[:,0]\ntest['L Score'] = np.squeeze(points)[:,1]","c64bd227":"test.head()","1ffc400e":"embeddings = joint_model.get_weights()[0] ### get embeddigns","58226faa":"from sklearn.manifold.t_sne import TSNE ### use tsne to reduce dimensions\nt = TSNE(n_components=2)\nembed_tsne = t.fit_transform(embeddings)","b2a10ca6":"from matplotlib.pyplot import figure\nfigure(num=None, figsize=(30, 30), dpi=80, facecolor='w', edgecolor='k')\n\nplt.scatter(embed_tsne[:,0],embed_tsne[:,1])\n\n\nnames = [val[1] for val in sorted([(key,val) for key,val in team_name_map.items()],key = lambda x: x[0])]\nfor i, txt in enumerate(names):\n    plt.annotate(txt, (embed_tsne[i,0], embed_tsne[i,1]))","632d772d":"I've used an embedding dimension of 2 and subtracted the embeddings of the right from the left. Here, we are just predicting whether the \"left\" stream team won.","f32be1d1":"### Takeaways and Conclusions\nFirst and foremost, I'll have to look into adding a home-court indicator as an input. Furthermore, utilizing tensorflow probability, I could implement hierarchical embedding (where the weights for embedding are drawn from a global distribution) as well as adding hierarchical elements for year, conference, etc. As this analysis was only done for the 2016-2017 season, adding hierarchical embedding for the years would be beneficial. \n\nLooking forward to hearing feedback and comments!","58e063df":"We see Villanova, Oregon, UCLA, Oklahoma St., etc (top teams) are located in one corner of the graph. Poorer teams are located in other corners. Interestingly, there are pockets of clusters. These could potentially be a function of points allowed, points scored, games won.","d3429062":"### Utilizing Embedding to Predict NCAA Games\nInspired by http:\/\/www.sloansportsconference.com\/wp-content\/uploads\/2018\/02\/1008.pdf, an outcome based embedding model, I was tempted to try and predict the outcome of NCAA games. Treating each team as an entity, I look at one season (2017) and see if there is anything to gain from the resulting embeddings afterwards. I then augment the model by jointly estimating the points scored by each team as well, and review the embeddings again. ","afbfcd1f":"We see that since we are jointly estimating the win probabilities, the initial training is unstable! However, over the course of training, we do reach an accuracy of around 70-75% which is what we saw from the original model!","da705456":"### Data\nHere is some simple data pre processing to get us going. ","29f7f6a2":"Looks reasonable... It also seems games closer in score are harder to predict. A further step would be to add a joint loss to estimate the points produced and use that to additionally produce the win-loss probabilities. ","edd4e6f5":"### Visualizing Embeddings\nWe can now visualize the embeddings produced by the model and see if certain points (teams) cluster at certain areas. We will use T-SNE manifold learning to reduce dimensionality.","84f5426c":"The model is slightly overfit but thats ok. We will now look at the model's predictions.\n\n### Model Criticism\n\nLet's get the teams available and see if our model predicts things which makes sense. ","5697e3c8":"### Model\nThe model will be set up such that one team will enter on the left, the other on the right. To augment the training data, I will show one team to each \"side\" of the model. For example, Duke vs. Kansas will appear twice in the data set, one instance where Duke enters the \"left\" stream, and one instance where it enters the \"right\" stream. The output is adjusted as such. ","3f4d7566":"### Adding a joint loss (predicting points scored)\nThis will have the same structure as before, however we will increase the embedding dimensionality and also use these dimensions to predict points scored by each team. This will help regularize our model, as well as provide additional predictive information.","6c919a60":"* Model training seemed to be volatile compared to the original model. However, we see that the joint model changes the win probabilities as a function of the predicted winning score and losing score. Observe how closer games are pushed to .5 and not so close games are pushed to the extremes of 0 and 1. ","79f97593":"### Train The Model ","a5ecf21a":"### Train Joint Model\nJust add an additional output here for the y_values since our input is the same."}}