{"cell_type":{"16887095":"code","4dc0f6fd":"code","a19c2a2c":"code","6eb0a2b8":"code","e62c330c":"code","19d9d567":"code","c86a2aab":"code","ec9d284f":"code","674f3d55":"code","093cd3f5":"code","2a5631c8":"code","deae8b9e":"code","6681eb2f":"code","a880bde6":"code","02918078":"code","028b34ec":"code","24ef1030":"code","5172eafb":"code","7d7fb6dc":"markdown","6b515134":"markdown","00d0bc7a":"markdown","64daaeca":"markdown"},"source":{"16887095":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport cv2\nimport matplotlib.pyplot as plt\nimport os\nprint(os.listdir(\"..\/input\"))\n\n\n# Any results you write to the current directory are saved as output.","4dc0f6fd":"a=cv2.imread('..\/input\/cambridge-orl-dataset\/att_faces\/s1\/1.pgm',0)\nprint(a.shape)\nplt.imshow(a,cmap='gray')\nplt.show()","a19c2a2c":"fig,ax=plt.subplots(2,5,figsize=(15,7))\nfor i in range(0,10):\n    a=cv2.imread('..\/input\/cambridge-orl-dataset\/att_faces\/s1\/{}.pgm'.format(i+1),0)\n    print(a.shape)\n    ax[i%2,i\/\/2].imshow(a,cmap='gray')\nplt.show()","6eb0a2b8":"# face_cascade = cv2.CascadeClassifier('..\/input\/trained-model-haarcascade\/repository\/opencv-opencv-8c25a8e\/data\/haarcascades\/haarcascade_frontalface_default.xml')\nface_cascade = cv2.CascadeClassifier('..\/input\/trained-model-haarcascade\/repository\/opencv-opencv-8c25a8e\/data\/lbpcascades\/lbpcascade_frontalface_improved.xml')\nfig, ax=plt.subplots(10,2,figsize=(10,50))\nfor i in range(10):\n    img=cv2.imread('..\/input\/cambridge-orl-dataset\/att_faces\/s1\/{}.pgm'.format(i+1))\n    gray=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    ax[i,0].imshow(gray)\n    ax[i,0].axis('off')\n    ax[i,0].set_title('Original Image')\n    faces = face_cascade.detectMultiScale(img)\n    for (x,y,w,h) in faces:\n        cv2.rectangle(gray,(x,y),(x+w,y+h),(255,0,0),2)\n        print(i,w*h)\n    ax[i,1].imshow(gray)\n    ax[i,1].axis('off')\n    ax[i,1].set_title('Face detected Image')\nplt.show()","e62c330c":"face_cascade = cv2.CascadeClassifier('..\/input\/trained-model-haarcascade\/repository\/opencv-opencv-8c25a8e\/data\/lbpcascades\/lbpcascade_frontalface_improved.xml')\ntp,fp,fn=[],[],[]\nexception=[]\ncount=0\nlength=48\ndataset=np.zeros((375,length*length+1),dtype=np.uint8)\nsh,sw=0,0\nfor test_case in range(1,41):\n    if test_case==34:\n        continue\n    for i in range(10):\n        img=cv2.imread('..\/input\/cambridge-orl-dataset\/att_faces\/s{}\/{}.pgm'.format(test_case,i+1),0)\n        assert img.shape==(112,92)\n        equ = cv2.equalizeHist(img)\n        blur=cv2.blur(equ,(3,3))\n        faces = face_cascade.detectMultiScale(blur)\n        if len(faces)==0:\n            fn.append((test_case,i+1))\n        elif len(faces)==1 and faces[0][2]*faces[0][3]>2300:\n            x,y,w,h=faces[0]\n#             print(faces[0])\n            tp.append((test_case,i+1))\n            reduced_img=img[x:x+w,y:y+h]\n            reduced_img=cv2.resize(reduced_img,(length,length))\n            reduced_img.shape=(1,length*length)\n            dataset[count,:length*length]=reduced_img\n            dataset[count,length*length]=test_case\n            count+=1\n            sw+=faces[0][2]\n            sh+=faces[0][3]\n            print(test_case,i,faces[0],sw,sh)\n        elif len(faces)==1 and faces[0][2]*faces[0][3]<2300:\n            fp.append((test_case,i+1,faces[0][2]*faces[0][3]))\n        else:\n            print('Multiple detection for {}\/{}'.format(test_case,i+1))\n            count=0\n            for (x,y,w,h) in faces:\n                if w*h>2300:\n                    if count==0:\n                        tp.append((test_case,i+1))\n#                         sw+=w\n#                         sh+=h\n                    else:\n                        fp.append((test_case,i+1,w*h))\n                    count+=1\n                else:\n                    fp.append((test_case,i+1,w*h))\n","19d9d567":"print(len(tp),len(fp),len(fn))","c86a2aab":"print(fn)","ec9d284f":"df=pd.DataFrame(dataset)\ndf.to_csv('dataset.csv')","674f3d55":"from sklearn.metrics import r2_score\nfrom sklearn.metrics import confusion_matrix\n#import xgboost as xgb\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression , Ridge\nfrom sklearn.svm import SVR\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.linear_model import LogisticRegression  # for Logistic Regression algorithm\n\nfrom sklearn import metrics #for checking the model accuracy\n\nfrom sklearn.tree import DecisionTreeClassifier #for using Decision Tree Algoithm\n\nfrom sklearn.ensemble import RandomForestClassifier # A combine model of many decision t\n\nfrom sklearn import svm\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.externals import joblib","093cd3f5":"df.head()","2a5631c8":"print('phase1')\nX_train, X_test, Y_train, Y_test = train_test_split(df.iloc[:,:length**2],df.iloc[:,length**2] , test_size = 0.25)\nprint('phase2')\nprint(X_train.shape,Y_train.shape,X_test.shape,Y_test.shape)\nprint(type(X_train))\nprint(X_train.describe())","deae8b9e":"# Ramdom Forest\nclf2 = RandomForestClassifier(n_estimators=100 ,random_state=15)\nclf2.fit(X_train, Y_train)\npre = clf2.predict(X_test)\n\n#Saving model\nfilename = 'random_forest2.sav'\njoblib.dump(clf2, filename)\n\n#Using Current Classfier\nprint('phase4')\npre=clf2.predict(X_test)\n\n#Printing the accuracy score\nprint('phase5')\nprint(accuracy_score(Y_test,pre))\nprint(confusion_matrix(Y_test, pre))","6681eb2f":"#decison tree classifier\nclf3 = DecisionTreeClassifier()\nclf3=clf3.fit(X_train,Y_train)\npre=clf3.predict(X_test)\n\n#Saving model\nfilename = 'Decison_Tree2.sav'\njoblib.dump(clf3, filename)\n\n#Using Current Classfier\nprint('phase4')\npre=clf3.predict(X_test)\n\n#Printing the accuracy score\nprint('phase5')\nprint(accuracy_score(Y_test,pre))\nprint(confusion_matrix(Y_test, pre))","a880bde6":"#light gbm\nimport lightgbm as lgb\nparams = {\n    'task': 'train',\n    'objective': 'multiclassova',\n    'num_class': '40',\n    'metric': {'multi_logloss', 'multi_error'},\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5\n}\ndtrain = lgb.Dataset(X_train, Y_train)\n#dval = lgb.Dataset(X_val, Y_val)\nclf = lgb.train(params, dtrain,num_boost_round=100)\npreds = clf.predict(X_test)\n\n#Printing the accuracy score\nprint('phase5')\n#print(accuracy_score(Y_test,preds))\nprint(preds)\nprint(confusion_matrix(Y_test, preds))\n\n","02918078":"params = {\n          \"objective\" : \"multiclass\",\n          \"num_class\" : 39,\n          \"num_leaves\" : 60,\n          \"max_depth\": -1,\n          \"learning_rate\" : 0.01,\n          \"bagging_fraction\" : 0.9,  # subsample\n          \"feature_fraction\" : 0.9,  # colsample_bytree\n          \"bagging_freq\" : 5,        # subsample_freq\n          \"bagging_seed\" : 2018,\n          \"verbosity\" : -1 }\n\n\nlgtrain, lgval = lgb.Dataset(X_train, Y_train), lgb.Dataset(X_test, Y_test)\nlgbmodel = lgb.train(params, lgtrain, 2000, valid_sets=[lgtrain, lgval], early_stopping_rounds=100, verbose_eval=200)\n\npreds = clf.predict(X_test)\n\n#Printing the accuracy score\nprint('phase5')\nprint(accuracy_score(Y_test,preds))\nprint(confusion_matrix(Y_test, preds))\n","028b34ec":"from sklearn.svm import SVC, LinearSVC\n#from sklearn.cross_validation import KFold","24ef1030":"n_train = df.shape[0]\nn_test =df.shape[0]\nNFOLDS = 5\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((n_train,))\n    oof_test = np.zeros((n_test,))\n    oof_test_skf = np.empty((NFOLDS, n_test))\n\n    for i, (train_index, test_index) in enumerate(kf):\n        x_tr = x_train.iloc[train_index]\n        y_tr = y_train.iloc[train_index]\n        x_te = x_train.iloc[test_index]\n\n        clf.fit(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train, oof_test.astype(np.uint8)\n","5172eafb":"clf = SVC(kernel='linear',C=0.025)\noof_train, oof_test = get_oof(clf,X_train,Y_train,X_test)\naccuracy_score(oof_train,train_price_range)","7d7fb6dc":"# Loading and displaying 10 .pgm files of same person","6b515134":"# Loading and displaying 1 .pgm file","00d0bc7a":"# Detecting rectangle on 10 images of s1 folder","64daaeca":"# Preprocessing and accuracy of face detection"}}