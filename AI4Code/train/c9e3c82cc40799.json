{"cell_type":{"c9e4cfcb":"code","c154bd33":"code","b40d78bf":"code","8e27f5b8":"code","834d0ac3":"code","505fc8ce":"code","fad9eae6":"code","17a496ce":"code","7977ecc4":"code","d5ddcad4":"code","c303923b":"code","aec5ed8f":"code","34e16af0":"code","597c70c1":"code","e257565e":"code","6507ffae":"code","7f5ee8a4":"code","4495525f":"code","0dfd768a":"code","a16ac5ea":"code","42fc2963":"code","47fdfa0b":"code","c2568e50":"code","b8b37b83":"code","427f6b3f":"code","bd9a0163":"code","97f9c464":"code","37a29317":"code","2884e19b":"code","b7a60cd5":"code","e032b067":"code","00b3ef11":"code","9cb585d0":"code","16d8aaee":"code","b7740db7":"code","7665e022":"code","7709c0ef":"code","9bd88b4d":"code","ec9b3a74":"code","ca01736c":"code","e76abf74":"code","bbdc7364":"code","b6bfce1b":"code","f47d9ef8":"code","19afd4fa":"code","81eff01a":"code","f91bc893":"code","c656e7c0":"code","156971fd":"code","d99189f1":"code","bf35aaff":"code","e7137b1b":"code","1d809986":"code","64d7c80f":"code","82600ccf":"code","08bae44e":"code","5c75c909":"code","69b0d85d":"code","4aabde2e":"code","2e7a988b":"code","65ab6203":"code","4beacef7":"code","f641c7ae":"code","eddd476a":"code","bb395b6b":"code","c4417fd2":"code","da9aad34":"code","16eeff45":"code","2abf60f0":"code","1c19dbc0":"markdown","aed3560f":"markdown","b693a807":"markdown","cb634751":"markdown"},"source":{"c9e4cfcb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport gc\nfrom sklearn import preprocessing\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgbm\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom tqdm import tqdm\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c154bd33":"# Original code from https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage by @gemartin\n# Modified to support timestamp type, categorical type\n# Modified to add option to use float16 or not. feather format does not support float16.\nfrom pandas.api.types import is_datetime64_any_dtype as is_datetime\nfrom pandas.api.types import is_categorical_dtype\n\ndef reduce_mem_usage(df, use_float16=False):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        if is_datetime(df[col]) or is_categorical_dtype(df[col]):\n            # skip datetime type or categorical type\n            continue\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if use_float16 and c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","b40d78bf":"train=pd.read_csv(\"\/kaggle\/input\/ashrae-energy-prediction\/train.csv\")\nbuilding_info=pd.read_csv(\"\/kaggle\/input\/ashrae-energy-prediction\/building_metadata.csv\")\n# weather_info_test=pd.read_csv(\"\/kaggle\/input\/ashrae-energy-prediction\/weather_test.csv\")\n# test=pd.read_csv(\"\/kaggle\/input\/ashrae-energy-prediction\/test.csv\")\nweather_info_train=pd.read_csv(\"\/kaggle\/input\/ashrae-energy-prediction\/weather_train.csv\")","8e27f5b8":"train_all=train.merge(building_info,on='building_id',how='left')\ntrain_all=train_all.merge(weather_info_train,on=['site_id', 'timestamp'],how='left')\ntrain_all.head()\ngc.collect()","834d0ac3":"del weather_info_train","505fc8ce":"# delete the columns I don't want\ntrain_all=train_all.drop(['precip_depth_1_hr','sea_level_pressure','wind_direction','wind_speed'],axis=1)\ngc.collect()\n# There  are 12 columns after dropping 4 columns related to weather. There are \n# 20216100 rows in the train_all dataframe before dropping any rows.","fad9eae6":"train_all=train_all.drop(train_all[(train_all['building_id']<= 104) & (train_all['meter']==0) & (train_all['timestamp']<= \"2016-05-21\")].index)\n","17a496ce":"train_all[\"timestamp\"] = pd.to_datetime(train_all[\"timestamp\"])\ntrain_all[\"hour\"] = train_all[\"timestamp\"].dt.hour# the hour of a day\n# I feel like which day it is in a month might not change the energy consumption pattern much.\ntrain_all[\"day\"] = train_all[\"timestamp\"].dt.day# the day of a month \ntrain_all[\"dayofweek\"] = train_all[\"timestamp\"].dt.weekday# the day of a week; same as dt.dayofweek\ntrain_all[\"month\"] = train_all[\"timestamp\"].dt.month# the month of a year\ntrain_all.drop('timestamp',axis=1,inplace=True)\ntrain_all\ngc.collect()","7977ecc4":"# By taking the logarithm of the meter_reading, the time series data will be more stable, and easier to be predicted.\ntrain_all['log_meter_reading'] = np.log1p(train_all['meter_reading'])\ngc.collect()","d5ddcad4":"del train_all['meter_reading']","c303923b":"train_all.head()","aec5ed8f":"# treating which time related variable as catogorical variable needs further study.\n# Here, I take 'month','hour of a day', 'day of a month' as timestamps, and take 'day of a week'\n# as categorical variable.\n# What's the difference between taking a time related variable as numerical and categorical colum?\ncategorical_feacture=['building_id', 'site_id','primary_use','dayofweek','meter','hour','day','month']\nnumerical_feature=['square_feet', 'year_built', 'floor_count','air_temperature', 'cloud_coverage', 'dew_temperature']\nfeature_columns=categorical_feacture+numerical_feature\nlabel_column='log_meter_reading'\ngc.collect()","34e16af0":"# fill in the NaNs in air_temperature and cloud_coverage with the mean of temperatures from \n# the same day of a month during a certain month and a certain site.\n# This is because, the temperature changes over month and the day in a month.\nair_temperature_na_substitue = pd.DataFrame(train_all.groupby(['site_id','day','month'])['air_temperature'].mean(),columns=[\"air_temperature\"])\ntrain_all=train_all.set_index(['site_id','day','month'])\ntrain_all.update(air_temperature_na_substitue,overwrite=False)\ndel air_temperature_na_substitue\n\ncloud_coverage_na_substitue = pd.DataFrame(train_all.groupby(['site_id','day','month'])['cloud_coverage'].mean(),columns=[\"cloud_coverage\"])\ntrain_all.update(cloud_coverage_na_substitue,overwrite=False)\ndel cloud_coverage_na_substitue\n\ndew_temp_na_substitue=pd.DataFrame(train_all.groupby(['site_id','day','month'])['dew_temperature'].mean(),columns=[\"dew_temperature\"])\ntrain_all.update(dew_temp_na_substitue,overwrite=False)\ndel dew_temp_na_substitue\n\ntrain_all=train_all.reset_index()\n\ngc.collect()","597c70c1":"# fill in NaNs of floor_count with mean\ntrain_all[\"floor_count\"].fillna(train_all['floor_count'].mean(), inplace=True)","e257565e":"# train_all.isna().sum()","6507ffae":"# train_all # 19867540 rows \u00d7 15 columns","7f5ee8a4":"# reduce the memory\ntrain_all=reduce_mem_usage(train_all)\n# building_info is 1449 rows \u00d7 6 columns\nbuilding_info=reduce_mem_usage(building_info)","4495525f":"train_all.head()","0dfd768a":"# prepare the train_y, train_x for each meter type\ndef create_x_y(train_df, target_meter):\n    target_train_df = train_df[train_df['meter'] == target_meter]\n    train_x = target_train_df[feature_columns]\n    train_y = target_train_df['log_meter_reading'].values\n\n    del target_train_df\n    return train_x, train_y\n# After this function, the train_x is dataframe, while train_y is a numpy array.\n# But what is the usual format of dataset to feed into a LGBM model?","a16ac5ea":"train_x0, train_y0 = create_x_y(train_all, target_meter=0)\nsns.distplot(train_y0)\ndel train_x0, train_y0","42fc2963":"train_x1, train_y1 = create_x_y(train_all, target_meter=1)\nsns.distplot(train_y1)\ndel train_x1, train_y1","47fdfa0b":"train_x2, train_y2 = create_x_y(train_all, target_meter=2)\nsns.distplot(train_y2)\ndel train_x2, train_y2","c2568e50":"train_x3, train_y3 = create_x_y(train_all, target_meter=3)\nsns.distplot(train_y3)\ndel train_x3, train_y3","b8b37b83":"def fit_models(train_df,target_meter,folds=2,seed=None,shuffle=False, num_rounds=1500,lr=0.1,bf=0.1,l2=0.2, nl = 30):\n    kfold = KFold(n_splits=folds, shuffle = shuffle, random_state = seed)\n    train_x, train_y = create_x_y(train_df, target_meter)\n    gc.collect()\n    \n    print('target_meter: ',target_meter)\n    print('shape: ',train_x.shape[0])\n    \n    categoricals = [train_x.columns.get_loc(c_col) for c_col in categorical_feacture]\n    print('categoricals: ', categoricals)\n    \n    models = []\n    \n    for train_idx, val_idx in kfold.split(train_x,train_y):\n        xtrain = train_x.iloc[train_idx,:]\n        xval = train_x.iloc[val_idx,:]\n        ytrain = train_y[train_idx]\n        yval = train_y[val_idx]\n        print('')\n        print('train shape: ', len(train_idx))\n        print('valid shape: ', len(val_idx))\n\n        params = {'boosting_type': 'gbdt',\n                  'objective': 'regression',\n                  'metric': {'rmse'}, # maybe l2?\n                  'bagging_freq': 5, # maybe try 1 or 5?\n                  'bagging_fraction': bf, # maybe try 0.7?\n                  'learning_rate': lr, # maybe try 0.3 or 0.05?\n                  'num_leaves': nl, # maybe try 330 or more?\n                  'feature_fraction': 0.9, # maybe try other values?\n                  'lambda_l2': l2 # maybe try other values?  \n        }\n        \n        early_stopping_condition = 30 # try other values\n        verbose_evaluation = 20 # try other values\n        \n        lgb_train_ds = lgbm.Dataset(xtrain, label = ytrain, categorical_feature = categoricals)\n        lgb_val_ds = lgbm.Dataset(xval, label = yval, categorical_feature = categoricals)\n        \n        print('Training GBM: ')\n        \n        model = lgbm.train(params,\n                           train_set = lgb_train_ds,\n                           num_boost_round = num_rounds,\n                           valid_sets = (lgb_train_ds, lgb_val_ds),\n                           early_stopping_rounds = early_stopping_condition,\n                           verbose_eval = verbose_evaluation)\n        \n        models.append(model)\n    \n    gc.collect()\n    return models\n        ","427f6b3f":"models_0 = fit_models(train_all,target_meter=0,folds=5,num_rounds = 1000, lr = 0.1,bf = 0.7,l2 = 0.2,nl = 50)","bd9a0163":"models_1 = fit_models(train_all,target_meter=1,folds=5,num_rounds = 1000, lr = 0.1,bf = 0.7,l2 = 0.2,nl = 50)","97f9c464":"models_2 = fit_models(train_all,target_meter=2,folds=5,num_rounds = 1000, lr = 0.1,bf = 0.7,l2 = 0.4, nl = 50)","37a29317":"models_3 = fit_models(train_all,target_meter=3,folds=5,num_rounds = 1000, lr = 0.1,bf = 0.7,l2 = 0.4, nl = 50)","2884e19b":"del train_all","b7a60cd5":"gc.collect()","e032b067":"#building_info=pd.read_csv(\"\/kaggle\/input\/ashrae-energy-prediction\/building_metadata.csv\")\nweather_info_test=pd.read_csv(\"\/kaggle\/input\/ashrae-energy-prediction\/weather_test.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/ashrae-energy-prediction\/test.csv\")","00b3ef11":"# Now it's going to be a pain in the ass, because I didn't wrap the data preprocessing in a function.\n# Repeat the process for the test data\ntest_all=test.merge(building_info,on='building_id',how='left')\ntest_all=test_all.merge(weather_info_test,on=['site_id', 'timestamp'],how='left')\ntest_all.head()\n# test_all=reduce_mem_usage(test_all)\ngc.collect()","9cb585d0":"del weather_info_test, building_info\ngc.collect()","16d8aaee":"test_all.drop('row_id',axis=1,inplace=True)\ngc.collect()","b7740db7":"test_all=test_all.drop(['precip_depth_1_hr','sea_level_pressure','wind_direction','wind_speed'],axis=1)\ngc.collect()","7665e022":"test_all.dtypes","7709c0ef":"test_all[\"timestamp\"] = pd.to_datetime(test_all[\"timestamp\"])\ntest_all[\"hour\"] = test_all[\"timestamp\"].dt.hour# the hour of a day\n# I feel like which day it is in a month might not change the energy consumption pattern much.\ntest_all[\"day\"] = test_all[\"timestamp\"].dt.day# the day of a month \ntest_all[\"dayofweek\"] = test_all[\"timestamp\"].dt.weekday# the day of a week; same as dt.dayofweek\ntest_all[\"month\"] = test_all[\"timestamp\"].dt.month# the month of a year\ntest_all\ngc.collect()","9bd88b4d":"test_all.drop(test_all[(test_all['building_id']<= 104) & (test_all['meter']==0) & (test_all['timestamp']<= \"2016-05-21\")].index,inplace=True)\ngc.collect()","ec9b3a74":"test_all.drop('timestamp',axis=1,inplace=True)","ca01736c":"test_all=reduce_mem_usage(test_all)","e76abf74":"air_temperature_na_substitue = pd.DataFrame(test_all.groupby(['site_id','day','month'])['air_temperature'].mean(),columns=[\"air_temperature\"])\ntest_all=test_all.set_index(['site_id','day','month'])\ntest_all.update(air_temperature_na_substitue,overwrite=False)\ndel air_temperature_na_substitue\n\ncloud_coverage_na_substitue = pd.DataFrame(test_all.groupby(['site_id','day','month'])['cloud_coverage'].mean(),columns=[\"cloud_coverage\"])\ntest_all.update(cloud_coverage_na_substitue,overwrite=False)\ndel cloud_coverage_na_substitue\n\ndew_temp_na_substitue=pd.DataFrame(test_all.groupby(['site_id','day','month'])['dew_temperature'].mean(),columns=[\"dew_temperature\"])\ntest_all.update(dew_temp_na_substitue,overwrite=False)\ndel dew_temp_na_substitue\n\ntest_all=test_all.reset_index()\n\ngc.collect()","bbdc7364":"test_all[\"floor_count\"].fillna(test_all['floor_count'].mean(), inplace=True)","b6bfce1b":"test_all","f47d9ef8":"def create_x(test_df,target_meter):\n    target_df = test_df[test_df['meter'] == target_meter]\n    test_x = target_df[feature_columns]\n    return test_x\n\ndef create_predictions(test_set, models, batch_size):\n    i = 0\n    ret = []\n    for j in tqdm(range(int(np.ceil(test_set.shape[0] \/ batch_size)))):\n        ret.append(np.expm1(sum([model.predict(test_set.iloc[i:i+batch_size]) for model in models]) \/ len(models)))\n        i += batch_size\n    return ret\n\ndef generate_results(test_df, target_meter, models,batch_size = 1):\n    test_x = create_x(test_df,target_meter)\n    gc.collect()\n\n    test_y = create_predictions(test_x,models,batch_size)\n\n    # consider sns plots\n\n    del test_x\n    gc.collect()\n    return test_y","19afd4fa":"%%time\ntest_y_0 = generate_results(test_all,target_meter = 0, models = models_0, batch_size = 100000)","81eff01a":"%%time\ntest_y_1 = generate_results(test_all,target_meter = 1, models = models_1, batch_size = 100000)","f91bc893":"%%time\ntest_y_2 = generate_results(test_all,target_meter = 2, models = models_2, batch_size = 100000)\n","c656e7c0":"%%time\ntest_y_3 = generate_results(test_all,target_meter = 3, models = models_3, batch_size = 100000)\n","156971fd":"sample_submission=pd.read_csv(\"\/kaggle\/input\/ashrae-energy-prediction\/sample_submission.csv\")\nsample_submission.head()\n","d99189f1":"test_y_0 = np.concatenate(test_y_0)\nsns.distplot(np.log1p(test_y_0))\ngc.collect()","bf35aaff":"test_y_1 = np.concatenate(test_y_1)\nsns.distplot(np.log1p(test_y_1))\ngc.collect()","e7137b1b":"test_y_2 = np.concatenate(test_y_2)\nsns.distplot(np.log1p(test_y_2))\ngc.collect()","1d809986":"test_y_3 = np.concatenate(test_y_3)\nsns.distplot(np.log1p(test_y_3))\ngc.collect()","64d7c80f":"sample_submission.loc[test_all['meter'] == 0, 'meter_reading'] = test_y_0\nsample_submission.loc[test_all['meter'] == 1, 'meter_reading'] = test_y_1\nsample_submission.loc[test_all['meter'] == 2, 'meter_reading'] = test_y_2\nsample_submission.loc[test_all['meter'] == 3, 'meter_reading'] = test_y_3\ngc.collect()","82600ccf":"sample_submission.head()","08bae44e":"sample_submission.to_csv('submission.csv', index = False, float_format='%.4f')\n","5c75c909":"# # test_y_0_export = pd.DataFrame(test_y_0)\n# test_y_0_export.to_csv('test_y_0.csv', index = False)","69b0d85d":"# test_y_1_export = pd.DataFrame(test_y_1)\n# test_y_1_export.to_csv('test_y_1.csv', index = False)","4aabde2e":"# test_y_2_export = pd.DataFrame(test_y_2)\n# test_y_2_export.to_csv('test_y_2.csv', index = False)","2e7a988b":"# test_y_3_export = pd.DataFrame(test_y_3)\n# test_y_3_export.to_csv('test_y_3.csv', index = False)","65ab6203":"# train111=pd.read_csv(\"\/kaggle\/input\/ashrae-energy-prediction\/train.csv\")\n","4beacef7":"# train111.head()","f641c7ae":"# train111.groupby('meter')['meter_reading'].describe()","eddd476a":"# np.mean(test_y_2)","bb395b6b":"# sample_submission=pd.read_csv(\"\/kaggle\/input\/ashrae-energy-prediction\/sample_submission.csv\")\n# sample_submission.head()\n","c4417fd2":"# path = '..\/input\/test-results\/'\n# test_y_0_df = pd.read_csv(path +'test_y_0.csv')\n# test_y_1_df = pd.read_csv(path +'test_y_1.csv')\n# test_y_2_df = pd.read_csv(path +'test_y_2.csv')\n# test_y_3_df = pd.read_csv(path +'test_y_3.csv')","da9aad34":"# test_y_0 = np.array(test_y_0_df).flatten()\n# test_y_1 = np.array(test_y_1_df).flatten()\n# test_y_2 = np.array(test_y_2_df).flatten()\n# test_y_3 = np.array(test_y_3_df).flatten()\n","16eeff45":"# sample_submission.loc[test_all['meter'] == 0, 'meter_reading'] = test_y_0\n# sample_submission.loc[test_all['meter'] == 1, 'meter_reading'] = test_y_1\n# sample_submission.loc[test_all['meter'] == 2, 'meter_reading'] = test_y_2\n# sample_submission.loc[test_all['meter'] == 3, 'meter_reading'] = test_y_3\n# gc.collect()\n# sample_submission.head()","2abf60f0":"# sample_submission.to_csv('submission.csv', index = False, float_format='%.4f')\n","1c19dbc0":"## Start Test data preparation","aed3560f":"For individual tests","b693a807":"The reason I used Light GBM:\nIn this dataset, the building_id column, as a categorical column,  identifies each building. To include a categorical variable in a ML model, we usually need to encode the variable by methods such as one-hot encoding, lable encoding, etc. However, since there are 1449 building ids in the variable, one-hot encoding will significantly increase the dimension of the data and use up the RAM. If lable encoding is applied, the building_id will be taken as a continuous column by the model, which is not correct. \n\nIn LGBM, the categorical variables can be used without one-hot or label encoding. Instead, Fisher's method is used to find the optimal split of catgorical variables. \n\nThus, tree-based algorithm LGBM is used instead of any neural network algorithms for its better strategy in dealing with categorical variables.\n\nAnother reason for me to choose the tree-based algorithm is that Deep neural networks are working much better with images, sound, language and other \u201cnatural\u201d data, while tree-boosting frameworks show better performance given good hand-crafted features. In this project, the dataset is structured and tabulated. Thus, the feature columns are clear and tree-based algorithms can be used.","cb634751":"As shown above, there are many NaN values in the following columns: 'year_built', 'floor_count', 'cloud_coverage', 'air_temperature', and 'dew_temperature'. The methods used to deal with the missing values can result in very different results in the model. The missing values in these columns are dealt in different ways.\n\nFor 'cloud_coverage', 'air_temperature', and 'dew_temperature', a missing value is filled with the mean by averaging the values from the same day of the same month at the same site as the missing value. This is because these variables change over site, month, and the day in a month.\n\nFor 'floor_count', the mean value of the floor_count column is used to fill in the missing values.\n\nFor 'year_built', the missing values are left with further operation. LGBM will ignore missing values during a split, then allocate them to whichever side reduces the loss the most"}}