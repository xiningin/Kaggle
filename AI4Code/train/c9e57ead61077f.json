{"cell_type":{"fec776c5":"code","63328285":"code","2e57f979":"code","30db94d1":"code","64972382":"code","5e3112b2":"code","578f1f1d":"code","5435dd19":"code","ded913ea":"code","094b29de":"code","254718fa":"code","a81a4ec4":"code","9630beb3":"code","7255f435":"code","0ceb9903":"code","ed0e4c04":"code","7308ec37":"code","9695257c":"code","607b0722":"code","00d16f05":"code","a609af32":"code","76c775a8":"code","42ace334":"code","435f5b3b":"code","a9edbd2e":"code","11bb292f":"code","41bb7daf":"code","91ed0815":"code","4dfbd62e":"code","6417eebd":"code","3d0bc515":"code","904e3133":"code","824d69e8":"code","779a2f9c":"code","331330b7":"code","d39f2053":"code","9ea32ec0":"code","32925900":"code","4d02ccd8":"code","562668ac":"code","ed7923fa":"code","5f767801":"code","6553d891":"code","399a8a73":"code","39357d64":"code","66c95cfa":"code","9dbce2b9":"code","757bdbcb":"code","6fc3fa00":"code","07f15e3e":"code","b049fde6":"code","52759a0c":"code","91e27db7":"code","74bc88d4":"code","cb09e684":"code","d1719e13":"code","3fe09c90":"code","a51815be":"code","9bdefa94":"code","bac446b2":"markdown","fd747494":"markdown","1f7270e8":"markdown","5750ee08":"markdown","31207dca":"markdown","0d4f3f77":"markdown","98255e83":"markdown","60592d9b":"markdown","5dd14889":"markdown","8cd76086":"markdown","85c7b52f":"markdown","a50e776f":"markdown","3fd65c2d":"markdown","c914c94c":"markdown","0472b28c":"markdown","920bd368":"markdown","68213f68":"markdown","32af30ed":"markdown","508c5ccf":"markdown","7dfaeb82":"markdown","27283d99":"markdown","4d72f4b4":"markdown","7162c640":"markdown","a74201d4":"markdown","43409ccd":"markdown","eb3fe133":"markdown","a057dd8a":"markdown","7ce7877b":"markdown","517065c5":"markdown","43b24f92":"markdown","1c63c09a":"markdown","70f7edf4":"markdown","bb6179ef":"markdown","7cbb3da4":"markdown","deb94057":"markdown","38cc22f7":"markdown","1d375c97":"markdown","1ebc145b":"markdown","2a2691e2":"markdown","0bd4cce3":"markdown","afb5d267":"markdown","61e68bb4":"markdown","99522315":"markdown","626e199f":"markdown","f2f872cc":"markdown","3060ab6d":"markdown"},"source":{"fec776c5":"import numpy as np\nimport pandas as pd\nimport os\n\nimport keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Flatten, Dense, Embedding, LSTM\nfrom keras import metrics\n\nimport sklearn.metrics\nfrom sklearn.preprocessing import LabelEncoder\n\nimport random\nimport sys\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","63328285":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2e57f979":"# path to data directory\ndata_dir = \"\/kaggle\/input\/tweet-emotion-dataset\/tweet_emotion_dataset.xlsx\"","30db94d1":"# Rreading and displaying first five rows of the data\ndf = pd.read_excel(data_dir)\ndf.head()","64972382":"# Looking for the total number of records and columns\ndf.shape     # (21051, 2) -> 21051 are total records and 2 are total columns","5e3112b2":"# Looking for label names\ndf['Label'].unique()","578f1f1d":"# Storing label names in a list to be referred later\nlabel_names = list(df['Label'].unique())\nlabel_names     # ['surprise', 'sadness', 'joy', 'disgust', 'fear', 'anger']","5435dd19":"# Checking for null values in the dataset\ndf.isnull().sum()  # Found 803 null values in \"Tweet\" column","ded913ea":"df.groupby('Label').count() # looking for total no. records with each label","094b29de":"df.dropna(how='any', inplace=True)  # Removing null values;inplace=True will affect the original dataset","254718fa":"df.isnull().sum()  # to verify that we have removed null values from the dataset","a81a4ec4":"df.shape   # After removing null values we have 20248 records now","9630beb3":"list(df[df['Label']=='joy']['Tweet'].values) # this is how we can get tweets for specific label","7255f435":"label_data = {}#Storing data for each label as (key:value) --> (labelname: all text for labelname as str)\n\nfor label in label_names: # iterating over all labels\n    # getting all tweets regarding each label and converting it as single string and store in dict\n    label_data[label] = \" \".join(list(df[df['Label']== label]['Tweet'].values)).lower()\n    ","0ceb9903":"label_data['joy'][:100] # this is how text has been now stored in label_data dictionary to show 100 chars","ed0e4c04":"le = LabelEncoder()    # LabelEncoder for scikitlearn to encode labels\nle.fit(df['Label'])    # fit on labels from the dataset\ndf['Label_encoded'] = le.transform(df['Label']) # transform data labels and add a new column in dataset\ndf.head()   # look at first five rows ","7308ec37":"texts = df['Tweet'].values           # storing into texts a list of all tweets from the dataset\nlabels = df['Label_encoded'].values  # corresponding labels\nlabels_values = df['Label'].values   # and their encoded values","9695257c":"print(texts[0])  # looking at the first tweet, its encoded value and label respectively\nprint(labels[0], labels_values[0])","607b0722":"le.classes_ # looking at total labels encoded","00d16f05":"# creating a mapping between label names and their encoded values\nlabel_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\nprint(label_mapping)","a609af32":"max_len = 200               # Maximum len of input\nmax_words = 20000           # Maximum unique words \nembedding_dim = 100         # Embedding Dimension to be used (100 in case of Glove-6b-100d)\ntraining_percentage = 0.9   # 90% data for trainging\ntraining_samples = int(len(texts) * training_percentage) # no. of training samples to be selected\nprint(training_samples)","76c775a8":"tokenizer = Tokenizer(num_words=max_words) # tokenizing on 20,000 unique words in our case\ntokenizer.fit_on_texts(texts)              # training tokenizer on text from dataset\nword_index = tokenizer.word_index          # storing (kye: value) --> (word: token) \nprint(\"total %s unique tokens \"%len(word_index))  # total unique tokens","42ace334":"sequences = tokenizer.texts_to_sequences(texts) # generate sequences of tokens from dataset text\ndata = pad_sequences(sequences, maxlen=max_len) # padding sequences upto the maximum considered length\n\nlabels = np.asarray(labels)            # converting labels into numpy array\nlabels = to_categorical(labels)        # one-hot-encoding of labels because we have multi-class labels\n\nprint(\"Shape of data tensor \", data.shape)    # looking at shape of data and labels tensors\nprint(\"Shape of label tensor \", labels.shape)\n\nindices = np.arange(data.shape[0]) # generating random list of indices for data selection\nnp.random.shuffle(indices)         # shuffling to randomize further for better selection of training data\ndata = data[indices]               # this will shuffle data using randomized indices\nlabels = labels[indices]         # shuffling their corresponding labels without loosing their connection\n\nx_train = data[:training_samples] # spliting data into training and testing x_train -> training features\ny_train = labels[:training_samples, :] # y_train --> training labels\nx_test = data[training_samples:]       # x_test --> validation features\ny_test = labels[training_samples:, :]  # y_test --> validation labels\n","435f5b3b":"# looking at shapes of [x_train, y_train, x_test, y_test]\nprint(\"Training Features : \", x_train.shape)\nprint(\"Training Labels : \", y_train.shape)\nprint(\"Testing Features : \", x_test.shape)\nprint(\"Testing Labels : \", y_test.shape)\n","a9edbd2e":"glove_dir = \"\/kaggle\/input\/glove6b\/glove.6B.100d.txt\" # path to glove-6b-100d word embedding file\n\nembeddings_index = {}       # to store embedding indices\nfile = open(glove_dir, encoding='utf8')  # reading the embedding file\nfor line in file:           # iterating over all lines in file\n    values = line.split()   # spliting line into word,and corresponding embedding vector\n    word = values[0]        # each word\n    coefs = np.asarray(values[1:], dtype='float32') # and corresponding embedding\n    embeddings_index[word] = coefs # storing in embedding_index dictionary\nfile.close() # close the file\nprint(\"Found %s word vectors \"%len(embeddings_index)) # look at total no. of embedding vectors","11bb292f":"# creating embedding matrix with (20000, 100) dimensions\nembedding_matrix = np.zeros((max_words, embedding_dim))\nfor word, i in word_index.items(): # iterating over the tokenized words from dataset\n    embedding_vector = embeddings_index.get(word) # get their vector from glove-6b-100d embedding\n    if i < max_words: # until the maximum words\n        if embedding_vector is not None: # if vector exists for the word\n            embedding_matrix[i] = embedding_vector # update embedding_matrix accordingly","41bb7daf":"model = Sequential()    # defining the Sequential mode\n#                   (max_words = 20000, embedding_dim = 100, input_length = 200)\nmodel.add(Embedding(max_words, embedding_dim, input_length=max_len)) # add an embedding layer\nmodel.add(LSTM(32))   # add an LSTM Layer with 32 units (dimentionality of the output space)\nmodel.add(Dense(6, activation='softmax')) # add Fully-connected Dense layer with 6 output layers\n                                          # because we have 6 labels and softmax activation function\nmodel.summary()                           # looking at the summary of the model","91ed0815":"model.layers[0].set_weights([embedding_matrix]) # set glove-embedding as weights of initial layer \nmodel.layers[0].trainable = False               # make initial layer as non-trainable","4dfbd62e":"# Compile Model on RMSprop optimizer, categorical_crossentropy loss, and accuracy, precision,\n# and recall metrics\nmodel.compile(optimizer='rmsprop',\n             loss = 'categorical_crossentropy',\n             metrics = ['acc', metrics.Precision(), metrics.Recall()])","6417eebd":"# Training the model on 10 epochs, 128 batch size, (x_train, y_train) -> training data and \n# (x_test, y_test) -> validation data\nmodel_history = model.fit(x_train, y_train,\n         epochs=10,\n         batch_size=128,\n         validation_data=(x_test, y_test))\nmodel.save_weights('model_weights.h5')   # storing the model weights","3d0bc515":"# plotting the model accuracy during the training\nplt.plot(model_history.history['acc'], 'green')\nplt.plot(model_history.history['val_acc'], 'red')\nplt.show()","904e3133":"# plotting the Model loss during training\nplt.plot(model_history.history['loss'], 'green')\nplt.plot(model_history.history['val_loss'], 'red')\nplt.show()","824d69e8":"model.load_weights('model_weights.h5')   # Loading the stored model weights\nresults = model.evaluate(x_test, y_test) # Evaluation the Model\nresults","779a2f9c":"predictions = model.predict(x_test) # make predictions on test features\npredictions","331330b7":"# Getting confusion matrix for Actual vs Predicted Labels\nmatrix = sklearn.metrics.confusion_matrix(y_test.argmax(axis=1), predictions.argmax(axis=1))\nmatrix","d39f2053":"print(\"Loss : %s \"%results[0])      # Validation Loss\nprint(\"Accuracy : %s \"%results[1])  # Validation Accuracy\nprint(\"Precision : %s \"%results[2]) # Validation Precision\nprint(\"Recall : %s \"%results[3])    # Validation Recall","9ea32ec0":"# sample -> function to return index of the character to be generated next\ndef sample(preds, temperature=0.2):\n    \"\"\"\n        Inputs: preds -> predictions given by model, temperature -> randomization factor\n        Output: (int) -> most probable index for predicted character\n    \"\"\"\n    preds = np.asarray(preds).astype('float64')  # converting predictions into numpy array\n    preds = np.log(preds)\/temperature            # calculate preds <-- [log(p) \/ temperature for p in preds]\n    exp_preds = np.exp(preds)                    # calculate exp_preds <-- [e^(pred) for pred in preds]\n    preds = exp_preds\/np.sum(exp_preds)          # calculate preds <-- [exp_p\/ sum(exp_preds) for exp_p in exp_preds]\n    probas = np.random.multinomial(1, preds, 1)  # calculate random probabilities for preds\n    return np.argmax(probas)                     # return index of maximum probability prediction character","32925900":"label_data.keys() # looking at the keys of the label_data dictionary","4d02ccd8":"nOfLabelRecords = {} # to store the total no. of records available for each label \nfor label in label_names:  # iterating over all labels\n    nOfLabelRecords[label] = df[df['Label']==label].shape[0]  #count and store no. of records\nnOfLabelRecords # look the calculated statistics","562668ac":"total_records = 8240                  # Total records required for each class label\nrequired_nOfLabelRecords = {}         # store no. of required no. of records for label\nfor k, v in nOfLabelRecords.items():                # iterating over the available no of label records\n    required_nOfLabelRecords[k] = total_records - v # calculate how many records are needed for label\nrequired_nOfLabelRecords","ed7923fa":"generated_label_texts={'Tweet':[], 'Label': [], 'Label_encoded': []}  # define dict for storing required dataset record information\nfor label in label_names:    #iterate over all class labels\n    print(\"--\"*50)\n    print(\"Starting with label : \"+label)  # showing initial message for starting process of each class label\n    print(\"--\"*50)\n    text = label_data[label]     # getting text related to that label\n    maxlen=60                    # Setting maximum length of sequence and step\n    step=3\n    sentences = []               # to store x features as sentences\n    next_characters = []         # to store y classes as next_characters\n    for i in range(0, len(text)-maxlen, step):  # iterating over label-text to develop training dataset for text generation\n        sentences.append(text[i:i+maxlen])      # spliting sentence and \n        next_characters.append(text[i+maxlen])  # next character\n    print(\"Text length : %s \"%len(text), end='\\t')                   # to show total length of text\n    print(\"Number of sequences : %s \"%len(sentences), end='\\t')      # to show total no. of sentences\n    chars = sorted(list(set(text)))                                  # getting unique characters used in text\n    print(\"Unique Characters: %s \"%len(chars))                       # to show the no. of unique characters\n    char_indices = dict((char, chars.index(char)) for char in chars) # create a mapping between chars and their indices\n    print(char_indices)\n    x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)# create x-features tensor initially zeros\n    y = np.zeros((len(sentences), len(chars)), dtype=np.bool)        # create y-classes tensor intially zeros\n    for i, sentence in enumerate(sentences):   # iterating over sentences\n        for j, char in enumerate(sentence):    # iterating over characters in each sentence\n            x[i, j, char_indices[char]] = 1    # setting character's position '1'\n        y[i, char_indices[next_characters[i]]] = 1 # and setting corresponding class's character position '1'\n\n    # Developing model for each label\n    label_model = Sequential()    # create Sequential Model\n    label_model.add(LSTM(32, input_shape=(maxlen, len(chars)))) # add an LSTM layer with 32 units and input shape of (200, no. of unique characters in text)\n    label_model.add(Dense(len(chars), activation='softmax'))    # add a fully-connected dense layer with no. of unique character in text as output dimension with softmax activation\n\n    optimizer = keras.optimizers.RMSprop(lr=0.01)   # define RMSprop (Root Mean Square) Optimizer\n    label_model.compile(loss='categorical_crossentropy', optimizer=optimizer) # comiple model with optimizer and categorical_crossentropy loss\n    label_model.fit(x, y,          # fit model on x, y training data with \n              batch_size=128,      # 128 batch size and 1 epoch\n              epochs=1)\n    count_labels_generated=0       # to track no. of generated label tweets \n    for n in range(200):      # restricted due to limited computation power \n#    for n in range(required_nOfLabelRecords[label]):  # to generate the required no of tweets for each category\n        count_labels_generated+=1                     # tracking generated tweet no.\n        start_index = random.randint(0, len(text) - maxlen - 1)  # randomly generate starting text index\n        generated_text = text[start_index: start_index + maxlen] # select starting text\n        start_generate_text = generated_text \n        temperature = 0.2                    # set randomization parameter \n        max_sentence_length = int(max([len(tweet) for tweet in list(df[df['Label'] == label]['Tweet'].values)]))\n        for i in range(max_sentence_length):               # generate character as much as there are in category's text\n            sampled = np.zeros((1, maxlen, len(chars)))    # prepare sample to be feed to model (setting initially zeros)\n            for t, char in enumerate(start_generate_text): # iterating over chars in starting text\n                sampled[0, t, char_indices[char]] = 1      # set occurring chars position 1\n            preds = label_model.predict(sampled, verbose=False)[0]  # make prediction\n            next_index = sample(preds, temperature)     # get next-character's index\n            next_char = chars[next_index]               # get next-character itself\n            generated_text += next_char                 # append it into the generated text\n            start_generate_text += next_char            \n            start_generate_text = start_generate_text[1:] # set start generate text with single character sliced\n        generated_label_texts['Tweet'].append(start_generate_text)          # append generated tweet in 'Tweet' column\n        generated_label_texts['Label'].append(label)                        # append its label in 'Label' column\n        generated_label_texts['Label_encoded'].append(label_mapping[label]) # append its encoded value in 'Label_encoded' column\n        if count_labels_generated % 100 == 0:\n            print(\"Generated \"+str(count_labels_generated)+\" sentence of \"+label)\n    \n    \n    print(\"--\"*50)       # to indicate that one category tweets has been generated with indicating number\n    print(\"Done with Label : \"+ label+\" having \"+str(count_labels_generated)+\" sentences generated.\")\n    print(\"--\"*50)\n    ","5f767801":"generated_df = pd.DataFrame(generated_label_texts) # create dataframe from generated dictionary\ngenerated_df.head() # looking at first five rows of the data","6553d891":"df = df.append(generated_df, ignore_index=True) # add generated records into dataframe\nprint(df.shape)","399a8a73":"df[['Tweet', 'Label']].groupby('Label').count()","39357d64":"texts = df['Tweet'].values           # storing into texts a list of all tweets from the dataset\nlabels = df['Label_encoded'].values  # corresponding labels\nlabels_values = df['Label'].values   # and their encoded values","66c95cfa":"max_len = 200               # Maximum len of input\nmax_words = 20000           # Maximum unique words \nembedding_dim = 100         # Embedding Dimension to be used (100 in case of Glove-6b-100d)\ntraining_percentage = 0.9   # 90% data for trainging\ntraining_samples = int(len(texts) * training_percentage) # no. of training samples to be selected\nprint(training_samples)","9dbce2b9":"tokenizer = Tokenizer(num_words=max_words) # tokenizing on 20,000 unique words in our case\ntokenizer.fit_on_texts(texts)              # training tokenizer on text from dataset\nword_index = tokenizer.word_index          # storing (kye: value) --> (word: token) \nprint(\"total %s unique tokens \"%len(word_index))  # total unique tokens","757bdbcb":"sequences = tokenizer.texts_to_sequences(texts) # generate sequences of tokens from dataset text\ndata = pad_sequences(sequences, maxlen=max_len) # padding sequences upto the maximum considered length\n\nlabels = np.asarray(labels)            # converting labels into numpy array\nlabels = to_categorical(labels)        # one-hot-encoding of labels because we have multi-class labels\n\nprint(\"Shape of data tensor \", data.shape)    # looking at shape of data and labels tensors\nprint(\"Shape of label tensor \", labels.shape)\n\nindices = np.arange(data.shape[0]) # generating random list of indices for data selection\nnp.random.shuffle(indices)         # shuffling to randomize further for better selection of training data\ndata = data[indices]               # this will shuffle data using randomized indices\nlabels = labels[indices]         # shuffling their corresponding labels without loosing their connection\n\nx_train = data[:training_samples] # spliting data into training and testing x_train -> training features\ny_train = labels[:training_samples, :] # y_train --> training labels\nx_test = data[training_samples:]       # x_test --> validation features\ny_test = labels[training_samples:, :]  # y_test --> validation labels","6fc3fa00":"# looking at shapes of [x_train, y_train, x_test, y_test]\nprint(\"Training Features : \", x_train.shape)\nprint(\"Training Labels : \", y_train.shape)\nprint(\"Testing Features : \", x_test.shape)\nprint(\"Testing Labels : \", y_test.shape)","07f15e3e":"model = Sequential()    # defining the Sequential mode\n#                   (max_words = 20000, embedding_dim = 100, input_length = 200)\nmodel.add(Embedding(max_words, embedding_dim, input_length=max_len)) # add an embedding layer\nmodel.add(LSTM(32))   # add an LSTM Layer with 32 units (dimentionality of the output space)\nmodel.add(Dense(6, activation='softmax')) # add Fully-connected Dense layer with 6 output layers\n                                          # because we have 6 labels and softmax activation function\nmodel.summary()","b049fde6":"model.layers[0].set_weights([embedding_matrix]) # set glove-embedding as weights of initial layer \nmodel.layers[0].trainable = False               # make initial layer as non-trainable","52759a0c":"# Compile Model on RMSprop optimizer, categorical_crossentropy loss, and accuracy, precision,\n# and recall metrics\nmodel.compile(optimizer='rmsprop',\n             loss = 'categorical_crossentropy',\n             metrics = ['acc', metrics.Precision(), metrics.Recall()])","91e27db7":"# Training the model on 10 epochs, 128 batch size, (x_train, y_train) -> training data and \n# (x_test, y_test) -> validation data\nmodel_history = model.fit(x_train, y_train,\n         epochs=10,\n         batch_size=128,\n         validation_data=(x_test, y_test))\nmodel.save_weights('model_weights.h5')   # storing the model weights","74bc88d4":"# plotting the model accuracy during the training\nplt.plot(model_history.history['acc'], 'green')\nplt.plot(model_history.history['val_acc'], 'red')\nplt.show()","cb09e684":"# plotting the Model loss during training\nplt.plot(model_history.history['loss'], 'green')\nplt.plot(model_history.history['val_loss'], 'red')\nplt.show()","d1719e13":"model.load_weights('model_weights.h5')   # Loading the stored model weights\nresults = model.evaluate(x_test, y_test) # Evaluation the Model\nresults","3fe09c90":"predictions = model.predict(x_test) # make predictions on test features\npredictions","a51815be":"# Getting confusion matrix for Actual vs Predicted Labels\nmatrix = sklearn.metrics.confusion_matrix(y_test.argmax(axis=1), predictions.argmax(axis=1))\nmatrix","9bdefa94":"print(\"Loss : %s \"%results[0])      # Validation Loss\nprint(\"Accuracy : %s \"%results[1])  # Validation Accuracy\nprint(\"Precision : %s \"%results[2]) # Validation Precision\nprint(\"Recall : %s \"%results[3])    # Validation Recall","bac446b2":"## Making Predictions","fd747494":"# Define LSTM Single Layer Model","1f7270e8":"## Plot Accuracy Graph (Training Accuracy vs Validation Accuracy)","5750ee08":"# Splitting Data into Training and Validation Sets","31207dca":"## Looking at Training and Validation Data's Shapes","0d4f3f77":"## Show Loss, Accuracy, Precision and Recall","98255e83":"## Hyperparameters","60592d9b":"# Importing Necessary Packages\/Modules","5dd14889":"# Evaluating the model","8cd76086":"# Visualizing Training Process","85c7b52f":"# Now, Again training the same model on Balanced Dataset","a50e776f":"## Tokenization","3fd65c2d":"# Separating Data for Each Category\n## Categories include ['surprise', 'sadness', 'joy', 'disgust', 'fear', 'anger']","c914c94c":"## Mapping Labels and their encoded values","0472b28c":"# Hyperparameters","920bd368":"# Visualize Training Process of First Model","68213f68":"## Plot Accuracy Graph (Training Accuracy vs Validatoin Accuracy)","32af30ed":"## Creating dataframe from the generated tweets' data","508c5ccf":"# Model 01: LSTM single layer with\n* Glove-6b-100d word embedding\n","7dfaeb82":"## Plot Loss Graph (Training Loss vs Validation Loss)","27283d99":"## Splitting training and validation sets","4d72f4b4":"# Read Glove-6B-100d Word Embedding\n## Getting word vectors from the file","7162c640":"## Available number of records for each category ","a74201d4":"# Reading and Understanding the Data","43409ccd":"## Show Confusion Matrix","eb3fe133":"## Looking at Training and Validation Data's Shapes","a057dd8a":"* We have Dictionary of labels and their corresponding text named \"label_data\"","7ce7877b":"## Creating Embedding Matrix from Glove-6B-100d Word Embedding Vectors","517065c5":"# Tokenization","43b24f92":"## Required number of records for each category","1c63c09a":"## Randomization Strategy","70f7edf4":"# Balancing the unbalanced Dataset","bb6179ef":"## Show Loss, Accuracy, Precision and Recall","7cbb3da4":"## Appending newly generated dataframe with original dataframe","deb94057":"## Confusion Matrix","38cc22f7":"## Plot Loss Graph (Training Loss vs Validation Loss)","1d375c97":"# Training the Model","1ebc145b":"## Make Predictions","2a2691e2":"# Train Separate LSTM Model for each class label to Generate Text for each label","0bd4cce3":"# Train First Model","afb5d267":"# Evaluate First Model","61e68bb4":"<div style=\"text-align:center\">\n    <p style=\"text-align:center; color:red;\">\n        Submitted By: <b style=\"color:blue\">Dayanand<\/b>\n    <\/p>\n    <p style=\"text-align: center; color:red;\">\n        Submitted To:<b style=\"color:blue\"> Dr. Sher Muhammad Daudpota<\/b>\n    <\/p>\n    <p style=\"text-align: center; color:red;\">\n        Topic: <b style=\"color:blue\">Text Generation<\/b>\n    <\/p>\n    <p style=\"text-align: center; color:red;\">\n        Class : <b style=\"color:blue\">Data Science<\/b>\n    <\/p>\n    <p style=\"text-align: center; color:red;\">\n        Department : <b style=\"color:blue\"> Computer Science <\/b>\n    <\/p>\n    <p style=\"text-align: center; color:red;\">\n        Institution: <b style=\"color:blue\">Sukkur IBA University<\/b>\n    <\/p>\n<\/div>","99522315":"## Getting text from balanced dataset","626e199f":"<h1 style=\"text-align:center\">Thank You<\/h1>\n<h1 style=\"text-align:center\">The End<\/h1>\n","f2f872cc":"# Text Generation Process For Each Category\n1. Getting text for each category\n2. Preprocessing and developping X,Y features\n3. Defining, developing, and training the model\n4. Generating the required number of tweets for each category\n5. Store generated tweets","3060ab6d":"# Encoding Labels"}}