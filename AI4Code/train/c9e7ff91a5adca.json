{"cell_type":{"0ec1a3a1":"code","2b748b2a":"code","d8b88752":"code","b8cb6366":"code","a560287e":"code","155f49ef":"code","70415451":"code","318e30f0":"code","2669512d":"code","1e21b236":"code","f4518998":"code","807c6f8c":"code","0f1ff119":"code","f16bf337":"code","18ca3f8e":"code","dbe08bea":"markdown","7cd8448f":"markdown","c33288a4":"markdown","98453e39":"markdown","0dd175d8":"markdown","befeb459":"markdown","220d42b5":"markdown"},"source":{"0ec1a3a1":"!pip uninstall -y dataclasses","2b748b2a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d8b88752":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\n","b8cb6366":"# Select which features to include\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\", \"Age\"]\nlabel = [\"Survived\"]\nX_train = pd.get_dummies(train_data[features + label])\nX_test = pd.get_dummies(test_data[features])\nX_train.head()","a560287e":"# Find na value in train_data\nX_train.isna().sum()","155f49ef":"# Find na value in test_data\nX_test.isna().sum()","70415451":"# Little tweak: fill na value as average value of column\n# We can fill better value way by considering correlation between other feature\nX_train = X_train.fillna(X_train.mean())\nX_test = X_test.fillna(X_train.mean())","318e30f0":"# Data\ny = X_train[label].values.ravel()\nX_train = X_train.drop(label, axis=1)","2669512d":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nfrom sklearn.svm import SVC\nfrom typing import Any, Callable, Dict\nfrom sklearn.model_selection import cross_val_score\nfrom ray import tune\n\ndt_config = {\n    \"class\": DecisionTreeClassifier, \n    \"criterion\": tune.choice(['gini', 'entropy']),\n    \"max_depth\": tune.randint(2, 8),\n    \"min_samples_split\": tune.randint(2, 10),\n    'min_samples_leaf': tune.randint(1, 10),\n    \"random_state\": 1\n}\nrf_config = {\n    \"class\": RandomForestClassifier,\n    \"max_depth\": tune.randint(2, 8), \n    \"n_estimators\": tune.qrandint(lower=10, upper=200, q=10), \n    'random_state': 1\n}\nxgb_config = {\n    \"class\": xgb.XGBClassifier,\n     \"max_depth\": tune.randint(1, 9),\n     \"min_child_weight\": tune.choice([1, 2, 3]),\n     \"subsample\": tune.uniform(0.5, 1.0),\n     \"eta\": tune.loguniform(1e-4, 1e-1),\n    'random_state': 1\n}\nsvm_config = {\n    \"class\": SVC,\n    \"kernel\": tune.choice(['linear', 'rbf', 'sigmoid']), # poly: too slow\n    \"gamma\": tune.choice(['scale', 'auto']),\n    \"random_state\": 1\n}","1e21b236":"from copy import deepcopy\nfrom hyperopt import hp\nfrom ray.tune.suggest.optuna import OptunaSearch\n\n# List of methods: Dict[str, [Callable, Dict[str, Any]]]\nmethods = {\"rf\": rf_config, \"xgb\": xgb_config, \"svm\": svm_config, \"dt\": dt_config}\n\n# utils\ndef export_csv(predictions, name:str):\n    output = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\n    filename = f'{name}_submission.csv'\n    output.to_csv(filename, index=False)\n    print(f\"Your submission({name}) was successfully saved! {filename}\")\n    \n\n# ray\ndef run_tune(method: str, num_samples: int) -> tune.ExperimentAnalysis:\n    optuna_search = OptunaSearch(metric=\"mean_accuracy\", mode=\"max\")\n    return tune.run(\n        trial,\n        config=methods[method],\n        num_samples=num_samples,\n        search_alg=optuna_search,\n        verbose=-1\n    )\n    \ndef trial(config: Dict[str, Any]):\n    _config = deepcopy(config)\n    model_class = config[\"class\"]\n    _config.pop(\"class\", None)\n    model = model_class(\n        **_config\n    )\n    scores = cross_val_score(model, X_train, y, cv=5, scoring=\"accuracy\")\n\n    tune.report(\n        mean_accuracy=np.mean(scores), \n        done=True\n    )\n\n# Test running\ntree_config ={\"max_depth\":5}\nmodel = methods[\"dt\"][\"class\"](**tree_config)\nscores = cross_val_score(model, X_train, y, cv=5, scoring=\"accuracy\")\nmodel = methods[\"rf\"][\"class\"](**tree_config)\nscores = cross_val_score(model, X_train, y, cv=5, scoring=\"accuracy\")\nmodel = methods[\"xgb\"][\"class\"](**tree_config)\nscores = cross_val_score(model, X_train, y, cv=5, scoring=\"accuracy\")\nmodel = methods[\"svm\"][\"class\"](**{\"kernel\": \"linear\"})\nscores = cross_val_score(model, X_train, y, cv=5, scoring=\"accuracy\")","f4518998":"# Wrapper\ndef run_experiment(method: str, num_samples: int=50) -> Dict[str, Any]:\n    result = run_tune(method, num_samples).get_best_trial(metric=\"mean_accuracy\", mode=\"max\")\n    _config = deepcopy(result.config)\n    \n    # Train model with params\n    model_class = _config[\"class\"]\n    _config.pop(\"class\")\n    model = model_class(**_config)\n    model.fit(X_train, y)\n    predictions = model.predict(X_test)\n    export_csv(predictions, method)\n    return result","807c6f8c":"dt = run_experiment(method=\"dt\", num_samples=100)\nprint(f\"result {dt.last_result}\")\nprint(f\"{dt.config}\")","0f1ff119":"rf = run_experiment(method=\"rf\", num_samples=100)\nprint(f\"result {rf.last_result}\")\nprint(f\"{rf.config}\")","f16bf337":"xgb = run_experiment(\"xgb\", num_samples=100)\nprint(f\"result {xgb.last_result}\")\nprint(f\"{xgb.config}\")","18ca3f8e":"svm = run_experiment(method=\"svm\", num_samples=8)\nprint(f\"result {svm.last_result}\")\nprint(f\"{svm.config}\")","dbe08bea":"# Evaluate","7cd8448f":"## Data preprocessing\nData preprocessing is the most important phase. But here, we skip the process handling only na value cases","c33288a4":"## Find model using ray tune\nHere, we try to find good params for each ml model. \nOf course, we can include ml model classes as a categorical choices into ray config and make ray to find suitable ml model and params as well.","98453e39":"### 1) DecisionTree","0dd175d8":"### 4) SVM(slow)","befeb459":"### 3) XGBoost","220d42b5":"### 2) RandomForest"}}