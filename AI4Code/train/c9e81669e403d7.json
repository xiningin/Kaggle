{"cell_type":{"cf7b2a02":"code","e4a3dcef":"code","46450b37":"code","ba9bf64c":"code","140da41c":"code","24b08ac1":"code","3dbc9606":"code","71e631cc":"code","9f10dde5":"code","d4a1381c":"code","92951bb8":"code","de01d901":"code","5eae9801":"code","4b1a21cb":"code","3a6d7acf":"code","7acbe072":"code","452b06f0":"code","90ecccd4":"code","b6e7ade3":"code","ee791dd3":"code","261e7763":"code","f8491fd0":"code","2d49346e":"code","08f302ce":"code","a3afb6a3":"code","f799c7b2":"code","93e412ea":"code","aa785643":"code","695d8ebf":"code","69a4a6da":"code","52648e0b":"code","56c450a2":"code","1c9469e2":"code","f3744509":"code","8ab8373e":"code","d27824aa":"code","db8b72aa":"code","f343aeda":"code","922e3680":"code","1e99d07c":"code","ee6e551c":"code","1b602cc3":"code","bca8608b":"code","4d2cd5f4":"code","2bb92b0d":"code","337dc64f":"code","f57274ce":"code","877c2fda":"code","1097f12f":"code","ba862a8d":"code","eb557570":"code","b1e1f356":"code","54ecdfd2":"code","efc26308":"code","2d78ce58":"code","81a045ba":"code","97e4779b":"code","019fec5b":"code","18d844ff":"code","92361574":"code","f110ee9f":"code","7d4b58e6":"code","23dbc02b":"markdown","afdc7edd":"markdown","813da931":"markdown","30272133":"markdown","dc3bb1e6":"markdown","ab0b33f4":"markdown","3a5d3736":"markdown","0ef5e4e9":"markdown","c4bf8019":"markdown"},"source":{"cf7b2a02":"import os\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn.model_selection as ms\nimport sklearn.metrics as metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tqdm.notebook import tqdm\nfrom sklearn.linear_model import LinearRegression","e4a3dcef":"model_save_folder = \"models\"\ncsv_folder = \"csv\"","46450b37":"os.makedirs(model_save_folder, exist_ok=True)\nos.makedirs(csv_folder, exist_ok=True)","ba9bf64c":"train_data = pd.read_csv(\"..\/input\/tabular-playground-series-jul-2021-more-features\/csv\/train_data.csv\")\ntest_data = pd.read_csv(\"..\/input\/tabular-playground-series-jul-2021-more-features\/csv\/test_data.csv\")\ntest_leaked_sub = pd.read_csv(\"..\/input\/tps-pycaret-data-leaked\/sub.csv\")","140da41c":"targets = ['target_carbon_monoxide', 'target_benzene', 'target_nitrogen_oxides']","24b08ac1":"features = ['deg_C', 'sensor_1', 'sensor_3', 'sensor_5', 'year', 'month', 'day', 'hour', 'dayofweek', 'week_of_year', 'quarter', 'weekend', 'phase', 'working_hours', 'season', 'r_rh_temp', 'deg_C_diff_0', 'deg_C_diff_2', 'deg_C_diff_6', 'relative_humidity_diff_0', 'relative_humidity_diff_6', 'absolute_humidity_diff_0', 'absolute_humidity_diff_2', 'absolute_humidity_diff_6', 'sensor_3_diff_2', 'sensor_3_diff_6', 'sensor_4_diff_7', 'sensor_4_diff_6', 'sensor_5_diff_2', 'r_rh_temp_diff_0', 'r_rh_temp_diff_2', 'r_rh_temp_diff_6', 'sensor_1_mvag_6', 'sensor_2_mvag_6', 'sensor_5_mvag_6', 'r_rh_temp_mvag_6', 'deg_C_pct_change', 'deg_C_pct_change_sign', 'relative_humidity_pct_change', 'relative_humidity_pct_change_sign', 'absolute_humidity_pct_change', 'absolute_humidity_pct_change_sign', 'sensor_1_pct_change', 'sensor_1_pct_change_sign', 'sensor_2_pct_change_sign', 'sensor_3_pct_change_sign', 'sensor_4_pct_change', 'sensor_4_pct_change_sign', 'sensor_5_pct_change', 'sensor_5_pct_change_sign', 'r_rh_temp_pct_change_sign', 'relative_humidity_quant_6', 'absolute_humidity_quant_6', 'sensor_3_quant_6', 'sensor_4_quant_6']","3dbc9606":"test_leaked_sub.head()","71e631cc":"test_data[targets] = test_leaked_sub[targets]","9f10dde5":"test_data.head()","d4a1381c":"scaler_features = MinMaxScaler()\nscaler_target = MinMaxScaler()","92951bb8":"scaler_features.fit(train_data[features])\nscaler_target.fit(train_data[targets])","de01d901":"train_data[features] = scaler_features.transform(train_data[features])\ntrain_data[targets] = scaler_target.transform(train_data[targets])","5eae9801":"train_data.head()","4b1a21cb":"test_data.head()","3a6d7acf":"np.where(np.isinf(train_data[features].values))","7acbe072":"np.where(np.isinf(test_data[features].values))","452b06f0":"test_data = test_data.replace([np.inf, -np.inf], 1e8)","90ecccd4":"np.where(np.isinf(test_data[features].values))","b6e7ade3":"test_data[features] = scaler_features.transform(test_data[features])\ntest_data[targets] = scaler_target.transform(test_data[targets])","ee791dd3":"test_data.head()","261e7763":"def msle(y_true, y_pred):\n    y_true = np.abs(y_true)\n    y_pred = np.abs(y_pred)\n    return np.sqrt(metrics.mean_squared_log_error(y_true, y_pred))","f8491fd0":"class CrossValidation:\n    def __init__(self, df, shuffle,random_state=None):\n        self.df = df\n        self.random_state = random_state\n        self.shuffle = shuffle\n        if shuffle is True:\n            self.df = df.sample(frac=1,\n                random_state=self.random_state).reset_index(drop=True)\n        if not shuffle:\n            self.random_state = None\n\n    def hold_out_split(self,percent,stratify=None):\n        if stratify is not None:\n            y = self.df[stratify]\n            train,val = ms.train_test_split(self.df, test_size=percent\/100,\n                stratify=y, random_state=self.random_state)\n            return train,val\n        size = len(self.df) - int(len(self.df)*(percent\/100))\n        train = self.df.iloc[:size,:]\n        val = self.df.iloc[size:,:]\n        return train,val\n\n    def kfold_split(self, splits, stratify=None):\n        if stratify is not None:\n            kf = ms.StratifiedKFold(n_splits=splits, \n                random_state=self.random_state)\n            y = self.df[stratify]\n            for train, val in kf.split(X=self.df,y=y):\n                t = self.df.iloc[train,:]\n                v = self.df.iloc[val, :]\n                yield t,v\n        else:\n            kf = ms.KFold(n_splits=splits, shuffle=self.shuffle,\n                random_state=self.random_state)\n            for train, val in kf.split(X=self.df):\n                t = self.df.iloc[train,:]\n                v = self.df.iloc[val, :]\n                yield t,v","2d49346e":"class TFSimpleDataset:\n    def __init__(self,batch_size, repeat, shuffle=False,\n        drop_remainder_in_batch=False, \n        num_parallel_calls=tf.data.experimental.AUTOTUNE,\n        buffer_size=tf.data.experimental.AUTOTUNE):\n        self.batch_size = batch_size\n        self.drop_remainder = drop_remainder_in_batch\n        self.num_parallel_calls = num_parallel_calls\n        self.buffer_size = buffer_size\n        self.repeat = repeat\n        self.shuffle = shuffle\n\n    def create_dataset(self, X, Y=None):\n        datasetX = tf.data.Dataset.from_tensor_slices(X)\n        if Y is not None :\n            datasetY = tf.data.Dataset.from_tensor_slices(Y)\n            dataset = tf.data.Dataset.zip((datasetX,datasetY))\n        else:\n            dataset = datasetX\n        if self.shuffle:\n            dataset = dataset.shuffle(int(self.shuffle))\n        dataset = dataset.batch(self.batch_size, \n            drop_remainder=self.drop_remainder)\n        if self.repeat:\n            dataset = dataset.repeat()\n        dataset = dataset.prefetch(buffer_size=self.buffer_size)\n        return dataset","08f302ce":"def get_model(num_features):\n    model = tf.keras.Sequential([\n            tf.keras.layers.Dense(num_features),\n            tf.keras.layers.Dense(64, activation=\"relu\"),\n            tf.keras.layers.Dense(16, activation=\"relu\"),\n            tf.keras.layers.Dropout(0.2),\n            tf.keras.layers.Dense(8, activation=\"relu\"),\n            tf.keras.layers.Dense(1)\n        ])\n    model.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer=\"adam\", \n              metrics=[\"mse\", \"msle\"])\n    return model","a3afb6a3":"def plot_history(history):\n    fig, ax = plt.subplots(2,1)\n    if history.history.get(\"loss\"):\n        ax[0].plot(history.history['loss'], color='b', \n            label=\"Training loss\")\n    if history.history.get(\"val_loss\"):\n        ax[0].plot(history.history['val_loss'], color='r', \n            label=\"Validation loss\",axes =ax[0])\n    legend = ax[0].legend(loc='best', shadow=True)\n    if history.history.get(\"mse\"):\n        ax[1].plot(history.history['mse'], color='b', \n            label=\"Training mse\")\n    if history.history.get(\"val_mse\"):\n        ax[1].plot(history.history['val_mse'], color='r', \n            label=\"Validation mse\")\n    legend = ax[1].legend(loc='best', shadow=True)\n    plt.show()","f799c7b2":"def train_loop(train_dataset, val_dataset, epochs, params, model_name, callbacks=None, verbose=True):\n    reg = get_model(**params)\n    \n    if callbacks is None:\n        callbacks = []\n        \n    callbacks.append(tf.keras.callbacks.ModelCheckpoint(\n        os.path.join(\"models\", f\"{model_name}.h5\"), monitor=\"val_msle\", verbose=1, \n        save_best_only=True, save_freq='epoch', mode=\"min\"\n    ))\n    history = reg.fit(train_dataset, epochs=epochs, \n                      validation_data=val_dataset, \n                      callbacks=callbacks, verbose=int(verbose))\n    \n    plot_history(history)\n    \n    msle_s = []\n    r2_s = []\n    for trainX, trainY in train_dataset:\n        predY_train = reg.predict(trainX)\n        msle_s.append(msle(trainY, predY_train))\n        r2_s.append(metrics.r2_score(trainY, predY_train))\n    \n    train_msle = np.mean(msle_s)\n    train_r2 = np.mean(r2_s)\n    \n    if verbose:\n        print(\"Training msle: \", train_msle)\n        print(\"Training r2: \", train_r2)\n    \n    msle_s = []\n    r2_s = []\n    for valX, valY in val_dataset:\n        predY_val = reg.predict(valX)\n        msle_s.append(msle(valY, predY_val))\n        r2_s.append(metrics.r2_score(valY, predY_val))\n    \n    val_msle = np.mean(msle_s)\n    val_r2 = np.mean(r2_s)\n    if verbose:\n        print(\"Validation msle: \", val_msle)\n        print(\"Validation r2: \", val_r2)\n        \n    return {\"model\": reg,\n            \"train_scores\":{\"r2\": train_r2, \"msle\": train_msle},\n            \"val_scores\":{\"r2\": val_r2, \"msle\": val_msle}\n           }","93e412ea":"def train_folds(cv, tf_dataset, feature_cols, target_col, not_target, num_folds, params, \n                epochs=10, callbacks=None, verbose=False):\n    fold_train_rmsle = []\n    fold_train_r2 = []\n    fold_val_rmsle = [] \n    fold_val_r2 = []\n    fold_models = []\n    for fold, (train_, val_) in enumerate(cv.kfold_split(splits=num_folds)):\n        print(\"Training Fold \",fold)\n        train_dataset = tf_dataset.create_dataset(X=train_[feature_cols + not_target].values,\n                                                    Y=train_[target_col].values)\n        val_dataset = tf_dataset.create_dataset(X=val_[feature_cols + not_target].values,\n                                                  Y=val_[target_col].values)\n        \n        model_name = f\"model_{target_col}_{fold+1}\"\n        result = train_loop(train_dataset,\n                            val_dataset,\n                            model_name=model_name,\n                            params=params,\n                            epochs=epochs,\n                            callbacks=callbacks,\n                            verbose=verbose\n                           )\n        fold_train_rmsle.append(result[\"train_scores\"][\"msle\"])\n        \n        fold_train_r2.append(result[\"train_scores\"][\"r2\"])\n\n        fold_val_rmsle.append(result[\"val_scores\"][\"msle\"])\n        fold_val_r2.append(result[\"val_scores\"][\"r2\"])\n\n        fold_models.append(result[\"model\"])\n        \n    return {\"models\":fold_models,\n            \"train_scores\":{\"r2\": np.mean(fold_train_r2), \"msle\": np.mean(fold_train_rmsle)},\n            \"val_scores\":{\"r2\":np.mean(fold_val_r2), \"msle\":np.mean(fold_val_rmsle)}\n           }","aa785643":"batch_size = 1024\nepochs = 100\nseed = 11\nfolds = 5","695d8ebf":"model_params = {\"num_features\": len(features)}","69a4a6da":"fold_models = {tar:[] for tar in targets}","52648e0b":"data_creator = TFSimpleDataset(batch_size, repeat=False, shuffle=100)","56c450a2":"cv = CrossValidation(train_data, shuffle=True, random_state=42)","1c9469e2":"callbacks = [\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=1, min_lr=0.001, verbose=1),\n    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n    ]","f3744509":"target = \"target_carbon_monoxide\"\nnot_target = [t for t in targets if t is not target]\nresults = train_folds(cv, data_creator, features, target, not_target, num_folds=folds, params=model_params, \n                      epochs=epochs, callbacks=callbacks)\nfold_models[target] = results[\"models\"]\n\nprint(\"=\"*50)\nprint(\"Training MSLE: \", results[\"train_scores\"][\"msle\"])\nprint(\"Training R2: \", results[\"train_scores\"][\"r2\"])\nprint(\"Validation MSLE: \", results[\"val_scores\"][\"msle\"])\nprint(\"Validation R2: \", results[\"val_scores\"][\"r2\"])","8ab8373e":"target = \"target_benzene\"\nnot_target = [t for t in targets if t is not target]\nresults = train_folds(cv, data_creator, features, target, not_target, num_folds=folds, \n                      params=model_params, \n                      epochs=epochs, callbacks=callbacks)\nfold_models[target] = results[\"models\"]\n\nprint(\"=\"*50)\nprint(\"Training MSLE: \", results[\"train_scores\"][\"msle\"])\nprint(\"Training R2: \", results[\"train_scores\"][\"r2\"])\nprint(\"Validation MSLE: \", results[\"val_scores\"][\"msle\"])\nprint(\"Validation R2: \", results[\"val_scores\"][\"r2\"])","d27824aa":"target = \"target_nitrogen_oxides\"\nnot_target = [t for t in targets if t is not target]\nresults = train_folds(cv, data_creator, features, target, not_target, num_folds=folds, \n                      params=model_params, \n                      epochs=epochs, callbacks=callbacks)\nfold_models[target] = results[\"models\"]\n\nprint(\"=\"*50)\nprint(\"Training MSLE: \", results[\"train_scores\"][\"msle\"])\nprint(\"Training R2: \", results[\"train_scores\"][\"r2\"])\nprint(\"Validation MSLE: \", results[\"val_scores\"][\"msle\"])\nprint(\"Validation R2: \", results[\"val_scores\"][\"r2\"])","db8b72aa":"def get_weights(predictions, targets):\n    lnr = get_model(5)\n    callbacks = [tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3, restore_best_weights=True, \n                                                  verbose=1)]\n    lnr.fit(predictions, targets, epochs=100, batch_size=128, callbacks=callbacks)\n    return lnr, lnr.predict(predictions)","f343aeda":"for key, models in fold_models.items():\n    for fold, model in enumerate(models):\n        model.load_weights(os.path.join(\"models\", f\"model_{key}_{fold+1}.h5\" ))","922e3680":"trainY = train_data[targets]","1e99d07c":"predictions = []","ee6e551c":"preds = []\nnot_target = [t for t in targets if t is not targets[0]]\ntrainX = train_data[features+not_target].values\nfor model in fold_models[targets[0]]:\n    preds.append(model.predict(trainX))\n\npreds = np.array(preds)\nco_mod, preds = get_weights(preds.transpose().squeeze(), trainY[targets[0]].values)\npredictions.append(preds)","1b602cc3":"preds = []\nnot_target = [t for t in targets if t is not targets[1]]\ntrainX = train_data[features+not_target].values\nfor model in fold_models[targets[1]]:\n    preds.append(model.predict(trainX))\n\npreds = np.array(preds)\nben_mod, preds = get_weights(preds.transpose().squeeze(), trainY[targets[1]].values)\npredictions.append(preds)","bca8608b":"preds = []\nnot_target = [t for t in targets if t is not targets[2]]\ntrainX = train_data[features+not_target].values\nfor model in fold_models[targets[2]]:\n    preds.append(model.predict(trainX))\n\npreds = np.array(preds)\nnox_mod, preds = get_weights(preds.transpose().squeeze(), trainY[targets[2]].values)\npredictions.append(preds)","4d2cd5f4":"predictions = np.array(predictions).transpose().squeeze()\nprint(predictions.shape)\nprint(trainY.shape)","2bb92b0d":"predictions = np.abs(predictions)","337dc64f":"print(\"R2 score: \", metrics.r2_score(trainY, predictions))\nprint(\"RMSLE score: \", msle(trainY, predictions))","f57274ce":"predictions.shape","877c2fda":"scaler_target.inverse_transform(predictions)","1097f12f":"new_df = pd.DataFrame()\nnew_df[features] = train_data[features]\nnew_df[targets] = scaler_target.inverse_transform(predictions)\nnew_df.to_csv(os.path.join(csv_folder, \"train_predictions.csv\"), index=False)","ba862a8d":"predictions = []","eb557570":"preds = []\nnot_target = [t for t in targets if t is not targets[0]]\ntestX = test_data[features+not_target].values\nfor model in fold_models[targets[0]]:\n    preds.append(model.predict(testX))\n\npreds = np.array(preds).transpose()\n\npreds = co_mod.predict(preds)\npredictions.append(preds)","b1e1f356":"preds = []\nnot_target = [t for t in targets if t is not targets[1]]\ntestX = test_data[features+not_target].values\nfor model in fold_models[targets[1]]:\n    preds.append(model.predict(testX))\n\npreds = np.array(preds).transpose()\n\npreds = ben_mod.predict(preds)\npredictions.append(preds)","54ecdfd2":"preds = []\nnot_target = [t for t in targets if t is not targets[2]]\ntestX = test_data[features+not_target].values\nfor model in fold_models[targets[2]]:\n    preds.append(model.predict(testX))\n\npreds = np.array(preds).transpose()\n\npreds = nox_mod.predict(preds)\npredictions.append(preds)","efc26308":"predictions = np.array(predictions).transpose()","2d78ce58":"predictions = np.abs(predictions)","81a045ba":"submission_csv = pd.read_csv(\"..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv\")","97e4779b":"submission_csv[targets] = scaler_target.inverse_transform(predictions.squeeze())","019fec5b":"scaler_target.inverse_transform(predictions.squeeze())","18d844ff":"submission_csv.to_csv(\"submission.csv\", index=False)","92361574":"submission_csv.head()","f110ee9f":"new_df = pd.DataFrame()\nnew_df[features] = test_data[features]\nnew_df[targets] = scaler_target.inverse_transform(predictions.squeeze())\nnew_df.to_csv(os.path.join(csv_folder, \"test_predictions.csv\"), index=False)","7d4b58e6":"!ls models","23dbc02b":"### load best weights","afdc7edd":"### removing any *np.inf* values from dataset","813da931":"# Loading Data\n\n- train feature engineered csv\n- test feature engineered csv\n- leak test submission csv from [this notebook](https:\/\/www.kaggle.com\/junhyeok99\/tps-pycaret-data-leaked)","30272133":"# Inference on test dataset","dc3bb1e6":"# cross validation setup and utils","ab0b33f4":"features from feature selection using lasso regression","3a5d3736":"# Training model folds","0ef5e4e9":"# Installing Dependencies","c4bf8019":"# Feature and target scaling"}}