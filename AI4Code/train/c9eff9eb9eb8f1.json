{"cell_type":{"1669aefb":"code","2d5e5bb2":"code","b91942e5":"code","c0320fdd":"code","2d6797d1":"code","4a9f15b8":"code","f5269be1":"code","a4329a90":"code","658e24c8":"code","a13aa90a":"code","4722b54f":"code","4da0a306":"code","1f20697e":"code","3fb064ab":"code","c244e750":"code","2cabedaa":"code","748070e2":"code","aee9b3df":"code","4ac7ed91":"code","0f2b1ab4":"code","8f1e4a92":"code","a411baa3":"code","8a509ff2":"code","8aedabd8":"code","566ce66f":"code","f8693855":"code","27e0c869":"code","da221781":"code","9e6ef94b":"code","592ed4cf":"markdown","06bf9914":"markdown","4416d37c":"markdown","b68cbecd":"markdown","5d5b2fb9":"markdown","7401c312":"markdown","7f0501bc":"markdown","d288a1f7":"markdown","0ebc7e1d":"markdown","d7644ac5":"markdown","f4c79313":"markdown","cb5b3e46":"markdown","9ab69566":"markdown","276af39d":"markdown","fad7392b":"markdown","3cb218ac":"markdown","1d55c1af":"markdown","585c7920":"markdown","523666b3":"markdown","ff9b5a0f":"markdown","37340074":"markdown","16777dd4":"markdown","b8e518d3":"markdown","34d4f7d0":"markdown","f3053110":"markdown","c1da8c61":"markdown"},"source":{"1669aefb":"!pip install fairlearn","2d5e5bb2":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom IPython.display import display, HTML","b91942e5":"df = pd.read_csv(\"..\/input\/loan-predication\/train_u6lujuX_CVtuZ9i (1).csv\")","c0320fdd":"df.head()","2d6797d1":"# Drop irrelevant columns\ndf.drop(columns=['Loan_ID', 'LoanAmount', 'Loan_Amount_Term'], inplace=True)","4a9f15b8":"df.isnull().sum()","f5269be1":"df = df[(~df['Gender'].isnull()) & (~df['Married'].isnull())]\ndf['Dependents'] = df['Dependents'].map({'0': '0',\n                                         '1': '1',\n                                         '2': '2',\n                                         '3+': '3'})\nfill_values = {'Self_Employed': 'NaN', 'Dependents': 'NaN', 'Credit_History': -1}\ndf.fillna(value=fill_values, inplace=True)","a4329a90":"df.isnull().sum()","658e24c8":"# Apply LabelEncoder on each of the categorical columns\ncategorical_cols = ['Gender', 'Married', 'Education', 'Self_Employed', \n                    'Credit_History', 'Property_Area', 'Dependents', 'Loan_Status']\nle = LabelEncoder()\n\ndf[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col))\n\n# Apply OneHotEncoder on each of the categorical columns\ncategorical_cols = ['Self_Employed', 'Credit_History', 'Property_Area', 'Dependents']\n\nencoded_features = []\nohe = OneHotEncoder()\nfor feature in categorical_cols:\n    encoded_feat = OneHotEncoder(drop='first').fit_transform(df[feature].values.reshape(-1, 1)).toarray()\n    n = df[feature].nunique()\n    cols = ['{}_{}'.format(feature, n) for n in range(0, n-1)]\n    encoded_df = pd.DataFrame(encoded_feat, columns=cols)\n    encoded_df.index = df.index\n    encoded_features.append(encoded_df)","a13aa90a":"df = pd.concat([df.drop(columns=categorical_cols), *encoded_features], axis=1)","4722b54f":"df.columns","4da0a306":"# Train test split\n\nA = df['Gender']\n\n\nX_train, X_test, y_train, y_test, A_train, A_test = train_test_split(df.drop(columns=\"Loan_Status\"), \n                                                                     df['Loan_Status'],\n                                                                     A,\n                                                                     stratify=df['Loan_Status'], \n                                                                     test_size=0.2)\n\n# Combine all training data into a single data frame and glance at a few rows\nall_train = pd.concat([X_train, y_train], axis=1)","1f20697e":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.rcParams['figure.figsize'] = (15, 5)\nplt.style.use('seaborn-white')\n\nplt.subplot(1, 2, 1)\n\ny_train.value_counts().plot(kind = 'pie',\n                            autopct = '%.2f%%',\n                            startangle = 90,\n                            labels = ['Loan granted', 'Loan not granted'],\n                            pctdistance = 0.5)\n\nplt.xlabel('Training dataset', fontsize = 14)\n\nplt.subplot(1, 2, 2)\n\ny_test.value_counts().plot(kind = 'pie',\n                           autopct = '%.2f%%',\n                           startangle = 90,\n                           labels = ['Loan granted', 'Loan not granted'],\n                           pctdistance = 0.5)\n\nplt.xlabel('Testing dataset', fontsize = 14)\n\nplt.suptitle('Target Class Balance', fontsize = 16)\nplt.show()","3fb064ab":"gender_grouped = all_train.groupby('Gender')\ncounts_by_gender = gender_grouped[['Loan_Status']].count().rename(\n    columns={'Loan_Status': 'count'})\n\nrates_by_gender = gender_grouped[['Loan_Status']].mean().rename(\n    columns={'Loan_Status': 'pass_loan_rate'})\n\nsummary_by_gender = pd.concat([counts_by_gender, rates_by_gender], axis=1)\ndisplay(summary_by_gender)","c244e750":"from sklearn.linear_model import LogisticRegression\n\nunmitigated_predictor = LogisticRegression(solver='liblinear', fit_intercept=True)\nunmitigated_predictor.fit(X_train, y_train)","2cabedaa":"from fairlearn.metrics import roc_auc_score_group_summary\n\ndef summary_as_df(name, summary):\n    a = pd.Series(summary.by_group)\n    a['overall'] = summary.overall\n    return pd.DataFrame({name: a})\n\nscores_unmitigated = pd.Series(unmitigated_predictor.predict_proba(X_test)[:,1], name=\"score_unmitigated\")\ny_pred = (scores_unmitigated >= np.mean(y_test)) * 1\nauc_unmitigated = summary_as_df(\n    \"auc_unmitigated\", roc_auc_score_group_summary(y_test, scores_unmitigated, sensitive_features=A_test))","748070e2":"from fairlearn.metrics import (\n    group_summary, selection_rate, selection_rate_group_summary,\n    demographic_parity_difference, demographic_parity_ratio,\n    balanced_accuracy_score_group_summary, roc_auc_score_group_summary,\n    equalized_odds_difference, difference_from_summary)\nfrom sklearn.metrics import balanced_accuracy_score, roc_auc_score\n\n\n# Helper functions\ndef get_metrics_df(models_dict, y_true, group):\n    metrics_dict = {\n        \"Overall selection rate\": (\n            lambda x: selection_rate(y_true, x), True),\n        \"Demographic parity difference\": (\n            lambda x: demographic_parity_difference(y_true, x, sensitive_features=group), True),\n        \"Demographic parity ratio\": (\n            lambda x: demographic_parity_ratio(y_true, x, sensitive_features=group), True),\n        \"-----\": (lambda x: \"\", True),\n        \"Overall balanced error rate\": (\n            lambda x: 1-balanced_accuracy_score(y_true, x), True),\n        \"Balanced error rate difference\": (\n            lambda x: difference_from_summary(\n                balanced_accuracy_score_group_summary(y_true, x, sensitive_features=group)), True),\n        \"Equalized odds difference\": (\n            lambda x: equalized_odds_difference(y_true, x, sensitive_features=group), True),\n        \"------\": (lambda x: \"\", True),\n        \"Overall AUC\": (\n            lambda x: roc_auc_score(y_true, x), False),\n        \"AUC difference\": (\n            lambda x: difference_from_summary(\n                roc_auc_score_group_summary(y_true, x, sensitive_features=group)), False),\n    }\n    df_dict = {}\n    for metric_name, (metric_func, use_preds) in metrics_dict.items():\n        df_dict[metric_name] = [metric_func(preds) if use_preds else metric_func(scores) \n                                for model_name, (preds, scores) in models_dict.items()]\n    return pd.DataFrame.from_dict(df_dict, orient=\"index\", columns=models_dict.keys())","aee9b3df":"models_dict = {\"Unmitigated\": (y_pred, scores_unmitigated)}\nget_metrics_df(models_dict, y_test, A_test)","4ac7ed91":"gs = group_summary(roc_auc_score, y_test, y_pred, sensitive_features=A_test)\ngs\n\nplt.figure()\nplt.style.use('seaborn-white')\nplt.title(\"AUC per group before mitigating model biases\", fontsize = 16)\nplt.bar(range(len(gs[\"by_group\"])), list(gs[\"by_group\"].values()), align='center')\nplt.xticks(range(len(gs[\"by_group\"])), ['Female', 'Male'])\nplt.ylim(0, 1)\nplt.show()","0f2b1ab4":"srg = selection_rate_group_summary(y_test, y_pred, sensitive_features=A_test)\n\nplt.figure()\nplt.style.use('seaborn-white')\nplt.title(\"Selection rate per group before mitigating model biases\", fontsize = 16)\nplt.bar(range(len(srg[\"by_group\"])), list(srg[\"by_group\"].values()), align='center')\nplt.xticks(range(len(srg[\"by_group\"])), ['Female', 'Male'])\nplt.ylim(0, 1)\nplt.show()","8f1e4a92":"from fairlearn.widget import FairlearnDashboard\n\nFairlearnDashboard(sensitive_features=A_test, sensitive_feature_names=['Gender'],\n                   y_true=y_test,\n                   y_pred={\"Unmitigated\": y_pred})","a411baa3":"from fairlearn.reductions import GridSearch, DemographicParity\nfrom sklearn.calibration import CalibratedClassifierCV\n\nsweep = GridSearch(LogisticRegression(solver='liblinear', fit_intercept=True),\n                   constraints=DemographicParity(),\n                   grid_size=200,\n                   grid_limit=10)\n\nsweep.fit(X_train, y_train, sensitive_features=A_train)\n\ncalibrated_predictors = []\nfor predictor in sweep._predictors:\n    calibrated = CalibratedClassifierCV(base_estimator=predictor, cv='prefit', method='sigmoid')\n    calibrated.fit(X_train, y_train)\n    calibrated_predictors.append(calibrated)","8a509ff2":"from scipy.stats import cumfreq\n\ndef compare_cdfs(data, A, num_bins=100):\n    cdfs = {}\n    assert len(np.unique(A)) == 2\n    \n    limits = ( min(data), max(data) )\n    s = 0.5 * (limits[1] - limits[0]) \/ (num_bins - 1)\n    limits = ( limits[0]-s, limits[1] + s)\n    \n    for a in np.unique(A):\n        subset = data[A==a]\n        \n        cdfs[a] = cumfreq(subset, numbins=num_bins, defaultreallimits=limits)\n        \n    lower_limits = [v.lowerlimit for _, v in cdfs.items()]\n    bin_sizes = [v.binsize for _,v in cdfs.items()]\n    actual_num_bins = [v.cumcount.size for _,v in cdfs.items()]\n    \n    assert len(np.unique(lower_limits)) == 1\n    assert len(np.unique(bin_sizes)) == 1\n    assert np.all([num_bins==v.cumcount.size for _,v in cdfs.items()])\n    \n    xs = lower_limits[0] + np.linspace(0, bin_sizes[0]*num_bins, num_bins)\n    \n    disparities = np.zeros(num_bins)\n    for i in range(num_bins):\n        cdf_values = np.clip([v.cumcount[i]\/len(data[A==k]) for k,v in cdfs.items()],0,1)\n        disparities[i] = max(cdf_values)-min(cdf_values)  \n    \n    return xs, cdfs, disparities\n\n\ndef plot_and_compare_cdfs(data, A, num_bins=100, loc='best'):\n    xs, cdfs, disparities = compare_cdfs(data, A, num_bins)\n    \n    for k, v in cdfs.items():\n        plt.plot(xs, v.cumcount\/len(data[A==k]), label=k)\n    \n    assert disparities.argmax().size == 1\n    d_idx = disparities.argmax()\n    \n    xs_line = [xs[d_idx],xs[d_idx]]\n    counts = [v.cumcount[d_idx]\/len(data[A==k]) for k, v in cdfs.items()]\n    ys_line = [min(counts), max(counts)]\n    \n    plt.plot(xs_line, ys_line, 'o--')\n    disparity_label = \"max disparity = {0:.3f}\\nat {1:0.3f}\".format(disparities[d_idx], xs[d_idx])\n    plt.text(xs[d_idx], 1, disparity_label, ha=\"right\", va=\"top\")\n    \n    plt.xlabel(data.name)\n    plt.ylabel(\"cumulative frequency\")\n    plt.legend(loc=loc)\n    plt.show()","8aedabd8":"from fairlearn.metrics import roc_auc_score_group_min\n\ndef auc_disparity_sweep_plot(predictors, names, marker='o', scale_size=1, zorder=-1):\n    roc_auc = np.zeros(len(predictors))\n    disparity = np.zeros(len(predictors))\n    \n    for i in range(len(predictors)):\n        preds = predictors[i].predict_proba(X_test)[:,1]\n        roc_auc[i] = roc_auc_score_group_min(y_test, preds, sensitive_features=A_test)\n        _, _, dis = compare_cdfs(preds, A_test)\n        disparity[i] = dis.max()\n        \n    plt.scatter(roc_auc, disparity,\n                s=scale_size * plt.rcParams['lines.markersize'] ** 2, marker=marker, zorder=zorder)\n    for i in range(len(roc_auc)):\n        plt.annotate(names[i], (roc_auc[i], disparity[i]), xytext=(3,2), textcoords=\"offset points\", zorder=zorder+1)\n    plt.xlabel(\"worst-case AUC\")\n    plt.ylabel(\"demographic disparity\")\n    \nauc_disparity_sweep_plot(calibrated_predictors, names=range(len(calibrated_predictors)))\nauc_disparity_sweep_plot([unmitigated_predictor], names=[''], marker='*', zorder=1, scale_size=5)\nplt.show()","566ce66f":"# a convenience function that transforms the result of a group metric call into a data frame\n\nscores_model101 = pd.Series(calibrated_predictors[101].predict_proba(X_test)[:,1], name=\"score_model101\")\n\nauc_model101 = summary_as_df(\n        \"auc_model101\", roc_auc_score_group_summary(y_test, scores_model101, sensitive_features=A_test))\n\ndisplay(HTML('<span id=\"grid_search_comparison\">'),\n        pd.concat([auc_model101, auc_unmitigated], axis=1),\n        HTML('<\/span>'))\nplot_and_compare_cdfs(scores_model101, A_test.reset_index(drop=True))\nplot_and_compare_cdfs(scores_unmitigated, A_test.reset_index(drop=True))","f8693855":"y_pred_mitigated = (scores_model101 >= np.mean(y_test)) * 1\n\ngs = group_summary(roc_auc_score, y_test, y_pred_mitigated, sensitive_features=A_test)\ngs\n\nplt.figure()\nplt.style.use('seaborn-white')\nplt.title(\"AUC per group after mitigating model biases\", fontsize = 16)\nplt.bar(range(len(gs[\"by_group\"])), list(gs[\"by_group\"].values()), align='center')\nplt.xticks(range(len(gs[\"by_group\"])), ['Female', 'Male'])\nplt.ylim(0, 1)\nplt.show()","27e0c869":"srg = selection_rate_group_summary(y_test, y_pred_mitigated, sensitive_features=A_test)\n\nplt.figure()\nplt.style.use('seaborn-white')\nplt.title(\"Selection rate per group after mitigating model biases\", fontsize = 16)\nplt.bar(range(len(srg[\"by_group\"])), list(srg[\"by_group\"].values()), align='center')\nplt.xticks(range(len(srg[\"by_group\"])), ['Female', 'Male'])\nplt.ylim(0, 1)\nplt.show()","da221781":"models_dict = {\"Unmitigated\": (y_pred, scores_unmitigated)}\nget_metrics_df(models_dict, y_test, A_test)","9e6ef94b":"models_dict = {\"Mitigated\": (y_pred_mitigated, scores_model101)}\nget_metrics_df(models_dict, y_test, A_test)","592ed4cf":"Model 101 has the lowest disparity (as seen from the graph, lower than the unmitigate model, marked by a star). We examine models 101: its AUC value is well above 0.6 and it substantially reduce the demographic disparity compared with the unmitigated model:","06bf9914":"Interestingly, demographic parity improved along with AUC using our newly trained model. However it is important to note that there is a trade-off observed between demographic parity and AUC as seen from the demographic disparity against worst-case AUC graph above. Improving selection rates across groups could also lead to more false positives in the group which we think are discriminated against. In conclusion, when pursuing better equality amongst different groups, we should look at how it affects misclassification by our model and whether we would be willing to accept this trade-off.","4416d37c":"**That's great and all but why should we even care?**","b68cbecd":"For simplicity, we will treat missing values as a separate category.\nIn the case of Gender and Married, we will drop rows with missing values","5d5b2fb9":"## Enabling Fairness in Student Loan Approvals","7401c312":"## Unmitigated Predictor\n---\n\nWe first train a standard logistic regression predictor that does not seek to incorporate any notion of fairness.","7f0501bc":"We have managed to successfully trained a model that reduces disparity between the genders (albeit trading off model performance in the process)","d288a1f7":"## Dataset\n---\n\nFor this demo, we will be using a dataset pertaining to student loan approval. Imagine if a company wants to automate its loan eligibility process (real time) based on customer detail provided while filling online application form. We will first check if the dataset is biased towards any gender and find ways to mitigate gender bias.\n\n**Dataset Description:**\n\n| Variable | Description | \n|------|------|\n| Loan_ID | Unique Loan ID | \n| Gender | Male\/ Female | \n| Married | Applicant married (Y\/N) | \n| Dependents | Number of dependents | \n| Education | Applicant Education (Graduate\/ Under Graduate) | \n| Self_Employed | Self employed (Y\/N) | \n| ApplicantIncome | Applicant income | \n| CoapplicantIncome | Coapplicant income | \n| LoanAmount | Loan amount in thousands | \n| Loan_Amount_Term | Term of loan in months | \n| Credit_History | credit history meets guidelines | \n| Property_Area | Urban\/ Semi Urban\/ Rural | \n| Loan_Status | Loan approved (Y\/N) | \n\n\nThere are 615 observations in this dataset\n\nThe dataset can be found in this link: https:\/\/www.kaggle.com\/ninzaami\/loan-predication","0ebc7e1d":"To obtain the AUC values for the overall student population as well as male and female subpopulations, we use the group metric variant of the sklearn metric roc_auc_score.","d7644ac5":"## Fairness of Model\n---\n\nThe fairlearn package provides fairness-related metrics that can be compared between groups and for the overall population. The goal is to assure that neither of the genders has substantially larger false-positive rates or false-negative rates than the other groups. Therefore, as a protected (sensitive) feature we will set gender that has a highest impact on predictions of the trained model.\n\nUsing existing metric definitions from scikit-learn we can evaluate metrics to get a group summary. As the overall performance metric we will apply the area under ROC curve (AUC), which is suited to classification problems with a large imbalance between positive and negative examples.\n\nAs the fairness metric we will use equalized odds and demographic parity.\n\n* DemographicParity: this metric states that the proportion of each segment of a protected feature (e.g. gender) should receive the positive outcome at equal rates.\n\n* EqualisedOdds this metric states that the model should correctly identify the positive outcome at equal rates across groups, but also miss-classify the positive outcome at equal rates across groups (creating the same proportion of False Positives across groups).","f4c79313":"# Fairlearn\n","cb5b3e46":"**Train-Test Distribution:**","9ab69566":"## Fairlearn Dashboard\n---\n\nMore details can be found in the Fairlearn Dashboard","276af39d":"It looks like males have a slightly higher chance of getting loans that are approved","fad7392b":"Now, let us examine the data more closely.","3cb218ac":"## Data Preparation\n---\nLet's do some data preparation\n\n1. Drop irrelevant columns (Loan_ID, LoanAmount, Loan_Amount_Term)\n2. Deal with missing data\n3. Encode Categorical Columns","1d55c1af":"Clearly Model 101 has less disparity between the genders for all thresholds","585c7920":"## Mitigating Discrimination with Fairlearn\n---\n\n* We can mitigate the demographic disparity using the GridSearch algorithm of Fairlearn. We will use this algorithm to obtain several models that achieve various trade-offs between accuracy (measured by AUC) and demographic disparity.\n\n* GridSearch generates models corresponding to various Lagrange multiplier vectors of the underlying constraint optimization problem. We will compute 200 models on a grid of Lagrange multiplier vectors whose L1-norm is bounded by 10. \n\nFor more details, refer to:\nhttps:\/\/github.com\/fairlearn\/fairlearn\/blob\/master\/notebooks\/Mitigating%20Disparities%20in%20Ranking%20from%20Binary%20Data.ipynb","523666b3":"In this notebook, we will use fairlearn to assess the fairness of our machine learning model, in allocating student loan approvals. Doing this will help mitigate any inherent gender\/demographic bias our model may have learnt from our dataset","ff9b5a0f":"![image.png](attachment:image.png)","37340074":"Let's assess the accuracy and disparity of the obtained predictors in a scatter plot, with x axis showing the worst-case AUC among the two subpopulations (of male and female students) and y axis showing the demographic disparity. Ideal models would be in the bottom right.","16777dd4":"Image from: https:\/\/vanrijmenam.nl\/why-we-need-ethical-ai-5-initiatives-ensure-ethics-ai\/","b8e518d3":"* One of the big risk that may occur with machine learning models is inherent bias towards certain groups\/categories of individuals. It will be highly unethical for our model deny information, products or services to groups of people simply because of their demographics. \n\n* It can also lead to groups of people having lower quality of services or products and less access to information.","34d4f7d0":"## Acknowledgements\n---\n\n* Fairlearn github repo (for code to get started with fairlearn) :\n\nhttps:\/\/github.com\/fairlearn\/fairlearn\/blob\/master\/notebooks\/Mitigating%20Disparities%20in%20Ranking%20from%20Binary%20Data.ipynb\n\n* Excellent kaggle kernel which shows how to plot visualizations from outputs of fairlearn:\n\nhttps:\/\/www.kaggle.com\/liananapalkova\/fairlearn-open-source-python-package-of-microsoft","f3053110":"The difference in accuracy rates (as indicated by the balanced error rate difference) seems to be slight between males and females, which shows little bias in the unmitigated predictions. \n\nWe can also analyze AUC and selection rate for each gender as shown below.","c1da8c61":"## What is Fairlearn?\n---\n\nFairlearn is a open-source Python package developed by Microsoft. It implements algorithms to uncover issues related to fairness of groups defined in a dataset and to mitigate these with the help of specially trained machine learning."}}