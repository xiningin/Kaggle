{"cell_type":{"32f38730":"code","d1222109":"code","ab7f11fa":"code","617425c3":"code","a323bfa6":"code","0705cf52":"code","60f81428":"code","f056d320":"code","fe81cd7c":"code","1f2d4c87":"code","35530fec":"code","e6bb3b25":"code","3828f393":"code","70811942":"code","55922274":"code","7e2b33c4":"code","7b0906f8":"code","04cac2aa":"code","87b607b2":"code","1585fe0b":"code","e85a34a7":"code","2d44591c":"code","b370c810":"code","73bac1b2":"code","94cc1748":"code","6d53dbcc":"code","5d4406b6":"code","2199f68e":"code","7c1e4722":"code","1f93872a":"code","2ecb1041":"code","8362ec93":"code","4cbdb33f":"code","cbc18689":"code","34b9f5a8":"code","3867b49f":"code","9ef2b3ac":"code","03a5cc90":"code","3291812c":"code","d19dc324":"code","57867612":"code","6b2d5af2":"code","76b69915":"code","27133a3e":"code","29db0887":"code","cc99a665":"code","fe0079ac":"markdown","2a594da2":"markdown","fae2029a":"markdown","d734bea1":"markdown","a1c98294":"markdown","f1d5598d":"markdown","eef264fc":"markdown","afb644c9":"markdown","c0c31035":"markdown","1b9a68ae":"markdown","18d12d3a":"markdown","a1cb3942":"markdown","68f354a2":"markdown","6b211a85":"markdown","0b8d1a6e":"markdown","bcd6ace4":"markdown","09ca1143":"markdown","d62b7afe":"markdown","c67b1bee":"markdown","0d8eb3c3":"markdown","22a2a869":"markdown","ecffdc0e":"markdown","185b6c86":"markdown","dc023d6d":"markdown","80ab8f1f":"markdown","985b0cea":"markdown","17728cca":"markdown"},"source":{"32f38730":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport xgboost as xgb\nimport lightgbm as lgb \nimport re\nimport shap\nfrom sklearn import preprocessing\nfrom category_encoders.one_hot import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\nfrom sklearn.model_selection import cross_val_score, KFold,GridSearchCV\nfrom sklearn.feature_selection import SelectKBest,SelectFromModel\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline,make_pipeline\nfrom sklearn.ensemble import RandomForestClassifier,VotingClassifier,BaggingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression,Perceptron,SGDClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB","d1222109":"#Read DataFrame\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest['Survived'] = np.nan\n\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\n\ndf = pd.concat([test,train],sort=False)\ndf.head()","ab7f11fa":"sns.countplot(x=df['Survived'], alpha=0.7, data=df)","617425c3":"numerics = ['float','int']\ndf2 = df[df['Survived']==0].select_dtypes(include=numerics)\ndf3 = df[df['Survived']==1].select_dtypes(include=numerics)\nsns.kdeplot(df3['Age'].values, bw=0.5,label='Survived==Yes')\nsns.kdeplot(df2['Age'].values, bw=0.5,label='Survived==NO')\nplt.xlabel('Age', fontsize=10)","a323bfa6":"sns.kdeplot(df3['Fare'].values, bw=0.5,label='Survived==Yes')\nsns.kdeplot(df2['Fare'].values, bw=0.5,label='Survived==NO')\nplt.xlabel('Fare', fontsize=10)","0705cf52":"categorics = ['object']\nc1 = df[df['Survived']==0].select_dtypes(include=categorics)\nc1['Survived'] = 'No'\nc2 = df[df['Survived']==1].select_dtypes(include=categorics)\nc2['Survived'] = 'Yes'\nc3 = pd.concat([c1,c2])\nc3 = c3.drop(['Name','Cabin','Ticket'],axis=1)\n\nfig, axes = plt.subplots(round(len(c3.columns) \/ 3), 3, figsize=(22, 9))\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(c3.columns):\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=35)\n        sns.countplot(x=c3.columns[i], alpha=0.7, data=c3, ax=ax,hue=\"Survived\")\n\nfig.tight_layout()","60f81428":"#Input median in null values\n#Median age of Pclass, because its high correlation (-0,40)\nprint(df.corr()['Age'].sort_values(ascending=False))\ndf['Age'] = df.groupby(\"Pclass\")['Age'].transform(lambda x: x.fillna(x.median()))","f056d320":"#Input median in null values\n#Median fare of Pclass, because its high correlation (-0,55)\nprint(df.corr()['Fare'].sort_values(ascending=False))\ndf['Fare'] = df.groupby(\"Pclass\")['Fare'].transform(lambda x: x.fillna(x.median()))","fe81cd7c":"#input mode in null values\ndf['Embarked'].fillna(df['Embarked'].mode()[0], inplace = True)\n\n#Define the size of family\ndf[\"Fsize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1\ndf['IsAlone'] = np.where(df[\"Fsize\"]==1,1,0)\n\n#Create a feature using age x pclass\ndf['Age_Class']= df['Age']* df['Pclass']\n\n#Transform Age in groups\ndf['Age_cut'] = pd.qcut(df['Age'],5,duplicates='drop')\n\n#Extract title of the string\ndf['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ndf['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr','Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndf['Title'] = df['Title'].replace('Mlle', 'Miss')\ndf['Title'] = df['Title'].replace('Ms', 'Miss')\ndf['Title'] = df['Title'].replace('Mme', 'Mrs')\n\n\n# Insert0 in null values \ndf['Title']= df['Title'].fillna(0)\n\n#Create a null feature fare per person\ndf['Fare_Per_Person'] = df['Fare']\/(df[\"Fsize\"]+1)\ndf['Fare_Per_Person'] = df['Fare_Per_Person'].astype(int)\n\n#Transform Fare in groups\ndf['Fare_cut'] =  pd.qcut(df['Fare'],5,duplicates='drop')\n\n#Extract len of cabin\ndf['Cabin_len'] = df['Cabin'].astype(str).apply(lambda x : len(x))\n\n#Extract the firstletter\ndf['Cabin'] = df['Cabin'].fillna(\"M\") #filter Na and input M (missing)\ndf['Cabin'] = df['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n\n#Ticktet\ndf['Ticket']=df.Ticket.apply(lambda x : len(x))\n\n\n#Drop some colunms\ndf = df.drop(['Name'], axis = 1)\n\ndf.head()","1f2d4c87":"scaler = preprocessing.StandardScaler().fit(df[['Age','Fare']])\ndf[['Age','Fare']] = scaler.transform(df[['Age','Fare']])                                          ","35530fec":"#graph individual features by survival\nfig, saxis = plt.subplots(3, 2,figsize=(22,15))\n\nsns.barplot(x = 'Sex', y = 'Survived', data=df, ax = saxis[0,0])\nsns.barplot(x = 'SibSp', y = 'Survived',data=df, ax = saxis[0,1])\nsns.barplot(x = 'Parch', y = 'Survived',  data=df, ax = saxis[1,0])\nsns.barplot(x = 'Fare_cut', y = 'Survived', data=df, ax = saxis[1,1])\nsns.barplot(x = 'Cabin', y = 'Survived', data=df, ax = saxis[2,0])\nsns.barplot(x = 'Age_cut', y = 'Survived', data=df, ax = saxis[2,1])","e6bb3b25":"#Transform categorics features in numeric\n\ndf['Age_cut'] =  LabelEncoder().fit_transform(df['Age_cut'])\n\ndf['Fare_cut'] =  LabelEncoder().fit_transform(df['Fare_cut'])\n\n\ntitles = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\ndf['Title']= df['Title'].map(titles)\n\ngroups = {\"M\": 1, \"C\": 2, \"B\": 3, \"D\": 4, \"E\": 5, \"A\": 6, \"F\": 7, \"G\": 8,'T':9}\ndf['Cabin'] = df['Cabin'].map(groups)\n\nports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ndf['Embarked'] = df['Embarked'].map(ports)\n\n#Transform sex in a dummies feature\ndf['Sex_female'] = np.where(df['Sex'] == 'female',1,0)\n\n#Transform sex in a dummies feature\ndf['Sex_male'] = np.where(df['Sex'] == 'male',1,0)\ndf = df.drop(['Sex'],axis=1)","3828f393":"from category_encoders.one_hot import OneHotEncoder\ndummies = OneHotEncoder(cols= ['Pclass','Age_cut','Fare_cut','Embarked','Title'],use_cat_names=True)\ndummies.fit(df)\ndf = dummies.transform(df)","70811942":"\n# Threshold for removing correlated variables\nthreshold = 0.8\n\n# Absolute value correlation matrix\ncorr_matrix = df.corr().abs()\ncorr_matrix.head()","55922274":"# Upper triangle of correlations\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nupper.head()","7e2b33c4":"# Select columns with correlations above threshold\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n\nprint('There are %d columns to remove.' % (len(to_drop)))\ndataset = df.drop(columns = to_drop)\nprint('Data shape: ', df.shape)\nprint('Size of the data', df.shape)","7b0906f8":"x_valid = df[df['Survived'].isna()].drop(['Survived'], axis = 1)\nsubmission = x_valid['PassengerId'].to_frame()\n#x_valid = x_valid.drop(['PassengerId'],axis = 1)\n\nx1 = df[df['Survived']==0]  \nx2 = df[df['Survived']==1]\nx= pd.concat([x1,x2])\n\ny =x.loc[:,['Survived','PassengerId']]\nx =x.drop(['Survived','PassengerId'], axis = 1)","04cac2aa":"#Read DataFrame\nteste = pd.read_csv(\"..\/input\/testey\/full-tita.csv\",sep=\";\")\nteste = teste.drop(['x'],axis=1)\nteste\n\nx_valid = x_valid.merge(teste)\nx_valid = x_valid.drop(['Survived'],axis = 1)\ny_valid = teste['Survived']\n#df = pd.concat([test,train],sort=False)\n#df.head()","87b607b2":"kf = KFold(n_splits=5, shuffle=True, random_state=42)","1585fe0b":"lr = LogisticRegression(class_weight = 'balanced', solver = 'liblinear',penalty=\"l2\")\nlr.fit(x,y['Survived'])\nprint(cross_val_score(lr, x, y['Survived'], cv=kf).mean())","e85a34a7":"lr = LogisticRegression(class_weight = 'balanced', solver = 'liblinear',penalty=\"l1\")\nlr.fit(x,y['Survived'])\nprint(cross_val_score(lr, x_valid, y_valid, cv=kf).mean())","2d44591c":"svm = SVC(gamma='auto',random_state=42)\nsvm.fit(x,y['Survived'])\nprint(cross_val_score(svm, x, y['Survived'], cv=kf).mean())","b370c810":"svm = SVC(gamma='auto',random_state=42)\nsvm.fit(x,y['Survived'])\nprint(cross_val_score(svm, x_valid, y_valid, cv=kf).mean())","73bac1b2":"sgd = linear_model.SGDClassifier(max_iter=5, tol=None)\nsgd.fit(x,y['Survived'])\nprint(cross_val_score(sgd, x, y['Survived'], cv=kf).mean())","94cc1748":"knn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(x,y['Survived'])\nprint(cross_val_score(knn, x, y['Survived'], cv=kf).mean())","6d53dbcc":"knn = KNeighborsClassifier(n_neighbors = 5)\nknn.fit(x,y['Survived'])\nprint(cross_val_score(knn, x_valid, y_valid, cv=kf).mean())","5d4406b6":"gaussian = GaussianNB()\ngaussian.fit(x,y['Survived'])\nprint(cross_val_score(gaussian, x, y['Survived'], cv=kf).mean())","2199f68e":"rf_with_standardScaler=make_pipeline(StandardScaler(),RandomForestClassifier())\nrf_with_standardScaler.fit(x,y['Survived'])\nprint(cross_val_score(rf_with_standardScaler, x, y['Survived'], cv=kf).mean())","7c1e4722":"rf_with_standardScaler=make_pipeline(StandardScaler(),RandomForestClassifier())\nrf_with_standardScaler.fit(x,y['Survived'])\nprint(cross_val_score(rf_with_standardScaler, x_valid, y_valid, cv=kf).mean())","1f93872a":"rf=RandomForestClassifier(n_estimators=100, oob_score = True)\nrf.fit(x,y['Survived'])\nprint(cross_val_score(rf, x, y['Survived'], cv=kf).mean())\nparametros = pd.DataFrame({'feature':x.columns,'Parameters':np.round(rf.feature_importances_,3)})\nparametros = parametros.sort_values('Parameters',ascending=False).set_index('feature')\nparametros","2ecb1041":"rf=RandomForestClassifier(n_estimators=100)\nrf.fit(x,y['Survived'])\nprint(cross_val_score(rf, x_valid, y_valid, cv=kf).mean())\n","8362ec93":"bagging = BaggingClassifier(bootstrap=True,n_jobs = -1,n_estimators=100)\nbagging.fit(x,y['Survived'])\nprint(cross_val_score(bagging, x, y['Survived'], cv=kf).mean())","4cbdb33f":"bagging = BaggingClassifier(bootstrap=True,n_jobs = -1,n_estimators=100)\nbagging.fit(x,y['Survived'])\nprint(cross_val_score(bagging, x_valid, y_valid, cv=kf).mean())","cbc18689":"xgboost = xgb.XGBClassifier(objective ='reg:logistic'\n                            , colsample_bytree = 0.7\n                            , learning_rate = 0.01\n                            ,max_depth = 6\n                            , n_estimators = 100\n                            ,random_state=42\n                            ,max_features= 0.8\n                          ,min_samples_leaf =0.5\n                           ,min_child_weight= 3)\nxgboost.fit(x,y['Survived'])\nprint(cross_val_score(xgboost, x, y['Survived'], cv=kf).mean())","34b9f5a8":"#121 - 79,66\n\nxgboost = xgb.XGBClassifier(colsample_bytree = 0.8\n                            , learning_rate = 0.05\n                            ,max_depth = 6\n                            , n_estimators = 100\n                            ,random_state=42 \n                           )\nxgboost.fit(x,y['Survived'])\nprint(cross_val_score(xgboost, x_valid, y_valid, cv=kf).mean())","3867b49f":"lgbm=lgb.LGBMClassifier()                          \nlgbm.fit(x,y['Survived'])\nprint(cross_val_score(lgbm, x, y['Survived'], cv=kf).mean())","9ef2b3ac":"lgbm=lgb.LGBMClassifier(colsample_bytree = 0.8\n                            , learning_rate = 0.015\n                            ,max_depth = 5\n                            , n_estimators = 100\n                            ,random_state=42 )                          \nlgbm.fit(x,y['Survived'])\nprint(cross_val_score(lgbm, x_valid, y_valid, cv=kf).mean())","03a5cc90":"stacking = VotingClassifier(estimators=[\n    ('lr',lr),('svm',svm),('rf', rf)], voting='hard')\nstacking.fit(x,y['Survived'])\nprint(cross_val_score(stacking, x, y['Survived'], cv=kf).mean())","3291812c":"stacking = VotingClassifier(estimators=[\n    ('lr',rf),('svm',knn),('xgboost', xgboost)], voting='hard')\nstacking.fit(x,y['Survived'])\nprint(cross_val_score(stacking, x_valid, y_valid, cv=kf).mean())","d19dc324":"parameters = {\n              'max_features': [0.8, 0.9],\n              'min_samples_leaf' :[0.5,0.9],\n              'colsample_bytree' :[0.8,0.9],\n              'learning_rate' : [0.01,0.1],\n               'min_child_weight': [3,5,7],\n              }\n\nmodel2 = xgb.XGBClassifier(objective ='reg:logistic' ,random_state=42,n_estimators=100,max_depth = 6)\ngrid_search2 = GridSearchCV(model2, parameters, cv=5,n_jobs=-1)\ngrid_search2.fit(x,y['Survived'])\n\nprint(grid_search2.best_params_)\nprint(grid_search2.best_score_)","57867612":"explainer = shap.TreeExplainer(grid_search2.best_estimator_)\nshap_values = explainer.shap_values(x)\nshap.summary_plot(shap_values, x)","6b2d5af2":"parametros = pd.DataFrame({'feature':x.columns,'Parameters':np.round(xgboost.feature_importances_,3)})\nparametros = parametros.sort_values('Parameters',ascending=False).set_index('feature')\ndrop = list(parametros.tail(30).index)\nprint(drop)\n\nx = x.drop(drop,axis=1)\n\nx_valid = x_valid.drop(drop,axis=1)","76b69915":"parameters = {\n              'max_features': [0.7,0.8, 0.9],\n              'min_samples_leaf' :[0.6,0.7,0.9],\n              'colsample_bytree' :[0.6,0.8,0.9],\n              'learning_rate' : [0.01,0.03,0.1],\n               'min_child_weight': [3,5,7],\n              }\n\nmodel2 = xgb.XGBClassifier(objective ='reg:logistic' ,random_state=42,n_estimators=100,max_depth = 7)\ngrid_search2 = GridSearchCV(model2, parameters, cv=5,n_jobs=-1)\ngrid_search2.fit(x,y['Survived'])\n\nprint(grid_search2.best_params_)\nprint(grid_search2.best_score_)","27133a3e":"parameters = {\n           'max_features': [0.7,0.8, 0.9],\n              'min_samples_leaf' :[0.6,0.7,0.9],\n              'colsample_bytree' :[0.6,0.8,0.9],\n              'learning_rate' : [0.01,0.03,0.1],\n               'min_child_weight': [3,5,7],\n              }\n\nmodel2 = xgb.XGBClassifier(objective ='reg:logistic' ,random_state=42,n_estimators=100,max_depth = 6)\ngrid_search2 = GridSearchCV(model2, parameters, cv=5,n_jobs=-1)\ngrid_search2.fit(x_valid,y_valid)\n\nprint(grid_search2.best_params_)\nprint(grid_search2.best_score_)","29db0887":"svc=make_pipeline(StandardScaler(),SVC(random_state=1))\nr=[0.0001,0.001,0.1,1,10,50,100]\nPSVM=[{'svc__C':r, 'svc__kernel':['linear']},\n      {'svc__C':r, 'svc__gamma':r, 'svc__kernel':['rbf']}]\nGSSVM=GridSearchCV(estimator=svc, param_grid=PSVM, scoring='accuracy', cv=2)\nscores_svm=cross_val_score(GSSVM, x_valid, y_valid,scoring='accuracy', cv=5)\nnp.mean(scores_svm)","cc99a665":"Survived = grid_search2.best_estimator_.predict(x_valid)\nsubmission['Survived'] = Survived  \nsubmission['Survived'] = submission['Survived'].astype(int)\nsubmission.to_csv('submission.csv' , index=False)","fe0079ac":"### Make submission","2a594da2":"### 4) k nearest neighbors","fae2029a":"### 3) Stochastic gradient descent","d734bea1":"### Create a final dataframe with all features","a1c98294":"### 10) Stacking (Logistic Regression + SVM + LGBM)","f1d5598d":"### Drop features with low impact ","eef264fc":"### Submission\nChoose the best models in this case Xgboost and try tuning hiper-parameters using gridsearch","afb644c9":"### Plotting numeric features","c0c31035":"### 5) Gaussian Naive Bayes","1b9a68ae":"### 2) SVM","18d12d3a":"### 7) Bagging","a1cb3942":"### 6) Random Forest","68f354a2":"### 1) Logistic Regression","6b211a85":"### Xgboost with hiper-parameters tuning","0b8d1a6e":"### Plotting target distribution","bcd6ace4":"Steps:\n* Import Data\n* Exploratory Data Analysis\n* Data Preparation + Feature Enginnering\n* Croos Validation - Kfold\n* Test some Machine Learning models\n* Choose the best ML and tuning hiper-parameters using Gridsearch\n* Plotting the feature importance\n* Make submission","09ca1143":"### Testing some Models to get the maximum accurancy ###\n1) Logistic Regression \n\n2) Support vector Machine\n\n3) Stochastic gradient descent\n\n4) k nearest neighbors\n\n5) Gaussian Naive Bayes\n\n6) Random Forest\n\n7) Bagging\n\n8) Xgboost\n\n9) Lgbm\n\n10) Stacking with voting classifier","d62b7afe":"Collinear variables are those which are highly correlated with one another. These can decrease the model's availablility to learn, decrease model interpretability, and decrease generalization performance on the test set.","c67b1bee":"### Plotting the feature importance using shap ","0d8eb3c3":"# Challenge: Create a model that predicts which passengers survived the Titanic","22a2a869":"### Data Preparation + Feature Enginnering","ecffdc0e":"### 9) LGBM","185b6c86":"### Import Data","dc023d6d":"### Exploratory Data Analysis","80ab8f1f":"### 8) Xgboost","985b0cea":"### Plotting categorics features","17728cca":"###  Cross Validation"}}