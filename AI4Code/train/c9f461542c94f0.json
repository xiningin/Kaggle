{"cell_type":{"f039faa7":"code","548daa43":"code","a65e9bba":"code","657a9088":"code","99a49a20":"code","f9258ab0":"code","514065d8":"code","0d437052":"code","9beb2a82":"code","7c7bf8cf":"code","0b276ae8":"code","243b2717":"code","84f36957":"code","7c4cebd3":"code","f245b282":"code","09ff653b":"code","a4453b8a":"code","cf9db883":"markdown","d7ac3536":"markdown","35684575":"markdown","5927f651":"markdown","a7e5f253":"markdown","1d978d14":"markdown","401b6b31":"markdown","b09a3cf7":"markdown","7b160e34":"markdown","69cc55fc":"markdown","cdcb81fa":"markdown","be769d89":"markdown","aae9a7f5":"markdown","fac12598":"markdown","00a5e152":"markdown","ea99e61a":"markdown","9fb5551c":"markdown","d0161f73":"markdown"},"source":{"f039faa7":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport re as re\n\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import History,LearningRateScheduler\nfrom  tensorflow.keras.layers import Dropout\nprint('TensorFlow %s, Keras %s, numpy %s, pandas %s'%(tf.__version__,keras.__version__, np.__version__,pd.__version__))\n__DEBUG__=False\n","548daa43":"## Structure du r\u00e9seau et nombre d'epochs (nombre de fois o\u00f9 on passe sur le DataSet)\nnum_hidden_layers=4\nfirst_layer_size = 128\nother_layer_size = 512\nepochs=50\n\n###Valeurs A tester dans la cross validation\nlst_init_learning_rate = [0.01,0.003, 0.1] \nlst_dropout_prob=[0.15,0.05]\nn_splits=10","a65e9bba":"#Calcule les valeurs min\/max et moyennes de chaque colonne dans lst_cols du dataframe pandas  df\ndef get_columns_metadata(df, lst_cols):            \n     header_df = pd.DataFrame( data = lst_cols, columns=['var_name'])    \n     header_df['mean']=df[lst_cols].mean().values\n     header_df['min']= df[lst_cols].min().values\n     header_df['max']= df[lst_cols].max().values\n     header_df.set_index('var_name',inplace=True)\n     return header_df\n\n#Normalisation de chaque colonne du dataframe pandas  df en utilisant les valeurs de header_df\ndef normalize(df,header_df):\n    for col in df.columns:        \n        if col in header_df.index : \n### Ici normaliser chaque  colonne. Pour l'instant on ne fait rien\n            df[col] = 2*((df[col]-df[col].mean())\/(df[col].max()-df[col].min()))","657a9088":"# La fonction pandas pd.read_csv permet de cr\u00e9er un objet Dataframe \u00e0 partir d'un csv\n\n# Donn\u00e9es avec labels\ntrain = pd.read_csv('..\/input\/train.csv', header = 0, dtype={'Age': np.float64})\n# Donn\u00e9es de tests sans label. Les pr\u00e9dictions de survie seront envoy\u00e9es \u00e0 kaggle\ntest  = pd.read_csv('..\/input\/test.csv' , header = 0, dtype={'Age': np.float64})\n# On r\u00e9unit les donn\u00e9es dans une liste (pour pouvoir boucler sur les 2 dataframes)\nfull_data = [train, test]\n#On garde les passagers ID des donn\u00e9es test, car on en aura besoin pour le fichiers r\u00e9sultats de kaggle (voir l'exemple gender_submission.csv)\nfinalfile_index=test.PassengerId #Index des donn\u00e9es de test pour le r\u00e9sultat final\n\n#La fonction info() permet de r\u00e9p\u00e9rer les colonnes avec des valeurs nulles\nprint('\\nTrain data:')\ntrain.info()\nprint('\\nTest data:')\ntest.info()","99a49a20":"print (train[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean())","f9258ab0":"print (train[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean())","514065d8":"for dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\nprint (train[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean())","0d437052":"for dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\nprint (train[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean())","9beb2a82":"for dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\nprint (train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean())","7c7bf8cf":"for dataset in full_data:\n    dataset.loc[dataset.Fare.isnull(), 'Fare'] = train['Fare'].mean()\ntrain['CategoricalFare'] = pd.qcut(train['Fare'], 4)\nprint (train[['CategoricalFare', 'Survived']].groupby(['CategoricalFare'], as_index=False).mean())","0b276ae8":"for dataset in full_data:\n    age_avg = dataset['Age'].mean() # Calcul de la valeur moyenne\n    age_std = dataset['Age'].std()  # Calcul de l'\u00e9cart type\n    age_null_count = dataset['Age'].isnull().sum() # nombre de valuer nulle\n    \n    #On g\u00e9n\u00e8re une valeur al\u00e9atoire pour chaque valeur nulle, puis on l'arrondit \u00e0 l'entier\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)   \n    dataset.loc[np.isnan(dataset['Age']),'Age'] = age_null_random_list    \n    dataset['Age'] = dataset['Age'].astype(int)\n\n#Impact de l'age sur le taux de survie\ntrain['CategoricalAge'] = pd.qcut(train['Age'],5)\nprint (train[['CategoricalAge', 'Survived']].groupby(['CategoricalAge'], as_index=False).mean())\n","243b2717":"for dataset in full_data:\n    # Traitement variable 'Sex'\n    dataset['Sex'].replace('female',0,inplace=True )\n    dataset['Sex'].replace('male',1,inplace=True)\n    \n   # Traitement variable 'Embarked'\n    dataset['Embarked'].replace('S',0,inplace=True)\n    dataset['Embarked'].replace('C',1,inplace=True)\n    dataset['Embarked'].replace('Q',2,inplace=True)    \n\n# Suppression des colonnes inutiles (Traitements diff\u00e9rents sur Train et Test => on ne peut pas mettre ces instruction dans la boucle)\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp','Parch', 'FamilySize']\ntrain = train.drop(drop_elements, axis = 1)\ntrain = train.drop(['CategoricalAge', 'CategoricalFare'], axis = 1)\n\n### N'oubliez pas de mettre \u00e0 jour la fonction normalize !\nheader_df=get_columns_metadata(train,list(train.columns.values)) \nprint(header_df)\nnormalize(train,header_df)\n\ntest  = test.drop(drop_elements, axis = 1)\nnormalize(test,header_df)\n\nprint('\\nTrain data:')\nprint (train.head(10))\nprint('\\nTest data:')\nprint (test.head(10))","84f36957":"def set_model(init_learning_rate,dropout_prob):\n    #Architecture du r\u00e9seau\n    model = keras.Sequential()\n    model.add(keras.layers.Dense(first_layer_size, activation='relu'))\n    \n    model.add(keras.layers.Dropout(0.15))\n### Ajouter ici une ligne  pour g\u00e9rer le sur-apprentissage\n\n    #Couches cach\u00e9es (Hidden Layers)\n    for i in range(num_hidden_layers):\n        # Adds a densely-connected layer  to the model:\n        model.add(keras.layers.Dense(other_layer_size, activation='relu'))\n        model.add(keras.layers.Dropout(0.15))\n### Ajouter ici une ligne  pour g\u00e9rer le sur-apprentissage\n    # Couche de Sortie (avec fonction Softmax):\n    model.add(keras.layers.Dense(2, activation='softmax'))    \n    global_step = tf.Variable(0, trainable=False)\n    learning_rate = tf.train.exponential_decay(init_learning_rate, global_step,1000, 0.96, staircase=True)\n    \n\n### Ici vous pouvez essayer diff\u00e9rents algos de descentes de gradients \n    #D\u00e9finiton de l'optimizer  en charge de la Gradient Descent, de la fonction de co\u00fbt et de la m\u00e9trique.\n    model.compile(optimizer=tf.train.GradientDescentOptimizer(learning_rate),#RMSPropOptimizer(learning_rate), #GradientDescentOptimizer(learning_rate),AdamOptimizer\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model","7c4cebd3":"###Essayez Diff\u00e9rents jeu de param\u00e8tre pour r\u00e9duire le sur-appentissage\ninit_learning_rate=0.15\ndropout_prob= 0\ncheck_epochs=112\npourcentage_validation= 0.2\n\n#A partir des donn\u00e9es Train, on s\u00e9pare features (X)  et labels \"Survived\"\nlst_col=list(train.columns.values)\nlst_col.remove('Survived')\nX=train[lst_col]\ny=train['Survived']\n\n# On calcule la position de la s\u00e9paration pour une r\u00e9partition 80\/20\nposition_validation_data=int(train.shape[0] * (1-pourcentage_validation))\nprint('position_validation_data=',position_validation_data)\n\n# Construction des Features pour l'apprentissage et la validation.  Transformation du Dataframe Pandas en Numpy Array (attendu par Keras) \nX_train, X_val = X[lst_col][:position_validation_data].values, X[lst_col][position_validation_data:].values\n\n# Construction des Labels pour l'apprentissage et la validation.  Hot Encoding \ny_train, y_val = np.transpose([1-y[:position_validation_data], y[:position_validation_data]]), \\\n                  np.transpose([1-y[position_validation_data:], y[position_validation_data:]]) \n\n\n#Construction du mod\u00e8le en appelant la fonction set_model\nmodel = set_model(init_learning_rate,dropout_prob) \n#d\u00e9finition d'une fonction History pour r\u00e9cup\u00e9rer la fonction de co\u00fbt et la m\u00e9trique \u00e0 chaque epoch.\nhist = History()\nmodel.fit(X_train, y_train, epochs=check_epochs, batch_size=128,validation_data=(X_val, y_val),verbose=False, callbacks=[hist])\n\nprint(hist.history.keys())\n\nplt.rcParams[\"figure.figsize\"] = (40,20)\nfig, ax1 = plt.subplots()\nax2 = ax1.twinx()\nax1.plot(hist.history['val_loss'], color= 'g')\nax2.plot(hist.history['loss'], color= 'b')\nax1.set_xlabel('epochs')\nax1.set_ylabel('Validation data Error', color='g')\nax2.set_ylabel('Training Data Error', color='b')\nplt.show()\n                                 ","f245b282":"#Pour un mod\u00e8le  donn\u00e9, on ex\u00e9cute la cross validation en utilisant un objet sss sklearn StratifiedShuffleSplit\ndef cv_run(model, name, sss):  \n    loop=1\n    for train_index, test_index in sss.split(X, y):\n### A vous de completer les 2 lignes ci-dessous.\n### Il faut extraire les donn\u00e9es d'apprentissage et de test des donn\u00e9es du dataframe train en utilisant les index renvoy\u00e9 par la fonction split\n### Vous pouvez vous inspirer du code du bloc \"V\u00e9rification du Sur-Apprentissage\"\n        X_train, X_val = X[train_index], X[test_index]\n        y_train, y_val = np.transpose([1-y[train_index], y[train_index]]), np.transpose([1-y[test_index], y[test_index]]) \n\n# Apprentissage et \u00e9valuation        \n        hist = History()\n        model.fit(X_train, y_train, epochs=epochs, batch_size=32,validation_data=(X_val, y_val),verbose=False, callbacks=[hist])\n        [loss, acc] = model.evaluate(X_val, y_val, batch_size=32,verbose=False)    \n\n#Ajout de la performance dans les dictionnaires \"loss_dict\" et \"acc_dict\"\n        if name in acc_dict:\n          acc_dict[name] += acc\n          loss_dict[name] += loss\n        else:\n          acc_dict[name] = acc\n          loss_dict[name] = loss\n#Affichage de l'avancement\n        print(loop,':',[loss, acc])\n        loop+=1    ","09ff653b":"#Donn\u00e9es utilis\u00e9es pour la m\u00e9thode split de l'objet StratifiedShuffleSplit\nX = train.values[0::, 1::]\ny = train.values[0::, 0]\n\n#Cr\u00e9atio d'un dictionnaire pour stocker les mod\u00e8les\nmodel_dict={}\n\nsss = StratifiedShuffleSplit(n_splits=n_splits, test_size=0.1, random_state=0)\n\n#Cr\u00e9atio d'un dataframe pour logger les r\u00e9sultatsc\nlog_cols = [\"Classifier\", \"Accuracy\"]\nlog = pd.DataFrame(columns=log_cols)\n\n#Boucle sur des valeurs de init_learning_rate et de dropout_prob\nfor init_learning_rate in lst_init_learning_rate:\n    for dropout_prob in  lst_dropout_prob :\n        #Initialisation des dictionnaires utilis\u00e9s dans la cross validation \n        acc_dict = {}\n        loss_dict = {}\n        #Construction du nom du mod\u00e8le, en fonction des param\u00e8tres\n        name=\"lr_%s_do_%s\"%(init_learning_rate,dropout_prob)\n        #Cr\u00e9ation de l'objet mod\u00e8le\n        model = set_model(init_learning_rate,dropout_prob) \n        #Ajout du mod\u00e8le au dico pour s\u00e9lectionner le meilleur dans le suivant\n        model_dict[name]=model\n        cv_run(model, name, sss)        \n        # Calcul de la performance du mod\u00e8le comme moyenne pour chaque it\u00e9ration dans cross-validation  \n        for clf in acc_dict:\n            acc_dict[clf] = acc_dict[clf] \/ n_splits\n            log_entry = pd.DataFrame([[clf, acc_dict[clf]]], columns=log_cols)\n            log = log.append(log_entry)\nprint (log.values)","a4453b8a":"###A vous de completer les 3 lignes ci-dessous, sans oublier la normalisation !\n### Analyser les r\u00e9sultats du bloc pr\u00e9c\u00e9dent pour choisir le meilleur param\u00e8tre\n# best_model = model_dict[ ??? ]\n# X = ???\n# y = ???\n\ny_hot = np.transpose([1-y, y])\n\n#Apprentissage sur toutes les donn\u00e9es, avec le mod\u00e8le s\u00e9lectionn\u00e9\nbest_model.fit(X,y_hot, epochs=epochs, batch_size=32,verbose=False)\nprint(pd.DataFrame(best_model.evaluate(X, y_hot, batch_size=32,verbose=False),index=model.metrics_names))\n\n#Inf\u00e9rence des donn\u00e9es du fichier test et Construction du fichier \u00e0 envoyer \u00e0 Kaggle \nprediction=best_model.predict(test.values, batch_size=32)\nresults=pd.DataFrame(np.argmax(prediction,axis=1), index = finalfile_index, columns=['Survived'])\nresults.to_csv('resultats.csv')\nprint(results.sum())\nresults.describe()","cf9db883":"Introduction d'une distinction sur les personnes seules","d7ac3536":"## 4. Embarked ##\nImpact du Port d'embarquement sur la Survie.","35684575":"# Mise en Forme des donn\u00e9es #\n### > Remplacement des donn\u00e9es textuelles par des donn\u00e9es num\u00e9riques\n### > Suppressions des colonnes inutiles (sans impact sur la survie ou cr\u00e9\u00e9es ci-dessus)###\n\n## ATTENTION : Il faut lancer \"Run All Above Selected Cell\" dans le menu Run pour pouvoir relancer ce bloc","5927f651":"## 2. Sex ##\nImpact du genre sur la Survie.","a7e5f253":"## Fonctions ##","1d978d14":"## Import des librairies ##","401b6b31":"# Prediction #\nMaintenant on utilise le meilleur jeu de param\u00e8tre pour faire la pr\u00e9diction","b09a3cf7":"## Fonction de cross validation ##","7b160e34":"## 1. Pclass ##\nImpact de la classe sur la Survie.","69cc55fc":"## V\u00e9rification du Sur-Apprentissage ##","cdcb81fa":"## 3. SibSp and Parch ##\nImpacte de la taille de la famille.","be769d89":"## lecture des donn\u00e9es ##","aae9a7f5":"## 6. Age ##\nPour les valeurs vides, on gn\u00e8re des ages al\u00e9atoires entre (mean - std) and (mean + std).\nEnsuite on analyse l'impact","fac12598":"## 5. Fare ##\nOn remplace les valeurs manquantes par la moyenne. Puis on regarde l'impact du prix du ticket","00a5e152":"## Hyperparametrage ##\n### Ce traitement va \u00eatre long. Commencer par une faible valeur du param\u00e8tre epochs","ea99e61a":"## Param\u00e8tres ##","9fb5551c":"## Cr\u00e9ation du mod\u00e8le et initialisation Training ##","d0161f73":"# Analyse des donn\u00e9es #"}}