{"cell_type":{"17a0c213":"code","fba9acf0":"code","f207aff6":"code","65bd51dd":"code","d1ecfe65":"code","36c8a42b":"code","ddcb625b":"code","f90b2e0c":"code","1ae55500":"code","8b789e0b":"code","eae77daf":"code","52d1f53f":"code","208e75fe":"code","bca128f7":"code","724209fa":"code","e56c60f4":"code","4a9c7665":"code","b96efac0":"code","b5439b28":"code","6b302d7b":"code","928d4247":"code","d23e3584":"code","09b3d6fc":"code","7f7cf672":"code","07ca818c":"code","bac4e86d":"code","eb70dac4":"code","14fb51a9":"code","26147ff0":"code","c8588784":"code","64a5232d":"markdown","ca3e8904":"markdown","b00d85aa":"markdown","25d42aa3":"markdown","6dbeb181":"markdown","899a43bc":"markdown"},"source":{"17a0c213":"import lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\n\nimport riiideducation\n\npd.options.display.max_columns = None","fba9acf0":"env = riiideducation.make_env()","f207aff6":"train_df = pd.read_csv('..\/input\/riiid-test-answer-prediction\/train.csv', low_memory=False, nrows=10**7,\n                      dtype={\n                          'row_id': 'int64', 'timestamp': 'int64', 'user_id': 'int32', 'content_id': 'int16', 'content_type_id': 'int8',\n                              'task_container_id': 'int16', 'user_answer': 'int8', 'answered_correctly': 'int8', 'prior_question_elapsed_time': 'float32', \n                             'prior_question_had_explanation': 'boolean',\n                      })\n\n# train_df = train_df.query('answered_correctly != -1').reset_index(drop=True)\n# train_df['prior_question_had_explanation'] = train_df['prior_question_had_explanation'].astype(float) \u5f8c\u3067\u3084\u308b","65bd51dd":"train_df","d1ecfe65":"# 900\u4e07\u884c\u30fb\u30fb\u30fb\u7279\u5fb4\u91cf\u4f5c\u6210\nfeatures_part_df = train_df.iloc[:int( 9 \/ 10 * len(train_df) )]\n# 100\u4e07\u884c\u30fb\u30fb\u30fb\u6700\u65b0\u306e100\u4e07\u4ef6 \ntrain_part_df = train_df.iloc[int( 9 \/ 10 * len(train_df) ):]","36c8a42b":"train_part_df","ddcb625b":"# \u8ffd\u52a0\u3059\u308b\u30c7\u30fc\u30bf1 user_answers_df\ntrain_questions_only_df = features_part_df[features_part_df['answered_correctly'] != -1]\n\ngrouped_by_user_df = train_questions_only_df.groupby('user_id')\n\nuser_answers_df = grouped_by_user_df.agg({'answered_correctly': ['mean', 'count', 'std', 'median', 'skew'], 'prior_question_had_explanation': ['mean']}).copy()\n\n# ---- \u3053\u3053\u304b\u3089\u65b0\u3057\u304f\u8ffd\u52a0 ---- #\n\nuser_answers_df[('test', 'score')] = user_answers_df[('answered_correctly', 'mean')] * (user_answers_df[('answered_correctly', 'count')])\n\n# \u5e73\u5747\nmean = user_answers_df[('test', 'score')].mean()\n# \u504f\u5dee\u3000deviation\nuser_answers_df[('test', 'deviation')] = user_answers_df[('test', 'score')] - mean\n# \u5e73\u65b9\u6570\u3000Square\nuser_answers_df[('test', 'square')] = user_answers_df[('test', 'deviation')] * user_answers_df[('test', 'deviation')]\n# \u5206\u6563 \u5168\u54e1\u306e\u5e73\u65b9\u6570\u306e\u5408\u8a08\u00f7\u4eba\u6570=\u5206\u6563\nvariance = user_answers_df[('test', 'square')].sum() \/ len(user_answers_df.index)\nstandard_deviation = np.sqrt(variance)\n\n# \u504f\u5dee\u5024\u3000(\u5e73\u5747\u3068\u306e\u5dee\u00d710\u00f7\u6a19\u6e96\u504f\u5dee)+50=\u25cb\u25cb\nuser_answers_df[('test', 'Deviation_Value')] = (user_answers_df[('test', 'deviation')] * 10 \/ standard_deviation) + 50","f90b2e0c":"user_answers_df[('test', 'smart_user')] = 0\n\n# \u5fa9\u7fd2\u7387\u304c\u300180%\u4ee5\u4e0a\u306e\u30e6\u30fc\u30b6\u30fc \uff1d\u3000smart_user 1 \u306b\u3059\u308b\n# \u504f\u5dee\u5024\u304c\u9ad8\u3044\u50be\u5411\u306b\u3042\u308b\u304b\u3089\n\nfor index, row in user_answers_df.iterrows():\n    if row[('prior_question_had_explanation','mean')] >= 0.8:\n        user_answers_df[('test','smart_user')][index] = 1\n        \nuser_answers_df.columns = ['mean_user_accuracy', 'questions_answered', 'std_user_accuracy', 'median_user_accuracy', 'skew_user_accuracy', 'mean_prior_question_had_explanation', 'user_score', 'score_deviation', 'score_square', 'Deviation_Value', 'smart_user']","1ae55500":"user_answers_df","8b789e0b":"# \u8ffd\u52a0\u3059\u308b\u30c7\u30fc\u30bf2 questions_df\nquestions_df = pd.read_csv('..\/input\/riiid-test-answer-prediction\/questions.csv')\n\ngrouped_by_content_df = train_questions_only_df.groupby('content_id')\n\ncontent_answers_df = grouped_by_content_df.agg({'answered_correctly': ['mean', 'count', 'std', 'median', 'skew'] }).copy()\ncontent_answers_df.columns = ['mean_accuracy', 'question_asked', 'std_accuracy', 'median_accuracy', 'skew_accuracy']\n\nquestions_df = questions_df.merge(content_answers_df, left_on = 'question_id', right_on = 'content_id', how = 'left')\n\nbundle_dict = questions_df['bundle_id'].value_counts().to_dict()\n\n# right_answers \u6b63\u89e3\u6570\nquestions_df['right_answers'] = questions_df['mean_accuracy'] * questions_df['question_asked']\n\nquestions_df['bundle_size'] = questions_df['bundle_id'].apply(lambda x: bundle_dict[x])","eae77daf":"questions_df","52d1f53f":"# \u8ffd\u52a0\u3059\u308b\u30c7\u30fc\u30bf3 bundle_answers_df\ngrouped_by_bundle_df = questions_df.groupby('bundle_id')\n\nbundle_answers_df = grouped_by_bundle_df.agg({'right_answers': 'sum', 'question_asked': 'sum'}).copy()\nbundle_answers_df.columns = ['bundle_right_answers', 'bundle_questions_asked']\n\nbundle_answers_df['bundle_accuracy'] = bundle_answers_df['bundle_right_answers'] \/ bundle_answers_df['bundle_questions_asked']","208e75fe":"bundle_answers_df","bca128f7":"# \u8ffd\u52a0\u3059\u308b\u30c7\u30fc\u30bf4 part_answers_df\ngrouped_by_part_df = questions_df.groupby('part')\n\npart_answers_df = grouped_by_part_df.agg({'right_answers': 'sum', 'question_asked': 'sum'}).copy()\n\npart_answers_df.columns = ['part_right_answers', 'part_questions_asked']\npart_answers_df['part_accuracy'] = part_answers_df['part_right_answers'] \/ part_answers_df['part_questions_asked']","724209fa":"part_answers_df","e56c60f4":"# \u8b1b\u7fa9(-1)\u4ee5\u5916\u3092\u62bd\u51fa train\ntrain_part_df = train_part_df[train_part_df['answered_correctly'] != -1]","4a9c7665":"# user_answers_df\ntrain_part_df = train_part_df.merge(user_answers_df, how='left', on='user_id')\n\n# questions_df\ntrain_part_df = train_part_df.merge(questions_df, how='left', left_on='content_id', right_on='question_id')\n\n# bundle_answers_df\ntrain_part_df = train_part_df.merge(bundle_answers_df, how='left', on='bundle_id')\n\n# part_answers_df\ntrain_part_df = train_part_df.merge(part_answers_df, how='left', on='part')","b96efac0":"# \u30e6\u30fc\u30b6\u30fc\u304c\u8cea\u554f\u306b\u56de\u7b54\u3057\u305f\u5f8c\u3001\u8aac\u660e\u3068\u6b63\u3057\u3044\u56de\u7b54\u3092\u78ba\u8a8d\u3057\u305f\u304b\u3069\u3046\u304b \u6b20\u640d\u5024\u3092False\u3068\u7f6e\u304f\u3001 astype\u3067\u30c7\u30fc\u30bf\u578b\u306e\u5909\u63db(\u30ad\u30e3\u30b9\u30c8)\ntrain_part_df['prior_question_had_explanation'] = train_part_df['prior_question_had_explanation'].fillna(value=False).astype(bool)\n\ntrain_part_df.fillna(value = -1, inplace = True)","b5439b28":"# \u30e9\u30d9\u30eb\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\nle = LabelEncoder()\ntrain_part_df[\"prior_question_had_explanation\"] = le.fit_transform(train_part_df[\"prior_question_had_explanation\"])","6b302d7b":"train_part_df","928d4247":"train_part_df.columns","d23e3584":"# \u65e7\n# features = [\n#     'timestamp','mean_user_accuracy', 'questions_answered','mean_accuracy',\n#     'question_asked','prior_question_elapsed_time', 'prior_question_had_explanation',\n#     'bundle_size', 'bundle_accuracy','part_accuracy', 'right_answers'\n# ]\n\n\n# features = [\n#     'timestamp','prior_question_elapsed_time', 'prior_question_had_explanation',\n#     'mean_user_accuracy', 'questions_answered', 'std_user_accuracy',\n#     'median_user_accuracy', 'skew_user_accuracy','mean_accuracy',\n#     'question_asked', 'std_accuracy', 'median_accuracy', 'skew_accuracy',\n#     'bundle_size','bundle_accuracy', 'part_accuracy','user_score',\n#     'score_deviation', 'score_square', 'Deviation_Value',\n# ]\n\n# \u65b0\nfeatures = [\n    'timestamp','prior_question_elapsed_time', 'prior_question_had_explanation',\n       'mean_user_accuracy', 'questions_answered', 'std_user_accuracy',\n       'median_user_accuracy', 'skew_user_accuracy',\n       'mean_prior_question_had_explanation','Deviation_Value', 'smart_user',\n       'mean_accuracy','question_asked', 'std_accuracy', 'median_accuracy', 'skew_accuracy',\n       'bundle_size','bundle_accuracy','part_accuracy'\n]\n\ntarget = 'answered_correctly'","09b3d6fc":"X_train = train_part_df[features]\ny_train = train_part_df[target]","7f7cf672":"X_train","07ca818c":"models = []\noof_train = np.zeros(len(X_train),) ### array([0., 0., 0., ..., 0., 0., 0.])\ncategorical_features = ['prior_question_had_explanation']\n\nparams = {\n    'objective': 'binary',\n    'max_bin': 300,\n    'learning_rate': 0.05,\n    'num_leaves': 40\n}\n\nn_tr = round(981094 * 0.9)\n\nX_tr = X_train[:n_tr]\nX_val = X_train[n_tr:]\n\ny_tr = y_train[:n_tr]\ny_val = y_train[n_tr:]\n\nlgb_train = lgb.Dataset(X_tr, y_tr, categorical_feature=categorical_features)\nlgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train, categorical_feature=categorical_features)\n\nmodel = lgb.train(\n    params,\n    lgb_train,\n    valid_sets=[lgb_train, lgb_eval],\n    verbose_eval=10,\n    num_boost_round=1000,\n    early_stopping_rounds=100 # 30, 50 \u304f\u3089\u3044\u3067\u8a66\u3059\u3000\n)\n\noof_train = model.predict(X_val, num_iteration=model.best_iteration)\n\nmodels.append(model)","bac4e86d":"# importance\u3092\u8868\u793a\u3059\u308b\nimportance = pd.DataFrame(model.feature_importance(), index=X_train.columns, columns=['importance'])\nresult = importance.sort_values('importance', ascending=False)","eb70dac4":"result","14fb51a9":"# ROC\u66f2\u7dda\u306eAUC\u30b9\u30b3\u30a2\u30fb\u30fb\u30fb\u66f2\u7dda\u4e0b\u306e\u9762\u7a4d\u3092\u610f\u5473\u3059\u308b\u3089\u3057\u3044\u3002\nroc_auc_score(y_val, oof_train)","26147ff0":"iter_test = env.iter_test()","c8588784":"for (test_df, sample_prediction_df) in iter_test:\n    y_preds = []\n    \n    test_df = test_df.merge(user_answers_df, how = 'left', on = 'user_id')\n    test_df = test_df.merge(questions_df, how = 'left', left_on = 'content_id', right_on = 'question_id')\n    test_df = test_df.merge(bundle_answers_df, how = 'left', on = 'bundle_id')\n    test_df = test_df.merge(part_answers_df, how = 'left', on = 'part')\n    \n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].fillna(value = False).astype(bool)\n    test_df.fillna(value = -1, inplace = True)\n    X_test = test_df[features]\n    \n    for model in models:\n        y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n        y_preds.append(y_pred)\n        \n    y_preds = sum(y_preds) \/ len(y_preds)\n    test_df['answered_correctly'] = y_preds\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","64a5232d":"features_part_df \u3067\u65b0\u3057\u3044\u7279\u5fb4\u91cf\u3092\u4f5c\u6210","ca3e8904":"* Knowledge tracing (KT)\u306b\u3064\u3044\u3066\n\nKnowledge tracing (KT) refers to the problem of predicting future learner performance given their past performance in educational applications\n\nEach student's knowledge is modeled by estimating the performance of the student on the learning activities.\n\nKnowledgeTracing\u3001EdNet, Artificial Intelligence in Education (AIEd)\u306b\u95a2\u3059\u308b\u8a18\u4e8b\nhttps:\/\/www.kaggle.com\/c\/riiid-test-answer-prediction\/discussion\/188911\n\n* \u6642\u5236\u306b\u95a2\u3057\u3066\n\n\u6642\u5236\u30c7\u30fc\u30bf\u306b\u306f\u3001\u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3092\u3059\u308b\u3053\u3068\u304c\u91cd\u8981\u3002(https:\/\/maxhalford.github.io\/blog\/pandas-tricks\/#target-encoding-for-time-series)\n\n\u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u306b\u3064\u3044\u3066 (https:\/\/maxhalford.github.io\/blog\/target-encoding\/)\n\nThe one thing I do in every time series competition is target encoding. In short, the goal of target encoding is to replace a category by the average of the target values of the rows that belong to said category. Naturally, you\u2019re not limited to using an average. You can also use Bayesian target encoding if some of your categories are rare.\n\nTarget encoding is very important for time series data. It has been used in almost every top model on Kaggle time series competitions, such as ASHRAE, Recruit Restaurants, and Wikipedia Web Traffic. When you do target encoding on temporal data, you need to be wary of not leaking current and future information into the aggregate of each row. Basically, you need to shift the target values within each group backwards. Indeed, for any given moment, we want our aggregate to pertain to past data only.\n\n\u53c2\u8003\u30c7\u30a3\u30b9\u30ab\u30b7\u30e7\u30f3 (https:\/\/www.kaggle.com\/c\/riiid-test-answer-prediction\/discussion\/189437)\n\n* Timestamp \u3068 prior_question_elaped_time \u306b\u3064\u3044\u3066\n\nhttps:\/\/www.kaggle.com\/c\/riiid-test-answer-prediction\/discussion\/189351\n\u3000","b00d85aa":"features_part_df \u3068 train_part_df \u306b\u5206\u3051\u308b\u3002","25d42aa3":"## discussion","6dbeb181":"\u30c7\u30fc\u30bf\u306f\u3001TOEIC\u306e\u5185\u5bb9\u3089\u3057\u3044\nhttps:\/\/www.kaggle.com\/c\/riiid-test-answer-prediction\/discussion\/190191","899a43bc":"* prior_question_elapsed_time\n\nCORRECT: Note that the time is the average time a user took to solve each question in the previous bundle\n\nThat is, if three questions are in a previous bundle, the questions in the following bundle share the same prior_question_elapsed_time and the value is the total time the user took to solve all three questions in the previous bundle, divided by three. We apologize for any inconvenience. The correction has been updated in the data description.\n\nSuppose a user spent 60 seconds answering their first question at 10:00. The timestamp for that row would read 10:00 (we'll skip the normalization for simplicity), and the prior_question_elapsed_time would be null since it's the first question. If they took 30 seconds to answer their next question at 11:00 the second row's timestamp would be 11:00 and the prior_question_elapsed_time would be 60 seconds."}}