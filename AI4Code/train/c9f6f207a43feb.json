{"cell_type":{"9421a1bd":"code","deebe313":"code","bb6794d6":"code","588c3d94":"code","5835ee5a":"code","3ada6889":"code","a40a3b48":"code","2b318959":"code","17c29d25":"code","a85d19aa":"code","42633a50":"code","18cf274a":"code","f3b3653f":"code","4f23cbb9":"code","8488cc00":"code","bf2d7c90":"code","42cbd09f":"code","de59c36f":"code","e2efebda":"code","ba01d297":"code","9c1654a1":"code","95776fe5":"code","4c75f5a4":"code","a120370e":"code","89e84891":"code","6f55dc44":"code","266676aa":"code","28dffbe7":"code","8a591d01":"code","4bd5a27e":"code","c60edd50":"code","cbf22462":"code","09e8e156":"code","41bf222c":"code","ab1d4e1a":"markdown","1360ae3d":"markdown","e1bdf651":"markdown","c024f6be":"markdown","131289b3":"markdown","85c1d4ce":"markdown","3d363fa9":"markdown","d10bf72f":"markdown","57d70dd0":"markdown","fe2f94b9":"markdown"},"source":{"9421a1bd":"# Octopus ML pakage - github.com\/gershonc\/octopus-ml\n!pip install octopus-ml","deebe313":"import warnings\nwarnings.simplefilter(\"ignore\")\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport time\nimport pandas as pd\nimport numpy as np\nimport lightgbm as lgb\nimport tracemalloc\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.metrics import classification_report\n%matplotlib inline\nsns.set_style(\"whitegrid\")\n\npd.set_option('display.max_columns', None)  # or 1000\npd.set_option('display.max_rows', None)  # or 1000\npd.set_option('display.max_colwidth', -1)  # or 199\n\n#check out https:\/\/github.com\/gershonc\/octopus-ml\nimport octopus_ml as oc\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bb6794d6":"train_df = pd.read_csv ( \"..\/input\/widsdatathon2021\/TrainingWiDS2021.csv\")\ntest_df = pd.read_csv(\"..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\")\ndata_dict = pd.read_csv ( \"..\/input\/widsdatathon2021\/DataDictionaryWiDS2021.csv\")","588c3d94":"# Data shape \nprint (\"Train set: \",train_df.shape)\nprint (\"Test set: \",test_df.shape)","5835ee5a":"# DataFrane Summary by pandas summary package (extension of pandas.describe method) \ndfs = DataFrameSummary(train_df)\ndfs.summary()","3ada6889":"data_dict.head(10)","a40a3b48":"import missingno as msno\nmsno.matrix(train_df)\n# The dataset is quite full, there are sparse areas which are mainly the lab results (low frequency data)","2b318959":"# Top 20 sparse features, mainly labs results \npd.Series(1 - train_df.count() \/ len(train_df)).sort_values(ascending=False).head(20)","17c29d25":"# Categorical features\n\ncategorical_features=[]\nfor c in train_df.columns:\n    col_type = train_df[c].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        train_df[c] = train_df[c].astype('category')\n        categorical_features.append(c)\nprint (categorical_features)","a85d19aa":"# Target distribution analysis\nfig, ax =plt.subplots(1,2)\n\n\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(3,4))\nsns.set_context(\"paper\", font_scale=1.2)                                                  \nsns.countplot('diabetes_mellitus',data=train_df, ax=ax[0])\ntrain_df['diabetes_mellitus'].value_counts().plot.pie(explode=[0,0.2],autopct='%1.2f%%',ax=ax[1])\nfig.show()\n\n# it's a slightly imbalanced datast  ","42633a50":"sns.displot(data = train_df, kind = 'hist', x = 'age', hue = 'diabetes_mellitus', multiple = 'stack',bins=25,height = 4, aspect = 1.7)\n\n# The higher the glucose_max the higher precented of patients with diabetes ","18cf274a":"sns.displot(data = train_df, kind = 'hist', x = 'd1_glucose_max', hue = 'diabetes_mellitus', multiple = 'stack',bins=25,height = 4, aspect = 1.7)\n\n# The higher the glucose_max the higher precented of patients with diabetes (especially >250 range)","f3b3653f":"sns.displot(data = train_df, kind = 'hist', x = 'icu_id', hue = 'diabetes_mellitus', multiple = 'stack',bins=25,height = 4, aspect = 1.7)\n\n# Several ICU units have a higher precentage of patients with diabetes ","4f23cbb9":"sns.displot(data = train_df, kind = 'hist', x = 'bmi', hue = 'diabetes_mellitus', multiple = 'stack',bins=25,height = 4, aspect = 1.7)\n\n# The distibution of BMI in diabetes patiend is slightly more skewed to the right (higher BMI scores)","8488cc00":"plt.style.use('fivethirtyeight')\nplt.figure(figsize=(3,4))\nsns.set_context(\"paper\", font_scale=1.2)   \nsns.factorplot(x=\"ethnicity\", y=\"bmi\", hue=\"diabetes_mellitus\", data=train_df, kind=\"bar\",height = 4, aspect = 2.3)\n\n# Ethnicity and BMI combination is very significant in Caucasians and Native Americans ","bf2d7c90":"plt.style.use('fivethirtyeight')\nsns.set_context(\"paper\", font_scale=1.1)   \ngcbest = [\"#0099ff\", \"#ff4d4d\"]\nsns.set_palette(gcbest,20)\nsns.factorplot(x=\"icu_type\", y=\"diabetes_mellitus\", data=train_df, kind=\"bar\",height = 4, aspect = 2.3)\n\n# CSICU (Cardiac Surgery ICU), CTICU (Cardio-Thoracic) and MICU (General medical ICU) have the highest precentage of patients with diabetes \n# while Neuro ICU have the lowest precentage. ","42cbd09f":"features=train_df.columns.to_list()\nprint ('Number of features ', len(features))","de59c36f":"features_remove=['Unnamed: 0','encounter_id','diabetes_mellitus']\nfor f in features_remove:\n    features.remove(f)","e2efebda":"X=train_df[features]\ny=train_df['diabetes_mellitus']","ba01d297":"from sklearn.base import clone\nfrom sklearn.model_selection import cross_val_score\nfrom bayes_opt import BayesianOptimization\n\nclass BayesianSearchCV:\n    '''\n    Bayesian Search with cross validation score.\n    \n    Arguments:\n    \n    base_estimator: sklearn-like model\n    param_bounds: dict\n        hyperparameter upper and lower bounds\n        example: {\n            'param1': [0, 10],\n            'param2': [-1, 2],\n        }\n    scoring: string or callable\n        scoring argument for cross_val_score\n    cv: int\n        number of folds\n    n_iter: int\n        number of bayesian optimization iterations\n    init_points: int\n        number of random iterations before bayesian optimization\n    random_state: int\n        random_state for bayesian optimization\n    int_parameters: list\n        list of parameters which are required to be of integer type\n        example: ['param1', 'param3']\n    '''\n    \n    def __init__(\n        self,\n        base_estimator,\n        param_bounds,\n        scoring,\n        cv=5,\n        n_iter=50,\n        init_points=10,\n        random_state=1,\n        int_parameters=[],\n    ):\n        self.base_estimator = base_estimator\n        self.param_bounds = param_bounds\n        self.cv = cv\n        self.n_iter = n_iter\n        self.init_points = init_points\n        self.scoring = scoring\n        self.random_state = random_state\n        self.int_parameters = int_parameters\n    \n    def objective(self, **params):\n        '''\n        We will aim to maximize this function\n        '''\n        # Turn some parameters into ints\n        for key in self.int_parameters:\n            if key in params:\n                params[key] = int(params[key])\n        # Set hyperparameters\n        self.base_estimator.set_params(**params)\n        # Calculate the cross validation score\n        cv_scores = cross_val_score(\n            self.base_estimator,\n            self.X_data,\n            self.y_data,\n            cv=self.cv,\n            scoring=self.scoring)\n        score = cv_scores.mean()\n        return score\n    \n    def fit(self, X, y):\n        self.X_data = X\n        self.y_data = y\n        \n        # Create the optimizer\n        self.optimizer = BayesianOptimization(\n            f=self.objective,\n            pbounds=self.param_bounds,\n            random_state=self.random_state,\n        )\n        \n        # The optimization itself goes here:\n        self.optimizer.maximize(\n            init_points=self.init_points,\n            n_iter=self.n_iter,\n        )\n        \n        del self.X_data\n        del self.y_data\n        \n        # Save best score and best model\n        self.best_score_ = self.optimizer.max['target']\n        self.best_params_ = self.optimizer.max['params']\n        for key in self.int_parameters:\n            if key in self.best_params_:\n                self.best_params_[key] = int(self.best_params_[key])\n        \n        self.best_estimator_ = clone(self.base_estimator)\n        self.best_estimator_.set_params(**self.best_params_)\n        self.best_estimator_.fit(X, y)\n        \n        return self\n    \n    def predict(self, X):\n        return self.best_estimator_.predict(X)\n    \n    def predict_proba(self, X):\n        return self.best_estimator_.predict_proba(X)","9c1654a1":"model = lgb.LGBMClassifier()\n\n# Set only upper and lower bounds for each parameter\nparam_grid = {\n    'learning_rate': (0.001, 0.1),\n    'n_estimators': (10, 1000),\n    'num_leaves': (1, 300),\n    'reg_alpha': (0, 1),\n    'reg_lambda': (0, 10),\n    'min_split_gain': (0, 1),\n    'feature_fraction': (0.1, 0.9),\n    'bagging_fraction': (0.8, 1),\n    'min_child_weight': (5, 50),\n    'lambda_l2': (0, 3),\n    'min_split_gain': (0.0001, 0.1),\n\n    \n}\n\nmodel_bayes = BayesianSearchCV(\n    model, param_grid, cv=6, n_iter=50, scoring='accuracy',\n    int_parameters=['n_estimators', 'num_leaves'])\n\nmodel_bayes.fit(X, y)","95776fe5":"def get_best_score(model):\n    print(model.best_score_)\n    print(model.best_params_)\n    print(model.best_estimator_)\n    return model.best_score_\nget_best_score(model_bayes)","4c75f5a4":"params = {\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric': 'auc',\n        'learning_rate': 0.007,\n        'subsample': 1,\n        'colsample_bytree': 0.25,\n        'reg_alpha': 3,\n        'reg_lambda': 1,\n        'scale_pos_weight': 3,\n        'n_estimators': 10000,\n        'verbose': 1,\n        'max_depth': -1,\n        'seed':100, \n        'force_col_wise': True\n\n}\n\nclf,arr_f1_weighted,arr_f1_macro,arr_f1_positive,prediction_folds,preds_folds,y_folds= oc.cv(X,y,0.5,16000,shuffle=True,params=params)","a120370e":"oc.cv_plot(arr_f1_weighted,arr_f1_macro,arr_f1_positive,'WIDS2021 Kaggle compatition')\n","89e84891":"print(classification_report(y_folds, prediction_folds))","6f55dc44":"oc.roc_curve_plot(y_folds,preds_folds)\n","266676aa":"oc.confusion_matrix_plot(y_folds,prediction_folds)","28dffbe7":"feature_imp_list=oc.plot_imp(clf,X,'LightGBM Mortality Kaggle',num=20)","8a591d01":"top_features=feature_imp_list.sort_values(by='Value', ascending=False).head(20)\ntop_features\n","4bd5a27e":"list_for_correlations=top_features['Feature'].to_list()\nlist_for_correlations.append('diabetes_mellitus')\noc.correlations(train_df,list_for_correlations)\n\n# we can see that all three glucose_max\/min,apache and h1_max have a positive correleation to diabetes as expected and interestingly that d1_hemaglobin_max has opposite correlation.","c60edd50":"def Kaggle_submission(file_name,model,test_data,ids_list):\n    if TARGET in test_data.columns:\n        test_data.drop([TARGET],axis=1,inplace=True)\n    #test_pred=model.predict(test_data[features])[:,1]\n    test_pred=model.predict(test_data[features])\n    print (test_pred[1:2])\n\n    submit=pd.DataFrame()\n    submit['encounter_id'] = ids_list\n    submit['diabetes_mellitus'] = test_pred\n    submit.to_csv(file_name,index=False)\n    return submit","cbf22462":"# Categorical features on testset\n\ncategorical_features=[]\nfor c in test_df.columns:\n    col_type = train_df[c].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        test_df[c] = test_df[c].astype('category')\n        categorical_features.append(c)\nprint (categorical_features)\n\nTARGET=\"diabetes_mellitus\"\nsubmit=Kaggle_submission(\"LGBM_baseline_v15.csv\",clf,test_df,test_df['encounter_id'].tolist())","09e8e156":"submit.head(20)","41bf222c":"#!kaggle competitions submit -c widsdatathon2021 -f LGBM_baseline.csv -m \"inital model\"\n","ab1d4e1a":"## OCTOPUS-ML functions\n[https:\/\/github.com\/gershonc\/octopus-ml](https:\/\/github.com\/gershonc\/octopus-ml)","1360ae3d":"### Mimissing data analysis ","e1bdf651":"## EDA","c024f6be":"## Data pre-processing ","131289b3":"\n<html>\n<h2 style=\"color:#1E90FF ; font-weight: bold; font-family: Roboto, sans-serif; text-align:center; font-size:24px\"><b>WIDS Datathon 2021 - initial EDA and Model  <\/h2>\n<\/html>","85c1d4ce":"## Load the datasets  ","3d363fa9":"## Model Training (LightGBM)","d10bf72f":"## HPO","57d70dd0":"## Model evaluation\n","fe2f94b9":"## Test Submission "}}