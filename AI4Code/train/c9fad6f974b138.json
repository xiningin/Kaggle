{"cell_type":{"0a3f7e64":"code","0efda4a8":"code","ab86aac7":"code","2f6775db":"code","96d030fa":"code","0d5dc03b":"code","8587fc85":"code","4dc34f53":"code","b39e3192":"code","cf84da2a":"code","27894148":"code","17a69ade":"code","fbe2008b":"code","ef96bf5a":"code","a7e93030":"code","0498d058":"code","ce148f62":"code","64bd2038":"code","9a71feaf":"code","86470ffb":"code","c89ea180":"code","d71c6bba":"code","e98fde67":"code","ea506102":"code","0e6327e2":"code","384cdd6b":"code","375ab501":"code","ec33e43f":"code","0bceb6ee":"code","157afc19":"markdown","fac01e5e":"markdown","ae893e8c":"markdown","f994d73a":"markdown","78aa7c40":"markdown","80428908":"markdown","9cdddb3e":"markdown","71eced3e":"markdown","fefca8cd":"markdown","0ae15a4e":"markdown","bf332859":"markdown","29605993":"markdown","e061b445":"markdown","2e786114":"markdown","5964ac75":"markdown","6cd127e5":"markdown","eb4838af":"markdown","41feaea0":"markdown","d712df97":"markdown","a272cdf3":"markdown","a8760971":"markdown","5b2c82e1":"markdown"},"source":{"0a3f7e64":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport os\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\nimport re\nimport nltk\nimport sys\n\nfrom IPython.display import clear_output\n\nprint(\"Importing Complete!\")","0efda4a8":"#importing the training data\nimdb_data = pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')\nprint(imdb_data.shape)\nimdb_data.head(10)\n\n","ab86aac7":"nltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom bs4 import BeautifulSoup\nimport pickle","2f6775db":"imdb_data_len = len(imdb_data.iloc[:, 0])\nall_stopwords = stopwords.words('english')\nall_stopwords.remove('not')\n\nps = PorterStemmer()\n\n\n# new list with cleaned data\ncorpus = []\nfor i in range(imdb_data_len): # this can also be written as imdb_data['review']\n  review = BeautifulSoup(imdb_data['review'][i], \"html.parser\").get_text()\n  review = re.sub('\\[[^]]*\\]',' ',review)\n  review = re.sub('[^a-zA-z0-9]',' ',review)\n  review = review.lower()\n  review = review.split()\n\n  review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n  review = ' '.join(review)\n\n  corpus.append(review)\n    \nprint(\"Done cleaning the data!\")","96d030fa":"pickle.dump(corpus, open( \".\/imdb_cleaned_reviews.p\", \"wb\" ))","0d5dc03b":"corpus = pickle.load(open( \".\/imdb_cleaned_reviews.p\", \"rb\" ))","8587fc85":"print(corpus[0])","4dc34f53":"cv = CountVectorizer(max_features= 1500)\n\nx = cv.fit_transform(corpus).toarray()\ny = imdb_data.iloc[:,1].values","b39e3192":"print(len(x[0]))","cf84da2a":"#Tfidf vectorizer\ntv=TfidfVectorizer(max_features=1500)\nx_tv=tv.fit_transform(corpus).toarray()","27894148":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=0)\nx_tv_train, x_tv_test, y_tv_train, y_tv_test = train_test_split(x_tv, y, test_size= 0.2, random_state=0)","17a69ade":"classifier = GaussianNB()\nclassifier.fit(x_train, y_train)\n\nclassifier_tv = GaussianNB()\nclassifier_tv.fit(x_tv_train, y_tv_train)","fbe2008b":"y_pred = classifier.predict(x_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","ef96bf5a":"y_pred = classifier.predict(x_test)\ncm = confusion_matrix(y_test, y_pred)\nprint(cm)\nprint(accuracy_score(y_test, y_pred))\n\ny_tv_pred = classifier_tv.predict(x_tv_test)\ncm_tv = confusion_matrix(y_tv_test, y_tv_pred)\nprint(cm_tv)\nprint(accuracy_score(y_tv_test, y_tv_pred))","a7e93030":"#Classification report for bag of words \nnb_bow_report=classification_report(y_test,y_pred,target_names=['Positive','Negative'])\nprint(nb_bow_report)\n\n#Classification report for tfidf features\nnb_tv_report=classification_report(y_tv_test,y_tv_pred,target_names=['Positive','Negative'])\nprint(nb_tv_report)","0498d058":"from sklearn.svm import SVC\nclassifier_svm = SVC(kernel= 'rbf', random_state= 0, max_iter=5, probability=True)\nclassifier_svm.fit(x_train, y_train)\n\nclassifier_svm_tv = SVC(kernel= 'rbf', random_state= 0, max_iter=5, probability=True)\nclassifier_svm_tv.fit(x_tv_train, y_tv_train)","ce148f62":"y_pred_svm = classifier_svm.predict(x_test)\ncm = confusion_matrix(y_test, y_pred_svm)\nprint(cm)\nprint(accuracy_score(y_test, y_pred_svm))\n\ny_tv_pred_svm = classifier_tv.predict(x_tv_test)\ncm_tv = confusion_matrix(y_tv_test, y_tv_pred_svm)\nprint(cm_tv)\nprint(accuracy_score(y_tv_test, y_tv_pred_svm))","64bd2038":"#Classification report for bag of words \nsvm_bow_report=classification_report(y_test,y_pred_svm,target_names=['Positive','Negative'])\nprint(svm_bow_report)\n\n#Classification report for tfidf features\nsvm_tv_report=classification_report(y_tv_test,y_tv_pred_svm,target_names=['Positive','Negative'])\nprint(svm_tv_report)","9a71feaf":"accuracy = 0\ntotal = len(x_tv_test)\nfor i in range(total):\n  if classifier_tv.predict(x_tv_test[i].reshape(1,-1)) == y_test[i]:\n    accuracy+=1\n  elif classifier_svm_tv.predict(x_tv_test[i].reshape(1,-1)) == y_test[i]:\n    accuracy+=1\n\naccuracy = accuracy\/total*100\n\nprint(accuracy)","86470ffb":"from sklearn.linear_model import LogisticRegression\nclassifier_LR = LogisticRegression(max_iter=500,random_state = 0)\nclassifier_LR.fit(x_train, y_train)\n\nclassifier_tv_LR = LogisticRegression(max_iter=500,random_state = 0)\nclassifier_tv_LR.fit(x_tv_train, y_tv_train)","c89ea180":"y_pred_LR = classifier_LR.predict(x_test)\ncm = confusion_matrix(y_test, y_pred_LR)\nprint(cm)\nprint(accuracy_score(y_test, y_pred_LR))\n\ny_tv_pred_LR = classifier_tv_LR.predict(x_tv_test)\ncm_tv = confusion_matrix(y_tv_test, y_tv_pred_LR)\nprint(cm_tv)\nprint(accuracy_score(y_tv_test, y_tv_pred_LR))","d71c6bba":"#Classification report for bag of words \nLR_bow_report=classification_report(y_test,y_pred_LR,target_names=['Positive','Negative'])\nprint(LR_bow_report)\n\n#Classification report for tfidf features\nLR_tv_report=classification_report(y_tv_test,y_tv_pred_LR,target_names=['Positive','Negative'])\nprint(LR_tv_report)","e98fde67":"from sklearn.ensemble import RandomForestClassifier\nclassifier_tv_rf = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\nclassifier_tv_rf.fit(x_tv_train, y_tv_train)","ea506102":"y_tv_pred_rf = classifier_tv_rf.predict(x_tv_test)\ncm_tv = confusion_matrix(y_tv_test, y_tv_pred_rf)\nprint(cm_tv)\nprint(accuracy_score(y_tv_test, y_tv_pred_rf))","0e6327e2":"accuracy = 0\ntotal = len(x_tv_test)\nfor i in range(total):\n  if classifier_tv.predict(x_tv_test[i].reshape(1,-1)) == y_test[i]:\n    accuracy+=1\n  elif classifier_svm_tv.predict(x_tv_test[i].reshape(1,-1)) == y_test[i]:\n    accuracy+=1\n  elif classifier_tv_LR.predict(x_tv_test[i].reshape(1,-1)) == y_test[i]:\n    accuracy+=1\n  elif classifier_tv_rf.predict(x_tv_test[i].reshape(1,-1)) == y_test[i]:\n    accuracy+=1\n\naccuracy = accuracy\/total*100\n\nprint(accuracy)","384cdd6b":"def newPredict(arr):\n  lr = classifier_tv_LR.predict_proba(arr)[0][0] # % negative\n  svm = classifier_svm_tv.predict_proba(arr)[0][0]\n  rf = classifier_tv_rf.predict_proba(arr)[0][0]\n\n  if (lr + svm + rf)\/3 >= 0.5:\n    return 'negative'\n  else:\n    return 'positive'\n\naccuracy = 0\ntotal = len(x_tv_test)\n\nfor i in range(total):\n  if newPredict(x_tv_test[i].reshape(1,-1)) == y_test[i]:\n    accuracy+=1\n\naccuracy = accuracy\/total*100\n\nprint(accuracy)","375ab501":"imdb_data_len = x_tv.shape[:][0]\nreviewLength = x_tv.shape[:][1]\n\npositive_list = [0] * reviewLength\nnegative_list = [0] * reviewLength\n\npositive_amount = 0\nnegative_amount = 0\n\ndef addVectorizedReviewRow(lst, reviewLst, reviewLength):\n  for i in range(reviewLength):\n    lst[i] = lst[i] + reviewLst[i]\n  return lst\n\ndef normalizeVectorizedReviewRow(lst, reviewLength, amount):\n  for i in range(reviewLength):\n    lst[i] = lst[i] \/ amount\n  return lst\n\nfor i in range(imdb_data_len):\n  if y[i] == 'positive':\n    positive_list = addVectorizedReviewRow(positive_list, x_tv[i], reviewLength)\n    positive_amount += 1\n  else:\n    negative_list = addVectorizedReviewRow(negative_list,x_tv[i], reviewLength)\n    negative_amount += 1\n\npositive_list = normalizeVectorizedReviewRow(positive_list, reviewLength, positive_amount)\nnegative_list = normalizeVectorizedReviewRow(negative_list, reviewLength, negative_amount)","ec33e43f":"bow=pd.DataFrame(positive_list, tv.get_feature_names())\npositive_text=bow.sort_values(by=0, ascending=False).iloc[:50]\n\nfont = {'size'   : 20}\n\nplt.rc('font', **font)\n\nfig, ax = plt.subplots(figsize=(20, 10))\n\nax.bar(positive_text.index.values, positive_text.T.iloc[:].values[0])\nplt.xticks(rotation=75)\nplt.xlabel(\"Stemmed Word\")\nplt.ylabel(\"Word_Frequency(Normalized)\")\nplt.show()\n","0bceb6ee":"bow=pd.DataFrame(negative_list, tv.get_feature_names())\nnegative_text=bow.sort_values(by=0, ascending=False).iloc[:50]\n\nfont = {'size'   : 20}\n\nplt.rc('font', **font)\n\nfig, ax = plt.subplots(figsize=(20, 10))\n\nax.bar(negative_text.index.values, negative_text.T.iloc[:].values[0])\nplt.xticks(rotation=75)\nplt.xlabel(\"Stemmed Word\")\nplt.ylabel(\"Word_Frequency(Normalized)\")\nplt.show()","157afc19":"### Coverage of the models over the test data if we used both models in a perfect world","fac01e5e":"## Importing the Libraries","ae893e8c":"### We save the cleaned reviews into a pickle file so we don't have to clean it every time we run the code","f994d73a":"## Training the Naive Bayes model on the training set","78aa7c40":"## Training other models: Logistic Regression","80428908":"I did not use the Naive Bayes model as it did not benefit in any way to the ensemble method.\n\nThe ensemble method using the average of the 3 models prediction ratio's did not improve the prediction accuracy compared to the logistic regression.\n","9cdddb3e":"## Making the Confusion matrix","71eced3e":"## Making use of 4 Models","fefca8cd":"### New prediction method making use of the 3 models in an ensemble algorithm","0ae15a4e":"### Coverage of the models over the test data if we used 4 models in a perfect world","bf332859":"## Making use of both Models","29605993":"## Cleaning the Text","e061b445":"## Training other models: Random Forest","2e786114":"## Reviewing the model","5964ac75":"## Creating the bag of words model","6cd127e5":"## Splitting the data in training and test set","eb4838af":"## Training other models: Support Vector Machine (RBF)","41feaea0":"## Word frequency for positive review words","d712df97":"## Predicting the test set results","a272cdf3":"# Natural Language Processing: Sentiment Analysis (IMDB reviews)","a8760971":"## Word frequency for negative review words","5b2c82e1":"## Importing the  Dataset"}}