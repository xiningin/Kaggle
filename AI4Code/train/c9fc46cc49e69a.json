{"cell_type":{"1c3b6486":"code","d6fae0d8":"code","b169058c":"code","1c09f470":"code","c8e0f6a4":"code","6f85ad52":"code","91e50b65":"code","ca1ec10a":"code","008bf34c":"code","ff02173b":"code","b2b2afee":"code","7bf39b3d":"code","744b4529":"code","8a542f84":"code","81d1676b":"code","fd1bd702":"code","815e7fa0":"code","3e73d333":"code","c1cf205b":"code","1dec3f44":"code","0e78190c":"code","a64c8788":"code","09bf91a7":"code","95984bdb":"code","7a1877c9":"code","3358f1ff":"code","94793dc7":"code","fb0c0ec0":"code","c2f3974b":"code","591460ef":"code","9053fcea":"code","8402d699":"code","7c651a2b":"code","2059f20c":"code","de7a9849":"code","2be103ff":"code","22aa803b":"code","308c8020":"code","d678219b":"code","832d644c":"code","0b93d62f":"code","de3253c1":"code","6c6060bc":"code","9de3b824":"code","deb66679":"code","46033fea":"code","017b6291":"code","9f8ec1e3":"code","71ae4567":"code","e8c27f96":"code","a24e70f1":"code","7483ec62":"code","8d2a84a4":"code","954c7d76":"code","ac954ef7":"code","7a443e48":"code","4565f4b3":"code","d0739410":"code","486ae2fc":"code","4c5a2b4d":"code","ba88fefe":"markdown","3e737a33":"markdown","1896199b":"markdown","8d3bd13f":"markdown","dff71035":"markdown"},"source":{"1c3b6486":"# Data manipulation and plotting modules\nimport numpy as np\nimport pandas as pd","d6fae0d8":"# Data pre-processing\n# z = (x-mean)\/stdev\nfrom sklearn.preprocessing import StandardScaler as ss","b169058c":"# Dimensionality reduction\nfrom sklearn.decomposition import PCA","1c09f470":"# Data splitting and model parameter search\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV","c8e0f6a4":"# Modeling modules\nfrom xgboost.sklearn import XGBClassifier","6f85ad52":"# Model pipelining\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline","91e50b65":"# Model evaluation metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import auc, roc_curve\nfrom sklearn.metrics import confusion_matrix","ca1ec10a":"# Modules for graph plotting\nimport matplotlib.pyplot as plt\nfrom xgboost import plot_importance\nimport seaborn as sns","008bf34c":"# Needed for Bayes optimization\n# Takes an estimator, performs cross-validation and gives out average score\nfrom sklearn.model_selection import cross_val_score","ff02173b":"# Baynesian Optimization Module\n# Refer: https:\/\/github.com\/fmfn\/BayesianOptimization\nfrom bayes_opt import BayesianOptimization","b2b2afee":"# Importing some supporting modules\nimport time\nimport random\nfrom scipy.stats import uniform","7bf39b3d":"# Data has 6498 rows including header and 13 columns.\n# Assuming it is very big data size and we cannot process it all\n# and so we have to make a model on data sample we will import only 90% of the data.\ndata = pd.read_csv(\n         \"..\/input\/winequalityN.csv\",\n         header = 0,   # First row is header-row\n         # 'and' operator returns True if both values are True\n         #  random.random() returns values between (0,1)\n         #  No of rows skipped will be around 60% of total\n         skiprows = lambda i: (i>0) and (random.random() > 0.9)\n         )","744b4529":"# Explore Data\nf\"Data Shape : { data.shape }\"","8a542f84":"print(\"\\033[1mColumns in this dataset\\033[0m\\n\\n\", \"\\n\".join(data.columns.values))","81d1676b":"print(\"\\033[1mData Types\\033[0m\\n\\n\",data.dtypes.value_counts())","fd1bd702":"print(\"\\033[1mData Types\\033[0m\\n\\n\",data.dtypes)","815e7fa0":"# Converting data type of quality column to float.\ndata[\"quality\"] = data[\"quality\"].astype(float)","3e73d333":"# Data Contents\ndata.head(10)","c1cf205b":"data.describe()","1dec3f44":"# Check any null values \/ NAN\ndata.isnull().any()","0e78190c":"data.isnull().sum()","a64c8788":"# Fill NaN by the most frequntly occuring number, we can choose mean also but for this example will go for mode.\ndf = data.isnull().any().reset_index()\ndf","09bf91a7":"na_columns = df.loc[df.iloc[:, 1] == True, \"index\"].tolist()\nprint(\"\\033[1mColumns list:\\033[0m \\n\\n{0}\".format(\"\\n\".join(na_columns)))","95984bdb":"# Getting mode for the columns in the list na_columns\nvalues = dict(map(lambda i: (i, float(data[i].mode())), na_columns))\nmaxlen = max([len(i) for i in values.keys()])\nprint(\"\\033[1mMode Values:\\033[0m \\n\")\nfor key, value in values.items():\n    print(key, \" \" * (maxlen - len(key) + 5), \"{:.2}\".format(value))","7a1877c9":"# Replacing Null \/ NaN values in data set.\ndata = data.fillna(value = values)","3358f1ff":"# Validating if any null values left\ndata.isnull().any()","94793dc7":"# Pairplot for some of the data feautres\nsns.pairplot(data.iloc[:, 4:10])","fb0c0ec0":"# Plotting correlation table\nfig = plt.figure(figsize=(20,15))\nsns.heatmap(data.corr(), annot=True, annot_kws={\"size\": 14})","c2f3974b":"# Divide data into predictors and target X is predictors and Y is target.\nX = data.drop(\"type\", axis=1)\nX.head()","591460ef":"# Y is type column\ny = data[\"type\"]\ny.value_counts()","9053fcea":"# Mapping white lable by 1 and red by 0\ny = y.map({'white': 1, 'red': 0})\ny.value_counts()","8402d699":"# Split dataset into train and validation parts\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, shuffle = True)","7c651a2b":"# Check training and test data set shape\nprint(\"X_train :\", X_train.shape)\nprint(\"X_test  :\", X_test.shape)\nprint(\"y_train :\", y_train.shape)\nprint(\"y_test  :\", y_test.shape)","2059f20c":"################# CC. Create pipeline #################\n#### Pipe using XGBoost\n\n# 5 Pipeline steps\n# steps: List of (name, transform) tuples\n#       (implementing fit\/transform) that are\n#       chained, in the order in which they\n#       are chained, with the last object an\n#       estimator.\n#      Format: [(name, transformer), (name, transformer)..(name, estimator)]\nsteps_xg = [('sts', ss()), ('pca', PCA()), ('xg',  XGBClassifier(silent = False, n_jobs=1))]\n\n# Instantiate Pipeline object\npipe_xg = Pipeline(steps_xg)","de7a9849":"##################### Grid Search #################\n\n# 6.  Specify xgboost parameter-range\n# 6.1 Dictionary of parameters (16 combinations)\n#     Syntax: {\n#              'transformerName_parameterName' : [ <listOfValues> ]\n#              }\n#\nparameters = {'xg__learning_rate':  [0.1, 0.3, 0.5, 0.8], # learning rate decides what percentage\n                                                  #  of error is to be fitted by\n                                                  #   by next boosted tree.\n                                                  # See this answer in stackoverflow:\n                                                  # https:\/\/stats.stackexchange.com\/questions\/354484\/why-does-xgboost-have-a-learning-rate\n                                                  # Coefficients of boosted trees decide,\n                                                  #  in the overall model or scheme, how much importance\n                                                  #   each boosted tree shall have. Values of these\n                                                  #    Coefficients are calculated by modeling\n                                                  #     algorithm and unlike learning rate are\n                                                  #      not hyperparameters. These Coefficients\n                                                  #       get adjusted by l1 and l2 parameters\n              'xg__n_estimators':   [50, 65, 85, 100],  # Number of boosted trees to fit\n                                                  # l1 and l2 specifications will change\n                                                  # the values of coeff of boosted trees\n                                                  # but not their numbers\n              'xg__max_depth':      [2, 3, 5, 7],\n              'pca__n_components' : [0.3, 0.5, 0.7, 0.9]}","2be103ff":"#    Grid Search iterations\n#    Create Grid Search object first with all necessary\n#    specifications. Note that data, X, as yet is not specified\n#    Pipeline avoids data leakage during GridSearch\n#    See this: https:\/\/towardsdatascience.com\/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976\nclf = GridSearchCV(pipe_xg,            # pipeline object\n                   parameters,         # possible parameters\n                   n_jobs = 1,         # USe parallel cpu threads\n                   cv =5 ,             # No of folds\n                   verbose = 1,         # Higher the value, more the verbosity\n                   scoring = ['accuracy', 'roc_auc'],  # Metrics for performance\n                   refit = 'roc_auc'   # Refitting final model on what parameters? those which maximise auc\n                   )","22aa803b":"# Start fitting data to pipeline\nclf.fit(X_train, y_train)","308c8020":"# Best Score and best parameters\nprint(\"Best score :\", clf.best_score_ * 100)\nprint(\"Best parameter set :\", clf.best_params_)","d678219b":"# Making predictions\ny_pred = clf.predict(X_test)","832d644c":"# Check Accuracy\naccuracy = accuracy_score(y_test, y_pred)\nf\"Accuracy: {accuracy * 100.0}\"","0b93d62f":"# Confusion matrix\ncm = confusion_matrix( y_test,y_pred)\nsns.heatmap(cm, annot=True, fmt='g', annot_kws={\"size\": 14})","de3253c1":"tn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()\nprint(\"True Positives : \", tp)\nprint(\"True Negatives : \", tn)\nprint(\"False Positives : \", fp)\nprint(\"False Negatives : \", fn)","6c6060bc":"# probbaility of occurrence of each class\ny_pred_prob = clf.predict_proba(X_test)\nprint(\"y_pred_prob shape : \", y_pred_prob.shape)","9de3b824":"# Probability values in y_pred_prob are ordered\n# column-wise, as:\nclf.classes_","deb66679":"# Draw ROC curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[: , 0], pos_label = 0)","46033fea":"# Plot the ROC curve\nfig = plt.figure(figsize=(10,10))          # Create window frame\nax = fig.add_subplot(111)   # Create axes\nax.plot(fpr, tpr)           # Plot on the axes\n# Also connect diagonals\nax.plot([0, 1], [0, 1], ls=\"--\")   # Dashed diagonal line\n# Labels etc\nax.set_xlabel('False Positive Rate')  # Final plot decorations\nax.set_ylabel('True Positive Rate')\nax.set_title('ROC curve for Higgs Boson particle')\n\nax.set_xlim([0.0, 1.0])\nax.set_ylim([0.0, 1.0])\nplt.show()","017b6291":"# AUC\nauc(fpr,tpr)","9f8ec1e3":"##################### EE. Randomized Search #################\n\n# Tune parameters using randomized search\n# Hyperparameters to tune and their ranges\nparameters = {'xg__learning_rate':  uniform(0, 1),\n              'xg__n_estimators':   range(50,100),\n              'xg__max_depth':      range(2,7),\n              'pca__n_components' : range(5, 10)}","71ae4567":"# Tune parameters using random search\n# Create the object first\nrs = RandomizedSearchCV(pipe_xg,\n                        param_distributions=parameters,\n                        scoring= ['roc_auc', 'accuracy'],\n                        n_iter=15,          # Max combination of parameter to try. Default = 10\n                        verbose = 3,\n                        refit = 'roc_auc',\n                        n_jobs = 4,          # Use parallel cpu threads\n                        cv = 7               # No of folds, so n_iter * cv combinations\n                        )","e8c27f96":"# Run random search for 25 iterations.\nrs.fit(X_train, y_train)","a24e70f1":"# Evaluation\nprint(\"Best score: \", rs.best_score_)\nprint(\"Best parameter set: \", rs.best_params_)","7483ec62":"# Make predictions\ny_pred = rs.predict(X_test)","8d2a84a4":"# Accuracy check\naccuracy = accuracy_score(y_test, y_pred)\nf\"Accuracy: {accuracy * 100.0}\"","954c7d76":"# Confusion matrix\ncm = confusion_matrix( y_test,y_pred)\nsns.heatmap(cm, annot=True, fmt='g', annot_kws={\"size\": 14})","ac954ef7":"tn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()\nprint(\"True Positives : \", tp)\nprint(\"True Negatives : \", tn)\nprint(\"False Positives : \", fp)\nprint(\"False Negatives : \", fn)","7a443e48":"\"\"\"\nStep 1: Define BayesianOptimization function.\n    It broadly acts as follows\"\n    s1. Gets a dictionary of parameters that specifies\n        possible range of values for each one of\n        the parameters. [Our set: para_set ]\n    s2. Picks one value for each one of the parameters\n        (from the specified ranges as in (s1)) evaluate,\n        a loss-function that is given to it, say,\n        accuracy after cross-validation.\n        [Our function: xg_eval() ]\n    s3. Depending upon the value of accuracy returned\n        by the evaluator and also past values of accuracy\n        returned, this function, creates gaussian\n        processes and picks up another set of parameters\n        from the given dictionary of parameters\n    s4. The parameter set is then fed back to (s2) above\n        for evaluation\n    s5. (s2) t0 (s4) are repeated for given number of\n        iterations and then final set of parameters\n        that optimizes objective is returned\n\n\"\"\"\n\n# Which parameters to consider and what is each one's range\npara_set = {\n           'learning_rate':  (0, 1),                 # any value between 0 and 1\n           'n_estimators':   (50,100),               # any number between 50 to 100\n           'max_depth':      (2,7),                 # any depth between 3 to 10\n           'n_components' :  (5,10)                 # any number between 20 to 30\n            }","4565f4b3":"# Create a function that when passed some parameters\n#    evaluates results using cross-validation\n#    This function is used by BayesianOptimization() object\n\ndef xg_eval(learning_rate,n_estimators, max_depth,n_components):\n    \n    # 12.1 Make pipeline. Pass parameters directly here\n    pipe_xg1 = make_pipeline (ss(),                        # Why repeat this here for each evaluation?\n                              PCA(n_components = int(round(n_components))),\n                              XGBClassifier(\n                                           silent = False,\n                                           n_jobs = 4,\n                                           learning_rate = learning_rate,\n                                           max_depth = int(round(max_depth)),\n                                           n_estimators = int(round(n_estimators))\n                                           )\n                             )\n\n    # 12.2 Now fit the pipeline and evaluate\n    cv_result = cross_val_score(estimator = pipe_xg1,\n                                X = X_train,\n                                y = y_train,\n                                cv = 2,\n                                n_jobs = 4,\n                                scoring = 'f1'\n                                ).mean()             # take the average of all results\n    # 12.3 Finally return maximum\/average value of result\n    return cv_result\n","d0739410":"# This is the main workhorse\n#      Instantiate BayesianOptimization() object\n#      This object  can be considered as performing an internal-loop\n#      i)  Given parameters, xg_eval() evaluates performance\n#      ii) Based on the performance, set of parameters are selected\n#          from para_set and fed back to xg_eval()\n#      (i) and (ii) are repeated for given number of iterations\n#\nxgBO = BayesianOptimization(\n                             xg_eval,     # Function to evaluate performance.\n                             para_set     # Parameter set from where parameters will be selected\n                             )","486ae2fc":"# Gaussian process parameters\n#     Modulate intelligence of Bayesian Optimization process\n#     This parameters controls how much noise the GP can handle,\n#     so increase it whenever you think that extra flexibility is needed.\n#     gp_params = {\"alpha\": 1e-5}      # Initialization parameter for gaussian process.\n#\n# Fit\/train (so-to-say) the BayesianOptimization() object\n#     Start optimization. 25minutes\n#     Our objective is to maximize performance (results)\nxgBO.maximize(init_points=5,    # Number of randomly chosen points to\n                                 # sample the target function before\n                                 #  fitting the gaussian Process (gp)\n                                 #  or gaussian graph\n               n_iter=9,        # Total number of times the\n               #acq=\"ucb\",       # ucb: upper confidence bound\n                                 #   process is to be repeated\n                                 # ei: Expected improvement\n               # kappa = 1.0     # kappa=1 : prefer exploitation; kappa=10, prefer exploration\n#              **gp_params\n               )","4c5a2b4d":"# Get values of parameters that maximise the objective\nmax_xgBO = xgBO.max\n\nprint(\"\\033[1mBest Score:\\033[0m \", max_xgBO[\"target\"])\n\n\n\nmaxlen = max([len(i) for i in max_xgBO[\"params\"].keys()])\n\nprint(\"\\n\\n\\033[1mMaximum performing combination:\\033[0m \\n\")\nfor key, value in max_xgBO[\"params\"].items():\n    print(key, \" \" * (maxlen - len(key) + 5), value)\n\n","ba88fefe":"# Randomized Search Section","3e737a33":"# Bayes Optimization","1896199b":" # Ending this Kernel here","8d3bd13f":"# Problem Introduction\n\nThis problem is from Kaggle. The data is about types of wine, its characteristics and wine quality.\n\nIt is a famous dataset. It is placed below.\n\nWine has a number of attributes depending upon its type. Some of the attributes are:\n\ni) fixed acidity ii) volatile acidity iii) citric acid iv) residual sugar v) chlorides vi) free sulfur dioxide  vii) total sulfur dioxide viii) density ix) pH, x) sulphates, xi) alcohol, xii) quality\n\nThe target variable in the dataset is 'type'. It has two values: white and red. All features are numeric.\n\nObjective is to, \n\nCreate a model using xgboost and tune its parameters either using Grid Search or Random Search or using Bayesian optimization (or all three), in this example we will go for all three of them.\n","dff71035":"# Grid Search Section"}}