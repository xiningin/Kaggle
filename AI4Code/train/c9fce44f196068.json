{"cell_type":{"479c05e7":"code","b6becb86":"code","c549c8cb":"code","b7e90923":"code","fe0c85ee":"code","faa1e29d":"code","7d5bc823":"code","ffc1aa7a":"code","dfdeaf8d":"code","01f9d120":"code","d141b857":"code","34d0cff1":"code","9e8739f2":"code","1740ef07":"markdown","a0abe900":"markdown","97e02c05":"markdown","edbb0ca6":"markdown","aab3cf4e":"markdown","c8ff8579":"markdown","f2383566":"markdown","fed7c17c":"markdown","e0ee7380":"markdown","64156f67":"markdown","9e4521de":"markdown","ae923b74":"markdown","364cc884":"markdown","142b15ae":"markdown","5a3537bf":"markdown"},"source":{"479c05e7":"import pandas as pd\nimport numpy as np\n\nfrom sklearn import preprocessing #needed for scaling attributes to the nterval [0,1]\n\nfrom sklearn import svm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFE\n\nfrom sklearn.model_selection import train_test_split","b6becb86":"df = pd.read_csv('..\/input\/seeds-dataset-binary\/seeds_dataset_binary.csv')\ndf.describe()","c549c8cb":"# target attribute\ntarget_attribute_name = 'type'\ntarget = df[target_attribute_name]\n\n# predictor attributes\npredictors = df.drop(target_attribute_name, axis=1).values","b7e90923":"# pepare independent stratified data sets for training and test of the final model\npredictors_train, predictors_test, target_train, target_test = train_test_split(predictors, target, test_size=0.20, shuffle=True, stratify=target)","fe0c85ee":"min_max_scaler = preprocessing.MinMaxScaler()\npredictors_train = min_max_scaler.fit_transform(predictors_train)\npredictors_test = min_max_scaler.fit_transform(predictors_test)","faa1e29d":"# create a base classifier used to evaluate a subset of attributes\nestimatorSVM = svm.SVR(kernel=\"linear\")\nselectorSVM = RFE(estimatorSVM, 3)\nselectorSVM = selectorSVM.fit(predictors_train, target_train)\n# summarize the selection of the attributes\nprint(selectorSVM.support_)\nprint(selectorSVM.ranking_)","7d5bc823":"# create a base classifier used to evaluate a subset of attributes\nestimatorLR = LogisticRegression(solver='lbfgs')\n# create the RFE model and select 3 attributes\nselectorLR = RFE(estimatorLR, 3)\nselectorLR = selectorLR.fit(predictors_train, target_train)\n# summarize the selection of the attributes\nprint(selectorLR.support_)\nprint(selectorLR.ranking_)","ffc1aa7a":"predictors_train_SVMselected = selectorSVM.transform(predictors_train)\npredictors_test_SVMselected = selectorSVM.transform(predictors_test)","dfdeaf8d":"predictors_train_LRselected = selectorLR.transform(predictors_train)\npredictors_test_LRselected = selectorLR.transform(predictors_test)","01f9d120":"classifier = svm.SVC(gamma='auto')","d141b857":"model1 = classifier.fit(predictors_train_SVMselected, target_train)\nmodel1.score(predictors_test_SVMselected, target_test)","34d0cff1":"model2 = classifier.fit(predictors_train_LRselected, target_train)\nmodel2.score(predictors_test_LRselected, target_test)","9e8739f2":"model3 = classifier.fit(predictors_train, target_train)\nmodel3.score(predictors_test, target_test)","1740ef07":"## Apply RFE with SVM for Selecting the Features","a0abe900":"## Evaluate on the Test Dataset","97e02c05":"### Train and evaluate SVM classifiers with both the selected features and all features \n\nHere we train three models:\n* model1 - with the features selected by SVM\n* model2 - with the features selected by Logistic Regression\n* model3 - with all features (i.e. without feature selection)","edbb0ca6":"This notebook builds on Tutoial 4 by introducing feature selection into the process of selecting the best classifier for a binary classification problem. It also demonstrates how to split a dataset into a training and test sets and use the test set for evaluation.\n\nThe feature selection method applied here is Recursive Feature Elimination (RFE) as demonstrated in the tutorial at https:\/\/machinelearningmastery.com\/feature-selection-in-python-with-scikit-learn\/.\n\nIn this demonstration we use a modified version of the seeds dataset (see https:\/\/archive.ics.uci.edu\/ml\/datasets\/seeds), which is the same dataset used in Tutorial 4.","aab3cf4e":"# TUTORIAL 5: FEATURE SELECTION\nby [Nikola S. Nikolov](http:\/\/bdarg.org\/niknikolov)\n\n-----","c8ff8579":"## Apply RFE with Logistic Regression for Selecting Features","f2383566":"# C. Conclusion and Further Work\n\nWhen you execute this code again, it is very likely to get different results.\n\nTo get more accurate results, accounting for the variance in the results, it is better to run the whole experiment multiple times and measure the variance in the results. Then pick the model that gives better results.\n\nThe process outlined in this tutorial can be further authomated with the use of scikit-learn pipelines. As an exercise build at least two pipelines for training classifiers for the seeds dataset. Each pipeline should include a feature-selection method, and the feature-selection method in pipeline 1 should be different from the feature-selection method in pipeline 2.\n\nTo do this follow the examples at:\n\n* https:\/\/machinelearningmastery.com\/automate-machine-learning-workflows-pipelines-python-scikit-learn\/\n* https:\/\/towardsdatascience.com\/a-simple-example-of-pipeline-in-machine-learning-with-scikit-learn-e726ffbb6976\n* https:\/\/ramhiser.com\/post\/2018-03-25-feature-selection-with-scikit-learn-pipeline\/","fed7c17c":"### Apply the selectors to prepare a training dataset only with the selected features.\n\n__Note:__ The same selectors are applied to the test dataset. However, it is important that the test dataset was not used by (it's invisible to) the selectors. ","e0ee7380":"## Import Python Modules","64156f67":"Split the dataset into a training (80%) and test (20%) datasets.","9e4521de":"## Load and Prepare the Dataset for Training and Evaluation","ae923b74":"# A. Preparation","364cc884":"[Continue with Tutorial 6: Clustering and Manifold Learning](https:\/\/www.kaggle.com\/nikniko101v\/tutorial-6-clustering-and-manifold-learning)","142b15ae":"Scale all predictor values to the range [0, 1]. Note the target attribute is already binary. This is a useful pre-processing technique to ensure that all attributes are treated equally during training. Applying a scaler (MinMaxScaler) can be seen as another parameter of the ML to be applied. It may or may not improve the accuracy of the trained model, which can be evaluated with a test dataset. \n\nNote that the MinMaxScaler is applied separately to the training and the testing datasets. \nThis is to ensure that this transformation when performed on the testing dataset is not influnced by the training dataset.","5a3537bf":"# B. Feature Selection"}}