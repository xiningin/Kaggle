{"cell_type":{"f31b179c":"code","efe13a44":"code","f84fbd60":"code","8664067b":"code","4da3ad7a":"code","e5bd6a85":"code","2db0597e":"code","7c975f48":"code","dda3b7bc":"code","8af061ec":"code","4585dfe7":"code","14799b0b":"code","c9161b2f":"code","17192433":"code","913712b5":"code","55f91e6a":"code","b9c564ce":"code","12ca8192":"code","b8eb6613":"code","4eb8907c":"code","013bec41":"code","a60a5ec3":"code","110e20bd":"code","288860d1":"code","0707f478":"code","19316145":"code","0d2d33fc":"code","689c34c5":"code","6c4dac56":"code","c9961847":"code","46cabf41":"code","0675dfda":"code","ed4366b1":"code","da05d212":"code","b04b8165":"code","54fad153":"code","fa029760":"code","b0525d04":"code","f66ad4df":"code","490fbcb4":"code","6ee13ca4":"code","b5438e2c":"code","54b0f2e9":"code","f0373767":"code","c0b95bef":"code","c7523f6c":"code","6fa7bc4c":"code","0dc5c133":"markdown","6a52624f":"markdown","cf22bb7a":"markdown","9baa9e1e":"markdown","a2b6af10":"markdown","0ea887df":"markdown","343aee04":"markdown","8de4f636":"markdown","fb570185":"markdown","dd7a04a7":"markdown","5c5715ad":"markdown","96b02964":"markdown","46f6c5f1":"markdown","04d132a3":"markdown","ee0a3290":"markdown","2110dc14":"markdown","ec5fa1a2":"markdown","de62556a":"markdown","8d4cd6ed":"markdown","15b0821d":"markdown","dea0be1b":"markdown","5a233e29":"markdown","247d3570":"markdown"},"source":{"f31b179c":"import pandas as pd\nimport numpy as np\nimport random as rnd\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import GridSearchCV","efe13a44":"# Load in the train and test datasets\n\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\nunchanged_data = test_df","f84fbd60":"print(train_df.columns.values)","8664067b":"# preview the data\ntrain_df.head()","4da3ad7a":"sns.countplot(x = \"Sex\", hue = \"Survived\", data = train_df )","e5bd6a85":"sns.countplot(x = \"Pclass\", hue = \"Sex\", data = train_df)","2db0597e":"sns.countplot(x = \"Parch\", hue =\"Survived\",data = train_df)","7c975f48":"sns.countplot(x = \"Embarked\", hue=\"Survived\", data = train_df)","dda3b7bc":"train_df.describe()","8af061ec":"print(train_df.keys())\nprint(test_df.keys())","4585dfe7":"def null_table(train_df, test_df):\n    print(\"Training Data Frame Imputation\")\n    print(pd.isnull(train_df).sum())\n    print(\" \")\n    print(\"Testing Data Frame Imputation\")\n    print(pd.isnull(test_df).sum())\n\nnull_table(train_df, test_df)","14799b0b":"train_df.drop(labels = [\"Cabin\", \"Ticket\"], axis = 1, inplace = True)\ntest_df.drop(labels = [\"Cabin\", \"Ticket\"], axis = 1, inplace = True)\n\nnull_table(train_df, test_df)","c9161b2f":"copy = train_df.copy()\ncopy.dropna(inplace = True)\nsns.distplot(copy[\"Age\"])","17192433":"train_df[\"Age\"].fillna(train_df[\"Age\"].median(), inplace = True)\ntest_df[\"Age\"].fillna(test_df[\"Age\"].median(), inplace = True) \ntrain_df[\"Embarked\"].fillna(\"S\", inplace = True)\ntest_df[\"Fare\"].fillna(test_df[\"Fare\"].median(), inplace = True)\n\nnull_table(train_df, test_df)","913712b5":"train_df.count()","55f91e6a":"test_df.count()","b9c564ce":"sns.barplot(x=\"Pclass\", y=\"Survived\", data=train_df)\nplt.ylabel(\"Survival Rate\")\nplt.xlabel(\"Passenger Classes\")\nplt.title(\"Distribution of Survival Based on Class\")\nplt.show()\n\ntotal_survived_one = train_df[train_df.Pclass == 1][\"Survived\"].sum()\ntotal_survived_two = train_df[train_df.Pclass == 2][\"Survived\"].sum()\ntotal_survived_three = train_df[train_df.Pclass == 3][\"Survived\"].sum()\ntotal_survived_class = total_survived_one + total_survived_two + total_survived_three\n\nprint(\"Count of people who survived: \" + str(total_survived_class))\nprint(\"Proportion of Class 1 Passengers who survived:\") \nprint(total_survived_one\/total_survived_class)\nprint(\"Proportion of Class 2 Passengers who survived:\")\nprint(total_survived_two\/total_survived_class)\nprint(\"Proportion of Class 3 Passengers who survived:\")\nprint(total_survived_three\/total_survived_class)","12ca8192":"train_df.sample(5)","b8eb6613":"train_df.sample(5)","4eb8907c":"def nullValueCount(train_df, test_df):\n    print(\"Training Data\")\n    print(pd.isnull(train_df).sum())\n    print(\"\\n\")\n    print(\"Testing Data\")\n    print(pd.isnull(test_df).sum())\n    \nnullValueCount(train_df, test_df)","013bec41":"train_df.loc[train_df[\"Sex\"] == \"male\", \"Sex\"] = 0\ntrain_df.loc[train_df[\"Sex\"] == \"female\", \"Sex\"] = 1\n\ntrain_df.loc[train_df[\"Embarked\"] == \"S\", \"Embarked\"] = 0\ntrain_df.loc[train_df[\"Embarked\"] == \"C\", \"Embarked\"] = 1\ntrain_df.loc[train_df[\"Embarked\"] == \"Q\", \"Embarked\"] = 2\n\ntest_df.loc[test_df[\"Sex\"] == \"male\", \"Sex\"] = 0\ntest_df.loc[test_df[\"Sex\"] == \"female\", \"Sex\"] = 1\n\ntest_df.loc[test_df[\"Embarked\"] == \"S\", \"Embarked\"] = 0\ntest_df.loc[test_df[\"Embarked\"] == \"C\", \"Embarked\"] = 1\ntest_df.loc[test_df[\"Embarked\"] == \"Q\", \"Embarked\"] = 2","a60a5ec3":"test_df.sample(10)","110e20bd":"train_df[\"FamSize\"] = train_df[\"SibSp\"] + train_df[\"Parch\"] + 1\ntest_df[\"FamSize\"] = test_df[\"SibSp\"] + test_df[\"Parch\"] + 1","288860d1":"train_df[\"Solo\"] = train_df.FamSize.apply(lambda x: 1 if x == 1 else 0)\ntest_df[\"Solo\"] = test_df.FamSize.apply(lambda x: 1 if x == 1 else 0)","0707f478":"for name in train_df[\"Name\"]:\n    train_df[\"Title\"] = train_df[\"Name\"].str.extract(\"([A-Za-z)]+)\\.\", expand = True)\n    \nfor name in test_df[\"Name\"]:\n    test_df[\"Title\"] = test_df[\"Name\"].str.extract(\"([A-Za-z)]+)\\.\", expand = True)","19316145":"train_df[\"Title\"].sample(5)","0d2d33fc":"title_replacements = {\"Mlle\": \"Other\", \n                      \"Major\": \"Other\", \n                      \"Col\": \"Other\", \n                      \"Sir\": \"Other\", \n                      \"Don\": \"Other\", \n                      \"Mme\": \"Other\",\n                      \"Jonkheer\": \"Other\", \n                      \"Lady\": \"Other\", \n                      \"Capt\": \"Other\", \n                      \"Countess\": \"Other\", \n                      \"Ms\": \"Other\", \n                      \"Dona\": \"Other\", \n                      \"Rev\": \"Other\", \n                      \"Dr\": \"Other\"}\n\ntrain_df.replace({\"Title\": title_replacements}, inplace=True)\ntest_df.replace({\"Title\": title_replacements}, inplace=True)","689c34c5":"train_df.loc[train_df[\"Title\"] == \"Miss\", \"Title\"] = 0\ntrain_df.loc[train_df[\"Title\"] == \"Mr\", \"Title\"] = 1\ntrain_df.loc[train_df[\"Title\"] == \"Mrs\", \"Title\"] = 2\ntrain_df.loc[train_df[\"Title\"] == \"Master\", \"Title\"] = 3\ntrain_df.loc[train_df[\"Title\"] == \"Other\", \"Title\"] = 4\n\ntest_df.loc[test_df[\"Title\"] == \"Miss\", \"Title\"] = 0\ntest_df.loc[test_df[\"Title\"] == \"Mr\", \"Title\"] = 1\ntest_df.loc[test_df[\"Title\"] == \"Mrs\", \"Title\"] = 2\ntest_df.loc[test_df[\"Title\"] == \"Master\", \"Title\"] = 3\ntest_df.loc[test_df[\"Title\"] == \"Other\", \"Title\"] = 4","6c4dac56":"set(train_df[\"Title\"])","c9961847":"train_df.sample(5)","46cabf41":"features = [\"Pclass\", \"Sex\", \"Age\", \"Embarked\", \"Fare\", \"FamSize\", \"Solo\",\n            \"Title\"]\n\nX_train = train_df[features]\ny_train = train_df[\"Survived\"]\nX_test_final = test_df[features]","0675dfda":"X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.2, random_state = 33)","ed4366b1":"svc_clf = SVC()\nsvc_clf.fit(X_train, y_train)\npred_svc_clf = svc_clf.predict(X_test)\naccuracy_svc = accuracy_score(y_test, pred_svc_clf)\n\nprint(accuracy_svc)","da05d212":"linsvc_clf = LinearSVC()\nlinsvc_clf.fit(X_train, y_train)\npred_linsvc_clf = linsvc_clf.predict(X_test)\naccuracy_linsvc = accuracy_score(y_test, pred_linsvc_clf)\n\nprint(accuracy_linsvc)","b04b8165":"rf_clf = RandomForestClassifier()\nrf_clf.fit(X_train, y_train)\npred_rf_clf = rf_clf.predict(X_test)\naccuracy_rf = accuracy_score(y_test, pred_rf_clf)\n\nprint(accuracy_rf)","54fad153":"logreg_clf = LogisticRegression()\nlogreg_clf.fit(X_train, y_train)\npred_logreg = logreg_clf.predict(X_test)\naccuracy_logreg = accuracy_score(y_test, pred_logreg)\n\nprint(accuracy_logreg)","fa029760":"knn_clf = KNeighborsClassifier()\nknn_clf.fit(X_train, y_train)\npred_knn = knn_clf.predict(X_test)\naccuracy_knn = accuracy_score(y_test, pred_knn)\n\nprint(accuracy_knn)","b0525d04":"gnb_clf = GaussianNB()\ngnb_clf.fit(X_train, y_train)\npred_gnb = gnb_clf.predict(X_test)\naccuracy_gnb = accuracy_score(y_test, pred_gnb)\n\nprint(accuracy_gnb)","f66ad4df":"dt_clf = DecisionTreeClassifier()\ndt_clf.fit(X_train, y_train)\npred_dt = dt_clf.predict(X_test)\naccuracy_dt = accuracy_score(y_test, pred_dt)\n\nprint(accuracy_dt)","490fbcb4":"from xgboost import XGBClassifier\n\nxg_clf = XGBClassifier(objective=\"binary:logistic\", n_estimators=38, seed=33)\nxg_clf.fit(X_train, y_train)\npred_xgb = xg_clf.predict(X_test)\naccuracy_xgb = accuracy_score(y_test, pred_xgb)\n\nprint(accuracy_xgb)","6ee13ca4":"model_performance = pd.DataFrame({\n    \"Model\": [\"SVC\", \"Linear SVC\", \"Random Forest\", \n              \"Logistic Regression\", \"K Nearest Neighbors\", \"Gaussian Naive Bayes\",  \n              \"Decision Tree\", \"XGBClassifier\"],\n    \"Accuracy\": [accuracy_svc, accuracy_linsvc, accuracy_rf, \n              accuracy_logreg, accuracy_knn, accuracy_gnb, accuracy_dt, \n                 accuracy_xgb]\n})\n\nmodel_performance.sort_values(by=\"Accuracy\", ascending=False)","b5438e2c":"rf_clf = RandomForestClassifier()\n\nparameters = {\"n_estimators\": [4, 5, 6, 7, 8, 9, 10, 15], \n              \"criterion\": [\"gini\", \"entropy\"],\n              \"max_features\": [\"auto\", \"sqrt\", \"log2\"], \n              \"max_depth\": [2, 3, 5, 10], \n              \"min_samples_split\": [2, 3, 5, 10],\n              \"min_samples_leaf\": [1, 5, 8, 10]\n             }\n\ngrid_cv = GridSearchCV(rf_clf, parameters, scoring = make_scorer(accuracy_score))\ngrid_cv = grid_cv.fit(X_train, y_train)","54b0f2e9":"print(\"GridSearchCV results:\")\ngrid_cv.best_estimator_","f0373767":"rf_clf_optimized = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=10, max_features='log2', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=3,\n            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nrf_clf_optimized.fit(X_train, y_train)\npred_rf_clf = rf_clf_optimized.predict(X_test)\naccuracy_rf = accuracy_score(y_test, pred_rf_clf)\n\nprint(accuracy_rf)","c0b95bef":"submission_predictions = rf_clf_optimized.predict(X_test_final)","c7523f6c":"submission_predictions_df = pd.DataFrame(submission_predictions)\nsubmission_predictions_df.count()","6fa7bc4c":"finalSubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": submission_predictions\n    })\n\nfinalsubmission.to_csv(\"titanicSubmission.csv\", index=False)\nprint(finalSubmission.shape)","0dc5c133":"# Logistic Regression","6a52624f":"# Train\/Test Split","cf22bb7a":"# Sumbit Results to Kaggle","9baa9e1e":"# NaN Values","a2b6af10":"After testing multiple parameters for n_estimators, it seems that n_estimators = 38 yields the best accuracy for XGBoost.","0ea887df":"# Evaluation based on accuracy scores","343aee04":"# Random Forest","8de4f636":"# eXtreme Gradient Boosting (XGBoost)\nSince we haven't imported the XGBoost package, we will call it first.","fb570185":"# Linear SVC","dd7a04a7":"# Re-coding the titles with numbers","5c5715ad":"# Data Prep for Model Building","96b02964":"# Gaussian Naive Bayes","46f6c5f1":"Let's impute the NaN values with a median age","04d132a3":"# Training Data Model Building","ee0a3290":"# Optimization using GridSearchCV","2110dc14":"# Let's check if the lamda function ran correctly or not.","ec5fa1a2":"# Combining SibSp and Parch","de62556a":"# Decision Tree","8d4cd6ed":"# One Hot Encoding","15b0821d":"# SVC","dea0be1b":"Clearly, Random Forest performs the best given this dataset. I also noticed that in some iterations the XGBClassifier ran with a better score. So in my opinion both models perform well, with the Random Forest having a slightly better perfomance on an average.","5a233e29":"# Survival Distribution based on Class","247d3570":"# K-Neighbors"}}