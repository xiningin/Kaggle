{"cell_type":{"e5c48a07":"code","1b9340ba":"code","01dfbd91":"code","7b715570":"code","eeeee1dd":"code","475d9b44":"code","49fa4a89":"code","f5990aa6":"code","180c0da8":"code","801aca3b":"code","d8e72dd0":"code","b54816be":"code","4d59ab30":"code","bcf8594c":"code","e2d6a030":"code","93388b32":"code","6bbea8a0":"code","b78d4803":"code","ac54e710":"code","f22923db":"code","dfce9307":"code","644e5ee3":"code","ee61d04d":"markdown","27842e96":"markdown","b7762354":"markdown","60aec472":"markdown","c855a96d":"markdown","0336f17d":"markdown","45d2ad9e":"markdown","986b51e7":"markdown","85db3e12":"markdown","8f3e98e5":"markdown","21e05f24":"markdown","28fef412":"markdown","d591e5f5":"markdown","19c31706":"markdown","7dec5a80":"markdown","a4d86892":"markdown","ad0906a2":"markdown","a4b21e78":"markdown","3db74391":"markdown"},"source":{"e5c48a07":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n%matplotlib inline\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1b9340ba":"!pip install pmdarima","01dfbd91":"# Function to read data into Kernel\n\nts = pd.read_csv(\"..\/input\/\" + 'nab\/realAdExchange\/realAdExchange\/exchange-2_cpc_results' + \".csv\")\nts.head()","7b715570":"ts['timestamp']= pd.to_datetime(ts['timestamp']) #convert the col to timestamo data\nts.set_index('timestamp',drop=True,inplace=True) #set it as the index\nts=ts.resample('h').mean() #sampling\nts.columns=['cpc'] #set the column name as cpc\nts=ts['cpc']\nts.head()","eeeee1dd":"ts.isna().sum()\n# 25 observations are missing ","475d9b44":"\nts.index[ts.isna()] #printng out the missing value location\n# we see that most of the missing values are on 02.09.2011","49fa4a89":"ts.plot(grid=True,figsize =(19,8))\n# in this plot we can see the missing values","f5990aa6":"\nts_clean = ts.loc[ts.index < '2011-09-02'] # selecting the missing values\nts_clean.fillna(method='pad', inplace=True)  # this is equivalent to both method='ffill' and .ffill()\nts_clean.head()","180c0da8":"ts_clean.plot(grid=True, figsize=(19,8)) # cleaned data","801aca3b":"# in practise 70-15-15 is the best way to split the data into Test-Train-Final test(not used for CV or training)\nn_obs = ts_clean.shape[0]  #shape -1512\nsplit1= ts_clean.index[int(0.7*n_obs)]  # split based on index\nsplit2= ts_clean.index[int(0.85*n_obs)]  # split based on index\n\ntrain_ts = ts_clean.loc[ts_clean.index <= split1]\nval_ts = ts_clean.loc[(ts_clean.index > split1) & (ts_clean.index <= split2)]\ntest_ts = ts_clean.loc[ts_clean.index > split2]","d8e72dd0":"pd.concat([train_ts.rename('Train'),test_ts.rename('Test'),val_ts.rename('Validation')],axis=1).plot(grid=True,figsize =(18,9),legend =True)","b54816be":"from statsmodels.tsa.seasonal import seasonal_decompose\n\n\ndecomposition = seasonal_decompose(train_ts,model='additive',freq =24) # frequency of 24hrs -- hourly data set\nplt.figure(figsize=(18,9))\nplt.plot(decomposition.trend)\n","4d59ab30":"trend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\n# Visualize \nplt.figure(figsize=(18,9))\nplt.subplot(411)\nplt.plot(train_ts, label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout()","bcf8594c":"\nfrom statsmodels.tsa.stattools import adfuller\n\nX = train_ts.values\nadf_result = adfuller(X,regression=\"c\")\nprint('ADF Statistic: %f' % adf_result[0])\nprint('p-value: %f' % adf_result[1])\n\nprint('Critical Values: The \u20181%\u2019, \u201810%\u2019 and \u20185%\u2019 values are the critical values for 99%, 90% and 95% confidence levels')\nfor key, value in adf_result[4].items():\n    print('\\t%s: %.3f' % (key, value))\n    \nif adf_result[0] < adf_result[4]['5%']:\n    print('H0 hypothesis rejected : Stationary time series')\nelse:\n    print('H1 hypothesis accepted : Non-Stationary time series')","e2d6a030":"resid = residual.dropna()\n\nadf_resid = adfuller(resid,regression=\"c\")\nprint ('ADF Statistic is :', adf_resid[0])\nprint ('p-value is :', adf_resid[1])","93388b32":"\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nresid = residual.dropna()\n\nplt.figure(figsize=(18,9))\nplt.subplot(211)\nplot_acf(resid, lags=52, ax=plt.gca())\nplt.grid()\nplt.subplot(212)\nplot_pacf(resid, lags=52, ax=plt.gca())\nplt.grid()\nplt.show()","6bbea8a0":"import pmdarima as pm\nstepwise_model = pm.auto_arima(train_ts, start_p=1, start_q=1,\n                           max_p=3, max_q=3, m=24,\n                           start_P=0, seasonal=True,\n                           d=1, D=1, trace=True,\n                           error_action='ignore',  \n                           suppress_warnings=True, \n                           stepwise=True)\nprint(stepwise_model.aic())","b78d4803":"arima = pm.auto_arima(train_ts, error_action='ignore', trace=True,\n                      suppress_warnings=True, maxiter=50,\n                      seasonal=True, m=24)","ac54e710":"from statsmodels.tsa.statespace.sarimax import SARIMAX\nimport time\n\nmodel = SARIMAX(train_ts, order=(3, 1, 3), seasonal_order=(1, 0, 1, 24))\nstart = time.time()\nmodel_fit = model.fit(dis=0)\nprint('fitting complete after {} seconds'.format(time.time()-start))","f22923db":"model_fit.summary()","dfce9307":"\nf_steps = val_ts.shape[0]\nresults = model_fit.get_forecast(f_steps)\n\nforecasts = pd.concat([results.predicted_mean, results.conf_int(alpha=0.05)], axis=1) \nforecasts.columns = ['Forecasts', 'Lower 95% CI', 'Upper 95% CI']\n\nforecasts.head()","644e5ee3":"rmse = ((val_ts.values - results.predicted_mean)**2).mean()\n\nplt.figure(figsize=(18,9))\nplt.plot(train_ts[-24*7:], label='History (actual)')\nplt.plot(val_ts, label='Future (actual)')\nplt.plot(forecasts['Forecasts'], label='Forecast')\nplt.fill_between(forecasts.index, forecasts['Lower 95% CI'], forecasts['Upper 95% CI'], color='0.8',label='95% confidence interval')\nplt.legend()\nplt.grid()\nplt.title('RMSE: '+ str(rmse))","ee61d04d":"# Check for stationarity:\n\nMost time series models require the data to be stationary. A time series is said to be stationary if its statistical properties such as mean, variance & covariance remain constant over time. The above graphs gives the info that the TS data has both trend & seasonality. That means it is not stationary.\n\nApart from this visual method, other way is to perform a statistical test namely Dickey-Fuller Hypothesis testing:\n\n**Null Hypothesis:** The series is not stationary.\n**Alternate Hypothesis:** The series is stationary.\n\nNote:\n* If ADF statistic < critical value => Reject the Null hypothesis\n* If ADF statistic > critical value => Failed to reject the Null hypothesis\n\nIf the p-value is less than 0.05, we reject the null hypothesis ","27842e96":"Rules for srelecting the parameters:\n1. https:\/\/www.datasciencecentral.com\/profiles\/blogs\/tutorial-forecasting-with-seasonal-arima\n2. https:\/\/people.duke.edu\/~rnau\/arimrule.htm","b7762354":"## **Import Data**","60aec472":"## **Convert to  time series**","c855a96d":"# Model Fitting","0336f17d":"![image.png](attachment:image.png)","45d2ad9e":"# Time series - Basics","986b51e7":"A time series is thought to be an aggregate or combination of these four components.\n\n* Level      : The average value in the series.\n* Trend      : The increasing or decreasing value in the series.\n* Seasonality: The repeating short-term cycle in the series.\n* Noise      : The random variation in the series.\n\n\n\n\n\nAll series have a level and noise. The trend and seasonality components are optional.\n\nThese components combine either additively or multiplicatively.\n\n**Additive Model**\nAn additive model suggests that the components are added together as follows:\n\n*y(t) = Level + Trend + Seasonality + Noise*\n\nAn additive model is linear where changes over time are consistently made by the same amount. A linear trend is a straight line.\nA linear seasonality has the same frequency (width of cycles) and amplitude (height of cycles).\n\n**Multiplicative Model**\nA multiplicative model suggests that the components are multiplied together as follows:\n\n*y(t) = Level * Trend * Seasonality * Noise*\n\n\nA multiplicative model is nonlinear, such as quadratic or exponential. Changes increase or decrease over time.A nonlinear trend is a curved line.\nA non-linear seasonality has an increasing or decreasing frequency and\/or amplitude over time.","85db3e12":"* Model1 - From the lecture - RMSE -0.001634420146630526\n* Model2 - Without any limits for p,q - RMSE: 0.0006874025223576442\n* Model3 - With limits - RMSE: 0.001420134501793429","8f3e98e5":"# Forecasting","21e05f24":"This is a basic intro kernel about TS and classical approaches to TS forecast\nThe main idea is from the conference of Dr. Ahmed Abdulaal, Data Scientist, eBay (https:\/\/www.youtube.com\/watch?v=xhuv8NaaroA&list=PLaF0NRp5ez3iPeT3JMyhqfGVV-9cqWL3Z&index=4&t=0s)\n","28fef412":"## Splitting the data and visualising it","d591e5f5":"Here we see that seasonality is present in the data so then the apt model would be SARIMA\nConfiguring a SARIMA requires selecting hyperparameters for both the trend and seasonal elements of the series.\n\n**Trend Elements**\nThere are three trend elements that require configuration which is the same for ARMA too:\n\np: Trend autoregression order.\nd: Trend difference order.\nq: Trend moving average order.\n\n**Seasonal Elements**\nThere are four seasonal elements that are not part of ARIMA that must be configured; they are:\n\nP: Seasonal autoregressive order.\nD: Seasonal difference order.\nQ: Seasonal moving average order.\nm: The number of time steps for a single seasonal period.\n\nTogether, the notation for an SARIMA model is specified as:\n\nSARIMA(p,d,q)(P,D,Q)m\n\n","19c31706":"Autocorrelation is the correlation of a signal with a delayed copy \u2014 or a lag \u2014 of itself as a function of the delay.\nWhen plotting the value of the ACF for increasing lags (a plot called a correlogram), the values tend to degrade to zero quickly for stationary time series ","7dec5a80":"Here we have 3 significant spikes in the ACF model anmd only 1 in the PACF model.\n- MA(3)\n- AR(1)","a4d86892":"The preference is given to the fit that has low values for AIC \/ BIC","ad0906a2":"![image.png](attachment:image.png)","a4b21e78":"## **Missing values**","3db74391":"# Validation"}}