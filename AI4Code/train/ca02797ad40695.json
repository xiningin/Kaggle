{"cell_type":{"2469eabf":"code","23252b0e":"code","35a62d79":"code","52c4d560":"code","25d834fe":"code","710df41e":"code","145b91e5":"code","cd3f986d":"code","df4a9b79":"code","4327f244":"code","79e5a388":"code","cad32551":"code","362372bd":"code","aab59869":"code","e6478f29":"code","161c77c4":"code","f6be53e9":"code","ec3b2e08":"code","f02c0a84":"code","ffc92a3e":"code","91c1e655":"code","f125db84":"code","57244746":"code","761a9ae3":"code","e8686715":"code","8fd26c38":"code","d08b292d":"code","9cb9837f":"code","562dae59":"code","67894121":"code","ce33e916":"code","abe01feb":"code","a0cff8ca":"code","0b176653":"code","725a4cfc":"code","8ffd90e2":"code","4f2b114c":"code","9f4713c4":"code","b6973e2e":"code","948052a4":"code","afb7b3c0":"code","bde03ef3":"code","3feac0ef":"code","91b2a38a":"code","426b144e":"code","2339ee6e":"code","ebf069fc":"code","1e441dca":"code","67bf38a2":"code","4e025e5a":"markdown","331fc514":"markdown","8001a5c4":"markdown","d4d06828":"markdown","bb430314":"markdown","de00316f":"markdown","d888949f":"markdown","f588bccf":"markdown","22f489ea":"markdown","cfa00506":"markdown","2be21498":"markdown","16753c80":"markdown","64e1ec72":"markdown","3f56f962":"markdown","cefa39ca":"markdown","83309a04":"markdown","6dc2ddf1":"markdown","f640b025":"markdown","aa745112":"markdown","86460cf6":"markdown","4062a114":"markdown","9877b23a":"markdown","1b0b5a13":"markdown","3c72b788":"markdown","145f01f0":"markdown","9329ee8d":"markdown","1bacfc5e":"markdown","21f24618":"markdown","3f8b4847":"markdown","3aede53f":"markdown","5b3a4149":"markdown","8a766f93":"markdown","e51e06f7":"markdown","89c629d4":"markdown","d16d26be":"markdown","41520b8e":"markdown","625d41ce":"markdown","9b0cde35":"markdown","4c18cc1c":"markdown","fb39091c":"markdown","6592720f":"markdown","3d6747da":"markdown","5c5eac19":"markdown","8728b32d":"markdown","866d58b1":"markdown"},"source":{"2469eabf":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nfrom fastai.learner import *\n\nimport torchtext\nfrom torchtext import vocab, data\nfrom torchtext.datasets import language_modeling\n\nfrom fastai.rnn_reg import *\nfrom fastai.rnn_train import *\nfrom fastai.nlp import *\nfrom fastai.lm_rnn import *\n\nimport dill as pickle\nimport spacy\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n","23252b0e":"PATH = \"..\/input\/Womens Clothing E-Commerce Reviews.csv\"\n\nPATH_WRITE = '\/kaggle\/working\/'\nTMP_PATH = '\/kaggle\/working\/tmp\/'\nMODELS_PATH = '\/kaggle\/working\/models\/'\n\n%mkdir -p {MODELS_PATH}\n%mkdir -p {TMP_PATH}","35a62d79":"reviews = pd.read_csv(\"..\/input\/Womens Clothing E-Commerce Reviews.csv\")","52c4d560":"reviews.head()","25d834fe":"reviews.info()","710df41e":"reviews.describe()","145b91e5":"fig, axarr = plt.subplots(2,1,figsize=(12,8))\n\nsns.distplot(reviews['Age'],ax=axarr[0])\nsns.countplot(x=reviews['Rating'],data=reviews['Rating'], order=reviews['Rating'].value_counts().index, ax=axarr[1])\naxarr[0].set_title(\"Distribution of Age\")\naxarr[1].set_title(\"Count of Ratings\")\nplt.show()","cd3f986d":"plots = [\"Division Name\", \"Department Name\"]\nfig, axarr = plt.subplots(1,len(plots),figsize=(14,8))\n\n\nfor i,x in enumerate(plots):\n     sns.countplot(y=x,data=reviews, order=reviews[x].value_counts().index, ax=axarr[i])\n     axarr[i].set_title(\"Count of Catergories in {}\".format(x))\n     \naxarr[0].set_ylabel(\"Category\")\naxarr[1].set_ylabel(\"\")\naxarr[0].set_xlabel(\"Frequency Count\")\naxarr[1].set_xlabel(\"Frequency Count\")\nplt.show()\n","df4a9b79":"plt.figure(figsize=(13,4))\nsns.countplot(reviews[\"Class Name\"], order=reviews[\"Class Name\"].value_counts().index)\nplt.title(\"Count of Class Name Items\")\nplt.xticks(rotation=90)","4327f244":"sns.countplot(y=\"Clothing ID\", data=reviews[reviews[\"Clothing ID\"].isin(reviews[\"Clothing ID\"].value_counts()[:30].index)], \n              \n              order=reviews[\"Clothing ID\"].value_counts()[:30].index)\nplt.figure(figsize=(14,7))\nplt.show()","79e5a388":"review_text = reviews[['Review Text', 'Rating']]\nreview_text.head()","cad32551":"review_text.shape","362372bd":"review_text.describe()","aab59869":"len(review_text.isnull())","e6478f29":"len(review_text['Rating'].isnull())","161c77c4":"#review_text[review_text['Review Text']==\"\"]=np.NaN\n#review_text['Review Text'].fillna(\"No review\", inplace=True)","f6be53e9":"split = np.random.randn(len(review_text)) <0.8\nTRN = review_text[split]\nTEST = review_text[~split]\nprint(\"Total rows in train:\" ,len(TRN),\"and test:\", len(TEST))","ec3b2e08":"lens=TRN['Review Text'].str.len()\nprint(\"Avg length:\",lens.mean(), \"Shortest Review:\",lens.min(), \"Longest Review:\",lens.max())","f02c0a84":"spacy_tok = spacy.load('en')","ffc92a3e":"' '.join([sent.string.strip() for sent in spacy_tok(TRN['Review Text'][4])])","91c1e655":"TEXT = data.Field(lower=True, tokenize=\"spacy\")","f125db84":"bs=64; bptt=70","57244746":"FILES =dict(train_df=TRN, val_df=TEST, test_df=TEST,col=\"Review Text\")\nmd = LanguageModelData.from_dataframes(PATH, TEXT, **FILES,bs=bs,bptt=bptt, min_freq=10)","761a9ae3":"pickle.dump(TEXT, open(f'{PATH_WRITE}models\/TEXT.pkl','wb'))","e8686715":"len(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)","8fd26c38":"TEXT.vocab.itos[:12]","d08b292d":"TEXT.vocab.stoi['ugly']","9cb9837f":"md.trn_ds[0].text[:50]","562dae59":"TEXT.numericalize([md.trn_ds[0].text[:50]])","67894121":"next(iter(md.trn_dl))","ce33e916":"em_sz = 200\nnh = 500\nnl =3","abe01feb":"opt_fn = partial(optim.Adam, betas=(0.7, 0.99))\n","a0cff8ca":"LEARNER_KWARGS = [\n    'tmp_name', 'models_name', 'metrics', 'clip', 'crit',\n]\n\ndef get_model(self, opt_fn, emb_sz, n_hid, n_layers, **kwargs):\n    lm_kwargs = {k:v for k,v in kwargs.items() if k not in LEARNER_KWARGS}\n    m = get_language_model(self.nt, emb_sz, n_hid, n_layers, self.pad_idx, **lm_kwargs)\n    model = SingleModel(to_gpu(m))\n    learner_kwargs = {k:v for k,v in kwargs.items() if k in LEARNER_KWARGS}\n    return RNN_Learner(self, model, opt_fn=opt_fn, **learner_kwargs)\n\nLanguageModelData.get_model = get_model","0b176653":"learner = md.get_model(opt_fn, em_sz, nh, nl,\n                      dropouti=0.05,dropout=0.05, wdrop=0.1,dropoute=0.02,dropouth=0.05,\n                      tmp_name=TMP_PATH,models_name=MODELS_PATH)\nlearner.reg_fn = partial(seq2seq_reg, alpha=2, beta=1)\nlearner.clip=0.3","725a4cfc":"learner","8ffd90e2":"learner.fit(3e-3,2,wds=1e-6,cycle_len=1, cycle_mult=2)","4f2b114c":"math.exp(3.495)","9f4713c4":"learner.save_encoder('adam1_enc')","b6973e2e":"learner.load_encoder('adam1_enc')","948052a4":"learner.fit(3e-3,1,wds=1e-6,cycle_len=5)","afb7b3c0":"math.exp(3.275)","bde03ef3":"pickle.dump(TEXT, open(f'{PATH_WRITE}models\/TEXT.pkl','wb'))","3feac0ef":"m=learner.model\nss=\"\"\". So, this is the best  \"\"\"\ns = [TEXT.preprocess(ss)]\nt=TEXT.numericalize(s)\n' '.join(s[0])","91b2a38a":"# Set batch size to 1\nm[0].bs=1\n# Turn off dropout\nm.eval()\n# Reset hidden state\nm.reset()\n# Get predictions from model\nres,*_ = m(t)\n# Put the batch size back to what it was\nm[0].bs=bs","426b144e":"nexts = torch.topk(res[-1], 10)[1]\n[TEXT.vocab.itos[o] for o in to_np(nexts)]","2339ee6e":"print(ss,\"\\n\")\nfor i in range(50):\n    n=res[-1].topk(2)[1]\n    n = n[1] if n.data[0]==0 else n[0]\n    print(TEXT.vocab.itos[n.data[0]], end=' ')\n    res,*_ = m(n[0].unsqueeze(0))\nprint('...')","ebf069fc":"TEXT = pickle.load(open(f'{PATH_WRITE}models\/TEXT.pkl','rb'))","1e441dca":"class ReviewsDataset(torchtext.data.Dataset):\n    def __init__(self, path, text_field, label_field,col, label, dfs, **kwargs):\n        fields = [(\"text\", text_field), (\"label\", label_field)]\n        examples = []\n        \n        \n        \n        for key, df in dfs.items():\n            for i, row in df.iterrows():\n            text = row[col]\n            label = dfs[label]\n            examples.append(data.Example.fromlist([text, label], fields))\n        super().__init__(examples, fields, **kwargs)\n\n    @staticmethod\n    def sort_key(ex): return len(ex.text)\n    \n    @classmethod\n    def splits(cls, text_field, label_field, path,\n               train,  test=None, dfs,col, label,valid=None, **kwargs):\n        dfs = {'train': train}\n        \n        if valid is not None:\n            dfs['valid'] = valid\n            has_validation = 'valid'\n        else:\n            has_validation = None\n        if test is not None:\n            dfs['test'] = test\n            has_test = 'test'\n        else:\n            has_test = None\n        \n        return super().splits(path,\n            text_field=text_field, label_field=label_field,\n            train='train', validation=has_validation, test=has_test,col=col,label=label, dfs=dfs, **kwargs)","67bf38a2":"TEXT = pickle.load(open(f'{PATH_WRITE}models\/TEXT.pkl','rb'))\nLABEL = data.Field(sequential=False)\nsplits = ReviewsDataset.splits(TEXT, LABEL, path='\/kaggle\/working\/',train=TRN, val=TEST,test=TEST,col=\"Review Text\", label=\"Rating\",dfs=review_text)","4e025e5a":"We can now see if it can generate a sentence:","331fc514":"For the word \"ugly\" we can display the mapping of token strings to numerical identifiers:","8001a5c4":"Torchtext will turn all the words above into integer IDs:","d4d06828":"# Introduction\n\nThe goal is to gather sentiment from reviews for clothes from shopping reviews. I will be using the Fast.ai library to achieve this. The dataset is [Womens Ecommerce Clothing Reviews](https:\/\/www.kaggle.com\/nicapotato\/womens-ecommerce-clothing-reviews) which has 23 000 customer reviews and ratings.  \n\nI will do some light EDA to make sense of the data. I will also do the basic NLP preparation techniques like tokenaztion with spacy. \n","bb430314":"We create a dictionary, that specifies the path to our files as well as create a ModelData object. This will fill the TEXT object with the `TEXT.vocab`, it will store which words have been seen in the text, as well as how each word will be mapped to a unique integer id. \n\n","de00316f":"# 4. Build the model","d888949f":"If we found null values, we could choose to fill those rows with something like \"no review\".","f588bccf":"# 2. EDA \n Univariavte Analysis\n\n\n\n\n","22f489ea":"At the moment we are unable to generate a sentence due to the error above.","cfa00506":"Lets do some cleaning up of the data:","2be21498":"bs is our batch size parameter, while bptt defines how many layers we need to backprop through. The higher it is, it will increase time and memory requirements but improve the models ability to handle longer sentences.","16753c80":"Here all we are doing are setting up some of the model's parameters:","64e1ec72":"Below is some tokenized text:","3f56f962":"# Sentiment Classification of Clothing Reviews\ud83d\udc83\ud83d\udc58\ud83d\udc57 [WIP]\n![Clothes](https:\/\/images.unsplash.com\/photo-1521335629791-ce4aec67dd15?ixlib=rb-0.3.5&ixid=eyJhcHBfaWQiOjEyMDd9&s=aa5acd64bf664bc9de6f22d03f6d9c91&auto=format&fit=crop&w=1350&q=80)\n","cefa39ca":"# Resources Used\n\n* [fast.ai](http:\/\/https:\/\/github.com\/fastai\/fastai\/blob\/master\/courses\/dl1\/lesson4-imdb.ipynb)\n* [How to interpert the 'perplexed' metric](https:\/\/www.quora.com\/What-is-perplexity-in-NLP)\n* [Pytorch Docs](https:\/\/torchtext.readthedocs.io\/en\/latest\/data.html#)\n* [Datacamp](https:\/\/github.com\/AmoDinho\/datacamp-python-data-science-track\/blob\/master\/Cleaning%20Data%20in%20Python\/Chapter%205%20-%20Case%20study.py)\n","83309a04":"`sequential=False` alerts torchtext that the text field should be tokenized ","6dc2ddf1":"# 5. Determine Sentiment\n\nIn this part we need to use the vocab from the model to develop our own torchtext dataset and then train it to determine negative and positive reviews. ","f640b025":"#of Batch,  #unique tokens, #tokens in training set, #sentences ","aa745112":"We can begin to train our model:","86460cf6":"Below we start off by creating a small snippet of text.","4062a114":"We are only interested in the review text and rating columns, so we will go ahead and extract that from the `reviews` dataset.","9877b23a":"We need to split the data. We are simply using a 80\/20 split :","1b0b5a13":"Below we are specifying our own get model function, specifically for Kaggle. If you are recreating this notebook elsewhere, do not include this function, as well as `tmp_name` and `models_name` in your `get_model()` function. ","3c72b788":"For NLP models, their accuracy is measured using a metric called perlexity. This interperts  how perplexed or confused it is about a given text. A score of 95% would imply that is very perplexed, but at 33% our model is not entirely perplexed.  ","145f01f0":"# 3. Natural Language Processing\n","9329ee8d":"We specify the number of parameters to set, which are the size of the embedding vector (`em_sz`), hidden activations per layer(`nh`) and number of layers(`nl`).","1bacfc5e":"Here we can see the start of mapping from integer IDs to unique tokens:","21f24618":"`isnull()` checks if there are null values in a dataframe and returns a boolean value. In this case we used len to count the instances where it is `False`. 23486 is the count of entires in the DataFrame, So we can safely assume there are not null values in this column.","3f8b4847":"This is an attempt to create the torchtext dataset for the review vocabulary, it is not functional at the moment.","3aede53f":"We need to make sure we use the saved vocab from our language model to ensure that the same words map to the same IDs ","5b3a4149":"Like wise for the `Rating` column.","8a766f93":"# Test time\n\nWe need to be able to play around with our model to see if it can predict words and sentences to offer after being given a string of text.","e51e06f7":"Fastai uses torchtext very tightly.  so we need to make it aware that it needs SpaCy for tokenization.","89c629d4":"Questions to explore for multivariate analysis: \n\n* Count the positive vs negative ratings\n* Can we find the which age give the most ratings on a particular item of clothing?\n* What kind of ratings to different age groups give?\n* Which item of clothing has the highest rating?","d16d26be":"## Lets find out the length of reviews","41520b8e":"I am struggling with Creating My own Torchtext dataset. I've looked at the following resources:\n\n[Lessons from custom torchtext datasets](http:\/\/forums.fast.ai\/t\/lessons-learned-setting-up-custom-dataset-torchtext\/8227)\n\n[A Comprehensive Introduction to Torchtext (Practical Torchtext part 1)](http:\/\/mlexplained.com\/2018\/02\/08\/a-comprehensive-tutorial-to-torchtext\/)\n","625d41ce":"# 1. Load Data","9b0cde35":"We use dill to let us save it and use it later. ","4c18cc1c":"Below we are able to see what the 10 ten predictions for the next are given the snippet of text from two cells ago. ","fb39091c":"What is great is that were are not overfitting! For the sentiment analysis section we will save the encoder part of the language model.","6592720f":"This is a Adam optimizer with less momentum, because the kind of RNN we are making will not work well with larges amounts of momentum.","3d6747da":"We setup our `learner` model object as well as specify the regulariztion for the different layers. ","5c5eac19":"We need to use [SpaCy](https:\/\/spacy.io\/) for our language model. SpaCy allows us to do powerfull Natural Language Processing, which will allow us to do tokenization, named entity recognition and much more. We will obvisouly be using the English module.\n\nYou can download it by typing the following in your terminal:\n`$ python -m spacy download en`","8728b32d":"# Table of Contents\n\n1. Load Data \n2. EDA\n3. Natural Language Processing\n4. Build The Model\n5. Determine Sentiment  [NB remember to build your own torch text dataset]","866d58b1":"Lets try and train our model for a bit longer. "}}