{"cell_type":{"bb5c3bbf":"code","2f434a14":"code","ec1f24ca":"code","9845eb6c":"code","405e0d7a":"code","b85c82e5":"code","e22a005a":"code","d1a153b3":"code","fd2e50ed":"code","88ae31a6":"code","7646781b":"code","263b9f4d":"code","59fe11f2":"code","dd88b652":"code","e65f289f":"code","b60c6749":"code","30259d38":"code","f74894cf":"code","516532ac":"code","292154e6":"code","53f73276":"code","f99170a5":"code","381c92a8":"code","2c804be3":"code","558a90de":"code","48dce2e9":"code","d774c049":"code","beafe6eb":"code","9537090d":"code","cdfdd0fe":"code","d8adadd3":"code","1af12152":"code","294dd115":"code","9deef73f":"code","61b3eb18":"code","e02b7c5a":"code","40fe14b6":"code","9e7ff827":"code","5ae8fc4d":"code","ff3d4429":"code","507c58c0":"code","7d9f7f32":"code","3d5c83b9":"code","4872872f":"code","4a9a0cd9":"code","26cd87bb":"code","82fc5902":"code","a7c96f0e":"code","19a58310":"code","b2e8c79c":"code","c9051833":"code","be619ed0":"code","7d8bb830":"code","0ed07bb4":"code","dbbd5022":"code","bec7fb95":"code","0cd15c82":"code","3a094330":"code","4a12aa83":"code","e452d635":"markdown","5a9c406e":"markdown","70350e20":"markdown","7dcd8df9":"markdown","3da20e29":"markdown","43cc43e2":"markdown","da5410e6":"markdown","26583530":"markdown","de4aa8ed":"markdown","5bbf18c4":"markdown","c158d4b1":"markdown","8965098b":"markdown","ebbb0318":"markdown","d3e65fd0":"markdown","8d43c038":"markdown","5232147a":"markdown","be8199f6":"markdown","20cb36f0":"markdown","d0dca644":"markdown","b84f0927":"markdown","e6829bd9":"markdown","cbda1d15":"markdown"},"source":{"bb5c3bbf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2f434a14":"import warnings\nwarnings.filterwarnings('ignore')","ec1f24ca":"candy_data = pd.read_csv('\/kaggle\/input\/the-ultimate-halloween-candy-power-ranking\/candy-data.csv')","9845eb6c":"candy_data.head()","405e0d7a":"## lets check shape of the dataframe\ncandy_data.shape","b85c82e5":"## lets check some info about candy data \ncandy_data.info()","e22a005a":"## lets visualise the data\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d1a153b3":"plt.figure(figsize=(15,5), dpi=80)\nplt.subplot(1,4,1)\nsns.countplot(x=candy_data['chocolate'])\nplt.title(\"candy contains chocolate\")\n\nplt.subplot(1,4,2)\nsns.countplot(x=candy_data['fruity'])\n\nplt.title(\"candy contains fruity\")\n\nplt.subplot(1,4,3)\nsns.countplot(x=candy_data['caramel'])\nplt.title(\"candy contains caramel\")\n\n\nplt.subplot(1,4,4)\nsns.countplot(x=candy_data['peanutyalmondy'])\nplt.title(\"candy contains peanutyalmondy\")\n#ax.set_yscale('log')\nplt.tight_layout()\nplt.show()","fd2e50ed":"def map_type(x,y):\n    if x == 1 and y==0:\n        return(\"hard\")\n    elif x==0 and y==1:\n        return (\"bar\")\n    elif x==0 and y==0:\n        return(\"soft\")\n    elif x==1 and y==1:\n        return(\"soft\")\n\ncandy_data['type'] = candy_data[['hard','bar']].apply(lambda x: map_type(x['hard'],x['bar']) , axis=1)","88ae31a6":"plt.figure(figsize=(15,5), dpi=80)\nplt.subplot(1,4,1)\nsns.countplot(x=candy_data['nougat'])\nplt.title(\"candy contains nougat\")\n\nplt.subplot(1,4,2)\nsns.countplot(x=candy_data['crispedricewafer'])\n\nplt.title(\"candy contains crispedricewafer\")\n\nplt.subplot(1,4,3)\nsns.countplot(x=candy_data['type'])\nplt.title(\"type of candy\")\n\n\nplt.subplot(1,4,4)\nsns.countplot(x=candy_data['pluribus'])\nplt.title(\"pluribus\")\n#ax.set_yscale('log')\nplt.tight_layout()\nplt.show()","7646781b":"plt.figure(figsize=(15,5), dpi=80)\nplt.subplot(1,3,1)\nsns.distplot(candy_data['sugarpercent'])\nplt.title(\"distribution of sugarpercent\")\n\nplt.subplot(1,3,2)\nsns.distplot(candy_data['pricepercent'])\nplt.title(\"distribution of pricepercent\")\n\n\n\n\nplt.subplot(1,3,3)\nsns.distplot(candy_data['winpercent'])\nplt.title(\"distribution of winpercent\")\n#ax.set_yscale('log')\nplt.tight_layout()\nplt.show()","263b9f4d":"## creating a feature that tells us one certain candy what type of features contains\ncandy_data['features'] = candy_data['chocolate']+candy_data['fruity']+candy_data['caramel']+candy_data['peanutyalmondy']+candy_data['nougat']+candy_data['crispedricewafer']","59fe11f2":"plt.figure(figsize=(15,20), dpi=80)\n#plt.subplot(1,4,1)\n\nsns.barplot(x=\"features\",y=\"competitorname\",data=candy_data)\nplt.show()","dd88b652":"top_candies_win = candy_data.sort_values(by='winpercent',ascending=False)\ntop_candies_win.head(10)\n## top 10 voted candies","e65f289f":"## let's look at least voted candies\ntop_candies_win.tail(10)","b60c6749":"top_candies_sugary= candy_data.sort_values(by='sugarpercent',ascending=False)\ntop_candies_sugary.head(10)","30259d38":"top_candies_costly = candy_data.sort_values(by='pricepercent',ascending = False)\ntop_candies_costly.head(10)","f74894cf":"## divided the data set into train and test\nfrom sklearn.model_selection import train_test_split\ndf_train,df_test = train_test_split(candy_data,train_size=0.7,test_size=0.3,random_state=100)\n","516532ac":"## checking heat map of variables\ncandy_data_corr = df_train[['chocolate','fruity','caramel','peanutyalmondy','nougat','crispedricewafer','hard','bar','pluribus','sugarpercent','pricepercent','winpercent','features']]\nsns.heatmap(candy_data_corr.corr(),annot=True)\nplt.show()","292154e6":"## lets scale some variables for use in our predictive model\nimport sklearn\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscale_var = ['winpercent','features']\ncandy_data_corr[scale_var] = scaler.fit_transform(candy_data_corr[scale_var])","53f73276":"## lets check the head once \ncandy_data_corr.head()","f99170a5":"## divided train set into x and y \n##x - predictors\n##y - we are going to predict this variable\ny_train = candy_data_corr.pop('chocolate')\nX_train = candy_data_corr","381c92a8":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\n","2c804be3":"## using RFE for feature selection in our model\nfrom sklearn.feature_selection import RFE\nrfe = RFE(logreg,7)\nrfe = rfe.fit(X_train,y_train)","558a90de":"cols = X_train.columns[rfe.support_]\ncols\n## feature selected by rfe ","48dce2e9":"## lets run one logistic regression model using features selected by rfe\nimport statsmodels.api as sm \nmodel1 = sm.GLM(y_train,sm.add_constant(X_train[cols]),family=sm.families.Binomial())\nres = model1.fit()\nres.summary()","d774c049":"## lets look for variance inflation factor of features \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['features'] = X_train[cols].columns\nvif['vif'] = [variance_inflation_factor(X_train[cols].values,i) for i in range(X_train[cols].shape[1])]\nvif['vif'] = round(vif['vif'], 2)\nvif = vif.sort_values(by='vif',ascending=False)\nvif","beafe6eb":"X_train_new = X_train[cols]\nX_train_new.drop('crispedricewafer',axis=1,inplace=True)\n## drop crispedricewafer due to its high p value that means its not statistically fit in our data","9537090d":"## lets check our new feature set\nX_train_new.columns","cdfdd0fe":"model2 = sm.GLM(y_train,sm.add_constant(X_train_new),family=sm.families.Binomial())\nmodel2 = model2.fit()\nmodel2.summary()\n## fit our model to new data set after removing one feature","d8adadd3":"## lets look for variance inflation factor of features \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['features'] = X_train_new.columns\nvif['vif'] = [variance_inflation_factor(X_train_new.values,i) for i in range(X_train_new.shape[1])]\nvif['vif'] = round(vif['vif'], 2)\nvif = vif.sort_values(by='vif',ascending=False)\nvif","1af12152":"## remove feature hard\nX_train_new.drop('hard',axis=1,inplace=True)\n## drop hard due to its high p value that means its not statistically fit in our data","294dd115":"model3 = sm.GLM(y_train,sm.add_constant(X_train_new),family=sm.families.Binomial())\nmodel3 = model3.fit()\nmodel3.summary()\n## fit model to our revised dataset","9deef73f":"## lets look for variance inflation factor of features \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['features'] = X_train_new.columns\nvif['vif'] = [variance_inflation_factor(X_train_new.values,i) for i in range(X_train_new.shape[1])]\nvif['vif'] = round(vif['vif'], 2)\nvif = vif.sort_values(by='vif',ascending=False)\nvif","61b3eb18":"X_train_new.drop('winpercent',axis=1,inplace=True)\n## drop winpercent due to its high p value that means its not statistically fit in our data","e02b7c5a":"model4 = sm.GLM(y_train,sm.add_constant(X_train_new),family=sm.families.Binomial())\nmodel4 = model4.fit()\nmodel4.summary()\n## fit our model in revised data set","40fe14b6":"## lets look for variance inflation factor of features \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['features'] = X_train_new.columns\nvif['vif'] = [variance_inflation_factor(X_train_new.values,i) for i in range(X_train_new.shape[1])]\nvif['vif'] = round(vif['vif'], 2)\nvif = vif.sort_values(by='vif',ascending=False)\nvif","9e7ff827":"X_train_new.drop('pricepercent',axis=1,inplace=True)\n## drop pricepercent due to its high p value that means its not statistically fit in our data","5ae8fc4d":"model5 = sm.GLM(y_train,sm.add_constant(X_train_new),family=sm.families.Binomial())\nmodel5 = model5.fit()\nmodel5.summary()\n## again fit our model to revised data set","ff3d4429":"## lets look for variance inflation factor of features \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['features'] = X_train_new.columns\nvif['vif'] = [variance_inflation_factor(X_train_new.values,i) for i in range(X_train_new.shape[1])]\nvif['vif'] = round(vif['vif'], 2)\nvif = vif.sort_values(by='vif',ascending=False)\nvif","507c58c0":"X_train_new.drop('bar',axis=1,inplace=True)\n## drop bar due to its high p value that means its not statistically fit in our data","7d9f7f32":"model6 = sm.GLM(y_train,sm.add_constant(X_train_new),family=sm.families.Binomial())\nmodel6 = model6.fit()\nmodel6.summary()\n## again fit our model","3d5c83b9":"## lets look for variance inflation factor of features \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['features'] = X_train_new.columns\nvif['vif'] = [variance_inflation_factor(X_train_new.values,i) for i in range(X_train_new.shape[1])]\nvif['vif'] = round(vif['vif'], 2)\nvif = vif.sort_values(by='vif',ascending=False)\nvif","4872872f":"## lets predict some candies whether they have chocolates or not\ny_train_pred = model6.predict(sm.add_constant(X_train_new)).values.reshape(-1)\ny_train_pred[:10]","4a9a0cd9":"final_pred = pd.DataFrame({'competitorname':df_train['competitorname'].values,'chocolate':y_train.values,'pred':y_train_pred})\nfinal_pred.head()\n## created a new dataframe with candies name and their predictio values","26cd87bb":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","82fc5902":"from sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve( final_pred.chocolate, final_pred.pred, drop_intermediate = False )","a7c96f0e":"draw_roc(final_pred.chocolate, final_pred.pred)","19a58310":"numbers = [float(x\/10) for x in range(10)]\nfor i in numbers:\n    final_pred[i] = final_pred['pred'].map(lambda x:1 if x>i else 0)\nfinal_pred.head()","b2e8c79c":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(final_pred.chocolate, final_pred[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","c9051833":"cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","be619ed0":"final_pred['result'] = final_pred['pred'].apply(lambda x:1 if x>0.5 else 0)\nfinal_pred.head()","7d8bb830":"confusion = confusion_matrix(final_pred.chocolate,final_pred.result)\nconfusion","0ed07bb4":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","dbbd5022":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","bec7fb95":"# Let us calculate specificity\nTN \/ float(TN+FP)","0cd15c82":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","3a094330":"# Positive predictive value \nprint (TP \/ float(TP+FP))","4a12aa83":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","e452d635":"An ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","5a9c406e":"sugarpercent,pricepercent and winpercent are normally distributed","70350e20":"# lets find the optimal cutoff point for predict ","7dcd8df9":"* top costly candies","3da20e29":"* top sugary candies","43cc43e2":"in our train data, we have achieved an accuracy of 89% with a positive predictive value of 80% and a negative predictive value of 96%\nour model successfully able to predict whether candies contain chocolate or not (that implies it is more successfully predict chocolate values which are 1 rather than which are 0s)","da5410e6":"* top voted candies","26583530":"costly candies are not much popular as per votes and maximum costly canides contain chocolates","de4aa8ed":"there is no null data in the dataset\n*our task is to predict whether a candy has chocolate in it or not*","5bbf18c4":"maximum candies are soft type and more candies are sold in box rather than single","c158d4b1":"# Let's Create A Logistic Regression Model To Predict Whether A Candy Contains Chocolate Or Not","8965098b":"# Let's Answer some Questions Regarding Ranking Among Candies","ebbb0318":"\n# lets plot the ROC curve \n\n\n","d3e65fd0":"sugary candies are not much popular as per votes","8d43c038":"finally we achived our desired model p values less than 0.05 and vif for all features is also less than 5 ","5232147a":"created a data frame with all the values for different cut offs","be8199f6":"lets find an optimal cutoff to decide basis on which we will decide whether a candy contains a choclate or not","20cb36f0":"let's remove 'bar' and re run the model.As per our model this feature is statistically unfit in the data  ","d0dca644":"some candies like snickers,snickes crisper and baby ruth has lots of features and flavors","b84f0927":"some feature's p value is really high need to drop some feature after checking vif of each","e6829bd9":"all top voted ten candies has chocolate as their ingredient","cbda1d15":"as per accuracy,specificity and sensitivity curve for different optimal cutoff points chose our optimal cutoff point as 0.5"}}