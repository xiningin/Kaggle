{"cell_type":{"785e88a8":"code","891806e2":"code","49d16c2c":"code","871d4d3b":"code","530d3fda":"code","72671f87":"code","7295062e":"code","a7a4d7ca":"code","c262a944":"code","3476703a":"code","66f4fb3b":"code","b0f9127c":"code","0c4aca75":"code","a2521655":"code","5d1e22cf":"code","468b74ff":"code","605e4d37":"code","5cc5f47d":"code","620f7607":"code","3eccc6a9":"code","caf7acd1":"code","2883eb6f":"code","26ce6349":"code","6fad7c22":"code","e910367c":"code","5d56a133":"code","ae03c803":"code","ef864d26":"code","285f24b9":"code","4875d151":"code","4aa6ba87":"code","32e3cb08":"code","e8993445":"code","afe80a45":"code","0b4fabb2":"code","68f65222":"code","01f5f063":"code","6c7b0058":"code","a937fddc":"code","6917e5ea":"code","753c2490":"code","16f02614":"code","f6cbef45":"code","8f80f100":"code","a05e5422":"code","b825d17d":"code","0b0db5bb":"code","9fa7803b":"code","0e15a173":"code","16c5d892":"code","13caf586":"code","e7cad5bd":"code","01d78ed9":"code","8b623e7a":"markdown","e7710b59":"markdown","166bab48":"markdown","a5a5b8ef":"markdown","032c26dc":"markdown","ec38f741":"markdown","dc3dad50":"markdown","1efd2c3e":"markdown","16fb78a5":"markdown","72a39396":"markdown","71d8a057":"markdown","c5bcdf98":"markdown","88429c7a":"markdown","040fa1af":"markdown"},"source":{"785e88a8":"import re\nimport nltk\nimport string\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly\nimport plotly.express as px\nimport ipywidgets as widgets\nfrom plotly.offline import init_notebook_mode\nfrom ipywidgets import interact, interactive, fixed, interact_manual\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.svm import SVR\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom statsmodels.graphics.gofplots import qqplot\nfrom scipy import stats\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\npd.set_option('display.max_columns', None)","891806e2":"init_notebook_mode(connected = True)","49d16c2c":"data = pd.read_csv('..\/input\/airbnb-price-prediction\/train.csv')\ndata.head()","871d4d3b":"df, validation_df  = train_test_split(data,\n                                test_size=0.25,\n                                random_state = 101)","530d3fda":"df_train, df_test  = train_test_split(df,\n                                test_size=0.25,\n                                random_state = 101)","72671f87":"df_train.info()","7295062e":"df_train.describe()","a7a4d7ca":"plt.figure(figsize = (15, 8))\nsns.distplot(df_train['log_price'])\nplt.title('Price distribution')\nplt.show()\n\n\nplt.figure(figsize = (15, 8))\nsns.distplot((df_train['log_price']-np.mean(df_train['log_price'])) \/ np.std(df_train['log_price']))\nplt.title('Price distribution converted to z')\nplt.show()","c262a944":"#qqplot\nqqplot(df_train['log_price'])","3476703a":"df_train[df_train['bedrooms'] == 0].head()","66f4fb3b":"plt.figure(figsize=(12,8))\nsns.heatmap(df_train.isnull(),yticklabels=False,cbar=False,cmap='viridis')","b0f9127c":"years_of_first_review = pd.DataFrame({\n    'year of first review':pd.to_datetime(df_train['first_review'], format='%Y-%m-%d', errors='coerce').dt.year.fillna(0),\n    'log_price': df_train['log_price']\n})\nplt.figure(figsize=(12,4))\n\nsns.countplot(x=\"year of first review\", data=years_of_first_review)\nplt.title('Row count')\nplt.show()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(data=years_of_first_review,orient='v', x = 'year of first review', y = 'log_price')\nplt.title('Years of first review and price')\nplt.show()","0c4aca75":"na_or_not = lambda x: 'na' if x else 'value'\n\nfirst_review_error_status= pd.DataFrame({\n    'first review error status':df_train['first_review'].isna().apply(na_or_not),\n    'log_price': df_train['log_price']\n})\n\nplt.figure(figsize=(12,3))\nsns.countplot(y='first review error status', data=first_review_error_status, orient='h')\nplt.title('Row count')\nplt.show()\n\nplt.figure(figsize=(12,3))\nsns.boxplot(data=first_review_error_status,orient='h', y = 'first review error status', x = 'log_price')\nplt.title('Price distribution for na and notna values in first_review column')\nplt.show()","a2521655":"plt.figure(figsize=(10,5))\ng = sns.FacetGrid(first_review_error_status, hue=\"first review error status\", height = 5, aspect = 2)\ng.map(sns.kdeplot, \"log_price\")\nplt.legend()\nplt.title('Price distribution for na and notna values in first_review column')\nplt.show()","5d1e22cf":"years_of_last_review = pd.DataFrame({\n    'year of last review':pd.to_datetime(df_train['last_review'], format='%Y-%m-%d', errors='coerce').dt.year.fillna(0),\n    'log_price': df_train['log_price']\n})\n\nplt.figure(figsize=(12,4))\nsns.countplot(x=\"year of last review\", data=years_of_last_review)\nplt.title('Row count')\nplt.show()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(data=years_of_last_review,orient='v', x = 'year of last review', y = 'log_price')\nplt.title('Years of last review and price')\nplt.show()","468b74ff":"popular_property_types = list(df_train['property_type'].value_counts()[df_train['property_type'].value_counts() > 50].index)\npopular_property_types","605e4d37":"df_train['room_type'].value_counts()","5cc5f47d":"df_train['bed_type'].value_counts()","620f7607":"df_train['cancellation_policy'].value_counts()\npopular_cancellation_policy = list(df_train['cancellation_policy'].value_counts()[df_train['cancellation_policy'].value_counts() > 100].index)\npopular_cancellation_policy","3eccc6a9":"df_train['city'].value_counts()","caf7acd1":"df_train['host_response_rate'].value_counts()","2883eb6f":"df_train['review_scores_rating'].value_counts()","26ce6349":"df_train[df_train['bathrooms'].isna()].head()","6fad7c22":"df_train['bathrooms'].value_counts()","e910367c":"df_train['host_has_profile_pic'].value_counts()","5d56a133":"years_of_host_since = pd.DataFrame({\n    'year of host_since':pd.to_datetime(df_train['host_since'], format='%Y-%m-%d', errors='coerce').dt.year.fillna(0),\n    'log_price': df_train['log_price']\n})\n\nplt.figure(figsize=(12,4))\nsns.countplot(x=\"year of host_since\", data=years_of_host_since)\nplt.title('Row count')\nplt.show()\n\nplt.figure(figsize=(12,8))\nsns.boxplot(data=years_of_host_since,orient='v', x = 'year of host_since', y = 'log_price')\nplt.title('Years of host_since and price')\nplt.show()","ae03c803":"#map\n\ndef create_map(city):\n    price_view = lambda x: 'price: ' + str(round(x, 2))\n\n    if city in {'NYC', 'LA'}:\n        fraction = 0.2\n    else:\n        fraction = 0.6\n        \n    df_temp = df_train[df_train['city'] == city].sample(frac=fraction,random_state=101)\n    df_temp['log_price'] = np.round(df_temp['log_price'], 2)\n    \n    fig = px.scatter_mapbox(df_temp, \n                            lat=\"latitude\", \n                            lon=\"longitude\", \n                            hover_data=[\"log_price\"],\n                            color='log_price', \n                            zoom=10)\n    \n    fig.update_layout(\n            title = f'Airbnb prices in {city}',\n            geo_scope='usa',\n            width=1000, \n            height=600,\n            mapbox_style=\"white-bg\",\n            mapbox_layers=[{\n                 \"below\": 'traces',\n                 \"sourcetype\": \"raster\",\n                 \"sourceattribution\": \"United States Geological Survey\",\n                 \"source\": [\"https:\/\/basemap.nationalmap.gov\/arcgis\/rest\/services\/USGSImageryOnly\/MapServer\/tile\/{z}\/{y}\/{x}\"]\n              }]\n    )\n    #fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n    fig.update_geos(fitbounds=\"locations\")\n    fig.show()\n    \n#city = interact(lambda x: create_map(x), x=['NYC', 'LA', 'SF', 'DC', 'Chicago', 'Boston'])  #widget\ncreate_map('NYC')","ef864d26":"price_distr = pd.DataFrame(np.histogram(df_train['log_price'], bins = 50)).T\nprice_distr.columns = ['#', 'border']\nprice_distr.dropna(inplace = True)\nprice_distr.astype({'#': 'int32'})\nprice_distr","285f24b9":"df_train[(df_train['log_price'] < 2.5) | (df_train['log_price'] > 7.5)].sort_values(by = 'log_price')","4875d151":"df_train.drop(df_train[df_train['log_price']==0].index, inplace = True)","4aa6ba87":"df_train[df_train['log_price']< 2]","32e3cb08":"amenities_set = set()\nfor amenitie in df_train['amenities']:\n    amenitie_set = set(re.sub(r'(\\\"|\\{|\\})', '', amenitie).split(','))\n    for piece in amenitie_set:\n        if \"translation missing\" not in piece and piece:\n            amenities_set.add(piece.strip())\n            \namenities_set","e8993445":"def dummification(df):\n    dummy_df = pd.DataFrame()\n    object_cols = ['property_type', 'room_type', 'bed_type', 'cancellation_policy', 'city', 'first_review', 'neighbourhood']\n    for col in object_cols:\n        dummy_df = pd.concat([dummy_df, create_dummy(col, df)], axis = 1)\n    \n    return pd.concat([df.drop(columns = object_cols), dummy_df], axis = 1)\n        \ndef create_dummy(col, df):\n    df_dummy = pd.get_dummies(df[col], drop_first = True)\n    df_dummy.columns = ['dum: ' + col + ': ' + name for name in df_dummy.columns]\n    return df_dummy\n\ndef dum_col_filling(main_col_val, dum_col_name, main_col_name):\n    if dum_col_name.replace(main_col_name + ': ', '') in main_col_val:\n        return 1\n    else:\n        return 0\n\ndef set_to_dummies(df, column_name):\n    all_values_of_sets = amenities_set\n    dummy_columns_name = [column_name + ': ' + dum_col for dum_col in all_values_of_sets if dum_col]\n    dummy_df = pd.DataFrame(0, index=df.index, columns=dummy_columns_name)\n    df = pd.concat([dummy_df, df], axis = 1)\n    for dum_col_name in dummy_columns_name:\n        for i in df.index:\n            df[dum_col_name][i] = dum_col_filling(df[column_name][i], dum_col_name, column_name)\n    return df\n\ndef property_type_proc(val):\n    if val in popular_property_types:\n        return val\n    else:\n        return 'other'\n    \ndef cancellation_policy_proc(val):\n    if val in popular_cancellation_policy:\n        return val\n    else:\n        return 'other'\n    \ndef host_response_rate_proc(val):\n    if pd.isna(val):\n        return -100\n    else:\n        return float(val.replace('%', ''))\n\ndef first_review_poc(val):\n    if pd.isna(val):\n        return 'no data'\n    elif datetime.strptime(val,'%Y-%m-%d').year < 2014:\n        return ' < 2014'\n    else:\n        return str(int(datetime.strptime(val,'%Y-%m-%d').year))\n    \ndef unknown_filling(val):\n    if pd.isna(val):\n        return 'no data'\n    else:\n        return val\n\ndef host_since_proc(val):\n    if pd.isna(val):\n        return 2008\n    else:\n        return int(datetime.strptime(val,'%Y-%m-%d').year)\n\ndef true_to_1(val):\n    if val in {'True', True, 't'}:\n        return 1\n    else:\n        return 0\n    \ndef processing(df):\n    df = df.copy()\n    df['property_type'] = df['property_type'].apply(property_type_proc)\n    df['cancellation_policy'] = df['cancellation_policy'].apply(cancellation_policy_proc)\n    df['host_response_rate'] = df['host_response_rate'].apply(host_response_rate_proc)\n    df['first_review'] = df['first_review'].apply(first_review_poc)\n    df['neighbourhood'] = df['neighbourhood'].apply(unknown_filling)\n    df['review_scores_rating'] = df['review_scores_rating'].fillna(0)\n    df['bathrooms'] = df['bathrooms'].fillna(-1)\n    df['bedrooms'] = df['bedrooms'].fillna(-1)\n    df['beds'] = df['beds'].fillna(-1)\n    df['host_has_profile_pic'] = df['host_has_profile_pic'].fillna('f')\n    df['host_identity_verified'] = df['host_identity_verified'].fillna('f')\n    df['host_since'] = df['host_since'].apply(host_since_proc)\n    df['cleaning_fee'] = df['cleaning_fee'].apply(true_to_1)\n    df['host_has_profile_pic'] = df['host_has_profile_pic'].apply(true_to_1)\n    df['host_identity_verified'] = df['host_identity_verified'].apply(true_to_1)\n    df['instant_bookable'] = df['instant_bookable'].apply(true_to_1)\n    df = set_to_dummies(df, 'amenities')\n    df = dummification(df)\n    df.drop(['amenities', 'thumbnail_url', 'description', 'id', 'last_review', 'zipcode', 'name'], axis = 1, inplace = True)\n    return df.sort_index(ascending=False, axis=1)\n\n#df_d = processing(df.sample(frac=0.01,random_state=101))\ndf_d = processing(df_train)\ndf_d.head()","afe80a45":"df_d[df_d.isnull().any(axis=1)]","0b4fabb2":"#adding missing columns to test \\ validation sets and deleting unnecessary\ncolumns_needed = set(df_d.columns)\ndef columns_standardization(df):\n    df = df.copy()\n    for col in columns_needed:\n        if col not in set(df.columns):\n            df.insert(loc = len(df.columns), column = col, value = 0, allow_duplicates=False)\n    \n    for col in set(df.columns):\n        if col not in columns_needed:\n            df.drop(columns = col, axis = 1, inplace = True)\n    \n    return df.sort_index(ascending=False, axis=1)","68f65222":"test = columns_standardization(processing(df_test))\ntest","01f5f063":"test[test.isnull().any(axis=1)]","6c7b0058":"def show_metrics(prediction_test, prediction_train, y_test, y_train):\n    MAE = round(metrics.mean_absolute_error(y_test, prediction_test), 2)\n    MSE = round(metrics.mean_squared_error(y_test, prediction_test), 2)\n    RMSE = round(np.sqrt(metrics.mean_squared_error(y_test, prediction_test)), 2)\n    RMSE_ratio_test= round(np.sqrt(metrics.mean_squared_error(y_test, prediction_test)) \/ np.mean(y_test),3)\n    RMSE_ratio_train = round(np.sqrt(metrics.mean_squared_error(y_train, prediction_train)) \/ np.mean(y_train),3)\n    R_2_test = round(metrics.explained_variance_score(y_test, prediction_test), 2)\n    R_2_train = round(metrics.explained_variance_score(y_train, prediction_train), 2)\n    \n    metrics_data = pd.DataFrame(data = [MAE, MSE, RMSE, RMSE_ratio_test, \n                                    RMSE_ratio_train, R_2_test, R_2_train]).T\n    \n    metrics_data.columns = ['MAE', 'MSE', 'RMSE', 'RMSE_ratio_test', \n                                    'RMSE_ratio_train', 'R_2_test', 'R_2_train']\n    display(metrics_data)\n           \n\n\ndef analysis(model, X_train, X_test, y_train, y_test):\n    \n    prediction_test = model.predict(X_test)\n    prediction_train = model.predict(X_train)\n        \n    \n    show_metrics(prediction_test, prediction_train, y_test, y_train)\n    \n    sns.regplot(x = y_test, y = prediction_test, fit_reg=False)\n    plt.title('Prediction and real')\n    plt.show()\n\n    sns.distplot(y_test - prediction_test, bins = 50)\n    plt.title('Error variance')\n    plt.show()","a937fddc":"lm = LinearRegression(\n        n_jobs = -1,\n        normalize = True\n)\n\nlm.fit(df_d.drop('log_price', axis = 1), df_d['log_price'])\n\ncomment = ''\n\nanalysis(\n    model = lm, \n    X_train = df_d.drop('log_price', axis = 1), \n    X_test = test.drop('log_price', axis = 1), \n    y_train = df_d['log_price'], \n    y_test = test['log_price']\n)","6917e5ea":"rfm = RandomForestRegressor(\n          max_depth = 10,\n          n_jobs = -1, \n          random_state = 101,\n          n_estimators = 700\n    \n)\nrfm.fit(df_d.drop('log_price', axis = 1), df_d['log_price'])\n\ncomment = ''\n\nanalysis(\n    model = rfm, \n    X_train = df_d.drop('log_price', axis = 1), \n    X_test = test.drop('log_price', axis = 1), \n    y_train = df_d['log_price'], \n    y_test = test['log_price']\n)","753c2490":"gbr_model = GradientBoostingRegressor(random_state = 101)\ngbr_model.fit(df_d.drop('log_price', axis = 1), df_d['log_price'])\n\ncomment = ''\n\nanalysis(\n    model = gbr_model, \n    X_train = df_d.drop('log_price', axis = 1), \n    X_test = test.drop('log_price', axis = 1), \n    y_train = df_d['log_price'], \n    y_test = test['log_price']\n)","16f02614":"#scaling\nscaler = MinMaxScaler()\nscaler.fit(df_d.drop('log_price', axis = 1))\nX_train_sc = scaler.transform(df_d.drop('log_price', axis = 1).values)\nX_test_sc = scaler.transform(test.drop('log_price', axis = 1).values)\ny_train = df_d['log_price'].values\ny_test = test['log_price'].values","f6cbef45":"nn_model1 = Sequential()\n\n\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n\n\nnn_model1.add(Dense(64, activation = 'relu'))\nnn_model1.add(Dropout(0.1))\nnn_model1.add(Dense(1))\n\nnn_model1.compile(\n    optimizer='rmsprop',\n    loss='mse'\n)\n\nnn_model1.fit(\n    x = X_train_sc,\n    y = y_train,\n    epochs = 100,\n    validation_data=(X_test_sc, y_test), \n    batch_size = 128,\n    callbacks=[es]\n)\n\n\npd.DataFrame(nn_model1.history.history).plot()\nplt.show()\n\nanalysis(model = nn_model1, \n         X_train = X_train_sc, \n         X_test = X_test_sc, \n         y_train = y_train, \n         y_test = y_test)","8f80f100":"df_train['description']","a05e5422":"def del_punct(text):\n    chars = [char for char in text if char not in string.punctuation]\n    return ''.join(chars)\n\ndef del_stopwords(text):\n    words = [word for word in text.split() if word.lower() not in stopwords.words('english')]\n    return words\n\ndef text_preparation(text):\n    text = text.lower()\n    text = re.sub(r'\\s+', ' ', text)\n    text = re.sub(r'\\d+', 'somenumbers', text)\n    text = del_stopwords(del_punct(text))\n    return text\n    \n    \ntext_preparation('Enjoy a beautiful 67 contemporary residence with')","b825d17d":"bow_transformer = CountVectorizer(analyzer = text_preparation, max_features = 1500).fit(df_train['description'])\nbow = bow_transformer.transform(df_train['description'])\nbow.shape","0b0db5bb":"tfidf_transformer = TfidfTransformer().fit(bow)\ntrain_tfidf = tfidf_transformer.transform(bow)\n\nbow_test = bow_transformer.transform(df_test['description'])\ntest_tfidf = tfidf_transformer.transform(bow_test)","9fa7803b":"rfm_text = RandomForestRegressor(\n    max_depth = 10, \n    n_estimators = 500,\n    n_jobs = -1, \n    random_state = 101\n)\nrfm_text.fit(train_tfidf, df_train['log_price'])\n\ncomment = ''\n\nanalysis(\n    model = rfm_text, \n    X_train = train_tfidf, \n    X_test = test_tfidf, \n    y_train = df_train['log_price'], \n    y_test = df_test['log_price']\n)","0e15a173":"class StackRegression:\n    \n    def __init__(self, model_general, model_secondary):\n        self.model_general = model_general\n        self.model_secondary = model_secondary\n    \n    def fit_predict(self, X_train_general, X_train_secondary, y_train, X_test_general, X_test_secondary, y_test):\n        self.model_secondary.fit(X_train_secondary, y_train)\n        \n        self.secondary_predictions_test =  self.model_secondary.predict(X_test_secondary)\n        self.secondary_predictions_train =  self.model_secondary.predict(X_train_secondary)\n        \n        self.X_train_full = X_train_general.copy()\n        self.X_train_full.assign(secondary_model_predictions = self.secondary_predictions_train)\n        \n        self.X_test_full = X_test_general.copy()\n        self.X_test_full.assign(secondary_model_predictions = self.secondary_predictions_test)\n                        \n        self.model_general.fit(self.X_train_full, y_train)\n        \n        return self.model_general.predict(self.X_test_full)","16c5d892":"model_general = RandomForestRegressor(\n          max_depth = 10,\n          n_jobs = -1, \n          random_state = 101,\n          n_estimators = 700\n    \n)\n\nmodel_for_text = RandomForestRegressor(\n          max_depth = 10,\n          n_jobs = -1, \n          random_state = 101,\n          n_estimators = 700\n    \n)\n\nstack_model = StackRegression(model_general, model_for_text)\n\n\nprediction_test = stack_model.fit_predict(\n    X_train_general = df_d.drop('log_price', axis = 1), \n    X_train_secondary = train_tfidf, \n    y_train = df_d['log_price'], \n    X_test_general = test.drop('log_price', axis = 1), \n    X_test_secondary = test_tfidf, \n    y_test = test['log_price']\n)\n\nprediction_train = stack_model.fit_predict(\n    X_train_general = df_d.drop('log_price', axis = 1), \n    X_train_secondary = train_tfidf, \n    y_train = df_d['log_price'], \n    X_test_general = df_d.drop('log_price', axis = 1), \n    X_test_secondary = train_tfidf, \n    y_test = df_d['log_price']\n)\n\nshow_metrics(prediction_test, prediction_train, y_test = test['log_price'], y_train = df_d['log_price'])\n\n\nsns.regplot(x = test['log_price'], y = prediction_test, fit_reg=False)\nplt.title('Prediction and real')\nplt.show()\n\nsns.distplot(test['log_price'] - prediction_test, bins = 50)\nplt.title('Error variance')\nplt.show()","13caf586":"val_data_for_model = columns_standardization(processing(validation_df))\nval_data_for_model","e7cad5bd":"val_data_for_model[val_data_for_model.isnull().any(axis=1)]","01d78ed9":"analysis(\n    model = rfm, \n    X_train = df_d.drop('log_price', axis = 1), \n    X_test = val_data_for_model.drop('log_price', axis = 1), \n    y_train = df_d['log_price'], \n    y_test = val_data_for_model['log_price']\n)","8b623e7a":"# EDA","e7710b59":"Columns with a lot of missing data:","166bab48":"# Validation","a5a5b8ef":"# Dense neural models","032c26dc":"This model doesn't have an advantage over more simple models. So for final submission I prefer to use just simple RFR","ec38f741":"# Using text data from 'description' column","dc3dad50":"# Outliers handling","1efd2c3e":"# Modelling","16fb78a5":"# Splits","72a39396":"Price distribution of na values looks like one for 2008 values. I think, best of worst is to use 2008 value to fill na in this situation.","71d8a057":"\n# Data preparation","c5bcdf98":"We can see that there is not much information in description of object, that can be used for price prediction, but we can try use this in general model. As an experiment we can try to create a model, that would we a stack of 2 models - prediction of text model will be used in model built on all available features as one more feature.","88429c7a":"Looks like price distribution is roughly Gaussian","040fa1af":"For modelling purposes it would be enough to detele just 0 price record"}}