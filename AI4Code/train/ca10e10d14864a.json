{"cell_type":{"75e09e1c":"code","239fafdf":"code","4a59169b":"code","abd87ae0":"code","3b10f112":"code","10ec44b6":"code","107f71b8":"code","924ef162":"code","aada61c3":"code","cf824369":"code","727be05a":"code","5521bd20":"code","19b302b4":"code","925ffd20":"code","1d1e5b38":"code","2d6c573d":"code","500d3d9b":"code","4ba639f3":"code","40b99a2b":"code","4ee0987d":"code","c16f5a15":"code","901fe6b3":"code","ed948404":"code","eca2beaf":"code","940f4451":"code","0af941e4":"code","65f6b816":"code","b40dbf24":"code","945b48f7":"code","6ab93a80":"code","536c9cdd":"code","70e4bb44":"code","92de17f9":"code","72101350":"code","fcc09a9d":"code","88965001":"code","f477e96f":"code","1bb2c9d5":"code","d3c6fbdd":"code","c144d6db":"code","824b199c":"code","d0e7bbf5":"code","a96a4f6c":"code","86a04742":"code","81fe2b8c":"code","1af619b6":"code","ea626662":"code","03a2bcf1":"code","4d4ec6c6":"code","146c2765":"code","d9ce164b":"code","c49b55f0":"code","70f8727d":"code","01b72852":"code","a28ee1b8":"code","e8770e73":"code","092822b0":"code","5d96cf20":"code","dfd36241":"code","58ba3fd8":"code","eaf05bb8":"code","1f57eb84":"code","dc8aa390":"code","000247b1":"code","5f31eda5":"code","bf330e2e":"code","2a9ed1e7":"code","f0293fd9":"code","070593f0":"code","29cff3bd":"code","5438d243":"code","e75227ad":"code","e3a825ef":"code","00164029":"code","7dcc7280":"code","6cb62cd6":"code","2a33f2a5":"code","ec6c05d9":"code","0d695276":"code","88b14e3b":"code","04d635b8":"code","0baab6e4":"code","9481d7f8":"code","622a39e5":"code","dfde2c7d":"code","67f4770a":"code","8e0e698a":"code","d44bf1ba":"code","50e8d9cc":"code","372f9f99":"code","1174a5e3":"markdown","de668b3e":"markdown","76a0d101":"markdown","6a4e16df":"markdown","8a19ee16":"markdown","15005b4b":"markdown","b3cfdb46":"markdown","abab9f98":"markdown","2286e483":"markdown","90e1aaa2":"markdown","d1e7efb7":"markdown"},"source":{"75e09e1c":"import pandas as pd\n","239fafdf":"study_df = pd.read_csv(\"..\/input\/siim-covid19-detection\/train_study_level.csv\")\nstudy_df[\"id\"] = study_df[\"id\"].str.replace(\"_study\", \"\")\n","4a59169b":"study_df.head()","abd87ae0":"import os\n","3b10f112":"def get_absolute_file_paths(directory):\n    all_abs_file_paths = []\n    for dirpath,_,filenames in os.walk(directory):\n        for f in filenames:\n            all_abs_file_paths.append(os.path.abspath(os.path.join(dirpath, f)))\n    return all_abs_file_paths","10ec44b6":"from tqdm.notebook import tqdm; tqdm.pandas();\n","107f71b8":"    study_df[\"study_dir\"] = \"\/kaggle\/input\/siim-covid19-detection\/train\/\"+study_df[\"id\"]\n    study_df[\"images_per_study\"] = study_df.study_dir.progress_apply(lambda x: len(get_absolute_file_paths(x)))\n#     study_df[\"images_per_study\"] = study_df.study_dir.apply(lambda x: len(get_absolute_file_paths(x)))\n","924ef162":"study_df.head()","aada61c3":"image_df = pd.read_csv(\"\/kaggle\/input\/siim-covid19-detection\/train_image_level.csv\")\n","cf824369":"image_df.head()","727be05a":"image_study_df = pd.merge(image_df, \n                          study_df, \n                          left_on='StudyInstanceUID',\n                          right_on='id')","5521bd20":"image_study_df.head()","19b302b4":"image_study_df.shape","925ffd20":"# df_image = pd.read_csv('..\/input\/siim-covid19-detection\/train_image_level.csv')\n# df_study['id'] = df_study['id'].str.replace('_study',\"\")\n# df_study.rename({'id': 'StudyInstanceUID'},axis=1, inplace=True)\n# df_train = df_image.merge(df_study, on='StudyInstanceUID')\n","1d1e5b38":"image_study_df.head()","2d6c573d":"image_study_df.loc[image_study_df['Negative for Pneumonia']==1, 'study_label'] = 'negative'\nimage_study_df.loc[image_study_df['Typical Appearance']==1, 'study_label'] = 'typical'\nimage_study_df.loc[image_study_df['Indeterminate Appearance']==1, 'study_label'] = 'indeterminate'\nimage_study_df.loc[image_study_df['Atypical Appearance']==1, 'study_label'] = 'atypical'\n","500d3d9b":"image_study_df.drop(['Negative for Pneumonia','Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance'], axis=1, inplace=True)\n","4ba639f3":"image_study_df.head(20)","40b99a2b":"image_study_df['id_jpg'] = image_study_df['id_x'].str.replace('_image', '.jpg')\nimage_study_df['image_label'] = image_study_df['label'].str.split().apply(lambda x : x[0])\n","4ee0987d":"image_study_df.head(20)","c16f5a15":"image_study_df.image_label.value_counts()","901fe6b3":"# df_size = pd.read_csv('..\/input\/covid-jpg-512\/size.csv')\n# df_train = df_train.merge(df_size, on='id')\n","ed948404":"# df_size.head()","eca2beaf":"# df_size.split.value_counts()","940f4451":"# import numpy as np\n# import os\n# from skimage import exposure\n# import matplotlib\n# matplotlib.rcParams.update({'font.size': 16})\n# import warnings\n# warnings.filterwarnings('ignore')\n# import tensorflow.keras.backend as K\n","0af941e4":"\n# def preprocess_image(img):\n#     equ_img = exposure.equalize_adapthist(img\/255, clip_limit=0.05, kernel_size=24)\n#     return equ_img\n\n# df_opa = df_train[df_train['image_label']=='opacity'].reset_index()\n# fig, axs = plt.subplots(5, 2, figsize=(10,20))\n# fig.subplots_adjust(hspace=.2, wspace=.2)\n# n=5\n# for i in range(n):\n#     img = cv2.imread(os.path.join(train_dir, df_opa['id'][i]))\n#     img_proc = preprocess_image(img)\n#     axs[i, 0].imshow(img)\n#     axs[i, 1].imshow(img_proc)\n#     axs[i, 0].axis('off')\n#     axs[i, 1].axis('off')\n#     boxes = literal_eval(df_opa['boxes'][i])\n#     for box in boxes:\n#         axs[i, 0].add_patch(Rectangle((box['x']*(512\/df_opa['dim1'][i]), box['y']*(512\/df_opa['dim0'][i])), box['width']*(512\/df_opa['dim1'][i]), box['height']*(512\/df_opa['dim0'][i]), fill=0, color='y', linewidth=3))\n#         axs[i, 0].set_title(df_opa['study_label'][i])\n#         axs[i, 1].add_patch(Rectangle((box['x']*(512\/df_opa['dim1'][i]), box['y']*(512\/df_opa['dim0'][i])), box['width']*(512\/df_opa['dim1'][i]), box['height']*(512\/df_opa['dim0'][i]), fill=0, color='r', linewidth=3))\n#         axs[i, 1].set_title('After CLAHE')\n    \n# plt.show()","65f6b816":"# batch_size = 32\nbatch_size = 16\n","b40dbf24":"train_dir = '..\/input\/covid-jpg-512\/train'\n","945b48f7":"! ls '..\/input\/covid-jpg-512\/train' | wc -l","6ab93a80":"! ls -ahl '..\/input\/covid-jpg-512\/train' | grep 000c3a3f293f","536c9cdd":"import cv2\n# img = cv2.imread('..\/input\/covid-jpg-512\/train\/000a312787f2.jpg')\nimg = cv2.imread('..\/input\/covid-jpg-512\/train\/000c3a3f293f.jpg')\n","70e4bb44":"img.shape","92de17f9":"# img_size = 150\nimg_size = 512\n","72101350":"from tensorflow.keras.preprocessing.image import ImageDataGenerator","fcc09a9d":"image_generator = ImageDataGenerator(\n        rescale = 1.\/255,\n        validation_split=0.25,\n        rotation_range=10,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        shear_range=0.1,\n        zoom_range=0.1,\n        horizontal_flip=False,\n        fill_mode='nearest',\n)\n\nimage_generator_valid = ImageDataGenerator(validation_split=0.25,\n                                           rescale = 1.\/255,  )\n","88965001":"train_generator = image_generator.flow_from_dataframe(\n        dataframe = image_study_df,\n        directory='..\/input\/covid-jpg-512\/train',\n        x_col = 'id_jpg',\n        y_col =  'image_label',  \n        target_size=(img_size, img_size),\n        batch_size=batch_size,\n#         class_mode='binary',\n        subset='training', \n        seed = 23) \n\nvalid_generator=image_generator_valid.flow_from_dataframe(\n    dataframe = image_study_df,\n    directory='..\/input\/covid-jpg-512\/train',\n    x_col = 'id_jpg',\n    y_col = 'image_label',\n    target_size=(img_size, img_size),\n    batch_size=batch_size,\n#     class_mode='binary',\n    subset='validation', \n    shuffle=False, \n    seed=23) \n","f477e96f":"len(valid_generator.filenames)\/valid_generator.batch_size","1bb2c9d5":"len(train_generator.filenames)\/train_generator.batch_size","d3c6fbdd":"valid_generator.batch_size, valid_generator.class_indices","c144d6db":"# valid_generator.allowed_class_modes\n# valid_generator.class_mode\n# valid_generator.classes\n# valid_generator.color_mode\n# valid_generator.directory\n# valid_generator.data_format\n# valid_generator.dtype\nvalid_generator.filenames[:5]","824b199c":"# valid_generator.filepaths[:5]\n# valid_generator.image_shape\n# valid_generator.labels[:5]\nvalid_generator.target_size\n","d0e7bbf5":"import matplotlib.pyplot as plt\n","a96a4f6c":"# for j in range(4):\n#     aug_images = [train_generator[0][0][j] for i in range(5)]\n#     fig, axes = plt.subplots(1, 5, figsize=(24,24))\n#     axes = axes.flatten()\n#     for img, ax in zip(aug_images, axes):\n#         ax.imshow(img)\n#         ax.axis('off')\n# plt.tight_layout()\n# plt.show()","86a04742":"chex_weights_path = '..\/input\/chexnet-weights\/brucechou1983_CheXNet_Keras_0.3.0_weights.h5'\n","81fe2b8c":"from tensorflow.keras.applications import DenseNet121\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Dropout, GlobalAveragePooling2D\n# from tensorflow.keras import models\n","1af619b6":"img_size","ea626662":"pre_model = DenseNet121(weights=None,\n                        include_top=False,\n                        input_shape=(img_size,img_size,3)\n                        )\nout = Dense(14, activation='sigmoid')(pre_model.output)\npre_model = Model(inputs=pre_model.input, outputs=out) \n","03a2bcf1":"pre_model.load_weights(chex_weights_path)\n","4d4ec6c6":"# pre_model.trainable = False\npre_model.trainable = True\n\n","146c2765":"pre_model.summary()","d9ce164b":"len(pre_model.layers)","c49b55f0":"pre_model.layers[420:]","70f8727d":"pre_model.layers[-2]","01b72852":"# last_layer = pre_model.get_layer('conv5_block16_concat')\nlast_layer = pre_model.layers[-2]\n\nprint('last layer output shape: ', last_layer.output_shape)\nlast_output = last_layer.output","a28ee1b8":"last_layer","e8770e73":"# Flatten the output layer to 1 dimension\nx = Flatten()(last_output)\n# Add a fully connected layer with 512 hidden units and ReLU activation\nx = Dense(512, activation='relu')(x)\n# Add a dropout rate of 0.2\nx = Dropout(0.2)(x)                  \n# # Add a fully connected layer with 128 hidden units and ReLU activation\n# x = Dense(128, activation='relu')(x)\n\n\n# Add final classification layer\n# x = Dense(1, activation='sigmoid')(x)  \nx = Dense(2, activation='softmax')(x)\n","092822b0":"from tensorflow.keras.optimizers import Adam, RMSprop\n","5d96cf20":"model = Model( pre_model.input, x) \n\n# model.compile(optimizer = RMSprop(lr=0.01), \n#               loss='binary_crossentropy', \n#               metrics=['accuracy'])\n\nmodel.compile(Adam(lr=1e-3),\n              loss='binary_crossentropy',\n              metrics='accuracy')\nmodel.summary()","dfd36241":"train_generator","58ba3fd8":"valid_generator","eaf05bb8":"from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping\n","1f57eb84":"rlr = ReduceLROnPlateau(monitor = 'val_accuracy', \n                        factor = 0.2, \n                        patience = 2, \n                        verbose = 1, \n                        min_delta = 1e-4, \n                        min_lr = 1e-4, \n                        mode = 'max')","dc8aa390":"es = EarlyStopping(monitor = 'val_accuracy', \n                   min_delta = 1e-4, \n                   patience = 3, \n                   mode = 'max', \n                   restore_best_weights = True, \n                   verbose = 1)\n","000247b1":"ckp = ModelCheckpoint('model.h5',\n                      monitor = 'val_accuracy',\n                      verbose = 0, \n                      save_best_only = True, \n                      mode = 'max')","5f31eda5":"history = model.fit(\n      train_generator,\n      validation_data=valid_generator,\n#       steps_per_epoch = 296,\n      epochs=10,\n      callbacks=[es, ckp, rlr],\n#       validation_steps=98,\n#       verbose=1\n)","bf330e2e":"%matplotlib inline\nimport matplotlib.pyplot as plt\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend(loc=0)\nplt.figure()\n\n\nplt.show()","2a9ed1e7":"import numpy as np","f0293fd9":"# from tensorflow.math import confusion_matrix\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n","070593f0":"actual =  valid_generator.labels\npreds = np.argmax(model.predict(valid_generator), axis=1)\ncfmx = confusion_matrix(actual, preds)\nacc = accuracy_score(actual, preds)","29cff3bd":"model.evaluate(valid_generator)","5438d243":"actual[:10]","e75227ad":"preds[:10]","e3a825ef":"valid_generator.class_indices","00164029":"cfmx","7dcc7280":"acc","6cb62cd6":"# from seaborn import heatmap\n","2a33f2a5":"# print ('Test Accuracy:', acc )\n# heatmap(cfmx, annot=True, cmap='plasma',\n#         xticklabels=['Normal','Opacity'],fmt='.0f', yticklabels=['Normal', 'Opacity'])\n# plt.show()","ec6c05d9":"# hist = pd.DataFrame(history.history)\n# fig, (ax1, ax2) = plt.subplots(figsize=(12,12),nrows=2, ncols=1)\n# hist['loss'].plot(ax=ax1,c='k',label='training loss')\n# hist['val_loss'].plot(ax=ax1,c='r',linestyle='--', label='validation loss')\n# ax1.legend()\n# hist['accuracy'].plot(ax=ax2,c='k',label='training accuracy')\n# hist['val_accuracy'].plot(ax=ax2,c='r',linestyle='--',label='validation accuracy')\n# ax2.legend()\n# plt.show()","0d695276":"def grad_cam(input_image, model, layer_name):\n\n    desired_layer = model.get_layer(layer_name)\n    grad_model = Model(model.inputs, [desired_layer.output, model.output])\n\n    with tf.GradientTape() as tape:\n        layer_output, preds = grad_model(input_image)\n        ix = (np.argsort(preds, axis=1)[:, -1]).item()\n        output_idx = preds[:, ix]\n\n    gradient = tape.gradient(output_idx, layer_output)\n    alpha_kc = np.mean(gradient, axis=(0,1,2))\n    L_gradCam = tf.nn.relu(np.dot(layer_output, alpha_kc)[0])\n    L_gradCam = (L_gradCam - np.min(L_gradCam)) \/ (np.max(L_gradCam) - np.min(L_gradCam)) \n    return L_gradCam.numpy()","88b14e3b":"import cv2\n","04d635b8":"def blend(img_path, gradCam_img, alpha, colormap = cv2.COLORMAP_JET):\n    origin_img = img_to_array(load_img(img_path))\n    gradCam_resized = cv2.resize(gradCam_img, (origin_img.shape[1], origin_img.shape[0]), interpolation = cv2.INTER_LINEAR)\n    heatmap  = cv2.applyColorMap(np.uint8(gradCam_resized*255), colormap)\n    superimposed_image = cv2.cvtColor(origin_img.astype('uint8'), cv2.COLOR_RGB2BGR) + heatmap * alpha\n    return heatmap, superimposed_image","0baab6e4":"def plot_results(model, gen, label=0):\n    n = 50\n    fig, axs = plt.subplots(10, 5, figsize=(20,60))\n    fig.subplots_adjust(hspace=.5, wspace=.1)\n    axs = axs.ravel()\n    gen.next()\n    classes = list(gen.class_indices.keys()) \n    if label==0:\n        idx = np.array(np.where(np.array(gen.labels) ==0)).ravel()\n    else:\n        idx = np.array(np.where(np.array(gen.labels) ==1)).ravel()\n   \n    layer_name = 'bn'\n    for i in range(n):\n        sample_img_path = os.path.join(train_dir, df_train['id_jpg'][idx[i]])\n        img = load_process(sample_img_path, img_size)\n        pred = model.predict(img)\n        grad_cam_img = grad_cam(img, model, layer_name)\n        heatmap_img, result_img = blend(sample_img_path, grad_cam_img, 0.5)\n        axs[i].imshow(result_img[:,:,::-1]\/255)\n        axs[i].set_xticklabels([])\n        axs[i].set_yticklabels([])\n        if type(df_train['boxes'][idx[i]])==str:\n            boxes = literal_eval(df_train['boxes'][idx[i]])\n            for box in boxes:\n#                 axs[i].add_patch(Rectangle((box['x']*(512\/df_train['dim1'][idx[i]]), box['y']*(512\/df_train['dim0'][idx[i]])), box['width']*(512\/df_train['dim1'][idx[i]]), box['height']*(512\/df_train['dim0'][idx[i]]), fill=0, color='y', linewidth=2))\n                axs[i].set_title(f\"{df_train['study_label'][idx[i]]}, {df_train['image_label'][idx[i]]}\")\n        else:\n            axs[i].set_title(df_train['study_label'][idx[i]])\n        \n        axs[i].set_xlabel(f\"{classes[np.argmax(pred)]}, {round(pred[0][np.argmax(pred)]*100, 2)}%\")","9481d7f8":"df_train = image_study_df","622a39e5":"from tensorflow.keras.preprocessing.image import load_img, img_to_array\n","dfde2c7d":"def load_process(img, img_size):\n    img = load_img(img, target_size = (img_size, img_size))\n    img = img_to_array(img)\n    img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n#     img = preprocess_image(img)\n    return img","67f4770a":"import tensorflow as tf\n","8e0e698a":"from ast import literal_eval\n","d44bf1ba":"from matplotlib.patches import Rectangle","50e8d9cc":"# plot_results(model, valid_generator,label=0)","372f9f99":"# plot_results(model, valid_generator,label=1)","1174a5e3":"## Model performance","de668b3e":"### nb work done on top of:\n\nhttps:\/\/www.kaggle.com\/sinamhd9\/chexnet-fine-tuned-model-interpretation","76a0d101":"### in this nb, we use the jpg variant of data","6a4e16df":"## Architecture","8a19ee16":"### Intro\n\n**CheXNet** [[1]](https:\/\/arxiv.org\/pdf\/1711.05225.pdf) is a 121 layer **DenseNet** developed by Stanford researchers that can detect pneumonia from chest X-rays at a level exceeding practicing radiologists. \n\nThe weights of the model are uploaded into this notebook and used to train on our data to classify normal vs opacity (typical, atypical, indeterminate) cases. \n\nContrast Limited Adaptive Histogram Equalization (**CLAHE**) is used for preprocessing and some augmentation techniques are applied. \nFor interpretability, **GRAD-CAM** is used to see if the model is paying attention to the opacities (comparing to the groundtruth bounding boxes).","15005b4b":"## Model Interpretation ","b3cfdb46":"### get data - \n\nstart from the eda notebook","abab9f98":"## ImageGenerators and Augmentations","2286e483":"### attempt1: image binary classifier - opacity vs. none","90e1aaa2":"### if acc does not change, try these ???:\n\nhttps:\/\/stackoverflow.com\/questions\/37213388\/keras-accuracy-does-not-change\n\nhttps:\/\/datascience.stackexchange.com\/questions\/30930\/accuracy-and-loss-dont-change-in-cnn-is-it-over-fitting\n\nhttps:\/\/www.kaggle.com\/questions-and-answers\/56171\n\nhttps:\/\/github.com\/keras-team\/keras\/issues\/2711\n","d1e7efb7":"### Things to consider from https:\/\/arxiv.org\/pdf\/1711.05225.pdf:\n\noptimize the weighted binary cross entropy loss\n\nminibatches of size 16. \n\npick the model with the lowest validation loss - done.\n\nLearning rate changes - done\n\nbetter test-train split based on Y balance\n\nnormalize based on the mean and standard deviation of images\n\naugment the training data with random horizontal flipping.\n\n\n"}}