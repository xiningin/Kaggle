{"cell_type":{"a56da720":"code","01351d8e":"code","ff820ae2":"code","e02b6733":"code","13b963f2":"code","671120b1":"code","9e4975fb":"code","55ad0de0":"code","0f65c151":"code","4f1d8a26":"code","d67de5be":"code","3d5a924b":"code","f507247a":"code","9dc5cd04":"code","16411d97":"code","04ba8bdf":"markdown","ac3b3702":"markdown","04f7e3af":"markdown","0156cd58":"markdown","80a324e3":"markdown","864c0dd3":"markdown","0e285cc7":"markdown","db52fc4b":"markdown","a9f124e6":"markdown","96a343ce":"markdown"},"source":{"a56da720":"from tensorflow.keras.layers import *\nfrom keras import backend as K\nimport tensorflow as tf\nfrom keras.models import load_model\nimport matplotlib.pyplot as plt","01351d8e":"def recall_m(y_true, y_pred):\n\n    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), 0.5),K.floatx())\n    true_positives = K.round(K.sum(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.clip(y_true, 0, 1))\n    recall_ratio = true_positives \/ (possible_positives + K.epsilon())\n    return recall_ratio\n\ndef precision_m(y_true, y_pred):\n\n    y_pred = K.cast(K.greater(K.clip(y_pred, 0, 1), 0.5), K.floatx())\n    true_positives = K.round(K.sum(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(y_pred)\n    precision_ratio = true_positives \/ (predicted_positives + K.epsilon())\n    return precision_ratio\n\ndef f1_m(y_true, y_pred):\n    \n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","ff820ae2":"img_width, img_height = 160, 160\n\nif K.image_data_format() == 'channels_first':\n    input_shape = (3, img_width, img_height)\nelse:\n    input_shape = (img_width, img_height, 3)\n    \nbatch_size = 128\nepochs = 25","e02b6733":"from keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale = 1.\/255,\n                                   validation_split=0.15)\n\ntraining_set = train_datagen.flow_from_directory('..\/input\/apparel-images-dataset',\n                                                 target_size = (img_width, img_height),\n                                                 batch_size = batch_size,\n                                                 subset='training')\n\nval_set = train_datagen.flow_from_directory('..\/input\/apparel-images-dataset',\n                                                 target_size = (img_width, img_height),\n                                                 batch_size = batch_size,\n                                                 subset='validation')","13b963f2":"fig, ax = plt.subplots(nrows=6, ncols=4,figsize = (15,20))\n\nfor X_batch, y_batch in training_set:\n    i=0\n    for row in ax:\n        for col in row:\n            col.imshow(X_batch[i])\n            i+=1\n    break\nplt.show()    ","671120b1":"c = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",patience=5,\n    verbose=1, mode=\"auto\",\n    restore_best_weights=True,\n)","9e4975fb":"model = tf.keras.Sequential()\n\nmodel.add( \n    \n    tf.keras.applications.Xception(\n        input_shape = input_shape,\n        include_top = True,\n        weights     = None,\n        pooling     = 'max'\n        )\n)\n\nmodel.add(Dense(512,activation='relu'))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dense(24,activation='softmax'))\n\nmodel.compile(\n    optimizer = 'adam', \n    loss = 'categorical_crossentropy',               \n    metrics=['accuracy',f1_m]\n)\nmodel.summary()","55ad0de0":"history = model.fit(training_set,\n                    validation_data = val_set,\n                    epochs=epochs,\n                    callbacks = [c],\n                    verbose = 1)","0f65c151":"print(f\"Max F1-score in validation dataset = {max(history.history['val_f1_m'])}\")","4f1d8a26":"model = tf.keras.Sequential()\n\nmodel.add( \n    tf.keras.applications.ResNet50V2(\n        input_shape = input_shape,\n        include_top = True,\n        weights     = None,\n        pooling     = 'max'\n        )\n)\n\nmodel.add(Dense(512,activation='relu'))\nmodel.add(Dense(64,activation='relu'))\nmodel.add(Dense(24,activation='softmax'))\n\nmodel.compile(\n    optimizer = 'adam', \n    loss = 'categorical_crossentropy',               \n    metrics=['accuracy',f1_m]\n)\n\nmodel.summary()","d67de5be":"history = model.fit(training_set,\n                    validation_data = val_set,\n                    epochs=epochs,\n                    callbacks = [c],\n                    verbose = 1)","3d5a924b":"print(f\"Max F1-score in validation dataset = {max(history.history['val_f1_m'])}\")","f507247a":"def models(input_size = input_shape):\n\n    inputs = tf.keras.layers.Input(input_size)\n    \n    base_model1 =     tf.keras.applications.ResNet50V2(\n        include_top = False,\n        weights     = None,\n        pooling     = 'max'\n        )(inputs)\n\n    base_model2 =     tf.keras.applications.Xception(\n        include_top = False,\n        weights     = None,\n        pooling     = 'max'\n        )(inputs)\n    \n    model = tf.keras.layers.Concatenate()([base_model1,base_model2])\n    \n    model = tf.keras.layers.Dense(512, activation='relu')(model)\n    model = tf.keras.layers.Dense(64, activation='relu')(model)\n    model = tf.keras.layers.Dense(24, activation='softmax')(model)\n\n    M = tf.keras.Model(inputs=inputs, outputs=model)\n    M.compile(\n        optimizer = 'adam', \n        loss = 'categorical_crossentropy',               \n        metrics=['accuracy',f1_m]\n    )\n    \n    return M\n\nmodel = models()\nmodel.summary()","9dc5cd04":"history = model.fit(training_set,\n                    validation_data = val_set,\n                    epochs=epochs,\n                    callbacks = [c],\n                    verbose = 1)","16411d97":"print(f\"Max F1-score in validation dataset = {max(history.history['val_f1_m'])}\")","04ba8bdf":"As you see there's a clear increase in F1-Score from 87% to 91% and can be increased with further fine-tuning and Image Augmenattion.  \nMy motive was to get you familiarised with this new concept.  \nI hoped you liked it and learned from it.  \n**Happy Learning**","ac3b3702":"# Xception Net\nFirst we'll see hte performance of Xception net","04f7e3af":"# Importing Packages\nI've decided to go with Keras for now, however you can soon expect to view similar code in PyTorch","0156cd58":"# Stacking","80a324e3":"# ResNet50V2","864c0dd3":"# Plotting some images","0e285cc7":"# F1-Score\nF1 score is great way to learn about your model on an unbalanced dataset. It is given as:  \n\nRecall = TruePositives \/ (TruePositives + FalseNegatives)\n\nPrecision = TruePositives \/ (TruePositives + FalsePositives)\n\nF1 = 2 (precision recall) \/ (precision + recall)","db52fc4b":"# Callbacks\nCallbacks are an easy way to stop training in case of Over fitting.  \nThere are many other Callbacks as well which can reduce Learning rate, or stop if error reaches NaN etc.  \nRefer [here](https:\/\/keras.io\/api\/callbacks\/) for more details","a9f124e6":"# Loading Dataset\n**Note** My focus in this notebook is on model stacking. For Image augmentation refer [this](https:\/\/medium.com\/data-science-community-srm\/from-50-to-5000-an-image-augmentation-story-1fc30111e39) blog","96a343ce":"# Keras model Stacking\nPreprocessing of data is an important process in machine learning and better handling of data always help you grab higher score in Machine Learning hackathons.  \nHowever when we talk of it's subset: **Deep Learning**, preprocessing is not extensively used.  \nThis is because neural nodes are themselves good at extracting relevant information and adjust parameters. It's amazing how a set of parallel linear computation attains so good results.  \n\nNever the less, data scientits always try to gather more data in Deep learning, in order to diversify the dataset.  \n\nA series of models are also used at times to increase score. For example in medical imaging, classification is usually done after segmentation.  \n\nAt times we observe certain pretrained model outperform other and after achieving similar score with maybe 2 or 3 models we try to implement one with less time consumed ( talking about models in pipeline )  \n\nHowever Much like my notebook explaining boosting my score with [Stacking Classifier](https:\/\/www.kaggle.com\/kabirnagpal\/feature-selection-and-stacking-f1-score-99), I've tried to explain implementation of a Stacking of several models into one.  \n**This will surely help for you to grab those final few points that might change your rank.**\n\n**Let's Get Started**"}}