{"cell_type":{"34b72e03":"code","23a5c4f7":"code","fe9cf820":"code","7df6f961":"code","7dd2817b":"code","909bcd4d":"code","37bb1d77":"code","75c337b2":"code","f511f349":"code","9c4add4c":"code","a82b4865":"code","1222fa79":"code","eefe6c8d":"code","762ba7fe":"code","c7bb2c79":"code","91c38e37":"code","5b544e26":"code","1c7b3bfa":"code","e0d2eefe":"code","a53835b2":"markdown","605e12e3":"markdown","8c2b944a":"markdown","8783e9db":"markdown"},"source":{"34b72e03":"import numpy as np\nimport pandas as pd\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport gc\n\nfrom colorama import Fore, Back, Style\n\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nc_ = Fore.CYAN\nrs_ = Style.RESET_ALL\n\nimport warnings\nwarnings.filterwarnings('ignore')\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","23a5c4f7":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv', index_col=None)\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv', index_col=None)\nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/sample_submission.csv', index_col=None)","fe9cf820":"train.shape, test.shape","7df6f961":"pd.set_option('display.max_columns', None)\ntrain","7dd2817b":"all_cols = list(train.columns[1:-1])\ntarget_count = train.groupby('target')['id'].count().reset_index()\n#target_count\ncolors = {'Class_1' : '#0722ab',\n'Class_2' : '#fdb913',\n'Class_3' : '#3d2256',\n'Class_4' : '#ef4022'}\ntarget_count.rename(columns={'id':'count'}, inplace=True)\ntarget_count['pct'] = (target_count['count'] \/ target_count['count'].sum())*100\n#target_count\ndef autopct_format(values):\n    def my_format(pct):\n        total = sum(values)\n        val = int(round(pct*total\/100.0))\n        return '{v:d}%'.format(v=val)\n    return my_format\n\nexplode = (0.05,0.05,0.05,0.05)\nfig1, ax1 = plt.subplots(1,1, figsize=(6, 6), facecolor='w', edgecolor='b')\nsizes = target_count['pct']\nlabels = target_count['target']\npatches, texts, autotexts = ax1.pie(sizes, \n          colors = [colors[key] for key in labels], \n          labels=labels, \n          autopct=autopct_format(sizes), \n          startangle=90, \n          pctdistance=0.85, \n          explode = explode,\n         textprops={'fontsize': 14,\n                   'fontfamily':'Computer Modern'\n                   })\n[text.set_color('#4a4b52') for text in texts]\n[autotext.set_color('white') for autotext in autotexts]\n[autotext.set_weight('bold') for autotext in autotexts]\n#draw circle\ncentre_circle = plt.Circle((0,0),0.70,fc='white')\nart = ax1.add_artist(centre_circle)\n# Equal aspect ratio ensures that pie is drawn as a circle\nax1 = ax1.axis('equal')  \nplt.tight_layout(pad=3.0)\nplt.subplots_adjust(top=0.91)\nplt.suptitle('Target class distribution',fontsize = 20)\nplt.show()\nall_cols = train.columns[1:-1]\ntrain[all_cols].describe().T.style.background_gradient(subset=['mean'], cmap='viridis_r')\\\n        .background_gradient(subset=['std'], cmap='viridis_r')\\\n        .background_gradient(subset=['min'], cmap='nipy_spectral')\\\n        .background_gradient(subset=['max'], cmap='binary')","909bcd4d":"test[all_cols].describe().T.style.background_gradient(subset=['mean'], cmap='viridis_r')\\\n        .background_gradient(subset=['std'], cmap='viridis_r')\\\n        .background_gradient(subset=['min'], cmap='nipy_spectral')\\\n        .background_gradient(subset=['max'], cmap='binary')","37bb1d77":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nenc = le.fit_transform(train.target)\ntrain = train.assign(target=enc)\ntrain.head()","75c337b2":"X = train[all_cols]\ny = train['target']","f511f349":"from imblearn.over_sampling import KMeansSMOTE\nfrom sklearn.cluster import MiniBatchKMeans\nfor label, count in zip(*np.unique(train['target'], return_counts=True)):\n    print('Class {} has {} samples'.format(label, count))\n\nkmeans_smote = KMeansSMOTE(\n    sampling_strategy = 'not majority',\n    random_state = 42,\n    k_neighbors = 10,\n    cluster_balance_threshold = 0.1,\n    kmeans_estimator = MiniBatchKMeans(n_clusters=100, random_state=42)\n    #kmeans_estimator = 100\n)\nX_resampled, y_resampled = kmeans_smote.fit_resample(train[all_cols], train['target'])\n\nfor label, count in zip(*np.unique(y_resampled, return_counts=True)):\n    print('Class {} has {} samples after oversampling'.format(label, count))","9c4add4c":"X_new = pd.DataFrame(X_resampled, columns=all_cols, index=None)","a82b4865":"import optuna\nfrom functools import partial\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.metrics import log_loss\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom tqdm import tqdm\n\ndef callback(study, trial):\n    if study.best_trial.number == trial.number:\n        study.set_user_attr(key=\"best_model\", value=trial.user_attrs[\"best_model\"])\n\ndef optimize(trial, X, y, n_splits):\n \n    n_estimators = trial.suggest_int(\"n_estimators\",500,2500)\n    max_depth = trial.suggest_int(\"max_depth\",10,25)\n    learning_rate = trial.suggest_uniform(\"learning_rate\", 0.01, 0.5)\n    gamma = trial.suggest_uniform(\"gamma\", 0.05, 0.8)\n    subsample = trial.suggest_uniform(\"subsample\", 0.5, 0.8)\n    min_child_weight = trial.suggest_uniform(\"min_child_weight\", 0.5, 3)\n    reg_lambda = trial.suggest_uniform(\"reg_lambda\", 1.3, 2.3)\n    reg_alpha = trial.suggest_uniform(\"reg_alpha\", 1.5, 2.2)\n    colsample_bytree = trial.suggest_uniform(\"colsample_bytree\", 0.25, 0.8)\n\n    params = dict(use_label_encoder=False,\n                  eval_metric='logloss',\n                  objective='multi:softmax',\n                  n_estimators = n_estimators,\n                  max_depth = max_depth,\n                  learning_rate = learning_rate,\n                  gamma = gamma,\n                  subsample = subsample,\n                  min_child_weight = min_child_weight,\n                  reg_lambda = reg_lambda,\n                  reg_alpha = reg_alpha,                  \n                  colsample_bytree = colsample_bytree,\n                  random_state=42)\n\n    gpu_params = dict(tree_method='gpu_hist',gpu_id=0)\n    params.update(gpu_params)\n\n    model = XGBClassifier(**params)\n    \n    strat_split = StratifiedShuffleSplit(n_splits=n_splits, \n                                         test_size = 0.2, \n                                         random_state=42)\n    lg_loss = []\n    for fold, (train_idx, test_idx) in tqdm(enumerate(strat_split.split(X=X, y=y))):\n        X_train = X.loc[train_idx]\n        y_train = y.loc[train_idx]      \n        X_val = X.loc[test_idx]\n        y_val = y.loc[test_idx]\n\n        #model.fit(X_train, y_train,eval_set=[(X_val,y_val)], early_stopping_rounds=100)\n        model.fit(X_train, y_train)\n        preds = model.predict_proba(X_val)\n        fold_lgloss = log_loss(y_val,preds)\n        lg_loss.append(fold_lgloss)\n\n    print(f\"{y_}Mean log_logss : {np.mean(lg_loss)}{rs_}\")\n    trial.set_user_attr(key=\"best_model\", value=model)\n    return np.mean(lg_loss)","1222fa79":"num_trails = 25\nstudy = optuna.create_study(direction='minimize', study_name='tps-may2021-xgboost-optuna')\noptimization_function = partial(optimize, X=X_new, y=y_resampled, n_splits=5)\nstudy.optimize(optimization_function,n_trials=num_trails, callbacks=[callback])","eefe6c8d":"print('Best trial: score {}, params {}'.format(study.best_trial.value, study.best_trial.params))\n","762ba7fe":"study.best_trial.params","c7bb2c79":"params = dict(use_label_encoder=False,\n                  eval_metric='logloss',\n                  objective='multi:softmax',\n              verbosity=1,\n              random_state=42)\nparams.update(study.best_trial.params)\ngpu_params = dict(tree_method='gpu_hist',gpu_id=0)\nparams.update(gpu_params)\nparams","91c38e37":"#best_model=study.user_attrs[\"best_model\"]\nbest_model = XGBClassifier(**params)\nbest_model.fit(X_new,y_resampled)","5b544e26":"predictions = best_model.predict_proba(test[all_cols])","1c7b3bfa":"submit = pd.DataFrame(predictions, columns = [\"Class_1\", \"Class_2\", \"Class_3\", \"Class_4\"])\nsubmit['id'] = submission['id']","e0d2eefe":"submit.to_csv(\"xgboost_baseline.csv\", index = False)","a53835b2":"# Train XGBoost with best parameters","605e12e3":"# K-Means and SMOTE","8c2b944a":"# Optuna + XGBoost ","8783e9db":"# Predict"}}