{"cell_type":{"50e76e69":"code","626dc2db":"code","f3e9775e":"code","b5141c81":"code","44044109":"code","597017b6":"code","9041dbf4":"code","0f37f701":"code","6760476e":"code","bf147fee":"code","ce5e9124":"code","02643a6b":"code","5c37e716":"code","fe8f946b":"code","c5571dfd":"code","2d7e5f57":"markdown","4b46b9d0":"markdown","226f7afe":"markdown","ee520b85":"markdown","aaf769e4":"markdown","8b93dcaa":"markdown","ce02c57a":"markdown","7318770d":"markdown","91b32498":"markdown","a1db0708":"markdown","cbafdc4b":"markdown","67a70aab":"markdown","a469a17e":"markdown","de6da8d3":"markdown"},"source":{"50e76e69":"import numpy as np \nimport pandas as pd \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_curve,roc_auc_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom xgboost import XGBClassifier","626dc2db":"train = pd.read_csv(\"\/kaggle\/input\/ml4einductions\/train.csv\")\n# convert the yes no labels to 0 and 1, where 0==No and 1==yes\ntrain[\"y\"] = train[\"y\"].apply(lambda x: 0 if x==\"no\" else 1)\n\ntest = pd.read_csv(\"\/kaggle\/input\/ml4einductions\/testing.csv\")","f3e9775e":"train.head()","b5141c81":"categorical = ['Nine_to_Five','padhai','Default','Housing','loan','day','month','poutcome']\nnumerical = ['age', '$$$Paisa', 'duration', 'campaign', 'pdays', 'previous']","44044109":"for i in categorical:\n    le=LabelEncoder()          #label encoding the categorical variables in order to transform \n    train[i]=le.fit_transform(train[i])    # it into suitable form to feed into the model\n    test[i]=le.transform(test[i])","597017b6":"X = train.loc[:,categorical+numerical]\ny = train.loc[:,'y']\n\nX_train,X_val,Y_train,Y_val = train_test_split(X,y,test_size=0.3,random_state=101)\nXGB = XGBClassifier()\nXGB.fit(X_train,Y_train)","9041dbf4":"pred = XGB.predict_proba(X_val)[:,1]","0f37f701":"fpr1,tpr1,thresh1 = roc_curve(Y_val,pred)","6760476e":"sns.lineplot(x=fpr1,y=tpr1,color=\"orange\");\nplt.ylabel(\"TPR\")\nplt.xlabel(\"FPR\")\nplt.title(\"ROC Curve with Probability predictions\");","bf147fee":"pred_labels = XGB.predict(X_val)","ce5e9124":"fpr2,tpr2,thresh2= roc_curve(Y_val,pred_labels)","02643a6b":"sns.lineplot(x=fpr2,y=tpr2,color=\"orange\");\nplt.ylabel(\"TPR\")\nplt.xlabel(\"FPR\")\nplt.title(\"ROC Curve with Label predictions\");","5c37e716":"print(\"AUC ROC score with probability prediction: {}\".format(roc_auc_score(Y_val,pred)))","fe8f946b":"print(\"AUC ROC score with label prediction: {}\".format(roc_auc_score(Y_val,pred_labels)))","c5571dfd":"prediction = XGB.predict_proba(test.loc[:,categorical+numerical])[:,1]\n\n#create a dataframe in the format mentioned in sample_submission.csv file\nsub = pd.DataFrame.from_dict({'ID':test.loc[:,\"ID\"],\"y\":prediction})\n#save your submission in csv format\nsub.to_csv(\"submission.csv\",index=False)","2d7e5f57":"# Let's Plot the ROC curves","4b46b9d0":"## Predicting probability of the positive class","226f7afe":"Once you have created your submission file you can save a version of your notebook and directly submit to the competition. If you are not using kaggle notebooks you can also download the CSV file and submit it in the `Submit Predictions` tab in the competition page","ee520b85":"# Model training\nI am using XGBOOST here","aaf769e4":"## Label Encoding of categorical features","8b93dcaa":"As you can see for the same model, the AUC roc score is 0.91 when you predict probabilty and 0.79 when you predict class labels!","ce02c57a":"We discussed about predicting positive class probabilty instead of class labels isn't it? Well let's see what happens in both cases","7318770d":"This is all from my side.\n## GOOD LUCK!!","91b32498":"# Segregating features into categorical and continuous","a1db0708":"# Predicting on the test set and Making a submission","cbafdc4b":"You can clearly see that the curves are different. This is because if you predict class labels, then there is no point in setting probability thresholds and therefore the ROC would be incorrect","67a70aab":"# What is ROC?\nThe evaluation metric for this competition is ROC. But what is exactly is ROC?\ud83e\udd14 Let's try and understand shall we?<br>\nBefore we understand ROC, it is essential that we understand True Positive Rate and False Positive Rate properly. For simplicity let us assume a binary classification problem where the test set has only 5 entries. 0 represents the `Negative` class and 1 represents `Positive` class.\n<table>\n    <tr>\n        <th>Id<\/th>\n        <th>Y<\/th>\n    <\/tr>\n    <tr>\n        <td>0\n        <td>0\n    <\/tr>\n    <tr>\n        <td>1\n        <td>1\n    <\/tr>\n    <tr>\n        <td>2\n        <td>0\n    <\/tr>\n    <tr>\n        <td>3\n        <td>0\n    <\/tr>\n    <tr>\n        <td>4\n        <td>1\n    <\/tr>\n <\/table>\n Now assume that I have trained a model which gives the following predictions\n <table>\n    <tr>\n        <th>Id<\/th>\n        <th>Y_pred<\/th>\n    <\/tr>\n    <tr>\n        <td>0\n        <td>0\n    <\/tr>\n    <tr>\n        <td>1\n        <td>0\n    <\/tr>\n    <tr>\n        <td>2\n        <td>1\n    <\/tr>\n    <tr>\n        <td>3\n        <td>0\n    <\/tr>\n    <tr>\n        <td>4\n        <td>1\n    <\/tr>\n <\/table>\n\nIf you observe closely, you'll notice that samples with Id 0, 3 and 4 have been correctly classified. Let us look at each sample individually.<br>\n`Id 0`: Sample of negative class classified correctly. Therfore a True Negative<br>\n`Id 1`: Sample of postitve class classified as negative. Therfore a False Negative<br>\n`Id 2`: Sample of negative class classified as positive. Therfore a False Positive<br>\n`Id 3`: Sample of negative class classified correctly. Therfore a True Negative<br>\n`Id 4`: Sample of positive class classified correctly. Therfore a True Positive<br>\n\nNow the True Positive Rate (TPR) also called Recall or Sensitivity is given as:<br>\n<img src=\"https:\/\/miro.medium.com\/max\/444\/1*HgxNKuUwXk9JHYBCt_KZNw.png\"><br>\n<br>In our example, `TP=1`, and `FN=1`. Hence TPR=(1\/(1+1))=0.5\n<br> False Positive Rate (FPR) is given as:<br>\n<img src=\"https:\/\/wikimedia.org\/api\/rest_v1\/media\/math\/render\/svg\/c5119dc2a74e72317ac2274c5b0d4d562597d8af\"><br>\n<br>In our example `FP=1` and `TN=2`. Therefore FPR = 0.33 <br><br>\nROC is the plot of TPR vs FPR and AUC ROC is nothing but the are under the ROC curve. But woaahhhh wait a second. Didn't we just calculate single values for both TPR and FPR. So how do you get a graph. This is the part about ROC that many people seem to miss.<br>\nWhile calculating ROC, the predicted values are regarded as the probability values for the positive class. If you select a probability threshold, you can assign the predicted value to the positive or negative class. For example, let my probability threshold be 0.5. So if my model predicts a probability of 0.67 for the positive class, the sample is assigned to +ve class(1) since 0.67>0.5 . If for example my model predicts 0.39 probability for the positive class, the sample is assigned to -ve class (0) since 0.39<0.5. What ROC does is that it varies the thresholds and for each threshold records the TPR and FPR. Therfore I am able to obtain a Graph for ROC.\n<br><br>\n#### Note:\n`In this competition too instead of predicting the class labels, you should predict the probability for the positive class (1)`. If you predict class labels instead, you'll get a drastically lower AUC ROC score. Why? Well after such a detailed explanation I'm sure you'll figure out why this will happen.\n\n<br><br>To help you get started with this competition with a bang, I have written out this notebook for you. Do read on.\n","a469a17e":"# Reading the data","de6da8d3":"## Predicting class labels instead of probability"}}