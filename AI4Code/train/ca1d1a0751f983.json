{"cell_type":{"b591d5ed":"code","2080d678":"code","d3e3f729":"code","d4c2772f":"code","26a7be7f":"code","9181e7ac":"code","671fadbb":"code","c0874686":"code","e8e5ef39":"code","07e0680a":"code","d253d660":"code","a0e6f14c":"code","9136a0ed":"code","7ff15722":"code","6c665b58":"code","77f034fb":"code","c44731c4":"code","2ffd5e25":"code","bd0eec80":"code","e3df07a1":"code","294f197f":"code","cf3c4300":"code","5aa848c7":"code","3567a48b":"code","dba96d33":"code","7a132170":"code","9284c632":"code","9a54a0bd":"code","614a6ccb":"code","75c0b3ae":"code","a1e2d714":"code","ea2dc243":"code","453df9f3":"code","b223ebc5":"code","2906cc8b":"code","896945d4":"code","f86b9b79":"code","c3145f09":"markdown"},"source":{"b591d5ed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nfrom fastai.vision import *\nfrom sklearn.model_selection import StratifiedShuffleSplit\n# Any results you write to the current directory are saved as output.\nimport warnings\nwarnings.simplefilter(\"ignore\")\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=1, gamma=2, logits=False, reduction =False):\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.logits = logits\n        self.reduction  = reduction \n    def forward(self, inputs, targets):\n        if self.logits:\n            BCE_loss = F.binary_cross_entropy_with_logits(inputs, targets, reduce =None)\n        else:\n            BCE_loss = F.binary_cross_entropy(inputs, targets, reduce =None)\n        pt = torch.exp(-BCE_loss)\n        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n\n        if self.reduction :\n            return torch.mean(F_loss)\n        else:\n            return F_loss","2080d678":"from pathlib import Path\npath=Path('..\/input')\ndf_trn=pd.read_csv(path\/'X_train.csv')\ndf_label=pd.read_csv(path\/'y_train.csv')\ndf_test=pd.read_csv(path\/'X_test.csv')\n","d3e3f729":"df_all=pd.concat([df_trn,df_test])\ndf_all['train']=['train']*len(df_trn)+['test']*len(df_test)","d4c2772f":"df_all.columns","26a7be7f":"import seaborn as sns","9181e7ac":"sns.pairplot(df_all.sample(frac=0.05),hue='train',vars=['orientation_X','orientation_Y','orientation_Z','orientation_W'])","671fadbb":"sns.pairplot(df_all.sample(frac=0.05),hue='train',vars=['angular_velocity_X', 'angular_velocity_Y', 'angular_velocity_Z'])","c0874686":"sns.pairplot(df_all.sample(frac=0.05),hue='train',vars=['linear_acceleration_X', 'linear_acceleration_Y', 'linear_acceleration_Z'])","e8e5ef39":"cols=['linear_acceleration_X','linear_acceleration_Y','linear_acceleration_Z']\nfor col in cols:\n\n    df_trn[col]=(df_trn[col])\/(85)\n    df_test[col]=(df_test[col])\/(85)\ncols=['orientation_X','orientation_Y','orientation_Z','orientation_W','angular_velocity_X', 'angular_velocity_Y', 'angular_velocity_Z','linear_acceleration_X','linear_acceleration_Y','linear_acceleration_Z']\n","07e0680a":"test_list=df_test.groupby('series_id')\ntrain_list=df_trn.groupby('series_id')","d253d660":"def open_image(self,i):\n    mn=(np.hstack([self.items[i][1]['measurement_number'][:,None] for j in range(8)])-64)\/256\n    #mn=np.zeros_like(mn)\n    feats=np.hstack([self.items[i][1][cols] for j in range(6)])\n    feats_mean=(feats-feats.mean())\/feats.std()\n    img= np.append(np.append(feats,mn,axis=1),feats_mean,axis=1)\n    img=np.stack([img,img.T,img[::-1,::-1]],axis=2)\n    return pil2tensor(img,np.float32)","a0e6f14c":"ImageList.get=open_image","9136a0ed":"src=(ImageList(df_trn.groupby('series_id'),inner_df=df_label).split_by_rand_pct(0.2).label_from_df(cols='surface'))\nsrc.add_test(test_list,label='concrete');\ndata=src.databunch(bs=32)\nstats=data.batch_stats()\ndata.normalize(stats);","7ff15722":"data.show_batch()","6c665b58":"data.show_batch(ds_type=DatasetType.Test)","77f034fb":"X=list(df_trn.groupby('series_id').indices.keys())\nlabel,group=df_label.set_index('series_id').loc[df_trn.groupby('series_id').indices.keys(),'surface'],df_label.set_index('series_id').loc[df_trn.groupby('series_id').indices.keys(),'group_id']\n","c44731c4":"sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\nsss.get_n_splits(X, label,group)\nidx_train,idx_val = next(sss.split(X, label,group)\narch=models.resnet50","2ffd5e25":"idx_train,idx_val = next(sss.split(X, label,group))\nsrc=(ImageList(train_list,inner_df=df_label).split_by_idxs(idx_train,idx_val).label_from_df(cols='surface'))\nsrc.add_test(test_list);\ndata=src.databunch(bs=32)\nstats=data.batch_stats()\ndata.normalize(stats)\nlearn=cnn_learner(data,base_arch=arch,pretrained=False)#,loss_func=FocalLoss(logits=True,gamma=1))\nlearn.lr_find(num_it=200)\nlearn.recorder.plot()","bd0eec80":"df_sub=pd.read_csv(path\/'sample_submission.csv')","e3df07a1":"src_list=ImageList(df_trn.groupby('series_id'),inner_df=df_label)\n#for i,(idx_train,idx_val) in enumerate(sss.split(np.unique(df_trn.series_id), df_label.surface)):\nsss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\nsss.get_n_splits(X, label,group)","294f197f":"def accuracy_mult(input:Tensor, targs:Tensor)->Rank0Tensor:\n    \"Compute accuracy with `targs` when `input` is bs * n_classes.\"\n    n = targs.shape[0]\n    input = input.argmax(dim=-1).view(n,-1)\n    targs = targs.argmax(dim=-1).view(n,-1).long()\n    return (input==targs).float().mean()","cf3c4300":"target_probs=[]","5aa848c7":"for i,(idx_train,idx_val) in enumerate(sss.split(X, label,group)):\n    src=(src_list.split_by_idxs(idx_train,idx_val).label_from_df(cols='surface'))\n    src.add_test(test_list);\n    data=src.databunch(bs=32)\n    stats=data.batch_stats()\n    data.normalize(stats)\n    learn=cnn_learner(data,base_arch=arch,pretrained=False,metrics=[accuracy])#,loss_func=FocalLoss(logits=True,gamma=1))\n    learn.fit_one_cycle(15,max_lr=slice(5e-4,5e-3))\n    learn.recorder.plot()\n    learn.recorder.plot_losses()\n    x,y=learn.get_preds(ds_type=DatasetType.Test)\n    target_probs.append(x)\n    ","3567a48b":"preds,y,losses = learn.get_preds(with_loss=True)\ninterp = ClassificationInterpretation(learn, preds, y, losses)","dba96d33":"interp.plot_confusion_matrix(figsize=(16,9))","7a132170":"np.unique(df_label.surface,return_counts=True)","9284c632":"def sigmoid(x, derivative=False):\n    sigm = 1. \/ (1. + np.exp(-x))\n    if derivative:\n        return sigm * (1. - sigm)\n    return sigm\ntarget_probs_stacked=np.stack(target_probs,axis=2)\ntarget_probs_stacked","9a54a0bd":"x=(target_probs_stacked).mean(axis=2)\ndf_sub['mean']=[learn.data.classes[idx] for idx in np.argmax(x,axis=1)]","614a6ccb":"x=np.median((target_probs_stacked),axis=2)\ndf_sub['median']=[learn.data.classes[idx] for idx in np.argmax(x,axis=1)]","75c0b3ae":"x=(target_probs_stacked).max(axis=2)\ndf_sub['max']=[learn.data.classes[idx] for idx in np.argmax(x,axis=1)]","a1e2d714":"x=target_probs_stacked[(target_probs_stacked<target_probs_stacked.max(axis=2)[:,:,None])&(target_probs_stacked>target_probs_stacked.min(axis=2)[:,:,None])].reshape(list(target_probs_stacked.shape[0:2])+[3])","ea2dc243":"x=x.mean(axis=2)","453df9f3":"df_sub['truncated']=[learn.data.classes[idx] for idx in np.argmax(x,axis=1)]","b223ebc5":"df_sub=df_sub.drop(columns='surface')","2906cc8b":"df_sub=df_sub.set_index('series_id')","896945d4":"df_sub['mean'].to_csv('mean.csv',header=['surface'])\ndf_sub['median'].to_csv('median.csv',header=['surface'])\ndf_sub['max'].to_csv('max.csv',header=['surface'])\ndf_sub['truncated'].to_csv('truncated.csv',header=['surface'])","f86b9b79":"for i in range(5):\n    df_sub[f'sub_{i}']=[learn.data.classes[idx] for idx in np.argmax(target_probs_stacked[:,:,i],axis=1)]\n    df_sub[f'sub_{i}'].to_csv(f'sub{i}.csv',header=['surface'])\n","c3145f09":"# Mean"}}