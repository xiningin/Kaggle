{"cell_type":{"31c407c8":"code","b306b358":"code","4c288389":"code","a9f2b705":"code","84dc40ab":"code","63295b55":"code","2eab0d2b":"code","4fdd1e03":"code","52997d27":"code","a81c47c0":"code","b0b6cd88":"code","adccbb94":"code","a04961cd":"code","3e52fc91":"code","d493e5e8":"code","f9f2c30d":"code","d8db7350":"code","04273a57":"code","04b6e20d":"code","9d199299":"code","586520dc":"code","6152a34c":"code","b208134b":"code","31106703":"code","655f57d1":"code","69ead55d":"code","e46aa1ca":"code","e444f723":"code","4f696a41":"markdown","bbe043f9":"markdown"},"source":{"31c407c8":"import pandas as pd\nimport numpy as np\nimport os\nimport shutil\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS \nimport re \nimport string\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense,Input,Flatten,Embedding,Lambda,Dropout,LSTM,Conv1D,Concatenate,Add\nfrom tensorflow.keras.models import Model\nfrom tqdm import tqdm\nfrom sklearn.model_selection import StratifiedKFold\nimport keras.backend as K","b306b358":"!pip install transformers","4c288389":"from transformers import  RobertaConfig, TFRobertaModel\nfrom tokenizers import Tokenizer\nfrom tokenizers.decoders import ByteLevel as ByteLevelDecoder\nfrom tokenizers.models import BPE\nfrom tokenizers.normalizers import Lowercase, NFKC, Sequence\nfrom tokenizers.pre_tokenizers import ByteLevel","a9f2b705":"train=pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ntrain.head()","84dc40ab":"#Referencia https:\/\/www.kaggle.com\/parulpandey\/eda-and-preprocessing-for-bert\n\ndef clean(tweet):\n    tweet = str(tweet)\n\n    tweet=tweet.lower()\n\n    #Remove html tags\n    tweet=re.sub('<.*?>','',tweet)\n\n    #Remove text in square brackets\n    tweet=re.sub('\\[.*?\\]','',tweet)\n\n    #Remove hyperlinks\n    tweet=re.sub('https?:\/\/\\S+|www\\.\\S+','',tweet)\n\n\n    return tweet","63295b55":"train.dropna(inplace = True)\ntrain[\"text\"] = train[\"text\"].apply(lambda x : x.strip())\ntrain[\"selected_text\"] = train[\"selected_text\"].apply(lambda x : x.strip())","2eab0d2b":"from sklearn.model_selection import train_test_split\n\nX_train,X_val,Y_train,Y_val=train_test_split(train,train['sentiment'],\n                                              test_size=0.2,random_state=42,stratify=train['sentiment'])\nX_train,X_test,Y_train,Y_test=train_test_split(X_train,Y_train,\n                                               test_size=0.2,random_state=42,stratify=X_train['sentiment'])\n\nX_train.reset_index(inplace=True,drop=True)\nX_val.reset_index(inplace=True,drop=True)\nX_test.reset_index(inplace=True,drop=True)\n\nY_train=Y_train.reset_index(drop=True)\nY_val=Y_val.reset_index(drop=True)\nY_test=Y_test.reset_index(drop=True)\n\nprint('X_train shape',X_train.shape,' Y_train shape ',Y_train.shape)\nprint('X_val shape',X_val.shape,' Y_val shape ',Y_val.shape)\nprint('X_test shape',X_test.shape,' Y_test shape ',Y_test.shape)","4fdd1e03":"\n# Definici\u00f3n de variables generales\nMAX_LEN = 128\ntokenizer = Tokenizer(BPE.from_file('..\/input\/tf-roberta\/vocab-roberta-base.json', '..\/input\/tf-roberta\/merges-roberta-base.txt'))\ntokenizer.pre_tokenizer = ByteLevel()\ntokenizer.decoder = ByteLevelDecoder()","52997d27":"resPos = tokenizer.encode('How I new that thing')\nresNeg = tokenizer.encode('negative')\nresNeu = tokenizer.encode('neutral')","a81c47c0":"print(f'Representaci\u00f3n ID positive: {resPos.ids}')\nprint(f'Representaci\u00f3n ID negative: {resNeg.ids}')\nprint(f'Representaci\u00f3n ID neutral: {resNeu.ids}')","b0b6cd88":"tokenizer.decode(resPos.ids)","adccbb94":"# Definici\u00f3n de sentimientos basados en el diccionario de RoBERTa\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","a04961cd":"\n#Referencia: https:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705\n\ndef createInputData(data,tokenizer):\n\n    row = data.shape[0]\n    input_ids = np.ones((row,MAX_LEN),dtype='int32')\n    attention_mask = np.zeros((row,MAX_LEN),dtype='int32')\n    token_type_ids = np.zeros((row,MAX_LEN),dtype='int32')\n    start_tokens = np.zeros((row,MAX_LEN),dtype='int32')\n    end_tokens = np.zeros((row,MAX_LEN),dtype='int32')\n\n    for k in range(data.shape[0]):\n        # B\u00fasqueda del indice inicial\n        text1 = \" \"+\" \".join(data.loc[k,'text'].split())\n        text2 = \" \".join(data.loc[k,'selected_text'].split())\n        idx = text1.find(text2)\n        # Selecci\u00f3n de las posiciones que ocupan los textos\n        chars = np.zeros((len(text1)))\n        chars[idx:idx+len(text2)]=1\n\n        if text1[idx-1]==' ': \n            chars[idx-1] = 1 \n\n        # Codificaci\u00f3n del texto completo\n        enc = tokenizer.encode(text1) \n\n        # Encuentro de offsets\n        token_offsets=[]\n        idx=0\n        for i in enc.ids:\n            word=tokenizer.decode([i])\n            token_offsets.append((idx,idx+len(word)))\n            idx+=len(word)\n\n        # Definici\u00f3n de tokens de inicio y finalizaci\u00f3n\n        target_idx = []\n        for i,(o1,o2) in enumerate(token_offsets):\n            if(sum(chars[o1:o2])>0):\n                target_idx.append(i)  \n        s_tok = sentiment_id[data.loc[k,'sentiment']]\n\n        input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n        attention_mask[k,:len(enc.ids)+5] = 1\n\n        #Se adiciona 1 a los tokens\n        if len(target_idx)>0:\n            start_tokens[k,target_idx[0]+1] = 1\n            end_tokens[k,target_idx[-1]+1] = 1\n\n    return (input_ids,attention_mask,token_type_ids,start_tokens,end_tokens)","3e52fc91":"#Convertir los datos de validaci\u00f3n y pruebas en la entrada de RoBERTa\nX_tr1,X_tr2,X_tr3,Y_tr1,Y_tr2=createInputData(X_train,tokenizer)\nX_val1,X_val2,X_val3,Y_val1,Y_val2=createInputData(X_val,tokenizer)\nX_te1,X_te2,X_te3,Y_te1,Y_te2=createInputData(X_test,tokenizer)","d493e5e8":"# Arquitectura de red reunal con RoBERTa\ndef build_model():\n    '''Builds the model'''\n\n    ids=Input((MAX_LEN),name='ids',dtype='int32')\n    att_mask=Input((MAX_LEN),name='att_mask',dtype='int32')\n    type_ids=Input((MAX_LEN),name='type_ids',dtype='int32')\n\n    roberta_conf = RobertaConfig.from_pretrained('roberta-base')\n    roberta_model = TFRobertaModel.from_pretrained('roberta-base',config=roberta_conf)\n\n    bert_output=roberta_model([ids,att_mask,type_ids])\n\n    dropout1=Dropout(0.1,name='dropout1')(bert_output[0])\n    conv1d_1 = Conv1D(1,1,kernel_initializer=tf.keras.initializers.glorot_uniform(seed=20),name='conv1d_1')(dropout1)\n    flatten_1 = Flatten(name='flatten_1')(conv1d_1)\n    out_1 = tf.keras.layers.Activation('softmax',name='activation1')(flatten_1)\n\n    dropout2=Dropout(0.1,name='dropout2')(bert_output[0])\n    conv1d_2 = Conv1D(1,1,kernel_initializer=tf.keras.initializers.glorot_uniform(seed=20),name='conv1d_2')(dropout2)\n    flatten_2 = Flatten(name='flatten_2')(conv1d_2)\n    out_2 = tf.keras.layers.Activation('softmax',name='activation2')(flatten_2)\n\n    model1 = Model(inputs=[ids, att_mask, type_ids], outputs=[out_1,out_2])\n\n    return model1\n  \nmodel=build_model()","f9f2c30d":"model.summary()","d8db7350":"tf.keras.utils.plot_model(model, '.\/ModeloRobertaRN.png',show_shapes=True)","04273a57":"import os\nif not os.path.exists('.\/model-roberta'):\n    os.makedirs('.\/model-roberta')","04b6e20d":"# Par\u00e1metros de RoBERTA para tensorboard\nfrom tensorflow.keras.callbacks import TensorBoard\n%load_ext tensorboard\n!rm -rf .\/logs\/ \n\nlog_dir='.\/model-roberta'\ntensorboard_callback = TensorBoard(log_dir=log_dir,histogram_freq=1, write_graph=True)","9d199299":"from tensorflow.keras.callbacks import ModelCheckpoint\n\ncheckpoint=ModelCheckpoint('.\/model-roberta\/roberta.h5', monitor='val_loss', verbose=1, save_best_only=True,\n        save_weights_only=True, mode='auto', save_freq='epoch')","586520dc":"# Compilaci\u00f3n del modelo con optimizador Adam\noptimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)","6152a34c":"# Entrenamiento del modelo\ncallback=[tensorboard_callback,checkpoint]\nmodel.fit([X_tr1,X_tr2,X_tr3],[Y_tr1,Y_tr2],\n           validation_data=([X_val1,X_val2,X_val3],[Y_val1,Y_val2]),\n           batch_size=32,epochs=4,callbacks=callback)","b208134b":"# Predecir con los datos de texto\nstart,end=model.predict([X_te1,X_te2,X_te3])","31106703":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","655f57d1":"\ndef find_selected_text(data,tokenizer,start,end):\n    '''Finds the selected text for the given tweet'''\n    selected_text_list=[]\n    for i in range(data.shape[0]):\n\n        # B\u00fasqueda de index\n        start_idx=np.argmax(start[i])\n        end_idx=np.argmax(end[i])\n\n\n        # Encuentra el texto de la predicci\u00f3n a partir de los indices\n        if (start_idx>end_idx):\n            predicted_text=data.loc[i,'text']\n\n        else:\n            text1 = \" \"+\" \".join(data.loc[i,'text'].split())\n            tokens=tokenizer.encode(text1)\n            predicted_text=tokenizer.decode(tokens.ids[start_idx-1:end_idx])        \n\n        selected_text_list.append(predicted_text)\n\n    return selected_text_list\n    \n","69ead55d":"selected_text=find_selected_text(X_test,tokenizer,start,end)\nX_test['predicted_text']=selected_text\n\nfor i,(_,row) in enumerate(X_test.iterrows()):\n    X_test.loc[i,'jaccard']=jaccard(row.selected_text,row.predicted_text)\n\nX_test.head(10)","e46aa1ca":"# Promedio del indice de Jaccard\n\npos_average=np.mean(X_test['jaccard'][X_test['sentiment']=='positive'])\nprint('Promedio del indice de jaccard para los sentimientos positivos  ',pos_average)\n\nneg_average=np.mean(X_test['jaccard'][X_test['sentiment']=='negative'])\nprint('Promedio del indice de jaccard para los sentimientos negativos  ',neg_average)\n\nneu_average=np.mean(X_test['jaccard'][X_test['sentiment']=='neutral'])\nprint('Promedio del indice de jaccard para los sentimientos neutrales  ',neu_average)","e444f723":"# Rendimiento general para el modelo de RoBERTa\nprint(np.mean(X_test['jaccard']))","4f696a41":"# RoBERTa - Modelo HuggingFace preentrenado TF ","bbe043f9":"Tratando de mejorar la primera implementaci\u00f3n de BERT con DistilBERT, usaremos el modelo entrenado de RoBERTa y la librer\u00eda transformers. El modelo pre-entrenado de RoBERTa elegido ser\u00e1 dla implementaci\u00f3n de HuggingFace preentrenado en tensorflow.."}}