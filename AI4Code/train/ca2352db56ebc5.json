{"cell_type":{"4bd7a26b":"code","0f0d1ea0":"code","b8784e3d":"code","75dfcbac":"code","3ad09146":"code","80a76224":"code","f1067fa1":"code","ceb01dec":"code","84c9605e":"code","b43f510c":"code","46f02e56":"code","68abe6c4":"code","967a8f4a":"code","3305aa14":"code","43d44869":"code","ce3e9df7":"code","85d85e4c":"code","9108b9cd":"code","18831f0e":"code","adf471f2":"code","71be229c":"code","67d981e3":"code","7b285fe6":"code","f351d3ea":"code","e9435e5b":"code","99910a74":"code","83c8492f":"code","c1851136":"code","f29bae25":"code","05477013":"code","1e098f67":"code","99e2ca2d":"code","de592624":"code","e087fe1a":"code","bc3c619b":"code","7751d9d3":"code","3b1d80fe":"code","97de6e1e":"code","44c5be81":"code","4a88ab77":"code","11b6c3ae":"code","3baa5bf9":"code","a555b561":"code","3fa4a56e":"code","d5f66d6c":"code","d2b9a29f":"code","ab1c7103":"code","5f950990":"code","65b49771":"code","8fbe222d":"code","8483a76d":"code","8ef42715":"code","75f8a2f9":"code","6cfe9387":"code","e4e7c5ed":"code","042bb458":"code","f5848825":"code","f36d351f":"code","ffb997fc":"code","fda353c5":"code","43134e49":"code","e26888af":"code","2283d781":"code","f42408fb":"code","87c68850":"code","0e32c733":"code","37ab9a4a":"code","550d19d3":"code","b5af0893":"code","7827116b":"code","73d40965":"code","c80f3676":"code","540a22cc":"code","7281c77e":"code","16b31e7f":"code","5f5cd87b":"code","c24d8464":"code","8d59713b":"code","91e2d82c":"code","10859710":"code","29449b79":"code","8fefd4d1":"code","a8b95128":"code","166b1a42":"code","79224eae":"code","025c8987":"code","38d30389":"code","0ac344f9":"code","f3e2d9c8":"code","99800bb9":"code","2013a6f9":"code","6b99e2d2":"code","e5736ed8":"code","a8825ef0":"code","49f7357f":"code","1f100f0b":"code","0155b21d":"code","0408cc45":"code","f1093fcb":"code","67d403be":"code","a7881177":"code","b05fbf1c":"code","9a729120":"code","7f902be4":"code","731c2443":"code","5853f917":"code","e6b114f5":"code","0bd27a51":"code","258b7d15":"code","e7e0b569":"code","61c68c2f":"code","4ed108ec":"code","a71acce6":"code","0b922d07":"code","5830e126":"code","99c6249c":"code","cac6067a":"code","92e1d3fa":"code","b7465ed3":"code","e23821bb":"code","a1807440":"code","a0106588":"markdown","66b1daaf":"markdown","0cb2794f":"markdown","181bd11d":"markdown","6a4038e0":"markdown","b008559f":"markdown","b1b80cf4":"markdown","b7dd2753":"markdown","8975771f":"markdown","62c0fdec":"markdown","c475ee29":"markdown"},"source":{"4bd7a26b":"#  Python 3 environment as defined by the kaggle\/python Docker \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\n\n#obtain data file names\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0f0d1ea0":"#First, open CSV file as a pandas dataframe\ndf = pd.read_csv('..\/input\/ufo-sightings-1969-to-2019\/nuforc_reports.csv')\n#visualize the first 5 rows of the dataframe\ndf.head()","b8784e3d":"#dropping columns I wont use\ndf1=df.drop( ['stats', 'posted', 'text'], axis=1)\ndf1.head(2)","75dfcbac":"df1.info()","3ad09146":"#explore the data\n#display cities with most UFO sighting reports\ndf1['city'].value_counts(dropna=False)","80a76224":"#verify the data is for North America\ndf1['state'].unique()","f1067fa1":"#looking for entries outside of North America\ndf1['state'].value_counts(dropna=False)","ceb01dec":"#5235 state entries are NaN\n#verify these are locations in North America\n\ndf1[df1['state'].isna()]","84c9605e":"#remove those rows with NaNs in the state column \ndf1.dropna(subset=['state'], how='all', inplace=True)\n\n#remove those rows with NaNs in the date and time column \ndf1.dropna(subset=['date_time'], how='all', inplace=True)\ndf1","b43f510c":"#find those rows with no latitude or longitude data\n\nneedData=df1[df1['city_latitude'].isna()]\nneedData","46f02e56":"# applying groupby() function to \n# group the data on city value. \ncities = df1.groupby('city') \n\n# Finding the values contained in the \"Peoria\" cities \nPeoria=cities.get_group('Peoria') \nPeoriaNY=Peoria.groupby('state')\n\n#looking for other Peoria, NY reports \n#to copy their lat and lon\nPeoriaNY=PeoriaNY.get_group('NY') \nPeoriaNY","68abe6c4":"#entries per state that need lat and lon\nneedData.groupby('state')[\"city\"].count() ","967a8f4a":"Alberta=needData[needData.state == 'AB']\nAlberta.head()","3305aa14":"df1Alberta=df1[df1.city == 'Lethbridge']\ndf1Alberta.head()","43d44869":"#replace NaN of rows from this city without coordinates\n#Example: Lethbridge, AB, Canada coordinates are 49.665183, -112.81\n## An entry error mislabed row 310's city as Lethbrdge and produced no coord.\n##in the original data source (CSV)\n## below, I fix this entry\n\ndf1.loc[df1['city'] == 'Lethbrdge', 'city_latitude'] = 49.665183\ndf1.loc[df1['city'] == 'Lethbrdge', 'city_longitude'] = -112.81\n\n#while the original row name is 310, the entry is in a new position: 261\n#verify the update of the coordinates occured\ndf1.iloc[261]","ce3e9df7":"#Reset row indexes of the DataFrame\n#to ignore those rows indexes previously removed\n\ndf2 = df1.reset_index(drop=True)\ndf2.head(2)","85d85e4c":"#some city entries are erroneously labeled by their state\n#I will remove those entries\n\n#Example: Alberta as city\ndf1Alb=df2[df2.city == 'Alberta']\ndf1Alb","9108b9cd":"#I create a new dataframe, where city does not include Alberta\n#and reset indexes\ndf2=df2[df2.city != 'Alberta'].reset_index(drop=True)\ndf2","18831f0e":"#before adding lat and lon to the city of St Albert,\n#I make sure that the city name only occurs in the state of Alberta\ndf2StA=df2[df2.city == 'Saint Albert']\ndf2StA.head(2)","adf471f2":"#add the lat and lon for the various St Albert, AB, CANADA entries\ndf2.loc[df2['city'] == 'Saint Albert', 'city_latitude'] = 53.630474\ndf2.loc[df2['city'] == 'Saint Albert', 'city_longitude'] = -113.625641","71be229c":"Wa=needData[needData.state == 'WA']\nWa['city'].value_counts()","67d981e3":"#add the lat and lon for the various WA city entries \n##I use 2 conditions to ensure I replace the right information\n\ndf2.loc[(df2[\"state\"]==\"WA\") & (df2[\"city\"]==\"South Hill\"), 'city_latitude'] = 47.1193\ndf2.loc[(df2[\"state\"]==\"WA\") & (df2[\"city\"]==\"South Hill\"), 'city_longitude'] = -122.2877","7b285fe6":"#Repeat for those cities with the most entries\n\ndf2.loc[(df2[\"state\"]==\"WA\") & (df2[\"city\"]==\"Shoreline\"), 'city_latitude'] = 47.755653\ndf2.loc[(df2[\"state\"]==\"WA\") & (df2[\"city\"]==\"Shoreline\"), 'city_longitude'] = -122.341515","f351d3ea":"df2.loc[(df2[\"state\"]==\"WA\") & (df2[\"city\"]==\"Spokane Valley\"), 'city_latitude'] = 47.6732\ndf2.loc[(df2[\"state\"]==\"WA\") & (df2[\"city\"]==\"Spokane Valley\"), 'city_longitude'] = -117.2394","e9435e5b":"df2.loc[(df2[\"state\"]==\"WA\") & (df2[\"city\"]==\"Burien\"), 'city_latitude'] = 47.4668\ndf2.loc[(df2[\"state\"]==\"WA\") & (df2[\"city\"]==\"Burien\"), 'city_longitude'] = -122.3405","99910a74":"df2.loc[(df2[\"state\"]==\"WA\") & (df2[\"city\"]==\"Des Moines\"), 'city_latitude'] = 47.4018\ndf2.loc[(df2[\"state\"]==\"WA\") & (df2[\"city\"]==\"Des Moines\"), 'city_longitude'] = -122.3243","83c8492f":"df2.loc[(df2[\"state\"]==\"WA\") & (df2[\"city\"]==\"Camano Island\"), 'city_latitude'] = 48.1740\ndf2.loc[(df2[\"state\"]==\"WA\") & (df2[\"city\"]==\"Camano Island\"), 'city_longitude'] = -122.5282","c1851136":"df2.loc[(df2[\"state\"]==\"WA\") & (df2[\"city\"]==\"Mount Rainier National Park\"), 'city_latitude'] = 46.8800\ndf2.loc[(df2[\"state\"]==\"WA\") & (df2[\"city\"]==\"Mount Rainier National Park\"), 'city_longitude'] = -121.7269","f29bae25":"#I noted that some cities have NaNs. \n#remove those rows with NaNs in the city column \ndf2.dropna(subset=['city'], how='all', inplace=True)","05477013":"#reset row indexes again\ndf2.reset_index(drop=True)","1e098f67":"#Arizona\nAZ=needData[needData.state == 'AZ']\nAZ['city'].value_counts()","99e2ca2d":"view=df2[df2.city=='Arizona']\nview","de592624":"#first, drop those cities misslabled as Arizona\n#and reset index\n#by creating a new dataframe, where city does not include Arizona\ndf2=df2[df2.city != 'Arizona'].reset_index(drop=True)","e087fe1a":"#add the lat and lon for various AZ city with several entries\n##I use 2 conditions to ensure I replace the right information\n\ndf2.loc[(df2[\"state\"]==\"AZ\") & (df2[\"city\"]==\"Lake Havasu\"), 'city_latitude'] = 34.4839\ndf2.loc[(df2[\"state\"]==\"AZ\") & (df2[\"city\"]==\"Lake Havasu\"), 'city_longitude'] = -114.3225","bc3c619b":"df2.loc[(df2[\"state\"]==\"AZ\") & (df2[\"city\"]==\"North Phoenix\"), 'city_latitude'] = 33.6894\ndf2.loc[(df2[\"state\"]==\"AZ\") & (df2[\"city\"]==\"North Phoenix\"), 'city_longitude'] = -112.0994","7751d9d3":"#Wisconsin\nWI=needData[needData.state == 'WI']\nWI['city'].value_counts()","3b1d80fe":"df2.loc[(df2[\"state\"]==\"WI\") & (df2[\"city\"]==\"Wauwatosa\"), 'city_latitude'] = 43.0495\ndf2.loc[(df2[\"state\"]==\"WI\") & (df2[\"city\"]==\"Wauwatosa\"), 'city_longitude'] = -88.0076","97de6e1e":"df2.loc[(df2[\"state\"]==\"WI\") & (df2[\"city\"]==\"West Allis\"), 'city_latitude'] = 43.0167\ndf2.loc[(df2[\"state\"]==\"WI\") & (df2[\"city\"]==\"West Allis\"), 'city_longitude'] = -88.0070","44c5be81":"#Alabama\nAL=needData[needData.state == 'AL']\nAL['city'].value_counts()","4a88ab77":"df2.loc[(df2[\"state\"]==\"AL\") & (df2[\"city\"]==\"Hoover\"), 'city_latitude'] = 33.4054\ndf2.loc[(df2[\"state\"]==\"AL\") & (df2[\"city\"]==\"Hoover\"), 'city_longitude'] = -86.8114","11b6c3ae":"df2.loc[(df2[\"state\"]==\"AL\") & (df2[\"city\"]==\"Homewood\"), 'city_latitude'] = 33.4718\ndf2.loc[(df2[\"state\"]==\"AL\") & (df2[\"city\"]==\"Homewood\"), 'city_longitude'] = -86.8008","3baa5bf9":"df2.loc[(df2[\"state\"]==\"AL\") & (df2[\"city\"]==\"Fort Morgan\"), 'city_latitude'] = 30.2256\ndf2.loc[(df2[\"state\"]==\"AL\") & (df2[\"city\"]==\"Fort Morgan\"), 'city_longitude'] = -88.0189","a555b561":"#Oregon\nOR=needData[needData.state == 'OR']\nOR['city'].value_counts()","3fa4a56e":"df2.loc[(df2[\"state\"]==\"OR\") & (df2[\"city\"]==\"Aloha\"), 'city_latitude'] = 45.4943\ndf2.loc[(df2[\"state\"]==\"OR\") & (df2[\"city\"]==\"Aloha\"), 'city_longitude'] = -122.8670","d5f66d6c":"df2.loc[(df2[\"state\"]==\"OR\") & (df2[\"city\"]==\"Chiloquin\"), 'city_latitude'] = 42.5776\ndf2.loc[(df2[\"state\"]==\"OR\") & (df2[\"city\"]==\"Chiloquin\"), 'city_longitude'] = -121.8661","d2b9a29f":"df2.loc[(df2[\"state\"]==\"OR\") & (df2[\"city\"]==\"Chiloquin\"), 'city_latitude'] = 44.3935\ndf2.loc[(df2[\"state\"]==\"OR\") & (df2[\"city\"]==\"Chiloquin\"), 'city_longitude'] = -122.9848","ab1c7103":"#British Columbia\nBC=needData[needData.state == 'BC']\nBC['city'].value_counts()","5f950990":"#The same city appears several times, mispelled\n#I added the lat and log for the different instances\n\ndf2.loc[(df2[\"state\"]==\"BC\") & (df2[\"city\"]==\"Langley\"), 'city_latitude'] = 49.1042\ndf2.loc[(df2[\"state\"]==\"BC\") & (df2[\"city\"]==\"Langley\"), 'city_longitude'] = -122.6604","65b49771":"#mispelled Langley city rows\ndf2.loc[(df2[\"state\"]==\"BC\") & (df2[\"city\"]==\"Langely\"), 'city_latitude'] = 49.1042\ndf2.loc[(df2[\"state\"]==\"BC\") & (df2[\"city\"]==\"Langely\"), 'city_longitude'] = -122.6604\n\ndf2.loc[(df2[\"state\"]==\"BC\") & (df2[\"city\"]==\"Lanley\"), 'city_latitude'] = 49.1042\ndf2.loc[(df2[\"state\"]==\"BC\") & (df2[\"city\"]==\"Lanley\"), 'city_longitude'] = -122.6604","8fbe222d":"df2.loc[(df2[\"state\"]==\"BC\") & (df2[\"city\"]==\"West Vancouver\"), 'city_latitude'] = 49.3286\ndf2.loc[(df2[\"state\"]==\"BC\") & (df2[\"city\"]==\"West Vancouver\"), 'city_longitude'] = -123.1602","8483a76d":"#re-verify for rows with no latitude or longitude data\n\nneedData2=df2[df2['city_latitude'].isna()]\nneedData2","8ef42715":"#entries per state that need lat and lon\nneedData2.groupby('state')[\"city\"].count()","75f8a2f9":"#CA\nCA=needData2[needData2.state == 'CA']\nCA['city'].value_counts()\n","6cfe9387":"df2.loc[(df2[\"state\"]==\"CA\") & (df2[\"city\"]==\"Hollywood\"), 'city_latitude'] = 34.0928\ndf2.loc[(df2[\"state\"]==\"CA\") & (df2[\"city\"]==\"Hollywood\"), 'city_longitude'] = -118.3287","e4e7c5ed":"df2.loc[(df2[\"state\"]==\"CA\") & (df2[\"city\"]==\"Eastvale\"), 'city_latitude'] = 33.9525\ndf2.loc[(df2[\"state\"]==\"CA\") & (df2[\"city\"]==\"Eastvale\"), 'city_longitude'] = -117.5848","042bb458":"df2.loc[(df2[\"state\"]==\"CA\") & (df2[\"city\"]==\"Red Bluff\"), 'city_latitude'] = 40.1785\ndf2.loc[(df2[\"state\"]==\"CA\") & (df2[\"city\"]==\"Red Bluff\"), 'city_longitude'] = -122.2358","f5848825":"#PA\n\nPA=needData2[needData2.state == 'PA']\nPA['city'].value_counts()\n","f36d351f":"df2.loc[(df2[\"state\"]==\"PA\") & (df2[\"city\"]==\"North Huntingdon\"), 'city_latitude'] = 40.3302\ndf2.loc[(df2[\"state\"]==\"PA\") & (df2[\"city\"]==\"North Huntingdon\"), 'city_longitude'] = -79.7307","ffb997fc":"df2.loc[(df2[\"state\"]==\"PA\") & (df2[\"city\"]==\"Yardley\"), 'city_latitude'] = 40.2457\ndf2.loc[(df2[\"state\"]==\"PA\") & (df2[\"city\"]==\"Yardley\"), 'city_longitude'] = -74.8460","fda353c5":"df2.loc[(df2[\"state\"]==\"PA\") & (df2[\"city\"]==\"Whitehall\"), 'city_latitude'] = 40.6572\ndf2.loc[(df2[\"state\"]==\"PA\") & (df2[\"city\"]==\"Whitehall\"), 'city_longitude'] = -75.4986","43134e49":"#AR\nAR=needData2[needData2.state == 'AR']\nAR['city'].value_counts()\n","e26888af":"df2.loc[(df2[\"state\"]==\"AR\") & (df2[\"city\"]==\"Hartman\"), 'city_latitude'] = 35.4326\ndf2.loc[(df2[\"state\"]==\"AR\") & (df2[\"city\"]==\"Hartman\"), 'city_longitude'] = -93.6155","2283d781":"#NY\nNY=needData2[needData2.state == 'NY']\nNY['city'].value_counts()","f42408fb":"df2.loc[(df2[\"state\"]==\"NY\") & (df2[\"city\"]==\"Cheektowaga\"), 'city_latitude'] = 42.9071\ndf2.loc[(df2[\"state\"]==\"NY\") & (df2[\"city\"]==\"Cheektowaga\"), 'city_longitude'] = -78.7543","87c68850":"df2.loc[(df2[\"state\"]==\"NY\") & (df2[\"city\"]==\"West Seneca\"), 'city_latitude'] = 42.8359\ndf2.loc[(df2[\"state\"]==\"NY\") & (df2[\"city\"]==\"West Seneca\"), 'city_longitude'] = -78.7539","0e32c733":"df2.loc[(df2[\"state\"]==\"NY\") & (df2[\"city\"]==\"Williamsville\"), 'city_latitude'] = 42.9639\ndf2.loc[(df2[\"state\"]==\"NY\") & (df2[\"city\"]==\"Williamsville\"), 'city_longitude'] = -78.7378","37ab9a4a":"#Manitoba\n\nMB=needData2[needData2.state == 'MB']\nMB['city'].value_counts()","550d19d3":"df2.loc[(df2[\"state\"]==\"MB\") & (df2[\"city\"]==\"Anola\"), 'city_latitude'] = 49.8812\ndf2.loc[(df2[\"state\"]==\"MB\") & (df2[\"city\"]==\"Anola\"), 'city_longitude'] = -96.6233","b5af0893":"#NM\nNM=needData2[needData2.state == 'NM']\nNM['city'].value_counts()\n","7827116b":"df2.loc[(df2[\"state\"]==\"NM\") & (df2[\"city\"]==\"Espanola\"), 'city_latitude'] = 35.9910\ndf2.loc[(df2[\"state\"]==\"NM\") & (df2[\"city\"]==\"Espanola\"), 'city_longitude'] = -106.0818","73d40965":"#NV\nNV=needData2[needData2.state == 'NV']\nNV['city'].value_counts()\n","c80f3676":"df2.loc[(df2[\"state\"]==\"NV\") & (df2[\"city\"]==\"Rachel\"), 'city_latitude'] = 37.6447\ndf2.loc[(df2[\"state\"]==\"NV\") & (df2[\"city\"]==\"Rachel\"), 'city_longitude'] = -115.7428","540a22cc":"#WV\nWV=needData2[needData2.state == 'WV']\nWV['city'].value_counts()\n","7281c77e":"df2.loc[(df2[\"state\"]==\"WV\") & (df2[\"city\"]==\"South Charleston\"), 'city_latitude'] = 38.3686\ndf2.loc[(df2[\"state\"]==\"WV\") & (df2[\"city\"]==\"South Charleston\"), 'city_longitude'] = -81.6999","16b31e7f":"df2.loc[(df2[\"state\"]==\"WV\") & (df2[\"city\"]==\"Cross Lanes\"), 'city_latitude'] = 38.4204\ndf2.loc[(df2[\"state\"]==\"WV\") & (df2[\"city\"]==\"Cross Lanes\"), 'city_longitude'] = -81.7907","5f5cd87b":"df2.loc[(df2[\"state\"]==\"WV\") & (df2[\"city\"]==\"Tomahawk\"), 'city_latitude'] = 39.5304\ndf2.loc[(df2[\"state\"]==\"WV\") & (df2[\"city\"]==\"Tomahawk\"), 'city_longitude'] = -78.0469","c24d8464":"#TX\nTX=needData2[needData2.state == 'TX']\nTX['city'].value_counts()","8d59713b":"df2.loc[(df2[\"state\"]==\"TX\") & (df2[\"city\"]==\"The Woodlands\"), 'city_latitude'] = 30.1658\ndf2.loc[(df2[\"state\"]==\"TX\") & (df2[\"city\"]==\"The Woodlands\"), 'city_longitude'] = -95.4613","91e2d82c":"df2.loc[(df2[\"state\"]==\"TX\") & (df2[\"city\"]==\"Lakeway\"), 'city_latitude'] = 30.3680\ndf2.loc[(df2[\"state\"]==\"TX\") & (df2[\"city\"]==\"Lakeway\"), 'city_longitude'] = -97.9917","10859710":"#remove the remaining rows with NaNs for latitude\ndf2.dropna(subset=['city_latitude'], how='all', inplace=True)\n\n#remove those rows with NaNs for longitude\ndf2.dropna(subset=['city_longitude'], how='all', inplace=True)\n\n#reset index\ndf2.reset_index(drop=True)","29449b79":"import datetime\n#remove time portion of date_time input\ndf2['date'] = df2['date_time'].apply(lambda x: pd.Timestamp(x).strftime('%Y-%m-%d'))\ndf2.drop('date_time', axis=1, inplace=True)","8fefd4d1":"#sorted by date\n#and reset indexes\ndf2.sort_values(by=['date'], inplace=True, ascending=True)\ndf2.reset_index(drop=True)","a8b95128":"#libraries for plotting\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns ","166b1a42":"plt.figure(figsize=(14,5))\nsns.countplot(df2['state'])","79224eae":"#get year as a new column and convert to integer\ndf2['year'] = df2['date'].apply(lambda x: pd.Timestamp(x).strftime('%Y'))\ndf2['year'].astype(int)","025c8987":"# Create a pie chart of dates with percentages\n\ndf2[\"year\"].value_counts().plot.pie(label=\"\", title=\"year\", figsize=(10, 10), autopct='%1.1f%%', subplots=True); \nplt.show(block=True);   ","38d30389":"#map plotting libraries\nimport folium\nfrom folium import plugins\nfrom folium.plugins import HeatMap","0ac344f9":"#a map of North America\nNorthAm = folium.Map(location=[54.5260, -105.2551],\n                   zoom_start = 2)\n\n# Ensure lats and longs datatype are floats\ndf2['city_latitude'] = df2['city_latitude'].astype(float)\ndf2['city_longitude'] = df2['city_longitude'].astype(float)\n\n# Filter the DF for 2014 rows\n\nheat_df = df2[df2['year']=='2014'] # Reducing data size so it runs faster\nheat_df = heat_df[['city_latitude', 'city_longitude']]\n\n# List comprehension to make a list of lists\nheat_data = [[row['city_latitude'],row['city_longitude']] for index, row in heat_df.iterrows()]\n\n# Plot it on the map\nHeatMap(heat_data).add_to(NorthAm)\n\n# Display the map\nNorthAm","f3e2d9c8":"#California sightings 1969-2019\n\nCali=df2[df2.state == 'CA']\n\n#a map of California\nInCali = folium.Map(location=[36.7783, -119.4179],\n                   zoom_start = 4)\n\nheat_df2 = Cali[['city_latitude', 'city_longitude']]\n\n# List comprehension to make a list of lists\nheat_data2 = [[row['city_latitude'],row['city_longitude']] for index, row in heat_df2.iterrows()]\n\n# Plot it on the map\nHeatMap(heat_data2).add_to(InCali)\n\n# Display the map\nInCali","99800bb9":"#Map of California sightings in 2019 alone\n\n#a map of California\nInCali2019 = folium.Map(location=[36.7783, -119.4179],\n                   zoom_start = 6)\n\nheat_df3 = Cali[Cali['year']=='2019'] # Reducing data size so it runs faster\nheat_df3 = heat_df3[['city_latitude', 'city_longitude']]\n\n# List comprehension to make a list of lists\nheat_data3 = [[row['city_latitude'],row['city_longitude']] for index, row in heat_df3.iterrows()]\n\n# Plot it on the map\nHeatMap(heat_data3).add_to(InCali2019)\n\n# Display the map\nInCali2019\n","2013a6f9":"import nltk #natural language toolkit\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords","6b99e2d2":"## Change the summary column to string\ndf2['words'] = df2['summary'].astype(str)\n\n## Lowercase all summaries\ndf2['words'] = df2['words'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n\n## remove punctuation\ndf2['words'] = df2['words'].str.replace('[^\\w\\s]','')","e5736ed8":"#remove common words (like the, are, all, etc.)\nstop = stopwords.words('english')\ndf2['words'] = df2['words'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))","a8825ef0":"#Most frequent words in the report summaries\nwordfreq=df2.words.str.split(expand=True).stack().value_counts()\nfreqs=pd.DataFrame(wordfreq)\nDescriptions=freqs.head(35)","49f7357f":"##numeric data to explore\n#word counts\n\n##Top 35 words found in the summaries\nDescriptions","1f100f0b":"#mean of numeric data\n##*not an interpretation of the summaries\nDescriptions.mean()","0155b21d":"Descriptions.std() #Standard deviation","0408cc45":"Descriptions.skew()","f1093fcb":"#all together\nDescriptions.describe()","67d403be":"Descriptions.sort_values(by=[0], ascending=True).plot(kind='barh',\n                                                     title='Frequent Words in UFO reports',\n                                                      figsize=(10, 10), color='darkgreen')","a7881177":"#categorize the shape column\n\n# Get one hot encoding of columns B\none_hot = pd.get_dummies(df2['shape'], prefix='shape', dummy_na=True)\n# Join the encoded df\ndf3 = df2.join(one_hot)\ndf3 = df3[['city',\"state\", 'year', \"shape_changing\",'shape_chevron',\n   'shape_cigar', 'shape_circle', 'shape_cone', 'shape_cross', 'shape_cylinder',\n           'shape_diamond','shape_disk','shape_egg','shape_fireball','shape_flash',\n           'shape_formation','shape_light','shape_other', 'shape_oval',\n           'shape_rectangle', 'shape_sphere', 'shape_teardrop', \"shape_triangle\"]]\ndf3  ","b05fbf1c":"df3.describe(include=\"all\")","9a729120":"#correlation\ndf3.corr()","7f902be4":"shapes=df3.drop(['city', 'state'], axis=1)\nshapes=shapes.astype(int)","731c2443":"shapes.sum()","5853f917":"#ploting shapes totals\n\nfig=plt.figure()\nfig.show()\nax=fig.add_subplot(111)\n\nax.plot(df3['shape_circle'].sum(),c='g',marker=\"o\",ls='--',label='circle')\nax.plot(df3['shape_triangle'].sum(),c='k',marker=\"^\", ls='-',label='triangle')\nax.plot(df3['shape_sphere'].sum(),c='r',marker=\"+\",ls='-',label='sphere')\nplt.xlim(-1, 5)\nplt.ylim(0, 9000)\n\nplt.legend(loc=1)\nplt.draw()","e6b114f5":"#### new df\ndf4=df1[['state','date_time', 'shape']]\n\n#Drop rows with NaNs\ndf4 = df4.dropna()\ndf4","0bd27a51":"df4.value_counts()","258b7d15":"df4.describe()","e7e0b569":"dates=df4[\"date_time\"].value_counts()\ndates","61c68c2f":"df4[\"shape\"].value_counts()","4ed108ec":"#make a column of the count list\nreportsPerDate = pd.DataFrame (dates)\nreportsPerDate.describe()","a71acce6":"#turn indexes to a column\nreportsPerDatemodified = reportsPerDate.reset_index()\n\n#renaming columns\nreportsPerDatemodified.rename(columns={'index': 'date_time', 'date_time': 'total_reports'}, inplace=True)","0b922d07":"reports=reportsPerDatemodified.sort_values(by=['total_reports'],ascending=False)\nreports","5830e126":"#get year as a new column and convert to integer\nreports['year'] = reports[\"date_time\"].apply(lambda x: pd.Timestamp(x).strftime('%Y'))\nreports['year']=reports['year'].astype(int)","99c6249c":"reports2014_2015=reports[(reports.year >= 2014) & (reports.year <= 2015)]","cac6067a":"reports2014_2015.plot.scatter(x='year', y='total_reports', color='salmon', alpha=0.5)\nplt.title('Reports per year')","92e1d3fa":"#predict\nreports2014_2015 = pd.get_dummies(reports2014_2015, columns=['year'], prefix='year')\nreports2014_2015 = reports2014_2015.drop(['date_time'], axis = 1) \nreports2014_2015","b7465ed3":"#Binary Classification\nimport sklearn as sk\nfrom sklearn import metrics #module for accuracy calculation\ny = reports2014_2015.iloc[:,0] #all rows, first column\nX = reports2014_2015.iloc[:,:2] #all rows, column 0 and 1  ","e23821bb":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","a1807440":"#decision tree\nfrom sklearn.tree import DecisionTreeClassifier\n\nclf = DecisionTreeClassifier().fit(X_train, y_train)\nprint('Accuracy of Decision Tree classifier on training set: {:.2f}'\n     .format(clf.score(X_train, y_train)))\nprint('Accuracy of Decision Tree classifier on test set: {:.2f}'\n     .format(clf.score(X_test, y_test)))","a0106588":"Most reports come from California, then Florida, and Washington state","66b1daaf":"### Visualization of dataset","0cb2794f":"### Numeric descriptions","181bd11d":"I looked for other entries from the same location to get its lat long, but I couldnt find similar entries. I have to add manually the latitudes and longitudes for those rows missing that data.","6a4038e0":"10877 entries of UFO sighting reports in North America do not include the latitude or longitude for their cities of occurance. Before visualization on a map, I need this data.","b008559f":"After dropping specific NaNs, rows went from 88125 to 81831    \nAlmost 7000 entries were not from North America.","b1b80cf4":"**Data analysis and visualization of**   \n**UFOs in North America 1969 to 2019 dataset**\n\nBy: Myrna M Figueroa Lopez   \n\nSource:    \nTHE NATIONAL UFO REPORTING CENTER:     \nDedicated to the Collection and Dissemination of Objective UFO Data http:\/\/www.nuforc.org\/.\n\nData limitations: The data depends on reports made to the NUFORC. Some entries are incomplete; These, I removed for the purpose of illustration in a timely fashion.   \n\nTasks:\n1. Visualization through plots and maps.\n2. Text mining of report summaries.\n3. Descriptive statistics.\n4. Predict\n\n","b7dd2753":"Of the data in df2, 2014 was the year with the most reports of UFO sightings.","8975771f":"### Text mining for common words","62c0fdec":"Now, I have 71540 entries with coordinates.","c475ee29":"After the changes above, the amount of rows went 81831 to 81715"}}