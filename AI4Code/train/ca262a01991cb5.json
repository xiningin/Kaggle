{"cell_type":{"bebe8db7":"code","fd87ce70":"code","eae04a46":"code","e55b9923":"code","f923595a":"code","70de2e69":"code","ea0d76e3":"code","ca1eba16":"code","5a0c6307":"code","b15a250e":"code","50671a25":"code","74df8a32":"code","04b01317":"code","0a5005dd":"code","8df0d4bd":"code","a983dbe6":"code","87d8954c":"code","ebb9fcd6":"code","fe4b1776":"code","cfba7b83":"code","d09b2ad7":"markdown","40fec434":"markdown","28a33825":"markdown","d70743a6":"markdown","8fa87e7d":"markdown","3fa28a94":"markdown"},"source":{"bebe8db7":"# Import packages\nimport numpy as np # Handling matrices\nimport pandas as pd # Data processing\nimport matplotlib.pyplot as plt # Plotting\nimport seaborn as sns # Plotting \nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler, LabelEncoder # Handling categorical data and normalization\nfrom sklearn.model_selection import train_test_split # Split data in train and test\nfrom sklearn.metrics import roc_auc_score,precision_score,confusion_matrix, accuracy_score, roc_curve, f1_score # Several useful metrics\nimport lightgbm as lgb # LightGBM\n\n# Set matplotlib configuration\n%matplotlib inline\nplt.style.use('seaborn')","fd87ce70":"# Import data\ndata = pd.read_csv('..\/input\/tabular-playground-series-mar-2021\/train.csv')\nprint(\"This dataset contains: {} rows and {} columns\".format(data.shape[0],data.shape[1]))\ndata.head()","eae04a46":"# Review the type of each feature\ndata.dtypes","e55b9923":"# Count the type of features\ndata.dtypes.value_counts()\nprint('This dataset contains {} categorical features'.format(data.dtypes.value_counts()[0]))\nprint('This dataset contains {} numerical features'.format(data.dtypes.value_counts()[1]))\n\n# Id and target are the unique integer features","f923595a":"# Analyse missing values\ndata.isna().sum()\n\n# Do not have missing values","70de2e69":"# Identify categorical features\ncat = (data.dtypes == 'object')\ncat_cols = list(cat[cat].index)\nprint(cat_cols)\n\n# Create a handful of plots\nfor cols in cat_cols:\n    plt.figure(figsize=(8,4));\n    sns.countplot(x = data[cols]);","ea0d76e3":"# Create a list of numerical_cols\nnumerical_cols = [cname for cname in data.columns if data[cname].dtype in ['float64']]\n\n# Also, we can see how numerical features are related with the target\ndata[numerical_cols].hist(bins=15, figsize=(20, 14), layout=(7, 3));","ca1eba16":"# Analyse our target colum\ndata['target'].hist(bins=15, figsize=(12,6));\n\n# We observe that our data is unbalanced. This is an important point.","5a0c6307":"# Separate independent features of target\ny = data['target']\nX = data.drop(['id','target'],axis = 1)","b15a250e":"# LightGBM also can handle categorical data directly We go to probe its inner method\n\n# Transform categorical features into the appropriate type that is expected by LightGBM\nfor c in X.columns:\n    col_type = X[c].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        X[c] = X[c].astype('category')","50671a25":"# Divide data into training and validation subsets. We use parameters stratify to ensure our data is split maintain the proportion of output classes\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, \n                                                                test_size=0.2,random_state = 123,stratify = y)","74df8a32":"# Print proportion of entire dataset\nprint(\"Proportion of classes in entire data: \")\nprint(100. * y.value_counts() \/ len(y),\"\\n\")\n\n# Print proportion of train and test sets \nprint(\"Proportion of classes in train data: \")\nprint(100. * y_train.value_counts() \/ len(y_train),\"\\n\")\nprint(\"Proportion of classes in valid data: \")\nprint(100. * y_valid.value_counts() \/ len(y_valid))","04b01317":"# Create the model \nd_train=lgb.Dataset(X_train, label=y_train) #Specifying the parameter\nparams={}\nparams['learning_rate']=0.05 # Learning rate \nparams['boosting_type']='gbdt' # GradientBoostingDecisionTree\nparams['objective']='binary' # Binary target feature\nparams['metric']='auc' # Metric for binary classification\nparams['max_depth']=500, # Set depth\nparams['bagging_fraction'] = 0.6,\nparams['force_row_wise'] = True, # Need to the model\nparams['unbalance'] =True, # To consider an unbalanced dataset\nparams['num_leaves'] = 100\nclf=lgb.train(params,d_train,200) # Train the model on 200 epocs\n\n# Prediction on the valid set\ny_pred=clf.predict(X_valid)","0a5005dd":"# Function to plot ROC curve\ndef plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()\n\nfpr, tpr, thresholds = roc_curve(y_valid, y_pred)\nplot_roc_curve(fpr, tpr)","8df0d4bd":"# Create Confusion Matrix\npred_class = y_pred > 0.5\npred_class = pred_class.astype(int)\ncm = confusion_matrix(y_valid, pred_class)\nprint(\"Confusion matrix: \\n\",cm,\"\\n\")\n\n# Get accuracy\naccuracy = round(accuracy_score(y_valid,pred_class),4)\nprint(\"Accuracy: {}\".format(accuracy),\"\\n\")\n\n# Get f1 score (it is required on the Task 1 of this dataset)\nf1 = f1_score(y_valid,pred_class)\nprint(\"F1: {}\".format(f1),\"\\n\")\n","a983dbe6":"# See the feature importance\nimportance_feature = pd.DataFrame({'Value':clf.feature_importance(),'Feature':clf.feature_name()}).sort_values(by=\"Value\", ascending=False)\n\n# Create a plot\nplt.figure(figsize=(20, 10))\nsns.barplot(x = 'Value',y = 'Feature',data = importance_feature);\nplt.title(\"Importance feature\");","87d8954c":"# Load test data\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-mar-2021\/test.csv\")\ntest.head()\n\n# Remove id\nX_test = test.drop(\"id\",axis = 1)\n\n# Transform categorical features into the appropriate type that is expected by LightGBM\nfor c in X_test.columns:\n    col_type = X_test[c].dtype\n    if col_type == 'object' or col_type.name == 'category':\n        X_test[c] = X_test[c].astype('category')\n","ebb9fcd6":"# Prediction on the valid set\ntest_pred=clf.predict(X_test)","fe4b1776":"# Create submission file\noutput = test[['id']].copy()\noutput['target'] = pd.Series(test_pred, index=output.index)\noutput.head()","cfba7b83":"# write csv\noutput.to_csv(\"submissionv1.csv\",index = False)","d09b2ad7":"# 1) Review and analysis of data","40fec434":"# LightGBM in Action \n\nWe will perform a LightGBM model to generate an accuracy prediction to submit in the [Tabular Playground Series - Mar 2021 Competition](https:\/\/www.kaggle.com\/c\/tabular-playground-series-mar-2021). We will use a LightGBM model due to its advantages such as speed and accuracy working wiht large datasets.","28a33825":"We can see that our categorical and numeric features have different behaviours. We have categorical features with low and high number of classes, while our numerical feature are different distributions. ","d70743a6":"# 2) Create a model","8fa87e7d":"# 3) Use model in test set","3fa28a94":"# 4) Write results"}}