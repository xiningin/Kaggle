{"cell_type":{"76824194":"code","00cd5ffb":"code","000fe2ef":"code","3356bed4":"code","3364e900":"code","59b95f38":"code","5d1ca0f2":"code","bde223ac":"code","e03ca711":"code","b3911c34":"code","45d2ee2a":"code","54c65f65":"code","45f78b31":"code","daaafb9e":"code","5f163efd":"code","956a7e9f":"code","92e90543":"code","d8b4d32e":"code","7aec7c70":"code","3de57cfd":"code","fd2b5975":"code","fd29e9d4":"code","6b77a5b8":"markdown"},"source":{"76824194":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n#importng libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns #data visualization\nfrom matplotlib import pyplot as plt #data visualization\n%matplotlib inline\nsns.set_style(\"darkgrid\")\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n#importing data and segregating columns using the 'names' parameter\ncolumn_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ntraining = pd.read_csv(\"..\/input\/housing.csv\",header=None, delimiter=r\"\\s+\", names=column_names)\n\n#displaying the dataset\ntraining.head(10)\n\n","00cd5ffb":"#describing the dataset\ntraining.describe()","000fe2ef":"#data visualization\nsns.barplot(palette=\"Blues_d\", x=\"RAD\", y=\"MEDV\", data=training)\nplt.title(\"Index of accessibility to radial highways vs Median value of owner-occupied homes\")\nplt.show()","3356bed4":"#dividing age into various groups for better visialization of data\ndata = [training]\nfor dataset in data:\n    dataset['GROUPAGE'] = dataset['AGE'].astype(int)\n    dataset.loc[ dataset['GROUPAGE'] <= 10, 'GROUPAGE'] = 0\n    dataset.loc[(dataset['GROUPAGE'] > 10) & (dataset['GROUPAGE'] <= 20), 'GROUPAGE'] = 1\n    dataset.loc[(dataset['GROUPAGE'] > 20) & (dataset['GROUPAGE'] <= 30), 'GROUPAGE'] = 2\n    dataset.loc[(dataset['GROUPAGE'] > 30) & (dataset['GROUPAGE'] <= 40), 'GROUPAGE'] = 3\n    dataset.loc[(dataset['GROUPAGE'] > 40) & (dataset['GROUPAGE'] <= 50), 'GROUPAGE'] = 4\n    dataset.loc[(dataset['GROUPAGE'] > 50) & (dataset['GROUPAGE'] <= 60), 'GROUPAGE'] = 5\n    dataset.loc[(dataset['GROUPAGE'] > 60) & (dataset['GROUPAGE'] <= 70), 'GROUPAGE'] = 6\n    dataset.loc[(dataset['GROUPAGE'] > 70) & (dataset['GROUPAGE'] <= 80), 'GROUPAGE'] = 7\n    dataset.loc[(dataset['GROUPAGE'] > 80) & (dataset['GROUPAGE'] <= 90), 'GROUPAGE'] = 8\n    dataset.loc[ dataset['GROUPAGE'] > 90, 'GROUPAGE'] = 9\n\n# let's see how it's distributed train_df['Age'].value_counts()\ntraining['GROUPAGE'].value_counts()","3364e900":"sns.swarmplot(x=\"GROUPAGE\", y=\"MEDV\", data=training)\nplt.title(\"Proportion of owner-occupied units built prior to 1940 vs Median value of owner-occupied homes\")\nplt.show()","59b95f38":"sns.barplot(x=\"GROUPAGE\", y=\"MEDV\", data=training)\nplt.title(\"Proportion of owner-occupied units built prior to 1940 vs Median value of owner-occupied homes\")\nplt.show()","5d1ca0f2":"#dividing LSTAT into various groups for better visialization of data\ndata = [training]\nfor dataset in data:\n    dataset['GROUPLSTAT'] = dataset['LSTAT'].astype(int)\n    dataset.loc[ dataset['GROUPLSTAT'] <= 5, 'GROUPLSTAT'] = 0\n    dataset.loc[(dataset['GROUPLSTAT'] > 5) & (dataset['GROUPLSTAT'] <= 10), 'GROUPLSTAT'] = 1\n    dataset.loc[(dataset['GROUPLSTAT'] > 10) & (dataset['GROUPLSTAT'] <= 15), 'GROUPLSTAT'] = 2\n    dataset.loc[(dataset['GROUPLSTAT'] > 15) & (dataset['GROUPLSTAT'] <= 20), 'GROUPLSTAT'] = 3\n    dataset.loc[(dataset['GROUPLSTAT'] > 20) & (dataset['GROUPLSTAT'] <= 25), 'GROUPLSTAT'] = 4\n    dataset.loc[(dataset['GROUPLSTAT'] > 25) & (dataset['GROUPLSTAT'] <= 30), 'GROUPLSTAT'] = 5\n    dataset.loc[(dataset['GROUPLSTAT'] > 30) & (dataset['GROUPLSTAT'] <= 35), 'GROUPLSTAT'] = 6\n    dataset.loc[ dataset['GROUPLSTAT'] > 35, 'GROUPLSTAT'] = 6\n\n# let's see how it's distributed train_df['Age'].value_counts()\ntraining['GROUPLSTAT'].value_counts()","bde223ac":"#swarmplot\nsns.swarmplot(x=\"GROUPLSTAT\",y=\"MEDV\", data=training)\nplt.title(\"Percentage of lower status of the population vs Median value of owner-occupied homes\")\nplt.show()\n#From this data we can see that the Lower is the LSTAT, the Higher is the Median Value of the House","e03ca711":"sns.barplot(x=\"GROUPLSTAT\", y=\"MEDV\", data=training)\nplt.title(\"Proportion of owner-occupied units built prior to 1940 vs Median value of owner-occupied homes\")\nplt.show()","b3911c34":"#Proportion of non-retail business acres per town vs Median value of owner-occupied homes\nsns.jointplot(x=\"INDUS\", y=\"MEDV\", data=training)\nplt.show()","45d2ee2a":"#Average number of rooms per dwelling vs Median value of owner-occupied homes\nsns.jointplot(x=\"RM\", y=\"MEDV\", data=training)\nplt.show()","54c65f65":"#dropping the newly created columns GROUPAGE and GROUPLSTAT\ntraining.drop(labels = [\"GROUPAGE\",\"GROUPLSTAT\"], axis = 1, inplace = True)\n#displaying the dataset\ntraining.head(10)","45f78b31":"#seperating features and target\nx=training.iloc[:,:-1].values\ny=training.iloc[:,-1].values","daaafb9e":"#splitting the dataset into training and test set\nfrom sklearn.model_selection import train_test_split,cross_val_score\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.30, random_state=0)","5f163efd":"#feature scaling\nfrom sklearn.preprocessing import StandardScaler\nsc_x=StandardScaler()\nx_train=sc_x.fit_transform(x_train)\nx_test=sc_x.transform(x_test)","956a7e9f":"#RandomForestRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nregressor=RandomForestRegressor(n_estimators=500, criterion='mae',min_samples_leaf=1,min_samples_split=4)\nregressor.fit(x_train,y_train)","92e90543":"#checking the score of the model\nregressor.score(x_test,y_test)","d8b4d32e":"#predicting the test set result\ny_pred=regressor.predict(x_test)","7aec7c70":"# checking the Mean Absolute Error value for accuracy\nfrom sklearn.metrics import mean_absolute_error\nmae=mean_absolute_error(y_test,y_pred)\nmae","3de57cfd":"# checking the Root Mean Squared Error value for accuracy\nfrom sklearn.metrics import mean_squared_error\nmse=mean_squared_error(y_test,y_pred)\nnp.sqrt(mse)","fd2b5975":"#checking the cross val score of the model\nresults = cross_val_score(regressor, x_train, y_train, cv=10, n_jobs=-1)\nresults.mean()","fd29e9d4":"#we can use GridSearchCV to find the optimal parameters for our model. Uncomment to use this section\n\"\"\"from sklearn.model_selection import GridSearchCV\nparam_grid = { \"criterion\" : [\"mse\", \"mae\"], \"min_samples_leaf\" : [1, 5, 10, 25, 50, 70], \"min_samples_split\" : [2, 4, 10, 12, 16, 18, 25, 35], \"n_estimators\": [25,50,75,100, 250,500]}\nclf = GridSearchCV(estimator=regressor, param_grid=param_grid, n_jobs=-1)\nclf.fit(x_train, y_train)\nclf.best_params_\"\"\"\n\n#result of running GridSearchCV\n\"\"\"{'criterion': 'mae',\n 'min_samples_leaf': 1,\n 'min_samples_split': 4,\n 'n_estimators': 500}\"\"\"","6b77a5b8":"**Since I am trying to learn Machine Learning and this is my first kernel please feel free to provide your inputs, remarks and suggestions about this kernel in the comment section and I will surely try to implements them and improve myself in the near future. Thank You**"}}