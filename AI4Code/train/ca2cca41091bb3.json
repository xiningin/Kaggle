{"cell_type":{"3c72da1c":"code","5d69de7d":"code","caad0aff":"code","c0f0476e":"code","abffb320":"code","d7f59fe3":"code","36a8acd6":"code","f6ad1cc6":"code","ac422923":"code","46f8e7d5":"markdown","c2408b85":"markdown","7ec24649":"markdown","981d5508":"markdown","eaa6738b":"markdown"},"source":{"3c72da1c":"from IPython.display import Image, display\ndisplay(Image(filename=\"..\/input\/ubiquant-models\/saved_models\/images\/rnn_v2.png\", width = 210, height = 65))","5d69de7d":"import pandas as pd\nimport numpy as np\nimport gc\nimport os\nimport pickle\nfrom lightgbm import LGBMRegressor\nfrom sklearn.model_selection import GroupKFold\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping","caad0aff":"def setup_cv(df, X, y, groups, splits=5):\n    kf = GroupKFold(n_splits=splits)\n    for f, (t_, v_) in enumerate(kf.split(X=X, y=y, groups=groups)):\n            df.loc[v_, 'fold'] = f\n\n    return df","c0f0476e":"def get_rnn_v2():\n    f300_in = L.Input(shape=(300,), name='300 feature input')\n    x = L.BatchNormalization(name='batch_norm1')(f300_in)\n    x = L.Dense(256, activation='swish', name='dense1')(x)\n    x = L.Dropout(0.1, name='dropout1')(x)\n    x = L.Reshape((1, -1), name='reshape1')(x)\n    x = L.BatchNormalization(name='batch_norm2')(x)\n    x = L.LSTM(128, dropout=0.3, recurrent_dropout=0.3, return_sequences=True, activation='relu', name='lstm1')(x)\n    x = L.LSTM(16, dropout=0.1, return_sequences=False, activation='relu', name='lstm2')(x)\n    output_layer = L.Dense(1, name='output')(x)\n\n    model = M.Model([f300_in], \n                    [output_layer])\n\n    model.compile(optimizer=tf.optimizers.Adam(lr=0.001),\n                  loss='mse', metrics=['mse'])\n\n    return model\n\nclass UbiquantRNNV2:\n    def __init__(self, df: pd.DataFrame, feature_cols: list=None, target: str='target'):\n\n        self.model = get_rnn_v2()\n\n        self.df = df\n\n        if feature_cols is not None:\n            self.feature_cols = feature_cols\n        else:\n            self.feature_cols = [f\"f_{i}\" for i in range(300)]\n\n        self.target_col = target\n\n    def train_one_fold(self, f: int, max_epochs=10):\n        X_train = self.df[self.df.fold!=f][self.feature_cols]\n        X_valid = self.df[self.df.fold==f][self.feature_cols]\n\n        y_train = self.df[self.df.fold!=f][self.target_col]\n        y_valid = self.df[self.df.fold==f][self.target_col]\n\n        self.model.fit(X_train, y_train,\n                       validation_data=(X_valid, y_valid),\n                       batch_size=128, epochs=max_epochs,\n                       callbacks=[\n                         ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_delta=1e-4, mode='min'),\n                         ModelCheckpoint(f'RNN_v2_checkpoint_{f}.hdf5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min'),\n                         EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5, mode='min', baseline=None, restore_best_weights=True)\n            ])\n\n        oof = self.model.predict(X_valid)\n        oof_score = np.sqrt(mean_squared_error(y_valid, oof))\n        print(f'oof rmse: {oof_score}')\n\n    def predict(self, X: np.ndarray):\n        preds = self.model.predict(X)\n        return preds\n\n    def save(self, path: str):\n        pickle.dump(self.model, open(path, 'wb'))","abffb320":"class UbiquantLGBM:\n    \"\"\"\n    This class is the Training Pipeline for an LGBM Regressor\n    \"\"\"\n    def __init__(self, df: pd.DataFrame, feature_cols: list=None, target: str='target'):\n        \"\"\" Creates the pipeline \"\"\"\n        params = {\n            'random_state': 42, \n            'verbosity': -1,\n            'metrics': 'rmse',\n        }  \n        self.model = LGBMRegressor(**params)\n\n        self.df = df\n\n        if feature_cols is not None:\n            self.feature_cols = feature_cols\n        else:\n            self.feature_cols = [f\"f_{i}\" for i in range(300)]\n\n        self.target_col = target\n\n    def train_one_fold(self, f: int):\n        \"\"\" Trains one fold of the lgbm \"\"\"\n        X_train = self.df[self.df.fold!=f][self.feature_cols]\n        X_valid = self.df[self.df.fold==f][self.feature_cols]\n\n        y_train = self.df[self.df.fold!=f][self.target_col]\n        y_valid = self.df[self.df.fold==f][self.target_col]\n\n        self.model.fit(X_train, y_train, \n                       eval_set=[(X_valid, y_valid)],\n                       eval_metric='rmse',\n                       verbose=False,\n                       early_stopping_rounds=30)\n\n        oof = self.model.predict(X_valid)\n        oof_score = np.sqrt(mean_squared_error(y_valid, oof))\n        print(f'oof rmse: {oof_score}')\n\n    def predict(self, X: np.ndarray):\n        \"\"\" Makes a prediction with the model \"\"\"\n        preds = self.model.predict(X)\n        return preds\n\n    def save(self, path: str):\n        \"\"\"Saves the model \"\"\"\n        pickle.dump(self.model, open(path, 'wb'))","d7f59fe3":"def load_model(file_path):\n    \"\"\" Loads a model pipeline object \"\"\"\n    file = open(file_path,'rb')\n    model = pickle.load(file)\n    file.close()\n    return model","36a8acd6":"\"\"\" Define our model with the trained weights \"\"\"\nrnn0 = get_rnn_v2()\nrnn0.load_weights(\"..\/input\/ubiquant-models\/saved_models\/rnn_checkpoints\/RNN_v2_checkpoint_0.hdf5\")\nrnn1 = get_rnn_v2()\nrnn1.load_weights(\"..\/input\/ubiquant-models\/saved_models\/rnn_checkpoints\/RNN_v2_checkpoint_1.hdf5\")\nrnn2 = get_rnn_v2()\nrnn2.load_weights(\"..\/input\/ubiquant-models\/saved_models\/rnn_checkpoints\/RNN_v2_checkpoint_2.hdf5\")\nrnn3 = get_rnn_v2()\nrnn3.load_weights(\"..\/input\/ubiquant-models\/saved_models\/rnn_checkpoints\/RNN_v2_checkpoint_3.hdf5\")\nrnn4 = get_rnn_v2()\nrnn4.load_weights(\"..\/input\/ubiquant-models\/saved_models\/rnn_checkpoints\/RNN_v2_checkpoint_4.hdf5\")","f6ad1cc6":"\"\"\" Make predictions for competition \"\"\"\nimport ubiquant\nenv = ubiquant.make_env()  \niter_test = env.iter_test()\nfeats = [f\"f_{i}\" for i in range(300)]\n\nfor (test_df, sample_prediction_df) in iter_test:\n    test_300 = test_df[feats]\n    test_invest_id = test_df[['investment_id']]\n    \n    pred0 = rnn0.predict(test_300)\n    pred1 = rnn1.predict(test_300)\n    pred2 = rnn2.predict(test_300)\n    pred3 = rnn3.predict(test_300)\n    pred4 = rnn4.predict(test_300)\n    pred = np.mean([pred0, pred1, pred2, pred3, pred4], axis=0)\n    \n    sample_prediction_df['target'] = pred\n    env.predict(sample_prediction_df)","ac422923":"pd.read_csv(\"submission.csv\")","46f8e7d5":"# Inference","c2408b85":"# Cross Validation","7ec24649":"# Overview\n\nThis is my starter notebook for how to start with a recurrent neural network in this competition. It has much room for improvement, but hopefully this gives you a baseline to work with. \n\nThis notebook currently score .142 on the latest version. I also have a lgbm training pipeline in this notebook that scores a .133 but it is not used in this inference.\n\nThe training pipelines are attached, the models were trained offline, and the weights are attached for inference.\n\n## Model\n- 300 feature input\n- Batch Normalization\n- Dense feature extractor with dropout\n- Reshape and Batch Normalization to setup for RNN\n- LSTM layers\n- Dense head\n\nSee diagram below\n\n## Callbacks\n- ReduceLROnPlateau\n- ModelCheckpoint\n- EarlyStopping\n\n## Future Ideas\n- add new input features\n- add investment_id as an input with an embedding layer\n- use attention\n- try different callbacks, learning rates, loss functions, etc\n- try 1-D CNN","981d5508":"## LGBM","eaa6738b":"# Models\n\n## RNN"}}