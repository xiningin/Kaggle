{"cell_type":{"2a529489":"code","020314f1":"code","b615a295":"code","c233226b":"code","5b6ae4fb":"code","70114dcd":"code","74a74055":"code","a79688b0":"code","14bb9545":"code","9c294fc3":"code","946a643c":"code","bbc63a4c":"code","94c9490b":"code","0492ed37":"code","8b68f229":"code","26969ea0":"code","88cd04f4":"code","8eba4ff0":"code","c311518c":"code","1fe50e91":"code","90eb3aa2":"code","85cd4060":"code","121eac2e":"code","1627e0d8":"code","b451afb6":"code","09cea2a6":"code","0582f34d":"code","e7cf2942":"code","bac63cd6":"code","154a7da1":"code","8dcad6b8":"code","3583eb6f":"code","9773488b":"code","6f15c92e":"code","488ab3b7":"code","65faa00c":"code","8dd3bc99":"code","85c0229b":"code","a570d5e0":"code","6867b644":"code","15d166e8":"code","dee78bcc":"code","1e06e24b":"code","266fb140":"code","62da0ca2":"code","c049422d":"code","a814d160":"code","e1b87826":"code","155c54aa":"code","3dd92790":"code","05bd9286":"code","c41bef42":"code","41bb2a67":"code","e045b016":"code","7eb1e57e":"code","7902e251":"code","ee707819":"code","28b9656c":"code","da245f0a":"code","9b6b20e8":"code","6af3edd5":"code","2737909f":"code","71fa2360":"code","cc5278a6":"code","7bad6d49":"code","8c1592f6":"code","5db74b71":"markdown","f8c71081":"markdown","9cf960d1":"markdown","9f24ee9e":"markdown","e645af87":"markdown","2775d7ce":"markdown","e44d9fe7":"markdown","9168e601":"markdown","b92149a4":"markdown","ecf1b226":"markdown","e336d158":"markdown","35910501":"markdown","e965e848":"markdown","0680fe43":"markdown","6d8e4646":"markdown"},"source":{"2a529489":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","020314f1":"emp_atr=pd.read_csv(\"..\/input\/HR-Employee-Attrition.csv\")\nemp_atr.shape","b615a295":"emp_atr.info()","c233226b":"emp_atr.describe().transpose()","5b6ae4fb":"emp_atr.Attrition.value_counts()","70114dcd":"emp_atr.head(10)","74a74055":"emp_atr.tail(10)","a79688b0":"emp_atr.isna().sum()","14bb9545":"emp_atr[emp_atr.duplicated()]","9c294fc3":"emp_atr.columns","946a643c":"cat_col = emp_atr.select_dtypes(exclude=np.number).columns\nnum_col = emp_atr.select_dtypes(include=np.number).columns\nprint(cat_col)\nprint(num_col)","bbc63a4c":"for i in cat_col:\n    print(emp_atr[i].value_counts())","94c9490b":"# Get discrete numerical value\nnum_col_disc=[]\nnum_col_medium=[]\nnum_col_cont=[]\nprint(\"Attributes with their distinct count\")\nfor i in num_col:\n    if emp_atr[i].nunique() <=10:\n        print(i,\"==\",emp_atr[i].nunique(),\"== disc\")\n        num_col_disc.append(i)\n    elif (emp_atr[i].nunique() >10 and emp_atr[i].nunique() <100):\n        num_col_medium.append(i)    \n        print(i,\"==\",emp_atr[i].nunique(),\"== medium\")\n    else:\n        num_col_cont.append(i)\n        print(i,\"==\",emp_atr[i].nunique(),\"== cont\")\n#print(num_col_disc)\n#print(num_col_medium)\n#print(num_col_cont)","0492ed37":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","8b68f229":"fig, ax = plt.subplots(3, 3, figsize=(30, 40))\nfor variable, subplot in zip(cat_col, ax.flatten()):\n    cp=sns.countplot(emp_atr[variable], ax=subplot,order = emp_atr[variable].value_counts().index,hue=emp_atr['Attrition'])\n    cp.set_title(variable,fontsize=40)\n    cp.legend(fontsize=30)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(90)\n        label.set_fontsize(36)                \n    for label in subplot.get_yticklabels():\n        label.set_fontsize(36)        \n        cp.set_ylabel('Count',fontsize=40)    \nplt.tight_layout()\n","26969ea0":"fig, ax = plt.subplots(3, 3, figsize=(30, 40))\nfor variable, subplot in zip(num_col_disc, ax.flatten()):\n    cp=sns.countplot(emp_atr[variable], ax=subplot,order = emp_atr[variable].value_counts().index,hue=emp_atr['Attrition'])\n    cp.set_title(variable,fontsize=40)\n    cp.legend(fontsize=30)\n    for label in subplot.get_xticklabels():\n        #label.set_rotation(90)\n        label.set_fontsize(36)                \n    for label in subplot.get_yticklabels():\n        label.set_fontsize(36)        \n        cp.set_ylabel('Count',fontsize=40)\nplt.tight_layout()","88cd04f4":"plt.figure(figsize=(20, 10))\nsns.countplot(emp_atr[\"WorkLifeBalance\"],hue=emp_atr[\"Attrition\"])\n#emp_atr[\"WorkLifeBalance\"]","8eba4ff0":"plt.figure(figsize=(20, 10))\nsns.boxplot(data=emp_atr[num_col_cont],orient=\"h\")","c311518c":"plt.figure(figsize=(20, 10))\nsns.barplot(data=emp_atr[num_col_cont],orient=\"h\")","1fe50e91":"#fill_num_attrition=lambda x: 1 if x==\"Yes\" else 0\n#type(fill_num_attrition)\nemp_atr[\"num_attrition\"]=emp_atr[\"Attrition\"].apply(lambda x: 1 if x==\"Yes\" else 0)\nemp_atr[\"num_attrition\"].value_counts()","90eb3aa2":"emp_atr_cov=emp_atr.cov()\nemp_atr_cov","85cd4060":"plt.figure(figsize=(40,20))\nsns.heatmap(emp_atr_cov,vmin=-1,vmax=1,center=0,annot=True)","121eac2e":"# Importing necessary package for creating model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","1627e0d8":"# one hot encoding num_attrition\ncat_col_rm_tgt=cat_col[1:]\nnum_col=emp_atr.select_dtypes(include=np.number).columns\none_hot=pd.get_dummies(emp_atr[cat_col_rm_tgt])\nemp_atr_df=pd.concat([emp_atr[num_col],one_hot],axis=1)\nemp_atr_df.head(10)\n","b451afb6":"X=emp_atr_df.drop(columns=['num_attrition'])\ny=emp_atr_df[['num_attrition']]","09cea2a6":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)","0582f34d":"train_Pred = logreg.predict(X_train)","e7cf2942":"metrics.confusion_matrix(y_train,train_Pred)","bac63cd6":"metrics.accuracy_score(y_train,train_Pred)","154a7da1":"test_Pred = logreg.predict(X_test)","8dcad6b8":"metrics.confusion_matrix(y_test,test_Pred)","3583eb6f":"metrics.accuracy_score(y_test,test_Pred)","9773488b":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, test_Pred))","6f15c92e":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nlogit_roc_auc = roc_auc_score(y_test, logreg.predict(X_test))\nfpr, tpr, thresholds = roc_curve(y_test, logreg.predict_proba(X_test)[:,1])\nplt.figure()\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()","488ab3b7":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom math import sqrt","65faa00c":"X_train, X_test, y_train, y_test = train_test_split(\nX, y, test_size = 0.3, random_state = 100)\ny_train=np.ravel(y_train)\ny_test=np.ravel(y_test)\n#y_train = y_train.ravel()\n#y_test = y_test.ravel()","8dd3bc99":"accuracy_train_dict={}\naccuracy_test_dict={}\ndf_len=round(sqrt(len(emp_atr_df)))\nfor k in range(3,df_len):\n    K_value = k+1\n    neigh = KNeighborsClassifier(n_neighbors = K_value, weights='uniform', algorithm='auto')\n    neigh.fit(X_train, y_train) \n    y_pred_train = neigh.predict(X_train)\n    y_pred_test = neigh.predict(X_test)    \n    train_accuracy=accuracy_score(y_train,y_pred_train)*100\n    test_accuracy=accuracy_score(y_test,y_pred_test)*100\n    accuracy_train_dict.update(({k:train_accuracy}))\n    accuracy_test_dict.update(({k:test_accuracy}))\n    print (\"Accuracy for train :\",train_accuracy ,\" and test :\",test_accuracy,\"% for K-Value:\",K_value)","85c0229b":"elbow_curve_train = pd.Series(accuracy_train_dict,index=accuracy_train_dict.keys())\nelbow_curve_test = pd.Series(accuracy_test_dict,index=accuracy_test_dict.keys())\nelbow_curve_train.head(10)","a570d5e0":"ax=elbow_curve_train.plot(title=\"Accuracy of train VS Value of K \")\nax.set_xlabel(\"K\")\nax.set_ylabel(\"Accuracy of train\")\n","6867b644":"ax=elbow_curve_test.plot(title=\"Accuracy of test VS Value of K \")\nax.set_xlabel(\"K\")\nax.set_ylabel(\"Accuracy of test\")","15d166e8":"from sklearn.naive_bayes import GaussianNB","dee78bcc":"NB=GaussianNB()\nNB.fit(X_train, y_train)","1e06e24b":"GaussianNB(priors=None,var_smoothing=1e-09)","266fb140":"train_pred=NB.predict(X_train)\naccuracy_score(train_pred,y_train)","62da0ca2":"test_pred=NB.predict(X_test)\naccuracy_score(test_pred,y_test)","c049422d":"from sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import tree\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures","a814d160":"clf_gini = DecisionTreeClassifier(criterion = \"gini\",\n                               max_depth=3, min_samples_leaf=5)\nclf_gini.fit(X_train, y_train)","e1b87826":"clf_entropy = DecisionTreeClassifier(criterion = \"entropy\",\n max_depth=3, min_samples_leaf=5)\nclf_entropy.fit(X_train, y_train)","155c54aa":"y_pred = clf_gini.predict(X_test)\ny_pred","3dd92790":"y_pred_en = clf_entropy.predict(X_test)\ny_pred_en","05bd9286":"print (\"Accuracy is \", accuracy_score(y_test,y_pred)*100)","c41bef42":"print (\"Accuracy is \", accuracy_score(y_test,y_pred_en)*100)","41bb2a67":"def fit_predict(train, test, y_train, y_test, scaler, max_depth, \n                criterion = 'entropy', max_features = 1, min_samples_split = 4):\n    train_scaled = scaler.fit_transform(train)\n    test_scaled = scaler.transform(test)        \n    dt = DecisionTreeClassifier(criterion = criterion, max_depth=max_depth, \n                                 max_features=max_features,\n                               min_samples_split=min_samples_split)\n    dt.fit(train_scaled, y_train)\n    y_pred = dt.predict(test_scaled)\n    print(accuracy_score(y_test, y_pred))","e045b016":"dt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\ny_pred = dt.predict(X_test)\nprint(accuracy_score(y_test, y_pred))","7eb1e57e":"for i in range(1, 20):\n    print('Accuracy score using max_depth =', i, end = ': ')\n    fit_predict(X_train, X_test, y_train, y_test, StandardScaler(), i)","7902e251":"for i in np.arange(0.1, 1.0, 0.1):\n    print('Accuracy score using max features =', i, end = ': ')\n    fit_predict(X_train, X_test, y_train, y_test, StandardScaler(), max_depth = 1, max_features=i)","ee707819":"for i in range(2, 10):\n    print('Accuracy score using min samples split =', i, end = ': ')\n    fit_predict(X_train, X_test, y_train, y_test, StandardScaler(), 1, max_features=0.1, min_samples_split=i)","28b9656c":"for i in ['gini', 'entropy']:\n    print('Accuracy score using criterion =', i, end = ': ')\n    fit_predict(X_train, X_test, y_train, y_test, StandardScaler(), 1, \n                max_features=0.1, min_samples_split=2, criterion = i)","da245f0a":"def create_poly(train,test,degree):\n    poly = PolynomialFeatures(degree=degree)\n    train_poly = poly.fit_transform(train)\n    test_poly = poly.fit_transform(test)\n    return train_poly,test_poly","9b6b20e8":"for degree in [1,2,3,4]:\n    train_poly, test_poly = create_poly(X_train, X_test, degree)\n    print('Polynomial degree',degree)\n    fit_predict(train_poly, test_poly, y_train, y_test, StandardScaler(), 1, \n                max_features=0.1, min_samples_split=2, criterion = 'gini')\n    print(10*'-')\n    \ntrain_poly, test_poly = create_poly(X_train, X_test, 2)\n","6af3edd5":"original_score = .8231292517006803\nbest_score = 0.8412698412698413\nimprovement = np.abs(np.round(100*(original_score - best_score)\/original_score,2))\nprint('overall improvement is {} %'.format(improvement))","2737909f":"from sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\n# Choose the type of classifier. \nclf = DecisionTreeClassifier()\n\n\n# Choose some parameter combinations to try\nparameters = {'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [i for i in range(1,20)], \n              'min_samples_split': [i for i in range(2,10)],\n              'min_samples_leaf': [i for i in range(2,10)]\n             }\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nclf = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nprint(clf.fit(X_train, y_train))\n\n#Predict target value and find accuracy score\ny_pred = clf.predict(X_test)\nprint(\"Accuracy score is \",accuracy_score(y_test, y_pred))","71fa2360":"# using Naive Bayes\nfrom sklearn.svm import SVC\nimport warnings\nwarnings.filterwarnings(\"ignore\")\ndef fit_predict(train, test, y_train, y_test, scaler, kernel = 'linear', C = 1.0, degree = 3):\n    train_scaled = scaler.fit_transform(train)\n    test_scaled = scaler.transform(test)        \n    lr = SVC(kernel = kernel, degree = degree, C = C)\n    lr.fit(train_scaled, y_train)\n    y_pred = lr.predict(test_scaled)\n    print(accuracy_score(y_test, y_pred))","cc5278a6":"y_train=np.ravel(y_train)\ny_test=np.ravel(y_test)\nfor kernel in ['linear', 'poly', 'rbf', 'sigmoid']:\n    print('Accuracy score using {0} kernel:'.format(kernel), end = ' ')\n    fit_predict(X_train, X_test, y_train, y_test, StandardScaler(), kernel)","7bad6d49":"for \u0441 in np.logspace(-1,3 ,base = 2, num = 6):\n    print('Accuracy score using penalty = {0} with rbf kernel:'.format(\u0441), end = ' ')\n    fit_predict(X_train, X_test, y_train, y_test, StandardScaler(), 'linear', \u0441)","8c1592f6":"for degree in range(2, 6):\n    print('Accuracy score using degree = {0} with poly kernel:'.format(degree), end = ' ')\n    fit_predict(X_train, X_test, y_train, y_test, StandardScaler(), 'linear', 0.5, degree = degree)","5db74b71":"****Penalty tuning****","f8c71081":"**You can clearly see that attrition value for No is quite high than Yes value. We get this biased data having more data for attrition=\"No\" . We try to see that we can get try to get something out of model **\n\n**Also with above plot we see that data is not evenly distributed for discrete values**","9cf960d1":"**We will try to implement KNN for this problem to see accuracy is better than logistic regression**","9f24ee9e":"**With above set of computations, we computed hyper paramter value individually and find best one manually. By using Grid search, we automate this to get optimized hyper parameter value considering all the hyper parameter at once **\n\nReference Kernel for gridsearchcv : https:\/\/www.kaggle.com\/cesartrevisan\/scikit-learn-and-gridsearchcv\n","e645af87":"**We will implement SVM to see if it improves accuracy**","2775d7ce":"**Choosing degree for poly kernel**","e44d9fe7":"**Polynomial tunning**","9168e601":"**From the above boxplot and barplot, we see that outlier exist for monthly income ,Daily rate and Employee number. Also data is distributed wider for monthly income and monthly rate variable **","b92149a4":"**Max features tuning******","ecf1b226":"**Min samples split tuning**","e336d158":"**Criterion tuning**","35910501":"**We will implement Decision tree for this to see how accuracy varies**","e965e848":"**From the above iteration we see K=12 had better accuracy**","0680fe43":"**Max depth tuning**","6d8e4646":"****Kernel tuning****"}}