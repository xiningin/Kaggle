{"cell_type":{"6d2b556a":"code","e0134799":"code","583adbee":"code","1cacc9fb":"code","4123be3d":"code","e762a798":"code","03471827":"code","016af332":"code","c0f6f636":"code","71769ae8":"code","0c902812":"code","480fab36":"code","3d18f083":"code","6515703b":"code","4b3878f0":"code","624bcf27":"code","91b92bd1":"code","263cd717":"code","c8382fd1":"code","9a4a8bb8":"code","5b5e4556":"code","52c54164":"code","64ebfc69":"code","9fa86ace":"code","ed6483dc":"code","471244f4":"code","2ee571d3":"code","867f6ec2":"code","ed8db661":"code","fca977d3":"code","c079c9e7":"code","ca935185":"code","3b883e92":"code","ccceb751":"code","32c73de6":"code","beec7db1":"code","a817f777":"code","5b506c72":"code","b65c90d6":"code","0d06477a":"code","314ae218":"code","94099dc8":"code","c8309389":"code","895caef3":"code","12819f1d":"code","d671d447":"code","2583f88e":"code","5aebaaea":"code","982f1394":"code","af86ab25":"code","4d9d911d":"code","8a2c8b98":"code","3c8c2186":"code","1ae1bb93":"code","43c15413":"code","74577e50":"code","429e32f5":"code","97b4423f":"code","f83a128e":"code","0c8973a3":"code","0d396e1a":"code","ed16993f":"code","f9cc8d82":"code","f5bcf65b":"code","10e9177f":"code","28ae1cbe":"code","d562e728":"code","5e54bf51":"markdown","a2846490":"markdown","4bf0496f":"markdown","5680418e":"markdown","1573ccb3":"markdown","4faccb92":"markdown","00866425":"markdown","1e1ca9fe":"markdown","aef8aa71":"markdown","54a95db8":"markdown","882a0f92":"markdown","94e0a873":"markdown","0e837d58":"markdown","b6a04b5f":"markdown","c762ab0c":"markdown","3aa7e3d9":"markdown","6804088b":"markdown","3c5d37c7":"markdown","eef95a92":"markdown","93775633":"markdown","7767383d":"markdown","a14316a0":"markdown","1b8f01fd":"markdown","b8b9aaa9":"markdown","8056e694":"markdown","2d545d74":"markdown","8de10005":"markdown","b1f03b26":"markdown","782e6cda":"markdown","8db52458":"markdown","985add66":"markdown","d2121b31":"markdown","e56f6a2f":"markdown","2d7ce2e7":"markdown","6b756ddf":"markdown","45d6adf2":"markdown","aab0d4ae":"markdown","028ade02":"markdown","776fae73":"markdown","74be3406":"markdown","64b9706b":"markdown","175fa4ff":"markdown","ed8104b0":"markdown","23b8dfe0":"markdown","4e926a3e":"markdown","20b506fe":"markdown","5d0865f1":"markdown","c399f716":"markdown","75e2b4ee":"markdown","d0e4f90c":"markdown","feb2f4b9":"markdown","9f2ba6e9":"markdown","5a8c0332":"markdown","667f6697":"markdown","39a41114":"markdown","f2fc20d7":"markdown","1f5a52b8":"markdown","16736a00":"markdown","397e8353":"markdown","bfc0e640":"markdown","4e0d79ca":"markdown","bfdd6221":"markdown","7c632942":"markdown","f840d69b":"markdown","c2ce904e":"markdown","0200b885":"markdown","c4e06941":"markdown","50815082":"markdown","89b6d5f0":"markdown","9fffe3fc":"markdown","1465c6a4":"markdown","989b3f6a":"markdown","2a5f56c9":"markdown","eeedaf72":"markdown","91e1b562":"markdown","6a0f21b7":"markdown","de81de1c":"markdown","49c2a8f2":"markdown"},"source":{"6d2b556a":"!pip install gcsfs","e0134799":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nfrom subprocess import check_output\nfrom kaggle_datasets import KaggleDatasets\nimport warnings\n\n%matplotlib inline\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x))\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","583adbee":"def ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn","1cacc9fb":"path = KaggleDatasets().get_gcs_path()\ntrain_path = path + '\/train.csv'\ntest_path = path + '\/test.csv'\n\ntrain = pd.read_csv(train_path)\ntest = pd.read_csv(test_path)","4123be3d":"# show first five rows of train data\ntrain.head()","e762a798":"# show first 5 rows of test data\ntest.head()","03471827":"#check the numbers of samples and features\nprint(\"The train data size before dropping Id feature is : {} \".format(train.shape))\nprint(\"The test data size before dropping Id feature is : {} \".format(test.shape))\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#check again the data size after dropping the 'Id' variable\nprint(\"\\nThe training data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The testing data data size after dropping Id feature is : {} \".format(test.shape))","016af332":"### check for any possible aoutliers in the train dataset\nfig, ax = plt.subplots()\nax.scatter(x=train['GrLivArea'], y=train['SalePrice'])\nplt.ylabel('SalsPrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","c0f6f636":"train = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n# plot the train data again\nfig, ax = plt.subplots()\nax.scatter(train['GrLivArea'], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","71769ae8":"sns.distplot(train['SalePrice'], fit=norm)\n\n# get the fitted parameter used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint('\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n# Now plot the distribution\nplt.legend(['Noraml dist. ($\\mu=$ {:0.2f} and $\\sigma=$ {:0.2f} )'.format(mu, sigma)],\n           loc='best')\nplt.ylabel(\"Frequency\")\nplt.title('SalePrice Distribution')\n\n## Get also the QQ plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","0c902812":"# apply log to traget variables\ntrain['SalePrice'] = np.log1p(train['SalePrice'])\n\n# Visualize the new distribution\nsns.distplot(train['SalePrice'], fit=norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","480fab36":"ntrain = train.shape[0]\nntest = test.shape[0]\n\ny_train = train.SalePrice.values\n\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\n\nprint(\"all_data size is : {}\".format(all_data.shape))","3d18f083":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\n\nmissing_data = pd.DataFrame({'Missing Ratio' : all_data_na})\nmissing_data.head(20)","6515703b":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel(\"Features\", fontsize=15)\nplt.ylabel(\"Percent of Missing Values\", fontsize=15)\nplt.title(\"Percent Missing Data by Featue\", fontsize=15)","4b3878f0":"# plot correlation heatmap to see how target variable is correlated with SalePrice\n\ncorrelation = train.corr()\nplt.subplots(figsize=(12,10))\nsns.heatmap(correlation, vmax=0.9, square=True)","624bcf27":"all_data['PoolQC'] = all_data['PoolQC'].fillna(\"None\")","91b92bd1":"all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")","263cd717":"all_data[\"Alley\"] = all_data[\"Fence\"].fillna(\"None\")","c8382fd1":"all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")","9a4a8bb8":"all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","5b5e4556":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","52c54164":"for col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","64ebfc69":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","9fa86ace":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","ed6483dc":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","471244f4":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","2ee571d3":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","867f6ec2":"all_data = all_data.drop(['Utilities'], axis=1)","ed8db661":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","fca977d3":"all_data['Electrical'] = all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","c079c9e7":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","ca935185":"all_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","3b883e92":"all_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","ccceb751":"all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","32c73de6":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","beec7db1":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","a817f777":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","5b506c72":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","b65c90d6":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","0d06477a":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)\n    \n#all_data[skewed_features] = np.log1p(all_data[skewed_features])","314ae218":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","94099dc8":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","c8309389":"from sklearn.linear_model import ElasticNet, Lasso, BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","895caef3":"#Validation function\n# define numebr of folds\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","12819f1d":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","d671d447":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","2583f88e":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","5aebaaea":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","982f1394":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","af86ab25":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","4d9d911d":"score = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","8a2c8b98":"score = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","3c8c2186":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","1ae1bb93":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","43c15413":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","74577e50":"score = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","429e32f5":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","97b4423f":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","f83a128e":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","0c8973a3":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)\n\nscore = rmsle_cv(stacked_averaged_models)\nprint(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","0d396e1a":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","ed16993f":"stacked_averaged_models.fit(train.values, y_train)\nstacked_train_pred = stacked_averaged_models.predict(train.values)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","f9cc8d82":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","f5bcf65b":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","10e9177f":"'''RMSE on the entire Train data when averaging'''\n\nprint('RMSLE score on train data:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","28ae1cbe":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","d562e728":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","5e54bf51":"#### Averaged base models score","a2846490":"4. **Fence:** Data description says NA means **\"No fence\"**","4bf0496f":"10. **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2:** For all these categorical basement-related features, NaN means that there is no basement.","5680418e":"**XGBoost:**","1573ccb3":"## Load Data","4faccb92":"* **Gradient Boosting Regression :** With huber loss that makes it robust to outliers","00866425":"We just average four models here ENet, GBoost, KRR and lasso. Of course we could easily add more models in the mix.","1e1ca9fe":"19. **MSSubClass:** Na most likely means No building class. We can replace missing values with None","aef8aa71":"### Final Training and Prediction","54a95db8":"We can see from the plot that there are some outliers present in the train dataset. These are huge outliers therefore, we can delete those instances.","882a0f92":"We can see from the plot that the target variable is right skewed. We also know that a linar model love the data which is normally distributed, therefore, we need to transform this variable and make it more normally distributed.","94e0a873":"18. **SaleType:** Fill in again with most frequent which is \"WD\"","0e837d58":"#### Stacking averaged Models Class","b6a04b5f":"## Igonre Warnings","c762ab0c":"* **LightGBM :**","3aa7e3d9":"####  Averaged base models class","6804088b":"11. **MasVnrArea and MasVnrType:** NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.","3c5d37c7":"### Adding a meta model","eef95a92":"We can see that even the simplest stacking approach imporves the score.","93775633":"9. **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath:** missing values are likely zero for having no basement","7767383d":"#### Stacking Averaged models Score","a14316a0":"5. **FirePlaceQu:** Data description says NA means **\"NO fireplace\"**","1b8f01fd":"Plese note that outlier removal is not always safe. We have removed the outlier because it is huge and it doesn't comply with out data. ","b8b9aaa9":"### Getting the new train and test set","8056e694":"* **LASSO Regression :**\nThis model may be very sensitive to outliers. So we need to made it more robust on them. For that we use the sklearn's Robustscaler() method on pipeline","2d545d74":"14. **Functional:** data description says NA means typical","8de10005":"### Base Models","b1f03b26":"### Adding one more important feature","782e6cda":"2. **MiscFeature:** As written in the dat description NA means **\"no misc features\"**","8db52458":"### Check for missig values in the data","985add66":"### Define cross validation strategy ","d2121b31":"* **XGBoost :**","e56f6a2f":"### Base models scores","2d7ce2e7":"In this approach, we add a meta-model on averaged base models and use the out-of-folds predictions of these base models to train our meta-model.\n\nThe procedure, for the training part, may be described as follows:\n\n1. Split the total training set into two disjoint sets (here train and .holdout )\n\n2. Train several base models on the first part (train)\n\n3. Test these base models on the second part (holdout)\n\nUse the predictions from 3) (called out-of-folds predictions) as the inputs, and the correct responses (target variable) as the outputs to train a higher level learner called meta-model.\n\nThe first three steps are done iteratively . If we take for example a 5-fold stacking , we first split the training data into 5 folds. Then we will do 5 iterations. In each iteration, we train every base model on 4 folds and predict on the remaining fold (holdout fold).\n\nSo, we will be sure, after 5 iterations , that the entire data is used to get out-of-folds predictions that we will then use as new feature to train our meta-model in the step 4.\n\nFor the prediction part , We average the predictions of all base models on the test data and used them as meta-features on which, the final prediction is done with the meta-model.","6b756ddf":"## Data Preprocessing","45d6adf2":"16 **KitchenQual:** Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.","aab0d4ae":"Let's see how these base models perform on the data by evaluating the cross-validation rmsle error","028ade02":"13. **Utilities:** For this categorical feature all records are \"AllPub\", except for one \"NoSeWa\" and 2 NA . Since the house with 'NoSewa' is in the training set, this feature won't help in predictive modelling. We can then safely remove it.","776fae73":"6. **LotFrontage:** Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.","74be3406":"### Handling Missing Values","64b9706b":"### More Feature Engineering","175fa4ff":"Before doing some feature engineering, let's first concatenate the train and test data in teh same dataframe.","ed8104b0":"## Import Libraries","23b8dfe0":"We can see from the plot that the data is more normally distributed now and the skew seems to be almost correct","4e926a3e":"### Correlation","20b506fe":"* **Kernel Ridge Regression :**","5d0865f1":"We add XGBoost and LightGBM to the StackedRegressor defined previously.\n\nWe first define a rmsle evaluation function","c399f716":"3. **Alley:** Data description says NA means **\"no alley access\"**","75e2b4ee":"### Getting dummy categorical features","d0e4f90c":"### Feature Engineering","feb2f4b9":"### Outlier Detection","9f2ba6e9":"### Transforming some numerical variables that are really categorical****","5a8c0332":"### Applying log transformation of the target variables","667f6697":"Lets do some analysis on the variable first which we have to predict. ie, **SalePrice**","39a41114":"### Import Libraries","f2fc20d7":"## Stacking Models","1f5a52b8":"**LightGBM:**","16736a00":"## Modelling","397e8353":"12. **MSZoning (The general zoning classification):** 'RL' is by far the most common value. So we can fill in missing values with 'RL'","bfc0e640":"17. **Exterior1st and Exterior2nd:** Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string","4e0d79ca":"15. **Electrical:** It has one NA value. Since this feature has mostly 'SBrkr', we can set that for the missing value.","bfdd6221":"### Ensembling StackedRegressor, XGBoost and LightGBM","7c632942":"We use the cross_val_score function of Sklearn. However this function has not a shuffle attribut, we add then one line of code, in order to shuffle the dataset prior to cross-validation","f840d69b":"1. **PoolQC:** As written in data description NA means **\"N Pool\"**. So, It makes sense, given the huge ratio of missingg values approx ~99.7% and majority of houses have no Pool at all in general.","c2ce904e":"### Box Cox Transformation of (highly) skewed features","0200b885":"#### StackedRegressor:","c4e06941":"* **Elastic Net Regression :** Again made robust to outliers","50815082":"We begin with this simple approach of averaging base models. We build a new class to extend scikit-learn with our model and also to laverage encapsulation and code reuse (inheritance)","89b6d5f0":"### Simplest Stacking Approach: Averaging Base Models","9fffe3fc":"7. **GarageType, GarageFinish, GarageQual and GarageCond:** Replacing missing data with None\n","1465c6a4":"### Check again for some missing values","989b3f6a":"To make the two approaches comparable (by using the same number of models) , we just average Enet KRR and Gboost, then we add lasso as meta-model.","2a5f56c9":"### Delete Outliers","eeedaf72":"### Label Encoding some categorical variables that may contain information in their ordering set","91e1b562":"We use the scipy function boxcox1p which computes the Box-Cox transformation of  1+x .\n\nNote that setting  \u03bb=0  is equivalent to log1p used above for the target variable.\n\nSee this page for more details on Box Cox Transformation as well as the scipy function's page","6a0f21b7":"8. **GarageYrBlt, GarageArea and GarageCars:** Replacing missing data with 0 (Since No garage = no cars in such garage.)","de81de1c":"### Skewed Features","49c2a8f2":"### Target Variable"}}