{"cell_type":{"f14e9f53":"code","1ccbf338":"code","cb4b40e9":"code","a0f784df":"code","2b954707":"code","8e45534b":"code","a2448c42":"code","255632e6":"code","ac22fbd4":"code","dc4a6711":"code","ba79f3c3":"code","e032e5cd":"code","1a10235d":"code","f2727ec3":"code","b25166e4":"code","461cc3cd":"code","545a3eca":"code","0c4548b0":"code","1afddeaf":"code","15893ca6":"code","22d222c6":"code","65db4757":"code","dca83fe1":"code","b4022c85":"code","22af4fbe":"code","847f60ac":"code","18a7d874":"code","f604aafb":"code","149b5298":"code","dc526d0b":"markdown","dd0545ed":"markdown","3b35dbb9":"markdown","1dc95ad3":"markdown","72f6b1be":"markdown","4acb7960":"markdown","856441b9":"markdown","704d9ba4":"markdown","319d0786":"markdown","91cfe08d":"markdown","33d5f57b":"markdown","dc0760f6":"markdown","3d173b8c":"markdown","a3fa9c35":"markdown","a88b9ff0":"markdown","093801b9":"markdown","3ef75bba":"markdown","af94e73a":"markdown","c302713e":"markdown","41a0cded":"markdown","07b7263f":"markdown","d309bd49":"markdown","7b9bf29c":"markdown"},"source":{"f14e9f53":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1ccbf338":"# Import the libraries\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nimport scipy.cluster.hierarchy as sch\nfrom sklearn.cluster import AgglomerativeClustering\nfrom sklearn.cluster import DBSCAN \nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.cluster import Birch\nfrom sklearn.cluster import MeanShift\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.cluster import AffinityPropagation\nfrom sklearn.cluster import OPTICS","cb4b40e9":"data = pd.read_csv('\/kaggle\/input\/mall-customers\/Mall_Customers.csv')","a0f784df":"X = data.iloc[:, [3, 4]].values","2b954707":"plt.figure(figsize=(15,10))\nsns.set(style = 'whitegrid')\nplt.title('Distribution',fontsize=15)\nplt.axis('off')\nsns.distplot(X,color='orange')\nplt.xlabel('Distribution')\nplt.ylabel('#Customers')","8e45534b":"plt.figure(figsize=(22,10))\nsns.countplot(data['Annual Income (k$)'], palette = 'rainbow')\nplt.title('Distribution of Annual Income (k$)', fontsize = 20)\nplt.show()","a2448c42":"plt.figure(figsize=(15,10))\nsns.set(style = 'whitegrid')\nsns.distplot(data['Annual Income (k$)'],color='g')\nplt.title('Distribution of Annual Income(k$)', fontsize = 15)\nplt.xlabel('Range of Annual Income(k$)')\nplt.ylabel('#Customers')","255632e6":"plt.figure(figsize=(15,10))\nsns.countplot(data['Age'], palette = 'cool')\nplt.title('Distribution of Age', fontsize = 20)\nplt.show()","ac22fbd4":"plt.figure(figsize=(15,10))\nsns.set(style = 'whitegrid')\nsns.distplot(data['Age'],color='m')\nplt.title('Distribution of Age', fontsize = 15)\nplt.xlabel('Range of Age')\nplt.ylabel('#Customers')","dc4a6711":"plt.figure(figsize=(25,10))\nsns.countplot(data['Spending Score (1-100)'], palette = 'copper')\nplt.title('Distribution of Spending Score (1-100)', fontsize = 20)\nplt.show()","ba79f3c3":"plt.figure(figsize=(15,10))\nsns.set(style = 'whitegrid')\nsns.distplot(data['Spending Score (1-100)'],color='r')\nplt.title('Distribution of Spending Score (1-100)', fontsize = 15)\nplt.xlabel('Range of Spending Score (1-100)')\nplt.ylabel('#Customers')","e032e5cd":"plt.figure(figsize=(10,10))\nsize = data['Genre'].value_counts()\ncolors = ['pink', 'yellow']\nplt.pie(size, colors = colors, explode = [0, 0.15], labels = ['Female', 'Male'], shadow = True, autopct = '%.2f%%')\nplt.title('Gender', fontsize = 15)\nplt.axis('off')\nplt.legend()\nplt.show()","1a10235d":"sns.catplot(x=\"Genre\", kind=\"count\", palette=\"cool\", data=data)","f2727ec3":"plt.figure(figsize=(15,10))\nplt.title('Heatmap',fontsize=15)\nsns.heatmap(X,cmap='BuPu')","b25166e4":"plt.figure(figsize=(15,10))\nsns.heatmap(data.corr(), cmap = 'Greens', annot = True)\nplt.title('Heatmap for the Data', fontsize = 15)\nplt.show()","461cc3cd":"sns.pairplot(data)","545a3eca":"# Use of the elbow method to find the optimal number of clusters\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', random_state = 0)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)\nplt.figure(figsize=(15,15))\nplt.scatter(range(1, 11),wcss,c='b',s=100)\nplt.plot(range(1, 11),wcss,c='r',linewidth=4)\nplt.title('The Elbow Method',fontsize=20)\nplt.xlabel('Number of clusters',fontsize=20)\nplt.ylabel('Within-cluster-sum-of-squares',fontsize=20)\nplt.show()","0c4548b0":"# Train the K-Means model on the dataset\nkmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = 0)\ny_kmeans = kmeans.fit_predict(X)","1afddeaf":"# Visualization of the clusters of customers\nplt.figure(figsize=(15,15))\nplt.scatter(X[y_kmeans == 0, 0], X[y_kmeans == 0, 1], s = 60, c = 'c', label = '#1')\nplt.scatter(X[y_kmeans == 1, 0], X[y_kmeans == 1, 1], s = 60, c = 'g', label = '#2')\nplt.scatter(X[y_kmeans == 2, 0], X[y_kmeans == 2, 1], s = 60, c = 'm', label = '#3')\nplt.scatter(X[y_kmeans == 3, 0], X[y_kmeans == 3, 1], s = 60, c = 'b', label = '#4')\nplt.scatter(X[y_kmeans == 4, 0], X[y_kmeans == 4, 1], s = 60, c = 'r', label = '#5')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s = 100, c = 'yellow', label = 'Centroids')\nplt.title('Clusters',fontsize=20)\nplt.xlabel('Annual Income (k$)',fontsize=20)\nplt.ylabel('Spending Score (1-100)',fontsize=20)\nplt.legend()\nplt.show()","15893ca6":"# Use the dendrogram to find the optimal number of clusters\nplt.figure(figsize=(25,15))\ndendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\nplt.title('Dendrogram',fontsize=20)\nplt.xlabel('Customers',fontsize=20)\nplt.ylabel('Euclidean distances',fontsize=20)\nplt.show()","22d222c6":"# Train the Hierarchical Clustering model on the dataset\nhc = AgglomerativeClustering(n_clusters = 5, affinity = 'euclidean', linkage = 'ward')\ny_hc = hc.fit_predict(X)","65db4757":"# Visualization of the clusters of customers\nplt.figure(figsize=(15,15))\nplt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 60, c = 'm', label = '#1')\nplt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 60, c = 'r', label = '#2')\nplt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 60, c = 'b', label = '#3')\nplt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 60, c = 'g', label = '#4')\nplt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 60, c = 'c', label = '#5')\nplt.title('Clusters',fontsize=20)\nplt.xlabel('Annual Income (k$)',fontsize=20)\nplt.ylabel('Spending Score (1-100)',fontsize=20)\nplt.legend()\nplt.show()","dca83fe1":"def dbscan(X, eps, min_samples):\n    ss = StandardScaler()\n    X = ss.fit_transform(X)\n    db = DBSCAN(eps=eps, min_samples=min_samples)\n    db.fit(X)\n    y_pred = db.fit_predict(X)\n    plt.figure(figsize=(10,10))\n    plt.title('Clusters',fontsize=20)\n    plt.xlabel('Annual Income (k$)',fontsize=20)\n    plt.ylabel('Spending Score (1-100)',fontsize=20)\n    plt.legend()\n    plt.scatter(X[:,0], X[:,1],c=y_pred, cmap='coolwarm')\n    plt.title(\"DBSCAN\")","b4022c85":"dbscan(X,eps=0.275,min_samples=5)","22af4fbe":"brc = Birch(n_clusters=5)\nbrc.fit(X)\nbrc_y_pred = brc.predict(X)\nplt.figure(figsize=(10,10))\nplt.title('Clusters',fontsize=20)\nplt.xlabel('Annual Income (k$)',fontsize=20)\nplt.ylabel('Spending Score (1-100)',fontsize=20)\nplt.legend()\nplt.scatter(X[:,0], X[:,1],c=brc_y_pred, cmap='coolwarm')\nplt.title(\"BIRCH\")","847f60ac":"gmm = GaussianMixture(n_components=5)\ngmm.fit(X)\ngmm_y_pred = gmm.predict(X)\nplt.figure(figsize=(10,10))\nplt.title('Clusters',fontsize=20)\nplt.xlabel('Annual Income (k$)',fontsize=20)\nplt.ylabel('Spending Score (1-100)',fontsize=20)\nplt.legend()\nplt.scatter(X[:,0], X[:,1],c=gmm_y_pred, cmap='coolwarm')\nplt.title(\"Gaussian Mixture Model\")","18a7d874":"ap = AffinityPropagation(random_state=0)\nap.fit(X)\nap_y_pred = ap.predict(X)\nplt.figure(figsize=(10,10))\nplt.title('Clusters',fontsize=20)\nplt.xlabel('Annual Income (k$)',fontsize=20)\nplt.ylabel('Spending Score (1-100)',fontsize=20)\nplt.legend()\nplt.scatter(X[:,0], X[:,1],c=ap_y_pred, cmap='spring')\nplt.title(\"Affinity Propagation Model\")","f604aafb":"ms = MeanShift(bandwidth=2)\nms.fit(X)\nms_y_pred = ms.predict(X)\nplt.figure(figsize=(10,10))\nplt.title('Clusters',fontsize=20)\nplt.xlabel('Annual Income (k$)',fontsize=20)\nplt.ylabel('Spending Score (1-100)',fontsize=20)\nplt.legend()\nplt.scatter(X[:,0], X[:,1],c=ms_y_pred, cmap='seismic')\nplt.title(\"MeanShift Model\")","149b5298":"opt = OPTICS(min_samples=5)\nopt_y_pred = opt.fit_predict(X)\nplt.figure(figsize=(10,10))\nplt.title('Clusters',fontsize=20)\nplt.xlabel('Annual Income (k$)',fontsize=20)\nplt.ylabel('Spending Score (1-100)',fontsize=20)\nplt.legend()\nplt.scatter(X[:,0], X[:,1],c=opt_y_pred, cmap='inferno')\nplt.title(\"OPTICS Model\")","dc526d0b":"Here comes the training part","dd0545ed":"Quite impressive!!","3b35dbb9":"The customers in first cluster(cyan) have average annual income as well as average spending score.In case of the second cluster(green),customers with lower annual income but higher spending score belong to it.People with both higher annual income and higher spending score belong to third cluster(magenta).The customers in fourth cluster(blue) have lower annual income and lower spending score.People in fifth cluster(red) have lower spending score but have a high annual income!","1dc95ad3":"# Mean Shift","72f6b1be":"**Import the dataset**","4acb7960":"**Please upvote this notebook if you like it.Have a nice day!Thank you..**","856441b9":"# K-Means Clustering","704d9ba4":"Training time!","319d0786":"As you can see the optimal number of clusters is five!","91cfe08d":"# Affinity Propagation","33d5f57b":"# DBSCAN","dc0760f6":"We will use the elbow method to check the optimal number of clusters","3d173b8c":"Nice!","a3fa9c35":"# OPTICS","a88b9ff0":"# Gaussian Mixture Model","093801b9":"# Hierarchical Clustering","3ef75bba":"**Visualizing data**","af94e73a":"Not that good:(","c302713e":"Don't think they are that impressive!","41a0cded":"Let us visualize the dendrogram :)","07b7263f":"**Import libraries**","d309bd49":"Visualization!","7b9bf29c":"# Birch"}}