{"cell_type":{"c6106f1c":"code","e8e9a27c":"code","03e15a82":"code","2661eab8":"code","696c8759":"code","338b3219":"code","7092058c":"code","9e1e85d8":"code","981ab349":"code","5389b598":"code","b86d825e":"code","8a7c178c":"code","4c1b1218":"code","613a9b49":"code","c4155be0":"code","45c1bb08":"code","a33db6cc":"code","2b255676":"code","cc430fb1":"code","8c7b1c8c":"code","a56b981c":"code","90a8f3de":"code","ca22fd10":"code","c2b89d41":"code","1ab7b886":"code","74fee1e3":"code","13546989":"code","bd3dc971":"code","c442ad6f":"code","088d8380":"code","04a354ae":"code","f1bade44":"code","5484946e":"code","cb8cb4eb":"code","11aaa4fd":"code","74fcd99f":"code","bd29c466":"code","dde06362":"code","a1da4c7a":"code","16af112d":"code","19a91ac1":"code","38568a2e":"code","444ee926":"code","0dd95a3b":"code","69c82da1":"code","669914a3":"code","b9385b67":"code","0aa9dbaa":"code","2062dc01":"code","9d3b4c05":"code","1e7bbefa":"code","2e072c70":"code","a2af1854":"code","531c7e97":"code","de339571":"code","62cdaa68":"code","0ea804c1":"code","a9956375":"code","61d2788d":"code","c3b197ed":"code","1c47bf84":"code","239d6295":"code","bed6323e":"markdown","4f4274cc":"markdown","28505c61":"markdown","3cb65011":"markdown","5fca1565":"markdown","69091a10":"markdown","b8913c1e":"markdown","96d731e1":"markdown","e4ba30d1":"markdown","4fb77bc6":"markdown","c7945d56":"markdown","92b0f74b":"markdown","67dc0b3e":"markdown","83ebbafd":"markdown","a384b517":"markdown","4e0ecfb4":"markdown","4abfe3d7":"markdown","584aa524":"markdown","83d545a4":"markdown","5c553b15":"markdown","be58efa0":"markdown","ce6763d2":"markdown","2e9d8559":"markdown","e0c7ecaf":"markdown","f54d60fc":"markdown","a0c9779c":"markdown"},"source":{"c6106f1c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e8e9a27c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pickle\nimport scipy.io\nimport tarfile\nimport csv\nimport sys\nimport os,shutil\nimport PIL\nimport re\nimport random \nfrom PIL import ImageOps, ImageFilter\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\n\nimport lightgbm as lgb\nfrom catboost import CatBoostClassifier\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.models as M\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.callbacks as C\nfrom tensorflow.keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.callbacks import Callback\nfrom tensorflow.keras.applications.xception import Xception\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras import regularizers\nimport zipfile","03e15a82":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","2661eab8":"DATA_DIR = '\/kaggle\/input\/tabular-playground-series-apr-2021\/'\ndf_train = pd.read_csv(DATA_DIR + 'train.csv')\ndf_test = pd.read_csv(DATA_DIR + 'test.csv')\nsample_submission = pd.read_csv(DATA_DIR + 'sample_submission.csv')","696c8759":"df_train['sample'] = 1  # \u043f\u043e\u043c\u0435\u0447\u0430\u0435\u043c \u0433\u0434\u0435 \u0443 \u043d\u0430\u0441 \u0442\u0440\u0435\u0439\u043d\ndf_test['sample'] = 0  # \u043f\u043e\u043c\u0435\u0447\u0430\u0435\u043c \u0433\u0434\u0435 \u0443 \u043d\u0430\u0441 \u0442\u0435\u0441\u0442\n# \u0432 \u0442\u0435\u0441\u0442\u0435 \u0443 \u043d\u0430\u0441 \u043d\u0435\u0442 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f price, \u043c\u044b \u0435\u0433\u043e \u0434\u043e\u043b\u0436\u043d\u044b \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u043f\u043e\u043a\u0430 \u043f\u0440\u043e\u0441\u0442\u043e \u0437\u0430\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043d\u0443\u043b\u044f\u043c\u0438\ndf_test['Survived'] = 0\n\ndata = df_test.append(df_train, sort=False).reset_index(drop=True)  # \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c","338b3219":"data.info()","7092058c":"NaN_Sum = lambda col: col.isnull().sum()  # \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u043d\u0443\u043b\u0435\u0432\u044b\u0445 \u0437\u043d\u0447\u0435\u043d\u0438\u0439  \ncolumns = list(data.columns)\nfor col in columns:\n    print(\"\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0443\u0441\u0442\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439\", col, NaN_Sum(data[col]), sep=' ')","9e1e85d8":"df=data.copy()","981ab349":"def ticket_label(tickets):\n    try:    \n        patern = re.compile('\\D+')\n        tic = (patern.findall(tickets.Ticket)[0])\n        return tic.rstrip()\n    except IndexError:\n        return 'r'\n    except TypeError:\n        return 'X'\n\ndf['Ticket']=df.apply(lambda df: ticket_label(df),axis=1)","5389b598":"df.Ticket.unique()","b86d825e":"fig, ax = plt.subplots(figsize=(29,8))\nplt.title('\u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u043e\u0432 \u043f\u043e \u0431\u0438\u043b\u0435\u0442\u0430\u043c \u0432 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0442 \u043a\u043b\u0430\u0441\u0441\u0430')\nsns.countplot(x=\"Ticket\", hue=\"Pclass\", data=df)","8a7c178c":"# \u0421\u0443\u043c\u043c\u0438\u0440\u0443\u0435\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0447\u043b\u0435\u043d\u043e\u0432 \u0441\u0435\u043c\u044c\u0438 \u043d\u0430 \u0431\u043e\u0440\u0442\u0443 \ndf['FamOnBoard'] = df['Parch']+df['SibSp']","4c1b1218":"# \u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0434\u043b\u044f \u0442\u0435\u0445 \u043a\u0442\u043e \u043f\u043b\u044b\u0432\u0435\u0442 \u043e\u0434\u0438\u043d \ndf['Alone'] = 1\n\ndef alone_on_board(dat):\n    if dat.SibSp+dat.Parch==0:\n        return 1\n    else:\n        return 0 \n    \ndf['Alone'] = df.apply(lambda df: alone_on_board(df),axis=1)","613a9b49":"print('\u041e\u0431\u0449\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u043e\u0432 \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430: %s'%len(df[df.Pclass==1].Cabin))\nprint('\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u043e\u0432 \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 \u043d\u0435 \u0438\u043c\u0435\u044e\u0449\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043e \u043d\u043e\u043c\u0435\u0440\u0435 \u043a\u0430\u0431\u0438\u043d\u044b: %s'%df[df.Pclass==1].Cabin.isnull().sum())\nprint('------------------------------------------------------------')\nprint('\u041e\u0431\u0449\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u043e\u0432 \u0432\u0442\u043e\u0440\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430: %s'%len(df[df.Pclass==2].Cabin))\nprint('\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u043e\u0432 \u0432\u0442\u043e\u0440\u043e\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 \u043d\u0435 \u0438\u043c\u0435\u044e\u0449\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043e \u043d\u043e\u043c\u0435\u0440\u0435 \u043a\u0430\u0431\u0438\u043d\u044b: %s'%df[df.Pclass==2].Cabin.isnull().sum())\nprint('------------------------------------------------------------')\nprint('\u041e\u0431\u0449\u0435\u0435 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u043e\u0432 \u0442\u0440\u0435\u0442\u044c\u0435\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430: %s'%len(df[df.Pclass==3].Cabin))\nprint('\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u043e\u0432 \u0442\u0440\u0435\u0442\u044c\u0435\u0433\u043e \u043a\u043b\u0430\u0441\u0441\u0430 \u043d\u0435 \u0438\u043c\u0435\u044e\u0449\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043e \u043d\u043e\u043c\u0435\u0440\u0435 \u043a\u0430\u0431\u0438\u043d\u044b: %s'%df[df.Pclass==3].Cabin.isnull().sum())","c4155be0":"def cabin_type(cabin):\n    try:\n        pattern_cabin=re.compile('\\w')\n        name=str((pattern_cabin.findall(cabin.Cabin))[0])\n        return name\n    except:\n        np.nan\n\ndf['Cabin']=df.apply(lambda df: cabin_type(df),axis=1)","45c1bb08":"fig, ax = plt.subplots(figsize=(29,8))\nplt.subplot(1, 4, 1)\nplt.title('\u0412\u044b\u0436\u0438\u043c\u0430\u0435\u043c\u043e\u0441\u0442\u044c \u043f\u043e \u043a\u0430\u0431\u0438\u043d\u0430\u043c')\nsns.barplot(data=df, x=\"Cabin\", y=\"Survived\")\nplt.subplot(1, 4, 2)\nplt.title('\u0412\u044b\u0436\u0438\u043c\u0430\u0435\u043c\u043e\u0441\u0442\u044c \u043f\u043e \u043a\u043b\u0430\u0441\u0441\u0430\u043c')\nsns.barplot(data=df, x=\"Pclass\", y=\"Survived\")\nplt.subplot(1, 4, 3)\nplt.title('\u0412\u044b\u0436\u0438\u043c\u0430\u0435\u043c\u043e\u0441\u0442\u044c \u043f\u043e \u043a\u0430\u0431\u0438\u043d\u0430\u043c \u0432\u043d\u0443\u0442\u0440\u0438 \u043a\u043b\u0430\u0441\u0441\u043e\u0432')\nsns.barplot(x = \"Pclass\", y = \"Survived\", hue = \"Cabin\", data = df)\nplt.subplot(1, 4, 4)\nplt.title('\u0420\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u043e\u0432 \u043f\u043e \u043a\u0430\u0431\u0438\u043d\u0430\u043c \u0432 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0442 \u043a\u043b\u0430\u0441\u0441\u0430')\nsns.countplot(x=\"Cabin\", hue=\"Pclass\", data=df)","a33db6cc":"x,y = 'Pclass', 'Cabin'\n\ndf1 = df.groupby(x)[y].value_counts(normalize=True)\ndf1 = df1.mul(100)\ndf1 = df1.rename('percent').reset_index()\n\n\ng = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=df1)\ng.ax.set_ylim(0,100)\n\nfor p in g.ax.patches:\n    txt = str(p.get_height().round(2)) + '%'\n    txt_x = p.get_x() \n    txt_y = p.get_height()\n    g.ax.text(txt_x,txt_y,txt)","2b255676":"df.Cabin=df.Cabin.fillna(0)\n\ndef cabin_zero(dat):\n    cab=0\n    class_ = ['C','B','A','D','E','F','G','T']\n    if dat.Cabin == 0: \n        if dat.Pclass == 1:\n            cab = random.choices(class_,weights=[0.3481, 0.2935, 0.2113,0.0938,0.0456,0.0058,0.0001,0.0018])\n        elif dat.Pclass == 2:\n            cab = random.choices(class_,weights=[0.1397,0.0848,0.2002,0.1517,0.1286,0.2299,0.0617,0.0034])\n        else:\n            cab = random.choices(class_,weights=[0.1358,0.0937,0.3043,0.1094,0.0979,0.2254,0.0309,0.0026])\n        return cab[0]\n    else:\n        return dat.Cabin\n\ndf['Cabin']=df.apply(lambda df: cabin_zero(df),axis=1)","cc430fb1":"x,y = 'Pclass', 'Cabin'\n\ndf1 = df.groupby(x)[y].value_counts(normalize=True)\ndf1 = df1.mul(100)\ndf1 = df1.rename('percent').reset_index()\n\n\ng = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=df1)\ng.ax.set_ylim(0,100)\n\nfor p in g.ax.patches:\n    txt = str(p.get_height().round(2)) + '%'\n    txt_x = p.get_x() \n    txt_y = p.get_height()\n    g.ax.text(txt_x,txt_y,txt)","8c7b1c8c":"x,y = 'Pclass', 'Embarked'\n\ndf1 = df.groupby(x)[y].value_counts(normalize=True)\ndf1 = df1.mul(100)\ndf1 = df1.rename('percent').reset_index()\n\n\ng = sns.catplot(x=x,y='percent',hue=y,kind='bar',data=df1)\ng.ax.set_ylim(0,100)\n\nfor p in g.ax.patches:\n    txt = str(p.get_height().round(2)) + '%'\n    txt_x = p.get_x() \n    txt_y = p.get_height()\n    g.ax.text(txt_x,txt_y,txt)","a56b981c":"df.Embarked=df.Embarked.fillna(0)\n\ndef embarked_zero(dat):\n    embark=0\n    port_ = ['S','C','Q']\n    if dat.Embarked == 0: \n        if dat.Pclass == 1:\n            embark = random.choices(port_,weights=[0.4757,0.3944,0.1299])\n        elif dat.Pclass == 2:\n            embark = random.choices(port_,weights=[0.7712,0.1982,0.0305])\n        else:\n            embark = random.choices(port_,weights=[0.80,0.1385,0.0519])\n        return embark[0]\n    else:\n        return dat.Embarked\n\ndf['Embarked']=df.apply(lambda df: embarked_zero(df),axis=1)","90a8f3de":"df.groupby('Pclass').Fare.mean()","ca22fd10":"df.Fare=df.Fare.fillna(0)\n\ndef fare_zero(dat):\n    class_1 = 103.209898\n    class_2 = 25.595244\n    class_3 = 19.498357\n    fare =0\n    if dat.Pclass == 1: \n        fare = class_1 + random.randint(0,20)\n    elif dat.Pclass == 2: \n        fare = class_2 + random.randint(0,7)\n    else:\n        fare = class_3 + random.randint(0,4)\n    return fare\n\ndf['Fare']=df.apply(lambda df: fare_zero(df),axis=1)","c2b89d41":"df['Fare_Medu'] = df['Fare'].fillna(df.Fare.median())","1ab7b886":"df.groupby('Pclass').Age.mean()","74fee1e3":"#df.Age=df.Age.fillna(df.Age.median())\ndf.Age=df.Age.fillna(0)\n\ndef age_zero(dat):\n    class_1 = 40.672757\n    class_2 = 36.855067\n    class_3 = 30.205570\n    fare =0\n    if dat.Pclass == 1: \n        fare = class_1 + random.randint(0,10)\n    elif dat.Pclass == 2: \n        fare = class_2 + random.randint(0,7)\n    else:\n        fare = class_3 + random.randint(0,4)\n    return fare\n\ndf['Age']=df.apply(lambda df: age_zero(df),axis=1)","13546989":"df['FirstName'] = 'f'\ndf['LastName'] = 'l'\n\ndef first_name(dat):\n    patern_1 = re.compile('[A-Z][a-z]+')\n    name = patern_1.findall(dat.Name)\n    first = name[1]\n    return first\n\ndef second_name(dat):\n    patern_2 = re.compile('[A-Z][a-z]+')\n    name = patern_2.findall(dat.Name)\n    last = name[0]\n    return last\n    \ndf['FirstName'] = df.apply(lambda df: first_name(df),axis=1)       \ndf['LastName'] = df.apply(lambda df: second_name(df),axis=1)  ","bd3dc971":"print('\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0438\u043c\u0435\u043d: %s'%len(df.FirstName.unique()))\nprint('\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0443\u043d\u0438\u043a\u0430\u043b\u044c\u043d\u044b\u0445 \u0444\u0430\u043c\u0438\u043b\u0438\u0439: %s'%len(df.LastName.unique()))","c442ad6f":"df.head()","088d8380":"df_final = df.copy()","04a354ae":"df_final.head()","f1bade44":"encoder = LabelEncoder()\nscaler = MinMaxScaler()\n\nlabel_futures = ['LastName',\n                 'FirstName',\n                 'Name',\n                 #'Ticket',\n                 #'Sex',\n                 #'Cabin',\n                 #'Embarked',\n                ]\ncategorical_features = ['Sex',\n                        'Cabin',\n                        'Embarked',\n                        'Pclass',\n                        'Alone',\n                        'FamOnBoard',\n                        'Ticket',\n                       ]\nnumerical_features = ['Age',\n                      'Fare',\n                      #'Fare_Medu',\n                      #'Parch',\n                      #'SibSp',\n                      #'Pclass',\n                     ]\n\n#for column in label:\n#    df_final[column] = encoder.fit_transform(df_final[column] )\n    \nfor column in label_futures:\n    df_final[column] = encoder.fit_transform(df_final[column] )\n    \ndf_final = pd.get_dummies(\n        df_final, columns=categorical_features, dummy_na=False)    \n    \n# \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445\nfor column in numerical_features:\n    df_final[column] = scaler.fit_transform(df_final[[column]])[:, 0]    ","5484946e":"df_final = df_final.drop(columns=[#'Ticket',\n                                  'Fare_Medu',\n                                  #'FirstName',\n                                  #'LastName'\n                                 ],\n                         axis=1)\ndf_final","cb8cb4eb":"# \u0422\u0435\u043f\u0435\u0440\u044c \u0432\u044b\u0434\u0435\u043b\u0438\u043c \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0447\u0430\u0441\u0442\u044c\ntrain_data = df_final.query('sample == 1').drop(['sample'], axis=1)\ntest_data = df_final.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.Survived.values     # \u043d\u0430\u0448 \u0442\u0430\u0440\u0433\u0435\u0442\nX = train_data.drop(['Survived'], axis=1)\nX_sub = test_data.drop(['Survived'], axis=1)","11aaa4fd":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.15, shuffle=True, random_state=42)","74fcd99f":"model = CatBoostClassifier(iterations=1000,\n                          depth=4,\n                          reg_lambda=0.1,\n                          learning_rate=0.13,\n                          eval_metric='Accuracy',\n                           use_best_model=True,\n                           random_seed=42\n                          )\n","bd29c466":"model.fit(X_train, y_train,\n          eval_set=(X_test, y_test),\n          verbose_eval=100,\n          #early_stopping_rounds=50,\n          use_best_model=True,\n          plot=True\n          )","dde06362":"feature_importances = pd.DataFrame()\nfi_tmp = pd.DataFrame()\nfi_tmp[\"feature\"] = X_test.columns.to_list()\nfi_tmp[\"importance\"] = model.get_feature_importance()\nfeature_importances = feature_importances.append(fi_tmp)","a1da4c7a":"# just to get ideas to improve\norder = list(feature_importances.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\nplt.figure(figsize=(10, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=feature_importances, order=order)\nplt.title(\"{} importance\".format(\"CatBoostClassifier\"))\nplt.tight_layout()","16af112d":"not_importante_df=feature_importances.sort_values(by='importance',ascending=False)[20:]\nctb_impotante_fut=df_final.drop(columns=list(not_importante_df.feature),axis=1)","19a91ac1":"# \u0422\u0435\u043f\u0435\u0440\u044c \u0432\u044b\u0434\u0435\u043b\u0438\u043c \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0447\u0430\u0441\u0442\u044c\ntrain_data = ctb_impotante_fut.query('sample == 1').drop(['sample'], axis=1)\ntest_data = ctb_impotante_fut.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data.Survived.values     # \u043d\u0430\u0448 \u0442\u0430\u0440\u0433\u0435\u0442\nX = train_data.drop(['Survived'], axis=1)\nX_sub_ctb = test_data.drop(['Survived'], axis=1)\n\nX_train_ctb, X_test_ctb, y_train_ctb, y_test_ctb = train_test_split(\n    X, y, test_size=0.15, shuffle=True, random_state=42)\n\nmodel = CatBoostClassifier(iterations=1000,\n                          depth=4,\n                          #reg_lambda=0.1,\n                          learning_rate=0.12,\n                          eval_metric='Accuracy',\n                           use_best_model=True,random_seed=42\n                          )\n\nmodel.fit(X_train_ctb, y_train_ctb,\n          eval_set=(X_test_ctb, y_test_ctb),\n          verbose_eval=100,\n          #early_stopping_rounds=50,\n          use_best_model=True,\n          plot=True\n          )","38568a2e":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\nregressor = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nregressor.fit(X_train, y_train)\ny_pred = regressor.predict(X_test)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\npredictions = regressor.predict(X_sub)","444ee926":"rdf_feature_importances = pd.DataFrame()\nrdf_fi_tmp = pd.DataFrame()\nrdf_fi_tmp[\"feature\"] = X_test.columns.to_list()\nrdf_fi_tmp[\"importance\"] = regressor.feature_importances_\nrdf_feature_importances = rdf_feature_importances.append(rdf_fi_tmp)","0dd95a3b":"# just to get ideas to improve\norder = list(rdf_feature_importances.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\nplt.figure(figsize=(10, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=rdf_feature_importances, order=order)\nplt.title(\"{} importance\".format(\"RandomForest\"))\nplt.tight_layout()","69c82da1":"not_importante_df=rdf_feature_importances.sort_values(by='importance',ascending=False)[20:]\nrdf_impotante_fut=df_final.drop(columns=list(not_importante_df.feature),axis=1)","669914a3":"# \u0422\u0435\u043f\u0435\u0440\u044c \u0432\u044b\u0434\u0435\u043b\u0438\u043c \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0447\u0430\u0441\u0442\u044c\ntrain_data_rdf = rdf_impotante_fut.query('sample == 1').drop(['sample'], axis=1)\ntest_data_rdf = rdf_impotante_fut.query('sample == 0').drop(['sample'], axis=1)\n\ny = train_data_rdf.Survived.values     # \u043d\u0430\u0448 \u0442\u0430\u0440\u0433\u0435\u0442\nX = train_data_rdf.drop(['Survived'], axis=1)\nX_sub_rdf = test_data_rdf.drop(['Survived'], axis=1)\n\nX_train_rdf, X_test_rdf, y_train_rdf, y_test_rdf = train_test_split(\n    X, y, test_size=0.15, shuffle=True, random_state=42)\n\nregressor = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nregressor.fit(X_train_rdf, y_train_rdf)\ny_pred = regressor.predict(X_test_rdf)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test_rdf, y_pred))\npredictions = regressor.predict(X_sub_rdf)","b9385b67":"train_data = df_final.query('sample == 1').drop(['sample'], axis=1)\n\nX_train, X_test, Y_train, Y_test = train_test_split(train_data.drop(['Survived'], axis=1), train_data.Survived,test_size=0.15)\nfeature_n = train_data.drop(columns='Survived',axis=1)\ntrain_dataset = lgb.Dataset(X_train, Y_train, feature_name=feature_n.columns.tolist())\ntest_dataset = lgb.Dataset(X_test, Y_test, feature_name=feature_n.columns.tolist())\n\nlgb_params = {\n        'boosting_type': 'gbdt',\n        'objective': 'binary',\n        'metric':'acc',\n        'learning_rate': 0.17,\n        #'is_unbalance': 'true',  #because training data is unbalance (replaced with scale_pos_weight)\n        'num_leaves': 31,  # we should let it be smaller than 2^(max_depth)\n        'max_depth': -1,  # -1 means no limit\n        'min_child_samples': 20,  # Minimum number of data need in a child(min_data_in_leaf)\n        'max_bin': 255,  # Number of bucketed bin for feature values\n        'subsample': 0.6,  # Subsample ratio of the training instance.\n        'subsample_freq': 0,  # frequence of subsample, <=0 means no enable\n        'colsample_bytree': 0.3,  # Subsample ratio of columns when constructing each tree.\n        'min_child_weight': 5,  # Minimum sum of instance weight(hessian) needed in a child(leaf)\n        'subsample_for_bin': 200000,  # Number of samples for constructing bin\n        'min_split_gain': 0,  # lambda_l1, lambda_l2 and min_gain_to_split to regularization\n        'reg_alpha': 0,  # L1 regularization term on weights\n        'reg_lambda': 0.13,  # L2 regularization term on weights\n        'nthread': 8,\n        'verbose': 0,\n    }\n","0aa9dbaa":"clf = lgb.LGBMClassifier()\nclf.fit(X_train, Y_train)\n\n# predict the results\ny_pred=clf.predict(X_test)\n# view accuracy\nfrom sklearn.metrics import accuracy_score\naccuracy=accuracy_score(y_pred, Y_test)\nprint('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(Y_test, y_pred)))","2062dc01":"lgbm_feature_importances = pd.DataFrame()\nlgbm_fi_tmp = pd.DataFrame()\nlgbm_fi_tmp[\"feature\"] = X_test.columns.to_list()\nlgbm_fi_tmp[\"importance\"] = clf.feature_importances_\nlgbm_feature_importances = lgbm_feature_importances.append(lgbm_fi_tmp)","9d3b4c05":"# just to get ideas to improve\norder = list(lgbm_feature_importances.groupby(\"feature\").mean().sort_values(\"importance\", ascending=False).index)\nplt.figure(figsize=(10, 10))\nsns.barplot(x=\"importance\", y=\"feature\", data=lgbm_feature_importances, order=order)\nplt.title(\"{} importance\".format(\"LGBMClassifier\"))\nplt.tight_layout()","1e7bbefa":"not_importante_df=lgbm_feature_importances.sort_values(by='importance',ascending=False)[20:]\nlgbm_impotante_fut=df_final.drop(columns=list(not_importante_df.feature),axis=1)","2e072c70":"train_data_clf = lgbm_impotante_fut.query('sample == 1').drop(['sample'], axis=1)\ntest_data_clf = lgbm_impotante_fut.query('sample == 0').drop(['sample'], axis=1)\nX_sub_clf = test_data_clf.drop(['Survived'], axis=1)\n\nX_train_clf, X_test_clf, Y_train_clf, Y_test_clf = train_test_split(train_data_clf.drop(['Survived'], axis=1), train_data_clf.Survived,test_size=0.15)\nfeature_n = train_data.drop(columns='Survived',axis=1)\ntrain_dataset = lgb.Dataset(X_train_clf, Y_train_clf, feature_name=feature_n.columns.tolist())\ntest_dataset = lgb.Dataset(X_test_clf, Y_test_clf, feature_name=feature_n.columns.tolist())\n\nclf = lgb.LGBMClassifier()\nclf.fit(X_train_clf, Y_train_clf)\n\n# predict the results\ny_pred_clf=clf.predict(X_test_clf)\n\n# view accuracy\nfrom sklearn.metrics import accuracy_score\naccuracy=accuracy_score(y_pred_clf, Y_test_clf)\nprint('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(Y_test_clf, y_pred_clf)))","a2af1854":"# \u0422\u0435\u043f\u0435\u0440\u044c \u0432\u044b\u0434\u0435\u043b\u0438\u043c \u0442\u0435\u0441\u0442\u043e\u0432\u0443\u044e \u0447\u0430\u0441\u0442\u044c\ntrain_data_nn = df_final.query('sample == 1').drop(['sample'], axis=1)\ntest_data_nn = df_final.query('sample == 0').drop(['sample'], axis=1)\n\ny_nn = train_data_nn.Survived.values     # \u043d\u0430\u0448 \u0442\u0430\u0440\u0433\u0435\u0442\nX_nn = train_data_nn.drop(['Survived'], axis=1).values\nSub_nn = test_data_nn.drop(['Survived'], axis=1).values\n\nX_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(\n    X_nn, y_nn, test_size=0.15, shuffle=True, random_state=42)","531c7e97":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_nn = sc.fit_transform(X_train_nn)\nX_test_nn = sc.fit_transform(X_test_nn)\n#y_train_nn = sc.fit_transform(y_train_nn)\n#y_test_nn = sc.fit_transform(y_test_nn)","de339571":"model_nn = M.Sequential()\nmodel_nn.add(L.Dense(256, activation='relu', input_dim = X_train_nn.shape[1], kernel_regularizer=regularizers.l1_l2(\n    l1=0.000000001, l2=0.000000001),))\nmodel_nn.add(L.Dense(128, activation=\"relu\", kernel_regularizer=regularizers.l1_l2(\n    l1=0.000000001, l2=0.000000001),))\nmodel_nn.add(L.Dropout(0.5))\nmodel_nn.add(L.Dense(1, activation='sigmoid'))\n\n\nsgd = optimizers.SGD(lr = 0.008, momentum = 0.99)\n\nmodel_nn.compile(loss='binary_crossentropy',\n              optimizer=sgd,\n              metrics=['accuracy'])\n\ncheckpoint = ModelCheckpoint('\/kaggle\/best_model_nn.hdf5', monitor=['val_accuracy'], verbose=0, mode='max')\nearlystop = EarlyStopping(monitor='val_accuracy', patience=50, restore_best_weights=True,)\ncallbacks_list = [checkpoint, earlystop]","62cdaa68":"history = model_nn.fit(X_train_nn, y_train_nn,\n                    batch_size=512,\n                    epochs=20,  # \u0444\u0430\u043a\u0442\u0438\u0447\u0435\u0441\u043a\u0438 \u043c\u044b \u043e\u0431\u0443\u0447\u0430\u0435\u043c \u043f\u043e\u043a\u0430 EarlyStopping \u043d\u0435 \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u0442 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435\n                    validation_data=(X_test_nn, y_test_nn),\n                    callbacks=callbacks_list,\n                    verbose=2,\n                    )","0ea804c1":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1,len(acc)+1)\n\nplt.plot(epochs, acc, 'b', label='Training accuracy')\nplt.plot(epochs, val_acc, 'r', label='Validation accuracy')\nplt.title('\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043d\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438 \u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training loss')\nplt.plot(epochs, val_loss, 'r', label='Validation loss')\nplt.title('\u041f\u043e\u0442\u0435\u0440\u0438 \u043d\u0430 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0438 \u0438 \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438')\nplt.legend()\n\nplt.show()","a9956375":"predictions_cat_0 = model.predict(X_test_ctb)\npredictions_rdf_0 = regressor.predict(X_test_rdf)\npredictions_lgb_0 = clf.predict(X_test_clf)\nprediction_keras_nn = model_nn.predict(X_test_nn)\nprediction_keras_nn = prediction_keras_nn[prediction_keras_nn>0]\n\nresult_0=(predictions_cat_0+predictions_rdf_0+predictions_lgb_0+prediction_keras_nn)\/4\nresult_0 = np.around(result_0).astype(int)\n\nprint('All Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, result_0)))","61d2788d":"predictions_cat = model.predict(X_sub)\npredictions_rdf = regressor.predict(X_sub_rdf)\npredictions_lgb = clf.predict(X_sub_clf)\nprediction_keras = model_nn.predict(Sub_nn)\nprediction_keras = prediction_keras_nn[prediction_keras_nn>0]","c3b197ed":"result = (predictions_cat+predictions_rdf+predictions_lgb+prediction_keras)\/4\nresult = np.around(result).astype(int)","1c47bf84":"my_submission = pd.DataFrame({'PassengerId': df_test.PassengerId ,\n                              'Survived': (result.astype(int)) })\n\nmy_submission.to_csv(\"submission.csv\",index=False)","239d6295":"my_submission","bed6323e":"## Fare \n\u041f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043e\u0447\u0435\u043d\u044c \u043c\u0430\u043b\u043e \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0437\u0430\u043c\u0435\u043d\u0438\u043c \u043d\u0435\u0438\u0437\u0432\u0435\u0441\u0442\u043d\u044b\u0435 \u043d\u0430 \u043c\u0435\u0434\u0438\u0430\u043d\u0443","4f4274cc":"## \u0412\u044b\u0434\u0435\u043b\u0438\u043c \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u043b\u0435\u0437\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0435\u0440\u0432\u044b\u0435 20 ","28505c61":"- version_1 0.778\n- version_2 0.7805333 noramalization Fare\n- version_4 0.7805333 noramalization Fare Age\n- version_4 0.7834000 normalization !=Fare, + Tickets(Random)\n- version_10 0.7869333 normalization !=Fare, + My_Tickets(Random)\n- version_11 0.7872000 normalization Fare, Age + My_Tickets(Random)\n","3cb65011":"# Model 4: Keras NN","5fca1565":"# \u0410\u043d\u0441\u0430\u043c\u0431\u043b\u0438\u0440\u0443\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u0438","69091a10":"# Model 1: CatBoostRegressor","b8913c1e":"## Embarked \n\u041d\u0430\u0439\u0434\u0435\u043c \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u044c \u043c\u0435\u0436\u0434\u0443 \u043a\u043b\u0430\u0441\u0441\u043e\u043c \u043f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u0430 \u0438 \u043c\u0435\u0442\u043e\u043c \u043f\u043e\u0441\u0430\u0434\u043a\u0438 \u0438 \u0437\u0430\u043c\u0435\u043d\u0438\u043c \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f ","96d731e1":"## Parch & SibSp","e4ba30d1":"### \u041f\u043e\u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0443\u0441\u0442\u044b\u0445 \u0441\u0442\u0440\u043e\u043a \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u0430 \u0432 \u0434\u0430\u0441\u0430\u0441\u0435\u0442\u0435","4fb77bc6":"### \u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u0433\u0440\u0430\u0444\u0438\u043a \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u0432 \u043f\u0440\u043e\u0446\u0435\u043d\u0442\u0430\u0445 \u043f\u043e\u043a\u0430\u0436\u0435\u043c \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043c\u0435\u0436\u0434\u0443 \u043a\u0430\u0431\u0438\u043d\u0430\u043c\u0438 \u0432 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0442 \u043a\u043b\u0430\u0441\u0441\u0430","c7945d56":"### \u0412\u0410\u0416\u041d\u041e! \u0434\u043b\u044f \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u043e\u0439 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c \u0442\u0440\u0435\u0439\u043d \u0438 \u0442\u0435\u0441\u0442 \u0432 \u043e\u0434\u0438\u043d \u0434\u0430\u0442\u0430\u0441\u0435\u0442","92b0f74b":"# Model 2: RandomForest","67dc0b3e":"## Name\n\n\u0421\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0434\u0432\u0430 \u0441\u0442\u043e\u043b\u0431\u0446\u0430 \u0434\u043b\u044f \u0438\u043c\u0435\u043d\u0438 \u0438 \u0444\u0430\u043c\u0438\u043b\u0438\u0438, \u0432\u043e\u0437\u044c\u043c\u0435\u043c \u0441\u0430\u043c\u044b\u0435 \u043f\u043e\u043f\u0443\u043b\u044f\u0440\u043d\u044b\u0435 \u0444\u0430\u043c\u0438\u043b\u0438\u0438 \u0438 \u0438\u043c\u0435\u043d\u0430","83ebbafd":"## \u0421\u043a\u043e\u043f\u0438\u0440\u0443\u0435\u043c \u0438\u0441\u0445\u043e\u043d\u044b\u0439 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0438 \u0431\u0443\u0434\u0435\u043c \u0440\u0430\u0431\u043e\u0442\u0430\u044c \u0443\u0436\u0435 \u0441 \u043d\u0438\u043c, \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0438\u0441\u0441\u043b\u0435\u0434\u043e\u0432\u0430\u0442\u044c \u043a\u0430\u0436\u0434\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446, \u043d\u0430\u0439\u0434\u0435\u043c \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043c\u0435\u0436\u0434\u0443 \u0441\u0442\u043e\u0431\u0446\u0430\u043c\u0438 \u0438 \u0440\u0435\u0448\u0438\u043c, \u0447\u0442\u043e \u0434\u0435\u043b\u0430\u0442\u044c \u0441 \u043a\u0430\u0436\u0434\u044b\u043c \u0438\u0437 \u043d\u0438\u0445 ","a384b517":"### Split data","4e0ecfb4":"## Pclass & Cabin\n- \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0443\u0441\u0442\u044b\u0445 \u0441\u0442\u0440\u043e\u043a \u0432 Cabin: 138697, Pclass: 0 ","4abfe3d7":"\u041f\u043e\u0441\u0442\u0440\u043e\u0438\u043c \u0433\u0430\u0440\u0444\u0438\u043a \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u0435\u0449\u0435 \u0440\u0430\u0437 \u0438 \u0443\u0431\u0435\u0434\u0438\u043c\u0441\u044f, \u0447\u0442\u043e \u043f\u0440\u043e\u043f\u043e\u0440\u0446\u0438\u0438 \u043e\u0441\u0442\u0430\u043b\u0438\u0441\u044c \u0432\u0435\u0440\u043d\u044b\u043c\u0438 ","584aa524":"# EDA\n\n\u041a\u043e\u043b\u043e\u043d\u043a\u0438 \u0434\u0430\u043d\u043d\u044b\u0445: \n- PassengerId: Id \u043f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u0430 \n- Survived: \u0416\u0438\u0432\\\u041c\u0435\u0440\u0442\u0432\n- Pclass: \u041a\u043b\u0430\u0441\u0441 \u043f\u0430\u0441\u0430\u0436\u0438\u0440\u0430\n    * 1st = \u0412\u044b\u0441\u0448\u0438\u0439 \n    + 2nd = \u0421\u0440\u0435\u0434\u043d\u0438\u0439\n    - 3rd = \u041d\u0438\u0437\u043a\u0438\u0439\n- Name: \u0418\u043c\u044f \u043f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u0430 \n- Sex: \u041f\u043e\u043b\n- Age: \u0412\u043e\u0437\u0440\u0430\u0441\u0442\n- SibSp: \u0431\u0440\u0430\u0442\u044c\u044f\u0445 \u0438 \u0441\u0435\u0441\u0442\u0440\u0430\u0445 \/ \u0441\u0443\u043f\u0440\u0443\u0433\u0430\u0445 \u043d\u0430 \u0431\u043e\u0440\u0442\u0443\n    * Sibling = brother, sister, stepbrother, stepsister\n    + Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n- Parch: \u0440\u043e\u0434\u0438\u0442\u0435\u043b\u0438\\ \u0434\u0435\u0442\u0438 \u043d\u0430 \u0431\u043e\u0440\u0442\u0443 \n    * Parent = mother, father\n    + Child = daughter, son, stepdaughter, stepson\n    - \u041d\u0435\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0434\u0435\u0442\u0438 \u043f\u0443\u0442\u0435\u0448\u0435\u0441\u0442\u0432\u0443\u044e\u0442 \u0441 \u043d\u044f\u043d\u044f\u043c\u0438, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 parch=0 \n- Ticket: \u0411\u0438\u043b\u0435\u0442\n- Fare:\n- Cabin: \u041d\u043e\u043c\u0435\u0440\\\u041a\u0430\u0431\u0438\u043d\u0430\n- Embarked: \u041f\u043e\u0440\u0442 \u043f\u043e\u0441\u0430\u0434\u043a\u0438 ","83d545a4":"# \u041f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u044b\u0445 \u0434\u043b\u044f \u043d\u0435\u0439\u0440\u043e\u043d\u043d\u044b\u0445 \u0441\u0435\u0442\u0435\u0439","5c553b15":"### \u041d\u0430\u043f\u0438\u0448\u0435\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0432\u044b\u0442\u0430\u0449\u0438\u0442 \u0438\u0437 \u043d\u043e\u043c\u0435\u0440\u0430 \u043a\u0430\u0431\u0438\u043d\u044b \u0435\u0451 \u0442\u0438\u043f (\u0440\u044f\u0434)","be58efa0":"### \u041d\u0430 \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u0438\u0438 \u044d\u0442\u0438\u0445 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432 \u043c\u043e\u0436\u043d\u043e \u0441\u043a\u0430\u0437\u0430\u0442\u044c:\n- \u041f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u044b 1 \u043a\u043b\u0430\u0441\u0441\u0430 \u0437\u0430\u043d\u0438\u043c\u0430\u043b\u0438 \u043a\u0430\u0431\u0438\u043d\u044b A,B,C,D,E\n- \u041f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u044b 2 \u043a\u043b\u0430\u0441\u0441\u0430 \u0437\u0430\u043d\u0438\u043c\u0430\u043b\u0438 \u043a\u0430\u0431\u0438\u043d\u044b F,A,D,C,E,G\n- \u041f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u044b 3 \u043a\u043b\u0430\u0441\u0441\u0430 \u0437\u0430\u043d\u0438\u043c\u0430\u043b\u0438 \u043a\u0430\u0431\u0438\u043d\u044b A,F,C,D,B,G\n\n### \u0422\u0435\u043f\u0435\u0440\u044c \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u0438\u0438 \u044d\u0442\u0438 \u0434\u0430\u043d\u043d\u044b\u0445 \u0437\u0430\u043c\u0435\u043d\u0438\u043c \u043f\u0443\u0441\u0442\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043a\u0430\u0431\u0438\u043d\u044b \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e \u0434\u0430\u043d\u043d\u044b\u0445 \u043e \u043a\u043b\u0430\u0441\u0441\u0435 \u043f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u0430 c \u043f\u0440\u0438\u0432\u044f\u0437\u043a\u043e\u0439 \u043a \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u0438 ","ce6763d2":"## Ticket\n- \u0412\u044b\u0442\u0430\u0449\u0438\u043c \u0438\u0437 \u0431\u0438\u043b\u0435\u0442\u043e\u0432 \u0431\u0443\u043a\u0432\u0435\u043d\u043d\u044b\u0435 \u043e\u0431\u0430\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \n- \u0411\u0438\u043b\u0435\u0442\u044b \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0438\u043c\u0435\u044e\u0442 \u0442\u043e\u043b\u044c\u043a\u043e \u043c\u043e\u043c\u0435\u0440\u0430 \u043e\u0431\u043e\u0437\u043d\u0430\u0447\u0438\u043c \u043a\u0430\u043a r\n- \u041f\u0443\u0441\u0442\u044b\u0435 \u0431\u0438\u043b\u0435\u0442\u044b \u0437\u0430\u043c\u0435\u043d\u0438\u043c \u043d\u0430 0 (\u043f\u043e\u043a\u0430 \u0447\u0442\u043e )","2e9d8559":"## Age\n\n\u041f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u043d\u0435 \u0442\u0430\u043a \u043c\u043d\u043e\u0433\u043e, \u043d\u043e \u0441\u0442\u043e\u0438\u0442 \u043f\u043e\u043f\u0440\u043e\u0431\u044b\u0432\u0430\u0442\u044c \u0437\u0430\u043c\u0435\u043d\u0438\u0442\u044c \u0438\u0445 \u043f\u043e \u0443\u043c\u043d\u043e\u043c\u0443 ","e0c7ecaf":"## \u0412\u044b\u0434\u0435\u043b\u0438\u043c \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u043f\u043e\u043b\u0435\u0437\u043d\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u0438 \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u0435\u0440\u0432\u044b\u0435 20 ","f54d60fc":"#### \u041e\u0446\u0435\u043d\u0438\u043c \u0443\u0440\u043e\u0432\u0435\u043d\u044c \u0432\u044b\u0436\u0438\u0432\u0430\u0435\u043c\u043e\u0441\u0442\u0438 \u0432 \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043e\u0442 \u043a\u0430\u044e\u0442\u044b, \u0430 \u0442\u0430\u043a\u0436\u0435 \u043a\u043b\u0430\u0441\u0441 \u043f\u0430\u0441\u0441\u0430\u0436\u0438\u0440\u0430 \u0438 \u0437\u0430\u043a\u0440\u0435\u043f\u043b\u0435\u043d\u043d\u0430\u044f \u0437\u0430 \u043d\u0438\u043c \u0442\u0438\u043f \u043a\u0430\u044e\u0442\u044b ","a0c9779c":"# Model 3: LightGBM"}}