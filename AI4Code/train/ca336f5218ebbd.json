{"cell_type":{"9f2fc05e":"code","84825c29":"code","c0daae86":"code","828c40a1":"code","865d67e8":"code","aca8cc9a":"code","c201fa89":"code","e2fe6ac8":"code","1dd5211f":"code","939b2710":"code","0b75d45b":"code","6c799022":"code","cc5dad24":"code","247034eb":"code","08429a0d":"code","080747d8":"code","4fa05064":"code","cfe83d89":"code","47423829":"code","7d818b19":"code","e54213a2":"code","d8aace3d":"code","7026ae58":"code","27e13c02":"code","4119fe42":"code","80425b90":"code","a05f25b3":"code","e4179df2":"code","70f6c99a":"code","20936264":"code","7ba4f34e":"code","a3aefe61":"code","6635d818":"code","aad7f5fb":"code","c3911b9b":"code","2c1333a7":"markdown","5d93bb6e":"markdown","2376b06a":"markdown","00cd4538":"markdown","ca2d0aac":"markdown"},"source":{"9f2fc05e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","84825c29":"from typing import List, Union","c0daae86":"DATA_DIR = '\/kaggle\/input\/nlp-getting-started\/'","828c40a1":"df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'), encoding='utf-8')","865d67e8":"# Quick look at structure of the data\n\ndf.info()","aca8cc9a":"# Quick look at labeled data\n\ndf.head()","c201fa89":"# How target labels are distributed\n\ndf['target'].value_counts() \/ df.shape[0]","e2fe6ac8":"sample_text = df.loc[154, 'text']\nprint(\"Sample text: \", sample_text)\nprint(\"Sample label: \", df.loc[42, 'target'])","1dd5211f":"# How long are tweets\n\ndf['text_len'] = df['text'].apply(len)\ndf['text_len'].describe()","939b2710":"# Quick look at \"long\" tweets\n\nlong_texts = df.loc[df['text_len'] > 120, ['text', 'target']]\nlong_texts.head(15)","0b75d45b":"# What are short tweets about \n\nshort_texts = df.loc[df['text_len'] <= 20, ['text', 'target']]\nshort_texts.head(15)","6c799022":"# It seems that short tweets usually are not about real disaster\nshort_texts['target'].value_counts()","cc5dad24":"import re\nimport string\nimport html\n\nprintable_chars = set(string.printable)\n\n\ndef preprocess_text(str_txt: str) -> str:\n    \"\"\"Preprocessing for raw text data\"\"\"\n    str_txt = str_txt.replace(\"\\n\", \" \").strip()\n    # Remove urls from tweet\n    str_txt = re.sub(r'(https?:\\\/\\\/)?([\\da-z\\.-]+)\\.([a-z\\.]{2,6})([\\\/\\w\\.-]*)', '', str_txt)\n    # Remove mentions\n    str_txt = re.sub(r'@([A-z0-9_]+)', '', str_txt)\n    # Remove # from hashtags\n    str_txt = re.sub(r'#([A-z0-9_]+)', '\\g<1>', str_txt)\n    # Replace time occurences\n    str_txt = re.sub(r'\\d{1,2}(?:(?:\\d{2})? (?:a|p)m)', 'TTIME', str_txt)\n    # Replace numbers\n    str_txt = re.sub(r'\\b\\d+(?:(?:[.,]?)\\d*)\\b', 'TNUMBER',str_txt)\n    # Unescape html\n    str_txt = html.unescape(str_txt)\n    # Remove _ from words like example_\n    str_txt = re.sub(r'(\\b[A-z]+)_\\b', '\\g<1>', str_txt)\n    # Remove RT\n    str_txt = re.sub(r'\\bRT\\b', '', str_txt)\n    # Remove non printable characters\n    str_txt = ''.join([ch for ch in str_txt if ch in printable_chars])\n    \n    \n    return str_txt","247034eb":"print(preprocess_text(sample_text))","08429a0d":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.preprocessing import OneHotEncoder\nimport spacy\n\nnlp = spacy.load(\"en_core_web_sm\")","080747d8":"print(nlp.Defaults.stop_words)","4fa05064":"import emoji\n\nstopwords = nlp.Defaults.stop_words\nstopwords.add('tnumber')\nstopwords.add('-pron-')\n\n\ndef preprocess_df(df: pd.DataFrame) -> pd.DataFrame:\n    df['mentions_count'] = df[    \n'text'].apply(lambda txt: len([t for t in txt.split() if t.startswith('@')]))\n    df['hashtags_count'] = df['text'].apply(lambda txt: len([t for t in txt.split() if t.startswith('#')]))\n    df['urls_count'] = df['text'].apply(lambda txt: len([t for t in txt.split() if t.startswith('http')]))\n    df['exclaimation_marks_count'] = df['text'].str.count('!')\n    df['question_marks_count'] = df['text'].str.count('\\?')\n#     df['text'] = df['text'].apply(clean)\n    df['text'] = df['text'].apply(preprocess_text)\n    df['text'] = df['text'].apply(emoji.demojize)\n    df['text_len'] = df['text'].apply(len)\n    docs = nlp.pipe(df['text'])\n    texts_processed = []\n    for doc in docs:\n        text_tokens = [token for token in doc if token.lower_ not in stopwords and token.text != \" \" and not token.is_punct]\n        texts_processed.append(text_tokens)\n    df.loc[:, 'text'] = [\" \".join(token.lemma_ for token in tokens) for tokens in texts_processed]\n    df.loc[:, 'word_count'] = [len(tokens) for tokens in texts_processed]\n\n    return df","cfe83d89":"df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'), encoding='utf-8')\ndf = preprocess_df(df)","47423829":"df.head()","7d818b19":"from nltk.tokenize import WhitespaceTokenizer\n\nwp_tokenizer = WhitespaceTokenizer()","e54213a2":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount_vec = CountVectorizer(tokenizer=wp_tokenizer.tokenize, lowercase=True)\ntext_vectorized = count_vec.fit_transform(df['text'])\npd_counts = pd.DataFrame(text_vectorized.todense(), columns=count_vec.get_feature_names(), index=df.index)\npd_counts.shape","d8aace3d":"# Most common words for positive class after data cleaning\n\npd_counts[df['target'] == 1].sum(axis=0).sort_values(ascending=False).head(15)","7026ae58":"# Most common words for negative class \n\npd_counts[df['target'] == 0].sum(axis=0).sort_values(ascending=False).head(15)","27e13c02":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import SelectKBest, chi2\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, roc_auc_score","4119fe42":"train_df, val_df = train_test_split(df, stratify=df['target'], random_state=42, test_size=0.2)","80425b90":"def df_cols_selector(col_names: Union[str, List]):\n    return FunctionTransformer(lambda x: x[col_names], validate=False)","a05f25b3":"from nltk.tokenize import WhitespaceTokenizer\n\nwp_tokenizer = WhitespaceTokenizer()\n\n\nnum_pipeline = Pipeline([ \n    ('selector', df_cols_selector(['text_len'])),\n    ('scaler', StandardScaler())\n])\ntext_pipeline = Pipeline([\n    ('selector', df_cols_selector('text')),\n    ('tfidf', TfidfVectorizer(tokenizer=wp_tokenizer.tokenize, ngram_range=(1, 2), lowercase=True))\n])\n\nfeature_union = FeatureUnion(transformer_list=[\n    ('num_pipeline', num_pipeline),\n    ('text_pipeline', text_pipeline)\n])\npipeline = Pipeline([\n    ('feature_union', feature_union),\n    ('logreg', LogisticRegression(C=5, max_iter=300))\n])","e4179df2":"pipeline.fit(train_df, train_df['target']);","70f6c99a":"y_val_test = pipeline.predict(val_df)","20936264":"print(\"Accuracy\", accuracy_score(val_df['target'], y_val_test))\nprint(\"ROC AUC\", roc_auc_score(val_df['target'], y_val_test))","7ba4f34e":"import csv\nimport uuid\n\n\ndef prepare_submission(test_data: pd.DataFrame, predicted: np.ndarray):\n    f_name = f\"submissions_{uuid.uuid4()}.csv\"\n    print(f_name)\n    with open(f_name, mode=\"w\") as f:\n        csv_writer = csv.DictWriter(f, fieldnames=['id', \"target\"])\n        csv_writer.writeheader()\n        for idx, df_row in test_data.iterrows():\n            csv_writer.writerow({\"id\": df_row['id'], \"target\": predicted[idx]})","a3aefe61":"test_df = pd.read_csv(os.path.join(DATA_DIR, 'test.csv'), encoding='utf-8')\ntest_df = preprocess_df(test_df)","6635d818":"pipeline.fit(df, df['target']);","aad7f5fb":"y_test_pred = pipeline.predict(test_df)","c3911b9b":"prepare_submission(test_df, y_test_pred)","2c1333a7":"I split labeled data on training and validation set to be able to estimate how model will perform on unseen data","5d93bb6e":"### Training model","2376b06a":"**Goal**: Predict which tweets are about real disasters and which ones are not.\n\nIn this notebook I will use logistic regression model and tf-idf vectorizer. Despite of the simplicity such approach gives quiet good result. Training and predicting don't consume large amount of resources and much faster than more complex models like transformers. This model could be used as a base line.","00cd4538":"### Making predictions","ca2d0aac":"### Data cleaning"}}