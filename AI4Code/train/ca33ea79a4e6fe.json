{"cell_type":{"32893c0b":"code","cf6102a2":"code","2bad0a85":"code","2e6bf3d6":"code","38ef9ca5":"code","370609d2":"code","6a2a57c8":"code","ecdab77b":"code","ca525d9a":"code","b32399bc":"code","c7f2d36d":"code","120efca6":"code","fd603015":"code","2447f3e7":"markdown"},"source":{"32893c0b":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression as sklearn_lr\n\nimport warnings\nwarnings.simplefilter('ignore')","cf6102a2":"seed = 13","2bad0a85":"data = pd.read_csv(\"..\/input\/qprereq\/assignment2_data.csv\", names=['X', 'y'])\nprint(f\"Data Shape : {data.shape}\")\n\ndata.head()","2e6bf3d6":"class LinearRegression():\n    \n    def __init__(self, lr, n_iter, normalize_f):\n        self.lr = lr\n        self.n_iter = n_iter\n        self.coef_ = None\n        self.intercept_ = None\n        self.normalize_f = normalize_f\n    \n    def init_constants(self, X, y):\n        self.X = X\n        if self.normalize_f:\n            self.X = self.normalize(self.X)\n        self.X = self.add_bias(self.X)\n        \n        self.y = y\n        self.n_samples = self.y.shape[0]\n        \n        self.n_features = X.shape[1]\n        self.params = np.zeros((self.n_features + 1, 1))\n    \n    def normalize(self, X):\n        mu = np.mean(X, 0)\n        std = np.std(X, 0)\n        \n        X = (X - mu)\/std\n        \n        return X\n    \n    def add_bias(self, X):\n        X = np.hstack((np.ones((X.shape[0], 1)), X))\n        \n        return X\n    \n    def fit(self, X, y):\n        self.init_constants(X, y)\n        \n        for i in range(self.n_iter):\n            self.params = self.params - (self.lr\/self.n_samples) * (self.X.T @ (self.X @ self.params - self.y))\n        \n        self.coef_ = self.params[1: ]\n        self.intercept_ = self.params[0]\n        \n        return self\n    \n    def predict(self, X):\n        X = self.normalize(X)\n        X = self.add_bias(X)\n        \n        y_pred = X @ self.params\n        \n        return y_pred\n    \n    def get_params(self):\n        return self.params\n    \n    def plot(self):\n        pass\n    \n    def r2_score(self, X=None, y=None):\n        if X is None:\n            X = self.X\n            y = self.y\n        else:\n            X = self.normalize(X)\n            X = self.add_bias(X)\n        \n        y_pred = X @ self.params\n        score = 1 - (((y - y_pred)**2).sum() \/ ((y - y.mean())**2).sum())\n        \n        return score","38ef9ca5":"X = data[['X']].values\ny = data[['y']].values\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=seed)\nprint(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)","370609d2":"lr = 0.01\nn_iter = 1_000\nnormalize = True\n\nmine = LinearRegression(lr=lr, n_iter=n_iter, normalize_f=normalize)\nskl = sklearn_lr(normalize=normalize)\n\nmine.fit(X_train, y_train)\nskl.fit(X_train, y_train)","6a2a57c8":"plt.scatter(X_train, y_train, color='blue', label='Train')\nplt.scatter(X_train, mine.predict(X_train), color='red', label='Mine')\nplt.scatter(X_train, skl.predict(X_train), color='green', label='Skl')\n\nplt.legend()\nplt.show()","ecdab77b":"plt.scatter(X_valid, y_valid, color='blue', label='Valid')\nplt.scatter(X_valid, mine.predict(X_valid), color='red', label='Mine')\nplt.scatter(X_valid, skl.predict(X_valid), color='green', label='Skl')\n\nplt.legend()\nplt.show()","ca525d9a":"mine_r2_train = mine.r2_score(X_train, y_train)\nmine_r2_valid = mine.r2_score(X_valid, y_valid)\n\nskl_r2_train = skl.score(X_train, y_train)\nskl_r2_valid = skl.score(X_valid, y_valid)\n\nprint(f\"R2 \\t\\tTrain\\t\\t\\tValid \\nMine : \\t{mine_r2_train}\\t{mine_r2_valid}\\nSKL  : \\t{skl_r2_train}\\t{skl_r2_valid}\")","b32399bc":"def mae(y_true, y_pred):\n    return np.mean(y_true - y_pred)\n\ndef mse(y_true, y_pred):\n    return np.mean((y_true - y_pred)**2)\n\ndef rmse(y_true, y_pred):\n    return np.sqrt(mse(y_true, y_pred))\n\ndef eval_metric(func, model, X, y):\n    return func(y, model.predict(X))","c7f2d36d":"# MAE\n\nmine_mae_train = eval_metric(mae, mine, X_train, y_train)\nmine_mae_valid = eval_metric(mae, mine, X_valid, y_valid)\n\nskl_mae_train = eval_metric(mae, skl, X_train, y_train)\nskl_mae_valid = eval_metric(mae, skl, X_valid, y_valid)\n\nprint(f\"MAE \\t\\tTrain\\t\\t\\tValid \\nMine : \\t{mine_mae_train}\\t{mine_mae_valid}\\nSKL  : \\t{skl_mae_train}\\t{skl_mae_valid}\")","120efca6":"# MSE\n\nmine_mse_train = eval_metric(mse, mine, X_train, y_train)\nmine_mse_valid = eval_metric(mse, mine, X_valid, y_valid)\n\nskl_mse_train = eval_metric(mse, skl, X_train, y_train)\nskl_mse_valid = eval_metric(mse, skl, X_valid, y_valid)\n\nprint(f\"MSE \\t\\tTrain\\t\\t\\tValid \\nMine : \\t{mine_mse_train}\\t{mine_mse_valid}\\nSKL  : \\t{skl_mse_train}\\t{skl_mse_valid}\")","fd603015":"# RMSE\n\nmine_rmse_train = eval_metric(rmse, mine, X_train, y_train)\nmine_rmse_valid = eval_metric(rmse, mine, X_valid, y_valid)\n\nskl_rmse_train = eval_metric(rmse, skl, X_train, y_train)\nskl_rmse_valid = eval_metric(rmse, skl, X_valid, y_valid)\n\nprint(f\"RMSE \\t\\tTrain\\t\\t\\tValid \\nMine : \\t{mine_rmse_train}\\t{mine_rmse_valid}\\nSKL  : \\t{skl_rmse_train}\\t{skl_rmse_valid}\")","2447f3e7":"Problem Statement : \n\nUsing numpy, Implement Linear regression using gradient descent\n(You can\u2019t use scikit-learn) \n\nDataset : https:\/\/drive.google.com\/file\/d\/1IX7GKikMJLwnLlVroLQ7Xx_6nGI-ixwA\/view?usp=sharing\n\nScoring Points - \n1. Explain what a cost function is.\n1. Understanding of derivatives.\n1. Should be able to explain the effects of having different learning rates."}}