{"cell_type":{"c62c9cd4":"code","9329bee4":"code","bf5b18ac":"code","e7fcb24a":"code","42767009":"code","0b608c5a":"code","3386e38b":"code","6dd7c8cf":"code","45eccacb":"code","53905497":"code","9b549c69":"code","a8e950e3":"code","bb65261d":"code","1c166041":"code","0a087555":"code","2d68519e":"code","913a3dac":"code","d295ea72":"code","82a9bec2":"code","706efe62":"code","1451f9cb":"code","853bee71":"code","4350f933":"code","4bdf694c":"code","b507af41":"code","83178ebe":"code","c3c1c419":"code","7aa6f252":"code","2178d0c3":"code","8034b53b":"code","5f5f466a":"code","2bc8976c":"code","382d723d":"code","d838384d":"code","ed681b56":"code","6943b101":"code","afa136a8":"code","171784f1":"code","1fd436e4":"code","11f26da6":"code","74bfa5c4":"code","cb2dcfbe":"code","bcd80350":"code","79e48888":"code","0c85f203":"code","5cbcce51":"code","ccab9a6c":"code","bdfe7ec7":"code","e0c552de":"code","0b8d958c":"code","c9c8c198":"code","99087af6":"code","fbbb1516":"code","7d739350":"code","356296b0":"code","a4d9a6fc":"code","dd32c720":"code","ec70640a":"code","54c78199":"markdown","8903d8db":"markdown","6fa09acc":"markdown","764414be":"markdown","45c0cc2c":"markdown","fbf23e02":"markdown","b9af7d9c":"markdown","084143f4":"markdown","700cd324":"markdown","9790fdd9":"markdown","fcc873eb":"markdown","634fa9ae":"markdown","7410a9f7":"markdown","6487ddfa":"markdown","6a6e456b":"markdown","3606705f":"markdown","375f06ab":"markdown","a11db8ec":"markdown","d62e5b95":"markdown","6197d4bb":"markdown","fafb7c5d":"markdown","090abb70":"markdown","7718f230":"markdown","95dac0c5":"markdown","8069e600":"markdown","ffff3034":"markdown","c7f80a7a":"markdown","d4d56344":"markdown","bfe5e360":"markdown","ce38ff01":"markdown","c4c59b30":"markdown","f55640a0":"markdown","e3dfbb8f":"markdown","71eafdf7":"markdown","8b1f3bda":"markdown","c72740c4":"markdown","9230ffde":"markdown","6536e6ac":"markdown","765fff21":"markdown","1347a7f2":"markdown","27e151f7":"markdown"},"source":{"c62c9cd4":"#For handling data\nimport pandas as pd\nimport numpy as np\nimport regex as re\n\n#For vizualization of data\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#Our ML algos\nfrom sklearn.linear_model import SGDClassifier\nimport lightgbm as lgb\n\n#SMOTE for imbalanced data\nfrom imblearn.over_sampling import SMOTE\n\n#Imputing\nfrom sklearn.impute import SimpleImputer\n\n#Encoders\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom category_encoders import MEstimateEncoder\n\n#Splitting data into training and test\nfrom sklearn.model_selection import train_test_split\n\n#Eval functions and model analysis\nimport eli5\nimport shap\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n#Hyperparameter tuning\nfrom bayes_opt import BayesianOptimization\nfrom skopt  import BayesSearchCV \n\n#Get rid of futurewarnings\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\n","9329bee4":"fp_train = \"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv\"\nfp_test = \"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv\"\n\ntrain = pd.read_csv(fp_train)\ntest = pd.read_csv(fp_test)\nprint(train.describe())\nprint(test.describe())","bf5b18ac":"train_cols = train.columns\ntest_cols = test.columns\n\nprint(\"Training columns are:\\n {}\".format(train_cols))\nprint(\"Testing columns are: \\n {}\".format(test_cols))","e7fcb24a":"train","42767009":"train.dtypes","0b608c5a":"cat_cols = [\"city\",\"gender\",\"relevent_experience\",\"enrolled_university\",\"education_level\",\"major_discipline\",\"experience\",\"company_size\",\"company_type\",\"last_new_job\"]\nnum_cols = [\"enrollee_id\", \"city_development_index\",\"training_hours\"]","3386e38b":"nulls = train.isnull().sum().sum()\nnn = train.notnull().sum().sum()\nnull_rows = train.shape[0] - train.dropna().shape[0]\ntotal = nulls+nn\n\nprint(\"Training data:\")\nprint(\"Total null values: {null}\" \"\\n\" \"Percent of all values: {prc}\".format(null=nulls,prc=nulls\/nn))\nprint(\"Total null rows: {nr}\" \"\\n\" \"Percent of all rows: {rprc}\".format(nr=null_rows,rprc=null_rows \/ train.shape[0]))\n\nprint(\"\\nTesting data:\")\nnulls = test.isnull().sum().sum()\nnn = test.notnull().sum().sum()\nnull_rows = test.shape[0] - test.dropna().shape[0]\ntotal = nulls+nn\n\nprint(\"Total null values: {null}\" \"\\n\" \"Percent of all values: {prc}\".format(null=nulls,prc=nulls\/nn))\nprint(\"Total null rows: {nr}\" \"\\n\" \"Percent of all rows: {rprc}\".format(nr=null_rows,rprc=null_rows \/ test.shape[0]))","6dd7c8cf":"# As a reminder take a look at the columns\ntrain.columns","45eccacb":"for column in train.columns:\n    # Only training data has target, skip that\n    if column == \"target\":\n        continue\n    \n    print(\"train.{col} nulls: {training}\" \"\\ntest.{col} nulls {test}\\n\".format(\n        training=train[column].isnull().sum(),\n        test=test[column].isnull().sum(),\n        col = column))","53905497":"# First fill gender and company type missing\ntrain[\"gender\"].fillna(\"Missing\",inplace=True)\ntest[\"gender\"].fillna(\"Missing\",inplace=True)\ntrain.company_type.fillna(\"Missing\",inplace=True)\ntest.company_type.fillna(\"Missing\",inplace=True)\n\n#Drop major_discipline\ntrain.drop(\"major_discipline\",axis=1,inplace=True)\ntest.drop(\"major_discipline\",axis=1,inplace=True)","9b549c69":"#Drop null so we don't encode those\ntrain_dropped = train.dropna(subset=[\"company_size\"])\ntest_dropped = test.dropna(subset=[\"company_size\"])","a8e950e3":"#Start by creating a dict to store the values and their numerical version in\nvalues = {}\n\n#Loop through the unique values\nfor value in train_dropped.company_size.unique():\n    \n    #Check to see if the value has a less than mark\n    if re.search(\"[<]\",value):\n        #We replace any non-numerical character with whitespace\n        num = re.sub(\"\\D\",\" \",value)\n        #We split the string by white spaces. This splits the string into the given numbers\n        spl = num.strip().split(\" \")\n        \n        #We take the first number, a.k.a the start of the range and remove one from it\n        #We do this so that we can more effectively sort the values\n        n = int(spl[0])-1\n        #Set the value into the dict, with the numerical version as key\n        values[n] = value\n        continue\n    \n    #This time we check if value has a greater than mark\n    if re.search(\"[>]\",value):\n        #Same process to find and split the number\n        num = re.sub(\"\\D\",\" \",value)\n        spl = num.strip().split(\" \")\n        #This time we add one.\n        n = int(spl[0])+1\n        values[n] = value\n        continue\n    \n    #If the value is just a purely numerical range\n    #We don't need to modify it in any way\n    #Just find it\n    num = re.sub(\"\\D\",\" \",value)\n    spl = num.strip().split(\" \")\n    values[int(spl[0])] = value\n\n    \nvalues","bb65261d":"#Setup a list of the classes we'll feed the encoder\nclasses = []\n\n#We keep looping as long as the dict has values\nwhile values:\n    #Find the smallest key value\n    min_key = min(values.keys())\n    #Find the class that matches the smaller key value\n    cl = values[min_key]\n    #We set this into the list\n    classes.append(cl)\n    #And remove it from the dict\n    del values[min_key]\n\nclasses","1c166041":"#Set the encode number\ncount = 0\n\n#Setup columns for encoding\ntrain_dropped[\"company_size_encoded\"] = train_dropped.company_size\ntest_dropped[\"company_size_encoded\"] = test_dropped.company_size\n\n#Go through each class in order\nfor cl in classes:\n    #Replace the matching class with the encode\n    train_dropped.company_size_encoded.replace(cl,count,inplace=True)\n    test_dropped.company_size_encoded.replace(cl,count,inplace=True)\n    #Up the encode \n    count += 1","0a087555":"#With the encoding done we can drop the normal column\ntrain_dropped.drop(\"company_size\",axis=1,inplace=True)\ntest_dropped.drop(\"company_size\",axis=1,inplace=True)","2d68519e":"# By joining the encoded column into the original dataframe\n# We get the encoded values for non-null rows, and null for rows that had originally null\ntrain = train.join(train_dropped.company_size_encoded)\ntest = test.join(test_dropped.company_size_encoded)","913a3dac":"# Now we can impute the encoded row missing values\n\n# Setup imputer\nsi = SimpleImputer(strategy=\"median\")\n\n#Fit and transform training set\ntrain[\"company_size_encoded\"] = si.fit_transform(np.array(train[\"company_size_encoded\"]).reshape(-1,1))\n#Transform test set\ntest[\"company_size_encoded\"] = si.transform(np.array(test[\"company_size_encoded\"]).reshape(-1,1))\n\n# Now we can drop the original company_size column\ntrain.drop(\"company_size\",axis=1,inplace=True)\ntest.drop(\"company_size\",axis=1,inplace=True)","d295ea72":"train.experience.unique()","82a9bec2":"def experience_oh(df):\n    \"\"\"\n    OneHotEncode the experience column\n    \"\"\"\n    #Create a new column with the >20 values and same with <1\n    df[\"experience_>20\"] = df.experience.eq(\">20\")\n    df[\"experience_<1\"] = df.experience.eq(\"<1\")\n\n    #Then replace the values in the original column with nan\n    df[\"experience\"] = df.experience.apply(lambda x: np.nan if x in [\">20\",\"<1\"] else x)\n    \n    return df[\"experience\"]","706efe62":"#Apply the function to both dataframes\ntrain[\"experience\"] = experience_oh(train)\ntest[\"experience\"] = experience_oh(test)\n#Check to see if got rid of wanted values\nprint(train.experience.unique())\nprint(test.experience.unique())","1451f9cb":"train.last_new_job.unique()","853bee71":"def last_job_oh(df):\n    \"\"\"\n    OneHotEncode the last_new_job column smartly\n    \"\"\"\n    df[\"last_new_job_>4\"] = df.last_new_job.eq(\">4\")\n    df[\"last_new_job_never\"] = df.last_new_job.eq(\"never\")\n\n    \n    df[\"last_new_job\"] = df.last_new_job.apply(lambda x: np.nan if x in [\">4\",\"never\"] else x)\n    \n    return df[\"last_new_job\"]","4350f933":"#Apply function to both dataframes\ntrain[\"last_new_job\"] = last_job_oh(train)\ntest[\"last_new_job\"] = last_job_oh(test)\n#Check to see if got rid of wanted values\nprint(train.last_new_job.unique())\nprint(test.last_new_job.unique())","4bdf694c":"#Drop nan from both dataframes\ntrain.dropna(inplace=True)\ntest.dropna(inplace=True)\ntrain.head()","b507af41":"train.relevent_experience.unique()","83178ebe":"#Check value, if \"has relevent experience\" turn the value into 1, if not 0\n#Apply to both dataframes\ntrain[\"relevent_experience\"] = train.relevent_experience.apply(lambda x: 1 if str(x) == 'Has relevent experience' else 0)\ntest[\"relevent_experience\"] = test.relevent_experience.apply(lambda x: 1 if str(x) == 'Has relevent experience' else 0)","c3c1c419":"#Check to make sure we only have 0 and 1\ntrain.relevent_experience.unique()","7aa6f252":"train.select_dtypes([\"object\"]).nunique()","2178d0c3":"train[\"city\"].value_counts()","8034b53b":"train_c = train.copy()\ntarget_train = train_c.pop(\"target\")\n\nenc_train = train_c.sample(frac=0.15)\nenc_target = target_train[enc_train.index]\n\ntrain_pre = train_c.drop(enc_train.index)\ntarget_pre = target_train[train_pre.index]","5f5f466a":"enc = MEstimateEncoder(cols=[\"city\"],m=6)\n\nenc.fit(enc_train,enc_target)\n\ntrain_aff = enc.transform(train_pre)\ntrain_aff[\"target\"] = target_train\ntrain = train_aff","2bc8976c":"#List the columns we've encoded in the last steps\nencoded = [\"last_new_job\",\"experience\",\"relevent_experience\",\"company_size\",\"city\"]\n#Also list the new ones we've created\nnew = [\"last_new_job_>4\",\"last_new_job_never\",\"experience_>20\",\"experience_<1\",\"company_size_encoded\"]\nfor col in encoded:\n    cat_cols.remove(col)\nfor col in new:\n    num_cols.append(col)\nfor col in encoded:\n    num_cols.append(col)\n\nnum_cols.remove(\"company_size\")\ncat_cols.remove(\"major_discipline\")","382d723d":"print(\"cat_cols: {}\\nnum_cols: {}]\".format(cat_cols,num_cols))","d838384d":"oh = OneHotEncoder(sparse=False)\n#Create copys to work on\ntrain_c = train.copy()\ntest_c = test.copy()\n#Fit and transform on training set\ntrain_c_cats = pd.DataFrame(oh.fit_transform(train_c[cat_cols]))\n#Transform test set\ntest_c_cats = pd.DataFrame(oh.transform(test_c[cat_cols]))\n\n#Put column names back\ntrain_c_cats.columns = oh.get_feature_names(cat_cols)\ntest_c_cats.columns = oh.get_feature_names(cat_cols)\n\n#Put the index back\ntrain_c_cats.index = train_c.index\ntest_c_cats.index = test_c.index\n\n#Drop the old categorical values\ntrain_c.drop(cat_cols,axis=1,inplace=True)\ntest_c.drop(cat_cols,axis=1,inplace=True)\n\n#Create the new dataframes\ntrain_oh = pd.concat([train_c,train_c_cats],axis=1)\ntest_oh = pd.concat([test_c,test_c_cats],axis=1)","ed681b56":"train_oh.dtypes","6943b101":"# Create a dict to store our columns to conver\ndtypes = {}\n\n# Go through all the numerical columns\nfor col in num_cols:\n    # Set the column to be converted to float\n    dtypes[col] = float\n\n# Convert training and testing data\ntrain_oh = train_oh.astype(dtypes)\n# We can only target encode once we have the targets at the end of the file\ndtypes.pop(\"city\")\ntest_oh = test_oh.astype(dtypes)","afa136a8":"train_oh.dtypes","171784f1":"test_oh.dtypes","1fd436e4":"#Fetch target from training data\ny = train_oh.target\n#Drop the target from the original training data\ntrain_oh.drop(\"target\",inplace=True,axis=1)","11f26da6":"#Now chech for imbalance\nsns.countplot(x=y)\nf = len([x for x in y if x == 0])\nt = len([x for x in y if x == 1])\nprint(r\"False: {f}\" \"\\n\" \"True: {t}\" \"\\n\" \"Percent of true: {p:%}\".format(f = f, t=t,p= round(t\/f,2)))","74bfa5c4":"sm = SMOTE(random_state=1)\nsm_train, sm_target = sm.fit_resample(train_oh,y)","cb2dcfbe":"x_train,x_test,y_train,y_test = train_test_split(sm_train, sm_target,random_state=1,test_size=0.3)","bcd80350":"#Setup the tuner\ndef bayes_tuner(X, y, init_round=15, opt_round=25, n_folds=3, random_seed=6,n_estimators=10000, output_process=False):\n    #Prep our data\n    lgb_train = lgb.Dataset(data=x_train, label=y_train, free_raw_data=False)\n    #Setup the parameters\n    def lgb_eval(learning_rate,num_leaves, feature_fraction, max_depth, max_bin, min_data_in_leaf,min_sum_hessian_in_leaf,subsample):\n        params = {'application':'binary', 'metric':'auc'}\n        params['learning_rate'] = max(min(learning_rate, 1), 0)\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['max_bin'] = int(round(max_depth))\n        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n        params['subsample'] = max(min(subsample, 1), 0)\n        \n        #Get the results with cv and return them\n        cv_result = lgb.cv(params, lgb_train, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n        return max(cv_result['auc-mean'])\n    \n    #Setup the optimizer\n    optimizer = BayesianOptimization(lgb_eval, {'learning_rate': (0.01, 1.0),\n                                            'num_leaves': (24, 80),\n                                            'feature_fraction': (0.1, 0.9),\n                                            'max_depth': (5, 30),\n                                            'max_bin':(20,90),\n                                            'min_data_in_leaf': (20, 200),\n                                            'min_sum_hessian_in_leaf':(0,100),\n                                           'subsample': (0.01, 1.0)}, random_state=0)\n    \n    #Maximize score\n    optimizer.maximize(init_points=init_round, n_iter=opt_round)\n    \n    #Fetch the auc scores\n    model_auc=[]\n    for model in range(len(optimizer.res)):\n        model_auc.append(optimizer.res[model]['target'])\n    \n    #Fetch the best param\n    return optimizer.res[pd.Series(model_auc).idxmax()]['target'],optimizer.res[pd.Series(model_auc).idxmax()]['params']\n\n#Finally run the optimizer to get the best params for our model\nopt_params = bayes_tuner(x_train, y_train, init_round=5, opt_round=10, n_folds=4, random_seed=0,n_estimators=10000)\nopt_params = opt_params[1]","79e48888":"opt_params['num_leaves'] = int(round(opt_params['num_leaves'],2))\nopt_params['max_depth'] = int(round(opt_params['max_depth'],2))\nopt_params['min_data_in_leaf'] = int(round(opt_params['min_data_in_leaf'],2))\nopt_params['max_bin'] = int(round(opt_params['max_bin'],2))\nopt_params['metric'] = 'auc'\nopt_params['objective'] = \"binary\"\nopt_params","0c85f203":"training_data = lgb.Dataset(x_train,y_train)\ntesting_data = lgb.Dataset(x_test,y_test)\nnum_rounds = 15000\nclf = lgb.train(opt_params,training_data,num_rounds,valid_sets=[training_data,testing_data],verbose_eval = 500, early_stopping_rounds=250)","5cbcce51":"#Make predictions using the testing data\ny_pred = clf.predict(x_test,num_iteration=clf.best_iteration)\n#Also make predictions with the training data for comparison\ny_train_pred = clf.predict(x_train,num_iteration=clf.best_iteration)","ccab9a6c":"#Fetch acc score for test and training predictions\nacc_test = roc_auc_score(y_test, y_pred)\nacc_train = roc_auc_score(y_train,y_train_pred)\n\nprint(r\"Test accuracy: {test}\" \"\\n\" \"Train accuracy: {train}\".format(test=acc_test,train=acc_train))","bdfe7ec7":"#Make preds into 0 or 1\ny_pred_rounded = y_pred.round()\n#Create the confusion matrix, and normalize results to easily see percentages\ncm = confusion_matrix(y_test,y_pred_rounded,normalize=\"all\")\n#Create a easy to display version of the confusion matrix\ncm_disp = ConfusionMatrixDisplay(confusion_matrix = cm)\n#Display the matrix\ncm_disp.plot()","e0c552de":"# We read in the final targets\nfinal_targets = pd.DataFrame(np.load(\"..\/input\/job-change-dataset-answer\/jobchange_test_target_values.npy\"),columns=[\"target\"])\n\n# We need to take out targets for rows that were dropped\nfor indx in final_targets.index:\n    # If the indx is in the preds then we do nothing\n    if indx in test_oh.index:\n        continue\n    else:\n        # If it isn't then drop it\n        final_targets.drop(index=indx,inplace=True)\n    \nfinal_targets","0b8d958c":"# Make a copy of the dataframe to work on\ntest_c = test_oh.copy()\n# We also make a copy of the targets\ntargets = final_targets.copy()\n\n# Take the targets out to index by them\ntarget_test = targets.pop(\"target\")\n# Take out a sample of the dataframe for encoding\nenc_test = test_c.sample(frac=0.3)\n\n# Take a the targets that match the testing data index\nenc_target = target_test[enc_test.index]\n\n# Take out the data used for encoding\ntest_pre = test_c.drop(enc_test.index)\n# Take out the targets used for encoding\ntarget_pre = target_test[test_pre.index]","c9c8c198":"# Setup the encoder\nenc = MEstimateEncoder(cols=[\"city\"],m=6)\n\n# Fit the encoder with the data we split for encoding\nenc.fit(enc_test,enc_target)\n\n# Use the encoder to transform the pretest data\ntest_aff = enc.transform(test_pre)\n# Set the the test data to the transformed one\ntest = test_aff","99087af6":"# Make a predictions on the test data\npreds = clf.predict(test)\n\n#Turn it into a dataframe, with the corresponding id\nsubmission_preds= pd.DataFrame({'enrollee_id': test.enrollee_id,'target':preds})\nsubmission_preds","fbbb1516":"# Save the predictions for submission\nsubmission_preds.to_csv('submission_preds.csv',index=False)","7d739350":"# We need to take out targets for rows that were used in encoding\nfor indx in final_targets.index:\n    # If the indx is in the preds then we do nothing\n    if indx in test.index:\n        continue\n    else:\n        # If it isn't then drop it\n        final_targets.drop(index=indx,inplace=True)\n    \nfinal_targets","356296b0":"#Fetch acc score\nacc = roc_auc_score(final_targets, preds)\n\nprint(r\"Test accuracy: {}\".format(acc))","a4d9a6fc":"# Set up a variable to easily look at different samples of data\nx = 2\n# Setup an explainer\nexp = shap.TreeExplainer(clf)\n\n# Fetch a part of the data to look at\nplot_data = test[x:x+1]\n\n# Get the shap values for the selected data\nshaps = exp.shap_values(plot_data)\n# Init JavaScript so we can disp. the plot\nshap.initjs()\n\n# Create a plot to see how each feature affects the prediction\nshap.force_plot(exp.expected_value[1], shaps[1], plot_data)","dd32c720":"# Plot the permutation importance scores for each feature\nlgb.plot_importance(clf)","ec70640a":"# Setup a tree graph for the model\ngraph = lgb.create_tree_digraph(clf)\n# Set the graph size to slightly bigger\ngraph.graph_attr.update(size=\"110,110\")\n# Display the graph\ngraph","54c78199":"In order to allow the model to learn the data in a numerical form when label encoded we need to make sure that as the label encoded values go up, it corresponds in the company size going up. That means that we need to setup the classes in the correct order for the encoder.\n\nWe coould type them out manually, but that in a real world application that would mean any new value for `company_size` would break the code.\n\nWe're going to automate the process of creating the list, so that it will always be up to date and in order.","8903d8db":"The worst offenders off the list are: \\\n`gender` \\\n`major_discipline` \\\n`company_size` \\\n`company_type` \n\nSince there's only a few different values for this feature and the missing values are a minority we can simply fill the missing values with a placeholder value. \\\n\nNext `major_discipline`. By taking a look at the data we can see that most of the values are \"STEM\". The next largest value is missing. Because of this we'll just drop the column \\\n\n`company_size` and `company_type` are both linked to each other quite strongly. Indeed we see that they have almost the exact same amount of missing values. What we will do, is first encode `company_size`. Then we will impute the missing values. For `company_type` we'll simply fill \"missing\"","6fa09acc":"We see here that the columns we manually encoded still have `object` datatype. We can fix this by converting all columns to floats.","764414be":"# Labeling and OneHotEncoding categorical data","45c0cc2c":"### Experience\n\nStart by looking at the unique values within the column.","fbf23e02":"We're going to manually OneHotEncode experience,relevent_experience and last_new_job \\\nThere's no point in encoding every single value, since the column is a combination of categorical and numerical data \\\nThese have only a few values that need encoding, we'll turn the rest into numerical values","b9af7d9c":"The dataset has enough of an unbalance that using smote will help us train the algo","084143f4":"`enrollee_id` should have no nan values so we can skip that column. \\\nNext we'll see how many nulls each column has in both datasets","700cd324":"### Dropping nan values\nWe drop nan values instead of imputing them bc we created nan values in the manual encoding. Imputing these would yield bad results.","9790fdd9":"### Confusion matrix\n\nWe'll also use a confusion matrix to see how many false positives and false negatives we got","fcc873eb":"# Exploratory data analysis (EDA)","634fa9ae":"## Permutation importance\n\nWe can also check permutation impo","7410a9f7":"Now we have a list of the classes ordered from the smallest company size to the largest.","6487ddfa":"We can see that city has a large amount of different values. Let's take a look at the distribution between them.","6a6e456b":"Let's also take a look at our missing values and asses if we should drop them, or impute them.","3606705f":"We then setup a function to manually OH-encode experience. \\\nWe do this so we can easily apply this to the train and test data","375f06ab":"# Final submission\n\nNow we'll create the final submission file for the task.\\\nWe then store the predictions in a dataframe with the IDs","a11db8ec":"## LabelEncoding\n\nThe only column we'll LabelEncode is `company_size`. We can label encode this column because the company size does follow a relationship from a smaller size to a bigger one. LabelEncoding this lets the model learn that relationship better.","d62e5b95":"### last_new_job\n\nSimilar process to experience. \\\nStart by checking unique values","6197d4bb":"### Target Encoding\n\nWe'll take a look at the amount and rarity of the remaining columns.","fafb7c5d":"Some of the categorical columns are already very close to being numerical (such as `last_new_job`) \\\nWe'll do some regex to turn these into numerical values. \\\nFor the rest, we'll use OH-encoding. ","090abb70":"# Examining our model\n\nNow that we've set the model up we can take a look at some different metrics to see what parts of the data actually drive the model. \\\nIn a real life scenario this would be the most valuable part.\n\n## SHAP\n\nWe'll start by looking at shap values","7718f230":"# Imports and reading in data","95dac0c5":"### Updating lists\n\nWe need to update the categorical and numerical columns lists.","8069e600":"### Checking data types\n\nWe can check to make sure we only have numerical data by looking at data types","ffff3034":"Using this info we'll manually make a list of the categorical columns and numerical columns","c7f80a7a":"## Algo OH-Encoding\n\nNow we'll OneHot-encode the rest of the categorical columns. \n\nWe start by creating the OH-Encoder and creating copys of our dataframes.","d4d56344":"## Decision tree\nWe can also plot our model into a decision tree.","bfe5e360":"# Model creation, prediction and assesment\n\n## Model creation\n\nWe start by splitting the data, so that we can measure the models performance.","ce38ff01":"From all of these, we can figure that the experience of the applicant is quite important. Based on shap values the more experience an applicant has the lower the chance that they're loking for a job change. It also seems that the training hours spent on an applicant heavily increase the chance that they're looking for a job change. ","c4c59b30":"### Hyperparameter tuning\n\nWe'll use hyperparameter tuning to create and tune our model","f55640a0":"We give a value of 0 if they have no experience and a value of 1 if they do","e3dfbb8f":"# Examining the target and fixing imbalance\n\nWe seperate the target and check how many true values we have compared to false","71eafdf7":"Let's take a look at the data to examine data types and also see what they've been classified as by pandas","8b1f3bda":"### relevent_experience\n\nSimilar to earlier steps, but a little simpler. This column only has two values, which we can describe with just 0 or 1. \\\nAs always start with checking unique values","c72740c4":"### Model setup and training\n\nNow we create the model. \\\nWe'll be using LGBMClassifier","9230ffde":"We can see that both of the datasets have a massive amount of rows with missing data. \\\nThis means that just dropping missing data would harm the model quite a lot.\n\nWhat we'll do instead is look at each column and how much nan values affect it and how we can impute them.","6536e6ac":"Start by taking a general look at the columns","765fff21":"## OneHotEncoding manually","1347a7f2":"## Assesment\n\nWe'll asses the models performance using multiple different metrics, starting with roc_auc_score\n### roc_auc_score","27e151f7":"We have quite a few rarely occuring values. Because of this this feature would be a good fit for target encoding."}}