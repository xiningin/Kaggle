{"cell_type":{"6c47503f":"code","86b4c212":"code","5ac6e0c7":"code","3a3b08fa":"code","33969a6e":"code","8b266a73":"code","50ec2248":"code","e1f9a381":"code","49651eda":"code","8de91099":"code","6549bd11":"code","440f9287":"code","2cae9500":"code","07d67dff":"code","3f1a545b":"code","842a59a3":"code","887b8fc7":"code","cb835a15":"code","7741f194":"code","246cb6c5":"code","466de7c0":"code","abf263ce":"code","aab78677":"code","61850153":"code","bc956bba":"code","7b9379af":"code","9fc9ddd8":"code","17fbc11d":"code","ed86612c":"code","02e2b6b9":"code","815603d4":"code","a7eeec9d":"code","38a9d02b":"code","408bb2d0":"code","dca2e440":"code","983a94f2":"code","c6589c9f":"code","164d90d7":"code","93b20d50":"code","f7518d10":"code","b961ccf3":"code","f48d1f7d":"code","8485d5e3":"code","efebe8b3":"code","3ea88d47":"code","8010f9d1":"code","8ffc5837":"code","4b98a417":"code","a42aac6d":"code","ef45c030":"code","288e58db":"code","8ace8bb1":"code","7dcc6285":"code","615938e3":"code","97ce9fbb":"code","a9fd30f0":"code","e2404442":"code","b8f58527":"code","a928b179":"code","24a8f49f":"code","ea48cf64":"code","18a4284d":"code","4a5335e9":"code","d12a2207":"code","ad035bec":"code","ce05446a":"code","afc6af23":"code","44d0eea4":"code","dc47e10a":"code","c0c493c6":"code","d795f772":"code","1fc62d96":"code","c85a0689":"code","9d63819a":"code","d3628a6c":"code","4f9a51ee":"code","2c835272":"code","33f366a7":"code","921f43c7":"code","dcfb2954":"code","76f46383":"code","469c844e":"code","037e5ac7":"code","29e47d74":"code","d3ccd538":"code","a1f488d9":"code","939bddd2":"code","eeae0b91":"code","9d7fff2b":"code","edb2d674":"code","29e9c03e":"code","93aeee0c":"code","cab9e74d":"code","5af0480e":"code","931806c4":"code","7c18731b":"code","15244ca2":"code","f6e76f07":"code","b1e3e9cd":"code","62668a81":"code","3b648b6c":"code","13887b92":"code","6164c065":"code","f3c0d6de":"code","6635571d":"code","e773b012":"code","6aa60fef":"code","9d6d1d84":"code","f29679ed":"code","538c1ed9":"code","199551f7":"code","b05c00f9":"code","54e06644":"markdown","89b7cdc0":"markdown","ab681553":"markdown","813c3ef5":"markdown","4e707fe3":"markdown","a0877b71":"markdown","14a5cf2f":"markdown","8fbd3e1d":"markdown","a7a948d3":"markdown","dcc46e15":"markdown","eb616c8c":"markdown","76d1a59b":"markdown","aae533e8":"markdown","3786f74b":"markdown"},"source":{"6c47503f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","86b4c212":"import pandas as pd, numpy as np\nimport matplotlib.pyplot as plt, seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","5ac6e0c7":"ship_train = pd.read_csv('..\/input\/titanic\/train.csv', index_col=\"PassengerId\")\nship_train.tail()","3a3b08fa":"ship_train.info()\n","33969a6e":"ship_test = pd.read_csv('..\/input\/titanic\/test.csv', index_col=\"PassengerId\")\nship_test.head()","8b266a73":"ship_test.info()","50ec2248":"ship = pd.concat([ship_train,ship_test],axis=0)\nship.head()","e1f9a381":"ship.shape","49651eda":"ship.info()","8de91099":"round(100*(ship.isnull().sum()\/len(ship.index)), 2)","6549bd11":"ship.describe()","440f9287":"sns.pairplot(ship)\nplt.show()","2cae9500":"import seaborn as sns\nsns.countplot(x=\"Survived\", data=ship_train);","07d67dff":"ship.Embarked.value_counts()","3f1a545b":"ship.Embarked.isnull().sum()","842a59a3":"ship.Embarked.fillna(ship.Embarked.mode()[0],inplace=True)\nship.info()","887b8fc7":"ship.Fare.fillna(ship.Fare.median(),inplace=True)\nship.info()","cb835a15":"sns.boxplot(data=ship,x='Sex',y='Age');","7741f194":"plt.figure(figsize=[10,6])\nsns.boxplot(data=ship,x='Pclass',y='Age',hue='Sex');","246cb6c5":"ship[ship['Age'].isnull()]","466de7c0":"#Imputing age\nship.groupby(['Sex','Pclass'])['Age'].median()","abf263ce":"ship['Age']= ship['Age'].fillna(ship.groupby(['Sex','Pclass'])['Age'].transform('median'))\n","aab78677":"ship.Age.isnull().sum()","61850153":"ship.Age.describe()","bc956bba":"# ship.Age.describe(percentiles=[.25, .5, .75, .90, .95, .99])","7b9379af":"def funcAgeBin(row):\n    age = row.Age\n    if age <=13:\n        return \"children\"\n    elif age <=18:\n        return \"teenagers\"\n    elif age <= 32:\n        return \"young_adult\"\n    elif age <= 55:\n        return \"middle_aged\"\n    elif age <= 80:\n        return \"old\"\n    else:\n        return \"missing\"\n# ship[\"AgeBin\"] = ship.apply(funcAgeBin, axis=1)\n\n","9fc9ddd8":"ship.head()","17fbc11d":"ship.info()","ed86612c":"ship.Fare.value_counts()","02e2b6b9":"ship['FamilySize'] = ship.SibSp+ship.Parch+1\nship.head()","815603d4":"dummy1 = pd.get_dummies(ship[['Sex','Embarked']],drop_first=True)\ndummy1.head()","a7eeec9d":"ship =pd.concat([ship,dummy1],axis=1)\nship.head()","38a9d02b":"ship = ship.drop(['Sex','SibSp','Parch','Embarked'],axis=1)\nship.head()","408bb2d0":"ship.info()","dca2e440":"#Checking for outliers\nnum_ship = ship[['Pclass','Age','Fare','FamilySize']]\nnum_ship.describe(percentiles=[.25, .5, .75, .90, .95, .99])","983a94f2":"num_cols = list(num_ship.describe().columns)\nlen(num_cols)\nplt.figure(figsize=(20,10))\nfor i in range(1,len(num_cols)+1):\n    plt.subplot(1,4,i)\n    sns.boxplot(ship[num_cols[i-1]],orient='v')\n    plt.title(\"Distribution for \"+num_cols[i-1])\nplt.show()","c6589c9f":"plt.figure(figsize=(20,10))\nfor i in range(1,len(num_cols)+1):\n    plt.subplot(2,4,i)\n    sns.distplot(ship[num_cols[i-1]])\n    plt.title(\"Distribution for \"+num_cols[i-1])\nplt.show()","164d90d7":"#Too many NAs in cabin\nship=ship.drop(['Cabin'],axis=1)","93b20d50":"#Dropping columns Name and Ticket since they won't have much impact on the prediction\nship=ship.drop(['Name','Ticket'],axis=1)","f7518d10":"ship.head()","b961ccf3":"# Dropping columns Name and Ticket since they won't have much impact on the prediction\n# ship=ship.drop(['Pclass','Fare'],axis=1)","f48d1f7d":"plt.figure(figsize=[10,10])\nsns.heatmap(ship.corr(),annot=True,cmap='YlGnBu');","8485d5e3":"#Splitting train and test data again\nship_train = ship.iloc[0:891,:]\nship_train.tail()","efebe8b3":"ship_test = ship.iloc[891:,:]\nship_test.head()","3ea88d47":"#Splitting data to train_X and train_y\ny_train=ship_train.pop('Survived')\n\nprint(y_train)","8010f9d1":"X_train = ship_train\nX_train.head()","8ffc5837":"#Rescaling the features using MinMax Scaling\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","4b98a417":"# Apply scaler() to all the columns\nnum_vars = ['FamilySize','Pclass','Fare']\n\nX_train[num_vars] = scaler.fit_transform(X_train[num_vars])\nX_train","a42aac6d":"X_train.head()","ef45c030":"X_train.describe()","288e58db":"#Building model using statsmodel\nimport statsmodels.api as sm\n# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","8ace8bb1":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","7dcc6285":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","615938e3":"#Dropping Embarked_Q due to insignificant coefficient\nX_train.drop(['Embarked_Q'],axis=1,inplace=True)","97ce9fbb":"logm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","a9fd30f0":"#Dropping Fare due to insignificant coefficient\nX_train.drop(['Fare'],axis=1,inplace=True)","e2404442":"logm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","b8f58527":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","a928b179":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train)\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","24a8f49f":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","ea48cf64":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","18a4284d":"y_train_pred_final = pd.DataFrame({'Survived':y_train.values, 'Survived_Prob':y_train_pred})\ny_train_pred_final['PassengerID'] = y_train.index\ny_train_pred_final.head()","4a5335e9":"y_train_pred_final['predicted'] = y_train_pred_final.Survived_Prob.map(lambda x: 1 if x > 0.5 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","d12a2207":"from sklearn import metrics","ad035bec":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final.predicted )\nprint(confusion)","ce05446a":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.Survived, y_train_pred_final.predicted))","afc6af23":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","44d0eea4":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","dc47e10a":"# Let us calculate specificity\nTN \/ float(TN+FP)","c0c493c6":"# Calculate false postive rate\nprint(FP\/ float(TN+FP))","d795f772":"# positive predictive value \nprint (TP \/ float(TP+FP))","1fc62d96":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","c85a0689":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","9d63819a":"fpr, tpr, thresholds = metrics.roc_curve(y_train_pred_final.Survived, y_train_pred_final.predicted , drop_intermediate = False )","d3628a6c":"draw_roc(y_train_pred_final.Survived, y_train_pred_final.Survived_Prob)","4f9a51ee":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Survived_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","2c835272":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","33f366a7":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","921f43c7":"y_train_pred_final['final_predicted'] = y_train_pred_final.Survived_Prob.map( lambda x: 1 if x > 0.4 else 0)\n\ny_train_pred_final.head()","dcfb2954":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.Survived, y_train_pred_final.final_predicted)","76f46383":"confusion2 = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final.final_predicted )\nconfusion2","469c844e":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","037e5ac7":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","29e47d74":"# Let us calculate specificity\nTN \/ float(TN+FP)","d3ccd538":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","a1f488d9":"# Positive predictive value \nprint (TP \/ float(TP+FP))","939bddd2":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","eeae0b91":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi\/TPR\/Recall\/HitRate','speci\/TNR',\n                                     'FPR','FNR','PositivePredictiveValue\/Precision','NegativePredictiveValue'])\nfrom sklearn.metrics import confusion_matrix\n\nnum = 0.4\n\ncm1 = metrics.confusion_matrix(y_train_pred_final.Survived, y_train_pred_final[0.4] )\nTP = cm1[1,1] # true positive \nTN = cm1[0,0] # true negatives\nFP = cm1[0,1] # false p;ositives\nFN = cm1[1,0] # false negatives\ntotal1=sum(sum(cm1))\naccuracy = (TN+TP)\/total1\n\nspeci = TN\/(TN+FP)#'speci\/TNR'\nsensi = TP\/(FN+TP)#'sensi\/TPR\/Recall\/HitRate'\nFPR = FP\/(TN+FP)#'FPR'\nFNR = FN\/(FN+TP)#'FNR'\nPPV = TP \/(TP+FP)#'PositivePredictiveValue\/Precision'\nNPV = TN \/(TN+ FN)#'NegativePredictiveValue'\n\ncutoff_df.loc[0.4] =[ 0.4 ,accuracy,sensi,speci,FPR,FNR,PPV,NPV]\ncutoff_df","9d7fff2b":"cols = X_train.columns","edb2d674":"y_test = ship_test.pop('Survived')\ny_test.head()","29e9c03e":"X_test = ship_test\nX_test.head()","93aeee0c":"X_test[num_vars] = scaler.transform(X_test[num_vars])\nX_test.head()","cab9e74d":"X_test=X_test[cols]","5af0480e":"X_test_sm = sm.add_constant(X_test)","931806c4":"X_test_sm","7c18731b":"y_test_pred = res.predict(X_test_sm)","15244ca2":"y_test_pred","f6e76f07":"y_test_df = pd.DataFrame(y_test_pred)","b1e3e9cd":"y_test_df.head()","62668a81":"y_test_df['PassengerId'] = y_test_df.index","3b648b6c":"y_test_df","13887b92":"y_test_df= y_test_df.rename(columns={ 0 : 'Survived_Prob'})","6164c065":"y_test_df =y_test_df.reset_index(drop=True)","f3c0d6de":"y_test_df","6635571d":"y_pred_final = y_test_df.reindex(['PassengerId','Survived_Prob'], axis=1)","e773b012":"y_pred_final","6aa60fef":"y_pred_final['final_predicted'] = y_pred_final.Survived_Prob.map(lambda x: 1 if x > 0.4 else 0)","9d6d1d84":"y_pred_final","f29679ed":"y_pred_final.drop('Survived_Prob',axis=1,inplace=True)","538c1ed9":"y_pred_final.columns = ['PassengerId','Survived']\ny_pred_final","199551f7":"y_pred_final.to_csv('survivor_predictions.csv', index=False)","b05c00f9":"# pd.read_csv('survivor_predictions.csv')","54e06644":"Dropping repeated columns","89b7cdc0":"## Step 4: Building a linear model","ab681553":"### Step 9: Plotting the ROC Curve","813c3ef5":"## Metrics beyond simply accuracy","4e707fe3":"### Step1: Importing data","a0877b71":"### Making predictions on the test set","14a5cf2f":"Optimal cutoff probability is that prob where we get balanced sensitivity and specificity","8fbd3e1d":"#### For categorical variables with multiple levels, create dummy features (one-hot encoded)","a7a948d3":"#### From the curve above, 0.4 is the optimum point to take it as a cutoff probability.","dcc46e15":"Finding Optimal Cutoff Point","eb616c8c":"### Step 3: Data Preparation","76d1a59b":"An ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","aae533e8":"From the distribution shown above, there don't appear to be too many outliers for PClass and Age columns. The data seems to be growing gradually. There, however, do appear to be outliers for Fare and FamilySize.","3786f74b":"### Step 2: Inspecting the Dataframe"}}