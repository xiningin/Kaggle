{"cell_type":{"b8861131":"code","544a0ca2":"code","aa357b25":"code","bd85d8d8":"code","9cfc21e5":"code","d468fdc1":"code","76350b7d":"code","bac97b0d":"code","8a27e04b":"code","ecbee0fc":"code","05fa8fc0":"code","9098e3ed":"code","d9af9507":"code","88f363c4":"markdown","41e1080a":"markdown","e873fab8":"markdown","1a2be83b":"markdown","0c7dde4d":"markdown","661d8632":"markdown","77893f35":"markdown","2b22008c":"markdown","768e7798":"markdown"},"source":{"b8861131":"!nvidia-smi","544a0ca2":"from tensorflow.keras import layers, regularizers\nfrom tensorflow import keras\n\nimport matplotlib.pyplot as plt\nimport tensorflow_addons as tfa\nimport tensorflow as tf\nimport numpy as np","aa357b25":"weight_decay = 0.0001\nbatch_size = 128\nnum_epochs = 150\nTOTAL_STEPS = int((50000 \/ batch_size) * num_epochs)\nWARMUP_STEPS = 10000\nINIT_LR = 0.01\nWAMRUP_LR = 0.002","bd85d8d8":"(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\nval_split = 0.1\n\nval_indices = int(len(x_train) * val_split)\nnew_x_train, new_y_train = x_train[val_indices:], y_train[val_indices:]\nx_val, y_val = x_train[:val_indices], y_train[:val_indices]\n\nprint(f\"Training data samples: {len(new_x_train)}\")\nprint(f\"Validation data samples: {len(x_val)}\")\nprint(f\"Test data samples: {len(x_test)}\")","9cfc21e5":"image_size = 32\nauto = tf.data.AUTOTUNE\n\ndata_augmentation = keras.Sequential(\n    [layers.RandomCrop(image_size, image_size), layers.RandomFlip(\"horizontal\"),],\n    name=\"data_augmentation\",\n)\n\n\ndef make_datasets(images, labels, is_train=False):\n    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n    if is_train:\n        dataset = dataset.shuffle(batch_size * 10)\n    dataset = dataset.batch(batch_size)\n    if is_train:\n        dataset = dataset.map(\n            lambda x, y: (data_augmentation(x), y), num_parallel_calls=auto\n        )\n    return dataset.prefetch(auto)\n\n\ntrain_dataset = make_datasets(new_x_train, new_y_train, is_train=True)\nval_dataset = make_datasets(x_val, y_val)\ntest_dataset = make_datasets(x_test, y_test)","d468fdc1":"def activation_block(x):\n    x = layers.Activation(\"gelu\")(x)\n    return layers.BatchNormalization()(x)\n\n\ndef conv_stem(x, filters: int, patch_size: int):\n    x = layers.Conv2D(filters, kernel_size=patch_size, strides=patch_size, kernel_regularizer=regularizers.l2(1e-2))(x)\n    return activation_block(x)\n\n\ndef conv_mixer_block(x, filters: int, kernel_size: int):\n    # Depthwise convolution.\n    x0 = x\n    x = layers.DepthwiseConv2D(kernel_size=kernel_size, padding=\"same\")(x)\n    x = layers.Add()([activation_block(x), x0])  # Residual.\n\n    # Pointwise convolution.\n    x = layers.Conv2D(filters, kernel_size=1, kernel_regularizer=regularizers.l2(1e-2))(x)\n    x = activation_block(x)\n\n    return x\n\n\ndef get_conv_mixer_256_8(\n    image_size=32, filters=256, depth=8, kernel_size=5, patch_size=2, num_classes=10\n):\n    \"\"\"ConvMixer-256\/8: https:\/\/openreview.net\/pdf?id=TVHS5Y4dNvM.\n    The hyperparameter values are taken from the paper.\n    \"\"\"\n    inputs = keras.Input((image_size, image_size, 3))\n    x = layers.Rescaling(scale=1.0 \/ 255)(inputs)\n\n    # Extract patch embeddings.\n    x = conv_stem(x, filters, patch_size)\n\n    # ConvMixer blocks.\n    for _ in range(depth):\n        x = conv_mixer_block(x, filters, kernel_size)\n\n    # Classification block.\n    x = layers.GlobalAvgPool2D()(x)\n    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n\n    return keras.Model(inputs, outputs)","76350b7d":"class WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(\n        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n    ):\n        super(WarmUpCosine, self).__init__()\n\n        self.learning_rate_base = learning_rate_base\n        self.total_steps = total_steps\n        self.warmup_learning_rate = warmup_learning_rate\n        self.warmup_steps = warmup_steps\n        self.pi = tf.constant(np.pi)\n\n    def __call__(self, step):\n        if self.total_steps < self.warmup_steps:\n            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n        learning_rate = (\n            0.5\n            * self.learning_rate_base\n            * (\n                1\n                + tf.cos(\n                    self.pi\n                    * (tf.cast(step, tf.float32) - self.warmup_steps)\n                    \/ float(self.total_steps - self.warmup_steps)\n                )\n            )\n        )\n\n        if self.warmup_steps > 0:\n            if self.learning_rate_base < self.warmup_learning_rate:\n                raise ValueError(\n                    \"Learning_rate_base must be larger or equal to \"\n                    \"warmup_learning_rate.\"\n                )\n            slope = (\n                self.learning_rate_base - self.warmup_learning_rate\n            ) \/ self.warmup_steps\n            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n            learning_rate = tf.where(\n                step < self.warmup_steps, warmup_rate, learning_rate\n            )\n        return tf.where(\n            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n        )","bac97b0d":"scheduled_lrs = WarmUpCosine(\n    learning_rate_base=INIT_LR,\n    total_steps=TOTAL_STEPS,\n    warmup_learning_rate=WAMRUP_LR,\n    warmup_steps=WARMUP_STEPS,\n)\n\nlrs = [scheduled_lrs(step) for step in range(TOTAL_STEPS)]\nplt.plot(lrs)\nplt.xlabel(\"Step\", fontsize=14)\nplt.ylabel(\"LR\", fontsize=14)\nplt.grid()\nplt.show()","8a27e04b":"def run_experiment(model):\n    optimizer = tfa.optimizers.AdamW(\n        learning_rate=scheduled_lrs, weight_decay=weight_decay\n    )\n\n    model.compile(\n        optimizer=optimizer,\n        loss=\"sparse_categorical_crossentropy\",\n        metrics=[\"accuracy\"],\n    )\n\n    train_callbacks = [\n        keras.callbacks.CSVLogger('.\/train-logs.csv'),\n        keras.callbacks.TensorBoard(histogram_freq=1),\n    ]\n\n    history = model.fit(\n        train_dataset,\n        validation_data=val_dataset,\n        epochs=num_epochs,\n        callbacks=train_callbacks,\n    )\n\n    _, accuracy = model.evaluate(test_dataset)\n    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n\n    return history, model","ecbee0fc":"conv_mixer_model = get_conv_mixer_256_8()\nhistory, conv_mixer_model = run_experiment(conv_mixer_model)","05fa8fc0":"conv_mixer_model.save('convmixer')","9098e3ed":"def plot_accuracy_metric(hist):\n    plt.plot(hist.history[\"accuracy\"])\n    plt.plot(hist.history[\"val_accuracy\"])\n    plt.title(\"Training Progress\")\n    plt.ylabel(\"Accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.legend([\"train_acc\", \"val_acc\"], loc=\"upper left\")\n    plt.grid()\n    plt.show()\n\ndef plot_loss_metric(hist):\n    plt.plot(hist.history[\"loss\"])\n    plt.plot(hist.history[\"val_loss\"])\n    plt.title(\"Training Progress\")\n    plt.ylabel(\"Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.legend([\"train_loss\", \"val_loss\"], loc=\"upper left\")\n    plt.grid()\n    plt.show()","d9af9507":"plot_accuracy_metric(history)\nplot_loss_metric(history)","88f363c4":"## Train the model","41e1080a":"## Warmup cosine","e873fab8":"## Get the TensorFlow data objects","1a2be83b":"## Define the hyperparameters","0c7dde4d":"## Load the CIFAR-10 dataset","661d8632":"## Plot the metrics","77893f35":"## Initial-Setup","2b22008c":"## Imports","768e7798":"## Build the model"}}