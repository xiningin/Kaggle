{"cell_type":{"9fb5c2e7":"code","95ca8901":"code","23fac09d":"code","a999e451":"code","5ba30260":"code","6d0b8c32":"code","3fab788a":"code","a02b37f0":"code","d41bde26":"code","53861631":"code","c15cc359":"code","efb4ab19":"code","e37e651a":"code","fc5037d8":"code","d8e41006":"code","13e83211":"code","13f3f01b":"code","d4a92ecd":"code","dc5fd28c":"code","1d3810b0":"code","1b26412b":"code","c3ead7f0":"code","b8a87672":"code","c5a59764":"code","e9f05212":"code","4ed49b65":"code","c483be4a":"code","8606c274":"code","aac13bb1":"code","fd02dae5":"code","e974bde5":"code","0985fbc1":"code","4de6880b":"code","712e4a5b":"code","5243645c":"code","677d5172":"code","b9f02323":"code","a27e464a":"code","688d1a82":"code","4b2b0b27":"code","e6cf8be0":"code","60347fd5":"code","b7439e10":"code","1cc5acbe":"code","12eace91":"code","564ab2c2":"code","810cd82c":"code","44a956c5":"code","26f54360":"code","97273901":"code","62a24d76":"code","d958c837":"code","4f4b40fd":"code","5ba61837":"code","3efe2971":"code","fe613243":"code","89c9f247":"code","fcf5b27f":"code","33e1e07e":"code","c321dc6d":"code","c8c050e7":"code","11b730f8":"code","198b3b5e":"code","14dce288":"code","456ddc66":"markdown","22f7be4c":"markdown","f14ce4e1":"markdown","52006627":"markdown","14bbd008":"markdown","3ec640f7":"markdown","f0e07c35":"markdown","e44bac06":"markdown","2edffad8":"markdown","90f3cf0e":"markdown","f5692eb3":"markdown","8839027e":"markdown","ca0f2cb5":"markdown","ee9b2bf9":"markdown","5f9b29b7":"markdown","8e144a23":"markdown","373c9dd8":"markdown","683da719":"markdown","cb0e40f0":"markdown","54d7bf85":"markdown","82458c45":"markdown","a7161885":"markdown","3a4a93e6":"markdown","a3a51011":"markdown"},"source":{"9fb5c2e7":"#import library\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.model_selection import GridSearchCV, cross_val_predict","95ca8901":"#import data\ndt_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')#training set\ndt_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')#test set\ndt_all = pd.concat([dt_train, dt_test], sort = True).reset_index(drop = True)#together\ndt_combine = [dt_train, dt_test]\nprint('Training data shape = {}'.format(dt_train.shape))\nprint('Testing data shape = {}'.format(dt_test.shape))\nprint('ALL data shape = {}'.format(dt_all.shape))\nprint('Training data column = {}'.format(dt_train.columns.tolist()))\nprint('Testing data column = {}'.format(dt_test.columns.tolist()))","23fac09d":"#Train data has 891 rows, test data has 418 rows\nprint(dt_train.info())#training dataset infomation\ndt_train.head(5)","a999e451":"print(dt_test.info())#training dataset infomation\ndt_test.sample(5)#5 random sample","5ba30260":"#only three graphs, ctrl + c, ctrl + v is faster\nfig, axs = plt.subplots(ncols=2, nrows=2)\nplt.subplots_adjust(right=1.5, top=1.25)\nplt.subplot(2,2,1)\nsns.countplot(x = 'Pclass', hue = 'Survived', data = dt_train)\nplt.subplot(2,2,2)\nsns.countplot(x = 'Sex', hue = 'Survived', data = dt_train)\nplt.subplot(2,2,3)\nsns.countplot(x = 'Embarked', hue = 'Survived', data = dt_train)\nplt.show()","6d0b8c32":"# explore correlations between existing variables\n# the correlations only includes non-categorical variables\nsns.heatmap(dt_all.corr(), annot = True)\ndt_all.corr()","3fab788a":"#number of missing value(you can try feature engineering for categorical data before missing value)\ndt_all.isnull().sum().sort_values(ascending = False)","a02b37f0":"#set expand to false. If True, return DataFrame with one column per capture group.\ndt_all['Name_Title'] = dt_all.Name.str.extract(' ([A-Za-z]+)\\.', expand = False)","d41bde26":"unique_title = dt_all ['Name_Title'].unique().tolist()\nprint(unique_title)","53861631":"fig, axs = plt.subplots(figsize=(22, 9))\nsns.countplot(x='Name_Title', hue='Survived', data=dt_all)\n\nplt.xlabel('Name_Title', size=15, labelpad=20)\nplt.ylabel('Passenger Count', size=15, labelpad=20)\nplt.tick_params(axis='x', labelsize=10)\nplt.tick_params(axis='y', labelsize=15)\n\nplt.legend(['Not Survived', 'Survived'], loc='upper right', prop={'size': 15})\nplt.title('Count of Survival in {} Feature'.format('Fare'), size=15, y=1.05)\n\nplt.show()","c15cc359":"dt_all['Name_Title'].value_counts(normalize = True) * 100","efb4ab19":"# Bundle rare salutations: 'Other' category\ndel unique_title[0:4]\ndt_all['Name_Title'] = dt_all['Name_Title'].replace(unique_title, 'Other')","e37e651a":"dt_all['Name_Title'].value_counts(normalize = True) * 100","fc5037d8":"new_category = {'Mr':1, 'Miss':2, 'Mrs':2, 'Master':3, 'Other':4}\ndt_all['Name_Title'] = dt_all['Name_Title'].map(new_category)\ndt_all['Name_Title'].unique().tolist()","d8e41006":"dt_all['family'] = dt_all.Name.str.extract('([A-Za-z]+)\\,', expand = False)\ndt_all.head()","13e83211":"print(len(dt_all.family.unique().tolist()))","13f3f01b":"#survive: green not survive: red\nsurvive = dt_train['Survived'] == 1\nsns.distplot(dt_train[survive]['Age'].dropna(), label='Survived', hist=True, color='#e74c3c')\nsns.distplot(dt_train[~survive]['Age'].dropna(), label='Survived', hist=True, color='#2ecc71')","d4a92ecd":"dt_all['Age'] = pd.qcut(dt_all['Age'], 5)\nsns.countplot(x='Age', hue='Survived', data=dt_all)","dc5fd28c":"dt_all['Age'].head(5)","1d3810b0":"dt_all['Family_Size'] = dt_all['SibSp'] + dt_all['Parch'] + 1\nsns.countplot(x = 'Family_Size', hue = 'Survived', data = dt_all)","1b26412b":"#by the plot above, grouping 1 as group1, 2,3,4 as group2, 5,6,7 as group3, 8,11 as group4 (since they looks similar)\nfamily_grouping = {1: 'group1', 2: 'group2', 3: 'group2', 4: 'group2', 5: 'group3', 6: 'group3', 7: 'group3', 8: 'group4', 11: 'group4'}\ndt_all['Family_Size_Grouped'] = dt_all['Family_Size'].map(family_grouping)\nsns.countplot(x = 'Family_Size_Grouped', hue = 'Survived', data = dt_all)","c3ead7f0":"#survive: green not survive: red\nplt.figure(figsize=(20,5))\nsurvive = dt_train['Survived'] == 1\nsns.distplot(dt_train[survive]['Fare'].dropna(), label='Survived', hist=True, color='#e74c3c')\nsns.distplot(dt_train[~survive]['Fare'].dropna(), label='Survived', hist=True, color='#2ecc71')\nplt.show()","b8a87672":"#fill missing value before qcut or ccut. look at the data\ndt_all[dt_all['Fare'].isnull()]","c5a59764":"dt_all.corr()","e9f05212":"print(dt_all.groupby(['Pclass','Family_Size','Name_Title']).Fare.median())","4ed49b65":"med_fare = dt_all.groupby(['Pclass', 'Family_Size', 'Name_Title']).Fare.median()[3][1][1]\n#dt_all['Fare'] = dt_all['Fare'].fillna(med_fare)","c483be4a":"plt.figure(figsize=(20,5))\ndt_all['Fare'] = pd.qcut(dt_all['Fare'], 12)\nsns.countplot(x='Fare', hue='Survived', data=dt_all)","8606c274":"# use M represent missing values\ndt_all['Cabin'] = dt_all['Cabin'].fillna('M')\ndt_all['Cabin'].unique().tolist()","aac13bb1":"#there might be one preson order one or more tickets. The first letter is unique. \n# Extract first letter\nimport re\n#pandas.Series.map\ndt_all['Cabin'] = dt_all['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group()) \ndt_all['Cabin'].unique().tolist()","fd02dae5":"type(dt_all['Cabin'])","e974bde5":"#since it is a series type\ndt_all['Cabin'].value_counts()","0985fbc1":"sort_list = ['M', 'C', 'E', 'G', 'D', 'A', 'B', 'F', 'T']\nsort_list = sorted(sort_list)\nprint(sort_list)","4de6880b":"#the first letter of tickets might relate to passenger class, according to daily experience.\n\ndt_all_decks = dt_all.groupby(['Cabin','Pclass']).count()['Survived']\ntotal_num_dict = {}\nfor letter in sort_list:\n    total_num = 0\n    for num in range(1,4):\n        try:\n            num_letter = dt_all_decks[letter][num]\n            total_num += num_letter\n            #print(total_num)\n            if num == 3:\n                total_num_dict[letter] = total_num\n                total_num = 0\n        except KeyError:\n            if num == 3:\n                total_num_dict[letter] = total_num\n                total_num = 0\nprint(total_num_dict)                \nprint(dt_all_decks['A'][1])\nprint(dt_all_decks)\ntype(dt_all_decks)","712e4a5b":"precentage_num_dict = {}\nfor letter in sort_list:\n    lis = []\n    total = total_num_dict[letter]\n    for num in range(1,4):\n        try:\n            lis.append(dt_all_decks[letter][num]\/total)\n        except KeyError:\n            lis.append(0)\n        if num == 3:\n            precentage_num_dict[letter] = lis\nprint(precentage_num_dict)","5243645c":"#precentage chart\npd.DataFrame.from_dict(precentage_num_dict)","677d5172":"dt_all['Cabin'] = dt_all['Cabin'].replace(['A', 'B', 'C', 'T'], 'A&B&C&T')\ndt_all['Cabin'] = dt_all['Cabin'].replace(['D', 'E'], 'D&E')\ndt_all['Cabin'] = dt_all['Cabin'].replace(['F', 'G'], 'F&G')\ndt_all['Cabin'].value_counts()","b9f02323":"dt_all.head(5)","a27e464a":"dt_all.drop(['Name', 'Family_Size'],axis=1,inplace=True)\ndt_all.head(5)","688d1a82":"dt_all['Ticket_Frequency'] = dt_all.groupby('Ticket')['Ticket'].transform('count')","4b2b0b27":"dt_all.head(5)","e6cf8be0":"#number of missing value\ndt_all.isnull().sum().sort_values(ascending = False)","60347fd5":"#label encoding, ignore NAN value\n#https:\/\/stackoverflow.com\/questions\/54444260\/labelencoder-that-keeps-missing-values-as-nan\n#https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.apply.html\ndt_all = dt_all.apply(lambda series: pd.Series(\n    LabelEncoder().fit_transform(series[series.notnull()]),\n    index=series[series.notnull()].index))\n#number of missing value\ndt_all.isnull().sum().sort_values(ascending = False)","b7439e10":"dt_all.head(5)","1cc5acbe":"dt_all[dt_all['Fare'].isnull()]","12eace91":"dt_all[dt_all['Embarked'].isnull()]","564ab2c2":"sns.heatmap(dt_all.corr(), annot = True)\ndt_all.corr()","810cd82c":"sort_age = dt_all.corr()['Age'].abs().sort_values(ascending = False)[1:]\n#sort_age_fit = sort_age[sort_age > 0.1][1:]#drop the sane one\nsort_age","44a956c5":"sort_Embarked = dt_all.corr()['Embarked'].abs().sort_values(ascending = False)[1:]\n#sort_Embarked_fit = sort_Embarked[sort_Embarked > 0.1][1:]\nsort_Embarked","26f54360":"sort_Fare = dt_all.corr()['Fare'].abs().sort_values(ascending = False)[1:]\n#sort_Fare_fit = sort_Fare[sort_Fare > 0.1][1:]\nsort_Fare","97273901":"sort_age_name = sort_age.index.tolist()\nsort_Embarked_name = sort_Embarked.index.tolist()\nsort_Fare_name = sort_Fare.index.tolist()\nsort_lis = [sort_age_name,sort_Embarked_name, sort_Fare_name ]\nfor i in range(0,3):\n    try:\n        sort_lis[i].remove('Survived')\n        sort_lis[i].remove('PassengerId')\n    except:\n        sort_lis[i] = sort_lis[i]\nprint('Age: ',sort_age_name)\nprint('Embarked: ',sort_Embarked_name)\nprint('Fare: ',sort_Fare_name)","62a24d76":"#sort_Embarked_name.remove('Age')\ndt_all['Fare'] = dt_all.groupby(sort_Fare_name[0])['Fare'].apply(lambda x: x.fillna(x.median()))\ndt_all['Embarked'] = dt_all.groupby(sort_Embarked_name[0:2])['Embarked'].apply(lambda x: x.fillna(x.median()))\ndt_all['Age'] = dt_all.groupby(['Sex',sort_age_name[0]])['Age'].apply(lambda x: x.fillna(x.median()))\ndt_all.isnull().sum().sort_values(ascending = False)","d958c837":"fare_null = dt_all.loc[dt_all['PassengerId'] == 1043]\nembark_null_1 = dt_all.loc[dt_all['PassengerId'] == 61]\nembark_null_2 = dt_all.loc[dt_all['PassengerId'] == 829]\nprint(fare_null)\nprint(embark_null_1)\nprint(embark_null_2)","4f4b40fd":"dt_all.columns","5ba61837":"sort_Survived = dt_all.corr()['Survived'].abs().sort_values(ascending = False)[1:]\n#sort_Fare_fit = sort_Fare[sort_Fare > 0.1][1:]\nsort_Fare","3efe2971":"dt_all.drop(['SibSp','Parch','family'], axis = 1, inplace = True)\ndt_all.columns","fe613243":"OHEncoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nsurvive_all = dt_all[['Survived','PassengerId']]\ndt_all_copy = dt_all.copy()\ndt_all_copy = dt_all_copy.drop(['Survived','PassengerId'], axis = 1)\ndt_all_copy_column = dt_all_copy.columns.tolist()","89c9f247":"low_cardinality_cols = [col for col in dt_all_copy_column if dt_all_copy[col].nunique() < 5]\nhigh_cardinality_cols = [col for col in dt_all_copy_column if dt_all_copy[col].nunique() >= 5]\nprint(low_cardinality_cols,high_cardinality_cols)","fcf5b27f":"OH_cols_all = pd.DataFrame(OHEncoder.fit_transform(dt_all_copy[low_cardinality_cols]))\ndt_all_onehog = pd.concat([OH_cols_all,survive_all,dt_all_copy[high_cardinality_cols]], axis = 1)\nprint(dt_all_onehog.shape)","33e1e07e":"dt_all_onehog.head()","c321dc6d":"dt_train_onehog = dt_all_onehog.loc[:890]\ndt_test_onehog = dt_all_onehog.loc[891:]\ndt_test_onehog.drop(['Survived'], axis = 1, inplace = True)\ndt_train_onehog.head()","c8c050e7":"train_X_onehog = StandardScaler().fit_transform(dt_train_onehog.drop(['Survived', 'PassengerId'],axis = 1))\ntrain_y_onehog = dt_train_onehog['Survived']\ntest_X_onehog = StandardScaler().fit_transform(dt_test_onehog.drop(['PassengerId'], axis = 1))\nprint(type(train_X_onehog))\nprint('train_X shape: {}'.format(train_X_onehog.shape))\nprint('train_y shape: {}'.format(train_y_onehog.shape))\nprint('test_X shape: {}'.format(test_X_onehog.shape))","11b730f8":"forest_classifier = RandomForestClassifier(random_state=42)\n'''\n#this runs for a long time\nforest_parameter = [\n    {'max_depth':[2,4,7,8,12,16,20],\n    'min_samples_split':[2,4,6,8,10],\n    'min_samples_leaf':[2,4,6,8,10],\n    'n_estimators':range(50,2000,50)}\n]\n'''\nforest_parameter = [\n    {'max_depth':[6],\n    'min_samples_split':[6],\n    'min_samples_leaf':[6],\n    'n_estimators':range(50,2000,50)}]\nramdom_forest_model = GridSearchCV(forest_classifier, forest_parameter, cv=5, verbose=1, n_jobs=-1)\nramdom_forest_model.fit(train_X_onehog, train_y_onehog)\nforest_best = ramdom_forest_model.best_estimator_\nforest_best.fit(train_X_onehog, train_y_onehog)\nforest_best_prediction = cross_val_predict(forest_best, train_X_onehog, train_y_onehog, cv=10)\nforest_best_accuracy_score = accuracy_score(train_y_onehog, forest_best_prediction)\nprint(forest_best_accuracy_score)","198b3b5e":"forest_test_prediction = forest_best.predict(test_X_onehog)\nforest_test_prediction = forest_test_prediction.astype(int)\nrandom_forest_new_submission = pd.DataFrame(columns=['PassengerId', 'Survived'])\nrandom_forest_new_submission['PassengerId'] = dt_test_onehog['PassengerId'].add(1).to_numpy()\nrandom_forest_new_submission['Survived'] = forest_test_prediction\nrandom_forest_new_submission.to_csv('forest_submission_20200602.csv', header=True, index=False)","14dce288":"random_forest_new_submission.tail(5)","456ddc66":"#### You can make some graphs to show the relation between variables and people's death, for varaiable Pclass, Sex, Embarked, since they are well grouped \nFor example, From the graph above, precentage of male dead much more than female.","22f7be4c":"3.2 convert values\n\n3.21 convert categorical value to numerical","f14ce4e1":"Now finishs feature engineering and null values part. \n<a id=\"4\"><\/a>\n\n4. model and prediction\n\nJust use a easy one, later I will improve it.\n\ntry the process for split data, make prediction model, sample score, and submission","52006627":"Now do the feature engineering of Name, Age, Sibsp&Parch, Fare, Cabin. Fare, Cabin and Age have missing values, so we need to ignore them now. After feature engineering, then fill the values according with correlations, or other methods.","14bbd008":"# Titanic: Machine Learning from Disaster\n### Jupyter notebook author: Tao Shan\n1. [prepare data](#1) \n2. [explore data](#2)\n3. [null value and data engineering](#3)\n4. [predicting model and submit solution](#4)\n\n>Hello. This is my first notebook. I'm a beginner to the competition.","3ec640f7":"3.3 OnehogEncoder\n\nI will use it in 4.3, 4.1 and 4.2 are using labelEncoder","f0e07c35":"<a id=\"2\"><\/a>\n2. [explore data]()","e44bac06":"For other graph other than Pclass, Sex, Embarked, I will draw in the next part, together with data engineering\nThen, we need to explore null value, then group and do some feature engineering works with Name, Age, Sibsp&Parch, Fare, Cabin","2edffad8":"Score: 0.79904","90f3cf0e":"3.1.4. fare\n\n(similar as age)","f5692eb3":"3.1.6 train tickets","8839027e":"3.1.5 Cabin","ca0f2cb5":"<a id=\"3\"><\/a>\n3. [null value and data engineering]()","ee9b2bf9":"So Mr and Mrs has similar survival rate","5f9b29b7":"### We need to know what does these columns mean\n#### (according to the sequence)\n- PassengerId and Survived are ignored, Since PassengerId is useless for prediction, Survived is our target.\n- Pclass: 1 2 3\n- Name: includes name title Mr, Mrs, Miss, Master, ...\n- Sex: male or female\n- Age: numerical value between 0-100\n- Sibsp & Parch: number of sibling and parents, numerical value (probably integer between 0-10)\n- Ticket: Random number and letters\n- Fare: Price for tickets, float value\n- Cabin: Start with a letter, probably determine which group of cabin, then number for the seat\n- Embarked: Where user boarded, such as 'S', 'C'","8e144a23":"3.1.1.2 group by family","373c9dd8":"3.22 fill values according with correlation","683da719":"3.1.2.age","cb0e40f0":"Learning materials: \nhttps:\/\/www.kaggle.com\/learn\/intro-to-machine-learning\n\nhttps:\/\/www.kaggle.com\/learn\/intermediate-machine-learning\n\nhttps:\/\/www.kaggle.com\/learn\/data-visualization\n\nhttps:\/\/www.kaggle.com\/learn\/feature-engineering\n\nhttps:\/\/www.kaggle.com\/kernelgenerator\/titanic-tutorial-for-beginners-part-1\n\nhttps:\/\/www.kaggle.com\/kernelgenerator\/titanic-tutorial-for-beginners-part-2\n\nhttps:\/\/www.kaggle.com\/kernelgenerator\/titanic-tutorial-for-beginners-part-3\n\nhttps:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial#2.-Feature-Engineering![image.png](attachment:image.png)","54d7bf85":"if you search from internet, embark should be 'S'(here is 2), just have a check","82458c45":"3.1 data engineering\n\n3.1.1. Name","a7161885":"So A,B,C,T are all in first Passenger class.\n\nD, E looks similar\n\nF,G looks similar\n\nM looks different from others\n\nSo that is why I grouping like this:","3a4a93e6":"<a id=\"1\"><\/a>\n1. [prepare data]() ","a3a51011":"3.1.3 sibsp & parch\n\nGroup sibsp and parch as a family."}}