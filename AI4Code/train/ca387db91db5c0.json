{"cell_type":{"c9c4bfbc":"code","3502f613":"code","78cac251":"code","2d5fcab3":"code","04b724a8":"code","5dbace69":"code","c8d2cf6d":"code","9a252ba9":"code","2514a380":"code","64a6382a":"code","9f886db5":"code","9ae7e412":"code","b01620ec":"code","a868bcac":"code","31ee1cc7":"code","666fde13":"code","bcc63009":"code","ab996611":"code","a2127792":"code","d175fff7":"code","acc1ce8e":"code","2a10eb61":"code","0d524ac9":"code","104eca1b":"code","88add0e1":"code","200dd11f":"code","89dfe0a3":"code","9abe1cd1":"code","40d8128c":"code","f58d86f3":"code","27ff9763":"code","da353776":"code","a5832a28":"code","0a56712e":"code","d0b7f754":"code","3078fcf9":"code","64840ad3":"code","8716803a":"code","59b94aa4":"code","14ff1e13":"code","9adf4d00":"markdown","5eaafed7":"markdown","51cb9e5e":"markdown","4682f14a":"markdown","abcdb03e":"markdown","d9c5361e":"markdown","71896c07":"markdown","8d94b039":"markdown","e8de7e2d":"markdown","b06a3432":"markdown","f76f243c":"markdown","a852a1dd":"markdown","118ebb3d":"markdown","6af5583a":"markdown","45a79516":"markdown"},"source":{"c9c4bfbc":"# Importing libraries\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight') \n# Above is a special style template for matplotlib, highly useful for visualizing time series data\n%matplotlib inline\nfrom pylab import rcParams\nfrom plotly import tools\n\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom numpy.random import normal, seed\nfrom scipy.stats import norm\nfrom statsmodels.tsa.arima_model import ARMA\n\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima_process import ArmaProcess\nfrom statsmodels.tsa.arima_model import ARIMA\nimport math\nfrom sklearn.metrics import mean_squared_error\nprint(os.listdir(\"..\/input\"))\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport gc\n\nimport seaborn as sns\nplt.style.use('fivethirtyeight') \n#import os\n#print(os.listdir(\"..\/input\"))\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","3502f613":"#Pytz brings the Olson tz database into Python and thus supports almost all time zones. \n#This module serves the date-time conversion functionalities and helps user serving international client\u2019s base. \n#It enables time-zone calculations in our Python applications and also allows us to create timezone aware datetime instances. \n#import datetime, pytz\n#We define a function to change timestamp data in csv file to datetime format.\n#def dateparse (time_in_secs):    \n    #return pytz.utc.localize(datetime.datetime.fromtimestamp(float(time_in_secs)))\n#data = pd.read_csv('..\/input\/bitcoin-historical-data\/bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv', parse_dates=[0], date_parser=dateparse, index_col='Timestamp')\n\n#Read data from CSV to dataframe and use dateparse function as date_parser\n#data = pd.read_csv('..\/input\/bitcoin-historical-data\/bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv', parse_dates=[0], date_parser=dateparse)\n\ndata = pd.read_csv('..\/input\/bitcoin-historical-data\/bitstampUSD_1-min_data_2012-01-01_to_2021-03-31.csv')","78cac251":"#Let's look at the first five rows of our data.\ndata.head()","2d5fcab3":"data.Timestamp = pd.to_datetime(data.Timestamp, unit='s')\ndata.head()","04b724a8":"# Assing Timestamp as a index\ndata.index = data.Timestamp\ndata.head()","5dbace69":"#Our data contains 8 column and  one of these datetime and the others float.\ndata.info()","c8d2cf6d":"#All columns except Timestamp contain null values.\ndata.isna().sum()","9a252ba9":"#Let's fill null values in 'Volume_(BTC)','Volume_(Currency)','Weighted_Price' with these clumns mean.\n\ndata['Volume_(BTC)'].fillna(data['Volume_(BTC)'].mean(),inplace=True)\n#data['Volume_(Currency)'].fillna(value=0, inplace=True)\ndata['Volume_(Currency)'].fillna(data['Volume_(Currency)'].mean(),inplace=True)\n#data['Weighted_Price'].fillna(value=0, inplace=True)\ndata['Weighted_Price'].fillna(data['Weighted_Price'].mean(),inplace=True)\n\n#data['Volume_(Currency)'].fillna(data['Volume_(Currency)'].interpolate(),inplace=True)\n#data['Volume_(BTC)']=data['Volume_(BTC)'].fillna(data['Volume_(BTC)'].interpolate(), inplace=True)\n#data['Volume_(BTC)'].fillna(value=0, inplace=True)\n#data['Weighted_Price'].fillna(data['Weighted_Price'].interpolate(), inplace=True)   \n\n#And fill null values in 'High','Low','Close' with forward fill method.\ndata['High'].fillna(method='ffill', inplace=True)\ndata['Low'].fillna(method='ffill', inplace=True)\ndata['Close'].fillna(method='ffill', inplace=True)\ndata['Open'].fillna(method='ffill', inplace=True)\n\ndata.head()","2514a380":"data.isna().sum()","64a6382a":"# Resampling to daily frequency\ndata = data.resample('D').mean()\n\n# Resampling to monthly frequency\ndata_month = data.resample('M').mean()\n\n# Resampling to annual frequency\ndata_year = data.resample('A-DEC').mean()\n\n# Resampling to quarterly frequency\ndata_Q = data.resample('Q-DEC').mean()","9f886db5":"data.head()","9ae7e412":"#Statistic of numerical values\ndata.describe()","b01620ec":"# PLOTS\n#Daily, monthly, quarterly, annual Volume Weighted Average Price changes\n#Daily, monthly, quarterly, annual Volume Weighted Average Price changesDaily, monthly, quarterly, annual Volume Weighted Average Price changes\nfig = plt.figure(figsize=[15, 7])\nplt.suptitle('Bitcoin exchanges, mean USD', fontsize=22)\n\nplt.subplot(221)\nplt.plot(data.Weighted_Price, '-', label='By Days')\nplt.legend()\n\nplt.subplot(222)\nplt.plot(data_month.Weighted_Price, '-', label='By Months')\nplt.legend()\n\nplt.subplot(223)\nplt.plot(data_Q.Weighted_Price, '-', label='By Quarters')\nplt.legend()\n\nplt.subplot(224)\nplt.plot(data_year.Weighted_Price, '-', label='By Years')\nplt.legend()\n\n#plt.tight_layout()\nplt.show()","a868bcac":"data[\"Volume_(Currency)\"].asfreq('M').plot() # asfreq method is used to convert a time series to a specified frequency. Here it is monthly frequency.\nplt.title('Volume_(Curr)')\nplt.show()","31ee1cc7":"data['2012':'2021'].plot(subplots=True, figsize=(10,12))\nplt.title('Bitcoin attributes from 2012 to 2021')\nplt.show()","666fde13":"data[\"Volume_(BTC)\"].asfreq('M').plot(legend=True)\nshifted = data[\"Volume_(BTC)\"].asfreq('M').shift(10).plot(legend=True)\nshifted.legend(['Volume_(BTC)','Volume_(Currency)'])\nplt.show()","bcc63009":"# OHLC chart of June 2014\ntrace = go.Ohlc(x=data['06-2014'].index,\n                open=data['06-2014'].Open,\n                high=data['06-2014'].High,\n                low=data['06-2014'].Low,\n                close=data['06-2014'].Close)\ndata1 = [trace]\niplot(data1, filename='simple_ohlc')","ab996611":"# OHLC chart of June 2021\ntrace = go.Ohlc(x=data['2021'].index,\n                open=data['2021'].Open,\n                high=data['2021'].High,\n                low=data['2021'].Low,\n                close=data['2021'].Close)\ndata2 = [trace]\niplot(data2, filename='simple_ohlc')","a2127792":"trace = go.Ohlc(x=data.index,\n                open=data.Open,\n                high=data.High,\n                low=data.Low,\n                close=data.Close)\ndata3 = [trace]\niplot(data3, filename='simple_ohlc')","d175fff7":"# Candlestick chart of march 2013\ntrace = go.Candlestick(x=data['03-2013'].index,\n                open=data['03-2013'].Open,\n                high=data['03-2013'].High,\n                low=data['03-2013'].Low,\n                close=data['03-2013'].Close)\ndata4 = [trace]\niplot(data4, filename='simple_candlestick')","acc1ce8e":"plt.figure(figsize=(10,4))\nplt.plot(data[\"Weighted_Price\"])\nplt.title(\"Weighted_Price) with Months\")\nplt.xlabel(\"Date\")\nplt.ylabel(\"Weighted_Price\")\nplt.show()","2a10eb61":"#We import statmodels library\n#statsmodels is a Python module that provides classes and functions for the estimation of many different statistical models,\n#as well as for conducting statistical tests, and statistical data exploration. \n#An extensive list of result statistics are available for each estimator.\nimport statsmodels.api as sm\nplt.figure(figsize=[15,7])\n#Creates the series, performs the decomposition, and plots the 4 resulting series.\n#We can see that the entire series was taken as the trend component and that there is a seasonality.\n#We can see that the trend and seasonality information extracted from the series does seem reasonable. \nsm.tsa.seasonal_decompose(data_month.Weighted_Price).plot()\n\n#In statistics, the Dickey\u2013Fuller test tests the null hypothesis that a unit root is present\n#in an autoregressive time series model.\nplt.show()","0d524ac9":"#data_month['Weighted_Price'].fillna(data_month['Weighted_Price'].mean(), inplace=True)   \nfrom statsmodels.tsa.stattools import adfuller\nadfuller_test = adfuller(data_month[\"Weighted_Price\"])\n\nprint('Test statistic: ' , adfuller_test[0])\nprint('p-value: '  ,adfuller_test[1])\nprint('Critical Values:' ,adfuller_test[4])","104eca1b":"if adfuller_test[1] > 0.05:\n    print('Series is not Stationary')\nelse:\n    print('Series is Stationary')","88add0e1":"from statsmodels.tsa.stattools import kpss\nkpss_test = kpss(data_month[\"Weighted_Price\"])\n\nprint('KPSS Statistic: %f' % kpss_test[0])\nprint('Critical Values @ 0.05: %.2f' % kpss_test[3]['5%'])\nprint('p-value: %f' % kpss_test[1])","200dd11f":"if kpss_test[1] < 0.05:\n    print('Series is not Stationary')\nelse:\n    print('Series is Stationary')","89dfe0a3":"data_month['Weighted_Price_diff'] = data_month['Weighted_Price'] - data_month['Weighted_Price'].shift(1)\ndata_month['Weighted_Price_diff'].dropna().plot()\ndata_month['Weighted_Price_diff'].fillna(data_month['Weighted_Price_diff'].mean(),inplace=True)","9abe1cd1":"data_month['Weighted_Price_log'] = np.log(data_month['Weighted_Price'])\ndata_month['Weighted_Price_log_diff'] = data_month['Weighted_Price_log'] - data_month['Weighted_Price_log'].shift(1)\ndata_month['Weighted_Price_log_diff'].dropna().plot()","40d8128c":"# adfuller library \n# check_mean_std\ndef check_mean_std(ts):\n    #Rolling statistics\n    rolmean =  data.rolling(7).mean()\n    rolstd =data.rolling(7).std()\n    plt.figure(figsize=(22,10))   \n    orig = plt.plot(data[\"Volume_(BTC)\"], color='red',label='Original')\n    mean = plt.plot(rolmean[\"Volume_(BTC)\"], color='black', label='Rolling Mean')\n    std = plt.plot(rolstd[\"Volume_(BTC)\"], color='green', label = 'Rolling Std')\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Volume_(BTC)\")\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.legend()\n    plt.show()\n    \n# check stationary: mean, variance(std)and adfuller test\ncheck_mean_std(data)\n\n# Time series is stationary.","f58d86f3":"from scipy import stats\n# Box-Cox Transformations\ndata_month['Weighted_Price_box'], lmbda = stats.boxcox(data_month.Weighted_Price)\nprint(\"Dickey\u2013Fuller test: p=%f\" % sm.tsa.stattools.adfuller(data_month.Weighted_Price)[1])","27ff9763":"if sm.tsa.stattools.adfuller(data_month.Weighted_Price)[1] > 0.05:\n    print('Series is not Stationary')\nelse:\n    print('Series is Stationary')","da353776":"# Seasonal differentiation\ndata_month['prices_box_diff'] = data_month.Weighted_Price_box - data_month.Weighted_Price_box.shift(12)\nprint(\"Dickey\u2013Fuller test: p=%f\" % sm.tsa.stattools.adfuller(data_month.prices_box_diff[12:])[1])","a5832a28":"if sm.tsa.stattools.adfuller(data_month.prices_box_diff[12:])[1] > 0.05:\n    print('Series is not Stationary')\nelse:\n    print('Series is Stationary')","0a56712e":"# Regular differentiation\ndata_month['prices_box_diff2'] = data_month.prices_box_diff - data_month.prices_box_diff.shift(1)\nplt.figure(figsize=(15,7))\n\n# STL-decomposition\nsm.tsa.seasonal_decompose(data_month.prices_box_diff2[13:]).plot()   \nprint(\"Dickey\u2013Fuller test: p=%f\" % sm.tsa.stattools.adfuller(data_month.prices_box_diff2[13:])[1])\n\nplt.show()","d0b7f754":"if sm.tsa.stattools.adfuller(data_month.prices_box_diff2[13:])[1] > 0.05:\n    print('Series is not Stationary')\nelse:\n    print('Series is Stationary')","3078fcf9":"from itertools import product\n# Initial approximation of parameters\nQs = range(0, 2)\nqs = range(0, 3)\nPs = range(0, 3)\nps = range(0, 3)\nD=1\nd=1\nparameters = product(ps, qs, Ps, Qs)\nparameters_list = list(parameters)\nlen(parameters_list)\n\n# Model Selection\nresults = []\nbest_aic = float(\"inf\")\nwarnings.filterwarnings('ignore')\nfor param in parameters_list:\n    try:\n        model=sm.tsa.statespace.SARIMAX(data_month.Weighted_Price_box, order=(param[0], d, param[1]), \n                                        seasonal_order=(param[2], D, param[3], 12)).fit(disp=-1)\n    except ValueError:\n        print('wrong parameters:', param)\n        continue\n    aic = model.aic\n    if aic < best_aic:\n        best_model = model\n        best_aic = aic\n        best_param = param\n    results.append([param, model.aic])","64840ad3":"# Best Models\nresult_table = pd.DataFrame(results)\nresult_table.columns = ['parameters', 'aic']\nprint(result_table.sort_values(by = 'aic', ascending=True).head())\nprint(best_model.summary())","8716803a":"# Inverse Box-Cox Transformation Function\ndef invboxcox(y,lmbda):\n   if lmbda == 0:\n      return(np.exp(y))\n   else:\n      return(np.exp(np.log(lmbda*y+1)\/lmbda))","59b94aa4":"# Prediction\nfrom datetime import datetime\ndata_month2 = data_month[['Weighted_Price']]\ndate_list = [datetime(2017, 6, 30), datetime(2017, 7, 31), datetime(2017, 8, 31), datetime(2017, 9, 30), \n             datetime(2017, 10, 31), datetime(2017, 11, 30), datetime(2017, 12, 31), datetime(2018, 1, 31),\n             datetime(2018, 1, 28)]\nfuture = pd.DataFrame(index=date_list, columns= data_month.columns)\ndata_month2 = pd.concat([data_month2, future])\ndata_month2['forecast'] = invboxcox(best_model.predict(start=0, end=75), lmbda)\nplt.figure(figsize=(15,7))\ndata_month2.Weighted_Price.plot()\ndata_month2.forecast.plot(color='r', ls='--', label='Predicted Weighted_Price')\nplt.legend()\nplt.title('Bitcoin exchanges, by months')\nplt.ylabel('mean USD')\nplt.show()","14ff1e13":"data_train=data_month.iloc[:-30]\ndata_test=data_month.iloc[-30:]\nprint(data_train.shape,data_test.shape)","9adf4d00":"# Stationarity of a Time Series #\n\n\u2018Stationarity\u2019 is one of the most important concepts you will come across when working with time series data. A stationary series is one in which the properties \u2013 mean, variance and covariance, do not vary with time.","5eaafed7":"A number of parametric and nonparametric tests are available to check for the stationarity of the series. Let's have a look at a couple of parametric tests and their Python implementation.\n\n## Augmented Dickey-Fuller Test ##\nThe Augmented Dickey-Fuller test is one of the most popular tests to check for stationarity. It tests the below hypothesis.\nNull Hypothesis, \n\n* H0: The time series is not stationary.\n\nAlternative Hypothesis,\n\n* H1: The time series is stationary.\nYou can use the adfuller method from the statsmodels library to perform this test in Python and compare the value of the test statistics or the p-value.\n\nIf the p-value is less than or equal to 0.05 or the absolute value of the test statistics is greater than the critical value, you reject H0 and conclude that the time series is stationary.\nIf the p-value is greater than 0.05 or the absolute value of the test statistics is less than the critical value, you fail to reject H0 and conclude that the time series is not stationary.","51cb9e5e":"\nExplanation of each columns\n\n* Timestamp: Start time of time window (60s window), in Unix time\n* Open: Open price at start time window\n* High: High price within time window\n* Low: Low price within time window\n* Close: Close price at end of time window\n* Volume_(BTC): Volume of BTC transacted in this window\n* Volume_(Currency): Volume of corresponding currency transacted in this window\n* Weighted_Price: VWAP- Volume Weighted Average Price","4682f14a":"## Rolling Mean ##\n\nTime series data can be noisy due to high fluctuations in the market. As a result, it becomes difficult to gauge a trend or pattern in the data.\nRolling mean (rolling average or running average) is a calculation to analyze data points by creating a series of averages of different subsets of the full data set. \nIt works by simply splitting and aggregating the data into windows according to function, such as mean(), median(), count(), etc. For this example, we\u2019ll use a rolling mean for 3, 7 and 30 days.","abcdb03e":"## Types of Stationarity ##\n\n* Strict Stationary: A strict stationary series satisfies the mathematical definition of a stationary process. For a strict stationary series, the mean, variance and covariance are not the function of time. The aim is to convert a non-stationary series into a strict stationary series for making predictions.\n\n* Trend Stationary: A series that has no unit root but exhibits a trend is referred to as a trend stationary series. Once the trend is removed, the resulting series will be strict stationary. The KPSS test classifies a series as stationary on the absence of unit root. This means that the series can be strict stationary or trend stationary.\n\n* Difference Stationary: A time series that can be made strict stationary by differencing falls under difference stationary. ADF test is also known as a difference stationarity test.\nIt\u2019s always better to apply both the tests, so that we are sure that the series is truly stationary. Let us look at the possible outcomes of applying these stationary tests.\n\n- Case 1: Both tests conclude that the series is not stationary -> series is not stationary\n\n- Case 2: Both tests conclude that the series is stationary -> series is stationary\n\n- Case 3: KPSS = stationary and ADF = not stationary  -> trend stationary, remove the trend to make series strict stationary\n\n- Case 4: KPSS = not stationary and ADF = stationary -> difference stationary, use differencing to make series stationary","d9c5361e":"The value of the test statistic is greater than the critical value at all confidence intervals, and hence we can say that the series is not stationary.","71896c07":"We can plot the data and determine if the properties of the series are changing with time or not.","8d94b039":"## Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test ##\n\nKPSS is another test for checking the stationarity of a time series (slightly less popular than the Dickey Fuller test). The null and alternate hypothesis for the KPSS test are opposite that of the ADF test, which often creates confusion. The Kwiatkowski\u2013Phillips\u2013Schmidt\u2013Shin (KPSS) test figures out if a time series is stationary around a mean or linear trend, or is non-stationary due to a unit root. A stationary time series is one where statistical properties \u2014 like the mean and variance \u2014 are constant over time.\n\n* The null hypothesis for the test is that the data is stationary.\n* The alternate hypothesis for the test is that the data is not stationary.","e8de7e2d":"In order to use time series forecasting models, it is necessary to convert any non-stationary series to a stationary series first.","b06a3432":"Since the p value is larger than the significance level of 0.05, we cannot reject the null hypothesis that the time series data is non-stationary. Thus, the time series data is non-stationary.","f76f243c":"## Making a Time Series Stationary ##","a852a1dd":"Although its clear that we have a no trend in the above series, this visual approach might not always give accurate results. It is better to confirm the observations using some statistical tests.","118ebb3d":" ## PREDICTION OF BITCOIN PRICES WITH TIME SERIES FORECASTING METHODS ## ","6af5583a":"The first step to build an ARIMA model is to make the time series stationary.\n\n\nBecause, term \u2018Auto Regressive\u2019 in ARIMA means it is a linear regression model that uses its own lags as predictors. Linear regression models, as you know, work best when the predictors are not correlated and are independent of each other.\n\nSo how to make a series stationary?\n\nThe most common approach is to difference it. That is, subtract the previous value from the current value. Sometimes, depending on the complexity of the series, more than one differencing may be needed.\n\nThe value of d, therefore, is the minimum number of differencing needed to make the series stationary. And if the time series is already stationary, then d = 0.\n\n\u2018p\u2019 is the order of the \u2018Auto Regressive\u2019 (AR) term. It refers to the number of lags of Y to be used as predictors. And \u2018q\u2019 is the order of the \u2018Moving Average\u2019 (MA) term. It refers to the number of lagged forecast errors that should go into the ARIMA Model.","45a79516":"## ARIMA ##\n\nARIMA, short for \u2018Auto Regressive Integrated Moving Average\u2019 is actually a class of models that \u2018explains\u2019 a given time series based on its own past values, that is, its own lags and the lagged forecast errors, so that equation can be used to forecast future values. \nAny \u2018non-seasonal\u2019 time series that exhibits patterns and is not a random white noise can be modeled with ARIMA models.\n\nAn ARIMA model is characterized by 3 terms: p, d, q\n\nwhere,\n\n* p is the order of the AR term\n\n* q is the order of the MA term\n\n* d is the number of differencing required to make the time series stationary\n\nIf a time series, has seasonal patterns, then you need to add seasonal terms and it becomes SARIMA, short for \u2018Seasonal ARIMA\u2019. More on that once we finish ARIMA.\n"}}