{"cell_type":{"c8958b14":"code","abc273dc":"code","dafc2ed4":"code","6e7e04c5":"code","d9b31d3b":"code","562e3356":"code","e36d2811":"code","01a166c5":"code","328e6591":"code","3bcdf4a7":"code","23c18dbf":"code","8a34cbad":"code","b1519dfa":"code","4a4ccb9e":"code","f19c33e4":"code","fbf5bfc6":"markdown","62a14771":"markdown","87dd2fe6":"markdown","bdf1fdda":"markdown","0ea38681":"markdown","c2f188e4":"markdown","c32f6988":"markdown","c5feb1b7":"markdown","57814990":"markdown","7d0f9025":"markdown","1d8d07b5":"markdown","a655cab7":"markdown","a8cc2e3c":"markdown","70efa0c9":"markdown"},"source":{"c8958b14":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")","abc273dc":"df = pd.read_csv(\"\/kaggle\/input\/ibm-hr-analytics-attrition-dataset\/WA_Fn-UseC_-HR-Employee-Attrition.csv\")\ndf.head()","dafc2ed4":"sns.countplot(x='Attrition', data=df)","6e7e04c5":"df.drop(['EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours'], axis=\"columns\", inplace=True)\n\ncategorical_col = []\nfor column in df.columns:\n    if df[column].dtype == object and len(df[column].unique()) <= 50:\n        categorical_col.append(column)\n        \ndf['Attrition'] = df.Attrition.astype(\"category\").cat.codes","d9b31d3b":"categorical_col.remove('Attrition')","562e3356":"# Transform categorical data into dummies\n# categorical_col.remove(\"Attrition\")\n# data = pd.get_dummies(df, columns=categorical_col)\n# data.info()\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel = LabelEncoder()\nfor column in categorical_col:\n    df[column] = label.fit_transform(df[column])","e36d2811":"from sklearn.model_selection import train_test_split\n\nX = df.drop('Attrition', axis=1)\ny = df.Attrition\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","01a166c5":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n\ndef print_score(clf, X_train, y_train, X_test, y_test, train=True):\n    if train:\n        pred = clf.predict(X_train)\n        clf_report = pd.DataFrame(classification_report(y_train, pred, output_dict=True))\n        print(\"Train Result:\\n================================================\")\n        print(f\"Accuracy Score: {accuracy_score(y_train, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_train, pred)}\\n\")\n        \n    elif train==False:\n        pred = clf.predict(X_test)\n        clf_report = pd.DataFrame(classification_report(y_test, pred, output_dict=True))\n        print(\"Test Result:\\n================================================\")        \n        print(f\"Accuracy Score: {accuracy_score(y_test, pred) * 100:.2f}%\")\n        print(\"_______________________________________________\")\n        print(f\"CLASSIFICATION REPORT:\\n{clf_report}\")\n        print(\"_______________________________________________\")\n        print(f\"Confusion Matrix: \\n {confusion_matrix(y_test, pred)}\\n\")","328e6591":"from sklearn.tree import DecisionTreeClassifier\n\ntree_clf = DecisionTreeClassifier(random_state=42)\ntree_clf.fit(X_train, y_train)\n\nprint_score(tree_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(tree_clf, X_train, y_train, X_test, y_test, train=False)","3bcdf4a7":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {\n    \"criterion\":(\"gini\", \"entropy\"), \n    \"splitter\":(\"best\", \"random\"), \n    \"max_depth\":(list(range(1, 20))), \n    \"min_samples_split\":[2, 3, 4], \n    \"min_samples_leaf\":list(range(1, 20)), \n}\n\n\ntree_clf = DecisionTreeClassifier(random_state=42)\ntree_cv = GridSearchCV(tree_clf, params, scoring=\"accuracy\", n_jobs=-1, verbose=1, cv=3)\ntree_cv.fit(X_train, y_train)\nbest_params = tree_cv.best_params_\nprint(f\"Best paramters: {best_params})\")\n\ntree_clf = DecisionTreeClassifier(**best_params)\ntree_clf.fit(X_train, y_train)\nprint_score(tree_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(tree_clf, X_train, y_train, X_test, y_test, train=False)","23c18dbf":"from IPython.display import Image\nfrom six import StringIO\nfrom sklearn.tree import export_graphviz\nimport pydot\n\nfeatures = list(df.columns)\nfeatures.remove(\"Attrition\")","8a34cbad":"dot_data = StringIO()\nexport_graphviz(tree_clf, out_file=dot_data, feature_names=features, filled=True)\ngraph = pydot.graph_from_dot_data(dot_data.getvalue())\nImage(graph[0].create_png())","b1519dfa":"from sklearn.ensemble import RandomForestClassifier\n\nrf_clf = RandomForestClassifier(n_estimators=100)\nrf_clf.fit(X_train, y_train)\n\nprint_score(rf_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(rf_clf, X_train, y_train, X_test, y_test, train=False)","4a4ccb9e":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 110, num=11)]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4]\nbootstrap = [True, False]\n\nrandom_grid = {'n_estimators': n_estimators, 'max_features': max_features,\n               'max_depth': max_depth, 'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}\n\nrf_clf = RandomForestClassifier(random_state=42)\n\nrf_cv = RandomizedSearchCV(estimator=rf_clf, scoring='f1',param_distributions=random_grid, n_iter=100, cv=3, \n                               verbose=2, random_state=42, n_jobs=-1)\n\nrf_cv.fit(X_train, y_train)\nrf_best_params = rf_cv.best_params_\nprint(f\"Best paramters: {rf_best_params})\")\n\nrf_clf = RandomForestClassifier(**rf_best_params)\nrf_clf.fit(X_train, y_train)\n\nprint_score(rf_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(rf_clf, X_train, y_train, X_test, y_test, train=False)","f19c33e4":"n_estimators = [100, 500, 1000, 1500]\nmax_features = ['auto', 'sqrt']\nmax_depth = [2, 3, 5]\nmax_depth.append(None)\nmin_samples_split = [2, 5, 10]\nmin_samples_leaf = [1, 2, 4, 10]\nbootstrap = [True, False]\n\nparams_grid = {'n_estimators': n_estimators, 'max_features': max_features,\n               'max_depth': max_depth, 'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf, 'bootstrap': bootstrap}\n\nrf_clf = RandomForestClassifier(random_state=42)\n\nrf_cv = GridSearchCV(rf_clf, params_grid, scoring=\"f1\", cv=3, verbose=2, n_jobs=-1)\n\n\nrf_cv.fit(X_train, y_train)\nbest_params = rf_cv.best_params_\nprint(f\"Best parameters: {best_params}\")\n\nrf_clf = RandomForestClassifier(**best_params)\nrf_clf.fit(X_train, y_train)\n\nprint_score(rf_clf, X_train, y_train, X_test, y_test, train=True)\nprint_score(rf_clf, X_train, y_train, X_test, y_test, train=False)","fbf5bfc6":"\n# \ud83c\udf33 Decision Tree & Random Forest\n\n# 1. Decision Tree\n\n![Capture.PNG](attachment:Capture.PNG)\n\nDecision Trees are an important type of algorithm for predictive modeling machine learning.\n\nThe classical decision tree algorithms have been around for decades and modern variations like random forest are among the most powerful techniques available.\n\nClassification and Regression Trees or `CART` for short is a term introduced by `Leo Breiman` to refer to Decision Tree algorithms that can be used for classification or regression predictive modeling problems.\n\nClassically, this algorithm is referred to as \u201c`decision trees`\u201d, but on some platforms like R they are referred to by the more modern term CART.\n\nThe `CART` algorithm provides a foundation for important algorithms like `bagged decision trees`, `random forest` and `boosted decision trees`.\n\n### CART Model Representation\nThe representation for the CART model is a binary tree.\n\nThis is your binary tree from algorithms and data structures, nothing too fancy. Each root node represents a single input variable (x) and a split point on that variable (assuming the variable is numeric).\n\nThe leaf nodes of the tree contain an output variable (y) which is used to make a prediction.\n\nGiven a new input, the tree is traversed by evaluating the specific input started at the root node of the tree.\n\n#### Some **advantages** of decision trees are:\n* Simple to understand and to interpret. Trees can be visualised.\n* Requires little data preparation. \n* Able to handle both numerical and categorical data.\n* Possible to validate a model using statistical tests. \n* Performs well even if its assumptions are somewhat violated by the true model from which the data were generated.\n\n#### The **disadvantages** of decision trees include:\n* Overfitting. Mechanisms such as pruning (not currently supported), setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem.\n* Decision trees can be unstable. Mitigant: Use decision trees within an ensemble.\n* Cannot guarantee to return the globally optimal decision tree. Mitigant: Training multiple trees in an ensemble learner\n* Decision tree learners create biased trees if some classes dominate. Recommendation: Balance the dataset prior to fitting\n\n# 2. Random Forest\nRandom Forest is one of the most popular and most powerful machine learning algorithms. It is a type of ensemble machine learning algorithm called Bootstrap Aggregation or bagging.\n![inbox_3363440_e322b7c76f2ca838ba3753e3c76c5efc_inbox_2301650_875af39bcc296f0a783519a400412dee_RF.jpg](attachment:inbox_3363440_e322b7c76f2ca838ba3753e3c76c5efc_inbox_2301650_875af39bcc296f0a783519a400412dee_RF.jpg)\nTo improve performance of Decision trees, we can use many trees with a random sample of features chosen as the split.","62a14771":"### a) Randomized Search Cross Validation","87dd2fe6":"# 3. Random Forest\n\nA random forest is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting.\n\n- **Random forest algorithm parameters:**\n- `n_estimators`: The number of trees in the forest.\n*** \n- `criterion`: The function to measure the quality of a split. Supported criteria are \"`gini`\" for the Gini impurity and \"`entropy`\" for the information gain.\n***\n- `max_depth`: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than `min_samples_split` samples.\n***\n- `min_samples_split`: The minimum number of samples required to split an internal node.\n***\n- `min_samples_leaf`: The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression.\n***\n- `min_weight_fraction_leaf`: The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n***\n- `max_features`: The number of features to consider when looking for the best split.\n***\n- `max_leaf_nodes`: Grow a tree with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n***\n- `min_impurity_decrease`: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n***\n- `min_impurity_split`: Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.\n***\n- `bootstrap`: Whether bootstrap samples are used when building trees. If False, the whole datset is used to build each tree.\n***\n- `oob_score`: Whether to use out-of-bag samples to estimate the generalization accuracy.","bdf1fdda":"## 1. Decision Tree Classifier\n\n**Decision Tree parameters:**\n- `criterion`: The function to measure the quality of a split. Supported criteria are \"`gini`\" for the Gini impurity and \"`entropy`\" for the information gain.\n***\n- `splitter`: The strategy used to choose the split at each node. Supported strategies are \"`best`\" to choose the best split and \"`random`\" to choose the best random split.\n***\n- `max_depth`: The maximum depth of the tree. If None, then nodes are expanded until all leaves are pure or until all leaves contain less than `min_samples_split` samples.\n***\n- `min_samples_split`: The minimum number of samples required to split an internal node.\n***\n- `min_samples_leaf`: The minimum number of samples required to be at a leaf node. A split point at any depth will only be considered if it leaves at least ``min_samples_leaf`` training samples in each of the left and right branches.  This may have the effect of smoothing the model, especially in regression.\n***\n- `min_weight_fraction_leaf`: The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.\n***\n- `max_features`: The number of features to consider when looking for the best split.\n***\n- `max_leaf_nodes`: Grow a tree with ``max_leaf_nodes`` in best-first fashion. Best nodes are defined as relative reduction in impurity. If None then unlimited number of leaf nodes.\n***\n- `min_impurity_decrease`: A node will be split if this split induces a decrease of the impurity greater than or equal to this value.\n***\n- `min_impurity_split`: Threshold for early stopping in tree growth. A node will split if its impurity is above the threshold, otherwise it is a leaf.","0ea38681":"### Visualization of a tree","c2f188e4":"# Exploratory Data Analysis","c32f6988":"# Summary\nIn this notebook we learned the following lessons:\n- Decsion tree and random forest algorithms and the parameters of each algorithm.\n- How to tune hyperparameters for both Decision tree and Random Forest.\n- Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant. \n  - By sampling an equal number of samples from each class  \n  - By normalizing the sum of the sample weights (sample_weight) for each class to the same value. \n\n  \n## References:\n- [Hyperparameter Tuning the Random Forest in Python](https:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74)\n- [Decision Trees](https:\/\/scikit-learn.org\/stable\/modules\/tree.html)\n- [Ensemble methods](https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#forests-of-randomized-trees)\n- [Bagging and Random Forest Ensemble Algorithms for Machine Learning](https:\/\/machinelearningmastery.com\/bagging-and-random-forest-ensemble-algorithms-for-machine-learning\/)","c5feb1b7":"## 4. Random Forest hyperparameter tuning","57814990":"# Data Processing","7d0f9025":"# 3. Decision Tree & Random Forest Implementation in python\n\nWe will use Decision Tree & Random Forest in Predicting the attrition of your valuable employees.","1d8d07b5":"### b) Grid Search Cross Validation","a655cab7":"# Applying Tree & Random Forest algorithms","a8cc2e3c":"`Random search` allowed us to narrow down the range for each hyperparameter. Now that we know where to concentrate our search, we can explicitly specify every combination of settings to try. We do this with `GridSearchCV`, a method that, instead of sampling randomly from a distribution, evaluates all combinations we define.","70efa0c9":"## 2. Decision Tree Classifier Hyperparameter tuning"}}