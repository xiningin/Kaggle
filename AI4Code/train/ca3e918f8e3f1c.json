{"cell_type":{"bb0ff9a2":"code","8021c792":"code","54b4512e":"code","031681e8":"code","d9b89bc1":"code","14f81e7a":"code","b70fa0e6":"code","f666c661":"code","41587efc":"code","aff781eb":"code","9cd921ca":"code","22fd6e40":"code","667e1d58":"markdown","26ec8e94":"markdown","68124d32":"markdown","4c028f5f":"markdown","e9f5674b":"markdown","5aa01437":"markdown"},"source":{"bb0ff9a2":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nfrom sklearn.svm import SVC\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","8021c792":"np.random.seed(1)\n\n# Create nonlinear dataset\nX_xor = np.random.randn(200, 2)\ny_xor = np.logical_xor(X_xor[:, 0] > 0,\n                      X_xor[:, 1] > 0)\ny_xor = np.where(y_xor, 1, -1)","54b4512e":"# scatter plot\nplt.scatter(X_xor[y_xor == 1, 0],\n           X_xor[y_xor == 1, 1],\n           c = 'b', marker = 'x',\n           label = '1')\nplt.scatter(X_xor[y_xor == -1, 0],\n           X_xor[y_xor == -1, 1],\n           c = 'r', marker = 's',\n           label = '-1')\nplt.xlim([-3, 3])\nplt.ylim([-3, 3])\nplt.legend(loc = 'best')\nplt.tight_layout()\nplt.show()","031681e8":"# define function about visualizing decision_regions\ndef plot_decision_regions(X, y, classifier, test_idx = None, resolution = 0.02):\n    \n    # set marker and colormap\n    markers = ('s', 'x', 'o', '^', 'v')\n    colors = ('red', 'blue', 'lightgreen', 'cyan')\n    cmap = ListedColormap(colors[:len(np.unique(y))])\n    \n    # draws a decision boundary.\n    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n                          np.arange(x2_min, x2_max, resolution))\n    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n    Z = Z.reshape(xx1.shape)\n    plt.contourf(xx1, xx2, Z, alpha = 0.3, cmap = cmap)\n    plt.xlim(xx1.min(), xx1.max())\n    plt.ylim(xx2.min(), xx2.max())\n    \n    for idx, cl in enumerate(np.unique(y)):\n        plt.scatter(X[y == cl, 0], X[y == cl, 1],\n                   alpha = 0.8, c = colors[idx],    # alpha : size of marker\n                   marker = markers[idx], label = cl,\n                   edgecolor = 'black')\n        \n    if test_idx:\n        X_test, y_test = X[test_idx, :], y[test_idx]\n        \n        plt.scatter(X_test[:, 0], X_test[:, 1],\n                   c = '', edgecolor = 'black', alpha = 1,\n                   s = 100, label = 'test set')","d9b89bc1":"svm = SVC(kernel = 'rbf', random_state = 1, gamma = 0.1, C = 10)\nsvm.fit(X_xor, y_xor)\nplot_decision_regions(X_xor, y_xor, classifier = svm)\nplt.legend(loc = 'upper left')\nplt.tight_layout()\nplt.show()","14f81e7a":"# load dataset\niris = datasets.load_iris()\nX = iris.data[:, [2, 3]]\ny = iris.target\nprint('Class label :', np.unique(y))","b70fa0e6":"# split training set and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1, stratify = y)","f666c661":"# standardize\nsc = StandardScaler()\nsc.fit(X_train) # calculate mu and sigma\nX_train_std = sc.transform(X_train) # standardize\nX_test_std = sc.transform(X_test)","41587efc":"X_combined_std = np.vstack((X_train_std, X_test_std)) # vlookup combine\ny_combined_std = np.hstack((y_train, y_test))  # hlookup combine","aff781eb":"svm = SVC(kernel = 'rbf', gamma = 0.2, C = 1, random_state =1)\nsvm.fit(X_train_std, y_train)\nplot_decision_regions(X_combined_std, y_combined_std,\n                     classifier = svm,\n                     test_idx = range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc = 'upper left')\nplt.tight_layout()\nplt.show()","9cd921ca":"svm = SVC(kernel = 'rbf', gamma = 100, C = 1, random_state =1)\nsvm.fit(X_train_std, y_train)\nplot_decision_regions(X_combined_std, y_combined_std,\n                     classifier = svm,\n                     test_idx = range(105, 150))\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc = 'upper left')\nplt.tight_layout()\nplt.show()","22fd6e40":"#fig, ax = plt.subplots(1, 2, figsize =(18, 8))\n\n\nsvm = SVC(kernel = 'rbf', gamma = 0.2, C = 1, random_state =1)\nsvm.fit(X_train_std, y_train)\nplot_decision_regions(X_combined_std, y_combined_std,\n                     classifier = svm,\n                     test_idx = range(105, 150))\n\nsvm = SVC(kernel = 'linear', gamma = 0.2, C = 1, random_state =1)\nsvm.fit(X_train_std, y_train)\nplot_decision_regions(X_combined_std, y_combined_std,\n                     classifier = svm,\n                     test_idx = range(105, 150))\n\nplt.xlabel('petal length [standardized]')\nplt.ylabel('petal width [standardized]')\nplt.legend(loc = 'upper left')\nplt.tight_layout()\nplt.show()","667e1d58":"### **gamma is a parameter that limit the size of the Gaussian sphere.** \n\nAs the value of gamma increases, the effect or range of the support vector decreases. The decision boundaries become closer to the sample and more winding. We can see it by handling Iris dataset.","26ec8e94":"### We can see that decision region of rbf SVM is nonlinear.","68124d32":"We can see that the decision boundaries are very close around class 0 and class 1.","4c028f5f":"### Comparing with 'linear' and 'rbf'","e9f5674b":"This dataset can't be classified by linear superplane such as linear regression or linear SVM. \n\n### However **kernel SVM** can make it be classify.\n\n\nBasic idea of kernel method handling nonlinear data is using mapping function to make it project the high dimensions.\nConverting to a new three-dimensional characteristic space allows you to distinguish classes.\nIn high-dimensional space, a linear hyperplane that separates the two classes becomes a nonlinear decision boundary when you revert to the original characteristic space.\n\n\n$$ ex) \\phi(x_1, x_2) = (z_1, z_2, z_3) = (x_1, x_2, x_1^2 + x_2^2)$$\n\n\nThere are 'rbf' in SVM's kernel. It means **radiation base function**. It is kernel function by being defined like that:\n\n$$ K(x^{(i)}, x^{(i)}) =exp\\left(-\\frac{||x^{(i)} - x^{(j)}||^2}{2\\sigma^2}\\right) = exp(-\\gamma||x^{(i)} - x^{(j)}||^2) $$\n\n","5aa01437":"# Iris classification with sklearn SVM (nonlinear)\n\nThis notebook goal:\n- Understanding Support Vector Machine algorithm principle by checking decision boundary of nonlinear dataset.\n- Comparing with rbf and linear in Iris dataset.\n\nYou can learn about nonlinear SVM principle in [my blog](https:\/\/konghana01.tistory.com\/18)."}}