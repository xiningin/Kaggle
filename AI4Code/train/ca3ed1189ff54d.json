{"cell_type":{"645b8ff2":"code","4291cedd":"code","2de31aad":"code","ebf23ef5":"code","e6dfaf47":"code","9cd25982":"code","01f6422b":"code","4995d3ec":"code","9809553c":"code","6fc50190":"code","1e17425b":"code","efc7daa0":"code","bbb3cacf":"code","4f7df34a":"code","12981552":"code","de959acb":"code","004c7a7b":"code","24da2eea":"code","3835df0c":"code","3073d082":"code","5d8aa6d2":"code","a416a3c0":"code","35372457":"code","6efd2f4e":"code","91490bfc":"code","0bbac3a1":"code","f8dd28eb":"code","55a0a27a":"code","72941b3f":"code","50ddabfd":"code","8a920793":"code","b18d99e5":"code","03dff627":"code","0754e602":"code","aee284ab":"code","7c17493f":"code","48c965f0":"code","40456b1e":"code","07ad5455":"code","ab45f9b8":"code","483b511c":"code","7152da18":"code","aa712e2b":"code","9c2b03a7":"code","7f183e1e":"code","eb972e88":"code","378e76a2":"code","3d0a11f3":"code","d984150e":"code","5ae6a681":"code","33e5e23b":"code","ce694570":"code","5d236d59":"code","d4964768":"code","881ebcb9":"code","c9099891":"code","4ee0e33e":"code","e5542d3d":"code","eb69bdc6":"code","abc008d9":"code","0e73f10e":"code","6e8dfa61":"code","af74778a":"code","80275e60":"code","f9bdaca6":"code","bb8117d6":"code","b4c0752b":"code","0089c7ec":"code","6873b8c8":"code","449bb535":"code","e4c84fb7":"code","238f9f34":"code","54248b72":"code","39ed596e":"code","9ee09c67":"code","2f89cd8c":"code","18312bd0":"code","2038b16c":"code","8b91d83e":"code","605c89b2":"code","e3eea26e":"code","3842c256":"code","97fb678e":"code","6eff18e2":"code","90060a5f":"code","7af2d23d":"code","b4427ac8":"code","8515b430":"code","f89f2292":"code","0547eda9":"code","018be18e":"code","defb9247":"code","08b9cb84":"code","20ef4458":"code","75285ff6":"code","f505f172":"code","05146207":"code","642d8c3b":"code","54152452":"code","87a1d6b1":"code","ece2b7e7":"code","bf751390":"code","e9480409":"code","4391f2c9":"code","8ed358fe":"code","42840dca":"code","f2023c78":"code","f631de58":"code","94fa7613":"code","59f55d59":"markdown","2489eca3":"markdown","3cba73bf":"markdown","2f71d400":"markdown","a6ec5167":"markdown","3654d569":"markdown","b129ab9c":"markdown","18a28248":"markdown","f6518103":"markdown","1637b712":"markdown","497db278":"markdown","2908a746":"markdown","8e2ae74a":"markdown","70aafd6e":"markdown","b5727065":"markdown","909f57fa":"markdown","fe76fca2":"markdown","95085bdc":"markdown","2e6a1c95":"markdown","b1a6bf7f":"markdown","4157b873":"markdown","a96f9aaf":"markdown","f52e39c4":"markdown","4a0e4514":"markdown","e12093e4":"markdown","09999b33":"markdown","2c4b9b1d":"markdown","fe05fde6":"markdown","9cee0e4e":"markdown","c859fc5c":"markdown","c621cf86":"markdown","e4466c82":"markdown","6fdd8705":"markdown","af0c568e":"markdown","9395cb75":"markdown","916ec60b":"markdown","94004caf":"markdown","fb8525aa":"markdown","c92c62d2":"markdown","4a402c12":"markdown","4c805a74":"markdown","e7f03076":"markdown","64d7ee7d":"markdown","d934a6a7":"markdown","db716bda":"markdown","cfc58d89":"markdown","535dc4d5":"markdown","31588b60":"markdown","8777b01c":"markdown","318f5173":"markdown","ad59ced3":"markdown","92413a06":"markdown","84f7f6d1":"markdown","c2b476e0":"markdown","7691cdd1":"markdown","006698a9":"markdown","195c1d95":"markdown","422a8280":"markdown","fbe28a4f":"markdown"},"source":{"645b8ff2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4291cedd":"import missingno as msno\n#for data processing\nimport re\nimport pandas as pd\nimport numpy as np\n#for data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier","2de31aad":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')      \ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')  ","ebf23ef5":"train_df","e6dfaf47":"test_df","9cd25982":"print(train_df.columns.tolist())\nprint(test_df.columns.tolist())","01f6422b":"train_df.dtypes  #checking the datatypes","4995d3ec":"test_df.dtypes","9809553c":"train_df.info()","6fc50190":"test_df.info()","1e17425b":"null_values = train_df.isna().sum().sort_values(ascending=True)\nnull_values ","efc7daa0":"msno.matrix(train_df)  #matrix plot","bbb3cacf":"msno.bar(train_df) #bar plot","4f7df34a":"nan_values = train_df.isna().sum().sort_values(ascending=True)\nnan_values","12981552":"msno.matrix(test_df)   #matrix plot","de959acb":"msno.bar(test_df)  #bar plot","004c7a7b":"msno.heatmap(train_df) #heatmap","24da2eea":"msno.heatmap(test_df) #heatmap","3835df0c":"msno.dendrogram(train_df) #dendogram","3073d082":"msno.dendrogram(test_df) #dendogram","5d8aa6d2":"SurvivedOrDied = train_df['Survived'].value_counts()\nSurvivedOrDied","a416a3c0":"pie, ax = plt.subplots(figsize=[10,6])\nlabels = SurvivedOrDied.keys()\nplt.pie(x=SurvivedOrDied, autopct=\"%.1f%%\", explode=[0.05]*2, labels=labels, pctdistance=0.5)\nplt.title(\"Passengers : Survived or Died\", fontsize=14);","35372457":"fig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 5))\nwomen = train_df[train_df['Sex']=='female']\nmen = train_df[train_df['Sex']=='male']\nsns.set_palette(\"pastel\")\nax1 = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = 'Survived', ax = axes[0], kde =False)\nax2 = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = 'Not_Survived', ax = axes[0], kde =False)\nax1.legend()\nax1.set_title('Female')\n\nax2 = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=15, label = 'Survived', ax = axes[1], kde = False)\nax2 = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = 'Not_Survived', ax = axes[1], kde = False)\n\nax2.legend()\n\nax2.set_title('Male')","6efd2f4e":"#facetgrid visualization\n\n\nFacetGrid = sns.FacetGrid(train_df, row='Embarked', height=4.5, aspect=1.6)\nFacetGrid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette=None,  order=None, hue_order=None )\nFacetGrid.add_legend()","91490bfc":"print(train_df.duplicated().sum())\nprint(test_df.duplicated().sum())","0bbac3a1":"train_df = train_df.drop(['Ticket','Name'], axis=1)\ntrain_df","f8dd28eb":"test_df = test_df.drop(['Ticket','Name'], axis=1)\ntest_df","55a0a27a":"print(train_df.columns.tolist())\nprint(test_df.columns.tolist())","72941b3f":"train_df['Age'] = train_df['Age'].fillna((train_df['Age'].mean()))\ntrain_df['Age']= train_df['Age'].astype(int)\nprint (train_df['Age'].unique())\nprint()\nprint(train_df['Age'].dtypes)\ntrain_df","50ddabfd":"test_df['Age'] = test_df['Age'].fillna((test_df['Age'].mean()))\ntest_df['Age']= test_df['Age'].astype(int)\nprint (test_df['Age'].unique())\nprint()\nprint(test_df['Age'].dtypes)\ntest_df","8a920793":"train_df['Embarked'].describe()  ","b18d99e5":"top_value = 'S'\ndata = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(top_value)\n    \ntrain_df","03dff627":"test_df","0754e602":"print(train_df['Embarked'].isnull().sum().any())\nprint(test_df['Embarked'].isnull().sum().any())","aee284ab":"print(train_df['Cabin'].unique())","7c17493f":"import re\ndeck = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"U\": 8}\ndata = [train_df, test_df]\n\nfor df in data:\n    df['Cabin'] = df['Cabin'].fillna(\"U0\")\n    df['Deck'] = df['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\n    df['Deck'] = df['Deck'].map(deck)\n    df['Deck'] = df['Deck'].fillna(0)\n    df['Deck'] = df['Deck'].astype(int)","48c965f0":"train_df = train_df.drop(['Cabin'], axis=1)\ntest_df = test_df.drop(['Cabin'], axis=1)","40456b1e":"train_df","07ad5455":"test_df","ab45f9b8":"train_df['Small Family'] = np.where((train_df['SibSp'] <= 2) & (train_df['SibSp'] != 0), 1, 0)\ntest_df['Small Family'] = np.where((test_df['SibSp'] <= 2) & (test_df['SibSp'] != 0), 1, 0)\ntrain_df['Lonely Child'] = np.where(train_df['Parch'] == 1, 1, 0)\ntest_df['Lonely Child'] = np.where(test_df['Parch'] == 1, 1, 0)\n\ntrain_df['Family'] = train_df['SibSp'] + train_df['Parch']\ntest_df['Family'] = test_df['SibSp'] + test_df['Parch']","483b511c":"train_df.head()","7152da18":"test_df.head()","aa712e2b":"train_df = train_df.drop(['SibSp','Parch'], axis=1)\ntrain_df.head()","9c2b03a7":"test_df = test_df.drop(['SibSp','Parch'], axis=1)\ntest_df.head()","7f183e1e":"df = [train_df, test_df]\nfor dataset in df:\n    dataset['Age'] = dataset['Age'].astype(int)\n    dataset.loc[ dataset['Age'] <= 1, 'Age'] = 0  #infants\n    dataset.loc[(dataset['Age'] > 1) & (dataset['Age'] <= 11), 'Age'] = 1  #Child\n    dataset.loc[(dataset['Age'] > 11) & (dataset['Age'] <= 19), 'Age'] = 2  #teenagers\n    dataset.loc[(dataset['Age'] > 19) & (dataset['Age'] <= 25), 'Age'] = 3  #young \n    dataset.loc[(dataset['Age'] > 25) & (dataset['Age'] <= 30), 'Age'] = 4  #adults\n    dataset.loc[(dataset['Age'] > 30) & (dataset['Age'] <= 40), 'Age'] = 5  #Senior adults\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 60), 'Age'] = 6  # Most Senior Adults\n    dataset.loc[ dataset['Age'] > 60, 'Age'] = 7  # old persons","eb972e88":"train_df.head()","378e76a2":"test_df.head()","3d0a11f3":"train_df['Age'].value_counts()","d984150e":"test_df['Age'].value_counts()","5ae6a681":"# Import label encoder \nfrom sklearn import preprocessing\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder()\n \ntrain_df['Sex']= label_encoder.fit_transform(train_df['Sex']) \nprint(train_df.head())","33e5e23b":"test_df['Sex']= label_encoder.fit_transform(test_df['Sex']) \nprint(test_df.head())","ce694570":"train_df['Embarked']= label_encoder.fit_transform(train_df['Embarked']) \ntrain_df.head()","5d236d59":"train_df['Embarked'].unique()  ","d4964768":"test_df['Embarked']= label_encoder.fit_transform(test_df['Embarked']) \ntest_df.head()","881ebcb9":"test_df['Embarked'].unique()  ","c9099891":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","4ee0e33e":"data = [train_df, test_df]\n\nfor dataset in data:\n    dataset.loc[ dataset['Fare'] <= 7, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7) & (dataset['Fare'] <= 14), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[(dataset['Fare'] > 31) & (dataset['Fare'] <= 99), 'Fare']   = 3\n    dataset.loc[(dataset['Fare'] > 99) & (dataset['Fare'] <= 250), 'Fare']   = 4\n    dataset.loc[ dataset['Fare'] > 250, 'Fare'] = 5\n","e5542d3d":"train_df.head()","eb69bdc6":"test_df.head()","abc008d9":"plt.rcParams.update({'figure.figsize':(10,8), 'figure.dpi':100})\n# The plt.rcParams.update() function is used to change the default parameters of the plot's figure.","0e73f10e":"sns.pairplot(train_df, hue=\"Survived\")","6e8dfa61":"sns.lineplot( x=train_df['Age'], y=train_df['Survived'], marker='o')","af74778a":"sns.lineplot( x=train_df['Pclass'], y=train_df['Survived'],marker='o')","80275e60":"sns.lineplot( x=train_df['Deck'], y=train_df['Survived'], marker='o')","f9bdaca6":"sns.lineplot( x=train_df['Small Family'], y=train_df['Survived'], marker='o')","bb8117d6":"sns.lineplot( x=train_df['Lonely Child'], y=train_df['Survived'], marker='o')","b4c0752b":"sns.lineplot( x=train_df['Family'], y=train_df['Survived'], marker='o')","0089c7ec":"sns.stripplot( x=train_df['Age'], y=train_df['Survived'])","6873b8c8":"sns.stripplot( x=train_df['Deck'], y=train_df['Survived'])","449bb535":"sns.swarmplot( x=train_df['Deck'], y=train_df['Survived'])","e4c84fb7":"sns.histplot(data=train_df, x=\"Age\") ","238f9f34":"sns.histplot(data=train_df, x=\"Pclass\") ","54248b72":"sns.histplot(data=train_df, x=\"Survived\") ","39ed596e":"sns.histplot(data=train_df, x=\"Embarked\")","9ee09c67":"sns.histplot(data=train_df, x=\"Small Family\") ","2f89cd8c":"sns.histplot(data=train_df, x=\"Lonely Child\") ","18312bd0":"sns.histplot(data=train_df, x=\"Family\") ","2038b16c":"sns.kdeplot(train_df.Sex, hue = train_df.Survived)","8b91d83e":"sns.kdeplot(train_df.Family, hue = train_df.Survived)","605c89b2":"sns.kdeplot(train_df['Small Family'], hue = train_df.Survived)","e3eea26e":"sns.kdeplot(train_df['Lonely Child'], hue = train_df.Survived)","3842c256":"sns.kdeplot(train_df['Deck'], hue = train_df.Survived)","97fb678e":"sns.boxplot(x = 'variable', y = 'value', data=pd.melt(train_df[['Survived', 'Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Deck', 'Small Family', 'Family']]))\nplt.title('Distribution of the features')  #boxplot for all the features in a frame","6eff18e2":"sns.countplot(x='Sex', hue='Survived', data=train_df)","90060a5f":"sns.countplot(x='Pclass', hue='Survived', data=train_df)","7af2d23d":"sns.countplot(x='Lonely Child', hue='Survived', data=train_df)","b4427ac8":"sns.countplot(x='Small Family', hue='Survived', data=train_df)","8515b430":"sns.countplot(x='Family', hue='Survived', data=train_df)","f89f2292":"sns.countplot(x='Age', hue='Survived', data=train_df)","0547eda9":"sns.countplot(x='Deck', hue='Survived', data=train_df)","018be18e":"sns.countplot(x='Survived', data=train_df)","defb9247":"sns.barplot(x = train_df.Sex, y = train_df.Survived)","08b9cb84":"train_df.describe().T.head(10)  #T attribute, stands for transpose","20ef4458":"test_df.describe().T.head(10)","75285ff6":"sns.scatterplot(data=train_df, x=train_df['Family'], y=train_df['Survived'],hue = \"Age\")","f505f172":"sns.scatterplot(data=train_df, x=train_df['Small Family'], y=train_df['Survived'],hue = \"Age\")","05146207":"sns.scatterplot(data=train_df, x=train_df['Embarked'], y=train_df['Survived'],hue = \"Age\")","642d8c3b":"plt.figure(figsize=(10,10))\nsns.heatmap(np.abs(train_df.corr()),annot=True,cmap='viridis_r',fmt=\"0.2f\")\nplt.title('Correlation between Features in Train')","54152452":"plt.figure(figsize=(10,10))\nsns.heatmap(np.abs(test_df.corr()),annot=True,cmap='viridis_r',fmt=\"0.2f\")\nplt.title('Correlation between Features in Train')","87a1d6b1":"print(train_df.columns.tolist())\nprint(test_df.columns.tolist())","ece2b7e7":"x_train = train_df[['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Deck', 'Small Family', 'Lonely Child', 'Family']]  #Selecting independent features\nx_test  = test_df [['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Deck', 'Small Family', 'Lonely Child', 'Family' ]]\ny = train_df['Survived'] #Selecting target","bf751390":"from sklearn.model_selection import train_test_split     #data splitting\nX_train, X_test, y_train, y_test = train_test_split(x_train,y, test_size=0.2, random_state=20)\n","e9480409":"#LogisticRegression\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(random_state=21).fit(X_train, y_train)\nlr.score(X_test, y_test)\n\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(y_train,lr.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(y_test,lr.predict(X_test))*100))","4391f2c9":"#RandomForestClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\nRF=RandomForestClassifier(n_estimators=150, max_depth=25, random_state=25)\nRF.fit(X_train,y_train)\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(y_train,RF.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(y_test,RF.predict(X_test))*100))","8ed358fe":"#KNeighborsClassifier\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3) \nknn.fit(X_train, y_train) \n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(y_train,knn.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(y_test,knn.predict(X_test))*100))","42840dca":"#DecisionTreeClassifier\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nDTC= DecisionTreeClassifier(criterion='entropy',max_depth=25)\nDTC.fit(X_train,y_train)\n\nprint(\"Train Set Accuracy:\"+str(accuracy_score(y_train,DTC.predict(X_train))*100))\nprint(\"Test Set Accuracy:\"+str(accuracy_score(y_test,DTC.predict(X_test))*100))","f2023c78":"ran = RandomForestClassifier(max_depth=10,random_state=1)\nran.fit(X_train,y_train)\ntest_data = test_df.drop(\"PassengerId\", axis=1).copy()\nprediction = ran.predict(test_data)\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": prediction\n    })","f631de58":"submission.to_csv('submission.csv', index=False)","94fa7613":"submission = pd.read_csv('submission.csv')\nsubmission.head()","59f55d59":"##### Dropping unnecessary columns","2489eca3":"**Matrix-Plot** : If you're working with depth-related or time-series data, the matrix plot is an excellent tool. Each column has a different color fill. When data is provided, the plot is coloured in grey (or your preferred color), but when data is not present, the plot is white.\n\n**Bar Plot** : The barplot is a simple plot in which each bar represents one of the dataframe's columns. The height of the bar represents the column's completeness, or the number of non-null values present.\n\n**Heatmap:**  \n- used to identify correlations of the nullity between each of the different columns. In other words, it can be used to identify if there is a relationship in the presence of null values between each of the columns.\n- Values close to positive 1 indicate that the presence of null values in one column is correlated with the presence of null values in another column.\n- Values close to negative 1 indicate that the presence of null values in one column is anti-correlated with the presence of null values in another column. In other words, when null values are present in one column, there are data values present in the other column, and vice versa.\n- Values close to 0, indicate there is little to no relationship between the presence of null values in one column compared to another.\n- There are a number of values that show as <-1. This indicates that the correlation is very close to being 100% negative.\n\n**Dendogram :** \nThe dendrogram plot shows a tree-like graph generated by hierarchical clustering, that groups together columns with high nullity correlations.\nWhen a collection of columns is grouped at level zero, the presence or absence of nulls in one of the columns is directly related to the presence or absence of nulls in the other columns. The less the tree's columns are divided, the better.","3cba73bf":"###### ---- Visualizing methods to observe missing values in the dataframe","2f71d400":"To solve any problem it is necessary to understand the data. The most important task is to determine the Dependent Variable and Independent Variables. ","a6ec5167":"##### NAN Value to numeric value : columns of Continuous data","3654d569":"###### Column data details:\n\n- Survived - Survival (0 = No; 1 = Yes)\n- Pclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n- Name - Name\n- Sex - Sex\n- Age - Age\n- SibSp - Number of Siblings\/Spouses Aboard\n- Parch - Number of Parents\/Children Aboard\n- Ticket - Ticket Number\n- Fare - Passenger Fare\n- Cabin - Cabin\n- Embarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)","b129ab9c":"##### Distribution plot for visualizing the survival ratio between Male and Female ","18a28248":"###Handling Outliers with Box Plots","f6518103":"### ___(a)Understand the data","1637b712":"**Strip plot**","497db278":"##### missing value imputing value for the column 'Cabin'","2908a746":"####Categorize Fare","8e2ae74a":"# Exploratory Data Analysis (EDA)","70aafd6e":"# Checking Multicolinearity & Creating more new features, normalizing data with common features\n\n**Multicolinearity** : Whwn an indwpwndwnt variable can be predicted frpm another independent variable that is called multicolinearity. \n\n**How to overcome?**\n- Remove that column\n- Add columns together to create new column","b5727065":"####Creating more features according to age ranges","909f57fa":"#### Check Duplicate data","fe76fca2":"## ___(c)Bivariate analysis:\n- through multicolinearity check that has been done before\n- visualization through Scatter plots \n- Correlation Plot","95085bdc":"**2.Summary plots :** Provides moere concise informations\/description of the location, dispersion, and distribution of a variable than an enumerative plot. It is the most efficient way to show an overall conclusion of the overall dataset. \nTypes of Summary Plots:\n- Histograms\n- Density Plots\n- BOX plot\n- count plot\n- barplot","2e6a1c95":" - Uni-variate plots for are of two types: **1)Enumerative plots and 2)Summary plots**","b1a6bf7f":"  ###### missing value imputing value for the column 'Embarked'","4157b873":"As the task is to do predictive analysis on survival, \n#### The Dependent Variable(the Target\/ Label), y = Survived\n\nThe survival of the passengers depends on different parameters: Sex, Age, SibSp, Parch, Pclass, Fare, Cabin, Embarked\n#### The Independent Variables, X \nafter checking multicolinearity, there will be added more features\n\n& the unnecesary columns will be dropped as they do not have any impact on the result. ","a96f9aaf":"###Count Plot","f52e39c4":"**Swarm Plot**","4a0e4514":"Univariate data visualization helps to comprehend the enumerative properties and a descriptive summary of the particular data variable. To understand the location\/position of observations in the data variable (the Central Tendancy & dispersion) this plots are important. What type of Uni-variate analysis need to be performed depends on the data types if it's (a) Continous\/ Numerical data or (b) Categorical \/ Non-numerical data","e12093e4":"# Data Splitting","09999b33":"# Feature Engineering","2c4b9b1d":"##### Pie chart for visualizing the percentage of death and survival","fe05fde6":"### ____(c) Univariate analysis","9cee0e4e":"Before building any Machine Learning model, it is important to perform some Data Analysis to get general idea. EDA is the process to examine and understand the data and extracting data insights as well as main characteristics. To understand the problem statement and vasrious relationship between the data features EDA is very essential.\n\nEDA is generally classified into two methods:\ngraphical analysis\nnon-graphical analysis.\n\n## Steps of EDA\n-Understand the data\n-Basic Exploration and Data cleaning\n-Univariate Analysis\n-Bivariate Analysis\n-Encoding Categorical variables\n-Normalizing and Scaling","c859fc5c":"**Scatter Plot**\n\nIn a scatter plot it uses dots to represent values for **two different numeric variables**. The position of each dot on the horizontal and vertical axis indicates values for an individual data point. Scatter plots are used to observe relationships between variables.\n- Two columns need to be selected from a data table for Scatter plot, one for each dimension of the plot. Each row of the table will become a single dot in the plot with position according to the column values.\n##### when to use scatter plot?\n- To observe and show relationships between two numeric variables\n- Identification of correlational relationships between the dependent variables(y-axis) and independent variables(X-axis) \n-  Relationships between variables can be described in many ways: positive or negative, strong or weak, linear or nonlinear.  ","c621cf86":"In case of survival, Countplot shows different ratios here. \n","e4466c82":"**Difference between nan values and null Values, do they mean same result?**   \n\n**null values** represents **\"no value\" or \"nothing\"**, it's **not even an empty string or zero**. It can be used to represent that nothing useful exists.\n\n**NaN** stands for **\"Not a Number\"**, it's usually the **result of a mathematical operation** that doesn't make sense, i.e: 0.0\/0.0.","6fdd8705":"### ____ (b)Basic Exploration and Data Cleaning","af0c568e":"**FacetGrid class** helps in visualizing distribution of one variable as well as the relationship between multiple variables separately within subsets of your dataset using multiple panels. A FacetGrid can be drawn with up to three dimensions.\n\nhere showing the survival rate of male and female based on the embarked class. - Woman on port Q and S has the highest survival rate, and the male has the lowest survival rate. \n- in the port C, the male has the highest survival rate than the female passengers.           ","9395cb75":"- a smoother version of a histogram","916ec60b":"# Model Training and Evaluation","94004caf":"The strip plot is similar to a scatter plot, often used along with other kinds of plots for better analysis. It is used to visualize the distribution of data points of the variable.","fb8525aa":"**Histogram**\n\n- similar to bar charts, displays the counts or relative frequencies of values that is in different class intervals or ranges\n- displays the shape and spread of continuous sample data and  helps us understand the skewness and kurtosis of the distribution of the data.\n","c92c62d2":"### Correlation Map","4a402c12":"- small family\n- lonely child\n- family","4c805a74":"#### Issues found from the Dataset:","e7f03076":"Here data has been plot based on the age factor, whether the passengers survived or not survived, and it created a distribution plot for women and men.\n\n- From this distribution, men have a high probability of death, when they are between the age 18 and 40 years old. For woman's survival, chances are higher between 14 to 40 years.\n- women have a higher chance of survival than the men, the ages between 14 years to 30 years.\n- for men, the chances of survival is higher than they are between 18 to 40 for 40, and the probability of survival is very low between five and 18.\n- the infants have a higher probability of survival. ","64d7ee7d":"### Encoding Categorical variables using Label Encoder","d934a6a7":"##### NAN Value to numeric value : columns of Categorical data\n- fill the nan values with the common value for that column\n- apply **Label Encoding Method** for data conversion in numeric form","db716bda":"### Basic Exploration","cfc58d89":"- Need to drop unnecessary columns that are not responsible for the Survival of the passengers\n- Missing values are available, need to Drop or Impute the Missing Values\n- Duplicate data are unavailable, no issues with handling them\n- Categorical data is present, need to perform Label Encoding","535dc4d5":"### Checking missing values in the dataframe","31588b60":"**Density Plots**","8777b01c":"#### Non-Graphical Univariate Analysis:","318f5173":"### Data Cleaning\n\n- Removing unnecessary columns\n- Missing values treatment\n- Outlier treatment","ad59ced3":"#### Workflow: \n- Import required libraries\n- Exploratory Data Analysis(EDA)\n- Feature Scaling \n- Feature Engineering\n- Model creation and Evaluation","92413a06":"The swarm-plot, similar to a strip-plot, provides a visualization technique for univariate data to view the spread of values in a continuous variable. The only difference between the strip-plot and the swarm-plot is that the swarm-plot spreads out the data points of the variable automatically to avoid overlap and hence provides a better visual overview of the data.","84f7f6d1":"**1. Enumerative Plots :**  enumerates\/shows every observation in data, provides information on a single data variable about the distribution of the observations. Types of Enumerative Plots:\n- pair plot\n- Line Plot(with Markers)\n- Strip plot\n- Swarm Plot","c2b476e0":"### Missing Values Treatment ","7691cdd1":"The cause of missing values can be through data extraction and collection. The handling of missing data is very important during the preprocessing of the dataset because most of the machine learning algorithms do not support missing values. The methods to treat missing values: \n- Data Deletion\n- Mean\/ Mode\/ Median Imputation for both categorical and continuous data\n- Filling data with numeric values\n- Filling with common\/top value among the categorical data for that specific column\n\nnote: A model trained with the removal of all missing values creates a robust model. There is loss of a lot of informations, Shows poor performance if the percentage of missing values is excessive in comparison to the complete dataset. On the other hand, data imputation mostly works well on small datasets as it's easy to implement and prevents data loss. Therefore, It's not a good practice to directly delete\/drop the missing values without trying other methods. It is recomended to check the performance applying both methods.\n\n","006698a9":"## Data Imputation for Continous\/ Numerical Data : Mean Imputation Method\n","195c1d95":"## Import Required libraries","422a8280":"**Line Plot(with Markers)**","fbe28a4f":"#### Graphical Univariate Analysis"}}