{"cell_type":{"02a6651b":"code","9ac25f6c":"code","83490c36":"code","527392c1":"code","063fab38":"code","17c6c216":"code","1e257637":"code","9e84e1c7":"code","e054a371":"code","8e828610":"code","670a1105":"code","41ab1b73":"code","babeb768":"code","281694a1":"code","43a621b8":"code","3cfac3dd":"code","db85f745":"code","a0e75696":"code","caed7107":"code","93fd7fe0":"code","092aaa14":"code","93e1ad4f":"code","40bce8e4":"code","7776fb7b":"code","f486ad84":"code","80e3cde8":"code","ef3a26ff":"code","0cdd8d94":"code","cff91923":"code","469ddf97":"code","3f5cdd68":"code","8117c833":"code","5b943c7f":"code","bb3a812c":"code","c3696db8":"code","9dea9401":"code","62ee30bc":"code","2ce30f66":"code","b0d9a168":"code","fb214b1b":"code","cbfe9c8b":"code","b6773213":"code","2058d14e":"code","c3e44c70":"code","06527f45":"code","b5151295":"code","2e1f87e8":"code","e7403aa5":"code","54cc1c9a":"markdown","8b67cdcf":"markdown","5be15d8a":"markdown","082fd0e8":"markdown","56f0ad7a":"markdown","6b2e680a":"markdown","467bfb56":"markdown","b3ab87d9":"markdown","815ffe00":"markdown","1896c1d3":"markdown","4d84ae27":"markdown","ea585927":"markdown","52fa6f51":"markdown","cc0bd094":"markdown","a7327281":"markdown","4514606f":"markdown","16b6d77d":"markdown","4f6255cc":"markdown","1ef3f1dc":"markdown","770ff509":"markdown","7c5197c9":"markdown","2e7e8004":"markdown","5fd0d31e":"markdown","0ca77074":"markdown","6016231a":"markdown","9bc2ad4f":"markdown","399d5053":"markdown","cf7a8a0e":"markdown","7b7a9ef3":"markdown","fca138fa":"markdown"},"source":{"02a6651b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport sklearn\nfrom sklearn import metrics\nimport tensorflow as tf\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nimport numpy as np","9ac25f6c":"train = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\n\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n","83490c36":"def evaluate(model, x_val, y_val):\n    y_pred = model.predict(x_val)\n    r2 = metrics.r2_score(y_val, y_pred)\n    mse = metrics.mean_squared_error(y_val, y_pred)\n    mae = metrics.mean_absolute_error(y_val, y_pred)\n    msle = metrics.mean_squared_log_error(y_val, y_pred)\n    mape = np.mean(tf.keras.metrics.mean_absolute_percentage_error(y_val, y_pred).numpy())\n    rmse = np.sqrt(mse)\n    rmlse_score = rmlse(y_val, y_pred).numpy()\n    print(\"R2 Score:\", r2)\n    print(\"MSE:\", mse)\n    print(\"MAE:\", mae)\n    print(\"MSLE:\", msle)\n    print(\"MAPE\", mape)\n    print(\"RMSE:\", rmse)\n    print(\"RMLSE\", rmlse_score)\n    return {\"r2\": r2, \"mse\": mse, \"mae\": mae, \"msle\": msle, \"mape\": mape, \"rmse\": rmse, \"rmlse\": rmlse_score}","527392c1":"def export_result(model, df, file_path, features = None):\n    if features == None:\n        x = df\n    else:\n        x = df[features]\n    SalePrice = model.predict(x)\n    submission = pd.DataFrame({\"Id\": df[\"Id\"], \"SalePrice\": SalePrice.reshape(-1)})\n    submission.to_csv(file_path, index=False)","063fab38":"def rmlse(y_true, y_pred):\n    return tf.sqrt(tf.reduce_mean(tf.square(tf.math.log(y_pred + 1) - tf.math.log(y_true + 1))))","17c6c216":"train.head()","1e257637":"train.shape","9e84e1c7":"train.info()","e054a371":"train[train.columns[train.dtypes==object]].value_counts()","8e828610":"train.describe()","670a1105":"correlation_scores = train.corr()\ncorrelation_scores","41ab1b73":"train.corr()[\"SalePrice\"].sort_values(key = lambda x: abs(x), ascending=False)","babeb768":"null_counts = train.isnull().sum()\nnull_counts[null_counts > 0]","281694a1":"null_columns = list(pd.DataFrame(null_counts[null_counts > 0]).index)","43a621b8":"train[null_columns].dtypes","3cfac3dd":"for column in null_columns:\n    if train[column].dtype == object:\n        train[column] = train[[column]].replace(np.NAN, \"Unknown\")\n    else:\n        train[column] = train[column].replace(np.NAN, train[column].median())","db85f745":"null_counts = test.isnull().sum()\nnull_counts[null_counts > 0]\nnull_columns = list(pd.DataFrame(null_counts[null_counts > 0]).index)\nfor column in null_columns:\n    if test[column].dtype == object:\n        test[column] = test[[column]].replace(np.NAN, test[column].mode()[0])\n    else:\n        test[column] = test[column].replace(np.NAN, test[column].median())","a0e75696":"train.info()","caed7107":"train_test_dummied = pd.get_dummies(pd.concat([train, test]))","93fd7fe0":"train_test_dummied.head()","092aaa14":"mean_value = train_test_dummied.mean()\nstd_value = train_test_dummied.std()\nprint(mean_value)\nprint(std_value)","93e1ad4f":"mean_value.pop(\"SalePrice\")\nstd_value.pop(\"SalePrice\")","40bce8e4":"train_dummied = train_test_dummied.iloc[0: len(train)]\ntest_dummied = train_test_dummied.iloc[len(train):]\n_ = test_dummied.pop(\"SalePrice\")\ntrain_dummied.head()","7776fb7b":"test_dummied.head()","f486ad84":"train_dummied, val_dummied = train_test_split(train_dummied, test_size=0.2, random_state=np.random.randint(1, 1000))","80e3cde8":"train_dummied.corr()","ef3a26ff":"correlated_scores = train_dummied.corr()[\"SalePrice\"]\ncorrelated_scores = correlated_scores[correlated_scores.abs() > 0.2]\ncorrelated_features = list(correlated_scores.index)\ncorrelated_features.remove(\"SalePrice\")\ncorrelated_features","0cdd8d94":"y_train = train_dummied.pop(\"SalePrice\")\nx_train_dummied = train_dummied\ny_val = val_dummied.pop(\"SalePrice\")\nx_val_dummied = val_dummied","cff91923":"linear_regression_2 = LinearRegression()\nlinear_regression_2.fit(x_train_dummied[correlated_features], y_train)\nlinear_regression_2_results = evaluate(linear_regression_2, x_val_dummied[correlated_features], y_val)","469ddf97":"export_result(linear_regression_2, test_dummied, \"submission_linear_regression_2.csv\", features=correlated_features)","3f5cdd68":"from sklearn.linear_model import Ridge\nridge1 = Ridge()\nridge1.fit(x_train_dummied, y_train)\nridge1_results = evaluate(ridge1, x_val_dummied, y_val)","8117c833":"export_result(ridge1, test_dummied, \"submission_ridge1.csv\")","5b943c7f":"from sklearn.linear_model import Ridge\nridge2 = Ridge()\nridge2.fit(x_train_dummied[correlated_features], y_train)\nridge2_results = evaluate(ridge2, x_val_dummied[correlated_features], y_val)","bb3a812c":"export_result(ridge2, test_dummied, \"submission_ridge2.csv\", features=correlated_features)","c3696db8":"neural_network_model_1 = tf.keras.Sequential([\n    tf.keras.Input(shape=(x_train_dummied.shape[1])),\n    tf.keras.layers.Dense(32, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2()),\n    tf.keras.layers.Dense(32, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2()),\n    tf.keras.layers.Dense(1)\n])\nearly_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_rmlse\", patience=20)\ncheckpoint_path = \"neural_network_model_1.h5\"\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, monitor=\"val_rmlse\", save_best_only=True)\nneural_network_model_1.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\n    \"mse\", \"mae\", \"mape\", rmlse\n])\nhistory = neural_network_model_1.fit(\n    x_train_dummied, y_train, epochs=100, \n    validation_data=(x_val_dummied, y_val),\n    callbacks=[early_stop, checkpoint],\n    verbose=2\n)","9dea9401":"pd.DataFrame(history.history, columns=[\"loss\", \"val_loss\"]).plot()","62ee30bc":"pd.DataFrame(history.history, columns=[\"mae\", \"val_mae\"]).plot()","2ce30f66":"pd.DataFrame(history.history, columns=[\"rmlse\", \"val_rmlse\"]).plot()","b0d9a168":"export_result(neural_network_model_1, test_dummied, \"submission_neural_network_model_1.csv\")","fb214b1b":"x_train_subset = x_train_dummied[correlated_features]\nx_val_subset = x_val_dummied[correlated_features]\nneural_network_model_2 = tf.keras.Sequential([\n    tf.keras.Input(shape=(x_train_subset.shape[1])),\n    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2()),\n    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2()),\n    tf.keras.layers.Dense(1)\n])\nearly_stop = tf.keras.callbacks.EarlyStopping(patience=20)\nneural_network_model_2_checkpoint = \"neural_network_model_2.h5\"\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(neural_network_model_2_checkpoint, save_best_only=True)\nneural_network_model_2.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\n    \"mse\", \"mae\", \"mape\", rmlse\n])\nhistory = neural_network_model_2.fit(\n    x_train_subset, y_train, epochs=100, \n    validation_data=(x_val_subset, y_val),\n    callbacks=[early_stop, checkpoint]\n)","cbfe9c8b":"x_train_subset = x_train_dummied[correlated_features]\nx_val_subset = x_val_dummied[correlated_features]\nx_train_subset_scaled = (x_train_subset - mean_value[correlated_features]) \/ std_value[correlated_features]\nx_val_subset_scaled = (x_val_subset - mean_value[correlated_features]) \/ std_value[correlated_features]\ntest_dummied_scaled = (test_dummied[correlated_features] - mean_value[correlated_features]) \/ std_value[correlated_features]","b6773213":"model_dataframe = pd.DataFrame({\n        \"num_hidden_layers\": [\n            4, 5, 5, 5\n        ], \n        \"bottom_hidden_layer_size\": [2, 8, 2, 16],\n        \"hidden_layer_size_growth_strategy\": [\"double\", \"same\", \"double\", \"same\"],\n        \"hidden_layer_activation\": [\"relu\", \"relu\", \"relu\", \"relu\"],\n        \"hidden_layer_dropout\": [0.5, 0.3, 0.4, 0.2],\n        \"kernel_regularizer\": [\"l1\", \"l2\", \"l1_l2\", \"l2\"],\n        \"model_path\": [\"dnn1.h5\", \"dnn2.h5\", \"dnn3.h5\", \"dnn4.h5\"],\n        \"csv_path\": [\"dnn1.csv\", \"dnn2.csv\", \"dnn3.csv\", \"dnn4.csv\"],\n        \"val_mse\": [0.0, 0.0, 0.0, 0.0],\n        \"val_mae\": [0.0, 0.0, 0.0, 0.0],\n        \"val_mape\": [0.0, 0.0, 0.0, 0.0],\n        \"val_rmlse\": [0.0, 0.0, 0.0, 0.0]\n})","2058d14e":"model_dataframe.head()","c3e44c70":"import matplotlib.pyplot as plt\ndef train_model(\n    x_train,\n    y_train,\n    x_val,\n    y_val,\n    test, \n    Id,\n    df\n):\n    for i in range(len(df)):\n        num_hidden_layers = df.loc[i, \"num_hidden_layers\"]\n        bottom_hidden_layer_size = df.loc[i, \"bottom_hidden_layer_size\"]\n        hidden_layer_size_growth_strategy = df.loc[i, \"hidden_layer_size_growth_strategy\"]\n        hidden_layers = []\n        if hidden_layer_size_growth_strategy == \"same\":\n            hidden_layers = [bottom_hidden_layer_size] * num_hidden_layers\n        if hidden_layer_size_growth_strategy == \"double\":\n            hidden_layers = list(bottom_hidden_layer_size * np.power(2, np.arange(4)))\n            hidden_layers.reverse()\n        hidden_layers_activation = df.loc[i, \"hidden_layer_activation\"]\n        hidden_layer_dropout = df.loc[i, \"hidden_layer_dropout\"]\n        kernel_regularizer = df.loc[i, \"kernel_regularizer\"]\n        model_path = df.loc[i, \"model_path\"]\n        print(model_path)\n        csv_path = df.loc[i, \"csv_path\"]\n        if kernel_regularizer == \"l1\":\n            kernel_regularizer = tf.keras.regularizers.l1()\n        elif kernel_regularizer == \"l2\":\n            kernel_regularizer = tf.keras.regularizers.l2()\n        elif kernel_regularizer == \"l1_l2\":\n            kernel_regularizer = tf.keras.regularizers.l1_l2()\n        tf.keras.backend.clear_session()\n        model = tf.keras.Sequential()\n        model.add(tf.keras.Input(shape=(x_train.shape[1])))\n        for index, layer in enumerate(hidden_layers):\n            model.add(\n                tf.keras.layers.Dense(\n                    layer, \n                    activation=hidden_layers_activation, \n                    kernel_regularizer=kernel_regularizer\n                )\n            )\n            if hidden_layer_dropout != 0:\n                model.add(\n                    tf.keras.layers.Dropout(hidden_layer_dropout)\n                )\n        model.add(tf.keras.layers.Dense(1))\n        early_stop = tf.keras.callbacks.EarlyStopping(patience=10)\n        checkpoint = tf.keras.callbacks.ModelCheckpoint(model_path, save_best_only=True)\n        model.compile(loss=\"mse\", optimizer=\"adam\", metrics=[\n            \"mse\", \"mae\", \"mape\", rmlse\n        ])\n        history = model.fit(\n            x_train, y_train, epochs=100, \n            validation_data=(x_val, y_val),\n            callbacks=[early_stop, checkpoint],\n            verbose=0\n        )\n        model.load_weights(model_path)\n        best_index = np.argmin(history.history[\"val_rmlse\"])\n        metrics=[\"mse\", \"mae\", \"mape\", \"rmlse\"]\n        for metric in metrics:\n            val_metric = \"val_\" + metric\n            df.loc[i, val_metric] = history.history[val_metric][best_index]\n            print(val_metric, history.history[val_metric][best_index])\n            pd.DataFrame(history.history, columns=[metric, val_metric]).plot()\n            plt.show()\n        SalePrice = model.predict(test)\n        submission = pd.DataFrame({\"Id\": Id, \"SalePrice\": SalePrice.reshape(-1)})\n        submission.to_csv(csv_path, index=False)","06527f45":"train_model(\n    x_train_subset_scaled,\n    y_train,\n    x_val_subset_scaled,\n    y_val,\n    test_dummied_scaled, \n    test[\"Id\"],\n    model_dataframe\n)","b5151295":"model_dataframe.head()","2e1f87e8":"model_dataframe.sort_values(ascending=True, by=\"val_rmlse\", inplace=True)\nmodel_dataframe[[\"model_path\", \"val_mse\" , \"val_mae\", \"val_mape\", \"val_rmlse\"]].head()","e7403aa5":"submission = pd.read_csv(model_dataframe.loc[0, \"csv_path\"])\nsubmission.to_csv(\"submission.csv\", index=False)","54cc1c9a":"### Train Ridge Model with correlated features","8b67cdcf":"**Calculate Correlated Features**","5be15d8a":"**Convert categorical features to one hot vector**","082fd0e8":"## House Price Predictor using Different Models\nIn this notebook, I will use different models to create House Price Predictor and evaluate their performance.","56f0ad7a":"### Train Linear Regression Model with features that's correlated to house price","6b2e680a":"**Do the same for Test data set**","467bfb56":"## Data Cleaning","b3ab87d9":"## Common Functions","815ffe00":"**Train Deep Neural Network Model with with correlated scaled features**","1896c1d3":"## Model Development and Evaluation","4d84ae27":"## Import Packages","ea585927":"**Evaluation Function**","52fa6f51":"**Correlation scores**","cc0bd094":"### Train Ridge Model with all features","a7327281":"## Import Datasets","4514606f":"**Root Mean Squared Logarithmic Error**","16b6d77d":"**First 5 rows**","4f6255cc":"**Export Results**","1ef3f1dc":"**Train Validation Split**","770ff509":"## Train Deep Neural Network Model with all features\u00b6","7c5197c9":"**Features that has missing values**","2e7e8004":"## Exploratory Data Analysis","5fd0d31e":"**Solve Missing values for Training Set**\n\nI will use following strategies to apply imputation to missing values. \n- For numerical columns, I will replace missing value with their median value.\n- For categorical columns, I will replace missing value with unknown, which is a new category.","0ca77074":"**Features that contains missing values**","6016231a":"## Best DNN Model","9bc2ad4f":"**Its shape**","399d5053":"**Statistic infos**","cf7a8a0e":"**Now the data are all non-nulls**","7b7a9ef3":"**Factors that impact house price most**","fca138fa":"### Train Deep Neural Network Model with with correlated features"}}