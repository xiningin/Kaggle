{"cell_type":{"2d9b780a":"code","d4783059":"code","1f7884fb":"code","f394ce54":"code","40467fc8":"code","81dd1304":"code","519717ee":"code","5fd377df":"code","60d57114":"code","a577b845":"code","aa2032f3":"code","7b8fc7a5":"code","34170d95":"code","831572cf":"code","a4680f4e":"code","146a86e4":"code","3dddfbf9":"code","b1bc8b4e":"code","b7279464":"code","318923e7":"markdown","d5b4ec4c":"markdown","56c57982":"markdown","788e8442":"markdown","5d563e58":"markdown","fa0988e3":"markdown","7fedd370":"markdown","63e9e56e":"markdown","a668b7e2":"markdown","7ab55a79":"markdown","d1b947e0":"markdown","63cd85d2":"markdown","18b14b2a":"markdown","8e0c20c4":"markdown","b4f06a5e":"markdown","d9a1d0e9":"markdown","969aadca":"markdown","320b5a62":"markdown","aeceb902":"markdown","43270f2f":"markdown","00b550a1":"markdown","5faed5ac":"markdown","7e00730b":"markdown","fc7ffdd7":"markdown","94888721":"markdown","5afdb6bf":"markdown","3e3070d0":"markdown","a4ff950b":"markdown","7f5ff520":"markdown"},"source":{"2d9b780a":"import pdb\nfrom nltk.corpus import stopwords, twitter_samples\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport string\nfrom nltk.tokenize import TweetTokenizer\nfrom os import getcwd\n\n#from utils import process_tweet, lookup\n\nimport re\nimport string\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer\n\nfrom matplotlib.patches import Ellipse\nimport matplotlib.transforms as transforms","d4783059":"def process_tweet(tweet):\n    \"\"\"Process tweet function.\n    Input:\n        tweet: a string containing a tweet\n    Output:\n        tweets_clean: a list of words containing the processed tweet\n    \"\"\"\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    # remove stock market tickers like $GE\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    # tokenize tweets\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n\n    tweets_clean = []\n    for word in tweet_tokens:\n        if (word not in stopwords_english and  # remove stopwords\n                word not in string.punctuation):  # remove punctuation\n            stem_word = stemmer.stem(word)  # stemming word\n            tweets_clean.append(stem_word)\n\n    return tweets_clean","1f7884fb":"def build_freqs(tweets, ys):\n    \"\"\"Build frequencies.\n    Input:\n        tweets: a list of tweets\n        ys: an m x 1 array with the sentiment label of each tweet\n            (either 0 or 1)\n    Output:\n        freqs: a dictionary mapping each (word, sentiment) pair to its\n        frequency\n    \"\"\"\n    # Convert np array to list since zip needs an iterable.\n    # The squeeze is necessary or the list ends up with one element.\n    # Also note that this is just a NOP if ys is already a list.\n    yslist = np.squeeze(ys).tolist()\n\n    # Start with an empty dictionary and populate it by looping over all tweets\n    # and over all processed words in each tweet.\n    freqs = {}\n    for y, tweet in zip(yslist, tweets):\n        for word in process_tweet(tweet):\n            pair = (word, y)\n            if pair in freqs:\n                freqs[pair] += 1\n            else:\n                freqs[pair] = 1\n\n    return freqs","f394ce54":"# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')","40467fc8":"print(\"An example of a positive tweet not preprocessed yet: \\n\\n {}\".format(all_positive_tweets[0]))","81dd1304":"# split the data into two pieces, one for training and one for testing (validation set) \ntest_pos = all_positive_tweets[4000:]\ntrain_pos = all_positive_tweets[:4000]\ntest_neg = all_negative_tweets[4000:]\ntrain_neg = all_negative_tweets[:4000]\n\ntrain_x = train_pos + train_neg \ntest_x = test_pos + test_neg","519717ee":"print(\"An example of a positive tweet not preprocessed yet: \\n\\n {}\".format(train_x[0]))","5fd377df":"preprocessed_tweet_ex = process_tweet(train_x[0])\n\nprint(\"An example of a preprocessed positive tweet: \\n\\n {}\".format(preprocessed_tweet_ex))","60d57114":"# combine positive and negative labels\ntrain_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\ntest_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)","a577b845":"# Print the shape train and test sets\nprint(\"train_y.shape = \" + str(train_y.shape))\nprint(\"test_y.shape = \" + str(test_y.shape))","aa2032f3":"# create frequency dictionary\nfreqs = build_freqs(train_x, train_y)\n\n# check the output\nprint(\"type(freqs) = \" + str(type(freqs)))\nprint(\"len(freqs) = \" + str(len(freqs.keys())))","7b8fc7a5":"# test the function below\nprint('This is an example of a positive tweet: \\n', train_x[100])\nprint('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(train_x[100]))","34170d95":"def train_naive_bayes(freqs, train_x, train_y):\n\n    # vocab is a dect contains unique words \n    vocab = set([word for word, categ in freqs.keys()])\n    # v is the number of unique words \n    v = len(vocab)\n\n\n    # N_pos is total number of positive words for all tweets\n    # N_neg is total number of negative words for all tweets\n    N_pos = N_neg = 0\n    for pair in freqs: \n\n        if pair[1] == 1:\n            N_pos += 1\n\n        else:\n            N_neg += 1\n\n    # D is the total number of tweets\n    D = len(train_y)\n\n    # D_pos is the total number of positive tweets\n    D_pos = np.sum(train_y == 1)\n    # D_neg is the total number of positive tweets\n    D_neg = D - D_pos\n\n    # prob_D_pos is the probability that the tweet is positive\n    prob_D_pos = D_pos \/ D\n    # prob_D_neg is the probability that the tweet is Negative\n    prob_D_neg = D_neg \/ D\n\n    # prior probability represents the underlying probability in the target population\n    #that a tweet is positive versus negative.prior probability = prob_D_pos \/ prob_D_neg \n    # log_prior is the log value of prior probability and can be calculated in different ways\n    log_prior = np.log(D_pos) - np.log(D_neg)\n\n\n    loglikelihood = {}\n    for word in vocab:\n        # freq_pos is positive frequency of word \n        freq_pos = freqs.get((word, 1), 0)\n        # freq_neg is negative frequency of word\n        freq_neg = freqs.get((word, 0), 0)\n\n        # prob_word_given_pos is the probability that word is positive \n        prob_word_given_pos = (freq_pos + 1) \/ (N_pos + v)\n        # prob_word_given_neg is the probability that word is positive negative\n        prob_word_given_neg = (freq_neg + 1) \/ (N_neg + v)\n        \n        # loglikelihood is a dict contains the log_liklihood for each word in the vocab dictionary\n        # loglikelihood will be used in the Naive Bayes equation\n        loglikelihood[word] = np.log(prob_word_given_pos\/prob_word_given_neg)\n\n    return log_prior, loglikelihood","831572cf":"log_prior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)","a4680f4e":"def naive_bayes_predict(tweet, logprior, loglikelihood):\n    \n    #preprocessing the tweet to get a list of clean words\n    tweet_words_list = process_tweet(tweet)\n    \n    prob = 0 \n    \n    # for each word in the tweet get the loglikelihood value amd add it to prob\n    for word in tweet_words_list:\n        if word in loglikelihood:\n            prob += loglikelihood[word]      \n    \n    # the second part of the equation is the log prior which will be added to prob\n    prob += logprior\n    \n    return prob","146a86e4":"my_tweet = '@naser I am very happy because I am applying NLP :)'\np = naive_bayes_predict(my_tweet, log_prior, loglikelihood)\nprint(p)","3dddfbf9":"my_tweet = '@naser I am very sad, because I lose my cat'\np = naive_bayes_predict(my_tweet, log_prior, loglikelihood)\nprint(p)","b1bc8b4e":"def evaluate_naive_bayes(test_x, test_y, logprior, loglikelihood):\n    \n    accuracy = 0 \n    \n    y_hats = []\n    # for each tweet in the test dataset we will predict the class\n    for tweet in test_x:\n        # predicted class for tweet i \n        prediction = naive_bayes_predict(tweet, log_prior, loglikelihood)\n        # if the prediction is higher that 0 then the tweet is positive\n        if prediction > 0 : \n            y_hat = 1\n        # if the prediction is lower that 0 then the tweet is negative    \n        else: \n            y_hat = 0\n        \n        # y_hats is a list of test dataset preditions\n        y_hats.append(y_hat)\n        \n        \n    y_hats = np.array(y_hats).reshape(test_y.shape)    \n    accuracy = np.sum(y_hats == test_y) \/ len(test_y)\n    \n    return accuracy ","b7279464":"print(\"Naive Bayes accuracy = %0.4f\" %\n      (evaluate_naive_bayes(test_x, test_y, log_prior, loglikelihood)))","318923e7":"# 4- Evaluation ","d5b4ec4c":"To perform Naive Bayes we need to find the components of the equation step by step:\n\nThe first part of the equation which is **log(P(positive)\/P(Negative))** is called the **log-prior** and is equal to the log value of the probability of positive words in the whole tweets devided by the log value of the probability of negative words in the whole tweets. \n\n**log-prior** can be found using different equations in the **train_naive_bayes** function bellow **log_prior = np.log(D_pos) - np.log(D_neg)** is the log value of the number of positive words minus the log value of the number of negative words it's the same as  **log_prior = np.log(D_pos\/D_neg)** by the laws of logs. ","56c57982":"# 2- Data Preprocessing ","788e8442":"Now we will calculate the **accuracy** of our model on the test dataset","5d563e58":"In probability Bayes Rule: P(A|B) = P(intersection between A and B) \/ P(B). \n\nP(A|B) can be read: probability of event A given that event B has occured. \n\nSince P(A|B) = P(intersection between A and B) \/ P(B) and\nP(B|A) = P(intersection between B and A) \/ P(A)\n\nwe can find P(B|A) by:  P(B|A) = P(A|B) * (P(A) \/ P(B)) and thats the Bayes Rule in statisticaly view. \n","fa0988e3":"# 5- Thank you","7fedd370":"# Sentiment Analysis Using Na\u00efve Bayes ","63e9e56e":"-----------------------------------------------------------","a668b7e2":"**By now we have our (x_train, y_train) pairs, (x_test, y_test) pairs and Frequencies . Now we are ready for feature extraction and LR training.**","7ab55a79":"Since **p > 0** this tweet is positive whcich actually is !! I am really happy to apply NLP :) ","d1b947e0":"-----------------------------------","63cd85d2":"The process_tweet transform the tweet that full of meaningless things that will not help the Logistic Regreesion (LR) to oerform its task, so we need to clean the tweet using utilities in nltk. \n\n- remove stock market tickers\n- remove old style retweet text \"RT\"\n- remove hyperlinks\n- remove hashtags,  only removing the hash # sign from the word\n- and then tokenize the tweet. To transform the tweet into list of meaningfull words we will remove stopwords and punctuation, and finally stem the word. ","18b14b2a":"Now for each word in the vocabulary **vocab**  we need to find the **likelihood.** \n\nTo find the likelihood for each word we need to find **P(word|positive)** and **P(word|negative)** which can be calculated using the lablacian smoothing **P(word|positive) = (freq_pos + 1) \/ (N_pos + v)** and **P(word|negative) = (freq_neg + 1) \/ (N_neg + v)** . \n\nWe already found **N_pos** and **N_neg,** **v** is is the number of unique words, **freq_pos** is the positive frequency of word and freq_neg is the negative frequency of word. \n\nwe can now apply the equation by taking the log of the likelihood for the word. ","8e0c20c4":"------","b4f06a5e":"The Second part of the equation which is **Log(P(word|positive)\/P(word|negative))** need more steps and focus. \n\nIn the preprocessing section we get **freqs** which is a frequency dictionary where each pair in it is a key (word, class) and  value of frequency of the word in positive tweets. \n\nWe need now to find the total number of positive words for all tweets which is **N_pos** and total number of negative words for all tweets which is **N_neg** ","d9a1d0e9":"**train_y** is an np array of the shape (8000, 1) where the first 4000 entries are equal to 1 and the second 4000 entries are equal to 0\n\n**test_y** is an np array of the shape (2000, 1) where the first 1000 entries are equal to 1 and the second 1000 entries are equal to 0","969aadca":"Since **p < 0** this tweet is Negative whcich actually is !! it's really sad to lose a cat :( ","320b5a62":"We will take the first 4000 tweets from the negative examples and the first 4000 tweets in the positive examples to form the training dataset and the rest will be used to form the test dataset","aeceb902":"Thank you for reading, I hope you enjoyed and benefited from it.\n\nIf you have any questions or notes please leave it in the comment section.\n\nIf you like this notebook please press upvote and thanks again.","43270f2f":"Now the data will be splitted into training and test datasets ","00b550a1":"- After the tweets are clean, to perform **Negative & Positive Frequencies ** method, we need to find the Frequencies  which is a dictionary mapping each (word, sentiment) pair to its frequency: the number of times that a word showed up in a class.\n\n\n\n- This will done using the following function **build_freqs**","5faed5ac":"The word Naive is used because this methos assumes that the features used for classification are independent. \n\nBayes equation which is **log(P(positive)\/P(Negative)) + Sum(Log(P(word|positive)\/P(word|negative)))** can be used for classification where if the answer is **> 0** the tweet is positive and if the answer to this equation is **< 0** the tweet is negative. ","7e00730b":"In the example bellow I will apply the **naive_bayes_predict** to predict wither **my_tweet** is positive or negative. ","fc7ffdd7":"# 3- Training Naive Bayes model","94888721":"**train_x** will contain 8000 tweets where the first 4000 are positive tweets and the next 4000 are the negative tweets. \n\n**test_x** will contain 2000 tweets where the first 1000 are positive tweets and the next 1000 are the negative tweets.","5afdb6bf":"**Negative & Positive Frequencies ** method need a clean tweets, so we need to preprocess the tweets to get the best performance using this mehtod. \n\nPreprocessing tweets can be done by removing any meaningless words, letters and symbols such as **stop words and punctuations.** Some examples of stop words: and, or, a, is, are, for, has, at, ...\n\n\n","3e3070d0":"-------------------","a4ff950b":"# 1- Imports","7f5ff520":"By now we have the **log-prior** and a dictionary of **likelihood** for each word. We can use these things to classify a new tweet using the equation **log(P(positive)\/P(Negative)) + Sum(Log(P(word|positive)\/P(word|negative))).**"}}