{"cell_type":{"5b49b1fa":"code","29835697":"code","359a893b":"code","a193d45a":"code","f662b14b":"code","ef85d213":"code","d2514894":"code","b5345deb":"code","dc7154e3":"code","0398ec05":"code","287e94b7":"code","6ecd4f7d":"code","f9f05f04":"code","52f86bec":"code","9d7b852b":"code","1f5a6786":"code","f54f1d38":"code","3885ff91":"code","9e1e28e6":"code","23326f25":"code","55d8dd9a":"code","63561da6":"code","fcafe7b5":"code","d36fcf45":"code","dc1f9bf7":"code","ec9a0bd8":"code","51202b1f":"code","c7f44147":"markdown","7aac92f6":"markdown","9651e112":"markdown","8c04eacf":"markdown","46136574":"markdown","baaf3591":"markdown","99c00608":"markdown","bc152573":"markdown","63142dc2":"markdown"},"source":{"5b49b1fa":"#Install Contractions library\n!pip install contractions -q","29835697":"#Generic \nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re,string,unicodedata\nfrom string import punctuation\nimport contractions #import contractions_dict\nimport chardet\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# SK Learn Libraries\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import feature_extraction, model_selection\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder,LabelBinarizer\nfrom sklearn.metrics import accuracy_score, confusion_matrix,classification_report\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC\n\n# Keras Libraries\nfrom keras.models import Model,Sequential\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.callbacks import EarlyStopping\n\n#NLTK Libraries\nimport nltk\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk.corpus import stopwords\n\n#Warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Garbage Collection\nimport gc\n\n#downloading wordnet\/punkt dictionary\nnltk.download('wordnet')\nnltk.download('punkt')\nnltk.download('stopwords')\n\n#WordCloud Generator\nfrom wordcloud import WordCloud,STOPWORDS\n\n#Tabulation Library\nfrom tabulate import tabulate","359a893b":"url = '..\/input\/binary-reviews-from-imdb\/pos_neg_reviews.csv'\n\n# to find encoding of the file. \nwith open(url, 'rb') as f:\n    result = chardet.detect(f.read()) \n\n# loading the data with the detected encoding from above\nraw_data = pd.read_csv(url, header='infer', encoding=result['encoding'])","a193d45a":"#backup of raw data\nraw_data_bkp = raw_data.copy()","f662b14b":"# creating a new dataframe with specific columns\ndata = raw_data[['text','polarity']]","ef85d213":"print(\"Dataset Shape: \", data.shape)","d2514894":"#Checking for null\/missing value\ndata.isnull().sum()","b5345deb":"#Checking the records per polarity\ndata.groupby('polarity').size()","dc7154e3":"#Encode the Polarity Label to convert it into numerical values\nlab_enc = LabelEncoder()\n\n#Applying to the dataset\ndata['polarity'] = lab_enc.fit_transform(data['polarity'])","0398ec05":"# Data Preparation Function \n\ndef data_prep(text):\n    \n    #Lowering the case\n    text = text.lower()\n\n    # Stripping leading spaces (if any)\n    text = text.strip()\n        \n    # Remove Punctuations\n    for punctuations in punctuation:\n        text = text.replace(punctuations, '')\n    \n    # Remove macrons & accented characters\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    \n    # Expand contractions\n    text = contractions.fix(text)\n\n    # Remove special characters & retain alphabets\n    pattern = r'[^a-zA-z0-9\\s\\w\\t\\.]'\n    text = re.sub(pattern, '', text)\n    \n    # Stopword removal\n    stopword_list = set(stopwords.words('english'))\n    tokenizer = ToktokTokenizer()\n    \n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    \n    filtered_tokens = [token for token in tokens if token not in stopword_list]\n    text = ' '.join(filtered_tokens)    \n    \n    #Normalize the text\n    ps = nltk.porter.PorterStemmer()\n    text = ' '.join([ps.stem(word) for word in text.split()])\n    \n    return text\n    \n#Applying the Data Prep function to 'text' column\ndata['text'] = data['text'].apply(data_prep)","287e94b7":"#Backup of cleaned & normalized text data\nnorm_data_bkup = data.copy()","6ecd4f7d":"#Splitting the normalized data into train [90%] & test[10%] data\nx_train,x_test,y_train,y_test = train_test_split(data['text'], data.polarity, test_size=0.1, random_state=0)","f9f05f04":"#Inspect the split dataset\ninfo = [ [\"Training\" , x_train.shape[0]], [\"Testing\", x_test.shape[0]]  ]\nprint(tabulate(info, headers=['Dataset','Shape']))","52f86bec":"#List of store model accuracy\nmod_acc = []","9d7b852b":"# Constructing Pipeline to Extract Features, Transform Count Matrix & then build\/train Model\n\npipe = Pipeline([('vect', CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', MultinomialNB()) ])\n\nmnb_model = pipe.fit(x_train, y_train)","1f5a6786":"# Making Prediction on Test Data & Calculating Accuracy\nmnb_pred = mnb_model.predict(x_test)\nprint(\"Multinomial Naive Bayes Model Accuracy: \",'{:.2%}'.format(accuracy_score(y_test,mnb_pred)))","f54f1d38":"mnb_acc = accuracy_score(y_test,mnb_pred)\nmod_acc.extend([[\"Multinomial Naive Bayes\",'{:.2%}'.format(mnb_acc)]]) ","3885ff91":"# Constructing Pipeline to Extract Features, Transform Count Matrix & then build\/train Model\n\npipe = Pipeline([('vect', CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,3))),\n                 ('tfidf', TfidfTransformer()),\n                 ('model', LinearSVC()) ])\n\nsvm_model = pipe.fit(x_train, y_train)","9e1e28e6":"# Making Prediction on Test Data & Calculating Accuracy\nsvm_pred = svm_model.predict(x_test)\nprint(\"Support Vector Machines Model Accuracy: \",'{:.2%}'.format(accuracy_score(y_test,svm_pred)))","23326f25":"svm_acc = accuracy_score(y_test,svm_pred)\nmod_acc.extend([[\"Support Vector Machines\",'{:.2%}'.format(svm_acc)]]) ","55d8dd9a":"# Creating a RNN Function\n\nmax_words = 500\nmax_len = 100\n\ntokn = Tokenizer(num_words=max_words) \ntokn.fit_on_texts(x_train)\nsequences = tokn.texts_to_sequences(x_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n\ndef RNN():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","63561da6":"#Instantiating the RNN Model\nrnn_mod = RNN()\n\n#Compile Model\nrnn_mod.compile(loss='binary_crossentropy', optimizer= 'Nadam' , metrics=['accuracy'] )","fcafe7b5":"#Model Summary\nrnn_mod.summary()","d36fcf45":"#Train the model\nrnn_mod.fit(sequences_matrix, y_train, batch_size=256, epochs=5,\n            validation_split=0.1)\n           ","dc1f9bf7":"#Calculating Accuracy\ntest_sequences = tokn.texts_to_sequences(x_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\naccr = rnn_mod.evaluate(test_sequences_matrix,y_test)\nprint('LSTM (Keras) Accuracy: ','{:.2%}'.format(accr[1]))","ec9a0bd8":"mod_acc.extend([[\"LSTM (Keras)\",'{:.2%}'.format(accr[1])]]) ","51202b1f":"#Tabulating the results:\nprint (tabulate(mod_acc, headers=[\"Models\", \"Accuracy\"]))","c7f44147":"# Libraries","7aac92f6":"# Explore & Prep Data","9651e112":"# Data Split","8c04eacf":"# LSTM [Keras]","46136574":"# Support Vector Machines [SKLearn]","baaf3591":"# Load Data","99c00608":"The text data is now cleaned, prep'd and normalized. This data is now ready for classification, but before that this data needs to be backed up.","bc152573":"# Text Classification using different Models\n\nIn this notebook I'll compare the accuracies of different classification models. I will try to keep this notebook clear & concise as I feel it will be a huge one :-), so without wasting anytime lets get straight to it !","63142dc2":"# Multinomial Naive Bayes Classifier [SKLearn]"}}