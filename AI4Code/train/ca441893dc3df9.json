{"cell_type":{"811d1b90":"code","17dc5133":"code","08677ae6":"code","ded8420e":"code","7f2cd7ce":"code","04a53c8c":"code","41cb758c":"code","6d819db6":"code","75e587cd":"code","1295cf22":"code","99cadfb0":"code","ba7179aa":"code","f836da60":"code","38674e02":"code","a899287d":"code","bd57e1a8":"code","bbb4a20f":"code","bbf01b30":"code","62541b32":"code","84857959":"code","76f0632e":"code","54812dd3":"code","f8a5fd32":"code","d9527d2f":"code","f2dfb672":"markdown","c09dd5ca":"markdown","90cb80ff":"markdown","4e200f0b":"markdown","80c38e9e":"markdown","834af832":"markdown","3e9b9187":"markdown","8e11c5bc":"markdown","1b7f9770":"markdown","86ff267a":"markdown","9b0cd602":"markdown"},"source":{"811d1b90":"!pip install mojimoji -q","17dc5133":"import re\nimport mojimoji\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom tqdm.notebook import tqdm\n\npd.set_option('display.max_colwidth', 10000)","08677ae6":"# the dataset has missing values and duplicates, so remove them\n# also, exclude data labeled with Neutral sentiment because they're very little\ndata = pd.read_csv(\n    '..\/input\/bitcoin-tweets-16m-tweets-with-sentiment-tagged\/mbsa.csv',\n    usecols=['text','Sentiment']\n).dropna(how='any').drop_duplicates(subset='text').query('Sentiment != \"Neutral\"')\n\nprint(f'Data: {len(data)}')","ded8420e":"data.head(10)","7f2cd7ce":"def value_counts_plot(df):\n    label_counts = df['Sentiment'].value_counts()\n\n    plt.figure(figsize=(12,8))\n    plt.style.use('seaborn')\n    plt.bar(label_counts.index.values, \n            label_counts.values, \n            color=['blue','red'], \n            linewidth=0, \n            alpha=0.6)\n\n    plt.ylabel('value counts')\n    plt.show()\n    \nvalue_counts_plot(data)","04a53c8c":"def text_length_plot(df):\n    df['text_len'] = df['text'].apply(lambda x: len(x))\n    \n    plt.figure(figsize=(12,8))\n    plt.style.use('seaborn')\n    df.query('Sentiment == \"Positive\"').text_len.plot(\n        bins=50, \n        kind='hist', \n        color='red', \n        label='Positive', \n        alpha=0.6\n    )\n    \n    df.query('Sentiment == \"Negative\"').text_len.plot(\n        bins=50, \n        kind='hist', \n        color='blue', \n        label='Negative', \n        alpha=0.6\n    )\n    plt.legend()\n    plt.xlabel('text length')\n    plt.show()\n\ntext_length_plot(data)","41cb758c":"def extract_japanese(df):\n    ja_df = df.copy()\n    # japanese hiragana, katakana patterns\n    ja_pattern = re.compile('[\u3041-\u3093\u30a1-\u30f3]')\n    extract_rows = [\n        i for i, text in enumerate(tqdm(df['text']))\\\n        if ja_pattern.search(text) is not None\n    ]\n    ja_df = df.iloc[extract_rows]\n    return ja_df\n\ndata = extract_japanese(data)","6d819db6":"def text_preprocess(text):\n    # normalize\n    text = mojimoji.han_to_zen(text, ascii=False, digit=False)\n    text = mojimoji.zen_to_han(text, kana=False)\n    # remove url\n    text = re.sub(r'(https?|ftp)(:\\\/\\\/[-_\\.!~*\\'()a-zA-Z0-9;\\\/?:\\@&=\\+$,%#]+)', '' , text)\n    text = text.replace('\\n', '')\n    text = text.strip()\n    return text\n\ndef apply_preprocess(df):\n    new_df = df.copy()\n    # text preprocessing\n    new_df['text'] = [text_preprocess(text) for text in tqdm(df['text'])]\n    # down sampling\n    new_df['text_len'] = new_df['text'].apply(lambda x: len(x))\n    new_df.query('10 < text_len <= 40', inplace=True)\n    new_df = new_df[['text','Sentiment']]\n    return new_df\n\ndata = apply_preprocess(data)","75e587cd":"data.sample(50)","1295cf22":"data['Sentiment'].value_counts()","99cadfb0":"# installation for japanese tokenizer use\n!pip install fugashi==1.1.1 ipadic==1.0.0 -q\n\n!pip install transformers==4.10.0 unidic_lite==1.0.8 -q","ba7179aa":"import os\nimport gc\nimport warnings\nimport logging\nimport multiprocessing\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport pytorch_lightning as pl\nfrom pytorch_lightning.utilities.seed import seed_everything\nfrom transformers import BertModel, BertJapaneseTokenizer\nfrom transformers.optimization import Adafactor, AdafactorSchedule\n\nwarnings.filterwarnings('ignore')\n# this will hide all transformers warnings, be careful!\nlogging.getLogger('transformers').setLevel(logging.ERROR)","f836da60":"class Cfg:\n    debug = False\n    seed = 42\n    max_len = 32\n    epochs = 10\n    n_folds = 4\n    train_batch_size = 256\n    val_batch_size = 4096\n    head_hidden_size = 256\n    num_classes = 2\n    data_name = 'text'\n    target_name = 'Sentiment'\n    model_name = 'cl-tohoku\/bert-base-japanese'\n    target_map = {'Negative': 0, 'Positive': 1}\n    n_gpus = torch.cuda.device_count()\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","38674e02":"if Cfg.debug == True:\n    data = data.sample(1000, random_state=Cfg.seed)","a899287d":"seed_everything(Cfg.seed, workers=True)","bd57e1a8":"class TweetsDataset(Dataset):\n    def __init__(self, data, tokenizer, config):\n        super().__init__()\n        self.data = data\n        self.tokenizer = tokenizer\n        self.config = config\n    \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        data_row = self.data.iloc[index]\n        text = data_row[self.config.data_name]\n        labels = data_row[self.config.target_name]\n        \n        encoded = self.tokenizer.encode_plus(\n            text,\n            add_special_tokens=True,\n            max_length=self.config.max_len,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        encoded = {k: torch.tensor(v) for k, v in encoded.items()}\n        return {\n            'input_ids': encoded['input_ids'].flatten(),\n            'attention_mask': encoded['attention_mask'].flatten(),\n            'labels': torch.tensor(labels)\n        }","bbb4a20f":"# reference: https:\/\/www.kaggle.com\/shonenkov\/tpu-training-super-fast-xlmroberta\nclass LabelSmoothing(nn.Module):\n    def __init__(self, smoothing=0.1):\n        super().__init__()\n        self.smoothing = smoothing\n        self.confidence = 1.0 - self.smoothing\n        \n    def linear_combination(self, x, y):\n        return self.smoothing * x + (self.confidence) * y\n    \n    def forward(self, preds, labels):\n        if self.training:\n            logprobs = F.log_softmax(preds, dim=-1)\n            smooth_loss = -(logprobs).sum(dim=-1).mean()\n            nll_loss = F.nll_loss(logprobs, labels)\n            return self.linear_combination(smooth_loss \/ preds.size(-1), nll_loss)\n        else:\n            return F.cross_entropy(preds, labels)","bbf01b30":"class CustomModel(pl.LightningModule):\n    def __init__(self, config):\n        super().__init__()\n        self.config = config\n        self.bert = BertModel.from_pretrained(self.config.model_name, return_dict=True)\n        self.head = nn.Sequential(\n            nn.Linear(self.bert.config.hidden_size, self.config.head_hidden_size),\n            nn.ReLU(),\n            nn.Linear(self.config.head_hidden_size, self.config.head_hidden_size),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(self.config.head_hidden_size, self.config.num_classes)\n        )\n        self.criterion = LabelSmoothing()\n        \n    def forward(self, input_ids, attention_mask, labels=None):\n        output = self.bert(input_ids, attention_mask=attention_mask)\n        preds = self.head(output.pooler_output)\n        if labels is not None:\n            loss = self.criterion(preds, labels)\n            return loss\n        else:\n            return preds\n    \n    def training_step(self, batch, batch_idx):\n        loss = self.forward(input_ids=batch['input_ids'],\n                            attention_mask=batch['attention_mask'],\n                            labels=batch['labels'])\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        val_loss = self.forward(input_ids=batch['input_ids'],\n                                attention_mask=batch['attention_mask'],\n                                labels=batch['labels'])\n        self.log('val_loss', val_loss)\n        return val_loss\n    \n    def test_step(self, batch, batch_idx):\n        return self.valadation_step(batch, batch_idx)\n        \n    def configure_optimizers(self):\n        # Adafactor doesn't set the lr since it adjusts it internally depending on the options\n        optimizer = Adafactor(self.parameters(), warmup_init=True)\n        scheduler = AdafactorSchedule(optimizer)\n        return [optimizer], [scheduler]","62541b32":"# categorical encoding\ndata[Cfg.target_name] = data[Cfg.target_name].map(Cfg.target_map)\n\n# split into train and test, and perform cross-validation using train\ntrain, test = train_test_split(\n    data, \n    test_size=0.2, \n    random_state=Cfg.seed, \n    stratify=data[Cfg.target_name]\n)\n\ntrain = train.reset_index(drop=True)\ntest = test.reset_index(drop=True)\n\ndel data\ngc.collect()\n\nprint(f'Train Data: {len(train)}')\nprint(f'Test Data: {len(test)}')","84857959":"def inference(model, data_loader, config):\n    all_outputs = []\n    all_labels = []\n    \n    for batch_idx, batch in enumerate(data_loader):\n        ids = batch['input_ids']\n        mask = batch['attention_mask']\n        labels = batch['labels']\n        \n        ids = ids.to(config.device)\n        mask = mask.to(config.device)\n        labels = labels.to(config.device)\n        \n        model.to(config.device)\n        model.eval()\n        with torch.no_grad():\n            outputs = model(input_ids=ids, attention_mask=mask)\n        outputs = outputs.cpu().detach().numpy()[:,1].tolist()\n        labels = labels.cpu().detach().numpy().tolist() \n        all_outputs.extend(outputs)\n        all_labels.extend(labels)\n    \n    return all_outputs, all_labels","76f0632e":"def run_train(fold, tokenizer, train, tr_idx, val_idx, config):\n    print(f'     ----- Fold: {fold} -----')\n    tr, val = train.iloc[tr_idx], train.iloc[val_idx]\n    \n    train_ds = TweetsDataset(tr, tokenizer, config)\n    val_ds = TweetsDataset(val, tokenizer, config)\n    \n    train_loader = DataLoader(train_ds, \n                              batch_size=config.train_batch_size, \n                              num_workers=multiprocessing.cpu_count(), \n                              pin_memory=True, \n                              drop_last=True,\n                              shuffle=True)\n    \n    val_loader = DataLoader(val_ds, \n                            batch_size=config.val_batch_size, \n                            num_workers=multiprocessing.cpu_count(), \n                            pin_memory=True, \n                            drop_last=False,\n                            shuffle=False)\n    \n    \n    checkpoint = pl.callbacks.ModelCheckpoint(monitor='val_loss', \n                                              mode='min', \n                                              save_top_k=1,  \n                                              save_weights_only=True, \n                                              dirpath=f'model_fold{fold}\/')\n    \n    es_callback = pl.callbacks.EarlyStopping(monitor='val_loss', \n                                             patience=3)\n    \n    tb_logger = pl.loggers.TensorBoardLogger(f'model_fold{fold}_logs\/')\n\n    trainer = pl.Trainer(max_epochs=config.epochs,\n                         gpus=config.n_gpus,\n                         logger=tb_logger,\n                         callbacks=[checkpoint,es_callback])\n    \n    model = CustomModel(config)\n    model.to(config.device)\n    trainer.fit(model, train_loader, val_loader)\n    \n    model.load_state_dict(torch.load(checkpoint.best_model_path)['state_dict'])\n    outputs, labels = inference(model, val_loader, config)\n    \n    del train_ds, val_ds, train_loader, val_loader, model\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    return outputs, labels","54812dd3":"skf = StratifiedKFold(n_splits=Cfg.n_folds, shuffle=True, random_state=Cfg.seed)\ntokenizer = BertJapaneseTokenizer.from_pretrained(Cfg.model_name)\nscores = 0.0\n\nfor i, (tr_idx, val_idx) in enumerate(skf.split(train, train[Cfg.target_name])):\n    outputs, labels = run_train(i, tokenizer, train, tr_idx, val_idx, Cfg)\n    outputs = np.array(outputs)\n    labels = np.array(labels)\n    \n    score = roc_auc_score(labels, outputs)\n    scores += score\n    \n    del outputs, labels\n    gc.collect()\n    print(f'FOLD{i} SCORE: {score:.5f}')\nprint(f'{Cfg.n_folds}FOLDS CV SCORE: {scores\/Cfg.n_folds:.5f}')","f8a5fd32":"test_ds = TweetsDataset(test, tokenizer, Cfg)\ntest_loader = DataLoader(test_ds, \n                         batch_size=Cfg.val_batch_size, \n                         num_workers=multiprocessing.cpu_count(), \n                         pin_memory=False, \n                         drop_last=False,\n                         shuffle=False)","d9527d2f":"folds_outputs = []\n\nfor fold in range(Cfg.n_folds):\n    best_model_path = os.listdir(f'model_fold{fold}\/')[0]\n    model = CustomModel(Cfg)\n    model.load_state_dict(torch.load(f'model_fold{fold}\/{best_model_path}')['state_dict'])\n    outputs, labels = inference(model, test_loader, Cfg)\n    folds_outputs.append(outputs)\n    \n    del model, outputs\n    gc.collect()\n    torch.cuda.empty_cache()\n    \nfolds_outputs = np.array(folds_outputs)\nlabels = np.array(labels)\npreds = np.mean(folds_outputs, axis=0)\n\nscores = roc_auc_score(labels, preds)\n\nprint(f'{Cfg.n_folds}FOLDS TEST DATA SCORE: {scores:.5f}')","f2dfb672":"## Value Counts\n\ncheck the amount of data by class.","c09dd5ca":"## Preprocess","90cb80ff":"### Text Preprocessing and Down Sampling","4e200f0b":"## Text Length","80c38e9e":"## Thanks for reading!!","834af832":"### CV: Stratified KFold\n\nSplit the training data and validation data so that the ratio of the sentiment label, \n\nthe objective variable, is aligned.\n\n<br>\n\n### Metrics: Area under the ROC curve (ROC AUC)\n\nhttps:\/\/en.wikipedia.org\/wiki\/Receiver_operating_characteristic","3e9b9187":"## Introduction\n\nThis notebook challenges the task of text classification using an interesting dataset published by [Gaurav Dutta](https:\/\/www.kaggle.com\/gauravduttakiit).\n\nExtract only Japanese data from the dataset and performs sentiment classification.","8e11c5bc":"#### Adafactor\n\nhttps:\/\/arxiv.org\/abs\/1804.04235\n\nCompared to Adam optimizer, you can expect to save memory. \n\nAlso, When combined with the learning rate scheduler, a slight score increase can be expected.\n\n<br>\n\n#### Adding a Custom headers to the model\n\nBy adding custom headers with randomly initialized weights, \n\nWe'll aim to create a more task-specific model.\n\n<br>\n\n#### Apply Label Smoothing\n\nhttps:\/\/arxiv.org\/abs\/1906.02629\n\nLabel Smoothing is a regularization technique that introduces noise for the labels. ","1b7f9770":"### Extract Japanese Lang data\n\nextracts japanese data based on the japanese hiragana and katakana patterns.","86ff267a":"## BERT FineTuning\n\nFinetune in the following way.","9b0cd602":"## Evaluate using test data"}}