{"cell_type":{"2cb11ac2":"code","d957e9d9":"code","f87a7a69":"code","eb559fbb":"code","b0ded0bf":"code","e3b233a4":"code","e3e6090d":"code","15952e8d":"code","f3333eab":"code","0fb24010":"code","1b1f015a":"code","9257a163":"code","d4434c98":"code","5dd77d2b":"code","732449a8":"code","bf4c445d":"code","db4264c1":"code","8329619a":"code","1935da93":"code","d36d3062":"code","d4b14198":"code","ade46e89":"code","85551927":"code","685d9a8e":"code","7ef3c142":"code","12ca32c4":"code","54bb3c00":"code","90607948":"code","1283d82a":"code","3b9b81f5":"code","b21aef01":"code","fa34afb0":"code","908cc478":"code","e0bacab1":"code","c455eb46":"code","29e748d7":"code","1b2be0a0":"code","93be1860":"code","8587bcf2":"code","c2264315":"code","deb03237":"code","e5558494":"code","fbb1bb10":"code","a411270f":"code","eab8319b":"code","392bfa6f":"code","84678397":"code","ccf70a58":"code","f5bbf4c1":"code","73516ad8":"code","a8325ead":"code","b7129331":"code","c1938bd3":"code","5f8393a2":"code","7f17258d":"code","a8114aaa":"code","16ca70e0":"code","4d562a4b":"code","6941689f":"code","18e29ce2":"code","ef1ac6eb":"code","535a0316":"code","8f81fbcd":"code","af8b1163":"code","fd7a2154":"code","af49eb31":"code","5d9259c2":"code","d87c5259":"code","fdf79e3a":"code","3c40c054":"code","85ebe57a":"code","b092308f":"code","23d5da51":"code","cab08f2b":"code","73ba512b":"code","d01c741c":"code","df413f28":"code","40abea3f":"code","74221f00":"code","fb3723d9":"code","5805ca29":"code","bcc483a0":"code","8575ff1a":"code","85a16da6":"code","330881f6":"code","2b4e5551":"code","34bb699e":"code","58c642bd":"code","597bb6ee":"code","2da3b31e":"code","66040a72":"code","a821379e":"code","940bcc0b":"code","eba629a0":"code","666f02d5":"code","37741a56":"code","31e77e15":"code","b07856c7":"code","a03c7544":"code","504515cc":"code","a9b0fabf":"code","da786b9a":"code","a4ef42ae":"code","de7df7b9":"code","bb9c3e8b":"code","e7997be5":"code","eef8f276":"code","5f967302":"code","879704d6":"code","d7ac6444":"code","393d5eec":"code","d988d713":"code","289e3177":"code","7859722e":"code","9ef58d26":"code","67b3434e":"code","41243a62":"code","fd4c821c":"markdown","b90cd568":"markdown","f899f48f":"markdown","9d6c380f":"markdown","4cc4d479":"markdown","23e030ad":"markdown","92a925db":"markdown","c4f12232":"markdown","51452237":"markdown","730c333f":"markdown","472d7464":"markdown","3e0078af":"markdown","b72aba05":"markdown","94112ebf":"markdown","e1bbc418":"markdown","3c20bb85":"markdown","a3b7db4e":"markdown","588a5d09":"markdown","0e0522c9":"markdown","b61661cb":"markdown","61ba75fe":"markdown","f3148b6c":"markdown","2591cc5b":"markdown","b3a74814":"markdown","f30128da":"markdown","bc74189d":"markdown","39151ef2":"markdown","348b9f3b":"markdown","7a56b236":"markdown","8b2dea08":"markdown","32027b73":"markdown","21cd7a80":"markdown","36897e06":"markdown","64c7a0c5":"markdown","fc0ebdf8":"markdown","6922cd74":"markdown","abbc96eb":"markdown","2f579ff7":"markdown","de667434":"markdown","dea12374":"markdown","c661041b":"markdown","4c3e876f":"markdown","e78516b2":"markdown","0d7bc533":"markdown","7d0fa779":"markdown","e3750743":"markdown","30a5b9f7":"markdown","d4ae8b49":"markdown","b08b3132":"markdown","8f839df7":"markdown","727a93ea":"markdown"},"source":{"2cb11ac2":"\n#Import all the libraries required for analysis and building predcitive modeling\n# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visullizong missing vlaues in data set \nimport missingno as msno \n\n# visualization libraries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#setting display options\npd.set_option('display.max_rows', 5000)\npd.set_option('display.max_columns', 500)\npd.set_option('max_colwidth', 500)\nnp.set_printoptions(linewidth =400)\n\n# ignore warnings\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn\n\n# imuters \n\nfrom sklearn.impute import KNNImputer \n\n# machine learning computation \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n# Split data\nfrom sklearn.model_selection import train_test_split\n\nprint(\"All necessary files read\")","d957e9d9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","f87a7a69":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\n#combine = [train_df, test_df]\n#combine\ntrain_df.head()","eb559fbb":"train_df.dtypes\n","b0ded0bf":"print(train_df .shape)\nprint(test_df.shape)","e3b233a4":"#full_data = train_df.append( test_df , ignore_index = True )\n#titanic_train = full_data[ :891 ]\n\n#del train , test\n\n#print ('Datasets:' , 'full_data:' , full_data.shape , 'titanic:' , titanic_train.shape)","e3e6090d":"#full_data.tail()","15952e8d":"#titanic_test= full_data[891:]\n#titanic_test.head()","f3333eab":"#Visualization\n# For categorical variable\ndef plot_categories( df , cat , target , **kwargs ):\n    row = kwargs.get( 'row' , None )\n    col = kwargs.get( 'col' , None )\n    facet = sns.FacetGrid( df , row = row , col = col )\n    facet.map( sns.barplot , cat , target )\n    facet.add_legend()\n\n# For numerical variable\ndef plot_distribution( df , var , target , **kwargs ):\n    row = kwargs.get( 'row' , None )\n    col = kwargs.get( 'col' , None )\n    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n    facet.map( sns.kdeplot , var , shade= True )\n    facet.set( xlim=( 0 , df[ var ].max() ) )\n    facet.add_legend()\n \n# Plot corelation\ndef plot_correlation_map( df ):\n    corr = train.corr()\n    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n    _ = sns.heatmap(\n        corr, \n        cmap = cmap,\n        square=True, \n        cbar_kws={ 'shrink' : .9 }, \n        ax=ax, \n        annot = True, \n        annot_kws = { 'fontsize' : 12 }\n    )\n \n\n# Total % of missing vlaues\ndef missing_percentage(df):\n    total = df.isnull().sum().sort_values(ascending = False)\n    percent = round(df.isnull().sum().sort_values(ascending = False)\/len(df)*100,2)\n    return pd.concat([total, percent], axis=1, keys=['Total','Percent'])","0fb24010":"# missing values in Train data: data quality\nmissing_percentage(train_df)","1b1f015a":"# missing values in Test data: data quality\nmissing_percentage(test_df)","9257a163":"# Count categorical feature instances and %\ndef percent_value_counts(df, feature):\n    \"\"\"This function takes in a dataframe and a column and finds the percentage of the value_counts\"\"\"\n    percent = pd.DataFrame(round(df.loc[:,feature].value_counts(dropna=False, normalize=True)*100,2))\n    ## creating a df with th\n    total = pd.DataFrame(df.loc[:,feature].value_counts(dropna=False))\n    ## concating percent and total dataframe\n\n    total.columns = [\"Total\"]\n    percent.columns = ['Percent']\n    return pd.concat([total, percent], axis = 1)","d4434c98":"percent_value_counts(train_df, 'Embarked')","5dd77d2b":"# what are those 2 Null vlaues in Embarked\ntrain_df[train_df.Embarked.isnull()]","732449a8":"#import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style('darkgrid')\nfig, ax = plt.subplots(figsize=(10,7),ncols=2)\nax1 = sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=train_df, ax = ax[0]);\nax2 = sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=test_df, ax = ax[1]);\nax1.set_title(\"Training Set\", fontsize = 15)\nax2.set_title('Test Set',  fontsize = 15)","bf4c445d":"# Replace NA in Embarked column with 'C'\ntrain_df.Embarked.fillna('C', inplace = True)","db4264c1":"#percent_value_counts(train_df, 'Embarked')","8329619a":"# See value count of Cabin\npercent_value_counts(train_df, 'Cabin')","1935da93":"# See value count of Cabin\npercent_value_counts(test_df, 'Cabin')","d36d3062":"#combine train and test to tackle Cabin featute\n## Concat train and test into a variable \"all_data\"\nsurvivers = train_df.Survived\n\ntrain_df.drop([\"Survived\"],axis=1, inplace=True)\n\nall_data = pd.concat([train_df,test_df], ignore_index=False)\n\n## Assign all the null values to N\nall_data.Cabin.fillna(\"N\", inplace=True)","d4b14198":"all_data.Cabin = [i[0] for i in all_data.Cabin]","ade46e89":"percent_value_counts(all_data, 'Cabin')","85551927":"all_data.groupby('Cabin')['Fare'].mean().sort_values()","685d9a8e":"with_N = all_data[all_data.Cabin == \"N\"]\nwithout_N = all_data[all_data.Cabin != \"N\"]","7ef3c142":"def cabin_estimator(i):\n    \"\"\"Grouping cabin feature by the first letter\"\"\"\n    a = 0\n    if i<16:\n        a = \"G\"\n    elif i>=16 and i<27:\n        a = \"F\"\n    elif i>=27 and i<38:\n        a = \"T\"\n    elif i>=38 and i<47:\n        a = \"A\"\n    elif i>= 47 and i<53:\n        a = \"E\"\n    elif i>= 53 and i<54:\n        a = \"D\"\n    elif i>=54 and i<116:\n        a = 'C'\n    else:\n        a = \"B\"\n    return a","12ca32c4":"##applying cabin estimator function. \nwith_N['Cabin'] = with_N.Fare.apply(lambda x: cabin_estimator(x))\n\n## getting back train. \nall_data = pd.concat([with_N, without_N], axis=0)\n\n## PassengerId helps us separate train and test. \nall_data.sort_values(by = 'PassengerId', inplace=True)\n\n## Separating train and test from all_data. \ntrain = all_data[:891]\n\ntest = all_data[891:]\n\n# adding saved target variable with train. \ntrain['Survived'] = survivers","54bb3c00":"test[test.Fare.isnull()]","90607948":"missing_Fare = test[(test.Pclass == 3) & (test.Sex == \"male\") & (test.Embarked == \"S\")].Fare.mean()\nmissing_Fare","1283d82a":"test.Fare.fillna(missing_Fare, inplace = True)","3b9b81f5":"print (\"Train age missing value: \" + str((train.Age.isnull().sum()\/len(train))*100)+str(\"%\"))\nprint (\"Test age missing value: \" + str((test.Age.isnull().sum()\/len(test))*100)+str(\"%\"))","b21aef01":"# Transform Sex into binary values 0 and 1\n \ntrain['Sex'] = train.Sex.apply(lambda x: 0 if x == \"female\" else 1)\ntest['Sex'] = test.Sex.apply(lambda x: 0 if x == \"female\" else 1)","fa34afb0":"# display the statistial overview of data\ntrain.describe()","908cc478":"# display survival summary\nSurvival_summary = train.groupby('Survived')\nSurvival_summary.mean()","e0bacab1":"#Survival_summary.head()","c455eb46":"#Survival_summary.std()","29e748d7":" # Plot correlation to see what features correate most with survived.\n\n#plot_correlation_map(train_df )","1b2be0a0":"#Type of variables in  Full dataset\n#titanic_train.dtypes","93be1860":"#titanic_train.Survived.value_counts(1)*100#","8587bcf2":"plt.figure(figsize=(8,4))\nSurvived = train.Survived.value_counts()\nsns.barplot(y=Survived.values, x=Survived.index, alpha=0.6)\nplt.title('Distribution of Passenger survival')\nplt.xlabel('Passenger survival', fontsize=10)\nplt.ylabel('Count', fontsize=10)","c2264315":"# Survival rate and relationship with Parch\nplot_categories(train, cat ='Parch', target= 'Survived')  # Keywork argument","deb03237":"# plot Embarked feature and realtion with survival\n#plot_categories(titanic_train, cat ='Embarked', target= 'Survived')","e5558494":"# plot sex feature and realtion with survival\nplot_categories(train, cat ='Sex', target= 'Survived')","fbb1bb10":"# Survival rate and relationship with Pclass\nplot_categories(train, cat ='Pclass', target= 'Survived')","a411270f":"# Survival rate and relationship with SibSp\nplot_categories(train, cat ='SibSp', target= 'Survived')","eab8319b":"# Plot distribution of Age of passenger with respect to Sex\nplot_distribution( train , var = 'Age' , target = 'Survived' , row = 'Sex' )","392bfa6f":"# Plot distribution of Fare of passenger with respect to Survival rate\nplot_distribution(train , var = 'Fare' , target = 'Survived')","84678397":"plot_correlation_map(train)","ccf70a58":"train.corr()","f5bbf4c1":"pd.DataFrame(abs(train.corr()['Survived']).sort_values(ascending = False))","73516ad8":"## Family_size seems like a good feature to create\ntrain['family_size'] = train.SibSp + train.Parch+1  # extra 1 is for a passenger\ntest['family_size'] = test.SibSp + test.Parch+1","a8325ead":"def family_group(size):\n    a= ''\n    if (size <=1):\n        a = 'alone'\n    elif (size <= 4):\n        a = 'small'\n    else:\n        a = 'large'\n    return a","b7129331":"train['family_group'] = train['family_size'].map(family_group)\ntest['family_group'] = test['family_size'].map(family_group)","c1938bd3":"#train.head()","5f8393a2":"## Calculating fare based on family size. \n#train['calculated_fare'] = train.Fare\/train.family_size\n#test['calculated_fare'] = test.Fare\/test.family_size","7f17258d":"#train['Fare'].dtypes","a8114aaa":"def fare_group(fare):\n    a= ''\n    if fare <= 4:\n        a = 'Very_low'\n    elif fare <= 10:\n        a = 'low'\n    elif fare <= 30:\n        a = 'mid'\n    elif fare <= 50:\n        a = 'high'\n    else:\n        a = \"very_high\"\n    return a","16ca70e0":"train['fare_group'] = train['Fare'].map(fare_group)\ntest['fare_group'] = test['Fare'].map(fare_group)","4d562a4b":"train.drop(['PassengerId', 'Name', 'Ticket'], axis =1, inplace = True)\ntest.drop(['PassengerId', 'Name', 'Ticket'], axis =1, inplace = True)","6941689f":"train.head()","18e29ce2":"#percent_value_counts(train ,'Fare')","ef1ac6eb":"# Separate caegorocal variables and numerical variables to do feature encoding and combine later\ncat_var = ['Embarked', 'Pclass', 'Cabin', 'fare_group', 'family_group']\nnum_var = ['Age', 'Fare']\nrest_var = ['Parch', 'SibSp' , 'Survived', 'Pclass']","535a0316":"#The drop_first=True drops one column from the resulted dummy features. The purpose is to avoid multicollinearity. Here is the results:\n\ntrain = pd.get_dummies(train, columns=cat_var, drop_first=True)\ntest = pd.get_dummies(test, columns=cat_var, drop_first=True)","8f81fbcd":"# drop variable, no longer use\ntrain.drop(['family_size', 'Parch', 'SibSp'], axis =1, inplace = True)\ntest.drop(['family_size',  'Parch', 'SibSp'], axis =1, inplace = True)","af8b1163":"train.head()","fd7a2154":"train[train.Age.isnull()].head()","af49eb31":"train.dtypes","5d9259c2":"test[test.Age.isnull()].head()","d87c5259":"train_new = train\ntest_new = test","fdf79e3a":"train_new.shape\ntest_new.shape","3c40c054":"# from sklearn.impute import KNNImputer\n# age1 = train_new[['Age']]\n# age1.head()\n# imputer = KNNImputer(n_neighbors=2)\n# agetrain_imp =imputer.fit_transform(age1)","85ebe57a":"# import sklearn.preprocessing as preprocessing\n# scaler = preprocessing.StandardScaler()\n# col_names = ['Age']\n# #age_scale_param = scaler.fit(df['Age'].values)\n# features = train_new[col_names]\n# scaler = scaler.fit(features.values)\n# features = scaler.transform(features.values)\n# train_new[col_names] = features\n# train_new.head()","b092308f":"# from sklearn.ensemble import RandomForestRegressor\n# def predict_missing_ages(train_new):\n    \n#     known_age = train_new[train_new.Age.notnull()]\n#     unknown_age = train_new[train_new.Age.isnull()]\n#     unknown_age = unknown_age.drop(['Age'], axis =1)\n#     y = known_age['Age']\n#     X = known_age.drop(['Age'], axis =1)\n    \n#     print(X.shape, y.shape, unknown_age.shape)\n    \n\n#     rfr = RandomForestRegressor(random_state=0, n_estimators=2000, n_jobs=-1)\n#     rfr.fit(X, y)\n\n#     predictedAges = rfr.predict(unknown_age)\n    \n#     print(predictedAges)\n    \n \n#     df.loc[ (df.Age.isnull()), 'Age' ] = predictedAges \n    \n#     return df, rfr","23d5da51":"print (\"Train age missing value: \" + str((known_age_trn.Age.isnull().sum()\/len(train))*100)+str(\"%\"))\nprint (\"Test age missing value: \" + str((unknown_age_trn.Age.isnull().sum()\/len(test))*100)+str(\"%\"))","cab08f2b":"from sklearn.ensemble import RandomForestRegressor\nknown_age_trn = train_new[train_new.Age.notnull()]\nunknown_age_trn = train_new[train_new.Age.isnull()]\nunknown_age_trn_rfr = unknown_age_trn.drop('Age', axis =1).values\n# unknown_age_trn = unknown_age_trn .values\ny = known_age_trn['Age'].values\nX = known_age_trn.drop('Age', axis =1).values\n\n# split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# print(X.shape, y.shape, unknown_age.shape)\n    \nRFReg = RandomForestRegressor(n_estimators = 1000, random_state = 0)\nRFReg.fit(X_train, y_train)\n\n# predictedAges = RFReg.predict(unknown_age)\n    \n# print(predictedAges)\n\n# unknown_age\n#Predicted Height from test dataset w.r.t Random Forest Regression\ny_predict_trn= RFReg.predict((X_test))\n\n#Model Evaluation using R-Square for Random Forest Regression\nfrom sklearn import metrics\nr_square = metrics.r2_score(y_test, y_predict_trn)\nprint('R-Square Error associated with Random Forest Regression is:', r_square)","73ba512b":"print (\"Train age missing value: \" + str((known_age_trn.Age.isnull().sum()\/len(train))*100)+str(\"%\"))\nprint (\"Test age missing value: \" + str((unknown_age_trn.Age.isnull().sum()\/len(test))*100)+str(\"%\"))","d01c741c":"y_predict_trn = RFReg.predict((unknown_age_trn_rfr ))\n# y_predict_trn = pd.DataFrame(y_predict_trn)\nunknown_age_trn['Age'] = y_predict_trn \n# unknown_age_trn.head(100)","df413f28":"# unknown_age_trn['Age']","40abea3f":"print(unknown_age_trn.shape)\nprint(y_predict_trn.shape)\nprint(unknown_age_trn_rfr.shape)","74221f00":"print (\"Train age missing value: \" + str((known_age_trn.Age.isnull().sum()\/len(train))*100)+str(\"%\"))","fb3723d9":"# Combine tow dataframes to make it orignal train data\ntrain_new = pd.concat([known_age_trn, unknown_age_trn],ignore_index=True)","5805ca29":"print (\"Train age missing value: \" + str((train_new.Age.isnull().sum()\/len(train))*100)+str(\"%\"))","bcc483a0":"# unknown_age1['Age'] = y_predict_rfr \n# unknown_age.head()","8575ff1a":"from sklearn.ensemble import RandomForestRegressor\nknown_age_tst = test_new[test_new.Age.notnull()]\nunknown_age_tst = test_new[test_new.Age.isnull()]\nunknown_age_tst_rfr = unknown_age_tst.drop('Age', axis =1).values\n# unknown_age_trn = unknown_age_trn .values\ny = known_age_tst['Age'].values\nX = known_age_tst.drop('Age', axis =1).values\n\n# split data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n\n# print(X.shape, y.shape, unknown_age.shape)\n    \nRFReg = RandomForestRegressor(n_estimators = 1000, random_state = 0)\nRFReg.fit(X_train, y_train)\n\n# predictedAges = RFReg.predict(unknown_age)\n    \n# print(predictedAges)\n\n# unknown_age\n#Predicted Height from test dataset w.r.t Random Forest Regression\ny_predict_tst= RFReg.predict((X_test))\n\n#Model Evaluation using R-Square for Random Forest Regression\nfrom sklearn import metrics\nr_square = metrics.r2_score(y_test, y_predict_tst)\nprint('R-Square Error associated with Random Forest Regression is:', r_square)","85a16da6":"y_predict_tst = RFReg.predict((unknown_age_tst_rfr ))\n# y_predict_tst = pd.DataFrame(y_predict_tst)\nunknown_age_tst['Age'] = y_predict_tst\nunknown_age_tst.head()","330881f6":"# Combine tow dataframes to make it orignal test data\ntest_new = pd.concat([known_age_tst, unknown_age_tst],ignore_index=True)\ntest_new.shape","2b4e5551":"print (\"Test age missing value: \" + str((test_new.Age.isnull().sum()\/len(train))*100)+str(\"%\"))","34bb699e":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\n## transforming \"train_x\"\n#df = sc.fit_transform(train_new[''])","58c642bd":"#categorical_df = pd.get_dummies(categorical_df) #\n#categorical_df.head()","597bb6ee":"#numerical_df = numerical_df .fillna(numerical_df .mean() )\n#numerical_df.head()","2da3b31e":"#full_data_FE['Survived']=full_data_FE['Survived'].astype(int)","66040a72":"# Define X and Y in train_new\ny = train_new['Survived']\nX = train_new.drop(['Survived'], axis =1)\nprint(\"done\")","a821379e":"#X.head()","940bcc0b":"# Define train and test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X ,y,test_size=0.20, random_state=123, stratify=y)","eba629a0":"# from sklearn.utils import resample\n# from imblearn.over_sampling import SMOTE \n\n# # Upsample minority class\n# X_train_u, y_train_u = resample(X_train[y_train == 1],\n#                                 y_train[y_train == 1],\n#                                 replace=True,\n#                                 n_samples=X_train[y_train == 0].shape[0],\n#                                 random_state=1)\n\n# X_train_u = np.concatenate((X_train[y_train == 0], X_train_u))\n# y_train_u = np.concatenate((y_train[y_train == 0], y_train_u))\n\n# # Upsample using SMOTE\n# #sm = SMOTE(random_state=12)\n# #x_train_sm, y_train_sm = sm.fit_sample(X_train, y_train)\n\n\n# # Downsample majority class\n# X_train_d, y_train_d = resample(X_train[y_train == 0],\n#                                 y_train[y_train == 0],\n#                                 replace=True,\n#                                 n_samples=X_train[y_train == 1].shape[0],\n#                                 random_state=1)\n# X_train_d = np.concatenate((X_train[y_train == 1], X_train_d))\n# y_train_d = np.concatenate((y_train[y_train == 1], y_train_d))\n\n\n# print(\"Original shape:\", X_train.shape, y_train.shape)\n# print(\"Upsampled shape:\", X_train_u.shape, y_train_u.shape)\n# #print (\"SMOTE sample shape:\", x_train_sm.shape, y_train_sm.shape)\n# print(\"Downsampled shape:\", X_train_d.shape, y_train_d.shape)","666f02d5":"# from sklearn.linear_model import LogisticRegression\n# from sklearn.pipeline import make_pipeline\n# from sklearn.preprocessing import StandardScaler\n# from sklearn.model_selection import cross_val_score\n\n# # Create the Original, Upsampled, and Downsampled training sets\n# methods_data = {\"Original\": (X_train, y_train),\n#                 \"Upsampled\": (X_train_u, y_train_u),\n#                 \"Downsampled\": (X_train_d, y_train_d)}\n\n# # Loop through each type of training sets and apply 5-Fold CV using Logistic Regression\n# # By default in cross_val_score StratifiedCV is used\n# for method in methods_data.keys():\n#     lr_results = cross_val_score(LogisticRegression(), methods_data[method][0], methods_data[method][1], cv=5, scoring='f1')\n#     print(f\"The best F1 Score for {method} data:\")\n#     print (lr_results.mean())\n \n# cross_val_score(LogisticRegression(class_weight='balanced'), X_test, y_test, cv=5, scoring='f1').mean()","37741a56":"from sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\nlr = LogisticRegression()\n\n# Fit the model to the Upsampling data\nlr = lr.fit(X_train, y_train)\n\nprint (\"\\n\\n ---Logistic Regression Model---\")\nlr_auc = roc_auc_score(y_test, lr.predict(X_test))\nprint (\"Logistic Regression AUC = %2.2f\" % lr_auc)\n\nlr_accuracy = accuracy_score(y_test, lr.predict(X_test))\nprint (\"Logistic Regression  = %2.2f\" % lr_auc)\n\n#lr2 = lr.fit(X_train, y_train)\nprint(classification_report(y_test, lr.predict(X_test)))","31e77e15":"# lr_ = LogisticRegression()\n\n# # Fit the model to the Upsampling data\n# lr_u = lr.fit(X_train_u, y_train_u)\n\n# print (\"\\n\\n ---Logistic Regression Model---\")\n# lr_auc = roc_auc_score(y_test, lr.predict(X_test))\n# print (\"Logistic Regression AUC = %2.2f\" % lr_auc)\n\n# lr_accuracy = accuracy_score(y_test, lr.predict(X_test))\n# print (\"Logistic Regression  = %2.2f\" % lr_auc)\n\n# #lr2 = lr.fit(X_train_d, y_train_d)\n# print(classification_report(y_test, lr_u.predict(X_test)))","b07856c7":"# Crosss validation logictic regression: original data\nfrom sklearn.model_selection import cross_val_score\nlr_result = cross_val_score(lr, X_train, y_train, cv=5, scoring='f1')\nlr_result.mean()","a03c7544":"# lr_result = cross_val_score(lr, X_train_d, y_train_d, cv=5, scoring='f1')\n# lr_result.mean()","504515cc":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\n\n# Random Forest Model\nrf = RandomForestClassifier()\n\nrf_result = cross_val_score(rf, X_train, y_train, cv=5, scoring='f1')\n\nrf_result.mean()","a9b0fabf":"from sklearn.metrics import roc_auc_score\n\nrf = rf.fit(X_train, y_train)\n\nprint (\"\\n\\n ---Random Forest Model---\")\nrf_roc_auc = roc_auc_score(y_test, rf.predict(X_test))\nprint (\"Random Forest AUC = %2.2f\" % rf_roc_auc)\nprint(classification_report(y_test, rf.predict(X_test)))","da786b9a":"#import sklearn\n#print(sklearn.__version__)","a4ef42ae":"\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier()  \n\ngbc = gbc.fit(X_train, y_train)\n\n#gbc","de7df7b9":"# # original sample\n# gbc_orignalsample = GradientBoostingClassifier()  \n\n# gbc_orignalsample = gbc_orignalsample.fit(X_train, y_train)\n\n# gbc_orignalsample","bb9c3e8b":"gbc_result = cross_val_score(gbc, X_train, y_train, cv=5, scoring='f1')\ngbc_result.mean()","e7997be5":"# gbc_result = cross_val_score(gbc, X_train_d, y_train_d, cv=5, scoring='f1')\n# gbc_result.mean()","eef8f276":"# gbc_u = GradientBoostingClassifier()  \n\n# gbc_u = gbc_u.fit(X_train_u, y_train_u)\n\n# gbc_u","5f967302":"# gbc_result_u = cross_val_score(gbc, X_train_u, y_train_u, cv=5, scoring='f1')\n# gbc_result_u.mean()","879704d6":"from sklearn.metrics import roc_auc_score\n\nprint (\"\\n\\n ---Gradient Boosting Model---\")\ngbc_auc = roc_auc_score(y_test, gbc.predict(X_test))\nprint (\"Gradient Boosting Classifier AUC = %2.2f\" % gbc_auc)\nprint(classification_report(y_test, gbc.predict(X_test)))","d7ac6444":"# Create ROC Graph\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, lr.predict_proba(X_test)[:,1])\nrf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, rf.predict_proba(X_test)[:,1])\ngbc_fpr, gbc_tpr, gbc_thresholds = roc_curve(y_test, gbc.predict_proba(X_test)[:,1])\n\n\nplt.figure()\n\n# Plot Logistic Regression ROC\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % lr_auc)\n\n# Plot Random Forest ROC\nplt.plot(rf_fpr, rf_tpr, label='Random Forest Classifier (area = %0.2f)' % rf_roc_auc)\n\n# Plot Decision Tree ROC\nplt.plot(gbc_fpr, gbc_tpr, label='Gradient Boosting Classifier (area = %0.2f)' % gbc_auc)\n\n# Plot Base Rate ROC\nplt.plot([0,1], [0,1],label='Base Rate')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Graph')\nplt.legend(loc=\"lower right\")\nplt.show()","393d5eec":"# # logistic regression prediction\n# predictions = lr.predict(X_test_f)\n\n# output = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': predictions})\n# output.to_csv('my_submission_lr.csv', index=False)\n# print(\"Your submission was successfully saved!\")","d988d713":"# #Gradient Boost Classifier\n# predictions = gbc.predict(X_test_f)\n# output = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': predictions})\n# output.to_csv('my_submission_gbc.csv', index=False)\n# print(\"Your submission was successfully saved!\")","289e3177":"# ##Gradient Boost Classifier with original Sample\n# predictions = gbc_orignalsample.predict(X_test_f)\n# output = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': predictions})\n# output.to_csv('my_submission_gbcOS.csv', index=False)\n# print(\"Your submission was successfully saved!\")","7859722e":"# Random Forest: orignal data result saving\npredictions = rf.predict(test_new)\noutput = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission_rf_orignal.csv', index=False)\nprint(\"Your submission was successfully saved!\")","9ef58d26":"# gbu , no scaling\npredictions = gbc.predict(test_new)\noutput = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission_gbc.csv', index=False)\nprint(\"Your submission was successfully saved!\")","67b3434e":"# lr\npredictions = lr.predict(test_new)\noutput = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission_lru.csv', index=False)\nprint(\"Your submission was successfully saved!\")","41243a62":"# gbc_u\npredictions = gbc_u.predict(X_test_f)\noutput = pd.DataFrame({'PassengerId': test_df.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission_gbcu.csv', index=False)\nprint(\"Your submission was successfully saved!\")","fd4c821c":"## Passenger Id\n****\n- Delete passenger id as it does not major play significant role for ML.\n- I am also deleting 'Name and 'Ticket' feature for now.","b90cd568":"**Summary**\n***\n- females between age 10-40 has higher chance of survival as compared to men between the same age.","f899f48f":"###  Random Forest:Orignal data","9d6c380f":"## Load data","4cc4d479":"### Some helper functions","23e030ad":"### Predicting Age in train data ","92a925db":"Random forest gives best result so far","c4f12232":"#### Variable Description\n\n* Survived: Survived (1) or died (0)\n* Pclass: Passenger's class\n* Name: Passenger's name\n* Sex: Passenger's sex\n* Age: Passenger's age\n* SibSp: Number of siblings\/spouses aboard\n* Parch: Number of parents\/children aboard\n* Ticket: Ticket number\n* Fare: Fare\n* Cabin: Cabin\n* Embarked: Port of embarkation","51452237":"Handling Cabin Feature","730c333f":"# How to Treat Imbalanced Datasets\n\nThere are many ways of dealing with imbalanced data. We will focus in the following approaches:\n\n1. Oversampling\u200a\u2014\u200aSMOTE\n2. Undersampling\u200a\u2014\u200aRandomUnderSampler","472d7464":"### Predciting Age in test data","3e0078af":"### Univariate Analysis","b72aba05":"### Handling Age feature","94112ebf":"All the cabin names start with an English alphabet following by multiple digits. It seems like there are some passengers that had booked multiple cabin rooms in their name. This is because many of them travelled with family. However, they all seem to book under the same letter followed by different numbers. It seems like there is a significance with the letters rather than the numbers. Therefore, we can group these cabins according to the letter of the cabin name.","e1bbc418":"## Fare group feature \n***\nCreate a new feature from Fare where the fare is low high medium.","3c20bb85":"Now, these means can help us determine the unknown cabins, if we compare each unknown cabin rows with the given mean's above. Let's write a simple function so that we can give cabin names based on the means.","a3b7db4e":"### Bivariate Analysis","588a5d09":"## family_size\n***Creating a new feature\"family_size\".***","0e0522c9":"# Split Train\/Test Set","b61661cb":"## Random Forest regressor for Age feature\n***\nKNN is an algorithm that is useful for matching a point with its closest k neighbors in a multi-dimensional space. It can be used for data that are continuous, discrete, ordinal and categorical which makes it particularly useful for dealing with all kind of missing data.\nThe assumption behind using KNN for missing values is that a point value can be approximated by the values of the points that are closest to it, based on other variables.","61ba75fe":"**Summary**\n***\n- It looks like ...\n    - ~ 63% first class passenger survived titanic tragedy\n    - ~ 48% second class and \n    - ~ only  24% third class passenger survived.","f3148b6c":"Distribution plot shows that only 38% of passesnger had survived from that catastrophic incident.","2591cc5b":"###  Gradient Boosting:Orignal data","b3a74814":"# Titanic Survival Prediction","f30128da":"Take the average of Fare where Pclass =3, Sex = male, Cabin = B and Embarked = S","bc74189d":"Handling Fare feature from test data","39151ef2":"**Summary**\n****\n- Parch 3 category has  ~ 60% survival rate.","348b9f3b":"# Exploratroty Analysis","7a56b236":"## Exploratory Data Analysis","8b2dea08":"### Data Understanding","32027b73":"# Resample Methods\n\nLet's train a base logistic regression model on the three types of samples to see which yields the best result:\n1. **Orginal Sample**\n2. **Upsampling Data**\n3. **Downsampling Data**","21cd7a80":"## Feature Scaling\n***\nWhy and Where to Apply Feature Scaling?\nReal world dataset contains features that highly vary in magnitudes, units, and range. Normalisation should be performed when the scale of a feature is irrelevant or misleading and not should Normalise when the scale is meaningful.\n\nThe algorithms which use **Euclidean Distance measure are sensitive to Magnitudes.** Here feature scaling helps to weigh all the features equally.\n\nFormally, If a feature in the dataset is big in scale compared to others then in algorithms where Euclidean distance is measured this big scaled feature becomes dominating and needs to be normalized.","36897e06":"In both Training and testing data, avergae Fare close to 80$ is for 'C' Embarked. So filling two NAN with C would be reasonable.","64c7a0c5":"## Distribution of Target variable","fc0ebdf8":"## Feature Engineering\n****\nFeature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy on unseen data.","6922cd74":"The correlation  shows\n****\n- Sex is most prominent feature for predictig survival rate.","abbc96eb":"## Age feature","2f579ff7":"Lets create anoter feature based on family size where passenger is travelling with small or big familt or alone.","de667434":"#### Categorical variables","dea12374":"**Summary**\n***\n- female passengers have higher survival rate  at a much better rate than male passengers. \n- It seems about right since females and children were the priority.","c661041b":"#### Positive Correlation Features:\n- Fare and Survived: 0.26\n\n#### Negative Correlation Features:\n- Fare and Pclass: -0.6\n- Sex and Survived: -0.55\n- Pclass and Survived: -0.33\n\n\n**So, Let's analyze these correlations a bit.** We have found some moderately strong relationships between different features. There is a definite positive correlation between Fare and Survived rated. This relationship reveals that the passenger who paid more money for their ticket were more likely to survive. This theory aligns with one other correlation which is the correlation between Fare and Pclass(-0.6). This relationship can be explained by saying that first class passenger(1) paid more for fare then second class passenger(2), similarly second class passenger paid more than the third class passenger(3). This theory can also be supported by mentioning another Pclass correlation with our dependent variable, Survived. The correlation between Pclass and Survived is -0.33. This can also be explained by saying that first class passenger had a better chance of surviving than the second or the third and so on.\n\nHowever, the most significant correlation with our dependent variable is the Sex variable, which is the info on whether the passenger was male or female. This negative correlation with a magnitude of -0.54 which points towards some undeniable insights. Let's do some statistics to see how statistically significant this correlation is.","4c3e876f":"# Pre-Processing","e78516b2":"### Without feature scaling","0d7bc533":"### Multi-variate Analysis","7d0fa779":"Let see how Fare is distributed between Pclass and Embarked feature value","e3750743":"# Test Logistic Regression Performance: Without featue scaling\n### Logistic Regression F1 Score (): Original Data","30a5b9f7":"This plot shows something impressive..\n- The spike in the plot under 100 dollar represents that a lot of passengers who bought the ticket within that range did not survive. \n- When fare is approximately more than 280 dollars, there is no gray shade which means, either everyone passed that fare point survived or maybe there is an outlier that clouds our judgment.","d4ae8b49":"# Acknowledgments\n\nMojorty  of motivation are drwan from the follwoing notebooks\/online content :\n\n1. https:\/\/www.kaggle.com\/arthurtok\/0-808-with-simple-stacking\n2. https:\/\/www.kaggle.com\/usharengaraju\/data-visualization-titanic-survival\n3. https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python\n4. https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n5. https:\/\/www.kaggle.com\/helgejo\/an-interactive-data-science-tutorial#2.-Data-Understanding\n6. https:\/\/towardsdatascience.com\/6-different-ways-to-compensate-for-missing-values-data-imputation-with-examples-6022d9ca0779\n7. https:\/\/towardsdatascience.com\/the-use-of-knn-for-missing-values-cf33d935c637","b08b3132":"# Feature Encoding","8f839df7":"# Combine data","727a93ea":"Lets try KNN imputer for filling missing Age values"}}