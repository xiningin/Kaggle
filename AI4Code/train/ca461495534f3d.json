{"cell_type":{"882fa191":"code","ca43ac6a":"code","9501b081":"code","4b765876":"code","fa03621b":"code","c83f9169":"code","e17c186f":"code","5373a4f9":"code","13644a1b":"code","b29a50d5":"code","64c78581":"code","4c085cbf":"code","6edca877":"code","83f06fa5":"markdown"},"source":{"882fa191":"!pip install git+https:\/\/github.com\/fastai\/fastai2 \n!pip install git+https:\/\/github.com\/fastai\/fastcore","ca43ac6a":"import fastai2\nfrom fastai2.vision.all import *\nfrom sklearn.metrics import recall_score\nprint(fastai2.__version__)","9501b081":"# Configs\nsz = 128\nbs = 128\nnfolds = 5\nfold = 0\ntrain_path = Path('\/kaggle\/input\/grapheme-imgs-128x128')\ncsv_file = Path('\/kaggle\/input\/iterative-stratification\/train_with_fold.csv')\narch = xresnet50","4b765876":"# Load dataframe\ndf = pd.read_csv(csv_file)\ndf.drop(columns=['id'], inplace=True)\ndf.head()","fa03621b":"dblock = DataBlock(\n  blocks=(ImageBlock(cls=PILImageBW), *(3*[CategoryBlock])),      # one image input and three categorical outputs\n  getters=[ColReader('image_id', pref=train_path, suff='.png'),   # image input\n           ColReader('grapheme_root'),                            # label 1\n           ColReader('vowel_diacritic'),                          # label 2\n           ColReader('consonant_diacritic')],                     # label 3\n  splitter=IndexSplitter(df.loc[df.fold==fold].index),            # train\/validation split\n  batch_tfms=[Normalize.from_stats([0.0692], [0.2051]),           # Normalize the images with the specified mean and standard deviation\n              *aug_transforms(do_flip=False, size=sz)])           # Add default transformations except for horizontal flip      \ndls = dblock.dataloaders(df, bs=bs)                               # Create the dataloaders\ndls.n_inp = 1                                                     # Set the number of inputs","c83f9169":"# Show an example\ndls.show_batch()","e17c186f":"# Model \nclass Head(Module):\n    def __init__(self, nc, n, ps=0.5):\n        self.fc = nn.Sequential(*[AdaptiveConcatPool2d(), nn.ReLU(inplace=True), Flatten(),\n             LinBnDrop(nc*2, 512, True, ps, nn.ReLU(inplace=True)),\n             LinBnDrop(512, n, True, ps)])\n        self._init_weight()\n        \n    def _init_weight(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                torch.nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1.0)\n                m.bias.data.zero_()\n        \n    def forward(self, x):\n        return self.fc(x)\n\nclass BengaliModel(Module):\n    def __init__(self, arch=arch, n=dls.c, pre=True):\n        m = arch(pre)\n        m = nn.Sequential(*children_and_parameters(m)[:-4])\n        conv = nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False)\n        w = (m[0][0].weight.sum(1)).unsqueeze(1)\n        conv.weight = nn.Parameter(w)\n        m[0][0] = conv\n        nc = m(torch.zeros(2, 1, sz, sz)).detach().shape[1]\n        self.body = m\n        self.heads = nn.ModuleList([Head(nc, c) for c in n])\n        \n    def forward(self, x):    \n        x = self.body(x)\n        return [f(x) for f in self.heads]","5373a4f9":"# Loss function\nclass Loss_combine(Module):\n    def __init__(self, func=F.cross_entropy, weights=[2, 1, 1]):\n        self.func, self.w = func, weights\n\n    def forward(self, xs, *ys):\n        for i, w, x, y in zip(range(len(xs)), self.w, xs, ys):\n            if i == 0: loss = w*self.func(x, y) \n            else: loss += w*self.func(x, y) \n        return loss","13644a1b":"# Metrics\nclass RecallPartial(Metric):\n    # based on AccumMetric\n    \"Stores predictions and targets on CPU in accumulate to perform final calculations with `func`.\"\n    def __init__(self, a=0, **kwargs):\n        self.func = partial(recall_score, average='macro', zero_division=0)\n        self.a = a\n\n    def reset(self): self.targs,self.preds = [],[]\n\n    def accumulate(self, learn):\n        pred = learn.pred[self.a].argmax(dim=-1)\n        targ = learn.y[self.a]\n        pred,targ = to_detach(pred),to_detach(targ)\n        pred,targ = flatten_check(pred,targ)\n        self.preds.append(pred)\n        self.targs.append(targ)\n\n    @property\n    def value(self):\n        if len(self.preds) == 0: return\n        preds,targs = torch.cat(self.preds),torch.cat(self.targs)\n        return self.func(targs, preds)\n\n    @property\n    def name(self): return df.columns[self.a+1]\n    \nclass RecallCombine(Metric):\n    def accumulate(self, learn):\n        scores = [learn.metrics[i].value for i in range(3)]\n        self.combine = np.average(scores, weights=[2,1,1])\n\n    @property\n    def value(self):\n        return self.combine","b29a50d5":"# Create learner\nlearn = Learner(dls, BengaliModel(), loss_func=Loss_combine(), cbs=CSVLogger(),\n                metrics=[RecallPartial(a=i) for i in range(len(dls.c))] + [RecallCombine()],\n                splitter=lambda m: [list(m.body.parameters()), list(m.heads.parameters())])","64c78581":"learn.fit_one_cycle(12, lr_max=slice(1e-3, 1e-2))","4c085cbf":"learn.recorder.plot_loss()","6edca877":"learn.save('model') ","83f06fa5":"# Fastai2 starter\nI decided to start working with fastai2, after a few hours of searching the notebooks and documentation I finnaly got to this point. I figured this may be helpfull to other people wanting to learn about the new fastai version so I'm sharing this notebook. Let me know if you find anything that can be improved, I'm just getting started with fastai2!\n\nSome references:\n* https:\/\/www.kaggle.com\/yiheng\/iterative-stratification\n* https:\/\/www.kaggle.com\/iafoss\/image-preprocessing-128x128\n* https:\/\/www.kaggle.com\/iafoss\/grapheme-fast-ai-starter-lb-0-964\n* https:\/\/github.com\/fastai\/fastai2\n* http:\/\/dev.fast.ai\/"}}