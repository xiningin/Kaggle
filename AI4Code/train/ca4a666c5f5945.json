{"cell_type":{"871c8761":"code","714bbd9c":"code","96f72c03":"code","36456739":"code","ecd56e99":"code","2d03821f":"code","ccd1e205":"code","7286a48d":"code","07433cf1":"code","d401d0a6":"code","4ce6080b":"code","43cc7ba7":"code","d9de1f04":"code","89c41bef":"code","80ad937d":"code","fa989bee":"code","2c2d00f7":"code","bb45e0fa":"code","561f8c11":"code","e3fe8c4f":"code","e545b421":"code","f832c85d":"code","1d6db999":"code","19558eec":"code","070fed4e":"code","61254063":"code","4a6a6b8d":"code","761fc8c4":"code","48bd875b":"code","b27cc31c":"code","b5fe860e":"code","b2545782":"code","c8c704fc":"code","d5ffe6bb":"code","a051d7ed":"code","86cb9a7d":"code","a9ce518e":"code","29eb70ef":"code","470466f4":"code","a7d002cf":"code","f49779be":"code","986371a4":"code","e21693bc":"code","478fbd1c":"code","43805d1e":"code","fae8365d":"code","ecdbeb5e":"code","fc34d309":"code","6f24cb71":"code","a91e12bd":"code","1c4ee33c":"code","81e7249f":"code","7218dae4":"code","6dbe23db":"code","ce0dc9d9":"code","a214f7ba":"code","bd50086b":"code","706e8bfc":"code","893f2a3d":"code","7f064875":"code","4ab3ef82":"code","9a6691f6":"code","3e88bb2b":"code","a3496775":"code","2405fc5a":"code","9490aa38":"code","05b48bcb":"code","9754402c":"code","8992027f":"code","0b528722":"code","7939b67c":"code","0d352fab":"code","8131245e":"code","86383f6d":"code","c28e82d8":"code","3a9b040a":"code","eb5becb6":"code","7293734d":"code","fcad4ad4":"code","79f23a95":"code","f6d89d78":"code","b367967c":"code","0f819a10":"code","6f280994":"code","b8bc5fb4":"code","cafaf38d":"code","4fd40e73":"code","f8829ea3":"code","4761fbcf":"code","e643eb99":"code","b1e5b99c":"code","bfe81d12":"markdown","3b372f5b":"markdown","9e025b57":"markdown","f2c8e83f":"markdown","3ac51ce1":"markdown","889eb851":"markdown","f125d3c0":"markdown","e0ce63ec":"markdown","451808c7":"markdown","985aab59":"markdown","39c4954a":"markdown","0bdefef6":"markdown","2d96dc56":"markdown","437964bb":"markdown","94fd4b87":"markdown","af006e4f":"markdown","5bc54a0f":"markdown","741a2ee4":"markdown","dc106784":"markdown","1ecbbd8f":"markdown","6db4bf79":"markdown","add53247":"markdown","51363bfc":"markdown","c294e3ff":"markdown","ffaf1df2":"markdown","523941ac":"markdown","c8a86188":"markdown","52f39c09":"markdown","d244e11b":"markdown","65990dc4":"markdown","10ee9fc5":"markdown","a24fdb0b":"markdown","db873462":"markdown","8da74697":"markdown","63423b42":"markdown","f7dd2a33":"markdown","404c8f97":"markdown","a2f83305":"markdown","74f322ec":"markdown","c21cfd4b":"markdown","563b9b58":"markdown","e8c18545":"markdown","427730ea":"markdown","83300dec":"markdown","819b4638":"markdown","c878a53d":"markdown","d469cc1f":"markdown","5cc54182":"markdown","1d06560a":"markdown","9719ee64":"markdown","cddbb9ff":"markdown","452936f8":"markdown","b60d2a25":"markdown","10173ad3":"markdown","9ef9f580":"markdown","6c46cc62":"markdown","48b3c8b2":"markdown","a9a9d561":"markdown","71eb9016":"markdown"},"source":{"871c8761":"\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport pandas as pd\nimport math\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom time import time\n\nimport pydot\nfrom IPython.display import Image\nfrom sklearn.externals.six import StringIO\n\n\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import roc_curve\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom mlxtend.classifier import EnsembleVoteClassifier\nfrom sklearn.ensemble import VotingClassifier, BaggingClassifier, \\\n    AdaBoostClassifier, GradientBoostingClassifier\nimport warnings\nimport xgboost as xgb\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom numpy import where\nfrom mpl_toolkits.mplot3d import axes3d\n\n","714bbd9c":"ap_train = pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\nap_test = pd.read_csv('..\/input\/home-credit-default-risk\/application_test.csv')\n# print(ap_train.shape, '_', ap_test.shape)\n\n#final data set for model\nap_trainf=ap_train.copy()","96f72c03":"# Presentation\n# ap_trainf.iloc[:,[0,1,4,40,41,42,55,65,95,96,119]].info()","36456739":"# 1\nplt.figure(figsize = (8,24))\nsns.heatmap(pd.DataFrame(ap_train.isnull().sum()\/ap_train.shape[0]*100),annot=True,\n            cmap=sns.color_palette(\"Blues_d\",10),linewidth=1,linecolor=\"white\")\nplt.title(\"application nulls\")\n\nplt.subplots_adjust(wspace = 1.6)","ecd56e99":"# 2\napNum=ap_train.iloc[:,5:41].select_dtypes(include=['int64','float64'])\nhist = apNum.hist(bins=10,figsize=(20,15))","2d03821f":"ap_trainf.drop(columns=['FLAG_CONT_MOBILE','FLAG_MOBIL'],inplace=True)\nap_test.drop(columns=['FLAG_CONT_MOBILE','FLAG_MOBIL'],inplace=True)","ccd1e205":"# 1 \nap_trainf['DAYS_EMPLOYED'].loc[ap_trainf['DAYS_EMPLOYED']==365243] = np.where((ap_trainf['NAME_INCOME_TYPE'].loc[ap_trainf['DAYS_EMPLOYED']==365243] == 'Pensioner'), -4500, 0)\nap_test['DAYS_EMPLOYED'].loc[ap_test['DAYS_EMPLOYED']==365243] = np.where((ap_test['NAME_INCOME_TYPE'].loc[ap_test['DAYS_EMPLOYED']==365243] == 'Pensioner'), -4500, 0)\n# ap_trainf['DAYS_EMPLOYED'].hist(bins=100)\n# 2\nap_trainf['inc_ratio'] = round(ap_trainf['AMT_ANNUITY'] \/ ap_trainf['AMT_INCOME_TOTAL'],2) \nap_test['inc_ratio'] = round(ap_test['AMT_ANNUITY'] \/ ap_test['AMT_INCOME_TOTAL'],2) \n# 3\nap_trainf['days_emp_ratio']=ap_trainf['DAYS_EMPLOYED'].div(ap_trainf['DAYS_BIRTH'])\nap_test['days_emp_ratio']=ap_test['DAYS_EMPLOYED'].div(ap_test['DAYS_BIRTH'])\n\nap_trainf['rtv_ratio'] = ap_trainf['AMT_ANNUITY'] \/ ap_trainf['AMT_GOODS_PRICE']\nap_test['rtv_ratio'] = ap_test['AMT_ANNUITY'] \/ ap_test['AMT_GOODS_PRICE']\n\nap_trainf['ltv_ratio'] = ap_trainf['AMT_CREDIT'] \/ ap_trainf['AMT_GOODS_PRICE']\nap_test['ltv_ratio'] = ap_test['AMT_CREDIT'] \/ ap_test['AMT_GOODS_PRICE']\n","7286a48d":"plt.figure(figsize = (12, 5))\n# iterate through the new features\nfor i, feature in enumerate(['rtv_ratio','ltv_ratio']):\n    \n    # create a new subplot for each source\n    plt.subplot(1, 2, i + 1)\n    # plot repaid loans\n    sns.kdeplot(ap_trainf.loc[ap_trainf['TARGET'] == 0, feature], label = 'target == 0')\n    # plot loans that were not repaid\n    sns.kdeplot(ap_trainf.loc[ap_trainf['TARGET'] == 1, feature], label = 'target == 1')\n    \n    # Label the plots\n    plt.title('Distribution of %s by Target Value' % feature)\n    plt.xlabel('%s' % feature); plt.ylabel('Density');\n    \nplt.tight_layout(h_pad = 2.5)","07433cf1":"ap_trainf['AMT_INCOME_TOTAL'].loc[ap_trainf['AMT_INCOME_TOTAL']>800000]=800000\nlog_income = np.log1p(ap_trainf.AMT_INCOME_TOTAL)\nap_trainf['AMT_log_INCOME_TOTAL'] = log_income\nap_trainf.drop(columns=['AMT_INCOME_TOTAL'],inplace=True)\n\n\nap_test['AMT_INCOME_TOTAL'].loc[ap_test['AMT_INCOME_TOTAL']>800000]=800000\nlog_income = np.log1p(ap_test.AMT_INCOME_TOTAL)\nap_test['AMT_log_INCOME_TOTAL'] = log_income\nap_test.drop(columns=['AMT_INCOME_TOTAL'],inplace=True)\n# ap_trainf.AMT_log_INCOME_TOTAL.hist(bins=100)","d401d0a6":"#  Train\nap_trainf['OCCUPATION_TYPE'].fillna(('Gen'), inplace=True)\nap_trainf['OCCUPATION_TYPE'] = ap_trainf['NAME_INCOME_TYPE'] + '_' + ap_trainf['OCCUPATION_TYPE']\nap_trainf.drop(columns=['NAME_INCOME_TYPE'],inplace=True)\n\n# Test\nap_test['OCCUPATION_TYPE'].fillna(('Gen'), inplace=True)\nap_test['OCCUPATION_TYPE'] = ap_test['NAME_INCOME_TYPE'] + '_' + ap_test['OCCUPATION_TYPE']\nap_test.drop(columns=['NAME_INCOME_TYPE'],inplace=True)","4ce6080b":"# converting occupation type to default rate\nocc_t = pd.DataFrame(ap_trainf.loc[:,['OCCUPATION_TYPE','TARGET']].groupby('OCCUPATION_TYPE')['TARGET'].agg(Mean='mean', Count='count'))\nap_trainf['OCCUPATION_TYPE'] = ap_trainf['OCCUPATION_TYPE'].map(occ_t['Mean'])\n# Test\nap_test['OCCUPATION_TYPE'] = ap_test['OCCUPATION_TYPE'].map(occ_t['Mean'])","43cc7ba7":"# 1 \nap_trainf.OWN_CAR_AGE.loc[(ap_trainf.OWN_CAR_AGE.isnull()) & (ap_trainf.FLAG_OWN_CAR == 'Y')] = float(ap_trainf.OWN_CAR_AGE.loc[ap_trainf.OWN_CAR_AGE.notnull()].mode())\n# 2\nap_trainf.OWN_CAR_AGE.loc[ap_trainf.OWN_CAR_AGE.isnull()] = (-1)\nap_trainf['car'] = pd.cut(x=ap_trainf['OWN_CAR_AGE'], bins=[-2,-0.1,8,11,100],labels=['no' ,'new','reg','old'])\n# ap_trainf.groupby('car')['TARGET'].agg(['mean','count'])\n# 3\nap_trainf = pd.get_dummies(data=ap_trainf, columns=['car'],drop_first=True)\n# 4\nap_trainf = ap_trainf.drop(columns=['FLAG_OWN_CAR','OWN_CAR_AGE'])\n\n","d9de1f04":"# 1 \nap_test.OWN_CAR_AGE.loc[(ap_test.OWN_CAR_AGE.isnull()) & (ap_test.FLAG_OWN_CAR == 'Y')] = float(ap_test.OWN_CAR_AGE.loc[ap_test.OWN_CAR_AGE.notnull()].mode())\n# 2\nap_test.OWN_CAR_AGE.loc[ap_test.OWN_CAR_AGE.isnull()] = (-1)\nap_test['car'] = pd.cut(x=ap_test['OWN_CAR_AGE'], bins=[-2,-0.1,8,11,100],labels=['no' ,'new','reg','old'])\n# ap_trainf.groupby('car')['TARGET'].agg(['mean','count'])\n# 3\nap_test = pd.get_dummies(data=ap_test, columns=['car'],drop_first=True)\n# 4\nap_test = ap_test.drop(columns=['FLAG_OWN_CAR','OWN_CAR_AGE'])","89c41bef":"#Spliting to work only with left part of th DF that includes clientes categorical data\napL=ap_trainf.iloc[:,0:44]\napLob=apL.select_dtypes(include='object')\napLob.fillna(value='NA',inplace=True)\napLob.head(5)","80ad937d":"apLob.drop(['NAME_CONTRACT_TYPE','CODE_GENDER','FLAG_OWN_REALTY'],axis=1,inplace=True)\nocn=apLob.columns\napLob['TARGET']=ap_train['TARGET']\nobject_to_probability_dic={}\nfor CN in ocn:\n   temp_PV=apLob.pivot_table(index=CN, values='TARGET', aggfunc={'TARGET': [sum, len]})\n   temp_PV['Ratio of 1']=temp_PV['sum']\/temp_PV['len']\n   temp_PV.sort_values(['Ratio of 1'],inplace=True)\n   temp_PV_sort=temp_PV.reindex()\n   temp=list(range(len(temp_PV_sort.index)))\n   #print(temp)\n   object_to_probability_dic[CN] = dict(zip(temp_PV.index, temp))\nprint(object_to_probability_dic)","fa989bee":"for CN in ocn:\n    ap_trainf.loc[:, CN] = ap_trainf[CN].replace(object_to_probability_dic[CN])\n    ap_test.loc[:, CN] = ap_test[CN].replace(object_to_probability_dic[CN])\n# ap_trainf.loc[:, ocn].head()","2c2d00f7":" fig = plt.figure(figsize=(18,6))\n\n ax1 = fig.add_subplot(131)\n ax2 = fig.add_subplot(132)\n ax3 = fig.add_subplot(133)\n ap_train.plot('EXT_SOURCE_1', 'EXT_SOURCE_2', kind='scatter',ax=ax1,\n           xlim=[0, 1], ylim=[0, 1],\n           c=where(ap_train.TARGET==1, 'blue', 'pink'),\n           s=1)\n ap_train.plot('EXT_SOURCE_1', 'EXT_SOURCE_3', kind='scatter',ax=ax2,\n           xlim=[0, 1], ylim=[0, 1],\n           c=where(ap_train.TARGET==1, 'blue', 'pink'),\n           s=1)\n ap_train.plot('EXT_SOURCE_3', 'EXT_SOURCE_2', kind='scatter',ax=ax3,\n           xlim=[0, 1], ylim=[0, 1],\n           c=where(ap_train.TARGET==1, 'blue', 'pink'),\n           s=1)","bb45e0fa":"def ext_source_mul31_2(X):\n  Ext_source=X.loc[:,['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']]\n  Ext_source['EXT_SOURCE_7']=Ext_source['EXT_SOURCE_3'].mul(Ext_source['EXT_SOURCE_1'])\n  #incase NA at 1 or 3 use 3^2 if 3 NA use 1^2 if 1 and 3 NA use 2^2\n  Ext_source['EXT_SOURCE_7'].loc[Ext_source['EXT_SOURCE_7'].isna()]=Ext_source['EXT_SOURCE_3'].pow(2)\n  Ext_source['EXT_SOURCE_7'].loc[Ext_source['EXT_SOURCE_7'].isna()]=Ext_source['EXT_SOURCE_1'].pow(2)\n  Ext_source['EXT_SOURCE_7'].loc[Ext_source['EXT_SOURCE_7'].isna()]=Ext_source['EXT_SOURCE_2'].pow(2)\n  #This features is very cruel to midle score customeres, thire for score will be mul by 4, and squared\n  #Ext_source['EXT_SOURCE_7']=Ext_source['EXT_SOURCE_7'].mul(4).pow(0.5)\n  return Ext_source['EXT_SOURCE_7']","561f8c11":"#Sigmoid function with gaine of g - at x=p sigmoid=0.5\ndef sigmoid(x,g,p):\n  return 1\/(1 + math.exp(-g*(-p+x)))\n\n\ndef ext_source_feature(X, g=6,p=0.5):\n  Ext_source=X.loc[:,['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']]\n  Ext_source['EXT_SOURCE_4']=Ext_source.mean(axis=1, skipna=True)\n  Ext_source['EXT_SOURCE_5']=Ext_source.max(axis=1, skipna=True)\n  Ext_source.loc[Ext_source['EXT_SOURCE_4'] < p,'EXT_SOURCE_6'] = Ext_source['EXT_SOURCE_4']\n  Ext_source.loc[Ext_source['EXT_SOURCE_4'] >= p,'EXT_SOURCE_6'] = Ext_source['EXT_SOURCE_5']\n  return Ext_source['EXT_SOURCE_6'].apply(sigmoid,args=(g,p))\n","e3fe8c4f":"def ext_source_312(X):\n  Ext_source=X.loc[:,['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']]\n  Ext_source['EXT_SOURCE_7']=Ext_source['EXT_SOURCE_3']\n  Ext_source['EXT_SOURCE_7'].loc[Ext_source['EXT_SOURCE_3'].isna()]=Ext_source['EXT_SOURCE_1']\n  Ext_source['EXT_SOURCE_7'].loc[Ext_source['EXT_SOURCE_7'].isna()]=Ext_source['EXT_SOURCE_2']\n  return Ext_source['EXT_SOURCE_7']","e545b421":"f_source = FunctionTransformer(ext_source_feature, validate=False, kw_args={'g': 6, 'p': 0.25})\n\nf312_source = FunctionTransformer(ext_source_312, validate=False)\n\nmul31_2_source = FunctionTransformer(ext_source_mul31_2, validate=False)\n","f832c85d":"\nap_trainf['EXT_SOURCE_f1']=f_source.transform(ap_train)\nap_trainf['EXT_SOURCE_312']=f312_source.transform(ap_train)\n#ap_trainf['EXT_SOURCE_mul31_2']=mul31_2_source.transform(ap_train)\nExt_source=ap_train[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']]\nap_trainf['EXT_SOURCE_min']=Ext_source.min(axis=1, skipna=True)\nap_trainf['EXT_SOURCE_max']=Ext_source.max(axis=1, skipna=True)\n#We should consider droping this very sagnificant features\n#ap_trainf.drop(columns=['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3'],inplace=True)\n\n# Test\nap_test['EXT_SOURCE_f1']=f_source.transform(ap_test)\nap_test['EXT_SOURCE_312']=f312_source.transform(ap_test)\n#ap_test['EXT_SOURCE_mul31_2']=mul31_2_source.transform(ap_test)\nExt_source=ap_test[['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']]\nap_test['EXT_SOURCE_min']=Ext_source.min(axis=1, skipna=True)\nap_test['EXT_SOURCE_max']=Ext_source.max(axis=1, skipna=True)\n\n#ap_test.drop(columns=['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3'],inplace=True)","1d6db999":"fig = plt.figure(figsize=(12,6))\n\nax1 = fig.add_subplot(121)\nax2 = fig.add_subplot(122)\n\nap_train.plot('EXT_SOURCE_3', 'EXT_SOURCE_2', kind='scatter',ax=ax1,\n           xlim=[0, 1], ylim=[0, 1],\n           c=where(ap_train.TARGET==1, 'navy', 'moccasin'),\n           s=1)\n\nap_trainf.plot('EXT_SOURCE_f1', 'EXT_SOURCE_312', kind='scatter',ax=ax2,\n          xlim=[0, 1], ylim=[0, 1],          \n          c=where(ap_trainf.TARGET==1, 'navy', 'moccasin'),\n          s=1)","19558eec":"# 1\nap_trainf['BUR_DAY'] = pd.cut(x=ap_trainf['AMT_REQ_CREDIT_BUREAU_DAY'], bins=[-1,0,10],labels=[0 ,1])\nap_trainf['BUR_MO'] = pd.cut(x=ap_trainf['AMT_REQ_CREDIT_BUREAU_MON'], bins=[-1,2,30],labels=[0 ,1])\n# 2\nap_trainf['BUR_null'] = ap_trainf['AMT_REQ_CREDIT_BUREAU_WEEK']*0 + 1\nap_trainf.loc[:,['BUR_DAY','BUR_MO','BUR_null']] = ap_trainf.loc[:,['BUR_DAY','BUR_MO','BUR_null']].fillna(0)\n# 3\nap_trainf = ap_trainf.drop(columns=[*ap_trainf.loc[:,ap_trainf.columns.str.contains(r'BUREAU')].columns])\nap_trainf[['BUR_DAY','BUR_MO']]=ap_trainf[['BUR_DAY','BUR_MO']].astype('int32')\n# ap_trainf.loc[:,ap_trainf.columns.str.contains(r'BUR')].info()\n\n# Test\nap_test['BUR_DAY'] = pd.cut(x=ap_test['AMT_REQ_CREDIT_BUREAU_DAY'], bins=[-1,0,10],labels=[0 ,1])\nap_test['BUR_MO'] = pd.cut(x=ap_test['AMT_REQ_CREDIT_BUREAU_MON'], bins=[-1,2,30],labels=[0 ,1])\nap_test['BUR_null'] = ap_test['AMT_REQ_CREDIT_BUREAU_WEEK']*0 + 1\nap_test.loc[:,['BUR_DAY','BUR_MO','BUR_null']] = ap_test.loc[:,['BUR_DAY','BUR_MO','BUR_null']].fillna(0)\nap_test = ap_test.drop(columns=[*ap_test.loc[:,ap_test.columns.str.contains(r'BUREAU')].columns])\nap_test[['BUR_DAY','BUR_MO']]=ap_test[['BUR_DAY','BUR_MO']].astype('int32')","070fed4e":"doc_col = list(ap_trainf.loc[:,ap_trainf.columns.str.contains(r'FLAG_DOCUMENT')])\nap_trainf['doc_list'] = ap_trainf.loc[:,doc_col].eq(1).dot(ap_trainf.loc[:,doc_col].columns + ',').str.rstrip(',').str.split(',')\nap_trainf['doc_list'] = ap_trainf['doc_list'].str.join('')\n# ap_trainf['doc_list'].head(10)","61254063":"# function assigns group mean as value to all group:\ndef group_mean (df,column,exp_list):\n    for exp in exp_list:\n        df[column].loc[df.index.str.contains(exp)] = df[column].loc[df.index.str.contains(exp)].mean() \n","4a6a6b8d":"# 2 Finding the default rate for each combination.\ndocs_data = pd.DataFrame(ap_trainf.loc[:,[*doc_col,'doc_list','TARGET']].groupby('doc_list')['TARGET'].agg(Mean='mean', Count='count'))\ndocs_data['Mean'] = round(docs_data['Mean'],4)\n# 3\ngroup_mean(docs_data,'Mean',[r'_8.*_(?!.*14)',r'_9(?!.*14)',r'_6(?!.*14)',r'_14|_4'])\n# 4\ndocs_data['Mean'].loc[(docs_data['Count'] < 30) & (docs_data['Mean'] < 0.01)] = 0.08\n# 5\nap_trainf['docs_val'] = ap_trainf['doc_list'].map(docs_data['Mean'])\n# ap_trainf.loc[:,['TARGET','docs_val','doc_list']].head(15)\nap_trainf.drop(columns=[*doc_col,'doc_list'],inplace=True)\n","761fc8c4":"# Test\nap_test['doc_list'] = ap_test.loc[:,doc_col].eq(1).dot(ap_test.loc[:,doc_col].columns + ',').str.rstrip(',').str.split(',')\nap_test['doc_list'] = ap_test['doc_list'].str.join('')\nap_test['docs_val'] = ap_test['doc_list'].map(docs_data['Mean'])\nap_test.drop(columns=[*doc_col,'doc_list'],inplace=True)","48bd875b":"apC=ap_train.iloc[:,44:91]\napC_col_Name=apC.columns\napC_columns=apC.columns\napC['TARGET']=ap_train['TARGET']\napC.info()","b27cc31c":"fig = plt.figure(figsize=(18,6))\n\nax1 = fig.add_subplot(131)\nax2 = fig.add_subplot(132)\nax3 = fig.add_subplot(133)\napC.plot('APARTMENTS_AVG', 'APARTMENTS_MODE', kind='scatter',ax=ax1,\n          xlim=[0, 1], ylim=[0, 1],\n          c=where(apC.TARGET==1, 'blue', 'pink'),\n          s=1)\napC.plot('APARTMENTS_MODE', 'APARTMENTS_MEDI', kind='scatter',ax=ax2,\n          xlim=[0, 1], ylim=[0, 1],\n          c=where(apC.TARGET==1, 'blue', 'pink'),\n          s=1)\napC.plot('APARTMENTS_MEDI', 'APARTMENTS_AVG', kind='scatter',ax=ax3,\n          xlim=[0, 1], ylim=[0, 1],\n          c=where(apC.TARGET==1, 'blue', 'pink'),\n          s=1)","b5fe860e":"apC_mode_list=[x for x in apC_columns if \"MODE\" in x] # list of the Mode columns\napC_mode=apC[apC_mode_list]\n#Droping object type fitcheres working at this stage with numerical values\napC_mode.drop(labels=['FONDKAPREMONT_MODE','HOUSETYPE_MODE','WALLSMATERIAL_MODE','EMERGENCYSTATE_MODE'],axis=1,inplace=True)\napC_colums=apC_mode.columns\nprint(apC_colums)\n#transfering the continuous statistical parameter, rounding them to 21 discratic values\napC_mode=apC_mode.mul(20).round(0) #tranfering Normalized values to dicrate int between 0 to 20\napC_mode['TARGET']=ap_train['TARGET']\nframes = {}\n#Finding probability of avery value\nfor CN in apC_colums:\n    frames[CN] = apC_mode.pivot_table(index=CN, values='TARGET', aggfunc={'TARGET': [sum, len]})\n    frames[CN]['Ratio of 1']=frames[CN]['sum']\/frames[CN]['len']\ntemp_PV = pd.DataFrame()\nfor CN in apC_colums:\n  temp_PV[CN + '_size']=frames[CN]['len']\n  temp_PV[CN + '_p']=frames[CN]['Ratio of 1']\ntemp_PV.head(14)","b2545782":"#Droping fitcheres\napC_mode.drop(labels=['BASEMENTAREA_MODE','YEARS_BEGINEXPLUATATION_MODE','YEARS_BUILD_MODE','COMMONAREA_MODE','FLOORSMIN_MODE','NONLIVINGAPARTMENTS_MODE','NONLIVINGAREA_MODE','TARGET'],axis=1,inplace=True)\napC_colums=apC_mode.columns\n#filling missing data and adding flag for ~50% not having data\napC_mode.loc[apC['EMERGENCYSTATE_MODE'].isna()]=-2\nap_trainf['Apartment_Flag']=True\nap_trainf['Apartment_Flag'].loc[apC.isna().sum(axis=1)==47]=False \n\napC_mode.fillna(value=-1,inplace=True)\napC_mode['TARGET']=ap_train['TARGET']\n#marging normelized values grater then 11\napC_mode.replace(range(13,20), 12,inplace=True)\nNormal_data_to_probability_dic = {}\nframes = {}\nfor CN in apC_colums:\n    frames[CN] = apC_mode.pivot_table(index=CN, values='TARGET', aggfunc={'TARGET': [sum, len]})\n    frames[CN]['Ratio of 1']=frames[CN]['sum']\/frames[CN]['len']\n\ntemp_PVf = pd.DataFrame()\nfor CN in apC_colums:\n  temp_PVf[CN + '_size']=frames[CN]['len']\n  temp_PVf[CN + '_p']=frames[CN]['Ratio of 1']\n  Normal_data_to_probability_dic[CN] = dict(zip(temp_PVf.index, temp_PVf[CN + '_p']))\n  apC_mode[CN].replace(Normal_data_to_probability_dic[CN],inplace=True)\ntemp_PVf.head(14)","c8c704fc":"ap_trainf.drop(columns=apC_col_Name,inplace=True)\napC_mode.drop(columns=['TARGET'],inplace=True)\nap_trainf=ap_trainf.join(apC_mode)","d5ffe6bb":"apCob=apC.select_dtypes(include='object')\napCob.loc[apC['EMERGENCYSTATE_MODE'].isna()]=\"NAall\"\napCob.fillna(value=\"NA\",inplace=True)\napCob_colums=apCob.columns\napCob['TARGET']=ap_train['TARGET']\nframes = {}\nNobject_to_probability_dic = {}\nfor CN in apCob_colums:\n    #construction of DF Pivot tables\n    frames[CN] = apCob.pivot_table(index=CN, values='TARGET', aggfunc={'TARGET': [sum, len]})\n    frames[CN]['Ratio of 1']=frames[CN]['sum']\/frames[CN]['len']\n    Nobject_to_probability_dic[CN] = dict(zip(frames[CN].index, frames[CN]['Ratio of 1']))\n\nfor CN in apCob_colums:\n    apCob[CN].replace(Nobject_to_probability_dic[CN],inplace=True)\n\napCob.drop(columns='TARGET',inplace=True)\nap_trainf=ap_trainf.join(apCob) \nap_trainf[apCob_colums].head(5)","a051d7ed":"ap_test.iloc[:,33:80].info()","86cb9a7d":"ap_test['Apartment_Flag']=True\nap_test['Apartment_Flag'].loc[ap_test[apC_col_Name].isna().sum(axis=1)==47]=False \napCob_colums.drop('EMERGENCYSTATE_MODE')\nfor CN in apC_col_Name:\n  if CN == 'EMERGENCYSTATE_MODE':\n    ap_test[CN].loc[apC['EMERGENCYSTATE_MODE'].isna()]=\"NAall\"\n    ap_test[CN].replace(Nobject_to_probability_dic[CN],inplace=True)\n  elif CN in apCob_colums:\n    ap_test[CN].loc[apC['EMERGENCYSTATE_MODE'].isna()]=\"NAall\"\n    ap_test[CN].fillna(value=\"NA\",inplace=True)\n    ap_test[CN].replace(Nobject_to_probability_dic[CN],inplace=True)\n  elif CN in apC_mode.columns:\n    ap_test[CN].loc[ap_test['EMERGENCYSTATE_MODE'].isna()]=-2\n    ap_test[CN].fillna(value=-1,inplace=True)\n    ap_test[CN]=ap_test[CN].mul(20).round(0)\n    ap_test[CN].replace(range(13,20), 12,inplace=True)\n    ap_test[CN].replace(Normal_data_to_probability_dic[CN],inplace=True)\n  else:\n    ap_test.drop(columns=CN,inplace=True)\n\n","a9ce518e":"apf_mode_list=[x for x in ap_trainf.columns if \"MODE\" in x] # list of the Mode columns\nap_trainf['Apart_Norm_sum']=ap_trainf[apf_mode_list].sum(axis=1)\nap_trainf['Apart_Norm_sum'].loc[ap_trainf['Apartment_Flag']==False] = np.NaN\nap_test['Apart_Norm_sum']=ap_test[apf_mode_list].sum(axis=1)\nap_test['Apart_Norm_sum'].loc[ap_test['Apartment_Flag']==False] = np.NaN\n# plt.figure(figsize = (5, 4))\n\n# # KDE plot of loans that were repaid on time\n# sns.kdeplot(ap_trainf.loc[ap_trainf['TARGET'] == 0, 'Apart_Norm_sum'] , label = 'target == 0')\n\n# # KDE plot of loans which were not repaid on time\n# sns.kdeplot(ap_trainf.loc[ap_trainf['TARGET'] == 1, 'Apart_Norm_sum'] , label = 'target == 1')\n\n# # Labeling of plot\n# plt.xlabel('Apart_Norm_sum'); plt.ylabel('Density'); plt.title('Distribution of Apart_Norm_sum');\n","29eb70ef":"print(apf_mode_list)","470466f4":"apDPD=ap_train.loc[:,ap_train.columns.str.contains(r'CNT_SOCIA')]\napDPD.boxplot(figsize=(15,6))\n","a7d002cf":"for col in apDPD.columns:\n  apDPD[col].replace(apDPD[col].max(),0,inplace=True)\napDPD.boxplot(figsize=(15,6))","f49779be":"#Diffine new feature- OBS30 + DEF30^3 + (OBS30-OBS60)^2 + (DEF30-DEF60)^4\n#Replacing the 0.3% null values with 0\napDPD['(DEF30-DEF60)^4']=apDPD['DEF_30_CNT_SOCIAL_CIRCLE'].sub(apDPD['DEF_60_CNT_SOCIAL_CIRCLE']).mul(4)\napDPD['(DEF30-DEF60)^4']=apDPD['(DEF30-DEF60)^4'].pow(4)\napDPD['(OBS30-OBS60)^2']=apDPD['OBS_30_CNT_SOCIAL_CIRCLE'].sub(apDPD['OBS_60_CNT_SOCIAL_CIRCLE']).mul(2)\napDPD['(OBS30-OBS60)^2']=apDPD['(OBS30-OBS60)^2'].pow(2)\napDPD['(DEF30)^3']=apDPD['DEF_30_CNT_SOCIAL_CIRCLE'].mul(5).pow(3)\napDPD['SOCIAL_CIRCLE']=apDPD['(DEF30)^3'].add(apDPD['OBS_30_CNT_SOCIAL_CIRCLE']).add(apDPD['(OBS30-OBS60)^2']).add(apDPD['(DEF30-DEF60)^4'])\napDPD.drop(columns=['(DEF30-DEF60)^4','(OBS30-OBS60)^2','(DEF30)^3'],inplace=True)\napDPD['SOCIAL_default_rate']=apDPD['DEF_60_CNT_SOCIAL_CIRCLE'].div(apDPD['OBS_30_CNT_SOCIAL_CIRCLE'])\napDPD.info()","986371a4":"# apDPD['SOCIAL_CIRCLE'].fillna(value=-5,inplace=True)\n# apDPD['SOCIAL_CIRCLE'] = pd.cut(x=apDPD['SOCIAL_CIRCLE'], bins=[11,30,100,1000,5000,22000,25000,60000,700000],labels=[11 ,12,13,14,15,16,17,18])\napDPD['SOCIAL_CIRCLE'].loc[(apDPD['SOCIAL_CIRCLE']<=30) & (apDPD['SOCIAL_CIRCLE']>10)] = 11\napDPD['SOCIAL_CIRCLE'].loc[(apDPD['SOCIAL_CIRCLE']<=100) & (apDPD['SOCIAL_CIRCLE']>30)] = 12\napDPD['SOCIAL_CIRCLE'].loc[(apDPD['SOCIAL_CIRCLE']<=1000) & (apDPD['SOCIAL_CIRCLE']>100)] = 13\napDPD['SOCIAL_CIRCLE'].loc[(apDPD['SOCIAL_CIRCLE']<=5000) & (apDPD['SOCIAL_CIRCLE']>1000)] = 14\napDPD['SOCIAL_CIRCLE'].loc[(apDPD['SOCIAL_CIRCLE']<=22000) & (apDPD['SOCIAL_CIRCLE']>1000)] = 15\napDPD['SOCIAL_CIRCLE'].loc[(apDPD['SOCIAL_CIRCLE']<=25000) & (apDPD['SOCIAL_CIRCLE']>22000)] = 16\napDPD['SOCIAL_CIRCLE'].loc[(apDPD['SOCIAL_CIRCLE']<=60000) & (apDPD['SOCIAL_CIRCLE']>25000)] = 17\napDPD['SOCIAL_CIRCLE'].loc[apDPD['SOCIAL_CIRCLE']>60000]=18\napDPD['SOCIAL_CIRCLE'].fillna(value=-5,inplace=True)\napDPD['SOCIAL_default_rate'].fillna(value=0,inplace=True)\napDPD['OBS_30_CNT_SOCIAL_CIRCLE'].fillna(value=-1,inplace=True)\napDPD.head(30)","e21693bc":"apDPD.fillna(value=0,inplace=True)\napDPD.drop(columns=['DEF_60_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE'],inplace=True)\nap_trainf.drop(columns=['DEF_60_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE','OBS_30_CNT_SOCIAL_CIRCLE'],inplace=True)\nap_trainf=ap_trainf.join(apDPD) \napDPD.info()","478fbd1c":"ap_test['(DEF30-DEF60)^4']=ap_test['DEF_30_CNT_SOCIAL_CIRCLE'].sub(ap_test['DEF_60_CNT_SOCIAL_CIRCLE']).mul(4)\nap_test['(DEF30-DEF60)^4']=ap_test['(DEF30-DEF60)^4'].pow(4)\nap_test['(OBS30-OBS60)^2']=ap_test['OBS_30_CNT_SOCIAL_CIRCLE'].sub(ap_test['OBS_60_CNT_SOCIAL_CIRCLE']).mul(2)\nap_test['(OBS30-OBS60)^2']=ap_test['(OBS30-OBS60)^2'].pow(2)\nap_test['(DEF30)^3']=ap_test['DEF_30_CNT_SOCIAL_CIRCLE'].mul(5).pow(3)\nap_test['SOCIAL_CIRCLE']=ap_test['(DEF30)^3'].add(ap_test['OBS_30_CNT_SOCIAL_CIRCLE']).add(ap_test['(OBS30-OBS60)^2']).add(ap_test['(DEF30-DEF60)^4'])\nap_test.drop(columns=['(DEF30-DEF60)^4','(OBS30-OBS60)^2','(DEF30)^3'],inplace=True)\nap_test['SOCIAL_default_rate']=ap_test['DEF_60_CNT_SOCIAL_CIRCLE'].div(ap_test['OBS_30_CNT_SOCIAL_CIRCLE'])\nap_test['SOCIAL_CIRCLE'].loc[(ap_test['SOCIAL_CIRCLE']<=30) & (ap_test['SOCIAL_CIRCLE']>10)] = 11\nap_test['SOCIAL_CIRCLE'].loc[(ap_test['SOCIAL_CIRCLE']<=100) & (ap_test['SOCIAL_CIRCLE']>30)] = 12\nap_test['SOCIAL_CIRCLE'].loc[(ap_test['SOCIAL_CIRCLE']<=1000) & (ap_test['SOCIAL_CIRCLE']>100)] = 13\nap_test['SOCIAL_CIRCLE'].loc[(ap_test['SOCIAL_CIRCLE']<=5000) & (ap_test['SOCIAL_CIRCLE']>1000)] = 14\nap_test['SOCIAL_CIRCLE'].loc[(ap_test['SOCIAL_CIRCLE']<=22000) & (ap_test['SOCIAL_CIRCLE']>1000)] = 15\nap_test['SOCIAL_CIRCLE'].loc[(ap_test['SOCIAL_CIRCLE']<=25000) & (ap_test['SOCIAL_CIRCLE']>22000)] = 16\nap_test['SOCIAL_CIRCLE'].loc[(ap_test['SOCIAL_CIRCLE']<=60000) & (ap_test['SOCIAL_CIRCLE']>25000)] = 17\nap_test['SOCIAL_CIRCLE'].loc[ap_test['SOCIAL_CIRCLE']>60000]=18\nap_test['SOCIAL_CIRCLE'].fillna(value=-5,inplace=True)\nap_test['OBS_30_CNT_SOCIAL_CIRCLE'].fillna(value=0,inplace=True)\nap_test['SOCIAL_default_rate'].fillna(value=0,inplace=True)\nap_test.drop(columns=['DEF_60_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE'],inplace=True)\n\nap_test[['SOCIAL_CIRCLE','OBS_30_CNT_SOCIAL_CIRCLE','SOCIAL_default_rate']].info()\n","43805d1e":"print(ap_trainf.shape, '_', ap_test.shape)","fae8365d":"apNNF_list=['CNT_CHILDREN', 'AMT_CREDIT', 'AMT_ANNUITY','AMT_GOODS_PRICE', 'DAYS_REGISTRATION', 'DAYS_ID_PUBLISH',\\\n            'CNT_FAM_MEMBERS','inc_ratio','OCCUPATION_TYPE','OBS_30_CNT_SOCIAL_CIRCLE','NAME_FAMILY_STATUS',\\\n            'rtv_ratio','ltv_ratio','docs_val','days_emp_ratio']\nap_trainf[apNNF_list].hist(bins=10,figsize=(15,10))","ecdbeb5e":"X = ap_trainf[apNNF_list]\nfeatures = list(X.columns)\nX_index=list(X.index)\nqt = QuantileTransformer(n_quantiles=100)\nqt.fit(X)\nX=pd.DataFrame(data=qt.transform(X),index=X_index, columns=features,dtype='float32')\nX.hist(bins=100,figsize=(15,10))\nap_trainf.drop(columns=features,inplace=True)\nap_trainf[features]=X","fc34d309":"Xt = ap_test[apNNF_list]\nfeatures = list(Xt.columns)\nXt_index=list(Xt.index)\nXt=pd.DataFrame(data=qt.transform(Xt),index=Xt_index, columns=features,dtype='float32')\nap_test.drop(columns=features,inplace=True)\nap_test[features]=Xt","6f24cb71":"#X_train = pd.read_csv('\/content\/drive\/My Drive\/NAYA_P_3\/X_train.csv')\n#y_train = pd.read_csv('\/content\/drive\/My Drive\/NAYA_P_3\/y_train.csv')\n","a91e12bd":"# Ap_mode_list","1c4ee33c":"# Ap_mode_list=[x for x in ap_trainf.columns if \"MODE\" in x] # list of the Mode columns\n# Ap_mode_list.append('Apart_Norm_sum')\n# train=ap_trainf.copy()\n# for CV in Ap_mode_list:\n#   train[CV].loc[train['Apartment_Flag']==False]=np.nan\n# train.info()","81e7249f":"# Drop all rows that the Apart_Norm is NA\nAp_mode_list=[x for x in ap_trainf.columns if \"MODE\" in x] # list of the Mode columns\nAp_mode_list.append('Apart_Norm_sum')\ntrain=ap_trainf.copy()\nfor CV in Ap_mode_list:\n  train[CV].loc[train['Apartment_Flag']==False]=np.nan\n#Droping all the rowes where thire is no data for Home normalized\n#train.drop(index=train['Apartment_Flag'].loc[train['Apartment_Flag']==False].index,inplace=True)\ntrain.drop(columns = ['Apartment_Flag'],inplace=True)\n#the customer number reflact when loan was taken, small number old loan big numberes more resent loanes\n#train.set_index('SK_ID_CURR',inplace=True)\ntrain = pd.get_dummies(train)\nX_train=train.drop(columns = ['TARGET'])\nX_train = X_train.reindex(sorted(X_train.columns), axis=1)\ny_train=train['TARGET']\nfeatures = list(X_train.columns)\nX_index=list(X_train.index)\n# Mean imputation of missing values\n#imputer = KNNImputer()\nimputer=SimpleImputer()\n# Fit on the training data\nimputer.fit(X_train)\n# Transform both training and testing data\nX_train = pd.DataFrame(data=imputer.transform(X_train), index=X_index, columns=features)\n# Scale each feature to 0-1\n#Scaler Transformet\nfeatures = list(X_train.columns)\nX_index=list(X_train.index)\nscaler = MinMaxScaler(feature_range = (0, 1))\nscaler.fit(X_train)\nX_train = pd.DataFrame(data=scaler.transform(X_train), index=X_index, columns=features,dtype='float32')\n\n\n\n","7218dae4":"ap_test_temp=ap_test.copy()\n#ap_test=ap_test_temp","6dbe23db":"# Drop all rows that the Apart_Norm is NA\nAp_mode_list=[x for x in ap_test.columns if \"MODE\" in x] # list of the Mode columns\nAp_mode_list.append('Apart_Norm_sum')\nfor CV in Ap_mode_list:\n  ap_test[CV].loc[ap_test['Apartment_Flag']==False]=np.nan\nap_test.drop(columns = ['Apartment_Flag'],inplace=True)\nap_test = pd.get_dummies(ap_test)\nap_test['CODE_GENDER_XNA']=0\nX_test=ap_test.copy()\nX_test = X_test.reindex(sorted(X_test.columns), axis=1)\nfeatures = list(X_test.columns)\nX_index=list(X_test.index)\nX_test = pd.DataFrame(data=imputer.transform(X_test), index=X_index, columns=features)\nX_test = pd.DataFrame(data=scaler.transform(X_test), index=X_index, columns=features,dtype='float32')","ce0dc9d9":"# EXT_list=['EXT_SOURCE_312','EXT_SOURCE_f1']\n# X = X_train[EXT_list]\n# features = list(X.columns)\n# X_index=list(X.index)\n# pca_transformer = PCA(n_components=1)\n# Xn_PCA=pca_transformer.fit(X)\n# Xn=Xn_PCA.transform(X)\n# Xn_PCA.explained_variance_ratio_.cumsum()\n# # X_train.drop(columns=features,inplace=True)\n# features=['EXT_SOURCE_PCA']\n# X_train.iloc[:,-1] = pd.DataFrame(Xn, index=X_index, columns=features)\n# X_test.iloc[:,-1] = pd.DataFrame(Xn_PCA.transform(X_test[EXT_list]), index=X_index, columns=features)","a214f7ba":"ap_train.size","bd50086b":"# Ap_mode_list=[x for x in X.columns if \"MODE\" in x] # list of the Mode columns\n# Ap_mode_list.append('Apart_Norm_sum')\n# X = X_train[Ap_mode_list]\n# features = list(X.columns)\n# X_index=list(X.index)\n# pca_transformer = PCA(n_components=4)\n# Xn_PCA=pca_transformer.fit(X)\n# Xn=Xn_PCA.transform(X)\n# Xn_PCA.explained_variance_ratio_.cumsum()\n# X_train.drop(columns=features,inplace=True)\n# features=['ApN_PCA_1','ApN_PCA_2','ApN_PCA_3','ApN_PCA_4']\n# X_train[features] = pd.DataFrame(Xn, index=X_index, columns=features)\n\n\n\n","706e8bfc":"\n# Xt = X_test[Ap_mode_list]\n# features = list(Xt.columns)\n# Xt_index=list(Xt.index)\n# Xnt=Xnt_PCA.transform(Xt)\n# X_test.drop(columns=features,inplace=True)\n# features=['ApN_PCA_1','ApN_PCA_2','ApN_PCA_3','ApN_PCA_4']\n# X_test[features] = pd.DataFrame(Xnt, index=Xt_index, columns=features)","893f2a3d":" Ap_mode_list=[x for x in X_train.columns if \"MODE\" in x] # list of the Mode columns\n Ap_mode_list.append('Apart_Norm_sum')\n X = X_train[Ap_mode_list]\n features = list(X.columns)\n X_index=list(X.index)\n ap_selector = SelectKBest(score_func=f_classif, k=4)\n X_train_ap_reduced = \\\n     pd.DataFrame(ap_selector.fit_transform(X, y_train), \n                  columns=X.columns[ap_selector.get_support()].values,index=X_index)\n\nfeatures=X_train_ap_reduced.columns\n#droping all AP col exept 4 best col\nAP_drop_list=set(Ap_mode_list) - set(features)\nX_train.drop(columns=AP_drop_list,inplace=True)\nX_test.drop(columns=AP_drop_list,inplace=True)\n# X_train_ap_reduced.head()\nX_test.info()","7f064875":"# from google.colab import files\n# y_train.to_csv('y_train.csv') \n# files.download('y_train.csv')\n# X_train.to_csv('X_train.csv') \n# files.download('X_train.csv')","4ab3ef82":"#significant_coll=['SK_ID_CURR','AMT_log_INCOME_TOTAL','NAME_TYPE_SUITE',  'REGION_POPULATION_RELATIVE',\n      #  'DAYS_BIRTH', 'DAYS_EMPLOYED', 'FLAG_EMP_PHONE', 'FLAG_WORK_PHONE',\n      #  'FLAG_PHONE', 'FLAG_EMAIL',  'REGION_RATING_CLIENT',\n      #  'REGION_RATING_CLIENT_W_CITY', 'WEEKDAY_APPR_PROCESS_START',\n      #  'HOUR_APPR_PROCESS_START', 'REG_REGION_NOT_LIVE_REGION',\n      #  'REG_REGION_NOT_WORK_REGION', 'LIVE_REGION_NOT_WORK_REGION',\n      #  'REG_CITY_NOT_LIVE_CITY', 'REG_CITY_NOT_WORK_CITY',\n      #  'LIVE_CITY_NOT_WORK_CITY', \n      #  'DAYS_LAST_PHONE_CHANGE', 'inc_ratio', 'days_emp_ratio',\n      #  'EXT_SOURCE_f1', 'EXT_SOURCE_312', 'EXT_SOURCE_min', 'EXT_SOURCE_max',\n      #  'docs_val', 'BUR_DAY', 'BUR_MO',\n      #  'BUR_null', 'OBS_30_CNT_SOCIAL_CIRCLE', 'SOCIAL_CIRCLE',\n      #  'SOCIAL_default_rate', 'AMT_CREDIT',\n      #  'AMT_ANNUITY', 'AMT_GOODS_PRICE', 'DAYS_REGISTRATION',\n      #  'DAYS_ID_PUBLISH',  'NAME_CONTRACT_TYPE_Cash loans',\n      #  'NAME_CONTRACT_TYPE_Revolving loans']","9a6691f6":"#X_train=X_train[significant_coll]\n#X_test=X_test[significant_coll]","3e88bb2b":"my_cv = StratifiedShuffleSplit(n_splits=7, train_size=0.8, test_size=0.2,random_state=0)\nmy_param_grid = {'C': [1e-4, 5e-4,1e-3, 5e-3, 1e-2, 5e-2, 1e-1, 5e-1], 'penalty': ['l1', 'l2']}\n\nlr_model_gs = GridSearchCV(estimator=LogisticRegression(max_iter=10,class_weight='balanced'), \n                        param_grid=my_param_grid, \n                        scoring='roc_auc', \n                        cv=my_cv)\nlr_model_gs.fit(X_train, y_train)\nlr_model_gs.best_score_","a3496775":"lr_model_gs.best_params_","2405fc5a":"my_cv = StratifiedShuffleSplit(n_splits=7, train_size=0.8, test_size=0.2,random_state=42)\nlog_reg=LogisticRegression(C=0.5,penalty='l2',max_iter=100)\nLGscore=np.array(cross_val_score(log_reg, X_train, y_train, cv=my_cv, scoring='roc_auc'))\nnp.set_printoptions(formatter={'float': '{: 0.3f}'.format})\nprint('Cross val scores')\nprint(LGscore)\nprint(\" Mean Score: {:.3f}\".format(LGscore.mean()))","9490aa38":"X_train, X_test_CV, y_train, y_test_CV = train_test_split(X_train, y_train, \n                                                    train_size=0.8, \n                                                    test_size=0.2,\n                                                    shuffle=True, \n                                                    stratify=y_train)","05b48bcb":"log_reg = LogisticRegression(C=0.005,penalty='l2',max_iter=500,class_weight='balanced').fit(X_train, y_train)\ntrain_roc=roc_auc_score(y_train,log_reg.predict_proba(X_train)[:,1])","9754402c":"test_roc=roc_auc_score(y_test_CV,log_reg.predict_proba(X_test_CV)[:,1])\nprint('Train ROC: {:.3f}  Test ROC: {:.3f}'.format(train_roc, test_roc))","8992027f":"def report(clf, X, y):\n    acc = accuracy_score(y_true=y, \n                         y_pred=clf.predict(X))\n    cm = pd.DataFrame(confusion_matrix(y_true=y, \n                                       y_pred=clf.predict(X)), \n                      index=clf.classes_, \n                      columns=clf.classes_)\n    rep = classification_report(y_true=y, \n                                y_pred=clf.predict(X))\n    return 'accuracy: {:.3f}\\n\\n{}\\n\\n{}'.format(acc, cm, rep)","0b528722":"print(report(log_reg, X_test_CV, y_test_CV))","7939b67c":"log_reg = LogisticRegression(C=0.0001,penalty='l2',max_iter=500)\nlog_reg.fit(X_train, y_train)","0d352fab":"log_reg = LogisticRegression(C=0.005,penalty='l2',max_iter=500)\nlog_reg.fit(X_train, y_train)\nSubmission=pd.DataFrame(X_test.index,columns=['SK_ID_CURR'])\nSubmission['TARGET'] =log_reg.predict_proba(X_test)[:,1]\nSubmission.head(30)","8131245e":"Submission.hist(bins=20)","86383f6d":"Submission.to_csv('Submissionori.csv',index=False)","c28e82d8":"scores = log_reg.predict_proba(X_train)[:, 1]\nroc_auc_score(y_train==1, scores)","3a9b040a":"roc_auc_score(y_test==1, scores)","eb5becb6":"X_train_SVC, X_test_SVC, y_train_SVC, y_test_SVC = train_test_split(X_train, y_train, \n                                                    train_size=0.05, \n                                                    test_size=0.95,\n                                                    shuffle=True, \n                                                    stratify=y_train)","7293734d":"SVC_reg = SVC(probability=True,class_weight='balanced',cache_size=1000).fit(X_train_SVC, y_train_SVC)\n","fcad4ad4":"Submission=pd.DataFrame(X_test.index,columns=['SK_ID_CURR'])\nSubmission['TARGET'] =SVC_reg.predict_proba(X_test)[:,1]\nSubmission.head(30)","79f23a95":"#Grid\nRF_model = RandomForestClassifier(max_depth=5,min_samples_split=10 ,random_state=0)\nRF_grid = {'min_samples_leaf': [100,200],\n                 'max_depth': [3, 6, 9],\n                 'criterion': ['gini', 'entropy'],\n                  'n_jobs':[-1],\n                  'max_samples':[0.5,0.3]\n           }\nRF_cv = StratifiedShuffleSplit(n_splits=5, train_size=0.7, test_size=0.3)\nRF_model_gs = GridSearchCV(estimator=RF_model, param_grid=RF_grid, cv=RF_cv,scoring='roc_auc')\nRF_model_gs.fit(X_train, y_train)\nRF_model_best = RF_model_gs.bes\n","f6d89d78":"RF_model = RandomForestClassifier(max_depth=8,min_samples_leaf=100,criterion='gini',n_jobs=-1,max_samples=0.5).fit(X_train, y_train)","b367967c":"Submission=pd.DataFrame(X_test.index,columns=['SK_ID_CURR'])\nSubmission['TARGET'] =RF_model.predict_proba(X_test)[:,1]\nSubmission.head(30)","0f819a10":"xgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\nxgb_model.fit(X_train, y_train)","6f280994":"Submission=pd.DataFrame(X_test.index,columns=['SK_ID_CURR'])\nSubmission['TARGET'] =xgb_model.predict_proba(X_test)[:,1]\nSubmission.head(30)","b8bc5fb4":"X_train, X_test_CV, y_train, y_test_CV = train_test_split(X_train, y_train, \n                                                    train_size=0.8, \n                                                    test_size=0.2,\n                                                    shuffle=True, \n                                                    stratify=y_train)","cafaf38d":"log_reg = log_reg = LogisticRegression(C=0.005,penalty='l2',max_iter=500).fit(X_train, y_train)\nRF_model1 = RandomForestClassifier(n_estimators=100,max_depth=8,min_samples_leaf=100,criterion='gini',n_jobs=-1,max_samples=0.5)\nRF_model2 = RandomForestClassifier(n_estimators=500,max_depth=3,min_samples_leaf=500,criterion='gini',n_jobs=-1,max_samples=0.5)\nRF_model3 = RandomForestClassifier(n_estimators=250,max_depth=12,min_samples_leaf=200,criterion='gini',n_jobs=-1,max_samples=0.5)\nxgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", max_depth=4,n_estimators=200,eta=0.1,min_child_weight=10,eval_metric='auc',gamma=2)","4fd40e73":"clf1 = log_reg\nclf2 = RF_model1\nclf3 = RF_model2\nclf4 = RF_model3\nclf5 = xgb_model\n\n","f8829ea3":"eclf = EnsembleVoteClassifier(clfs=[clf1, clf2, clf3, clf4, clf5], weights=[1.5,1,1.5,1,2],voting='soft',verbose=2).fit(X_train, y_train)","4761fbcf":"train_roc=roc_auc_score(y_train,eclf.predict_proba(X_train)[:,1])\ntest_roc=roc_auc_score(y_test_CV,eclf.predict_proba(X_test_CV)[:,1])\nprint('Train ROC: {:.3f}'.format(train_roc))\nprint('CV Test ROC: {:.3f}'.format(test_roc))","e643eb99":"Submission=pd.DataFrame(ap_test_temp['SK_ID_CURR'],columns=['SK_ID_CURR'])\nSubmission['TARGET'] =eclf.predict_proba(X_test)[:,1]\nSubmission.head(30)","b1e5b99c":"Submission.to_csv('Submissionori.csv',index=False)","bfe81d12":"Adding PCA fitcher from Extenall soucess f1 and 312","3b372f5b":"## General Data Visualization\n1. Missing values\n2. Distributions\n","9e025b57":"### Replacing columns of categorical data with category probability\n","f2c8e83f":"Competition score 0.724\n\n","3ac51ce1":"## Social Circle\nThis data section represents Client main social connections (most likly found threw social network) who have or had loan at Home Credit (OBS), and how many of them had default in loan paiment (Target=1) over the last 30 and 60 days, [link](https:\/\/www.kaggle.com\/c\/home-credit-default-risk\/discussion\/57054#371849) for more detailes.","889eb851":"# <center>Home Credit Default Risk<center>![HCpic.JPG](attachment:HCpic.JPG)  \n# <center>Naya classification project #4<\/center>\n\n# **<center>Ori Bein & Ori Roval<\/center>**\n\n\n\n\n   \n","f125d3c0":"Competition score - 0.741 using only the main competition data set","e0ce63ec":"Replacing two outlayeres with 0.","451808c7":"## Loan information & Cliantes detailes\n","985aab59":"Competition Score 0.7215","39c4954a":"Reducing Appartment fitcher from 14 columes to 4 using **PCA**","0bdefef6":"After avaluating every fitcher default probability distribution and the fitcher population it made sens not to use the 7 following fitcheres:\nBASEMENTAREA, YEARS_BEGINEXPLUATATION,YEARS_BUILD, COMMONAREA, FLOORSMIN, NONLIVINGAPARTMENTS, NONLIVINGAREA.\n\nIn order to avoid over-fit, it make sens to marge all normalized values that are grater then 11 to one been, with single probability.\n\nAll the missing data from EMERGENCYSTATE_MODE ~50%, is marged to a bin -2 and the rest of missing datamarged to bin -1, \nAnd get the probability for default of missing data population, calculated for every fitcher.","2d96dc56":"The blue dots representes cliant with default flag.\nWe can see bad corelation between the sourcess(the dots scater has shape of squre far from a diagonal line).\nThire is good corelation between low grade and higher rate of default cases.\nIt lookes that when cliant got highe score from at least two sourcess , thire only few default cases.\nIt lookes that EXT-2 predictiones are not that good, thire are many cliantes with highe score that got default flag.\nHaving that in mind we set 3 new features, and avoiding Null:\n1. Minimum grade of cliant.\n2. Maximum grade of cliant.\n3. Multiple of EXT-1 and EXT-3 to emphasies the beter predictiones they have.\n3. In case mean out of the 3 is lower then p (diffult 0.5 tested and found 0.25 is better), the grade is the mean grade,else the grade is the maximum grade out of the 3. \nIn order to amplify the fitcher, the grade is transfared threw a Sigmoid function.\n\nThe new feature was diffaine as an transformer function that receives two argumants: g - the gaine of the sigmoid function, and p - defines minimum grade.\n\nThe two parameters can later be optimized using a grid search function.","437964bb":"## Data Cleaning\n \n","94fd4b87":"## Normalized information about building where the client lives","af006e4f":"**<center>This project is the middle submission, classification assignment, at the course: Data Science in Naya College<center>**","5bc54a0f":"Replacing object data type at 8 columns with more then 3 cateories.\nThe object data is replaced with numeric value found from the sub category probability for default","741a2ee4":"## Seting up X and y \n\nReplacing NA values with feature mean value.\nScailing all features to be from 0 to 1.","dc106784":"This part of the Data-Set includes 46 statisticl and categoriesed fitchers, giving information on where the client lives. At this saction of the data set, the data is partial,and the available data varies between fitcheres with 50% to 30% from data-set.","1ecbbd8f":"Testing predict on CV data","6db4bf79":"### Seting up sugnificant ratioes\n\n1. 'DAYS_EMPLOYED' has a set value (365243) both for Unemployed and Pensioner Income type, it is changed to values that represents the default probability of each group.\n2. Loan to Income ratio - setting a ratio that represent how much of a borower income\n3. Days Employed ratio - 'DAYS_EMPLOYED' to 'DAYS_BIRTH' (Age)","add53247":"# Logistic Regration Model","51363bfc":"### UNITING_TYPE & OCCUPATION_TYPE  \n'OCCUPATION_TYPE' is a sub category of 'NAME_INCOME_TYPE' and also has Null for 30% of values ","c294e3ff":"# Feature engineering\n","ffaf1df2":"As data is not linear a non linear SVM is used, with only 5% of the train data.","523941ac":"Two new features from the social data criated:\nfunction that tryies to colaberate all data of the 4 fitcher to one fitcher, soming the features together and using polinomial function to give higher weight to contactes that have default at thire loan paiments.\nFitcher of the rate of contacts with default issue to contact with nithout default.","c8a86188":"Adding feature Eng value sum of the all appertment fitcheres probability","52f39c09":"The data is mainly numeric, normalized from 0 to 1 out of data-set.\nThe data of every category is givien using 3 statistical parameters AVG (Mean) MODE (Most frequent value in a data set) and MEDI (Median), first step is to understand what statistical parameter will be most helpfull to the model","d244e11b":"## Fitst Submission","65990dc4":"As the 7 CV scores are balanced, the CV will be done using 20% of shuffled Train data","10ee9fc5":"## Enquiries to Credit Bureau\n1. Creating 0,1 bins for 'AMT_REQ_CREDIT_BUREAU_DAY' and 'AMT_REQ_CREDIT_BUREAU_MON'.  \nBoth features remained due to negative correlaiton), other had little info regarding TARGET.\n2. Creating dummy for nulls (all rows with null had null for all bureau enquiries).\n4. Dropping all original AMT_REQ_CREDIT_BUREAU features.","a24fdb0b":"Competition score - 0.7025 using only 5% of train data","db873462":"As shown, The Apartments 3 statistical parameter has strong correlation, and it is better to work only with one of them.\n\nBecause thire is higher probabilityes that Mode parameter reflects better the cliante sosial status,  only the Mode parameter be in use","8da74697":"Reducing Appartment features from 14 columes to 4 using **KBest**","63423b42":"This project data-set was taken from Kaggle compatition.\nhttps:\/\/www.kaggle.com\/c\/home-credit-default-risk\/discussion\/57054\n\nIn this project **only the competition main data set- application - is used**.  \nThe data includes information from 307,511 loans, with an 8.4% default rate. The goal is to predict probability of default for each loan.\n\nCompetition score is measured using the area under ROC curve.\n","f7dd2a33":"## External Rating\n\n---\n\nThe 3 External Rating features found to be very meaningful\n\nThire for the 3 different external credit ratings are process to improve thire value, engineering 5 more features from them.\n\n\n","404c8f97":"Submission","a2f83305":"Uniforming the total income destribution to help model","74f322ec":"Repiting the process, this time to the object type columns","c21cfd4b":"Divaiding the new social circle feature to bins to reduce over fit of new feature","563b9b58":"After applaying transformation only 172 cliants, 0.05% of data, have no grade data","e8c18545":"From 124 features the data set has now 64 Fitcheres including dummies","427730ea":"## Model output analysis using Confusion Matrix","83300dec":"## Introduction\n**Application Train - Dataset**","819b4638":"Droping 'FLAG_CONT_MOBILE' and 'FLAG_MOBIL' - both has same value for all data (or 99.8% of data).","c878a53d":"## Grid Search","d469cc1f":"## Document flags  \nThere are 20 columns of \"Did client provide document\" with no further data regarding nature of each document.  \n**Findings:**\n* There is a diferrence regarding the default rates.  \n* Some columns have very small number of documents submited, sometimes with no defaults.\n\n**Our feature engineering includes:**\n1. Creating a column all combinations of submited documents.\n2. Finding the default rate for each combination.\n3. Finding groups of combinations and assigning default rate as group average.\n4. Any combination with no defaults recives a rate of 0.08 (same as all population mean)\n5. Assigning the default rate as a new feature.\n6. Droping all document columns.","5cc54182":"#XGB","1d06560a":"#Random Forest","9719ee64":"## Forcing natural numeric feature to uniform destribution\n","cddbb9ff":"Because EXT_3 lookes as a very good predictor, compose features from EXT_3 in case ECT_3 is null use EXT_1 in case EXT_1 is null use EXT_2","452936f8":"#Voting Clssifier","b60d2a25":"Seting 3 RondomForest dels:\n1. Small number of treas, tall with small leafs size.\n2. Big number of treas, very short with big leafs size.\n3. Midum number of treas, very tall  with midum leafs size.\n\nAding  2 extra models: \n\n*   Logistic-Regration\n*   XGB with max depth of 4 and 200 estimatores\n\nSeting to the more sefisticated models higher weights in the voting classifier setup.","10173ad3":"# **EDA**","9ef9f580":"# Preparing data for model","6c46cc62":"# SVM","48b3c8b2":"### Car Owners\n1. 5 Rows have both Y (Yes) for owned car and no car age, so we use MODE of all car ages to complete it. \n2. Creating bins for car age (including no car as a separate bin).\n3. Converting to car age to dummies.\n4. Dropping original OWN_CAR_AGE','FLAG_OWN_CAR' features.","a9a9d561":"The data includes 7 data sectiones:\n\n\n\n1.   Loan information.\n2.  Cliante detailes, mainly categorical fitcheres describing cliante social and financial status,also the day and the time cliante aproched.\n3.   Data from 3 External Credit Due-Diligence companies evaluating cliante risk.\n4.   Data from external database giving normalized information about building where the client lives.\n1.   Cliantes Social Circle - number of cliant social connectiones that took a loan from company.\n2.   Flag for 20 documantes cliante submited.\n1.   Information on inquires done on cliante at financiale data sorce.","71eb9016":"## Cross Validation - Score method ROC AUC\n\n\n\n"}}