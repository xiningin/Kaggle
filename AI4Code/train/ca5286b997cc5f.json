{"cell_type":{"87858237":"code","b69870d0":"code","d2a5b64c":"code","a5dcd3ee":"code","7329c95b":"code","21a9acf3":"code","c5df9982":"code","c92db3ac":"markdown","e952d6ae":"markdown","37821459":"markdown","7b29f5c5":"markdown","fc55d76e":"markdown","4b723c07":"markdown","abb98615":"markdown","8a5db75c":"markdown"},"source":{"87858237":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b69870d0":"# Import Libraries\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split","d2a5b64c":"# Prepare Dataset\n# load data\ntrain = pd.read_csv(r\"\/kaggle\/input\/digit-recognizer\/train.csv\",dtype = np.float32)\n\n# split data into features(pixels) and labels(numbers from 0 to 9)\ntargets_numpy = train.label.values\nfeatures_numpy = train.loc[:,train.columns != \"label\"].values\/255 # normalization\n\n# train test split. Size of train data is 80% and size of test data is 20%. \nfeatures_train, features_test, targets_train, targets_test = train_test_split(features_numpy,\n                                                                             targets_numpy,\n                                                                             test_size = 0.2,\n                                                                             random_state = 42) \n\n# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\nfeaturesTrain = torch.from_numpy(features_train)\ntargetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n\n# create feature and targets tensor for test set.\nfeaturesTest = torch.from_numpy(features_test)\ntargetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long\n\n# batch_size, epoch and iteration\nbatch_size = 100\nn_iters = 10000\nnum_epochs = n_iters \/ (len(features_train) \/ batch_size)\nnum_epochs = int(num_epochs)\n\n# Pytorch train and test sets\ntrain = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\ntest = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n\n# data loader\ntrain_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\ntest_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n","a5dcd3ee":"# Create ANN Model\nclass ANNModel(nn.Module):\n    \n    def __init__(self, input_dim, hidden_dim, output_dim):\n        super(ANNModel, self).__init__()\n        \n        # Linear function 1: 784 --> 150\n        self.fc1 = nn.Linear(input_dim, hidden_dim) \n        # Non-linearity 1\n        self.relu1 = nn.ReLU()\n        \n        # Linear function 2: 150 --> 150\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 2\n        self.tanh2 = nn.Tanh()\n        \n        # Linear function 3: 150 --> 150\n        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n        # Non-linearity 3\n        self.elu3 = nn.ELU()\n        \n        # Linear function 4 (readout): 150 --> 10\n        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n    \n    def forward(self, x):\n        # Linear function 1\n        out = self.fc1(x)\n        # Non-linearity 1\n        out = self.relu1(out)\n        \n        # Linear function 2\n        out = self.fc2(out)\n        # Non-linearity 2\n        out = self.tanh2(out)\n        \n        # Linear function 2\n        out = self.fc3(out)\n        # Non-linearity 2\n        out = self.elu3(out)\n        \n        # Linear function 4 (readout)\n        out = self.fc4(out)\n        return out\n\n# instantiate ANN\ninput_dim = 28*28\nhidden_dim = 150 #hidden layer dim is one of the hyper parameter and it should be chosen and tuned. For now I only say 150 there is no reason.\noutput_dim = 10\n\n# Create ANN\nmodel = ANNModel(input_dim, hidden_dim, output_dim)\n\n# Cross Entropy Loss \nerror = nn.CrossEntropyLoss()\n\n# SGD Optimizer\nlearning_rate = 0.02\noptimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)","7329c95b":"# ANN model training\ncount = 0\nloss_list = []\niteration_list = []\naccuracy_list = []\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n\n        train = Variable(images.view(-1, 28*28))\n        labels = Variable(labels)\n        \n        # Clear gradients\n        optimizer.zero_grad()\n        \n        # Forward propagation\n        outputs = model(train)\n        \n        # Calculate softmax and ross entropy loss\n        loss = error(outputs, labels)\n        \n        # Calculating gradients\n        loss.backward()\n        \n        # Update parameters\n        optimizer.step()\n        \n        count += 1\n        \n        if count % 50 == 0:\n            # Calculate Accuracy         \n            correct = 0\n            total = 0\n            # Predict test dataset\n            for images, labels in test_loader:\n\n                test = Variable(images.view(-1, 28*28))\n                \n                # Forward propagation\n                outputs = model(test)\n                \n                # Get predictions from the maximum value\n                predicted = torch.max(outputs.data, 1)[1]\n                \n                # Total number of labels\n                total += len(labels)\n\n                # Total correct predictions\n                correct += (predicted == labels).sum()\n            \n            accuracy = 100 * correct \/ float(total)\n            \n            # store loss and iteration\n            loss_list.append(loss.data)\n            iteration_list.append(count)\n            accuracy_list.append(accuracy)\n        if count % 500 == 0:\n            # Print Loss\n            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))","21a9acf3":"# visualization loss \nplt.plot(iteration_list,loss_list)\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Loss\")\nplt.title(\"ANN: Loss vs Number of iteration\")\nplt.show()","c5df9982":"# visualization accuracy \nplt.plot(iteration_list,accuracy_list,color = \"red\")\nplt.xlabel(\"Number of iteration\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"ANN: Accuracy vs Number of iteration\")\nplt.show()","c92db3ac":"<a id=\"6\"><\/a> <br>\n### visualization accuracy","e952d6ae":"<a id=\"0\"><\/a> <br>\n### Artificial Neural Network (ANN)\n- Logistic regression is good at classification but when complexity(non linearity) increases, the accuracy of model decreases.\n- Therefore, we need to increase complexity of model.\n- In order to increase complexity of model, we need to add more non linear functions as hidden layer. \n- What we expect from artificial neural network is that when complexity increases, we use more hidden layers and our model can adapt better. As a result accuracy increase.\n- **Steps of ANN:**\n    1. Import Libraries\n\n    1. Prepare Dataset\n\n    1. Create ANN Model\n        - We add 3 hidden layers.\n        - We use ReLU, Tanh and ELU activation functions for diversity.\n    1. Instantiate Model Class\n        - input_dim = 28*28 # size of image px*px\n        - output_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n        - Hidden layer dimension is 150. I only choose it as 150 there is no reason. Actually hidden layer dimension is hyperparameter and it should be chosen and tuned. You can try different values for hidden layer dimension and observe the results.\n        - create model\n    1. Instantiate Loss\n        - Cross entropy loss\n        - It also has softmax(logistic function) in it.\n    1. Instantiate Optimizer\n        - SGD Optimizer\n    1. Traning the Model\n    1. Prediction\n- As a result, as you can see from plot, while loss decreasing, accuracy is increasing and our model is learning(training). \n- Thanks to hidden layers model learnt better and accuracy(almost 95%) is better than accuracy of logistic regression model.","37821459":"<a id=\"4\"><\/a> <br>\n### ANN model training","7b29f5c5":"<a id=\"1\"><\/a> <br>\n### Import Necessary Libraries","fc55d76e":"<a id=\"2\"><\/a> <br>\n### Prepare Dataset","4b723c07":"## INTRODUCTION\n- It\u2019s a Python based scientific computing package targeted at two sets of audiences:\n    - A replacement for NumPy to use the power of GPUs\n    - Deep learning research platform that provides maximum flexibility and speed\n- pros: \n    - Interactively debugging PyTorch. Many users who have used both frameworks would argue that makes pytorch significantly easier to debug and visualize.\n    - Clean support for dynamic graphs\n    - Organizational backing from Facebook\n    - Blend of high level and low level APIs\n- cons:\n    - Much less mature than alternatives\n    - Limited references \/ resources outside of the official documentation\n- I accept you know neural network basics. If you do not know, you should learn the basics first of all.  Because I will not explain neural network concepts detailed, I only explain how to use pytorch for neural network\n<br>\n<br> **Content:**\n0. [Introduction to ANN](#0)\n1. [Import necessary Libraries](#1)\n1. [Prepare Dataset](#2)\n1. [Create ANN Model](#3)\n1. [ANN model training](#4)\n1. [visualization loss](#5)\n1. [visualization accuracy](#6)","abb98615":"<a id=\"5\"><\/a> <br>\n### visualization loss ","8a5db75c":"<a id=\"3\"><\/a> <br>\n### Create ANN Model"}}