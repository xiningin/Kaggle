{"cell_type":{"de87fbcf":"code","b15c0d2f":"code","7de08aeb":"code","c9abd338":"code","72b0a9f2":"code","b9ccd464":"code","86bac281":"code","b59f3d7f":"code","a1e14a1b":"code","c2da4166":"code","4112b7d8":"code","86c5efb1":"code","ba6df2a5":"code","c36e018e":"code","45d9240a":"code","9becd3b9":"code","dc950dcf":"code","84333441":"code","3bac8979":"code","75726963":"code","ca3ddac4":"code","e6d7371a":"code","7cd36943":"code","98146117":"code","5941a0cd":"code","da764fba":"code","780225a6":"code","91b2a807":"markdown","1bc8c327":"markdown","04f7be10":"markdown","a56a3481":"markdown","021c289c":"markdown","fbc6e8ac":"markdown","c2ee7852":"markdown","13ce94cd":"markdown","ed15e90d":"markdown","24d6d46c":"markdown"},"source":{"de87fbcf":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport re","b15c0d2f":"_1 = pd.read_csv('..\/input\/elon-musk-tweets-2010-2021\/2010.csv')\n_2 = pd.read_csv('..\/input\/elon-musk-tweets-2010-2021\/2011.csv')\n_3 = pd.read_csv('..\/input\/elon-musk-tweets-2010-2021\/2012.csv')\n_4 = pd.read_csv('..\/input\/elon-musk-tweets-2010-2021\/2013.csv')\n_5 = pd.read_csv('..\/input\/elon-musk-tweets-2010-2021\/2014.csv')\n_6 = pd.read_csv('..\/input\/elon-musk-tweets-2010-2021\/2015.csv')\n_7 = pd.read_csv('..\/input\/elon-musk-tweets-2010-2021\/2016.csv')\n_8 = pd.read_csv('..\/input\/elon-musk-tweets-2010-2021\/2017.csv')\n_9 = pd.read_csv('..\/input\/elon-musk-tweets-2010-2021\/2018.csv')\n_10 = pd.read_csv('..\/input\/elon-musk-tweets-2010-2021\/2019.csv')\n_11 = pd.read_csv('..\/input\/elon-musk-tweets-2010-2021\/2020.csv')\n_12 = pd.read_csv('..\/input\/elon-musk-tweets-2010-2021\/2021.csv')","7de08aeb":"full_df = pd.concat([_1, _2, _3, _4, _5, _6, _7, _8, _9, _10, _11, _12], axis = 0)","c9abd338":"pd.set_option(\"display.max_columns\", 50) # So all columns are shown\nfull_df.head()","72b0a9f2":"full_df.info()","b9ccd464":"tweets = full_df['tweet']\ndel full_df","86bac281":"from transformers import GPT2Tokenizer\n\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2', \n                                          bos_token='<|sos|>', \n                                          eos_token='<|eos|>', \n                                          pad_token='<|pad|>')\n\n\ntokenizer.encode(\"Sample Text\")","b59f3d7f":"max_tweet = max([len(tokenizer.encode(tweet)) for tweet in tweets])\n\nprint(f'The longest tweet is {max_tweet} tokens long.')","a1e14a1b":"batch_size = 32\n\nimport torch\nfrom torch.utils.data import Dataset\n\nclass TweetDataset(Dataset):\n    def __init__(self,tweets,tokenizer,gpt2_type=\"gpt2\",max_length=max_tweet):\n        self.tokenizer = tokenizer\n        self.input_ids = []\n        self.attention_masks = []\n        \n        for tweet in tweets:\n            encoding_dict = tokenizer('<|sos|>'+ tweet +'<|eos|>',truncation=True,\n                                     max_length=max_length,\n                                     padding='max_length')\n            \n            self.input_ids.append(torch.tensor(encoding_dict['input_ids']))\n            self.attention_masks.append(torch.tensor(encoding_dict['attention_mask']))\n        \n    def __len__(self):\n        return len(self.input_ids)\n    \n    def __getitem__(self,idx):\n        return self.input_ids[idx], self.attention_masks[idx]","c2da4166":"from torch.utils.data import random_split\ndataset = TweetDataset(tweets,tokenizer,max_length=max_tweet)","4112b7d8":"train_size = int(0.9 * len(dataset)) #90% train, 10% validation\nval_size = len(dataset)-train_size\n\ntrain,val = random_split(dataset,[train_size,val_size])\nprint(f'No of train samples = {train_size} and Number of validation samples = {val_size}')","86c5efb1":"from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n\ntrain_dataloader = DataLoader(train,sampler = RandomSampler(train),\n                             batch_size = batch_size)\n\nval_dataloader = DataLoader(val,sampler = SequentialSampler(val),\n                           batch_size = batch_size)","ba6df2a5":"import random\nfrom transformers import GPT2LMHeadModel, GPT2Config","c36e018e":"configuration = GPT2Config.from_pretrained('gpt2', output_hidden_states=False)\nmodel = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=configuration)\nmodel.resize_token_embeddings(len(tokenizer))\n\ndevice = torch.device(\"cuda\")\nmodel.cuda()\n\nseed_val = 42\n\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","45d9240a":"# the warmup steps are steps at the start of training that are ignored\n# every x steps we will sample the model to test the output\n\nepochs = 3\nwarmup_steps = 1e2\nsample_every = 100","9becd3b9":"from transformers import AdamW\n\noptimizer = AdamW(model.parameters(),\n                  lr = 5e-4,\n                  eps = 1e-8\n                )","dc950dcf":"from transformers import get_linear_schedule_with_warmup\n\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps = warmup_steps, \n                                            num_training_steps = total_steps)","84333441":"import random\nimport time\nimport datetime\n\ndef format_time(elapsed):\n    return str(datetime.timedelta(seconds=int(round((elapsed)))))\n\ntotal_t0 = time.time()\n\ntraining_stats = []\n\nmodel = model.to(device)\n\nfor epoch_i in range(0, epochs):\n\n    print(f'Beginning epoch {epoch_i + 1} of {epochs}')\n\n    t0 = time.time()\n\n    total_train_loss = 0\n\n    model.train()\n\n    for step, batch in enumerate(train_dataloader):\n\n        b_input_ids = batch[0].to(device)\n        b_labels = batch[0].to(device)\n        b_masks = batch[1].to(device)\n\n        model.zero_grad()        \n\n        outputs = model(  b_input_ids,\n                          labels=b_labels, \n                          attention_mask = b_masks,\n                          token_type_ids=None\n                        )\n\n        loss = outputs[0]  \n\n        batch_loss = loss.item()\n        total_train_loss += batch_loss\n\n        # Get sample every 100 batches.\n        if step % sample_every == 0 and not step == 0:\n\n            elapsed = format_time(time.time() - t0)\n            print(f'Batch {step} of {len(train_dataloader)}. Loss:{batch_loss}. Time:{elapsed}')\n\n            model.eval()\n\n            sample_outputs = model.generate(\n                                    bos_token_id=random.randint(1,30000),\n                                    do_sample=True,   \n                                    top_k=50, \n                                    max_length = 200,\n                                    top_p=0.95, \n                                    num_return_sequences=1\n                                )\n            for i, sample_output in enumerate(sample_outputs):\n                  print(f'Example output: {tokenizer.decode(sample_output, skip_special_tokens=True)}')\n            \n            model.train()\n\n        loss.backward()\n\n        optimizer.step()\n\n        scheduler.step()\n\n    # Calculate the average loss over all of the batches.\n    avg_train_loss = total_train_loss \/ len(train_dataloader)       \n    \n    # Measure how long this epoch took.\n    training_time = format_time(time.time() - t0)\n\n    print(f'Average Training Loss: {avg_train_loss}. Epoch time: {training_time}')\n\n    t0 = time.time()\n\n    model.eval()\n\n    total_eval_loss = 0\n    nb_eval_steps = 0\n\n    # Evaluate data for one epoch\n    for batch in val_dataloader:\n        \n        b_input_ids = batch[0].to(device)\n        b_labels = batch[0].to(device)\n        b_masks = batch[1].to(device)\n        \n        with torch.no_grad():        \n\n            outputs  = model(b_input_ids,  \n                             attention_mask = b_masks,\n                             labels=b_labels)\n          \n            loss = outputs[0]  \n            \n        batch_loss = loss.item()\n        total_eval_loss += batch_loss        \n\n    avg_val_loss = total_eval_loss \/ len(val_dataloader)\n    \n    validation_time = format_time(time.time() - t0)    \n\n    print(f'Validation loss: {avg_val_loss}. Validation Time: {validation_time}')\n\n    # Record all statistics from this epoch.\n    training_stats.append(\n        {\n            'epoch': epoch_i + 1,\n            'Training Loss': avg_train_loss,\n            'Valid. Loss': avg_val_loss,\n            'Training Time': training_time,\n            'Validation Time': validation_time\n        }\n    )\n\nprint(f'Total training took {format_time(time.time()-total_t0)}')","3bac8979":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\npd.set_option('precision', 2)\ndf_stats = pd.DataFrame(data=training_stats)\ndf_stats = df_stats.set_index('epoch')\n\nsns.set(style='darkgrid')\n\nsns.set(font_scale=1.5)\nplt.rcParams[\"figure.figsize\"] = (12,6)\n\nplt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\nplt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n\nplt.title(\"Training & Validation Loss\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\nxt = [i for i in range(1,epochs+1)]\nplt.xticks(xt)\n\nplt.show()\n","75726963":"import os\nout_dir = '\/ElonTweets'\nmodel_to_save = model.module if hasattr(model, 'module') else model\nmodel_to_save.save_pretrained('.\/')\ntokenizer.save_pretrained('.\/')","ca3ddac4":"model.eval()\n\nprompt = \"<|sos|>\"\n\ngenerated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\ngenerated = generated.to(device)\n\nsample_outputs = model.generate(\n                                generated, \n                                do_sample=True,   \n                                top_k=50, \n                                max_length = 300,\n                                top_p=0.95, \n                                num_return_sequences=10\n                                )\n\nfor i, sample_output in enumerate(sample_outputs):\n    print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))","e6d7371a":"#Load Saved Model\n\nmodel = GPT2LMHeadModel.from_pretrained('.\/')\ntokenizer = GPT2Tokenizer.from_pretrained('.\/')\nmodel.to(device)","7cd36943":"test_tweet = 'Why would I ever'","98146117":"def generateTweets(prompt, n_return):\n    model.eval()\n\n    prompt = prompt\n\n    generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n    generated = generated.to(device)\n\n    sample_outputs = model.generate(\n                                    generated, \n                                    do_sample=True,   \n                                    top_k=500, \n                                    max_length = 300,\n                                    top_p=0.95, \n                                    num_return_sequences=n_return\n                                    )\n\n    for i, sample_output in enumerate(sample_outputs):\n        print(\"{}: {}\\n\\n\".format(i, tokenizer.decode(sample_output, skip_special_tokens=True)))","5941a0cd":"generateTweets(test_tweet, 3)","da764fba":"test_tweet = 'Tesla is going to'","780225a6":"generateTweets(test_tweet, 3)","91b2a807":"## 1.2. Importing the data\n### The data is organized in 12 .csv files, I loaded each one independently and then concatenated all of them into a single dataframe.","1bc8c327":"# GPT-2\n## Training\n## What is GPT-2 and why do we use it?\n### GPT-2 Is a transformer-based architecture NN that was trained on a massive amount of unlabeled raw text data in a self-supervised fashion in order to predict the next word in a given sentence, and the attempts at using it in a transfer-learning manner have been very successful so far. <br \/>  <br \/> You can use it yourself to create models that do anything from answering questions, generating stories, to mimicing someone on Twitter- which we're going to do here.\n#### Next we create a custom dataloader for our tweets using torch Dataset. \\ Each entry in the dataset will be two tensors, one which is the encoding for the string and one which is the attention mask","04f7be10":"# 2. Data\n## 2.1 Preprocessing the text\n### In order to handle the raw text data, we should first preprocess it, when doing transfer-learning you should always pre-process the data in the same way it was processed when the model was first trained, so we'll use the GPT2Tokenizer class from transformers and define the tokens.\n### Add three new tokens in the pre-trained GPT2 tokenizer: \\ <|sos|> : start of sentence \\ <|eos|> : end of sentence \\ <|pad|> : padding token","a56a3481":"### Example outputs, manually testing the model.","021c289c":"# Generating Elon Musk Tweets Using GPT-2\n### IMPORTANT: This notebook was made by [ajay19](https:\/\/www.kaggle.com\/ajay19) and his original work can be found [here](https:\/\/www.kaggle.com\/ajay19\/transformers-tweet-generation\/comments), I applied his notebook to the dataset I collected because I wanted to mess around with transformers for a bit, have fun!.\n![wp2048440.jpg](attachment:ba9d5bef-af99-4479-926a-fc388f934aeb.jpg)\n\n1. Preparations\n    1. Importing the necessary libraries\n    2. Importing the data\n2. Data\n    1. Preprocessing the text\n3. GPT-2\n    1. Training\n    2. Testing\n# 1. Preparations\n## 1.1. Importing the necessary libraries for data loading","fbc6e8ac":"##### END ","c2ee7852":"## 2.2. Testing\n#### Evaluate the model by changing the prompt below and manually inspecting the results, have fun!","13ce94cd":"### Take a quick look at the data","ed15e90d":"### We only need the tweets themselves and can get rid of all of the metadata, so I saved the tweets in a single variable and deleted the rest of the dataframe","24d6d46c":"### The time for training will depend on the number of samples divided by the batch size, then multiplied by the epochs, I encourage people to expirement with hyperparameters such as batch size, epochs, learning rate, LR scheduling and optimizers, and compare results. <br \/>\n### I'm only going to use 3 epochs for now, I recommend doing at least 5-6 if you want good results."}}