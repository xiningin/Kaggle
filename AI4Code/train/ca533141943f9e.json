{"cell_type":{"fac7172f":"code","80da48b8":"code","91976b8b":"code","c7405fc8":"code","20d6f7ee":"code","f44cb1d6":"code","d90f2701":"code","026c512f":"code","698663cc":"code","30714e3e":"code","a369fae9":"code","ab95da12":"code","be917787":"code","bd17098e":"code","b5ecd002":"code","9f1e1f2c":"code","e7754f5e":"code","6c35512c":"code","06f4b320":"code","07458d8f":"code","950854d9":"code","bb077065":"code","0dda5877":"code","84263ccf":"code","bc237d02":"code","cdba6767":"code","d4e3ef2a":"code","24c5ce91":"code","b727b0b2":"code","f864f01c":"code","7f2d842d":"code","537e27f8":"markdown","c83c9a0a":"markdown","99ce9149":"markdown","084e8a31":"markdown","96e59808":"markdown","f1fae0e7":"markdown","2dca195b":"markdown","b4e6aff0":"markdown","a64aa0d3":"markdown","d6652550":"markdown","0a0de29d":"markdown","066660cf":"markdown","86c65350":"markdown","78103aea":"markdown","c2bea78d":"markdown","a2d725ba":"markdown","a09c6278":"markdown"},"source":{"fac7172f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport glob\nfrom keras.preprocessing import sequence\nimport keras\nimport tensorflow as tf\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom PIL import Image\nfrom wordcloud import WordCloud\nimport nltk\nimport spacy\nfrom nltk import word_tokenize\nfrom nltk.util import ngrams\nimport re\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","80da48b8":"\ntext = \"\"\nfolder_name = \"..\/input\/friends-tv-series-screenplay-script\/\"\nfor f in glob.glob(folder_name + '\/*.txt'):\n    temp = open(f,'r')    \n    text += temp.read()\n    temp.close()","91976b8b":"print ('Length of text: {} characters'.format(len(text)))","c7405fc8":"print(text[:500])","20d6f7ee":"# adding screenplay notes to stopwords\nnlp = spacy.load(\"en\")\nnlp.Defaults.stop_words |= {\"d\",\"ll\",\"m\",\"re\",\"s\",\"ve\", \"t\", \"oh\", \"uh\", \"na\", \"okay\",\n                           \"didn\",\"don\",\"gon\",\"j\",\"hm\",\"um\",\"dr\",\"room\",\"int\", \"ext\", \n                           \"cut\", \"day\", \"night\", \"theme\", \"tune\",\"music\", \"ends\",\"view\",\"opening credits scene\", \n                            \"commercial break scene\", \"hey hey hey\", \"hey\", \"closing credits scene\",\"scene\",\n                            \"closeup\", 'freshly', 'squeezed', 'fade'}\nstopwords = nlp.Defaults.stop_words","f44cb1d6":"all_words = nltk.tokenize.word_tokenize(text.lower())\nall_words_no_stop = nltk.FreqDist(w.lower() for w in all_words if w not in stopwords)","d90f2701":"def color_func(word, font_size, position, orientation, random_state=None,\n                    **kwargs):\n    return \"hsl(1, 90%, 47%)\"\n\nwc = WordCloud(background_color=\"black\", max_words=1000,\n               stopwords=stopwords, contour_width=4, contour_color='steelblue')\n\nwc.generate(\" \".join(all_words_no_stop.keys()))\n\nplt.figure(figsize=(18, 10))\nplt.imshow(wc.recolor(color_func=color_func, random_state=3),interpolation=\"bilinear\")\nplt.axis(\"off\");","026c512f":"print(\"Chandler has been called {} times\".format(all_words_no_stop['chandler']))\nprint(\"Joey has been called {} times\".format(all_words_no_stop['joey']))\nprint(\"Monica has been called exactly {} times\".format(all_words_no_stop['monica']))\nprint(\"Ross has been called exactly {} times\".format(all_words_no_stop['ross']))\nprint(\"Rachel has been called exactly {} times\".format(all_words_no_stop['rachel']))\nprint(\"Phoebe has been called exactly {} times\".format(all_words_no_stop['phoebe']))","698663cc":"print(\"{} times Joey used how you doin?\".format(all_words_no_stop['doin']))","30714e3e":"print(\"Joey asked for sandwiches {} times\".format(all_words_no_stop['sandwich']))","a369fae9":"vocab = sorted(set(text))\n# Creating a mapping from unique characters to indices\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\ndef text_to_int(text):\n  return np.array([char2idx[c] for c in text])\n\ntext_as_int = text_to_int(text)","ab95da12":"#extra step, better to do it\ndef int_to_text(ints):\n  try:\n    ints = ints.numpy()\n  except:\n    pass\n  return ''.join(idx2char[ints])\n\nprint(int_to_text(text_as_int[:13]))","be917787":"\nprint(\"Text:\", text[:13])\nprint(\"Encoded:\", text_to_int(text[:13]))","bd17098e":"seq_length = 1000  # length of sequence for a training example\nexamples_per_epoch = len(text)\/\/(seq_length+1)\n\n# Create training examples \/ targets\nchar_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n\nsequences = char_dataset.batch(seq_length+1, drop_remainder=True)","b5ecd002":"def split_input_target(chunk):  # for the example: hello\n    input_text = chunk[:-1]  # hell\n    target_text = chunk[1:]  # ello\n    return input_text, target_text  # hell, ello\n\ndataset = sequences.map(split_input_target)  # we use map to apply the above function to every entry","9f1e1f2c":"BATCH_SIZE = 64\nVOCAB_SIZE = len(vocab)  # vocab is number of unique characters\nEMBEDDING_DIM = 256\nRNN_UNITS = 1024\n\n# Buffer size to shuffle the dataset\n# (TF data is designed to work with possibly infinite sequences,\n# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n# it maintains a buffer in which it shuffles elements).\nBUFFER_SIZE = 10000\n\ndata = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)","e7754f5e":"\n\ndef build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n  model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                              batch_input_shape=[batch_size, None]),\n    tf.keras.layers.LSTM(rnn_units,\n                        return_sequences=True,\n                        stateful=True,\n                        recurrent_initializer='glorot_uniform'),\n    tf.keras.layers.Dense(vocab_size)\n  ])\n  return model\n\nmodel = build_model(VOCAB_SIZE,EMBEDDING_DIM, RNN_UNITS, BATCH_SIZE)\nmodel.summary()","6c35512c":"for input_example_batch, target_example_batch in data.take(1):\n  example_batch_predictions = model(input_example_batch)  \n  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")  # print out the output shape","06f4b320":"\nprint(len(example_batch_predictions))\nprint(example_batch_predictions)","07458d8f":"\npred = example_batch_predictions[0]\nprint(len(pred))\nprint(pred)\n# notice this is a 2d array of length 1000, where each interior array is the prediction for the next character at each time step","950854d9":"\ntime_pred = pred[0]\nprint(len(time_pred))\nprint(time_pred)","bb077065":"# If we want to determine the predicted character we need to sample the output distribution (pick a value based on probabillity)\nsampled_indices = tf.random.categorical(pred, num_samples=1)\n\n# now we can reshape that array and convert all the integers to numbers to see the actual characters\nsampled_indices = np.reshape(sampled_indices, (1, -1))[0]\npredicted_chars = int_to_text(sampled_indices)\n\npredicted_chars  # and this is what the model predicted for training sequence 1","0dda5877":"def loss(labels, logits):\n  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)","84263ccf":"model.compile(optimizer='adam', loss=loss)","bc237d02":"# Directory where the checkpoints will be saved\ncheckpoint_dir = '.\/training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","cdba6767":"history = model.fit(data, epochs=80, callbacks=[checkpoint_callback])","d4e3ef2a":"model = build_model(VOCAB_SIZE, EMBEDDING_DIM, RNN_UNITS, batch_size=1)","24c5ce91":"model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\nmodel.build(tf.TensorShape([1, None]))","b727b0b2":"#checkpoint_num = 10\n#model.load_weights(tf.train.load_checkpoint(\".\/training_checkpoints\/ckpt_\" + str(checkpoint_num)))\n#model.build(tf.TensorShape([1, None]))","f864f01c":"def generate_text(model, start_string):\n  # Evaluation step (generating text using the learned model)\n\n  # Number of characters to generate\n  num_generate = 1500\n\n  # Converting our start string to numbers (vectorizing)\n  input_eval = [char2idx[s] for s in start_string]\n  input_eval = tf.expand_dims(input_eval, 0)\n\n  # Empty string to store our results\n  text_generated = []\n\n  # Low temperatures results in more predictable text.\n  # Higher temperatures results in more surprising text.\n  # Experiment to find the best setting.\n  temperature = 1.0\n\n  # Here batch size == 1\n  model.reset_states()\n  for i in range(num_generate):\n      predictions = model(input_eval)\n      # remove the batch dimension\n    \n      predictions = tf.squeeze(predictions, 0)\n\n      # using a categorical distribution to predict the character returned by the model\n      predictions = predictions \/ temperature\n      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n      # We pass the predicted character as the next input to the model\n      # along with the previous hidden state\n      input_eval = tf.expand_dims([predicted_id], 0)\n\n      text_generated.append(idx2char[predicted_id])\n\n  return (start_string + ''.join(text_generated))","7f2d842d":"inp = input(\"Type a starting string: \")\nprint(generate_text(model, inp))","537e27f8":"**Integer to text**","c83c9a0a":"**Testing if conversion is fine**","99ce9149":"**Input a string and see the magic(could be 18+, sorry!)**","084e8a31":"**Word Cloud**","96e59808":"**And finally well look at a prediction at the first timestep**","f1fae0e7":"**Note: Above is trained only on 80 epochs. Resources get exhausted after that you should go for more epochs on local or cloud and you'll much more interesting play!**","2dca195b":"# **EDA**","b4e6aff0":"# **Importing Libraries**","a64aa0d3":"**Conversion into array like object**","d6652550":"**Lets look at how part of our text is encoded**","0a0de29d":"**Using LSTM layers for NLP**","066660cf":"# **Finally, generating the play!**","86c65350":"**We can see that the predicition is an array of 64 arrays, one for each entry in the batch**","78103aea":"# **Play Generator using NLP**","c2bea78d":"**Ask our model for a prediction on our first batch of training data (64 entries)**","a2d725ba":"**Text to integers**","a09c6278":"**Lets examine one prediction**"}}