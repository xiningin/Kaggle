{"cell_type":{"7bda5671":"code","5815d802":"code","7aea024f":"code","e55de2d4":"code","371a7b2b":"code","2a117550":"code","d40441a1":"code","3f1cd674":"code","2448e22d":"code","055cc12d":"code","532d0390":"code","54a44fe3":"code","490761ca":"markdown","567b37c1":"markdown","d2b5cd59":"markdown","ed04c890":"markdown","e92fbcae":"markdown","f4aab96d":"markdown","72a5b1d1":"markdown","2cf374d4":"markdown","46623c39":"markdown","1f593da0":"markdown"},"source":{"7bda5671":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5815d802":"np.random.seed(0)   # If you use the same seed you will get the same random numbers, good for repeatability\n\ndef create_target():\n    x = []\n    y = []\n    for i in range(100):\n        a = np.random.random() * 2 * np.pi   # Angle from the origin\n        r = np.random.random() * 5   # This is the distance\n        x.append(np.cos(a) * r)   # Getting Cartesian coords from polar coords\n        y.append(np.sin(a) * r)   # Getting Cartesian coords from polar coords\n    return x, y\n        \n            \nX, y = create_target()   # Create lists\n\n\n# Now we can plot the points\nfig = plt.figure()\nplt.scatter(X, y)\nax = fig.add_subplot(111)\nplt.xlim(-10,10)\nplt.ylim(-10,10)\nax.set_aspect('equal')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.title(\"One evenly distributed circle\")\nplt.show()\n            ","7aea024f":"def create_outside():\n    x = []\n    y = []\n    for i in range(100):\n        a = np.random.ranf() * 2 * np.pi   # Angle from the origin\n        r = np.random.ranf() * 5 + 5   # This is the distance, going to create a ring around the previous circle because of the +5\n        x.append(np.cos(a) * r)   # Getting Cartesian coords from polar coords\n        y.append(np.sin(a) * r)   # Getting Cartesian coords from polar coords\n    return x, y\n\nX2, y2 = create_outside()\n\nfig2 = plt.figure()\nplt.scatter(X, y, c='b')   # The first group is blue\nplt.scatter(X2, y2, c='r')   # The second group is red\nax2 = fig2.add_subplot(111)\nplt.xlim(-10, 10)\nplt.ylim(-10, 10)\nax2.set_aspect('equal')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.show()","e55de2d4":"data = pd.DataFrame(list(zip(X, y)), columns=['X', 'y'])\ndata['Target'] = 0   # The first set of points, closer to the origin, will be identified as 0\nprint(data.shape)\ndata.head()","371a7b2b":"data2 = pd.DataFrame(list(zip(X2, y2)), columns=['X', 'y'])\ndata2['Target'] = 1   # The second set of points will be identified as 1\nprint(data2.shape)\ndata2.head()","2a117550":"dataframes = [data, data2]\njoined_data = pd.concat(dataframes)\njoined_data.sample(n=5)","d40441a1":"train = joined_data.drop(['Target'], axis=1)\ntarget = joined_data['Target']\n\ntrain.head()","3f1cd674":"target.head()","2448e22d":"from sklearn.model_selection import train_test_split\ntrain, test_train, target, test_target = train_test_split(train, target, test_size=0.2)   # Note that the test_size is 0.2, or 20% of the entire dataset, while 80% is used for training","055cc12d":"from sklearn.svm import SVC\nmodel = SVC(kernel='rbf', gamma=0.1, C=10, random_state=0)   # The model is a Support Vector Classifier with a Radial Basis Function kernel. gamma and C are adjustable hyperparameters.\nmodel.fit(train, target)","532d0390":"training_accuracy = round(model.score(train, target) * 100, 2)\ntraining_accuracy","54a44fe3":"accuracy = round(model.score(test_train, test_target) * 100, 2)\naccuracy","490761ca":"Let's split train and target into the training group and testing group with sklearn. The general rule is to use 80% of the data for training, and 20% for testing. Different splits will still work though, although the accuracy of the model may vary by a little.","567b37c1":"Well it seems that we were not punished by overfitting. If one were to add some outliers into the data, the results might be more interesting. Strict adherence and flexibility are often at odds for machine learning models due to the nature of outliers in natural data. But it seems that for this randomly-generated pattern, the model has figured it out completely.","d2b5cd59":"This is our joined dataset, however, we must separate train and target","ed04c890":"Now that we have all of our data in dataframe form, we can join them together by row.","e92fbcae":"Well, all the points are inside of a circle with radius 5 and centered on the origin. We can now create our second group of points.","f4aab96d":"Let us inspect the accuracy of the model based on the training data now","72a5b1d1":"All of our points are in the correct locations, so now let's join the two sets of points together into the same dataframe, with a column \"Target\" that defines which group the coordinates belong to.","2cf374d4":"At this point, we can create our model and fit it.","46623c39":"Well, 99% accuracy on the training accuracy is pretty good! However, it's all about the testing\/validation accuracy so let's get that.","1f593da0":"First we must create the data, or import it if you are using a dataset. However, I'm just going to create two groups of points.\nThe first group will be 100 points in a circle of radius 5 centered on the origin."}}