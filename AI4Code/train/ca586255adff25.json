{"cell_type":{"21901ca0":"code","f5d57019":"code","039b5d42":"code","7e670b67":"code","8ad9e135":"code","cbf0922c":"code","8c5550e6":"code","d3054096":"code","6ba219f6":"code","8f3c7d7c":"code","5185e0bc":"code","7031657e":"code","0a2c845a":"code","4d201006":"code","4e6cfa1c":"code","cd2b04ac":"code","a01ffc29":"code","ce7f0e9b":"code","99b3993c":"markdown","8977751d":"markdown","e93278a2":"markdown","9e64ff4e":"markdown"},"source":{"21901ca0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-whitegrid')\nimport seaborn as sns\n\nfrom itertools import islice\nfrom collections import Counter\nimport re\n\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom scipy.stats import pearsonr\nimport wordcloud\nfrom textblob import TextBlob\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f5d57019":"def jaccard(chain_one, chain_two):\n    \"\"\"\n        Returns the jaccard simalarity between two strings of text 0<=jaccard<=1\n    chain_one: type string\n    chain_two: type string\n    \"\"\"\n    chain_one = chain_one.split()\n    chain_two = chain_two.split()\n    \n    anb = set(chain_one).intersection(set(chain_two))\n    \n    \n    return len(anb)\/(len(chain_one) + len(chain_two) - len(anb) + .01)","039b5d42":"def clean_text(text):\n    text = re.sub(r\"[^a-zA-Z]\",\" \",text)\n    text =  re.sub(r\"\\b\\w{1}\\b\", \"\", text)\n    text = text.lower()\n    text = re.sub(r\"http\", \"\",text)\n    \n    tokens = text.split()\n    \n    text = \" \".join([word for word in tokens if word not in ENGLISH_STOP_WORDS])\n    \n    return text\n    ","7e670b67":"def visualize_most_common_words(frame, text_col_name = \"text\", class_column = None, return_top = 10, cleanse_text = False):\n    all_words = list()\n    colors = ['orange','red', 'green']\n    \n    if cleanse_text:\n        frame[text_col_name] = frame[text_col_name].fillna('').apply(lambda x: clean_text(x))\n    \n        \n    if not class_column:\n        for _, row in frame.fillna('').iterrows():\n            for word in row[text_col_name].split():\n                all_words.append(word)\n        words_dict = Counter(all_words).most_common(return_top)\n        \n        words_dict = dict(words_dict)\n        \n        \n        \n        fig, ax =plt.subplots(nrows = 1, ncols = 1, figsize = (12,7))\n        \n        ax.barh(list(words_dict.keys()), list(words_dict.values()), edgecolor = 'black', color = 'steelblue')\n    else:\n\n        classes = frame[class_column].unique()\n        fig, ax =plt.subplots(nrows = 1, ncols = classes.size, figsize = (20,9))\n        \n        for i, class_ in enumerate(classes):\n            for _, row in frame[frame[class_column] == class_].fillna('').iterrows():\n                for word in row[text_col_name].split():\n                    all_words.append(word)\n            words_dict = Counter(all_words).most_common(return_top)\n\n            words_dict = dict(words_dict)\n\n            ax[i].barh(list(words_dict.keys()), list(words_dict.values()), edgecolor = 'black', color = colors[i])\n            ax[i].set_title(class_)\n            \n            all_words = list()\n        \n                ","8ad9e135":"def visualize_text_lenght(frame, text_col_name = \"text\", class_column = None, cleanse_text = False, separated_plots = True):\n    colors = ['orange','red', 'green']\n    \n    if cleanse_text:\n        frame[text_col_name] = frame[text_col_name].fillna('').apply(lambda x: clean_text(x))\n        \n    frame[\"doc_lenght\"] = frame[text_col_name].fillna('').apply(lambda x: len(x.split()))\n    \n    avg_length = np.mean(frame.doc_lenght)\n    \n    if not separated_plots:\n        fig, ax = plt.subplots(nrows=1, ncols=1, figsize = (12,7))\n        if not class_column:\n            sns.kdeplot(frame[\"doc_lenght\"],color = 'orange', alpha = .5, shade = True, ax = ax)\n            ax.set_title(f\"Documents lenght distribution\\nMean words by document: {int(avg_length)}\")\n            ax.set_xlabel(\"Number of words\")\n        else:\n            for color, class_ in zip(colors, frame[class_column].unique()):\n                sns.kdeplot(frame[frame[class_column] == class_][\"doc_lenght\"],color = color, alpha = .5, shade = True, ax = ax, label = class_)\n            ax.set_title(f\"Documents lenght distribution by class\\nMean words by document: {int(avg_length)}\")\n            ax.set_xlabel(\"Number of words\")\n    else:\n        classes = frame[class_column].unique()\n        nrows = 1\n        ncols = classes.size\n        fig, ax = plt.subplots(nrows = nrows, ncols = ncols, figsize = (20,7))\n        for i, class_ in enumerate(classes):\n            avg_length = np.mean(frame[frame[class_column] == class_].doc_lenght)\n            sns.kdeplot(frame[frame[class_column] == class_][\"doc_lenght\"],color = colors[i], alpha = .5, shade = True, ax = ax[i], label = class_)\n            ax[i].set_title(f\"Class: {class_}\\nMean words by document: {int(avg_length)}\")\n            ax[i].set_xlabel(\"Number of words\")\n            \n            \n            \n                                                       \n                                                       \n    ","cbf0922c":"def visualize_text_vs_selected_lenght(frame, text_col_name = \"text\", sel_text_col_name = \"selected_text\", class_column = None, cleanse_text = False, separated_plots = True):\n    colors = ['orange','red', 'green']\n    if cleanse_text:\n        frame[text_col_name] = frame[text_col_name].fillna('').apply(lambda x: clean_text(x))\n        frame[sel_text_col_name] = frame[sel_text_col_name].fillna('').apply(lambda x: clean_text(x))\n    frame[\"doc_lenght\"] = frame[text_col_name].fillna('').apply(lambda x: len(x.split()))\n    frame[\"sel_doc_lenght\"] = frame[sel_text_col_name].fillna('').apply(lambda x: len(x.split()))\n    \n    text_corr = pearsonr(frame[\"doc_lenght\"], frame[\"sel_doc_lenght\"])[0]\n    \n    if not separated_plots:\n        fig, ax = plt.subplots(nrows=1, ncols=1, figsize = (12,7))\n        if not class_column:\n            ax.scatter(frame[\"doc_lenght\"], frame[\"sel_doc_lenght\"],facecolors = 'none',edgecolor = 'orange')\n            ax.set_title(f\"Documents lenght distribution\\Correlation for words in selected and original text: {round(text_corr, 3)}\")\n            ax.set_xlabel(\"Text words lenght\")\n            ax.set_ylabel(\"Selected text words lenght\")\n        else:\n            for color, class_ in zip(colors, frame[class_column].unique()):\n                ax.scatter(frame[\"doc_lenght\"], frame[\"sel_doc_lenght\"],facecolors = 'none',c = frame[class_column].map({'positive':'green','negative':'red','neutral':'orange'}))\n            ax.set_title(f\"Documents lenght distribution by class\\nCorrelation for words in selected and original text: {round(text_corr, 3)}\")\n            ax.set_xlabel(\"Text words lenght\")\n            ax.set_ylabel(\"Selected text words lenght\")\n    else:\n        classes = frame[class_column].unique()\n        nrows = 1\n        ncols = classes.size\n        fig, ax = plt.subplots(nrows = nrows, ncols = ncols, figsize = (20,7))\n        for i, class_ in enumerate(classes):\n            text_corr = pearsonr(frame[frame[class_column] == class_][\"doc_lenght\"], frame[frame[class_column] == class_][\"sel_doc_lenght\"])[0]\n            ax[i].scatter(frame[frame[class_column] == class_][\"doc_lenght\"], frame[frame[class_column] == class_][\"sel_doc_lenght\"],facecolors = 'none',color = colors[i])\n            ax[i].set_title(f\"Class: {class_}\\nMean words by document: {round(text_corr,3)}\")\n            ax[i].set_xlabel(\"Text words lenght\")\n            ax[i].set_ylabel(\"Selected text words lenght\")\n            \n    ","8c5550e6":"def visualize_jaccard_similarity(frame, document_one_col_name = \"text\", document_two_col_name = \"selected_text\", class_column = None, cleanse_text = False, separated_plots = True):\n    colors = ['orange','red', 'green']\n    if cleanse_text:\n        frame[document_one_col_name] = frame[document_one_col_name].fillna('').apply(lambda x: clean_text(x))\n        frame[document_two_col_name] = frame[document_two_col_name].fillna('').apply(lambda x: clean_text(x))\n    frame[\"jaccard_similarity\"] = frame.apply(lambda x: jaccard(x[document_one_col_name] ,x[document_two_col_name]), axis = 1)\n    \n    \n    avg_js = np.mean(frame.jaccard_similarity)\n    \n    if not separated_plots:\n        fig, ax = plt.subplots(nrows=1, ncols=1, figsize = (12,7))\n        if not class_column:\n            sns.kdeplot(frame[\"jaccard_similarity\"],color = 'orange', alpha = .5, shade = True, ax = ax)\n            ax.set_title(f\"Documents lenght distribution\\nMean words by document: {int(avg_js)}\")\n            ax.set_xlabel(\"Number of words\")\n        else:\n            for color, class_ in zip(colors, frame[class_column].unique()):\n                sns.kdeplot(frame[frame[class_column] == class_][\"jaccard_similarity\"],color = color, alpha = .5, shade = True, ax = ax, label = class_)\n            ax.set_title(f\"Documents Jaccard similarityon by class\\nJaccard similarity by document: {int(avg_js)}\")\n            ax.set_xlabel(\"Jaccard similarity\")\n    else:\n        classes = frame[class_column].unique()\n        nrows = 1\n        ncols = classes.size\n        fig, ax = plt.subplots(nrows = nrows, ncols = ncols, figsize = (20,7))\n        for i, class_ in enumerate(classes):\n            avg_js = np.mean(frame[frame[class_column] == class_].jaccard_similarity)\n            sns.kdeplot(frame[frame[class_column] == class_][\"jaccard_similarity\"],color = colors[i], alpha = .5, shade = True, ax = ax[i], label = class_)\n            ax[i].set_title(f\"Class: {class_}\\nJaccard similarity: {round(avg_js, 3)}\")\n            ax[i].set_xlabel(\"Jaccard similarity\")","d3054096":"def make_word_cloud(frame, text_col_name = 'text', class_column = None):\n    colors = ['Oranges','Reds', 'Greens']\n    if not class_column:\n        wc = wordcloud.WordCloud(\n            width= 900,\n            height= 500,\n            max_words = 500,\n            max_font_size= 100,\n            relative_scaling= .5,\n            colormap = 'Oranges',\n            normalize_plurals = True,\n            stopwords = ENGLISH_STOP_WORDS\n        ).generate(frame[text_col_name].to_string())\n        \n        plt.figure(figsize = (12,7))\n        plt.imshow(wc, interpolation= 'bilinear')\n        plt.axis(\"off\")\n    else:\n\n        classes = frame[class_column].unique()\n        fig, ax =plt.subplots(nrows = 1, ncols = classes.size, figsize = (20,20))\n        \n        for i, class_ in enumerate(classes):\n            wc = wordcloud.WordCloud(\n            width= 900,\n            height= 500,\n            max_words = 500,\n            max_font_size= 100,\n            relative_scaling= .5,\n            colormap = colors[i],\n            normalize_plurals = True,\n            stopwords = ENGLISH_STOP_WORDS\n        ).generate(frame[frame[class_column] == class_][text_col_name].to_string())\n        \n            ax[i].imshow(wc, interpolation= 'bilinear')\n            ax[i].axis(\"off\")\n            ax[i].set_title(class_)","6ba219f6":"train = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')","8f3c7d7c":"train.sample(7)","5185e0bc":"train.info()","7031657e":"train = train[train.text.notnull()]","0a2c845a":"train.sentiment.value_counts(normalize = True).plot.bar(edgecolor = 'black', color = 'steelblue')\nplt.title(\"Class distribution\");","4d201006":"visualize_text_lenght(train, cleanse_text = True, separated_plots= True, class_column= 'sentiment', text_col_name= 'selected_text')","4e6cfa1c":"visualize_jaccard_similarity(train, class_column= 'sentiment', separated_plots= True, cleanse_text= False)","cd2b04ac":"visualize_text_vs_selected_lenght(train, class_column= 'sentiment', separated_plots= True)","a01ffc29":"visualize_most_common_words(train, cleanse_text= True, class_column= 'sentiment', return_top= 20)","ce7f0e9b":"make_word_cloud(train, class_column= 'sentiment')","99b3993c":"# Initial modeling\nFor this innitial approach I'm going to use Texblob to measure the polarity of each word and we are going to keep the n most important for each class","8977751d":"## This plot show the relation ship between original text number of words and selected words by class\n### As we can see there is no linear relationship between those two","e93278a2":"## By examin the most frequent terms by class we can notice that there are wprds with a \"positive context\" that defined both negative and positive texts. Say for example the word like","9e64ff4e":"## From these plots we can see that the number of words that define neutral documents is much higher than positive and negative ones, so It might imply a most difficult task for finfing the correct words in positives and negatives since for neutrals, the lenght of selected text is closer to the original one (see scatter plot below)"}}