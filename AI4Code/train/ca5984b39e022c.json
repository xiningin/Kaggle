{"cell_type":{"06538f2c":"code","01216e0d":"code","426018a7":"code","aebd98bb":"code","05e67bda":"code","aa35958e":"code","1b67d508":"code","ece83677":"code","86dabbf0":"code","be54e0a6":"code","6874b5ba":"code","3ba6af69":"markdown","24415164":"markdown","87a0c245":"markdown"},"source":{"06538f2c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","01216e0d":"import csv\nfrom dataclasses import dataclass\nfrom typing import List, Dict\nimport os\nimport random\nimport time\nfrom datetime import timedelta\nfrom logging import handlers\nimport logging\n\n\nimport torch\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nfrom torch import Tensor\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset, DataLoader,RandomSampler\nfrom transformers import BertTokenizer, BertConfig\nimport torch.nn as nn\nfrom transformers import BertPreTrainedModel, BertModel\nfrom transformers.modeling_outputs import TokenClassifierOutput\n\nimport numpy as np\nfrom torch.utils.data import DataLoader, RandomSampler\nfrom tqdm import tqdm\nfrom transformers import BertConfig, AdamW, get_linear_schedule_with_warmup\n","426018a7":"from dataclasses import dataclass, field\n\nWORK_DIR = \".\/\"\n# BERT_PATH = \"\/home\/sl\/workspace\/data\/nlp\/bert-base-chinese\"\n# BERT_PATH = \"\/home\/sl\/workspace\/data\/nlp\/bert-ner\"\nBERT_PATH = \"bert-base-uncased\"\n\nSEMEVAL_TEST8_DATA_DIR = \"..\/input\/semeval-2010-task-8-dataset\"\nDATA_DIR = SEMEVAL_TEST8_DATA_DIR\n# BERT_MODEL_NAME = \"bert-base-chinese\"\nDATASET_TYPE_NAME = \"semeval_2010_task8\"\nBERT_MODEL_NAME = BERT_PATH\nADDITIONAL_SPECIAL_TOKENS = [\"<e1>\", \"<\/e1>\", \"<e2>\", \"<\/e2>\"]\n\n@dataclass\nclass ModelArguments:\n    dropout_rate: float = field(default=0.1, metadata={\"help\": \"\u9884\u8bad\u7ec3\u6a21\u578b\u8f93\u51fa\u5411\u91cf\u8868\u793a\u7684dropout\"})\n    num_labels: int = field(default=19, metadata={\"help\": \"\u9700\u8981\u9884\u6d4b\u7684\u6807\u7b7e\u6570\u91cf\"})\n    model_name_or_path: str = field(default=BERT_PATH, metadata={\"help\": \"BERT\u6a21\u578b\u540d\u79f0\"})\n    task_name: str = field(default=\"R_BERT\", metadata={\"help\": \"\u4efb\u52a1\u540d\u79f0\"})\n\n    train_file: str = field(default=DATA_DIR + \"\/train.tsv\", metadata={\"help\": \"\u8bad\u7ec3\u6570\u636e\u7684\u8def\u5f84\"})\n    dev_file: str = field(default=DATA_DIR + \"\/dev.tsv\", metadata={\"help\": \"\u6d4b\u8bd5\u6570\u636e\u7684\u8def\u5f84\"})\n    test_file: str = field(default=DATA_DIR + \"\/test.tsv\", metadata={\"help\": \"\u6d4b\u8bd5\u6570\u636e\u7684\u8def\u5f84\"})\n\n    checkpoint_dir: str = field(default=WORK_DIR + \"\/models\/checkpoints\", metadata={\"help\": \"\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684checkpoints\u7684\u4fdd\u5b58\u8def\u5f84\"})\n    best_dir: str = field(default=WORK_DIR + \"\/models\/best\", metadata={\"help\": \"\u6700\u4f18\u6a21\u578b\u7684\u4fdd\u5b58\u8def\u5f84\"})\n    do_train: bool = field(default=True, metadata={\"help\": \"\u662f\u5426\u8fdb\u884c\u8bad\u7ec3\"})\n    do_eval: bool = field(default=True, metadata={\"help\": \"\u662f\u5426\u5728\u8bad\u7ec3\u65f6\u8fdb\u884c\u8bc4\u4f30\"})\n    do_predict: bool = field(default=True, metadata={\"help\": \"\u662f\u5426\u5728\u8bad\u7ec3\u65f6\u8fdb\u884c\u9884\u6d4b\"})\n    no_cuda: bool = field(default=False, metadata={\"help\": \"\u662f\u5426\u4e0d\u7528CUDA\"})\n    epoch: int = field(default=5, metadata={\"help\": \"\u8bad\u7ec3\u7684epoch\"})\n    train_batch_size: int = field(default=128, metadata={\"help\": \"\u8bad\u7ec3\u65f6\u7684batch size\"})\n    eval_batch_size: int = field(default=64, metadata={\"help\": \"\u8bc4\u4f30\u65f6\u7684batch size\"})\n    bert_model_name: str = field(default=BERT_PATH, metadata={\"help\": \"BERT\u6a21\u578b\u540d\u79f0\"})\n    output_dir: str = field(default=WORK_DIR + \"\/output\/\", metadata={\"help\": \"\u8f93\u51fa\u8def\u5f84\"})\n    eval_dir: str = field(default=WORK_DIR + \"\/\", metadata={\"help\": \"\u8f93\u51fa\u8def\u5f84\"})\n\n    eval_steps: int = field(default=4, metadata={\"help\": \"\u6bcf\u591a\u5c11step\u8fdb\u884c\u9a8c\u8bc1\u4e00\u6b21\"})\n    save_steps: int = field(default=4, metadata={\"help\": \"\u6bcf\u591a\u5c11step\u8fdb\u884c\u4fdd\u5b58\u4e00\u6b21\"})\n    learning_rate: float = field(default=2e-5, metadata={\"help\": \"The initial learning rate for Adam.\"})\n    adam_epsilon: float = field(default=1e-8, metadata={\"help\": \"Epsilon for Adam optimizer.\"})\n    warmup: float = field(default=0.05, metadata={\"help\": \"Linear warmup over warmup_steps.\"})\n    weight_decay: float = field(default=0.0, metadata={\"help\": \"weight_decay.\"})\n    max_grad_norm: float = field(default=1.0, metadata={\"help\": \"Linear warmup over warmup_steps.\"})\n    seed: int = field(default=42, metadata={\"help\": \"seed\"})\n","aebd98bb":"class Logger(object):\n    level_relations = {\n        'debug': logging.DEBUG,\n        'info': logging.INFO,\n        'warning': logging.WARNING,\n        'error': logging.ERROR,\n        'crit': logging.CRITICAL\n    }  # \u65e5\u5fd7\u7ea7\u522b\u5173\u7cfb\u6620\u5c04\n\n    def __init__(self, filename, level='info', when='D', backCount=3,\n                 fmt='%(asctime)s %(levelname)s %(pathname)s[%(lineno)d]: %(message)s'):\n        filename_str = get_log_file_name(filename)\n        self.logger = logging.getLogger(filename_str)\n        format_str = logging.Formatter(fmt)  # \u8bbe\u7f6e\u65e5\u5fd7\u683c\u5f0f\n        self.logger.setLevel(self.level_relations.get(level))  # \u8bbe\u7f6e\u65e5\u5fd7\u7ea7\u522b\n        sh = logging.StreamHandler()  # \u5f80\u5c4f\u5e55\u4e0a\u8f93\u51fa\n        sh.setFormatter(format_str)  # \u8bbe\u7f6e\u5c4f\u5e55\u4e0a\u663e\u793a\u7684\u683c\u5f0f\n        th = handlers.TimedRotatingFileHandler(filename=filename_str, when=when, backupCount=backCount,\n                                               encoding='utf-8')  # \u5f80\u6587\u4ef6\u91cc\u5199\u5165#\u6307\u5b9a\u95f4\u9694\u65f6\u95f4\u81ea\u52a8\u751f\u6210\u6587\u4ef6\u7684\u5904\u7406\u5668\n        # \u5b9e\u4f8b\u5316TimedRotatingFileHandler\n        # interval\u662f\u65f6\u95f4\u95f4\u9694\uff0cbackupCount\u662f\u5907\u4efd\u6587\u4ef6\u7684\u4e2a\u6570\uff0c\u5982\u679c\u8d85\u8fc7\u8fd9\u4e2a\u4e2a\u6570\uff0c\u5c31\u4f1a\u81ea\u52a8\u5220\u9664\uff0cwhen\u662f\u95f4\u9694\u7684\u65f6\u95f4\u5355\u4f4d\uff0c\u5355\u4f4d\u6709\u4ee5\u4e0b\u51e0\u79cd\uff1a\n        # S \u79d2\n        # M \u5206\n        # H \u5c0f\u65f6\u3001\n        # D \u5929\u3001\n        # W \u6bcf\u661f\u671f\uff08interval==0\u65f6\u4ee3\u8868\u661f\u671f\u4e00\uff09\n        # midnight \u6bcf\u5929\u51cc\u6668\n        th.setFormatter(format_str)  # \u8bbe\u7f6e\u6587\u4ef6\u91cc\u5199\u5165\u7684\u683c\u5f0f\n        self.logger.addHandler(sh)  # \u628a\u5bf9\u8c61\u52a0\u5230logger\u91cc\n        self.logger.addHandler(th)\n\n\n\ndef get_labels_from_list(label_type='bios'):\n    \"CLUENER TAGS\"\n    bios_tag_list = [\"<pad>\", \"B-address\", \"B-book\", \"B-company\", 'B-game', 'B-government', 'B-movie', 'B-name',\n                     'B-organization', 'B-position', 'B-scene', \"I-address\",\n                     \"I-book\", \"I-company\", 'I-game', 'I-government', 'I-movie', 'I-name',\n                     'I-organization', 'I-position', 'I-scene',\n                     \"S-address\", \"S-book\", \"S-company\", 'S-game', 'S-government', 'S-movie',\n                     'S-name', 'S-organization', 'S-position',\n                     'S-scene', 'O', \"<start>\", \"<eos>\"]\n\n    span_tag_list = [\"O\", \"address\", \"book\", \"company\", 'game', 'government', 'movie', 'name', 'organization',\n                     'position', 'scene']\n\n    semeval_2010_task8_tag_list = [\"Other\", \"Cause-Effect(e1,e2)\", \"Cause-Effect(e2,e1)\", \"Instrument-Agency(e1,e2)\",\n                                   \"Instrument-Agency(e2,e1)\", \"Product-Producer(e1,e2)\", \"Product-Producer(e2,e1)\",\n                                   \"Content-Container(e1,e2)\", \"Content-Container(e2,e1)\", \"Entity-Origin(e1,e2)\",\n                                   \"Entity-Origin(e2,e1)\", \"Entity-Destination(e1,e2)\", \"Entity-Destination(e2,e1)\",\n                                   \"Component-Whole(e1,e2)\", \"Component-Whole(e2,e1)\", \"Member-Collection(e1,e2)\",\n                                   \"Member-Collection(e2,e1)\", \"Message-Topic(e1,e2)\", \"Message-Topic(e2,e1)\"]\n\n    if label_type == 'bios':\n        return bios_tag_list\n    elif label_type == 'span':\n        return span_tag_list\n    else:\n        return semeval_2010_task8_tag_list\n\n\ndef load_tag_from_file(path):\n    result = []\n    with open(path, \"r\", encoding=\"utf-8\") as file:\n        lines = file.readlines()\n        for idx, line in enumerate(lines):\n            result.append(line.strip())\n    return result\n\n\ndef load_tag(path=None, label_type='bios'):\n    if path is not None:\n        tags = load_tag_from_file(path)\n    else:\n        tags = get_labels_from_list(label_type)\n\n    id2tag = {i: label for i, label in enumerate(tags)}\n    tag2id = {label: i for i, label in enumerate(tags)}\n\n    return tags, tag2id, id2tag\n\n\ndef load_tokenizer(model_name_or_path):\n    tokenizer = BertTokenizer.from_pretrained(model_name_or_path)\n    tokenizer.add_special_tokens({\"additional_special_tokens\": ADDITIONAL_SPECIAL_TOKENS})\n    return tokenizer\n\n\ndef write_prediction(args, output_file, preds):\n    \"\"\"\n    For official evaluation script\n    :param output_file: prediction_file_path (e.g. eval\/proposed_answers.txt)\n    :param preds: [0,1,0,2,18,...]\n    \"\"\"\n    tags, tag2id, id2tag = load_tag(label_type=DATASET_TYPE_NAME)\n    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n        for idx, pred in enumerate(preds):\n            f.write(\"{}\\t{}\\n\".format(8001 + idx, id2tag[pred]))\n\n\ndef now_str(format=\"%Y-%m-%d_%H\"):\n    return time.strftime(format, time.localtime())\n\n\ndef init_logger(filename='test'):\n    log = Logger('{}_{}.log'.format(filename, now_str()), level='debug')\n    return log.logger\n\n\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if not args.no_cuda and torch.cuda.is_available():\n        torch.cuda.manual_seed_all(args.seed)\n\n\ndef compute_metrics(preds, labels):\n    assert len(preds) == len(labels)\n    return acc_and_f1(preds, labels)\n\n\ndef simple_accuracy(preds, labels):\n    return (preds == labels).mean()\n\n\ndef acc_and_f1(preds, labels, average=\"macro\"):\n    acc = simple_accuracy(preds, labels)\n    metrics = dict()\n    metrics[\"f1\"] = f1_score(labels, preds, average=average)\n    metrics[\"precision\"] = precision_score(labels, preds, average=average)\n    metrics[\"acc\"] = acc\n    metrics[\"recall\"] = recall_score(labels, preds, average=average)\n\n    return metrics\n\n\ndef get_time_dif(start_time):\n    \"\"\"\u83b7\u53d6\u5df2\u4f7f\u7528\u65f6\u95f4\"\"\"\n    end_time = time.time()\n    time_dif = end_time - start_time\n    return timedelta(seconds=int(round(time_dif)))\n\n\ndef check_file_exists(filename, delete=False):\n    \"\"\"\u68c0\u67e5\u6587\u4ef6\u662f\u5426\u5b58\u5728\"\"\"\n    dir_name = os.path.dirname(filename)\n    if not os.path.exists(dir_name):\n        os.makedirs(dir_name)\n        print(\"\u6587\u4ef6\u5939\u4e0d\u5b58\u5728,\u521b\u5efa\u76ee\u5f55:{}\".format(dir_name))\n\n\ndef get_checkpoint_dir(file_dir=\".\/\"):\n    checkpoints = []\n    for root, dirs, files in os.walk(file_dir):\n        checkpoints.extend(dirs)\n\n    max_step = 0\n    checkpoint_dir = None\n    for checkpoint in checkpoints:\n        global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else 0\n        if int(global_step) > max_step:\n            max_step = int(global_step)\n            checkpoint_dir = checkpoint\n    return checkpoint_dir, max_step\n\ndef get_log_file_name(file_name):\n    name_list = str(file_name).split(\".\")\n    return \"{}_{}.{}\".format(name_list[0], get_time(), name_list[1])\n\ndef get_time():\n    return time.strftime(\"%Y-%m-%d\", time.localtime())\n\n\nlogger = init_logger(\"test\")\n","05e67bda":"\n@dataclass\nclass Example:\n    guid: str = None\n    text: str = None\n    label: str = None\n\n\n# \u8bfb\u53d6\u6570\u636e\u96c6:json \u683c\u5f0f\ndef read_dataset_txt(input_file, set_type=\"train\"):\n    \"\"\"read dataset \"\"\"\n    examples = []\n    tags, tag2id, id2tag = load_tag(label_type=DATASET_TYPE_NAME)\n    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n        reader = csv.reader(file, delimiter=\"\\t\", quotechar=None)\n        for (i, line) in enumerate(reader):\n            guid = \"%s-%s\" % (set_type, i)\n            text_a = line[1]\n            label = tag2id[line[0]]\n            if i % 1000 == 0:\n                logger.info(line)\n            examples.append(Example(guid=guid, text=text_a, label=label))\n    return examples\n\n\ndef read_data(path, set_type=\"train\"):\n    examples = read_dataset_txt(path, set_type)\n    return examples\n\n\nclass SemevalTask8Dataset(Dataset):\n    def __init__(self, examples: List[Example], max_length=384,\n                 tokenizer=BertTokenizer.from_pretrained(BERT_MODEL_NAME)):\n        self.max_length = 512 if max_length > 512 else max_length\n        self.tags, self.tag2id, self.id2tag = load_tag(label_type=DATASET_TYPE_NAME)\n        self.texts = []\n        self.labels = []\n        self.e1_masks = []\n        self.e2_masks = []\n        self.token_type_ids = []\n        self.input_ids = []\n\n        for (ex_index, example) in enumerate(examples):\n            if ex_index % 5000 == 0:\n                logger.info(\"Writing example %d of %d\" % (ex_index, len(examples)))\n\n            tokens_a = tokenizer.tokenize(example.text)\n            if len(tokens_a) > max_length - 1:\n                tokens_a = tokens_a[: (max_length - 1)]\n\n            e11_p = tokens_a.index(\"<e1>\")  # the start position of entity1\n            e12_p = tokens_a.index(\"<\/e1>\")  # the end position of entity1\n            e21_p = tokens_a.index(\"<e2>\")  # the start position of entity2\n            e22_p = tokens_a.index(\"<\/e2>\")  # the end position of entity2\n\n            # Replace the token\n            tokens_a[e11_p] = \"$\"\n            tokens_a[e12_p] = \"$\"\n            tokens_a[e21_p] = \"#\"\n            tokens_a[e22_p] = \"#\"\n\n            # Add 1 because of the [CLS] token\n            e11_p += 1\n            e12_p += 1\n            e21_p += 1\n            e22_p += 1\n\n            tokens = tokens_a\n            token_type_ids = [0] * len(tokens)\n\n            # add cls\u3000token\n            tokens = [tokenizer.cls_token] + tokens\n            token_type_ids = [0] + token_type_ids\n\n            # e1 mask, e2 mask\n            e1_mask = [0] * len(tokens)\n            e2_mask = [0] * len(tokens)\n\n            for i in range(e11_p, e12_p + 1):\n                e1_mask[i] = 1\n            for i in range(e21_p, e22_p + 1):\n                e2_mask[i] = 1\n\n            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n\n            label_id = int(example.label)\n            self.texts.append(tokens)\n            self.labels.append(label_id)\n            self.e1_masks.append(torch.LongTensor(e1_mask))\n            self.e2_masks.append(torch.LongTensor(e2_mask))\n            self.token_type_ids.append(torch.LongTensor(token_type_ids))\n            self.input_ids.append(torch.LongTensor(input_ids))\n\n            if ex_index < 5:\n                logger.info(\"*** Example ***\")\n                logger.info(\"guid: %s\" % example.guid)\n                logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n                logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n                logger.info(\"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids]))\n                logger.info(\"label: %s (id = %d)\" % (self.id2tag[example.label], label_id))\n                logger.info(\"e1_mask: %s\" % \" \".join([str(x) for x in e1_mask]))\n                logger.info(\"e2_mask: %s\" % \" \".join([str(x) for x in e2_mask]))\n\n        for text, mask in zip(self.texts, self.e1_masks):\n            assert len(text) == len(mask)\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        return {\n            \"input_ids\": self.input_ids[item],\n            \"labels\": self.labels[item],\n            \"token_type_ids\": self.token_type_ids[item],\n            \"e1_mask\": self.e1_masks[item],\n            \"e2_mask\": self.e2_masks[item],\n            \"token\": self.texts[item],\n        }\n\n\ndef collate_fn_rbert(features) -> Dict[str, Tensor]:\n    batch_input_ids = [feature[\"input_ids\"] for feature in features]\n    batch_token_type_ids = [feature[\"token_type_ids\"] for feature in features]\n    batch_e1_mask = [feature[\"e1_mask\"] for feature in features]\n    batch_e2_mask = [feature[\"e2_mask\"] for feature in features]\n\n    batch_labels = torch.tensor([feature[\"labels\"] for feature in features], dtype=torch.long)\n\n    batch_attention_mask = [torch.ones_like(feature[\"input_ids\"]) for feature in features]\n\n    # padding\n    batch_input_ids = pad_sequence(batch_input_ids, batch_first=True, padding_value=0)\n    batch_token_type_ids = pad_sequence(batch_token_type_ids, batch_first=True, padding_value=0)\n    batch_e1_mask = pad_sequence(batch_e1_mask, batch_first=True, padding_value=0)\n    batch_e2_mask = pad_sequence(batch_e2_mask, batch_first=True, padding_value=0)\n    batch_attention_mask = pad_sequence(batch_attention_mask, batch_first=True, padding_value=0)\n    # batch_labels = pad_sequence(batch_labels, batch_first=True, padding_value=0)\n\n    assert batch_input_ids.shape == batch_token_type_ids.shape\n    assert batch_input_ids.shape == batch_e1_mask.shape\n    assert batch_input_ids.shape == batch_e2_mask.shape\n    assert batch_input_ids.shape == batch_attention_mask.shape\n\n    return {\n        \"input_ids\": batch_input_ids,\n        \"attention_mask\": batch_attention_mask,\n        \"token_type_ids\": batch_token_type_ids,\n        \"labels\": batch_labels,\n        \"e1_mask\": batch_e1_mask,\n        \"e2_mask\": batch_e2_mask\n    }\n","aa35958e":"tokenizer = load_tokenizer(BERT_PATH)\n\ntrain_filename = \"..\/input\/semeval-2010-task-8-dataset\/train.tsv\"\n\n# \u6784\u5efadataset\ntrain_dataset = SemevalTask8Dataset(read_data(train_filename), tokenizer=tokenizer)\nprint(train_dataset[0])\n\ntrain_dataloader = DataLoader(train_dataset, shuffle=False, batch_size=32,\n                              collate_fn=collate_fn_rbert)\n\nbatch = next(iter(train_dataloader))\nprint(batch.keys())\nprint(type(batch[\"input_ids\"]))\nprint(batch[\"input_ids\"].shape)\nprint(type(batch[\"labels\"]))\nprint(batch[\"labels\"].shape)\nprint(type(batch[\"attention_mask\"]))\nprint(batch[\"attention_mask\"].shape)\n\ntags, tag2id, id2tag = load_tag(label_type=DATASET_TYPE_NAME)\nprint(tags)\nprint(tag2id)\nprint(id2tag)","1b67d508":"\nclass FCLayer(nn.Module):\n    def __init__(self, input_dim, output_dim, dropout_rate=0.0, use_activation=True):\n        super(FCLayer, self).__init__()\n        self.use_activation = use_activation\n        self.dropout = nn.Dropout(dropout_rate)\n        self.linear = nn.Linear(input_dim, output_dim)\n        self.tanh = nn.Tanh()\n\n    def forward(self, x):\n        x = self.dropout(x)\n        if self.use_activation:\n            x = self.tanh(x)\n        x = self.linear(x)\n        return x\n\n\nclass RBERT(BertPreTrainedModel):\n    def __init__(self, config, args):\n        super(RBERT, self).__init__(config)\n        self.bert = BertModel(config)\n        self.num_labels = config.num_labels\n        self.cls_fc_layer = FCLayer(config.hidden_size, config.hidden_size, args.dropout_rate)\n        self.entity_fc_layer = FCLayer(config.hidden_size, config.hidden_size, args.dropout_rate)\n        self.label_classifier = FCLayer(config.hidden_size * 3, config.num_labels,\n                                        args.dropout_rate, use_activation=False)\n\n    @staticmethod\n    def entity_average(hidden_output, e_mask):\n        \"\"\"\n        Average the entity hidden state vectors (H_i ~ H_j)\n        :param hidden_output: [batch_size, j-i+1, dim]\n        :param e_mask: [batch_size, max_seq_len]\n                e.g. e_mask[0] == [0, 0, 0, 1, 1, 1, 0, 0, ... 0]\n        :return: [batch_size, dim]\n        \"\"\"\n        e_mask_unsqueeze = e_mask.unsqueeze(1)  # [b, 1, j-i+1]\n        length_tensor = (e_mask != 0).sum(dim=1).unsqueeze(1)  # [batch_size, 1]\n\n        # [b, 1, j-i+1] * [b, j-i+1, dim] = [b, 1, dim] -> [b, dim]\n        sum_vector = torch.bmm(e_mask_unsqueeze.float(), hidden_output).squeeze(1)\n        avg_vector = sum_vector.float() \/ length_tensor.float()  # broadcasting\n        return avg_vector\n\n    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None,\n                e1_mask=None, e2_mask=None, output_attentions=None, output_hidden_states=None, return_dict=False,\n                ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )  # sequence_output, pooled_output, (hidden_states), (attentions)\n\n        sequence_output = outputs[0]\n        pooled_output = outputs[1]  # CLS\n\n        # Average\n        e1_h = self.entity_average(sequence_output, e1_mask)\n        e2_h = self.entity_average(sequence_output, e2_mask)\n\n        # Dropout -> tanh -> fc_layer (Share FC layer for e1 and e2)\n        pooled_output = self.cls_fc_layer(pooled_output)\n        e1_h = self.entity_fc_layer(e1_h)\n        e2_h = self.entity_fc_layer(e2_h)\n\n        # Concat -> fc_layer\n        concat_h = torch.cat([pooled_output, e1_h, e2_h], dim=1)\n        logits = self.label_classifier(concat_h)\n\n        outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n\n        # \u3000Softmax\n        loss = None\n        if labels is not None:\n            if self.num_labels == 1:\n                loss_fct = nn.MSELoss()\n                loss = loss_fct(logits.reshape(-1), labels.reshape(-1))\n            else:\n                loss_fct = nn.CrossEntropyLoss()\n                loss = loss_fct(logits.reshape(-1, self.num_labels), labels.reshape(-1))\n            outputs = (loss,) + outputs\n\n        if not return_dict:\n            return outputs\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n","ece83677":"class Trainer(object):\n    def __init__(self, args, train_dataset=None, dev_dataset=None, test_dataset=None):\n        self.args = args\n        self.train_dataset = train_dataset\n        self.dev_dataset = dev_dataset\n        self.test_dataset = test_dataset\n\n        self.tags, self.tag2id, self.id2tag = load_tag(label_type=DATASET_TYPE_NAME)\n        self.num_labels = len(self.tags)\n\n        self.config = BertConfig.from_pretrained(\n            args.model_name_or_path,\n            num_labels=self.num_labels,\n            finetuning_task=args.task_name,\n            id2label=self.id2tag,\n            label2id=self.tag2id,\n        )\n        self.model = RBERT.from_pretrained(args.model_name_or_path, config=self.config, args=args)\n        self.tokenizer = load_tokenizer(args.model_name_or_path)\n\n        # GPU or CPU\n        self.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n        self.model.to(self.device)\n\n    def train(self):\n        train_sampler = RandomSampler(self.train_dataset)\n        train_dataloader = DataLoader(self.train_dataset, sampler=train_sampler,\n                                      batch_size=self.args.train_batch_size,\n                                      collate_fn=collate_fn_rbert)\n\n        t_total = len(train_dataloader) * self.args.epoch\n        num_warmup_steps = t_total * self.args.warmup\n\n        # Prepare optimizer and schedule (linear warmup and decay)\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n                \"weight_decay\": self.args.weight_decay,\n            },\n            {\n                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        optimizer = AdamW(\n            optimizer_grouped_parameters,\n            lr=self.args.learning_rate,\n            eps=self.args.adam_epsilon,\n        )\n        scheduler = get_linear_schedule_with_warmup(\n            optimizer,\n            num_warmup_steps=num_warmup_steps,\n            num_training_steps=t_total,\n        )\n\n        # Train!\n        logger.info(\"***** Running training *****\")\n        logger.info(\"  Device = %s\", self.device)\n        logger.info(\"  Num examples = %d\", len(self.train_dataset))\n        logger.info(\"  Num Epochs = %d\", self.args.epoch)\n        logger.info(\"  Total train batch size = %d\", self.args.train_batch_size)\n        logger.info(\"  Total optimization steps = %d\", t_total)\n        logger.info(\"  Eval steps = %d\", self.args.eval_steps)\n        logger.info(\"  Save steps = %d\", self.args.save_steps)\n\n        global_step = 0\n        tr_loss = 0.0\n        self.model.zero_grad()\n        start_time = time.time()\n\n        train_result = []\n        for epoch in range(self.args.epoch):\n            epoch_start_time = time.time()\n            logger.info('Epoch [{}\/{}]'.format(epoch + 1, self.args.epoch))\n\n            # pbar = ProgressBar(n_total=total_train_len, desc='Training')\n            pbar = tqdm(total=len(train_dataloader), desc='Training-Batch[{}\/{}]'.format(epoch + 1, self.args.epoch))\n            for step, batch in enumerate(train_dataloader):\n                self.model.train()\n\n                for k, v in batch.items():\n                    # if k not in [\"input_len\", \"subjects_id\"]:\n                    batch[k] = v.to(self.device)\n\n                inputs = {\"input_ids\": batch[\"input_ids\"], \"attention_mask\": batch[\"attention_mask\"],\n                          \"labels\": batch[\"labels\"], \"e1_mask\": batch[\"e1_mask\"], \"e2_mask\": batch[\"e2_mask\"],\n                          \"token_type_ids\": batch[\"token_type_ids\"]}\n\n                optimizer.zero_grad()\n                outputs = self.model(**inputs)\n                loss = outputs[0]\n\n                loss.backward()\n\n                time_dif = get_time_dif(start_time)\n                pbar.set_postfix({'loss': loss.item(), 'time': time_dif})\n                pbar.update()\n\n                tr_loss += loss.item()\n\n                # \u68af\u5ea6\u88c1\u526a\u4e0d\u518d\u5728AdamW\u4e2d\u4e86(\u56e0\u6b64\u4f60\u53ef\u4ee5\u6beb\u65e0\u95ee\u9898\u5730\u4f7f\u7528\u653e\u5927\u5668)\n                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.args.max_grad_norm)\n\n                optimizer.step()\n                scheduler.step()\n                self.model.zero_grad()\n\n                global_step += 1\n\n                # \u6bcf\u591a\u5c11\u8f6e\u8f93\u51fa\u5728\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u4e0a\u7684\u6548\u679c\n#                 if global_step % self.args.eval_steps == 0:\n                if global_step % len(train_dataloader) == 0:\n                    metrics_result = self.evaluate(\"test\")  # There is no dev set for semeval task\n                    time_dif = get_time_dif(start_time)\n                    msg = 'Iter: {0:>6},  Train Loss: {1:>5.2},  Train Acc: {2:>6.2%}, ' \\\n                          ' Val Loss: {3:>5.2},  Val F1: {4:>6.2%},  Val Acc: {5:>6.2%}, ' \\\n                          ' Val recall: {6:>6.2%},  Time: {7} '\n                    logger.info(\n                        msg.format(global_step, tr_loss \/ global_step, 0.0, metrics_result[\"loss\"],\n                                   metrics_result[\"f1\"],\n                                   metrics_result[\"precision\"], metrics_result[\"recall\"], time_dif))\n\n                    metrics_result[\"total_batch\"] = global_step\n                    metrics_result[\"train_loss\"] = tr_loss \/ global_step\n                    metrics_result[\"train_time\"] = time_dif\n                    train_result.append(metrics_result)\n\n                # \u6bcf\u591a\u5c11\u8f6e\u4fdd\u5b58\u4e00\u6b21\n#                 if global_step % self.args.save_steps == 0:\n                if global_step % len(train_dataloader) == 0:\n                    self.save_model(global_step)\n                    logger.info('Epoch [{}\/{}] finished, use time :{}'.format(epoch + 1, self.args.epoch,\n                                                                    get_time_dif(epoch_start_time)))\n            if 'cuda' in str(self.device):\n                torch.cuda.empty_cache()\n\n        logger.info(\"\u8bad\u7ec3\u5b8c\u6bd5\")\n        logger.info(\"Epoch\tTraining Loss\tValidation Loss\tF1\tPrecision\tRecall  Use Time \")\n        logger.info(\"-\" * 100)\n        for index, var in enumerate(train_result):\n            msg = 'Iter: {0:>6},  Train Loss: {1:>5.2}, ' \\\n                  ' Val Loss: {2:>5.2},  Val F1: {3:>6.2%},  Val Acc: {4:>6.2%}, ' \\\n                  ' Val recall: {5:>6.2%},  Time: {6} '\n            logger.info(msg.format(index + 1, var[\"train_loss\"], var[\"loss\"], var[\"f1\"], var[\"precision\"],\n                             var[\"recall\"], var[\"train_time\"]))\n\n        return global_step, tr_loss \/ global_step\n\n    def evaluate(self, mode=\"test\"):\n        # We use test dataset because semeval doesn't have dev dataset\n        if mode == \"test\":\n            dataset = self.test_dataset\n        elif mode == \"dev\":\n            dataset = self.dev_dataset\n        else:\n            raise Exception(\"Only dev and test dataset available\")\n\n        eval_dataloader = DataLoader(dataset, shuffle=False, batch_size=self.args.eval_batch_size,\n                                     collate_fn=collate_fn_rbert)\n\n        # Eval!\n        logger.info(\"***** Running evaluation on %s dataset *****\", mode)\n        logger.info(\"  Num examples = %d\", len(dataset))\n        logger.info(\"  Batch size = %d\", self.args.eval_batch_size)\n        eval_loss = 0.0\n        nb_eval_steps = 0\n        preds = None\n        out_label_ids = None\n\n        self.model.eval()\n\n        for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n            for k, v in batch.items():\n                # if k not in [\"input_len\", \"subjects_id\"]:\n                batch[k] = v.to(self.device)\n\n            inputs = {\"input_ids\": batch[\"input_ids\"], \"attention_mask\": batch[\"attention_mask\"],\n                      \"labels\": batch[\"labels\"], \"e1_mask\": batch[\"e1_mask\"], \"e2_mask\": batch[\"e2_mask\"],\n                      \"token_type_ids\": batch[\"token_type_ids\"]}\n\n            with torch.no_grad():\n                outputs = self.model(**inputs)\n                tmp_eval_loss, logits = outputs[:2]\n\n                eval_loss += tmp_eval_loss.mean().item()\n            nb_eval_steps += 1\n\n            if preds is None:\n                preds = logits.detach().cpu().numpy()\n                out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n            else:\n                preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n                out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n\n        eval_loss = eval_loss \/ nb_eval_steps\n        results = {\"loss\": eval_loss}\n        preds = np.argmax(preds, axis=1)\n        write_prediction(self.args, os.path.join(self.args.eval_dir, \"proposed_answers.txt\"), preds)\n\n        result = compute_metrics(preds, out_label_ids)\n        results.update(result)\n        results[\"precision\"] = results[\"acc\"]\n\n        logger.info(\"***** Eval results *****\")\n        for key in sorted(results.keys()):\n            logger.info(\"  {} = {:.4f}\".format(key, results[key]))\n\n        return results\n\n    def save_model(self, global_step):\n        # Save model checkpoint (Overwrite)\n        output_dir = os.path.join(self.args.output_dir, \"checkpoint-{}\".format(global_step))\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        model_to_save = self.model.module if hasattr(self.model, \"module\") else self.model\n        model_to_save.save_pretrained(output_dir)\n\n        # Save training arguments together with the trained model\n        torch.save(self.args, os.path.join(output_dir, \"training_args.bin\"))\n        self.tokenizer.save_vocabulary(output_dir)\n        logger.info(\"\")\n        logger.info(\"Saving model checkpoint to %s\", output_dir)\n\n    def load_model(self):\n        # Check whether model exists\n        if not os.path.exists(self.args.output_dir):\n            raise Exception(\"Model doesn't exists! Train first!\")\n\n        checkpoint_dir, max_step = get_checkpoint_dir(self.args.output_dir)\n\n        self.args = torch.load(os.path.join(self.args.output_dir, \"training_args.bin\"))\n\n        self.model = RBERT.from_pretrained(os.path.join(self.args.output_dir, checkpoint_dir), args=self.args)\n        self.model.to(self.device)\n        logger.info(\"***** Model Loaded *****\")\n","86dabbf0":"def main(args):\n    set_seed(args)\n    tokenizer = load_tokenizer(args.model_name_or_path)\n\n    check_file_exists(args.output_dir + \"\/test.txt\")\n    check_file_exists(args.eval_dir + \"\/test.txt\")\n    \n    train_dataset = SemevalTask8Dataset(read_data(args.train_file), tokenizer=tokenizer)\n    test_dataset = SemevalTask8Dataset(read_data(args.test_file), tokenizer=tokenizer)\n\n    trainer = Trainer(args, train_dataset=train_dataset, test_dataset=test_dataset)\n\n    if args.do_train:\n        trainer.train()\n\n    if args.do_eval:\n        trainer.load_model()\n        trainer.evaluate(\"test\")\n","be54e0a6":"torch.cuda.empty_cache()\nmodel_args = ModelArguments(save_steps=100,train_batch_size=32,epoch=10)\nmain(model_args)","6874b5ba":"torch.cuda.memory_summary(device=None, abbreviated=False)","3ba6af69":"utils.py","24415164":"trainer.py","87a0c245":"model.py\n"}}