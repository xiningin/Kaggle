{"cell_type":{"dcf0482f":"code","9d194434":"code","11f9973b":"code","bef213cb":"code","70a12c89":"code","15321716":"code","2063a369":"code","32283f81":"code","c32cbbec":"code","be4fbf14":"code","89b1e4a2":"code","3ef7e091":"code","85cafdc9":"code","8786beb7":"code","aca92631":"code","8ea91edf":"code","9104f702":"code","0fef062f":"code","d1f8d2e9":"code","48e77612":"code","ad553300":"code","b8dd222e":"code","1d210756":"code","1e606854":"code","638c7de0":"code","8168b5af":"code","ce2a8a3b":"code","66f16556":"code","962bec82":"code","cb13f184":"code","c101b6b9":"code","648c54bf":"code","b50bf2e0":"code","f80cb686":"code","b14a63b7":"code","5b7bead4":"code","19a91208":"code","dbc77dda":"code","9f7278c0":"code","131ae1a3":"code","df07e7c9":"code","f04b0feb":"code","b9a3c0fb":"code","5be98cbf":"code","622fd376":"code","bde27ea0":"code","a4cccd2c":"code","c1762964":"markdown","8beb6848":"markdown","72ecad81":"markdown","fc545036":"markdown","903adda5":"markdown","3a1ffdb7":"markdown","01ad9e0b":"markdown","f35f9643":"markdown","5d3ecb2c":"markdown","340340f8":"markdown","14182d22":"markdown","166e98ef":"markdown","b353d6b7":"markdown","eb88e9c8":"markdown","0264784a":"markdown","4520edd6":"markdown","2876360f":"markdown","e3f79a8c":"markdown","863bba4f":"markdown","f66aa77b":"markdown","f70b2ae6":"markdown","e0475788":"markdown","ad8a92cd":"markdown","f265f4da":"markdown","fd9a426b":"markdown","f6f49dd6":"markdown","7d5fac7d":"markdown","edfaf4c8":"markdown","354dd624":"markdown","2f92c5ad":"markdown","ad180039":"markdown","075c8520":"markdown"},"source":{"dcf0482f":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime as dt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom haversine import haversine\nimport statsmodels.formula.api as sm\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nimport warnings; warnings.simplefilter('ignore')","9d194434":"#import the data from a csv file.\ndata = pd.read_csv(\"..\/input\/nyc-taxi-trip-duration\/train.zip\")","11f9973b":"data.head()","bef213cb":"#Convert timestamp to datetime format to fetch the other details as listed below\ndata['pickup_datetime'] = pd.to_datetime(data['pickup_datetime'])\n#data['dropoff_datetime'] = pd.to_datetime(data['dropoff_datetime'])","70a12c89":"#Calculate and assign new columns to the dataframe such as weekday,\n#month and pickup_hour which will help us to gain more insights from the data.\n#data['weekday'] = data.pickup_datetime.dt.weekday_name\ndata['month'] = data.pickup_datetime.dt.month\ndata['weekday_num'] = data.pickup_datetime.dt.weekday\ndata['pickup_hour'] = data.pickup_datetime.dt.hour","15321716":"#calc_distance is a function to calculate distance between pickup and dropoff coordinates using Haversine formula.\ndef calc_distance(df):\n    pickup = (df['pickup_latitude'], df['pickup_longitude'])\n    drop = (df['dropoff_latitude'], df['dropoff_longitude'])\n    return haversine(pickup, drop)","2063a369":"#Calculate distance and assign new column to the dataframe.\nif 'distance' not in data.columns:\n    data['distance'] = data.apply(lambda x: calc_distance(x), axis = 1)","32283f81":"#Calculate Speed in km\/h for further insights\nif 'speed' not in data.columns:\n    data['speed'] = (data.distance\/(data.trip_duration\/3600))","c32cbbec":"#Dummify all the categorical features like \"store_and_fwd_flag, vendor_id, month, weekday_num, pickup_hour, passenger_count\" except the label i.e. \"trip_duration\"\n\ndummy = pd.get_dummies(data.month, prefix='month')\ndummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\ndata = pd.concat([data,dummy], axis = 1)\n\ndummy = pd.get_dummies(data.weekday_num, prefix='weekday_num')\ndummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\ndata = pd.concat([data,dummy], axis = 1)\n\ndummy = pd.get_dummies(data.pickup_hour, prefix='pickup_hour')\ndummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\ndata = pd.concat([data,dummy], axis = 1)\n\nif 'passenger_count' in data.columns:\n    dummy = pd.get_dummies(data.passenger_count, prefix='passenger_count')\n    dummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\n    data = pd.concat([data,dummy], axis = 1)","be4fbf14":"data.head()","89b1e4a2":"data['passenger_count'] = data.passenger_count.map(lambda x: 1 if x == 0 else x)","3ef7e091":"data = data[data.passenger_count <= 6]","85cafdc9":"data = data[data.trip_duration <= 86400]","8786beb7":"data = data[data.speed <= 104]","aca92631":"data = data[~((data.distance == 0) & (data.trip_duration >= 60))]","8ea91edf":"duo = data.loc[(data['distance'] <= 1) & (data['trip_duration'] >= 3600),['distance','trip_duration']].reset_index(drop=True)","9104f702":"data = data[~((data['distance'] <= 1) & (data['trip_duration'] >= 3600))]","0fef062f":"data = data[data.pickup_longitude != data.pickup_longitude.min()]","d1f8d2e9":"data = data[data.pickup_longitude != data.pickup_longitude.min()]\n#map_marker(data)","48e77612":"#First chech the index of the features and label\ndel data['id']\ndel data['dropoff_datetime']\ndel data['passenger_count']\ndel data['store_and_fwd_flag']\nlist(zip( range(0,len(data.columns)),data.columns))","ad553300":"Y = data.iloc[:,6].values\ndel data['trip_duration']\ndel data['pickup_datetime']\ndel data['vendor_id']\nlist(zip( range(0,len(data.columns)),data.columns))","b8dd222e":"X = data.iloc[:,range(0,46)].values","1d210756":"X1 = np.append(arr = np.ones((X.shape[0],1)).astype(int), values = X, axis = 1)","1e606854":"X1.shape","638c7de0":"#Select all the features in X array\nX_opt = X1[:,range(0,47)]\n#regressor_OLS = sm.OLS(endog = Y, exog = X_opt).fit()\n\n#Fetch p values for each feature\n#p_Vals = regressor_OLS.pvalues\n\n#define significance level for accepting the feature.\n#sig_Level = 0.05\n\n#Loop to iterate over features and remove the feature with p value less than the sig_level\n#while max(p_Vals) > sig_Level:\n    #print(\"Probability values of each feature \\n\")\n    #print(p_Vals)\n    #X_opt = np.delete(X_opt, np.argmax(p_Vals), axis = 1)\n    #print(\"\\n\")\n    #print(\"Feature at index {} is removed \\n\".format(str(np.argmax(p_Vals))))\n    #print(str(X_opt.shape[1]-1) + \" dimensions remaining now... \\n\")\n    #regressor_OLS = sm.OLS(endog = Y, exog = X_opt).fit()\n    #p_Vals = regressor_OLS.pvalues\n    #print(\"=================================================================\\n\")\n    \n#Print final summary\n#print(\"Final stat summary with optimal {} features\".format(str(X_opt.shape[1]-1)))\n#regressor_OLS.summary()","8168b5af":"#Split raw data\nX_train, X_test, y_train, y_test = train_test_split(X,Y, random_state=4, test_size=0.2)\n\n#Split data from the feature selection group\nX_train_fs, X_test_fs, y_train_fs, y_test_fs = train_test_split(X_opt,Y, random_state=4, test_size=0.2)","ce2a8a3b":"X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X,Y, random_state=4, test_size=0.2)","66f16556":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_pca = scaler.fit_transform(X_train_pca)\nX_test_pca = scaler.transform(X_test_pca)","962bec82":"from sklearn.decomposition import PCA\npca = PCA().fit(X_train_pca)\n#.plot(np.cumsum(pca.explained_variance_ratio_))\n#plt.xlabel(\"number of components\")\n#plt.ylabel(\"Cumulative explained variance\")\n#plt.show()","cb13f184":"arr = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\nlist(zip(range(1,len(arr)), arr))","c101b6b9":"pca_10 = PCA(n_components=43)\nX_train_pca = pca_10.fit_transform(X_train_pca)\nX_test_pca = pca_10.transform(X_test_pca)","648c54bf":"#Linear regressor for the raw data\n#regressor = LinearRegression() \n#regressor.fit(X_train,y_train) \n\n#Linear regressor for the Feature selection group\n#regressor1 = LinearRegression() \n#regressor1.fit(X_train_fs,y_train_fs) \n\n#Linear regressor for the Feature extraction group\nregressor2 = LinearRegression() \nregressor2.fit(X_train_pca,y_train_pca) ","b50bf2e0":"#Predict from the test features of raw data\n#y_pred = regressor.predict(X_test) \n\n#Predict from the test features of Feature Selection group\n#y_pred = regressor1.predict(X_test_fs) \n\n#Predict from the test features of Feature Extraction group\ny_pred_pca = regressor2.predict(X_test_pca) ","f80cb686":"#Evaluate the regressor on the raw data\n#print('RMSE score for the Multiple LR raw is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test,y_pred))))\n#print('Variance score for the Multiple LR raw is : %.2f' % regressor.score(X_test, y_test))\n#print(\"\\n\")\n\n#Evaluate the regressor on the Feature selection group\n#print('RMSE score for the Multiple LR FS is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_fs,y_pred))))\n#print('Variance score for the Multiple LR FS is : %.2f' % regressor1.score(X_test_fs, y_test_fs))\n#print(\"\\n\")\n\n#Evaluate the regressor on the Feature extraction group\nprint('RMSE score for the Multiple LR PCA is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_pca,y_pred_pca))))\nprint('Variance score for the Multiple LR PCA is : %.2f' % regressor2.score(X_test_pca, y_test_pca))","b14a63b7":"import pickle\nimport joblib\nfilename = 'mlr.pkl'\njoblib.dump(regressor2, filename)","5b7bead4":"X_train.shape","19a91208":"#Find linear correlation of each feature with the target variable\nfrom scipy.stats import pearsonr\ndf1 = pd.DataFrame(np.concatenate((X_train,y_train.reshape(len(y_train),1)),axis=1))\ndf1.columns = df1.columns.astype(str)\n\nfeatures = df1.iloc[:,:35].columns.tolist()\ntarget = df1.iloc[:,35].name\n\ncorrelations = {}\nfor f in features:\n    data_temp = df1[[f,target]]\n    x1 = data_temp[f].values\n    x2 = data_temp[target].values\n    key = f + ' vs ' + target\n    correlations[key] = pearsonr(x1,x2)[0]\n    \ndata_correlations = pd.DataFrame(correlations, index=['Value']).T\ndata_correlations.loc[data_correlations['Value'].abs().sort_values(ascending=False).index]","dbc77dda":"#instantiate the object for the Random Forest Regressor with default params from raw data\n#regressor_rfraw = RandomForestRegressor(n_jobs=-1)\n\n#instantiate the object for the Random Forest Regressor with default params for Feature Selection Group\n#regressor_rf = RandomForestRegressor(n_jobs=-1)\n\n# #instantiate the object for the Random Forest Regressor with tuned hyper parameters for Feature Selection Group\n# regressor_rf1 = RandomForestRegressor(n_estimators = 26,\n#                                      max_depth = 22,\n#                                      min_samples_split = 9,\n#                                      n_jobs=-1)\n\n#instantiate the object for the Random Forest Regressor for Feature Extraction Group\nregressor_rf2 = RandomForestRegressor(n_jobs=-1)\n\n\n#Train the object with default params for raw data\n#regressor_rfraw.fit(X_train,y_train)\n\n#Train the object with default params for Feature Selection Group\n#regressor_rf.fit(X_train_fs,y_train_fs)\n\n# #Train the object with tuned params for Feature Selection Group\n# regressor_rf1.fit(X_train_fs,y_train_fs)\n\n# #Train the object with default params for Feature Extraction Group\nregressor_rf2.fit(X_train_pca,y_train_pca)\n\nprint(\"\\n\")","9f7278c0":"#Predict the output with object of default params for Feature Selection Group\n#y_pred_rfraw = regressor_rfraw.predict(X_test)\n\n#Predict the output with object of default params for Feature Selection Group\n#y_pred_rf = regressor_rf.predict(X_test_fs)\n\n# #Predict the output with object of hyper tuned params for Feature Selection Group\n# y_pred_rf1 = regressor_rf1.predict(X_test_fs)\n\n#Predict the output with object of PCA params for Feature Extraction Group\ny_pred_rfpca = regressor_rf2.predict(X_test_pca)\n\nprint(\"\\n\")","131ae1a3":"#Evaluate the model with default params for raw data\n#print('RMSE score for the RF regressor raw is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test,y_pred_rfraw))))\n#print('RMSLE score for the RF regressor raw is : {}'.format(np.sqrt(metrics.mean_squared_log_error(y_test,y_pred_rfraw))))\n#print('Variance score for the RF regressor raw is : %.2f' % regressor_rfraw.score(X_test, y_test))\n\n#print(\"\\n\")\n\n#Evaluate the model with default params for Feature Selection Group\n#print('RMSE score for the RF regressor is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_fs,y_pred_rf))))\n#print('RMSLE score for the RF regressor is : {}'.format(np.sqrt(metrics.mean_squared_log_error(y_test_fs,y_pred_rf))))\n#print('Variance score for the RF regressor is : %.2f' % regressor_rf.score(X_test_fs, y_test_fs))\n\n# print(\"\\n\")\n\n# #Evaluate the model with tuned params for Feature Selection Group\n# print('RMSE score for the RF regressor1 is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_fs,y_pred_rf1))))\n# print('RMSLE score for the RF regressor1 is : {}'.format(np.sqrt(metrics.mean_squared_log_error(y_test_fs,y_pred_rf1))))\n# print('Variance score for the RF regressor1 is : %.2f' % regressor_rf1.score(X_test_fs, y_test_fs))\n\n#print(\"\\n\")\n\n#Evaluate the model with PCA params  for Feature Extraction Group\nprint('RMSE score for the RF regressor2 is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_pca, y_pred_rfpca))))\nprint('Variance score for the RF regressor2 is : %.2f' % regressor_rf2.score(X_test_pca, y_test_pca))","df07e7c9":"filename = 'rfr.pkl'\njoblib.dump(regressor_rf2, filename)","f04b0feb":"#instantiate the object for the XGBoost Regressor with default params for raw data\n#regressor_xgbraw = XGBRegressor(n_jobs=-1)\n\n#instantiate the object for the XGBoost Regressor with default params for Feature Selection Group\n#regressor_xgb = XGBRegressor(n_jobs=-1)\n\n#instantiate the object for the XGBoost Regressor with tuned hyper parameters for Feature Selection Group\nregressor_xgb1 = XGBRegressor(n_estimators=300,\n                            learning_rate=0.09,\n                            gamma=0,\n                            subsample=0.75,\n                            colsample_bytree=1,\n                            max_depth=7,\n                            min_child_weight=4,\n                            silent=1,\n                            n_jobs=-1)\n\n#instantiate the object for the XGBoost Regressor for Feature Extraction Group\n#regressor_xgb2 = XGBRegressor(n_jobs=-1)\n\n\n#Train the object with default params for raw data\n#regressor_xgbraw.fit(X_train,y_train)\n\n#Train the object with default params for Feature Selection Group\n#regressor_xgb.fit(X_train_fs,y_train_fs)\n\n#Train the object with tuned params for Feature Selection Group\nregressor_xgb1.fit(X_train_pca,y_train_pca)\n\n#Train the object with default params for Feature Extraction Group\n#regressor_xgb2.fit(X_train_pca,y_train_pca)\n\nprint(\"\\n\")","b9a3c0fb":"#Predict the output with object of default params for raw data\n#y_pred_xgbraw = regressor_xgbraw.predict(X_test)\n\n#Predict the output with object of default params for Feature Selection Group\n#y_pred_xgb = regressor_xgb.predict(X_test_fs)\n\n#Predict the output with object of hyper tuned params for Feature Selection Group\ny_pred_xgb1 = regressor_xgb1.predict(X_test_pca)\n\n#Predict the output with object of PCA params for Feature Extraction Group\n#y_pred_xgb_pca = regressor_xgb2.predict(X_test_pca)\n\nprint(\"\\n\")","5be98cbf":"#Evaluate the model with default params for raw data\n#print('RMSE score for the XGBoost regressor raw is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test,y_pred_xgbraw))))\n# print('RMSLE score for the XGBoost regressor is : {}'.format(np.sqrt(metrics.mean_squared_log_error(y_test,y_pred_xgb))))\n#print('Variance score for the XGBoost regressor raw is : %.2f' % regressor_xgbraw.score(X_test, y_test))\n\nprint(\"\\n\")\n\n#Evaluate the model with default params for Feature Selection Group\n#print('RMSE score for the XGBoost regressor is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_fs,y_pred_xgb))))\n# print('RMSLE score for the XGBoost regressor is : {}'.format(np.sqrt(metrics.mean_squared_log_error(y_test,y_pred_xgb))))\n#print('Variance score for the XGBoost regressor is : %.2f' % regressor_xgb.score(X_test_fs, y_test_fs))\n\nprint(\"\\n\")\n\n#Evaluate the model with Tuned params for Feature Selection Group\n#print('RMSE score for the XGBoost regressor1 is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_fs,y_pred_xgb1))))\n# print('RMSLE score for the XGBoost regressor1 is : {}'.format(np.sqrt(metrics.mean_squared_log_error(y_test_fs,y_pred_xgb1))))\n#print('Variance score for the XGBoost regressor1 is : %.2f' % regressor_xgb1.score(X_test_fs,y_test_fs))\n\nprint(\"\\n\")\n\n#Evaluate the model with PCA params  for Feature Extraction Group\nprint('RMSE score for the XGBoost regressor2 is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_pca, y_pred_xgb1))))\nprint('Variance score for the XGBoost regressor2 is : %.2f' % regressor_xgb1.score(X_test_pca, y_test_pca))","622fd376":"filename = 'xgbr.pkl'\njoblib.dump(regressor_xgb1, filename)","bde27ea0":"#Comparing test results for the XGBoost and RF regressor\nprint(\"Total sum of difference between the actual and the predicted values for the RF regressor is : %d\"%np.abs(np.sum(np.subtract(y_test,y_pred_rfpca))))\nprint(\"Total sum of difference between the actual and the predicted values for the tuned XGB regressor is : %d\"%np.abs(np.sum(np.subtract(y_test,y_pred_xgb1))))","a4cccd2c":"#Define a function to plot learning curve.\ndef learning_curves(estimator, title, features, target, train_sizes, cv, n_jobs=-1):\n    plt.figure(figsize = (14,5))\n    train_sizes, train_scores, validation_scores = learning_curve(estimator, features, target, train_sizes = train_sizes, cv = cv, scoring = 'neg_mean_squared_error',  n_jobs=n_jobs)\n    train_scores_mean = -train_scores.mean(axis = 1)\n    validation_scores_mean = -validation_scores.mean(axis = 1)\n    \n    plt.grid()\n    \n    plt.plot(train_sizes, train_scores_mean,'o-', color=\"r\", label = 'Training error')\n    plt.plot(train_sizes, validation_scores_mean,'o-', color=\"g\", label = 'Validation error')\n\n    plt.ylabel('MSE', fontsize = 14)\n    plt.xlabel('Training set size', fontsize = 14)\n    \n    title = 'Learning curves for a ' + title + ' model'\n    plt.title(title, fontsize = 18, loc='left')\n    \n    plt.legend(loc=\"best\")\n    \n    return plt\n\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=4)\n\n# Plot learning curve for the RF Regressor\ntitle = \"Random Forest Regressor\"\n\n# Call learning curve with all dataset i.e. traininig and test combined because CV will take of data split.\nlearning_curves(regressor_xgb1, title, X_opt,Y, train_sizes=np.linspace(.1, 1.0, 5), cv=cv, n_jobs=-1)\n\n#Plot learning curve for the XGBoost Regressor\n#title = \"XGBoost Regressor\"\n\n# Call learning curve on less number of estimators than the tuned estimator because it took too much time for the compilation.\n#learning_curves(XGBRegressor(n_estimators=111,\n                            #learning_rate=0.08,\n                            #gamma=0,\n                            #subsample=0.75,\n                           # colsample_bytree=1,\n                            #max_depth=7,\n                            #min_child_weight=4,\n                            #silent=1), title, X_opt,Y, train_sizes=np.linspace(.1, 1.0, 5), cv=cv, n_jobs=-1)\n\nplt.show()","c1762964":"PCA is applied on the training and the test dataset. Our input features are now ready for the regression.","8beb6848":"<a id=lin_reg><\/a>\n## Multiple Linear Regression\n***\nIt is used to explain the relationship between one continuous dependent variable and two or more independent variables. Let's proceed","72ecad81":"### Interesting find\n- There is approx **200% improvement** on the RMSE score for the Random forest regressor over the Linear regressor of the feature selection group.\n- Even the variance score is approx 1 which is a good score.\n- RMSE score for the RF regressor of feature extraction group is still very bad along with the variance score.\n- RMSE score for the feature selection group is more or less same as the raw data score. Sometimes the RMSE score for the raw data is better and vice versa. It fluctuates on every iteration and this is quite weird!\n\nLet's see if we can improve this further with the most sought after algorigthm i.e. XGBoost!!","fc545036":"### Model training\n***\nNow we will train the model on the filtered features. Our data has already been split so we will not split the data further.\n\n#### Note:\nWe used **GridSearch** to tune the **hyperparameters** of random forest regressor to get the best possible test score. We tried various combination of the allowed hyper params values. _But any kind of combination could not produce significantly better results than the default settings. There can be many reasons for that and it totally depends on the type of data we have in hand. Therefore we will not show tuned regressor results here._","903adda5":"### PCA application\nLet's apply PCA technique on the training features to understand how many principal components should we select for our model to capture atleast 90% variance. For that we will take help of plot and cumsum function of numpy package.","3a1ffdb7":"<a id=curve><\/a>\n## Learning curves\n***\nLearning curves constitute a great tool to diagnose bias and variance in any supervised learning algorithm. It shows how error changes as the training set size increases. We'll use the learning_curve() [function](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.learning_curve.html) from the scikit-learn library to generate a learning curve for the regression model. There's no need to put aside a validation set because learning_curve() will take care of that and that's why we will plot the learning curve over whole dataset.","01ad9e0b":"### General inference\n- XGBoost proved to be much more efficient in predicting the output. But it takes much more time to train it over the large dataset wih more complexity as compared to the RF and Linear regression model but less time then the SVR.\n- It didn't helped us much to generalize the model by tuning hyper parameters for the RF model as there is not much difference in the RMSE scores of the default model and the tuned model of the feature selection group infact both varies on every iteration and sometimes the tuned model gives poor results than the default model. Though we tried many possible alterations with GSCV but the tuning could not achieve a significant improvement over the default model which also depends on the contents of the dataset.\n- Contrast to the RF regressor, XGBoost regressor prediction results were consistent on every iteration i.e. for each param configuration the results were the same.\n- Feature extraction didn't helped in anyway to improve the RMSE score with any of the regressor models. This shows us that the feature extraction is somewhat not a good technique to preprocess the data before feeding it into the regressor models for the continous target value prediction. Whereas it also depends on the type and features of data that how it behaves with the model.","f35f9643":"## Further Scope..\n***\nThere's always a room for the improvement and a lot more to explore, and **if this helped you** in any way, I'd like to see **One Upvote!**. Also, please **leave comments** about any further improvements to this notebook!! Your feedback or any constructive criticism is highly appreciated.","5d3ecb2c":"<a id=rf_reg><\/a>\n## Random Forest Regressor\n***\nA random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting","340340f8":"### Observations\n- There is a significant **improvement** in the RMSE score for the **tuned** XGBoost regressor over the Random forest regressor when trained on the feature selection group.\n- But the performance of the **default** XGBoost regressor is quite **worse** than the default RF regressor on the same data.\n- Also, the RMSE score on the raw data and feature selected data are same, which disproves the theory that it is always better to select the relevant features which are statistically important. As the data behaves differently in different models.\n- Not to mention the fact that RMSE score for the XGBoost regressor of the feature extraction group is still bad along with the variance score. ","14182d22":"### Observations:\n- We can observe that both the models shows somewhat similar learning rate but with visible differences in error rates. \n- RF training curve initially starts high but later on improves as the training size increases and then seems to plateaud by the end.\n- XGBoost training curve on the other hand starts quite low and further improves with the increase in the training size and it too plateau towards the end.\n- Validation curve seems to show similar trend in both the models i.e. starts very high but improves with the training size with some differences in error rate i.e. XGBoost curve learning is quite fast and more accurate as compared to the RF one.\n- Both the models seems to suffer from **high variance** since the training curve error is very less in both the models.\n- The large gap at the end also indicates that the model suffers from quite a **low bias** i.e. overfitting the training data.\n- Also, both the model's still has potential to decrease and converge towards the training curve by the end.\n\n**At this point, here are a few things we could do to improve our model:**\n\n1. Add more training instances to improve validation curve in the XGBoost model.\n2. Increase the regularization for the learning algorithm. This should decrease the variance and increase the bias towards the validation curve.\n3. Reduce the numbers of features in the training data that we currently use. The algorithm will still fit the training data very well, but due to the decreased number of features, it will build less complex models. This should increase the bias and decrease the variance.","166e98ef":"### Observations\n - Very poor **Root mean squared** value. \n - And the low **variance score** which is also bad.\n - Both the models i.e. from the feature selection and the feature extraction group resulted quite bad in prediction.\n \n **Let's find out the reason of this behaviour:-**","b353d6b7":"Now our dataset is complete for the further analysis before we train our model with optimal variables.","eb88e9c8":"<a id=final><\/a>\n## End Notes\n***\nIn this project we covered various aspects of the Machine learning development cycle. We observed that the data exploration and variable analysis is a very important aspect of the whole cycle and should be done for thorough understanding of the data. We also cleaned the data while exploring as there were some outliers which should be treated before feature engineering. Further we did feature engineering to filter and gather only the optimal features which are more significant and covered most of the variance in the dataset. Then finally we trained the models on the optimum featureset to get the results. ","0264784a":"### Model Evaluation","4520edd6":"<a id=predict><\/a>\n## Model prediction\n***\nSo now, our model has been fitted to the training set. It's time to predict the dependent variable. Let's do that now.","2876360f":"<a id=evaluate><\/a>\n## Model evaluation\n***\nWe will evaluate our model's accuracy through two suggested metrics for the regression models. i.e. RMSE and variance score. Where RMSE of 0 and variance of 1 is considered as the best score for a prediction model.","e3f79a8c":"### Model prediction","863bba4f":"### Observations\n - All of the features shows **NO** correlation at all. Because feature extraction removes all collinearity.\n \nLet's move on to the Model now.","f66aa77b":"<a id=model><\/a>\n# Model\n***\nWe need a model to train on our dataset to serve our purpose of prediciting the NYC taxi trip duration given the other features as training and test set. Since our dependent variable contains continous values so we will use regression technique to predict our output.","f70b2ae6":"<a id=xgboost><\/a>\n## XGBoost Regressor\n***\nXGBoost (Extreme Gradient Boosting) is an optimized distributed gradient boosting library. It uses gradient boosting (GBM) framework at core. It belongs to a family of boosting algorithms that convert weak learners into strong learners. A weak learner is one which is slightly better than random guessing.\n\n'Boosting' here is a sequential process; i.e., trees are grown using the information from a previously grown tree one after the other. This process slowly learns from data and tries to improve its prediction in the subsequent iterations.","e0475788":"### Model evaluation","ad8a92cd":"<a id=train><\/a>\n## Model training\n***\nWe will first try with the default instantiation of the regressor object without using any generalization parameter. We will also **not perform any scaling** of the features because linear regression model takes care of that inherently. This is a plus point to use Linear regression model. It is quite fast to train even on very large datasets. So considering the size of our dataset this seems to be the correct approach as of now. Let's see how it performs.","f265f4da":"There we go, our feature set is now ready for the feature selection model with 1s in the first column for a0 constant.\n\nLet's fit stats model on the X array to figure out an optimal set of features by recursively checking for the highest p value and removing the feature of that index.\n\n### Note:\nHere we will take the level of significance as 0.05 i.e. 5% which means that we will reject feature from the list of array and re-run the model till p value for all the features goes below .05 to find out the optimal combination for our model.","fd9a426b":"### Observations\nWe can see that none of the feature is linearly correlated with the target variable **\"46\"**. That is why it is not a good model for the prediction of the trip duration. So let's move ahead and try the **random forest regressor**. We are not using decision tree regressor because the random forest will anyways consist of almost all its properties. Also, we will not use SVR because it takes too much time to train on this huge dataset even with the default settings. It seems to be not good with high dimensional dataset as well as for the huge instances.","f6f49dd6":"### Model training\n***\nWe will train the model on the filtered features. Our data has already been split so we will not split the data further.\n\n#### Note:\nWe used **GridSearch** to tune the **hyperparameters** of XGBoost regressor to get the best possible test score.  We will compare results from the default regressor and the tuned regressor.","7d5fac7d":"### Model prediction","edfaf4c8":"## Thank you guys... Yayyy!!!!\n\n\n<img src='https:\/\/media.agoramt.com.br\/2018\/08\/Minions.gif' align='left'\/>","354dd624":"### Observation\n- Here we can see that almost 40 variables are needed for capturing atleast 99% of the variance in the training dataset. Hence we will use the same set of variables.","2f92c5ad":"### Interesting find:\n - It took **approx 1 second to train the model** on dataset of more than 1 million records.\n - It is evident that Linear regression model is **extremely fast** to train on the high dimension datasets consisting of even **millions** of records.\n - Linear regression object for the feature extraction group took less time to train on the input features.","ad180039":"### Scale Data\nIt is suggested to scale the input varibles first before applying PCA to standardise the variance and avoid the bias. Lets Scale the data using StandardScaler.","075c8520":"<img src='http:\/\/www.sixthcents.net\/images\/macbook.gif'\/>"}}