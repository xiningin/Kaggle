{"cell_type":{"5d9bd7da":"code","22b1b82e":"code","6478e297":"code","597ba6aa":"code","89b6ca70":"code","9acb0dd1":"code","bb31fdb5":"code","b747a441":"code","ddb69c51":"code","776ff799":"code","2e9d8a1e":"code","82570ddc":"code","fc7b3423":"code","62645a76":"code","6da7a894":"code","1bcfdc48":"code","76b3c85d":"code","be9e0d99":"code","356dd905":"code","d9faae34":"code","8f96b321":"code","2f7f2469":"code","155105fc":"code","d70a2fa3":"code","71a4977c":"code","89a9ca36":"code","59490bfc":"code","a83dd8b1":"code","cad0481b":"code","443fea7d":"code","6da6bb02":"code","13d95957":"code","84ee0438":"code","cf12c5db":"code","c06a8e5d":"code","800a7890":"code","8f57bd6e":"code","673d21df":"code","843969ae":"code","c32260f0":"code","ed27af8c":"code","0b20d46a":"code","b0759378":"code","27d2f5c5":"code","e6350f45":"code","29528856":"code","4fefb500":"code","f325d311":"code","49e7fbc9":"code","c596061c":"code","2620e566":"code","c9e7a98e":"code","1c0c5e40":"code","9d32914e":"code","05a99a29":"code","b4d3b588":"code","be7309a3":"code","24151fad":"code","efd57dc9":"markdown"},"source":{"5d9bd7da":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","22b1b82e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport glob\nimport gc\nimport os","6478e297":"def convert_to_32bit(df):\n    for f in df.columns:\n        if df[f].dtype == 'int64':\n            df[f] = df[f].astype('int32')\n        if df[f].dtype == 'float64':\n            df[f] = df[f].astype('float32')\n    return df","597ba6aa":"pd.set_option('max_rows', 300)\npd.set_option('max_columns', 300)\npd.set_option('max_colwidth', 300)","89b6ca70":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgb\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n\n","9acb0dd1":"#data_dir=r'C:\/91_data_science\/optiver\/full_input'","bb31fdb5":"data_dir = '..\/input\/optiver-realized-volatility-prediction\/' ","b747a441":"def rmspe(y_true, y_pred):\n        return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\ndef feval_RMSPE(preds, train_data):\n    labels = train_data.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False","ddb69c51":"def convert_to_32bit(df):\n    for f in df.columns:\n        if df[f].dtype == 'int64':\n            df[f] = df[f].astype('int32')\n        if df[f].dtype == 'float64':\n            df[f] = df[f].astype('float32')\n    return df","776ff799":"train_target = pd.read_csv(data_dir + '\/train.csv')\ntime_ids = np.unique(train_target.time_id) \ntrain_target['row_id'] = train_target['stock_id'].astype(str) + '-' + train_target['time_id'].astype(str)\ntrain_target = train_target[['row_id','target','stock_id']]\ntrain_target.head()\nprint(f\" Time ids shape {time_ids.shape}\")","2e9d8a1e":"# Add group column in target dataframe","82570ddc":"test_target = pd.read_csv(data_dir + '\/test.csv')\nprint(test_target.shape)\ntest_target = test_target[['row_id','stock_id']]\ndisplay(test_target.head(2))\nprint(test_target.shape)","fc7b3423":"# Constants\n#stock_id_filter=[0,1,10,120,122,123,124,125, 126,2,3,4,41,42,43,44,46,47,48,5,50,6,7,8, 81,82,83,84,85,86,87,88, 89,9,90]\n#stock_id_filter=[0,102,3,112,103,12,116,118,126,22,5,55,78,83,87,88]\n#stock_id_filter=[0,31,]\n#test_target=test_target[test_target['stock_id'].isin(stock_id_filter)]\n#train_target=train_target[train_target['stock_id'].isin(stock_id_filter)]","62645a76":"from numba import jit\n\n@jit\ndef vola(x):\n    return np.sqrt(np.sum(x*x))\n\n@jit\ndef log_wap_max(x):\n    return np.sqrt(np.max(x*x))\n","6da7a894":"def calculate_diff(df, diff_cols, sort_cols=['time_id','seconds_in_buckets']):\n    \n    df.sort_values(by=sort_cols, inplace=True)\n    \n    for key, value in diff_cols.items():\n        df[value]=df.groupby([\"time_id\"])[key].diff().fillna(0)\n        \n        \n    #print(diff_cols.values())    \n    return df","1bcfdc48":"def calc_vola (df, by_cols=['time_id'], starts_seconds=0):\n    \n    df = df[df['seconds_in_bucket'] > starts_seconds].groupby(by_cols).agg(\n    \n            vola_1 = ('return1_diff', lambda x: vola(x.values)),\n            vola_2 = ('return2_diff', lambda x: vola(x.values)),\n            \n         ).reset_index()\n    \n    return df","76b3c85d":"def group_quintiles(df, by_cols=['time_id'], col_names=['r1_diff','r2_diff'], prefix=None):\n    \n    # NO absolute Value columns\n    df = df.groupby(by_cols)[col_names].quantile([.1, .5, .7, .98, 1.0 ]).reset_index()\n    \n    # pivot the frame\n    \n    df = pd.pivot_table(df, index=[\"time_id\"], columns=[\"level_1\"], values=col_names)\n    df.columns = ['_'.join((i, str(int(j*100)))).strip('') for  i,j in df.columns]\n    df = df.reset_index()\n    \n    if prefix:    \n        columns_list = df.columns.to_list()\n        cols_to_rename = [col for col in columns_list if col not in ['time_id']]\n        renamed_cols = [col + '_' + str(prefix) for col in columns_list if col not in ['time_id']]\n        rename_dict = dict(zip(cols_to_rename, renamed_cols))\n        df.rename(columns=rename_dict, inplace=True)\n\n    \n    return df","be9e0d99":"def trade_groups(df, by_cols=['time_id'], start_seconds=0, prefix='m0'):\n    \n    df = df.groupby(by_cols).agg(\n    \n                        price_mean = ('price', np.nanmean),\n                        size_mean  = ('size', np.nanmean)  ,\n                        cnt_mean   = ('order_count', np.nanmean) ,\n                        trade_vol_per_cnt_mean = ( 'trade_vol_per_cnt', np.nanmean),\n        \n                        price_std = ('price', np.nanstd),\n                        size_std  = ('size', np.nanstd)  ,\n                        trade_vol_per_cnt_std = ( 'trade_vol_per_cnt', np.nanstd),\n                    \n                        tv_sum = ('tv', np.sum),\n                        cnt_sum   = ('order_count', np.sum) ,\n        \n        ).reset_index()\n    \n    columns_list = df.columns.to_list()\n    cols_to_rename = [col for col in columns_list if col not in ['time_id']]\n    renamed_cols = [col + '_' + str(prefix) for col in columns_list if col not in ['time_id']]\n    rename_dict = dict(zip(cols_to_rename, renamed_cols))\n    df.rename(columns=rename_dict, inplace=True)\n    \n    return df","356dd905":"%%time\n\ndef trade_processing(data_dir=data_dir, datatype='train', stock_id=0):\n    \n    df_td = pd.read_parquet(f'{data_dir}\/trade_{datatype}.parquet\/stock_id={stock_id}')\n    df_td = convert_to_32bit(df_td)\n    df_td['trade_vol_per_cnt'] =df_td['price']*df_td['size']\/df_td['order_count']\n    df_td['tv'] =df_td['price']*df_td['size']\n    \n    df1 = trade_groups (df_td, by_cols=['time_id'], start_seconds=0, prefix='t0')\n    df2 = trade_groups (df_td, by_cols=['time_id'], start_seconds=200, prefix='t200')\n    df3 = trade_groups (df_td, by_cols=['time_id'], start_seconds=400, prefix='t400')\n    \n    \n    df_qnt = group_quintiles(df_td, by_cols=['time_id'], col_names=['price','size'], prefix=None)\n    \n    df_trade = df1.merge(df2, on='time_id', how='left')\n    df_trade = df_trade.merge(df3, on='time_id', how='left')\n    \n    \n    df_trade = df_trade.merge(df_qnt, on='time_id', how='left')\n    \n    df_trade['row_id'] = df_trade['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    df_trade['stock_id'] = stock_id\n   \n    \n    return df_trade","d9faae34":"%%time\ntr1 = trade_processing(data_dir=data_dir, datatype='train', stock_id=0)\ntr1.columns","8f96b321":"tr1.head()","2f7f2469":"def book_groups(df, by_cols=['time_id'], start_seconds=0, prefix='m0'):\n    \n    df = df[df['seconds_in_bucket'] > start_seconds].groupby(by_cols).agg(\n    \n                        size_spread1_mean = ('size_spread1', np.mean),\n                        size_spread1_std = ('size_spread1', np.std),\n\n                        size_spread2_mean = ('size_spread2', np.mean),\n                        size_spread2_std = ('size_spread2', np.std),\n\n                        price_spread1_mean = ('price_spread1', np.mean),\n                        price_spread1_std = ('price_spread1', np.std),\n\n                        price_spread2_mean = ('price_spread2', np.mean),\n                        price_spread2_std = ('price_spread2', np.std),\n\n                        price_spread3_mean = ('price_spread3', np.mean),\n                        price_spread3_std = ('price_spread3', np.std),\n                      \n                        mid1_mean = ('mid1', np.mean),\n                        mid1_std = ('mid1', np.std),\n\n                        mid2_mean = ('mid2', np.mean),\n                        mid2_std = ('mid2', np.std),\n        \n                        return1_diff_max = ('return1_diff' ,np.std),\n                        return2_diff_max = ('return2_diff' ,np.std),\n        \n                        log_return1 = ('log_return1', np.max),\n                        log_return2 =('log_return2',   np.max),\n        \n                        log_return1_sum = ('log_return1', np.sum),\n                        log_return2_sum =('log_return2',   np.sum),\n\n        ).reset_index()\n    \n    columns_list = df.columns.to_list()\n    cols_to_rename = [col for col in columns_list if col not in ['time_id']]\n    renamed_cols = [col + '_' + str(prefix) for col in columns_list if col not in ['time_id']]\n    rename_dict = dict(zip(cols_to_rename, renamed_cols))\n    df.rename(columns=rename_dict, inplace=True)\n    \n    return df","155105fc":"%%time\n\ndef stock_processing(data_dir=data_dir, datatype='train', stock_id=0):\n    \n    df = pd.read_parquet(f'{data_dir}\/book_{datatype}.parquet\/stock_id={stock_id}')\n    df = convert_to_32bit(df)\n    bpr1, bsz1, apr1, asz1 = (df[col].values for col in [ 'bid_price1','bid_size1','ask_price1','ask_size1' ])\n    bpr2, bsz2, apr2, asz2 = (df[col].values for col in [ 'bid_price2','bid_size2','ask_price2','ask_size2'])\n   \n    df['log_return1']=np.log(((bpr1 * asz1) + (apr1 * bsz1)) \/ (asz1 + bsz1))\n    \n    df.loc[df['bid_size2'] > 0 , 'log_return2']=np.log(((bpr2 * asz2) + (apr2 * bsz2)) \/ (asz2 + bsz2))\n    df.loc[df['bid_size2'] > 0 , 'size_spread2'] = (bsz2 -asz2)\/ (asz2 + bsz2)\n    df.loc[df['bid_size2'] > 0 , 'price_spread2']=(apr2\/bpr2) - 1\n    df.loc[df['bid_size2'] > 0 , 'mid2']=(apr2 - bpr2)\/2\n    \n    df['size_spread1'] = (bsz1 -asz1)\/ (asz1 + bsz1)\n    df['price_spread1']=(apr1\/bpr1) - 1\n    df['mid1']=(apr1 - bpr1)\/2\n    \n    df.loc[df['bid_size2'] > 0 , 'price_spread3']=(apr2\/bpr1) - 1\n    \n    diff_cols = { 'log_return1' : 'return1_diff', \n                 'log_return2' : 'return2_diff',\n                }\n    \n    df = calculate_diff(df, diff_cols, sort_cols=['time_id','seconds_in_bucket'])\n    \n    df['return1_diff_sqr']=np.sqrt(df['return1_diff']**2)\n    df['return2_diff_sqr']=np.sqrt(df['return2_diff']**2)\n    \n    df_vola = calc_vola (df, by_cols=['time_id'],starts_seconds=0)\n    \n    \n    # cretae group for book data\n    df_1 = book_groups(df, by_cols=['time_id'], start_seconds=0, prefix='m0')\n    df_2 = book_groups(df, by_cols=['time_id'], start_seconds=200, prefix='m2')\n    df_4 = book_groups(df, by_cols=['time_id'], start_seconds=400, prefix='m6')\n    \n    df_qnt = group_quintiles(df, by_cols=['time_id'], col_names=['return1_diff','price_spread1','return2_diff',\n                                                                 'size_spread1','price_spread2', 'size_spread2',\n                                                                 'return1_diff_sqr', 'return2_diff_sqr', 'price_spread3',\n                                                                ], prefix=None)\n    \n    df_vola = df_vola.merge(df_1, on='time_id', how='left')\n    df_vola = df_vola.merge(df_qnt, on='time_id', how='left')\n    \n  \n    df_vola['row_id'] = df_vola['time_id'].apply(lambda x:f'{stock_id}-{x}')\n    df_vola['stock_id'] = stock_id\n   \n    \n    return df_vola","d70a2fa3":"%%time\nb1 = stock_processing(data_dir=data_dir, datatype='train', stock_id=0)\nb1.columns","71a4977c":"b1.head(3)","89a9ca36":"def preprocessor(data_dir,list_stock_ids, datatype='train', book_trade='book'):\n    from joblib import Parallel, delayed # parallel computing to save time\n    df = pd.DataFrame()\n    \n    def for_joblib(stock_id):\n        if book_trade=='book':\n            df_tmp = stock_processing(data_dir, datatype=datatype, stock_id=stock_id)\n            return pd.concat([df,df_tmp])\n        else:\n            df_tmp = trade_processing(data_dir, datatype=datatype, stock_id=stock_id)\n            return pd.concat([df,df_tmp])\n   \n    df = Parallel(n_jobs=-1, verbose=1)(\n        delayed(for_joblib)(stock_id) for stock_id in list_stock_ids\n        )\n\n    df =  pd.concat(df,ignore_index = True)\n    return df\n","59490bfc":"%%time\ntrain_ids= train_target.stock_id.unique()\ntrain_book_df = preprocessor(data_dir,train_ids,datatype='train',book_trade='book')\ntrain_trade_df = preprocessor(data_dir,train_ids,datatype='train',book_trade='trade')\n","a83dd8b1":"%%time\ntest_ids= test_target.stock_id.unique()\ntest_book_df = preprocessor(data_dir,test_ids,datatype='test',book_trade='book')\ntest_trade_df = preprocessor(data_dir,test_ids,datatype='test',book_trade='trade')\n","cad0481b":"%%time\ntrain_vola = train_book_df.merge(train_trade_df, on =['time_id','row_id','stock_id'], how='left')\ntest_vola = test_book_df.merge(test_trade_df, on =['time_id','row_id','stock_id'], how='left')\n","443fea7d":"# Take log transformation of volatility\n\n#train_vola['vola_1']=np.log1p(train_vola['vola_1'])\n#test_vola['vola_2']=np.log1p(test_vola['vola_2'])\n","6da6bb02":"train = train_vola.copy()\ntest = test_vola.copy()\ntrain.shape, test.shape","13d95957":"test.head()","84ee0438":"# We don't have all stocks in test dataset. bring one random time_id from train, this will not output results much !\n\nrandom_test_data = train[train['time_id']== time_ids[5]].copy()\nrandom_test_data.head()\n\ntest = pd.concat([test, random_test_data])\ntest.head()","cf12c5db":"from sklearn.cluster import KMeans\n# making agg features\n\ntrain_p = pd.read_csv(data_dir + '\/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\n\ncorr = train_p.corr()\n\nids = corr.index\n\nkmeans = KMeans(n_clusters=7, random_state=0, max_iter=2000, n_init=5).fit(corr.values)\nprint(kmeans.labels_)\ndf = pd.DataFrame( {'stock_id': [ f for f in corr.columns ], 'tgt_cluster': kmeans.labels_} )\ndf = convert_to_32bit(df)\n\ndel train_p, corr, kmeans\n_ = gc.collect()\n\ntrain = train.merge(df, on='stock_id', how='left')\ntest = test.merge(df, on='stock_id', how='left')\n\n# Clusters found\ntrain.groupby('tgt_cluster')['time_id'].agg('count')\n","c06a8e5d":"# Add Spread based Second cluster ********","800a7890":"from sklearn.cluster import KMeans\n# making agg features\n\ntrain_p = train.pivot(index='time_id', columns='stock_id', values='price_spread2_std_m0')\n\ncorr = train_p.corr()\n\nids = corr.index\n\nkmeans = KMeans(n_clusters=5, random_state=0, max_iter=2000, n_init=5).fit(corr.values)\nprint(kmeans.labels_)\ndf = pd.DataFrame( {'stock_id': [ f for f in corr.columns ], 'bas_cluster': kmeans.labels_} )\ndf = convert_to_32bit(df)\n\ndel train_p, corr, kmeans\n_ = gc.collect()\n\ntrain = train.merge(df, on='stock_id', how='left')\ntest = test.merge(df, on='stock_id', how='left')\n\n# Clusters found\ntrain.groupby('bas_cluster')['time_id'].agg('count')\n","8f57bd6e":"from sklearn.cluster import KMeans\n# making agg features\n\ntrain_p = train.pivot(index='time_id', columns='stock_id', values='tv_sum_t0')\n\ncorr = train_p.corr()\n\nids = corr.index\n\nkmeans = KMeans(n_clusters=4, random_state=0, max_iter=2000, n_init=5).fit(corr.values)\nprint(kmeans.labels_)\ndf = pd.DataFrame( {'stock_id': [ f for f in corr.columns ], 'tvsize_cluster': kmeans.labels_} )\ndf = convert_to_32bit(df)\n\ndel train_p, corr, kmeans\n_ = gc.collect()\n\ntrain = train.merge(df, on='stock_id', how='left')\ntest = test.merge(df, on='stock_id', how='left')\n\n# Clusters found\ntrain.groupby('tvsize_cluster')['time_id'].agg('count')\n","673d21df":"from sklearn.cluster import KMeans\n# making agg features\n\ntrain_p = train.pivot(index='time_id', columns='stock_id', values='log_return1_sum_m0')\n\ncorr = train_p.corr()\n\nids = corr.index\n\nkmeans = KMeans(n_clusters=5, random_state=0, max_iter=2000, n_init=5).fit(corr.values)\nprint(kmeans.labels_)\ndf = pd.DataFrame( {'stock_id': [ f for f in corr.columns ], 'logr_cluster': kmeans.labels_} )\ndf = convert_to_32bit(df)\n\ndel train_p, corr, kmeans\n_ = gc.collect()\n\ntrain = train.merge(df, on='stock_id', how='left')\ntest = test.merge(df, on='stock_id', how='left')\n\n# Clusters found\ntrain.groupby('logr_cluster')['time_id'].agg('count')","843969ae":"# First Level Aggrregation -- time id","c32260f0":"#test.columns.to_list()","ed27af8c":"def group_kpis(group_vars=['time_id'], datatype='train'):\n    if datatype=='train':\n        df=train\n    else :\n        df=test\n    \n    kpis_at_time_level = df.groupby(group_vars).agg(    t_vola1_mean         =  ('vola_1', np.mean),\n                                                        t_vola2_mean         =  ('vola_2', np.mean),\n                                                      \n                                                        t_price_std_t0_mean  =  ('price_std_t0', np.mean),\n                                                        t_price_std_t300_mean  = ('price_std_t400', np.mean),\n                                                   \n                                                        t_tv_sum_t0_mean = ('tv_sum_t0', np.mean),\n                                                        t_tv_sum_t300_mean = ('tv_sum_t400', np.mean),\n                                                        \n                                                        t_return1_diff_max_m0_mean = ('return1_diff_max_m0', np.mean),\n                                                        t_return2_diff_max_m0_mean = ('return2_diff_max_m0', np.mean),\n                                                       \n                                                        t_cnt_mean_t0_mean  = ('cnt_mean_t0', np.mean),\n                                                        t_trade_vol_per_cnt_mean_t0_mean =('trade_vol_per_cnt_mean_t0', np.mean),\n                                                    \n                                                        t_price_spread1_std = ('price_spread1_std_m0', np.mean),\n                                                        t_price_spread2_std = ('price_spread2_std_m0', np.mean),\n                                                        t_price_spread3_std = ('price_spread3_std_m0', np.mean),\n\n\n                                                    ).reset_index()\n    \n    print(\"Number of records in {} are {}\".format(datatype, kpis_at_time_level.shape))\n    return kpis_at_time_level","0b20d46a":"train_vola_by_time = group_kpis(group_vars=['time_id'], datatype='train')\ntest_vola_by_time = group_kpis(group_vars=['time_id'], datatype='test')\n\ndisplay(train_vola_by_time.head(2))\ndisplay(test_vola_by_time.head(2))\n","b0759378":"# Second  Aggrregation -- time id, tgt_cluster\ntrain_vola_by_tgt_time = group_kpis(group_vars=['time_id','tgt_cluster'], datatype='train')\ntest_vola_by_tgt_time = group_kpis(group_vars=['time_id','tgt_cluster'], datatype='test')\n\n# Third Aggrregation -- time id, bas_cluster\ntrain_vola_by_bas_time = group_kpis(group_vars=['time_id','bas_cluster'], datatype='train')\ntest_vola_by_bas_time = group_kpis(group_vars=['time_id','bas_cluster'], datatype='test')\n\n# Fourth Aggrregation -- time id, tvsize_cluster\ntrain_vola_by_tv_time = group_kpis(group_vars=['time_id','tvsize_cluster'], datatype='train')\ntest_vola_by_tv_time = group_kpis(group_vars=['time_id','tvsize_cluster'], datatype='test')\n\n# fifth Aggrregation -- time id, tvsize_cluster\ntrain_vola_by_logr_time = group_kpis(group_vars=['time_id','logr_cluster'], datatype='train')\ntest_vola_by_logr_time = group_kpis(group_vars=['time_id','logr_cluster'], datatype='test')\n","27d2f5c5":"# do the pivot \ndef create_pivot(df,columns, pivot_cols):\n\n    df = pd.pivot_table(df, index=[\"time_id\"], columns=columns, values=pivot_cols)\n    df.columns = ['_'.join((i, str(int(j))+ '_' + columns )).strip('') for  i,j in df.columns]\n    df = df.reset_index()\n    df.head()\n    \n    return df\n","e6350f45":"%%time\n\ntrain_tvsize = create_pivot(train_vola_by_tv_time ,columns='tvsize_cluster',  pivot_cols =['t_vola1_mean', 't_vola2_mean','t_price_std_t0_mean', \n                                                                                           't_tv_sum_t0_mean','t_return1_diff_max_m0_mean','t_return2_diff_max_m0_mean'])\n\ntrain_bas = create_pivot(train_vola_by_bas_time ,columns='bas_cluster',  pivot_cols =['t_vola1_mean', 't_vola2_mean','t_price_std_t0_mean',\n                                                                                      't_tv_sum_t0_mean' ,'t_return1_diff_max_m0_mean','t_return2_diff_max_m0_mean'])\n\ntrain_tgt = create_pivot(train_vola_by_tgt_time ,columns='tgt_cluster',  pivot_cols =['t_vola1_mean', 't_vola2_mean','t_price_std_t0_mean','t_tv_sum_t0_mean',\n                                                                                      't_return1_diff_max_m0_mean','t_return2_diff_max_m0_mean'])\n\ntrain_logr = create_pivot(train_vola_by_logr_time ,columns='logr_cluster',  pivot_cols =['t_price_std_t0_mean','t_tv_sum_t0_mean',\n                                                                                      't_return1_diff_max_m0_mean','t_return2_diff_max_m0_mean'])\n\n\n\ntest_tvsize = create_pivot(test_vola_by_tv_time ,columns='tvsize_cluster',  pivot_cols =['t_vola1_mean', 't_vola2_mean','t_price_std_t0_mean','t_tv_sum_t0_mean',\n                                                                                         't_return1_diff_max_m0_mean','t_return2_diff_max_m0_mean'])\n\n\ntest_bas = create_pivot(test_vola_by_bas_time ,columns='bas_cluster',  pivot_cols =['t_vola1_mean', 't_vola2_mean','t_price_std_t0_mean','t_tv_sum_t0_mean',\n                                                                                    't_return1_diff_max_m0_mean','t_return2_diff_max_m0_mean'])\n\n\ntest_tgt = create_pivot(test_vola_by_tgt_time ,columns='tgt_cluster',  pivot_cols =['t_vola1_mean', 't_vola2_mean','t_price_std_t0_mean','t_tv_sum_t0_mean',\n                                                                                    't_return1_diff_max_m0_mean','t_return2_diff_max_m0_mean'])\n\ntest_logr = create_pivot(test_vola_by_logr_time ,columns='logr_cluster',  pivot_cols =['t_price_std_t0_mean','t_tv_sum_t0_mean',\n                                                                                      't_return1_diff_max_m0_mean','t_return2_diff_max_m0_mean'])\n","29528856":"train_vola_by_time = train_vola_by_time.merge(train_tvsize, on='time_id', how='left').fillna(0)\ntrain_vola_by_time = train_vola_by_time.merge(train_bas, on='time_id', how='left').fillna(0)\ntrain_vola_by_time = train_vola_by_time.merge(train_tgt, on='time_id', how='left').fillna(0)\ntrain_vola_by_time = train_vola_by_time.merge(train_logr, on='time_id', how='left').fillna(0)\n\n\ntest_vola_by_time = test_vola_by_time.merge(test_tvsize, on='time_id', how='left').fillna(0)\ntest_vola_by_time = test_vola_by_time.merge(test_bas, on='time_id', how='left').fillna(0)\ntest_vola_by_time = test_vola_by_time.merge(test_tgt, on='time_id', how='left').fillna(0)\ntest_vola_by_time = test_vola_by_time.merge(test_logr, on='time_id', how='left').fillna(0)\n\n\ntrain_vola_by_time.head()\n\n","4fefb500":"# join with stock Level data\n# join time level KPIs'\ntrain_data_t = train.merge(train_vola_by_time, on = [\"time_id\"], how = \"left\").reset_index(drop=True)\ntest_data_t = test.merge(test_vola_by_time, on = [\"time_id\"], how = \"left\").reset_index(drop=True)\n\ndisplay(train_data_t.head(2))\ndisplay(test_data_t.head(2))\n\ntrain_data_t.shape, test_data_t.shape","f325d311":"train_data_with_target= train_target.merge(train_data_t, on = [\"stock_id\" , \"row_id\"], how = \"left\").fillna(0)\ntest_data_without_target= test_target.merge(test_data_t, on = [\"stock_id\" , \"row_id\"], how = \"left\").fillna(0)\ndisplay(train_data_with_target.head(2))\ndisplay(test_data_without_target.head(2))","49e7fbc9":"train_data_with_target.shape,  test_data_without_target.shape","c596061c":"\n#transform traget to factor( Multiplication Factor)\ntrain_data_with_target['multi_factor']=train_data_with_target['target']\/train_data_with_target['vola_1']\ntrain_data_with_target['multi_factor']=np.log1p(train_data_with_target['multi_factor'])\n\n\n","2620e566":"features_to_consider =   [col for col in train_data_with_target.columns if col not in {\"time_id\", \"target\", \"row_id\" , \"multi_factor\",\n                                                                                       \"pred_lgb1\" , \n                                                                                       'tgt_cluster','bas_cluster','tvsize_cluster' }]\n\n#features_to_consider\n\n","c9e7a98e":"sample_df = train_data_with_target[train_data_with_target['stock_id'].isin([0,46,31])].drop(columns='multi_factor')\nsample_df.rename(columns={\"target\": \"orignal_target\"}, inplace=True)\nsample_df.head()","1c0c5e40":"from sklearn import preprocessing, model_selection\nimport lightgbm as lgb\n\ncats = ['stock_id']\n#categorical_feature=cats,\nfeatures_to_consider =   [col for col in train_data_with_target.columns if col not in {\"time_id\", \"target\", \"row_id\" , \"multi_factor\",\n                                                                                       \"pred_lgb1\" , \"orignal_target\",\n                                                                                       'tgt_cluster','bas_cluster','tvsize_cluster' }]\n\n\nn_folds = 5\nn_rounds = 3000\n\ntrain=train_data_with_target\ntest=test_data_without_target\ntest['multi_factor'] = 0\nsample_df['multi_factor']=0\n\ntarget_name = 'multi_factor'\nscores_folds = {}\n\nmodel_name = 'lgb1'\npred_name = 'pred_{}'.format(model_name)\nprint('We consider {} features'.format(len(features_to_consider)))\n\ntrain[pred_name] = 0\n\nparams_lgbm =  {\n    'objective': 'rmse',\n    'boosting_type': 'gbdt',\n    'max_depth': -1,\n    'max_bin':255,\n    'min_data_in_leaf':750,\n    'learning_rate': 0.05,\n    'subsample': 0.72,\n    'subsample_freq': 3,\n    'feature_fraction': 0.5,\n    'lambda_l1': 0.5,\n    'lambda_l2': 1.0,\n    'categorical_column':[0],\n    'seed':2021,\n    'n_jobs':-1,\n    'verbose': -1,\n    'device': 'gpu',\n    'num_gpu': 1,\n    'gpu_platform_id':-1,\n    'gpu_device_id':-1,\n    'gpu_use_dp': False,\n}\n\n\nkf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2016)\nscores_folds[model_name] = []\ncounter = 1\nfor dev_index, val_index in kf.split(range(len(train))):\n    print('CV {}\/{}'.format(counter, n_folds))\n    X_train = train.loc[dev_index, features_to_consider]\n    y_train = train.loc[dev_index, target_name].values\n    X_val = train.loc[val_index, features_to_consider]\n    vola_1 = train.loc[val_index, 'vola_1']\n    y_val = train.loc[val_index, target_name].values\n   \n    \n    #############################################################################################\n    #LGB\n    #############################################################################################\n    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=cats, weight=1\/np.power(y_train, 3))\n    val_data = lgb.Dataset(X_val, label=y_val,categorical_feature=cats)\n    \n    model = lgb.train(params_lgbm, \n                      train_data, \n                      n_rounds, \n                      valid_sets=val_data, \n                      feval=feval_RMSPE,\n                      verbose_eval= 250,\n                      early_stopping_rounds=1000\n                     )\n    preds = model.predict(train.loc[val_index, features_to_consider])\n    preds =np.expm1(preds)*vola_1\n    train.loc[val_index, pred_name] = preds\n    score = round(rmspe(y_true = np.expm1(y_val)*vola_1, y_pred = preds),5)\n    \n    print('Fold {} {}: {}'.format(counter, model_name, score))\n    scores_folds[model_name].append(score)\n    counter += 1\n    \n    \n    test['multi_factor'] += model.predict(test[features_to_consider])\n    sample_df['multi_factor'] += model.predict(sample_df[features_to_consider])\n    \ndel train_data, val_data","9d32914e":"importances = pd.DataFrame({'Feature': model.feature_name(), \n                        'Importance': model.feature_importance(importance_type='gain')})\nimportances.sort_values(by = 'Importance', inplace=True)\nimportances2 = importances.nlargest(30,'Importance', keep='first').sort_values(by='Importance', ascending=True)\nimportances2[['Importance', 'Feature']].plot(kind = 'barh', x = 'Feature', figsize = (8,8), color = 'blue', fontsize=11);plt.ylabel('Feature', fontsize=12)","05a99a29":"train[['row_id', 'vola_1', 'return1_diff_100', 'target', 'pred_lgb1']].query(\"row_id=='31-4142'\")","b4d3b588":"sample_df['target'] = np.expm1(sample_df[target_name]\/n_folds )*sample_df['vola_1']\ndisplay(sample_df[['row_id',  'vola_1', 'return1_diff_100', 'orignal_target', 'multi_factor','target']].tail(4))","be7309a3":"test['target'] = np.expm1(test[target_name]\/n_folds )*test['vola_1']\ndisplay(test[['row_id', 'target']].head(4))","24151fad":"test[['row_id', 'target']].to_csv('submission.csv',index = False)","efd57dc9":"# Clustering "}}