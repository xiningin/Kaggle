{"cell_type":{"5f7c80f4":"code","cd176d71":"code","73b225cc":"code","4edc5e19":"code","a139e812":"code","ab056156":"code","6f35d367":"code","8e5db63c":"code","63a0ee55":"code","c29e09e2":"code","2daf0d14":"code","01ba43af":"code","8727a01b":"code","984340b4":"code","d8ac2e16":"code","2a10b4ab":"code","c4fee236":"code","ebb5fee4":"code","46121b10":"code","76203abe":"code","3e0b6a79":"code","df694591":"code","b1659381":"code","a01843d2":"code","e684dfd2":"code","6d6694dd":"code","4c248c44":"code","83b32003":"code","95828946":"code","3b1fe8c6":"code","dcfc62c3":"code","08fc1307":"code","74ed24b3":"code","cf3a8456":"code","5d9db847":"code","d1a103d2":"code","6725cfae":"code","cef1cc00":"code","bb38ed00":"code","69d696e3":"code","e92bab08":"code","76b01ba8":"code","199c412f":"code","fe5fd249":"code","049a6b0b":"code","b4af2e91":"code","25b5cdee":"markdown","54d78b2a":"markdown","0748f910":"markdown","7fd84507":"markdown"},"source":{"5f7c80f4":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","cd176d71":"import nltk\nfrom nltk import word_tokenize\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom nltk import pos_tag\nstop_words = stopwords.words('english')\n\nimport spacy\nnlp = spacy.load('en_core_web_lg')\nfrom spacy import displacy\nimport string\nimport re\nfrom collections import Counter","73b225cc":"train_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\nsubmission = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')","4edc5e19":"train_df.head()","a139e812":"def removeStopwords(text):\n    doc = nlp(text)\n    clean_text = ' '\n    for txt in doc:\n        if (txt.is_stop == False):\n            clean_text = clean_text + ' ' + str(txt)\n    return clean_text","ab056156":"def removePunctuations(text):\n    return text.translate(str.maketrans('','', string.punctuation))","6f35d367":"def removeLinks(text):\n    clean_text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    \n    return clean_text","8e5db63c":"def removeNumbers(text):\n    clean_text = re.sub(r'\\d+','',text)\n    return clean_text","63a0ee55":"def clean(text):\n    text = text.lower()\n    text = removeStopwords(text)\n    text = removePunctuations(text)\n    text = removeLinks(text)\n    text = removeNumbers(text)\n    return text","c29e09e2":"train_df['excerpt_clean'] = train_df['excerpt'].apply(clean)\ntest_df['excerpt_clean'] = test_df['excerpt'].apply(clean) ","2daf0d14":"results = Counter()\ntrain_df['excerpt_clean'].str.lower().str.split().apply(results.update)\nprint(len(results.keys()))","01ba43af":"train_df['excerpt_clean']","8727a01b":"stemmer = SnowballStemmer(language = 'english')\ntokens = train_df['excerpt_clean'][1].split()\nclean_text = ''\nfor token in tokens:\n    print(token + '-->' + stemmer.stem(token))","984340b4":"def stemWord(text):\n    stemmer = SnowballStemmer(language='english')\n    tokens = text.split()\n    clean_text = ''\n    for token in tokens:\n        clean_text = clean_text + ' ' + stemmer.stem(token)\n    return clean_text","d8ac2e16":"train_df['excerpt_clean'] = train_df['excerpt_clean'].apply(stemWord)","2a10b4ab":"test_df['excerpt_clean'] = test_df['excerpt_clean'].apply(stemWord)","c4fee236":"results = Counter()\ntrain_df['excerpt_clean'].str.lower().str.split().apply(results.update)\nprint(len(results.keys()))","ebb5fee4":"from sklearn.model_selection import train_test_split","46121b10":"X = train_df['excerpt_clean']\ny = train_df['target']\n\nX_test = test_df['excerpt_clean']\n\n\nprint(len(X),len(y))\nprint(len(X_test))","76203abe":"text = train_df['excerpt_clean']","3e0b6a79":"vocab_size = 16662\nembedding_dim = 64\nmax_length = 50\ntranc_type = 'post'\npad_type = 'post'\noov_tok = '<OOV>'","df694591":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(num_words = vocab_size,oov_token=oov_tok)\ntokenizer.fit_on_texts(text)\nword_index = tokenizer.word_index","b1659381":"training_sequences = tokenizer.texts_to_sequences(text)\ntraining_padded = pad_sequences(training_sequences, maxlen = max_length,truncating=tranc_type,padding = pad_type)\ntraining_label_final = np.array(train_df['target'])","a01843d2":"testing_sequences = tokenizer.texts_to_sequences(test_df['excerpt_clean'])\ntesting_padded = pad_sequences(testing_sequences , maxlen = max_length, truncating=tranc_type,padding = pad_type)","e684dfd2":"training_padded ","6d6694dd":"training_padded.shape","4c248c44":"testing_padded.shape","83b32003":"from sklearn.model_selection import train_test_split\nX_train,X_val,y_train,y_val = train_test_split(training_padded,training_label_final,random_state = 42)","95828946":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error as mse\n\nlr_model = LinearRegression()\nlr_model.fit(X_train,y_train)\ny_pred = lr_model.predict(X_val)\nprint(\"MSE:\", mse(y_val,y_pred))\nprint(\"RMSE:\", np.sqrt(mse(y_val,y_pred)))","3b1fe8c6":"from sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error as mse\n\ngb_model = GradientBoostingRegressor(random_state = 42)\ngb_model.fit(X_train,y_train)\ny_pred = gb_model.predict(X_val)\nprint(\"MSE:\", mse(y_val,y_pred))\nprint(\"RMSE:\", np.sqrt(mse(y_val,y_pred)))","dcfc62c3":"from sklearn.model_selection import RandomizedSearchCV\ngb_model = GradientBoostingRegressor()\n\nparam_grid = {'n_estimators': range(20,81,10), 'max_depth':range(5,16,2), 'min_samples_split':range(200,1001,200),'max_features':range(7,20,2),\n             'subsample':[0.6,0.7,0.75,0.8,0.85,0.9]}\n\n\nrandom_search = RandomizedSearchCV(gb_model,param_grid, cv = 5,scoring = 'neg_mean_squared_error',verbose=1)\nrandom_search.fit(training_padded,training_label_final)","08fc1307":"random_search.best_estimator_","74ed24b3":"model = GradientBoostingRegressor(max_depth = 13, max_features = 19,min_samples_split = 600, n_estimators = 70,subsample=0.75)\nmodel.fit(X_train,y_train)\ny_pred = model.predict(X_val)\nprint(\"MSE:\", mse(y_val,y_pred))\nprint(\"RMSE:\", np.sqrt(mse(y_val,y_pred)))","cf3a8456":"embeddings_index = {}\nGLOVE_DIR = '..\/input\/glove6b\/glove.6B.50d.txt'\nf = open(GLOVE_DIR)\nfor line in f:\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()","5d9db847":"embeddings_index['dog'].shape","d1a103d2":"def sent2vec(s):\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for word in words:\n        try:\n            M.append(embeddings_index[word])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(50)\n    return v \/ np.sqrt((v ** 2).sum())\n    ","6725cfae":"X_train,X_val,y_train,y_valid = train_test_split(train_df['excerpt_clean'],train_df['target'],test_size = 0.2,random_state = 42)","cef1cc00":"X_train_glove = [sent2vec(x) for x in X_train]\nX_val_glove = [sent2vec(x) for x in X_val]\nX_test_glove = [sent2vec(x) for x in X_test]","bb38ed00":"from sklearn.model_selection import RandomizedSearchCV\ngb_model = GradientBoostingRegressor()\n\nparam_grid = {'n_estimators': range(20,81,10), 'max_depth':range(5,16,2), 'min_samples_split':range(200,1001,200),'max_features':range(7,20,2),\n             'subsample':[0.6,0.7,0.75,0.8,0.85,0.9]}\n\n\nrandom_search = RandomizedSearchCV(gb_model,param_grid, cv = 5,scoring = 'neg_mean_squared_error',verbose=2)\nrandom_search.fit(X_train_glove,y_train)","69d696e3":"random_search.best_params_","e92bab08":"random_search.best_estimator_","76b01ba8":"model = random_search.best_estimator_\nmodel.fit(X_train_glove,y_train)\ny_pred = model.predict(X_val_glove)\nprint(\"MSE:\", mse(y_valid,y_pred))\nprint(\"RMSE:\", np.sqrt(mse(y_valid,y_pred)))","199c412f":"submission","fe5fd249":"submission['target'] = model.predict(X_test_glove)","049a6b0b":"submission","b4af2e91":"submission.to_csv('submission.csv',index=False)","25b5cdee":"Gradient Boosting With Hyperparameter Tunning","54d78b2a":"Linear Regresion Model","0748f910":"Gradient Boosting Model","7fd84507":"GLOVE EMBEDDING"}}