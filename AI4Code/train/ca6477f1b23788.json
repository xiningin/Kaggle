{"cell_type":{"9e1e444c":"code","cd211abd":"code","02cc012c":"code","fc68ed62":"code","1c8b856e":"code","8b5772f7":"code","21b02d45":"code","eea4cd32":"code","6dda505a":"code","b3066b82":"code","f10caa7e":"code","0e083e7d":"code","ae7132eb":"code","a30d9fd9":"code","ea79b9d8":"code","a2dc725e":"code","f8e03215":"code","f85f3ec3":"code","ee16ca1b":"code","624eab04":"code","6478fb4c":"code","bc2e05b9":"code","2fd8743c":"code","d241fce6":"markdown","befdc380":"markdown","c78783bd":"markdown","13e2028b":"markdown","158199fb":"markdown","8988f800":"markdown"},"source":{"9e1e444c":"\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split \nimport matplotlib.pyplot as plt \n\nimport warnings \n# Filter Warnings \nwarnings.filterwarnings('ignore')\n\nfrom PIL import Image\nimport os\n","cd211abd":"PATH_TRAIN_CAT = '\/kaggle\/input\/dogs-cats-images\/dataset\/training_set\/cats' \nPATH_TRAIN_DOG = '\/kaggle\/input\/dogs-cats-images\/dataset\/training_set\/dogs' \nPATH_TEST_CAT = '\/kaggle\/input\/dogs-cats-images\/dataset\/test_set\/cats' \nPATH_TEST_DOG = '\/kaggle\/input\/dogs-cats-images\/dataset\/test_set\/dogs' \n\nCAT = 1 \nDOG = 0\n\nIMG_WIDTH = 200 \nIMG_HEIGHT = 200 \nNR_PIX = IMG_HEIGHT * IMG_WIDTH\nNR_CHANELS = 3\nNR_FEATURES = NR_PIX * NR_CHANELS","02cc012c":"def load_data_asanarray(path):\n    data = []\n    for dirname, _, filenames in os.walk(path):\n        for filename in filenames:\n            specific_path = os.path.join(dirname, filename)\n            image = Image.open(specific_path).resize((200,200))\n            img_array = np.asarray(image)\n            data.append(img_array)\n    return np.asarray(data).reshape(len(data),IMG_WIDTH, IMG_HEIGHT, NR_CHANELS)","fc68ed62":"%%time\ndata_train_cat = load_data_asanarray(PATH_TRAIN_CAT)\ndata_train_dog = load_data_asanarray(PATH_TRAIN_DOG)\nprint(f'data_train_cat size is {len(data_train_cat)} data_train_dog size is {len(data_train_dog)}')","1c8b856e":"y_label_cat = np.ones((len(data_train_cat),1))\ny_label_dog = np.zeros((len(data_train_dog),1))","8b5772f7":"x_train = np.concatenate((data_train_cat, data_train_dog), axis=0)\ny_train = np.concatenate((y_label_cat, y_label_dog), axis=0)","21b02d45":"%%time\ndata_test_cat = load_data_asanarray(PATH_TEST_CAT)\ndata_test_dog = load_data_asanarray(PATH_TEST_DOG)\nprint(f'data_train_cat size is {len(data_train_cat)} data_train_dog size is {len(data_train_dog)}')","eea4cd32":"y_label_cat = np.ones((len(data_test_cat),1))\ny_label_dog = np.zeros((len(data_test_dog),1))\nx_test = np.concatenate((data_test_cat, data_test_dog), axis=0)\ny_test = np.concatenate((y_label_cat, y_label_dog), axis=0)\nprint(f'x_test shape is {x_test.shape} and y_shape is {y_test.shape}')","6dda505a":"X = np.concatenate((x_train, x_test), axis=0)\nY = np.concatenate((y_train, y_test), axis=0) \n\nx_train,x_test,y_train, y_test = train_test_split(X,Y, test_size=0.15, random_state=45)","b3066b82":"print(x_train.shape, y_train.shape)","f10caa7e":"print(x_test.shape, y_test.shape)","0e083e7d":"x_train = x_train.reshape(len(x_train), NR_FEATURES).T\nx_test = x_test.reshape(len(x_test), NR_FEATURES).T","ae7132eb":"y_train = y_train.T \ny_test = y_test.T ","a30d9fd9":"print(x_train.shape, y_train.shape)","ea79b9d8":"print(x_test.shape, y_test.shape)","a2dc725e":"def init_param(dimension):\n    w = np.full((dimension,1), 0.01)\n    b = 0.0 \n    \n    return w,b ","f8e03215":"def sigmoid(x):\n    sig = 1 \/ (1 + np.exp(-x))     # Define sigmoid function\n    sig = np.minimum(sig, 0.9999)  # Set upper bound\n    sig = np.maximum(sig, 0.0001)  # Set lower bound\n    return sig","f85f3ec3":"x_train, x_test = x_train \/ 255, x_test\/ 255 ","ee16ca1b":"def forward_propagation(w,b,x_train,y_train):\n    z = np.dot(w.T,x_train) + b \n    y_head = sigmoid(z)\n    \n    loss = -y_train * np.log(y_head) - (1-y_train) * np.log(1-y_head)\n    cost = (np.sum(loss)) \/ x_train.shape[1]\n    return cost ","624eab04":"def forward_and_backward_propagation(w,b,x_train, y_train):\n    z = np.dot(w.T, x_train) + b \n    y_head = sigmoid(z) \n        \n    loss = -y_train * np.log(y_head) - (1-y_train) * np.log(1-y_head)\n    cost = (np.sum(loss)) \/ x_train.shape[1]\n    \n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]  \n   \n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    \n    return cost, gradients","6478fb4c":"def train(w,b,x_train,y_train,learning_rate, nr_iteration):\n    cost_list = [] \n    cost_list2 = [] \n    index = [] \n    \n    for i in range(nr_iteration):\n        cost, gradients = forward_and_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        \n        w = w - learning_rate * gradients['derivative_weight']\n        b = b - learning_rate * gradients['derivative_bias']\n        \n        if i%10 == 0 :\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iteration %i: %f\" %(i, cost))\n            \n            \n    parameters = {'weight': w, 'bias': b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    return parameters, gradients,cost_list","bc2e05b9":"def predict(w,b,x_test):\n    z = sigmoid(np.dot(w.T, x_test)+b)\n    y_predict = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5:\n            y_predict[0,i] = 0 \n        else : \n            y_predict[0,i] = 1 \n            \n    return y_predict\n        ","2fd8743c":"def logistic_regression(x_train, y_train,x_test, y_test,learning_rate,num_of_iterarion):\n    dimension = x_train.shape[0]\n    w,b = init_param(dimension)\n    \n    parameters, gradients, cost_list = train(w,b,x_train,y_train, learning_rate,num_of_iterarion)\n    \n    y_prediction_test = predict(parameters['weight'], parameters['bias'], x_test)    \n    y_prediction_train = predict(parameters['weight'], parameters['bias'], x_train)\n    \n     # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 0.001, num_of_iterarion = 200)\n","d241fce6":"## Shuffling Data\n* Our data is seperetad from each other in their folders, but I want to shuffle them \n* so we need to concatenate all together and after this, we can use train_test_split function from sklearn ","befdc380":"## Reshape For Traning And T \n* Now, we have 3d array for our X's and have 2d array our test.\n* We need 2d array for training proccess, in this case we have already 2d array for test datasets\n* So we need to reshape our train sets","c78783bd":"# Logistic Regression \n**we should initialize our first weights and bias**","13e2028b":"# Data Pre-processing","158199fb":"# Imports","8988f800":"# Constants "}}