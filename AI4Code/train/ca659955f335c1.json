{"cell_type":{"39d450e4":"code","7a0c992c":"code","9a3b4429":"code","9b94c523":"code","b2c24aa3":"code","0ec445aa":"code","428373f9":"code","77d19c82":"code","35461092":"code","895e3f9d":"code","f8846a10":"code","4bbd3360":"code","d6e637d0":"code","b4cbe472":"code","cbd97037":"code","ee220f02":"code","495cea15":"code","07529b0a":"code","8262d9e1":"code","3fe38068":"code","a0794671":"code","c3b11148":"code","f53afd23":"code","03f247a0":"code","a949b414":"code","4e54d11a":"code","2135ccc5":"code","f5cf23e6":"markdown","45749d8b":"markdown","ff610396":"markdown","b567bd57":"markdown","8fa25334":"markdown","f860febe":"markdown"},"source":{"39d450e4":"import numpy as np\nimport pandas as pd\nimport re","7a0c992c":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')","9a3b4429":"train","9b94c523":"train = train.drop(['keyword', 'location'], axis = 1)","b2c24aa3":"train.isnull().sum()","0ec445aa":"import re #remove punctuations\nimport nltk\nnltk.download('all')\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords #and, in, the, a ... etc\nfrom nltk.stem import WordNetLemmatizer","428373f9":"corpus = []\nfor i in range(0, 7613):\n    review_train = re.sub(r\"[^a-zA-Z]\", ' ', train['text'][i]) #replace anything that is not a letter like \"\" , ... with space\n    review_train = re.sub(r'http\\S+', r'', review_train)\n    review_train = re.sub(r'#([^\\s]+)', r'\\1', review_train)  \n    review_train = re.sub('[\\s]+', ' ', review_train)\n\n    review_train = review_train.lower()  #all letters to lower-case\n    review_train = review_train.split() #splitting review in diferent words\n    \n\n    lemmatizer = WordNetLemmatizer()\n    stopwords1 = stopwords.words('english')\n    \n    review_train = [lemmatizer.lemmatize(word) for word in review_train if not word in stopwords1]\n\n    review_train = ' '.join(review_train) #separating words with space and joining\n    corpus.append(review_train)","77d19c82":"#corpus","35461092":"test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')","895e3f9d":"corpus_test = []\nfor i in range(0, 3263):\n    review = re.sub(r\"[^a-zA-Z]\", ' ', test['text'][i]) #replace anything that is not a letter like \"\" , ... with space\n    review = re.sub(r'http\\S+', r'', review)\n    review = re.sub(r'#([^\\s]+)', r'\\1', review)\n    review = re.sub('[\\s]+', ' ', review)\n\n    review = review.lower()  #all letters to lower-case\n    review = review.split() #splitting review in diferent words\n    \n    lemmatizer_2 = WordNetLemmatizer()\n    stopwords_2 = stopwords.words('english')\n    \n    review = [lemmatizer_2.lemmatize(word) for word in review if not word in stopwords_2]\n\n    review = ' '.join(review) #separating words with space and joining\n    corpus_test.append(review)","f8846a10":"#corpus_test","4bbd3360":"from sklearn.feature_extraction.text import TfidfVectorizer","d6e637d0":"vect = TfidfVectorizer(min_df=2 ,max_features = None,analyzer=\"word\",  ngram_range=(1,3))","b4cbe472":"X_vect = vect.fit_transform(corpus)","cbd97037":"y_vect = train['target']","ee220f02":"X_vect.shape","495cea15":"y_vect.shape","07529b0a":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(X_vect, y_vect, test_size=0.2, random_state=1)","8262d9e1":"from sklearn.linear_model import LogisticRegression\nmodel_1 = LogisticRegression(penalty = 'l2', C=3, max_iter = 550)","3fe38068":"model_1.fit(x_train, y_train)","a0794671":"model_1.score(x_test, y_test)","c3b11148":"X_vect_test = vect.transform(corpus_test)","f53afd23":"predict_n = model_1.predict(X_vect_test)","03f247a0":"test_id = test['id']","a949b414":"submission_2 = pd.DataFrame({'id':test_id, 'target':predict_n})","4e54d11a":"submission_2","2135ccc5":"submission_2.to_csv('.\/NLP_start_9.csv', index=False)","f5cf23e6":"# Predicting","45749d8b":"# TfidfVectorizer","ff610396":"## Cleaning Test text","b567bd57":"# Submission","8fa25334":"# Text Cleaning","f860febe":"## Cleaning Train text"}}