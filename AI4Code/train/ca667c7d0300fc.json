{"cell_type":{"d05d9292":"code","91787427":"code","72122018":"code","1d39a49c":"code","a4091f65":"code","7058aa60":"code","fad26cca":"code","aa431a86":"code","4416b0ce":"code","6ba797df":"code","caf24662":"code","4a058716":"code","fbfdf1f3":"code","5fd715af":"code","e4064f6c":"code","35432caa":"code","11d7563d":"code","e0fd1430":"code","7abf4e45":"code","4f1109db":"code","a0075d98":"code","2078e923":"code","faf8ba7f":"code","6f41374f":"code","384ab0d2":"code","757feea4":"code","842650d4":"code","dc55fbc2":"code","15b8e9fe":"code","27781bf2":"code","cd40a5c2":"code","6c13ad94":"code","626e425c":"code","080a53eb":"code","226b54d6":"code","bb841f70":"code","af4ca00b":"code","816d15d5":"code","7536227b":"code","9b390f06":"code","d3830574":"code","057e368e":"code","d23a9826":"code","00223f8a":"code","4a7ac5ef":"code","d031a359":"code","36fd7f06":"code","763598b1":"code","c425e79b":"markdown","07ad2190":"markdown","4cd63331":"markdown","73f06dfe":"markdown","fac5af43":"markdown","b8376beb":"markdown","c33ea994":"markdown","41716d16":"markdown","298f222e":"markdown","d2acbe08":"markdown","eb2ca801":"markdown","d2d0144d":"markdown","45bdbc02":"markdown","eb05673c":"markdown","70fd9611":"markdown","34b71c33":"markdown","ac7183a8":"markdown","1cebc85e":"markdown","ea99a2db":"markdown","c5be5248":"markdown","1caa611a":"markdown","d5edf8ba":"markdown","8b7480e4":"markdown","92a0a32f":"markdown","227700e8":"markdown","b461b41e":"markdown","1f008b70":"markdown","355277af":"markdown","b02cfb35":"markdown","441ed3df":"markdown","670ba824":"markdown","6290bba9":"markdown"},"source":{"d05d9292":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","91787427":"# Reading the data\ndf = pd.read_csv('..\/input\/data-science-and-stem-salaries\/Levels_Fyi_Salary_Data.csv')\ndata = df.copy()\ndata.head()","72122018":"# Variable data types - are they numerical or categorical?\ndata.dtypes","1d39a49c":"# Sorting of variables into numerical and categorical variables\ncategorical = []\nnumerical = []\n\nfor column in data.columns:\n    if data[column].dtype == 'object':\n        categorical.append(column)\n    else:\n        numerical.append(column)","a4091f65":"correlation = pd.DataFrame(data[numerical].corrwith(data['totalyearlycompensation']), columns = ['values'])\ncorrelation","7058aa60":"data.isna().sum()","fad26cca":"data['otherdetails'].value_counts()","aa431a86":"del data['otherdetails']","4416b0ce":"data[data['Race'] == 'Asian'].head(5)","6ba797df":"del data['Race']\ndel data['Education']","caf24662":"del data['timestamp']","4a058716":"# Splitting the data to get the information\ndata['area'] = data.location.apply(lambda x: x.split(',')[0])\ndata['city'] = data.location.apply(lambda x: x.split(',')[1])\ndata['country'] = data.location.apply(lambda x: x.split(',')[-1])","fbfdf1f3":"# Stripping the values of any white spaces\ndata['area'] = data['area'].apply(lambda x: x.strip())\ndata['city'] =  data['city'].apply(lambda x: x.strip())\ndata['country'] = data['country'].apply(lambda x: x.strip())","5fd715af":"data[data['city'] == 'TA'].tail(5)","e4064f6c":"del data['city']\ndel data['location']\ndel data['cityid']","35432caa":"# We define the function united_states so that we can catch cases with Area, City. e.g. Cupertino, CA \ndef united_states(value):\n    if len(value) != 2:\n        return value\n    else:\n        return 'United States'","11d7563d":"data['country'] = data.country.apply(lambda x: united_states(x))","e0fd1430":"data.country.isna().sum()","7abf4e45":"data.area.isna().sum()","4f1109db":"data.gender.unique()","a0075d98":"data['gender'] = data['gender'].fillna('Unknown')\ndata['gender'].iloc[11010] = 'Unknown'","2078e923":"data.level.nunique()","faf8ba7f":"data.level.value_counts().head(25)","6f41374f":"del data['level']","384ab0d2":"data.company.nunique()","757feea4":"def faang_classifier(company):\n    faang = ['Facebook', 'Amazon', 'Apple', 'Netflix', 'Google']\n    if company in faang:\n        return 1 \n    else:\n        return 0","842650d4":"data['is_faang'] = data['company'].apply(lambda x: faang_classifier(x))\ndata['is_faang'] = data['is_faang'].fillna(0)","dc55fbc2":"del data['company']","15b8e9fe":"del data['tag']","27781bf2":"import matplotlib.pyplot as plt\nimport seaborn as sns ","cd40a5c2":"plt.figure(figsize=(12,8))\nsns.histplot(x='dmaid', data=data)","6c13ad94":"data['dmaid'] = data.dmaid.fillna(data.dmaid.median())","626e425c":"for x in ['basesalary', 'stockgrantvalue', 'bonus']:\n    del data[x]","080a53eb":"plt.figure(figsize=(12,8))\nsns.histplot(x='totalyearlycompensation', data = data)","226b54d6":"data['totalyearlycompensation'] = np.log1p(data['totalyearlycompensation'])","bb841f70":"plt.figure(figsize=(12,8))\nsns.histplot(x='totalyearlycompensation', data = data)","af4ca00b":"del data['rowNumber']","816d15d5":"data.isna().sum()","7536227b":"categorical = []\nnumerical = []\n\nfor column in data.columns:\n    if data[column].dtype == 'object':\n        categorical.append(column)\n    else:\n        numerical.append(column)","9b390f06":"print(categorical)\nprint(numerical)","d3830574":"fig, axes = plt.subplots(1,5, sharey=True, figsize = (40, 8))\nfig.suptitle('Education')\neducation = ['Highschool','Bachelors_Degree', 'Masters_Degree', 'Doctorate_Degree', 'Some_College']\nfor i, x in enumerate(education):\n    sns.countplot(ax = axes[i], data=data, x=x)","057e368e":"fig, axes = plt.subplots(1,5, sharey=True, figsize = (40, 8))\nfig.suptitle('Race')\nrace = ['Race_Asian', 'Race_White', 'Race_Two_Or_More', 'Race_Black', 'Race_Hispanic']\nfor i, x in enumerate(race):\n    sns.countplot(ax = axes[i], data=data, x=x)","d23a9826":"sns.countplot(x = 'is_faang', data = data)","00223f8a":"x = data['yearsofexperience']\ny = data['yearsatcompany']\nfig = plt.figure(figsize=(12,8))\nplt.hist([x, y], color=['r','b'], alpha=0.5)","4a7ac5ef":"fig = plt.figure(figsize = (20,15))\nsns.heatmap(data.corr(), annot=True)","d031a359":"from sklearn.metrics import mutual_info_score\n\ndef mutual_info_salary_score(series):\n    return mutual_info_score(series, data.totalyearlycompensation)","36fd7f06":"mi = data[categorical].apply(mutual_info_salary_score)\n\nmi.sort_values(ascending=False)","763598b1":"data.to_csv('.\/cleaned_data.csv', index = False)","c425e79b":"If we look at the dataset itself, we realise that a huge proportion of 'Race' and 'Education' are missing. This is likely because the content is already reflected in the other columns. \n\nBut we can take a further look with that later! Let's start with 'Other Details' first!","07ad2190":"## Base Salary, Stock Grant Value, Bonus\n\nThese factors are basically a cheat of finding the prices, and therefore I will not involve these into our predictions. ","4cd63331":"## Levels \n\nThis was a column that I took some time to figure out, and I was trying to be ambitious by matching each company's levels to one uniform set of values. However, let's look at how many unique levels there are, as well as the different levels. ","73f06dfe":"Yikes. Looking at how variable the values can get, it is nearly impossible to consolidate everything into consistent values. Therefore, I have decided to not use this column instead. The years of experience as well as the years in the company would probably be a good representation of the level instead.","fac5af43":"## Tag\n\nThe tag does tell us very useful things like being a backend engineer, frontend engineer, etc. However, on further inspection, it would seem like there are too many tags to consider in the 'tag' column. \nThis could be replaced by the \"title\" column instead, so therefore we will be deleting the 'tag' column.","b8376beb":"## Total Yearly Compensation\nThis is our target variable. Let's take a look at the visualisation for this. ","c33ea994":"We can tell that this is not normally distributed, and therefore a good way to fill in the dmaid values is to use the median to fill up the missing values.","41716d16":"## Gender\nIf we recall in the number of missing values, here is a significant number of missing values in \"Gender\", however we can take that as 'Unknown' since they probably did not want to divulge their gender.\n\nThere is 1 datapoint that we will have manually change to 'Unknown', since its gender is... Title: Senior Software Engineer? I find that very, very unlikely. XD","298f222e":"## Timestamp\nI have decided to not covert the timestamp column to a datetime format and instead deleted the column. Since our problem is a linear regression problem that is not involved with time series, it is safe to delete the column. Moreover, the data that will be trained on the model once deployed will be future data, and therefore the year of posting should not be considered. \n\nIt could be argued that we do not consider other factors like inflation, rate of growth of salary, etc. but since we do not have data of the same job descriptions over the years, we cannot look at the trends over years to see if it has an influence over the total yearly compensation. ","d2acbe08":"## Location\nThe location can be feature engineered to give both the area and the country. \n\nThe format for the location comes in a few different forms: \n\n1. Area, City, Country OR\n2. Area, City \/ Country\n\nThis is obviously rather complicated on its own. However, there are other problems with the location itself such as the fact that the City might be abbreviated into 2 letters, and therefore there might be cities with the same abbreviations. Such an example can be seen below.","eb2ca801":"Area has the strongest relationship with the target variable, as compared to the rest of the categorical variables.","d2d0144d":"For the most part, we can see Total Yearly Compensation being affected heavily by years of experience, stockgrantvalue, dmaid as well as is_faang.","45bdbc02":"## Dmaid\n\nI honestly have no idea what this is, therefore let's use a staistical approach to see how we can fill in some missing columns here.","eb05673c":"The data has been cleaned, and therefore we are able to do some Exploratory Data Analysis (EDA) on the dataset.","70fd9611":"## Company \n\nThere are a lot of companies in this dataset - slightly over 1500 companies. When we encode this, this will definitely be a problem as it will create too many variables.\n\nInstead, I will be doing some form of encoding for the companies itself. It is a rather well-known and rather arguable statement that FAANG companies pay more than non-FAANG companies. Therefore, I am changing the company column to \"is_faang\" instead by applying a function called faang_classifier onto the column itself.","34b71c33":"## Correlation between numerical columns","ac7183a8":"## Visualisations of numerical columns\nThere are 3 visualisations that I will bunch together.\n- Education (i.e. High School, College, Bachelors Degree, Masters Degree, Doctorate Degree)\n- Race (i.e. Asian, White, Black, Hispanic, Two or More)\n- Years of Experience vs Years at Company \n\nBase Salary, Stock Grant Value, Bonus and Total Yearly Compensation is visualised as above.\n\nFor more information on enumerate(), please visit the [link attached](https:\/\/www.programiz.com\/python-programming\/methods\/built-in\/enumerate) for how to use it!","1cebc85e":"We can tell that most of the people working in STEM and\/or Data Science have a Master's Degree, followed by a Bachelor's Degree.","ea99a2db":"## Other Details ","c5be5248":"Here, we can tell that there are two clashing areas with the same location, T-ai-chung and Tel Aviv. Therefore, it would be a problem to use the city itself, therefore let us use the area and country instead. \n\nWe will also be clearing the cityid, even though it gives us the location. This is because we already have the area and the country of these places!","1caa611a":"## Saving the clean dataset\nThe next part of the project will be held on https:\/\/github.com\/kwangyy\/, as I will be deploying this model online. Thank you for reading my EDA! ","d5edf8ba":"We can tell that there is a very long tail for total yearly compensation, therefore let's use np.log1p to shorten the tail.","8b7480e4":"## Mutual Information score between categorical columns \n\nWe can figure out the relationship between categorical columns and our target variable by using mutual information.","92a0a32f":"This is a project on predicting Data Science and STEM Salaries. This is done as part of the Machine Learning Engineering course held by Alexey Grigorev. \n\nThis notebook is specifically for cleaning data as well as Exploratory Data Analysis (EDA).","227700e8":"## Race & Education\nAs mentioned, the columns Race and Education took up quite a lot of the missing values. Let us take a closer look at this, with a very specific query on Race.","b461b41e":"## rowNumber\n\nThat's just an index. It's not important.","1f008b70":"## Missing Data\n\nA very essential part of data preprocessing is how many values are missing, and deciding what to do with the missing values themselves. Let's take a look! ","355277af":"From this, we can already tell that the values are accounted for, as seen in Race_Asian and Race. Moreover, we can see that Doctorate_Degree reflects someone having a PhD. Therefore, we are able to remove the columns Race and Education, since these are already reflected in the encoded columns.","b02cfb35":"## EDA \n\nFirst, let's split the columns into numerical columns and categorical columns.","441ed3df":"Most of these 'Other Details' are actually details about their race, title and education. Therefore, this column shouldn't be really necessary for us as we have other columns like these in the dataset.","670ba824":"Great! It even looks like a normal distribution - wonderful :) ","6290bba9":"## Correlation with 'Total Yearly Compensation'\nWe can find the numerical variables that have a pretty good correlation with our target variable, \"Total Yearly Compensation\"."}}