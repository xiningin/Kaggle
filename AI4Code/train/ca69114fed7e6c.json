{"cell_type":{"bc1e004b":"code","b7756d1e":"code","c668c6f6":"code","1ba5883c":"code","d0571293":"code","60397bb5":"code","cc8d31fa":"code","0c7e1d97":"code","1bc1478f":"code","612f61d7":"code","2daffc3d":"code","669e1592":"code","5b6d25f7":"code","9f4956ba":"code","a9649f65":"code","02ee58d8":"code","8d50baa4":"code","c9254696":"code","ecdad295":"code","72b7e814":"code","7e3321ff":"code","fcfae53f":"code","4f35e831":"code","d9ddfcaa":"code","d5dfae29":"code","1539d629":"code","080b0909":"code","bd4e0c28":"code","c81c1d77":"code","97f67295":"code","e481fca2":"code","80d90262":"code","10c32522":"code","3e96418b":"markdown","ef53a6c7":"markdown","337ae8fe":"markdown","4bae3a46":"markdown","a37d5839":"markdown"},"source":{"bc1e004b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nfrom scipy.stats import skew, norm \nfrom warnings import filterwarnings as filt \nimport seaborn as sns \n\nplt.rcParams['figure.figsize'] = (12,6)\nplt.style.use('fivethirtyeight')\nfilt('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b7756d1e":"df = pd.read_csv('\/kaggle\/input\/email-spam-dataset\/completeSpamAssassin.csv')\ndf = df.drop(['Unnamed: 0'], axis = 1)\ndf.head()","c668c6f6":"sns.countplot(df.Label);","1ba5883c":"df.isnull().sum()\ndf = df.dropna().reset_index(drop = True)","d0571293":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport gensim\nimport re\nfrom string import punctuation\nfrom num2words import num2words\n\n# !pip install num2words","60397bb5":"for ind, txt in enumerate(df.Body.head()):\n    print(f\" Body {ind} | label : {df.Label.iloc[ind]} \".center(60, '='))\n    print(txt)\n    print()","cc8d31fa":"stops = stopwords.words('english')\nlemme = WordNetLemmatizer()\n\ndef is_eng(text):\n    return text if re.match(r'[a-zA-Z]+', text) else ''\n\ndef is_num(text):\n    return text if re.match(r'[0-9]+', text) else ''\n\ndef convnum2word(x):\n    xx = []\n    for txt in x:\n        if txt.isdecimal():\n            xx.append(num2words(txt))\n        else:\n            if txt.isalpha():\n                xx.append(txt)\n                \n    return xx\n\ndef clean_process(x, return_type = 's'):\n    x = x.lower()\n    x = re.sub(r'http:\/\/[a-zA-z0-9{}]+'.format(punctuation),'http', x)\n    token = nltk.word_tokenize(x)\n    lem = [lemme.lemmatize(txt) for txt in token if (is_eng(txt) or is_num(txt)) and txt not in stops]\n    lem = convnum2word(lem)\n    jlem = ' '.join(lem)\n    x = re.sub(r'[\\.]', ' ', jlem).strip()\n    x = re.sub(r'[{}]'.format(punctuation + \"''\"), '', x)\n    x = [txt for txt in x.split() if len(txt) > 1 and len(txt) <= 45]\n    \n    return ' '.join(x) if return_type == 's' else x\n\ndef clean(x, return_type = 's'):\n    if type(x) == str:\n        return clean_process(x, return_type)\n    elif (type(x) == pd.core.series.Series) or (type(x) == pd.core.frame.DataFrame):\n        return x.apply(lambda x : clean_process(x, return_type))\n    else:\n        assert type(x) == str, 'x should either str or pandas.core.series.Series'    ","0c7e1d97":"type(df)","1bc1478f":"%%time\nfor ind, txt in enumerate(df.Body.iloc[:4].apply(clean).head()):\n    print(f\" Body {ind} | label : {df.Label.iloc[ind]} \".center(60, '='))\n    print(txt)\n    print()","612f61d7":"%%time\ndf['CleanBody'] = df.Body.apply(clean)\ndf['CLeanBodyLi'] = df.CleanBody.apply(lambda x : x.split())\ndf.head()","2daffc3d":"df.shape","669e1592":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nimport plotly.express as px ","5b6d25f7":"cvec = CountVectorizer()\ncx = pd.DataFrame(cvec.fit_transform(df.CleanBody).toarray(), columns = cvec.get_feature_names())\ncx.head()","9f4956ba":"tfvec = TfidfVectorizer()\ntfx = pd.DataFrame(tfvec.fit_transform(df.CleanBody).toarray(), columns = tfvec.get_feature_names())\ntfx.head()","a9649f65":"model = gensim.models.Word2Vec(sentences= df.CLeanBodyLi, window = 5, min_count= 2)\nmodel","02ee58d8":"# dir(model.wv)\n# dir(model.wv.get_vector\n# model.wv.most_similar('adult')","8d50baa4":"def dim_red(model, model_param, x, target = None):\n    mod = model(**model_param)\n    rudx = pd.DataFrame(mod.fit_transform(x))\n    if target is not None:\n        rudx['outcome'] = target\n    return rudx","c9254696":"# tsne = TSNE(n_components= = 3, perplexity = 100, n_iter = 5000)\n# rudx = pd.DataFrame(tsne.fit_transform(tfx)\n# rudx['outcome'] = df.Label\n# pca = PCA(n_components=3)\n\nparams = {\n    'n_components' : 3, \n#     'perplexity' : 100, \n#     'n_iter' : 250\n}\nrudx = dim_red(PCA, params, tfx, target = df.Label)","ecdad295":"px.scatter_3d(data_frame = rudx, x = 0, y = 1, z = 2, color = 'outcome')","72b7e814":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBRFClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split","7e3321ff":"def best_model(x, y):\n    models = [LogisticRegression(), MultinomialNB(), RandomForestClassifier(), LGBMClassifier()]\n    names = ['logistic reg', 'multinomial nb', 'random forest', 'lgbm']\n    scores = []\n    for ind, model in enumerate(models):\n        score = cross_val_score(model, x, y, scoring = 'f1', cv = 3).mean()\n        scores.append(score)\n        print(f\"finished :===> {ind + 1} \/ {len(names)}\")\n    return pd.DataFrame(scores, index = names)\n\n\ndef report(yt, pred):\n    print()\n    print(classification_report(yt, pred))\n    print()\n    \ndef all_scores(model, xt, xtest, yt, ytest, pred):\n    print(' report '.center(70, '='))\n    print()\n    print(f'Training score :=====> : {model.score(xt, yt)}')\n    print(f'Testing score  :=====> : {model.score(xtest, ytest)}')\n    report(ytest, pred)\n    sns.heatmap(confusion_matrix(ytest, pred), fmt = '.2f', annot = True)\n    plt.xlabel('predicted value')\n    plt.ylabel('actual value');\n    \ndef get_score(model, xt, yt, xtest, ytest):\n    model.fit(xt, yt)\n    pred = model.predict(xtest)\n    all_scores(model, xt, xtest, yt, ytest, pred)","fcfae53f":"# train test split\n \ncx_train, cx_test, cy_train, cy_test = train_test_split(cx, df.Label, test_size = 0.2, stratify = df.Label, random_state = 0)\ntx_train, tx_test, ty_train, ty_test = train_test_split(tfx, df.Label, test_size = 0.2, stratify = df.Label, random_state = 1)","4f35e831":"# count vec \nbest_model(cx_train, cy_train)","d9ddfcaa":"#  tf-idf\nbest_model(tx_train, ty_train)","d5dfae29":"get_score(MultinomialNB(), cx_train, cy_train, cx_test, cy_test)","1539d629":"get_score(LGBMClassifier(), tx_train, ty_train, tx_test, ty_test)","080b0909":"from sklearn.preprocessing import FunctionTransformer\ntransformer = FunctionTransformer(clean)\ntransformer","bd4e0c28":"# dir(transformer)\ntransformer.fit_transform(df.Body).iloc[0]","c81c1d77":"\npipeline = Pipeline(steps = [\n    ('clean_text', transformer),\n    ('vect', TfidfVectorizer()),\n    ('model', LGBMClassifier())\n])\n\npipeline.fit(df.Body, df.Label)","97f67295":"df.Body[df.Label == 1].iloc[0]","e481fca2":"txt = '''\nfrom:\n    muhammed jaabir,\n    somewhere\n    \ndear someone,\n    do you know how to solve this problem?\n\n'''\n\ntxt2 = '''\n\nfrom:\n    muhammed jaabir,\n    somewhere\n    \ndear someone,\nSave up to 70% on Life Insurance.\\nWhy Spend More Than You Have To?Life Quote Savings\\nEnsuring your \\n      family's financial security is very important. Life Quote Savings makes \\n      buying life insurance simple and affordable. We Provide FREE Access to The \\n      Very Best Companies and The Lowest Rates.Life Quote Savings is FAST, EASY and \\n            SAVES you money! Let us help you get started with the best values in \\n            the country on new coverage. You can SAVE hundreds or even thousands \\n            of dollars by requesting a FREE quote from Lifequote Savings. Our \\n            service will take you less than 5 minutes to complete. Shop and \\n            compare. SAVE up to 70% on all types of Life insurance! Click Here For Your \\n            Free Quote!Protecting your family is the best investment you'll ever \\n          make!\\nIf you are in receipt of this email \\n      in error and\/or wish to be removed from our list, PLEASE CLICK HERE AND TYPE REMOVE. If you \\n      reside in any state which prohibits e-mail solicitations for insurance, \\n      please disregard this \\n      email.\\n\n\n'''","80d90262":"pipeline.predict(pd.Series(txt))","10c32522":"pipeline.predict(pd.Series(txt2))","3e96418b":"### word2vec","ef53a6c7":"let's compare count vec multinomial nb and tf-df lgbm","337ae8fe":"### Count vectorizer","4bae3a46":"### tf-idf vectorizer","a37d5839":"#### creating a custom pipline : clean text --> vectorize --> train & predict"}}