{"cell_type":{"2df56be0":"code","c75c19a1":"code","458b3c27":"code","38ed6a0c":"code","cbe3514d":"code","5ed134da":"code","25184ca7":"code","6d6b9116":"code","143c9640":"code","6633721c":"code","26f91d8b":"code","8e9311fc":"code","7043ef0a":"code","7ff0ed45":"code","2b2352a5":"code","f5581eff":"code","9b85d214":"code","5bce9b8f":"code","92c2219f":"markdown","4b63a9a0":"markdown","66c7143f":"markdown"},"source":{"2df56be0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c75c19a1":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv')\nX_test =  pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv')\ntrain.pop('id')\n\ny = train['claim']\ntrain.pop('claim')\nX_test.pop('id')\nX=train","458b3c27":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import Input, Dense, Activation,  BatchNormalization, Dropout, Concatenate, Embedding,  Flatten, Conv1D\nfrom tensorflow.keras.models import Model\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler,  QuantileTransformer,  KBinsDiscretizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom tensorflow import keras\nfrom sklearn import metrics\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\n#import tensorflow_decision_forests as tfdf","38ed6a0c":"# X_train, X_val, y_train, y_val  = train_test_split(X,y,test_size=0.05,random_state=2021,stratify=y)","cbe3514d":"# imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n# X_train = imp.fit_transform(X_train)\n# X_val = imp.transform(X_val)\n# X_test = imp.transform(X_test)","5ed134da":"# scaler = MinMaxScaler()\n# X_train = scaler.fit_transform(X_train)\n# X_val = scaler.transform(X_val)\n# X_test = scaler.transform(X_test)","25184ca7":"# qt = QuantileTransformer(n_quantiles=150, output_distribution='uniform')\n\n# X_train = qt.fit_transform(X_train)\n# X_val = qt.transform(X_val)\n# X_test = qt.transform(X_test)","6d6b9116":"# bin_cat = KBinsDiscretizer(n_bins=1200, encode='ordinal',strategy='uniform')\n# X_train = bin_cat.fit_transform(X_train)\n# X_val = bin_cat.transform(X_val)\n# X_test = bin_cat.transform(X_test)","143c9640":"def ClassModel(input_shape):\n\n    \n\n    X_input = Input(input_shape)\n    X = Embedding (input_dim=1200, output_dim=64)(X_input)\n    X = Flatten()(X)\n    X = Dropout(0.3)(X)\n    X = Dense(50, kernel_initializer=tf.keras.initializers.GlorotNormal(), activation='swish')(X)\n#     X = BatchNormalization()(X)\n    X = Dropout(0.5)(X)\n    X = Dense(1, kernel_initializer=tf.keras.initializers.GlorotNormal(),activation='sigmoid', name='output2')(X)\n    model = Model(inputs = X_input, outputs = X, name='ClassModel')\n\n    return model","6633721c":"# BATCH_SIZE=1024\n# SHUFFLE_BUFFER_SIZE = 1024\n# train_dataset = tf.data.Dataset.from_tensor_slices((np.float32(X_train),np.float32((y_train))))\n# val_dataset = tf.data.Dataset.from_tensor_slices((np.float32(X_val),np.float32((y_val))))\n# train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n# val_dataset = val_dataset.batch(BATCH_SIZE)\n# test_dataset = tf.data.Dataset.from_tensor_slices(np.float32(X_test))\n# test_dataset = test_dataset.batch(BATCH_SIZE)","26f91d8b":"# checkpoint_filepath = '\/kaggle\/working\/ckpt2'\n# model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n#     filepath=checkpoint_filepath,\n#     save_weights_only=True,\n#     monitor='val_aucroc',\n#     mode='max',\n#     save_best_only=True)","8e9311fc":"# keras.backend.clear_session()\n# classmodel  = ClassModel(X_train.shape[1:])\n\n# classmodel.compile(loss='binary_crossentropy', optimizer = keras.optimizers.Adam(learning_rate=0.0005), metrics=[tf.keras.metrics.AUC(name='aucroc')])\n        \n# classmodel.fit(train_dataset,\n#         epochs = 80,\n#         validation_data=val_dataset,\n#         callbacks=[model_checkpoint_callback])","7043ef0a":"# classmodel.load_weights(checkpoint_filepath)\n# classmodel.evaluate(val_dataset)\n# main_pred = classmodel.predict(test_dataset)\n# main_pred[:10]","7ff0ed45":"# sub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\n# sub.iloc[:,1]=main_pred\n# sub=sub.set_index('id')\n# sub.to_csv('baseline_nn.csv')","2b2352a5":"imp = SimpleImputer(missing_values=np.nan, strategy='median')\nX = imp.fit_transform(X)\nX_test = imp.transform(X_test)\nqt = QuantileTransformer(n_quantiles=150, output_distribution='uniform')\nX = qt.fit_transform(X)\nX_test = qt.transform(X_test)\nbin_cat = KBinsDiscretizer(n_bins=1200, encode='ordinal',strategy='uniform')\nX = bin_cat.fit_transform(X)\nX_test = bin_cat.transform(X_test)\n\ndef prediction (X_train, y_train, X_test):\n    \n    keras.backend.clear_session()\n\n    kfold = StratifiedKFold(n_splits = 10, random_state=2021, shuffle=True)\n\n    y_pred = np.zeros((493474,1))\n    BATCH_SIZE=1024\n    SHUFFLE_BUFFER_SIZE = 1024\n\n    for idx in kfold.split(X=X_train, y=y_train):\n        train_idx, val_idx = idx[0], idx[1]\n        xtrain = X_train[train_idx]\n        ytrain = y_train[train_idx]\n        xval = X_train[val_idx]\n        yval = y_train[val_idx]\n\n\n\n        \n        checkpoint_filepath = '\/kaggle\/working\/ckpt_cv'\n        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_filepath,\n    save_weights_only=True,\n    monitor='val_aucroc',\n    mode='max',\n    save_best_only=True)\n        \n        \n        train_dataset = tf.data.Dataset.from_tensor_slices((np.float32(xtrain), np.float32(ytrain)))\n        val_dataset = tf.data.Dataset.from_tensor_slices((np.float32(xval), np.float32(yval)))\n        train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n        val_dataset = val_dataset.batch(BATCH_SIZE)\n        \n        keras.backend.clear_session()\n        # fit model for current fold\n        classmodel = ClassModel(xtrain.shape[1:])\n        classmodel.compile(loss='binary_crossentropy', optimizer = keras.optimizers.Adam(learning_rate=0.0005), metrics=[tf.keras.metrics.AUC(name='aucroc')])        \n        classmodel.fit(train_dataset,\n        epochs = 10,\n        validation_data=val_dataset,\n        callbacks=[model_checkpoint_callback])\n        keras.backend.clear_session()\n        classmodel.load_weights(checkpoint_filepath)\n        #create predictions\n        y_pred += classmodel.predict(X_test)\/kfold.n_splits\n    \n    return y_pred,  classmodel","f5581eff":"BATCH_SIZE=1024\ntest_dataset = tf.data.Dataset.from_tensor_slices(np.float32(X_test))\ntest_dataset = test_dataset.batch(BATCH_SIZE)\nmain_pred, classmodel = prediction(X, y, test_dataset)","9b85d214":"main_pred[:10]","5bce9b8f":"sub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')\nsub.iloc[:,1]=main_pred\nsub=sub.set_index('id')\nsub.to_csv('baseline_nn_cv1.csv')","92c2219f":"**1 - fold section**","4b63a9a0":"**10 - fold section**","66c7143f":"**found this amazing notebook suggetion tried to simplify a little , getting a good auc score , i know the model is little slow \nplease feel free to share your valuable feedback how to train it more efficiently in the comments **thankyou.!****"}}