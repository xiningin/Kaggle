{"cell_type":{"7f5299cc":"code","85325d27":"code","16132543":"code","2eddc78d":"code","e8d69731":"code","774c409a":"code","1af9bcd2":"code","294703bb":"code","c9307c92":"code","a0447a4d":"code","9ce6f8a0":"code","8591b51b":"code","387658ca":"code","d6f0c66a":"code","d112c55a":"code","336b10ab":"code","f1ba9ec9":"code","eae64def":"code","e8331baf":"code","2701feef":"code","b429a631":"code","7886e05a":"code","79bc9bec":"code","b47f9502":"code","a7e5404d":"code","c5412408":"code","8cd4b91a":"code","d0028e39":"code","c811d8d8":"code","ca207a03":"code","fce7c58c":"code","bf72dd14":"markdown","6fcb794a":"markdown","0625ac33":"markdown","52840b67":"markdown","d535ece7":"markdown","4817542b":"markdown","af7f859c":"markdown","37b299bc":"markdown","68a267a5":"markdown","1c35fb1f":"markdown","f388366e":"markdown","773dd090":"markdown","d50575e0":"markdown","f184ca7b":"markdown","0f408d03":"markdown","a2bfd57a":"markdown","42811f9d":"markdown","efde02fc":"markdown"},"source":{"7f5299cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","85325d27":"pip install bs4","16132543":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom gensim.models import Word2Vec\nfrom scipy import sparse","2eddc78d":"df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ndf.info()","e8d69731":"df.drop([\"url_legal\", \"license\", \"standard_error\", \"id\"], axis=1, inplace=True)\nprint(df.shape)","774c409a":"df.drop_duplicates(subset={\"excerpt\"}, keep='first', inplace=True)\nexc = df[\"excerpt\"]\nprint(df.shape)","1af9bcd2":"# Printing some random excerpts\nprint(exc[0])\nprint(\"=\"*50)\nprint(exc[5])","294703bb":"import re\n\ndef decontracted(phrase):\n    # Specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # General\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase","c9307c92":"# https:\/\/gist.github.com\/sebleier\/554280\n# We are removing the words from the stop words list: 'no', 'nor', 'not'\n\nstopwords = set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])","a0447a4d":"# Combining all the above steps\nfrom tqdm import tqdm\nfrom bs4 import BeautifulSoup\npreprocessed_excerpts = []\n\n# tqdm is for printing the status bar\nfor sen in tqdm(exc):\n    sen = re.sub(r\"http\\S+\", \"\", sen)\n    sen = BeautifulSoup(sen, 'lxml').get_text()\n    sen = decontracted(sen)\n    sen = re.sub(\"\\S*\\d\\S*\", \"\", sen).strip()\n    sen = re.sub('[^A-Za-z]+', ' ', sen)\n    sen = ' '.join(e.lower() for e in sen.split() if e.lower() not in stopwords)\n    preprocessed_excerpts.append(sen.strip())","9ce6f8a0":"df[\"excerpt\"] = preprocessed_excerpts\nX = df.drop([\"target\"], axis=1, inplace=False)\ny = df[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","8591b51b":"# count_vect = CountVectorizer() \n# exc_train = X_train[\"excerpt\"]\n# exc_train = count_vect.fit_transform(exc_train)\n# print(type(exc_train), exc_train.shape)\n\n# exc_test = X_test[\"excerpt\"]\n# exc_test = count_vect.transform(exc_test)\n# print(type(exc_test), exc_test.shape)","387658ca":"count_vect = CountVectorizer(binary=True) \nexc_train = X_train[\"excerpt\"]\nexc_train = count_vect.fit_transform(exc_train)\nprint(type(exc_train), exc_train.shape)\n\nexc_test = X_test[\"excerpt\"]\nexc_test = count_vect.transform(exc_test)\nprint(type(exc_test), exc_test.shape)","d6f0c66a":"# tf_vect = TfidfVectorizer() \n# exc_train = X_train[\"excerpt\"]\n# exc_train = tf_vect.fit_transform(exc_train)\n# print(type(exc_train), exc_train.shape)\n\n# exc_train = exc_train.todense()\n# print(type(exc_train), exc_train.shape)\n\n# exc_test = X_test[\"excerpt\"]\n# exc_test = tf_vect.transform(exc_test)\n# print(type(exc_test), exc_test.shape)\n\n# exc_test = exc_test.todense()\n# print(type(exc_test), exc_test.shape)","d112c55a":"# list_of_exc_train = []\n# exc_train = X_train[\"excerpt\"]\n# for exc in exc_train:\n#     list_of_exc_train.append(exc.split())\n    \n# list_of_exc_test = []\n# exc_test = X_test[\"excerpt\"]\n# for exc in exc_test:\n#     list_of_exc_test.append(exc.split())\n    \n# # Training W2V model\n# w2v_model = Word2Vec(list_of_exc_train, min_count=5, vector_size=300, workers=4, epochs=50)\n# w2v_words = list(w2v_model.wv.key_to_index)","336b10ab":"# # Converting exc_train from text to vectors\n# sent_vectors = []\n# for sent in tqdm(list_of_exc_train):\n#     sent_vec = np.zeros(300) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\n#     cnt_words =0; # num of words with a valid vector in the sentence\/review\n#     for word in sent: # for each word in a review\/sentence\n#         if word in w2v_words:\n#             vec = w2v_model.wv[word]\n#             sent_vec += vec\n#             cnt_words += 1\n#     if cnt_words != 0:\n#         sent_vec \/= cnt_words\n#     sent_vectors.append(sent_vec)\n\n# exc_train = sparse.csr_matrix(sent_vectors).toarray()\n# print(type(exc_train), exc_train.shape)","f1ba9ec9":"# # Converting exc_test from text to vectors\n# sent_vectors = []\n# for sent in tqdm(list_of_exc_test):\n#     sent_vec = np.zeros(300) # as word vectors are of zero length 50, you might need to change this to 300 if you use google's w2v\n#     cnt_words =0; # num of words with a valid vector in the sentence\/review\n#     for word in sent: # for each word in a review\/sentence\n#         if word in w2v_words:\n#             vec = w2v_model.wv[word]\n#             sent_vec += vec\n#             cnt_words += 1\n#     if cnt_words != 0:\n#         sent_vec \/= cnt_words\n#     sent_vectors.append(sent_vec)\n\n# exc_test = sparse.csr_matrix(sent_vectors).toarray()\n# print(type(exc_test), exc_test.shape)","eae64def":"# model = TfidfVectorizer()\n# tf_idf_train_matrix = model.fit_transform(exc_train)\n# tf_idf_test_matrix = model.transform(exc_test)\n\n# # We are converting a dictionary with word as a key, and the idf as a value\n# dictionary = dict(zip(model.get_feature_names(), list(model.idf_)))\n# tfidf_feat = model.get_feature_names()","e8331baf":"# tfidf_sent_vectors = []; # the tfidf-w2v for each sentence\/review is stored in this list\n# row=0;\n# for sent in tqdm(list_of_exc_train): # for each review\/sentence \n#     sent_vec = np.zeros(300) # as word vectors are of zero length\n#     weight_sum =0; # num of words with a valid vector in the sentence\/review\n#     for word in sent: # for each word in a review\/sentence\n#         if word in w2v_words and word in tfidf_feat:\n#             vec = w2v_model.wv[word]\n#             tf_idf = tf_idf_train_matrix[row, tfidf_feat.index(word)]\n#             # To reduce the computation, we can use the following\n#             # dictionary[word] = idf value of word in whole courpus\n#             # sent.count(word) = tf valeus of word in this review\n#             # tf_idf = dictionary[word]*(sent.count(word)\/len(sent))\n#             sent_vec += (vec * tf_idf)\n#             weight_sum += tf_idf\n#     if weight_sum != 0: sent_vec \/= weight_sum\n#     tfidf_sent_vectors.append(sent_vec)\n#     row += 1\n\n# exc_train = sparse.csr_matrix(tfidf_sent_vectors).toarray()\n# print(type(exc_train), exc_train.shape)","2701feef":"# tfidf_sent_vectors = []; # the tfidf-w2v for each sentence\/review is stored in this list\n# row=0;\n# for sent in tqdm(list_of_exc_test): # for each review\/sentence \n#     sent_vec = np.zeros(300) # as word vectors are of zero length\n#     weight_sum =0; # num of words with a valid vector in the sentence\/review\n#     for word in sent: # for each word in a review\/sentence\n#         if word in w2v_words and word in tfidf_feat:\n#             vec = w2v_model.wv[word]\n#             tf_idf = tf_idf_test_matrix[row, tfidf_feat.index(word)]\n#             # To reduce the computation, we can use the following\n#             # dictionary[word] = idf value of word in whole courpus\n#             # sent.count(word) = tf valeus of word in this review\n#             # tf_idf = dictionary[word]*(sent.count(word)\/len(sent))\n#             sent_vec += (vec * tf_idf)\n#             weight_sum += tf_idf\n#     if weight_sum != 0: sent_vec \/= weight_sum\n#     tfidf_sent_vectors.append(sent_vec)\n#     row += 1\n\n# exc_test = sparse.csr_matrix(tfidf_sent_vectors).toarray()\n# print(type(exc_test), exc_test.shape)","b429a631":"# # Binary Bag of Words = 0.72\n# lr = LinearRegression(normalize=True)\n# lr.fit(exc_train, y_train)\n# y_pred = lr.predict(exc_test)\n# error = np.sqrt(mean_squared_error(y_test, y_pred))\n# print(error)","7886e05a":"# # Binary Bag of Words = 1.462\n# exc_train_dense = exc_train.todense()\n# exc_test_dense = exc_test.todense()\n# lr = LinearRegression(normalize=True, fit_intercept=False, positive=True)\n# lr.fit(exc_train_dense, y_train)\n# y_pred = lr.predict(exc_test_dense)\n# error = np.sqrt(mean_squared_error(y_test, y_pred))\n# print(error)","79bc9bec":"# # Binary Bag of Words = 0.94\n# abr = AdaBoostRegressor(n_estimators=100, learning_rate=0.025, loss='square')\n# abr.fit(exc_train, y_train)\n# y_pred = abr.predict(exc_test)\n# error = np.sqrt(mean_squared_error(y_test, y_pred))\n# print(error)","b47f9502":"# # Binary Bag of Words = 0.873\n# from sklearn.ensemble import BaggingRegressor\n# br = BaggingRegressor()\n# br.fit(exc_train, y_train)\n# y_pred = br.predict(exc_test)\n# error = np.sqrt(mean_squared_error(y_test, y_pred))\n# print(error)","a7e5404d":"# # Binary Bag of Words = 1.1583\n# from sklearn.ensemble import ExtraTreesRegressor\n# etr = ExtraTreesRegressor()\n# etr.fit(exc_train, y_train)\n# y_pred = etr.predict(exc_test)\n# error = np.sqrt(mean_squared_error(y_test, y_pred))\n# print(error)","c5412408":"# # Binary Bag of Words = 0.806\n# from sklearn.ensemble import GradientBoostingRegressor\n# gbr = GradientBoostingRegressor()\n# gbr.fit(exc_train, y_train)\n# y_pred = gbr.predict(exc_test)\n# error = np.sqrt(mean_squared_error(y_test, y_pred))\n# print(error)","8cd4b91a":"# # Binary Bag of Words = 0.813\n# from sklearn.ensemble import RandomForestRegressor\n# rfr = RandomForestRegressor()\n# rfr.fit(exc_train, y_train)\n# y_pred = rfr.predict(exc_test)\n# error = np.sqrt(mean_squared_error(y_test, y_pred))\n# print(error)","d0028e39":"# # Binary Bag of Words = 0.785\n# from sklearn.experimental import enable_hist_gradient_boosting\n# from sklearn.ensemble import HistGradientBoostingRegressor\n# exc_train_dense = exc_train.todense()\n# exc_test_dense = exc_test.todense()\n# hgbr = HistGradientBoostingRegressor()\n# hgbr.fit(exc_train_dense, y_train)\n# y_pred = hgbr.predict(exc_test_dense)\n# error = np.sqrt(mean_squared_error(y_test, y_pred))\n# print(error)","c811d8d8":"# parameters = {\n#     'n_estimators': [25, 50, 75, 100],\n#     'learning_rate': [0.001, 0.01, 0.1, 1, 5],\n#     'loss': ['linear', 'square', 'exponential']\n# }\n# abr = AdaBoostRegressor()\n# sco = make_scorer(mean_squared_error)\n# reg = RandomizedSearchCV(abr, parameters, scoring = sco)\n# reg.fit(exc_train, y_train)\n# print(reg.best_estimator_)","ca207a03":"# pip install lazypredict","fce7c58c":"# from lazypredict.Supervised import LazyRegressor\n\n# reg = LazyRegressor(verbose=0, ignore_warnings=False, custom_metric=None)\n# models, predictions = reg.fit(exc_train, exc_test, y_train, y_test)\n# print(models)","bf72dd14":"# Applying Lazy Predict","6fcb794a":"# Pre-Processing","0625ac33":"# Applying Bagging Regressor","52840b67":"# Average W2V","d535ece7":"# TF-IDF Weighted W2V","4817542b":"# Featurization (TF-IDF)","af7f859c":"# Splitting the Dataset","37b299bc":"# Applying Linear Regression","68a267a5":"# Featurization (Bag of Words)","1c35fb1f":"# Applying RandomizedSearchCV","f388366e":"# Applying Extra Trees Regressor","773dd090":"# Applying AdaBoost Regressor","d50575e0":"# Applying Histogram Gradient Boosting Regressor","f184ca7b":"# Featurization (Word2Vec)","0f408d03":"# Applying Random Forest Regressor","a2bfd57a":"# Featurization (Binary Bag of Words)","42811f9d":"# Applying Gradient Boosting Regressor","efde02fc":"# CommonLit Readability Prize\n- This notebook covers some of the most basic ML models and pre-processing technqiues that a beginner can approach easily\n- Some of the basic pre-processing includes removing stop-words, converting all the text to lower-case, removing links and converting short representations like won't, couldn't, etc.\n- For converting text to vectors, the notebook includes Bag of words, Binary Bag of words, TF-IDF, Average Word2Vec and TF-IDF weighted Word2Vec.\n- In terms of the ML models, the notebook covers Linear Regression, AdaBoost (with RandomizedSearchCV), Bagging Regressor, Extra Trees Regressor, Gradient Boosting Regressor, Random Forest, Histogram Gradient Boosting Regressor.\n- Also, the notebook uses LazyPredict just to see the performance of different regression models."}}