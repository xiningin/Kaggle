{"cell_type":{"7de13470":"code","6f71276f":"code","7717a2e4":"code","477a8ab6":"code","7163d452":"code","bae75825":"code","4de48ef1":"code","9240ff82":"code","f1c1fd26":"code","f30aac3f":"code","435e8cdf":"code","d202b0bf":"code","5ad063df":"code","c38178e2":"code","341629b4":"code","9ef238d6":"code","dd6a6378":"markdown","8355d005":"markdown","33612465":"markdown","d2fdc185":"markdown","1d603d69":"markdown","49bbbe32":"markdown","0936c5fc":"markdown","525212d2":"markdown","484f82de":"markdown","18662de6":"markdown","9b4b72fb":"markdown","5fb36cf0":"markdown"},"source":{"7de13470":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom urllib.parse import unquote\nimport numpy as np\nfrom nltk.tokenize import TweetTokenizer\nimport string\nimport html\nimport re\nimport unicodedata\nimport unidecode\nimport math\nimport itertools\nfrom functools import partial\nimport warnings\n\nfrom nltk.corpus import stopwords\nimport nltk\nfrom nltk.stem.snowball import SnowballStemmer\n","6f71276f":"class MultiWordSplitter:    \n    \"\"\"\n       Segments a string into individual tokens where each token is a word.\n       For example 'londonRiots' will be segmented into ['london', 'riots']\n    \n       This is specifically designed to work on hash tags and mentions in a twitter feed.\n       The idea is to segment first on puncuation (if there is any), and then left to right on \n       words that exist in a dictionary.\n       \n       They are usually multiple splits for a term.\n       For example #halloffame might be split into ['hall', 'of', 'fame'] or \n       ['hallo', 'ffa', 'me'].\n       \n       The possibilites are scored using a heuristic and the highest score is returned.\n       \n       Computed segments for a multi-word are cached to improve performance of subsequent calls\n       for the same term.\n    \"\"\"\n    \n    def __init__(self, word_file='\/kaggle\/input\/linux-dictionary\/words'):\n        \"\"\"\n        Initializes the dictionary.\n        \n        Linux comes with an extensive dictionary in \/usr\/share\/dict\/words. \n        This dictionary includes single leters.\n        \"\"\"\n        self.term_cache = {}\n        with open(word_file, 'r') as f:\n            self.known_words = set(t.lower()  for l in f for t in re.split(r'[\\W_]+', l.strip()))                        \n    \n    @staticmethod\n    def default_scorer(list_of_parsed_terms, set_of_uppercase_dividers, raw_term):\n        \"\"\"\n        This heuristic for scoring candidate segment incorporates:\n        - number of tokens\n        - length of first and last token\n        - number of tokens where the prior token ends in a lower case letter \n          and the token starts with an upper case letter\n         \n        Parameters:\n         list_of_parsed_terms - (start, end) tuples of indexes for each token\n         set_of_uppercase_dividers - indexes of uppercase characters where the \n                                    previous character is lowercase.\n         raw_term - the term (original case) that the segmented tokens are being scored against\n        \"\"\"\n        l = list_of_parsed_terms\n        uppers = set_of_uppercase_dividers\n        \n        score = (\n            ((l[0][1] - l[0][0]) # length of first word\n            + (l[-1][1] - l[-1][0]) # length of last word\n            # points for upper-case divisions between words\n            + sum([1.25 for s in l if s[0] in uppers and s[0]-1 not in uppers])\n            ) \/ len(l)\n        )\n\n        return score\n    \n    @staticmethod\n    def make_default_scorer(raw_term):\n        \"\"\"\n        Makes the default scorer for a term (in original case) that is to be segmented \n        into words.\n        \"\"\"\n        # indexes of upper-case letters followed by lower-case letter\n        uppers = set(i for i,s in enumerate(raw_term) if s.isupper())\n        \n        return partial(MultiWordSplitter.default_scorer, set_of_uppercase_dividers=uppers, raw_term=raw_term)\n    \n    \n    def split(self, raw_term, make_scorer=None):\n        \"\"\"\n        Splits a term into segmented tokens.\n        \n        This first splits on any non-alphanumeric characters, and then uses the dictionary to\n        find and split words embeded within each token.\n        \"\"\"\n        if raw_term in self.term_cache:\n            return self.term_cache[raw_term]\n        else:\n            splits = re.split(\"[^A-Za-z0-9]\", raw_term)\n            tokens = [t for split_term in splits \n                        for t in self.splitTermLeftToRight(split_term, make_scorer)]            \n            # If found nothing, use the original term unsplit\n            tokens = [raw_term] if not tokens else tokens\n            self.term_cache[raw_term] = tokens\n            return tokens\n    \n    def splitTermLeftToRight(self, raw_term, make_scorer=None):\n        \"\"\"\n        From left to right, find candidate splits for a term.\n        \n        Return the highest scoring candidate, or empty list if no splits could be found.\n        \"\"\"\n        score = (MultiWordSplitter.make_default_scorer(raw_term) if not make_scorer \n                 else make_scorer(raw_term))\n        term = raw_term.lower()                        \n                    \n        # list of candidates\n        # each candidate is a list of (start, end) position tuples that define the words found\n        # in raw term.\n        candidates = [] \n\n        for end_pos in range(1, len(term)+1, 1):\n            new_candidates = []            \n            \n            # If begining to current position is a word, track\n            # begin to current end as a candidate split\n            if (term[0:end_pos] in self.known_words \n                and [(0, end_pos)] not in candidates):\n                new_candidates.append([(0, end_pos)])                \n            \n            for candidate in candidates:                \n                is_new = lambda cand: not(cand in candidates or cand in new_candidates)\n                \n                # If a previously seen candidate's last word can be \n                # extended with the new end position, then \n                # create a new candidate with\n                # the last word extended to the end position.\n                # For example:\n                # Term: #riotingCity\n                # Candidate: ['riot']\n                # Endpos: end of string\n                # New Candidate: ['rioting']\n                if (term[candidate[-1][0]:end_pos] in self.known_words\n                    or\n                    term[candidate[-1][0]:end_pos].isnumeric()\n                   ):\n                    # extension\n                    new_candidate = candidate.copy()\n                    new_candidate[-1] = (candidate[-1][0], end_pos)\n                    if is_new(new_candidate):\n                        new_candidates.append(new_candidate)\n                \n                # If the last word of the previous candidate cannot be extended to create a new\n                # word, is there a word between the end of the prior word and the end position.\n                # If so, create a candidate with the two words.\n                # For example:\n                # Term: #rioitingCity\n                # Candidate: ['rioting']\n                # Endpos: end of string\n                # New Candidate: ['rioting', 'city']\n                elif (term[candidate[-1][1]:end_pos] in self.known_words\n                      or\n                      term[candidate[-1][1]:end_pos].isnumeric()\n                     ):\n                    # following word\n                    new_candidate = candidate.copy()\n                    new_candidate.append((candidate[-1][1], end_pos))\n                    if is_new(new_candidate):\n                        new_candidates.append(new_candidate)                    \n\n            # Update the candidates with all the new possible splits that were found\n            candidates.extend(new_candidates)\n        \n        # remove any candidates that are not at the end of the string \n        # these are the ones still \"open\" for more matches and not valid splits\n        candidates = [candidate for candidate in candidates if candidate[-1][1] == len(term)]        \n        \n        # score results and return only top scored candidate                \n        scored_results = [([term[s:e] for s,e in candidate], score(candidate)) \n                          for candidate in candidates]\n        \n        if scored_results:\n            best_score = max([r[1] for r in scored_results])\n            best_results = [term[0] for term in scored_results if term[1] == best_score]\n            # If a determination couldn't be made, pick the first (basically a coin toss)\n            return best_results[0]\n        else:\n            return []\n    ","7717a2e4":"splitter = MultiWordSplitter()\nprint(splitter.split('Dreamer_22'))\nprint(splitter.split('HallOfFame'))\nprint(splitter.split('nicklawrence'))\nprint(splitter.split('londonriots'))\nprint(splitter.split('goal'))\nprint(splitter.split('WeAreTheChampions'))\nprint(splitter.split('endOfTheWorld'))\nprint(splitter.split('newyorktimes'))\nprint(splitter.split('worldwar3'))\nprint(splitter.split('greatbritishbakeoff'))\nprint(splitter.split('greatbritishbakeoff')) # test cache\nprint(splitter.split('greatBritishBakingShow'))\nprint(splitter.split('londonearthquake'))\nprint(splitter.split('greatestshowonearth'))\nprint(splitter.split('londonhasfallenmovie'))\nprint(splitter.split('IBMwatsonHealth'))\nprint(splitter.split('onFire'))\nprint(splitter.split('ldnriots'))\nprint(splitter.split('XMLTable'))\nprint(splitter.split('@_OnlyFTF'))","477a8ab6":"_ = nltk.download('stopwords') and nltk.download('wordnet')","7163d452":"BAD_HTML_SPACE_PATTERN = re.compile(r\"%20\")\n# Seen a few examples where the amount is something like $100M, that's why the M \n# is in the expression\nMONEY_PATTERN = re.compile(r\"\\b[$]+[0-9,.]+[mM]?\\b\")\n\ndef pre_process(text):\n    \"\"\"\n    Normalizes text and cleans up bad HTML space characters.\n    \"\"\"\n    if pd.isnull(text):\n        return text\n    \n    t = text    \n    t = html.unescape(unidecode.unidecode(unicodedata.normalize('NFKD', t)))\n    t = re.sub(BAD_HTML_SPACE_PATTERN, ' ', t)\n    t = re.sub(MONEY_PATTERN, ' dollars ', t)\n    return t\n\n\n\nHTTP_PATTERN = re.compile(r\"[Hh][Tt][Tt][Pp].*\")\nTOKEN_SPLIT_PATTERN = re.compile(\"[^A-Za-z0-9]\")\n\ndef tokenize(tweet, tweet_tknzr, word_splitter, stemmer, stop_words):\n    \"\"\"\n    Tokenizes a tweet\n    \n    tweet_tknzr - the tokenizer\n    word_splitter - instance of MultiWordSpliter\n    stemmer - stemmer to use, or None if stemming is not required\n    stop_words - set of stop words to remove\n    \"\"\"\n    if pd.isnull(tweet):\n        return []\n    \n    try:\n        tokens = tweet_tknzr.tokenize(tweet)\n        tokens = [t for t in tokens if not re.match(HTTP_PATTERN, t)]\n        tokens = [word_splitter.split(t[1:]) if t[0] in '#@'\n                  else re.split(TOKEN_SPLIT_PATTERN, t)               \n                  for t in tokens if not pd.isnull(t) \n                  and not all(c in string.punctuation for c in t) \n                  and len(t) >= 1]\n        tokens = [pt.lower() for t in tokens for pt in t]\n\n        tokens = [stemmer.stem(t) if stemmer else t for t in tokens \n                  if (not stop_words or t not in stop_words) and not t.isnumeric()\n                  and not t in string.punctuation]\n    except:\n        print(tokens)\n        raise\n    \n    \n    return tokens\n\ndef make_tokenizer_without_stemmer():\n    \"\"\"\n    Returns a tokenizer with no stemming\n    \"\"\"\n    return partial(tokenize,\n                   tweet_tknzr=TweetTokenizer(strip_handles=True, reduce_len=True),\n                   word_splitter=MultiWordSplitter(),\n                   stemmer=None,\n                   stop_words=set(stopwords.words('english'))\n                  )\n\ndef make_tokenizer():    \n    \"\"\"\n    Returns a tokenizer with stemming and stop words.\n    \"\"\"\n    \n    return partial(tokenize,\n                   tweet_tknzr=TweetTokenizer(strip_handles=True, reduce_len=True),\n                   word_splitter=MultiWordSplitter(),\n                   stemmer=SnowballStemmer(language='english'),\n                   stop_words=set(stopwords.words('english'))\n                  )","bae75825":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\nfrom sklearn.base import TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import FeatureUnion\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.neural_network import MLPClassifier\nimport gensim\nimport gensim.downloader\n\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import plot_precision_recall_curve\nimport matplotlib.pyplot as plt","4de48ef1":"class DenseTransformer(TransformerMixin):\n    \"\"\"\n    Converts a sparse matrix to a desne one for operations\n    that only work on a dense matrix.\n    \"\"\"\n\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def get_params(self, deep=False):\n        return {}\n    \n    def transform(self, X, y=None, **fit_params):\n        return X.todense()\n\n    \nclass SelectSingleColumn(TransformerMixin):\n    \"\"\"\n    Transforms a dataframe into a series for pipeline operations\n    \"\"\"\n    def __init__(self, column=None):\n        self.column = column\n    \n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def set_params(self, column=None):\n        if column:\n            self.column = column\n        \n    def get_params(self, deep=False):\n        return {'column' : self.column}\n    \n    def transform(self, X, y=None, **fit_params):\n        return X.loc[:, self.column].fillna(' ')\n    \nclass Counter(TransformerMixin):\n    \"\"\"\n    Counts the number of words in a tweet, spliting on spaces.\n    (This is actually words, not tokenized tokens)\n    \"\"\"\n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def get_params(self, deep=False):\n        return {}\n    \n    def transform(self, X, y=None, **fit_params):        \n        return pd.DataFrame({'length' : X['text'].fillna(' ').agg(lambda s: len(s.split(' '))),                             \n                            },\n                            index=X.index)\n\nclass ColToWordVec(TransformerMixin):\n    \"\"\"\n    Transforms a text column into a word vector that represents the \n    words in the column.\n    \"\"\"\n    vectors = gensim.downloader.load('glove-twitter-25')\n    \n    def __init__(self, column=None):\n        self.column = column       \n        self.tokenizer = make_tokenizer_without_stemmer()\n        \n    def fit(self, X, y=None, **fit_params):\n        return self\n\n    def set_params(self, column=None):\n        if column:\n            self.column = column\n        \n    def get_params(self, deep=False):\n        return {'column' : self.column}\n    \n    def vectorize(self, text):\n        vecs = [ColToWordVec.vectors[t] \n                for t in self.tokenizer(pre_process(text))\n                if t in ColToWordVec.vectors]\n        return list(np.add.reduce(vecs)) if vecs else [np.NaN] * 25\n    \n    \n    def transform(self, X, y=None, **fit_params):\n        df = pd.DataFrame([self.vectorize(text) for text in X.loc[:, self.column]],\n                          columns=['kwd_' + str(i) for i in range(25)],\n                          index=X.index)\n        return df \n    \n","9240ff82":"pipeline = Pipeline([\n    ('union', FeatureUnion(transformer_list=[\n        ('tfidf', Pipeline([\n            ('col', SelectSingleColumn()),\n            ('vect',  TfidfVectorizer()),\n            ('dense', DenseTransformer()),\n            ('scale', StandardScaler()),\n            ('pca',   PCA())]\n        )),\n        \n        ('tweet_vec', Pipeline([\n            ('w2v', ColToWordVec()),\n            ('impute', SimpleImputer())\n        ])),\n        \n        ('keyword', Pipeline([\n            ('w2v',  ColToWordVec()),\n            ('impute', SimpleImputer())\n            ]\n        )),\n        \n        ('countFeatures', Pipeline([\n            ('count', Counter()),\n        ]))\n    ])),\n   \n    ('clf',   MLPClassifier()),\n])\n\n\n","f1c1fd26":"# The paramters included here are the best ones found from prior Grid Search results\nparameters = {\n    'union__tfidf__col__column' : ['text'],\n    'union__tfidf__vect__input' : ['content'],\n    'union__tfidf__vect__strip_accents' : ['unicode'],\n    'union__tfidf__vect__analyzer' : ['word'],\n    'union__tfidf__vect__preprocessor' : [pre_process],\n    'union__tfidf__vect__tokenizer' : [make_tokenizer()],\n    'union__tfidf__vect__ngram_range' : [(1,3)],\n    'union__tfidf__vect__max_df' : [.75],\n    'union__tfidf__vect__min_df' : [15],\n    'union__tfidf__vect__max_features' : [None],    \n    'union__tfidf__pca__n_components' : [.7],\n    \n    'union__tweet_vec__w2v__column' : ['text'],\n    'union__keyword__w2v__column' : ['keyword'],\n    \n    \"clf__solver\" : ['adam'],\n    \"clf__hidden_layer_sizes\" :[50],\n    \"clf__activation\" : ['logistic'],\n    \"clf__random_state\" : [0], \n    \"clf__max_iter\" : [1000],\n    \"clf__early_stopping\" : [True],\n}","f30aac3f":"data = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\", \n                   dtype={'id' : np.int64,\n                          'keyword' : str, \n                          'location' : str, \n                          'text' :str, \n                          'target' : np.int64\n                         }\n                  )\n\ndata.set_index('id', inplace=True)\ntrain_df, test_df = train_test_split(data, test_size=.2, random_state=11, shuffle=True)","435e8cdf":"grid_search = GridSearchCV(pipeline, parameters, n_jobs=1, cv=5)\nmodel = grid_search.fit(train_df, train_df['target'])","d202b0bf":"train_predictions = model.predict(train_df)\nprint(classification_report(train_df['target'], train_predictions))","5ad063df":"plot_confusion_matrix(model, train_df, train_df['target'])","c38178e2":"test_predictions = model.predict(test_df)\nprint(classification_report(test_df['target'], test_predictions))\nplot_confusion_matrix(model, test_df, test_df['target'])","341629b4":"disp = plot_precision_recall_curve(model, train_df, train_df['target'])\ndisp2 = plot_precision_recall_curve(model, test_df, test_df['target'])\n","9ef238d6":"submission_test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\", \n                   dtype={'id' : np.int64,\n                          'keyword' : str, \n                          'location' : str, \n                          'text' :str, \n                          'target' : np.int64\n                         }\n                  ).set_index('id')\n\n\nsubmission_predictions = model.predict(submission_test)\n\nsubmission = pd.DataFrame({'id': submission_test.index, 'target' : submission_predictions})\nsubmission.to_csv('\/kaggle\/working\/submission.csv', index=False)","dd6a6378":"# Submit predictions for kaggle test.csv","8355d005":"## Imports for machine learning","33612465":"# Hashtags\nTwitter hashtags are often composed of multiple words. The individual words are very important to NLP solutions, but determining what the individual words are is difficult.\n\nOther high scoring Kaggle solutions used regular expressions to split hash tags. In many cases those expressions were derived by looking at the competition's test data as well as the training data. That approach does well for the competition, but has serious problems because the test score no longer reflects how well the solution might perform on unseen data. So I implemented a solution that is more general.\n\nMany sources suggest that using a dictionary is the best way to extract keywords from a hashtag. I spent a fair amount of time looking at this paper: https:\/\/www.aclweb.org\/anthology\/R15-1015.pdf, although I invented my own solution.\n\nMy MultiWordSplitter class uses a widely available dictionary and heuristics to find the tokens that might represent the way the hash tag could be read by a human.","d2fdc185":"# Additional comments and next steps\n\nThe actual results on the Kaggle test set got a slightly higher evaluation than for my test set from their training data.\n\nBetter results might be obtained with an Long Short Term Memory Network (LSTM), or with Google's Deep Bidirectional Transformers (BERT). My intention is to compare a solution the latter with my solution shown above.\n\nAnother area to consider is improving the quality of the training data. I have noticed that the ground truth appears to have inconsistencies on what is considered a disaster. Improving the quality of the testing data is a strategy has been promoted by Andrew Ng of Landing AI and deeplearning.ai. This idea may work for this exercise. For example these tweets would not be classified as disasters by most humans, but are disasters in the ground truth:\n* \"tomorrow's going to be a year since I went to the Panic! concert ...\"\n* \"Reddit's new content policy goes into effect many horrible subreddits banned or quarantined\"\n* \"Hey the #Royals love doing damage with 2 outs.\"\n* \"China's Stock Market Crash: Are There Gems In The Rubble?\"","1d603d69":"# NLP Pipeline\n\nThe classifier used by the NLP pipeline is a neural network with 50 hidden layers.\n\nThere are four groups of features that are used as input to the neural network.\n* TF-IDF model for non-stopwords, stemmed words from the tweet text. The model uses PCA for dimension reduction.\n* word encoding vector (glove-twitter-25) for recognized words in the tweet text. \n* word encoding vector (glove-twitter-25) of keywords.\n* number of tokens in the tweet. Initial exploration suggested that longer tweets are more likely to be disasters.\n\nThis approach was found to score higher than other variations on the training set while controlling over-fitting.\n\nGrid Search was used to tune the parameters","49bbbe32":"# Train the model","0936c5fc":"## Splitter Examples\nExamples of how the splitter works. While far from perfect, it seems to do OK for these made up examples.","525212d2":"## Pipeline definition and Parameters","484f82de":"# Preprocessing and tokenization\n\nPreprocessing tweets means cleaning up some bad spaces and normalizing the unicode. Since numerics get removed by tokenization later, I replace money patterns such as \"$100\" with the word \"dollars\".\n\nTokenization uses the NLTK package. In addition to using the standard NLTK TweetTokenizer, I also split on numerics and punctuation, and I split hashtags using the MultiWordSplitter class that I invented.\n\nHTTP Links are removed from the text, since I did not find any evidence that the number of links was a useful feature. Numerics are also removed as well for the same reason.\n\nTokenization can optionally remove stop words and perform stemming.","18662de6":"# Load data\n\nThe data has been split into 80% training and 20% test","9b4b72fb":"## Custom Transformations","5fb36cf0":"# Analyze performance"}}