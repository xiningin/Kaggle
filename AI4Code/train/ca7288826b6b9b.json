{"cell_type":{"942a306d":"code","2bde1d01":"code","e147aac4":"code","b085cd34":"code","194fa1da":"code","8f234f3a":"code","52b78d7a":"code","b1edaab2":"code","8198f9b2":"code","e1c9ef57":"code","78a69b0e":"code","106979f8":"code","3e506835":"code","6d61554f":"code","dec294a8":"code","f99083e9":"code","c33f29c8":"code","209dde59":"code","25d71556":"code","da024c92":"code","9569e50d":"code","8bf3a2af":"code","b19d7681":"code","b4392fff":"code","331c8953":"code","de415c3e":"code","2f75dcb4":"code","ec81212d":"code","515e37d5":"code","327ef2d9":"code","6ceb32af":"code","09871ab2":"code","08c2acf1":"code","22c09764":"code","c4c9effd":"code","8ad6ca57":"code","c4523866":"code","c4a0aad2":"code","0109e31b":"code","2e6a144a":"code","1f95bf14":"code","b1574279":"code","c5dbec79":"markdown","89e834b1":"markdown","5c142687":"markdown","030227d8":"markdown","b3bc5ae2":"markdown","2ff269ce":"markdown","4ef47908":"markdown","8de50a57":"markdown","28ef18dc":"markdown","7d961688":"markdown","3eb6dd22":"markdown","63d85bdc":"markdown","da0accc3":"markdown","c1ee886f":"markdown","7d46bc04":"markdown","60b57c27":"markdown","e244213e":"markdown","fb6c3d6c":"markdown","5c65cdb0":"markdown","a2be0407":"markdown","87b964fb":"markdown","6cd05e27":"markdown","10fcd843":"markdown","61a2b504":"markdown","86b5fdef":"markdown","ae1fae32":"markdown","ef771190":"markdown","69ceec90":"markdown","c23e2bae":"markdown","4977fdf1":"markdown","5c1aacf3":"markdown","63e1d269":"markdown","d6179b90":"markdown","b1ca88bd":"markdown","9b09571a":"markdown","9d82c0be":"markdown","1d4e51cc":"markdown","857af518":"markdown","26c9ed83":"markdown","0e9c2f92":"markdown","2efd43f4":"markdown","b010c99f":"markdown","122ccae0":"markdown","b4043a0b":"markdown","7740a37e":"markdown"},"source":{"942a306d":"# Data wrangling\nimport pandas as pd\nimport numpy as np\n\n# Data visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Machine learning\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Remove warnings\nimport warnings\nwarnings.filterwarnings('ignore')","2bde1d01":"data = pd.read_csv(\"..\/input\/students-performance-in-exams\/StudentsPerformance.csv\")\ndata.head()","e147aac4":"print(\"Data shape: \", data.shape)","b085cd34":"data.info()","194fa1da":"data.dtypes.value_counts()","8f234f3a":"score = data[['math score', 'reading score', 'writing score']]\nscore.head()","52b78d7a":"# Correlation between test scores\n\ncorrelation = score.corr()\nsns.heatmap(correlation, annot = True, fmt = '.2f', cmap = 'coolwarm')","b1edaab2":"# Calculate average of all test results\ndata['mean score'] = data.mean(axis = 1)\n\n# Drop math score, reading score and writing score\ndata = data.drop(['math score', 'reading score', 'writing score'], axis = 1)\n\n# Show first 5 rows of new dataframe\ndata.head()","8198f9b2":"# Mean score data type\n\nprint(\"Mean score data type: \", data['mean score'].dtype)","e1c9ef57":"# Mean score by gender \n\ndata[['gender', 'mean score']].groupby('gender', as_index = False).mean().sort_values(by = 'mean score', ascending = False, ignore_index = True)","78a69b0e":"# Mean score by gender barplot\n\nplt.figure(figsize = (5, 5))\nsns.barplot(x = 'gender', y = 'mean score', data = data)\nplt.title('Mean Score by Gender')","106979f8":"# Mean score by race\/ethnicity\n\ndata[['race\/ethnicity', 'mean score']].groupby('race\/ethnicity', as_index = False).mean().sort_values(by = 'mean score', ascending = False, ignore_index = True)","3e506835":"# Mean score by race\/ethnicity barplot\n\nplt.figure(figsize = (6, 6))\nsns.barplot(x = 'race\/ethnicity', y = 'mean score', data = data)\nplt.title('Mean Score by Race\/Ethnicity')","6d61554f":"# Mean score by parental level of education \n\ndata[['parental level of education', 'mean score']].groupby('parental level of education', as_index = False).mean().sort_values(by = 'mean score', ascending = False, ignore_index = True)","dec294a8":"# Mean score by parental level of education barplot\n\nplt.figure(figsize = (6, 6))\nsns.barplot(x = 'parental level of education', y = 'mean score', data = data)\nplt.title('Mean Score by Parental Level of Education')\nplt.xticks(rotation = 90)","f99083e9":"# Mean score by lunch\n\ndata[['lunch', 'mean score']].groupby('lunch', as_index = False).mean().sort_values(by = 'mean score', ascending = False, ignore_index = True)","c33f29c8":"# Mean score by lunch barplot\n\nplt.figure(figsize = (5, 5))\nsns.barplot(x = 'lunch', y = 'mean score', data = data)\nplt.title('Mean Score by Lunch')","209dde59":"# Mean score by test preparation course \n\ndata[['test preparation course', 'mean score']].groupby('test preparation course', as_index = False).mean().sort_values(by = 'mean score', ascending = False, ignore_index = True)","25d71556":"# Mean score by test preparation course barplot\n\nplt.figure(figsize = (5, 5))\nsns.barplot(x = 'test preparation course', y = 'mean score', data = data)\nplt.title('Mean Score by Test Preparation Course')","da024c92":"# Mean score summary statistics\n\ndata['mean score'].describe()","9569e50d":"# Distribution of mean score \n\nplt.figure(figsize = (8, 5))\nsns.distplot(data['mean score'], label = 'Skewness: {:.2f}'.format(data['mean score'].skew()))\nplt.legend(loc = 'best')\nplt.title('Distribution of Mean Score')","8bf3a2af":"data.head()","b19d7681":"# Instantiate OneHotEncoder\n\nohe = OneHotEncoder(sparse = False)","b4392fff":"# Apply OneHotEncoder to the gender column \n\nohe.fit_transform(data[['gender']])[:5]","331c8953":"# The first 5 rows the gender column for comparison\n\ndata['gender'].head()","de415c3e":"# Gender categories in OneHotEncoder \n\nohe.categories_","2f75dcb4":"# Pandas get_dummies approach\n\npd.get_dummies(data['gender']).head()","ec81212d":"# Unique values in the parental level of education column\n\nlist(data['parental level of education'].unique())","515e37d5":"# Specify the order for the level of education \n\neducation_categories = ['some high school', 'high school', 'some college', \"associate's degree\", \"bachelor's degree\", \"master's degree\"]","327ef2d9":"# Instantiate ordinal encoder\n\noe = OrdinalEncoder(categories = [education_categories])","6ceb32af":"# Apply ordinal encoder to parental level of education column\n\noe.fit_transform(data[['parental level of education']])[:5]","09871ab2":"# First 5 rows of parental level of education for comparison \n\ndata['parental level of education'].head()","08c2acf1":"# Pandas map method \n\ndata['parental level of education'].map({'some high school': 0, \n                                         'high school': 1,\n                                         'some college': 2,\n                                         \"associate's degree\": 3, \n                                         \"bachelor's degree\": 4,\n                                         \"master's degree\": 5}).head()","22c09764":"# Get predictor variables and target variable from data\n\nX = data.drop('mean score', axis = 1)\nY = data['mean score']","c4c9effd":"# Make column transformer which consists of OneHotEncoder and OrdincalEncoder\n\ncolumn_transform = make_column_transformer(\n    (ohe, ['gender', 'race\/ethnicity', 'lunch', 'test preparation course']), \n    (oe, ['parental level of education']))","8ad6ca57":"# Apply column transformer to predictor variables \n\ncolumn_transform.fit_transform(X)[:5]","c4523866":"# Train test split \n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 42)\n\nprint(\"X_train shape: \", X_train.shape)\nprint(\"Y_train shape: \", Y_train.shape)\nprint(\"X_test shape: \", X_test.shape)\nprint(\"Y_test shape: \", Y_test.shape)","c4a0aad2":"# Instantiate pipeline with linear regression\n\nlm = LinearRegression()\nlm_pipeline = make_pipeline(column_transform, lm)","0109e31b":"# Instantiate pipeline with gradient boosting\n\ngbm = GradientBoostingRegressor()\ngbm_pipeline = make_pipeline(column_transform, gbm)","2e6a144a":"# Fit pipeline to training set and make predictions on test set \n\nlm_pipeline.fit(X_train, Y_train)\nlm_predictions = lm_pipeline.predict(X_test)\nprint(\"First 5 LM predictions: \", list(lm_predictions[:5]))\n\ngbm_pipeline.fit(X_train, Y_train)\ngbm_predictions = gbm_pipeline.predict(X_test)\nprint(\"First 5 GBM predictions: \", list(gbm_predictions[:5]))","1f95bf14":"# Compare the number of predictions with the size of test set\n\nprint(\"Number of LM predictions: \", len(lm_predictions))\nprint(\"Number of GBM predictions: \", len(gbm_predictions))\nprint(\"Size of test set: \", len(Y_test))","b1574279":"# Calculate mean square error and root mean squared error \n\nlm_mae = mean_absolute_error(lm_predictions, Y_test)\nlm_rmse =  np.sqrt(mean_squared_error(lm_predictions, Y_test))\nprint(\"LM MAE: {:.2f}\".format(round(lm_mae, 2)))\nprint(\"LM RMSE: {:.2f}\".format(round(lm_rmse, 2)))\n\ngbm_mae = mean_absolute_error(gbm_predictions, Y_test)\ngbm_rmse =  np.sqrt(mean_squared_error(gbm_predictions, Y_test))\nprint(\"GBM MAE: {:.2f}\".format(round(gbm_mae, 2)))\nprint(\"GBM RMSE: {:.2f}\".format(round(gbm_rmse, 2)))","c5dbec79":"# 6. Exploratory data analysis (EDA)\n\nExploratory data analysis is the process of analysing and visualising the variables in a dataset. This step is not necessary for feature encoding but I personally like to examine my data first before performing any machine learning. So, feel free to skip past this section if you are only interested to learn about feature encoding. ","89e834b1":"The number of predictions matches the size of the test set. We can now proceed to evaluate the accuracy of each model. ","5c142687":"The nominal variables in our dataset include:\n\n- Gender\n- Race\/ethnicity\n- Lunch\n- Test prepation course\n\nIn this section, we will explore two different ways to encode nominal variables, one using the Scikit-learn OneHotEncoder while the other using the [Pandas get_dummies](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.get_dummies.html). ","030227d8":"For simplicity purposes, let's take the average of the 3 test scores so that we are left with only one target variable to predict. ","b3bc5ae2":"# 7.2 Encoding ordinal variable\n\nThe only ordinal variable in our dataset is the parental level of education feature. \n\nIn this section, we will again consider two approaches to encoding ordinal variable, one using Scikit-learn OrdinalEncoder while the other using Pandas map method. ","2ff269ce":"Although both approaches give the same result, OneHotEncoder is generally preferred over get_dummies due to the following reasons:\n\n- Under OneHotEncoder, our original dataframe remains the same size and therefore easier to manage and explore.\n- OneHotEncoder can be incorporated as part of a pipeline in Scikit-learn whereas get_dummies require a more manual approach to encoding. This makes OneHotEncoder more efficient at transforming both the training set as well as the test set. \n- Under OneHotEncoder, we can use GridSearch to evaluate and choose preprocessing parameters.\n- Cross-validation scores are more reliable under OneHotEncoder than get_dummies.\n\nWe will explore the idea of building a machine learning pipeline later on.","4ef47908":"# 7.1 Encoding nominal variables","8de50a57":"# 0. Introduction","28ef18dc":"It is important that we specify the ordering of an ordinal variable. For our parental level of education feature, we want the order to go as follows:\n\n1. Some high school\n2. High school\n3. Some college\n4. Associate's degree\n5. Bachelor's degree\n6. Master's degree \n","7d961688":"# 3. Data description","3eb6dd22":"Female students perform better than male students on average. ","63d85bdc":"To summarise, in this notebook, we have learned the difference between a nominal variable and an ordinal variable as well as how to properly encode them using Scikit-learn OneHotEncoder and LabelEncoder.\n\nWe also discussed the benefits of performing feature encoding using the Scikit-learn library over the Pandas library i.e. the flexibility to chain data preprocessing together with a machine learning model to form a cohesive machine learning pipeline.\n\nFinally, we compared the accuracy of two separate pipelines, one with linear regression and the other with gradient boosting, at predicting the student's mean test score. We concluded that linear regression is slighly more accurate than gradient boosting due to the lower mean absolute error (MAE) and root mean squared error (RMSE). ","da0accc3":"## 6.1.1 Gender","c1ee886f":"## 6.1.4 Lunch","7d46bc04":"There seems to be an increasing trend in mean test scores as we move from group A to group E i.e. group A students perform the worst while group E students perform the best.\n\nHowever, in the absence of any further information, it is hard for us to draw any conclusion. ","60b57c27":"One of the preprocessing steps in machine learning is feature encoding. It is the process of turning categorical data in a dataset into numerical data. It is important that we perform feature encoding because most machine learning algorithms only handle numerical data and not data in text form.\n\nIn this notebook, we will learn the difference between nominal variables and ordinal variables. In addition, we will explore how [OneHotEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html) and [OrdinalEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder) can be used to transform these variables as part of a machine learning pipeline.\n\nWe will use this pipeline to predict the mean test score of different students. This is a regression problem in machine learning.","e244213e":"# 7. Feature encoding\n\nNow that we have fully explored the variables in the dataset, we can move on to getting the dataset reading for modelling. More specifically, we want to turn the categorical data in our dataset into numerical data. This process is otherwise known as feature encoding.\n\nBut before we dive into feature encoding, it is important that we first contrast the difference between a nominal variable and an ordinal variable. This is so that we can match the right encoder to the right variable. \n\nNominal variable is a categorical variable where its data does not have a logical ordering. Some examples of nominal data are:\n- Male and female\n- Location A, location B and location C\n\nOrdinal variable, on the other hand, is also a categorical variable but its data follows a logical ordering. Some example of ordinal data include:\n- Small, medium and large\n- Bad, neutral and good\n- Children, adults and seniors\n- Low, medium and high income\n\nAs we will see in this section, nominal variables are encoded using OneHotEncoder while ordinal variables are encoded using OrdinalEncoder.","fb6c3d6c":"## 7.1.3 Pandas get_dummies","5c65cdb0":"## 6.1.2 Race\/ethnicity","a2be0407":"For the most part, the results are aligned with what we would expect i.e. as parental level of education increases, mean test score also increases. ","87b964fb":"# 8. Build machine learning pipeline\n\nA pipeline chains together multiple steps in the machine learning process where the output of each step is used as input to the next step. It is typically used to chain data preprocessing procedures together with modelling into one cohesive workflow. \n\nHere, we will build two pipelines that share the same column transformer that we have created above but with a different machine learning model, one using linear regression and the other using gradient boosting. \n\nWe will then compare the accuracy of the prediction results using mean absolute error (MAE) as well as root mean squared error (RMSE). The model with a lower prediction error is deemed more accurate than the other. ","6cd05e27":"# 2. Import and read data","10fcd843":"The [students performance in exams dataset](https:\/\/www.kaggle.com\/spscientist\/students-performance-in-exams) on Kaggle consists of marks secured by 1,000 students in the math, reading and writing subjects. Along with these test scores, the description of each student such as their gender, race\/ethnicity, parental level of education, lunch and test preparation course are also included in the dataset. \n\nThe goal of this project is to build a machine learning model that can predict students' test score given their description. To reiterate, this is a regression problem in machine learning. ","61a2b504":"# 4. Missing values and data types","86b5fdef":"Note that OrdinalEncoder has assigned the value of 0 to some high school, 1 to high school, 2 to some college, 3 to associate's degree, 4 to bachelor's degree and finally, 5 to master's degree. ","ae1fae32":"We have 5 categorical variables and 3 numerical variables.\n\nAs it turns out, all the predictor variables are categorical variables and all the target variables are numerical variables. ","ef771190":"# 10. Follow me on my platforms ","69ceec90":"## 7.2.1 Scikit-learn OrdinalEncoder\n\nOrdinalEncoder differs from OneHotEncoder such that it assigns incremental values to unique values in an ordinal variable rather than just 0 and 1. \n\nThis helps machine learning models to recognise an ordinal variable and subsequently use the information that it has learned to make more accurate predictions. \n\nLet's now see how we can encode the parental level of education feature using OrdinalEncoder. ","c23e2bae":"## 6.1.3 Parental level of education","4977fdf1":"As we can see, OneHotEncoder has created two columns to represent the gender feature in our dataframe, one for female and one for male.\n\nFemale students will receive a value of 1 in the female column and 0 in the male column whereas male students will receive a value of 0 in the female column and 1 in the male column. \n\nBut most importantly, OneHotEncoder has successfully transformed what was originally a categorical variable into a numerical variable. ","5c1aacf3":"Students with standard lunch perform better than those with free\/reduced lunch. \n\nLunch can be seen as a proxy for the financial background of the students. Therefore, it makes sense that students that financially better off do better in school compared to those that are of financially worse off, on average. ","63e1d269":"## 7.1.1 Scikit-learn OneHotEncoder","d6179b90":"# 9. Conclusion ","b1ca88bd":"Hooray, no missing values!","9b09571a":"#  1. Import libraries","9d82c0be":"## 6.1.5 Test preparation course","1d4e51cc":"# 5. Create target variable (mean score)","857af518":"Since the 3 test scores are highly positively correlated with each other, it is appropriate for us to take the average. ","26c9ed83":"We can conclude that linear regression is slightly more accurate than gradient boosting. ","0e9c2f92":"# 6.1 Predictor variables\n\nThe predictor variables in the dataset are:\n\n- Gender\n- Race\/ethnicity\n- Parental level of education\n- Lunch\n- Test preparation course\n\nIn this section, we will explore how these different features influence the outcome of a student's test score. ","2efd43f4":"- [Facebook](https:\/\/www.facebook.com\/chongjason914)\n- [Instagram](https:\/\/www.instagram.com\/chongjason914)\n- [Twitter](https:\/\/www.twitter.com\/chongjason914)\n- [LinkedIn](https:\/\/www.linkedin.com\/in\/chongjason914)\n- [YouTube](https:\/\/www.youtube.com\/channel\/UCQXiCnjatxiAKgWjoUlM-Xg?view_as=subscriber)\n- [Medium](https:\/\/www.medium.com\/@chongjason)","b010c99f":"This should come at no surprise at all. The more prepared students are for their exam, the better they will perform. ","122ccae0":"# 6.2 Target variable (mean score)","b4043a0b":"# 7.3 Make column transformer\n\nFor the remainder of this notebook, we will only consider OneHotEncoder and OrdinalEncoder as means to encode the categorical variables in our dataset.\n\nIn this section, we will combine OneHotEncoder and OrdinalEncoder into a single-step column transformer.","7740a37e":"## 7.2.2 Pandas map method\n\nPandas map method is a more manual approach to encoding ordinal variable. Although it replicates the result of the OrdinalEncoder, it is not ideal for encoding ordinal variables with a high number of unique values."}}