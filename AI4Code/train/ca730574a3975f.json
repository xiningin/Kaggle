{"cell_type":{"f4e798a8":"code","6dcc646d":"code","8596e992":"code","22e16d04":"code","43207e12":"code","4bc96d11":"code","c3be7ad0":"code","6aae2be0":"code","e5aee9e2":"code","f6120fc2":"code","e83af891":"code","d6f1ae16":"code","3b6f1682":"code","b5bca404":"code","32f1b6c1":"code","da209519":"code","613a7b3f":"code","0766c022":"code","bfcec2c3":"code","3dfa4ced":"code","0061c8cf":"code","08831c19":"code","b7f91546":"code","d523a5f7":"code","73965734":"code","6a2b0253":"code","0eb56c01":"code","2a93d9c7":"code","bd019ca2":"code","b92a0fcb":"code","9f7638dd":"code","d7270848":"code","d91d5428":"code","fba8339b":"code","4dfc10aa":"code","f11c7e92":"code","7d1fb016":"code","1a60fe9c":"code","136021a4":"code","74999f95":"code","a8584951":"code","b9cf42da":"code","7fa5dfb3":"code","897bb3c5":"code","d8e5293c":"code","24f6cacb":"code","5bb237ef":"code","53397223":"markdown","0bc50f04":"markdown","6eff31f1":"markdown","5cb4b1bc":"markdown","c9c73b40":"markdown","483eb008":"markdown","c70f7685":"markdown","9ad512e9":"markdown","8e9d4294":"markdown","36584ac3":"markdown","aa22979b":"markdown","4ba37185":"markdown","0615b283":"markdown","44c44dc1":"markdown","39322681":"markdown","001cf092":"markdown","2f30e6e4":"markdown","02c3739e":"markdown","8edba49d":"markdown","e01db1f1":"markdown","88629144":"markdown","b0e48f78":"markdown","4c42895d":"markdown","eb3367fb":"markdown"},"source":{"f4e798a8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline","6dcc646d":"cd \/kaggle\/input\/titanic-data-preparation","8596e992":"train_data = pd.read_csv('train_prep.csv')\ntest_data = pd.read_csv('test_prep.csv')\ntrain_data.columns","22e16d04":"train_data.set_index('PassengerId',inplace = True)\ntest_data.set_index('PassengerId',inplace = True)","43207e12":"all_data = pd.concat([train_data,test_data])","4bc96d11":"print(train_data.shape,test_data.shape)\nprint(all_data.shape)","c3be7ad0":"from sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()","6aae2be0":"all_data['FareBin'] = pd.qcut(all_data['Fare'], 5)\nall_data['AgeBin'] = pd.cut(all_data['Age'],[0,2,5,12,18,80])","e5aee9e2":"all_data['FareBin_Code'] = label.fit_transform(all_data['FareBin'])\nall_data['AgeBin_Code'] = label.fit_transform(all_data['AgeBin'])","f6120fc2":"all_data.drop(columns=['AgeBin'],inplace=True)\nall_data.drop(columns=['FareBin'],inplace=True)","e83af891":"for i in range(1,len(all_data)+1):\n    all_data.loc[i,'Srate']=0.5","d6f1ae16":"for i in range(1,len(all_data)+1):\n    number = all_data.loc[i,'TicketNumber']\n    group = all_data.loc[i,'TicketGroup'] \n    if group>1:\n        max = all_data.drop(i)[all_data.drop(i)['TicketNumber']==number]['Survived'].max()\n        if not pd.isna(max):\n            all_data.loc[i,'Srate']=all_data.drop(i)[all_data.drop(i)['TicketNumber']==number]['Survived'].max()","3b6f1682":"all_data.head()","b5bca404":"train_data = all_data[:891]\ntest_data = all_data[891:]","32f1b6c1":"numerical = [\"Pclass\",\"Age\",\"SibSp\",\"Parch\",\"Fare\",\"TicketGroup\",\"FamilySize\",\"FareBin_Code\",\"AgeBin_Code\",\"Srate\",\"TicketNumber\"]\ncategorical = [\"Embarked\",\"Title\",\"Sex\"]\nall_features = numerical + categorical","da209519":"sns.set(rc={'figure.figsize':(12,8)})\nsns.heatmap(train_data[[\"Survived\"]+numerical].corr(),annot=True, fmt = \".3f\")","613a7b3f":"g = sns.catplot(x=\"Sex\",y=\"Survived\",data=train_data,kind=\"bar\", height = 6 , palette = \"muted\")\ng.set_ylabels(\"Survival probability\")","0766c022":"g = sns.catplot(x=\"Title\", y=\"Survived\",col=\"TicketGroup\", data=train_data, kind=\"bar\", col_wrap=3,height=4, aspect=.8,legend=True,sharex=False, ci=None)\nfor ax in g.axes.flatten():\n    ax.set_xticklabels([\"Mr\",\"Mrs\",\"Miss\",\"Master\",\"Rev\"])","bfcec2c3":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import VotingClassifier","3dfa4ced":"y = train_data['Survived']\nX = train_data.drop(columns = ['Survived'])","0061c8cf":"from sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression(C=1.9)","08831c19":"num_att_LR = [\"Pclass\",\"Age\",\"TicketNumber\",\"SibSp\",\"Parch\",\"Srate\",\"Fare\"]\ncat_att1_LR = [\"Sex\"]\ncat_att2_LR = [\"Title\"]\ndrop_att_LR = list(set(all_features) - set(num_att_LR+cat_att1_LR+cat_att2_LR))\n\nfull_pipline_LR = ColumnTransformer([\n    (\"toDrop\",'drop',drop_att_LR),\n    (\"numerical\",StandardScaler(),num_att_LR),\n    (\"categorical1\",OrdinalEncoder(),cat_att1_LR),\n    (\"categorical2\",OneHotEncoder(),cat_att2_LR)\n])","b7f91546":"X_prep = full_pipline_LR.fit_transform(X)\ncross_val_score(log_reg,X_prep,y,cv=10,scoring='roc_auc').mean()","d523a5f7":"from sklearn.naive_bayes import CategoricalNB\ncat_bayes = CategoricalNB(alpha=0.05)","73965734":"num_att_NB = [\"FareBin_Code\",\"AgeBin_Code\"]\ncat_att_NB = [\"Title\"]\ndrop_att_NB = list(set(all_features) - set(num_att_NB+cat_att_NB))\n\nfull_pipline_NB = ColumnTransformer([\n    (\"cat\",OrdinalEncoder(),cat_att_NB),\n    (\"num\",'passthrough',num_att_NB),\n    (\"drop\",'drop',drop_att_NB)\n])","6a2b0253":"X_prep = full_pipline_NB.fit_transform(X)\ncross_val_score(cat_bayes,X_prep,y,cv=10,scoring='roc_auc').mean()","0eb56c01":"from sklearn import tree","2a93d9c7":"num_att_DT =[\"Pclass\",\"FamilySize\",\"Srate\",\"AgeBin_Code\",\"FareBin_Code\"]\ncat_att_DT = [\"Sex\"]\ndrop_att_DT = list(set(all_features) - set(num_att_DT + cat_att_DT))\n\nfull_pipline_DT = ColumnTransformer([\n    (\"droping\",'drop',drop_att_DT),\n    (\"num\",'passthrough',num_att_DT),\n    (\"cat\",OrdinalEncoder(),cat_att_DT),\n])","bd019ca2":"X_prep = full_pipline_DT.fit_transform(X)","b92a0fcb":"criterion = [\"gini\", \"entropy\"]\nmax_depth = list(range(1,12,1))\nmax_features = [1,2,3,4,5,6]\nsplitter = [\"best\",\"random\"]\nmin_samples_split = [2,3]\nhyperparams = {'criterion': criterion, 'max_depth': max_depth, 'max_features': max_features, \n               'splitter': splitter, 'min_samples_split': min_samples_split}\n\ngd = GridSearchCV(estimator = tree.DecisionTreeClassifier(random_state=1), param_grid = hyperparams, verbose=0, \n                cv=10, scoring = \"roc_auc\")","9f7638dd":"gd.fit(X_prep, y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","d7270848":"DT_clf = tree.DecisionTreeClassifier(criterion='entropy', max_depth=5, max_features=6,\n                       random_state=1, splitter='random') ","d91d5428":"from sklearn.svm import LinearSVC\nfrom sklearn.svm import SVC\nsvm_clf = LinearSVC(C=0.1, loss = 'hinge', max_iter=100000, random_state=1,)","fba8339b":"num_att_SVM = numerical\ncat_att1_SVM = [\"Sex\"]\ncat_att2_SVM = [\"Title\"]\ndrop_att_SVM = list(set(all_features) - set(num_att_SVM + cat_att1_SVM+cat_att2_SVM))\n\nfull_pipline_SVM = ColumnTransformer([\n    (\"drop\",'drop',drop_att_SVM),\n    (\"cat\",OrdinalEncoder(),cat_att1_SVM),\n    (\"num\",StandardScaler(),num_att_SVM),\n    (\"cat2\",OneHotEncoder(),cat_att2_SVM),\n])","4dfc10aa":"X_prep = full_pipline_SVM.fit_transform(X)\ncross_val_score(svm_clf,X_prep,y,cv=10,scoring='roc_auc').mean()","f11c7e92":"LR_pipe = Pipeline([('logReg pipline', full_pipline_LR), \n                  ('log_reg', log_reg)])\n\ncross_val_score(LR_pipe,X,y,cv=10,scoring='accuracy').mean()","7d1fb016":"NB_pipe = Pipeline([('NaiveBayes pipline', full_pipline_NB), \n                  ('cat_bayes', cat_bayes)])\n\ncross_val_score(NB_pipe,X,y,cv=10,scoring='accuracy').mean()","1a60fe9c":"DT_pipe = Pipeline([('DecisionTree pipline', full_pipline_DT), \n                  ('Decision Tree', DT_clf)])\n\ncross_val_score(DT_pipe,X,y,cv=10,scoring='accuracy').mean()","136021a4":"SVM_pipe = Pipeline([('SVM pipline', full_pipline_SVM), \n                  ('SVM', svm_clf)])\n\ncross_val_score(SVM_pipe,X,y,cv=10,scoring='accuracy').mean()","74999f95":"voting_clf_4 = VotingClassifier(\n    estimators = [('Logistic Regresion',LR_pipe),('SVM',SVM_pipe),('Naive Bayes',NB_pipe),('Decision Tree',DT_pipe)],\n    voting = 'hard'\n)\n\ncross_val_score(voting_clf_4,X,y,cv=10,scoring='accuracy').mean()","a8584951":"voting_clf_SVM_DT = VotingClassifier(\n    estimators = [('SVM',SVM_pipe),('Decision Tree',DT_pipe)],\n    voting = 'hard'\n)\n\ncross_val_score(voting_clf_SVM_DT,X,y,cv=10,scoring='accuracy').mean()","b9cf42da":"voting_clf_NB_DT = VotingClassifier(\n    estimators = [('Naive Bayes',NB_pipe),('Decision Tree',DT_pipe)],\n    voting = 'hard'\n)\ncross_val_score(voting_clf_NB_DT,X,y,cv=10,scoring='accuracy').mean()","7fa5dfb3":"X_test = test_data.drop(columns=['Survived'])","897bb3c5":"voting_clf_NB_DT.fit(X,y)","d8e5293c":"predictions = voting_clf_NB_DT.predict(X_test)\npredictions = pd.to_numeric(predictions, downcast='integer')\noutput = pd.DataFrame({'PassengerId': list(X_test.index), 'Survived': predictions})","24f6cacb":"cd \/kaggle\/working","5bb237ef":"output.to_csv('Submission_NB_DT.csv', index=False)","53397223":"This assembly of SVM and DT models gives us improvment to **0.8205**","0bc50f04":"## 1. Logistic Regression","6eff31f1":"On the LB NaiveBayes gets score of **0.7703**","5cb4b1bc":"Those parameters get us a score of **0.7847**. The interesting thing is when we use a model that is different from this one only in one thing - **max_features = 6**, we get score of **0.811**. So we use this model:","c9c73b40":"Naive Bayes with Decision Tree gets us to **0.82535**","483eb008":"## 4. Suport Vector Machines\nI tried LinearSVC and SVC with the polynomial kernel, but my grid search didn't, gave a significant result compared to Linear SVC, so I left Linear kernel.","c70f7685":"Here we load prepared data from notebook [Titanic data preparation](https:\/\/www.kaggle.com\/mkulio\/titanic-data-preparation).\nPrepared data has new features:\n* \"Title\" - title of the passenger, derived from \"Name\" column, which can be Master, Miss, Mrs, Mr, and Revenant\n* \"FamilyName\" - derived from original \"Name\" column\n* \"TicketNumber\" - derived from original \"Ticket\" column\n* \"Mix\" - combination of \"TicketNumber\", \"Pclass\" and \"Embarked\" features\n* \"TicketGroup\" - number of passengers that have the same \"Mix\" feature \n* \"FamilySize\" - this is \"Parch\" + \"SibSp\" + 1\n\nA detailed description of the above features and code behind them you may found in the mentioned notebook.","9ad512e9":"## 5. Voting\nHere, we create pipelines for four models that we use, and we tried all sorts of combinations. The conclusion is that we get the best LB result with a combination of the Decision Tree model and with the Naive Bayes classification model.","8e9d4294":"## 2. Naive Bayes","36584ac3":"# Modeling","aa22979b":"On the LB this model gets **0.7799**","4ba37185":"# Exploring features","0615b283":"# Adding new features","44c44dc1":"*From the above corr table we see the impact of creating bins for \"Fare\" and \"Age\" and \"Srate\" column*","39322681":"With SVM model we get **0.7966** on the LB","001cf092":"First, we tried all four models, and this wasn't a good idea.","2f30e6e4":"# Output","02c3739e":"# Models that I used \n1. Linear Regresion (LB = **0.7799**)\n2. Naive Bayes (LB = **0.7703**)\n3. Decision Tree (LB = **0.811**)\n4. Linear SVM (LB = **0.7966**)\n\nAnd in the end we do assembly of models, and we got:\n* LR + NB + DT + SVM (LB = **0.7994**)\n* SVM + DT (LB = **0.8205**)\n* NB + DT (LB = **0.8253** )\n\nFor choosing parameters I used function cross_val_score(cv=10,scoring='roc_auc')\nwith: 10 folds and with scoring function: ROC accuracy. Also, all models get better on CV score than on the LB. Only in the Decision Tree models, I used parameters according to LB. ","8edba49d":"Next, we create \"Srate\" column which represents passenger's survival rate. Idea is from the notebooks:\n* [Titanic [0.82] - [0.83]](https:\/\/www.kaggle.com\/konstantinmasich\/titanic-0-82-0-83) by KonstantinMasich\n* [Blood is thicker than water & friendship forever](https:\/\/www.kaggle.com\/shunjiangxu\/blood-is-thicker-than-water-friendship-forever) by ShunjiangXu\n\nI modified code a lot, which may be seen in the next few lines:","e01db1f1":"# Importing data","88629144":"## 3. Decision Tree\nHere, I used grid search for finding the best hyperparameters for Decision Tree. If we use more features to decide splitting, we get better LB score.","b0e48f78":"Creating Bins for Age column and Fare column","4c42895d":"This gets us to **0.7994** on the Ladder Board.","eb3367fb":"Looking at the above graphs, one may presume that \"Title\" has a significant impact on creating a model. That is not the case with my single best model - we did not use \"Title column\" in Decision Tree classification. But in the end, with assembly, we used the Naive Bayes model which uses the \"Title\" column."}}