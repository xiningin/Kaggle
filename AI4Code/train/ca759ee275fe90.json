{"cell_type":{"36b2f8dd":"code","039346da":"code","eea04c8c":"code","b5b17939":"code","e8c0135d":"code","ba03142e":"code","f41ae846":"code","d4745776":"code","d7317030":"code","271ddb0d":"code","6342d384":"code","b7984a63":"code","abfd8270":"code","152a2ce3":"code","71a381d2":"code","281d3f56":"code","cc30ec05":"code","b4440e3f":"code","f1c16a98":"code","26a4ec3a":"code","d29aa260":"code","5ea84e1c":"code","2a40f1cc":"code","277e1546":"code","5187f129":"code","86d9c208":"code","1471e7f2":"code","6694583e":"code","592de5a4":"code","1531915a":"code","9ed98428":"code","b3579290":"code","cf8e83d0":"code","f5e572f6":"code","bf3e614e":"code","8875b1b1":"code","6eb36244":"code","ad59f420":"code","12a8cda0":"code","e74be80f":"code","c5f06a53":"code","38562168":"code","a0f765de":"code","f0c9d9ce":"markdown","e7470382":"markdown","f813cf61":"markdown","d8792464":"markdown","d1d34018":"markdown","2dcd2b63":"markdown","1eb13c42":"markdown","b1441a51":"markdown","881d2b7a":"markdown","482de782":"markdown","65b7be4a":"markdown","6f440f71":"markdown","9dbfdb88":"markdown","0eef84e3":"markdown","3e7979e0":"markdown","1ff4ac86":"markdown","ea475f1f":"markdown","f1fe3ced":"markdown","1d400229":"markdown","074c48e5":"markdown","5a0e4989":"markdown","c5d8368e":"markdown","84a5dfe1":"markdown","3041e9c8":"markdown","95c0c2ae":"markdown","722609cf":"markdown","47da241a":"markdown","efb6d817":"markdown"},"source":{"36b2f8dd":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, TFAutoModel\nfrom tqdm.notebook import tqdm","039346da":"!pip install nlp\nimport nlp","eea04c8c":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","b5b17939":"MODEL_NAME = 'jplu\/tf-xlm-roberta-large'\nEPOCHS = 10\nMAX_LEN = 80\nRATE = 1e-5\n\nBATCH_SIZE = 64 * strategy.num_replicas_in_sync","e8c0135d":"train = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/contradictory-my-dear-watson\/sample_submission.csv')","ba03142e":"train.info()","f41ae846":"train.head()","d4745776":"train = train[['premise', 'hypothesis', 'label']]","d7317030":"multigenre_data = nlp.load_dataset(path='glue', name='mnli')","271ddb0d":"index = []\npremise = []\nhypothesis = []\nlabel = []\n\nfor example in multigenre_data['train']:\n    premise.append(example['premise'])\n    hypothesis.append(example['hypothesis'])\n    label.append(example['label'])","6342d384":"multigenre_df = pd.DataFrame(data={\n    'premise': premise,\n    'hypothesis': hypothesis,\n    'label': label\n})","b7984a63":"multigenre_df.head()","abfd8270":"adversarial_data = nlp.load_dataset(path='anli')","152a2ce3":"index = []\npremise = []\nhypothesis = []\nlabel = []\n\nfor example in adversarial_data['train_r1']:\n    premise.append(example['premise'])\n    hypothesis.append(example['hypothesis'])\n    label.append(example['label'])\n    \nfor example in adversarial_data['train_r2']:\n    premise.append(example['premise'])\n    hypothesis.append(example['hypothesis'])\n    label.append(example['label'])\n    \nfor example in adversarial_data['train_r3']:\n    premise.append(example['premise'])\n    hypothesis.append(example['hypothesis'])\n    label.append(example['label'])","71a381d2":"adversarial_df = pd.DataFrame(data={\n    'premise': premise,\n    'hypothesis': hypothesis,\n    'label': label\n})","281d3f56":"train = pd.concat([train, multigenre_df, adversarial_df])","cc30ec05":"train.info()","b4440e3f":"tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)","f1c16a98":"train_text = train[['premise', 'hypothesis']].values.tolist()\ntest_text = test[['premise', 'hypothesis']].values.tolist()","26a4ec3a":"train_encoded = tokenizer.batch_encode_plus(\n    train_text,\n    pad_to_max_length=True,\n    max_length=MAX_LEN\n)","d29aa260":"test_encoded = tokenizer.batch_encode_plus(\n    test_text,\n    pad_to_max_length=True,\n    max_length=MAX_LEN\n)","5ea84e1c":"train.premise.values[0]","2a40f1cc":"print(train_encoded.input_ids[0][0:14])","277e1546":"vocab = tokenizer.get_vocab()\n\nprint(vocab['<s>'])\nprint(vocab['\u2581and'])\nprint(vocab['\u2581these'])\nprint(vocab['\u2581comments'])\nprint(vocab['\u2581were'])\nprint(vocab['\u2581considered'])\nprint(vocab['\u2581in'])\nprint(vocab['\u2581formula'])\nprint(vocab['ting'])\nprint(vocab['\u2581the'])\nprint(vocab['\u2581inter'])\nprint(vocab['im'])\nprint(vocab['\u2581rules'])\nprint(vocab['.'])","5187f129":"train.hypothesis.values[0]","86d9c208":"print(train_encoded.input_ids[0][14:32])","1471e7f2":"print(vocab['<\/s>'])\nprint(vocab['\u2581The'])\nprint(vocab['\u2581rules'])\nprint(vocab['\u2581developed'])\nprint(vocab['\u2581in'])\nprint(vocab['\u2581the'])\nprint(vocab['\u2581inter'])\nprint(vocab['im'])","6694583e":"train_encoded.keys()","592de5a4":"print(train_encoded.attention_mask[0][0:35])","1531915a":"x_train, x_valid, y_train, y_valid = train_test_split(\n    train_encoded['input_ids'], \n    train.label.values, \n    test_size=0.2, \n    random_state=2020\n)","9ed98428":"x_test = test_encoded['input_ids']","b3579290":"auto = tf.data.experimental.AUTOTUNE\n\ntrain_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(auto)\n)","cf8e83d0":"valid_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(BATCH_SIZE)\n    .cache()\n    .prefetch(auto)\n)","f5e572f6":"test_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(BATCH_SIZE)\n)","bf3e614e":"with strategy.scope():\n    backbone = TFAutoModel.from_pretrained(MODEL_NAME)","8875b1b1":"with strategy.scope():\n    x_input = tf.keras.Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_ids\")\n\n    x = backbone(x_input)[0]\n\n    x = x[:, 0, :]\n\n    x = tf.keras.layers.Dense(3, activation='softmax')(x)\n\n    model = tf.keras.models.Model(inputs=x_input, outputs=x)","6eb36244":"model.compile(\n    tf.keras.optimizers.Adam(lr=RATE), \n    loss='sparse_categorical_crossentropy', \n    metrics=['accuracy']\n)","ad59f420":"model.summary()","12a8cda0":"steps = len(x_train) \/\/ BATCH_SIZE\n\nhistory = model.fit(\n    train_dataset,\n    validation_data=valid_dataset,\n    epochs=EPOCHS,\n    steps_per_epoch=steps,\n)","e74be80f":"fig, ax = plt.subplots(2, 2, figsize=(15, 5))\n\nax[0,0].set_title('Train Loss')\nax[0,0].plot(history.history['loss'])\n\nax[0,1].set_title('Train Accuracy')\nax[0,1].plot(history.history['accuracy'])\n\nax[1,0].set_title('Val Loss')\nax[1,0].plot(history.history['val_loss'])\n\nax[1,1].set_title('Val Accuracy')\nax[1,1].plot(history.history['val_accuracy'])","c5f06a53":"test_preds = model.predict(test_dataset, verbose=1)\nsubmission['prediction'] = test_preds.argmax(axis=1)","38562168":"submission.head()","a0f765de":"submission.to_csv('submission.csv', index=False)","f0c9d9ce":"## Evaluate\n\nLet's see a summary of how the model did.","e7470382":"### Augment dataset\n\nI've learnt recently that many data scientists do well in Kaggle competitions because they augment the training data with extra data they have found. This competiton has 12k examples which isn't a huge volume of training data. While a decent model can be trained with this we could do better with more examples. Luckily [hugging face](https:\/\/huggingface.co\/) (the people who produced the transformers library) have released a library called [nlp](https:\/\/huggingface.co\/datasets) that contains a bunch of good datasets. Kudos to Yih-Dar SHIEH whose [notebook](https:\/\/www.kaggle.com\/yihdarshieh\/more-nli-datasets-hugging-face-nlp-library#Datasets) I used to learn how to use the nlp library.\n\nI'll start by downloading the [Multi-Genre NLI Corpus](https:\/\/cims.nyu.edu\/~sbowman\/multinli\/) dataset.","f813cf61":"And take a look at what it looks like.","d8792464":"Then get all the sentences from the datasets and convert them from strings to arrays of tokens.","d1d34018":"Then take the backbone and apply the softmax layer that produces the class.","2dcd2b63":"## Load data\n\nNext load the data and have a look at it.","1eb13c42":"When a model has two inputs (like a premise and hypothesis) the transformer will merge the tokens from the two sentences into the one array. The \"\\<\\s\\>\" token is used to denote the end of the the premise and the beginning of the hypothesis. In this example we can see that at character 13 the tokens representing the first premise end with the \"\\<\\s\\>\" token and the tokens following it represent the hypothesis sentence.","b1441a51":"And write the test results to file.","881d2b7a":"You may have noticed that there is another imput per sentence. The input ids are the token arrays that were explored above. ","482de782":"nlp datasets can be reshaped into pandas dataframes using the below code.","65b7be4a":"## Make predictions\n\nFinally use the model to make predicitons against the test set.","6f440f71":"## Train\n\nWith the pipeline and model ready to go we can begin training.","9dbfdb88":"Unlike GPUs a TPU needs to be found and setup to work with the model in the notebook. Specifically a \"strategy\" needs to be defined regarding how the model will be replicated across the eight GPU chips on the TPU board and how these replica models will be merged back together once training has completed. This piece of code finds a TPU (or gets a GPU or CPU if one is not available) and sets up this strategy.","0eef84e3":"## Pipeline\n\nWhen using tensorflow and TPUs it is best to build a data pipeline using tensorflows data api. This produces better performance during training.\n\nThe pipeline is reasinably straight forward. Insert the data using the from tensor slices commmand, shuffle it, batch it and prefetch the next batch while the model is training on the current batch.","3e7979e0":"So the data is quite straight forward. Each example contains two sentences (a premise and a hypothesis) and a class telling us if the sentences are saying the same thing (entailment), disagree with one another (contradiction) or they are talking about different things (neutral). So the model needs to take in two inputs (the two sentences) and return one of three classes.\n\nWe don't need all the columns so I'll drop the language columns.","1ff4ac86":"## Model\n\nIf you have trained computer vision models you may have built models with \"backbones\". These are pre-trained models whose weights can be generalised to a new task. Stick some extra layers (the head) to the end of the model to handle the new task and you have a model that benefits from cutting edge trained but that is still built to complete the current task in mind. \n\nI don't know if the terminology of backbones and heads apply to language models but I'm going to build a model with this in mind. I'll be using roberta as the backbone and a sofmax layer on the end to apply the correct class (entailment, neutral, contradiction or 0, 1, 2).\n\nBERT (the original language transformer model that models like roberta are based on) is quite a complex model. If you'd like to understand how it works check out this [notebook](https:\/\/www.kaggle.com\/abhinand05\/bert-for-humans-tutorial-baseline). It does an awesome job of explaining BERT.\n\nFirst I'll load the BERT backbone. If you're new to TPUs in Kaggle the strategy scope here relates to the TPU setup earlier in the notebook. As we load the model the strategy will replicate it across the eight GPU chips of the TPU board.","ea475f1f":"So let's have a look at what the tokeniser has done to the sentences. Let's pick one from the dataset and have a look at it in its textual form.","f1fe3ced":"And take a look at how many examples we now have.","1d400229":"The attention mask shows where the words are in the sentence (as each sentence was padded with zeros to make them all the same length). The ones represent words while the zeros are padding. The padding doesn't hold any meaningful information so this mask helps the model focus on only the words that contain meaning.","074c48e5":"And now let's have a look at the first few tokens of the sentence in its textual form.","5a0e4989":"Compile the model.","c5d8368e":"## Train, validation, test split\n\nNow split the training dataset into training anfd validation.","84a5dfe1":"### Merge data into one dataframe\n\nConcat the datasets together.","3041e9c8":"I've been playing around with Kaggles TPUs for a bit now but have only used them for computer vision tasks. This looks like a cool opportunity to try them out on a language task. Let's begin by importing the libraries.\n\nI looked at a number of notebooks to get started with transformers but this [one](https:\/\/www.kaggle.com\/xhlulu\/contradictory-watson-concise-keras-xlm-r-on-tpu) from xhlulu was the greatest source of inspiration for my notebook.","95c0c2ae":"To easily tweak the parameters of the model I have put them as globals at the top of the notebook.\n\nYou'll notice that the batch size needed to be multiplied by the number of replicas (8). This is simply to make sure each of the eight GPU chips in the TPU uses the specified batch size and not one eighth of that number.","722609cf":"So a sentence has been split into an array where each word is represented by a number index. The tokeniser even splits the words themselves up into sub words. The word \"formulating\" for example is split into the sub words \"formula\" and \"ting\". Let's have a look at some of the words that the tokeniser has a token for. The get vocab command can be used for this.\n\nJust a note that the token 0 represents \"\\<\\s\\>\" which represents the start of the sentence.","47da241a":"## Encode training data\n\nLike all Machine Learning models a language model works with numbers, not text. To prepare the sentences for training then they need to be tokenised. These tokens are number indexes that represent each of the words. Each model has it's own unique set of tokens. Let's get the tokeniser for this model.","efb6d817":"### Add another dataset\n\nThe nlp library has another dataset that could be added called the [Adversarial Natural Language Inference](https:\/\/huggingface.co\/datasets\/anli). Let's load it in the same way."}}