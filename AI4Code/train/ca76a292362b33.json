{"cell_type":{"80d6e0bb":"code","7ba2a87b":"code","aa6b656b":"code","cf24ef62":"code","db8dae0d":"code","ad204f9a":"code","40392f47":"code","61fe7f1d":"code","6992f138":"code","83c304e0":"code","2f59157d":"code","7d5cb0c7":"code","b989fa30":"code","f803cb02":"code","df311374":"code","fed75a5c":"code","771edc55":"code","b42e6661":"code","b36a2325":"code","6c0b2a61":"code","4b317161":"code","9d392687":"code","9dfb9e39":"code","9613960b":"code","36b0ae23":"code","ff4656dc":"code","2f540e5d":"code","7b94c4c4":"code","87d89a65":"code","84772006":"code","ff7cdf49":"code","c9acdc4c":"code","6b6d0da6":"code","bf5a6345":"code","f1b277e2":"code","82b6c093":"code","89cc5eaf":"code","f81fa9a7":"code","a42981df":"code","cb71fd05":"code","cca3eb1b":"code","2d1535a3":"code","2c64a6b4":"code","62cc39f6":"code","e34c5c06":"code","d70b3ab0":"code","11e38a4f":"code","ce07020c":"code","6e1f242d":"code","8fa716f0":"code","07d960d5":"code","40357d01":"code","bcac9bfb":"code","79fd6f92":"code","457f01f2":"code","968f445d":"code","333f51d6":"code","94e8924e":"code","0b913902":"markdown","9fff50fc":"markdown","539b7266":"markdown","fdc629f0":"markdown","a0b9c6b4":"markdown","f8bb0cb6":"markdown","056de5f9":"markdown","d7fc38e8":"markdown","9b5f4f3e":"markdown","8d5f7416":"markdown","5b5c701f":"markdown","e8a1ae45":"markdown","74aa1b5f":"markdown","3450793a":"markdown","9f5a4117":"markdown","d5c95fa5":"markdown","3da5c74a":"markdown","608ddc21":"markdown","e42d301f":"markdown","9933c902":"markdown","db5a330b":"markdown","ea44539a":"markdown","8f650fd6":"markdown","1d94981f":"markdown","f217bf40":"markdown"},"source":{"80d6e0bb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","7ba2a87b":"# Importing Libraries\n\n# Importing Mathematical Library\nimport numpy as np\n#Importing Data Manipulation Library\nimport pandas as pd\n\n# Importing Visualization Libraries\nimport matplotlib.pyplot as plt # Basic Visualization Library\nimport seaborn as sns # Statistical Visualization Library\n\n# Importing Machine Learning Libraries\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn import metrics\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom prettytable import PrettyTable\n\nfrom imblearn.over_sampling import SMOTE","aa6b656b":"data = pd.read_csv('..\/input\/DATA.csv')","cf24ef62":"data.head(20)","db8dae0d":"# Understanding the Dataset\ndata.info()","ad204f9a":"# Understanding information of all the Columns in the Dataset\ndata.info(verbose=True)","40392f47":"def null_function():\n    null_values = pd.DataFrame((data.isnull().sum()\/len(data.index)*100),columns=['Percent_Null'])\n    only_missing_variables = null_values[null_values['Percent_Null'] !=0 ]\n    return pd.DataFrame(only_missing_variables.sort_values(by='Percent_Null', ascending=False))","61fe7f1d":"pd.DataFrame(null_function())","6992f138":"\n# Finding the Total No. of Null Values in each Column:\npd.DataFrame(data.isnull().sum())[pd.DataFrame(data.isnull().sum())[0] != 0].sort_values(by=0, ascending=False).rename(columns={0:'Total No. of Null Values'})","83c304e0":"\n# Displaying the Columns with null values more than 30%\nnull_values = pd.DataFrame((data.isnull().sum()\/len(data.index)*100),columns=['Percent_Null'])\nonly_missing_variables = null_values[(null_values['Percent_Null'] !=0) & (null_values['Percent_Null'] >=30)]\npd.DataFrame(only_missing_variables.sort_values(by='Percent_Null', ascending=False))","2f59157d":"# Dropping the Columns:\ndata.drop(['Medical_History_10','Medical_History_32','Medical_History_24','Medical_History_15','Family_Hist_5','Family_Hist_3','Family_Hist_2','Insurance_History_5','Family_Hist_4'],axis=1,inplace=True)","7d5cb0c7":"null_function()","b989fa30":"np.dtype(data['Employment_Info_1'])","f803cb02":"data['Employment_Info_1'].nunique()","df311374":"data['Employment_Info_1'].value_counts().sort_values()","fed75a5c":"\nsns.boxplot(data['Employment_Info_1'])\n# Note: distplot will not excute if there are nan in the data.","771edc55":"data['Employment_Info_1'].describe()","b42e6661":"\n#Checking the median of 'Employment_Info_1':\ndata['Employment_Info_1'].median()","b36a2325":"\n#Imputing missing values:\ndata['Employment_Info_1'] = data['Employment_Info_1'].fillna(data['Employment_Info_1'].median())","6c0b2a61":"\n# Again Checking for Missing Values:\nnull_function()","4b317161":"np.dtype(data['Employment_Info_6'])","9d392687":"data['Employment_Info_6'].nunique()","9dfb9e39":"sns.boxplot(data['Employment_Info_6'])","9613960b":"\ndata['Employment_Info_6'].describe()","36b0ae23":"data['Employment_Info_6'] = data['Employment_Info_6'].fillna(data['Employment_Info_6'].median())","ff4656dc":"# Again Checking for Missing Values if any:\nnull_function()","2f540e5d":"np.dtype(data['Employment_Info_4'])","7b94c4c4":"data['Employment_Info_4'].nunique()","87d89a65":"sns.boxplot(data['Employment_Info_4'])","84772006":"data['Employment_Info_4'].median()","ff7cdf49":"data['Employment_Info_4'].value_counts()","c9acdc4c":"sns.boxplot(data['Medical_History_1'])","6b6d0da6":"data['Medical_History_1'].describe()","bf5a6345":"data['Medical_History_1'].nunique()","f1b277e2":"data['Medical_History_1'].value_counts()","82b6c093":"data['Medical_History_1'].mode()","89cc5eaf":"data['Medical_History_1'] = data['Medical_History_1'].fillna(data['Medical_History_1'].mode()[0])","f81fa9a7":"#Checking for Null Values:\nnull_function()","a42981df":"\ndata = data.drop(['Id'],axis=1)","cb71fd05":"\ndata.head()","cca3eb1b":"data['Product_Info_2_char'] = data.Product_Info_2.str[0]\ndata['Product_Info_2_num'] = data.Product_Info_2.str[1]","2d1535a3":"data['Product_Info_2_char'] = pd.factorize(data['Product_Info_2_char'])[0]\ndata['Product_Info_2_num'] = pd.factorize(data['Product_Info_2_num'])[0]","2c64a6b4":"\n# Dropping 'Product_Info_2' Column\ndata = data.drop('Product_Info_2',axis=1)","62cc39f6":"data.head()","e34c5c06":"data.fillna(0,inplace=True)","d70b3ab0":"\n#Checking the Shape of the Final Data:\ndata.shape","11e38a4f":"X = data.drop('Response',axis=1)\ny = data.Response","ce07020c":"# Splitting the data into Train and Test\nX_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.3,random_state=2)","6e1f242d":"\n\n\nreport= PrettyTable()\nreport.field_names=['Model name','Accuracy Score','Precision Score','Recall Score','F1_score','Cohen Kappa Score']\n\n\nclassifier=['LogisticRegression','KNN','DecisionTreeClassifier','RandomForestClassifier']\naccuracy=[]\nprecision=[]\nrecall=[]\nf1_score=[]\nkappa=[]\n\nfor i in classifier:\n    if i=='LogisticRegression':\n        model1=LogisticRegression()\n        model1.fit(X_train,y_train)\n        log_pred=pd.DataFrame(model1.predict(X_test))\n        #Evaluation metrics\n        report.add_row([i,\n                    metrics.accuracy_score(y_test,log_pred),\n                    metrics.precision_score(y_test,log_pred,average='weighted'),\n                    metrics.recall_score(y_test,log_pred,average='weighted'),\n                    metrics.f1_score(y_test,log_pred,average='weighted'),\n                    metrics.cohen_kappa_score(y_test,log_pred)])\n        \n    elif i=='KNN': \n        model2=KNeighborsClassifier()\n        model2.fit(X_train,y_train)\n        knn_pred=model2.predict(X_test)\n        #Evaluation metrics\n        report.add_row([i,\n                    metrics.accuracy_score(y_test,knn_pred),\n                    metrics.precision_score(y_test,knn_pred,average='weighted'),\n                    metrics.recall_score(y_test,knn_pred,average='weighted'),\n                    metrics.f1_score(y_test,knn_pred,average='weighted'),\n                    metrics.cohen_kappa_score(y_test,log_pred)])\n        \n    elif i=='DecisionTreeClassifier':\n        model3=DecisionTreeClassifier()\n        model3.fit(X_train,y_train)\n        dec_pred=model3.predict(X_test)\n        #Evaluation metrics\n        report.add_row([i,\n                    metrics.accuracy_score(y_test,dec_pred),\n                    metrics.precision_score(y_test,dec_pred,average='weighted'),\n                    metrics.recall_score(y_test,dec_pred,average='weighted'),\n                    metrics.f1_score(y_test,dec_pred,average='weighted'),\n                    metrics.cohen_kappa_score(y_test,log_pred)])\n        \n    elif i=='RandomForestClassifier':\n        model4=RandomForestClassifier()\n        model4.fit(X_train,y_train)\n        random_pred=model4.predict(X_test)\n        #Evaluation metrics\n        report.add_row([i,\n                    metrics.accuracy_score(y_test,random_pred),\n                    metrics.precision_score(y_test,random_pred,average='weighted'),\n                    metrics.recall_score(y_test,random_pred,average='weighted'),\n                    metrics.f1_score(y_test,random_pred,average='weighted'),\n                    metrics.cohen_kappa_score(y_test,log_pred)])\nprint(report)\n","8fa716f0":"## Code to find the best hyper parameters for the KNN and DecisionTree and Random forest\n\nfrom sklearn.model_selection import RandomizedSearchCV\n## code to find the best model for the dataset\n\n\nbest_par= PrettyTable()\nbest_par.field_names=['Model name','Best Parameters','Best Score']\n\n\nclassifier=['KNN','DecisionTreeClassifier','RandomForestClassifier']\n\n\nfor i in classifier:\n    if i=='KNN': \n        grid1={'n_neighbors': np.arange(1,50,2),'p': np.arange(1,50)}\n        ran_search1=RandomizedSearchCV(model2,grid1,cv=5)\n        ran_search1.fit(X_train,y_train)\n        best_par.add_row([i,\n                          ran_search1.best_params_,\n                          ran_search1.best_score_])\n        \n    elif i=='DecisionTreeClassifier':\n        grid2={'criterion':['gini','entropy'],'max_depth': np.arange(2,10),'max_leaf_nodes':np.arange(2,10),'min_samples_leaf':np.arange(2,10)}\n        ran_search2=RandomizedSearchCV(model3,grid2,cv=5)\n        ran_search2.fit(X_train,y_train)\n        best_par.add_row([i,\n                          ran_search2.best_params_,\n                          ran_search2.best_score_])\n        \n    elif i=='RandomForestClassifier':\n        grid3={'criterion':['gini','entropy'],'n_estimators':np.arange(1,100),'max_features':np.arange(1,10)}\n        ran_search3=RandomizedSearchCV(model4,grid3,cv=3)\n        ran_search3.fit(X_train,y_train)\n        best_par.add_row([i,\n                          ran_search3.best_params_,\n                          ran_search3.best_score_])\n        \nprint(best_par)","07d960d5":"# Since Random Forest Classifier outperformed, let's use it with parameter obtained and test it with unseen test data.\nmodel4=RandomForestClassifier(n_estimators=88,max_features=9,criterion='gini')\nmodel4.fit(X_train,y_train)\nrandom_pred=model4.predict(X_test)\n    #Evaluation metrics\nprint(\"Accuracy Score:\",metrics.accuracy_score(y_test,random_pred))\nprint(\"Precision Score:\",metrics.precision_score(y_test,random_pred,average='weighted'))\nprint(\"Recall Score:\",metrics.recall_score(y_test,random_pred,average='weighted'))\nprint(\"F1 Score:\",metrics.f1_score(y_test,random_pred,average='weighted'))\n\nprint(metrics.confusion_matrix(y_test,random_pred))\n\nprint(metrics.classification_report(y_test,random_pred))","40357d01":"\n#Checking the distribution of Target Variable:\nsns.countplot(data.Response)","bcac9bfb":"\n# SMOTE Sampling Technique:\nsmote = SMOTE()\nX_sm, y_sm = smote.fit_sample(X,y)","79fd6f92":"\nsns.countplot(y_sm)","457f01f2":"# Splitting the Balanced Data for finally modelling:\nX_sm_train, X_sm_test, y_sm_train, y_sm_test = train_test_split(X_sm,y_sm,test_size=0.3,random_state=2)","968f445d":"\n\n\nreport= PrettyTable()\nreport.field_names=['Model name','Accuracy Score','Precision Score','Recall Score','F1_score','Cohen Kappa Score']\n\n\nclassifier=['LogisticRegression','KNN','DecisionTreeClassifier','RandomForestClassifier']\naccuracy=[]\nprecision=[]\nrecall=[]\nf1_score=[]\nkappa=[]\n\nfor i in classifier:\n    if i=='LogisticRegression':\n        model1=LogisticRegression()\n        model1.fit(X_sm_train,y_sm_train)\n        log_pred=pd.DataFrame(model1.predict(X_sm_test))\n        #Evaluation metrics\n        report.add_row([i,\n                    metrics.accuracy_score(y_sm_test,log_pred),\n                    metrics.precision_score(y_sm_test,log_pred,average='weighted'),\n                    metrics.recall_score(y_sm_test,log_pred,average='weighted'),\n                    metrics.f1_score(y_sm_test,log_pred,average='weighted'),\n                    metrics.cohen_kappa_score(y_sm_test,log_pred)])\n        \n    elif i=='KNN': \n        model2=KNeighborsClassifier()\n        model2.fit(X_sm_train,y_sm_train)\n        knn_pred=model2.predict(X_sm_test)\n        #Evaluation metrics\n        report.add_row([i,\n                    metrics.accuracy_score(y_sm_test,knn_pred),\n                    metrics.precision_score(y_sm_test,knn_pred,average='weighted'),\n                    metrics.recall_score(y_sm_test,knn_pred,average='weighted'),\n                    metrics.f1_score(y_sm_test,knn_pred,average='weighted'),\n                    metrics.cohen_kappa_score(y_sm_test,log_pred)])\n        \n    elif i=='DecisionTreeClassifier':\n        model3=DecisionTreeClassifier()\n        model3.fit(X_sm_train,y_sm_train)\n        dec_pred=model3.predict(X_sm_test)\n        #Evaluation metrics\n        report.add_row([i,\n                    metrics.accuracy_score(y_sm_test,dec_pred),\n                    metrics.precision_score(y_sm_test,dec_pred,average='weighted'),\n                    metrics.recall_score(y_sm_test,dec_pred,average='weighted'),\n                    metrics.f1_score(y_sm_test,dec_pred,average='weighted'),\n                    metrics.cohen_kappa_score(y_sm_test,log_pred)])\n        \n    elif i=='RandomForestClassifier':\n        model4=RandomForestClassifier()\n        model4.fit(X_sm_train,y_sm_train)\n        random_pred=model4.predict(X_sm_test)\n        #Evaluation metrics\n        report.add_row([i,\n                    metrics.accuracy_score(y_sm_test,random_pred),\n                    metrics.precision_score(y_sm_test,random_pred,average='weighted'),\n                    metrics.recall_score(y_sm_test,random_pred,average='weighted'),\n                    metrics.f1_score(y_sm_test,random_pred,average='weighted'),\n                    metrics.cohen_kappa_score(y_sm_test,log_pred)])\nprint(report)","333f51d6":"\n\nfrom sklearn.model_selection import RandomizedSearchCV\n## code to find the best model for the dataset\n\n\nbest_par= PrettyTable()\nbest_par.field_names=['Model name','Best Parameters','Best Score']\n\n\nclassifier=['KNN','DecisionTreeClassifier','RandomForestClassifier']\n\n\nfor i in classifier:\n    if i=='KNN': \n        grid1={'n_neighbors': np.arange(1,50,2),'p': np.arange(1,50)}\n        ran_search1=RandomizedSearchCV(model2,grid1,cv=5)\n        ran_search1.fit(X_sm_train,y_sm_train)\n        best_par.add_row([i,\n                          ran_search1.best_params_,\n                          ran_search1.best_score_])\n        \n    elif i=='DecisionTreeClassifier':\n        grid2={'criterion':['gini','entropy'],'max_depth': np.arange(2,10),'max_leaf_nodes':np.arange(2,10),'min_samples_leaf':np.arange(2,10)}\n        ran_search2=RandomizedSearchCV(model3,grid2,cv=5)\n        ran_search2.fit(X_sm_train,y_sm_train)\n        best_par.add_row([i,\n                          ran_search2.best_params_,\n                          ran_search2.best_score_])\n        \n    elif i=='RandomForestClassifier':\n        grid3={'criterion':['gini','entropy'],'n_estimators':np.arange(1,100),'max_features':np.arange(1,10)}\n        ran_search3=RandomizedSearchCV(model4,grid3,cv=3)\n        ran_search3.fit(X_sm_train,y_sm_train)\n        best_par.add_row([i,\n                          ran_search3.best_params_,\n                          ran_search3.best_score_])\n        \nprint(best_par)\n","94e8924e":"# Since Random Forest Classifier outperformed, let's use it with parameter obtained and test it with unseen test data.\nmodel4=RandomForestClassifier(n_estimators=96,max_features=4,criterion='gini')\nmodel4.fit(X_sm_train,y_sm_train)\nrandom_pred=model4.predict(X_sm_test)\n    #Evaluation metrics\nprint(\"Accuracy Score:\",metrics.accuracy_score(y_sm_test,random_pred))\nprint(\"Precision Score:\",metrics.precision_score(y_sm_test,random_pred,average='weighted'))\nprint(\"Recall Score:\",metrics.recall_score(y_sm_test,random_pred,average='weighted'))\nprint(\"F1 Score:\",metrics.f1_score(y_sm_test,random_pred,average='weighted'),'\\n')\n\nprint(metrics.confusion_matrix(y_sm_test,random_pred),'\\n')\n\nprint(metrics.classification_report(y_sm_test,random_pred))","0b913902":"### Dealing with Missing Values:\u00b6\nDropping the columns which are having more the 30% of Missing Values\n\nColumns which are having more than 30% of missing values are:\n\n1. Medical_History_10 = 99.061990\n2. Medical_History_32 = 98.135767\n3. Medical_History_24 = 93.598963\n4. Medical_History_15 = 75.101463\n5. Family_Hist_5 = 70.411411\n6. Family_Hist_3 = 57.663226\n7. Family_Hist_2 = 48.257860\n8. Insurance_History_5 = 42.767889\n9. Family_Hist_4 = 32.306630","9fff50fc":"### From, the above figure we can infer that data is imbalanced and to make it Balanced we are used SMOTE Sampling Technique:**","539b7266":"# If you like my kernal give upvote","fdc629f0":"### From, the above figure we can infer that data is balanced.","a0b9c6b4":"### From the above table, we can see that there is 'Product_Info_2' Column which is still Categorical, and it has to be encoded.\n\n#### Since it is Nominal Categorical Variable, it is splited with characters into one column and numbers into another and finally each column is encoded.\n\n#### After encoding is done the original column 'Product_Info_2' is dropped.\n\n","f8bb0cb6":"\n## Working with 'Product_Info_2'","056de5f9":"### Dealing with 'Employment_Info_4' Column:","d7fc38e8":"From the above information, we can infer that 'Employment_Info_1' Variable is continuous and right skewed, so for missing value imputation is done using median.\n\n### Missing Value Imputation for 'Employment_Info_1'","9b5f4f3e":"# Modelling without SMOTE:","8d5f7416":"### Finally the Data is Cleaned and ready for modelling.","5b5c701f":"### Technique used to improve the Score : Sampling (SMOTE)","e8a1ae45":"### Modelling:\u00b6\n    Baseline Model\n    Base model without SMOTE Technique(with imbalanced data)\n    Modelling with Hyper-parameters and without SMOTE Technique(with imbalanced data)\n    Base model with SMOTE Technique(with balanced data)\n    Modelling with Hyper-parameters and with SMOTE Technique(with balanced data)\n    Classification Models used:\n\n        1. Logistic Regression\n        2. k Nearest Neighbors\n        3. Decision Tree\n        4. Random Forest","74aa1b5f":"# Modelling with SMOTE:","3450793a":"### Dividing the Independent Variables and Dependent Variable from the Dataset","9f5a4117":"From the above information, we can infer that there are no null values in the dataset.","d5c95fa5":"\n### Now, checking for the data distribution:","3da5c74a":"# Code to find the best hyper parameters for the KNN and DecisionTree and Random forest","608ddc21":"### Missing Value Imputations:\n    Checking for the columns having Missing Values, after dropping the Columns which are having missing values more than 30%.","e42d301f":"#### Dropping unnecessary Columns from the Dataset:\u00b6\n    Columns like 'Id' can be dropped which only a record identification number.","9933c902":"### Missing Value Imputation for 'Employment_Info_4'","db5a330b":"### Processing the Data further to improve the Score:\n","ea44539a":"We are left with four columns that are having missing values more than 0% and less than 30%\n\n### Dealing with 'Employment_Info_1' Column:","8f650fd6":"### Missing Value Imputation for 'Medical_History_1'","1d94981f":"### Dealing with 'Employment_Info_6' Column:","f217bf40":"From the above information, we can infer that 'Employment_Info_6' Variable is continuous and right skewed, so for missing value imputation is done using median.\n\n### Missing Value Imputation for 'Employment_Info_6'"}}