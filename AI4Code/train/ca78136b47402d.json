{"cell_type":{"0ea28cfc":"code","4e4cd304":"code","23e492a5":"code","4b7d30a3":"code","8a60e351":"code","f2598f3e":"code","1ac8dba4":"code","51a04ef8":"code","fa03248a":"code","81ae634c":"code","575432e4":"code","99d612d7":"code","3f442cbf":"code","4c96bcde":"code","4c964bd9":"code","765edcd5":"code","9baa56af":"code","cbd63241":"code","61449af4":"code","f2d83a01":"code","d0f1e896":"code","17bdd846":"code","2343d916":"code","835ae314":"code","a0566077":"code","27e4bc1d":"code","fa7471d2":"code","8b793399":"code","878ee9f5":"code","aff4c122":"code","9d658f20":"code","a633b10c":"code","7a64174f":"code","3394e645":"code","a046516d":"code","686c054c":"code","70a505b7":"code","ab23f0f7":"code","56dd189b":"code","913d50aa":"code","5c20e056":"code","4e73f4be":"code","8b825ae6":"code","622b8da8":"code","539dd4df":"code","197df75e":"code","1f234b5c":"code","5f63a828":"code","bb42b85f":"code","8996a08e":"code","ebbfd2e1":"code","392538e2":"code","444d8337":"code","07150145":"code","a6b83e02":"code","c7484d7d":"code","23495135":"code","ae5d251d":"code","7f4ef7d4":"code","e7772bab":"code","7284c7f6":"code","06358d39":"code","4a1cc111":"code","071f4baa":"code","ac6cec79":"code","f3b62688":"code","bffedc4c":"code","cb49cf8f":"code","8b22fc27":"markdown","dced9abc":"markdown","8085904e":"markdown","d422e48f":"markdown","5a381d22":"markdown","1b38ea15":"markdown","fa912ecd":"markdown","9aaef842":"markdown","52bd59f7":"markdown","28725e2e":"markdown","27e69096":"markdown","478a1418":"markdown","4fa5c50a":"markdown","b562b5ef":"markdown","a25f67ef":"markdown","c972149c":"markdown","04a3cf3c":"markdown","49b85846":"markdown","e1fa995f":"markdown","56c2b45a":"markdown","2207ed06":"markdown","ec256bf5":"markdown","a495038e":"markdown","4b91a4c0":"markdown","97f5e332":"markdown","5ca75bb0":"markdown","66806c44":"markdown","31d52382":"markdown","df347bce":"markdown","55e7f81f":"markdown","b3828534":"markdown","fe026990":"markdown","5fc3898e":"markdown","62bcbb92":"markdown","bc31340c":"markdown","bb23e4c6":"markdown","7f728eb3":"markdown","20b11443":"markdown","c67711ad":"markdown","7aee9878":"markdown","c8dd50d8":"markdown","a027029a":"markdown","3d107bee":"markdown","fbb4f1ec":"markdown","fa61af32":"markdown","0a490d0d":"markdown","27a07b6b":"markdown","0784a1fb":"markdown","150d83d8":"markdown","8fa8efcd":"markdown","5ff5c4e5":"markdown","162d2eb5":"markdown","b8369cf7":"markdown","97967025":"markdown"},"source":{"0ea28cfc":"try:\n    import zarr\nexcept  ModuleNotFoundError:\n    ! pip install zarr > \/dev\/null\n    ! pip install ipytree > \/dev\/null","4e4cd304":"import zarr\nimport pandas as pd, numpy as np\nimport itertools as it # I will be using the `itertools.chain` function\nfrom pathlib import Path # for better file\/path operations management","23e492a5":"z = zarr.zeros((10000, 10000), chunks=(1000, 1000), dtype='i4')\nz","4b7d30a3":"z[:] = 42","8a60e351":"z[0, :] = np.arange(10000)\nz[:, 0] = np.arange(10000)","f2598f3e":"z[0, 0]","1ac8dba4":"z[-1, -1]","51a04ef8":"z[0, :]","fa03248a":"z[:]","81ae634c":"z1 = zarr.open('data\/example.zarr', mode='w', shape=(10000, 10000), chunks=(1000, 1000), dtype='i4')\nz1","575432e4":"z1[:] = 42","99d612d7":"z1[0, :] = np.arange(10000)","3f442cbf":"z1[:, 0] = np.arange(10000)","4c96bcde":"z2 = zarr.open('data\/example.zarr', mode='r')","4c964bd9":"np.all(z1[:] == z2[:])","765edcd5":"a = np.arange(10)\nzarr.save('data\/example.zarr', a)\nzarr.load('data\/example.zarr')","9baa56af":"root = zarr.group()\nroot","cbd63241":"foo = root.create_group('foo')\nbar = foo.create_group('bar')","61449af4":"z1 = bar.zeros('baz', shape=(10000, 10000), chunks=(1000, 1000), dtype='i4')\nz1","f2d83a01":"root['foo']","d0f1e896":"root['foo\/bar']","17bdd846":"root['foo\/bar\/baz']","2343d916":"print(root.tree(expand=True))","835ae314":"root = zarr.group()\nroot.attrs['foo'] = 'bar'\nz = root.zeros('zzz', shape=(10000, 10000))\nz.attrs['baz'] = 42\nz.attrs['qux'] = [1, 4, 7, 12]\nsorted(root.attrs)","a0566077":"'foo' in root.attrs","27e4bc1d":"root.attrs['foo']","fa7471d2":"sorted(z.attrs)","8b793399":"z.attrs['baz']","878ee9f5":"z.attrs['qux']","aff4c122":"# Set your root path to data\nDATA_ROOT = Path(\"..\/input\/lyft-motion-prediction-autonomous-vehicles\")","9d658f20":"zl5 = zarr.open(DATA_ROOT.joinpath(\"scenes\/sample.zarr\").as_posix(), mode=\"r\")\nzl5","a633b10c":"zl5.info","7a64174f":"print(zl5.tree(expand=True))","3394e645":"zl5.scenes.info","a046516d":"zl5.scenes.dtype","686c054c":"def parse_scene(scene):\n    scene_dict = {\n            \"frame_index_interval_start\": scene[0][0],\n            \"frame_index_interval_end\": scene[0][1],\n            \"host\":  scene[1],\n            \"start_time\": scene[2],\n            \"end_time\": scene[3]\n        }\n    return scene_dict","70a505b7":"scene = zl5.scenes[0]\nscene","ab23f0f7":"parse_scene(scene)","56dd189b":"class BaseParser:\n    \"\"\"\n    A robust and fast interface to load l5kit data into  Pandas dataframes.\n\n    Parameters\n    ----------\n    chunk_size: int, default=1000\n        How many items do you want in a single slice. The larger the better;\n        as long as you have enough memory. Nevertheless, chunk sizes above `10_000` won't lead to\n        significant speed gain as the original zarr files was chunked at 10_000.\n\n    max_chunks: int, default=10\n        How many chunks do you want to read from memory.\n\n    root:\n        Zarr data root path\n\n    zarr_path:\n        relative path or key to the data.\n    \"\"\"\n    \n    field = \"scenes\"\n    dtypes = {}\n    \n    def __init__(self, start=0, end=None, chunk_size=1000, max_chunks=10, root=DATA_ROOT, zarr_path=\"scenes\/sample.zarr\"):\n        \n        self.start = start\n        self.end = end\n        self.chunk_size = chunk_size\n        self.max_chunks = max_chunks\n        \n\n        self.root = Path(root)\n        assert self.root.exists(), \"There is nothing at {}!\".format(self.root)\n        self.zarr_path = Path(zarr_path)\n        \n     \n    def parse(self):\n        raise NotImplementedError\n        \n    def to_pandas(self, start=0, end=None, chunk_size=None, max_chunks=None):\n        start = start or self.start\n        end = end or self.end\n        chunk_size = chunk_size or self.chunk_size\n        max_chunks = max_chunks or self.max_chunks\n        \n        if not chunk_size or  not max_chunks: # One shot load, suitable for small zarr files\n            df = zarr.load(self.root.joinpath(self.zarr_path).as_posix()).get(self.field)\n            df = df[start:end]\n            df = map(self.parse, df) \n        else: # Chunked load, suitable for large zarr files\n            df = []\n            with zarr.open(self.root.joinpath(self.zarr_path).as_posix(), \"r\") as zf:\n                end = start+max_chunks*chunk_size if end is None else min(end, start+max_chunks*chunk_size)\n                for i_start in range(start, end, chunk_size ):\n                    items = zf[self.field][i_start: min(i_start + chunk_size,end)]\n                    items = map(self.parse, items)\n                    df.append(items)\n            df = it.chain(*df)\n            \n        df = pd.DataFrame.from_records(df)\n        for col, col_dtype in self.dtypes.items():\n            df[col] = df[col].astype(col_dtype, copy=False)\n        return df","913d50aa":"class SceneParser(BaseParser):\n    field = \"scenes\"\n    \n    @staticmethod\n    def parse(scene):\n        scene_dict = {\n            \"frame_index_interval_start\": scene[0][0],\n            \"frame_index_interval_end\": scene[0][1],\n            \"host\":  scene[1],\n            \"start_time\": scene[2],\n            \"end_time\": scene[3]\n        }\n        return scene_dict","5c20e056":"sp = SceneParser(chunk_size=None, max_chunks=None, zarr_path=\"scenes\/sample.zarr\")","4e73f4be":"scenes = sp.to_pandas()\nscenes.shape","8b825ae6":"scenes.head()","622b8da8":"scenes[\"duration\"] = (scenes[\"end_time\"] -  scenes[\"start_time\"])\/1e9\nscenes[\"num_frames\"] = scenes[\"frame_index_interval_end\"] - scenes[\"frame_index_interval_start\"]","539dd4df":"scenes.describe()","197df75e":"scene = scenes.iloc[-1]\nscene","1f234b5c":"zl5.frames","5f63a828":"scene_frames = zl5.frames[scene.frame_index_interval_start:scene.frame_index_interval_end]\nframe = scene_frames[0]\nframe","bb42b85f":"def parse_frame(frame):\n    frame_dict = {\n        'timestamp': frame[0],\n        'agent_index_interval_start': frame[1][0],\n        'agent_index_interval_start': frame[1][1],\n        'traffic_light_faces_index_interval_start': frame[2][0],\n        'traffic_light_faces_index_interval_end': frame[2][1],\n        'ego_translation_x': frame[3][0],\n        'ego_translation_y': frame[3][1],\n        'ego_translation_z': frame[3][2],\n        'ego_rotation_xx': frame[4][0][0],\n        'ego_rotation_xy': frame[4][0][1],\n        'ego_rotation_xz': frame[4][0][2],\n        'ego_rotation_yx': frame[4][1][0],\n        'ego_rotation_yy': frame[4][1][1],\n        'ego_rotation_yz': frame[4][1][2],\n        'ego_rotation_zx': frame[4][2][0],\n        'ego_rotation_zy': frame[4][2][1],\n        'ego_rotation_zz': frame[4][2][2],\n        \n    }\n    return frame_dict","8996a08e":"parse_frame(frame)","ebbfd2e1":"class FrameParser(BaseParser):\n    field = \"frames\"\n    \n    @staticmethod\n    def parse(frame):\n        frame_dict = {\n            'timestamp': frame[0],\n            'agent_index_interval_start': frame[1][0],\n            'agent_index_interval_end': frame[1][1],\n            'traffic_light_faces_index_interval_start': frame[2][0],\n            'traffic_light_faces_index_interval_end': frame[2][1],\n            'ego_translation_x': frame[3][0],\n            'ego_translation_y': frame[3][1],\n            'ego_translation_z': frame[3][2],\n            'ego_rotation_xx': frame[4][0][0],\n            'ego_rotation_xy': frame[4][0][1],\n            'ego_rotation_xz': frame[4][0][2],\n            'ego_rotation_yx': frame[4][1][0],\n            'ego_rotation_yy': frame[4][1][1],\n            'ego_rotation_yz': frame[4][1][2],\n            'ego_rotation_zx': frame[4][2][0],\n            'ego_rotation_zy': frame[4][2][1],\n            'ego_rotation_zz': frame[4][2][2],\n\n        }\n        return frame_dict\n\n    def to_pandas(self, start=0, end=None, chunk_size=None, max_chunks=None, scene=None):\n        if scene is not None:\n            start = scene.frame_index_interval_start\n            end = scene.frame_index_interval_end\n        \n        df = super().to_pandas(start=start, end=end, chunk_size=chunk_size, max_chunks=max_chunks)\n        return df","392538e2":"fp = FrameParser()","444d8337":"scene","07150145":"frames = fp.to_pandas(scene=scene)\n# frames = fp.to_pandas(scene=None)\nframes.shape","a6b83e02":"frames.head()","c7484d7d":"frame = frames.iloc[0]\nframe","23495135":"zl5.agents","ae5d251d":"frame_agents = zl5.agents[int(frame.agent_index_interval_start):int(frame.agent_index_interval_end)]\nagent = frame_agents[0]\nagent","7f4ef7d4":"PERCEPTION_LABELS = [\n    \"PERCEPTION_LABEL_NOT_SET\",\n    \"PERCEPTION_LABEL_UNKNOWN\",\n    \"PERCEPTION_LABEL_DONTCARE\",\n    \"PERCEPTION_LABEL_CAR\",\n    \"PERCEPTION_LABEL_VAN\",\n    \"PERCEPTION_LABEL_TRAM\",\n    \"PERCEPTION_LABEL_BUS\",\n    \"PERCEPTION_LABEL_TRUCK\",\n    \"PERCEPTION_LABEL_EMERGENCY_VEHICLE\",\n    \"PERCEPTION_LABEL_OTHER_VEHICLE\",\n    \"PERCEPTION_LABEL_BICYCLE\",\n    \"PERCEPTION_LABEL_MOTORCYCLE\",\n    \"PERCEPTION_LABEL_CYCLIST\",\n    \"PERCEPTION_LABEL_MOTORCYCLIST\",\n    \"PERCEPTION_LABEL_PEDESTRIAN\",\n    \"PERCEPTION_LABEL_ANIMAL\",\n    \"AVRESEARCH_LABEL_DONTCARE\",\n]","e7772bab":"class AgentParser(BaseParser):\n    field = \"agents\"\n    \n    @staticmethod\n    def parse(agent):\n        frame_dict = {\n            'centroid_x': agent[0][0],\n            'centroid_y': agent[0][1],\n            'extent_x': agent[1][0],\n            'extent_y': agent[1][1],\n            'extent_z': agent[1][2],\n            'yaw': agent[2],\n            \"velocity_x\":  agent[3][0],\n            \"velocity_y\":  agent[3][1],\n            \"track_id\":  agent[4],\n        }\n        for p_label, p in zip(PERCEPTION_LABELS, agent[5]):\n            frame_dict[\"label_probabilities_{}\".format(p_label)] = p\n        return frame_dict\n\n    def to_pandas(self, start=0, end=None, chunk_size=None, max_chunks=None, frame=None):\n        if frame is not None:\n            start = int(frame.agent_index_interval_start)\n            end = int(frame.agent_index_interval_end)\n        \n        df = super().to_pandas(start=start, end=end, chunk_size=chunk_size, max_chunks=max_chunks)\n        return df","7284c7f6":"ap = AgentParser()","06358d39":"agents = ap.to_pandas(frame=frame)\n# agents = ap.to_pandas(frame=None)\nagents.shape","4a1cc111":"agents.head()","071f4baa":"zl5.traffic_light_faces","ac6cec79":"class TrafficLightParser(BaseParser):\n    field = \"traffic_light_faces\"\n    \n    @staticmethod\n    def parse(light):\n        frame_dict = {\n            'face_id': light[0],\n            'traffic_light_id': light[1],\n            'traffic_light_face_status_0': light[2][0],\n            'traffic_light_face_status_1': light[2][1],\n            'traffic_light_face_status_2': light[2][2],\n        }\n        return frame_dict\n\n    def to_pandas(self, start=0, end=None, chunk_size=None, max_chunks=None, frame=None):\n        if frame is not None:\n            start = int(frame.traffic_light_faces_index_interval_start)\n            end = int(frame.traffic_light_faces_index_interval_end)\n        \n        df = super().to_pandas(start=start, end=end, chunk_size=chunk_size, max_chunks=max_chunks)\n        return df","f3b62688":"tlp = TrafficLightParser()","bffedc4c":"lights = tlp.to_pandas(frame = frame)\n# lights = tlp.to_pandas(frame = None)\nlights.shape","cb49cf8f":"lights.head()","8b22fc27":"## 1.3 Persistent arrays","dced9abc":"As we said it, **scenes** are made of **frames**. Each scene holds a reference to its frames whicht starts at ***frame_index_interval_start*** and ends at ***frame_index_interval_end***.","8085904e":"**Nice one ! A scene is less ugly now !**. We can just iterate over all the scenes and got them into a pandas DataFrame where we could make deeper analysis and create more features to train a good model.","d422e48f":"Groups have a similar API to the Group class from h5py. For example, groups can contain other groups:","5a381d22":"# 2. The L5Kit dataset","1b38ea15":"## 2.4 The L5Kit dataset: agents","fa912ecd":"To achieve our goals, we will need some ingredients:\n* **Zarr** : as you may have guessed, it is the main package for handling **zarr** files\n* **Numpy** :  zarr files are built in front of **Numpy arrays**\n* **Pandas**:  our zarr files will be parsed into **Pandas DataFrames** for further analysis","9aaef842":"Let's get more info from the **scenes** :","52bd59f7":"In this notebook, we will discover together, step by step, what is a **zarr** file and as a use-case, we will apply our discoveries to the **L5kit zarr dataset**. Let's start right away !","28725e2e":"## 1.1 Creating an array","27e69096":"## 2.1 Introduction","478a1418":"The zarr.hierarchy.Group.tree() method can be used to print a tree representation of the hierarchy,","4fa5c50a":"## 2.2 The L5Kit dataset: scenes","b562b5ef":"### 2.2.1. What is a scene ?","a25f67ef":"Let's take a look into a **scene**'s `dtype`:","c972149c":">The array above will store its configuration metadata and all compressed chunk data in a directory called \u2018data\/example.zarr\u2019 relative to the current working directory. The zarr.convenience.open() function provides a convenient way to create a new persistent array or continue working with an existing array. Note that although the function is called \u201copen\u201d, there is no need to close an array: data are automatically flushed to disk, and files are automatically closed whenever an array is modified.","04a3cf3c":"The \u2018\/\u2019 character can be used to access multiple levels of the hierarchy in one call, e.g.:","49b85846":"## 1.4 Groups","e1fa995f":">When I saw this competition for the first time, I said to myself: great, I'm going to have a lot of fun :) . And very quickly, I signed up. But, in fact I hadn't seen the real data yet. The data, although it seems quite rich to me, is presented in a format that makes you want to go kill yourself rather than touch it (I'm exaggerating a bit of course).\n\n>The few public notebooks that try to approach this ogre, all use the same logic that totally prevents you from understanding the the data. Indeed, they all use the L5kit API which is a great API (I think it is optimized from an application point of view). But let's not hide it, this data format is far from the one we are used to caress with Pandas. So all is lost? Well no! This is what I will try to show in this work.","56c2b45a":"As for any **zarr** file, there must be a root folder. In our case, the root folder would likely look like **<...>\/lyft-motion-prediction-autonomous-vehicles**. I set mine in the **DATA_ROOT** global variable as below :","2207ed06":"In the examples above, compressed data for each chunk of the array was stored in main memory. Zarr arrays can also be stored on a file system, enabling persistence of data between sessions. For example:","ec256bf5":"# Zarr files and L5kit data for dummies","a495038e":"## 1.5 User attributes","4b91a4c0":"### 2.2.2 Parsing scenes into a Pandas DataFrame","97f5e332":"## 2.3 The L5Kit dataset: frames","5ca75bb0":"The `L5Kit data` is stored in **zarr** format which is basically a set of numpy structured arrays. Conceptually, it is similar to a set of CSV files with records and different columns as we've seen it above.","66806c44":"Zarr provides classes and functions for working with N-dimensional arrays that behave like NumPy arrays but whose data is divided into chunks and each chunk is compressed. If you are already familiar with HDF5 then Zarr arrays provide similar functionality, but with some additional flexibility.","31d52382":"An agent is actually an object which is in move with the host (AV).","df347bce":"Regions of the array can also be written to, e.g.:","55e7f81f":"<h5 style=\"color:blue;text-align:center;\">Please upvote the kernel if you find it useful. You'll motivate me to go through the junky documentations in order to make this competition Great Again :) !<\/h5>","b3828534":"Zarr arrays and groups support custom key\/value attributes, which can be useful for storing application-specific metadata. For example:","fe026990":"Instead of making a simple function that naively iterates over the scenes, I will expose a robust interface that takes into account the fact that accessing (indexing) a **zarr** file is a somehow **expensive operation**  as **Zarr** needs to unpack the compressed chunk before taking the right index. In place and lieu of taking a single index, I will take a range (slice) in order to make **Zarr** faster.","5fc3898e":"> The dataset name is obivously equal to \"**\/**\" as we're in the root folder. The has 5 memebers, namely:\n* **Scenes** :  a collection of frames\n* **Frames** :  a collection of agents (the host agents + other agents)\n* **Agents** : Any object in circulation with the automatic vehicle (AV)\n* **Traffic_light_faces** : traffic lights and their faces (bulbs)","62bcbb92":"We can also see the dataset's **tree** by doing :","bc31340c":"The contents of the array can be retrieved by slicing, which will load the requested region into memory as a NumPy array, e.g.:","bb23e4c6":"Zarr supports hierarchical organization of arrays via groups. As with arrays, groups can be stored in memory, on disk, or via other storage systems that support a similar interface.\n\nTo create a group, use the zarr.group() function:","7f728eb3":"The code above creates a 2-dimensional array of 32-bit integers with 10000 rows and 10000 columns, divided into chunks where each chunk has 1000 rows and 1000 columns (and so there will be 100 chunks in total).","20b11443":"So, the **L5Kit** data consists of groups of zarr datasets. We take a look by doing:","c67711ad":"So there are:\n * 100 **scenes** in the sample dataset\n * 24838 **frames**\n * 1893736 **agents**\n * 316008 **traffic_light_faces**","7aee9878":"Members of a group can be accessed via the suffix notation, e.g.:","c8dd50d8":"Groups can also contain arrays, e.g.:","a027029a":"A **frame** consists of :\n* ***timestamp***: the timesatamp at which the state of the worl was filmed\n* ***agents***: the agent detected by the host (just a reference)\n* ***traffic lights***\n* ***informations about the host***: translation, rotation","3d107bee":"**Here is the for our Zarr tutorial. For more ressources, you can visit [the official documentation](https:\/\/zarr.readthedocs.io\/en\/stable\/tutorial.html#creating-an-array) where I took most of the above examples.**","fbb4f1ec":"<h2 style=\"color:blue;text-align:center;\">Kkiller<\/h2>","fa61af32":"Check that the data have been written and can be read again:","0a490d0d":"If you are just looking for a fast and convenient way to save NumPy arrays to disk then load back into memory later, the functions zarr.convenience.save() and zarr.convenience.load() may be useful. E.g.:","27a07b6b":"Zarr arrays support a similar interface to NumPy arrays for reading and writing data. For example, the entire array can be filled with a scalar value:","0784a1fb":"## 1.2 Reading and writing data","150d83d8":"**I will make a small function which take in a scene and outputs those components as a `dict`.**","8fa8efcd":"So, a **scene**  consists of  3 block of things :\n* **Frames** : a scene has a list of frames that start from ***scene.frame_index_interval\\[0\\]*** and ends at ***scene.frame_index_interval\\[1\\]***\n* **Host** : a scene has a ***host*** which is the AV that films the scene.\n* **Timestamps**: a scene has a ***start_time***  and an ***end_time***","5ff5c4e5":"Persistent arrays support the same interface for reading and writing data, e.g.:","162d2eb5":"Zarr has several functions for creating arrays. For example:","b8369cf7":"## 2.5 The L5Kit dataset: traffic lights","97967025":"# 1. Off-topic zarr tutorial for dummies\n> This part is off-topic :), but not the less. Here, I will make a general introduction to zarr files. If you can't wait to get down to business, feel free to jump tho #part2 ."}}