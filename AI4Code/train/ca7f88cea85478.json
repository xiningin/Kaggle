{"cell_type":{"a698afcf":"code","be08c16b":"code","6b697af8":"code","ff21bfab":"code","aa2f5228":"code","c61bce13":"code","2f5b6e36":"code","a775dcab":"code","64a2611f":"code","f12677c1":"code","b86b844d":"code","70f6aa99":"code","d73a6981":"code","1588913e":"code","ea3210f3":"code","d91192de":"code","5aaee112":"code","94b2a5ec":"code","62b58cb3":"code","67c0f673":"code","0841acff":"code","c95af696":"code","2afdc647":"code","c6f766b0":"markdown","441e3af6":"markdown","2beb8586":"markdown","a053175b":"markdown","51b05c6c":"markdown","21d3bf4e":"markdown","07138151":"markdown","dcfbe5b7":"markdown","333a44a3":"markdown"},"source":{"a698afcf":"!pip install numpyro","be08c16b":"import pandas as pd\nimport numpy as np\nimport arviz as az\n\nimport numpyro\nimport numpyro.distributions as dist\nfrom numpyro.infer.reparam import LocScaleReparam\nfrom numpyro.handlers import reparam\nfrom numpyro.distributions.transforms import AffineTransform\nfrom numpyro.infer import MCMC, NUTS, Predictive\n\nimport jax\nfrom jax import random\nimport jax.numpy as jnp\n\n# plotting\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nimport seaborn as sns; sns.set()\nimport matplotlib.pyplot as plt\nimport arviz as az\naz.style.use(\"arviz-darkgrid\")\n\nnumpyro.set_host_device_count(4)","6b697af8":"all_free_throws = pd.read_csv(\"\/kaggle\/input\/nba-free-throws\/free_throws.csv\")\nall_free_throws = all_free_throws[all_free_throws.playoffs == \"regular\"] # regular season only\n\nfree_throws = all_free_throws[all_free_throws.season == \"2015 - 2016\"].reset_index(drop=True)\n\nfirst_half = free_throws[free_throws.game_id < np.quantile(free_throws.game_id, 0.25)].copy()\nprint(\"Number of games in first quarter of season: \", len(first_half.game_id.unique()))\nsecond_half = free_throws[free_throws.game_id >= np.quantile(free_throws.game_id, 0.25)].copy()\nprint(\"Number of games in rest of season: \", len(second_half.game_id.unique()))","ff21bfab":"# Use first half of the season and estimate free throw % for for the top 15 players\nnum_players = 16\ntop_player_names = list(first_half.groupby('player')\\\n                         .sum(['shot_made'])\\\n                         .sort_values(\"shot_made\", ascending = False)[:25].sample(num_players, random_state = 2).index)\n\ntop_player_data = first_half[first_half.player.isin(top_player_names)]\\\n                    .groupby(\"player\")[\"shot_made\"].agg({'count', 'sum'}).reset_index()\ntop_player_data.columns = [\"player\", \"shots_made\", \"total_shots\"]\ntop_player_data[\"free_throw_percentage\"] = top_player_data.shots_made \/ top_player_data.total_shots\n\ntest_data = second_half[second_half.player.isin(top_player_names)]\\\n                    .groupby(\"player\")[\"shot_made\"].agg({'count', 'sum'}).reset_index()\ntest_data.columns = [\"player\", \"shots_made\", \"total_shots\"]\ntest_data[\"free_throw_percentage\"] = test_data.shots_made \/ test_data.total_shots\n\ntop_player_data.sort_values(\"free_throw_percentage\", ascending = False)","aa2f5228":"def run_inference(model, ft_attempts, ft_makes, rng_key,\n                  num_chains = 2,\n                  num_warmup = 50, \n                  num_samples = 200):\n    kernel = NUTS(model)\n    mcmc = MCMC(\n        kernel,\n        num_warmup=num_warmup,\n        num_samples=num_samples,\n        num_chains=num_chains,\n        progress_bar=False\n    )\n    mcmc.run(rng_key, ft_attempts, ft_makes)\n    return mcmc","c61bce13":"def fully_pooled(ft_attempts, ft_makes=None):\n    r\"\"\"\n    Number of hits in $K$ at bats for each player has a Binomial\n    distribution with a common probability of success, $\\phi$.\n\n    :param (jnp.DeviceArray) at_bats: Number of at bats for each player.\n    :param (jnp.DeviceArray) hits: Number of hits for the given at bats.\n    :return: Number of hits predicted by the model.\n    \"\"\"\n    num_players = ft_attempts.shape[0]\n    alpha = numpyro.sample(\"alpha\", dist.Normal(1, 1))\n    theta = numpyro.deterministic(\"theta\", jax.nn.sigmoid(alpha))\n    with numpyro.plate(\"num_players\", num_players):\n        numpyro.sample(\"obs\", dist.BinomialLogits(total_count = ft_attempts, logits=alpha), \n                       obs=ft_makes)","2f5b6e36":"rng_key, rng_key_predict = random.split(random.PRNGKey(1))\n\npooled_mcmc = run_inference(fully_pooled, \n                            ft_attempts = jnp.array(top_player_data.total_shots),\n                            ft_makes = jnp.array(top_player_data.shots_made),\n                            num_warmup = 500,\n                            num_samples = 1000,\n                            rng_key = rng_key)\npooled_samples = pooled_mcmc.get_samples()\n\npooled_posterior_predictive = Predictive(fully_pooled, pooled_samples)(\n    random.PRNGKey(1), jnp.array(top_player_data.total_shots))\n\npooled_prior = Predictive(fully_pooled, num_samples=500)(\n    random.PRNGKey(2), jnp.array(top_player_data.total_shots))\n\npooled_az = az.from_numpyro(\n    pooled_mcmc,\n    prior = pooled_prior,\n    posterior_predictive = pooled_posterior_predictive,\n)\n\npooled_mcmc.print_summary()","a775dcab":"az.plot_density(\n    pooled_az,\n    var_names=[\"theta\"],\n    shade=0.1,\n    figsize = (6, 3),\n)\nplt.show()","64a2611f":"print(\"Number of players with FT percentage higher than 83.5 percent: %i\" % sum(top_player_data.free_throw_percentage > 0.835))\n\nprint(\"Number of players with FT percentage lower than 81.5 percent: %i\" % sum(top_player_data.free_throw_percentage < 0.815))","f12677c1":"def no_pooling (ft_attempts, ft_makes = None):\n    \"\"\"\n    Number of hits in $K$ bats modelled without sharing parameters\n    :param at_bats Number of at_bats for a player\n    :param hits Number of hits for a given player\n    \"\"\"\n    num_players = ft_attempts.shape[0]\n    with numpyro.plate(\"players\", num_players):\n        alpha = numpyro.sample(\"alpha\", dist.Normal(1, 1)) # prior\n        assert alpha.shape == (num_players,), \"alpha shape wrong\"\n        theta = numpyro.deterministic(\"theta\", jax.nn.sigmoid(alpha))\n        return numpyro.sample(\"obs\", dist.BinomialLogits(total_count=ft_attempts, logits=alpha), \n                              obs = ft_makes) # likelihood","b86b844d":"rng_key, rng_key_predict = random.split(random.PRNGKey(1))\n\nnon_pooled_mcmc = run_inference(no_pooling, \n                                ft_attempts = jnp.array(top_player_data.total_shots),\n                                ft_makes = jnp.array(top_player_data.shots_made),\n                                num_warmup = 500,\n                                num_samples = 1000,\n                                num_chains = 4,\n                                rng_key = rng_key)\nnon_pooled_samples = non_pooled_mcmc.get_samples()\n\nnon_pooled_posterior_predictive = Predictive(no_pooling, non_pooled_samples)(\n    random.PRNGKey(1), jnp.array(top_player_data.total_shots))\n\nnon_pooled_prior = Predictive(no_pooling, num_samples=500)(\n    random.PRNGKey(2), jnp.array(top_player_data.total_shots))\n\nnon_pooled_az = az.from_numpyro(\n    non_pooled_mcmc,\n    prior = non_pooled_prior,\n    posterior_predictive = non_pooled_posterior_predictive,\n    coords = {\"theta_dim_0\": top_player_data.player}\n)\n\nnon_pooled_mcmc.print_summary()","70f6aa99":"az.plot_density(\n    data = non_pooled_az,\n    var_names=[\"theta\"],\n    shade=0.1,\n    grid = (4, 4)\n)\nplt.show()","d73a6981":"def partial_pooling (ft_attempts, ft_makes = None):\n    \"\"\"\n    Number of hits in $K$ bats modelled by partially sharing parameters\n    :param at_bats Number of at_bats for a player\n    :param hits Number of hits for a given player\n    \"\"\"\n    num_players = ft_attempts.shape[0]\n    mu = numpyro.sample(\"mu\", dist.Normal(1, 1))\n    sigma = numpyro.sample(\"sigma\", dist.Normal(0, 1))\n    with numpyro.plate(\"players\", num_players):\n        alpha = numpyro.sample(\"alpha\", dist.Normal(mu, sigma))\n        theta = numpyro.deterministic(\"theta\", jax.nn.sigmoid(alpha))\n        assert alpha.shape == (num_players, ), \"alpha shape wrong\"\n        return numpyro.sample(\"y\", dist.BinomialLogits(logits = alpha, total_count = ft_attempts), \n                              obs = ft_makes)","1588913e":"rng_key, rng_key_predict = random.split(random.PRNGKey(1))\n\npartial_pooled_mcmc = run_inference(partial_pooling, \n                                    ft_attempts = jnp.array(top_player_data.total_shots),\n                                    ft_makes = jnp.array(top_player_data.shots_made),\n                                    num_warmup = 500,\n                                    num_samples = 1000,\n                                    num_chains = 4,\n                                    rng_key = rng_key)\npartial_pooled_samples = partial_pooled_mcmc.get_samples()\n\npartial_pooled_posterior_predictive = Predictive(partial_pooling, partial_pooled_samples)(\n    random.PRNGKey(1), jnp.array(top_player_data.total_shots))\n\npartial_pooled_prior = Predictive(partial_pooling, num_samples=500)(\n    random.PRNGKey(2), jnp.array(top_player_data.total_shots))\n\npartial_pooled_az = az.from_numpyro(\n    partial_pooled_mcmc,\n    prior = partial_pooled_prior,\n    posterior_predictive = partial_pooled_posterior_predictive,\n    coords = {\"theta_dim_0\": top_player_data.player}\n)\n\npartial_pooled_mcmc.print_summary()","ea3210f3":"az.plot_density(\n    data = [partial_pooled_az, non_pooled_az],\n    data_labels = [\"partially pooled\", \"no pooling\"],\n    var_names=[\"theta\"],\n    shade=0.1,\n    grid = (4, 4)\n)\nplt.show()","d91192de":"# compare p(\\alpha) for no-pooling and partially pooled models\nfig, ax = plt.subplots(nrows=1, ncols=3, figsize=(14, 4), sharey = True)\nfig.suptitle(r'p($\\alpha$) for Non-Pooled and Partially Pooled Models')\n\nsns.histplot(partial_pooled_prior[\"alpha\"].reshape(500*num_players,),\n             ax = ax[0],\n             color = \"red\",\n             stat=\"density\", common_norm=False,\n             label = \"partially pooled\")\nsns.histplot(non_pooled_prior[\"alpha\"].reshape(500*num_players,), \n             stat=\"density\", common_norm=False,\n             ax = ax[0],\n             label = \"non-pooled\")\nax[0].legend(fontsize = \"small\")\n\nsns.histplot(partial_pooled_prior[\"alpha\"].reshape(500*num_players,),\n             ax = ax[1],\n             stat=\"density\", common_norm=False,\n             color = \"red\",\n             label = \"partially pooled\")\nsns.histplot(np.random.normal(1, 1, 1000),\n             ax = ax[1],\n             stat=\"density\", common_norm=False,\n             label = \"N(1, 1)\")\nax[1].legend(fontsize = \"small\");\n\nsns.histplot(non_pooled_prior[\"alpha\"].reshape(500*num_players,), \n             color = \"red\",\n             stat=\"density\", common_norm=False,\n             ax = ax[2],\n             label = \"non-pooled\")\nsns.histplot(np.random.normal(1, 1, 500),\n             ax = ax[2],\n             stat=\"density\", common_norm=False,\n             label = \"N(1, 1)\")\nax[2].legend(fontsize = \"small\");","5aaee112":"\"\"\"\nCompare \\theta in the complete pooling model and \\mu in the partial pooling model. Based on our interpretation they should be very similar.\n\n\\theta = sigmoid(mu) ?\n\"\"\"\n\nsns.histplot(jax.nn.sigmoid(partial_pooled_samples[\"mu\"]), \n             color = \"red\", \n             label = r\"$\\mu$ mean: %.3f\" % jax.nn.sigmoid(np.mean(partial_pooled_samples[\"mu\"])),\n             stat = \"density\")\nsns.histplot(pooled_samples[\"theta\"],\n             label = r\"$\\theta_{pooled}$ mean: %.3f\" % np.mean(pooled_samples[\"theta\"]),\n             stat = \"density\")\\\n.set_title(r\"Posterior distributions for $\\mu$ and $\\theta_{pooled}$\")\nplt.legend(fontsize = \"small\");","94b2a5ec":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 5), sharey = True)\n\nsns.scatterplot(y = np.log(partial_pooled_samples[\"sigma\"]), \n                x = partial_pooled_samples[\"theta\"][:, 0],\n                alpha = 0.2, ax = ax[0]\n               ).set(title = \"Centered Hierarchical Model Funnel Plot\",\n                     xlabel = r\"$\\theta_0$\",\n                     ylabel = \"SD for chance of success, sigma\")\n\nsns.scatterplot(y = np.log(partial_pooled_samples[\"sigma\"]), \n                x = partial_pooled_samples[\"theta\"][:, 1],\n                alpha = 0.2, ax = ax[1]\n               ).set(title = \"Centered Hierarchical Model Funnel Plot\",\n                     xlabel = r\"$\\theta_1$\",\n                     ylabel = \"SD for chance of success, sigma\");","62b58cb3":"posterior_predictions = Predictive(partial_pooling, partial_pooled_samples)(\n    random.PRNGKey(1), jnp.array(test_data.total_shots))\n\ntest_data_predictions = test_data.copy()\ntest_data_predictions[\"FT_prediction_mean\"] = jnp.apply_along_axis(jnp.mean, 0, posterior_predictions[\"theta\"])\ntest_data_predictions[\"FT_prediction_median\"] = jnp.apply_along_axis(jnp.median, 0, posterior_predictions[\"theta\"])\ntest_data_predictions","67c0f673":"posterior_predictions_no_pooling = Predictive(no_pooling, non_pooled_samples)(\n    random.PRNGKey(1), jnp.array(test_data.total_shots))\n\ntest_data_predictions_no_pool = test_data.copy()\ntest_data_predictions_no_pool[\"FT_prediction_mean\"] = jnp.apply_along_axis(jnp.mean, 0, posterior_predictions_no_pooling[\"theta\"])\ntest_data_predictions_no_pool[\"FT_prediction_median\"] = jnp.apply_along_axis(jnp.median, 0, posterior_predictions_no_pooling[\"theta\"])\ntest_data_predictions_no_pool","0841acff":"rng_key, rng_key_predict = random.split(random.PRNGKey(1))\n\nreparam_model = reparam(partial_pooling, config={'alpha': LocScaleReparam(0)})\nreparam_mcmc = run_inference(reparam_model, \n                             ft_attempts = jnp.array(top_player_data.total_shots),\n                             ft_makes = jnp.array(top_player_data.shots_made),\n                             num_warmup = 500,\n                             num_samples = 1000,\n                             num_chains = 4,\n                             rng_key = rng_key)\nreparam_samples = reparam_mcmc.get_samples()\n\nreparam_posterior_predictive = Predictive(reparam_model, reparam_samples)(\n    random.PRNGKey(1), jnp.array(top_player_data.total_shots))\n\nreparam_prior = Predictive(reparam_model, num_samples=500)(\n    random.PRNGKey(2), jnp.array(top_player_data.total_shots))\n\nreparam_az = az.from_numpyro(\n    reparam_mcmc,\n    prior = reparam_prior,\n    posterior_predictive = reparam_posterior_predictive,\n    coords = {\"theta_dim_0\": top_player_data.player}\n)\n\nreparam_mcmc.print_summary()","c95af696":"az.plot_density(\n    data = [partial_pooled_az, reparam_az],\n    data_labels = [\"partially pooled\", \"reparam\"],\n    var_names=[\"theta\"],\n    shade=0.1,\n    grid = (4, 4)\n)\nplt.show()","2afdc647":"fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 5), sharey = True)\n\nsns.scatterplot(y = np.log(reparam_samples[\"sigma\"]), \n                x = reparam_samples[\"theta\"][:, 0],\n                alpha = 0.2, ax = ax[0]\n               ).set(title = \"Non-Centered Hierarchical Model Funnel Plot\",\n                     xlabel = r\"\\theta_0\",\n                     ylabel = \"SD for chance of success, sigma\")\n\nsns.scatterplot(y = np.log(reparam_samples[\"sigma\"]), \n                x = reparam_samples[\"theta\"][:, 1],\n                alpha = 0.2, ax = ax[1]\n               ).set(title = \"Non-Centered Hierarchical Model Funnel Plot\",\n                     xlabel = r\"\\theta_1\",\n                     ylabel = \"SD for chance of success, sigma\");","c6f766b0":"## Hierarchical Model - Partial Pooling\n\nWe ideally want a balance between these two extremes, and this comes in the form of a partially pooled model. This model has a very subtle but important difference to the `no pooling` model. The difference is in how we generate $\\alpha_i$. Instead of sampling $\\alpha_i$ directly from $N(-1, 1)$, we estimate the mean, $\\mu$, and standard deviation, $\\sigma$, of $p(\\alpha_i)$ using hyper-priors. Here, $\\mu$ can be interpreted as the population chance of success. \n\n$$ p(y_i \\mid K_i, \\theta_i) = \\text{Binomial}(K_i, \\theta) $$\n\n$$ \\alpha = \\text{logit}(\\theta) = \\text{log}\\frac{\\theta}{1 - \\theta}$$\n\n$$ p(y_i \\mid K_i, \\alpha_i) = \\text{BinomialLogit}(K_i, \\alpha) $$\n\n$$ p(\\alpha_i \\mid \\mu, \\sigma) = \\text{Normal}(\\mu, \\sigma) $$\n\n$$ p(\\mu) = N(1, 1) $$\n\n$$ p(\\sigma) = N(0, 1) $$\n\nPosterior:\n\n$$ p(\\alpha_i, \\mu, \\sigma \\mid y_i, K_i) = p(y_i \\mid K_i, \\alpha_i) p(\\alpha_i \\mid \\mu, \\sigma) p(\\mu) p(\\sigma)$$","441e3af6":"## Complete Pooling\n\nIn complete pooling we assume that each player has the same probability of batting success, $\\theta$. Assuming that each player's at-bat are independent Bernoulli trials, we can model the number of hits for player $i$, $y_i$:\n\n$$ p(y_i \\mid \\theta) = \\text{Binomial}(K_i, \\theta) $$\n\n$$ \\alpha = \\text{logit}(\\theta) $$\n\n$$ p(y \\mid \\theta) =  \\prod_{i = 1}^N \\text{BinomialLogit}(K_i, \\alpha) $$\n\n$$ p(\\alpha) $$","2beb8586":"## Posterior Predictive Distribution and Predictions","a053175b":"### Where does the difference come from?\n\nThe partially pooled and non-pooled models have very similar formulations, but produce very different posterior distributions. The most obvious difference for me is the prior on $\\alpha_i$. The partial pooling formulation has more flexibility here as both $\\mu$ an $\\sigma$ are estimated from the data. Below I compare $p(\\alpha)$ for the partially pooled and non-pooled models and it seems like the partially pooled prior has more variance than the non-pooled model.\n\nNow I'm interested to see what the impact of flatter priors would have on the model. After increasing the prior variance for the non-pooled model, interval estimates were too wide to be useful, this is because we have such small data on each player. On the other hand, the interval estimates produced by the hierarchical model were very similar to before, this is because the hyperpriors are estimated using population data, which we have more of because of pooling. ","51b05c6c":"## Centered and Non-Centered Models","21d3bf4e":"## Partial Pooling - Non-Centered Paramaterization\n\n- What is the problem with the centered parameterization?\n- How does this new formulation solve the problem?\n- Write up non-centered model from scratch and compare it to `reparam`\n- Why are the posteriors for the non-centered formulation smoother?\n- Why does the `effective_sample_size` change for the non-centered and centered formulations?","07138151":"### Inference Functions","dcfbe5b7":"## No Pooling\n\nIn the complete pooling example, all observations shared the same `chance of success` parameter. The no pooling model is the exact opposite, where there is no sharing of parameters between observations. Again, we'll model the log-odds of the chance of success with $\\alpha = \\text{logit}(\\theta)$\n\n$$ p(y_i \\mid \\theta_i) = \\text{Binomial}(K_i, \\theta_i) $$\n\n$$ \\alpha = \\text{logit}(\\theta) $$\n\n$$ p(y_i \\mid \\theta_i) = \\text{BinomialLogit}(K_i, \\alpha_i) $$\n\n$$ p(\\alpha_i) = \\text{Normal}(1, 1) $$\n\nThis prior specification for $\\alpha$ corresponds to $95\\%$ of values between $0.26$ and $0.95$ chance of success. ","333a44a3":"# Pooling in Hierarchical Models\n\nCode for [Estimating NBA Free Throw % with Hierarchical Models](https:\/\/jramkiss.github.io\/2021\/01\/29\/hierarchical-models\/)"}}