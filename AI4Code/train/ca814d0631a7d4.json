{"cell_type":{"d6d3beac":"code","f61a92eb":"code","754cf07c":"code","4c34d27d":"code","6b375662":"code","92c53cff":"code","855c8f80":"code","ae55a711":"code","5dfe9cb0":"code","99307d6f":"code","99369ea7":"code","1172ab3a":"code","30379166":"code","821c68da":"code","6c07ad8e":"code","76a6cbb7":"code","b036d706":"code","029817c9":"code","baf479ac":"code","4237b29d":"code","139c65ec":"code","258c9ecb":"code","e9615ff5":"code","60c19c6e":"code","87bd0e50":"code","ca015f5b":"code","711409c0":"code","4a3bbda7":"code","e7c120e0":"code","9a86f204":"code","de2f4529":"code","9409a3d7":"code","8d21d168":"code","1e3ae8b3":"code","7c691db7":"code","1ebc0b9e":"code","94bfafb6":"code","7478cd10":"code","21568a1f":"code","0adf4853":"code","f34e0a8f":"code","076bcfb3":"code","223cc4f9":"code","c3ddf636":"code","f3994e84":"code","d08341e5":"code","fe116a88":"code","658701ca":"code","2ed21b64":"code","a1ebf209":"markdown","22a86dd9":"markdown","2d18909a":"markdown","83145204":"markdown","e0f64a77":"markdown","cd37b07f":"markdown","382b8733":"markdown","56771132":"markdown","132b80f2":"markdown","10d454f3":"markdown","a1b6d2cc":"markdown","4a134a11":"markdown","9f7b0ae3":"markdown","cd5d8515":"markdown","1a469ff1":"markdown","d3769207":"markdown","a2ae22c9":"markdown","b6e5e50b":"markdown","71c93b8c":"markdown","d14752cf":"markdown","1b5b0e1d":"markdown","c83c0a67":"markdown","daeb25a6":"markdown","9d1bb9e4":"markdown","dea98ee8":"markdown","4c9072e0":"markdown","8ae07dc0":"markdown","4ea61449":"markdown","04d0acd9":"markdown"},"source":{"d6d3beac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom matplotlib import pyplot as plt\nimport keras as keras\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f61a92eb":"\nfrom matplotlib import pyplot as plt\nimport cv2\ndata_dir = \"..\/input\/cell-images-for-detecting-malaria\/cell_images\/\"\nparasite = os.listdir('..\/input\/cell-images-for-detecting-malaria\/cell_images\/Parasitized\/') \nuninfected = os.listdir('..\/input\/cell-images-for-detecting-malaria\/cell_images\/Uninfected\/')\nparasite.remove(\"Thumbs.db\")               #databse file in both folders\nuninfected.remove(\"Thumbs.db\")\n      ","754cf07c":"from numpy import expand_dims\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.preprocessing.image import ImageDataGenerator\n\ndf_para, label_para = [], [] \n\n\n\nfor filename in parasite:\n    img = load_img(data_dir + \"Parasitized\/\" + filename, target_size = (50, 50)) #Images resized to 50x50x3\n    i = img_to_array(img)\n    samples = expand_dims(i, 0)\n    for j in range(2):\n        datagen = ImageDataGenerator(rotation_range=360, fill_mode='nearest')   #Add 2 randomly rotated images from original\n        aug_iter = datagen.flow(samples, batch_size=1)\n        \n        image = aug_iter.next()[0].astype('uint8')\n        df_para.append(image)\n    \n    df_para.append(i)                          \n    for i in range(3):\n        label_para.append(1)        \n    \ndf_un, label_un = [],[]            \n\nfor filename in uninfected:\n    img = load_img(data_dir + \"Uninfected\/\" + filename, target_size = (50, 50))\n    i = img_to_array(img)\n    samples = expand_dims(i, 0)\n    for j in range(2):\n        datagen = ImageDataGenerator(rotation_range=360, fill_mode='nearest')\n        aug_iter = datagen.flow(samples, batch_size=1)\n        \n        image = aug_iter.next()[0].astype('uint8')\n        df_un.append(image)\n    df_un.append(i)                          \n    for i in range(3):\n        label_un.append(0)                            #label is a column of labels, 1 = parasitised, 0 = uninfected\n    \n","4c34d27d":"images = df_para + df_un\nlabels = label_para + label_un\nimages = np.array(images)\nlabels = np.array(labels)\nimages = images\/255              #scaling the values so that they are between 0 and 1","6b375662":"np.save(\"images\",images)  #Saved the arrays to remove the need for reading imeages from files again\nnp.save(\"labels\",labels)","92c53cff":"from keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\nfrom keras import optimizers\nimport numpy as np \nimport pandas as pd \nfrom matplotlib import pyplot as plt\nimport keras as keras\nimages = np.load(\"..\/input\/finaldata\/images.npy\")\nlabels = np.load(\"..\/input\/finaldata\/labels.npy\")","855c8f80":"import sklearn as sk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,f1_score,precision_score,recall_score\nnp.random.seed(10)\nX_train_full, X_test, y_train_full, y_test = sk.model_selection.train_test_split(images, labels, \n                                                                     test_size = 0.2, random_state = 10)\n\n#Further split training dataset into training and validation\nX_train, X_val, y_train, y_val = sk.model_selection.train_test_split(X_train_full, y_train_full, \n                                                                     test_size = 0.2, random_state = 10)","ae55a711":"from sklearn.linear_model import Perceptron\n\nclf = Perceptron(tol = None, max_iter = 1000,random_state=10,early_stopping=True,verbose=1)\nmlp_X_train = np.array(X_train).reshape((-1,50*50*3))     #Reshape tensors to vectors \nmlp_y_train = np.array(y_train)\nclf.fit(mlp_X_train,mlp_y_train)\n\n# Now predict the value of the images in the test:\n#expected = y_val\n#predicted = clf.predict(X_val)\n\n#print(\"The classification score %.5f\\n\" % classifier.score(X_val, y_val))","5dfe9cb0":"from sklearn.metrics import confusion_matrix,accuracy_score, f1_score, precision_score,recall_score\ny_true = np.array(y_val)\ny_pred = clf.predict(np.array(X_val).reshape((-1,7500)))\nprint(confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None))\nprint(\"Accuracy is \",accuracy_score(y_true, y_pred))\nprint(\"F1-Score is \",f1_score(y_true, y_pred))\nprint(\"Precision is \",precision_score(y_true, y_pred))\nprint(\"Recall is \",recall_score(y_true, y_pred))","99307d6f":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nLRmodel = LogisticRegression(random_state=10,max_iter=1000).fit(mlp_X_train, mlp_y_train)","99369ea7":"from sklearn.metrics import confusion_matrix,accuracy_score, f1_score, precision_score,recall_score\ny_true = np.array(y_val)\ny_pred = LRmodel.predict(np.array(X_val).reshape((-1,7500)))\nprint(confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None))\nprint(\"Accuracy is \",accuracy_score(y_true, y_pred))\nprint(\"F1-Score is \",f1_score(y_true, y_pred))\nprint(\"Precision is \",precision_score(y_true, y_pred))\nprint(\"Recall is \",recall_score(y_true, y_pred))","1172ab3a":"from keras import optimizers\nbasemodel = keras.Sequential([\n    \n    keras.layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (50, 50, 3)),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Conv2D(128, (3, 3), activation = 'relu'),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Flatten(),\n    keras.layers.Dropout(0.5),\n    \n    keras.layers.Dense(512, activation = 'relu'),\n    keras.layers.Dense(1, activation = 'sigmoid')\n    \n    ])\nbasemodel.compile( optimizer=optimizers.SGD(lr = 1e-3),loss='binary_crossentropy', metrics=['accuracy','AUC'])\nbasehistory = basemodel.fit(X_train, y_train,validation_data=(X_val,y_val), epochs=40, batch_size=32) ","30379166":"y_true = y_val\ny_pred = basemodel.predict_classes(X_val)\nprint(confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None))\nprint(\"Accuracy is \",accuracy_score(y_true, y_pred))\nprint(\"F1-Score is \",f1_score(y_true, y_pred))\nprint(\"Precision is \",precision_score(y_true, y_pred))\nprint(\"Recall is \",recall_score(y_true, y_pred))","821c68da":"cmbase = confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None)\nprint(cmbase[0][1]\/(cmbase[0][1]+cmbase[0][0])) #FPR\nprint(cmbase[1][0]\/(cmbase[1][0]+cmbase[1][1])) #FNR","6c07ad8e":"#Hyperas\n\n#When running Hyperas we cant access global variables so need to save numpy arrays locally first\nnp.save('images.npy', images)\nnp.save('labels.npy', labels)\n\nfrom hyperopt import Trials, STATUS_OK, tpe\nfrom hyperas import optim\nfrom hyperas.distributions import choice, uniform\n\ndef data():\n    import numpy as np\n    import sklearn as sk\n    from sklearn.model_selection import train_test_split\n \n    images = np.load('images.npy')\n    labels = np.load('labels.npy') \n\n    X_train, X_val, y_train, y_val = sk.model_selection.train_test_split(images, labels, test_size = 0.2, random_state = np.random)\n    X_train = X_train\/255\n    X_val= X_val\/255\n\n    return X_train, y_train, X_val, y_val\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, Dropout\nfrom keras import optimizers\n\ndef model(X_train, y_train, X_val, y_val):\n    model = Sequential()\n\n    model.add(Conv2D(32, (3, 3), activation = 'relu', padding = 'same', input_shape = (50, 50, 3)))\n    model.add(MaxPooling2D(2, 2))\n    \n    model.add(Conv2D(64, (3, 3), padding = 'same', activation = 'relu'))\n    model.add(MaxPooling2D(2, 2))\n   \n    model.add(Conv2D(128, (3, 3), padding = 'same', activation = 'relu'))\n    model.add(MaxPooling2D(2, 2))\n\n    model.add(Conv2D(256, (3, 3), padding = 'same', activation = 'relu'))\n    model.add(MaxPooling2D(2, 2))\n\n    model.add(Conv2D(512, (3, 3), padding = 'same', activation = 'relu'))\n    model.add(MaxPooling2D(2, 2))\n    \n    model.add(Flatten())\n    model.add(Dropout({{uniform(0, 1)}}))\n   \n    model.add(Dense({{choice([1024, 2048, 4096])}}, activation = 'relu'))\n    model.add(Dense({{choice([1024, 2048, 4096])}}, activation = 'relu'))\n    model.add(Dense(1, activation = {{choice(['sigmoid', 'softmax'])}})) #or model.add(Dense(2, activation = 'softmax'))\n\n    adam = keras.optimizers.Adam(lr={{choice([10**-3, 10**-2, 10**-1])}})\n    rmsprop = keras.optimizers.RMSprop(lr={{choice([10**-3, 10**-2, 10**-1])}})\n    sgd = keras.optimizers.SGD(lr={{choice([10**-3, 10**-2, 10**-1])}})\n    adamax = keras.optimizers.Adamax(lr={{choice([10**-3, 10**-2, 10**-1])}})\n    nadam = keras.optimizers.Nadam(lr={{choice([10**-3, 10**-2, 10**-1])}})\n   \n    choiceval = {{choice(['adam', 'rmsprop', 'sgd', 'adamax', 'nadam'])}}\n    if choiceval == 'adam':\n        optim = adam\n    elif choiceval == 'rmsprop':\n        optim = rmsprop\n    elif choiceval == 'sgd':\n        optim = sgd\n    elif choiceval == 'adamax':\n        optim = adamax\n    else:\n        optim = nadam\n        \n    model.compile(loss = 'binary_crossentropy', optimizer = optim, metrics = ['accuracy'])\n\n    model.fit(X_train, y_train,\n              batch_size = {{choice([16, 32, 50, 64])}},\n              epochs= {{choice([10, 20, 50])}},\n              verbose = 2,\n              validation_data = (X_val, y_val))\n    score, acc = model.evaluate(X_val, y_val, verbose = 0)\n    print('Test accuracy:', acc)\n    return {'loss': -acc, 'status': STATUS_OK, 'model': model}\n\n#print(\"Tuning hyperparams\")\n\nX_train, y_train, X_val, y_val = data()\n\nbest_run, best_model = optim.minimize(model = model,\n                                      data = data,\n                                      algo = tpe.suggest,\n                                      max_evals = 30,\n                                      trials = Trials())\nprint(best_run)","76a6cbb7":"from keras import optimizers\nmodel = keras.Sequential([\n    \n    keras.layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (50, 50, 3)),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Conv2D(128, (3, 3), activation = 'relu'),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Flatten(),\n    keras.layers.Dropout(0.17),\n    \n    keras.layers.Dense(128, activation = 'relu'),\n    keras.layers.Dense(1, activation = 'sigmoid')\n    \n    ])\nmodel.compile( optimizer=optimizers.RMSprop(lr = 1e-3),loss='binary_crossentropy', metrics=['accuracy','AUC'])\nhistoryrms = model.fit(X_train, y_train,validation_data=(X_val,y_val), epochs=20, batch_size=64)\nnp.save('historyrms.npy',historyrms.history)","b036d706":"from keras import optimizers\nmodel = keras.Sequential([\n    \n    keras.layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (50, 50, 3)),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Conv2D(128, (3, 3), activation = 'relu'),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Flatten(),\n    keras.layers.Dropout(0.17),\n    \n    keras.layers.Dense(128, activation = 'relu'),\n    keras.layers.Dense(1, activation = 'sigmoid')\n    \n    ])\n\nmodel.compile( optimizer=optimizers.Nadam(lr = 1e-3),loss='binary_crossentropy', metrics=['accuracy','AUC'])\nhistorynadam = model.fit(X_train, y_train,validation_data=(X_val,y_val), epochs=20, batch_size=64)\nnp.save('historynadam.npy',historynadam.history)","029817c9":"from keras import optimizers\nmodel = keras.Sequential([\n    \n    keras.layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (50, 50, 3)),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Conv2D(128, (3, 3), activation = 'relu'),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Flatten(),\n    keras.layers.Dropout(0.17),\n    \n    keras.layers.Dense(128, activation = 'relu'),\n    keras.layers.Dense(1, activation = 'sigmoid')\n    \n    ])\n#chg optimizer accordingly SGD,RMSprop,Adam,Adadelta,Adagrad,Adamax,Nadam,Ftrl\n\nmodel.compile( optimizer=optimizers.Adamax(lr = 1e-3),loss='binary_crossentropy', metrics=['accuracy','AUC'])\nhistoryadamax= model.fit(X_train, y_train,validation_data=(X_val,y_val), epochs=20, batch_size=64)\nnp.save('historyadamax.npy',historyadamax.history)","baf479ac":"from keras import optimizers\nmodel = keras.Sequential([\n    \n    keras.layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (50, 50, 3)),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Conv2D(128, (3, 3), activation = 'relu'),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Flatten(),\n    keras.layers.Dropout(0.17),\n    \n    keras.layers.Dense(128, activation = 'relu'),\n    keras.layers.Dense(1, activation = 'sigmoid')\n    \n    ])\n#chg optimizer accordingly SGD,RMSprop,Adam,Adadelta,Adagrad,Adamax,Nadam,Ftrl\n\n\nmodel.compile( optimizer=optimizers.Adam(lr = 1e-3),loss='binary_crossentropy', metrics=['accuracy','AUC'])\nhistoryadam= model.fit(X_train, y_train,validation_data=(X_val,y_val), epochs=20, batch_size=64)\nnp.save('historyadam.npy',historyadam.history)","4237b29d":"from keras import optimizers\nmodel = keras.Sequential([\n    \n    keras.layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (50, 50, 3)),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Conv2D(128, (3, 3), activation = 'relu'),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Flatten(),\n    keras.layers.Dropout(0.17),\n    \n    keras.layers.Dense(128, activation = 'relu'),\n    keras.layers.Dense(1, activation = 'sigmoid')\n    \n    ])\n#chg optimizer accordingly SGD,RMSprop,Adam,Adadelta,Adagrad,Adamax,Nadam,Ftrl\n\nmodel.compile( optimizer=optimizers.SGD(lr = 1e-3),loss='binary_crossentropy', metrics=['accuracy','AUC'])\nhistorysgd = model.fit(X_train, y_train,validation_data=(X_val,y_val), epochs=20, batch_size=64)\nnp.save('historysgd.npy',historysgd.history)","139c65ec":"from keras import optimizers\nmodel = keras.Sequential([\n    \n    keras.layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (50, 50, 3)),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Conv2D(128, (3, 3), activation = 'relu'),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Flatten(),\n    keras.layers.Dropout(0.17),\n    \n    keras.layers.Dense(128, activation = 'relu'),\n    keras.layers.Dense(1, activation = 'sigmoid')\n    \n    ])\n#chg optimizer accordingly SGD,RMSprop,Adam,Adadelta,Adagrad,Adamax,Nadam,Ftrl\nmodel.compile( optimizer=optimizers.Adadelta(lr = 1e-3),loss='binary_crossentropy', metrics=['accuracy','AUC'])\nhistoryadadelta = model.fit(X_train, y_train,validation_data=(X_val,y_val), epochs=20, batch_size=64)\nnp.save('historyadadelta.npy',historyadadelta.history)","258c9ecb":"historysgd=np.load('historysgd.npy',allow_pickle='TRUE').item()              #Load the histories\nhistoryrms=np.load('historyrms.npy',allow_pickle='TRUE').item()\nhistoryadam=np.load('historyadam.npy',allow_pickle='TRUE').item()\nhistorynadam=np.load('historynadam.npy',allow_pickle='TRUE').item()\nhistoryadamax=np.load('historyadamax.npy',allow_pickle='TRUE').item()\nhistoryadadelta=np.load('historyadadelta.npy',allow_pickle='TRUE').item()","e9615ff5":"plt.figure(figsize=(5,4))\nplt.title(\"val_loss\")\nplt.plot(historysgd['val_loss'],'-b',label=\"sgd\")\nplt.plot(historyrms['val_loss'],'-g',label=\"rms\")\nplt.plot(historyadam['val_loss'],'-r',label=\"adam\")\nplt.plot(historynadam['val_loss'],'-o',label=\"nadam\")\nplt.plot(historyadamax['val_loss'],'-y',label=\"adamax\")\nplt.plot(historyadadelta['val_loss'],'-p',label=\"adadelta\")\nplt.legend(fontsize=9)\nplt.show()","60c19c6e":"wt = 4 #Repeated for 2,4,6,8,10\nfrom keras import optimizers\nbaseweightedmodel = keras.Sequential([\n    \n    keras.layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (50, 50, 3)),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Conv2D(128, (3, 3), activation = 'relu'),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Flatten(),\n    keras.layers.Dropout(0.5),\n    \n    keras.layers.Dense(512, activation = 'relu'),\n    keras.layers.Dense(1, activation = 'sigmoid')\n    \n    ])\nbaseweightedmodel.compile( optimizer=optimizers.SGD(lr = 1e-3),loss='binary_crossentropy', metrics=['accuracy','AUC'])\nbaseweightedmodel.fit(X_train, y_train,validation_data=(X_val,y_val), epochs=20, batch_size=32, class_weight={0:1,1:wt}) ","87bd0e50":"from sklearn.metrics import confusion_matrix,accuracy_score, f1_score, precision_score,recall_score\ny_true = y_val\ny_pred = baseweightedmodel.predict_classes(X_val)\nprint(confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None))\nprint(\"Accuracy is \",accuracy_score(y_true, y_pred))\nprint(\"F1-Score is \",f1_score(y_true, y_pred))\nprint(\"Precision is \",precision_score(y_true, y_pred))\nprint(\"Recall is \",recall_score(y_true, y_pred))\ncmweightedbase = confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None)\n#print(cmbase[0][1]\/(cmbase[0][1]+cmbase[0][0])) #FPR\nprint(cmweightedbase[1][0]\/(cmweightedbase[1][0]+cmweightedbase[1][1])) #FNR","ca015f5b":"#7 Layers VGG (Our simplified model)\nmodel = Sequential([\n\n    #1st stack\n    Conv2D(32, (3, 3), padding='same', activation = 'relu', input_shape = (50, 50, 3)),\n    #Conv2D(32, (3, 3), padding='same', activation= 'relu'), \n    MaxPooling2D(2, 2),\n\n    #2nd stack\n    Conv2D(64, (3, 3), padding = 'same', activation = 'relu'),\n    #Conv2D(64, (3, 3), padding = 'same', activation = 'relu'),\n    MaxPooling2D(2, 2),\n\n    #3rd stack\n    Conv2D(128, (3, 3), padding = 'same', activation = 'relu'),\n    #Conv2D(128, (3, 3), padding = 'same', activation = 'relu'),\n    #Conv2D(128, (3, 3), padding = 'same', activation = 'relu'),\n    MaxPooling2D(2, 2),\n\n    #4th stack\n    Conv2D(256, (3, 3), padding = 'same', activation = 'relu'),\n    #Conv2D(256, (3, 3), padding = 'same', activation = 'relu'),\n    #Conv2D(256, (3, 3), padding = 'same', activation = 'relu'),\n    MaxPooling2D(2, 2),\n\n    #5th stack\n    Conv2D(512, (3, 3), padding = 'same', activation = 'relu'),\n    #Conv2D(512, (3, 3), padding = 'same', activation = 'relu'),\n    #Conv2D(512, (3, 3), padding = 'same', activation = 'relu'),\n    MaxPooling2D(2, 2),\n    \n    Flatten(),\n    \n    Dense(128, activation = 'relu'),\n    #Dense(128, activation = 'relu'), \n    Dense(1, activation = 'sigmoid')\n    \n    ])\n    \nmodel.compile( optimizer=optimizers.RMSprop(lr = 10**-3),loss='binary_crossentropy', metrics=['accuracy','AUC'])\nhistory = model.fit(X_train, y_train, validation_data = (X_val,y_val), epochs=20, batch_size=64)","711409c0":"y_true = y_val\ny_pred = model.predict_classes(X_val)\nprint(confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None))\nprint(\"Accuracy is \",accuracy_score(y_true, y_pred))\nprint(\"F1-Score is \",f1_score(y_true, y_pred))\nprint(\"Precision is \",precision_score(y_true, y_pred))\nprint(\"Recall is \",recall_score(y_true, y_pred))\ncmbase = confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None)\nprint(cmbase[0][1]\/(cmbase[0][1]+cmbase[0][0])) #FPR\nprint(cmbase[1][0]\/(cmbase[1][0]+cmbase[1][1])) #FNR","4a3bbda7":"#12 Layers VGG\nmodel = Sequential([\n\n    #1st stack\n    Conv2D(32, (3, 3), padding='same', activation = 'relu', input_shape = (50, 50, 3)),\n    Conv2D(32, (3, 3), padding='same', activation= 'relu'), \n    MaxPooling2D(2, 2),\n\n    #2nd stack\n    Conv2D(64, (3, 3), padding = 'same', activation = 'relu'),\n    Conv2D(64, (3, 3), padding = 'same', activation = 'relu'),\n    MaxPooling2D(2, 2),\n\n    #3rd stack\n    Conv2D(128, (3, 3), padding = 'same', activation = 'relu'),\n    Conv2D(128, (3, 3), padding = 'same', activation = 'relu'),\n    #Conv2D(128, (3, 3), padding = 'same', activation = 'relu'),\n    MaxPooling2D(2, 2),\n\n    #4th stack\n    Conv2D(256, (3, 3), padding = 'same', activation = 'relu'),\n    Conv2D(256, (3, 3), padding = 'same', activation = 'relu'),\n    #Conv2D(256, (3, 3), padding = 'same', activation = 'relu'),\n    MaxPooling2D(2, 2),\n\n    #5th stack\n    Conv2D(512, (3, 3), padding = 'same', activation = 'relu'),\n    Conv2D(512, (3, 3), padding = 'same', activation = 'relu'),\n    #Conv2D(512, (3, 3), padding = 'same', activation = 'relu'),\n    MaxPooling2D(2, 2),\n    \n    Flatten(),\n    \n    Dense(128, activation = 'relu'),\n    #Dense(128, activation = 'relu'), \n    Dense(1, activation = 'sigmoid')\n    \n    ])\n    \nmodel.compile( optimizer=optimizers.RMSprop(lr = 10**-3),loss='binary_crossentropy', metrics=['accuracy','AUC'])\nhistory = model.fit(X_train, y_train, validation_data = (X_val,y_val), epochs=20, batch_size=64)","e7c120e0":"from sklearn.metrics import confusion_matrix,accuracy_score, f1_score, precision_score,recall_score\ny_true = y_val\ny_pred = model.predict_classes(X_val)\nprint(confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None))\nprint(\"Accuracy is \",accuracy_score(y_true, y_pred))\nprint(\"F1-Score is \",f1_score(y_true, y_pred))\nprint(\"Precision is \",precision_score(y_true, y_pred))\nprint(\"Recall is \",recall_score(y_true, y_pred))\ncmbase = confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None)\nprint(cmbase[0][1]\/(cmbase[0][1]+cmbase[0][0])) #FPR\nprint(cmbase[1][0]\/(cmbase[1][0]+cmbase[1][1])) #FNR","9a86f204":"#16 Layers VGG (Full model)\nmodel = Sequential([\n\n    #1st stack\n    Conv2D(32, (3, 3), padding='same', activation = 'relu', input_shape = (50, 50, 3)),\n    Conv2D(32, (3, 3), padding='same', activation= 'relu'), \n    MaxPooling2D(2, 2),\n\n    #2nd stack\n    Conv2D(64, (3, 3), padding = 'same', activation = 'relu'),\n    Conv2D(64, (3, 3), padding = 'same', activation = 'relu'),\n    MaxPooling2D(2, 2),\n\n    #3rd stack\n    Conv2D(128, (3, 3), padding = 'same', activation = 'relu'),\n    Conv2D(128, (3, 3), padding = 'same', activation = 'relu'),\n    Conv2D(128, (3, 3), padding = 'same', activation = 'relu'),\n    MaxPooling2D(2, 2),\n\n    #4th stack\n    Conv2D(256, (3, 3), padding = 'same', activation = 'relu'),\n    Conv2D(256, (3, 3), padding = 'same', activation = 'relu'),\n    Conv2D(256, (3, 3), padding = 'same', activation = 'relu'),\n    MaxPooling2D(2, 2),\n\n    #5th stack\n    Conv2D(512, (3, 3), padding = 'same', activation = 'relu'),\n    Conv2D(512, (3, 3), padding = 'same', activation = 'relu'),\n    Conv2D(512, (3, 3), padding = 'same', activation = 'relu'),\n    MaxPooling2D(2, 2),\n    \n    Flatten(),\n    Dropout(0.5),\n    \n    Dense(128, activation = 'relu'),\n    Dense(128, activation = 'relu'), \n    Dense(1, activation = 'sigmoid')\n    \n    ])\n    \nmodel.compile( optimizer=optimizers.RMSprop(lr = 10**-3),loss='binary_crossentropy', metrics=['accuracy','AUC'])\nhistory = model.fit(X_train, y_train, validation_data = (X_val,y_val), epochs=20, batch_size=64)","de2f4529":"from sklearn.metrics import confusion_matrix,accuracy_score, f1_score, precision_score,recall_score\ny_true = y_val\ny_pred = model.predict_classes(X_val)\nprint(confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None))\nprint(\"Accuracy is \",accuracy_score(y_true, y_pred))\nprint(\"F1-Score is \",f1_score(y_true, y_pred))\nprint(\"Precision is \",precision_score(y_true, y_pred))\nprint(\"Recall is \",recall_score(y_true, y_pred))\ncmbase = confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None)\nprint(cmbase[0][1]\/(cmbase[0][1]+cmbase[0][0])) #FPR\nprint(cmbase[1][0]\/(cmbase[1][0]+cmbase[1][1])) #FNR","9409a3d7":"#This is using Keras' VGG16 architecture via transfer learning\n\nfrom keras.applications.vgg16 import VGG16\nfrom keras.models import Model\n\nbase_model = VGG16(weights = 'imagenet', include_top = False, input_shape = (50, 50, 3))\nbase_model = Model(inputs = base_model.input, outputs = base_model.get_layer('block5_conv2').output)\n\nbase_model.trainable = False\n\nx = base_model.output\nx = MaxPooling2D()(x)\nx = Flatten()(x)\nx = Dense(128, activation='relu')(x)\nx = Dense(128, activation='relu')(x)\n\n#Add a logistic layer \npredictions = Dense(1, activation = 'sigmoid', name = 'predictions')(x)\n\n#This is the model we will train\nmodel = Model(inputs = base_model.input, outputs = predictions)\nmodel.compile(optimizer = optimizers.RMSprop(lr = 10**-3), loss = 'binary_crossentropy', metrics = ['accuracy','AUC'])\nhistory = model.fit(X_train, y_train, validation_data = (X_val,y_val), epochs=20, batch_size=64)","8d21d168":"from sklearn.metrics import confusion_matrix,accuracy_score, f1_score, precision_score,recall_score\ny_true = y_val\ny_pred = np.around(model.predict(X_val))\n\nprint(confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None))\nprint(\"Accuracy is \",accuracy_score(y_true, y_pred))\nprint(\"F1-Score is \",f1_score(y_true, y_pred))\nprint(\"Precision is \",precision_score(y_true, y_pred))\nprint(\"Recall is \",recall_score(y_true, y_pred))\ncmbase = confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None)\nprint(cmbase[0][1]\/(cmbase[0][1]+cmbase[0][0])) #FPR\nprint(cmbase[1][0]\/(cmbase[1][0]+cmbase[1][1])) #FNR","1e3ae8b3":"finalmodel = keras.Sequential([\n\n    #1st stack\n    Conv2D(32, (3, 3), padding='same', activation = 'relu', input_shape = (50, 50, 3)),\n    Conv2D(32, (3, 3), padding='same', activation= 'relu'), \n    MaxPooling2D(2, 2),\n\n    #2nd stack\n    Conv2D(64, (3, 3), padding = 'same', activation = 'relu'),\n    Conv2D(64, (3, 3), padding = 'same', activation = 'relu'),\n    MaxPooling2D(2, 2),\n\n    #3rd stack\n    Conv2D(128, (3, 3), padding = 'same', activation = 'relu'),\n    #Conv2D(128, (3, 3), padding = 'same', activation = 'relu'),\n    #Conv2D(128, (3, 3), padding = 'same', activation = 'relu'),\n    MaxPooling2D(2, 2),\n\n    #4th stack\n    Conv2D(256, (3, 3), padding = 'same', activation = 'relu'),\n    #Conv2D(256, (3, 3), padding = 'same', activation = 'relu'),\n    #Conv2D(256, (3, 3), padding = 'same', activation = 'relu'),\n    MaxPooling2D(2, 2),\n\n    #5th stack\n    Conv2D(512, (3, 3), padding = 'same', activation = 'relu'),\n    #Conv2D(512, (3, 3), padding = 'same', activation = 'relu'),\n    #Conv2D(512, (3, 3), padding = 'same', activation = 'relu'),\n    MaxPooling2D(2, 2),\n    \n    Flatten(),\n    #Dropout(0.5), #Can try without dropout\n    \n    Dense(128, activation = 'relu'),\n    #Dense(128, activation = 'relu'), \n    Dense(1, activation = 'sigmoid') \n    \n    ])\n\nfinalmodel.compile( optimizer=optimizers.SGD(lr = 1e-3),loss='binary_crossentropy', metrics=['accuracy','AUC'])\nhistory = finalmodel.fit(X_train_full, y_train_full,validation_data=(X_val,y_val), epochs=40, batch_size=32,class_weight={0:1,1:4}) #weight = 4","7c691db7":"ypred = pd.DataFrame(finalmodel.predict_classes(X_test))\nerrordf = pd.concat([ypred,pd.DataFrame(y_test)],axis=1)","1ebc0b9e":"from sklearn.metrics import confusion_matrix,accuracy_score, f1_score, precision_score,recall_score\ny_true = y_test\ny_pred = finalmodel.predict_classes(X_test)\nfinalcmbase= confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None)\nprint(confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None))\nprint(\"Accuracy is \",accuracy_score(y_true, y_pred))\nprint(\"F1-Score is \",f1_score(y_true, y_pred))\nprint(\"Precision is \",precision_score(y_true, y_pred))\nprint(\"Recall is \",recall_score(y_true, y_pred))\nprint(finalcmbase[0][1]\/(finalcmbase[0][1]+finalcmbase[0][0])) #FPR\nprint(finalcmbase[1][0]\/(finalcmbase[1][0]+finalcmbase[1][1])) #FNR\n","94bfafb6":"from keras import optimizers\nbasemodel = keras.Sequential([\n    \n    keras.layers.Conv2D(32, (3, 3), activation = 'relu', input_shape = (50, 50, 3)),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Conv2D(64, (3, 3), activation = 'relu'),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Conv2D(128, (3, 3), activation = 'relu'),\n    keras.layers.MaxPooling2D(2, 2),\n    \n    keras.layers.Flatten(),\n    keras.layers.Dropout(0.5),\n    \n    keras.layers.Dense(512, activation = 'relu'),\n    keras.layers.Dense(1, activation = 'sigmoid')\n    \n    ])\nbasemodel.compile( optimizer=optimizers.SGD(lr = 1e-3),loss='binary_crossentropy', metrics=['accuracy','AUC'])\nbasehistory = basemodel.fit(X_train_full, y_train_full,validation_data=(X_val,y_val), epochs=40, batch_size=32,class_weight={0:1,1:4}) ","7478cd10":"from sklearn.metrics import confusion_matrix,accuracy_score, f1_score, precision_score,recall_score\ny_true = y_test\ny_pred = basemodel.predict_classes(X_test)\ncmbase= confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None)\nprint(confusion_matrix(y_true, y_pred, labels=None, sample_weight=None, normalize=None))\nprint(\"Accuracy is \",accuracy_score(y_true, y_pred))\nprint(\"F1-Score is \",f1_score(y_true, y_pred))\nprint(\"Precision is \",precision_score(y_true, y_pred))\nprint(\"Recall is \",recall_score(y_true, y_pred))\nprint(cmbase[0][1]\/(cmbase[0][1]+cmbase[0][0])) #FPR\nprint(cmbase[1][0]\/(cmbase[1][0]+cmbase[1][1])) #FNR","21568a1f":"ypred = pd.DataFrame(basemodel.predict_classes(X_test))\nerrordf = pd.concat([ypred,pd.DataFrame(y_test)],axis=1)","0adf4853":"#This is to find out how off is the predicted labels from actual labels\n\nimport matplotlib.pyplot as pltpy\n\npredictions = (finalmodel.predict(X_test) > 0.5).astype(int).flatten()\nwrong_predictions = finalmodel.predict(X_test)[predictions != y_test]\nwrong_pred_list = []\nfor i in range(wrong_predictions.shape[0]):\n    wrong_pred_list.append(wrong_predictions[i][0])\n\nfn_list_pre = []\nfp_list_pre = []\n\nfor i in range(len(wrong_pred_list)):\n        if wrong_pred_list[i] >= 0.5:\n            fp_list_pre.append(wrong_pred_list[i])\n        else:\n            fn_list_pre.append(wrong_pred_list[i])\n\nfn_list = []\nfp_list = []\n            \nfor i in range(len(fn_list_pre)):\n    fn_list.append(0.5 - fn_list_pre[i])\n    \nfor i in range(len(fp_list_pre)):\n    fp_list.append(fp_list_pre[i] - 0.5)\n    \npltpy.hist(fn_list, bins = 20)\n\n#pltpy.hist(fp_list, bins = 20)","f34e0a8f":"pltpy.hist(fp_list, bins = 20)","076bcfb3":"#False Negatives and False Positives of Final Model\nypredfinal = pd.DataFrame(finalmodel.predict_classes(X_test))\nerrordffinal = pd.concat([ypred,pd.DataFrame(y_test)],axis=1)\nypredbase = pd.DataFrame(basemodel.predict_classes(X_test))\nerrordfbase = pd.concat([ypred,pd.DataFrame(y_test)],axis=1)","223cc4f9":"%matplotlib inline\nimport matplotlib.image as mpimg\nmisclassified = errordffinal[errordffinal.iloc[:,0] != errordffinal.iloc[:,1]]\nFP = misclassified[misclassified.iloc[:,0]==1]\nFN = misclassified[misclassified.iloc[:,0]==0]","c3ddf636":"plt.figure(figsize=(11,5))\nfor i in range(1,11):\n    plt.subplot(2,5,i)\n    plt.imshow(X_test[FP.index[i]])\n        ","f3994e84":"plt.figure(figsize=(10,5))\nfor i in range(1,11):\n    plt.subplot(2,5,i)\n    plt.imshow(X_test[FN.index[i]])","d08341e5":"#False negatives and false positives of LogReg\ny_true = np.array(y_val)\ny_pred = LRmodel.predict(np.array(X_val).reshape((-1,7500)))\nypred = pd.DataFrame(y_pred)\nerrordf = pd.concat([ypred,pd.DataFrame(y_true)],axis=1)\n\nmisclassified = errordf[errordf.iloc[:,0] != errordf.iloc[:,1]]\nFP = misclassified[misclassified.iloc[:,0]==1]\nFN = misclassified[misclassified.iloc[:,0]==0]","fe116a88":"#False Positives\nplt.title(\"False Positives\")\nplt.figure(figsize=(10,5))\nfor i in range(1,11):\n    plt.title()\n    plt.subplot(2,5,i)\n    plt.imshow(X_test.reshape((-1,50,50,3))[FP.index[i]])\n#False Negatives\nplt.title(\"False Negatives\")\nplt.figure(figsize=(10,5))\nfor i in range(1,11):\n    plt.subplot(2,5,i)\n    plt.imshow(X_test.reshape((-1,50,50,3))[FN.index[i]])","658701ca":"#False positives and negatives for PLA\ny_true = np.array(y_val)\ny_pred = clf.predict(np.array(X_val).reshape((-1,7500)))\nypred = pd.DataFrame(y_pred)\nerrordf = pd.concat([ypred,pd.DataFrame(y_true)],axis=1)\n\nmisclassified = errordf[errordf.iloc[:,0] != errordf.iloc[:,1]]\nFP = misclassified[misclassified.iloc[:,0]==1]\nFN = misclassified[misclassified.iloc[:,0]==0]","2ed21b64":"#False Positives\nplt.title(\"False Positives\")\nplt.figure(figsize=(10,5))\nfor i in range(1,11):\n    plt.title()\n    plt.subplot(2,5,i)\n    plt.imshow(X_test.reshape((-1,50,50,3))[FP.index[i]])\n#False Negatives\nplt.title(\"False Negatives\")\nplt.figure(figsize=(10,5))\nfor i in range(1,11):\n    plt.subplot(2,5,i)\n    plt.imshow(X_test.reshape((-1,50,50,3))[FN.index[i]])","a1ebf209":"### Weighted Model Metrics","22a86dd9":"## Experiments on Optimisers\n### RMSprop","2d18909a":"### Adamax","83145204":"## Effect of number of VGG-16 layers on accuracy\nHere we try out the different layers of VGG-16 to see if they achieve better results\n### 7 layer model","e0f64a77":"### False Positive Images","cd37b07f":"# Main Objective\nMain goal of this notebook is to explore 3 machine learning methods on image detection:\n- 1) Perceptron Learning Algorithm\n- 2) Logistic Regression\n- 3) Convolutional Neural Network \n\nFor disease detection, accuracy should not be the only metric of assessment, but False Negative Rate(FNR) as well, as we seek to minimise the chances of diagonosing someone as malaria-uninfected when he\/she is indeed infected. \n\nThis notebook covers baseline models for the 3 machine learning methods, then dive deeper into making a more robust adapted VGG-16 CNN for best results(accuracy and FNR). We also plot the images of False Negatives and False Positives to determine what kind of images were wrongly detected. \n\n### This notebook cannot run in one pass, due to Kaggle's RAM limitations.","382b8733":"## Base Models\n### Perceptron Learning Algorithm","56771132":"### Base model with weighted penalty","132b80f2":"## Plotting validation loss over epochs for each optimiser","10d454f3":"## Identifying False Negative and False Positives","a1b6d2cc":"### Logistic Regression","4a134a11":"## Effect of weights on FNR","9f7b0ae3":"## Transfer Learning with pre-trained VGG-16","cd5d8515":"### Logistic Regression Metrics","1a469ff1":"### Load the training and test arrays","d3769207":"### Nadam","a2ae22c9":"### 12 layer model","b6e5e50b":"### Adam","71c93b8c":"### Adadelta","d14752cf":"### Base VGG-16 Metrics","1b5b0e1d":"### 16 layer (Full VGG-16) model","c83c0a67":"### Simplified VGG-16(Base)","daeb25a6":"### False Negative Images","9d1bb9e4":"## Final Model chosen\nWe run the final model as well as the base model with the full training set and test their performance on the test set. ","dea98ee8":"### SGD","4c9072e0":"## Plotting the errors","8ae07dc0":"### Train Test Split\nSplitting the data into training, validation and test sets. X_train_full is the complete training set to be used on test dataset. ","4ea61449":"### PLA Metrics","04d0acd9":"## Hyperas Experiments\nThis section takes a very long time to run"}}