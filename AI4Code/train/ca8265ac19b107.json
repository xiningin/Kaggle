{"cell_type":{"8f99eb12":"code","4996dcb1":"code","e235dde5":"code","f4848a34":"code","8cbb4b86":"code","7a37262e":"code","704f19e9":"code","2b1e5e59":"code","74dfa260":"code","df1df77e":"code","48d7d685":"code","859d6f52":"code","a6ef19f8":"code","12f5f4e1":"code","3f94e43c":"code","cdbd47bc":"code","da4a269b":"code","40cd87d6":"code","f8af4711":"code","696123eb":"code","211c5b3b":"code","66d3bc22":"code","24622cb0":"code","9eb73f71":"code","1a0df704":"code","f4959323":"code","9cdaecdf":"code","9bf9fc2f":"code","c64759d4":"code","ac31e84c":"code","5d633216":"code","d712e634":"code","a9dce607":"code","702b026b":"code","72ed8dd3":"markdown","652898ac":"markdown","0c828016":"markdown","1f259b61":"markdown","a4d36964":"markdown","19a70066":"markdown","3fe5256e":"markdown","af8c874a":"markdown","8be5a67f":"markdown","375bcccf":"markdown","aec30909":"markdown","7e48af12":"markdown"},"source":{"8f99eb12":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4996dcb1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n","e235dde5":"#load the data\ndata = pd.read_csv('..\/input\/churn-for-bank-customers\/churn.csv')\n\nprint(\"churn bank modeling data  has {} data points with {} variables each.\".format(*data.shape))\ndata.head()","f4848a34":"#check is there is missing data\ndata.isnull().sum()","8cbb4b86":"#check for data formate\ndata.info()","7a37262e":"#describe the non-numerical data\ndata[['Geography','Gender']].describe()","704f19e9":"## visualization of the features \ndata.hist(figsize=(10,20), xrot=-45)\nplt.show()","2b1e5e59":"#Create a list that contains the number of exited customer \nExitedList = [data.Exited[data['Exited']==1].count(), data.Exited[data['Exited']==0].count()]\n\n#set figure size and title\nplt.subplots(figsize=(10, 10))\nplt.title('Proportion of Customer Churn', size = 12)\n\n#display the proportion of Customer Churn\nplt.pie(ExitedList,labels = ['Exited', 'Not Exited'], autopct='%.2f%%', explode = (0 , 0.05))\nplt.legend(labels = ['Exited', 'Not Exited'], loc = \"upper right\")\nplt.show()","74dfa260":"#preparing the figure size \nfig, axarr = plt.subplots(2, 3, figsize=(15, 15))\n\n#visulazie the count of Exited and NotExited for each feature\n\nsns.countplot('Geography', hue = 'Exited',data = data, ax = axarr[0][0])\nsns.countplot('Gender', hue = 'Exited',data = data, ax = axarr[0][1])\nsns.countplot('Tenure', hue = 'Exited',data = data, ax = axarr[0][2])\nsns.countplot('NumOfProducts', hue = 'Exited',data = data, ax = axarr[1][0])\nsns.countplot('HasCrCard', hue = 'Exited',data = data, ax = axarr[1][1])\nsns.countplot('IsActiveMember', hue = 'Exited',data = data, ax = axarr[1][2])\n","df1df77e":"#visulaization relation between Age and Exited\nFacetGrid = sns.FacetGrid(data, hue='Exited', aspect=4)\nFacetGrid.map(sns.kdeplot, 'Age', shade=True )\nFacetGrid.set(xlim=(16, data['Age'].max()))\nFacetGrid.add_legend(labels = ['Stay', \"Exited\"])","48d7d685":"#Vislization of the count of each value in Geography featrure\nsns.countplot(y=data['Geography'], data=data, order = data['Geography'].value_counts().index)\nplt.show()\n#Vislization of the count of each value in Gender feature\n\nsns.countplot(y=data['Gender'], data=data, order = data['Gender'].value_counts().index)\nplt.show()\n","859d6f52":"plt.figure(figsize=(7,7))\nsns.heatmap(data.corr(), cmap=\"YlGnBu\")","a6ef19f8":"data.corr().at['IsActiveMember', 'Exited']\n","12f5f4e1":"data.corr().at['Balance', 'NumOfProducts']\n","3f94e43c":"data.corr().at['Age', 'Exited']\n","cdbd47bc":"data.head()","da4a269b":"#Drop the Surname,CustmerId,HasCrCard features from the data set as it will not considert in prediction \ndata.drop(\"CustomerId\", axis=1, inplace=True)\ndata.drop(\"Surname\", axis=1, inplace=True)\ndata.drop(\"HasCrCard\", axis=1, inplace=True)","40cd87d6":"#apply one hot encodeing to  Gender and Geography\ndata = pd.get_dummies(data = data ,columns=['Gender', 'Geography'])\ndata.head()","f8af4711":"#rename the coulmuns\ndata.rename(columns={'Gender_Female':'Female',\n                     'Gender_Male':'Male',\n                     'Geography_France': 'France',\n                     'Geography_Germany' : 'Germany',\n                     'Geography_Spain':'Spain'}, inplace = True)\ndata.head()","696123eb":"#preparing data for split and prediction\nfeatures = data.drop(['Exited'], axis = 1)\npredict = data['Exited']","211c5b3b":"from sklearn.model_selection import train_test_split \n\n#split tha data\nX_train, X_test, y_train, y_test = train_test_split(features, predict, test_size = 0.4,random_state = 200)\n# Show the results of the split\nprint(\"Training set has {} samples.\".format(X_train.shape[0]))\nprint(\"Testing set has {} samples.\".format(X_test.shape[0]))","66d3bc22":"#improting necassery libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier as DecisionTree\nfrom sklearn.metrics import roc_auc_score,confusion_matrix,f1_score,accuracy_score","24622cb0":"# using Logistic Regresiion algorithm to build first model\nlogreg_model = LogisticRegression(solver='liblinear')\nfit = logreg_model.fit(X_train, y_train)\nfit_prediction_train = fit.predict(X_train)\nfit_prediction_test = fit.predict(X_test)\n\n#validation of  Logistic Regresiion\nprint('Accuracy Score for testing set :' +'{}'.format(accuracy_score(fit_prediction_test, y_test)))\nprint('Accuracy Score for training set :' +'{}'.format(accuracy_score(fit_prediction_train, y_train)))\n\nprint('\\nF1 score for testing set :'+'{}'.format(f1_score(fit_prediction_test, y_test)))\nprint('F1 score for training set:'+'{}'.format(f1_score(fit_prediction_train, y_train)))\n\n\nprint('\\nROC AUC Score for testing set: ' +'{}'.format(roc_auc_score(y_test, fit.predict_proba(X_test)[::,1])))\nprint('ROC AUC Score for train set: ' +'{}'.format(roc_auc_score(y_train, fit.predict_proba(X_train)[::,1])))\n\nconfusion_matrix(y_test, fit.predict(X_test))","9eb73f71":"# using RandomForestClassifier algorithm to build second model\nrf = RandomForestClassifier(n_estimators=100)\nrf_fit = rf.fit(X_train, y_train)\nrf_prediction_train_ = rf_fit.predict(X_train)\nrf_prediction_test = rf_fit.predict(X_test)\n\n#validation of RandomForestClassifier\nprint('Accuracy Score for testing set :' +'{}'.format(accuracy_score(rf_prediction_test, y_test)))\nprint('Accuracy Score for training set :' +'{}'.format(accuracy_score(rf_prediction_train_, y_train)))\n\nprint('\\nF1 score for testing set :'+'{}'.format(f1_score(rf_prediction_test, y_test)))\nprint('F1 score for training set:'+'{}'.format(f1_score(rf_prediction_train_, y_train)))\n\n\nprint('\\nROC AUC Score for testing set: ' +'{}'.format(roc_auc_score(y_test, fit.predict_proba(X_test)[::,1])))\nprint('ROC AUC Score for train set: ' +'{}'.format(roc_auc_score(y_train, fit.predict_proba(X_train)[::,1])))\n\nconfusion_matrix(y_test, fit.predict(X_test))","1a0df704":"# using GradientBoostingClassifier algorithm to build second model\n\ngb = GradientBoostingClassifier(n_estimators=100)\ngb_fit = gb.fit(X_train, y_train)\ngb_prediction_train_ = gb_fit.predict(X_train)\ngb_prediction_test = gb_fit.predict(X_test)\n\n#validation of GradientBoostingClassifier\nprint('Accuracy Score for testing set :' +'{}'.format(accuracy_score(gb_prediction_test, y_test)))\nprint('Accuracy Score for training set :' +'{}'.format(accuracy_score(gb_prediction_train_, y_train)))\n\nprint('\\nF1 score for testing set :'+'{}'.format(f1_score(gb_prediction_test, y_test)))\nprint('F1 score for training set:'+'{}'.format(f1_score(gb_prediction_train_, y_train)))\n\n\nprint('\\nROC AUC Score for testing set: ' +'{}'.format(roc_auc_score(y_test, fit.predict_proba(X_test)[::,1])))\nprint('ROC AUC Score for train set: ' +'{}'.format(roc_auc_score(y_train, fit.predict_proba(X_train)[::,1])))\n\nconfusion_matrix(y_test, fit.predict(X_test))\n","f4959323":"# store No. of Exited and indices\nExited_records = data['Exited'].sum()\nExited_indices = np.array(data[data.Exited == 1].index)\n\n# Picking the indices of the normal Exited\nnormal_indices = data[data.Exited == 0].index\n\n# Out of the indices we picked, randomly select number of normal records = number of Exited records \nrandom_normal_indices = np.random.choice(normal_indices, Exited_records, replace = False)\nrandom_normal_indices = np.array(random_normal_indices)\n\n# Merge the 2 indices\nunder_sample_indices = np.concatenate([Exited_indices,random_normal_indices])\n\n# Copy under sample dataset  \nunder_sample_data = data.iloc[under_sample_indices,:]\n\n# Split data into features and target labels \nfeatures_undersample = under_sample_data.drop(['Exited'], axis = 1)\ntarget_undersample = under_sample_data['Exited']\n\n# Show ratio\nprint(\"Percentage of NotExited: \", under_sample_data.Exited[under_sample_data['Exited'] == 0].count())\nprint(\"Percentage of Exited: \", under_sample_data.Exited[under_sample_data['Exited'] == 1].count())\nprint(\"Total number of  resampled data: \", under_sample_data['Exited'].count())","9cdaecdf":"under_sample_Exited_Real = [under_sample_data.Exited[under_sample_data['Exited'] == 0].count(), Exited_records]\n\n# Plot the proportion \nplt.subplots(figsize = (7, 7))\nplt.title(\"Proportion of Exited after resampling data\", size = 20)\nax = sns.countplot(x = under_sample_data['Exited'], data= under_sample_data)\nax.legend(labels=['NotExited', 'Exited'], loc = 'upper left')","9bf9fc2f":"# Split the 'features_undersample' and 'target_anderSample' data into training and testing sets\nX_train_sampled, X_test_sampled, y_train_sampled, y_test_sampled = train_test_split(features_undersample,\n                                                                                    target_undersample,\n                                                                                    test_size = 0.15,\n                                                                                    random_state = 25)\n# Show the results of the split\nprint(\"Training set has {} samples.\".format(X_train_sampled.shape[0]))\nprint(\"Testing set has {} samples.\".format(X_test_sampled.shape[0]))","c64759d4":"# using Logistic Regresiion algorithm to fit it on the resampled data\nlogreg_model = LogisticRegression(solver='liblinear')\nfit = logreg_model.fit(X_train_sampled, y_train_sampled)\nfit_prediction_train_sampled = fit.predict(X_train_sampled)\nfit_prediction_test_sampled = fit.predict(X_test_sampled)\n\n#validation of  Logistic Regresiion\nprint('Accuracy Score for testing set :' +'{}'.format(accuracy_score(fit_prediction_test_sampled, y_test_sampled)))\nprint('Accuracy Score for training set :' +'{}'.format(accuracy_score(fit_prediction_train_sampled, y_train_sampled)))\n\nprint('\\nF1 score for testing set :'+'{}'.format(f1_score(fit_prediction_test_sampled, y_test_sampled)))\nprint('F1 score for training set:'+'{}'.format(f1_score(fit_prediction_train_sampled, y_train_sampled)))\n\n\nprint('\\nROC AUC Score for testing set: ' +'{}'.format(roc_auc_score(y_test_sampled, fit.predict_proba(X_test_sampled)[::,1])))\nprint('ROC AUC Score for train set: ' +'{}'.format(roc_auc_score(y_train_sampled, fit.predict_proba(X_train_sampled)[::,1])))\n\nconfusion_matrix(y_test_sampled, fit.predict(X_test_sampled))","ac31e84c":"# using RandomForestClassifier algorithm to to fit it on the resampled data\nrf = RandomForestClassifier(n_estimators=100)\nrf_fit = rf.fit(X_train_sampled, y_train_sampled)\nrf_prediction_train_sampled = rf_fit.predict(X_train_sampled)\nrf_prediction_test_sampled= rf_fit.predict(X_test_sampled)\n\n#validation of RandomForestClassifier\nprint('Accuracy Score for testing set :' +'{}'.format(accuracy_score(rf_prediction_test_sampled, y_test_sampled)))\nprint('Accuracy Score for training set :' +'{}'.format(accuracy_score(rf_prediction_train_sampled, y_train_sampled)))\n\nprint('\\nF1 score for testing set :'+'{}'.format(f1_score(rf_prediction_test_sampled, y_test_sampled)))\nprint('F1 score for training set:'+'{}'.format(f1_score(rf_prediction_train_sampled, y_train_sampled)))\n\n\nprint('\\nROC AUC Score for testing set: ' +'{}'.format(roc_auc_score(y_test_sampled, fit.predict_proba(X_test_sampled)[::,1])))\nprint('ROC AUC Score for train set: ' +'{}'.format(roc_auc_score(y_train_sampled, fit.predict_proba(X_train_sampled)[::,1])))\n\nconfusion_matrix(y_test_sampled, fit.predict(X_test_sampled))\n","5d633216":"gb = GradientBoostingClassifier(n_estimators=100)\ngb_fit = gb.fit(X_train, y_train)\ngb_prediction_train_sampled = gb_fit.predict(X_train_sampled)\ngb_prediction_test_sampled = gb_fit.predict(X_test_sampled)\n\n#validation of GradientBoostingClassifier\nprint('Accuracy Score for testing set :' +'{}'.format(accuracy_score(gb_prediction_test_sampled, y_test_sampled)))\nprint('Accuracy Score for training set :' +'{}'.format(accuracy_score(gb_prediction_train_sampled, y_train_sampled)))\n\nprint('\\nF1 score for testing set :'+'{}'.format(f1_score(gb_prediction_test_sampled, y_test_sampled)))\nprint('F1 score for training set:'+'{}'.format(f1_score(gb_prediction_train_sampled, y_train_sampled)))\n\n\nprint('\\nROC AUC Score for testing set: ' +'{}'.format(roc_auc_score(y_test_sampled, fit.predict_proba(X_test_sampled)[::,1])))\nprint('ROC AUC Score for train set: ' +'{}'.format(roc_auc_score(y_train_sampled, fit.predict_proba(X_train_sampled)[::,1])))\n\nconfusion_matrix(y_test_sampled, fit.predict(X_test_sampled))","d712e634":"# Use the logistic Regresiion model to predict the training and testing set of whole datasetafter \nfit_prediction_train_after_sampled = fit.predict(X_train)\nfit_prediction_test_after_sampled = fit.predict(X_test)\n\nprint('Accuracy Score for testing set :' +'{}'.format(accuracy_score(fit_prediction_test_after_sampled, y_test)))\nprint('Accuracy Score for training set :' +'{}'.format(accuracy_score(fit_prediction_train_after_sampled, y_train)))\n\nprint('\\nF1 score for testing set :'+'{}'.format(f1_score(fit_prediction_test_after_sampled, y_test)))\nprint('F1 score for training set:'+'{}'.format(f1_score(fit_prediction_train_after_sampled, y_train)))\n\n\nprint('\\nROC AUC Score for testing set: ' +'{}'.format(roc_auc_score(y_test, fit_prediction_test_after_sampled)))\nprint('ROC AUC Score for train set: ' +'{}'.format(roc_auc_score(y_train,fit_prediction_train_after_sampled )))\n","a9dce607":"# Use the Random ForestClassifier model to predict the training and testing set of whole dataset \nrf_fit_prediction_train_after_sampled = rf_fit.predict(X_train)\nrf_fit_prediction_test_after_sampled = rf_fit.predict(X_test)\n\nprint('Accuracy Score for testing set :' +'{}'.format(accuracy_score(rf_fit_prediction_test_after_sampled, y_test)))\nprint('Accuracy Score for training set :' +'{}'.format(accuracy_score(rf_fit_prediction_train_after_sampled, y_train)))\n\nprint('\\nF1 score for testing set :'+'{}'.format(f1_score(rf_fit_prediction_test_after_sampled, y_test)))\nprint('F1 score for training set:'+'{}'.format(f1_score(rf_fit_prediction_train_after_sampled, y_train)))\n\n\nprint('\\nROC AUC Score for testing set: ' +'{}'.format(roc_auc_score(y_test, rf_fit_prediction_test_after_sampled)))\nprint('ROC AUC Score for train set: ' +'{}'.format(roc_auc_score(y_train,rf_fit_prediction_train_after_sampled )))\n\n","702b026b":"# Use this GradientBoostingClassifier model to predict the training and testing set of whole dataset \n\ngb_fit_prediction_train_after_sampled = gb_fit.predict(X_train)\ngb_fit_prediction_test_after_sampled = gb_fit.predict(X_test)\n\nprint('Accuracy Score for testing set :' +'{}'.format(accuracy_score(gb_fit_prediction_test_after_sampled, y_test)))\nprint('Accuracy Score for training set :' +'{}'.format(accuracy_score(gb_fit_prediction_train_after_sampled, y_train)))\n\nprint('\\nF1 score for testing set :'+'{}'.format(f1_score(gb_fit_prediction_test_after_sampled, y_test)))\nprint('F1 score for training set:'+'{}'.format(f1_score(gb_fit_prediction_train_after_sampled, y_train)))\n\n\nprint('\\nROC AUC Score for testing set: ' +'{}'.format(roc_auc_score(y_test, gb_fit_prediction_test_after_sampled)))\nprint('ROC AUC Score for train set: ' +'{}'.format(roc_auc_score(y_train,gb_fit_prediction_train_after_sampled )))\n","72ed8dd3":"-check for colleration between data \n","652898ac":"# 3-Preprocessing on data","0c828016":"### Clean the data","1f259b61":"# Split The Data For Training and Testing","a4d36964":"# 2.Visualization\n","19a70066":"# Resampling the data","3fe5256e":"## Bank Customer Churn Modeling","af8c874a":"### showing the colleration value of each of them ","8be5a67f":"#there is colleration between balance and number of products \n#and also Age is making colleration between it and exited\n#and also there is colleration between isActive member and exited\n#almost there is no colleration between CustomerId and Exited\n#almost there is no colleration between HasCrCard and Exited","375bcccf":"#1-Proportion of Customer churn.\n#2-Relation between 'Geography' and 'Exited'.\n#3-Relation between 'Gender' and 'Exited'.\n#4-Relation between 'HasCrCard' and 'Exited'.\n#5-Relation between 'IsActiveMember' and 'Exited'.\n#6-Relation between 'Age' and 'Exited'","aec30909":"### First importing the necassery libraries ","7e48af12":"# Model Evalution"}}