{"cell_type":{"2aef8de5":"code","1c765e07":"code","f0c1fed2":"code","35a80260":"code","f760650f":"code","40251227":"code","f15d011e":"code","0a22edde":"code","c15e4c2c":"code","b984deb9":"code","a0740d99":"code","fcc36623":"code","14e843cb":"code","53e3ddb5":"code","66f35d5b":"markdown","52c9fa86":"markdown","c71cb3bc":"markdown","8ed8c7ee":"markdown","bff35301":"markdown","c1e8d159":"markdown","0df46979":"markdown","88dde481":"markdown","a4fc5bf2":"markdown","cebcb99d":"markdown","bd0ae7d2":"markdown","33fca599":"markdown","885ca1c1":"markdown","6b7cb799":"markdown","4bd5f38b":"markdown","f091c895":"markdown","094e5028":"markdown","a3a66f50":"markdown","1c710c7c":"markdown","e6cba892":"markdown","18ca1f9c":"markdown","13c797c3":"markdown","2c17ebec":"markdown","f40a480a":"markdown","86331b87":"markdown","9e1a250f":"markdown"},"source":{"2aef8de5":"import nltk                                # Python library for NLP\nfrom nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\nimport matplotlib.pyplot as plt            # library for visualization\nimport random                              # pseudo-random number generator","1c765e07":"# downloads sample twitter dataset. uncomment the line below if running on a local machine.\nnltk.download('twitter_samples')","f0c1fed2":"# select the set of positive and negative tweets\nall_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')","35a80260":"print('Number of positive tweets: ', len(all_positive_tweets))\nprint('Number of negative tweets: ', len(all_negative_tweets))\n\nprint('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\nprint('The type of a tweet entry is: ', type(all_negative_tweets[0]))","f760650f":"# Declare a figure with a custom size\nfig = plt.figure(figsize=(5, 5))\n\n# labels for the two classes\nlabels = 'Positives', 'Negative'\n\n# Sizes for each slide\nsizes = [len(all_positive_tweets), len(all_negative_tweets)] \n\n# Declare pie chart, where the slices will be ordered and plotted counter-clockwise:\nplt.pie(sizes, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\n\n# Equal aspect ratio ensures that pie is drawn as a circle.\nplt.axis('equal')  \n\n# Display the chart\nplt.show()","40251227":"# print positive in greeen\nprint('\\033[92m' + all_positive_tweets[random.randint(0,5000)])\n\n# print negative in red\nprint('\\033[91m' + all_negative_tweets[random.randint(0,5000)])","f15d011e":"# Our selected sample. Complex enough to exemplify each step\ntweet = all_positive_tweets[2277]\nprint(tweet)","0a22edde":"# download the stopwords from NLTK\nnltk.download('stopwords')","c15e4c2c":"import re                                  # library for regular expression operations\nimport string                              # for string operations\n\nfrom nltk.corpus import stopwords          # module for stop words that come with NLTK\nfrom nltk.stem import PorterStemmer        # module for stemming\nfrom nltk.tokenize import TweetTokenizer   # module for tokenizing strings","b984deb9":"print('\\033[92m' + tweet)\nprint('\\033[94m')\n\n# remove old style retweet text \"RT\"\ntweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n\n# remove hyperlinks\ntweet2 = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet2)\n\n# remove hashtags\n# only removing the hash # sign from the word\ntweet2 = re.sub(r'#', '', tweet2)\n\nprint(tweet2)","a0740d99":"print()\nprint('\\033[92m' + tweet2)\nprint('\\033[94m')\n\n# instantiate tokenizer class\ntokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n                               reduce_len=True)\n\n# tokenize tweets\ntweet_tokens = tokenizer.tokenize(tweet2)\n\nprint()\nprint('Tokenized string:')\nprint(tweet_tokens)","fcc36623":"#Import the english stop words list from NLTK\nstopwords_english = stopwords.words('english') \n\nprint('Stop words\\n')\nprint(stopwords_english)\n\nprint('\\nPunctuation\\n')\nprint(string.punctuation)","14e843cb":"print()\nprint('\\033[92m')\nprint(tweet_tokens)\nprint('\\033[94m')\n\ntweets_clean = []\n\nfor word in tweet_tokens: # Go through every word in your tokens list\n    if (word not in stopwords_english and  # remove stopwords\n        word not in string.punctuation):  # remove punctuation\n        tweets_clean.append(word)\n\nprint('removed stop words and punctuation:')\nprint(tweets_clean)","53e3ddb5":"print()\nprint('\\033[92m')\nprint(tweets_clean)\nprint('\\033[94m')\n\n# Instantiate stemming class\nstemmer = PorterStemmer() \n\n# Create an empty list to store the stems\ntweets_stem = [] \n\nfor word in tweets_clean:\n    stem_word = stemmer.stem(word)  # stemming word\n    tweets_stem.append(stem_word)  # append to the list\n\nprint('stemmed words:')\nprint(tweets_stem)","66f35d5b":"# What Next\nI have thought to list all notebooks that I used to learn NLP so that one can understand the basics of NLP and can enhance himself from ZERO-to-HERO in NLP.<br>\n**So if you want to read more notebooks stay tuned with me**.\n","52c9fa86":"### Stemming\n\nStemming is the process of converting a word to its most general form, or stem. This helps in reducing the size of our vocabulary.\n\nConsider the words: \n * **learn**\n * **learn**ing\n * **learn**ed\n * **learn**t\n \nAll these words are stemmed from its common root **learn**. However, in some cases, the stemming process produces words that are not correct spellings of the root word. For example, **happi** and **sunni**. That's because it chooses the most common stem for related words. For example, we can look at the set of words that comprises the different forms of happy:\n\n * **happ**y\n * **happi**ness\n * **happi**er\n \nWe can see that the prefix **happi** is more commonly used. We cannot choose **happ** because it is the stem of unrelated words like **happen**.\n \nNLTK has different modules for stemming and we will be using the [PorterStemmer](https:\/\/www.nltk.org\/api\/nltk.stem.html#module-nltk.stem.porter) module which uses the [Porter Stemming Algorithm](https:\/\/tartarus.org\/martin\/PorterStemmer\/). Let's see how we can use it in the cell below.","c71cb3bc":"That's it! Now we have a set of words we can feed into to the next stage of our machine learning project.","8ed8c7ee":"# Preprocessing\n\nwe will be exploring how to preprocess tweets for sentiment analysis. You will see how to use the [NLTK](http:\/\/www.nltk.org) package to perform a preprocessing pipeline for Twitter datasets.","bff35301":"# Future Work\nAll concepts from Basic To Advanced with research Papers is aimed to write here.","c1e8d159":"Next, we'll print a report with the number of positive and negative tweets. It is also essential to know the data structure of the datasets","0df46979":"## Looking at raw texts\n\nBefore anything else, we can print a couple of tweets from the dataset to see how they look. Understanding the data is responsible for 80% of the success or failure in data science projects. We can use this time to observe aspects we'd like to consider when preprocessing our data.\n\nBelow, you will print one random positive and one random negative tweet. We have added a color mark at the beginning of the string to further distinguish the two. (Warning: This is taken from a public dataset of real tweets and a very small portion has explicit content.)","88dde481":"# Table Of Contents\n<br>\n1. Intro to preprocesssing and twitter dataset<br>\n2. Tokenizing the string<br>\n3. Lowercasing<br>\n4. Removing stop words and punctuation<br>\n5. Stemming<br>","a4fc5bf2":"I have started a series of NLP for beginners and for those who want to understand behind the scenes and Maths of NLP.<br>\nCurrently I am doing Course of Natural Language Procesing on Coursera i.e. best course for understanding basics and to strong your foundation so I thought who is on this kaggle platform can also get knowledge about basics and maths of NLP. So I have started this.<br>\nThere are more to come. <br>\n                           ","cebcb99d":"That's it for this lab! You now know what is going on when you call the preprocessing helper function in this week's assignment. Hopefully, this exercise has also given you some insights on how to tweak this for other types of text datasets.","bd0ae7d2":"# This is Notebook-1 of NLP(Zero-To-Hero) series.","33fca599":"### Tokenize the string\n\nTo tokenize means to split the strings into individual words without blanks or tabs. In this same step, we will also convert each word in the string to lower case. The [tokenize](https:\/\/www.nltk.org\/api\/nltk.tokenize.html#module-nltk.tokenize.casual) module from NLTK allows us to do these easily:","885ca1c1":"Let's import a few more libraries for this purpose.","6b7cb799":"One observation you may have is the presence of [emoticons](https:\/\/en.wikipedia.org\/wiki\/Emoticon) and URLs in many of the tweets. This info will come in handy in the next steps.","4bd5f38b":"### Remove stop words and punctuations\n\nThe next step is to remove stop words and punctuation. Stop words are words that don't add significant meaning to the text. You'll see the list provided by NLTK when you run the cells below.","f091c895":"We can see that the data is stored in a list and as you might expect, individual tweets are stored as strings.\n\nYou can make a more visually appealing report by using Matplotlib's [pyplot](https:\/\/matplotlib.org\/tutorials\/introductory\/pyplot.html) library. Let us see how to create a [pie chart](https:\/\/matplotlib.org\/3.2.1\/gallery\/pie_and_polar_charts\/pie_features.html#sphx-glr-gallery-pie-and-polar-charts-pie-features-py) to show the same information as above. This simple snippet will serve you in future visualizations of this kind of data.","094e5028":"## About the Twitter dataset\n\nThe sample dataset from NLTK is separated into positive and negative tweets. It contains 5000 positive tweets and 5000 negative tweets exactly. The exact match between these classes is not a coincidence. The intention is to have a balanced dataset. That does not reflect the real distributions of positive and negative classes in live Twitter streams. It is just because balanced datasets simplify the design of most computational methods that are required for sentiment analysis. However, it is better to be aware that this balance of classes is artificial. \n","a3a66f50":"### Remove hyperlinks,  Twitter marks and styles\n\nSince we have a Twitter dataset, we'd like to remove some substrings commonly used on the platform like the hashtag, retweet marks, and hyperlinks. We'll use the [re](https:\/\/docs.python.org\/3\/library\/re.html) library to perform regular expression operations on our tweet. We'll define our search pattern and use the `sub()` method to remove matches by substituting with an empty character (i.e. `''`)","1c710c7c":"# Audience\nThis tutorial uses NLTK to preprocess text. It is absolute beginner friendly. So if you have no backgroud of NLP then there is no problem. Only the basic python programming understanding is required for this tutorial.","e6cba892":"Data preprocessing is one of the critical steps in any machine learning project. It includes cleaning and formatting the data before feeding into a machine learning algorithm. For NLP, the preprocessing steps are comprised of the following tasks:\n\n* Tokenizing the string\n* Lowercasing\n* Removing stop words and punctuation\n* Stemming\n\nLet's see how we can do these to a given tweet. We will choose just one and see how this is transformed by each preprocessing step.","18ca1f9c":"## Preprocess raw text for Sentiment analysis","13c797c3":"## Setup\n\nYou will be doing sentiment analysis on tweets in the first two weeks of this course. To help with that, we will be using the [Natural Language Toolkit (NLTK)](http:\/\/www.nltk.org\/howto\/twitter.html) package, an open-source Python library for natural language processing. It has modules for collecting, handling, and processing Twitter data, and you will be acquainted with them as we move along the course.\n\nFor this exercise, we will use a Twitter dataset that comes with NLTK. This dataset has been manually annotated and serves to establish baselines for models quickly. Let us import them now as well as a few other libraries we will be using.","2c17ebec":"We can load the text fields of the positive and negative tweets by using the module's `strings()` method like this:","f40a480a":"We can see that the stop words list above contains some words that could be important in some contexts. \nThese could be words like _i, not, between, because, won, against_. You might need to customize the stop words list for some applications. For our exercise, we will use the entire list.\n\nFor the punctuation, we saw earlier that certain groupings like ':)' and '...'  should be retained when dealing with tweets because they are used to express emotions. In other contexts, like medical analysis, these should also be removed.\n\nTime to clean up our tokenized tweet!","86331b87":"Please note that the words **happy** and **sunny** in this list are correctly spelled. ","9e1a250f":"# Reference \nThis Noteboook is based on Coursera Course \u201cNatural Language Processing Specialization\u201d by Deeplearning.ai . Instructors of this specialization are Younes Bensouda Mourri , \u0141ukasz Kaiser and Eddy Shyu"}}