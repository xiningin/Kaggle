{"cell_type":{"e48ccbea":"code","2ab0c01c":"code","376fb1cc":"code","304f7c4d":"code","50c4d849":"code","062f1a33":"code","cffb926f":"code","3cd95ba5":"code","feb52854":"markdown","a31b13d8":"markdown","fd6c77d3":"markdown","356fe10c":"markdown","09a8e598":"markdown","fb22894e":"markdown","661bf6d9":"markdown","9e7c2bb5":"markdown","fbc6fa39":"markdown"},"source":{"e48ccbea":"import joblib\nimport torch\nimport torch.nn as nn\nimport transformers\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\n\nfrom tqdm import tqdm\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup","2ab0c01c":"class config:\n    MAX_LEN = 128\n    TRAIN_BATCH_SIZE = 32\n    VALID_BATCH_SIZE = 8\n    EPOCHS = 3\n    BASE_MODEL_PATH = \"..\/input\/bert-base-uncased\/\"\n    MODEL_PATH = \"model.bin\"\n    TRAINING_FILE = \"..\/input\/entity-annotated-corpus\/ner_dataset.csv\"\n    TOKENIZER = transformers.BertTokenizer.from_pretrained(\n        BASE_MODEL_PATH,\n        do_lower_case=True\n    )","376fb1cc":"class EntityDataset:\n    def __init__(self, texts, pos, tags):\n        self.texts = texts\n        self.pos = pos\n        self.tags = tags\n    \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, item):\n        text = self.texts[item]\n        pos = self.pos[item]\n        tags = self.tags[item]\n\n        ids = []\n        target_pos = []\n        target_tag =[]\n\n        for i, s in enumerate(text):\n            inputs = config.TOKENIZER.encode(\n                s,\n                add_special_tokens=False\n            )\n            \n            input_len = len(inputs)\n            ids.extend(inputs)\n            target_pos.extend([pos[i]] * input_len)\n            target_tag.extend([tags[i]] * input_len)\n\n        ids = ids[:config.MAX_LEN - 2]\n        target_pos = target_pos[:config.MAX_LEN - 2]\n        target_tag = target_tag[:config.MAX_LEN - 2]\n\n        ids = [101] + ids + [102]\n        target_pos = [0] + target_pos + [0]\n        target_tag = [0] + target_tag + [0]\n\n        mask = [1] * len(ids)\n        token_type_ids = [0] * len(ids)\n\n        padding_len = config.MAX_LEN - len(ids)\n\n        ids = ids + ([0] * padding_len)\n        mask = mask + ([0] * padding_len)\n        token_type_ids = token_type_ids + ([0] * padding_len)\n        target_pos = target_pos + ([0] * padding_len)\n        target_tag = target_tag + ([0] * padding_len)\n\n        return {\n            \"ids\": torch.tensor(ids, dtype=torch.long),\n            \"mask\": torch.tensor(mask, dtype=torch.long),\n            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n            \"target_pos\": torch.tensor(target_pos, dtype=torch.long),\n            \"target_tag\": torch.tensor(target_tag, dtype=torch.long),\n        }","304f7c4d":"def train_fn(data_loader, model, optimizer, device, scheduler):\n    model.train()\n    final_loss = 0\n    for data in tqdm(data_loader, total=len(data_loader)):\n        for k, v in data.items():\n            data[k] = v.to(device)\n        optimizer.zero_grad()\n        _, _, loss = model(**data)\n        loss.backward()\n        optimizer.step()\n        scheduler.step()\n        final_loss += loss.item()\n    return final_loss \/ len(data_loader)\n\n\ndef eval_fn(data_loader, model, device):\n    model.eval()\n    final_loss = 0\n    for data in tqdm(data_loader, total=len(data_loader)):\n        for k, v in data.items():\n            data[k] = v.to(device)\n        _, _, loss = model(**data)\n        final_loss += loss.item()\n    return final_loss \/ len(data_loader)","50c4d849":"def loss_fn(output, target, mask, num_labels):\n    lfn = nn.CrossEntropyLoss()\n    active_loss = mask.view(-1) == 1\n    active_logits = output.view(-1, num_labels)\n    active_labels = torch.where(\n        active_loss,\n        target.view(-1),\n        torch.tensor(lfn.ignore_index).type_as(target)\n    )\n    loss = lfn(active_logits, active_labels)\n    return loss\n\n\nclass EntityModel(nn.Module):\n    def __init__(self, num_tag, num_pos):\n        super(EntityModel, self).__init__()\n        self.num_tag = num_tag\n        self.num_pos = num_pos\n        self.bert = transformers.BertModel.from_pretrained(\n            config.BASE_MODEL_PATH\n        )\n        self.bert_drop_1 = nn.Dropout(0.3)\n        self.bert_drop_2 = nn.Dropout(0.3)\n        self.out_tag = nn.Linear(768, self.num_tag)\n        self.out_pos = nn.Linear(768, self.num_pos)\n    \n    def forward(\n        self, \n        ids, \n        mask, \n        token_type_ids, \n        target_pos, \n        target_tag\n    ):\n        o1, _ = self.bert(\n            ids, \n            attention_mask=mask, \n            token_type_ids=token_type_ids\n        )\n\n        bo_tag = self.bert_drop_1(o1)\n        bo_pos = self.bert_drop_2(o1)\n\n        tag = self.out_tag(bo_tag)\n        pos = self.out_pos(bo_pos)\n\n        loss_tag = loss_fn(tag, target_tag, mask, self.num_tag)\n        loss_pos = loss_fn(pos, target_pos, mask, self.num_pos)\n\n        loss = (loss_tag + loss_pos) \/ 2\n\n        return tag, pos, loss","062f1a33":"def process_data(data_path):\n    df = pd.read_csv(data_path, encoding=\"latin-1\")\n    df.loc[:, \"Sentence #\"] = df[\"Sentence #\"].fillna(method=\"ffill\")\n\n    enc_pos = preprocessing.LabelEncoder()\n    enc_tag = preprocessing.LabelEncoder()\n\n    df.loc[:, \"POS\"] = enc_pos.fit_transform(df[\"POS\"])\n    df.loc[:, \"Tag\"] = enc_tag.fit_transform(df[\"Tag\"])\n\n    sentences = df.groupby(\"Sentence #\")[\"Word\"].apply(list).values\n    pos = df.groupby(\"Sentence #\")[\"POS\"].apply(list).values\n    tag = df.groupby(\"Sentence #\")[\"Tag\"].apply(list).values\n    return sentences, pos, tag, enc_pos, enc_tag","cffb926f":"sentences, pos, tag, enc_pos, enc_tag = process_data(config.TRAINING_FILE)\n\nmeta_data = {\n    \"enc_pos\": enc_pos,\n    \"enc_tag\": enc_tag\n}\n\njoblib.dump(meta_data, \"meta.bin\")\n\nnum_pos = len(list(enc_pos.classes_))\nnum_tag = len(list(enc_tag.classes_))\n\n(\n    train_sentences,\n    test_sentences,\n    train_pos,\n    test_pos,\n    train_tag,\n    test_tag\n) = model_selection.train_test_split(\n    sentences, \n    pos, \n    tag, \n    random_state=42, \n    test_size=0.1\n)\n\ntrain_dataset = EntityDataset(\n    texts=train_sentences, pos=train_pos, tags=train_tag\n)\n\ntrain_data_loader = torch.utils.data.DataLoader(\n    train_dataset, batch_size=config.TRAIN_BATCH_SIZE, num_workers=4\n)\n\nvalid_dataset = EntityDataset(\n    texts=test_sentences, pos=test_pos, tags=test_tag\n)\n\nvalid_data_loader = torch.utils.data.DataLoader(\n    valid_dataset, batch_size=config.VALID_BATCH_SIZE, num_workers=1\n)\n\ndevice = torch.device(\"cuda\")\nmodel = EntityModel(num_tag=num_tag, num_pos=num_pos)\nmodel.to(device)\n\nparam_optimizer = list(model.named_parameters())\nno_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\noptimizer_parameters = [\n    {\n        \"params\": [\n            p for n, p in param_optimizer if not any(\n                nd in n for nd in no_decay\n            )\n        ],\n        \"weight_decay\": 0.001,\n    },\n    {\n        \"params\": [\n            p for n, p in param_optimizer if any(\n                nd in n for nd in no_decay\n            )\n        ],\n        \"weight_decay\": 0.0,\n    },\n]\n\nnum_train_steps = int(\n    len(train_sentences) \/ config.TRAIN_BATCH_SIZE * config.EPOCHS\n)\noptimizer = AdamW(optimizer_parameters, lr=3e-5)\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=0, \n    num_training_steps=num_train_steps\n)\n\nbest_loss = np.inf\nfor epoch in range(config.EPOCHS):\n    train_loss = train_fn(\n        train_data_loader, \n        model, \n        optimizer, \n        device, \n        scheduler\n    )\n    test_loss = eval_fn(\n        valid_data_loader,\n        model,\n        device\n    )\n    print(f\"Train Loss = {train_loss} Valid Loss = {test_loss}\")\n    if test_loss < best_loss:\n        torch.save(model.state_dict(), config.MODEL_PATH)\n        best_loss = test_loss","3cd95ba5":"meta_data = joblib.load(\"meta.bin\")\nenc_pos = meta_data[\"enc_pos\"]\nenc_tag = meta_data[\"enc_tag\"]\n\nnum_pos = len(list(enc_pos.classes_))\nnum_tag = len(list(enc_tag.classes_))\n\nsentence = \"\"\"\nIndependence Day is celebrated annually on 15 August as a national holiday in India commemorating the nation's independence from the United Kingdom on 15 August 1947\n\"\"\"\ntokenized_sentence = config.TOKENIZER.encode(sentence)\n\nsentence = sentence.split()\nprint(sentence)\nprint(tokenized_sentence)\n\ntest_dataset = EntityDataset(\n    texts=[sentence], \n    pos=[[0] * len(sentence)], \n    tags=[[0] * len(sentence)]\n)\n\ndevice = torch.device(\"cuda\")\nmodel = EntityModel(num_tag=num_tag, num_pos=num_pos)\nmodel.load_state_dict(torch.load(config.MODEL_PATH))\nmodel.to(device)\n\nwith torch.no_grad():\n    data = test_dataset[0]\n    for k, v in data.items():\n        data[k] = v.to(device).unsqueeze(0)\n    tag, pos, _ = model(**data)\n\n    print(\n        enc_tag.inverse_transform(\n            tag.argmax(2).cpu().numpy().reshape(-1)\n        )[:len(tokenized_sentence)]\n    )\n    print(\n        enc_pos.inverse_transform(\n            pos.argmax(2).cpu().numpy().reshape(-1)\n        )[:len(tokenized_sentence)]\n    )","feb52854":"## Training","a31b13d8":"## Dataset","fd6c77d3":"## Data processing","356fe10c":"## Some config","09a8e598":"## Import everything important","fb22894e":"## Loss function and model","661bf6d9":"## Inference","9e7c2bb5":"# Entity extraction using BERT\n\nReference -https:\/\/www.kaggle.com\/abhishek\/entity-extraction-model-using-bert-pytorch","fbc6fa39":"## Training and evaluation functions"}}