{"cell_type":{"10df57c8":"code","7a569f77":"code","9b8f47d0":"code","c66f78f0":"code","459715e5":"code","bf892329":"code","012a7625":"code","4ff03fa7":"code","32e3a567":"code","ee4c1f4d":"code","036ed5a9":"code","fc93180b":"code","e988ad9c":"code","696a7464":"code","5014ba5d":"code","2f9a11c0":"code","974f3a0b":"code","de0f7fde":"code","30431004":"code","1e8c34c7":"code","98dbc1e1":"code","a09538db":"code","1bcf1cd2":"code","ba650856":"code","37ef25fc":"code","ddc32e80":"code","3fd9e286":"code","3a115033":"code","54773687":"code","8d092918":"code","60222284":"code","86029ee4":"code","c47f376b":"code","94336a89":"code","03da43a4":"code","d5a802b4":"code","b21a06d4":"markdown","e88acc7e":"markdown","f2548edf":"markdown","d2d6559a":"markdown","200454a8":"markdown","b77389ba":"markdown","9263a3c1":"markdown","505fb248":"markdown","2aa9f54f":"markdown","e83b8fd2":"markdown","75923c6d":"markdown","63157587":"markdown","882b29d4":"markdown","a0590538":"markdown","1e837bd1":"markdown","c80f6ed3":"markdown","23f22cde":"markdown","a19aeca8":"markdown","f2cb5d89":"markdown","2624c0ed":"markdown","cd6521d0":"markdown","df919a29":"markdown","60414d72":"markdown"},"source":{"10df57c8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt \n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7a569f77":"# loading the dataset\ndf = pd.read_csv(\"\/kaggle\/input\/heart-failure-prediction\/heart.csv\")\ndf.head()","9b8f47d0":"df.shape","c66f78f0":"df.describe()","459715e5":"df.info()","bf892329":"# to check for missing values\ndf.isna().sum()","012a7625":"# observe the various categories in the categorical variables\ndf['ChestPainType'].unique(), df['ExerciseAngina'].unique(), df['RestingECG'].unique(), df['ST_Slope'].unique(), df['Sex'].unique()","4ff03fa7":"df['Cholesterol'].replace(0,df['Cholesterol'].median(), inplace=True)\ndf['RestingBP'].replace(0,df['RestingBP'].median(), inplace=True)","32e3a567":"df.describe()","ee4c1f4d":"num_df = df.select_dtypes(include=np.number)\nnum_df.head()","036ed5a9":"cat_df = df.select_dtypes(exclude=np.number)\ncat_df.head()","fc93180b":"plt.figure(figsize=(20,35))\nfor i, col in enumerate(num_df.columns):\n        ax = plt.subplot(9, 2, i+1)\n        sns.kdeplot(df[col], ax=ax)\n        plt.xlabel(col)\n        \nplt.show()","e988ad9c":"cat_dum=pd.get_dummies(cat_df,drop_first=True)\ncat_dum.head()","696a7464":"# we concat the numerical dataframe with the categorical dataframe\nfinal_df=pd.concat([cat_dum,num_df],axis=1)\nfinal_df.head()","5014ba5d":"y=final_df['HeartDisease']\ny.head()","2f9a11c0":"X=final_df.iloc[: , :-1]\nX.head()","974f3a0b":"# we split the data into test and train\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15, random_state = 0)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","de0f7fde":"# we scale the data\ncols = X_train.columns\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nX_train = pd.DataFrame(X_train, columns=[cols])\nX_test = pd.DataFrame(X_test, columns=[cols])\nX_train.head()","30431004":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)","1e8c34c7":"y_pred = knn.predict(X_test)\ny_pred","98dbc1e1":"from sklearn.metrics import accuracy_score\nprint('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","a09538db":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred)\n\nimport seaborn as sns\nsns.heatmap(cm, annot=True)\n#print('Confusion matrix\\n\\n', cm)\n#print('\\nTrue Positives(TP) = ', cm[0,0])\n#print('\\nTrue Negatives(TN) = ', cm[1,1])\n#print('\\nFalse Positives(FP) = ', cm[0,1])\n#print('\\nFalse Negatives(FN) = ', cm[1,0])","1bcf1cd2":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","ba650856":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(solver='liblinear', random_state=0)\nlogreg.fit(X_train, y_train)\ny_pred_test = logreg.predict(X_test)\ny_pred_test","37ef25fc":"print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred_test)))","ddc32e80":"cm = confusion_matrix(y_test, y_pred_test)\nsns.heatmap(cm, annot=True)\n'''\nprint('Confusion matrix\\n\\n', cm)\nprint('\\nTrue Positives(TP) = ', cm[0,0])\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\nprint('\\nFalse Positives(FP) = ', cm[0,1])\nprint('\\nFalse Negatives(FN) = ', cm[1,0])'''","3fd9e286":"print(classification_report(y_test, y_pred_test))","3a115033":"from sklearn.ensemble import RandomForestClassifier\nclf_tree=RandomForestClassifier(criterion='gini', n_estimators=100)\nclf_tree.fit(X_train, y_train)\ny_pred_tree = clf_tree.predict(X_test)\ny_pred_tree","54773687":"print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred_tree)))","8d092918":"clf_tree.feature_importances_\nplt.figure(figsize=(10,6))\nplt.barh(X.columns.values, clf_tree.feature_importances_)\nplt.xlabel(\"Random Forest Feature Importance\")","60222284":"cm = confusion_matrix(y_test, y_pred_tree)\nsns.heatmap(cm, annot=True)\n'''\nprint('Confusion matrix\\n\\n', cm)\nprint('\\nTrue Positives(TP) = ', cm[0,0])\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\nprint('\\nFalse Positives(FP) = ', cm[0,1])\nprint('\\nFalse Negatives(FN) = ', cm[1,0])'''","86029ee4":"print(classification_report(y_test, y_pred_tree))","c47f376b":"from sklearn import svm\nclf = svm.SVC(kernel='poly') \nclf.fit(X_train, y_train)\ny_pred_svm = clf.predict(X_test)\ny_pred_svm","94336a89":"print('Model accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred_svm)))","03da43a4":"cm = confusion_matrix(y_test, y_pred_svm)\nsns.heatmap(cm, annot=True)\n'''\nprint('Confusion matrix\\n\\n', cm)\nprint('\\nTrue Positives(TP) = ', cm[0,0])\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\nprint('\\nFalse Positives(FP) = ', cm[0,1])\nprint('\\nFalse Negatives(FN) = ', cm[1,0])'''","d5a802b4":"print(classification_report(y_test, y_pred_svm))","b21a06d4":"### Logistic Regression\nWe will now move to logistic regression. Logistic regression is used for binary classification. In this problem, logistic regression classifies if the patient has a heart disease or no. It is interesting to note that logistic regression does not need feature scaling. This is because the logistic regression algorithm is dependent on the coefficients of the variables and not the magnitude of the variables. Therefore, the above scaling had no effect on the model's performance.","e88acc7e":"Random forest classifier also provided us with feature selection, which tells us which which features contribute most during the training phase. The sum of all the feature selection score equals to one. ","f2548edf":"From the graphs, we observe that Age, Resting BP, Cholestrol and Maximum HR resembles more or less like a normal distribution curve. Cholestrol is a bit right skewed.","d2d6559a":"It is observed that the dataset doesn't have any null values.","200454a8":"It is observed that we obtain an accuracy of 85.51% using SVM model.","b77389ba":"We now replace the minimum values of 'Cholestrol' and 'Resting BP' with the median values","9263a3c1":"It is seen that the minimum values of 'Resting BP' and 'Cholestrol' has been replaced by their median values.","505fb248":"### Random Forest Classifier","2aa9f54f":"It is observed that we obtain an accuracy of 85.51% using random forest classifier.","e83b8fd2":"We now segregate the numerical variables and the categorical variables. Separation is necessary because we treat numerical and categorical data differently in data wrangling.\n\nFor numerical data, we check for mean, median, we check for missing values, we replace those missing values. We observe the distribution, we study correlation and perform normalization and standardization.\n\nFor categorical data, we check for missing values, we use label encoding, dummies or one hot encoding.","75923c6d":"It is observed that 'Fasting BS' is a binary variable, which has values of 0s and 1s. It is strange to observe that the minimum values of 'Resting BP' and 'Cholestrol' is 0. We will later replace it with the median of the respective columns. We choose the median to avoid the influence of outliers.","63157587":"We create dummy variables for the categorical variables","882b29d4":"SVM like KNN is a distance based algorithm. ","a0590538":"We now plot the distribution of the various numerical variables","1e837bd1":"It is observed that we get an accuracy of 85.51% through KNN.","c80f6ed3":"Data scaling is usally done by normalization or standardization. \n\nNormalization scales the data in such a manner that it becomes a floating point value between the range 0 and 1.\n\nStandard scaler transforms the data in such a manner that it has mean as 0 and standard deviation as 1. In short, it standardizes the data. Standardization is useful for data which has negative values. It arranges the data in a standard normal distribution. It is more useful in classification than regression. Thus we use standard scalar here. Standard scalar is usually used when there is a wide variation among the data points.","23f22cde":"### KNN\nThe first algorithm that we will be using is the KNN (K- nearest neighbours). It is a distance based algorithm. This algorithm esentially uses distances between data points to determine similarities. Standard scaling has made sure that the data is not biased towards a certain feature due to its high magnitude. Standard scaling ensures that all features contribute equally to the result.\n\nIn short KNN algorithm calculates the distance between each data point. It then selects K closest neighbors from the point and then votes the most frequent labels. Thus we usually keep K value as an odd integer. For example, if the algorithm has found 3 points, out of which 2 are 'red' and 1 is 'yellow', it choses the red colour for the data point. ","a19aeca8":"The dataset has 918 rows or observations and 12 columns or features.","f2cb5d89":"Random forest classifier is an esemble bagging algorithm. The algorithm creates multiple decision trees on randomly sampled decision trees and then selects the best prediction from those decision trees by means of voting. It is interesting to note that the random forest classifier algorithm does not suffer from over fitting problem. The biases are cancelled out due to the use of many decision trees.","2624c0ed":"### SVM","cd6521d0":"It can be observed that the most important features are: ST_Slop_UP, MaxHR, Oldpeak and ST_Slope_Flat. The least important features are RestingECG_ST and ChestPainType_TA.","df919a29":"Let us check if the values have been replaced by the median values ","60414d72":"It is observed that we get an accuracy of 83.33% using logistic regression."}}