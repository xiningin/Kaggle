{"cell_type":{"95d5375a":"code","14045d20":"code","66a41c58":"code","19738e2c":"code","70d28ad8":"code","fae67c8d":"code","d5a4c9bb":"code","5e8e1b9d":"code","47c01b37":"code","88939e1b":"code","480e5ff9":"code","1dc31fde":"code","ee9eea26":"code","e0b3d2bb":"code","48edb5b5":"code","39470139":"code","cbb5a4e4":"code","fdb39674":"code","3ccb26bf":"code","4ba48112":"code","be69a65a":"code","fec72135":"code","5413de33":"code","38774639":"code","b6fb911d":"code","5d873cfe":"code","f52cd560":"code","207b53d4":"code","d4f856e5":"code","8b3dbe91":"code","fd484de2":"code","0e0ddd64":"code","e2d2f122":"code","0ad88c26":"code","d1dfe5a9":"code","fc23adc2":"code","afa604a3":"code","16cf4b19":"code","94b5a772":"code","2f7080a7":"code","46c0adbf":"code","d1fad092":"code","775f6973":"code","3f8fc372":"code","20aea95b":"code","28351716":"code","93dff73b":"code","79a06881":"code","79f0756a":"code","dd16a2b1":"code","0e3e5f5f":"code","a8a7411c":"code","2c387ef3":"code","62d869f6":"code","c1249692":"code","e41cf676":"code","df3866bc":"code","5f80964c":"code","443d6767":"code","e9ab8e93":"code","d69e792c":"code","8ab640de":"code","cb62b238":"code","e5786e68":"code","44820c8f":"code","450f48c6":"code","5bc57c94":"code","37e04bb7":"code","53eab933":"code","a2361438":"code","87fc39c4":"code","672c76fb":"code","1d7ac6f8":"code","72e3ebb4":"code","5357cc04":"code","65d169b7":"code","e84344cc":"code","f4541207":"code","ee4fa0bd":"code","e2561a28":"code","c4eb74c6":"markdown","75a68762":"markdown","a51c59cd":"markdown","c95ab88b":"markdown","4437701a":"markdown","b5ca6429":"markdown","ef3846a0":"markdown","7ad5f0c7":"markdown","1944f363":"markdown","ed28ce6c":"markdown","9daaab04":"markdown","e22ba2b8":"markdown","a0411bc1":"markdown","526c2e13":"markdown","4f159019":"markdown","7f7c5e10":"markdown","b7d75577":"markdown","1c98ca05":"markdown","9e3ce385":"markdown","9abd5e3c":"markdown","8cfa9133":"markdown","2b80afa1":"markdown","9d27f83c":"markdown","f6974bb1":"markdown","b17b3e11":"markdown","82d3cc87":"markdown","d7a5b2ea":"markdown","e6c07f16":"markdown","a177a1e2":"markdown","8bdeebf7":"markdown","f581dd67":"markdown","35318e4d":"markdown","09fd85e2":"markdown","e2d7a98d":"markdown","02da15c6":"markdown","ba008455":"markdown","baca5ad2":"markdown","5a543813":"markdown","59dbefbb":"markdown","7c98fa17":"markdown","fa8b6886":"markdown","618e1dbb":"markdown","a64bf50b":"markdown","a7781627":"markdown","e84aae51":"markdown","56f500c8":"markdown","13288622":"markdown","ad8b1b1f":"markdown","61c7f18a":"markdown","d06829d6":"markdown"},"source":{"95d5375a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\n\n%matplotlib inline\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","14045d20":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","66a41c58":"train.head()","19738e2c":"test.head()","70d28ad8":"train.describe()","fae67c8d":"train.info()","d5a4c9bb":"dataset = pd.concat((train, test))","5e8e1b9d":"dataset = dataset.fillna(np.nan)\ndataset.isnull().sum()","47c01b37":"g = sns.heatmap(train[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","88939e1b":"g = sns.factorplot(x=\"SibSp\",y='Survived',data=train,kind=\"bar\", size = 6, \n)\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","480e5ff9":"g  = sns.factorplot(x=\"Parch\",y=\"Survived\",data=train,kind=\"bar\", size = 6,)\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","1dc31fde":"dataset['Age'] = dataset['Age'].fillna(dataset['Age'].mean())","ee9eea26":"g = sns.FacetGrid(train, col='Survived')\ng = g.map(sns.distplot, \"Age\")","e0b3d2bb":"dataset[\"Fare\"] = dataset[\"Fare\"].fillna(dataset[\"Fare\"].mean())","48edb5b5":"sns.distplot(dataset['Fare'])\nprint(dataset[\"Fare\"].skew())","39470139":"dataset[\"Fare\"] = dataset[\"Fare\"].map(lambda x: np.log(x) if x > 0 else 0)","cbb5a4e4":"sns.distplot(dataset['Fare'])\nprint(dataset[\"Fare\"].skew())","fdb39674":"g = sns.barplot(x=\"Sex\",y=\"Survived\",data=train)\ng = g.set_ylabel(\"Survival Probability\")","3ccb26bf":"train[[\"Sex\",\"Survived\"]].groupby('Sex').mean()","4ba48112":"g = sns.factorplot(x=\"Pclass\",y=\"Survived\",data=train,kind=\"bar\", size = 6)\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","be69a65a":"dataset[\"Embarked\"] = dataset[\"Embarked\"].fillna(\"S\")","fec72135":"g = sns.factorplot(x=\"Embarked\", y=\"Survived\",  data=train,\n                   size=6, kind=\"bar\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","5413de33":"dataset_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in dataset[\"Name\"]]\ndataset[\"Title\"] = pd.Series(dataset_title)\ndataset[\"Title\"].value_counts()","38774639":"dataset[\"Title\"] = dataset[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndataset[\"Title\"] = dataset[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ndataset[\"Title\"] = dataset[\"Title\"].astype(int)","b6fb911d":"g = sns.countplot(dataset[\"Title\"])\ng = g.set_xticklabels([\"Master\",\"Miss\/Ms\/Mme\/Mlle\/Mrs\",\"Mr\",\"Rare\"])","5d873cfe":"g = sns.factorplot(x=\"Title\",y=\"Survived\",data=dataset,kind=\"bar\")\ng = g.set_xticklabels([\"Master\",\"Miss-Mrs\",\"Mr\",\"Rare\"])\ng = g.set_ylabels(\"survival probability\")","f52cd560":"dataset[\"FamilySize\"] = dataset[\"SibSp\"] + dataset[\"Parch\"] + 1\ndataset['Single'] = dataset['FamilySize'].map(lambda s: 1 if s == 1 else 0)\ndataset['SmallF'] = dataset['FamilySize'].map(lambda s: 1 if  s == 2  else 0)\ndataset['MedF'] = dataset['FamilySize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndataset['LargeF'] = dataset['FamilySize'].map(lambda s: 1 if s >= 5 else 0)","207b53d4":"dataset['Cabin'].describe()","d4f856e5":"dataset['Cabin'].isnull().sum()","8b3dbe91":"dataset[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in dataset['Cabin'] ])","fd484de2":"dataset[\"Ticket\"].head()","0e0ddd64":"Ticket = []\nfor i in list(dataset.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0])\n    else:\n        Ticket.append(\"X\")\n        \ndataset[\"Ticket\"] = Ticket\ndataset[\"Ticket\"].head()","e2d2f122":"dataset = pd.get_dummies(dataset, columns = [\"Sex\", \"Title\"])\ndataset = pd.get_dummies(dataset, columns = [\"Embarked\"], prefix=\"Em\")\ndataset = pd.get_dummies(dataset, columns = [\"Ticket\"], prefix=\"T\")\ndataset[\"Pclass\"] = dataset[\"Pclass\"].astype(\"category\")\ndataset = pd.get_dummies(dataset, columns = [\"Pclass\"],prefix=\"Pc\")\ndataset = pd.get_dummies(dataset, columns = [\"Cabin\"],prefix=\"Cabin\")","0ad88c26":"dataset.drop(labels = [\"Name\"], axis = 1, inplace = True)\ndataset.drop(labels = [\"PassengerId\"], axis = 1, inplace = True)","d1dfe5a9":"dataset.head(10)","fc23adc2":"X_train = dataset[:train.shape[0]]\nX_test = dataset[train.shape[0]:]\ny = train['Survived']","afa604a3":"X_train = X_train.drop(labels='Survived', axis=1)\nX_test = X_test.drop(labels='Survived', axis=1)","16cf4b19":"from sklearn.preprocessing import StandardScaler","94b5a772":"headers_train = X_train.columns\nheaders_test = X_test.columns","2f7080a7":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","46c0adbf":"pd.DataFrame(X_train, columns=headers_train).head()","d1fad092":"pd.DataFrame(X_test, columns=headers_test).head()","775f6973":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_val_score","3f8fc372":"cv = StratifiedShuffleSplit(n_splits = 10, test_size = .25, random_state = 0 )\naccuracies = cross_val_score(LogisticRegression(solver='liblinear'), X_train, y, cv  = cv)\nprint (\"Cross-Validation accuracy scores:{}\".format(accuracies))\nprint (\"Mean Cross-Validation accuracy score: {}\".format(round(accuracies.mean(),5)))","20aea95b":"from sklearn.model_selection import GridSearchCV","28351716":"C_vals = [0.2,0.3,0.4,0.5,1,5,10]\n\npenalties = ['l1','l2']\n\ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25)\n\n\nparam = {'penalty': penalties, 'C': C_vals}\n\nlogreg = LogisticRegression(solver='liblinear')\n \ngrid = GridSearchCV(estimator=LogisticRegression(), \n                           param_grid = param,\n                           scoring = 'accuracy',\n                            n_jobs =-1,\n                           cv = cv\n                          )\n\ngrid.fit(X_train, y)","93dff73b":"print (grid.best_score_)\nprint (grid.best_params_)\nprint(grid.best_estimator_)","79a06881":"logreg_grid = grid.best_estimator_\nlogreg_score = round(logreg_grid.score(X_train,y), 4)\nlogreg_score","79f0756a":"from sklearn.neighbors import KNeighborsClassifier","dd16a2b1":"k_range = range(1,31)\nweights_options=['uniform','distance']\nparam = {'n_neighbors':k_range, 'weights':weights_options}\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.30, random_state=15)\ngrid = GridSearchCV(KNeighborsClassifier(), param,cv=cv,verbose = False, n_jobs=-1)\n\ngrid.fit(X_train,y)","0e3e5f5f":"print (grid.best_score_)\nprint (grid.best_params_)\nprint(grid.best_estimator_)","a8a7411c":"knn_grid= grid.best_estimator_\nknn_score = round(knn_grid.score(X_train,y), 4)\nknn_score","2c387ef3":"from sklearn.naive_bayes import GaussianNB\n\ncv = StratifiedShuffleSplit(n_splits = 10, test_size = .25, random_state = 0 )\naccuracies = cross_val_score(GaussianNB(), X_train, y, cv  = cv)\nprint (\"Cross-Validation accuracy scores:{}\".format(accuracies))\nprint (\"Mean Cross-Validation accuracy score: {}\".format(round(accuracies.mean(),5)))\n\nbayes_score = round(accuracies.mean(), 4)\nbayes_score","62d869f6":"from sklearn.svm import SVC\n\nC = [0.1, 1,1.5]\ngammas = [0.001, 0.01, 0.1]\nkernels = ['rbf', 'poly', 'sigmoid']\nparam_grid = {'C': C, 'gamma' : gammas, 'kernel' : kernels}\n\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.25, random_state=8)\n\ngrid_search = GridSearchCV(SVC(probability=True), param_grid, cv=cv)\ngrid_search.fit(X_train,y)","c1249692":"print(grid_search.best_score_)\nprint(grid_search.best_params_)\nprint(grid_search.best_estimator_)","e41cf676":"svm_grid = grid_search.best_estimator_\nsvm_score = round(svm_grid.score(X_train,y), 4)\nsvm_score","df3866bc":"from sklearn.tree import DecisionTreeClassifier\nmax_depth = range(1,31)\nmax_feature = [21,22,23,24,25,26,28,29,30,'auto']\ncriterion=[\"entropy\", \"gini\"]\n\nparam = {'max_depth':max_depth, \n         'max_features':max_feature, \n         'criterion': criterion}\n\ncv=StratifiedShuffleSplit(n_splits=10, test_size =.25, random_state=9)\n\ngrid = GridSearchCV(DecisionTreeClassifier(), \n                                param_grid = param, \n                                 verbose=False, \n                                 cv=cv,\n                                n_jobs = -1)\ngrid.fit(X_train, y) ","5f80964c":"print( grid.best_params_)\nprint (grid.best_score_)\nprint (grid.best_estimator_)","443d6767":"dectree_grid = grid.best_estimator_\ndectree_score = round(dectree_grid.score(X_train,y), 4)\ndectree_score","e9ab8e93":"import os     \nos.environ[\"PATH\"] += os.pathsep + 'C:\/Program Files (x86)\/Graphviz2.38\/bin\/'\n\nfrom sklearn.externals.six import StringIO\nfrom sklearn.tree import export_graphviz\nimport pydot\nfrom IPython.display import Image\ndot_data = StringIO()  \nexport_graphviz(dectree_grid, out_file=dot_data,  \n                feature_names=headers_train,  class_names = ([\"Survived\" if int(i) is 1 else \"Not_survived\" for i in y.unique()]),\n                filled=True, rounded=True,\n                proportion=True,\n                special_characters=True)  \n(graph,) = pydot.graph_from_dot_data(dot_data.getvalue())\nImage(graph.create_png())","d69e792c":"from sklearn.ensemble import RandomForestClassifier\nn_estimators = [140,145,150];\nmax_depth = range(1,10);\ncriterions = ['gini', 'entropy'];\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.25, random_state=10)\n\n\nparameters = {'n_estimators':n_estimators,\n              'max_depth':max_depth,\n              'criterion': criterions\n              \n        }\ngrid = GridSearchCV(estimator=RandomForestClassifier(max_features='auto'),\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X_train,y)","8ab640de":"print (grid.best_score_)\nprint (grid.best_params_)\nprint (grid.best_estimator_)","cb62b238":"rf_grid = grid.best_estimator_\nrf_score = round(rf_grid.score(X_train,y), 4)\nrf_score","e5786e68":"from xgboost import XGBClassifier","44820c8f":"XGBClassifier = XGBClassifier(colsample_bytree = 0.3, subsample = 0.7, reg_lambda = 1)\n\n#colsample_bytree = [0.3, 0.5]\n#subsample = [0.7, 1]\nn_estimators = [400, 450]\nmax_depth = [2,3,4]\nlearning_rate = [0.01, 0.1]\nreg_alpha = [0, 0.0001, 0.0005]\n#reg_lambda = [0.3, 1, 5]\ncv = StratifiedShuffleSplit(n_splits=10, test_size=.25, random_state=13)\n\n\nparameters = {#'colsample_bytree':colsample_bytree,\n              #'subsample': subsample,\n              'n_estimators':n_estimators,\n              'max_depth':max_depth,\n              'learning_rate':learning_rate,\n              'reg_alpha':reg_alpha,\n              #'reg_lambda':reg_lambda\n        }\ngrid = GridSearchCV(estimator=XGBClassifier,\n                                 param_grid=parameters,\n                                 cv=cv,\n                                 n_jobs = -1)\ngrid.fit(X_train,y)","450f48c6":"print (grid.best_score_)\nprint (grid.best_params_)\nprint (grid.best_estimator_)","5bc57c94":"xgb_grid = grid.best_estimator_\nxgb_score = round(xgb_grid.score(X_train,y), 4)\nxgb_score","37e04bb7":"from keras.models import Sequential\nfrom keras.layers.core import Dense\nimport keras\nfrom keras.optimizers import *\nfrom keras.initializers import *","53eab933":"NN_train = dataset[:train.shape[0]]\nNN_test = dataset[train.shape[0]:]\nNN_y = train['Survived'].values\n#NN_y = NN_y.reshape(-1,1)\n\nNN_train = NN_train.drop(labels='Survived', axis=1)\nNN_test = NN_test.drop(labels='Survived', axis=1)","a2361438":"sc = StandardScaler()\nNN_train = sc.fit_transform(NN_train.values)\nNN_test = sc.transform(NN_test.values)","87fc39c4":"n_cols = NN_train.shape[1]","672c76fb":"model = Sequential()","1d7ac6f8":"model.add(Dense(128, activation='relu', input_shape=(n_cols,)))\n\nmodel.add(Dense(128, activation=\"elu\"))\nmodel.add(Dense(256, activation=\"elu\"))\nmodel.add(Dense(128, activation=\"elu\"))\nmodel.add(keras.layers.Dropout(0.3))\n\nmodel.add(Dense(512, activation=\"elu\"))\nmodel.add(Dense(1024, activation=\"elu\"))\nmodel.add(Dense(512, activation=\"elu\"))\nmodel.add(keras.layers.Dropout(0.3))\n\nmodel.add(Dense(1024, activation=\"elu\"))\nmodel.add(Dense(2048, activation=\"elu\"))\nmodel.add(Dense(1024, activation=\"elu\"))\nmodel.add(keras.layers.Dropout(0.3))\n\nmodel.add(Dense(512, activation=\"elu\"))\nmodel.add(Dense(1024, activation=\"elu\"))\nmodel.add(Dense(512, activation=\"elu\"))\nmodel.add(keras.layers.Dropout(0.3))\n\nmodel.add(Dense(256, activation=\"elu\"))\nmodel.add(Dense(128, activation=\"elu\"))\nmodel.add(Dense(64, activation=\"elu\"))\nmodel.add(Dense(32, activation=\"elu\"))\nmodel.add(keras.layers.Dropout(0.3))\n\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.compile(optimizer=\"Adam\", loss='binary_crossentropy', metrics=[\"binary_accuracy\"])","72e3ebb4":"model.summary()","5357cc04":"#from keras.callbacks import EarlyStopping\n#early_stopping_monitor = EarlyStopping(patience = 10)","65d169b7":"model_result = model.fit(NN_train, NN_y, batch_size=100, epochs=200, validation_split = 0.25, shuffle = True)","e84344cc":"keras_score = round(max(model_result.history[\"val_binary_accuracy\"]), 4)\nkeras_score","f4541207":"results = pd.DataFrame({\n    'Model': ['Logistic Regression', 'KNN', 'Naive Bayes','Support Vector Machines',   \n               'Decision Tree', 'Random Forest', 'XGBoost', 'Keras'],\n    'Score': [logreg_score, knn_score, bayes_score, \n              svm_score, dectree_score, rf_score, xgb_score, keras_score]})\nresults.sort_values(by='Score', ascending=False)\n#print df.to_string(index=False)","ee4fa0bd":"predict = logreg_grid.predict(X_test)","e2561a28":"my_submission = pd.DataFrame({'PassengerId': test.PassengerId, 'Survived': predict})\nmy_submission.to_csv('submission__logreg.csv', index=False)","c4eb74c6":"## Results","75a68762":"### Decision Tree","a51c59cd":"Age and Cabin features have an important part of missing values. Missing values in Survived correspond to the join testing dataset(Survived column doesn't exist in test set ). ","c95ab88b":"Only Fare feature seems to have a significative correlation with the survival probability.","4437701a":"## Categorical values","b5ca6429":"Small families have more chance to survive, more than single (Parch 0), medium (Parch 3,4) and large families (Parch 5,6 ).","ef3846a0":"### Family size","7ad5f0c7":"## Modeling","1944f363":"## Feature Scaling","ed28ce6c":"We have two missing values , i decided to fill them with the most fequent value of \"Embarked\" (S)","9daaab04":"Skewness is reduced after the log transformation","e22ba2b8":"### Random Forest","a0411bc1":"## Splitting the training data","526c2e13":"Cross validation has also been applied.","4f159019":"Distributions are not the same in the survived and not survived subpopulations. There is a peak corresponding to young passengers, that have survived.","7f7c5e10":"All of these categorical variables are important for prediction","b7d75577":"It seems that passengers having a lot of siblings\/spouses have less chance to survive","1c98ca05":"Input shape was added according to the number of features of train dataset after preprocessing. Adam was used as the optimaseira, binary_accuracy was used as the metrics","9e3ce385":"### Keras","9abd5e3c":"I also implement classifications neural network using the keras library","8cfa9133":"## Feature engineering\n","2b80afa1":"You can see that the highest accuracy scores are XGBoost, Keras. Other models have similar accuracy of the estimate. While Naive Bayes predicts the worst of all other models. ","9d27f83c":"Combine train and test datasets for further work with features","f6974bb1":"The Name feature contains information on passenger's title.","b17b3e11":"## Load and check data","82d3cc87":"### Cabin","d7a5b2ea":"It seems that passenger coming from Cherbourg (C) have more chance to survive.","e6c07f16":"Fare distribution is very skewed. This can lead to overweigth very high values in the model. We need to use log transformation to Fare","a177a1e2":"### Name","8bdeebf7":"### Ticket","f581dd67":"### Encoding categorical variables","35318e4d":"**Although the logistic regression did not show the best result on the training data, it is the one that has the highest submission accuracy. Therefore, I decided to use it as the main model**","09fd85e2":"### Naive Bayes","e2d7a98d":"We have only one missing value. I decided to fill it with the mean value","02da15c6":"### KNN","ba008455":"## Numerical values","baca5ad2":"There is 17 titles in the dataset, most of them are very rare and we can group them in 4 categories","5a543813":"First class passengers have more chance to survive than second class and third class passengers.","59dbefbb":"Women are more likely to survive than men","7c98fa17":"Male have less chance to survive than Femal","fa8b6886":"I decided to replace the Ticket feature column by the ticket prefixe","618e1dbb":"### XGBoost","a64bf50b":"Datasets after scaling","a7781627":"## Prediction","e84aae51":"Check for null and missing values","56f500c8":"### SVM","13288622":"As models, I used the most popular methods such as Logistic Regression, KNN, SVM, Random Forest and etc. In addition, a grid approach was applied to each model to optimize parameters and cross validation to check the model prediction on test data.","ad8b1b1f":"### Logistic Regression","61c7f18a":" Family size feature which is the sum of SibSp , Parch and 1 (including the passenger). I decided to created 4 categories of family size.","d06829d6":"The Cabin feature column contains 295 values and 10014 missing values. Replace the Cabin number by the type of cabin 'X' if not"}}