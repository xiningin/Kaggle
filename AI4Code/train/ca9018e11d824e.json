{"cell_type":{"0728c771":"code","c852d34c":"code","44e8b3e9":"code","533d9537":"code","341acc0e":"code","44bdda57":"code","32bf55d3":"code","809ddb14":"code","13865708":"code","dc06ff24":"code","ec897b37":"code","b824632e":"code","3cc10dcc":"code","aba3c4b0":"code","71ee4959":"code","7d35f343":"code","b02f5f6c":"code","939b7aeb":"code","d5ac0a22":"code","af7c0c28":"code","706fb64f":"code","775c1e57":"code","b8fa5312":"code","6eba0ad5":"code","b55cee16":"code","3a6e9730":"code","fe819a68":"code","3f6ef920":"code","c0af9a0a":"markdown","c7cb5ffd":"markdown","3cbcd806":"markdown","408c633a":"markdown","8504811f":"markdown","a24d2fde":"markdown","61272ad6":"markdown","86d13c5f":"markdown","9ad483c0":"markdown","a51151a3":"markdown","30caa124":"markdown"},"source":{"0728c771":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\n\nfrom tqdm import tqdm_notebook as tqdm\n\nimport fastai\nfrom fastai.vision import *\nfrom fastai.callbacks import *\nfrom fastai.utils.mem import *\nfrom fastai.vision.gan import *\nimport gc\nfrom torchvision.models import vgg16_bn","c852d34c":"## These folders contain crappy images in different resolution with differnt crappafication logic (randomly selected)\norig_path = Path('..\/input\/flickrproc\/hr\/hr')\nfnames_df = pd.read_csv('..\/input\/flickrproc\/files.csv')\nbs = 16\nFOLDERS = {256:Path('..\/input\/flickrproc\/crappy_256\/crappy\/'), 320:Path('..\/input\/flickrproc\/crappy_320\/crappy\/'), }\nFOLDERS","44e8b3e9":"##loading training data\n## if dummy=True is provided, then dataset of ony 32 images is retured\ndef get_data(size=None, bs=None, folder=320, split=0.9, dummy=False):\n    if dummy:\n        if bs is None: bs = 1\n            \n        if size is None: \n            data = ImageImageList.from_df(fnames_df.iloc[:32], path = FOLDERS[320], cols='name').split_by_rand_pct(0.2, seed=34).label_from_func(lambda x: orig_path\/Path(x).name).databunch(bs=bs).normalize(imagenet_stats, do_y=True)\n        else:\n            data = ImageImageList.from_df(fnames_df.iloc[:32], path = FOLDERS[320], cols='name').split_by_rand_pct(0.2, seed=34).label_from_func(lambda x: orig_path\/Path(x).name).transform([], size=size, tfm_y=True).databunch(bs=bs).normalize(imagenet_stats, do_y=True)\n        data.c = 3\n        return data\n    \n    if bs is None: \n        raise ValueError('Batchsize is not provided')\n    if size is None:\n        raise ValueError('Size of image is not provided')\n    \n    folder = FOLDERS[folder]\n    src = ImageImageList.from_df(fnames_df, \n                           path = folder, cols='name')\n    src = src.split_by_idx(np.arange(int(src.items.shape[0]*split), src.items.shape[0]))\n    \n    data = src.label_from_func(lambda x: orig_path\/Path(x).name).transform(get_transforms(max_zoom=1.2), size=size, tfm_y=True).databunch(bs=bs).normalize(imagenet_stats, do_y=True)\n    data.c = 3\n    return data","533d9537":"base_loss = F.l1_loss\narch = models.resnet34\n\nvgg_m = vgg16_bn(True).features.cuda().eval()\nrequires_grad(vgg_m, False)\nblocks = [i-1 for i,o in enumerate(children(vgg_m)) if isinstance(o,nn.MaxPool2d)]\n\ndef gram_matrix(x):\n    n,c,h,w = x.size()\n    x = x.view(n, c, -1)\n    return (x @ x.transpose(1,2))\/(c*h*w)","341acc0e":"class FeatureLoss(nn.Module):\n    def __init__(self, m_feat, layer_ids, layer_wgts):\n        super().__init__()\n        self.m_feat = m_feat\n        self.loss_features = [self.m_feat[i] for i in layer_ids]\n        self.hooks = hook_outputs(self.loss_features, detach=False)\n        self.wgts = layer_wgts\n        self.metric_names = ['pixel',] + [f'feat_{i}' for i in range(len(layer_ids))\n              ] + [f'gram_{i}' for i in range(len(layer_ids))]\n\n    def make_features(self, x, clone=False):\n        self.m_feat(x)\n        return [(o.clone() if clone else o) for o in self.hooks.stored]\n    \n    def forward(self, input, target):\n        out_feat = self.make_features(target, clone=True)\n        in_feat = self.make_features(input)\n        self.feat_losses = [base_loss(input,target)]\n        self.feat_losses += [base_loss(f_in, f_out)*w\n                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n        self.feat_losses += [base_loss(gram_matrix(f_in), gram_matrix(f_out))*w**2 * 5e3\n                             for f_in, f_out, w in zip(in_feat, out_feat, self.wgts)]\n        self.metrics = dict(zip(self.metric_names, self.feat_losses))\n        return sum(self.feat_losses)\n    \n    def __del__(self): self.hooks.remove()","44bdda57":"feat_loss = FeatureLoss(vgg_m, blocks[2:5], [5,15,2])","32bf55d3":"gen_data = get_data(bs=bs, size=128, folder=320)","809ddb14":"wd = 1e-3\ny_range = (-3.,3.)\n\nlearn_gen = unet_learner(gen_data, arch, wd=wd, loss_func=feat_loss, callback_fns=LossMetrics, blur=True, norm_type=NormType.Weight, model_dir=\"\/kaggle\/working\", y_range=y_range)\ngc.collect();","13865708":"learn_gen.load('..\/input\/pretrain-gan-feature-loss\/gen_pre')","dc06ff24":"learn_gen.show_results(rows=10, figsize=(24, 100))","ec897b37":"def get_critic_data(bs, size=256, split=0.9):\n    \n    def labeler(x):\n        ret = 'generated' if Path(x).parent.name == 'crappy' else 'original'\n        return ret\n    \n    df = fnames_df\n    valid_names = list(df['name'].iloc[int(split*len(df)):])\n    \n    src1 = ImageList.from_df(df, path = Path('..\/input\/flickrproc\/crappy_320')\/'crappy', cols='name')\n    src2 = ImageList.from_df(df, path = orig_path, cols='name')\n    src1.add(items=src2)\n    \n    src = src1.split_by_valid_func(lambda x : Path(x).name in valid_names)\n    data = src.label_from_func(labeler)\n    data = data.transform(get_transforms(), size=size).databunch(bs=bs).normalize(imagenet_stats)\n    \n    data.c = 3\n    return data","b824632e":"data_critic = get_critic_data(bs, 128)","3cc10dcc":"data_critic.show_batch()","aba3c4b0":"loss_critic = AdaptiveLoss(nn.BCEWithLogitsLoss())\ndef create_critic_learner(data, metrics):\n    return   Learner(data_critic, gan_critic(), metrics=metrics, loss_func=loss_critic, wd=wd, model_dir=\"\/kaggle\/working\")\nlearn_critic = create_critic_learner(data_critic, accuracy_thresh_expand)\nlearn_critic.load('..\/input\/pretrain-gan-feature-loss\/critic-pre')","71ee4959":"del learn_gen.data\nlearn_gen.data = get_data(bs=bs, size=128, folder=320)\n\ngc.collect()\ntorch.cuda.empty_cache()\ngpu_mem_get_free()","7d35f343":"switcher = partial(AdaptiveGANSwitcher, critic_thresh=0.65)\nlearn = GANLearner.from_learners(learn_gen, learn_critic, weights_gen=(1.,50.), show_img=True, switcher=switcher,\n                                 opt_func=partial(optim.Adam, betas=(0.,0.99)), wd=wd, model_dir=\"\/kaggle\/working\", gen_first=True)\nlearn.callback_fns.append(partial(GANDiscriminativeLR, mult_lr=5.))","b02f5f6c":"lr = 2e-4","939b7aeb":"learn.fit(7,lr)","d5ac0a22":"learn.show_results(rows=20, )\n","af7c0c28":"learn_gen.save('gen-128')\nlearn_critic.save('critic-128')","706fb64f":"del learn.data\nlearn.data = get_data(256,bs\/\/2, 320)\ngc.collect()\ntorch.cuda.empty_cache()\ngpu_mem_get_free()","775c1e57":"learn.show_results(rows=20, figsize=(30, 100))","b8fa5312":"learn.fit(7,lr\/2)","6eba0ad5":"learn.show_results(rows=20, figsize=(30, 100))","b55cee16":"learn_gen.save('gen-256')\nlearn_critic.save('critic-256')","3a6e9730":"learn_gen.export(\"\/kaggle\/working\/export.pkl\")","fe819a68":"!ls","3f6ef920":"print('finish')","c0af9a0a":"## How good is pretrained model?","c7cb5ffd":"A DCGAN model is trained for image enhancement(superres, decrappify). <br>The dataset used is Flicker Image Dataset, availabe on Kaggle.\nFor training the model synthetic data is generated as in kernel: https:\/\/www.kaggle.com\/greenahn\/crappify-imgs<br>and saved to disk, which in conjunction with high resolution images are used to train the model.<br><br>\nThis model is trained by incorporating feature generated from vgg16 model in loss function, as in paper on neural art transfer.<br>\nFor more details, find the github repository at: https:\/\/github.com\/nupam\/GANs-for-Image-enhancement","3cbcd806":"Training is done by adaptiveliy switching between discriminator and generator.<br>Discriminator is trained whenever discriminator loss drops below 0.65.","408c633a":"Increasing image size and training again","8504811f":"Saving models","a24d2fde":"# Training GAN with pretrained models","61272ad6":"## Loading critic","86d13c5f":"Both pretrained generator and discriminator models are loaded from disk, output file of kernel,<br> https:\/\/www.kaggle.com\/greenahn\/pretrain-gan-feature-loss.<br>\nThey are then put together as a GAN, and trained.","9ad483c0":"### GAN\n**Putting both models together as a GAN**","a51151a3":"### Feature loss\nAs in paper on neural art tranfer, https:\/\/arxiv.org\/abs\/1508.06576.\nL1 pixel distance is also added to loss.<br>\nPrevents mode collapse and supervises for stabe and faster training.","30caa124":"## Loading Generator"}}