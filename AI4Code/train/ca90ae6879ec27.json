{"cell_type":{"b5016621":"code","d31f6ed7":"code","52700be0":"code","31a314a0":"code","b1327d25":"code","2d2cdfd1":"code","8a9ba871":"code","8fd1a8fa":"code","a06fa5d6":"code","0659318e":"code","b169d077":"code","b70f4e2a":"code","aeacee87":"code","8ed6c486":"code","8fafa35a":"code","6c4fcb83":"code","37495551":"code","2146f5c1":"code","6b1a0a36":"code","90792781":"code","65239881":"code","140e22bd":"code","d351f2f9":"code","2a5957ca":"code","bf103517":"code","3e0c031d":"code","5a67224e":"code","bc658050":"code","aa74e7f9":"code","7c1ccfdf":"code","64f6d4a5":"code","244ea466":"code","cd08c4fd":"code","a18b8234":"code","086fe1c3":"code","ec9d58dc":"code","4c6fd44b":"code","0c7cd733":"code","b2d262b1":"code","80f46c4c":"code","69ba7aa7":"code","71d25706":"code","8c8352ee":"code","8e588a93":"code","1ae69fa4":"code","8f42d4e7":"code","611a605e":"code","897ead03":"code","ec7e9caf":"code","b60a4c6f":"code","bb355449":"code","f8dcda67":"code","d9ce0bfc":"code","a74932ca":"code","73e817de":"code","c4ba0e7b":"code","2b7a5e88":"markdown","cfe3a5f9":"markdown","6c06b15d":"markdown","e497ef77":"markdown","3d5b2239":"markdown","df5677bf":"markdown","04049dde":"markdown","bc53665a":"markdown","26e0a1a8":"markdown","04012ec6":"markdown","99576be4":"markdown","cbb77a29":"markdown","c58c4598":"markdown","d2946c2c":"markdown","677d19be":"markdown","719beec1":"markdown","b2c73253":"markdown","7ed7ef61":"markdown","29014e1b":"markdown","0badd6e1":"markdown","9872d506":"markdown","22b1e4c4":"markdown","b2377473":"markdown","ea05364f":"markdown","5cf449b3":"markdown","a8180981":"markdown","e7e239da":"markdown","969a699b":"markdown","1fde42de":"markdown","999de6a7":"markdown","d1567902":"markdown","3a63ce76":"markdown","b218450a":"markdown","9c192f6d":"markdown","facedc51":"markdown","cad49a5b":"markdown","93a57ab1":"markdown","95ad0d5f":"markdown","226900e7":"markdown","48c86f37":"markdown"},"source":{"b5016621":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","d31f6ed7":"# Suppress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","52700be0":"auto = pd.read_csv(\"..\/input\/CarPricePrediction.csv\")\nauto.head()","31a314a0":"auto.info()","b1327d25":"# auto.rename(columns={\"CarName\":\"company\"})\n\nauto[\"CarName\"] = auto.CarName.map(lambda x: x.split(\" \", 1)[0])\n\n# As we have some redundant data in carName lets fix it \nauto.CarName = auto['CarName'].str.lower()\nauto['CarName'] = auto['CarName'].str.replace('vw','volkswagen')\nauto['CarName'] = auto['CarName'].str.replace('vokswagen','volkswagen')\nauto['CarName'] = auto['CarName'].str.replace('toyouta','toyota')\nauto['CarName'] = auto['CarName'].str.replace('porcshce','porsche')\nauto['CarName'] = auto['CarName'].str.replace('maxda','mazda')\nauto['CarName'] = auto['CarName'].str.replace('maxda','mazda')\n\nauto.CarName.unique()","2d2cdfd1":"auto.info()","8a9ba871":"sns.set(font_scale=2)\nsns.pairplot(auto)\nplt.show()","8fd1a8fa":"plt.figure(figsize = (20, 20))\nsns.set(font_scale=1.25)\nsns.heatmap(auto.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","a06fa5d6":"plt.figure(figsize=(10,10))\nsns.boxplot(y='CarName', x='price', data = auto)","0659318e":"plt.figure(figsize=(20,20))\n\nplt.subplot(3,3,1)\nsns.boxplot(x='fueltype', y='price', data = auto)\n\nplt.subplot(3,3,2)\nsns.boxplot(x='aspiration', y='price', data = auto)\n\nplt.subplot(3,3,3)\nsns.boxplot(x='doornumber', y='price', data = auto)\n\nplt.subplot(3,3,4)\nsns.boxplot(x='enginelocation', y='price', data = auto)\n\nplt.subplot(3,3,5)\nsns.boxplot(x='drivewheel', y='price', data = auto)\n\nplt.subplot(3,3,6)\nsns.boxplot(x='enginetype', y='price', data = auto)\n\nplt.subplot(3,3,7)\nsns.boxplot(x='cylindernumber', y='price', data = auto)\n\nplt.subplot(3,3,8)\nsns.boxplot(x='fuelsystem', y='price', data = auto)\n\nplt.subplot(3,3,9)\nsns.boxplot(x='carbody', y='price', data = auto)\n","b169d077":"auto.drop(['car_ID'], axis =1, inplace = True)","b70f4e2a":"# Converting Yes to 1 and No to 0\nauto['fueltype'] = auto['fueltype'].map({'gas': 1, 'diesel': 0})\nauto['aspiration'] = auto['aspiration'].map({'std': 1, 'turbo': 0})\nauto['doornumber'] = auto['doornumber'].map({'two': 1, 'four': 0})\nauto['enginelocation'] = auto['enginelocation'].map({'front': 1, 'rear': 0})","aeacee87":"auto.drop(['carwidth','curbweight','wheelbase','highwaympg'], axis =1, inplace = True)\nauto.info()","8ed6c486":"\n# # drivewheel, enginetype, cylindernumber, fuelsystem, carbody\n\n# dummy_drivewheel = pd.get_dummies(auto['drivewheel'], drop_first = True)\n# dummy_enginetype = pd.get_dummies(auto['enginetype'], drop_first = True)\n# dummy_cylindernumber = pd.get_dummies(auto['cylindernumber'], drop_first = True)\n# dummy_fuelsystem = pd.get_dummies(auto['fuelsystem'], drop_first = True)\n# dummy_carbody = pd.get_dummies(auto['carbody'], drop_first = True)\n\n\n# ## Concatenating all the Dummy Variables to the DataFrame\n\n# auto = pd.concat([auto, dummy_drivewheel,dummy_enginetype, dummy_cylindernumber, dummy_fuelsystem, dummy_carbody], axis = 1)\n\n\n# # Dropping the Columns where Dummy variables have been created\n# auto.drop(['drivewheel'],axis = 1, inplace=True)\n# auto.drop(['enginetype'],axis = 1, inplace=True)\n# auto.drop(['cylindernumber'],axis = 1, inplace=True)\n# auto.drop(['fuelsystem'],axis = 1, inplace=True)\n# auto.drop(['carbody'],axis = 1, inplace=True)\n# auto.head()","8fafa35a":"df = pd.get_dummies(auto)\ndf.columns\ndf.head()","6c4fcb83":"#defining a normalisation function \ncols_to_norm = ['symboling', 'carlength', 'carheight', \n         'enginesize', 'boreratio', 'stroke', 'compressionratio','horsepower', 'peakrpm', 'citympg', 'price']\n# Normalising only the numeric fields \nnormalised_df = df[cols_to_norm].apply(lambda x: (x-np.mean(x))\/ (max(x) - min(x)))\nnormalised_df.head()\n\ndf['symboling'] = normalised_df['symboling']\ndf['carlength'] = normalised_df['carlength']\ndf['carheight'] = normalised_df['carheight']\ndf['enginesize'] = normalised_df['enginesize']\ndf['boreratio'] = normalised_df['boreratio']\ndf['stroke'] = normalised_df['stroke']\ndf['price'] = normalised_df['price']\ndf['compressionratio'] = normalised_df['compressionratio']\ndf['horsepower'] = normalised_df['horsepower']\ndf['peakrpm']= normalised_df['peakrpm']\ndf['citympg'] = normalised_df['citympg']\ndf.head()\n","37495551":"from sklearn.model_selection import train_test_split","2146f5c1":"y = df.pop('price')\nX = df","6b1a0a36":"X.info()","90792781":"from sklearn.model_selection import train_test_split\n\nnp.random.seed(0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7 ,test_size = 0.3, random_state=100)","65239881":"from sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n\n# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\n\n# Running RFE -Recursive Feature Elimination\nrfe = RFE(lm, 15)             \nrfe = rfe.fit(X_train, y_train)\n\n# Printing the boolean results\nprint(rfe.support_)           \nprint(rfe.ranking_)  ","140e22bd":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","d351f2f9":"# variables to be dropped\ncol = X_train.columns[~rfe.support_] \ncol","2a5957ca":"X_train.columns\nX_train.drop(col,1, inplace = True)\nX_train.columns","bf103517":"plt.figure(figsize = (15, 15))\nsns.set(font_scale=1.0)\nsns.heatmap(X_train.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","3e0c031d":"X_train_1 = X_train","5a67224e":"# Add a constant\nX_train_1 = sm.add_constant(X_train_1)\n\n# Create Model1\nlr1 = sm.OLS(y_train, X_train_1).fit()\nprint(lr1.params)\nprint(lr1.summary())","bc658050":"from statsmodels.stats.outliers_influence import variance_inflation_factor","aa74e7f9":"# Function to create a dataframe that will contain the names of all the feature variables and their respective VIFs\n\ndef calculate_VIF(data_frame):\n    vif = pd.DataFrame(columns = ['Features', 'VIF'])\n    vif['Features'] = data_frame.columns\n    vif['VIF'] = [variance_inflation_factor(data_frame.values, i) for i in range(data_frame.shape[1])]\n    vif['VIF'] = round(vif['VIF'], 2)\n    vif = vif.sort_values(by = \"VIF\", ascending = False)\n    return vif","7c1ccfdf":"# Model1 VIF\ncalculate_VIF(X_train)","64f6d4a5":"plt.figure(figsize = (15, 15))\nsns.set(font_scale=1)\nsns.heatmap(X_train.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","244ea466":"X_train_2 = X_train_1.drop('enginetype_rotor', 1)\n\n# Add a constant\nX_train_2 = sm.add_constant(X_train_2)\n\n# Create Model1\nlr2 = sm.OLS(y_train, X_train_2).fit()\nprint(lr2.params)\nprint(lr2.summary())","cd08c4fd":"# Model2 VIF\nX_train.drop('enginetype_rotor', axis =1, inplace = True)\ncalculate_VIF(X_train)","a18b8234":"X_train_3 = X_train_2.drop('cylindernumber_eight', 1)\n\n# Add a constant\nX_train_3 = sm.add_constant(X_train_3)\n\n# Create Model1\nlr3 = sm.OLS(y_train, X_train_3).fit()\nprint(lr3.params)\nprint(lr3.summary())","086fe1c3":"# Model3 VIF\nX_train.drop('cylindernumber_eight', axis =1, inplace = True)\ncalculate_VIF(X_train)","ec9d58dc":"X_train_4 = X_train_3.drop('enginetype_dohcv', 1)\n\n# Add a constant\nX_train_4 = sm.add_constant(X_train_4)\n\n# Create Model1\nlr4 = sm.OLS(y_train, X_train_4).fit()\nprint(lr4.params)\nprint(lr4.summary())","4c6fd44b":"# Model4 VIF\nX_train.drop('enginetype_dohcv', axis =1, inplace = True)\ncalculate_VIF(X_train)","0c7cd733":"X_train_5 = X_train_4.drop('cylindernumber_four', 1)\n\n# Add a constant\nX_train_5 = sm.add_constant(X_train_5)\n\n# Create Model1\nlr5 = sm.OLS(y_train, X_train_5).fit()\nprint(lr5.params)\nprint(lr5.summary())","b2d262b1":"# Model5 VIF\nX_train.drop('cylindernumber_four', axis =1, inplace = True)\ncalculate_VIF(X_train)","80f46c4c":"X_train_6 = X_train_5.drop('cylindernumber_twelve', 1)\n\n# Add a constant\nX_train_6 = sm.add_constant(X_train_6)\n\n# Create Model1\nlr6 = sm.OLS(y_train, X_train_6).fit()\nprint(lr6.params)\nprint(lr6.summary())","69ba7aa7":"# Model6 VIF\nX_train.drop('cylindernumber_twelve', axis =1, inplace = True)\ncalculate_VIF(X_train)","71d25706":"plt.figure(figsize = (10, 10))\nsns.set(font_scale=1)\nsns.heatmap(X_train.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","8c8352ee":"X_train_7 = X_train_6.drop('stroke', 1)\n\n# Add a constant\nX_train_7 = sm.add_constant(X_train_7)\n\n# Create Model1\nlr7 = sm.OLS(y_train, X_train_7).fit()\nprint(lr7.params)\nprint(lr7.summary())","8e588a93":"# Model7 VIF\nX_train.drop('stroke', axis =1, inplace = True)\ncalculate_VIF(X_train)","1ae69fa4":"X_train_8 = X_train_7.drop('boreratio', 1)\n\n# Add a constant\nX_train_8 = sm.add_constant(X_train_8)\n\n# Create Model1\nlr8 = sm.OLS(y_train, X_train_8).fit()\nprint(lr8.params)\nprint(lr8.summary())","8f42d4e7":"# Model8 VIF\nX_train.drop('boreratio', axis =1, inplace = True)\ncalculate_VIF(X_train)","611a605e":"X_train_9 = X_train_8.drop('cylindernumber_three', 1)\n\n# Add a constant\nX_train_9 = sm.add_constant(X_train_9)\n\n# Create Model1\nlr9 = sm.OLS(y_train, X_train_9).fit()\nprint(lr9.params)\nprint(lr9.summary())","897ead03":"# Model9 VIF\nX_train.drop('cylindernumber_three', axis =1, inplace = True)\ncalculate_VIF(X_train)","ec7e9caf":"y_train_price = lr9.predict(X_train_9)","b60a4c6f":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_price), bins = 10)\n# Plot heading \nfig.suptitle('Error Terms', fontsize = 20)                  \nplt.xlabel('Errors', fontsize = 18)    ","bb355449":"X_train.columns","f8dcda67":"X_test_m9 =  X_test[['carlength', 'enginesize', 'CarName_audi', 'CarName_bmw','CarName_buick', 'CarName_porsche', 'cylindernumber_two']]\nX_test_m9.head()\n","d9ce0bfc":"# Adding  constant variable to test dataframe\nX_test_m9 = sm.add_constant(X_test_m9)\n\ny_pred_m9 = lr9.predict(X_test_m9)","a74932ca":"# Actual vs Predicted\nc = [i for i in range(1,63,1)]\n\nfig = plt.figure()\nplt.figure(figsize = (6, 6))\n\nplt.plot(c,y_test, color=\"red\", linewidth=2, linestyle=\"-\")     \nplt.plot(c,y_pred_m9, color=\"green\",  linewidth=2, linestyle=\"-\")  \n\nfig.suptitle('Actual vs Predicted', fontsize=15)              \n\nplt.xlabel('Index', fontsize=15)                              \nplt.ylabel('Car Price', fontsize=15)  ","73e817de":"#Plotting y_test and y_pred to understand the spread.\nfig = plt.figure()\nplt.figure(figsize = (5,5))\nplt.scatter(y_test,y_pred_m9)\nfig.suptitle('y_test vs y_pred', fontsize=20)              # Plot heading \nplt.xlabel('y_test', fontsize=18)                          # X-label\nplt.ylabel('y_pred', fontsize=16)     ","c4ba0e7b":"from sklearn.metrics import r2_score\nr2_score(y_test, y_pred_m9)","2b7a5e88":"## Step 1: Reading and Understanding the Data","cfe3a5f9":"The R-squared score indicates `Good Fit`\n\n`Train = .90`\n\n`Test = 0.898 = 0.90`\n","6c06b15d":"### `train_test_split` ","e497ef77":"### all the p-value are zero and VIF are below 3, we will predict on this model","3d5b2239":"#### `Visualising all the Numeric Variables with Pairplot`","df5677bf":"### Buliding Model by dropping columns after RFE `Top-Down` approach)","04049dde":"##  `Model 5 ` \n\n`cylindernumber_four` has high p-value, dropping `cylindernumber_four`","bc53665a":"### `Creating Dummy Variables Individually, concating and Dropping the Columns `","26e0a1a8":"## Step 3: Data Preparation","04012ec6":"### Rescaling the the Features using Normalisation","99576be4":"### Conclusion","cbb77a29":"### Price Variation for Car Attributes","c58c4598":"####  Dropping highly correlated(VIF) variables and insignificant variables(P-Value)\n`Priority of Dropping`\n1. High VIF - High P-Value\n2. High VIF - Low P-Value\n3. Low VIF - High P-Value\n4. Retainig the Low VIF and Low P-Value","d2946c2c":"##  `Model 9 ` \n\n`cylindernumber_three` has high p-value, dropping `cylindernumber_three`","677d19be":"### Price Variation for Different Companies","719beec1":"#### `as we can see there are occurences where the car comany name is misspelled, we will replace it with correct name`","b2c73253":"### `RFE` Recursive Feature Elimination","7ed7ef61":"## Step 5: Building a linear model\n\nFit a regression line through the training data using `statsmodels`. Remember that in `statsmodels`, you need to explicitly fit a constant using `sm.add_constant(X)` because if we don't perform this step, `statsmodels` fits a regression line passing through the origin, by default.","29014e1b":"## Step 4: Spliting the Data into Training and Testing Sets","0badd6e1":"## `Model 1`","9872d506":"### `Creating Dummy Variables all together, concating and Dropping the Columns `","22b1e4c4":"Now we have unique car comapnies\n\n### Now there are 2 ways to proceed\n1. we can consider the `Company` as a feature {We will Consider this Scenario}\n\n2. we will consider only car attributes excluding the `Company`","b2377473":"Variables with Car Attributes and Car Companies that should be considered while setting the price of a car in a new Region.\n\n`carlength`, `enginesize`,  `cylindernumber_two`, `CarName_audi`, `CarName_bmw`,`CarName_buick`, `CarName_porsche`,","ea05364f":"## Step 9: R-squared score on the test set.","5cf449b3":"##  `Model 7 ` \n\n`stroke` has high p-value, dropping `stroke`","a8180981":"### Data Clean Up - Seggregating Car Company and Model","e7e239da":"##  `Model 4 ` \n\n`enginetype_dohcv` has high p-value, dropping `enginetype_dohcv`","969a699b":"### Checking VIF\n\nVariance Inflation Factor or VIF, gives a basic quantitative idea about how much the feature variables are correlated with each other. It is an extremely important parameter to test our linear model. The formula for calculating `VIF` is:\n\n### $ VIF_i = \\frac{1}{1 - {R_i}^2} $","1fde42de":"Converting Binary variables to Numbers 1,0: `fueltype`, `aspiration`, `doornumber`, `enginelocation`","999de6a7":"## Step 6: Residual Analysis of the train data","d1567902":"##  `Model 3 ` \n\n`enginetype_dohcv` has high p-value and it is corelated to `cylindernumber_eight`, dropping `cylindernumber_eight`","3a63ce76":"> In this Predictive Analysis I am *considering* the **CarCompany** as on of the variables.\n\n> I have different kernel where I am *not considering* the Car Company and creating the Model.","b218450a":"#### Visualising the Categorical Variables","9c192f6d":"## Step 7: Making Predictions Using the Final Model\n\nNow that we have fitted the model and checked the normality of error terms, it's time to go ahead and make predictions using the final, i.e. fourth model.","facedc51":"#### `No null values found for the dataset`","cad49a5b":"##  `Model 2 ` \nFrom heat map the `cylindernumber_two` and `enginetype_rotor` are highly corelated, the corelation is 1.\n\n\n`enginetype_rotor` is infinity. Drop `enginetype_rotor`","93a57ab1":"## Step 8: Model Evaluation","95ad0d5f":"## Step 2: Visualising the Data","226900e7":"##  `Model 6 ` \n\n`cylindernumber_twelve` has high p-value, dropping `cylindernumber_twelve`","48c86f37":"##  `Model 8 ` \n\n`boreratio` has high p-value, dropping `boreratio`"}}