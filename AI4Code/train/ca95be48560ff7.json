{"cell_type":{"ee0d0a3c":"code","08cc560e":"code","956ed799":"code","5f8b8f6d":"code","ed9e72e2":"code","e0b6d8d1":"code","62a200ac":"code","b4f6b42e":"code","b3b26886":"code","c5828239":"code","8f1e3f90":"code","6a55b146":"code","4b457a8d":"code","d0520b88":"code","a6071132":"markdown","f30fc1e1":"markdown","8f2d80e8":"markdown","9424a872":"markdown","402d4017":"markdown","650c61cb":"markdown","35e753a5":"markdown","5ae3476e":"markdown","7170a834":"markdown","a8c2b5a4":"markdown","e6e86268":"markdown","f7bde8eb":"markdown","77578f61":"markdown","f6689a16":"markdown","c08dae0d":"markdown","5bfd6d20":"markdown","0eed67ed":"markdown","03a213a8":"markdown","cd2f6c75":"markdown","06c3aa9f":"markdown","9ffb4c06":"markdown"},"source":{"ee0d0a3c":"import numpy as np # Biblioteca de funciones matem\u00e1ticas de alto nivel para operar con esos vectores o matrices\nimport pandas as pd # Manipulaci\u00f3n y an\u00e1lisis de datos, Data Frames, lectura de CSV \nfrom lightgbm import LGBMClassifier # Implementaci\u00f3n de un algoritmo de Boosting de Arboles de Decisi\u00f3n con descenso de gradiente\nfrom sklearn import model_selection # Separaci\u00f3n en Train y Test\nfrom sklearn.metrics import roc_auc_score # Calculo del Area bajo la Curva ROC\n","08cc560e":"# Leemos el archivo \"pageviews\" y observamos las primeras filas\ndata = pd.read_csv(\"..\/input\/pageviews\/pageviews.csv\",\n                   parse_dates=[\"FEC_EVENT\"]) # Le indicamos que la columna FEC_EVENT debe leerse como tipo Fecha\ndata.head()","956ed799":"X_test = [] # Primero creamos el objeto vac\u00edo\nfor c in data.drop([\"USER_ID\", \"FEC_EVENT\"], axis=1).columns: # iteramos sobre todas las columnas de \"data\", menos la fecha y el Id de Usuario\n    print(\"haciendo\", c) # Mostramos en que variable est\u00e1 trabajando el loop\n    temp = pd.crosstab(data.USER_ID, data[c]) # * Realizamos una tabla cruzada de la Variable por Usuario colocando la frecuencia de cada valor posible como columna \n    temp.columns = [c + \"_\" + str(v) for v in temp.columns] # El nombre de cada columna lo renombramos como: Variable + \"_\" + Valor de la Variable\n    X_test.append(temp.apply(lambda x: x \/ x.sum(), axis=1)) # Aplicamos una funci\u00f3n lambda para calcular la proporci\u00f3n de frecuencia de cada variable\nX_test = pd.concat(X_test, axis=1) # Concatenamos todas las variables en el mismo objeto","5f8b8f6d":"# SOLO A MODO DE EXPLICACI\u00d6N DEL CODIGO ANTERIOR\n# * Podemos ver un ejemplo con PAGE de como se ve la ejecuci\u00f3n solo de esta linea, armando la tabla de contingencia y graficando las primeras 5 filas\npd.crosstab(data.USER_ID, data[\"PAGE\"]).head()","ed9e72e2":"X_test.head()","e0b6d8d1":"X_test.shape","62a200ac":"X_test.iloc[0,0:1725].sum()","b4f6b42e":"data = data[data.FEC_EVENT.dt.month < 10] # Limitamos los registros a eventos anteriores a Octubre (mes 10)\nX_train = [] # Creo un objeto vac\u00edo para Train\nfor c in data.drop([\"USER_ID\", \"FEC_EVENT\"], axis=1).columns: # Repito el proceso que vimos anteriormente\n    print(\"haciendo\", c)\n    temp = pd.crosstab(data.USER_ID, data[c])\n    temp.columns = [c + \"_\" + str(v) for v in temp.columns]\n    X_train.append(temp.apply(lambda x: x \/ x.sum(), axis=1))\nX_train = pd.concat(X_train, axis=1)","b3b26886":"features = list(set(X_train.columns).intersection(set(X_test.columns))) # Creamos una lista con las columnas que se encuentran en ambos datasets\nX_train = X_train[features] # Filtramos en el dataset de Train las columnas que son comunes a ambos\nX_test = X_test[features] # Filtramos en el dataset de Test las columnas que son comunes a ambos","c5828239":"X_train.head()","8f1e3f90":"y_prev = pd.read_csv(\"..\/input\/conversiones\/conversiones.csv\") # Leemos el archivo CSV\ny_train = pd.Series(0, index=X_train.index) # Creamos un objeto para cada Usuario con valor cero en todos los casos\nidx = set(y_prev[y_prev.mes >= 10].USER_ID.unique()).intersection(\n        set(X_train.index)) # Buscamos a los Usuarios que hayan convertido de Octubre en adelante\ny_train.loc[list(idx)] = 1 # Asignamos el valor \"1\" a los casos que crucen con el objeto creado antes","6a55b146":"y_train.head(23)","4b457a8d":"# Entrenamos el modelo LGBM Classifier\n\nfi = [] # Creamos un objeto vac\u00edo para guardar la importancia de las variables de los modelos entrenados\ntest_probs = [] # Creamos un objeto vac\u00edo para guardar las probabilidades estimadas\ni = 0 \nfor train_idx, valid_idx in model_selection.KFold(n_splits=10, shuffle=True).split(X_train): # Iterams sobre 10 folds que creamos para entrenar y validar\n    i += 1\n    Xt = X_train.iloc[train_idx] # Definimos el set de entrenamiento a partir de los indices que coincidan con los 9 folds de entrenamiento\n    yt = y_train.loc[X_train.index].iloc[train_idx] # Definimos el Target de entrenamiento con los Usuarios que coincidan con los que se encuentran en estos folds\n\n    Xv = X_train.iloc[valid_idx] # Definimos el set de validaci\u00f3n a partir los indices que coincidan con el fold de validaci\u00f3n\n    yv = y_train.loc[X_train.index].iloc[valid_idx] # Definimos el Target de validaci\u00f3n con los Usuarios que coincidan con los que se encuentran en este fold\n\n    learner = LGBMClassifier(n_estimators=10000) # Entrenamos un modelo lightgbm con los par\u00e1metros por default usando un m\u00e1ximo de 10.000 \u00e1rboles\n    learner.fit(Xt, yt,  early_stopping_rounds=10, eval_metric=\"auc\",\n                eval_set=[(Xt, yt), (Xv, yv)]) # Definimos un early stop de 10 y el m\u00e9todo de evaluaci\u00f3n como AUC\n    \n    test_probs.append(pd.Series(learner.predict_proba(X_test)[:, -1],\n                                index=X_test.index, name=\"fold_\" + str(i))) # Predecimos sobre la base total y nos guardamos las probabilidades estimadas\n    fi.append(pd.Series(learner.feature_importances_ \/ learner.feature_importances_.sum(), index=Xt.columns)) # Guardamos la proporci\u00f3n de importancia de cada variable en el modelo\n\ntest_probs = pd.concat(test_probs, axis=1).mean(axis=1) # Caluclamos la media de las probabilidades\ntest_probs.index.name=\"USER_ID\" # Renombramos las columnas\ntest_probs.name=\"SCORE\" # Renombramos las columnas\ntest_probs.to_csv(\"benchmark.zip\", header=True, compression=\"zip\") # Guardamos la predicci\u00f3n final en un zip para subirlo a la plataforma\nfi = pd.concat(fi, axis=1).mean(axis=1) # Como explicaci\u00f3n del modelo guardamos la importancia de las variables de los modelos entrenados","d0520b88":"test_probs","a6071132":"Podemos ver las primeras lineas del objeto X_test para comprobar como qued\u00f3 construido","f30fc1e1":"Ahora si, lleg\u00f3 la hora de entrenar el modelo!  \n  \nVamos a separar la base de entrenamiento en 10 folds para poder hacer la validaci\u00f3n, utilizando como target las conversiones de los Usuarios que est\u00e1n contenidos en ese fold determinado, en los meses que separamos para validar (Oct-Dic).  \n    \nPara entrenar el modelo vamos a utilizar LightGBM (gradient boosting de \u00e1rboles de decisi\u00f3n) que es una librer\u00eda desarrollada por Microsoft. podemos ver la documentaci\u00f3n ac\u00e1: https:\/\/lightgbm.readthedocs.io  \n  \nPor el momento no vamos cambiar los par\u00e1metros que utiliza LightGBM por default, por lo que solo vamos a entrenar un modelo para cada \"sub-base\" iterando hasta un m\u00e1ximo de 10.000 veces, pero deteniendo el modelo cuando no haya una mejora en el score de validaci\u00f3n durante m\u00e1s de 10 iteraciones.  \n  \nFinalmente vamos a guardar las probabilidades medias estimadas y lo vamos a preparar para hacer la subida correspondiente en la plataforma","8f2d80e8":"### Lectura de las Bases","9424a872":"Ahora que tenemos ambas bases construidas, vamos a filtrarlas quedandonos con las columnas que existen en ambos datasets, para poder entrenar y predecir sobre los mismos atributos.","402d4017":"Ahora, como la predicci\u00f3n que tenemos que hacer es a nivel Usuario, vamos a realizar un agrupamiento de toda la navegaci\u00f3n de los mismos, de forma que nos quede la misma cantidad de filas que los usuarios que tenemos en la base de entrenamiento.  \n  \nPara ello vamos a crear el objeto \"X_test\" que luego vamos a usar como base para generar nuestra predicci\u00f3n final, ya que contiene todas las filas del entrenamiento (y l\u00f3gicamente no incluye al target).  \n  \nLuego, para cada una de las variables explicativas que tenemos (PAGE, CONTENT_CATEGORY, CONTENT_CATEGORY_TOP, CONTENT_CATEGORY_BOTTOM, SITE_ID, ON_SITE_SEARCH_TERM) vamos a :\n- Sumar su frecuencia de ocurrencia de cada valor de cada una de las variables \n- Y luego calcular la proporci\u00f3n de frecuencia de cada valor posible, en relaci\u00f3n con todos los valores que puede tomar la variable (ej: para PAGE = 1, sumamos la cantidad de veces que el usuario visit\u00f3 la PAGE 1 y luego lo dividimos por el total de visitas que hizo ese usuario a todas las PAGE)\n  \nPor ultimo, nos quedamos con una tabla que tiene 2.166 columnas, que corresponden a la proporci\u00f3n de frecuencia para cada usuario de cada uno de los valores de cada una de estas variables explicativas","650c61cb":"### Agrupamos la navegaci\u00f3n por Usuario - Base de Entrenamiento","35e753a5":"Si sumamos los valores de, por ejemplo, las primeras 1725 columnas (correspondientes a PAGE) del primer registro, la suma deber\u00eda dar **1**","5ae3476e":"**Hola! **  \n  \nEste Script est\u00e1 basado en la soluci\u00f3n \"Benchmark\" subida en un Kernel por **Rafael Crescenzi**.  \n  \nEs la misma ejecuci\u00f3n agregando una peque\u00f1a explicaci\u00f3n de lo que se hace en cada paso para ayudar a los que est\u00e1n un poco perdidos :-)","7170a834":"### Agrupamos la navegaci\u00f3n por Usuario - Base de Predicci\u00f3n Final","a8c2b5a4":"## Ejemplo de Procesamiento de los Datos, Modelado Simple y Generaci\u00f3n de una Soluci\u00f3n para la usar de referencia para nuevos participantes que inicien el desaf\u00edo de este Dataton","e6e86268":"Vemos finalmente las probabilidades que calculamos y vamos a subir en nuestro submition:","f7bde8eb":"Como podemos ver, la primera columna corresponde a la fecha del evento, incluyendo  la hora, minuto y segundo en que se realiz\u00f3.  \n  \nEn principio, esta variable va a ser la que utilicemos para separar nuestras bases de entrenamiento y testeo e intentaremos  construir un esquema similar a lo que luego vamos a tener que predecir en la submission.  \n  \nLas dem\u00e1s variables que tenemos son:\n- La codificaci\u00f3n de las p\u00e1ginas que visit\u00f3 el usuario\n- La categor\u00eda que corresponde\n- El ID del sitio\n- El termino buscado en la b\u00fasqueda si corresponde\n- El ID del usuario\n","77578f61":"Si observamos el tama\u00f1o del objeto vemos que tiene:\n- 11.676 filas: Que corresponde a la cantidad de Usuarios de la base\n- 2.166 columnas: Que corresponde a la proporci\u00f3n de frecuencia para cada usuario de cada uno de los valores de cada una de estas variables","f6689a16":"Volvemos a mirar ahora las primeras filas de la base de entrenamiento y vemos que ahora tenemos muchas menos columnas, ya que solo quedaron las que existen en ambas bases.  \n  \nRecordemos que esto surge de una separaci\u00f3n \"arbitraria\" que hicimos de seprar a partir de Octubre para Validar, si el corte fuera distinto (en otro mes) podr\u00edan ser otras las columnas \"coincidentes\".","c08dae0d":"Podemos ver las primeras 23 lineas de este archivo y ver que el usuario con ID 22 convirti\u00f3 (valor = 1) mientras que los dem\u00e1s no","5bfd6d20":"Ahora importamos el archivo **conversiones.csv** que tiene la variable objetivo (Y) para el entrenamiento de nuestro modelo, que corresponde a las conversiones realizadas durante 2018  \n  \n  Luego creamos un objeto **y_train** con las conversiones que queremos predecir en nuestro modelo de entrenamiento (Oct-Dic 18)","0eed67ed":"# Script para generar la soluci\u00f3n \"Benchmark\"","03a213a8":"Ahora hacemos el mismo agrupamiento, pero para armar una base de entrenamiento, donde solo vamos a permitirle al modelo observar las navegaciones hasta Octubre 2018 (no inclusive).  \n  \nEsto significa que nuestra variable dependientes (Y) van a ser las conversiones desde Octubre en adelante, para simular la situaci\u00f3n que vamos a tener que predecir en la competencia, donde con datos hasta Diciembre 2018 vamos a predecir las conversiones en los primeros meses de 2019","cd2f6c75":"Observamos los datos que tenemos disponibles en https:\/\/www.kaggle.com\/c\/banco-galicia-dataton-2019\/data\n\nVemos que tenemos, por un lado, dos archivos principales del comportamiento de los clientes:\n- pageviews.csv - Que nos trae los datos de navegaci\u00f3n de 2018\n- device_data.csv - Que nos trae los datos de los dispositivos asociados a los usuarios\n\nCon estos 2 archivos deberemos trabajar para lograr generar variables explicativas que expliquen el comportamiento que estamos tratando de predecir: la propensi\u00f3n a la conversi\u00f3n\n\nPor otro lado tenemos 4 archivos que nos sirven para indexar variables que est\u00e1n codificadas en el archivo de **pageviews** y que se condicen los nombres de las variables con los archivos.\n- CONTENT_CATEGORY.csv\n- CONTENT_CATEGORY_TOP.csv\n- CONTENT_CATEGORY_BOTTOM.csv\n- SITE_ID.csv\n\nEstas variables de tipo \"texto\" no nos aportan valor en si mismas de forma directa, por lo que no se usan en el c\u00f3digo del Benchmark, pero pueden ser \u00fatiles como complemento explicativo a la hora de construir variables entendiendo las distintas categor\u00edas y eventos relacionados que los disparan.","06c3aa9f":"### Entrenamiento del Modelo","9ffb4c06":"### Importamos las librer\u00edas que vamos a utilizar"}}