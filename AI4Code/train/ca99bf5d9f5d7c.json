{"cell_type":{"05acd190":"code","e63f1037":"code","d80eb9e2":"code","27421b02":"code","986c51ec":"code","856699bd":"code","f2319f66":"code","7a6c7f6b":"code","0571ae95":"code","879fa869":"code","11128435":"code","34b99364":"code","bafa3b0d":"code","85d1850e":"code","5409fb52":"code","e26abfe6":"code","5353860d":"code","1b9dce1c":"code","314093d6":"code","152d0b10":"code","f802adc8":"code","88eee438":"code","15e53d97":"code","93ffba4b":"code","a1c2d43f":"code","ef2654ba":"code","24167862":"code","c3909d37":"code","993b6cbc":"code","53db2828":"code","8553cb27":"markdown","755dbbba":"markdown","660df7e0":"markdown","8bade803":"markdown","e78b7ed3":"markdown","bf4a4119":"markdown","7e50f1da":"markdown","4e342b62":"markdown","ca2c7852":"markdown","435d6152":"markdown","5f5e8ee0":"markdown","971fce20":"markdown","8ae63445":"markdown","c12d58a9":"markdown","e045bb85":"markdown","ae78615a":"markdown","565a22e9":"markdown","769a2da6":"markdown","ae1f5454":"markdown","c25ba5f0":"markdown","e7a9baa8":"markdown","37baa574":"markdown","2fd2ad4d":"markdown","cb7ad3cc":"markdown","d394ad03":"markdown","b406972a":"markdown","cf610b93":"markdown","4117b805":"markdown","10238fd9":"markdown"},"source":{"05acd190":"import datetime\nimport pandas as pd\nimport numpy as np\nimport random\nimport time\n\n# Data Viz\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport os\nfrom yellowbrick.text import TSNEVisualizer\n\n# Hide Warnings\nWarning = True\nif Warning is False:\n    import warnings\n    warnings.filterwarnings(action='ignore')\n    warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n    warnings.filterwarnings(action='ignore', category=FutureWarning)\n\n#Modeling \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn import preprocessing\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\n# Tf-Idf\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import FeatureUnion\nfrom scipy.sparse import hstack, csr_matrix\nfrom keras.preprocessing import text, sequence\n    \n\nnp.random.seed(2018)","e63f1037":"train = pd.read_csv(\"..\/input\/train.csv\", index_col= 'qid')#.sample(50000)\ntest = pd.read_csv(\"..\/input\/test.csv\", index_col= 'qid')#.sample(5000)\ntestdex = test.index\n\ntarget_names = [\"Sincere\",\"Insincere\"]\ny = train['target'].copy()","d80eb9e2":"print(train.shape)\ntrain.head()","27421b02":"print(\"Distribution of Classes:\")\ntrain.target.value_counts(normalize=False)","986c51ec":"all_text = pd.concat([train['question_text'],test['question_text']], axis =0)\n\nword_vect = TfidfVectorizer(\n            sublinear_tf=True,\n            strip_accents='unicode',\n            analyzer='word',\n            token_pattern=r'\\w{1,}',\n            stop_words='english',\n            ngram_range=(1, 2),\n            max_features=20000)\nword_vect.fit(all_text)\nX  = word_vect.transform(train['question_text'])\ntesting  = word_vect.transform(test['question_text'])","856699bd":"X_train, X_valid, y_train, y_valid = train_test_split(\n        X, y, test_size=0.20, random_state=123, stratify=y)","f2319f66":"# Create the visualizer and draw the vectors\nplt.figure(figsize = [15,9])\ntsne = TSNEVisualizer()\nn = 20000\ntsne.fit(X_train[:n], train.target[:n].map({1: target_names[1],0:target_names[0]}))\ntsne.poof()","7a6c7f6b":"def model_fit(model,X_train,X_val,y_train,y_val):\n    model.fit(X_train, y_train)\n    \n    # Predict\n    valid_logistic_pred = model.predict(X_val)\n    train_logistic_pred = model.predict(X_train)\n    \n    print(\"Train Set F1 Score: {:.3f}\".format(metrics.f1_score(train_logistic_pred, y_train)))\n    print(\"Validation Set F1 Score: {:.3f}\".format(metrics.f1_score(valid_logistic_pred, y_val)))\n\n    # Confusion Matrix\n    C = metrics.confusion_matrix(valid_logistic_pred, y_val)\/len(y_val)\n    sns.heatmap(C, annot=True)","0571ae95":"# Fit Model\nmodel_fit(LogisticRegression(solver = 'sag'),X_train,X_valid,y_train,y_valid)","879fa869":"from keras import backend as K\n\ndef f1(y_true, y_pred):\n    def recall(y_true, y_pred):\n        \"\"\"Recall metric.\n\n        Only computes a batch-wise average of recall.\n\n        Computes the recall, a metric for multi-label classification of\n        how many relevant items are selected.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n        recall = true_positives \/ (possible_positives + K.epsilon())\n        return recall\n\n    def precision(y_true, y_pred):\n        \"\"\"Precision metric.\n\n        Only computes a batch-wise average of precision.\n\n        Computes the precision, a metric for multi-label classification of\n        how many selected items are relevant.\n        \"\"\"\n        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n        precision = true_positives \/ (predicted_positives + K.epsilon())\n        return precision\n    precision = precision(y_true, y_pred)\n    recall = recall(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))","11128435":"def model_val(model,X_valid,y_valid):\n    preds = model.predict([X_valid],batch_size=1024,verbose = True)\n    for i in np.arange(.1,.6,.025):\n        i = np.round(i, 3)\n        score = metrics.f1_score(y_valid,(preds > i))\n        print(\"F1 score at threshold {0} is {1}\".format(i, score))","34b99364":"from keras.models import Model, Sequential\nfrom keras.layers import CuDNNGRU,CuDNNLSTM,Input, Dense, Embedding,Dropout, concatenate, Bidirectional,Flatten,GlobalAveragePooling1D, GlobalMaxPool1D","bafa3b0d":"maxlen = 100\nmax_features = 20000\n\ninp = Input((max_features,))\nhidden1 = Dense(units = maxlen)(inp)\nhidden2 = Dense(units = maxlen)(hidden1)\n\nfinal = Dense(units = 1,activation = 'sigmoid')(hidden2)\n\nmodel = Model(inp,final)\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=[f1])\nmodel.summary()","85d1850e":"model.fit(X_train,y_train,batch_size = 1024,epochs = 2,validation_data = (X_valid,y_valid),verbose = True)","5409fb52":"model_val(model,X_valid,y_valid)","e26abfe6":"## some config values \nembed_size = 300 # how big is each word vector\nmax_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 100 # max number of words in a question to use\n\n## split to train and val\ntrain_df, val_df = train_test_split(train, test_size=0.1, random_state=2018)\n\n## fill up the missing values\ntrain_X = train_df[\"question_text\"].fillna(\"_na_\").values\nval_X = val_df[\"question_text\"].fillna(\"_na_\").values\ntest_X = test[\"question_text\"].fillna(\"_na_\").values\n\n## Tokenize the sentences\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(train_X))\nX_train = tokenizer.texts_to_sequences(train_X)\nX_valid = tokenizer.texts_to_sequences(val_X)\nX_test = tokenizer.texts_to_sequences(test_X)\n\n## Pad the sentences \nX_train = sequence.pad_sequences(X_train, maxlen=maxlen)\nX_valid = sequence.pad_sequences(X_valid, maxlen=maxlen)\nX_test = sequence.pad_sequences(X_test, maxlen=maxlen)\n\n## Get the target values\ny_train = train_df['target'].values\ny_valid = val_df['target'].values","5353860d":"inp = Input(shape=(maxlen,))\nemb = Embedding(max_features, embed_size)(inp)\n\nhidden1 = Dense(units = 64)(emb)\nflat = Flatten()(hidden1)\nhidden2 = Dense(units = 16,activation = 'relu')(flat)\n\nfinal = Dense(units = 1,activation = 'sigmoid')(hidden2)\n\nmodel = Model(inp,final)\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=[f1])","1b9dce1c":"model.fit(X_train, y_train, batch_size=1024, epochs=2, validation_data=(X_valid, y_valid),verbose = True)","314093d6":"model_val(model,X_valid,y_valid)","152d0b10":"inp = Input(shape=(maxlen,))\nemb = Embedding(max_features, embed_size)(inp)\n\nhidden1 = Dense(units = 64)(emb)\nmax_pool = GlobalMaxPool1D()(hidden1)\navg_pool = GlobalAveragePooling1D()(hidden1)\nconc = concatenate([max_pool,avg_pool])\n\nhidden2 = Dense(units = 16,activation = 'relu')(conc)\n\nfinal = Dense(units = 1,activation = 'sigmoid')(hidden2)\n\nmodel = Model(inp,final)\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=[f1])","f802adc8":"model.fit(X_train, y_train, batch_size=1024, epochs=2, validation_data=(X_valid, y_valid),verbose = True)","88eee438":"model_val(model,X_valid,y_valid)","15e53d97":"inp = Input(shape=(maxlen,))\nemb = Embedding(max_features, embed_size)(inp)\n\nhidden1 = Bidirectional(CuDNNLSTM(units = 64,return_sequences = True))(emb)\nmax_pool = GlobalMaxPool1D()(hidden1)\navg_pool = GlobalAveragePooling1D()(hidden1)\nconc = concatenate([max_pool,avg_pool])\n\nhidden2 = Dense(units = 16,activation = 'relu')(conc)\n\nfinal = Dense(units = 1,activation = 'sigmoid')(hidden2)\n\nmodel = Model(inp,final)\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=[f1])\nmodel.summary()","93ffba4b":"model.fit(X_train, y_train, batch_size=1024, epochs=2, validation_data=(X_valid, y_valid),verbose = True)","a1c2d43f":"model_val(model,X_valid,y_valid)","ef2654ba":"EMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","24167862":"inp = Input(shape=(maxlen,))\nemb = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n\nhidden1 = Bidirectional(CuDNNLSTM(units = 64,return_sequences = True))(emb)\nmax_pool = GlobalMaxPool1D()(hidden1)\navg_pool = GlobalAveragePooling1D()(hidden1)\nconc = concatenate([max_pool,avg_pool])\n\nhidden2 = Dense(units = 16,activation = 'relu')(conc)\ndrop = Dropout(0.1)(hidden2)\nfinal = Dense(units = 1,activation = 'sigmoid')(drop)\n\nmodel = Model(inp,final)\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=[f1])\nmodel.summary()","c3909d37":"model.fit(X_train, y_train, batch_size=1024, epochs=2, validation_data=(X_valid, y_valid),verbose = True)","993b6cbc":"model_val(model,X_valid,y_valid)","53db2828":"pred_test_y = model.predict(X_test,batch_size = 1024,verbose = True)\npred_test_y = (pred_test_y>0.35).astype(int)\nout_df = pd.DataFrame({\"qid\":test.index.values})\nout_df['prediction'] = pred_test_y\nout_df.to_csv(\"submission.csv\", index=False)","8553cb27":"#### Logistic Regression","755dbbba":"### Add Neural Network Embeddings","660df7e0":"The flattening layer leads to a very wide layer of the neural network so instead of use a pooling layer. This layer takes the maximum and average value from each word vector and uses that as the single value in the neuron.","8bade803":"### Make Predictions","e78b7ed3":"### Build word vectors and Visualize Results","bf4a4119":"[Term Frequency Inverse Document Frequency](https:\/\/www.tfidf.com) Technique commonly used when dealing with words in documents, the words that appear most often across documents are usually not that helpful because they don't distinguish documents. TF-IDF is a way to weight a word higher for appearing more often within an article but decrease the weighting for a word appearing more often between articles.)","7e50f1da":"### Add external embeddings","4e342b62":"#### Marco Gancitano\n#### 27 November 2018","ca2c7852":"I've added an Embedding layer in the beginning of the neural network so embeddings are made before going into the network. Additionally, i added a flatten layer so the data goes from 2D (a matrix with each word having it's own embedding array) to 1D which is better for the Dense layers.","435d6152":"### Modeling ","5f5e8ee0":"<hr>","971fce20":"<hr>","8ae63445":"The Quora Kaggle competition is evaluated off of F1-Score so that is the score used for testing models.","c12d58a9":"### Load Packages","e045bb85":"## Quora Insincere Questions Classification","ae78615a":"We can see here that the data isn't very seperable by traditional means. A non-parametric model will most likely do the best (Gradient Boosting or Neural Networks)","565a22e9":"#### Build model validation function to test parameters","769a2da6":"As we can see most questions are sincere with only <b>6.2% of questions being insincere<\/b>","ae1f5454":"The last piece of the puzzle for creating a robust neural network for NLP is adding some memory to the system. Long-short term memory neural networks are a type of Recurrent Neural Network which have the benefit of using previous runs outputs as input into the next run which has been proven to work well for classifiying speech. In addition, the layer is bidirectional which means the first half of the layer are the inputs normally and the second half is them reversed. This allows for a more robust understanding of the language used.","c25ba5f0":"#### t-SNE - Visual Cluster Plot","e7a9baa8":"#### Build F1-score function for keras use","37baa574":"I use this technique to create a word vector for my models that is more robust than typical one-hot encoding","2fd2ad4d":"#### Neural Networks","cb7ad3cc":"#### TF-IDF","d394ad03":"### Read in Data","b406972a":"#### Create function ","cf610b93":"t-SNE ( [t-Distributed Stochastic Neighbor Embedding](https:\/\/lvdmaaten.github.io\/tsne\/)) is a technique for dimensionality reduction suited well for visulizating high-dimensional datasets.","4117b805":"#### Take a quick look at the data","10238fd9":"For this part I lessen the power the TF-IDF has, I still use it to tokenize the word vectors but make them smaller with only 100 words per question. I also pad them so that all sentences have the proper words aligned instead of having misaligned rows. I then add an embedding layer to the neural network so it can learn how the questions are related, this is a more robust technique for finding \"distances\" between sentences."}}