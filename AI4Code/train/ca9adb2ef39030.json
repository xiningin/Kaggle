{"cell_type":{"8e44913f":"code","a0bb59ee":"code","05baca6e":"code","32dc4ff9":"code","6f333917":"code","21512f6d":"code","f6004d6c":"code","f51271f2":"code","84b67955":"code","0695385c":"code","50b09605":"code","0fdf4939":"code","581dab8a":"code","013d64b7":"code","9da039b8":"code","a76b0da5":"code","8ce707cb":"code","eabef750":"code","dee3fbc8":"code","cc0e471b":"code","d87406a7":"code","caecd107":"code","72e15aeb":"code","17919501":"code","199ce104":"code","2b0fedd1":"code","3ae499ef":"code","e7646ffd":"markdown","b2cd8ca2":"markdown","ad3138fb":"markdown"},"source":{"8e44913f":"import numpy as np\nimport pandas as pd\n\ndf_train = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nprint(f\"Rows in train.csv = {len(df_train)}\")\nprint(f\"Rows in test.csv = {len(df_test)}\")\npd.set_option('display.max_colwidth', None)\ndf_train.head()","a0bb59ee":"df_train_pos = df_train[df_train.target == 1]\ndf_train_neg = df_train[df_train.target == 0]\nprint(f\"No. of positive training examples = {len(df_train_pos)}\")\nprint(f\"No. of negative training examples = {len(df_train_neg)}\")\ntrain_keywords_unique = df_train.keyword.unique()\nprint(f\"No. of unique keywords = {len(train_keywords_unique)}\")\ndf_train_notnull_keywords = df_train[~df_train.keyword.isnull()]\nprint(f\"No of train examples with keyword not null = {len(df_train_notnull_keywords)}\")\ndf_train_notnull_keywords.head()","05baca6e":"df_test.head()","32dc4ff9":"import re\nimport string\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.tokenize import RegexpTokenizer\n\ndef process_tweet(tweet):\n    \"\"\"Process tweet function.\n    Input:\n        tweet: a string containing a tweet\n    Output:\n        tweets_clean: a list of words containing the processed tweet\n    \"\"\"\n    stemmer = PorterStemmer()\n    lemmatizer = WordNetLemmatizer()\n    stopwords_english = stopwords.words('english')\n    # remove stock market tickers like $GE\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    # remove hashtags\n    # only removing the hash #, @, ... sign from the word\n    tweet = re.sub(r'\\.{3}|@|#|\u00fb_', '', tweet)\n    #tweet = decontracted(tweet)\n    #tweet = re.sub(r'[^a-zA-Z\\d\\s:]', '', tweet)\n    # tokenize tweets\n    #tokenizer = RegexpTokenizer(r'[^\\d\\W]+')\n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n    tweet_tokens = tokenizer.tokenize(tweet)\n\n    tweets_clean = []\n    for word in tweet_tokens:\n        # remove stopwords and punctuation\n        if (word not in stopwords_english and word not in string.punctuation):\n            #stem_word = stemmer.stem(word)  # stemming word\n            lemma_word = lemmatizer.lemmatize(word)\n            tweets_clean.append(lemma_word)\n            #tweets_clean.append(word)\n    return tweets_clean","6f333917":"\r\ndf_train['text_processed'] = df_train['text'].apply(lambda text: process_tweet(text))\r\ndf_train.head()","21512f6d":"df_test['text_processed'] = df_test['text'].apply(lambda text: process_tweet(text))\r\ndf_test.head()","f6004d6c":"# from common_utils import build_freqs\r\nimport numpy as np\r\n\r\ndef add_or_increment(key, dict):\r\n    if key in dict:\r\n        dict[key] += int(1)\r\n    else:\r\n        dict[key] = int(1)\r\n\r\ndef build_freqs(processed_tweets, ys):\r\n    \"\"\"Build frequencies.\r\n    Input:\r\n        tweets: a list of tweets\r\n        ys: an m x 1 array with the sentiment label of each tweet (either 0 or 1)\r\n    Output:\r\n        freqs: a dictionary mapping each (word, sentiment) pair to its frequency\r\n    \"\"\"\r\n    # Convert np array to list since zip needs an iterable.\r\n    # The squeeze is necessary or the list ends up with one element.\r\n    # Also note that this is just a NOP if ys is already a list.\r\n    yslist = np.squeeze(ys).tolist()\r\n\r\n    # Start with an empty dictionary and populate it by looping over all tweets\r\n    # and over all processed words in each tweet.\r\n    freqs = {}\r\n    pos_freqs = {}\r\n    neg_freqs = {}\r\n    for y, tweet in zip(yslist, processed_tweets):        \r\n        for word in tweet:\r\n            pair = (word, y)\r\n            add_or_increment(pair, freqs)\r\n            if y == 1:\r\n                add_or_increment(word, pos_freqs)\r\n            else:\r\n                add_or_increment(word, neg_freqs)\r\n    return freqs, pos_freqs, neg_freqs\r\n\r\nX_input_tweets = df_train['text_processed']\r\nX_test_tweets = df_test['text_processed']\r\ny_input = df_train['target']\r\nfreqs, pos_word_freqs, neg_word_freqs = build_freqs(X_input_tweets, y_input)\r\n","f51271f2":"df_train_pos = df_train[df_train.target == 1]\r\ndf_train_neg = df_train[df_train.target == 0]\r\n# keyword frequency in disaster tweets\r\npos_keyword_freq = df_train_pos['keyword'].value_counts()\r\n# location frequency in disaster tweets\r\npos_location_freq = df_train_pos['location'].value_counts()\r\n# keyword frequency in non disaster tweets\r\nneg_keyword_freq = df_train_neg['keyword'].value_counts()\r\n# location frequency in non disaster tweets\r\nneg_location_freq = df_train_neg['location'].value_counts()","84b67955":"# Word cloud for disaster tweets\r\nfrom wordcloud import WordCloud\r\nimport matplotlib.pyplot as plt\r\n\r\ndef plot_wordcloud(freq_dict):\r\n    wc = WordCloud(width=1000, height=500).generate_from_frequencies(freq_dict)\r\n    plt.figure(figsize=(12,8))\r\n    plt.imshow(wc)\r\n\r\nplot_wordcloud(pos_word_freqs)","0695385c":"# plot top 10 words in disaster tweets as a bar plot\ndef get_topn_dictitems_byvalue(dict_data, n):\n    freq_word = [[int(value), key] for key, value in dict_data.items()]\n    freq_word.sort(reverse=True, key=lambda k: k[0])\n    return freq_word[:n]\n\n\ndef barplot(freq_dict, n, x_label, y_label, title, y_range = []):\n    topn_keys_byvalue = get_topn_dictitems_byvalue(freq_dict, n)\n    key_counts = [count for count, key in topn_keys_byvalue]\n    key = [key for count, key in topn_keys_byvalue]\n\n    fig, ax = plt.subplots(figsize=(10, 5))\n    if len(y_range) > 0:\n        ax.set_ylim(y_range)\n    bars = ax.bar(key, key_counts)\n    ax.set_ylabel(y_label)\n    ax.set_xlabel(x_label)\n    plt.title(title)\n    plt.xticks(rotation=45)\n    # add the y value on top of each bar\n    for bar in bars:\n        y_val = bar.get_height()\n        plt.text(bar.get_x(), y_val+0.05, y_val)\n    plt.show()\n\nbarplot(pos_word_freqs, 10, 'words', 'frequency', 'Top words in disaster tweets', [100, 375])","50b09605":"# plot word cloud for words in non disaster tweets\r\nplot_wordcloud(neg_word_freqs)","0fdf4939":"barplot(neg_word_freqs, 10, 'words', 'frequency', 'Top words in non disaster tweets', [100, 400])","581dab8a":"barplot(pos_location_freq, 10, 'location', 'frequency', 'Top locations in disaster tweets', [10, 75])","013d64b7":"barplot(neg_location_freq, 10, 'location', 'frequency', 'Top locations in non disaster tweets')","9da039b8":"barplot(pos_keyword_freq, 10, 'keyword', 'frequency', 'Top keywords in disaster tweets', [30, 40])","a76b0da5":"barplot(neg_keyword_freq, 10, 'keyword', 'frequency', 'Top keywords in non disaster tweets', [30, 40])","8ce707cb":"def extract_features(tweet, keyword, loc, freqs, pos_keyword_freqs, neg_keyword_freqs, \r\n                    pos_loc_freqs, neg_loc_freqs):\r\n    tweet_words = tweet\r\n    # first column is bias, second positive word count, third negative word count\r\n    feature_vector = np.zeros((1, 7))\r\n    #feature_vector = np.zeros((1, 3))\r\n    feature_vector[0, 0] = 1\r\n    #print(f\"keyword: {keyword}\")\r\n    for word in tweet_words:\r\n        feature_vector[0, 1] += freqs.get((word, 1.0), 0)        \r\n        feature_vector[0, 2] += freqs.get((word, 0.0), 0)\r\n    if not pd.isnull(keyword):   \r\n        #print(keyword)       \r\n        if keyword in pos_keyword_freqs.keys():  \r\n            feature_vector[0, 3] += pos_keyword_freqs[keyword]\r\n        if keyword in neg_keyword_freqs.keys():            \r\n            feature_vector[0, 4] += neg_keyword_freqs[keyword]\r\n    if not pd.isnull(loc):   \r\n        #print(keyword)       \r\n        if loc in pos_loc_freqs.keys():  \r\n            feature_vector[0, 5] += pos_loc_freqs[loc]\r\n        if loc in neg_loc_freqs.keys():            \r\n            feature_vector[0, 6] += neg_loc_freqs[loc]            \r\n    return feature_vector","eabef750":"X_input_keywords = df_train['keyword']","dee3fbc8":"test_features = extract_features(X_input_tweets[0], 'aftershock', 'usa', freqs, \r\n                pos_keyword_freq, neg_keyword_freq, pos_location_freq, neg_location_freq)\r\ntest_features","cc0e471b":"#X_input = np.zeros((len(X_input_tweets), 5))\r\nX_input = np.zeros((len(X_input_tweets), 7))\r\nfor i, row in df_train.iterrows():\r\n    X_input[i, :] = extract_features(row['text_processed'], row['keyword'], row['location'], \r\n                            freqs, pos_keyword_freq, neg_keyword_freq, pos_location_freq, neg_location_freq)\r\n\r\nprint(X_input.shape)\r\n\r\n#X_test = np.zeros((len(X_test_tweets), 5))\r\nX_test = np.zeros((len(X_test_tweets), 7))\r\nfor i, row in df_test.iterrows():\r\n    X_test[i, :] = extract_features(row['text_processed'], row['keyword'], row['location'], \r\n                            freqs, pos_keyword_freq, neg_keyword_freq, pos_location_freq, neg_location_freq)\r\n    \r\nprint(X_test.shape)    ","d87406a7":"from sklearn.naive_bayes import ComplementNB\r\nmnb = ComplementNB(alpha=1)\r\nmnb.fit(X_input, y_input.ravel())","caecd107":"from sklearn.model_selection import cross_validate\r\ncv_results = cross_validate(mnb, X_input, y_input, cv=5, scoring=\"f1\")\r\ncv_results['test_score'].mean()","72e15aeb":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X_input, y_input, test_size=0.2, random_state=42)\nmnb_split = ComplementNB(alpha=1)\nmnb_split.fit(X_train, y_train.ravel())\nmnb_split.score(X_val, y_val)","17919501":"y_val_pred = mnb_split.predict(X_val)","199ce104":"from sklearn.metrics import classification_report\r\nprint(classification_report(y_val, y_val_pred))","2b0fedd1":"y_test_pred = mnb.predict(X_test)\r\ndf_test['target'] = y_test_pred\r\ndf_test.head(20)","3ae499ef":"df_submit = df_test[['id', 'target']]\r\ndf_submit.to_csv('results.csv', index=False)","e7646ffd":"Build the frequency dictionary for positive and negative words in tweets ( positive means target value is 1 and negative means 0 )","b2cd8ca2":"Preprocess the tweets ","ad3138fb":"Split training data into 80% training set and 20% validation set"}}