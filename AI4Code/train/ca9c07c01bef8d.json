{"cell_type":{"a976b81b":"code","faed5805":"code","75cc5f99":"code","91c94f1f":"code","31ac1730":"code","bfdc494c":"code","9beaa570":"code","3b299e9d":"code","f2612649":"code","84dd015c":"code","9d423fae":"code","b17910a6":"code","99c70fe0":"code","ae11feaa":"code","a267d6ee":"code","5fe25e6b":"code","3d8c8b10":"code","bbd487db":"code","c03bdd65":"code","fb7acc35":"code","ad560701":"code","72f8ec32":"code","459208ac":"code","790aec4e":"code","90945b52":"code","9c3a2954":"code","92d6cf83":"code","835584f8":"code","7a15d438":"code","f1f1881b":"code","4cde7c3a":"code","f08bbc5d":"code","02ffbbb6":"code","f8960cf1":"code","cf6993c7":"code","c11b20ef":"markdown","bf062f10":"markdown","060bf71d":"markdown","c5ccaf52":"markdown","6f57547c":"markdown","1f350fe5":"markdown","3ffb738f":"markdown","d8420eb9":"markdown","d4395dc5":"markdown","4a8e9674":"markdown","a8d31ae2":"markdown","27490fd1":"markdown","939532c0":"markdown","f32810ce":"markdown","74227c60":"markdown","d359c7d6":"markdown","292ecaaf":"markdown","622ef611":"markdown","df16d20c":"markdown"},"source":{"a976b81b":"import pandas as pd\nimport numpy as np\n\n# Import and suppress warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Visualiazation\nfrom seaborn import countplot\n\n# Generate dataset\nCars = {'Date': ['10.10.2018', '14.10.2018', '07.11.2018', '20.11.2018', '06.12.2018', '01.01.2019', '07.01.2019', '07.02.2019'],\n        'Brand': ['Honda Civic', np.NaN, 'Toyota Corolla', 'Ford Focus',  np.NaN, 'Audi A4', np.NaN, 'Honda'],\n        'Price1': [22000, 25000, 27000, np.NaN, 35000, 15000, 1000, 1500],\n        'Price2': [23000, 21000, 25000, np.NaN, 35000, 11000, 1200, 1100],\n        'Engine': ['150.0 horsepower', '100.0 horsepower', '250.0 horsepower', '100.5 horsepower', '50.0 horsepower', '40.0 horsepower', '50.5 horsepower', '45.0 horsepower'],\n        'Color': ['red', 'blue', 'green', 'red', np.NaN, 'blue', 'red', 'yellow'],\n        'Year': [2000, 2010, 2015, 2011, 2019, 2005, 1999, 1995],\n        'Label': [1, 0, 0, 0, 0, 0, 1, 1]\n        }\n\ndf = pd.DataFrame(Cars, columns= ['Date', 'Brand', 'Price1', 'Price2', 'Engine', 'Color', 'Year', 'Label'])\ndf","faed5805":"# Columns of the DataFrame\ndf.columns","75cc5f99":"# Shape (number of columns, rows)\ndf.shape","91c94f1f":"# Types of the columns\ndf.dtypes","31ac1730":"# Statistic for numerical columns\ndf.describe()","bfdc494c":"df.info()","9beaa570":"round(df['Color'].value_counts(normalize=True)*100, 2)","3b299e9d":"# Check the share of the classes\nprint('Share of the classes:')\nprint(round(df['Label'].value_counts(normalize=True)*100, 2))\ncountplot(x='Label', data=df)","f2612649":"from sklearn.model_selection import train_test_split\n\n# Create a data with all columns except Label\ndf_X = df.drop('Label', axis=1)\n\n# Create a category_desc labels dataset\ndf_y = pd.DataFrame(df['Label'])\n\n# Use stratified sampling to split up the dataset according to the df_y dataset\nX_train, X_test, y_train, y_test = train_test_split(df_X, df_y, stratify=df_y)\n\n# Print out the Label counts on the training y labels\nprint(round(y_train['Label'].value_counts(normalize=True)*100, 2))","84dd015c":"from sklearn.linear_model import LogisticRegression\n\ndef check_score(X_, y_):\n    \n    # Split X and the y labels into training and test sets\n    X_train_, X_test_, y_train_, y_test_ = train_test_split(X_, y_)\n\n    lgr = LogisticRegression()\n    # Fit lgr to the training data\n    lgr.fit(X_train_, y_train_)\n\n    # Score knn on the test data and print it out\n    return(lgr.score(X_test_, y_test_)) ","9d423fae":"# Number of missing values per column\nprint('Number of missing values per column:')\ndf.isnull().sum()","b17910a6":"# Delete all rows with missing\nprint('Number of rows:', df.shape[0])\nprint('Number of rows after deleting all rows with missing:', df.dropna().shape[0])\ndf.dropna()","99c70fe0":"# Subset dataset without missing\ndf_no_missing = df[df.notnull()]\ndf_no_missing","ae11feaa":"# Delete all cols with missing\nprint('Number of crows:', df.shape[1])\nprint('Number of cows after deleting all cows with missing:', df.dropna(axis=1).shape[1])\ndf.dropna(axis=1)","a267d6ee":"# Columns with at least 5 not missing\ndf.dropna(axis=1, thresh=5)","5fe25e6b":"# Fill in missing values\ndf['Brand'].fillna('missing', inplace=True)\ndf['Color'].fillna('missing', inplace=True)\n\ndf['Price1'].fillna(df['Price1'].mean(), inplace=True)\ndf['Price2'].fillna(df['Price2'].mean(), inplace=True)\n\ndf","3d8c8b10":"# Check Data Types\ndf.dtypes","bbd487db":"# Reduce memory usage\nif df['Price1'].min() > np.finfo(np.float32).min and df['Price1'].max() < np.finfo(np.float32).max:\n    df['Price1'] = df['Price1'].astype(np.float32)\n    \nif df['Price2'].min() > np.finfo(np.float32).min and df['Price2'].max() < np.finfo(np.float32).max:\n    df['Price2'] = df['Price2'].astype(np.float32)\n\nif df['Year'].min() > np.iinfo(np.int32).min and df['Year'].max() < np.iinfo(np.int32).max:\n    df['Year'] = df['Year'].astype(np.int32)\n\n# Check Data Types\ndf.dtypes","c03bdd65":"df.var()","fb7acc35":"df['log_Year'] = np.log(df['Year'])\ndf['log_Year'].var()","ad560701":"# Import StandardScaler from scikit-learn\nfrom sklearn.preprocessing import StandardScaler\n\n# Create the scaler\nss = StandardScaler()\n\n# Take a subset of the DataFrame you want to scale \ndf_subset = df[['Price1', 'Price2', 'Year']]\n\n# Apply the scaler to the DataFrame subset\ndf_subset_scaled = ss.fit_transform(df_subset)\n\ndf_subset_scaled","72f8ec32":"df['Price_type'] = 'low'\ndf.loc[(df['Price1'] > 10000), 'Price_type'] = 'higth'\ndf[['Price1', 'Price_type']] ","459208ac":"from sklearn.preprocessing import LabelEncoder\n\n# Set up the LabelEncoder object\nenc = LabelEncoder()\n\n# Apply the encoding to the Color column\ndf['Color_enc'] = enc.fit_transform(df['Color'])\n\n# Compare the two columns\nprint(df[['Color', 'Color_enc']].head())","790aec4e":"# Transform the Color column and concatinate with DataSet\ndf = pd.concat([df, pd.get_dummies(df['Price_type'])], axis=1)\n\ndf[['Price_type', 'higth', 'low']]","90945b52":"# Create a list of the columns to average\nprice_columns = ['Price1', 'Price2']\n\n# Use apply to create a mean column\ndf['Price_average'] = df.apply(lambda row: row[price_columns].mean(), axis=1)\ndf['Price_average_log'] = np.log(df['Price_average'])\n\n# Take a look at the results\nprint(df[['Price1', 'Price2', 'Price_average', 'Price_average_log']] )","9c3a2954":"# First, convert string column to date column\ndf['Date_converted'] = pd.to_datetime(df['Date'])\n\n# Extract just the month and year from the converted column\ndf['Date_month'] = df.apply(lambda row: row['Date_converted'].month, axis=1)\ndf['Date_year'] = df.apply(lambda row: row['Date_converted'].year, axis=1)\n\n# Take a look at the converted and new month columns\nprint(df[['Date_converted', 'Date_month', 'Date_year']])","92d6cf83":"import re\n\n# Write a pattern to extract numbers and decimals\ndef return_hp(str):\n    pattern = re.compile(r'\\d+\\.\\d+')\n    \n    # Search the text for matches\n    hp = re.match(pattern, str)\n    \n    # If a value is returned, use group(0) to return the found value\n    if hp is not None:\n        return float(hp.group(0))\n        \n# Apply the function to the Length column and take a look at both columns\ndf['Engine_hp'] = df['Engine'].apply(lambda row: return_hp(row))\ndf[['Engine', 'Engine_hp']]","835584f8":"# transformation into a text vector\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Create the vectorizer method\ntfidf_vec = TfidfVectorizer()\n\n# Transform the text into tf-idf vectors\ndf['Brand_text_tfidf'] = tfidf_vec.fit_transform(df['Brand'])\n\ndf[['Brand', 'Brand_text_tfidf']]","7a15d438":"# Create a list of redundant column names to drop\nto_drop = ['Brand', 'Price1', 'Price2', 'Price_type', 'higth', 'Price_average', 'Engine', 'Color', 'Year', 'Date', 'Date_converted']\n\n# Drop those columns from the dataset\ndf_subset_1 = df.drop(to_drop, axis=1)\n\n# Print out the head of the new dataset\ndf_subset_1","f1f1881b":"# Print out the column correlations of the dataset\ndf_subset_1.corr()","4cde7c3a":"# Take a minute to find the column where the correlation value is greater than 0.75 at least twice\nto_drop = ['log_Year', 'Price_average_log']\n\n# Drop that column from the DataFrame\ndf_subset_2 = df_subset_1.drop(to_drop, axis=1)\n\n# Print out the column correlations of the current dataset\ndf_subset_2.corr()","f08bbc5d":"df_subset_2","02ffbbb6":"# Score on the test data and print it out\ndf_X = df_subset_2.drop(['Label', 'Brand_text_tfidf'], axis=1)\ncheck_score(df_X, df_subset_2['Label'])","f8960cf1":"from sklearn.decomposition import PCA\n\n# Set up PCA and the X vector for diminsionality reduction\npca = PCA()\ndf_X = df_subset_2.drop(['Label', 'Brand_text_tfidf'], axis=1)\n\n# Apply PCA to the dataset X vector\ntransformed_X = pca.fit_transform(df_X)\n\n# Look at the percentage of variance explained by the different components\nprint(pca.explained_variance_ratio_)","cf6993c7":"# Score on the test data and print it out\ncheck_score(transformed_X, df['Label'])","c11b20ef":"#### Datetime","bf062f10":"## Feature engineering ","060bf71d":"### Correlated columns","c5ccaf52":"## Split per Train\/Test","6f57547c":"## Dimensionality reduction","1f350fe5":"## Data types","3ffb738f":"# Preprocessing","d8420eb9":"### Redundant columns","d4395dc5":"#### Text classification","4a8e9674":"## Missing Data","a8d31ae2":"### Engineering numerical features ","27490fd1":"#### One Hot Encoder","939532c0":"### Scaling data ","f32810ce":"### Log normalization\n\n_Example of using: Column with hight varience_","74227c60":"#### Label Encoder","d359c7d6":"## Removing redundant features\n__remove noisy\/correlated\/duplicated features__","292ecaaf":"#### Average","622ef611":"## Standartization","df16d20c":"### Encoding categorical variables"}}