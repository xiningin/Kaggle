{"cell_type":{"728e60c3":"code","81c2fe6b":"code","a9579678":"code","251880ce":"code","f417545b":"code","e06ddef6":"code","80d6b272":"code","2666a688":"code","6fc56919":"markdown","6ea0f1c4":"markdown","a4f6b3f1":"markdown","c2c7d3e5":"markdown","3d914b84":"markdown","679533bc":"markdown","a2170158":"markdown","b17b3667":"markdown","42b73884":"markdown"},"source":{"728e60c3":"%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\nfrom IPython.display import display\nfrom fastai.imports import *\nfrom sklearn import metrics","81c2fe6b":"class DecisionTree():\n    def __init__(self, x, y, idxs = None, min_leaf=2):\n        if idxs is None: idxs=np.arange(len(y))\n        self.x,self.y,self.idxs,self.min_leaf = x,y,idxs,min_leaf\n        self.n,self.c = len(idxs), x.shape[1]\n        self.val = np.mean(y[idxs])\n        self.score = float('inf')\n        self.find_varsplit()\n        \n    def find_varsplit(self):\n        for i in range(self.c): self.find_better_split(i)\n        if self.score == float('inf'): return\n        x = self.split_col\n        lhs = np.nonzero(x<=self.split)[0]\n        rhs = np.nonzero(x>self.split)[0]\n        self.lhs = DecisionTree(self.x, self.y, self.idxs[lhs])\n        self.rhs = DecisionTree(self.x, self.y, self.idxs[rhs])\n\n    def find_better_split(self, var_idx):\n        x,y = self.x.values[self.idxs,var_idx], self.y[self.idxs]\n        sort_idx = np.argsort(x)\n        sort_y,sort_x = y[sort_idx], x[sort_idx]\n        rhs_cnt,rhs_sum,rhs_sum2 = self.n, sort_y.sum(), (sort_y**2).sum()\n        lhs_cnt,lhs_sum,lhs_sum2 = 0,0.,0.\n\n        for i in range(0,self.n-self.min_leaf-1):\n            xi,yi = sort_x[i],sort_y[i]\n            lhs_cnt += 1; rhs_cnt -= 1\n            lhs_sum += yi; rhs_sum -= yi\n            lhs_sum2 += yi**2; rhs_sum2 -= yi**2\n            if i<self.min_leaf or xi==sort_x[i+1]:\n                continue\n\n            lhs_std = std_agg(lhs_cnt, lhs_sum, lhs_sum2)\n            rhs_std = std_agg(rhs_cnt, rhs_sum, rhs_sum2)\n            curr_score = lhs_std*lhs_cnt + rhs_std*rhs_cnt\n            if curr_score<self.score: \n                self.var_idx,self.score,self.split = var_idx,curr_score,xi\n\n    @property\n    def split_name(self): return self.x.columns[self.var_idx]\n    \n    @property\n    def split_col(self): return self.x.values[self.idxs,self.var_idx]\n\n    @property\n    def is_leaf(self): return self.score == float('inf')\n    \n    def __repr__(self):\n        s = f'n: {self.n}; val:{self.val}'\n        if not self.is_leaf:\n            s += f'; score:{self.score}; split:{self.split}; var:{self.split_name}'\n        return s\n\n    def predict(self, x):\n        return np.array([self.predict_row(xi) for xi in x])\n\n    def predict_row(self, xi):\n        if self.is_leaf: return self.val\n        t = self.lhs if xi[self.var_idx]<=self.split else self.rhs\n        return t.predict_row(xi)","a9579678":"x = np.arange(0,50)\nx = pd.DataFrame({'x':x})","251880ce":"# just random uniform distributions in differnt range\n\ny1 = np.random.uniform(10,15,10)\ny2 = np.random.uniform(20,25,10)\ny3 = np.random.uniform(0,5,10)\ny4 = np.random.uniform(30,32,10)\ny5 = np.random.uniform(13,17,10)\n\ny = np.concatenate((y1,y2,y3,y4,y5))\ny = y[:,None]","f417545b":"x.shape, y.shape","e06ddef6":"plt.figure(figsize=(7,5))\nplt.plot(x,y, 'o')\nplt.title(\"Scatter plot of x vs. y\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.show()","80d6b272":"def std_agg(cnt, s1, s2): return math.sqrt((s2\/cnt) - (s1\/cnt)**2)","2666a688":"xi = x # initialization of input\nyi = y # initialization of target\n# x,y --> use where no need to change original y\nei = 0 # initialization of error\nn = len(yi)  # number of rows\npredf = 0 # initial prediction 0\n\nfor i in range(30): # like n_estimators\n    tree = DecisionTree(xi,yi)\n    tree.find_better_split(0)\n    \n    r = np.where(xi == tree.split)[0][0]    \n    \n    left_idx = np.where(xi <= tree.split)[0]\n    right_idx = np.where(xi > tree.split)[0]\n    \n    predi = np.zeros(n)\n    np.put(predi, left_idx, np.repeat(np.mean(yi[left_idx]), r))  # replace left side mean y\n    np.put(predi, right_idx, np.repeat(np.mean(yi[right_idx]), n-r))  # right side mean y\n    \n    predi = predi[:,None]  # make long vector (nx1) in compatible with y\n    predf = predf + predi  # final prediction will be previous prediction value + new prediction of residual\n    \n    ei = y - predf  # needed originl y here as residual always from original y    \n    yi = ei # update yi as residual to reloop\n    \n    \n    # plotting after prediction\n    xa = np.array(x.x) # column name of x is x \n    order = np.argsort(xa)\n    xs = np.array(xa)[order]\n    ys = np.array(predf)[order]\n    \n    #epreds = np.array(epred[:,None])[order]\n\n    f, (ax1, ax2) = plt.subplots(1, 2, sharey=True, figsize = (13,2.5))\n\n    ax1.plot(x,y, 'o')\n    ax1.plot(xs, ys, 'r')\n    ax1.set_title(f'Prediction (Iteration {i+1})')\n    ax1.set_xlabel('x')\n    ax1.set_ylabel('y \/ y_pred')\n\n    ax2.plot(x, ei, 'go')\n    ax2.set_title(f'Residuals vs. x (Iteration {i+1})')\n    ax2.set_xlabel('x')\n    ax2.set_ylabel('Residuals')\n    \n    ","6fc56919":"## Scatter plot of data","6ea0f1c4":"Errors are not changing much after `20th iteration` and pattern in residuals is also removed. Residuals look distributed around the mean","a4f6b3f1":"## Gradient boosting from scratch (No Math)\n-  In-depth and simple explanation of gradient boosting algorithm","c2c7d3e5":"where, $y_i$ = ith target value, $y_i^p$ = ith prediction, $ L(y_i, y_i^p) $ is Loss function, $\\alpha$ is learning rate. So the last equation tells us that, we need to adjust predictions based on our residuals, i.e. $\\sum {(y_i - y_i^p)}$. This is what we did, we adjusted our predictions using the fit on residuals. (accordingly adjusting value of $\\alpha$","3d914b84":"## Gradient Boosting (DecisionTrees in a loop)\n","679533bc":"$ Predictions = y_i^p $  \n$ Loss = L(y_i, y_i^p) $  \n$ Loss = MSE = \\sum {(y_i - y_i^p)}^2 $  \n$ y_i^p = y_i^p + \\alpha * \\delta {L(y_i, y_i^p)}\/ \\delta{y_i^p } $  \n$ y_i^p = y_i^p + \\alpha * \\delta {\\sum {(y_i - y_i^p)}^2}\/ \\delta{y_i^p } $  \n$ y_i^p = y_i^p - \\alpha * 2*{\\sum {(y_i - y_i^p)}} $  ","a2170158":"## Data simulation","b17b3667":"The logic of **gradient boosting** is very simple (if explained intuitively, without using mathematical notation). I expect that whoever is reading this would have done simple linear regression modeling.\nOne of the very basic assumption of linear regression is that it's sum of residuals is 0. Although, tree based models are not based on any of such assumptions, but if we think logic (not statistics) behind these assumptions, we might argue that, if sum of residuals is not 0, then most probably there is some pattern in the residuals of our model which can be leveraged to make our model better.\nSo, the intuition behind gradient boosting algorithm is to `leverage the pattern in residuals and strenghten a weak prediction model, until our residuals become randomly (maybe random normal too) distributed`. Once we reach a stage that residuals do not have any pattern that could be modeled, we can stop modeling residuals (otherwise it might lead to overfitting). Algorithmically, we are minimizing our loss function, such that test loss reach it\u2019s minima.","42b73884":"### Maths behind this logic"}}