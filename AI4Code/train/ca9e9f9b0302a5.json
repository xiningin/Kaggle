{"cell_type":{"b2bfeaaa":"code","384a2d49":"code","a66ff644":"code","1aacca46":"code","358dc2c9":"code","aafc5ab0":"code","2be3036f":"code","c96b7b93":"code","7114ef48":"code","952c582c":"code","da92970d":"code","2ced59ea":"code","e5b16d1b":"code","e4db2621":"code","bed72010":"code","b6c420b9":"code","b1db390e":"code","d0057607":"code","4fec591e":"code","1190b57b":"code","ec3d8726":"code","45b2fffe":"code","e86a176b":"code","e740a06f":"code","d0291901":"code","5238175f":"code","81028800":"code","2fbe0356":"code","75da7c79":"code","e25b4042":"code","45815f1e":"code","1de0feca":"code","1ceb68b7":"code","e50cd673":"code","cabf17d4":"code","71790e2d":"code","c6cf154c":"code","f3a4339a":"code","a89a295d":"code","d1f5dfb8":"markdown","994fbcbe":"markdown","da3fbcb3":"markdown"},"source":{"b2bfeaaa":"import numpy as np \nimport pandas as pd","384a2d49":"from  datetime import datetime, timedelta\nimport gc\nimport lightgbm as lgb","a66ff644":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","1aacca46":"# historical daily unit sales data per product and store\nstv = pd.read_csv(\"..\/input\/m5-forecasting\/sales_train_validation.csv\")","358dc2c9":"stv.head()","aafc5ab0":"# information about the price of the products sold per store and date\nsp = pd.read_csv(\"..\/input\/m5-forecasting\/sell_prices.csv\")","2be3036f":"sp.head()","c96b7b93":"# calendar dates info\ncal = pd.read_csv(\"..\/input\/m5-forecasting\/calendar.csv\")","7114ef48":"cal.head()","952c582c":"ss = pd.read_csv(\"..\/input\/m5-forecasting\/sample_submission.csv\")\nss.head()","da92970d":"stv.info()","2ced59ea":"# drop NA values from 'sales_train_validation.csv'\nstv.dropna(inplace = True)\nstv.shape","e5b16d1b":"CAL_DTYPES = {\"event_name_1\": \"category\", \"event_name_2\": \"category\", \"event_type_1\": \"category\", \n         \"event_type_2\": \"category\", \"weekday\": \"category\", 'wm_yr_wk': 'int16', \"wday\": \"int16\",\n        \"month\": \"int16\", \"year\": \"int16\", \"snap_CA\": \"float32\", 'snap_TX': 'float32', 'snap_WI': 'float32' }\nPRICE_DTYPES = {\"store_id\": \"category\", \"item_id\": \"category\", \"wm_yr_wk\": \"int16\",\"sell_price\":\"float32\" }","e4db2621":"pd.options.display.max_columns = 50","bed72010":"cal.tail()","b6c420b9":"h = 28 \nmax_lags = 57\ntrain_last = 1913\nf_day = datetime(2016,4, 25) \nf_day","b1db390e":"def create_stv(is_train = True, nrows = None, first_day = 1500):\n    sales_price = pd.read_csv(\"..\/input\/m5-forecasting\/sell_prices.csv\", dtype = PRICE_DTYPES)\n    for col, col_dtype in PRICE_DTYPES.items():\n        if col_dtype == \"category\":\n            sales_price[col] = sales_price[col].cat.codes.astype(\"int16\")\n            sales_price[col] -= sales_price[col].min()\n            \n    cal = pd.read_csv(\"..\/input\/m5-forecasting\/calendar.csv\", dtype = CAL_DTYPES)\n    cal[\"date\"] = pd.to_datetime(cal[\"date\"])\n    for col, col_dtype in CAL_DTYPES.items():\n        if col_dtype == \"category\":\n            cal[col] = cal[col].cat.codes.astype(\"int16\")\n            cal[col] -= cal[col].min()\n    \n    start_day = max(1 if is_train  else train_last-max_lags, first_day)\n    numcols = [f\"d_{day}\" for day in range(start_day,train_last+1)]\n    catcols = ['id', 'item_id', 'dept_id','store_id', 'cat_id', 'state_id']\n    dtype = {numcol:\"float32\" for numcol in numcols} \n    dtype.update({col: \"category\" for col in catcols if col != \"id\"})\n    stv = pd.read_csv(\"..\/input\/m5-forecasting\/sales_train_validation.csv\", \n                     nrows = nrows, usecols = catcols + numcols, dtype = dtype)\n    \n    for col in catcols:\n        if col != \"id\":\n            stv[col] = stv[col].cat.codes.astype(\"int16\")\n            stv[col] -= stv[col].min()\n    \n    if not is_train:\n        for day in range(train_last+1, train_last+ 28 +1):\n            stv[f\"d_{day}\"] = np.nan\n    \n    stv = pd.melt(stv,\n                  id_vars = catcols,\n                  value_vars = [col for col in stv.columns if col.startswith(\"d_\")],\n                  var_name = \"d\",\n                  value_name = \"sales\")\n    \n    stv = stv.merge(cal, on= \"d\", copy = False)\n    stv = stv.merge(sales_price, on = [\"store_id\", \"item_id\", \"wm_yr_wk\"], copy = False)\n    \n    return stv","d0057607":"def create_features(stv):\n    lags = [7, 28]\n    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n    for lag, lag_col in zip(lags, lag_cols):\n        stv[lag_col] = stv[[\"id\",\"sales\"]].groupby(\"id\")[\"sales\"].shift(lag)\n\n    wins = [7, 28]\n    for win in wins :\n        for lag,lag_col in zip(lags, lag_cols):\n            stv[f\"rmean_{lag}_{win}\"] = stv[[\"id\", lag_col]].groupby(\"id\")[lag_col].transform(lambda x : x.rolling(win).mean())\n\n    \n    \n    date_features = {\n        \n        \"wday\": \"weekday\",\n        \"week\": \"weekofyear\",\n        \"month\": \"month\",\n        \"quarter\": \"quarter\",\n        \"year\": \"year\",\n        \"mday\": \"day\",\n    }\n    \n    \n    for date_feat_name, date_feat_func in date_features.items():\n        if date_feat_name in stv.columns:\n            stv[date_feat_name] = stv[date_feat_name].astype(\"int16\")\n        else:\n            stv[date_feat_name] = getattr(stv[\"date\"].dt, date_feat_func).astype(\"int16\")","4fec591e":"FIRST_DAY = 500","1190b57b":"%%time\n\nstv = create_stv(is_train=True, first_day= FIRST_DAY)\nstv.shape","ec3d8726":"stv.info()","45b2fffe":"stv.head()","e86a176b":"%%time\n\ncreate_features(stv)\nstv.shape","e740a06f":"stv.info()","d0291901":"stv.head()","5238175f":"stv.dropna(inplace = True)\nstv.shape","81028800":"cat_features = ['item_id', 'dept_id','store_id', 'cat_id', 'state_id'] + ['event_name_1', 'event_name_2', 'event_type_1', 'event_type_2']\nuseless_columns = ['id', 'date', 'sales','d', 'wm_yr_wk', 'weekday']\ntrain_columns = stv.columns[~stv.columns.isin(useless_columns)]\nX_train = stv[train_columns]\ny_train = stv['sales']","2fbe0356":"%%time\n\nnp.random.seed(1000)\n\nfake_valid_inds = np.random.choice(X_train.index.values, 2_000_000, replace = False)\ntrain_inds = np.setdiff1d(X_train.index.values, fake_valid_inds)\ntrain_data = lgb.Dataset(X_train.loc[train_inds], label = y_train.loc[train_inds], \n                         categorical_feature=cat_features, free_raw_data=False)\nfake_valid_data = lgb.Dataset(X_train.loc[train_inds], label = y_train.loc[train_inds],\n                              categorical_feature=cat_features,\n                 free_raw_data=False)# This is a random sample.","75da7c79":"#train_data.savebinary('train.bin')","e25b4042":"del stv, X_train, y_train, fake_valid_inds,train_inds ; gc.collect()","45815f1e":"# Defining parameters for the model\nparams = {\n        \"objective\" : \"poisson\",\n        \"metric\" :\"rmse\",\n        \"force_row_wise\" : True,\n        \"learning_rate\" : 0.01,\n#         \"sub_feature\" : 0.8,\n        \"sub_row\" : 0.75,\n        \"bagging_freq\" : 1,\n        \"lambda_l2\" : 0.1,\n#         \"nthread\" : 4\n        \"metric\": [\"rmse\"],\n    'verbosity': 1,\n    'num_iterations' : 100,\n    'num_leaves': 100,\n    \"min_data_in_leaf\": 100,\n}","1de0feca":"%%time\n# We will use LightGBM model\n\n\nm_lgb = lgb.train(params, train_data, valid_sets = [fake_valid_data], verbose_eval=1)","1ceb68b7":"m_lgb.save_model(\"model.lgb\")","e50cd673":"ss = pd.read_csv(\"..\/input\/m5-forecasting\/sample_submission.csv\")","cabf17d4":"ss.head()","71790e2d":"%%time\n\nalphas = [1.028, 1.023, 1.018]\nweights = [1\/len(alphas)]*len(alphas)\nsub = 0.\n\nfor icount, (alpha, weight) in enumerate(zip(alphas, weights)):\n\n    te = create_stv(False)\n    cols = [f\"F{i}\" for i in range(1,29)]\n\n    for tdelta in range(0, 28):\n        day = f_day + timedelta(days=tdelta)\n        print(tdelta, day)\n        tst = te[(te.date >= day - timedelta(days=max_lags)) & (te.date <= day)].copy()\n        create_features(tst)\n        tst = tst.loc[tst.date == day , train_columns]\n        te.loc[te.date == day, \"sales\"] = alpha*m_lgb.predict(tst) # magic multiplier by kyakovlev\n\n\n\n    te_sub = te.loc[te.date >= f_day, [\"id\", \"sales\"]].copy()\n\n    te_sub[\"F\"] = [f\"F{rank}\" for rank in te_sub.groupby(\"id\")[\"id\"].cumcount()+1]\n    te_sub = te_sub.set_index([\"id\", \"F\" ]).unstack()[\"sales\"][cols].reset_index()\n    te_sub.fillna(0., inplace = True)\n    te_sub.sort_values(\"id\", inplace = True)\n    te_sub.reset_index(drop=True, inplace = True)\n    te_sub.to_csv(f\"submission_{icount}.csv\",index=False)\n    if icount == 0 :\n        sub = te_sub\n        sub[cols] *= weight\n    else:\n        sub[cols] += te_sub[cols]*weight\n    print(icount, alpha, weight)\n\n\nsub2 = sub.copy()\nsub2[\"id\"] = sub2[\"id\"].str.replace(\"validation$\", \"evaluation\")\nsub = pd.concat([sub, sub2], axis=0, sort=False)\nsub.to_csv(\"submission.csv\",index=False)","c6cf154c":"sub.head(10)","f3a4339a":"sub.id.nunique(), sub[\"id\"].str.contains(\"validation$\").sum()","a89a295d":"sub.shape","d1f5dfb8":"<font color = \"blue\">As we can see the RMSE is about 3.4%. It can be further reduced by increasing the number of itertions.","994fbcbe":"# Project goal is forecasting item sales at stores in various locations for two 28-day time periods.\n\nDatasets provided for the project are as follows:\n    \ncalendar.csv - Contains information about the dates on which the products are sold.\n\nsales_train_validation.csv - Contains the historical daily unit sales data per product and store [d_1 - d_1913]\n\nsample_submission.csv - The correct format for submissions. Reference the Evaluation tab for more info.\n\nsell_prices.csv - Contains information about the price of the products sold per store and date.\n\nsales_train_evaluation.csv - NOT available, will be made available one month before competition deadline. Will include sales [d_1 - d_1941]\n\nData provided cover stores for three US States (California, Texas, and Wisconsin) and includes item level, department, product categories, and store details and has explanatory variables such as price, promotions, day of the week, and special events.","da3fbcb3":"# Formatting datasets"}}