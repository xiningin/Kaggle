{"cell_type":{"2b237d76":"code","040e131f":"code","fd813fb5":"code","eed6d858":"code","4184a2a7":"code","3ffa0cfc":"code","527e358e":"code","71c525e0":"code","64f88f85":"code","0d3972bb":"code","5477c6e8":"code","d0ef52f9":"code","3ed2536b":"code","d1ad03a8":"code","664a6d46":"code","85a3e194":"code","f23c2982":"code","34e05b2c":"code","beee18a1":"code","ee3798b9":"code","fc9228fa":"code","e3535679":"code","67dd83bf":"code","968e778e":"code","29613c30":"markdown"},"source":{"2b237d76":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n%matplotlib inline","040e131f":"# Loading our dataset\ndf = pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')","fd813fb5":"#Checking the format of the data\ndf.head()","eed6d858":"#To view some basic statistical details\ndf.describe()","4184a2a7":"df.info()","3ffa0cfc":"sns.color_palette(\"hls\", 8)\nsns.pairplot(df,hue='Outcome')","527e358e":"#The distribution curve of Glucose wrt Outcome shows that there are less number of people with high Glucose level but they have higher chances of diabetes.","71c525e0":"##The plots tells the following -\n#Over the Pregnancy range, females with high glucose have Diabetes.\n#As Insulin increase, and as Glucose, there are higher chances of Diabetes.\n#As BMI increase, and as Glucose, there are higher chances of Diabetes.\n#Age alone isn't really an indicator of Diabetes.","64f88f85":"sns.heatmap(df.corr(), cmap='viridis')","0d3972bb":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()","5477c6e8":"scaler.fit(df.drop('Outcome', axis=1))","d0ef52f9":"scaler_features = scaler.transform(df.drop('Outcome', axis=1))","3ed2536b":"df_feat = pd.DataFrame(scaler_features,columns=df.columns[:-1])\ndf_feat.head()","d1ad03a8":"from sklearn.model_selection import train_test_split\n","664a6d46":"X_train, X_test, y_train, y_test = train_test_split(scaler_features,df['Outcome'],\n                                                    test_size=0.30)","85a3e194":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()","f23c2982":"knn.fit(X_train, y_train)\npred = knn.predict(X_test)","34e05b2c":"#Now let's evaluate our model performance\n#There are two widely used persormance metrics\nfrom sklearn.metrics import classification_report, confusion_matrix","beee18a1":"print(confusion_matrix(y_test, pred))\nprint('\\n')\nprint(classification_report(y_test, pred))","ee3798b9":"#Lets go ahead and choose an elbow methodto get correct k value\nerror_rate = []\n\n# Will take some time\nfor i in range(1,50):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","fc9228fa":"plt.figure(figsize=(10,6))\nplt.plot(range(1,50),error_rate,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","e3535679":"# FIRST A QUICK COMPARISON TO OUR ORIGINAL K=1\nknn = KNeighborsClassifier(n_neighbors=1)\n\nknn.fit(X_train,y_train)\npred = knn.predict(X_test)\n\nprint('WITH K=1')\nprint('\\n')\nprint(confusion_matrix(y_test,pred))\nprint('\\n')\nprint(classification_report(y_test,pred))\n","67dd83bf":"# NOW WITH K=17\nknn = KNeighborsClassifier(n_neighbors=17)\n\nknn.fit(X_train,y_train)\npred = knn.predict(X_test)\n\nprint('WITH K=17')\nprint('\\n')\nprint(confusion_matrix(y_test,pred))\nprint('\\n')\nprint(classification_report(y_test,pred))","968e778e":"#All our metrices showing an improved model.","29613c30":"**No Null Values. We can proceed with our data analysis.**"}}