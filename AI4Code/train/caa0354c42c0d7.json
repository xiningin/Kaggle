{"cell_type":{"86f4d6b6":"code","af0a4a7a":"code","c89ef22c":"code","00017912":"code","f51cd335":"code","4f9af743":"code","0a22c73a":"code","9d8872a8":"code","1e710808":"code","4295e9c5":"code","373cef49":"code","bbc99428":"code","339192dd":"code","77b199ba":"code","cfb4f588":"code","a1de86aa":"code","09d9abdf":"code","52fb2870":"code","07a52a71":"code","cb57c1da":"code","a1bfcf54":"code","4fd7176f":"code","bbd303d5":"code","18286155":"code","519038c9":"code","f3b662a9":"code","3eefa4f3":"code","6711cb40":"code","c1a5013e":"code","4f266809":"markdown","9a2b3f2b":"markdown","29c2d0de":"markdown","dcd12452":"markdown","20d24ac0":"markdown","c5f5fe66":"markdown","ca5b46da":"markdown","ec745d5b":"markdown","fc2622a7":"markdown","aa171f63":"markdown","28a5d083":"markdown","382a772b":"markdown","524bb9f6":"markdown","9380aa20":"markdown"},"source":{"86f4d6b6":"_ = !pip install fugue[sql] pandera[io] great_expectations\n\nfrom dask.distributed import Client\nclient = Client(processes=True, n_workers=4)","af0a4a7a":"import pandas as pd\nimport numpy as np\nimport pandera as pa\n\ndef make_sample(n=100):\n    np.random.seed(0)\n    \n    def make_color(c, mean, std, ranges):\n        df = pd.DataFrame(dict(color=c, weight=np.random.normal(mean, std, n)))\n        result = df[df.weight>10]\n        ranges[c] = (float(result.weight.min())-5, float(result.weight.max())+5)\n        return result\n    \n    ranges = {}\n    df = pd.concat([\n        make_color(\"red\", 100, 10.0, ranges),\n        make_color(\"yellow\", 200, 20.0, ranges),\n        make_color(\"green\", 150, 15.0, ranges),\n        make_color(\"blue\", 300, 30.0, ranges),\n        make_color(\"pink\", 220, 10.0, ranges),\n        make_color(\"black\", 120, 5.0, ranges),\n        make_color(\"brown\", 250, 30.0, ranges),\n        make_color(\"purple\", 180, 30.0, ranges),\n    ]).sample(frac=1.0)\n    \n    # df[\"radius\"] = np.random.normal(10, 1, df.shape[0])\n    return df.reset_index(drop=True), ranges","c89ef22c":"import dask.dataframe as dd\nimport json\n\nbig_sample, ranges = make_sample(1000000)\ndd.from_pandas(big_sample, 8).to_parquet(\".\/data.parquet\")\nwith open(\"ranges.json\", \"w\") as f:\n    json.dump(ranges, f)\nsample, _ = make_sample(1000)\nsample.to_parquet(\"sample.parquet\")","00017912":"import pandas as pd\n\ndata = pd.read_parquet(\"data.parquet\").reset_index(drop=True)\nsample = pd.read_parquet(\"sample.parquet\")\nwith open(\"ranges.json\", \"r\") as f:\n    ranges = json.load(f)","f51cd335":"import seaborn\n\nseaborn.histplot(sample, x=\"weight\", hue=\"color\")\nsample","4f9af743":"data","0a22c73a":"ranges","9d8872a8":"import pandera as pa\n\nrule = pa.DataFrameSchema(\n    {\n        \"weight\": pa.Column(float, checks=pa.Check.in_range(10,500)),\n    }\n)\nbad_rule = pa.DataFrameSchema(\n    {\n        \"weight\": pa.Column(float, checks=pa.Check.in_range(100,500)),\n    }\n)\nrule.validate(sample) # returns the DataFrame","1e710808":"bad_rule.validate(sample)","4295e9c5":"from fugue import transform\n\ntransform(sample, rule.validate, schema=\"*\")","373cef49":"transform(sample, bad_rule.validate, schema=\"*\")","bbc99428":"import fugue_dask\nimport dask.dataframe as dd\n\ndask_df = dd.read_parquet(\"data.parquet\")\ntransform(dask_df, rule.validate, schema=\"*\", engine=\"dask\").compute()","339192dd":"from scipy.stats import normaltest\n\ndef norm_check(color):\n    return pa.Hypothesis(\n        test=normaltest,\n        relationship=lambda stat, pvalue, alpha=0.001: pvalue >= alpha,\n        samples=[color],\n        groupby=\"color\",\n    )\n\nchecks = [norm_check(color) for color in sample.color.unique()]\n\nnorm_rule = pa.DataFrameSchema(\n    {\n        \"color\": pa.Column(str),\n        \"weight\": pa.Column(checks = checks),\n    }\n)\n\nnorm_rule.validate(sample);","77b199ba":"%%timeit -n1 -r2\nnorm_rule.validate(data)","cfb4f588":"NORMAL = pa.Hypothesis(\n    test=normaltest,\n    relationship=lambda stat, pvalue, alpha=0.001: pvalue >= alpha,\n)\n\nsimple_norm_rule = pa.DataFrameSchema(\n    {\n        \"weight\": pa.Column(checks = NORMAL)\n    },\n)\n\ntransform(sample, simple_norm_rule.validate, schema=\"*\", partition={\"by\":\"color\"});","a1de86aa":"%%timeit -n1 -r2\ntransform(data, simple_norm_rule.validate, schema=\"*\", partition={\"by\":\"color\"})","09d9abdf":"%%timeit -n1 -r2\ntransform(dd.read_parquet(\"data.parquet\"), simple_norm_rule.validate, schema=\"*\", partition={\"by\":\"color\"}, engine=\"dask\").compute()","52fb2870":"ranges","07a52a71":"from scipy.stats import normaltest\n\ndef norm_check(color):\n    return pa.Hypothesis(\n        test=normaltest,\n        relationship=lambda stat, pvalue, alpha=0.001: pvalue >= alpha,\n        samples=[color],\n        groupby=\"color\",\n    )\n\ndef range_check(groups, ranges):\n    for color in groups.keys():\n        if groups[color].min()<ranges[color][0] or groups[color].max()>ranges[color][1]:\n            return False\n    return True\n\nchecks = [norm_check(color) for color in sample.color.unique()]\nchecks.append(\n    pa.Check(\n        lambda g:range_check(g, ranges),\n        groupby=\"color\"),\n)\n\np_rule = pa.DataFrameSchema(\n    {\n        \"color\": pa.Column(str),\n        \"weight\": pa.Column(checks = checks),\n    }\n)\n\np_rule.validate(sample);","cb57c1da":"%%timeit -r2 -n1\np_rule.validate(data);","a1bfcf54":"NORMAL = pa.Hypothesis(\n    test=normaltest,\n    relationship=lambda stat, pvalue, alpha=0.001: pvalue >= alpha,\n)\n\nf_rule = pa.DataFrameSchema(\n    {\n        \"weight\": pa.Column(checks = NORMAL)\n    },\n    checks = [pa.Check(lambda df: df.weight.between(*ranges[df.color.iloc[0]]))]\n)\n\ntransform(sample, f_rule.validate, schema=\"*\", partition={\"by\":\"color\"});","4fd7176f":"%%timeit -r2 -n1\ntransform(data, f_rule.validate, schema=\"*\", partition={\"by\":\"color\"});","bbd303d5":"%%timeit -n1 -r2\ntransform(dd.read_parquet(\"data.parquet\"), f_rule.validate, schema=\"*\", partition={\"by\":\"color\"}, engine=\"dask\").compute()","18286155":"import great_expectations as ge\n\ngdf = ge.from_pandas(sample)\nprint(gdf.expect_column_values_to_be_between(\"weight\", 70, 500)[\"success\"])\nprint(gdf.expect_column_values_to_be_between(\"weight\", 70, 500, mostly=0.8)[\"success\"])","519038c9":"from typing import List, Dict, Any, Iterable\n\n# schema: color:str,error:str\ndef validate(min_max:List[Dict[str,Any]], subset:pd.DataFrame, mostly:float) -> Iterable[List[Any]]:\n    gdf = ge.from_pandas(subset)\n    result = gdf.expect_column_values_to_be_between(\"weight\", min_max[0][\"min\"], min_max[0][\"max\"], mostly=mostly)\n    if not result[\"success\"]:\n        yield [min_max[0][\"color\"], str(result)]\n        \nprint(list(validate([dict(color=\"red\",min=70,max=200)], sample[sample.color==\"red\"], 0.8)))\nprint(list(validate([dict(color=\"red\",min=70,max=200)], sample[sample.color==\"red\"], 1.0)))","f3b662a9":"from fugue import FugueWorkflow\nfrom fugue.column import col, functions as ff\n\ndef summarize(errors:List[List[Any]]) -> None:\n    assert len(errors)==0, str(errors[:3])\n    \ndef profile_and_validate(sample, data, mostly, engine=None, engine_conf=None):\n    with FugueWorkflow(engine, engine_conf) as dag:\n        ranges = dag.df(sample).select(col(\"color\"), ff.min(col(\"weight\")).alias(\"min\"), ff.max(col(\"weight\")).alias(\"max\"))\n        errors = ranges.zip(dag.df(data)).transform(validate, params=dict(mostly=mostly))\n        errors.output(summarize)\n    return data\n\nprofile_and_validate(sample, sample, mostly=1.0)","3eefa4f3":"profile_and_validate(sample, data, mostly=1.0)","6711cb40":"profile_and_validate(sample, data, mostly=0.8)","c1a5013e":"profile_and_validate(sample, dd.read_parquet(\"data.parquet\"), mostly=0.8, engine=\"dask\").compute()","4f266809":"## Scenario 4: Profile and Validate By Color Distributedly\n\nIn this scenario, we are going to see how Fugue can work with Great Expectations. We are going to profile the sample dataset to get the weight range of each color, and then use GE to validate on each color distributedly.\n\n### GE Code\n\nGE has a very useful feature `mostly` so that you can assert how much percentage of data is valid.","9a2b3f2b":"### Pandera + Fugue Version","29c2d0de":"### Pandera + Fugue Version\n\nWith Fugue `transform` function, we can make the Pandera rules to run in another equivalent way","dcd12452":"### Distributed Validation\n\nThe true value of the transform function is to make this validation fully distributed. Now we can use the same Pandera validation on a dask dataframe. Actually, we can make the validation work on Spark just by changing `engine` to a Spark session.\n\n<img src=\"https:\/\/i.ibb.co\/xgxQcFw\/transform-no-partition.png\" width=\"600\">\n","20d24ac0":"### Distributed Validation","c5f5fe66":"### Distributed Validation","ca5b46da":"## Scenario 2: Validate Weight Distributions By Color\n\nFrom the plot of sample, we can see that for each color, the weights seem to be normally distributed. So we can use hypothesis testing to make sure they are.\n\n### Pandera Version","ec745d5b":"When we get the value ranges from sample, since we know weights are normally distributed in each color, there can be outliers when we have more data, but as long as they are in certain percentage, we consider them to be valid. So the `mostly` can do exactly what we want to achieve.\n\nNow let's write the validation function to work on one color group","fc2622a7":"## Scenario 3: Validate Weight Ranges & Distributions By Color\n\nIn the previous example, the hypothesis testing rule doesn't change on different colors. But if we want to validate the weight ranges for each color, the rules will change.\n\n### Pandera Version","aa171f63":"### Great Expectations + Fugue\n\nFor this particular case, we will not use GE profiler to get the range from samples, because that can be a very complicated task. Fugue has very elegant solution to get ranges, and it's framework agnostic and scale agnostic","28a5d083":"# PyData 2021 Scalable Data Validation With Fugue\n\n## Environment Preparation","382a772b":"## Scenario 1: Validate Weight In Range\n\n### Pandera Version\n\nPandera is so far one of the most lightweight python data validation frameworks. So we will use Pandera to validate the dataset.","524bb9f6":"## Load Data & Visualize\n\nWe have a large collection of colored balls. We have dataframes recording their colors and weights. We have some validation tasks on these dataframes","9380aa20":"### Pandera + Fugue Version\n\n<img src=\"https:\/\/i.ibb.co\/2cdfrcp\/transform-partition.png\" width=\"600\">"}}