{"cell_type":{"fc70c5ee":"code","4c0c06fb":"code","daf068a0":"code","9810010b":"code","b7b4acfa":"code","0f541ae6":"code","e101de83":"code","08fa4144":"code","4f592f14":"code","4e75cba3":"code","928fcd56":"code","f22a0dc7":"code","a532fb0f":"code","7b528a7f":"code","21b891a0":"code","a61be55a":"code","9103b8ed":"code","87bac96c":"code","e2e7eb56":"code","f463b45a":"code","494867b1":"code","da6d6f2e":"code","fae294e3":"code","b96a42ff":"code","a42d1731":"code","698f051f":"code","696e5af5":"code","29ffb005":"code","706321e7":"code","3056c448":"code","10adab30":"code","1b0bd5e4":"code","17e39272":"code","1f494d52":"markdown","446f0424":"markdown","9da5fda2":"markdown","5fb3ab99":"markdown","01bd587c":"markdown","22fc4479":"markdown","39bcc0fe":"markdown","be08ce5f":"markdown","35185436":"markdown","79f12b34":"markdown","6603701b":"markdown","8b744f7f":"markdown","61ff28f4":"markdown","11f158db":"markdown","10293b7e":"markdown","f258d57d":"markdown","bc14a7f0":"markdown","5379fc05":"markdown","87e5fa46":"markdown","b7b756c0":"markdown","8bbfc55f":"markdown","cd353d7d":"markdown","ac74c7eb":"markdown","20193a17":"markdown","1161de8b":"markdown","e1a2cb87":"markdown","4d7902d2":"markdown","8fad62fd":"markdown","828b5d51":"markdown","b8e4ffbe":"markdown","454e15d6":"markdown","2528e93d":"markdown","a5fb188d":"markdown","82063257":"markdown","b6e4a165":"markdown","6db607c2":"markdown","1b2e08ee":"markdown","fe060409":"markdown"},"source":{"fc70c5ee":"import pandas as pd\nimport numpy as np\nfrom numpy.random import uniform\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom random import sample\nfrom math import isnan\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import cut_tree\nfrom scipy.cluster.hierarchy import dendrogram\n\nfrom yellowbrick.cluster import silhouette_visualizer\n\nimport warnings\nwarnings.filterwarnings('ignore')","4c0c06fb":"customers= pd.read_csv('..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv')\ncustomers.head()","daf068a0":"len(customers[customers.duplicated()])","9810010b":"customers.isna().sum()","b7b4acfa":"customers.info()","0f541ae6":"# Changing Column names\n\ncustomers.columns= ['CustomerID', 'Gender', 'Age', 'Annual Income', 'Spending Score']\n","e101de83":"customers.describe()","08fa4144":"for i in customers.columns:\n    print(f'Column {i} is {customers[i].dtype} with {customers[i].nunique()} unique values')","4f592f14":"# This copy is made only for EDA activity\neda_df= customers.copy(deep= True)\neda_df.head()","4e75cba3":"# Dropping CustomerID\neda_df.drop(labels= 'CustomerID', axis= 1, inplace= True)","928fcd56":"eda_df.head(2)","f22a0dc7":"fig, ax= plt.subplots(nrows= 2, ncols= 3, figsize= (16, 4))\nplt.suptitle('data distribution of continuous variables'.upper(), fontsize= 20)\n\nj= 0\nfor i in eda_df.columns:\n    if i != 'Gender':\n        sns.distplot(eda_df[i], ax= ax[0, j])\n        sns.boxplot(eda_df[i], ax= ax[1, j])\n        \n        ax[0,j].set_title(f'data Distribution of {i}'.upper())\n        ax[0,j].set_xlabel('')\n        ax[0,j].set_ylabel('')\n        ax[1,j].set_xlabel('')\n        ax[1,j].set_ylabel('')\n        \n        plt.tight_layout()\n        j+=1\n        \nplt.show()","a532fb0f":"fig, ax= plt.subplots(nrows= 1, ncols= 3, figsize= (16, 4))\nplt.suptitle('genderwise distribution of continuous variables'.upper(), fontsize= 20)\n\nj= 0\nfor i in eda_df.columns:\n    if i != 'Gender':\n        sns.boxplot(x= eda_df['Gender'], y= eda_df[i], ax= ax[j], showmeans= True, \n                    meanprops= {'markerfacecolor': 'white', 'marker': 'o', 'markersize': '10'})\n        ax[j].set_title(f'data Distribution of {i}'.upper())\n        ax[j].set_xlabel('')\n        ax[j].set_ylabel('')\n        \n        plt.tight_layout()\n        j+=1\n        \nplt.show()","7b528a7f":"sns.pairplot(eda_df)","21b891a0":"sns.heatmap(eda_df.corr(), annot= True)\nplt.show()","a61be55a":"# Preparing copy of original dataframe to be used\ncustomersml= customers.copy(deep= True)\ncustomersml.head()\n","9103b8ed":"customersml.drop(labels= 'CustomerID', axis= 1, inplace= True)\ncustomersml.head(2)","87bac96c":"customersml['Gender']= customersml['Gender'].map({'Male': 1, 'Female': 0})\ncustomersml.head()","e2e7eb56":"\nscaler= MinMaxScaler()\ncustomersml[['Age', 'Annual Income', 'Spending Score']]= scaler.fit_transform(customersml[['Age', 'Annual Income', 'Spending Score']])\ncustomersml.head()\n","f463b45a":"# Hopkins Test Algorithm\ndef hopkins(X):\n    d = X.shape[1]\n    #d = len(vars) # columns\n    n = len(X) # rows\n    m = int(0.1 * n) \n    nbrs = NearestNeighbors(n_neighbors=1).fit(X.values)\n \n    rand_X = sample(range(0, n, 1), m)\n \n    ujd = []\n    wjd = []\n    for j in range(0, m):\n        u_dist, _ = nbrs.kneighbors(uniform(np.amin(X,axis=0),np.amax(X,axis=0),d).reshape(1, -1), 2, return_distance=True)\n        ujd.append(u_dist[0][1])\n        w_dist, _ = nbrs.kneighbors(X.iloc[rand_X[j]].values.reshape(1, -1), 2, return_distance=True)\n        wjd.append(w_dist[0][1])\n \n    H = sum(ujd) \/ (sum(ujd) + sum(wjd))\n    if isnan(H):\n        print(ujd, wjd)\n        H = 0\n \n    return H\n","494867b1":"# Applying above algorithm on our data\nhopkins(customersml)","da6d6f2e":"# Generating a list of k values between 2 and 15\nk_experiment= [i for i in range(2,16)]\nk_experiment","fae294e3":"# Identifying ssd at each of the k values and plotting\n\nssd= []\n\nfor i in k_experiment:\n    kmeans= KMeans(n_clusters= i, max_iter= 300)\n    kmeans.fit(customersml)\n    ssd.append(kmeans.inertia_)\n    \nplt.figure(figsize= (8,4))\nplt.title(f'elbow curve showing k means inertia for k values between {min(k_experiment)} to {max(k_experiment)}'.upper())\nplt.plot(ssd, 'r--x', label= 'Sum of Squared Distances')\nplt.xlabel('k value experiment'.title())\nplt.ylabel('ssd'.upper())\nplt.xticks([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14, 15,16], [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15, 16, 17])\nplt.show()\n","b96a42ff":"for i in k_experiment:\n    kmeans= KMeans(n_clusters= i, max_iter= 300)\n    kmeans.fit(customersml)\n    sil_score= silhouette_score(X= customersml, labels= kmeans.labels_)\n    print(f'Silhouette score for {i} is {round(sil_score, 4)}')","a42d1731":"silhouette_visualizer(KMeans(n_clusters= 3, random_state= 42), X= customersml[['Gender', 'Age', 'Annual Income', 'Spending Score']])\nplt.show()","698f051f":"kmeans= KMeans(n_clusters= 3, max_iter= 300)\nkmeans.fit(customersml)\nkmeans.labels_","696e5af5":"# Incorporating labels in orignial dataset\ncustomers['k_means_clusters']= kmeans.labels_","29ffb005":"customers.head(2)","706321e7":"mergings= linkage(customersml, method= 'complete', metric= 'euclidean')\ndendrogram(mergings)\nplt.show()","3056c448":"cut_tree(mergings, n_clusters= 3).reshape(-1,)","10adab30":"# Incorporating labels in original dataset\ncustomers['hierarchical_clusters']= cut_tree(mergings, n_clusters= 3).reshape(-1,)\ncustomers.head(2)","1b0bd5e4":"cluster_sizes= pd.DataFrame()\n\ncluster_sizes['Cluster Number']= [i for i in range(customers['k_means_clusters'].nunique())]\ncluster_sizes['k_means size']= [len(customers[customers['k_means_clusters']== i]) \n                                for i in range(customers['k_means_clusters'].nunique())]\ncluster_sizes['dendrogram size']= [len(customers[customers['hierarchical_clusters']== i]) \n                                   for i in range(customers['hierarchical_clusters'].nunique())]\ncluster_sizes\n","17e39272":"cluster_type= ['k_means_clusters', 'hierarchical_clusters']\nattributes= ['Age', 'Annual Income', 'Spending Score']\n\nfor i in range(len(attributes)):\n    fig, ax= plt.subplots(nrows= 1, ncols= 2, figsize= (20, 5), sharey= True)\n\n    for j in range(len(cluster_type)):\n        sns.boxplot(x= cluster_type[j], y= attributes[i], data= customers, ax= ax[j])\n        ax[j].set_title(f'{attributes[i]} as per {cluster_type[j]}', fontsize= 20)\n        ax[j].set_xlabel('')\n        ax[j].set_ylabel('')\n        plt.tight_layout()\n\nplt.show()\n","1f494d52":"### Using k-means algorithm to create customer clusters","446f0424":"No null values found","9da5fda2":"- From the results, apart from a few descripancies, the business results are similar:\n    - Need of Focus:\n        - Mall management needs to focus on cluster 2 since these people have similar annual income bracket as other 2 cluesters but still have relatively lesser spending score.\n        - Since these customers are relatively older, the marketing campaigns can be planned more suited for this age bracket (40 to 60 years)\n    - Need of Retention:\n        - Mall management must work at retaining cluster 0 since these people have similar annual income bracket as other 2 cluesters but still have highest spending score.\n        - Since these customers are relatively younger, the promotions aimed at younger crowd must not be discontinued.\n        \n- `We will need further data to assess. The above results could also be a reflection of type of retail outlets present in the mall (Maybe they are more attractive to younger generation.`","5fb3ab99":"# Modelling","01bd587c":"## Preparing Copy of original Dataframe to be used and basic cleaning","22fc4479":"## Null Values","39bcc0fe":"### Pairplots","be08ce5f":"- Above values of silhouette score also signifies k=3 as optimum","35185436":"## Quick Analysis of Continuous Variables","79f12b34":"# Loading Data","6603701b":"- All fields in int64 except Gender\n- Column names will be changed slightly to make them more suitable for rest of analysis","8b744f7f":"# Exploratory Data Analysis","61ff28f4":"#### Creating Elbow Curve to identify optimum k value","11f158db":"- A high hopkins statistic of 0.824 shows that dataset has tendency to form clusters.","10293b7e":"## Finding Duplicates","f258d57d":"No duplicates found","bc14a7f0":"### Hierarchical Clustering using k= 3","5379fc05":"- Age variance of males is spread much further out as compared to that of females. Mean age is slightly higher for males.\n- Annual incomes of feamles is spread more as comapred to that of males. Mean is almost similar.\n- Spending score for females is more prominent in 50th to 75th percentile. Mean score is similar for both genders.","87e5fa46":"- Possible presence of outliers","b7b756c0":"- Skews noticed in continuous data\n- Outliers not apparant. Outliers evident in describe functions were actually skews and not really outliers.","8bbfc55f":"#### Using Silhouette Score to asses the optimum k value","cd353d7d":"## Scaling continuous variables","ac74c7eb":"- Elbow curve shows a possible optimized k value at k=3","20193a17":"### K Means Clustering using k= 3","1161de8b":"- Clusters seem to be stable and well away from each other.\n- Cluster 1 is expected to be largest. Cluster 0 and 2 are expected to be relatively smaller.","e1a2cb87":"## Gender Based Analysis of Data","4d7902d2":"### Heatmap","8fad62fd":"## Checking clustering tendency using Hopkins Statistic","828b5d51":"- No apparent string relationships detected between independent variables except age and spend score that seem to be moderately related.","b8e4ffbe":"## Clustering","454e15d6":"### Statistical Check to confirm cluster stability","2528e93d":"### Cluster Size Comparisons","a5fb188d":"# Basic Data Understanding","82063257":"## Creating Dummies of categorical column","b6e4a165":"# Business Inferences","6db607c2":"- Visual Clusters evident between Annual Income and Spending Score\n- Slight clustering tendency noticed between Age and Spending score","1b2e08ee":"# Importing Libraries","fe060409":"# Data Integrity Checks"}}