{"cell_type":{"8bcda3cd":"code","7da59bd7":"code","e42d89d9":"code","607169be":"code","717151e7":"code","ed467530":"code","11b36c44":"code","d9dfa526":"code","daee2c1d":"code","e3c6d775":"code","a1443d14":"code","9f3dc891":"code","d14e754e":"code","6459478d":"code","c702f10c":"code","7cac9fa4":"code","7ce8f238":"code","c51bfd34":"code","10afe14b":"code","2db9b2c6":"code","9c5491fc":"code","69be3fe4":"code","216232d5":"code","ebfa41f8":"code","f7579fbe":"code","3351bf08":"code","796360b6":"code","5d1c2b8f":"code","709fc970":"code","6df9b8cd":"code","3ab038d4":"code","74694d12":"code","50b71da2":"code","1b649e1c":"code","ad9e9fc6":"code","1fd7c6fa":"code","218fc50b":"code","5dd3e75c":"code","da3cf15f":"code","5fa226ef":"code","f4de9a5c":"code","1f5cc1e1":"code","de86b364":"code","9b06d607":"code","79456489":"code","afadd0d6":"code","2b3918a1":"code","11e0ce6a":"markdown","f7796d5d":"markdown","5b3f0aa4":"markdown","0655541e":"markdown","7e621cff":"markdown","e1043d87":"markdown","00910528":"markdown","68dc69ed":"markdown","97ba4462":"markdown","2752d1a8":"markdown","2ab1c06a":"markdown","f44fdb8a":"markdown","65c50316":"markdown","77b53705":"markdown","fa72f0d1":"markdown","c0e5a2c0":"markdown","1da3fdec":"markdown","15b1b43d":"markdown","4f232c01":"markdown","cac33d86":"markdown","ade72993":"markdown","266a4720":"markdown","5ac4d80a":"markdown","3cedb792":"markdown","2ee7be03":"markdown","33629641":"markdown","f60d3fd7":"markdown"},"source":{"8bcda3cd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7da59bd7":"df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv');print(df.shape)\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv');print(test.shape)\n\n# Lets combine train set( df) and test together so its easier to process everything in one go\n# Just remember test set doesnt have 'Survived' column, so everything in NaN when we combine them\ndf = df.append(test, ignore_index=True)\n\n# Lets just drop 'PassengerID' cause it won't be useful in this case\ndf.drop(columns='PassengerId',inplace=True)\ndf.info()\n\n# Double checking how I should seperate train & test set for later\n# temp = data.loc[:890,:]\n# temp2 = data.loc[891:,:]","e42d89d9":"df.Name.head(30)","607169be":"# From what I see aove, we can extract the title by getting text that follows by a dot(.)\n# Now lets get the title for each person\n\nfor name_string in df['Name']:\n    df['Title']=df['Name'].str.extract('([A-Za-z]+)\\.',expand=True) # Use '([A-Za-z]+)\\.' here to get string follow by dot\n\n# Okay lets see how many value trns out\ndf['Title'].value_counts()","717151e7":"# Okay, after looking at several other kernels, I realised that Don, Sir, Col, Major, Jonkheer, Capt can be map as Mr. \n# Similarly, Mlle, Mme, Ms - Miss, and Lady, Countess, Dona as Mrs.\n\n# Mapping by changing several similar titles to common ones (ie: Don to Mr)\nmapping = {'Mlle': 'Miss', 'Major': 'Mr', 'Col': 'Mr', 'Sir': 'Mr', 'Don': 'Mr', 'Mme': 'Miss',\n          'Jonkheer': 'Mr', 'Lady': 'Mrs', 'Capt': 'Mr', 'Countess': 'Mrs', 'Ms': 'Miss', 'Dona': 'Mrs'}\n\n# So replace the title that we want using the mapping dictionary\ndf.replace({'Title': mapping}, inplace=True)\n\n# Get the new title count\ndf.Title.value_counts()","ed467530":"# Now lets start the imputation stuff. First we impute the age using the median of each title. \n# Get the median age in each categorical title, then impute it to the null value in age\n\n# From above we get the following titles:\ntitles=['Mr','Miss','Mrs','Master','Rev','Dr']\n# So use this to get the median age \nfor title in titles:\n    listOfMedianAgeForEachTitle = df.groupby('Title')['Age'].median() # Get the median for each title and put it in a list\n    age_to_impute = listOfMedianAgeForEachTitle[titles.index(title)] # For the speciic title in the current loop, get the median age using the index\n    print(title, age_to_impute) # Print the title and age for the current loop\n    \n    # So if the age of the current title is null, replace is to the age_to_impute\n    df.loc[(df['Age'].isnull()) & (df['Title'] == title), 'Age'] = age_to_impute\n    \n    # End Loop\n\n# Check how which columns still null\ndf.isnull().sum()","11b36c44":"# From what I observed in the data description in Kaggle, seems like ticket number varies for each person\n# meaning that it might just be a non-meaningfull number. Lets doublecheck the number of values\ndf.Ticket.value_counts()","d9dfa526":"# Ticket got 929 distinct values in 1300++ entries, so won't be useful for the machine learning. Drop em'\n# We already used Name and replace it to title, so lets drop em'\n# Cabin got heaps of NaN. So lets just drop em'\ndf.drop(['Cabin','Name','Ticket'],axis=1,inplace=True)\n\ndf.info()","daee2c1d":"# So embarked & fare got null entries (Survived too but we know that test fle doesnt have em so lets ignore it now)\n# Fare is numeric so can easily fill NaN as mean value\n# Embarked is an object so lets check the value counts\ndf.Embarked.value_counts()","e3c6d775":"# Fill the NaN in Embarked column with the highest value count and Fare with the mean()\ndf.Embarked.fillna(df.Embarked.value_counts().argmax(),inplace=True)\ndf.Fare.fillna(df.Fare.mean(),inplace=True)\ndf.info()","a1443d14":"# I learned that some category is easier if we convert them from category to Numeric. \n# So lets figure out the number of distinct values in each object columns\n\nfor col in df.columns:\n    if df[col].dtype not in ['int64','float64']:\n        totalDistinct = df[col].value_counts().count()\n        print(col,totalDistinct)\n","9f3dc891":"# Okay so we have 3 in embarked, 2 in sex and 6 in title. maybe lets convert the embarked and sex first\n# SEX - Convert female = 0, male = 1\ndf['Sex'] = df.Sex.apply(lambda x: 0 if x == \"female\" else 1)\n\n#EMBARKED - Convert Q=0, C=1, S=2\ndf['Embarked'] = df.Embarked.apply(lambda x: 0 if x == \"Q\" else (1 if x==\"C\" else 2))\n\nprint(df.Sex.value_counts())\nprint(df.Embarked.value_counts())","d14e754e":"# So wait, parch is the number of parents aboard & SibSp is the number of childern aboard.\n# Maybe its better if we add both of them to a column 'NoOfFamilies'. Arguable here but I wanna take this route \n# to simplify things\n\ndf['NoOfFamilies']=df['SibSp']+df['Parch']\n\ndf.NoOfFamilies.value_counts()","6459478d":"# Not bad, 790 were single and the rest got families aboard. I could split them into category of 2 class, single & taken \n# but not too sure maybe theres somebody who came with 1 parent or something like that. \n# I'll maybe try and split them to 0-no family and 1-have family. Will come back to see if that improves the accuracy\nhaveFamilies = (df.NoOfFamilies>0) # For later. Not sure whether i'll use it or not\n\n# Drop the Parch & SibSp columns cause I alread have the NoOfFamilies column\ndf.drop(columns=['Parch','SibSp'],inplace=True)\n\n# Sweet, now i think we are readdy to move. Lets just double check that everything is good\ndf.info()","c702f10c":"# Convert all object to category (in our case just the title actually)\nfor col in df.columns:\n    if df[col].dtype not in ['int64','float64']:\n        df[col]=df[col].astype('category')","7cac9fa4":"# Checking the correlation & put it in a heatmap plot\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ncorrmat =df.corr()\ncorrInd = corrmat.index[abs(corrmat['Survived'])>0]\nplt.figure(figsize=(8,8))\nsns.heatmap(df.corr(),\n            annot=True, cmap = 'Blues',\n            vmin=0,vmax=1,square=True\n           )\n","7ce8f238":"# Use describe just to inspect the values from each numeric columns. So I know whether column is boolean or not, whats the mean and std\ndf.describe()","c51bfd34":"# Plot Embarked countplot with Survived as the hue. Recall 0-Q, 1-C, 2-S\nsns.countplot('Embarked',data=df,hue='Survived')\nplt.show()\n\n# What I can say here is that embarked may not be a huge factor for predicting the survival rate. You can see that\n# the percentage of survival in each case is pretty much 50-50 except for ","10afe14b":"# Dropping the embarked column\ndf.drop(columns='Embarked',inplace=True)","2db9b2c6":"# Plot the sex with survived as hue. Female-0, Male-1\nsns.countplot('Sex',data=df,hue='Survived')\nplt.show()\n\n# The plot kinda makes sense as female were prioritised compared to male. So less female died & more male died","9c5491fc":"# Plot the age and survived as hue\nsns.countplot('Age',data=df,hue='Survived')\nplt.show()","69be3fe4":"bins = [0, 6, 15, 21, 35, 50, 100]\nnames = ['Baby', 'Kids', 'Teenage', 'YoungAdult', 'Adult','Senior']\n\ndf['AgeRange'] = pd.cut(df['Age'], bins, labels=names)\n\nprint(df.AgeRange.value_counts())","216232d5":"# Plot the ageRange and survived as hue\nsns.countplot('AgeRange',data=df,hue='Survived')\nplt.show()\n\n# Sweet and lets delete the age column cause its not necessary anymore\ndf.drop(columns='Age',inplace=True)","ebfa41f8":"# So I know fare will be the same as age. so I need to sepearte them into groups first. \n# But not sure whats the group to divide them into. According to this wiki page,\n# $0-40 for 3rd class, $60 for 2nd class and $150 for 1st class. Check the link here:\n# http:\/\/www.jamescamerononline.com\/TitanicFAQ.htm \n\nbins = [-1, 50, 149, 1000] # (-1,51) (51,149) (150,1000)\nnames = [3, 2, 1]\n\ndf['FareRange'] = pd.cut(df['Fare'], bins, labels=names)\n\nprint(df.FareRange.value_counts())\nprint(df.Pclass.value_counts())\n\n# So compare between both FareRange and Pclass, there were a bit difference here. Maybe some of them paid discounted price\n# or early birds, so they got cheap price for a good Pclass.","f7579fbe":"# # This part was here because I previously set the lower limit in bins as 0 (see above) \n# # and got several NaN values in FareRange.\n# # After looking at the Fare value of the ones that appear as NaN in the FareRange, I noticed all was 0.\n# # So change the lower limit from 0 to -1 and it solves the issue. \n\n# Null_FareRange=df.FareRange[df.FareRange.isnull()==True]\n# df.Fare.loc[Null_FareRange.index]","3351bf08":"# Similarly, I think Fare is not important anymore as we have FareRange now. Lets drop Fare\n\ndf.drop(columns='Fare',inplace=True)","796360b6":"# countplot for FareRange with survived as hue\nsns.countplot('FareRange',data=df,hue='Survived')\nplt.show()\n\n# Both FareRange 1 & 2 showed higher percentage of survival compaed to FareRange 3 (lowest range).\n# If theres anything we learn here, its a good idea to pay for better class when you go for a cruise holiday.\n# At least its evident here that they have better chance of survival!","5d1c2b8f":"sns.countplot('NoOfFamilies',data=df,hue='Survived')","709fc970":"# Looks to me that we can divide them into 3 groups:\n# 0 - NoFamily (0)\n# 1 - SmallFamily(1-3)\n# 2 - BigFamily (4-10)\n\nbins = [-1,0,3,1000] # (-1,0) (1,3) (4,1000)\nnames = [0,1,2]\n\ndf['FamilySize'] = pd.cut(df['NoOfFamilies'], bins, labels=names)\n\nsns.countplot('FamilySize',data=df,hue='Survived')\nplt.show()","6df9b8cd":"# Okay we can drop NoOfFamilies column\n\ndf.drop(columns='NoOfFamilies',inplace=True)","3ab038d4":"# Import necessary tools\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\nencoder = LabelEncoder()\nsc = StandardScaler()\n\ndef labelNscale(df):\n    # Find the columns of all category data and put it in list\n    colsCateg = [col for col in df.columns if df[col].dtype not in ['int64','float64']]\n    # For all category data, apply LabelEncoder on them\n    for col in colsCateg:\n        df[col]=encoder.fit_transform(df[col])\n    \n    # Now apply scaler on all data (numeric & category)\n    df_ = sc.fit_transform(df)\n    df = pd.DataFrame(data=df_,columns=df.columns)\n    return df\n\ndf_R=labelNscale(df)\ndf_R.describe()","74694d12":"# Recall we have train and test set? \n# Train has a shape of 891 rows & test has 418. \n# Now lets split them \ntrainSet = df_R.loc[:890,:]\nfor_y = df.loc[:890,:]\ntestSet = df_R.loc[891:,:]\n\nprint(trainSet.shape)\nprint(testSet.shape)\n\n# Doublecheck train info\ntrainSet.info()\n\n# Drop the testSet survived column\ntestSet.drop(columns='Survived',inplace=True)\ntestSet.info()\n","50b71da2":"# Sweet everythings good. Now split the trainSet into X & y\ny = for_y['Survived']\nX = trainSet.drop(columns = ['Survived']).copy()\nprint(X.shape)\ny","1b649e1c":"# Splitting the train set into train_train and train_test.\n# Im using StratifiedShuffleSplit cause I want the train_Train set to have \n# equal survival percentage values to the train_Test set. \nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=1)\n\nfor train_index, test_index in sss.split(X, y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n","ad9e9fc6":"# Double check percentage split of y_train and y_test\nprint((y_train.value_counts())\/len(y_train))\nprint(y_test.value_counts()\/len(y_test))","1fd7c6fa":"# Use knn & XGB as first guess model\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nrfc = RandomForestClassifier(max_depth=10,n_estimators =15,random_state=5)\nrfc.fit(X_train,y_train)\ny_pred_rfc = rfc.predict(X_test)\nprint('rfcScore: {:.4f}'.format(rfc.score(X_train,y_train)))\nacc_rfc=accuracy_score(y_test,y_pred_rfc)\nprint('Score using rfc is: ',acc_rfc)\n\nknn = KNeighborsClassifier()\nknn.fit(X_train,y_train)\ny_pred = knn.predict(X_test)\nprint('knnScore: {:.4f}'.format(knn.score(X_train,y_train)))\nacc_knn=accuracy_score(y_test,y_pred)\nprint('Score using knn is: ',acc_knn)\n\nxgb = XGBClassifier()\nxgb.fit(X_train,y_train)\ny_pred_xgb = xgb.predict(X_test)\nprint('xgbScore: {:.4f}'.format(xgb.score(X_train,y_train)))\nacc_xgb=accuracy_score(y_test,y_pred_xgb)\nprint('Score using XGB is: ',acc_xgb)","218fc50b":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\n\nlr = LogisticRegression()\ndtc = DecisionTreeClassifier()\n\nclassifiers = [('K Nearest Neighbours', knn),('Random Forest Classifier',rfc),('XGB',xgb)]\n\nfor clf_name, clf in classifiers:\n    clf.fit(X_train,y_train)\n    \n    y_pred = clf.predict(X_test)\n    \n    print('{:s} : {:.4f}'.format(clf_name,accuracy_score(y_test, y_pred)))\n    \nvc = VotingClassifier(estimators=classifiers)\nvc.fit(X_train,y_train)\ny_pred_vc = vc.predict(X_test)\nprint('VC : {:.4f}'.format(accuracy_score(y_test, y_pred)))","5dd3e75c":"# Define the process to split and predict as function with several input that we want to change\n# This wil return the accuracy of teh XGB model\n\ndef findOptimalXGB(X,y,nsplit,testsize):\n\n    sss = StratifiedShuffleSplit(n_splits=nsplit, test_size=testsize, random_state=1)\n    for train_index, test_index in sss.split(X, y):\n        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    xgb = XGBClassifier()\n    xgb.fit(X_train,y_train)\n    y_pred_xgb = xgb.predict(X_test)\n    acc_xgb=accuracy_score(y_test,y_pred_xgb)\n    \n    return acc_xgb, xgb","da3cf15f":"print('Optimal test size')\ntestSize = []\nnsplits = []\naccuracy = []\n\nfor i in np.arange(0.1,0.35,0.05):\n    for j in range(2,17,2):\n        acc,xgb = findOptimalXGB(X,y,j,i)\n#         print('nsplits:{:.1f}, testSize:{:.2f}, acc:{:.4f}'.format(j,i,acc))\n        testSize.append(i)\n        nsplits.append(j)\n        accuracy.append(acc)\n\n\n# Now repeat for optimal settings so that its default for our test prediction next\n# acc = findOptimalXGB(X,y,10,0.3)","5fa226ef":"# Plot graph\n\ncmap = sns.cubehelix_palette(dark=.3, light=.8, as_cmap=True)\nsns.scatterplot(testSize,accuracy,hue=nsplits,size = nsplits,sizes=(20, 200), palette=cmap)\nplt.xlabel('Test Size (The legend shows nsplits size)')\nplt.ylabel('Accuracy')\nplt.show()","f4de9a5c":"acc, optXGB = findOptimalXGB(X,y,10,0.25)\n\nprint(acc)\npredictions = optXGB.predict(testSet)\n","1f5cc1e1":"from sklearn.model_selection import GridSearchCV\nparam_grid = {'max_depth':range(3,11,1),'learning_rate':np.arange(0.01,0.2,0.04)}\nfor i in range(5,10,1):\n    grid = GridSearchCV(xgb,param_grid=param_grid,cv=i)\n    grid.fit(X_train,y_train)\n    # print(grid.grid_scores_)\n    print('cv:',i, grid.best_params_, 'Score:{:.4f}'.format(grid.best_score_))\n","de86b364":"# Set and get the opimal grid\ngrid = GridSearchCV(xgb,param_grid=param_grid,cv=6)\ngrid.fit(X_train,y_train)\noptimalGrid = grid.best_estimator_","9b06d607":"print(xgb.score(X_test,y_test))\nprint(optimalGrid.score(X_test,y_test))","79456489":"# Apply confusio matrix to see how good our fitting is\nfrom sklearn.metrics import confusion_matrix, recall_score\ny_pred = xgb.predict(X_test)\ntn, fp, fn, tp = confusion_matrix(y_test,y_pred).ravel()\nprint(confusion_matrix(y_test,y_pred).ravel())\nprint(recall_score(y_test,y_pred))\nprint(tp\/(tp+fn))\n","afadd0d6":"predictions = xgb.predict(testSet)\n","2b3918a1":"test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nPassengerId = test['PassengerId']\n\nsubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\nsubmission.to_csv(path_or_buf =\"Titanic_Submission.csv\", index=False)\nprint(\"Submission file is formed\")\n","11e0ce6a":"NoOfFamilies","f7796d5d":"<br><br><br><br><br><br>\n**1. Data Cleaning**<br><br>\nAfter importing & inspecting the train.csv, I've noticed that Cabin has more than half of null object. So I decided to remove it from the df. <br><br>Name and ticket can also be dropped. But wait, maybe good idea to categorise the name according to the persons title<br><br>The next part will be on cleaning the data & converting the object to category","5b3f0aa4":"> **\"\"\"The objective of this work is to use regression\/machine learning process to predict whether or not the titanic passenger survive based on their personal data (ie: name, age, gender, socio-economic class, etc).\"\"\"**","0655541e":"You can see that its a bit struggling to identify the age because the gap is huge. So what I'm thinking is converting this into categorical data where:<br>\n<= 5 is Baby<br> \n6-14 is Kids<br>\n15-20 is Teenage<br>\n21-34 is YoungAdult<br>\n35-49 is Adult<br>\n50 is Senior\n","7e621cff":"What I observed here is that :<br>\n* 'Survived' is positively correlated with 'Fare'<br>\n* 'Survived' is negatively correlated with sex, pclass & age. (kinda logic here cause females (sex), younglings (age) were the priorities.)<br>\n* 'Survived' is negatively correlated with Embarked. Recall emabarked Q-0, C-1 & S-2. Seems passenger emarked from Q(queenstown) have better chances of survival. But also maybe because theres less passenger from Q. Lets look at it later <br>\n<br>Pclass 1 (1st class) seem to survive better here. Also makes sense, you pay more for better insurance I guess.\n<br><br> The rest had minor correlation, but maybe still useful. I think its best if we make a plot for each column against the survived","e1043d87":"**3.1 Splitting data into training and test set**\n<br><br>Now that I have labelise them, its time to split the data into training & test sets","00910528":"Certainly not the best model but I'm satisfied enough by how I understand the data and use manpulate it to predict unseen data. Wht I'll do next is to broaden my knowledge on making pipelines & perhaps use pd.get_dummies on some columns. Then I'll come back and see if it can improves the accuracy of the model. Cheers!","68dc69ed":"<br><br><br><br><br><br><br><br><br>\n**5. Predict the test and put into submission file **","97ba4462":"<br><br><br><br><br><br><br><br><br>\n**2. A bit of statistics on the data**<br><br>\nOkay, all the data cleaning is done. Let's have a look at the stats. Here I will look at:<br>1. The count of each variables with 'Survived' set as hue. This is to understand how different class\/group affects the chance of survival<br>2. For continous data, I will segregate them into different groups particularly for Age & Fare. <br><br>\nSo we'll take a closer look at each column starting from correlation plot, then Embarked, Sex, Age, Fare and NoOfFamilies. Other columns may not be necessary at this stage but will certainly be included later to see potential ways to improve accuracy","2752d1a8":"Embarked","2ab1c06a":"Sex","f44fdb8a":"Age","65c50316":"Fare","77b53705":"Just finished an online data science course that took 100hours to complete, so I thought I challenge myself to my first proper Kaggle submission. The aim here is try to do simple analysis and perform machine learning on the data with reasonable accuracy (>70%). Theres much more to be learn, but for now I'm happy to settle with this. So lets start.","fa72f0d1":"Now I am trying to find ways to improve the models. ","c0e5a2c0":"Before that, heres the Table of Content\n<br><br><br>\n0. Importing data<br><br>\n1. Data cleaning<br><br>\n2. See stats feature (mainly countplot) & segregation of continous data<br><br>\n3. Applying labelEncoder, scaling the data & splitting the training sets into train_train & train_test<br><br>\n4. Applying the ML (KNN & XGB as starting point). Then make a bit of optimal investigation on data splitting<br><br>\n5. Predict the test & prepare submission file","1da3fdec":"<br><br><br><br><br><br><br><br><br>\n**4. Now apply the ML on them. Classifier not Regressor.**\n<br><br>Alright, now let's see how it goes when I apply several models to predict them.\nI used regressor at first and got really low accuracy. Only then realised I should use classifier instead<br><br> The outline here is:<br>\na) Apply Knn & XGB, see whos more accurate<br>\nb) Compile the process of splitting data into function so we can loop over and see if changing the split size or nplits affect the accuracy by how much. Then choose the appropriate one.","15b1b43d":"What we learn here:<br><br>1. Change all continous data into categorical type cause at the end of the day, we want to predict categorical outcome.\n<br>2. So Fare, NoOfFamilies and Age have been converted to different group class\n<br>3. Embarked seems pretty uncorrelated in my point of view. so dropping it.\n<br><br> Okay, now lets move on and get our hands dirty with ML! Wubba lubba dup dup!!!!","4f232c01":"Lets use the mid point - test size of 0.2, nsplits of 15","cac33d86":"> My comments were written in a mixed markdown section & with the hashtag '#' in the code section. Sorry I'm used to do this with Matlab, so trying to slowly get off the habit.  ","ade72993":"<br><br><br><br><br><br><br><br><br>\n**3. Turn the category data to numeric (and scaling)**\n<br><br>Okay, done with the stats stuff, now lets get to the real meat! <br> \nI'm going to introduce LabelEncoder to labelise the category<br><br>\nI dont think we need StandardScaler cause the min and max for each colum range between 0 and 5. But maybe I'll try use it later and see if it improves the accuracy","266a4720":"What I can say here is that embarked may not be a huge factor for predicting the survival rate. You can see that the percentage of survival in each case is pretty much 50-50 except for embarked from Southampton (2) but again, this might just be accidental because a lot of people died that day. I'm struggling to see the correlation whether you die or not depending on where you board the ship(Unless if the distance between the points of boarding location is really far). So I'm dropping this column in the dataset.\n<br>\n![image.png](attachment:image.png)\n<br> ","5ac4d80a":"I'll choose cv6 here cause the score dropped in cv=7 and started to rise again in cv=8 & cv=9 which may be overfitting. ","3cedb792":"<br><br><br><br><br>\n**0. Importing data**","2ee7be03":"So using default option with n_splits of 10 and tset_size of 0.3 gave the best prediction of 86%. Lets stick with that. ","33629641":"<a href=\"Titanic_Submission.csv\"> Download File <\/a>","f60d3fd7":"Not bad with 84% of accuracy using the XGB. Definitely can get better accuracy if I put more effort on segregating the data. But happy with this right now. Lets try improve model?"}}