{"cell_type":{"536f6ac0":"code","8f0af3d9":"code","9059171f":"code","e7de02a3":"code","44286346":"code","03256323":"code","ad42767e":"code","04cc5baf":"code","a3caf723":"code","d2886f01":"code","8e14ca69":"code","a2cadc27":"code","4df6b25d":"code","cbb56e77":"code","1e19788c":"code","4e6d0931":"code","47a6f598":"code","a99af453":"code","bdcef40a":"code","6b63535e":"code","5fd6b598":"code","b06b4abc":"code","6a775517":"code","9fa9ff96":"code","22d03cd1":"code","49026c2f":"code","0483c0c8":"code","0df537ea":"code","c1035357":"code","739af015":"code","c5c8ea20":"code","a0efdaaf":"code","07062cae":"code","1b4f194f":"code","70f1c51c":"code","46484ebe":"code","d3eefaea":"code","390a0645":"code","c3d7fbfa":"code","e4f3e9a4":"code","d400fb2f":"code","94db83f0":"code","3b5caf35":"code","9571a568":"code","4cfd95e1":"code","40ee1558":"code","af5bd694":"code","e0fcb66c":"code","41cf7bed":"code","9aeaf843":"code","6c3405e5":"code","53fa89ae":"code","44ecf2fe":"code","f7ce3a75":"code","bc060df9":"code","27bd0cd6":"code","264752d9":"code","463baf52":"code","f76c5241":"code","56a4cb80":"code","8aabe50e":"code","a0e2f8c9":"code","59cc384a":"code","8e0c6d71":"code","a79d7f6e":"code","6de8d10a":"code","6217639a":"code","75479265":"code","53048ae8":"code","87f3710d":"code","6daff776":"code","9a785a20":"code","6d556168":"code","71b2570a":"code","b7c771b2":"code","f1d6848f":"code","3bb438b5":"markdown","5ee1c170":"markdown","78f5eb39":"markdown","b533dc8e":"markdown","42c4b374":"markdown","34c149c9":"markdown","7e478609":"markdown","06111d95":"markdown","da3d1055":"markdown","4d14b16c":"markdown","2d6ffed3":"markdown","b537cbcd":"markdown","cb86ad29":"markdown","aa0ec360":"markdown","afccb77c":"markdown","c05e0e0b":"markdown","e0b4ac57":"markdown","057d90cf":"markdown","2006f7af":"markdown","e17a2efd":"markdown","4cd74704":"markdown","b4b130bd":"markdown","731f98cf":"markdown","0d7ca2a3":"markdown","eaaae681":"markdown","4d3a9589":"markdown","a5969fff":"markdown","34af728d":"markdown","17f59142":"markdown","f8f33f80":"markdown","fb47b14b":"markdown","1660b877":"markdown","0e48b79d":"markdown","d5328fec":"markdown","9811876c":"markdown","52f7d095":"markdown","036f79e6":"markdown","4bbdad56":"markdown","23edf8ce":"markdown","dbc6bce3":"markdown","a6bf8338":"markdown","a41e3eb3":"markdown","c15f58b0":"markdown","dcdf88ee":"markdown","2d738ea0":"markdown","78655700":"markdown","54506dc5":"markdown","277ed9bf":"markdown","53953158":"markdown","ed4abd3e":"markdown","053bc6fa":"markdown","dbfdcd96":"markdown","c6550757":"markdown","4317ac50":"markdown","997af04d":"markdown","89d60d7a":"markdown","2c53d0bb":"markdown","2821429a":"markdown","d524dd8c":"markdown","6e3c66ec":"markdown","217d7ae5":"markdown","2311e58b":"markdown","e795ce2d":"markdown"},"source":{"536f6ac0":"import warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn import preprocessing\nfrom IPython.core.interactiveshell import InteractiveShell\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\n# from collections import defaultdicta\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n%matplotlib inline","8f0af3d9":"# READ DATA\n\nbikedata = pd.read_csv(\"..\/input\/boombikes\/day.csv\",parse_dates=['dteday']) \nprint(bikedata.head())","9059171f":"#shape check \nprint(bikedata.shape)","e7de02a3":"#  descriptive information check\n\nprint(bikedata.info())","44286346":"#descriptive  statistical information check\n\nprint(bikedata.describe())","03256323":"# percentage of missing values in each column\n\nround(100*(bikedata.isnull().sum()\/len(bikedata.index)), 2).sort_values(ascending=False)","ad42767e":"\nbike_duplicate = bikedata\n\n# Checking for duplicates and dropping the entire duplicate row if any\nbike_duplicate.drop_duplicates(subset=None, inplace=True)\nbike_duplicate.shape","04cc5baf":"bikedata.columns","a3caf723":"bikedata_new=bikedata[['season', 'yr', 'mnth', 'holiday', 'weekday',\n       'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed',\n       'cnt']]","d2886f01":"bikedata_new.info()","8e14ca69":"#converting the datatype to category\nbikedata_new['season']=bikedata_new['season'].astype('category')\nbikedata_new['weathersit']=bikedata_new['weathersit'].astype('category')\nbikedata_new['mnth']=bikedata_new['mnth'].astype('category')\nbikedata_new['weekday']=bikedata_new['weekday'].astype('category')\nbikedata_new.info()","a2cadc27":"#creating the dummy variables\n#using drop_first to drop the first variable for each set of dummies created\n\nbikedata_new = pd.get_dummies(bikedata_new, drop_first=True)\n\nbikedata_new.info()","4df6b25d":"bikedata_new.head()","cbb56e77":"from sklearn.model_selection import train_test_split\n\nnp.random.seed(0)\ndf_train, df_test = train_test_split(bikedata_new, train_size = 0.70, test_size = 0.30, random_state = 333)","1e19788c":"#checking out training set info\ndf_train.info()","4e6d0931":"#checking out training set size\ndf_train.shape","47a6f598":"#checking out testing set info\ndf_test.info()","a99af453":"#checking out testing set size\ndf_test.shape","bdcef40a":"\nbikedata_num=df_train[[ 'temp', 'atemp', 'hum', 'windspeed','cnt']] #taking only numerical variable\n\nsns.pairplot(bikedata_num, diag_kind='kde')\nplt.show()","6b63535e":"#taking categorical variables before creating dummy variables\n\nplt.figure(figsize=(25, 10))\nplt.subplot(2,3,1)\nsns.boxplot(x = 'season', y = 'cnt', data = bikedata)\nplt.subplot(2,3,2)\nsns.boxplot(x = 'mnth', y = 'cnt', data = bikedata)\nplt.subplot(2,3,3)\nsns.boxplot(x = 'weathersit', y = 'cnt', data = bikedata)\nplt.subplot(2,3,4)\nsns.boxplot(x = 'holiday', y = 'cnt', data = bikedata)\nplt.subplot(2,3,5)\nsns.boxplot(x = 'weekday', y = 'cnt', data = bikedata)\nplt.subplot(2,3,6)\nsns.boxplot(x = 'workingday', y = 'cnt', data = bikedata)\nplt.show()","5fd6b598":"plt.figure(figsize = (25,20))\nax=sns.heatmap(bikedata_new.corr(), annot = True, cmap=\"YlGnBu\")\nbottom,top = ax.get_ylim()\nax.set_ylim(bottom + 0.5, top - 0.5)\nplt.show()\n","b06b4abc":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n    ","6a775517":"df_train.head()","9fa9ff96":"df_train.columns","22d03cd1":"# Apply scaler()\n\nnumerical_vars = ['temp', 'atemp', 'hum', 'windspeed','cnt']\n\ndf_train[numerical_vars] = scaler.fit_transform(df_train[numerical_vars])","49026c2f":"# Checking values after scaling\ndf_train.head()","0483c0c8":"df_train.describe()","0df537ea":"y_train = df_train.pop('cnt')\nX_train = df_train","c1035357":"# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","739af015":"# Running RFE with the output number of the variable equal to 15\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 15)             # running RFE\nrfe = rfe.fit(X_train, y_train)\nlist(zip(X_train.columns,rfe.support_,rfe.ranking_))","c5c8ea20":"col = X_train.columns[rfe.support_]\ncol\n","a0efdaaf":"X_train.columns[~rfe.support_]","07062cae":"# Creating X_test dataframe with RFE selected variables\nX_train_rfe = X_train[col]","1b4f194f":"#VIF CHECK \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_rfe.columns\nvif['VIF'] = [variance_inflation_factor(X_train_rfe.values, i) for i in range(X_train_rfe.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","70f1c51c":"import statsmodels.api as sm\n\n# Add a constant\nX_train_lm1 = sm.add_constant(X_train_rfe)\n\n# Create a first fitted model\nlr1 = sm.OLS(y_train, X_train_lm1).fit()","46484ebe":"# parameter check\n\nlr1.params","d3eefaea":"#model Summary\nprint(lr1.summary())","390a0645":"X_train_new = X_train_rfe.drop([\"atemp\"], axis = 1)","c3d7fbfa":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","e4f3e9a4":"# Add a constant\nX_train_lm2 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlr2 = sm.OLS(y_train, X_train_lm2).fit()","d400fb2f":"# Check the parameters obtained\n\nlr2.params","94db83f0":"print(lr2.summary())","3b5caf35":"X_train_new = X_train_new.drop([\"hum\"], axis = 1)","9571a568":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","4cfd95e1":"# Add a constant\nX_train_lm3 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlr3 = sm.OLS(y_train, X_train_lm3).fit()","40ee1558":"lr3.params","af5bd694":"print(lr3.summary())","e0fcb66c":"X_train_new = X_train_new.drop([\"season_3\"], axis = 1)","41cf7bed":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","9aeaf843":"# Add a constant\nX_train_lm4 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlr4 = sm.OLS(y_train, X_train_lm4).fit()","6c3405e5":"lr4.params","53fa89ae":"print(lr4.summary())","44ecf2fe":"X_train_new = X_train_new.drop([\"mnth_10\"], axis = 1)","f7ce3a75":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","bc060df9":"# Add a constant\nX_train_lm5 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlr5 = sm.OLS(y_train, X_train_lm5).fit()","27bd0cd6":"lr5.params","264752d9":"print(lr5.summary())","463baf52":"X_train_new = X_train_new.drop([\"mnth_3\"], axis = 1)","f76c5241":"from statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","56a4cb80":"# Add a constant\nX_train_lm6 = sm.add_constant(X_train_new)\n\n# Create a first fitted model\nlr6 = sm.OLS(y_train, X_train_lm6).fit()","8aabe50e":"lr6.params","a0e2f8c9":"print(lr6.summary())","59cc384a":"lr6.params","8e0c6d71":"y_train_predict = lr6.predict(X_train_lm6)","a79d7f6e":"residual = y_train-y_train_predict\n\n\nfig = plt.figure()\nsns.distplot((residual), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)  \nplt.xlabel('Errors', fontsize = 18)    ","6de8d10a":"bikedata_new=bikedata_new[[ 'temp', 'atemp', 'hum', 'windspeed','cnt']]\n\nsns.pairplot(bikedata_num, diag_kind='kde')\nplt.show()","6217639a":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nvif = pd.DataFrame()\nvif['Features'] = X_train_new.columns\nvif['VIF'] = [variance_inflation_factor(X_train_new.values, i) for i in range(X_train_new.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","75479265":"#applying scaling\n\nnumerical_vars = ['temp', 'atemp', 'hum', 'windspeed','cnt']\n\ndf_test[numerical_vars] = scaler.transform(df_test[numerical_vars])\ndf_test.head()","53048ae8":"df_test.describe()","87f3710d":"#Dividing into X_test and y_test\n\ny_test = df_test.pop('cnt')\nX_test = df_test\n\nX_test.info()","6daff776":"#Selecting the variables that are part of final model.\ncol1=X_train_new.columns\n\nX_test=X_test[col1]\n\n# Adding constant variable to test dataframe\nX_test_lm6 = sm.add_constant(X_test)\n\nX_test_lm6.info()","9a785a20":"# Making predictions using the final model (lr6)\n\ny_predict = lr6.predict(X_test_lm6)","6d556168":"# Plotting y_test and y_pred to understand the spread\n# import matplotlib.pyplot as plt\n# import numpy as np\n\n\nfig = plt.figure()\nplt.scatter(y_test, y_predict, alpha=.5)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_pred', fontsize = 16) ","71b2570a":"#r2 = 1-(RSS\/TSS)\n\nfrom sklearn.metrics import r2_score\nr2_score(y_test, y_predict)","b7c771b2":"r2=0.8203092200749708","f1d6848f":"# n is number of rows in X\n\nn = X_test.shape[0]\n\n\n# Number of features (predictors, p) is the shape along axis 1\np = X_test.shape[1]\n\n# We find the Adjusted R-squared using the formula\n\nadjusted_r2 = 1-(1-r2)*(n-1)\/(n-p-1)\nadjusted_r2","3bb438b5":"# MODEL EVALUATION","5ee1c170":"# MODEL 5","78f5eb39":"Using the pair plot, we could see there is a linear relation between temp and atemp variable with the predictor \u2018cnt\u2019.","b533dc8e":"Removing the variable 'hum' based on its Very High 'VIF' value. - choosing 'hum' over 'temp' because based on general knowledge we can say temp can have effects on businessess like bike rental","42c4b374":"# Assumptions Validation","34c149c9":"Final Model Interpretation","7e478609":"# MODEL 6","06111d95":"##### FINAL RESULT COMPARISON: \n\n##### Train R^2 :0.824  \n\n##### Train Adjusted R^2 :0.821  \n\n##### Test R^2 :0.820  \n\n##### Test Adjusted R^2 :0.812","da3d1055":"## Hypothesis Testing:","4d14b16c":"We will split the entire data set into training set and testing set in 70:30 ration","2d6ffed3":"Now we have a model in which we have all the P-values as zero and also all the VIFs are less than 5 depicting very low multicollinearity between the predictors.So we can consider this as our final model.","b537cbcd":"# MODEL 4","cb86ad29":"The overall Significance of the final model is determined by the F-statistics value (higher the value greater the significance of the model).From the above summary of lr6 model we can see that it has :\n\nF-statistic:          233.8\n\nThe F-Statistics value of 233 (which is greater than 1) states that the overall model is significant\n","aa0ec360":"# DATA QUALITY CHECK","afccb77c":"# R^2 Test","c05e0e0b":"### Author : Diprodeep Ghosh","e0b4ac57":"# Model 3","057d90cf":"##### As no VIF value is above 5 so we can assume that there is no multicollinearity between the pradictor variables","2006f7af":"##### From the normally distributed residuals we can assume that the linear regression is valid","e17a2efd":"# RESCALING ","4cd74704":"After running the drop duplicate command the shape of the dataframe is same as that of the original.So we can say that there is no duplicate value in the dataset.","b4b130bd":"## Linear relationship between X and Y","731f98cf":"# READ DATA","0d7ca2a3":"# Adjusted R^2  TEST","eaaae681":"# **Problem Statement:**\nA US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state.\n\nIn such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\nThey have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:\n\nWhich variables are significant in predicting the demand for shared bikes.\nHow well those variables describe the bike demand\nBased on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors.","4d3a9589":"# Duplicate data Checking","a5969fff":"We will create boxplot for each of the categorical\nvariable to see how it stacks up with the target variable","34af728d":"# Data Cleaning","17f59142":"Removing the variable 'season3' based on its Very High 'VIF' value.","f8f33f80":"# **Business Goal:**\nWe are required to model the demand for shared bikes with the available independent variables. It will be used by the management to understand how exactly the demands vary with different features. They can accordingly manipulate the business strategy to meet the demand levels and meet the customer's expectations. Further, the model will be a good way for management to understand the demand dynamics of a new market.","fb47b14b":"Removing the variable 'mnth_10' based on its Very High p-value campared to others.","1660b877":"# EDA on Training Dataset","0e48b79d":"# FINAL RESULT","d5328fec":"### Removing Unwanted Columns","9811876c":"#### Residual Analysis Of Training Data","52f7d095":"## NULL\/MISSING values checking :","036f79e6":"# LINEAR MODEL","4bbdad56":"# Interpretation","23edf8ce":"### Significance of the final model","dbc6bce3":"# BOOM BIKES\n","a6bf8338":"# Categorical Variables","a41e3eb3":"##### From the above pair plot we can see a LINEAR relationship between temp,atemp and cnt","c15f58b0":"From the list above it can be seen that none of the coefficient are equal to zero which means we can reject the null hypothesis.\n","dcdf88ee":"Will create dummy variable for the following coloumns:\n\n1.'mnth'\n\n2.'weekday'\n\n3.'season' \n\n4.'weathersit'       ","2d738ea0":"so the data has been sucessfully split into 70% train data and 30% test data","78655700":"Removing the variable 'mnth_3' based on its High 'p-value' caomparing with others","54506dc5":"## Multicollinearity between the predictor variables","277ed9bf":"# Applying the Final Model(lr6) to make Prediction","53953158":"# Model 2","ed4abd3e":"From a high level analysis of the data dictionary we can conclude that the columns namely instant, dteday, casual & registered can be droped from the dataset.Following are the reasons why:\n\n1.instant : its only an index value \n\n2.dteday: we already have a seperate coloumn for month and year so we dont need date seperately.\n\n3.casual & registered : cotains count of bike with different category.since our count will not be specific to any category so we dont need it.\n\nI will be creating a new dataframe named bikedata_new which will have the dataframe with the droped coloumns,","053bc6fa":"H0 : B1 = B2 = ......... = Bn = 0\n\nH1 : at least one Bi != 0    ","dbfdcd96":"### Dividing the training dataset into X and Y sets for the model building","c6550757":"# Data Splitting\n","4317ac50":"### Recursive feature elimination:","997af04d":"# Model 1","89d60d7a":"cnt=0.084143+(yr\u00d70.230846)+(workingday\u00d70.043203)+(temp\u00d70.563615)\u2212(windspeed\u00d70.155191)+(season2\u00d70.082706)+(season4\u00d70.128744)+(mnth9\u00d70.094743)+(weekday6\u00d70.056909)\u2212(weathersit2\u00d70.074807)\u2212(weathersit3\u00d70.306992)","2c53d0bb":"###  Numeric Variables  ","2821429a":"# The equation of best fitted surface based on model lr6","d524dd8c":"# **Background:**\nA bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.","6e3c66ec":"# Creating Dummy Variable","217d7ae5":" we can see all the percentage value is zero so there is no missing or NULL value","2311e58b":"Removing the variable 'atemp' based on its High p-value & High VIF -","e795ce2d":"Following are the analysis from the above boxplots - \n\nseason: Season 3 has the highest nbr of booking with a median close to 5000 closely followed by season 2 and season 3. This indicates, season can be a good predictor for the dependent variable. - \n\nmnth: months 5,6,7,8 & 9 have majority bike booking with a median of hovering around 4000 booking per month. This indicates, mnth has some trend for bookings and can be a good predictor for the dependent variable. \n\nweathersit: Majority of the booking is happening during \u2018weathersit1 with a median of close to 5000 booking (for the period of 2 years). This was followed by weathersit2 .This indicates, weathersit does show some trend towards the bike bookings can be a good predictor for the dependent variable. \n\nholiday: Almost majority of the bike booking were happening when it is not a holiday which means this data is clearly biased so it will not be a good predictor for the dependent variable.\n\nweekday: weekday variable shows very close trend having their independent medians between 4000 to 5000 bookings. \n\nworkingday: greater no.of bike booking were happening in \u2018workingday\u2019 with a median of close to 5000 booking "}}