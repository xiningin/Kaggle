{"cell_type":{"3d66eb06":"code","3b380882":"code","aaebc400":"code","eb06ea34":"code","eda6154f":"code","44b94469":"code","576450e9":"code","0ecf6c75":"code","400e0409":"code","f3f34c38":"code","4b3d3920":"code","97145266":"code","7ac62a97":"code","9e4e0b3c":"code","895dd4e4":"code","4806714a":"code","c2921eb9":"code","6d99f209":"code","7e6898ff":"code","aee92b45":"code","554c5f8e":"code","bca2f3e9":"code","43fc5e4e":"code","b4f02fb3":"code","0226e6db":"code","38b61ce8":"code","afb29b1e":"code","ee4b9146":"code","32d4ccfe":"code","d2a61138":"code","9de8d7e7":"code","37c61373":"code","d62ecf90":"code","0e555658":"markdown","480f9dcd":"markdown","91ae8abf":"markdown","ac7d557f":"markdown","724ee526":"markdown","438b3645":"markdown","8cf5a3ce":"markdown","06b249cf":"markdown","b51a72a4":"markdown","c055af31":"markdown","6a74625b":"markdown","8b6199ee":"markdown","1a9c1230":"markdown","6029a309":"markdown","fd4438e0":"markdown","cdc590c2":"markdown","892db124":"markdown","4e0ca70e":"markdown","f6456820":"markdown","494c8f53":"markdown","f4b370d0":"markdown","8535bcef":"markdown"},"source":{"3d66eb06":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import models\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.applications.vgg16 import VGG16","3b380882":"import cv2\nimport matplotlib.pyplot as plt","aaebc400":"from albumentations import (\n    Compose, RandomBrightness, JpegCompression, HueSaturationValue, RandomContrast, HorizontalFlip, Rotate, GaussianBlur, Cutout\n)","eb06ea34":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","eda6154f":"tf.random.set_seed(0)","44b94469":"df = pd.read_csv('..\/input\/lego-minifigures-classification\/index.csv')\ndf.describe()","576450e9":"df.head()","0ecf6c75":"train_set = df[df['train-valid'] == 'train']\nvalid_set = df[df['train-valid'] == 'valid']","400e0409":"train_set","f3f34c38":"valid_set","4b3d3920":"train_paths = []\nfor path in train_set['path'].values:\n    train_paths.append(os.path.join('..\/input\/lego-minifigures-classification\/',path))\ntrain_paths","97145266":"valid_paths = []\nfor path in valid_set['path'].values:\n    valid_paths.append(os.path.join('..\/input\/lego-minifigures-classification\/',path))\nvalid_paths","7ac62a97":"train_labels = train_set['class_id'].values\ntrain_labels","9e4e0b3c":"valid_labels = valid_set['class_id'].values\nvalid_labels","895dd4e4":"dfmeta = pd.read_csv('..\/input\/lego-minifigures-classification\/metadata.csv')\nno_of_classes = dfmeta.shape[0]\nno_of_classes","4806714a":"class DataGenerator(tf.keras.utils.Sequence):\n    \n    def __init__(self, paths, labels = None, image_size = (512,512), batch_size = 32, num_classes = None, shuffle = False, transforms = False):\n        self.paths = paths\n        self.labels = labels\n        self.image_size = image_size\n        self.batch_size = batch_size\n        self.num_classes = num_classes\n        self.shuffle = shuffle\n        self.transforms = transforms\n        self.on_epoch_end()\n        \n    def __len__(self):\n        return len(self.paths)\/\/self.batch_size\n    \n    def __getitem__(self, index):\n        indices = self.indices[index * self.batch_size : (index + 1) * self.batch_size]\n        X, y = self.__get_data(indices)\n        return X, y\n    \n    def __get_data(self, indices):\n        batch = [self.paths[k] for k in indices]\n        images = []\n        for i in range(self.batch_size):\n            img = cv2.imread(batch[i])\n            img = cv2.resize(img, self.image_size)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            if self.transforms:\n                img = self.transforms(image = img)['image']\n            img = img\/255.0\n            images.append(img)\n        labels = [self.labels[k] - 1 for k in indices]\n        return np.array(images), np.array(labels)       \n    \n    # this function is called at the end of every epoch\n    def on_epoch_end(self):\n        self.indices = np.arange(len(self.paths))\n        if self.shuffle:\n            np.random.shuffle(self.indices)\n            \n# function to carry out image augmentation\ndef transforms():\n    return Compose([\n                    Rotate(limit=40),\n                    HorizontalFlip(p=0.5),\n                    RandomBrightness(limit=0.2,p=0.5),\n                    RandomContrast(limit=0.2, p=0.5),\n                    JpegCompression(quality_lower=85, quality_upper=100, p=0.5),\n                    HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.5),\n                    GaussianBlur(blur_limit=(3, 7), always_apply=False, p=0.5),\n                    Cutout(num_holes=8, max_h_size=8, max_w_size=8, fill_value=0, always_apply=False, p=0.5)\n                    ])\n    ","c2921eb9":"IMAGE_SIZE = (512,512)","6d99f209":"train_generator = DataGenerator(train_paths,\n                               train_labels,\n                               image_size = IMAGE_SIZE,\n                               batch_size = 4,\n                               num_classes = no_of_classes,\n                               shuffle = True,\n                               transforms = transforms())","7e6898ff":"valid_generator = DataGenerator(valid_paths,\n                               valid_labels,\n                               image_size = IMAGE_SIZE,\n                               batch_size = 1,\n                               num_classes = no_of_classes,\n                               shuffle = False)","aee92b45":"plt.figure(figsize = (16,16))\nfor row in range(4):\n    images, labels = train_generator[row]\n    for col in range(4):\n        plt.subplot(4,4,(row * 4 + col) + 1)\n        plt.imshow(images[col])\n        plt.title(labels[col])","554c5f8e":"plt.figure(figsize = (16,16))\nfor i in range(16):\n    image, label = valid_generator[i]\n    plt.subplot(4,4,i + 1)\n    plt.imshow(image[0])\n    plt.title(label[0])","bca2f3e9":"def create_model(input_shape):\n    # initialize the base model as VGG16 model with input shape as (512,512,3)\n    base_model = VGG16(input_shape = input_shape,\n                       include_top = False,\n                       weights = 'imagenet')\n\n    # we do not have to train all of the layers\n    for layer in base_model.layers:\n        layer.trainable = False\n        \n    x = layers.Flatten()(base_model.output)\n    x = layers.Dense(512, activation = 'relu')(x)\n    x = layers.Dropout(0.5)(x)\n    x = layers.Dense(no_of_classes, activation = 'softmax')(x)\n    \n    return models.Model(base_model.input,x)","43fc5e4e":"model = create_model((512,512,3))","b4f02fb3":"model.compile(loss = 'sparse_categorical_crossentropy',\n             optimizer = Adam(learning_rate=0.0001),\n             metrics = ['accuracy'])","0226e6db":"# Stop training when the validation loss metric has stopped decreasing for 5 epochs.\nearly_stopping = EarlyStopping(monitor = 'val_loss',\n                               patience = 5,\n                               mode = 'min',\n                               restore_best_weights = True)","38b61ce8":"# Save the model with the minimum validation loss\ncheckpoint = ModelCheckpoint('best_model.hdf5', \n                             monitor = 'val_loss',\n                             verbose = 1,\n                             mode = 'min', \n                             save_best_only = True)","afb29b1e":"EPOCHS = 50","ee4b9146":"history = model.fit(train_generator,\n                    validation_data = valid_generator,\n                    epochs = EPOCHS,\n                    steps_per_epoch = len(train_generator),\n                    validation_steps = len(valid_generator),\n                    callbacks = [early_stopping, checkpoint])","32d4ccfe":"model.summary()","d2a61138":"acc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'c-', label='Training accuracy')\nplt.plot(epochs, val_acc, 'y-', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\n\nplt.figure()\n\nplt.plot(epochs, loss, 'c-', label='Training Loss')\nplt.plot(epochs, val_loss, 'y-', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","9de8d7e7":"# load the best saved model as a new model\nnew_model = models.load_model('best_model.hdf5')\n\n# Check its architecture\nnew_model.summary()","37c61373":"# Evaluate the restored model\n\nactual_y = []\npred_y = []\n\nfor image, label in valid_generator:\n    pred_y.extend(new_model.predict(image).argmax(axis = 1))\n    actual_y.extend(label)","d62ecf90":"from sklearn.metrics import accuracy_score\nacc = accuracy_score(actual_y, pred_y)\nprint('Accuracy: {:5.2f}%'.format(100*acc))","0e555658":"Creating the train paths list to hold all the image paths to training images","480f9dcd":"Creating the valid labels list to hold all the corresponding classes of validation images","91ae8abf":"# Pre Trained Models in keras, tensorflow\n\nKeras has a wide range of pre trained models to choose from and implement in our code.\nTo read about all the pre trained models available in keras and their usage refer:\nhttps:\/\/keras.rstudio.com\/articles\/applications.html","ac7d557f":"Plotting a few images from the training set","724ee526":"Load the best trained model and check its summary","438b3645":"Adding some callback functions from the tensorflow library\n* EarlyStopping - https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/EarlyStopping\n* ModelCheckpoint - https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/callbacks\/ModelCheckpoint","8cf5a3ce":"Plotting a few images from the validation set","06b249cf":"# 3. Creating a custom data generator for our dataset.\nFor more information and a detailed explanation of the concept, do check out these amazing blog posts:\n* https:\/\/towardsdatascience.com\/implementing-custom-data-generators-in-keras-de56f013581c \n* https:\/\/stanford.edu\/~shervine\/blog\/keras-how-to-generate-data-on-the-fly","b51a72a4":"# 1. Read the dataset","c055af31":"Creating the train labels list to hold all the corresponding classes of training images","6a74625b":"Getting the number of classes of images from the metadata.csv file","8b6199ee":"# 4. Specifying the train and validation data generators","1a9c1230":"# 7. Evaluating the model","6029a309":"Plotting the training v\/s validation accuracy and training v\/s validation loss","fd4438e0":"Creating the valid paths list to hold all the image paths to validation images","cdc590c2":"An overview of the validation set","892db124":"# Importing all the necessary libraries ","4e0ca70e":"# 2. Splitting the dataset into training and validation sets\nWe can see that:\n* path - path to the image\n* class_id - class of the image\n* train-valid - indication of training and validation data\nWe use train-valid to split the dataset into training and validation set","f6456820":"# 5. Plotting images from the train and validation sets","494c8f53":"# 6. Creating and training a VGG16 model\n\nWe will be using the VGG16 model which is built for image classification. It was developed by Visual Graphics Group (VGG) at Oxford and described in the \"Very Deep Convolutional Networks for Large-Scale Image Recognition\" paper.\n\nSome useful links:\n* https:\/\/keras.io\/api\/applications\/vgg\/\n* https:\/\/keras.rstudio.com\/reference\/application_vgg.html\n* https:\/\/www.analyticsvidhya.com\/blog\/2020\/08\/top-4-pre-trained-models-for-image-classification-with-python-code\/","f4b370d0":"An overview of the training set","8535bcef":"Image augmentations are also performed by the data generator using the albumentations package.\n* https:\/\/albumentations.ai\/docs\/getting_started\/image_augmentation\/\n\nWe'll be using augmentations.transforms for performing augmentations like flip, rotation, blur, contrast, brightness, blur\n* https:\/\/albumentations.ai\/docs\/api_reference\/augmentations\/transforms\/"}}