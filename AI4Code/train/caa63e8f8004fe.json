{"cell_type":{"04fc46bc":"code","6b97474b":"code","e9293cfe":"code","b00d5544":"code","2f0f349a":"code","3ce4b618":"code","d29e6dab":"code","c7d1ab4d":"code","faf83f2d":"code","59a6a6c0":"code","fbe55a13":"code","502d72b8":"code","affe2ff5":"code","f7837aff":"code","bf7c588b":"code","71ddf961":"code","beefd31c":"code","451a1635":"code","516e24e4":"code","05ff76db":"code","ce4542e0":"code","c5e1e960":"code","d9f0ad26":"code","fc399e77":"code","055c7ea7":"code","24de3d21":"code","3c82a603":"code","59cb2b1b":"code","926a7f9c":"code","8c4adb8b":"code","acd5616b":"code","dd9b8613":"code","ae033ab4":"code","353136bc":"code","f17e1d62":"code","1cd30a1e":"code","11bbac01":"code","9ff99f84":"code","48d2a433":"code","07118e1f":"code","8660ec02":"code","4a1e4abc":"code","2832e055":"code","659efec9":"code","536f6926":"code","4748efe8":"code","082b1a40":"code","b03ac516":"code","a1ad6abe":"code","9017349e":"code","e53b220b":"code","27f1b7ed":"code","50d73e11":"code","83ea3959":"code","6d1d3434":"code","bf632102":"code","ae9c2c6e":"code","25e25f92":"code","d737f1de":"code","80550ede":"code","2e388af0":"code","89a22f19":"code","81b25449":"code","cbc2e677":"code","a5aea400":"code","cc40c83d":"code","58aee8bd":"code","4084ee07":"code","6177b586":"code","02d0fac1":"code","717ae990":"code","c6de70c4":"code","1809110e":"code","77bab959":"code","6223920a":"code","28d948e0":"code","b8714312":"code","52faaa93":"code","8e7e77de":"code","97aba9c8":"code","dc875dae":"code","95e1c145":"code","ae462420":"code","6adca4be":"code","1a342153":"code","709935c8":"code","a980fe44":"code","9e8cbfc4":"code","d80c8f99":"code","6eb4538b":"code","afa4c09d":"markdown","ca102bb3":"markdown","33377a02":"markdown","c132b679":"markdown","239ae3ff":"markdown","949f4f45":"markdown","f16daaa5":"markdown","e7cbc11a":"markdown","4d20dec5":"markdown","554cc624":"markdown","f3abd8d9":"markdown","ea604889":"markdown","62360673":"markdown","72be337f":"markdown","9bb8b35c":"markdown","2ceba10c":"markdown","abc0013a":"markdown","c72cd081":"markdown","960cd7c2":"markdown","091e1547":"markdown","861ab05c":"markdown","f140c75f":"markdown"},"source":{"04fc46bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6b97474b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn","e9293cfe":"# Import training and validation sets\ndf = pd.read_csv(\"..\/input\/bluebook-for-bulldozers\/TrainAndValid.csv\",\n                low_memory=False)","b00d5544":"df.info()","2f0f349a":"df.isna().sum()","3ce4b618":"df.columns","d29e6dab":"fig, ax = plt.subplots()\nax.scatter(df[\"saledate\"][:1000], df[\"SalePrice\"][:1000]);","c7d1ab4d":"df.saledate[:1000]","faf83f2d":"df.saledate.dtype","59a6a6c0":"df.SalePrice.plot.hist();","fbe55a13":"# Import data again but this time parse dates\ndf = pd.read_csv(\"..\/input\/bluebook-for-bulldozers\/TrainAndValid.csv\",\n                low_memory=False,\n                parse_dates=[\"saledate\"])","502d72b8":"df.saledate.dtype","affe2ff5":"df.saledate[:1000]","f7837aff":"fig, ax = plt.subplots()\nax.scatter(df[\"saledate\"][:1000], df[\"SalePrice\"][:1000]);","bf7c588b":"df.head()","71ddf961":"df.head().T","beefd31c":"df.saledate.head(20)","451a1635":"# Sort DataFrame in date order\ndf.sort_values(by=[\"saledate\"], inplace=True, ascending=True)\ndf.saledate.head(20)","516e24e4":"# Make a copy of the original DataFrame to perform edits on\ndf_tmp = df.copy()","05ff76db":"df_tmp[\"saleYear\"] = df_tmp.saledate.dt.year\ndf_tmp[\"saleMonth\"] = df_tmp.saledate.dt.month\ndf_tmp[\"saleDay\"] = df_tmp.saledate.dt.day\ndf_tmp[\"saleDayOfWeek\"] = df_tmp.saledate.dt.dayofweek\ndf_tmp[\"saleDayOfYear\"] = df_tmp.saledate.dt.dayofyear","ce4542e0":"df_tmp.head().T","c5e1e960":"# Now we've enriched our DataFrame with date time features, we can remove 'saledate'\ndf_tmp.drop(\"saledate\", axis=1, inplace=True)","d9f0ad26":"# Check the values of different columns\ndf_tmp.state.value_counts()","fc399e77":"df_tmp.head()","055c7ea7":"len(df_tmp)","24de3d21":"# Building a machine learning model\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_jobs=-1,\n                             random_state=12)\n\nmodel.fit(df_tmp.drop(\"SalePrice\", axis=1), df_tmp[\"SalePrice\"])","3c82a603":"df_tmp.info()","59cb2b1b":"df_tmp[\"Thumb\"].dtype","926a7f9c":"df_tmp.isna().sum()","8c4adb8b":"df_tmp.head().T","acd5616b":"pd.api.types.is_string_dtype(df_tmp[\"Thumb\"])","dd9b8613":"# Find the columns which contain string\nfor label, content in df_tmp.items():\n    if pd.api.types.is_string_dtype(content):\n        print(label)","ae033ab4":"# This will turn all of the string value into category values\nfor label, content in df_tmp.items():\n    if pd.api.types.is_string_dtype(content):\n        df_tmp[label] = content.astype(\"category\").cat.as_ordered()","353136bc":"df_tmp.info()","f17e1d62":"df_tmp.state.cat.categories","1cd30a1e":"df_tmp.state.cat.codes","11bbac01":"# Check missing data\ndf_tmp.isnull().sum()\/len(df_tmp)","9ff99f84":"# Export current tmp dataframe\ndf_tmp.to_csv(\"\/kaggle\/working\/train_tmp.csv\",\n             index=False)","48d2a433":"# Import preprocessed data\ndf_tmp = pd.read_csv(\"\/kaggle\/working\/train_tmp.csv\",\n                    low_memory=False)\ndf_tmp.head().T","07118e1f":"df_tmp.isna().sum()","8660ec02":"# Fill numerical missing values first\nfor label, content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        print(label)","4a1e4abc":"df_tmp.ModelID","2832e055":"# Check for which numeric columns have null values\nfor label, content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            # Add a binary column, tells if the data was missing or not\n            df_tmp[label +\"_is_missing\"] = pd.isnull(content)\n            # Fill missing numeric values with median\n            df_tmp[label] = content.fillna(content.median())","659efec9":"# Check if there's any null numeric values\nfor label, content in df_tmp.items():\n    if pd.api.types.is_numeric_dtype(content):\n        if pd.isnull(content).sum():\n            print(label)","536f6926":"# Check to see how many examples were missing\ndf_tmp.auctioneerID_is_missing.value_counts()","4748efe8":"df_tmp.isna().sum()","082b1a40":"# Check for columns which aren't numeric\nfor label, content in df_tmp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        print(label)","b03ac516":"# Turn categorical variables into numbers and fill missing\nfor label, content in df_tmp.items():\n    if not pd.api.types.is_numeric_dtype(content):\n        # Add binary column to indicate whether sample had missing value\n        df_tmp[label+\"_is_missing\"] = pd.isnull(content)\n        # Turn categories into numbers and add +1\n        df_tmp[label] = pd.Categorical(content).codes+1","a1ad6abe":"pd.Categorical(df_tmp[\"state\"]).codes+1","9017349e":"df_tmp.info()","e53b220b":"df_tmp.head().T","27f1b7ed":"df_tmp.isna().sum()","50d73e11":"df_tmp.head()","83ea3959":"len(df_tmp)","6d1d3434":"%%time\n# Instantiate model\nmodel = RandomForestRegressor(n_jobs=-1,\n                             random_state=12)\n\n# Fit the model\nmodel.fit(df_tmp.drop(\"SalePrice\", axis=1), df_tmp[\"SalePrice\"])","bf632102":"# Score the model\nmodel.score(df_tmp.drop(\"SalePrice\", axis=1), df_tmp[\"SalePrice\"])","ae9c2c6e":"df_tmp.saleYear","25e25f92":"df_tmp.saleYear.value_counts()","d737f1de":"# Split data into training and validation\ndf_val = df_tmp[df_tmp.saleYear == 2012]\ndf_train = df_tmp[df_tmp.saleYear != 2012]\n\nlen(df_val), len(df_train)","80550ede":"# Split data into X & y\nX_train, y_train = df_train.drop(\"SalePrice\", axis=1), df_train.SalePrice\nX_valid, y_valid = df_val.drop(\"SalePrice\", axis=1), df_val.SalePrice\n\nX_train.shape, y_train.shape, X_valid.shape, y_valid.shape","2e388af0":"# Create evaluation  function (the competition uses RMSLE)\nfrom sklearn.metrics import mean_squared_log_error, mean_absolute_error, r2_score\n\ndef rmsle(y_test, y_preds):\n    \"\"\"\n    Calculates root mean squared error between predictions and truelabels.\n    \"\"\"\n    return np.sqrt(mean_squared_log_error(y_test, y_preds))\n\n# Create function to evaluate model on a few different levels\ndef show_scores(model):\n    train_preds = model.predict(X_train)\n    val_preds = model.predict(X_valid)\n    scores = {\"Training MAE\": mean_absolute_error(y_train, train_preds),\n             \"Valid MAE\": mean_absolute_error(y_valid, val_preds),\n             \"Training RMSLE\": rmsle(y_train, train_preds),\n             \"Valid RMSLE\": rmsle(y_valid, val_preds),\n             \"Training R^2\": r2_score(y_train, train_preds),\n             \"Valid R^2\": r2_score(y_valid, val_preds)}\n    return scores","89a22f19":"# It's takes too long for experiment\n#%%time\n#model = RandomForestRegressor(n_jobs=-1,\n #                            random_state=12)\n\n#model.fit(X_train, y_train)","81b25449":"len(X_train), len(y_train)","cbc2e677":"# Change max_samples value\nmodel = RandomForestRegressor(n_jobs=-1,\n                             random_state=12,\n                             max_samples=10000)","a5aea400":"%%time\n# Cutting down on the max number of samples each estimator can see improves training time\nmodel.fit(X_train, y_train)","cc40c83d":"(X_train.shape[0] * 100 \/ 1000000)","58aee8bd":"10000 * 100","4084ee07":"show_scores(model)","6177b586":"%%time\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Different RandomForestRegressor hyperparameters\nrf_grid = {\"n_estimators\": np.arange(10, 100, 10),\n          \"max_depth\": [None, 3, 5, 10],\n          \"min_samples_split\": np.arange(2, 20, 2),\n          \"min_samples_leaf\": np.arange(1, 20, 2),\n          \"max_features\": [0,5, 1, \"sqrt\", \"auto\"],\n          \"max_samples\": [10000]}\n\n# Instantiate RandomizedSearchCV\nrs_model = RandomizedSearchCV(RandomForestRegressor(n_jobs=-1,\n                                                   random_state=12),\n                             param_distributions=rf_grid,\n                             n_iter=2,\n                             cv=5,\n                             verbose=True)\n\n# Fit the RandomizedSearchCV\nrs_model.fit(X_train, y_train)","02d0fac1":"# Find the best model hyperparameters\nrs_model.best_params_","717ae990":"# Evaluate the RandomizedSearch model\nshow_scores(rs_model)","c6de70c4":"%%time\n\n# Most ideal hyperparameters\nideal_model = RandomForestRegressor(n_estimators=40,\n                                   min_samples_leaf=1,\n                                   min_samples_split=14,\n                                   max_features=0.5,\n                                   n_jobs=-1,\n                                   max_samples=None,\n                                   random_state=12)\n\n# Fit the ideal model\nideal_model.fit(X_train, y_train)","1809110e":"# Scores for ideal_model (trained on all the data)\nshow_scores(ideal_model)","77bab959":"# Scores on rs_model (only trained on ~10,000 examples)\nshow_scores(rs_model)","6223920a":"# Import the test data\ndf_test = pd.read_csv(\"..\/input\/bluebook-for-bulldozers\/Test.csv\",\n                     low_memory=False,\n                     parse_dates=[\"saledate\"])\n\ndf_test.head()","28d948e0":"# Make predictions on the test dataset\ntest_preds = ideal_model.predict(df_test)","b8714312":"def preprocess_data(df):\n    \"\"\"\n    Performs transformations on df and returns transformed df.\n    \"\"\"\n    df[\"saleYear\"] = df.saledate.dt.year\n    df[\"saleMonth\"] = df.saledate.dt.month\n    df[\"saleDay\"] = df.saledate.dt.day\n    df[\"saleDayOfWeek\"] = df.saledate.dt.dayofweek\n    df[\"saleDayOfYear\"] = df.saledate.dt.dayofyear\n    \n    df.drop(\"saledate\", axis=1, inplace=True)\n    \n    # Fill the numeric rows with median\n    for label, content in df.items():\n        if pd.api.types.is_numeric_dtype(content):\n            if pd.isnull(content).sum(): \n                # Add a binary column which tells us if the data was missing or not\n                df[label+\"_is_missing\"] = pd.isnull(content)\n                # Fill missing numeric values with median\n                df[label] = content.fillna(content.median())\n    \n        # Filled categorical missing data and turn categories into numbers\n        if not pd.api.types.is_numeric_dtype(content):\n            df[label+\"_is_missing\"] = pd.isnull(content)\n            # We add +1 to the category code because pandas encodes missing categories as -1\n            df[label] = pd.Categorical(content).codes+1\n    \n    return df ","52faaa93":"# Process the test data\ndf_test = preprocess_data(df_test)\ndf_test.head()","8e7e77de":"# Make predictions on updated test data\ntest_preds = ideal_model.predict(df_test)","97aba9c8":"X_train.head()","dc875dae":"# We can find how the columns differ using sets\nset(X_train.columns) - set(df_test.columns)","95e1c145":"# Manually adjust df_test to have auctioneedID_is_missing column\ndf_test[\"auctioneedID_is_missing\"] = False\ndf_test.head()","ae462420":"# Make predictions on the test data\ntest_preds = ideal_model.predict(df_test)","6adca4be":"test_preds","1a342153":"df_preds = pd.DataFrame()\ndf_preds[\"SalesID\"] = df_test[\"SalesID\"]\ndf_preds[\"SalesPrice\"] = test_preds\ndf_preds","709935c8":"# Export prediction data\ndf_preds.to_csv(\"\/kaggle\/working\/test_predictions.csv\", index=False)","a980fe44":"# Find feature importance of our best model\nideal_model.feature_importances_","9e8cbfc4":"# Helper function for plotting importance\ndef plot_features(columns, importances, n=20):\n    df = (pd.DataFrame({\"features\": columns,\n                       \"feature_importances\": importances,})\n         .sort_values(\"feature_importances\", ascending=False)\n         .reset_index(drop=True))\n    \n    # Plot the dataframe\n    fig, ax = plt.subplots()\n    ax.barh(df[\"features\"][:n], df[\"feature_importances\"][:20])\n    ax.set_ylabel(\"Features\")\n    ax.set_xlabel(\"Feature importance\")\n    ax.invert_yaxis()","d80c8f99":"plot_features(X_train.columns, ideal_model.feature_importances_)","6eb4538b":"df[\"Enclosure\"].value_counts()","afa4c09d":"# Hyperparameter tunning with RandomizedSearchCV","ca102bb3":"# Fill missing values","33377a02":"# Sort DataFrame by saledate\nWhen working with time series data, it's a good idea to sort it by date.","c132b679":"# Add datetime parameters for `saledate` column","239ae3ff":"## 1. Problem definition \n> How well can we predict the future sale price of a bulldozer, given its characteristics and previous examples of how much similiar bulldozers have been sold for?","949f4f45":"# Building an evaluation function","f16daaa5":"Now that all of data is numeric as well as our dataframe has no missing values, we should be able to build a machine learning model.","e7cbc11a":"## 2. Data\n\nThe data is downloaded from the Kaggle Bluebook for Bulldozers competition: https:\/\/www.kaggle.com\/c\/bluebook-for-bulldozers\/data\n\nThere are 3 main datasets:\n\n* Train.csv is the training set, which contains data through the end of 2011.\n* Valid.csv is the validation set, which contains data from January 1, 2012 - April 30, 2012.\n* Test.csv is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012.","4d20dec5":"# Parsing dates\nWhen we work with time series data, we want to enrich the time & date component as much as possible.\n\nWe can do that by telling pandas which of our columns has dates in it using the `parse_dates` parameter.","554cc624":"# Spliting data into train\/validation sets","f3abd8d9":"# Make predictions on test data","ea604889":"# Convert string to categories\nOne way we can turn all our data into numbers is by converting them into pandas categories.","62360673":"## 3. Evaluation\n\nThe evaluation metric for this competition is the RMSLE (root mean squared log error) between the actual and predicted auction prices.\n\nFor more on the evaluation of this project check: https:\/\/www.kaggle.com\/c\/bluebook-for-bulldozers\/overview\/evaluation\n\n**Note:** The goal for most regression evaluation metrics is to minimize the error.","72be337f":"Predictions are not in the same format Kaggle is asking for","9bb8b35c":"### Train a model with the best hyperparameters\n**Note:** These were found after 100 iterations of `RandomizedSearchCV`.","2ceba10c":"Finally now our test dataframe has the same features as our training dataframe, we can make predictions!","abc0013a":"# Filling and turning categorical variables into numbers","c72cd081":"# Save preprocessed data","960cd7c2":"## 4. Features\n\nKaggle provides a data dictionary detailing all of the features of the dataset. You can view this data dictionary on Google Sheets: https:\/\/docs.google.com\/spreadsheets\/d\/18ly-bLR8sbDJLITkWG7ozKm8l3RyieQ2Fpgix-beSYI\/edit?usp=sharing","091e1547":"# Preprocessing the data (getting the test dataset in the same format as our training dataset)","861ab05c":"# Featute Importance\nFeature importance seeks to figure out which different attributes of the data were most importance when it comes to predicting the **target variable** (SalePrice).","f140c75f":"## 5.Modelling\nLet's do some model-driven EDA."}}