{"cell_type":{"a1560119":"code","68da3028":"code","0696f7b8":"code","b388311b":"code","7cd122c7":"code","249ee447":"code","4f30afaa":"code","6325ed03":"code","6fd17feb":"code","e49244b3":"code","9ec1ec24":"code","9e0182ae":"code","8a6995c6":"markdown","8e591b7b":"markdown","093f8c63":"markdown","5015e9a0":"markdown","6db236d7":"markdown","e6b5ee04":"markdown","9bd4da17":"markdown","b0aa9790":"markdown","14452866":"markdown"},"source":{"a1560119":"import os\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nimport tensorflow_addons as tfa\n\nfrom sklearn.metrics import *\nimport scikitplot as skplt\n\nfrom functools import partial\nimport albumentations as A\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nimg_dir = '..\/input\/covidnet\/processed\/'\ntrain_df = pd.read_csv('..\/input\/covidnet\/train.csv')\nvalid_df = pd.read_csv('..\/input\/covidnet\/valid.csv')\ntests_df = pd.read_csv('..\/input\/covidnet\/test.csv')\nclass_names = ['normal', 'pneumonia', 'covid']\ntrain_df.head(3)","68da3028":"train_df['set'] = 'train'\nvalid_df['set'] = 'valid'\ntests_df['set'] = 'tests'\ndata = pd.concat([train_df,valid_df, tests_df])\nax = sns.displot(data=data, x='label', col='set')","0696f7b8":"print(tests_df.label.value_counts())\ntests_df","b388311b":"# upsampling dataset\nmax_count = np.max(train_df.label.value_counts())\nmin_count = np.min(train_df.label.value_counts())\ntrain_df = train_df.groupby('label').sample(n=max_count, replace=True)\ntrain_df = train_df.reset_index(drop=True)\ntrain_df.label.value_counts()\n\ndata = pd.concat([train_df,valid_df, tests_df])\nax = sns.displot(data=data, x='label', col='set')","7cd122c7":"model_handle_map = {\n  \"efficientnetv2-s\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_s\/feature_vector\/1\",\n  \"efficientnetv2-m\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_m\/feature_vector\/1\",\n  \"efficientnetv2-l\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_l\/feature_vector\/1\",\n  \"efficientnetv2-s-21k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_s\/feature_vector\/1\",\n  \"efficientnetv2-m-21k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_m\/feature_vector\/1\",\n  \"efficientnetv2-l-21k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_l\/feature_vector\/1\",\n  \"efficientnetv2-s-21k-ft1k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_s\/feature_vector\/1\",\n  \"efficientnetv2-m-21k-ft1k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_m\/feature_vector\/1\",\n  \"efficientnetv2-l-21k-ft1k\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet21k_ft1k_l\/feature_vector\/1\",\n  \"efficientnetv2-b0\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_b0\/feature_vector\/2\",\n  \"efficientnetv2-b1\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_b1\/feature_vector\/1\",\n  \"efficientnetv2-b2\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_b2\/feature_vector\/1\",\n  \"efficientnetv2-b3\": \"https:\/\/tfhub.dev\/google\/imagenet\/efficientnet_v2_imagenet1k_b3\/feature_vector\/1\",\n  \"efficientnet_b0\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b0\/feature-vector\/1\",\n  \"efficientnet_b1\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b1\/feature-vector\/1\",\n  \"efficientnet_b2\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b2\/feature-vector\/1\",\n  \"efficientnet_b3\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b3\/feature-vector\/1\",\n  \"efficientnet_b4\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b4\/feature-vector\/1\",\n  \"efficientnet_b5\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b5\/feature-vector\/1\",\n  \"efficientnet_b6\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b6\/feature-vector\/1\",\n  \"efficientnet_b7\": \"https:\/\/tfhub.dev\/tensorflow\/efficientnet\/b7\/feature-vector\/1\",\n  \"bit_s-r50x1\": \"https:\/\/tfhub.dev\/google\/bit\/s-r50x1\/1\",\n  \"inception_v3\": \"https:\/\/tfhub.dev\/google\/imagenet\/inception_v3\/feature_vector\/4\",\n  \"inception_resnet_v2\": \"https:\/\/tfhub.dev\/google\/imagenet\/inception_resnet_v2\/feature-vector\/4\",\n  \"resnet_v1_50\": \"https:\/\/tfhub.dev\/google\/imagenet\/resnet_v1_50\/feature-vector\/4\",\n  \"resnet_v1_101\": \"https:\/\/tfhub.dev\/google\/imagenet\/resnet_v1_101\/feature-vector\/4\",\n  \"resnet_v1_152\": \"https:\/\/tfhub.dev\/google\/imagenet\/resnet_v1_152\/feature-vector\/4\",\n  \"resnet_v2_50\": \"https:\/\/tfhub.dev\/google\/imagenet\/resnet_v2_50\/feature_vector\/5\",\n  \"resnet_v2_101\": \"https:\/\/tfhub.dev\/google\/imagenet\/resnet_v2_101\/feature_vector\/5\",\n  \"resnet_v2_152\": \"https:\/\/tfhub.dev\/google\/imagenet\/resnet_v2_152\/feature-vector\/4\",\n  \"nasnet_large\": \"https:\/\/tfhub.dev\/google\/imagenet\/nasnet_large\/feature_vector\/4\",\n  \"nasnet_mobile\": \"https:\/\/tfhub.dev\/google\/imagenet\/nasnet_mobile\/feature_vector\/4\",\n  \"pnasnet_large\": \"https:\/\/tfhub.dev\/google\/imagenet\/pnasnet_large\/feature_vector\/4\",\n  \"mobilenet_v2_100_224\": \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v2_100_224\/feature_vector\/4\",\n  \"mobilenet_v2_130_224\": \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v2_130_224\/feature_vector\/4\",\n  \"mobilenet_v2_140_224\": \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v2_140_224\/feature_vector\/4\",\n  \"mobilenet_v3_small_100_224\": \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v3_small_100_224\/feature_vector\/5\",\n  \"mobilenet_v3_small_075_224\": \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v3_small_075_224\/feature_vector\/5\",\n  \"mobilenet_v3_large_100_224\": \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v3_large_100_224\/feature_vector\/5\",\n  \"mobilenet_v3_large_075_224\": \"https:\/\/tfhub.dev\/google\/imagenet\/mobilenet_v3_large_075_224\/feature_vector\/5\",\n}\n\nmodel_image_size_map = {\n  \"efficientnetv2-s\": 384,\n  \"efficientnetv2-m\": 480,\n  \"efficientnetv2-l\": 480,\n  \"efficientnetv2-s-21k\": 384,\n  \"efficientnetv2-m-21k\": 480,\n  \"efficientnetv2-l-21k\": 480,\n  \"efficientnetv2-s-21k-ft1k\": 384,\n  \"efficientnetv2-m-21k-ft1k\": 480,\n  \"efficientnetv2-l-21k-ft1k\": 480,\n  \"efficientnetv2-b0\": 224,\n  \"efficientnetv2-b1\": 240,\n  \"efficientnetv2-b2\": 260,\n  \"efficientnetv2-b3\": 300,\n  \"efficientnet_b0\": 224,\n  \"efficientnet_b1\": 240,\n  \"efficientnet_b2\": 260,\n  \"efficientnet_b3\": 300,\n  \"efficientnet_b4\": 380,\n  \"efficientnet_b5\": 456,\n  \"efficientnet_b6\": 528,\n  \"efficientnet_b7\": 600,\n  \"inception_v3\": 299,\n  \"inception_resnet_v2\": 299,\n  \"nasnet_large\": 331,\n  \"pnasnet_large\": 331,\n}","249ee447":"model_name = 'efficientnetv2-b0'\nmodel_handle = model_handle_map.get(model_name)\nIMAGE_SIZE = model_image_size_map.get(model_name, 224)\nBATCH_SIZE = 16\nEPOCHS = 12\nSAMPLE_SIZE = len(train_df)\n\nprint(f\"Selected model: {model_name} : {model_handle}\")\nprint(f\"Input size {IMAGE_SIZE}\")","4f30afaa":"def parse_image(path, label):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_png(img, channels=3)\n    return img, label\n\ndef resize_rescale(image, label):\n    img = tf.cast(image, tf.float32)\n    img = tf.image.resize(img, [IMAGE_SIZE, IMAGE_SIZE])\/255\n    return img, label\n\ndef aug_fn(image): \n    transforms = A.Compose([\n        A.HorizontalFlip(p=0.5),\n        A.Rotate(p=0.5, limit=15),\n        A.RandomBrightnessContrast(p=0.5, brightness_limit=(-0.2, 0.2), contrast_limit=(-0.1, 0.1), brightness_by_max=True),\n        A.RandomResizedCrop(p=0.8, height=IMAGE_SIZE, width=IMAGE_SIZE, scale=(0.9, 1.1), ratio=(0.05, 1.1), interpolation=0),\n        A.Blur(p=0.3, blur_limit=(1, 1)),\n    ])\n    data = {\"image\":image}\n    aug_data = transforms(**data)\n    aug_img = aug_data[\"image\"]\n    aug_img = tf.cast(aug_img, tf.float32)\n    aug_img = tf.image.resize(aug_img, [IMAGE_SIZE, IMAGE_SIZE])\/255\n    return aug_img\n\ndef augmentor(image, label):\n    aug_img = tf.numpy_function(func=aug_fn, inp=[image], Tout=tf.float32)\n    return aug_img, label\n\ndef view_image(ds, col=8, row=2, size=(25,7)):\n    plt.figure(figsize=size)\n    plt.subplots_adjust(wspace=0.05, hspace=0.15)\n    for images, labels in ds.take(1):\n        for i in range(col*row):\n            ax = plt.subplot(row, col, i + 1)\n            shape = str(images[i].numpy().shape)\n            plt.imshow(images[i].numpy())\n            plt.title(class_names[np.argmax(labels[i].numpy())])\n            plt.axis(\"off\") \n    plt.tight_layout\n    return None\n\ndef training_history(history):\n    accuracy = history['accuracy']\n    val_accuracy = history['val_accuracy']\n\n    loss = history['loss']\n    val_loss = history['val_loss']\n\n    epochs_range = range(len(history['loss']))\n\n    plt.figure(figsize=(16, 4))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs_range, accuracy, label='Training accuracy')\n    plt.plot(epochs_range, val_accuracy, label='Validation accuracy')\n    plt.legend(loc='lower right')\n    plt.title('Training and Validation Loss')\n\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs_range, loss, label='Training Loss')\n    plt.plot(epochs_range, val_loss, label='Validation Loss')\n    plt.legend(loc='upper right')\n    plt.title('Training and Validation Loss')\n\n    plt.show()\n    return None\n\ndef decode_test(path):\n    img = tf.io.read_file(path)\n    img = tf.image.decode_png(img, channels=3)\n    img = tf.cast(img, tf.float32)\n    img = tf.image.resize(img, [224, 224])\/255\n    return img\n\ndef build_network(num_classes, image_size):\n    print('building model...')\n    model = tf.keras.Sequential([\n        layers.InputLayer(input_shape=(image_size, image_size, 3)),\n        hub.KerasLayer(model_handle, trainable=True, name='base_model'),\n        layers.Dense(512, activation='swish'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.5),\n        layers.Dense(128, activation='swish'),\n        layers.BatchNormalization(),\n        layers.Dropout(0.2),\n        layers.Dense(64, activation='swish'),\n        layers.BatchNormalization(),\n        layers.Dense(num_classes, activation='softmax', name='classifier') \n    ],name=model_name)\n    model.build((None, image_size, image_size, 3))\n    model.summary()\n    print('model loaded!!!')\n    return model","6325ed03":"train_loader = tf.data.Dataset.from_tensor_slices((img_dir+train_df.filename, train_df[class_names]))\nvalid_loader = tf.data.Dataset.from_tensor_slices((img_dir+valid_df.filename, valid_df[class_names]))\n\ntrain_ds = (\n    train_loader.shuffle(len(train_df))\n    .map(parse_image, num_parallel_calls=AUTOTUNE)\n    .map(partial(augmentor),num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTOTUNE) \n)\nvalid_ds = (\n    valid_loader.shuffle(len(valid_df))\n    .map(parse_image, num_parallel_calls=AUTOTUNE)\n    .map(resize_rescale, num_parallel_calls=AUTOTUNE)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTOTUNE)\n)","6fd17feb":"view_image(train_ds)","e49244b3":"tf.keras.backend.clear_session()\nmodel = build_network(len(class_names), IMAGE_SIZE)\n\nclr_scheduler = tfa.optimizers.CyclicalLearningRate( \n    initial_learning_rate=3e-7,  maximal_learning_rate=7e-3, \n    step_size=3*(SAMPLE_SIZE\/\/BATCH_SIZE),  \n    scale_fn=lambda x: 1 \/ (2.0 ** (x - 1)), \n    scale_mode='cycle'\n)\nMETRICS = [\n    'accuracy',\n    tf.keras.metrics.Precision(name='precision'),\n    tf.keras.metrics.Recall(name='recall'),\n]\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.SGD(learning_rate=clr_scheduler) , \n    loss=tf.keras.losses.CategoricalCrossentropy(), \n    metrics=METRICS\n)\ncheckpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"best_model.h5\", save_best_only=True)\nhistory = model.fit(\n    train_ds, \n    epochs=EPOCHS,\n    batch_size=BATCH_SIZE,\n    callbacks = [checkpoint_cb],\n    verbose=1,\n    validation_data=valid_ds,\n)","9ec1ec24":"training_history(history.history)","9e0182ae":"test_ds = tf.data.Dataset.from_tensor_slices('..\/input\/covidnet\/processed\/'+tests_df.filename) \ntest_ds = test_ds.map(decode_test,num_parallel_calls=AUTOTUNE).batch(len(tests_df))\ntest_img = next(iter(test_ds))\ntest_index = np.argmax(tests_df[class_names].values, axis=1)\ntest_label = tests_df.label.values\n\ntest_pred = model.predict(test_ds)\npred_index = np.argmax(test_pred, axis=1)\npred_label = np.array(class_names)[pred_index]\n\nprint(classification_report(test_index, pred_index, target_names=class_names,zero_division=0))\nprint('f1_score        :', f1_score(test_index, pred_index, average='micro'))\nprint('accuracy_score  :', accuracy_score(test_index, pred_index))\n\ncm = skplt.metrics.plot_confusion_matrix(test_label, pred_label, figsize=(8, 8), normalize=False)\nroc = skplt.metrics.plot_roc(test_index, test_pred, figsize=(10,8))","8a6995c6":"### Load Library","8e591b7b":"## A Lightweight yet Efficient Deep Neural Network for COVID-19 Diagnosis Using CXR\n\n\n\nMalaysia, like many other countries in South-east Asia, is battling a surge in Covid-19 infections, driven in part by new highly infectious and deadlier variants. As of July 26, 2021, There have been 1,078,646 infections and 8,725 coronavirus-related deaths reported since the pandemic began. On a weekly average, new infections had exceeded 16,000 cases. As a result, a more aggressive strategy is implemented, which includes tightening lockdowns in high-risk areas and increasing targeted testing.\n\nCOVID-19 affects individuals differently. The majority of infected individuals will have mild to moderate sickness and will recover without the need for hospitalisation. A person with COVID-19 may develop a productive cough, fever, muscle pains, fatigue, and shortness of breath over the course of the disease. The virus can spread through the respiratory tract and into the lungs of an infected individual, leading to a variety of complications, including pneumonia.\n\nHowever, Like other pneumonias, COVID-19 pneumonia causes the density of the lungs to increase and look very similar on CXR imaging. For a radiologist, it is not easy to distinguish COVID-19 to other viral pneumonia. Furthermore, due to the high infectious rate of COVID-19 virus, the number of patients who require an CXR examination is increasing compared with a fewer number of available radiologists. This can keep the radiologists and the hospital overloaded, delay diagnosis and lead to cross-infection. Hence, It is critical to establish a rapid and automated interpretation of CXR images for conducting pneumonia diagnosis and differentiating COVID-19 from other types of viral pneumonia.\n\n---\n\n#### **Datasets**\nA total of 17,366 CXR pictures were collected from five (5) repositories. For training and validation, only 17,066 CXRs were used. The remaining 300 CXRs were deleted from the dataset to assess the model's performance.\n<br>Thanks to https:\/\/github.com\/lindawangg\/COVID-Net for the effort.\n\nThe performance of the models will be tested on predicting one of the following three possible labels: \n - **normal** - for healthy patients\n - **pneumonia** - for patients with other viral pneumonia \n - **covid** - for patients with COVID-19 pneumonia\n","093f8c63":"### Prepare dataset","5015e9a0":"### Define Prameter","6db236d7":"### Load Dataset","e6b5ee04":"### Visualize training history","9bd4da17":"### Visualize dataset","b0aa9790":"### Training!!!","14452866":"### Evaluate model performance with test dataset"}}