{"cell_type":{"3f4811f2":"code","324c93ed":"code","bdfb8a94":"code","17e32f20":"code","7bc5618a":"code","a09c0a13":"code","e62b456e":"code","4c9f2cd0":"code","6d758a57":"code","8bc5611f":"code","6c875868":"code","58ebdffe":"code","9b772342":"code","323262e8":"code","d75bcdcc":"code","85a60b1b":"code","44d9131e":"code","0f1a1518":"markdown","0184398f":"markdown","b31e8568":"markdown","998f0e26":"markdown","94d2530d":"markdown","797c71c3":"markdown","35306afb":"markdown","35c4aa27":"markdown","36566ee9":"markdown","82908b8b":"markdown","30f3cca2":"markdown","26999e1c":"markdown","a6e35071":"markdown"},"source":{"3f4811f2":"import os\nfrom os import listdir, makedirs\nfrom os.path import join, exists, expanduser\n\nfrom keras import applications\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import optimizers\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense\nfrom keras import backend as K\nimport tensorflow as tf\n# Any results you write to the current directory are saved as output.","324c93ed":"from subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\/dataset\/dataset\/\"]).decode(\"utf8\"))\n","bdfb8a94":"# dimensions of our images.\nimg_width, img_height = 224, 224 # we set the img_width and img_height according to the pretrained models we are\n# going to use. The input size for ResNet-50 is 224 by 224 by 3.\n\n#train_data_dir = '..\/input\/fruits\/fruits-360_dataset_2018_06_03\/fruits-360\/Training\/'\ndata_dir = '..\/input\/dataset\/dataset\/'\n#validation_data_dir = '..\/input\/fruits\/fruits-360_dataset_2018_06_03\/fruits-360\/Validation\/'\n#nb_train_samples = 30000\n#nb_validation_samples = 10000\nbatch_size = 16","17e32f20":"# Will generate augmented images : image augmentation is curcial part of training deep learning networks\n\ntrain_datagen = ImageDataGenerator(\n    #featurewise_center=True, \n    samplewise_center=False,\n    #featurewise_std_normalization=True,\n    samplewise_std_normalization=False,\n    zca_whitening=False,\n    #zca_epsilon=1e-06,\n    rotation_range=30,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    brightness_range=[0.05 , 0.1],\n    shear_range=0.2,\n    zoom_range=0.2,\n    channel_shift_range=0.1,\n    fill_mode='nearest',\n    cval=0.0,\n    horizontal_flip=True,\n    validation_split=0.3)\n\n\n#test_datagen = ImageDataGenerator(rescale=1. \/ 255)\n\ntrain_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset = 'training',\n    interpolation = 'nearest',\n    #cval = 0,\n)\n\nvalidation_generator = train_datagen.flow_from_directory(\n    data_dir,\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset = 'validation',\n   interpolation = 'nearest',\n    #cval = 0,\n)","7bc5618a":"import pandas as pd\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\ninit_notebook_mode(connected=True)","a09c0a13":"training_data = pd.DataFrame(train_generator.classes, columns=['classes'])\ntesting_data = pd.DataFrame(validation_generator.classes, columns=['classes'])","e62b456e":"def create_stack_bar_data(col, df):\n    aggregated = df[col].value_counts().sort_index()\n    x_values = aggregated.index.tolist()\n    y_values = aggregated.values.tolist()\n    return x_values, y_values","4c9f2cd0":"x1, y1 = create_stack_bar_data('classes', training_data)\nx1 = list(train_generator.class_indices.keys())\n\ntrace1 = go.Bar(x=x1, y=y1, opacity=0.75, name=\"Class Count\")\nlayout = dict(height=400, width=1200, title='Class Distribution in Training Data', legend=dict(orientation=\"h\"), \n                yaxis = dict(title = 'Class Count'))\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);","6d758a57":"x1, y1 = create_stack_bar_data('classes', testing_data)\nx1 = list(validation_generator.class_indices.keys())\n\ntrace1 = go.Bar(x=x1, y=y1, opacity=0.75, name=\"Class Count\")\nlayout = dict(height=400, width=1100, title='Class Distribution in Validation Data', legend=dict(orientation=\"h\"), \n                yaxis = dict(title = 'Class Count'))\nfig = go.Figure(data=[trace1], layout=layout);\niplot(fig);","8bc5611f":"### \n\n\n\n'''#import inception with pre-trained weights. do not include fully #connected layers\ninception_base = applications.InceptionResNetV2(weights='imagenet', include_top=False)\n\n# add a global spatial average pooling layer\nx = inception_base.output\nx = GlobalAveragePooling2D()(x)\n# add a fully-connected layer\nx = Dense(512, activation='relu')(x)\n# and a fully connected output\/classification layer\npredictions = Dense(13, activation='softmax')(x)\n# create the full network so we can train on it\ninception_transfer = Model(inputs=inception_base.input, outputs=predictions)'''","6c875868":"#import inception with pre-trained weights. do not include fully #connected layers\ninception_base_vanilla = applications.DenseNet201(weights='imagenet', include_top=False)\n\n# add a global spatial average pooling layer\nx = inception_base_vanilla.output\nx = GlobalAveragePooling2D()(x)\n# add a fully-connected layer\nx = Dense(2048, activation='relu')(x)\n#x = Dense(128, activation='relu')(x)\n# and a fully connected output\/classification layer\npredictions = Dense(13, activation='softmax')(x)\n# create the full network so we can train on it\ninception_transfer_vanilla = Model(inputs=inception_base_vanilla.input, outputs=predictions)","58ebdffe":"'''inception_transfer.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.Adam(lr=1e-4),\n              metrics=['accuracy'])\n'''\ninception_transfer_vanilla.compile(loss='categorical_crossentropy',\n              optimizer=optimizers.SGD(lr=1e-4),\n              metrics=['accuracy'])","9b772342":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","323262e8":"import keras\nbatch_size = 16\n\nnb_train_samples = 15000\nnb_validation_samples = 5960\nimport numpy as np\nsteps_per_epoch = np.ceil(nb_train_samples \/ batch_size)\nval_steps = np.ceil(nb_validation_samples \/ batch_size)\n\npld = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=1, mode='auto', min_lr= 1e-8)\n","d75bcdcc":"import tensorflow as tf\nwith tf.device(\"\/device:GPU:0\"):\n    history_vanilla = inception_transfer_vanilla.fit_generator(train_generator,\n    epochs=50, shuffle = True, verbose = 1, validation_data = validation_generator,\n    validation_steps = val_steps , steps_per_epoch = steps_per_epoch , use_multiprocessing = True)","85a60b1b":"'''with tf.device(\"\/device:GPU:0\"):\n    history_vanilla = inception_transfer_vanilla.fit_generator(train_generator,\n    epochs=12, shuffle = True, verbose = 1, validation_data = validation_generator,\n    validation_steps = val_steps , steps_per_epoch = steps_per_epoch , use_multiprocessing = True)'''","44d9131e":"import matplotlib.pyplot as plt\n# summarize history for accuracy\n#plt.plot(history_pretrained.history['val_acc'])\nplt.plot(history_vanilla.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['Pretrained'], loc='upper left')\nplt.show()\n# summarize history for loss\n#plt.plot(history_pretrained.history['val_loss'])\nplt.plot(history_vanilla.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['Pretrained'], loc='upper left')\nplt.show()","0f1a1518":"## 1. Transfer Learning\n\nIn transfer learning, we first train a base network on a base dataset and task, and then we repurpose the learned features, or transfer them, to a second target network to be trained on a target dataset and task. This process will tend to work if the features are general, meaning suitable to both base and target tasks, instead of specific to the base task.\n\nLisa Torrey and Jude Shavlik in their chapter on transfer learning describe three possible benefits to look for when using transfer learning:\n\n* Higher start. The initial skill (before refining the model) on the source model is higher than it otherwise would be.\n* Higher slope. The rate of improvement of skill during training of the source model is steeper than it otherwise would be.\n* Higher asymptote. The converged skill of the trained model is better than it otherwise would be.\n\n<center><img src=\"https:\/\/3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com\/wp-content\/uploads\/2017\/09\/Three-ways-in-which-transfer-might-improve-learning.png\"><\/center>\n\n\nBasically, we take a pre-trained model (the weights and parameters of a network that has been trained on a large dataset by somebody else) and \u201cfine-tune\u201d the model with our own dataset. The idea is that this pre-trained model will either provide the initialized weights leading to a faster convergence or it will act as a fixed feature extractor for the task of interest.\n\n\n\nThese two major transfer learning scenarios look as follows:\n\n* Finetuning the convnet: Instead of random initializaion, we initialize the network with a pretrained network, like the one that has been trained on a large dataset like imagenet 1000. Rest of the training looks as usual. In this scenario the entire network needs to be retrained on the dataset of our interest\n\n* ConvNet as fixed feature extractor: Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained.\n\nIn this notebook we will demonstrate the first scenario.\n","0184398f":"*  ### Building the Model","b31e8568":"## **Pld** here helps keep our learning rate variable. \n* So that , initially the learning rate is higher and then decreases","998f0e26":"## 5. Training and Validating the Pretrained Model\n\nWe use the **fit_generator()** function because we are using object of the **ImageDataGenerator** class to fetch data.","94d2530d":"### Visualizing the Data\n* Plotting info about the kind of data that we have here .","797c71c3":"# 13-dresses - Transfer Learning using Keras and DenseNet50\n - Created by \n ### *Nitesh Chaudhry*\n### How to implement transfer learning on categorical data using keras\n* This notebook is a brief application of transfer learning on the 13-dresses dataset. \n* This data set consists of 1411 images of 13 dresses.\n* We compare and apply various transfer learning approaches , and find the optimal one..\n* The size of sample images for each target is too low. This is one to be a ***Challenging*** task. :}\n\n\n### **Contents:**\n\n*  **1. Brief Explanation of Transfer Learning**\n*  **2. Transfer Learning using Kaggle Kernels**\n*  **3. Reading and Visualizing the Data**   \n*  **4. Building and Compiling the Models**    \n*  **5. Training and Validating the Pretrained Model** \n*  **6. Performance of Pretrained Models**\n\n\n","35306afb":"### What these arguments mean ? https:\/\/keras.io\/preprocessing\/image\/","35c4aa27":"This (Best Performing) Model was trained on DenseNet201","36566ee9":"## 2. Transfer Learning using Kaggle Kernels\n\n### Using the Keras Pretrained Models dataset\nKaggle Kernels *can* use a network connection to download pretrained keras models.","82908b8b":"We used InceptionResNetV2 for training the above model.","30f3cca2":"> - As we can see, all the classes are extremely well-balanced in training as well as the validation.\n\n## 4. Building and Compiling the Model\n### Building the Models\n\nHere, we load the DenseNet with the ImageNet weights. We remove the top so that we can add our own layer according to the number of our classes. We then add our own layers to complete the model architecture.\n\n\n## Things to know : \n Choosing parameters in Deep learning is more of an art. To get good performance on pre-trained models , we need to know that :\n * Our performance will depend on what that model was trained on . For ex. a model trained on animals will detect animals better than one pre-triained on , lets say , cars.\n * There is no one-size-fits-all algorithm when it comes to transfer learning . What we can do instead is to try and see what works and what does'nt.\n * These models were trained on imagenet dataset. this dataset was trained on a variety of images. \n \n See the availiable models here : https:\/\/keras.io\/applications\/\n \n","26999e1c":"### Compiling the Models\nWe set the loss function, the optimization algorithm to be used and metrics to be calculated at the end of each epoch.","a6e35071":"## 3. Reading and Visualizing the Data\n### Reading the Data\n\nLike the rest of Keras, the image augmentation API is simple and powerful. We will use the **ImageDataGenerator** to fetch data and feed it to our network\n\nKeras provides the **ImageDataGenerator** class that defines the configuration for image data preparation and augmentation. Rather than performing the operations on your entire image dataset in memory, the API is designed to be iterated by the deep learning model fitting process, creating augmented image data for you just-in-time. This reduces your memory overhead, but adds some additional time cost during model training.\n\nThe data generator itself is in fact an iterator, returning batches of image samples from the directory when requested. We can configure the batch size and prepare the data generator and get batches of images by calling the **flow_from_directory()** function."}}