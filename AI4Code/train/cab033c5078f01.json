{"cell_type":{"6333e499":"code","dec91d20":"code","defad153":"code","bad84177":"code","9928b2c4":"code","a75cacd4":"code","56cc8d07":"code","23ef0823":"code","4e3ce793":"code","a93fa46e":"code","4c180d6e":"code","db576207":"code","236b4fb5":"code","5b8ffa4d":"code","8927e91c":"code","9e6ab694":"code","ac6d0939":"code","c66adeb0":"code","a9231228":"code","28126046":"code","88641639":"code","71c1e532":"code","be36a7ee":"code","8585d86a":"code","64d4d220":"code","46bc071e":"code","1b45dd1e":"code","4f4dfa39":"code","9d7d3674":"code","20ab7594":"code","c1e5dcab":"code","73f364ce":"code","f6a2a3f3":"code","a00d61a4":"code","e21429b3":"code","3c0a8042":"code","444b7d47":"code","21330d43":"code","98b68418":"code","aecaff38":"code","c77b70b3":"code","c41ebf46":"code","48cf625c":"code","f46aa93c":"code","ece65fb0":"code","60ff78a4":"code","48b2743b":"code","47463404":"code","628b5e5e":"code","8916de8e":"code","fccaac68":"code","5ef7639a":"code","ad4196e5":"code","6c5dec24":"code","163cf7e6":"code","2f35940d":"code","85d00b8e":"code","634f9737":"code","3d6390d8":"code","dfd46e74":"code","d3cd29b7":"code","6603f8d2":"code","69e2eba9":"code","9cae10c5":"code","8d084c23":"code","998a0024":"code","f2724dea":"code","55bb0407":"code","06f5247e":"code","1140cab3":"code","e8615bc3":"code","f41185d6":"code","477b585d":"code","173a671d":"code","8ac150d9":"code","522894d7":"code","905cc1e9":"code","1248d087":"code","94644d5e":"code","9e1d2ea3":"code","8e861841":"code","84b95684":"code","ade35c02":"code","02d45e1a":"code","a1058d7d":"code","33bee0e0":"code","46c5c6bf":"code","42ef2c5f":"code","32217484":"code","3f500901":"code","e460b51d":"code","97326582":"code","7ac3c7a0":"code","3f80d67a":"code","de5674c3":"code","4f13238d":"code","89460df1":"code","ec726a1e":"code","bdd60ff2":"code","96a3779a":"code","8fae8016":"code","d7bf08ca":"code","559b0501":"code","a1ec9c68":"code","ce87d083":"code","d6b7e8bf":"code","eb1d8c5d":"code","6dea713b":"code","389ac3ec":"code","14e6fb10":"code","6f72fdaf":"code","13501b94":"code","89af8d22":"code","a8e7e43c":"code","64002e02":"code","c9adbf36":"markdown","883e3d92":"markdown","cb0df657":"markdown","94a24799":"markdown","27fb7c95":"markdown","6d9946f0":"markdown","d39c73b4":"markdown","ab5c59cf":"markdown","6aff6231":"markdown","b369b57a":"markdown","0e2d71d6":"markdown","5669c5f4":"markdown","a579c3b8":"markdown","0e739c2d":"markdown","4e3bd4f4":"markdown","8981ba35":"markdown","e0701319":"markdown","96c16531":"markdown","75744961":"markdown","4eacd47e":"markdown","0f0e7f2a":"markdown","33751d26":"markdown","71706691":"markdown","6b7dade6":"markdown","ef5c82a2":"markdown","b3634d14":"markdown","35414838":"markdown","12f95572":"markdown","da326970":"markdown","bdb75c54":"markdown","6a0c081b":"markdown","d7cb2c2c":"markdown","a8dee759":"markdown","12431b8f":"markdown","a5856778":"markdown","4691eab0":"markdown","fcb23fb2":"markdown","0d6b8b2a":"markdown","356c0e00":"markdown","65c97c71":"markdown","9baaad1f":"markdown","22b9d6fe":"markdown","779425a7":"markdown","65f84b9e":"markdown","9a7f53a7":"markdown","1043de67":"markdown","7ed42d42":"markdown","aa24cd5e":"markdown","8625df17":"markdown","e1dc6ab6":"markdown","f43cc46d":"markdown","a67923f4":"markdown","4b81c129":"markdown","d53aed3b":"markdown","8130aa34":"markdown","f95b4005":"markdown","13a1ee2b":"markdown","3f6604a8":"markdown","9344da31":"markdown","fe510a1d":"markdown","7d510464":"markdown","2c3dd74e":"markdown","b1180c7e":"markdown","1b6632b4":"markdown","66411530":"markdown","67f5a41c":"markdown","a15519cf":"markdown","9738b3e8":"markdown","9c38acfb":"markdown","60054fd6":"markdown","8e3c098b":"markdown","e2a3b574":"markdown","b52481a4":"markdown","32f139f7":"markdown","43ec68ba":"markdown","35b43f9b":"markdown","82c2b241":"markdown","83ff59ea":"markdown","306e45f5":"markdown","8ae75319":"markdown","1c327d19":"markdown","56886945":"markdown","f6da7f9b":"markdown","69f0bde2":"markdown","6547335e":"markdown","46d3dfa4":"markdown","73056275":"markdown","d8885680":"markdown","b41a286f":"markdown","691654e1":"markdown"},"source":{"6333e499":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set()\n\nfrom catboost import CatBoostRegressor, Pool, cv\nfrom catboost import MetricVisualizer\n\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\n\nfrom scipy.stats import boxcox\nfrom os import listdir\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=RuntimeWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\nimport shap\nshap.initjs()","dec91d20":"print(listdir(\"..\/input\"))","defad153":"data = pd.read_csv(\"..\/input\/ecommerce-data\/data.csv\", encoding=\"ISO-8859-1\", dtype={'CustomerID': str})\ndata.shape","bad84177":"data.head()","9928b2c4":"missing_percentage = data.isnull().sum() \/ data.shape[0] * 100\nmissing_percentage","a75cacd4":"data[data.Description.isnull()].head()","56cc8d07":"data[data.Description.isnull()].CustomerID.isnull().value_counts()","23ef0823":"data[data.Description.isnull()].UnitPrice.value_counts()","4e3ce793":"data[data.CustomerID.isnull()].head()","a93fa46e":"data.loc[data.CustomerID.isnull(), [\"UnitPrice\", \"Quantity\"]].describe()","4c180d6e":"data.loc[data.Description.isnull()==False, \"lowercase_descriptions\"] = data.loc[\n    data.Description.isnull()==False,\"Description\"\n].apply(lambda l: l.lower())\n\ndata.lowercase_descriptions.dropna().apply(\n    lambda l: np.where(\"nan\" in l, True, False)\n).value_counts()","db576207":"data.lowercase_descriptions.dropna().apply(\n    lambda l: np.where(\"\" == l, True, False)\n).value_counts()","236b4fb5":"data.loc[data.lowercase_descriptions.isnull()==False, \"lowercase_descriptions\"] = data.loc[\n    data.lowercase_descriptions.isnull()==False, \"lowercase_descriptions\"\n].apply(lambda l: np.where(\"nan\" in l, None, l))","5b8ffa4d":"data = data.loc[(data.CustomerID.isnull()==False) & (data.lowercase_descriptions.isnull()==False)].copy()","8927e91c":"data.isnull().sum().sum()","9e6ab694":"data[\"InvoiceDate\"] = pd.to_datetime(data.InvoiceDate, cache=True)\n\ndata.InvoiceDate.max() - data.InvoiceDate.min()","ac6d0939":"print(\"Datafile starts with timepoint {}\".format(data.InvoiceDate.min()))\nprint(\"Datafile ends with timepoint {}\".format(data.InvoiceDate.max()))","c66adeb0":"data.InvoiceNo.nunique()","a9231228":"data[\"IsCancelled\"]=np.where(data.InvoiceNo.apply(lambda l: l[0]==\"C\"), True, False)\ndata.IsCancelled.value_counts() \/ data.shape[0] * 100","28126046":"data.loc[data.IsCancelled==True].describe()","88641639":"data = data.loc[data.IsCancelled==False].copy()\ndata = data.drop(\"IsCancelled\", axis=1)","71c1e532":"data.StockCode.nunique()","be36a7ee":"stockcode_counts = data.StockCode.value_counts().sort_values(ascending=False)\nfig, ax = plt.subplots(2,1,figsize=(20,15))\nsns.barplot(stockcode_counts.iloc[0:20].index,\n            stockcode_counts.iloc[0:20].values,\n            ax = ax[0], palette=\"Oranges_r\")\nax[0].set_ylabel(\"Counts\")\nax[0].set_xlabel(\"Stockcode\")\nax[0].set_title(\"Which stockcodes are most common?\");\nsns.distplot(np.round(stockcode_counts\/data.shape[0]*100,2),\n             kde=False,\n             bins=20,\n             ax=ax[1], color=\"Orange\")\nax[1].set_title(\"How seldom are stockcodes?\")\nax[1].set_xlabel(\"% of data with this stockcode\")\nax[1].set_ylabel(\"Frequency\");","8585d86a":"def count_numeric_chars(l):\n    return sum(1 for c in l if c.isdigit())\n\ndata[\"StockCodeLength\"] = data.StockCode.apply(lambda l: len(l))\ndata[\"nNumericStockCode\"] = data.StockCode.apply(lambda l: count_numeric_chars(l))","64d4d220":"fig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.countplot(data[\"StockCodeLength\"], palette=\"Oranges_r\", ax=ax[0])\nsns.countplot(data[\"nNumericStockCode\"], palette=\"Oranges_r\", ax=ax[1])\nax[0].set_xlabel(\"Length of stockcode\")\nax[1].set_xlabel(\"Number of numeric chars in the stockcode\");","46bc071e":"data.loc[data.nNumericStockCode < 5].lowercase_descriptions.value_counts()","1b45dd1e":"data = data.loc[(data.nNumericStockCode == 5) & (data.StockCodeLength==5)].copy()\ndata.StockCode.nunique()","4f4dfa39":"data = data.drop([\"nNumericStockCode\", \"StockCodeLength\"], axis=1)","9d7d3674":"data.Description.nunique()","20ab7594":"description_counts = data.Description.value_counts().sort_values(ascending=False).iloc[0:30]\nplt.figure(figsize=(20,5))\nsns.barplot(description_counts.index, description_counts.values, palette=\"Purples_r\")\nplt.ylabel(\"Counts\")\nplt.title(\"Which product descriptions are most common?\");\nplt.xticks(rotation=90);","c1e5dcab":"def count_lower_chars(l):\n    return sum(1 for c in l if c.islower())","73f364ce":"data[\"DescriptionLength\"] = data.Description.apply(lambda l: len(l))\ndata[\"LowCharsInDescription\"] = data.Description.apply(lambda l: count_lower_chars(l))","f6a2a3f3":"fig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.countplot(data.DescriptionLength, ax=ax[0], color=\"Purple\")\nsns.countplot(data.LowCharsInDescription, ax=ax[1], color=\"Purple\")\nax[1].set_yscale(\"log\")","a00d61a4":"lowchar_counts = data.loc[data.LowCharsInDescription > 0].Description.value_counts()\n\nplt.figure(figsize=(15,3))\nsns.barplot(lowchar_counts.index, lowchar_counts.values, palette=\"Purples_r\")\nplt.xticks(rotation=90);","e21429b3":"def count_upper_chars(l):\n    return sum(1 for c in l if c.isupper())\n\ndata[\"UpCharsInDescription\"] = data.Description.apply(lambda l: count_upper_chars(l))","3c0a8042":"data.UpCharsInDescription.describe()","444b7d47":"data.loc[data.UpCharsInDescription <=5].Description.value_counts()","21330d43":"data = data.loc[data.UpCharsInDescription > 5].copy()","98b68418":"dlength_counts = data.loc[data.DescriptionLength < 14].Description.value_counts()\n\nplt.figure(figsize=(20,5))\nsns.barplot(dlength_counts.index, dlength_counts.values, palette=\"Purples_r\")\nplt.xticks(rotation=90);","aecaff38":"data.StockCode.nunique()","c77b70b3":"data.Description.nunique()","c41ebf46":"data.groupby(\"StockCode\").Description.nunique().sort_values(ascending=False).iloc[0:10]","48cf625c":"data.loc[data.StockCode == \"23244\"].Description.value_counts()","f46aa93c":"data.CustomerID.nunique()","ece65fb0":"customer_counts = data.CustomerID.value_counts().sort_values(ascending=False).iloc[0:20] \nplt.figure(figsize=(20,5))\nsns.barplot(customer_counts.index, customer_counts.values, order=customer_counts.index)\nplt.ylabel(\"Counts\")\nplt.xlabel(\"CustomerID\")\nplt.title(\"Which customers are most common?\");\n#plt.xticks(rotation=90);","60ff78a4":"data.Country.nunique()","48b2743b":"country_counts = data.Country.value_counts().sort_values(ascending=False).iloc[0:20]\nplt.figure(figsize=(20,5))\nsns.barplot(country_counts.index, country_counts.values, palette=\"Greens_r\")\nplt.ylabel(\"Counts\")\nplt.title(\"Which countries made the most transactions?\");\nplt.xticks(rotation=90);\nplt.yscale(\"log\")","47463404":"data.loc[data.Country==\"United Kingdom\"].shape[0] \/ data.shape[0] * 100","628b5e5e":"data[\"UK\"] = np.where(data.Country == \"United Kingdom\", 1, 0)","8916de8e":"data.UnitPrice.describe()","fccaac68":"data.loc[data.UnitPrice == 0].sort_values(by=\"Quantity\", ascending=False).head()","5ef7639a":"data = data.loc[data.UnitPrice > 0].copy()","ad4196e5":"fig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(data.UnitPrice, ax=ax[0], kde=False, color=\"red\")\nsns.distplot(np.log(data.UnitPrice), ax=ax[1], bins=20, color=\"tomato\", kde=False)\nax[1].set_xlabel(\"Log-Unit-Price\");","6c5dec24":"np.exp(-2)","163cf7e6":"np.exp(3)","2f35940d":"np.quantile(data.UnitPrice, 0.95)","85d00b8e":"data = data.loc[(data.UnitPrice > 0.1) & (data.UnitPrice < 20)].copy()","634f9737":"data.Quantity.describe()","3d6390d8":"fig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(data.Quantity, ax=ax[0], kde=False, color=\"limegreen\");\nsns.distplot(np.log(data.Quantity), ax=ax[1], bins=20, kde=False, color=\"limegreen\");\nax[0].set_title(\"Quantity distribution\")\nax[0].set_yscale(\"log\")\nax[1].set_title(\"Log-Quantity distribution\")\nax[1].set_xlabel(\"Natural-Log Quantity\");","dfd46e74":"np.exp(4)","d3cd29b7":"np.quantile(data.Quantity, 0.95)","6603f8d2":"data = data.loc[data.Quantity < 55].copy()","69e2eba9":"data[\"Revenue\"] = data.Quantity * data.UnitPrice\n\ndata[\"Year\"] = data.InvoiceDate.dt.year\ndata[\"Quarter\"] = data.InvoiceDate.dt.quarter\ndata[\"Month\"] = data.InvoiceDate.dt.month\ndata[\"Week\"] = data.InvoiceDate.dt.week\ndata[\"Weekday\"] = data.InvoiceDate.dt.weekday\ndata[\"Day\"] = data.InvoiceDate.dt.day\ndata[\"Dayofyear\"] = data.InvoiceDate.dt.dayofyear\ndata[\"Date\"] = pd.to_datetime(data[['Year', 'Month', 'Day']])","9cae10c5":"grouped_features = [\"Date\", \"Year\", \"Quarter\",\"Month\", \"Week\", \"Weekday\", \"Dayofyear\", \"Day\",\n                    \"StockCode\"]","8d084c23":"daily_data = pd.DataFrame(data.groupby(grouped_features).Quantity.sum(),\n                          columns=[\"Quantity\"])\ndaily_data[\"Revenue\"] = data.groupby(grouped_features).Revenue.sum()\ndaily_data = daily_data.reset_index()\ndaily_data.head(5)","998a0024":"daily_data.loc[:, [\"Quantity\", \"Revenue\"]].describe()","f2724dea":"low_quantity = daily_data.Quantity.quantile(0.01)\nhigh_quantity = daily_data.Quantity.quantile(0.99)\nprint((low_quantity, high_quantity))","55bb0407":"low_revenue = daily_data.Revenue.quantile(0.01)\nhigh_revenue = daily_data.Revenue.quantile(0.99)\nprint((low_revenue, high_revenue))","06f5247e":"samples = daily_data.shape[0]","1140cab3":"daily_data = daily_data.loc[\n    (daily_data.Quantity >= low_quantity) & (daily_data.Quantity <= high_quantity)]\ndaily_data = daily_data.loc[\n    (daily_data.Revenue >= low_revenue) & (daily_data.Revenue <= high_revenue)]","e8615bc3":"samples - daily_data.shape[0]","f41185d6":"fig, ax = plt.subplots(1,2,figsize=(20,5))\nsns.distplot(daily_data.Quantity.values, kde=True, ax=ax[0], color=\"Orange\", bins=30);\nsns.distplot(np.log(daily_data.Quantity.values), kde=True, ax=ax[1], color=\"Orange\", bins=30);\nax[0].set_xlabel(\"Number of daily product sales\");\nax[0].set_ylabel(\"Frequency\");\nax[0].set_title(\"How many products are sold per day?\");","477b585d":"class CatHyperparameter:\n    \n    def __init__(self,\n                 loss=\"RMSE\",\n                 metric=\"RMSE\",\n                 iterations=1000,\n                 max_depth=4,\n                 l2_leaf_reg=3,\n                 #learning_rate=0.5,\n                 seed=0):\n        self.loss = loss,\n        self.metric = metric,\n        self.max_depth = max_depth,\n        self.l2_leaf_reg = l2_leaf_reg,\n        #self.learning_rate = learning_rate,\n        self.iterations=iterations\n        self.seed = seed","173a671d":"class Catmodel:\n    \n    def __init__(self, name, params):\n        self.name = name\n        self.params = params\n    \n    def set_data_pool(self, train_pool, val_pool):\n        self.train_pool = train_pool\n        self.val_pool = val_pool\n    \n    def set_data(self, X, y, week):\n        cat_features_idx = np.where(X.dtypes != np.float)[0]\n        x_train, self.x_val = X.loc[X.Week < week], X.loc[X.Week >= week]\n        y_train, self.y_val = y.loc[X.Week < week], y.loc[X.Week >= week]\n        self.train_pool = Pool(x_train, y_train, cat_features=cat_features_idx)\n        self.val_pool = Pool(self.x_val, self.y_val, cat_features=cat_features_idx)\n    \n    def prepare_model(self):\n        self.model = CatBoostRegressor(\n                loss_function = self.params.loss[0],\n                random_seed = self.params.seed,\n                logging_level = 'Silent',\n                iterations = self.params.iterations,\n                max_depth = self.params.max_depth[0],\n                #learning_rate = self.params.learning_rate[0],\n                l2_leaf_reg = self.params.l2_leaf_reg[0],\n                od_type='Iter',\n                od_wait=40,\n                train_dir=self.name,\n                has_time=True\n            )\n    \n    def learn(self, plot=False):\n        self.prepare_model()\n        self.model.fit(self.train_pool, eval_set=self.val_pool, plot=plot);\n        print(\"{}, early-stopped model tree count {}\".format(\n            self.name, self.model.tree_count_\n        ))\n    \n    def score(self):\n        return self.model.score(self.val_pool)\n    \n    def show_importances(self, kind=\"bar\"):\n        explainer = shap.TreeExplainer(self.model)\n        shap_values = explainer.shap_values(self.val_pool)\n        if kind==\"bar\":\n            return shap.summary_plot(shap_values, self.x_val, plot_type=\"bar\")\n        return shap.summary_plot(shap_values, self.x_val)\n    \n    def get_val_results(self):\n        self.results = pd.DataFrame(self.y_val)\n        self.results[\"prediction\"] = self.predict(self.x_val)\n        self.results[\"error\"] = np.abs(\n            self.results[self.results.columns.values[0]].values - self.results.prediction)\n        self.results[\"Month\"] = self.x_val.Month\n        self.results[\"SquaredError\"] = self.results.error.apply(lambda l: np.power(l, 2))\n    \n    def show_val_results(self):\n        self.get_val_results()\n        fig, ax = plt.subplots(1,2,figsize=(20,5))\n        sns.distplot(self.results.error, ax=ax[0])\n        ax[0].set_xlabel(\"Single absolute error\")\n        ax[0].set_ylabel(\"Density\")\n        self.median_absolute_error = np.median(self.results.error)\n        print(\"Median absolute error: {}\".format(self.median_absolute_error))\n        ax[0].axvline(self.median_absolute_error, c=\"black\")\n        ax[1].scatter(self.results.prediction.values,\n                      self.results[self.results.columns[0]].values,\n                      c=self.results.error, cmap=\"RdYlBu_r\", s=1)\n        ax[1].set_xlabel(\"Prediction\")\n        ax[1].set_ylabel(\"Target\")\n        return ax\n    \n    def get_monthly_RMSE(self):\n        return self.results.groupby(\"Month\").SquaredError.mean().apply(lambda l: np.sqrt(l))\n        \n    def predict(self, x):\n        return self.model.predict(x)\n    \n    def get_dependence_plot(self, feature1, feature2=None):\n        explainer = shap.TreeExplainer(self.model)\n        shap_values = explainer.shap_values(self.val_pool)\n        if feature2 is None:\n            return shap.dependence_plot(\n                feature1,\n                shap_values,\n                self.x_val,\n            )\n        else:\n            return shap.dependence_plot(\n                feature1,\n                shap_values,\n                self.x_val,\n                interaction_index=feature2\n            )\n    \n    ","8ac150d9":"import GPyOpt\n\nclass Hypertuner:\n    \n    def __init__(self, model, max_iter=10, max_time=10,max_depth=6, max_l2_leaf_reg=20):\n        self.bounds = [{'name': 'depth','type': 'discrete','domain': (1,max_depth)},\n                       {'name': 'l2_leaf_reg','type': 'discrete','domain': (1,max_l2_leaf_reg)}]\n        self.model = model\n        self.max_iter=max_iter\n        self.max_time=max_time\n        self.best_depth = None\n        self.best_l2_leaf_reg = None\n    \n    def objective(self, params):\n        params = params[0]\n        params = CatHyperparameter(\n            max_depth=params[0],\n            l2_leaf_reg=params[1]\n        )\n        self.model.params = params\n        self.model.learn()\n        return self.model.score()\n    \n    def learn(self):\n        np.random.seed(777)\n        optimizer = GPyOpt.methods.BayesianOptimization(\n            f=self.objective, domain=self.bounds,\n            acquisition_type ='EI',\n            acquisition_par = 0.2,\n            exact_eval=True)\n        optimizer.run_optimization(self.max_iter, self.max_time)\n        optimizer.plot_convergence()\n        best = optimizer.X[np.argmin(optimizer.Y)]\n        self.best_depth = best[0]\n        self.best_l2_leaf_reg = best[1]\n        print(\"Optimal depth is {} and optimal l2-leaf-reg is {}\".format(self.best_depth, self.best_l2_leaf_reg))\n        print('Optimal RMSE:', np.min(optimizer.Y))\n    \n    def retrain_catmodel(self):\n        params = CatHyperparameter(\n            max_depth=self.best_depth,\n            l2_leaf_reg=self.best_l2_leaf_reg\n        )\n        self.model.params = params\n        self.model.learn(plot=True)\n        return self.model","522894d7":"class CatFamily:\n    \n    def __init__(self, params, X, y, n_splits=2):\n        self.family = {}\n        self.cat_features_idx = np.where(X.dtypes != np.float)[0]\n        self.X = X.values\n        self.y = y.values\n        self.n_splits = n_splits\n        self.params = params\n    \n    def set_validation_strategy(self):\n        self.cv = TimeSeriesSplit(max_train_size = None,\n                                  n_splits = self.n_splits)\n        self.gen = self.cv.split(self.X)\n    \n    def get_split(self):\n        train_idx, val_idx = next(self.gen)\n        x_train, x_val = self.X[train_idx], self.X[val_idx]\n        y_train, y_val = self.y[train_idx], self.y[val_idx]\n        train_pool = Pool(x_train, y_train, cat_features=self.cat_features_idx)\n        val_pool = Pool(x_val, y_val, cat_features=self.cat_features_idx)\n        return train_pool, val_pool\n    \n    def learn(self):\n        self.set_validation_strategy()\n        self.model_names = []\n        self.model_scores = []\n        for split in range(self.n_splits):\n            name = 'Model_cv_' + str(split) + '\/'\n            train_pool, val_pool = self.get_split()\n            self.model_names.append(name)\n            self.family[name], score = self.fit_catmodel(name, train_pool, val_pool)\n            self.model_scores.append(score)\n    \n    def fit_catmodel(self, name, train_pool, val_pool):\n        cat = Catmodel(name, train_pool, val_pool, self.params)\n        cat.prepare_model()\n        cat.learn()\n        score = cat.score()\n        return cat, score\n    \n    def score(self):\n        return np.mean(self.model_scores)\n    \n    def show_learning(self):\n        widget = MetricVisualizer(self.model_names)\n        widget.start()\n\n    def show_importances(self):\n        name = self.model_names[-1]\n        cat = self.family[name]\n        explainer = shap.TreeExplainer(cat.model)\n        shap_values = explainer.shap_values(cat.val_pool)\n        return shap.summary_plot(shap_values, X, plot_type=\"bar\")","905cc1e9":"daily_data.head()","1248d087":"week = daily_data.Week.max() - 2\nprint(\"Validation after week {}\".format(week))\nprint(\"Validation starts at timepoint {}\".format(\n    daily_data[daily_data.Week==week].Date.min()\n))","94644d5e":"X = daily_data.drop([\"Quantity\", \"Revenue\", \"Date\"], axis=1)\ndaily_data.Quantity = np.log(daily_data.Quantity)\ny = daily_data.Quantity\nparams = CatHyperparameter()\n\nmodel = Catmodel(\"baseline\", params)\nmodel.set_data(X,y, week)\nmodel.learn(plot=True)","9e1d2ea3":"model.score()","8e861841":"model.show_val_results();","84b95684":"model.show_importances()","ade35c02":"model.show_importances(kind=None)","02d45e1a":"np.mean(np.abs(np.exp(model.results.prediction) - np.exp(model.results.Quantity)))","a1058d7d":"np.median(np.abs(np.exp(model.results.prediction) - np.exp(model.results.Quantity)))","33bee0e0":"#search = Hypertuner(model, max_depth=5, max_l2_leaf_reg=30)\n#search.learn()\n#model = search.retrain_catmodel()\n#print(model.score())\n#model.show_importances(kind=None)","46c5c6bf":"products = pd.DataFrame(index=data.loc[data.Week < week].StockCode.unique(), columns = [\"MedianPrice\"])\n\nproducts[\"MedianPrice\"] = data.loc[data.Week < week].groupby(\"StockCode\").UnitPrice.median()\nproducts[\"MedianQuantities\"] = data.loc[data.Week < week].groupby(\"StockCode\").Quantity.median()\nproducts[\"Customers\"] = data.loc[data.Week < week].groupby(\"StockCode\").CustomerID.nunique()\nproducts[\"DescriptionLength\"] = data.loc[data.Week < week].groupby(\"StockCode\").DescriptionLength.median()\n#products[\"StockCode\"] = products.index.values\norg_cols = np.copy(products.columns.values)\nproducts.head()","42ef2c5f":"for col in org_cols:\n    if col != \"StockCode\":\n        products[col] = boxcox(products[col])[0]","32217484":"fig, ax = plt.subplots(1,3,figsize=(20,5))\nax[0].scatter(products.MedianPrice.values, products.MedianQuantities.values,\n           c=products.Customers.values, cmap=\"coolwarm_r\")\nax[0].set_xlabel(\"Boxcox-Median-UnitPrice\")\nax[0].set_ylabel(\"Boxcox-Median-Quantities\")","3f500901":"X = products.values\nscaler = StandardScaler()\nX = scaler.fit_transform(X)","e460b51d":"km = KMeans(n_clusters=30)\nproducts[\"cluster\"] = km.fit_predict(X)\n\ndaily_data[\"ProductType\"] = daily_data.StockCode.map(products.cluster)\ndaily_data.ProductType = daily_data.ProductType.astype(\"object\")\ndaily_data.head()","97326582":"daily_data[\"KnownStockCodeUnitPriceMedian\"] = daily_data.StockCode.map(\n    data.groupby(\"StockCode\").UnitPrice.median())\n\nknown_price_iqr = data.groupby(\"StockCode\").UnitPrice.quantile(0.75) \nknown_price_iqr -= data.groupby(\"StockCode\").UnitPrice.quantile(0.25) \ndaily_data[\"KnownStockCodeUnitPriceIQR\"] = daily_data.StockCode.map(known_price_iqr)","7ac3c7a0":"to_group = [\"StockCode\", \"Year\", \"Month\", \"Week\", \"Weekday\"]\n\ndaily_data = daily_data.set_index(to_group)\ndaily_data[\"KnownStockCodePrice_WW_median\"] = daily_data.index.map(\n    data.groupby(to_group).UnitPrice.median())\ndaily_data[\"KnownStockCodePrice_WW_mean\"] = daily_data.index.map(\n    data.groupby(to_group).UnitPrice.mean().apply(lambda l: np.round(l, 2)))\ndaily_data[\"KnownStockCodePrice_WW_std\"] = daily_data.index.map(\n    data.groupby(to_group).UnitPrice.std().apply(lambda l: np.round(l, 2)))\n\ndaily_data = daily_data.reset_index()","3f80d67a":"daily_data.head()","de5674c3":"plt.figure(figsize=(20,5))\nplt.plot(daily_data.groupby(\"Date\").Quantity.sum(), marker='+', c=\"darkorange\")\nplt.plot(daily_data.groupby(\"Date\").Quantity.sum().rolling(window=30, center=True).mean(),\n        c=\"red\")\nplt.xticks(rotation=90);\nplt.title(\"How many quantities are sold per day over the given time?\");","4f13238d":"fig, ax = plt.subplots(1,2,figsize=(20,5))\n\nweekdays = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\nyearmonth = [\"Dec-2010\", \"Jan-2011\", \"Feb-2011\", \"Mar-2011\", \"Apr-2011\", \"May-2011\",\n             \"Jun-2011\", \"Jul-1011\", \"Aug-2011\", \"Sep-2011\", \"Oct-2011\", \"Nov-2011\", \n             \"Dec-2011\"]\n\ndaily_data.groupby(\"Weekday\").Quantity.sum().plot(\n    ax=ax[0], marker='o', label=\"Quantity\", c=\"darkorange\");\nax[0].legend();\nax[0].set_xticks(np.arange(0,7))\nax[0].set_xticklabels(weekdays);\nax[0].set_xlabel(\"\")\nax[0].set_title(\"Total sales per weekday\");\n\nax[1].plot(daily_data.groupby([\"Year\", \"Month\"]).Quantity.sum().values,\n    marker='o', label=\"Quantities\", c=\"darkorange\");\nax[1].set_xticklabels(yearmonth, rotation=90)\nax[1].set_xticks(np.arange(0, len(yearmonth)))\nax[1].legend();\nax[1].set_title(\"Total sales per month\");","89460df1":"daily_data[\"PreChristmas\"] = (daily_data.Dayofyear <= 358) & (daily_data.Dayofyear >= 243) ","ec726a1e":"for col in [\"Weekday\", \"Month\", \"Quarter\"]:\n    daily_data = daily_data.set_index(col)\n    daily_data[col+\"Quantity_mean\"] = daily_data.loc[daily_data.Week < week].groupby(col).Quantity.mean()\n    daily_data[col+\"Quantity_median\"] = daily_data.loc[daily_data.Week < week].groupby(col).Quantity.median()\n    daily_data[col+\"Quantity_mean_median_diff\"] = daily_data[col+\"Quantity_mean\"] - daily_data[col+\"Quantity_median\"]\n    daily_data[col+\"Quantity_IQR\"] = daily_data.loc[\n        daily_data.Week < week].groupby(col).Quantity.quantile(0.75) - daily_data.loc[\n        daily_data.Week < week].groupby(col).Quantity.quantile(0.25)\n    daily_data = daily_data.reset_index()\ndaily_data.head()","bdd60ff2":"to_group = [\"StockCode\", \"PreChristmas\"]\ndaily_data = daily_data.set_index(to_group)\ndaily_data[\"PreChristmasMeanQuantity\"] = daily_data.loc[\n    daily_data.Week < week].groupby(to_group).Quantity.mean().apply(lambda l: np.round(l, 1))\ndaily_data[\"PreChristmasMedianQuantity\"] = daily_data.loc[\n    daily_data.Week < week].groupby(to_group).Quantity.median().apply(lambda l: np.round(l, 1))\ndaily_data[\"PreChristmasStdQuantity\"] = daily_data.loc[\n    daily_data.Week < week].groupby(to_group).Quantity.std().apply(lambda l: np.round(l, 1))\ndaily_data = daily_data.reset_index()","96a3779a":"for delta in range(1,4):\n    to_group = [\"Week\",\"Weekday\",\"ProductType\"]\n    daily_data = daily_data.set_index(to_group)\n        \n    daily_data[\"QuantityProducttypeWeekWeekdayLag_\" + str(delta) + \"_median\"] = daily_data.groupby(\n        to_group).Quantity.median().apply(lambda l: np.round(l,1)).shift(delta)\n    \n    daily_data = daily_data.reset_index()\n    daily_data.loc[daily_data.Week >= (week+delta),\n                   \"QuantityProductTypeWeekWeekdayLag_\" + str(delta) + \"_median\"] = np.nan\n    ","8fae8016":"data[\"ProductType\"] = data.StockCode.map(products.cluster)","d7bf08ca":"daily_data[\"TransactionsPerProductType\"] = daily_data.ProductType.map(data.loc[data.Week < week].groupby(\"ProductType\").InvoiceNo.nunique())","559b0501":"delta = 1\nto_group = [\"Week\", \"Weekday\", \"ProductType\"]\ndaily_data = daily_data.set_index(to_group)\ndaily_data[\"DummyWeekWeekdayAttraction\"] = data.groupby(to_group).CustomerID.nunique()\ndaily_data[\"DummyWeekWeekdayMeanUnitPrice\"] = data.groupby(to_group).UnitPrice.mean().apply(lambda l: np.round(l, 2))\n\ndaily_data[\"WeekWeekdayAttraction_Lag1\"] = daily_data[\"DummyWeekWeekdayAttraction\"].shift(1)\ndaily_data[\"WeekWeekdayMeanUnitPrice_Lag1\"] = daily_data[\"DummyWeekWeekdayMeanUnitPrice\"].shift(1)\n\ndaily_data = daily_data.reset_index()\ndaily_data.loc[daily_data.Week >= (week + delta), \"WeekWeekdayAttraction_Lag1\"] = np.nan\ndaily_data.loc[daily_data.Week >= (week + delta), \"WeekWeekdayMeanUnitPrice_Lag1\"] = np.nan\ndaily_data = daily_data.drop([\"DummyWeekWeekdayAttraction\", \"DummyWeekWeekdayMeanUnitPrice\"], axis=1)","a1ec9c68":"daily_data[\"TransactionsPerStockCode\"] = daily_data.StockCode.map(\n    data.loc[data.Week < week].groupby(\"StockCode\").InvoiceNo.nunique())","ce87d083":"daily_data.head()","d6b7e8bf":"daily_data[\"CustomersPerWeekday\"] = daily_data.Month.map(\n    data.loc[data.Week < week].groupby(\"Weekday\").CustomerID.nunique())","eb1d8c5d":"X = daily_data.drop([\"Quantity\", \"Revenue\", \"Date\", \"Year\"], axis=1)\ny = daily_data.Quantity\nparams = CatHyperparameter()\n\nmodel = Catmodel(\"new_features_1\", params)\nmodel.set_data(X,y, week)\nmodel.learn(plot=True)","6dea713b":"model.score()","389ac3ec":"model.show_importances(kind=None)","14e6fb10":"model.show_val_results();","6f72fdaf":"np.mean(np.abs(np.exp(model.results.prediction) - np.exp(model.results.Quantity)))","13501b94":"np.median(np.abs(np.exp(model.results.prediction) - np.exp(model.results.Quantity)))","89af8d22":"search = Hypertuner(model)\n#search.learn()","a8e7e43c":"#model = search.retrain_catmodel()\n#print(model.score())","64002e02":"#model.show_importances(kind=None)","c9adbf36":"### The Time period <a class=\"anchor\" id=\"timeperiod\"><\/a>\n\nHow long is the period in days?","883e3d92":"#### How are the quantities and revenues distributed?","cb0df657":"### Bayesian Hyperparameter Search with GPyOpt <a class=\"anchor\" id=\"hypersearch\"><\/a>\n\nOk, now we have gained some feeling for a single model. Let's find out if we can obtain a better score after hyperparameter search:","94a24799":"**Time series validation Catfamily**\n\nThis model holds the information about how to split the data into validation chunks and it organizes the training with sliding window validation. Furthermore it can return a score as the mean over all RMSE scores of its models. ","27fb7c95":"Let's create a feature to indicate inside or outside of the UK:","6d9946f0":"And the unit price?","d39c73b4":"**All cancellations have negative quantites but positive, non-zero unit prices**. Given this data we are not easily able to understand why a customer made a return and it's very difficult to predict such cases as there could be several, hidden reasons why a cancellation was done. Let's drop them:","ab5c59cf":"**Hidden missing descriptions**\n\nCan we find \"nan\"-Strings?","6aff6231":"How many unique stockcodes do we have?","b369b57a":"How many different invoice numbers do we have?","0e2d71d6":"Wow, we still have stockcodes with multiple descriptions. Let's look at an example:","5669c5f4":"### Validation strategy\n\nAs the **data covers only one year and we have a high increase of sold products during pre-christmas period**, we need to select validation data carefully. I will start with validation data that covers at least 8 full weeks (+ remaining days). After generating new features by exploring the data, I will use a sliding window time series validation that should help us to understand if the model is able to solve the prediction task during both times: pre-christmas season and non-christmas season. ","a579c3b8":"As the key task of this kernel is to predict the amount of products sold per day, we can sum up the daily quantities per product stockcode :","0e739c2d":"**Hyperparameter Class**\n\nThis class holds all important hyperparameters we have to set before training like the loss function, the evaluation metric, the max depth of trees, the number of max number of trees (iterations) and the l2_leaf_reg for regularization to avoid overfitting. ","4e3bd4f4":"* We can see that the **distribution of absolute errors of single predictions is right skewed.** \n* The **median single error (black) is half of the RMSE score and significantly lower**. \n* By plotting the target versus prediction we can see that we **made higher errors for validation entries that have high true quantity values above 30**. The strong blue line shows the identity where predictions are close to target values. **To improve we need to make better predictions for products with true high quantities during validation time**. ","8981ba35":"We can see that the distributions are **right skewed. Lower values are more common**. In addition the daily sales quantities seem to be **multimodal**. A daily sale of 1 is common as well as a quantity of 12 and 24. This pattern is very interesting and leads to the conclusion that quantities are often divisible by 2 or 3. In a nutshell we can say that specific products are often bought as single quantites or in a small bunch.","e0701319":"* We can see that the **stock code as well as the description of the products are very important**. They do not have a color as they are not numerical and do not have low or high values. \n* The **weekday** is an important feature as well. We have already seen this by exploring the data. Low values from monday up to thursday are those days where the retailer sales most products. In contrast high values (friday to sunday) only yield a few sales. ","96c16531":"How often do we miss the customer as well?","75744961":"Ok, descriptions with small length look valid and we should not drop them. Ok, now let's see how many unique stock codes do we have and how many unique descriptions?","4eacd47e":"That's bad as well. **The price and the quantities of entries without a customer ID can show extreme outliers**. As we might want to create features later on that are based on historical prices and sold quantities, this is very disruptive. Our first **advice for the retailer is to setup strategies for transactions that are somehow faulty or special**. And the question remains: Why is it possible for a transaction to be without a customer ID. Perhaps you can purchase as a quest but then it would of a good and clean style to plugin a special ID that indicates that this one is a guest. Ok, next one: Do we have hidden nan-values in Descriptions? To find it out, let's create a new feature that hold descriptions in lowercase:","0f0e7f2a":"Ihh, again something that we don't want to predict. Again this indicates that the retailer does not speparate well between special kind of transactions and valid customer-retailer transactions. Let's drop all of these occurences:","33751d26":"### Customers <a class=\"anchor\" id=\"customers\"><\/a>","71706691":"How many % of missing values do we have for each feature?","6b7dade6":"## How to predict daily product sales? <a class=\"anchor\" id=\"model\"><\/a>\n\nIn this kernel I like to use [catboost ](https:\/\/catboost.ai\/) as predictive model. The prediction of daily quantities and revenues are both regression tasks and consequently I will use the catboost regressor. The loss and metric I like to use is the [root mean square error](https:\/\/en.wikipedia.org\/wiki\/Root-mean-square_deviation) (RMSE):\n\n$$ E = \\sqrt{ \\frac{1}{N}\\sum_{n=1}^{N} (t_{n} - y_{n})^{2}}$$\n\nIt computes the error between the target value $t_{n}$ and the predicted value $y_{n}$ per sample, takes the square to make sure that both, positive and negative deviations, contribute to the sum the same way. Then the mean is taken by dividing with the total amount $N$ of samples (entries) in the data. And finally to obtain an impression of the error for single predictions, the root is taken. What should we keep in mind when working with this loss and metric function? :-) It's heavily influenced by outliers! **If we have some predictions that are far away from the targets, they will guide the mean towards higher values as well. Hence it could be that we will make nice predictions for a majority of samples but the RMSE is still high due to high errors for a minority of samples**. ","ef5c82a2":"### Unit Price <a class=\"anchor\" id=\"unitprice\"><\/a>","b3634d14":"Ok, browsing through the cases we can see that stockcodes are sometimes named a bit differently due to missing or changed words or typing errors. None the less they look ok and we can continue.","35414838":"### Countries <a class=\"anchor\" id=\"countries\"><\/a>\n\nHow many unique countries are delivered by the retailer?","12f95572":"**Catmodel class**\n\nThis model obtains a train & validation pool as data or pandas dataframes for features X and targets y together with a week. It's the first week of our validation data and all other weeks above are used as well. It trains the model and can show learning process as well as feature importances and some figures for result analysis. It's the fastest choice you can make for playing around.","da326970":"### Descriptions <a class=\"anchor\" id=\"descriptions\"><\/a>\n\nHow many unique descriptions do we have?","bdb75c54":"In the data description we can find that a cancelled transactions starts with a \"C\" in front of it. Let's create a feature to easily filter out these cases:","6a0c081b":"Ok, we can see that **some descriptions correspond to a similar product type**. Do you see the multiple occurences of lunch bags? We often have **color information about the product** as well. Furthermore the most common descriptions seem to confirm that **the retailer sells various different kinds of products**. All descriptions seem to consist of **uppercase chars**. Ok, now let's do some addtional analysis on the descriptions by counting the length and the number of lowercase chars. ","d7cb2c2c":"**Next day carriage and high resolution image are strange!** Let's compute the fraction of lower with respect to uppercase letters:","a8dee759":"And which are most common?","12431b8f":"Oh, great! **Almost all descriptions do not have a lowercase chars, but we have found exceptional cases!**","a5856778":"In this case we would still cover more than 95 % of the data!","4691eab0":"### Missing values <a class=\"anchor\" id=\"missing\"><\/a>","fcb23fb2":"In **cases of missing descriptions we always miss the customer and the unit price as well**. Why does the retailer records such kind of entries without a further description? It seems that there is no sophisticated procedure how to deal with and record such kind of transactions. This is already a hint that **we could expect strange entries in our data and that it can be difficult to detect them**! ","0d6b8b2a":"We found **additional, hidden nan-values that show a string \"nan\" instead of a nan-value**. Let's transform them to NaN: ","356c0e00":"### Temporal patterns","65c97c71":"### Baseline model & result analysis  <a class=\"anchor\" id=\"baseline\"><\/a>\n\nLet's see how good this model performs without feature engineering and hyperparameter search:","9baaad1f":"We can see that the retailer sells almost all products in the UK, followed by many european countries. How many percentage of entries are inside UK?","22b9d6fe":"Ok, most products are sold in quantities from 1 to 12. But, we have extreme, unrealistic outliers again:","779425a7":"**Lag-Features:**","65f84b9e":"### Quantities <a class=\"anchor\" id=\"quantities\"><\/a>\n\nOk, the most important one - the target. Let's take a look at its distribution:","9a7f53a7":"## Table of contents\n\n**Caution - Everything is heavily under construction ;-)** \n\n### [Get ready to take-off](#takeoff) \n\n1. [Prepare to start](#load) (complete)\n2. [Get familiar with the data](#intro) (complete)\n3. [Get an initial feeling for the data by exploration](#feeling) \n    * [Missing values](#missing) (complete)\n    * [The time period](#timeperiod) (complete)\n    * [The invoice number](#invoiceno) (complete)\n    * [Stockcodes](#stockcodes) (complete)\n    * [Descriptions](#descriptions) (complete)\n    * [Customers](#customers)\n    * [Countries](#countries) (complete)\n    * [Unit Price](#unitprice)\n    * [Quantities](#quantities) (complete)\n    * [Revenues](#revenues)\n    * [Conclusion](#expconclusion)\n4. [Focus on daily product sales](#daily) ","1043de67":"### Revenues <a class=\"anchor\" id=\"revenues\"><\/a>","7ed42d42":"Almost 25 % of the customers are unknown! That's very strange. In addition we have 0.2 % of missing descriptions. This looks dirty. Let's gain a further impression by considering some examples.","aa24cd5e":"* Do you the the **POST** in the most common stockcode counts?! **That's a strange one!** Hence we could expect strange occurences not only in the descriptions and customerIDs but also in the stockcode. OHOHOH! It's code is shorter than the others as well as not numeric. \n* Most stockcodes are very seldom. This indicates that the **retailer sells many different products** and that there is no strong secialization of a specific stockcode. Nevertheless we have to be careful as this must not mean that the retailer is not specialized given a specific product type. The stockcode could be a very detailed indicator that does not yield information of the type, for example water bottles may have very different variants in color, name and shapes but they are all water bottles.  ","8625df17":"### About countries and customers\n\n","e1dc6ab6":"**Hyperparameter-Search class**\n\nThis is a class for hyperparameter search that uses Bayesian Optimization and Gaussian Process Regression to find optimal hyperparameters. I decided to use this method as the computation of the score for one catfamily model may be expensive. In this case bayesian optimization could be a plus. As this optimization methods takes some time as well you should try random search as well as this may be faster. ","f43cc46d":"That's not good again. It's not obvious if they are gifts to customers or not :-( Let's drop them:","a67923f4":"Just to be sure: Is there a missing value left?","4b81c129":"### Conclusion <a class=\"anchor\" id=\"expconclusion\"><\/a>","d53aed3b":"Can we find empty \"\"-strings?","8130aa34":"## 3. Get an initial feeling for the data by exploration <a class=\"anchor\" id=\"feeling\"><\/a>","f95b4005":"### Catboost family and hyperparameter class <a class=\"anchor\" id=\"classes\"><\/a>\n\nTo easily generate new models and compare results between them I wrote some classes:","13a1ee2b":"**Missing Customer IDs**","3f6604a8":"The data has 541909 entries and 8 variables.","9344da31":"### Stockcodes <a class=\"anchor\" id=\"stockcodes\"><\/a>","fe510a1d":"We can see that the datafile has information given for **each single transaction**. Take a look at the InvoiceNo and the CustomerID of the first entries. Here we can see that **one customer with ID 17850 of the United Kingdom made a single order that has the InvoideNo 536365**. The customer ordered **several products with different stockcodes, descriptions, unit prices and quantities**. In addition we can see that the InvoiceDate was the same for these products. ","7d510464":"### The invoice number <a class=\"anchor\" id=\"invoiceno\"><\/a>","2c3dd74e":"Both visualisations yield further interesting insights:\n\n* **Thursday** seems to be the day on which most products are sold. \n* In contrast **friday, and sunday** have very **low transactions** \n* On **saturday** there are **no transactions** at all\n* The **pre-Christmas season** starts in **september and shows a peak in november**\n* Indeed **february and april are month with very low sales**. \n\nLet's create some new features for our daily aggregation that may be helpful to make better predictions:","b1180c7e":"## Focus on daily product sales <a class=\"anchor\" id=\"daily\"><\/a>\n\nAs we like to predict the daily amount of product sales, we need to compute a daily aggregation of this data. For this purpose we need to extract temporal features out of the InvoiceDate. In addition we can compute the revenue gained by a transaction using the unit price and the quantity:","1b6632b4":"**Missing descriptions**","66411530":"2,2 % of all entries are cancellations.  ","67f5a41c":"Let's **only use target ranges data that are occupied by  90 % of the data entries**. This is a first and easy strategy to exclude heavy outliers but we should always be aware of the fact that we have lost some information given by the remaining % we have excluded. It could be nice and useful in general to understand and analyse what has caused these outliers. ","a15519cf":"We still have more descriptions than stockcodes and we should continue to find out why they differ.","9738b3e8":"If you have forked this kernel and are in interactive model you can see that the model loss has converged. How big is the evaluated root mean square error on validation data?","9c38acfb":"This way we loose information abount customers, countries and price information but we will recover it later on during this kernel. Besides the quantities let's aggregate the revenues as well:","60054fd6":"As we can see by the min and max values the **target variable shows extreme outliers**.  If we would like to use it as targets, we **should exclude them as they will mislead our validation. As I like to use early stopping this will directly influence training of predictive models as well**.  ","8e3c098b":"And what about the descriptions with a length below 14?","e2a3b574":"As we don't know why customers or descriptions are missing and we have seen strange outliers in quantities and prices as well as zero-prices, **let's play safe and drop all of these occurences**.","b52481a4":"# Get ready to take-off <a class=\"anchor\" id=\"takeoff\"><\/a>\n\n## 1. Prepare to start <a class=\"anchor\" id=\"load\"><\/a>","32f139f7":"Let's take a look at the remaining distributions of daily quantities:","43ec68ba":"Let's count the number of numeric chars in and the length of the stockcode:","35b43f9b":"As you can see by the log-transformed distribution it would make sense to make a cut at:","82c2b241":"## 2. Get familiar with the data <a class=\"anchor\" id=\"intro\"><\/a>","83ff59ea":"Let's focus transactions with prices that fall into this range as we don't want to make predictions for very seldom products with high prices. Starting easy is always good!","306e45f5":"Again, we have strange occurences: zero unit prices!","8ae75319":"### Creating product types","1c327d19":"It's strange that they differ from the others. Let's drop them:","56886945":"Which codes are most common?","f6da7f9b":"Even though the majority of samples has a stockcode that consists of 5 numeric chars, we can see that there are other occurences as well. The length can vary between 1 and 12 and there are stockcodes with no numeric chars at all!","69f0bde2":"* Take a look at the **weekday to understand this plot**: Low values (0 to 3) correspond to Monday, Tuesday, Wednesday and Thursday. These are days with high amount of product sales (high quantity target values). They are colored in blue and **push towards higher sharp values and consequently to higher predicted quantity values**. Higher weekday values suite to friday, saturday and sunday. They are colored in red and **push towards negative sharp values and to lower predicted values**. This confirms to the observations we made during exploration of weekday and the sum of daily quantities.    \n* The **StockCode and the Description** are important features but they are also **very complex**. We have seen that we have close to 4000 different stock codes and even more descriptions. To improve we should **try to engineer features that are able to descripe the products in a more general way**. ","6547335e":"## Baseline for product types","46d3dfa4":"And which ones are most common?","73056275":"How much entries have we lost?","d8885680":"This value is high, but as already mentioned the RSME is influenced by outliers and we should take a look at the distribution of individual absolute errors:","b41a286f":"## Warehouse optimization\n\nWithin this kernel we will analyse sales data of an UK online retailer. As storage area may be expensive and fast delivery on time is important to prevail over the competition we like to help the retailer by predicting daily amounts of sold products. ","691654e1":"## Feature engineering"}}