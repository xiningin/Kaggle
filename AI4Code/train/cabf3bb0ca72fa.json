{"cell_type":{"accf7ab2":"code","ac89b979":"code","c1b5b1c1":"code","2fd78106":"code","069b6c5d":"code","fc7f6473":"code","4c243501":"code","33d1f9b0":"code","088bb82b":"code","d8788f69":"code","11dfaa07":"code","b39d157d":"markdown","64a3d626":"markdown","fbd225b4":"markdown","09b81655":"markdown","1abe88a3":"markdown","fa45dddb":"markdown","4ca2ce8f":"markdown","06f951eb":"markdown","6fad8f55":"markdown","2062b99e":"markdown","baab38b3":"markdown"},"source":{"accf7ab2":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression","ac89b979":"train = pd.read_csv(\"..\/input\/train.csv\") #Load train data (Write train.csv directory)\ntest = pd.read_csv(\"..\/input\/test.csv\") #Load test data (Write test.csv directory)\n\ndata = train.append(test,sort=False) #Make train set and test set in the same data set\n\ndata #Visualize the DataFrame data","c1b5b1c1":"#Plot features with more than 1000 NULL values\n\nfeatures = []\nnullValues = []\nfor i in data:\n    if (data.isna().sum()[i])>1000 and i!='SalePrice':\n        features.append(i)\n        nullValues.append(data.isna().sum()[i])\ny_pos = np.arange(len(features)) \nplt.bar(y_pos, nullValues, align='center', alpha=0.5)\nplt.xticks(y_pos, features)\nplt.ylabel('NULL Values')\nplt.xlabel('Features')\nplt.title('Features with more than 1000 NULL values')\nplt.show()","2fd78106":"#Dealing with NULL values\n\ndata = data.dropna(axis=1, how='any', thresh = 1000) #Drop columns that contain more than 1000 NULL values\ndata = data.fillna(data.mean()) #Replace NULL values with mean values","069b6c5d":"#Dealing with NULL values\n\ndata = pd.get_dummies(data) #Convert string values to integer values","fc7f6473":"#Drop features that are correlated to each other\n\ncovarianceMatrix = data.corr()\nlistOfFeatures = [i for i in covarianceMatrix]\nsetOfDroppedFeatures = set() \nfor i in range(len(listOfFeatures)) :\n    for j in range(i+1,len(listOfFeatures)): #Avoid repetitions \n        feature1=listOfFeatures[i]\n        feature2=listOfFeatures[j]\n        if abs(covarianceMatrix[feature1][feature2]) > 0.8: #If the correlation between the features is > 0.8\n            setOfDroppedFeatures.add(feature1) #Add one of them to the set\n#I tried different values of threshold and 0.8 was the one that gave the best results\n\ndata = data.drop(setOfDroppedFeatures, axis=1)","4c243501":"#Drop features that are not correlated with output\n\nnonCorrelatedWithOutput = [column for column in data if abs(data[column].corr(data[\"SalePrice\"])) < 0.05]\n#I tried different values of threshold and 0.045 was the one that gave the best results\n\ndata = data.drop(nonCorrelatedWithOutput, axis=1)","33d1f9b0":"#Plot one of the features with outliers\n\nplt.plot(data['LotArea'], data['SalePrice'], 'bo')\nplt.axvline(x=75000, color='r')\nplt.ylabel('SalePrice')\nplt.xlabel('LotArea')\nplt.title('SalePrice in function of LotArea')\nplt.show()","088bb82b":"#First, we need to seperate the data (Because removing outliers \u21d4 removing rows, and we don't want to remove rows from test set)\n\nnewTrain = data.iloc[:1460]\nnewTest = data.iloc[1460:]\n\n#Second, we will define a function that returns outlier values using percentile() method\n\ndef outliers_iqr(ys):\n    quartile_1, quartile_3 = np.percentile(ys, [25, 75]) #Get 1st and 3rd quartiles (25% -> 75% of data will be kept)\n    iqr = quartile_3 - quartile_1\n    lower_bound = quartile_1 - (iqr * 1.5) #Get lower bound\n    upper_bound = quartile_3 + (iqr * 1.5) #Get upper bound\n    return np.where((ys > upper_bound) | (ys < lower_bound)) #Get outlier values\n\n#Third, we will drop the outlier values from the train set\n\ntrainWithoutOutliers = newTrain #We can't change train while running through it\n\nfor column in newTrain:\n    outlierValuesList = np.ndarray.tolist(outliers_iqr(newTrain[column])[0]) #outliers_iqr() returns an array\n    trainWithoutOutliers = newTrain.drop(outlierValuesList) #Drop outlier rows\n    \ntrainWithoutOutliers = newTrain","d8788f69":"X = trainWithoutOutliers.drop(\"SalePrice\", axis=1) #Remove SalePrice column\nY = np.log1p(trainWithoutOutliers[\"SalePrice\"]) #Get SalePrice column {log1p(x) = log(x+1)}\nreg = LinearRegression().fit(X, Y)","11dfaa07":"#Make prediction\n\nnewTest = newTest.drop(\"SalePrice\", axis=1) #Remove SalePrice column\npred = np.expm1(reg.predict(newTest)) #{expm1(x) = exp(x)-1}\n\n#Submit prediction\n\nsub = pd.DataFrame() #Create a new DataFrame for submission\nsub['Id'] = test['Id']\nsub['SalePrice'] = pred\nsub.to_csv(\"submission.csv\", index=False) #Convert DataFrame to .csv file\n\nsub #Visualize the DataFrame sub","b39d157d":"That is it with this competition of predicting sale prices. This was my first project in Data Analysis and I hope that you appreciate this work. I will write more guides of Kaggle's competitions in the future.\n\nNote that I reached the rank of 1197 with these basics of feature engineering and with a simple linear regression model.\n\n<img src='https:\/\/i.imgur.com\/PgilH0n.png'>","64a3d626":"## 2. Clean the data\n\nCleaning the data is the most important step in data analysis. It is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. (Wikipedia)\n\n### 2.1 Dealing with NULL values\n\nOne of the things that can be noticed after visualizing the data is the existence of 'NaN' (Not a Number) values a.k.a NULL values, i.e. undefined or unrepresentable value.\n\nIt is not possible to work with a data set having NULL values, that is why we need to deal with them, either by giving them some defined values, or just removing them.\n\nSome of the features contain NULL values more than non-NULL values. These features are considered 'useless' and won't contribute much to the prediction later on. We will remove them then using the 'dropna' function.\n\nFor the rest of the features that contain some NULL values, we will just replace the latters with the mean of the values for each feature using 'fillna' and 'mean' functions. \n\nExample: If house X has NULL value for feature Y, we will calculate the mean of Y for all houses, and replace the NULL value with the calculated mean.","fbd225b4":"## 3. Train the data\n\nNow, everything is ready! We will apply linear regression on our train set thanks to LinearRegression() model imported from Scikit-learn. We will apply logarithmic function on the output to make the distribution more standard, i.e. more smooth in function of the features.\n\nAs you can see, it is short and simple. The most important part was cleaning the data and getting the right values of hyperparameters and features.","09b81655":"### 2.3 Dealing with correlations\n\n\nCorrelation is a statistical measure that describes the association between random variables. It is one of the most widely used statistical concepts. In our case, we can view correlation from two different points of view:\n\n- Correlation of features with each other\n- Correlation of features with output\n\nIn the first case, we will tend to drop features that are correlated to each other, since a high correlation between features indicates that the features bring up almost the same information, so it will be a waste of time and a waste of resources to consider such features.\n\nIn the second case, we will tend to drop features that are not correlated to the output, since a low correlation between a feature and the output indicates that such feature is not really associatied with output and doesn't contribute much to the final result.","1abe88a3":"### 2.2 Dealing with string values\n\nOn one hand, if you haven't noticed already, some features present string values. On the other hand, linear regression and calculations that we make on the data can only be done on numerical values. Therefore, having string values is a problem that we have to deal with. \n\nFortunately, Pandas library provide 'get_dummies' method that transforms n features containing string values into m features, m being greater than n.\n\nExample: If feature X has \"A\", \"B\" and \"C\" possible string values. The method 'get_dummies' will generate 3 features X_A, X_B and X_C having binary values. If a house Y used to have value \"A\" for feature X, house Y will now have 1 for X_A and 0 for X_B and X_C. As simple as that.\n\n<table>\n    <tr>\n        <td> <div> Id <\/div> <\/td>\n        <td> <div> FeatureX <\/div> <\/td>\n    <\/tr>\n    <tr>\n        <td> <div> 1 <\/div> <\/td>\n        <td> <div> \"A\" <\/div> <\/td>\n    <\/tr>\n    <tr>\n        <td> <div> 2 <\/div> <\/td>\n        <td> <div> \"B\" <\/div> <\/td>\n    <\/tr>\n    <tr>\n        <td> <div> 3 <\/div> <\/td>\n        <td> <div> \"C\" <\/div> <\/td>\n    <\/tr>\n<\/table>\n\nbecomes\n\n<table>\n    <tr>\n        <td> <div> Id <\/div> <\/td>\n        <td> <div> FeatureX_A <\/div> <\/td>\n        <td> <div> FeatureX_B <\/div> <\/td>\n        <td> <div> FeatureX_C <\/div> <\/td>\n    <\/tr>\n    <tr>\n        <td> <div> 1 <\/div> <\/td>\n        <td> <div> 1 <\/div> <\/td>\n        <td> <div> 0 <\/div> <\/td>\n        <td> <div> 0 <\/div> <\/td>\n    <\/tr>\n    <tr>\n        <td> <div> 2 <\/div> <\/td>\n        <td> <div> 0 <\/div> <\/td>\n        <td> <div> 1 <\/div> <\/td>\n        <td> <div> 0 <\/div> <\/td>\n    <\/tr>\n    <tr>\n        <td> <div> 3 <\/div> <\/td>\n        <td> <div> 0 <\/div> <\/td>\n        <td> <div> 0 <\/div> <\/td>\n        <td> <div> 1 <\/div> <\/td>\n    <\/tr>\n<\/table>","fa45dddb":"# House Prices: Advanced Regression Techniques - Full Guide\n\nThe goal of this project is to apply what we have learned in the module \"Data Analysis\" to build a regression model in the competition \"House Prices: Advanced Regression Techniques\" in Kaggle ( https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques ). \n\nThe main objective of the competition is to predict sales prices and practice feature engineering, RFs, and gradient boosting.\n\nThis project was carried out as part of the training of engineers at Tunisia Polytechnic School under the module \"Data Analysis\" taught in second year.\n\n<img src='https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/5407\/logos\/front_page.png'>\n\n\n**Necessary Skills:**\n- Python programming language\n- Machine Learning basics\n- Feature engineering\n\n**Instructions:**\n0. Import libraries\n1. Load the data \n2. Clean the data\n3. Train the data\n4. Make & Submit prediction\n\nLet's get started!","4ca2ce8f":"## 4. Make & Submit prediction\n\nWe finally reached the end. Now our model is trained and is ready to make prediction on test set. We will use predict() function imported from Scikit-learn too. We will create a Pandas DataFrame containing the Id and the correspending SalePrice of each house in the test set. \n\nThe last step consists of converting the DataFrame into a .csv file and uploading it on Kaggle by clicking \"Submit Predictions\" and choosing the .csv file.\n\n<img src='https:\/\/i.imgur.com\/wjneHXc.png'>\n\nNote that we used exponential function to cancel the effect of logarithmic while training the data.","06f951eb":"As you can seen, some features present outlier values that need to be removed in order to create a more accurate model.","6fad8f55":"## 1. Load the data\n\nThat is it with python libraries. We will now load the data to be able to manipulate it and train it later on. \n\nTo be able to realize this step, we need to follow these instructions:\n\n1. Go to https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\n2. Click on 'Data', situated next to 'Overview'\n3. Click on 'Download All', situated below. You can find description of the data and the different features\n \n<img src='https:\/\/i.imgur.com\/tcY98rh.png'>\n\n4. Extract the RAR file and check the data manually using Microsoft Excel or any spreadsheet program\n5. Load the data to the program using this command\n\nNote that we will put train set and test set in the same data set, so that we can make the same manipulations to the features.\n\nPS: The data will be of type 'DataFrame', a table of data, i.e. a single set of formatted two-dimensional data, columns being the features and rows being examples.","2062b99e":"### 2.4 Dealing with outliers\n\nWhat is an outlier? In statistics, an outlier is an observation point that is distant from other observations. An outlier may be due to variability in the measurement or it may indicate experimental error; the latter are sometimes excluded from the data set. (Wikipedia)\n\nIn short, an outlier is a data point that is significantly different from other data points in a data set. It can cause serious problems in statistical analysis. And in the end, detecting and handling outliers is often a somewhat subjective exercise.\n\nWe will use the 'percentile' numpy method in order to get the first quartile and the third quartile. We will later on calculate the iqr a.k.a the interquartile range to remove all the values that are greater than the upper bound or smaller than the lower bound.","baab38b3":"## 0. Import libraries\n\nIn order to be able to manipulate the data, we need to import the necessary python libraries:\n- Numpy: This library offers mathematical functions to operatore on arrays. We will need it in order to manipulate multi-dimensional arrays and matrices.\n- Pandas: This library offers offers data structures and operations for manipulating numerical tables and time series. We will need it in order to manipulate and analyse data. \n- Matplotlib: This library offers an object-oriented API for embedding plots into applications. We will need it in order to visualize some data graphs and functions.\n- Scikit-learn: This library offers various machine learning algorithms. We will need it in order to train the data using a linear regression model"}}