{"cell_type":{"4b62ffc2":"code","8aab0cd5":"code","2f4df4e1":"code","ccc0875e":"code","669c939e":"code","6df4a876":"code","76a0f669":"code","686aca58":"code","2329b3e9":"code","39674d3b":"code","2889f19f":"code","d22ba82e":"code","79f334dc":"code","90129ede":"code","a10cbaa4":"code","e0b68b67":"code","280a7032":"code","46cc6d0e":"code","b2c5e123":"markdown","cc73cf7e":"markdown","011c6e69":"markdown","5d676833":"markdown","d2f968a4":"markdown","386021f1":"markdown","cd72cadb":"markdown","d7315c92":"markdown","e748ab72":"markdown","e17f2d40":"markdown","2d04af8d":"markdown","4c29d9bf":"markdown","3843f006":"markdown","65c29bab":"markdown","611923a3":"markdown","fc369863":"markdown"},"source":{"4b62ffc2":"import numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nseed=1234\nnp.random.seed(seed)\ntf.random.set_seed(seed)\n%config IPCompleter.use_jedi = False","8aab0cd5":"# We will initialize a few Variables here\n\nx = tf.Variable(3.0)\ny = tf.Variable(4.0)\n\nprint(f\"Variable x: {x}\")\nprint(f\"Is x trainable?: {x.trainable}\")\nprint(f\"\\nVariable y: {y}\")\nprint(f\"Is y trainable?: {y.trainable}\")","2f4df4e1":"# Remember we need to execute the operations inside the context\n# of GradientTape so that we can record them\n\nwith tf.GradientTape() as tape:\n    z = x * y\n    \ndx, dy = tape.gradient(z, [x, y])\n\nprint(f\"Input Variable x: {x.numpy()}\")\nprint(f\"Input Variable y: {y.numpy()}\")\nprint(f\"Output z: {z}\\n\")\n\n# dz \/ dx\nprint(f\"Gradient of z wrt x: {dx}\")\n\n# dz \/ dy\nprint(f\"Gradient of z wrt y: {dy}\")","ccc0875e":"with tf.GradientTape() as tape:\n    z = x * y\n\ntry:\n    dx = tape.gradient(z, x)\n    dy = tape.gradient(z, y)\n\n    print(f\"Gradient of z wrt x: {dx}\")\n    print(f\"Gradient of z wrt y: {dy}\")\nexcept Exception as ex:\n    print(\"ERROR! ERROR! ERROR!\\n\")\n    print(type(ex).__name__, ex)","669c939e":"# Set the persistent argument\nwith tf.GradientTape(persistent=True) as tape:\n    z = x * y\n\ntry:\n    dx = tape.gradient(z, x)\n    dy = tape.gradient(z, y)\n\n    print(f\"Gradient of z wrt x: {dx}\")\n    print(f\"Gradient of z wrt y: {dy}\")\nexcept Exception as ex:\n    print(\"ERROR! ERROR! ERROR!\\n\")\n    print(type(ex).__name__, ex)","6df4a876":"# What if one of the Variables is non-trainable?\n# Let's make y non-trainable in the above example and run\n# the computation again\n\nx = tf.Variable(3.0)\ny = tf.Variable(4.0, trainable=False)\n\nwith tf.GradientTape() as tape:\n    z = x * y\n    \ndx, dy = tape.gradient(z, [x, y])\n\nprint(f\"Variable x: {x}\")\nprint(f\"Is x trainable?: {x.trainable}\")\nprint(f\"\\nVariable y: {y}\")\nprint(f\"Is y trainable?: {y.trainable}\\n\")\n\nprint(f\"Gradient of z wrt x: {dx}\")\nprint(f\"Gradient of z wrt y: {dy}\")","76a0f669":"# Note the dtypes\n\nx = tf.Variable(3.0, dtype=tf.float32)\ny = tf.Variable(4, dtype=tf.int32)\n\nwith tf.GradientTape() as tape:\n    z = x * tf.cast(y, x.dtype)\n    \ndx, dy = tape.gradient(z, [x, y])\n\nprint(f\"Input Variable x: {x}\")\nprint(f\"Input Variable y: {y}\")\nprint(f\"Output z: {z}\\n\")\n\nprint(f\"Gradient of z wrt x: {dx}\")\nprint(f\"Gradient of z wrt y: {dy}\")","686aca58":"# There is no gradient flow defined for int and string types\n\nx = tf.Variable(3, dtype=tf.int32)\ny = tf.Variable(4, dtype=tf.int32)\n\nwith tf.GradientTape() as tape:\n    z = x * y\n    \ndx, dy = tape.gradient(z, [x, y])\n\nprint(f\"Input Variable x: {x}\")\nprint(f\"Input Variable y: {y}\")\nprint(f\"Output z: {z}\\n\")\n\nprint(f\"Gradient of z wrt x: {dx}\")\nprint(f\"Gradient of z wrt y: {dy}\")","2329b3e9":"x = tf.Variable(3.0, name=\"x\")\ny = tf.Variable(4.0, name=\"y\")\nt = tf.Variable(tf.random.normal(shape=(2, 2)), name=\"t\")\n\nwith tf.GradientTape() as tape:\n    z = x * y\n\nprint(\"Tape is watching all of these:\")\nfor var in tape.watched_variables():\n    print(f\"{var.name} and it's value is {var.numpy()}\")","39674d3b":"# The ugly way\n\nx = tf.Variable(3.0, name=\"x\")\ny = tf.Variable(4.0, name=\"y\")\n\nwith tf.GradientTape(persistent=True) as tape:\n    z = x * y\n    \n    # Stop the grasdient flow\n    with tape.stop_recording():\n        zz = x*x + y*y\n\ndz_dx, dz_dy = tape.gradient(z, [x, y])\ndzz_dx, dzz_dy = tape.gradient(zz, [x, y])\n\nprint(f\"Gradient of z wrt x: {dz_dx}\")\nprint(f\"Gradient of z wrt y: {dz_dy}\\n\")\nprint(f\"Gradient of zz wrt x: {dzz_dx}\")\nprint(f\"Gradient of zz wrt y: {dzz_dy}\")","2889f19f":"# The better way!\n\nx = tf.Variable(3.0, name=\"x\")\ny = tf.Variable(4.0, name=\"y\")\n\nwith tf.GradientTape() as tape:\n    z = x * tf.stop_gradient(y)\n\ndz_dx, dz_dy = tape.gradient(z, [x, y])\nprint(f\"Gradient of z wrt x: {dz_dx}\")\nprint(f\"Gradient of z wrt y: {dz_dy}\")","d22ba82e":"# Both variables are trainable\nx = tf.Variable(3.0, name=\"x\")\ny = tf.Variable(4.0, name=\"y\")\n\n# Telling the tape: Hey! I will tell you what to record.\n# Don't start recording automatically!\nwith tf.GradientTape(watch_accessed_variables=False) as tape:\n    # Watch x but not y\n    tape.watch(x)\n    z = x * y\n\ndz_dx, dz_dy = tape.gradient(z, [x, y])\nprint(f\"Gradient of z wrt x: {dz_dx}\")\nprint(f\"Gradient of z wrt y: {dz_dy}\")","79f334dc":"# What if something that you wanted to watch,\n# wasn't present in the computation done inside \n# the context?\n\nx = tf.Variable(3.0, name=\"x\")\ny = tf.Variable(4.0, name=\"y\")\nt = tf.Variable(5.0, name=\"t\")\n\n# Telling the tape: Hey! I will tell you what to record.\n# Don't start recording automatically!\nwith tf.GradientTape(watch_accessed_variables=False) as tape:\n    # Watch x but not y\n    tape.watch(x)\n    z = x * y\n    \n    # `t` isn't involved in any computation here\n    # but what if we want to record it as well\n    tape.watch(t)\n\nprint(\"Tape watching only these objects that you asked it to watch\")\nfor var in tape.watched_variables():\n    print(f\"{var.name} and it's value is {var.numpy()}\")","90129ede":"x = tf.Variable(3.0, name=\"x\")\ny = tf.Variable(4.0, name=\"y\")\n\nwith tf.GradientTape() as tape_for_x, tf.GradientTape() as tape_for_y:\n    # Watching different variables with different tapes\n    tape_for_x.watch(x)\n    tape_for_y.watch(y)\n    \n    z = x * y\n\ndz_dx = tape_for_x.gradient(z, x)\ndz_dy = tape_for_y.gradient(z, y)\nprint(f\"Gradient of z wrt x: {dz_dx}\")\nprint(f\"Gradient of z wrt y: {dz_dy}\")","a10cbaa4":"x = tf.Variable(3.0, name=\"x\")\n\nwith tf.GradientTape() as tape1:\n    with tf.GradientTape() as tape0:\n        y = x * x * x\n    first_order_grad = tape0.gradient(y, x)\nsecond_order_grad = tape1.gradient(first_order_grad, x)\n\nprint(f\"Variable x: {x.numpy()}\")\nprint(\"\\nEquation is y = x^3\")\nprint(f\"First Order Gradient wrt x (3 * x^2): {first_order_grad}\")\nprint(f\"Second Order Gradient wrt x (6^x): {second_order_grad}\")","e0b68b67":"# What happens when you tries to take gradient wrt a Tensor?\nx = tf.constant(3.0)\n\nwith tf.GradientTape() as tape:\n    y = x * x\n    \ndy_dx = tape.gradient(y, x)\n\nprint(x)\nprint(\"\\nGradient of y wrt x: \", dy_dx)","280a7032":"# Let's modify the above code a bit\n\nx = tf.constant(3.0)\n\nwith tf.GradientTape() as tape:\n    tape.watch(x)\n    y = x * x\n    \ndy_dx = tape.gradient(y, x)\n\nprint(x)\nprint(\"\\nGradient of y wrt x: \", dy_dx)","46cc6d0e":"x = tf.Variable(3.0)\ny = tf.Variable(4.0)\n\nwith tf.GradientTape() as tape:\n    # Change the state of x by making x = x + y\n    x.assign_add(y)\n    \n    # Let's do some computation e.g z = x * x \n    # This is equivalent to z = (x + y) * (x + y) because of above assign_add\n    z = x * x\n    \ndy = tape.gradient(z, y)\nprint(\"Gradients of z wrt y: \", dy)","b2c5e123":"A better way to stop gradient flow is to use `tf.stop_gradient(...)`. Why?\n1. Doesn't require access to tape\n2. Clean with much better semantics ","cc73cf7e":"#### States and Gradients\n\n`GradientTape` can only read from the current state, not from the history that ead to it. State blocks gradient calculations from going farther back. Let's look at an example to make it more clear","011c6e69":"That's it for part 3! I hope you liked the content and I am also hoping that it would have given you a much clear picture of Automatic Differentiation and Gradients calculation. We will be looking at other things in the next tutorial!<br>\n\n\n**References**:\n1. https:\/\/www.tensorflow.org\/guide\/autodiff\n2. https:\/\/keras.io\/getting_started\/intro_to_keras_for_researchers\/","5d676833":"#### Gotchas\n\nLet's look at a few things that you **should** be aware of so that your code doesn't fail silently!\n\nWe already looked at that gradients for `int` or `string` dtypes isn't defined. Here are a few other things","d2f968a4":"## Automatic Differentiation and Gradients\n\nLet's say you apply a sequence of operations on an input in a *forward* pass. To differentiate automatically, you need some sort of mechanism to \nfigure out:\n1. What operations were applied in the forward pass?\n2. What was the order in which the operations were applied?\n\nFor autodiff, you need to remember the above two. Different frameworks can implement the same idea in different ways but the fundamentals remain the same.\n\n\n### Gradients in TensorFlow\n\nTensorFlow provides the `tf.GradientTape` API for automatic differentiation. Any relevant operation executed inside the context of `GradientTape` gets recorded for gradients computation. To compute gradients, you need to do the following:\n\n1. Record operations inside the `tf.GradientTape` context\n2. Compute the gradients using `GradientTape.gradient(target, sources)`\n\nLet's code up a few examples for this.","386021f1":"**Update - 23rd Dec, 2021**\n\nWe have completed the TF-JAX tutorials series. 10 notebooks that covers every fundamental aspect of both TensorFlow and JAX. Here are the links to the notebooks along with the Github repo details:\n\n### TensorFlow Notebooks:\n\n* [TF_JAX_Tutorials - Part 1](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part1)\n* [TF_JAX_Tutorials - Part 2](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part2)\n* [TF_JAX_Tutorials - Part 3](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part3)\n\n### JAX Notebooks:\n\n* [TF_JAX_Tutorials - Part 4 (JAX and DeviceArray)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-4-jax-and-devicearray)\n* [TF_JAX_Tutorials - Part 5 (Pure Functions in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-5-pure-functions-in-jax\/)\n* [TF_JAX_Tutorials - Part 6 (PRNG in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-6-prng-in-jax\/)\n* [TF_JAX_Tutorials - Part 7 (JIT in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-7-jit-in-jax)\n* [TF_JAX_Tutorials - Part 8 (Vmap and Pmap)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-8-vmap-pmap)\n* [TF_JAX_Tutorials - Part 9 (Autodiff in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-9-autodiff-in-jax)\n* [TF_JAX_Tutorials - Part 10 (Pytrees in JAX)](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part-10-pytrees-in-jax)\n\n### Github Repo with all notebooks in one place\nhttps:\/\/github.com\/AakashKumarNain\/TF_JAX_tutorials\n\n---\n\n<img src=\"https:\/\/i.ytimg.com\/vi\/yjprpOoH5c8\/maxresdefault.jpg\" width=\"300\" height=\"300\" align=\"center\"\/>\n\nWelcome to another TensorFlow\/JAX tutorial. This is the third tutorial in this series. If you haven't looked at the previous tutorials,\nI would highly recommend checking them out.\n\n1. [TF-JAX Tutorials - Part 1](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part1)\n2. [TF-JAX Tutorials - Part 2](https:\/\/www.kaggle.com\/aakashnain\/tf-jax-tutorials-part2)\n\n**Note** The tutorials are in the following format:\n1. TF Fundamentals (2-3 notebooks)\n2. JAX Fundamentals (2-3 notebooks)\n3. Advanced TF (2-3 notebooks)\n4. Advanced JAX (2-3 notebooks)\n\n\nToday we will be taking a deep dive into a very important topic: **`Gradients`**\n\n`Automatic Differentiation` and `Gradients` are so important concepts that they deserve a few dedicated chapters. Understanding every bit of it isn't necessary but the more you dive into it, the more you will appreciate the beauty of it. I am planning to do an `advanced` tutorial on these topics if there is enough interest from the readers. Do let me know in the comments section what you think","cd72cadb":"We will do a simple operation here: **`z = x * y`**. And we will calculate the gradients of `z` wrt `x` and `y` . We are taking a simple example so that the readers can easily verify that things are working as expected. We will work on complex examples in a bit","d7315c92":"#### 2. Stopping the gradients","e748ab72":"You would say that your code is correct, which in some sense is, but you will get `None` as the gradient value for the variables where the data types of the source and the target are different. Before we move to a very important concept, let's summarize all the things we have learned so far:\n\n1. `tf.GradientTape` is the API for doing AD in TensorFlow\n2. For computing gradients using Tape, we need:\n   * Record the relevant operations inside the context of Tape\n   * Compute the gradients by calling the `GradientTape.gradient(...)` method\n3. If you wish to call the `gradient(...)` method multiple times, make sure to set the `persistent` argument to `GradientTape`\n4. If any `non-trainable` variable is involved in the computation, then the gradient wrt to that variable would be `None`\n5. Mixing `dtype` topology is a blunder. Your code will run but will fail silently!\n\n\n### Fine-gain control\n\nA few questions that comes to mind naturally after seeing the above examples:\n\n1. How to access all the objects that are being watched?\n2. How to stop the flow of a gradient through a specific Variable\/path?\n3. What if you don't want to watch all the variables inside the `GradientTape` context?\n4. What if you want to watch something that isn't inside the context?\n\nWe will take a few examples for each of the above case to understand it in a better way.\n\n#### 1. Accessing all the watched objects","e17f2d40":"#### Higher order gradients\n\nAny computation done nside the `GradientTape` context gets recorded. If the computations involves gradient calculations, it gets recorded as well. This makes it easy to compute the `higher-order` gradients using the same API. Check this out","2d04af8d":"Woah! What just happened? Don't look further down but pause for a minute and think for a while about what just happened and why such a behavior","4c29d9bf":"**What just happened?**<br>\nAs soon as the `GradientTape.gradient(...)` is called, all the resources held by a `GradientTape` are released. So, if you computed the `gradient` once, then you won't be able to call it again.\n\n**What's the solution then?**<br>\nThe solution is to use set the `persistent` argument to `True`. This allows multiple calls to the gradient() method as resources are released when the tape object is garbage collected. Let's try the above example again","3843f006":"#### Multiple Tapes\nYou can use more than one `GradientTape` for recording different objects. Tapes interact seamlessly","65c29bab":"#### 3. Select what to watch inside the context?\n\nBy default `GradientTape` will automatically watch any trainable variables that are accessed inside the context but if you want gradients for selected variables only, then you can disable automatic tracking by passing `watch_accessed_variables=False` to the tape constructor","611923a3":"Easy enough! Similarly, you can calculate gradients of many many variables wrt to some computation say `loss` by just passing all the trainable variables involved in that computation in a nested way (can be a list or dictionary for example). The returned gradients will follow the same nested structure in which the inputs are passed to the tape.\n\nWhat happens if we calculate the gradients in the above code wrt x and y separately?","fc369863":"**Note:** An important point to remember is that you should never mix the `topology` of the `dtypes` for AD and computing the graidents. When I say `topology`, I mean don't mix `float`, `int`, `string` types. In fact, you can't take a gradient for any op that has a dtype of `int` or `string`. Let us take an example to make this clear"}}