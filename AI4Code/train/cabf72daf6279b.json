{"cell_type":{"53287208":"code","2c8bd500":"code","f69388d4":"code","82d86810":"code","3fd734a3":"code","964324da":"code","3c75468a":"code","74b9b820":"code","375c8b19":"code","12f7d695":"code","34073fda":"code","e9a0ac30":"code","731aeb70":"code","355a4a70":"code","d337ae61":"code","a8c09b43":"code","d2e48d33":"code","db7c9989":"code","ecfdf59c":"code","c986f6b8":"code","cb391457":"code","ac00b7d4":"code","54eee392":"code","0074ee17":"code","8bdfa4ca":"code","15a2eb74":"code","1949e402":"code","ebbed406":"code","3abbef45":"code","605bbecb":"code","b0abc4b0":"code","c5c9a051":"code","606df2e5":"code","6802baec":"code","9de44847":"code","1e4ce1bf":"code","f3d491fa":"code","a4085baa":"code","ac4b8b1e":"code","a763db95":"code","e1480c5b":"code","03f0f131":"markdown","3e4f9225":"markdown","0ef29f8b":"markdown","bc161858":"markdown"},"source":{"53287208":"import sys\nsys.path.append('..\/input\/iterativestratification')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold","2c8bd500":"import warnings\nwarnings.filterwarnings(\"ignore\")","f69388d4":"import numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M\nimport tensorflow.keras.backend as K\n\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\nfrom tensorflow.keras.losses import BinaryCrossentropy\nimport tensorflow_addons as tfa\n\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import log_loss\nfrom sklearn import preprocessing\n\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\nfrom tqdm.notebook import tqdm\n\nimport math","82d86810":"train_features = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n\ndata = train_features.append(test_features)\n\nss = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')","3fd734a3":"import random, os, torch\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \nseed_everything(seed=42)","964324da":"import seaborn as sns\nimport matplotlib.pyplot as plt\nz= np.random.randint(0, 100, size=10)\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(18, 8))\nsns.distplot(train_features.iloc[:, z], bins=30, color='red', label='Test')\nsns.distplot(test_features.iloc[:, z], bins=30, color='green', label='Train')\nplt.legend()\nplt.title('Train \/ Test Distribution for z Features Before Featuring Eng.')\nplt.xlabel('z Features')\nplt.ylabel('Frequency')\nplt.show()","3c75468a":"def scaling_ss(train, test):\n    features = train.columns[4:]\n    scaler = preprocessing.StandardScaler()\n    scaler.fit(pd.concat([train[features], test[features]], axis = 0))\n    train[features] = scaler.transform(train[features])\n    test[features] = scaler.transform(test[features])\n    return train, test, features\n\n#train_features, test_features, features = scaling_ss(train_features, test_features)","74b9b820":"def scaling_mm(train, test):\n    features = train.columns[2:]\n    scaler = preprocessing.MinMaxScaler()\n    scaler.fit(pd.concat([train[features], test[features]], axis = 0))\n    train[features] = scaler.transform(train[features])\n    test[features] = scaler.transform(test[features])\n    return train, test, features\n\n#train_features, test_features, features = scaling_mm(train_features, test_features)","375c8b19":"def scaling_rs(train, test):\n    features = train.columns[4:]\n    scaler = preprocessing.RobustScaler()\n    scaler.fit(pd.concat([train[features], test[features]], axis = 0))\n    train[features] = scaler.transform(train[features])\n    test[features] = scaler.transform(test[features])\n    return train, test, features\n\ntrain_features, test_features, features = scaling_rs(train_features, test_features)","12f7d695":"GENES = [col for col in train_features.columns if col.startswith('g-')]\nCELLS = [col for col in train_features.columns if col.startswith('c-')]","34073fda":"#RankGauss\nfor col in (GENES + CELLS):\n    transformer = QuantileTransformer(n_quantiles=206,random_state=0, output_distribution=\"normal\")\n    vec_len = len(train_features[col].values)\n    vec_len_test = len(test_features[col].values)\n    raw_vec = train_features[col].values.reshape(vec_len, 1)\n    transformer.fit(raw_vec)\n\n    train_features[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n    test_features[col] = transformer.transform(test_features[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]","e9a0ac30":"# GENES PCA\nn_comp = 600  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","731aeb70":"#CELLS PCA\nn_comp = 60  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (PCA(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'pca_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'pca_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","355a4a70":"# GENES SVD\nn_comp = 450  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[GENES]), pd.DataFrame(test_features[GENES])])\ndata2 = (TruncatedSVD(n_components=n_comp, random_state=42).fit_transform(data[GENES]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'svd_G-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'svd_G-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(GENES))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","d337ae61":"#CELLS SVD\nn_comp = 45  #<--Update\n\ndata = pd.concat([pd.DataFrame(train_features[CELLS]), pd.DataFrame(test_features[CELLS])])\ndata2 = (TruncatedSVD(n_components=n_comp, random_state=42).fit_transform(data[CELLS]))\ntrain2 = data2[:train_features.shape[0]]; test2 = data2[-test_features.shape[0]:]\n\ntrain2 = pd.DataFrame(train2, columns=[f'svd_C-{i}' for i in range(n_comp)])\ntest2 = pd.DataFrame(test2, columns=[f'svd_C-{i}' for i in range(n_comp)])\n\n# drop_cols = [f'c-{i}' for i in range(n_comp,len(CELLS))]\ntrain_features = pd.concat((train_features, train2), axis=1)\ntest_features = pd.concat((test_features, test2), axis=1)","a8c09b43":"def c_squared(train, test):\n    \n    features_c = list(train.columns[776:876])\n    for df in [train, test]:\n        for feature in features_c:\n            df[f'{feature}_squared'] = df[feature] ** 2\n    return train, test\n\ntrain_features,test_features=c_squared(train_features,test_features)","d2e48d33":"def c_cubed(train, test):\n    \n    features_c = list(train.columns[776:876])\n    for df in [train, test]:\n        for feature in features_c:\n            df[f'{feature}_cubed'] = df[feature] ** 3\n    return train, test\n\ntrain_features,test_features=c_cubed(train_features,test_features)","db7c9989":"def c_sqrt(train, test):\n    \n    features_c = list(train.columns[776:876])\n    for df in [train, test]:\n        for feature in features_c:\n            df[f'{feature}_sqrt'] = df[feature] ** 0.5\n    return train, test\n\ntrain_features,test_features=c_cubed(train_features,test_features)","ecfdf59c":"def scaling_rs(train, test):\n    features = train.columns[4:]\n    scaler = preprocessing.RobustScaler()\n    scaler.fit(pd.concat([train[features], test[features]], axis = 0))\n    train[features] = scaler.transform(train[features])\n    test[features] = scaler.transform(test[features])\n    return train, test, features\n\n#train_features, test_features, features = scaling_rs(train_features, test_features)","c986f6b8":"print(f'New Train\/Test Features Dataset Contains [{train_features.shape[1]}] Features.')","cb391457":"train_features","ac00b7d4":"threshold = 0.9\nvar_thresh = VarianceThreshold(threshold)\ndata = train_features.append(test_features)\ndata_transformed = var_thresh.fit_transform(data.iloc[:, 4:])\n\ntrain_features_transformed = data_transformed[ : train_features.shape[0]]\ntest_features_transformed = data_transformed[-test_features.shape[0] : ]\n\ntrain_features = pd.DataFrame(train_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                              columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntrain_features = pd.concat([train_features, pd.DataFrame(train_features_transformed)], axis=1)\n\n\ntest_features = pd.DataFrame(test_features[['sig_id','cp_type','cp_time','cp_dose']].values.reshape(-1, 4),\\\n                             columns=['sig_id','cp_type','cp_time','cp_dose'])\n\ntest_features = pd.concat([test_features, pd.DataFrame(test_features_transformed)], axis=1)","54eee392":"print(f'Variance Threshold Select [{train_features.shape[1]}] Features From [1836]]')","0074ee17":"train = train_features.copy()\ntarget = train_targets.copy()\ntest = test_features.copy()\n\ntarget = target[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntarget.drop(['sig_id'], axis=1, inplace=True)\n\ntrain = train[train['cp_type']!='ctl_vehicle'].reset_index(drop=True)\ntrain.drop(['sig_id', 'cp_type'], axis=1, inplace=True)\n\ntest.drop(['sig_id', 'cp_type'], axis=1, inplace=True)","8bdfa4ca":"train, test, features = scaling_mm(train, test)","15a2eb74":"def preprocess(df):\n    df = df.copy()\n    df.loc[:, 'cp_dose'] = df.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n    #df.loc[:, 'cp_time'] = df.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})    \n    df = pd.get_dummies(df, columns=['cp_time','cp_dose'])\n    return df\n\ntrain = preprocess(train)\ntest = preprocess(test)\ndata = train.append(test)","1949e402":"import seaborn as sns\nimport matplotlib.pyplot as plt\nz= np.random.randint(0, 100, size=10)\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(18, 8))\nsns.distplot(test.iloc[:, z], bins=30, color='red', label='Test')\nsns.distplot(train.iloc[:, z], bins=30, color='green', label='Train')\nplt.legend()\nplt.title('Train \/ Test Distribution for z Features After Featuring Eng.')\nplt.xlabel('z Features')\nplt.ylabel('Frequency')\nplt.show()","ebbed406":"train","3abbef45":"train.describe()","605bbecb":"np.mean(train.values), np.std(train.values), np.min(train.values), np.max(train.values)","b0abc4b0":"somthing_rate = 1e-3\nP_MIN = somthing_rate\nP_MAX = 1 - P_MIN\n\ndef loss_fn(yt, yp):\n    yp = np.clip(yp, P_MIN, P_MAX)\n    return log_loss(yt, yp, labels=[0,1])\n\nNUM_FEATURES = train.shape[1]\nNUM_FEATURES","c5c9a051":"def create_model(num_columns, hidden_layers=1500, SEED=None):\n    model = tf.keras.Sequential([tf.keras.layers.Input(num_columns)])\n    #initializer = tf.keras.initializers.VarianceScaling(scale=2., mode='fan_in', distribution='truncated_normal', seed=SEED)#math.sqrt(6. \/ n) \n    initializer = tf.keras.initializers.TruncatedNormal(mean=0.5, stddev=1., seed=SEED) \n\n    model.add(tf.keras.layers.BatchNormalization())\n    #model.add(tf.keras.layers.Dropout(0.4))\n    model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(hidden_layers, kernel_initializer=initializer)))\n    #model.add(tf.keras.layers.Activation('elu'))\n    model.add(tf.keras.layers.LeakyReLU())\n    \n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.2654321))\n    model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(hidden_layers, kernel_initializer=initializer)))\n    #model.add(tf.keras.layers.Activation('elu'))\n    model.add(tf.keras.layers.LeakyReLU())\n\n    #============ Final Layer =================\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(0.2678923456789))\n    model.add(tfa.layers.WeightNormalization(tf.keras.layers.Dense(206, kernel_initializer=initializer)))\n    model.add(tf.keras.layers.Activation('sigmoid'))\n    \n    tfa_opt = tfa.optimizers.Lookahead(tfa.optimizers.AdamW(lr = 1e-2, weight_decay = 1e-5), sync_period=10)\n    tf_opt = tfa.optimizers.Lookahead(tf.keras.optimizers.Adam(learning_rate=1e-3, epsilon=1e-15), sync_period=10)\n    \n    model.compile(optimizer=tfa_opt, \n                  loss=BinaryCrossentropy(),\n                  metrics=BinaryCrossentropy(label_smoothing=somthing_rate)\n                  )\n    return model","606df2e5":"# Use All feats as top feats\ntop_feats = [i for i in range(train.shape[1])]\nprint(\"Top feats length:\",len(top_feats))","6802baec":"mod = create_model(len(top_feats))\nmod.summary()","9de44847":"def metric(y_true, y_pred):\n    metrics = []\n    for _target in train_targets.columns:\n        metrics.append(loss_fn(y_true.loc[:, _target], y_pred.loc[:, _target].astype(float)))\n    return np.mean(metrics)","1e4ce1bf":"N_STARTS = 5\n\ntrain_targets = target\n\nres = train_targets.copy()\nss.loc[:, train_targets.columns] = 0\nres.loc[:, train_targets.columns] = 0\n\nhistorys = dict()\n\n#tf.random.set_seed(42)\nseed_everything(seed=42)\nfor seed in range(N_STARTS):\n    for n, (tr, te) in enumerate(MultilabelStratifiedKFold(n_splits=7, random_state=seed, shuffle=True).split(train_targets, train_targets)):\n        print(f\"======{train_targets.values[tr].shape}========{train_targets.values[te].shape}=====\")\n        print(f'Seed: {seed} => Fold: {n}')\n        \n        checkpoint_path = f'repeat:{seed}_Fold:{n}.hdf5'\n        reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.5, min_lr=1e-20, patience=6, verbose=1, mode='min')\n        cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 1, save_best_only = True,\n                                     save_weights_only = True, mode = 'min')\n        early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", restore_best_weights=True, patience= 14, verbose = 1)\n        \n        model = create_model(len(top_feats), SEED=seed)\n        \n        history = model.fit(train.values[tr][:, top_feats],\n                  train_targets.values[tr],\n                  validation_data=(train.values[te][:, top_feats], train_targets.values[te]),\n                  epochs=100, batch_size=128,\n                  callbacks=[reduce_lr_loss, cb_checkpt, early], verbose=2\n                 )\n        historys[f'history_seed_{seed+1}_fold_{n+1}'] = history\n        print(\"Model History Saved.\")\n        \n        model.load_weights(checkpoint_path)\n        test_predict = model.predict(test.values[:, top_feats])\n        val_predict = model.predict(train.values[te][:, top_feats])\n        \n        ss.loc[:, train_targets.columns] += test_predict\n        res.loc[te, train_targets.columns] += val_predict\n        \n        print(f'OOF Metric For SEED {seed} => FOLD {n} : {metric(train_targets.loc[te, train_targets.columns], pd.DataFrame(val_predict, columns=train_targets.columns))}')\n        print('+-' * 10)","f3d491fa":"# Show Model loss in plots\nfor k,v in historys.items():\n    loss = []\n    val_loss = []\n    loss.append(v.history['loss'][:35])\n    val_loss.append(v.history['val_loss'][:35])\n    \n# Show Model loss in plots\nfor k,v in historys.items():\n    bin_loss = []\n    bin_val_loss = []\n    bin_loss.append(v.history['binary_crossentropy'][:35])\n    bin_val_loss.append(v.history['val_binary_crossentropy'][:35])\n    \nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(1, 3, figsize = (23, 6))\n\nax[0].plot(np.mean(bin_loss, axis=0), 'b', label='Smoothing Loss')\nax[0].plot(np.mean(bin_val_loss, axis=0), 'r--', label='Smoothing Val Loss')\nax[0].set(title=f'{somthing_rate}-Somthing Model', yscale='log', yticks=[1,1e-1,1e-2], xlabel='Epoches', ylabel='Average Logloss')\nax[0].legend()\n\nax[1].plot(np.mean(loss, axis=0), 'b', label='Non-Smoothing Loss')\nax[1].plot(np.mean(val_loss, axis=0), 'g--',label='Non-Smoothing Val Loss')\nax[1].set(title='Non-Somthing Model', yscale='log', yticks=[1,1e-1,1e-2], xlabel='Epoches', ylabel='Average Logloss')\nax[1].legend()\n\n\nax[2].plot(np.mean(bin_val_loss, axis=0), 'r+', label='Smoothing Val Loss')\nax[2].plot(np.mean(val_loss, axis=0), 'g*',label='Non-Smoothing Val Loss')\nax[2].set(title='Somthing vs Non-Somthing Model', yscale='log', xlabel='Epoches', ylabel='Average Logloss')\nax[2].legend()","a4085baa":"ss.loc[:, train_targets.columns] \/= ((n+1) * N_STARTS)\nres.loc[:, train_targets.columns] \/= N_STARTS","ac4b8b1e":"print(f'OOF Metric: {metric(train_targets, res)}')","a763db95":"np.save('oof_keras', res)\nnp.save('pred_keras', ss)\n\nss.to_csv('submission_test.csv', index=False)","e1480c5b":"ss.to_csv('submission.csv', index=False)","03f0f131":"## Smoothing vs Non-Smoothing","3e4f9225":"## Model Based on: <a href='https:\/\/www.kaggle.com\/elcaiseri\/moa-keras-multilabel-classifier-nn-starter'>MoA | Keras Multilabel Classifier NN | Starter <\/a> Kernel.\n\n\n### What is new in this Kernel?\n 1. Features Engineering, and it contains:\n- 3 SKlearn preprocessing scaler\n- Apply Rank Gauss.\n- PCA\n- SVD <== NEW\n\n 2. Feature Selection:\n- VarianceThreshold\n\n 3. Clean Data:\n- Mapping Data\n- drop train['cp_type'] column\n\n 4. Model:\n- using LeakyReLU rather than 'relu'\n- Add model smoothing\n \n* Initialize Dense Layers with \"VarianceScaling\" \/ \"TruncatedNormal\" ==> ' https:\/\/keras.io\/api\/layers\/initializers\/ '\n* Monitor the loss without smoothing as well and Plot the results. (From @imeintanis comment on V5)\n\n<hr><h4>Pls <span style='color:red'>UPVOTE<\/span>, if you find it useful. Feedbacks is also very much appreciated.<h4>","0ef29f8b":"Kernel still under modification.. **<span style='color:red'>Feedbacks<\/span>** is also very much appreciated.\nPls **<span style='color:red'>UPVOTE<\/span>**, if you find it useful. \n","bc161858":"<center><h2 style='color:red'>MoA | Keras [NewBaseLine] with Features Engineering<br>Smoothing Vs Non-Smoothing<\/h2><\/center><hr>"}}