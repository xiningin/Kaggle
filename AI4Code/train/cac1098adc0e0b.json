{"cell_type":{"9ec72e0d":"code","39ac95d1":"code","76ee9a70":"code","3c3ef55e":"code","d672dccf":"code","00b714d9":"code","078a77ff":"code","cf1dafdf":"code","7b091254":"code","9c8c9e4d":"code","70b8d77c":"code","6aba05ec":"code","36f33869":"code","07a31817":"code","26c810da":"code","b8f17e3a":"code","f1a3a9ad":"code","907c95b8":"code","347cb9b4":"code","3c8757c7":"code","6393db8b":"code","0f5082bf":"code","f63874eb":"code","6283c45b":"code","f9b4da8f":"code","59439265":"code","0a2b1a8d":"code","f40b3b6c":"code","d5ab1d5a":"code","97a8507a":"code","c4ab8a92":"code","d08040d2":"code","56246f42":"code","a98c1209":"code","69a65cff":"code","36c12de7":"code","9c76d8e2":"code","455f934b":"code","e091dddf":"code","a2a812ee":"code","a7b12383":"code","9a591a53":"code","6e72e3be":"markdown","cc4b1f04":"markdown","fa6c7d8d":"markdown","8446104c":"markdown","9d8ca36d":"markdown","5c352dbb":"markdown","0c1fe022":"markdown","4cea68ef":"markdown","a919c8ca":"markdown","51bfcee9":"markdown","407aa1d4":"markdown","1b42258d":"markdown"},"source":{"9ec72e0d":"# import library \nimport  pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom numpy import isnan\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder,StandardScaler\n\nfrom sklearn.model_selection import  train_test_split\n\nfrom sklearn.svm import LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neural_network import MLPClassifier\n\n\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix","39ac95d1":"def plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n    target_names: given classification classes such as [0, 1, 2]\n                  the class names, for example: ['high', 'medium', 'low']\n\n    title:        the text to display at the top of the matrix\n\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                  plt.get_cmap('jet') or plt.cm.Blues\n\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import itertools\n\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","76ee9a70":"# Load data\ntrain= pd.read_csv('..\/input\/home-credit-default-risk\/application_train.csv')\ntest= pd.read_csv('..\/input\/home-credit-default-risk\/application_test.csv')\n","3c3ef55e":"print('shape train data',train.shape)\nprint('shape test data',test.shape)","d672dccf":"missing_value_train = train.isnull().mean()\nplt.subplots(figsize=(20,15))\nplt.xticks(rotation=90)\nplt.plot(missing_value_train.sort_values() )\n","00b714d9":"df_train=train.drop(['TARGET'],axis=1)\ndf_test =test.copy()","078a77ff":"train","cf1dafdf":"\n# filling the nan for df_tain set\nmissing_value_train = df_train.isnull().mean()\n\nfor col in list(missing_value_train.index):\n    if missing_value_train[col]<0.5:\n        if df_train[col].dtype == 'object':\n            df_train[col].fillna(df_train[col].value_counts().index[0], inplace=True)\n        else:\n            df_train[col].fillna(df_train[col].mean(), inplace=True)\n    else:\n        if df_train[col].dtype == 'object':\n            df_train[col].fillna('missing', inplace=True)\n    \n# filling the nan for df_test set\nmissing_value_test = test.isnull().mean()\n\nfor col in list(missing_value_train.index):\n    if missing_value_train[col]<0.5:\n        if df_test[col].dtype == 'object':\n            df_test[col].fillna(df_test[col].value_counts().index[0], inplace=True)\n        else:\n            df_test[col].fillna(df_test[col].mean(), inplace=True)\n    else:\n        if df_test[col].dtype == 'object':\n            df_test[col].fillna('missing', inplace=True)\n    \n\n","7b091254":"missing_value_train2 = df_train.isnull().mean()\nplt.subplots(figsize=(20,15))\nplt.xticks(rotation=90)\nplt.plot(missing_value_train2.sort_values())","9c8c9e4d":"\nnan_values = df_train.isnull().sum()\npourcentage= pd.DataFrame(np.array(nan_values) ,index=nan_values.index)\npour=pourcentage.loc[pourcentage[0]>0.5]\na=df_train[pour.index]\na[~isnan(a)] = 0\n# mark all nan as 1\na[isnan(a)] = 1\ndf_train[pour.index]=a","70b8d77c":"missing_value_train3 = df_train.isnull().mean()\nplt.subplots(figsize=(20,15))\nplt.xticks(rotation=90)\nplt.plot(missing_value_train3.sort_values())","6aba05ec":"label_encoders = {}\nfor col in list(df_train.columns):\n    if df_train[col].dtype == 'object':\n        label_encoders[col] = LabelEncoder()\n        df_train[col]=label_encoders[col].fit_transform(df_train[col])\n        df_test[col]=label_encoders[col].transform(df_test[col])","36f33869":"df_train.columns","07a31817":"a['TARGET']=train['TARGET']","26c810da":"a","b8f17e3a":"# get correlation matrix \ncorrolation=a.corr()\ncorrolation","f1a3a9ad":"corrolation= a.corr()\nmask = np.triu(np.ones_like(corrolation, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nplt.subplots(figsize=(20,15))\n\nsns.heatmap(corrolation, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True)","907c95b8":"df_train.drop(pour.index,axis=1,inplace=True)\ndf_test.drop(pour.index,axis=1,inplace=True)","347cb9b4":"df_train['TARGET']=train['TARGET']\ntarget_coo = df_train.corr()['TARGET']\ntarget_coo","3c8757c7":"plt.subplots(figsize=(20,15))\nplt.xticks(rotation=90)\nplt.plot(target_coo.sort_values())\n\n","6393db8b":"feauture_todrop= target_coo[(target_coo>-0.05)& (target_coo<0.05)]\ndf_train.drop(feauture_todrop.index,axis=1,inplace=True)\ndf_test.drop(feauture_todrop.index,axis=1,inplace=True)","0f5082bf":"after_drop = df_train.corr()['TARGET']\nplt.subplots(figsize=(20,15))\nplt.xticks(rotation=90)\nplt.plot(after_drop.sort_values())\n","f63874eb":"mask = np.triu(np.ones_like(df_train.corr(), dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nplt.subplots(figsize=(20,15))\n\nsns.heatmap(df_train.corr(), mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True)","6283c45b":"df_train['TARGET'].value_counts()","f9b4da8f":"plt.bar((0,1),height=df_train['TARGET'].value_counts())","59439265":"X=df_train.drop(['TARGET'],axis=1).values\ny= df_train.TARGET.values","0a2b1a8d":"X.shape","f40b3b6c":"from imblearn.under_sampling import RandomUnderSampler\nrus = RandomUnderSampler(random_state=0)\nX,y = rus.fit_resample(X,y)","d5ab1d5a":"X.shape","97a8507a":"from imblearn.over_sampling import SMOTE\noversample = SMOTE()\nX,y = oversample.fit_resample(X,y)","c4ab8a92":"scaler = StandardScaler()\nX = scaler.fit_transform(X)","d08040d2":"X_train , X_test ,y_train, y_test= train_test_split(X,y,test_size=0.3)\n","56246f42":"# Spot Check Algorithms\nmodels = []\nmodels.append(('RFC', RandomForestClassifier()))\nmodels.append(('KNC', KNeighborsClassifier( )))\nmodels.append(('DTC', DecisionTreeClassifier( )))\nmodels.append(('GBC', GradientBoostingClassifier( )))\n","a98c1209":"# evaluate each model in turn\nresults = []\nnames = []\nfor name, model in models:\n    kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')\n    results.append(cv_results)\n    names.append(name)\n    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))","69a65cff":"# Compare Algorithms\nfrom matplotlib import pyplot\n\npyplot.boxplot(results, labels=names)\npyplot.title('Algorithm Comparison')\npyplot.show()","36c12de7":"\ngbc = GradientBoostingClassifier()\ngbc.fit(X_train, y_train)\npred= gbc.predict(X_test)","9c76d8e2":"cm=confusion_matrix(y_test,pred)\nplot_confusion_matrix(cm,[0,1])\n","455f934b":"t=scaler.transform(df_test)","e091dddf":"test_predict =gbc.predict(t)","a2a812ee":"test_predict","a7b12383":"test['target']= test_predict","9a591a53":"plt.bar((0,1),test['target'].value_counts())","6e72e3be":"what I have learned :\n- imbalanced data may cause the model to overfit on one target and ignore other \n- balancing the data by oversampling cause overfit on the target generated \n-the best idea is to undersampling the higher target ","cc4b1f04":"this project is to predict how capable each applicant is of repaying a loan from the Kaggle website [Home Credit Default Risk\n](https:\/\/www.kaggle.com\/c\/home-credit-default-risk\/overview)\n","fa6c7d8d":"After exploring the course of correlation with target we will drop the columns that have less 0.05 correlation with target ","8446104c":"# Home Credit Default Risk\n","9d8ca36d":"# dealing with missing data","5c352dbb":"for columns that contain less than 50% missing data we will use imputation for  object columns we will fill with most frequent and other with the mean \nfor columns of object type with more then 50% nan values will fill it with 'missing'","0c1fe022":"first, we will drop the columns that contains more than 50% nan values and before w drop them we will check if these features are correlated with the target does missing values have meaning","4cea68ef":"After transforming the columns of more than 50% of nan values into  binary columns turns out there no correlation between these feature and the target so we will delete them","a919c8ca":"Now we will label encoding every categorical features in the data","51bfcee9":"as we see in next plot there's no nan value left","407aa1d4":"in this part, we will transform the features of more than 50% of missing data and into a labeled column \nif not nan value then take 0 \nif nan value then take 1","1b42258d":"Since the data is not balanced we will use oversample to balanced"}}