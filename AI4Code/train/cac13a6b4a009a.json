{"cell_type":{"b0029a54":"code","f90def21":"code","fd20c156":"code","1ac57058":"code","a2ba8f87":"code","24983f30":"code","86b05fd4":"code","67fea8e8":"code","5d169116":"code","b12b2017":"code","16fd174d":"code","3da162c6":"code","855313e0":"code","213b9082":"code","11fec545":"code","63d84dec":"code","c0b64ad1":"code","92e7dc89":"code","22edf8be":"code","5d8e6cda":"code","f5aea8ce":"code","4b43b21b":"code","ede8d8ab":"code","cfca8586":"code","612fc763":"code","ab502a83":"code","7c8b3a39":"code","2009b24f":"code","071cec94":"code","aca6cb26":"code","8997774e":"code","108aa715":"code","a336125a":"code","21fc0a32":"code","aaf8e266":"code","a906ef07":"code","8a92b66e":"code","617da8f0":"code","61859329":"code","d7efdbef":"code","0d05cb07":"code","b6846d47":"code","bb2964f8":"code","70e7e28c":"code","60091e17":"code","f4da523d":"code","9eb43d9e":"code","9e47e34b":"code","cb03aa44":"code","73a9801e":"code","569ba4f7":"code","7dcab148":"code","5c905c79":"code","a879210c":"code","68aae32b":"code","7b77da4e":"code","7b941282":"code","a7c0b400":"code","2d39569a":"code","e8236741":"markdown","443d2ddd":"markdown","b08a70cf":"markdown","4c401c4a":"markdown","24efa6ff":"markdown","0e25c2bc":"markdown","7f1bde54":"markdown","370c0086":"markdown","2ea003fc":"markdown","435ad824":"markdown","0b2672b3":"markdown","fbf437e3":"markdown","e6077763":"markdown","531889b0":"markdown","404a0137":"markdown","078ddc8c":"markdown","d6c65cb1":"markdown","1c9bd4a8":"markdown","12f3b7b1":"markdown","41d3059d":"markdown","34f9ca87":"markdown","9ebd3750":"markdown","2b31dca6":"markdown","0a5b5dd9":"markdown","41937857":"markdown","2cbdbe65":"markdown","b429363b":"markdown","d1f91256":"markdown","39c79c59":"markdown","02cd8697":"markdown","ae79a92d":"markdown","0d25535f":"markdown","4ee5548c":"markdown","b204e431":"markdown","fd8a8574":"markdown","b8d30569":"markdown","b65c3230":"markdown","c25ca522":"markdown","31390a13":"markdown","924afc6b":"markdown","54517755":"markdown","1f1f1834":"markdown","9f532c9f":"markdown"},"source":{"b0029a54":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f90def21":"data=pd.read_csv('..\/input\/bank-marketing-dataset\/bank.csv')","fd20c156":"data.info()","1ac57058":"data.columns","a2ba8f87":"data.head(10)","24983f30":"data.describe()","86b05fd4":"import matplotlib.pyplot as plt","67fea8e8":"def bar_plot(variable):\n    var =data[variable]\n    varValue = var.value_counts()\n    plt.figure(figsize=(15,3))\n    plt.bar(varValue.index, varValue,color=['#00008b','#00e5ee','#cd1076', '#008080','#cd5555','red','blue',])\n    plt.xticks(varValue.index, varValue.index.values)\n    plt.ylabel(\"Frequency\")\n    plt.title(variable)\n    \n    plt.show()\n    print(\"{}: \\n {}\".format(variable,varValue))","5d169116":"categoryc = [\"job\",\"marital\",\"education\", \"housing\", \"loan\",\"contact\",\"poutcome\",\"month\",\"deposit\"]\nfor c in categoryc:\n    bar_plot(c)\n    ","b12b2017":"def plot_hist(variable):\n    plt.figure(figsize=(9,6))\n    plt.hist(data[variable], bins=40,color='#cd1076')\n    plt.xlabel(variable)\n    plt.ylabel(\"frequency\")\n    plt.title(\"{} distrubition with hist\".format(variable))\n    plt.show()","16fd174d":"numericVar = [\"age\",\"campaign\",\"duration\"]\nfor n in numericVar:\n    plot_hist(n)","3da162c6":"pd.crosstab(data.age,data.deposit).plot(kind=\"area\",figsize=(15,7),color=['#0000ff','#000000' ])\nplt.title('Age Distribution')\nplt.xlabel('Age')\nplt.ylabel('Frequency')\nplt.show()","855313e0":"pd.crosstab(data.job,data.deposit).plot(kind=\"barh\",figsize=(15,7),color=['#0000ff','#000000'])\nplt.title('Deposit Age Distribution')\nplt.xlabel('Frequency')\nplt.ylabel('Job')\nplt.show()","213b9082":"from collections import Counter\ndef detect_outliers(data,features):\n    outlier_indices = []\n    for c in features:\n        # 1st quartile\n        Q1 = np.percentile(data[c],25)\n        # 3rd quartile\n        Q3 = np.percentile(data[c],75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indeces\n        outlier_list_col = data[(data[c] < Q1 - outlier_step) | (data[c] > Q3 + outlier_step)].index\n        # store indeces\n        outlier_indices.extend(outlier_list_col)\n    \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","11fec545":"data.loc[detect_outliers(data,['age',\n                               'day','duration','campaign','previous'])]\n","63d84dec":"data = data.drop([3945], axis=0)","c0b64ad1":"data.isnull().sum()","92e7dc89":"import seaborn as sns\nfig, ax = plt.subplots(figsize=(13,13))         # Sample figsize in inches\nsns.heatmap(data.corr(), annot=True, linewidths=.5, ax=ax)","22edf8be":"data=data.drop(['duration'],axis=1)","5d8e6cda":"data.head()","f5aea8ce":"columns=data.select_dtypes(include=[object]).columns\ndata=pd.concat([data,pd.get_dummies(data[columns])],axis=1)\ndata=data.drop(['job','marital','education','default','housing','loan','contact','month','day','poutcome'],axis=1)\ndata.info()\ndata.head()","4b43b21b":"def pdayswork(pdays):\n    if(pdays == -1):\n        return(0)\n    elif(pdays >= 0):\n        return(1)\ndata['pdays2'] = data['pdays'].apply(pdayswork)","ede8d8ab":"data=data.drop(['deposit_no', 'deposit_yes'],axis=1)","cfca8586":"def deposit1(deposit):\n    if(deposit=='yes'):\n        return(1)\n    elif(deposit=='no'):\n        return(0)\ndata['depositNew'] = data['deposit'].apply(deposit1)","612fc763":"data=data.drop(['deposit'],axis=1)","ab502a83":"data.head()","7c8b3a39":"data.info()","2009b24f":"from sklearn.preprocessing import StandardScaler\nX = data.iloc[:, 0:50]\nY = data.iloc[:, 50]\nnd = StandardScaler()\nnd.fit(X)\nX =nd.transform(X)\nprint(X)","071cec94":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.metrics import f1_score\nX = data.iloc[:, 0:50]\nY = data.iloc[:, 50]\nX_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.2, random_state = 100)\n\naccuracies = {}\nkappaScores= {}\nf1scores={}","aca6cb26":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression(random_state=101,multi_class='ovr',solver='liblinear',class_weight='balanced',C=0.2)\nlr.fit(X_train,y_train)\nprediction = lr.predict(X_test)","8997774e":"print(classification_report(y_test,prediction))\nacc = accuracy_score(y_test,prediction)*100\nprint(\"Logistic Regression accuracy:\",acc)\naccuracies['Logistic Regression']=acc\n\nf1=f1_score(y_test,prediction)*100\nprint(\"F1-Score: \",f1)\nf1scores['Logistic Regression']=f1\n\n\ncohen_kappa = cohen_kappa_score(y_test, prediction)*100\nprint('Cohen Kappa score: ',cohen_kappa)\nkappaScores['Logistic Regression']=cohen_kappa","108aa715":"score=round(accuracy_score(y_test,prediction),3)\ncm= confusion_matrix\ncm1=cm(y_test,prediction)\nsns.heatmap(cm1, annot=True,fmt=\".1f\",linewidths=3,square=True, cmap='PuBu',color=\"#cd1076\")\nplt.ylabel('actual label')\nplt.xlabel('predicted label')\nplt.title('accuracy score: {0}'.format(score),size=12)\nplt.show()","a336125a":"from sklearn.ensemble import RandomForestClassifier","21fc0a32":"clf = RandomForestClassifier(n_estimators=100, max_depth=12,\n                             random_state=50)\n\nclf.fit(X_train,y_train)\n\nprediction = clf.predict(X_test)","aaf8e266":"acc = accuracy_score(y_test,prediction)*100\nprint(\"Random Forest accuracy:\",acc)\naccuracies['Random Forest']=acc\n\nf1=f1_score(y_test,prediction)*100\nprint(\"F1-Score: \",f1)\nf1scores['Random Forest']=f1\n\ncohen_kappa = cohen_kappa_score(y_test, prediction)*100\nprint('Cohen Kappa score: ',cohen_kappa)\nkappaScores['Random Forest']=cohen_kappa","a906ef07":"from sklearn.naive_bayes import GaussianNB","8a92b66e":"nb=GaussianNB()\nnb.fit(X_train,y_train)\nnaiveb=nb.predict(X_test)\nprediction= nb.predict(X_test)","617da8f0":"acc = accuracy_score(y_test,prediction)*100\nprint(\"Naive Bayes accuracy:\",acc)\naccuracies['Naive Bayes']=acc\n\nf1=f1_score(y_test,prediction)*100\nprint(\"F1-Score: \",f1)\nf1scores['Naive Bayes']=f1\n\ncohen_kappa = cohen_kappa_score(y_test, prediction)*100\nprint('Cohen Kappa score: ',cohen_kappa)\nkappaScores['Naive Bayes']=cohen_kappa","61859329":"from sklearn.linear_model import SGDClassifier","d7efdbef":"sgd=SGDClassifier(loss='modified_huber',shuffle=True,random_state=100,penalty='l1',alpha=0.004\n                  ,max_iter=100,eta0=0.2,learning_rate='optimal')\nsgd.fit(X_train,y_train)\nprediction=sgd.predict(X_test)","0d05cb07":"acc = accuracy_score(y_test,prediction)*100\nprint(\"SGD Classifier accuracy:\",acc)\naccuracies['SGDC']=acc\n\nf1=f1_score(y_test,prediction)*100\nprint(\"F1-Score: \",f1)\nf1scores['SGDC']=f1\n\ncohen_kappa = cohen_kappa_score(y_test, prediction)*100\nprint('Cohen Kappa score: ',cohen_kappa)\nkappaScores['SGDC']=cohen_kappa","b6846d47":"from sklearn.neighbors import KNeighborsClassifier","bb2964f8":"knn= KNeighborsClassifier(n_neighbors = 4,algorithm='ball_tree')\nknn.fit(X_train, y_train)\nprediction=knn.predict(X_test)","70e7e28c":"acc = accuracy_score(y_test,prediction)*100\nprint(\"Knn accuracy:\",acc)\naccuracies['KNN']=acc\n\nf1=f1_score(y_test,prediction)*100\nprint(\"F1-Score: \",f1)\nf1scores['KNN']=f1\n\ncohen_kappa = cohen_kappa_score(y_test, prediction)*100\nprint('Cohen Kappa score: ',cohen_kappa)\nkappaScores['KNN']=cohen_kappa","60091e17":"from sklearn.tree import DecisionTreeClassifier","f4da523d":"dtree= DecisionTreeClassifier(criterion='gini',max_depth=10,random_state=100,min_samples_leaf=10)\ndtree.fit(X_train, y_train)\nprediction=dtree.predict(X_test)","9eb43d9e":"acc = accuracy_score(y_test,prediction)*100\nprint(\"Decision Tree accuracy:\",acc)\naccuracies['Decision Tree']=acc\n\nf1=f1_score(y_test,prediction)*100\nprint(\"F1-Score: \",f1)\nf1scores['Decision Tree']=f1\n\ncohen_kappa = cohen_kappa_score(y_test, prediction)*100\nprint('Cohen Kappa score: ',cohen_kappa)\nkappaScores['Decision Tree']=cohen_kappa","9e47e34b":"from sklearn.linear_model import Perceptron","cb03aa44":"pr = Perceptron(alpha=0.07,max_iter=100, random_state=100,penalty='l1')\npr.fit(X_train, y_train)\nprediction = pr.predict(X_test)","73a9801e":"acc = accuracy_score(y_test, prediction)*100\nprint(\"Perceptron accuracy:\",acc)\naccuracies['Perceptron']=acc\n\nf1=f1_score(y_test,prediction)*100\nprint(\"F1-Score: \",f1)\nf1scores['Perceptron']=f1\n\ncohen_kappa = cohen_kappa_score(y_test, prediction)*100\nprint('Cohen Kappa score: ',cohen_kappa)\nkappaScores['Perceptron']=cohen_kappa","569ba4f7":"from sklearn.ensemble import GradientBoostingClassifier","7dcab148":"clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.8,\n    max_depth=2, random_state=0)\nclf.fit(X_train, y_train)\nprediction = clf.predict(X_test)","5c905c79":"acc = accuracy_score(y_test, prediction)*100\nprint(\"Gradient Boosting Classifier accuracy:\",acc)\naccuracies['Gradient Boosting']=acc\n\nf1=f1_score(y_test,prediction)*100\nprint(\"F1-Score: \",f1)\nf1scores['Gradient Boosted']=f1\n\ncohen_kappa = cohen_kappa_score(y_test, prediction)*100\nprint('Cohen Kappa score: ',cohen_kappa)\nkappaScores['Gradient Boosting']=cohen_kappa","a879210c":"from xgboost import XGBClassifier","68aae32b":"xgb =XGBClassifier(n_estimators=100, learning_rate=0.08, gamma=0, subsample=0.78,\n                           colsample_bytree=1, max_depth=7)\nxgb.fit(X_train,y_train)\nprediction = xgb.predict(X_test)","7b77da4e":"acc = accuracy_score(y_test, prediction)*100\nprint(\"Xgboost Classifier accuracy:\",acc)\naccuracies['Xgboost Classifier']=acc\n\nf1=f1_score(y_test,prediction)*100\nprint(\"F1 Score: \",f1)\nf1scores['Xgboost Classifier']=f1\n\ncohen_kappa = cohen_kappa_score(y_test, prediction)*100\nprint('Cohen Kappa score: ',cohen_kappa)\nkappaScores['Xgboost Classifier']=cohen_kappa","7b941282":"colors = [\"#00008b\", \"#00e5ee\", \"#cd1076\", \"#008080\",\"#cd5555\",'black']\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,3))\nplt.ylabel(\"Accuracy %\")\nplt.xlabel(\"\\n\\n Algorithms\")\nsns.barplot(x=list(accuracies.keys()), y=list(accuracies.values()), palette=colors)\nplt.show()","a7c0b400":"colors = [\"#00008b\", \"#00e5ee\", \"#cd1076\", \"#008080\",\"#cd5555\",'black']\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,3))\nplt.ylabel(\"Kappa Score %\")\nplt.xlabel(\"\\n\\n Algorithms\")\nsns.barplot(x=list(kappaScores.keys()), y=list(kappaScores.values()), palette=colors)\nplt.show()","2d39569a":"colors = [\"#00008b\", \"#00e5ee\", \"#cd1076\", \"#008080\",\"#cd5555\",'black']\n\nsns.set_style(\"whitegrid\")\nplt.figure(figsize=(16,5))\nplt.yticks(np.arange(0,100,3))\nplt.ylabel(\"F1 Score %\")\nplt.xlabel(\"\\n\\n Algorithms\")\nsns.barplot(x=list(f1scores.keys()), y=list(f1scores.values()), palette=colors)\nplt.show()","e8236741":"<a id = \"4\"><\/a><br>\n## Categorical Variable ","443d2ddd":" <a id = \"13\"><\/a><br>\n # Random Forest","b08a70cf":"<a id = \"12\"><\/a><br>\n# Logistic Regression ","4c401c4a":"<a id = \"10\"><\/a><br>\n# Data Normalization","24efa6ff":"<a id = \"14\"><\/a><br>\n# Naive Bayes","0e25c2bc":"<a id = \"16\"><\/a><br>\n# KNN","7f1bde54":" ### The F1 Score value shows us the harmonic mean of the Precision and Recall values.\n \n## The main reason for using the F1 Score value instead of Accuracy is not to make an incorrect model selection in non-uniform data sets.","370c0086":"<font color = 'red'>\nContent:\n    \n1. [Load and check data](#1)\n3. [Univariate Variable Analysis](#3)\n    *          [Categorical Variable ](#4)\n    *          [Numerical Variable ](#5)\n1. [Outlier Detection](#20)    \n1. [Missing Value](#21)     \n1. [Correlation matrix](#6)    \n1. [Data Manipulation](#7) \n    *           [One-Hot Encoding ](#8)\n    *           [Others](#9)\n    *           [Data Normalization](#10)\n                       \n1. [Algorithm Works](#222) \n    *           [Logistic Regression](#12)\n    *           [Random Forest](#13)\n    *           [Naive Bayes Classifier](#14)\n    *           [Stochastic Gradient Descent Classifier](#15) \n    *           [KNN](#16)  \n    *           [Decision Tree](#17) \n    *           [Neural Network-Perceptron](#18)\n    *           [Gradient Boosting Classifier](#22)\n    *           [Xgboost Classifier](#23)\n1. [Comparison of algorithms](#19) \n2. [Comparison of Kappa Scores](#24)\n3. [Comparison of F1 Scores](#25) \n    ","2ea003fc":"# the current state of our data set.","435ad824":"Logistic regression is a predictive linear model that aims to explain the relationship between a dependent binary variable and one or more independent variables. \nThe output of Logistic Regression is a number between  *0 and 1* which you can think about as being the probability that a given class is true or not.\n","0b2672b3":"### StandartScaler, normalizes the data with a standard deviation of 1 with an average of 0.\n\n**The target column is not normalized.**","fbf437e3":"<a id = \"5\"><\/a><br>\n## Numerical Variable","e6077763":"<a id = \"25\"><\/a><br>\n# Comparison of F1 Scores","531889b0":"<a id = \"17\"><\/a><br>\n# Decision Tree","404a0137":"<a id = \"3\"><\/a><br>\n# Univariate Variable Analysis\n* Categorical Variables:job,marital, default, education,housing,loan,contact,poutcome,mounth,deposit,day\n* Numerical Variables: age, campaign,duration, pdays,balance,previous","078ddc8c":"<a id = \"9\"><\/a><br>\n# Others..","d6c65cb1":"<a id = \"7\"><\/a><br>\n# Data Manipulation","1c9bd4a8":"### No missing value..","12f3b7b1":"\n1. Age \/ Age\n2. Job \/ Job\n3. Marital Status \/ Marital Status\n4. Education \/ Education Level\n5. Default \/ Having a previously broken credit\n6. Housing \/ home loan?\n7. Loan \/ Personal Loan?\n8. Contact \/ Was the customer contacted on his home or mobile phone?\n9. Month: Last month of contact\n10. Day: The day of the contacted.\n11. Duration: Talk time on last call\n12. Campaign: The number of contacts reaching the customer during the current campaign (including the last contact)\n13. Pdays: The number of days since the previous campaign, if reached (-1 if it was never reached before)\n14. Previous: The number of contacts that reached the customer before this campaign\n15. Poutcome: Previous campaign success, failure or failure","41d3059d":"<a id = \"21\"><\/a><br>\n# Missing Value","34f9ca87":"<a id = \"24\"><\/a><br>\n# Comparison of Kappa Scores","9ebd3750":" <a id = \"15\"><\/a><br>\n # Stochastic Gradient Descent Classifier","2b31dca6":"### Cohen's kappa, (7), symbolized by the lowercase Greek letter, is a powerful statistic useful for testing reliability. Similar to the correlation coefficients, between -1 and +1; where 0 represents the availability that can be expected from random chance, and 1 represents the perfect match between raters.\n\n* 0 indicates no information agreement\n* 0.01-0.20 **Slight agreement**\n* 0.21-0.40 **Fair agreement**\n* 0.41-0.60 **Moderate agreement**\n* 0.61-0.80 **Substantial agreement**\n* 0.81-1.00 **Almost perfect agreement**","0a5b5dd9":"<a id = \"8\"><\/a><br>\n# One-Hot Encoding ","41937857":"## Accuracy is a metric used to measure the success of a model but is not sufficient by itself.","2cbdbe65":"\n# Please do not forget to write down your positive or negative opinions.","b429363b":"<a id = \"1\"><\/a><br>\n# Load and check data","d1f91256":"\n### The number of people who are 25 to 40 years old with a time deposit account is high.","39c79c59":"### I do not include the *Duration column* in the dataset, as it is unknown data at the time of the prediction.\n**duration: Talk Time on Last Call**","02cd8697":"<a id = \"6\"><\/a><br>\n# Correlation matrix","ae79a92d":"## * Calculated correlation between two variables (r) gets a value between -1 and 1.\n* No correlaiton r=0\n* Very weak correlation: r<20\n* Weak correlation: between 0.20-0.49\n* Moderate correlation: between 0.5-0.79\n* Strong correlation: between 0.8-0.99\n* Perfect correlation: r=1 \n\n### Looking at it, there is a moderate correlation between the *days* and the *previous* ones. (r=0.51)","0d25535f":"<a id = \"19\"><\/a><br>\n# Comparison of accuracies","4ee5548c":"<a id = \"18\"><\/a><br>\n# Neural Network - Perceptron","b204e431":"<a id = \"20\"><\/a><br>\n# Outlier Detection","fd8a8574":"<a id = \"22\"><\/a><br>\n# Gradient Boosting Classifier","b8d30569":"### 2. For a single target column","b65c3230":"## 1. The ***pdays*** data indicates how many times the customer has been contacted before.\n\n### Updated as follows.\n\nif the **pdays = 0**, it indicates that it has not been contacted before\n\nif the **pdays = 1**, it indicates that it was contacted earlier","c25ca522":"<a id = \"23\"><\/a><br>\n# Xgboost Classifier","31390a13":"### **One Hot Encoding means that categorical variables are represented as binary.**","924afc6b":"### In people at the executive level  have more deposit accounts.","54517755":"<a id = \"222\"><\/a><br>\n# Algorithm Works","1f1f1834":"### I do not include this row","9f532c9f":"### In this way, our target column, whose data type is object, turned into numerical values. And new target column name is *depositNew*. Also as this is a classification problem, the target column can remain as an object. But I chose to convert it to int data type."}}