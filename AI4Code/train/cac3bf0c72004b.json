{"cell_type":{"bce59120":"code","ae2ba28f":"code","bf36c6c7":"code","95afc3db":"code","626809fe":"code","65fefe96":"code","978b8874":"code","9bb2074d":"code","fbefb6a7":"code","212ebb2a":"code","5e05c3ca":"code","962010e8":"code","886116bc":"code","d5825e9a":"code","eecb837e":"code","cf27363c":"code","76f3694c":"markdown","5d6b81dd":"markdown","6c8b1c30":"markdown","e07ff2f6":"markdown","bc0614e4":"markdown","b24145d8":"markdown","9d97603c":"markdown","1f753360":"markdown","ee893675":"markdown","02e4c43c":"markdown","dc72d385":"markdown","d8a2770f":"markdown"},"source":{"bce59120":"import matplotlib.pyplot as plt\nimport os\nimport inspect\nimport numpy as np\nimport pandas as pd","ae2ba28f":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\nfrom tensorflow.keras.models import Sequential,load_model,model_from_json\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import utils as kutils","bf36c6c7":"model = load_model(\"..\/input\/models-trained-on-docknet-data\/MobXDock3.hd5\")","95afc3db":"docnet_train_path = \"..\/input\/open-sprayer-images\/docknet\/Docknet\/train\"\ndocnet_valid_path = \"..\/input\/open-sprayer-images\/docknet\/Docknet\/valid\"","626809fe":"docks = [docnet_train_path + \"\/docks\/\" + fn for fn in os.listdir(docnet_train_path + \"\/docks\")]\ndocks += [docnet_valid_path + \"\/docks\/\" + fn for fn in os.listdir(docnet_valid_path + \"\/docks\")]\nnot_docks = [docnet_train_path + \"\/notdocks\/\" + fn for fn in os.listdir(docnet_train_path + \"\/notdocks\")]\nnot_docks += [docnet_valid_path + \"\/notdocks\/\" + fn for fn in os.listdir(docnet_valid_path + \"\/notdocks\")]","65fefe96":"dock_df = pd.DataFrame()\ndock_df['image_path'] = docks + not_docks\n","978b8874":"dock_df['weed'] = ['no' if nd else 'yes' for nd in dock_df['image_path'].str.contains('notdocks')]","9bb2074d":"not_docks = dock_df[dock_df['weed'].str.contains('no')]\ndocks = dock_df[dock_df['weed'].str.contains('yes')]\nprint(len(not_docks), len(docks))","fbefb6a7":"fit_these = not_docks.sample(len(docks))\nfit_these = fit_these.append(docks)\nlen(fit_these)","212ebb2a":"datagen=ImageDataGenerator(\n    rescale=1.\/255.,\n    width_shift_range = 0.1,\n    height_shift_range = 0.1,\n    rotation_range = 5,\n    shear_range = 5,\n    zoom_range = (0.90,1.10),\n    fill_mode = \"constant\",\n    cval = 0,\n    validation_split = 0.0\n    )","5e05c3ca":"for image_path in dock_df.sample(8).image_path:\n    ima = plt.imread(image_path)\n    txfm = datagen.get_random_transform(np.shape(ima))\n    imt = datagen.apply_transform(ima,txfm)\n    plt.subplot(121)\n    plt.imshow(ima)\n    plt.subplot(122)\n    plt.imshow(imt)\n    plt.show()\n    ","962010e8":"train_generator = datagen.flow_from_dataframe(\n    fit_these,\n    x_col = 'image_path',\n    y_col = 'weed',\n    target_size = (256,256),\n    color_mode=\"rgb\")","886116bc":"cd = train_generator.class_indices\nivd = {v: k for k, v in cd.items()}\ncddf = pd.DataFrame.from_dict(cd,orient='index')\ncd","d5825e9a":"history = model.fit_generator(train_generator, epochs=1,\n    use_multiprocessing = False,\n    verbose=1,shuffle=True\n    )","eecb837e":"#plt.plot(history.history['accuracy'])\n#plt.show()","cf27363c":"#model.save(\"MobXDockDemo.hd5\")","76f3694c":"Rather than build from a topless version of the base_model, I just transfer relevant weights.\n\nInspecting them shows the last two are for 1000 classes instead of my 2, so trim them:","5d6b81dd":"For anything with more classes, keep track of them. Write this dictionary along with the model. ","6c8b1c30":"from keras.applications.mobilenet import MobileNet","e07ff2f6":"base_model = MobileNet(input_shape=(224,224,3), \n                    alpha=1.0, \n                    depth_multiplier=1, \n                    dropout=1e-3, \n                    include_top=True, \n                    weights='imagenet', \n                    input_tensor= None, \n                    pooling=None)","bc0614e4":"Grab a sampling of the not_docks to get a ballanced set for fitting","b24145d8":"There was one oversized image in the not_docks directory. I have not excluded it here, \n(the keras pre-processor resizes it, I think),\nbut it ought to be.","9d97603c":"base_weights = base_model.get_weights()\nmy_weights = model.get_weights()\nbase_weights[-2] = base_weights[-2][:,:,:,0:2]\nbase_weights[-1] = my_weights[-1]","1f753360":"model.set_weights(base_weights)\nmodel.compile(optimizer='rmsprop',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","ee893675":"model = MobileNet(  input_shape=(256,256,3), \n                    alpha=1.0, \n                    depth_multiplier=1, \n                    dropout=1e-3, \n                    include_top=True, \n                    weights=None, #'imagenet', \n                    input_tensor= None, \n                    pooling=None, classes=2)","02e4c43c":"Fitting runs my 1080ti to almost 100%. I5 CPU isn't too stressed tho.\n\nSo 10 epochs took about 5 min.\n\nMobXDock3 trained on three 10 epoch runs with different samples of the non_dock data,\nto get an accuracy of 99, with no false-negatives.","dc72d385":"OpenSprayer \n Keras mobilnet model\n ","d8a2770f":"Checkout the ImageDataGenerator before using it! I had been using the brightness range,\n\nand getting just black images. Here I apply it to 8 samples to see what it does.\n\nI think a cropped version would be better. Maybe later."}}