{"cell_type":{"627732f3":"code","28a8191e":"code","1018eebd":"code","7573e978":"code","a2bdf6e6":"code","40449d31":"code","b7828df5":"code","1b43ff1d":"code","3fc45452":"code","94208d79":"code","32d488ae":"code","af08d355":"code","fbd6274e":"code","c507bc2c":"code","712bae05":"code","8c45b938":"markdown","038fef4e":"markdown","5f5acc6b":"markdown","a2e7eca6":"markdown","e5ebfb56":"markdown"},"source":{"627732f3":"# Asthetics\nimport warnings\nimport sklearn.exceptions\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\", category=sklearn.exceptions.UndefinedMetricWarning)\n\n# General\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nimport random\nimport gc\nimport glob\npd.set_option('display.max_columns', None)\nnp.seterr(divide='ignore', invalid='ignore')\ngc.enable()\n\n# Deep Learning\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim.lr_scheduler import OneCycleLR\n# NLP\nfrom transformers import AutoTokenizer, AutoModel\n\n# Random Seed Initialize\nRANDOM_SEED = 42\n\ndef seed_everything(seed=RANDOM_SEED):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything()\n\n# Device Optimization\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    \nprint(f'Using device: {device}')\n","28a8191e":"data_dir = '..\/input\/jigsaw-toxic-severity-rating'\nmodels_dir = '..\/input\/jrstc-models'\ntest_file_path = os.path.join(data_dir, 'comments_to_score.csv')\nprint(f'Train file: {test_file_path}')","1018eebd":"test_df = pd.read_csv(test_file_path)","7573e978":"def text_cleaning(text):\n    '''\n    Cleans text into a basic form for NLP. Operations include the following:-\n    1. Remove special charecters like &, #, etc\n    2. Removes extra spaces\n    3. Removes embedded URL links\n    4. Removes HTML tags\n    5. Removes emojis\n    \n    text - Text piece to be cleaned.\n    '''\n    template = re.compile(r'https?:\/\/\\S+|www\\.\\S+') #Removes website links\n    text = template.sub(r'', text)\n    \n    soup = BeautifulSoup(text, 'lxml') #Removes HTML tags\n    only_text = soup.get_text()\n    text = only_text\n    \n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'', text)\n    \n    text = re.sub(r\"[^a-zA-Z\\d]\", \" \", text) #Remove special Charecters\n    text = re.sub(' +', ' ', text) #Remove Extra Spaces\n    text = text.strip() # remove spaces at the beginning and at the end of string\n\n    return text\n","a2bdf6e6":"tqdm.pandas()\ntest_df['text'] = test_df['text'].progress_apply(text_cleaning)","40449d31":"test_df.sample(10)","b7828df5":"params = {\n    'device': device,\n    'debug': False,\n    'checkpoint': '..\/input\/roberta-base',\n    'output_logits': 768,\n    'max_len': 256,\n    'batch_size': 32,\n    'dropout': 0.2,\n    'num_workers': 2\n}","1b43ff1d":"if params['debug']:\n    train_df = train_df.sample(frac=0.01)\n    print('Reduced training Data Size for Debugging purposes')","3fc45452":"class BERTDataset:\n    def __init__(self, text, max_len=params['max_len'], checkpoint=params['checkpoint']):\n        self.text = text\n        self.max_len = max_len\n        self.checkpoint = checkpoint\n        self.tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        self.num_examples = len(self.text)\n\n    def __len__(self):\n        return self.num_examples\n\n    def __getitem__(self, idx):\n        text = str(self.text[idx])\n\n        tokenized_text = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            truncation=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_attention_mask=True,\n            return_token_type_ids=True,\n        )\n\n        ids = tokenized_text['input_ids']\n        mask = tokenized_text['attention_mask']\n        token_type_ids = tokenized_text['token_type_ids']\n\n        return {'ids': torch.tensor(ids, dtype=torch.long),\n                'mask': torch.tensor(mask, dtype=torch.long),\n                'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)}\n","94208d79":"class ToxicityModel(nn.Module):\n    def __init__(self, checkpoint=params['checkpoint'], params=params):\n        super(ToxicityModel, self).__init__()\n        self.checkpoint = checkpoint\n        self.bert = AutoModel.from_pretrained(checkpoint, return_dict=False)\n        self.layer_norm = nn.LayerNorm(params['output_logits'])\n        self.dropout = nn.Dropout(params['dropout'])\n        self.dense = nn.Sequential(\n            nn.Linear(params['output_logits'], 256),\n            nn.LeakyReLU(negative_slope=0.01),\n            nn.Dropout(params['dropout']),\n            nn.Linear(256, 1)\n        )\n\n    def forward(self, input_ids, token_type_ids, attention_mask):\n        _, pooled_output = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n        pooled_output = self.layer_norm(pooled_output)\n        pooled_output = self.dropout(pooled_output)\n        preds = self.dense(pooled_output)\n        return preds\n","32d488ae":"predictions_nn = None\nfor model_name in glob.glob(models_dir + '\/*.pth'):\n    model = ToxicityModel()\n    model.load_state_dict(torch.load(model_name))\n    model = model.to(params['device'])\n    model.eval()\n\n    test_dataset = BERTDataset(\n        text = test_df['text'].values\n    )\n    test_loader = DataLoader(\n        test_dataset, batch_size=params['batch_size'],\n        shuffle=False, num_workers=params['num_workers'],\n        pin_memory=True\n    )\n\n    temp_preds = None\n    with torch.no_grad():\n        for batch in tqdm(test_loader, desc=f'Predicting. '):\n            ids= batch['ids'].to(device)\n            mask = batch['mask'].to(device)\n            token_type_ids = batch['token_type_ids'].to(device)\n            predictions = model(ids, token_type_ids, mask).to('cpu').numpy()\n            \n            if temp_preds is None:\n                temp_preds = predictions\n            else:\n                temp_preds = np.vstack((temp_preds, predictions))\n\n    if predictions_nn is None:\n        predictions_nn = temp_preds\n    else:\n        predictions_nn += temp_preds\n        \npredictions_nn = (len(glob.glob(models_dir + '\/*.pth')))\n","af08d355":"sub_df = pd.DataFrame()\nsub_df['comment_id'] = test_df['comment_id']\nsub_df['score'] = predictions_nn\nsub_df['score'] = sub_df['score'].rank(method='first')","fbd6274e":"sub_df.head()","c507bc2c":"sub_df.to_csv('submission.csv', index=False)","712bae05":"print('My heart is beating like Thunder')\nprint('By Elvis Presley')","8c45b938":"## NLP Model\n","038fef4e":"## Predication \n","5f5acc6b":"## CFG","a2e7eca6":"## Dataset \n","e5ebfb56":"## Submission"}}