{"cell_type":{"d6cb47c7":"code","4610a9d9":"code","4eec49e9":"code","9d403536":"code","6d340a96":"code","f6a9aa85":"code","473695f6":"code","76655b4e":"code","ae8c219c":"code","ed588587":"code","b325d7a0":"code","69de2d3d":"code","b8688ac2":"code","23f605ff":"code","e3258d71":"code","83b5f817":"code","a7632469":"code","1f523f83":"code","2a1a8d74":"code","613eec5e":"code","31d61b6f":"code","a0896214":"code","207c2c4f":"code","ccc27289":"code","f8d0025f":"code","a474f2bf":"markdown","86fe37b8":"markdown","4a8e16f8":"markdown","e426abe1":"markdown"},"source":{"d6cb47c7":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4610a9d9":"import pandas as pd\n\nfilepath_dict = {'yelp':   '\/kaggle\/input\/sentiment-labelled-sentences-data-set\/sentiment labelled sentences\/yelp_labelled.txt',\n                 'amazon': '\/kaggle\/input\/sentiment-labelled-sentences-data-set\/sentiment labelled sentences\/amazon_cells_labelled.txt',\n                 'imdb':   '\/kaggle\/input\/sentiment-labelled-sentences-data-set\/sentiment labelled sentences\/imdb_labelled.txt'}\n\ndf_list = []\nfor source, filepath in filepath_dict.items():\n    df = pd.read_csv(filepath, names=['sentence', 'label'], sep='\\t')\n    df['source'] = source  # Add another column filled with the source name\n    df_list.append(df)\n\ndf = pd.concat(df_list)\nprint(df.iloc[0])","4eec49e9":"from sklearn.model_selection import train_test_split\ndf_yelp = df[df['source'] == 'yelp']\nsentences = df_yelp['sentence'].values\ny = df_yelp['label'].values\nsentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=100)","9d403536":"from sklearn.feature_extraction.text import CountVectorizer\n\nvectorizer = CountVectorizer()\nvectorizer.fit(sentences_train)\n\nX_train = vectorizer.transform(sentences_train)\nX_test  = vectorizer.transform(sentences_test)\nX_train","6d340a96":"from sklearn.linear_model import LogisticRegression\n\nclassifier = LogisticRegression()\nclassifier.fit(X_train, y_train)\nscore = classifier.score(X_test, y_test)\n\nprint(\"Accuracy:\", score)","f6a9aa85":"for source in df['source'].unique():\n    df_source = df[df['source'] == source]\n    sentences = df_source['sentence'].values\n    y = df_source['label'].values\n\n    sentences_train, sentences_test, y_train, y_test = train_test_split(sentences, y, test_size=0.25, random_state=1000)\n\n    vectorizer = CountVectorizer()\n    vectorizer.fit(sentences_train)\n    X_train = vectorizer.transform(sentences_train)\n    X_test  = vectorizer.transform(sentences_test)\n\n    classifier = LogisticRegression()\n    classifier.fit(X_train, y_train)\n    score = classifier.score(X_test, y_test)\n    print('Accuracy for {} data: {:.4f}'.format(source, score))","473695f6":"from keras.models import Sequential\nfrom keras import layers\n\ninput_dim = X_train.shape[1]  # Number of features\n\nmodel = Sequential()\nmodel.add(layers.Dense(10, input_dim=input_dim, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","76655b4e":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","ae8c219c":"history = model.fit(X_train, y_train,epochs=25,verbose=False,validation_data=(X_test, y_test),batch_size=40)","ed588587":"loss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","b325d7a0":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\ndef plot_history(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training accuracy')\n    plt.plot(x, val_acc, 'r', label='Validation accuracy')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()","69de2d3d":"plot_history(history)","b8688ac2":"from keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(sentences_train)\n\nX_train = tokenizer.texts_to_sequences(sentences_train)\nX_test = tokenizer.texts_to_sequences(sentences_test)\n\nvocab_size = len(tokenizer.word_index) + 1  # Adding 1 because of reserved 0 index\n\nprint(sentences_train[2])\nprint(X_train[2])","23f605ff":"from keras.preprocessing.sequence import pad_sequences\n\nmaxlen = 100\n\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)\n\nprint(X_train[0, :])\n","e3258d71":"from keras import layers\n\nembedding_dim = 50\n\nmodel = Sequential()\nmodel.add(layers.Embedding(input_dim=vocab_size, \n                           output_dim=embedding_dim, \n                           input_length=maxlen))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","83b5f817":"history = model.fit(X_train, y_train,\n                    epochs=15,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=14)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)","a7632469":"from keras.models import Sequential\nfrom keras import layers\n\nembedding_dim = 50\n\nmodel = Sequential()\nmodel.add(layers.Embedding(input_dim=vocab_size, \n                           output_dim=embedding_dim, \n                           input_length=maxlen))\nmodel.add(layers.GlobalMaxPool1D())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","1f523f83":"history = model.fit(X_train, y_train,\n                    epochs=20,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=25)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)","2a1a8d74":"import numpy as np\n\ndef create_embedding_matrix(filepath, word_index, embedding_dim):\n    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n\n    with open(filepath) as f:\n        for line in f:\n            word, *vector = line.split()\n            if word in word_index:\n                idx = word_index[word] \n                embedding_matrix[idx] = np.array(\n                    vector, dtype=np.float32)[:embedding_dim]\n\n    return embedding_matrix","613eec5e":"embedding_dim = 50\nembedding_matrix = create_embedding_matrix('\/kaggle\/input\/nlpword2vecembeddingspretrained\/glove.6B.50d.txt',tokenizer.word_index, embedding_dim)","31d61b6f":"nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\nnonzero_elements \/ vocab_size","a0896214":"from keras.models import Sequential\nfrom keras import layers\n\nembedding_dim = 50\n\nmodel = Sequential()\nmodel.add(layers.Embedding(vocab_size, embedding_dim, \n                           weights=[embedding_matrix], \n                           input_length=maxlen, \n                           trainable=False))\nmodel.add(layers.GlobalMaxPool1D())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","207c2c4f":"history = model.fit(X_train, y_train,\n                    epochs=50,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=10)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)","ccc27289":"model = Sequential()\nmodel.add(layers.Embedding(vocab_size, embedding_dim, \n                           weights=[embedding_matrix], \n                           input_length=maxlen, \n                           trainable=True))\nmodel.add(layers.GlobalMaxPool1D())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","f8d0025f":"history = model.fit(X_train, y_train,\n                    epochs=25,\n                    verbose=False,\n                    validation_data=(X_test, y_test),\n                    batch_size=20)\nloss, accuracy = model.evaluate(X_train, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))\nplot_history(history)","a474f2bf":"Made all vectors of equal length by appending zeros denoting no word matching with vocab, which was made using all the words. So no words at all!","86fe37b8":"Now try word embedding","4a8e16f8":"Using Pretained embedded dataset. Skip making vocab and embedding.","e426abe1":"a : 2, his : 4, this : 6 More common the word, lower the number. The : 1. 0 is reserved for padding. Not assigned to any text.\n"}}