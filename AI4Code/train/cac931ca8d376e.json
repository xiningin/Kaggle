{"cell_type":{"3a0d4f6b":"code","224a432b":"code","0faba8b2":"code","55ec8d0c":"code","d57b64f7":"code","eb68abd3":"code","7b02ebc9":"code","d999ea82":"code","3ebef68c":"code","85754c39":"code","0d0313c9":"code","48559347":"code","067dfede":"code","8440e2fe":"code","ef5da1e5":"code","b3d9c3ce":"code","31392823":"code","a5bd3729":"code","4d22a0d3":"code","c76016b6":"code","2e30c934":"code","4f40c08a":"code","3ad48771":"code","0803a56c":"code","6458b594":"code","2adcc6b8":"code","fb4c2d21":"code","55fd7757":"code","344402c8":"code","2bf9781b":"code","88331955":"code","3a89ab2b":"code","b4b3f8fd":"markdown","637e664f":"markdown","db8d2b3d":"markdown","07e78b54":"markdown","8669c5fa":"markdown","858e4353":"markdown","346d9112":"markdown","6c2c196e":"markdown","c30ffe90":"markdown","72737bea":"markdown","6f53aa30":"markdown","5cf29bb3":"markdown","04bbe874":"markdown","8f0f3325":"markdown","42bf6f7f":"markdown","46a0d2bc":"markdown","3f773227":"markdown","3222c4cf":"markdown","b292c4df":"markdown","b56a449e":"markdown"},"source":{"3a0d4f6b":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten,GlobalAveragePooling2D,BatchNormalization, Activation\nimport glob\n\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow import keras\n\nimport os","224a432b":"SEED = 42\nDEBUG = False","0faba8b2":"df = pd.read_csv('..\/input\/cassava-leaf-disease-classification\/train.csv')\ndf.head()","55ec8d0c":"df['path'] = '..\/input\/cassava-leaf-disease-classification\/train_images\/' + df['image_id']\ndf.label.value_counts(normalize=True) * 100\n","d57b64f7":"## Reading Test Images\n\ntest_images = glob.glob('..\/input\/cassava-leaf-disease-classification\/test_images\/*.jpg')\ndf_test = pd.DataFrame(test_images, columns = ['path'])","eb68abd3":"if DEBUG:\n    _, df = train_test_split(df, test_size = 0.1, random_state=SEED, shuffle=True, stratify=df['label'])\n","7b02ebc9":"def my_image_augmentation(train=True):\n    if train:\n        my_idg = ImageDataGenerator(#rescale=1. \/ 255.0,\n                                    horizontal_flip = True, \n                                    vertical_flip = True, \n                                    height_shift_range=0.2, \n                                    width_shift_range=0.2, \n                                    brightness_range=[0.7, 1.5],\n                                    rotation_range=30, \n                                    shear_range=0.2,\n                                    fill_mode='nearest',\n                                    zoom_range=[0.3,0.6],\n            \n            #featurewise_center=True, samplewise_center=True,\n        )\n    else:\n        #my_idg = ImageDataGenerator(#rescale=1. \/ 255.0) # No transformations on the validation\/test set\n        my_idg = ImageDataGenerator()\n    \n    return my_idg\n\n\n","d999ea82":"def make_train_gen(dataframe, target_size_dim, x_col, y_col,batch_size=64, my_train_idg=my_image_augmentation(train=True)):\n    train_gen = my_train_idg.flow_from_dataframe(dataframe=dataframe,  \n                                                x_col = x_col,\n                                                y_col = y_col,\n                                                class_mode=\"categorical\",\n                                                target_size=(target_size_dim, target_size_dim), \n                                                 color_mode='rgb',\n                                                batch_size = batch_size)\n\n    return train_gen\n\n\ndef make_val_gen(dataframe, target_size_dim, x_col, y_col,batch_size=64, my_val_idg=my_image_augmentation(train=False)):\n    \n    val_gen = my_val_idg.flow_from_dataframe(dataframe = dataframe, \n                                              x_col = x_col,\n                                              y_col = y_col,\n                                              class_mode=\"categorical\",\n                                              target_size=(target_size_dim, target_size_dim), \n                                              batch_size = batch_size,\n                                                shuffle=False) \n    \n    return val_gen\n\ndef make_test_gen(dataframe, target_size_dim, x_col,batch_size=64, my_test_idg=my_image_augmentation(train=False)):\n    \n    test_gen = my_test_idg.flow_from_dataframe(dataframe=dataframe,\n                                                x_col=x_col,\n                                                y_col=None,\n                                                batch_size=batch_size,\n                                                seed=SEED,\n                                                shuffle=False,\n                                                class_mode=None,\n                                                target_size=(target_size_dim, target_size_dim))\n    return test_gen","3ebef68c":"target_size_dim = 300\nbatch_size = 32","85754c39":"df['label'] = df['label'].astype('str') # Since we are using inbuilt generator it takes label as string\n\nX_train, X_valid = train_test_split(df, test_size = 0.1, random_state=SEED, shuffle=True)","0d0313c9":"train_gen = make_train_gen(X_train, x_col = 'path', y_col='label', batch_size=batch_size, target_size_dim=target_size_dim)\nvalid_gen = make_val_gen(X_valid, x_col = 'path', y_col='label', batch_size=batch_size*2, target_size_dim=target_size_dim)","48559347":"t_x, t_y = next(train_gen)\nfig, m_axs = plt.subplots(4, 6, figsize = (32, 16))\nfor (c_x, c_y, c_ax) in zip(t_x, t_y, m_axs.flatten()):\n    c_ax.imshow(c_x.astype(np.uint8))\n    c_ax.set_title(np.argmax(c_y))\n    c_ax.axis('off')","067dfede":"t_x, t_y = next(valid_gen)\nfig, m_axs = plt.subplots(4, 6, figsize = (32, 16))\nfor (c_x,  c_ax) in zip(t_x, m_axs.flatten()):\n    c_ax.imshow(c_x.astype(np.uint8))\n    c_ax.set_title(np.argmax(c_y))\n    c_ax.axis('off')","8440e2fe":"test_gen = make_test_gen(df_test, target_size_dim = target_size_dim, x_col='path', batch_size=batch_size*2)","ef5da1e5":"## Only available in tf2.3+\n\nfrom tensorflow.keras.applications import EfficientNetB3 \nfrom tensorflow.keras.losses import CategoricalCrossentropy","b3d9c3ce":"def load_pretrained_model(weights_path, drop_connect, target_size_dim, layers_to_unfreeze=5):\n    model = EfficientNetB3(\n            weights=None, \n            include_top=False, \n            input_shape=(target_size_dim, target_size_dim, 3),\n            drop_connect_rate=0.4\n        )\n    \n    model.load_weights(weights_path)\n    \n    model.trainable = True\n\n    # for layer in model.layers[-layers_to_unfreeze:]:\n    #     if not isinstance(layer, tf.keras.layers.BatchNormalization): \n    #         layer.trainable = True\n\n    if DEBUG:\n        for layer in model.layers:\n            print(layer.name, layer.trainable)\n\n    return model\n\ndef build_my_model(base_model, optimizer, loss='categorical_crossentropy', metrics = ['categorical_accuracy']):\n    \n    my_model = Sequential()    \n    my_model.add(base_model)\n    my_model.add(GlobalAveragePooling2D())\n    my_model.add(Dense(256))\n    my_model.add(BatchNormalization())\n    my_model.add(Activation('relu'))\n    my_model.add(Dropout(0.3))\n    my_model.add(Dense(5, activation='softmax'))\n    my_model.compile(\n        optimizer=optimizer,\n        loss=CategoricalCrossentropy(label_smoothing=0.05),\n        metrics=metrics\n    )\n    return my_model\n\n","31392823":"#!wget https:\/\/storage.googleapis.com\/keras-applications\/efficientnetb3_notop.h5\n## to get model weights","a5bd3729":"model_weights_path = '..\/input\/noisystudent\/efficientnetb3_notop.h5'\nmodel_weights_path","4d22a0d3":"drop_rate = 0.4 ## value of dropout to be used in loaded network\nbase_model = load_pretrained_model( model_weights_path, drop_rate, target_size_dim )\n\noptimizer = tf.keras.optimizers.Adam(lr = 1e-4)\nmy_model = build_my_model(base_model, optimizer)\nmy_model.summary()","c76016b6":"weight_path_save = 'best_model.hdf5'\nlast_weight_path = 'last_model.hdf5'\n\ncheckpoint = ModelCheckpoint(weight_path_save, \n                             monitor= 'val_loss', \n                             verbose=1, \n                             save_best_only=True, \n                             mode= 'min', \n                             save_weights_only = False)\ncheckpoint_last = ModelCheckpoint(last_weight_path, \n                             monitor= 'val_loss', \n                             verbose=1, \n                             save_best_only=False, \n                             mode= 'min', \n                             save_weights_only = False)\n\n\nearly = EarlyStopping(monitor= 'val_loss', \n                      mode= 'min', \n                      patience=10)\n\nreduceLROnPlat = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=2, verbose=1, mode='auto', epsilon=0.0001, cooldown=5, min_lr=0.00001)\ncallbacks_list = [checkpoint, checkpoint_last, early, reduceLROnPlat]","2e30c934":"if DEBUG:\n    epochs = 3\nelse:\n    epochs = 20","4f40c08a":"# from sklearn.utils import class_weight\n\n# classes_to_predict =[0, 1, 2, 3, 4]\n# class_weights = class_weight.compute_class_weight(\"balanced\", classes_to_predict, train_gen.labels)\n# class_weights_dict = {i : class_weights[i] for i,label in enumerate(classes_to_predict)}\n\n# print(class_weights_dict)\n","3ad48771":"if DEBUG:\n    history = my_model.fit(train_gen, \n                          validation_data = valid_gen, \n                          epochs = epochs, \n                          callbacks = callbacks_list,\n                           steps_per_epoch=1\n                           #class_weight=class_weights_dict\n                          )\nelse:\n    history = my_model.fit(train_gen, \n                          validation_data = valid_gen, \n                          epochs = epochs, \n                          callbacks = callbacks_list,\n                           #class_weight=class_weights_dict\n                          )","0803a56c":"def plot_hist(hist):\n    plt.figure(figsize=(15,5))\n    plt.plot(np.arange(epochs), hist.history[\"categorical_accuracy\"], '-o', label='Train Accuracy',color='#ff7f0e')\n    plt.plot(np.arange(epochs), hist.history[\"val_categorical_accuracy\"], '-o',label='Val Accuracy',color='#1f77b4')\n    plt.xlabel('Epoch',size=14)\n    plt.ylabel('Accuracy',size=14)\n    plt.legend(loc=2)\n    \n    plt2 = plt.gca().twinx()\n    plt2.plot(np.arange(epochs) ,history.history['loss'],'-o',label='Train Loss',color='#2ca02c')\n    plt2.plot(np.arange(epochs) ,history.history['val_loss'],'-o',label='Val Loss',color='#d62728')\n    plt.legend(loc=3)\n    plt.ylabel('Loss',size=14)\n    plt.title(\"Model Accuracy and loss\")\n    \n    #plt.legend([\"train\", \"validation\"], loc=\"upper left\")\n    \n    plt.savefig('loss.png')\n    plt.show()\n","6458b594":"plot_hist(history)","2adcc6b8":"from sklearn.metrics import confusion_matrix, classification_report","fb4c2d21":"my_model.load_weights(weight_path_save) ## load the best model or all your metrics would be on the last run not on the best one","55fd7757":"pred_valid_y = my_model.predict(valid_gen,  verbose = True)\npred_valid_y_labels = np.argmax(pred_valid_y, axis=-1)\nvalid_labels=valid_gen.labels\n\nprint(classification_report(valid_labels, pred_valid_y_labels ))","344402c8":"print(confusion_matrix(valid_labels, pred_valid_y_labels ))","2bf9781b":"pred_test = my_model.predict(test_gen, verbose = True)\npred_test_labels = np.argmax(pred_test, axis = -1)","88331955":"final_submission = df_test\n\nfinal_submission['image_id'] = final_submission.path.str.split('\/').str[-1]\nfinal_submission['label'] = pred_test_labels\n\nfinal_csv = final_submission[['image_id', 'label']]\nfinal_csv.head()\n\n","3a89ab2b":"final_csv.to_csv('submission.csv', index=False)\n","b4b3f8fd":"## Creating Model","637e664f":"### Update 2 (13\/12):\nTrying out Noisy Student weights\n\n### Update 1 (11\/12):\n\nI am removing normalization step in generator since in EfficientNet, normalization is done within the model itself and the model expects input in the range of [0,255]","db8d2b3d":"## Creating Submission File","07e78b54":"The input size comes from Keras blog which recommends an input size of 300 for EfficientNetB3\n\n![Screenshot%202020-12-05%20at%2011.34.08%20PM.png](attachment:Screenshot%202020-12-05%20at%2011.34.08%20PM.png)","8669c5fa":"## Getting Predictions on Test Set","858e4353":"Once we have the Generator we will feed in the data to generator.","346d9112":"## Inference kernel is [here](https:\/\/www.kaggle.com\/harveenchadha\/efficientnetb3-baseline-inference-keras-tf2)","6c2c196e":"### Callbacks","c30ffe90":"### Some important parameters for our configuration","72737bea":"In this kernel I would use Keras to build a baseline, this type of baseline can be helpful to you in solving similar problems as well.","6f53aa30":"Adding Seed helps to reproduce results. Setting Debug Parameter will run the model on smaller number of epochs to validate the architecture.","5cf29bb3":"## Evaluating Model on Validation Set","04bbe874":"## Train Model","8f0f3325":"## Augmentation","42bf6f7f":"## Train Valid Split","46a0d2bc":"Distribution of dataset:","3f773227":"### Visualizing Output of generator","3222c4cf":"### Creating Test Generator","b292c4df":"**Work In Progress. I am thinking on why the validation loss is fluctuating. This indicates me to the fact that the dataset is too noisy. Will surely work on some other approach for this noisy dataset. If you learnt something from this kernel kindly upvote :)**","b56a449e":"## Prepare Data"}}