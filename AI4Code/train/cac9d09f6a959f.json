{"cell_type":{"0a2151ae":"code","f379b9c5":"code","d3bc574e":"code","345aadad":"code","0da105c1":"code","3573bfbd":"code","7c2d3183":"code","f2ad777e":"code","2334e60d":"code","d641a861":"code","efa7c97b":"code","61bfe5c0":"code","f3143022":"code","4d4c51f9":"code","adca6808":"code","783de646":"code","c35caef0":"code","f250ec81":"code","ab7fcb17":"code","345024df":"code","2b64a783":"code","4224059c":"code","729c114f":"code","f0c79291":"code","f4dff3e6":"code","01b0efa0":"code","4d5ef36b":"code","6f60dafd":"code","08aa78b4":"code","5cd2b6fc":"code","231ac493":"code","9df33b4c":"code","ed4da115":"markdown","7a8fa75d":"markdown","9339bbe1":"markdown","2b282605":"markdown","b427bb8f":"markdown","91331d81":"markdown","3d003f87":"markdown","9a64ae8c":"markdown","3d54dc26":"markdown","82c19ad3":"markdown","df603e72":"markdown","b2f9b592":"markdown","30960fe8":"markdown","27300677":"markdown","8b97ab56":"markdown","84539db7":"markdown","9fecb5e4":"markdown"},"source":{"0a2151ae":"data_create = False\ntest =False","f379b9c5":"from IPython.core.display import display, HTML\n\nimport pandas as pd\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport glob\nimport os\nimport gc\nimport pickle\nfrom joblib import Parallel, delayed\n\nfrom sklearn import preprocessing, model_selection\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.metrics import r2_score\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport numpy.matlib\nfrom keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\n\n\npath_submissions = '\/'\n\ntarget_name = 'target'\nscores_folds = {}","d3bc574e":"# data directory\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'\n\n# Function to calculate first WAP\ndef calc_wap1(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1']) \/ (df['bid_size1'] + df['ask_size1'])\n    return wap\n\n# Function to calculate second WAP\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2']) \/ (df['bid_size2'] + df['ask_size2'])\n    return wap\n\n# Function to calculate the log of the return\n# Remember that logb(x \/ y) = logb(x) - logb(y)\ndef log_return(series):\n    return np.log(series).diff()\n\n# Calculate the realized volatility\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\ndef realized_normal(series):\n    return series\n\ndef realized_quarticity(series):\n    return np.sum(series**4)*series.shape[0]\/3\n    #df.groupby(df.index.date).agg(realized_quarticity)\ndef realized_quadpower_quarticity(series):\n    series = series.rolling(window=4).apply(np.product, raw=True)\n    return (np.sum(series) * series.shape[0] * (np.pi**2))\/4\n    #df.groupby(df.index.date).agg(realized_quadpower_quarticity)\ndef realized_1(series):\n    return np.sqrt(np.sum(series**4)\/(6*np.sum(series**2)))\n    #df.groupby(df.index.date).agg(realized_1)\n\n# Function to count unique elements of a series\ndef count_unique(series):\n    return len(np.unique(series))# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test\n\n\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n        'order_count':[np.mean,np.sum,np.max],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n# data directory\ndata_dir = '..\/input\/optiver-realized-volatility-prediction\/'\n\n# Function to read our base train and test set\ndef read_train_test():\n    train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\n    test = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/test.csv')\n    # Create a key to merge with book and trade data\n    train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\n    test['row_id'] = test['stock_id'].astype(str) + '-' + test['time_id'].astype(str)\n    print(f'Our training set has {train.shape[0]} rows')\n    return train, test\n\n# Function to preprocess book data (for each stock id)\ndef book_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    # Calculate Wap\n    df['wap1'] = calc_wap1(df)\n    df['wap2'] = calc_wap2(df)\n    # Calculate log returns\n    df['log_return1'] = df.groupby(['time_id'])['wap1'].apply(log_return)\n    df['log_return2'] = df.groupby(['time_id'])['wap2'].apply(log_return)\n    # Calculate wap balance\n    df['wap_balance'] = abs(df['wap1'] - df['wap2'])\n    # Calculate spread\n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1']) \/ 2)\n    df['price_spread2'] = (df['ask_price2'] - df['bid_price2']) \/ ((df['ask_price2'] + df['bid_price2']) \/ 2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df[\"bid_ask_spread\"] = abs(df['bid_spread'] - df['ask_spread'])\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n    #add\n    df['h_spread_l1'] = df['ask_price1'] - df['bid_price1']\n    df['h_spread_l2'] = df['ask_price2'] - df['bid_price2']\n    df['v_spread_b'] = df['bid_price1'] - df['bid_price2']\n    df['v_spread_a'] = df['ask_price1'] - df['ask_price2'] \n    ##\n    df['bid1_diff'] = df.bid_price1.diff()\n    df['ask1_diff'] = df.ask_price1.diff()\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'wap1': [np.sum, np.mean, np.std,np.min,np.max,np.median], \n        'wap2': [np.sum, np.mean, np.std,np.min,np.max,np.median],  #wap1_sum\u3068\u540c\u3058\u305f\u3081\u524a\u9664\n        'log_return1': [np.sum, realized_volatility, np.mean, np.std,np.min,np.max,np.median],\n        'log_return2': [np.sum, realized_volatility, np.mean, np.std,np.min,np.max,np.median],\n        'wap_balance': [np.sum, np.mean, np.std,np.min,np.max,np.median],\n        'price_spread':[np.sum, np.mean, np.std,np.min,np.max,np.median],\n        'price_spread2':[np.sum, np.mean, np.std,np.min,np.max,np.median],\n        'bid_spread':[np.sum, np.mean, np.std,np.max,np.median],\n        'ask_spread':[np.sum, np.mean, np.std,np.min,np.median],\n        'total_volume':[np.sum, np.mean, np.std,np.min,np.max,np.median],\n        'volume_imbalance':[np.sum, np.mean, np.std,np.min,np.max,np.median],\n        \"bid_ask_spread\":[np.sum, np.mean, np.std,np.min,np.max,np.median],\n        'h_spread_l1':[np.sum, np.mean, np.std,np.min,np.max,np.median],\n        'h_spread_l2':[np.sum, np.mean, np.std,np.min,np.max,np.median],\n        'v_spread_a':[np.sum, np.mean, np.std,np.min,np.max,np.median],\n        'v_spread_b':[np.sum, np.mean, np.std,np.min,np.max,np.median],\n        'bid1_diff':[np.sum, np.mean, np.std,np.min,np.max,np.median],\n        'ask1_diff':[np.sum, np.mean, np.std,np.min,np.max,np.median],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n#     df_feature_500 = get_stats_window(seconds_in_bucket = 500, add_suffix = True)\n#     df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n#     df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n\n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n#     df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n#     df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150'], axis = 1, inplace = True)\n    \n    \n    # Create row_id so we can merge\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x: f'{stock_id}-{x}')\n    df_feature.drop(['time_id_'], axis = 1, inplace = True)\n    return df_feature\n\n# Function to preprocess trade data (for each stock id)\ndef trade_preprocessor(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    # Dict for aggregations\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum, realized_volatility, np.mean, np.std, np.max, np.min],\n        'order_count':[np.mean,np.sum,np.max],\n    }\n    \n    # Function to get group stats for different windows (seconds in bucket)\n    def get_stats_window(seconds_in_bucket, add_suffix = False):\n        # Group by the window\n        df_feature = df[df['seconds_in_bucket'] >= seconds_in_bucket].groupby(['time_id']).agg(create_feature_dict).reset_index()\n        # Rename columns joining suffix\n        df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n        # Add a suffix to differentiate windows\n        if add_suffix:\n            df_feature = df_feature.add_suffix('_' + str(seconds_in_bucket))\n        return df_feature\n    \n\n    # Get the stats for different windows\n    df_feature = get_stats_window(seconds_in_bucket = 0, add_suffix = False)\n    df_feature_450 = get_stats_window(seconds_in_bucket = 450, add_suffix = True)\n#     df_feature_500 = get_stats_window(seconds_in_bucket = 500, add_suffix = True)\n#     df_feature_400 = get_stats_window(seconds_in_bucket = 400, add_suffix = True)\n    df_feature_300 = get_stats_window(seconds_in_bucket = 300, add_suffix = True)\n#     df_feature_200 = get_stats_window(seconds_in_bucket = 200, add_suffix = True)\n    df_feature_150 = get_stats_window(seconds_in_bucket = 150, add_suffix = True)\n    \n    def tendency(price, vol):    \n        df_diff = np.diff(price)\n        val = (df_diff\/price[1:])*100\n        power = np.sum(val*vol[1:])\n        return(power)\n    \n    lis = []\n    for n_time_id in df['time_id'].unique():\n        df_id = df[df['time_id'] == n_time_id]        \n        tendencyV = tendency(df_id['price'].values, df_id['size'].values)      \n        f_max = np.sum(df_id['price'].values > np.mean(df_id['price'].values))\n        f_min = np.sum(df_id['price'].values < np.mean(df_id['price'].values))\n        df_max =  np.sum(np.diff(df_id['price'].values) > 0)\n        df_min =  np.sum(np.diff(df_id['price'].values) < 0)\n        # new\n        abs_diff = np.median(np.abs( df_id['price'].values - np.mean(df_id['price'].values)))        \n        energy = np.mean(df_id['price'].values**2)\n        iqr_p = np.percentile(df_id['price'].values,75) - np.percentile(df_id['price'].values,25)\n        \n        # vol vars\n        \n        abs_diff_v = np.median(np.abs( df_id['size'].values - np.mean(df_id['size'].values)))        \n        energy_v = np.sum(df_id['size'].values**2)\n        iqr_p_v = np.percentile(df_id['size'].values,75) - np.percentile(df_id['size'].values,25)\n        \n        lis.append({'time_id':n_time_id,'tendency':tendencyV,'f_max':f_max,'f_min':f_min,'df_max':df_max,'df_min':df_min,\n                   'abs_diff':abs_diff,'energy':energy,'iqr_p':iqr_p,'abs_diff_v':abs_diff_v,'energy_v':energy_v,'iqr_p_v':iqr_p_v})    \n    df_lr = pd.DataFrame(lis)\n        \n   \n    df_feature = df_feature.merge(df_lr, how = 'left', left_on = 'time_id_', right_on = 'time_id')\n    \n    # Merge all\n    df_feature = df_feature.merge(df_feature_450, how = 'left', left_on = 'time_id_', right_on = 'time_id__450')\n    df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n#     df_feature = df_feature.merge(df_feature_300, how = 'left', left_on = 'time_id_', right_on = 'time_id__300')\n    df_feature = df_feature.merge(df_feature_150, how = 'left', left_on = 'time_id_', right_on = 'time_id__150')\n#     df_feature = df_feature.merge(df_feature_100, how = 'left', left_on = 'time_id_', right_on = 'time_id__100')\n    # Drop unnecesary time_ids\n    df_feature.drop(['time_id__450', 'time_id__300', 'time_id__150','time_id'], axis = 1, inplace = True)\n    \n    \n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature.drop(['trade_time_id_'], axis = 1, inplace = True)\n    return df_feature\n# Function to get group stats for the stock_id and time_id\ndef get_time_stock(df):\n    # Get realized volatility columns\n    vol_cols = ['log_return1_realized_volatility', 'log_return2_realized_volatility', 'log_return1_realized_volatility_450', 'log_return2_realized_volatility_450', \n                'log_return1_realized_volatility_300', 'log_return2_realized_volatility_300', 'log_return1_realized_volatility_150', 'log_return2_realized_volatility_150', \n                'trade_log_return_realized_volatility', 'trade_log_return_realized_volatility_450', 'trade_log_return_realized_volatility_300', 'trade_log_return_realized_volatility_150']\n\n    # Group by the stock id\n    df_stock_id = df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n    df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n    # Group by the stock id\n    df_time_id = df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min', ]).reset_index()\n    # Rename columns joining suffix\n    df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n    df_time_id = df_time_id.add_suffix('_' + 'time')\n    \n    # Merge with original dataframe\n    df = df.merge(df_stock_id, how = 'left', left_on = ['stock_id'], right_on = ['stock_id__stock'])\n    df = df.merge(df_time_id, how = 'left', left_on = ['time_id'], right_on = ['time_id__time'])\n    df.drop(['stock_id__stock', 'time_id__time'], axis = 1, inplace = True)\n    return df\n    \n# Funtion to make preprocessing function in parallel (for each stock id)\ndef preprocessor(list_stock_ids, is_train = True):\n    \n    # Parrallel for loop\n    def for_joblib(stock_id):\n        # Train\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        # Test\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = pd.merge(book_preprocessor(file_path_book), trade_preprocessor(file_path_trade), on = 'row_id', how = 'left')\n        \n        # Return the merge dataframe\n        return df_tmp\n    \n    # Use parallel api to call paralle for loop\n    df = Parallel(n_jobs = -1, verbose = 1)(delayed(for_joblib)(stock_id) for stock_id in list_stock_ids)\n    # Concatenate all the dataframes that return from Parallel\n    df = pd.concat(df, ignore_index = True)\n    return df\n\n# Function to calculate the root mean squared percentage error\ndef rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\n# Function to early stop with root mean squared percentage error\ndef feval_rmspe(y_pred, lgb_train):\n    y_true = lgb_train.get_label()\n    return 'RMSPE', rmspe(y_true, y_pred), False","345aadad":"if test:\n        stock_id =0\n        file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n        file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n    \n        # Preprocess book and trade data and merge them\n        df_tmp = book_preprocessor(file_path_book)","0da105c1":"train, test = read_train_test()\n\nif data_create:# Read train and test\n    print('train processing')\n    # Get unique stock ids \n    train_stock_ids = train['stock_id'].unique()\n    # Preprocess them using Parallel and our single stock id functions\n    train_ = preprocessor(train_stock_ids, is_train = True)\n    train = train.merge(train_, on = ['row_id'], how = 'left')\n\n\n    print('test processing')\n    # Get unique stock ids \n    test_stock_ids = test['stock_id'].unique()\n    # Preprocess them using Parallel and our single stock id functions\n    test_ = preprocessor(test_stock_ids, is_train = False)\n    test = test.merge(test_, on = ['row_id'], how = 'left')\n\n    # Get group stats of time_id and stock_id\n    train = get_time_stock(train)\n    test = get_time_stock(test)\n    \nelse:\n    print('data loadding')\n    train = pd.read_csv('..\/input\/optiva-data\/train_.csv')\n    \n    print('test processing')\n    # Get unique stock ids \n    \n    test_stock_ids = test['stock_id'].unique()\n    # Preprocess them using Parallel and our single stock id functions\n    test_ = preprocessor(test_stock_ids, is_train = False)\n    test = test.merge(test_, on = ['row_id'], how = 'left')\n    \n    test = get_time_stock(test)\n    ","3573bfbd":"#train.to_csv('train_.csv',index=None)","7c2d3183":"# replace by order sum (tau)\ntrain['size_tau'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique'] )\ntest['size_tau'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique'] )\ntrain['size_tau_450'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_450'] )\ntest['size_tau_450'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_450'] )\ntrain['size_tau_300'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_300'] )\ntest['size_tau_300'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_300'] )\ntrain['size_tau_150'] = np.sqrt( 1\/ train['trade_seconds_in_bucket_count_unique_150'] )\ntest['size_tau_150'] = np.sqrt( 1\/ test['trade_seconds_in_bucket_count_unique_150'] )\ntrain['size_tau2'] = np.sqrt( 1\/ train['trade_order_count_sum'] )\ntest['size_tau2'] = np.sqrt( 1\/ test['trade_order_count_sum'] )\ntrain['size_tau2_450'] = np.sqrt( 0.25\/ train['trade_order_count_sum'] )\ntest['size_tau2_450'] = np.sqrt( 0.25\/ test['trade_order_count_sum'] )\ntrain['size_tau2_300'] = np.sqrt( 0.5\/ train['trade_order_count_sum'] )\ntest['size_tau2_300'] = np.sqrt( 0.5\/ test['trade_order_count_sum'] )\ntrain['size_tau2_150'] = np.sqrt( 0.75\/ train['trade_order_count_sum'] )\ntest['size_tau2_150'] = np.sqrt( 0.75\/ test['trade_order_count_sum'] )\n\n# delta tau\ntrain['size_tau2_d'] = train['size_tau2_450'] - train['size_tau2']\ntest['size_tau2_d'] = test['size_tau2_450'] - test['size_tau2']","f2ad777e":"def add_process(df):\n    #8\/24 \u3053\u306e\u30b9\u30c6\u30c3\u30d7\u304c\u3042\u308b\u65b9\u304c\u30b9\u30b3\u30a2\u304c\u60aa\u5316\u3059\u308b\u305f\u3081\u5927\u5e45\u306b\u30b3\u30e1\u30f3\u30c8\u30a2\u30a6\u30c8\n    #df[\"log1_wap1_sum\"] = np.log1p(df['wap1_sum'])\n    #df[\"log1_wap2_sum\"] = np.log1p(df['wap2_sum'])\n    #df[\"log1_wap1_mean\"] = np.log1p(df['wap1_mean'])\n    #df[\"log1_wap2_mean\"] = np.log1p(df['wap2_mean'])\n    #df[\"log1_wap1_std\"] = np.log1p(df['wap1_std'])\n    #df[\"log1_wap2_std\"] = np.log1p(df['wap2_std'])\n    #df[\"log1_wap1_median\"] = np.log1p(df['wap1_median'])\n    #df[\"log1_wap2_median\"] = np.log1p(df['wap2_median'])\n    #df[\"log1_wap1_min\"] = np.log1p(df['wap1_amin'])\n    #df[\"log1_wap2_min\"] = np.log1p(df['wap2_amin'])\n    #df[\"log1_wap1_max\"] = np.log1p(df['wap1_amax'])\n    #df[\"log1_wap2_max\"] = np.log1p(df['wap2_amax'])\n    \n    ##\n    df[\"log_wap2_sum\"] = np.log(df['wap2_sum'])\n    df[\"log_wap1_sum\"] = np.log(df['wap1_sum'])\n    df[\"log_wap1_mean\"] = np.log(df['wap1_mean'])\n    df[\"log_wap2_mean\"] = np.log(df['wap2_mean'])\n    df[\"log_wap1_std\"] = np.log(df['wap1_std'])\n    df[\"log_wap2_std\"] = np.log(df['wap2_std'])\n    df[\"log_wap1_median\"] = np.log(df['wap1_median'])\n    df[\"log_wap2_median\"] = np.log(df['wap2_median'])\n    df[\"log_wap1_min\"] = np.log(df['wap1_amin'])\n    df[\"log_wap2_min\"] = np.log(df['wap2_amin'])\n    df[\"log_wap1_max\"] = np.log(df['wap1_amax'])\n    df[\"log_wap2_max\"] = np.log(df['wap2_amax'])\n    ##\n    \n    df[\"sqrt_wap1_sum_0.25\"] = np.sqrt(0.25\/(df['wap1_sum']))\n    df[\"sqrt_wap2_sum_0.25\"] = np.sqrt(0.25\/(df['wap2_sum']))    \n    df[\"sqrt_wap1_mean_0.25\"] = np.sqrt(0.25\/(df['wap1_mean']))\n    df[\"sqrt_wap2_mean_0.25\"] = np.sqrt(0.25\/(df['wap2_mean']))\n    df[\"sqrt_wap1_std_0.25\"] = np.sqrt(0.25\/(df['wap1_std']))\n    df[\"sqrt_wap2_std_0.25\"] = np.sqrt(0.25\/(df['wap2_std']))\n    df[\"sqrt_wap1_amin_0.25\"] = np.sqrt(0.25\/(df['wap1_amin']))\n    df[\"sqrt_wap2_amin_0.25\"] = np.sqrt(0.25\/(df['wap2_amin']))\n    df[\"sqrt_wap1_amax_0.25\"] = np.sqrt(0.25\/(df['wap1_amax']))\n    df[\"sqrt_wap2_amax_0.25\"] = np.sqrt(0.25\/(df['wap2_amax']))\n    df[\"sqrt_wap1_median_0.25\"] = np.sqrt(0.25\/(df['wap1_median']))\n    df[\"sqrt_wap2_median_0.25\"] = np.sqrt(0.25\/(df['wap2_median']))\n                                \n                \n    df[\"sqrt_wap1_sum_0.5\"] = np.sqrt(0.5\/(df['wap1_sum']))\n    df[\"sqrt_wap2_sum_0.5\"] = np.sqrt(0.5\/(df['wap2_sum']))    \n    df[\"sqrt_wap1_mean_0.5\"] = np.sqrt(0.5\/(df['wap1_mean']))\n    df[\"sqrt_wap2_mean_0.5\"] = np.sqrt(0.5\/(df['wap2_mean']))\n    df[\"sqrt_wap1_std_0.5\"] = np.sqrt(0.5\/(df['wap1_std']))\n    df[\"sqrt_wap2_std_0.5\"] = np.sqrt(0.5\/(df['wap2_std']))\n    df[\"sqrt_wap1_amin_0.5\"] = np.sqrt(0.5\/(df['wap1_amin']))\n    df[\"sqrt_wap2_amin_0.5\"] = np.sqrt(0.5\/(df['wap2_amin']))\n    df[\"sqrt_wap1_amax_0.5\"] = np.sqrt(0.5\/(df['wap1_amax']))\n    df[\"sqrt_wap2_amax_0.5\"] = np.sqrt(0.5\/(df['wap2_amax']))\n    df[\"sqrt_wap1_median_0.5\"] = np.sqrt(0.5\/(df['wap1_median']))\n    df[\"sqrt_wap2_median_0.5\"] = np.sqrt(0.5\/(df['wap2_median']))\n                                         \n    df[\"sqrt_wap1_sum_0.75\"] = np.sqrt(0.75\/(df['wap1_sum']))\n    df[\"sqrt_wap2_sum_0.75\"] = np.sqrt(0.75\/(df['wap2_sum']))    \n    df[\"sqrt_wap1_mean_0.75\"] = np.sqrt(0.75\/(df['wap1_mean']))\n    df[\"sqrt_wap2_mean_0.75\"] = np.sqrt(0.75\/(df['wap2_mean']))\n    df[\"sqrt_wap1_std_0.75\"] = np.sqrt(0.75\/(df['wap1_std']))\n    df[\"sqrt_wap2_std_0.75\"] = np.sqrt(0.75\/(df['wap2_std']))\n    df[\"sqrt_wap1_amin_0.75\"] = np.sqrt(0.75\/(df['wap1_amin']))\n    df[\"sqrt_wap2_amin_0.75\"] = np.sqrt(0.75\/(df['wap2_amin']))\n    df[\"sqrt_wap1_amax_0.75\"] = np.sqrt(0.75\/(df['wap1_amax']))\n    df[\"sqrt_wap2_amax_0.75\"] = np.sqrt(0.75\/(df['wap2_amax']))\n    df[\"sqrt_wap1_median_0.75\"] = np.sqrt(0.75\/(df['wap1_median']))\n    df[\"sqrt_wap2_median_0.75\"] = np.sqrt(0.75\/(df['wap2_median']))\n    ###\n    \n\n    #df[\"shift1_wap1_sum\"] = df['wap1_sum'].shift(1)\n    #df[\"shift1_wap2_sum\"] = df['wap2_sum'].shift(1)\n    #df[\"shift7_wap1_sum\"] = df['wap1_sum'].shift(7)\n    #df[\"shift7_wap2_sum\"] = df['wap2_sum'].shift(7)\n    #df[\"shift14_wap1_sum\"] = df['wap1_sum'].shift(14)\n    #df[\"shift14_wap2_sum\"] = df['wap2_sum'].shift(14)\n    #df[\"shift-1_wap1_sum\"] = df['wap1_sum'].shift(-1)\n    #df[\"shift-1_wap2_sum\"] = df['wap2_sum'].shift(-1)\n    #df[\"shift-7_wap1_sum\"] = df['wap1_sum'].shift(-7)\n    #df[\"shift-7_wap2_sum\"] = df['wap2_sum'].shift(-7)\n    #df[\"shift-14_wap1_sum\"] = df['wap1_sum'].shift(-14)\n    #df[\"shift-14_wap2_sum\"] = df['wap2_sum'].shift(-14)\n                                      \n    df[\"volatility1\"] = (df[\"log_wap1_sum\"])**0.5\n    df[\"volatility2\"] = (df[\"log_wap2_sum\"])**0.5\n    df[\"vol1_mean\"] = df[\"log_wap1_mean\"].fillna(0).values\n    df[\"vol2_mean\"] = df[\"log_wap2_mean\"].fillna(0).values\n    df[\"vol1_std\"] = df[\"log_wap1_std\"].fillna(0).values\n    df[\"vol2_std\"] = df[\"log_wap2_std\"].fillna(0).values\n    df[\"vol1_median\"] = df[\"log_wap1_median\"].fillna(0).values\n    df[\"vol2_median\"] = df[\"log_wap2_median\"].fillna(0).values\n    df[\"vol1_max\"] = df[\"log_wap1_max\"].fillna(0).values\n    df[\"vol2_max\"] = df[\"log_wap2_max\"].fillna(0).values\n    df[\"vol1_min\"] = df[\"log_wap1_min\"].fillna(0).values\n    df[\"vol2_min\"] = df[\"log_wap2_min\"].fillna(0).values\n    df[\"volatility_rate\"] = (df[\"volatility1\"] \/ df[\"volatility2\"]).fillna(0).values\n    df[\"mean_volatility_rate\"] = (df[\"vol1_mean\"] \/ df[\"vol2_mean\"]).fillna(0).values\n    df[\"std_volatility_rate\"] = (df[\"vol1_std\"] \/ df[\"vol2_std\"]).fillna(0).values\n    df[\"median_volatility_rate\"] = (df[\"vol1_median\"] \/ df[\"vol2_median\"]).fillna(0).values\n    df[\"max_volatility_rate\"] = (df[\"vol1_max\"] \/ df[\"vol2_max\"]).fillna(0).values\n    df[\"min_volatility_rate\"] = (df[\"vol1_min\"] \/ df[\"vol2_min\"]).fillna(0).values\n    \n    \n    '''\n    df['pca_7_wap1_mean'] =df['wap1_mean'].pct_change(7).fillna(0).values\n    df['pca_25_wap1_mean'] =df['wap1_mean'].pct_change(25).fillna(0).values\n    df['pca_75_wap1_mean'] =df['wap1_mean'].pct_change(75).fillna(0).values\n    df['pca_7_wap2_mean'] =df['wap2_mean'].pct_change(7).fillna(0).values\n    df['pca_25_wap2_mean'] =df['wap2_mean'].pct_change(25).fillna(0).values\n    df['pca_75_wap2_mean'] =df['wap2_mean'].pct_change(75).fillna(0).values\n    df['rolling_diff_7_wap1_mean'] =df['wap1_mean'].diff().rolling(7).std().fillna(0).values\n    df['rolling_diff_25_wap1_mean'] =df['wap1_mean'].diff().rolling(25).std().fillna(0).values\n    df['rolling_diff_75_wap1_mean'] =df['wap1_mean'].diff().rolling(75).std().fillna(0).values\n    df['rolling_diff_7_wap2_mean'] =df['wap2_mean'].diff().rolling(7).std().fillna(0).values\n    df['rolling_diff_25_wap2_mean'] =df['wap2_mean'].diff().rolling(25).std().fillna(0).values\n    df['rolling_diff_75_wap2_mean'] =df['wap2_mean'].diff().rolling(75).std().fillna(0).values\n    \n    df['pca_7_log_return1_mean'] =df['log_return1_mean'].pct_change(7).fillna(0).values\n    df['pca_25_log_return1_mean'] =df['log_return1_mean'].pct_change(25).fillna(0).values\n    df['pca_75_log_return1_mean'] =df['log_return1_mean'].pct_change(75).fillna(0).values\n    df['pca_7_log_return2_mean'] =df['log_return2_mean'].pct_change(7).fillna(0).values\n    df['pca_25_log_return2_mean'] =df['log_return2_mean'].pct_change(25).fillna(0).values\n    df['pca_75_log_return2_mean'] =df['log_return2_mean'].pct_change(75).fillna(0).values\n    df['rolling_diff_7_log_return1_mean'] =df['log_return1_mean'].diff().rolling(7).std().fillna(0).values\n    df['rolling_diff_25_log_return1_mean'] =df['log_return1_mean'].diff().rolling(25).std().fillna(0).values\n    df['rolling_diff_75_log_return1_mean'] =df['log_return1_mean'].diff().rolling(75).std().fillna(0).values\n    df['rolling_diff_7_log_return2_mean'] = df['log_return2_mean'].diff().rolling(7).std().fillna(0).values\n    df['rolling_diff_25_log_return2_mean'] =df['log_return2_mean'].diff().rolling(25).std().fillna(0).values\n    df['rolling_diff_75_log_return2_mean'] =df['log_return2_mean'].diff().rolling(75).std().fillna(0).values  \n    \n    df['pca_7_wap_balance_mean'] =df['wap_balance_mean'].pct_change(7).fillna(0).values\n    df['pca_25_wap_balance_mean'] =df['wap_balance_mean'].pct_change(25).fillna(0).values\n    df['pca_75_wap_balance_mean'] =df['wap_balance_mean'].pct_change(75).fillna(0).values\n    df['rolling_diff_7_wap_balance_mean'] =df['wap_balance_mean'].diff().rolling(7).std().fillna(0).values\n    df['rolling_diff_25_wap_balance_mean'] =df['wap_balance_mean'].diff().rolling(25).std().fillna(0).values\n    df['rolling_diff_75_wap_balance_mean'] =df['wap_balance_mean'].diff().rolling(75).std().fillna(0).values\n    \n    #df['pca_7_volume_imbalance_mean'] =df['volume_imbalance_mean'].pct_change(7).fillna(0).values\n    #df['pca_25_volume_imbalance_mean'] =df['volume_imbalance_mean'].pct_change(25).fillna(0).values\n    #df['pca_75_volume_imbalance_mean'] =df['volume_imbalance_mean'].pct_change(75).fillna(0).values\n    #df['rolling_diff_7_volume_imbalance_mean'] =df['volume_imbalance_mean'].diff().rolling(7).std().fillna(0).values\n    #df['rolling_diff_25_volume_imbalance_mean'] =df['volume_imbalance_mean'].diff().rolling(25).std().fillna(0).values\n    #df['rolling_diff_75_volume_imbalance_mean'] =df['volume_imbalance_mean'].diff().rolling(75).std().fillna(0).values\n    \n    #df['pca_7_bid_ask_spread_mean'] =df['bid_ask_spread_mean'].pct_change(7).fillna(0).values\n    #df['pca_25_bid_ask_spread_mean'] =df['bid_ask_spread_mean'].pct_change(25).fillna(0).values\n    #df['pca_75_bid_ask_spread_mean'] =df['bid_ask_spread_mean'].pct_change(75).fillna(0).values\n    #df['rolling_diff_7_bid_ask_spread_mean'] =df['bid_ask_spread_mean'].diff().rolling(7).std().fillna(0).values\n    #df['rolling_diff_25_bid_ask_spread_mean'] =df['bid_ask_spread_mean'].diff().rolling(25).std().fillna(0).values\n    #df['rolling_diff_75_bid_ask_spread_mean'] =df['bid_ask_spread_mean'].diff().rolling(75).std().fillna(0).values\n  \n    #df['pca_7_bid_log_return1_mean_450'] =df['log_return1_mean_450'].pct_change(7).fillna(0).values\n    #df['pca_25_bid_log_return1_mean_450'] =df['log_return1_mean_450'].pct_change(25).fillna(0).values\n    #df['pca_75_bid_log_return1_mean_450n'] =df['log_return1_mean_450'].pct_change(75).fillna(0).values\n    #df['rolling_diff_7_log_return1_mean_450'] =df['log_return1_mean_450'].diff().rolling(7).std().fillna(0).values\n    #df['rolling_diff_25_log_return1_mean_450'] =df['log_return1_mean_450'].diff().rolling(25).std().fillna(0).values\n    #df['rolling_diff_75_log_return1_mean_450'] =df['log_return1_mean_450'].diff().rolling(75).std().fillna(0).values\n    #df['pca_7_bid_log_return2_mean_450'] =df['log_return2_mean_450'].pct_change(7).fillna(0).values\n    #df['pca_25_bid_log_return2_mean_450'] =df['log_return2_mean_450'].pct_change(25).fillna(0).values\n    #df['pca_75_bid_log_return2_mean_450'] =df['log_return2_mean_450'].pct_change(75).fillna(0).values\n    #df['rolling_diff_7_log_return2_mean_450'] =df['log_return2_mean_450'].diff().rolling(7).std().fillna(0).values\n    #df['rolling_diff_25_log_return2_mean_450'] =df['log_return2_mean_450'].diff().rolling(25).std().fillna(0).values\n    #df['rolling_diff_75_log_return2_mean_450'] =df['log_return2_mean_450'].diff().rolling(75).std().fillna(0).values\n    \n    #df['pca_7_bid_log_return1_mean_300'] =df['log_return1_mean_300'].pct_change(7).fillna(0).values\n    #df['pca_25_bid_log_return1_mean_300'] =df['log_return1_mean_300'].pct_change(25).fillna(0).values\n    #df['pca_75_bid_log_return1_mean_300'] =df['log_return1_mean_300'].pct_change(75).fillna(0).values\n    #df['rolling_diff_7_log_return1_mean_300'] =df['log_return1_mean_300'].diff().rolling(7).std().fillna(0).values\n    #df['rolling_diff_25_log_return1_mean_300'] =df['log_return1_mean_300'].diff().rolling(25).std().fillna(0).values\n    #df['rolling_diff_75_log_return1_mean_300'] =df['log_return1_mean_300'].diff().rolling(75).std().fillna(0).values\n    #df['pca_7_bid_log_return2_mean_300'] =df['log_return2_mean_300'].pct_change(7).fillna(0).values\n    #df['pca_25_bid_log_return2_mean_300'] =df['log_return2_mean_300'].pct_change(25).fillna(0).values\n    #df['pca_75_bid_log_return2_mean_300'] =df['log_return2_mean_300'].pct_change(75).fillna(0).values\n    #df['rolling_diff_7_log_return2_mean_300'] =df['log_return2_mean_300'].diff().rolling(7).std().fillna(0).values\n    #df['rolling_diff_25_log_return2_mean_300'] =df['log_return2_mean_300'].diff().rolling(25).std().fillna(0).values\n    #df['rolling_diff_75_log_return2_mean_300'] =df['log_return2_mean_300'].diff().rolling(75).std().fillna(0).values\n    \n    #df['pca_7_bid_log_return1_mean_15'] =df['log_return1_mean_150'].pct_change(7).fillna(0).values\n    #df['pca_25_bid_log_return1_mean_150'] =df['log_return1_mean_150'].pct_change(25).fillna(0).values\n    #df['pca_75_bid_log_return1_mean_150'] =df['log_return1_mean_150'].pct_change(75).fillna(0).values\n    #df['rolling_diff_7_log_return1_mean_150'] =df['log_return1_mean_150'].diff().rolling(7).std().fillna(0).values\n    #df['rolling_diff_25_log_return1_mean_150'] =df['log_return1_mean_150'].diff().rolling(25).std().fillna(0).values\n    #df['rolling_diff_75_log_return1_mean_150'] =df['log_return1_mean_150'].diff().rolling(75).std().fillna(0).values\n    #df['pca_7_bid_log_return2_mean_150'] =df['log_return2_mean_150'].pct_change(7).fillna(0).values\n    #df['pca_25_bid_log_return2_mean_150'] =df['log_return2_mean_150'].pct_change(25).fillna(0).values\n    #df['pca_75_bid_log_return2_mean_150'] =df['log_return2_mean_150'].pct_change(75).fillna(0).values\n    #df['rolling_diff_7_log_return2_mean_150'] =df['log_return2_mean_150'].diff().rolling(7).std().fillna(0).values\n    #df['rolling_diff_25_log_return2_mean_150'] =df['log_return2_mean_150'].diff().rolling(25).std().fillna(0).values\n    #df['rolling_diff_75_log_return2_mean_150'] =df['log_return2_mean_150'].diff().rolling(75).std().fillna(0).values\n    '''\n    \n    return df","2334e60d":"#train = add_process(train)\n#test = add_process(test)","d641a861":"out_train = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\nout_train = out_train.pivot(index='time_id', columns='stock_id', values='target')\n\n# out_train[out_train.isna().any(axis=1)]\nout_train = out_train.fillna(out_train.mean())\nout_train.head()\n\n# Code to add the just the read data after first execution\n\n# Data separation based on knn ++\nnfolds = 5 # number of folds\nindex = []\ntotDist = []\nvalues = []\n\n# Generates a matriz with the values of \nmat = out_train.values\nscaler = MinMaxScaler(feature_range=(-1, 1))\nmat = scaler.fit_transform(mat)\nnind = int(mat.shape[0]\/nfolds) # number of individuals\n\n# Adds index in the last column\nmat = np.c_[mat,np.arange(mat.shape[0])]\nlineNumber = np.random.choice(np.array(mat.shape[0]), size=nfolds, replace=False)\nlineNumber = np.sort(lineNumber)[::-1]\nfor n in range(nfolds):\n    totDist.append(np.zeros(mat.shape[0]-nfolds))\n\n# Saves index\nfor n in range(nfolds):    \n    values.append([lineNumber[n]])\n\ns=[]\nfor n in range(nfolds):\n    s.append(mat[lineNumber[n],:])\n    mat = np.delete(mat, obj=lineNumber[n], axis=0)\n\nfor n in range(nind-1):    \n    luck = np.random.uniform(0,1,nfolds)\n    \n    for cycle in range(nfolds):\n        # Saves the values of index           \n        s[cycle] = np.matlib.repmat(s[cycle], mat.shape[0], 1)\n        sumDist = np.sum( (mat[:,:-1] - s[cycle][:,:-1])**2 , axis=1)   \n        totDist[cycle] += sumDist        \n                \n        # Probabilities\n        f = totDist[cycle]\/np.sum(totDist[cycle]) # normalizing the totDist\n        j = 0\n        kn = 0\n        for val in f:\n            j += val        \n            if (j > luck[cycle]): # the column was selected\n                break\n            kn +=1\n        lineNumber[cycle] = kn\n        \n        # Delete line of the value added    \n        for n_iter in range(nfolds):\n            totDist[n_iter] = np.delete(totDist[n_iter],obj=lineNumber[cycle], axis=0)\n            j= 0\n        \n        s[cycle] = mat[lineNumber[cycle],:]\n        values[cycle].append(int(mat[lineNumber[cycle],-1]))\n        mat = np.delete(mat, obj=lineNumber[cycle], axis=0)\n\nfor n_mod in range(nfolds):\n    values[n_mod] = out_train.index[values[n_mod]]","efa7c97b":"def root_mean_squared_per_error(y_true, y_pred):\n         return K.sqrt(K.mean(K.square( (y_true - y_pred)\/ y_true )))\n    \n","61bfe5c0":"colNames = list(train)\ncolNames.remove('time_id')\ncolNames.remove('target')\ncolNames.remove('row_id')\ncolNames.remove('stock_id')","f3143022":"train.replace([np.inf, -np.inf], np.nan,inplace=True)\ntest.replace([np.inf, -np.inf], np.nan,inplace=True)\nqt_train = []\n\nfor col in colNames:\n    qt = QuantileTransformer(random_state=21,n_quantiles=2000, output_distribution='normal')\n    train[col] = qt.fit_transform(train[[col]])\n    test[col] = qt.transform(test[[col]])    \n    qt_train.append(qt)","4d4c51f9":"train_p = pd.read_csv('..\/input\/optiver-realized-volatility-prediction\/train.csv')\ntrain_p = train_p.pivot(index='time_id', columns='stock_id', values='target')\ncorr = train_p.corr()\nids = corr.index\nkmeans = KMeans(n_clusters=7, random_state=0).fit(corr.values)\nprint(kmeans.labels_)\nl = []\nfor n in range(7):\n    l.append ( [ (x-1) for x in ( (ids+1)*(kmeans.labels_ == n)) if x > 0] )\n\nmat = []\nmatTest = []\nn = 0\nfor ind in l:\n    print(ind)\n    newDf = train.loc[train['stock_id'].isin(ind) ]\n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    mat.append ( newDf )\n    newDf = test.loc[test['stock_id'].isin(ind) ]    \n    newDf = newDf.groupby(['time_id']).agg(np.nanmean)\n    newDf.loc[:,'stock_id'] = str(n)+'c1'\n    matTest.append ( newDf )\n    n+=1\n    \nmat1 = pd.concat(mat).reset_index()\nmat1.drop(columns=['target'],inplace=True)\nmat2 = pd.concat(matTest).reset_index()","adca6808":"matTest = []\nmat = []\nkmeans = []\nmat2 = pd.concat([mat2,mat1.loc[mat1.time_id==5]])\nmat1 = mat1.pivot(index='time_id', columns='stock_id')\nmat1.columns = [\"_\".join(x) for x in mat1.columns.ravel()]\nmat1.reset_index(inplace=True)\n\nmat2 = mat2.pivot(index='time_id', columns='stock_id')\nmat2.columns = [\"_\".join(x) for x in mat2.columns.ravel()]\nmat2.reset_index(inplace=True)\nnnn = ['time_id',\n     'log_return1_realized_volatility_0c1',\n     'log_return1_realized_volatility_1c1',     \n     'log_return1_realized_volatility_3c1',\n     'log_return1_realized_volatility_4c1',     \n     'log_return1_realized_volatility_6c1',\n     'total_volume_mean_0c1',\n     'total_volume_mean_1c1', \n     'total_volume_mean_3c1',\n     'total_volume_mean_4c1', \n     'total_volume_mean_6c1',\n     'trade_size_mean_0c1',\n     'trade_size_mean_1c1', \n     'trade_size_mean_3c1',\n     'trade_size_mean_4c1', \n     'trade_size_mean_6c1',\n     'trade_order_count_mean_0c1',\n     'trade_order_count_mean_1c1',\n     'trade_order_count_mean_3c1',\n     'trade_order_count_mean_4c1',\n     'trade_order_count_mean_6c1',      \n     'price_spread_mean_0c1',\n     'price_spread_mean_1c1',\n     'price_spread_mean_3c1',\n     'price_spread_mean_4c1',\n     'price_spread_mean_6c1',   \n     'bid_spread_mean_0c1',\n     'bid_spread_mean_1c1',\n     'bid_spread_mean_3c1',\n     'bid_spread_mean_4c1',\n     'bid_spread_mean_6c1',       \n     'ask_spread_mean_0c1',\n     'ask_spread_mean_1c1',\n     'ask_spread_mean_3c1',\n     'ask_spread_mean_4c1',\n     'ask_spread_mean_6c1', \n     'volume_imbalance_mean_0c1',\n     'volume_imbalance_mean_1c1',\n     'volume_imbalance_mean_3c1',\n     'volume_imbalance_mean_4c1',\n     'volume_imbalance_mean_6c1',       \n     'bid_ask_spread_mean_0c1',\n     'bid_ask_spread_mean_1c1',\n     'bid_ask_spread_mean_3c1',\n     'bid_ask_spread_mean_4c1',\n     'bid_ask_spread_mean_6c1',\n     'size_tau2_0c1',\n     'size_tau2_1c1',\n     'size_tau2_3c1',\n     'size_tau2_4c1',\n     'size_tau2_6c1'] ","783de646":"train = pd.merge(train,mat1[nnn],how='left',on='time_id')\ntest = pd.merge(test,mat2[nnn],how='left',on='time_id')\nmat1 = []\nmat2 = []\n# Thanks to https:\/\/bignerdranch.com\/blog\/implementing-swish-activation-function-in-keras\/\ndef swish(x, beta = 1):\n    return (x * sigmoid(beta * x))\n\nget_custom_objects().update({'swish': Activation(swish)})","c35caef0":"import lightgbm as lgbm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import accuracy_score","f250ec81":"'''\nimport optuna \nimport optuna.integration.lightgbm as lgbo\n\nparams = { 'objective': 'mean_squared_error', 'metric': 'rmse' ,'device':'gpu'}\nfrom sklearn.metrics import mean_absolute_error\n\nx_train, x_test, y_train, y_test = train_test_split(X[col], y, test_size=0.3, random_state=42)\nlgb_train = lgb.Dataset(x_train, y_train)\nlgb_valid = lgb.Dataset(x_test, y_test)\n\nmodel = lgbo.train(params, lgb_train, valid_sets=[lgb_valid], verbose_eval=False, num_boost_round=100, early_stopping_rounds=5) \nmodel.params\n'''","ab7fcb17":"#best_lgb_params = model.params\nbest_lgb_params ={'objective': 'mean_squared_error',\n 'metric': 'rmse',\n 'feature_pre_filter': False,\n 'lambda_l1': 0.006813376998521786,\n 'lambda_l2': 0.0008339804762310761,\n 'num_leaves': 231,\n 'feature_fraction': 1.0,\n 'bagging_fraction': 1.0,\n 'bagging_freq': 0,\n 'min_child_samples': 20,\n 'num_iterations': 100,\n 'early_stopping_round': 5}\nbest_lgb_params[\"learning_rate\"] = 0.01\nbest_lgb_params[\"early_stopping_round\"] = 200\nbest_lgb_params[\"num_iterations\"] = 80000 \nbest_lgb_params[\"device\"] = 'gpu'\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=5, random_state=77, shuffle=True)\noof = pd.DataFrame()               \nmodels = []                          \nscores = 0.0  ","345024df":"train['stock_id'] =train['stock_id'].astype(int)\n#X['time_id']  =X['time_id'].astype(int)","2b64a783":"test['stock_id'] =test['stock_id'].astype(int)\ntest['time_id']  =test['time_id'].astype(int)","4224059c":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\ndef feval_RMSPE(preds, lgbm_train):\n    labels = lgbm_train.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False","729c114f":"'''\n    # create dataset\n    x_train,x_valid,y_train,y_valid = train_test_split(X[col], y, test_size=0.2,random_state = 77)\n   \n    #weights = 1\/np.square(y_train)\n    #lgbm_train = lgbm.Dataset(x_train,y_train,weight = weights)\n    lgbm_train = lgbm.Dataset(x_train,y_train)\n    #weights = 1\/np.square(y_valid)\n    #lgbm_valid = lgbm.Dataset(x_valid,y_valid,reference = lgbm_train,weight = weights)\n    lgbm_valid = lgbm.Dataset(x_valid,y_valid)\n\n    model = lgbm.train(params=best_lgb_params,\n                      train_set=lgbm_train,\n                      valid_sets=[lgbm_train, lgbm_valid],\n                      num_boost_round=5000,         \n                      feval=feval_RMSPE,\n                      verbose_eval=100 #,\n                      #categorical_feature = ['stock_id','time_id']                \n                     )\n\n    y_pred = model.predict(x_valid, num_iteration=model.best_iteration)\n    \n    pred = model.predict(X_test[col], num_iteration=model.best_iteration)\n\n    RMSPE = round(rmspe(y_true = y_valid, y_pred = y_pred),3)\n    print(f'Performance of the\u3000prediction: , RMSPE: {RMSPE}')\n\n    #keep scores and models\n    scores += RMSPE \/ 5\n    models.append(model)\n    print(\"*\" * 100)\n '''","f0c79291":"len(cat_data.values)","f4dff3e6":"len(num_data)","01b0efa0":"len(target)","4d5ef36b":"nfolds = 5\nmodel_name = 'LGBM'\npred_name = 'pred_{}'.format(model_name)\n\nn_folds = 5\nkf = model_selection.KFold(n_splits=n_folds, shuffle=True, random_state=2020)\nscores_folds[model_name] = []\ncounter = 1\n\nfeatures_to_consider = list(train)\n\nfeatures_to_consider.remove('time_id')\nfeatures_to_consider.remove('target')\nfeatures_to_consider.remove('row_id')\ntry:\n    features_to_consider.remove('pred_LGBM')\nexcept:\n    pass\n\ntrain[features_to_consider] = train[features_to_consider].fillna(train[features_to_consider].mean())\ntest[features_to_consider] = test[features_to_consider].fillna(train[features_to_consider].mean())\n\ntrain[pred_name] = 0\ntest['target'] = 0\n\nfor n_count in range(n_folds):\n    print('CV {}\/{}'.format(counter, n_folds))\n    \n    indexes = np.arange(nfolds).astype(int)    \n    indexes = np.delete(indexes,obj=n_count, axis=0) \n    indexes = np.r_[values[indexes[0]],values[indexes[1]],values[indexes[2]],values[indexes[3]]]\n    \n    X_train = train.loc[train.time_id.isin(indexes), features_to_consider]\n    y_train = train.loc[train.time_id.isin(indexes), target_name]\n    X_test = train.loc[train.time_id.isin(values[n_count]), features_to_consider]\n    y_test = train.loc[train.time_id.isin(values[n_count]), target_name]# NN\n    #model = base_model()\n    \n    #model.compile(\n    #    keras.optimizers.Adam(learning_rate=learning_rate),\n    #    loss=root_mean_squared_per_error\n    #)\n    \n    try:\n        features_to_consider.remove('stock_id')\n    except:\n        pass\n    \n    num_data = X_train[features_to_consider]\n    \n    scaler = MinMaxScaler(feature_range=(-1, 1))         \n    num_data = scaler.fit_transform(num_data.values)    \n    \n    cat_data = X_train['stock_id']    \n    target =  y_train\n    \n    num_data_test = X_test[features_to_consider]\n    num_data_test = scaler.transform(num_data_test.values)\n    cat_data_test = X_test['stock_id']\n\n    lgbm_train = lgbm.Dataset([cat_data.values, num_data],target)\n    lgbm_valid = lgbm.Dataset([cat_data_test.values, num_data_test], y_test)\n\n    model = lgbm.train(params=best_lgb_params,\n                      train_set=lgbm_train,\n                      valid_sets=[lgbm_train, lgbm_valid],\n                      num_boost_round=5000,         \n                      feval=feval_RMSPE,\n                      verbose_eval=100 #,\n                      #categorical_feature = ['stock_id','time_id']                \n                     )\n    #\n\n    preds = model.predict([cat_data_test, num_data_test]).reshape(1,-1)[0]\n    \n    score = round(rmspe(y_true = y_test, y_pred = preds),5)\n    print('Fold {} {}: {}'.format(counter, model_name, score))\n    scores_folds[model_name].append(score)\n    \n    tt =scaler.transform(test[features_to_consider].values)\n    test[target_name] += model.predict([test['stock_id'], tt]).reshape(1,-1)[0].clip(0,1e10)\n       \n    counter += 1\n    features_to_consider.append('stock_id')\n    \n    del X_train\n    del y_train\n    del X_test\n    del y_test\n    del indexes\n    del num_data_test\n    del cat_data_test","6f60dafd":"'''\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n\n    print(\"Fold :\", fold+1)\n    \n    # create dataset\n    X_train, y_train = X[col].loc[trn_idx], y[trn_idx]\n    X_valid, y_valid = X[col].loc[val_idx], y[val_idx]\n\n    lgbm_train = lgbm.Dataset(x_train,y_train)\n    lgbm_valid = lgbm.Dataset(x_valid,y_valid)\n    \n    #weights = 1\/np.square(y_train)\n    #lgbm_train = lgbm.Dataset(X_train,y_train,weight = weights)\n\n    #weights = 1\/np.square(y_valid)\n    #lgbm_valid = lgbm.Dataset(X_valid,y_valid,reference = lgbm_train,weight = weights)\n    \n\n    model = lgbm.train(params=best_lgb_params,\n                      train_set=lgbm_train,\n                      valid_sets=[lgbm_train, lgbm_valid],\n                      num_boost_round=5000,         \n                      feval=feval_RMSPE,\n                      verbose_eval=100 #,\n                      #categorical_feature = ['stock_id','time_id']                \n                     )\n    pickle.dump(model, open(f'optiva_lightGBM_{fold}', 'wb'))\n    \n    y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n\n    RMSPE = round(rmspe(y_true = y_valid, y_pred = y_pred),3)\n    print(f'Performance of the\u3000prediction: , RMSPE: {RMSPE}')\n\n    #keep scores and models\n    scores += RMSPE \/ 5\n    models.append(model)\n    print(\"*\" * 100)\n'''","08aa78b4":"#lgbm.plot_importance(model, figsize=(12, 12))\n#plt.show()","5cd2b6fc":"test[target_name] = test[target_name]\/n_folds\n\nscore = round(rmspe(y_true = train[target_name].values, y_pred = train[pred_name].values),5)\nprint('RMSPE {}: {} - Folds: {}'.format(model_name, score, scores_folds[model_name]))\n\ndisplay(test[['row_id', target_name]].head(2))\n\n# Submission\ntest[['row_id', target_name]].to_csv('submission.csv',index = False)","231ac493":"'''\ny_pred = test[['row_id']]\nX_test = test.drop(['time_id', 'row_id'], axis = 1)\n\ntarget = np.zeros(len(X_test))\n\nfor model in models:\n    pred = model.predict(X_test[col], num_iteration=model.best_iteration)\n    target += pred \/ len(models)\n    \ny_pred = y_pred.assign(target = target)\ny_pred.to_csv('submission.csv',index = False)\ny_pred\n'''","9df33b4c":"\ud83d\ude3a\ud83d\ude05\u3299\ud83d\udd30\ud83d\uddd1\u2b1b\ud83d\udfe5\ud83d\udfe8\ud83d\udfe9","ed4da115":"<span style=\"color: orange; font-family: Segoe UI; font-size: 1.9em; font-weight: 300;\">Pre-processing<\/span>","7a8fa75d":"### If you do not set it to int, it will be moss.","9339bbe1":"<span style=\"color: orange; font-family: Segoe UI; font-size: 1.9em; font-weight: 300;\">add1 columns<\/span>","2b282605":"### It seems that it is not related to stock_id and time_id, so I will remove it from the explanation function.\n\nI don't know how to do it, so I'll specify it in category_feature. => It was no good.","b427bb8f":"### Thanks to everyone, it has started to work. It was a simple mistake. Sorry about that.","91331d81":"### <font color=\"orange\">copy from keras kernel<\/font>","3d003f87":"<span style=\"color: orange; font-family: Segoe UI; font-size: 1.9em; font-weight: 300;\">Optuna<\/span>","9a64ae8c":"<pre>\n[100]\ttraining's rmse: 0.00200028\ttraining's RMSPE: 0.73383\tvalid_1's rmse: 0.00197203\tvalid_1's RMSPE: 0.72595","3d54dc26":"<span style=\"color: orange; font-family: Segoe UI; font-size: 1.9em; font-weight: 300;\">add2 columns<\/span>","82c19ad3":"![image.png](attachment:de1645aa-926c-4880-b281-6115ca9797bb.png)","df603e72":"<pre>\n----------------- weight -----------------\n8\/24 23:00   Performance of the\u3000prediction: , RMSPE: 0.214 add\u3042\u308a\n8\/25 9:30 Performance of the\u3000prediction: , RMSPE: 0.214 add\u4fee\u6b63\n8\/25 14:00 Performance of the\u3000prediction: , RMSPE: 0.212            add\u306a\u3057\n8\/25 17:00 Performance of the\u3000prediction: , RMSPE: 0.212           stock_id\u3042\u308a\n                 lr=0.01\n                 lr=0.0055","b2f9b592":"<pre>\n\u30fbstock_id\u81ea\u4f53\u3082\u30e2\u30c7\u30eb\u3067\u4f7f\u7528\u3057\u306a\u3044\u3067\u304f\u3060\u3055\u3044\u3002\u3053\u308c\u306b\u3088\u308a\u3001\u30e2\u30c7\u30eb\u304c\u30b9\u30c8\u30c3\u30af\u306e\u52d5\u4f5c\u306b\u904e\u5270\u9069\u5408\u3059\u308b\u3053\u3068\u306f\u3042\u308a\u307e\u305b\u3093\u3002\u30e2\u30c7\u30eb\u306f\u3001\u30aa\u30fc\u30c0\u30fc\u30d6\u30c3\u30af\u3068\u53d6\u5f15\u30c7\u30fc\u30bf\u306e\u307f\u3092\u8a8d\u8b58\u3057\u3066\u3044\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\u6b63\u898f\u5316\u306f\u3001\u30b0\u30ed\u30fc\u30d0\u30eb\u5e73\u5747\u3068\u6a19\u6e96\u3067\u306f\u306a\u304f\u3001\u30b9\u30c8\u30c3\u30af\u5e73\u5747\u3068\u6a19\u6e96\u3067\u3082\u5b9f\u884c\u3059\u308b\u5fc5\u8981\u304c\u3042\u308a\u307e\u3059\u3002\n\u30fbCNN\u3068RNN\u306f\u307e\u3068\u3082\u306a\u30b9\u30b3\u30a2\u306b\u9054\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059\u304c\u3001\u30d1\u30e9\u30e1\u30fc\u30bf\u30fc\u304c\u305f\u304f\u3055\u3093\u3042\u308a\u3001\u7c21\u5358\u306b\u30aa\u30fc\u30d0\u30fc\u30d5\u30a3\u30c3\u30c8\u3059\u308b\u53ef\u80fd\u6027\u304c\u3042\u308a\u307e\u3059\u3002\u79c1\u306f\u307e\u3068\u3082\u306aCNN\u3068RNN\u3092\u6301\u3063\u3066\u3044\u307e\u3059\u304c\u3001\u304a\u305d\u3089\u304f\u52fe\u914d\u30d6\u30fc\u30b9\u30c6\u30a3\u30f3\u30b0\u6c7a\u5b9a\u6728\u306b\u56fa\u57f7\u3059\u308b\u3067\u3057\u3087\u3046\u3002","30960fe8":"<span style=\"color: orange; font-family: Segoe UI; font-size: 1.9em; font-weight: 300;\">Dedicated function<\/span>","27300677":"### Make sure that train and test have the same number of columns","8b97ab56":"### kfold","84539db7":"![image.png](attachment:93adc227-8297-475a-ba5f-6555fb7043ce.png)","9fecb5e4":"![image.png](attachment:4dd6d667-9a3d-4550-80f4-6ddce86ad4c6.png)"}}