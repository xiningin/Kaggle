{"cell_type":{"cd75a15d":"code","db525ac4":"code","9b323644":"code","656de552":"code","7eaba581":"code","ffd9ad94":"code","5d206525":"code","ef4a9573":"code","1c62b750":"code","b5f7c21f":"code","27e16f40":"code","d2cd50c2":"code","2fcfa4a7":"code","c40c5dc0":"code","ad971847":"code","8d29de49":"code","1ab0d934":"code","d775a658":"code","cdba11e0":"code","34ff2f19":"code","67ba78da":"code","1aa8b395":"code","e40a3614":"code","1fe4caf1":"code","cce04724":"code","a382f760":"code","d05711de":"code","40360713":"code","d8bf2237":"code","f34ea998":"code","c0d39086":"code","af6c76e6":"code","8f0ff423":"code","0cd243fe":"code","74142a4c":"code","48429cc4":"code","fde343b8":"code","f6a593c7":"code","b7fd7ea8":"code","c27ddaac":"code","38d99e0c":"code","813b5337":"code","43658d36":"code","6699d275":"code","803a501c":"code","4d248ff1":"code","af764ffa":"code","f3f1166c":"code","beecb44b":"code","f0c26883":"code","f42ab98d":"code","a0e8a0ea":"code","92ba3b17":"code","ca8fb177":"code","af0a955a":"code","8590fd0c":"code","acd10ca2":"code","9c81ef45":"code","873d80e2":"code","7e7a7efb":"code","7ea98979":"code","11fe5f50":"code","23bcd9e6":"code","931775b9":"code","27f6cce6":"code","f18e2015":"code","594da019":"code","c9d99297":"code","0db8b8b7":"code","eee22611":"code","58b65a11":"code","4d8c62a5":"code","7dd8b452":"code","df15fd7f":"code","ed6a1dde":"code","428b304c":"code","a4356db4":"code","35506f58":"code","c260fa51":"code","d7e4e57f":"code","6cf03048":"code","6c5ba219":"code","3701839b":"code","3d18a2d3":"code","0650949c":"code","9b0f11fd":"code","25af625f":"code","1f34bbd6":"code","166be6cb":"code","84eff227":"code","006fb69d":"code","0cd7fed6":"code","ce66fb1f":"code","5b89245b":"markdown","d30cc1ba":"markdown","2569a4b9":"markdown","d229a292":"markdown","eb5f9051":"markdown","aa33d4d3":"markdown","63a1bcd1":"markdown","dedfdbea":"markdown","3961831e":"markdown","38e58f9a":"markdown","27d1d46d":"markdown","2311f57c":"markdown","751db860":"markdown","868deb58":"markdown","f03c2f02":"markdown","3e93873b":"markdown","b520d6f5":"markdown","9fc084da":"markdown","31b9e690":"markdown","63a745c2":"markdown","6a36908e":"markdown","492710e0":"markdown","e7cc0d8b":"markdown","5c72b3a2":"markdown","b1d42e1c":"markdown","58f71045":"markdown","9b4c6998":"markdown","a3ef48b3":"markdown","b77bb181":"markdown","47f73bce":"markdown","ef6a2f09":"markdown","b2e0e65d":"markdown","33a555bd":"markdown","fd47b677":"markdown","42b5ccd0":"markdown","d2cc0e42":"markdown","30248687":"markdown","e1284d30":"markdown","18e6fe47":"markdown","5565ae68":"markdown","36b2017a":"markdown","75578a51":"markdown","a5416679":"markdown","65a0bc5a":"markdown","074f88f7":"markdown","3ba4701f":"markdown","538deb62":"markdown","38b7f451":"markdown","42cae3b0":"markdown","0689bbd6":"markdown","b137a7a7":"markdown"},"source":{"cd75a15d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","db525ac4":"datas = pd.read_csv(\"..\/input\/enopenfoodfactsorgproducts\/en.openfoodfacts.org.products.csv\",\n                    sep='\\t', low_memory=False)","9b323644":"datas.sample(5)","656de552":"print (\"Le dataset compte {} lignes et {} variables\".format(datas.shape[0], datas.shape[1]))","7eaba581":"def null_factor(df, tx_threshold=50):\n  null_rate = ((datas.isnull().sum() \/ datas.shape[0])*100).sort_values(ascending=False).reset_index()\n  null_rate.columns = ['Variable','Taux_de_Null']\n  high_null_rate = null_rate[null_rate.Taux_de_Null >= tx_threshold]\n  return high_null_rate","ffd9ad94":"full_null_rate = null_factor(datas, 100)\nfull_null_rate","5d206525":"filling_features = null_factor(datas, 0)\nfilling_features[\"Taux_de_Null\"] = 100-filling_features[\"Taux_de_Null\"]\nfilling_features = filling_features.sort_values(\"Taux_de_Null\", ascending=False) \n\n#Seuil de suppression\nsup_threshold = 25\n\nfig = plt.figure(figsize=(20, 35))\n\nfont_title = {'family': 'serif',\n              'color':  '#114b98',\n              'weight': 'bold',\n              'size': 18,\n             }\n\nsns.barplot(x=\"Taux_de_Null\", y=\"Variable\", data=filling_features, palette=\"flare\")\n#Seuil pour suppression des varaibles\nplt.axvline(x=sup_threshold, linewidth=2, color = 'r')\nplt.text(sup_threshold+2, 65, 'Seuil de suppression des variables', fontsize = 16, color = 'r')\n\nplt.title(\"Taux de remplissage des variables dans le jeu de donn\u00e9es (%)\", fontdict=font_title)\nplt.xlabel(\"Taux de remplissage (%)\")\nplt.show()","ef4a9573":"#Liste des variables \u00e0 conserver\nfeatures_to_conserve = list(filling_features.loc[filling_features['Taux_de_Null']>=sup_threshold, 'Variable'].values)\n#Liste des variables supprim\u00e9es\ndeleted_features = list(filling_features.loc[filling_features['Taux_de_Null']<sup_threshold, 'Variable'].values)\n\n#Nouveau Dataset avec les variables conserv\u00e9es\ndatas = datas[features_to_conserve].sort_values([\"created_datetime\",\"last_modified_datetime\"], ascending=True)\ndatas.sample(5)","1c62b750":"def search_componant(df, suffix='_100g'):\n  componant = []\n  for col in df.columns:\n      if '_100g' in col: componant.append(col)\n  df_subset_columns = df[componant]\n  return df_subset_columns","b5f7c21f":"df_subset_nutients = search_componant(datas,'_100g')\ndf_subset_nutients.head()","27e16f40":"print('Lignes nutriments (_100g) vides: {}'.format(df_subset_nutients.isnull().all(axis=1).sum()))","d2cd50c2":"datas = datas[df_subset_nutients.notnull().any(axis=1)]\ndatas.shape","2fcfa4a7":"# Suppression des doublons en fonction du code\ndatas.drop_duplicates(subset =\"code\", keep = 'last', inplace=True)","c40c5dc0":"datas[(datas[\"product_name\"].isnull()==False) \n      & (datas[\"brands\"].isnull()==False)].groupby(by=[\"product_name\",\"brands\"])[\"code\"].nunique().sort_values(ascending=False)","ad971847":"datas = datas[(datas[\"product_name\"]!=\"\ud83e\udd2c\") \n              & (datas[\"brands\"]!=\"\ud83e\udd2c\")]","8d29de49":"# Suppression des doublons sur marque et produit en conservant les valeurs nulles\ndatas = datas[(~datas.duplicated([\"product_name\",\"brands\"],keep=\"last\")) \n      | ((datas['product_name'].isnull()) & (datas['brands'].isnull()))]","1ab0d934":"datas.shape","d775a658":"category_columns = ['categories','categories_tags','categories_en']\ndatas[datas[category_columns].notnull().any(axis=1)][['product_name'] + category_columns].sample(5)","cdba11e0":"def search_redundant_col(df):\n  redundant_columns = []\n  for col in df.columns:\n    if \"_en\" in col:\n      en = col.replace('_en','')\n      tags = col.replace('_en','_tags')\n      print(\"{:<20} 'Sans suffixe' -> {} ; 'Suffixe _tags' -> {}\".format(col,\n                                                                        en in df.columns, tags in df.columns))\n      if en in df.columns : \n        redundant_columns.append(en)\n      if tags in df.columns : \n        redundant_columns.append(tags)\n  \n    if '_tags' in col:\n      tags_2 = col.replace('_tags','')\n      print(\"{:<20} 'Suffixe _tags' -> {} ;\".format(tags_2, tags_2 in df.columns))\n      if tags_2 in df.columns :\n        redundant_columns.append(col)\n\n  return redundant_columns","34ff2f19":"datas.drop(search_redundant_col(datas), axis=1, inplace=True)","67ba78da":"datas['created_datetime'] = pd.to_datetime(datas['created_t'], unit='s')\ndatas['last_modified_datetime'] = pd.to_datetime(datas['last_modified_t'], unit='s')\ndatas = datas.drop(['created_t','last_modified_t'], axis=1)\ndatas.head()","1aa8b395":"add_per_year = datas[['created_datetime', 'code']].groupby(by=datas['created_datetime'].dt.year).nunique()\n\nfig=plt.figure(figsize=(12,8))\n\nsns.set_style(\"whitegrid\")\nsns.barplot(data=add_per_year, x=add_per_year.index, y='code', color='#00afe6')\n\nplt.title(\"Evolution des cr\u00e9ations de produits dans la base par ann\u00e9e\", fontdict=font_title)\nplt.xlabel(\"Ann\u00e9e de cr\u00e9ation\")\nplt.ylabel(\"Nombre de cr\u00e9ations\")\nplt.show()","e40a3614":"datas.shape","1fe4caf1":"countries = datas.groupby(by=\"countries_en\").nunique()","cce04724":"countries[['code']].head()","a382f760":"def split_words(df, column = 'countries_en'):\n  list_words = set()\n  for word in df[column].str.split(','):\n    if isinstance(word, float):\n      continue\n    list_words = set().union(word, list_words)\n  return list(list_words)","d05711de":"#Liste contenant tous les pays du jeu de donn\u00e9es (bons ou mauvais)\nlist_countries = split_words(datas, 'countries_en')","40360713":"print(\"Nombre de pays repr\u00e9sent\u00e9s : {}\".format(len(list_countries)))","d8bf2237":"df_countries = pd.read_csv(\"..\/input\/enopenfoodfactsorgproducts\/countries-en.csv\",\n                        sep=\",\", header=None, index_col = 0).rename(columns={0:\"index\", 1:\"country_id\", 2:\"country_code_2\", 3:\"country_code_3\", 4:\"country_fr\", 5:\"country_en\"})\ndf_countries.head()","f34ea998":"df_countries = pd.merge(pd.DataFrame(list_countries, columns=[\"countries_dataset\"]),df_countries, how=\"left\", \n         left_on=\"countries_dataset\", right_on=\"country_en\")","c0d39086":"false_country_list = list(df_countries[df_countries.isnull().sum(axis=1)>0].countries_dataset)\nfalse_country_list[0:15]","af6c76e6":"for index, countries in datas['countries_en'].str.split(',').items():\n  if isinstance(countries, float):\n    continue\n  country_name = []\n  found = False\n  for country in countries:\n    if country in false_country_list:\n      found = True\n    else:\n      country_name.append(country)\n  if found:\n    datas.loc[index, 'countries_en'] = ','.join(country_name)","8f0ff423":"print(\"Nouveau nombre de pays repr\u00e9sent\u00e9s : {}\".format(len(split_words(datas, 'countries_en'))))","0cd243fe":"datas['countries_en'] = np.where((datas['countries_en'].isnull()==True), \"unknown\", \n                                 np.where(datas['countries_en'] == \"\", \"unknown\", datas['countries_en']))","74142a4c":"def top_words(df, column=\"countries_en\", nb_top=10):\n  count_keyword = dict()\n  for index, col in df[column].iteritems():\n    if isinstance(col, float):\n      continue\n    for word in col.split(','):\n      if word in count_keyword.keys():\n        count_keyword[word] += 1\n      else :\n        count_keyword[word] = 1\n  \n  keyword_top = []\n  for k,v in count_keyword.items():\n    keyword_top.append([k,v])\n  keyword_top.sort(key = lambda x:x[1], reverse = True)\n\n  return keyword_top[:nb_top]","48429cc4":"df_top_countries = pd.DataFrame(top_words(df=datas, column=\"countries_en\", nb_top=10), \n                                columns=[\"Keyword\",\"count\"])\ndf_top_countries","fde343b8":"datas[['categories_en','pnns_groups_1','pnns_groups_2','main_category_en',]].sample(10)","f6a593c7":"categories = split_words(df = datas, column = 'categories_en')\nprint(\"{} cat\u00e9gories sont repr\u00e9sent\u00e9es dans le jeu de donn\u00e9es.\".format(len(categories)))","b7fd7ea8":"datas['categories_en'] = np.where((datas['categories_en'].isnull()==True), \"unknown\", \n                                 np.where(datas['categories_en'] == \"\", \"unknown\", datas['categories_en']))\ndatas['main_category_en'] = np.where((datas['main_category_en'].isnull()==True), \"unknown\", \n                                 np.where(datas['main_category_en'] == \"\", \"unknown\", datas['main_category_en']))\ndatas['pnns_groups_1'] = np.where((datas['pnns_groups_1'].isnull()==True), \"unknown\", \n                                 np.where(datas['pnns_groups_1'] == \"\", \"unknown\", datas['pnns_groups_1']))\ndatas['pnns_groups_2'] = np.where((datas['pnns_groups_2'].isnull()==True), \"unknown\", \n                                 np.where(datas['pnns_groups_2'] == \"\", \"unknown\", datas['pnns_groups_2']))","c27ddaac":"df_top_categories = pd.DataFrame(top_words(df=datas, column=\"categories_en\", nb_top=10), \n                                 columns=[\"Keyword\",\"count\"])\ndf_top_categories","38d99e0c":"from wordcloud import WordCloud\n\ndef plot_world_cloud(df=datas,column=\"categories_en\",nb_top=100):\n  fig = plt.figure(1, figsize=(20,15))\n  ax1 = fig.add_subplot(1,1,1)\n\n  words = dict()\n  trunc_occurences = top_words(df=df, column=column, nb_top=nb_top)\n  for s in trunc_occurences:\n    words[s[0]] = s[1]\n\n  word_cloud = WordCloud(width=900,height=500, normalize_plurals=False,\n                        background_color=\"white\")\n  word_cloud.generate_from_frequencies(words)\n  ax1.imshow(word_cloud, interpolation=\"bilinear\")\n  ax1.axis('off')\n  plt.title(\"Nuage de mots des {} meilleures {}\\n\".format(nb_top, column), fontsize=22)\n  plt.show()","813b5337":"plot_world_cloud(df=datas,column=\"categories_en\",nb_top=100)","43658d36":"pnns_groups_1 = split_words(df = datas, column = 'pnns_groups_1')\npnns_groups_2 = split_words(df = datas, column = 'pnns_groups_2')\nprint(\"{} cat\u00e9gories sont repr\u00e9sent\u00e9es dans la variable pnns_group_1.\".format(len(pnns_groups_1)))\nprint(\"{} cat\u00e9gories sont repr\u00e9sent\u00e9es dans la variable pnns_group_2.\".format(len(pnns_groups_2)))","6699d275":"pnns_groups_1","803a501c":"datas[\"pnns_groups_1\"] = datas[\"pnns_groups_1\"].str.lower().str.replace('-', ' ')","4d248ff1":"pnns_groups_1 = split_words(df = datas, column = 'pnns_groups_1')\nprint(\"{} cat\u00e9gories sont repr\u00e9sent\u00e9es dans la variable pnns_group_1.\".format(len(pnns_groups_1)))\nprint(pnns_groups_1)","af764ffa":"pnns_groups_2","f3f1166c":"datas[\"pnns_groups_2\"] = datas[\"pnns_groups_2\"].str.lower().str.replace('-', ' ').replace('pizza pies and quiche','pizza pies and quiches')\npnns_groups_2 = split_words(df = datas, column = 'pnns_groups_2')\nprint(\"{} cat\u00e9gories sont repr\u00e9sent\u00e9es dans la variable pnns_group_2.\".format(len(pnns_groups_2)))","beecb44b":"plot_world_cloud(df=datas,column=\"pnns_groups_1\",nb_top=10)","f0c26883":"plot_world_cloud(df=datas,column=\"pnns_groups_2\",nb_top=len(pnns_groups_2))","f42ab98d":"datas.info()","a0e8a0ea":"datas.describe()","92ba3b17":"datas[datas['energy_100g'] == datas['energy_100g'].max()]","ca8fb177":"datas_cleaned = datas[~((datas.product_name.isnull()) \n                        & ((datas.pnns_groups_1 == \"unknown\") \n                           | (datas.main_category_en == \"unknown\")))]","af0a955a":"datas_cleaned[((datas_cleaned.pnns_groups_1 == \"unknown\") & (datas_cleaned.main_category_en == \"unknown\") &\n              (datas_cleaned.pnns_groups_2 == \"unknown\") & (datas_cleaned.categories_en == \"unknown\"))].shape[0]","8590fd0c":"#On rep\u00e8re les numerical_features\nnumerical_features = list(datas_cleaned.select_dtypes(include=[\"float64\",\"int64\"]).columns)\n#On supprime les nutriscores qui eux peuvent \u00eatre n\u00e9gatifs\nnumerical_features.remove('nutriscore_score')\nnumerical_features.remove('nutrition-score-fr_100g')\nnumerical_features.remove('ingredients_that_may_be_from_palm_oil_n')\nnumerical_features.remove('ingredients_from_palm_oil_n')\nnumerical_features.remove('nova_group')","acd10ca2":"#On supprime les lignes dont toutes les numerical_features sont \u00e0 0 ou nulles\ndatas_cleaned = datas_cleaned.loc[~((datas_cleaned[numerical_features]==0) | (datas_cleaned[numerical_features].isnull())).all(axis=1)]","9c81ef45":"#On supprime les lignes contenant des valeurs n\u00e9gatives et des max aberrants\ndatas_cleaned = datas_cleaned[~(datas_cleaned[numerical_features] < 0).any(axis=1)]\ndatas_cleaned = datas_cleaned[~(datas_cleaned[numerical_features].isin([999999,9999999])).any(axis=1)]","873d80e2":"g_per_100g_features = ['proteins_100g','fat_100g','carbohydrates_100g','sugars_100g','salt_100g',\n                       'sodium_100g','saturated-fat_100g','fiber_100g']\ndatas_cleaned = datas_cleaned[~(datas_cleaned[g_per_100g_features] > 100).any(axis=1)]","7e7a7efb":"datas_cleaned = datas_cleaned[~((datas_cleaned['saturated-fat_100g'] > datas_cleaned['fat_100g']) \n                                | (datas_cleaned['sodium_100g'] > datas_cleaned['salt_100g']))]","7ea98979":"datas_cleaned = datas_cleaned[~((datas_cleaned['energy_100g'] > 3700) \n                                | (datas_cleaned['energy-kcal_100g'] > 900))]","11fe5f50":"sigma_features = ['additives_n','serving_quantity']","23bcd9e6":"#On initialise l'\u00e9cart-type et la m\u00e9diane\nsigma = [0 for _ in range(len(sigma_features))]\nmedian = [0 for _ in range(len(sigma_features))]\n#Puis on compl\u00e8tes les valeurs avec le dataset sans les valeurs nulles\nfor i in range(len(sigma_features)):\n  median[i] = datas_cleaned[pd.notnull(datas_cleaned[sigma_features[i]])][sigma_features[i]].median()\n  serie = datas_cleaned[pd.notnull(datas_cleaned[sigma_features[i]])][sigma_features[i]]\n  serie = serie.sort_values()\n  sigma[i] = np.std(serie[:-25])","931775b9":"plt.style.use('ggplot')\ntPlot, axes = plt.subplots(nrows=1, ncols=2, sharex=False, sharey=False, figsize=(21,10))\naxes = np.array(axes)\n\ni=0\nfor ax in axes.reshape(-1):\n    colonne = sigma_features[i]\n    test = datas_cleaned[pd.notnull(datas_cleaned[colonne])][colonne]\n    ax.tick_params(labelcolor='black',top='off',bottom='on',left='on',right='off',labelsize=8)\n    ax.set_ylabel(colonne.rstrip(\"_100g\"), fontsize = 12)\n    ax.set_yscale(\"log\")\n    ax.plot(list(test), 'b.', markeredgewidth = 0.3, markeredgecolor='w')\n    for tick in ax.get_xticklabels():\n        tick.set_rotation(30)\n    ax.axhline(y=median[i], color='g', linestyle='dashdot')\n    ax.axhline(y=median[i] + 5*sigma[i], color='r', linestyle='-')\n    ax.text(0., 0.02, 'm\u00e9diane:{:.3} \\n sigma:{:.3} \\n Seuil:{:.3}'.format(median[i], sigma[i], (median[i] + 5*sigma[i])),\n            style='italic', transform=ax.transAxes, fontsize = 12,\n            bbox={'facecolor':'#00afe6', 'alpha':0.5, 'pad':0})\n    i += 1\n\ntPlot.text(0.5, 1.01, r\"Dispersion des donn\u00e9es nutritionnelles\" \"\\n\" \"Visualisation des outliers\", ha='center', fontdict=font_title)\nplt.tight_layout()","27f6cce6":"for i in range(len(sigma_features)):\n    col = sigma_features[i]\n    threshold = (median[i] + 5*sigma[i])\n    print('{:30}: suppression de la ligne si valeur > {}'.format(col, round(threshold,3)))\n    mask = datas_cleaned[col] > threshold\n    datas_cleaned = datas_cleaned.drop(datas_cleaned[mask].index)","f18e2015":"datas_cleaned.shape","594da019":"datas_cleaned.describe()","c9d99297":"import scipy.stats as stats\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set_style(\"whitegrid\")\nfig = plt.figure(figsize=(21,30))\n\nsub = 0\nfor i in range(len(numerical_features)):\n    fig.add_subplot(4,3,i+1)\n    \n    left, width = 0, 1\n    bottom, height = 0, 1\n    right = left + width\n    top = bottom + height\n    \n    colonne = numerical_features[i]\n    kstest = stats.kstest(datas_cleaned[colonne].notnull(),'norm')\n    ax = sns.distplot(datas_cleaned[colonne], fit=stats.norm, kde=False)\n    ax.set_title(\"Distribution vs loi normale : {}\".format(colonne))\n    ax.text(right, top, 'Test Kolmogorov-Smirnov \\n Pvalue: {:.2} \\n Stat: {:.2}'.format(kstest.pvalue, kstest.statistic),\n            horizontalalignment='right',\n            verticalalignment='top',\n            style='italic', transform=ax.transAxes, fontsize = 12,\n            bbox={'facecolor':'#00afe6', 'alpha':0.5, 'pad':0})\n    sub += 1\nplt.show()","0db8b8b7":"plt.style.use('ggplot')\nfig = plt.figure(figsize=(21,40))\n\nsub = 0\nfor i in range(len(numerical_features)):\n    fig.add_subplot(6,2,i+1)\n    colonne = numerical_features[i]\n    ax = sns.boxplot(x=\"pnns_groups_1\", y=colonne, data=datas_cleaned[datas_cleaned[\"pnns_groups_1\"]!=\"unknown\"])\n    for tick in ax.get_xticklabels():\n        tick.set_rotation(30)\n    sub += 1\n\nfig.text(0.5, 0.90, r\"Distribution des variables nutritionnelles\" \"\\n\" \"par cat\u00e9gories pnns_groups_1\", ha=\"center\", \n         fontdict=font_title)\nplt.show()","eee22611":"plt.style.use('ggplot')\nfig = plt.figure(figsize=(21,60))\n\nsub = 0\nfor i in range(len(numerical_features[:6])):\n    fig.add_subplot(6,1,i+1)\n    colonne = numerical_features[i]\n    ax = sns.boxplot(y=\"pnns_groups_2\", x=colonne, data=datas_cleaned[datas_cleaned[\"pnns_groups_2\"]!=\"unknown\"])\n    for tick in ax.get_xticklabels():\n        tick.set_rotation(30)\n    sub += 1\n\nfig.text(0.5, 0.90, r\"Distribution des variables nutritionnelles\" \"\\n\" \"par cat\u00e9gories pnns_groups_2\", ha=\"center\", \n         fontdict=font_title)\nplt.show()","58b65a11":"datas_cleaned[numerical_features].isnull().sum()","4d8c62a5":"#On supprime les 2 colonnes les moins compl\u00e9t\u00e9es et la variable redondante energie\ndatas_cleaned.drop(['serving_quantity','additives_n','energy-kcal_100g'], axis=1, inplace=True)","7dd8b452":"numerical_features.remove('serving_quantity')\nnumerical_features.remove('additives_n')\nnumerical_features.remove('energy-kcal_100g')","df15fd7f":"# KNN pour les autres variables\nfrom sklearn.impute import KNNImputer\n\n# On entraine le mod\u00e8le d'imputation sur un \u00e9chantillon de donn\u00e9es\nknn_features = ['energy_100g','proteins_100g','saturated-fat_100g','sugars_100g','salt_100g']\nsample_datas = datas_cleaned[knn_features].sample(frac=0.25, random_state=1)\nimputer = KNNImputer(n_neighbors=5, missing_values=np.nan)\nimputer.fit(sample_datas)","ed6a1dde":"# Puis on applique le mod\u00e8le sur l'ensemble des donn\u00e9es\ndatas_imputed = imputer.transform(datas_cleaned[knn_features])\ndf_datas_imputed = pd.DataFrame(datas_imputed, columns=knn_features)","428b304c":"for col_knn in knn_features:\n    datas_cleaned[col_knn] = df_datas_imputed[col_knn].values","a4356db4":"# On compl\u00e8te les variables restantes avec la m\u00e9diane du groupe pnns 2\nfor col in ['carbohydrates_100g','sodium_100g','fat_100g','fiber_100g']:\n    datas_cleaned[col] = datas_cleaned.groupby('pnns_groups_2')[col].transform(lambda x: x.fillna(x.median()))","35506f58":"sns.set_style(\"whitegrid\")\nfig = plt.figure(figsize=(21,20))\n\nsub = 0\nfor i in range(len(numerical_features)):\n    fig.add_subplot(3,3,i+1)\n    colonne = numerical_features[i]\n    kstest = stats.kstest(datas_cleaned[colonne].notnull(),'norm')\n    ax = sns.distplot(datas_cleaned[colonne], fit=stats.norm, kde=False)\n    ax.set_title(\"Distribution vs loi normale : {}\".format(colonne))\n    sub += 1\nplt.show()","c260fa51":"datas_cleaned.isnull().sum().sort_values(ascending=False)","d7e4e57f":"deleted_features = ['brands','serving_size','nova_group','image_ingredients_small_url',\n                    'image_ingredients_url','ingredients_text','ingredients_from_palm_oil_n',\n                    'ingredients_that_may_be_from_palm_oil_n','image_nutrition_url','image_nutrition_small_url',\n                    'image_url','image_small_url', 'nutrition-score-fr_100g','quantity']\ndatas_cleaned.drop(deleted_features, axis=1, inplace=True)","6cf03048":"nutriscore_features = ['pnns_groups_1', 'pnns_groups_2', 'nutriscore_grade', 'nutriscore_score',\n                       'energy_100g','sugars_100g','saturated-fat_100g','sodium_100g', 'fiber_100g', 'proteins_100g']\ndatas_cleaned[nutriscore_features].sample(10)","6c5ba219":"datas_cleaned.pnns_groups_2.unique()","3701839b":"high_rate_fruit = ['fruit juices','dried fruits','legumes','vegetables','fruits', 'soups','potatoes','fruit nectars']\n\nmedium_rate_fruit = ['unknown', 'sweetened beverages', 'dressings and sauces', 'ice cream', 'pastries', 'dairy desserts',\n                     'pizza pies and quiche', 'pizza pies and quiches']\n\nlow_rate_fruit = ['waters and flavored waters','chocolate products', 'fish and seafood', 'salty and fatty products', \n                  'cheese', 'cereals', 'appetizers', 'one dish meals', 'bread', 'fats', 'plant based milk substitutes',\n                  'alcoholic beverages', 'processed meat', 'breakfast cereals', 'meat', 'eggs', 'sandwiches',\n                  'offals', 'teas and herbal teas and coffees', 'biscuits and cakes', 'sweets', 'milk and yogurt',\n                  'artificially sweetened beverages', 'unsweetened beverages','nuts']","3d18a2d3":"datas_cleaned['fruits-vegetables-rate_100g'] = [81 if cat in high_rate_fruit else 45 if cat in medium_rate_fruit else 25 for cat in datas_cleaned.pnns_groups_2]","0650949c":"def calc_globalscore(row):\n    #Energy\n    if row[\"energy_100g\"] <= 335:\n        a = 0\n    elif ((row[\"energy_100g\"] > 335) & (row[\"energy_100g\"] <= 1675)):\n        a = 5\n    else:\n        a = 10 \n    #Sugar\n    if row[\"sugars_100g\"] <= 4.5:\n        b = 0\n    elif ((row[\"sugars_100g\"] > 4.5) & (row[\"sugars_100g\"] <= 22.5)):\n        b = 5\n    else:\n        b = 10\n    #saturated-fat\n    if row[\"saturated-fat_100g\"] <= 1:\n        c = 0\n    elif ((row[\"saturated-fat_100g\"] > 1) & (row[\"saturated-fat_100g\"] <= 5)):\n        c = 5\n    else:\n        c = 10\n    #sodium\n    if (row[\"sodium_100g\"]\/1000) <= 90:\n        d = 0\n    elif (((row[\"sodium_100g\"]\/1000) > 90) & ((row[\"sodium_100g\"]\/1000) <= 450)):\n        d = 5\n    else:\n        d = 10\n    #fruits-vegetables-rate\n    if row[\"fruits-vegetables-rate_100g\"] <= 40:\n        e = 0\n    elif ((row[\"fruits-vegetables-rate_100g\"] > 40) & (row[\"fruits-vegetables-rate_100g\"] <= 80)):\n        e = -2\n    else:\n        e = -5\n    #fiber\n    if row[\"fiber_100g\"] <= 0.7:\n        f = 0\n    elif ((row[\"fiber_100g\"] > 0.7) & (row[\"fiber_100g\"] <= 3.5)):\n        f = -2\n    else:\n        f = -5\n    #proteins\n    if row[\"proteins_100g\"] <= 1.6:\n        g = 0\n    elif ((row[\"proteins_100g\"] > 1.6) & (row[\"proteins_100g\"] <= 8)):\n        g = -2\n    else:\n        g = -5\n    \n    #Global_score\n    global_score = a+b+c+d+e+f+g\n    \n    return global_score","9b0f11fd":"#Nutriscore\ndef calc_nutriscore(row):\n    if row[\"calc_global_score\"] < 0 :\n        nutriscore = \"a\"\n    elif ((row[\"calc_global_score\"] >= 0) & (row[\"calc_global_score\"] < 5)) :\n        nutriscore = \"b\"\n    elif ((row[\"calc_global_score\"] >= 5) & (row[\"calc_global_score\"] < 10)) :\n        nutriscore = \"c\"\n    elif ((row[\"calc_global_score\"] >= 10) & (row[\"calc_global_score\"] < 20)) :\n        nutriscore = \"d\"\n    else:\n        nutriscore = \"e\"\n        \n    return nutriscore","25af625f":"datas_cleaned['calc_global_score'] = datas_cleaned.apply(lambda row: calc_globalscore(row),axis=1)\ndatas_cleaned['calc_nutriscore'] = datas_cleaned.apply(lambda row: calc_nutriscore(row),axis=1)","1f34bbd6":"nutriscore_features.append('calc_global_score')\nnutriscore_features.append('calc_nutriscore')","166be6cb":"datas_cleaned[nutriscore_features].sample(10)","84eff227":"df_scores = datas_cleaned[['nutriscore_grade', 'nutriscore_score', 'calc_nutriscore', 'calc_global_score']][datas_cleaned['nutriscore_grade'].isnull()==False]\n\nfrom sklearn.metrics import accuracy_score\n\naccuracy_nutrigrade = accuracy_score(df_scores['nutriscore_grade'].values, df_scores['calc_nutriscore'].values)\nprint(\"L'accuracy_score sur les Nutrigrades calcul\u00e9s est de : {:.2f} %.\".format(accuracy_nutrigrade*100))","006fb69d":"datas_cleaned.drop(['calc_nutriscore','calc_global_score'], axis=1, inplace=True)","0cd7fed6":"datas_cleaned.info()","ce66fb1f":"datas_cleaned.set_index(\"code\").to_csv(\"cleaned_openfoodfacts.csv\", sep='\\t')","5b89245b":"Nous pouvons \u00e0 pr\u00e9sent regarder la **distribution des contributions par ann\u00e9e** :","d30cc1ba":"### <font color=\"#ea1c60\" id=\"section_1\">1.1. Colonnes et lignes vides \/ Doublons<\/font>\nRegardons \u00e0 pr\u00e9sent les **variables qui comptent le plus de valeurs null**:","2569a4b9":"**La France et les Etats-Unis sont massivement repr\u00e9sent\u00e9s** dans le Dataset. Le 10\u00e8me Pays repr\u00e9sent\u00e9 en terme de volume ne compte plus que 5 500 entr\u00e9es - soit 100 fois moins que le 1er. Cette diff\u00e9rence devra \u00eatre prise en compte dans les futurs mod\u00e8les d\u00e9velopp\u00e9s. ","d229a292":"Les variables imput\u00e9es \u00e9tant bien distribu\u00e9es conform\u00e9ment aux variables d'origine, nous allons \u00e0 pr\u00e9sent **supprimer les derni\u00e8res variables non pertinentes**, peu renseign\u00e9es ou inutiles dans nos futurs mod\u00e8les :","eb5f9051":"La variable indiquant la proportion de fruit\/l\u00e9gumes des produits a \u00e9t\u00e9 supprim\u00e9e car tr\u00e8s peu renseign\u00e9e. Nous allons donc tenter de l'estimer en fonction de la cat\u00e9gorie :","aa33d4d3":"Nous allons charger une **base de donn\u00e9es des pays, en Anglais**, pour d\u00e9terminer les pays qui ne matchent pas *(mauvaise orthographe ou autre)*.","63a1bcd1":"On remarque d\u00e9j\u00e0 que les valeurs minimum et maximum de **certaines variables** (comme les nutriments ou la valeur energetique) **peuvent contenir des valeurs aberrantes**, ce qui impacte \u00e9galement l'\u00e9cart-type et donc la variance.","dedfdbea":"On remarque dans ces quelques exemples que les variables sont bien redondantes. Nous allons donc, lorsque la colonne suffix\u00e9e `_en` existe, supprimer toutes les autres colonnes identiques non suffix\u00e9es ou contenant `_tags` :","3961831e":"Ces repr\u00e9sentations nous indiquent clairement les \u00e9ventuels **outliers qui diff\u00e8rent de la valeur m\u00e9diane de plus de 5 fois l'\u00e9cart-type ($5\\sigma$ - valeur s\u00e9lectionn\u00e9e apr\u00e8s plusieurs essais)**.","38e58f9a":"Le jeu de donn\u00e9es t\u00e9l\u00e9charg\u00e9 en CSV sur le site [Open Food Facts](https:\/\/world.openfoodfacts.org\/) est import\u00e9 dans le Notebook via la m\u00e9thode `read_csv` de la librairie `pandas`","27d1d46d":"A pr\u00e9sent, notre dataset compte 40 variables qui ne contienent plus de redondance et 1 124 710 lignes. Nous allons maintenant nous int\u00e9resser \u00e0 une variable importante : le pays.","2311f57c":"Ici \u00e9galement, nous allons regarder les **cat\u00e9gories les plus repr\u00e9sent\u00e9es**, apr\u00e8s avoir remplac\u00e9 les valeurs nulles et compl\u00e9t\u00e9 les valeurs inconnues des pnns avec la cat\u00e9gorie principale :","751db860":"Les ann\u00e9es de 2012 \u00e0 2016 sont visiblement bien moins repr\u00e9sent\u00e9es que les ann\u00e9es 2017 \u00e0 2019. La mise en place du Nutri-Score a \u00e9t\u00e9 vot\u00e9e dans le cadre de la **loi Sant\u00e9 de 2016**.","868deb58":"Les variables suffix\u00e9es avec `_100g` nous indiquent la quantit\u00e9 de nutriment pour 100 grammes de produit. Les valeurs renseign\u00e9es ne peuvent donc logiquement pas exc\u00e9der 100. Nous allons donc **supprimer les lignes dont au moins 1 des variables de nutriments est sup\u00e9rieur au seuil** :","f03c2f02":"On voit ici que certaines cat\u00e9gories sont pr\u00e9sentes plusieurs fois mais orthographi\u00e9es diff\u00e9rement :\n- 'Cereals and potatoes' et 'cereals-and-potatoes'\n- 'fruits-and-vegetables' et 'Fruits and vegetables'\n- ...\n\nNous allons donc corriger le probl\u00e8me en passant le texte en miniscule et en rempla\u00e7ant les caract\u00e8res sp\u00e9ciaux par un espace :","3e93873b":"Plusieurs pays sont regroup\u00e9s dans la m\u00eame variable, nous allons devoir effectuer un split du texte pour visualiser tout nos pays :","b520d6f5":"### <font color=\"#ea1c60\" id=\"section_5\">1.5. Compl\u00e9ter les valeurs manquantes ou aberrantes<\/font>\n\nLe jeu de donn\u00e9es est nettoy\u00e9 mais comme le montre les informations ci-dessus, il reste un grand nombre de valeurs manquantes qu'il va falloir compl\u00e9ter mais \u00e9galement des outliers \u00e0 identifier.\n\nRegardons dans un premier temps une rapide description du Dataset :","9fc084da":"Enfin, les densit\u00e9 d'energie maximales pour 100g sont de 3700 kJ\/100g *(source [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Food_energy))*. Nous allons donc supprimer toutes les lignes dont la variable `energy_100g` est sup\u00e9rieur \u00e0 3700 *(ou 900 kcal\/100g)*.","31b9e690":"**Les dates \u00e9galement comportent une certaine redondance**. Entre les timestamp et les dates au format \"yyyy-mm-dd\", il est n\u00e9cessaire d'en \u00e9liminer :","63a745c2":"Nous allons donc **isoler les variables n\u00e9cessaires au calcul du Nutriscore et v\u00e9rifier leur taux de remplissage** :","6a36908e":"A pr\u00e9sent, nous allons d\u00e9finir la **fonction de calcul du Nutriscore** et l'appliquer \u00e0 notre dataset :","492710e0":"Le nombre de pays repr\u00e9sent\u00e9 est \u00e0 pr\u00e9sent plus coh\u00e9rent et la perte d'informations est limit\u00e9e. Il nous reste \u00e0 pr\u00e9sent \u00e0 **compl\u00e9ter les valeurs nulles pour cette variable**.","e7cc0d8b":"## <font color=\"#00afe6\">Sommaire<\/font>\n[1.1. Colonnes et lignes vides \/ Doublons](#section_1)     \n[1.2. Variables redondantes](#section_2)     \n[1.3. Les pays de l'analyse](#section_3)     \n[1.4. Les cat\u00e9gories de produits](#section_4)          \n[1.5. Compl\u00e9ter les valeurs manquantes ou aberrantes](#section_5)     \n[1.6. Calcul des Nutri-Scores manquants](#section_6)     ","5c72b3a2":"155 features sur 184 ont un taux de valeurs nulles sup\u00e9rieur \u00e0 50%, ce qui repr\u00e9sente une tr\u00e8s grosse part de donn\u00e9es manquantes. **La majeure partie de ces variables ne peuvent pas \u00eatre recalcul\u00e9es** puisqu'il s'agit de mesures physico-chimiques directes sur les produits.\n\nIl reste \u00e0 pr\u00e9sent 50 features dans notre dataset. Pour la suite des analyses, nous aurons besoin d'un minimum d'informations sur **les nutriments** des produits. Ces donn\u00e9es sont **suffix\u00e9es avec** `_100g`. Nous allons donc **conserver les lignes pour lesquelles au moins une de ces features est compl\u00e9t\u00e9e** :","b1d42e1c":"Pour les variables restantes, nous allons nous baser sur la m\u00e9diane et l'\u00e9cart-type pour \u00e9liminer les outliers. Commen\u00e7ons par afficher la dispersion des donn\u00e9es et ces m\u00e9triques :","58f71045":"Il reste tout de m\u00eame **510 484 produits qui ne contiennent aucune indication de cat\u00e9gorie**.\n\nAfin de pouvoir calculer des indicateurs empiriques correctes, nous allons remplacer par des valeurs nulles tous les max et min aberrants *(0, -1 ou 999999...)* :","9b4c6998":"Ici, ce produit qui poss\u00e8de la plus grande valeur energ\u00e9tique n'est quasi pas compl\u00e9t\u00e9. Le manque le nom du produit, les cat\u00e9gories et toutes les valeurs nutritionnelles. Nous allons **supprimer tous les produits qui n'ont ni nom, ni cat\u00e9gorie** et qui ne pourront donc \u00eatre clairement identifi\u00e9s.","a3ef48b3":"### <font color=\"#ea1c60\" id=\"section_4\">1.4. Les cat\u00e9gories de produits<\/font>\n\nInt\u00e9ressons nous \u00e0 pr\u00e9sent \u00e0 la cat\u00e9gorisation des produits. 4 variables repr\u00e9sentent cette cat\u00e9gorisation de mani\u00e8re plus ou moins pr\u00e9cise : `categories_en`, `main_category_en`, `pnns_groups_1`, `pnns_groups_2`.","b77bb181":"# <font color=\"#114b98\">Conception d'une application au service de la sant\u00e9 publique<\/font>\n\n![Sante-publique-France-logo.png](attachment:2a8179b9-00b5-4c08-b0d8-d33ae5398f20.png)\n\nL'agence **\"Sant\u00e9 publique France\"** a lanc\u00e9 un appel \u00e0 projets pour trouver des **id\u00e9es innovantes d\u2019applications en lien avec l'alimentation**.\n\nNous allons dans un premier temps **traiter le jeu de donn\u00e9es afin de rep\u00e9rer des variables pertinentes** pour les traitements \u00e0 venir et **automatiser ces traitements** pour \u00e9viter de r\u00e9p\u00e9ter ces op\u00e9rations.","47f73bce":"Apr\u00e8s ces quelques op\u00e9rations de nettoyage, les valeurs m\u00e9dianes, d'\u00e9carts-type, de minimum et maximum sont plus coh\u00e9rentes. Nous allons pouvoir exploiter ces donn\u00e9es pour **estimer les valeurs manquantes**.\n\nNous allons d\u00e9j\u00e0 regarder la **distribution de ces variables et d\u00e9finir si elles suivent une loi normale**, ce qui nous aidera \u00e0 prendre une d\u00e9cision pour la m\u00e9thode d'imputation des valeurs manquantes.","ef6a2f09":"Nous devons \u00e0 pr\u00e9sent **v\u00e9rifier les erreurs d'imputation sur les scores d\u00e9j\u00e0 connus**, ce qui nous permettra de voir si les calculs sont satisfaisants et utilisables :","b2e0e65d":"Nous allons regarder le **nombre total de cat\u00e9gories repr\u00e9sent\u00e9es** :","33a555bd":"La pr\u00e9cision du calul est donc inf\u00e9rieure \u00e0 50%. **Ces calculs ne peuvent donc pas \u00eatre utilis\u00e9s pour compl\u00e9ter nos donn\u00e9es**.     **<font color=\"green\">Notre application aura donc pour but d'\u00e9stimer le nutrigrade d'un produit en fonction de ses caract\u00e9ristiques connues, comme la cat\u00e9gorie, sa teneur en nutriments ... gr\u00e2ce \u00e0 des algorithmes simples de Machine Learning<\/font>**. ","fd47b677":"Les valeurs manquantes \u00e9tant \u00e0 pr\u00e9sent toutes compl\u00e9t\u00e9es pour ces variables num\u00e9riques, nous allons v\u00e9rifier que les distributions n'ont pas chang\u00e9es :","42b5ccd0":"<table width=\"100%\" style=\"border:solid 1px; text-align:center;\" align=\"center\">\n    <thead style=\"border:solid 1px; text-align:center;\">\n        <th bgcolor=\"#feadb3\"><b>Points<\/b><\/th>\n        <th bgcolor=\"#feadb3\">Energie (kJ)<\/th>\n        <th bgcolor=\"#feadb3\">Sucres simples (g)<\/th>\n        <th bgcolor=\"#feadb3\">Acide gras satur\u00e9s (g)<\/th>\n        <th bgcolor=\"#feadb3\">Sodium (mg)<\/th>\n        <th bgcolor=\"#b6faa3\"><b>Points<\/b><\/th>\n        <th bgcolor=\"#b6faa\">Fruits,leg(%)<\/th>\n        <th bgcolor=\"#b6faa\">Fibres (g)<\/th>\n        <th bgcolor=\"#b6faa\">Prot\u00e9ines (g)<\/th>\n    <\/thead>\n    <tbody>\n        <tr>\n            <td bgcolor=\"#feadb3\"><b>0<\/b><\/td>\n            <td bgcolor=\"#feadb3\">$\\leq$ 335<\/td>\n            <td bgcolor=\"#feadb3\">$\\leq$ 4,5<\/td>\n            <td bgcolor=\"#feadb3\">$\\leq$ 1<\/td>\n            <td bgcolor=\"#feadb3\">$\\leq$ 90<\/td>\n            <td bgcolor=\"#b6faa\"><b>0<\/b><\/td>\n            <td bgcolor=\"#b6faa\">$\\leq$ 40<\/td>\n            <td bgcolor=\"#b6faa\">$\\leq$ 0,7<\/td>\n            <td bgcolor=\"#b6faa\">$\\leq$ 1,6<\/td>\n        <\/tr>\n        <tr>\n            <td bgcolor=\"#feadb3\"><b>5<\/b><\/td>\n            <td bgcolor=\"#feadb3\">$>$ 335<\/td>\n            <td bgcolor=\"#feadb3\">$>$ 4,5<\/td>\n            <td bgcolor=\"#feadb3\">$>$ 1<\/td>\n            <td bgcolor=\"#feadb3\">$>$ 90<\/td>\n            <td bgcolor=\"#b6faa\"><b>-2<\/b><\/td>\n            <td bgcolor=\"#b6faa\">$>$ 40<\/td>\n            <td bgcolor=\"#b6faa\">$>$ 0,7<\/td>\n            <td bgcolor=\"#b6faa\">$>$ 1,6<\/td>\n        <\/tr>\n        <tr>\n            <td bgcolor=\"#feadb3\"><b>10<\/b><\/td>\n            <td bgcolor=\"#feadb3\">$>$ 1675<\/td>\n            <td bgcolor=\"#feadb3\">$>$ 22,5<\/td>\n            <td bgcolor=\"#feadb3\">$>$ 5<\/td>\n            <td bgcolor=\"#feadb3\">$>$ 450<\/td>\n            <td bgcolor=\"#b6faa\"><b>-5<\/b><\/td>\n            <td bgcolor=\"#b6faa\">$>$ 80<\/td>\n            <td bgcolor=\"#b6faa\">$>$ 3,5<\/td>\n            <td bgcolor=\"#b6faa\">$>$ 8<\/td>\n        <\/tr>\n        <tr style=\"border:solid 1px; text-align:center;\">\n            <td> <\/td>\n            <td>(a)<\/td>\n            <td>(b)<\/td>\n            <td>(c)<\/td>\n            <td>(d)<\/td>\n            <td> <\/td>\n            <td>(e)<\/td>\n            <td>(f)<\/td>\n            <td>(g)<\/td>\n        <\/tr>\n        <tr>\n            <td><b>Total<\/b><\/td>\n            <td colspan=\"8\"><b>(a) + (b) + (c) + (d) + (e) + (f) + (g)<\/b><\/td>\n        <\/tr>\n    <\/tbody>\n<\/table>\n\nAttribution des classes *(version simplifi\u00e9e)* :\n\n<table width=\"50%\" style=\"border:solid 1px; text-align:center;\" align=\"center\">\n    <thead>\n        <th>Aliment solide<\/th>\n        <th>Lettre<\/th>\n        <th>Couleur<\/th>\n    <\/thead>\n    <tbody>\n        <tr>\n            <td>Mimimum -1<\/td>\n            <td>A<\/td>\n            <td>Vert fonc\u00e9<\/td>\n        <\/tr>\n        <tr>\n            <td>0 \u00e0 5<\/td>\n            <td>B<\/td>\n            <td>Vert<\/td>\n        <\/tr>\n        <tr>\n            <td>5 \u00e0 10<\/td>\n            <td>C<\/td>\n            <td>Jaune<\/td>\n        <\/tr>\n        <tr>\n            <td>10 \u00e0 20<\/td>\n            <td>D<\/td>\n            <td>Orange<\/td>\n        <\/tr>\n        <tr>\n            <td>20 et plus<\/td>\n            <td>E<\/td>\n            <td>Orange fonc\u00e9<\/td>\n        <\/tr>\n    <\/tbody>\n<\/table>","d2cc0e42":"Affichons \u00e0 pr\u00e9sent quelques lignes au hasard du dataset puis regardons sa `shape`","30248687":"En se basant sur les projections obtenus et les r\u00e9sultats des tests de Kolmogorov-Smirnov (Pvalue < au niveau de test de 5%) **on rejette donc l'hypoth\u00e8se de normalit\u00e9 des distributions de ces variables**. Il serait donc inexacte d'imputer les valeurs manquantes par la moyenne.\n\nPour confirmer cette approche, regardons \u00e0 pr\u00e9sent quelque unes de ces distributions en fonction de la cat\u00e9gorie `pnns_groups_1` :","e1284d30":"Pour ces valeurs nulles ci-dessus, les variables `serving_quantity` et `additives_n` sont tr\u00e8s peu renseign\u00e9es, nous allons donc les supprimer de notre jeu de donn\u00e9es.     \n`fiber_100g` est \u00e9galement mal renseign\u00e9 mais nous en aurons besoin pour la suite. Nous allons donc compl\u00e9ter les valeurs nulles par la **m\u00e9diane de la cat\u00e9gorie** `pnns_groups_2`.     \nEnfin, pour les autres variables, avec peu de null et dont les distributions ne suivent pas la loi gaussiene, nous allons imputer avec l'**algorithme des K Nearest Neighbours** (KNN).","18e6fe47":"Les variables sont \u00e0 pr\u00e9sent filtr\u00e9es et les donn\u00e9es nettoy\u00e9es et compl\u00e9t\u00e9es. Nous allons sauvegarder le dataset cleaned pour l'utiliser \u00e0 pr\u00e9sent dans nos **analyses exploratoires**.","5565ae68":"### <font color=\"#ea1c60\" id=\"section_2\">1.2. Variables redondantes<\/font>\n\nCertaines variables dans le dataset sont en r\u00e9alit\u00e9 des variables redondantes. C'est par exemple le cas des variables suffix\u00e9es par `_tags` ou `_en` qui ne font que reprendre d'autres features traduites ou simplifi\u00e9es.\n\nC'est par exemple de cas des cat\u00e9gories produits :","36b2017a":"puis nous allons **supprimer ces pays \"fant\u00f4mes\" de la base de donn\u00e9es** :","75578a51":"### <font color=\"#ea1c60\" id=\"section_6\">1.6. Calcul des Nutri-Scores manquants<\/font>\n\nUne des variables primoridale de ce jeu de donn\u00e9es est le Nutriscore. Or, cette variable compte beaucoup de manquants. \nAfin de calculer ce nutriscore, nous avons besoin de plusieurs autres variables.\n\nLe score prend en comptepour 100g de produit, la teneur :\n- En nutriments et aliments \u00e0 favoriser (fibres, prot\u00e9ines, fruits et l\u00e9gumes),\n- En nutriments \u00e0 limiter (\u00e9nergie, acide gras satur\u00e9s, sucres, sel).\n\nApr\u00e8s calcul, le score obtenu par un produit permet de lui attribuer une lettre et une couleur.\nVoici un tableau simplifi\u00e9 permettant de calculer l'attribution des points :","a5416679":"Notre jeu de donn\u00e9es est \u00e0 pr\u00e9sent d\u00e9barass\u00e9 des lignes et colonnes peu compl\u00e9t\u00e9es. Nous allons \u00e0 pr\u00e9sent **rechercher et supprimer les doublons**.","65a0bc5a":"Nous pouvons \u00e9galement regarder quels sont les **pays les plus repr\u00e9sent\u00e9s** :","074f88f7":"Il existe donc \u00e9galement une grande disparit\u00e9 dans les cat\u00e9gories repr\u00e9sent\u00e9es et les \"unknown\", les cat\u00e9gories inconnues l'emportent. \n\nPour une meilleure visualisation, nous allons repr\u00e9senter le top 100 des cat\u00e9gories dans un nuage de mots gr\u00e2ce \u00e0 la librairie `WordCloud` :","3ba4701f":"Si l'on regarde la distribution de quelques unes de ces m\u00eames variables sur la cat\u00e9gorie `pnns_groups_2` :","538deb62":"### <font color=\"#ea1c60\" id=\"section_3\">1.3. Les pays de l'analyse<\/font>\n\nRegardons tout d'abord le nombre de pays repr\u00e9sent\u00e9s *(s'il est trop \u00e9lev\u00e9, c'est que des erreurs existent)*","38b7f451":"## <font color=\"#00afe6\">1. Nettoyage des donn\u00e9es<\/font>","42cae3b0":"Nous allons regarder le taux de remplissage des variables graphiquement et fixer un **seuil de suppression \u00e0 25% de taux de remplissage** :","0689bbd6":"Passons \u00e0 pr\u00e9sent aux variables `pnns_groups_1` et `pnns_groups_2` qui nous apporteront des informations plus pr\u00e9cises :","b137a7a7":"D'autre part, les valeurs de `saturated-fat_100g` ne peuvent pas \u00eatre sup\u00e9rieurs \u00e0 `fat_100g`, de m\u00eame pour les valeurs de `sodium_100g` qui ne peuvent pas \u00eatre sup\u00e9rieurs \u00e0 `salt_100g`.     \nNous allons donc supprimer les lignes qui remplissent ces conditions :"}}