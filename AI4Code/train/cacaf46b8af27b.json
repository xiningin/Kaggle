{"cell_type":{"0a56c3fc":"code","3ae840a2":"code","44f7a7cd":"code","ca541656":"code","649b16c1":"code","4b2e36d8":"code","b890ad69":"code","4152f819":"code","951964e3":"code","2b32a362":"code","d18f9d90":"code","5d649c6a":"code","3730f915":"code","60c11fad":"code","a48d478e":"code","69eca645":"code","a539df8a":"code","8d2800ff":"code","e21dc768":"code","34ef61d6":"code","2351b6d1":"code","3a7ac228":"code","0b6273b6":"code","1a01317d":"code","8389c21d":"code","3610e8ba":"code","91c9ad5a":"code","fdfa81e1":"code","3dcc3ace":"code","6dc93c87":"markdown","969c5d27":"markdown","3f45af4c":"markdown","4687105f":"markdown"},"source":{"0a56c3fc":"!pip --quiet install ..\/input\/treelite\/treelite-0.93-py3-none-manylinux2010_x86_64.whl\n!pip --quiet install ..\/input\/treelite\/treelite_runtime-0.93-py3-none-manylinux2010_x86_64.whl\n","3ae840a2":"import pandas as pd\nimport numpy as np\nimport gc\n\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler, QuantileTransformer\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport optuna\n\nimport random\nimport pathlib\nfrom tqdm import tqdm\nfrom typing import List, NoReturn, Union, Tuple, Optional, Text, Generic, Callable, Dict\n\nimport lightgbm as lgbm\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n#plt.style.use('fivethirtyeight')\nimport xgboost as xgb\nimport sklearn\nimport random\nimport janestreet\nimport tensorflow as tf\n\n# treelite\nimport treelite\nimport treelite_runtime\n\nimport warnings\nwarnings.filterwarnings('ignore')","44f7a7cd":"##TREELITE","ca541656":"SEED=1111\nNFOLD = 4","649b16c1":"random.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)","4b2e36d8":"INPUT_DIR = '..\/input\/janestreet-save-as-feather\/'","b890ad69":"#train = pd.read_csv(\"\/kaggle\/input\/jane-street-market-prediction\/train.csv\")\n\n# load data blitz fast!\ndef load_data(input_dir=INPUT_DIR):\n    train = pd.read_feather(pathlib.Path(input_dir + 'train.feather'))\n    features = pd.read_feather(pathlib.Path(input_dir + 'features.feather'))\n    example_test = pd.read_feather(pathlib.Path(input_dir + 'example_test.feather'))\n    ss = pd.read_feather(pathlib.Path(input_dir + 'example_sample_submission.feather'))\n    return train, features, example_test, ss\n\ntrain, features, example_test, ss = load_data(INPUT_DIR)","4152f819":"# delete irrelevant files to save memory\ndel features, example_test, ss\ngc.collect()\ntrain.shape","951964e3":"train.head(50)","2b32a362":"#train = train.drop(['feature_113','feature_89','feature_101'], 1)","d18f9d90":"train = train.query('date > 85').reset_index(drop = True) \ntrain = train[train['weight'] != 0]\n\n#train.fillna(train.mean(),inplace=True)\n\ntrain['action'] = ((train['resp'].values) > 0).astype(int)\n\n\nfeatures = [c for c in train.columns if \"feature\" in c]\n","5d649c6a":"train.fillna(train.mean(),inplace=True)","3730f915":"features.remove('feature_0')\n","60c11fad":"#features.remove('feature_48')\n#features.remove('feature_45')\n#features.remove('feature_3')","a48d478e":"len(features)","69eca645":"train.shape","a539df8a":"train['resp'] = (((train['resp'].values)*train['weight']) > 0).astype(int)\ntrain['resp_1'] = (((train['resp_1'].values)*train['weight']) > 0).astype(int)\ntrain['resp_2'] = (((train['resp_2'].values)*train['weight']) > 0).astype(int)\ntrain['resp_3'] = (((train['resp_3'].values)*train['weight']) > 0).astype(int)\ntrain['resp_4'] = (((train['resp_4'].values)*train['weight']) > 0).astype(int)","8d2800ff":"\nf_mean = np.mean(train[features[1:]].values,axis=0)\n\nresp_cols = ['resp_1', 'resp_2', 'resp_3', 'resp']\n\n#X_train = train.loc[:, train.columns.str.contains('feature')]\n","e21dc768":"#features.extend(['cross_41_42_43', 'cross_1_2'])","34ef61d6":"len(features)","2351b6d1":"X_train=train[features].values\n#y_train = (train.loc[:, 'action'])\n\ny_train = np.stack([(train[c] > 0).astype('int') for c in resp_cols]).T\nprint(y_train.shape)\n\n## these are numpy arrays now","3a7ac228":"import random\nfrom collections import Counter, defaultdict\nfrom sklearn import model_selection\n\n# ---- GroupKFold ----\nclass GroupKFold(object):\n    \"\"\"\n    GroupKFold with random shuffle with a sklearn-like structure\n    \"\"\"\n\n    def __init__(self, n_splits=4, shuffle=True, random_state=42):\n        self.n_splits = n_splits\n        self.shuffle = shuffle\n        self.random_state = random_state\n\n    def get_n_splits(self, X=None, y=None, group=None):\n        return self.n_splits\n\n    def split(self, X, y, group):\n        kf = model_selection.KFold(n_splits=self.n_splits, shuffle=self.shuffle, random_state=self.random_state)\n        unique_ids = X[group].unique()\n        for fold, (tr_group_idx, va_group_idx) in enumerate(kf.split(unique_ids)):\n            # split group\n            tr_group, va_group = unique_ids[tr_group_idx], unique_ids[va_group_idx]\n            train_idx = np.where(X[group].isin(tr_group))[0]\n            val_idx = np.where(X[group].isin(va_group))[0]\n            yield train_idx, val_idx\n\n","0b6273b6":"# modeling step \nparams={\"num_leaves\":300,\n       \"max_bin\":450,\n       \"feature_fraction\":0.52,\n       \"bagging_fraction\":0.52,\n       \"objective\":\"binary\",\n       \"learning_rate\":0.05,\n       \"boosting_type\":\"gbdt\",\n       \"metric\":\"auc\"\n       }\n\n## should we split by k-fold as well?\n# because the tree is definitely overfitting\n# it just ekes out a good enough signal in the end\n\ncv = GroupKFold(n_splits=NFOLD, shuffle=True, random_state=SEED)\ngroup = 'date'\ntarget = 'action'\noof = np.zeros(train.shape[0])\nmodels = []\n\n## OUTER FOLD\nfor fold, (train_idx, val_idx) in tqdm(enumerate(cv.split(train, train[target], group))):\n    # train test split\n#     x_train, x_val = train[features].iloc[train_idx], train[features].iloc[val_idx]\n#     y_train, y_val = train[target].iloc[train_idx], train[target].iloc[val_idx]\n    \n    \n    xtr,xval = X_train[train_idx,:], X_train[val_idx,:]\n    ytr,yval = y_train[train_idx,:], y_train[val_idx,:]\n    \n    ## ensemble by y_train\n    for i in range(y_train.shape[1]):\n        print('MODEL: ', str(fold)+'+'+str(i))\n        #xtr,xval,ytr,yval = train_test_split(X_train ,y_train[:,i],test_size=0.2,stratify=y_train[:,i])\n\n        d_train = lgbm.Dataset(xtr,label=ytr[:,i])\n        d_eval = lgbm.Dataset(xval,label=yval[:,i],reference=d_train)\n        clf = lgbm.train(params,d_train,valid_sets=[d_train,d_eval],num_boost_round=1000,\\\n                        early_stopping_rounds=50,verbose_eval=50)\n        clf.save_model('model_fold_'+str(fold)+'_'+str(i)+'.txt')\n\n        models.append(clf)\n    \n    ## there is technically a holdout set in y_val x_val now here...\n","1a01317d":"## evaluate model's raw classification accuracy\n# for i in range(y_train.shape[1]):\n#     xtr,xval,ytr,yval = train_test_split(X_train ,y_train[:,i],test_size=0.2,stratify=y_train[:,i])\n   \n#     d_train = lgbm.Dataset(xtr,label=ytr)\n#     d_eval = lgbm.Dataset(xval,label=yval,reference=d_train)\n#     for model in models:\n#         model.predict(d_eval);\n    ","8389c21d":"# log_models = []\n# from sklearn.metrics import roc_auc_score\n# for i in range(y_train.shape[1]):\n#     print('model: ',i)\n#     xtr,xval,ytr,yval = train_test_split(X_train ,y_train[:,i],test_size=0.2,stratify=y_train[:,i])\n\n#     logreg = LogisticRegression(max_iter = 2000).fit(xtr, ytr);\n#     print(logreg.score(xval, yval), roc_auc_score(yval, logreg.predict(xval)));\n#     log_models.append(clf)","3610e8ba":"fig,ax = plt.subplots(figsize=(25,50))\nlgbm.plot_importance(clf, ax=ax,importance_type='gain',max_num_features=130)\nplt.show()","91c9ad5a":"## treelite post\npredictors = []\nfor fold in range(NFOLD):\n    # load LGB with Treelite\n    for i in range(y_train.shape[1]):\n\n        model = treelite.Model.load('model_fold_'+str(fold)+'_'+str(i)+'.txt', model_format='lightgbm')\n\n        # generate shared library\n        toolchain = 'gcc'\n        model.export_lib(toolchain=toolchain, libpath=f'.\/mymodel{fold}.so',\n                         params={'parallel_comp': 32}, verbose=True)# predictor from treelite\n\n        # predictors\n        predictor = treelite_runtime.Predictor(f'.\/mymodel{fold}.so', verbose=True)\n        predictors.append(predictor)","fdfa81e1":"f = np.median\nth = 0.502; ## don't adjust we have the median on pred\nimport janestreet\nenv = janestreet.make_env()\nfor (test_df, pred_df) in env.iter_test():\n    if test_df['weight'].item() > 0:\n        x_tt = test_df.loc[:, features].values\n        #x_tt=test_df[features].values\n        if np.isnan(x_tt[:, 1:].sum()):\n            x_tt[:, 1:] = np.nan_to_num(x_tt[:, 1:]) + np.isnan(x_tt[:, 1:]) * f_mean\n        \n        #drop test_df\n        ## log_models or normal models\n        pred = np.mean([model.predict(x_tt) for model in models],axis=0)\n        #pred = f(pred)\n        pred_df.action = np.where(pred >= th, 1, 0).astype(int)\n    else:\n        pred_df.action = 0\n    env.predict(pred_df)","3dcc3ace":"#preds = clf.predict(xtr)\n#pred_labels = np.rint(preds)\n\n\n    \n#accuracy = sklearn.metrics.accuracy_score(ytr, pred_labels)\n","6dc93c87":"## read data in efficiently\n","969c5d27":"## Logistic Regression as well\n","3f45af4c":"## prediction structure\nDoes not change with the fold and multiple label ensemble","4687105f":"**Let us check important feature using logistic relation**\n"}}