{"cell_type":{"de59e224":"code","66517a81":"code","6a90518a":"code","9cca0ade":"code","8bca0e01":"code","796684fd":"code","b726b42f":"code","6242e69d":"code","5e5bc14c":"code","12e08b99":"code","bd66c782":"code","0eb74e72":"code","6136f8a8":"code","a6248cdb":"code","932dfb79":"code","aa8df367":"code","a7381865":"code","326f41b4":"code","781ce079":"code","cd77450c":"code","477854b7":"code","cbbc40fa":"code","9ddd55a8":"code","8518cca4":"code","2459d4f4":"code","710fd7c0":"code","4afe3a80":"code","a1c193c0":"code","69360e39":"code","5abeffdc":"code","bc22f262":"code","ed120b7f":"code","55c11e64":"code","dd130cbd":"code","1d374e0c":"code","5186bd4d":"code","c0989c8a":"code","e2d941e2":"code","a659aeff":"code","37dc5421":"code","7e191ab6":"code","02a4314e":"code","62990a5b":"code","94a07288":"code","1b5c4da5":"code","65c423c5":"code","418795d6":"code","d281839c":"code","bf972280":"code","fd6a7aaa":"markdown","d6b8da7c":"markdown","dfaf8160":"markdown","ece6a8fa":"markdown","e504a4a4":"markdown","da4ac7e8":"markdown","792ac69c":"markdown","eb29b56a":"markdown","6aeb10c3":"markdown","66b9b300":"markdown","2d9de5a1":"markdown","faca1e81":"markdown","3555b35e":"markdown","ff8c1956":"markdown","fe52c897":"markdown","e71fb783":"markdown","8f574406":"markdown","64ba23ab":"markdown","5d4cdecc":"markdown"},"source":{"de59e224":"# data science\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# regression models\nfrom sklearn.preprocessing import StandardScaler # for feature scaling\nfrom sklearn.pipeline import Pipeline # for using pipeline\nfrom sklearn.linear_model import LinearRegression # for linear regression\nfrom sklearn.preprocessing import PolynomialFeatures # for adding polynomial features\nfrom sklearn.linear_model import Ridge # for ridge regression\nfrom sklearn.linear_model import Lasso # for lasso regression\nfrom sklearn.svm import SVR # for support vector regression\nfrom sklearn.tree import DecisionTreeRegressor # for decisiton tree regression\nfrom sklearn.ensemble import RandomForestRegressor # for random forest regression\n# hyptertuning\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\n# extra\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom collections import Counter\nfrom IPython.core.display import display, HTML\nsns.set_style('darkgrid')","66517a81":"dataset = pd.read_csv('..\/input\/insurance\/insurance.csv')\ndataset.head()","6a90518a":"dataset.count()","9cca0ade":"dataset.describe()","8bca0e01":"dataset.isnull().sum()","796684fd":"corr = dataset.corr()\n#Plot figsize\nfig, ax = plt.subplots(figsize=(10, 8))\n#Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(corr, cmap='coolwarm', annot=True, fmt=\".2f\")\n#Apply xticks\nplt.xticks(range(len(corr.columns)), corr.columns);\n#Apply yticks\nplt.yticks(range(len(corr.columns)), corr.columns)\n#show plot\nplt.show()","b726b42f":"f, axes = plt.subplots(1,2,figsize=(14,4))\n\nsns.distplot(dataset['age'], ax = axes[0])\naxes[0].set_xlabel('Age', fontsize=14)\naxes[0].set_ylabel('Count', fontsize=14)\naxes[0].yaxis.tick_left()\n\nsns.scatterplot(x = 'charges', y = 'age', data = dataset, hue = 'smoker', ax = axes[1])\naxes[1].set_xlabel('Charges', fontsize=14)\naxes[1].set_ylabel('Age', fontsize=14)\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\n\nplt.show()","6242e69d":"f, axe = plt.subplots(1,1,figsize=(20,4))\nsns.boxenplot(x = 'age', y = 'charges', data = dataset, ax = axe)\naxe.set_xlabel('Age', fontsize=14)\naxe.set_ylabel('Charges', fontsize=14)\nplt.show()","5e5bc14c":"f, axes = plt.subplots(1,2,figsize=(14,4))\n\nsns.distplot(dataset['bmi'], ax = axes[0])\naxes[0].set_xlabel('Body Mass Index (BMI)', fontsize=14)\naxes[0].set_ylabel('Count', fontsize=14)\naxes[0].yaxis.tick_left()\n\nsns.scatterplot(x = 'charges', y = 'bmi', data = dataset, hue = 'sex',ax = axes[1])\naxes[1].set_xlabel('Charges', fontsize=14)\naxes[1].set_ylabel('Body Mass Index (BMI)', fontsize=14)\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\n\nplt.show()","12e08b99":"sex_list = Counter(dataset['sex'])\nlabels = sex_list.keys()\nsizes = sex_list.values()\n\nf, axes = plt.subplots(1,2,figsize=(14,4))\n\nsns.countplot(dataset['sex'], ax = axes[0], palette=\"Set1\")\naxes[0].set_xlabel('Sex', fontsize=14)\naxes[0].set_ylabel('Count', fontsize=14)\naxes[0].yaxis.tick_left()\n\nsns.boxenplot(x = 'sex', y = 'charges', data = dataset, hue = 'sex', ax = axes[1])\naxes[1].set_xlabel('Charges', fontsize=14)\naxes[1].set_ylabel('Sex', fontsize=14)\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\naxes[1].legend(bbox_to_anchor=(0.6,1), loc=1, borderaxespad=0.)\n\nplt.show()","bd66c782":"children_list = Counter(dataset['children'])\nlabels = children_list.keys()\nsizes = children_list.values()\n\nf, axes = plt.subplots(1,2,figsize=(14,4))\n\nsns.countplot(dataset['children'], ax = axes[0], palette=\"Set1\")\naxes[0].set_xlabel('Children', fontsize=14)\naxes[0].set_ylabel('Count', fontsize=14)\naxes[0].yaxis.tick_left()\n\nsns.boxplot(x = 'children', y = 'charges', data = dataset, ax = axes[1])\naxes[1].set_xlabel('Children', fontsize=14)\naxes[1].set_ylabel('Charges', fontsize=14)\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\n\nplt.show()","0eb74e72":"smoker_list = Counter(dataset['smoker'])\nlabels = smoker_list.keys()\nsizes = smoker_list.values()\n\nf, axes = plt.subplots(1,2,figsize=(14,4))\n\nsns.countplot(dataset['smoker'], ax = axes[0], palette=\"Set1\")\naxes[0].set_xlabel('Smoker', fontsize=14)\naxes[0].set_ylabel('Count', fontsize=14)\naxes[0].yaxis.tick_left()\n\nsns.violinplot(x = 'smoker', y = 'charges', data = dataset, hue = 'smoker', ax = axes[1])\naxes[1].set_xlabel('Smoker', fontsize=14)\naxes[1].set_ylabel('Charges', fontsize=14)\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\n\nplt.show()","6136f8a8":"region_list = Counter(dataset['region'])\nlabels = region_list.keys()\nsizes = region_list.values()\n\nf, axes = plt.subplots(1,2,figsize=(14,4))\n\nsns.countplot(dataset['region'], ax = axes[0], palette=\"Set1\")\naxes[0].set_xlabel('Region', fontsize=14)\naxes[0].set_ylabel('Count', fontsize=14)\naxes[0].yaxis.tick_left()\n\nsns.swarmplot(x = 'region', y = 'charges', data = dataset, hue = 'region', ax = axes[1])\naxes[1].set_xlabel('Region', fontsize=14)\naxes[1].set_ylabel('Charges', fontsize=14)\naxes[1].yaxis.set_label_position(\"right\")\naxes[1].yaxis.tick_right()\naxes[1].legend(bbox_to_anchor=(1.15, 1), loc=2, borderaxespad=0.)\n\nplt.show()","a6248cdb":"dataset = pd.get_dummies(dataset)","932dfb79":"dataset.head()","aa8df367":"X = dataset.drop('charges', axis = 1).values\ny = dataset['charges'].values.reshape(-1,1)","a7381865":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)","326f41b4":"print(\"Shape of X_train: \",X_train.shape)\nprint(\"Shape of X_test: \", X_test.shape)\nprint(\"Shape of y_train: \",y_train.shape)\nprint(\"Shape of y_test\",y_test.shape)","781ce079":"# Creating the linear regressor\nregressor_linear = LinearRegression()\nregressor_linear.fit(X_train, y_train)","cd77450c":"# Predicting Cross Validation Score the Test set results\ncv_linear = cross_val_score(estimator = regressor_linear, X = X, y = y, cv = 10)\n\n# Predicting R2 Score the Train set results\ny_pred_linear_train = regressor_linear.predict(X_train)\nr2_score_linear_train = r2_score(y_train, y_pred_linear_train)\n\n# Predicting R2 Score the Test set results\ny_pred_linear_test = regressor_linear.predict(X_test)\nr2_score_linear_test = r2_score(y_test, y_pred_linear_test)\n\n# Predicting RMSE the Test set results\nrmse_linear = (np.sqrt(mean_squared_error(y_test, y_pred_linear_test)))\nprint(\"CV: \", cv_linear.mean())\nprint('R2_score (train): ', r2_score_linear_train)\nprint('R2_score (test): ', r2_score_linear_test)\nprint(\"RMSE: \", rmse_linear)","477854b7":"# Creating the polynomial features and regressor\npoly_reg = PolynomialFeatures(degree = 2)\nX_poly = poly_reg.fit_transform(X)\nX_train_poly = poly_reg.fit_transform(X_train)\npoly_reg.fit(X_train_poly, y_train)\n\nregressor_poly2 = LinearRegression()\nregressor_poly2.fit(X_train_poly, y_train)","cbbc40fa":"# Predicting Cross Validation Score the Test set results\ncv_poly2 = cross_val_score(estimator = regressor_poly2, X = X_poly, y = y, cv = 10)\n\n# Predicting R2 Score the Train set results\ny_pred_poly2_train = regressor_poly2.predict(poly_reg.fit_transform(X_train))\nr2_score_poly2_train = r2_score(y_train, y_pred_poly2_train)\n\n# Predicting R2 Score the Test set results\ny_pred_poly2_test = regressor_poly2.predict(poly_reg.fit_transform(X_test))\nr2_score_poly2_test = r2_score(y_test, y_pred_poly2_test)\n\n# Predicting RMSE the Test set results\nrmse_poly2 = (np.sqrt(mean_squared_error(y_test, y_pred_poly2_test)))\nprint('CV: ', cv_poly2.mean())\nprint('R2_score (train): ', r2_score_poly2_train)\nprint('R2_score (test): ', r2_score_poly2_test)\nprint(\"RMSE: \", rmse_poly2)","9ddd55a8":"steps = [\n    ('scalar', StandardScaler()),\n    ('poly', PolynomialFeatures(degree=2)),\n    ('model', Ridge())\n]\n\nridge_pipe = Pipeline(steps)","8518cca4":"# Applying Grid Search to find the best model and the best parameters\n# step 1: alpha:[200, 230, 250,265, 270, 275, 290, 300, 500] -> 200\n# step 2: alpha:[10,50,100,150,200] -> 50\n# step 3: alpha: np.arange(30, 75, 1) -> 43\n\nparameters =  {  'model__alpha' : [43],\n                 'model__fit_intercept' : [True],\n                 'model__tol' : [0.0001],\n                 'model__solver' : ['auto'],\n                'model__random_state': [42] \n}\nregressor_ridge = GridSearchCV(ridge_pipe, parameters, iid=False, cv=10)\nregressor_ridge = regressor_ridge.fit(X, y.ravel())","2459d4f4":"print(regressor_ridge.best_score_)\nprint(regressor_ridge.best_params_)","710fd7c0":"# Predicting Cross Validation Score the Test set results\ncv_ridge = regressor_ridge.best_score_\n\n# Predicting R2 Score the Test set results\ny_pred_ridge_train = regressor_ridge.predict(X_train)\nr2_score_ridge_train = r2_score(y_train, y_pred_ridge_train)\n\n# Predicting R2 Score the Test set results\ny_pred_ridge_test = regressor_ridge.predict(X_test)\nr2_score_ridge_test = r2_score(y_test, y_pred_ridge_test)\n\n# Predicting RMSE the Test set results\nrmse_ridge = (np.sqrt(mean_squared_error(y_test, y_pred_ridge_test)))\nprint('CV: ', cv_ridge.mean())\nprint('R2_score (train): ', r2_score_ridge_train)\nprint('R2_score (test): ', r2_score_ridge_test)\nprint(\"RMSE: \", rmse_ridge)","4afe3a80":"steps = [\n    ('scalar', StandardScaler()),\n    ('poly', PolynomialFeatures(degree=2)),\n    ('model', Lasso())\n]\n\nlasso_pipe = Pipeline(steps)","a1c193c0":"# Applying Grid Search to find the best model and the best parameters\n# step 1: alpha:np.arange(0.01, 1, 0.005) -> 0.9949\n\nparameters =  {  'model__alpha' : [0.9949],\n                 'model__fit_intercept' : [True],\n                 'model__tol' : [0.0001],\n                 'model__max_iter' : [5000],\n                'model__random_state': [42] \n}\nregressor_lasso = GridSearchCV(lasso_pipe, parameters, iid=False, cv=10, n_jobs = -1, verbose = 4)\nregressor_lasso = regressor_lasso.fit(X, y.ravel())","69360e39":"# Predicting Cross Validation Score\ncv_lasso = regressor_lasso.best_score_\n\n# Predicting R2 Score the Test set results\ny_pred_lasso_train = regressor_lasso.predict(X_train)\nr2_score_lasso_train = r2_score(y_train, y_pred_lasso_train)\n\n# Predicting R2 Score the Test set results\ny_pred_lasso_test = regressor_lasso.predict(X_test)\nr2_score_lasso_test = r2_score(y_test, y_pred_lasso_test)\n\n# Predicting RMSE the Test set results\nrmse_lasso = (np.sqrt(mean_squared_error(y_test, y_pred_lasso_test)))\nprint('CV: ', cv_lasso.mean())\nprint('R2_score (train): ', r2_score_lasso_train)\nprint('R2_score (test): ', r2_score_lasso_test)\nprint(\"RMSE: \", rmse_lasso)","5abeffdc":"# Feature Scaling\nsc_X = StandardScaler()\nsc_y = StandardScaler()\nX_scaled = sc_X.fit_transform(X)\ny_scaled = sc_y.fit_transform(y.reshape(-1,1))","bc22f262":"# Creating the SVR regressor\nregressor_svr = SVR()","ed120b7f":"# Applying Grid Search to find the best model and the best parameters\nparameters =  { 'kernel' : ['rbf', 'sigmoid'],\n                 'gamma' : [0.001, 0.01, 0.1, 1, 'scale'],\n                 'tol' : [0.0001],\n                 'C': [0.001, 0.01, 0.1, 1, 10, 100] }\nregressor_svr = GridSearchCV(estimator = regressor_svr,\n                           param_grid = parameters,\n                           cv = 10,\n                           verbose = 4,\n                           iid = True,\n                           n_jobs = -1)\nregressor_svr = regressor_svr.fit(X_scaled, y_scaled.ravel())","55c11e64":"print(regressor_svr.best_params_)\nprint(regressor_svr.best_score_)","dd130cbd":"# Predicting Cross Validation Score\ncv_svr = regressor_svr.best_score_\n\n# Predicting R2 Score the Train set results\ny_pred_svr_train = sc_y.inverse_transform(regressor_svr.predict(sc_X.transform(X_train)))\nr2_score_svr_train = r2_score(y_train, y_pred_svr_train)\n\n# Predicting R2 Score the Test set results\ny_pred_svr_test = sc_y.inverse_transform(regressor_svr.predict(sc_X.transform(X_test)))\nr2_score_svr_test = r2_score(y_test, y_pred_svr_test)\n\n# Predicting RMSE the Test set results\nrmse_svr = (np.sqrt(mean_squared_error(y_test, y_pred_svr_test)))\nprint('CV: ', cv_svr.mean())\nprint('R2_score (train): ', r2_score_svr_train)\nprint('R2_score (test): ', r2_score_svr_test)\nprint(\"RMSE: \", rmse_svr)","1d374e0c":"# Creating the Decision Tree regressor\nregressor_dt = DecisionTreeRegressor(random_state = 42)","5186bd4d":"# Applying Grid Search to find the best model and the best parameters\nparameters = [ { \"max_depth\": np.arange(1,21),\n              \"min_samples_leaf\": [1, 5, 10, 20, 50, 100],\n              \"min_samples_split\": np.arange(2, 11),\n              \"criterion\": [\"mse\"],\n              \"random_state\" : [42]}\n            ]\nregressor_dt = GridSearchCV(estimator = regressor_dt,\n                           param_grid  = parameters,\n                           cv = 10,\n                           verbose = 4,\n                           iid = False,\n                           n_jobs = -1)\nregressor_dt = regressor_dt.fit(X_scaled, y_scaled)","c0989c8a":"print(regressor_dt.best_params_)\nprint(regressor_dt.best_score_)","e2d941e2":"# Predicting Cross Validation Score\ncv_dt = regressor_dt.best_score_\n\n# Predicting R2 Score the Train set results\ny_pred_dt_train = sc_y.inverse_transform(regressor_dt.predict(sc_X.transform(X_train)))\nr2_score_dt_train = r2_score(y_train, y_pred_dt_train)\n\n# Predicting R2 Score the Test set results\ny_pred_dt_test = sc_y.inverse_transform(regressor_dt.predict(sc_X.transform(X_test)))\nr2_score_dt_test = r2_score(y_test, y_pred_dt_test)\n\n# Predicting RMSE the Test set results\nrmse_dt = (np.sqrt(mean_squared_error(y_test, y_pred_dt_test)))\nprint('CV: ', cv_dt.mean())\nprint('R2_score (train): ', r2_score_dt_train)\nprint('R2_score (test): ', r2_score_dt_test)\nprint(\"RMSE: \", rmse_dt)","a659aeff":"# Creating the Random Forest regressor\nregressor_rf = RandomForestRegressor()","37dc5421":"# RANDOM SEARCH - STEP 1 - TOOK 17.6 MINUTES\n#parameters =  { \"n_estimators\": [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n#                \"max_features\": [\"auto\", \"sqrt\"],\n#                \"max_depth\": np.linspace(10, 110, num = 11),\n#                \"min_samples_split\": [2, 5, 10],\n#                \"min_samples_leaf\": [1, 2, 4],\n#                \"bootstrap\": [True, False],\n#                \"criterion\": [\"mse\"],\n#                \"random_state\" : [42] }\n#            \n#regressor_rf = RandomizedSearchCV(estimator = regressor_rf,\n#                                  param_distributions = parameters,\n#                                  n_iter = 100,\n#                                  cv = 10,\n#                                  random_state=42,\n#                                  verbose = 4,\n#                                  n_jobs = -1)\n#regressor_rf = regressor_rf.fit(X_scaled, y.ravel())\n#\n# Best Parameters and Score:\n# {'random_state': 42, 'n_estimators': 1200, 'min_samples_split': 10, 'min_samples_leaf': 4, 'max_features': 'auto', 'max_depth': 50.0, 'criterion': 'mse', 'bootstrap': True}\n# 0.8541297253461337","7e191ab6":"# GRID SEARCH - STEP 2 - TOOK 1.5 MINUTES\n#parameters =  { \"n_estimators\": [1200],\n#                \"max_features\": [\"auto\"],\n#                \"max_depth\": [50],\n#                \"min_samples_split\": [7,10,13],\n#                \"min_samples_leaf\": [4,7,10],\n#                \"bootstrap\": [True],\n#                \"criterion\": [\"mse\"],\n#                \"random_state\" : [42] }\n#            \n#regressor_rf = GridSearchCV(estimator = regressor_rf,\n#                                  param_grid = parameters,\n#                                  cv = 10,\n#                                  verbose = 4,\n#                                  n_jobs = -1)\n#regressor_rf = regressor_rf.fit(X_scaled, y.ravel())\n#\n# Best Parameters and Score:\n# {'bootstrap': True, 'criterion': 'mse', 'max_depth': 50, 'max_features': 'auto', 'min_samples_leaf': 10, 'min_samples_split': 7, 'n_estimators': 1200, 'random_state': 42}\n# 0.8587579970188238","02a4314e":"# Applying RandomSearch and GridSearch to find the best model and the best parameters\nparameters =  { \"n_estimators\": [1200],\n                \"max_features\": [\"auto\"],\n                \"max_depth\": [50],\n                \"min_samples_split\": [7],\n                \"min_samples_leaf\": [10],\n                \"bootstrap\": [True],\n                \"criterion\": [\"mse\"],\n                \"random_state\" : [42] }\n            \nregressor_rf = GridSearchCV(estimator = regressor_rf,\n                                  param_grid = parameters,\n                                  cv = 10,\n                                # verbose = 4,\n                                  n_jobs = -1)\nregressor_rf = regressor_rf.fit(X_scaled, y.ravel())","62990a5b":"print(regressor_rf.best_params_)\nprint(regressor_rf.best_score_)","94a07288":"from sklearn.metrics import r2_score\n\n# Predicting Cross Validation Score\ncv_rf = regressor_rf.best_score_\n\n# Predicting R2 Score the Train set results\ny_pred_rf_train = regressor_rf.predict(sc_X.transform(X_train))\nr2_score_rf_train = r2_score(y_train, y_pred_rf_train)\n\n# Predicting R2 Score the Test set results\ny_pred_rf_test = regressor_rf.predict(sc_X.transform(X_test))\nr2_score_rf_test = r2_score(y_test, y_pred_rf_test)\n\n# Predicting RMSE the Test set results\nrmse_rf = (np.sqrt(mean_squared_error(y_test, y_pred_rf_test)))\nprint('CV: ', cv_rf.mean())\nprint('R2_score (train): ', r2_score_rf_train)\nprint('R2_score (test): ', r2_score_rf_test)\nprint(\"RMSE: \", rmse_rf)","1b5c4da5":"models = [('Linear Regression', rmse_linear, r2_score_linear_train, r2_score_linear_test, cv_linear.mean()),\n          ('Polynomial Regression (2nd)', rmse_poly2, r2_score_poly2_train, r2_score_poly2_test, cv_poly2.mean()),\n          ('Ridge Regression', rmse_ridge, r2_score_ridge_train, r2_score_ridge_test, cv_ridge.mean()),\n          ('Lasso Regression', rmse_lasso, r2_score_lasso_train, r2_score_lasso_test, cv_lasso.mean()),\n          ('Support Vector Regression', rmse_svr, r2_score_svr_train, r2_score_svr_test, cv_svr.mean()),\n          ('Decision Tree Regression', rmse_dt, r2_score_dt_train, r2_score_dt_test, cv_dt.mean()),\n          ('Random Forest Regression', rmse_rf, r2_score_rf_train, r2_score_rf_test, cv_rf.mean())   \n         ]","65c423c5":"predict = pd.DataFrame(data = models, columns=['Model', 'RMSE', 'R2_Score(training)', 'R2_Score(test)', 'Cross-Validation'])\npredict","418795d6":"f, axe = plt.subplots(1,1, figsize=(18,6))\n\npredict.sort_values(by=['Cross-Validation'], ascending=False, inplace=True)\n\nsns.barplot(x='Cross-Validation', y='Model', data = predict, ax = axe, palette='viridis')\n#axes[0].set(xlabel='Region', ylabel='Charges')\naxe.set_xlabel('Cross-Validaton Score', size=16)\naxe.set_ylabel('Model')\naxe.set_xlim(0,1.0)\naxe.set_xticks(np.arange(0, 1.1, 0.1))\nplt.show()","d281839c":"f, axes = plt.subplots(2,1, figsize=(14,10))\n\npredict.sort_values(by=['R2_Score(training)'], ascending=False, inplace=True)\n\nsns.barplot(x='R2_Score(training)', y='Model', data = predict, palette='Blues_d', ax = axes[0])\n#axes[0].set(xlabel='Region', ylabel='Charges')\naxes[0].set_xlabel('R2 Score (Training)', size=16)\naxes[0].set_ylabel('Model')\naxes[0].set_xlim(0,1.0)\naxes[0].set_xticks(np.arange(0, 1.1, 0.1))\n\npredict.sort_values(by=['R2_Score(test)'], ascending=False, inplace=True)\n\nsns.barplot(x='R2_Score(test)', y='Model', data = predict, palette='Reds_d', ax = axes[1])\n#axes[0].set(xlabel='Region', ylabel='Charges')\naxes[1].set_xlabel('R2 Score (Test)', size=16)\naxes[1].set_ylabel('Model')\naxes[1].set_xlim(0,1.0)\naxes[1].set_xticks(np.arange(0, 1.1, 0.1))\n\nplt.show()","bf972280":"predict.sort_values(by=['RMSE'], ascending=False, inplace=True)\n\nf, axe = plt.subplots(1,1, figsize=(18,6))\nsns.barplot(x='Model', y='RMSE', data=predict, ax = axe)\naxe.set_xlabel('Model', size=16)\naxe.set_ylabel('RMSE', size=16)\n\nplt.show()","fd6a7aaa":"### <span id=\"13\"><\/span> ** Visualizing Models Performance **","d6b8da7c":"Columns:\n- **age: ** age of primary beneficiary \n- **sex: ** insurance contractor gender, female, male \n- **bmi: ** body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg \/ m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9 \n- **children: ** number of children covered by health insurance \/ number of dependents\n- **smoker: ** smoking\n- **region: ** the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n- **charges: ** individual medical costs billed by health insurance","dfaf8160":"### <span id=\"5\"><\/span> ** Linear Regression **","ece6a8fa":"### <span id=\"9\"><\/span> ** Support Vector Regression **","e504a4a4":"### <span id=\"10\"><\/span> ** Decision Tree Regression **","da4ac7e8":"<font size=5 color='#217638'> Medical Costs Regression Analysis<\/font> \n\n","792ac69c":"### <span id=\"6\"><\/span> ** Polynomial Regression - 2nd degree **","eb29b56a":"In this kernel, I have built 7 regression models using Medical Costs Personal Dataset. These are linear, polynomial, ridge, lasso,  svr, decision tree and random forest regression. Then measured and visualized the performance of the models. Please make a comment and let me know how to improve model performance, visualization or something.\n\n<b><font color=\"red\">Don't forget to <\/font><\/b> <b><font color=\"green\">UPVOTE <\/font><\/b> if you liked this kernel, thank you. \ud83d\ude42\ud83d\udc4d","6aeb10c3":"### <span id=\"8\"><\/span> ** Lasso Regression **","66b9b300":"Because of RandomSearch and GridSeach have took about 20 minutes, I didn't include these steps in the kernel's final form. But you can expand the following 2 cells and view how changed the parameters.","2d9de5a1":"### <span id=\"11\"><\/span> ** Random Forest Regression **","faca1e81":"## <span id=\"2\"><\/span> ** 2. Importing Libraries and Reading the Dataset **","3555b35e":"## <span id=\"12\"><\/span> ** 5. Measuring the Error **","ff8c1956":"## <span id=\"1\"><\/span> ** 1. Overview **","fe52c897":"<hr\/>\n[**Tolgahan Cepel**](https:\/\/www.kaggle.com\/tolgahancepel)\n<hr\/>\n<font color=green>\n1. [Overview](#1)\n1. [Importing Libraries and Reading the Dataset](#2)\n1. [Data Visualization and Preprocessing](#3) \n1. [Regression Models](#4) \n    * [Linear Regression](#5) \n    * [Polynomial Regression - 2nd degree](#6)\n    * [Ridge Regression](#7)\n    * [Lasso Regression](#8)\n    * [Support Vector Regression](#9)\n    * [Decision Tree Regression](#10) \n    * [Random Forest Regression](#11)\n1. [Measuring the Error](#12)\n    * [Visualizing Models Performance](#13)\n1. [Conclusion](#14)\n<hr\/>","e71fb783":"### <span id=\"7\"><\/span> ** Ridge Regression **","8f574406":"## <span id=\"14\"><\/span> ** 6. Conclusion **","64ba23ab":"## <span id=\"4\"><\/span> ** 4. Regression Models **","5d4cdecc":"## <span id=\"3\"><\/span> ** 3. Data Visualization and Preprocessing **"}}