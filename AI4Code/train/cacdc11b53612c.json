{"cell_type":{"6a235233":"code","d2d6956b":"code","262422a3":"code","a17b7aa1":"code","2ff0ee21":"code","c134fb24":"code","d5443dd9":"code","beb40209":"code","06eca8c9":"code","01ee2187":"code","49c5fc7c":"code","e1cd8985":"code","b49c1d1b":"code","7d27464b":"code","effafe69":"code","c4524152":"code","fb89fdb4":"code","2a8bc961":"code","810e0d9a":"code","b65a5a54":"code","83eab242":"code","619d9902":"code","01b47841":"code","fbd4dcbd":"code","1ca625f1":"code","e3d3868d":"code","ee460332":"code","8a69a8bb":"code","164f8382":"code","92b2c577":"code","f26e440d":"code","6ed506b9":"markdown","c08517d6":"markdown","1fe6bd6f":"markdown","44f6cae6":"markdown","529040ae":"markdown","465f2c58":"markdown","7d95d617":"markdown","165bcef0":"markdown","e40d97e0":"markdown","e8f727e2":"markdown","8b39b9c8":"markdown","0cc30eb9":"markdown","31245c2b":"markdown","91688fa1":"markdown","6ea9d69a":"markdown","fe95be68":"markdown","d44cb092":"markdown","d138493d":"markdown","00a59434":"markdown","8d320128":"markdown","1025b42e":"markdown","d0b8f5a7":"markdown","978cb98c":"markdown","fd94026d":"markdown","df93df4c":"markdown","3800c504":"markdown","53e84beb":"markdown","f3383809":"markdown","6af94e64":"markdown","0a9e799f":"markdown","45a3bebc":"markdown","1b287712":"markdown","a4767074":"markdown","211e88f9":"markdown"},"source":{"6a235233":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n","d2d6956b":"import numpy as np\nimport gym\nimport random\nimport math","262422a3":"env = gym.make(\"Taxi-v2\")","a17b7aa1":"env.reset()\nenv.render()","2ff0ee21":"# Pick a random state and render it\nenv.env.s = 420\nenv.render()","c134fb24":"# Remember right corresponds to 2\nenv.step(2)\nenv.render()","d5443dd9":"env.step(2)","beb40209":"state, reward, done, info = env.step(2)\nprint('state: {}'.format(state))\nprint('reward: {}'.format(reward))\nprint('done: {}'.format(done))\nprint('info: {}'.format(info))","06eca8c9":"state = env.reset()\nreward = None\nsteps = 0\n\nwhile reward != 20:\n    state, reward, done, info = env.step(env.action_space.sample())\n    steps += 1\n\nprint('Random driving took {} steps to complete a journey'.format(steps))","01ee2187":"random_driving_store = []\nfor i in range(1,100):\n    state = env.reset()\n    reward = None\n    steps = 0\n\n    while reward != 20:\n        state, reward, done, info = env.step(env.action_space.sample())\n        steps += 1\n\n    random_driving_store.append(steps)\n    \nfrom matplotlib import pyplot as plt\nimport numpy as np\n\nplt.figure()\nplt.hlines(0.5,0.5,2)  # Draw a horizontal line\nplt.eventplot(random_driving_store, orientation='horizontal', colors='k')\n\nplt.show()\nprint('Average number of steps for a random drive {}'.format(np.mean(random_driving_store)))","49c5fc7c":"state_size = env.observation_space.n\naction_size = env.action_space.n\n\nprint('number of possible states: {}'.format(state_size))\nprint('number of possible actions: {}'.format(action_size))","e1cd8985":"Q = np.zeros([state_size, action_size])\nQ","b49c1d1b":"total_reward = 0\nlearning_rate = 0.7","7d27464b":"done = False\ntotal_reward, reward = 0,0\nstate = env.reset()\nwhile done != True: # Keeps making actions until episode completes\n        action = np.argmax(Q[state]) # Finds the action with the greatest reward. TIP each state is a row in the Q-table, find the best action at this state by finding the max value\n        new_state, reward, done, info = env.step(action) #Takes the action with the greatest reward\n        Q[state, action] += learning_rate * (reward + np.max(Q[new_state]) - Q[state, action]) # Updates our Q-table based on the state and actions. TIP If your stuck have a look at this pseudo code below\n        total_reward += reward # Update our total reward\n        state = new_state # Update our current state\n#         env.render() # Print the current agent-environment interaction\nprint('Total reward for this episode: {}'.format(total_reward))\n\n\n# New Q value = Current Q value + learning rate * (Reward + (maximum value of new state) \u2014 Current Q value )","effafe69":"Q = np.zeros([state_size, action_size])\n\ntotal_reward = 0\nlearning_rate = 0.7\n\nfor episode in range(1,2001):\n    done = False\n    total_reward, reward = 0,0\n    state = env.reset()\n    while done != True:\n        action = np.argmax(Q[state])\n        new_state, reward, done, info = env.step(action)\n        Q[state, action] += learning_rate * (reward + np.max(Q[new_state]) - Q[state, action])\n        total_reward += reward\n        state = new_state   \n    if episode % 50 == 0:\n        print('Episode {} Total Reward: {}'.format(episode, total_reward))","c4524152":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random","fb89fdb4":"# Importing the dataset ..\/input\/ad-datacsv\/Ads_Optimisation.csv\ndataset = pd.read_csv('..\/input\/ad-datacsv\/Ads_Optimisation.csv')\nprint('First user clicked Ad 1, 5 and 9')\ndataset.head(1)","2a8bc961":"number_of_users = 10000\nnumber_of_ads = 10\nads_selected = []\ntotal_reward = 0\nfor user in range(0, number_of_users):\n    ad_picked = random.randrange(number_of_ads)\n    ads_selected.append(ad_picked)\n    reward = dataset.values[user, ad_picked]\n    total_reward = total_reward + reward","810e0d9a":"total_reward","b65a5a54":"pd.Series(ads_selected).value_counts(normalize=True)","83eab242":"pd.Series(ads_selected).value_counts(normalize=True).plot(kind='bar')","619d9902":"def UCB_multi_armed_bandit(number_of_users, number_of_ads, dataset):\n\n    ads_selected = []\n    numbers_of_selections = [0] * number_of_ads\n    sums_of_reward = [0] * number_of_ads\n    total_reward = 0\n\n    for user in range(0, number_of_users):\n        ad = 0\n        max_upper_bound = 0\n        for i in range(0, number_of_ads):\n            if (numbers_of_selections[i] > 0):\n                average_reward = sums_of_reward[i] \/ numbers_of_selections[i]\n                delta_i = math.sqrt(2 * math.log(user+1) \/ numbers_of_selections[i])\n                upper_bound = average_reward + delta_i\n            else:\n                upper_bound = 1e400\n            if upper_bound > max_upper_bound:\n                max_upper_bound = upper_bound\n                ad = i\n        ads_selected.append(ad)\n        numbers_of_selections[ad] += 1\n        reward = dataset.values[user, ad]\n        sums_of_reward[ad] += reward\n        total_reward += reward\n    return ads_selected, total_reward","01b47841":"ads_selected, total_rewards = UCB_multi_armed_bandit(10000, 10, dataset)","fbd4dcbd":"pd.Series(ads_selected).head(1500).value_counts(normalize=True).plot(kind='bar')","1ca625f1":"total_rewards","e3d3868d":"dataset.sum()\/len(dataset)","ee460332":"total_episodes = 20000        # Total episodes\ntotal_test_episodes = 100     # Total test episodes\nmax_steps = 99                # Max steps per episode\n\nlearning_rate = 0.7           # Learning rate\ngamma = 0.618                 # Discounting rate\n\n# Exploration parameters\nepsilon = 1.0                 # Exploration rate\nmax_epsilon = 1.0             # Exploration probability at start\nmin_epsilon = 0.01            # Minimum exploration probability \ndecay_rate = 0.01             # Exponential decay rate for exploration prob\n\n\nqtable = np.zeros((state_size, action_size))","8a69a8bb":"\n# List of rewards\nrewards = []\n\n# 2 For life or until learning is stopped\nfor episode in range(total_episodes):\n    # Reset the environment\n    state = env.reset()\n    step = 0\n    done = False\n    total_rewards = 0\n    \n    for step in range(max_steps):\n        # 3. Choose an action a in the current world state (s)\n        ## First we randomize a number\n        exp_exp_tradeoff = random.uniform(0, 1)\n        \n        ## If this number > greater than epsilon --> exploitation (taking the biggest Q value for this state)\n        if exp_exp_tradeoff > epsilon:\n            action = np.argmax(qtable[state,:])\n\n        # Else doing a random choice --> exploration\n        else:\n            action = env.action_space.sample()\n\n        # Take the action (a) and observe the outcome state(s') and reward (r)\n        new_state, reward, done, info = env.step(action)\n\n        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a') - Q(s,a)]\n        # qtable[new_state,:] : all the actions we can take from new state\n        qtable[state, action] = qtable[state, action] + learning_rate * (reward + gamma * np.max(qtable[new_state, :]) - qtable[state, action])\n        \n        total_rewards += reward\n        \n        # Our new state is state\n        state = new_state\n        \n        # If done (if we're dead) : finish episode\n        if done == True: \n            break\n        \n    # Reduce epsilon (because we need less and less exploration)\n    epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode) \n    rewards.append(total_rewards)\n\nprint (\"Score over time: \" +  str(sum(rewards)\/total_episodes))\nprint(qtable)","164f8382":"env.reset()\n\nfor episode in range(5):\n    state = env.reset()\n    step = 0\n    done = False\n    print(\"****************************************************\")\n    print(\"EPISODE \", episode)\n\n    for step in range(max_steps):\n        \n        # Take the action (index) that have the maximum expected future reward given that state\n        action = np.argmax(qtable[state,:])\n        \n        new_state, reward, done, info = env.step(action)\n        \n        if done:\n            # Here, we decide to only print the last state (to see if our agent is on the goal or fall into an hole)\n            env.render()\n            \n            # We print the number of step it took.\n            print(\"Number of steps\", step)\n            break\n        state = new_state\nenv.close()","92b2c577":"env = gym.make('FrozenLake-v0')","f26e440d":"gym.envs.registry.all()","6ed506b9":"# Greedy epsilon algorithm\nHere's another algorithm for learning `greedy epsilon`. Have a go with this code and try tweaking the hyperpara****meter","c08517d6":"Just like the last exercise it's best practice to start benchmarking with an agent taking random actions.\n\nLet's start with assigning users to a random advert.","1fe6bd6f":"*Every breath you take*\n\n*Every move you make*\n\n*Every bond you break*\n\n*Every step you take*\n\n*I'll be <s>watching<\/s> rewarding you (with -1) *\n\nThe reason why we give a negative reward at each turn, it basically forces the agent to find the quickest possible solution.","44f6cae6":"We've now got the dimensions of our Q-table, let's initialize our Q-table as a blank canvas (all zeros)","529040ae":"The results are in from the random assignment. ","465f2c58":"Let's see a movement in action","7d95d617":"Let's move to the right by taking a `step`","165bcef0":"# Multi armed bandits\n\nIn the last exercise we were playing with taxis in a highly sterile environment. What we're going to do now is apply a reinforcement learning technique known as a multi armed bandit in a business case scenario.\n\nThe name multi armed bandit comes from a type of slot machine. You pull the lever, bleep bloop and either the machine pays out some money, or it doesn't.\n\nYou're faced with 6 slot machines with different probabilities of a payout. If you knew which one had the highest likelihood of a payout you would just stick with that one, right? \nThe aim of a multi armed bandit is find the action with the greatest amount of reward, while still earning reward during this exploration phase.\n\nThis method of determining probability distrobutions while exploiting the most successful can be used in a huge variety of industries.\n* **Clinical trials**- Exploration is akin to identifying an optimum treatment, while the exploitation occurs when treating patients efficiently as rapidly as possible from when the trial begins.\n* **Game design**- Creating and experimenting with variants of a game mechanic, more players will be allocated to the variant with the greater level of success, thus ensuring maximized user interaction throughout the experiment.\n\nHowever, the most used application of a multi armed bandit is as a turbo charged A\/B test for product and advertising.\n\nA\/B testing is by far the most common way to gage user preference. The main problem of traditional A\/B tests is that it is split into two separate stages exploration and **then** exploitation. \n\nWheras multi armed bandits combine both these phases, ie gives you the ability to earn a reward whilst still trying to find the optimum action.\n\nThe goal of this next exercise to optimize the performance of your companies display ads. You make money everytime a user interacts with the ad. Your collegue has whipped up 10 variations of the ad, let's see if we can find the highest performing variant!\n\n","e40d97e0":"Run it a few more times and see how long it takes.","e8f727e2":"![Multi-armed-bandit](https:\/\/conversionxl.com\/wp-content\/uploads\/2015\/09\/multiarmedbandit.jpg)","8b39b9c8":"Reward is kind of easy to misinterpret, it's normally a positive, however, in reinforcement learning it can either a positive or a negative.\n\n**Positive**\n* The agent receives `+20` points for a successful drop off\n\n**Negative**\n* The agent receives `-10` points for each mistake it makes when picking up or dropping off a passenger\n* The agent receives `-1` point for every step it makes.\n","0cc30eb9":"1. Initialize the Q table \n * Done\n2. Choose an action\n * Pick an action `a` in the agents current state `s` based selection criteria of Q-value estimates\n3. Perform action\n * Perform the action chosen\n4. Measure reward\n * Given the action taken `a` what is the new observed outcome state `s\u2019` and reward `r`\n 5. Update Q \n  * Use Bellman equation to update Q-table... \n \n![bellman](https:\/\/cdn-images-1.medium.com\/max\/2600\/1*jmcVWHHbzCxDc-irBy9JTw.png) \n\n\n\n\n\n\nummmmm sorry, what?\n\nThe Bellman equation is an optimization technique used in dynamic programming. It aims to find the greatest value between a decision at the current state and a new state. By breaking the dynamic optimization problem into a set of smaller problems it makes the overarchng problem easier to solve.\n","31245c2b":"We will start with a single episode, ie the episode completes when the agent fails or completes the trip.","91688fa1":"We're going to have to do a bit of pretending here, the data we will be working with is a static file (not streamed data). Each row represents a user and each column represents the advert that user had seen. If the user clicked the ad, the cell has a value of 1, else it's 0.","6ea9d69a":"These outputs are the 'bread and butter' of reinforcement learning\n* state- Current situation of the agent in the environment\n* reward- Feedback from an action by the agent in the environment\n* done- Boolean indicating whether the agent has terminated or completed its environment\n* info- Diagnositic information about the agents last action. \n\n\nYou receive +20 points for a successful dropoff, and lose 1 point for every timestep it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.","fe95be68":"Now we have the empty table, we get our agent to update it with information.\n\nThe Q-learning algorithm Process:\n![process](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*QeoQEqWYYPs1P8yUwyaJVQ.png)","d44cb092":"Now we've built the logic, it's time to learn and build our Q-table across episodes. This will be easy, we'll stick our previous code in a `for loop` and iterate through `2000 episodes`","d138493d":"# Start exploring the frozen lake environment\nOpenAI give us pleanty more environments to choose from. Check out the Frozen lake environment here\n","00a59434":"Please enter Upper Confidence Bound (UCB)....\n\nThis is one of the most commonly used optimization processes for multi armed bandits. The main point to grasp about this algorithm is that the less familiar the bandit is with a particular action, the more curious it becomes, and in turn increases the chance to explore said action.\n\nCheck out the graph below.\n![dist](https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2018\/09\/im_18.jpg)\n\n\nWe can see the distribution of three different adverts above. You notice that a1 is much more spread out that either a2 or a3. This high level of spread indicates that this advert has not been explored enough. Next iteration the bandit will make a1 its top priority for exploration. It will keep doing this until the spread of that adverts reward meets a desired threshold.\n\n\nHere's a bit of pseudo code to help you understand the algorithm a bit more clearly\n\nStep 1. Run each of the adverts once, this will give us an ititial idea of the reward landscape\nStep 2. For each iteration of t\nStep 3. Count number of times each ad was shown so far (Nt(a))\nStep 4. Plug values into the following expression\n![formula](https:\/\/s3-ap-south-1.amazonaws.com\/av-blog-media\/wp-content\/uploads\/2018\/09\/im_19.jpg)\n\nStep 5. Calculate and store the reward\n\nIf this seems a bit too complicated don't worry, it was for me. I've grabbed some code that will help us. Try and follow along...","8d320128":"# Extra content","1025b42e":"There we go! Our bandit was correct. Ad 5 was the top performer.\n\nThat's the main content complete for this months meetup. If you have time why don't you check out the extra exercises.","d0b8f5a7":"Check that out! The total reward is much greater now and from the graph we have an ad that is clearly outperforming the rest, ad 5! (remember zero indexing is a thing in python). \n\nSince we had the the entire dataset from the start, we can check to see which ad did in fact have the highest click rate.","978cb98c":"Now what?\n\nWell we have the agent, environment and rewards. Lets explore.\n\nWhen we build any machine learning model we want to start simple. \n\nWe know our objective is to pick up and drive our passenger to their destination, let's pick random actions until we get it right.\n\nUse ` env.action_space.sample()` to take random actions until you have a successful trip.\n\n**Tip** Think how a successful trip is rewarded.\n","fd94026d":"No surprises here, asigning a random action will get our taxi nowhere fast.\n\nWe need a way for our agent to learn how to manouver the mean streets of the gym. Lets try building a `Q-table`.\n\nThink of a Q-table initially as a blank map of the environment. Your agent will navigate the space and updates the table with anything it finds interesting.\n\nWith each iteration the agent gets more and more information about the environment. In no time at all, your agent is whizzing around the environment.\n\nYour map has now become a cheatsheet!\n\nThe Q-table is made up of rows equal to the number of `states` and columns equal to the number of `actions`.\n\nThe value at cell indicates the expected reward for an action given that state.\n\nLet's build our Q-table...","df93df4c":"# Or check out any of these","3800c504":"We will now code up this Q-learning process to build our optimized Q-table.\n","53e84beb":"These should be unpacked like this...","f3383809":"Now we can see our environment!\n* Yellow square represents the taxi\n* \u201c|\u201d represents a wall\n* The letter coloured blue represents the passenger pick-up location\n* The letter coloured purple represents the passengers destination \n\n**Note** The taxi will turn green when a passenger has been picked up.\n\nThere are six possible moves a taxi can take:\n* Up `0`\n* Down `1`\n* Right `2`\n* Left `3`\n* Pickup `4`\n* Dropoff `5`","6af94e64":"![openAI](https:\/\/openai.com\/assets\/images\/home\/openai-homepage@2x-4e2e39cbd1.svg)\nFounded with the aim to build a safe artificial general intelligence, OpenAI have developed the Gym platform. This is a collection of environments and problems designed to flex the muscles of reinforcement learning algorithms. \n\nLet's get our gym membership...","0a9e799f":"Let's look a bit more closely at the output of `step`","45a3bebc":"A good multi arm bandit with clever user assignment should give us a clear winner. As you can see from our results, there is no clear winner.\n\nTotal reward for the random selection algorithm comes out to be ~1000. \n\nAs this algorithm is not learning anything, it has no intuition into which ad is giving the maximum return. Therefore we would expect out of 10000 users it only gets ~1000, basically, a 1\/10 chance, which makes sense as there are only 10 ads to choose from.\n\nRandom!\n\nThere must be a better way to optimize this problem...\n\n\n\n","1b287712":"We can view the state of our environment by using `render`.","a4767074":"You're just off the phone to  Uber, they're having a bit of trouble with their new self driving car and they want you to fix it.\n\n\"No worries, Dara Khosrowshahi, I've got this under control!\", *click*.\n\nThere are **`4`** locations represented by different letters and it's your job to pick up the passenger at one location and drop them off in another.\n\nLet's load up our first environment `Taxi-v2`. ","211e88f9":"See, we've moved a space to the right. "}}