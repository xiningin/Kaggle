{"cell_type":{"77942ddb":"code","774e9c08":"code","ef8e2c10":"code","c8158986":"code","7d6d770b":"code","d1caaf7d":"code","a835b76c":"code","772804c7":"code","cfc20a73":"code","3823a208":"code","fccc4b11":"code","8ad79c63":"code","4ddb7ae9":"code","98d361a3":"code","67cec16d":"code","f404902e":"code","873fdc51":"code","9e5d4194":"code","b4053bd2":"code","bf23977b":"code","f59cc7a0":"code","3086d921":"code","4aa984e1":"code","96d2c238":"code","b8040b2a":"code","48d24404":"code","aed9723d":"code","bc14115f":"code","ec7c31ce":"code","951ba35b":"code","d07b3e4a":"code","8b35cd1f":"code","35c680d3":"code","35c9e01e":"code","b0b1633a":"code","f62340d4":"code","caf6517b":"code","f63c2902":"code","4c52813e":"code","698b4a97":"code","772722c5":"code","57a5f3f3":"code","0d7e3f17":"code","5d32604e":"code","15e1f398":"code","2c746dd6":"code","d2e2c9ee":"code","ede4c2ad":"code","c16f15ed":"code","6c26cfe7":"code","fd466a53":"code","6b43ad03":"code","14b4601e":"code","1a66f182":"code","53c9a894":"code","bef243d9":"code","035262c8":"code","d71a189b":"code","bd99659d":"code","b77dc9a9":"code","9c235dda":"code","222fb1b1":"code","1c21873e":"code","39af797a":"code","786719cd":"code","b6391358":"code","96d1af55":"code","346dac4b":"code","d58257d9":"code","921e2639":"code","f1c89a13":"code","c5188a3e":"code","4daf417a":"code","6a0175c9":"code","9046d443":"code","4e9fe5ac":"code","1de27f64":"code","07cf94aa":"code","904fcac0":"code","17c9ddf6":"code","1f701e94":"code","2773a6f6":"code","9e86df98":"code","37e84942":"code","be1e022b":"code","975d1682":"code","2765914e":"code","41683379":"code","b9d53264":"code","fd008960":"code","8e63a7da":"code","cad030ac":"code","b50be6a8":"code","b84db378":"markdown","6182f298":"markdown","60d25bb8":"markdown","81447b34":"markdown","3f325a7d":"markdown","71ddaecb":"markdown","9c78f2bb":"markdown","e1c6590a":"markdown","e6ed4afa":"markdown","9834a523":"markdown","760fc2d2":"markdown","04ccddf6":"markdown","6a0789f5":"markdown","a883be96":"markdown","a059388b":"markdown","43cc34cc":"markdown","cbe4d551":"markdown","f744a943":"markdown","eb2a6039":"markdown","80844d61":"markdown","b9b15453":"markdown","4e72a8f2":"markdown","a359323a":"markdown","1c082851":"markdown","5865b9d9":"markdown","daaca895":"markdown","10648b26":"markdown","3f6283fd":"markdown","ab594ae1":"markdown","79421b5a":"markdown","3923c272":"markdown","a40d405d":"markdown","188b8f5f":"markdown","fc196634":"markdown","77240183":"markdown","0c2557d7":"markdown","df3647fa":"markdown"},"source":{"77942ddb":"import numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport datetime\nimport gc\nfrom itertools import product\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nimport time\nfrom statsmodels.tsa.stattools import acf\ndata_path = '\/kaggle\/input\/competitive-data-science-predict-future-sales\/'","774e9c08":"def downcast_dtypes(df):\n    start_size = df.memory_usage(deep = True).sum() \/ 1024**2\n    print('Memory usage: {:.2f} MB'.format(start_size))\n\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int32)\n    end_size = df.memory_usage(deep = True).sum() \/ 1024**2\n    print('New Memory usage: {:.2f} MB'.format(end_size))\n    return df\n\ndef create_record_for_features(df, attrs, target, time_col, aggfunc = np.sum, fill = 0):\n    target_for_attrs = df.pivot_table(index = attrs,\n                                   values = target, \n                                   columns = time_col, \n                                   aggfunc = aggfunc, \n                                   fill_value = fill,\n                                  ).reset_index()\n    target_for_attrs.columns = target_for_attrs.columns.map(str)\n    target_for_attrs = target_for_attrs.reset_index(drop = True).rename_axis(None, axis = 1)\n    return target_for_attrs\n\ndef display_df_info(df, name):\n    print('-----------Shape of '+ name + '-------------')\n    print(df.shape)\n    print('-----------Missing values---------')\n    print(df.isnull().sum())\n    print('-----------Null values------------')\n    print(df.isna().sum())\n    print('-----------Data types-------------')\n    print(df.dtypes)\n    print('-----------Memory usage (MB)------')\n    print(np.round(df.memory_usage(deep = True).sum() \/ 1024**2, 2))","ef8e2c10":"sales = pd.read_csv(data_path + 'sales_train.csv')\nsales = downcast_dtypes(sales)","c8158986":"items = pd.read_csv(data_path + 'items.csv')\nitems = downcast_dtypes(items)","7d6d770b":"item_categories = pd.read_csv(data_path + 'item_categories.csv')\nitem_categories = downcast_dtypes(item_categories)","d1caaf7d":"shops = pd.read_csv(data_path + 'shops.csv')\nshops = downcast_dtypes(shops)","a835b76c":"test = pd.read_csv(data_path + 'test.csv')\ntest = downcast_dtypes(test)","772804c7":"display_df_info(sales, 'Sales')","cfc20a73":"display_df_info(items, 'items')","3823a208":"display_df_info(item_categories, 'item Categories')","fccc4b11":"display_df_info(shops, 'shops')","8ad79c63":"display_df_info(test, 'Test set')","4ddb7ae9":"sales_sampled = sales.sample(n = 10000)\nsns.pairplot(sales_sampled[['date_block_num', 'shop_id', 'item_id', 'item_price', 'item_cnt_day']], diag_kind = 'kde')\nplt.show()","98d361a3":"del sales_sampled\ngc.collect()","67cec16d":"sns.boxplot(x = sales['item_price'])\nplt.show()","f404902e":"sales.loc[:, 'item_price'] = sales.loc[:, 'item_price'].clip(-1, 10**5)\nsale_with_negative_price = sales[sales['item_price'] < 0]\nsale_with_negative_price","873fdc51":"sale = sales[(sales.shop_id == 32) & (sales.item_id == 2973) & (sales.date_block_num == 4) & (sales.item_price > 0)]\nmedian = sale.item_price.median()\nsales.loc[sales.item_price < 0, 'item_price'] = median","9e5d4194":"del sale \ndel median\ndel sale_with_negative_price\ngc.collect()","b4053bd2":"sns.boxplot(sales['item_cnt_day'])\nplt.show()","bf23977b":"sales_temp = sales[sales['item_cnt_day'] > 500]\nprint('Sold item outliers')\nitems[items['item_id'].isin(sales_temp['item_id'].values)].merge(sales_temp[['item_id', 'item_cnt_day', 'date_block_num']], on = 'item_id')","f59cc7a0":"del sales_temp\ngc.collect()","3086d921":"print('Number of duplicates:', len(sales[sales.duplicated()]))","4aa984e1":"sales = sales.drop_duplicates(keep = 'first')\nprint('Number of duplicates:', len(sales[sales.duplicated()]))","96d2c238":"start = time.time()\nsales.date = sales.date.apply(lambda x: datetime.datetime.strptime(x, '%d.%m.%Y'))\nprint('First sale took place: ', sales.date.min())\nprint('Last sale took place: ', sales.date.max())\nprint('It tooks: ', round(time.time() - start), 'seconds')","b8040b2a":"start = time.time()\npairs_trans = create_record_for_features(sales, ['shop_id', 'item_id'], 'item_cnt_day', 'date_block_num', aggfunc = np.count_nonzero, fill = 0)\nprint('It tooks: ', round(time.time() - start), 'seconds')","48d24404":"for month in range(1, 12):\n    pairs_temp = pairs_trans[['shop_id', 'item_id']][pairs_trans.loc[:,'0': str(month)].sum(axis = 1) == 0]\n    print('From month: 0 until ', month,', ', np.round(100 * len(pairs_temp) \/ len(pairs_trans), 2), '% of the item\/shop pairs have made no sales')","aed9723d":"for month in range(21, 33):\n    pairs_temp = pairs_trans[['shop_id', 'item_id']][pairs_trans.loc[:,str(month): '33'].sum(axis = 1) == 0]\n    print('From month: ', month, ' until month: 33, ', np.round(100 * len(pairs_temp) \/ len(pairs_trans), 2), '% of the item\/shop pairs have made no sales')","bc14115f":"del pairs_trans\ndel pairs_temp\ngc.collect()","ec7c31ce":"start = time.time()\nitems_trans = create_record_for_features(sales, ['item_id'], 'item_cnt_day', 'date_block_num', aggfunc = np.count_nonzero, fill = 0)\nprint('It tooks: ', round(time.time() - start), 'seconds')","951ba35b":"for month in range(1, 12):\n    items_temp = items_trans['item_id'][items_trans.loc[:,'0': str(month)].sum(axis = 1) == 0]\n    print('From month: 0 until ', month,', ', np.round(100 * len(items_temp) \/ len(items_trans), 2), '% of the items have made no sales')","d07b3e4a":"for month in range(21, 33):\n    items_temp = items_trans['item_id'][items_trans.loc[:,str(month): '33'].sum(axis = 1) == 0]\n    print('From month: ', month, ' until month: 33, ', np.round(100 * len(items_temp) \/ len(items_trans), 2), '% of the items have made no sales')","8b35cd1f":"del items_trans\ndel items_temp\ngc.collect()","35c680d3":"start = time.time()\nshops_trans = create_record_for_features(sales, ['shop_id'], 'item_cnt_day', 'date_block_num', aggfunc = np.count_nonzero, fill = 0)\nprint('It tooks: ', round(time.time() - start), 'seconds')","35c9e01e":"for month in range(1, 12):\n    shops_temp = shops_trans['shop_id'][shops_trans.loc[:,'0': str(month)].sum(axis = 1) == 0]\n    print('From month: 0 until ', month,', ', np.round(100 * len(shops_temp) \/ len(shops_trans), 2), '% of the shops have made no sales')","b0b1633a":"for month in range(21, 33):\n    shops_temp = shops_trans['shop_id'][shops_trans.loc[:, str(month): '33'].sum(axis = 1) == 0]\n    print('From month: ', month, ' until month: 33, ', np.round(100 * len(shops_temp) \/ len(shops_trans), 2), '% of the shops have made no sales')","f62340d4":"del shops_trans\ndel shops_temp\ngc.collect()\n","caf6517b":"item_id = 20949\nshop_ids = [25, 24]\nitem_sales = sales[(sales['item_id'] == item_id) & (sales['shop_id'].isin(shop_ids))]\nfig, axs = plt.subplots(figsize = (10, 6),  constrained_layout=True)\nsns.pointplot(x = 'date_block_num', y = 'item_cnt_day', hue = 'shop_id', data = item_sales)\naxs.set_title('Sales for item: ' + str(item_id))\nplt.show()","f63c2902":"print('test shape: ', test.shape)\nprint('number of items in test set: ', test['item_id'].nunique())\nprint('number of shops in test set: ', test['shop_id'].nunique())\n","4c52813e":"test['item_id'].nunique() * test['shop_id'].nunique() == len(test)","698b4a97":"items_test = set(test.item_id)\nitems_sales = set(sales.item_id)\nitem_in_test_and_sales = items_test.intersection(items_sales)\nitem_in_test_not_sales = set(test.item_id) - items_sales.intersection(items_test)\nprint('There is sales history for:', np.round(100 * len(item_in_test_and_sales) \/ len(items_test), 2) , '% items in test set')\nprint('There is No sales history for:', np.round(100 * len(item_in_test_not_sales) \/ len(items_test), 2), '% items in test set')","772722c5":"shops_test = set(test.shop_id)\nshops_sales = set(sales.shop_id)\nshops_in_test_and_sales = shops_test.intersection(shops_sales)\nshops_in_test_not_sales = set(test.shop_id) - shops_test.intersection(shops_sales)\nprint('There is sales history for:', np.round(100 * len(shops_in_test_and_sales) \/ len(shops_test), 2), '% shops in test set')\nprint('There is No sales history for:', np.round(100 * len(shops_in_test_not_sales) \/ len(shops_test), 2), '% shops in test set')","57a5f3f3":"item_shop_test = set(test.item_id.astype(str) + '_' + test.shop_id.astype(str))\nitem_shop_sales = set(sales.item_id.astype(str) + '_' + sales.shop_id.astype(str))\npairs_with_history = len(item_shop_test.intersection(item_shop_sales) )\npairs_with_no_history = len(shops_in_test_and_sales) * len(item_in_test_not_sales)\njust_item_with_history = test.shape[0] - (pairs_with_history + pairs_with_no_history)\nprint('There is sales history for:', np.round(100 * pairs_with_history \/ len(test), 2) , '% items in the same shops')\nprint('There is No sales history for:',  np.round(100 * pairs_with_no_history \/ len(test), 2), '% shop\/item pairs')\nprint('There is sales history for:',  np.round(100 * just_item_with_history \/ len(test), 2), '% items but in different shops')","0d7e3f17":"del items_test\ndel items_sales\ndel item_in_test_and_sales\ndel item_in_test_not_sales\ndel shops_test\ndel shops_sales\ndel shops_in_test_and_sales\ndel shops_in_test_not_sales\ndel item_shop_test\ndel item_shop_sales\ndel pairs_with_history\ndel pairs_with_no_history\ndel just_item_with_history\ngc.collect()","5d32604e":"shops['shop_name'] = shops['shop_name'].apply(lambda x: x.lower()).str.replace('[^\\w\\s]', '').str.replace('\\d+','').str.strip()\nshops['shop_city'], shops['shop_name'] = shops['shop_name'].str.split(' ', 1).str\nshops['shop_type'] = shops['shop_name'].apply(lambda x: '\u043c\u0442\u0440\u0446' if '\u043c\u0442\u0440\u0446' in x else '\u0442\u0440\u0446' if '\u0442\u0440\u0446' in x else '\u0442\u0440\u043a' if '\u0442\u0440\u043a' in x else '\u0442\u0446' if '\u0442\u0446' in x else '\u0442\u043a' if '\u0442\u043a' in x else 'unkown')\nshops.head()","15e1f398":"print('Shops number:', shops['shop_id'].nunique())\nprint('Shop names number:', shops['shop_name'].nunique())\nprint('Shop cities number:', shops['shop_city'].nunique())\nprint('Shop types number:', shops['shop_type'].nunique())","2c746dd6":"sales.loc[sales['shop_id'] == 11, 'shop_id'] = 10\nshops.loc[shops['shop_id'] == 11, 'shop_id'] = 10\n\nsales.loc[sales['shop_id'] == 23, 'shop_id'] = 24\nshops.loc[shops['shop_id'] == 23, 'shop_id'] = 24\n\nsales.loc[sales['shop_id'] == 0, 'shop_id'] = 57\nshops.loc[shops['shop_id'] == 0, 'shop_id'] = 57\n\nsales.loc[sales['shop_id'] == 1, 'shop_id'] = 58\nshops.loc[shops['shop_id'] == 1, 'shop_id'] = 58\n\nsales.loc[sales['shop_id'] == 40, 'shop_id'] = 39\nshops.loc[shops['shop_id'] == 40, 'shop_id'] = 39\n\nshops = shops.drop_duplicates(subset = 'shop_id')","d2e2c9ee":"print('Shops number:', shops['shop_id'].nunique())\nprint('Shop names number:', shops['shop_name'].nunique())\nprint('Shop cities number:', shops['shop_city'].nunique())\nprint('Shop types number:', shops['shop_type'].nunique())","ede4c2ad":"def clean_names(df, cols):\n    for col in cols:\n        df[col] = df[col].str.replace('[^A-Za-z0-9\u0410-\u042f\u0430-\u044f]+', ' ').str.lower()\n        df[col] = df[col].str.strip()\n        df.loc[df[col] == '', col] = 'unknown'","c16f15ed":"items['item_name'], items['item_type'] = items['item_name'].str.split('[', 1).str\nitems['item_name'], items['item_subtype'] = items['item_name'].str.split('(', 1).str\nclean_names(items, ['item_name', 'item_type', 'item_subtype'])\nitems = items.fillna('unkown')\nitems.head()\n\n\n","6c26cfe7":"print('Number of items:', items['item_id'].nunique())\nprint('Number of item_name:', items['item_name'].nunique())\nprint('Number of item_type:', items['item_type'].nunique())\nprint('Number of item_subtype:', items['item_subtype'].nunique())","fd466a53":"sales = sales.merge(items[['item_id', 'item_category_id']], on = 'item_id', how = 'left')","6b43ad03":"time_shift = 14","14b4601e":"pairs_sales = create_record_for_features(sales, ['shop_id', 'item_id'], 'item_cnt_day', 'date_block_num', aggfunc = np.sum, fill = 0)","1a66f182":"shops_items_sales_temp = pairs_sales.sample(1000)#.nlargest(10**4, columns = [str(itr) for itr in range(27, 33)])\npairs_acf = np.zeros((shops_items_sales_temp.shape[0], time_shift + 1))\nfor i, (ind, shop_item_sales) in enumerate(shops_items_sales_temp.iterrows()):\n    pair = shop_item_sales.loc['0': ]\n    acf_12 = acf(pair, nlags = time_shift, fft = True)\n    pairs_acf[i, :] = acf_12","53c9a894":"avgs = np.mean(pairs_acf, axis = 0)\nplt.bar(x = np.arange(time_shift + 1), height = avgs)\nplt.title('lag importance of shop\/item pairs')\nplt.show()","bef243d9":"pair_lags = [1, 2, 3, 4, 8, 10, 11, 12]","035262c8":"del shops_items_sales_temp\ndel pairs_sales\ndel shop_item_sales\ndel pair\ndel acf_12\ndel pairs_acf\ngc.collect()","d71a189b":"items_sales = create_record_for_features(sales, ['item_id'], 'item_cnt_day', 'date_block_num', aggfunc = np.sum, fill = 0)","bd99659d":"items_acf = np.zeros((items_sales.shape[0], time_shift + 1))\nfor i, item_sales in items_sales.iterrows():\n    item_temp = item_sales.loc['0': ]\n    if np.sum(item_temp) != 0:\n        acf_12 = acf(item_temp, nlags = time_shift, fft = True)\n        items_acf[i, :] = acf_12","b77dc9a9":"avgs = np.mean(items_acf, axis = 0)\nplt.bar(x = np.arange(time_shift + 1), height = avgs)\nplt.title('lag importance of items')\nplt.show()","9c235dda":"item_lags = [1, 2, 3, 4, 5, 10, 11, 12]","222fb1b1":"del items_sales\ndel item_sales\ndel items_acf\ndel item_temp\ndel acf_12\ndel avgs\ngc.collect()","1c21873e":"shops_sales = create_record_for_features(sales, ['shop_id'], 'item_cnt_day', 'date_block_num', aggfunc = np.sum, fill = 0)\nshops_acf = np.zeros((shops_sales.shape[0], time_shift + 1))\nfor i, shop_sales in shops_sales.iterrows():\n    shop_temp = shop_sales.loc['0': ]\n    acf_12 = acf(shop_temp, nlags = time_shift, fft = True)\n    shops_acf[i, :] = acf_12","39af797a":"avgs = np.mean(shops_acf, axis = 0)\nplt.bar(x = np.arange(time_shift + 1), height = avgs)\nplt.title('lag importance of shops')\nplt.show()","786719cd":"shop_lags = [1, 2, 3, 4, 7, 8, 10, 12]","b6391358":"del shops_sales\ndel shop_sales\ndel shops_acf\ndel shop_temp\ndel acf_12\ndel avgs\ngc.collect()","96d1af55":"categories_sales = create_record_for_features(sales, ['item_category_id'], 'item_cnt_day', 'date_block_num', aggfunc = np.sum, fill = 0)\ncategories_acf = np.zeros((categories_sales.shape[0], time_shift + 1))\nfor i, category_sales in categories_sales.iterrows():\n    category_temp = category_sales.loc['0': ]\n    acf_12 = acf(category_temp, nlags = time_shift, fft = True)\n    categories_acf[i, :] = acf_12","346dac4b":"avgs = np.mean(categories_acf, axis = 0)\nplt.bar(x = np.arange(time_shift + 1), height = avgs)\nplt.title('lag importance of categories')\nplt.show()\n","d58257d9":"category_lags = [1, 2, 3, 4, 5, 6, 12]","921e2639":"del categories_sales\ndel category_sales\ndel categories_acf\ndel category_temp\ndel acf_12\ndel avgs\ngc.collect()","f1c89a13":"index_cols = ['shop_id', 'item_id', 'date_block_num'] \n\ndef create_train_set(df, index_cols = index_cols):\n    grid = []\n    for month in df['date_block_num'].unique():\n        curr_shops = df[df['date_block_num'] == month]['shop_id'].unique()\n        curr_items = df[df['date_block_num'] == month]['item_id'].unique()\n        grid.append(np.array(list(product(*[curr_shops, curr_items, [month]])), dtype = 'int32'))\n\n    grid = pd.DataFrame(np.vstack(grid), columns = index_cols, dtype = np.int32)\n\n    gb = df.groupby(index_cols, as_index = False).agg({'item_cnt_day':'sum'})\n    gb.columns = index_cols + ['target']\n    all_data = pd.merge(grid, gb, how = 'left', on = index_cols).fillna(0)\n    all_data.sort_values(['date_block_num','shop_id','item_id'], inplace = True)\n    all_data.loc[:, 'target'] = all_data['target'].clip(0, 20).astype(np.float32)\n    print('Sales data shape:', df.shape)\n    print('Generated Train data shape:', all_data.shape)\n    del curr_shops\n    del curr_items\n    del gb\n    del grid\n    gc.collect()\n    return all_data","c5188a3e":"start = time.time()\ngrid = create_train_set(sales, index_cols)\ntest['date_block_num'] = 34\ngrid = pd.concat([grid, test[['item_id', 'shop_id', 'date_block_num']]], ignore_index = True, sort = False, keys = index_cols)\ngrid = grid.merge(items[['item_id', 'item_category_id']], on = 'item_id', how = 'left')\nprint(round(time.time() - start), 'seconds')","4daf417a":"def lag_features(df, features, go_back_in_time):\n    for month_shift in go_back_in_time:\n        df_shift = df[index_cols + features].copy()\n        df_shift['date_block_num'] = df_shift['date_block_num'] + month_shift\n        lag_cols = lambda x: '{}_lag_{}'.format(x, month_shift) if x in features else x\n        df_shift = df_shift.rename(columns = lag_cols)\n        df = pd.merge(df, df_shift, on = index_cols, how='left')\n    return df\n\ndef fast_lag_features(df, features, go_back_in_time): \n    features_sales = create_record_for_features(sales, features, 'item_cnt_day', 'date_block_num', aggfunc = np.sum, fill = 0)\n    for month in go_back_in_time:\n        max_month = df.date_block_num.max()\n        cols = [str(itr) for itr in np.arange(0, max_month)]\n        gb = features_sales.melt( id_vars = features, \n                                 var_name = 'date_block_num' , \n                                 value_vars= cols, \n                                 value_name = 'target_' + '_'.join(features) + '_lag_' + str(month)\n                                )\n        gb.date_block_num = gb.date_block_num.astype(np.int16)\n        gb.date_block_num = gb.date_block_num + month\n        df = pd.merge(df, gb, on = features + ['date_block_num'], how='left')\n    return df","6a0175c9":"start = time.time()\ngrid = lag_features(grid, ['target'], pair_lags)\nprint(round(time.time() - start), 'seconds')","9046d443":"start = time.time()\ngrid = fast_lag_features(grid, ['item_id'], item_lags)\nprint(round(time.time() - start), 'seconds')","4e9fe5ac":"start = time.time()\ngrid = fast_lag_features(grid, ['shop_id'], shop_lags)\nprint(round(time.time() - start), 'seconds')","1de27f64":"start = time.time()\ngrid = fast_lag_features(grid, ['item_category_id'], category_lags)\nprint(round(time.time() - start), 'seconds')","07cf94aa":"grid.isnull().sum()","904fcac0":"grid = grid[grid['date_block_num'] > 11]\ngrid = grid.fillna(0)","17c9ddf6":"grid = downcast_dtypes(grid)","1f701e94":"dates_train = sales.loc[:, ['date', 'date_block_num']].drop_duplicates()\ndates_train = dates_train.reset_index(drop = True)\ndates_test = dates_train.loc[dates_train.loc[:, 'date_block_num'] == 34 - 12]\ndates_test = dates_test.reset_index(drop=True)\ndates_test.loc[:,'date_block_num'] = 34\ndates_test.loc[:, 'date'] = dates_test.loc[:, 'date'] + pd.DateOffset(years = 1)\ndates_all = pd.concat([dates_train, dates_test])\ndates_all.loc[:, 'dow'] = dates_all.loc[:, 'date'].dt.dayofweek\ndates_all.loc[:, 'year'] = dates_all.loc[:, 'date'].dt.year\ndates_all.loc[:, 'month'] = dates_all.loc[:, 'date'].dt.month\n\ndates_all = pd.get_dummies(dates_all, columns = ['dow'])\ndow_col = ['dow_' + str(x) for x in range(7)]\ndate_features = dates_all.groupby(['year', 'month', 'date_block_num'])[dow_col].agg('sum').reset_index()\ndate_features.loc[:, 'days_of_month'] = date_features.loc[:, dow_col].sum(axis=1)\ndate_features.loc[:, 'year'] = date_features.loc[:, 'year'] - 2013\ndate_features = date_features.loc[:, ['month', 'date_block_num']]","2773a6f6":"grid = grid.merge(date_features, on = 'date_block_num', how = 'left')","9e86df98":"del dates_train\ndel dates_test\ndel dates_all\ndel dow_col\ndel date_features\ngc.collect()","37e84942":"grid['category_shop_inter'] = grid['item_category_id'].astype(str) + '_' + grid['shop_id'].astype(str)\ngrid.loc[:, 'category_shop_inter'] = LabelEncoder().fit_transform(grid.loc[:, 'category_shop_inter'].values)","be1e022b":"grid = downcast_dtypes(grid)","975d1682":"cols_to_drop = ['target', 'date_block_num']\ndef train_val_test_split(df):\n    dates = df['date_block_num']\n    last_block = dates.max()\n    print('Test `date_block_num` is %d' % last_block)\n    print('Validation `date_block_num` is %d' % (last_block - 1))\n    print('Train `date_block_num` is < %d' % (last_block - 1))\n    print('------------------------------')\n\n    X_train = df.loc[dates < last_block - 1].drop(cols_to_drop, axis = 1)\n    X_val = df.loc[dates == last_block - 1].drop(cols_to_drop, axis = 1)\n    X_test =  df.loc[dates == last_block].drop(cols_to_drop, axis = 1)\n\n    y_train = df.loc[dates < last_block - 1, 'target'].values\n    y_val =  df.loc[dates == last_block - 1, 'target'].values\n    \n    print('X_train shape: ', X_train.shape)\n    print('y_train shape: ', y_train.shape)\n    print('------------------------------')\n    print('X_val shape: ', X_val.shape)\n    print('y_val shape: ', y_val.shape)\n    print('------------------------------')\n    print('X_test shape: ', X_test.shape)\n    print('------------------------------')\n    return (X_train, y_train, X_val, y_val, X_test)\n\ndef rmse(y, y_hat):\n    return np.sqrt(mean_squared_error(y, y_hat))\n\ndef create_lgbm_model(X_train, y_train, X_val, y_val, params, cat_feats):\n    n_estimators = 8000\n    d_train = lgb.Dataset(X_train, y_train)\n    d_valid = lgb.Dataset(X_val, y_val)\n    watchlist = [d_train, d_valid]\n    evals_result = {}\n    model = lgb.train(params, \n                      d_train, \n                      n_estimators,\n                      valid_sets = watchlist, \n                      evals_result = evals_result, \n                      early_stopping_rounds = 50,\n                      verbose_eval = 0,\n                      categorical_feature = cat_feats,\n                    )\n    lgb.plot_metric(evals_result)\n    return model\n\ndef evaluate_model(model, X_train, y_train, X_val, y_val): \n    y_hat = model.predict(X_train)\n    print('Training error;', rmse(y_train, y_hat))\n    y_val_hat = model.predict(X_val)\n    print('Validation error:', rmse(y_val, y_val_hat))","2765914e":"categorical_features = ['shop_id', 'item_category_id', 'month', 'category_shop_inter']\nfor col in categorical_features:\n    grid.loc[:, col] = grid[col].astype('category')","41683379":"X_train, y_train, X_val, y_val, X_test = train_val_test_split(grid)","b9d53264":"start = time.time()\nparams = {\n  'metric': 'rmse',\n  'objective': 'mse',\n  'verbose': 0, \n  'learning_rate': 0.1,\n  'num_leaves': 31,\n  'min_data_in_leaf': 20 ,\n  'max_depth': -1,\n  'save_binary': True,\n  'bagging_fraction': 0.8,\n  'bagging_freq': 1,\n  'bagging_seed': 2**7, \n  'feature_fraction': 0.8,\n}\nlgbm_model = create_lgbm_model(X_train, y_train, X_val, y_val, params, categorical_features)\nprint('it tooks: ', round(time.time() - start), 'seconds')","fd008960":"start = time.time()\nevaluate_model(lgbm_model, X_train, y_train, X_val, y_val)\nprint('it tooks: ', round(time.time() - start), 'seconds')","8e63a7da":"ax = lgb.plot_importance(lgbm_model, max_num_features = 40, figsize = (8, 10))\nplt.show()","cad030ac":"y_test_pred = lgbm_model.predict(X_test).clip(0, 20)\n\nsubmission = pd.DataFrame({\n    \"ID\": test.ID, \n    \"item_cnt_month\": y_test_pred\n})\nsubmission.to_csv('lgbm_submission.csv', index = False)","b50be6a8":"submission.item_cnt_month.hist()","b84db378":"## Create training grid","6182f298":"# Data cleaning","60d25bb8":"**We will clip all sales amount for shops\/items to [0, 20] when we constract the train set.**","81447b34":"### How much to go back in time for shops","3f325a7d":"## Remove duplicated shops\n\nThe shops below have similar names","71ddaecb":"## Train a model","9c78f2bb":"# Modeling\n\n","e1c6590a":"## train\/val split","e6ed4afa":"In order to create lag features, we need to determine how much the previous values of the features affect the prediction of the current one.\n\nI will use Autocorrelation function **(acf)** to perfom this analysis.","9834a523":"## Shop name preprocessing\n\nShop name contains the following info(https:\/\/www.kaggle.com\/kyakovlev):\n\ncity | shop_type | shop_name","760fc2d2":"### How much to go back in time for pairs(shop\/item)\nI will choose 1000 pairs and compute how much the previous values of the series (lags) may be helpful in predicting the current value.","04ccddf6":"## Delete duplicated records","6a0789f5":"## Concatenate test set to the grid","a883be96":"## Pair plot for sales\nThis is an interesting plot because:\n- it shows the histogram (the diagonal) for each columns\n- it shows if there is outliers in each columns\n- it draws each columns with respect to other columns","a059388b":"## Lag features","43cc34cc":"## Date Features","cbe4d551":"### shop sales history","f744a943":"## item_price outliers","eb2a6039":"## Add lag feature to grid","80844d61":"# Submission","b9b15453":"## item_cnt_day outliers","4e72a8f2":"## Convert sales.date form string to datetime","a359323a":"### How much to go back in time for categories","1c082851":"**Introduction**\n\nOur mission in this project is to find a mapping between **a(t)** and **a(t - k)**, where ***a*** is the amount of selling each item in each shop and ***t*** is time (months). In other words we want to find a series A where n = A(n-1). \n\n**Table of contents**\n- Loading and downcasting data\n- Exploratory data analysis\n    * Pair plot\n    * Outliers removal\n    * Duplicated sales removal\n    * Sales history\n    * Test set distribution\n- Cleaning data\n    * Shops names preprocessing\n    * Duplicated shops removal\n    * Items names preprocessing\n- Feature extraction\n    * [Lag features](https:\/\/machinelearningmastery.com\/basic-feature-engineering-time-series-data-python\/)\n    * Date features\n    * Feature interactions\n- Modeling\n    * X_train, X_val splitting\n    * Training LightGBM model\n    * Evaluate model","5865b9d9":"### Summary\nAs we see, We don't have full sale history for items, shops and item\/shop pairs during 2 years and 10 months. For example\n\n- for the first year 38.54 % of the item\/shop pairs have made no sales\n- for the last year 57.91 % of the item\/shop pairs have made no sales\n\nThe absence of sales historical data will make the prediction is a challenging task.\n\nWe have 214200 pairs of shop\/item that we want to predict their future sales for the month 34. The problem is modeled as:\n\ntarget_pair1(t = 0), target_pair1(t = 1), target_pair1(t = 2), ..., target_pair1(t = k) >>>*predict*>>> target_pair1(t = 34).\n\ntarget_pair2(t = 0), target_pair2(t = 1), target_pair2(t = 2), ..., target_pair2(t = k) >>>*predict*>>> target_pair2(t = 34)\n\ntarget_pair_m(t = 0), target_pair_m(t = 1), target_pair_m(t = 2), ..., target_pair_m(t = k) >>>*predict*>>> target_pair_m(t = 34)\n\n**Note**\n\nThe amount of selling items is dependant on the time and the shop as well, see the following plot:","daaca895":"## Feature Interaction","10648b26":"## Item_name preprocessing\nitem_name contains the name, type, and subtype in following format (https:\/\/www.kaggle.com\/kyakovlev):\n    \n- item_name [item_type] (item_subtype).","3f6283fd":"### How much to go back in time for items","ab594ae1":"## Evaluate model","79421b5a":"# Load data","3923c272":"## Sales history","a40d405d":"### shop\/item pair sales history","188b8f5f":"### item sales history","fc196634":"### Category\/Shop","77240183":"# Feature Engineering","0c2557d7":"## Test set distribution","df3647fa":"# EDA"}}