{"cell_type":{"66a432b1":"code","d143cb61":"code","e6bfce03":"code","2933dc7c":"code","4b4c65b3":"code","c476c1dd":"code","71c263b9":"code","f6196523":"code","9c8d2bf2":"code","bbd24097":"code","64c48fe0":"code","6fbcc12a":"code","1d33e0e2":"code","e0194c6a":"code","3b63362c":"code","a1a75f58":"code","fb2d2568":"code","35e8efd6":"code","e73d4df0":"code","cd589b34":"code","8b750085":"code","715a209b":"code","5a21652d":"code","d4260b87":"code","3bc23e39":"markdown","43b92be0":"markdown","ec17f60c":"markdown","7b2a4549":"markdown","fa456b16":"markdown","c916bd54":"markdown","52debbdc":"markdown","e9b7e895":"markdown","f8be6bf4":"markdown","4f491beb":"markdown","c6664842":"markdown","0b145508":"markdown","d866b1ba":"markdown","330ae7de":"markdown"},"source":{"66a432b1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d143cb61":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport keras\nfrom keras.utils import to_categorical\n\nimport re\nfrom nltk.corpus import stopwords\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n","e6bfce03":"tweets = pd.read_csv(\"..\/input\/twitter-airline-sentiment\/Tweets.csv\")\ntweets.head(5)","2933dc7c":"print(tweets.columns)\ndf = pd.DataFrame({\n    \"sentiment\":tweets.airline_sentiment,\n    \"text\":tweets.text\n})\nprint(f\"Shape of our dataset >> {df.shape}\")\ndf.head()","4b4c65b3":"df.sentiment.replace(['negative','neutral','positive'],[-1,0,1],inplace=True)\ndf.sample(5)\n\nsns.set_style(\"whitegrid\")\nsns.countplot(data=df,x='sentiment')\nindex = [0,1,2]\nplt.xticks(index,['negative','neutral','positive'])\nplt.title(\"Distribution of sentiment labels\")\nplt.show()","c476c1dd":"stop_words = set(stopwords.words('english'))\n\nshortword = re.compile(r\"\\b\\w{1,2}\\b\")\nurl = re.compile(r\"https?:*\/+[a-zA-Z0-9.\/]*\")\n\ndef clean(text):\n    text = re.sub(url,'',text)\n    text = re.sub(shortword,'',text)\n    text = text.replace('@','')\n    text = text.replace('#','')\n    \n    text = text.split()\n    text = [word for word in text if word not in stop_words]\n    text = \" \".join(text)\n    \n    return text\n\ndf.text = df.text.apply(clean)\ndf.text[:15]","71c263b9":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(df.text)\nprint(f\"{len(tokenizer.word_index)} words are used\\n\")\n\ncounts = tokenizer.word_counts\nprint(len(counts))\n\ntotal_freq = 0\nrare_freq = 0\nrare_counts = 0\nthread=2\n\nfor key,value in counts.items():\n    total_freq += value\n    if value<thread:\n        rare_freq += value\n        rare_counts += value\n\nprint(f\"{rare_counts} are used less than {thread} times\")\nprint(f\"And these words accounts for {np.round(rare_freq\/total_freq*100,2)}% of whole texts\")","f6196523":"print(\"Tokenize only 7000 words.\\nOther words are considered OOV\")\nword_size=7000\nvocab_size = word_size+1\ntokenizer = Tokenizer(num_words=word_size)\n\ntokenizer.fit_on_texts(df.text)\ntokenized = tokenizer.texts_to_sequences(df.text)\n\nprint(\"\\nSamples\\n\")\nprint(tokenized[0])\nprint(tokenized[1])\nprint(len(tokenized))","9c8d2bf2":"lengths = [len(s) for s in tokenized]\nprint(f\"Average length of each row >> {np.mean(lengths)}\")\nprint(f\"Maximum length of each row >> {np.max(lengths)}\")\n\nplt.hist(lengths,bins=50)\nplt.show()\n\nsequence_size = 20\nprint(f\"Pad all sequences into size of {sequence_size}\")\n\npadded = pad_sequences(tokenized,maxlen=sequence_size,padding='post',truncating='post')\nprint(padded.shape)\nprint(\"Padded samples\")\nprint(padded[0])\nprint(padded[1])","bbd24097":"data = padded\nlabel = to_categorical(df.sentiment,num_classes=3)\n\nprint(\"shape of data >>\",data.shape)\nprint(\"shape of label >>\",label.shape)\n\nprint(\"\\nSamples of label data\")\nprint(label[0])\nprint(label[1])","64c48fe0":"train_data,test_data,train_label,test_label = train_test_split(data,label,test_size=0.3,stratify=label,random_state=42)\n\nprint(\"shape of train data >>\",train_data.shape)\nprint(\"shape of test data >>\",test_data.shape)\n\nfig = plt.figure(figsize=(12,6))\nax1 = fig.add_subplot(1,2,1)\nsns.countplot(x=np.argmax(train_label,axis=1))\nplt.title(\"Distribution of train label\")\n\nax2 = fig.add_subplot(1,2,2)\nsns.countplot(x=np.argmax(test_label,axis=1))\nplt.title(\"Distribution of test label\")\nplt.show()\n\nindex_to_sentiment = {\n    0:'neutral',\n    1:'positive',\n    -1:'negative'\n}","6fbcc12a":"import os\n\n!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip glove*.zip","1d33e0e2":"embedding_dict=dict()\n\nf = open(os.path.join('glove.6B.100d.txt'),encoding='utf-8')\nfor line in f:\n    tokens = line.split()\n    word = tokens[0]\n    word_vector = np.asarray(tokens[1:],dtype='float32')\n    embedding_dict[word] = word_vector\n\nf.close()\n\nprint(f\"There are {len(embedding_dict)} embedding vectors in total\")\nprint(f\"Dimension of each vector >> {len(embedding_dict['read'])}\")\nembedding_size = len(embedding_dict['read'])\n\n\nembedding_matrix = np.zeros((vocab_size,embedding_size))\n\nfor word,idx in tokenizer.word_index.items():\n    if idx <= 7000:\n        vector = embedding_dict.get(word)\n        if vector is not None:\n            embedding_matrix[idx] = np.asarray(vector,dtype='float32')","e0194c6a":"from keras.layers import Input,Embedding,TimeDistributed,Bidirectional,LSTM,BatchNormalization,Dense,GlobalMaxPool1D,GlobalAveragePooling1D,Dropout,Masking\nfrom keras.callbacks import EarlyStopping,ReduceLROnPlateau\nfrom keras.utils import plot_model\n\nword_vec_size=100\nhidden_size=128\n\ndef create_lstm():\n    X = Input(shape=[sequence_size])\n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size,weights=[embedding_matrix],trainable=False)(X)\n    H = Masking(mask_value=0.0)(H)\n    \n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = Bidirectional(LSTM(int(hidden_size\/2),return_sequences=True))(H)\n    H = Bidirectional(LSTM(int(hidden_size\/2),return_sequences=True))(H)\n    \n    H = GlobalMaxPool1D()(H)\n    H = BatchNormalization()(H)\n    H = Dense(32,activation='relu')(H)\n    H = BatchNormalization()(H)\n    Y = Dense(3,activation='softmax')(H)\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    return model\n\nes = EarlyStopping(monitor='val_accuracy',mode='min',patience=4,verbose=1)\nrl = ReduceLROnPlateau(monitor='val_loss',mode='min',patience=3,verbose=1)","3b63362c":"lstm_model = create_lstm()\nplot_model(lstm_model)\nlstm_history = lstm_model.fit(train_data,train_label,epochs=10,batch_size=64,validation_split=0.2,callbacks=[rl])\nlstm_model.evaluate(test_data,test_label)","a1a75f58":"word_vec_size=100\nhidden_size=256\n\ndef create_lstm_no_emb():\n    X = Input(shape=[sequence_size])\n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size)(X)\n    H = Masking(mask_value=0.0)(H)\n    \n    H = Bidirectional(LSTM(hidden_size,return_sequences=True))(H)\n    H = BatchNormalization()(H)\n    H = Bidirectional(LSTM(int(hidden_size\/2),return_sequences=True))(H)\n    H = BatchNormalization()(H)\n    H = Bidirectional(LSTM(int(hidden_size\/2),return_sequences=True))(H)\n    \n    H = GlobalMaxPool1D()(H)\n    H = Dense(64,activation='relu')(H)\n    H = Dense(32,activation='relu')(H)\n    H = Dropout(0.2)(H)\n    Y = Dense(3,activation='softmax')(H)\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    return model","fb2d2568":"lstm_no = create_lstm_no_emb()\nplot_model(lstm_no)\nlstm_no_hist = lstm_model.fit(train_data,train_label,epochs=15,batch_size=64,validation_split=0.2,callbacks=[rl])\nlstm_no.evaluate(test_data,test_label)","35e8efd6":"from keras.layers import Conv1D,Concatenate,LeakyReLU,Flatten\n\ndef create_conv1d():\n    X = Input(shape=[sequence_size])\n    H = Embedding(vocab_size,word_vec_size,input_length=sequence_size,weights=[embedding_matrix],trainable=False,mask_zero=True)(X)\n    H = Dropout(0.3)(H)\n    \n    num_filters=[256,256,128,128]\n    kernel_sizes=[3,4,5,6]\n    conv_blocks=[]\n    \n    for i in range(len(kernel_sizes)):\n        conv = Conv1D(filters=num_filters[i],kernel_size=kernel_sizes[i],padding='valid',activation='relu')(H)\n        conv = GlobalMaxPool1D()(conv)\n        conv = Flatten()(conv)\n        conv_blocks.append(conv)\n    \n    H = Concatenate()(conv_blocks)\n    H = Dropout(0.2)(H)\n    \n    H = Dense(128)(H)\n    H = BatchNormalization()(H)\n    H = LeakyReLU()(H)\n    \n    H = Dense(16)(H)\n    H = BatchNormalization()(H)\n    H = LeakyReLU()(H)\n    \n    Y = Dense(3,activation='softmax')(H)\n    \n    model = keras.models.Model(X,Y)\n    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n    return model","e73d4df0":"conv1d = create_conv1d()\nplot_model(conv1d)\nhist = conv1d.fit(train_data,train_label,epochs=10,validation_split=0.2,batch_size=64,callbacks=[rl])\nconv1d.evaluate(test_data,test_label)","cd589b34":"df.head()","8b750085":"from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n\nvectorizer = CountVectorizer()\ntransformer = TfidfTransformer()\n\nx = vectorizer.fit_transform(df.text)\nprint(f\"shape >> {x.toarray().shape}\")\nprint(\"samples\\n\")\nprint(x.toarray()[0])\n\nx = transformer.fit_transform(x)\nprint(f\"\\n\\nshape >> {x.toarray().shape}\")\nprint(\"samples\\n\")\nprint(x.toarray()[0])\n\nx = x.toarray()\ny = df.sentiment","715a209b":"train_x,test_x,train_y,test_y = train_test_split(x,y,test_size=0.3,random_state=42,stratify=y)\n\nprint(train_x.shape)\nprint(train_y.shape)\nprint(test_x.shape)\nprint(test_y.shape)","5a21652d":"from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\n\nmodels_NB = []\nmodels_NB.append(GaussianNB())\nmodels_NB.append(MultinomialNB())\nmodels_NB.append(BernoulliNB())\n\nfor model in models_NB:\n    model.fit(train_x,train_y)\n    pred = model.predict(test_x)\n    acc = accuracy_score(test_y,pred)\n    print(f\"Model {model.__class__.__name__} accuracy on test dataset >> {acc}\")","d4260b87":"from sklearn.ensemble import RandomForestClassifier\n\n","3bc23e39":"(4) Tokenize (words to integers)","43b92be0":"(9)Multi Kernel Conv1D model (using pre-trained Embedding Vectors)","ec17f60c":"(1) Load Libraries","7b2a4549":"(5) Pad & truncate sequences (post)","fa456b16":"(6) Transform label (setiment data) into one-hot vectors","c916bd54":"(7) Train\/Test split","52debbdc":"(2) Load tweets dataset","e9b7e895":"(8) Train and Test with LSTM model ( using pre-trained embedding vectors)","f8be6bf4":"(10) Naive Bayes Models : GaussianNB, MultiNomailNB, BernoulliNB","4f491beb":"(8-3) LSTM model without using pre-trained embedding vectors","c6664842":"(8-2) Build Stacked LSTM model (+ bidirectional, many-to-many)","0b145508":"(3) Clean text data: \n* remove urls\n* remove shortwords ( words of which length is 1 or 2)\n* remove @\n* remove #\n* remove stopwords from nltk.corpus module","d866b1ba":"(11) RandomForestClassifier","330ae7de":"(8-1) Load pre-trained Embedding vectors and make an embedding matrix  customized for words we will use"}}