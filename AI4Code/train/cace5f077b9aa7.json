{"cell_type":{"f40d6ff0":"code","ce030860":"code","d9158a2d":"code","b6cbabe4":"code","cb573bc7":"code","4e68036d":"code","4955d5a0":"code","519ee9c1":"code","c48883a5":"code","5b016c67":"code","66b08fbe":"code","deb69c9e":"code","0854e6a5":"code","e570ba02":"code","8ff23261":"code","c628f9ac":"code","6ccae8f0":"code","d1ae4360":"code","557eb150":"code","5a7f1307":"code","650373b8":"code","61b3bea4":"code","080e0a50":"code","16c501d7":"code","8662cd77":"code","425b257b":"code","a1b4f2df":"code","a207a33a":"code","548c702d":"code","2a779f17":"code","9582f425":"code","7a7639b0":"code","4dae8516":"code","a8340ffc":"code","3940af97":"code","a461bebb":"code","9b296ee6":"code","9e9e3166":"code","4de5f966":"code","6979ca5c":"code","1024a94f":"code","54336ca3":"code","715ec1e1":"code","442232a8":"code","7da470c5":"code","8bcf1f8b":"code","cfecd7a2":"code","dc1f6d15":"code","e72ff481":"code","8746283d":"code","39d130eb":"code","a0d9b4b9":"code","a4fa4fd2":"code","f2932104":"code","4a9608df":"code","3c8d0d8c":"code","baac15e0":"code","d5f1bee0":"code","f9cf5a01":"code","d483ab1a":"code","9f987876":"code","f85195df":"code","4a7f2253":"code","bbd2f09b":"code","efcaae8f":"code","cfc0b005":"markdown","e667da53":"markdown","c2db1e4f":"markdown","fa1d0822":"markdown","74d7f819":"markdown","415cf779":"markdown","810fd8cb":"markdown","c7311805":"markdown","f89e457d":"markdown","5e2fd1e9":"markdown","486d89e5":"markdown","c6f9e429":"markdown","de66e528":"markdown","9996e441":"markdown","74712b15":"markdown","366e695a":"markdown","42bd1972":"markdown","0e04c42b":"markdown","9303c93c":"markdown","5e0f6e9c":"markdown","decc86de":"markdown","368013d3":"markdown","1730ddc0":"markdown","caacd8d1":"markdown","85845f4b":"markdown","6d4e6315":"markdown","ffe9af7d":"markdown","94e218d6":"markdown","92b86901":"markdown","a7b6ac69":"markdown","5386c96e":"markdown","e773eb16":"markdown","d35f8eb8":"markdown","61020aa0":"markdown","f6c162b7":"markdown","4aa2e986":"markdown","89120c38":"markdown","289eaa12":"markdown","32d825dd":"markdown","b52708ef":"markdown","8df0e1ae":"markdown","f32dedd2":"markdown","21a7f4bd":"markdown","3410bb26":"markdown","7a0adff1":"markdown","48a2e6cc":"markdown","d97d3664":"markdown","1edfa14f":"markdown","5cac581c":"markdown","ccb409d6":"markdown","2fba7114":"markdown","27ff8fac":"markdown"},"source":{"f40d6ff0":"import numpy as np\nimport pandas as pd\nimport pandas_profiling as pp\nimport math\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# preprocessing\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, learning_curve, ShuffleSplit\nfrom sklearn.model_selection import cross_val_predict as cvp\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, accuracy_score, confusion_matrix, explained_variance_score\n\n# models\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn import metrics\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\n\n# Modeling - NN models\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras import optimizers\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.callbacks import ReduceLROnPlateau\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ce030860":"cv_n_split = 3\nrandom_state = 40\ntest_train_split_part = 0.2\nnum_models = 10","d9158a2d":"metrics_all = {1 : 'r2_score', 2: 'acc', 3 : 'rmse', 4 : 're'}\nmetrics_now = [1, 2, 3, 4] # you can only select some numbers of metrics from metrics_all","b6cbabe4":"data = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\")","cb573bc7":"data.head(3)","4e68036d":"data.describe([.05, .95])","4955d5a0":"# data = data[(data['chol'] <= 326.9) & (data['oldpeak'] <=3.4)].reset_index(drop=True)\n# data","519ee9c1":"data.describe()","c48883a5":"data.info()","5b016c67":"#pp.ProfileReport(data)","66b08fbe":"data = data.drop_duplicates()\ndata.shape","deb69c9e":"data.describe()","0854e6a5":"data","e570ba02":"def features_creation(df):\n    # Features creation\n    df['thalach2'] = df['thalach']\/\/40\n    df['oldpeak2'] = df['oldpeak']\/\/0.4\n    for col1 in ['sex']:\n        for col2 in ['oldpeak2', 'thalach2']:\n            df[col1 + \"_\" + col2] = df[col1].astype('str') + \"_\" + df[col2].astype('str')\n    df = df.drop(columns = ['oldpeak2', 'thalach2'])\n    return df","8ff23261":"# Features creation\ndata = features_creation(data)\nlen(data.columns)","c628f9ac":"# Determination categorical features\ncategorical_columns = []\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nfeatures = data.columns.values.tolist()\nfor col in features:\n    if data[col].dtype in numerics: continue\n    categorical_columns.append(col)\ncategorical_columns","6ccae8f0":"# Encoding categorical features\nfor col in categorical_columns:\n    if col in data.columns:\n        le = LabelEncoder()\n        le.fit(list(data[col].astype(str).values))\n        data[col] = le.transform(list(data[col].astype(str).values))","d1ae4360":"data","557eb150":"#pp.ProfileReport(data)","5a7f1307":"data.describe()","650373b8":"data.describe().applymap(\"{0:.1f}\".format)","61b3bea4":"# Target\ntarget_name = 'target'\ntarget_all = data.pop(target_name)","080e0a50":"# Standartization data\nscaler = StandardScaler()\ndata = pd.DataFrame(scaler.fit_transform(data), columns = data.columns)","16c501d7":"# Synthesis valid as test for selection models\ntrain, test, target, target_test = train_test_split(data, target_all, test_size=test_train_split_part, random_state=random_state)","8662cd77":"train.head(3)","425b257b":"test.head(3)","a1b4f2df":"train.info()","a207a33a":"test.info()","548c702d":"# list of accuracy of all model - amount of metrics_now * 2 (train & test datasets)\nacc_train = []\nacc_test = []\nacc_all = np.empty((len(metrics_now)*2, 0)).tolist()\nacc_all","2a779f17":"acc_all_pred = np.empty((len(metrics_now), 0)).tolist()\nacc_all_pred","9582f425":"# Splitting train data for model tuning with cross-validation\ncv_train = ShuffleSplit(n_splits=cv_n_split, test_size=test_train_split_part, random_state=random_state)","7a7639b0":"def acc_d(y_meas, y_pred):\n    # Relative error between predicted y_pred and measured y_meas values\n    return mean_absolute_error(y_meas, y_pred)*len(y_meas)\/sum(abs(y_meas))\n\ndef acc_rmse(y_meas, y_pred):\n    # RMSE between predicted y_pred and measured y_meas values\n    return (mean_squared_error(y_meas, y_pred))**0.5","4dae8516":"def plot_cm(target, train_pred, target_test, test_pred):\n    # Building the confusion matrices\n    \n    def cm_calc(y_true, y_pred):\n        cm = confusion_matrix(y_true, y_pred, labels=np.unique(y_true))\n        cm_sum = np.sum(cm, axis=1, keepdims=True)\n        cm_perc = cm \/ cm_sum.astype(float) * 100\n        annot = np.empty_like(cm).astype(str)\n        nrows, ncols = cm.shape\n        for i in range(nrows):\n            for j in range(ncols):\n                c = cm[i, j]\n                p = cm_perc[i, j]\n                if i == j:\n                    s = cm_sum[i]\n                    annot[i, j] = '%.1f%%\\n%d\/%d' % (p, c, s)\n                elif c == 0:\n                    annot[i, j] = ''\n                else:\n                    annot[i, j] = '%.1f%%\\n%d' % (p, c)\n        cm = pd.DataFrame(cm, index=np.unique(y_true), columns=np.unique(y_true))\n        cm.index.name = 'Actual'\n        cm.columns.name = 'Predicted'\n        return cm, annot\n\n    \n    # Building the confusion matrices\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(8, 3), sharex=True)\n    \n    # Training data\n    ax = axes[0]\n    ax.set_title(\"for training data\")\n    cm0, annot0 = cm_calc(target, train_pred)    \n    sns.heatmap(cm0, cmap= \"YlGnBu\", annot=annot0, fmt='', ax=ax)\n    \n    # Test data\n    ax = axes[1]\n    ax.set_title(\"for test (validation) data\")\n    cm1, annot1 = cm_calc(target_test, test_pred)\n    sns.heatmap(cm1, cmap= \"YlGnBu\", annot=annot1, fmt='', ax=ax)\n    \n    fig.suptitle('CONFUSION MATRICES', y=1.05)\n    plt.show()","a8340ffc":"def acc_metrics_calc(num,model,train,test,target,target_test):\n    # The models selection stage\n    # Calculation of accuracy of model by different metrics\n    global acc_all\n\n    ytrain = model.predict(train).astype(int)\n    ytest = model.predict(test).astype(int)\n    if num != 17:\n        print('target = ', target[:5].values)\n        print('ytrain = ', ytrain[:5])\n        print('target_test =', target_test[:5].values)\n        print('ytest =', ytest[:5])\n\n    num_acc = 0\n    for x in metrics_now:\n        if x == 1:\n            #r2_score criterion\n            acc_train = round(r2_score(target, ytrain) * 100, 2)\n            acc_test = round(r2_score(target_test, ytest) * 100, 2)\n        elif x == 2:\n            #accuracy_score criterion\n            acc_train = round(metrics.accuracy_score(target, ytrain) * 100, 2)\n            acc_test = round(metrics.accuracy_score(target_test, ytest) * 100, 2)\n        elif x == 3:\n            #rmse criterion\n            acc_train = round(acc_rmse(target, ytrain) * 100, 2)\n            acc_test = round(acc_rmse(target_test, ytest) * 100, 2)\n        elif x == 4:\n            #relative error criterion\n            acc_train = round(acc_d(target, ytrain) * 100, 2)\n            acc_test = round(acc_d(target_test, ytest) * 100, 2)\n        \n        print('acc of', metrics_all[x], 'for train =', acc_train)\n        print('acc of', metrics_all[x], 'for test =', acc_test)\n        acc_all[num_acc].append(acc_train) #train\n        acc_all[num_acc+1].append(acc_test) #test\n        num_acc += 2\n    \n    #  Building the confusion matrices\n    plot_cm(target, ytrain, target_test, ytest)","3940af97":"def acc_metrics_calc_pred(num,model,name_model,train,test,target):\n    # The prediction stage\n    # Calculation of accuracy of model for all different metrics and creates of the main submission file for the best model (num=0)\n    global acc_all_pred\n\n    ytrain = model.predict(train).astype(int)\n    ytest = model.predict(test).astype(int)\n\n    print('**********')\n    print(name_model)\n    if num != 17:\n        print('target = ', target[:15].values)\n        print('ytrain = ', ytrain[:15])\n        print('ytest =', ytest[:15])\n    \n    num_acc = 0\n    for x in metrics_now:\n        if x == 1:\n            #r2_score criterion\n            acc_train = round(r2_score(target, ytrain) * 100, 2)\n        elif x == 2:\n            #accuracy_score criterion\n            acc_train = round(metrics.accuracy_score(target, ytrain) * 100, 2)\n        elif x == 3:\n            #rmse criterion\n            acc_train = round(acc_rmse(target, ytrain) * 100, 2)\n        elif x == 4:\n            #relative error criterion\n            acc_train = round(acc_d(target, ytrain) * 100, 2)\n\n        print('acc of', metrics_all[x], 'for train =', acc_train)\n        acc_all_pred[num_acc].append(acc_train) #train\n        num_acc += 1\n    \n    # Save the submission file\n    submission[target_name] = ytest\n    submission.to_csv('submission_' + name_model + '.csv', index=False)    ","a461bebb":"# Thanks to https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_learning_curve.html#sphx-glr-auto-examples-model-selection-plot-learning-curve-py\ndef plot_learning_curve(estimator, title, X, y, cv=None, axes=None, ylim=None, \n                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 5), random_state=0):\n    \"\"\"\n    Generate 2 plots: \n    - the test and training learning curve, \n    - the training samples vs fit times curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    axes : array of 3 axes, optional (default=None)\n        Axes to use for plotting the curves.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n\n          - None, to use the default 5-fold cross-validation,\n          - integer, to specify the number of folds.\n          - :term:`CV splitter`,\n          - An iterable yielding (train, test) splits as arrays of indices.\n\n        For integer\/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    train_sizes : array-like, shape (n_ticks,), dtype float or int\n        Relative or absolute numbers of training examples that will be used to\n        generate the learning curve. If the dtype is float, it is regarded as a\n        fraction of the maximum size of the training set (that is determined\n        by the selected validation method), i.e. it has to be within (0, 1].\n        Otherwise it is interpreted as absolute sizes of the training sets.\n        Note that for classification the number of samples usually have to\n        be big enough to contain at least one sample from each class.\n        (default: np.linspace(0.1, 1.0, 5))\n    \n    random_state : random_state\n    \n    \"\"\"\n    fig, axes = plt.subplots(2, 1, figsize=(20, 10))\n    \n    if axes is None:\n        _, axes = plt.subplots(1, 2, figsize=(20, 5))\n\n    axes[0].set_title(title)\n    if ylim is not None:\n        axes[0].set_ylim(*ylim)\n    axes[0].set_xlabel(\"Training examples\")\n    axes[0].set_ylabel(\"Score\")\n\n    cv_train = ShuffleSplit(n_splits=cv_n_split, test_size=test_train_split_part, random_state=random_state)\n    \n    train_sizes, train_scores, test_scores, fit_times, _ = \\\n        learning_curve(estimator=estimator, X=X, y=y, cv=cv,\n                       train_sizes=train_sizes,\n                       return_times=True)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    fit_times_mean = np.mean(fit_times, axis=1)\n    fit_times_std = np.std(fit_times, axis=1)\n\n    # Plot learning curve\n    axes[0].grid()\n    axes[0].fill_between(train_sizes, train_scores_mean - train_scores_std,\n                         train_scores_mean + train_scores_std, alpha=0.1,\n                         color=\"r\")\n    axes[0].fill_between(train_sizes, test_scores_mean - test_scores_std,\n                         test_scores_mean + test_scores_std, alpha=0.1,\n                         color=\"g\")\n    axes[0].plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n                 label=\"Training score\")\n    axes[0].plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n                 label=\"Cross-validation score\")\n    axes[0].legend(loc=\"best\")\n\n    # Plot n_samples vs fit_times\n    axes[1].grid()\n    axes[1].plot(train_sizes, fit_times_mean, 'o-')\n    axes[1].fill_between(train_sizes, fit_times_mean - fit_times_std,\n                         fit_times_mean + fit_times_std, alpha=0.1)\n    axes[1].set_xlabel(\"Training examples\")\n    axes[1].set_ylabel(\"fit_times\")\n    axes[1].set_title(\"Scalability of the model\")\n\n    plt.show()\n    return","9b296ee6":"# Linear Regression\nlinreg = LinearRegression()\nlinreg_CV = GridSearchCV(linreg, param_grid={}, cv=cv_train, verbose=False)\nlinreg_CV.fit(train, target)\nacc_metrics_calc(0,linreg_CV,train,test,target,target_test)","9e9e3166":"# Building learning curve of model\nplot_learning_curve(linreg, \"Linear Regression\", train, target, cv=cv_train)","4de5f966":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg_CV = GridSearchCV(estimator=logreg, param_grid={'C' : [.2, .3, .4]}, cv=cv_train, verbose=False)\nlogreg_CV.fit(train, target)\nprint(logreg_CV.best_params_)\nacc_metrics_calc(1,logreg_CV,train,test,target,target_test)","6979ca5c":"# Building learning curve of model\nplot_learning_curve(logreg, \"Logistic Regression\", train, target, cv=cv_train)","1024a94f":"# Decision Tree Classifier - simplest model\n\ndecision_tree = DecisionTreeClassifier(max_depth=3, random_state=random_state)\ndecision_tree.fit(train, target)\nacc_metrics_calc(2,decision_tree,train,test,target,target_test)","54336ca3":"# Building learning curve of model\nplot_learning_curve(decision_tree, \"Decision Tree\", train, target, cv=cv_train)","715ec1e1":"# Visualization - build a plot with Decision Tree\nplt.figure(figsize=(20,12))\nplot_tree(decision_tree, filled=True, rounded=True, class_names=[\"1\", \"0\"], feature_names=train.columns) ","442232a8":"# Decision Tree Classifier with GridSearchCV\n\ndecision_tree = DecisionTreeClassifier()\nparam_grid = {'min_samples_leaf': [i for i in range(2,10)]}\ndecision_tree_CV = GridSearchCV(decision_tree, param_grid=param_grid, cv=cv_train, verbose=False)\ndecision_tree_CV.fit(train, target)\nprint(decision_tree_CV.best_params_)\nacc_metrics_calc(3,decision_tree_CV,train,test,target,target_test)","7da470c5":"# Building learning curve of model\nplot_learning_curve(decision_tree, \"Decision Tree with GridSearchCV\", train, target, cv=cv_train)","8bcf1f8b":"%%time\n# Random Forest\n# Parameters of model (param_grid) taken from the notebook https:\/\/www.kaggle.com\/morenovanton\/titanic-random-forest\n\nrandom_forest = RandomForestClassifier()\nparam_grid = {'n_estimators': [40, 50, 60], 'min_samples_split': [40, 50, 60, 70], 'min_samples_leaf': [12, 13, 14, 15, 16, 17], \n              'max_features': ['auto'], 'max_depth': [3, 4, 5, 6], 'criterion': ['gini'], 'bootstrap': [False]}\nrandom_forest_CV = GridSearchCV(estimator=random_forest, param_grid=param_grid, \n                             cv=cv_train, verbose=False)\nrandom_forest_CV.fit(train, target)\nprint(random_forest_CV.best_params_)\nacc_metrics_calc(4,random_forest_CV,train,test,target,target_test)","cfecd7a2":"# Building learning curve of model\nplot_learning_curve(random_forest, \"Random Forest\", train, target, cv=cv_train)","dc1f6d15":"%%time\n# XGBoost Classifier\nxgb_clf = xgb.XGBClassifier(objective='reg:squarederror') \nparameters = {'n_estimators': [50, 60, 70, 80, 90], \n              'learning_rate': [0.09, 0.1, 0.15, 0.2],\n              'max_depth': [3, 4, 5]}\nxgb_reg = GridSearchCV(estimator=xgb_clf, param_grid=parameters, cv=cv_train).fit(train, target)\nprint(\"Best score: %0.3f\" % xgb_reg.best_score_)\nprint(\"Best parameters set:\", xgb_reg.best_params_)\nacc_metrics_calc(5,xgb_reg,train,test,target,target_test)","e72ff481":"# Building learning curve of model\nplot_learning_curve(xgb_clf, \"XGBoost Classifier\", train, target, cv=cv_train)","8746283d":"#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(train, target, test_size=test_train_split_part, random_state=random_state)\nmodelL = lgb.LGBMClassifier(n_estimators=1000, num_leaves=40)\nmodelL.fit(Xtrain, Ztrain, eval_set=[(Xval, Zval)], early_stopping_rounds=50, verbose=True)","39d130eb":"acc_metrics_calc(6,modelL,train,test,target,target_test)","a0d9b4b9":"fig =  plt.figure(figsize = (10,10))\naxes = fig.add_subplot(111)\nlgb.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();\nplt.close()","a4fa4fd2":"def build_nn():\n\n    # Initializing the NN with 3 layers including 2 hidden layers\n    model = Sequential()\n\n    # The first hidden layer of the NN with input data\n    model.add(Dense(units=16, activation='relu', input_shape=(len(train.columns),)))\n    \n    # The second hidden layer of the NN\n    model.add(Dense(units=8, activation='sigmoid'))\n    \n    # The output layer\n    model.add(Dense(units=1, activation='sigmoid'))\n\n    # Compiling the NN\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                    patience=3, \n                                    verbose=1, \n                                    factor=0.15, \n                                    min_lr=0.0001)\n    return model\n\nXtrain, Xval, Ztrain, Zval = train_test_split(train, target, test_size=test_train_split_part, random_state=random_state)\nnn_model = build_nn()\nnn_model.fit(Xtrain, Ztrain, batch_size=16, epochs=200, validation_data=(Xval, Zval), verbose=1)\n\n# Drawing metrics plot\nplt.plot(nn_model.history.history['val_loss'])\nplt.title('Metrics of NN model')\nplt.xlabel('Epochs')\nplt.ylabel('val_loss') \nplt.show()\n\nacc_metrics_calc(7,nn_model,train,test,target,target_test)","f2932104":"nn_model.summary()","4a9608df":"def build_nn2():\n\n    # Initializing the NN with 3 layers including 2 hidden layers\n    model = Sequential()\n\n    # The first hidden layer of the NN with input data\n    model.add(Dense(units=16, activation='relu', input_shape=(len(train.columns),)))\n    \n    # Dropout\n    model.add(Dropout(0.2))\n    \n    # The second hidden layer of the NN\n    model.add(Dense(units=8, activation='sigmoid'))\n    \n    # The output layer\n    model.add(Dense(units=1, activation='sigmoid'))\n\n    # Compiling the NN\n    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                    patience=3, \n                                    verbose=1, \n                                    factor=0.15, \n                                    min_lr=0.0001)\n    return model\n\nXtrain, Xval, Ztrain, Zval = train_test_split(train, target, test_size=test_train_split_part, random_state=random_state)\nnn_model2 = build_nn2()\nnn_model2.fit(Xtrain, Ztrain, batch_size=16, epochs=200, validation_data=(Xval, Zval), verbose=0)\n\n# Drawing metrics plot\nplt.plot(nn_model2.history.history['val_loss'])\nplt.title('Metrics of NN model')\nplt.xlabel('Epochs')\nplt.ylabel('val_loss') \nplt.show()\n\nacc_metrics_calc(8,nn_model2,train,test,target,target_test)","3c8d0d8c":"%%time\n# MLPClassifier\n\nmlp = MLPClassifier()\nparam_grid = {'hidden_layer_sizes': [i for i in range(2,10)],\n              'solver': ['sgd'],\n              'learning_rate': ['adaptive'],\n              'max_iter': [100]\n              }\nmlp_GS = GridSearchCV(mlp, param_grid=param_grid, cv=cv_train, verbose=False)\nmlp_GS.fit(train, target)\nprint(mlp_GS.best_params_)\nacc_metrics_calc(9,mlp_GS,train,test,target,target_test)","baac15e0":"# Building learning curve of model\nplot_learning_curve(mlp, \"MLP Classifier\", train, target, cv=cv_train)","d5f1bee0":"models = pd.DataFrame({\n    'Model': ['Linear Regression', 'Logistic Regression', 'Decision Tree Classifier', 'DTC with GridSearchCV',\n              'Random Forest Classifier', 'XGB Classifier', 'LGBM Classifier',\n              'NN - Keras', 'NN with Dropout', 'MLP Classifier']})","f9cf5a01":"for x in metrics_now:\n    xs = metrics_all[x]\n    models[xs + '_train'] = acc_all[(x-1)*2]\n    models[xs + '_test'] = acc_all[(x-1)*2+1]\n    if xs == \"acc\":\n        models[xs + '_diff'] = models[xs + '_train'] - models[xs + '_test']\n#models","d483ab1a":"print('Prediction accuracy for models')\nms = metrics_all[metrics_now[1]] # the first from metrics\nmodels[['Model', ms + '_train', ms + '_test', 'acc_diff']].sort_values(by=[(ms + '_test'), (ms + '_train')], ascending=False)","9f987876":"pd.options.display.float_format = '{:,.2f}'.format","f85195df":"for x in metrics_now:   \n    # Plot\n    xs = metrics_all[x]\n    xs_train = metrics_all[x] + '_train'\n    xs_test = metrics_all[x] + '_test'\n    plt.figure(figsize=[15,6])\n    xx = models['Model']\n    plt.tick_params(labelsize=14)\n    plt.plot(xx, models[xs_train], label = xs_train)\n    plt.plot(xx, models[xs_test], label = xs_test)\n    plt.legend()\n    plt.title(str(xs) + ' criterion for ' + str(num_models) + ' popular models for train and test datasets')\n    plt.xlabel('Models')\n    plt.ylabel(xs + ', %')\n    plt.xticks(xx, rotation='vertical')\n    plt.show()","4a7f2253":"# Choose the number of metric by which the best models will be determined =>  {1 : 'r2_score', 2: 'accuracy_score', 3 : 'relative_error', 4 : 'rmse'}\nmetrics_main = 2 \nxs = metrics_all[metrics_main]\nxs_train = metrics_all[metrics_main] + '_train'\nxs_test = metrics_all[metrics_main] + '_test'\nprint('The best models by the',xs,'criterion:')\ndirect_sort = False if (metrics_main >= 2) else True\nmodels_sort = models.sort_values(by=[xs_test, xs_train], ascending=direct_sort)","bbd2f09b":"# Selection the best models except VotingClassifier\nmodels_best = models_sort[(models_sort.acc_diff < 10) & (models_sort.acc_test > 86)]\nmodels_best[['Model', ms + '_train', ms + '_test']].sort_values(by=['acc_test'], ascending=False)","efcaae8f":"# Selection the best models from the best\nmodels_best_best = models_best[(models_best.acc_test > 90)]\nmodels_best_best[['Model', ms + '_train', ms + '_test']].sort_values(by=['acc_test'], ascending=False)","cfc0b005":"### 3.3. EDA<a class=\"anchor\" id=\"3.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","e667da53":"## 4. Preparing to modeling <a class=\"anchor\" id=\"4\"><\/a>\n\n[Back to Table of Contents](#0.1)","c2db1e4f":"## 6. Models selection <a class=\"anchor\" id=\"6\"><\/a>\n\n[Back to Table of Contents](#0.1)","fa1d0822":"## Dataset [Heart Disease UCI](https:\/\/www.kaggle.com\/ronitf\/heart-disease-uci): solving the problem of medical diagnostics\n\n### This notebook is based on the notebook [Heart Disease - Automatic AdvEDA & FE & 20 models](https:\/\/www.kaggle.com\/vbmokin\/heart-disease-automatic-adveda-fe-20-models)\n\n(In more detail, the technology and results of modeling and forecasting are described in my post: [Automation and visualization of EDA & FE & Model selection](https:\/\/www.kaggle.com\/getting-started\/187917) in **\"Getting Started\"** Discussion)\n\n### I. Feature engineering (FE)\nNew features and different combinations of feature pairs are formed. \nThere are many techniques for **selection features**, see the example:\n- [a collection of notebooks for FE](https:\/\/www.kaggle.com\/vbmokin\/data-science-for-tabular-data-advanced-techniques#3)\n- [Titanic - Featuretools (automatic FE&FS)](https:\/\/www.kaggle.com\/vbmokin\/titanic-featuretools-automatic-fe-fs)\n- [sklearn library documentation](https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.feature_selection)\n\n             \nI apply the 7 techniques of features selection and **automatic selection** the best features from them:\n - by the Pearson correlation\n - by the SelectFromModel with LinearSVC\n - by the SelectFromModel with Lasso\n - by the SelectKBest with Chi-2\n - by the Recursive Feature Elimination (RFE) with Logistic Regression\n - by the Recursive Feature Elimination (RFE) with Random Forest\n - by the VarianceThreshold\n\n\n### II. Automatic EDA\nI used packages and methods of automatic EDA with good visualization:\n\n* AV.AutoViz\n* pandas-profiling.ProfileReport\n* pandas.describe\n\n\n### III. Preprocessing \nFor models from Sklearn library, **scaling and standardization** are applied.\n\n\n### IV. Model selection \nModeling is carried out with 20 model-classifier (with tuning and cross-validation):\n- Linear Regression, Logistic Regression\n- Naive Bayes \n- k-Nearest Neighbors algorithm\n- Neural network with Keras\n- Support Vector Machines and Linear SVC\n- Stochastic Gradient Descent, Gradient Boosting Classifier, RidgeCV, Bagging Classifier\n- Decision Tree Classifier, Random Forest Classifier, AdaBoost Classifier, XGB Classifier, LGBM Classifier, ExtraTrees Classifier \n- Gaussian Process Classification\n- MLP Classifier (Deep Learning)\n- Voting Classifier\n\nFor each model, the following are calculated and built:\n- **learning curve plot**\n- **confusion matrices** for train and test data\n\n4 metrics are automatically calculated for each model and the best models are selected with the highest accuracy on test data and the smallest (less 10) difference between the forecast accuracy of the training and test data at the same time - this choice of model reduces the risks of choosing a model with overfitting.\n\nYour comments, votes and feedback are most welcome.","74d7f819":"Thanks to [AI-ML-DS Training. L3AT: NH4 - NN models](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l3at-nh4-nn-models)\n\nThe architecture of neural network:\n![image.png](attachment:image.png)","415cf779":"This model uses a **Decision Tree** as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning).","810fd8cb":"The analysis showed that the available features are poorly divided according to the target values. It is advisable to generate a number of new features.","c7311805":"### 5.4 Decision Tree Classifier with GridSearchCV<a class=\"anchor\" id=\"5.4\"><\/a>\n\n[Back to Table of Contents](#0.1)","f89e457d":"Thanks to notebook [AI-ML-DS Training. L2A: NH4 - Tree Regress models](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l2t-nh4-tree-regress-models)","5e2fd1e9":"## 1. Import libraries <a class=\"anchor\" id=\"1\"><\/a>\n\n[Back to Table of Contents](#0.1)","486d89e5":"Thanks to notebooks:\n* model from [AI-ML-DS Training. L1A: NH4 - linear regression](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l1t-nh4-linear-regression)\n* model tuning from [AI-ML-DS Training. L2A: NH4 - Tree Regress models](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l2t-nh4-tree-regress-models)","c6f9e429":"### 5.5 Random Forest Classifier<a class=\"anchor\" id=\"5.5\"><\/a>\n\n[Back to Table of Contents](#0.1)","de66e528":"The analysis shows different patterns, but most importantly, it confirms that the features are quite diverse. Some features clustering target values quite well, but there are none that do it with 100% accuracy. Those, a good dataset has been formed, but it is impossible to unambiguously choose the optimal model. There is little data, so any model can overfit.","9996e441":"Larger values **r2_score_diff** mean overfitting.","74712b15":"Thanks to [AI-ML-DS Training. L3AT: NH4 - NN models](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l3at-nh4-nn-models)","366e695a":"**Linear Regression** is a linear approach to modeling the relationship between a scalar response (or dependent variable) and one or more explanatory variables (or independent variables). The case of one explanatory variable is called simple linear regression. For more than one explanatory variable, the process is called multiple linear regression. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Linear_regression).\n\nNote the confidence score generated by the model based on our training dataset.","42bd1972":"**Light GBM** is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithms. It splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word \u2018Light\u2019. Reference [Analytics Vidhya](https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/which-algorithm-takes-the-crown-light-gbm-vs-xgboost\/).","0e04c42b":"**Random Forest** is one of the most popular model. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators= [100, 300]) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Random_forest).","9303c93c":"### 5.1 Linear Regression <a class=\"anchor\" id=\"5.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","5e0f6e9c":"### 5.3 Decision Tree Classifier<a class=\"anchor\" id=\"5.3\"><\/a>\n\n[Back to Table of Contents](#0.1)","decc86de":"## 5. Models tuning<a class=\"anchor\" id=\"5\"><\/a>\n\n[Back to Table of Contents](#0.1)","368013d3":"### 5.10 MLP Classifier<a class=\"anchor\" id=\"5.10\"><\/a>\n\n[Back to Table of Contents](#0.1)","1730ddc0":"### 5.9 Neural network with Dropout - Keras<a class=\"anchor\" id=\"5.9\"><\/a>\n\n[Back to Table of Contents](#0.1)","caacd8d1":"Thanks to https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions\n\nNow we are ready to train a model and predict the required solution. There are very many predictive modeling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification problem. With these two criteria - Supervised Learning, we can narrow down our choice of models to a few. These include:\n\n- Linear Regression, Logistic Regression (Sklearn)\n- Decision Tree Classifier, Random Forest Classifier (Sklearn)\n- XGB Classifier (Xgboost, Google) \n- LGBM Classifier (Lightgbm, Microsoft) \n- Neural network (DL - Keras)\n- MLP Classifier (DL - Sklearn)\n\nSome model is built using cross-validation. The parameters of the model are selected to ensure the maximum matching of accuracy on the training and validation data. A plot is being built for this purpose with [learning_curve](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.learning_curve.html?highlight=learning_curve#sklearn.model_selection.learning_curve) from sklearn library.","85845f4b":"**XGBoost** is an ensemble tree method that apply the principle of boosting weak learners (CARTs generally) using the gradient descent architecture. XGBoost improves upon the base Gradient Boosting Machines (GBM) framework through systems optimization and algorithmic enhancements. Reference [Towards Data Science.](https:\/\/towardsdatascience.com\/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)","6d4e6315":"### 3.1. Initial EDA for FE<a class=\"anchor\" id=\"3.1\"><\/a>\n\n[Back to Table of Contents](#0.1)","ffe9af7d":"## 2. Download datasets <a class=\"anchor\" id=\"2\"><\/a>\n\n[Back to Table of Contents](#0.1)","94e218d6":"We can now rank our evaluation of all the models to choose the best one for our problem.","92b86901":"### It is recommended to start studying this course from notebooks:\n* [AI-ML-DS Training. L1T : Titanic - Decision Tree](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l1t-titanic-decision-tree)\n* [AI-ML-DS Training. L1T : NH4 - linear regression](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l1t-nh4-linear-regression)\n* [AI-ML-DS Training. L2A: NH4 - Tree Regress models](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l2t-nh4-tree-regress-models)\n* [BOD prediction in river - 15 regression models](https:\/\/www.kaggle.com\/vbmokin\/bod-prediction-in-river-15-regression-models)\n\nand then move on to this notebook.","a7b6ac69":"**Logistic Regression** is a useful model to run early in the workflow. Logistic regression measures the relationship between the categorical dependent variable (feature) and one or more independent variables (features) by estimating probabilities using a logistic function, which is the cumulative logistic distribution. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Logistic_regression).","5386c96e":"Thanks to notebooks:\n* model from [AI-ML-DS Training. L1A: Titanic - Decision Tree](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l1t-titanic-decision-tree)\n* model tuning from [AI-ML-DS Training. L2A: NH4 - Tree Regress models](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l2t-nh4-tree-regress-models)","e773eb16":"Thanks to notebook [AI-ML-DS Training. L1A: Titanic - Decision Tree](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l1t-titanic-decision-tree)","d35f8eb8":"The **MLPClassifier** optimizes the squared-loss using LBFGS or stochastic gradient descent by the Multi-layer Perceptron regressor. Reference [Sklearn documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor).","61020aa0":"1. Pandas Profiling","f6c162b7":"Thanks to notebook [AI-ML-DS Training. L2A: NH4 - Tree Regress models](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l2t-nh4-tree-regress-models)","4aa2e986":"## 7. Conclusion <a class=\"anchor\" id=\"7\"><\/a>\n\n[Back to Table of Contents](#0.1)","89120c38":"### 5.2 Logistic Regression <a class=\"anchor\" id=\"5.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","289eaa12":"Your comments and feedback are most welcome.","32d825dd":"I hope you find this kernel useful and enjoyable.","b52708ef":"The next code from in my notebook [FE & EDA with Pandas Profiling](https:\/\/www.kaggle.com\/vbmokin\/fe-eda-with-pandas-profiling)","8df0e1ae":"The analysis revealed the presence of one duplicate line. Let's remove it.","f32dedd2":"### 5.8 Neural network - Keras<a class=\"anchor\" id=\"5.8\"><\/a>\n\n[Back to Table of Contents](#0.1)","21a7f4bd":"<a class=\"anchor\" id=\"0\"><\/a>\n# [AI-ML-DS : Training for beginners](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-for-beginners-in-kaggle). Level 4 (very difficult). 2021\n## Kaggle GM, Prof. [@vbmokin](https:\/\/www.kaggle.com\/vbmokin)\n### [Vinnytsia National Technical University](https:\/\/vntu.edu.ua\/), Ukraine\n#### [Chair of the System Analysis and Information Technologies](http:\/\/mmss.vntu.edu.ua\/index.php\/ua\/)","3410bb26":"This model uses a **Decision Tree** as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning).","7a0adff1":"<a class=\"anchor\" id=\"0.1\"><\/a>\n\n## Table of Contents\n\n1. [Import libraries](#1)\n1. [Download datasets](#2)\n1. [EDA & FE](#3)\n    -  [Initial EDA - for FE](#3.1)\n    -  [FE](#3.2)\n    -  [EDA](#3.3)\n1. [Preparing to modeling](#4)\n1. [Models tuning](#5)\n    -  [Linear Regression](#5.1)\n    -  [Logistic Regression](#5.2)\n    -  [Decision Tree Classifier](#5.3)\n    -  [Decision Tree Classifier with GridSearchCV](#5.4)\n    -  [Random Forest Classifier](#5.5)    \n    -  [XGB Classifier](#5.6)\n    -  [LGBM Classifier](#5.7)\n    -  [Neural network - Keras](#5.8)\n    -  [Neural network with Dropout - Keras](#5.9)\n    -  [MLP Classifier](#5.10)\n1. [Models selection](#6)\n1. [Conclusion](#7)\n","48a2e6cc":"## 3. EDA & FE<a class=\"anchor\" id=\"3\"><\/a>\n\n[Back to Table of Contents](#0.1)","d97d3664":"[Go to Top](#0)","1edfa14f":"### 3.2. FE<a class=\"anchor\" id=\"3.2\"><\/a>\n\n[Back to Table of Contents](#0.1)","5cac581c":"### The best models:","ccb409d6":"Thanks to [AI-ML-DS Training. L3AT: NH4 - NN models](https:\/\/www.kaggle.com\/vbmokin\/ai-ml-ds-training-l3at-nh4-nn-models)","2fba7114":"### 5.7 LGBM Classifier <a class=\"anchor\" id=\"5.7\"><\/a>\n\n[Back to Table of Contents](#0.1)","27ff8fac":"### 5.6 XGB Classifier<a class=\"anchor\" id=\"5.6\"><\/a>\n\n[Back to Table of Contents](#0.1)"}}