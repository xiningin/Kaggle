{"cell_type":{"85ae75b3":"code","8788da50":"code","7aa16011":"code","87c9604d":"code","57157589":"code","283fa1df":"code","dc1d6329":"code","352d1a10":"code","a38cb37f":"code","749f7f14":"code","09d6ccef":"code","37f21aa1":"code","6c62ccfb":"code","523194a3":"code","b5e900eb":"code","a16f9794":"code","dd3dbab7":"code","2036fb81":"code","11a87be7":"code","dbe9dd96":"code","6424aed3":"code","a04fed64":"code","42774d84":"markdown","0c89e71b":"markdown","1a896588":"markdown","165c9b83":"markdown","7b95e437":"markdown","2b49b436":"markdown","6097b664":"markdown","9bbcfa42":"markdown","6bdf5b59":"markdown","f6c7471c":"markdown","b9f984d9":"markdown","50e7321a":"markdown","09255455":"markdown","afb40ae7":"markdown","209d9326":"markdown","e7ab6e4c":"markdown","b1388630":"markdown","8d0f945d":"markdown"},"source":{"85ae75b3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8788da50":"data = pd.read_csv('..\/input\/gooogle-stock-price\/Google_Stock_Price_Train.csv',sep=\",\")\n\n# We assign column \"Open\" to variable \"Data\"\ndata = data.loc[:,[\"Open\"]].values\n\n\ntrain = data[:len(data)-50] \ntest = data[len(train):] # last 50 data will be our test data\n\n# reshape\ntrain=train.reshape(train.shape[0],1)","7aa16011":"# feature scalling\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler(feature_range= (0,1)) # defining of Scaler\ntrain_scaled = scaler.fit_transform(train) # applying to Scaler to train\n\nplt.plot(train_scaled)\nplt.show()","87c9604d":"# We add first 50 locution to \"X_train\" and we 51. locution to \"y_train\" .\nX_train = []\ny_train = []\ntimesteps = 50\n\nfor i in range(timesteps, train_scaled.shape[0]):\n    X_train.append(train_scaled[i-timesteps:i,0])\n    y_train.append(train_scaled[i,0])\n\nX_train, y_train = np.array(X_train), np.array(y_train)\n\n\n# Reshaping\nX_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)  # Dimension of array is 3.","57157589":"# --- RNN ---\n\n# Importing the Keras libraries and packages\n\nfrom keras.models import Sequential  \nfrom keras.layers import Dense \nfrom keras.layers import SimpleRNN\nfrom keras.layers import Dropout # it block to overfitting \n\n# Initialising the RNN\nregressor = Sequential()\n\n# Adding the first RNN layer and some Dropout regularisation\nregressor.add(SimpleRNN(units = 50,activation='tanh', return_sequences = True, input_shape = (X_train.shape[1], 1)))\nregressor.add(Dropout(0.2)) \n\n# Adding a second RNN layer and some Dropout regularisation.\nregressor.add(SimpleRNN(units = 50,activation='tanh', return_sequences = True))\nregressor.add(Dropout(0.2))\n\n# Adding a third RNN layer and some Dropout regularisation. \nregressor.add(SimpleRNN(units = 50,activation='tanh', return_sequences = True))\nregressor.add(Dropout(0.2))\n\n# Adding a fourth RNN layer and some Dropout regularisation.\nregressor.add(SimpleRNN(units = 50))\nregressor.add(Dropout(0.2))\n\n\n# Adding the output layer\nregressor.add(Dense(units = 1))\n\n# Compiling the RNN\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n# Fitting the RNN to the Training set\nregressor.fit(X_train, y_train, epochs = 100, batch_size = 32)","283fa1df":"inputs = data[len(data) - len(test) - timesteps:]\ninputs = scaler.transform(inputs)  # min max scaler","dc1d6329":"X_test = []\nfor i in range(timesteps, inputs.shape[0]):\n    X_test.append(inputs[i-timesteps:i, 0]) # 0 dan 50 ye, 1 den 51 e gibi kaydirarark 50 eleman aliyoruz \nX_test = np.array(X_test)\nX_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)","352d1a10":"predicted_data = regressor.predict(X_test)\npredicted_data = scaler.inverse_transform(predicted_data)","a38cb37f":"plt.figure(figsize=(8,4), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(test,color=\"orange\",label=\"Real value\")\nplt.plot(predicted_data,color=\"c\",label=\"RNN predicted result\")\nplt.legend()\nplt.xlabel(\"Days\")\nplt.ylabel(\"Values\")\nplt.grid(True)\nplt.show()","749f7f14":"# ------ LSTM --------------\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error","09d6ccef":"model = Sequential()\nmodel.add(LSTM(10, input_shape=(None,1))) # We want to add 10 LSTM block. One layer has 10 LSTM unit (node).\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(X_train, y_train, epochs=50, batch_size=1)","37f21aa1":"predicted_data2=model.predict(X_test)\npredicted_data2=scaler.inverse_transform(predicted_data2)","6c62ccfb":"plt.figure(figsize=(8,4), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(test,color=\"LimeGreen\",label=\"Real values\")\nplt.plot(predicted_data2,color=\"Gold\",label=\"Predicted LSTM result\")\nplt.legend()\nplt.xlabel(\"Days\")\nplt.ylabel(\"Values\")\nplt.grid(True)\nplt.show()\n","523194a3":"plt.figure(figsize=(8,4), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(test,color=\"green\", linestyle='dashed',label=\"Real values\")\nplt.plot(predicted_data2,color=\"blue\", label=\"LSTM predicted result\")\nplt.plot(predicted_data,color=\"red\",label=\"RNN predicted result\") # ben ekledim\nplt.legend()\nplt.xlabel(\"Days)\")\nplt.ylabel(\"Real values\")\nplt.grid(True)\nplt.show()","b5e900eb":"# RNN Modified\n\nfrom keras.models import Sequential  \nfrom keras.layers import Dense \nfrom keras.layers import SimpleRNN\nfrom keras.layers import Dropout \n\n# Initialising the RNN\nregressor = Sequential()\n\n\nregressor.add(SimpleRNN(units = 100,activation='relu', return_sequences = True, input_shape = (X_train.shape[1], 1)))\nregressor.add(Dropout(0.2))\n\n\nregressor.add(SimpleRNN(units = 50))\nregressor.add(Dropout(0.2))\n\n\n# Adding the output layer\nregressor.add(Dense(units = 1)) \n\n# Compiling the RNN\nregressor.compile(optimizer = 'adam', loss = 'mean_squared_error')\n\n# Fitting the RNN to the Training set\nregressor.fit(X_train, y_train, epochs = 500, batch_size = 16)","a16f9794":"predicted_data_modified = regressor.predict(X_test)\npredicted_data_modified = scaler.inverse_transform(predicted_data_modified)\n","dd3dbab7":"plt.figure(figsize=(8,4), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(test,color=\"gray\",label=\"Real values\")\nplt.plot(predicted_data,color=\"cyan\",label=\"RNN result\")\nplt.plot(predicted_data_modified,color=\"blue\",label=\"RNN Modified Result\")\n\nplt.legend()\nplt.xlabel(\"Days\")\nplt.ylabel(\"Values\")\nplt.grid(True)\nplt.show()","2036fb81":"#  LSTM Modified\n\nimport numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom sklearn.preprocessing import MinMaxScaler \n\n\nmodel = Sequential()\nmodel.add(LSTM(10, input_shape=(None,1)))\nmodel.add(Dense(1))\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(X_train, y_train, epochs=200, batch_size=4) #degistirdim train leri RNN kilerle","11a87be7":"predicted_data2_modified=model.predict(X_test)\npredicted_data2_modified=scaler.inverse_transform(predicted_data2_modified)\n","dbe9dd96":"plt.figure(figsize=(8,4), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(test,color=\"DimGray\",label=\"Real values\", linestyle=\"dashed\")\nplt.plot(predicted_data2,color=\"Magenta\",label=\"LSTM predicted\")\nplt.plot(predicted_data2_modified,color=\"c\", label=\"Modified LSTM predicted\")\nplt.legend()\nplt.xlabel(\"Days\")\nplt.ylabel(\"Values\")\nplt.grid(True)\nplt.show()\n","6424aed3":"# visualization LSTM vs LSTM modified vs RNN vs RNN modified\n\nplt.figure(figsize=(16,8), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(test,color=\"DimGray\",label=\"Real value\", linestyle=\"dashed\")\nplt.plot(predicted_data2,color=\"blue\",label=\"LSTM predicted\")\nplt.plot(predicted_data2_modified,color=\"red\", linestyle=\"dashed\", label=\"LSTM Modified predicted\")\nplt.plot(predicted_data,color=\"c\",label=\"RNN predicted\")\nplt.plot(predicted_data_modified,color=\"green\", linestyle=\"dashed\", label=\"RNN modified predicted\")\nplt.legend()\nplt.xlabel(\"Days\")\nplt.ylabel(\"Values\")\nplt.grid(True)\nplt.show()\n","a04fed64":"# Visualization Modified RNN vs Modified LSTM\n\nplt.figure(figsize=(16,8), dpi=80, facecolor='w', edgecolor='k')\nplt.plot(test,color=\"DimGray\", linestyle= \"dashed\", label=\"true result\")\nplt.plot(predicted_data2_modified,color=\"Magenta\",  label=\"LSTM Modified predicted\")\nplt.plot(predicted_data_modified,color=\"c\",  label=\"RNN modified predicted\")\nplt.legend()\nplt.xlabel(\"Days\")\nplt.ylabel(\"Values\")\nplt.grid(True)\nplt.show()","42774d84":"<a id=\"7\"><\/a>\n### Visualization of LSTM","0c89e71b":"<a id=\"8\"><\/a>\n### Visualization of RNN vs LSTM","1a896588":"It looks Modified RNN more successful than RNN.","165c9b83":"<a id=\"9\"><\/a>\n### Modified RNN","7b95e437":"<a id=\"13\"><\/a>\n### Visualization of LSTM vs Modified LSTM vs RNN vs Modified RNN","2b49b436":"LSTM looks greatly successful from the RNN.\n\nNow we change hyperparemeter like \"units, number of layers, epochs, batch_size, activation\" in RNN","6097b664":"<a id=\"10\"><\/a>\n### Visualization of RNN vs Modified RNN","9bbcfa42":"<a id=\"12\"><\/a>\n### Visualization of LSTM vs Modified LSTM","6bdf5b59":"<a id=\"4\"><\/a>\n### RNN\n","f6c7471c":"### Conclusion\nChanging hyperparameter can increase to accuracy.\n\nLSTM looks more successful than RNN in predict to values about time.","b9f984d9":"<a id=\"3\"><\/a>\n### Import Data","50e7321a":"Modified LSTM looks more successful than LSTM.","09255455":"### **Content**\n* [Import Data](#3)\n* [RNN](#4)\n* [Visualizing of RNN](#5)\n* [LSTM](#6)\n* [Visualizing of LSTM](#7)\n* [Visualizing of RNN vs LSTM](#8)\n* [Modified RNN](#9)\n* [Visualizing of RNN vs Modified RNN](#10)\n* [Modified LSTM](#11)\n* [Visualizing of LSTM vs Modified LSTM](#12)\n* [Visualizing of RNN vs Modified RNN vs LSTM vs Modified LSTM](#13)\n* [Visualizing of Modified RNN vs Modified LSTM](#14)\n* [Conclusion](#15)","afb40ae7":"![RNNvsLSTM.png](attachment:RNNvsLSTM.png)","209d9326":"<a id=\"5\"><\/a>\n### Visualization of RNN\n","e7ab6e4c":"<a id=\"6\"><\/a>\n### LSTM","b1388630":"<a id=\"11\"><\/a>\n### Modified LSTM","8d0f945d":"**Introduction**\n\nThe aim of this study is to compare to accuracies of RNN and LSTM. I compare results of RNN and LSTM to each others and I changed hyperparameters in these methods and I compared them again with their modified versions.\n\nThe main differece of LSTM from RNN is that LSTM can store data longer than RNN."}}