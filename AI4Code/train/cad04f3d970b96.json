{"cell_type":{"6f8321fa":"code","567baa4e":"code","71f69d5e":"code","b9c09026":"code","71564bf9":"code","cfb5390c":"code","e3b57215":"code","f6e9e8a2":"code","71168ccd":"code","4141d6bc":"code","c579f4b9":"code","830a8979":"code","e8fa1eb6":"code","878fc933":"code","543907bd":"code","24ed53ad":"code","f679be2d":"code","a17ea816":"code","082cb4cc":"code","3924305c":"code","56a07e7b":"code","54362dbd":"code","cb283c10":"code","618436be":"code","056c6e36":"code","a3ab163a":"code","68f55d50":"code","38052069":"code","217a2455":"code","ccd2980d":"code","ba7d1afd":"code","7f203b4b":"code","2ef3ec56":"code","f562d15d":"code","75ee9805":"code","b7f1aec3":"code","f59ad829":"code","53293df7":"code","0f603582":"code","bad6ad8a":"code","943e929e":"code","48240d46":"code","ef17afb4":"code","3a8836db":"code","3d2633fc":"code","ee294c33":"code","f72b7b5a":"code","cd6f178a":"code","d58eddf3":"code","610a0f48":"code","5f4daac5":"code","b65255cc":"code","8cf6476a":"code","6a8366ae":"markdown","8df5f086":"markdown","c4f8b4b2":"markdown","2f31d076":"markdown","4231ff6b":"markdown","98112a43":"markdown","b937729d":"markdown","0fac86a4":"markdown","65de9244":"markdown","132353f3":"markdown","c3a02a34":"markdown","df04b505":"markdown","b0ddc275":"markdown","2592a047":"markdown","2acfb975":"markdown","206a21e4":"markdown","afee62bf":"markdown","58409a40":"markdown","62a4e706":"markdown","71406a1f":"markdown","acba56af":"markdown","e89586d3":"markdown","30b83ec5":"markdown","4354a698":"markdown","075d50b5":"markdown","512ced4f":"markdown","9f268d60":"markdown","320f6794":"markdown","07a89bb3":"markdown","33f3a193":"markdown","263c12c9":"markdown","cf21d6e1":"markdown","f1aa2357":"markdown","58a00f64":"markdown","fbb261c3":"markdown","21176eb2":"markdown","8246ed75":"markdown","b6d90a48":"markdown","1dac5f7a":"markdown","00cbbcdb":"markdown","1db48e14":"markdown","306744b3":"markdown","944f7fc9":"markdown","b3ae17e1":"markdown","9dac1a7d":"markdown","cce023ca":"markdown","cc1b61b6":"markdown","a3f26c84":"markdown","dc0e86a7":"markdown","23a3afec":"markdown","6c637f4d":"markdown","00058f15":"markdown","04b33425":"markdown","44ec5b6b":"markdown","acd9cd67":"markdown"},"source":{"6f8321fa":"# for data manipulation\nimport numpy as np \nimport pandas as pd \npd.set_option('display.max_columns', 60)\npd.set_option('display.max_rows', 60)\n\nfrom IPython.display import display, HTML\n\n\n# for visualization\nfrom IPython.core.pylabtools import figsize\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n# to render graphs inline \nimport seaborn as sns\nsns.set_context(font_scale=2)\n\n\n# ML models\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\n# feature scalers and model selectors\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.preprocessing import StandardScaler\n\n\n","567baa4e":"trees = pd.read_csv(\"..\/input\/forest-cover-type-dataset\/covtype.csv\")\nprint(\"The size of the dataset is: \", trees.shape)","71f69d5e":"display(HTML(trees.head(15).to_html()))","b9c09026":"display(HTML(trees.tail(15).to_html()))","71564bf9":"display(pd.DataFrame.info(trees))\nprint( 'Null values? :' , trees.isna().any().any())","cfb5390c":"display(trees.describe())","e3b57215":"def find_outlier_IQR(df, col_name):\n    '''\n    This function takes in a dataset and a column name, and creates a normal range based on the IQR, and gives an outlier count.\n    \n    Parameters: a pandas dataframe, a string of column name in the dataframe\n    \n    Output: lower limit int , upper limit int, outlier count int\n    \n    '''\n    Q1=np.percentile(np.array(df[col_name].tolist()), 25)\n    Q3=np.percentile(np.array(df[col_name].tolist()), 75)\n    IQR=Q3-Q1\n    UL= Q3 + (3*IQR)\n    LL= Q1 - (3*IQR)\n    outlier_count = 0\n                      \n    for value in df[col_name].tolist():\n        if (value < LL) | (value > UL):\n            outlier_count +=1\n\n    return LL, UL, outlier_count\n\n\nfor column in trees[['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points']]:\n    a,b,c =find_outlier_IQR(trees, column)\n    if c > 0:\n        print(\"There are {} outliers in {}. The normal range is {} - {}. \".format(c, column, a, b))","f6e9e8a2":"trees = trees[(trees['Horizontal_Distance_To_Hydrology'] > find_outlier_IQR(trees, 'Horizontal_Distance_To_Hydrology')[0]) & \n                (trees['Horizontal_Distance_To_Hydrology'] < find_outlier_IQR(trees, 'Horizontal_Distance_To_Hydrology')[1])&\n                (trees['Vertical_Distance_To_Hydrology'] > find_outlier_IQR(trees, 'Vertical_Distance_To_Hydrology')[0]) & \n                (trees['Vertical_Distance_To_Hydrology'] < find_outlier_IQR(trees, 'Vertical_Distance_To_Hydrology')[1])]\nprint(trees.shape)","71168ccd":"for column in trees[['Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Horizontal_Distance_To_Fire_Points']]:\n    a,b,c =find_outlier_IQR(trees, column)\n    if c > 0:\n        print(\"There are {} outliers in {}. The normal range is {} - {}. \".format(c, column, a, b))","4141d6bc":"cont_vars = trees.loc[:,'Elevation':'Horizontal_Distance_To_Fire_Points']\ncont_vars_vis = cont_vars.copy()\ncont_vars_vis['Cover_Type'] = trees['Cover_Type'] \nsoiltype = trees.loc[:,'Soil_Type1':'Soil_Type40']\nsoiltype_vis = soiltype.copy()\nsoiltype_vis['Cover_Type'] = trees['Cover_Type'] \nwilderness = trees.loc[:,'Wilderness_Area1':'Wilderness_Area4']\nwilderness_vis = wilderness.copy()\nwilderness_vis['Cover_Type'] = trees['Cover_Type'] \nprint(wilderness.shape)\nprint(soiltype.shape)\nprint(cont_vars.shape)","c579f4b9":"labels= '1- Spruce\/Fir', '2- Lodgepole Pine', '3- Ponderosa Pine', '4- Cottonwood\/Willow', '5- Aspen', '6- Douglas-fir', '7- Krummholz'\n\ntrees_dist = trees.groupby('Cover_Type').size()\nfig1, ax1 = plt.subplots()\nfig1.set_size_inches(15,10)\nax1.pie(trees_dist, labels=labels, autopct='%1.1f%%')\nax1.axis('equal')\nplt.title('Percentages of Tree Cover Types',fontsize=20)\nplt.show()","830a8979":"skew = trees.skew()\ndisplay(skew)","e8fa1eb6":"skew =pd.DataFrame(skew,index=None,columns=['Skewness'])\n\nplt.figure(figsize=(15,7))\nsns.barplot(x=skew.index,y=skew.Skewness)\nplt.xticks(rotation=90)\n","878fc933":"plt.figure(figsize=(14, 28))\nfor i,col in enumerate(cont_vars.columns.values):\n    l,u,ct = find_outlier_IQR(cont_vars_vis,col)\n    plt.subplot(5,2,i+1)\n    sns.kdeplot(cont_vars[col])\n    plt.axvline(x=l, color = 'g')\n    plt.axvline(x=u, color = 'r')\n    \nplt.show()","543907bd":"\nfor i, col in enumerate(cont_vars.columns):\n    plt.figure(i,figsize=(10,4))\n    ax = sns.boxplot(x=cont_vars_vis['Cover_Type'], y=col, data=cont_vars_vis, palette=\"rocket\")\n    ax.set_xticklabels(labels, rotation=30)\n    plt.show()","24ed53ad":"corr = trees.corr()['Cover_Type']\ncorr = pd.DataFrame(data =corr)\ncorr = corr.rename({'Cover_Type': 'Correlation'}, axis = 1)\nprint(corr)","f679be2d":"ax = cont_vars_vis.corr()[\"Cover_Type\"].plot(kind=\"bar\")\nax.axhline(linewidth=0.5,y=0, color = 'k')\n","a17ea816":"ax= soiltype_vis.corr()[\"Cover_Type\"].plot(kind=\"bar\")\nax.axhline(linewidth=0.5,y=0, color = 'k')\n","082cb4cc":"ax = wilderness_vis.corr()[\"Cover_Type\"].plot(kind=\"bar\")\nax.axhline(linewidth=0.5,y=0, color = 'k')\n","3924305c":"plt.figure(figsize=(50,50))\nsns.heatmap(trees.corr(),cmap='BuGn',linecolor='white',linewidths=1,annot=True, xticklabels = True, yticklabels= True)\n","56a07e7b":"plt.figure(figsize=(8,8))\n\nsns.heatmap(cont_vars_vis.corr(),cmap='BuGn',linecolor='white',linewidths=1,annot=True, xticklabels = True, yticklabels= True)","54362dbd":"plt.figure(figsize=(40,40))\ndisc = trees.drop(trees.columns[0:10], axis=1)\nsns.heatmap(disc.corr(),cmap='BuGn',linecolor='white',linewidths=1,annot=True, xticklabels = True, yticklabels= True)","cb283c10":"plt.figure(figsize=(8,8))\n\nx = sns.PairGrid(cont_vars)\nx.map(plt.scatter)\n","618436be":"acc =[]\nalgs = []\ntrees_training = trees.drop('Cover_Type', axis=1)\nlabels_training = trees[\"Cover_Type\"]\ntrain, test, train_labels, test_labels = train_test_split(trees_training, labels_training, test_size=0.3, random_state=1)\ndummy = DummyClassifier(strategy='stratified', random_state=1)\n\n# training the model with the training data (70% of our dataset)\ndummy.fit(train, train_labels)\n\n# get accuracy score, add it to our list for comparison later\nbaseline_accuracy = dummy.score(test, test_labels)\nacc.append(baseline_accuracy)\nalgs.append('Dummy Classifier')\nprint(\"Our dummy algorithm classified {:0.2f}% of the of the trees correctly\".format(baseline_accuracy*100))","056c6e36":"dtree = DecisionTreeClassifier()\ndtree.fit(train, train_labels)\ndectree_pred = dtree.predict(test)\ndtree_accuracy = accuracy_score(dectree_pred , test_labels)\nacc.append(dtree_accuracy)\nfeat_dt = dtree.feature_importances_\n\nalgs.append('Decision Tree')\nprint(\"Our decision tree classified {:0.2f}% of the of the trees correctly\".format(dtree_accuracy*100))","a3ab163a":"plt.figure(figsize=(10,15))\nplt.barh(train.columns.values, feat_dt)\nplt.title('Feature Importance for Decision Tree Algorithm',fontsize=10)\nplt.ylabel('Feature Name')\nplt.xlabel('Relevance')\nplt.show()","68f55d50":"rf = RandomForestClassifier()\nrf.fit(train, train_labels)\nrf_pred = rf.predict(test)\nrf_acc = accuracy_score(rf_pred , test_labels)\nacc.append(rf_acc)\nalgs.append('Random Forest')\nfeat_rf = rf.feature_importances_\nprint(\"Our random forest classified {:0.2f}% of the of the trees correctly\".format(rf_acc*100))","38052069":"plt.figure(figsize=(10,15))\nplt.barh(train.columns.values, feat_rf)\nplt.title('Feature Importance for Random Forest Algorithm',fontsize=10)\nplt.ylabel('Feature Name')\nplt.xlabel('Relevance')\nplt.show()","217a2455":"etree = ExtraTreesClassifier()\netree.fit(train, train_labels)\netree_pred = etree.predict(test)\netree_acc = accuracy_score(etree_pred , test_labels)\nacc.append(etree_acc)\nalgs.append('ExtraTrees')\nfeat_et = etree.feature_importances_\nprint(\"ExtraTrees classified {:0.2f}% of the of the trees correctly\".format(etree_acc*100))","ccd2980d":"plt.figure(figsize=(10,15))\nplt.barh(train.columns.values,feat_et)\nplt.title('Feature Importance for ExtraTrees Algorithm',fontsize=10)\nplt.ylabel('Feature Name')\nplt.xlabel('Relevance')\nplt.show()","ba7d1afd":"\n\ndef reverse_encode(relevant_subset):\n    '''\n    This function reverses one-hot encoding for into rank-encoded representations\n    \n    Parameters: an subset of relevant columns of the same type, which we are trying to reverse encode\n    \n    Output: a title-less list of the encoded variable in numerical data\n    \n    '''\n\n    num_list =[]\n    for i in relevant_subset.iloc:\n        ix = 1\n        for j in i:\n            if j ==1:\n                num_list.append(ix)\n            else: \n                ix +=1\n                \n    print('The new list has the following elements: ' + str(set(num_list)))\n    if relevant_subset.shape[0] == len(num_list):\n        return num_list\n    else: print('Error in compiling list: lengths of the two lists do not match.')\n\n\nw = reverse_encode(wilderness)\nprint(len(w))\n                \n            ","7f203b4b":"s = reverse_encode(soiltype)\nprint(len(s))\n","2ef3ec56":"lab =list(range(1,5))\n\nw = pd.DataFrame(w, columns=['Wilderness'])\nwild_dist = w.groupby('Wilderness').size()\nfig1, ax1 = plt.subplots()\nfig1.set_size_inches(15,10)\nax1.pie(wild_dist, labels= lab, autopct='%1.1f%%')\nax1.axis('equal')\nplt.title('Percentages of Wilderness Areas',fontsize=20)\nplt.show()","f562d15d":"s = pd.DataFrame(s,columns= ['Soil'] )\nsoil_dist = s.groupby('Soil').size()\nprint('Soil type with percentage of total')\nprint((soil_dist\/sum(soil_dist))*100)","75ee9805":"\nprint('The following columns have the lowest correlation with the cover types: \\n' +  str(corr.abs().sort_values(by = 'Correlation').head(10)))","b7f1aec3":"trees_num = trees.loc[:,'Elevation':'Horizontal_Distance_To_Fire_Points']\ntrees_num['Wilderness']  = w.values\ntrees_num['Soil']  = s.values\ntrees_num['Cover'] = trees[\"Cover_Type\"]\ncorr = trees_num.corr()['Cover']\ncorr = pd.DataFrame(data =corr)\ncorr = corr.rename({'Cover': 'Correlation'}, axis = 1)\nprint( 'Null values? :' , trees_num.isna().any().any())\nprint(corr)\n","f59ad829":"plt.figure(figsize=(10,10))\n\nsns.heatmap(trees_num.corr(),cmap='BuGn',linecolor='white',linewidths=1,annot=True, xticklabels = True, yticklabels= True)","53293df7":"feat_df = pd.DataFrame(zip(feat_rf,feat_et, feat_rf), index=[train.columns], columns= ['RF', 'ET', 'DT'])\nprint(feat_df.shape)\nprint('The following features have the least importance in the random forest classifier: \\n' + str(feat_df.sort_values(by = 'RF').head(15)))\nl = [np.mean(feat_rf), np.mean(feat_dt), np.mean(feat_et)]\nprint('Mean:' + str(np.mean(l)))","0f603582":"trees_tr = trees_num.drop('Cover', axis = 1)\nlabels_tr = trees_num['Cover']\n\ntrain, test, train_labels, test_labels = train_test_split(trees_tr, labels_training, test_size=0.3, random_state=1)\nrf = RandomForestClassifier()\nrf.fit(train, train_labels)\nrf_pred = rf.predict(test)\nrf_acc = accuracy_score(rf_pred , test_labels)\nfeat_rf = rf.feature_importances_\nprint(\"Our random forest classified {:0.2f}% of the of the trees correctly\".format(rf_acc*100))","bad6ad8a":"plt.figure(figsize=(10,15))\nplt.barh(train.columns.values,feat_rf)\nplt.title('Feature Importance for Random Forest Algorithm',fontsize=10)\nplt.ylabel('Feature Name')\nplt.xlabel('Relevance')\nplt.show()","943e929e":"df = trees_num.copy()\ndf['Euc_Distance_To_Hydrology'] = (trees_num['Horizontal_Distance_To_Hydrology']**2 + trees_num['Vertical_Distance_To_Hydrology']**2)**0.5\ndf['LC_Horizontal_Roadways_FirePoints'] = 0.5*(trees_num['Horizontal_Distance_To_Roadways'] + trees_num['Horizontal_Distance_To_Fire_Points'])\ndf['LC_Horizontal_Roadways_Hydrology'] = 0.5*(trees_num['Horizontal_Distance_To_Roadways'] + trees_num['Horizontal_Distance_To_Hydrology'])\ndf['LC_Horizontal_Roadways_Vertical_Hydrology'] = 0.5*(trees_num['Horizontal_Distance_To_Roadways'] + trees_num['Vertical_Distance_To_Hydrology'])\n\n# Sqrt columns\ndf['sqrt_Elevation'] =(trees_num['Elevation'])**0.5\ndf['sqrt_Horizontal_Distance_To_Hydrology'] =(trees_num['Horizontal_Distance_To_Hydrology'])**0.5\ndf['sqrt_Horizontal_Distance_To_Roadways'] =(trees_num['Horizontal_Distance_To_Roadways'])**0.5\ndf['sqrt_Horizontal_Distance_To_Fire_Points'] =(trees_num['Horizontal_Distance_To_Fire_Points'])**0.5\ndf['sqrt_Euc_Distance_To_Hydrology'] = (df['Euc_Distance_To_Hydrology'])**0.5\n\n# Normalize Hillshade & drop the Hillshade_3pm\ndf = df.drop(['Hillshade_3pm'], axis = 1)\ndf['norm_Hillshade_Noon'] = df['Hillshade_Noon']\/254\ndf['norm_Hillshade_9am'] = df['Hillshade_9am']\/254\ndf['norm_Aspect'] = df['Aspect']\/360\n\n#print(df[~df.isin([np.nan, np.inf, -np.inf]).any(1)])\n\nprint( 'Null values? :' , df.isna().any().any())\n\n\ncorr = df.corr()['Cover']\ncorr = pd.DataFrame(data =corr)\ncorr = corr.rename({'Cover': 'Correlation'}, axis = 1)\ncorr_vis = corr.abs()\nprint(corr_vis.sort_values(by = 'Correlation'))\n","48240d46":"print('Nan values?: '+str(np.any(np.isnan(df))))\nprint('No inf values?: '+str(np.all(np.isfinite(df))))\ndisplay(pd.DataFrame.describe(df))\ndisplay(pd.DataFrame.info(df))\n","ef17afb4":"\ntrees_tr = df.drop('Cover', axis = 1)\nlabels_tr = df['Cover']\n\ntrain, test, train_labels, test_labels = train_test_split(trees_tr, labels_training, test_size=0.3, random_state=1)\nrf = RandomForestClassifier()\nrf.fit(train, train_labels)\nrf_pred = rf.predict(test)\nrf_acc = accuracy_score(rf_pred , test_labels)\n\nfeat_rf_new = rf.feature_importances_\nprint(\"Our random forest classified {:0.2f}% of the of the trees correctly\".format(rf_acc*100))","3a8836db":"l = pd.DataFrame(zip(train.columns.values,feat_rf_new))\nprint(l.sort_values(by = 1))\nplt.figure(figsize=(10,15))\nplt.barh(train.columns.values,feat_rf_new)\nplt.title('Feature Importance for Random Forest Algorithm',fontsize=10)\nplt.ylabel('Feature Name')\nplt.xlabel('Relevance')\nplt.show()","3d2633fc":"df = df.drop([ 'Hillshade_Noon','Hillshade_9am','Horizontal_Distance_To_Hydrology', 'sqrt_Horizontal_Distance_To_Hydrology','Euc_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways'  ,'Aspect'], axis = 1)\n","ee294c33":"plt.figure(figsize=(10,10))\n\nsns.heatmap(df.corr(),cmap='BuGn',linecolor='white',linewidths=1,annot=True, xticklabels = True, yticklabels= True)","f72b7b5a":"df = df.drop([ 'sqrt_Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points' ,'Elevation', 'LC_Horizontal_Roadways_Vertical_Hydrology' ], axis = 1)\nprint(df.shape)","cd6f178a":"\ndef fit_evaluate_model(model, X_train, y_train, X_valid, Y_valid, method_name):\n    \n    '''\n    This function trains a given model, generates predictions, appends the name of the method and the accuracy to a pre-defined list; then it returns accuracy rate\n\n    Parameters: scikit-learn model, training data, training labels, test data, test labels\n    \n    Output: none; accuracy rate is printed\n    \n    '''\n    model.fit(X_train, y_train)\n    y_predicted = model.predict(X_valid)\n    accuracy = accuracy_score(Y_valid, y_predicted)\n    acc.append(accuracy)\n    algs.append(method_name)\n    print(\"Our {} algorithm classified {:0.2f}% of the of the trees correctly\".format(method_name,accuracy*100))\n","d58eddf3":"\ntrees_tr = df.drop('Cover', axis = 1)\nlabels_tr = df['Cover']\n\ntrain, test, train_labels, test_labels = train_test_split(trees_tr, labels_training, test_size=0.3, random_state=1)\n\nscaler = StandardScaler()\n\n# apply normalization to training set and transform training set\ntrain_scaled = scaler.fit_transform(train, train_labels)\ntrain_scaled = pd.DataFrame(train_scaled)\n# transform validation set\ntest_scaled = scaler.transform(test)","610a0f48":"knn = KNeighborsClassifier()\nfit_evaluate_model(knn, train_scaled, train_labels, test_scaled, test_labels, 'K-NN')","5f4daac5":"lgbm = LGBMClassifier()\nfit_evaluate_model(lgbm, train, train_labels, test, test_labels, 'LightGBM')","b65255cc":"xgb = XGBClassifier()\nfit_evaluate_model(xgb, train_scaled, train_labels, test_scaled, test_labels, 'XGBoost')","8cf6476a":"predictions = pd.DataFrame(zip(algs,acc), columns= ['Model', 'Accuracy'])\npredictions = predictions.sort_values(by = 'Accuracy')\nplt.figure(figsize=(10,10))\nax = sns.barplot(x='Accuracy', y= 'Model', data=predictions, palette='Greens')\nplt.title(\"Prediction Accuracy of Different Models\", size=14)\n","6a8366ae":"## Predicting trees with tree-based algorithms","8df5f086":"### Outlier Detection & Removal","c4f8b4b2":"This whole dataset has 581012 samples and 55 features. Let's look at the first 15 samples and the last 15 samples to see what some of this data looks like. ","2f31d076":"### Decision Tree","4231ff6b":"Let's visualize what some of these relationships between variables look like, at least with the continuous data.","98112a43":"## Comparing Models  & Conclusion","b937729d":"#### Tree-based algorithms and feature selection","0fac86a4":"Here, we can see all of the columns have non-null values and that the datatype for each is int64. The describe function will tell us more about the numerical distribution of each of these columns (or features).","65de9244":"Our left-skewed variables are:\n* Elevation\n* Hillshade_9am\n* Hillshade_Noon\n* Hillshade_3pm\n* Horizontal_Distance_To_Fire_Points\n\nOur right-skewed variables are:\n* Slope\n* Horizontal_Distance_To_Roadways\n\nThe correlation offers some insight into the trends between variables. From the heat map, we can see the following positive and negative relationships:\n\nPositive correlation pairs:\n* Hillshade_9am and Aspect\n* Hillshade_3pm and Aspect\n* Horizontal_Distance_To_Hydrology and Vertical_Distance_To_Hydrology\n* Horizontal_Distance_To_Roadways and  Horizontal_Distance_To_Fire_Points\n* Hillshade_3pm and Hillshade_Noon\n\nNegative correlation pairs:\n* Elevation and Horizontal_Distance_To_Hydrology\n* Elevation and  Horizontal_Distance_To_Roadways\n* Aspect and Hillshade_9am\n* Slope and Hillshade_Noon\n* Hillshade_9am and Hillshade_3pm\n* Wilderness_Area3 and Wilderness_Area1\n\n\n\n\n","132353f3":"This tells us a lot about the correlation with respect to the class. We can also explore the correlation between every pair of features with the heat map of a correlation matrix below.","c3a02a34":"Here, we can see that the first 10 columns are continuous variables, describing some distance or angle, while the next 4 columns are one-hot encoded columns for the wilderness area of the sample. The winderness areas correspond with the following areas in the Roosevelt National Forest:\n* Wilderness area 1 = Rawah Wilderness Area\n* Wilderness area 2 = Neota Wilderness Area\n* Wilderness area 3 = Comanche Peak Wilderness Area\n* Wilderness area 4 = Cache la Poudre Wilderness Area\n\nAfter the wilderness area columns, we have again one-hot encoded columns for 40 soil types, depending on what sort of soil the specific sample contains. The last column here is the cover type. This is a discrete variable with values 1-7 depending on the cover type. The numbers correspond with the following tree cover types:\n* 1- Spruce\/Fir\n* 2- Lodgepole Pine\n* 3- Ponderosa Pine\n* 4- Cottonwood\/Willow\n* 5- Aspen\n* 6- Douglas-fir\n* 7- Krummholz\n\n\nWe should also look at the distribution of the values of each column to get a general sense of whether we need to consider any missing values, what the normal range for each feature looks like, and what they describe. The info and describe commands in from the pandas library can be useful for this.","df04b505":"We can now take a look at the correlation of the variables to the cover type. First, lets see a summary; then, we can visualize these.","b0ddc275":"Now that we have a bit of description about our dataset, let's begin univariate analysis on this dataset. In this section, we will look at the skew and densities of the varaibles. First, we have to split up the dataframe to destinguish the different types of variables, like continuous vs discrete (in this case, one-hot encoded) variables. As we saw, the continuous variables are the first 10 variables, so we will group them into a continuous variables list. The wilderness is one category of the ont-hot encoded columns, so will split that into a seperate list, and same with the soil type. This will make visualization easier as we want to avoid comparing different types of variables to one another. ","2592a047":"# Forest Cover - Data Analysis and Prediction using Deep Learning ","2acfb975":"To run some of our algorithms for classification, we will need to create a small function that can fit the model to the data, find the test accuracy, and append this info so we can visualize and compare models later. We also need to normalize the data with z-scores for some of the models, and especially K-NN, so we will do that pre-processing as well. This way, the model performs its best so we can evaluate the models. ","206a21e4":"I'm going to drop the column 'Hillshade_3pm' since it is strongly corelated with the 'Hillshade_Noon' column as well as the Aspect column, but also has low relevance to the model. In addition, I will be adding some other columns related to the continuous variables:\n* Euclidean distance to hydrology\n* Linear combination of Elevation and Aspect \n* Linear combination of Horizontal_Distance_To_Roadways and Horizontal_Distance_To_Fire_Points\n* Linear combination of Horizontal_Distance_To_Roadways and Horizontal_Distance_To_Hydrology \n* Linear combination of Horizontal_Distance_To_Roadways and Vertical_Distance_To_Hydrology\n\nIn addition, I'm going to normalize the Hillshade columns (in this case, divide them by 254), and perform a log transform on the Elevation and Horizontal Distance columns, as they have positive values. We will also perform square root on the Elevation and Horizontal Distance columns. This will help normalize some of those large values, and we will see the correlation and the random forest classifier feature importance to determine which features we will keep.\n","afee62bf":"## Exploratory Data Analysis (EDA)\nFirst, let's go through the exploratory stage of data analysis for this dataset, also called EDA. Here, we seek to understand the relationships between the features by creating graphs of their correlations, distributions, and skewness. Let's load some libraries we might use in this project. ","58409a40":"So now we know we have to improve upon a 38% accuracy rate! I'm sure the decision trees and random forest algorithms can help with that. ","62a4e706":"## Light Gradient Boosting Machine (LightGBM)","71406a1f":"We can see the ranges and the average value for each function. What we can't see so far is the distribution, so we don't know whether any outliers exist and if they create any problems in our analysis. To see this, we'll define a function to detect outliers in the dataset, using the inter quartile range (IQR) method. In this method, we calculate the first quarentile, Q1 and the third quarentile, Q3, and call the IQR the difference between the two. The IQR will then help us construct a normal range for the dataset using the following equations: $$UL = Q3 + 3\\cdot IQR $$ $$LL = Q1 - 3\\cdot IQR $$\nHere, UL is the upper limit and LL is the lower limit. Values outside of this range will be considered outliers. Typically, 1.5 is used instead of 3 for ourliers, but I personally feel this excludes many samples that might be relevant. 3 is used to determine extreme outliers, so in order to keep our models flexible and accurate, we want to preserve as many rows as possible. This analysis mostly applies to continuous variables, so we will focus on the first 10 columns in selecting which outliers to exclude. We will also not consider the hillshade columns in this, as they have a pre-set range of 0-255, which does not require normalization.","acba56af":"Looking at the above chart as well as the correlation to Cover Type, I'm going to drop the following columns:\n* Hillshade_Noon\n* Hillshade_9am\n* Horizontal_Distance_To_Hydrology \n* sqrt_Horizontal_Distance_To_Hydrology\n* Euc_Distance_To_Hydrology  \n* Horizontal_Distance_To_Roadways  \n* Aspect\n\nThis is because other columns have a high correlation (and mututal information) with these columns, so they're redundant and will most likely create noise for our algorithms.","e89586d3":"First, let's see the distribution of the class, ie the tree cover type. Since we want to see it as a percentage of all cover types, a pie chart can best help us visualize this. ","30b83ec5":"Thus, these columns will be least helpful in predicting the cover type, which is the goal of this project. Let's see if we can find out more about which features we should remove. First, note","4354a698":"Let's take a look at the skew to see if the outlier removal was able to mitigate the skew a bit. Recall skewness for a normal distribution is 0; this means the data is mostly even\/symmentrical. Negative skew indicate that the column is \"skewed left\", which typically means the left tail, associated with relatively smaller values in the range, is longer than the right tail. Similarly, skewed right means the column has more relatively large values, which are in the right tail, and thus the right tail is longer than the left tail.","075d50b5":"The above chart displays the features previous tree-based algorithms determined were the least important. I'm interested to see how important soil type and wilderness area is overall. Let's run a new random forest without the one-hot encoding for soil type and wilderness.","512ced4f":"## K-Nearest Neighbors (KNN)","9f268d60":"So we were able to obtain a 2% increase in accuracy! That's certainly quite an improvement, and although it took a lot longer than the decision tree. This is generally why many prefer random forests for accuracy, and why random forests are so popular in classification problems. Let's see if the feature importance is the same here. ","320f6794":"### Random Forest","07a89bb3":"It seems the Random Forest algorithm was able to predict the tree cover most accurately. In fact, the accuracy is a bit higher for random forest after feature engineering, so random forest would be the algorithm of choice for predicting tree canopies.","33f3a193":"### Univariate Analysis","263c12c9":"This is a good number of features, we can move on into other algorithms for prediction. ","cf21d6e1":"### Conclusions from EDA","f1aa2357":"Since decision trees are one of the more basic tree-based algorithms, I wanted to see how it compared in this setting to random forests and ExtraTrees. Let's take a look at what the implementation of this looks like.","58a00f64":"This algorithm actually performed somewhat on par with the random forest, although the random forest still beat it by a small margin. ","fbb261c3":"We are finally ready to do some analysis on this dataset. Although we haven't done any sort of feature engineering to remove or add features yet, we will use tree-based algorithms to figure out some important features in the dataset. One helpful aspect of tree-based algorithms is that they do not require type c data preprocessing-- that is, normalization, feature selection\/ dimensionality reduction, log transformation, etc. Thus, we can use this algorithm to simultaneously get started on our data analysis and get some insight into important features. \n\n","21176eb2":"For this dataset, we will consider outliers from the following columns:\n* Horizontal_Distance_To_Hydrology\n* Vertical_Distance_To_Hydrology\n* Horizontal_Distance_To_Fire_Points\n\nThis is becausem, as discussed earlier, the other columns are discrete variables (ie, one-hot encoded columns like soil type, wilderness area, etc) or have a pre-set range that does not need normalization, like the hillshade columns. We will first remove outliers from the Horizontal_Distance_To_Hydrology and Vertical_Distance_To_Hydrology columns, as they seem to have a wide range and a good number of outliers. ","8246ed75":"Let's now begin the bivariate analysis phase. In bivariate analysis, we are looking at the relationship between features. First, we can look at the below boxplots to see each continuous variable's frequency with respect to the cover type. ","b6d90a48":"First, we will create a dummy algorithm to get a sort of control group for our later algorithms. This is also called a common-sense baseline, and it serves as a metric to make sure our algorithm is actually improving on an algorithm that follows minimal rules, as the DummyClassifier in scikit-learn does.","1dac5f7a":"It seems the skewness is somewhat close to 0 for the continuous variables; let's visualize this to see what it looks like. This will help us get a better understanding of the skewnesss of variables relative to one another.","00cbbcdb":"## Introduction \nIn this project, we want to use a variety of algorithms and classifiers to predict the type of tree cover in a sample area within Roosevelt National Forest of Northern Colorado given various other features about the area, including the soil type, the vertical and horizontal distance to hydrology, the elevation, etc. We will use the following algorithms to make our predictions:\n* K-Nearest Neighbor\n* ExtraTrees\n* Decision Trees\n* Random Forest\n* Light Gradient Boosting Machine\n* XGBoost\n\n\nWe will do some data exploration, preprocessing, feature engineering, and other processes to ensure the algorithms we choose can make the best possible predictions. ","1db48e14":"## Feature Selection","306744b3":"Let's see if there are any other columns left with very high correlation to other features.","944f7fc9":"It seems our observations were correct, the most skewed features are the discrete or one-hot encoded features. This is probably not an issue since there is also a bias in the class representation (two types of trees are over-represented). We can see if we can explore some relationships between features during the bivariate and multivariate analysis.","b3ae17e1":"### Bivariate Analysis","9dac1a7d":"This seems to have trimmed away some of the more extreme outliers from this dataset. Note that the Vertical_Distance_To_Hydrology column still has many outliers because the IQR is recomputing the ranges based on the new data, so these are not true extreme outliers, and we don't need to worry about them. \n","cce023ca":"We were able to unencode the one-hot encoded variables successfully. Now let's see the plot chart for the wilderness area. Since soil type has more than 7 classes, its not as effective to visualize it with a pie chart. We can use a bar chart instead.","cc1b61b6":"Clearly, the Lodgepole Pine and the Spruce\/Fir are the most common types of trees in this sample. \n","a3f26c84":"## Extra Gradient Boosting (XGBoost) ","dc0e86a7":"I have a feeling some soil types are under-represented. To investigate this, I will create a method that will turn soil type into a numerical value, or reverse-encode it. Then, we can create a pie chart to see which soil types are most common.","23a3afec":"Looks like there are a couple more columns we can remove. We will remove:\n* Elevation\n* Horizontal_Distance_To_Fire_Points\n* sqrt_Horizontal_Distance_To_Roadways\n","6c637f4d":"To make the next couple algorithms as efficient as possible, we can use some of the information we gathered from thetree-based classifiers to see which features are the least important in prediction. The following tables show us the features with the lowest importance values in the random forest classifier, which performed the best in our tree-based analysis. Removing those features can help us reduce noise in the later algorithms' predictions. ","00058f15":"It seems the one-hot encoding did create a bit of noise, because this random forest was able to classify the tree cover type much better. Let's look at the feature relevance.","04b33425":"### Establishing a Control Group","44ec5b6b":"### ExtraTrees ","acd9cd67":"Looks like the decision tree was pretty accurate! Let's take a look at what features this algorithm found important in prediction. "}}