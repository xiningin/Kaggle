{"cell_type":{"364686f4":"code","10661455":"code","cbf130b3":"code","424a95b0":"code","0cbf14be":"code","e115c45c":"code","9ac37a45":"code","de144981":"code","fcde0c38":"code","5a37fe26":"code","6172b9fd":"code","3dad984f":"code","dce9f1c2":"code","4a36866f":"code","882bc52a":"code","9dda979f":"code","c25f0be6":"code","82fbd0ea":"code","39146121":"code","b4703c6b":"code","0a958b97":"code","a3f00511":"code","4a4b71fa":"code","bc5abf8e":"code","4f9a30ff":"code","297e80b5":"code","a64eda8c":"code","a7954a8f":"code","28c0db69":"code","a8404bf9":"code","4e1c2bc5":"code","d6dbd6b9":"code","a3935dc0":"code","1d3e7cf0":"code","31c081ba":"code","b4154eae":"markdown","160de5e9":"markdown","9ec8830d":"markdown","f956ece7":"markdown","f7cb7414":"markdown","e5803be1":"markdown","95f14873":"markdown","fb15f98e":"markdown","2b59df05":"markdown","dacbe745":"markdown","18a76abf":"markdown","8a0e5013":"markdown","aa583136":"markdown"},"source":{"364686f4":"import warnings\n\nwarnings.filterwarnings(\"ignore\")","10661455":"import pandas as pd\n\ndf_train = pd.read_csv(\"..\/input\/nlp-dataset\/labeledTrainData.tsv\", header = 0,\n                      delimiter = \"\\t\", quoting = 3)\ndf_test = pd.read_csv(\"..\/input\/nlp-dataset\/testData.tsv\", header = 0,\n                      delimiter = \"\\t\", quoting = 3)\ndf_unlabeled = pd.read_csv(\"..\/input\/nlp-dataset\/unlabeledTrainData.tsv\", header = 0,\n                      delimiter = \"\\t\", quoting = 3)\n\nprint(df_train.shape)\nprint(df_test.shape)\nprint(df_unlabeled.shape)\n\nprint(df_train[\"review\"].size)\nprint(df_test[\"review\"].size)\nprint(df_unlabeled[\"review\"].size)","cbf130b3":"df_train.head()","424a95b0":"df_test.head()\n\n# \uc608\uce21\ud574\uc57c\ud558\ub294 sentiment \ud53c\uccd0\uac00 \uc5c6\ub2e4.","0cbf14be":"import re\nimport nltk\n\nimport pandas as pd\nimport numpy as np\n\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\n\nfrom multiprocessing import Pool\n\nclass KaggleWord2VecUtility(object):\n\n    @staticmethod\n    def review_to_wordlist(review, remove_stopwords=False):\n        # 1. HTML \uc81c\uac70\n        review_text = BeautifulSoup(review, \"html.parser\").get_text()\n        # 2. \ud2b9\uc218\ubb38\uc790\ub97c \uacf5\ubc31\uc73c\ub85c \ubc14\uafd4\uc90c\n        review_text = re.sub('[^a-zA-Z]', ' ', review_text)\n        # 3. \uc18c\ubb38\uc790\ub85c \ubcc0\ud658 \ud6c4 \ub098\ub208\ub2e4.\n        words = review_text.lower().split()\n        # 4. \ubd88\uc6a9\uc5b4 \uc81c\uac70\n        if remove_stopwords:\n            stops = set(stopwords.words('english'))\n            words = [w for w in words if not w in stops]\n        # 5. \uc5b4\uac04\ucd94\ucd9c\n        stemmer = SnowballStemmer('english')\n        words = [stemmer.stem(w) for w in words]\n        # 6. \ub9ac\uc2a4\ud2b8 \ud615\ud0dc\ub85c \ubc18\ud658\n        return(words)\n\n    @staticmethod\n    def review_to_join_words( review, remove_stopwords=False ):\n        words = KaggleWord2VecUtility.review_to_wordlist(\\\n            review, remove_stopwords=False)\n        join_words = ' '.join(words)\n        return join_words\n\n    @staticmethod\n    def review_to_sentences( review, remove_stopwords=False ):\n        # punkt tokenizer\ub97c \ub85c\ub4dc\ud55c\ub2e4.\n        \"\"\"\n        \uc774 \ub54c, pickle\uc744 \uc0ac\uc6a9\ud558\ub294\ub370\n        pickle\uc744 \ud1b5\ud574 \uac12\uc744 \uc800\uc7a5\ud558\uba74 \uc6d0\ub798 \ubcc0\uc218\uc5d0 \uc5f0\uacb0 \ub41c \ucc38\uc870\uac12 \uc5ed\uc2dc \uc800\uc7a5\ub41c\ub2e4.\n        \uc800\uc7a5\ub41c pickle\uc744 \ub2e4\uc2dc \uc77d\uc73c\uba74 \ubcc0\uc218\uc5d0 \uc5f0\uacb0\ub418\uc5c8\ub358\n        \ubaa8\ub4e0 \ub808\ud37c\ub7f0\uc2a4\uac00 \uacc4\uc18d \ucc38\uc870 \uc0c1\ud0dc\ub97c \uc720\uc9c0\ud55c\ub2e4.\n        \"\"\"\n        tokenizer = nltk.data.load('tokenizers\/punkt\/english.pickle')\n        # 1. nltk tokenizer\ub97c \uc0ac\uc6a9\ud574\uc11c \ub2e8\uc5b4\ub85c \ud1a0\ud070\ud654 \ud558\uace0 \uacf5\ubc31 \ub4f1\uc744 \uc81c\uac70\ud55c\ub2e4.\n        raw_sentences = tokenizer.tokenize(review.strip())\n        # 2. \uac01 \ubb38\uc7a5\uc744 \uc21c\ud68c\ud55c\ub2e4.\n        sentences = []\n        for raw_sentence in raw_sentences:\n            # \ube44\uc5b4\uc788\ub2e4\uba74 skip\n            if len(raw_sentence) > 0:\n                # \ud0dc\uadf8\uc81c\uac70, \uc54c\ud30c\ubcb3\ubb38\uc790\uac00 \uc544\ub2cc \uac83\uc740 \uacf5\ubc31\uc73c\ub85c \uce58\ud658, \ubd88\uc6a9\uc5b4\uc81c\uac70\n                sentences.append(\\\n                    KaggleWord2VecUtility.review_to_wordlist(\\\n                    raw_sentence, remove_stopwords))\n        return sentences\n\n\n    # \ucc38\uace0 : https:\/\/gist.github.com\/yong27\/7869662\n    # http:\/\/www.racketracer.com\/2016\/07\/06\/pandas-in-parallel\/\n    # \uc18d\ub3c4 \uac1c\uc120\uc744 \uc704\ud574 \uba40\ud2f0 \uc2a4\ub808\ub4dc\ub85c \uc791\uc5c5\ud558\ub3c4\ub85d\n    @staticmethod\n    def _apply_df(args):\n        df, func, kwargs = args\n        return df.apply(func, **kwargs)\n\n    @staticmethod\n    def apply_by_multiprocessing(df, func, **kwargs):\n        # \ud0a4\uc6cc\ub4dc \ud56d\ubaa9 \uc911 workers \ud30c\ub77c\uba54\ud130\ub97c \uaebc\ub0c4\n        workers = kwargs.pop('workers')\n        # \uc704\uc5d0\uc11c \uac00\uc838\uc628 workers \uc218\ub85c \ud504\ub85c\uc138\uc2a4 \ud480\uc744 \uc815\uc758\n        pool = Pool(processes=workers)\n        # \uc2e4\ud589\ud560 \ud568\uc218\uc640 \ub370\uc774\ud130\ud504\ub808\uc784\uc744 \uc6cc\ucee4\uc758 \uc218 \ub9cc\ud07c \ub098\ub220 \uc791\uc5c5\n        result = pool.map(KaggleWord2VecUtility._apply_df, [(d, func, kwargs)\n                for d in np.array_split(df, workers)])\n        pool.close()\n        # \uc791\uc5c5 \uacb0\uacfc\ub97c \ud569\uccd0\uc11c \ubc18\ud658\n        return pd.concat(result)\n    \n    \n# KaggleWord2VecUtility\ub97c class\ub85c \uc0dd\uc131\ud558\uc5ec \uc0ac\uc6a9 \n# \ucf54\ub4dc \ucd9c\ucc98: https:\/\/github.com\/corazzon\/KaggleStruggle\/blob\/master\/word2vec-nlp-tutorial\/KaggleWord2VecUtility.py","e115c45c":"KaggleWord2VecUtility.review_to_wordlist(df_train[\"review\"][0])[:10]","9ac37a45":"sentences = []\nfor review in df_train[\"review\"]:\n    sentences += KaggleWord2VecUtility.review_to_sentences(\n    review, remove_stopwords = False)\n    \n# KaggleWord2VecUtility\uc744 \uc0ac\uc6a9\ud558\uc5ec train \ub370\uc774\ud130\ub97c \uc815\uc81c\ud574\uc900\ub2e4.","de144981":"for review in df_unlabeled[\"review\"]:\n    sentences += KaggleWord2VecUtility.review_to_sentences(\n    review, remove_stopwords = False)\n    \n# KaggleWord2VecUtility\uc744 \uc0ac\uc6a9\ud558\uc5ec unlabeled train \ub370\uc774\ud130\ub97c \uc815\uc81c\ud574\uc900\ub2e4.    ","fcde0c38":"len(sentences)","5a37fe26":"sentences[0][:10]","6172b9fd":"import logging\nlogging.basicConfig(\n    format = \"%(asctime)s : %(levelname)s : %(message)s\",\n    level = logging.INFO)","3dad984f":"# \ud30c\ub77c\ubbf8\ud130 \uac12\uc744 \uc9c0\uc815\ud574\uc900\ub2e4. \n\nnum_features = 300 # \ubb38\uc790 \ubca1\ud130 \ucc28\uc6d0 \uc218 (size)\nmin_word_count = 40 # \ucd5c\uc18c \ubb38\uc790 \uc218 (min_count)\nnum_workers = 4 # \ubcd1\ub82c \ucc98\ub9ac \uc2a4\ub808\ub4dc \uc218 (workers)\ncontext = 10 # \ubb38\uc790\uc5f4 \ucc3d \ud06c\uae30 (window)\ndownsampling = 1e-3 # \ubb38\uc790 \ube48\ub3c4 \uc218 Downsample (sample)\n\n# \ucd08\uae30\ud654 \ubc0f \ubaa8\ub378 \ud559\uc2b5\nfrom gensim.models import word2vec\n\nmodel = word2vec.Word2Vec(sentences,\n                         workers = num_workers,\n                         size = num_features,\n                         min_count = min_word_count,\n                         window = context,\n                         sample = downsampling)\n\nmodel","dce9f1c2":"# \ud559\uc2b5\uc774 \uc644\ub8cc\ub418\uba74 \ud544\uc694\uc5c6\ub294 \uba54\ubaa8\ub9ac\ub97c unload \uc2dc\ud0a8\ub2e4.\nmodel.init_sims(replace = True)\n\nmodel_name = \"300features_40minwindows_10text\"\nmodel.save(model_name)","4a36866f":"# \uc720\uc0ac\ub3c4\uac00 \uc5c6\ub294 \ub2e8\uc5b4 \ucd94\ucd9c\nmodel.wv.doesnt_match(\"man woman child kitchen\".split())","882bc52a":"model.wv.doesnt_match(\"france england germany berlin\".split())","9dda979f":"# \uac00\uc7a5 \uc720\uc0ac\ud55c \ub2e8\uc5b4\ub97c \ucd94\ucd9c\nmodel.wv.most_similar(\"man\")","c25f0be6":"model.wv.most_similar(\"queen\")","82fbd0ea":"model.wv.most_similar(\"film\")","39146121":"model.wv.most_similar(\"happi\")","b4703c6b":"from sklearn.manifold import TSNE\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport gensim\nimport gensim.models as g\n\n# \uadf8\ub798\ud504\uc5d0\uc11c \ub9c8\uc774\ub108\uc2a4 \ud3f0\ud2b8 \uae68\uc9c0\ub294 \ubb38\uc81c\uc5d0 \ub300\ud55c \ub300\ucc98\nmpl.rcParams[\"axes.unicode_minus\"] = False\n\nmodel_name = \"300features_40minwindows_10text\"\nmodel = g.Doc2Vec.load(model_name)\n\nvocab = list(model.wv.vocab)\nX = model[vocab]\n\nprint(len(X))\nprint(X[0][:10])\ntsne = TSNE(n_components = 2)\n\n# 100\uac1c\uc758 \ub2e8\uc5b4\uc5d0 \ub300\ud574\uc11c\ub9cc \uc2dc\uac01\ud654\nX_tsne = tsne.fit_transform(X[:100,:])","0a958b97":"df = pd.DataFrame(X_tsne, index = vocab[:100], columns = [\"x\", \"y\"])\ndf.shape","a3f00511":"df.head()","4a4b71fa":"fig = plt.figure()\nfig.set_size_inches(40, 20)\nax = fig.add_subplot(1, 1, 1)\n\nax.scatter(df[\"x\"], df[\"y\"])\n\nfor word, pos in df.iterrows():\n    ax.annotate(word, pos, fontsize = 30)\n\nplt.show()","bc5abf8e":"import numpy as np\n\n\n# def\ub97c \uc774\uc6a9\ud574\uc11c \uc8fc\uc5b4\uc9c4 \ubb38\uc7a5\uc5d0\uc11c \ub2e8\uc5b4 \ubca1\ud130\uc758 \ud3c9\uade0\uc744 \uad6c\ud558\ub294 \ud568\uc218\ub97c \ub9cc\ub4e0\ub2e4.\ndef makeFeatureVec(words, model, num_features):\n    featureVec = np.zeros((num_features,),dtype = \"float32\")\n    \n    # \uc18d\ub3c4\ub97c \uc704\ud574 0\uc73c\ub85c \ucc44\uc6b4 \ubc30\uc5f4\ub85c \ucd08\uae30\ud654 \ud55c\ub2e4.\n    nwords = 0.\n    # Index2word\ub294 \ubaa8\ub378\uc758 \uc0ac\uc804\uc5d0 \uc788\ub294 \ub2e8\uc5b4\uba85\uc744 \ub2f4\uc740 \ub9ac\uc2a4\ud2b8\uc774\ub2e4.\n    # \uc18d\ub3c4\ub97c \uc704\ud574 set \ud615\ud0dc\ub85c \ucd08\uae30\ud654 \ud55c\ub2e4.\n    index2word_set = set(model.wv.index2word)\n    # \ub8e8\ud504\ub97c \ub3cc\uba70 \ubaa8\ub378 \uc0ac\uc804\uc5d0 \ud3ec\ud568\uc774 \ub418\ub294 \ub2e8\uc5b4\ub77c\uba74 \ud53c\ucc98\uc5d0 \ucd94\uac00\ud55c\ub2e4.\n    for word in words:\n        if word in index2word_set:\n            nwords = nwords + 1.\n            featureVec = np.add(featureVec, model[word])\n    # \uacb0\uacfc\ub97c \ub2e8\uc5b4\uc218\ub85c \ub098\ub204\uc5b4 \ud3c9\uade0\uc744 \uad6c\ud55c\ub2e4.        \n    featureVec = np.divide(featureVec, nwords)\n    return featureVec","4f9a30ff":"def getAvgFeatureVecs(reviews, model, num_features):\n    # \ub9ac\ubdf0 \ub2e8\uc5b4 \ubaa9\ub85d\uc758 \uac01\uac01\uc5d0 \ub300\ud55c \ud3c9\uade0 feature \ubca1\ud130\ub97c \uacc4\uc0b0\ud558\uace0\n    # 2d Numpy Array\ub85c \ubc18\ud658\ud55c\ub2e4.\n    \n    # \uce74\uc6b4\ud130\ub97c \ucd08\uae30\ud654 \ud55c\ub2e4.\n    counter = 0.\n    # \uc18d\ub3c4\ub97c \uc704\ud574 2D \ub118\ud30c\uc774 \ubc30\uc5f4\uc744 \ubbf8\ub9ac \ud560\ub2f9\ud55c\ub2e4.\n    reviewFeatureVecs = np.zeros(\n        (len(reviews), num_features), dtype = \"float32\")\n    \n    for review in reviews:\n        # \ub9e4 1000\uac1c \ub9ac\ubdf0\ub9c8\ub2e4 \uc0c1\ud0dc\ub97c \ucd9c\ub825\n        if counter%1000. == 0.:\n            print(\"Review %d of %d\"%(counter, len(reviews)))\n        # \ud3c9\uade0 \ud53c\ucc98 \ubca1\ud130\ub97c \ub9cc\ub4e4\uae30 \uc704\ud574 \uc704\uc5d0\uc11c \uc815\uc758\ud55c \ud568\uc218\ub97c \ud638\ucd9c\ud55c\ub2e4.    \n        reviewFeatureVecs[int(counter)] = makeFeatureVec(review,\n                                                         model,\n                                                        num_features)\n        # \uce74\uc6b4\ud130\ub97c \uc99d\uac00\uc2dc\ud0a8\ub2e4.\n        counter = counter + 1.\n    return reviewFeatureVecs","297e80b5":"# \uba40\ud2f0\uc2a4\ub808\ub4dc\ub85c 4\uac1c\uc758 \uc6cc\ucee4\ub97c \uc0ac\uc6a9\ud574 \ucc98\ub9ac\ud55c\ub2e4.\n\ndef getCleanReviews(reviews):\n    clean_reviews = []\n    clean_reviews = KaggleWord2VecUtility.apply_by_multiprocessing(\\\n        reviews[\"review\"], KaggleWord2VecUtility.review_to_wordlist,\\\n        workers = 4)\n    return clean_reviews","a64eda8c":"%time trainDataVecs = getAvgFeatureVecs(\\\n    getCleanReviews(df_train), model, num_features)","a7954a8f":"%time testDataVecs = getAvgFeatureVecs(\\\n    getCleanReviews(df_test), model, num_features)","28c0db69":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(\n    n_estimators = 100, n_jobs = -1, random_state = 42)","a8404bf9":"%time rf.fit(trainDataVecs, df_train[\"sentiment\"])","4e1c2bc5":"from sklearn.model_selection import cross_val_score\n%time score= np.mean(cross_val_score(\\\n    rf, trainDataVecs, df_train[\"sentiment\"], cv = 10, scoring = \"roc_auc\"))","d6dbd6b9":"score","a3935dc0":"result = rf.predict(testDataVecs)","1d3e7cf0":"output = pd.DataFrame(data = {\"id\": df_test[\"id\"], \"sentiment\": result})\noutput.to_csv(\".\/Word2Vec_Tutorial_{:.5f}.csv\".format(score),\n             index = False, quoting = 3)","31c081ba":"output_sentiment = output[\"sentiment\"].value_counts()\nprint(output_sentiment[0] - output_sentiment[1])\noutput_sentiment","b4154eae":"# Gensim\n\n* [gensim: models.word2vec - Deep learning with word2vec](https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html)","160de5e9":"# IMDb Review Tutorial \n#### ***[\ucf54\ub4dc\ucd9c\ucc98: \uc624\ub298\ucf54\ub4dc(\ubc15\uc870\uc740 \ub2d8)](https:\/\/github.com\/corazzon)***","9ec8830d":"## \ud3c9\uade0 feature \ubca1\ud130 \uacc4\uc0b0, 2D numpy \ubc30\uc5f4\ub85c \ubc18\ud658","f956ece7":"# \ubaa8\ub378 \uacb0\uacfc \ud0d0\uc0c9","f7cb7414":"![](https:\/\/static.amazon.jobs\/teams\/53\/images\/IMDb_Header_Page.jpg?1501027252)","e5803be1":"# Word2Vec\uc73c\ub85c \ubca1\ud130\ud654 \ud55c \ub2e8\uc5b4\ub97c t-SNE\uc744 \ud1b5\ud574 \uc2dc\uac01\ud654","95f14873":"## Word2Vec \ubaa8\ub378\uc758 \ud30c\ub77c\uba54\ud130\n\n* \uc544\ud0a4\ud14d\ucc98: \uc544\ud0a4\ud14d\ucc98 \uc635\uc158\uc740 skip-gram(default) \ub610\ub294 CBOW \ubaa8\ub378\uc774\ub2e4. skip-gram\uc740 \ub290\ub9ac\uc9c0\ub9cc \ub354 \ub098\uc740 \uacb0\uacfc\ub97c \ub0b8\ub2e4.\n* \ud559\uc2b5 \uc54c\uace0\ub9ac\uc998: Hierarchical softmax(default) \ub610\ub294 negative \uc0d8\ud50c\ub9c1. \uc5ec\uae30\uc5d0\uc11c\ub294 \uae30\ubcf8\uac12\uc774 \uc798 \ub3d9\uc791\ud55c\ub2e4.\n* \ube48\ubc88\ud558\uac8c \ub4f1\uc7a5\ud558\ub294 \ub2e8\uc5b4\uc5d0 \ub300\ud55c \ub2e4\uc6b4 \uc0d8\ud50c\ub9c1: Google \ubb38\uc11c\ub294 0.00001\uc5d0\uc11c 0.001 \uc0ac\uc774\uc758 \uac12\uc744 \uad8c\uc7a5\ud55c\ub2e4. \uc5ec\uae30\uc5d0\uc11c\ub294 0.001\uc5d0 \uac00\uae4c\uc6b4 \uac12\uc774 \ucd5c\uc885 \ubaa8\ub378\uc758 \uc815\ud655\ub3c4\ub97c \ub192\uc774\ub294 \uac83\uc73c\ub85c \ubcf4\uc5ec\uc9c4\ub2e4.\n* \ub2e8\uc5b4 \ubca1\ud130 \ucc28\uc6d0: \ub9ce\uc740 feature\ub97c \uc0ac\uc6a9\ud55c\ub2e4\uace0 \ud56d\uc0c1 \uc88b\uc740 \uac83\uc740 \uc544\ub2c8\uc9c0\ub9cc \ub300\uccb4\uc801\uc73c\ub85c \uc880 \ub354 \ub098\uc740 \ubaa8\ub378\uc774 \ub41c\ub2e4. \ud569\ub9ac\uc801\uc778 \uac12\uc740 \uc218\uc2ed\uc5d0\uc11c \uc218\ubc31 \uac1c\uac00 \ub420 \uc218 \uc788\uace0 \uc5ec\uae30\uc5d0\uc11c\ub294 300\uc73c\ub85c \uc9c0\uc815\ud588\ub2e4.\n* \ucee8\ud14d\uc2a4\ud2b8 \/ \ucc3d\ud06c\uae30: \ud559\uc2b5 \uc54c\uace0\ub9ac\uc998\uc774 \uace0\ub824\ud574\uc57c \ud558\ub294 \ucee8\ud14d\uc2a4\ud2b8\uc758 \ub2e8\uc5b4\uc218\ub294 \uc5bc\ub9c8\ub098 \ub420\uae4c? Hierarchical softmax\ub97c \uc704\ud574 \uc880 \ub354 \ud070 \uc218\uac00 \uc88b\uc9c0\ub9cc 10\uc815\ub3c4\uac00 \uc801\ub2f9\ud558\ub2e4.\n* Worker threads: \uc2e4\ud589\ud560 \ubcd1\ub82c \ud504\ub85c\uc138\uc2a4\uc758 \uc218\ub85c \ucef4\ud4e8\ud130\ub9c8\ub2e4 \ub2e4\ub974\uc9c0\ub9cc \ub300\ubd80\ubd84\uc758 \uc2dc\uc2a4\ud15c\uc5d0\uc11c 4~6\uc758 \uac12\uc744 \uc0ac\uc6a9\ud55c\ub2e4.\n* \ucd5c\uc18c \ub2e8\uc5b4 \uc218: \uc5b4\ud718\uc758 \ud06c\uae30\ub97c \uc758\ubbf8\uc788\ub294 \ub2e8\uc5b4\ub85c \uc81c\ud55c\ud558\ub294 \ub370 \ub3c4\uc6c0\uc774 \ub41c\ub2e4. \ubaa8\ub4e0 \ubb38\uc11c\uc5d0\uc11c \uc5ec\ub7ec \ubc88 \ubc1c\uc0dd\ud558\uc9c0 \uc54a\ub294 \ub2e8\uc5b4\ub294 \ubb34\uc2dc\ub41c\ub2e4. 10\uc5d0\uc11c 100\uc0ac\uc774\uac00 \uc801\ub2f9\ud558\uba70, \uc774 \ub300\ud68c\uc758 \ub370\uc774\ud130\ub294 \uac01 \uc601\ud654\uac00 30\uac1c\uc529\uc758 \ub9ac\ubdf0\uac00 \uc788\uae30 \ub54c\ubb38\uc5d0 \uac1c\ubcc4 \uc601\ud654 \uc81c\ubaa9\uc5d0 \ub108\ubb34 \ub9ce\uc740 \uc911\uc694\uc131\uc774 \ubd99\ub294 \uac83\uc744 \ud53c\ud558\uae30 \uc704\ud574 \ucd5c\uc18c \ub2e8\uc5b4 \uc218\ub97c 40\uc73c\ub85c \uc124\uc815\ud55c\ub2e4. \uadf8 \uacb0\uacfc \uc804\uccb4 \uc5b4\ud718 \ud06c\uae30\ub294 \uc57d 15,000 \ub2e8\uc5b4\uac00 \ub41c\ub2e4. \ub192\uc740 \uac12\uc740 \uc81c\ud55c \ub41c \uc2e4\ud589\uc2dc\uac04\uc5d0 \ub3c4\uc6c0\uc774 \ub41c\ub2e4.\n","fb15f98e":"# Word2Vec \ubaa8\ub378\uc744 \ud559\uc2b5\n\n### \uc804\ucc98\ub9ac\ub97c \uac70\uccd0 \ud30c\uc2f1\ub41c \ubb38\uc7a5\uc758 \ubaa9\ub85d\uc73c\ub85c \ubaa8\ub378\uc744 \ud559\uc2b5\uc2dc\ud0ac \uc900\ube44\uac00 \ub418\uc5c8\ub2e4.","2b59df05":"# \ud29c\ud1a0\ub9ac\uc5bc PART 2 Word2Vec\n### - \ub525\ub7ec\ub2dd \uae30\ubc95\uc778 Word2Vec\uc744 \ud1b5\ud574 \ub2e8\uc5b4\ub97c \ubca1\ud130\ud654 \ud574\ubcf8\ub2e4.\n### - t-SNE\ub97c \ud1b5\ud574 \ubca1\ud130\ud654 \ud55c \ub370\uc774\ud130\ub97c \uc2dc\uac01\ud654 \ud574\ubcf8\ub2e4.\n### - \ub525\ub7ec\ub2dd\uacfc \uc9c0\ub3c4\ud559\uc2b5\uc758 \ub79c\ub364\ud3ec\ub808\uc2a4\ud2b8\ub97c \uc0ac\uc6a9\ud558\ub294 \ud558\uc774\ube0c\ub9ac\ub4dc \ubc29\uc2dd\uc744 \uc0ac\uc6a9\ud55c\ub2e4.","dacbe745":"![](https:\/\/1.bp.blogspot.com\/-Q7F8ulD6fC0\/UgvnVCSGmXI\/AAAAAAAAAbg\/MCWLTYBufhs\/s1600\/image00.gif)\n\n### \ucd9c\ucc98 : https:\/\/opensource.googleblog.com\/2013\/08\/learning-meaning-behind-words.html","18a76abf":"### Word2Vec\uc740 \ud06c\uac8c \ub450\uac00\uc9c0\ub85c \ub098\ub25c\ub2e4.\n\n#### CBOW\uc640 Skip-Gram \uae30\ubc95\uc774 \uc788\ub2e4.\n1. CBOW(continuous bag of words)\ub294 \ud14d\uc2a4\ud2b8\ub85c \ud558\ub098\uc758 \ub2e8\uc5b4\ub97c \uc608\uce21\ud558\uae30 \ub54c\ubb38\uc5d0 \uc791\uc740 \ub370\uc774\ud130\uc14b\uc77c\uc218\ub85d \uc720\ub9ac\ud558\ub2e4.\n - \uc544\ub798 \uc608\uc81c\uc5d0\uc11c ___ \uc5d0 \ub4e4\uc5b4\uac08 \ub2e8\uc5b4\ub97c \uc608\uce21\ud55c\ub2e4.\n    \n    1) ___ \uac00 \ub9db\uc788\ub2e4.\n    \n    2) ___ \ub97c \ud0c0\ub294 \uac83\uc774 \uc7ac\ubbf8\uc788\ub2e4.\n    \n    3) \ud3c9\uc18c\ubcf4\ub2e4 \ub450 ___ \ub85c \uba39\uc5b4\uc11c ___ \uac00 \uc544\ud504\ub2e4.\n\n2. Skip_Gram\uc740 \ud0c0\uac9f \ub2e8\uc5b4\ub4e4\ub85c\ubd80\ud130 \uc6d0\ubcf8 \ub2e8\uc5b4\ub97c \uc5ed\uc73c\ub85c \uc608\uce21\ud558\ub294 \uac83\uc774\ub2e4. CBOW\uc640\ub294 \ubc18\ub300\ub85c \ucee8\ud14d\uc2a4\ud2b8-\ud0c0\uac9f \uc30d\uc744 \uc0c8\ub85c\uc6b4 \ubc1c\uacac\uc73c\ub85c \ucc98\ub9ac\ud558\uace0 \ud070 \uaddc\ubaa8\uc758 \ub370\uc774\ud130\uc14b\uc744 \uac00\uc9c8 \ub54c \uc720\ub9ac\ud558\ub2e4.\n - '\ubc30'\ub77c\ub294 \ub2e8\uc5b4 \uc8fc\ubcc0\uc5d0 \uc62c \uc218 \uc788\ub294 \ub2e8\uc5b4\ub97c \uc608\uce21\ud55c\ub2e4.\n    \n    1) *\ubc30*\uac00 \ub9db\uc788\ub2e4.\n    \n    2) *\ubc30*\ub97c \ud0c0\ub294 \uac83\uc774 \uc7ac\ubbf8\uc788\ub2e4.\n    \n    3) \ud3c9\uc18c\ubcf4\ub2e4 \ub450 *\ubc30*\ub85c \uba39\uc5b4\uc11c *\ubc30*\uac00 \uc544\ud504\ub2e4.","8a0e5013":"## Word2Vec \ucc38\uace0\uc790\ub8cc\n- [word2vec \ubaa8\ub378 \u00b7 \ud150\uc11c\ud50c\ub85c\uc6b0 \ubb38\uc11c \ud55c\uae00 \ubc88\uc5ed\ubcf8](https:\/\/tensorflowkorea.gitbooks.io\/tensorflow-kr\/g3doc\/tutorials\/word2vec\/)\n- [Word2Vec\uc73c\ub85c \ubb38\uc7a5 \ubd84\ub958\ud558\uae30 \u00b7 ratsgo's blog](https:\/\/ratsgo.github.io\/natural%20language%20processing\/2017\/03\/08\/word2vec\/)\n- [Efficient Estimation of Word Representations in\\n\",\"Vector Space](https:\/\/arxiv.org\/pdf\/1301.3781v3.pdf)\n- [Distributed Representations of Words and Phrases and their Compositionality](http:\/\/papers.nips.cc\/paper\/5021-distributed-representations- -of-words-and-phrases-and-their-compositionality.pdf)\n- [CS224n: Natural Language Processing with Deep Learning](http:\/\/web.stanford.edu\/class\/cs224n\/syllabus.html)\n- [Word2Vec Tutorial - The Skip-Gram Model \u00b7 Chris McCormick](http:\/\/mccormickml.com\/2016\/04\/19\/word2vec-tutorial-the-skip-gram-model\/)\n\n## Gensim\n- [gensim: models.word2vec \u2013 Deep learning with word2vec](https:\/\/radimrehurek.com\/gensim\/models\/word2vec.html)\n- [gensim: Tutorials](https:\/\/radimrehurek.com\/gensim\/tutorial.html)|\n- [\ud55c\uad6d\uc5b4\uc640 NLTK, Gensim\uc758 \ub9cc\ub0a8 - PyCon Korea 2015](https:\/\/www.lucypark.kr\/docs\/2015-pyconkr\/)","aa583136":"## Word2Vec (Word Embedding to Vector)\n#### \ucef4\ud4e8\ud130\ub294 \uc22b\uc790\ub9cc \uc778\uc2dd\ud560 \uc218 \uc788\uace0 \ud55c\uae00, \uc774\ubbf8\uc9c0\ub294 \ubc14\uc774\ub108\ub9ac \ucf54\ub4dc\ub85c \uc800\uc7a5\ub41c\ub2e4. \ud29c\ud1a0\ub9ac\uc5bc \ud30c\ud2b81\uc5d0\uc11c\ub294 Bag of Words \ub77c\ub294 \uac1c\ub150\uc744 \uc0ac\uc6a9\ud574\uc11c \uba38\uc2e0\ub7ec\ub2dd \uc54c\uace0\ub9ac\uc998\uc774 \uc774\ud574\ud560 \uc218 \uc788\ub3c4\ub85d \ubb38\uc790\ub97c \ubca1\ud130\ud654 \ud574\uc8fc\ub294 \uc791\uc5c5\uc744 \ud558\uc600\ub2e4.\n- one hot encoding \ud639\uc740 Bag of words\uc5d0\uc11c vector size\uac00 \ub9e4\uc6b0 \ud06c\uace0 sparse\ud558\ubbc0\ub85c neural net \uc131\ub2a5\uc774 \uc798 \ub098\uc624\uc9c0 \uc54a\ub294\ub2e4.\n- '\uc8fc\uc704 \ub2e8\uc5b4\uac00 \ube44\uc2b7\ud558\uba74 \ud574\ub2f9 \ub2e8\uc5b4\uc758 \uc758\ubbf8\ub294 \uc720\uc0ac\ud558\ub2e4'\ub77c\ub294 \uc544\uc774\ub514\uc5b4\n- \ub2e8\uc5b4\ub97c \ud2b8\ub808\uc774\ub2dd \uc2dc\ud0ac \ub54c \uc8fc\uc704 \ub2e8\uc5b4\ub97c label\ub85c \ub9e4\uce58\ud558\uc5ec \ucd5c\uc801\ud654\n- \ub2e8\uc5b4\ub97c \uc758\ubbf8\ub97c \ub0b4\ud3ec\ud55c dense vector\ub85c \ub9e4\uce6d \uc2dc\ud0a4\ub294 \uac83\n- Word2Vec\uc740 \ubd84\uc0b0 \ub41c \ud14d\uc2a4\ud2b8 \ud45c\ud604\uc744 \uc0ac\uc6a9\ud558\uc5ec \uac1c\ub150 \uac04 \uc720\uc0ac\uc131\uc744 \ubcf8\ub2e4. \uc608\ub97c \ub4e4\uc5b4, \ud30c\ub9ac\uc640 \ud504\ub791\uc2a4\uac00 \ubca0\ub97c\ub9b0\uacfc \ub3c5\uc77c\uacfc \uac19\uc740 \ubc29\uc2dd\uc73c\ub85c \uad00\ub828\ub418\uc5b4 \uc788\uc74c\uc744 \uc774\ud574\ud55c\ub2e4 (\uc218\ub3c4\uc640 \uad6d\uac00)"}}