{"cell_type":{"ff7076e3":"code","4701d29e":"code","1fb8d2e2":"code","faeb55ae":"code","8d104b61":"code","3d56e4b6":"code","4bf0eef7":"code","1982c29e":"code","294ad43b":"code","d1d5a092":"code","6aa5c3a9":"code","25827a4a":"code","9a8001d3":"code","f9fe4dca":"code","6b49fc41":"code","1412ded2":"code","60e7a6ff":"code","d4f9b72d":"code","e559dbae":"code","156b4949":"code","34776774":"code","a2c69fbe":"code","293d2740":"code","1f7e8c8d":"code","f042dbe2":"code","91ea3f3f":"code","b75e63b8":"code","5df95019":"code","a7191232":"code","df70894c":"code","6008c1a7":"code","cb5e0669":"code","2c17e04d":"code","5b923599":"code","5e5cf561":"code","55ca120d":"code","aaad68d3":"code","781193fe":"code","178f0881":"code","ff82c107":"code","fcb0d333":"code","e6710229":"code","defb5805":"code","1f166ccd":"code","b7aeddaa":"code","4c5187e3":"code","dbdf32ef":"code","6018e299":"code","0bb7ba0f":"code","18be58d4":"code","99425745":"code","13522ef5":"code","0d617d8e":"markdown","4c274e33":"markdown","c932777d":"markdown","7aa0c7b2":"markdown","57f52ceb":"markdown","d83b5459":"markdown","30337682":"markdown","622c6a5c":"markdown","7a2be41d":"markdown","419a5ed4":"markdown","9bba862f":"markdown","fc857cdc":"markdown","1141e662":"markdown","b6995c2f":"markdown","981da9c1":"markdown","3c48981d":"markdown","83d4de52":"markdown","bee39902":"markdown","bdc40dae":"markdown","1b7c5fbe":"markdown","98f85096":"markdown","95d11d70":"markdown","65942af3":"markdown","115daebd":"markdown","a897d90d":"markdown","95006fbc":"markdown","b237f69f":"markdown","8aae5ab4":"markdown","92591480":"markdown","591de1b3":"markdown","d9abd9b7":"markdown","eb33cce0":"markdown","0ce7d9fe":"markdown","387ed8e2":"markdown","884e2fad":"markdown","ae1c43f0":"markdown","edd02ae8":"markdown","33e898e3":"markdown","c9c91043":"markdown","15538679":"markdown","f861a2bd":"markdown","d1802f2a":"markdown","36dcc9a4":"markdown","0f7aec95":"markdown","2954db9c":"markdown","29b9086e":"markdown","80063abc":"markdown","839ffaa1":"markdown","adb5a605":"markdown","c937c101":"markdown","d78fceb5":"markdown","d47f15fd":"markdown","ec4647dd":"markdown","72f38a19":"markdown","bb3d0c6f":"markdown","84ed0e29":"markdown","6f8c8ae1":"markdown","44e7d707":"markdown","5e2447fa":"markdown","57d40306":"markdown","3e49d9db":"markdown"},"source":{"ff7076e3":"\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport os\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport plotly.express as px\nfrom plotly.offline import init_notebook_mode\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\ninit_notebook_mode(connected=True)\npd.set_option('display.max_columns', 5000)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#os.mkdir('\/kaggle\/working\/individual_charts\/')\nimport matplotlib.pyplot as plt\n# Load the data\n#Will come in handy to wrap the lengthy texts\nimport textwrap\n#useful libraries and functions\nfrom itertools import repeat\n#Libraries that give a different visual possibilities\nfrom pandas import option_context \nfrom plotly.subplots import make_subplots\n\ndef long_sentences_seperate(sentence, width=30):\n    try:\n        splittext = textwrap.wrap(sentence,width)\n        text = '<br>'.join(splittext)#whitespace is removed, and the sentence is joined\n        return text\n    except:\n        return sentence\n\ndef load_csv(base_dir,file_name):\n    \"\"\"Loads a CSV file into a Pandas DataFrame\"\"\"\n    file_path = os.path.join(base_dir,file_name)\n    df = pd.read_csv(file_path,low_memory=False)\n    return df    \n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n","4701d29e":"base_dir = '..\/input\/widsdatathon2022\/'\nfile_name = 'train.csv'\ndataset_main = load_csv(base_dir,file_name)","1fb8d2e2":"def uni_factor(factor):\n    grp_factor = dataset_main.groupby(factor)['floor_area'].count().reset_index()\n    grp_factor.sort_values(by='floor_area',inplace=True,ascending = False)\n    grp_factor[factor] = grp_factor[factor].astype('category')\n    vis = px.bar(data_frame=grp_factor,y = factor,x ='floor_area',color= factor)\n    vis.update_layout(yaxis = {'categoryorder' : 'total ascending'},\n                      title = 'Number of Buildings based on ' + factor,\n                     height = 800)\n    vis.show()","faeb55ae":"def uni_hist_plot(independent,dependent):\n    vis = px.histogram(data_frame=dataset_main,x=dependent,color=independent)\n    vis.update_layout(title='Distribution of '+ dependent + ' based on '+ independent)\n    vis.show()","8d104b61":"def two_factor(factor1, factor2,independent):\n    vis = px.scatter(data_frame=dataset_main,y = factor1,x =factor2,color=independent,\n                     facet_col=independent,facet_col_wrap=3)\n    vis.update_layout(title = 'Relation between ' + factor1 + ' and ' +factor2 + ' in ' + independent + 'condition',\n                     height = 1000)\n    vis.show()","3d56e4b6":"def avg_on_factor(factor,target):\n    grp_factor = dataset_main.groupby(factor)[target].mean().reset_index()\n    grp_factor.sort_values(by=target,inplace=True,ascending = False)\n    grp_factor[factor] = grp_factor[factor].astype('category')\n    vis = px.bar(data_frame=grp_factor,y = factor,x =target,color= factor)\n    vis.update_layout(yaxis = {'categoryorder' : 'total ascending'},\n                      title = 'Average of '+target +' on basis of ' + factor,\n                     height = 800)\n    vis.show()","4bf0eef7":"#Start by getting the distribution of the categorical values\nuni_factor('Year_Factor')","1982c29e":"#Start by getting the distribution of the categorical values\nuni_factor(factor='State_Factor')","294ad43b":"#Start by getting the distribution of the categorical values\nuni_factor(factor='building_class')","d1d5a092":"uni_factor(factor='facility_type')","6aa5c3a9":"# How the year factor impacts the Energy star ratings or Site EUI. This will be histogram of the dependent variable\nuni_hist_plot(independent='Year_Factor',dependent='energy_star_rating')","25827a4a":"uni_hist_plot(independent='State_Factor',dependent='energy_star_rating')","9a8001d3":"uni_hist_plot(independent='building_class',dependent='energy_star_rating')","f9fe4dca":"# How the year factor impacts the Site EUI. This will be histogram of the dependent variable\nuni_hist_plot(independent='Year_Factor',dependent='site_eui')","6b49fc41":"uni_hist_plot(independent='State_Factor',dependent='site_eui')","1412ded2":"uni_hist_plot(independent='building_class',dependent='site_eui')","60e7a6ff":"vis11 = px.scatter(data_frame=dataset_main,x='site_eui',y='energy_star_rating')\nvis11.update_layout(title='Site EUI and Energy star rating relationship')\nvis11.show()","d4f9b72d":"#Building the heat map of the dataset to find the highly correlated data\ndata_num = dataset_main[dataset_main.columns[dataset_main.dtypes != 'object']]\n\ndata_corr_mat = data_num.corr() #building the correlation matrix\n\n#Building the lower triagle of the correlation matrix\ndata_corr_mat_lt = data_corr_mat.where(np.tril(np.ones(data_corr_mat.shape)).astype(np.bool))\n\nvis12 = px.imshow(data_corr_mat_lt,text_auto=True, aspect=\"auto\",\n                  height=1000,color_continuous_scale='spectral',width=900)\n\nvis12.add_shape(\n    dict(type=\"circle\", x0=1, y0=40, x1=20, y1=52),line_color=\"green\")\n\nvis12.add_annotation(x= 0.1,y=0.35,text='Negatively Correlated',xref=\"paper\",\n                   yref=\"paper\",showarrow=False,font_size=20)\n\nvis12.add_shape(\n    dict(type=\"circle\", x0=1, y0=5, x1=20, y1=17),line_color=\"green\")\n\nvis12.add_annotation(x= 0.05,y=0.95,text='Positively Correlated',xref=\"paper\",\n                   yref=\"paper\",showarrow=False,font_size=20)\n\nvis12.show()","e559dbae":"#Plotting the values that are positively and negatively correlated\nvis13 = go.Figure()\n\nvis13.add_trace(go.Bar(x=data_corr_mat[data_corr_mat.site_eui < 0].site_eui.values,\n                       y=data_corr_mat[data_corr_mat.site_eui < 0].site_eui.index,\n                      name='Negative_Correlation',orientation='h'))\n\nvis13.add_trace(go.Bar(x=data_corr_mat[data_corr_mat.site_eui > 0].site_eui.values,\n                       y=data_corr_mat[data_corr_mat.site_eui > 0].site_eui.index,\n                      name='Posiive_Correlation',orientation='h'))\n\nvis13.update_layout(height=1000,width=700,title='Correlation with Site EUI',\n                   yaxis = {'categoryorder' : 'total descending'})\nvis13.show()","156b4949":"#Lets do some two factor data analysis based on \ntwo_factor(factor1='site_eui',factor2='energy_star_rating',independent='State_Factor')","34776774":"#Lets do some two factor data analysis based on \ntwo_factor(factor1='site_eui',factor2='energy_star_rating',independent='building_class')","a2c69fbe":"#Lets do some two factor data analysis based on \ntwo_factor(factor1='site_eui',factor2='snowfall_inches',independent='State_Factor')","293d2740":"#Lets do some two factor data analysis based on \ntwo_factor(factor1='site_eui',factor2='snowfall_inches',independent='building_class')","1f7e8c8d":"avg_on_factor(factor='building_class',target='energy_star_rating')","f042dbe2":"avg_on_factor(factor='State_Factor',target='energy_star_rating')","91ea3f3f":"avg_on_factor(factor='facility_type',target='energy_star_rating')","b75e63b8":"#Collecting garbage memory and deleting unwanted Dataframes, that have served their purpose earlier\nimport gc\ndel data_corr_mat, data_corr_mat_lt,data_num\ngc.collect()","5df95019":"# Segregating the columns with categorical value\ndata_cat = dataset_main[dataset_main.columns[dataset_main.dtypes == 'object']]\ndata_cat['Year_Factor'] = dataset_main.Year_Factor\n\ndata_num = dataset_main[dataset_main.columns[dataset_main.dtypes != 'object']]","a7191232":"#Some of the null values have to filled to '0' lets check them out\n\n# energy_star_rating is critical factor for modeling the site-eui rating. Assuming any value will \n# impact the model accuracy. The analysis will be taken forward with forward filling the null values. \n\ndata_num.energy_star_rating = data_num.energy_star_rating.fillna(method='ffill')\n\n#Below variables are made 0, since the sensors would have malfunctioned. So forward filling \ndata_num.direction_max_wind_speed = data_num.direction_max_wind_speed.fillna(method='ffill')\ndata_num.direction_peak_wind_speed = data_num.direction_peak_wind_speed.fillna(method='ffill')\ndata_num.max_wind_speed = data_num.max_wind_speed.fillna(method='ffill')\ndata_num.days_with_fog = data_num.days_with_fog.fillna(method='ffill')\ndata_num.days_with_fog = data_num.days_with_fog.fillna(method='bfill')\n","df70894c":"#A simple and straight_forward one-hot encoding using Pandas' Get_Dummies\ndata_cat = pd.get_dummies(data_cat,columns=data_cat.columns)\n\ndata_model = pd.merge(left=data_cat,right=data_num,left_index=True,right_index=True)\nprint(data_model.shape)\n\n#That is lot more columns than what started with. ","6008c1a7":"#https:\/\/stackoverflow.com\/a\/46581125\/16388185\ndef clean_dataset(df):\n    assert isinstance(df, pd.DataFrame), \"df needs to be a pd.DataFrame\"\n    df.dropna(inplace=True)\n    indices_to_keep = ~df.isin([np.nan, np.inf, -np.inf]).any(1)\n    return df[indices_to_keep].astype(np.float64)\n\ndata_model = clean_dataset(data_model)\n#data_model.drop('index',axis=1,inplace=True)","cb5e0669":"#Creating the X the Independent variables and Y the Target or Dependent variables\nX = data_model.iloc[:,:-2]\nY = data_model.site_eui","2c17e04d":"#Enter the Scaler the Standard Scaler. We normalise the values to ensure the data is easier for the \n#Algorithms to chew.\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().fit(X) #learns about the dataset\n\nrescaledX = pd.DataFrame(scaler.fit_transform(X),columns = X.columns, \n                               index = X.index)\n\n# summarize transformed data\nrescaledX.head(2)","5b923599":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nbestfeatures = SelectKBest(k=12)\n#instantiate the bestfeatures class\nbestfeatures\n\n\nfit = bestfeatures.fit(rescaledX,Y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(rescaledX.columns)\n\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\n\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns","5e5cf561":"#removing scores with infinity and nulls\nfeatureScores = featureScores[featureScores.Score != np.inf]\nfeatureScores = featureScores[~featureScores.isna()]\nfeatureScores.sort_values(by='Score',ascending=False,inplace=True)\n\n#Choosing the variables that have score more than 10\nfeatureScores[featureScores.Score >10].shape\n\n#Creating the updated X dataframe based on the new features for actual modeling\nX_new = rescaledX[featureScores.loc[featureScores.Score >10,'Specs'].values]\nX_new.shape","55ca120d":"#The statistics did not select the Energy rating column, but I am adding it for analysis\nX_new['energy_star_rating'] = dataset_main.energy_star_rating\nX_new_vis = X_new\n\n#Attaching the target value into the dataset for visualisation purpose.\nX_new_vis['site_eui'] = dataset_main.site_eui\n\nX_new_corr = X_new_vis.corr()\n\n#Building the lower triagle of the correlation matrix\nX_new_corr_lt = X_new_corr.where(np.tril(np.ones(X_new_corr.shape)).astype(np.bool))\n\nvis_feat = px.imshow(X_new_corr_lt,text_auto=True, aspect=\"auto\",\n                  height=1000,color_continuous_scale='spectral',width=900)\n\nvis_feat.update_layout(title='Correlation matrix of the selected variables and SiteEUI')\nvis_feat.show()\n","aaad68d3":"data_model_reduced = clean_dataset(X_new_vis)\n#data_model.drop('index',axis=1,inplace=True)","781193fe":"#Forming the new Independent terms for proceeding with the Analysis\nY = data_model_reduced.site_eui\ndata_model_reduced.drop('site_eui',axis=1,inplace=True)\nX = data_model_reduced","178f0881":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n\n# Error Metrics\nfrom sklearn.metrics import mean_squared_error\n","ff82c107":"# split out validation dataset for the end\n\nvalidation_size = 0.2\n\n#In case the data is not dependent on the time series, then train and test split randomly\nseed = 7\n# X_train, X_validation, Y_train, Y_validation = train_test_split(X, Y, test_size=validation_size, random_state=seed)\n\n#In case the data is not dependent on the time series, then train and test split should be done based on sequential sample\n#This can be done by selecting an arbitrary split point in the ordered list of observations and creating two new datasets.\n\ntrain_size = int(len(X) * (1-validation_size))\nX_train, X_test = X[0:train_size], X[train_size:len(X)]\nY_train, Y_test = Y[0:train_size], Y[train_size:len(X)]","fcb0d333":"# test options for regression\nnum_folds = 10\nscoring = 'neg_mean_squared_error'\n#scoring ='neg_mean_absolute_error'\n#scoring = 'r2'","e6710229":"# spot check the traditional Machine Learning algorithms\nmodels = []\nmodels.append(('LR', LinearRegression()))\nmodels.append(('LASSO', Lasso()))\nmodels.append(('EN', ElasticNet()))\nmodels.append(('KNN', KNeighborsRegressor()))\nmodels.append(('CART', DecisionTreeRegressor()))\n#models.append(('SVR', SVR()))\n\n#Ensable Models \n# Boosting methods\nmodels.append(('ABR', AdaBoostRegressor()))\nmodels.append(('GBR', GradientBoostingRegressor()))\n# Bagging methods\nmodels.append(('RFR', RandomForestRegressor()))\nmodels.append(('ETR', ExtraTreesRegressor()))","defb5805":"names = []\nkfold_results = []\ntest_results = []\ntrain_results = []\nfor name, model in models:\n    names.append(name)\n    \n    ## K Fold analysis:\n    \n    kfold = KFold(n_splits=num_folds, random_state=seed)\n    #converted mean square error to positive. The lower the beter\n    cv_results = -1* cross_val_score(model, X_train, Y_train, cv=kfold, scoring=scoring)\n    kfold_results.append(cv_results)\n    \n\n    # Full Training period\n    print('{} model fit Started'.format(model))\n    res = model.fit(X_train, Y_train)\n    print('{} model fit Completed'.format(model))\n    #The error function is root of mean_squared_error\n    train_result = np.sqrt(mean_squared_error(res.predict(X_train), Y_train))\n    train_results.append(train_result)\n    \n    # Test results\n    #The error function is root of mean_squared_error\n    test_result = np.sqrt(mean_squared_error(res.predict(X_test), Y_test))\n    test_results.append(test_result)\n    \n    msg = \"%s: %f (%f) %f %f\" % (name, cv_results.mean(), cv_results.std(), train_result, test_result)\n    print(msg)","1f166ccd":"kfold = pd.DataFrame(kfold_results,columns=range(1,11)).T\nkfold.columns = ['LR','LASSO','EN','KNN','CART','ABR','GBR','RFR','ETR']","b7aeddaa":"vis21 = go.Figure()\nfor kf in kfold.columns:\n    vis21.add_trace(go.Box(x=kfold[kf],name=kf))\nvis21.update_xaxes(type='log')\nvis21.update_layout(title='Kfold Error Results')\nvis21.show()","4c5187e3":"# compare algorithms\nfig = plt.figure()\n\nind = np.arange(len(names))  # the x locations for the groups\nwidth = 0.35  # the width of the bars\n\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.bar(ind - width\/2, train_results,  width=width, label='Train Error')\nplt.bar(ind + width\/2, test_results, width=width, label='Test Error')\nfig.set_size_inches(15,8)\nplt.legend()\nax.set_xticks(ind)\nax.set_xticklabels(names)\nplt.show()","dbdf32ef":"test_main = load_csv(base_dir='..\/input\/widsdatathon2022', file_name='test.csv')\n\n#Take only the values that has been chosen in the model training phase.\n\n#Creating seperate category and numerical datasets\ntest_cat = test_main[test_main.columns[test_main.dtypes == 'object']]\ntest_cat['Year_Factor'] = test_main.Year_Factor\ntest_num = test_main[test_main.columns[test_main.dtypes != 'object']]\n\ntest_num.energy_star_rating = test_num.energy_star_rating.fillna(method='ffill')\ntest_num.direction_max_wind_speed = test_num.direction_max_wind_speed.fillna(method='ffill')\ntest_num.direction_peak_wind_speed = test_num.direction_peak_wind_speed.fillna(method='ffill')\ntest_num.max_wind_speed = test_num.max_wind_speed.fillna(method='ffill')\ntest_num.days_with_fog = test_num.days_with_fog.fillna(method='ffill')\ntest_num.days_with_fog = test_num.days_with_fog.fillna(method='bfill')\n\n#A simple and straight_forward one-hot encoding using Pandas' Get_Dummies\ntest_cat = pd.get_dummies(test_cat,columns=test_cat.columns)\n\ntest_model = pd.merge(left=test_cat,right=test_num,left_index=True,right_index=True)\nprint(test_model.shape)\n\n#The earlier feature scores contained the Year_factors 3,4, and 5. We train the final model without \n#these 3 factors. ","6018e299":"#There are couple of reasons for doing the following features reduction\n\n# Obvious is, the computation time reduces, and the accuracy of the final result is expected to be goov\n\n# Not obvious reason is, when using whole test dataset, errors due to \"huge number that float64 cannot handle\" pops up.\n\nX_train = data_model[['days_below_0F', 'days_below_10F', 'facility_type_Laboratory',\n                      'december_max_temp', 'june_min_temp','march_max_temp',\n                      'september_min_temp', 'september_max_temp',\n                      'days_below_20F', 'snowdepth_inches', 'ELEVATION',\n                      'days_with_fog', 'energy_star_rating']]\n\ntest_model = test_model[['days_below_0F', 'days_below_10F', 'facility_type_Laboratory',\n                      'december_max_temp', 'june_min_temp','march_max_temp',\n                      'september_min_temp', 'september_max_temp',\n                      'days_below_20F', 'snowdepth_inches', 'ELEVATION',\n                      'days_with_fog', 'energy_star_rating']]\n\nX_train = X_train.astype('float64')\ntest_model = test_model.astype('float64')","0bb7ba0f":"#Instantiating the ElasticNet model\nfinal_model = ElasticNet()\nY_train = data_model.site_eui #The complete train data target\nres = final_model.fit(X_train, Y_train)","18be58d4":"Y_test = res.predict(test_model)","99425745":"my_submission = pd.DataFrame({'id': test_main.id, 'site_eui': Y_test})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","13522ef5":"my_submission.head(7)","0d617d8e":"#### <a id='pred'> Prediction with Final Model and submission<\/a>","4c274e33":"#### [Back to Contents](#contents)","c932777d":"#### [Back to Contents](#contents)","7aa0c7b2":"#### [Back to Contents](#contents)","57f52ceb":"#### [Back to Contents](#contents)","d83b5459":"#### <a id='vis_5'> Visual_5 Energy ratings of building depending on Year_factor <\/a>","30337682":"#### <a id='vis_8'> Visual_8 Site EUI of building depending on Year_factor <\/a>","622c6a5c":"#### [Back to Contents](#contents)","7a2be41d":"#### <a id='vis_9'> Visual_9 Site EUI of building depending on State_factor <\/a>","419a5ed4":"##### <a id='vis'> Visualising the Heatmap of the features <\/a>","9bba862f":"1) Elastic Net and Lasso seems to have the least test_error. There is only little over fitting in both these models. \n\n2) Grid_searching these 2 models may yield better test_error reduction\n\n3) Exploring feature engineering through PCA, KPCA or tSNE also can be an option. \n\n4) Site_EUI is deeply impacted by the Energy_star_Rating of the building.","fc857cdc":"#### [Back to Contents](#contents)","1141e662":"#### <a id='vis_14'> Visual_14 Site EUI and Energy star rating variation among states <\/a>","b6995c2f":"#### [Back to Contents](#contents)","981da9c1":"#### [Back to Contents](#contents)","3c48981d":"#### [Back to Contents](#contents)","83d4de52":"#### <a id='vis_16'> Visual_16 Site EUI and Snow fall inches variation among states <\/a>","bee39902":"#### <a id='vis_20'> Visual_20 Average Energy star rating based on Facility Type <\/a>","bdc40dae":"#### [Back to Contents](#contents)","1b7c5fbe":"#### [Back to Contents](#contents)","98f85096":"#### [Back to Contents](#contents)","95d11d70":"The model that comes close to best performance is Elastic Net. We will create prediction with the X_test data that has been provided","65942af3":"#### [Back to Contents](#contents)","115daebd":"#### [Back to Contents](#contents)","a897d90d":"#### <a id='mod_set'> Setting up models and Training <\/a>","95006fbc":"#### [Back to Contents](#contents)","b237f69f":"#### <a id='obs'> Observation and model finalisation <\/a>","8aae5ab4":"#### <a id='dat_pre'> Preparing data for modeling <\/a>","92591480":"#### <a id='vis_15'> Visual_15 Site EUI and Energy star rating variation among Building Class <\/a>","591de1b3":"#### <a id='vis_10'> Visual_10 Site EUI of building depending on Building Class <\/a>","d9abd9b7":"#### [Back to Contents](#contents)","eb33cce0":"#### <a id='vis_6'> Visual_6 Energy ratings of building depending on State_Factor <\/a>","0ce7d9fe":"#### <a id='vis_4'> Visual_4 Building Count based Facility Type <\/a>","387ed8e2":"##### <a id='scale'> Scaling data with Standard Scaler <\/a>","884e2fad":"#### <a id='vis_12'> Visual_12 Heat map of the Numerical values in the Dataset <\/a>","ae1c43f0":"#### [Back to Contents](#contents)","edd02ae8":"## <a id='contents'>  Contents <\/a>\n\n### [Visual_1  Building Count based Year Factor](#vis_1)\n### [Visual_2 Building Count based State Factor](#vis_2)\n### [Visual_3 Building Count based Building Type](#vis_3)\n### [Visual_4 Building Count based Facility Type](#vis_4)\n### [Visual_5 Energy ratings of building depending on Year_factor](#vis_5)\n### [Visual_6 Energy ratings of building depending on State_Factor](#vis_6)\n### [Visual_7 Energy ratings of building depending on Building Class](#vis_7)\n### [Visual_8 Site EUI of building depending on Year_factor](#vis_8)\n### [Visual_9 Site EUI of building depending on State_factor](#vis_9)\n### [Visual_10 Site EUI of building depending on Building Class](#vis_10)\n### [Visual_11 Site EUI and Energy Star Rating relationship](#vis_11)\n### [Visual_12 Heat map of the Numerical values in the Dataset](#vis_12)\n### [Visual_13 Correlation Bar of Site EUI with other variables](#vis_13)\n### [Visual_14 Site EUI and Energy star rating variation among states ](#vis_14)\n### [Visual_15 Site EUI and Energy star rating variation among Building Class](#vis_15)\n### [Visual_16 Site EUI and Snow fall inches variation among states](#vis_16)\n### [Visual_17 Site EUI and Snow fall inches variation among building class](#vis_17)\n### [Visual_18 Average Energy star rating based on  building class](#vis_18)\n### [Visual_19 Average Energy star rating based on State Factor](#vis_19)\n### [Visual_20 Average Energy star rating based on Facility Type](#vis_20)\n\n### [Dimensionality Reduction by selecting most useful features](#dimred)\n### [Preparing data for modeling](#dat_pre)\n### [Setting up models and Training](#mod_set)\n### [Results Obervation and Model Finalisation](#obs)\n### [Results and Model finalisation](#result)\n### [Prediction with Final Model and submission](#pred)\n","33e898e3":"#### <a id='result'> Results and Model finalisation <\/a>","c9c91043":"#### [Back to Contents](#contents)","15538679":"##### <a id='1hot'> Complete One - Hot Encoding <\/a>","f861a2bd":"#### <a id='vis_18'> Visual_18 Average Energy star rating based on  building class <\/a>","d1802f2a":"##### <a id='select'> Selecting the Features using Statistical methods <\/a>","36dcc9a4":"#### <a id='vis_17'> Visual_17 Site EUI and Snow fall inches variation among building class <\/a>","0f7aec95":"#### [Back to Contents](#contents)","2954db9c":"#### [Back to Contents](#contents)","29b9086e":"#### <a id='vis_3'> Visual_3 Building Count based Building Type <\/a>","80063abc":"#### [Back to Contents](#contents)","839ffaa1":"#### <a id='vis_7'> Visual_7 Energy ratings of building depending on Building Class <\/a>","adb5a605":"#### [Back to Contents](#contents)","c937c101":"#### <a id='vis_11'> Visual_11 Site EUI and Energy Star Rating relationship <\/a>","d78fceb5":"### Purpose of the Notebook:\n\nData Analysis Part\nTo explore the data on the buildings and environment using the Plotly Express and Graph Objects libraries. Has colorful graphs\n\nData Science Part\nFollow that with modeling regression with multiple algorithms and provide prediction analysis andd results. Tried make it bit colorful, but the workings are simply hidden from our lowly human eyes.\n\n### What to Expect\n\nThe [Final predictions](#pred) that were made using Elastic Net algorithm\n\nResults of the Analysis done on [multiple algorithms](#result) before selecting Elastic Net\n\nFeature selection and the [visualisation](#vis) on how they correlate to Site-EUI\n\nNotebook that follows the Data analysis followed with Data Science Pipelines. \n\nOne factor, and two factor Exploratory data analysis of the dataset. \n\nDataset itself is very mostly clean, and the fields are of datatypes that render well for graphing. The notebooks dives directly into visualisations, without any display of the dataset. Also, the functions created at the beginning of the notebook is used extensively to render the graphs.\n\n### Sneek Peek\n\nThere is always some simple [plot](vis_13) with a neat insight, which brings the solution in sight. There are [plots](vis_11) which give so much information and no insights in a single view. This exploration till now is filled with such enjoyable moments and [deadends](vis_12).","d47f15fd":"#### <a id='vis_13'> Visual_13 Correlation Bar of Site EUI with other variables <\/a>","ec4647dd":"#### <a id='vis_2'> Visual_2 Building Count based State Factor <\/a>","72f38a19":"Statistical tests can be used to select those features that have the strongest relationship with the output variable.The scikit-learn library provides the SelectKBest class that can be used with a suite of different statistical tests to select a specific number of features. The example below uses the chi-squared (chi\u00b2) statistical test for non-negative features to select 10 of the best features from the Dataset.","bb3d0c6f":"#### [Back to Contents](#contents)","84ed0e29":"#### <a id='vis_1'> Visual_1  Building Count based Year Factor <\/a>","6f8c8ae1":"#### <a id='vis_19'> Visual_19 Average Energy star rating based on State Factor <\/a>","44e7d707":"#### [Back to Contents](#contents)","5e2447fa":"#### [Back to Contents](#contents)","57d40306":"#### [Back to Contents](#contents)","3e49d9db":"#### <a id='dimred'> Selecting the Best features <\/a>\n\nDataset contains both numerical and categorical variables. There are 2 ways to encode, Label and One-Hot. Based on the impact the categories have had on the site-eui, we will best use One-Hot Encoding. The number of columns will increase, but then we can reduce the dimension algorithmically.\n\n[1) Complete One-Hot Encoding](#1hot)\n\n[2) Scale the data using Standard Scaler](#scale)\n\n[3) Apply the Dim Reduction algorithms](#select)"}}