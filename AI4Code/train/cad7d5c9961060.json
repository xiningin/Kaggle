{"cell_type":{"778ee6c9":"code","eae46920":"code","73e96d68":"code","53fd1863":"code","86a3ecff":"code","aa968b1b":"code","e024904b":"code","f36e5876":"code","59de227e":"code","1c471c64":"code","151f94b1":"code","fcb873cc":"code","31868dc7":"code","4615faee":"code","212742dc":"code","b1d7c865":"code","832f867a":"code","16403e8b":"code","2548b599":"code","fddac166":"code","0cac6801":"code","2ee50acc":"code","305616d7":"code","e829da75":"code","97ebf419":"markdown","1b294875":"markdown","2002875a":"markdown","c0b26fb4":"markdown","affc33a9":"markdown","9b439d04":"markdown","02537df7":"markdown","14fcfbdb":"markdown","29fbe9aa":"markdown","a627ab28":"markdown","a0d3a777":"markdown","31625254":"markdown","56c7853d":"markdown","791a4959":"markdown","990094c7":"markdown","daf41481":"markdown","07c34d89":"markdown","b1bdd66d":"markdown","0bc113ab":"markdown","a4dced82":"markdown","26ab60bd":"markdown","2a6ab7a7":"markdown","34a1f38c":"markdown","8a1a386d":"markdown","9e71f0bf":"markdown","f777e31b":"markdown","8aedd387":"markdown","23c20e44":"markdown","12a3ead7":"markdown","18f17140":"markdown","91dc6a07":"markdown","7e551caa":"markdown","1a38e1a2":"markdown","75a69077":"markdown","e22a6b43":"markdown","faabe8f0":"markdown","71f6eea3":"markdown","ae9866aa":"markdown","ea6bec05":"markdown","08c5e2e0":"markdown","61f64e31":"markdown"},"source":{"778ee6c9":"import pandas as pd\nimport numpy as np\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder\nfrom sklearn.metrics import precision_score\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import LinearSVC\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\n\nimport tensorflow as tf\nimport keras\nimport warnings\nfrom tensorflow.keras import layers\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.layers import Activation, Dropout, Flatten, Dense, ThresholdedReLU\nfrom keras.optimizers import Adam\nfrom keras import regularizers\nfrom keras.utils.np_utils import to_categorical\n\nfrom lightgbm import LGBMClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","eae46920":"data_filepath = \"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv\"\ndata = pd.read_csv(data_filepath)\n\ntest_filepath = \"..\/input\/hr-analytics-job-change-of-data-scientists\/aug_test.csv\"\ntest = pd.read_csv(test_filepath)\n\ndata.sample(10)","73e96d68":"data.isnull().sum(axis = 0).sort_values(ascending=False)","53fd1863":"data['company_type'] = data['company_type'].fillna(\"Unspecified\")\n\ndata['company_size'] = data['company_size'].fillna(0)\ndict_company_size = {'<10': 1, '10\/49': 2, '50-99': 3, '100-500': 4, '500-999': 5, '1000-4999': 6, '5000-9999': 7, '10000+': 8}\ndata = data.replace({\"company_size\": dict_company_size})\n\ndata['gender'] = data['gender'].fillna(0)\ndict_gender = {'Male': 1, 'Female': 2, 'Other': 3}\ndata = data.replace({\"gender\": dict_gender})\n\ndata['major_discipline'] = data['major_discipline'].fillna(\"Unspecified\")\n\ndata['education_level'] = data['education_level'].fillna(\"Unspecified\")\n\ndata['last_new_job'] = data['last_new_job'].fillna(0)\ndict_last_new_job = {'>4': 5, 'never': 6}\ndata = data.replace({\"last_new_job\": dict_last_new_job})\n\ndata['enrolled_university'] = data['enrolled_university'].fillna(\"Unspecified\")\n\ndata['experience'] = data['experience'].fillna(22)\ndict_experience = {'<1': 0, '>20': 21}\ndata = data.replace({\"experience\": dict_experience})\n\ncol_nums = ['company_size', 'gender', 'last_new_job', 'experience', 'enrollee_id', 'city_development_index', 'training_hours', 'target']\ndata[col_nums] = data[col_nums].apply(pd.to_numeric, errors='coerce')\ndata['target'] = data['target'].astype(int)\n\ncolumns = data.columns\nfor i in columns:\n    print(\"\\n{} :\\n{}\".format(i, data[i].unique()))","86a3ecff":"data.info()","aa968b1b":"X_for_labelencoding = data[['city_development_index', 'gender', 'city', 'relevent_experience', 'enrolled_university', 'education_level', 'major_discipline', 'company_type', 'company_size', 'experience', 'last_new_job', 'training_hours']]\nX = X_for_labelencoding.copy()\ny = data['target']\n\nlabelencoder = LabelEncoder()\n\nstr_cols = X_for_labelencoding.iloc[:, 2:8]\nclfs = {c:labelencoder for c in str_cols}\n\nfor col, clf in clfs.items():\n    X[col] = clfs[col].fit_transform(X_for_labelencoding[col])\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=30)","e024904b":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train.astype(np.float32))\nX_val = sc.transform(X_val.astype(np.float32))\n\nX_train_sm_cp, X_val_sm, y_train_sm_cp, y_val_sm = X_train, X_val, y_train, y_val\nX_train_nn, X_val_nn, y_train_nn, y_val_nn = X_train, X_val, y_train, y_val\n\ny_train_nn = to_categorical(y_train_nn, 2)\ny_val_nn = to_categorical(y_val_nn, 2)","f36e5876":"lr = LogisticRegression(max_iter = 10000)\nlr.fit(X_train, y_train)\n\ny_pred = lr.predict(X_val)\npipe_lr = make_pipeline(StandardScaler(), LogisticRegression(max_iter=1000, random_state=1))\npipe_lr.fit(X_train, y_train)\nlr_pacs = pipe_lr.score(X_val, y_val)*100\nprint('Logistic Regression -')\nprint('   Pipeline Accuracy Score = ', lr_pacs)\n\nlogreg_clf_report = classification_report(y_val, y_pred, output_dict=True)\ncm_logreg = confusion_matrix(y_val, y_pred)","59de227e":"knn = KNeighborsClassifier(n_neighbors = 5, metric = 'manhattan', p = 1)\nknn.fit(X_train,y_train)\n\ny_pred = knn.predict(X_val)\npipe_knn = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean', p = 2))\npipe_knn.fit(X_train, y_train)\nknn_pacs = pipe_knn.score(X_val, y_val)*100\nprint('K-Nearest Neighbours -')\nprint('   Pipeline Accuracy Score = ', knn_pacs)\n\nknn_clf_report = classification_report(y_val, y_pred, output_dict=True)\ncm_knn = confusion_matrix(y_val, y_pred)","1c471c64":"svc = LinearSVC(max_iter=11000)\nsvc.fit(X_train,y_train)\n\ny_pred = svc.predict(X_val)\npipe_svc = make_pipeline(StandardScaler(), LinearSVC(penalty='l2', loss='squared_hinge', class_weight='balanced', verbose=0, random_state=101))\npipe_svc.fit(X_train, y_train)\nsvc_pacs = pipe_svc.score(X_val, y_val)*100\nprint('Support Vector Classifier -')\nprint('   Pipeline Accuracy Score = ', svc_pacs)\n\nsvc_clf_report = classification_report(y_val, y_pred, output_dict=True)\ncm_svc = confusion_matrix(y_val, y_pred)","151f94b1":"nb = GaussianNB()\nnb.fit(X_train,y_train)\n\ny_pred = nb.predict(X_val)\npipe_gnb = make_pipeline(StandardScaler(), GaussianNB())\npipe_gnb.fit(X_train, y_train)\nnb_pacs = pipe_gnb.score(X_val, y_val)*100\nprint('Gaussian Naive Bayes -')\nprint('   Pipeline Accuracy Score = ', nb_pacs)\n\nnb_clf_report = classification_report(y_val, y_pred, output_dict=True)\ncm_nb = confusion_matrix(y_val, y_pred)","fcb873cc":"dt = DecisionTreeClassifier(criterion = 'entropy')\ndt.fit(X_train,y_train)\n\ny_pred = dt.predict(X_val)\npipe_dtc = make_pipeline(StandardScaler(), DecisionTreeClassifier(criterion='entropy', max_depth=3))\npipe_dtc.fit(X_train, y_train)\ndt_pacs = pipe_dtc.score(X_val, y_val)*100\nprint('Decision Tree Classifier -')\nprint('   Pipeline Accuracy Score = ', dt_pacs)\n\ndt_clf_report = classification_report(y_val, y_pred, output_dict=True)\ncm_dt = confusion_matrix(y_val, y_pred)","31868dc7":"rf = RandomForestClassifier()\nrf.fit(X_train,y_train)\n\ny_pred = rf.predict(X_val)\npipe_rft = make_pipeline(StandardScaler(), RandomForestClassifier(criterion='entropy', random_state=0))\npipe_rft.fit(X_train, y_train)\nrf_pacs = pipe_rft.score(X_val, y_val)*100\nprint('Random Forest Classifier -')\nprint('   Pipeline Accuracy Score = ', rf_pacs)\n\nrf_clf_report = classification_report(y_val, y_pred, output_dict=True)\ncm_rf = confusion_matrix(y_val, y_pred)","4615faee":"clf_lgrg_df = pd.DataFrame(logreg_clf_report).transpose().rename_axis('parameters').reset_index()\nclf_knn_df = pd.DataFrame(knn_clf_report).transpose().rename_axis('parameters').reset_index()\nclf_svc_df = pd.DataFrame(svc_clf_report).transpose().rename_axis('parameters').reset_index()\nclf_nb_df = pd.DataFrame(nb_clf_report).transpose().rename_axis('parameters').reset_index()\nclf_dt_df = pd.DataFrame(dt_clf_report).transpose().rename_axis('parameters').reset_index()\nclf_rf_df = pd.DataFrame(rf_clf_report).transpose().rename_axis('parameters').reset_index()\n\nprec0_x = [clf_lgrg_df.iloc[0]['precision'], clf_knn_df.iloc[0]['precision'], clf_svc_df.iloc[0]['precision'], clf_nb_df.iloc[0]['precision'], clf_dt_df.iloc[0]['precision'], clf_rf_df.iloc[0]['precision']]\nprec1_x = [clf_lgrg_df.iloc[1]['precision'], clf_knn_df.iloc[1]['precision'], clf_svc_df.iloc[1]['precision'], clf_nb_df.iloc[1]['precision'], clf_dt_df.iloc[1]['precision'], clf_rf_df.iloc[1]['precision']]\nprecac_x = [clf_lgrg_df.iloc[2]['precision'], clf_knn_df.iloc[2]['precision'], clf_svc_df.iloc[2]['precision'], clf_nb_df.iloc[2]['precision'], clf_dt_df.iloc[2]['precision'], clf_rf_df.iloc[2]['precision']]\nprecma_x = [clf_lgrg_df.iloc[3]['precision'], clf_knn_df.iloc[3]['precision'], clf_svc_df.iloc[3]['precision'], clf_nb_df.iloc[3]['precision'], clf_dt_df.iloc[3]['precision'], clf_rf_df.iloc[3]['precision']]\nprecwe_x = [clf_lgrg_df.iloc[4]['precision'], clf_knn_df.iloc[4]['precision'], clf_svc_df.iloc[4]['precision'], clf_nb_df.iloc[4]['precision'], clf_dt_df.iloc[4]['precision'], clf_rf_df.iloc[4]['precision']]\n\nrec0_x = [clf_lgrg_df.iloc[0]['recall'], clf_knn_df.iloc[0]['recall'], clf_svc_df.iloc[0]['recall'], clf_nb_df.iloc[0]['recall'], clf_dt_df.iloc[0]['recall'], clf_rf_df.iloc[0]['recall']]\nrec1_x = [clf_lgrg_df.iloc[1]['recall'], clf_knn_df.iloc[1]['recall'], clf_svc_df.iloc[1]['recall'], clf_nb_df.iloc[1]['recall'], clf_dt_df.iloc[1]['recall'], clf_rf_df.iloc[1]['recall']]\nrecac_x = [clf_lgrg_df.iloc[2]['recall'], clf_knn_df.iloc[2]['recall'], clf_svc_df.iloc[2]['recall'], clf_nb_df.iloc[2]['recall'], clf_dt_df.iloc[2]['recall'], clf_rf_df.iloc[2]['recall']]\nrecma_x = [clf_lgrg_df.iloc[3]['recall'], clf_knn_df.iloc[3]['recall'], clf_svc_df.iloc[3]['recall'], clf_nb_df.iloc[3]['recall'], clf_dt_df.iloc[3]['recall'], clf_rf_df.iloc[3]['recall']]\nrecwe_x = [clf_lgrg_df.iloc[4]['recall'], clf_knn_df.iloc[4]['recall'], clf_svc_df.iloc[4]['recall'], clf_nb_df.iloc[4]['recall'], clf_dt_df.iloc[4]['recall'], clf_rf_df.iloc[4]['recall']]\n\nf10_x = [clf_lgrg_df.iloc[0]['f1-score'], clf_knn_df.iloc[0]['f1-score'], clf_svc_df.iloc[0]['f1-score'], clf_nb_df.iloc[0]['f1-score'], clf_dt_df.iloc[0]['f1-score'], clf_rf_df.iloc[0]['f1-score']]\nf11_x = [clf_lgrg_df.iloc[1]['f1-score'], clf_knn_df.iloc[1]['f1-score'], clf_svc_df.iloc[1]['f1-score'], clf_nb_df.iloc[1]['f1-score'], clf_dt_df.iloc[1]['f1-score'], clf_rf_df.iloc[1]['f1-score']]\nf1ac_x = [clf_lgrg_df.iloc[2]['f1-score'], clf_knn_df.iloc[2]['f1-score'], clf_svc_df.iloc[2]['f1-score'], clf_nb_df.iloc[2]['f1-score'], clf_dt_df.iloc[2]['f1-score'], clf_rf_df.iloc[2]['f1-score']]\nf1ma_x = [clf_lgrg_df.iloc[3]['f1-score'], clf_knn_df.iloc[3]['f1-score'], clf_svc_df.iloc[3]['f1-score'], clf_nb_df.iloc[3]['f1-score'], clf_dt_df.iloc[3]['f1-score'], clf_rf_df.iloc[3]['f1-score']]\nf1we_x = [clf_lgrg_df.iloc[4]['f1-score'], clf_knn_df.iloc[4]['f1-score'], clf_svc_df.iloc[4]['f1-score'], clf_nb_df.iloc[4]['f1-score'], clf_dt_df.iloc[4]['f1-score'], clf_rf_df.iloc[4]['f1-score']]\n\nsup0_x = [clf_lgrg_df.iloc[0]['support'], clf_knn_df.iloc[0]['support'], clf_svc_df.iloc[0]['support'], clf_nb_df.iloc[0]['support'], clf_dt_df.iloc[0]['support'], clf_rf_df.iloc[0]['support']]\nsup1_x = [clf_lgrg_df.iloc[1]['support'], clf_knn_df.iloc[1]['support'], clf_svc_df.iloc[1]['support'], clf_nb_df.iloc[1]['support'], clf_dt_df.iloc[1]['support'], clf_rf_df.iloc[1]['support']]\nsupac_x = [clf_lgrg_df.iloc[2]['support'], clf_knn_df.iloc[2]['support'], clf_svc_df.iloc[2]['support'], clf_nb_df.iloc[2]['support'], clf_dt_df.iloc[2]['support'], clf_rf_df.iloc[2]['support']]\nsupma_x = [clf_lgrg_df.iloc[3]['support'], clf_knn_df.iloc[3]['support'], clf_svc_df.iloc[3]['support'], clf_nb_df.iloc[3]['support'], clf_dt_df.iloc[3]['support'], clf_rf_df.iloc[3]['support']]\nsupwe_x = [clf_lgrg_df.iloc[4]['support'], clf_knn_df.iloc[4]['support'], clf_svc_df.iloc[4]['support'], clf_nb_df.iloc[4]['support'], clf_dt_df.iloc[4]['support'], clf_rf_df.iloc[4]['support']]\n\nclassifiers = [\"Logistic Regression\", \"K-Nearest Neighbours\", \"Support Vector Classifier\", \"Gaussian Naive-Bayes\", \"Decision Tree Classifier\", \"Random Forest Classifier\"]\n\nfig = plt.figure(figsize=(10,30), dpi=150)\ngs = fig.add_gridspec(19, 3)\ngs.update(wspace=0.2, hspace=0.5)\nsns.set_theme(style=\"whitegrid\")\nsns.despine(bottom=True, right=True)\nmpl.rcParams['xtick.labelsize'] = 6\nmpl.rcParams['ytick.labelsize'] = 7\n\naxs1 = fig.add_subplot(gs[0, 0])\naxs2 = fig.add_subplot(gs[0, 1])\naxs3 = fig.add_subplot(gs[0, 2])\naxs5 = fig.add_subplot(gs[1, 0])\naxs6 = fig.add_subplot(gs[1, 1])\naxs7 = fig.add_subplot(gs[1, 2])\naxs9 = fig.add_subplot(gs[2, 0])\naxs10 = fig.add_subplot(gs[2, 1])\naxs11 = fig.add_subplot(gs[2, 2])\naxs13 = fig.add_subplot(gs[3, 0])\naxs14 = fig.add_subplot(gs[3, 1])\naxs15 = fig.add_subplot(gs[3, 2])\naxs17 = fig.add_subplot(gs[4, 0])\naxs18 = fig.add_subplot(gs[4, 1])\naxs19 = fig.add_subplot(gs[4, 2])\n\n\ncolors_prec = [0.1, 0.2, 0.3, 0.4]\ncolors_rec = [0.2, 0.4, 0.6, 0.8]\ncolors_f1 = [0.5, 0.625, 0.75, 0.875]\n\naxs1.set(title = \"Precision\\n\\nMajority Class\")\nclfplt1 = sns.barplot(x=[i for i in prec0_x], y=classifiers, ax=axs1, color=colors_prec)\nclfplt1.set(xticklabels=[])\n\naxs2.set(title = \"Recall\\n\\nMajority Class\")\nclfplt2 = sns.barplot(x=[i for i in rec0_x], y=classifiers, ax=axs2, color=colors_rec)\nclfplt2.set(xticklabels=[], yticklabels=[])\n\naxs3.set(title = \"F-1 Score\\n\\nMajority Class\")\nclfplt3 = sns.barplot(x=[i for i in f10_x], y=classifiers, ax=axs3, color=colors_f1)\nclfplt3.set(xticklabels=[], yticklabels=[])\n\n\naxs5.set(title = \"Minority Class\")\nclfplt5 = sns.barplot(x=[i for i in prec1_x], y=classifiers, ax=axs5, color=colors_prec)\nclfplt5.set(xticklabels=[])\n\naxs6.set(title = \"Minority Class\")\nclfplt6 = sns.barplot(x=[i for i in rec1_x], y=classifiers, ax=axs6, color=colors_rec)\nclfplt6.set(xticklabels=[], yticklabels=[])\n\naxs7.set(title = \"Minority Class\")\nclfplt7 = sns.barplot(x=[i for i in f11_x], y=classifiers, ax=axs7, color=colors_f1)\nclfplt7.set(xticklabels=[], yticklabels=[])\n\n\naxs9.set(title = \"Accuracy\")\nclfplt9 = sns.barplot(x=[i for i in precac_x], y=classifiers, ax=axs9, color=colors_prec)\nclfplt9.set(xticklabels=[])\n\naxs10.set(title = \"Accuracy\")\nclfplt10 = sns.barplot(x=[i for i in recac_x], y=classifiers, ax=axs10, color=colors_rec)\nclfplt10.set(xticklabels=[], yticklabels=[])\n\naxs11.set(title = \"Accuracy\")\nclfplt11 = sns.barplot(x=[i for i in f1ac_x], y=classifiers, ax=axs11, color=colors_f1)\nclfplt11.set(xticklabels=[], yticklabels=[])\n\n\naxs13.set(title = \"Macro Average\")\nclfplt13 = sns.barplot(x=[i for i in precma_x], y=classifiers, ax=axs13, color=colors_prec)\nclfplt13.set(xticklabels=[])\n\naxs14.set(title = \"Macro Average\")\nclfplt14 = sns.barplot(x=[i for i in recma_x], y=classifiers, ax=axs14, color=colors_rec)\nclfplt14.set(xticklabels=[], yticklabels=[])\n\naxs15.set(title = \"Macro Average\")\nclfplt15 = sns.barplot(x=[i for i in f1ma_x], y=classifiers, ax=axs15, color=colors_f1)\nclfplt15.set(xticklabels=[], yticklabels=[])\n\n\naxs17.set(title = \"Weighted Average\")\nclfplt17 = sns.barplot(x=[i for i in precwe_x], y=classifiers, ax=axs17, color=colors_prec)\nclfplt17.set()\n\naxs18.set(title = \"Weighted Average\")\nclfplt18 = sns.barplot(x=[i for i in recwe_x], y=classifiers, ax=axs18, color=colors_rec)\nclfplt18.set(yticklabels=[])\n\naxs19.set(title = \"Weighted Average\")\nclfplt19 = sns.barplot(x=[i for i in f1we_x], y=classifiers, ax=axs19, color=colors_f1)\nclfplt19.set(yticklabels=[])\n\nsns.set_theme(style=\"whitegrid\")\nsns.despine(bottom=True, right=True)\nmpl.rcParams['xtick.labelsize'] = 6\nmpl.rcParams['ytick.labelsize'] = 7\nplt.show()","212742dc":"fig = plt.figure(figsize=(10,15), dpi=150)\ngs = fig.add_gridspec(5, 2)\ngs.update(wspace=0.1, hspace=0.5)\n\nax1 = fig.add_subplot(gs[0, 0])\nax2 = fig.add_subplot(gs[0, 1])\nax3 = fig.add_subplot(gs[1, 0])\nax4 = fig.add_subplot(gs[1, 1])\nax5 = fig.add_subplot(gs[2, 0])\nax6 = fig.add_subplot(gs[2, 1])\n\ncolors = [\"black\", \"lightgrey\"]\ncolormap = mpl.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nax1.set(title = \"Logistic Regression\")\nsns.heatmap(cm_logreg, linewidths=2.5, yticklabels=['Actual Non-Job Seeker','Actual Job Seeker'], xticklabels=['Predicted Non-Job Seeker','Predicted Job Seeker'], cmap=colormap, cbar=None, annot=True, fmt='d',ax=ax1, annot_kws={\"fontsize\":10})\nax2.set(title = \"K-Nearest Neighbours\")\nsns.heatmap(cm_knn, linewidths=2.5, yticklabels=False, xticklabels=['Predicted Non-Job Seeker','Predicted Job Seeker'], cmap=colormap, cbar=None, annot=True,fmt='d',ax=ax2,annot_kws={\"fontsize\":10})\nax3.set(title = \"Support Vector Classifier\")\nsns.heatmap(cm_svc, linewidths=2.5, yticklabels=['Actual Non-Job Seeker','Actual Job Seeker'],xticklabels=['Predicted Non-Job Seeker','Predicted Job Seeker'], cmap=colormap, cbar=None, annot=True, fmt='d', ax=ax3, annot_kws={\"fontsize\":10})\nax4.set(title = \"Gaussian Naive Bayes\")\nsns.heatmap(cm_nb, linewidths=2.5, yticklabels=False, xticklabels=['Predicted Non-Job Seeker','Predicted Job Seeker'], cmap=colormap, cbar=None, annot=True, fmt='d',ax=ax4,annot_kws={\"fontsize\":10})\nax5.set(title = \"Decision Tree Classifier\")\nsns.heatmap(cm_dt, linewidths=2.5, yticklabels=['Actual Non-Job Seeker','Actual Job Seeker'],xticklabels=['Predicted Non-Job Seeker','Predicted Job Seeker'], cmap=colormap, cbar=None, annot=True, fmt='d', ax=ax5, annot_kws={\"fontsize\":10})\nax6.set(title = \"Random Forest Classifier\")\nsns.heatmap(cm_rf, linewidths=2.5, yticklabels=False, xticklabels=['Predicted Non-Job Seeker','Predicted Job Seeker'], cmap=colormap, cbar=None, annot=True, fmt='d',ax=ax6,annot_kws={\"fontsize\":10})\n\nplt.show()","b1d7c865":"ml_class = [\"Logistic Regression\", \"K-Nearest Neighbours\", \"Support Vector Classifier\", \"Gaussian Naive-Bayes\", \"Decision Tree Classifier\", \"Random Forest Classifier\"]\nml_score = [lr_pacs, knn_pacs, svc_pacs, nb_pacs, dt_pacs, rf_pacs]\n\nfig = go.Figure(data=[\n    go.Bar(name='Pipeline Classification Scores', x = ml_class, y = ml_score, hovertemplate = \"Classification Levels: <br>Classification Technique: %{x} <\/br>Percentage Score: %{y} <\/br><extra><\/extra>\")\n])\n\nfig.update(layout_title_text='Classification Models and their Scores')\nfig.update_layout(hoverlabel=dict(bgcolor=\"white\", font_size=16, font_family=\"Rockwell\"), plot_bgcolor='white')\nfig.update_layout(xaxis=dict(showline=True, showgrid=False, showticklabels=True, linecolor='rgb(2, 2, 2)',\n                             linewidth=2, ticks='outside', tickfont=dict(family='Arial', size=12, color='rgb(82, 82, 82)')),\n                  yaxis=dict(showgrid=True, zeroline=True, showline=True, showticklabels=True, linecolor='rgb(2, 2, 2)',\n                             linewidth=2, ticks='outside', tickfont=dict(family='Arial', size=12, color='rgb(82, 82, 82)')))\nfig.update_layout(barmode='group')\nfig.show()","832f867a":"sm = SMOTE(random_state=101)\nX_train_sm, y_train_sm = sm.fit_resample(X_train_sm_cp, y_train_sm_cp.ravel())","16403e8b":"(unique_sm, counts_sm) = np.unique(y_train_sm, return_counts=True)\n(unique, counts) = np.unique(y_train, return_counts=True)\n\nfig = make_subplots(rows=1, cols=2, specs=[[{\"type\": \"domain\"}, {\"type\": \"domain\"}]])\n\nfig.add_trace(go.Pie(labels=[\"Job non-seeker\", \"Job seeker\"], values=counts, title='Unique Values', scalegroup='one', \n                     hovertemplate = \"Unique Value Distribution: <br>Value: %{label} <\/br>Percentage Score: %{percent} <\/br><extra><\/extra>\",\n                     marker = dict(line = dict(color='#000000', width=0.5)), pull=[0.05, 0.05]), 1, 1)\nfig.add_trace(go.Pie(labels=[\"Job non-seeker\", \"Job seeker\"], values=counts_sm, title='Unique Values - After SMOTE', scalegroup='one', \n                     hovertemplate = \"Unique Value Distribution: <br>Value: %{label} <\/br>Percentage Score: %{percent} <\/br><extra><\/extra>\",\n                     marker = dict(line = dict(color='#000000', width=0.5)), pull=[0.05, 0.05]), 1, 2)\n\nfig.update_traces(hoverinfo='label+percent')\nfig.update(layout_title_text='Unique Value Distribution - Before & After SMOTE', layout_showlegend=False)\nfig.update_layout(hoverlabel=dict(bgcolor=\"white\", font_size=16, font_family=\"Rockwell\"), hovermode='x')\nfig = go.Figure(fig)\nfig.show()","2548b599":"lr = LogisticRegression(max_iter=10000)\nlr.fit(X_train_sm, y_train_sm)\ny_pred_sm_lr = lr.predict(X_val_sm)\npipe_lr_sm = make_pipeline(StandardScaler(), LogisticRegression())\npipe_lr_sm.fit(X_train_sm, y_train_sm)\nlr_pacs_sm = pipe_lr_sm.score(X_val_sm, y_val_sm)*100\nprint('Logistic Regression -')\nprint('   Pipeline Accuracy Score = ', lr_pacs_sm)\nlogreg_clf_report_sm = classification_report(y_val_sm, y_pred_sm_lr, output_dict=True)\ncm_logreg_sm = confusion_matrix(y_val_sm, y_pred_sm_lr)\n\nknn = KNeighborsClassifier(algorithm='ball_tree')\nknn.fit(X_train_sm, y_train_sm)\ny_pred_sm_knn = knn.predict(X_val_sm)\npipe_knn_sm = make_pipeline(StandardScaler(), KNeighborsClassifier(algorithm='ball_tree'))\npipe_knn_sm.fit(X_train_sm, y_train_sm)\nknn_pacs_sm = pipe_knn_sm.score(X_val_sm, y_val_sm)*100\nprint('\\nK-Nearest Neighbours -')\nprint('   Pipeline Accuracy Score = ', knn_pacs_sm)\nknn_clf_report_sm = classification_report(y_val_sm, y_pred_sm_knn, output_dict=True)\ncm_knn_sm = confusion_matrix(y_val_sm, y_pred_sm_knn)\n\nsvc = LinearSVC(max_iter=11000)\nsvc.fit(X_train_sm, y_train_sm)\ny_pred_sm_svc = svc.predict(X_val_sm)\npipe_svc_sm = make_pipeline(StandardScaler(), LinearSVC(penalty='l2', loss='squared_hinge', class_weight='balanced', verbose=0, random_state=101))\npipe_svc_sm.fit(X_train_sm, y_train_sm)\nsvc_pacs_sm = pipe_svc_sm.score(X_val_sm, y_val_sm)*100\nprint('\\nSupport Vector Classifier -')\nprint('   Pipeline Accuracy Score = ', svc_pacs_sm)\nsvc_clf_report_sm = classification_report(y_val_sm, y_pred_sm_svc, output_dict=True)\ncm_svc_sm = confusion_matrix(y_val_sm, y_pred_sm_svc)\n\nnb = GaussianNB()\nnb.fit(X_train_sm, y_train_sm)\ny_pred_sm_nb = nb.predict(X_val_sm)\npipe_gnb_sm = make_pipeline(StandardScaler(), GaussianNB())\npipe_gnb_sm.fit(X_train_sm, y_train_sm)\nnb_pacs_sm = pipe_gnb_sm.score(X_val_sm, y_val_sm)*100\nprint('\\nGaussian Naive Bayes -')\nprint('   Pipeline Accuracy Score = ', nb_pacs_sm)\nnb_clf_report_sm = classification_report(y_val_sm, y_pred_sm_nb, output_dict=True)\ncm_nb_sm = confusion_matrix(y_val_sm, y_pred_sm_nb)\n\ndt = DecisionTreeClassifier(criterion = 'entropy')\ndt.fit(X_train_sm, y_train_sm)\ny_pred_sm_dt = dt.predict(X_val_sm)\npipe_dtc_sm = make_pipeline(StandardScaler(), DecisionTreeClassifier())\npipe_dtc_sm.fit(X_train_sm, y_train_sm)\ndt_pacs_sm = pipe_dtc_sm.score(X_val_sm, y_val_sm)*100\nprint('\\nDecision Tree Classifier -')\nprint('   Pipeline Accuracy Score = ', dt_pacs_sm)\ndt_clf_report_sm = classification_report(y_val_sm, y_pred_sm_dt, output_dict=True)\ncm_dt_sm = confusion_matrix(y_val_sm, y_pred_sm_dt)\n\nrf = RandomForestClassifier()\nrf.fit(X_train_sm, y_train_sm)\ny_pred_sm_rf = rf.predict(X_val_sm)\npipe_rft_sm = make_pipeline(StandardScaler(), RandomForestClassifier())\npipe_rft_sm.fit(X_train_sm, y_train_sm)\nrf_pacs_sm = pipe_rft_sm.score(X_val_sm, y_val_sm)*100\nprint('\\nRandom Forest Classifier -')\nprint('   Pipeline Accuracy Score = ', rf_pacs_sm)\nrf_clf_report_sm = classification_report(y_val_sm, y_pred_sm_rf, output_dict=True)\ncm_rf_sm = confusion_matrix(y_val_sm, y_pred_sm_rf)","fddac166":"lr = LogisticRegression(max_iter=10000)\nlr.fit(X_train_sm, y_train_sm)\ny_pred_sm_lr = lr.predict(X_val_sm)\npipe_lr_sm = make_pipeline(StandardScaler(), LogisticRegression(class_weight = 'balanced', random_state=101, solver='lbfgs', max_iter=10000))\npipe_lr_sm.fit(X_train_sm, y_train_sm)\nlr_pacs_sm = pipe_lr_sm.score(X_val_sm, y_val_sm)*100\nprint('Logistic Regression -')\nprint('   Pipeline Accuracy Score = ', lr_pacs_sm)\nlogreg_clf_report_sm = classification_report(y_val_sm, y_pred_sm_lr, output_dict=True)\ncm_logreg_sm = confusion_matrix(y_val_sm, y_pred_sm_lr)\n\nknn = KNeighborsClassifier(algorithm='ball_tree')\nknn.fit(X_train_sm, y_train_sm)\ny_pred_sm_knn = knn.predict(X_val_sm)\npipe_knn_sm = make_pipeline(StandardScaler(), KNeighborsClassifier(n_neighbors=12, weights='uniform', algorithm='ball_tree', leaf_size=85, p=1, metric='manhattan'))\npipe_knn_sm.fit(X_train_sm, y_train_sm)\nknn_pacs_sm = pipe_knn_sm.score(X_val_sm, y_val_sm)*100\nprint('\\nK-Nearest Neighbours -')\nprint('   Pipeline Accuracy Score = ', knn_pacs_sm)\nknn_clf_report_sm = classification_report(y_val_sm, y_pred_sm_knn, output_dict=True)\ncm_knn_sm = confusion_matrix(y_val_sm, y_pred_sm_knn)\n\nsvc = LinearSVC(max_iter=11000)\nsvc.fit(X_train_sm, y_train_sm)\ny_pred_sm_svc = svc.predict(X_val_sm)\npipe_svc_sm = make_pipeline(StandardScaler(), LinearSVC(penalty='l2', loss='squared_hinge', class_weight='balanced', verbose=0, random_state=101))\npipe_svc_sm.fit(X_train_sm, y_train_sm)\nsvc_pacs_sm = pipe_svc_sm.score(X_val_sm, y_val_sm)*100\nprint('\\nSupport Vector Classifier -')\nprint('   Pipeline Accuracy Score = ', svc_pacs_sm)\nsvc_clf_report_sm = classification_report(y_val_sm, y_pred_sm_svc, output_dict=True)\ncm_svc_sm = confusion_matrix(y_val_sm, y_pred_sm_svc)\n\nnb = GaussianNB()\nnb.fit(X_train_sm, y_train_sm)\ny_pred_sm_nb = nb.predict(X_val_sm)\npipe_gnb_sm = make_pipeline(StandardScaler(), GaussianNB())\npipe_gnb_sm.fit(X_train_sm, y_train_sm)\nnb_pacs_sm = pipe_gnb_sm.score(X_val_sm, y_val_sm)*100\nprint('\\nGaussian Naive Bayes -')\nprint('   Pipeline Accuracy Score = ', nb_pacs_sm)\nnb_clf_report_sm = classification_report(y_val_sm, y_pred_sm_nb, output_dict=True)\ncm_nb_sm = confusion_matrix(y_val_sm, y_pred_sm_nb)\n\ndt = DecisionTreeClassifier(criterion = 'gini')\ndt.fit(X_train_sm, y_train_sm)\ny_pred_sm_dt = dt.predict(X_val_sm)\npipe_dtc_sm = make_pipeline(StandardScaler(), DecisionTreeClassifier(criterion='entropy', splitter='best', max_depth=6, min_samples_split=2, min_samples_leaf=2, random_state=101, class_weight='balanced'))\npipe_dtc_sm.fit(X_train_sm, y_train_sm)\ndt_pacs_sm = pipe_dtc_sm.score(X_val_sm, y_val_sm)*100\nprint('\\nDecision Tree Classifier -')\nprint('   Pipeline Accuracy Score = ', dt_pacs_sm)\ndt_clf_report_sm = classification_report(y_val_sm, y_pred_sm_dt, output_dict=True)\ncm_dt_sm = confusion_matrix(y_val_sm, y_pred_sm_dt)\n\nrf = RandomForestClassifier()\nrf.fit(X_train_sm, y_train_sm)\ny_pred_sm_rf = rf.predict(X_val_sm)\npipe_rft_sm = make_pipeline(StandardScaler(), RandomForestClassifier(criterion='entropy', max_depth=10, min_samples_split=2, min_samples_leaf=2, random_state=101, class_weight='balanced'))\npipe_rft_sm.fit(X_train_sm, y_train_sm)\nrf_pacs_sm = pipe_rft_sm.score(X_val_sm, y_val_sm)*100\nprint('\\nRandom Forest Classifier -')\nprint('   Pipeline Accuracy Score = ', rf_pacs_sm)\nrf_clf_report_sm = classification_report(y_val_sm, y_pred_sm_rf, output_dict=True)\ncm_rf_sm = confusion_matrix(y_val_sm, y_pred_sm_rf)","0cac6801":"clf_lgrg_df = pd.DataFrame(logreg_clf_report_sm).transpose().rename_axis('parameters').reset_index()\nclf_knn_df = pd.DataFrame(knn_clf_report_sm).transpose().rename_axis('parameters').reset_index()\nclf_svc_df = pd.DataFrame(svc_clf_report_sm).transpose().rename_axis('parameters').reset_index()\nclf_nb_df = pd.DataFrame(nb_clf_report_sm).transpose().rename_axis('parameters').reset_index()\nclf_dt_df = pd.DataFrame(dt_clf_report_sm).transpose().rename_axis('parameters').reset_index()\nclf_rf_df = pd.DataFrame(rf_clf_report_sm).transpose().rename_axis('parameters').reset_index()\n\nprec0_x = [clf_lgrg_df.iloc[0]['precision'], clf_knn_df.iloc[0]['precision'], clf_svc_df.iloc[0]['precision'], clf_nb_df.iloc[0]['precision'], clf_dt_df.iloc[0]['precision'], clf_rf_df.iloc[0]['precision']]\nprec1_x = [clf_lgrg_df.iloc[1]['precision'], clf_knn_df.iloc[1]['precision'], clf_svc_df.iloc[1]['precision'], clf_nb_df.iloc[1]['precision'], clf_dt_df.iloc[1]['precision'], clf_rf_df.iloc[1]['precision']]\nprecac_x = [clf_lgrg_df.iloc[2]['precision'], clf_knn_df.iloc[2]['precision'], clf_svc_df.iloc[2]['precision'], clf_nb_df.iloc[2]['precision'], clf_dt_df.iloc[2]['precision'], clf_rf_df.iloc[2]['precision']]\nprecma_x = [clf_lgrg_df.iloc[3]['precision'], clf_knn_df.iloc[3]['precision'], clf_svc_df.iloc[3]['precision'], clf_nb_df.iloc[3]['precision'], clf_dt_df.iloc[3]['precision'], clf_rf_df.iloc[3]['precision']]\nprecwe_x = [clf_lgrg_df.iloc[4]['precision'], clf_knn_df.iloc[4]['precision'], clf_svc_df.iloc[4]['precision'], clf_nb_df.iloc[4]['precision'], clf_dt_df.iloc[4]['precision'], clf_rf_df.iloc[4]['precision']]\n\nrec0_x = [clf_lgrg_df.iloc[0]['recall'], clf_knn_df.iloc[0]['recall'], clf_svc_df.iloc[0]['recall'], clf_nb_df.iloc[0]['recall'], clf_dt_df.iloc[0]['recall'], clf_rf_df.iloc[0]['recall']]\nrec1_x = [clf_lgrg_df.iloc[1]['recall'], clf_knn_df.iloc[1]['recall'], clf_svc_df.iloc[1]['recall'], clf_nb_df.iloc[1]['recall'], clf_dt_df.iloc[1]['recall'], clf_rf_df.iloc[1]['recall']]\nrecac_x = [clf_lgrg_df.iloc[2]['recall'], clf_knn_df.iloc[2]['recall'], clf_svc_df.iloc[2]['recall'], clf_nb_df.iloc[2]['recall'], clf_dt_df.iloc[2]['recall'], clf_rf_df.iloc[2]['recall']]\nrecma_x = [clf_lgrg_df.iloc[3]['recall'], clf_knn_df.iloc[3]['recall'], clf_svc_df.iloc[3]['recall'], clf_nb_df.iloc[3]['recall'], clf_dt_df.iloc[3]['recall'], clf_rf_df.iloc[3]['recall']]\nrecwe_x = [clf_lgrg_df.iloc[4]['recall'], clf_knn_df.iloc[4]['recall'], clf_svc_df.iloc[4]['recall'], clf_nb_df.iloc[4]['recall'], clf_dt_df.iloc[4]['recall'], clf_rf_df.iloc[4]['recall']]\n\nf10_x = [clf_lgrg_df.iloc[0]['f1-score'], clf_knn_df.iloc[0]['f1-score'], clf_svc_df.iloc[0]['f1-score'], clf_nb_df.iloc[0]['f1-score'], clf_dt_df.iloc[0]['f1-score'], clf_rf_df.iloc[0]['f1-score']]\nf11_x = [clf_lgrg_df.iloc[1]['f1-score'], clf_knn_df.iloc[1]['f1-score'], clf_svc_df.iloc[1]['f1-score'], clf_nb_df.iloc[1]['f1-score'], clf_dt_df.iloc[1]['f1-score'], clf_rf_df.iloc[1]['f1-score']]\nf1ac_x = [clf_lgrg_df.iloc[2]['f1-score'], clf_knn_df.iloc[2]['f1-score'], clf_svc_df.iloc[2]['f1-score'], clf_nb_df.iloc[2]['f1-score'], clf_dt_df.iloc[2]['f1-score'], clf_rf_df.iloc[2]['f1-score']]\nf1ma_x = [clf_lgrg_df.iloc[3]['f1-score'], clf_knn_df.iloc[3]['f1-score'], clf_svc_df.iloc[3]['f1-score'], clf_nb_df.iloc[3]['f1-score'], clf_dt_df.iloc[3]['f1-score'], clf_rf_df.iloc[3]['f1-score']]\nf1we_x = [clf_lgrg_df.iloc[4]['f1-score'], clf_knn_df.iloc[4]['f1-score'], clf_svc_df.iloc[4]['f1-score'], clf_nb_df.iloc[4]['f1-score'], clf_dt_df.iloc[4]['f1-score'], clf_rf_df.iloc[4]['f1-score']]\n\nsup0_x = [clf_lgrg_df.iloc[0]['support'], clf_knn_df.iloc[0]['support'], clf_svc_df.iloc[0]['support'], clf_nb_df.iloc[0]['support'], clf_dt_df.iloc[0]['support'], clf_rf_df.iloc[0]['support']]\nsup1_x = [clf_lgrg_df.iloc[1]['support'], clf_knn_df.iloc[1]['support'], clf_svc_df.iloc[1]['support'], clf_nb_df.iloc[1]['support'], clf_dt_df.iloc[1]['support'], clf_rf_df.iloc[1]['support']]\nsupac_x = [clf_lgrg_df.iloc[2]['support'], clf_knn_df.iloc[2]['support'], clf_svc_df.iloc[2]['support'], clf_nb_df.iloc[2]['support'], clf_dt_df.iloc[2]['support'], clf_rf_df.iloc[2]['support']]\nsupma_x = [clf_lgrg_df.iloc[3]['support'], clf_knn_df.iloc[3]['support'], clf_svc_df.iloc[3]['support'], clf_nb_df.iloc[3]['support'], clf_dt_df.iloc[3]['support'], clf_rf_df.iloc[3]['support']]\nsupwe_x = [clf_lgrg_df.iloc[4]['support'], clf_knn_df.iloc[4]['support'], clf_svc_df.iloc[4]['support'], clf_nb_df.iloc[4]['support'], clf_dt_df.iloc[4]['support'], clf_rf_df.iloc[4]['support']]\n\nclassifiers = [\"Logistic Regression\", \"K-Nearest Neighbours\", \"Support Vector Classifier\", \"Gaussian Naive-Bayes\", \"Decision Tree Classifier\", \"Random Forest Classifier\"]\n\nfig = plt.figure(figsize=(10,30), dpi=150)\ngs = fig.add_gridspec(19, 3)\ngs.update(wspace=0.2, hspace=0.5)\n\naxs1 = fig.add_subplot(gs[0, 0])\naxs2 = fig.add_subplot(gs[0, 1])\naxs3 = fig.add_subplot(gs[0, 2])\naxs5 = fig.add_subplot(gs[1, 0])\naxs6 = fig.add_subplot(gs[1, 1])\naxs7 = fig.add_subplot(gs[1, 2])\naxs9 = fig.add_subplot(gs[2, 0])\naxs10 = fig.add_subplot(gs[2, 1])\naxs11 = fig.add_subplot(gs[2, 2])\naxs13 = fig.add_subplot(gs[3, 0])\naxs14 = fig.add_subplot(gs[3, 1])\naxs15 = fig.add_subplot(gs[3, 2])\naxs17 = fig.add_subplot(gs[4, 0])\naxs18 = fig.add_subplot(gs[4, 1])\naxs19 = fig.add_subplot(gs[4, 2])\n\n\ncolors_f1 = [0.1, 0.2, 0.3, 0.4]\ncolors_rec = [0.2, 0.4, 0.6, 0.8]\ncolors_prec = [0.5, 0.625, 0.75, 0.875]\n\naxs1.set(title = \"Precision\\n\\nMajority Class\")\nclfplt1 = sns.barplot(x=[i for i in prec0_x], y=classifiers, ax=axs1, color=colors_prec)\nclfplt1.set(xticklabels=[])\n\naxs2.set(title = \"Recall\\n\\nMajority Class\")\nclfplt2 = sns.barplot(x=[i for i in rec0_x], y=classifiers, ax=axs2, color=colors_rec)\nclfplt2.set(xticklabels=[], yticklabels=[])\n\naxs3.set(title = \"F-1 Score\\n\\nMajority Class\")\nclfplt3 = sns.barplot(x=[i for i in f10_x], y=classifiers, ax=axs3, color=colors_f1)\nclfplt3.set(xticklabels=[], yticklabels=[])\n\n\naxs5.set(title = \"Minority Class\")\nclfplt5 = sns.barplot(x=[i for i in prec1_x], y=classifiers, ax=axs5, color=colors_prec)\nclfplt5.set(xticklabels=[])\n\naxs6.set(title = \"Minority Class\")\nclfplt6 = sns.barplot(x=[i for i in rec1_x], y=classifiers, ax=axs6, color=colors_rec)\nclfplt6.set(xticklabels=[], yticklabels=[])\n\naxs7.set(title = \"Minority Class\")\nclfplt7 = sns.barplot(x=[i for i in f11_x], y=classifiers, ax=axs7, color=colors_f1)\nclfplt7.set(xticklabels=[], yticklabels=[])\n\n\naxs9.set(title = \"Accuracy\")\nclfplt9 = sns.barplot(x=[i for i in precac_x], y=classifiers, ax=axs9, color=colors_prec)\nclfplt9.set(xticklabels=[])\n\naxs10.set(title = \"Accuracy\")\nclfplt10 = sns.barplot(x=[i for i in recac_x], y=classifiers, ax=axs10, color=colors_rec)\nclfplt10.set(xticklabels=[], yticklabels=[])\n\naxs11.set(title = \"Accuracy\")\nclfplt11 = sns.barplot(x=[i for i in f1ac_x], y=classifiers, ax=axs11, color=colors_f1)\nclfplt11.set(xticklabels=[], yticklabels=[])\n\n\naxs13.set(title = \"Macro Average\")\nclfplt13 = sns.barplot(x=[i for i in precma_x], y=classifiers, ax=axs13, color=colors_prec)\nclfplt13.set(xticklabels=[])\n\naxs14.set(title = \"Macro Average\")\nclfplt14 = sns.barplot(x=[i for i in recma_x], y=classifiers, ax=axs14, color=colors_rec)\nclfplt14.set(xticklabels=[], yticklabels=[])\n\naxs15.set(title = \"Macro Average\")\nclfplt15 = sns.barplot(x=[i for i in f1ma_x], y=classifiers, ax=axs15, color=colors_f1)\nclfplt15.set(xticklabels=[], yticklabels=[])\n\n\naxs17.set(title = \"Weighted Average\")\nclfplt17 = sns.barplot(x=[i for i in precwe_x], y=classifiers, ax=axs17, color=colors_prec)\nclfplt17.set()\n\naxs18.set(title = \"Weighted Average\")\nclfplt18 = sns.barplot(x=[i for i in recwe_x], y=classifiers, ax=axs18, color=colors_rec)\nclfplt18.set(yticklabels=[])\n\naxs19.set(title = \"Weighted Average\")\nclfplt19 = sns.barplot(x=[i for i in f1we_x], y=classifiers, ax=axs19, color=colors_f1)\nclfplt19.set(yticklabels=[])\n\nsns.set_theme(style=\"whitegrid\")\nsns.despine(bottom=True, right=True)\nmpl.rcParams['xtick.labelsize'] = 6\nmpl.rcParams['ytick.labelsize'] = 7\nplt.show()","2ee50acc":"fig = plt.figure(figsize=(10,15), dpi=150)\ngs = fig.add_gridspec(5, 2)\ngs.update(wspace=0.1, hspace=0.5)\n\nax1 = fig.add_subplot(gs[0, 0])\nax2 = fig.add_subplot(gs[0, 1])\nax3 = fig.add_subplot(gs[1, 0])\nax4 = fig.add_subplot(gs[1, 1])\nax5 = fig.add_subplot(gs[2, 0])\nax6 = fig.add_subplot(gs[2, 1])\n\ncolors = [\"black\", \"lightgrey\"]\ncolormap = mpl.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nax1.set(title = \"Logistic Regression\")\nsns.heatmap(cm_logreg_sm, linewidths=2.5, yticklabels=['Actual Non-Job Seeker','Actual Job Seeker'], xticklabels=['Predicted Non-Job Seeker','Predicted Job Seeker'], cmap=colormap, cbar=None, annot=True, fmt='d',ax=ax1, annot_kws={\"fontsize\":10})\nax2.set(title = \"K-Nearest Neighbours\")\nsns.heatmap(cm_knn_sm, linewidths=2.5, yticklabels=False, xticklabels=['Predicted Non-Job Seeker','Predicted Job Seeker'], cmap=colormap, cbar=None, annot=True,fmt='d',ax=ax2,annot_kws={\"fontsize\":10})\nax3.set(title = \"Support Vector Classifier\")\nsns.heatmap(cm_svc_sm, linewidths=2.5, yticklabels=['Actual Non-Job Seeker','Actual Job Seeker'],xticklabels=['Predicted Non-Job Seeker','Predicted Job Seeker'], cmap=colormap, cbar=None, annot=True, fmt='d', ax=ax3, annot_kws={\"fontsize\":10})\nax4.set(title = \"Gaussian Naive Bayes\")\nsns.heatmap(cm_nb_sm, linewidths=2.5, yticklabels=False, xticklabels=['Predicted Non-Job Seeker','Predicted Job Seeker'], cmap=colormap, cbar=None, annot=True, fmt='d',ax=ax4,annot_kws={\"fontsize\":10})\nax5.set(title = \"Decision Tree Classifier\")\nsns.heatmap(cm_dt_sm, linewidths=2.5, yticklabels=['Actual Non-Job Seeker','Actual Job Seeker'],xticklabels=['Predicted Non-Job Seeker','Predicted Job Seeker'], cmap=colormap, cbar=None, annot=True, fmt='d', ax=ax5, annot_kws={\"fontsize\":10})\nax6.set(title = \"Random Forest Classifier\")\nsns.heatmap(cm_rf_sm, linewidths=2.5, yticklabels=False, xticklabels=['Predicted Non-Job Seeker','Predicted Job Seeker'], cmap=colormap, cbar=None, annot=True, fmt='d',ax=ax6,annot_kws={\"fontsize\":10})\n\nplt.show()","305616d7":"norm = tf.keras.layers.LayerNormalization(epsilon=0.001, center=True, scale=True)\n\nmodel_bin = Sequential()\nmodel_bin.add(Dense(200, activation='relu'))\nmodel_bin.add(Dense(100, activation='relu'))\nmodel_bin.add(Dense(2, activation='softmax'))\nmodel_bin.add(norm)\n    \nmodel_bin.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nmodel_bin.fit(X_train_nn, y_train_nn, validation_data=(X_val_nn, y_val_nn), epochs=50)","e829da75":"lgbm = LGBMClassifier()\nlgbm.fit(X_train, y_train)\n\ny_pred = lgbm.predict(X_val)\npipe_lgbm = make_pipeline(StandardScaler(), LGBMClassifier(objective='binary', num_leaves=10, learning_rate=0.05, max_depth=1, n_estimators=50, boosting_type='goss'))\npipe_lgbm.fit(X_train, y_train)\nlgbm_pacs = pipe_dtc.score(X_val, y_val)*100\nprint('Light Gradient Booster Machine Classifier -')\nprint('   Pipeline Accuracy Score = ', lgbm_pacs)\n\nlgbm_clf_report = classification_report(y_val, y_pred, output_dict=True)\ncm_lgbm = confusion_matrix(y_val, y_pred)","97ebf419":"A **Confusion Matrix** provides a summary of the **predictive results** in a **classification problem**. Correct and incorrect predictions are summarized in a table with their values and broken down by each class.\n\nIt is predominantly an N x N matrix used for evaluating the performance of a classification model, where N is the number of target classes. The matrix compares the actual target values with those predicted by the machine learning model. This gives us a holistic view of how well our classification model is performing and what kinds of errors it is making\n\nWe can not rely on a single value of accuracy in classification when the classes are imbalanced. ","1b294875":"### Importing Libraries","2002875a":"### Treating Null Values","c0b26fb4":"#### We will be doing the following things in this notebook - \n\n* **Predicting the probability of a prospective candidate to apply for a Data Science job**\n* **Improving our Predictions for a better result**","affc33a9":"Variables that are **measured at different scales** do **not** contribute equally to the **model fitting** & **model learned function** and might end up **creating a bias**. \nThus, to deal with this potential problem **feature-wise** **standardized (\u03bc=0, \u03c3=1)** is usually used **prior to model fitting**.\n\nThe main idea is to **normalize\/standardize i.e. \u03bc = 0 and \u03c3 = 1** your **features\/variables\/columns** of X, individually, before applying any machine learning model. Thus, **StandardScaler()** will **normalize** the features i.e. each column of X, **INDIVIDUALLY** so that each **column\/feature\/variable** will have \u03bc = 0 and \u03c3 = 1\n","9b439d04":"Now that our libraries are imported, let's import our dataset.","02537df7":"#### What do we understand from this?\n\n**True Positive (TP)**\n\n* The predicted value matches the actual value\n* The actual value was positive and the model predicted a positive value\n\n\n**True Negative (TN)** \n\n* The predicted value matches the actual value\n* The actual value was negative and the model predicted a negative value\n\n\n**False Positive (FP) \u2013 Type 1 error**\n\n* The predicted value was falsely predicted\n* The actual value was negative but the model predicted a positive value\n* Also known as the Type 1 error\n\n\n**False Negative (FN) \u2013 Type 2 error**\n\n* The predicted value was falsely predicted\n* The actual value was positive but the model predicted a negative value\n* Also known as the Type 2 error","14fcfbdb":"This is the part where we use relevant factors to process our data to create prediction models.\n\nFor this, we will use **Classification** techniques, since we have to **categorise people** into job takers and non-takers.\n\nBefore that, we will process our data, as shown below.","29fbe9aa":"## **Neural Networks** with Keras","a627ab28":"### Information Sources\n\n* [Standard Scaler](https:\/\/towardsdatascience.com\/how-and-why-to-standardize-your-data-996926c2c832)\n* [Classification Report](https:\/\/medium.com\/@kohlishivam5522\/understanding-a-classification-report-for-your-machine-learning-model-88815e2ce397)\n* [Confusion Matrix](https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/confusion-matrix-machine-learning\/)","a0d3a777":"#### Let's explore what information each column has to offer\n\n\n* **enrollee_id** : Unique ID for candidate\n\n* **city** : City code\n\n* **city_development_index** : Developement index of the city (scaled)\n\n* **gender** : Gender of candidate\n\n* **relevent_experience** : Relevant experience of candidate\n\n* **enrolled_university** : Type of University course enrolled if any\n\n* **education_level** : Education level of candidate\n\n* **major_discipline** : Education major discipline of candidate\n\n* **experience** : Candidate total experience in years\n\n* **company_size** : No of employees in current employer's company\n\n* **company_type** : Type of current employer\n\n* **last_new_job** : Difference in years between previous job and current job\n\n* **training_hours** : training hours completed\n\n* **target** : 0 \u2013 *Not looking for job change*, 1 \u2013 *Looking for a job change*","31625254":"## **Hyperparameter Tuning**","56c7853d":"### Data Selection and Splitting","791a4959":"Our dataset does not seem to have any null values anymore.\n\nWe went a step further and converted all **numerical columns** to **numerical datatypes**.\n\nWe can now explore our training dataset below.","990094c7":"The classification report visualizer displays the **precision**, **recall**, **F1**, and **support scores** for the model.\n","daf41481":"## Predicting the probability of a prospective candidate to apply for a Data Science job","07c34d89":"### Algorithm 5 - Decision Tree Classification\n\n<br>\n\n[Learn more here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html)","b1bdd66d":"### Is there another way?\n\nIn any case, the simplest way to **avoid the disparity** in target data value is to **fill more** of the **minority values**. In order to do that, we implement a technique called **oversampling**.\n\n**Oversampling** is a way of duplicating the minority class values without disrupting the remainder of the dataset. **Simply put**, we will be adding more values of the target **\"1\"** value (**those who are job seekers**).\n\nWe could also **undersample** our dataset, which is the process of **removing members of the majority class values** to **reduce the disparity ratio** between the two classes, but in doing so we will be **losing key information and factors** that would otherwise be essential to the prediction process. ","0bc113ab":"### What oversampling technique should we use?\n\n**Oversampling** in general just **replicates data to reduce the ratio**. However, if we want a **higher accuracy** from our prediction model, we have to **enter relevant data**. ","a4dced82":"### **SMOTE (Synthetic Minority Oversampling Technique)**\n\n**SMOTE** or **Synthetic Minority Oversampling Technique** is an oversampling technique but SMOTE works differently from your typical oversampling.\n\n**SMOTE** works by **utilizing a k-nearest neighbor algorithm** to create **synthetic data**. \n\n**SMOTE** first start by **choosing random data** from the **minority class**, then **k-nearest neighbors from the data are set**. Synthetic data would then be made **between the random data** and **the randomly selected k-nearest neighbor**.","26ab60bd":"### Checking for Null Values","2a6ab7a7":"### Algorithm 2 - K-Nearest Neighbours Classification\n\n<br>\n\n[Learn more here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html)","34a1f38c":"### Algorithm 4 - Gaussian Naive Bayes\n\n<br>\n\n[Learn more here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.GaussianNB.html)","8a1a386d":"### **Classification Report**","9e71f0bf":"We have our dataset. Couple of things we need to understand before we proceed further.","f777e31b":"### **Confusion Matrix**","8aedd387":"### Algorithm 3 - Support Vector Classification\n\n<br>\n\n[Learn more here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html)","23c20e44":"### Algorithm 1 - Logistic Regression\n\n<br>\n\n[Learn more here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)","12a3ead7":"# Data Science - Job Hunters","18f17140":"### Classification Model Scores","91dc6a07":"### Importing Datasets","7e551caa":"There are a couple of columns that have null values. \n\nNormally, this would be resolved by either simply **dropping the columns** with a higher number of null values, or complicating our work with an algorithm like **Nearest Neighbours**.\n\nIn this case, however, we will treat the null values as their own entity and designate them with a numerical value, or a string value such as **Unspecified**.","1a38e1a2":"## **Light GBM Classifier**","75a69077":"### What do we learn?","e22a6b43":"## Understanding our Data","faabe8f0":"\nThere are four ways to check if the predictions are right or wrong:\n\n* **TN \/ True Negative** : the case was **negative** and **predicted negative**\n* **TP \/ True Positive** : the case was **positive** and **predicted positive**\n* **FN \/ False Negative** : the case was **positive** but **predicted negative**\n* **FP \/ False Positive** : the case was **negative** but **predicted positive**\n\n<br>\n\n#### **Precision** \u2014 ***What percent of predictions were correct?***\n\n**Precision** is the ability of a classifier **not to label an instance positive that is actually negative**. For each class, it is defined as **the ratio of true positives** to **the sum of a true positive and false positive**.\n\n\n**Precision = TP\/(TP + FP)**\n\n<br>\n\n#### **Recall** \u2014 ***What percent of the positive cases were caught?***\n\n**Recall** is the ability of a classifier to **find all positive instances**. For each class it is defined as **the ratio of true positives** to the **sum of true positives** and **false negatives**.\n\n\n**Recall = TP\/(TP+FN)**\n\n<br>\n\n#### **F1 score** \u2014 ***What percent of positive predictions were correct?***\n\nThe **F1 score** is a **weighted harmonic mean** of precision and recall such that the **best score is 1.0** and the **worst score is 0.0**. F1 scores are **lower than accuracy measures** as they **embed precision** and recall into their computation.\n\n\n**F1 Score = 2*(Recall * Precision) \/ (Recall + Precision)**\n\n<br>\n\n#### **Support**\n\n\n**Support** is the number of **actual occurrences** of the class in the specified dataset. **Imbalanced support** in the training data may indicate **structural weaknesses** in the **reported scores** of the classifier and could indicate the need for **stratified sampling** or **rebalancing**. Support **doesn\u2019t change** between models but instead **diagnoses the evaluation process**.","71f6eea3":"Let's however first start with understanding what we are working with. For that, we first need to import our datasets.","ae9866aa":"#### The ***train_test_split***\n\nThe train-test split is a technique for evaluating the performance of a machine learning algorithm.\n\nIt can be used for classification or regression problems and can be used for any supervised learning algorithm.\n\nThe procedure involves taking a dataset and dividing it into two subsets. The first subset is used to fit the model and is referred to as the training dataset. The second subset is not used to train the model; instead, the input element of the dataset is provided to the model, then predictions are made and compared to the expected values. This second dataset is referred to as the test dataset.\n\n* **Train Dataset** : Used to fit the machine learning model.\n* **Test Dataset** : Used to evaluate the fit machine learning model.\n\nThe objective is to estimate the performance of the machine learning model on new data: data not used to train the model.","ea6bec05":"#### The ***LabelEncoder***\n\nMachine Learning algorithms understand the **numbers** and **not texts**. Hence, all the **text values** must be **converted** into **numerical values** to make it understandable for the algorithm.\n\n**Categorical variables** represent types of data which may be **divided into groups**. It has a limited and usually **fixed number** of possible values called **categories**. But, if this data is encoded into **numerical values** then only it can be processed in a machine learning algorithm.\n\n**Label encoding** is a simple and straight forward approach. This **converts** each value in a **categorical column** into a **numerical value**. Each value in a categorical column is called ***Label***.","08c5e2e0":"### Algorithm 6 - Random Forest Classification\n\n<br>\n\n[Learn more here](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html)","61f64e31":"### Applying Standard Scaler"}}