{"cell_type":{"07b2b278":"code","e59887b1":"code","d4d60e65":"code","7cf37853":"code","d45c3caa":"code","2af412bc":"code","82715253":"code","43924d26":"code","e4cf8f47":"code","7a0344db":"code","622adbef":"code","0067e7c4":"code","b9e65e5f":"code","ce5282e9":"code","d8f706c5":"code","6a6a4373":"code","7996fb84":"code","1b8424c1":"code","740183c9":"code","afa3c57c":"code","f1c419fc":"code","97987845":"code","ad7b8210":"code","277a6804":"code","699c2cd5":"code","fb2e1753":"code","a057a202":"code","f4f3582d":"code","7cae683c":"code","0f166a05":"code","a44280cf":"code","54d8e2f7":"code","69411e72":"code","62918509":"code","64759b24":"code","56f95cd8":"code","826c91f5":"code","8e05ae3f":"code","a0ed9cec":"code","91cc91f1":"code","baef1a6a":"code","726f5d04":"code","44c40723":"code","9125546a":"code","9b63beb6":"code","f5c2cac6":"code","2466b435":"code","eb56f44d":"code","7ee9a970":"code","7dc34b5c":"code","85e8ba37":"code","fafdc496":"code","49be760d":"code","a1cc6e47":"code","27f6ca35":"code","6bf4295f":"code","85c43573":"code","88776963":"code","eab6a6df":"code","2fcbfa45":"code","e5c6d0bc":"code","ac98570b":"code","9d7edd96":"code","4868fc64":"code","6b5e48f9":"markdown","42b6c238":"markdown","f95d173b":"markdown","e0c619fd":"markdown","d9a29ba7":"markdown","73f68b03":"markdown","a4e51dcc":"markdown","4943697c":"markdown","6264f796":"markdown","6b296c9a":"markdown","1eb953d4":"markdown","594355c4":"markdown","223b7733":"markdown","209f0cdd":"markdown","781da15a":"markdown","74b6c37c":"markdown","3d5d6a5c":"markdown","503bd650":"markdown"},"source":{"07b2b278":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport soundfile as sf\nimport librosa\nimport librosa.display\nimport IPython.display as display\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport IPython.display as ipd\nimport librosa\nimport librosa.display\nimport os\nfrom tqdm import tqdm\nimport sklearn\nimport seaborn as sns\nimport plotly.express as px\n\n\nimport geopandas as gpd\nfrom shapely.geometry import Point, Polygon\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.utils import Sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv1D, MaxPool1D, BatchNormalization\nfrom keras.optimizers import RMSprop,Adam\nfrom keras.applications import VGG19, VGG16, ResNet50\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e59887b1":"path = '\/kaggle\/input\/birdclef-2021\/'\nos.listdir(path)","d4d60e65":"def read_ogg_file(path, file):\n    \"\"\" Read ogg audio file and return numpay array and samplerate\"\"\"\n    \n    data, samplerate = sf.read(path+file)\n    return data, samplerate\n\n\ndef plot_audio_file(data, samplerate):\n    \"\"\" Plot the audio data\"\"\"\n    \n    sr = samplerate\n    fig = plt.figure(figsize=(8, 4))\n    x = range(len(data))\n    y = data\n    plt.plot(x, y)\n    plt.plot(x, y, color='red')\n    plt.legend(loc='upper center')\n    plt.grid()\n    \n    \ndef plot_spectrogram(data, samplerate):\n    \"\"\" Plot spectrogram with mel scaling \"\"\"\n    \n    sr = samplerate\n    spectrogram = librosa.feature.melspectrogram(data, sr=sr)\n    log_spectrogram = librosa.power_to_db(spectrogram, ref=np.max)\n    librosa.display.specshow(log_spectrogram, sr=sr, x_axis='time', y_axis='mel')","7cf37853":"train_labels = pd.read_csv(path + 'train_soundscape_labels.csv')\ntrain_meta = pd.read_csv(path + 'train_metadata.csv')\ntest_data = pd.read_csv(path + 'test.csv')\nsamp_subm = pd.read_csv(path + 'sample_submission.csv')","d45c3caa":"print('Number train label samples:', len(train_labels))\nprint('Number train meta samples:', len(train_meta))\nprint('Number train short folder:', len(os.listdir(path+'train_short_audio')))\nprint('Number train audios:', len(os.listdir(path+'train_soundscapes')))\nprint('Number test samples:', len(test_data))","2af412bc":"os.listdir(path + 'train_short_audio\/caltow')[:2]","82715253":"train_labels.head()","43924d26":"train_meta.head()","e4cf8f47":"print(f\"Training Dataset Shape: {train_meta.shape}\")\nprint(f\"Training Dataset Labels Shape: {train_labels.shape}\")","7a0344db":"print(\"Data: train\")\nprint(\"-----------\")\nfor col in train_meta.columns:\n    print(col + \":\" + str(len(train_meta[col].unique())))\n\nprint(\"\\nData: train_labels\")\nprint(\"-----------\")\nfor col in train_labels.columns:\n    print(col + \":\" + str(len(train_labels[col].unique())))","622adbef":"train_meta['year'] = train_meta['date'].apply(lambda x: x.split(\"-\")[0])\ntrain_meta['month'] = train_meta['date'].apply(lambda x: x.split(\"-\")[1])\nplt.figure(figsize=(16, 6))\nax = sns.countplot(train_meta['year'].sort_values(ascending=False), palette=\"hls\")\n\nplt.title(\"Audio Files Registration per Year Made\", fontsize=16)\nplt.xticks(rotation=70, fontsize=13)\nplt.yticks(fontsize=13)\nplt.ylabel(\"Frequency\", fontsize=14)\nplt.xlabel(\"\");","0067e7c4":"plt.figure(figsize=(16, 6))\nax = sns.countplot(train_meta['month'].sort_values(ascending=False), palette=\"hls\")\n\nplt.title(\"Audio Files Registration per Month Made\", fontsize=16)\nplt.xticks(fontsize=13)\nplt.yticks(fontsize=13)\nplt.ylabel(\"Frequency\", fontsize=14)\nplt.xlabel(\"\");","b9e65e5f":"row = 1000\ntrain_meta.iloc[row]","ce5282e9":"label = train_meta.loc[row, 'primary_label']\nfilename = train_meta.loc[row, 'filename']\n\n# Check if the file is in the folder\nfilename in os.listdir(path+'train_short_audio\/' + label)","d8f706c5":"filename = f'..\/input\/birdclef-2021\/train_short_audio\/{label}\/{filename}'\nfilename","6a6a4373":"plt.figure(figsize=(18, 5))\n\n# by default librosa.load returns a sample rate of 22050\n# librosa converts input to mono, hence always \ndata, sample_rate = librosa.load(filename)\nlibrosa.display.waveplot(data, sr=sample_rate)\nprint(\"Sample Rate: \", sample_rate)\nipd.Audio(filename)","7996fb84":"plt.figure(figsize=(18, 5))\nX = librosa.stft(data)\nXdb = librosa.amplitude_to_db(abs(X))\nlibrosa.display.specshow(Xdb, sr=sample_rate, x_axis='time', y_axis='hz')\nplt.colorbar();","1b8424c1":"spectral_centroids = librosa.feature.spectral_centroid(data, sr=sample_rate)[0]\nplt.figure(figsize=(25, 9))\nframes = range(len(spectral_centroids))\nt = librosa.frames_to_time(frames)\n\n# Normalising the spectral centroid for visualisation\ndef normalize(x, axis=0):\n    return sklearn.preprocessing.minmax_scale(x, axis=axis)\n\n#Plotting the Spectral Centroid along the waveform\nlibrosa.display.waveplot(data, sr=sample_rate, alpha=0.4)\nplt.plot(t, normalize(spectral_centroids), color='b');","740183c9":"plt.figure(figsize=(25, 9))\nspectral_rolloff = librosa.feature.spectral_rolloff(data+0.01, sr=sample_rate)[0]\nlibrosa.display.waveplot(data, sr=sample_rate, alpha=0.4)\nplt.plot(t, normalize(spectral_rolloff), color='r');","afa3c57c":"spectral_bandwidth_2 = librosa.feature.spectral_bandwidth(data+0.01, sr=sample_rate)[0]\nspectral_bandwidth_3 = librosa.feature.spectral_bandwidth(data+0.01, sr=sample_rate, p=3)[0]\nspectral_bandwidth_4 = librosa.feature.spectral_bandwidth(data+0.01, sr=sample_rate, p=4)[0]\nplt.figure(figsize=(25, 9))\nlibrosa.display.waveplot(data, sr=sample_rate, alpha=0.4)\nplt.plot(t, normalize(spectral_bandwidth_2), color='r')\nplt.plot(t, normalize(spectral_bandwidth_3), color='g')\nplt.plot(t, normalize(spectral_bandwidth_4), color='y')\nplt.legend(('p = 2', 'p = 3', 'p = 4'));  # p: order of spectral bandwidth","f1c419fc":"#Plot the signal:\nplt.figure(figsize=(25, 9))\n# librosa.display.waveplot(data, sr=sample_rate)\n# Zooming in\nn0 = 9000\nn1 = 9100\n\nplt.plot(data[n0:n1])\nplt.grid()","97987845":"zero_crossings = librosa.zero_crossings(data[n0:n1], pad=False)\nprint(sum(zero_crossings))","ad7b8210":"mfccs = librosa.feature.mfcc(data, sr=sample_rate)\n\n#Displaying  the MFCCs:\nplt.figure(figsize=(15, 7))\nlibrosa.display.specshow(mfccs, sr=sample_rate, x_axis='time')\nplt.colorbar();","277a6804":"hop_length=512\nchromagram = librosa.feature.chroma_stft(data, sr=sample_rate, hop_length=hop_length)\nplt.figure(figsize=(20, 8))\nlibrosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=hop_length, cmap='coolwarm')\nplt.colorbar();","699c2cd5":"print(\"There are total {} species\".format(train_meta['primary_label'].nunique()))","fb2e1753":"def plotbar(series, pal):\n    plt.figure(figsize=(20, 9))\n    chart = sns.barplot(x=series.index, y=series.values, edgecolor=(0,0,0), linewidth=2, palette=(pal))\n    chart.set_xticklabels(chart.get_xticklabels(), rotation=45)\n    \n    \nspecies = train_meta['primary_label'].value_counts()[:100]\nplotbar(species, \"Blues_r\") # series, palette","a057a202":"sns.set(rc={'figure.figsize':(20,6)})\nsns.countplot(x='rating', data=train_meta, edgecolor=(0,0,0), linewidth=2, palette=('cubehelix'));","f4f3582d":"authors = train_meta['author'].value_counts()[:10]\nplotbar(authors, \"YlOrBr_r\") # series, palette","7cae683c":"print(\"Common Name\")\ncommon = train_meta['common_name'].value_counts()[:100]\nplotbar(authors, \"light:b_r\") # series, palette","0f166a05":"print(\"Scientific Name- Top-50\")\nscien = train_meta['scientific_name'].value_counts()[:50]\nplotbar(scien, \"Greens_r\") # series, palette","a44280cf":"plt.figure(figsize=(18, 5))\n\n# by default librosa.load returns a sample rate of 22050\n# librosa converts input to mono, hence always \nsig, sample_rate = librosa.load(filename)\nlibrosa.display.waveplot(data, sr=sample_rate)\nprint(\"Sample Rate: \", sample_rate)\nipd.Audio(filename)","54d8e2f7":"# First, compute the spectrogram using the \"short-time Fourier transform\" (stft)\nspec = librosa.stft(sig)\n\n# Scale the amplitudes according to the decibel scale\nspec_db = librosa.amplitude_to_db(spec, ref=np.max)\n\n# Plot the spectrogram\nplt.figure(figsize=(15, 5))\nlibrosa.display.specshow(spec_db, \n                         sr=32000, \n                         x_axis='time', \n                         y_axis='hz', \n                         cmap=plt.get_cmap('viridis'));","69411e72":"print('SPEC SHAPE:', spec_db.shape)","62918509":"train_labels['audio_id'].unique()","64759b24":"train_labels.groupby(by=['audio_id']).count()['birds'][:4]","56f95cd8":"print('original label:', train_labels.loc[458, 'birds'])\nprint('split into list:', train_labels.loc[458, 'birds'].split(' '))","826c91f5":"labels = []\nfor row in train_labels.index:\n    labels.extend(train_labels.loc[row, 'birds'].split(' '))\nlabels = list(set(labels))\n\nprint('Number of unique bird labels:', len(labels))","8e05ae3f":"df_labels_train = pd.DataFrame(index=train_labels.index, columns=labels)\nfor row in train_labels.index:\n    birds = train_labels.loc[row, 'birds'].split(' ')\n    for bird in birds:\n        df_labels_train.loc[row, bird] = 1\ndf_labels_train.fillna(0, inplace=True)\n\n# We set a dummy value for the target label in the test data because we will need for the Data Generator\ntest_data['birds'] = 'nocall'\n\ndf_labels_test = pd.DataFrame(index=test_data.index, columns=labels)\nfor row in test_data.index:\n    birds = test_data.loc[row, 'birds'].split(' ')\n    for bird in birds:\n        df_labels_test.loc[row, bird] = 1\ndf_labels_test.fillna(0, inplace=True)","a0ed9cec":"df_labels_train.sum().sort_values(ascending=False)[:10]","91cc91f1":"train_labels = pd.concat([train_labels, df_labels_train], axis=1)\ntest_data = pd.concat([test_data, df_labels_test], axis=1)","baef1a6a":"file = os.listdir(path + 'train_soundscapes')[0]\nfile","726f5d04":"data, samplerate = read_ogg_file(path + 'train_soundscapes\/', file)","44c40723":"audio_id = file.split('_')[0]\nsite = file.split('_')[1]\nprint('audio_id:', audio_id, ', site:', site)","9125546a":"train_labels[(train_labels['audio_id']==int(audio_id)) & (train_labels['site']==site) & (train_labels['birds']!='nocall')]","9b63beb6":"sub_data = data[int(455\/5)*160000:int(460\/5)*160000]","f5c2cac6":"plt.figure(figsize=(14, 5))\nlibrosa.display.waveplot(sub_data, sr=samplerate)\nplt.grid()\nplt.show()","2466b435":"display.Audio(sub_data, rate=samplerate)","eb56f44d":"data_lenght = 160000\naudio_lenght = 5\nnum_labels = len(labels)","7ee9a970":"batch_size = 16","7dc34b5c":"list_IDs_train, list_IDs_val = train_test_split(list(train_labels.index), test_size=0.33, random_state=2021)\nlist_IDs_test = list(samp_subm.index)","85e8ba37":"class DataGenerator(Sequence):\n    def __init__(self, path, list_IDs, data, batch_size):\n        self.path = path\n        self.list_IDs = list_IDs\n        self.data = data\n        self.batch_size = batch_size\n        self.indexes = np.arange(len(self.list_IDs))\n        \n    def __len__(self):\n        len_ = int(len(self.list_IDs)\/self.batch_size)\n        if len_ * self.batch_size < len(self.list_IDs):\n            len_ += 1\n        return len_\n    \n    def __getitem__(self, index):\n        indexes = self.indexes[index * self.batch_size: (index+1) * self.batch_size]\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n        X, y = self.__data_generation(list_IDs_temp)\n        X = X.reshape((self.batch_size, 100, 1600\/\/2))\n        return X, y\n    \n    def __data_generation(self, list_IDs_temp):\n        X = np.zeros((self.batch_size, data_lenght\/\/2))\n        y = np.zeros((self.batch_size, num_labels))\n        for i, ID in enumerate(list_IDs_temp):\n            prefix = str(self.data.loc[ID, 'audio_id']) + '_' + self.data.loc[ID, 'site']\n            file_list = [s for s in os.listdir(self.path) if prefix in s]\n            if len(file_list) == 0:\n                # Dummy for missing test audio files\n                audio_file_fft = np.zeros((data_lenght\/\/2))\n            else:\n                file = file_list[0]#[s for s in os.listdir(self.path) if prefix in s][0]\n                audio_file, audio_sr = read_ogg_file(self.path, file)\n                audio_file = audio_file[\n                    int((self.data.loc[ID, 'seconds']-5)\/audio_lenght)*data_lenght:\n                    int(self.data.loc[ID, 'seconds']\/audio_lenght)*data_lenght\n                ]\n                audio_file_fft = np.abs(np.fft.fft(audio_file)[: len(audio_file)\/\/2])\n                # scale data\n                audio_file_fft = (audio_file_fft-audio_file_fft.mean())\/audio_file_fft.std()\n            X[i, ] = audio_file_fft\n            y[i, ] = self.data.loc[ID, self.data.columns[5:]].values\n        return X, y","fafdc496":"train_generator = DataGenerator(path+'train_soundscapes\/', list_IDs_train, train_labels, batch_size)\nval_generator = DataGenerator(path+'train_soundscapes\/', list_IDs_val, train_labels, batch_size)\ntest_generator = DataGenerator(path+'test_soundscapes\/', list_IDs_test, test_data, batch_size)","49be760d":"epochs = 2\nlr = 1e-3","a1cc6e47":"num_labels","27f6ca35":"for batch in train_generator:\n    print(batch[0][0][0].shape)\n    print(batch[1][0].shape)\n    break","6bf4295f":"model = Sequential()\nmodel.add(Conv1D(128, input_shape=batch, 1600\/\/2,), kernel_size=5, strides=4, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool1D(pool_size=(4)))\nmodel.add(Conv1D(64, kernel_size=3, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(num_labels, activation='sigmoid'))","85c43573":"model.compile(optimizer = Adam(lr=lr),\n              loss='binary_crossentropy',\n              metrics=['binary_accuracy'])","88776963":"model.summary()","eab6a6df":"# history = model.fit_generator(generator=train_generator,\n#                               validation_data=val_generator,\n#                               epochs=epochs,\n#                               workers=4)","2fcbfa45":"from keras.models import load_model\n\n# model.save('model_1d_conv.h5')\nmodel = load_model('..\/input\/bird-model-conv1d\/model_1d_conv.h5')\nmodel.summary()","e5c6d0bc":"y_pred = model.predict_generator(test_generator, verbose=1)","ac98570b":"y_test = np.where(y_pred > 0.5, 1, 0)","9d7edd96":"for row in samp_subm.index:\n    string = ''\n    for col in range(len(y_test[row])):\n        if y_test[row][col] == 1:\n            if string == '':\n                string += labels[col]\n            else:\n                string += ' ' + labels[col]\n    if string == '':\n        string = 'nocall'\n    samp_subm.loc[row, 'birds'] = string","4868fc64":"output = samp_subm\noutput.to_csv('submission.csv', index=False)","6b5e48f9":"## Spectral Centroid\nThe spectral centroid is a measure used in digital signal processing to characterise a spectrum. It indicates where the center of mass of the spectrum is located. ","42b6c238":"## Spectral bandwidth\nThe spectral bandwidth is defined as the width of the band of light at one-half the peak maximum (or full width at half maximum [FWHM]) and is represented by the two vertical red lines and \u03bbSB on the wavelength axis.","f95d173b":"## Chrome features\nA chroma feature or vector is typically a 12-element feature vector indicating how much energy of each pitch class, {C, C#, D, D#, E, \u2026, B}, is present in the signal.","e0c619fd":"## Zero-Crosing Rate\nThe zero-crossing rate (ZCR) is the rate at which a signal changes from positive to zero to negative or from negative to zero to positive.","d9a29ba7":"There are only few files with low ratings","73f68b03":"Column-wise unique values","a4e51dcc":"## Model 1 - 1D CNN Neural Network","4943697c":"Data Size","6264f796":"## Top 100","6b296c9a":"## Mel-Frequency Cepstral Coefficients (MFCCs)\nThe Mel frequency cepstral coefficients (MFCCs) of a signal are a small set of features (usually about 10\u201320) which concisely describe the overall shape of a spectral envelope.","1eb953d4":"## Spectral Rolloff\nIt is a measure of the shape of the signal. It represents the frequency at which high frequencies decline to 0.","594355c4":"## Spectrogram\nA spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time.","223b7733":"In the past that convolutional neural networks (CNN) perform particularly well for sound classification. But CNN need 2D inputs. Luckily, we can transform an audio signal into a 2D representation: a so-called spectrogram.","209f0cdd":"Time of the Recording","781da15a":"## Top 100 training samples per species","74b6c37c":"Visualizing the audio","3d5d6a5c":"Richard E. Webster is author having maximum file entries.","503bd650":"## EDA"}}