{"cell_type":{"b883bf64":"code","55938468":"code","0fdc34e5":"code","ca39e9c9":"code","6125ffc4":"code","95b919a2":"code","2bf3e9c7":"code","66f9e961":"code","4b6064e4":"code","833cc4a1":"code","05a66910":"code","82200c5d":"code","7774818a":"code","97633aa6":"code","2f1aaa29":"code","8f6689ea":"code","e56b0b85":"code","aab91d57":"code","84cca206":"code","23d9e2e6":"code","b4816b32":"code","f8d21359":"code","155fc0d6":"code","137ae735":"code","5c6842ea":"code","9f239fa9":"markdown","1a363683":"markdown","614f429c":"markdown","9b681081":"markdown","d5c94cb9":"markdown","d5fefcd8":"markdown","b40b0b2a":"markdown"},"source":{"b883bf64":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","55938468":"import pandas as pd\nimport numpy as np\nfrom IPython.display import display\nimport random\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nimport time\n# for visualizing\n\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nimport seaborn as sns\nimport plotly.express as px\n\n# for adding extra statistical stuff\n\nfrom scipy.stats import skew, norm","0fdc34e5":"train_feat = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_target = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\n\ntest_feat = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\n","ca39e9c9":"print('Train Feature Samples:')\ntrain_feat.head(3)","6125ffc4":"print('Test Feature Samples:')\ntest_feat.head(3)","95b919a2":"print('Number of rows in training set: ', train_feat.shape[0])\nprint('Number of columns in training set: ', train_feat.shape[1])\nprint('Number of rows in test set: ', test_feat.shape[0])\nprint('Number of columns in test set: ', test_feat.shape[1])","2bf3e9c7":"train_miss=train_feat.isnull().sum().sum()\ntest_miss=train_feat.isnull().sum().sum()\nprint('Number of Null values in training set: ',train_miss)\nprint('Number of Null values in training set: ',train_miss)","66f9e961":"train_feat.info()","4b6064e4":"plt.style.use('ggplot')\n\nfig = plt.figure(constrained_layout=True, figsize=(20, 12))\n\n\ngrid = gridspec.GridSpec(ncols=6, nrows=3, figure=fig)\n\nax1 = fig.add_subplot(grid[0, :3])\n\nax1.set_title(f'Train cp_type Distribution',weight='bold')\n\nsns.countplot(x='cp_type',\n                    data=train_feat,\n                    palette=\"rocket\",\n                    ax=ax1,\n                    order=train_feat['cp_type'].value_counts().index)\n\ntotal = float(len(train_feat['cp_type']))\n\n\nfor p in ax1.patches:\n    height = p.get_height()\n    ax1.text(p.get_x() + p.get_width() \/ 2.,\n            height + 2,\n            '{:1.2f}%'.format((height \/ total) * 100),\n            ha='center')\n\n\nax2 = fig.add_subplot(grid[0, 3:])\n\n\n\nsns.countplot(x='cp_type',\n                    data=test_feat,\n                    palette=\"rocket\",\n                    ax=ax2,\n                    order=test_feat['cp_type'].value_counts().index)\n\ntotal = float(len(test_feat['cp_type']))\n\nax2.set_title(f'Test cp_type Distribution', weight='bold')\n\n\nfor p in ax2.patches:\n    height = p.get_height()\n    ax2.text(p.get_x() + p.get_width() \/ 2.,\n            height + 2,\n            '{:1.2f}%'.format((height \/ total) * 100),\n            ha='center')\nax3 = fig.add_subplot(grid[1, :3])\n\nax3.set_title(f'Train cp_time Distribution', weight='bold')\n\nsns.countplot(x='cp_time',\n                    data=train_feat,\n                    palette=\"rocket\",\n                    ax=ax3,\n                    order=train_feat['cp_time'].value_counts().index)\n\ntotal = float(len(train_feat['cp_time']))\n\n\nfor p in ax3.patches:\n    height = p.get_height()\n    ax3.text(p.get_x() + p.get_width() \/ 2.,\n            height + 2,\n            '{:1.2f}%'.format((height \/ total) * 100),\n            ha='center')\n\nax4 = fig.add_subplot(grid[1, 3:])\n\nax4.set_title(f'Test cp_time Distribution', weight='bold')\n\nsns.countplot(x='cp_time',\n                    data=test_feat,\n                    palette=\"rocket\",\n                    ax=ax4,\n                    order=train_feat['cp_time'].value_counts().index)\n\ntotal = float(len(test_feat['cp_time']))\n\n\nfor p in ax4.patches:\n    height = p.get_height()\n    ax4.text(p.get_x() + p.get_width() \/ 2.,\n            height + 2,\n            '{:1.2f}%'.format((height \/ total) * 100),\n            ha='center')\n    \nax5 = fig.add_subplot(grid[2, :3])\n\nax5.set_title(f'Train cp_dose Distribution', weight='bold')\n\nsns.countplot(x='cp_dose',\n                    data=train_feat,\n                    palette=\"rocket\",\n                    ax=ax5,\n                    order=train_feat['cp_dose'].value_counts().index)\n\ntotal = float(len(train_feat['cp_dose']))\n\n\nfor p in ax5.patches:\n    height = p.get_height()\n    ax5.text(p.get_x() + p.get_width() \/ 2.,\n            height + 2,\n            '{:1.2f}%'.format((height \/ total) * 100),\n            ha='center')\n\nax6 = fig.add_subplot(grid[2, 3:])\n\nax6.set_title(f'Test cp_dose Distribution', weight='bold')\n\nsns.countplot(x='cp_dose',\n                    data=test_feat,\n                    palette=\"rocket\",\n                    ax=ax6,\n                    order=train_feat['cp_dose'].value_counts().index)\n\ntotal = float(len(test_feat['cp_dose']))\n\n\nfor p in ax6.patches:\n    height = p.get_height()\n    ax6.text(p.get_x() + p.get_width() \/ 2.,\n            height + 2,\n            '{:1.2f}%'.format((height \/ total) * 100),\n            ha='center')","833cc4a1":"def plot_distplot(f1):\n    plt.style.use('seaborn')\n    sns.set_style('whitegrid')\n\n    fig= plt.figure(figsize=(20,20))\n    #2 rows 2 cols\n    #first row, first col\n    for i in range(0,12):\n        ax1 = plt.subplot2grid((5,4),((i \/\/ 3) + 1, (i % 3) + 1))\n        sns.distplot(train_feat[f1])\n        plt.title(f1[i],weight='bold', fontsize=12)\n        plt.yticks(weight='bold')\n        plt.xticks(weight='bold')\n    return plt.show()","05a66910":"train_columns = train_feat.columns.to_list()\ng_list = [i for i in train_columns if i.startswith('g-')]\nc_list = [i for i in train_columns if i.startswith('c-')]","82200c5d":"c_plot= [c_list[random.randint(0, len(c_list)-1)] for i in range(50)]\nplot_distplot(c_plot[:12])","7774818a":"cells=train_feat[c_list]\n#Plot heatmap\nplt.figure(figsize=(25,12))\nsns.heatmap(cells.corr(), cmap='coolwarm', alpha=0.9)\nplt.title('Correlation: Cell', fontsize=15, weight='bold')\nplt.xticks(weight='bold')\nplt.yticks(weight='bold')\nplt.show()","97633aa6":"correlations = cells.corr().abs().unstack().sort_values(kind=\"quicksort\",ascending=False).reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']] #preventing 1.0 corr\ncorr_max=correlations.level_0.head(150).tolist()\ncorr_max=list(set(corr_max)) #removing duplicates\n\ncorr_min=correlations.level_0.tail(50).tolist()\ncorr_min=list(set(corr_min)) #removing duplicates","2f1aaa29":"correlations.head()","8f6689ea":"correlations.tail()","e56b0b85":"g_plot= [g_list[random.randint(0, len(g_list)-1)] for i in range(50)]\nplot_distplot(g_plot[:12])","aab91d57":"geans=train_feat[g_list]\n#Plot heatmap\nplt.figure(figsize=(25,12))\nsns.heatmap(geans.corr(), cmap='coolwarm', alpha=0.9)\nplt.title('Correlation: Cell', fontsize=15, weight='bold')\nplt.xticks(weight='bold')\nplt.yticks(weight='bold')\nplt.show()","84cca206":"correlations = geans.corr().abs().unstack().sort_values(kind=\"quicksort\",ascending=False).reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']] #preventing 1.0 corr\ncorr_max=correlations.level_0.head(150).tolist()\ncorr_max=list(set(corr_max)) #removing duplicates\n\ncorr_min=correlations.level_0.tail(50).tolist()\ncorr_min=list(set(corr_min)) #removing duplicates","23d9e2e6":"correlations.head()","b4816b32":"correlations.tail()","f8d21359":"print('Train Target Samples:')\ndisplay(train_target.head(3))","155fc0d6":"train_target.describe()","137ae735":"x = train_target.drop(['sig_id'], axis=1).sum(axis=0).sort_values().reset_index()\nx.columns = ['column', 'nonzero_records']\n\nfig = px.bar(\n    x.tail(50), \n    x='nonzero_records', \n    y='column',\n    title='Columns with the higher number of positive samples (top 50)', \n    height=1000, \n    width=800,\n    color='nonzero_records'\n)\n\nfig.show()","5c6842ea":"fig = px.bar(\n    x.head(50), \n    x='nonzero_records', \n    y='column', \n    title='Columns with the lowest number of positive samples (top 50)', \n    height=1000, \n    width=800,\n    color='nonzero_records'\n)\n\nfig.show()","9f239fa9":"# **Target Analysis**","1a363683":"# Loading and Exploring the data","614f429c":"# Mechanisms of Action (MoA) Prediction.(EDA)\nIn this competition, we are suposed to develop algorithms and train models **to determine the mechanism of action of a new drug based on the gene expression and cell viability information**. In this EDA, we will try to find patterns in the data, interactions between the drugs in both scored and nonscored datasets and the relationship between drugs and their target genes.","9b681081":"3. **Gean Analysis**","d5c94cb9":"2. **Cell Analysis**","d5fefcd8":"# Feature data analysis","b40b0b2a":"1. check whether train and test feature data are balanced?"}}