{"cell_type":{"1a341f2e":"code","571ad30a":"code","7baff26d":"code","fb708476":"code","d7947ab4":"code","4755f1dc":"code","0d7deb4a":"code","0a50b933":"code","ae8bfaf7":"code","6baa09e8":"code","87c5a606":"code","cbe21151":"code","8515b07b":"code","3bbe5bf5":"code","8a292b30":"code","06d7feb1":"code","9881cdea":"code","2b84b92c":"code","ea1630d2":"code","07a9a318":"code","6b2f6d47":"code","0a79213a":"code","e70f44c1":"code","86e9aff8":"code","e6adb0c0":"code","aba1d572":"code","7214860c":"code","2de6de3a":"code","fca66a42":"code","309f0190":"code","b79049b6":"code","fbebde4d":"code","b3619162":"code","ba551833":"code","16f357f8":"code","313fe15f":"code","c75db858":"code","8e0c1ca7":"code","318d61db":"code","5a97a0fc":"code","7a5a5965":"code","9c067b44":"code","a170beff":"code","5399947c":"code","d78adb44":"code","0a5c4f02":"code","e76530b8":"code","39e9ad43":"code","32e5bf7f":"code","cd116ea3":"code","ffec68d7":"code","6d849be2":"code","4b21018d":"code","9bee8c15":"code","ee1a74b3":"code","2dda3387":"code","ff3551b6":"code","71bd5c07":"code","c96a1eb2":"code","3487ac4e":"code","bbe7e746":"code","fa7a2bae":"code","ebac496e":"code","ed3c9cde":"code","460e005d":"code","b9e20a54":"code","d53a88f7":"code","e94bf32c":"markdown","5ce78105":"markdown","ef71740d":"markdown","b109b125":"markdown","598a977e":"markdown","4c1b8875":"markdown","27b5f8cb":"markdown","d257365e":"markdown","cd0ffa24":"markdown","74087ae5":"markdown","909ffcf8":"markdown","5bc43dfb":"markdown","5ab77aa6":"markdown","f8cd0c3e":"markdown","89ce39ec":"markdown","4120a491":"markdown","a0da01f5":"markdown","4ee017e9":"markdown","4dcead23":"markdown","c5a24c4c":"markdown","0360c431":"markdown","c3c44800":"markdown","536ebd8c":"markdown","90e3563d":"markdown","8d0a718d":"markdown","66cc87e2":"markdown","9e01d331":"markdown","72bea688":"markdown","90a15a94":"markdown","13ab2ce7":"markdown"},"source":{"1a341f2e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\nfrom sklearn.model_selection import train_test_split\nimport gc\n\nimport lightgbm as lgb\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\ncolor = sns.color_palette()\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","571ad30a":"train2016_df = pd.read_csv(\"\/kaggle\/input\/zillow-prize-1\/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\ntrain2017_df = pd.read_csv(\"\/kaggle\/input\/zillow-prize-1\/train_2017.csv\", parse_dates=[\"transactiondate\"])\nprint ('train 2016 data has {0} rows and {1} columns'.format(train2016_df.shape[0],train2016_df.shape[1]))\nprint ('----------------------------')\nprint ('train 2017 data has {0} rows and {1} columns'.format(train2017_df.shape[0],train2017_df.shape[1]))","7baff26d":"train2016_df.head()","fb708476":"(train2016_df['parcelid'].value_counts().reset_index())['parcelid'].value_counts()","d7947ab4":"# Look at the distribution of the target variable (log-error)\nprint(train2016_df['logerror'].describe())","4755f1dc":"print('Skewness is', train2016_df['logerror'].skew())\ntarget = train2016_df.loc[abs(train2016_df['logerror']) < 0.4, 'logerror']\nprint('Skewness after tranforms is', target.skew())\nprint('train data has rows', target.shape)\ntarget.hist(bins=40)","0d7deb4a":"# bi\u1ec3u di\u1ec5n th\u00e1ng sales b\u1eb1ng bar plot\ntrain2016_df['transaction_month'] = train2016_df['transactiondate'].dt.month\ncnt_srs = train2016_df['transaction_month'].value_counts()\nplt.figure(figsize=(12,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color=color[3])\nplt.xticks(rotation='vertical')\nplt.xlabel('Month of transaction', fontsize=12)\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.show()\ntrain2016_df.drop(['transaction_month'], axis=1, inplace=True)","0a50b933":"# check the distribution of the target variable logerror 2017\nprint('Skewness is', train2017_df['logerror'].skew())\ntarget = train2017_df.loc[abs(train2017_df['logerror']) < 0.4, 'logerror']\nprint('Skewness after tranforms is', target.skew())\nprint('train data has rows', target.shape)\nsns.distplot(target)","ae8bfaf7":"prop2016_df = pd.read_csv(\"\/kaggle\/input\/zillow-prize-1\/properties_2016.csv\")\nprop2016_df.shape","6baa09e8":"pd.set_option('display.max_columns', None)\npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', -1)\nprop2016_df.head()","87c5a606":"prop2017_df = pd.read_csv(\"\/kaggle\/input\/zillow-prize-1\/properties_2017.csv\")\nprop2017_df.shape","cbe21151":"# merge 2 file\ntrain_2016 = train2016_df.merge(prop2016_df, how='left', on='parcelid')\ntrain_2017 = train2017_df.merge(prop2017_df, how='left', on='parcelid')","8515b07b":"dicts = pd.read_excel('\/kaggle\/input\/zillow-prize-1\/zillow_data_dictionary.xlsx')\ndicts.head(10)","3bbe5bf5":"catvars = ['airconditioningtypeid','architecturalstyletypeid','buildingqualitytypeid','buildingclasstypeid',\n           'decktypeid','fips','hashottuborspa', 'fireplaceflag','heatingorsystemtypeid',\n           'propertycountylandusecode','propertylandusetypeid','propertyzoningdesc','regionidcity',\n           'regionidcounty','regionidneighborhood','regionidzip','storytypeid','typeconstructiontypeid','yearbuilt',\n           'taxdelinquencyflag', 'latitude', 'longitude', 'parcelid', 'assessmentyear']\n\nnumvars = [i for i in prop2016_df.columns if i not in catvars]\nprint (\"C\u00f3 {} numeric v\u00e0 {} categorical columns\".format(len(numvars),len(catvars)))","8a292b30":"corr = prop2016_df[numvars].corr()\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(19, 19))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\ncmap ='coolwarm'\n\n# Draw the heatmap with the mask and correct aspect ratio\nax = sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0, annot=True,\n            square=True, linewidths=.3, cbar_kws={\"shrink\": .5})\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","06d7feb1":"del corr, target, ax, dicts\ngc.collect()\nprint('Memory usage reduction\u2026')","9881cdea":"# create numeric plots\nsns.set(style=\"whitegrid\", color_codes=True)\nnd = pd.melt(train_2016, value_vars = numvars)\nn1 = sns.FacetGrid(nd, col='variable', col_wrap=6, sharex=False, sharey = False)\nn1 = n1.map(sns.distplot, 'value')\nn1","2b84b92c":"del nd, n1\ngc.collect()\nprint('Memory usage reduction\u2026')","ea1630d2":"# Missing value trong t\u1eebng c\u1ed9t\nmissing_df = prop2016_df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df = missing_df.loc[missing_df['missing_count']>0]\nmissing_df = missing_df.sort_values(by='missing_count')\n\nind = np.arange(missing_df.shape[0])\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(12,18))\nrects = ax.barh(ind, missing_df.missing_count.values, color='blue')\nax.set_yticks(ind)\nax.set_yticklabels(missing_df.column_name.values, rotation='horizontal')\nax.set_xlabel(\"Count of missing values\")\nax.set_title(\"Number of missing values in each column\")\nplt.show()","07a9a318":"# miss > 99%\nmissing_df = prop2016_df.isnull().sum(axis=0).reset_index()\nmissing_df.columns = ['column_name', 'missing_count']\nmissing_df['missing_ratio'] = missing_df['missing_count'] \/ prop2016_df.shape[0]\nmissing_df.loc[missing_df['missing_ratio']>0.99]","6b2f6d47":"def poolhottubor_process(property_data):\n    # 0 pools\n    property_data.poolcnt.fillna(0,inplace = True)\n    # 0 hot tubs or spas\n    property_data.hashottuborspa.fillna(0,inplace = True)\n    # Convert \"True\" to 1\n    property_data.hashottuborspa.replace(to_replace = True, value = 1,inplace = True)\n\n    # Set properties that have a pool but no info on poolsize equal to the median poolsize value.\n    property_data.loc[property_data.poolcnt==1, 'poolsizesum'] = property_data.loc[property_data.poolcnt==1, 'poolsizesum'].fillna(property_data[property_data.poolcnt==1].poolsizesum.median())\n    # \"0 pools\" = \"0 sq ft of pools\"\n    property_data.loc[property_data.poolcnt==0, 'poolsizesum']=0\n\n    # \"0 pools with a spa\/hot tub\"\n    property_data.pooltypeid2.fillna(0,inplace = True)\n    # \"0 pools without a hot tub\"\n    property_data.pooltypeid7.fillna(0,inplace = True)\n\n    # Drop redundant feature\n    property_data.drop('pooltypeid10', axis=1, inplace=True)\n    \n    return property_data","0a79213a":"# s\u1ed1 l\u00f2 s\u01b0\u1edfi trung b\u00ecnh \nprint(prop2016_df['fireplacecnt'].value_counts())","e70f44c1":"def fireplace_process(property_data):\n    # fireplaceflag l\u00e0 True v\u00e0 fireplacecnt NaN, ta s\u1ebd thay th\u1ebf b\u1eb1ng trung b\u00ecnh c\u00e1c fireplace.\n    property_data.loc[(property_data['fireplaceflag'] == True) & (property_data['fireplacecnt'].isnull()), ['fireplacecnt']] = 1\n    # If \"fireplacecnt\" is 1 or larger \"fireplaceflag\" is \"NaN\", we will set \"fireplaceflag\" to \"True\".\n    property_data.loc[(property_data['fireplacecnt'] >= 1.0) & (property_data['fireplaceflag'].isnull()), ['fireplaceflag']] = True\n    \n    # Convert \"NaN\" th\u00e0nh 0\n    property_data.fireplaceflag.fillna(0,inplace = True)\n    # Convert \"True\" th\u00e0nh 1\n    property_data.fireplaceflag.replace(to_replace = True, value = 1,inplace = True)\n    \n    # If 'fireplacecnt' is \"NaN\", replace with \"0\"\n    property_data.fireplacecnt.fillna(0,inplace = True)\n    \n    return property_data","86e9aff8":"def garage_process(property_data):\n    property_data.garagecarcnt.fillna(0,inplace = True)\n    property_data.garagetotalsqft.fillna(0,inplace = True)\n    return property_data","e6adb0c0":"def tax_process(property_data):\n    # Replace \"NaN\" with \"0\"\n    property_data.taxdelinquencyflag.fillna(0,inplace = True)\n    # Change \"Y\" to \"1\"\n    property_data.taxdelinquencyflag.replace(to_replace = 'Y', value = 1,inplace = True)\n    \n    \n    property_data.landtaxvaluedollarcnt.fillna(0,inplace = True)\n    property_data.structuretaxvaluedollarcnt.fillna(0,inplace = True)\n\n    property_data['taxvaluedollarcnt'].fillna((property_data['taxvaluedollarcnt'].mean()), inplace=True)\n    \n    # Drop \"regionidcity\"\n    property_data.drop('regionidcity', axis=1, inplace=True)\n    # Fill in \"NaN\" \"yearbuilt\" with most common\n    yearbuilt = property_data['yearbuilt'].value_counts().idxmax()\n    property_data['yearbuilt'] = property_data['yearbuilt'].fillna(yearbuilt)\n    \n    return property_data","aba1d572":"squarefeet = prop2016_df[ prop2016_df['finishedsquarefeet15'].notnull() & prop2016_df['finishedsquarefeet50'].notnull() & prop2016_df['lotsizesquarefeet'].notnull()]\nsquarefeet[['calculatedfinishedsquarefeet','finishedsquarefeet6','finishedsquarefeet12','finishedsquarefeet15','finishedsquarefeet50','numberofstories','lotsizesquarefeet']].sample(10)","7214860c":"def squarefeet_process(property_data):\n    \n    # Drop \"finishedsquarefeet6\"\n    property_data.drop('finishedsquarefeet6', axis=1, inplace=True)\n    # Drop \"finishedsquarefeet12\"\n    property_data.drop('finishedsquarefeet12', axis=1, inplace=True)\n    # Drop \"finishedfloor1squarefeet\"\n    property_data.drop('finishedfloor1squarefeet', axis=1, inplace=True)\n\n    # Replace \"NaN\" \"calculatedfinishedsquarefeet\" values with mean.\n    property_data['calculatedfinishedsquarefeet'].fillna((property_data['calculatedfinishedsquarefeet'].mean()), inplace=True)\n\n    # If \"numberofstories\" is equal to \"1\", then we can replace the \"NaN\"s with the \"calculatedfinishedsquarefeet\" value. Fill in the rest with the average values.\n    property_data.loc[property_data['numberofstories'] == 1.0,'finishedsquarefeet50'] = property_data['calculatedfinishedsquarefeet']\n    property_data['finishedsquarefeet50'].fillna((property_data['finishedsquarefeet50'].mean()), inplace=True)\n\n    # Replace \"NaN\" \"finishedsquarefeet15\" values with calculatedfinishedsquarefeet.\n    property_data.loc[property_data['finishedsquarefeet15'].isnull(),'finishedsquarefeet15'] = property_data['calculatedfinishedsquarefeet']\n    # Replace rest valule \"NaN\" \"finishedsquarefeet15\" values with mean.\n    property_data['finishedsquarefeet15'].fillna((property_data['finishedsquarefeet15'].mean()), inplace=True)\n    # change numberofstories with common value \n    property_data.numberofstories.fillna(1,inplace = True)\n    \n    return property_data","2de6de3a":"plt.figure(figsize=(12,4))\nsns.countplot(x=\"calculatedbathnbr\", data=prop2016_df)\nplt.ylabel('Count', fontsize=8)\nplt.xlabel('Bathroom', fontsize=12)\nplt.title('Frequency of bathroom count', fontsize=15)\nplt.show()\n\n# look at some data example \nbathrooms = prop2016_df[prop2016_df['fullbathcnt'].notnull() & prop2016_df['threequarterbathnbr'].notnull() & prop2016_df['calculatedbathnbr'].notnull()]\nbathrooms[['fullbathcnt','threequarterbathnbr','calculatedbathnbr']].sample(10)","fca66a42":"def bathroom_process(property_data):\n    # Drop \"threequarterbathnbr\"\n    property_data.drop('threequarterbathnbr', axis=1, inplace=True)\n    # Drop \"fullbathcnt\"\n    property_data.drop('fullbathcnt', axis=1, inplace=True)\n\n    # Fill in \"NaN\" \"calculatedbathnbr\" with most common\n    bathroommode = property_data['calculatedbathnbr'].value_counts().idxmax()\n    property_data['calculatedbathnbr'] = property_data['calculatedbathnbr'].fillna(bathroommode)\n    return property_data","309f0190":"# # identify levels of missingness\n# missing = prop2016_df.isnull().sum().sort_values(ascending = False)\n# vartypes = prop2016_df.dtypes\n# missingpercent = (prop2016_df.isnull().sum()\/prop2016_df.shape[0]).sort_values(ascending=False)\n# pd.concat([vartypes, missing, missingpercent], axis = 1,\n#           keys =['var type', 'missing n', 'percent']\n#         ).sort_values(by = 'missing n', ascending = False).head(10)","b79049b6":"def rest_process(property_data):\n    # Drop \"taxdelinquencyyear\"\n    property_data.drop('taxdelinquencyyear', axis=1, inplace=True)\n    # Drop 'basementsqft'\n    property_data.drop('basementsqft', axis=1, inplace = True)\n    # Drop \"storytypeid\"\n    property_data.drop('storytypeid', axis=1, inplace=True)\n    # Drop \"architecturalstyletypeid\"\n    property_data.drop('architecturalstyletypeid', axis=1, inplace=True)\n    # Drop \"typeconstructiontypeid\" and \"finishedsquarefeet13\"\n    property_data.drop('typeconstructiontypeid', axis=1, inplace=True)\n    property_data.drop('finishedsquarefeet13', axis=1, inplace=True)\n    # Drop \"buildingclasstypeid\"\n    property_data.drop('buildingclasstypeid', axis=1, inplace=True)\n    ##------------------------------------------------------------\n    # Replace 'yardbuildingsqft17' \"NaN\"s with \"0\".\n    property_data.yardbuildingsqft17.fillna(0,inplace = True)\n    # Replace 'yardbuildingsqft26' \"NaN\"s with \"0\".\n    property_data.yardbuildingsqft26.fillna(0,inplace = True)\n    # Change \"decktypeid\" \"Nan\"s to \"0\"\n    property_data.decktypeid.fillna(0,inplace = True)\n    # Convert \"decktypeid\" \"66.0\" to \"1\"\n    property_data.decktypeid.replace(to_replace = 66.0, value = 1,inplace = True)\n    # change \"airconditioningtypeid\" NaN to \"5\"\n    property_data.airconditioningtypeid.fillna(5,inplace = True)\n    # change \"heatingorsystemtypeid\" NaN to \"13\"\n    property_data.heatingorsystemtypeid.fillna(13,inplace = True)\n\n    # Fill in \"NaN\" \"buildingqualitytypeid\" b\u1eb1ng gi\u00e1 tr\u1ecb ph\u1ed5 bi\u1ebfn\n    buildingqual = property_data['buildingqualitytypeid'].value_counts().idxmax()\n    property_data['buildingqualitytypeid'] = property_data['buildingqualitytypeid'].fillna(buildingqual)\n    # Fill in \"NaN\" \"unitcnt\" b\u1eb1ng gi\u00e1 tr\u1ecb ph\u1ed5 bi\u1ebfn\n    unitcommon = property_data['unitcnt'].value_counts().idxmax()\n    property_data['unitcnt'] = property_data['unitcnt'].fillna(unitcommon)\n    \n\n    property_data['lotsizesquarefeet'].fillna((property_data['lotsizesquarefeet'].mean()), inplace=True)\n\n    # Drop \"regionidneighborhood\"\n    property_data.drop('regionidneighborhood', axis=1, inplace=True)\n    # Drop 'regionidcounty'\n    property_data.drop('regionidcounty', axis=1, inplace=True)\n    \n    return property_data","fbebde4d":"def fillcommonvalue(property_data):    \n    # Drop \"censustractandblock\"\n    property_data.drop('censustractandblock', axis=1, inplace=True)\n    ##-------------------------------------------------------------\n    # Fill in \"regionidzip\" b\u1eb1ng gi\u00e1 tr\u1ecb ph\u1ed5 bi\u1ebfn\n    regionidzip = property_data['regionidzip'].value_counts().idxmax()\n    property_data['regionidzip'] = property_data['regionidzip'].fillna(regionidzip)\n\n    # Fill in \"fips\" b\u1eb1ng gi\u00e1 tr\u1ecb ph\u1ed5 bi\u1ebfn\n    fips = property_data['fips'].value_counts().idxmax()\n    property_data['fips'] = property_data['fips'].fillna(fips)\n\n    # Fill in \"propertylandusetypeid\" b\u1eb1ng gi\u00e1 tr\u1ecb ph\u1ed5 bi\u1ebfn\n    propertylandusetypeid = property_data['propertylandusetypeid'].value_counts().idxmax()\n    property_data['propertylandusetypeid'] = property_data['propertylandusetypeid'].fillna(propertylandusetypeid)\n\n    # Fill in \"latitude\"  b\u1eb1ng gi\u00e1 tr\u1ecb ph\u1ed5 bi\u1ebfn\n    latitude = property_data['latitude'].value_counts().idxmax()\n    property_data['latitude'] = property_data['latitude'].fillna(latitude)\n\n    # Fill in \"longitude\" b\u1eb1ng gi\u00e1 tr\u1ecb ph\u1ed5 bi\u1ebfn\n    longitude = property_data['longitude'].value_counts().idxmax()\n    property_data['longitude'] = property_data['longitude'].fillna(longitude)\n    \n    # Normal value\n    property_data[['latitude', 'longitude']] \/= 1e6\n    property_data['rawcensustractandblock'] \/= 1e6\n\n    # Fill in \"rawcensustractandblock\" b\u1eb1ng gi\u00e1 tr\u1ecb ph\u1ed5 bi\u1ebfn\n    rawcensustractandblock = property_data['rawcensustractandblock'].value_counts().idxmax()\n    property_data['rawcensustractandblock'] = property_data['rawcensustractandblock'].fillna(rawcensustractandblock)\n\n    # Fill in \"assessmentyear\" b\u1eb1ng gi\u00e1 tr\u1ecb ph\u1ed5 bi\u1ebfn\n    assessmentyear = property_data['assessmentyear'].value_counts().idxmax()\n    property_data['assessmentyear'] = property_data['assessmentyear'].fillna(assessmentyear)\n\n    # Fill in \"bedroomcnt\" b\u1eb1ng gi\u00e1 tr\u1ecb ph\u1ed5 bi\u1ebfn\n    bedroomcnt = property_data['bedroomcnt'].value_counts().idxmax()\n    property_data['bedroomcnt'] = property_data['bedroomcnt'].fillna(bedroomcnt)\n\n    # Fill in \"bathroomcnt\" b\u1eb1ng gi\u00e1 tr\u1ecb ph\u1ed5 bi\u1ebfn\n    bathroomcnt = property_data['bathroomcnt'].value_counts().idxmax()\n    property_data['bathroomcnt'] = property_data['bathroomcnt'].fillna(bathroomcnt)\n\n    # Fill in \"roomcnt\" b\u1eb1ng gi\u00e1 tr\u1ecb ph\u1ed5 bi\u1ebfn\n    roomcnt = property_data['roomcnt'].value_counts().idxmax()\n    property_data['roomcnt'] = property_data['roomcnt'].fillna(roomcnt)\n    \n    # Fill in \"propertycountylandusecode\" b\u1eb1ng gi\u00e1 tr\u1ecb ph\u1ed5 bi\u1ebfn\n    propertycountylandusecode = property_data['propertycountylandusecode'].value_counts().idxmax()\n    property_data['propertycountylandusecode'] = property_data['propertycountylandusecode'].fillna(propertycountylandusecode)\n    \n    # Fill in \"NaN\" \"propertyzoningdesc\" with most common\n    propertyzoningdesc = property_data['propertyzoningdesc'].value_counts().idxmax()\n    property_data['propertyzoningdesc'] = property_data['propertyzoningdesc'].fillna(propertyzoningdesc)\n    \n    return property_data","b3619162":"# reduce 58 to 42 columns \nprop2016_df.shape","ba551833":"def convert_transactiondate(train_with_months):\n    train_with_months['sale_month'] = train_with_months['transactiondate'].apply(lambda x: (x.to_pydatetime()).month)\n    train_with_months['sale_day'] = train_with_months['transactiondate'].apply(lambda x: (x.to_pydatetime()).day)\n    train_with_months['sale_year'] = train_with_months['transactiondate'].apply(lambda x: (x.to_pydatetime()).year)\n    train_with_months.drop(['transactiondate'],axis=1,inplace=True)\n    return train_with_months","16f357f8":"def preprocess_data(property_data):\n    property_data = poolhottubor_process(property_data)\n    property_data = fireplace_process(property_data)\n    property_data = garage_process(property_data)\n    property_data = tax_process(property_data)\n    property_data = squarefeet_process(property_data)\n    property_data = bathroom_process(property_data)\n    \n    property_data = rest_process(property_data)\n    property_data = fillcommonvalue(property_data)\n    return property_data","313fe15f":"prop2016 = preprocess_data(prop2016_df)\nprop2017 = preprocess_data(prop2017_df)\n\nprint ('prop 2016 data has {0} rows and {1} columns'.format(prop2016.shape[0],prop2016.shape[1]))","c75db858":"# identify levels of missingness\nmissing = prop2016.isnull().sum().sort_values(ascending = False)\nvartypes = prop2016.dtypes\nmissingpercent = (prop2016.isnull().sum()\/prop2016.shape[0]).sort_values(ascending=False)\npd.concat([vartypes, missing, missingpercent], axis = 1,\n          keys =['var type', 'missing n', 'percent']\n        ).sort_values(by = 'missing n', ascending = False).head(10)","8e0c1ca7":"for c in prop2016.columns:\n    if prop2016[c].dtype == 'object':\n        print(c)","318d61db":"from sklearn.preprocessing import LabelEncoder\ncountylandusecode = LabelEncoder()\nprop2016[\"propertycountylandusecode\"] = countylandusecode.fit_transform(prop2016[\"propertycountylandusecode\"])\nprop2017[\"propertycountylandusecode\"] = countylandusecode.fit_transform(prop2017[\"propertycountylandusecode\"])\n\nzoningdesc = LabelEncoder()\nprop2016[\"propertyzoningdesc\"] = zoningdesc.fit_transform(prop2016[\"propertyzoningdesc\"])\nprop2017[\"propertyzoningdesc\"] = zoningdesc.fit_transform(prop2017[\"propertyzoningdesc\"])\n","5a97a0fc":"print(prop2016_df['propertyzoningdesc'].value_counts())","7a5a5965":"prop2016_df['parcelid']","9c067b44":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nprop2016 = scaler.fit_transform(prop2016.loc[:, 'airconditioningtypeid':])\nprop2016 = pd.DataFrame(prop2016, columns=prop2016_df.loc[:, 'airconditioningtypeid':].columns)\nprop2016['parcelid'] = prop2016_df['parcelid']\nprop2017 = scaler.fit_transform(prop2017.loc[:, 'airconditioningtypeid':])\nprop2017 = pd.DataFrame(prop2017, columns=prop2017_df.loc[:, 'airconditioningtypeid':].columns)\nprop2017['parcelid'] = prop2017_df['parcelid']","a170beff":"prop2016.head()","5399947c":"def feature_engineering(property_data):\n    property_data['avg_garage_size'] = property_data['garagetotalsqft'] \/ property_data['garagecarcnt']\n    property_data['avg_garage_size'].fillna(0, inplace=True)\n    \n    # Rotated Coordinates\n    property_data['location_1'] = property_data['latitude'] + property_data['longitude']\n    property_data['location_2'] = property_data['latitude'] - property_data['longitude']\n    property_data['location_3'] = property_data['latitude'] + 0.5 * property_data['longitude']\n    property_data['location_4'] = property_data['latitude'] - 0.5 * property_data['longitude']\n\n    property_data['taxpercentage'] = property_data['taxamount'] \/ property_data['taxvaluedollarcnt']\n    property_data['taxpercentage'].fillna((property_data['taxpercentage'].mean()), inplace=True)\n    # Drop \"taxamount\"\n    property_data.drop('taxamount', axis=1, inplace=True)\n\n    # Th\u00eam derived room_cnt feature b\u1eb1ng c\u00e1ch c\u1ed9ng bathroom_cnt v\u00e0 bedroom_cnt\n    property_data['derived_room_cnt'] = property_data['bedroomcnt'] + property_data['bathroomcnt']\n    \n    return property_data\n\nprop2016 = feature_engineering(prop2016)\nprop2017 = feature_engineering(prop2017)\nprint ('prop 2016 data has {0} rows and {1} columns'.format(prop2016.shape[0],prop2016.shape[1]))\nprint ('prop 2017 data has {0} rows and {1} columns'.format(prop2017.shape[0],prop2017.shape[1]))","d78adb44":"# import featuretools as ft\n# # creating and entity set 'es'\n# es = ft.EntitySet(id = 'zillow')\n\n# # adding a dataframe \n# es.entity_from_dataframe(entity_id = 'properties', dataframe = prop2016, index = 'parcelid')\n# # create entity\n#es.normalize_entity(base_entity_id = 'properties', new_entity_id='land_zone', index = 'propertyzoningdesc', additional_variables = ['propertycountylandusecode', 'propertylandusetypeid'])","0a5c4f02":"# feature_matrix, feature_names = ft.dfs(entityset=es, \n#                                       target_entity = 'properties', \n#                                       max_depth = 2, \n#                                       verbose = 1)","e76530b8":"train_2016 = convert_transactiondate(train2016_df.copy()).merge(prop2016, how='left', on='parcelid')\ntrain_2017 = convert_transactiondate(train2017_df.copy()).merge(prop2017, how='left', on='parcelid')\ntrain = pd.concat([train_2016, train_2017], axis=0, ignore_index=True)\n\nprint ('prop 2016 data has {0} rows and {1} columns'.format(prop2016.shape[0],prop2016.shape[1]))\nprint ('train 2016 data has {0} rows and {1} columns'.format(train_2016.shape[0],train_2016.shape[1]))","39e9ad43":"dtype_df = prop2016.dtypes.reset_index()\ndtype_df.columns = ['Count', 'Column type']\ndtype_df.groupby('Column type').aggregate('count').reset_index()","32e5bf7f":"# export prop after data processing\nprop2016.to_csv(\"properties_2016_proc.csv.gz\", index=False, compression='gzip')\nprop2017.to_csv(\"properties_2017_proc.csv.gz\", index=False, compression='gzip')","cd116ea3":"for c in train.columns:\n    if train[c].dtype != 'object' and c in catvars:\n        print(\"{0} have type: {1}\".format(c,train[c].dtype ))","ffec68d7":"# to change use .astype() \n# CatBoost requires all the categorical variables to be in the string format\ndef float2string(train):\n    train['airconditioningtypeid'] = train.airconditioningtypeid.astype(str)\n    train['buildingqualitytypeid'] = train.buildingqualitytypeid.astype(str)\n    train['decktypeid'] = train.decktypeid.astype(str)\n    train['fips'] = train.fips.astype(str)\n    train['hashottuborspa'] = train.fips.astype(str)\n    train['heatingorsystemtypeid'] = train.heatingorsystemtypeid.astype(str)\n    train['latitude'] = train.latitude.astype(str)\n    train['longitude'] = train.longitude.astype(str)\n    train['propertycountylandusecode'] = train.fips.astype(str)\n    train['propertylandusetypeid'] = train.fips.astype(str)\n    train['propertyzoningdesc'] = train.fips.astype(str)\n    train['regionidzip'] = train.regionidzip.astype(str)\n    train['yearbuilt'] = train.yearbuilt.astype(str)\n    train['fireplaceflag'] = train.fips.astype(str)\n    train['assessmentyear'] = train.assessmentyear.astype(str)\n    train['taxdelinquencyflag'] = train.fips.astype(str)\n    \n    return train\ntrain_cat = float2string(train)","6d849be2":"prop2016 = float2string(prop2016)\nprop2017 = float2string(prop2017)","4b21018d":"catboot_features = train_cat.drop(['parcelid', 'logerror', 'sale_month', 'sale_day', 'sale_year'], axis=1)\n\nprint(\"Number of features for CastBoot: {}\".format(len(catboot_features.columns)))","9bee8c15":"# Prepare training and cross-validation data\ncatboot_label = train_cat.logerror.astype(np.float32)\nprint(catboot_label.head())\n\n# Transform to Numpy matrices\ncatboot_X = catboot_features.values\ncatboot_y = catboot_label.values\n\n# Perform shuffled train\/test split\nnp.random.seed(42)\nrandom.seed(10)\nX_train, X_val, y_train, y_val = train_test_split(catboot_X, catboot_y, test_size=0.2)\n\n# Remove outlier examples from X_train and y_train; Keep them in X_val and y_val for proper cross-validation\noutlier_threshold = 0.4\nmask = (abs(y_train) <= outlier_threshold)\nX_train = X_train[mask, :]\ny_train = y_train[mask]\n\nprint(\"X_train shape: {}\".format(X_train.shape))\nprint(\"y_train shape: {}\".format(y_train.shape))\nprint(\"X_val shape: {}\".format(X_val.shape))\nprint(\"y_val shape: {}\".format(y_val.shape))","ee1a74b3":"del prop2016_df, prop2017_df, catboot_X, catboot_y\ngc.collect()\nprint('Memory usage reduction...')","2dda3387":"feature_names = [s for s in catboot_features.columns]\n\ncategorical_indices = []\nfor i, n in enumerate(catboot_features.columns):\n    if n in catvars:\n        categorical_indices.append(i)\nprint(categorical_indices)","ff3551b6":"# CatBoost parameters after tuned hyperparameter\nparams = {}\nparams['loss_function'] = 'MAE'\nparams['eval_metric'] = 'MAE'\nparams['nan_mode'] = 'Min'  # Method to handle NaN (set NaN to either Min or Max)\nparams['random_seed'] = 0\n\nparams['iterations'] = 1000  # default 1000, use early stopping during training\nparams['learning_rate'] = 0.03  # default 0.03\nparams['max_depth'] = 10  # default 6 (must be <= 16, 6 to 10 is recommended)\nparams['l2_leaf_reg'] = 9  # default 3 (used for leaf value calculation, try different values)\n\nparams['border_count'] = 254  # default 254 (alias max_bin, suggested to keep at default for best quality)\nparams['bagging_temperature'] = 1  # default 1 (higher value -> more aggressive bagging, try different values)","71bd5c07":"%%time\nfrom catboost import CatBoostRegressor, Pool\n# Train CatBoost Regressor with cross-validated early-stopping\nval_pool = Pool(X_val, y_val, cat_features=categorical_indices)\n\nnp.random.seed(42)\nrandom.seed(36)\nmodel = CatBoostRegressor(**params)\nmodel.fit(X_train, y_train,\n          cat_features=categorical_indices,\n          use_best_model=True, eval_set=val_pool, early_stopping_rounds=30,\n          logging_level='Silent', plot=True)\n\n# Evaluate model performance\nprint(\"Train score: {}\".format(abs(model.predict(X_train) - y_train).mean() * 100))\nprint(\"Val score: {}\".format(abs(model.predict(X_val) - y_val).mean() * 100))","c96a1eb2":"def predict_and_export(models, features_2016, features_2017, file_name):\n    # Construct DataFrame for prediction results\n    submission_2016 = pd.DataFrame()\n    submission_2017 = pd.DataFrame()\n    submission_2016['ParcelId'] = features_2016.parcelid\n    submission_2017['ParcelId'] = features_2017.parcelid\n    \n    test_features_2016 = features_2016.drop(['parcelid'], axis=1)\n    test_features_2017 = features_2017.drop(['parcelid'], axis=1)\n    \n    pred_2016, pred_2017 = [], []\n    for i, model in enumerate(models):\n        print(\"Start model {} (2016)\".format(i))\n        pred_2016.append(model.predict(test_features_2016))\n        print(\"Start model {} (2017)\".format(i))\n        pred_2017.append(model.predict(test_features_2017))\n    \n    # Take average across all models\n    mean_pred_2016 = np.mean(pred_2016, axis=0)\n    mean_pred_2017 = np.mean(pred_2017, axis=0)\n    \n    submission_2016['201610'] = [float(format(x, '.4f')) for x in mean_pred_2016]\n    submission_2016['201611'] = submission_2016['201610']\n    submission_2016['201612'] = submission_2016['201610']\n\n    submission_2017['201710'] = [float(format(x, '.4f')) for x in mean_pred_2017]\n    submission_2017['201711'] = submission_2017['201710']\n    submission_2017['201712'] = submission_2017['201710']\n    \n    submission = submission_2016.merge(how='inner', right=submission_2017, on='ParcelId')\n    \n    print(\"Length of submission DataFrame: {}\".format(len(submission)))\n    print(\"Submission header:\")\n    print(submission.head())\n    \n    submission.to_csv(file_name, index=False, compression='gzip')\n    return submission, pred_2016, pred_2017","3487ac4e":"train_cat.sample(10)","bbe7e746":"prop2016.sample(10)","fa7a2bae":"%%time\nfile_name = 'v23_EDA_catboost_single.csv.gz'\nsubmission, pred_2016, pred_2017 = predict_and_export([model], prop2016, prop2017, file_name)","ebac496e":"del model, submission, pred_2016, pred_2017\ngc.collect()\nprint('Memory usage reduction\u2026')","ed3c9cde":"%%time\nbags = 3\nmodels = []\nfor i in range(bags):\n    print(\"Start training model {}\".format(i))\n    params['random_seed'] = i\n    np.random.seed(42)\n    random.seed(36)\n    model = CatBoostRegressor(**params)\n    model.fit(X_train, y_train, cat_features=categorical_indices, verbose=False)\n    models.append(model)\n    \n# Sanity check (make sure scores on a small portion of the dataset are reasonable)\nfor i, model in enumerate(models):\n    print(\"model {}: {}\".format(i, abs(model.predict(X_val) - y_val).mean() * 100))","460e005d":"file_name = 'v23_EDA_catboost_ensemble_x3.csv.gz'\nsubmission, pred_2016, pred_2017 = predict_and_export(models, prop2016, prop2017, file_name)","b9e20a54":"# %%time\n# train_y = train['logerror'].values\n# cat_cols = [\"hashottuborspa\", \"propertycountylandusecode\", \"propertyzoningdesc\", \"fireplaceflag\", \"taxdelinquencyflag\"]\n# train_df = train.drop(['parcelid', 'logerror']+cat_cols, axis=1)\n# feat_names = train_df.columns.values\n\n# from sklearn import ensemble\n# train_df_new=train_df.fillna(train_df.mean())\n# model = ensemble.ExtraTreesRegressor(n_estimators=25, max_depth=30, max_features=0.3, n_jobs=-1, random_state=0)\n# model.fit(train_df_new, train_y)\n\n# ## plot the importance\n# importances = model.feature_importances_\n# std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n# indices = np.argsort(importances)[::-1][:20]\n\n# plt.figure(figsize=(12,12))\n# plt.title(\"Feature importances\")\n# plt.bar(range(len(indices)), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\n# plt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\n# plt.xlim([-1, len(indices)])\n# plt.show()","d53a88f7":"# import xgboost as xgb\n# xgb_params = {\n#     'eta': 0.05,\n#     'max_depth': 8,\n#     'subsample': 0.7,\n#     'colsample_bytree': 0.7,\n#     'objective': 'reg:linear',\n#     'silent': 1,\n#     'seed' : 0\n# }\n# dtrain = xgb.DMatrix(train_df, train_y, feature_names=train_df.columns.values)\n# model = xgb.train(dict(xgb_params, silent=0), dtrain, num_boost_round=50)\n\n# # plot the important features #\n# fig, ax = plt.subplots(figsize=(12,18))\n# xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n# plt.show()","e94bf32c":"Target varibale l\u00e0 logerror ","5ce78105":"## Data Exploration\n### Corralation matrix plot\n* dataset c\u00f3 nhi\u1ec1u c\u1ed9t th\u00ec c\u00e1ch nhanh d\u1ec3 x\u00e1c \u0111\u1ecbnh mqh gi\u1eefa c\u00e1c c\u1ed9t l\u00e0 d\u00f9ng heatmaps (bi\u1ec3u \u0111\u1ed3 nhi\u1ec7t)\n* Cho ph\u00e9p nh\u1eadn di\u1ec7n m\u1ed1i quan h\u1ec7 gi\u1eefa 2 c\u1ed9t\n    * m\u00e0u c\u00e0ng \u0111\u1ecf th\u1ec3 hi\u1ec7n m\u1ed1i quan h\u1ec7 positive m\u1ea1nh\n    * m\u00e0u c\u00e0ng xanh th\u1ec3 hi\u1ec7n m\u1ed1i quan h\u1ec7 negative m\u1ea1nh \n","ef71740d":"## File transaction train 2016 and 2017\n* C\u00f3 90275 giao d\u1ecbch tr\u01b0\u1edbc th\u00e1ng 10 trong n\u0103m 2016","b109b125":"## Explore file properties 2016 and 2017\n* C\u00f3 58 \u0111\u1eb7c tr\u01b0ng kh\u00e1c nhau mi\u00eau t\u1ea3 cho m\u1ed7i ng\u00f4i nh\u00e0 trong g\u1ea7n 3 tri\u1ec7u ng\u00f4i nh\u00e0","598a977e":"## zillow data dictionary","4c1b8875":"**Univariate analysis**\n* v\u00ec c\u00f3 nhi\u1ec1u bi\u1ebfn n\u00ean c\u00f3 th\u1ec3 c\u00f3 m\u1ed1i quan h\u1ec7 gi\u1eefa c\u00e1c bi\u1ebfn thu\u1ed9c t\u00ednh v\u00e0 'logerror' m\u1ee5c ti\u00eau \n    * kh\u00f4ng c\u00f3 t\u01b0\u01a1ng quan m\u1ea1nh n\u00e0o c\u1ee7a c\u00e1c bi\u1ebfn v\u00e0 logerror","27b5f8cb":"## Implement XGBoot","d257365e":"Tr\u01b0\u1edbc khi quy\u1ebft \u0111\u1ecbnh lo\u1ea1i b\u1ecf m\u1ed9t s\u1ed1 feature (h\u1ea7u h\u1ebft b\u1ecb thi\u1ebfu),c\u1ea7n \u0111\u1ea3m b\u1ea3o r\u1eb1ng m\u1ed9t \u00f4 c\u00f3 ch\u1eef \"NaN\" th\u1ef1c s\u1ef1 mang \u00fd ngh\u0129a l\u00e0 \"Kh\u00f4ng\". \n* V\u00ed d\u1ee5: n\u1ebfu m\u1ed9t ng\u00f4i nh\u00e0 c\u00f3 m\u1ed9t h\u1ed3 b\u01a1i, \"hashottuborspa\"  n\u1ebfu m\u1ed9t ng\u00f4i nh\u00e0 kh\u00f4ng c\u00f3 m\u1ed9t h\u1ed3 b\u01a1i \"NaN\" \u0111\u01b0\u1ee3c nh\u1eadp v\u00e0o","cd0ffa24":"T\u1eeb correlation plot th\u1ea5y m\u1ed9t s\u1ed1 nh\u00f3m bi\u1ebfn c\u00f3 s\u1ef1 t\u01b0\u01a1ng quan m\u1ea1nh. \n* taxes: taxvaluedollarcnt, landtaxvaluedollarcnt v\u00e0 taxamount. \n* square footage: finishedfloor1squarefeet, finishedsquarefeet12, finishedsquarefeet13, finishedsquarefeet15, finishedsquarefeet50, finishedsquarefeet6, and calculatedfinishedsquarefeet.\n* bathrooms: fullbathcnt, calculatedbathnbr, and bathroomcnt.","74087ae5":"# Data Pre-Processing\n* D\u1eef li\u1ec7u m\u00e2u thu\u1eabn c\u00f3 th\u1ec3 \u0111\u01b0a ra k\u1ebft qu\u1ea3 ph\u00e2n t\u00edch kh\u00f4ng ch\u00ednh x\u00e1c\n* x\u1eed l\u00fd kh\u00f4ng \u0111\u1ea7y \u0111\u1ee7, c\u00f3 r\u1ea5t nhi\u1ec1u c\u1ed9t c\u00f3 gi\u00e1 tr\u1ecb NaN\n* x\u1eed l\u00fd nhi\u1ec5u\n* D\u1eef li\u1ec7u b\u1ecb tr\u00f9ng l\u1eb7p b\u1ecb lo\u1ea1i b\u1ecf\n","909ffcf8":"# Get Data","5bc43dfb":"## Pools & Hot tubs\n* \"poolcnt\" - S\u1ed1 h\u1ed3 b\u01a1i. \"NaN\" ngh\u0129a l\u00e0 \"0 h\u1ed3 b\u01a1i\", n\u00ean c\u00f3 th\u1ec3 update gi\u00e1 tr\u1ecb \"0\" thay v\u00ec \"NaN\".\n* \"hashottuborspa\" - nh\u00e0 c\u00f3 b\u1ed3n t\u1eafm n\u01b0\u1edbc n\u00f3ng hay spa kh\u00f4ng. \n    * \"NaN\" ngh\u0129a l\u00e0 \"0\", n\u00ean c\u00f3 th\u1ec3 update gi\u00e1 tr\u1ecb \"0\" thay v\u00ec \"NaN\".\n    * Thay \"true\" th\u00e0nh \"1\"\n* \"poolsizesum\" - T\u1ed5ng di\u1ec7n t\u00edch b\u1ec3 b\u01a1i. \n    * \"NaN\" ngh\u0129a l\u00e0 di\u1ec7n t\u00edch \"0 feet vu\u00f4ng\" n\u00ean c\u00f3 th\u1ec3 update gi\u00e1 tr\u1ecb \"0\".\n    * V\u1edbi nh\u00e0 c\u00f3 b\u1ec3 b\u01a1i (poolcnt=1) nh\u01b0ng b\u1ecb missing di\u1ec7n tich th\u00ec s\u1ebd thay b\u1eb1ng gi\u00e1 tr\u1ecb di\u1ec7n t\u00edch trung b\u00ecnh\n* \"pooltypeid2\" & \"pooltypeid7\" & \"pooltypeid10\" - Ki\u1ec3u lo\u1ea1i h\u1ed3 ho\u1eb7c b\u1ed3n t\u1eafm n\u01b0\u1edbc n\u00f3ng, nh\u1eefng category n\u00e0y ch\u1ec9 cung c\u1ea5p th\u00f4ng tin v\u1edbi nh\u1eefng gi\u00e1 tr\u1ecb kh\u00e1c null n\u00ean ta c\u00f3 th\u1ec3 update gi\u00e1 tr\u1ecb \"Nan\" b\u1eb1ng \"0\". \"pooltypeid10\" cung c\u1ea5p th\u00f4ng tin gi\u1ed1ng \"hashottuborspa\" n\u00ean c\u00f3 th\u1ec3 lo\u1ea1i kh\u1ecfi t\u1eadp d\u1eef li\u1ec7u","5ab77aa6":"## Di\u1ec7n t\u00edch\n* \"finishedsquarefeet6\" - Khu v\u1ef1c dang d\u1edf\n    * drop do \u00edt xu\u1ea5t hi\u1ec7n v\u00e0 n\u1ebfu c\u00f3 c\u0169ng b\u1eb1ng calculatedfinishedsquarefeet\n* \"finishedsquarefeet12\" - Khu v\u1ef1c sinh ho\u1ea1t \u0111\u00e3 ho\u00e0n th\u00e0nh\n    * drop do missing nhi\u1ec1u\n* \"finishedsquarefeet15\" - T\u1ed5ng di\u1ec7n t\u00edch\n    * xu\u1ea5t hi\u1ec7n 6.4% \u00edt v\u00e0 \u0111a s\u1ed1 \u0111\u1ec1u b\u1eb1ng calculatedfinishedsquarefeet n\u00ean fill \"NaN\" b\u1eb1ng gi\u00e1 tr\u1ecb calculatedfinishedsquarefeet  \n* \"finishedsquarefeet50\" - K\u00edch th\u01b0\u1edbc khu v\u1ef1c sinh ho\u1ea1t \u0111\u00e3 ho\u00e0n th\u00e0nh t\u1ea1i t\u1ea7ng 1 (l\u1ed1i v\u00e0o)\n    * N\u1ebfu numberofstories=1 thay \"NaN\" b\u1eb1ng gi\u00e1 tr\u1ecb calculatedfinishedsquarefeet\n    * Thay \"NaN\" c\u00f2n l\u1ea1i b\u1eb1ng gi\u00e1 tr\u1ecb trung b\u00ecnh\n* \"finishedfloor1squarefeet\" - K\u00edch th\u01b0\u1edbc khu v\u1ef1c sinh ho\u1ea1t \u0111\u00e3 ho\u00e0n th\u00e0nh t\u1ea1i t\u1ea7ng 1 (l\u1ed1i v\u00e0o)\n    * drop do l\u1eb7p th\u00f4ng tin v\u1edbi finishedsquarefeet50\n* \"calculatedfinishedsquarefeet\" - T\u1ed5ng khu v\u1ef1c sinh ho\u1ea1t \u0111\u00e3 ho\u00e0n th\u00e0nh (\u0111\u01a1n v\u1ecb feet vu\u00f4ng)\n    * thay \"NaN\" b\u1eb1ng gi\u00e1 tr\u1ecb trung b\u00ecnh\n* \"numberofstories\" - s\u1ed1 t\u1ea7ng\n    * thay \"NaN\" b\u1eb1ng gi\u00e1 tr\u1ecb ph\u1ed5 bi\u1ebfn l\u00e0 1 t\u1ea7ng","f8cd0c3e":"**Distribution of the target variable logerror**\n* skew: th\u01b0\u1edbc \u0111o s\u1ef1 b\u1ea5t \u0111\u1ed1i x\u1ee9ng c\u1ee7a ph\u00e2n ph\u1ed1i x\u00e1c su\u1ea5t c\u1ee7a bi\u1ebfn ng\u1eabu nhi\u00ean c\u00f3 gi\u00e1 tr\u1ecb th\u1ef1c v\u1ec1 gi\u00e1 tr\u1ecb trung b\u00ecnh c\u1ee7a n\u00f3","89ce39ec":"## Ph\u1ea7n c\u00f2n l\u1ea1i\n* => C\u00e1c thu\u1ed9c t\u00ednh category missing 99% n\u00ean lo\u1ea1i b\u1ecf thu\u1ed9c t\u00ednh\n* \"taxdelinquencyyear\" - N\u0103m m\u00e0 ch\u01b0a n\u1ed9p thu\u1ebf\n* \"basementsqft\" - Khu v\u1ef1c sinh ho\u1ea1t \u0111\u00e3 ho\u00e0n th\u00e0nh ho\u1eb7c m\u1ed9t ph\u1ea7n \u1edf t\u1ea7ng h\u1ea7m\n* \"storytypeid\" - M\u00e3 m\u00f4 t\u1ea3 t\u1ea5t c\u1ea3 lo\u1ea1i nh\u00e0 nh\u01b0 ng\u00f4i nh\u00e0 nhi\u1ec1u t\u1ea7ng\n* \"architecturalstyletypeid\" - Phong c\u00e1ch ki\u1ebfn tr\u00fac c\u1ee7a ng\u00f4i nh\u00e0\n* \"typeconstructiontypeid\" - Lo\u1ea1i v\u1eadt li\u1ec7u \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 x\u00e2y nh\u00e0\n* \"finishedsquarefeet13\" - Chu vi khu v\u1ef1c sinh ho\u1ea1t \u0111\u00e3 ho\u00e0n th\u00e0nh. H\u1ea7u nh\u01b0 missing n\u00ean drop category\n* \"buildingclasstypeid\" - Id lo\u1ea1i khung c\u1ee7a t\u00f2a nh\u00e0. Kh\u00f4ng mang l\u1ea1i nhi\u1ec1u th\u00f4ng tin v\u00e0 missing nhi\u1ec1u n\u00ean drop\n------\n* \"yardbuildingsqft17\" - Di\u1ec7n t\u00edch s\u00e2n b\u00ean trong, thay th\u1ebf NaN b\u1eb1ng \"0\"\n* \"yardbuildingsqft26\" - Di\u1ec7n t\u00edch nh\u00e0 kho\/s\u00e2n sau, thay th\u1ebf NaN b\u1eb1ng \"0\"\n* \"decktypeid\" - Lo\u1ea1i s\u00e0n ch\u1ec9 c\u00f3 2 gi\u00e1 tr\u1ecb \"66.0\" ho\u1eb7c \"NaN\", n\u00ean s\u1ebd thay 66 b\u1eb1ng 1 v\u00e0 NaN b\u1eb1ng 0\n* \"airconditioningtypeid\" - lo\u1ea1i h\u1ec7 th\u1ed1ng l\u00e0m m\u00e1t. trong dic None g\u00e1n l\u00e0 5, n\u00ean thay \"NaN\" b\u1eb1ng 5\n* \"heatingorsystemtypeid\" - Lo\u1ea1i h\u1ec7 th\u1ed1ng s\u01b0\u1edfi trong nh\u00e0. thay \"Nan\" b\u1eb1ng 13 cho \"None\"\n* \"buildingqualitytypeid\" - \u0111\u00e1nh gi\u00e1 chung v\u1ec1 t\u00ecnh tr\u1ea1ng c\u1ee7a t\u00f2a nh\u00e0, t\u1eeb t\u1ed1t (gi\u00e1 tr\u1ecb nh\u1ecf) \u0111\u1ebfn cao (gi\u00e1 tr\u1ecb to)\n    * Thay b\u1eb1ng gi\u00e1 tr\u1ecb common\n* \"unitcnt\" - S\u1ed1 kh\u1ed1i x\u00e2y d\u1ef1ng \u0111\u01b0\u1ee3c t\u00edch h\u1ee3p v\u00e0o th\u00e0nh nh\u00e0. \n    * Thay \"NaN\" gia tr\u1ecb ph\u1ed5 bi\u1ebfn\n* \"lotsizesquarefeet\" - Di\u1ec7n t\u00edch l\u00f4 \u0111\u1ea5t t\u00ednh b\u1eb1ng feet vu\u00f4ng\n    * Thay \"NaN\" b\u1eb1ng gi\u00e1 tr\u1ecb trung b\u00ecnh\n* \"regionidneighborhood\" - id h\u00e0ng x\u00f3m. c\u1ea7n map \u0111\u1ec3 x\u00e1c \u0111\u1ecbnh h\u00e0ng x\u00f3m v\u00e0 alongitude & latitude v\u1ec1 c\u01a1 b\u1ea3n cung c\u1ea5p th\u00f4ng tin n\u00e0y\n    * drop c\u1ed9t n\u00e0y\n* \"regionidcounty\" - id qu\u1eadn c\u1ee7a l\u00f4 \u0111\u1ea5t\n    * drop c\u1ed9t n\u00e0y\n    ","4120a491":"# Understand the problem","a0da01f5":"## Ensemble Training & Prediction catboot","4ee017e9":"## Fireplace\n* fireplaceflag - Ng\u00f4i nh\u00e0 c\u00f3 l\u00f2 s\u01b0\u1edfi. Gi\u00e1 tr\u1ecb \"True\" ho\u1eb7c \"NaN\"\n    * Th\u1ec3 thay \"True\" b\u1eb1ng \"1\" v\u00e0 \"NaN\" b\u1eb1ng \"0\".\n    * v\u1edbi nh\u00e0 c\u00f3 s\u1ed1 l\u00f2 s\u01b0\u1edfi > 1 thay th\u1ebf fireplaceflag b\u1eb1ng 1\n* fireplacecnt - S\u1ed1 l\u00f2 s\u01b0\u1edfi. update gi\u00e1 tr\u1ecb \"NaN\" b\u1eb1ng \"0\"\n    * v\u1edbi nh\u00e0 c\u00f3 fireplaceflag b\u1eb1ng True thay th\u1ebf b\u1eb1ng gi\u00e1 tr\u1ecb trung b\u00ecnh s\u1ed1 l\u00f2 s\u01b0\u1edfi l\u00e0 1","4dcead23":"## Garage\n* \"garagecarcnt\" - C\u00f3 bao nhi\u00eau gara trong ng\u00f4i nh\u00e0, update gi\u00e1 tr\u1ecb \"NaN\" b\u1eb1ng \"0\"\n* \"garagetotalsqft\" - di\u1ec7n t\u00edch gara, n\u1ebfu ng\u1ed3i nh\u00e0 kh\u00f4ng c\u00f3 gara ta c\u00f3 th\u1ec3 thay th\u1ebf \"NaN\" b\u1eb1ng \"0\" ","c5a24c4c":"# Model Training","0360c431":"### plot histograms numeric \n*  categorical variables th\u00ec t\u1ea1o boxplot","c3c44800":"Train data bao t\u1ea5t c\u1ea3 c\u00e1c transaction tr\u01b0\u1edbc ng\u00e0y 15\/10, 2016, v\u00e0 th\u00eam m\u1ed9t s\u1ed1 transaction sau 15\/10, 2016\n\nTest data trong public leaderboard ph\u1ea7n c\u00f2n l\u1ea1i transactions 15\/10 \u0111\u1ebfn 31\/12, 2016. C\u00f2n test data trong private leaderboard s\u1eed d\u1ee5ng data 17\/10 \u0111\u1ebfn 15\/12, 2017.\n\nY\u00eau c\u1ea7u predict 6 time points: October 2016 (201610), November 2016 (201611), December 2016 (201612), October 2017 (201710), November 2017 (201711) v\u00e0 December 2017 (201712). \nIf a property was not sold in a certain time period, that particular row will be ignored when calculating your score.\n","536ebd8c":"## Chu\u1ea9n h\u00f3a d\u1eef li\u1ec7u\nChu\u1ea9n ho\u00e1 s\u1ebd gi\u00fap c\u00e2n b\u1eb1ng m\u1ee9c \u0111\u1ed9 \u1ea3nh h\u01b0\u1edfng \u0111\u1ebfn nh\u00e3n c\u1ee7a c\u00e1c \u0111\u1eb7c tr\u01b0ng\n\n\nC\u00f4ng th\u1ee9c \u0111\u01a1n gi\u1ea3n: z = (x - u) \/ s\n\ntrong \u0111\u00f3 u is trung b\u00ecnh c\u00e1c \u0111\u1eb7c tr\u01b0ng and s \u0111\u1ed9 l\u1ec7ch chu\u1ea9n.\n","90e3563d":"## Taxs value\n* \"taxdelinquencyflag\" - Tax Delinquency (n\u1ee3 thu\u1ebf) thu\u1ebf t\u00e0i s\u1ea3n qu\u00e1 h\u1ea1n t\u1eeb n\u0103m 2015, gi\u00e1 tr\u1ecb l\u00e0 'NaN' ho\u1eb7c 'Y'\n* \"landtaxvaluedollarcnt\" - Gi\u00e1 tr\u1ecb \u0111\u00e1nh gi\u00e1 c\u1ee7a di\u1ec7n t\u00edch \u0111\u1ea5t c\u1ee7a th\u1eeda \u0111\u1ea5t\n    * Thay \"NaN\" b\u1eb1ng 0\n* \"structuretaxvaluedollarcnt\" - Gi\u00e1 tr\u1ecb \u0111\u00e1nh gi\u00e1 c\u1ee7a c\u1ea5u tr\u00fac x\u00e2y d\u1ef1ng tr\u00ean l\u00f4 \u0111\u1ea5t\n    * Thay \"NaN\" b\u1eb1ng 0\n* \"taxvaluedollarcnt\" -  T\u1ed5ng gi\u00e1 tr\u1ecb thu\u1ebf \u0111\u00e1nh gi\u00e1 tr\u00ean l\u00f4 \u0111\u1ea5t\n    * Thay \"NaN\" b\u1eb1ng gi\u00e1 tr\u1ecb trung b\u00ecnh \n* \"taxamount\" - T\u1ed5ng thu\u1ebf t\u00e0i s\u1ea3n trong n\u0103m \u0111\u00e1nh gi\u00e1 \u0111\u00f3\n    * ph\u1ea7n th\u00f4ng tin th\u1eeba ph\u1ea7n feature engineering s\u1ebd lo\u1ea1i b\u1ecf\n* \"regionidcity\" - M\u00e3 th\u00e0nh ph\u1ed1 t\u1ea1i v\u1ecb tr\u00ed \u0111\u1ea5t\n    * Th\u00f4ng tin th\u1eeba n\u00ean lo\u1ea1i b\u1ecf\n* \"yearbuilt\" - N\u0103m nh\u00e0 \u0111\u01b0\u1ee3c x\u00e2y d\u1ef1ng\n    * Thay \"NaN\" b\u1eb1ng gi\u00e1 tr\u1ecb ph\u1ed5 bi\u1ebfn","8d0a718d":"# Feature Engineering\n\n* B\u1ed5 sung \"taxpercentage\" - taxamount\/taxvaluedollarcnt thay cho taxamount","66cc87e2":"## rawcensustractandblock & censustractandblock\n* \"rawcensustractandblock = censustractandblock\" - K\u1ebft h\u1ee3p \u0111i\u1ec1u tra d\u00e2n s\u1ed1 v\u00e0 ID kh\u1ed1i - c\u0169ng nh\u01b0 ph\u00e2n c\u00f4ng nh\u00f3m theo ph\u1ea7n m\u1edf r\u1ed9ng\n    * tr\u00f9ng l\u1eb7p n\u00ean s\u1ebd lo\u1ea1i b\u1ecf feature censustractandblock\n-------\n## Fill rest NaN with most common value\n* regionidzip, fips, propertylandusetypeid, latitude, longitude, rawcensustractandblock, assessmentyear, bedroomcnt, bathroom, roomcnt, propertycountylandusecode, propertyzoningdesc (m\u00f4 t\u1ea3 \u0111\u01b0\u1ee3c ph\u00e9p s\u1eed d\u1ee5ng \u0111\u1ea5t cho t\u00e0i s\u1ea3n \u0111\u00f3) b\u1eb1ng gi\u00e1 tr\u1ecb ph\u1ed5 bi\u1ebfn","9e01d331":"## Ki\u1ec3m tra ki\u1ec3u d\u1eef li\u1ec7u ","72bea688":"## bathroom features\n* threequarterbathnbr - S\u1ed1 ph\u00f2ng t\u1eafm trong nh\u00e0 c\u00f3 3\/4 (v\u00f2i sen + b\u1ed3n r\u1eeda + nh\u00e0 v\u1ec7 sinh)\n* fullbathcnt - S\u1ed1 l\u01b0\u1ee3ng ph\u00f2ng t\u1eafm \u0111\u1ea7y \u0111\u1ee7 (sink, shower + bathtub v\u00e0  toilet) trong nh\u00e0\n* calculatedbathnbr - S\u1ed1 ph\u00f2ng t\u1eafm (bao g\u1ed3m c\u1ea3 fractional bathroom)\n---------------\n**relation**\n* calculatedbathnbr bao g\u1ed3m c\u1ea3 threequarterbathnbr v\u00e0 fullbathcnt\n    * drop threequarterbathnbr v\u00e0 fullbathcnt v\u00e0 thay th\u1ebf missing value b\u1eb1ng s\u1ed1 l\u01b0\u1ee3ng ph\u00f2ng t\u1eafm ph\u1ed5 bi\u1ebfn","90a15a94":"## Implement CatBoot\n","13ab2ce7":"##  Chuy\u1ec3n d\u1eef li\u1ec7u t\u1eeb ch\u1eef th\u00e0nh s\u1ed1"}}