{"cell_type":{"ac90f713":"code","700b4cfc":"code","6e157fb4":"code","7bb83aa2":"code","8e84cf60":"code","278545e4":"code","ae177837":"code","663558b9":"code","7f8846cc":"code","c86d689a":"code","201abfc4":"code","bc3da68c":"code","bfd996b5":"code","e610afa5":"code","7fc7a988":"code","fc9b0ef5":"code","d9bc8ec8":"code","db29ed8d":"code","2069fbc0":"code","cfa6b4f5":"code","46a5ff48":"code","ef267c0b":"code","982cc2f1":"code","57a2b9b3":"code","d461882b":"code","6d920ead":"code","64eb0ed5":"code","66bf2828":"code","0db0539f":"code","f323ddb9":"code","0c23852f":"code","ce431e1a":"code","3eff2709":"code","37f3afd3":"code","85b927b1":"code","5a9c6efc":"code","c1ff3b28":"code","8a554d3c":"code","4ffccf9d":"code","7fd77250":"code","e9294a45":"code","aad799df":"markdown","0f2226d1":"markdown","88402aa9":"markdown","f8a2a3c7":"markdown","1da4c28a":"markdown","1816a330":"markdown","2065a541":"markdown","10206d4e":"markdown","2acfdaa5":"markdown","5b93492d":"markdown","d800bede":"markdown","0f091cb5":"markdown","c6b0d70d":"markdown","108e479e":"markdown","a09bbe9b":"markdown","dcd910f7":"markdown","834902ac":"markdown","3f9cf09f":"markdown","be73033f":"markdown","d7ee579b":"markdown","0f197fa8":"markdown","efc1fbea":"markdown"},"source":{"ac90f713":"%reload_ext autoreload\n%autoreload 2\n%matplotlib inline\n\nfrom fastai.nlp import *\nfrom sklearn.linear_model import LogisticRegression","700b4cfc":"# path to the dataset\nPATH='\/kaggle\/input\/aclimdb_v1\/aclImdb\/'\nnames = ['neg','pos']","6e157fb4":"%ls {PATH}","7bb83aa2":"%ls {PATH}train","8e84cf60":"# The next line fails with \"ls: write error\", so comment it out\n# %ls {PATH}train\/pos | head","278545e4":"trn,trn_y = texts_labels_from_folders(f'{PATH}train',names)\nval,val_y = texts_labels_from_folders(f'{PATH}test',names)","ae177837":"trn[0]","663558b9":"trn_y[0]","7f8846cc":"veczr = CountVectorizer(tokenizer=tokenize)","c86d689a":"trn_term_doc = veczr.fit_transform(trn)\nval_term_doc = veczr.transform(val)","201abfc4":"trn_term_doc","bc3da68c":"trn_term_doc[0]","bfd996b5":"vocab = veczr.get_feature_names(); vocab[5000:5005]","e610afa5":"w0 = set([o.lower() for o in trn[0].split(' ')]); w0","7fc7a988":"len(w0)","fc9b0ef5":"veczr.vocabulary_['absurd']","d9bc8ec8":"trn_term_doc[0,1297]","db29ed8d":"trn_term_doc[0,5000]","2069fbc0":"def pr(y_i):\n    p = x[y==y_i].sum(0)\n    return (p+1) \/ ((y==y_i).sum()+1)","cfa6b4f5":"x=trn_term_doc\ny=trn_y\n\nr = np.log(pr(1)\/pr(0))\nb = np.log((y==1).mean() \/ (y==0).mean())","46a5ff48":"pre_preds = val_term_doc @ r.T + b\npreds = pre_preds.T>0\n(preds==val_y).mean()","ef267c0b":"x=trn_term_doc.sign()\nr = np.log(pr(1)\/pr(0))\n\npre_preds = val_term_doc.sign() @ r.T + b\npreds = pre_preds.T>0\n(preds==val_y).mean()","982cc2f1":"m = LogisticRegression(C=1e8, dual=True)\nm.fit(x, y)\npreds = m.predict(val_term_doc)\n(preds==val_y).mean()","57a2b9b3":"m = LogisticRegression(C=1e8, dual=True)\nm.fit(trn_term_doc.sign(), y)\npreds = m.predict(val_term_doc.sign())\n(preds==val_y).mean()","d461882b":"m = LogisticRegression(C=0.1, dual=True)\nm.fit(x, y)\npreds = m.predict(val_term_doc)\n(preds==val_y).mean()","6d920ead":"m = LogisticRegression(C=0.1, dual=True)\nm.fit(trn_term_doc.sign(), y)\npreds = m.predict(val_term_doc.sign())\n(preds==val_y).mean()","64eb0ed5":"veczr =  CountVectorizer(ngram_range=(1,3), tokenizer=tokenize, max_features=800000)\ntrn_term_doc = veczr.fit_transform(trn)\nval_term_doc = veczr.transform(val)","66bf2828":"trn_term_doc.shape","0db0539f":"vocab = veczr.get_feature_names()","f323ddb9":"vocab[200000:200005]","0c23852f":"y=trn_y\nx=trn_term_doc.sign()\nval_x = val_term_doc.sign()","ce431e1a":"r = np.log(pr(1) \/ pr(0))\nb = np.log((y==1).mean() \/ (y==0).mean())","3eff2709":"m = LogisticRegression(C=0.1, dual=True)\nm.fit(x, y);\n\npreds = m.predict(val_x)\n(preds.T==val_y).mean()","37f3afd3":"r.shape, r","85b927b1":"np.exp(r)","5a9c6efc":"x_nb = x.multiply(r)\nm = LogisticRegression(dual=True, C=0.1)\nm.fit(x_nb, y);\n\nval_x_nb = val_x.multiply(r)\npreds = m.predict(val_x_nb)\n(preds.T==val_y).mean()","c1ff3b28":"sl=2000","8a554d3c":"# Here is how we get a model from a bag of words\nmd = TextClassifierData.from_bow(trn_term_doc, trn_y, val_term_doc, val_y, sl)","4ffccf9d":"learner = md.dotprod_nb_learner()\nlearner.fit(0.02, 1, wds=1e-6, cycle_len=1)","7fd77250":"learner.fit(0.02, 2, wds=1e-6, cycle_len=1)","e9294a45":"learner.fit(0.02, 2, wds=1e-6, cycle_len=1)","aad799df":"`fit_transform(trn)` finds the vocabulary in the training set. It also transforms the training set into a term-document matrix. Since we have to apply the *same transformation* to your validation set, the second line uses just the method `transform(val)`. `trn_term_doc` and `val_term_doc` are sparse matrices. `trn_term_doc[i]` represents training document i and it contains a count of words for each document for each word in the vocabulary.","0f2226d1":"* Baselines and Bigrams: Simple, Good Sentiment and Topic Classification. Sida Wang and Christopher D. Manning [pdf](https:\/\/www.aclweb.org\/anthology\/P12-2018)","88402aa9":"...and the regularized version","f8a2a3c7":"### Trigram with NB features","1da4c28a":"Here we fit regularized logistic regression where the features are the trigrams' log-count ratios.","1816a330":"Here we fit regularized logistic regression where the features are the trigrams.","2065a541":"## fastai NBSVM++","10206d4e":"## References","2acfdaa5":"We define the **log-count ratio** $r$ for each word $f$:\n\n$r = \\log \\frac{\\text{ratio of feature $f$ in positive documents}}{\\text{ratio of feature $f$ in negative documents}}$\n\nwhere ratio of feature $f$ in positive documents is the number of times a positive document has a feature divided by the number of positive documents.","5b93492d":"## IMDB dataset and the sentiment classification task","d800bede":"Our next model is a version of logistic regression with Naive Bayes features described [here](https:\/\/www.aclweb.org\/anthology\/P12-2018). For every document we compute binarized features as described above, but this time we use bigrams and trigrams too. Each feature is a log-count ratio. A logistic regression model is then trained to predict sentiment.","0f091cb5":"...and binarized Naive Bayes.","c6b0d70d":"The [large movie review dataset](http:\/\/ai.stanford.edu\/~amaas\/data\/sentiment\/) contains a collection of 50,000 reviews from IMDB. The dataset contains an even number of positive and negative reviews. The authors considered only highly polarized reviews. A negative review has a score \u2264 4 out of 10, and a positive review has a score \u2265 7 out of 10. Neutral reviews are not included in the dataset. The dataset is divided into training and test sets. The training set is the same 25,000 labeled reviews.\n\nThe **sentiment classification task** consists of predicting the polarity (positive or negative) of a given text.\n\nTo get the dataset, in your terminal run the following commands:\n\n`wget http:\/\/ai.stanford.edu\/~amaas\/data\/sentiment\/aclImdb_v1.tar.gz`\n\n`gunzip aclImdb_v1.tar.gz`\n\n`tar -xvf aclImdb_v1.tar`","108e479e":"### Logistic regression","a09bbe9b":"Here is the $\\text{log-count ratio}$ `r`.  ","dcd910f7":"Here is how we can fit logistic regression where the features are the unigrams.","834902ac":"Here is the text of the first review","3f9cf09f":"* \n\nThis is a copy of Jeremy Howard's lesson 5 notebook from the **fastai** course **_Introduction to Machine Learning for Coders_** at `https:\/\/course.fast.ai\/ml`\n\nI've uploaded the [large movie review dataset](http:\/\/ai.stanford.edu\/~amaas\/data\/sentiment\/)  to kaggle and made it available to this kernel. I've made a few modifications to the kernel to make it run on kaggle. Enjoy and learn!\n\nSource: `https:\/\/github.com\/fastai\/fastai\/blob\/master\/courses\/ml1\/lesson5-nlp.ipynb`\n\nBased on notebook version: `bbcd4e0`\n","be73033f":"Here is the formula for Naive Bayes.","d7ee579b":"## Naive Bayes","0f197fa8":"[`CountVectorizer`](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.CountVectorizer.html) converts a collection of text documents to a matrix of token counts (part of `sklearn.feature_extraction.text`).","efc1fbea":"### Tokenizing and term document matrix creation"}}