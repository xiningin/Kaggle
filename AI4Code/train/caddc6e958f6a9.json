{"cell_type":{"db9bf762":"code","b5dae372":"code","4afcf58b":"code","00e30607":"code","b94a5e71":"code","0e7e8147":"code","b5547c67":"code","90281eee":"code","6da2f13a":"code","d16e228a":"code","762517c7":"code","98dc6522":"code","deaf982c":"code","436c576c":"code","627dbf0c":"code","901bb835":"code","977db95e":"code","87ce0af5":"code","92bfc768":"code","2c7cf460":"code","702b6fb7":"code","e066d11f":"code","9c840775":"code","4283d633":"code","e8ffcf88":"code","b43ba0c6":"code","6e2ff6ab":"code","cc925e76":"code","59968688":"markdown","90d2b9dc":"markdown","030ee095":"markdown","fe8cb5ef":"markdown","3c6e62d1":"markdown","8ce18a51":"markdown"},"source":{"db9bf762":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b5dae372":"data = pd.read_csv(\"..\/input\/heart.csv\")\ndata.info()\n","4afcf58b":"df = pd.DataFrame(data)\ndf.columns\n","00e30607":"df.index","b94a5e71":"data.describe()","0e7e8147":"import seaborn as sns # used for plot interactive graph.\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(12,8))\nsns.countplot(data['age'],label =\"count\")\n","b5547c67":"#correlation graph to find the correlation\ncorr = data.corr()\nplt.figure(figsize = (14,14))\nsns.heatmap(corr, cbar = True,  square = True, annot=True, fmt= '.2f',annot_kws={'size': 15},\n           cmap= 'coolwarm')","90281eee":"#conclusion, none of the features are highlt correltaed, so we could use all ","6da2f13a":"selected_features = ['age', 'cp', 'chol', 'fbs', 'restecg', 'thalach',\n       'exang', 'oldpeak', 'slope', 'ca', 'thal']\n\n\nplt.figure(figsize = (14,14))\ncolor_function = {0: \"blue\", 1: \"red\"} # Here Red color will be 1 which means M and blue foo 0 means B\ncolors = data[\"target\"].map(lambda x: color_function.get(x))# mapping the color fuction with diagnosis column\npd.scatter_matrix(data[selected_features], c=colors, alpha = 0.5, figsize = (15, 15)); # plotting scatter plot matrix","d16e228a":"#First get an general overview\ndata.hist(figsize=(15,20))\nplt.figure()","762517c7":"#cp: chest pain type\ndata[\"cp\"].value_counts()\n","98dc6522":"plt.xlabel('pain type')\nplt.ylabel('count')\nplt.title('Four type chest pain')\nsns.countplot(data['cp'])","deaf982c":"import plotly\nimport plotly.graph_objs as go\n\nx_data = data['age']\ny_data = data['thalach']\ncolors = np.random.rand(2938)\nsz = np.random.rand(2000)*30\n\nfig = go.Figure()\nfig.add_scatter(x = x_data,\n                y = y_data,\n                mode = 'markers',\n                marker = {'size': sz,\n                         'color': colors,\n                         'opacity': 0.6,\n                         'colorscale': 'Portland'\n                       })\nplotly.offline.iplot(fig)","436c576c":"#adding filter to data\ndata[(data['thalach']>190)]","627dbf0c":"#adding filter to data\n\ndata[(data['age']>40) & (data['sex']==0)]\n   ","901bb835":"#using swarmplot to learn about age, gender ad orobability of having heart disearse.\n#first append a new row about age measuremnet based on probability\nthreshold = sum(data.age)\/len(data.age)\nthreshold_chol = sum(data.chol)\/len(data.chol)\nprint(\"threshold of age:\" , threshold)\nprint(\"threshold of chol: \", threshold_chol)\n\ndata['probability'] = ['high' if i> threshold else 'low' for i in data.age]\ndata['probability']\n\nsns.swarmplot(x = 'sex', y = 'age', hue = \"probability\", data = data)","977db95e":"data_original = pd.read_csv(\"..\/input\/heart.csv\")\ndata['sex'] = data_original.sex\ndata['sex']=['female' if i == 0 else 'male' for i in data.sex]\nplt.figure(figsize=(14,8))\nsns.swarmplot(x = 'age', y = 'chol', hue = \"sex\", data = data)","87ce0af5":"data.head(5)","92bfc768":"#aplit the dataset ready for training and testing, and apply machine leaning ALG\nfrom sklearn.model_selection import train_test_split\nx,y = data.loc[:,data.columns != 'target'], data.loc[:,'target']\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3,random_state = 1)\n","2c7cf460":"data_orginal = pd.read_csv(\"..\/input\/heart.csv\")\ndata['sex']=data_orginal.sex\ndata.dtypes.sample(10)\n#data.select_dtypes(exclude=['object'])\n\n","702b6fb7":"#Machine learning by KNN\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nknn = KNeighborsClassifier(n_neighbors = 3)\n#x,y = data.loc[:,data.columns != 'class'], data.loc[:,'class']\nx_train_one_hot_encoded = pd.get_dummies(x_train)\nx_test_one_hot_encoded = pd.get_dummies(x_test)\nknn.fit(x_train_one_hot_encoded,y_train)\nprediction = knn.predict(x_test_one_hot_encoded)\nacc = accuracy_score(y_test, prediction)\nk = knn.score(x_test_one_hot_encoded,y_test)\nprint(\"acc score with one hot encoded: \", acc)\nprint(\"knn score: \", k)\nprint(\"just only drop the non-numrical feature--->\")\nx_train_2 = x_train.select_dtypes(exclude=['object'])\nx_test_2 = x_test.select_dtypes(exclude=['object'])\nknn.fit(x_train_2,y_train)\nprediction = knn.predict(x_test_2)\nacc = accuracy_score(y_test, prediction)\nprint(\"acc score without one hot encoded: \", acc)","e066d11f":"# the training score is relativly low. I will apply Grid Search to find the best parameters of KNN","9c840775":"x_train = x_train.select_dtypes(exclude=['object'])\nx_test = x_test.select_dtypes(exclude=['object'])","4283d633":"# grid search cross validation with 1 hyperparameter\nfrom sklearn.model_selection import GridSearchCV\ngrid = {'n_neighbors': np.arange(1,100)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, grid, cv=3) # GridSearchCV\nknn_cv.fit(x_train,y_train)# Fit\n\n# Print hyperparameter\nprint(\"Tuned hyperparameter k: {}\".format(knn_cv.best_params_)) \nprint(\"Best score: {}\".format(knn_cv.best_score_))","e8ffcf88":"#After apply GridSearch the score is not much impoved","b43ba0c6":"#apply sklearn decision tree classifier on the dataset\n#reference https:\/\/www.kaggle.com\/drgilermo\/playing-with-the-knobs-of-sklearn-decision-tree\nimport time\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.metrics import confusion_matrix\nfrom subprocess import check_output\nfrom sklearn import tree\nfrom IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image, ImageDraw, ImageFont\nimport re\n\nclassifier = DecisionTreeClassifier(max_depth = 3)\nclassifier.fit(x_train, y_train)\n\nprint(\"Decision tree score : {}\".format(classifier.score(x_test, y_test))) \n\nclf = DecisionTreeClassifier(max_depth = 3, criterion = \"entropy\")\nclf.fit(x_train,y_train)\nprint(\"Decision tree score : {}\".format(clf.score(x_test, y_test))) ","6e2ff6ab":"#apply best split and random split\nt = time.time()\nclf_random = DecisionTreeClassifier(max_depth = 3, splitter = 'random')\nclf_random.fit(x_train,y_train)\nprint('random Split accuracy...',clf_random.score(x_test,y_test))\nclf_best = DecisionTreeClassifier(max_depth = 3, splitter = 'best')\nclf_best.fit(x_train,y_train)\nprint('best Split accuracy...',clf_best.score(x_test,y_test))\n# conclusion: random split is not nessesary worse than best split","cc925e76":"with open(\"tree1.dot\", 'w') as f:\n     f = tree.export_graphviz(clf_best,\n                              out_file=f,\n                              max_depth = 5,\n                              impurity = False,# true will show thw im-purity of each node~ gini value\n                              feature_names = x_test.columns.values,\n                             class_names = ['No', 'Yes'],\n                            #  class_names = True,\n                              rounded = True,\n                              filled= True )#False no color indication\n        \n#Convert .dot to .png to allow display in web notebook\ncheck_call(['dot','-Tpng','tree1.dot','-o','tree1.png'])\n\n# Annotating chart with PIL\nimg = Image.open(\"tree1.png\")\ndraw = ImageDraw.Draw(img)\nimg.save('sample-out.png')\nPImage(\"sample-out.png\")\n# we have generared rhe random decision tree classifier, see below.\n","59968688":" Doc reference: https:\/\/docs.rapidminer.com\/latest\/studio\/operators\/modeling\/predictive\/trees\/parallel_decision_tree.html\nThis decison tree is generated spilting attribute selection on:gain_ratio, and the maximal depth set to 4. \nFrom the 1st tree of RapidMiner, we could conclude the following:\n1. When a patient has non-typical anagina (cp type 2-4 , cp>0.5), and when resting with blood pressure (trestbps) higher than 179, ST depression(oldoeak) smaller than 2.45, have a high chance diagnose as heart disease. (target as YES)\n2. When a patient has typical anagina (cp type 1, cp< 0.9), alone with the maximum heart rate (thalach) reached over 181.50, then the patient as a high chance have heart disease. (target as YES)\n\n","90d2b9dc":"\n![](C:\/Users\/s134225\/Downloads\/wetransfer-2abfb3\/tree.JPG)","030ee095":"![](https:\/\/raw.githubusercontent.com\/xuanxuanzhang\/image\/master\/tree2.JPG)","fe8cb5ef":"![](https:\/\/raw.githubusercontent.com\/xuanxuanzhang\/image\/master\/tree.JPG)\n","3c6e62d1":"Conculsion, when using the best decision tree classifier, we have reached the best score around 70%. We could make  the following conclustions from the best_classifier decision tree:\n1. When a patient has typical anagina (cp type 1), but there no obvious result of anagina pain during excercise, then the patient have a high chance NO heart disease. (target as NO)\n2. When a patient shows not -typical anagina, with a low ST depression induced by exercise (oldpeak <= 2.1), the patinet is probabaly HAS heart disease. (target as YES)\n3. When a patient shows typical anagina (cp type 1), no obvious result of anagina pain during excercise, but shows a low number of major vessels (0-3) colored by flourosopy  (ca <= 0.5), then this patient has more than 50% HAVE heart disease. (target as YES)\n\nBesides, I have tried to generate decison tree by other data mining tool, such as RapidMiner, two of the results are shown below:\n","8ce18a51":"When I have adjusted the settings with set the spliting attribute on the least entropy one (information_gain), maximum depth kept as 4, then we get the second decison tree, which age, sex ,ca, and thal involved for prediction. Then we could get the following conclusion:\n1. When a patient has non-typical anagina (cp type 2-4 , cp>0.5), but he\/she is younger than 56, then he\/she has a high chance to diagnose as heart disease.(target as YES)\n2. When a female patient has non-typical anagina (cp type 2-4 , cp>0.5) and older than 56, then she has a high chance to diagnose as heart disease.(target as YES)\n3. When a patient has a typical anagina (cp type 1, cp<= 0.5),  and he\/she has number of major vessels bigger than 0.5 (ca> 0.5, maority patient has ca smaller than 0.4), then he\/she probablty has NO heart disease. (target as NO)\n"}}