{"cell_type":{"79745701":"code","ec670573":"code","e61cfa43":"code","45968da8":"code","96dd6800":"code","64d86e0a":"code","b7c72662":"code","3fe04d04":"code","a0289a01":"code","54655f70":"code","31007c45":"code","60bfbb38":"code","c0cb5c43":"code","303f62d2":"code","1c12edab":"code","4b31dd98":"code","1922fd58":"code","e1c7d5ff":"code","84e2eb1a":"code","b6c95f48":"code","e91c2907":"code","809afd33":"code","38c04e32":"code","ff324276":"code","d45f4e7e":"code","2059db8a":"code","2e48d4a7":"code","194675d6":"code","674d21fe":"code","fdcb8baa":"code","e2ea5e49":"code","b12bc1c1":"code","84c13cf0":"code","00baf63c":"code","ad9e2262":"code","f09cefd8":"code","d15e061d":"code","22970b25":"code","9ae5a03a":"code","a6d73149":"code","e46b56e5":"code","3e357d9c":"code","8492af9b":"code","6bdec8ad":"code","fad3321d":"markdown","381273eb":"markdown","e30f542e":"markdown","d0121042":"markdown"},"source":{"79745701":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec670573":"import pandas as pd\nimport seaborn as sn\nimport numpy as np\nimport plotly.express as px\nimport matplotlib.pyplot as plt\n%matplotlib inline","e61cfa43":"pd.set_option('display.max_columns',None)\ndf = pd.read_csv('\/kaggle\/input\/passenger-list-for-the-estonia-ferry-disaster\/estonia-passenger-list.csv')\ndf.head()","45968da8":"def unique(df):\n    for i in df.columns:\n        print(f' feature <{i}> has {df[i].unique()} values')\n        print('='*75)","96dd6800":"def valuecounts(df):\n    for i in df.columns:\n        print(f' feature <{i}> has {df[i].value_counts()} value counts')\n        print('='*75)","64d86e0a":"unique(df)","b7c72662":"pd.set_option('display.max_rows',None)\nvaluecounts(df)","3fe04d04":"df = df.drop(['PassengerId'],axis = 1)\ndf.head()","a0289a01":"px.bar(data_frame=df,x = 'Country',y = 'Survived',labels={'x':'Country','y':'Survived'})","54655f70":"age_df = df.sort_values(['Age'],ascending=False)\nage_df.head()","31007c45":"px.bar(data_frame=age_df,x = age_df['Age'],y = age_df['Survived'],labels={'x':'Age','y':'Survived'})","60bfbb38":"px.bar(data_frame=age_df,x = age_df['Sex'],y = age_df['Survived'],labels={'x':'Age','y':'Survived'})","c0cb5c43":"df = df.drop(['Firstname','Lastname'],axis = 1)\ndf.head()","303f62d2":"df['Sex'] = pd.get_dummies(df['Sex'])\ndf['Category'] = pd.get_dummies(df['Category'])\ndf.head()","1c12edab":"mean_map = df.groupby(['Country'])['Survived'].mean()\nmean_map","4b31dd98":"data = df.loc[(df['Country']!='Belarus') & (df['Country']!='Canada')]\ndata.head()","1922fd58":"mean_map1 = data.groupby(['Country'])['Survived'].mean()\nmean_map1","e1c7d5ff":"data['Country'] = data['Country'].map(mean_map1)","84e2eb1a":"data.head()","b6c95f48":"test = data['Survived']\ntrain = data.drop(['Survived'],axis = 1)\n","e91c2907":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import RandomizedSearchCV,cross_val_score,train_test_split\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn import metrics","809afd33":"X_train, X_test, y_train, y_test = train_test_split(train,test,test_size = 0.2)","38c04e32":"clf = DecisionTreeClassifier(random_state=0)\nclf.fit(X_train,y_train)\nplt.figure(figsize=(16,9))\ntree.plot_tree(clf,filled=True,feature_names=train.columns,class_names=['Survived','Not Survived'])","ff324276":"path = clf.cost_complexity_pruning_path(X_train,y_train)\nccp_alphas = path.ccp_alphas","d45f4e7e":"alpha_list = []\nfor i in  ccp_alphas:\n    clf = DecisionTreeClassifier(random_state=0,ccp_alpha=i)\n    clf.fit(X_train,y_train)\n    alpha_list.append(clf)","2059db8a":"train_score = [clf.score(X_train,y_train) for clf in alpha_list]\ntest_score =  [clf.score(X_test,y_test) for clf in alpha_list]\n\nplt.xlabel('alpha')\nplt.ylabel('accuracy')\nplt.plot(ccp_alphas,train_score,marker = 'o',label = 'training',color = 'magenta',drawstyle = 'steps-post')\nplt.plot(ccp_alphas,test_score,marker = '+',label = 'testing',color = 'red',drawstyle = 'steps-post')\nplt.legend()\nplt.show()","2e48d4a7":"dt = DecisionTreeClassifier(random_state=0,ccp_alpha=0.0075)\ndt.fit(X_train,y_train)\nplt.figure(figsize=(16,9))\ntree.plot_tree(dt,filled=True,feature_names=train.columns,class_names=['Survived','Not Survived'])","194675d6":"params = {\n    'RandomForest':{\n        'model':RandomForestClassifier(),\n        'params':{\n            'criterion':['gini','entropy'],\n            'n_estimators':[int(x) for x in np.linspace(100,1200,10)],\n            'max_depth':[int(x) for x in np.linspace(1,30,5)],\n            'max_features':['auto','sqrt','log2'],\n            'ccp_alpha':[x for x in np.linspace(0.0050,0.0090,5)],\n            'min_samples_split':[2,5,10,14],\n            'min_samples_leaf':[2,5,10,14],\n        }\n    },\n    'logistic':{\n        'model':LogisticRegression(),\n        'params':{\n            'penalty':['l1', 'l2', 'elasticnet'],\n            'C':[0.25,0.50,0.75,1.0],\n            'tol':[1e-10,1e-5,1e-4,1e-3,0.025,0.25,0.50],\n            'solver':['lbfgs','liblinear','saga','newton-cg'],\n            'multi_class':['auto', 'ovr', 'multinomial'],\n            'max_iter':[int(x) for x in np.linspace(start=1,stop=250,num=10)],\n        }\n    },\n    'D-tree':{\n        'model':DecisionTreeClassifier(),\n        'params':{\n            'criterion':['gini','entropy'],\n            'splitter':['best','random'],\n            'min_samples_split':[1,2,5,10,12],\n            'min_samples_leaf':[1,2,5,10,12],\n            'max_features':['auto','sqrt'],\n            'ccp_alpha':[x for x in np.linspace(0.0050,0.0090,5)],\n        }\n    },\n    'SVM':{\n        'model':SVC(),\n        'params':{\n            'C':[0.25,0.50,0.75,1.0,5,10,100],\n            'gamma':['scale',1,0.1,0.01,0.001,0.0001],\n            'tol':[1e-10,1e-5,1e-4,0.025,0.50,0.75],\n            'kernel':['linear','poly','sigmoid','rbf'],\n            'max_iter':[int(x) for x in np.linspace(start=1,stop=250,num=10)],\n        }\n    },\n    'G-Boost':{\n        'model':GradientBoostingClassifier(),\n        'params':{\n            'n_estimators':[int(x) for x in np.linspace(100,1200,10)],\n            'max_depth':[int(x) for x in np.linspace(1,30,5)],\n            'max_features':['auto','sqrt','log2'],\n            'ccp_alpha':[x for x in np.linspace(0.0050,0.0090,5)],\n            'min_samples_split':[2,5,10,14],\n            'min_samples_leaf':[2,5,10,14],\n            'loss':['deviance', 'exponential'],\n            'learning_rate':[0.1,0.5,1.0,1.5],\n            'tol':[1e-10,1e-5,1e-4,0.025,0.50,0.75],\n        }\n    }\n    \n    \n}","674d21fe":"scores = []\nfor model_name,mp in params.items():\n    clf = RandomizedSearchCV(mp['model'],param_distributions=mp['params'],cv = 5,n_iter=10,scoring='accuracy',verbose=2)\n    clf.fit(X_train,y_train)\n    scores.append({\n        'model_name':model_name,\n        'best_score':clf.best_score_,\n        'best_estimator':clf.best_estimator_,\n    })","fdcb8baa":"scores_df = pd.DataFrame(scores,columns=['model_name','best_score','best_estimator'])\nscores_df","e2ea5e49":"for i in scores_df['best_estimator']:\n    print(i)\n    print('='*100)","b12bc1c1":"rf = RandomForestClassifier(ccp_alpha=0.006, criterion='entropy', max_depth=8,\n                       max_features='sqrt', min_samples_leaf=2,\n                       min_samples_split=10, n_estimators=711)\nlr = LogisticRegression(max_iter=167, multi_class='ovr', solver='liblinear',\n                   tol=0.025)\ndt = DecisionTreeClassifier(ccp_alpha=0.005, criterion='entropy',\n                       max_features='sqrt', splitter='random')\nsvc = SVC(C=0.5, gamma=0.001, max_iter=250, tol=0.025)\n\ngb = GradientBoostingClassifier(ccp_alpha=0.006, learning_rate=0.5,\n                           loss='exponential', max_depth=1, max_features='sqrt',\n                           min_samples_leaf=5, n_estimators=711, tol=1e-05)\nxgb = XGBClassifier(booster='dart', gamma=0.8346938775510204, learning_rate=0.325,\n              max_depth=22, n_estimators=344, reg_alpha=1.0, reg_lambda=3)","84c13cf0":"model_list = [rf,lr,dt,svc,gb,xgb]\nfor i in model_list:\n    i.fit(X_train,y_train)","00baf63c":"train_score = []\nfor i in model_list:\n    print(f' model is <{i}_classifier> is \\n training score is :{i.score(X_train,y_train)}')\n    print('='*100)","ad9e2262":"train_score = []\nfor i in model_list:\n    print(f' model is <{i}_classifier> is \\n testing score is :{i.score(X_test,y_test)}')\n    print('='*100)","f09cefd8":"rf = RandomForestClassifier(ccp_alpha=0.006, criterion='entropy', max_depth=8,\n                       max_features='sqrt', min_samples_leaf=2,\n                       min_samples_split=10, n_estimators=711)\nrf.fit(X_train,y_train)\nrf.score(X_train,y_train)","d15e061d":"rf.score(X_test,y_test)","22970b25":"metrics.plot_confusion_matrix(rf,X_test,y_test,display_labels=['Survived','Not Survived'],cmap = 'plasma')","9ae5a03a":"clf = DecisionTreeClassifier(ccp_alpha=0.006)\nclf.fit(X_train,y_train)\nplt.figure(figsize=(15,10))\ntree.plot_tree(clf,filled=True,feature_names=train.columns,class_names=['Survived','Not Survived'])","a6d73149":"temp_train = np.array(train)\npredict = []\nfor i in range(0,len(temp_train)):\n    predict.append(rf.predict([temp_train[i]]))","e46b56e5":"predict = np.array(predict)\ndata['predict'] = predict","3e357d9c":"compare_list = ['Survived','predict']\nfor i in compare_list:\n    print(f' feature <{i}> has {data[i].value_counts()}')\n    print('='*100)","8492af9b":"y_pred = rf.predict(X_test)\ny_pred = np.array(y_pred)\ny_test = np.array(y_test)\nsn.distplot(y_test,hist = True,kde = False,color = 'magenta',label = 'Actual')\nsn.distplot(y_pred,hist = True,kde = False,color='red',label='Predicted')\nplt.legend()\nplt.show()","6bdec8ad":"report = metrics.classification_report(y_test,y_pred)\nprint(report)","fad3321d":"## let's check how many passengers survived with respect to Age","381273eb":"### The best value for ccp_alpha can be selected from 0.0050 to 0.0090 so that the model does'nt overfit the training data","e30f542e":"### APPLYING MEAN ENCODING FOR THE COUNTRY COLUMN","d0121042":"## As we see that countrie like 'Belarus' and 'Canada' have almost null contribution..so i can drop them"}}