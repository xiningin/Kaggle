{"cell_type":{"c998374b":"code","94d0ad96":"code","0d329f69":"code","bf202c53":"code","7fc82aa5":"code","807dd67a":"code","244870a3":"code","54eb82a1":"code","e12e8594":"code","4e4f7fce":"code","7cba4322":"code","ce562e8a":"code","5c262e97":"code","0bdc11f1":"code","4cf76dbd":"markdown","542c1f78":"markdown","093e433c":"markdown","f9fe9605":"markdown","7f61f4d0":"markdown","0603279f":"markdown","390db241":"markdown","4144f79a":"markdown","971ac41d":"markdown","3866ed17":"markdown","e62b6515":"markdown","dc508683":"markdown","06cd5463":"markdown","e417f0e8":"markdown","ec9b4349":"markdown"},"source":{"c998374b":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport skimage.measure\nimport numpy as np\nimport gc\n\nimg=mpimg.imread('..\/input\/alaska2-image-steganalysis\/Cover\/00001.jpg')\nimgplot = plt.imshow(img)\nplt.title('Original')\nplt.show()\n\ntest_pool = skimage.measure.block_reduce(img, (3,3,1), np.max)\n\nimgplot = plt.imshow(test_pool)\nplt.title('3*3 Pooling')\nplt.show()\n\nd1, d2, d3 = test_pool.shape\ndel test_pool\ngc.collect()\n","94d0ad96":"import pandas as pd\nimport tqdm\nfrom PIL import Image\nimport glob\nimport lightgbm as lgb\nfrom skopt import BayesSearchCV\nfrom sklearn.decomposition import PCA\nfrom bayes_opt import BayesianOptimization\nfrom sklearn import metrics\nimport warnings\nwarnings.filterwarnings(\"ignore\")","0d329f69":"def alaska_weighted_auc(y_valid, y_true):\n    tpr_thresholds = [0.0, 0.4, 1.0]\n    weights =        [       2,   1]\n\n    fpr, tpr, thresholds = metrics.roc_curve(y_true.get_label(), y_valid, pos_label=1)\n    \n    # size of subsets\n    areas = np.array(tpr_thresholds[1:]) - np.array(tpr_thresholds[:-1])\n    \n    # The total area is normalized by the sum of weights such that the final weighted AUC is between 0 and 1.\n    normalization = np.dot(areas, weights)\n    \n    competition_metric = 0\n    for idx, weight in enumerate(weights):\n        y_min = tpr_thresholds[idx]\n        y_max = tpr_thresholds[idx + 1]\n        mask = (y_min < tpr) & (tpr < y_max)\n        if mask.sum() == 0:\n            continue\n\n        x_padding = np.linspace(fpr[mask][-1], 1, 100)\n\n        x = np.concatenate([fpr[mask], x_padding])\n        y = np.concatenate([tpr[mask], [y_max] * len(x_padding)])\n        y = y - y_min # normalize such that curve starts at y=0\n        score = metrics.auc(x, y)\n        submetric = score * weight\n        best_subscore = (y_max - y_min) * weight\n        competition_metric += submetric\n        \n    return 'alaska_weighted_auc' ,competition_metric \/ normalization, True\n\ndef bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=25, n_folds=5, random_seed=6, output_process=False):\n    # prepare data\n    train_data = lgb.Dataset(data=X, label=y, free_raw_data=False)\n    # parameters\n    def lgb_eval(num_leaves, feature_fraction, bagging_fraction, max_depth, lambda_l1, lambda_l2, min_split_gain, min_child_weight, learning_rate, n_estimators):\n        params = {'application':'binary', 'early_stopping_round':100, 'metric':'auc', 'objective' : 'binary'}\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n        params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n        params['max_depth'] = int(round(max_depth))\n        params['lambda_l1'] = max(lambda_l1, 0)\n        params['lambda_l2'] = max(lambda_l2, 0)\n        params['min_split_gain'] = min_split_gain\n        params['min_child_weight'] = min_child_weight\n        params['learning_rate'] = max(min(learning_rate, 1), 0.001)\n        params['n_estimators'] = int(round(n_estimators))\n        cv_result = lgb.cv(params, train_data, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'], feval = alaska_weighted_auc)\n        return max(cv_result['auc-mean'])\n    # range \n    lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (10, 80),\n                                            'feature_fraction': (0.1, 0.9),\n                                            'bagging_fraction': (0.6, 1),\n                                            'max_depth': (5, 20),\n                                            'lambda_l1': (0, 10),\n                                            'lambda_l2': (0, 10),\n                                            'min_split_gain': (0.001, 0.1),\n                                            'min_child_weight': (5, 50),\n                                            'learning_rate' : (0.001, 0.1),\n                                            'n_estimators' : (100, 10000)}, random_state=0)\n    \n    # optimize\n    lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n    \n    # output optimization process\n    if output_process==True: lgbBO.points_to_csv(\"bayes_opt_result.csv\")\n    \n    # return best parameters\n    return lgbBO\n\ndef img_reader(nbr_images = 10, df = None, file_name = 'Cover', from_ = 0, status = 'neg') :\n    from_ = from_\n    nbr_images  = nbr_images\n    image_list = []\n    i=0\n    j=0\n    df = df\n    file_name = file_name\n    for filename in tqdm.tqdm(glob.glob('..\/input\/alaska2-image-steganalysis\/'+file_name+'\/*.jpg')): \n        if j >= from_ :\n            im=mpimg.imread(filename)\n            im=skimage.measure.block_reduce(im, (3,3,1), np.max)\n            image_list.append(np.sum(im.reshape((d3, d1*d2)), axis = 0).tolist())\n            i+=1\n            if i%1000 == 0 :\n                if df is None:\n                    df = pd.DataFrame(image_list).astype('int16')\n                    del image_list\n                    gc.collect()\n                    image_list = []\n                else :\n                    df = pd.concat([df , pd.DataFrame(image_list).astype('int16')])\n                    del image_list\n                    gc.collect()\n                    image_list = []\n                    if i == nbr_images :    \n                        del image_list\n                        gc.collect()\n                        break\n        j=j+1\n        \n    if status == 'neg' :\n        df['output'] = 0\n        df['output'] = df['output'].astype('int16')\n        gc.collect()\n    else :\n        df['output'] = 1\n        df['output'] = df['output'].astype('int16')\n        gc.collect()\n        \n    return df","bf202c53":"img=mpimg.imread('..\/input\/alaska2-image-steganalysis\/Cover\/00001.jpg')\ntest_pool = skimage.measure.block_reduce(img, (3,3,1), np.max)\nd1, d2, d3 = test_pool.shape\ndel test_pool\ngc.collect()\n\ndf_neg = img_reader(nbr_images = 12000, df = None, file_name = 'Cover', from_ = 0, status = 'neg')\n\ndf_pos = img_reader(nbr_images = 4000, df = None, file_name = 'JMiPOD', from_ = 0, status = 'pos')\nprint('JMiPOD Done!')\ndf_pos = img_reader(nbr_images = 4000, df = df_pos, file_name = 'JUNIWARD', from_ = 4000, status = 'pos')\nprint('JUNIWARD Done!')\ndf_pos = img_reader(nbr_images = 4000, df = df_pos, file_name = 'UERD', from_ = 8000, status = 'pos')\nprint('UERD Done!')\n\ndf_test = img_reader(nbr_images = 6000, df = None, file_name = 'Test', from_ = 0, status = 'neg')\nprint('Test Done!')","7fc82aa5":"df_train = pd.concat([df_pos, df_neg], ignore_index = True)\ndel df_pos, df_neg\n\ndf_train.to_pickle('df_train3*3.pkl')\ndf_test.to_pickle('df_test3*3.pkl')\n\ndel df_train, df_test\ngc.collect()","807dd67a":"df_train = pd.read_pickle('.\/df_train3*3.pkl')\ndf_test = pd.read_pickle('.\/df_test3*3.pkl')","244870a3":"pca1 = PCA(n_components=500)\ndf_train_pca = pca1.fit_transform(df_train.loc[:, df_train.columns != 'output'].values)\n\npca2 = PCA(n_components=500)\ndf_test_pca = pca2.fit_transform(df_test.loc[:, df_test.columns != 'output'].values)","54eb82a1":"X = df_train_pca\ny = df_train['output']\ndel df_train_pca ,\ngc.collect()","e12e8594":"opt_params = bayes_parameter_opt_lgb(X, y, init_round=15, opt_round=30, n_folds=5, random_seed=6)","4e4f7fce":"print('Best Params :')\n\nprint(opt_params.max['params'])","7cba4322":"params = opt_params.max['params']\nparams['num_leaves'] = int(params['num_leaves'])\nparams['max_depth'] = int(params['max_depth'])\nparams['n_estimators'] = int(params['n_estimators'])\n\nd_train = lgb.Dataset(data=X, label=y, free_raw_data=False)\n\nclf = lgb.train(params, train_set = d_train,  feval = alaska_weighted_auc)","ce562e8a":"lgb.plot_importance(clf, max_num_features = 10)","5c262e97":"lgb.create_tree_digraph(clf)","0bdc11f1":"y_pred=clf.predict(df_test_pca)\nsub = pd.read_csv('..\/input\/alaska2-image-steganalysis\/sample_submission.csv')\nsub['Label'] = y_pred\nsub.to_csv('submission.csv', index=False)","4cf76dbd":"# Hyperparameter Optimization","542c1f78":"![Sans%20titre.png](attachment:Sans%20titre.png)","093e433c":"U see ? it still make sense, almost nothing change ;)\n\nafter that we sum RGB colors, we flat the result and we apply dimension reduction using PCA and we give that food to lgbm for hyperparameters tuning.\n\nin reality it gives that","f9fe9605":"# Images Preprocess","7f61f4d0":"<img src=\"https:\/\/i.postimg.cc\/pXzt75rk\/tenor.gif\" align=\"right\" width=\"300\" height=\"200\">\n\nEveryone using CNN transfer learning for get high in LB, but why not use classic classifications techniques like binary classification with lgbm or somthing like that. \n\nIt dont work realy well but i did it in this notebook, what u gonna do ?\n\nunfortunatly we havent enough power of calculus for that ill make som transformations on images to make them edible.\n\nLets go !\n\n<BR CLEAR=\u201dleft\u201d \/>","0603279f":"![Sans%20titre%202.png](attachment:Sans%20titre%202.png)","390db241":"Save result as pkl format for later.","4144f79a":"Apply PCA with 500 components","971ac41d":"For each image we gonna apply pooling 3 x 3 x 1 on it that make images very small comparing to the original en gardant un maximum sa nature. it destroy information about steganography but we dont have choice =\/.","3866ed17":"Thanks for reading my notebook, if you have any suggestion i will be happy to receive it.\n\nThis solution don't work well andit will not propel you to the top of the LB so dont UpVote.","e62b6515":"# Predict and Submit","dc508683":"# Libraries for fun","06cd5463":"# Train LGBM","e417f0e8":"I am gonna use 3 functions :\n\nalaska_weighted_auc : Competition custom metric <a href=\"https:\/\/www.kaggle.com\/anokas\/weighted-auc-metric-updated\">reference<\/a>\n\nbayes_parameter_opt_lgb : useful for select best hyperparameters for lgb using bayesian optimisation <a href=\"https:\/\/www.kaggle.com\/sz8416\/simple-bayesian-optimization-for-lightgbm\">reference<\/a>\n\nimg_reader : Import images and all process that I quoted above except PCA, made by me =D\n","ec9b4349":"# Alaska 2 LGBM Hyperparameter Optimization"}}