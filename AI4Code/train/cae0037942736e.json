{"cell_type":{"055b36fc":"code","ad5330d7":"code","a91342d5":"code","b3029870":"code","0f14860b":"code","72fa73da":"code","5a0d259f":"code","cceff7c2":"code","dbf1ab0d":"code","2eaf9f0a":"code","0f82a432":"code","a2850f25":"code","f02a509c":"code","ffc528cd":"code","534987a0":"code","41794415":"code","8f1c2a09":"code","0e0d5ea2":"code","b4e28103":"code","3e8aa96e":"code","43e3d382":"code","73905997":"code","7ca0a93d":"code","9a1a9d91":"code","06bd8674":"code","88c189a8":"code","e7bc9f83":"code","bcf7cb2a":"code","c7df7100":"markdown","25d3ab7c":"markdown","d872487b":"markdown","8a62daa9":"markdown","e8a70bd3":"markdown","1eb174e4":"markdown","66cf21a6":"markdown","c233d900":"markdown","ac1b1873":"markdown","e8f925d8":"markdown","4e98327d":"markdown","cfcc60a9":"markdown","0e9144fc":"markdown","9f4c16d8":"markdown","3479b9be":"markdown","3c85586d":"markdown","14fe9dcb":"markdown","bcf78bb5":"markdown","7c36cd1c":"markdown","155623c5":"markdown","909b18c0":"markdown","d59d3c08":"markdown","859a0cfe":"markdown","00d8f64a":"markdown","8ade9aa0":"markdown","9b6cfcf5":"markdown","610d788e":"markdown","978a1489":"markdown"},"source":{"055b36fc":"import numpy as np\nimport pandas as pd \nimport os\nfrom tqdm import tqdm\nimport dask.bag as dask_bag\nimport glob\nimport dask.dataframe as dask_df\n\nprint(os.listdir(\"..\/input\"))","ad5330d7":"for idx, df in tqdm(enumerate(pd.read_csv('..\/input\/rating.csv', chunksize = 500000))):\n    df.to_csv('rating_k{}.csv.gz'.format(idx), compression = 'gzip', index = False)","a91342d5":"#reads whole file\npd.read_csv('..\/input\/movie.csv')\n#reads 10 rows from file\npd.read_csv('..\/input\/movie.csv', nrows = 10)\n#reads whole file skipping 10 first rows\npd.read_csv('..\/input\/movie.csv', skiprows = 10)\n#iterate over file with 10 rows chunks\npd.read_csv('..\/input\/movie.csv', chunksize = 10)","b3029870":"#as we can see we dont have to do anything to handle compressed files\nfiles_to_read = glob.glob('rating_k*.csv.gz')\nprint('Files to load: {}'.format(len(files_to_read)))","0f14860b":"%%time\ndfs = []\nfor f in files_to_read:\n    dfs.append(pd.read_csv(f))","72fa73da":"%%time\nbag = dask_bag.from_sequence(files_to_read)\ndfs = bag.map(lambda f: pd.read_csv(f)).compute(scheduler='threads')","5a0d259f":"%%time\nratings = pd.DataFrame()\nfor df in dfs:\n    ratings = ratings.append(df, ignore_index = True)\nprint(\"Ratings shape: {}\".format(ratings.shape))","cceff7c2":"%%time\nratings = pd.concat(dfs, ignore_index = True)\nprint(\"Ratings shape: {}\".format(ratings.shape))","dbf1ab0d":"ratings.head(10)","2eaf9f0a":"print('Ratings rows number: {}'.format(len(ratings)))\nprint('Ratings have NaN values: {}'.format(ratings.isna().values.any()))\nprint('Ratings have null values: {}'.format(ratings.isnull().values.any()))","0f82a432":"ratings.dtypes","a2850f25":"ratings.nunique()","f02a509c":"ratings['rating'].describe()","ffc528cd":"%%time\nratings['timestamp_dt'] = ratings['timestamp'].map(pd.to_datetime)","534987a0":"%%time\nddf = dask_df.from_pandas(ratings, npartitions=4)\nratings['timestamp_dt'] = ddf['timestamp'].map(np.datetime64, meta = pd.Series(dtype=np.dtype(str))).compute()\n","41794415":"%%time\nratings['year'] = ratings.apply(lambda x: x['timestamp_dt'].year, axis = 1)\nratings['month'] = ratings.apply(lambda x: x['timestamp_dt'].month, axis = 1)\nratings['day_of_week'] = ratings.apply(lambda x: x['timestamp_dt'].day_name(), axis = 1)","8f1c2a09":"%%time\nratings['year'] = ratings['timestamp_dt'].map(lambda x: x.year)\nratings['month'] = ratings['timestamp_dt'].map(lambda x: x.month)\nratings['day_of_week'] = ratings['timestamp_dt'].map(lambda x: x.day_name())","0e0d5ea2":"# del ratings['timestamp']\nratings = ratings.drop(columns = ['timestamp'])","b4e28103":"ratings.head()","3e8aa96e":"%%time\nby_movieId = ratings.groupby(['movieId'])['rating'].agg(['count', 'mean', 'max', 'min'])","43e3d382":"by_movieId.head()","73905997":"top_20_ranked = by_movieId.sort_values(by = ['count'], ascending  = False).iloc[0:20]","7ca0a93d":"%%time\nmovies = pd.read_csv('..\/input\/movie.csv')\ntop_20_ranked = pd.merge(left = top_20_ranked, right = movies, on = ['movieId'], how = 'inner')","9a1a9d91":"top_20_ranked","06bd8674":"top_20_ranked.plot.bar(x = 'title', y = 'count', figsize = (20, 7))","88c189a8":"#group by movieId and calculate mean and count\ndf = ratings.groupby(['movieId'])['rating'].agg(['mean', 'count'])\n#drop movies without votes number less than 500\ndf = df.loc[df['count'] > 1000]\n#sort to get top 5\ndf = df.sort_values(by = 'mean', ascending = False)\ndf = df.iloc[0:5]\n#merge with titles\ndf = pd.merge(left = df, right = movies, on=['movieId'], how = 'inner')\ndf","e7bc9f83":"#filter movies with correct titles\nfiltered_movies = movies.loc[(movies['title'].str.contains('Harry Potter') | movies['title'].str.contains('Star Wars'))]\n#merge with titles\ndf = pd.merge(left = ratings, right = filtered_movies, on=['movieId'], how = 'inner')\n#group by title and calculate mean and count\ndf = df.groupby(['title'])['rating'].agg(['mean', 'count'])\n# add a feature for sorting purposes\ndf['production_year'] = df.index.map(lambda x: int(x[-5:-1]))\ndf['m'] = df.index.map(lambda x: x[0])\n#drop movies without votes number less than 500 because we collect some documentaries about StarWars\ndf = df.loc[df['count'] > 500]\n#sort to show trend on plot\ndf = df.sort_values(by = ['m', 'production_year'])\n#plot\ndf.plot.bar(y = 'mean', figsize = (20, 7))","bcf7cb2a":"#filter movies with correct titles\nfiltered_movies = movies.loc[movies['title'].str.contains('Star Wars')]\n#merge with titles\ndf = pd.merge(left = ratings, right = filtered_movies, on=['movieId'], how = 'inner')\n#group by title and calculate mean and count\ndf = df.groupby(['title', 'year'])['rating'].agg(['mean', 'count'])\n#drop movies without votes number less than 500 because we collect some documentaries about StarWars\ndf = df.loc[df['count'] > 500]\n#sort to show trend on plot\ndf = df.sort_values(by = 'year')\n#plot\ndf.unstack(level = 0).plot(y = 'mean', figsize = (20, 7))\n","c7df7100":"More can be read here:\n* [Visualization](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/visualization.html)","25d3ab7c":"More can be read here:\n* [Concatenating objects](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/merging.html#concatenating-objects)","d872487b":"### Appending\nIf our dataset is stored in separate files, we can think that joining it together will be heavy operation. Nothing could be further from the truth!<br>\nPandas has method for appending new rows to DataFrame but it is expensive operation. After each append new DataFrame object must be created.\n","8a62daa9":"### Data checks\n\nWe can easily print first *n* rows of DataFrame using **head()** method.","e8a70bd3":" Pandas provides functions to handle missing data. For example, we can check against Nulls or NaNs and then replace them with calculated values or drop from data set.\n","1eb174e4":"# IO operations\nWith Pandas we can easily read files right into memory. It provides support for many data formats like CSVs, SQL, JSON or Python Pickle. It can even automatically decompress files compressed with GZIP,  XZ, ZIP and BZ2.  In this Kernel we will focus on CSV files.\n\nWhen we read csv with Pandas , we don't have to worry about header handling, columns separator, rows separator or column data types. Pandas takes care about everything with default settings, which are satisfying most use cases, but any customization is easy with its clear documentation.\n\n\n\n\nWe can effortlessly do things like:\n* skipping rows if we need to read from middle of the file\n* reading specific number of rows if we don't need to read the whole file\n* iterating through file if we want to process it chunk by chunk\n\n","66cf21a6":"## Purpose of this kernel\n\nIn this kernel I would like to introduce you to Pandas and his basic operations. We will see how to do some input\/output, transform data and plot meaningful information. In some places I will show how to boost operations by proper use of Pandas API or by Dask support. \n\n### It is not comprehensive tutorial about how to use Pandas and Dask. It is more like sneak peek to show what can be done and why worth to know both libraries.\nAfter each chapter I left links where you can find more information about described operations.","c233d900":"# Exploration examples\n\nNow when we know how to do some basic stuff in Pandas I want to show how it can be used to explore the dataset. With few lines of code we will find answers for following questions:\n* Which movies belongs to top 5 rated movies?\n* Is Harry Potter more popular than Star Wars?\n* Is Star Wars popularity decrease over time?","ac1b1873":"Now we will extract some meaningful information from timestamp to use them later. We want to have separate columns for year, month and day of week. ","e8f925d8":"# Data wrangling\n\nPandas is super powerful where comes to transforming and mapping data from one \"raw\" data form into another format more appropriate in analytic or machine learning.<br>\n","4e98327d":"Much better. \n\nNow we can easily drop unnecessary columns.","cfcc60a9":"More can be read here:\n* [Function application](https:\/\/pandas.pydata.org\/pandas-docs\/version\/0.23.4\/basics.html#function-application)\n* [Dask DataFrame](http:\/\/docs.dask.org\/en\/stable\/dataframe.html)\n* [Thread State and the Global Interpreter Lock](https:\/\/docs.python.org\/3\/c-api\/init.html#thread-state-and-the-global-interpreter-lock)\n* [How to deal with Python\u2019s GIL](https:\/\/realpython.com\/python-gil\/#how-to-deal-with-pythons-gil)","0e9144fc":"### Is Harry Potter more popular than Star Wars?","9f4c16d8":"More can be read here:\n* [IO Tools (Text, CSV, HDF5, \u2026)](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/io.html)\n* [Dask Bag](http:\/\/docs.dask.org\/en\/stable\/bag.html)","3479b9be":"# Visualizations\n\nAnd the last one feature left to show is plotting. With Pandas we can plot our data in different ways. We can create bar plots, histograms, scatter plots, pie plots any many more. It is also simple to customize our figures using flexible API. Worth to know that Pandas plots are backed by matplotlib so we can mix usage of both APIs together in our visualizations.\n\n\n","3c85586d":"### Merging\n\nPandas implements high performance, in memory, SQL-like joins. The documenation states that these methods perform significantly better than other open source implementations.<br><br>\nPandas provides a single function, **merge()**, as the entry point for all standard database join operations between DataFrame objects.<br>\nHere are several cases to consider which are very important to understand:\n\n* one-to-one joins: for example when joining two DataFrame objects on their indexes (which must contain unique values).\n* many-to-one joins: for example when joining an index (unique) to one or more columns in a different DataFrame.\n* many-to-many joins: joining columns on columns.\n\nWe can now join movie titles with movieIds.","14fe9dcb":"**Ouch!**\n\nLast operation was quite long.  Often it is much faster to map single column than apply a function to each row.","bcf78bb5":"# Pandas&Dask\n\n## Python's role in Data Science\n\nPython has grown to become the dominant language, both in data analytics and general programming. It is considered as the best choice for beginners because of his simple syntax and low entry level. There are a lot of online courses which helps to learn programming and Python.  \n\nDespite this Python isn't only for newbies, it is broadly adopted in many places like web applications, cloud services or machine learning systems.\n\n<img src=\"https:\/\/zgab33vy595fw5zq-zippykid.netdna-ssl.com\/wp-content\/uploads\/2017\/09\/growth_major_languages-1-1024x878.png\" width=\"612\" height=\"440\" \/>\n\nBut what would be a language without his environment like libraries and utils packages. \n\nPython has some great support for numerical computations thanks to Numpy. There is also valuable Scikit-Learn which helps build machine learning models. Plotting and visualizations can be done by Matplotlib. A lot of web applications are backed by Django which is high-level Python Web framework.\n\nBut Pandas appears to be most popular these days.\n\n\n\n<img src=\"https:\/\/zgab33vy595fw5zq-zippykid.netdna-ssl.com\/wp-content\/uploads\/2017\/09\/related_tags_over_time-1-1024x1024.png\" width=\"512\" height=\"512\" \/>\n\n\n## Pandas\n\nPandas is a Python package providing fast, flexible and expressive data structures designed to make wirking with relational or labeled data both and intuitive. Its goal is to be the most powerful and flexible open source data analysis tool available in any language. Quite ambitious but it is already well on its way toward this goal.\n\n## Dask\n\nHowever, these packages were not designed to scale beyond a single machine. Dask was developed to scale these packages and the surrounding ecosystem. It works with the existing Python ecosystem to scale it to multi-core machines and distributed clusters.\n\n","7c36cd1c":"More can be read here:\n* [Group By: split-apply-combine](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/groupby.html)","155623c5":" More can be read here: \n * [Essential Basic Functionality](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/basics.html#)\n * [Working with missing data](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/missing_data.html)\n * [Descriptive statistics](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/basics.html#descriptive-statistics)","909b18c0":" There are also some more descriptive methods like **describe()** which prints basic statistics and **nunique()** to count unique values.","d59d3c08":"### Which movies belongs to The 5 highest rated movies?","859a0cfe":"There is a more efficient way to connect DataFrames.\n\n\n### Concatenating\n\nThe **concat()** function does all of the heavy lifting of performing concatenation operations along an axis while performing optional set logic of the indexes.<br>\nThis operation can be visualized with image. We have date frames which should be connected to single one. More can be read in the documentation. \n![](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/_images\/merging_concat_basic.png)","00d8f64a":"### Is Star Wars popularity decrease over time?","8ade9aa0":"### Transforming\n\nPandas has three main transforming functions:\n* **apply()** - As the name suggests, applies a function along any axis of the DataFrame\n* **map()**  - It iterates over each element of a Series.\n* **applymap()** - This helps to apply a function to each element of dataframe\n","9b6cfcf5":"### Groupping\n\nAnalysing groups of entities in Pandas is ultra simple, it supplies SQL-like API for groupBy operations.<br>\n\nBy \u201cgroup by\u201d we are referring to a process involving one or more of the following steps:\n\n* Splitting the data into groups based on some criteria.\n* Applying a function to each group independently.\n* Combining the results into a data structure.\n\nThe first step is pretty obvious, we split whole data set into groups by selected property. In the apply step, we might wish to do one of the following: aggregation, transformation or filtration. <br>\n\nBelow we **groupBy()** ratings by *movieId* to find out 20 most ranked movies. We also will use **sort_values()** method to sort result by descending order.","610d788e":"There are situations where Pandas is not good enough. When our data set is split into many csv files, reading them one by one can take a lot of our precious time. But what can we do?\n\nIf we have an understanding of multithreading, we can figure out, that we can read files in asynchronous way. If we are not  familiar with Python threading support, we can google \"python multithreading support\"  and read some blog posts. I am always scared how many lines of code are needed to do such a simple thing. But don't worry, let me present Dask Bag.\n\n\n### Dask Bag\n\nDask Bag implements operations like map, filter, fold and groupby on collections of Python objects. It does this in parallel and it is similar to a PySpark. Dask bags are often used to parallelize simple computations.\n\n\n\n\nExecution on bags provide two benefits:\n* Parallel: data is split up, allowing multiple cores to execute in parallel. Dask Bag uses the multiprocessing scheduler by default which is sometimes inadequate for use.\n* Iterating: data processes lazily, allowing smooth execution of larger-than-memory data\n\n\n","978a1489":"**Pfffff!**\n\nWe could take a nap, while were waiting for results.<br>\nBut we can try use Dask again to optimize this operation. Let me introduce Dask DataFrame.\n\n### Dask DataFrame\n\nA Dask DataFrame is a large parallel DataFrame composed of many smaller PandasDataFrames, split along the index. These Pandas DataFrames may live on disk for larger-than-memory computing on a single machine, or on many different machines in a cluster. One Dask DataFrame operation triggers many operations on the constituent Pandas DataFrames.\n\nDask DataFrame application programming interface is a subset of the Pandas API.\n\nPros:\n* Manipulating large datasets, even when those datasets don\u2019t fit in memory\n* Accelerating long computations by using many cores\n* Distributed computing on large datasets with standard Pandas operations like groupby, join, and time series computations\n\nThere are few things to condiser when we want to use Dask DataFrame:\n* If your dataset fits comfortably into RAM on your laptop, then you may be better off just using Pandas. There may be simpler ways to improve performance than through parallelism\n* If your dataset doesn\u2019t fit neatly into the Pandas tabular model, then you might find more use in dask.bag or dask.array\n* If you need functions that are not implemented in Dask DataFrame, then you might want to look at dask.delayed which offers more flexibility\n* If you need a proper database with all that databases offer you might prefer something like Postgres\n\nYes, you just read that we should use it when dataset fits into memory. They are mostly right, parallelism is really poor in Python thanks to GIL. Because The Python interpreter is not fully thread-safe, there is a global lock, called GIL, that must be held by the current thread before it can safely access Python objects. What does is mean? This means that only one thread can be in a state of execution at any point in time. But there are some exceptions, the lock is released around potentially blocking I\/O operations and some of the libraries (Numpy) are implemented to realease GIL properly. You can find more information below in the link section.  \n\n\nAnyway, we can sometimes optimize code by using parallel DataFrame. Using Numpy functions in mapping helps to reduce execution time."}}