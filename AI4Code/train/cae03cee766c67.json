{"cell_type":{"24ca5bf6":"code","bae96aae":"code","35e81626":"code","a4789889":"code","bd4df7e8":"code","1692ccad":"code","4d4d7c04":"code","d92ace13":"code","25f9adf8":"code","59b7bfbb":"code","75b37937":"code","23c83bdd":"code","0a103c7e":"code","df5c64fe":"code","4000814e":"code","a90c762d":"code","b3a7222f":"code","54a677bc":"code","c50f441c":"code","e4d82c2f":"code","10f69fc4":"code","e29722ad":"code","35ece382":"code","48777c8f":"code","60ea9a4b":"code","38e53fd8":"code","197cb3d4":"code","9de65e65":"code","a4535fc1":"code","19edbd00":"code","dd3ca139":"code","fce8c4ce":"code","0bea55ef":"markdown"},"source":{"24ca5bf6":"import tensorflow as tf\nimport pandas as pd\nimport os\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom keras import Sequential\nfrom keras import layers\nfrom keras import backend as K\nfrom keras.layers.core import Dense\nfrom keras import regularizers\nfrom keras.layers import Dropout\nfrom keras.constraints import max_norm","bae96aae":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nimport warnings\nimport gc\nimport time\nimport sys\nimport datetime\nimport PIL, os, numpy as np, math, collections, threading, json,  random, scipy, cv2\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm\nfrom sklearn.metrics import mean_squared_error\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nfrom sklearn import metrics\n# Plotly library\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nfrom plotly import tools\ninit_notebook_mode(connected=True)\npd.set_option('display.max_columns', 500)\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, KFold, TimeSeriesSplit\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn import metrics\nimport gc\nfrom catboost import CatBoostClassifier\nfrom tqdm import tqdm_notebook\nimport plotly.offline as py","35e81626":"# Import data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","a4789889":"train.shape","bd4df7e8":"test.shape","1692ccad":"#Check num of cases in label \nprint(train.target.value_counts())\nprint(train.target.value_counts()[1]\/train.target.value_counts()[0])","4d4d7c04":"train_features = train.drop(['target', 'ID_code'], axis=1)\ntrain_targets = train['target']\ntest_features = test.drop(['ID_code'], axis=1)","d92ace13":"train.describe()","25f9adf8":"train_features= pd.DataFrame(train_features)","59b7bfbb":"from sklearn.preprocessing import power_transform\nfeatures = [c for c in train.columns if c not in ['ID_code', 'target']]\nfor feature in features:\n    train_features['mean_'+feature] = (train_features[feature].mean()-train_features[feature])\n    train_features['z_'+feature] = (train_features[feature] - train_features[feature].mean())\/train_features[feature].std(ddof=0)\n    train_features['sq_'+feature] = (train_features[feature])**2\n    train_features['sqrt_'+feature] = (train_features['sq_'+feature])**(1\/4)","75b37937":"for feature in features:\n    test_features['mean_'+feature] = (test_features[feature].mean()-test_features[feature])\n    test_features['z_'+feature] = (test_features[feature] - test_features[feature].mean())\/test_features[feature].std(ddof=0)\n    test_features['sq_'+feature] = (test_features[feature])**2\n    test_features['sqrt_'+feature] = (test_features['sq_'+feature])**(1\/4)","23c83bdd":"train_features.head()","0a103c7e":"# Feature Scaling\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler\nsc = StandardScaler()\ntrain_features = sc.fit_transform(train_features)\ntest_features = sc.transform(test_features)","df5c64fe":"gc.collect()","4000814e":"# Add RUC metric to monitor NN\ndef auc(y_true, y_pred):\n    auc = tf.metrics.auc(y_true, y_pred)[1]\n    K.get_session().run(tf.local_variables_initializer())\n    return auc","a90c762d":"input_dim = train_features.shape[1]\ninput_dim","b3a7222f":"from keras import callbacks\nfrom sklearn.metrics import roc_auc_score\n\nclass printAUC(callbacks.Callback):\n    def __init__(self, X_train, y_train):\n        super(printAUC, self).__init__()\n        self.bestAUC = 0\n        self.X_train = X_train\n        self.y_train = y_train\n        \n    def on_epoch_end(self, epoch, logs={}):\n        pred = self.model.predict(np.array(self.X_train))\n        auc = roc_auc_score(self.y_train, pred)\n        print(\"Train AUC: \" + str(auc))\n        #pred = self.model.predict(self.validation_data[0])\n        #auc = roc_auc_score(self.validation_data[1], pred)\n        #print (\"Validation AUC: \" + str(auc))\n        if (self.bestAUC < auc) :\n            self.bestAUC = auc\n            self.model.save(\"bestNet.h5\", overwrite=True)\n        return","54a677bc":"from keras.layers import Dense,Dropout,BatchNormalization\nfrom keras import regularizers\nimport keras\nfrom keras.callbacks import LearningRateScheduler,EarlyStopping\nimport tensorflow as tf\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\nfrom keras.constraints import max_norm","c50f441c":"def step_decay(epoch):\n   initial_lrate = 0.1\n   drop = 0.5\n   epochs_drop = 10.0\n   lrate = initial_lrate * math.pow(drop,  \n           math.floor((1+epoch)\/epochs_drop))\n   return lrate\nlrate = LearningRateScheduler(step_decay)","e4d82c2f":"class LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n       self.losses = []\n       self.lr = []\n \n    def on_epoch_end(self, batch, logs={}):\n       self.losses.append(logs.get('loss'))\n       self.lr.append(step_decay(len(self.losses)))","10f69fc4":"import random\nfrom keras import models\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.layers.advanced_activations import PReLU,LeakyReLU\n#kernel_regularizer=regularizers.l2(0.01)\nmodel = models.Sequential()\nmodel.add(Dense(64, activation='relu',input_shape=(train_features.shape[1],)))\n#model.add(PreLU(alpha=.001))\nmodel.add(Dropout(0.6))\nmodel.add(BatchNormalization())\nmodel.add(Dense(32,activation='relu'))\n#model.add(PreLU(alpha=.001))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(Dense(1,activation='sigmoid'))\n\nannealer = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** x)","e29722ad":"def auc(y_true, y_pred):\n    try:\n        return tf.py_func(metrics.roc_auc_score, (y_true, y_pred), tf.double)\n    except:\n        return 0.5","35ece382":"model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n","48777c8f":"gc.collect()","60ea9a4b":"\ndef mixup_data(x, y, alpha=1.0):\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    sample_size = x.shape[0]\n    index_array = np.arange(sample_size)\n    np.random.shuffle(index_array)\n    \n    mixed_x = lam * x + (1 - lam) * x[index_array]\n    mixed_y = (lam * y) + ((1 - lam) * y[index_array])\n#     print((1 - lam) * y[index_array])\n#     print((lam * y).shape,((1 - lam) * y[index_array]).shape)\n    return mixed_x, mixed_y\n\ndef make_batches(size, batch_size):\n    nb_batch = int(np.ceil(size\/float(batch_size)))\n    return [(i*batch_size, min(size, (i+1)*batch_size)) for i in range(0, nb_batch)]\n\n\ndef batch_generator(X,y,batch_size=128,shuffle=True,mixup=False):\n    sample_size = X.shape[0]\n    index_array = np.arange(sample_size)\n    \n    while 1:\n        if shuffle:\n            np.random.shuffle(index_array)\n        batches = make_batches(sample_size, batch_size)\n        for batch_index, (batch_start, batch_end) in enumerate(batches):\n            batch_ids = index_array[batch_start:batch_end]\n            X_batch = X[batch_ids]\n            y_batch = y[batch_ids]\n            \n            if mixup:\n                X_batch,y_batch = mixup_data(X_batch,y_batch,alpha=1.0)\n#                 print(X_batch.shape,y_batch.shape)\n                \n                \n            yield X_batch,y_batch\n            \nfrom sklearn.model_selection import StratifiedShuffleSplit\nbatch_size = 512\nloss_history = LossHistory()\nlrate = LearningRateScheduler(step_decay)\ncallbacks_list = [EarlyStopping(monitor='val_loss', patience=10,mode='min'),loss_history, annealer]\nsss = StratifiedShuffleSplit(n_splits=10)\nfor train_index, test_index in sss.split(train_features, train_targets):\n    X_train, X_val = train_features[train_index], train_features[test_index]\n    Y_train, Y_val = train_targets.values[train_index], train_targets.values[test_index]\n#    print(\"{} iteration\".format(i+1))\n#     history= model.fit(X_train,Y_train,batch_size=512,epochs=500,verbose=1,callbacks=callbacks_list,validation_data=(X_val,Y_val))\n    \n    tr_gen = batch_generator(X_train,Y_train,batch_size=batch_size,shuffle=True,mixup=True)\n    \n    model.fit_generator(\n            tr_gen, \n            steps_per_epoch=np.ceil(float(len(X_train)) \/ float(batch_size)),\n            nb_epoch=30000, \n            verbose=1, \n            callbacks=callbacks_list, \n            validation_data=(X_val,Y_val),\n            max_q_size=10,\n            )\n    del X_train, X_val, Y_train, Y_val\n    gc.collect()","38e53fd8":"# Try early stopping\n#from keras.callbacks import EarlyStopping\n#callback = EarlyStopping(monitor='loss', min_delta=0, patience=10, verbose=0, mode='auto', baseline=None, restore_best_weights=True)","197cb3d4":"train_features.shape","9de65e65":"del train, train_features\ngc.collect()","a4535fc1":"id_code_test = test['ID_code']\n# Make predicitions\npred = model.predict(test_features)\npred_ = pred[:,0]","19edbd00":"print(train['target'].mean())\npred.mean()","dd3ca139":"# To CSV\nmy_submission = pd.DataFrame({\"ID_code\" : id_code_test, \"target\" : pred_})","fce8c4ce":"my_submission.to_csv('submission.csv', index = False, header = True)","0bea55ef":"Forked from @VisheshShrivastav. Using the basic framework from vishesh's Kernel "}}