{"cell_type":{"71b444d4":"code","7eada8d8":"code","af354815":"code","e4861739":"code","7b829b96":"code","928dcde1":"code","b73f52af":"code","92468069":"code","bcee9eef":"code","766827ce":"code","fec54354":"code","b77eda43":"code","07b4e443":"code","c070a2bc":"code","3feeda42":"code","06696112":"code","981479d7":"code","f3659155":"code","31c59e96":"code","1dc251cf":"code","bf46843d":"code","8d89d249":"code","60f93a59":"code","98aa39f0":"code","fddd556e":"code","7b06e648":"code","e4ddeea2":"code","8b93fa99":"code","3b95fcef":"code","6f7072cd":"code","bcc30e38":"code","a492a85e":"code","3c2ffba7":"code","70e82c67":"code","f3761dd2":"code","37e0977f":"code","b091eef3":"code","af64975c":"code","56c4b692":"code","c9c258a7":"code","30ebe794":"code","627cb4e7":"code","7dd35b49":"code","ce660b83":"code","4453178a":"code","7d2f25c0":"code","1f28dd92":"code","34cdd090":"code","5482835e":"code","233efe3d":"code","f8956b0b":"code","b93b2885":"code","0bd9558a":"code","f1f32d0a":"code","a0eba026":"code","928020df":"code","19eb140b":"code","d4e4124f":"code","67e735c4":"markdown","1b3594f4":"markdown","da6c6122":"markdown","8b534756":"markdown","2f35bd00":"markdown","c470b55e":"markdown","cb1481ee":"markdown","4742eef1":"markdown","d28aa760":"markdown","a6a7ea18":"markdown","800b7f94":"markdown","192c21ce":"markdown","7943ea5c":"markdown","e34c9a72":"markdown","c5f69d10":"markdown","392fc989":"markdown"},"source":{"71b444d4":"# general packages used\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n% matplotlib inline\nimport pycountry\nimport re\nimport os\n\nimport nltk\nimport nltk.stem\nfrom nltk.tokenize import word_tokenize, sent_tokenize, PunktSentenceTokenizer, RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom string import punctuation","7eada8d8":"# convenient way to not see annoying warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","af354815":"# some pre-set defaults for plotting\n\nfrom matplotlib.pylab import rcParams\nplt.rcParams['xtick.labelsize']=14\nplt.rcParams['ytick.labelsize']=14","e4861739":"data = pd.read_csv('..\/input\/un-general-debates\/un-general-debates.csv', index_col=0) # data is from Kaggle and downloaded to the same directory where the notebook is","7b829b96":"data.head() # quick peek, what the data looks like","928dcde1":"print(\"There are {} observations and {} features in this dataset. \\n\".format(data.shape[0],data.shape[1]))","b73f52af":"# add column for full country name\ncountry_name = []\nfor code in data['country']:\n    try:\n        country_name.append(pycountry.countries.lookup(code).name)\n    except LookupError:\n        country_name.append('')\ndata['country_name'] = country_name","92468069":"# remove data with null value in year column\ndata = data[data['year'].notnull()]\n\n# remove data with null values in country_name column\ndata = data[data['country_name'].notnull()]","bcee9eef":"print(\"There are {} countries speaking in this dataset such as {}... \\n\".format(len(data.country_name.unique()),\n                                                                                      \", \".join(data.country_name.unique()[0:5])))","766827ce":"# lets do some cleaning of the text, remove unusual symbols from the text, creating new text_clean column\n\n# convert text data to lower case (for easier analysis)\ndata['text_clean'] = data['text'].str.lower()\n\ndef clean(s):    \n    # Remove any tags:\n    cleaned = re.sub(r\"(?s)<.?>\", \" \", s)\n    # Keep only regular chars:\n    cleaned = re.sub(r\"[^A-Za-z0-9(),*!?\\'\\`]\", \" \", cleaned)\n    # Remove unicode chars\n    cleaned = re.sub(\"\\\\\\\\u(.){4}\", \" \", cleaned)\n    return cleaned.strip()\n\n# clean text\ndata['text_clean'] = data.text_clean.apply(lambda x: clean(x))","fec54354":"# create a unique ID for index\n\ndata['ID'] = range(1, len(data.index)+1)\n\ndata = data.set_index('ID')","b77eda43":"# tockenize text\ndata['token'] = data['text_clean'].apply(word_tokenize)","07b4e443":"stop_words = set(stopwords.words('english'))\n# I noticed that \"'s\" is not included in stopwords, while I think it doesn't bring much meaning in a text, so I'll add it to the set to remove from the cleaned tokens.\nstop_words.add(\"'s\")\nstop_words.add(\"'\")\nstop_words.add(\"-\")\nstop_words.add(\"'\")\ndata['clean'] = data['token'].apply(lambda x: [w for w in x if not w in stop_words and not w in punctuation])","c070a2bc":"data.head() #  check out how data looks like now, after all this cleaning and munging","3feeda42":"all_per_year = data.groupby('year').agg({'year': 'mean', 'clean': 'sum'})\n# all_per_year.to_csv('all_per_year.csv', sep='|') # in case you want to write it out as csv file","06696112":"rcParams['figure.figsize'] = 12, 8 # I like bigger plots\n\nfor i, row in all_per_year.iterrows():\n    sess = dict(nltk.FreqDist(row['clean']))\n    sort_sess = sorted(sess.items(), key=lambda x: x[1], reverse=True)[0:10] # top how many? \n    plt.barh(range(len(sort_sess)), [val[1] for val in sort_sess], align='center', height=0.6, color='#462a0d')\n    plt.yticks(range(len(sort_sess)), [val[0] for val in sort_sess])\n    plt.yticks(rotation=0)\n    plt.yticks(fontsize=16)\n    plt.title(\"10 most used words in %d's session\" % row['year'],fontsize=24)\n    ax = plt.gca()\n    ax.set_facecolor('#F4EEDA')\n    ax.invert_yaxis() # makes sure it is ordered from most to least\n    plt.show()","981479d7":"# get frequencies by year\n\nfreqs = {}\nfor i, speech in data.iterrows():\n    year = speech['year']\n    for token in speech['clean']:\n        if token not in freqs:\n            freqs[token] = {\"total_freq\":1, year:1}\n        else:\n            freqs[token][\"total_freq\"] += 1\n            if not freqs[token].get(year):\n                freqs[token][year] = 1\n            else:\n                freqs[token][year] += 1","f3659155":"freqs_df = pd.DataFrame.from_dict(freqs, orient='index')\nfreqs_df['word'] = freqs_df.index","31c59e96":"# Example of data for the \"clean\" of the word \"refuge\"\nfreqs_df[freqs_df.index == \"refuge\"]","1dc251cf":"new_cols = [\"total_freq\", \"word\"] + sorted(freqs_df.columns.tolist()[1:-1])\nfreqs_df = freqs_df[new_cols]\n\nfreqs_df = freqs_df.sort_values('total_freq', ascending=False)\n\n# freqs_df.to_csv('all_per_year_clean.csv', sep='|') #  in case you want to write this out as csv file\n\nfreqs_df.head()","bf46843d":"# you can plot words frequency over time\n\nfreqs_df[freqs_df['word'].isin(['nations'])].iloc[:, 1:47].transpose().iloc[1:].plot(title=\"Nations\")\naxes = plt.gca()\naxes.set_facecolor('#F4EEDA')\nplt.show()","8d89d249":"# you can plot words frequency over time\n\nfreqs_df[freqs_df['word'].isin(['inequality', 'refugee', 'humanitarian', 'immigration', 'freedom'])].iloc[:, 1:47].transpose().iloc[1:].plot(title=\"People\")\naxes = plt.gca()\naxes.set_facecolor('#F4EEDA')\nplt.show()","60f93a59":"import re\nfrom nltk.corpus import stopwords as nltk_stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import PunktSentenceTokenizer, RegexpTokenizer\n\nfrom gensim.corpora import Dictionary\nfrom gensim.models import LdaModel\n\nimport pyLDAvis\nimport pyLDAvis.gensim as gensimvis","98aa39f0":"class SentenceTokenizer(PunktSentenceTokenizer):\n    pass\n\n\nclass ParagraphTokenizer(object):\n    \"\"\"A simple paragraph tokenizer that creates a paragraph break whenever\n    the newline character appears between two sentences.\"\"\"\n\n    sentence_tokenizer = SentenceTokenizer()\n\n    def span_tokenize(self, text):\n        '''Returns a list of paragraph spans.'''\n        sentence_spans = list(self.sentence_tokenizer.span_tokenize(text))\n        breaks = []\n        for i in range(len(sentence_spans) - 1):\n            sentence_divider = text[sentence_spans[i][1]: \\\n                sentence_spans[i+1][0]]\n            if '\\n' in sentence_divider:\n                breaks.append(i)\n        paragraph_spans = []\n        start = 0\n        for break_idx in breaks:\n            paragraph_spans.append((start, sentence_spans[break_idx][1]))\n            start = sentence_spans[break_idx+1][0]\n        paragraph_spans.append((start, sentence_spans[-1][1]))\n        return paragraph_spans\n","fddd556e":"# splitting data into paragraphs\n\ndebates = data.sort_values(['year', 'country_name']).reset_index(drop=True)\n\nparagraph_tokenizer = ParagraphTokenizer()\nparagraphs = pd.Series(\n    debates.text\n    .apply(lambda x: [x[start:end] for start, end\n                        in paragraph_tokenizer.span_tokenize(x)])\n    .apply(lambda x: pd.Series(x))\n    .stack()\n    .reset_index(level=1, drop=True), name='text')\ndebates_paragraphs = (debates\n                        .drop('text', axis=1)\n                        .join(paragraphs)\n                        .reset_index())\n    # Must retain this new index to preserve ordering of paragraphs within\n    # each speech.\ndebates_paragraphs.index.name = 'paragraph_index'\n","7b06e648":"debates_paragraphs.head()","e4ddeea2":"# lets do some cleaning of the text, remove unusual symbols from the text, creating new text_clean column\n\n# convert text data to lower case (for easier analysis)\ndebates_paragraphs['text_clean'] = debates_paragraphs['text'].str.lower()\n\ndef clean(s):    \n    # Remove any tags:\n    cleaned = re.sub(r\"(?s)<.?>\", \" \", s)\n    # Keep only regular chars:\n    cleaned = re.sub(r\"[^A-Za-z0-9(),*!?\\'\\`]\", \" \", cleaned)\n    # Remove unicode chars\n    cleaned = re.sub(\"\\\\\\\\u(.){4}\", \" \", cleaned)\n    return cleaned.strip()\n\n# clean text\ndebates_paragraphs['text_clean'] = debates_paragraphs.text_clean.apply(lambda x: clean(x))\n\n# tockenize text\ndebates_paragraphs['token'] = debates_paragraphs['text_clean'].apply(word_tokenize)\n\nstop_words = set(stopwords.words('english'))\n# I noticed that \"'s\" is not included in stopwords, while I think it doesn't bring much meaning in a text, so I'll add it to the set to remove from the cleaned tokens.\nstop_words.add(\"'s\")\nstop_words.add(\"'\")\nstop_words.add(\"-\")\nstop_words.add(\"'\")\ndebates_paragraphs['clean'] = debates_paragraphs['token'].apply(lambda x: [w for w in x if not w in stop_words and not w in punctuation])","8b93fa99":"debates_paragraphs.head()","3b95fcef":"texts = debates_paragraphs['clean']","6f7072cd":"# Remove numbers, but not words that contain numbers.\ntexts = [[token for token in text if not token.isnumeric()] for text in texts]\n\n# Remove words that are only one character.\ntexts = [[token for token in text if len(token) > 1] for text in texts]","bcc30e38":"# Create a dictionary representation of the documents.\ndictionary = Dictionary(texts)\n\n# Filter out words that occur less than 100 documents.\ndictionary.filter_extremes(no_below=100)","a492a85e":"# Vectorize data.\n\n# Bag-of-words representation of the documents.\ncorpus = [dictionary.doc2bow(text) for text in texts]\n\nprint('Number of unique tokens: %d' % len(dictionary))\nprint('Number of documents: %d' % len(corpus))","3c2ffba7":"# Train LDA model.\n\n# Set training parameters.\nnum_topics = 15\nchunksize = 2000\npasses = 20\niterations = 400\neval_every = None  # Don't evaluate model perplexity, takes too much time.\n\n# Make a index to word dictionary.\ntemp = dictionary[0]  # This is only to \"load\" the dictionary.\nid2word = dictionary.id2token\n\n%time model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n                       alpha='auto', eta='auto', \\\n                       iterations=iterations, num_topics=num_topics, \\\n                       passes=passes, eval_every=eval_every)","70e82c67":"vis_data = gensimvis.prepare(model, corpus, dictionary)\n\n# warnings.filterwarnings(\"ignore\")\npyLDAvis.display(vis_data)","f3761dd2":"# model  3 -  human rights, \nfreqs_df[freqs_df['word'].isin(['human', 'right', 'nation', 'security', 'united'])].iloc[:, 1:47].transpose().iloc[1:].plot(title=\"Human Rights\")\naxes = plt.gca()\naxes.set_facecolor('#F4EEDA')\nplt.show()","37e0977f":"#model 10 Nuclear Destruction\nfreqs_df[freqs_df['word'].isin(['weapon', 'nuclear', 'proliferation', 'arm', 'destruction', 'education'])].iloc[:, 1:47].transpose().iloc[1:].plot(title=\"Nuclear Destruction\")\naxes = plt.gca()\naxes.set_facecolor('#F4EEDA')\nplt.show()","b091eef3":"#model 8 Economy\nfreqs_df[freqs_df['word'].isin(['debt', 'economy', 'trade', 'financial', 'growth'])].iloc[:, 1:47].transpose().iloc[1:].plot(title=\"Economy\")\naxes = plt.gca()\naxes.set_facecolor('#F4EEDA')\nplt.show()","af64975c":"# creating subset of just \"refuge\" and \"immigration\"\n\ntext_refuge = debates_paragraphs[debates_paragraphs['text'].str.contains(\"refuge\")]\ntext_immigrant = debates_paragraphs[debates_paragraphs['text'].str.contains(\"immigra\")]\nframes = [text_refuge, text_immigrant]\ndebates_immigration = pd.concat(frames)\n\n# text_refuge.to_csv('text_refuge.csv', sep='|') # in case you want to write this out as csv file\n\ndebates_immigration.shape","56c4b692":"texts_immigration = debates_immigration['clean']","c9c258a7":"# Remove numbers, but not words that contain numbers.\ntexts_immigration = [[token for token in text if not token.isnumeric()] for text in texts_immigration]\n\n# Remove words that are only one character.\ntexts_immigration = [[token for token in text if len(token) > 1] for text in texts_immigration]","30ebe794":"# Create a dictionary representation of the documents.\ndictionary = Dictionary(texts_immigration)\n\n# Filter out words that occur less than 20 documents, or more than 50% of the documents.\ndictionary.filter_extremes(no_below=10, no_above=0.50)","627cb4e7":"# Vectorize data.\n\n# Bag-of-words representation of the documents.\ncorpus = [dictionary.doc2bow(text) for text in texts_immigration]","7dd35b49":"print('Number of unique tokens: %d' % len(dictionary))\nprint('Number of documents: %d' % len(corpus))","ce660b83":"# Train LDA model for just immigration and refugee mentions.\n\n# Set training parameters.\nnum_topics = 10\nchunksize = 2000\npasses = 20\niterations = 400\neval_every = None  # Don't evaluate model perplexity, takes too much time.\n\n# Make a index to word dictionary.\ntemp = dictionary[0]  # This is only to \"load\" the dictionary.\nid2word = dictionary.id2token\n\n%time model = LdaModel(corpus=corpus, id2word=id2word, chunksize=chunksize, \\\n                       alpha='auto', eta='auto', \\\n                       iterations=iterations, num_topics=num_topics, \\\n                       passes=passes, eval_every=eval_every)","4453178a":"vis_data = gensimvis.prepare(model, corpus, dictionary)\n\n# warnings.filterwarnings(\"ignore\")\npyLDAvis.display(vis_data)","7d2f25c0":"#model 2 Israeli\/Palestinian Conflict\nfreqs_df[freqs_df['word'].isin(['right', 'problem', 'israel', 'palestinian'])].iloc[:, 1:47].transpose().iloc[1:].plot(title=\"Israeli\/Palestinian Conflict\")\naxes = plt.gca()\naxes.set_facecolor('#F4EEDA')\nplt.show()","1f28dd92":"#model 4 - Economic issues\nfreqs_df[freqs_df['word'].isin(['economic', 'burden', 'financial', 'unemployment', 'need'])].iloc[:, 1:47].transpose().iloc[1:].plot(title=\"Economic Issues\")\naxes = plt.gca()\naxes.set_facecolor('#F4EEDA')\nplt.show()","34cdd090":"#model 1 Afghanistan\nfreqs_df[freqs_df['word'].isin(['afghanistan', 'problem', 'government', 'situation'])].iloc[:, 1:47].transpose().iloc[1:].plot(title=\"Afghanistan\")\naxes = plt.gca()\naxes.set_facecolor('#F4EEDA')\nplt.show()","5482835e":"rcParams['figure.figsize'] = 12, 8 # I like bigger plots\n\n# some more plots\n\nfreqs_df[freqs_df['word'].isin(['woman', 'man', 'gender', 'humankind', 'mankind'])].iloc[:, 1:47].transpose().iloc[1:].plot(title=\"People\")\naxes = plt.gca()\naxes.set_facecolor('#F4EEDA')\nplt.show()","233efe3d":"text_refuge_2015 = debates_immigration[debates_immigration['year']== 2015]","f8956b0b":"text_refuge_2015.shape","b93b2885":"# text_refuge_2015.to_csv('text_refuge_2015.csv', sep='|')","0bd9558a":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom palettable.matplotlib import Inferno_8\nimport random","f1f32d0a":"def color_func(word, font_size, position, orientation, random_state=None, **kwargs):\n    return tuple(Inferno_8.colors[random.randint(1,6)])","a0eba026":"text_refuge_2015[[\"country_name\", \"text\"]].head()","928020df":"# we can see, what each country said \n\ntext1 = text_refuge_2015.text[279508]\n\nwordcloud = WordCloud(font_path='..\/input\/truefont\/ELEPHNT.TTF', background_color=\"#F4EEDA\").generate(text1)\nwordcloud.recolor(color_func=color_func, random_state=2)\n\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","19eb140b":"text1 = text_refuge_2015.text[279506]\n\nwordcloud = WordCloud(font_path='..\/input\/truefont\/ELEPHNT.TTF', background_color=\"#F4EEDA\").generate(text1)\nwordcloud.recolor(color_func=color_func, random_state=2)\n\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","d4e4124f":"# or take a look of all paragraphs mentioning refugee or immigraion in 2015\n\ntext2 = \" \".join(debate for debate in text_refuge_2015.text)\n\nwordcloud = WordCloud(font_path='..\/input\/truefont\/ELEPHNT.TTF', background_color=\"#F4EEDA\").generate(text2)\n\nwordcloud.recolor(color_func=color_func, random_state=2)\n\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","67e735c4":"## Introduction","1b3594f4":"This project is an analyses of General Debates in the United Nations General Assemly from 1970 to 2015. Although the depates touch many major issues in world politics, this project is looking for a narroer scope - immigration.\n\nI am using Machine Learning tools, to find out, how goverments sentiment has changed in regards to immigration during the time interval studied. If there have been any major shifts during that time and what is the general world view currently, when it comes to imiggiration.\n\nThe project is split to 5 parts: \n\nI - Short intoduction  \nII - EDA, familiarizing ourselves with the dataset  \nIII - Simple word aggregation by year  \nIV - Dynamic Topic Models (LDA) for the whole corpus and specifically for words Immigration and Refuge  \nV - Findings and Conclusion  ","da6c6122":"It is interesting to see, how word usages has changed over time. We can see that \"mankind\" is used less and less, which seems to have been replaced by \"humankind\". Countries are starting to talk about \"woman\" and \"gender\", and using generic \"man\", less. ","8b534756":"- The General Assembly is one of the six main organizations of the United Nations, the only one in which all Member States have equal representation: one nation, one vote.\n- All 193 Member States of the United Nations discuss and work together on a wide array of international issues covered by the UN Charter, such as development, peace and security, international law, etc.\n- Each year, in September, all the Members meet in the General Assembly Hall in New York for the annual General Assembly session and debates. \n\n### Preamble of the UN Charter on Human Rights:\n\n_\"We the peoples of the United Nations determined \u2026 to reaffirm faith in fundamental human rights, in the dignity and worth of the human person, in the equal rights of men and women and of nations large and small..._\n\n_And for these ends \u2026 to practice tolerance and live together in peace with one another as good neighbours\"_\n\nBelow, I will try to find out, what does the data tell us - how have human rights issues been covered in the dabates over the last 45 years. ","2f35bd00":"## Findings and Conclusion","c470b55e":"Note, every time you run the LDA model, you will get different results, but the topics are distinct and relevat. Below I plotted some of the topics that I saw from one of the previous model runs.","cb1481ee":"I hope my approach has been useful\/intersting. If you have questions, please drop me an email at lilly.raud@gmail.com","4742eef1":"## Simple word aggregation by year","d28aa760":"The most talked about word was \"nations\", which was mentioned 124,780 times. Here how this looks over the years:","a6a7ea18":"As stated earlier, immigration was mentioned very seldom and in some years it was not mentioned at all. My guess is, that immigration is word used in USA more than in other countries, hence in the past has not come up as much. It would be very interesting to see, if this has changed in last few years.\n\nFinally, I will create a \"pretty picture\" of words used last year of the data, 2015. What was the most prominent topic, when contries were talking about immiration and refugees? ","800b7f94":"For LDA modeling, we need to split the text to paragraphs, so that we can capture more meaningful categories\/groups. ","192c21ce":"### LDA on Immigration \/ Refugee\n\nSince we saw, how few times the specific topic I was interested in was talked about during the dabates, I wanted to see, what were the topics talked, when immigration and refugees were mentioned. ","7943ea5c":"## NLP Project - UN General Debates\n\n### Lilianne Raud","e34c9a72":"I was interested in immigraion and refugees, as you can see, these words are not much used, especially immigration, which is missing some years all togehter.","c5f69d10":"## Dynamic Topic Models (LDA) for the whole corpus and specifically for words Immigration and Refuge","392fc989":"## EDA , familiarizing ourselves with the dataset"}}