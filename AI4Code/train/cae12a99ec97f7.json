{"cell_type":{"b98a5d88":"code","c097ca87":"code","d9f3e32c":"code","d82a25ea":"code","4f2001b9":"code","8638bc2e":"code","28131eb0":"code","d24ffc77":"code","6c95451d":"code","7ecea78e":"code","b959ef80":"code","43cbe6b9":"code","a68cffc6":"code","84d8a5e9":"code","a13007ad":"markdown","684577b1":"markdown","2c2df834":"markdown","c45549aa":"markdown","9bb0cb5d":"markdown","32e74288":"markdown","5690ea2a":"markdown","a16a99a5":"markdown"},"source":{"b98a5d88":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom keras.optimizers import Adam\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\nnp.set_printoptions(suppress=True)","c097ca87":"# Load the dataset\nraw_data = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\nraw_data.head()","d9f3e32c":"x_train, x_test, y_train, y_test = train_test_split(raw_data.drop(['Class','Time'],axis=1), raw_data.Class, test_size=0.15, stratify=raw_data.Class)","d82a25ea":"amount_mean = x_train.Amount.mean()\namount_std = x_train.Amount.std()\namount_max = x_train.Amount.max()\n\nx_train.Amount = x_train.Amount \/ amount_max\nx_train.Amount = (x_train.Amount - amount_mean) \/ amount_std\n\nx_test.Amount = x_test.Amount \/ amount_max\nx_test.Amount = (x_test.Amount - amount_mean) \/ amount_std","4f2001b9":"num_fraud = len(y_train.loc[y_train == 1])\nnum_genuine = len(y_train.loc[y_train == 0])\n\nprint(y_train.value_counts()) \nplt.pie([num_genuine, num_fraud],labels=[\"Genuine\", \"Fraud\"],autopct='%1.2f%%',startangle=45)\nplt.show()","8638bc2e":"smt = SMOTE()\nx_train, y_train = smt.fit_sample(x_train, y_train) ","28131eb0":"num_fraud = len(y_train[y_train == 1])\nnum_genuine = len(y_train[y_train == 0]) \n \nplt.pie([num_genuine, num_fraud],labels=[\"Genuine\", \"Fraud\"],autopct='%1.2f%%',startangle=45)\nplt.show()","d24ffc77":"model = Sequential()\nmodel.add(Dense(48, activation='tanh', input_shape=x_train.shape[1:]))\nmodel.add(BatchNormalization()) \nmodel.add(Dropout(0.25))\nmodel.add(Dense(48, activation='tanh'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\nmodel.add(Dense(48, activation='tanh'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25)) \nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","6c95451d":"model.compile(optimizer='adam', loss = 'binary_crossentropy', metrics=['accuracy'])\nhistory = model.fit(x_train, y_train, batch_size=32, epochs=10, shuffle=True)","7ecea78e":"y_predict = model.predict(x_test)","b959ef80":"precision, recall, threshold = precision_recall_curve(y_test,y_predict)\nplt.plot(threshold, recall[1:], 'b')\nplt.plot(threshold, precision[1:], 'y')\nplt.xlabel('Threshold')\nplt.show()","43cbe6b9":"intersection = np.where(precision == recall)[0][0]\noptimal_cutoff = 0.95\nprint(\"Optimal Threshold =\",optimal_cutoff)","a68cffc6":"genuine_idx = np.where(y_test == 0)[0]\nfraud_idx = np.where(y_test == 1)[0]\n\nplt.plot([y_predict[i] for i in fraud_idx], 'ro')\nplt.axhline(y=optimal_cutoff, color='g', linestyle='-')\nplt.title('Class 1 (Fraud)')\nplt.show()\n\nplt.plot([y_predict[i] for i in genuine_idx], 'bo')\nplt.axhline(y=optimal_cutoff, color='g', linestyle='-')\nplt.title('Class 0 (Genuine)')\nplt.show()","84d8a5e9":"print(classification_report(y_test,y_predict >= optimal_cutoff))","a13007ad":"Since features V1 to V28 have resulted from PCA transformation, they are already standardized.","684577b1":"Let's see how imbalanced our data is.","2c2df834":"We need to standardize the Time, Amount columns because they have a much higher scale compared to rest of the features. We must use the same transformation for both train and test data.","c45549aa":"We have only 0.17% positive samples. We use SMOTE to generate new positive samples.","9bb0cb5d":"1. Dataset was balanced with SMOTE\n2. 25% Dropout was crucial to reduce overfitting","32e74288":"Although there is no best method to select a threshold, I will use the intersection of precision and recall curves.","5690ea2a":"In cases of anomaly detection, it is more valuable to use a precision-recall curve rather than ROC.","a16a99a5":"Now we have a balanced dataset.\nI'm going to build a neural network."}}