{"cell_type":{"97fbe728":"code","532ee562":"code","80d2605f":"code","341924ef":"code","ebe6c1f3":"code","de034848":"code","ac4793d2":"code","c9e401c5":"code","c27baa29":"code","f8dc3ada":"code","228827b2":"code","4826858f":"code","681a037f":"code","af832c11":"code","5452ad63":"code","65397d71":"code","f9d40cf4":"code","89eceb94":"code","4c7317cd":"code","3a395ae0":"code","ea0d9ed7":"code","760eb5c6":"code","a2fe49c6":"code","fb95ecbf":"code","fe6621f9":"code","32730123":"code","959cdf99":"code","ce279678":"code","d33ecc3c":"code","26b4ba2a":"code","b87f097f":"code","99535cec":"code","1b68a48f":"code","dc0ede77":"code","f0295976":"code","6be05350":"code","67b715e6":"code","e1588c4e":"code","a8abfa46":"code","22369340":"code","6d9380bc":"code","857ea42f":"code","61495bb7":"code","3f94041c":"code","c8011f59":"code","d69e5ed8":"code","a1026ba0":"code","98c2e769":"code","d55f0ae2":"code","a400e95d":"code","8376fa94":"code","80403e05":"markdown","ca3ffd4d":"markdown","f5dd34c7":"markdown","f1523184":"markdown","3d3c1b27":"markdown","d24e8e32":"markdown","23e71665":"markdown","f3c8a299":"markdown","b2506512":"markdown","fc73feee":"markdown","9878c56b":"markdown","23c32ec2":"markdown","eec72f3f":"markdown","2c0a3a85":"markdown","0b928e3c":"markdown","ea50c660":"markdown","c3ea925f":"markdown","4ef5f13c":"markdown","b430d696":"markdown","d4dfa48f":"markdown","2d547226":"markdown","ebcb0473":"markdown","5893a7f8":"markdown","869dfef0":"markdown","02b5ea97":"markdown","40e9526f":"markdown","d548abdf":"markdown","74cf169b":"markdown","638ccf91":"markdown","a28e8f6c":"markdown","9e0db5cd":"markdown","ea68a593":"markdown","7787af06":"markdown","e3636b6f":"markdown","7b46eb59":"markdown","76ce8148":"markdown","9b92a8e5":"markdown","ae2dac7a":"markdown","2f17fb44":"markdown"},"source":{"97fbe728":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport math\nimport xgboost\nimport matplotlib.pyplot as plt\nimport warnings\nimport seaborn as sn\nwarnings.filterwarnings('ignore')\nfrom sklearn import preprocessing\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","532ee562":"training_set = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_set = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","80d2605f":"training_set.head()","341924ef":"training_set.dtypes","ebe6c1f3":"training_set.describe()","de034848":"test_set.head()","ac4793d2":"test_set.describe()","c9e401c5":"print(training_set.isnull().sum())","c27baa29":"sn.heatmap(training_set.isnull())","f8dc3ada":"def missing_data_percentage(column):\n    percentage = (training_set[column].isna().sum()\/len(training_set[column])) * 100\n    print('{} column - missing data percentage: {:.2f} %' .format(column, percentage))","228827b2":"missing_data_percentage(\"Age\")","4826858f":"missing_data_percentage(\"Cabin\")","681a037f":"training_set.drop(columns=['Cabin'], axis=1, inplace=True)\ntest_set.drop(columns=['Cabin'], axis=1, inplace=True)","af832c11":"training_set['Age'].fillna(training_set['Age'].median(), inplace=True)\ntraining_set['Embarked'].fillna(training_set['Embarked'].mode(), inplace=True)\n\ntest_set['Age'].fillna(test_set['Age'].median(), inplace=True)\ntest_set['Fare'].fillna(test_set['Fare'].median(), inplace=True)","5452ad63":"def box_plot(column):\n    training_set.boxplot(by = \"Survived\",column = [column],grid = True, layout=(1, 1))\n\n# Small circles or unfilled dots are drawn on the chart to indicate where suspected outliers lie.\n# Filled circles are used for known outliers.","65397d71":"box_plot(\"Pclass\")","f9d40cf4":"box_plot(\"SibSp\")","89eceb94":"training_set[\"SibSp\"].value_counts()","4c7317cd":"indexes = training_set.iloc[:891,:].index[training_set.iloc[:891,:].SibSp == 8]\ntraining_set.drop(indexes, inplace=True)","3a395ae0":"box_plot(\"Parch\")","ea0d9ed7":"training_set[\"Parch\"].value_counts()","760eb5c6":"indexes = training_set.index[training_set.Parch == 6]\ntraining_set.loc[indexes]\ntraining_set.drop(indexes, inplace=True)","a2fe49c6":"box_plot(\"Fare\")","fb95ecbf":"training_set[\"Fare\"].value_counts()","fe6621f9":"indexes = training_set.index[training_set.Fare > 100]\ntraining_set.loc[indexes]\nindexes\ntraining_set.drop(indexes, inplace=True)","32730123":"f = plt.figure(figsize=(15, 10))\ncorrelation_matrix = np.triu(training_set.corr())\nsn.heatmap(training_set.corr(), annot = True, mask = correlation_matrix)","959cdf99":"training_set.drop(columns=['Name', 'Ticket'], axis=1, inplace=True)\ntest_set.drop(columns=['Name', 'Ticket'], axis=1, inplace=True)","ce279678":"columns = ['Embarked', 'Parch', 'Pclass', 'Survived', 'Sex', 'SibSp']\n\nplt.figure(figsize = (16, 14))\nsn.set(font_scale = 1.2)\nsn.set_style('ticks')\n\nfor i, column in enumerate(columns):\n    plt.subplot(3, 3, i + 1)\n    sn.countplot(data = training_set, x = column, hue = 'Survived', palette = ['#d02f52',\"#55a0ee\"])\n    \nsn.despine()","d33ecc3c":"#training_set['Family_Size'] = training_set['Parch'] + training_set['SibSp']","26b4ba2a":"#training_set['Ticket'] = training_set.Ticket.str.split().apply(lambda x : 0 if x[:][-1] == 'LINE' else x[:][-1])\n#training_set.Ticket = training_set.Ticket.values.astype('int64')","b87f097f":"#training_set['Title'] = training_set.Name.str.extract('([A-Za-z]+)\\.', expand = False)\n#training_set.Title.value_counts()\n\n#rarest_titles = ['Rev','Dr','Major', 'Col', 'Capt','Jonkheer','Countess']\n\n#training_set.Title = training_set.Title.replace(['Ms', 'Mlle','Mme','Lady'], 'Miss')\n#training_set.Title = training_set.Title.replace(['Countess','Dona'], 'Mrs')\n#training_set.Title = training_set.Title.replace(['Don','Sir'], 'Mr')\n#training_set.Title = training_set.Title.replace(rarest_titles,'Other')","99535cec":"#training_set['Last_Name'] = training_set.Name.str.extract('^(.+?),', expand = False)","1b68a48f":"#training_set['Woman_Child'] = ((training_set.Title == 0) | (training_set.Sex == 'female'))","dc0ede77":"#family = training_set.groupby([training_set.Last_Name, training_set.Pclass, training_set.Ticket]).Survived\n\n#training_set['FamilyTotalCount'] = family.transform(lambda s: s.fillna(0).count())\n\n#training_set['FamilySurvivedCount'] = family.transform(lambda s: s.fillna(0).sum())\n\n#training_set['FamilySurvivalRate'] = (training_set.FamilySurvivedCount \/ training_set.FamilyTotalCount.replace(0, np.nan))","f0295976":"training_set = pd.get_dummies(training_set, columns=['Pclass', 'Sex', 'Embarked' ], drop_first= True)\ntest_set = pd.get_dummies(test_set, columns=['Pclass', 'Sex', 'Embarked' ], drop_first= True)","6be05350":"training_set","67b715e6":"#from sklearn.preprocessing import LabelEncoder\n\n#training_set['FareBin'] = pd.qcut(training_set['Fare'], 5)\n\n#label = LabelEncoder()\n#training_set['FareBin_Code'] = label.fit_transform(training_set['FareBin'])\n\n#training_set.drop(['Fare'], 1, inplace=True)","e1588c4e":"#training_set['AgeBin'] = pd.qcut(training_set['Age'], 4)\n#test_set['AgeBin'] = pd.qcut(test_set['Age'], 4)\n\n#training_set['AgeBin_Code'] = label.fit_transform(training_set['AgeBin'])\n#test_set['AgeBin_Code'] = label.fit_transform(test_set['AgeBin'])\n\n#training_set.drop(['Age'], 1, inplace=True)\n#test_set.drop(['Age'], 1, inplace=True)","a8abfa46":"#training_set.drop(['Name', 'Title', 'SibSp', 'Parch', 'FareBin', 'Last_Name', 'FamilyTotalCount', 'FamilySurvivedCount'], axis = 1, inplace = True)","22369340":"x_train = training_set.iloc[:,training_set.columns != 'Survived']\ny_train = training_set.iloc[:,training_set.columns == 'Survived'].values.reshape(-1,1)\n\nx_test = test_set","6d9380bc":"training_set","857ea42f":"test_set","61495bb7":"x_test.describe()","3f94041c":"x_test.isna().sum()","c8011f59":"from sklearn.tree import DecisionTreeClassifier\n\nclassifier_dt = DecisionTreeClassifier()\n#classifier_dt.fit(x_train, y_train)\n\n#y_pred = classifier_dt.predict(x_test)\n\n#importances = pd.DataFrame(classifier_dt.feature_importances_, index = x_train.columns)\n#importances.sort_values(by = 0, inplace=True, ascending = False)\n\n#plt.figure(figsize = (8, 5)) \n#sn.barplot(x = 0, y = importances.index, data = importances)\n#sn.despine()","d69e5ed8":"training_set.head()","a1026ba0":"test_set.head()","98c2e769":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nfeatures_to_scale = ['Age','SibSp','Parch','Fare']\n\nx_train[features_to_scale] = scaler.fit_transform(x_train[features_to_scale])\nx_test[features_to_scale] = scaler.fit_transform(x_test[features_to_scale])","d55f0ae2":"from xgboost import XGBClassifier\nclassifier_xgb = XGBClassifier()\nclassifier_xgb.fit(x_train, y_train)","a400e95d":"y_pred_xgb = classifier_xgb.predict(x_test)","8376fa94":"test_set = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\noutput = pd.DataFrame({'PassengerId': test_set.PassengerId, 'Survived': y_pred_xgb})\n\noutput.to_csv('my_submission_xgb.csv', index=False)\nprint(\"Your submission was successfully saved!\")","80403e05":"# Detecting and removing outliers\n****\nReducing the noise of the data","ca3ffd4d":"# Survival rates\n****","f5dd34c7":"# Standard scaling\n****","f1523184":"Title column","3d3c1b27":"Family size column","d24e8e32":"# Feature engineering\n\nThis part is commented because I've actually been getting worse prediction results with it.\n****","23e71665":"Fare feature has possible outliers, so we will remove all rows with values of feature Fare higher than 100","f3c8a299":"# Categorical data\n****","b2506512":"Age and Cabin columns have the most missing data. Next, percentage of missing data for each of those columns.","fc73feee":"# **Titanic Kaggle Competition Predictions**\n********","9878c56b":"# Fitting the model\n****","23c32ec2":"Last Name column","eec72f3f":"Ticket","2c0a3a85":"**Dataset features:**\n****\n\nSurvived: Outcome of survival (0 = No; 1 = Yes)\n\nPclass: Socio-economic class (1 = Upper class; 2 = Middle class; 3 = Lower class)\n\nName: Name of passenger\n\nSex: Sex of the passenger\n\nAge: Age of the passenger (Some entries contain NaN)\n\nSibSp: Number of siblings and spouses of the passenger aboard\n\nParch: Number of parents and children of the passenger aboard\n\nTicket: Ticket number of the passenger\n\nFare: Fare paid by the passenger\n\nCabin Cabin number of the passenger (Some entries contain NaN)\n\nEmbarked: Port of embarkation of the passenger (C = Cherbourg; Q = Queenstown; S = Southampton)\n","0b928e3c":"# Creating the matrix of features and dependent variable vector\n****","ea50c660":"Woman or child column","c3ea925f":"Creating Family size","4ef5f13c":"Most of the data (77.46 %) in the Cabin column is missing, so we will remove it from the datasets.","b430d696":"# Data exploration\n****","d4dfa48f":"* PassengerId column correlation with other features shouldn't be taken into account  \n* Survived column has **moderate** correlation to columns Fare and Pclass\n* Pclass column has **moderate** correlation to Age and **high** correlation to Fare\n* Age is **moderately** correlated to SibSp\n* SibSp is **moderately** correlated to Parch","2d547226":"****","ebcb0473":"# Importing the data\n****","5893a7f8":"Parch feature has possible outliers, so we will remove all rows with values of feature Parch == 6","869dfef0":"**Extreme Gradient Boosting** - turned out to be more efficient without hyperparameter boosting than SVM or Random Forest with boosted hyperparameters","02b5ea97":"# Importing the libraries\n****","40e9526f":"Citations\n\n* https:\/\/arjan-hada.github.io\/titanic-survival-exploration.html\n* https:\/\/www.kaggle.com\/javiervallejos\/titanic-top-3\n* https:\/\/www.kaggle.com\/vipin20\/titanic-prediction-eda-hyperparameter-top-10\/comments#Prediction-on-Test-data\n* https:\/\/www.kaggle.com\/sociopath00\/random-forest-using-gridsearchcv","d548abdf":"# Problem description\n****","74cf169b":"SibSp feature has possible outliers, so we will remove all rows with values of feature SibSp == 8","638ccf91":"# Correlation matrix\n****\nUsed to detect correlation between features in order to reduce dimensionality if possible - showing lower triangle matrix below.","a28e8f6c":"****","9e0db5cd":"The sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nIn this challenge, the goal is to build a predictive model that answers the question: \u201cwhat sorts of people were more likely to survive?\u201d using passenger data (ie name, age, gender, socio-economic class, etc).","ea68a593":"Filling unknown data with mean\/median or removing the data","7787af06":"# Output","e3636b6f":"Creating bins for Fare and Age features**\n\n** Commented out because the result was worse with Fare bins","7b46eb59":"There is room for improvement in the area of XGB model hyperparameter tuning.","76ce8148":"# Predicting the test set results","9b92a8e5":"# Detecting and dealing with unknown data\n****","ae2dac7a":"# Feature importance\n****","2f17fb44":"Features Name and Ticket will be removed because they have a lot of distinct values and aren't good predictors"}}