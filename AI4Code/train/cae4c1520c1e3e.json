{"cell_type":{"4d247dfb":"code","e4c065ee":"code","45480b67":"code","0daadac6":"code","e80f6c06":"code","c76eb35f":"code","d21e2235":"code","00fdce93":"code","ae3db9a5":"code","a0a36ed2":"code","c119e751":"code","50c8bd49":"code","a870a399":"code","bc8a0e96":"code","fae30a44":"code","18fc9851":"code","d11e90dc":"code","9f3cbd91":"code","6924e3d2":"code","170e6f5e":"code","40135f04":"code","339cd9c4":"code","037baf4a":"code","25640914":"code","b3fdd66c":"code","8cef4424":"code","2f276b0d":"code","ef05bebe":"code","a2ad5cd1":"code","57e26654":"code","c4b15ea9":"code","8af45499":"code","3c647fe9":"code","bd41b296":"code","f02ab66f":"code","280156f3":"code","eaf70422":"code","57e5f2c8":"code","ad312451":"code","4a65d606":"markdown","1790dc95":"markdown","b5eac045":"markdown","15c14421":"markdown","c68c654a":"markdown","9cf5d49d":"markdown","6ba04f8a":"markdown","38cf10e2":"markdown","ab8deb37":"markdown","053a3e48":"markdown","f215248f":"markdown","1ce8f77d":"markdown","bbc74615":"markdown","9d423280":"markdown","d164ab4f":"markdown","ccd3fc26":"markdown","2b9fff5c":"markdown","cfc9b958":"markdown","020d0f47":"markdown","0e9c7e4b":"markdown","ae11b74a":"markdown","3c094896":"markdown","60173377":"markdown","6e19e1ac":"markdown","ca6ace15":"markdown","57189567":"markdown","b0899e28":"markdown","366f451e":"markdown","5697a867":"markdown","a208e09d":"markdown","878baf67":"markdown","d301dbc7":"markdown","8c843167":"markdown","c2b858ae":"markdown","5a0528eb":"markdown","5d10f071":"markdown","c63186a5":"markdown"},"source":{"4d247dfb":"import os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nprint(os.listdir(\"..\/input\/cleanedloanfico\/\"))","e4c065ee":"def univariate(df,col,vartype,hue =None):\n    \n    '''\n    Univariate function will plot the graphs based on the parameters.\n    df      : dataframe name\n    col     : Column name\n    vartype : variable type : continuos or categorical\n                Continuos(0)   : Distribution, Violin & Boxplot will be plotted.\n                Categorical(1) : Countplot will be plotted.\n    hue     : It's only applicable for categorical analysis.\n    \n    '''\n    sns.set(style=\"darkgrid\")\n    \n    if vartype == 0:\n        fig, ax=plt.subplots(nrows =1,ncols=3,figsize=(20,8))\n        ax[0].set_title(\"Distribution Plot\")\n        sns.distplot(df[col],ax=ax[0])\n        ax[1].set_title(\"Violin Plot\")\n        sns.violinplot(data =df, x=col,ax=ax[1], inner=\"quartile\")\n        ax[2].set_title(\"Box Plot\")\n        sns.boxplot(data =df, x=col,ax=ax[2],orient='v')\n    \n    if vartype == 1:\n        temp = pd.Series(data = hue)\n        fig, ax = plt.subplots()\n        width = len(df[col].unique()) + 6 + 4*len(temp.unique())\n        fig.set_size_inches(width , 7)\n        ax = sns.countplot(data = df, x= col, order=df[col].value_counts().index,hue = hue) \n        if len(temp.unique()) > 0:\n            for p in ax.patches:\n                ax.annotate('{:1.1f}%'.format((p.get_height()*100)\/float(len(loan))), (p.get_x()+0.05, p.get_height()+20))  \n        else:\n            for p in ax.patches:\n                ax.annotate(p.get_height(), (p.get_x()+0.32, p.get_height()+20)) \n        del temp\n    else:\n        exit\n        \n    plt.show()","45480b67":"def crosstab(df,col):\n    '''\n    df : Dataframe\n    col: Column Name\n    '''\n    crosstab = pd.crosstab(df[col], df['loan_status'],margins=True)\n    crosstab['Probability_Charged Off'] = round((crosstab['Charged Off']\/crosstab['All']),3)\n    crosstab = crosstab[0:-1]\n    max1 = max(crosstab['Probability_Charged Off'])\n    maxx = crosstab.loc[crosstab['Probability_Charged Off']==max1]\n    \n    return crosstab,maxx","0daadac6":"# Probability of charge off\ndef bivariate_prob(df,col,stacked= True):\n    '''\n    df      : Dataframe\n    col     : Column Name\n    stacked : True(default) for Stacked Bar\n    '''\n    # get dataframe from crosstab function\n    plotCrosstab,maxx = crosstab(df,col)\n    \n    linePlot = plotCrosstab[['Probability_Charged Off']]      \n    barPlot =  plotCrosstab.iloc[:,0:2]\n    ax = linePlot.plot(figsize=(20,8), marker='o',color = 'b')\n    ax2 = barPlot.plot(kind='bar',ax = ax,rot=1,secondary_y=True,stacked=stacked)\n    ax.set_title(df[col].name.title()+' vs Probability Charge Off',fontsize=20,weight=\"bold\")\n    ax.set_xlabel(df[col].name.title(),fontsize=14)\n    ax.set_ylabel('Probability of Charged off',color = 'b',fontsize=14)\n    ax2.set_ylabel('Number of Applicants',color = 'g',fontsize=14)\n    plt.show()","e80f6c06":"loan = pd.read_csv('..\/input\/cleanedloanfico\/cleanedData.csv')","c76eb35f":"# Drop\ndrop = ['last_fico_range_high','last_fico_range_low']\nloan.drop(drop, axis=1, inplace=True)\nr,c=loan.shape\nprint(f\"The number of rows {r}\\nThe number of columns {c}\")\nloan.dropna(axis=0, how = 'any', inplace = True)\nr1,c1=loan.shape\nprint(f\"The difference between earlier and dropped Nan rows: {r-r1}\")","d21e2235":"loan['emp_length'] = loan['emp_length'].replace({'1 year':1,'10+ years':'10','2 years':2,'3 years':3,\"4 years\":4,\"5 years\":5,\n                                                 \"6 years\":6,\"7 years\":7,\"8 years\":8,\"9 years\":9,\"< 1 year\":0})\n\n#a_dataframe.drop(a_dataframe[a_dataframe.B > 3].index, inplace=True)\n\nloan.drop(loan[loan['emp_length']==\"Self-Employed\"].index,inplace = True)\nloan['emp_length'] = loan['emp_length'].apply(pd.to_numeric)","00fdce93":"univariate(df=loan,col='emp_length',vartype=1)","ae3db9a5":"# average fico score from range\n#loan[\"fico_score\"] = (loan['fico_range_high'] + loan['fico_range_low'])\/2\nloan['fico_score'] = loan[['fico_range_low', 'fico_range_high']].mean(axis=1)\ndrop = ['fico_range_low','fico_range_high']\nloan.drop(drop, axis=1, inplace=True)\nloan['fico_score']","a0a36ed2":"loan.head(5)","c119e751":"fico,maxx = crosstab(loan,'fico_score')\ndisplay(fico)\n\nprint(\"maximum\")\ndisplay(maxx)\n\nbivariate_prob(df=loan,col='fico_score')","50c8bd49":"mask = (loan.loan_status == 'Charged Off')\nloan['target'] = 0\nloan.loc[mask,'target'] = 1","a870a399":"del loan['loan_status']","bc8a0e96":"loan.loc[loan['target']==0]","fae30a44":"loan.loc[loan['target']==1]","18fc9851":"loan['target'].value_counts()","d11e90dc":"loan.dtypes","9f3cbd91":"categorical = loan.columns[loan.dtypes == 'object']\ncategorical","6924e3d2":"X = pd.get_dummies(loan[loan.columns], columns=categorical).astype(float)\ny = loan['target']","170e6f5e":"X","40135f04":"if 'target' in X:\n    del X['target']\nX.columns","339cd9c4":"y","037baf4a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom imblearn.over_sampling import SMOTE\nimport pickle\nscaler = preprocessing.MinMaxScaler()\n#X_scaled = scaler.fit_transform(X)\n#X_scaledd = pd.DataFrame(X_scaled, columns=X.columns)\n\n#X_scaled = preprocessing.scale(X)\n#print(X_scaled)\n#print('   ')\n#print(X_scaled.shape)\n#X_scaledd\n#X_scaledd.isnull().sum()","25640914":"def run_models(X_train, y_train, X_test, y_test, model_type = 'Non-balanced'):\n    \n    clfs = {'GradientBoosting': GradientBoostingClassifier(verbose=1,max_depth= 8, n_estimators=150, max_features = 0.3),\n            'LogisticRegression' : LogisticRegression(verbose=1,C=10.0,solver='saga',penalty = 'elasticnet',l1_ratio = 1),\n            #'GaussianNB': GaussianNB(),\n            'RandomForestClassifier': RandomForestClassifier(verbose=1,n_estimators=10,criterion='entropy') #10\n            }\n    cols = ['model','accuracy_score','matthews_corrcoef', 'roc_auc_score', 'precision_score', 'recall_score','f1_score']\n\n    models_report = pd.DataFrame(columns = cols)\n    conf_matrix = dict()\n\n    for clf, clf_name in zip(clfs.values(), clfs.keys()):\n        \n        cross_val_score(clf, X_train, y_train , cv=5)\n        \n        clf.fit(X_train, y_train)\n        \n        filename = f'{clf_name}_{model_type}.sav'\n        pickle.dump(clf, open(filename, 'wb'))\n        \n        y_pred = clf.predict(X_test)\n        y_score = clf.predict_proba(X_test)[:,1]\n\n        print('computing {} - {} '.format(clf_name, model_type))\n\n        tmp = pd.Series({'model_type': model_type,\n                         'model': clf_name,\n                         'accuracy_score': metrics.accuracy_score(y_test, y_pred),\n                         'roc_auc_score' : metrics.roc_auc_score(y_test, y_score),\n                         'matthews_corrcoef': metrics.matthews_corrcoef(y_test, y_pred),\n                         'precision_score': metrics.precision_score(y_test, y_pred),\n                         'recall_score': metrics.recall_score(y_test, y_pred),\n                         'f1_score': metrics.f1_score(y_test, y_pred)})\n\n        models_report = models_report.append(tmp, ignore_index = True)\n        conf_matrix[clf_name] = pd.crosstab(y_test, y_pred, rownames=['True'], colnames= ['Predicted'], margins=False)\n        fpr, tpr, thresholds = metrics.roc_curve(y_test, y_score, drop_intermediate = False, pos_label = 1)\n\n        plt.figure(1, figsize=(6,6))\n        plt.xlabel('false positive rate')\n        plt.ylabel('true positive rate')\n        plt.title('ROC curve - {}'.format(model_type))\n        plt.plot(fpr, tpr, label = clf_name )\n        plt.legend(loc=2, prop={'size':11})\n    plt.plot([0,1],[0,1], color = 'black')\n    \n    return models_report, conf_matrix","b3fdd66c":"X_train, X_test, y_train, y_test = train_test_split(X, y.values, test_size=0.4, random_state=0, shuffle = True)\nX_train_sc = scaler.fit_transform(X_train)\nX_test_sc = scaler.fit_transform(X_test)\n\n#scores = cross_val_score(clf, X_scaled, y , cv=5, scoring='roc_auc')\n\n%time models_report, conf_matrix = run_models(X_train_sc, y_train, X_test_sc, y_test, model_type = 'Non-balanced')","8cef4424":"X_Train_scaled = pd.DataFrame(X_train_sc, columns=X.columns)\nX_Train_scaled","2f276b0d":"X_Test_scaled = pd.DataFrame(X_test_sc, columns=X.columns)\nX_Test_scaled","ef05bebe":"X_Train_scaled.describe()","a2ad5cd1":"X_Test_scaled.describe()","57e26654":"models_report","c4b15ea9":"conf_matrix['LogisticRegression']","8af45499":"conf_matrix['GradientBoosting']","3c647fe9":"conf_matrix['RandomForestClassifier']","bd41b296":"p = X.iloc[20]\nloan.iloc[20]","f02ab66f":"k=p.array\ndd = dict(zip(X.columns,k))\np = pd.DataFrame(dd, columns = p.index, index=[0])","280156f3":"p","eaf70422":"import pickle\nloaded_model = pickle.load(open('LogisticRegression_Non-balanced.sav', 'rb'))\nresult = loaded_model.predict(p)\nprob = loaded_model.predict_proba(p)\nprint(\"Target\",result[0],\"\\nProbability\",prob)","57e5f2c8":"loaded_model = pickle.load(open('RandomForestClassifier_Non-balanced.sav', 'rb'))\nresult = loaded_model.predict(p) \nprob = loaded_model.predict_proba(p)\nprint(\"Target\",result[0],\"\\nProbability\",prob)","ad312451":"loaded_model = pickle.load(open('GradientBoosting_Non-balanced.sav', 'rb'))\nresult = loaded_model.predict(p)\nprob = loaded_model.predict_proba(p)\nprint(\"Target\",result[0],\"\\nProbability\",prob)","4a65d606":"X_train, X_test, y_train, y_test = train_test_split(X_scaled, y.values, test_size=0.4, random_state=0, shuffle = True)","1790dc95":"y_pred = clf.predict_classes(X_test)\nmetrics.classification_report(y_test,y_pred)","b5eac045":"index_split = int(len(X)\/2)\n%time X_train, y_train = SMOTE().fit_sample(X_scaled[0:index_split, :], y[0:index_split])\nX_test, y_test = X_scaled[index_split:], y[index_split:]\n\n#scores = cross_val_score(clf, X_scaled, y , cv=5, scoring='roc_auc')\n\nmodels_report_bal, conf_matrix_bal = run_models(X_train, y_train, X_test, y_test, model_type = 'Balanced')","15c14421":"* Dummies - converting categorical columns into binary values so that our ML model can understand\n* X - Training Data\n* Y - Label Data","c68c654a":"## Accuracy Metric of balanced Model with SMOTE","9cf5d49d":"### Rows which charged off","6ba04f8a":"#### SMOTE Synthetic Minority Over-sampling Technique (SMOTe)","38cf10e2":"## Converting Fico Ranges into Fico Score","ab8deb37":"models_report_bal","053a3e48":"conf_matrix_bal['GradientBoosting']","f215248f":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Dropout\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau","1ce8f77d":"- We will first split the data then scale using the same scaler, because if we scale first same standardization is done across both train and test set. \n- Whereas, in real world scenario we will have different ranges for validation.","bbc74615":"## Testing with one row","9d423280":"## Converting loan status into 0,1 <br>\n### 0 - Fully paid\n### 1 - Charged off","d164ab4f":"## Deserialization","ccd3fc26":"Deleting the target variable from the training data.","2b9fff5c":"### Deep Learning","cfc9b958":"import pickle\nloaded_model = pickle.load(open('LogisticRegression_Balanced.sav', 'rb'))\nresult = loaded_model.predict(p)\nprob = loaded_model.predict_proba(p)\nprint(\"Target\",result[0],\"\\nProbability\",prob)","020d0f47":"loaded_model = pickle.load(open('RandomForestClassifier_Balanced.sav', 'rb'))\nresult = loaded_model.predict(p) \nprob = loaded_model.predict_proba(p)\nprint(\"Target\",result[0],\"\\nProbability\",prob)","0e9c7e4b":"### Employment Length changes","ae11b74a":"conf_matrix_bal['LogisticRegression']","3c094896":"#### overall","60173377":"## Accuracy Metric of Unbalanced Model","6e19e1ac":"- Scaling the data through sklearn preprocessing scale\n- Importing Libraries for Model Building","ca6ace15":"### Drop any nan rows if still there..","57189567":"conf_matrix_bal['RandomForestClassifier']","b0899e28":"clf.fit(x = X_train,y = y_train, batch_size = 128,epochs = 25,validation_data = (X_test,y_test),callbacks = callbacks)","366f451e":"clf = Sequential() # sigmoid because its range is 0-1 and we have scaled our data accordingly,tanh -1 to 1, relu se 0 to infinity,\nclf.add(Dense(128,activation = \"sigmoid\",input_shape = shape))\n\nclf.add(Dense(256,activation = \"sigmoid\"))\nclf.add(Dense(512,activation = \"sigmoid\"))\nclf.add(Dense(256,activation = \"sigmoid\"))\nclf.add(Dense(128,activation = \"sigmoid\"))\nclf.add(Dense(16,activation = \"sigmoid\"))\n\n\nclf.add(Dense(1,activation = 'sigmoid'))\nclf.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics=['acc'])\ncallbacks = [\n    EarlyStopping(patience=12, verbose=1),\n    ReduceLROnPlateau(factor=0.1, patience=5, min_lr=0.000001, verbose=1),\n    ModelCheckpoint('.\/Alrs-{epoch:03d}.h5', monitor='val_loss', verbose=1,save_best_only=True,save_weights_only=True,mode='min')\n] ","5697a867":"shape = X_train.shape\nshape","a208e09d":"User Defined function \n## UDF - Creating, saving, accuracy metric of models","878baf67":"loaded_model = pickle.load(open('GradientBoosting_Balanced.sav', 'rb'))\nresult = loaded_model.predict(p)\nprob = loaded_model.predict_proba(p)\nprint(\"Target\",result[0],\"\\nProbability\",prob)","d301dbc7":"len(X_train),len(X_test)","8c843167":"metrics.confusion_matrix(y_test, y_pred)","c2b858ae":"### Rows which fully paid the loan","5a0528eb":"## Due to limit in kernel Memory constraints the following notebook is executeed here [Fork of Model Training for ALRS](https:\/\/www.kaggle.com\/vasantvohra\/fork-of-model-training-for-alrs\/)","5d10f071":"loss_history = pd.DataFrame(clf.history.history)\nloss_history.plot(figsize = (10,6))","c63186a5":"clf.save(\"Ann.h5\")"}}