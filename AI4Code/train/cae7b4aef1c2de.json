{"cell_type":{"b9981f5c":"code","741fe4e6":"code","28010b73":"code","6fbd0a42":"code","51bd4479":"code","acdb2b8c":"code","7ba20027":"code","07dae018":"code","2d83efdf":"code","410dc55a":"code","7b928a5f":"code","74c042fe":"code","be020944":"code","31d184a7":"code","e3c72a2a":"code","457e1dca":"code","9a34e0d4":"code","c668fe88":"code","46e8b5fd":"code","72af68f0":"code","e3b5f0dd":"code","e3a6266a":"code","039f51e6":"code","8ff3d1e6":"code","c3930f70":"code","83b7e8ed":"code","e7425900":"code","9339bbf6":"code","7e8087a0":"code","367654b8":"code","f80e21ee":"code","83121356":"code","235d5e8b":"code","8533a605":"code","2f2b7460":"code","dcdca638":"code","ab6467a2":"code","553feeaf":"code","017e9b69":"code","aed9a289":"code","a9118caf":"code","50727c62":"code","83ca318d":"code","21cf7405":"code","8c7cf92f":"code","5b85aaf9":"code","fd8aa7a5":"code","62280520":"code","21bf39e9":"code","dfcab343":"code","1ffee986":"code","e6e8f973":"code","2f88e9bd":"code","fa511bdc":"code","29ac834c":"code","d16a33c4":"code","0b246046":"code","6149aa84":"markdown","757fcf0d":"markdown","942db940":"markdown","0ceefbf3":"markdown","e6de2785":"markdown","fc166477":"markdown","c1f562e1":"markdown","6a1d0673":"markdown","6f9eac3e":"markdown","bcba912a":"markdown","1b8ba6e8":"markdown","1ad19fe1":"markdown","7ad855da":"markdown","d669dd19":"markdown","5458a437":"markdown","4e60129d":"markdown","8798e3b6":"markdown","5b6bbafc":"markdown","1c25027e":"markdown"},"source":{"b9981f5c":"import numpy as np\n%matplotlib inline\nimport pandas as pd\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objs as go\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import  LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom plotly.offline import init_notebook_mode,iplot","741fe4e6":"df_train = pd.read_csv('..\/input\/customer-segmentation\/Train.csv')\ndf_train","28010b73":"df_train.isna().sum(axis=0)","6fbd0a42":"sns.heatmap(df_train.isnull());","51bd4479":"df_train.fillna(df_train.mean(), inplace = True)","acdb2b8c":"df_train.dropna(axis=0, inplace=True)","7ba20027":"df_train.isna().sum(axis=0)","07dae018":"df_train = df_train.drop(columns=[\"ID\"]) ","2d83efdf":"temp = df_train.describe()\ntemp.style.background_gradient(cmap='Oranges')","410dc55a":"g1 = [go.Box(y=df_train.Work_Experience,name=\"Work_Experience\",marker=dict(color=\"rgba(51,0,0,0.9)\"),hoverinfo=\"name+y\")]\ng2 = [go.Box(y=df_train.Family_Size,name=\"Family_Size\",marker=dict(color=\"rgba(0,102,102,0.9)\"),hoverinfo=\"name+y\")]\nlayout2 = go.Layout(title=\"Work Experience | Family Size\",yaxis=dict(range=[0,13])) \nfig2 = go.Figure(data=g1+g2,layout=layout2)\niplot(fig2)","7b928a5f":"grafico = px.box(df_train, y='Age')\ngrafico.show()","74c042fe":"fig2 = px.histogram(df_train,x='Age',color='Age',template='plotly_dark')\nfig2.show()","be020944":"fig2 = px.histogram(df_train,x='Gender',color='Gender',template='plotly_dark')\nfig2.show()","31d184a7":"fig2 = px.histogram(df_train,x='Ever_Married',color='Ever_Married',template='plotly_dark')\nfig2.show()","e3c72a2a":"fig2 = px.histogram(df_train,x='Graduated',color='Graduated',template='plotly_dark')\nfig2.show()","457e1dca":"fig2 = px.histogram(df_train,x='Profession',color='Profession',template='plotly_dark')\nfig2.show()","9a34e0d4":"fig2 = px.histogram(df_train,x='Work_Experience',color='Work_Experience',template='plotly_dark')\nfig2.show()","c668fe88":"fig2 = px.histogram(df_train,x='Spending_Score',color='Spending_Score',template='plotly_dark')\nfig2.show()","46e8b5fd":"fig2 = px.histogram(df_train,x='Family_Size',color='Family_Size',template='plotly_dark')\nfig2.show()","72af68f0":"fig2 = px.histogram(df_train,x='Var_1',color='Var_1',template='plotly_dark')\nfig2.show()","e3b5f0dd":"mk = LabelEncoder()\ndf_train['Gender'] = mk.fit_transform(df_train['Gender'])\ndf_train['Ever_Married'] = mk.fit_transform(df_train['Ever_Married'])\ndf_train['Graduated'] = mk.fit_transform(df_train['Graduated'])\ndf_train['Spending_Score'] = mk.fit_transform(df_train['Spending_Score'])\ndf_train['Var_1'] = mk.fit_transform(df_train['Var_1'])\ndf_train['Profession'] = mk.fit_transform(df_train['Profession'])\ndf_train['Family_Size'] = mk.fit_transform(df_train['Family_Size'])\ndf_train['Work_Experience'] = mk.fit_transform(df_train['Work_Experience'])\ndf_train['Segmentation'] = mk.fit_transform(df_train['Segmentation'])","e3a6266a":"df_train","039f51e6":"scaler = StandardScaler()                                         \ndf_scaled = scaler.fit_transform(df_train)","8ff3d1e6":"type(df_scaled)","c3930f70":"min(df_scaled[0]), max(df_scaled[0]) ","83b7e8ed":"df_scaled","e7425900":"wcss_1 = []                                     \nrange_values = range(1, 10)                    \nfor i in range_values:                        \n  kmeans = KMeans(n_clusters=i)                 \n  kmeans.fit(df_scaled)            \n  wcss_1.append(kmeans.inertia_)  ","9339bbf6":"print(wcss_1) ","7e8087a0":"grafico = px.line(x = range(1,10), y = wcss_1)\nplt.plot(wcss_1, '-o',)  \ngrafico.show()","367654b8":"kmeans = KMeans(n_clusters=5)           \nkmeans.fit(df_scaled)         \nlabels = kmeans.labels_","f80e21ee":"labels, len(labels)  ","83121356":"np.unique(labels, return_counts=True)","235d5e8b":"cluster_centers = pd.DataFrame(data = kmeans.cluster_centers_, columns = [df_train.columns])\ncluster_centers","8533a605":"cluster_centers = scaler.inverse_transform(cluster_centers)\ncluster_centers = pd.DataFrame(data = cluster_centers, columns = [df_train.columns])\ncluster_centers","2f2b7460":"df_mk_cluster = pd.concat([df_train, pd.DataFrame({'cluster': labels})], axis = 1) \ndf_mk_cluster.head()","dcdca638":"sns.set(rc={'axes.facecolor':'black', 'figure.facecolor':'black', 'axes.grid' : False, 'font.family': 'Ubuntu'})\n\nfor i in df_mk_cluster:\n    g = sns.FacetGrid(df_mk_cluster, col = \"cluster\", hue = \"cluster\", palette = \"Set2\")\n    g.map(plt.hist, i, bins=10, ec=\"k\") \n    g.set_xticklabels(rotation=30, color = 'white')\n    g.set_yticklabels(color = 'white')\n    g.set_xlabels(size=15, color = 'white')\n    g.set_titles(size=15, color = '#FFC300', fontweight=\"bold\")\n    g.fig.set_figheight(5);","ab6467a2":"import matplotlib as mpl","553feeaf":"clusters_count = df_mk_cluster['cluster'].value_counts()                       \nclusters_count = clusters_count.to_frame().reset_index()                      \nclusters_count.columns = ['clusters', 'count']                               \nclusters_count = clusters_count.sort_values('clusters', ascending = True)     \n\nlabels = [\n        \"B\", \n        \"A\", \n        \"D\", \n        \"E\",\n        \"C\"\n        ]\n\nplt.figure(figsize=(15,9))\nmpl.rcParams['font.size'] = 17\ncolors = sns.color_palette('Set2')[0:5]\nplt.pie(clusters_count['count'], \n        explode=(0.05, 0.05, 0.05, 0.05, 0.05), \n        labels = labels,\n        colors= colors,\n        autopct='%1.1f%%',\n        textprops = dict(color =\"white\", fontsize=19),\n        counterclock = False,\n        startangle=180,\n        wedgeprops={\"edgecolor\":\"gray\",'linewidth':1}\n        )\n\nplt.axis('equal')\nplt.text(-0.8, 1.2, \"Clusters\", size=30, color=\"#FFC300\", fontweight=\"bold\")\nplt.text(-1.8, 1.2, \"Distribution\", size=30, color=\"white\")\nplt.show();","017e9b69":"df_scaled","aed9a289":"import matplotlib.pyplot as plt\nfrom scipy.cluster.hierarchy import dendrogram, linkage","a9118caf":"dendrograma = dendrogram(linkage(df_scaled, method='ward'))\nplt.title('Dendrograma')\nplt.xlabel('X')\nplt.ylabel('Y');","50727c62":"from sklearn.cluster import AgglomerativeClustering","83ca318d":"hc_g = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage = 'ward')\nrotulos = hc_g.fit_predict(df_scaled)","21cf7405":"rotulos","8c7cf92f":"grafico = px.scatter(x = df_scaled[:,0], y = df_scaled[:,1], color = rotulos)\ngrafico.show()","5b85aaf9":"df_scaled","fd8aa7a5":"from sklearn.cluster import DBSCAN","62280520":"dbscan_g = DBSCAN(eps = 0.95, min_samples=2)\ndbscan_g.fit(df_train)","21bf39e9":"rotulos = dbscan_g.labels_\nrotulos","dfcab343":"grafico = px.scatter(x = df_scaled[:,0], y = df_scaled[:,1], color = rotulos)\ngrafico.show()","1ffee986":"from sklearn import datasets","e6e8f973":"X_random, y_random = datasets.make_moons(n_samples=1500, noise = 0.09)","2f88e9bd":"X_random","fa511bdc":"grafico = px.scatter(x = X_random[:,0], y = X_random[:,1])\ngrafico.show()","29ac834c":"kmeans = KMeans(n_clusters=2)\nrotulos = kmeans.fit_predict(X_random)\ngrafico = px.scatter(x = X_random[:,0], y = X_random[:, 1], color = rotulos)\ngrafico.show()","d16a33c4":"hc = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')\nrotulos = hc.fit_predict(X_random)\ngrafico = px.scatter(x = X_random[:,0], y = X_random[:, 1], color = rotulos)\ngrafico.show()","0b246046":"dbscan = DBSCAN(eps=0.1)\nrotulos = dbscan.fit_predict(X_random)\ngrafico = px.scatter(x = X_random[:,0], y = X_random[:, 1], color = rotulos)\ngrafico.show()","6149aa84":"There are some categorical data that could not be filled with the mean value, so we will delete them, so we try to delete as little as possible in order to keep the database.","757fcf0d":"There are some groups that he could not associate, which are dark blue in color;\n\nWe can verify that the DBDCAN algorithm did better in the grouping, we defined the radio 0.1, which helps a lot in the separation;\n\nWe have a small database however, we could use the two previous more traditional algorithms for the type of study in this database,\n\nIn general DBDCAN presents better results than k-means and also faster, what improves the algorithm a lot is the threshold distance, it ends up being more interesting when we have more complex problems and with a much larger database.\n","942db940":"# **If you find this notebook useful, support with an upvote** \ud83d\udc4d","0ceefbf3":"# DBSCAN","e6de2785":"# **Hierarchical Grouping**","fc166477":"#  KMeans","c1f562e1":"# Data Visualization","6a1d0673":"# **Processing**","6f9eac3e":"# **K-means x Hierarchical x DBSCAN**","bcba912a":"# **DBSCAN**","1b8ba6e8":"**It wasn't very efficient in the separation either.**","1ad19fe1":"# **Context**\n\nAn automobile company has plans to enter new markets with their existing products (P1, P2, P3, P4, and P5). After intensive market research, they\u2019ve deduced that the behavior of the new market is similar to their existing market.\n\n\nIn their existing market, the sales team has classified all customers into 4 segments (A, B, C, D ). Then, they performed segmented outreach and communication for a different segment of customers. This strategy has work e exceptionally well for them. They plan to use the same strategy for the new markets and have identified 2627 new potential customers.\n\n\nYou are required to help the manager to predict the right group of the new customers.\n\n\n# **Content**\n* Variable\t- Definition\n* ID\t- Unique ID\n* Gender\t- Gender of the customer\n* Ever_Married - \tMarital status of the customer\n* Age\t- Age of the customer\n* Graduated -\tIs the customer a graduate?\n* Profession -\tProfession of the customer\n* Work_Experience -\tWork Experience in years\n* Spending_Score\t- Spending score of the customer\n* Family_Size\t- Number of family members for the customer (including the customer)\n* Var_1 -\tAnonymised Category for the customer\n* Segmentation -\t(target) Customer Segment of the customer\n\nAcknowledgements\nThis dataset was acquired from the Analytics Vidhya hackathon.","7ad855da":"# Agglomerative Clustering","d669dd19":"# **Exploring the Data**","5458a437":"There is a lot of null data in the training base, if we delete it it will decrease the database a lot.\n\nIf it was a company and we had contact with the database supplier, we would try to fill in the missing data.\n\nIn order not to lag the database too much, let's fill in the average of each missing value.","4e60129d":"**Through the graphics it was possible to analyze all the columns and have details of each one of them.**","8798e3b6":"# **Clearing the Data**","5b6bbafc":"# **Training Data**","1c25027e":"**As we can see Kmena was not able to do an efficient separation of the data, it would not be efficient for sorting this data;**"}}