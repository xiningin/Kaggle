{"cell_type":{"bf09ece9":"code","bee15583":"code","65bb0884":"code","99eda794":"code","4bf6ea37":"code","9aacb7d6":"code","46a10c4e":"code","f7ef9ba9":"code","6dc9c5c0":"code","d4485770":"code","62a2b0b0":"code","e161fa05":"code","07cbe93a":"code","6b303f01":"code","266cc01d":"code","767f7929":"code","75f3f55f":"code","956267f9":"code","a23b58b5":"code","bd8fd005":"code","afd5307e":"code","f0c375fa":"code","6376183b":"code","9cdaaec2":"code","14f02483":"code","3db28d1e":"code","5a44b7d3":"code","b4ac1735":"code","3ba2120b":"code","aaed8c23":"code","c9d46bc7":"code","9ae6dcf3":"code","2c5caafa":"code","165ee164":"code","bd71498e":"code","4a0d68bb":"code","96ee37d1":"code","f686c135":"code","a2a859e3":"code","828ec6d9":"code","7d7320e1":"markdown","680fd4e2":"markdown","c0504eb7":"markdown","fd3e17b5":"markdown","002e4ddc":"markdown","395c20e9":"markdown","471e16d9":"markdown","3999a8a3":"markdown","006e5328":"markdown","2fbfa07a":"markdown","023eecca":"markdown","f0e8e212":"markdown","ca95d906":"markdown","eb00ec5a":"markdown","178f024a":"markdown","ba846f79":"markdown","43770e6e":"markdown","814817d3":"markdown","cd94b386":"markdown","2ce937e2":"markdown","198473f7":"markdown","0a67b64b":"markdown"},"source":{"bf09ece9":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom tqdm import tqdm\nimport time\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom wordcloud import WordCloud\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold,KFold, cross_val_score\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.naive_bayes import MultinomialNB\nimport xgboost as xgb\nfrom sklearn import preprocessing, model_selection, pipeline\nfrom sklearn.metrics import f1_score, roc_auc_score,roc_curve\n\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, GRU\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils import np_utils\nfrom keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\nfrom keras.preprocessing import sequence, text\nfrom keras.callbacks import EarlyStopping","bee15583":"df=pd.read_csv(\"..\/input\/real-or-fake-fake-jobposting-prediction\/fake_job_postings.csv\")\ndf.tail()","65bb0884":"df.info()","99eda794":"df","4bf6ea37":"df.shape","9aacb7d6":"df.isnull().sum()","46a10c4e":"text_df = df[[\"title\", \"company_profile\", \"description\", \"requirements\", \"benefits\",\"fraudulent\"]]\ntext_df = text_df.fillna(' ')\ntext_df.head()","f7ef9ba9":"cat_df = df[[\"telecommuting\", \"has_company_logo\", \"has_questions\", \"employment_type\", \"required_experience\", \"required_education\", \"industry\", \"function\",\"fraudulent\"]]\ncat_df = cat_df.fillna(\"None\")\n\ncat_df.head()","6dc9c5c0":"df.drop(['salary_range','job_id'], axis = 1, inplace = True)","d4485770":"df","62a2b0b0":"df.fillna(\" \",inplace = True)","e161fa05":"df","07cbe93a":"df['text'] = df['title'] + ' ' + df['location'] + ' ' + df['department'] + ' ' + df['company_profile'] + ' ' + df['description'] + ' ' + df['requirements'] + ' ' + df['benefits'] + ' ' + df['employment_type'] + ' ' + df['required_education'] + ' ' + df['industry'] + ' ' + df['function'] ","6b303f01":"df.text[0]","266cc01d":"df.drop(['title','location','department','company_profile','description','requirements','benefits','employment_type','required_experience','required_education','industry','function'], axis = 1, inplace = True)","767f7929":"df.head()","75f3f55f":"df['text_length'] = df.text.apply(len)\ndf.head()","956267f9":"fraud = df[\"fraudulent\"].value_counts()\nsns.barplot(fraud.index, fraud, color=\"#FF1700\")","a23b58b5":"df.fraudulent.value_counts()","bd8fd005":"plt.figure(figsize=(12, 8))\n\ndf[df.fraudulent==0].text_length.plot(bins=35, kind='hist', color='#FF1700', \n                                       label='Ger\u00e7ek ilanlar', alpha=0.6)\ndf[df.fraudulent==1].text_length.plot(kind='hist', color='#95CD41', \n                                       label='Sahte ilanlar', alpha=0.6)\nplt.legend()\nplt.xlabel(\"Text Length\")","afd5307e":"cat_cols = [\"telecommuting\", \"has_company_logo\", \"has_questions\", \"employment_type\", \"required_experience\", \"required_education\",]\n\nimport matplotlib.gridspec as gridspec \ngrid = gridspec.GridSpec(3, 3, wspace=0.5, hspace=0.5) \nplt.figure(figsize=(15,25)) \n\n\nfor n, col in enumerate(cat_df[cat_cols]): \n    ax = plt.subplot(grid[n]) \n    sns.countplot(x=col, data=cat_df, hue='fraudulent', palette='YlOrBr') \n    ax.set_ylabel('Say\u0131', fontsize=12) \n    ax.set_title(f'Sahtelik de\u011ferine g\u00f6re {col}', fontsize=15)\n    ax.set_xlabel(f'{col} de\u011feri', fontsize=12)\n    xlabels = ax.get_xticklabels() \n    ylabels = ax.get_yticklabels() \n    ax.set_xticklabels(xlabels,  fontsize=10)\n    ax.set_yticklabels(ylabels,  fontsize=10)\n    plt.legend(fontsize=8)\n    plt.xticks(rotation=90) \n    total = len(cat_df)\n    sizes=[] \n    for p in ax.patches: \n        height = p.get_height()\n        sizes.append(height)\n        ax.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\", fontsize=10) \n    ax.set_ylim(0, max(sizes) * 1.15)\n\n\nplt.show()","f0c375fa":"fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\nnum=text_df[text_df[\"fraudulent\"]==1]['company_profile'].str.split().map(lambda x: len(x))\nax1.hist(num,bins = 20,color='#753188')\nax1.set_title('Sahte ilanlar')\nnum=text_df[text_df[\"fraudulent\"]==0]['company_profile'].str.split().map(lambda x: len(x))\nax2.hist(num, bins = 20,color='#9AE66E')\nax2.set_title('Ger\u00e7ek ilanlar')\nplt.show()","6376183b":"fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\nnum=text_df[text_df[\"fraudulent\"]==1]['description'].str.split().map(lambda x: len(x))\nax1.hist(num,bins = 20,color='#753188')\nax1.set_title('Sahte ilanlar')\nnum=text_df[text_df[\"fraudulent\"]==0]['description'].str.split().map(lambda x: len(x))\nax2.hist(num, bins = 20,color='#9AE66E')\nax2.set_title('Ger\u00e7ek ilanlar')\nplt.show()","9cdaaec2":"fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\nnum=text_df[text_df[\"fraudulent\"]==1]['requirements'].str.split().map(lambda x: len(x))\nax1.hist(num,bins = 20,color='#753188')\nax1.set_title('Sahte ilanlar')\nnum=text_df[text_df[\"fraudulent\"]==0]['requirements'].str.split().map(lambda x: len(x))\nax2.hist(num,bins = 20,color='#9AE66E')\nax2.set_title('Ger\u00e7ek ilanlar')\nplt.show()","14f02483":"fig,(ax1,ax2)= plt.subplots(ncols=2, figsize=(17, 5), dpi=100)\nnum=text_df[text_df[\"fraudulent\"]==1]['benefits'].str.split().map(lambda x: len(x))\nax1.hist(num,bins = 20,color='#753188')\nax1.set_title('Sahte ilanlar')\nnum=text_df[text_df[\"fraudulent\"]==0]['benefits'].str.split().map(lambda x: len(x))\nax2.hist(num, bins = 20,color='#9AE66E')\nax2.set_title('Ger\u00e7ek ilanlar')\nplt.show()","3db28d1e":"lemmatizer = WordNetLemmatizer()\ndef text_cleaning(text):\n    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n    text = text.lower()\n    text = text.split()\n    text = [lemmatizer.lemmatize(word) for word in text if not word in stopwords.words('english')]\n    return ' '.join(text)    ","5a44b7d3":"df['text'] = df['text'].apply(text_cleaning)","b4ac1735":"df.text[0]","3ba2120b":"plt.figure(figsize = (20,20))\nwc = WordCloud(width = 1600 , height = 800 , max_words = 3000).generate(\" \".join(df[df.fraudulent == 0].text))\nplt.imshow(wc , interpolation = 'bilinear')","aaed8c23":"plt.figure(figsize = (20,20)) \nwc = WordCloud(width = 1600 , height = 800 , max_words = 3000).generate(\" \".join(df[df.fraudulent == 1].text))\nplt.imshow(wc , interpolation = 'bilinear')","c9d46bc7":"df.info()","9ae6dcf3":"X = df.text\ny = df.fraudulent","2c5caafa":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 , random_state = 0)","165ee164":"tfid = TfidfVectorizer(min_df=0, max_df=1, ngram_range=(1,3))\ntfid_train = tfid.fit_transform(X_train)\ntfid_test = tfid.transform(X_test)\nprint('Tfidf_train : ', tfid_train.shape)\nprint('Tfidf_test : ', tfid_test.shape)","bd71498e":"mnb=MultinomialNB()\nmnb_tfidf=mnb.fit(tfid_train, y_train)","4a0d68bb":"mnb_tfidf_predict=mnb.predict(tfid_test)","96ee37d1":"mnb_bow_predict=mnb.predict(tfid_test)\ncm_cv = confusion_matrix(y_test,mnb_bow_predict)\n\ncm_cv = pd.DataFrame(cm_cv, index=[0,1], columns=[0,1])\ncm_cv.index.name = 'Ger\u00e7ek'\ncm_cv.columns.name = 'Tahmin'","f686c135":"plt.figure(figsize = (10,10))\nsns.heatmap(cm_cv,cmap= \"YlGn\",annot = True, fmt='')","a2a859e3":"mnb_tfidf_score=accuracy_score(y_test, mnb_tfidf_predict)\nprint(\"Accuracy Skoru(do\u011fruluk oran\u0131) :\", mnb_tfidf_score)","828ec6d9":"fpr, tpr, thr = roc_curve(y_test, mnb.predict_proba(tfid_test)[:,1])\nauc = roc_auc_score(y_test, mnb_tfidf_predict)\nlw = 2\nplt.figure(figsize=(10, 8))\nplt.plot(fpr, tpr, color='#4C3F91', lw=lw, label=\"Curve Area = %0.3f\" % auc)\nplt.plot([0, 1], [0, 1], color='#9145B6', lw=lw, linestyle='dotted')\nplt.xlabel('Yanl\u0131\u015f Pozitif Oran\u0131')\nplt.ylabel('Do\u011fru Pozitif Oran\u0131')\nplt.legend(loc=\"lower right\")\nplt.show()","7d7320e1":"#### Company Profile s\u00fctunu","680fd4e2":"### Naive Bayes","c0504eb7":"#### Benefits s\u00fctunu","fd3e17b5":"# Kelime say\u0131s\u0131","002e4ddc":"### K\u00fct\u00fcphaneler","395c20e9":"### Gereksiz S\u00fctunlar\u0131 Silme","471e16d9":"#### Requirements s\u00fctunu","3999a8a3":"### Null De\u011ferler","006e5328":"### AUC: ROC E\u011frisi ","2fbfa07a":"### TF-IDF","023eecca":"### Veri setini okuma","f0e8e212":"### Textlerin Temizli\u011fi, D\u00fczeni","ca95d906":"### WordCloud","eb00ec5a":"### Hedef","178f024a":"### Train Test Ayr\u0131m\u0131","ba846f79":"### Veri Bilgisi","43770e6e":"### Confusion matrisi","814817d3":"### Textlerin uzunlu\u011fu","cd94b386":"### Bo\u015f kategorisel de\u011ferleri d\u00fczenleme","2ce937e2":"#### Description s\u00fctunu","198473f7":"Veri setinin dengesizli\u011fini g\u00f6r\u00fcyoruz...","0a67b64b":"### Bo\u015f text verilerini d\u00fczenleme"}}