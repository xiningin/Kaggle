{"cell_type":{"d9343c93":"code","09deb162":"code","97cc4aad":"code","ddd9e61e":"code","39dce8cf":"code","abe7c68b":"code","297d3fff":"code","33e2dd61":"code","a1e95728":"code","97a46ffc":"code","6e0ca996":"markdown","70baff05":"markdown"},"source":{"d9343c93":"import numpy as np\nimport pandas as pd\nimport os\n\nprint(os.listdir(\"..\/input\"))\n\n# running our benchmark code in this kernel lead to memory errors, so \n# we do a slightly less memory intensive procedure if this is True, \n# set this as False if you are running on a computer with a lot of RAM\n# it should be possible to use less memory in this kernel using generators\n# rather than storing everything in RAM, but we won't explore that here\nRUNNING_KAGGLE_KERNEL = True ","09deb162":"rspct_df = pd.read_csv('..\/input\/rspct.tsv', sep='\\t')\ninfo_df  = pd.read_csv('..\/input\/subreddit_info.csv')","97cc4aad":"rspct_df.head(5)","ddd9e61e":"# note that info_df has information on subreddits that are not in data, \n# we filter them out here\n\ninfo_df = info_df[info_df.in_data].reset_index()\ninfo_df.head(5)","39dce8cf":"# we join the title and selftext into one field\n\ndef join_text(row):\n    if RUNNING_KAGGLE_KERNEL:\n        return row['title'][:100] + \" \" + row['selftext'][:512]\n    else:\n        return row['title'] + \" \" + row['selftext']\n\nrspct_df['text'] = rspct_df[['title', 'selftext']].apply(join_text, axis=1)","abe7c68b":"# take the last 20% as a test set - N.B data is already randomly shuffled,\n# and last 20% is a stratified split (equal proportions of subreddits)\n\ntrain_split_index = int(len(rspct_df) * 0.8)\n\ntrain_df, test_df = rspct_df[:train_split_index], rspct_df[train_split_index:]\nX_train , X_test  = train_df.text, test_df.text\ny_train, y_test   = train_df.subreddit, test_df.subreddit","297d3fff":"from sklearn.preprocessing import LabelEncoder\n\n# label encode y\n\nle = LabelEncoder()\nle.fit(y_train)\ny_train = le.transform(y_train)\ny_test  = le.transform(y_test)\n\ny_train[:5]","33e2dd61":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# extract features from text using bag-of-words (single words + bigrams)\n# use tfidf weighting (helps a little for Naive Bayes in general)\n# note : you can do better than this by extracting more features, then \n# doing feature selection, but not enough memory on this kernel!\n\nprint('this cell will take about 10 minutes to run')\n\nNUM_FEATURES = 30000 if RUNNING_KAGGLE_KERNEL else 100000\n\ntf_idf_vectorizer = TfidfVectorizer(max_features = NUM_FEATURES,\n                                min_df=5,\n                                ngram_range=(1,2),\n                                stop_words=None,\n                                token_pattern='(?u)\\\\b\\\\w+\\\\b',\n                            )\n\nX_train = tf_idf_vectorizer.fit_transform(X_train)\nX_test  = tf_idf_vectorizer.transform(X_test)\n\nfrom sklearn.feature_selection import chi2, SelectKBest\n\n# if we have more memory, select top 100000 features and select good features\nif not RUNNING_KAGGLE_KERNEL:\n    chi2_selector = SelectKBest(chi2, 30000)\n\n    chi2_selector.fit(X_train, y_train) \n\n    X_train = chi2_selector.transform(X_train)\n    X_test  = chi2_selector.transform(X_test)\n\nX_train.shape, X_test.shape","a1e95728":"from sklearn.naive_bayes import MultinomialNB\n\n# train a naive bayes model, get predictions\n\nnb_model = MultinomialNB(alpha=0.1)\nnb_model.fit(X_train, y_train)\n\ny_pred_proba = nb_model.predict_proba(X_test)\ny_pred = np.argmax(y_pred_proba, axis=1)","97a46ffc":"# we use precision-at-k metrics to evaluate performance\n# (https:\/\/en.wikipedia.org\/wiki\/Evaluation_measures_(information_retrieval)#Precision_at_K)\n\ndef precision_at_k(y_true, y_pred, k=5):\n    y_true = np.array(y_true)\n    y_pred = np.array(y_pred)\n    y_pred = np.argsort(y_pred, axis=1)\n    y_pred = y_pred[:, ::-1][:, :k]\n    arr = [y in s for y, s in zip(y_true, y_pred)]\n    return np.mean(arr)\n\nprint('precision@1 =', np.mean(y_test == y_pred))\nprint('precision@3 =', precision_at_k(y_test, y_pred_proba, 3))\nprint('precision@5 =', precision_at_k(y_test, y_pred_proba, 5))\n\n# RUNNING_KAGGLE_KERNEL == True\n# precision@1 = 0.610528134254689\n# precision@3 = 0.7573692003948668\n# precision@5 = 0.8067670286278381\n\n# RUNNING_KAGGLE_KERNEL == False\n# precision@1 = 0.7292102665350444\n# precision@3 = 0.8512240868706812\n# precision@5 = 0.8861500493583415","6e0ca996":"## Naive Bayes benchmark","70baff05":"## Basic data analysis"}}