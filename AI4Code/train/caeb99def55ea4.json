{"cell_type":{"2400cad5":"code","1b3c854c":"code","7f556444":"code","0ef8f903":"code","4aa5cd5a":"code","c3011585":"code","3c9d0ddb":"code","efaacc56":"code","a1c7e70e":"code","050ee793":"code","5f604ed4":"code","d9d1fec0":"code","ab40ab18":"code","4f27c6a7":"code","b0882615":"code","4a2d3ec9":"code","7cce4a9d":"code","c2e6a307":"code","5e453c65":"code","2fe6676b":"code","a5aeadc0":"markdown","cd47ffea":"markdown","70309b90":"markdown","a6a37f8e":"markdown","a9f39c9a":"markdown","6a85ea3c":"markdown","91b266a6":"markdown","9f7c99a0":"markdown","7c242652":"markdown","b9f233e7":"markdown"},"source":{"2400cad5":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Import Competition Datasets\ncomp_train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ncomp_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\nfull = pd.concat([comp_train, comp_test], axis=0, ignore_index=True)\n\nprint('Train instances:', comp_train.shape[0])\nprint('Test instances:', comp_test.shape[0])\nprint('Number of features:', comp_train.shape[1])\n\n# Drop PassengerId\nfull.drop('PassengerId', axis=1, inplace=True)\n\n# Take glance at the data set\nfull.head()","1b3c854c":"# Calculate proportion of passengers that survived\ndisplay(comp_train.Survived.value_counts(normalize=True))","7f556444":"# Calculate Gender proportions of passengers\ndisplay(comp_train.Sex.value_counts(normalize=True))\nsns.countplot(x='Survived', hue='Sex', data=comp_train)\nplt.show()","0ef8f903":"display(comp_train.groupby('Pclass').Survived.value_counts(normalize=True))\nsns.countplot(x='Survived', hue='Pclass', data=comp_train)\nplt.show()","4aa5cd5a":"sns.kdeplot(full['Age'][full['Survived'] == 1], label='Survived', color='green')\nsns.kdeplot(full['Age'][full['Survived'] == 0], label='Perished', color='red'); ","c3011585":"full.info()","3c9d0ddb":"# Fill by median\nmissing_val_median = ['Age', 'Fare']\n\nfor col  in missing_val_median:\n    full[col] = full[col].fillna(value=full[col].median())\n    \n# Fill by mode\nmissing_val_median = ['Embarked']\n\nfor col  in missing_val_median:\n    full[col] = full[col].fillna(value=full[col].mode()[0])","efaacc56":"full['Pclass'] = full['Pclass'].astype(str)\nfull['TravelAlone'] = np.where(full['SibSp'] + full['Parch'] > 0, '1', '0')\nfull.drop(['Name', 'Ticket'], axis=1, inplace=True)\n\nfull.info()","a1c7e70e":"print(full.var())\n\nsns.kdeplot(full['Fare'])\nplt.show()","050ee793":"full['logFare'] = np.log(full['Fare'] + 1)\nfull['AgeScaled'] = (full['Age'] - full['Age'].mean()) \/ full['Age'].std()\n\nfull.drop(['Age', 'Fare', 'Cabin'], axis=1, inplace=True)","5f604ed4":"print(full.var())\nprint(full.describe())","d9d1fec0":"full_dummies = pd.get_dummies(full, drop_first=True)\n\nfull_dummies.info()","ab40ab18":"from sklearn.model_selection import train_test_split\n\ntrain = full_dummies[full_dummies['Survived'].notnull()]\ntest = full_dummies[full_dummies['Survived'].isnull()]\n\nprint(train.shape)\nprint(test.shape)\n\n# Create train and test sets\n# Stratify along Survived to ensure the split remains representative of the original dataset\nX_train, X_test, y_train, y_test = train_test_split(\n    train.drop('Survived', axis=1), \n    train['Survived'], \n    test_size=0.2, \n    stratify=train['Survived'],\n    random_state=12)\n\nprint(y_train.value_counts(normalize=True))\nprint(y_test.value_counts(normalize=True))","4f27c6a7":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\nlogreg = LogisticRegression(max_iter=3000)\n\n# Define parameter search grid\nparameters = [#{'penalty': ['elasticnet'], 'l1_ratio': np.linspace(0,1,10), 'C': [0.01, 0.1, 0.1, 1, 10], 'solver': ['saga']},\n              {'penalty': ['l1', 'l2'], 'C': [0.01, 0.1, 0.1, 1, 10], 'solver': ['liblinear']},\n              {'penalty': ['none']}]\n\n# Pass the pipeline and parameter dict to GridSearchCV\ncv = GridSearchCV(logreg, param_grid=parameters, cv=5)\n\n# Fit model to the training set\ncv.fit(X_train, y_train)\n\n# View best cross-validated fit\nprint(cv.best_params_)\nprint(cv.best_score_)\n\n# Check training set accuracy\nprint('Training set accuracy:', cv.score(X_train, y_train))\n# Check test set accuracy\nprint('Test set accuracy:', cv.score(X_test, y_test))","b0882615":"from sklearn.model_selection import cross_val_score\n\nout_logreg = LogisticRegression(penalty=cv.best_params_['penalty'], C=cv.best_params_['C'], solver=cv.best_params_['solver'], max_iter=800)\n\n# Obtain Cross-Validated Accuracy for the model on the entire training set\ncross_val_score(out_logreg, X=train.drop('Survived', axis=1), y=train['Survived'], cv=5)","4a2d3ec9":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier()\n\n# Define parameter search grid\nrf_parameters = [{'n_estimators': [100, 200, 250, 300, 400], 'criterion': ['gini', 'entropy']}]\n\n# Pass the pipeline and parameter dict to GridSearchCV\nrf_cv = GridSearchCV(rfc, param_grid=rf_parameters, cv=10)\n\n# Fit model to the training set\nrf_cv.fit(X_train, y_train)\n\n# View best cross-validated fit\nprint(rf_cv.best_params_)\nprint(rf_cv.best_score_)\n\n# Check training and test set accuracy\nprint('Training set accuracy:', rf_cv.score(X_train, y_train))\nprint('Test set accuracy:', rf_cv.score(X_test, y_test))","7cce4a9d":"out_rf = RandomForestClassifier(n_estimators=rf_cv.best_params_['n_estimators'], criterion=rf_cv.best_params_['criterion'])\n\n# Obtain Cross-Validated Accuracy for the model on the entire training set\ncross_val_score(out_rf, X=train.drop('Survived', axis=1), y=train['Survived'], cv=10)","c2e6a307":"# Refit model to the entire training set and make predictions\nout_rf.fit(train.drop('Survived', axis=1), train['Survived'])\n\ny_pred = out_rf.predict(test.drop('Survived', axis=1))","5e453c65":"sub = pd.DataFrame()\nsub['PassengerId'] = comp_test['PassengerId']\nsub['Survived'] = y_pred\nsub['Survived'] = sub['Survived'].astype(int)\nsub.to_csv('cv_logreg_submission.csv', index=False)","2fe6676b":"print(sub.info())\ndisplay(sub.head())","a5aeadc0":"The distribution of age with respect to survival shows that younger children survived at a higher rate with a the curve for surivals peaking twice, once for very young children and again for young adults.","cd47ffea":"# Titanic Competition Dataset: Logistic Regression\n\n## Import and Data Overview\n\nThe Titanic data set is composed of 10 Features and a target variabele (Survived). The features are a mix of numeric (Age, Fare, SibSp, Parch) and categorical (Pclass, Sex, Ticket, Cabin, Embarked) variables.\n\nThe target variable is binary variable indicating whether a given passenger survived or perished in the Titanic disaster.\n\nThis notebook will use Logistic Regression to predict the probability of a passenger surviving.","70309b90":"## Train Test Split","a6a37f8e":"Extract best model parameters and create new pipeline to run cross validation score on the entire **competition** training set (pre-train_test_split) and to use for **competition** predictions.","a9f39c9a":"Grouping by Passenger class shows that passengers of higher class had better chances of suviving, with 63% of first class passengers surviving, 47% of second class and only 24% of 3rd class passengers surviving. Pclass should given relatively high explanatory power. ","6a85ea3c":"## Exploratory Analysis\n\n","91b266a6":"## Feature Engineering\n\nPclass is actually a categorical variable and should be one hot encoded.\n\nThe variance of Fare and Age quite high. Additionally, Fare is very skewed. Log transforming Fare will reduce the right skewness. Additionally, scaling can be applied to align the variances of Fare and Age.","9f7c99a0":"## Impute Missing Values","7c242652":"Of the training data set, only 38% of passengers survived. ","b9f233e7":"With respect to gender, far more women survived than men."}}