{"cell_type":{"a64532c6":"code","b3ddf603":"code","a56f58a2":"code","d962a011":"code","3be7bc5c":"code","73eb8bec":"code","52d82f7f":"code","1912805e":"code","92c25b3c":"code","f97f0a7b":"code","c69ac056":"code","4b144629":"code","c2857b1b":"markdown","05d13baf":"markdown","7f3c0040":"markdown","d0af0649":"markdown","f7b112f8":"markdown","c6680eab":"markdown","afc38afe":"markdown"},"source":{"a64532c6":"pip install scikit-learn","b3ddf603":"import sklearn\nprint(sklearn.__version__)","a56f58a2":"#import the module that we need\n\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split","d962a011":"import pandas as pd\n\n#load the iris data\niris = load_iris()\n\n#feature data\niris_data = iris.data\n\n#lable data\niris_label = iris.target\nprint('value of iris target \\n\\n', iris_label)\nprint('type of iris target \\n\\n ', iris.target_names)\n\n#transform to DataFrame\niris_df = pd.DataFrame(data=iris_data,columns=iris.feature_names)\niris_df['label']=iris.target\niris_df.head(3)\n","3be7bc5c":"#split the data\n#test_size : the ratio of test data\n#random_state : use same split data\n\nX_train, X_test, y_train, y_test = train_test_split(iris_data,iris_label,test_size=0.2,random_state=1)","73eb8bec":"# DecisionTreeClassifier\nmodel_1 = DecisionTreeClassifier(random_state=1)\n\n# training\nmodel_1.fit(X_train, y_train)\n\n# predict\nprediction = model_1.predict(X_test)\n\n# calculate the accuracy\nfrom sklearn.metrics import accuracy_score\nprint('accuracy of model 1: ',accuracy_score(y_test,prediction))","52d82f7f":"from sklearn.tree import export_graphviz\nimport graphviz\n\n# make the decision tree as visual\nexport_graphviz(model_1,# fitted estimator\n                out_file=\"tree.dot\", # name of output file\n                class_names=iris.target_names, # target name\n               feature_names=iris.feature_names, #feature name\n               impurity=True,\n               filled=True)\n\n# visualize decision tree\nwith open(\"tree.dot\") as f:\n    dot_graph = f.read()\ngraphviz.Source(dot_graph)","1912805e":"model_2 = DecisionTreeClassifier(min_samples_split = 6,\n                                 random_state=1)\nmodel_2.fit(X_train,y_train)\nprediction = model_2.predict(X_test)\nprint('accuracy of model 2: ',accuracy_score(y_test,prediction))","92c25b3c":"# make the decision tree as visual\nexport_graphviz(model_2,# fitted estimator\n                out_file=\"tree.dot\", # name of output file\n                class_names=iris.target_names, # target name\n               feature_names=iris.feature_names, #feature name\n               impurity=True,\n               filled=True)\n\n# visualize decision tree\nwith open(\"tree.dot\") as f:\n    dot_graph = f.read()\ngraphviz.Source(dot_graph)","f97f0a7b":"# k-fold cross validation\nfrom sklearn.model_selection import KFold\nimport numpy as np\n\nfeatures = iris.data\nlabel = iris.target\ndt_clf = DecisionTreeClassifier(random_state=156)\n\nkfold=KFold(n_splits=5)\ncv_accuracy = []","c69ac056":"n_iter = 0\naccuracy_list=[]\n\nfor train_index, test_index in kfold.split(features):\n    X_train,X_test = features[train_index], features[test_index]\n    y_train,y_test = label[train_index], label[test_index]\n    \n    dt_clf.fit(X_train,y_train)\n    pred = dt_clf.predict(X_test)\n    n_iter +=1\n    \n    accuracy = accuracy_score(y_test,pred)\n    accuracy_list.append(accuracy)\n    \n    print(n_iter, 'accuracy: ',accuracy)\n\nprint('mean accuracy: ', np.mean(accuracy_list))","4b144629":"#stratified k fold\nfrom sklearn.model_selection import StratifiedKFold\n\nskfold=StratifiedKFold(n_splits=3)\nn_iter_2 = 0\naccuracy_list_2=[]\n\nfor train_index, test_index in skfold.split(features,label):\n    X_train,X_test = features[train_index], features[test_index]\n    y_train,y_test = label[train_index], label[test_index]\n    \n    dt_clf.fit(X_train,y_train)\n    pred = dt_clf.predict(X_test)\n    n_iter_2 +=1\n    \n    accuracy_2 = accuracy_score(y_test,pred)\n    accuracy_list_2.append(accuracy_2)\n    \n    print(n_iter_2, 'accuracy: ',accuracy_2)\n\nprint('mean accuracy: ', np.mean(accuracy_list_2))    ","c2857b1b":"# Cross-validation\n\nWe make the model according to the train data. But in this process, there is some big problem known as 'overfitting'. Overfitting can be represented when the model suitable only in train data so it can't be apply to test data. To overcome this situation, we can use cross-validation and perform more variable traing and evaluation. There are two type of cross-validation\n\n* K fold\n* stratified k fold\n\nStaratified k fold is used when the distribution of dependent data is imbalanced. It is easy to understand that when we use cross-validation in classificaion, use stratified K-fold and in regression, use just plain K-fold","05d13baf":"# Make the first ML : using iris data\n\nWe are going to make the first ML model using classificaion to predict the iris species based on various feature. Classification is one of the typical Supervised learning method. Supervised learning learn the model using lable data that have various feature values and predict value, and predict the mystery lable of the test data. To sum up, supervised learning is the method that learn with data that have clarified answer at first, and then predict the unknown answer.\n\n![1](https:\/\/ifh.cc\/g\/sig5wX.png)\n","7f3c0040":"As you can see the 4th node,the value is [0,36,4]. It is pretty enough the stop the split, but we haven't set the pruning rule so the tree still split the node. If the number of node of decision tree increase, the accuracy decrease because of overfitting. We can handle this problem according to set the parameter.\n\n* min_samples_split : the minimum sample value that split the node. The defalut value is 2\n* min_samples_leaf : the minimum sample value to be the leaf.\n* max_features : the number of feature that consider to make the model.\n* max_depth : the maximum value of depth of tree\n* max_leaf_nodes : the maximum value of the number of leaf","d0af0649":"We can check the version of scikit-learn like this. Just be careful using 2 underbar : not _ but __ ","f7b112f8":"# Introduction of scikit-learning\n\nscikit-learning is used the most python ML library. It provide easy and efficient development instructure of ML using python. Of course, these days, there are many python ML library like tensorflow or keras, but it is still representable library at all.\n\nAt first, Let's install the scikit-learn library!","c6680eab":"Yes! we make the decision tree classifier. But it is not intuitive. We don't know the rule of branch and leaf node. But don't worry. There is some library that help to visual the decision tree. Using graphviz, we can see the branch node and leaf node and the rule that split the tree.\n\n* petal length(cm) <= 2.45 : rule that need to make the split.\n* gini : gini's coefficient. It is equility as going to 0, inequility as going to 1.\n* samples : the data that fit in this node.\n* value = [] : the number of data that fit in target variable","afc38afe":"independent variable : sepal length, sepal width, petal length, petal width.\n\ndependent variable : 0(setosa), 1(versicolor), 2(virginica)\n\n\nThe process of classification is as follow\n\n1. split the data to train and test\n2. training the model using the ML technique\n3. predict dependent variable of test data \n4. evaluate the accuracy"}}