{"cell_type":{"7579b2a3":"code","b28410d2":"code","be8f9d8d":"code","14e6b11c":"code","586cc373":"code","88e5180d":"code","84027e3b":"code","678fe75d":"code","9deb6b1a":"code","d64522fd":"code","3456a1b6":"code","021cf6cf":"code","a8bbf181":"code","11f6d7e0":"code","6dc0da0e":"code","8b35558a":"code","5866a147":"code","c0e7d910":"code","5975ec75":"code","bd8fe73f":"code","1373ea3d":"code","e003c2ea":"code","869fc783":"code","11023daf":"code","b6f86e68":"code","681f0672":"code","2a98d728":"code","b7323b7b":"code","ccb5227f":"code","bdcfdd14":"code","7789ff00":"code","becbe146":"code","67e8087b":"code","8349bac3":"code","76b3c6ff":"code","88d7d6cf":"code","0cc2ad5f":"code","9506f3ca":"code","2500209e":"code","555eab4e":"code","7e235045":"code","e2f01131":"code","ed51d205":"code","a8998b25":"code","bfc8f5ca":"code","220af96e":"code","a0d260c2":"code","b8a884bb":"code","9907e990":"code","411417a9":"code","623c16f2":"code","930c0d79":"code","f07c1821":"markdown","3f1be5f3":"markdown","54695b95":"markdown","20eb2ae5":"markdown","16f12c5a":"markdown","a1e38db1":"markdown","4ca286dd":"markdown","cfe9ba56":"markdown","f930c57c":"markdown","f20e6332":"markdown","afa16e44":"markdown","a82b966d":"markdown","493d0883":"markdown","7c276257":"markdown","75b94bd1":"markdown","343ee5aa":"markdown","ecbce93d":"markdown","e1f41481":"markdown","a1ef065a":"markdown","39c4a070":"markdown","01c041f3":"markdown","814df16c":"markdown","7ae53a71":"markdown","dfb62cd9":"markdown","22a42703":"markdown","f165c94d":"markdown","db466d63":"markdown","d480716b":"markdown","5edb1257":"markdown","7e647561":"markdown","fe2047e7":"markdown","7276fe17":"markdown","67ad76a1":"markdown","ff0a33b8":"markdown"},"source":{"7579b2a3":"#!pip install seaborn==0.9.0","b28410d2":"import pandas as pd\nimport numpy as np\nimport os \n\nimport plotly.plotly as py\nimport plotly.graph_objs as go\n\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\nimport warnings\nwarnings.filterwarnings('ignore')","be8f9d8d":"# Private DataSet path: ..\/input\/kddbr2018dataset\/kddbr-2018-dataset\/dataset. This dataset is the same of competitions\n#\npath = '..\/input\/kddbr2018dataset\/kddbr-2018-dataset\/dataset\/'\nprint(os.listdir(path))","14e6b11c":"df_train = pd.read_csv(path+'train.csv')\ndf_test  = pd.read_csv(path+'test.csv')\ndf_all   = pd.concat([df_train, df_test])\n\nprint(df_train.shape, df_test.shape, df_all.shape)\ndf_all.head(3)","586cc373":"def to_date(df):\n    return pd.to_datetime((df.harvest_year*10000+df.harvest_month*100+1)\\\n                                  .apply(str),format='%Y%m%d')\n# Add date variable \nfor d in [df_train, df_test, df_all]:\n    d['date'] = to_date(d)","88e5180d":"sns.distplot(df_all.production, hist=False, color=\"g\",rug=True, kde_kws={\"shade\": True})\n#sns.boxplot(x=\"production\", y=\"field\", data=df_train, palette=\"vlag\")","84027e3b":"print(\"Mean: \", df_all.production.mean())","678fe75d":"#df_all = df_all[(df_all.production < (df_all.production.mean()+df_all.production.std()*4)) | (df_all.production.isna())]","9deb6b1a":"#data = [go.Scatter(x=df_all.date, y=df_all.production)]\n#py.iplot(data)\n\nf, ax = plt.subplots(figsize=(12, 6))\nsns.lineplot(x='date', y=\"production\", data=df_all, palette=\"tab10\", linewidth=2.5)\n\nsns.despine(left=True)","d64522fd":"#df_all = df_all[df_all.harvest_year >= 2006]\ndf_all.shape","3456a1b6":"f, ax = plt.subplots(figsize=(12, 6))\nsns.lineplot(x='harvest_month', y=\"production\", data=df_all)\n\nsns.despine(left=True)","021cf6cf":"mean_production = df_all.groupby(['harvest_month']).mean()['production'].reset_index()\nmean_production.columns = ['harvest_month', 'production_mean']\nmean_production\n\nprint(mean_production.shape)\nmean_production.head(2)","a8bbf181":"f, ax = plt.subplots(figsize=(12, 6))\n#sns.lineplot(x='age', y=\"production\", data=df_group, palette=\"tab10\", linewidth=2.5)\nsns.lineplot(x='age', y=\"production\", data=df_all, palette=\"tab10\", linewidth=2.5)\n\nsns.despine(left=True)","11f6d7e0":"df_all.type.unique()","6dc0da0e":"f, ax = plt.subplots(figsize=(12, 6))\n#print(df_all.groupby(['type'])['date'].count())\nsns.countplot(x=\"type\", palette=\"tab10\", data=df_all)","8b35558a":"ordered_days = df_all.type.value_counts().index\ng = sns.FacetGrid(df_all, row=\"type\", row_order=ordered_days, height=1.7, aspect=4,)\ng.map(sns.distplot, \"production\", hist=False, rug=True, kde_kws={\"shade\": True});","5866a147":"type_prod = df_all.groupby(['date', 'type']).mean()['production'].reset_index()\n\nprint(mean_production.shape)\nmean_production.head(2)\n\n# Initialize a grid of plots with an Axes for each walk\ngrid = sns.FacetGrid(type_prod, col=\"type\", hue=\"type\", palette=\"tab10\",\n                     col_wrap=3, height=2.5)\n\ngrid.map(plt.plot, \"date\", \"production\")\n\ngrid.fig.tight_layout(w_pad=1)\nsns.despine(left=True)","c0e7d910":"f, ax = plt.subplots(figsize=(12, 6))\n#sns.lineplot(x='age', y=\"production\", data=df_group, palette=\"tab10\", linewidth=2.5)\nsns.lineplot(x='age', y=\"production\", data=df_all, hue='type', palette=\"tab10\", linewidth=2.5, legend='full')\n\nsns.despine(left=True)","5975ec75":"df_all.field.unique()","bd8fe73f":"f, ax = plt.subplots(figsize=(12, 6))\nsns.countplot(x=\"field\", data=df_all)","1373ea3d":"fields    = [0, 9, 27] \ndf_filter = df_all[df_all.field.isin(fields)]\n\nf, ax = plt.subplots(figsize=(12, 6))\nsns.lineplot(x='date', y=\"production\", data=df_filter, hue='field', palette=\"tab10\", linewidth=2.5, legend='full')\n\nsns.despine(left=True)","e003c2ea":"field_prod = df_all.groupby(['date', 'field']).mean()['production'].reset_index()\n\n# Initialize a grid of plots with an Axes for each walk\ngrid = sns.FacetGrid(field_prod, col=\"field\", hue=\"field\", palette=\"tab10\",\n                     col_wrap=6, height=2.5)\n\ngrid.map(plt.plot, \"date\", \"production\")\n\ngrid.fig.tight_layout(w_pad=1)\nsns.despine(left=True)","869fc783":"f, ax = plt.subplots(figsize=(12, 6))\n\n# Draw a nested boxplot to show bills by day and time\nsns.boxplot(x=\"field\", y=\"production\", data=df_all)\nsns.despine(offset=10, trim=True)","11023daf":"# read\ndf_field = pd.read_csv(path+'field-0.csv')\ndf_field['field'] = 0\nfor i in range(1, 28):\n    _df_field = pd.read_csv(path+'field-{}.csv'.format(i))\n    _df_field['field'] = i\n    df_field = pd.concat([df_field, _df_field])\n\n# remove duplicates\ndf_field = df_field.drop_duplicates()\n\n# Group \ndf_field = df_field.groupby(['month', 'year', 'field']).mean().reset_index()\nprint(df_field.shape)\ndf_field.head()","b6f86e68":"# df_all\ndf_all   = pd.merge(df_all, df_field, left_on=['harvest_year', 'harvest_month','field'], \n                    right_on=['year', 'month', 'field'], how='inner').reset_index()\n\nprint(df_all.shape)\ndf_all.head()","681f0672":"df_all.columns","2a98d728":"f, ax = plt.subplots(figsize=(10, 10))\nfeatures  = ['temperature', 'dewpoint',\n               'windspeed', 'Soilwater_L1', 'Soilwater_L2', 'Soilwater_L3',\n               'Soilwater_L4', 'Precipitation']\ncorr = df_all[features+['production']].corr()\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, annot=True, linewidths=.5,cmap=\"YlGnBu\")","b7323b7b":"# Features i will duplicate with the past months\nfeatures  = ['temperature', 'dewpoint', 'windspeed', 'Precipitation', 'Soilwater_L1']\n\ndf_all    = df_all.drop(columns=['Soilwater_L2', 'Soilwater_L3','Soilwater_L4'])","ccb5227f":"#df_all2 = df_all.copy()\ndf_all.head()","bdcfdd14":"df_group = df_all.groupby(['field', 'date']).mean().reset_index()[['field', 'date', 'production'] + features ]\ndf_group = df_group.sort_values(['field', 'date'])\nprint(df_group.shape)\ndf_group.head()","7789ff00":"# Collect shift values of variables in all features time\nperiod = 2\n\nnew_features = {}\nfor f in features:\n    new_features[f] = []\n    for i in range(1, period):\n        new_features[f].append('{}_{}'.format(f, i))\n        df_group['{}_{}'.format(f, i)] = df_group[f].shift(i).fillna(df_group[f].mean())\n        #df_group['{}_{}'.format(f, i)] = df_group[f].rolling(i, min_periods=1).mean().fillna(df_group.temperature.mean())","becbe146":"fig = plt.figure(figsize=(18, 8))\n\nfor i in range(1, len(features)+1):\n    ax1 = fig.add_subplot(240+i)\n    \n    f = features[i-1]\n    f_filter = [f] + new_features[f]+['production']\n    corr    = df_group[f_filter].corr()\n    g = sns.barplot(x=[i-1 for i in range(1, period+1)], y=corr['production'].values[:-1], palette=\"YlGnBu\", ax=ax1)\n    plt.title(f)\n    plt.xticks(rotation=45)\nplt.show() #corr['production'].keys().values[:-1]","67e8087b":"df_all.head()","8349bac3":"df_group= df_group.drop(features+['production'], axis=1)\ndf_group.head()","76b3c6ff":"df_all = df_all.drop(['index', 'month', 'year'], axis=1)\ndf_all = pd.merge(df_all, df_group, left_on=['field', 'date'], right_on=['field','date'], how='inner').reset_index()\n\nprint(df_all.shape)\ndf_all.head()","88d7d6cf":"df_soil = pd.read_csv(path+'soil_data.csv')\nprint(df_soil.shape)\ndf_soil.head()","0cc2ad5f":"# Join datasets\ndf_all_soil = pd.merge(df_all, df_soil, on='field', how='inner')\nprint(df_all_soil.shape)\ndf_all_soil.head()","9506f3ca":"f, ax = plt.subplots(figsize=(10, 10))\nfeatures  = list(df_soil.columns.values) + ['production']\ncorr      = df_all_soil[features].corr()\n\n#Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, linewidths=.5,cmap=\"YlGnBu\")","2500209e":"df_all.columns","555eab4e":"## Import the random forest model.\nfrom sklearn.ensemble import RandomForestRegressor\n\n## This line instantiates the model. \nrf = RandomForestRegressor() \n\n# data\ndf      = df_all_soil[~df_all_soil.production.isna()]\nX_train = df.drop(['production', 'date', 'Id', 'index'], axis=1)\ny_train = df.production.values\n\n## Fit the model on your training data.\nrf.fit(X_train, y_train) ","7e235045":"# feature_importances\nfeature_importances = pd.DataFrame(rf.feature_importances_, \n                                   index = X_train.columns, \n                                   columns=['importance']).sort_values('importance', ascending=False).reset_index()\nfeature_importances.head(3)","e2f01131":"size = 50\nf, ax = plt.subplots(figsize=(10, 10))\nsns.barplot(x=\"importance\", y='index', data=feature_importances.head(size), palette=\"rocket\")","ed51d205":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nimport sklearn.model_selection","a8998b25":"# Load Dataset\n# y~X\ndf_train = df_all_soil[~df_all_soil.production.isna()]\nX        = df_train.drop(['production', 'date', 'Id'], axis=1)\n\n#Filter importance\nfeatures = list(feature_importances['index'].values)[:15]\nX        = X[features]\n# y\ny        = df.production.values\n\n# normalize\nscaler = StandardScaler()\nnorm_X = scaler.fit_transform(X)\n\n# Split\nX_train, X_test, y_train, y_test = sklearn.model_selection.train_test_split(norm_X, y, test_size=0.2, random_state=1)\n(X_train.shape, X_test.shape)","bfc8f5ca":"base_model = RandomForestRegressor()\nbase_model.fit(X_train, y_train)","220af96e":"y_hat = base_model.predict(X_test)\n\nscore_mae = sklearn.metrics.mean_absolute_error(y_test, y_hat)\nr2        = sklearn.metrics.r2_score(y_test, y_hat)\n\n#MAE score 0.04333937538802412\nprint(\"MAE score\", score_mae)","a0d260c2":"sns.jointplot(x=y_test, y=y_hat, kind=\"reg\", color=\"m\", height=7)\n#sns.scatterplot(y=(y_test-y_hat), x=[i for i in range(len(y_test))], ax=ax2)","b8a884bb":"#df_all.to_csv('dataset\/df_all.csv')","9907e990":"df_test = df_all[df_all.production.isna()]\nprint(df_test.shape)\ndf_test.tail(2)","411417a9":"#Filter importance\nX  = df_test[features]\nX  = scaler.transform(X) # normalize\n\n# y\ny = df.production.values","623c16f2":"prod = base_model.predict(X) \nprod[:10]","930c0d79":"## create a submission.csv\nimport math\nf = open('submission.csv', 'w')\nf.write(\"Id,production\\n\")\nfor i in range(len(df_test.Id.values)):\n    _id = df_test.Id.values[i]\n    p   = math.fabs(prod[i])\n    f.write(\"{},{}\\n\".format(_id, p))\nf.close()","f07c1821":"Build test dataset to predict ","3f1be5f3":"It may be necessary to remove the data before 2006 because the production pattern has changed dramatically. This information prior to 2006 may bring noise, a better analysis should be made","54695b95":"Regarding age, apparently some types are only harvested at a certain age, that is, not all types have coliros during the clinical of 25 years. Only type 1 has production over 25 years.","20eb2ae5":"The productive cycle and data are also different by type. This may reinforce the idea of separating forecast models by type.","16f12c5a":"The average production per field is similar. Few variations.","a1e38db1":"Relationship between the thermal variables, in the case this relation is for the harvest month itself. There is a good chance that previous months have relations with future production as well, it is necessary to investigate this relationship and how far it extends.","4ca286dd":"### **train.csv** and **test.csv**\n\nBasic data containing palm tree information","cfe9ba56":"### Feature Importance Measures\n\nFind the main features for the production target. Uses a RandomRorest to identify features\n","f930c57c":"#### Production per type\n\nThe 'type' of train\/test is not established in the documentation, but, I will try to understand this variable.\n","f20e6332":"Field 0 has the largest number of production records. Let's map some fields in time to identify if the pattern changes a lot in relation to the general.","afa16e44":"The plant reaches its maximum production at 15 years of age. Decreasing after this period. But depende the type","a82b966d":"Most of the production is carried out using the type 5.","493d0883":"#### Train Model","7c276257":"...to be continued","75b94bd1":"Average production per date follows a pattern. The end and beginning of the year is always low production, growing throughout the year until reaching peak production.\n\nThe year of 2011 had the highest peak of production of the entire time series, which has been growing over the years.","343ee5aa":"### soil_data.csv\n\nInformation on the soil on which each field is","ecbce93d":"### Submission\n\nIt makes the predict of the basic model and creates the sample to substrate in kaggle, finishing the complete pipeline","e1f41481":"## Base Model \n\n#### Creation of a baseline model to finalize the competition submission pipeline. The idea is to create the most basic for future improvements.","a1ef065a":"## Data Analisys\n\nWe will visualize the variables and datasets present in the competition to understand some existing patterns.","39c4a070":"# Initial Data Exploration\n\n### Competition\n\nIn this competition, you will predict palm oil harvest productivity with data provided by AGROPALMA. The dataset contains information about palm trees, their harvest dates, atmospheric data during development of the plants and soil characteristics of the fields where the trees are located in.\n\nhttp:\/\/www.agropalma.com.br\/\n\n### What is palm oil?\n\nPalm oil is an important and versatile vegetable oil which is used as a raw material for both food and non-food industries.\n\n* Palm oil trees can grow up to 20 metres tall with an average life of 25 years.\n* Palm oil grows across or 10 degrees north \/ south of the equator.\n* The tree starts to bear fresh fruit bunches (FFBs) after three years.\n* The oil palm tree is also know as Elaeis Guineensis.\n* Each individual piece of fruit on fruit bunch contains 50% oil.\n* The nut, referred to as a kernel, at the centre of each piece of fruit, is where palm kernel oil is extracted from.\n* Palm oil can be harvested 12 months of the year.\n* Each tree can produce 10 tonnes of fresh fruit bunches per hectare.\n* On average 3.9 tonnes of crude palm oil and 0.5 tonnes of palm kernel oil can be extracted per hectare.\n* Leftover fibre from the palm kernel mill process provides a product called palm kernel expeller. This is used in animal feed, but can also be used to make products like paper or fertilizer.\n* Palm oil requires 10 times less land than other oil-producing crops.\n* It is entirely GM free\n\n### Objective\n\nThe purpose of this notebook is to perform the initial exploration of the presented data. Identify patterns, most important variables and distributions.\n\n* Data analysis\n* Data visualization\n* Creating the Basic Model\n* Submission File Creation\n\nKaggle: https:\/\/www.kaggle.com\/c\/kddbr-2018","01c041f3":"#### Production per age\n\nWith each harp the palmetto is a certain age. The production may vary with age, I also believe that some types of palm trees can only be harvested in a period of age","814df16c":"An everage of production in the year from month to month. It follows an increase in the year util the peak of production in month 10 and a sudden fall","7ae53a71":"An important point is that the variables Soilwater_l * have high correlation between each other and with the variable dewpoint. I do not think it's necessary to use all.","dfb62cd9":"### field_*.csv\n\nThese files hold atmospheric data from January 2002 to December 2017, and can be used to estimate the weather conditions during the development of the plant. Notice that weather does influence the production. Using only a single month prior to harvest is probably too little data. Participants should decide how far back in the past they want to look when training models.\n\n","22a42703":"Some changes, but the default is apparently the same as previously shown. Apparently all fields have production in the dataset presented for the period.","f165c94d":"Correlation analysis of the production variable with each variable in time. Uses a period of 11 months, a complete annual cycle. Each month is related to the correlation of production","db466d63":"The #production is normalized betwenn [0,1] with mean 0.16. ","d480716b":"#### #production\n\nAnalisys of the variable production","5edb1257":"#### Create a file *submission.csv*","7e647561":"#### Production per month","fe2047e7":"#### #field\n\nfield: an anonymous id given to each field in the dataset","7276fe17":"Apparently the features coming from *soil_data.csv* do not have much relevance to the model. I think using only the field field is a better feature than adding 73 features of *soil_data.csv*","67ad76a1":"#### Prepare Dataset","ff0a33b8":"The production distribution by type is different. Although the predominant type is even type = 5, it may make sense to separate one model for each type."}}