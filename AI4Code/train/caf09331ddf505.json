{"cell_type":{"36e871bd":"code","7590ab82":"code","4529e385":"code","b2f3362d":"code","776af530":"code","2b6489a1":"code","300e9ee0":"code","9084393f":"code","c1347795":"code","7f0836ef":"code","f1227567":"code","c4793010":"code","bd95d86a":"code","ec8f79ab":"code","b41bd061":"code","3c1362ef":"code","c26b0995":"code","b5ea33b8":"code","956f9724":"code","22c61e20":"code","c87b953e":"code","143017f0":"code","f5f1fc03":"code","1e750d68":"code","890c5f43":"code","699e6a80":"code","4c8c504b":"code","20299938":"code","1cc29ccd":"code","79f316a2":"code","d69d1442":"code","3c1f9f03":"code","c8244877":"code","7d69e871":"code","9ad8be70":"code","944af82f":"code","6dab4380":"code","0ff9d3b9":"code","924311a6":"code","9a77258b":"code","989732b1":"code","25f6bbf5":"code","dc127fab":"code","866ce4c9":"code","e96ea1a2":"code","93c18817":"code","18b6b5a9":"code","b06928a0":"code","168c07ba":"code","fd7265df":"code","804e89ed":"code","4bc77132":"code","9782863e":"code","98eb2d3b":"code","0f37666c":"code","734f54c2":"code","48a197b4":"markdown","7c16315a":"markdown","087dc012":"markdown","d949193f":"markdown","a3f907d4":"markdown","f92ab8f4":"markdown","c84c3997":"markdown","10913ac5":"markdown","5ce02139":"markdown","1047e26a":"markdown","9f3afeb9":"markdown","b41697e8":"markdown","0742c2ac":"markdown","fc86ec11":"markdown","e8ea0bf9":"markdown","f798b64a":"markdown","a9b06fc3":"markdown","27040554":"markdown","7ba5c8d7":"markdown","416eee8c":"markdown","a2a6f607":"markdown","cdb8d361":"markdown","db8b5c43":"markdown","aea240e8":"markdown","1fb6c7c7":"markdown","c4af85c0":"markdown","2d662a2c":"markdown","72e6de9c":"markdown","7080342e":"markdown","6dca8959":"markdown","c9e4921c":"markdown","38937abf":"markdown"},"source":{"36e871bd":"#working with data\nimport pandas as pd\nimport numpy as np\n\n#visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n## Scikit-learn features various classification, regression and clustering algorithms\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn import preprocessing\nfrom sklearn.metrics import average_precision_score, confusion_matrix, accuracy_score, classification_report,f1_score\n\n## Algo\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n","7590ab82":"\n#loading Data\nData = pd.read_csv('..\/input\/Bank_Personal_Loan_Modelling.csv')","4529e385":"#Fetching First 5 col\nData.head()","b2f3362d":"Data.columns","776af530":"#checking data Type of each attributes\nData.dtypes","2b6489a1":"shape_data=Data.shape\nprint('Data set contains \"{x}\" number of rows and \"{y}\" number of columns columns'.format(x=shape_data[0],y=shape_data[1]))","300e9ee0":"#checking for Null Values\nData.isnull().sum()","9084393f":"sns.heatmap(Data.isnull(),yticklabels=False,cbar=False,cmap='viridis')","c1347795":"#overview of data\nData.describe().transpose()","7f0836ef":"#Value Counts of all Binary Catagorical Data\nData[['Personal Loan','Securities Account','CD Account','Online','CreditCard']].apply(pd.value_counts)","f1227567":"#lets visualize data apart from target variables via count plot\nfig, axes = plt.subplots(2, 2, figsize=(12, 6))\nsns.countplot(x=Data['Securities Account'],ax=axes[0,0])\nsns.countplot(x=Data['CD Account'],palette='Set1',ax=axes[0,1])\nsns.countplot(x=Data['Online'],palette='Set2',ax=axes[1,0])\nsns.countplot(x=Data['CreditCard'],palette='Set3',ax=axes[1,1])\nfig.tight_layout()","c4793010":"sns.countplot(x=Data['Personal Loan'])","bd95d86a":"#Watching the graphs above we can say there is a significant diffrenece among binary catagorical data encluding 'Online' feature.\n#infact out Traget Variable i.e. 'Personal Loan' is also having significant difference.\n#now lets see how our Target variable is forming up with our other Catagorical binary data.\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 6))\nsns.countplot(x=\"Securities Account\",data=Data, hue=\"Personal Loan\",ax=axes[0,0])\nsns.countplot(x=\"CD Account\",data=Data, hue=\"Personal Loan\",palette='Set1',ax=axes[0,1])\nsns.countplot(x=\"Online\",data=Data, hue=\"Personal Loan\",palette='Set2',ax=axes[1,0])\nsns.countplot(x=\"CreditCard\",data=Data, hue=\"Personal Loan\",palette='Set3',ax=axes[1,1])\nfig.tight_layout()","ec8f79ab":"#distribution of Family and Education\nfig, axes = plt.subplots(2, figsize=(12, 6))\nsns.countplot(x=\"Family\",data=Data,ax=axes[0])\nsns.countplot(x=\"Education\",data=Data,palette='Set1',ax=axes[1])\nfig.tight_layout()","b41bd061":"#how this distribution is spread out or segregated over Personal Loan\nfig, axes = plt.subplots(2, figsize=(12, 6))\nsns.countplot(x=\"Family\",data=Data, hue=\"Personal Loan\",ax=axes[0])\nsns.countplot(x=\"Education\",data=Data, hue=\"Personal Loan\",palette='Set1',ax=axes[1])\nfig.tight_layout()","3c1362ef":"#getting summary of numerical data, over-viewing data\nData[['Age','Experience','Income','CCAvg','Mortgage']].describe().transpose()","c26b0995":"#A Skewness value of 0 in the output denotes a symmetrical distribution\n#A negative Skewness value in the output denotes tail is larger towrds left hand side of data so we can say left skewed\n#A Positive Skewness value in the output denotes tail is larger towrds Right hand side of data so we can say Right skewed\nData[['Age','Experience','Income','CCAvg','Mortgage']].skew()","b5ea33b8":"#The distplot shows the distribution of a univariate set of observations.\nfig, axes = plt.subplots(2, 2, figsize=(14, 4))\nsns.distplot(Data['Age'],ax=axes[0,0])\nsns.distplot(Data['Experience'],ax=axes[0,1])\nsns.distplot(Data['Income'],ax=axes[1,0])\nsns.distplot(Data['CCAvg'],ax=axes[1,1])\naxes[0,0].set_title('Age Distribution')\naxes[0,1].set_title('Experience Distribution')\naxes[1,0].set_title('Income Distribution')\naxes[1,1].set_title('CCAvg Distribution')\nplt.tight_layout()","956f9724":"sns.distplot(Data['Mortgage'])","22c61e20":"#lets try correlation metrix as well to get more insight of the pairing of all data\n#and visualise it via heatmap\nfig,ax= plt.subplots(figsize=(10, 10))\nsns.heatmap(Data.corr())\nfig.tight_layout()","c87b953e":"#pair plot to check the distribution and correlation of all variables\nsns.pairplot(Data)","143017f0":"#checking the spread of Target Column\nsns.countplot(x=Data['Personal Loan'])","f5f1fc03":"#from the above observations lets check the spread of Target coloumn with correlated features.\n#1.spread over CCAvg \nsns.kdeplot(Data[Data['Personal Loan'] == 0]['CCAvg'], shade=False)\nsns.kdeplot(Data[Data['Personal Loan'] == 1]['CCAvg'], shade=True)\nplt.title(\"CCAvg spread over Personal Loan\")","1e750d68":"#2.Spread over Mortage\nsns.kdeplot(Data[Data['Personal Loan'] == 0]['Mortgage'], shade=False)\nsns.kdeplot(Data[Data['Personal Loan'] == 1]['Mortgage'], shade=True)\nplt.title(\"CCAvg spread over Personal Loan\")","890c5f43":"#checking the variance of Zip Code\n#high variance means fearure does not affect the target variable\nData[['ZIP Code','ID']].var()\n","699e6a80":"#Creating Dummy Variables for multi-Catagorical Column\nFamily_dummies= pd.get_dummies(Data['Family'],drop_first=True,prefix='Family_Size')\nEducation_dummies= pd.get_dummies(Data['Education'],drop_first=True,prefix='Education_Level')","4c8c504b":"#concatinating the dummy variables to Data\nData = pd.concat([Data,Family_dummies,Education_dummies],axis=1)","20299938":"Cleaned_Data=Data.drop(['ID','ZIP Code','Family','Education','Experience'],axis=1)","1cc29ccd":"Cleaned_Data.head()","79f316a2":"#Split the data into training and test set in the ratio of 70:30 respectively\nX = Cleaned_Data.drop('Personal Loan',axis=1)\ny = Cleaned_Data['Personal Loan']\n\n# split data into train subset and test subset\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)\n\n# checking the dimensions of the train & test subset\n# to print dimension of train set\nprint(X_train.shape)\n# to print dimension of test set\nprint(X_test.shape)","d69d1442":"# Train and Fit model\nmodel = LogisticRegression(random_state=0)\nmodel.fit(X_train, y_train)","3c1f9f03":"#predict the Personal Loan Values\ny_Logit_pred = model.predict(X_test)\ny_Logit_pred","c8244877":"predicted_Logit_probas = model.predict_proba(X_test)\npredicted_Logit_probas","7d69e871":"# Let's measure the accuracy of this model's prediction\nprint(\"confusion_matrix\")\nprint(confusion_matrix(y_test,y_Logit_pred))","9ad8be70":"# And some other metrics for Test\nprint(classification_report(y_test, y_Logit_pred, digits=2))","944af82f":"# instantiate learning model (k = 3)\nknn = KNeighborsClassifier(n_neighbors = 3, weights = 'uniform', metric='euclidean')\n# fitting the model\nknn.fit(X_train, y_train)\n\n# predict the response\ny_Knn_pred = knn.predict(X_test)\n\n# evaluate Model Score\nprint(classification_report(y_test, y_Knn_pred, digits=2))","6dab4380":"# creating odd list of K for KNN\nmyList = list(range(3,30,2))\n\n# empty list that will hold accuracy scores\nac_scores = []\n\n# perform accuracy metrics for values from 3,5....29\nfor k in myList:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    # predict the response\n    y_pred = knn.predict(X_test)\n    # evaluate F1 Score\n    scores = f1_score(y_test, y_pred)\n    ac_scores.append(scores)\n\n# changing to misclassification error\nMSE = [1 - x for x in ac_scores]\n\n# determining best k\noptimal_k = myList[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","0ff9d3b9":"#propability prediction\npredicted_Logit_probas = knn.predict_proba(X_test)\npredicted_Logit_probas","924311a6":"naive_model = GaussianNB()\nnaive_model.fit(X_train, y_train)\n","9a77258b":"predicted_probas_NB = naive_model.predict_proba(X_test)\n\npredicted_probas_NB","989732b1":"prediction_NB = naive_model.predict(X_test)","25f6bbf5":"print(classification_report(y_test, prediction_NB, digits=2))","dc127fab":"#confusion matric via Heat Map\nconf_mat = confusion_matrix(y_test, y_Logit_pred)\nconf_mat\n","866ce4c9":"df_conf_mat = pd.DataFrame(conf_mat)\nplt.figure(figsize = (7,4))\nsns.heatmap(df_conf_mat, annot=True,cmap='Blues', fmt='g')","e96ea1a2":"#determining false positive rate and True positive rate, threshold\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_Logit_pred)\nroc_auc_logit = metrics.auc(fpr, tpr)\n# print AUC\nprint(\"AUC : % 1.4f\" %(roc_auc_logit)) ","93c18817":"#plotting ROC curve\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc_logit)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","18b6b5a9":"#confusion matric via Heat Map\nconf_mat = confusion_matrix(y_test, y_Knn_pred)\nconf_mat\n","b06928a0":"df_conf_mat = pd.DataFrame(conf_mat)\nplt.figure(figsize = (7,4))\nsns.heatmap(df_conf_mat, annot=True,cmap='Blues', fmt='g')","168c07ba":"#determining false positive rate and True positive rate, threshold\nfpr, tpr, threshold = metrics.roc_curve(y_test, y_Knn_pred)\nroc_auc_knn = metrics.auc(fpr, tpr)\n# print AUC\nprint(\"AUC : % 1.4f\" %(roc_auc_knn)) ","fd7265df":"#plotting ROC curve\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc_knn)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","804e89ed":"#confusion matric via Heat Map\nconf_mat = confusion_matrix(y_test, prediction_NB)\nconf_mat","4bc77132":"df_conf_mat = pd.DataFrame(conf_mat)\nplt.figure(figsize = (7,4))\nsns.heatmap(df_conf_mat, annot=True,cmap='Blues', fmt='g')","9782863e":"#determining false positive rate and True positive rate, threshold\nfpr, tpr, threshold = metrics.roc_curve(y_test, prediction_NB)\nroc_auc_NB = metrics.auc(fpr, tpr)\n# print AUC\nprint(\"AUC : % 1.4f\" %(roc_auc_NB)) ","98eb2d3b":"#plotting ROC curve\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc_NB)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","0f37666c":"#Using K fold to check how my algorighm varies throughout my data if we split it in 10 equal bins\nmodels = []\nmodels.append(('Logistic Regression', LogisticRegression()))\nmodels.append(('K-NN', KNeighborsClassifier()))\nmodels.append(('Naive Bayes', GaussianNB()))\n\n# evaluate each model\nresults = []\nnames = []\nscoring = 'f1'\nfor name, model in models:\n\tkfold = model_selection.KFold(n_splits=10, random_state=101)\n\tcv_results = model_selection.cross_val_score(model, X_train, y_train, cv=kfold, scoring=scoring)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tprint(\"Name = %s , Mean F1-Score = %f, SD F1-Score = %f\" % (name, cv_results.mean(), cv_results.std()))","734f54c2":"# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.plot(results[0],label='Logistic')\nplt.plot(results[1],label='KNN')\nplt.plot(results[2],label='Naive Bayes')\nplt.legend()\nplt.show()","48a197b4":"#### <font color='green'><p>a. Binary Catagorical Data distribution<\/p><\/font>\n <p>1. Personal Loan - Did this customer accept the personal loan offered in the last campaign? (Target)<\/p>\n <p>2. Securities Account - Does the customer have a securities account with the bank?<\/p>\n <p>3. CD Account - Does the customer have a certificate of deposit (CD) account with the bank?<\/p>\n <p>4. Online - Does the customer use internet banking facilities?<\/p>\n <p>5. Credit Card - Does the customer use a credit card issued by UniversalBank?<\/p>","7c16315a":"From the above grap we can conclude that the customers who do not have personal loan and customer who has personal loan have high mortgage","087dc012":"the above graph represent that Customers having personal loan have a higher credit card average","d949193f":"### <font color='red'>Step 1 <\/font> Importing Libraries, Data and Undestanding the column description.","a3f907d4":"#### Model Scoring\n<p>1. Accuracy :: 88% <\/p>\n<p>2. Re-call ::  46%<\/p>\n<p>3. Precision :: 63% <\/p>\n<p>4. F1-Score :: 53%<\/p>\n<p> *Accuracy is not a good measure in this situation because Among these 5000 customers, only 480 (= 9.6%) accepted the personal loan that was offered to them in the earlier campaign\n    so we will go for F1-Score and 53% as out model Score.","f92ab8f4":"### 1.Logistic Regression Model Evaluation","c84c3997":"### <u> <font color='red'>Step 7 :<\/font> Determining Which model is better<\/u>","10913ac5":"#### Observation :\n<p><b>1.Age<\/b> feature is normally distributed, can be confirmed as\n        <p>   a. skewness is -0.029 that means very slightly left skewed<\/p>\n        <p>   b. Mean(45.33) and Median(45.00) are very close to each other<\/p><\/p>\n<p><b>2.Experience<\/b> is normally distributed <\/p>\n        <p>   a. skewness is -0.026 that means very slightly left skewed<\/p>\n        <p>   b. Mean(21.10) and Median(21) are very close to each other<\/p>\n        <p>   c.There are negative values, This could be a data input error, can simply remove as they are quite few<\/p><\/p>\n<p><b>3.Income<\/b> is positively skewed, can be confirmed as\n        <p>   a. skewness is 0.841 that means highly right skewed<\/p>\n        <p>   b. Mean(73.77) is greater than the Median(64.0)<\/p><\/p>\n<p><b>4.CCAvg<\/b> is positively skewed, can be confirmed as\n        <p>   a. skewness is 1.598443 that means highly right skewed<\/p>\n        <p>   b. Mean(1.93) is greater than the Median(1.5)<\/p><\/p>\n<p><b>5.Mortgage<\/b> is positively skewed, can be confirmed as\n        <p>   a. skewness is 2.104002 that means highly right skewed<\/p>\n        <p>   b. Mean(56.49) is greater than the Median(0.0)<\/p>\n        <p>   c. Most of the Customers have a mortgage of less than 40K. However the max value is 635K<\/p><\/p>\n        \n<p>\n<p><b>Experience and age are highly correlated which is quite logical<\/b><\/p>\n<p><b>Income is correlated with CCAvg and Mortgage<\/b>\n<\/p>\n        \n\n","5ce02139":"### <u> <font color='red'>Step 2: <\/font> Study the data distribution in each attribute.<\/u>","1047e26a":"# Supervised Classification Project with KNN,Naive Bayes,Logistic Regression\n### <u>Data Description:<\/u>\nThe file Bank.xls contains data on 5000 customers. The data include customer\ndemographic information (age, income, etc.), the customer's relationship with\nthe bank (mortgage, securities account, etc.), and the customer response to the\nlast personal loan campaign (Personal Loan). Among these 5000 customers,\nonly 480 (= 9.6%) accepted the personal loan that was offered to them in the\nearlier campaign.\n\n### <u>Domain:<\/u>\nBanking\n\n### <u>Context:<\/u>\nThis case is about a bank (Thera Bank) whose management wants to explore\nways of converting its liability customers to personal loan customers (while\nretaining them as depositors). A campaign that the bank ran last year for liability\ncustomers showed a healthy conversion rate of over 9% success. This has\nencouraged the retail marketing department to devise campaigns with better\ntarget marketing to increase the success ratio with minimal budget.\n\n### <u>Objective:<\/u>\nThe classification goal is to predict the likelihood of a liability customer buying\npersonal loans.\n","9f3afeb9":"### <u> <font color='red'>Step 5 :<\/font> Use different classification models (Logistic, K-NN and Na\u00efve Bayes) to predict the likelihood of a customer buying personal loans<\/u>","b41697e8":"### <u> <font color='red'>Step 6 :<\/font> Print the confusion matrix for all the above models<\/u>","0742c2ac":"#### ROC-AUC for Logistic Regression","fc86ec11":"#### <font color='green'><p>b. Ordinal Categorical Data Distribution:<\/p><\/font>\n <p>1. Family - Family size of the customer<\/p>\n <p>2. Education - education level of the customer (1. Undergrad 2. Graduate 3. Advanced\/Professional)<\/p>\n","e8ea0bf9":"### <font color='green'>B. K-NN<\/font>","f798b64a":"### <font color='green'>A. Logistic Regression<\/font>","a9b06fc3":"### <u> <font color='red'>Step 3: <\/font> Study the data distribution in Target Coloumn.<\/u>","27040554":"#### Model Scoring\n<p>1. Accuracy :: 90% <\/p>\n<p>2. Re-call ::  38%<\/p>\n<p>3. Precision :: 57% <\/p>\n<p>4. F1-Score :: 45%<\/p>\n<p> *Accuracy is not a good measure in this situation because Among these 5000 customers, only 480 (= 9.6%) accepted the personal loan that was offered to them in the earlier campaign\n    so we will go for F1-Score and 45% as out model Score.","7ba5c8d7":"#### Droping Columns\n<p>1. ID :: Reason=> High Variance,no correlation <\/p>\n<p>2. ZIP Code :: Reason=> High Variance,no correlation <\/p>\n<p>3. Family :: Reason=> Already Created Dummy Variable for this feature <\/p>\n<p>4. Education :: Reason=> Already Created Dummy Variable for this feature<\/p>\n<p>5. Experience :: Reason => Age and Experience are highly Correlated thus we can drop one and since Experience column also have some negative or incorrect data thus it is more feasable to drop Experience Column.","416eee8c":"### THE END","a2a6f607":"#### Observation :\n<p>1.Most of the customers who does not have 'Personal Loan' have one member in family, else data is not much of segregated<\/p>\n<p>2.Most of the customers who does not have 'Personal Loan' are UnderGraduate, and we can see if education level increases then more customer have personal loan <\/p>","cdb8d361":"### 2.KNN Model Evaluation","db8b5c43":"### <font color='green'>C. Naive-Bayes Algorithm<\/font>","aea240e8":"#### AUC-ROC for KNN","1fb6c7c7":"#### <font color='green'><p>c. Quantitative Data Distribution:<\/p><\/font>\n <p>1. Age - Age of the customer<\/p>\n <p>2. Experience - Years of experience<\/p>\n <p>3. Income - Annual income of the customer (000)<\/p>\n <p>4. CCAvg - Average credit card spending (000)<\/p>\n <p>5. Mortage - Value of house mortgage if any. (000)<\/p>\n","c4af85c0":"### 3.Naive Bayes Model Evaluation","2d662a2c":"### <font color='green'>Conclusion<\/font>\n<p>After compairing Logistic,KNN,Naive Bayes algo we can conclude that <b>Logistic Regression is Best among three<\/b><\/p>\n<p>1.ROC-AUC of Logistic is 80%<\/p>\n<p>2.F1-Score of Logistic is 73%<\/p>\n<p>3.Mean F1-Score is also 0.72, with Standard Deviation of 0.041 <\/p>\n\n#### <p>Why Logistic Performs better in this Dataset? <\/p>\n<p>If we look at the distribution of Target variable we can see data contain lots of Noise, as we only have 9.6% of data that helps us determine our resultant.<\/p>\n<p>KNN fails becasue as the noise increses so the density of false data points increases, so while capturing the nearest data point it captured false data point as well, thus decresing the Capture rate and presision.<\/p>\n<p>Naive Bayes outperforms as number of target variable is only 9.6% so there are chances that for given set of evidence we have both true results and false results thus the probability of missclassification increases which leads to low performance of Naive Bayes<\/p>\n<p>Naive Bayes is an Generative (sometimes called Informative) Classifier,  so it model the densities of classes and select the class that most likely produce the features<\/p>\n<p>Logistic Regression is a Discriminative Classifer, it try to model class boundary and membership directly, e.g. in a simple 2-feature dimension case this could mean trying to finding the line that best separates the classes, so if we see the distribution of 'Personal Loan' over other features we can see clearly that this data falls under Discrimative Classifer Problem<\/p>","72e6de9c":"#### Model Scoring\n<p>1. Accuracy :: 95% <\/p>\n<p>2. Re-call ::  63%<\/p>\n<p>3. Precision :: 85% <\/p>\n<p>4. F1-Score :: 72%<\/p>\n<p> *Accuracy is not a good measure in this situation because Among these 5000 customers, only 480 (= 9.6%) accepted the personal loan that was offered to them in the earlier campaign\n    so we will go for F1-Score and 72% as out model Score.","7080342e":"### <u> <font color='red'>Step 4 :<\/font>Preparing Test and Train Data<\/u>","6dca8959":"### Data Uderstanding\n#### <p>1.There is NO MISSING data.<\/p>\n#### <p>2.The feature ID does not add any information, there is no realationship between Id and Loan, We can remove this row.<\/p>\n\n#### <p>3.We Have Two nominal variable is :<\/p>\n<p>a. ID - Customer ID<\/p>\n<p>b. Zip Code - Home Address ZIP code.<\/p>\n\n#### <p>4.We have Five Binary Catagorical Data:<\/p>\n <p>a. Personal Loan - Did this customer accept the personal loan offered in the last campaign? (Target)<\/p>\n <p>b. Securities Account - Does the customer have a securities account with the bank?<\/p>\n <p>c. CD Account - Does the customer have a certificate of deposit (CD) account with the bank?<\/p>\n <p>d. Online - Does the customer use internet banking facilities?<\/p>\n <p>e. Credit Card - Does the customer use a credit card issued by UniversalBank?<\/p>\n \n#### <p>5.We Have Two Ordinal Categorical Data:<\/p>\n<p>a. Family - Family size of the customer<\/p>\n<p>b. Education - education level of the customer (1. Undergrad 2. Graduate 3. Advanced\/Professional)<\/p>\n\n#### <p>6.We have Five Quantitative Data :<\/p>\n<p>a. Age - Age of the customer<\/p>\n<p>b. Experience - Years of experience<\/p>\n<p>c. Income -  Annual income of the customer (000) <\/p>\n<p>d. CCAvg - Average credit card spending (000) <\/p>\n<p>e. Mortage -  Value of house mortgage if any. (000)<\/p>\n","c9e4921c":"#### AUC-ROC for Naive Bayes","38937abf":"#### Observation :\n<p>1.Most of the customers who does not have 'Personal Loan' have 'Securities Account'<\/p>\n<p>2.Almost All customers who has 'CD Account' have 'Personal Loan' as well.<\/p>\n<p>3.Majority of customers who have 'Personal Loan' also uses 'Online banking facilities', but the difference is not so significant <\/p>\n<p>4.Most of the Customer who does not have 'Personal Loan' also does not have 'Credit Card'<\/p>"}}