{"cell_type":{"ee7f13fe":"code","c92b4d21":"code","5e0d95c3":"code","03177c72":"code","f082ede9":"code","36a53e87":"code","5befb28a":"code","b89e2fd0":"code","bb4e49e5":"code","d0e03131":"code","28c03c75":"code","480a7ead":"code","6ddca897":"code","c40a3d56":"code","15e6a3d9":"code","4be5b8b2":"code","4500c1f0":"code","2dd07448":"code","75c0daa3":"code","094e899a":"code","4732ce82":"code","237fab5a":"code","5b108d7a":"code","6b88d3fb":"code","2f586e95":"code","7c6a2fd2":"code","017da9fb":"code","faa03a7b":"code","9ae19774":"code","0450aca8":"markdown","a2e03ab2":"markdown","819f15bd":"markdown","c25e5f82":"markdown","2ac5f977":"markdown","cb05ea31":"markdown","4401c7e4":"markdown","ecf0a351":"markdown","d6865331":"markdown","bf4c3aee":"markdown","de2f4f4a":"markdown","1dc001bc":"markdown","74fb2e3a":"markdown","b271d5cc":"markdown","ae479ce2":"markdown","06290bdd":"markdown","cf7631a7":"markdown","d8ea5d88":"markdown","591ab150":"markdown","f473ac77":"markdown","0011044f":"markdown","41a36645":"markdown","5f38ee66":"markdown","5eaa885c":"markdown"},"source":{"ee7f13fe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n #   for filename in filenames:\n  #      print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c92b4d21":"from torchvision.datasets import ImageFolder\nimport torchvision.transforms as T\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.dataset import random_split\nimport torch.nn.functional as F\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom skimage.color import rgb2lab, lab2rgb, rgb2gray","5e0d95c3":"DATA_DIR = '..\/input\/imagenet\/imagenet\/'","03177c72":"dataset = ImageFolder(DATA_DIR, transform=T.Compose([T.Resize((256, 256)),T.ToTensor()]))","f082ede9":"len(dataset)","36a53e87":"random_seed = 42\ntorch.manual_seed(random_seed)\n\nval_size = 1000\ntrain_size = len(dataset) - val_size\n\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])\nlen(train_ds), len(val_ds)","5befb28a":"batch_size = 128","b89e2fd0":"train_loader = DataLoader(train_ds, batch_size = batch_size, shuffle = True, num_workers = 4, pin_memory = True)\nval_loader = DataLoader(val_ds, batch_size = batch_size, num_workers = 4, pin_memory = True)","bb4e49e5":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","d0e03131":"def to_device(data, device):\n    if isinstance(data, (list, tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking = True)","28c03c75":"class DeviceDataLoader():\n    def __init__(self, dl, device):\n        self.dl = dl\n        self.device = device\n\n    def __iter__(self):\n        for batch in self.dl:\n            yield to_device(batch, self.device)\n  \n    def __len__(self):\n        return len(self.dl)","480a7ead":"train_loader = DeviceDataLoader(train_loader, device)\nval_loader = DeviceDataLoader(val_loader, device)","6ddca897":"def generate_l_ab(images): \n    lab = rgb2lab(images.permute(0, 2, 3, 1).cpu().numpy())\n    X = lab[:,:,:,0]\n    X = X.reshape(X.shape+(1,))\n    Y = lab[:,:,:,1:] \/ 128\n    return to_device(torch.tensor(X, dtype = torch.float).permute(0, 3, 1, 2), device),to_device(torch.tensor(Y, dtype = torch.float).permute(0, 3, 1, 2), device)","c40a3d56":"class BaseModel(nn.Module):\n    def training_batch(self, batch):\n        images, _ = batch\n        X, Y = generate_l_ab(images)\n        outputs = self.forward(X)\n        loss = F.mse_loss(outputs, Y)\n        return loss\n\n    def validation_batch(self, batch):\n        images, _ = batch\n        X, Y = generate_l_ab(images)\n        outputs = self.forward(X)\n        loss = F.mse_loss(outputs, Y)\n        return {'val_loss' : loss.item()}\n\n    def validation_end_epoch(self, outputs):\n        epoch_loss = sum([x['val_loss'] for x in outputs]) \/ len(outputs)\n        return {'epoch_loss' : epoch_loss}","15e6a3d9":"def get_padding(kernel_size: int, stride: int = 1, dilation: int = 1, **_) -> int:\n    padding = ((stride - 1) + dilation * (kernel_size - 1)) \/\/ 2\n    return padding","4be5b8b2":"class Encoder_Decoder(BaseModel):\n    def __init__(self):\n        super().__init__()\n        self.network = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size = 3, stride = 2, padding = get_padding(3, 2)),\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(64, 128, kernel_size = 3, padding = get_padding(3)),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n        \n            nn.Conv2d(128, 128, kernel_size = 3, stride = 2, padding = get_padding(3, 2)),\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.Conv2d(128, 256, kernel_size = 3, padding = get_padding(3)),\n            nn.ReLU(),\n            nn.BatchNorm2d(256),\n        \n            nn.Conv2d(256, 256, kernel_size = 3, stride = 2, padding = get_padding(3, 2)),\n            nn.ReLU(),\n            nn.BatchNorm2d(256),\n            nn.Conv2d(256, 512, kernel_size = 3, padding = get_padding(3)),\n            nn.ReLU(),\n            nn.BatchNorm2d(512),\n            \n            nn.Conv2d(512, 512, kernel_size = 3, padding = get_padding(3)),\n            nn.ReLU(),\n            nn.BatchNorm2d(512),\n            nn.Conv2d(512, 256, kernel_size = 3, padding = get_padding(3)),\n            nn.ReLU(),\n            nn.BatchNorm2d(256),\n        \n            nn.Conv2d(256, 128, kernel_size = 3, padding = get_padding(3)),\n            nn.Upsample(size = (64,64)),\n            nn.Conv2d(128, 64, kernel_size = 3, padding = get_padding(3)),\n            nn.Upsample(size = (128,128)),\n            nn.Conv2d(64, 32, kernel_size = 3, padding = get_padding(3)),\n            nn.Conv2d(32, 16, kernel_size = 3, padding = get_padding(3)),\n            nn.Conv2d(16, 2, kernel_size = 3, padding = get_padding(3)),\n            nn.Tanh(),\n            nn.Upsample(size = (256,256))\n    )\n\n    def forward(self, images):\n        return self.network(images)     \n    ","4500c1f0":"model = Encoder_Decoder()\nto_device(model, device)","2dd07448":"@torch.no_grad()\ndef validate(model, val_loader):\n    model.eval()\n    outputs = [model.validation_batch(batch) for batch in val_loader]\n    return model.validation_end_epoch(outputs)\n\ndef fit(model, epochs, learning_rate, train_loader, val_loader, optimization_func = torch.optim.SGD):\n    torch.cuda.empty_cache()\n    history = []\n    optimizer = optimization_func(model.parameters(), learning_rate)\n    for epoch in range(epochs):\n        train_losses = []\n        model.train()\n        for batch in tqdm(train_loader):\n            loss = model.training_batch(batch)\n            train_losses.append(loss)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n        result = validate(model, val_loader)\n        result['train_loss'] = torch.stack(train_losses).mean().item()\n        history.append(result)\n        print('Epoch: {}, Train loss: {:.4f}, Validation loss: {:.4f}'.format(epoch, result['train_loss'], result['epoch_loss']))\n    return history","75c0daa3":"history = [validate(model, val_loader)]\nhistory","094e899a":"history += fit(model, 5, 0.001, train_loader, val_loader, torch.optim.Adam)","4732ce82":"checkpoint = {'model': Encoder_Decoder(),\n              'state_dict': model.state_dict()}","237fab5a":"torch.save(checkpoint, 'test.pth')","5b108d7a":"def load_checkpoint(filepath):\n    checkpoint = torch.load(filepath)\n    model = checkpoint['model']\n    model.load_state_dict(checkpoint['state_dict'])\n    for parameter in model.parameters():\n        parameter.requires_grad = True\n    \n    model.eval()\n    \n    return model","6b88d3fb":"model = load_checkpoint('..\/input\/testing\/test21.pth')\nprint(model)\nto_device(model, device)","2f586e95":"def to_rgb(grayscale_input, ab_output, save_path=None, save_name=None):\n    color_image = torch.cat((grayscale_input, ab_output), 0).numpy() # combine channels\n    color_image = color_image.transpose((1, 2, 0))  # rescale for matplotlib\n    color_image[:, :, 0:1] = color_image[:, :, 0:1]\n    color_image[:, :, 1:3] = (color_image[:, :, 1:3]) * 128\n    color_image = lab2rgb(color_image.astype(np.float64))\n    grayscale_input = grayscale_input.squeeze().numpy()\n    return color_image","7c6a2fd2":"def prediction(img):\n    a = rgb2lab(img.permute(1, 2, 0))\n    a = torch.tensor(a[:,:,0]).type(torch.FloatTensor)\n    a = a.unsqueeze(0)\n    a = a.unsqueeze(0)\n    xb = to_device(a, device)\n    ab_img = model(xb)\n    xb = xb.squeeze(0)\n    ab_img = ab_img.squeeze(0)\n    return to_rgb(xb.detach().cpu(), ab_img.detach().cpu())","017da9fb":"import glob\nfrom PIL import Image\n\nimages = glob.glob(\"..\/input\/test-images\/*jpg\")","faa03a7b":"for img in range(len(images)):\n    image = Image.open(images[img])\n    trans1 = T.Resize((256, 256))\n    trans2 = T.ToTensor()\n    images[img] = trans2(trans1(image))","9ae19774":"img = images[1]\nf, arr = plt.subplots(1, 2, sharey=True)\narr[0].imshow(img.permute(1, 2, 0))\narr[1].imshow(prediction(img))","0450aca8":"Loading the pth file to continue training from the same parameters onwards.","a2e03ab2":"Since we are using a GPU, load the training and validation datasets into cuda(the selected device).","819f15bd":"Defining the directory for the dataset to be used. The dataset used here is a small 6 GB snippet of the ImageNet dataset. It has 2 classes: 'train' and 'val', containing 45000 and 5000 images respectively.","c25e5f82":"Testing the model on different black and white images.","2ac5f977":"The model is defined and loaded to cuda.","cb05ea31":"Initializing the loss.","4401c7e4":"The Base Model class is defined as containing three functions. \n1. The *training_batch* function takes the batch of 128 images as input, generates the values of L and ab channels using the *generate_l_ab* function, gets the predicted ab channels using the forward function of the model, and calculates the MSE Loss between the actual and predicted ab channels.\n2. The *validation_batch* functions performs the same task as the *training_batch* function, except for the images in the validation dataset, which is evident from the function names.\n3. The function *validation_end_epoch* returns the average loss on the validation dataset.","ecf0a351":"Loading the training and validation datasets into the CPU using DataLoader.","d6865331":"# Testing the Model\nCombining the L channel and the predicted ab channels and converting it to RGB to obtain the final colour image.","bf4c3aee":"Define a class to load data into a GPU, if it's available, otherwise into a CPU.","de2f4f4a":"Creating the training and validation sets. A seed 42 is set to have the same training and validation datasets each time the notebook is run, and the two datasets are split using the random_split function from the pytorch libraries.","1dc001bc":"# Defining the Model\nThe images are originally using the RGB colormodel. But for the purpose of image colourization, they have to be converted to Lab. The CIELAB is a colorspace which has 3 channels: L, a, and b. The function defined below converts images from RGB to CIELAB, and then splits the L and ab channels into the variables X and Y respectively. The range of ab channels is from -128 to 127, hence, the ab channels are normalized by dividing it by 128. The function returns finally returns X and Y loaded into cuda.","74fb2e3a":"Make cuda(GPU) the device if availability permits.","b271d5cc":"Define a function to load data into the device assigned above.","ae479ce2":"Saving the model to be able to continue training the model from the same point.","06290bdd":"The batch size is set to be 128.","cf7631a7":"Loading images contained in the data directory into a avriable called dataset using ImageFolder, and applying two transforms to all the images:\n1. Resizing the non-uniformly sized images into 256x256 images.\n2. Converting them into tensors.","d8ea5d88":"# Loading the Data\nImporting all the necessary libraries required to run the following code for image colourization.","591ab150":"A helper function is defined to get the appropriate padding in order to keep the size of the output image same as the input image during convolution.","f473ac77":"Training.","0011044f":"The *Encoder_Decoder* class is an extension of the Base Model and it contains the code for the architecture of the model.","41a36645":"# Training the Model\nThe *validate* and *fit* functions are defined to keep track of the loss and train the model.","5f38ee66":"As mentioned above, the dataset has a total 50000 images.","5eaa885c":"Given a black and white image with 3 channels(R==G==B), converting it to Lab and giving the L channel as input to predict the ab channels and finally, obtaining the predicted coloured version of the image."}}