{"cell_type":{"c977c9c4":"code","cd1f1c6b":"code","e830bff6":"code","8f98ec56":"code","cb9bb0e7":"code","182b9687":"code","33f0698e":"code","ac8842df":"code","3c8f0b25":"code","4bb7dbcf":"code","42beae2d":"code","4175e892":"code","7f512315":"code","382eb567":"code","521b47ab":"code","24b11058":"code","771360b7":"code","694e57fe":"code","b4f6a222":"code","aebf3733":"code","7926a5f7":"code","389073c4":"code","8d23fd68":"code","26b691b7":"code","81fac933":"code","93afc72e":"code","4f7454d9":"code","70fbeb3a":"code","fb23cee9":"code","9c3fbc9e":"code","aebe2235":"code","15b9fb40":"code","eb2f36cd":"code","430b0b57":"code","0709dfb6":"code","7f8590b7":"code","71808113":"code","270c308a":"code","c0f3d026":"code","d8329af0":"code","470a5e46":"code","6079042e":"code","a563aa1a":"code","095e1315":"code","70adc53f":"code","e8f2fbd7":"code","f72d5daf":"code","23f1f405":"code","08a59646":"code","a55b1395":"code","fcf7442a":"code","fed6c091":"code","5344e757":"code","ac461b71":"code","25b4d652":"code","4b6aae17":"code","8bf0e4ef":"code","acaf7edc":"code","84001726":"code","b435fbc1":"code","9a20f28c":"code","8b9bcc05":"code","6c89022a":"code","7fe78bb6":"code","c48323aa":"code","8f0ec822":"code","ccb7c445":"code","8b0010e4":"code","32006731":"code","8aa1c2a2":"code","e6c9645c":"code","5b26db80":"code","d6c2338a":"code","eea9d021":"code","e5200877":"code","429cb956":"code","5fb4c357":"code","6e218bf3":"markdown","a55c6c46":"markdown","8b571624":"markdown","50f43a7d":"markdown","3fe37b2e":"markdown","78e8f94d":"markdown","025078f5":"markdown","c7f13aea":"markdown","4897168e":"markdown","8d35919a":"markdown","7cc87502":"markdown","0f8180b7":"markdown","2ccab46d":"markdown","7311fd7e":"markdown","adf90bf0":"markdown","ef1d92dc":"markdown","146d6c03":"markdown","9fe2d6d2":"markdown","55189bc6":"markdown","e99db618":"markdown","ae51e4f3":"markdown","f634eb60":"markdown","4245f246":"markdown","a2944ad5":"markdown","a9f5fa5b":"markdown","0b8b2ec6":"markdown","37a7e4a5":"markdown","7223b18b":"markdown","57b69368":"markdown","7bfb2931":"markdown","bc48ceaf":"markdown","54ccef2a":"markdown","e80f68bc":"markdown","5965825a":"markdown","e498db83":"markdown","eb8a3ba7":"markdown","4c5fe430":"markdown","6deff6fc":"markdown","3e718da3":"markdown","3c6b657d":"markdown","769351cc":"markdown","617a6904":"markdown","892b7c5e":"markdown","ed06cd05":"markdown","c9984725":"markdown","efe140f8":"markdown","d6d0279f":"markdown","97593064":"markdown","77314e20":"markdown","bb6dfcbf":"markdown","8416b062":"markdown","5cb52b99":"markdown","f8f953be":"markdown","4cc168a6":"markdown","82e3565a":"markdown","1411d525":"markdown","0bbe2ece":"markdown","3016ea00":"markdown","560fb591":"markdown","cc2dc3cb":"markdown","28bbd76a":"markdown","d515cde5":"markdown","5c815180":"markdown","51522fbc":"markdown","c2ee7b03":"markdown","e7a0eb2c":"markdown","92a90e2d":"markdown","e365efe2":"markdown","42ce49ec":"markdown","00dfe17a":"markdown","0f402d83":"markdown","8b37b09d":"markdown","ae096602":"markdown","f2c4b857":"markdown","940f3506":"markdown","74ca615a":"markdown","dd312d6e":"markdown","63111d5d":"markdown"},"source":{"c977c9c4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n### For data analysis and wrangling\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random as rnd\n\n# For visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n#%matplotlib.inline\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC , LinearSVC\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cd1f1c6b":"# Load the train and Display the First 5 Rows of Titanic Dataset\ntrain = pd.read_csv(\"..\/input\/titanic-solution-for-beginners-guide\/train.csv\")\ntrain.head()","e830bff6":"# Load the test and Display the First 5 Rows of Titanic Dataset\ntest = pd.read_csv(\"..\/input\/titanic-solution-for-beginners-guide\/test.csv\")\ntest.head()","8f98ec56":"combine = [train,test]","cb9bb0e7":"print('Number of Rows and column in training dataset',train.shape)\nprint('Number of Rows and column in testing dataset',test.shape)","182b9687":"print('Training columns : ')\nprint(train.columns.values)\nprint('Testing columns : ')\nprint(test.columns.values)","33f0698e":"target_variable = list(set(train) - set(test))\nprint('Target Column is :',target_variable)","ac8842df":"train.info()","3c8f0b25":"# select_dtypes : Return a subset of the DataFrame\u2019s columns based on the column dtypes.\n\n#it will return column which contain discrete (integer) value  \ndiscrete_data = train.select_dtypes(include=['int64'])\n#it will return column which contain continous (float) value\ncontinous_data = train.select_dtypes(include=['float64'])\n#it will return column which contain categorical (object) value\ncategorical_data = train.select_dtypes(include=['object'])","4bb7dbcf":"print('Discrete Features : ',discrete_data.columns.values)\nprint('Continous Features : ',continous_data.columns.values)\nprint('Categorical Features : ',categorical_data.columns.values)","42beae2d":"categorical_data.head()","4175e892":"discrete_data.tail()","7f512315":"continous_data.head()","382eb567":"train.head()","521b47ab":"train.tail()","24b11058":"sns.heatmap(train.isnull(),yticklabels=False,cmap='bwr')","771360b7":"train.info()","694e57fe":"sns.heatmap(test.isnull(),yticklabels=False,cmap='bwr')","b4f6a222":"train.info()\nprint('--'*40)\ntest.info()","aebf3733":"train.describe()","7926a5f7":"categorical_data.describe()","389073c4":"discrete_data.describe()","8d23fd68":"continous_data.describe()","26b691b7":"from numpy import percentile\nfrom numpy.random import rand\n\nprint('Sibling Ratio : ',percentile(train['SibSp'],[10, 20, 30, 40, 50, 60,65,68, 70, 80, 90]))\nprint('Fare Charge : ',percentile(train['Fare'],[10, 20, 30, 40, 50, 60,70, 80, 90,95,99]))","81fac933":"train['Pclass'].value_counts()","93afc72e":"train[['Pclass','Survived']].groupby(['Pclass'],as_index=False).mean().sort_values(by='Survived',ascending=False)","4f7454d9":"train[['SibSp','Survived']].groupby(['SibSp'],as_index=False).mean().sort_values(by='Survived',ascending=False)","70fbeb3a":"train[['Parch','Survived']].groupby(['Parch'],as_index=False).mean().sort_values(by='Survived',ascending=False)","fb23cee9":"train[['Sex','Survived']].groupby('Sex',as_index=False).mean().sort_values(by='Survived',ascending=False)","9c3fbc9e":"grid1 =sns.FacetGrid(train,col='Survived')\ngrid1.map(plt.hist, 'Age', bins=20)","aebe2235":"grid2 = sns.FacetGrid(train,col='Survived',row='Pclass')\ngrid2.map(plt.hist,'Age',alpha=0.5,bins=20)\ngrid2.add_legend()","15b9fb40":"grid2 = sns.FacetGrid(train,row='Embarked' , size=5.2, aspect=1.6)\ngrid2.map(sns.pointplot,'Pclass','Survived','Sex', palette='deep')\ngrid2.add_legend()\n","eb2f36cd":"grid2 = sns.FacetGrid(train,row='Embarked',col='Survived',size=3.0,aspect=2.0)\ngrid2.map(sns.barplot,'Sex','Fare',alpha=0.5,ci=None)\ngrid2.add_legend()","430b0b57":"print('Before : ',train.shape,test.shape)\ncolumns = ['Cabin','Ticket']\nnew_train = train.drop(columns,axis=1)\nnew_test = test.drop(columns,axis=1) \ncombine = [new_train,new_test]\nprint('After : ',new_train.shape,new_test.shape)","0709dfb6":"new_train.head()","7f8590b7":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ndataset['Title'].head()","71808113":"pd.crosstab(new_train['Title'],new_train['Sex'])","270c308a":"new_train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","c0f3d026":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Capt','Col','Countess','Don','Dr','Jonkheer','Major','Rev','Sir','Dona','Lady'],'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\nnew_train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean().sort_values(by='Survived',ascending=False)","d8329af0":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\nnew_train.head()","470a5e46":"new_train = new_train.drop(['Name', 'PassengerId'], axis=1)\nnew_test = new_test.drop(['Name'], axis=1)\ncombine = [new_train, new_test]\nnew_train.shape, new_test.shape","6079042e":"for dataset in combine:\n    dataset['Sex'] = dataset['Sex'].map({\"male\" : 0 , \"female\" : 1}).astype(int)\n\nnew_train.head()","a563aa1a":"new_train.info()","095e1315":"grid = sns.FacetGrid(new_train, row='Pclass', col='Sex', size=2.2, aspect=1.6)\ngrid.map(plt.hist, 'Age', alpha=.5, bins=20)\ngrid.add_legend()","70adc53f":"guess_ages = np.zeros((2,3))\nguess_ages\n\nfor dataset in combine:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & (dataset['Pclass'] == j+1)]['Age'].dropna()\n            age_guess =  guess_df.median()\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),'Age'] = guess_ages[i,j]\n    dataset['Age'] = dataset['Age'].astype(int)\nnew_train.head()","e8f2fbd7":"new_train.info()","f72d5daf":"new_train['AgeBand'] = pd.cut(new_train['Age'], 5)\nnew_train[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","23f1f405":"new_train.head()","08a59646":"for dataset in combine:\n    dataset.loc[dataset['Age'] <= 16,'Age'] = 0\n    dataset.loc[(dataset['Age']>16) & (dataset['Age'] <=32),'Age'] = 1\n    dataset.loc[(dataset['Age']>32) & (dataset['Age'] <=48), 'Age'] = 2\n    dataset.loc[(dataset['Age']>38) & (dataset['Age'] <=64),'Age'] = 3\n    dataset.loc[(dataset['Age']>64), 'Age'] = 4","a55b1395":"new_train.head()","fcf7442a":"new_test.head()","fed6c091":"for dataset in combine:\n    dataset['Familysize'] = dataset['Parch'] + dataset['SibSp'] + 1\nnew_train[['Familysize', 'Survived']].groupby(['Familysize'], as_index=False).mean().sort_values(by='Survived', ascending=False)","5344e757":"for dataset in combine:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['Familysize'] == 1,'IsAlone'] = 1","ac461b71":"new_train[['IsAlone','Survived']].groupby(['IsAlone'],as_index=False).mean().sort_values(by='Survived',ascending=False)","25b4d652":"new_train = new_train.drop(['Parch', 'SibSp', 'Familysize'], axis=1)\nnew_test = new_test.drop(['Parch', 'SibSp', 'Familysize'], axis=1)\n","4b6aae17":"combine = [new_train, new_test]\n\nnew_train.head()","8bf0e4ef":"for dataset in combine:\n    dataset['Age*class'] = dataset.Age * dataset.Pclass\nnew_train.loc[:,['Age*class','Age','Pclass']].head()","acaf7edc":"freq_port = new_train.Embarked.dropna().mode()[0]\nfreq_port","84001726":"dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \nnew_train[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","b435fbc1":"for dataset in combine:\n    print(dataset.info())\n    print(dataset.dropna(how='any',inplace=True))\n    print(dataset.info())","9a20f28c":"from sklearn import preprocessing \n  \n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder()  ","8b9bcc05":"\nfor dataset in combine:\n    dataset['Embarked'] = label_encoder.fit_transform(dataset['Embarked'])","6c89022a":"dataset['Embarked'].unique() \n","7fe78bb6":"new_train.head()","c48323aa":"new_test['Fare'].fillna(new_test['Fare'].dropna().median(), inplace=True)\nnew_test.head()","8f0ec822":"new_train['FareBand'] = pd.qcut(new_train['Fare'], 4,duplicates='drop')\nnew_train[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)","ccb7c445":"for dataset in combine:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\nnew_train = new_train.drop(['FareBand','AgeBand'], axis=1)\ncombine = [new_train, new_test]\n    \nnew_train.head(10)","8b0010e4":"X_train = new_train.drop(\"Survived\", axis=1)\nY_train = new_train[\"Survived\"]\nX_test  = new_test.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, Y_train.shape, X_test.shape","32006731":"from sklearn.linear_model import LogisticRegression\nlogistic_model = LogisticRegression()\nlogistic_model.fit(X_train,Y_train)\n### Check the training accuracy\ntraining_accuracy = logistic_model.score(X_train,Y_train)\ny_pred = logistic_model.predict(X_test)\nacc_log = round((training_accuracy)*100,2)\nprint('Training Accuracy For the Logistic Regression Model is ',acc_log)","8aa1c2a2":"### Coeffecient of each feature\ncoeff_df = pd.DataFrame(new_train.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logistic_model.coef_[0])\ncoeff_df.sort_values(by='Correlation', ascending=False)","e6c9645c":"from sklearn.svm import SVC\nsvc_model = SVC()\nsvc_model.fit(X_train,Y_train)\nacc_svc = round(svc_model.score(X_train,Y_train) * 100 ,2)\nprint('Training Accuracy For the SVC Model ',acc_svc)\ny_pred = svc_model.predict(X_test)","5b26db80":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nY_pred = knn.predict(X_test)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nprint('Training Accuracy For the KNN Neighbour is ',acc_knn)","d6c2338a":"from sklearn.naive_bayes import GaussianNB\nnaive_bayes_model = GaussianNB()\nnaive_bayes_model.fit(X_train,Y_train)\nacc_gaussian = round(naive_bayes_model.score(X_train,Y_train)*100,2)\nprint('Training Accuracy For the Navie Bayes Model is : ',acc_gaussian)\ny_pred = naive_bayes_model.predict(X_test)","eea9d021":"from sklearn.tree import DecisionTreeClassifier\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nY_pred = decision_tree.predict(X_test)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nprint('Training Accuracy For the Decision Tree Classifier Model is :',acc_decision_tree)","e5200877":"from sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier()\nrandom_forest.fit(X_train, Y_train)\nY_pred = random_forest.predict(X_test)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nprint('Training Accuracy For the RandomForestClassifier Model is :',acc_random_forest)","429cb956":"import xgboost\n\nxgb = xgboost.XGBClassifier(colsample_bytree=0.8, subsample=0.5,\n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.8, n_estimators=2000,\n                             reg_alpha=0.1, reg_lambda=0.3, gamma=0.01, \n                             silent=1, random_state =7, nthread = -1)\n\nxgb.fit(X_train, Y_train)\nY_pred = xgb.predict(X_test)\nacc_xgb = round(xgb.score(X_train, Y_train) * 100, 2)\nprint('Training Accuracy For the XGBoost Model is :',acc_xgb)","5fb4c357":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n               'Naive Bayes',  \n              'Decision Tree','Random Forest','XGBoost'],\n    'Score': [acc_svc, acc_knn, acc_log, \n              acc_gaussian, acc_decision_tree,acc_random_forest,acc_xgb]})\nmodels.sort_values(by='Score', ascending=False)","6e218bf3":"##### Analyze by describing data\nPandas also helps describe the datasets answering following questions early in our project.\n\n#### Which features are available in the dataset?","a55c6c46":"A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner call recursive partitioning. This flowchart-like structure helps you in decision making. It's visualization like a flowchart diagram which easily mimics the human level thinking. That is why decision trees are easy to understand and interpret.\n![image.png](attachment:image.png)\nDecision Tree is a white box type of ML algorithm. It shares internal decision-making logic, which is not available in the black box type of algorithms such as Neural Network. Its training time is faster compared to the neural network algorithm. The time complexity of decision trees is a function of the number of records and number of attributes in the given data. The decision tree is a distribution-free or non-parametric method, which does not depend upon probability distribution assumptions. Decision trees can handle high dimensional data with good accuracy.\n","8b571624":"#### Categorical Description","50f43a7d":"#### Completing a categorical feature\nEmbarked feature takes S, Q, C values based on port of embarkation. Our training dataset has two missing values. We simply fill these with the most common occurance.","3fe37b2e":"![image.png](attachment:image.png)\n\nWhere,\n\n* |Dj|\/|D| acts as the weight of the jth partition.\n* v is the number of discrete values in attribute A.\n\nThe gain ratio can be defined as","78e8f94d":"## Model, predict and solve\nNow we are ready to train a model and predict the required solution. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n\n1. Logistic Regression\n2. Decision Tree Classfier\n3. Random Forest Classifer\n4. Navie Bayes\n5. KNN (K Nearest Neighbour)\n6. SVM (Support Vector Machine)\n7. Artifical Neural Network\n8. Perceptron\n9. Relevance Vector Machine (RVM)\n10. Perceptron","025078f5":"### Tuning Hyperparameters\n\n**1.Kernel**: The main function of the kernel is to transform the given dataset input data into the required form. There are various types of functions such as linear, polynomial, and radial basis function (RBF). Polynomial and RBF are useful for non-linear hyperplane. Polynomial and RBF kernels compute the separation line in the higher dimension. In some of the applications, it is suggested to use a more complex kernel to separate the classes that are curved or nonlinear. This transformation can lead to more accurate classifiers.\n\n**2.Regularization**: Regularization parameter in python's Scikit-learn C parameter used to maintain regularization. Here C is the penalty parameter, which represents misclassification or error term. The misclassification or error term tells the SVM optimization how much error is bearable. This is how you can control the trade-off between decision boundary and misclassification term. A smaller value of C creates a small-margin hyperplane and a larger value of C creates a larger-margin hyperplane.\n\n**3.Gamma**: A lower value of Gamma will loosely fit the training dataset, whereas a higher value of gamma will exactly fit the training dataset, which causes over-fitting. In other words, you can say a low value of gamma considers only nearby points in calculating the separation line, while the a value of gamma considers all the data points in the calculation of the separation line.\n\n### Advantages\nSVM Classifiers offer good accuracy and perform faster prediction compared to Na\u00efve Bayes algorithm. They also use less memory because they use a subset of training points in the decision phase. SVM works well with a clear margin of separation and with high dimensional space.\n\n### Disadvantages\nSVM is not suitable for large datasets because of its high training time and it also takes more time in training compared to Na\u00efve Bayes. It works poorly with overlapping classes and is also sensitive to the type of kernel used.","c7f13aea":"### Describing the Data","4897168e":"#### Quick completing and converting a numeric feature\u00b6\nWe can now complete the Fare feature for single missing value in test dataset using mode to get the value that occurs most frequently for this feature. We do this in a single line of code.\n\nNote that we are not creating an intermediate new feature or doing any further analysis for correlation to guess missing feature as we are replacing only a single value. The completion goal achieves desired requirement for model algorithm to operate on non-null values.\n\nWe may also want round off the fare to two decimals as it represents currency.","8d35919a":"#### Training dataset","7cc87502":"The attribute with minimum Gini index is chosen as the splitting attribute.\n\n","0f8180b7":"![image.png](attachment:image.png)\n","2ccab46d":"## 3.K-Nearest Neighbor(KNN) Classification ","7311fd7e":"#### Converting Categorical Feature to numerical\nWe can now convert the EmbarkedFill feature by creating a new numeric Port feature.\n","adf90bf0":"#### Observations.\n* Female passengers had much better survival rate than males. Confirms classifying (#1).\n* Exception in Embarked=C where males had higher survival rate. This could be a correlation between Pclass and Embarked and in turn Pclass and Survived, not necessarily direct correlation between Embarked and Survived.\n* Males had better survival rate in Pclass=3 when compared with Pclass=2 for C and Q ports. Completing (#2).\n* Ports of embarkation have varying survival rates for Pclass=3 and among male passengers. Correlating (#1).\n\n#### Decisions.\n\nAdd Sex feature to model training.\nComplete and add Embarked feature to model training.","ef1d92dc":"KNN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution. In other words, the model structure determined from the dataset. This will be very helpful in practice where most of the real world datasets do not follow mathematical theoretical assumptions. Lazy algorithm means it does not need any training data points for model generation. All training data used in the testing phase. This makes training faster and testing phase slower and costlier. Costly testing phase means time and memory. In the worst case, KNN needs more time to scan all data points and scanning all data points will require more memory for storing training data.\n\n","146d6c03":"## Feature Data Type:\n    1. Numerical:\n        a. Continuos\n        b. Discrete\n    2. Categorical:\n        a. Nominal\n        b. Ordinal","9fe2d6d2":"#### Testing dataset","55189bc6":"Whenever you perform classification, the first step is to understand the problem and identify potential features and label. Features are those characteristics or attributes which affect the results of the label. For example, in the case of a loan distribution, bank manager's identify customer\u2019s occupation, income, age, location, previous loan history, transaction history, and credit score. These characteristics are known as features which help the model classify customers.\n\nThe classification has two phases, a learning phase, and the evaluation phase. In the learning phase, classifier trains its model on a given dataset and in the evaluation phase, it tests the classifier performance. Performance is evaluated on the basis of various parameters such as accuracy, error, precision, and recall.\n\n\n#### What is Naive Bayes Classifier?\nNaive Bayes is a statistical classification technique based on Bayes Theorem. It is one of the simplest supervised learning algorithms. Naive Bayes classifier is the fast, accurate and reliable algorithm. Naive Bayes classifiers have high accuracy and speed on large datasets.\n\nNaive Bayes classifier assumes that the effect of a particular feature in a class is independent of other features. For example, a loan applicant is desirable or not depending on his\/her income, previous loan and transaction history, age, and location. Even if these features are interdependent, these features are still considered independently. This assumption simplifies computation, and that's why it is considered as naive. This assumption is called class conditional independence.\n\n\n* P(h): the probability of hypothesis h being true (regardless of the data). This is known as the prior probability of h.\n* P(D): the probability of the data (regardless of the hypothesis). This is known as the prior probability.\n* P(h|D): the probability of hypothesis h given the data D. This is known as posterior probability.\n* P(D|h): the probability of data d given that the hypothesis h was true. This is known as posterior probability.\n\n**How Naive Bayes classifier works?**\nLet\u2019s understand the working of Naive Bayes through an example. Given an example of weather conditions and playing sports. You need to calculate the probability of playing sports. Now, you need to classify whether players will play or not, based on the weather condition.\n\nFirst Approach (In case of a single feature)\nNaive Bayes classifier calculates the probability of an event in the following steps:\n\n* Step 1: Calculate the prior probability for given class labels\n* Step 2: Find Likelihood probability with each attribute for each class\n* Step 3: Put these value in Bayes Formula and calculate posterior probability.\n* Step 4: See which class has a higher probability, given the input belongs to the higher probability class.\n\nFor simplifying prior and posterior probability calculation you can use the two tables frequency and likelihood tables. Both of these tables will help you to calculate the prior and posterior probability. The Frequency table contains the occurrence of labels for all features. There are two likelihood tables. Likelihood Table 1 is showing prior probabilities of labels and Likelihood Table 2 is showing the posterior probability.","e99db618":"## 5.Decision Tree Classifier","ae51e4f3":"#### Discreate description","f634eb60":"How does the Decision Tree algorithm work?\nThe basic idea behind any decision tree algorithm is as follows:\n\n1. Select the best attribute using Attribute Selection Measures(ASM) to split the records.\n2. Make that attribute a decision node and breaks the dataset into smaller subsets.\n3. Starts tree building by repeating this process recursively for each child until one of the condition will match:\n    * All the tuples belong to the same attribute value.\n    * There are no more remaining attributes.\n    * There are no more instances.\n\n![image.png](attachment:image.png)","4245f246":"![image.png](attachment:image.png)","a2944ad5":"What is the distribution of categorical features?\n\n* Names are unique across the dataset\n* Sex variable has two possible values with 65% male\n* Embarked takes three possible values. S port used by most passengers (top=S) i.e. Most Passenger has started their jouney from SouthHampton\n* Cabin contain several unique values also it several passenger has shared the cabin\n* Ticket has high ratio of duplicate values","a9f5fa5b":"1. Cabin and Ticket Contain Mixed datatype (numerical + textual) information","0b8b2ec6":"> **Acquire Data**","37a7e4a5":"### Analyze by visualizing the data\nNow we can continue confirming some assumption using visualization for analyzing the data\n\n#### Correlating Numerical Feature\n\nLet us start by understanding correlation between numerical feature and our soultion goal Survived (Categorical variable)\n\nA histrogram chart is useful for analyzing the continous numerical feature like **Age : where banding or ranges will be useful to identify the patterns**  The histogram can indicate distribution of samples using automatically defined bins or equally ranged bands. This helps us answer questions relating to specific bands (Did infants have better survival rate?)","7223b18b":"Let us drop Parch, SibSp, and FamilySize features in favor of IsAlone.\n\n","57b69368":"This helps us determine, among other early insights, how representative is the training dataset of the actual problem domain.\n* Most Passenger (>75%) did not travel with parents or children\n* Survived is a categorical feature with 0 or 1 values.\n* Around 38% samples survived representative of the actual survival rate at 32%.\n* Fare vary significatly for few passengers paying as high as $512","7bfb2931":"## 1.Logistic Regression","bc48ceaf":"#### Combine training and testing data****","54ccef2a":"#### Correlating numerical and ordinal features\n","e80f68bc":"#### Complete training data","5965825a":"### We can convert the categorical titles to ordinal.","e498db83":"Name feature may contain errors or typos as there are several ways used to describe a name including titles, round brackets, and quotes used for alternative or short names.","eb8a3ba7":"We can replace many titles with a more common name or classify them as Rare.\n","4c5fe430":"We can not create FareBand.\n\n","6deff6fc":"Correcting by dropping features\nThis is a good starting goal to execute. By dropping features we are dealing with fewer data points. Speeds up our notebook and eases the analysis.\nBased on our assumptions and decisions we want to drop the Cabin (correcting #2) and Ticket (correcting #1) features.\n\nNote that where applicable we perform operations on both training and testing datasets together to stay consistent.Based on our assumptions and decisions we want to drop the Cabin (correcting #2) and Ticket (correcting #1) features.\n\nNote that where applicable we perform operations on both training and testing datasets together to stay consistent.","3e718da3":"#### Correlating categorical features\nNow we can correlate categorical features with our solution goal.\n\n","3c6b657d":"#### Reading Training dataset","769351cc":"The attribute with the highest gain ratio is chosen as the splitting attribute.\n\n**Gini index**\nAnother decision tree algorithm CART (Classification and Regression Tree) uses the Gini method to create split points.\n\n![image.png](attachment:image.png)\n\nWhere, pi is the probability that a tuple in D belongs to class Ci.\n\nThe Gini Index considers a binary split for each attribute. You can compute a weighted sum of the impurity of each partition. If a binary split on attribute A partitions data D into D1 and D2, the Gini index of D is:","617a6904":"## 2.Support Vector Machine \n\nGenerally, Support Vector Machines is considered to be a classification approach, it but can be employed in both types of classification and regression problems. It can easily handle multiple continuous and categorical variables. SVM constructs a hyperplane in multidimensional space to separate different classes. SVM generates optimal hyperplane in an iterative manner, which is used to minimize an error. The core idea of SVM is to find a maximum marginal hyperplane(MMH) that best divides the dataset into classes.\n\n![image.png](attachment:image.png)\n\n**Support Vectors**\nSupport vectors are the data points, which are closest to the hyperplane. These points will define the separating line better by calculating margins. These points are more relevant to the construction of the classifier.\n\n**Hyperplane**\nA hyperplane is a decision plane which separates between a set of objects having different class memberships.\n\n**Margin**\nA margin is a gap between the two lines on the closest class points. This is calculated as the perpendicular distance from the line to support vectors or closest points. If the margin is larger in between the classes, then it is considered a good margin, a smaller margin is a bad margin.\n\n\n**How does SVM work?**\nThe main objective is to segregate the given dataset in the best possible way. The distance between the either nearest points is known as the margin. The objective is to select a hyperplane with the maximum possible margin between support vectors in the given dataset. SVM searches for the maximum marginal hyperplane in the following steps:\n\nGenerate hyperplanes which segregates the classes in the best way. Left-hand side figure showing three hyperplanes black, blue and orange. Here, the blue and orange have higher classification error, but the black is separating the two classes correctly.\n\nSelect the right hyperplane with the maximum segregation from the either nearest data points as shown in the right-hand side figure.\n1. enerate hyperplanes which segregates the classes in the best way. Left-hand side figure showing three hyperplanes black, blue and orange. Here, the blue and orange have higher classification error, but the black is separating the two classes correctly.\n2. Select the right hyperplane with the maximum segregation from the either nearest data points as shown in the right-hand side figure.\n\n![image.png](attachment:image.png)\n\n\n\n","892b7c5e":"## Logistic Regression:\n**Advantages** :\nBecause of its efficient and straightforward nature, doesn't require high computation power, easy to implement, easily interpretable, used widely by data analyst and scientist. Also, it doesn't require scaling of features. Logistic regression provides a probability score for observations.\n\n**Disadvantages** : \nLogistic regression is not able to handle a large number of categorical features\/variables. It is vulnerable to overfitting. Also, can't solve the non-linear problem with the logistic regression that is why it requires a transformation of non-linear features. Logistic regression will not perform well with independent variables that are not correlated to the target variable and are very similar or correlated to each other.","ed06cd05":"We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. This can be done by calculating the coefficient of the features in the decision function.\n\nPositive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).\n\n","c9984725":"> Pclass We observe significant correlation (>0.5) among Pclass=1 and Survived (classifying #3). We decide to include this feature in our model.","efe140f8":"#### Reading Training Dataset","d6d0279f":"* Sex is highest positivie coefficient, implying as the Sex value increases (male: 0 to female: 1), the probability of Survived=1 increases the most.\n* Inversely as Pclass increases, probability of Survived=1 decreases the most.\n* This way Age*Class is a good artificial feature to model as it has second highest negative correlation with Survived.\n* So is Title as second highest positive correlation.","97593064":"Well, you got a classification rate of 84.06%, considered as good accuracy.\n\nHere, you have increased the number of neighbors in the model and accuracy got increased. But, this is not necessary for each case that an increase in many neighbors increases the accuracy. For a more detailed understanding of it, you can refer section \"How to decide the number of neighbors?\" of this tutorial.\n\n**Pros**\nThe training phase of K-nearest neighbor classification is much faster compared to other classification algorithms. There is no need to train a model for generalization, That is why KNN is known as the simple and instance-based learning algorithm. KNN can be useful in case of nonlinear data. It can be used with the regression problem. Output value for the object is computed by the average of k closest neighbors value.\n\n**Cons**\nThe testing phase of K-nearest neighbor classification is slower and costlier in terms of time and memory. It requires large memory for storing the entire training dataset for prediction. KNN requires scaling of data because KNN uses the Euclidean distance between two data points to find nearest neighbors. Euclidean distance is sensitive to magnitudes. The features with high magnitudes will weight more than features with low magnitudes. KNN also not suitable for large dimensional data.\n\nHow to improve KNN?\nFor better results, normalizing data on the same scale is highly recommended. Generally, the normalization range considered between 0 and 1. KNN is not suitable for the large dimensional data. In such cases, dimension needs to reduce to improve the performance. Also, handling missing values will help us in improving results.","77314e20":"### Analyze by pivoting features\n\nTo confirm some of our observations and assumptions, we can quickly analyze our feature correlations by pivoting features against each other. We can only do so at this stage for features which do not have any empty values. It also makes sense doing so only for features which are categorical (Sex), ordinal (Pclass) or discrete (SibSp, Parch) type.","bb6dfcbf":"#### Which is the target Column ?[](http:\/\/)","8416b062":"![image.png](attachment:image.png)\nWhere,\n\n* Info(D) is the average amount of information needed to identify the class label of a tuple in D.\n* |Dj|\/|D| acts as the weight of the jth partition.\n* InfoA(D) is the expected informa-tion required to classify a tuple from D based on the partitioning by A.\n* The attribute A with the highest information gain, Gain(A), is chosen as the splitting attribute at node N().\n\n**Gain Ratio**\nInformation gain is biased for the attribute with many outcomes. It means it prefers the attribute with a large number of distinct values. For instance, consider an attribute with a unique identifier such as customer_ID has zero info(D) because of pure partition. This maximizes the information gain and creates useless partitioning.\n\nC4.5, an improvement of ID3, uses an extension to information gain known as the gain ratio. Gain ratio handles the issue of bias by normalizing the information gain using Split Info. Java implementation of the C4.5 algorithm is known as J48","5cb52b99":"We can combine multiple features for identifying correlations using a single plot. This can be done with numerical and categorical features which have numeric values.","f8f953be":"Converting a categorical feature\nNow we can convert features which contain strings to numerical values. This is required by most model algorithms. Doing so will also help us in achieving the feature completing goal.\n\nLet us start by converting Sex feature to a new feature called Gender where female=1 and male=0.","4cc168a6":"![image.png](attachment:image.png)\n\nIn case of a discrete-valued attribute, the subset that gives the minimum gini index for that chosen is selected as a splitting attribute. In the case of continuous-valued attributes, the strategy is to select each pair of adjacent values as a possible split-point and point with smaller gini index chosen as the splitting point.","82e3565a":"#### Inference : By seeing the above feature information we can infer we have 2 float Features , Integer Feature 5 and Categorical feature 5.\n\nMissing Values: We can also see the count of Age(714) , Cabin(204) and Embarked(889) column is less as compared to other columns. that means it contain some missing value","1411d525":"K Nearest Neighbor(KNN) is a very simple, easy to understand, versatile and one of the topmost machine learning algorithms. KNN used in the variety of applications such as finance, healthcare, political science, handwriting detection, image recognition and video recognition. In Credit ratings, financial institutes will predict the credit rating of customers. In loan disbursement, banking institutes will predict whether the loan is safe or risky. In political science, classifying potential voters in two classes will vote or won\u2019t vote. KNN algorithm used for both classification and regression problems. KNN algorithm based on feature similarity approach.\n\n","0bbe2ece":"> Sex We confirm the observation during problem definition that Sex=female had very high survival rate at 74% (classifying #1).","3016ea00":"Now we can safely drop the Name feature from training and testing datasets. We also do not need the PassengerId feature in the training dataset.","560fb591":"> Cabin > Age > Embarked features contain a number of null values in that order for the training dataset.","cc2dc3cb":"## **How does the KNN algorithm work?**\nIn KNN, K is the number of nearest neighbors. The number of neighbors is the core deciding factor. K is generally an odd number if the number of classes is 2. When K=1, then the algorithm is known as the nearest neighbor algorithm. This is the simplest case. Suppose P1 is the point, for which label needs to predict. First, you find the one closest point to P1 and then the label of the nearest point assigned to P1.\n\n![image.png](attachment:image.png)\n\nSuppose P1 is the point, for which label needs to predict. First, you find the k closest point to P1 and then classify points by majority vote of its k neighbors. Each object votes for their class and the class with the most votes is taken as the prediction. For finding closest similar points, you find the distance between points using distance measures such as Euclidean distance, Hamming distance, Manhattan distance and Minkowski distance. KNN has the following basic steps:\n\n1. Calculate distance\n2. Find closest neighbors\n3. Vote for labels\n\n![image.png](attachment:image.png)\n\n#### Eager Vs. Lazy Learners\nEager learners mean when given training points will construct a generalized model before performing prediction on given new points to classify. You can think of such learners as being ready, active and eager to classify unobserved data points.\n\nLazy Learning means there is no need for learning or training of the model and all of the data points used at the time of prediction. Lazy learners wait until the last minute before classifying any data point. Lazy learner stores merely the training dataset and waits until classification needs to perform. Only when it sees the test tuple does it perform generalization to classify the tuple based on its similarity to the stored training tuples. Unlike eager learning methods, lazy learners do less work in the training phase and more work in the testing phase to make a classification. Lazy learners are also known as instance-based learners because lazy learners store the training points or instances, and all learning is based on instances.\n\n#### Curse of Dimensionality\nKNN performs better with a lower number of features than a large number of features. You can say that when the number of features increases than it requires more data. Increase in dimension also leads to the problem of overfitting. To avoid overfitting, the needed data will need to grow exponentially as you increase the number of dimensions. This problem of higher dimension is known as the Curse of Dimensionality.\n\nTo deal with the problem of the curse of dimensionality, you need to perform principal component analysis before applying any machine learning algorithm, or you can also use feature selection approach. Research has shown that in large dimension Euclidean distance is not useful anymore. Therefore, you can prefer other measures such as cosine similarity, which get decidedly less affected by high dimension.\n\n#### How do you decide the number of neighbors in KNN?\nNow, you understand the KNN algorithm working mechanism. At this point, the question arises that How to choose the optimal number of neighbors? And what are its effects on the classifier? The number of neighbors(K) in KNN is a hyperparameter that you need choose at the time of model building. You can think of K as a controlling variable for the prediction model.\n\nResearch has shown that no optimal number of neighbors suits all kind of data sets. Each dataset has it's own requirements. In the case of a small number of neighbors, the noise will have a higher influence on the result, and a large number of neighbors make it computationally expensive. Research has also shown that a small amount of neighbors are most flexible fit which will have low bias but high variance and a large number of neighbors will have a smoother decision boundary which means lower variance but higher bias.\n\nGenerally, Data scientists choose as an odd number if the number of classes is even. You can also check by generating the model on different values of k and check their performance. You can also try Elbow method here.\n\n![image.png](attachment:image.png)","28bbd76a":"#### Number of Rows and column in training dataset","d515cde5":"#### Observations:\n* Pclass=3 Had most passenger,however they didn't survived.Confirm Classifying Assumption\n* Infrant Passenger in Pclass=2 and Pclass=3. mostly survived.Further qualifies for classification.\n* Most Passenger in Pclass=1 has Survived. Confirm Classifying assumption 3\n* Pclass varies in terms of Age Distribution\n\n#### Decision:\n* Consider Pclass for Modelling","5c815180":"**Attribute Selection Measures**\nAttribute selection measure is a heuristic for selecting the splitting criterion that partition data into the best possible manner. It is also known as splitting rules because it helps us to determine breakpoints for tuples on a given node. ASM provides a rank to each feature(or attribute) by explaining the given dataset. Best score attribute will be selected as a splitting attribute . In the case of a continuous-valued attribute, split points for branches also need to define. Most popular selection measures are Information Gain, Gain Ratio, and Gini Index.\n\n**Information Gain**\nShannon invented the concept of entropy, which measures the impurity of the input set. In physics and mathematics, entropy referred as the randomness or the impurity in the system. In information theory, it refers to the impurity in a group of examples. Information gain is the decrease in entropy. Information gain computes the difference between entropy before split and average entropy after split of the dataset based on given attribute values. ID3 (Iterative Dichotomiser) decision tree algorithm uses information gain.\n\n![image.png](attachment:image.png)\n\nWhere, Pi is the probability that an arbitrary tuple in D belongs to class Ci.\n","51522fbc":"Cabin > Age >Fare Contain number of null values in that order for testing dataset","c2ee7b03":"#### Which Feature may contain errors","e7a0eb2c":"#### Observation:\n* Infrant(Age<=4) had high survival rate\n* Oldest Passenger 80 is Survived.\n* Large Number of of people between 16-28 did not Survived\n* Most Passenger are in th range of 15-3%5\n\n#### Decision:\nThis simple analysis confirms our assumptions as decisions for subsequent workflow stages.\n\n* We should consider the Age Feature in our model training \n* Complete the Age Feature for null values. (Completion 1)\n* we should band age groups (Creation 3)\n","92a90e2d":"Creating new feature extracting from existing We want to analyze if Name feature can be engineered to extract titles and test correlation between titles and survival, before dropping Name and PassengerId features. In the following code we extract Title feature using regular expressions. The RegEx pattern (\\w+\\.) matches the first word which ends with a dot character within Name feature. The expand=False flag returns a DataFrame.","e365efe2":"**Zero Probability Problem:**\n\nSuppose there is no tuple for a risky loan in the dataset, in this scenario, the posterior probability will be zero, and the model is unable to make a prediction. This problem is known as Zero Probability because the occurrence of the particular class is zero.\n\nThe solution for such an issue is the Laplacian correction or Laplace Transformation. Laplacian correction is one of the smoothing techniques. Here, you can assume that the dataset is large enough that adding one row of each class will not make a difference in the estimated probability. This will overcome the issue of probability values to zero.\n\nFor Example: Suppose that for the class loan risky, there are 1000 training tuples in the database. In this database, income column has 0 tuples for low income, 990 tuples for medium income, and 10 tuples for high income. The probabilities of these events, without the Laplacian correction, are 0, 0.990 (from 990\/1000), and 0.010 (from 10\/1000)\n\nNow, apply Laplacian correction on the given dataset. Let's add 1 more tuple for each income-value pair. The probabilities of these events:\n\n\n**Advantages**\n1. It is not only a simple approach but also a fast and accurate method for prediction.\n2. Naive Bayes has very low computation cost.\n3. It can efficiently work on a large dataset.\n4. It performs well in case of discrete response variable compared to the continuous variable.\n5. It can be used with multiple class prediction problems.\n6. It also performs well in the case of text analytics problems.\n7. When the assumption of independence holds, a Naive Bayes classifier performs better compared to other models like logistic regression.\n\n**Disadvantages**\n1. The assumption of independent features. In practice, it is almost impossible that model will get a set of predictors which are entirely independent.\n2. If there is no training tuple of a particular class, this causes zero posterior probability. In this case, the model is unable to make predictions. This problem is known as Zero Probability\/Frequency Problem.\n","42ce49ec":"![image.png](attachment:image.png)","00dfe17a":"* Nearly 32% of the passenger had the spouse and siblings\n* Less than 2% of the passenger had paid more fares****","0f402d83":"#### This Features (Parch and SibSp) have zero correlation for certain values. it may be best to derive a new feature from a feature or set of feature from these individual features","8b37b09d":"## Data Wrangling\n\n#### Data wrangling, sometimes referred to as data munging, is the process of transforming and mapping data from one \"raw\" data form into another format with the intent of making it more appropriate and valuable for a variety of downstream purposes such as analytics.","ae096602":"#### Which Feature Contain Blank values ?\nThese will require correcting.\n","f2c4b857":"We have collected several assumptions and decisions regarding our datasets and solution requirements. So far we did not have to change a single feature or value to arrive at these. Let us now execute our decisions and assumptions for correcting, creating, and completing goals.","940f3506":"Completing a numerical continuous feature\nNow we should start estimating and completing features with missing or null values. We will first do this for the Age feature.\n\nWe can consider three methods to complete a numerical continuous feature.\n\nWe can consider three methods to complete a numerical continuous feature.\n\n1. A simple way is to generate random numbers between mean and standard deviation.\n\n2. More accurate way of guessing missing values is to use other correlated features. In our case we note correlation among Age, Gender, and Pclass. Guess Age values using median values for Age across sets of Pclass and Gender feature combinations. So, median Age for Pclass=1 and Gender=0, Pclass=1 and Gender=1, and so on...\n\n3. Combine methods 1 and 2. So instead of guessing age values based on median, use random numbers between mean and standard deviation, based on sets of Pclass and Gender combinations.\n\nMethod 1 and 3 will introduce random noise into our models. The results from multiple executions might vary. We will prefer method 2.","74ca615a":"### Assumption based on the data analysis:\nWe arrive at following assumptions based on data analysis done so far. We may validate these assumptions \nfurther before taking appropriate actions.\n#### 1.Correlation :\n* We want to know how well does each feature correlate with Survival. We want to do this early in our project and match these quick correlations with modelled correlations later in the project.\n\n#### 2.Correcting : \n* Ticket feature may be dropped from our analysis as it contains high ratio of duplicates (22%) and there may not be a correlation between Ticket and survival.\n* PassengerId may be dropped from training dataset as it does not contribute to survival.\n* Name feature is relatively non-standard, may not contribute directly to survival, so maybe dropped.\n* Cabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset.\n\n#### 3. Creation :\n* We may want to create a new feature called Family based on Parch and SibSp to get total count of family members on board.\n* We may want to engineer the Name feature to extract Title as a new feature.\n* We may want to create new feature for Age bands. This turns a continous numerical feature into an ordinal categorical feature.\n* We may also want to create a Fare range feature if it helps our analysis\n\n#### 4. Classification : \nWe may also add to our assumptions based on the problem description noted earlier.\n\n* Women \/ Men were more likely to have survived.\n* Children (Age<?) were more likely to have survived.\n* Which  passengers class (Pclass=1 , 2, 3 ) were more likely to have survived.","dd312d6e":"#### Create new feature combining existing features\n\nWe can create a new feature for FamilySize which combines Parch and SibSp. This will enable us to drop Parch and SibSp from our datasets.\n\n","63111d5d":"## 4.Naive Bayes\n\nSuppose you are a product manager, you want to classify customer reviews in positive and negative classes. Or As a loan manager, you want to identify which loan applicants are safe or risky? As a healthcare analyst, you want to predict which patients can suffer from diabetes disease. All the examples have the same kind of problem to classify reviews, loan applicants, and patients.\n\nNaive Bayes is the most straightforward and fast classification algorithm, which is suitable for a large chunk of data. Naive Bayes classifier is successfully used in various applications such as spam filtering, text classification, sentiment analysis, and recommender systems. It uses Bayes theorem of probability for prediction of unknown class.\n\n"}}