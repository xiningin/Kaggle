{"cell_type":{"a022b254":"code","c7201da0":"code","5c50bf90":"code","2f036846":"code","20ab026a":"code","d9f72a51":"code","b3a6c04c":"code","8cd788db":"code","eb5b6c5d":"code","2c615f86":"code","b91388bf":"code","189e1590":"code","6dd1d6b8":"code","61e5de56":"code","8ff51661":"code","8fe143f1":"code","9ad4df0f":"code","24153c15":"code","4bd453a7":"code","21b6cef3":"code","5325cd1f":"code","a3597c0a":"code","ba37084d":"code","8bcab8bb":"code","cebe1a7c":"code","ec7300d4":"code","461297f3":"code","504e445f":"code","f5d1325e":"code","a78c94e4":"code","20b44652":"code","e35d8c19":"code","6962ff8f":"code","ff46e46f":"code","6229f9a9":"code","08bb6f17":"code","ca69d119":"code","7ee00328":"code","7072e989":"code","bae04d3d":"code","fd09f2ae":"code","5b4b3a4c":"code","40d44739":"code","5f1da366":"code","44bca868":"code","fabc42af":"code","bda93bcb":"code","6d581bad":"code","e45507e1":"code","7c4ec3e3":"code","0c915d6b":"code","4811c7c2":"code","0d019ccb":"code","5b486a03":"code","81367c63":"code","611a2970":"code","77f6a80e":"code","eb2d966d":"code","36b5bf9c":"code","b009acb5":"code","cce8103b":"code","00d860df":"code","c398b9e9":"code","e508cd66":"code","03d167c7":"code","680bf0ef":"code","01bd4f28":"markdown","8e6a8b90":"markdown","96dac33f":"markdown","f0cebe96":"markdown","ca14e397":"markdown","adee18cc":"markdown","d57dd9d7":"markdown","96f648f5":"markdown","ebe03f4a":"markdown","306d0b98":"markdown","a821117c":"markdown","623faabd":"markdown","ade13320":"markdown","858e928f":"markdown","a56a06e1":"markdown","30e1d5b0":"markdown","a096d433":"markdown","7664da49":"markdown","fb0406d8":"markdown","58bf6181":"markdown"},"source":{"a022b254":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sbn\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.metrics import mean_squared_error,r2_score, confusion_matrix, plot_confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import  RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","c7201da0":"training = pd.read_csv('..\/input\/my-dataset\/credit_train.csv')\n","5c50bf90":"train_columns = training.columns\nfor columns in enumerate(train_columns):\n    print(columns)","2f036846":"training.head()","20ab026a":"training.tail(10)","d9f72a51":"print('The shape of the dataset is {} and the size is {}'.format(training.shape, training.size) )","b3a6c04c":"vb = training.isna().sum()\nvb","8cd788db":"training.info()","eb5b6c5d":"training.describe()","2c615f86":"training.dtypes","b91388bf":"def calculate_null_values(dataframe):\n    d_frame = dataframe\n    # get the sum of the null value of  each column \n    d_frame_null_values = pd.DataFrame(dataframe.isna().sum())\n    # reset the dataframe index\n    d_frame_null_values.reset_index(inplace=True)\n    # add colume header to the dataframe\n    d_frame_null_values.columns = ['Field_names', 'Null_value']\n    #calculate the percentage of null or missing values \n    d_frame_null_value_percentage = dataframe.isnull().sum() \/ len(dataframe) * 100\n    d_frame_null_value_percentage = pd.DataFrame(d_frame_null_value_percentage)\n    d_frame_null_value_percentage.reset_index(inplace=True)\n    d_frame_null_value_percentage = pd.DataFrame(d_frame_null_value_percentage)\n    d_frame_null_values['Null_values_percentage'] = d_frame_null_value_percentage[0]\n#d_frame_null_values['neww']= d_frame_null_values['Null_value'].apply(lambda d_frame_null_values:(d_frame_null_values\/len(training))*100)\n    return d_frame_null_values\n    \n    ","189e1590":"calculate_null_values(training)","6dd1d6b8":"training[training['Credit Score']> 850]","61e5de56":"def credit_error(value):\n    credit_value = value\n    credit_value['Credit Score'] = np.where(value['Credit Score'] > 850, value['Credit Score'].values \/10, value['Credit Score'])\n    return credit_value\n    \n    ","8ff51661":"c_training = credit_error(training)","8fe143f1":"c_training.describe()","9ad4df0f":"training.drop(columns=['Months since last delinquent','Loan ID','Customer ID'],axis=1, inplace=True )","24153c15":"training.head(10)","4bd453a7":"calculate_null_values(training)","21b6cef3":"training.tail(516)","5325cd1f":"training.drop(training.tail(514).index, inplace=True)","a3597c0a":"training.tail(516)","ba37084d":"calculate_null_values(training)","8bcab8bb":"training.describe()","cebe1a7c":"training.interpolate(inplace=True)","ec7300d4":"calculate_null_values(training)","461297f3":"training['Years in current job'].hist(figsize=(10,10))","504e445f":"training['Years in current job'].describe()","f5d1325e":"training['Years in current job'].fillna('10+ years', inplace=True)","a78c94e4":"calculate_null_values(training)","20b44652":"sbn.pairplot(training)","e35d8c19":"sbn.countplot(x='Home Ownership',data=training)","6962ff8f":"training['Purpose'].value_counts().sort_values(ascending=True).plot(kind='barh', \n                    title=\"Purpose for Loans\", figsize=(15,10))","ff46e46f":"training['Years in current job']=training['Years in current job'].str.extract(r\"(\\d+)\")\ntraining['Years in current job'] =training['Years in current job'].astype(float)","6229f9a9":"training","08bb6f17":"sbn.heatmap(training.corr())","ca69d119":"sbn.distplot(training['Years of Credit History'])","7ee00328":"training","7072e989":"#train = training\n#train.drop(['Term', 'Home Ownership', 'Purpose'], axis=1, inplace=True)","bae04d3d":"cat_data = ['Loan Status','Term','Home Ownership','Purpose']\ntransformer = ColumnTransformer([('transform', OneHotEncoder(), cat_data )],  remainder = 'passthrough')\ntra=  np.array(transformer.fit_transform(training), dtype = np.float)\n","fd09f2ae":"tra = pd.DataFrame(tra)","5b4b3a4c":"tra","40d44739":"training","5f1da366":"training['Loan Status'].hist()","44bca868":"training['Term'].hist()","fabc42af":"training['Home Ownership'].hist()","bda93bcb":"rename={1:'Paid',0:'Charged Off',2:'Long Term',3:'Short Term',5:'Home Mortgage',6:'Own Home', 7:'Rent',4:'Have Mortage'\n        ,13:'Home Improvements', 11:'Debt Consolidation',19:'other', 15:'Other', 17:'major_purchase', 21:'small_business'\n        ,14:'Medical Bills', 8:'Business Loan', 9:'Buy House', 10:'Buy a Car', 16:'Take a Trip', 23:'wedding', 22:'vacation'\n        ,18:'moving', 12:'Educational Expenses', 20:'renewable energy', 24:\"Loan Amount\", 25:'credit score'\n        ,26:'Annaual InCOME',27:'Years in Job',28:'monthly debt',29:'credit history',30:'Open account',31:'credit Problem'\n        ,32:'Current credit balance',33:'Maximum open credit',34:'Bankruptcies',35:'Tax Liens', }","6d581bad":"tra.rename(columns=rename, inplace=True)\ntra","e45507e1":"#pd.set_option('display.max_rows',)\n#aaaa= tra[[12,20]]\n#aaaa =tra[[]]\n#asaa= np.array(trr)\n#asaa[:]\n#aaaa.head(98565)","7c4ec3e3":"tra.drop(columns=['Charged Off', 'Long Term', 'Have Mortage', 'renewable energy'], axis=1, inplace=True)","0c915d6b":"dependent = tra['Paid']\nfeature = tra.drop(columns=['Paid'])\nx_train, x_test, y_train, y_test = train_test_split(feature,dependent, test_size=0.25, random_state=0)\nprint('The x_train shape is {} and the x_test shape is {} while the y_train shape is {} and the y_test shape is {}'\n     .format(x_train.shape,x_test.shape,y_train.shape,x_test.shape))\n","4811c7c2":"scaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)","0d019ccb":"y_train.to_numpy()\ny_test.to_numpy()\ny_train","5b486a03":"x_train","81367c63":"x_test","611a2970":"y_train","77f6a80e":"y_test","eb2d966d":"# Takes in a classifier, trains the classifier, and evaluates the classifier on the dataset\ndef do_prediction(classifier):\n    \n    # training the classifier on the dataset\n    classifier.fit(x_train, y_train)\n    \n    #Do prediction and evaluting the prediction\n    prediction = classifier.predict(x_test)\n    evaluate_prediction = cross_val(x_train,y_train, classifier)\n    coff_metrix = confusion_matrix(y_test, prediction)\n    \n    return evaluate_prediction,coff_metrix\n\ndef cross_val(x_train, y_train, classifier):\n    # Applying k-Fold Cross Validation\n    \n    accuracies = cross_val_score(estimator = classifier, X = x_train, y = y_train, cv = 5)\n    return accuracies.mean()\n","36b5bf9c":"#Training and Making prediction with Logistic Regression classifier\nlogreg = LogisticRegression(random_state=0)\nevaluate_logreg, log_metric = do_prediction(logreg)\nprint('LogisticRegression Performace on the test_set have an accuracy score of {}'.format((evaluate_logreg *100).round()) )\ngroup_names = [\"True Neg\",\"False Pos\",\"False Neg\",'True Pos']\ngroup_counts = ['{0:0.0f}'.format(value) for value in\n                log_metric.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in\n                     log_metric.flatten()\/np.sum(log_metric)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nax = plt.axes()\nsbn.heatmap(log_metric, annot=labels, fmt='', cmap='Blues', ax=ax)\nax.set_title('Logistic Regression Confussion Metrix')","b009acb5":"knn= KNeighborsClassifier(n_neighbors=7, p=2, metric='minkowski')\nevaluate_knn, knn_metric = do_prediction(knn)\nprint('KNeighborsClassifier Performace on the test_set have an accuracy score of {}'.format((evaluate_knn *100).round()) )\ngroup_names = [\"True Neg\",\"False Pos\",\"False Neg\",'True Pos']\ngroup_counts = ['{0:0.0f}'.format(value) for value in\n                knn_metric.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in\n                     knn_metric.flatten()\/np.sum(knn_metric)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nax = plt.axes()\nsbn.heatmap(knn_metric, annot=labels, fmt='', cmap='Blues', ax=ax)\nax.set_title('KNeighbors Confussion Metrix')","cce8103b":"gaussian = GaussianNB()\nevaluate_gaussian, gaussian_metric = do_prediction(gaussian)\nprint('GaussianNB Performace on the test_set have an accuracy score of {}'.format((evaluate_gaussian *100).round()) )\ngroup_names = [\"True Neg\",\"False Pos\",\"False Neg\",'True Pos']\ngroup_counts = ['{0:0.0f}'.format(value) for value in\n                gaussian_metric.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in\n                     gaussian_metric.flatten()\/np.sum(gaussian_metric)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nax = plt.axes()\nsbn.heatmap(gaussian_metric, annot=labels, fmt='', cmap='gist_heat_r', ax=ax)\nax.set_title('GaussianNB Confussion Metrix')\n","00d860df":"rand = RandomForestClassifier()\nevaluate_rand, rand_metric= do_prediction(rand)\nprint('RandomForest Performace on the test_set have an accuracy score of {}'.format((evaluate_rand *100).round()) )\ngroup_names = [\"True Neg\",\"False Pos\",\"False Neg\",'True Pos']\ngroup_counts = ['{0:0.0f}'.format(value) for value in\n                rand_metric.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in\n                     rand_metric.flatten()\/np.sum(rand_metric)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nax = plt.axes()\nsbn.heatmap(rand_metric, annot=labels, fmt='', cmap='twilight', ax=ax)\nax.set_title('RandomForest Confussion Metrix')","c398b9e9":"gboost = GradientBoostingClassifier()\nevaluate_gboost, gboost_metric = do_prediction(gboost)\nprint('GradientBoosting performace on the test_set have an accuracy score of {}'.format((evaluate_gboost *100).round()) )\ngroup_names = [\"True Neg\",\"False Pos\",\"False Neg\",'True Pos']\ngroup_counts = ['{0:0.0f}'.format(value) for value in\n                gboost_metric.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in\n                     gboost_metric.flatten()\/np.sum(gboost_metric)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nax = plt.axes()\nsbn.heatmap(gboost_metric, annot=labels, fmt='', cmap='icefire', ax=ax)\nax.set_title('GradientBoost Confussion Metrix')","e508cd66":"d_tree = DecisionTreeClassifier()\nd_tree.maxi_dept=100\nevaluate_d_tree, d_tree_metric = do_prediction(d_tree)\nprint('DecisiomTree performace on the test_set have an accuracy score of {}'.format((evaluate_d_tree *100).round()) )\ngroup_names = [\"True Neg\",\"False Pos\",\"False Neg\",'True Pos']\ngroup_counts = ['{0:0.0f}'.format(value) for value in\n                d_tree_metric.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in\n                     d_tree_metric.flatten()\/np.sum(d_tree_metric)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nax = plt.axes()\nsbn.heatmap(d_tree_metric, annot=labels, fmt='', cmap='terrain', ax=ax)\nax.set_title('DecisionTree Confussion Metrix')","03d167c7":"xboost = XGBClassifier()\nevaluate_xboost, xboost_metric = do_prediction(xboost)\nprint('XBoost Classifier performace on the test_set have an accuracy score of {}'.format((evaluate_xboost *100).round()) )\ngroup_names = [\"True Neg\",\"False Pos\",\"False Neg\",'True Pos']\ngroup_counts = ['{0:0.0f}'.format(value) for value in\n                xboost_metric.flatten()]\ngroup_percentages = ['{0:.2%}'.format(value) for value in\n                     xboost_metric.flatten()\/np.sum(xboost_metric)]\nlabels = [f'{v1}\\n{v2}\\n{v3}' for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nax = plt.axes()\nsbn.heatmap(xboost_metric, annot=labels, fmt='', cmap='binary', ax=ax)\nax.set_title('XBoost Classifier Confussion Metrix')","680bf0ef":"plt.style.use('fivethirtyeight')\nfigsize=(8, 6)\n\n# Dataframe to hold the results\nweigh_up = pd.DataFrame({'model': ['Logistic Regression', 'K-NN', 'Decision Tree','Gradiant Boost', 'Random Forest',\n                                  'GaussianNG'],\n                        'score': [evaluate_logreg, evaluate_knn, evaluate_d_tree\n                                  ,evaluate_gboost,evaluate_rand,evaluate_gaussian]})\n\n# Horizontal bar chart of test mae\nweigh_up.sort_values('score', ascending = True).plot(x = 'model', y = 'score', kind = 'barh',\n                                                           color = 'red', edgecolor = 'black')\n\n# Plot formatting\nplt.ylabel(''); plt.yticks(size = 14); plt.xlabel('K-Fold Cross Validation'); plt.xticks(size = 14)\nplt.title('Model Comparison on Score', size = 20);","01bd4f28":"The number of customers who have paid op their loan is 3 times more than those who did not, in other to make informed observation from visualization we have to standardize or normalize some of the values, because they are very large to observe.","8e6a8b90":"Based on the finding here, it seems this is a data entry error, where a random zero was added to some values, the solution\nis to remove this zeros from the values, so we will write a function to remove the zeros from vales greater than 850","96dac33f":"Now, to better understand the results, I will show in a graph the model that has the better Cross Validation Score","f0cebe96":"Now the avarage or the mean score look intutive, so we will move forward by dropping some unwanted columns, we are dropping \nLoan ID, Customer ID, these are categorical data and they actually have no effect on the data, and we are droping the columns with 50% or more null values ","ca14e397":"For the rest of the missing data we will have to a data imputation technique to replace those values","adee18cc":"                             Printing out the list of columns in the training datasets","d57dd9d7":"now we have to drop one of our new collumn from the new categories we created in other to avoid the dummmy variable trap,we\nwill drop 'charged off', 'Long Term', 'Have Mortage', 'renewable energy' from our dataset","96f648f5":"                                        Loading the datasets","ebe03f4a":"Everything is now okay we are prepare to test and compare different classification Model","306d0b98":"                        A function to calculate and print out the missing value and it percentage ","a821117c":"                                            Some first few observation for now","623faabd":"right now before will go ahead with creating the model, since we have handled the categorical data, ww will Split the dataset into training and testing set and fit and transform ","ade13320":"#                                                   ##Conclusion","858e928f":"Looking at the table above majority of the attribute have excatly 514 data entry missing, we will\ninvestigate and see if droping those row will be a good idea","a56a06e1":"Conclusions :\nUsing Logistic Regression, Gradiant Boost, K-NN, and Decisiom Tree were above 70%  Accuracy score is good(79%), but \nDecision Tree algorithm worked very well compared to the others with an accuracy score of about 78% \nFinally what I want say that is :\nThere will be a good improvement in Logistic Regression model and the others after some more Feature Selection.\n","30e1d5b0":"Left with the missing value for our categorical data","a096d433":"We dealtwith the missing values, next we we valiaze some of this data for more insight","7664da49":"1) We have two unique customer IDs that are of no use to us\n2) the \"Months since last delinquent\" columns have a data high 53% null value compared to the others \n3) so firstly we will be dropping the three columns \n4) out of the 19 atrributes we have 7 as objects and 12 as float values\n5) our dependent variable is also an object so we will have to manipulate it \n6) Based on my research a typical credit score ranges from 300 to 850\n7) but the average value of our credit score is 1076 and some values \n8) we will have to investigate that before we follow with the rest of the observation","fb0406d8":"It the columns have the last 514 data entry empty so droping them will be a good idea","58bf6181":"Our next step is to perform feature scalling, this is because some categories like annaul income and loan amount "}}