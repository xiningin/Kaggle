{"cell_type":{"e2139e61":"code","247ce999":"code","589e2479":"code","95233066":"code","04262a6d":"code","6cb78f09":"code","4661e782":"code","645ec180":"code","a27092b2":"code","625a024d":"code","2e14719a":"code","97469d75":"code","7fdae9df":"code","1c1205bc":"code","46c94a63":"code","e46f5a1c":"code","ef7875e9":"code","df3ae6bd":"code","62397f68":"code","995ee449":"code","000f4397":"code","13c4ed79":"code","1883114d":"code","ae2bb191":"code","e7bf7cbf":"code","65a185fc":"code","900a1aeb":"code","2a356571":"code","c6e9f9b8":"code","b14d93d1":"code","460fc465":"code","b2c11d4c":"code","8aabce2b":"code","17275994":"code","6b4d6337":"code","19494525":"code","ec6c2731":"code","f5925261":"code","70880fc8":"code","e9277b81":"code","13507600":"code","0f674c84":"code","d8dcad6b":"code","4573f26b":"code","fdd34e66":"code","bbe68a1b":"code","21f1e41c":"code","1814c7d3":"code","cca94bf8":"code","dea09ced":"code","1ee6b52c":"code","9bc603b9":"markdown","5366ca39":"markdown","c314db6e":"markdown","1ac5ab4d":"markdown","1612bf29":"markdown","81a5ca9c":"markdown","b40631ce":"markdown","0523af36":"markdown","8dc8e576":"markdown","7047ba94":"markdown","cd8afd3f":"markdown","98a747dc":"markdown","aac22ac5":"markdown","92fc48b6":"markdown","e5c8969f":"markdown","37242558":"markdown","528523dc":"markdown","a681c2b6":"markdown","e0ab7aea":"markdown","00d92667":"markdown","445646bf":"markdown","f0251876":"markdown","df24578a":"markdown","79805e09":"markdown","b23fdcf7":"markdown","5ff82206":"markdown","47a25247":"markdown","5d712e9f":"markdown","fd3af88d":"markdown","1b6ec62c":"markdown","b62498ee":"markdown"},"source":{"e2139e61":"!pip install collinearity","247ce999":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xbg\nimport matplotlib.pyplot as plt\nfrom collinearity import SelectNonCollinear\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import balanced_accuracy_score, roc_auc_score , make_scorer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import recall_score, precision_score, f1_score\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter","589e2479":"data = pd.read_csv('..\/input\/uci-semcom\/uci-secom.csv')","95233066":"data.shape","04262a6d":"data.columns","6cb78f09":"y = data['Pass\/Fail']","4661e782":"sns.histplot(data = y, stat='count' , discrete='True', shrink =1.8 , cbar= True , color= 'grey')","645ec180":"y.value_counts()","a27092b2":"X = data.drop(columns=['Pass\/Fail','Time'], axis = 1) ","625a024d":"X.head().T","2e14719a":"sns.heatmap(X.corr())","97469d75":"X.isnull().sum().sort_values(ascending=False)","7fdae9df":"X.fillna(value=0, inplace=True)","1c1205bc":"X.isnull().sum()","46c94a63":"y.isnull().sum()","e46f5a1c":"X.shape","ef7875e9":"uni_list =[]\nfor column in X.columns:\n  if (X[column].nunique() == 1):\n    uni_list.append(column)","df3ae6bd":"X.drop(columns=uni_list, axis =1, inplace=True)","62397f68":"X.shape","995ee449":"X_column = X.columns","000f4397":"selector = SelectNonCollinear(correlation_threshold=0.4,scoring=f_classif)","13c4ed79":"X = X.values\ny = y.values","1883114d":"selector.fit(X,y)","ae2bb191":"mask = selector.get_support()","e7bf7cbf":"mask[:5]","65a185fc":"sum(mask)","900a1aeb":"X_preprprocessed = pd.DataFrame(X[:,mask],columns = np.array(X_column)[mask])","2a356571":"sns.heatmap(X_preprprocessed.corr())","c6e9f9b8":"X_preprprocessed.shape","b14d93d1":"X_train , X_test , y_train , y_test = train_test_split(X_preprprocessed , y , \n                                                       random_state = 42 , test_size = 0.2 , \n                                                       stratify = y )","460fc465":"sum(y_train)\/len(y_train)","b2c11d4c":"sum(y_test)\/len(y_test)","8aabce2b":"oversample = SMOTE()\nX_train, y_train = oversample.fit_resample(X_train, y_train)","17275994":"counter = Counter(y_train)\nprint(counter)","6b4d6337":"clf_xbg = xbg.XGBClassifier( seed = 42 , objective = 'binary:logistic', missing = 0)","19494525":"clf_xbg.fit( X_train, y_train,\n            verbose = True,\n            early_stopping_rounds = 5,\n            eval_metric = 'error',  \n            eval_set = [(X_test, y_test)])","ec6c2731":"unique_elements, counts_elements = np.unique(y_test, return_counts=True)\nprint(\"Frequency of unique values of the said array:\")\nprint(np.asarray((unique_elements, counts_elements)))","f5925261":"plot_confusion_matrix( clf_xbg, X_test, y_test,\n                      values_format = 'd', display_labels = [\"Pass\",\"Fail\"])","70880fc8":"y_prediction = clf_xbg.predict(X_test)","e9277b81":"print(\"Accuracy of XGBoost :- \", clf_xbg.score(X_test,y_test)*100)","13507600":"param_grid = {\n    'max_depth' : [2,3],\n    'learning_rate' : [ 0.05, 0.1],\n    'gamma' : [0, 0.25],\n    'reg_lambda' : [0, 0.25],\n    'scale_pos_weight' : [1, 3]\n}","0f674c84":"optimal_params = GridSearchCV(\n    estimator = xbg.XGBClassifier(objective = 'binary:logistic',\n                                  seed = 42,\n                                  subsample = 0.9,\n                                  colsample_bytree = 0.5),\n                              param_grid = param_grid,\n                              scoring = 'max_error',\n                              verbose = 0,\n                              n_jobs = 10,\n                              cv = 3\n)","d8dcad6b":"optimal_params.fit(X_train,\n                   y_train,\n                   early_stopping_rounds = 25,\n                   eval_set = [(X_test,y_test)],\n                   verbose = True\n                   )","4573f26b":"print(optimal_params.best_params_)","fdd34e66":"clf_xbg = xbg.XGBClassifier( seed = 42,\n                            objective = 'binary:logistic',\n                            gamma = 0,\n                            learning_rate = 0.05,\n                            max_depth = 2,\n                            reg_lambda = 0,\n                            scale_pos_weight = 1,\n                            subsample = 0.9,\n                            colsample_bytree = 0.5)","bbe68a1b":"clf_xbg.fit(X_train, y_train,\n            verbose = True,\n            early_stopping_rounds = 5,\n            eval_metric = 'error',\n            eval_set = [(X_test,y_test)])","21f1e41c":"plot_confusion_matrix( clf_xbg, X_test, y_test,\n                      values_format = 'd', display_labels = [\"Pass\",\"Fail\"])","1814c7d3":"y_prediction = clf_xbg.predict(X_test)\nprint(\"Accuracy of XGBoost :- \", clf_xbg.score(X_test,y_test)*100)","cca94bf8":"recall_score(y_true = y_test, y_pred = y_prediction)","dea09ced":"precision_score(y_true = y_test, y_pred = y_prediction)","1ee6b52c":"f1_score(y_true = y_test, y_pred = y_prediction)","9bc603b9":"Unfortunately, most are just numbers .","5366ca39":"# Introduction","c314db6e":"# Splitting into train & test","1ac5ab4d":"We have built a XGBoost model, used SMOTE to balance the imbalanced data and tried Hyperparameter Tuning to find the best Hyperparameters . And yet our model is very far from ideal . Maybe, a different model would be more promising . ","1612bf29":"# Preprocessing & Exploratory Data Analysis (EDA)","81a5ca9c":"Since the y dataset is imbalanced . Let us use SMOTE .","b40631ce":"We have a marginally better recall, but still we have heavily compromised on the accuracy score .","0523af36":"Now let's split the data into dependant **y** dataset and independant **X** dataset .","8dc8e576":"X_preprprocessed is the dataset without any multicollinearity . The following heatmap should reflect that .","7047ba94":"We must remove the columns which contain only a single unique value as they are not useful to the classifier . ","cd8afd3f":"Stratify is used so that the test set is representative of the much larger train set . Let's see if that worked by taking the average value of y_train and y_test .","98a747dc":"Let's see, if we can infer something from the column name .","aac22ac5":"# Hyperparameter Tuning","92fc48b6":"We've removed 112 columns .","e5c8969f":"From 590 columns in X, we have reduced it to 150 .","37242558":"In this notebook we will be building an XGBoost Classifier model to predict if a semiconductor has passed or failed the test . We will be using the [UCI SECOM Data Set](https:\/\/archive.ics.uci.edu\/ml\/datasets\/SECOM) .\n\nWe will use SMOTE(Synthetic Minority Oversampling Technique) and will try Hyperparameter Tuning .","528523dc":"# Importing the necessary libraries","a681c2b6":"Perfect, we've taken care of the Null values .","e0ab7aea":"# Reading the data","00d92667":"In the following grid, you can pass any values you feel to be promising . Be warned , it takes a long time to run .","445646bf":"Unfortunately, the dependant dataset is imbalanced .","f0251876":"We will be using the object SelectNonCollinear from the library collinearity . We set the threshold to 0.4 meaning columns with collinearity greater than 0.4 will be removed .","df24578a":"get_support returns a list of boolean values .","79805e09":"Let's see the distribution of y variable .","b23fdcf7":"Plenty of Null values in the X dataset . ","5ff82206":"# Building the XGBoost model","47a25247":"Have a look at XGBoost [documentation](https:\/\/xgboost.readthedocs.io\/en\/latest\/) for a better understanding of the code below .","5d712e9f":"Though , the accuracy of the model is great . the recall is poor . The model does a bad job of classifying true positives .","fd3af88d":"This isn't great, there seems to be a great deal of multicollinearity among the independant X variables . We'll have to take care of that later, after we remove the null values(if any).","1b6ec62c":"We will also be using the collinearity library . Read this [blog](https:\/\/www.yourdatateacher.com\/2021\/06\/28\/a-python-library-to-remove-collinearity\/) for a better understanding of the library .","b62498ee":"# Conclusion"}}