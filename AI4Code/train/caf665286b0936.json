{"cell_type":{"dd32fedb":"code","e28c44a6":"code","bbec8e18":"code","3ab7609a":"code","eba014fe":"code","efe43389":"code","a9faf049":"code","bfd13154":"code","defb427d":"code","234e3791":"code","1588d68a":"code","e9148955":"code","7ca79f56":"code","6eb03a4c":"code","e57acafc":"code","04ab7d6b":"code","8e4db372":"code","3b6af903":"code","215bd3f8":"code","29e36768":"code","ff352915":"code","c6fc87cb":"code","7cc0d46e":"markdown","194fe167":"markdown","0a51c0a2":"markdown","239a00a6":"markdown","e8618f72":"markdown"},"source":{"dd32fedb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e28c44a6":"df=pd.read_csv('\/kaggle\/input\/startup-logistic-regression\/50_Startups.csv')","bbec8e18":"df.info()","3ab7609a":"df.head(n=4)","eba014fe":"df.describe()","efe43389":"#Check Null Value\ndf.isnull().sum()","a9faf049":"#Split Dataset into X and y\nX=df.drop(columns='Profit')\ny=df['Profit']\nprint(X.shape)\nprint(y.shape)","bfd13154":"X['State'].unique()","defb427d":"#Make object type variable into numeric\n# from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n# LE=LabelEncoder()\n# df.iloc[:,3]=LE.fit_transform(df.iloc[:,3])\n# OHE=OneHotEncoder()\n# OHE.fit_transform(df[['State']]).toarray()\nX=pd.get_dummies(X,drop_first=True)","234e3791":"#Check any outlier on features having numeric values\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfor i in X.iloc[:,0:3]:\n    plt.boxplot(df[i],notch=True,patch_artist=True)\n    plt.show()","1588d68a":"#Check Multicollinearity. \n#VIF=1\/1-R2 (R2 is required to determine the coefficient in linear regression)\n#Greater the value of R2, greater the value of VIF. Value above 5 considers the high collinearity.\n#VIF (Variance Influation factor is the method through which we can check the multicollinearity)\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\npd.DataFrame({'Features':X.columns,'VIF':[ VIF(X.values,i) for i in range(len(X.columns))]})","e9148955":"#Split X and y into test and train\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2,random_state=21)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","7ca79f56":"from sklearn.linear_model import LinearRegression\nmodel_mlr=LinearRegression()\nmodel_mlr.fit(X_train,y_train)\nprint(model_mlr.coef_)\nprint(model_mlr.intercept_)","6eb03a4c":"y_pred=model_mlr.predict(X_test)\nx_pred=model_mlr.predict(X_train)","e57acafc":"#Check model performance for traina and test\nfrom sklearn.metrics import mean_squared_error\ntrain_score=model_mlr.score(X_train,y_train)\ntest_score=model_mlr.score(X_test,y_test)\nprint(train_score)\nprint(test_score)\nprint(f\"RMSE score of training dataset is : {np.sqrt(mean_squared_error(y_train,x_pred))}\")\nprint(f\"RMSE score of testing dataset is {np.sqrt(mean_squared_error(y_test,y_pred))}\")","04ab7d6b":"#Test this model with unseen or new data\n[110221,120223,423112,0,0,1]\nnp.round(model_mlr.predict(np.array([[110221,120223,423112,1,0]])),2)","8e4db372":"print(f\"The difference between train score and test score is {test_score-train_score}\")","3b6af903":"#Preparation of Backward Elimination\nimport statsmodels.api as sma\n\n#Here we have to manually add b0 which is constant features in MLR but not associated with any of the column\ndataset=X.copy()\ndataset['x0']=np.ones((len(X),1),dtype='int')\n#Now we will choose all columns and fit them into OLS and then check which p-value is greater than SL value (0.05)\ndataset_opt=dataset.iloc[:,[5,3,4,0,1,2]]\nsma_ola=sma.OLS(endog=y,exog=dataset_opt).fit()\nsma_ola.summary()","215bd3f8":"dataset_opt=dataset_opt.iloc[:,[0,3,5]]\nsma_ola=sma.OLS(y,dataset_opt).fit()\nsma_ola.summary()","29e36768":"dataset_opt=dataset_opt[['x0','R&D Spend']]\nresults=sma.OLS(y,dataset_opt).fit()\nresults.summary()","ff352915":"#Make the model using this feature and see what is the difference we are getting\n#Extract X and y from dataframe (Here we are using only R&D as independent variable)\n\nX=df.iloc[:,[0]].values\ny=df.iloc[:,-1].values\n\n#Split X and y into test and train\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2,random_state=21)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\n#Make model\nfrom sklearn.linear_model import LinearRegression\nmodel_mlr=LinearRegression()\nmodel_mlr.fit(X_train,y_train)\nprint(model_mlr.coef_)\nprint(model_mlr.intercept_)\n\ny_pred=model_mlr.predict(X_test)\nx_pred=model_mlr.predict(X_train)\n\n#Check model performance for traina and test\nfrom sklearn.metrics import mean_squared_error\ntrain_score=model_mlr.score(X_train,y_train)\ntest_score=model_mlr.score(X_test,y_test)\nprint(train_score)\nprint(test_score)\nprint(f\"RMSE score of training dataset is : {np.sqrt(mean_squared_error(y_train,x_pred))}\")\nprint(f\"RMSE score of testing dataset is {np.sqrt(mean_squared_error(y_test,y_pred))}\")","c6fc87cb":"print(f\"The difference between train score and test score is {test_score-train_score}\")","7cc0d46e":"R&D spend also having high p value which is greater than SL value of 0.05. So we need to remove this. ","194fe167":"In this dataset we have R&D spend,Administrator, Marketing Spend and profit. Our goal is to make a model which can best predict the profit gain by which company. So Profit is our dependent variable and other features are independent variables. ","0a51c0a2":"To minimize the difference we have to use \"Backward Elimination\", this method provides the importance features which gives the good prediction result. ","239a00a6":"From the above table we can see that R&D Spend and Marketing Spend having little collinearity. So we are considering both feature for model building","e8618f72":"From the above OLS method we can see that feature \"State_Florida\",\"State New York\" and \"Administration\" having high p value which is greater than SL of 0.05. So we will remove those features and prepare a model"}}