{"cell_type":{"4fc77fb5":"code","af2125d3":"code","ab0205b2":"code","7ef78ac8":"code","e6af9f63":"code","1f256f54":"code","2f6f1bdb":"code","c7a5db70":"code","d2f7b93e":"code","d35fcdf9":"code","72c14771":"code","4d7978d4":"code","846fca36":"code","3a01a2db":"code","46895a9a":"code","2344baf2":"code","ac44e7e9":"code","033b5e22":"code","9f5a786f":"code","37625d9a":"code","7557798f":"code","6090ff57":"code","bf1ea296":"code","472eb6ef":"code","bc41a8d0":"code","e0e99d9f":"code","53b623ae":"code","1c72f0be":"code","6e90300f":"code","3fb409b4":"code","3dfc57ce":"code","e582ff6c":"code","9a6ff6af":"code","6704dd26":"code","65391798":"code","9413a5bd":"code","3012bbb6":"code","ed8699d5":"code","1b0cd7dd":"code","4cd6bbfc":"code","2c829156":"code","f5a8238e":"code","9bad83c6":"code","2f785715":"code","6c008db4":"code","2caac314":"code","e62e1aed":"code","28908c1e":"code","ff4c72e1":"code","b2c2b768":"code","f0e0dbca":"code","ad404350":"code","1656e6af":"code","78fd969a":"code","92e889c5":"code","d29f6755":"code","c253c0c0":"code","9aff82f6":"code","61596488":"code","d4dde050":"code","a7d0d586":"code","5313818d":"code","8288e9b4":"code","15c99754":"code","d08f1196":"code","20536b63":"code","ad8c324e":"code","304e46cf":"code","a12060ed":"code","0089309f":"code","a7da715a":"code","18e88d66":"code","c1fbba31":"code","f3f60bbe":"code","8f15898b":"code","22dbf5da":"code","0b230500":"code","15905158":"code","e5da48b9":"code","88650cd2":"code","9770ceca":"code","873f679f":"code","7c701a15":"code","5e140602":"code","a0592d15":"code","c719f0ec":"code","f86eff7c":"code","0f1ffdfc":"code","1a8bb85a":"code","39e583d2":"code","a92a5499":"code","84a6dd3a":"code","590c20d0":"code","b7bf1935":"code","36db7ec2":"code","b23232bb":"code","857668da":"code","2f1e06e2":"code","3e311e63":"code","6f774610":"code","27425c6e":"code","60e41393":"code","7e2ad723":"code","04277a07":"code","1a526868":"markdown","91919376":"markdown","b728c15c":"markdown","f9c11b44":"markdown","18d832b4":"markdown","71835384":"markdown","abd80b6a":"markdown","4888b218":"markdown","e4313e72":"markdown","0d250076":"markdown","7a23ed90":"markdown","a266b1a1":"markdown","c8922f11":"markdown","41838735":"markdown","d87995f6":"markdown","cd581bd1":"markdown","6bf3ac0c":"markdown","1455b1a6":"markdown","8ad62892":"markdown","2ec3739d":"markdown","e4d89355":"markdown","517e4bd0":"markdown","39496bed":"markdown","30ef076f":"markdown","b6476866":"markdown","2440d852":"markdown","8a00f3b3":"markdown","0f07908b":"markdown","5e20d07a":"markdown","8872ad0d":"markdown","390e9e89":"markdown","3010a352":"markdown","d8fd140a":"markdown","45cd3042":"markdown","5cfaa79a":"markdown","8f93e75c":"markdown","b1565a77":"markdown","8fd655ed":"markdown","0ec333f7":"markdown","ae952284":"markdown","f265d40f":"markdown","f26042a2":"markdown","80ffa8e1":"markdown","6c22db59":"markdown","c1ea5fef":"markdown","7e8f3df4":"markdown","df6fbe2f":"markdown","21be61a4":"markdown","1c9854d4":"markdown","6dfd73bb":"markdown","046f70f9":"markdown"},"source":{"4fc77fb5":"!pip install pydotplus\n!pip install six","af2125d3":"# General tools\nimport os\nimport pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom IPython.display import Image, HTML\nfrom plotnine import *\nimport pydot\nfrom plotnine import *\nfrom tqdm import tqdm\n\n# For transformations and predictions\nfrom scipy.optimize import curve_fit\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import pairwise_distances\nfrom sklearn.tree import DecisionTreeRegressor, export_graphviz\nfrom six import StringIO\n\n# For scoring\nfrom sklearn.metrics import mean_squared_error as mse\n\n# For validation\nfrom sklearn.model_selection import train_test_split as split\n\n# sns.set_theme(style=\"darkgrid\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ab0205b2":"path = '\/kaggle\/input\/spotify-dataset-19212020-160k-tracks\/data.csv'\ndf = pd.read_csv(path)\ndf.head()","7ef78ac8":"df.info()","e6af9f63":"columns = ['id', 'name', 'artists', 'release_date', 'year']\nfor col in columns:\n  print(f'{col:<15}: {df[col].nunique()} unique values')","1f256f54":"df = df.drop(labels=['id', 'name', 'release_date'], axis=1)\ndf.shape","2f6f1bdb":"df.duplicated().sum()","c7a5db70":"df = df[~df.duplicated()==1]\ndf.shape","d2f7b93e":"df.isnull().sum().sum()","d35fcdf9":"numeric_columns = df.columns[df.dtypes != 'object']\nstring_columns = df.columns[df.dtypes == 'object']\nprint(f'There are {len(numeric_columns)} numeric columns & {len(string_columns)} string columns')","72c14771":"# Numeric Heatmap\nnumeric_df = pd.DataFrame(data=df, columns=numeric_columns, index=df.index)\ncorr = np.abs(numeric_df.corr())\nfig, ax = plt.subplots(figsize=(8, 8))\ncmap = sns.color_palette(\"magma\")\nsns.heatmap(corr, cmap=cmap, square=True)\nplt.title('Correlation between numerical features: abs values')\nplt.show()","4d7978d4":"series = np.abs(corr['popularity']).sort_values(ascending=False)\nprint('The most linear correlated features to POPULARITY are:')\nfor i, row in enumerate(series):\n    if 0.2 <= row < 1:\n      print(f'{series.index[i]:17} --> {row: .2f} (abs)')","846fca36":"# 'Pairplot of Numerical Features exc. dummies'\n# Data is sampled here due to long running time\nsns_plot = sns.pairplot(df.sample(1000), height=1, vars=['popularity', 'acousticness', 'danceability', 'duration_ms', 'energy',\n       'instrumentalness', 'key', 'liveness', 'loudness', \n       'speechiness', 'tempo', 'valence', 'year'])\n        # sns_plot.savefig('pairplot.png')\nplt.show()","3a01a2db":"df['artists'].nunique()","46895a9a":"# # Plotting\nfig, ax = plt.subplots(figsize = (12, 10))\nlead_artists = df.groupby('artists')['popularity'].sum().sort_values(ascending=False).head(20)\nax = sns.barplot(x=lead_artists.values, y=lead_artists.index, palette=\"Greens\", orient=\"h\", edgecolor='black', ax=ax)\nax.set_xlabel('Sum of Popularity', c='r', fontsize=12)\nax.set_ylabel('Artist', c='r', fontsize=12)\nax.set_title('20 Most Popular Artists in Dataset', c='r', fontsize=14, weight = 'bold')\nplt.show()","2344baf2":"# find out artists with more than 100 tracks\ndf['popular_artist'] = df['artists'].map(df['artists'].value_counts()>100)\npop_arts  = df.groupby(['artists', 'popular_artist'])['popularity'].mean().sort_values(ascending=False).reset_index(1)\ndf_pop_arts = pop_arts.loc[pop_arts['popular_artist'] == True,['popularity']]\n\n# # Plotting\nfig, ax = plt.subplots(figsize = (12, 10))\nlead_artists = df_pop_arts.groupby('artists')['popularity'].mean().sort_values(ascending=False).head(20)\nax = sns.barplot(x=lead_artists.values, y=lead_artists.index, palette=\"Greens\", orient=\"h\", edgecolor='black', ax=ax)\nax.set_xlabel('Mean of Popularity', c='r', fontsize=12)\nax.set_ylabel('Artist', c='r', fontsize=12)\nax.set_title('20 Most Popular Artists in Dataset with > 100 tracks', c='r', fontsize=14, weight = 'bold')\nplt.show()","ac44e7e9":"# Adding Mean & Count values to each artist\ndf['mean'] = df.groupby('artists')['popularity'].transform('mean')\ndf['count'] = df.groupby('artists')['popularity'].transform('count')\n# plotting\nfig, ax = plt.subplots(figsize = (20, 3))\nax = sns.distplot(df['count'], bins = 600)\nax.set_xlabel('Count of apperances in data', fontsize=12, c='r')\nax.set_ylabel('% of artists', fontsize=12, c='r')\nplt.show()","033b5e22":"fig, ax = plt.subplots(figsize = (15, 3))\nax = sns.distplot(df['count'], bins=600, kde=False)\nax.set_xlabel('Count of appearances in data', fontsize=12, c='r')\nax.set_ylabel('# of artists', fontsize=12, c='r')\nax.set_xlim(1,20)\nax.set_xticks(range(1,21,1))\nax.axvline(x=3, ymin=0, ymax=1, color='orange', linestyle='dashed', linewidth=3)\nfont = {'family': 'serif',\n        'color':  'red',\n        'weight': 'bold',\n        'size': 14,\n        }\nax.annotate(\"\", xy=(3, 19000), xytext=(4.8, 19000), arrowprops=dict(arrowstyle=\"->\", color='r', linestyle='dashed', linewidth=3))\nax.text(x = 5, y = 19000, s='cutoff = 3', fontdict=font)\n\nplt.show()","9f5a786f":"fig, ax = plt.subplots(figsize = (15, 3))\nstat = df.groupby('count')['mean'].mean().to_frame().reset_index()\nax = stat.plot(x='count', y='mean', marker='.', linestyle = '', ax=ax)\nax.set_xlabel('Count of appearances in data', fontsize=12, c='r')\nax.set_ylabel('Mean Popularity', fontsize=12, c='r')\nplt.show()","37625d9a":"frequent_flyer = df[['artists','mean']].loc[df['count']>600].value_counts(ascending=False).reset_index()\nfrequent_flyer.columns=['Artist','Mean Popularity', '# of Tracks']\nfrequent_flyer","7557798f":"fig, ax = plt.subplots(figsize = (15, 5))\ny_ticks = ['No Explicit\\nContent', 'Explicit\\nContent']\nax = sns.barplot(x = (df['explicit'].value_counts(normalize=True)*100),\n                 y= y_ticks, \n                 data=df, palette='Spectral')\nax.set_xlabel('% of records', fontsize=12, weight='bold')\nax.set_xlim(0.0,100.0)\nax.set_xticks(ticks=range(0,101,10))\nax.set_yticklabels(labels=y_ticks, fontsize=12)\nax.text(s='91.48%\\n(154,829)', x=1, y=0.1, fontdict={'weight': 'bold', 'c':'red'})\nax.text(s='8.51%\\n(14,411)', x=1, y=1.1, fontdict={'weight': 'bold', 'c':'red'})\n\nplt.suptitle('Occurence of Explicit Data', fontsize=14, c='r')\nplt.show()","6090ff57":"exp_data = sns.catplot(data=df, kind=\"bar\",\n                        x=\"explicit\", y=\"popularity\",\n                        ci=\"sd\", palette='Accent',\n                        height=6)\nexp_data.fig.suptitle('Popularity vs. Explicit', y=1.05, c='r')\nexp_data.set_axis_labels(\"\", \"Popularity\")\nexp_data.set_xticklabels(['No Explicit\\nContent', 'Explicit\\nContent'])\nplt.show()","bf1ea296":"df['duration_ms'].corr(df['popularity'])","472eb6ef":"fig, ax = plt.subplots(figsize = (15, 5))\nkey_data = df['key'].value_counts(normalize=True, sort=True, ascending=True)*100\ny_ticks = df['key'].value_counts().index\nax = sns.barplot(x = key_data.values, y= y_ticks, data = key_data, orient='h', palette='Spectral')\nax.set_xlabel('% of records', fontsize=12, weight='bold')\nax.set_ylabel('Key', fontsize=12, weight='bold')\nax.set_xlim(0.0,15.0, 0)\nax.set_xticks(ticks=range(0,16,1))\nax.set_yticklabels(labels=y_ticks, fontsize=12) #\n\nrects = ax.patches\n\n# For each bar: Place a label\nfor rect in rects:\n    # Get X and Y placement of label from rect.\n    x_value = rect.get_width() #f'{rect.get_width():.2f}'\n    y_value = rect.get_y() + rect.get_height() \/ 2\n    # Number of points between bar and label. Change to your liking.\n    space = 5\n    # Vertical alignment for positive values\n    ha = 'left'\n\n    # Use X value as label and format number with one decimal place\n    label = f'{x_value:.1f}'\n\n    # Create annotation\n    plt.annotate(\n        label,                      # Use `label` as label\n        (x_value, y_value),         # Place label at end of the bar\n        xytext=(space, 0),          # Horizontally shift label by `space`\n        textcoords=\"offset points\", # Interpret `xytext` as offset in points\n        va='center',                # Vertically center label\n        ha=ha)                      # Horizontally align label differently for\n                                    # positive and negative values.\n\nplt.suptitle('Distribution of Key (%)', fontsize=14, c='r')\nplt.show()","bc41a8d0":"exp_data = sns.catplot(data=df, kind=\"bar\",\n                        x=\"key\", y=\"popularity\",\n                        palette='pastel',\n                        ci = 'sd',\n                        height=5, aspect = 3)\nexp_data.fig.suptitle('Popularity by Key Category', y=1.05, c='r', weight='bold')\nexp_data.set_axis_labels(\"key\", \"Popularity\")\nplt.show()","e0e99d9f":"ax = df.groupby('mode')['popularity'].count().plot.pie(labels=['Minor', 'Major'],\n                                 autopct='%1.1f%%', \n                                 fontsize=12)\nax.set_ylabel('')\nax.set_title('Mode Ratios')\nax.legend(labels=['Minor (n = 49298)', 'Major (n = 119942)'], bbox_to_anchor=(1, 1))\na = ax.get_anchor\nplt.show()","53b623ae":"exp_data = sns.catplot(data=df, kind=\"bar\",\n                        x=\"mode\", y=\"popularity\",\n                        ci=\"sd\", palette='Accent',\n                        height=4)\nexp_data.fig.suptitle('Popularity vs. Mode', y=1.05, c='r')\nexp_data.set_axis_labels(\"\", \"Popularity\")\nexp_data.set_xticklabels(['Minor (0)', 'Major(1)'])\nplt.show()","1c72f0be":"fig, ax = plt.subplots(figsize=(16, 4))\nsns.distplot(df['acousticness'], kde=False, bins=30)\nplt.show()","6e90300f":"fig, ax = plt.subplots(figsize=(15, 6))\nax1_data =  df.groupby('acousticness')['popularity'].mean().to_frame().reset_index()\nax = sns.scatterplot(x = ax1_data['acousticness'], y = ax1_data['popularity'], color='blue', ax=ax)\nax.set_title('Acousticness vs. Mean Popularity')\nax.set_ylabel('Mean Popularity', fontsize=12)\nplt.tight_layout()\nplt.show()","3fb409b4":"a= df['danceability'].corr(df['popularity']).round(2)\nprint(f'The pear\u05d3on corr is -->{a:^10}')","3dfc57ce":"df['danceability'].describe()","e582ff6c":"sns.distplot(df['danceability'],kde=False, bins=30)\nplt.show()","9a6ff6af":"fig, ax = plt.subplots(1, figsize=(15, 6), sharey=True, sharex = True)\nax_data =  df.groupby('danceability')['popularity'].mean().to_frame().reset_index()\nax = sns.scatterplot(x='danceability', y='popularity', data=ax_data, color='blue', ax=ax)\nax.set_title('danceability')\nax.set_ylabel('Mean Popularity', fontsize=12)\nplt.tight_layout()\nplt.show()","6704dd26":"# convert the miliseconds to minutes\ndf['duration_min'] = df['duration_ms']\/60000\ndf['duration_min'].describe()","65391798":"long_tracks = df.loc[df['duration_min']>20]\nshort_tracks = df.loc[df['duration_min']<=20]","9413a5bd":"fig, ax = plt.subplots(figsize = (15, 4))\nax = sns.distplot(short_tracks['duration_min'], kde = False)\nax.set_title(f'Short Tracks (<=20 min): {short_tracks.shape[0]} tracks')\nax.set_xticks(range(0,20,1))\nax.set_xlim(0,20)\nplt.show()","3012bbb6":"fig, ax = plt.subplots(figsize = (15, 5))\nax = sns.distplot(long_tracks['duration_min'], kde=False, bins=60)\nax.set_title(f'Long Tracks (>20 min): {long_tracks.shape[0]} tracks')\nplt.show()","ed8699d5":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6), sharey=True)\n\n# Short Tracks\nax1_data =  short_tracks.groupby('duration_min')['popularity'].mean().to_frame().reset_index()\nax1 = sns.scatterplot(x='duration_min', y='popularity', data=ax1_data, color='blue', ax=ax1)\nax1.set_xticks(range(0,20,1))\nax1.set_xlim(0,20)\nax1.set_title('Short tracks')\n\n# Long Tracks\nax2_data =  long_tracks.groupby('duration_min')['popularity'].mean().to_frame().reset_index()\nax2 = sns.scatterplot(x=ax2_data['duration_min'], y=ax2_data['popularity'], color='green', ax=ax2)\nax2.set_xticks(range(20,101,10))\nax2.set_xlim(20,100)\nax2.set_title('Long Tracks')\nfont = {'family': 'serif',\n        'color':  'red',\n        'weight': 'normal',\n        'size': 16,\n        }\nax1.set_ylabel('Mean Popularity', fontsize=12)\nplt.tight_layout()\nplt.show()","1b0cd7dd":"a= df['energy'].corr(df['popularity']).round(3)\nprint(f'The pearson corr is -->{a:^10}')","4cd6bbfc":"df['energy'].describe()","2c829156":"sns.distplot(df['energy'], kde = False, bins=30)\nplt.show()","f5a8238e":"fig, ax = plt.subplots(1, figsize=(15, 6), sharey=True, sharex = True)\nax_data =  df.groupby('energy')['popularity'].mean().to_frame().reset_index()\nax = sns.scatterplot(x='energy', y='popularity', data=ax_data, color='blue', ax=ax)\nax.set_title('energy')\nax.set_ylabel('Mean Popularity', fontsize=12)\nplt.tight_layout()\nplt.show()","9bad83c6":"df_test=df[['energy','popularity']]\nggplot(df_test, aes(x='energy', y='popularity')) + geom_bin2d(bins = 20)","2f785715":"a= df['instrumentalness'].corr(df['popularity']).round(3)\nprint(f'The pearson corr is -->{a:^10}')","6c008db4":"fig, ax = plt.subplots(figsize = (15,3))\nax = sns.distplot(df['instrumentalness'], kde = False, bins=30)\nplt.show()","2caac314":"fig, ax = plt.subplots(1, figsize=(15, 6), sharey=True, sharex = True)\nax_data =  df.groupby('instrumentalness')['popularity'].mean().to_frame().reset_index()\nax = sns.scatterplot(x='instrumentalness', y='popularity', data=ax_data, color='blue', ax=ax)\nax.set_title('instrumentalness')\nax.set_ylabel('Mean Popularity', fontsize=12)\nplt.tight_layout()\nplt.show()","e62e1aed":"criteria= [df['instrumentalness'].between(0, 0.1),\n           df['instrumentalness'].between(0.1000001, 0.95),\n           df['instrumentalness'].between(0.950001, 1)]\nvalues = [1, 2, 3]\ndf['instrumentalness_criteria'] = np.select(criteria, values, 0)\nfig, ax = plt.subplots()\nsns.distplot(df['instrumentalness_criteria'],ax=ax, kde=False, bins=3 )\nax.set_xticks(range(1,4))\nplt.show()","28908c1e":"a= df['liveness'].corr(df['popularity']).round(3)\nprint(f'The pearson corr is -->{a:^10}')","ff4c72e1":"fig, ax = plt.subplots(figsize=(15, 4))\nsns.distplot(df['liveness'], kde = False, bins=30)\nplt.show()","b2c2b768":"fig, ax = plt.subplots(1, figsize=(15, 6), sharey=True, sharex = True)\nax_data =  df.groupby('liveness')['popularity'].mean().to_frame().reset_index()\nax = sns.scatterplot(x='liveness', y='popularity', data=ax_data, color='blue', ax=ax)\nax.set_title('liveness')\nax.set_ylabel('Mean Popularity', fontsize=12)\nplt.tight_layout()\nplt.show()","f0e0dbca":"df['loudness'].corr(df['popularity'])","ad404350":"df['loudness'].describe()","1656e6af":"fig, ax = plt.subplots(figsize = (15, 6))\nsns.scatterplot(x='loudness', y='popularity', data=df, color='blue', alpha=0.3)\nplt.show()","78fd969a":"# evaluate the amount of tracks in different ranges of loudness\nfor i in range(5,-60,-5):\n  print(f\"There are {df.loc[df['loudness'] < i, 'loudness'].count()} tracks below {i}\")","92e889c5":"# Describe - all\ndf['popularity'].describe()","d29f6755":"# Describe - Popularity > 0\ndf.loc[df['popularity']>0,'popularity'].describe()","c253c0c0":"fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 11))\nax1 = sns.distplot(df['popularity'], kde=False, ax=ax1)\nax2 = sns.distplot(df.loc[df['popularity']>0,'popularity'], ax=ax2, kde=False)\nax1.set_xlim(0,100)\nax2.set_xlim(0,100)\nax1.set_xlabel('')\nax1.set_title('Up: All Data, Bottom: Popularity > 0', c='r', weight='bold')\nplt.show()","9aff82f6":"fig, ax = plt.subplots(figsize=(15, 4))\nax = df.groupby('year')['popularity'].mean().plot()\nax.set_title('Mean Popularity over the years', c='r', weight='bold')\nax.set_ylabel('Mean Popularity', weight='bold')\nax.set_xlabel('Year', weight='bold')\nax.set_xticks(range(1920, 2021, 5))\nplt.show()","61596488":"a= df['speechiness'].corr(df['popularity']).round(2)\nprint(f'The pearson corr is -->{a:^10}')","d4dde050":"fig, ax = plt.subplots(figsize=(15, 4))\nsns.distplot(df['speechiness'], kde = False, bins=30)\nplt.show()","a7d0d586":"fig, ax = plt.subplots(1, figsize=(15, 6), sharey=True, sharex = True)\nax_data =  df.groupby('speechiness')['popularity'].mean().to_frame().reset_index()\nax = sns.scatterplot(x='speechiness', y='popularity', data=ax_data, color='blue', ax=ax)\nax.axvline(x=0.62,ymin=0,ymax=1,color='red',linestyle='dashed')\nfont = {'family': 'serif',\n        'color':  'red',\n        'weight': 'normal',\n        'size': 16,\n        }\nax.text(x = 0.63, y = 55, s='Split Value-0.62',fontdict=font)\nax.set_title('speechiness')\nax.set_ylabel('Mean Popularity', fontsize=12)\nplt.tight_layout()\nplt.show()","5313818d":"import pydotplus\n\nX_sp = df['speechiness'].to_frame()\ny_sp = df['popularity']\nX_train, X_test, y_train, y_test = split(X_sp, y_sp , test_size = 0.2 , random_state = 42 )\nmodel = DecisionTreeRegressor(max_leaf_nodes=2)\nmodel.fit(X_train, y_train)\n\ndot_data = StringIO()\nexport_graphviz(model, out_file=dot_data)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(graph.create_png())","8288e9b4":"y_pred = model.predict(X_train)\nrmse = np.sqrt(mse(y_train, y_pred))\nprint(f\"RMSE_Train = {rmse:.2f}\")","15c99754":"y_test_pred = model.predict(X_test)\nrmse = np.sqrt(mse(y_test, y_test_pred))\nprint(f\"RMSE_test = {rmse:.2f}\")","d08f1196":"a = df['tempo'].corr(df['popularity']).round(2)\nprint(f\"Linear correlation is {a}\")","20536b63":"sns.jointplot(x='tempo', y='popularity', data=df)\nplt.show()","ad8c324e":"fig, ax = plt.subplots(figsize = (15, 6))\nax = sns.distplot(df['tempo'], bins=250, kde=False)\nax.text(s='145\\nOutliers?', x= 5, y=700, fontdict={'size': 12, 'c': 'darkred'})\nax.text(s='All values except 0', x= 125, y=2750, fontdict={'size': 12, 'c': 'darkred'})\nax.text(s='Corrected\\nmedian\\n114.87', x= 116, y=300, fontdict={'size': 10, 'c': 'darkgreen', 'weight': 'bold'})\nax.axvline(x=114.87, ymin=0, ymax=0.7, color='green', linestyle='dashed', linewidth=2)\nax.axvline(x=30.95, ymin=0, ymax=1, color='orange', linestyle='dashed', linewidth=3)\nax.axvline(x=244.09, ymin=0, ymax=1, color='orange', linestyle='dashed', linewidth=3)\nax.annotate(\"\", xy=(31, 2700), xytext=(244, 2700), arrowprops=dict(arrowstyle=\"<->\", color='r', linestyle='dashed', linewidth=2))\nax.annotate(\"\", xy=(0, 300), xytext=(0, 900), arrowprops=dict(arrowstyle=\"->\", color='r', linestyle='dashed', linewidth=3))\nplt.show()","304e46cf":"a = df['valence'].corr(df['popularity']).round(2)\nprint(f\"Linear correlation is {a}\")","a12060ed":"sns.jointplot(x='valence', y='popularity', data=df)\nplt.show()","0089309f":"fig, ax = plt.subplots(figsize = (15, 6))\nsns.scatterplot(x='valence', y='popularity', data=df, color='blue', alpha=0.3)\nplt.show()","a7da715a":"a = df['year'].corr(df['popularity']).round(2)\nprint(f\"Linear correlation is {a}\")","18e88d66":"fig, ax = plt.subplots(figsize=(17, 4))\nax = sns.distplot(df['year'], bins = 100, kde = False)\nax.set_xlim(1920,2020)\nax.set_xticks(range(1920, 2021, 10))\nax.set_title('# of tracks per Year')\nplt.show()","c1fbba31":"fig, ax = plt.subplots(figsize=(12, 4))\nax = df['year'].loc[df['popularity']==0].plot.hist(bins=50)\nax.set_title('0 Popularity by Year')\nax.text(s='Outliers?', x= 2015, y=1000, fontdict={'size': 12, 'c': 'darkred'})\nax.annotate(\"\", xy=(2019, 300), xytext=(2019, 900), arrowprops=dict(arrowstyle=\"->\", color='r', linestyle='dashed', linewidth=3))\nplt.show()","f3f60bbe":"# Read column names from file\ncols = list(pd.read_csv(path, nrows =1))\ndf = pd.read_csv(path, usecols=[i for i in cols if i not in ['id','name','release_date','year']])\n\n# Remove duplicated\ndf = df[~df.duplicated()==1]\n# df = df.sample(frac=0.3)\n#Split the data to train and test\nX_train, X_test, y_train, y_test = split(df.drop('popularity', axis=1), df['popularity'], test_size = 0.2, random_state = 12345)","8f15898b":"class ArtistsTransformer():\n  \"\"\" This transformer recives a DF with a feature 'artists' of dtype object\n      and convert the feature to a float value as follows:\n      1. Replace the data with the artists mean popularity\n      2. Replace values where artists appear less than MinCnt with y.mean()\n      3. Replace values where artists appear more than MaxCnt with 0\n      \n      PARAMETERS:\n      ----------\n      MinCnt (int): Minimal treshold of artisits apear in dataset, default = 3\n      MaxCnt (int): Maximal treshold of artisits apear in dataset, default = 600\n\n      RERTURN:\n      ----------\n      A DataFrame with converted artists str feature to ordinal floats\n  \"\"\"\n\n  def __init__(self, MinCnt = 3.0, MaxCnt = 600.0):\n      self.MinCnt = MinCnt\n      self.MaxCnt = MaxCnt\n      self.artists_df = None\n  \n  def fit (self, X, y):\n      self.artists_df =  y.groupby(X.artists).agg(['mean', 'count'])\n      self.artists_df.loc['unknown'] = [y.mean(), 1]\n      self.artists_df.loc[self.artists_df['count'] <= self.MinCnt, 'mean'] = y.mean()\n      self.artists_df.loc[self.artists_df['count'] >= self.MaxCnt, 'mean'] = 0\n      return self\n\n  def transform(self, X, y=None):\n      X['artists'] = np.where(X['artists'].isin(self.artists_df.index), X['artists'], 'unknown')\n      X['artists'] = X['artists'].map(self.artists_df['mean'])\n      return X","22dbf5da":"# Apply AritistsTransformer on train and test seperatly\nartists_transformer = ArtistsTransformer(MinCnt=2)\nX_train = artists_transformer.fit(X_train, y_train).transform(X_train, y_train)\nX_test = artists_transformer.transform(X_test, y_test)","0b230500":"def instrumentalness_criteria(X):\n    X['instrumentalness'] = list(map((lambda x: 1 if x < 0.1 else (3 if x > 0.95 else 2)), X.instrumentalness))\n\ninstrumentalness_tranformer = FunctionTransformer(instrumentalness_criteria)\ninstrumentalness_tranformer.transform(X_train)\ninstrumentalness_tranformer.transform(X_test)","15905158":"class ReplaceZeroTransformer():\n    \"\"\"Eliminates Zero values from tempo columns and replace it \n       with the median or mean of non-zero values as specified.\n       defaut is set to 'median'.\n    \"\"\"\n\n    def __init__(self, method='median'):\n        self.method = method\n\n    def transform(self, X):\n        if self.method == 'median':\n            X.loc[X['tempo']==0, 'tempo'] = X.loc[X['tempo']>0, 'tempo'].median()\n        elif self.method == 'mean':\n            X.loc[X['tempo']==0, 'tempo'] = X.loc[X['tempo']>0, 'tempo'].mean()\n        else:\n            raise Exception(\"Method can be 'median' or 'mean' only!\")\n        return X","e5da48b9":"tempo_transformer = ReplaceZeroTransformer()\nX_train = tempo_transformer.transform(X_train)\nX_test = tempo_transformer.transform(X_test)","88650cd2":"ohe = OneHotEncoder(categories='auto', drop='first')\n\n# Train\nfeature_arr = ohe.fit_transform(X_train[['instrumentalness','key']]).toarray()\ncolumns_key = ['key_'+str(i) for i in list(set(X_train['key'].values))[1:]]\ninstrumentalness_key = ['ins_'+str(i) for i in list(set(X_train['instrumentalness'].values))[1:]]\nfeature_labels = columns_key + instrumentalness_key\nfeature_labels = np.concatenate((feature_labels), axis=None)\nfeatures = pd.DataFrame(feature_arr, columns = feature_labels, index = X_train.index)\nX_train = pd.concat([X_train, features], axis=1).drop(['key','instrumentalness'], axis=1)\n\n# Test\nfeature_arr = ohe.fit_transform(X_test[['instrumentalness','key']]).toarray()\ncolumns_key = ['key_'+str(i) for i in list(set(X_test['key'].values))[1:]]\ninstrumentalness_key = ['ins_'+str(i) for i in list(set(X_test['instrumentalness'].values))[1:]]\nfeature_labels = columns_key + instrumentalness_key\nfeature_labels = np.concatenate((feature_labels), axis=None)\nfeatures = pd.DataFrame(feature_arr, columns = feature_labels, index = X_test.index)\nX_test = pd.concat([X_test, features], axis=1).drop(['key','instrumentalness'], axis=1)","9770ceca":"scaler = MinMaxScaler()\ncols = ['artists','duration_ms','loudness','tempo']\nX_train[cols] = scaler.fit_transform(X_train[cols])\nX_test[cols] = scaler.fit_transform(X_test[cols])","873f679f":"# Divide the popularity by 100\ny_train = y_train \/ 100\ny_test = y_test \/ 100","7c701a15":"nl = '\\n'\nprint(f'X_train shape is: {X_train.shape} {nl}y_train shape is: {y_train.shape} {nl}X_test shape is: {X_test.shape} {nl}y_test shape is: {y_test.shape}')","5e140602":"X_train.head(3)","a0592d15":"X_train.describe().drop(['count','25%', '50%', '75%'])","c719f0ec":"y_train.describe().drop(['count','25%', '50%', '75%'])","f86eff7c":"LR = LinearRegression()\ncols = [col for col in X_train.columns if abs(X_train[col].corr(y_train))>0.2]\n\n# Fit the model and\nLR.fit(X_train.drop(columns=cols), y_train)\n\n# Train Predicting with the model\ny_train_pred = LR.predict(X_train.drop(columns=cols)).clip(0, 1)\n\n# RMSE Train\nLR_rmse = np.sqrt(mse(y_train, y_train_pred))\nprint(f\"RMSE Train = {LR_rmse:.5f}\")\n\n#Predicting with the model\ny_test_pred = LR.predict(X_test.drop(columns=cols)).clip(0, 1)\n\n# RMSE Test\nLR_rmse = np.sqrt(mse(y_test, y_test_pred))\nprint(f\"RMSE Test = {LR_rmse:.5f}\")","0f1ffdfc":"LR = LinearRegression()\n\n# Fit the model and\nLR.fit(X_train, y_train)\n\n# Train Predicting with the model\ny_train_pred = LR.predict(X_train).clip(0, 1)\n\n# RMSE Train\nLR_rmse = np.sqrt(mse(y_train, y_train_pred))\nprint(f\"RMSE Train = {LR_rmse:.6f}\")\n\n#Predicting with the model\ny_test_pred = LR.predict(X_test).clip(0, 1)\n\n# RMSE Test\nLR_rmse = np.sqrt(mse(y_test, y_test_pred))\nprint(f\"RMSE Test = {LR_rmse:.6f}\")","1a8bb85a":"from IPython.display import display, Math\n\ndef get_printable_equt(df, inter, coef):\n  equtation = '$y = ' + f'{inter:.2f} '\n  \n  for i, co in enumerate(coef):\n    if i == len(coef) - 1:\n        equtation = equtation + f'+ {co:.2f} {(df.columns)[i]}$'\n    else:\n        equtation = equtation + f'+ {co:.2f} {(df.columns)[i]}'\n\n  return display(Math(equtation))\n\nget_printable_equt(X_train, LR.intercept_, LR.coef_)","39e583d2":"fig, ax = plt.subplots(figsize=(6, 6))\nax = sns.scatterplot(x=y_test, y=y_test_pred)\nsns.lineplot(x=y_test, y=y_test, color='red', ax=ax)\nax.set_xlabel('Y_test')\nax.set_ylabel('Y_test_pred')\nax.set_title('y_test vs. y_test_pred', fontsize=14, color='red')\nplt.show()","a92a5499":"import statsmodels.api as sm\nmodel = sm.OLS(y_train, X_train).fit()\nmodel.summary()","84a6dd3a":"RMSE1_train, RMSE1_test = [], []\n  \nfor i in range(5,101,5):\n  knn = KNeighborsRegressor(n_neighbors=i)\n  knn.fit(X_train,y_train)\n  y_train_pred = knn.predict(X_train)\n  knn_train_rmse = np.sqrt(mse(y_train, y_train_pred))\n  RMSE1_train.append(knn_train_rmse.round(3))\n  y_test_pred = knn.predict(X_test)\n  knn_test_rmse = np.sqrt(mse(y_test, y_test_pred))\n  RMSE1_test.append(knn_test_rmse.round(3))","590c20d0":"fig, ax = plt.subplots(figsize=(15,4))\nx = np.arange(5, 101, 5)\nax = sns.lineplot(x=x, y=RMSE1_train)\nsns.lineplot(x=x, y=RMSE1_test, ax=ax)\nax.legend(labels=['RMSE_train', 'RMSE_test'], bbox_to_anchor=(1, 1))\nax.set_xlabel('Neighbors')\nax.set_ylabel('RMSE')\nax.set_xticks(np.arange(5,101,5))\nax.set_title('KNN with 10 <= n-neighbors < 100', c='r', fontdict={'c':'r', 'fontsize':14, 'weight':'bold'})\nplt.show()","b7bf1935":"gap1 = [RMSE1_test[num]-RMSE1_train[num] for num, i in enumerate(RMSE1_train)]\nprint(f'RMSE Train: {RMSE1_train[gap1.index(min(gap1))]}, RMSE_test: {RMSE1_test[gap1.index(min(gap1))]}')","36db7ec2":"RMSE_train, RMSE_test = [], []\n  \nfor i in range(100,201,5):\n  knn = KNeighborsRegressor(n_neighbors=i)\n  knn.fit(X_train,y_train)\n  y_train_pred = knn.predict(X_train)\n  knn_train_rmse = np.sqrt(mse(y_train, y_train_pred))\n  RMSE_train.append(knn_train_rmse.round(3))\n  y_test_pred = knn.predict(X_test)\n  knn_test_rmse = np.sqrt(mse(y_test, y_test_pred))\n  RMSE_test.append(knn_test_rmse.round(3))","b23232bb":"fig, ax = plt.subplots(figsize=(15,4))\nx = np.arange(100, 201, 5)\nax = sns.lineplot(x=x, y=RMSE_train)\nsns.lineplot(x=x, y=RMSE_test, ax=ax)\n# ax.axvline(x=180, ymin=0, ymax=0.3, color='blue')\n# ax.axvline(x=139, ymin=0.5, ymax=0.8, color='orange')\nax.legend(labels=['RMSE_train', 'RMSE_test'], bbox_to_anchor=(1, 1))\nax.set_xticks(np.arange(100,201,5))\nax.set_xlabel('K neighbours')\nax.set_ylabel('RMSE')\nax.set_title('KNN with 100 <= K < 201', c='r', fontdict={'c':'r', 'fontsize':14, 'weight':'bold'})\nplt.show()","857668da":"gap2 = [RMSE_test[num]-RMSE_train[num] for num, i in enumerate(RMSE_train)]\nprint(f'RMSE Train: {RMSE_train[gap2.index(min(gap2))]}, RMSE_test: {RMSE_test[gap2.index(min(gap2))]}')","2f1e06e2":"# single run\ntree = DecisionTreeRegressor(max_leaf_nodes=41, min_samples_split=2000)\ntree.fit(X_train, y_train)\ny_train_pred = tree.predict(X_train).clip(0, 1)\ntrain_rmse = np.sqrt(mse(y_train, y_train_pred))\ny_test_pred = tree.predict(X_test).clip(0, 1)\ntest_rmse = np.sqrt(mse(y_test, y_test_pred))\nprint(f'train: {train_rmse}')\nprint(f'test: {test_rmse}')","3e311e63":"RMSE3_train, RMSE3_test = [], []\n\nfor i in range(2,200):\n  tree = DecisionTreeRegressor(random_state = 15, max_leaf_nodes=i)\n  tree.fit(X_train, y_train)\n  y_train_pred = tree.predict(X_train).clip(0, 1)\n  train_rmse = np.sqrt(mse(y_train, y_train_pred))\n  RMSE3_train.append(train_rmse.round(3))\n  y_test_pred = tree.predict(X_test).clip(0, 1)\n  test_rmse = np.sqrt(mse(y_test, y_test_pred))\n  RMSE3_test.append(test_rmse.round(3))","6f774610":"fig, ax = plt.subplots(figsize=(15,4))\nx = np.arange(2, 200)\nax = sns.lineplot(x=x, y=RMSE3_train)\nsns.lineplot(x=x, y=RMSE3_test, ax=ax)\nax.axvline(x=178, ymin=0, ymax=0.3, color='blue')\n# ax.axvline(x=139, ymin=0.5, ymax=0.8, color='orange')\nax.legend(labels=['RMSE_train', 'RMSE_test'], bbox_to_anchor=(1, 1))\nax.set_xlabel('Max Leaf Nodes')\nax.set_ylabel('RMSE')\nax.set_title('DT with 2<= max_leaf_nodes < 200', c='r', fontdict={'c':'r', 'fontsize':14, 'weight':'bold'})\nplt.show()","27425c6e":"ax = sns.scatterplot(x=y_train, y=y_train_pred)\nax.plot(y_train, y_train, 'r')\nax.set_ylabel('popularity prediction')\nplt.show()","60e41393":"gap3 = [(RMSE3_train[i]\/RMSE3_test[i]) for i, num in enumerate(RMSE3_train)]\n\nprint(f'Minimum validiation is: {min(gap3):.3%} in index number {gap3.index(min(gap3))}')\nprint(f'Maximum validiation is: {max(gap3):.3%} in index number {gap3.index(max(gap3))}')","7e2ad723":"print('Feature importances:')\nfor i, col in enumerate(X_train.columns):\n  print(f'{col:12}: {tree.feature_importances_[i]:.3f}')","04277a07":"dot_data = StringIO()\nexport_graphviz(tree, out_file=dot_data,feature_names=X_train.columns)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())\nImage(graph.create_png())","1a526868":"There are 33,375 different artists in this feature.\n\nSince we believe there should be a correlation between the artist and the popularity of the track, we would like to use the **Target Encoding** approach, and replace the artist with some derivative of its popularity.\n\nFirst, we check distribution of number of times artists appear in the data.\n\nIt is clear that most artists apear less than 50 times","91919376":"# **Pre-Processing**\n\nFirst, we import a clean version of the data, without id, name, year and release_date and remove the duplicates.","b728c15c":"### **Mode**\n\n* Mode is a binary feature.\n\n* There is no distinguish linear connection between mode and popularity (corr = -0.033)","f9c11b44":"# **Cleaning**","18d832b4":"### **Duration_ms**\n\n* Conveting the miliseconds to minutes, so the column is more intuitive\n\n* mean (3.85 min) and median (3.47 min) are close\n\n* Some tracks are unexpectedly long... to understand the data it was divided to long tracks and short tracks:\n\n    * Long tracks are mainly classic music and atmosphere music\n\n    * plottibg it vs. mean popularity, it seems as a natural extension\n\n    * There are not enoughe samples of tracks longer than 45 minutes","71835384":"Lets zoom in to the left of the scale, to decide what is the cut-off for using artist mean OR overall data mean:","abd80b6a":"### **Energy**\n\n>mesure the intensity and activity, energetic track feels faster, louder and noisier.\n\n>It seem tat there is a releativly strong linear correlation of 0.497 correlation to the target.","4888b218":"## **Features Selection**\n\nLooking closely at the features, there are some features to consider whether to include in the final model or not:\n\n* **id:** id is unique for each track, therfore cannot assist a model and will be dropped.\n* **name:** There are 132,940 unique values. In our opinion, this is a problematic categorical feature to insert in a model, and will be dropped.\n* **artists:** there are 33,375 unique features. Since we enticipate the artist will influence the popularity of the track, this feature will remain and be handled seperatley.\n* **release_date \\ year:** The year feature is a derivative to the release_date feature, therfore one of them should be excluded. Since the release date contains rows with full date and others with year only, the release_date will be dropped.  ","e4313e72":"This feature will need an imputer and scaler.","0d250076":"## **Duplicates & Nulls**\n\nPrior to the feature dropped above, there were not any duplicates in the data set, due to the unique id feature.\n\nAfter the a.m. features removal, there are 669 duplicates in the dataset.\n\nOnly the first copy of the duplicate will remain for the model.\n\nThere are not null values in the data set. ","7a23ed90":"**About The Data**\n\n1. The dataset was uploaded to Kaggle in 06\/2020 by *Yama\u00e7 Eren Ay*, who used the Spotify Web API for develepors to build a data that contains more than 160,000 songs. These are NOT all the spotify tracks, but quite alot of it...\n\n2. The task in this dataset is to predict the popularity of the track according to the track's features. \n\n3. Each row in the data represent a unique track, identified by a unique ID feature generated by Spotify.\n\n4. The Features of the tracks are: ","a266b1a1":"## **Overview**","c8922f11":"### **Danceability**\n\n>Danceability- measure if the song is dancable based on the combination of tempo rythm and beat strength\n\n>Danceability seems to have a normal distribution between 0-1\n\n>the majoirity of Danceability values is between 0.2-0.8 with mean 0.53","41838735":"### **Option 2: All features**","d87995f6":"### **Instrumentalness Transformer**","cd581bd1":"# **Models**\n\n## **Linear Regression**\n\n### **Option 1: selected features only (corr > 0.2)**","6bf3ac0c":"## **Categorical Features**\n---","1455b1a6":"### **liveness**\n\n>Detects the presence of an audience. high livness suggests of the track was live","8ad62892":"### **Verification**","2ec3739d":"### **MinMaxScaler Encoder**","e4d89355":"### **100 to 200 neighbors**","517e4bd0":"## **Numeric Features**\n---","39496bed":"### **Valence**","30ef076f":"## **KNN Model**\n\nDue to long running time, we sampled the original data with frac = 0.3.\n\n### **5 to 100 neighbors**","b6476866":"### **AritistsTransformer**\n\nHere we need to replace object data of the artists with some numerical indicator that identify the artist.","2440d852":"### **Speechiness**\n\n>Due to an obvious change in in speechiness trand vs. poopularity, we decided to divide the records into two sub-groups.\nThe cutoff point is based on a decision tree model.","8a00f3b3":"### **Target scaling**","0f07908b":"# **A First Glance...**","5e20d07a":"# **EDA**\n---","8872ad0d":"We also check for correlation between the number of appearances in data and the mean of popularity.\n\nIt is clear that artists that appear in the data more than 600 times, are getting almost 0 popularity.\n\nthere are 4 artists [Francisco Canaro, Ignacio Corsini, \u042d\u0440\u043d\u0435\u0441\u0442 \u0425\u0435\u043c\u0438\u043d\u0433\u0443\u044d\u0439, \u042d\u0440\u0438\u0445 \u041c\u0430\u0440\u0438\u044f \u0420\u0435\u043c\u0430\u0440\u043a] responsible for 3,546 tracks!","390e9e89":"* There is a small sampled data for tracks longer than 45 min. \n\n* Transformer \\ pre-processing: sklearn.preprocessing.MinMaxScaler()","3010a352":"### **Acousticness**\n\n> The acousticness of the majority of tracks is either close to 0 or 1\n\n> The more confidence we have that a track is acoustic, the less is its mean popularity (apply for acousitcness > 0.1)","d8fd140a":"### **Instrumentalness**\n\n> The closer the Instrumentalness to 1 - the greater liklyhood the track contains NO vocals\n\n> Most of the data is distributed in both sides of scope.\n\n> We decide to divide the feature into three categorical groups and create a onehotencoder transformer to reshpae our data","45cd3042":"### **Option 2: loop**","5cfaa79a":"**Processed by: Guy Kahana & Anat Peled**\n\nThis project is the 3rd assignment required in Data Science Course @Naya College and submitted to the instructor Dror Geva.\nThe project main purpose was to explore and practice the ML regression processes","8f93e75c":"In this notebook we explored the data, dropped unnecessary features, and tested several regression models: Linear Regression, KNN & DT.\n\nWe also tested the data with different test size, in order to check the influence on the results.\n\nEventually, the best model was a Decision Tree run with test size = 0.2, max_leaf_nodes = 178, mincnt = 2.\n\nThis gave a RMSE train of 0.102 and RMSE test of 0.112.\n\nA pdf version of the summary presentation is available here: [GitHub](https:\/\/github.com\/pelanat1207\/Spotify-Project)","b1565a77":"Therefore to handle this feature we will use a transformer that transforms the 'artists' feature from object\n       to a float value as follows:\n       \n1. Replace the data with the artists mean popularity\n\n2. Replace values where artists appear less than minimal times with popularity mean\n\n3. Replace values where artists appear more than maximaum times with 0","8fd655ed":"### **Artists**","0ec333f7":"After exploring the data, some features need pre-processing. This will be done in the following order:\n\n1. Unique feature transformers\n\n2. OneHotEncoder (aka dummies) for relevant features\n\n3. MinMaxScaling for relevant features\n\n4. Target scaling","ae952284":"### **Loudness**","f265d40f":"# **Conclusion**","f26042a2":"### **Tempo Transformer**","80ffa8e1":"### **Popularity (Target)**","6c22db59":"### **Year**\n\n* The year attribute is linearly close to the popularity target (corr = 0.88)\n\n* Each year contains different quantities of tracks. Most of the years are up to 2000, probably due to the 2000 maximal batch limit in the Spoyify's API.\n\n* There are some values with 0 popularity in 2020. These is a relatively new songs in the dataset, from 6\/2020, prior to data extraction.","c1ea5fef":"#### **Decision Tree**","7e8f3df4":"## **Decision Tree Model**\n\n### **Option 1: single run**","df6fbe2f":"### **Key**\n\n* The 11-values of keys distribute between 4-13% each \n\n* Keys popularity' std is aprox. 20, makes it difficult to establish any correlation.\n\n* Key 3 has the lowest mean (of 24.67) ","21be61a4":"### **Explicit**\n\nExplicit content usually gets The Parental Advisory label (abbreviated PAL) which is a warning label. \n\nIt is placed on audio recordings in recognition of profanity or inappropriate references, with the intention of alerting parents of material potentially unsuitable for children. \n\nThe label has been included on digital listings offered by online music stores (as Spotify).\n\n* Linear correlation to popularity is 0.2134","1c9854d4":"### **OneHotEncoder**\n\nWe use OneHotEncoder from SKlearn to create dummies","6dfd73bb":"### **Tempo**\n\n* The linear correlation between tempo and popularity is 0.1335.\n\n* Outliers - there is a 145 values of 0. need to be replaced. ","046f70f9":"**Due to the very high linear correlation between the year and the popularity, this feature will be excluded from our models.**"}}