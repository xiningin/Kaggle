{"cell_type":{"647307d0":"code","05b5128a":"code","6e4a8c38":"code","370118f8":"code","76784a28":"code","fedebee6":"code","bcfa5ae8":"code","f7f51784":"code","4a5426ed":"code","3d5e3d8e":"code","aba7cf2d":"code","9b998755":"code","9320e151":"code","521bf7c2":"code","6c077c84":"code","36b4952f":"code","4969bee6":"code","69e45aa7":"code","60e78409":"code","fb997510":"code","847b8fba":"code","76e0108e":"code","77d5f195":"code","0ad16aee":"code","a0cf1786":"code","8428523e":"code","f4bba8da":"code","fc309378":"code","d02f843d":"code","ee2166b3":"code","a796e03a":"code","4b77e676":"code","d001b996":"code","a185bc73":"code","7a219f3c":"code","11848e98":"code","80ce8e97":"code","3538e608":"code","e8fe2ac4":"code","1f58dd9a":"code","54110a32":"code","242eebbe":"code","834e40b5":"code","78c93320":"code","b82bf2c7":"code","6dab531e":"code","c89cb5a5":"code","b8405b38":"markdown","a3762cd1":"markdown","37691557":"markdown","7c0b9c20":"markdown"},"source":{"647307d0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport operator\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n# import warnings\nimport warnings\n# ignore warnings\nwarnings.filterwarnings(\"ignore\")\nfrom subprocess import check_output\nfrom IPython.display import Markdown as md\nfrom prettytable import PrettyTable\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom numpy import sqrt\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","05b5128a":"data = pd.read_csv('..\/input\/vertebralcolumndataset\/column_2C.csv')","6e4a8c38":"# to see features and target variable\ndata.head()","370118f8":"# Well know question is is there any NaN value and length of this data so lets look at info\ndata.info()","76784a28":"data.describe()","fedebee6":"color_list = ['red' if i=='Abnormal' else 'green' for i in data.loc[:,'class']]\npd.plotting.scatter_matrix(data.loc[:, data.columns != 'class'],\n                                       c=color_list,\n                                       figsize= [15,15],\n                                       diagonal='hist',\n                                       alpha=0.5,\n                                       s = 200,\n                                       marker = '*',\n                                       edgecolor= \"black\")\nplt.show()","bcfa5ae8":"sns.countplot(x=\"class\", data=data)\ndata.loc[:,'class'].value_counts()","f7f51784":"# create data1 that includes pelvic_incidence that is feature and sacral_slope that is target variable\ndata1 = data[data['class'] =='Abnormal']\nx = np.array(data1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(data1.loc[:,'sacral_slope']).reshape(-1,1)\n# Scatter\nplt.figure(figsize=[10,10])\nplt.scatter(x=x,y=y)\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()","4a5426ed":"# LinearRegression\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\npredict_space = np.linspace(min(x), max(x)).reshape(-1,1)\n# Fit\nreg.fit(x,y)\n# Predict\npredicted = reg.predict(predict_space)\n# # R^2 \n# print('R^2 score: ',reg.score(x, y))\ndegree = 1\n\nx = np.array(data1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(data1.loc[:,'sacral_slope']).reshape(-1,1)\npolynomial_features= PolynomialFeatures(degree=degree)\nx_poly = polynomial_features.fit_transform(x)\n\nmodel = LinearRegression()\nmodel.fit(x_poly, y)\ny_poly_pred = model.predict(x_poly)\n\nrmse = sqrt(mean_squared_error(y,y_poly_pred))\nr2 = r2_score(y,y_poly_pred)\n\n\n# tabel parameter\ntabel_parameter = PrettyTable(['parameter', 'nilai'])\ntabel_parameter.add_row(['Polynomial\\nDegree', degree])\ntabel_parameter.add_row(['RMSE','{:.10}'.format(rmse)])\ntabel_parameter.add_row(['R^2', '{:.10}'.format(r2)])\n\n# Plot regression line and scatter\nplt.plot(predict_space, predicted, color='black', linewidth=3)\nplt.scatter(x=x,y=y)\nplt.title('Biomechanical features of orthopedic patients\\n')\nplt.xlabel('pelvic_incidence')\nplt.ylabel('sacral_slope')\nplt.show()\nprint(tabel_parameter)","3d5e3d8e":"def display_equation(model):\n    for coef in model.coef_:\n        pass\n    streq = \"$y = \" +str(model.intercept_[0])\n    for i,c in enumerate(coef):\n        j = len(coef)-i-1\n        if abs(c) > c:\n            sign = \"-\"\n        else:\n            sign = \"+\"\n        if i > 1:\n            streq += sign +str(abs(c)) + \" \\cdot x^{\"+str(i)+\"}\"\n        elif i == 1:\n            streq += sign +str(abs(c)) + \" \\cdot x\"\n\n    streq =   streq + \"$\"\n    return md(streq)\ndisplay_equation(model)","aba7cf2d":"#Polynomial\ndegree = 10\n\nx = np.array(data1.loc[:,'pelvic_incidence']).reshape(-1,1)\ny = np.array(data1.loc[:,'sacral_slope']).reshape(-1,1)\npolynomial_features= PolynomialFeatures(degree=degree)\nx_poly = polynomial_features.fit_transform(x)\n\nmodel = LinearRegression()\nmodel.fit(x_poly, y)\ny_poly_pred = model.predict(x_poly)\n\nrmse = sqrt(mean_squared_error(y,y_poly_pred))\nr2 = r2_score(y,y_poly_pred)","9b998755":"# tabel parameter\ntabel_parameter = PrettyTable(['parameter', 'nilai'])\ntabel_parameter.add_row(['Polynomial\\nDegree', degree])\ntabel_parameter.add_row(['RMSE','{:.10}'.format(rmse)])\ntabel_parameter.add_row(['R^2', '{:.10}'.format(r2)])","9320e151":"plt.scatter(x, y, s=10)\nsort_axis = operator.itemgetter(0)\nsorted_zip = sorted(zip(x,y_poly_pred), key=sort_axis)\nx, y_poly_pred = zip(*sorted_zip)\nplt.title('Biomechanical features of orthopedic patients\\n')\nplt.xlabel('\\npelvic_incidence')\nplt.ylabel('sacral_slope')\n\nplt.plot(x, y_poly_pred, color='m')\nplt.show()\nprint(tabel_parameter)","521bf7c2":"def display_equation(model):\n    for coef in model.coef_:\n        pass\n    streq = \"$y = \" +str(model.intercept_[0])\n    for i,c in enumerate(coef):\n        j = len(coef)-i-1\n        if abs(c) > c:\n            sign = \"-\"\n        else:\n            sign = \"+\"\n        if i > 1:\n            streq += sign +str(abs(c)) + \" \\cdot x^{\"+str(i)+\"}\"\n        elif i == 1:\n            streq += sign +str(abs(c)) + \" \\cdot x\"\n\n    streq =   streq + \"$\"\n    return md(streq)\ndisplay_equation(model)","6c077c84":"# Load data\ndata = pd.read_csv('..\/input\/vertebralcolumndataset\/column_2C.csv')\n# get_dummies\ndf = pd.get_dummies(data)\ndf.head(10)","36b4952f":"# drop one of the feature\ndf.drop(\"class_Normal\",axis = 1, inplace = True) \ndf.head(10)\n# instead of two steps we can make it with one step pd.get_dummies(data,drop_first = True)","4969bee6":"df.info()","69e45aa7":"df.head()","60e78409":"X = df\n\ny = df['class_Abnormal']","fb997510":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\n\nX['class_Abnormal'] = le.fit_transform(X['class_Abnormal'])\n\ny = le.transform(y)","847b8fba":"X.info()","76e0108e":"X.head()","77d5f195":"cols = X.columns","0ad16aee":"from sklearn.preprocessing import MinMaxScaler\n\nms = MinMaxScaler()\n\nX = ms.fit_transform(X)","a0cf1786":"X = pd.DataFrame(X, columns=[cols])","8428523e":"X.head()","f4bba8da":"from sklearn.cluster import KMeans\nfrom sklearn.datasets import make_blobs\n\nfrom yellowbrick.cluster import KElbowVisualizer\n\n# Generate synthetic dataset with 3 random clusters\nX, y = make_blobs(n_samples=310, n_features=10, centers=3, random_state=0)\n\n# Instantiate the clustering model and visualizer\nmodel = KMeans()\nvisualizer = KElbowVisualizer(model, k=(1,11))\n\nvisualizer.fit(X)        # Fit the data to the visualizer\nvisualizer.show()        # Finalize and render the figure","fc309378":"# As you can see there is no labels in data\nplt.scatter(data['pelvic_radius'],data['degree_spondylolisthesis'])\nplt.xlabel('pelvic_radius')\nplt.ylabel('degree_spondylolisthesis')\nplt.show()","d02f843d":"#K = 2\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=2,random_state=0)\n\nkmeans.fit(X)\n\nlabels = kmeans.labels_\n\n# check how many of the samples were correctly labeled\n\ncorrect_labels = sum(y == labels)\n\nprint(\"Result: %d out of %d samples were correctly labeled.\" % (correct_labels, y.size))\n\nprint('Accuracy score: {0:0.2f}'. format(correct_labels\/float(y.size)))","ee2166b3":"# k = 2 (sembarang)\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=2, random_state=0) \n\nkmeans.fit(X)","a796e03a":"kmeans.cluster_centers_","4b77e676":"kmeans.inertia_","d001b996":"#K = 3\nkmeans = KMeans(n_clusters=3, random_state=0)\n\nkmeans.fit(X)\n\n# check how many of the samples were correctly labeled\nlabels = kmeans.labels_\n\ncorrect_labels = sum(y == labels)\nprint(\"Result: %d out of %d samples were correctly labeled.\" % (correct_labels, y.size))\nprint('Accuracy score: {0:0.2f}'. format(correct_labels\/float(y.size)))","a185bc73":"#k = 3 (OPTIMAL)\nfrom sklearn.cluster import KMeans\n\nkmeans = KMeans(n_clusters=3, random_state=0) \n\nkmeans.fit(X)","7a219f3c":"kmeans.cluster_centers_","11848e98":"kmeans.inertia_","80ce8e97":"#k = 2\nfrom sklearn.datasets.samples_generator import make_blobs\nX, y_true = make_blobs(n_samples=300, centers=2,cluster_std=0.60, shuffle=True, random_state=0)\nplt.scatter(X[:, 0], X[:, 1], edgecolor='blue', s=50);","3538e608":"from sklearn.cluster import KMeans\n\nkm = KMeans(\n    n_clusters=2, init='random',\n    n_init=10, max_iter=300, \n    tol=1e-04, random_state=0\n)\ny_km = km.fit_predict(X)","e8fe2ac4":"# plot the 2 clusters\nplt.scatter(\n    X[y_km == 0, 0], X[y_km == 0, 1],\n    s=50, c='lightgreen',\n    marker='s', edgecolor='black',\n    label='cluster 1'\n)\n\nplt.scatter(\n    X[y_km == 1, 0], X[y_km == 1, 1],\n    s=50, c='orange',\n    marker='o', edgecolor='black',\n    label='cluster 2'\n)\n\n# plot the centroids\nplt.scatter(\n    km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n    s=250, marker='*',\n    c='red', edgecolor='black',\n    label='centroids'\n)\nplt.legend(scatterpoints=1)\nplt.grid()\nplt.show()","1f58dd9a":"#K = 3\nfrom sklearn.datasets.samples_generator import make_blobs\nX, y_true = make_blobs(n_samples=300, centers=3,cluster_std=0.60, shuffle=True, random_state=0)\nplt.scatter(X[:, 0], X[:, 1], edgecolor='blue', s=50);","54110a32":"from sklearn.cluster import KMeans\n\nkm = KMeans(\n    n_clusters=3, init='random',\n    n_init=10, max_iter=300, \n    tol=1e-04, random_state=0\n)\ny_km = km.fit_predict(X)","242eebbe":"# plot the 3 clusters\nplt.scatter(\n    X[y_km == 0, 0], X[y_km == 0, 1],\n    s=50, c='lightgreen',\n    marker='s', edgecolor='black',\n    label='cluster 1'\n)\n\nplt.scatter(\n    X[y_km == 1, 0], X[y_km == 1, 1],\n    s=50, c='orange',\n    marker='o', edgecolor='black',\n    label='cluster 2'\n)\n\nplt.scatter(\n    X[y_km == 2, 0], X[y_km == 2, 1],\n    s=50, c='lightblue',\n    marker='v', edgecolor='black',\n    label='cluster 3'\n)\n\n\n# plot the centroids\nplt.scatter(\n    km.cluster_centers_[:, 0], km.cluster_centers_[:, 1],\n    s=250, marker='*',\n    c='red', edgecolor='black',\n    label='centroids'\n)\nplt.legend(scatterpoints=1)\nplt.grid()\nplt.show()","834e40b5":"#K = 2 (sembarang) \nfrom yellowbrick.cluster import SilhouetteVisualizer\nfrom sklearn.cluster import KMeans\nmodel = SilhouetteVisualizer(KMeans(n_clusters=2))\nmodel.fit(X)\nmodel.show()","78c93320":"#K= 3 (OPTIMAL)\nfrom yellowbrick.cluster import SilhouetteVisualizer\nfrom sklearn.cluster import KMeans\nmodel = SilhouetteVisualizer(KMeans(n_clusters=3))\nmodel.fit(X)\nmodel.show()","b82bf2c7":"from sklearn.metrics import silhouette_samples, silhouette_score","6dab531e":"n_clusters = 2\n# Initialize the clusterer with n_clusters value and a random generator\n# seed of 10 for reproducibility.\nclusterer = KMeans(n_clusters=n_clusters, random_state=10)\ncluster_labels = clusterer.fit_predict(X)\n# clusters\nsilhouette_avg = silhouette_score(X, cluster_labels)\nprint(\"For n_clusters =\", n_clusters,\n      \"The average silhouette_score is :\", silhouette_avg)\n# Compute the silhouette scores for each sample\nsample_silhouette_values = silhouette_samples(X, cluster_labels)","c89cb5a5":"n_clusters = 3\n# Initialize the clusterer with n_clusters value and a random generator\n# seed of 10 for reproducibility.\nclusterer = KMeans(n_clusters=n_clusters, random_state=10)\ncluster_labels = clusterer.fit_predict(X)\n# clusters\nsilhouette_avg = silhouette_score(X, cluster_labels)\nprint(\"For n_clusters =\", n_clusters,\n      \"The average silhouette_score is :\", silhouette_avg)\n# Compute the silhouette scores for each sample\nsample_silhouette_values = silhouette_samples(X, cluster_labels)","b8405b38":"Model Polynomial Regression (Degree = 10)","a3762cd1":"Silhouette Plot of KMeans","37691557":"> ******REGRESSION**","7c0b9c20":"KMEANS CLUSTERING"}}