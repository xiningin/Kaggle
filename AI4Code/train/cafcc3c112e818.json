{"cell_type":{"4fa45280":"code","552868e2":"code","c16c1ed2":"code","9d3ad613":"code","e292f516":"code","cd451865":"code","f3c4eea7":"code","eb5d5c60":"code","8c127fa5":"code","fe0066cc":"code","732e26cb":"code","debb7de0":"code","fd3fb6ca":"code","75a1cedc":"code","7690e111":"code","8d8b785f":"code","79aefc23":"code","6cd98d15":"code","94107ea4":"code","e46cc8ab":"code","86dd1efd":"code","5d7438c7":"code","58afb500":"code","047ed464":"markdown","0d29acd6":"markdown","e1afef6f":"markdown","dfad848f":"markdown","dd10b7c3":"markdown","2844bc7b":"markdown","c8390d09":"markdown","0bd8b1a0":"markdown","9e314514":"markdown","2c0fa91e":"markdown","8055ee22":"markdown","4f401602":"markdown","4bba8d1a":"markdown","1a680492":"markdown","f17fdddb":"markdown","48ffb03f":"markdown","35609d5e":"markdown","795bdcec":"markdown","2f85a853":"markdown","af33d755":"markdown","f44c3b50":"markdown","2f537749":"markdown","f0ddb683":"markdown","467bf474":"markdown"},"source":{"4fa45280":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport math","552868e2":"dataset = pd.read_csv('..\/input\/surgical-dataset-binary-classification\/Surgical-deepnet.csv')\ndataset = dataset.fillna(0)","c16c1ed2":"dataset.head()","9d3ad613":"dataset.info()","e292f516":"import pandas_profiling\npandas_profiling.ProfileReport(dataset)","cd451865":"X = dataset.iloc[:,:-1].values\ny = dataset.iloc[:,-1:].values\n","f3c4eea7":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","eb5d5c60":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","8c127fa5":"def initialize_params(layer_dims):\n    \n    \"\"\"\n    Arguments:\n    layer_dims -- python array (list) containing the size of each layer.\n    \n    Returns:\n    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n                    b1 -- bias vector of shape (layers_dims[1], 1)\n                    ...\n                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n                    bL -- bias vector of shape (layers_dims[L], 1)\n    \"\"\"\n    params = {}\n    np.random.seed(42)\n    L = len(layer_dims)-1\n    for l in range (L):\n        params[\"W\"+str(l+1)] = np.random.randn(layer_dims[l+1],layer_dims[l])*(2\/layer_dims[l])**0.5\n        params[\"b\"+str(l+1)] = np.zeros((layer_dims[l+1],1))\n    \n    return params","fe0066cc":"def Linear_forward(A,W,b):\n    Z = np.dot(W,A)+b\n    cache = (A,W,b)\n    return Z,cache","732e26cb":"def Activation_forward(A,W,b,Activation):\n    if Activation == \"relu\":\n        Z,Linear_cache = Linear_forward(A,W,b)\n        A  = np.maximum(0,Z)\n        A, activation_cache = A,Z\n    elif Activation == 'sigmoid':\n        Z,Linear_cache = Linear_forward(A,W,b)\n        A,activation_cache = (1\/(1+np.exp(-Z)),Z)\n    cache= (Linear_cache,activation_cache)\n    return A,cache","debb7de0":"def forward_prop(X,params):\n    A=X\n    caches = []\n    L = len(params)\/\/2\n    for l in range (L-1):\n        A_prev = A\n        A, cache = Activation_forward(A_prev,params[\"W\"+str(l+1)],params[\"b\"+str(l+1)],\"relu\")\n        caches.append(cache)\n    AL,cache = Activation_forward(A,params[\"W\"+str(L)],params[\"b\"+str(L)],\"sigmoid\")\n    caches.append(cache)\n    return AL,caches","fd3fb6ca":"def cost(AL,Y) :\n    m = Y.shape[1]\n    cost = -np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\/m\n    return np.squeeze(cost)","75a1cedc":"def linear_backward(dZ,cache):\n    A_prev,W,b = cache\n    m =A_prev.shape[1]\n    dW = (np.dot(dZ,A_prev.T)\/m) \n    db = np.sum(dZ,axis=1,keepdims=True)\/m\n    dA_prev = np.dot(W.T,dZ)\n    return dA_prev , dW , db","7690e111":"def activation_backward(dA,cache,activation):\n    linear_cache,activation_cache = cache\n    Z=activation_cache\n    if activation == \"relu\":\n        dZ = (Z>0).astype(int)\n        dZ = dA*dZ\n        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n    elif activation == \"sigmoid\":\n        dZ = np.multiply(dA,(1\/(1+np.exp(-Z)))*(1-(1\/(1+np.exp(-Z)))))\n        dA_prev, dW, db = linear_backward(dZ,linear_cache)\n    \n    return dA_prev, dW, db","8d8b785f":"def backward_prop(AL,Y,caches):\n    \n    L = len(caches)\n    m = AL.shape[1]\n    Y = Y.reshape(AL.shape)\n    grads = {}\n    \n    grads[\"dA\"+str(L)] = -(np.divide(Y,AL)-np.divide(1-Y,1-AL))\n    \n    current_cache = caches[L-1]\n    \n    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] =  activation_backward(grads[\"dA\"+str(L)], current_cache, 'sigmoid')\n    for l in reversed (range(L-1)):\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp =  activation_backward(grads['dA'+str(l+1)], current_cache, 'relu')\n        grads[\"dA\" + str(l)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n        \n    return grads\n        ","79aefc23":"def random_mini_batches(X, Y, mini_batch_size ):\n    \"\"\"\n    Creates a list of random minibatches from (X, Y)\n    \n    Arguments:\n    X -- input data, of shape (input size, number of examples)\n    Y -- true \"label\" vector (1 for blue dot \/ 0 for red dot), of shape (1, number of examples)\n    mini_batch_size -- size of the mini-batches, integer\n    \n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n    \"\"\"\n    \n\n    m = X.shape[1]                  # number of training examples\n    mini_batches = []\n    np.random.seed(42)\n    # Step 1: Shuffle (X, Y)\n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[:, permutation]\n    shuffled_Y = Y[:, permutation].reshape((1,m))\n\n    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n    num_complete_minibatches = math.floor(m\/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        \n        mini_batch_X = shuffled_X[:,k*mini_batch_size:(k+1)*mini_batch_size]\n        mini_batch_Y = shuffled_Y[:,k*mini_batch_size:(k+1)*mini_batch_size]\n     \n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        \n        mini_batch_X = shuffled_X[:,num_complete_minibatches*mini_batch_size:m]\n        mini_batch_Y = shuffled_Y[:,num_complete_minibatches*mini_batch_size:m]\n        \n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches","6cd98d15":"def initialize_adam(parameters) :\n    L = len(parameters) \/\/ 2 # number of layers in the neural networks\n    v = {}\n    s = {}\n    \n    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n    for l in range(L):\n    ### START CODE HERE ### (approx. 4 lines)\n        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\"+str(l+1)].shape)\n        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\"+str(l+1)].shape)\n        s[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\"+str(l+1)].shape)\n        s[\"db\" + str(l+1)] = np.zeros(parameters[\"b\"+str(l+1)].shape)\n    ### END CODE HERE ###\n    \n    return v, s","94107ea4":"def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01, beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n    L = len(parameters) \/\/ 2                 # number of layers in the neural networks\n    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n    \n    # Perform Adam update on all parameters\n    for l in range(L):\n        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n       \n        v[\"dW\" + str(l+1)] = beta1*v[\"dW\"+str(l+1)]+(1-beta1)*grads[\"dW\"+str(l+1)]\n        v[\"db\" + str(l+1)] = beta1*v[\"db\"+str(l+1)]+(1-beta1)*grads[\"db\"+str(l+1)]\n       \n\n        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n        \n        v_corrected[\"dW\" + str(l+1)] = v[\"dW\"+str(l+1)]\/(1-beta1**t)\n        v_corrected[\"db\" + str(l+1)] = v[\"db\"+str(l+1)]\/(1-beta1**t)\n        \n        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n        s[\"dW\" + str(l+1)] = beta2*s[\"dW\"+str(l+1)]+(1-beta2)*grads[\"dW\"+str(l+1)]**2\n        s[\"db\" + str(l+1)] = beta2*s[\"db\"+str(l+1)]+(1-beta2)*grads[\"db\"+str(l+1)]**2\n\n\n        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n     \n        s_corrected[\"dW\" + str(l+1)] = s[\"dW\"+str(l+1)]\/(1-beta2**t)\n        s_corrected[\"db\" + str(l+1)] = s[\"db\"+str(l+1)]\/(1-beta2**t)\n        \n\n        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n\n        parameters[\"W\" + str(l+1)] = parameters[\"W\"+str(l+1)] - learning_rate*(v_corrected[\"dW\"+str(l+1)]\/((s_corrected[\"dW\"+str(l+1)]**0.5)+epsilon))\n        parameters[\"b\" + str(l+1)] = parameters[\"b\"+str(l+1)] - learning_rate*(v_corrected[\"db\"+str(l+1)]\/((s_corrected[\"db\"+str(l+1)]**0.5)+epsilon))\n   \n\n    return parameters, v, s","e46cc8ab":"def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 16, beta = 0.9,beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 700, print_cost = True):\n    L = len(layers_dims)             # number of layers in the neural networks\n    costs = []                       # to keep track of the cost\n    t = 0  \n    m =X.shape[1]\n    \n    params = initialize_params(layers_dims)\n    \n    v, s = initialize_adam(params)\n    \n    for i in range(num_epochs):\n        minibatches = random_mini_batches(X, Y, mini_batch_size)\n        cost_total = 0\n        for minibatch in minibatches:\n            \n            (minibatch_X,minibatch_Y) = minibatch\n            AL, caches = forward_prop(minibatch_X, params)\n            \n            cost_total += cost(AL, minibatch_Y)\n            \n            grads = backward_prop(AL, minibatch_Y, caches)\n            t=t+1\n            params, v, s = update_parameters_with_adam(params, grads, v, s, t, learning_rate, beta1, beta2,  epsilon)\n            \n        cost_avg = cost_total \/ m\n            \n        if print_cost and i % 50 == 0:\n            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n            costs.append(cost_avg)\n                \n    plt.plot(costs)\n    plt.ylabel('cost')\n    plt.xlabel('epochs (per 100)')\n    plt.title(\"Learning rate = \" + str(learning_rate))\n    plt.show()\n\n    return params","86dd1efd":"def predict(X,y,parameters):\n    \n    pre, cache = forward_prop(X,parameters)\n    predictions = (pre>0.5).astype(int)\n    from sklearn.metrics import accuracy_score\n    \n    print(accuracy_score(predictions[0],y[0]))\n    return predictions\n\n    ","5d7438c7":"layers_dims = [X.shape[1],4,4,4,1]\nparams = model(X_train.T, y_train.T ,layers_dims, optimizer = \"adam\",learning_rate=7e-4)","58afb500":"print(\"Train Accuracy:\")\npredictions_train = predict(X_train.T, y_train.T, params)\nprint(\"Test Accuracy:\")\npredictions_test = predict(X_test.T, y_test.T, params)","047ed464":"## Feature Scaling","0d29acd6":"# Don't forget to upvote \ud83d\ude09","e1afef6f":"# 3. Backward Propagation","dfad848f":"## It's just a demonstration for building a neural network, so we'll not look for EDA in depth.\n","dd10b7c3":"# 9. Making Predictions","2844bc7b":"# 5. Gradient descent with ADAM optimization","c8390d09":"## Importing Libs","0bd8b1a0":"## Detiled info of dataset ","9e314514":"### We can further improve performance by Data cleaning and doing Hyperparameter tuning for parameters like Learning rate, Layer_dims, no. of Hidden units, mini_batch_size etc.","2c0fa91e":"# 4. Making Mini Batches","8055ee22":"## Train test split","4f401602":"# 1. Intializing Parameters","4bba8d1a":"### no null value found \ud83d\udc40","1a680492":"## He Initialization\n","f17fdddb":"# 8. Training our model","48ffb03f":"# 2. Forward Propagation","35609d5e":"# Constructing a Neural Network from Scratch ","795bdcec":"### Scaled data will help in training of neural network, i generally speeds up the gradient descent!!","2f85a853":"# 6. Model code (Combining Every thing) ","af33d755":"### If there's any bug I left in this so much messy code, please inform in comments, still a beginner \ud83d\ude05 ","f44c3b50":"# Let's get started with our network","2f537749":"We are using a model with last layer having activation sigmoid and all other layers with ReLu as activation function","f0ddb683":"# 7. Making Predictions","467bf474":"### A Sample notebook which gives a method to construct a neural network from scratch, that means we are not using any of the predefined libraries like tensorflow, keras etc.\n\nFor this purpose I'm using real life Dataset.\n\nHope you like the work! \ud83d\ude0a"}}