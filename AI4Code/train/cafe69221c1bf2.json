{"cell_type":{"d9191e02":"code","f8fbdc45":"code","0c5d9259":"code","0d2c6373":"code","88372bcc":"code","a29b616d":"code","3b6d44d5":"code","4c3a9f04":"markdown","7fb3ceb5":"markdown","89aafa92":"markdown"},"source":{"d9191e02":"%%time\n\nimport pandas as pd\nimport numpy as np\n\n# Load data\ntrain = pd.read_csv('..\/input\/cat-in-the-dat\/train.csv')\ntest = pd.read_csv('..\/input\/cat-in-the-dat\/test.csv')\n\nprint(train.shape)\nprint(test.shape)","f8fbdc45":"%%time\n\n# Subset\ntarget = train['target'].values\ntrain_id = train['id'].tolist()\ntest_id = test['id'].tolist()\ntrain.drop(['target', 'id'], axis=1, inplace=True)\ntest.drop('id', axis=1, inplace=True)\n\nprint(train.shape)\nprint(test.shape)","0c5d9259":"def reduce_dim(df, column):\n    # summarize those showing only once to one category\n    for index, dup in df[column].duplicated(keep=False).iteritems():\n        if dup == False:\n            df.at[index, column] = -1\n    # re-index\n    new_index = {idx:i for i, idx in enumerate(df[column].unique())}\n    df[column] = df[column].map(new_index)\n    return df\n\n\ndef data_treatment(df):\n    for col in list(df.columns):\n        if col.startswith('bin'):\n            bins = df[col].unique()\n            df[col] = df[col].map({bins[0]:0, bins[1]:1}).astype('int8')\n    \n    df['ord_5'] = df['ord_5'].str[0]\n    df['isweekend'] = (df['day'] >= 5).astype('int8')\n    \n    return df","0d2c6373":"%%time\n\n# Preprocessing\nwhole = pd.concat([train, test])\nwhole = data_treatment(whole)\n\ncat_cols = whole.columns[5:-1]\nnon_cat_cols = list(set(whole.columns)-set(cat_cols))\nfor category in cat_cols:\n    whole = reduce_dim(whole, category)","88372bcc":"%%time\n\n# One-hot encoding\nfrom sklearn.preprocessing import OneHotEncoder\nfrom scipy.sparse import hstack\n\nenc = OneHotEncoder(handle_unknown='ignore')\nenc = enc.fit(whole[cat_cols])\nwhole_ohe = enc.transform(whole[cat_cols])\nwhole_ohe = hstack((whole_ohe, whole[non_cat_cols]))\nwhole_ohe = whole_ohe.tocsr()\n\ntrain_ohe = whole_ohe[:train.shape[0], :]\ntest_ohe = whole_ohe[train.shape[0]:, :]\n\nprint(train_ohe.shape)\nprint(test_ohe.shape)","a29b616d":"%%time\n\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score as auc\nfrom sklearn.linear_model import LogisticRegression\n\n# Model\ndef run_cv_model(train, test, target, model_fn, params={}, eval_fn=None, model_type='logreg'):\n    kf = StratifiedKFold(n_splits=5)\n    fold_splits = kf.split(train, target)\n    \n    trn_scores = []\n    cv_scores = []\n    pred_oof = np.zeros((train.shape[0]))\n    pred_full_test = 0\n    \n    for i, (dev_index, val_index) in enumerate(fold_splits, 1):\n        print(f'Start fold {i}\/5')\n        trn_X, val_X = train[dev_index], train[val_index]\n        trn_y, val_y = target[dev_index], target[val_index]\n        pred_trn, pred_val, pred_test = model_fn(trn_X, trn_y, val_X, val_y, test, params)\n        pred_oof[val_index] = pred_val\n        pred_full_test += pred_test \/ 5.0\n        if eval_fn is not None:\n            trn_sc = eval_fn(trn_y, pred_trn)\n            cv_sc = eval_fn(val_y, pred_val)\n            trn_scores.append(trn_sc)\n            cv_scores.append(cv_sc)\n            print(f'trn score {i}: {trn_sc}')\n            print(f'cv score {i}: {cv_sc}')\n            print()\n    \n    print(f'trn scores : {trn_scores}')\n    print(f'trn mean score : {np.mean(trn_scores)}')\n    print(f'trn std score : {np.std(trn_scores)}')\n    print()\n    \n    print(f'oof cv scores : {eval_fn(target, pred_oof)}')\n    print(f'cv scores : {cv_scores}')\n    print(f'cv mean score : {np.mean(cv_scores)}')\n    print(f'cv std score : {np.std(cv_scores)}')\n    print()\n    \n    results = {'model_type': model_type,\n               'pred_oof': pred_oof, 'pred_test': pred_full_test,\n               'trn_scores': trn_scores, 'cv_scores': cv_scores}\n    \n    return results\n\n\ndef runLR(train_X, train_y, val_X, val_y, test_X, params):\n    print('Training Logistic Regression...')\n    model = LogisticRegression(**params)\n    model.fit(train_X, train_y)\n    print('Predicting 1\/3...')\n    pred_trn = model.predict_proba(train_X)[:, 1]\n    print('Predicting 2\/3...')\n    pred_val = model.predict_proba(val_X)[:, 1]\n    print('Predicting 3\/3...')\n    pred_test = model.predict_proba(test_X)[:, 1]\n    return pred_trn, pred_val, pred_test\n\n\nlr_params = {'solver': 'lbfgs', 'C': 0.1, 'max_iter': 1000}\nresults = run_cv_model(train_ohe, test_ohe, target, runLR, lr_params, auc)","3b6d44d5":"# Make submission\n\nsubmission = pd.DataFrame({'id': test_id, 'target': results['pred_test']})\nsubmission.to_csv('submission.csv', index=False)","4c3a9f04":"Reference:  \nhttps:\/\/www.kaggle.com\/peterhurford\/why-not-logistic-regression  \nhttps:\/\/www.kaggle.com\/superant\/oh-my-cat  \n\nJust enter this competition and read the above kernels. This notebook is modified from the references. Approaches therein are simple feature engineering and logistic regression, which is elegant. Keep exploring more advanced approach.","7fb3ceb5":"Rather than using pd.get_dummies(), OneHotEncoder in sklearn could be much faster with the sparse output.","89aafa92":"Some simple preprocessing and feature engineering"}}