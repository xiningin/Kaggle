{"cell_type":{"e3af35d3":"code","87b5ccde":"code","a8736143":"code","76a1e8df":"code","f5beaf31":"code","b1c899f3":"code","ad133da5":"markdown","a3b60279":"markdown","c89a2145":"markdown","d903f360":"markdown","f6267d0b":"markdown","4e31356b":"markdown","ce30cfc3":"markdown","b20a9b56":"markdown"},"source":{"e3af35d3":"import sys\nprint(\"Python version:\", sys.version)\n\nimport tensorflow as tf\nprint(\"TensorFlow version:\", tf.__version__)","87b5ccde":"x = tf.ones((2, 2 ))\n\nwith tf.GradientTape() as t:\n    t.watch(x)\n    y = tf.reduce_sum(x)\n    z = tf.multiply(y, y)\n    \n# derivative of z with respect to the original input tensor x\ndz_dx = t.gradient(z, x)\nfor i in [0, 1]:\n    for j in [0, 1]:\n        assert dz_dx[i][j].numpy() == 8.0","a8736143":"x = tf.ones((2, 2))\n\nwith tf.GradientTape() as t:\n    t.watch(x)\n    y = tf.reduce_sum(x)\n    z = tf.multiply(y, y)\n\n# Use the tape to compute the derivative of z with respect to the intermedia value y.\ndz_dy = t.gradient(z, y)\nassert dz_dy.numpy() == 8.0","76a1e8df":"x = tf.constant(3.0)\nwith tf.GradientTape(persistent=True) as t:\n    t.watch(x)\n    y =  x * x\n    z =  y * y\n\ndz_dx = t.gradient(z, x) # 108.0 (4*x^3 at x = 3)\ndy_dx = t.gradient(y, x) # 6.0\ndel t # Drop the reference to the tape","f5beaf31":"def f(x, y):\n    output = 1.0\n    for i in range(y):\n        if i > 1 and i < 5:\n            output = tf.multiply(output, x)\n    return output\n\ndef grad(x, y):\n    with tf.GradientTape() as t:\n        t.watch(x)\n        out = f(x, y)\n    return t.gradient(out, x)\n\nx = tf.convert_to_tensor(2.0)\n\nassert grad(x, 6).numpy() == 12.0\nassert grad(x, 5).numpy() == 12.0\nassert grad(x, 4).numpy() == 4.0","b1c899f3":"x = tf.Variable(1.0) # Create a TensorFlow variable initialized to 1.0\nwith tf.GradientTape() as t:\n    with tf.GradientTape() as t2:\n        y =  x * x * x\n    \n    # compute the graident inside the 't' context manager which means the gradient computation is differentiable as well\n    dy_dx = t2.gradient(y, x)\nd2y_dx2 = t.gradient(dy_dx, x)\n\nassert dy_dx.numpy() == 3.0\nassert d2y_dx2.numpy() == 6.0","ad133da5":"By default, the resources held by a `GradientTape` are released as soon as `GradientTape.gradient()` method is called. To compute multiple gradients over the same computation, create a `persistent` gradient tape. This allows multiple calls to the `gradient()` method, as resources are released when the tape object is garbage collected. For example:","a3b60279":"## Recording control flow\n\nBecause tapes record operations as they are executed, Python control flow (using `ifs` and `whiles` for example) is naturally handled:","c89a2145":"## Higher-order gradients\n\nOperations inside of the `GradientTape` context manageer are recorded for automatic differentiation. If gradients are computed in that context, then the gradient computation is recorded as well. As a result, the exact same API works for higher-order gradients as well. For example:","d903f360":"# Automatic differentiation and gradient tape\n\nIn the previous tutorial we inroduced Tensors and operations on them. In this Tutorial we will cover [automatic differentiation](https:\/\/en.wikipedia.org\/wiki\/Automatic_differentiation), a key technique for optimizing machine learning models.","f6267d0b":"## Gradient tapes\n\nTensorFlow provides the [tf.GradientTape API](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/GradientTape) for automatic differentiation - computing the gradient of a computing with respect to its input variables. TensorFlow \"records\" all operations executed inside the context of a `tf.GradientTape` onto a \"tape\". TensorFlow then uses that tape and the gradients associated with each recorded operation to compute the gradients of a \"recorded\" computation using [reverse mode differeentiation](https:\/\/en.wikipedia.org\/wiki\/Automatic_differentiation).\n\nFor example:","4e31356b":"You can also request gradients of the output with respect to intermediate values computed during a \"recorded\" `tf.GradientTape` context.","ce30cfc3":"NOTE: I rewrite various notebooks because that's how I learn. I do it on Kaggle because I like their community and other features. Please use and credit original source.\nSource: https:\/\/github.com\/tensorflow\/docs\/blob\/master\/site\/en\/r2\/tutorials\/eager\/automatic_differentiation.ipynb","b20a9b56":"**Next Steps**\n\nIn this tutorial we covered gradient computation in TensorFlow. WIth that we have enough of the primitives required to build and train neural networks."}}