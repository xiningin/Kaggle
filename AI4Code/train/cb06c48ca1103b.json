{"cell_type":{"7ffc4482":"code","8ad0d334":"code","01a2202b":"code","698717c9":"code","6f7a4b4c":"code","db768efa":"code","21ab4d4f":"code","1c3e0bdf":"code","9965950b":"code","a178b2ca":"code","d19fdc20":"code","fe3475a9":"code","ba86e76c":"code","ae07fdbf":"code","5dc18b35":"code","703bd899":"code","0d428821":"code","a95a0c24":"code","cbb51078":"code","4f2f88e6":"code","806564e5":"code","a54d9430":"code","5ae356cc":"code","ec0b1eb4":"code","8535f291":"code","2c622700":"code","7f2f6692":"code","d641c4ea":"code","1f67a44a":"code","c245b534":"markdown","4195526c":"markdown","a97b15eb":"markdown","fb23d3c2":"markdown","d5d65c6c":"markdown","d281e5b8":"markdown","102d610e":"markdown","3b07e925":"markdown","ba9853ba":"markdown","f750fda4":"markdown","506afe07":"markdown","10d65ad1":"markdown","5eb4cbaf":"markdown","2a578503":"markdown","1d1b6e32":"markdown","2f3d3bb6":"markdown","0353f5b6":"markdown","27b063b3":"markdown","92275a5c":"markdown","ce86054d":"markdown","72cf81b5":"markdown","63249213":"markdown","765d77c6":"markdown","36da04b6":"markdown"},"source":{"7ffc4482":"import pandas as pd\nimport seaborn as sns\nfrom sklearn.ensemble import ExtraTreesClassifier, VotingClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import cross_val_score\nimport warnings\nimport os\n\nwarnings.simplefilter('ignore')\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8ad0d334":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/train.csv',index_col = 'PassengerId')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/test.csv',index_col = 'PassengerId')\nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-apr-2021\/sample_submission.csv')","01a2202b":"train.head()","698717c9":"test.head()","6f7a4b4c":"train.isna().sum()\/train.shape[0] * 100","db768efa":"test.isna().sum()\/test.shape[0] * 100","21ab4d4f":"sns.heatmap(train.isna(),yticklabels = False,cbar = False,cmap = 'viridis')","1c3e0bdf":"sns.heatmap(test.isna(),yticklabels = False,cbar = False,cmap = 'viridis')","9965950b":"train['Age'].fillna(train['Age'].mean(),inplace = True)\ntrain['Age'] = train['Age'].apply(lambda x: '80s' if x >= 80 else '70s' if x>=70 else '60s' if x>=60 else '50s' if x>=50 else '40s' if x>=40 else '30s' if x>=30 else '20s' if x>=20 else '10s' if x>=10 else '0s')\ntest['Age'].fillna(test['Age'].mean(),inplace = True)\ntest['Age'] = test['Age'].apply(lambda x: '80s' if x >= 80 else '70s' if x>=70 else '60s' if x>=60 else '50s' if x>=50 else '40s' if x>=40 else '30s' if x>=30 else '20s' if x>=20 else '10s' if x>=10 else '0s')","a178b2ca":"train['FamName'] = train['Name'].str.extract('([A-Za-z]+)\\,', expand = False)\ntest['FamName'] = test['Name'].str.extract('([A-Za-z]+)\\,', expand = False)\nFamName = train['FamName'].append(test['FamName']).value_counts()\nFamName = FamName.apply(lambda x: 'UltraCommon' if x >= 512 else 'VeryCommon' if x >= 256 else 'ModeratelyCommon' if x >= 128 else 'Common' if x >= 64 else 'SlightlyCommon' if x >= 32 else 'SlightlyRare' if x >= 16 else 'Rare' if x >= 8 else 'ModeratelyRare' if x >= 4 else 'VeryRare' if x >= 2 else 'UltraRare')\ntrain['FamName'] = train['FamName'].apply(lambda x: FamName[x])\ntest['FamName'] = test['FamName'].apply(lambda x: FamName[x])\ntrain.drop(columns = 'Name',inplace = True)\ntest.drop(columns = 'Name',inplace = True)","d19fdc20":"train['Ticket'] = train['Ticket'].apply(lambda x: x[0] if type(x) == str else 'Missing')\ntest['Ticket'] = test['Ticket'].apply(lambda x: x[0] if type(x) == str else 'Missing')","fe3475a9":"train['Cabin'] = train['Cabin'].apply(lambda x: x[0] if type(x) == str else 'Missing')\ntest['Cabin'] = test['Cabin'].apply(lambda x: x[0] if type(x) == str else 'Missing')","ba86e76c":"train['FamSize'] = train['SibSp'] + train['Parch'] + 1\ntrain['FamSize'] = train['FamSize'].apply(lambda x: 'VeryBig' if x >= 12 else 'Big' if x >= 8 else 'Medium' if x >= 5 else 'Small' if x >= 3 else 'Couple' if x ==2 else 'Alone')\ntest['FamSize'] = test['SibSp'] + test['Parch'] + 1\ntest['FamSize'] = test['FamSize'].apply(lambda x: 'VeryBig' if x >= 12 else 'Big' if x >= 8 else 'Medium' if x >= 5 else 'Small' if x >= 3 else 'Couple' if x ==2 else 'Alone')","ae07fdbf":"train['Fare'].fillna(train['Fare'].mean(),inplace = True)\ntrain['Fare'] = train['Fare'].apply(lambda x: 'CrazyRich' if x >= 640 else 'UltraRich' if x >= 320 else 'VeryRich' if x >= 160 else 'Rich' if x >= 80 else 'SlightlyRich' if x >= 40 else 'SlightlyPoor' if x >= 20 else 'Poor' if x >= 10 else 'VeryPoor' if x >= 5 else 'UltraPoor')\ntest['Fare'].fillna(test['Fare'].mean(),inplace = True)\ntest['Fare'] = test['Fare'].apply(lambda x: 'CrazyRich' if x >= 640 else 'UltraRich' if x >= 320 else 'VeryRich' if x >= 160 else 'Rich' if x >= 80 else 'SlightlyRich' if x >= 40 else 'SlightlyPoor' if x >= 20 else 'Poor' if x >= 10 else 'VeryPoor' if x >= 5 else 'UltraPoor')","5dc18b35":"train.head()","703bd899":"test.head()","0d428821":"train = pd.get_dummies(train)\ntest = pd.get_dummies(test)","a95a0c24":"X = train.drop(columns = 'Survived')\ny = train['Survived']","cbb51078":"X.head()","4f2f88e6":"y.head()","806564e5":"test.head()","a54d9430":"results = []\nscore = 0\nn_estimators = [100,200,500,1000]\n\nfor trees in n_estimators:\n    clf = ExtraTreesClassifier(n_estimators = trees,oob_score = True,bootstrap = True,n_jobs = -1,random_state = 42)\n    clf.fit(X,y)\n    if clf.oob_score_ > score:\n        score = clf.oob_score_\n        best = trees\n    results.append(clf.oob_score_)\n\nprint(f'n_estimators = {best}')\npd.Series(results,n_estimators).plot()","5ae356cc":"results = []\nscore = 0\nmax_depth = [13,15, 17, 18, 19]\n\nfor depth in max_depth:\n    clf = ExtraTreesClassifier(n_estimators = 1000,max_depth = depth,oob_score = True,bootstrap = True,n_jobs = -1,random_state = 42)\n    clf.fit(X,y)\n    if clf.oob_score_ > score:\n        score = clf.oob_score_\n        best = depth\n    results.append(clf.oob_score_)\n\nprint(f'max_depth = {best}')\npd.Series(results,max_depth).plot()","ec0b1eb4":"results = []\nscore\nmin_samples_split = [24,25,26,27]\n\nfor split in min_samples_split:\n    clf = ExtraTreesClassifier(n_estimators = 1000,max_depth = 17,min_samples_split = split,oob_score = True,bootstrap = True,n_jobs = -1,random_state = 42)\n    clf.fit(X,y)\n    if clf.oob_score_ > score:\n        score = clf.oob_score_\n        best = split\n    results.append(clf.oob_score_)\n\nprint(f'min_samples_split = {best}')\npd.Series(results,min_samples_split).plot()","8535f291":"results = []\nscore = 0\nmin_samples_leaf = [14,17,18,19,20]\n\nfor leaves in min_samples_leaf:\n    clf = ExtraTreesClassifier(n_estimators = 1000,max_depth = 17,min_samples_split = 25,min_samples_leaf = leaves,oob_score = True,bootstrap = True,n_jobs = -1,random_state = 42)\n    clf.fit(X,y)\n    if clf.oob_score_ > score:\n        score = clf.oob_score_\n        best = leaves\n    results.append(clf.oob_score_)\n\nprint(f'min_samples_leaf = {best}')\npd.Series(results,min_samples_leaf).plot()","2c622700":"clf_ext = ExtraTreesClassifier(n_estimators = 1000,max_depth = 17,min_samples_split = 25,min_samples_leaf = 18,n_jobs = -1,random_state = 42)","7f2f6692":"clf_lgbm = LGBMClassifier(boosting_type = 'dart',num_leaves = 32,max_depth = 10,colsample_bytree = 0.8,extra_trees = True,n_jobs = -1,random_state = 42)","d641c4ea":"clf = VotingClassifier(estimators=[('ext',clf_ext),('lgbm', clf_lgbm)], voting='soft')\nclf = clf.fit(X,y)","1f67a44a":"submission['Survived'] = pd.Series(clf.predict(test))\nsubmission.set_index('PassengerId').to_csv('submission.csv')","c245b534":"Terhadap *missing values* yang sudah ditemukan sebelumnya, sebagian diimputasi dengan nilai rata-rata dan sebagian diisi dengan string `Missing`.\n\nPada tahap ini juga sekaligus dilakukan *binning* pada beberapa peubah. Misalnya peubah Age di-*binning* ke dalam kategori berikut: 80s, 70s, 60s, ..., 0s menggunakan metode apply() dan fungsi lambda.","4195526c":"Secara visual, proporsi `missing values` dapat diamati pada grafik *heatmap* berikut.","a97b15eb":"## 1. Memuat pustaka yang diperlukan","fb23d3c2":"Untuk peubah Fare, strateginya juga kira-kira mirip dengan yang dilakukan untuk peubah Age. Saya melakukan *binning* terhadap peubah Fare ini untuk menghasilkan kategori seperti Rich, Poor, dst. Bisa jadi harga tiket tertentu adalah tanda seorang penumpang memiliki keistimewaan, termasuk dalam hal akses terhadap kapal sekoci.","d5d65c6c":"Pada kompetisi ini, saya mencoba melakukan penyetelan (*tuning*) dan menerapkan klasifikasi berdasar voting untuk pertama kalinya.\n\nUntuk penyetelan, saya tidak memakai *method* yang lebih lazim seperti GridSearchCV() -- mungkin ke depan akan saya coba. Kali ini saya coba mengadopsi cara yang diajarkan oleh Mike Bernico pada salah satu [video](https:\/\/www.youtube.com\/watch?v=0GrciaGYzV0) di kanal youtube-nya.\n\nPertama, saya menentukan parameter n_estimators paling optimal pada ExtraTreesClassifier sebagai berikut. Di susul kemudian, saya berturut-turut menjalankan cara yang sama untuk memperoleh nilai paling optimal untuk parameter max_depth, min_samples_split, dan min_samples_leaf.","d281e5b8":"Untuk peubah Name, terdapat sedikit perbedaan dengan dataset Titanic yang asli, di mana tidak terdapatnya gelar atau sebutan seperti *Sir*, *Lady*, dst. Biasanya informasi tentang gelar bisa dianggap menandakan kelas sosial dan kemungkinan berhubungan dengan keistimewaan seseorang untuk diselamatkan terlebih dahulu. Namun demikian, walaupun tidak ada informasi gelar tadi, saya mencoba untuk menarik informasi marga atau nama keluarga dari setiap penumpang. Marga seperti Smith adalah marga yang sangat banyak dijumpai di antara penumpang. Ada juga marga-marga langka seperti Barefield, Proffer, dan sejenisnya. Terhadap marga tersebut, saya melakukan pengelompokan berdasarkan keumuman atau kelangkaan sebagai berikut.","102d610e":"## 3. Memeriksa keberadaan *missing values*","3b07e925":"Hal serupa di atas juga saya terapkan untuk mengkategorisasikan peubah Cabin. Patut diduga bahwa huruf pertama pada peubah Cabin mencerminkan nomor geladak. Di sisi lain, jika asumsi tersebut benar, nomor geladak juga dapat dianggap memiliki kontribusi terhadap peluang selamatnya seorang penumpang. Sebagaimana yang sudah saya tulis di [notebook](https:\/\/www.kaggle.com\/bagusbpg\/my-3rd-notebook) saya untuk kompetisi Titanic dan juga berdasarkan keterangan pada [laman](https:\/\/en.wikipedia.org\/wiki\/Titanic) Wikipedia, geladak tertentu memiliki akses yang lebih mudah ke kapal sekoci. Untuk *missing values*, saya mengisinya dengan string`Missing`, sama seperti pada langkah sebelumnya.","ba9853ba":"Untuk peubah kategorikal yang dihasilkan, saya mencoba menerapkan skema one hot encoding melalui *method* get_dummies()","f750fda4":"## 3. Imputasi *missing values* dan pemrosesan data awal","506afe07":"## 2. Memuat berkas csv ke dalam dataframe pandas","10d65ad1":"Secara sekilas, berikut adalah isi dari dataframe train dan test. Bagi yang pernah mengikuti kompetisi Titanic sebelumnya pasti tidak merasa asing dengan peubah (*feature*) yang ada, seperti SibSp (*siblings and spouse*), Parch (*parents and children*), dll. Silakan merujuk pada kompetisi tersebut untuk penjelasan lebih lengkap dari masing-masing peubah (tautan di bagian awal notebook). Ini mungkin kemajuan yang perlu dicatat untuk kompetisi ini karena peubah yang disajikan semakin beragam. Keberagaman peubah yang ada membuat saya merasa diajak untuk menyadari bahwa data tidaklah semata-mata angka, melainkan juga memiliki narasi. Narasilah yang membuat hubungan sebab-akibat yang ada terasa lebih masuk akal.","5eb4cbaf":"ExtraTreesClassifier adalah estimator *ensemble* yang menerapkan konsep *bagging*. Di sini saya ingin mengkombinasikan estimator *bagging* dengan estimator lain yang menerapkan konsep *boosting*, yaitu LGBMClassifier. Untuk LGBMClassifier ini, saya juga melakukan pencarian nilai optimal untuk beberapa parameter sebagaimana yang saya lakukan sebelumnya pada estimator ExtraTreesClassifier. Hasil akhirnya adalah sebagai berikut","2a578503":"## 5. Prediksi","1d1b6e32":"Seperti biasa, saya memeriksa terlebih dahulu apakah ada data ganda (duplikat), baik pada dataset train maupun test, yaitu dengan menjalankan kode `train.duplicated().any()` dan `test.duplicated().any()`, dan ternyata kedua dataset tidak mengandung data ganda. Seandainya ada, adalah praktik yang baik untuk membuang data ganda tersebut.\n\nKemudian, termasuk tahapan baku juga untuk memeriksa keberadaan *missing values* pada kedua dataset, sebagaimana ditunjukkan di bawah ini. Keberadaan *missing values* dihitung pada setiap peubah atau kolom sebagai persentase terhadap jumlah keseluruhan observasi. Mirip dengan dataset Titanic yang asli, peubah Fare, Embarked, Age, Ticket, dan Cabin berturut-turut memiliki *missing values* dengan proporsi dari yang paling sedikit ke yang paling banyak.","2f3d3bb6":"Ini adalah tahap akhir dari keseluruhan proses, yaitu dilakukannya prediksi terhadap dataset test menggunakan model yang sudah di-*fit*-kan terhadap masukan X dan luaran y. Skor akurasi yang dihasilkan dapat dilihat pada *public leaderboard*.","0353f5b6":"## 4. Pemodelan dan penyetelan parameter","27b063b3":"Kompetisi *Tabular Playground Series* bulan April 2021 ini cukup spesial karena terinspirasi oleh dataset [Titanic](https:\/\/www.kaggle.com\/c\/titanic\/overview), di mana di dalamnya terdapat peubah numerikal maupun kategorikal yang lebih bermakna dan beraneka ragam, sangat cocok bagi pemula seperti saya untuk mempraktikkan keterampilan pemrosesan awal data. Mari, langsung kita garap kompetisi ini!","92275a5c":"Setelah semua peubah sudah dipastikan hanya mengandung isian numerikal, saya melakukan pemisahan dataset training menjadi masukan X dan luaran y.","ce86054d":"Hasilnya, masukan X, luaran y, dan sekaligus dataset test, adalah sebagai berikut.","72cf81b5":"Setelah penyetelan selesai, kedua estimator dikombinasikan melalui suatu estimator final yang menerapkan *soft voting*. Saya beranggapan bahwa estimator *bagging* dan *boosting* akan saling melengkapi satu sama lain, sesuai dengan keunggulan dan kelemahan masing-masing, dengan harapan akan dihasilkan estimator final yang memiliki kekuatan prediktif yang lebih mumpuni.","63249213":"Sampai dengan tahap ini, hasil imputasi *missing values* dan *binning* adalah sebagai berikut.\n\nCatatan: peubah Embarked masih menyisakan missing values, dan saya akan membiarkannya seperti itu.","765d77c6":"Kemudian, dari peubah SibSp dan Parch, saya bisa membuat peubah baru FamSize. Langkah ini sudah menjadi kelaziman yang dapat dijumpai pada mungkin hampir seluruh *notebook* pada kompetisi Titanic yang asli. Hal khusus yang saya lakukan kali ini adalah melakukan *binning* terhadap peubah FamSize berdasarkan ukuran keluarga, mulai dari kategori Alone, Couple, Small, dst. Bisa jadi, keluarga dengan jumlah anggota tertentu memiliki peluang selamat yang lebih tinggi daripada yang lain.","36da04b6":"Untuk peubah Ticket, dapat diamati bahwa terdapat pola khusus, misalnya tiket yang berupa alfanumerikal atau numerikal. Dalam hal ini, saya melakukan pengelompokan berdasarkan pola tersebut, khususnya mengambil karakter pertama dari setiap nomor tiket. Secara naif, anggaplah bahwa pola tiket tertentu mencerminkan fasilitas yang dapat dinikmati oleh penumpang, yang bisa saja terkait dengan peluang keselamatan penumpang tersebut. Kemudian, khusus untuk *missing values*, saya mengisinya dengan string `Missing`."}}