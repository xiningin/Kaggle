{"cell_type":{"1aed8813":"code","731a98d4":"code","bffb96a9":"code","d0e9611c":"code","0cff8753":"code","8b250fdd":"code","62af55e2":"code","c46761bd":"code","0ebafa3a":"code","6210b93a":"code","274f7b58":"code","33d97e17":"code","8cae42b8":"code","1c3658dd":"code","118cff1f":"code","d1ef824b":"code","21487624":"code","fb1ad069":"code","0c898da3":"code","83045e6d":"code","52b47a95":"code","38285a72":"code","e3c488a2":"code","587e1399":"code","5af1ae6b":"code","60fa8f3c":"code","572a22ac":"markdown","b251ca03":"markdown","aba695fb":"markdown","8f292883":"markdown","11f37a1f":"markdown","07e7395c":"markdown","77e8be12":"markdown","f952de92":"markdown","d58360db":"markdown","1f7f35d1":"markdown","2dc51578":"markdown","11d521d0":"markdown","5a43b73b":"markdown","5a5bed97":"markdown","c84c8d66":"markdown","493f7e68":"markdown","8f10be26":"markdown","f2d416a6":"markdown","40a2c932":"markdown","3d672615":"markdown"},"source":{"1aed8813":"import os\nimport subprocess\n\ntry:\n    import matplotlib\nexcept ModuleNotFoundError:\n    os.system('conda install -y matplotlib')\n    import matplotlib\n","731a98d4":"!nvidia-smi","bffb96a9":"!nvcc --version","d0e9611c":"import sys\n!rsync -ah --progress ..\/input\/rapids\/rapids.0.14.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!rsync -ah --progress \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","0cff8753":"from matplotlib.colors import ListedColormap\nimport matplotlib.pyplot as plt\n\n\n%matplotlib inline","8b250fdd":"import numpy as np; print('NumPy Version:', np.__version__)\n\n\n# create the relationship: y = 2.0 * x + 1.0\n\nn_rows = 43000\nw = 2.0\nx = np.random.normal(loc=0, scale=1, size=(n_rows,))\nb = 1.0\ny = w * x + b\n\n# add a bit of noise\nnoise = np.random.normal(loc=0, scale=2, size=(n_rows,))\ny_noisy = y + noise","62af55e2":"plt.scatter(x, y_noisy, label='empirical data points')\nplt.plot(x, y, color='black', label='true relationship')\nplt.legend()","c46761bd":"import sklearn; print('Scikit-Learn Version:', sklearn.__version__)\nfrom sklearn.linear_model import LinearRegression\n\n\n# instantiate and fit model\nlinear_regression = LinearRegression()","0ebafa3a":"%%time\n\nlinear_regression.fit(np.expand_dims(x, 1), y)","6210b93a":"# create new data and perform inference\ninputs = np.linspace(start=-5, stop=5, num=1000)\noutputs = linear_regression.predict(np.expand_dims(inputs, 1))","274f7b58":"plt.scatter(x, y_noisy, label='empirical data points')\nplt.plot(x, y, color='black', label='true relationship')\nplt.plot(inputs, outputs, color='red', label='predicted relationship (cpu)')\nplt.legend()","33d97e17":"import cudf; print('cuDF Version:', cudf.__version__)\n\n\n# create a cuDF DataFrame\ndf = cudf.DataFrame({'x': x, 'y': y_noisy})\nprint(df.head())","8cae42b8":"import cuml; print('cuML Version:', cuml.__version__)\nfrom cuml.linear_model import LinearRegression as LinearRegression_GPU\n\n\n# instantiate and fit model\nlinear_regression_gpu = LinearRegression_GPU()","1c3658dd":"%%time\n\nlinear_regression_gpu.fit(df['x'], df['y'])","118cff1f":"# create new data and perform inference\nnew_data_df = cudf.DataFrame({'inputs': inputs})\noutputs_gpu = linear_regression_gpu.predict(new_data_df[['inputs']])","d1ef824b":"plt.scatter(x, y_noisy, label='empirical data points')\nplt.plot(x, y, color='black', label='true relationship')\nplt.plot(inputs, outputs, color='red', label='predicted relationship (cpu)')\nplt.plot(inputs, outputs_gpu.to_array(), color='green', label='predicted relationship (gpu)')\nplt.legend()","21487624":"from sklearn.datasets import make_moons\n\n\nX, y = make_moons(n_samples=int(1e3), noise=0.05, random_state=0)\nprint(X.shape)","fb1ad069":"figure = plt.figure()\naxis = figure.add_subplot(111)\naxis.scatter(X[y == 0, 0], X[y == 0, 1], \n             edgecolor='black',\n             c='lightblue', marker='o', s=40, label='cluster 1')\n\naxis.scatter(X[y == 1, 0], X[y == 1, 1], \n             edgecolor='black',\n             c='red', marker='s', s=40, label='cluster 2')\nplt.legend()\nplt.tight_layout()\nplt.show()","0c898da3":"X_df = cudf.DataFrame()\nfor column in range(X.shape[1]):\n    X_df['feature_' + str(column)] = np.ascontiguousarray(X[:, column])\n\ny_df = cudf.Series(y)","83045e6d":"from cuml.neighbors import NearestNeighbors\n\n\nknn = NearestNeighbors()","52b47a95":"knn.fit(X_df)","38285a72":"k = 3\n\ndistances, indices = knn.kneighbors(X_df, n_neighbors=k)","e3c488a2":"distances","587e1399":"indices","5af1ae6b":"predictions = []\n\nfor i in range(indices.shape[0]):\n    row = indices.iloc[i, :]\n    vote = sum(y_df[j] for j in row) \/ k\n    predictions.append(1.0 * (vote > 0.5))\n\npredictions = np.asarray(predictions).astype(np.float32)","60fa8f3c":"f, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 3))\n\n\nax1.scatter(X[y == 0, 0], X[y == 0, 1],\n            edgecolor='black',\n            c='lightblue', marker='o', s=40, label='cluster 1')\nax1.scatter(X[y == 1, 0], X[y == 1, 1],\n            edgecolor='black',\n            c='red', marker='s', s=40, label='cluster 2')\nax1.set_title('empirical data points')\n\n\nax2.scatter(X[predictions == 0, 0], X[predictions == 0, 1], c='lightblue',\n            edgecolor='black',\n            marker='o', s=40, label='cluster 1')\nax2.scatter(X[predictions == 1, 0], X[predictions == 1, 1], c='red',\n            edgecolor='black',\n            marker='s', s=40, label='cluster 2')\nax2.set_title('KNN predicted classes')\n\nplt.legend()\nplt.tight_layout()\nplt.show()","572a22ac":"Next, let's load some helper functions from `matplotlib` and configure the Jupyter Notebook for visualization.","b251ca03":"The mathematical operations underlying many machine learning algorithms are often matrix multiplications. These types of operations are highly parallelizable and can be greatly accelerated using a GPU. cuML makes it easy to build machine learning models in an accelerated fashion while still using an interface nearly identical to Scikit-Learn. The below shows how to accomplish the same Linear Regression model but on a GPU.\n\nFirst, let's convert our data from a NumPy representation to a cuDF representation.","aba695fb":"Before we build a KNN classification model, we first have to convert our data to a cuDF representation.","8f292883":"<a id=\"knn\"><\/a>\n## K Nearest Neighbors\n\nNearestNeighbors is a unsupervised algorithm where if one wants to find the \u201cclosest\u201d datapoint(s) to new unseen data, one can calculate a suitable \u201cdistance\u201d between each and every point, and return the top K datapoints which have the smallest distance to it.\n\nWe'll generate some fake data using the `make_moons` function from the `sklearn.datasets` module. This function generates data points from two equations, each describing a half circle with a unique center. Since each data point is generated by one of these two equations, the cluster each data point belongs to is clear. The ideal classification algorithm will identify two clusters and associate each data point with the equation that generated it. \n\nThese data points are generated using a non-linear relationship - so using a linear regression approach won't adequately solve problem. Instead, we can use a distance-based algorithm K Nearest Neighbors to classify each data point.\n\nFirst, let's generate out data.","11f37a1f":"Once our model has been built and fitted to the data, we can query the model for the `k` nearest neighbors to each data point. The query returns a matrix representating the distances of each data point to its nearest `k` neighbors as well as the indices of those neighbors.","07e7395c":"Lastly, we can visualize the predictions from our K Nearest Neighbors classifier - we see that despite the non-linearity of the data, the algorithm does an excellent job of classifying the data.","77e8be12":"We can iterate through each of our data points and do a majority vote to determine which class it belongs to.","f952de92":"Lastly, we can overlay our predicted relationship using our GPU accelerated Linear Regression model (green line) over our empirical data points (light blue circles), the true relationship (blue line), and the predicted relationship from a model built on the CPU (red line). We see that our GPU accelerated model's estimate of the true relationship (green line) is identical to the CPU based model's estimate of the true relationship (red line)!","d58360db":"Let's visualize our data:","1f7f35d1":"Next, we'll instantiate and fit a nearest neighbors model using the `NearestNeighbors` class from cuML.","2dc51578":"We'll use the `LinearRegression` class from Scikit-Learn to instantiate a model and fit it to our data.","11d521d0":"Let's now visualize our empirical data points, the true relationship of the data, and the relationship estimated by the model. Looks pretty close!","5a43b73b":"We can now visualize our data using the `matplotlib` library.","5a5bed97":"Next, let's see what CUDA version we have:","c84c8d66":"<a id=\"introduction\"><\/a>\n## Introduction to Machine Learning with RAPIDS\n#### By Paul Hendricks\n#### modified by Beniel Thileepan\n--------\n\nThis work is modified inorder to run in Kaggle with additional rapids dataset.integers and strings.\n\nIn this notebook, we will show how to work with cuDF DataFrames in RAPIDS (https:\/\/www.kaggle.com\/cdeotte\/rapids)\n\nIn this notebook, we will show to do GPU accelerated Supervised Learning, unsupervised learning in RAPIDS.  \n\n**Table of Contents**\n\n* [Introduction to Supervised Learning](#introduction)\n* [Linear Regression](#linear)\n* [K Nearest Neighbors](#knn)\n* [Setup](#setup)\n* [Conclusion](#conclusion)\n\nBefore going any further, let's make sure we have access to `matplotlib`, a popular Python library for visualizing data.","493f7e68":"Next, we'll load the GPU accelerated `LinearRegression` class from cuML, instantiate it, and fit it to our data.","8f10be26":"<a id=\"linear\"><\/a>\n## Linear Regression\n\nAfter our data has been preprocessed, we often want to build a model so as to understand the relationships between different variables in our data. Scikit-Learn is an incredibly powerful toolkit that allows data scientists to quickly build models from their data. Below we show a simple example of how to create a Linear Regression model.","f2d416a6":"<a id=\"setup\"><\/a>\n## Setup and install RAPIDS\n","40a2c932":"We can use this model to predict values for new data points, a step often called \"inference\" or \"scoring\". All model fitting and predicting steps are GPU accelerated.","3d672615":"<a id=\"conclusion\"><\/a>\n## Conclusion\n\nIn this notebook, we showed to do GPU accelerated Supervised Learning in RAPIDS. \n\nTo learn more about RAPIDS, be sure to check out: \n\n* [Open Source Website](http:\/\/rapids.ai)\n* [GitHub](https:\/\/github.com\/rapidsai\/)\n* [Press Release](https:\/\/nvidianews.nvidia.com\/news\/nvidia-introduces-rapids-open-source-gpu-acceleration-platform-for-large-scale-data-analytics-and-machine-learning)\n* [NVIDIA Blog](https:\/\/blogs.nvidia.com\/blog\/2018\/10\/10\/rapids-data-science-open-source-community\/)\n* [Developer Blog](https:\/\/devblogs.nvidia.com\/gpu-accelerated-analytics-rapids\/)\n* [NVIDIA Data Science Webpage](https:\/\/www.nvidia.com\/en-us\/deep-learning-ai\/solutions\/data-science\/)\n"}}