{"cell_type":{"35f90f1c":"code","5e64b017":"code","2272394b":"code","573818d0":"code","ba217980":"code","427b1e83":"code","9950d595":"code","726ed7b8":"code","e7075873":"code","4cfe6c66":"code","041fca81":"code","f36d8a30":"code","8d7d6582":"code","734fcafc":"code","02ca9b32":"code","b04f84bf":"markdown"},"source":{"35f90f1c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.\n\nimport matplotlib.pyplot as plt\nget_ipython().magic(u'matplotlib inline')\nplt.style.use('ggplot')\n","5e64b017":"'''Trains a Siamese MLP on pairs of digits from the MNIST dataset.\nIt follows Hadsell-et-al.'06 [1] by computing the Euclidean distance on the\noutput of the shared network and by optimizing the contrastive loss (see paper\nfor mode details).\n# References\n- Dimensionality Reduction by Learning an Invariant Mapping\n    http:\/\/yann.lecun.com\/exdb\/publis\/pdf\/hadsell-chopra-lecun-06.pdf\nGets to 97.2% test accuracy after 20 epochs.\n2 seconds per epoch on a Titan X Maxwell GPU\n'''\nfrom __future__ import absolute_import\nfrom __future__ import print_function\n\nimport random\nfrom keras.models import Model\nfrom keras.layers import Input, Flatten, Dense, Dropout, Lambda\nfrom keras.optimizers import RMSprop\nfrom keras import backend as K\n\nnum_classes = 10\nepochs = 10\n\n\ndef euclidean_distance(vects):\n    x, y = vects\n    sum_square = K.sum(K.square(x - y), axis=1, keepdims=True)\n    return K.sqrt(K.maximum(sum_square, K.epsilon()))\n\n\ndef eucl_dist_output_shape(shapes):\n    shape1, shape2 = shapes\n    return (shape1[0], 1)\n\n\ndef contrastive_loss(y_true, y_pred):\n    '''Contrastive loss from Hadsell-et-al.'06\n    http:\/\/yann.lecun.com\/exdb\/publis\/pdf\/hadsell-chopra-lecun-06.pdf\n    '''\n    margin = 1\n    sqaure_pred = K.square(y_pred)\n    margin_square = K.square(K.maximum(margin - y_pred, 0))\n    return K.mean(y_true * sqaure_pred + (1 - y_true) * margin_square)\n\n\ndef create_pairs(x, digit_indices):\n    '''Positive and negative pair creation.\n    Alternates between positive and negative pairs.\n    labels = [1, 0, 1, 0, 1, 0, ...]\n    '''\n    pairs = []\n    labels = []\n    n = min([len(digit_indices[d]) for d in range(num_classes)]) - 1\n    for d in range(num_classes):\n        for i in range(n):\n            # horizental \n            z1, z2 = digit_indices[d][i], digit_indices[d][i + 1]\n            pairs.append([x[z1], x[z2]])\n            \n            # vertical \n            inc = random.randrange(1, num_classes)\n            dn = (d + inc) % num_classes\n            z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n            pairs.append([x[z1], x[z2]])\n            \n            labels += [1, 0]\n    return np.array(pairs), np.array(labels)\n\n\ndef create_base_network(input_shape):\n    '''Base network to be shared (eq. to feature extraction).\n    '''\n    input = Input(shape=input_shape)\n    x = Flatten()(input)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(128, activation='relu')(x)\n    x = Dropout(0.1)(x)\n    x = Dense(128, activation='relu')(x)\n    x = Lambda(lambda  x: K.l2_normalize(x,axis=1))(x)\n    x = Lambda(lambda  x: K.l2_normalize(x,axis=1))(x)\n    return Model(input, x)\n\n\ndef compute_accuracy(y_true, y_pred):\n    '''Compute classification accuracy with a fixed threshold on distances.\n    '''\n    pred = y_pred.ravel() < 0.5\n    return np.mean(pred == y_true)\n\n\ndef accuracy(y_true, y_pred):\n    '''Compute classification accuracy with a fixed threshold on distances.\n    '''\n    return K.mean(K.equal(y_true, K.cast(y_pred < 0.5, y_true.dtype)))","2272394b":"# load and reshape data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\ntrain_labels =train['label']\ntrain_images =train.drop(['label'],axis=1)\n\ntrain_data = np.array(train_images).reshape(-1,28,28)\ntrain_label = np.array(train_labels)\ntest_x = np.array(test).reshape(-1,28,28)","573818d0":"# the data, split between train and test sets\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(train_data, train_labels, test_size=0.2, shuffle=True, random_state=1)\nprint(\"Train data shape: {}.\".format(x_train.shape))\nprint(\"Test data shape {}.\".format(test_x.shape))\n\nx_train = x_train.astype('float32')\/255 # train\nx_test = x_test.astype('float32')\/255 # validation\ntest_x = test_x\/255  # produce submission.csv\n\ninput_shape = x_train.shape[1:]","ba217980":"# create training+test positive and negative pairs\ndigit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]\ntr_pairs, tr_y = create_pairs(x_train, digit_indices)\n# digit_indices.shape = [10, ?] \n# tr_pairs.shape = (61260, 2, 28, 28)\n# tr_y.shape = (61260, )\n\ndigit_indices = [np.where(y_test == i)[0] for i in range(num_classes)]\nte_pairs, te_y = create_pairs(x_test, digit_indices)\n# digit_indices.shape = [10, ?] \n# te_pairs.shape = (14600, 2, 28, 28)\n# te_y.shape = (14600, )","427b1e83":"# network definition\nbase_network = create_base_network(input_shape)\n\ninput_a = Input(shape=input_shape)\ninput_b = Input(shape=input_shape)\n\n# because we re-use the same instance `base_network`,\n# the weights of the network\n# will be shared across the two branches\nprocessed_a = base_network(input_a)\nprocessed_b = base_network(input_b)\n\ndistance = Lambda(euclidean_distance,\n                  output_shape=eucl_dist_output_shape)([processed_a, processed_b])\n\nmodel = Model([input_a, input_b], distance)\n\n# train\nrms = RMSprop()\n#rms = Adam()\n#rms = SGD()\n\nmodel.compile(loss=contrastive_loss, optimizer=rms, metrics=[accuracy])\n# train\n\n# tr_pairs[:, 0].shape = (61260, 28, 28)\n# tr_pairs[:, 1]\n# tr_y.shape = (61260, )\n# te_pairs[:, 0].shape = (14600, 28, 28)\n\n\nhistory = model.fit([tr_pairs[:, 0], tr_pairs[:, 1]], tr_y,\n          batch_size=128,\n          epochs=epochs,\n          validation_data=([te_pairs[:, 0], te_pairs[:, 1]], te_y))\n\n# compute final accuracy on training and test sets\ny_pred_tr = model.predict([tr_pairs[:, 0], tr_pairs[:, 1]])\ntr_acc = compute_accuracy(tr_y, y_pred_tr)\ny_pred_te = model.predict([te_pairs[:, 0], te_pairs[:, 1]])\nte_acc = compute_accuracy(te_y, y_pred_te)\n\nprint('* Accuracy on training set: %0.2f%%' % (100 * tr_acc))\nprint('* Accuracy on test set: %0.2f%%' % (100 * te_acc))\n","9950d595":"# Loss Learning Curves\nhistory_dict = history.history\n\nloss = history_dict['loss']\nval_loss = history_dict['val_loss']\naccuracy = history_dict['accuracy']\nval_accuracy = history_dict['val_accuracy']\n\n\nepochs = range(1, (len(accuracy) + 1))\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss,'b', label='Validation loss')\nplt.title('Traning and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n","726ed7b8":"# Accuracy Learning Curve\nplt.plot(epochs, accuracy, 'bo', label='Training accuracy')\nplt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","e7075873":"# create test pairs\n# make compare template \ndigit_indices = [np.where(y_train == i)[0] for i in range(num_classes)]\nexample_numbers = 10\nexample_indexes = [np.random.choice((digit_indices[i]), example_numbers, replace=False) for i in range(num_classes)]\n# (10, 10)","4cfe6c66":"from PIL import Image\n\npredictions = []\nfor sub in range(0, len(test_x)): # 28000\n    print('Processing {}'.format(sub))\n    image_sub = test_x[sub].reshape(1, 28, 28)\n    \n    pred_sum = []\n    for i in range(0, num_classes):\n        raw_prediction = []\n        for exam in range(0, example_numbers):\n            example_image = x_train[example_indexes[i][exam]].reshape(1, 28, 28) # 0-9 random image\n            prd = model.predict([image_sub, example_image])\n            prd = np.squeeze(prd)\n            raw_prediction.append(prd)\n        pred_sum.append(sum(raw_prediction))\n        \n    prediction = [np.argmin(pred_sum), min(pred_sum)]   \n    predictions.append(prediction[0])\n    ","041fca81":"sample_submission = pd.read_csv('..\/input\/sample_submission.csv') # DataFrame\nresult = pd.DataFrame({'ImageId': sample_submission.ImageId, 'Label': predictions})\nresult.to_csv('submission.csv', index=False)\nimport os\nos.listdir('..\/working')","f36d8a30":"# import the modules we'll need\nfrom IPython.display import HTML\nimport pandas as pd\nimport numpy as np\nimport base64\n\n# function that takes in a dataframe and creates a text link to  \n# download it (will only work for files < 2MB or so)\ndef create_download_link(df, title = \"Download CSV file\", filename = \"submission.csv\"):  \n    csv = df.to_csv(index=False)\n    b64 = base64.b64encode(csv.encode())\n    payload = b64.decode()\n    html = '<a download=\"{filename}\" href=\"data:text\/csv;base64,{payload}\" target=\"_blank\">{title}<\/a>'\n    html = html.format(payload=payload,title=title,filename=filename)\n    return HTML(html)\n\n\n# create a link to download the dataframe\ncreate_download_link(result)\n\n# \u2193 \u2193 \u2193  Yay, download link! \u2193 \u2193 \u2193 ","8d7d6582":"# Plot the representation learned from the siamese network\n#embedding_model = model.layers[2]\n#embeddings = embedding_model.predict(x_train)\nembeddings = base_network.predict(x_train)","734fcafc":"from sklearn.manifold import TSNE\nX_embedded = TSNE(n_components=2,random_state=10).fit_transform(embeddings)","02ca9b32":"mnist_classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\ncolors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',\n              '#9467bd', '#8c564b', '#e377c2', '#7f7f7f',\n              '#bcbd22', '#17becf']\n\nplt.figure(figsize=(10,10))\nfor i in range(10):\n    inds = np.where(y_train==i)[0]\n    plt.scatter(X_embedded[inds,0], X_embedded[inds,1], alpha=0.5, color=colors[i])\nplt.legend(mnist_classes)","b04f84bf":"*  Applied siamese NN from the Keras examples to the Kaggle MNIST dataset.\n\n*  Added simple prediction based on the distance from the 10 random samples of every class.\n\n*  Added NN embeddings visualisation."}}