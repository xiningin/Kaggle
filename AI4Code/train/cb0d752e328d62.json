{"cell_type":{"11358958":"code","a689c0f9":"code","a1a3d39a":"code","825f9024":"code","d92091e7":"code","90fb9bac":"code","7768be7d":"code","6daa4cc5":"code","140732ef":"code","18752166":"code","3a456315":"markdown","fed96be5":"markdown","2c022f2b":"markdown","dab064ab":"markdown","44fa8549":"markdown","a8eecea9":"markdown","4fb89e76":"markdown","79fcdc30":"markdown"},"source":{"11358958":"%matplotlib inline\nimport matplotlib\nimport seaborn as sns\nsns.set()\nmatplotlib.rcParams['figure.dpi'] = 144","a689c0f9":"import numpy as np\nimport matplotlib.pyplot as plt","a1a3d39a":"import graphviz\nfrom sklearn.datasets import load_iris\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\n\n# load data set\ndata = load_iris()\nX = data['data']\ny = data['target']\n\n# train decision tree\ntree = DecisionTreeClassifier(max_depth=3,)\ntree.fit(X, y)\n\n# visual tree\ngraphviz.Source(export_graphviz(tree, \n                                out_file=None,\n                                feature_names=data['feature_names'],\n                                class_names=data['target_names']))","825f9024":"from ipywidgets import interact, IntSlider\n\ndef iris_tree(depth=1):\n    plt.scatter(X[:, 2], X[:, 3], c=y, cmap=plt.cm.viridis)\n    \n    if depth >= 1:\n        plt.hlines(0.8, 0.8, 7, linewidth=2)\n    if depth >= 2:\n        plt.hlines(1.75, 0.8, 7, linewidth=2)\n    if depth >= 3:\n        plt.vlines(4.85, 1.75, 2.6, linewidth=2)\n        plt.vlines(4.95, 0.8, 1.74, linewidth=2)\n\n    plt.xlabel('Petal Length (cm)')\n    plt.ylabel('Petal Width (cm)')\n    plt.xlim([0.8, 7])\n    plt.ylim([0, 2.6])\n    \ndepth_slider = IntSlider(value=0, min=0, max=3, step=1, description='depth')\ninteract(iris_tree, depth=depth_slider);","d92091e7":"p = np.linspace(1E-6, 1-1E-6, 100)\ngini = p*(1-p) + (1-p)*p\n\nplt.plot(p, gini)\nplt.xlabel('$p$')\nplt.ylabel('Gini');","90fb9bac":"from sklearn.datasets import fetch_california_housing\nfrom sklearn.tree import DecisionTreeRegressor\n\n# load data set\ndata = fetch_california_housing()\nX = data['data']\ny = data['target']\n\n# train decision tree\ntree = DecisionTreeRegressor(max_depth=3)\ntree.fit(X, y)\n\n# visual tree\ngraphviz.Source(export_graphviz(tree, \n                                out_file=None,\n                                feature_names=data['feature_names']))","7768be7d":"from sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\n\nX, y = make_moons(n_samples=250, noise=0.25, random_state=0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\ndef tree_decision_boundary(max_depth=5, min_samples_leaf=2):\n    tree = DecisionTreeClassifier(max_depth=max_depth, min_samples_leaf=min_samples_leaf)\n    tree.fit(X_train, y_train)\n    accuracy = tree.score(X_test, y_test)\n    \n    X1, X2 = np.meshgrid(np.linspace(-2, 3), np.linspace(-2, 2))\n    y_proba = tree.predict_proba(np.hstack((X1.reshape(-1, 1), X2.reshape(-1, 1))))[:, 1]\n    plt.contourf(X1, X2, y_proba.reshape(50, 50),  16, cmap=plt.cm.bwr, alpha=0.75)\n    plt.colorbar()\n\n    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='white', cmap=plt.cm.bwr)\n    plt.xlabel('$x_1$')\n    plt.ylabel('$x_2$')\n    plt.title('accuracy: {}'.format(accuracy));\n\ndepth_slider = IntSlider(min=1, max=40, step=1, description='max depth')\nmin_samples_leaf_slider = IntSlider(min=1, max=20, step=1, description='min leaf size')\ninteract(tree_decision_boundary, max_depth=depth_slider, min_samples_leaf=min_samples_leaf_slider);","6daa4cc5":"from sklearn.datasets import make_regression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nX, y = make_regression(n_samples=1000, n_features=100, n_informative=20, random_state=0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\ndef rf_mse(max_features='sqrt', n_max=50):\n    \"\"\"Generate mean squared errors for growing random forest.\"\"\"\n    \n    rgr = RandomForestRegressor(max_features=max_features,\n                                max_depth=8, n_estimators=1, \n                                warm_start=True, \n                                random_state=0)\n    mse = np.zeros(n_max)\n\n    for n in range(1, n_max):\n        rgr.set_params(n_estimators=n)\n        rgr.fit(X_train, y_train)\n        mse[n-1] = mean_squared_error(y_test, rgr.predict(X_test))\n\n    return mse\n\nfor param in ('sqrt', 'log2'):\n    mse = rf_mse(max_features=param)\n    plt.plot(mse[:-1])\n\nplt.xlabel('number of trees')\nplt.ylabel('mean squared error')\nplt.legend(['sqrt', 'log2', 'all']);","140732ef":"\n\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ndef gb_mse(learning_rate=1.0, subsample=1.0, n_max=80):\n    \"\"\"Generate mean squared errors for growing gradient boosting trees.\"\"\"\n    \n    rgr = GradientBoostingRegressor(learning_rate=learning_rate,\n                                    subsample=subsample,\n                                    max_depth=2, \n                                    n_estimators=1, \n                                    warm_start=True, \n                                    random_state=0)\n    mse = np.zeros(n_max)\n\n    for n in range(1, n_max):\n        rgr.set_params(n_estimators=n)\n        rgr.fit(X_train, y_train)\n        mse[n-1] = mean_squared_error(y_test, rgr.predict(X_test))\n\n    return mse\n\ndef gen_legend_str(hparams):\n    \"\"\"Generate strings for legend in plot.\"\"\"\n    \n    base_str = 'learning rate: {} subsample: {}'\n    \n    return [base_str.format(d['learning_rate'], d['subsample']) for d in hparams]\n\nhparams = ({'learning_rate': 1.0, 'subsample': 1.0},\n           {'learning_rate': 1.0, 'subsample': 0.5},\n           {'learning_rate': 0.75, 'subsample': 0.5},\n           {'learning_rate': 0.5, 'subsample': 0.5})\n\nfor kwargs in hparams:\n    mse = gb_mse(**kwargs)\n    plt.plot(mse[:-1])\n\nlegend_strs = gen_legend_str(hparams)\nplt.xlabel('number of trees')\nplt.ylabel('mean squared error')\nplt.legend(legend_strs);\n\n","18752166":"import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# load data set\ndata = load_iris()\nX = data['data']\ny = data['target']\nfeature_names = data['feature_names']\n\n# tune random forest\ntree = RandomForestClassifier(n_estimators=20, random_state=0)\nparam_grid = {'max_depth': range(2, 10), 'min_samples_split': [2, 4, 6, 8, 10]}\ngrid_search = GridSearchCV(tree, param_grid, cv=3, n_jobs=2, verbose=1, iid=True)\ngrid_search.fit(X, y)\nbest_model = grid_search.best_estimator_\n\n# plot feature importance\ndf = pd.DataFrame({'importance': best_model.feature_importances_}, index=feature_names)\ndf.plot.bar();","3a456315":"# Feature importance","fed96be5":"# Ensemble models","2c022f2b":"## Gini impurity","dab064ab":"# Training decision tree classifiers\n\nThe best way to understand a decision tree is to construct one and visualize it. We'll train a decision tree classifier on the iris data set and visualize the tree with the `Graphviz` package. The iris data set is a famous data set of 150 observations of three different iris species: setosa, versicolor, and virginica. Each observation has measurements of the petal length and width and sepal length and width, for a total of four features.","44fa8549":"## Gradient Boosting Trees","a8eecea9":"# Geometric interpretation","4fb89e76":"## Random forests","79fcdc30":"#  Constructing decision trees for regression"}}