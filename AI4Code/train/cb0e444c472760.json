{"cell_type":{"38604866":"code","44d48265":"code","1b08b77b":"code","012a32c3":"code","e2b791a1":"code","94b4fb74":"code","fc7d85c2":"code","52d4d252":"code","c3b57b17":"code","96db3831":"code","0a70a06e":"code","dc1c30a0":"code","50d8e80c":"code","678cb925":"code","8ba3a86b":"code","fd1b09ac":"code","ffe939cf":"code","730d481b":"code","b56c11a9":"code","f9c9a981":"code","92cea9c2":"code","eeb61b6b":"code","0ec42583":"code","a873cd89":"code","eb0af68b":"markdown","2bd3b83d":"markdown","7ab5d552":"markdown","5293d368":"markdown","5c82c41f":"markdown"},"source":{"38604866":"import tensorflow as tf\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle","44d48265":"def fetch_data(classes):\n  X = []\n  Y = []\n  for c in classes:\n    path = os.path.join('\/content\/dataset', c)\n    filenames = [os.path.join(path, name) for name in os.listdir(path)]\n    X += filenames\n    Y += [c, ] * len(filenames)\n  return pd.DataFrame(zip(X,Y), columns=['filename', 'class'])","1b08b77b":"df = fetch_data(['with_mask', 'without_mask'])\ndf.head()","012a32c3":"df = shuffle(df)\ndf.reset_index(inplace=True, drop=True)","e2b791a1":"fig, ax = plt.subplots(4, 3)\nfig.set_size_inches(15, 20)\n\nfor i in range(12):\n  img = cv2.imread(df.iloc[i]['filename'])\n  img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n  img = cv2.resize(img, (224, 224))\n  ax[i\/\/3, i%3].imshow(img)\n  ax[i\/\/3, i%3].set_title(df.iloc[i]['class'])\n\nfig.tight_layout()","94b4fb74":"train_df, test_df = train_test_split(df, test_size=0.15)\ntrain_df, val_df = train_test_split(train_df, test_size=0.15)","fc7d85c2":"train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n    width_shift_range=0.1,\n    height_shift_range=0.1,\n    brightness_range=(0.9, 1.3),\n    zoom_range=0.1,\n    horizontal_flip=True,\n    rotation_range=10,\n    rescale=1.0\/255\n)\nval_datagen=tf.keras.preprocessing.image.ImageDataGenerator(rescale=1.0\/255)","52d4d252":"BATCH_SIZE = 64\nSIZE = 224\n\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe=train_df,\n    target_size=(SIZE,SIZE),\n    batch_size=BATCH_SIZE,\n    class_mode='binary',\n    shuffle=True,\n    seed=12\n)\nval_generator = val_datagen.flow_from_dataframe(\n    dataframe=val_df,\n    class_mode='binary',\n    target_size=(SIZE,SIZE),\n    batch_size=BATCH_SIZE\n)\ntest_generator = val_datagen.flow_from_dataframe(\n    dataframe=test_df,\n    class_mode='binary',\n    target_size=(SIZE,SIZE),\n    batch_size=BATCH_SIZE\n)","c3b57b17":"from tensorflow.keras.layers import Conv2D, AveragePooling2D, Dense, Flatten, Dropout, BatchNormalization, Input\nfrom tensorflow.keras.models import Model","96db3831":"def build_model(_input):\n  x = Conv2D(64, (3,3), padding='same', activation='relu')(_input)\n  x = Conv2D(64, (3,3), padding='same', activation='relu')(x)\n  x = Conv2D(64, (3,3), padding='same', activation='relu')(x)\n  x = AveragePooling2D(pool_size=(2,2), strides=(2,2))(x)\n  x = BatchNormalization()(x)\n  x = Dropout(0.2)(x)\n  \n  x = Conv2D(128, (3,3), padding='same', activation='relu')(x)\n  x = Conv2D(128, (3,3), padding='same', activation='relu')(x)\n  x = Conv2D(128, (3,3), padding='same', activation='relu')(x)\n  x = AveragePooling2D(pool_size=(2,2), strides=(2,2))(x)\n  x = BatchNormalization()(x)\n  x = Dropout(0.2)(x)\n\n  x = Conv2D(256, (3,3), padding='same', activation='relu')(x)\n  x = Conv2D(256, (3,3), padding='same', activation='relu')(x)\n  # x = Conv2D(256, (3,3), padding='same', activation='relu')(x)\n  x = AveragePooling2D(pool_size=(2,2), strides=(2,2))(x)\n  x = BatchNormalization()(x)\n  x = Dropout(0.2)(x)\n\n  x = Conv2D(512, (3,3), padding='same', activation='relu')(x)\n  x = Conv2D(512, (3,3), padding='same', activation='relu')(x)\n  # x = Conv2D(512, (3,3), padding='same', activation='relu')(x)\n  x = AveragePooling2D(pool_size=(2,2), strides=(2,2))(x)\n  x = BatchNormalization()(x)\n  x = Dropout(0.15)(x)\n\n  x = Conv2D(512, (3,3), padding='same', activation='relu')(x)\n  x = Conv2D(512, (3,3), padding='same', activation='relu')(x)\n  # x = Conv2D(512, (3,3), padding='same', activation='relu')(x)\n  x = AveragePooling2D(pool_size=(2,2), strides=(2,2))(x)\n  x = BatchNormalization()(x)\n  x = Dropout(0.1)(x)\n\n  x = Flatten()(x)\n  x = Dense(512, activation='relu')(x)\n  x = Dense(1, activation='sigmoid')(x)\n\n  return x","0a70a06e":"_input = Input((SIZE,SIZE,3))\noutput = build_model(_input)\n\nmodel = Model(_input, output)\n\nmodel.summary()","dc1c30a0":"losses = []\nval_losses = []\n\naccuracies = []\nval_accuracies = []\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])","50d8e80c":"hist = model.fit(train_generator, batch_size=BATCH_SIZE, epochs=10, verbose=1, validation_data=val_generator)","678cb925":"losses += hist.history['loss']\nval_losses += hist.history['val_loss']\n\naccuracies += hist.history['accuracy']\nval_accuracies += hist.history['val_accuracy']\n","8ba3a86b":"plt.plot(losses, 'r-', label='loss')\nplt.plot(val_losses, 'b-', label='val_loss')\nplt.legend()\nplt.show()","fd1b09ac":"plt.plot(accuracies, 'm--', label='accuracy')\nplt.plot(val_accuracies, 'g--', label='val_accuracy')\nplt.legend()","ffe939cf":"def get_accuracy(pred):\n  pred = pred[0][0]\n  if pred > 0.5:\n    return int(pred * 100)\n  return int((1-pred) * 100)","730d481b":"ret = model.evaluate(test_generator, batch_size=64)","b56c11a9":"batch_img, batch_label = next(test_generator)","f9c9a981":"fig, ax = plt.subplots(4, 3)\nfig.set_size_inches(15, 20)\nlabels_encoder = ['with_mash', 'without_mask']\n\nposes = np.random.choice(range(64), 12, replace=False)\nfor i in range(12):\n  img, gt = batch_img[poses[i]], batch_label[poses[i]]\n  pred = model.predict(img[np.newaxis, ...])\n  img = (img * 255).astype(np.uint8)\n  # img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n  ax[i\/\/3, i%3].imshow(img)\n  title = 'Ground truth: {0}\\nPredict: {1}   {2}%'.format(labels_encoder[int(gt)], labels_encoder[int(pred)], get_accuracy(pred))\n  ax[i\/\/3, i%3].set_title(title)\n  \n\nfig.tight_layout()","92cea9c2":"!wget https:\/\/vinmec-prod.s3.amazonaws.com\/images\/20200203_093825_470517_unnamed.max-1800x1800.jpg -O image.jpg\n!wget http:\/\/benhvienthanhvubaclieu.com\/cms\/static\/site\/sale_medicbaclieu\/uploads\/ckeditor\/images.thumb.d06d5e1c-4cb4-4828-b607-e555aa58a618.jpg -O image1.jpg","eeb61b6b":"img = cv2.imread('image.jpg')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nimg = cv2.resize(img, (224, 224))\nimg = img \/ 255.0\n","0ec42583":"pred = model.predict(img[np.newaxis, ...])","a873cd89":"plt.imshow(img)\ntitle = '{0} {1}%'.format(labels_encoder[int(pred)], get_accuracy(pred))\nplt.title(title)","eb0af68b":"## Training","2bd3b83d":"## Build model","7ab5d552":"## Prepare Data","5293d368":"## Eveluate","5c82c41f":"#### Test with arbitrary image"}}