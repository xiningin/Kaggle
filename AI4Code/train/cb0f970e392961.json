{"cell_type":{"238de289":"code","1a760a0a":"code","6e398984":"code","a10ca24e":"code","487f0d6f":"code","67acb783":"code","18f6aaa7":"code","7bc2e51d":"code","e3692f73":"code","ab232307":"code","b7106f25":"code","b92ca81a":"code","ae77328b":"code","9b0c1cd6":"code","17f17d28":"code","713b9798":"code","9d0d4963":"code","dfdfcae5":"code","734ab3d0":"code","f1ecde29":"code","74a7ad19":"code","39827809":"code","b03887bd":"code","f78afa6f":"code","dba13937":"code","d765938b":"code","4cf4eeb8":"code","089baba9":"code","48026120":"code","a57ae05b":"code","d7e2f49f":"code","b7da5cb4":"code","5873386e":"code","d9e2ae7b":"code","a675d178":"code","b2ce17b0":"code","0c9226b1":"code","0623b3de":"code","b41afb9a":"code","1279c2b1":"code","c792b7d4":"code","cea1927a":"code","34830769":"markdown","47a42feb":"markdown","9e19d410":"markdown","07d8ac41":"markdown","c590f47a":"markdown","43626d37":"markdown","e50bda71":"markdown","30c977ff":"markdown","a4a2182e":"markdown","c135877d":"markdown","f9e2732a":"markdown","d2f14b61":"markdown","e5c6f16a":"markdown","6c22016d":"markdown","e133aefb":"markdown","9f28ceef":"markdown","20bb99b7":"markdown","e764fb7b":"markdown","d52ecc3d":"markdown","3eb3805d":"markdown","92745c5e":"markdown","f2e656a4":"markdown","d551a3c4":"markdown","effd47e4":"markdown","274ac02a":"markdown","d5fa29eb":"markdown","c67d9fb2":"markdown"},"source":{"238de289":"#Load the librarys\nimport pandas as pd #To work with dataset\nimport numpy as np #Math library\nimport seaborn as sns #Graph library that use matplot in background\nimport matplotlib.pyplot as plt #to plot some parameters in seaborn\nimport warnings\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.neighbors import KNeighborsClassifier\n# Import StandardScaler from scikit-learn\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.manifold import TSNE\nfrom sklearn.model_selection import GridSearchCV\n# Import train_test_split()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom datetime import datetime, date\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.model_selection import cross_val_score\nimport lightgbm as lgbm\nimport  tensorflow as tf \nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport smogn\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport lightgbm as lgb\nfrom sklearn.neighbors import KNeighborsRegressor \n#import smong \nfrom sklearn.linear_model import LinearRegression, RidgeCV\nwarnings.filterwarnings('ignore')","1a760a0a":"# Read spreadsheet and assign it to swiss_loan\nswiss_loan= pd.read_excel('..\/input\/loan-credit-risk\/ATUCE_Case_study_data_2021.xlsx')","6e398984":"# Create a boolean mask on whether each feature less than 40% missing values.\nmask = swiss_loan.isna().sum() \/ len(swiss_loan) < 0.4\n# Create a reduced dataset by applying the mask\nreduced_df =swiss_loan.loc[:, mask]\n\n# drop ID\nreduced_df.drop('ID', axis=1, inplace=True)\nreduced_df['Pays_corr']=reduced_df['Pays'].str.strip()\nreduced_df['Taux_corr']=reduced_df['Taux'].str.replace('%','').str.strip().str.replace(',','.').str.extract(r'(\\d+.\\d+)')\nreduced_df['Taux_corr'] = np.where(reduced_df['Taux_corr'].isnull(), 0.1, reduced_df['Taux_corr'])\nreduced_df['Taux_corr'] = pd.to_numeric(reduced_df['Taux_corr'], errors='coerce')\n# Specify the boundaries of the bins\nbins = [0.001,5.5,  6.5, 10]\n# Bin labels\nlabels = [ 'Low', 'Medium', 'High']\n# Bin the continuous variable ConvertedSalary using these boundaries\nreduced_df['Taux_corr_binned'] = pd.cut(reduced_df['Taux_corr'], \n                                         bins=bins,labels=labels )\n# Print the first 5 rows of the boundary_binned column\nreduced_df['Montant_corr']=reduced_df['Montant'].str.replace('\u20ac','').str.replace('\\xa0','').str.strip().str.replace('\\s+','')\nreduced_df['Montant_corr'] = np.where(reduced_df['Montant_corr'].isnull(), 0.1, reduced_df['Montant_corr'])\nreduced_df['Montant_corr'] = pd.to_numeric(reduced_df['Montant_corr'], errors='coerce')\nreduced_df['Niveau_risque_corr']=reduced_df['Niveau de risque'].str.rstrip().str.replace('\\s+','')\nEmprunteurs = reduced_df['Emprunteur']\n\n\nEmprunteurs_counts = Emprunteurs.value_counts()\n\n# Create a mask for only categories that occur less than 5 times\nmask = Emprunteurs.isin(Emprunteurs_counts[Emprunteurs_counts<5].index)\n# Label all other categories as Other\nreduced_df['Emprunteur'][mask] = 'Other'\nreduced_df['capital_social_corr']=reduced_df['capital social'].str.replace('\u20ac','').str.replace('\\xa0','').str.strip().str.replace('\\s+','')\nreduced_df['capital_social_corr'] = np.where(reduced_df['capital_social_corr'].isnull(), 0.1, reduced_df['capital_social_corr'])\nreduced_df['capital_social_corr'] = pd.to_numeric(reduced_df['capital_social_corr'], errors='coerce')\n\nreduced_df['Effectifse_corr']=reduced_df['effectifs'].str.rstrip().str.replace('\\s+','')\nreduced_df['Effectifse_corr'][reduced_df['Effectifse_corr'] == '-'] = np.nan\n\nreduced_df['Nombre_mois_p\u00e9riode16_corr']=reduced_df['Nombre de mois de la p\u00e9riode 16'].str.rstrip().str.replace('mois','').str.replace(',','.').str.replace('\\s+','').str.extract(r\"(\\d+\\.\\d+|\\d+)\")\nreduced_df['Nombre_mois_p\u00e9riode16_corr'][reduced_df['Nombre_mois_p\u00e9riode16_corr'] == '-'] = np.nan\nreduced_df['Nombre_mois_p\u00e9riode16_corr'] = pd.to_numeric(reduced_df['Nombre_mois_p\u00e9riode16_corr'], errors='coerce')\n\nreduced_df['Chiffre_Affaires_16_corr']=reduced_df.iloc[:,12].str.replace('\\xa0','').str.strip().str.replace('\\s+','')\nreduced_df['Chiffre_Affaires_16_corr'] = pd.to_numeric(reduced_df['Chiffre_Affaires_16_corr'], errors='coerce')\n\nreduced_df['Total_Bilan_16_corr']= reduced_df['Total Bilan 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['Total_Bilan_16_corr']= pd.to_numeric(reduced_df['Total_Bilan_16_corr'], errors='coerce')\n\nreduced_df['Capacit\u00e9_remboursement_FCCR_16_corr']= reduced_df['Capacit\u00e9 de remboursement (FCCR) 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.replace(',','.').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['Capacit\u00e9_remboursement_FCCR_16_corr']= pd.to_numeric(reduced_df['Capacit\u00e9_remboursement_FCCR_16_corr'], errors='coerce')\n\n\nreduced_df['Fonds_Propres_16_corr']= reduced_df['Fonds Propres 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.replace(',','.').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['Fonds_Propres_16_corr']= pd.to_numeric(reduced_df['Fonds_Propres_16_corr'], errors='coerce')\n\nreduced_df['Fonds_Propres_Total_Bilan_corr']= reduced_df['Fonds Propres \/ Total Bilan 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.replace(',','.').str.replace('%','').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['Fonds_Propres_Total_Bilan_corr']= pd.to_numeric(reduced_df['Fonds_Propres_Total_Bilan_corr'], errors='coerce')\n\nreduced_df['Dettes_Nettes_EBE_16_corr']= reduced_df['Dettes Nettes \/ EBE(* ann\u00e9es) 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.replace(',','.').str.replace('*','').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['Dettes_Nettes_EBE_16_corr']= pd.to_numeric(reduced_df['Dettes_Nettes_EBE_16_corr'], errors='coerce')\n\nreduced_df['DettesNettes_Fonds_propres_16_corr']= reduced_df['Dettes Nettes \/ Fonds propres 16'].str.replace('\\xa0','').str.strip().str.replace('\\s+','').str.replace(',','.').str.replace('%','').str.extract(r\"([-+]?\\d*\\.*\\d+|\\d+)\")\nreduced_df['DettesNettes_Fonds_propres_16_corr']= pd.to_numeric(reduced_df['DettesNettes_Fonds_propres_16_corr'], errors='coerce')\n\n\n# Apply the log normalization function \nreduced_df['Montant_corr_log'] = np.log(reduced_df['Montant_corr'])\n\n# Apply the log normalization function \nreduced_df['Chiffre_Affaires_16_corr_log'] = np.log(reduced_df['Chiffre_Affaires_16_corr'])\n# Apply the log normalization function t\nreduced_df['capital_social_corr_log'] = np.log(reduced_df['capital_social_corr']+1)\n# This function converts given date to age\ndef age(creation):\n    born = int(creation)\n    today = date.today()\n    return today.year - born\n  \nreduced_df['Age'] = reduced_df['ann\u00e9e de cr\u00e9ation'].apply(age)","a10ca24e":"reduced_df.shape","487f0d6f":"reduced_df.columns","67acb783":"list_to_keep= [ 'Pays_corr','Mois','Age' ,'Taux_corr', 'Taux_corr_binned', 'Montant_corr_log',\n       'Niveau_risque_corr','Emprunteur', 'capital_social_corr_log', 'Effectifse_corr',\n       'Nombre_mois_p\u00e9riode16_corr', 'Capacit\u00e9_remboursement_FCCR_16_corr','Total_Bilan_16_corr',\n       'Fonds_Propres_16_corr', 'Fonds_Propres_Total_Bilan_corr',\n       'Dettes_Nettes_EBE_16_corr', 'DettesNettes_Fonds_propres_16_corr','Chiffre_Affaires_16_corr_log']\nclean_reduced_df= reduced_df[list_to_keep].copy()\nclean_reduced_df.shape","18f6aaa7":"# Create arrays for the features and the response variable\ncolonne_cible = \"Taux_corr\"\n\nData_regression =clean_reduced_df.drop(['Taux_corr_binned'], axis=1).copy()\nX = Data_regression.drop(['Taux_corr'], axis=1)\ny= Data_regression['Taux_corr']\n# Split the dataset and labels into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\nprint(\"{} rows in test set vs. {} in training set. {} Features.\".format(X_test.shape[0], X_train.shape[0], X_test.shape[1]))","7bc2e51d":"# select the float columns\nnum_columns = X.select_dtypes(include=['int64','float64']).columns\n# select non-numeric columns\ncat_columns = X.select_dtypes(exclude=['int64','float64']).columns\n","e3692f73":"num_columns=['Mois', 'Age', 'Montant_corr_log', 'capital_social_corr_log',\n       'Nombre_mois_p\u00e9riode16_corr', 'Capacit\u00e9_remboursement_FCCR_16_corr',\n       'Fonds_Propres_16_corr', 'Fonds_Propres_Total_Bilan_corr',\n       'Dettes_Nettes_EBE_16_corr', 'DettesNettes_Fonds_propres_16_corr',\n       'Chiffre_Affaires_16_corr_log']\ncat_columns=['Pays_corr', 'Niveau_risque_corr', 'Emprunteur',\n       'Effectifse_corr']\nall_columns = (num_columns\n               +cat_columns)\n\nif set(all_columns) == set(X1.columns):\n    print('Ok')\nelse:\n    # Alors je veux voir les diff\u00e9rences\n    print('dans all_columns mais pas dans X1.columns   :', set(all_columns) - set(X1.columns))\n    print('dans X.columns   mais pas dans all_columns :', set(X1.columns) - set(all_columns))","ab232307":"\"==>i have just create Taux_corr_binned in  order to do a classifcation task \"","b7106f25":"fill_missing_then_one_hot_encoder = make_pipeline(\n    SimpleImputer(strategy='constant', fill_value='manquante',add_indicator=True),\n    OneHotEncoder(handle_unknown='ignore')\n)","b92ca81a":" fill_missing_then_StandardScaler = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n    StandardScaler()\n)","ae77328b":"data_preprocess = make_column_transformer(\n    ( fill_missing_then_one_hot_encoder , cat_columns),\n    ( fill_missing_then_StandardScaler, num_columns)\n)\n","9b0c1cd6":"data_preprocess.fit(X)\ndata_preprocess.transform(X)\ndata_preprocess.transform(X_test)\n\nprint(\"Ok , Every thing is well \")","17f17d28":"cross_validation_design = KFold(n_splits=5,\n                                shuffle=True,\n                                random_state=77)\n\ncross_validation_design","713b9798":"from sklearn.neighbors import KNeighborsRegressor as KNN\n\n# params = {'n_neighbors':[2,3,4,5,6,7,8,9]}\n\n\nKNN_MODEL = {}\n\n# D\u00e9finir la pipeline\nKNN_MODEL['pipeline'] = Pipeline([\n                                  ('data_process', data_preprocess),\n                                  ('knn', KNN())\n                                  ])\n\n# D\u00e9finir la grille\nKNN_MODEL['hyperparams'] = {}\nKNN_MODEL['hyperparams']['knn__n_neighbors'] = [1, 3,5,8, 9,10,11,15, 20,21, 51]\nKNN_MODEL['hyperparams']['knn__weights'] = ['uniform','distance']\nKNN_MODEL['hyperparams']['knn__metric'] = ['euclidean', 'manhattan', 'minkowski']\n\n# Effectuer la GridSearch\nKNN_MODEL['gridsearch'] = GridSearchCV(\n    estimator=KNN_MODEL['pipeline'],\n    param_grid=KNN_MODEL['hyperparams'],\n    cv=cross_validation_design,\n    scoring='r2'\n    )\n\nKNN_MODEL['gridsearch'].fit(X_train, y_train)","9d0d4963":"KNN_MODEL['gridsearch'].best_params_","dfdfcae5":"KNN_MODEL['gridsearch'].best_score_","734ab3d0":"from sklearn.linear_model import LinearRegression\n\n# D\u00e9finir la pipeline\nREGRESSION_MODEL = {}\nREGRESSION_MODEL['pipeline'] = Pipeline([\n                                        ('data_cleaning', data_preprocess ),\n                                        ('reg', LinearRegression())\n])\n\n# D\u00e9finir la grille\nREGRESSION_MODEL['hyperparams'] = {}\n#REGRESSION_MODEL['hyperparams']['reg__alpha'] = np.arange(.1, 10., .1)\n\n# Faire la recherche\nREGRESSION_MODEL['gridsearch'] = GridSearchCV(\n                                      estimator=REGRESSION_MODEL['pipeline'],\n                                      param_grid=REGRESSION_MODEL['hyperparams'],\n                                      cv=cross_validation_design,\n                                      scoring='r2'\n                                      )\n\nREGRESSION_MODEL['gridsearch'].fit(X_train, y_train)","f1ecde29":"model = REGRESSION_MODEL['gridsearch'].fit(X_train, y_train)\nr_sq = model.score(X_train, y_train)\nprint('coefficient of determination:', r_sq)","74a7ad19":"y_pred = model.predict(X_test)\ndf_LR = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\ndf_LR.tail()","39827809":"from sklearn import metrics\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))","b03887bd":"REGRESSION_MODEL['gridsearch'].best_score_","f78afa6f":"# instantiate linear regression object\nridge = RidgeCV()\nridge_MODEL = Pipeline([\n                                        ('data_cleaning', data_preprocess ),\n                                        ('reg', ridge)\n])\n# fit or train the linear regression model on the training set and store parameters\nridge_MODEL.fit(X_train, y_train)\n\nr_sq = ridge_MODEL.score(X_train, y_train)\nprint('coefficient of determination:', r_sq)","dba13937":"# use trained RidgeCV regression model to predict interest rates of training and test data\n\ny_pred = ridge_MODEL.predict(X_test)\ndf_Ridge = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n\n# print RMSE of training predictions\nprint('RMSE on training data: ', np.sqrt(mean_squared_error(y_test, y_pred)))\ndf_Ridge.head()","d765938b":"from sklearn.svm import SVR\nregressor = SVR()\nSVR_MODEL = {}\n\n# D\u00e9finir la pipeline\nSVR_MODEL['pipeline'] = Pipeline([\n                                  ('data_process', data_preprocess),\n                                  ('SVR', regressor)\n                                  ])\n\n# D\u00e9finir la grille\nSVR_MODEL['hyperparams'] = {}\nSVR_MODEL['hyperparams']['SVR__kernel'] = ['linear', 'poly', 'rbf', 'sigmoid', 'precomputed']\nSVR_MODEL['hyperparams']['SVR__degree'] = [0,1,2, 3, 4,5,6,7,8,9]\nSVR_MODEL['hyperparams']['SVR__gamma'] = ['scale', 'auto',1, 0.1, 0.01, 0.001, 0.0001]\nSVR_MODEL['hyperparams']['SVR__C'] = [50, 10, 1.0, 0.1, 0.01,100]\n\n# Effectuer la GridSearch\nSVR_MODEL['gridsearch'] = GridSearchCV(\n    estimator=SVR_MODEL['pipeline'],\n    param_grid=SVR_MODEL['hyperparams'],\n    cv=cross_validation_design,\n    scoring='r2'\n    )\n\n#Define SVR classifier\nSVR_MODEL['gridsearch'].fit(X_train, y_train)\nsvr_accuracy = SVR_MODEL['gridsearch'].score(X_test, y_test)\n\nprint('Support Vector Classifier Accuracy : ', svr_accuracy)","4cf4eeb8":"regressor.get_params().keys()","089baba9":"SVR_MODEL['gridsearch'].best_params_","48026120":"\"\"\"{'SVR__C': 10, 'SVR__degree': 0, 'SVR__gamma': 1, 'SVR__kernel': 'rbf'}\"\"\"","a57ae05b":"from xgboost import XGBRegressor\nXGBR = XGBRegressor()\nXGBR_MODEL = {}\n# D\u00e9finir la pipeline\nXGBR_MODEL['pipeline'] = Pipeline([\n                                  ('data_process', data_preprocess),\n                                  ('XGBR', XGBR)\n                                  ])\n\n# D\u00e9finir la grille\nXGBR_MODEL['hyperparams'] = {}\n#XGBR_MODEL['hyperparams']['XGBR__n_estimators'] = [10,20]\n#XGBR_MODEL['hyperparams']['XGBR__colsample_bytree'] = [0.7, 0.8]\n#XGBR_MODEL['hyperparams']['XGBR__max_depth'] = [5,7,15]\n#XGBR_MODEL['hyperparams']['XGBR__reg_alpha'] = [1.1, 1.2 ]\n#XGBR_MODEL['hyperparams']['XGBR__reg_lambda'] = [1.1, 1.2, 1.3]\n#XGBR_MODEL['hyperparams']['XGBR__subsample'] = [0.7, 0.8, 0.9]\n# Effectuer la GridSearch\nXGBR_MODEL['gridsearch'] = GridSearchCV(\n    estimator=XGBR_MODEL['pipeline'],\n    param_grid=XGBR_MODEL['hyperparams'],\n    cv=cross_validation_design,\n    scoring='r2'\n    )\n#Define SVR classifier\nXGBR_MODEL['gridsearch'].fit(X_train, y_train)\nXGBR_accuracy = XGBR_MODEL['gridsearch'].score(X_test, y_test)\nprint('SXGBR Accuracy : ', XGBR_accuracy)","d7e2f49f":"XGBR_MODEL['gridsearch'].best_params_","b7da5cb4":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n        return self\n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)\nENet = ElasticNet()\nlasso = Lasso()  \nKRR = KernelRidge()\nGBoost = GradientBoostingRegressor()\nmodel_xgb = xgb.XGBRegressor()\nmodel_lgb = lgb.LGBMRegressor()\naveraged_models = AveragingModels(models = (ENet,model_lgb,model_xgb, GBoost, KRR, lasso))\naveraged_models_pipe = Pipeline([('data_cleaning', data_preprocess),\n                        ('Stack_reg1', averaged_models)\n                        ])\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(averaged_models_pipe,X_train, y_train, scoring=\"neg_mean_squared_error\", cv = cross_validation_design))\n    return(rmse)\ndef r2_cv(model):\n    rmse= np.sqrt(-cross_val_score(averaged_models_pipe,X_train, y_train, scoring=\"r2\", cv = cross_validation_design))\n    return(rmse)\nscore = rmse_cv(averaged_models)\nr2_score = r2_cv(averaged_models)\nprint(\" Averaged RMSE base models score: {:.4f}\".format(score.mean()))\nprint(\" Averaged R2 base models score: {:.4f}\".format(r2_score.mean()))","5873386e":"from sklearn.datasets import load_diabetes\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.svm import LinearSVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import StackingRegressor\n\n\nestimators = [('Ridge', RidgeCV()),\n    ('svr', LinearSVR(random_state=42)) ,\n    ('XGBR',XGBRegressor()),\n    ('KNN',KNeighborsRegressor())]\n     \nStack_reg1 = StackingRegressor(estimators=estimators,final_estimator=RandomForestRegressor(n_estimators=10, random_state=42))\nStack_reg1_pipe = Pipeline([('data_cleaning', data_preprocess),\n                        ('Stack_reg1', Stack_reg1)\n                        ])\n\nStack_reg1_pipe.fit(X_train, y_train)    \nprint(\"Mean Squared Error: %.4f\"% np.mean((Stack_reg1_pipe.predict(X_test) - y_test) ** 2))\nprint('Variance Score: %.4f' % Stack_reg1_pipe.score(X_test, y_test))","d9e2ae7b":"from mlxtend.regressor import StackingRegressor\nfrom mlxtend.data import boston_housing_data\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\n# Initializing models\n\nlr = LinearRegression()\nsvr_lin = SVR(kernel='linear')\nridge = Ridge(random_state=1)\nsvr_rbf = SVR(kernel='rbf')\n\nStack_reg2= StackingRegressor(regressors=[svr_lin, lr, ridge], \n                           meta_regressor=svr_rbf)\n\n# Training the stacking classifier\n\nStack_reg2_pipe = Pipeline([('data_cleaning', data_preprocess),\n                        ('Stack_reg2', Stack_reg2)\n                        ])\n\nStack_reg2_pipe.fit(X_train, y_train)    \nprint(\"Mean Squared Error: %.4f\"% np.mean((Stack_reg2_pipe.predict(X_test) - y_test) ** 2))\nprint('Variance Score: %.4f' % Stack_reg2_pipe.score(X_test, y_test)) ","a675d178":"from mlxtend.regressor import StackingRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\nimport numpy as np\nimport warnings\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Lasso\n\n# Initializing models\n\nlr = LinearRegression()\nsvr_lin = SVR(kernel='linear')\nridge = Ridge(random_state=1)\nlasso = Lasso(random_state=1)\nsvr_rbf = SVR(kernel='rbf')\nregressors = [svr_lin, lr, ridge, lasso]\nstregr3 = StackingRegressor(regressors=regressors, \n                           meta_regressor=svr_rbf)\n\n\n#params = {'lasso__alpha': [0.1, 1.0, 10.0],\n          #'ridge__alpha': [0.1, 1.0, 10.0],\n          #'svr__C': [0.1, 1.0, 10.0],\n          #'meta_regressor__C': [0.1, 1.0, 10.0, 100.0],\n          #'meta_regressor__gamma': [0.1, 1.0, 10.0]}\n\nStack_reg3_pipe = Pipeline([('data_cleaning', data_preprocess),\n                        ('Stack_reg3', stregr3)\n                        ])\n\n#grid_reg3_pipe = GridSearchCV(estimator=Stack_reg3_pipe, param_grid=params, cv=cross_validation_design, refit=True)\n\nStack_reg3_pipe.fit(X_train, y_train)    \nprint(\"Mean Squared Error: %.4f\"% np.mean((Stack_reg3_pipe.predict(X_test) - y_test) ** 2))\nprint('Variance Score in test : %.4f' % Stack_reg3_pipe.score(X_test, y_test))\n\n#print(\"Best score o, training : %f using %s\" % (grid_reg3_pipe.best_score_, grid_reg3_pipe.best_params_))","b2ce17b0":"def build_model():\n    model1 = tf.keras.Sequential()\n    model1.add( layers.Dense(30, activation='relu',\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=0.01, l2=0.01),\n                             input_shape=[X_pre.shape[1]]))\n    model1.add(layers.Dropout(0.2))\n    model1.add(layers.Dense(20, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n    model1.add(layers.Dropout(0.2))\n    model1.add(layers.Dense(10, activation='relu',kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n    model1.add(layers.Dropout(0.1))\n    model1.add(layers.Dense(1,activation='relu'))  \n    #optimizer='adam'\n    optimizer =tf.keras.optimizers.Adam()\n    #optimizer = tf.keras.optimizers.RMSprop(0.001)\n     ## Compile model\n    #epochs = 50\n    #learning_rate = 0.1\n    #decay_rate = learning_rate \/ epochs\n    #momentum = 0.8\n    #sgd = SGD(lr=learning_rate, momentum=momentum, decay=decay_rate, nesterov=False)\n    #https:\/\/machinelearningmastery.com\/tensorflow-tutorial-deep-learning-with-tf-keras\/\n    #optimizer=sgd\n    model1.compile(loss='mse',\n                optimizer=optimizer,\n                metrics=['mae', 'mse'])\n    return model1\n\nMLP = build_model()\nMLP.summary()","0c9226b1":"EPOCHS =1000\n# configure early stopping\nes = EarlyStopping(monitor='val_loss',min_delta=0.0000000000001, patience=5)\n#batch_size=1000\n#MLP_pipe = Pipeline([('data_cleaning', data_preprocess),('MLP', MLP)])\nX = Data_regression.drop(['Taux_corr'], axis=1)\ny_pre= Data_regression['Taux_corr'].to_numpy()\nX_pre=data_preprocess.fit_transform(X)\nhistory = MLP.fit(X_pre,y_pre,batch_size=32,epochs=EPOCHS, validation_split = 0.1, verbose=0 ,callbacks=[es])","0623b3de":"MLP.save('my_modelMLP.h5') ","b41afb9a":"#Let\u2019s see what this looks like when we plot our respective losses:\n# \"Loss\"\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","1279c2b1":"# evaluate the keras model\nloss, mae, mse = MLP.evaluate( X_pre, y_pre, verbose=2)\nprint(\" Mean Abs Error: {:5.2f} MPG\".format(mse))","c792b7d4":"data_preprocess = make_column_transformer(\n    ( fill_missing_then_one_hot_encoder , cat_columns),\n    ( fill_missing_then_StandardScaler, num_columns)\n)\n\nmodel_final = Pipeline([('data_cleaning', data_preprocess),\n                        ('ridge', ridge\n                        )])\n\n\n# on fit la meilleur pipe sur toute nos donn\u00e9es de train\nmodel_final.fit(X, y)","cea1927a":"# use trained RidgeCV regression model to predict interest rates of training and test data\n\ny_pred = model_final.predict(X_test)\ndf_model_final = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n\n# print RMSE of training predictions\nprint('RMSE on training data: ', np.sqrt(mean_squared_error(y_test, y_pred)))\ndf_model_final.tail()","34830769":"# SVR pipe ","47a42feb":"# Pipe Numeric Columns : ","9e19d410":"#  Model Selection\n\nIn machine learning, we usually select our final model after evaluating several candidate models. This process is called model selection. Sometimes the models subject to comparison are fundamentally different in nature (say, decision trees vs. linear models). At other times, we are comparing members of the same class of models that have been trained with different hyperparameter settings.\n\nWith MLPs, for example, we may wish to compare models with different numbers of hidden layers, different numbers of hidden units, and various choices of the activation functions applied to each hidden layer. In order to determine the best among our candidate models, we will typically employ a validation dataset.","07d8ac41":"# Model 1 : REGRESSION : predict \"Taux_corr\"\n## Extract X and y","c590f47a":"# Model Stacking\nImprove your Predictive Model\u2019s Score using a Stacking Regressor\n\n      \u201cThe whole is greater than the sum of its parts.\u201d \u2013 Aristotle\n## Type 1: Simplest Stacking Regressor approach: Averaging Base models\n\nWe begin with this simple approach of averaging base models. Build a new class to extend scikit-learn with our model and also to leverage encapsulation and code reuse.\n\n### Averaged base models class","43626d37":"### Second Stacking Model : ","e50bda71":"# Compose num+cat : ColumnTransformer","30c977ff":"# Keep Clean Columns","a4a2182e":"# Preprocess Pipeline (preprocessing ,cleaning , features eng, features selection)","c135877d":"# Load the librarys","f9e2732a":"# check that we have all column ","d2f14b61":"##  XGBOOST Regressor  pipe :\n","e5c6f16a":"##  Third Stacking Model  ","6c22016d":"# Train Model :","e133aefb":"# Create complexe transformer  in order to  put all transformations in the same pipe \nIn our case :\n\n    'num_columns' :Cleaning->Valeur Manquante -> Standar_Scaler\n    'cat_columns' : Cleaning -> Valeur Manquante -> Categorique [One Hot]\n**==> Cleaning is done by pandas , in the future this step sould be intergeted the pipe using \"custumer Function\" like this Example:**\n\n    fill_missing_then_Standar_scaler = make_pipeline( SimpleImputer(strategy='median',add_indicator=True),\n        StandardScaler()\n    )\n\n    \"Write a pattern to extract numbers and decimals\"\n    def return_number(string):\n        pattern = re.compile(r\"\\d+\\.\\d+\")\n        # Search the text for matches\n        number = re.match(pattern, string)\n        # If a value is returned, use group(0) to return the found value\n        if number is not None:\n            return float(number.group(0))\n\n    extraire_number_then_imput_then_scale = make_pipeline(\n        FunctionTransformer(extract_number),\n        fill_missing_then_Standar_scaler,\n    )    \n","9f28ceef":"# Deep Learning Approch :\n## Step Model Life-Cycle\nA model has a life-cycle,modeling a dataset and understanding the tf.keras API.\nThe five steps in the life-cycle are as follows:\n   - Define the model.\n   - Compile the model.\n   - Fit the model.\n   - Evaluate the model.\n   - Make predictions.\n\n","20bb99b7":"#### you can obtain the coefficient of determination (\ud835\udc45\u00b2) with .score() called on model","e764fb7b":"### KNN pipe","d52ecc3d":"# Find Best Pipe\n## Step 1: Cross-Validation","3eb3805d":"# Select the best model : retrain on all the data(without cross_val)with the best param \n\n This pipline will be used in production ","92745c5e":"# Pipe Cat columns : \n","f2e656a4":"# Sum up :\n##  Dataset Size\n\nThe other big consideration to bear in mind is the dataset size. Fixing our model, the fewer samples we have in the training dataset, the more likely (and more severely) we are to encounter overfitting. As we increase the amount of training data, the generalization error typically decreases. Moreover, in general, more data never hurt. For a fixed task and data distribution, there is typically a relationship between model complexity and dataset size. Given more data, we might profitably attempt to fit a more complex model. Absent sufficient data, simpler models may be more difficult to beat. For many tasks, deep learning only outperforms linear models when many thousands of training examples are available. \n# Underfitting \n\nUnderfitting is a modeling error which occurs when a function does not fit the data points well enough. It is the result of a simple model with an insufficient number of training points. A model that is under fitted is inaccurate because the trend does not reflect the reality of the data.\nHandling Underfitting:\n\n    Get more training data.\n    Increase the size or number of parameters in the model.\n    Increase the complexity of the model.\n    Increasing the training time, until cost function is minimised.\n\nWith these techniques, you should be able to improve your models and correct any overfitting or underfitting issues.\n# Prefer Confidence Intervals to Point Estimates with use of synthetic samples\n**Regression problem to a Classification problem**","d551a3c4":"## Check ","effd47e4":"## Type 2: Adding a Meta-model\nThe meta-model is used to find the pattern between the base model predictions as features and actual predictions as the target variables.\n## First Stacking Model :","274ac02a":"# What should we do for each colmun ","d5fa29eb":"# Load Data","c67d9fb2":"### LR pipe"}}