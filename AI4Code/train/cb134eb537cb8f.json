{"cell_type":{"75ce7781":"code","181b9b22":"code","4baccbf0":"code","894a9869":"code","6bda78ce":"code","17e775b7":"code","be3251e2":"code","6334ac3b":"code","aa48babb":"code","af7af5ca":"code","134b3163":"code","fff8df31":"code","09605e94":"code","3ea7b500":"code","ed47001a":"code","655df595":"code","5d530d71":"code","1b657800":"code","5692b89a":"code","aa812b08":"code","143972ac":"code","8a37e2bb":"code","9773da57":"code","c9d05a07":"code","6f2ab356":"code","b042c93e":"code","14e098ec":"code","e025765d":"code","7a02aa98":"code","ac097640":"code","6c023038":"markdown","cfdc004b":"markdown","fe9400e4":"markdown","308c554a":"markdown","f149285a":"markdown","f1a69eb3":"markdown","c6c75568":"markdown","a0f4735a":"markdown","1f9c406d":"markdown","caf1860a":"markdown"},"source":{"75ce7781":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","181b9b22":"# Read the data and assign it as df\ndf=pd.read_csv(\"..\/input\/heart.csv\")","4baccbf0":"# Let's have a quick look into data.This code shows first 5 rows and all columns\ndf.head()","894a9869":"# If there is unknown,missing or unproper data, this codes shows the number of them\n# We can also learn about features such as data type of the features\ndf.info()\n","6bda78ce":"# statistical data is important to learn about balance inside or among the features.\ndf.describe()","17e775b7":"# Seaborn countplot gives the number of data in the each class\nsns.countplot(x=\"target\", data=df)\n","be3251e2":"# y has target data (clases) such as 1 and 0. \ny = df.target.values\n# This means that take target data out from the datasets and assign them to x_data variable\nx_data = df.drop([\"target\"],axis=1)","6334ac3b":"#Normalization is used to handle with unbalanced features\n#This gives the values to the features which range from zero to 1.\nx = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values","aa48babb":"#The data is splited into two part for training and testing\n#Here test_size=0.2 means %20 is splited as test_data\n#we need to give any number to random_state in order to split data in the same way when it is reruned\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2,random_state=42)","af7af5ca":"# Build Linear Regression Algorithm\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train,y_train)\n# Here lr.score first predict the y_test and then gives the accuracy\nprint(\"test accuracy {}\".format(lr.score(x_test,y_test)))\n\nlr_score=lr.score(x_test,y_test)\n","134b3163":"# Here we use confusion matrix to evaluate the linear regression algorithm\nfrom sklearn.metrics import confusion_matrix\ny_prediction = lr.predict(x_test)\ny_actual=y_test\ncm = confusion_matrix(y_actual,y_prediction)","fff8df31":"# Heatmap visualization of cunfusion matrix of Linear regression model\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_prediction\")\nplt.ylabel(\"y_actual\")\nplt.show()\n","09605e94":"# KNN Classification Model\nfrom sklearn.neighbors import KNeighborsClassifier\nk = 3\nknn = KNeighborsClassifier(n_neighbors = k)\nknn.fit(x_train,y_train)\nprediction = knn.predict(x_test)\nprint(\" {} nn score: {}\".format(k,knn.score(x_test,y_test)))\n\nknn_score = knn.score(x_test,y_test)\n","3ea7b500":"# We can determine best k values with plotting k values versus accuracy\n# Here we give values to k from 1 to 15 and calculate the accuracy each time,then plot them.\nscore_list = []\nfor each in range(1,15):\n    knn2 = KNeighborsClassifier(n_neighbors = each)\n    knn2.fit(x_train,y_train)\n    score_list.append(knn2.score(x_test,y_test))\n    \nplt.plot(range(1,15),score_list)\nplt.xlabel(\"k values\")\nplt.ylabel(\"accuracy\")\nplt.show()\n","ed47001a":"# Here we use confusion matrix to evaluate the KNN Classification Model\nfrom sklearn.metrics import confusion_matrix\ny_prediction = knn.predict(x_test)\ny_actual=y_test\ncm = confusion_matrix(y_actual,y_prediction)","655df595":"# Heatmap visualization of cunfusion matrix of the KNN Classification Model\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_prediction\")\nplt.ylabel(\"y_actual\")\nplt.show()","5d530d71":"# Build Decision Tree Classification Model\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(random_state = 42)\ndt.fit(x_train,y_train)\n\nprint(\"score: \", dt.score(x_test,y_test))\n\ndt_score=dt.score(x_test,y_test)","1b657800":"# Here we use confusion matrix to evaluate the Decision Tree Classification Model\nfrom sklearn.metrics import confusion_matrix\ny_prediction = dt.predict(x_test)\ny_actual = y_test\ncm = confusion_matrix(y_actual,y_prediction)\n","5692b89a":"# Heatmap visualization of cunfusion matrix of the Decision Tree Classification Model\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_prediction\")\nplt.ylabel(\"y_actual\")\nplt.show()","aa812b08":"# Visulization of the Decision Tree Classification Model\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image  \nfrom sklearn.tree import export_graphviz\nimport pydotplus\ndot_data = StringIO()\nexport_graphviz(dt, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True)\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \nImage(graph.create_png())\n\n\n","143972ac":"# Build Random Forest Classification Model\nfrom sklearn.ensemble import RandomForestClassifier\n# n_estimators = 100 means this model will use 100 subsets.\nrf = RandomForestClassifier(n_estimators = 100,random_state = 42)\nrf.fit(x_train,y_train)\nprint(\"random forest algo result: \",rf.score(x_test,y_test))\n\nrf_score = rf.score(x_test,y_test)\n","8a37e2bb":"# Here we use confusion matrix to evaluate the Random Forest Classification Model\nfrom sklearn.metrics import confusion_matrix\ny_prediction = rf.predict(x_test)\ny_actual = y_test\ncm = confusion_matrix(y_actual,y_prediction)","9773da57":"# Heatmap visualization of cunfusion matrix of the Random Forest Classification Model\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_prediction\")\nplt.ylabel(\"y_actual\")\nplt.show()","c9d05a07":"# Build Support Vector Machine Model\nfrom sklearn.svm import SVC\nsvm = SVC(random_state = 42)\nsvm.fit(x_train,y_train)\n# prediction and accuracy \nprint(\"print accuracy of svm algo: \",svm.score(x_test,y_test))\n\nsvm_score = svm.score(x_test,y_test)","6f2ab356":"# Here we use confusion matrix to evaluate the Support Vector Machine Model\nfrom sklearn.metrics import confusion_matrix\ny_prediction = svm.predict(x_test)\ny_actual = y_test\ncm = confusion_matrix(y_actual,y_prediction)","b042c93e":"# Heatmap visualization of cunfusion matrix of the Support Vector Machine Model\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_prediction\")\nplt.ylabel(\"y_actual\")\nplt.show()","14e098ec":"# Build Naive Bayes Classification Model\nfrom sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n \nprint(\"print accuracy of naive bayes algo: \",nb.score(x_test,y_test))\n\nnb_score = nb.score(x_test,y_test)\n ","e025765d":"# Here we use confusion matrix to evaluate the Support Vector Machine Model\nfrom sklearn.metrics import confusion_matrix\ny_prediction = nb.predict(x_test)\ny_actual = y_test\ncm = confusion_matrix(y_actual,y_prediction)","7a02aa98":"# Heatmap visualization of cunfusion matrix of the Support Vector Machine Model\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_prediction\")\nplt.ylabel(\"y_actual\")\nplt.show()","ac097640":"class_name = (\"Logistic Regression\",\"KNN\",\"Decision Tree\",\"Random Forest\",\"SVM\",\"Naive Bayes\")\nclass_score = (lr_score,knn_score,dt_score,rf_score,svm_score,nb_score)\ny_pos= np.arange(len(class_score))\ncolors = (\"red\",\"gray\",\"purple\",\"green\",\"orange\",\"blue\")\nplt.figure(figsize=(20,12))\nplt.bar(y_pos,class_score,color=colors)\nplt.xticks(y_pos,class_name,fontsize=20)\nplt.yticks(np.arange(0.00, 1.05, step=0.05))\nplt.ylabel('Accuracy')\nplt.grid()\nplt.title(\" Confusion Matrix Comparision of the Classes\",fontsize=15)\nplt.savefig('graph.png')\nplt.show()\n","6c023038":"<a id=\"4\"><\/a> <br>\n4.Decision Tree Classification\n\n\"Decision tree builds classification or regression models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed\".\n\nAccording to \u0131nformation entropy, we can determine which feature is the most important. And we put the most important one to the top of the related tree.\n\nDecision tree classification can be used for both binary and multi classes\n\nCoding is the same for all supervised classes and we jus need to change the last part of the code.","cfdc004b":"<a id=\"8\"><\/a> <br>\n8.Confusion Matrix Comparision with Visualization\n\nHere we will visualize all confusion matrices of all above classifiers.","fe9400e4":"## Supervised Machine Learning Classifications\nIn this tutorial I will apply supervised machine learning classifications to the canser data sets in order to determine if tested data has heart diseases or not. I will use KNN classification, decision tree classification, random forest classification,Support vector machine, logistig regression and naive bayes algorithms. I will show also how to determine accuracy of the each classificaiton and make evaluation by using confusion matrix.\n\n1. [EDA(Exploratory Data Analaysis)](#1)\n2. [Logistic Regression Classification](#2)\n3. [KNN Classification](#3)\n4. [Decision Tree Classification](#4)\n5. [Random Forest Classification](#5)\n6. [Support Vector Machine(SVM)](#6)\n7. [Naive Bayes Classification](#7)\n8. [Confusion Matrix Comparision with Visualization](#8)\n9. [Conclusion](#9)","308c554a":"<a id=\"2\"><\/a> <br>\n2.Logistic Regression Classification\n\nIt is very powerfull algorithm to use with binary classification.","f149285a":"<a id=\"7\"><\/a> <br>\n7.Naive Bayes Classification\n\n\"Naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features.\"\n\nHere we basically determine similarity range and calculate probabilty of the X point in the A feature P(A_feature|x).","f1a69eb3":"<a id=\"3\"><\/a> <br>\n\n3.KNN Classification\n\nIn this method we need to choose k value.It means that we chose k number of points of classes which are nearest to the out test point. We can call this small data set. We count the number of classes in the small dataset and determine the highest number of class. Finally we can say our test point belongs to the class.\n\nWhile choosing k number we have to be carefull because small k value causes overfitting while big k value causes underfitting.\n\nCoding is the same for all supervised classes and we jus need to change the last part of the code.","c6c75568":"<a id=\"9\"><\/a> <br>\n9.Conclusion\n\nIn this tutorial supervised machine learning algorithms are used to build model which determines if patient has heart disease. Confusiion matrix of each classifier is found and ploted as heatmap by using seaborn algorithm. \n\nIf you have suggestion, advice or question, please do not hesitate to share with me.","a0f4735a":"<a id=\"6\"><\/a> <br>\n6.Support Vector Machine (SVM)\n\nSVM is used fo both regression and classification problems, but generally for classification.\nThere is a C parameter inside the SVM algoritma and the default value of C parameter is 1. If C is small, it causes the misclassification. If C is big, it causes ovetfitting. So we need to try C parameter to find best value.\n","1f9c406d":"<a id=\"5\"><\/a> <br>\n5.Random Forest Classification\n\nThis methods basically use multiple number of decision trees and take the avarage of the results of these decision trees. And we use this avarage to determine the class of the test point.\n\nThis is one of ensamble method which uses multiple classes to predict the target, and very powerfull technique.","caf1860a":"<a id=\"1\"><\/a> <br>\n1. EDA(Exploratory Data Analaysis)\n\nEDA is very important to look at what is inside the data. For example, if there is object(string) in the data, we need to change it to integer or float because sci-learn is not handling with object data. There are also missdata in the datasets, we need to handle them."}}