{"cell_type":{"7f9761c5":"code","b453d877":"code","58b593fc":"code","c7ca9e22":"code","9a252c4d":"code","bb596cc3":"code","66e7a564":"code","0998c1ca":"code","3fb52c2a":"code","f7304916":"code","e441a890":"code","356233be":"code","b594bfb3":"code","64fa773f":"code","1db0f796":"code","8254f0d0":"markdown","ef25df7f":"markdown","87e4833b":"markdown","3dc91255":"markdown","d0b016c5":"markdown","6546fefb":"markdown","5a4443d3":"markdown","99c09fb2":"markdown","faccf699":"markdown","8a275d83":"markdown","68b9e685":"markdown","5f9f9bdc":"markdown","e51faa3d":"markdown","c7fa397b":"markdown"},"source":{"7f9761c5":"import torch\nimport torchvision as tv\n\nimport numpy as np\nimport matplotlib.pyplot as plt","b453d877":"# Download training data\ntraining_data = tv.datasets.FashionMNIST(\n    root=\"data\",  # data path\n    train=True,  # get training data\n    download=True,  # downloads data if not avilable\n    transform=tv.transforms.ToTensor(),  # transform to tensor\n)\n\n# Download testing data\ntest_data = tv.datasets.FashionMNIST(\n    root=\"data\",\n    train=False,  # get test data\n    download=True,\n    transform=tv.transforms.ToTensor(),\n)","58b593fc":"labels_map = {\n    0: \"T-Shirt\",\n    1: \"Trouser\",\n    2: \"Pullover\",\n    3: \"Dress\",\n    4: \"Coat\",\n    5: \"Sandal\",\n    6: \"Shirt\",\n    7: \"Sneaker\",\n    8: \"Bag\",\n    9: \"Ankle Boot\",\n}\nfigure = plt.figure(figsize=(8, 8))\ncols, rows = 3, 3\nfor i in range(1, cols * rows + 1):\n    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n    img, label = training_data[sample_idx]\n    figure.add_subplot(rows, cols, i)\n    plt.title(labels_map[label])\n    plt.axis(\"off\")\n    plt.imshow(img.squeeze(), cmap=\"gray\")\nplt.show()","c7ca9e22":"# Directly from data\ndata = [[10, 20],[30, 40]]\ndata_tensor = torch.tensor(data)\nprint('Tensor directly from data:', data_tensor, '-'*30, sep='\\n')\n\n# from a numpy array\nnp_array = np.array(data)\nnp_tensor = torch.from_numpy(np_array)\nprint('Tensor from numpy array:', np_tensor, '-'*30, sep='\\n')\n\n# fro another tensor\nx_ones = torch.ones_like(data_tensor) # retains the properties of x_data\nprint(f\"Ones Tensor from data tensor: \\n {x_ones} \\n\")\nx_rand = torch.rand_like(data_tensor, dtype=torch.float) # overrides the datatype of x_data\nprint(f\"Random Tensor from data tensor: \\n {x_rand}\", '-'*30, sep='\\n')\n\n# With random of constant values\nshape = (3,2, 2)\nrand_tensor = torch.rand(shape)\nones_tensor = torch.ones(shape)\nzeros_tensor = torch.zeros(shape)\n\nprint(f\"Random Tensor: \\n {rand_tensor} \\n\")\nprint(f\"Ones Tensor: \\n {ones_tensor} \\n\")\nprint(f\"Zeros Tensor: \\n {zeros_tensor}\")","9a252c4d":"tensor = torch.rand(3,4)\n\nprint(f\"Shape of tensor: {tensor.shape}\")\nprint(f\"Datatype of tensor: {tensor.dtype}\")\nprint(f\"Device tensor is stored on: {tensor.device}\")","bb596cc3":"# moving tensors\nif torch.cuda.is_available():\n    tensor = tensor.to('cuda')","66e7a564":"# Create a tensor\ntensor = torch.rand(4, 4)\nprint('Initial tensor:', tensor, '-'*50, sep='\\n')\n\nprint('First row:',tensor[0], '-'*50, sep='\\n')\nprint('First column:', tensor[:, 0], '-'*50, sep='\\n')\nprint('Last column:', tensor[..., -1], '-'*50, sep='\\n')\ntensor[:,1] = 0  # changing the values of a column\nprint('Changing column values:', tensor, '-'*50, sep='\\n')","0998c1ca":"joined_tensor = torch.cat([tensor, tensor, tensor], dim=0)\nprint(joined_tensor)","3fb52c2a":"# Specify a batch size.\nbatch_size = 64\n\n# Create data loaders.\ntrain_dataloader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\ntest_dataloader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n\nfor X, y in train_dataloader:\n    print(\"Shape of X: \", X.shape, X.dtype)\n    print(\"Shape of y: \", y.shape, y.dtype)\n    break","f7304916":"# defining the model\nclass FashionModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.flatten = torch.nn.Flatten()\n        self.linear_relu_stack = torch.nn.Sequential(\n            torch.nn.Linear(28*28, 512),\n            torch.nn.ReLU(),\n            torch.nn.Linear(512, 512),\n            torch.nn.ReLU(),\n            torch.nn.Linear(512, 10),\n            torch.nn.ReLU()\n        )\n\n    def forward(self, x):\n        x = self.flatten(x)\n        logits = self.linear_relu_stack(x)\n        return logits\n    \n# check if GPU is availavle\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using {} device\".format(device))\n\n# get the model and move to avialable device\nmodel = FashionModel().to(device)\nprint(model)","e441a890":"# definde the loss function and the optimizer.\nloss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\n# The training function\ndef train(dataloader, model, loss_function, optimizer):\n    size = len(dataloader.dataset)\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        # Forward propagation or Compute prediction error\n        pred = model(X)\n        loss = loss_function(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(X)\n            print(f\"loss: {loss:>7f}  [{current:>5d}\/{size:>5d}]\")\n            \n\n# The test function\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    model.eval()\n    test_loss, correct = 0, 0\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    test_loss \/= num_batches\n    correct \/= size\n    print(f\"Test Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")","356233be":"epochs = 15\nfor t in range(epochs):\n    print(f\"Epoch {t+1}\\n-------------------------------\")\n    train(train_dataloader, model, loss, optimizer)\n    test(test_dataloader, model, loss)\nprint(\"Done!\")","b594bfb3":"torch.save(model.state_dict(), \"model.ckpt\")\nprint(\"Saved PyTorch Model State to model.ckpt\")","64fa773f":"model = FashionModel()\nmodel.load_state_dict(torch.load(\".\/model.ckpt\"))","1db0f796":"classes = [\n    \"T-shirt\/top\",\n    \"Trouser\",\n    \"Pullover\",\n    \"Dress\",\n    \"Coat\",\n    \"Sandal\",\n    \"Shirt\",\n    \"Sneaker\",\n    \"Bag\",\n    \"Ankle boot\",\n]\n\nmodel.eval()\nx, y = test_data[0][0], test_data[0][1]\nwith torch.no_grad():\n    pred = model(x)\n    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')","8254f0d0":"# The model\nA neural network class in PyTorch should be inherited from [torch.nn.Module](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.Module.html). The layers of the networks are defined in the ``__init__()`` function. How the computation will work or data will flow is defined in the ``forward()`` function. ","ef25df7f":"# References\nThis notebook is based on the following article(s):\n\n - https:\/\/pytorch.org\/tutorials\/beginner\/basics\/quickstart_tutorial.html\n","87e4833b":"# Making the predictions\nNow the loaded model can be used to predict data","3dc91255":"# Downloading the dataset\nTorchvision contains many real-world vision data like CIFAR, COCO, etc ([full list](https:\/\/pytorch.org\/vision\/stable\/datasets.html)). Fot this notebook, we will use the FashionMNIST dataset. Fashion-MNIST is a dataset of Zalando\u2019s article images consisting of 60,000 training examples and 10,000 test examples. Each example comprises a 28\u00d728 grayscale image and an associated label from one of 10 classes.Fashion-MNIST is a dataset of Zalando\u2019s article images consisting of 60,000 training examples and 10,000 test examples. Each example comprises a 28\u00d728 grayscale image and an associated label from one of 10 classes.","d0b016c5":"# Data loaders\nPyTorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow a user to use pre-loaded datasets as well as own data. `Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the `Dataset` to enable easy access to the samples. PyTorch provides two data primitives: `torch.utils.data.DataLoader` and `torch.utils.data.Dataset` that allow a user to use pre-loaded datasets as well as own data. `Dataset` stores the samples and their corresponding labels, and `DataLoader` wraps an iterable around the `Dataset` to enable easy access to the samples. \n\nThe datasets are passed to a DataLoader. This wraps an iterable over a dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading.","6546fefb":"# Tensors\nWe transform the dataset into tensors. Now, what is tensors? Tensors are a specialized data structure that are very similar to arrays and matrices. Tensors are similar to [NumPy\u2019s](https:\/\/numpy.org\/) ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data. Tensors are also optimized for automatic differentiation. In PyTorch, tensors are used to encode the inputs and outputs of a model, as well as the model\u2019s parameters.\n\n## Initializing a Tensor\nTensors can be initialized in different ways. Like\n - Directly from data\n - From a numpy array\n - From another Tensor\n - With random of constant values","5a4443d3":"## Training the model\nTo train a model we need a loss function and an optimizer. Fos this task, we will use Cross entropy loss and SGD optimizer. During training, in a training loop, the model makes predictions in the forward pass and in the back pass the parameters are optimized.","99c09fb2":"Data does not always come in its final processed form that is required for training machine learning algorithms. We use transforms to perform some manipulation of the data and make it suitable for training.\n\n``ToTensor`` converts a PIL image or NumPy ndarray into a FloatTensor. and scales the image\u2019s pixel intensity values in the range [0., 1.]\n## Visualizing the data set","faccf699":"### NumPy like indexing and slicing\nTensor API supports several NumPy operations like tensors support numpy like indexing and slicing. ","8a275d83":"# Loading Models\n\nThe process for loading a model includes re-creating the model structure and loading the state dictionary into it.","68b9e685":"# Saving the model\nWe often need to save a model for using latter. A common way to save a model is to serialize the internal state dictionary (containing the model parameters).","5f9f9bdc":"## Tensor operations\nThere are several opetarion on tensor including arithmatic operation, linerar algebra, matrix manipulation, etc. The whole list can be found [here](https:\/\/pytorch.org\/docs\/stable\/torch.html). Each of the operatin can be runed on GPU. By default, the tensors are created in CPU. So we explecitly need to move the tensors to the GPU using the `.to` method. However, copying large tensors can be expencive in terms of time and memory. ","e51faa3d":"### Joining tensors \n`torch.cat` can be used to concatenate a sequence of tensors along a given dimension.","c7fa397b":"## Attributes of a Tensor\nTensor attributes describe their shape, datatype, and the device on which they are stored."}}