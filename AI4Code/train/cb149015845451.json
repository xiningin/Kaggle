{"cell_type":{"d1187179":"code","74ac63b3":"code","b3b5e5f8":"code","f29e734f":"code","0fe2cc9e":"code","51457706":"code","644486af":"code","8b4b1b79":"code","59c47c7e":"code","766a45b5":"code","27579545":"code","f8b874fb":"markdown","e998c414":"markdown","77eb98b3":"markdown","1dae5bb1":"markdown","bf12d499":"markdown","198e4785":"markdown","864ce84b":"markdown","9f3e92f5":"markdown","85fe670c":"markdown","9a926f71":"markdown","6b9a4b86":"markdown"},"source":{"d1187179":"import joblib\nimport pandas as pd\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_validate, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nimport seaborn as sns","74ac63b3":"# Data Pre-processing\n\ndef grab_col_names(dataframe, cat_th=10, car_th=20):\n    \n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car","b3b5e5f8":"# Outlier Detection\n\ndef outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit\n\ndef replace_with_thresholds(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit","f29e734f":"# One-hot Encoding\n\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe","0fe2cc9e":"# Feature Engineering Process and Data Prep\n\ndef diabetes_data_prep(dataframe):\n    dataframe.columns = [col.upper() for col in dataframe.columns]\n\n    # Glucose\n    dataframe['NEW_GLUCOSE_CAT'] = pd.cut(x=dataframe['GLUCOSE'], bins=[-1, 139, 200], labels=[\"normal\", \"prediabetes\"])\n\n    # Age\n    dataframe.loc[(dataframe['AGE'] < 35), \"NEW_AGE_CAT\"] = 'young'\n    dataframe.loc[(dataframe['AGE'] >= 35) & (dataframe['AGE'] <= 55), \"NEW_AGE_CAT\"] = 'middleage'\n    dataframe.loc[(dataframe['AGE'] > 55), \"NEW_AGE_CAT\"] = 'old'\n\n    # BMI\n    dataframe['NEW_BMI_RANGE'] = pd.cut(x=dataframe['BMI'], bins=[-1, 18.5, 24.9, 29.9, 100],\n                                        labels=[\"underweight\", \"healty\", \"overweight\", \"obese\"])\n\n    # BloodPressure\n    dataframe['NEW_BLOODPRESSURE'] = pd.cut(x=dataframe['BLOODPRESSURE'], bins=[-1, 79, 89, 123],\n                                            labels=[\"normal\", \"hs1\", \"hs2\"])\n    \n    # Cehcking the categorical and numerical columns after the feature engineeering process\n    cat_cols, num_cols, cat_but_car = grab_col_names(dataframe, cat_th=5, car_th=20)\n\n    cat_cols = [col for col in cat_cols if \"OUTCOME\" not in col]\n    \n    # One-hot encoding for categorical columns\n    df = one_hot_encoder(dataframe, cat_cols, drop_first=True)\n\n    cat_cols, num_cols, cat_but_car = grab_col_names(df, cat_th=5, car_th=20)\n    \n    # Replacing the outlier variavles with the threshold\n    replace_with_thresholds(df, \"INSULIN\")\n    \n    # Feature Scaling\n    X_scaled = StandardScaler().fit_transform(df[num_cols])\n    df[num_cols] = pd.DataFrame(X_scaled, columns=df[num_cols].columns)\n    \n    # Splitting the dependent and independent columns as X and y\n    y = df[\"OUTCOME\"]\n    X = df.drop([\"OUTCOME\"], axis=1)\n\n    return X, y","51457706":"# Base Models\n\ndef base_models(X, y, scoring=\"roc_auc\"):\n    print(\"Base Models....\")\n    classifiers = [('LR', LogisticRegression()),\n                   ('KNN', KNeighborsClassifier()),\n                   (\"SVC\", SVC()),\n                   (\"CART\", DecisionTreeClassifier()),\n                   (\"RF\", RandomForestClassifier()),\n                   ('Adaboost', AdaBoostClassifier()),\n                   ('GBM', GradientBoostingClassifier()),\n                   ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n                   ('LightGBM', LGBMClassifier()),\n                   #('CatBoost', CatBoostClassifier(verbose=False))\n                   ]\n\n    for name, classifier in classifiers:\n        cv_results = cross_validate(classifier, X, y, cv=3, scoring=scoring)\n        print(f\"{scoring}: {round(cv_results['test_score'].mean(), 4)} ({name}) \")\n","644486af":"# Manually Specified Subset of the Hyperparameter Space for learning algorithms\n\nknn_params = {\"n_neighbors\": range(2, 50)}\n\ncart_params = {'max_depth': range(1, 20),\n               \"min_samples_split\": range(2, 30)}\n\nrf_params = {\"max_depth\": [8, 15, None],\n             \"max_features\": [5, 7, \"auto\"],\n             \"min_samples_split\": [15, 20],\n             \"n_estimators\": [200, 300]}\n\nxgboost_params = {\"learning_rate\": [0.1, 0.01],\n                  \"max_depth\": [5, 8],\n                  \"n_estimators\": [100, 200],\n                  \"colsample_bytree\": [0.5, 1]}\n\nlightgbm_params = {\"learning_rate\": [0.01, 0.1],\n                   \"n_estimators\": [300, 500],\n                   \"colsample_bytree\": [0.7, 1]}\n\nclassifiers = [('KNN', KNeighborsClassifier(), knn_params),\n               (\"CART\", DecisionTreeClassifier(), cart_params),\n               (\"RF\", RandomForestClassifier(), rf_params),\n               ('XGBoost', XGBClassifier(use_label_encoder=False, eval_metric='logloss'), xgboost_params),\n               ('LightGBM', LGBMClassifier(), lightgbm_params)]\n\n# Hyperparameter Optimization\n\ndef hyperparameter_optimization(X, y, cv=3, scoring=\"roc_auc\"):\n    print(\"Hyperparameter Optimization....\")\n    best_models = {}\n    for name, classifier, params in classifiers:\n        print(f\"########## {name} ##########\")\n        cv_results = cross_validate(classifier, X, y, cv=cv, scoring=scoring)\n        print(f\"{scoring} (Before): {round(cv_results['test_score'].mean(), 4)}\")\n\n        gs_best = GridSearchCV(classifier, params, cv=cv, n_jobs=-1, verbose=False).fit(X, y)\n        final_model = classifier.set_params(**gs_best.best_params_)\n\n        cv_results = cross_validate(final_model, X, y, cv=cv, scoring=scoring)\n        print(f\"{scoring} (After): {round(cv_results['test_score'].mean(), 4)}\")\n        print(f\"{name} best params: {gs_best.best_params_}\", end=\"\\n\\n\")\n        \n        best_models[name] = final_model\n    \n    return best_models","8b4b1b79":"# Stacking & Ensemble Learning\n\ndef voting_classifier(best_models, X, y):\n    print(\"Voting Classifier...\")\n    voting_clf = VotingClassifier(estimators=[('KNN', best_models[\"KNN\"]), ('RF', best_models[\"RF\"]),\n                                              ('LightGBM', best_models[\"LightGBM\"])],\n                                  voting='soft').fit(X, y)\n    cv_results = cross_validate(voting_clf, X, y, cv=3, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])\n    print(f\"Accuracy: {cv_results['test_accuracy'].mean()}\")\n    print(f\"F1Score: {cv_results['test_f1'].mean()}\")\n    print(f\"ROC_AUC: {cv_results['test_roc_auc'].mean()}\")\n    return voting_clf","59c47c7e":"################################################\n# Pipeline Main Function\n################################################\n\nimport os\n\ndef main():\n    df = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\n    X, y = diabetes_data_prep(df)\n    base_models(X, y)\n    best_models = hyperparameter_optimization(X, y)\n    voting_clf = voting_classifier(best_models, X, y)\n    os.chdir(\".\/\")\n    joblib.dump(voting_clf, \"voting_clf_diabetes.pkl\")\n    print(\"Voting_clf has been created\")\n    return voting_clf\n\nif __name__ == \"__main__\":\n    main()","766a45b5":"# Prediction\ndf = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\nX, y = diabetes_data_prep(df)\nrandom_user = X.sample(1, random_state=45)\nnew_model = joblib.load(\".\/voting_clf_diabetes.pkl\")\nnew_model.predict(random_user)","27579545":"random_user","f8b874fb":"# Pipeline Main Function\n\nAfter defining data preparation, outlier detection, encoding the categorical columns, feature engineering, hyperparameter optimization and voting classifier functions, we can run all the process with the main function.\n\n__Basically, we can imagine that all the pre-process functions are pipelines and the main function is the gate valve to start flowing the related dataset to the production stage.__\n\n![image.png](https:\/\/thumbs.dreamstime.com\/b\/gate-valve-vector-isolated-picture-gate-valve-gas-pipeline-engineering-communications-vector-isolated-picture-186421126.jpg)","e998c414":"# Data Preprocessing and Feature Engineering\n\nBasically, all machine learning algorithms use some input data to create outputs. This input data comprise features, which are usually in the form of structured columns. Algorithms require features with some specific characteristic to work properly. Here, the need for feature engineering arises.\n\nYou can check out my notebooks related to advanced feature engineering as below.\n\n[Advanced Feature Engineering](https:\/\/www.kaggle.com\/seneralkan\/advanced-feature-engineering)","77eb98b3":"# Dataset Definition\n\nWe will use Pima Indians Diabetes dataset.\n\nThe datasets consist of several medical predictor (independent) variables and one target (dependent) variable, Outcome. Independent variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n\n### __Context__\nThe objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\nSource: [Kaggle Dataset](https:\/\/www.kaggle.com\/uciml\/pima-indians-diabetes-database)\n","1dae5bb1":"# Base Models\n\n        -In this fuction, we will define base machine learning algorithms such as Logistic Regression, KNN, Support Vector, CART, Random Forest,        AdaBoost, Gradient Boosting, XGBoost and LightGBM.\n\n        -After defining algorithms, we will validate with cross validation and compare the base models test scores.\n\n        -The main idea of this function is to determine test score and modelling insight for our dataset before applying hyper-parameter optimization.\n\n![image.png](https:\/\/analyticsinsight.b-cdn.net\/wp-content\/uploads\/2021\/08\/How-Does-an-AI-Algorithm-Work-6-Problems-Solved-with-AI-Algorithm.jpg)\n","bf12d499":"# Hyperparameter Optimization Approaches (Grid Search)\n\nThe objective function takes a tuple od hyperparameters and return loss function. In order to estimate the generalization performance, cross validation methods are using.\n\nThere are several cross validation methods as below.\n\n- Grid Search\n\n- Random Search\n\n- Gradient-based optimization\n\n- Random Search\n\n- Automated Hyperparamter Tuning (Bayesian Optimization and Genetic Algorithms)\n\n- Population Based\n\n- Early-stopping Based\n\nWe have used Grid Search CV in this ML pipeline case. So let's dive in the Grid Search cross validation method.\n\n## Grid Search CV\n\n![image.png](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/b\/b6\/Hyperparameter_Optimization_using_Grid_Search.svg\/800px-Hyperparameter_Optimization_using_Grid_Search.svg.png)\n\nThe traditional way of performing hyperparameter optimization has been grid search, or a parameter sweep, which is simply an exhaustive searching through a __manually specified subset__ of the hyperparameter space of a learning algorithm. A grid search algorithm must be guided by some performance metric, typically measured by cross-validation on the training set or evaluation on a held-out validation set.\n\nSource: \n\n[Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Hyperparameter_optimization#Evolutionary_optimization)\n\n[Towards Data Science](https:\/\/towardsdatascience.com\/hyperparameters-optimization-526348bb8e2d)\n","198e4785":"# What are the Benefits of Machine Learning Pipeline?\n\nConstructing ML Pipelines provides many advantages. Some of them are:\n\n- __Flexibility__\n    Computation units are easy to replace. For better implementation, it is possible to rework that part without changing the rest of the system.\n    \n- __Extensibility__\n    When the system is partitioned into pieces, it is easy to create new functionality.\n    \n- __Scalability__\n    Each part of the computation is presented via a standard interface. If any part has an issue, it is possible to scale that component separately.","864ce84b":"# Hyperparameter Optimization\n\n__Hyperparameters__\n\nIn machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm. A hyperparameter is a parameter whose value is used to control the learning process. \n\n__Model Parameters__\n\nThe same kind of machine learning model can require different constraints, weights or learning rates to generalize different data patterns. These measures are called hyperparameters, and have to be tuned so that the model can optimally solve the machine learning problem. Hyperparameter optimization finds a tuple of hyperparameters that yields an optimal model which minimizes a predefined loss function on given independent data.\n\n    The model parameters define how to use input data to get the desired output and are learned at training time. Instead, Hyperparameters determine how our model is structured in the first place.\n    \n    Machine Learning models tuning is a type of optimization of the model. We need to define hyperparameters in order to analyze and find the right combination of the values which have the minimum loss or maximum accuracy.\n\n\n![image.png](https:\/\/miro.medium.com\/max\/1400\/1*UMtPi1QBHpfE4iF_egJ8sQ.gif)\n\n","9f3e92f5":"# Machine Learning Pipeline\n\n### What is a Machine Learnning Pipeline ?\n\nA machine learning pipeline is the end-to-end process in order to orchestrate the flow of data into, and output from, a machine learning model (or set of multiple models). It includes raw data input, data cleaning, feature engineerig, the machine learning models and model parameters, model comparison and prediction outputs.\n\nThe main objective of the Machine Learning Pipeline is to utilize control over the ML model. A well-planned pipeline helps to makes the implementation more flexible.\n\n![image.png](https:\/\/www.tuv.com\/content-media-files\/master-content\/services\/industrial-services\/0042-tuv-rheinland-pipeline-technology\/tuv-rheinland-pipeline-technology_core_2_2_1.jpg)","85fe670c":"# How does a Machine Learning Pipeline work ?\n\n![image.png](https:\/\/cdn.dribbble.com\/users\/63407\/screenshots\/3715279\/media\/07c7e6d4e8fde817a561392283a53d47.png?compress=1)\n\nA ML Pipeline consist of 4 main stages as below.\n\n\n- __Pre-processeing__\n        This stage is covering the processing of the raw data. In this step, we basically need to transform the raw data to understandable format applying outlier detection, feature extraction, feature engineerinng steps, dimensionnally reduction, feature scaling and sampling for machine learning algorithms.\n\n\n- __Learning__\n        A learning algorithm is used to process understandable data to extract patterns appropriate for application in a new situation. In particular, the aim is to utilize a system for a specific input-output transformation task. For this, choose the best-performing model from a set of models produced by different hyperparameter settings, metrics, and cross-validation techniques.\n\n\n- __Evaluation__\n        To Evaluate the Machine Learning model's performance, fit a model to the training data, and predict the labels of the test set. Further, count the number of wrong predictions on the test dataset to compute the model\u2019s prediction accuracy.\n\n\n- __Prediction__\n        The model's performance to determine the outcomes of the test data set was not used for any training or cross-validation activities.","9a926f71":"# Voting Classifier\n\nBasically, Voting classifier is a method to combine different type of of machine learning algorithms in order to predict the output.\n\n![image.png](https:\/\/external-content.duckduckgo.com\/iu\/?u=https%3A%2F%2Fmedia.istockphoto.com%2Fvectors%2Fvoting-vector-id532255962%3Fk%3D6%26m%3D532255962%26s%3D170667a%26w%3D0%26h%3DXDCqTyqSSMUbbven6NdLOy10_9JiwDdzPuQUyToCON8%3D&f=1&nofb=1)\n\n__Why do we need voting classsifier ?__\n\nImagine that different classification methods were asked to make decisions based on the data instances inputs. \n\nOn the other hand, we can imagine a real life scenario for instance, you have been confused to use random forest classifier or a logistic regression in order to predict the type of the flower based on the dimension using Iris dataset.\n\n__In this ML Pipeline case, we have choosen the best 3 learning algorithms to predict the output which are KNN, Random Forest and LightGBM.__\n","6b9a4b86":"# Lets Code and Practice \ud83d\ude80\ud83d\udc68\ud83c\udffc\u200d\ud83d\udcbb"}}