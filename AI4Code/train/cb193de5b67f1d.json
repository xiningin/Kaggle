{"cell_type":{"34907604":"code","6f1006f3":"code","c8534893":"code","db8e8a76":"code","786f4f18":"code","760af66b":"code","4ee3cf2a":"code","04d2bc53":"code","e124f2fb":"code","6bae8937":"code","fa060849":"code","6f3d358d":"code","00afab3b":"markdown","12139667":"markdown","fc9b3952":"markdown","bed0cad0":"markdown","e36a12a7":"markdown","bc3e9c07":"markdown","bdaf1249":"markdown"},"source":{"34907604":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import confusion_matrix, classification_report, mean_squared_error, r2_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\npd_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')","6f1006f3":"#g = sns.heatmap(train[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")\n\n# Explore Sex vs Survived\ng = sns.barplot(x=\"Sex\",y=\"Survived\",data=pd_data)\ng = g.set_ylabel(\"Survival Probability\")","c8534893":"# Explore Parch vs Survived\ng  = sns.factorplot(x=\"Parch\",y=\"Survived\",data=pd_data,kind=\"bar\", size = 6 , palette = \"muted\")\ng.despine(left=True)","db8e8a76":"# Explore Age vs Survived\n#g = sns.FacetGrid(pd_data, col='Survived')\n#g = g.map(sns.distplot, \"Age\")\n#g = g.set_ylabels(\"Survival probability\")\n\nsns.distplot(pd_data['Age'].dropna(), [0,20,40,100])","786f4f18":"# Explore Pclass vs Embarked \ng = sns.factorplot(\"Pclass\", col=\"Embarked\",  data=pd_data, size=6, kind=\"count\", palette=\"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"Count\")","760af66b":"# Explore Fare distribution \n#g = sns.distplot(pd_data[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(pd_data[\"Fare\"].skew()))\n#g = g.legend(loc=\"best\")\nsns.distplot(pd_data['Fare'].dropna().map(lambda i: np.log(i) if i > 0 else 0))","4ee3cf2a":"X = pd_data.loc[:, ('Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked')]\n\nlabelencoder = LabelEncoder()\nX['Sex'] = labelencoder.fit_transform(X['Sex'])\n\nX['Embarked'] = np.where(X['Embarked'].isna(), 'S', X['Embarked'])\n#pd.get_dummies(X)\none_hot = pd.get_dummies(X['Embarked'])\nX = X.drop('Embarked', axis=1)\nX = X.join(one_hot)\n\n## Fill missing value of Age with the median age of similar rows according to Pclass, Parch and SibSp\n# Index of NaN age rows\nindex_NaN_age = list(X[\"Age\"][X[\"Age\"].isnull()].index)\n\nfor i in index_NaN_age :\n    age_med = X[\"Age\"].median()\n    age_pred = X[\"Age\"][((X['SibSp'] == X.iloc[i][\"SibSp\"]) & (X['Parch'] == X.iloc[i][\"Parch\"]) & (X['Pclass'] == X.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        X['Age'].iloc[i] = age_pred\n    else :\n        X['Age'].iloc[i] = age_med\n\nX['CategoricalAge'] = pd.cut(X['Age'], [0,20,40,100])\none_hot = pd.get_dummies(X['CategoricalAge'])\nX = X.drop('Age', axis=1)\nX = X.drop('CategoricalAge', axis=1)\nX = X.join(one_hot)\n\n#Fill Fare missing values with the median value\nX[\"Fare\"] = X[\"Fare\"].fillna(X[\"Fare\"].median())\n# Apply log to Fare to reduce skewness distribution\nX[\"Fare\"] = X[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n\n#scaler = StandardScaler()\n#scaler.fit(X)\n#StandardScaler(copy=True, with_mean=True, with_std=True)\n#X = scaler.transform(X)\n\ny = pd_data.loc[:, 'Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","04d2bc53":"train_target = pd_data['Survived'].values\npossible_features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'C', 'Q', 'S']\n\n# Check feature importances\nselector = SelectKBest(f_classif, len(possible_features))\nselector.fit(X, train_target)\nscores = -np.log10(selector.pvalues_)\nindices = np.argsort(scores)[::-1]\n\nprint('Feature importances:')\nfor i in range(len(scores)):\n    print('%.2f %s' % (scores[indices[i]], possible_features[indices[i]]))","e124f2fb":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(solver='lbfgs', multi_class='auto')\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","6bae8937":"# Neural Network\nfrom sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier(hidden_layer_sizes=(9,9,9),max_iter=500)\nmlp.fit(X_train,y_train)\ny_pred = mlp.predict(X_test)\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))","fa060849":"pd_test_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\nX_t = pd_test_data.loc[:, ('Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked')]\n\n## Fill missing value of Age with the median age of similar rows according to Pclass, Parch and SibSp\n# Index of NaN age rows\nindex_NaN_age = list(X_t[\"Age\"][X_t[\"Age\"].isnull()].index)\n\nfor i in index_NaN_age :\n    age_med = X_t[\"Age\"].median()\n    age_pred = X_t[\"Age\"][((X_t['SibSp'] == X_t.iloc[i][\"SibSp\"]) & (X_t['Parch'] == X_t.iloc[i][\"Parch\"]) & (X_t['Pclass'] == X_t.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        X_t['Age'].iloc[i] = age_pred\n    else :\n        X_t['Age'].iloc[i] = age_med\n\nX_t['CategoricalAge'] = pd.cut(X_t['Age'], [0,20,40,100])\none_hot = pd.get_dummies(X_t['CategoricalAge'])\nX_t = X_t.drop('Age', axis=1)\nX_t = X_t.drop('CategoricalAge', axis=1)\nX_t = X_t.join(one_hot)\n\n#Fill Fare missing values with the median value\nX_t[\"Fare\"] = X_t[\"Fare\"].fillna(X_t[\"Fare\"].median())\n# Apply log to Fare to reduce skewness distribution\nX_t[\"Fare\"] = X_t[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\n\nlabelencoder = LabelEncoder()\nX_t['Sex'] = labelencoder.fit_transform(X_t['Sex'])\n\nX_t['Embarked'] = np.where(X_t['Embarked'].isna(), 'S', X_t['Embarked'])\none_hot = pd.get_dummies(X_t['Embarked'])\nX_t = X_t.drop('Embarked', axis=1)\nX_t = X_t.join(one_hot)\n\npred = pd.DataFrame(pd.read_csv(\"..\/input\/titanic\/test.csv\")['PassengerId'])\n\ny_logreg_pred = logreg.predict(X_t)\npred['Survived'] = y_logreg_pred.astype(int)\npred.to_csv(\"..\/working\/submission_logreg_5.csv\", index = False)\n\ny_mlp_pred = mlp.predict(X_t)\npred['Survived'] = y_mlp_pred.astype(int)\npred.to_csv(\"..\/working\/submission_mlp_5.csv\", index = False)","6f3d358d":"for dirname, _, filenames in os.walk('..\/working'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nos.chdir(r'..\/working')\nfrom IPython.display import FileLink\n#FileLink(r'submission_logreg_5.csv')\nFileLink(r'submission_mlp_5.csv')","00afab3b":"Have a look at the data","12139667":"Reference:\nhttps:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling\n","fc9b3952":"Import libs","bed0cad0":"Give Neural Network a shot then","e36a12a7":"Save output to files","bc3e9c07":"Let's play with Logistic Regression first","bdaf1249":"Pre-process data"}}