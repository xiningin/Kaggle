{"cell_type":{"0b2ff974":"code","927d7364":"code","082409d6":"code","47087688":"code","a15d4695":"code","35a20a62":"code","55fb109f":"code","7a8843fc":"code","b5c28e55":"code","d1d9c7c9":"code","9970492f":"code","45391113":"code","972b91d7":"code","d1d2323e":"code","1a3bfc57":"code","c7455c63":"code","c9d1dd19":"code","cc3bd0e2":"code","9e31cbac":"code","4fab6bf5":"code","40b1a11a":"code","7d3a7df1":"code","0febfddf":"code","21924582":"code","18b2b3b6":"code","f273a2a4":"code","afb96e3f":"code","2ffc5939":"code","0c55ba5f":"code","eed5d08f":"code","73a0f9ce":"code","6197c0a8":"code","91ec997c":"code","ffc33bf0":"markdown","01aef103":"markdown","3447bd7f":"markdown","16b7fc10":"markdown","6b34f7b4":"markdown","32008f4d":"markdown","c28e15ae":"markdown","605787ed":"markdown","88c75cbc":"markdown","074e4494":"markdown","78edb63e":"markdown","d2f5d4fb":"markdown","b2a74744":"markdown","d7e24dda":"markdown","8644b483":"markdown","f11f745e":"markdown","6c497019":"markdown","c2c32084":"markdown","a9be4ef0":"markdown","bc65e307":"markdown","b6df48c5":"markdown","f49fa164":"markdown","44c20cab":"markdown","11539388":"markdown"},"source":{"0b2ff974":"import numpy as np\n\nimport pandas as pd\nfrom pandas.plotting import scatter_matrix\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\n\n\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error","927d7364":"HOURS = {7:00, 7:30, 8:00, 8:30, 9:00, 9:30, 10:00, 10:30, 11:00, 11:30, 12:00, 12:30, 13:00, 13:30, 14:00, 14:30, 15:00, 15:30, 16:00, 16:30, 17:00, 17:30, 18:00, 18:30, 19:00, 19:30, 20:00} # reference from dataset folder\nDATA_PATH = \"..\/input\/spurbantraffic\/urban_traffic_sp.csv\"\nTARGET_VAR = \"Slowness in traffic (%)\"\nMONDAY  = 26\nTUESDAY = 53\nWEDNESDAY = 80\nTHURSDAY = 107\nFRIDAY = 134\nDAYS_TO_CODE = {'Monday': 1, 'Tuesday': 2, 'Wednesday': 3, 'Thursday': 4, 'Friday': 5}","082409d6":"def load_data(path:str=DATA_PATH, sep:str=\";\"):\n    try:\n        df = pd.read_csv(path, sep=sep)\n        print(\"Data loaded with success\")\n        return df\n    except FileNotFoundError:\n        print(\"Check your data directory! Nothing there yet...\")\n        return False","47087688":"df = load_data()\ndf.head()","a15d4695":"df.isnull().sum()","35a20a62":"print(f\"Dataframe shape: {df.shape}\\n\")\ndf.info()","55fb109f":"df.describe()","7a8843fc":"df.hist(bins=50, figsize=(20, 15))\nplt.show()","b5c28e55":"def transform_target(df, target_var=TARGET_VAR, to=float):\n    df[target_var] = df[target_var].str.replace(',', '.').astype(to)\ntransform_target(df)","d1d9c7c9":"# New attribute\ndef transform_days(df, create_column=False, to_numerical=False):\n    #check is day column exists if not create\n    #if numerical transformation, go from day to number\n    #else go from number to day names\n    if create_column:\n        df['Day'] = '0'\n\n    position=-1\n    if to_numerical is False:\n        for idx in df.index:\n            if idx <= MONDAY:\n                df.iloc[idx, position] = 'Monday'\n            elif idx <= TUESDAY:\n                df.iloc[idx, position] = 'Tuesday'\n            elif idx <= WEDNESDAY:\n                df.iloc[idx, position] = 'Wednesday'\n            elif idx <= THURSDAY:\n                df.iloc[idx, position] = 'Thursday'\n            elif idx <= FRIDAY:\n                df.iloc[idx, position] = 'Friday'\n    else:\n        df_values = df[\"Day\"].unique()\n        for key, value in DAYS_TO_CODE.items():\n            assert key in df_values, \"First transform your data into weekday by setting to_numerical=False, then apply the numerical transformation\"\n            df.loc[(df.Day == key), 'Day'] = value\n        df['Day'] = df['Day'].astype(int)\n        \ntransform_days(df, create_column=True)","9970492f":"# Create code to hour dict\ndef set_hours_dict(df, hours:dict =HOURS)-> dict:\n    hours_arr = []\n\n    for hour, minute in hours.items():\n      s1 = str(hour) + ':' + '00'\n      s2 = str(hour) + ':' + str(minute)\n      if hour != 20:\n        hours_arr.append(s1)\n        hours_arr.append(s2)\n      else:\n        hours_arr.append(s1)\n\n    code_to_hour = {}\n    for code, hour in zip(df['Hour (Coded)'], hours_arr):\n      code_to_hour[code] = hour\n\n    return code_to_hour\n\ncode_to_hour = set_hours_dict(df)\n\ndef code_hour(code):\n  return code_to_hour[code]","45391113":"transform_days(df, to_numerical=True)\nplt.figure(figsize=(12, 8))\nsns.heatmap(df.corr(), cmap='Blues', annot=True)\nplt.show()","972b91d7":"corr_matrix = df.corr()\ncorr_matrix['Slowness in traffic (%)'].sort_values(ascending=False)","d1d2323e":"attributes = [\"Slowness in traffic (%)\", \"Hour (Coded)\"]\nscatter_matrix(df[attributes], figsize=(12, 8))","1a3bfc57":"def slowness_over_time(df, coded_hours=False):\n    fig = plt.figure(figsize=(20, 12))\n    ax = fig.add_axes([0, 0, 1, 1])\n\n    colors = {'Monday': 'r', 'Tuesday': 'b', 'Wednesday': 'g', 'Thursday': 'yellow', 'Friday':'black'}\n    transform_days(df)\n    for e in df['Day'].unique():\n        subset = df[df['Day'] == e]\n        ax.plot(subset['Hour (Coded)'], subset['Slowness in traffic (%)'],color=colors[e])\n\n    ax.set_title('Slowness in traffic VS. Hour of the day', fontsize=25, pad=15)\n    ax.set_xlabel('Hour of the day', fontsize=15)\n    ax.set_ylabel('Slowness in traffic (%)', fontsize=15)\n    \n    if coded_hours is False:\n        ax.set_xticks(range(1, 28))\n        ax.set_xticklabels(map(code_hour, subset['Hour (Coded)'].unique()))\n\n    ax.legend(colors, fontsize=20)\n\n    plt.show()\nslowness_over_time(df)\ntransform_days(df, to_numerical=True)","c7455c63":"num_cols = df.nunique()[df.nunique() > 2].keys() # Discriminate non-categorical data\nnum_cols = num_cols.drop('Day')\n\nl = num_cols.values\nnumber_of_columns=len(num_cols.values)\nnumber_of_rows = len(l)-1\/number_of_columns\nplt.figure(figsize=(number_of_columns,5*number_of_rows))\n\nfor i in range(0,len(l)):\n    plt.subplot(number_of_rows + 1,number_of_columns,i+1)\n    sns.set_style('whitegrid')\n    sns.boxplot(df[l[i]],color='green',orient='v')\n    plt.tight_layout()","c9d1dd19":"df.groupby('Day')['Slowness in traffic (%)'].mean()","cc3bd0e2":"train_set, test_set = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"Hour (Coded)\"])\n\n# Verifying stratified distribution\nprint(\"Train set class proportions:\\n\")\nprint(train_set[\"Hour (Coded)\"].value_counts() \/ len(train_set))\nprint(\"\\nFull set:\")\nprint(df[\"Hour (Coded)\"].value_counts() \/ len(df))","9e31cbac":"X_train = train_set.drop(\"Slowness in traffic (%)\", axis=1)\ny_train = train_set[\"Slowness in traffic (%)\"].copy()","4fab6bf5":"x_num_cols = X_train.nunique()[X_train.nunique() > 2].keys()\nx_num_cols = x_num_cols.drop('Day')\nnumerical_data = list(x_num_cols)","40b1a11a":"num_pipeline = Pipeline([('std_scaler', StandardScaler())])\nfull_pipeline = ColumnTransformer([(\"num\", num_pipeline, numerical_data)], remainder='passthrough')","7d3a7df1":"X_train_prepared = full_pipeline.fit_transform(X_train)","0febfddf":"tree_reg = DecisionTreeRegressor(random_state=42)\nscores = cross_val_score(tree_reg, X_train_prepared, y_train, scoring=\"neg_mean_squared_error\", cv=10)\ntree_rmse_scores = np.sqrt(-scores)","21924582":"svm_reg = SVR(kernel=\"linear\")\nsvr_scores = cross_val_score(svm_reg, X_train_prepared, y_train, scoring=\"neg_mean_squared_error\", cv=10)\nsvr_rmse_scores = np.sqrt(-svr_scores)","18b2b3b6":"lin_reg = LinearRegression()\nlin_scores = cross_val_score(lin_reg, X_train_prepared, y_train, scoring=\"neg_mean_squared_error\", cv=10)\nlin_rmse_scores = np.sqrt(-lin_scores)","f273a2a4":"forest_reg = RandomForestRegressor(n_estimators=100, random_state=42)\nforest_scores = cross_val_score(forest_reg, X_train_prepared, y_train, scoring=\"neg_mean_squared_error\", cv=10)\nforest_rmse_scores = np.sqrt(-forest_scores)","afb96e3f":"data = {\n    \"Model\":[\"Linear Reg\", \"Decision Tree\", \"SVR\", \"Random Forest\"],\n    \"Mean Score\": [lin_rmse_scores.mean(), tree_rmse_scores.mean(), svr_rmse_scores.mean(), forest_rmse_scores.mean()],\n    \"Standard Deviation\": [lin_rmse_scores.std(), tree_rmse_scores.std(), svr_rmse_scores.std(), forest_rmse_scores.std()]\n}\nscores_df = pd.DataFrame(data)\nscores_df","2ffc5939":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [\n    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8, 10, 12, 14, 16, 18]},\n    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [10, 12, 14, 16, 18]},\n]\n\nforest_reg = RandomForestRegressor(random_state=42)\ngrid_search = GridSearchCV(forest_reg, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\ngrid_search.fit(X_train_prepared, y_train)","0c55ba5f":"best_grid_model = grid_search.best_estimator_\nprint(\"Best model paramateres:\", grid_search.best_params_)","eed5d08f":"feature_importances = grid_search.best_estimator_.feature_importances_\nsorted(zip(feature_importances, list(X_train)), reverse=True)","73a0f9ce":"final_model = grid_search.best_estimator_\n\nX_test = test_set.drop(\"Slowness in traffic (%)\", axis=1)\ny_test = test_set[\"Slowness in traffic (%)\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions = final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(y_test, final_predictions)\nfinal_rmse = np.sqrt(final_mse)","6197c0a8":"final_rmse","91ec997c":"# Here are a few examples...\npredictions = final_predictions[:10]\nactual_results = y_test[:10]\n\nfor p, a in zip(predictions, actual_results):\n    print(\"Predicted: {:.2f} - Expected: {}\".format(p, a))","ffc33bf0":"### Transformation Pipelines","01aef103":"### Defining sets\nHere we create the test and train set; we decided to maintain the frequency of each 30 minute period among both sets for generalization purposes (avoiding model bias):","3447bd7f":"As expected, the time of the day is the variable that has the largest impact on the Slowness in Traffic among the other features. Features such as Lack of Electricity and Flooding have a considerable effect on the target variable as well.","16b7fc10":"Here you can find what the model defines as the most important features in predicting the Slowness in Traffic, after learning with the train set.","6b34f7b4":"We are not (yet) urban traffic experts, so defining an acceptable error margin isn't clear for us. We would love to have your feedback on that!","32008f4d":"# Introduction\nThis notebook covers a simple exploratory analysis over the *Behavior of the urban traffic of the city of Sao Paulo in Brazil Data Set*. In addition, we shortlisted a few machine learning models to predict the Slowness in Traffic variable (0-100%). Although a very interesting dataset, its size is considerably small, limiting the models performance and the exploratory analysis. **The notebook was built with self-learning purposes only, and we'll be happy to have your feedback on misunderstandings and improvements.** The dataset covers 135 instances, each representing 30 minute timeframes over the week, from Monday 12\/14\/2009 to Friday 12\/18\/2009.\n\nA special thanks to Aurelien Geron, author of *Hands-On Machine Learning* (*O'Reilly*) for the lessons that guided us through this notebook. Similarly, we thank the original owners and donors of the dataset [*Behavior of the urban traffic of the city of Sao Paulo in Brazil Data Set*](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Behavior+of+the+urban+traffic+of+the+city+of+Sao+Paulo+in+Brazil), used in this notebook: Ricardo Pinto Ferreira, Andrea Martiniano and Renato Jose Sassi.\n\nThis notebook was made by Guilherme Matunaga and Paris Mollo.\n\n* [1. Data Overview](#a)\n* [2. Data Manipulation](#b)\n* [3. Data Visualization](#c)\n* [4. Data preprocessing](#d)\n* [5. Model Building](#e)\n* [6. Model Performance](#f)\n\n\n","c28e15ae":"Traffic peaks in the late afternoon and early evening exceed those in the morning; Wednesday has the highest peak in traffic while Monday afternoon is the period with the lowest traffic levels.","605787ed":"The Hour column in the dataset is coded, so the `HOURS` dictionary below translates the codes into the actual time during the day. Similarly, we've created that same logic for the weekdays.","88c75cbc":"# <div id=\"d\">4. Data Preprocessing<\/div>\n","074e4494":"## Final Results\n\nLastly, we applied the model on the test set and calculated the final score!\n","78edb63e":"The dataset instances represent each a 30 minute period that starts at 7:00 AM until 8:00 PM; here is the function that deals with that:","d2f5d4fb":"### Plot overtime","b2a74744":"# <div id=\"c\">3. Exploratory Data Analysis<\/div>\n","d7e24dda":"# <div id=\"b\">2. Data Manipulation<\/div>\nLet's convert the Slowness in Traffic variable into a float object, which is the appropriate data type:","8644b483":"We decided not to remove the outliers, since the dataset is small and outliers in this situation represents an important characteristc of traffic peaks, also known as the \"rush hour\".","f11f745e":"# <div id=\"a\">1. Data Overview<\/div>\n","6c497019":"### Correlation matrices","c2c32084":"## Boxplot","a9be4ef0":"Luckily, there are no null values.","bc65e307":"Here we make available a function so that you can create the day column as weekday nomenclature or as ordinal numbers:","b6df48c5":"# <div id=\"e\">5. Model Building<\/div>\nWe selected a few machine learning models based on the book (*Hands On Machine Learning*) notebook and trained them using 10 fold cross-validation. ","f49fa164":"# <div id=\"f\">6. Model Performances<\/div>\n\nWe preselected the model with the best performance\/lowest mean score (Random Forest) and ran a GridSearch in order to determine the best hyperparameters for optimization purposes.","44c20cab":"We apply the standard scaling on the numerical features through the scikit-learn classes Pipeline and Column Transformer. ","11539388":"Here we're discriminating the numerical values set by removing the categorical features (variable *Day* and variables with only 2 unique values); this is necessary for future feature scaling."}}