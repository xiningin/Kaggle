{"cell_type":{"5e802e0c":"code","daf5827e":"code","8c8d6ba6":"code","dc9ad91f":"code","c9a47a34":"code","cd444c60":"code","4b0f5942":"code","f3f849b0":"code","35327059":"code","1bfb1acd":"code","5a84cb11":"code","e311dd74":"code","1a0b3101":"code","3084b685":"code","1c3b31f0":"code","d12a1afc":"code","9e1e0abe":"markdown","6d2b5447":"markdown","bf2eabf8":"markdown"},"source":{"5e802e0c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","daf5827e":"import seaborn as sns\npd.set_option('display.max_rows', 500)\nimport matplotlib.pyplot as plt\nimport math\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\nfrom sklearn.metrics import confusion_matrix, roc_auc_score ,roc_curve,auc\nfrom sklearn.model_selection import GridSearchCV,StratifiedKFold\nfrom sklearn.preprocessing import LabelEncoder\nfrom collections import Counter\n%pip install ppscore # installing ppscore, library used to check non-linear relationships between our variables\nimport ppscore as pps # importing ppscore\nimport string\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.over_sampling import SMOTE, ADASYN\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\nSEED =45\nsm = SMOTE(random_state=SEED)\nsmk=SMOTETomek(random_state=SEED)\nrus = RandomUnderSampler(random_state=SEED)\nros = RandomOverSampler(random_state=SEED)\nadasyn = ADASYN(random_state=SEED)\nlogreg = LogisticRegression()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n# plt.style.use('fivethirtyeight')","8c8d6ba6":"train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","dc9ad91f":"train.head()","c9a47a34":"## thanks to @Nadezda Demidova  https:\/\/www.kaggle.com\/demidova\/titanic-eda-tutorial-with-seaborn\ntrain.loc[train['PassengerId'] == 631, 'Age'] = 48\n\n# Passengers with wrong number of siblings and parch\ntrain.loc[train['PassengerId'] == 69, ['SibSp', 'Parch']] = [0,0]\ntest.loc[test['PassengerId'] == 1106, ['SibSp', 'Parch']] = [0,0]","cd444c60":"## checking for Survived dependence of Sex feature\ntrain[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","4b0f5942":"## checking for Survived dependence of Sex feature\ntrain[[\"Pclass\", \"Survived\"]].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","f3f849b0":"## checking for Survived dependence of Sex feature\ntrain[[\"Embarked\", \"Survived\"]].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","35327059":"## let's concatenate test and train datasets excluding ID and Target features\ndf = pd.concat((train.loc[:,'Pclass':'Embarked'], test.loc[:,'Pclass':'Embarked'])).reset_index(drop=True)","1bfb1acd":"## for Age imputation let's check its dependence from Pclass\npd.DataFrame(df.groupby('Pclass')['Age'].describe())","5a84cb11":"# New Title feature\ndf['Title'] = df.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\ndf['Title'] = df['Title'].replace(['Lady', 'Countess','Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\ndf['Title'] = df['Title'].map(title_mapping)\ndf['Title'] = df['Title'].fillna(0)\n\n##dropping Name feature\ndf = df.drop(['Name'], axis=1)\n\n# Convert 'Sex' variable to integer form!\ndf[\"Sex\"][df[\"Sex\"] == \"male\"] = 0\ndf[\"Sex\"][df[\"Sex\"] == \"female\"] = 1\ndf[\"Sex\"] = df[\"Sex\"].astype(int)\n\n## Age tuning:\ndf['Age'] = df.groupby(['Pclass'])['Age'].transform(lambda x: x.fillna(x.median()))\ndf[\"Age\"] = df[\"Age\"].astype(int)\n\n# Ticket tuning\ntickets = []\nfor i in list(df.Ticket):\n    if not i.isdigit():\n        tickets.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(\" \")[0])\n    else:\n        tickets.append(\"x\")\ndf[\"Ticket\"] = tickets\n# df['Ticket_Frequency'] = df.groupby('Ticket')['Ticket'].transform('count')\ndf = pd.get_dummies(df, columns= [\"Ticket\"], prefix = \"T\")\n\n\n## Fare tuning:\ndf['Fare'] = df.groupby(\"Pclass\")['Fare'].transform(lambda x: x.fillna(x.median())) \ndf['Zero_Fare'] = df['Fare'].map(lambda x: 1 if x == 0 else (0))\n\n\ndef fare_category(fr): \n    if fr <= 6.91:\n        return 1\n    elif fr <= 13.454 and fr > 6.91:\n        return 2\n    elif fr <= 30 and fr > 13.454:\n        return 3\n    return 4\n\ndf['Fare_cat'] = df['Fare'].apply(fare_category)\n\n# Replace missing values with 'U' for Cabin\ndf['Cabin'] = df['Cabin'].fillna('U')\nimport re\n# Extract first letter\ndf['Cabin'] = df['Cabin'].map(lambda x: re.compile(\"([a-zA-Z]+)\").search(x).group())\ncabin_category = {'A':9, 'B':8, 'C':7, 'D':6, 'E':5, 'F':4, 'G':3, 'T':2, 'U':1}\n# Mapping 'Cabin' to group\ndf['Cabin'] = df['Cabin'].map(cabin_category)\n\n\ndf[\"Embarked\"] = df[\"Embarked\"].fillna(\"C\")\ndf[\"Embarked\"][df[\"Embarked\"] == \"S\"] = 1\ndf[\"Embarked\"][df[\"Embarked\"] == \"C\"] = 2\ndf[\"Embarked\"][df[\"Embarked\"] == \"Q\"] = 3\ndf[\"Embarked\"] = df[\"Embarked\"].astype(int)\n\n# New 'familySize' feature & dripping 2 features:\ndf['FamilySize'] = df['SibSp'] + df['Parch'] + 1\n\ndf['SmallF'] = df['FamilySize'].map(lambda s: 1 if  s == 2  else 0)\ndf['MedF']   = df['FamilySize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndf['LargeF'] = df['FamilySize'].map(lambda s: 1 if s >= 5 else 0)\n\ndf['Senior'] = df['Age'].map(lambda s:1 if s>70 else 0)\ndf['FareCat_Sex'] = df['Fare_cat']*df['Sex']\ndf['Pcl_Sex'] = df['Pclass']*df['Sex']\ndf['Pcl_Title'] = df['Pclass']*df['Title']\ndf['Title_Sex'] = df['Title']*df['Sex']\ndf['Rich_woman'] = 0\ndf['Men_3Class'] = 0\ndf.loc[(df['Pclass']<=2) & (df['Sex']==0), 'Rich_woman'] = 1\ndf.loc[(df['Pclass']==3) & (df['Sex']==1), 'Men_3Class'] = 1\ndf['Rich_woman'] = df['Rich_woman'].astype(np.int8)\ndf['Alone'] = [1 if i == 1 else 0 for i in df['FamilySize']]\n","e311dd74":"#creating matrices for feature selection:\nX_train = df[:train.shape[0]]\nX_test_fin = df[train.shape[0]:]\ny = train.Survived\nX_train['Survived'] = y\ndf = X_train","1a0b3101":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom imblearn.over_sampling import RandomOverSampler\n\nX = df.drop('Survived', axis=1)\ny = df.Survived\n\nx_train, x_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=SEED)\nx_train, y_train= ros.fit_resample(x_train, y_train)\n\nd_train = xgb.DMatrix(x_train, label=y_train)\nd_valid = xgb.DMatrix(x_valid, label=y_valid)\nd_test = xgb.DMatrix(X_test_fin)\n","3084b685":"params = {\n        'objective':'binary:hinge',\n        'max_depth':13, # first pair to tune from 0 to 13\n        'learning_rate':0.1, # 4th to tune\n        'eval_metric':'auc',\n        'min_child_weight':7, # first pair to tune from 0 to 12\n        'subsample':0.8,\n        'colsample_bytree':0.8,\n        'seed':29,\n#         'reg_lambda':1,\n        'reg_alpha':0, ## 3rd to tune \n        'gamma':0, # second for tune with step 0.1 from 0 to 0.5\n        'scale_pos_weight':1,\n        'n_estimators': 5000,\n        'nthread':-1\n}\n\nwatchlist = [(d_train, 'train'), (d_valid, 'valid')]\nnrounds=10000  \nmodel = xgb.train(params, d_train, nrounds, watchlist, early_stopping_rounds=50, \n                           maximize=True, verbose_eval=10)\nfrom sklearn import metrics\n\ny_pred = model.predict(d_valid)\nprint('Accuracy :{0:0.5f}'.format(metrics.accuracy_score(y_valid, y_pred))) \n","1c3b31f0":"sub = pd.DataFrame()\nsub['PassengerId'] = test['PassengerId']\nsub['Survived'] = model.predict(d_test)\n# sub['Survived'] = sub['Survived'].apply(lambda x: 1 if x>0.8 else 0)\n# sub['Survived'] = sub.apply(lambda r: leaks[int(r['PassengerId'])] if int(r['PassengerId']) in leaks else r['Survived'], axis=1)\nsub.to_csv('submission.csv', index=False)\n\nsub.head()","d12a1afc":"## Dmatrix crossvalscore\ndmatrix_data = xgb.DMatrix(data=x_train, label=y_train)\ncross_val = xgb.cv(\n    params=params,\n    dtrain=dmatrix_data, \n    nfold=3,\n    num_boost_round=500, \n    early_stopping_rounds=500, \n    metrics='auc', \n    as_pandas=True, \n    seed=29)\ncross_val.max()","9e1e0abe":"Loading datasets...","6d2b5447":"Final model's accuracy:","bf2eabf8":"Loading libraries..."}}