{"cell_type":{"13310bd5":"code","4c4537ed":"code","82c7ab16":"code","83e6de28":"code","22bba85a":"code","b6777479":"code","5cd480d7":"code","6ae1934c":"code","ba29d840":"code","495a6f7c":"code","0d32d39d":"code","447c1dcf":"code","e458e09f":"code","1ee4089c":"code","146a9e36":"code","ccfbd893":"code","5d01b4e3":"code","e49b9b3c":"code","80b7af14":"code","ed85d25a":"code","cf42c883":"code","a2523b69":"code","4e6e7aea":"code","7789e662":"code","c88cff06":"code","d4e219b4":"code","f0197b6f":"code","2b954a66":"code","77580097":"code","7dda0787":"code","76aaf4b9":"code","fc7897a1":"code","66ae9994":"code","f9dab645":"code","07c62a74":"code","4a985109":"code","e8b4f5bc":"code","ffb285f6":"markdown","953ec1c3":"markdown","11ac6db1":"markdown","62cb20ee":"markdown","d9cf181b":"markdown","02035d7c":"markdown","fa122390":"markdown","b14f12de":"markdown","df7a8433":"markdown","fb41f42b":"markdown","63bafb93":"markdown","29814cd2":"markdown","1f7ca2ee":"markdown","08180a9c":"markdown","4a59ceea":"markdown","bcdc8847":"markdown","1e1915e1":"markdown","89c296f7":"markdown","a813e965":"markdown","043d6695":"markdown","411ccf98":"markdown","88d2a921":"markdown","df16982b":"markdown","2f3bade6":"markdown","b12d4990":"markdown","bd42fbd0":"markdown","64b5ae7d":"markdown","2751df7b":"markdown","c7d7f2eb":"markdown","927359ec":"markdown","54e1bd00":"markdown","7bbd8ccd":"markdown","a1340a38":"markdown","2ebeedfd":"markdown","6045aa25":"markdown"},"source":{"13310bd5":"import umap\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","4c4537ed":"!pip install -U tensorflow_datasets","82c7ab16":"import tensorflow as tf","83e6de28":"# Import TensorFlow Datasets\nimport tensorflow_datasets as tfds\ntfds.disable_progress_bar()\n\n# Helper libraries\nimport math\nimport numpy as np\nimport matplotlib.pyplot as plt","22bba85a":"import logging\nlogger = tf.get_logger()\nlogger.setLevel(logging.ERROR)","b6777479":"dataset, metadata = tfds.load('fashion_mnist', as_supervised=True, with_info=True)\ntrain_dataset, test_dataset = dataset['train'], dataset['test']","5cd480d7":"class_names = metadata.features['label'].names\nprint(\"Class names: {}\".format(class_names))","6ae1934c":"num_train_examples = metadata.splits['train'].num_examples\nnum_test_examples = metadata.splits['test'].num_examples\nprint(\"Number of training examples: {}\".format(num_train_examples))\nprint(\"Number of test examples:     {}\".format(num_test_examples))","ba29d840":"def normalize(images, labels):\n  images = tf.cast(images, tf.float32)\n  images \/= 255\n  return images, labels\n\n# The map function applies the normalize function to each element in the train\n# and test datasets\ntrain_dataset =  train_dataset.map(normalize)\ntest_dataset  =  test_dataset.map(normalize)\n\n# The first time you use the dataset, the images will be loaded from disk\n# Caching will keep them in memory, making training faster\ntrain_dataset =  train_dataset.cache()\ntest_dataset  =  test_dataset.cache()","495a6f7c":"# Take a single image, and remove the color dimension by reshaping\nfor image, label in test_dataset.take(1):\n  break\nimage = image.numpy().reshape((28,28))\n\n# Plot the image - voila a piece of fashion clothing\nplt.figure()\nplt.imshow(image, cmap=plt.cm.binary)\nplt.colorbar()\nplt.grid(False)\nplt.show()","0d32d39d":"plt.figure(figsize=(10,10))\nfor i, (image, label) in enumerate(test_dataset.take(25)):\n    image = image.numpy().reshape((28,28))\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(image, cmap=plt.cm.binary)\n    plt.xlabel(class_names[label])\nplt.show()","447c1dcf":"model = tf.keras.Sequential([\n    tf.keras.layers.Flatten(input_shape=(28, 28, 1)),\n    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n    tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n])","e458e09f":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics=['accuracy'])","1ee4089c":"BATCH_SIZE = 32\ntrain_dataset = train_dataset.cache().repeat().shuffle(num_train_examples).batch(BATCH_SIZE)\ntest_dataset = test_dataset.cache().batch(BATCH_SIZE)","146a9e36":"model.fit(train_dataset, epochs=5, steps_per_epoch=math.ceil(num_train_examples\/BATCH_SIZE))","ccfbd893":"test_loss, test_accuracy = model.evaluate(test_dataset, steps=math.ceil(num_test_examples\/32))\nprint('Accuracy on test dataset:', test_accuracy)","5d01b4e3":"for test_images, test_labels in test_dataset.take(1):\n  test_images = test_images.numpy()\n  test_labels = test_labels.numpy()\n  predictions = model.predict(test_images)","e49b9b3c":"predictions.shape","80b7af14":"predictions[0]","ed85d25a":"np.argmax(predictions[0])","cf42c883":"test_labels[0]","a2523b69":"def plot_image(i, predictions_array, true_labels, images):\n  predictions_array, true_label, img = predictions_array[i], true_labels[i], images[i]\n  plt.grid(False)\n  plt.xticks([])\n  plt.yticks([])\n  \n  plt.imshow(img[...,0], cmap=plt.cm.binary)\n\n  predicted_label = np.argmax(predictions_array)\n  if predicted_label == true_label:\n    color = 'blue'\n  else:\n    color = 'red'\n  \n  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n                                100*np.max(predictions_array),\n                                class_names[true_label]),\n                                color=color)\n\ndef plot_value_array(i, predictions_array, true_label):\n  predictions_array, true_label = predictions_array[i], true_label[i]\n  plt.grid(False)\n  plt.xticks([])\n  plt.yticks([])\n  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n  plt.ylim([0, 1]) \n  predicted_label = np.argmax(predictions_array)\n  \n  thisplot[predicted_label].set_color('red')\n  thisplot[true_label].set_color('blue')","4e6e7aea":"i = 0\nplt.figure(figsize=(6,3))\nplt.subplot(1,2,1)\nplot_image(i, predictions, test_labels, test_images)\nplt.subplot(1,2,2)\nplot_value_array(i, predictions, test_labels)","7789e662":"i = 12\nplt.figure(figsize=(6,3))\nplt.subplot(1,2,1)\nplot_image(i, predictions, test_labels, test_images)\nplt.subplot(1,2,2)\nplot_value_array(i, predictions, test_labels)","c88cff06":"# Plot the first X test images, their predicted label, and the true label\n# Color correct predictions in blue, incorrect predictions in red\nnum_rows = 5\nnum_cols = 3\nnum_images = num_rows*num_cols\nplt.figure(figsize=(2*2*num_cols, 2*num_rows))\nfor i in range(num_images):\n  plt.subplot(num_rows, 2*num_cols, 2*i+1)\n  plot_image(i, predictions, test_labels, test_images)\n  plt.subplot(num_rows, 2*num_cols, 2*i+2)\n  plot_value_array(i, predictions, test_labels)","d4e219b4":"# Grab an image from the test dataset\nimg = test_images[0]\n\nprint(img.shape)","f0197b6f":"# Add the image to a batch where it's the only member.\nimg = np.array([img])\n\nprint(img.shape)","2b954a66":"predictions_single = model.predict(img)\n\nprint(predictions_single)","77580097":"plot_value_array(0, predictions_single, test_labels)\n_ = plt.xticks(range(10), class_names, rotation=45)","7dda0787":"np.argmax(predictions_single[0])","76aaf4b9":"# https:\/\/www.kaggle.com\/nulldata\/tsne-alternate-umap-3d-viz-on-fashion-mnist\nfashion_train = pd.read_csv(\"..\/input\/fashionmnist\/fashion-mnist_train.csv\")","fc7897a1":"data = fashion_train.iloc[:, 1:].values.astype(np.float32)\ntarget = fashion_train['label'].values","66ae9994":"reduce = umap.UMAP(random_state = 223) #just for reproducibility\nembedding = reduce.fit_transform(data)","f9dab645":"df = pd.DataFrame(embedding, columns=('x', 'y'))\ndf[\"class\"] = target","07c62a74":"labels = { 0: 'T-shirt\/top', 1: 'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat', \n          5: 'Sandal', 6: 'Shirt', 7: 'Sneaker', 8 : 'Bag', 9 : 'Ankle boot'}\n\ndf[\"class\"].replace(labels, inplace=True)","4a985109":"sns.set_style(\"whitegrid\", {'axes.grid' : False})\n#adjusting plot dots with plot_kws\nax = sns.pairplot(x_vars = [\"x\"], y_vars = [\"y\"],data = df, \n             hue = \"class\",size=11, plot_kws={\"s\": 4});\nax.fig.suptitle('Fashion MNIST clustered with UMAP') ;","e8b4f5bc":"#code courtesy: @huseinzol05 \n#copied from here: https:\/\/www.kaggle.com\/huseinzol05\/3d-visualization\nfrom ast import literal_eval\nimport plotly.offline as py\npy.init_notebook_mode(connected = True)\nimport plotly.graph_objs as go\ncolors = ['rgb(0,31,63)', 'rgb(255,133,27)', 'rgb(255,65,54)', 'rgb(0,116,217)', 'rgb(133,20,75)', 'rgb(57,204,204)',\n'rgb(240,18,190)', 'rgb(46,204,64)', 'rgb(1,255,112)', 'rgb(255,220,0)',\n'rgb(76,114,176)', 'rgb(85,168,104)', 'rgb(129,114,178)', 'rgb(100,181,205)']\ndata_graph = []\nfor no, name in enumerate(np.unique(df[\"class\"])):\n    graph = go.Scatter3d(\n    x = df[df[\"class\"] == name][\"x\"],\n    y = df[df[\"class\"] == name][\"y\"],\n    z = df[df[\"class\"] == name][\"class\"],\n    name = labels[no],\n    mode = 'markers',\n    marker = dict(\n        size = 12,\n        line = dict(\n            color = '#%02x%02x%02x' % literal_eval(colors[no][3:]),\n            width = 0.5\n            ),\n        opacity = 0.5\n        )\n    )\n    data_graph.append(graph)\n    \nlayout = go.Layout(\n    scene = dict(\n        camera = dict(\n            eye = dict(\n            x = 0.5,\n            y = 0.5,\n            z = 0.5\n            )\n        )\n    ),\n    margin = dict(\n        l = 0,\n        r = 0,\n        b = 0,\n        t = 0\n    )\n)\nfig = go.Figure(data = data_graph, layout = layout)\npy.iplot(fig, filename = '3d-scatter')","ffb285f6":"### Making the representation a Pandas Dataframe for Modelling and Visualization Purposes","953ec1c3":"As it turns out, the accuracy on the test dataset is smaller than the accuracy on the training dataset. This is completely normal, since the model was trained on the `train_dataset`. When the model sees images it has never seen during training, (that is, from the `test_dataset`), we can expect performance to go down. ","11ac6db1":"A prediction is an array of 10 numbers. These describe the \"confidence\" of the model that the image corresponds to each of the 10 different articles of clothing. We can see which label has the highest confidence value:","62cb20ee":"This network has three layers:\n\n* **input** `tf.keras.layers.Flatten` \u2014 This layer transforms the images from a 2d-array of 28 $\\times$ 28 pixels, to a 1d-array of 784 pixels (28\\*28). Think of this layer as unstacking rows of pixels in the image and lining them up. This layer has no parameters to learn, as it only reformats the data.\n\n* **\"hidden\"** `tf.keras.layers.Dense`\u2014 A densely connected layer of 128 neurons. Each neuron (or node) takes input from all 784 nodes in the previous layer, weighting that input according to hidden parameters which will be learned during training, and outputs a single value to the next layer.\n\n* **output**  `tf.keras.layers.Dense` \u2014 A 128-neuron, followed by 10-node *softmax* layer. Each node represents a class of clothing. As in the previous layer, the final layer takes input from the 128 nodes in the layer before it, and outputs a value in the range `[0, 1]`, representing the probability that the image belongs to that class. The sum of all 10 node values is 1.\n\n> Note: Using `softmax` activation and `SparseCategoricalCrossentropy()` has issues and which are patched by the `tf.keras` model. A safer approach, in general, is to use a linear output (no activation function) with `SparseCategoricalCrossentropy(from_logits=True)`.\n\n\n### Compile the model\n\nBefore the model is ready for training, it needs a few more settings. These are added during the model's *compile* step:\n\n\n* *Loss function* \u2014 An algorithm for measuring how far the model's outputs are from the desired output. The goal of training is this measures loss.\n* *Optimizer* \u2014An algorithm for adjusting the inner parameters of the model in order to minimize loss.\n* *Metrics* \u2014Used to monitor the training and testing steps. The following example uses *accuracy*, the fraction of the images that are correctly classified.","d9cf181b":"Loading the dataset returns metadata as well as a *training dataset* and *test dataset*.\n\n* The model is trained using `train_dataset`.\n* The model is tested against `test_dataset`.\n\nThe images are 28 $\\times$ 28 arrays, with pixel values in the range `[0, 255]`. The *labels* are an array of integers, in the range `[0, 9]`. These correspond to the *class* of clothing the image represents:\n\n<table>\n  <tr>\n    <th>Label<\/th>\n    <th>Class<\/th>\n  <\/tr>\n  <tr>\n    <td>0<\/td>\n    <td>T-shirt\/top<\/td>\n  <\/tr>\n  <tr>\n    <td>1<\/td>\n    <td>Trouser<\/td>\n  <\/tr>\n    <tr>\n    <td>2<\/td>\n    <td>Pullover<\/td>\n  <\/tr>\n    <tr>\n    <td>3<\/td>\n    <td>Dress<\/td>\n  <\/tr>\n    <tr>\n    <td>4<\/td>\n    <td>Coat<\/td>\n  <\/tr>\n    <tr>\n    <td>5<\/td>\n    <td>Sandal<\/td>\n  <\/tr>\n    <tr>\n    <td>6<\/td>\n    <td>Shirt<\/td>\n  <\/tr>\n    <tr>\n    <td>7<\/td>\n    <td>Sneaker<\/td>\n  <\/tr>\n    <tr>\n    <td>8<\/td>\n    <td>Bag<\/td>\n  <\/tr>\n    <tr>\n    <td>9<\/td>\n    <td>Ankle boot<\/td>\n  <\/tr>\n<\/table>\n\nEach image is mapped to a single label. Since the *class names* are not included with the dataset, store them here to use later when plotting the images:","02035d7c":"## Let's make it Interactive with a 3D Plot!","fa122390":"# Train the model\n\nFirst, we define the iteration behavior for the train dataset:\n1. Repeat forever by specifying `dataset.repeat()` (the `epochs` parameter described below limits how long we perform training).\n2. The `dataset.shuffle(60000)` randomizes the order so our model cannot learn anything from the order of the examples.\n3. And `dataset.batch(32)` tells `model.fit` to use batches of 32 images and labels when updating the model variables.\n\nTraining is performed by calling the `model.fit` method:\n1. Feed the training data to the model using `train_dataset`.\n2. The model learns to associate images and labels.\n3. The `epochs=5` parameter limits training to 5 full iterations of the training dataset, so a total of 5 * 60000 = 300000 examples.\n\n(Don't worry about `steps_per_epoch`, the requirement to have this flag will soon be removed.)","b14f12de":"Now predict the image:","df7a8433":"Let's look at the 0th image, predictions, and prediction array. ","fb41f42b":"Let's plot several images with their predictions. Correct prediction labels are blue and incorrect prediction labels are red. The number gives the percent (out of 100) for the predicted label. Note that it can be wrong even when very confident. ","63bafb93":"We can graph this to look at the full set of 10 class predictions\n\n> Bloc en retrait","29814cd2":"And, as before, the model predicts a label of 6 (shirt).","1f7ca2ee":"# Import the Fashion MNIST dataset\n\n#### This guide uses the [Fashion MNIST](https:\/\/github.com\/zalandoresearch\/fashion-mnist) dataset, which contains 70,000 grayscale images in 10 categories. The images show individual articles of clothing at low resolution (28 \u00d7 28 pixels), as seen here:\n\n![](https:\/\/raw.githubusercontent.com\/zalandoresearch\/fashion-mnist\/master\/doc\/img\/fashion-mnist-sprite.png)\n\n#### Fashion MNIST is intended as a drop-in replacement for the classic [MNIST](http:\/\/yann.lecun.com\/exdb\/mnist\/) dataset\u2014often used as the \"Hello, World\" of machine learning programs for computer vision. The MNIST dataset contains images of handwritten digits (0, 1, 2, etc) in an identical format to the articles of clothing we'll use here.\n\n#### This guide uses Fashion MNIST for variety, and because it's a slightly more challenging problem than regular MNIST. Both datasets are relatively small and are used to verify that an algorithm works as expected. They're good starting points to test and debug code.\n\n#### We will use 60,000 images to train the network and 10,000 images to evaluate how accurately the network learned to classify images. You can access the Fashion MNIST directly from TensorFlow, using the [Datasets](https:\/\/www.tensorflow.org\/datasets) API:","08180a9c":"# What is UMAP?\n\n\nUniform Manifold Approximation and Projection (UMAP) is a dimension reduction technique that can be used for visualisation similarly to t-SNE, but also for general non-linear dimension reduction.\n\n# Why UMAP?\n\n* UMAP is fast. It can handle large datasets and high dimensional data without too much difficulty, scaling beyond what most t-SNE packages can manage.\n\n* UMAP scales well in embedding dimension -- it isn't just for visualisation! You can use UMAP as a general purpose dimension reduction technique as a preliminary step to other machine learning tasks. With a little care (documentation on how to be careful is coming) it partners well with the hdbscan clustering library.\n\n* UMAP often performs better at preserving aspects of global structure of the data than t-SNE. This means that it can often provide a better \"big picture\" view of your data as well as preserving local neighbor relations.\n\n* UMAP supports a wide variety of distance functions, including non-metric distance functions such as cosine distance and correlation distance. You can finally embed word vectors properly using cosine distance!\n\n* UMAP supports adding new points to an existing embedding via the standard sklearn transform method. This means that UMAP can be used as a preprocessing transformer in sklearn pipelines.\n\n* UMAP supports supervised and semi-supervised dimension reduction. This means that if you have label information that you wish to use as extra information for dimension reduction (even if it is just partial labelling) you can do that -- as simply as providing it as the y parameter in the fit method.\n\n* UMAP has solid theoretical foundations in manifold learning (see the paper on ArXiv). This both justifies the approach and allows for further extensions that will soon be added to the library (embedding dataframes etc.).\n\n\nRead more here: https:\/\/github.com\/lmcinnes\/umap\n\n**The purpose of the kernel is to introduce UMAP implementation in Python**\n","4a59ceea":"# References\n\nThe Training part from this [Colab Notebook](https:\/\/colab.research.google.com\/github\/tensorflow\/examples\/blob\/master\/courses\/udacity_intro_to_tensorflow_for_deep_learning\/l03c01_classifying_images_of_clothing.ipynb#scrollTo=o_rzNSdrCaXY)\n\nThe details for the underlying mathematics can be found in the paper on [ArXiv](https:\/\/arxiv.org\/abs\/1802.03426) :\n\nMcInnes, L, Healy, J, *UMAP: Uniform Manifold Approximation and Projection\nfor Dimension Reduction*, ArXiv e-prints 1802.03426, 2018\n\nThe important thing is that you don't need to worry about that -- you can use\nUMAP right now for dimension reduction and visualisation as easily as a drop\nin replacement for scikit-learn's t-SNE.\n\nDocumentation is available via [ReadTheDocs](https:\/\/umap-learn.readthedocs.io\/).\n\n### UMAP in R\n\nFor R users, There is a package avaialble that wraps on the above mentioned Python package.\n\nhttps:\/\/github.com\/ropenscilabs\/umapr","bcdc8847":"# Evaluate accuracy\n\nNext, compare how the model performs on the test dataset. Use all examples we have in the test dataset to assess accuracy.","1e1915e1":"##### Display the first 25 images from the *training set* and display the class name below each image. Verify that the data is in the correct format and we're ready to build and train the network.","89c296f7":"# Creating a 2-dimensional representation of the data","a813e965":"## Setup the layers\n\nThe basic building block of a neural network is the *layer*. A layer extracts a representation from the data fed into it. Hopefully, a series of connected layers results in a representation that is meaningful for the problem at hand.\n\nMuch of deep learning consists of chaining together simple layers. Most layers, like `tf.keras.layers.Dense`, have internal parameters which are adjusted (\"learned\") during training.","043d6695":"# Explore the data\n\nLet's explore the format of the dataset before training the model. The following shows there are 60,000 images in the training set, and 10000 images in the test set:","411ccf98":"### Replacing Class numeric Values with Fashion Labels ","88d2a921":"# Visualizing the reduced Dataframe","df16982b":"# Install and import dependencies","2f3bade6":"# Classifying Images of Clothing\n\n#### In this tutorial, we'll build and train a neural network to classify images of clothing, like sneakers and shirts.\n\n#### It's okay if you don't understand everything. This is a fast-paced overview of a complete TensorFlow program, with explanations along the way. The goal is to get the general sense of a TensorFlow project, not to catch every detail.\n\n#### This guide uses [tf.keras](https:\/\/www.tensorflow.org\/guide\/keras\/sequential_model), a high-level API to build and train models in TensorFlow.","b12d4990":"`tf.keras` models are optimized to make predictions on a *batch*, or collection, of examples at once. So even though we're using a single image, we need to add it to a list:","bd42fbd0":"# Build the model\n\nBuilding the neural network requires configuring the layers of the model, then compiling the model.","64b5ae7d":"# Explore the processed data\n\nLet's plot an image to see what it looks like.","2751df7b":"Finally, use the trained model to make a prediction about a single image. ","c7d7f2eb":"# Make predictions and explore\n\nWith the model trained, we can use it to make predictions about some images.","927359ec":"Here, the model has predicted the label for each image in the testing set. Let's take a look at the first prediction:","54e1bd00":"`model.predict` returns a list of lists, one for each image in the batch of data. Grab the predictions for our (only) image in the batch:","7bbd8ccd":"So the model is most confident that this image is a shirt, or `class_names[6]`. And we can check the test label to see this is correct:","a1340a38":"As the model trains, the loss and accuracy metrics are displayed. This model reaches an accuracy of about 0.89 (or 89%) on the training data.","2ebeedfd":"# Preprocess the data\n\nThe value of each pixel in the image data is an integer in the range `[0,255]`. For the model to work properly, these values need to be normalized to the range `[0,1]`. So here we create a normalization function, and then apply it to each image in the test and train datasets.","6045aa25":"# TSNE Alternate: UMAP 3D Viz on Fashion MNIST\n\nThis part of code is from this great [kernel](https:\/\/www.kaggle.com\/nulldata\/tsne-alternate-umap-3d-viz-on-fashion-mnist)"}}