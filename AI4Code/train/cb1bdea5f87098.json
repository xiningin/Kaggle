{"cell_type":{"2360cc4b":"code","c9635da4":"code","d4863f5e":"code","4b34b289":"code","ac8e7760":"code","8d128f61":"code","c1bb76d0":"code","9d198940":"code","bbcd5ef8":"code","f0c8e95a":"code","96e6f6d9":"code","7b8755f1":"code","1200510f":"code","0b0ab598":"code","f5ffafa2":"code","4274bc0f":"code","1cf4fdd8":"code","0558f027":"code","c686a4f8":"code","bbe14c8d":"code","481c4bb3":"code","d9325550":"code","5c2f5170":"code","c72546bd":"code","d4f2d21a":"code","67e3a348":"code","d1ccd735":"code","9084971c":"code","a1b69a63":"code","4e0a452c":"code","a9d3bfb3":"code","099943b9":"code","5f25e784":"code","4215eb8c":"code","50202dfc":"code","4aaa6261":"code","46ea24a8":"code","9999ab35":"code","c2cd53f1":"code","f8835a83":"code","82823a3f":"code","d801317a":"code","9547310e":"code","3f203e19":"code","a9893515":"code","0f1d8384":"code","7db1c9de":"code","e83509ea":"code","f8b24c82":"code","0868c2ed":"code","2f65cf2f":"code","8c258ff4":"code","f7ab3859":"code","f065bc6f":"code","9f0dc58e":"code","17c2c71e":"code","1ec0a8bc":"code","a1d082c8":"code","1dee720e":"code","0d829d62":"code","c915b481":"code","d0a623b6":"code","32c64182":"code","d6839cd4":"code","8d9ffbe1":"code","f4184280":"code","fecd4fc9":"code","9cda04f9":"code","771faccf":"code","6058e1a2":"code","64d7a3e8":"code","0fed68c1":"code","1e36c02f":"code","c6255941":"code","7808035b":"code","28d7680a":"code","1a2fc9ad":"code","b950690a":"code","c20febc1":"code","2dca175a":"code","a7eea1db":"code","7d69e8a1":"code","078b0d50":"code","a72e31e5":"code","e4683450":"code","4ef3485a":"markdown","eff670c9":"markdown","0d39b56d":"markdown","aacb2080":"markdown","714b9203":"markdown","4f60ad73":"markdown","83909cc8":"markdown","3ac1b502":"markdown","ae2612f9":"markdown","adc24671":"markdown","eb7afb40":"markdown","91bd053a":"markdown","cd791cf3":"markdown","1c65f976":"markdown","159cb0d1":"markdown"},"source":{"2360cc4b":"!wget \"https:\/\/www.dropbox.com\/s\/qpxhukoqy9sntaq\/e5c05d46301011ec.zip?dl=0\"","c9635da4":"!unzip \".\/e5c05d46301011ec.zip?dl=0\"","d4863f5e":"# Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegressionCV\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import f1_score\nimport warnings\nwarnings.filterwarnings('ignore')","4b34b289":"# Store the data files path\ntrain_path = \".\/Animal State Prediction - dataset\/train.csv\"\ntest_path = \".\/Animal State Prediction - dataset\/test.csv\"\n\n# Load the data\ntrain = pd.read_csv(train_path)\ntest = pd.read_csv(test_path)","ac8e7760":"train.head()","8d128f61":"# Print the size and shape of the data\ndef print_size(data, data_type='train'):\n\n  # find the number of rows and columns in the data\n  num_rows = data.shape[0]\n  num_col = data.shape[1]\n\n  print(\"Number of rows in the {} dataset : {}\".format(data_type, num_rows))\n  print(\"Number of columns in the {} dataset : {}\".format(data_type, num_col))\n\n# Call on the train and the test set\nprint_size(train, data_type='train')\nprint_size(test, data_type='test')","c1bb76d0":"# Check for null values in both the train and test set\ntrain.isnull().sum()","9d198940":"test.isnull().sum()","bbcd5ef8":"# Print the percentage of null values in the train data\ndef percentage_null_values(train):\n  # total number of rows\n  total_rows = train.shape[0]\n  for col in train.columns:\n\n    # find the number of null values for that particular column\n    null_count = train[col].isnull().sum()\n\n    # if the number of null values is greater than 0 find the percentage\n    if null_count > 0:\n      perc_null_values = null_count*100\/total_rows\n      print(\"Total number of null values in the {} column is {:.4f}%\".format(col, perc_null_values))\n\n# Call the function\npercentage_null_values(train)","f0c8e95a":"# Check the percentage of information loss on dropping the null values\ntotal_rows = train.shape[0]\ntotal_rows_after_dropping = train.dropna(axis=0).shape[0]\n\nperc_info_loss = (total_rows - total_rows_after_dropping)*100\/total_rows\nprint(\"Percentage information loss after dropping null rows {:.3f}%\".format(perc_info_loss))","96e6f6d9":"train['outcome_datetime'] = pd.to_datetime(train['outcome_datetime'])","7b8755f1":"# Extract the data with null values\nnull_outcome_data = train[train['outcome_datetime'].isnull() == True]\nnull_outcome_data.head()","1200510f":"# Encode the outcome_weekday\ndef encode_outcome_weekday(x):\n\n  if x == 'Monday':\n    x = 1\n  elif x == 'Tuesday':\n    x = 2\n  elif x == 'Wednesday':\n    x = 3\n  elif x == 'Thursday':\n    x = 4\n  elif x == 'Friday':\n    x = 5\n  elif x == 'Saturday':\n    x = 6\n  elif x == 'Sunday':\n    x = 7\n\n  return x\n\n# Apply the function\ntrain['outcome_weekday'] = train['outcome_weekday'].apply(lambda x : encode_outcome_weekday(x))\ntest['outcome_weekday'] = test['outcome_weekday'].apply(lambda x : encode_outcome_weekday(x))\n\ntrain['intake_weekday'] = train['intake_weekday'].apply(lambda x : encode_outcome_weekday(x))\ntest['intake_weekday'] = test['intake_weekday'].apply(lambda x : encode_outcome_weekday(x))","0b0ab598":"# Store the null index\nnull_idx = train[train['outcome_datetime'].isnull() == True].index.tolist()","f5ffafa2":"# Encode the null data to calculate dates to impute null values\nnull_outcome_data['outcome_weekday'] = null_outcome_data['outcome_weekday'].apply(lambda x : encode_outcome_weekday(x)) ","4274bc0f":"# Create the values to impute\nvalues_to_impute = pd.to_datetime(null_outcome_data['outcome_monthyear'] + '-' \n                                  + null_outcome_data['outcome_weekday'].astype('str') + \n                                  \" \" + null_outcome_data['outcome_hour'].astype('str') + \n                                  ':' + str(00) + ':' + str(00))","1cf4fdd8":"# impute the null values\nfor idx, value in zip(null_idx, values_to_impute):\n  train['outcome_datetime'].iloc[idx] = value","0558f027":"# null values from the outcome_datetime column have been imputed\ntrain.isnull().sum()","c686a4f8":"# drop the 2 other rows with null values\ntrain.dropna(axis=0, inplace=True)","bbe14c8d":"train.isnull().sum()","481c4bb3":"train.head()","d9325550":"print(train.info())","5c2f5170":"# Change the datatypes of the variables\n\n# extract age number from age_upon_intake and age_upon_outcome\ndef extract_age_numbers(x):\n  return int(re.sub('[a-zA-Z]', '', x))\n\ntrain['age_upon_intake'] = train['age_upon_intake'].apply(lambda x : extract_age_numbers(x))\ntrain['age_upon_outcome'] = train['age_upon_outcome'].apply(lambda x : extract_age_numbers(x))\n\ntest['age_upon_intake'] = test['age_upon_intake'].apply(lambda x : extract_age_numbers(x))\ntest['age_upon_outcome'] = test['age_upon_outcome'].apply(lambda x : extract_age_numbers(x))","c72546bd":"# Plot the distribution of the dob_year, dob_month\nfigure, ax = plt.subplots(1, 2, figsize=(15,4))\nplt.style.use('seaborn')\n\nax[0].hist(train['dob_year'])\nax[0].set_xlabel(\"Year of Birth\")\nax[0].set_ylabel(\"Frequency\")\n\nax[1].hist(train['dob_month'])\nax[1].set_xlabel(\"Month of Birth\")\nax[1].set_ylabel(\"Frequency\")\n\nfigure.show()","d4f2d21a":"train.head()","67e3a348":"train[['age_upon_intake', 'age_upon_outcome']]","d1ccd735":"# Plot the distribution of age_upon_intake and age_upon_outcome\nfigure, ax = plt.subplots(1, 2, figsize=(15,4))\nplt.style.use('seaborn')\n\nax[0].hist(train['age_upon_intake'])\nax[0].set_xlabel(\"Age of Animal\")\nax[0].set_ylabel(\"Frequency\")\nax[0].set_title('Age of Animal upon intake')\n\nax[1].hist(train['age_upon_outcome'])\nax[1].set_xlabel(\"Age of Animal\")\nax[1].set_ylabel(\"Frequency\")\nax[1].set_title('Age of Animal upon outcome')\n\nfigure.show()","9084971c":"# Convert into datetime\ntrain['intake_datetime'] = pd.to_datetime(train['intake_datetime'])\ntrain['outcome_datetime'] = pd.to_datetime(train['outcome_datetime'])\n\ntest['intake_datetime'] = pd.to_datetime(test['intake_datetime'])\ntest['outcome_datetime'] = pd.to_datetime(test['outcome_datetime'])","a1b69a63":"# plot the number of days of stay of an animal\n\nfigure, ax = plt.subplots(1, 2, figsize=(15, 5))\ntrain['time_in_shelter_days'].hist(bins=50, ax=ax[0])\nax[0].set_xlabel(\"Number of days in shelter\")\nax[0].set_title(\"Train Set\")\n\ntest['time_in_shelter_days'].hist(bins=50, ax=ax[1])\nax[1].set_xlabel(\"Number of days in shelter\")\nax[1].set_title(\"Test Set\")\n\nfigure.show()","4e0a452c":"# finding number of days each animal stayed at the center\ntrain['number_of_days_stayed'] = (train['outcome_datetime'] - train['intake_datetime']).dt.days\ntest['number_of_days_stayed'] = (test['outcome_datetime'] - test['intake_datetime']).dt.days","a9d3bfb3":"(test['outcome_datetime'] - test['intake_datetime'])","099943b9":"# Plot the above distribution\ntrain['number_of_days_stayed'].hist(bins=50)\nplt.title(\"Number of Days an Animal stayed at the center\")\nplt.xlim(-100, 500)\nplt.xticks(np.arange(-100, 510, 50))\nplt.show()","5f25e784":"# Plot the same distribution in test set\ntest['number_of_days_stayed'].hist(bins=50)\nplt.title(\"Number of Days an Animal stayed at the center (test set)\")\nplt.xlim(-100, 500)\nplt.xticks(np.arange(-100, 510, 50))\nplt.show()","4215eb8c":"# find the percentage of such incorrect points in the train and test set\n\nincorrect_rows = train[train['number_of_days_stayed'] < 0].shape[0]\nperc_incorrect_rows = incorrect_rows*100\/total_rows\nprint(\"Percentage of incorrect data points in the train set {:.3f}%\".format(perc_incorrect_rows))\n\nincorrect_rows = test[test['number_of_days_stayed'] < 0].shape[0]\nperc_incorrect_rows = incorrect_rows*100\/total_rows\nprint(\"Percentage of incorrect data points in the test set {:.3f}%\".format(perc_incorrect_rows))","50202dfc":"train.info()","4aaa6261":"# Analyse the incorrect data points\nincorrect_data_points = train[train['number_of_days_stayed'] < 0][['number_of_days_stayed', \n                                                                   'time_in_shelter', \n                                                                   'time_in_shelter_days', \n                                                                   'intake_datetime', \n                                                                   'outcome_datetime']]\n\n# Extract the incorrect index\nincorrect_idx = incorrect_data_points.index.tolist()","46ea24a8":"train.loc[incorrect_idx]['number_of_days_stayed']","9999ab35":"values_to_replace = incorrect_data_points['intake_datetime'] + pd.to_timedelta(incorrect_data_points['time_in_shelter'])\n\n# Replace the incorrect outcome datetime values\nfor idx, value in zip(incorrect_idx, values_to_replace):\n  train['outcome_datetime'].loc[idx] = value\n","c2cd53f1":"# Check the distribution after imputing\n(train['outcome_datetime'] - train['intake_datetime']).dt.days.hist(bins=50)\nplt.show()","f8835a83":"# perform the same operations on the test set\n# Analyse the incorrect data points\nincorrect_data_points_test = test[test['number_of_days_stayed'] < 0][['number_of_days_stayed', \n                                                                   'time_in_shelter', \n                                                                   'time_in_shelter_days', \n                                                                   'intake_datetime', \n                                                                   'outcome_datetime']]\n\n# Extract the incorrect index\nincorrect_idx_test = incorrect_data_points_test.index.tolist()\n\nvalues_to_replace_test = incorrect_data_points_test['intake_datetime'] + pd.to_timedelta(incorrect_data_points_test['time_in_shelter'])\n\n# Replace the incorrect outcome datetime values\nfor idx, value in zip(incorrect_idx_test, values_to_replace_test):\n  test['outcome_datetime'].loc[idx] = value","82823a3f":"# Check the distribution after imputing\n(test['outcome_datetime'] - test['intake_datetime']).dt.days.hist(bins=50)\nplt.show()","d801317a":"# Replace the outcome month, outcome_year, outcome_monthyear, outcome_hour\ntrain['outcome_month'] = train['outcome_datetime'].dt.month\ntrain['outcome_year'] = train['outcome_datetime'].dt.year\ntrain['outcome_monthyear'] = train['outcome_year'].astype('str') + '-' + train['outcome_month'].astype('str')\ntrain['outcome_hour'] = train['outcome_datetime'].dt.hour\n\n# Perform the same analysis on test set\ntest['outcome_month'] = test['outcome_datetime'].dt.month\ntest['outcome_year'] = test['outcome_datetime'].dt.year\ntest['outcome_monthyear'] = test['outcome_year'].astype('str') + '-' + test['outcome_month'].astype('str')\ntest['outcome_hour'] = test['outcome_datetime'].dt.hour","9547310e":"# animal_type analysis\nsns.countplot(data=train, x='animal_type')\nplt.show()","3f203e19":"sns.countplot(data=train, x='outcome_type')\nplt.show()","a9893515":"# dob_year, dob_month, age_upon_intake across animal type\nfigure, ax = plt.subplots(2, 2, figsize=(12, 12))\n\nax1 = ax[0]\nax2 = ax[1]\n\nsns.boxplot(data=train, x='animal_type', y='dob_year', ax=ax1[0])\nsns.boxplot(data=train, x='animal_type', y='dob_month', ax=ax1[1])\nsns.boxplot(data=train, x='animal_type', y='age_upon_intake', ax=ax2[0])\nsns.boxplot(data=train, x='animal_type', y='time_in_shelter_days', ax=ax2[1])\n\nfigure.show()","0f1d8384":"# visulaise the number of intakes and outcomes with time\n\nfigure, ax = plt.subplots(1, 2, figsize=(15,6))\nplt.style.use('seaborn')\n\ntrain.groupby('intake_datetime').count()['animal_id_outcome'].plot(ax=ax[0])\nax[0].set_xlabel(\"Intake Date\")\nax[0].set_ylabel(\"Number of intakes\")\nax[0].set_title(\"Number of Intakes with time\")\n\ntrain.groupby('outcome_datetime').count()['animal_id_outcome'].plot(ax=ax[1])\nax[1].set_xlabel(\"Outcome Date\")\nax[1].set_ylabel(\"Number of ouctome\")\nax[1].set_title(\"Number of Outcome with time\")\n\nfigure.show()\n","7db1c9de":"# Plot the correlations matrix\ncorr_matrix = train.corr()\nplt.figure(figsize=(20,15))\nsns.heatmap(corr_matrix, annot=True)\nplt.show()","e83509ea":"# drop the features with constant variance like count\ntrain.drop('count', axis=1, inplace=True)\ntest.drop('count', axis=1, inplace=True)","f8b24c82":"train.shape, test.shape","0868c2ed":"# Remove the highly correlated features\ncorr_matrix_2 = train.drop(['age_upon_intake_(days)', \n                            'age_upon_intake_(years)', \n                            'age_upon_outcome', \n                            'age_upon_outcome_(days)', \n                            'age_upon_outcome_(years)', \n                            'outcome_year',\n                            'number_of_days_stayed',\n                            'intake_number'], axis=1).corr()\n# Plot the heatmap\nplt.figure(figsize=(12,8))\nsns.heatmap(corr_matrix_2, annot=True)\nplt.show()","2f65cf2f":"# Store the final data\ntrain_final = train.drop(['age_upon_intake_(days)', \n                            'age_upon_intake_(years)', \n                            'age_upon_outcome', \n                            'age_upon_outcome_(days)', \n                            'age_upon_outcome_(years)', \n                            'outcome_year',\n                            'number_of_days_stayed',\n                            'intake_number',\n                            'age_upon_outcome_age_group'], axis=1)\n\ntest_final = test.drop(['age_upon_intake_(days)', \n                            'age_upon_intake_(years)', \n                            'age_upon_outcome', \n                            'age_upon_outcome_(days)', \n                            'age_upon_outcome_(years)', \n                            'outcome_year',\n                            'number_of_days_stayed',\n                            'intake_number',\n                            'age_upon_outcome_age_group'], axis=1)\ntrain_final.head()","8c258ff4":"# Encode the columns\ncols_to_encode = ['animal_type', 'breed', 'color', \n                  'intake_condition', 'intake_type',\n                  'sex_upon_intake', 'age_upon_intake_age_group', \n                  'sex_upon_outcome']\n# concat train and test\nX = train_final.drop('outcome_type', axis=1)\ntrain_len = X.shape[0]\nconcatenated_data = pd.concat((X, test_final), axis=0)","f7ab3859":"# Encode the column\nfor col in cols_to_encode:\n  le = LabelEncoder()\n  concatenated_data[col] = le.fit_transform(concatenated_data[col])","f065bc6f":"# Extract day\nconcatenated_data['intake_day'] = concatenated_data['intake_datetime'].dt.day","9f0dc58e":"# drop the datetime values\nconcatenated_data.drop(['intake_datetime', 'date_of_birth', 'outcome_datetime', 'outcome_monthyear'], axis=1, inplace=True)","17c2c71e":"concatenated_data['time_in_shelter'] = pd.to_timedelta(concatenated_data['time_in_shelter'])","1ec0a8bc":"concatenated_data['hours_in_shelter'] = concatenated_data['time_in_shelter'].dt.total_seconds()\/\/3600","a1d082c8":"concatenated_data.drop(['time_in_shelter'], axis=1, inplace=True)","1dee720e":"# encode year-month column\nconcatenated_data['intake_monthyear'] = le.fit_transform(concatenated_data['intake_monthyear'])","0d829d62":"concatenated_data.head()","c915b481":"# Encode the target column\ny = train_final['outcome_type']\ny_encoded = le.fit_transform(y)","d0a623b6":"# Split the concatenated data into train and test\nX_final = concatenated_data.iloc[ : train_len]\ntest_data = concatenated_data.iloc[train_len : ]","32c64182":"X_final.drop('animal_id_outcome', axis=1, inplace=True)\ntest_IDs = test_data['animal_id_outcome']\ntest_data.drop('animal_id_outcome', axis=1, inplace=True)","d6839cd4":"# Scale the data\nX_columns = X_final.columns\n\nss = StandardScaler()\nX_scaled = ss.fit_transform(X_final)\ntest_scaled = ss.transform(test_data)","8d9ffbe1":"X_scaled_df = pd.DataFrame(X_scaled, columns = X_columns)\ntest_scaled_df = pd.DataFrame(test_scaled, columns = X_columns)","f4184280":"X_scaled_df.head()","fecd4fc9":"test_scaled_df.head()","9cda04f9":"# Split the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y_encoded, test_size=0.2, random_state=42)","771faccf":"# 1. Compute f1 Score\ndef compute_f1_score(model_dict, X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test):\n\n  model_name = list(model_dict.keys())[0]\n  model_obj = list(model_dict.values())[0]\n\n  # Make predictions\n\n  # 1. Training predictions\n  train_preds = model_obj.predict(X_train)\n\n  # 2. Testing predictions\n  test_preds = model_obj.predict(X_test)\n\n  # Compute Recall Score\n\n  # 1. Training Score\n  train_recall = f1_score(y_train, train_preds, average='micro')\n\n  # 2. Testing score\n  test_recall = f1_score(y_test, test_preds, average='micro')\n\n  # Display the result\n  result_arr = np.array([train_recall, test_recall])\n  result_df = pd.DataFrame(data = result_arr.reshape(1,2), columns = ['Train_F1', 'Test_F1'], index=[model_name])\n\n  return result_df","6058e1a2":"# 1. Baseline Model -> KNN\nknn_clf = KNeighborsClassifier()\nknn_clf.fit(X_train, y_train)\n\n# Compute Scores and plot confusion matrix\nmodel_dict={'KNN' : knn_clf}\nknn_results = compute_f1_score(model_dict=model_dict)","64d7a3e8":"# 2. Logistic Regression\nlr_clf = LogisticRegressionCV(solver='liblinear')\nlr_clf.fit(X_train, y_train)\n\n# Compute Scores and plot confusion matrix\nmodel_dict={'LogisticRegression' : lr_clf}\nlr_results = compute_f1_score(model_dict=model_dict)","0fed68c1":"# 3. Decision Tree\ndt_clf = DecisionTreeClassifier()\ndt_clf.fit(X_train, y_train)\n\n# Compute Scores and plot confusion matrix\nmodel_dict={'DecisionTree' : dt_clf}\ndt_results = compute_f1_score(model_dict=model_dict)","1e36c02f":"# 4. SVM\nsvm_clf = SVC()\nsvm_clf.fit(X_train, y_train)\n\n# Compute Scores and plot confusion matrix\nmodel_dict={'SVM' : svm_clf}\nsvm_results = compute_f1_score(model_dict=model_dict)","c6255941":"# 5. Random Forest\nrf_clf = RandomForestClassifier()\nrf_clf.fit(X_train, y_train)\n\n# Compute Scores and plot confusion matrix\nmodel_dict={'RandomForest' : rf_clf}\nrf_results = compute_f1_score(model_dict=model_dict)","7808035b":"# 6. Extra Trees\next_clf = ExtraTreesClassifier()\next_clf.fit(X_train, y_train)\n\n# Compute Scores and plot confusion matrix\nmodel_dict={'ExtraTrees' : ext_clf}\next_results = compute_f1_score(model_dict=model_dict)","28d7680a":"# 7. XGBoost\nxgb_clf = XGBClassifier()\nxgb_clf.fit(X_train, y_train)\n\n# Compute Scores and plot confusion matrix\nmodel_dict={'XGBoost' : xgb_clf}\nxgb_results = compute_f1_score(model_dict=model_dict)","1a2fc9ad":"# 8. LightGBM\nlgbm_clf = LGBMClassifier()\nlgbm_clf.fit(X_train, y_train)\n\n# Compute Scores and plot confusion matrix\nmodel_dict={'LightGBM' : lgbm_clf}\nlgbm_results = compute_f1_score(model_dict=model_dict)","b950690a":"# Concatenate the results\nfinal_results = pd.concat((knn_results, lr_results, \n                           svm_results, dt_results, \n                           rf_results, ext_results, \n                           xgb_results, lgbm_results), axis=0).sort_values(by='Test_F1', ascending=False)\nfinal_results","c20febc1":"# Make submissions using LightGBM and XGBoost\n\n# 1. LightGBM Submissions\nlgbm_predictions = lgbm_clf.predict(test_scaled_df)\nlgbm_predictions_labels = le.inverse_transform(lgbm_predictions)\nlgbm_predictions_arr = np.concatenate((test_IDs.values.reshape(-1,1), lgbm_predictions_labels.reshape(-1,1)), axis=1)\nlgbm_submission = pd.DataFrame(lgbm_predictions_arr, columns=['animal_id_outcome', 'outcome_type'])","2dca175a":"lgbm_submission.to_csv('.\/lgbm_submissions.csv', index=False)","a7eea1db":"# 2. XGBoost Submissions\nxgb_predictions = xgb_clf.predict(test_scaled_df)\nxgb_predictions_labels = le.inverse_transform(xgb_predictions)\nxgb_predictions_arr = np.concatenate((test_IDs.values.reshape(-1,1), xgb_predictions_labels.reshape(-1,1)), axis=1)\nxgb_submission = pd.DataFrame(xgb_predictions_arr, columns=['animal_id_outcome', 'outcome_type'])","7d69e8a1":"xgb_submission.to_csv('.\/xgb_submissions.csv', index=False)","078b0d50":"# Hyperparameter Tuning\n# 1. Light GBM\nlgbm_params = {\"num_leaves\" : [31, 50, 70, 90, 110],\n               \"max_depth\" : [10, 20, 30, 40, 50, 60],\n               \"learning_rate\" : [0.1, 0.5, 1, 1.5, 2.0],\n               \"n_estimators\" : [100, 150, 200, 250, 300, 350],\n               \"reg_alpha\" : [0.0, 0.25, 0.50, 0.75, 1.0, 2.0],\n               \"reg_lambda\" : [0.0, 0.25, 0.50, 0.75, 1.0, 2.0],\n               \"colsample_bytree\" : [0.0, 0.25, 0.50, 0.75, 1.0]\n               }\n\nlgbm_clf_2 = LGBMClassifier()\n\n# using randomised search cv\nrscv_lgbm_clf = RandomizedSearchCV(lgbm_clf_2, lgbm_params, n_iter=20, n_jobs=-1, cv=3, verbose=3, random_state=0)\nrscv_lgbm_clf.fit(X_train, y_train)","a72e31e5":"compute_f1_score(model_dict={'LGBM_Tuned' : rscv_lgbm_clf})","e4683450":"# 3. LightGBM Submissions\nlgbm_predictions = rscv_lgbm_clf.predict(test_scaled_df)\nlgbm_predictions_labels = le.inverse_transform(lgbm_predictions)\nlgbm_predictions_arr = np.concatenate((test_IDs.values.reshape(-1,1), lgbm_predictions_labels.reshape(-1,1)), axis=1)\nlgbm_submission = pd.DataFrame(lgbm_predictions_arr, columns=['animal_id_outcome', 'outcome_type'])\nlgbm_submission.to_csv('.\/lgbm_submissions_2.0.csv', index=False)","4ef3485a":"**Most animals are dogs, followed by cat, other and bird**","eff670c9":"**No more null values left.**","0d39b56d":"# Preprocessing steps\n\n*   Encode intake and outcome weekdays\n*   Convert intake_datetime and outcome_datetime to datetime\n\n","aacb2080":"**We have more number of animals that are born in recent year. A left skewed distribution is observed. While in case of Month of Birth a bi-modal distribution can be observed.**","714b9203":"# Data Analysis","4f60ad73":"## Null values treatment","83909cc8":"# Unzip the data","3ac1b502":"# Machine Learning Modelling","ae2612f9":"**From the above table and the plots, it can be inferred that either the animals are not kept in the center for more than a year or there is some error in the age_of_intake and age_of_outcome columns. Next, further analysis has been done to test the above hypothesis.**","adc24671":"**Negative number of Days of stay are found. There is probably some error in our outcome and intake dates because the time_in_shelter_days column doesn't show any negative value.**","eb7afb40":"**Null values in the ouctome datetime can be recreated using the columns : outcome_year, outcome_month, outcome_weekday, outcome_hour**","91bd053a":"Features like age_upon_intake, age_upon_outcome needs to converted into integer type\n\n","cd791cf3":"**~12% of data is incorrect or wrong.**","1c65f976":"\n\n1.   train data has 3 columns with null values : sex_upon_intake, sex_upon_outcome, outcome_datetime.\n2.   There are no null values in the test set.\n\n","159cb0d1":"**Use the following dropbox link for the dataset**"}}