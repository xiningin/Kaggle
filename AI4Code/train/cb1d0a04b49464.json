{"cell_type":{"567c9a12":"code","52c1a63c":"code","f2369c23":"code","d7359c38":"code","ce361ea0":"code","0fcba38d":"code","c15413c1":"code","63940e22":"code","5059d989":"code","709a960e":"code","7a7776dd":"code","001a324b":"code","f469b6fc":"code","c1ea3e19":"code","8ccdf19d":"code","f8437420":"code","8a4ae3b4":"code","85b447c3":"code","6d38c91c":"code","390e8cfb":"code","0051580c":"code","69750dd3":"code","57b442ad":"code","857231d4":"code","da7f5452":"code","baed137e":"code","39cb8c7a":"code","ecdfaa4d":"code","4d20fe8c":"code","2aa85549":"code","33de63ab":"code","085e63f5":"code","f50974b8":"code","3da4a969":"code","0cb32592":"code","2cc0cd4e":"code","ac731694":"code","f252324e":"code","b479c611":"code","7d016596":"markdown","53b10d87":"markdown","0cdf6ca6":"markdown","bf79dd0e":"markdown","22957e73":"markdown","332d85e4":"markdown","fc066c9c":"markdown","56499206":"markdown","b8588e95":"markdown","4577fe24":"markdown","c936631d":"markdown","7b3c641b":"markdown","d8ddfd5b":"markdown"},"source":{"567c9a12":"# make sure latest version of fastai is installed\n#!conda install -c pytorch -c fastai fastai --yes\n#!conda install -c anaconda nltk --yes","52c1a63c":"!pip install pyenchant\n# Enchant needs libenchant to be installed\n! apt-get update\n! apt-get install libenchant-dev -y","f2369c23":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport time\nfrom datetime import datetime\n\nfrom sklearn.model_selection import train_test_split\n\nfrom google.cloud import storage\nfrom google.cloud import automl_v1beta1 as automl\n# from google.cloud import automl\n\nfrom automlwrapper import AutoMLWrapper\n\nfrom fastai.text import *\nimport spacy\nimport pandas as pd\nimport numpy as np\nimport re\nimport nltk\nfrom nltk import word_tokenize\nfrom tqdm import tqdm\nimport enchant\nfrom nltk.metrics import edit_distance","d7359c38":"replacement_patterns = [\n    (r'won\\'t', 'will not'),\n    (r'can\\'t', 'cannot'),\n    (r'i\\'m', 'i am'),\n    (r'ain\\'t', 'is not'),\n    (r'(\\w+)\\'ll', '\\g<1> will'),\n    (r'(\\w+)n\\'t', '\\g<1> not'),\n    (r'(\\w+)\\'ve', '\\g<1> have'),\n    (r'(\\w+)\\'s', '\\g<1> is'),\n    (r'(\\w+)\\'re', '\\g<1> are'),\n    (r'(\\w+)\\'d', '\\g<1> would'),\n]\n\nclass RegexpReplacer(object):\n    # Replaces regular expression in a text.\n    def __init__(self, patterns=replacement_patterns):\n        self.patterns = [(re.compile(regex), repl) for (regex, repl) in patterns]\n    \n    def replace(self, text):\n        s = text\n        \n        for (pattern, repl) in self.patterns:\n            s = re.sub(pattern, repl, s)\n        \n        return s\n\nclass SpellingReplacer(object):\n    \"\"\" Replaces misspelled words with a likely suggestion based on shortest\n    edit distance\n    \"\"\"\n    def __init__(self, dict_name='en', max_dist=2):\n        self.spell_dict = enchant.Dict(dict_name)\n        self.max_dist = max_dist\n    \n    def replace(self, word):\n        if self.spell_dict.check(word):\n            return word\n        \n        suggestions = self.spell_dict.suggest(word)\n        \n        if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n            return suggestions[0]\n        else:\n            return word","ce361ea0":"input_path = '\/kaggle\/input\/'\noutput_path = '\/kaggle\/working\/'","0fcba38d":"# relabel incorrect tweets\nids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\ntrain_orig = pd.read_csv(input_path+'nlp-getting-started\/train.csv', encoding = 'latin-1')\nincorrect = train_orig[train_orig['id'].isin(ids_with_target_error)]","c15413c1":"df_train = pd.read_csv(input_path+'disasters-on-social-media\/socialmedia-disaster-tweets-DFE.csv', encoding = 'latin-1')\ndf_train = df_train[['_unit_id','keyword','location','text','choose_one']]\n#now grab ids that were identified earlier as mislabelled\nincorrect_ids = incorrect.merge(df_train,on=['text'])['_unit_id']\ndf_train.rename(columns = {'_unit_id': 'id'}, inplace = True)\ndf_train['target'] = df_train['choose_one'].map({'Relevant': 1, 'Not Relevant': 0})\ndf_train.drop(['choose_one'], axis = 1, inplace = True)\ndf_train = df_train[-df_train['target'].isna()]\ndf_train = df_train.astype({'target': 'int64'})\n#reassign incorrect labels\ndf_train.loc[df_train['id'].isin(incorrect_ids),'target'] = 0\ndf_train.head()","63940e22":"df_train['text'][1]","5059d989":"df_test = pd.read_csv(input_path+'nlp-getting-started\/test.csv')\ndf_test.head()","709a960e":"# Additional training data\ncols = ['target','id','date','flag','user','text']\ntrain_add = pd.read_csv(input_path+'disastertweetsinput\/train_clean_add.csv',names=cols,encoding = 'latin-1', skiprows=1)\ntrain_add.head()","7a7776dd":"train_add = df_train[['id','text','target']].append(train_add[['id','text','target']])","001a324b":"def clean_tweet(text) :\n    # remove urls\n    #text = df.apply(lambda x: re.sub(r'http\\S+', '', x))\n    text = re.sub(r'http\\S+', '', text)\n\n    # replace contractions\n    replacer = RegexpReplacer()\n    text = replacer.replace(text)\n\n    #split words on - and \\\n    text = re.sub(r'\\b', ' ', text)\n    text = re.sub(r'-', ' ', text)\n\n    # replace negations with antonyms\n\n    #nltk.download('punkt')\n    tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n    tokens = tokenizer.tokenize(text)\n\n    # spelling correction\n    replacer = SpellingReplacer()\n    tokens = [replacer.replace(t) for t in tokens]\n\n    # lemmatize\/stemming\n    wnl = nltk.WordNetLemmatizer()\n    tokens = [wnl.lemmatize(t) for t in tokens]\n    porter = nltk.PorterStemmer()\n    tokens = [porter.stem(t) for t in tokens]\n    # filter insignificant words (using fastai)\n    # swap word phrases\n\n    text = ' '.join(tokens)\n    return(text)","f469b6fc":"tweets = df_train['text']\ntqdm.pandas(desc=\"Cleaning tweets\")\ntweets_cleaned = tweets.progress_apply(clean_tweet)\ndf_train['text_clean'] = tweets_cleaned","c1ea3e19":"tweets = df_test['text']\ntweets_cleaned = tweets.progress_apply(clean_tweet)\ndf_test['text_clean'] = tweets_cleaned","8ccdf19d":"df = df_test[['text_clean']].append(df_train[['text_clean']])\ndf = df.drop_duplicates().reset_index()['text_clean']\ndf.head()","f8437420":"#append text_token to df_train and df_test\ntrain = []\ntokenizer = Tokenizer()\ntok = SpacyTokenizer('en')\nfor line in tqdm(df_train.text_clean):\n    lne = ' '.join(tokenizer.process_text(line, tok))\n    train.append(lne)\n\ndf_train['text_tokens'] = train\n    \ntest = []\ntokenizer = Tokenizer()\ntok = SpacyTokenizer('en')\nfor line in tqdm(df_test.text_clean):\n    lne = ' '.join(tokenizer.process_text(line, tok))\n    test.append(lne)\n    \ndf_test['text_tokens'] = test","8a4ae3b4":"# Set your own values for these. bucket_name should be the project_id + '-lcm'.\nPROJECT_ID = 'kaggle-tweets-0234'\nbucket_name = 'kaggle-tweets-0234-lcm'\n\nregion = 'us-central1' # Region must be us-central1\ndataset_display_name = 'disaster_tweets'\nmodel_display_name = 'disaster_tweets_model1'\nstorage_client = storage.Client(project=PROJECT_ID)\nclient = automl.AutoMlClient()","85b447c3":"# TODO(developer): Uncomment and set the following variables\n# project_id = 'YOUR_PROJECT_ID'\n\n# A resource that represents Google Cloud Platform location.\nproject_location = client.location_path(PROJECT_ID, 'us-central1')\nresponse = client.list_models(project_location, '')\n\nprint('List of models:')\nfor model in response:\n    # Display the model information.\n    if model.deployment_state == \\\n            automl.enums.Model.DeploymentState.DEPLOYED:\n        deployment_state = 'deployed'\n    else:\n        deployment_state = 'undeployed'\n\n    print(u'Model name: {}'.format(model.name))\n    print(u'Model id: {}'.format(model.name.split('\/')[-1]))\n    print(u'Model display name: {}'.format(model.display_name))\n    print(u'Model create time:')\n    print(u'\\tseconds: {}'.format(model.create_time.seconds))\n    print(u'\\tnanos: {}'.format(model.create_time.nanos))\n    print(u'Model deployment state: {}'.format(deployment_state))","6d38c91c":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","390e8cfb":"#nlp_train_df = pd.read_csv(\"\/kaggle\/input\/tokenized-disaster-tweets\/train_token.csv\")\nnlp_train_df =  df_train\n#nlp_test_df = pd.read_csv(\"\/kaggle\/input\/tokenized-disaster-tweets\/test_token.csv\")\nnlp_test_df =  df_test\ndef callback(operation_future):\n    result = operation_future.result()","0051580c":"nlp_train_df.tail()","69750dd3":"def upload_blob(bucket_name, source_file_name, destination_blob_name):\n    \"\"\"Uploads a file to the bucket. https:\/\/cloud.google.com\/storage\/docs\/ \"\"\"\n    bucket = storage_client.get_bucket(bucket_name)\n    blob = bucket.blob(destination_blob_name)\n    blob.upload_from_filename(source_file_name)\n    print('File {} uploaded to {}'.format(\n        source_file_name,\n        'gs:\/\/' + bucket_name + '\/' + destination_blob_name))\n    \ndef download_to_kaggle(bucket_name,destination_directory,file_name,prefix=None):\n    \"\"\"Takes the data from your GCS Bucket and puts it into the working directory of your Kaggle notebook\"\"\"\n    os.makedirs(destination_directory, exist_ok = True)\n    full_file_path = os.path.join(destination_directory, file_name)\n    blobs = storage_client.list_blobs(bucket_name,prefix=prefix)\n    for blob in blobs:\n        blob.download_to_filename(full_file_path)","57b442ad":"bucket = storage.Bucket(storage_client, name=bucket_name)\nif not bucket.exists():\n    bucket.create(location=region)","857231d4":"# Select the text body and the target value, for sending to AutoML NL\nnlp_train_df[['text_tokens','target']].to_csv('train.csv', index=False, header=False) ","da7f5452":"nlp_train_df[['id','text_tokens','target']].head()","baed137e":"training_gcs_path = 'uploads\/kaggle_getstarted\/full_train.csv'\n#upload_blob(bucket_name, 'train.csv', training_gcs_path)","39cb8c7a":"amw = AutoMLWrapper(client=client, \n                    project_id=PROJECT_ID, \n                    bucket_name=bucket_name, \n                    region='us-central1', \n                    dataset_display_name=dataset_display_name, \n                    model_display_name=model_display_name)     ","ecdfaa4d":"if not amw.get_dataset_by_display_name(dataset_display_name):\n    print('dataset not found')\n    amw.create_dataset()\n    amw.import_gcs_data(training_gcs_path)\n\namw.dataset","4d20fe8c":"if not amw.get_model_by_display_name():\n    amw.train_model()\n#amw.deploy_model()\namw.model","2aa85549":"amw.model_full_path","33de63ab":"nlp_test_df.head()","085e63f5":"# Create client for prediction service.\nprediction_client = automl.PredictionServiceClient()\namw.set_prediction_client(prediction_client)\n\npredictions_df = amw.get_predictions(nlp_test_df, \n                                     input_col_name='text_tokens', \n#                                    ground_truth_col_name='target', # we don't have ground truth in our test set\n                                     limit=None, \n                                     threshold=0.5,\n                                     verbose=False)\n","f50974b8":"#amw.undeploy_model()","3da4a969":"predictions_df.head()","0cb32592":"submission_df = pd.concat([nlp_test_df['id'], predictions_df['class']], axis=1)\nsubmission_df.head()","2cc0cd4e":"# predictions_df['class'].iloc[:10]\n# nlp_test_df['id']","ac731694":"submission_df = submission_df.rename(columns={'class':'target'})\nsubmission_df.head()","f252324e":"submission_df.to_csv(\"submission.csv\", index=False, header=True)","b479c611":"! ls -l submission.csv","7d016596":"## AutoML","53b10d87":"## Prediction\nNote that prediction will not run until deployment finishes, which takes a bit of time.\nHowever, once you have your model deployed, this notebook won't re-train the model, thanks to the various safeguards put in place. Instead, it will take the existing (trained) model and make predictions and generate the submission file.","0cdf6ca6":"## (optional) Undeploy model\nUndeploy the model to stop charges","bf79dd0e":"## Create our class instance","22957e73":"## Create submission output","332d85e4":"### Export to CSV and upload to GCS","fc066c9c":"## Kick off the training for the model\nAnd retrieve the training info after completion. \nStart model deployment.","56499206":"### GCS upload\/download utilities\nThese functions make upload and download of files from the kernel to Google Cloud Storage easier. This is needed for AutoML","b8588e95":"# Cleaning text data","4577fe24":"## Preparing the data","c936631d":"## Regex and Spelling functions\nSource: https:\/\/github.com\/japerk\/nltk3-cookbook","7b3c641b":"## Create (or retreive) dataset\nCheck to see if this dataset already exists. If not, create it","d8ddfd5b":"## Submit predictions to the competition!"}}