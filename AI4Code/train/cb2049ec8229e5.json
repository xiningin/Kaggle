{"cell_type":{"fdb0b30f":"code","ae162aea":"code","383baad2":"code","062fa4d6":"code","c61e83dc":"code","ee3e20df":"code","3cb7c77b":"code","e865b9bd":"code","6547383f":"code","04c6a35c":"code","09e630a6":"code","8b8274e7":"code","18747b14":"code","bde2c3b5":"code","784c1933":"code","70c4a8f5":"code","a97e4eec":"code","061b996c":"code","8227e32e":"code","b3d3115f":"code","34e5638c":"code","886d2616":"code","002d9d62":"code","60fdbb0a":"code","85c77209":"code","b40fcc20":"code","22a27fb5":"code","529577ec":"code","6a72c241":"code","bba64c92":"code","a4b6f4d8":"code","7c71d4e4":"code","4420de3c":"code","b5c1914e":"code","ad102da4":"markdown","aa2ac4cf":"markdown","d7f03eda":"markdown","a9ca01a0":"markdown","2656fd98":"markdown","214bdd01":"markdown","c869427c":"markdown","fbedac06":"markdown","ab130a95":"markdown","0a80f772":"markdown","3f094833":"markdown","a5f46c18":"markdown","987ff71b":"markdown","388d4fa1":"markdown","bf22585f":"markdown","e5e54c38":"markdown","c3f2ea89":"markdown","7253259f":"markdown","7a7e6202":"markdown","36cf0c4e":"markdown"},"source":{"fdb0b30f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ae162aea":"df=pd.read_csv(\"..\/input\/breast-cancer-prediction-dataset\/Breast_cancer_data.csv\")","383baad2":"df.shape","062fa4d6":"df.head()","c61e83dc":"df.dtypes","ee3e20df":"df.isnull().sum()","3cb7c77b":"def histogram(variable):\n    plt.figure(figsize=(8,4))\n    plt.hist(df[variable])\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} frequency with histogram\".format(variable))\n    plt.show()","e865b9bd":"df.columns","6547383f":"variables=['mean_radius', 'mean_texture', 'mean_perimeter', 'mean_area',\n       'mean_smoothness']\nfor i in variables:\n    histogram(i)\n","04c6a35c":"# mean_radius-diagnosis\ng= sns.FacetGrid(df,col=\"diagnosis\")\ng.map(sns.distplot,\"mean_radius\",bins=25)","09e630a6":"# mean_texture-diagnosis\ng= sns.FacetGrid(df,col=\"diagnosis\")\ng.map(sns.distplot,\"mean_texture\",bins=25)","8b8274e7":"# mean_perimeter-diagnosis\ng= sns.FacetGrid(df,col=\"diagnosis\")\ng.map(sns.distplot,\"mean_perimeter\",bins=25)","18747b14":"# mean_area-diagnosis\ng= sns.FacetGrid(df,col=\"diagnosis\")\ng.map(sns.distplot,\"mean_area\",bins=25)","bde2c3b5":"# mean_smoothness-diagnosis\ng= sns.FacetGrid(df,col=\"diagnosis\")\ng.map(sns.distplot,\"mean_smoothness\",bins=25)","784c1933":"def boxplot(variable):\n    plt.subplots()\n    plt.boxplot(df[variable])\n    plt.xlabel(variable)","70c4a8f5":"for i in variables:\n    boxplot(i)","a97e4eec":"df.drop(df[df[\"mean_radius\"]>25].index,axis=0,inplace=True)\ndf.drop(df[df[\"mean_texture\"]>35].index,axis=0,inplace=True)\ndf.drop(df[df[\"mean_perimeter\"]>180].index,axis=0,inplace=True)\ndf.drop(df[df[\"mean_area\"]>2000].index,axis=0,inplace=True)\ndf.drop(df[df[\"mean_smoothness\"]>0.15].index,axis=0,inplace=True)","061b996c":"df.shape","8227e32e":"def scatter(data,x,y):\n    sns.scatterplot(x=x, y=y, data=data, hue = 'diagnosis')","b3d3115f":"for i in variables:\n    for j in variables:\n        if i==j:\n            pass\n        else:\n            plt.subplots()\n            scatter(df,i,j)","34e5638c":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix","886d2616":"x=df.drop([\"diagnosis\"],axis=1)\ny=df[\"diagnosis\"]","002d9d62":"scaler=MinMaxScaler()\nx_scaled=scaler.fit_transform(x)","60fdbb0a":"random_state=42\nx_train,x_test,y_train,y_test = train_test_split(x_scaled,y,random_state=random_state)","85c77209":"Confusion_Matrices=[]\nClassifiers=[]\nScores=[]\n\ndtc=DecisionTreeClassifier(random_state = random_state)\ndtc.fit(x_train,y_train)\nScores.append(cross_val_score(dtc, x_test, y_test, cv=5).mean())\nConfusion_Matrices.append(confusion_matrix(y_test, dtc.predict(x_test)))\nClassifiers.append(\"Dtc\")","b40fcc20":"svc=SVC(random_state = random_state)\nsvc.fit(x_train,y_train)\nScores.append(cross_val_score(svc, x_test, y_test, cv=5).mean())\nConfusion_Matrices.append(confusion_matrix(y_test, svc.predict(x_test)))\nClassifiers.append(\"Svc\")","22a27fb5":"for i in [20,50,80,100]:\n    rf=RandomForestClassifier(n_estimators=i,random_state = random_state)\n    rf.fit(x_train,y_train)\n    Scores.append(cross_val_score(rf, x_test, y_test, cv=5).mean())\n    Confusion_Matrices.append(confusion_matrix(y_test, rf.predict(x_test)))\n    Classifiers.append(\"Rfc{}\".format(str(i)))","529577ec":"lr=LogisticRegression(random_state = random_state)\nlr.fit(x_train,y_train)\nScores.append(cross_val_score(lr, x_test, y_test, cv=5).mean())\nConfusion_Matrices.append(confusion_matrix(y_test, lr.predict(x_test)))\nClassifiers.append(\"Lr\")","6a72c241":"for i in [5,6,7,8,9,10]:\n    knn=KNeighborsClassifier(n_neighbors=i)\n    knn.fit(x_train,y_train)\n    Scores.append(cross_val_score(knn, x_test, y_test, cv=5).mean())\n    Confusion_Matrices.append(confusion_matrix(y_test, knn.predict(x_test)))\n    Classifiers.append(\"Knn{}\".format(str(i)))","bba64c92":"graph_data= pd.DataFrame(list(zip(Classifiers,Scores)),columns =['Classifiers', 'Scores']) \ngraph_data=graph_data.sort_values(\"Scores\",ascending=False)\nplt.figure(figsize=(16,8))\nsns.barplot(x=graph_data[\"Classifiers\"],y=graph_data[\"Scores\"])\n","a4b6f4d8":"for i in np.arange(0,13):\n    plt.subplots()\n    sns.heatmap(Confusion_Matrices[i],annot=True)\n    plt.title(\"Confusion Matrix of {}\".format(Classifiers[i]))","7c71d4e4":"Ensemble_Model = VotingClassifier(estimators=[('knn6', KNeighborsClassifier(n_neighbors=6)), ('knn5', KNeighborsClassifier(n_neighbors=5)), ('knn10', KNeighborsClassifier(n_neighbors=10)),(\"svc\",SVC(random_state = 42))], voting='hard')","4420de3c":"Ensemble_Model.fit(x_train,y_train)","b5c1914e":"print(cross_val_score(Ensemble_Model, x_test, y_test, cv=5).mean())","ad102da4":"There is no missing value so it's good","aa2ac4cf":"# Basic Data Analysis\nAll Variables - Diagnosis","d7f03eda":"# Variable Description\n\n1-mean_radius\n\n2-mean_texture\n\n3-mean_perimeter\n\n4-mean_area\n\n5-mean_smoothness","a9ca01a0":"# Missing Value","2656fd98":"This variable's behavior is similar to mean_radius, when diagnosis=1 mean_perimeter is less and stacked in a small space.","214bdd01":"# Ensemble Modeling\n","c869427c":"It may not be appropriate for us to discard all outliers so I will only discard the really extreme outliers.","fbedac06":"# Visualization","ab130a95":"It is similar to texture variable.For every case of diagnosis,distribution is similar but when diagnosis is 1 it is less than other case.","0a80f772":"All variables are numerical so we can only do numerical variable analysis ","3f094833":"We can see that when mean_radius is close to 12 it is likely to be breast cancer and after 20 there is almoast no chance to be breast cancer","a5f46c18":"Again it is similar to perimeter and radius but different from other we can see diagnosis=0 for every area","987ff71b":"7 outlier is dropped.","388d4fa1":"# Numerical Variable Analysis","bf22585f":"It seems that the svc,knn6, knn5 and knn10 models do not show any obvious errors so we can use them for ensemble modeling","e5e54c38":"We can say that when diagnosis is 1 mean_texture is likely to be closer to 18 while at diagnosis=0 mean is at near 21","c3f2ea89":"# Outlier Detection","7253259f":"# **Load and Check Data**","7a7e6202":"As we can see KNN and Svc have most accurate answers but we have to look Confusion Matrix to decide before going ensemble modeling.","36cf0c4e":"# Modeling"}}