{"cell_type":{"55f8c658":"code","f427acc9":"code","56bc365b":"code","721eceed":"code","cfdb1ec6":"code","d5d72a3d":"code","4327fea2":"code","3446cb03":"code","d87db58b":"code","d9f08dbb":"code","74566256":"code","9e9a9d57":"code","ce404506":"code","b31bf582":"code","c26dd8a4":"code","66c02a0b":"code","a0d6f915":"code","43e08b6f":"code","c252db9d":"code","fc95701a":"code","262566ad":"code","22eb81f2":"code","7e3f063b":"code","2670e2e4":"code","923b95d5":"code","b0af2b8a":"code","d33c744c":"code","50ce2a55":"code","51997e19":"code","37353306":"code","5c52eabe":"code","1f0d8957":"code","bfbb0c07":"code","4e61d4f5":"code","5fc2cd53":"code","127f4e2c":"code","dae91c00":"code","d651256a":"code","61e00f40":"code","b93bf459":"code","504aafdd":"code","58222f66":"markdown","8432a6d3":"markdown","95bf1a6a":"markdown","4b7b9fa5":"markdown","46be0634":"markdown","a5e4ac74":"markdown","8d3c805a":"markdown","b127f32c":"markdown","9eeb62b7":"markdown","8c100f52":"markdown","04cbe0ab":"markdown","b86047a4":"markdown","a62f73a0":"markdown","fd686298":"markdown","8d40085b":"markdown","1b402a06":"markdown","fc8055d8":"markdown","8364bec8":"markdown"},"source":{"55f8c658":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)","f427acc9":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=UserWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\nfrom IPython.display import HTML\n\npd.set_option('max_columns', 50)","56bc365b":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","721eceed":"from sklearn.metrics import roc_auc_score,precision_recall_curve,roc_curve\nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, confusion_matrix, auc\nfrom sklearn import model_selection","cfdb1ec6":"from sklearn.model_selection import GridSearchCV, StratifiedKFold, KFold\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport xgboost as xgb\nimport lightgbm as lgb\n","d5d72a3d":"def evalBinaryClassifier(model, x, y, labels=['Positives','Negatives']):\n    '''\n    source: https:\/\/towardsdatascience.com\/how-to-interpret-a-binary-logistic-regressor-with-scikit-learn-6d56c5783b49\n    Visualize the performance of  a Logistic Regression Binary Classifier.\n    \n    Displays a labelled Confusion Matrix, distributions of the predicted\n    probabilities for both classes, the ROC curve, and F1 score of a fitted\n    Binary Logistic Classifier. Author: gregcondit.com\/articles\/logr-charts\n    \n    Parameters\n    ----------\n    model : fitted scikit-learn model with predict_proba & predict methods\n        and classes_ attribute. Typically LogisticRegression or \n        LogisticRegressionCV\n    \n    x : {array-like, sparse matrix}, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples\n        in the data to be tested, and n_features is the number of features\n    \n    y : array-like, shape (n_samples,)\n        Target vector relative to x.\n    \n    labels: list, optional\n        list of text labels for the two classes, with the positive label first\n        \n    Displays\n    ----------\n    3 Subplots\n    \n    Returns\n    ----------\n    F1: float\n    '''\n    #model predicts probabilities of positive class\n    p = model.predict_proba(x)\n    if len(model.classes_)!=2:\n        raise ValueError('A binary class problem is required')\n    if model.classes_[1] == 1:\n        pos_p = p[:,1]\n    elif model.classes_[0] == 1:\n        pos_p = p[:,0]\n    \n    #FIGURE\n    plt.figure(figsize=[15,4])\n    \n    #1 -- Confusion matrix\n    cm = confusion_matrix(y,model.predict(x))\n    plt.subplot(131)\n    ax = sns.heatmap(cm, annot=True, cmap='Blues', cbar=False, \n                annot_kws={\"size\": 14}, fmt='g')\n    cmlabels = ['True Negatives', 'False Positives',\n              'False Negatives', 'True Positives']\n    for i,t in enumerate(ax.texts):\n        t.set_text(t.get_text() + \"\\n\" + cmlabels[i])\n    plt.title('Confusion Matrix', size=15)\n    plt.xlabel('Predicted Values', size=13)\n    plt.ylabel('True Values', size=13)\n      \n    #2 -- Distributions of Predicted Probabilities of both classes\n    df = pd.DataFrame({'probPos':pos_p, 'target': y})\n    plt.subplot(132)\n    plt.hist(df[df.target==1].probPos, density=True, bins=25,\n             alpha=.5, color='green',  label=labels[0])\n    plt.hist(df[df.target==0].probPos, density=True, bins=25,\n             alpha=.5, color='red', label=labels[1])\n    plt.axvline(.5, color='blue', linestyle='--', label='Boundary')\n    plt.xlim([0,1])\n    plt.title('Distributions of Predictions', size=15)\n    plt.xlabel('Positive Probability (predicted)', size=13)\n    plt.ylabel('Samples (normalized scale)', size=13)\n    plt.legend(loc=\"upper right\")\n    \n    #3 -- ROC curve with annotated decision point\n    fp_rates, tp_rates, _ = roc_curve(y,p[:,1])\n    roc_auc = auc(fp_rates, tp_rates)\n    plt.subplot(133)\n    plt.plot(fp_rates, tp_rates, color='green',\n             lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n    plt.plot([0, 1], [0, 1], lw=1, linestyle='--', color='grey')\n    #plot current decision point:\n    tn, fp, fn, tp = [i for i in cm.ravel()]\n    plt.plot(fp\/(fp+tn), tp\/(tp+fn), 'bo', markersize=8, label='Decision Point')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate', size=13)\n    plt.ylabel('True Positive Rate', size=13)\n    plt.title('ROC Curve', size=15)\n    plt.legend(loc=\"lower right\")\n    plt.subplots_adjust(wspace=.3)\n    plt.show()\n    #Print and Return the F1 score\n    tn, fp, fn, tp = [i for i in cm.ravel()]\n    precision = tp \/ (tp + fp)\n    recall = tp \/ (tp + fn)\n    F1 = 2*(precision * recall) \/ (precision + recall)\n    printout = (\n        f'Precision: {round(precision,2)} | '\n        f'Recall: {round(recall,2)} | '\n        f'F1 Score: {round(F1,2)} | '\n    )\n    print(printout)\n    return F1","4327fea2":"%%time\nsampleEntry = pd.read_csv('\/kaggle\/input\/GiveMeSomeCredit\/sampleEntry.csv')\ntrain = pd.read_csv('\/kaggle\/input\/GiveMeSomeCredit\/cs-training.csv')\ntest = pd.read_csv('\/kaggle\/input\/GiveMeSomeCredit\/cs-test.csv')","3446cb03":"print('train shape  ',train.shape)","d87db58b":"train.dtypes.value_counts()","d9f08dbb":"train.info()","74566256":"train = train.dropna()\ntrain = train.drop(columns = ['Unnamed: 0'])\ntrain.head()","9e9a9d57":"features = train.columns.values[0:30]\nunique_max_train = []\nunique_max_test = []\nfor feature in features:\n    values = train[feature].value_counts()\n    unique_max_train.append([feature, values.max(), values.idxmax()])\n\nnp.transpose((pd.DataFrame(unique_max_train, columns=['Feature', 'Max duplicates', 'Value'])).\\\n            sort_values(by = 'Max duplicates', ascending=False).head(15))","ce404506":"train.SeriousDlqin2yrs.value_counts()","b31bf582":"train_X = train.drop([\"SeriousDlqin2yrs\"], axis=1)\ntrain_y = np.log1p(train[\"SeriousDlqin2yrs\"].values)","c26dd8a4":"from sklearn import ensemble\nmodel = ensemble.ExtraTreesRegressor(n_estimators=200, max_depth=20, max_features=0.5, n_jobs=-1, random_state=0)\nmodel.fit(train_X, train_y)\n\n## plotando as import\u00e2ncias ##\nfeat_names = train_X.columns.values\nimportances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\nindices = np.argsort(importances)[::-1][:20]\n\nplt.figure(figsize=(12,12))\nplt.title(\"Feature importances\")\nplt.bar(range(len(indices)), importances[indices], color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical')\nplt.xlim([-1, len(indices)])\nplt.show()","66c02a0b":"def plot_new_feature_distribution(df1, df2, label1, label2, features, n):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(2,n,figsize=(18,8))\n\n    for feature in features:\n        i += 1\n        plt.subplot(2,n,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=11)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","a0d6f915":"def plot_roc(y_test,prob):\n    fpr, tpr, thresholds = roc_curve(y_test, probs)\n    # plot no skill\n    plt.plot([0, 1], [0, 1], linestyle='--')\n    # plot the roc curve for the model\n    plt.plot(fpr, tpr, marker='.')\n    plt.title(\"ROC curve\")\n    plt.xlabel('false positive rate')\n    plt.ylabel('true positive rate')\n    # show the plot\n    plt.show()","43e08b6f":"def logistic(X,y):\n    y_train=train['SeriousDlqin2yrs'].astype('uint8')\n    X_train,X_test,y_train,y_test=train_test_split(train.drop('SeriousDlqin2yrs',axis=1),y_train,test_size=.2,random_state=2020)\n    lr=LogisticRegression()\n    lr.fit(X_train,y_train)\n    prob=lr.predict_proba(X_test)\n    \n    roc=roc_auc_score(y_test, lr.predict_proba(X_test)[:,1])\n    print('roc ',roc)\n\n    return (prob[:,1],y_test)\ny_train=train['SeriousDlqin2yrs'].astype('uint8')\nprobs,y_test=logistic(train.drop('SeriousDlqin2yrs',axis=1),y_train)","c252db9d":"plot_roc(y_test,probs)","fc95701a":"\ntrain_x, val_x, train_y, val_y=train_test_split(train.drop('SeriousDlqin2yrs',axis=1),y_train,test_size=.2,random_state=2020)\n\nclf = LogisticRegression(solver=\"lbfgs\").fit(train_x, train_y)\npred_y = clf.predict(val_x)","262566ad":"rocrl=roc_auc_score(val_y, pred_y)\nrocrl","22eb81f2":"plot_roc(val_y, pred_y)","7e3f063b":"%%time\nmodel = RandomForestClassifier(n_estimators=220).fit(train_x,train_y)\npredictionforest = model.predict(val_x)","2670e2e4":"%%time\nrocrf=roc_auc_score(val_y, predictionforest)\nprint('roc ',rocrf)","923b95d5":"plot_roc(val_y, predictionforest)","b0af2b8a":"xgb_cfl = xgb.XGBClassifier(n_jobs = -1, \n                            n_estimators = 220)\n\nxgb_cfl.fit(train_x, train_y)\ny_scorexgb = xgb_cfl.predict_proba(val_x)[:,1]","d33c744c":"rocxgb=roc_auc_score(val_y, y_scorexgb)\nprint('roc ',rocxgb)","50ce2a55":"plot_roc(val_y, y_scorexgb)","51997e19":"xgb_cfl = xgb.XGBClassifier(n_jobs = -1)\nxgb_cfl.fit(train_x, train_y)\ny_pred = xgb_cfl.predict(val_x)\ny_score = xgb_cfl.predict_proba(val_x)[:,1]","37353306":"%%time\nparam_grid = {\n            'n_estimators': [50, 100, 200]\n              }\n\nCV_xgb_cfl = GridSearchCV(estimator = xgb_cfl, param_grid = param_grid, scoring ='roc_auc', verbose = 2)\nCV_xgb_cfl.fit(train_x, train_y)\n\nbest_parameters = CV_xgb_cfl.best_params_\nprint(\"The best parameters: \", best_parameters)","5c52eabe":"xgb_cfl2 = xgb.XGBClassifier(n_jobs = -1, \n                            n_estimators = 120)\n\nxgb_cfl2.fit(train_x, train_y)\ny_score2 = xgb_cfl2.predict_proba(val_x)[:,1]","1f0d8957":"rocxgb=roc_auc_score(val_y, y_score2)\nprint('roc ',rocxgb)","bfbb0c07":"plot_roc(val_y, y_score2)","4e61d4f5":"F1 = evalBinaryClassifier(xgb_cfl2, val_x, val_y)","5fc2cd53":"## Target 1\ntarget1 = pd.DataFrame({\"Target\": val_y, 'Probability': y_score2 })\nprint(target1.loc[target1.Target == 1, 'Probability'].describe())\ntarget1.loc[target1.Target == 1, 'Probability'].hist()\n","127f4e2c":"test=test.drop([\"SeriousDlqin2yrs\",\"Unnamed: 0\"],1)","dae91c00":"res = xgb_cfl2.predict_proba(test)[:,1]\nsampleEntry[\"Probability\"]=res\nsampleEntry.head()","d651256a":"sampleEntry.to_csv(\"submission.csv\",index=False)","61e00f40":"from platform import python_version\nprint(python_version())","b93bf459":"test.describe()","504aafdd":"# save model to file\nxgb_cfl2.save_model(\"model.bst\")\n","58222f66":"Probability Analysis","8432a6d3":"# Submission","95bf1a6a":"Variable Name\tDescription\tType\n- ``SeriousDlqin2yrs``\tPerson experienced 90 days past due delinquency or worse\tY\/N\n- ``RevolvingUtilizationOfUnsecuredLines``\tTotal balance on credit cards and personal lines of credit except real estate and no installment debt like car loans divided by the sum of credit limits\tpercentage\n- ``age``\tAge of borrower in years\tinteger\n- ``NumberOfTime3059DaysPastDueNotWorse``\tNumber of times borrower has been 30-59 days past due but no worse in the last 2 years.\tinteger\n- ``DebtRatio``\tMonthly debt payments, alimony,living costs divided by monthy gross income\tpercentage\n- ``MonthlyIncome``\tMonthly income\treal\n- ``NumberOfOpenCreditLinesAndLoans``\tNumber of Open loans (installment like car loan or mortgage) and Lines of credit (e.g. credit cards)\tinteger\n- ``NumberOfTimes90DaysLate``\tNumber of times borrower has been 90 days or more past due.\tinteger\n- ``NumberRealEstateLoansOrLines``\tNumber of mortgage and real estate loans including home equity lines of credit\tinteger\n- ``NumberOfTime60-89DaysPastDueNotWorse``\tNumber of times borrower has been 60-89 days past due but no worse in the last 2 years.\tinteger\n- ``NumberOfDependents``\tNumber of dependents in family excluding themselves (spouse, children etc.)\tinteger","4b7b9fa5":"# Imports","46be0634":"#### Score Analysis","a5e4ac74":"# Give Me Some Credit\n![](https:\/\/www.freshfacs.com\/v\/vspfiles\/photos\/D3-2.jpg)\nImprove on the state of the art in credit scoring by predicting the probability that somebody will experience financial distress in the next two years.\n","8d3c805a":"# Final","b127f32c":"# Grid Search CV ","9eeb62b7":"## Deploy model using mia marketplace\n![](https:\/\/miro.medium.com\/max\/1773\/1*QuP5nFnvBNg-IrHvQ2-hCA.png)\nmia is a platform for building and sharing machine learning apps\n","8c100f52":"### Feature Importance","04cbe0ab":"XGBoost","b86047a4":"###### XGB Classifier","a62f73a0":"##### Around 6% of samples defaulted\n- MonthlyIncome and NumberOfDependents have 29731 (19.82%) and 3924 (2.61%) null values respectively\n- We also notice that when NumberOfTimes90DaysLate has values above 17, there are 267 instances where the three columns \n- NumberOfTimes90DaysLate, NumberOfTime60-89DaysPastDueNotWorse, NumberOfTime30-59DaysPastDueNotWorse share the same values, specifically 96 and 98.\n    - We can see that sharing the same values of 96 and 98 respectively is not logical since trivial calculations can reveal that being 30 days past due for 96 times for a single person within a timespan of 2 years is not possible.\n- RevolvingUtilizationOfUnsecuredLines\n    - Defined as ratio of the total amount of money owed to total credit limit\ndistribution of values is right-skewed, consider removing outliers\n    - It is expected that as this value increases, the proportion of people defaulting should increase as well\n    - However, we can see that as the minimum value of this column is set to 13, the proportion of defaulters is smaller than that belonging to the pool of clients with total amount of money owed not exceeding total credit limit.\n    - Thus we should remove those samples with RevolvingUtilizationOfUnsecuredLines's value more than equal to 13\n- age\n    - There seems to be more younger people defaulting and the distribution seems fine on the whole\n- NumberOfTimes90DaysLate\n    - It is interesting to note that there are no one who is 90 or more days past due between 17 and 96 times.\n- NumberOfTime60-89DaysPastDueNotWorse\n    - It is interesting to note that there are no one who is 60-89 days past due between 11 and 96 times.\n- NumberOfTime30-59DaysPastDueNotWorse\n    - It is interesting to note that there are no one who is 30-59 days past due between 13 and 96 times.\n- DebtRatio\n    - 2.5% of clients owe around 3490 or more times what they own\n    - For the people who have monthly income in this 2.5%, only 185 people have values for their monthly incomes and the values are either 0 or 1.\n    - There are 164 out of these 185 people who are of two different types, first with no monthly income and does not default and second with monthly income and does default.\n- MonthlyIncome\n    - Distribution of values is skewed, we can consider imputation with median.\n    - We can also consider imputing with normally distributed values with its mean and standard deviation.\n- Numberof Dependents\n    - We can consider imputing with its mode, which is zero.","fd686298":"Mia Platform link, **Risk Models**: https:\/\/miamarketplace.com\/pages\/riskmodels","8d40085b":"### Read Data","1b402a06":"### Value Counts Target - SeriousDlqin2yrs","fc8055d8":"# Baseline","8364bec8":"## Data Duplicate Analysis"}}