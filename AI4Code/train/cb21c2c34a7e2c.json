{"cell_type":{"bffe31fd":"code","3bba37c9":"code","a3bf92dc":"code","25b14b88":"code","e7654e7c":"code","b00087aa":"code","05ae4c96":"code","eb3754ce":"code","3cfcb2b3":"code","8aab5d05":"code","b77a0baa":"code","8a3a8233":"code","a8318428":"code","8e20ca14":"code","aee23805":"code","deb9ba14":"code","136fbb43":"code","84471fe9":"code","94b79060":"code","c4dcde40":"code","45e3961c":"code","56b58a93":"code","9a482f8d":"code","4577f4b6":"code","42f881ef":"code","c72ec0be":"code","e8841f9a":"code","590ef86e":"code","7472dcd6":"code","1a769ad1":"markdown"},"source":{"bffe31fd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3bba37c9":"credit = pd.read_csv('\/kaggle\/input\/creditcarddefault\/credit-card-default.csv')\ncredit.head()","a3bf92dc":"credit.info()","25b14b88":"X = credit.drop('defaulted', axis=1)\nY = credit['defaulted']\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.3, random_state=101)","e7654e7c":"model = RandomForestClassifier()\nmodel.fit(X_train, Y_train)","b00087aa":"predicted = model.predict(X_test)","05ae4c96":"print('Accuracy', round(accuracy_score(predicted, Y_test)*100))","eb3754ce":"n_folds = 5\nparameters = {'max_depth':range(2,20,5)}","3cfcb2b3":"rfModel = RandomForestClassifier()\nrfModel = GridSearchCV(rfModel, parameters, cv=n_folds, scoring='accuracy')","8aab5d05":"rfModel.fit(X_train, Y_train)","b77a0baa":"scores = rfModel.cv_results_\npd.DataFrame(scores).head()","8a3a8233":"# GridSearchCV to find optimal n_estimators\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'n_estimators': range(100, 1500, 400)}\n\n# instantiate the model (note we are specifying a max_depth)\nrf = RandomForestClassifier(max_depth=4)\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_train, Y_train)","a8318428":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","8e20ca14":"# GridSearchCV to find optimal max_features\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'max_features': [4, 8, 14, 20, 24]}\n\n# instantiate the model\nrf = RandomForestClassifier(max_depth=4)\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_train, Y_train)","aee23805":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","deb9ba14":"# GridSearchCV to find optimal min_samples_leaf\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_leaf': range(100, 400, 50)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_train, Y_train)","136fbb43":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","84471fe9":"# GridSearchCV to find optimal min_samples_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n\n# specify number of folds for k-fold CV\nn_folds = 5\n\n# parameters to build the model on\nparameters = {'min_samples_split': range(200, 500, 50)}\n\n# instantiate the model\nrf = RandomForestClassifier()\n\n\n# fit tree on training data\nrf = GridSearchCV(rf, parameters, \n                    cv=n_folds, \n                   scoring=\"accuracy\")\nrf.fit(X_train, Y_train)","94b79060":"# scores of GridSearch CV\nscores = rf.cv_results_\npd.DataFrame(scores).head()","c4dcde40":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [4,8,10],\n    'min_samples_leaf': range(100, 400, 200),\n    'min_samples_split': range(200, 500, 200),\n    'n_estimators': [100,200, 300], \n    'max_features': [5, 10]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = -1,verbose = 1)","45e3961c":"# Fit the grid search to the data\ngrid_search.fit(X_train, Y_train)","56b58a93":"# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","9a482f8d":"# model with the best hyperparameters\nfrom sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(bootstrap=True,\n                             max_depth=10,\n                             min_samples_leaf=100, \n                             min_samples_split=200,\n                             max_features=10,\n                             n_estimators=100)","4577f4b6":"# fit\nrfc.fit(X_train,Y_train)","42f881ef":"# predict\npredictions = rfc.predict(X_test)","c72ec0be":"# evaluation metrics\nfrom sklearn.metrics import classification_report,confusion_matrix","e8841f9a":"print(classification_report(Y_test,predictions))","590ef86e":"print(confusion_matrix(Y_test,predictions))","7472dcd6":"((6752+689)\/(6752+306+1253+689))","1a769ad1":"# ** Final Model Accuracy : 82% **"}}