{"cell_type":{"17031ed2":"code","7f859647":"code","a4335518":"code","ff72daa7":"code","16636127":"code","9bfc563a":"code","aa964b5e":"code","1736c4b8":"code","c7d4db4d":"code","d65b3f86":"code","e04798d4":"code","0458c202":"code","f201ecf9":"code","7a1f7e45":"code","0f7a1a42":"code","32b216b6":"code","b2756d5d":"code","b94f034b":"code","a5e2438e":"code","874a0119":"code","4b16fa91":"code","0382c184":"code","d4c60ff2":"code","a7846990":"code","0c5b59cc":"code","92d34491":"code","9d8f1b06":"code","75000812":"code","a5e0b9c1":"code","8d523e9c":"code","4a6f0840":"code","d0d125ee":"code","fab0bdce":"code","edfa292c":"code","b4c82658":"code","59eea841":"code","f589d184":"code","9bf150bd":"code","52d973eb":"code","3f0b283e":"code","7e21dea4":"code","a39dedb2":"code","fdd2be3f":"code","61d565ff":"code","efd3b204":"code","c5993c29":"code","6af6cf47":"code","fabcca6a":"code","49d2c3f0":"code","16ea43d8":"code","9e189a34":"code","8267b1c1":"code","d64f3923":"code","b36b8160":"code","da19adbf":"code","99610d3e":"code","6a6de5c1":"markdown","b346c980":"markdown","61249c60":"markdown","e2f957dd":"markdown"},"source":{"17031ed2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7f859647":"import matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\nimport seaborn as sns\nplt.style.use(\"fivethirtyeight\")\n%matplotlib inline\nplt.rcParams['figure.figsize']=10,6\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport textwrap","a4335518":"from colorama import Fore, Back, Style\ny_ = Fore.YELLOW\nr_ = Fore.RED\ng_ = Fore.GREEN\nb_ = Fore.BLUE\nm_ = Fore.MAGENTA\nsr_ = Style.RESET_ALL","ff72daa7":"train_df = pd.read_csv(\"..\/input\/lish-moa\/train_features.csv\")\ntest_df = pd.read_csv(\"..\/input\/lish-moa\/test_features.csv\")\n\ntrain_scored = pd.read_csv(\"..\/input\/lish-moa\/train_targets_scored.csv\")\ntrain_nonscored = pd.read_csv(\"..\/input\/lish-moa\/train_targets_nonscored.csv\")","16636127":"print(\"Shape of Training Data...\",train_df.shape)\nprint(\"Shape of Testing Data...\",test_df.shape)\nprint(\"Shape of Trainscored Data...\",train_scored.shape)\nprint(\"Shape of Trainnonscored Data...\",train_nonscored.shape)","9bfc563a":"#######################################################################\n## Helper Function##\n#######################################################################\n\n\n    \ndef plott(f1):\n    plt.style.use('seaborn')\n    sns.set_style('whitegrid')\n    fig = plt.figure(figsize=(15,5))\n    #1 rows 2 cols\n    #first row, first col\n    ax1 = plt.subplot2grid((1,3),(0,0))\n    plt.hist(cp24[f1], bins=3, color='deepskyblue',alpha=0.5)\n    plt.title(f'Treatment duration 24h: {f1}',weight='bold', fontsize=14)\n    #first row sec col\n    ax1 = plt.subplot2grid((1,3),(0,1))\n    plt.hist(cp48[f1], bins=3, color='lightgreen',alpha=0.5)\n    plt.title(f'Treatment duration 48h: {f1}',weight='bold', fontsize=14)\n    #first row 3rd column\n    ax1 = plt.subplot2grid((1,3),(0,2))\n    plt.hist(cp72[f1], bins=3, color='gold',alpha=0.5)\n    plt.title(f'Treatment duration 72h: {f1}',weight='bold', fontsize=14)\n    plt.show()\n\ndef plotf(f1, f2, f3, f4):\n    plt.style.use('seaborn')\n    sns.set_style('whitegrid')\n\n    fig= plt.figure(figsize=(15,10))\n    #2 rows 2 cols\n    #first row, first col\n    ax1 = plt.subplot2grid((2,2),(0,0))\n    sns.distplot(train_df[f1], color='crimson')\n    plt.title(f1,weight='bold', fontsize=18)\n    plt.yticks(weight='bold')\n    plt.xticks(weight='bold')\n    #first row sec col\n    ax1 = plt.subplot2grid((2,2), (0, 1))\n    sns.distplot(train_df[f2], color='gainsboro')\n    plt.title(f2,weight='bold', fontsize=18)\n    plt.yticks(weight='bold')\n    plt.xticks(weight='bold')\n    #Second row first column\n    ax1 = plt.subplot2grid((2,2), (1, 0))\n    sns.distplot(train_df[f3], color='deepskyblue')\n    plt.title(f3,weight='bold', fontsize=18)\n    plt.yticks(weight='bold')\n    plt.xticks(weight='bold')\n    #second row second column\n    ax1 = plt.subplot2grid((2,2), (1, 1))\n    sns.distplot(train_df[f4], color='black')\n    plt.title(f4,weight='bold', fontsize=18)\n    plt.yticks(weight='bold')\n    plt.xticks(weight='bold')\n\n    return plt.show()\n\ndef ploth(data, w=15, h=9):\n    plt.figure(figsize=(w,h))\n    sns.heatmap(data.corr(), cmap='hot')\n    plt.title('Correlation between targets', fontsize=18, weight='bold')\n    return plt.show()\n\n# corrs function: Show dataframe of high correlation between features\ndef corrs(data, col1='Gene 1', col2='Gene 2',rows=5,thresh=0.8, pos=[1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53]):\n        #Correlation between genes\n        corre= data.corr()\n         #Unstack the dataframe\n        s = corre.unstack()\n        so = s.sort_values(kind=\"quicksort\", ascending=False)\n        #Create new dataframe\n        so2= pd.DataFrame(so).reset_index()\n        so2= so2.rename(columns={0: 'correlation', 'level_0':col1, 'level_1': col2})\n        #Filter out the coef 1 correlation between the same drugs\n        so2= so2[so2['correlation'] != 1]\n        #Drop pair duplicates\n        so2= so2.reset_index()\n        pos = pos\n        so3= so2.drop(so2.index[pos])\n        so3= so2.drop('index', axis=1)\n        #Show the first 10 high correlations\n        cm = sns.light_palette(\"Red\", as_cmap=True)\n        s = so3.head(rows).style.background_gradient(cmap=cm)\n        print(f\"{len(so2[so2['correlation']>thresh])\/2} {col1} pairs have +{thresh} correlation.\")\n        return s","aa964b5e":"train_df.head()","1736c4b8":"sns.countplot(x=train_df['cp_type'],data=train_df)\nplt.xlabel(\"cp_type\",weight='bold',fontsize = 14)\nplt.ylabel(\"Count\",weight='bold',fontsize = 14)\nplt.title(\"Count of cp_type\",weight='bold',fontsize = 18)\nplt.show()","c7d4db4d":"def bar_chart(df,parameter, figsize=(10,6)):\n    target_counts = df[parameter].value_counts()\n    target_perc = target_counts.div(target_counts.sum(), axis=0)\n    plt.figure(figsize=figsize)\n    ax = sns.barplot(x=target_counts.index.values, y=target_counts.values, order=target_counts.index)\n#     plt.xticks(rotation=90)\n    plt.xlabel(f'{parameter}', weight ='bold',fontsize=16)\n    plt.ylabel('# of occurances', weight ='bold',fontsize=16)\n    plt.title(\"Count of \"+f'{parameter}', weight ='bold',fontsize=20)\n\n    rects = ax.patches\n    labels = np.round(target_perc.values*100, 2)\n    for rect, label in zip(rects, labels):\n        height = rect.get_height()\n        ax.text(rect.get_x() + rect.get_width()\/2, height + 5, f'{label}%', ha='center', va='bottom')\n    \n    try:\n        labels =target_counts.index.tolist()\n\n    #     labels.sort()\n        labels=[textwrap.fill(text,12) for text in labels]\n        pos = np.arange(len(labels)) \n        plt.xticks(pos, labels,fontsize=12)\n    except:\n        pass\n","d65b3f86":"bar_chart(train_df,\"cp_time\")","e04798d4":"bar_chart(train_df,\"cp_dose\")","0458c202":"def distribution1(feature, color):\n    plt.figure(figsize=(15,7))\n    plt.subplot(121)\n    sns.distplot(train_df[feature],color=color)\n    plt.subplot(122)\n    sns.violinplot(train_df[feature])\n    print(\"{}Max value of {} is: {} {:.2f} \\n{}Min value of {} is: {} {:.2f}\\n{}Mean of {} is: {}{:.2f}\\n{}Standard Deviation of {} is:{}{:.2f}\"\\\n      .format(y_,feature,r_,train_df[feature].max(),g_,feature,r_,train_df[feature].min(),b_,feature,r_,train_df[feature].mean(),m_,feature,r_,train_df[feature].std()))","f201ecf9":"distribution1(\"g-1\",\"blue\")","7a1f7e45":"train_df['g_mean'] = train_df[[x for x in train_df.columns if x.startswith(\"g-\")]].mean(axis=1)\ntest_df['g_mean'] = test_df[[x for x in test_df.columns if x.startswith(\"g-\")]].mean(axis=1)\n\ndistribution1(\"g_mean\",\"yellow\")","0f7a1a42":"#Distribution of single cell viability\ndistribution1(\"c-0\",\"green\")","32b216b6":"#Distribution of mean of cell viability\u00b6\ntrain_df['c_mean'] = train_df[[x for x in train_df.columns if x.startswith(\"c-\")]].mean(axis=1)\ntest_df['c_mean'] = test_df[[x for x in test_df.columns if x.startswith(\"c-\")]].mean(axis=1)\n\ndistribution1('c_mean','orange')","b2756d5d":"#Distribution of g_mean based on cp_type,cp_time, cp_dose\ndef distribution2(feature):\n    plt.figure(figsize=(15,14))\n    plt.subplot(231)\n    for i in train_df.cp_type.unique():\n        sns.distplot(train_df[train_df['cp_type']==i][feature],label=i)\n    plt.title(f\"{feature} based on cp_type\")\n    plt.legend()\n\n    plt.subplot(232)\n    for i in train_df.cp_time.unique():\n        sns.distplot(train_df[train_df['cp_time']==i][feature],label=i)\n    plt.title(f\" {feature}  based on cp_time\")\n    plt.legend()\n    \n    plt.subplot(233)\n    for i in train_df.cp_dose.unique():\n        sns.distplot(train_df[train_df['cp_dose']==i][feature],label=i)\n    plt.title(f\" {feature} based on cp_dose \")\n    \n    plt.subplot(234)\n    sns.violinplot(data=train_df,y=feature,x='cp_type')\n    plt.title(f\"{feature} based on cp_type\")\n    plt.legend()\n\n    plt.subplot(235)\n    sns.violinplot(data=train_df,y=feature,x='cp_time')\n    plt.title(f\" {feature}  based on cp_time\")\n    plt.legend()\n    \n    plt.subplot(236)\n    sns.violinplot(data=train_df,y=feature,x='cp_dose')\n    plt.title(f\" {feature} based on cp_dose \")\n    plt.legend()","b94f034b":"distribution2('g_mean')","a5e2438e":"#Distribution of c_mean based on cp_type,cp_time, cp_dose\ndistribution2('c_mean')","874a0119":"#Gene exp of 4 random samples\ng_cols = [x for x in train_df.columns if x.startswith(\"g-\")]\nc_cols = [x for x in train_df.columns if x.startswith(\"c-\")]\ndef plot1(features):\n    rnd = np.random.randint(0,train_df.shape[0]-16)\n    plt.figure(figsize=(10,7))\n    \n    for i in range(4):\n        data = train_df.loc[rnd+i,features]\n        mean = np.mean(data.values)\n        plt.subplot(2,2,i+1)\n        sns.scatterplot(data=data.values,marker=\">\") \n        plt.tick_params(\n        axis='x',          \n        which='both',      \n        bottom=False,    \n        top=False,        \n        labelbottom=False)\n        sns.lineplot(x=list(range(len(data))), y = [mean]*len(data),color='r',linewidth=2)\n        \n    plt.show()\n","4b16fa91":"plot1(g_cols)","0382c184":"#cell validity of 4 random sample\nplot1(c_cols)","d4c60ff2":"#Heat map of n random gene exp columns\u00b6\ndef heat(n):\n    plt.figure(figsize=(13,13))\n    rnd = np.random.randint(0,len(g_cols)-n)\n    data = train_df[g_cols]\n    data = data.iloc[:,rnd:rnd+n]\n    sns.heatmap(data.corr())\n    plt.show()","a7846990":"heat(30)","0c5b59cc":"#Count of top 50 targets\ndf = train_scored.iloc[:,1:].sum(axis=0).sort_values(ascending=True)[-50:]","92d34491":"df = df.sort_values(ascending=False)","9d8f1b06":"df","75000812":"plt.figure(figsize=(8,13))\nsns.barplot(y=df.index.tolist(),x=df)\nplt.show()","a5e0b9c1":"# count of lowest 50 target\ndf = train_scored.iloc[:,1:].sum(axis=0).sort_values(ascending=True)[:50]\ndf = df.sort_values(ascending= False)","8d523e9c":"plt.figure(figsize=(8,13))\nsns.barplot(y=df.index.tolist(),x=df)\nplt.show()","4a6f0840":"data = train_df.merge(train_scored,on='sig_id')\ntop_50 = train_scored.drop(\"sig_id\",axis=1).columns[train_scored.iloc[:,1:].sum(axis=0)>=89]\nbottom_50 = train_scored.drop(\"sig_id\",axis=1).columns[train_scored.iloc[:,1:].sum(axis=0)<=19]\ndata_top_50 = data[data[top_50].any(axis=1)][g_cols]\ndata_bottom_50  = data[data[bottom_50].any(axis=1)][g_cols]","d0d125ee":"#Plotting of mean of gene exp for top 50\nplt.figure(dpi=70)\nsns.distplot(data_top_50.mean(axis=1),color='violet')\nplt.show()","fab0bdce":"#random 4 gene exp from top 50\ndef plot2(df):\n    rnd = np.random.randint(0,df.shape[0]-5)\n    plt.figure(figsize=(10,7))\n    \n    for i in range(4):\n        data = df.iloc[rnd+i,:]\n        mean = np.mean(data.values)\n        plt.subplot(2,2,i+1)\n        sns.scatterplot(data=data.values,marker=\">\") \n        plt.tick_params(\n        axis='x',          \n        which='both',      \n        bottom=False,    \n        top=False,        \n        labelbottom=False)\n        sns.lineplot(x=list(range(len(data))), y = [mean]*len(data),color='r',linewidth=2)\n        \n    plt.show()","edfa292c":"plot2(data_top_50)","b4c82658":"#Plot of mean of gene exp for top 50\nplt.figure(dpi=80)\nsns.distplot(data_bottom_50.mean(axis=1),color='blue')\nplt.show()","59eea841":"#random 4 gene exp from bottom 50\nplot2(data_bottom_50)","f589d184":"#Top 50 sample with highest count of target\ndf = train_scored.iloc[:,1:].sum(axis=1).sort_values(ascending=True)[-50:]","9bf150bd":"plt.figure(figsize=(8,13))\nsns.barplot(y=df.index.tolist(),x=df)\nplt.show()","52d973eb":"fig, axs = plt.subplots(ncols=2 , nrows = 2 , figsize=(9, 9))\nsns.distplot(train_df['c-0'] ,color=\"b\", kde_kws={\"shade\": True}, ax=axs[0][0] )\nsns.distplot(train_df['c-1'] ,color=\"r\", kde_kws={\"shade\": True}, ax=axs[0][1] )\nsns.distplot(train_df['c-2'], color=\"g\", kde_kws={\"shade\": True}, ax=axs[1][0] )\nsns.distplot(train_df['c-3'] ,color=\"y\", kde_kws={\"shade\": True}, ax=axs[1][1] )\nplt.show()","3f0b283e":"DO_EDA = True ","7e21dea4":"if DO_EDA:\n    # sample_cols = ['g-0']\n    # Create a sampled dataframe and use hue to denote different histograms?\n\n    sns.set_context('poster')\n\n    ax = sns.distplot(train_df['g-0'])\n    ax2 = sns.distplot(train_df['g-100'])\n    ax3 = sns.distplot(train_df['g-200'])\n    ax4 = sns.distplot(train_df['g-300'])\n    ax5 = sns.distplot(train_df['g-400'])\n    ax6 = sns.distplot(train_df['g-500'])\n    ax7 = sns.distplot(train_df['g-600'])\n    ax8 = sns.distplot(train_df['g-700'])\n    ax9 = sns.distplot(train_df['g-750'])\n    ax10 = sns.distplot(train_df['g-150'])\n\n\n    ax.set(title = \"Regulation of 10 Random Genes\",\n          xlabel = \"Upregulation or Downregulation\",\n          ylabel = \"Percent of Sample\")\n\n    plt.annotate(\"Gene Deeply Downregulated\", xy = (-9.9, .01), xytext = (-7.8, 0.21),\n                 size = 16,\n                 arrowprops = {'facecolor':'grey', 'width':3})\n\n    plt.annotate(\"Somewhat Downregulated\", xy = (-5, 0.05), xytext = (-7.8, 0.11), size = 16,\n                arrowprops = {'facecolor':'grey', 'width':3},\n                backgroundcolor = 'white')\n\n    plt.annotate(\"Genes Upregulated.  Slight Right Skew\", xy = (2.5, 0.06), xytext = (2.5, 0.11), size = 16,\n                arrowprops = {'facecolor':'grey', 'width':3},\n                backgroundcolor = 'white')\n\n    plt.legend()\n    plt.show()","a39dedb2":"if DO_EDA:\n    sns.set_context('poster')\n\n    ax = sns.distplot(train_df['c-0'])\n    ax2 = sns.distplot(train_df['c-10'])\n    ax3 = sns.distplot(train_df['c-20'])\n    ax4 = sns.distplot(train_df['c-30'])\n    ax5 = sns.distplot(train_df['c-40'])\n    ax6 = sns.distplot(train_df['c-50'])\n    ax7 = sns.distplot(train_df['c-60'])\n    ax8 = sns.distplot(train_df['c-70'])\n    ax9 = sns.distplot(train_df['c-80'])\n    ax10 = sns.distplot(train_df['c-90'])\n\n    ax.set(title = \"Viability of 10 Random Cell Samples\",\n          xlabel = \"Increased or decreased viability\",\n          ylabel = \"Percent of Sample\")\n\n    plt.annotate(\"Drug effective at killing cells \/ Error?\", xy = (-9.9, .08), xytext = (-7.8, 0.21),\n                 size = 16,\n                 arrowprops = {'facecolor':'grey', 'width':3})\n\n    plt.annotate(\"More cells are killed in general\", xy = (-4, 0.02), xytext = (-7.8, 0.11), size = 16,\n                arrowprops = {'facecolor':'grey', 'width':3},\n                backgroundcolor = 'white')\n\n    plt.annotate(\"Cell viability enhanced less often\", xy = (1.5, 0.06), xytext = (2.5, 0.11), size = 16,\n                arrowprops = {'facecolor':'grey', 'width':3},\n                backgroundcolor = 'white')\n\n    plt.legend()\n    plt.show()","fdd2be3f":"plt.style.use('seaborn')\n# sns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,5))\n#1 rows 2 cols\n#first row, first col\nax1 = plt.subplot2grid((1,2),(0,0))\nsns.countplot(x='cp_type', data=train_df, palette='rainbow', alpha=0.75)\nplt.title('Train: Control and treated samples', fontsize=15, weight='bold')\n#first row sec col\nax1 = plt.subplot2grid((1,2),(0,1))\nsns.countplot(x='cp_dose', data=train_df, palette='Purples', alpha=0.75)\nplt.title('Train: Treatment Doses: Low and High',weight='bold', fontsize=18)\nplt.show()","61d565ff":"plt.figure(figsize=(15,5))\nsns.distplot( train_df['cp_time'], color='red', bins=5)\nplt.title(\"Train: Treatment duration \", fontsize=15, weight='bold')\nplt.show()","efd3b204":"plotf('g-10','g-100','g-200','g-400')","c5993c29":"train_df.head()","6af6cf47":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import KFold\nfrom category_encoders import CountEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import log_loss\n\nfrom sklearn.multioutput import MultiOutputClassifier\n\nimport os\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold,StratifiedKFold,RepeatedStratifiedKFold\nfrom lightgbm import LGBMClassifier\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder","fabcca6a":"SEED = 42\nNFOLDS = 5\nDATA_DIR = \"\/kaggle\/input\/lish-moa\/\"\nnp.random.seed(SEED)","49d2c3f0":"train = pd.read_csv(DATA_DIR+\"train_features.csv\")\ntargets = pd.read_csv(DATA_DIR + \"train_targets_scored.csv\")\n\ntest = pd.read_csv(DATA_DIR+\"test_features.csv\")\nsub = pd.read_csv(DATA_DIR+\"sample_submission.csv\")\n\n# drop id col\nX = train.iloc[:,1:].to_numpy()\nX_test = test.iloc[:,1:].to_numpy()\ny = targets.iloc[:,1:].to_numpy()","16ea43d8":"classifier = MultiOutputClassifier(XGBClassifier(tree_method='gpu_hist'))\n\nclf = Pipeline([('encode', CountEncoder(cols=[0, 2])),\n                ('classify', classifier)\n               ])","9e189a34":"params = {'classify__estimator__colsample_bytree': 0.6522,\n          'classify__estimator__gamma': 3.6975,\n          'classify__estimator__learning_rate': 0.0503,\n          'classify__estimator__max_delta_step': 2.0706,\n          'classify__estimator__max_depth': 10,\n          'classify__estimator__min_child_weight': 31.5800,\n          'classify__estimator__n_estimators': 166,\n          'classify__estimator__subsample': 0.8639\n         }\n\n_ = clf.set_params(**params)","8267b1c1":"oof_preds  = np.zeros(y.shape)\ntest_preds = np.zeros((test.shape[0],y.shape[1]))\noof_losses = []\nkf = KFold(n_splits=NFOLDS)\n\nfor fn,(trn_idx,val_idx) in enumerate(kf.split(X,y)):\n    print(\"Starting fold: \",fn)\n    X_train,X_val = X[trn_idx],X[val_idx]\n    y_train,y_val = y[trn_idx],y[val_idx]\n    \n    # drop where cp_type == ctl_vehicle(baseline)\n    ctl_mask = X_train[:,0]==\"ctl_vehicle\"\n    X_train = X_train[~ctl_mask,:]\n    y_train = y_train[~ctl_mask]\n    \n    clf.fit(X_train,y_train)\n    val_preds = clf.predict_proba(X_val) # list of preds per class\n    \n    val_preds = np.array(val_preds)[:,:,1].T  # take the positive class\n    \n    oof_preds[val_idx] = val_preds\n    \n    loss = log_loss(np.ravel(y_val),np.ravel(val_preds))\n    oof_losses.append(loss)\n    preds = clf.predict_proba(X_test)\n    preds = np.array(preds)[:,:,1].T # take the positive class\n    test_preds +=preds\/NFOLDS\n    \nprint(oof_losses)\nprint(\"Mean OOF loss across folds\",np.mean(oof_losses))\nprint(\"STD OOF loss across folds\",np.std(oof_losses))","d64f3923":"# set control train preds to 0h\ncontrol_mask = train[\"cp_type\"] == \"ctl_vehicle\"\noof_preds[control_mask] = 0\n\nprint(\"OOF log loss: \",log_loss(np.ravel(y),np.ravel(oof_preds)))","b36b8160":"# Analysis of OOF preds\n\n# set control test preds to 0\ncontrol_mask = test[\"cp_type\"]=='ctl_vehicle'\n\ntest_preds[control_mask] = 0","da19adbf":"# Create the submission file\n\nsub.iloc[:,1:] = test_preds\nsub.to_csv(\"submission.csv\",index=False)","99610d3e":"# Still working in progress!!","6a6de5c1":"> Cell Features","b346c980":"## About MoA\n> In pharmacology, the term mechanism of action (MOA) refers to the specific biochemical interaction through which a drug substance produces its pharmacological effect. A mechanism of action usually includes mention of the specific molecular targets to which the drug binds, such as an enzyme or receptor.","61249c60":"> Train the model\nFraming this problem as a binary classification problem has the disadvantage that you need to train as many models as you have classes. For this problem this means training 206 models per fold, for the large number of features included in this dataset this may take a long time...","e2f957dd":"## Taking input"}}