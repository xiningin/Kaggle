{"cell_type":{"51474863":"code","333984b0":"code","56655300":"code","7875581a":"code","ea336fe1":"code","18201d8e":"code","f8cd8b08":"code","773683b7":"code","cc00f260":"code","7ed9ae69":"code","b7749bd8":"markdown","c8aa2cce":"markdown","75701c0b":"markdown","7520662d":"markdown","ebb26eac":"markdown","3ced6ff1":"markdown"},"source":{"51474863":"import random\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport torchvision.models as models\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torchvision import transforms\nfrom sklearn.metrics import accuracy_score\nimport cv2\nfrom typing import Optional, List","333984b0":"# Setting device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\n\n# Matplotlib\nplt.rcParams[\"figure.figsize\"] = (13, 7)","56655300":"resize = transforms.Resize((128, 128))\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    resize,\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n])\n\ninv_normalize = transforms.Normalize(\n    mean=[-0.485\/0.229, -0.456\/0.224, -0.406\/0.255],\n    std=[1\/0.229, 1\/0.224, 1\/0.255]\n)","7875581a":"TRAIN_PATH = \"..\/input\/fastai-imagenet\/FastAI_ImageNet_v2\/train\/\"\nTEST_PATH = \"..\/input\/fastai-imagenet\/FastAI_ImageNet_v2\/val\/\"\n\ntrain_dataset = ImageFolder(root=TRAIN_PATH, transform=transform)\ntest_dataset = ImageFolder(root=TEST_PATH, transform=transform)","ea336fe1":"# Show example image from dataset\n\ndef show_image(image, normalized=True): \n    \"\"\"\n    Plots an image from a tensor.\n    \"\"\"\n    if normalized: \n        image = inv_normalize(image)\n    image = image.detach().cpu().permute(1, 2, 0)\n    plt.imshow(image)\n    plt.axis(\"off\")\n    \nshow_image(train_dataset[0][0])","18201d8e":"from torchvision.models import vgg11\n\nclass CustomVGG(nn.Module): \n    BLOCKS = [0, 0, 0, \n              1, 1, 1,\n              2, 2, 2, 2, 2,\n              3, 3, 3, 3, 3,\n              4, 4, 4, 4, 4]\n    \n    def __init__(self, num_out: int): \n        super().__init__()\n        self.vgg = vgg11(pretrained=True)\n        # Taken directly from source code: \n        # https:\/\/pytorch.org\/vision\/stable\/_modules\/torchvision\/models\/vgg.html#vgg11\n        self.vgg.classifier = nn.Sequential(\n            nn.Linear(512 * 7 * 7, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, 4096),\n            nn.ReLU(True),\n            nn.Dropout(),\n            nn.Linear(4096, num_out)\n        )\n    \n        self.vgg_layers = {}\n        for layer, block_id in zip(self.vgg.features.children(), self.BLOCKS): \n            if block_id not in self.vgg_layers: \n                self.vgg_layers[block_id] = []\n            self.vgg_layers[block_id].append(layer)\n    \n    def freeze_layers(self): \n        for name, layer in self.vgg.named_children(): \n            if name != \"classifier\":\n                for param in layer.parameters():\n                    param.require_grad = False\n    \n    def get_result_from_layer(self, x:torch.Tensor, block: int):\n        \"\"\"\n        0-indexed\n        \"\"\"\n        modules = []\n        for block_id in range(block+1):\n            modules.extend(self.vgg_layers[block_id])\n        \n        layer = nn.Sequential(*modules)\n        result = layer(x)\n        return result\n                \n    def forward(self, x): \n        return self.vgg(x)\n    \nmodel = CustomVGG(len(train_dataset.classes))\nmodel.freeze_layers()\nmodel.to(device)\noptimizer = optim.AdamW(model.parameters())\ncriterion = nn.CrossEntropyLoss()","f8cd8b08":"# Creates a 4x4 \nSQUARE_SIZE = 4 \n# Determines how much of the heatmap is visible on superimposed image\nHEATMAP_INTENSITY = 0.4 \n\ndef plot_visuals(model: nn.Module,\n                 dataset: Dataset,\n                 idx: int,\n                 block:int, \n                 channel_seed:Optional[int]=None,\n                 super_impose:bool=True,\n                 channels:Optional[List]=None,\n                 include_ori:bool=True): \n    \"\"\"\n    idx: ID of image from dataset to choose\n    block: Which block to visualize from\n    channel_seed: Determines which channels are showed\n    channels: Force which channels to use\n    \"\"\"\n    image = dataset[idx][0].unsqueeze(0).to(device)\n    image_np = dataset[idx][0]\n    image_np = inv_normalize(image_np)\n    image_np = image_np.permute(1, 2, 0).numpy()\n\n    vis = model.get_result_from_layer(image, block)\n    vis = vis.detach().cpu().squeeze(0).numpy()\n    np.random.seed(channel_seed)\n    if channels is None:\n        channels = list(np.random.choice(range(vis.shape[0]), size=SQUARE_SIZE**2, replace=False))\n    if len(channels) < SQUARE_SIZE ** 2: \n        available = list(set(range(vis.shape[0])) - set(channels))\n        additional = np.random.choice(available, size=(SQUARE_SIZE**2)-len(channels), replace=False)\n        channels.extend(additional)\n    \n    if include_ori: \n        channels.insert(0, -1)\n        channels.pop()\n    \n    fig, axs = plt.subplots(SQUARE_SIZE, SQUARE_SIZE, figsize=(15, 15))\n    fig.suptitle(f\"ID: {idx} block: {block}\")\n    for i in range(SQUARE_SIZE): \n        for j in range(SQUARE_SIZE): \n            channel = channels[(i*SQUARE_SIZE)+j]\n            val = vis[channel, ...]\n            if include_ori and channel == -1: \n                result = image_np\n            elif super_impose:\n                val = cv2.resize(val, (128, 128), interpolation = cv2.INTER_LINEAR)\n                val = val \/ val.max()\n                val = np.uint8(255 * val)\n                val = cv2.applyColorMap(val, cv2.COLORMAP_JET)\n                val = np.float64(val)\/255\n                result = (image_np * (1 - HEATMAP_INTENSITY)) + (HEATMAP_INTENSITY * val)\n            else: \n                val = cv2.resize(val, (128, 128), interpolation = cv2.INTER_LINEAR)\n                result = val\n            axs[i][j].imshow(result, cmap=\"jet_r\")\n            axs[i][j].axis(\"off\")\n            axs[i][j].set_title(f\"Channel {channel}\")\n    plt.tight_layout()\n    plt.show()\n    \ndef get_random_id(dataset): \n    np.random.seed(None)\n    return np.random.choice(len(dataset)-1)","773683b7":"ridx = get_random_id(train_dataset)\nplot_visuals(model, train_dataset,\n             idx=ridx, block=0, \n             super_impose=False,\n             channels=[28],\n             include_ori=True,\n             channel_seed=None)","cc00f260":"ridx = get_random_id(train_dataset)\nplot_visuals(model, train_dataset,\n             idx=ridx, block=0, \n             super_impose=False,\n#              channels=[28],\n             include_ori=True,\n             channel_seed=None)","7ed9ae69":"# TRAINING: \n\n# EPOCHS=5\n# BATCH_SIZE=32\n# train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n# test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# train_losses = []\n# test_losses = []\n\n# for epoch in range(EPOCHS): \n#   # Train loop\n#     t = tqdm(train_dataloader, desc=f\"Train Epoch {epoch}\")\n#     all_loss = 0\n#     model.train()\n#     for images, target in t:\n#         images = images.to(device)\n#         target = target.to(device)\n#         prediction = model(images)\n#         loss = criterion(prediction, target)\n#         loss.backward()\n#         optimizer.step()\n#         optimizer.zero_grad()\n#         all_loss += loss.item() * images.shape[0]\n#         train_losses.append(all_loss)\n\n# # Eval loop\n# model.eval()\n# all_loss = 0\n# with torch.no_grad(): \n#     t = tqdm(test_dataloader, desc=f\"Test Epoch {epoch}\")\n#     for images, target in t: \n#         images = images.to(device)\n#         target = target.to(device)\n#         prediction = model(images)\n#         loss = criterion(prediction, target)\n#         all_loss += loss.item() * images.shape[0]\n#         test_losses.append(all_loss)","b7749bd8":"# VGG11 Visualizer\n\nThis short notebook aims to visualize the pretrained layers within a VGG11 CNN. ","c8aa2cce":"## Visualization","75701c0b":"## Dataset and preprocessing","7520662d":"## Training loop\n\nThis is currently disabled as we don't need a training loop to visualize the pretrained vgg. \nHowever, we can unfreeze the layers and update the layers to see what changes arise.\n\nFurthermore, we can utilize CAMs (Class Activation Maps) to create saliency maps. This might be done in a future version of this notebook.\nAnother technique of creating saliency maps that are interesting: [RISE](https:\/\/arxiv.org\/pdf\/1806.07421)","ebb26eac":"## Imports and Setup ","3ced6ff1":"## Model Init"}}