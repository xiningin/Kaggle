{"cell_type":{"30afd53b":"code","828f3925":"code","8705582a":"code","17ec3d69":"code","d8710658":"code","71ddd670":"code","d667ccdf":"code","3fb7bcdb":"code","7f64a0b7":"code","f7f68c53":"code","76fbc9d5":"code","533ba8cd":"code","2dcb95bf":"code","bab9f35a":"code","c31231ab":"code","752413e0":"code","e772418f":"code","c7c8bbab":"code","45efae99":"markdown","342f44ca":"markdown","0202a161":"markdown","e91b009d":"markdown","2641cffc":"markdown","ec08804f":"markdown","f1946c4b":"markdown","f71e3a11":"markdown","85160608":"markdown","afcb2f6e":"markdown","a734377e":"markdown","5fc347bc":"markdown","290bc0d3":"markdown","549a1270":"markdown","93127aee":"markdown","fc43f7cf":"markdown","e2ca1c9f":"markdown","215d256f":"markdown","2849e5e4":"markdown","bda5c9f9":"markdown","f92f1e57":"markdown","c8081da9":"markdown","6753a546":"markdown","d53ed2c4":"markdown","518cc613":"markdown","af5c65ab":"markdown"},"source":{"30afd53b":"import os,re,gc,pickle,random,sys\n\nimport numpy as np \nimport pandas as pd \nimport tensorflow as tf\nimport transformers\nfrom transformers import TFAutoModel\nfrom tensorflow.data.experimental import sample_from_datasets\nfrom tensorflow.data import Dataset\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm","828f3925":"AUTO = tf.data.experimental.AUTOTUNE\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(strategy.num_replicas_in_sync)\nBATCH_SIZE = 32 * strategy.num_replicas_in_sync","8705582a":"np.random.seed(1234)\nrandom.seed(1234)\nos.environ['TF_DETERMINISTIC_OPS'] = '1'\nwith strategy.scope():\n    tf.random.set_seed(1234)","17ec3d69":"MAX_LEN = 192\nMODEL = 'jplu\/tf-xlm-roberta-large'\nHEAD = \"mean\"\n\ninput1 = \"\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/\"\ninpath = \"..\/input\/jwtc-xlmroberta-encoding-192-pickle\/datain\/\"\nform = \"training\/encode_{}.pkl\"\nlangs = [\"en\",\"en2\",\"es\",\"fr\",\"it\",\"pt\",\"ru\",\"tr\"]\nused_data = langs #[\"en\",\"en2\"]","d8710658":"def pick_load_format(path):\n    with open(inpath+path,\"rb\") as f:\n        return pickle.load(f)\n\ndef load(path):\n    return pick_load_format(form.format(path))","71ddd670":"def get_cong(n,verb=True):\n    tot = round(1+(n*2)\/10_000)*10_000\n    if verb: print(\"Pos: {}, Sample neg: {}, Total: {}\".format(n,tot-n,tot))\n    return tot,tot-n\n\ndef load_data(seed=1214):\n    train = []\n    for i in used_data:\n        df1 = load(i+\"_l1\")\n        size, sample_size = get_cong(df1.shape[0])\n        df0 = load(i+\"_l0\").sample(n=sample_size, random_state=seed)\n        train += [df1,df0]\n    train = pd.concat(train)\n\n    train = np.stack(train.comment_text.values, axis=0).astype(\"int32\"),train.toxic.values\n\n    valid = pick_load_format(\"valid.pkl\")\n    x_valid = np.stack(valid.comment_text.values, axis=0).astype(\"int32\")\n    y_valid = valid.toxic.values\n\n    test = pick_load_format(\"test.pkl\")\n    x_test = np.stack(test.content.values, axis=0).astype(\"int32\")\n    \n    return train,(x_valid,y_valid),x_test","d667ccdf":"%%time\ntrain,valid,test = load_data()\ngc.collect()\n\nvalid_size = len(valid[1])\ntrain_size = len(train[1])\nprint(train_size,valid_size)","3fb7bcdb":"def make_dataset_pipeline(dataset, cache=False,repeat_and_shuffle=False,shuffle_size=128_000,seed=386491):\n    if cache: dataset = dataset.cache()\n    if repeat_and_shuffle:\n        dataset = dataset.repeat().shuffle(shuffle_size,seed)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO)\n    return dataset","7f64a0b7":"train_dataset = Dataset.from_tensor_slices(train)\ntrain_dataset = make_dataset_pipeline(train_dataset,True, repeat_and_shuffle=True)\n\nvalid_dataset = make_dataset_pipeline(Dataset.from_tensor_slices(valid) ) \nvalid_dataset2 = make_dataset_pipeline(Dataset.from_tensor_slices((valid)), True,\n                                       shuffle_size=8000, repeat_and_shuffle=True ) \n\ntest_dataset = make_dataset_pipeline(Dataset.from_tensor_slices(test))","f7f68c53":"from tensorflow.keras.layers import Input,Dropout,Dense,GlobalAveragePooling1D,GlobalMaxPool1D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import AUC \nfrom tensorflow.keras.initializers import GlorotUniform\n\ndef get_cls(x):\n    return x[:, 0, :]\n\ndic = {\"mean\":GlobalAveragePooling1D(),\n      \"max\":GlobalMaxPool1D(),\n      \"cls\":get_cls}\n\ndef build_model(transformer,head=\"cls\" , loss='binary_crossentropy',\n                max_len=512, drop_rate=None, lr=1e-5,seed=940208):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    x = dic[head](sequence_output)\n    if drop_rate is not None: \n        x = Dropout(drop_rate)(x)\n    out = Dense(1, activation='sigmoid',kernel_initializer=GlorotUniform(seed))(x)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    model.compile(Adam(lr=lr), loss=loss, metrics=[AUC()])\n    \n    return model","76fbc9d5":"%%time\nwith strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model = build_model(transformer_layer,head=HEAD,loss='binary_crossentropy', max_len=MAX_LEN)\nmodel.summary()","533ba8cd":"from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler,ReduceLROnPlateau\n\nmodel_path = \"xlm-roberta.h5\"\ncheckpoint = ModelCheckpoint(model_path, monitor='val_auc', mode='max', save_best_only=True, save_weights_only=True, verbose=1)\nes = EarlyStopping(monitor='val_auc', mode='max', patience=6, restore_best_weights=False, verbose=1)\nrp = ReduceLROnPlateau(monitor='val_auc', factor=0.8, patience=3, verbose=1, mode='max')\n\ncallback_list = [checkpoint,es,rp]","2dcb95bf":"%%time\nN_STEPS = train_size \/\/ (BATCH_SIZE*4)\nEPOCHS = 10\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=N_STEPS,\n    validation_data=valid_dataset,\n    callbacks=callback_list,\n    epochs=EPOCHS\n)","bab9f35a":"del model\ngc.collect()\ntf.tpu.experimental.initialize_tpu_system(tpu)","c31231ab":"%%time\nwith strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model = build_model(transformer_layer,head=HEAD,loss='binary_crossentropy', max_len=MAX_LEN,lr=8e-6)\n    model.load_weights(model_path)","752413e0":"%%time\nn_steps = valid_size \/\/ (BATCH_SIZE)\nEPOCHS = 1\ntrain_history_2 =model.fit(\n    valid_dataset2,\n    steps_per_epoch=n_steps,\n    epochs= EPOCHS\n)","e772418f":"!rm xlm-roberta.h5","c7c8bbab":"sub = pd.read_csv(input1 + \"sample_submission.csv\")\nsub['toxic'] = model.predict(test_dataset, verbose=1)\nsub.to_csv('submission.csv', index=False)","45efae99":"The build_model function is defined to allow different top layers: CLS, globalmean, globalaverage. One can add other structure into the 'dic' and referred by self-defined string.","342f44ca":"<h3>Transfer to monolingual model<\/h3>\n\nAfter achieve <span style=\"color:red\"> lb.9462<\/span>, I follow [first place](https:\/\/www.kaggle.com\/c\/jigsaw-multilingual-toxic-comment-classification\/discussion\/160862) to transfer knowledge of cross language model to monolingual models. \n\nhttps:\/\/www.kaggle.com\/mint101\/transfer-to-monolingual-mix is an example of transfer to a spanish model. It scores <span style=\"color:red\"> .9461<\/span> before and <span style=\"color:red\">.9467<\/span> after ensemble with the result being transferred.\n\nAfter transfer to monolingual models in each language, I combine all these models and get <span style=\"color:red\"> lb.9500<\/span>. With slight adjustment, I achieve [lb.9508](https:\/\/www.kaggle.com\/mint101\/jmtc-20-lb-9508-mono-lingual-models).","0202a161":"The shuffle_size must be large need to ensure dataset well shuffled.","e91b009d":"# TF dataset","2641cffc":"# The trap about BERT-like model volatility","ec08804f":"**After Bert, there are two main directions in this field:**\n\n**The first one is to change structure of BERT(including training pattern):**\n1. [Roberta](https:\/\/arxiv.org\/pdf\/1907.11692.pdf):  fine tunes BERT and get better performance.\n2. [XLNet](https:\/\/arxiv.org\/pdf\/1906.08237):  invents Transformer-XL for super long context by involving a segment-level \"RNN\" on Decoder, then use Permutated language model(PLM) to train a generative model with knowledge of context in both side.\n3. [Albert](https:\/\/arxiv.org\/pdf\/1909.11942.pdf):  use word embdedding decomposition and weight sharing to save training cost and  time. With this saving, it makes BERT wider and deeper for better performance.\n4. [T5](https:\/\/arxiv.org\/abs\/1910.10683):  unifies different tasks to the same text-to-text pattern, use multiple task learning(MTL) to achieve a new SOTA.\n5. [Electra](https:\/\/arxiv.org\/pdf\/2003.10555.pdf):  use the concept of GAN to train BERT with generator and discriminator.\n\n\n**The second one is to enlarge training corpus and include different languges:**\n1. A lot of monolinguish BERT models are trained with languge other than English.\n2. Cross languge model like mbert and [XLM-Roberta](https:\/\/arxiv.org\/pdf\/1911.02116.pdf) are trained on multiple language corpus. \n","f1946c4b":"<h3> Pseudo labeling <\/h3>\n\nAfter achieve <span style=\"color:red\">lb.9455<\/span> with ensmeble basic XLM-R model, I followed the [second place solution](https:\/\/www.kaggle.com\/xiwuhan\/jmtc-2nd-place-solution?scriptVersionId=37463887) to train XLM-R model with pseudo-label on test data. It suggests using high quality predictions on test data as augmentations. Rather than hard label, I followed [first place](https:\/\/www.kaggle.com\/c\/jigsaw-multilingual-toxic-comment-classification\/discussion\/160862) to use soft label. \n\nhttps:\/\/www.kaggle.com\/mint101\/example-code-of-pseudo-label-on-xlm-r is an example code of training XLM-R with pseudo-labeling.\n\nTwo lessons are learned from training with pseudo-labeling:\n1. Mixing the pseudo-labeling with original training data and training together is better than training two data sequentially.\n2. Training with pseudo-labeling does not guarantee a better result, but ensemble it with the one generating pseudo-label is likely to give better result.\n\n\n\n\n\nEnsemble on multiple results with pseudo-label boost my score to <span style=\"color:red\"> lb.9462<\/span>\n\n<u> With multiple turn of pseudo-label and fine-tune, it can achieve [lb.9475](https:\/\/www.kaggle.com\/hmendonca\/jigsaw20-xlm-r-lb0-9487-singel-model). (<span style=\"color:red\">Need huge resource and extensive search<\/span>) <\/u>","f71e3a11":"# Configuration","85160608":"Jigsaw Toxic Comment competition has been hold for three years. The first year(2018) one gives toxic and non-toxic comments, while the second year(2019) one enlarges data size and gives soft label of toxicity. For the third year, JWTC-2020 combines the training data of last two year, while try to judge toxicity cross language.\n\nThe JWTC task is a typical sentimental analysis task in the area of nature language processing(NLP). Before going deep into the task, we'd better look at the current literature of NLP. \n\n<br\/>\n\nThe NLP area is quite different before and after 2018. Before 2018, people usually design different models for different tasks, while model consists of fixed word embedding plus RNN\/CNN. After 2018, Bert-like pretrained models dominate all tasks. An additional top layer with fine-tuning is enough to transfer them to down-streaming task and get a deccent result.\n\n**The following graph shows this development process:**","afcb2f6e":"Since we will train on the same data many times, I follow https:\/\/www.kaggle.com\/yeayates21\/xlm-roberta-augmentation-ssl-0-9417-pub-lb to do encoding in a seperate kernal and save the result as a [dataset](https:\/\/www.kaggle.com\/mint101\/jwtc-xlmroberta-encoding-192-pickle). This method save about 20 min per kernel run.","a734377e":"1. For the first idea, the typical starter kernel is @xhlulu 's https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-xlm-roberta. (lb.9383). \n\nHe only use the English training data, sample the negative data for label balancing, train on training data for 2 epoch and valid data for 2 epcoh. His result illustrated the importance of label balancing and ultilizing valid data in this competition.\n\n<br\/>\n\n2. For the second one, the starter of this \"translation to English\" path is @miklgr500 's https:\/\/www.kaggle.com\/miklgr500\/jigsaw-tpu-bert-with-huggingface-and-keras (lb .9158). \n\nHe traina BERT-base on fraction of 2018 data and elaborate on learning rate schedule. I tried his way with other advanced BERT-like model and more data, but the lb score always <.92 indicates that using translation to inference may be misleading.","5fc347bc":"# Two ideas and Corresponding Starter Kernel","290bc0d3":"# Load Data","549a1270":"# Build the model and check summary","93127aee":"In contrast, the first one seems better. To further boost its performance, people start to do different data augmentations. They find that the second idea can also be used as augmentations, so they translate the training data into target languages, then use XLM or other monolinguish models over the translations. \n\n<h4><u>This \"use both\" idea is the mainstream choice in this competition and is used in this notebook.<\/u><h4\/>","fc43f7cf":"Back to JMTC-2020. One special point of the task is that the training data is in English, while valid and test data are in other languages. It is a zero-shot like task, where competition orgnizer want to see whether pure training on english can transfer to other language directly.\n\nTo solve this problem, we can either use a **pretrained cross language model**, or some kinds of **translation**. (**or both**)","e2ca1c9f":"![process.png](attachment:process.png)","215d256f":"# Training","2849e5e4":"The BERT-like model are very powerful but volatile. From https:\/\/arxiv.org\/pdf\/2002.06305.pdf, we can find that changing random seed of weight initialization and data order alone can affect performance a lot.\n\nMoreover, training with TPU intorduing [uncontrollable randomness](https:\/\/cloud.google.com\/tpu\/docs\/troubleshooting#deterministic-training) in data order and parameter updating. That means the training result can vary a lot even all hyperparameters remain the same and random seed is fixed.\n\nCombine the two facts above, we can understand that hyperparameter tuning and model structure choosing is more difficult on TPU. If one just use one-time training result to select hyperparameter and structure, it can be easily mislead by randomness.\n\n<br\/><br\/>\n\nFor example, the following three kernels use similar structure and complexer setting but have smaller lb score:\n\n1. https:\/\/www.kaggle.com\/yeayates21\/xlm-roberta-augmentation-ssl-0-9417-pub-lb (.9399): used extra open-subtitle dataset and additional augmentations.\n2. https:\/\/www.kaggle.com\/riblidezso\/train-from-mlm-finetuned-xlm-roberta-large (.9422): used seperated learning rate by layers.\n3. https:\/\/www.kaggle.com\/shonenkov\/tpu-training-super-fast-xlmroberta (.9416): used huge amount of augmented data and upsampled the positve data rather than downsampling the negative one. \n\n<span style=\"color:blue\">They all add extra setting before thoroughly explore the basic one, which consumes more resource but does not give better result.<\/span>\n\n<br\/>\n\n<u> I actually followed the third one and implemented upsampling with tf.data.experimental.sample_from_datasets in kaggle kernel. It is more volatile but not giving significant better result. Moreover, it need futher efforts to reduce sizes due to [memory leak](https:\/\/www.kaggle.com\/questions-and-answers\/164991). One can check the commented place in version-1 to know the detailed implemention. <\/u>\n\n<h3>Lessons learned<\/h3>\n\n* ***Try basic setting enough times*** before moving to complexer ones when training BERT-like model. \n* ***Train multiple times with the same setting***, and make the decision with mean or the best prediction about hyperparameter and structure.\n","bda5c9f9":"# Let's start coding now","f92f1e57":"# Simple Setting and Result\nThis simple pure XLM-R notebook uses the original training data and [multiple language translation](https:\/\/www.kaggle.com\/miklgr500\/jigsaw-train-multilingual-coments-google-api) of the 2018 one. It sampled the negative data to nearly the same size of the positive ones, and combined all of them into the final training data. \n\nThe learning schedule is to train 2.5 epoch on training data (save and load best model by ModelCheckPoint) and 1 epoch on valid data, both with nearly constant learning rate (1e-5 and 8e-6, chosen heuristically).\n\nWith this simple setting, one can get lb around <span style=\"color:red\">0.9420 - 0.9445<\/span> within 1h. By simple ensembling results of multiple attempts, one can get about <span style=\"color:red\">lb.9455+<\/span>.\n\n<u>(One can check version-1 for exact training logs. I do not rerun the kernal as I am lack of TPU quota)<\/u>","c8081da9":"# About this Notebook\n\n**This notebook is the first part of my notebook series to reach [lb.9508](https:\/\/www.kaggle.com\/mint101\/jmtc-20-lb-9508-mono-lingual-models) before blending with others result. Hope this kernel can serve as a starter notebook of this competition, and help people avoid some traps in the begining.**","6753a546":"# Further Steps","d53ed2c4":"# Make Submission","518cc613":"1. In the begining, people used RNN\/CNN over fixed word embedding like Word2Vec.\n2. In translation and text generation tasks, [Attention mechanism](https:\/\/arxiv.org\/pdf\/1409.0473.pdf) is found to be useful.\n3. Based on Self-attention and embedded ensembling(multi-head), [Transformer](https:\/\/arxiv.org\/pdf\/1706.03762.pdf) is proposed and replace RNN\/CNN in this field.\n4. Fixed word embedding is showed unable to capture difference of the same word in varied contents. <***Hence, combining word embedding and layers atop to provide contextualized word representations become mainstream.***>\n5. [ELMO](https:\/\/arxiv.org\/pdf\/1802.05365.pdf) is proposed first based on ***Bi-directional LSTM***.\n6. [GPT](https:\/\/s3-us-west-2.amazonaws.com\/openai-assets\/research-covers\/language-unsupervised\/language_understanding_paper.pdf) is the next with ***Decoder part of Transformer***. (It uses the generative part, so only in one direction.)\n7. [BERT](https:\/\/arxiv.org\/pdf\/1810.04805.pdf) is the last but the best one. It use ***Encoder part of Transformer and Masked language model(MLM)*** to look up context on both side from the begining.","af5c65ab":"# Intro. of the task and literature (- early 2020)"}}