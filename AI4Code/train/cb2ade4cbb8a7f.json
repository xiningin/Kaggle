{"cell_type":{"dc8100ba":"code","3971810d":"code","0feca1a6":"code","181a072b":"code","5d2dab7a":"code","ac9e7845":"code","bea616f6":"code","6dd949a4":"code","518a725f":"code","b6f3c856":"code","26f51e4e":"code","eee1ecd4":"code","05f4f49f":"code","ee104e4c":"code","4ef0aa67":"code","f2d803fe":"code","a0f8ceb6":"code","22a55c26":"code","9b4bf874":"code","2c77028c":"code","e4d3335f":"code","cc7d462a":"code","2163f450":"code","2aebe8fc":"code","b012771e":"code","c7899e06":"code","71aed2bf":"code","ce198857":"code","a8be09e1":"code","77b8faf1":"markdown","b8e40d35":"markdown","96c1f7fa":"markdown","8cf1fd6a":"markdown","12a34655":"markdown","e1cdbcb1":"markdown","f368c9c2":"markdown","0f0e2b9f":"markdown","862def03":"markdown","9dc5ac93":"markdown","7e87fe4e":"markdown","1fc61f18":"markdown","cf642694":"markdown","5640b05a":"markdown","b63da538":"markdown","e3dbdeb0":"markdown","20ea28a8":"markdown","0be532b9":"markdown","c80cab53":"markdown","a49f9f18":"markdown","db045913":"markdown","99eee904":"markdown","b4d36c15":"markdown","83572c5f":"markdown","94e60e39":"markdown","463d2f20":"markdown","e55c3db6":"markdown","ca8ace6e":"markdown","2367a374":"markdown","18eb358a":"markdown"},"source":{"dc8100ba":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3971810d":"pd.set_option(\"display.max_colwidth\",180)\ndf = pd.read_csv('..\/input\/unsupervised-learning-on-country-data\/Country-data.csv')\ndata_dict = pd.read_csv('..\/input\/unsupervised-learning-on-country-data\/data-dictionary.csv')","0feca1a6":"data_dict","181a072b":"df.info()","5d2dab7a":"df.shape","ac9e7845":"df.describe().T","bea616f6":"df['country'].value_counts()","6dd949a4":"df.isnull().sum()","518a725f":"df.isna().sum()","b6f3c856":"#Import ploting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots \ncolors = ['#DB1C18','#DBDB3B','#51A2DB']\nsns.set(palette=colors, font='Serif', style='white', rc={'axes.facecolor':'whitesmoke', 'figure.facecolor':'whitesmoke'})","26f51e4e":"df['country'].count()","eee1ecd4":"fig, ax = plt.subplots(nrows=3,ncols=3, figsize=(15,8), constrained_layout=True)\nplt.suptitle(\"Univariated Data Analyis\")\nax=ax.flatten()\nint_cols= df.select_dtypes(exclude='object').columns\nfor x, i in enumerate(int_cols):\n    sns.histplot(df[i], ax=ax[x], kde=True, color=colors[2])","05f4f49f":"fig, ax = plt.subplots(nrows=3,ncols=3, figsize=(15,8), constrained_layout=True)\nplt.suptitle(\"Univariated Data Analyis\")\nax=ax.flatten()\nint_cols= df.select_dtypes(exclude='object').columns\nfor x, i in enumerate(int_cols):\n    sns.boxplot(x=df[i], ax=ax[x], color=colors[2])","ee104e4c":"px.scatter(data_frame=df, x='exports', y='imports',size='gdpp', text='country', color='gdpp', title='Countries by Export & Import and corresponding GDP')","4ef0aa67":"for i in int_cols:\n    fig=px.choropleth(data_frame=df, locationmode='country names', locations='country', color=i, title=f'{i} rate by countries')\n    fig.show()","f2d803fe":"sns.pairplot(df, corner =True)","a0f8ceb6":"fig=plt.figure(figsize=(15,8))\nsns.heatmap(df.corr(), annot=True, square=True)","22a55c26":"from sklearn.preprocessing import StandardScaler\ndf_scaled = StandardScaler().fit_transform(df.drop(['country'], axis=1))","9b4bf874":"from sklearn.decomposition import PCA\ndecom = PCA(svd_solver='auto')\ndecom.fit(df_scaled)","2c77028c":"cum_exp_ratio = np.cumsum(np.round(decom.explained_variance_ratio_,2))\nprint(cum_exp_ratio)\nfig=plt.figure(figsize=(10,8))\nax=sns.lineplot(y=cum_exp_ratio, x=np.arange(0,len(cum_exp_ratio)))\nax=sns.scatterplot(y=cum_exp_ratio, x=np.arange(0,len(cum_exp_ratio)))\nax.set_xlabel('No of components')\nax.set_ylabel('explaned variance ratio')","e4d3335f":"import scipy.cluster.hierarchy as sch\nfig=plt.figure(figsize=(15,8))\ndendrogram = sch.dendrogram(sch.linkage(df_scaled, method = 'ward'))\nplt.suptitle('Hierarchial clustering - Dendrogram')\nplt.xlabel('Countries')\nplt.ylabel('Euclidean Distances')\nplt.show()","cc7d462a":"from sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\nmodel = KMeans()\nvisualize = KElbowVisualizer(model, k=(1,10))\nvisualize.fit(df_scaled)\nvisualize.poof()\n","2163f450":"model = KMeans(n_clusters=3, random_state=1)\nmodel.fit(df_scaled)\ndf['KMean_labels']=model.labels_\nfig,ax=plt.subplots(nrows=1,ncols=3,figsize=(18,8))\nsns.scatterplot(data=df, x='exports', y='income', hue='KMean_labels', ax=ax[0])\nsns.scatterplot(data=df, x='exports', y='gdpp', hue='KMean_labels', ax=ax[1])\nsns.scatterplot(data=df, x='child_mort', y='health', hue='KMean_labels', ax=ax[2])","2aebe8fc":"df.groupby(['KMean_labels','country']).mean()","b012771e":"from sklearn.metrics import silhouette_score\nsilhouette_score(df_scaled,labels=model.labels_)","c7899e06":"#df['KMean_labels']=df['KMean_labels'].astype('category')\ncat = {0:'Need Help',1:'Might need help',2:'No Help needed'}\ndf['KMean_labels']=df['KMean_labels'].map(cat)\n\npx.choropleth(data_frame=df, locationmode='country names', locations='country', color=df['KMean_labels'], title='Countries by category that need help',\n              color_discrete_map={'Need Help':'#DB1C18','Might need help':'#DBDB3B','No Help needed':'#51A2DB'} ,projection='equirectangular')","71aed2bf":"px.choropleth(data_frame=df, locationmode='country names', locations='country', color=df['KMean_labels'], title='African Countries by category that need help',\n              color_discrete_map={'Need Help':'#DB1C18','Might need help':'#DBDB3B','No Help needed':'#51A2DB'} ,projection='equirectangular', scope='africa')","ce198857":"px.choropleth(data_frame=df, locationmode='country names', locations='country', color=df['KMean_labels'], title='Asian Countries by category that need help',\n              color_discrete_map={'Need Help':'#DB1C18','Might need help':'#DBDB3B','No Help needed':'#51A2DB'} ,projection='equirectangular', scope='asia')","a8be09e1":"df[df['KMean_labels']=='Need Help']['country']","77b8faf1":"### K_Mean Clustering","b8e40d35":"## Data Modeling","96c1f7fa":"***Observations***\n1. Both Histogram and the boxplot clearly shows that the numerical features are contineous or discreate values. there are no features with categorical values. \n2. Box plot shows us there are clear outliers in child_mort, exports, imports, income, gdpp features. however, these informations are belongs to each country. so, we can't expect the values to be normaly distributed wihtout outliers. \n3. Also, the problem statement clearly describes the we need to cluster the countries that need help. so, there are clustering algorithms like ***Manhaten distance*** are less sensible to outliers.","8cf1fd6a":"### Check if there is null or na values","12a34655":"# <center> <font size=20 color='Blue'> Unsupervised learning <\/font> <\/center>\nClustering the Countries by using Unsupervised Learning for HELP International\n\n### Objective: \nTo categorise the countries using socio-economic and health factors that determine the overall development of the country.\n\n### Problem Statement:\nHELP International have been able to raise around $ 10 million. Now the CEO of the NGO needs to decide how to use this money strategically and effectively. So, CEO has to make decision to choose the countries that are in the direst need of aid. Hence, your Job as a Data scientist is to categorise the countries using some socio-economic and health factors that determine the overall development of the country. Then you need to suggest the countries which the CEO needs to focus on the most.","e1cdbcb1":"### Statistical Analysis","f368c9c2":"# Read Data","0f0e2b9f":"***We can clearly see that there 3 cluster***","862def03":"***Observations:***\nI have clustred the countries in 3 categories. \n1. Need Help\n2. Might need help\n3. No Help needed\n\n***Conclusion:***\n1. Most African countries and Pakistan, Afganistan, Iraq, Yemen, Lao etc falls in the category of \"Help Needed\" based on the GDP, Income, Health rate etc\n2. Most Asian countires fall in 2nd category\n3. American, Australian countires, Canada & Europian may not need help. ","9dc5ac93":"***Elbow method is common method used to validate the clustering algorithm. here we can see the K value 3 with relatively good distortion score.***","7e87fe4e":"***Except country feature, other features are either float or integer. there is no text data in the dataframe***","1fc61f18":"## Exploratory Data Analysis","cf642694":"***Each row in the dataset belongs to each country data***","5640b05a":"### Bivariated Data Analysis","b63da538":"***Observations***\n1. country feature is identical value, cant be considered as categorical as there is no multiple entries. so, this particular feature might not be helpful for the modeling. but, we shall use for EDA.","e3dbdeb0":"### Please revewi the Kernel and provide your input for further improvements. Appriciate your feedback and comments","20ea28a8":"Kmeans Algorithm is an Iterative algorithm that divides a group of n datasets into k subgroups \/clusters based on the similarity and their mean distance from the centroid of that particular subgroup\/ formed. KMean is mostly commonly used clustering algorithm","0be532b9":"### PCA - Principal component analysis\nPCA is used in exploratory data analysis and for making predictive models. It is commonly used for dimensionality reduction by projecting each data point onto only the first few principal components to obtain lower-dimensional data while preserving as much of the data's variation as possible. The first principal component can equivalently be defined as a direction that maximizes the variance of the projected data. ","c80cab53":"***From the above Graphs we can clearly see that there are 2 clusters. Aftican and south Asian countires and rest of the world countries. however, further exploration would help us to learn better***","a49f9f18":"***PCA with number of clusters 3 and 4 as deviation in the variance ratio. even 5 also can be considerd as the difference is less. so, lets us try to use the 3,4,5 cluster combination in K_Mean clustering***","db045913":"### Hierarachial Clustering","99eee904":"### Dataframe details","b4d36c15":"# Dataset feature details","83572c5f":"### Univariated data analysis","94e60e39":"***Observations:***\n1. child_mort, exports, imports, income, inflation, gdpp - seems to have large difference between 75% percentile and max value. it looks like these features are right scewed.","463d2f20":"# Different type of Clustering Alogrithm\n1. Affinity Propagation\n2. Agglomerative Clustering\n3. BIRCH\n4. DBSCAN\n5. K-Means\n6. Mini-Batch K-Means\n7. Mean Shift\n8. OPTICS\n9. Spectral Clustering\n10. Gaussian Mixture Model\n\n### 1. Affinity Propagation\nAffinity Propagation involves finding a set of exemplars that best summarize the data.\n\n### 2. Agglomerative clustering\nAgglomerative clustering involves merging examples until the desired number of clusters is achieved.\n\n### 3. BIRCH Clustering \n(BIRCH is short for Balanced Iterative Reducing and Clustering using Hierarchies) involves constructing a tree structure from which cluster centroids are extracted.\n\n### 4. DBSCAN Clustering \nwhere DBSCAN is short for Density-Based Spatial Clustering of Applications with Noise involves finding high-density areas in the domain and expanding those areas of the feature space around them as clusters.\n\n### 5. K-Means\nK-Means Clustering may be the most widely known clustering algorithm and involves assigning examples to clusters in an effort to minimize the variance within each cluster.\n\n### 6. Mini-Batch K-Means\nMini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise.\n\n### 7. Mean Shift\nMean shift clustering involves finding and adapting centroids based on the density of examples in the feature space.\n\n### 8. OPTICS\nOPTICS clustering (where OPTICS is short for Ordering Points To Identify the Clustering Structure) is a modified version of DBSCAN described above.\n\n### 9. Spectral Clustering\nSpectral Clustering is a general class of clustering methods, drawn from linear algebra.\n\n### 10. Gaussian Mixture Model\nA Gaussian mixture model summarizes a multivariate probability density function with a mixture of Gaussian probability distributions as its name suggests.\n\n![](https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_cluster_comparison_0011.png)\n\nRef: https:\/\/machinelearningmastery.com\/clustering-algorithms-with-python\/\nRef: https:\/\/scikit-learn.org\/stable\/modules\/clustering.html\n","e55c3db6":"***There are 167 rows and 10 columns(features)***","ca8ace6e":"***Fortunately there is no null value identified.***","2367a374":"# What is Clustering\nCluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense) to each other than to those in other groups (clusters). It is a main task of exploratory data analysis, and a common technique for statistical data analysis, used in many fields, including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. \n\nClustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group and dissimilar to the data points in other groups. It is basically a collection of objects on the basis of similarity and dissimilarity between them.\n\n![](https:\/\/media.geeksforgeeks.org\/wp-content\/uploads\/merge3cluster.jpg)\n\nRef: Wikipedia & https:\/\/www.geeksforgeeks.org\/clustering-in-machine-learning\/\n","18eb358a":"***Observations***:\n1. Child_mort has negative relationship with GDP as the child mortality is less the GDP also increases and vice versa.\n2. Export, Income, Income has clear postivite relationship with GDP. \n3. Total_fer and child_mort has postive relationship. \n4. total_fer and life_expec has negative relationship. \n5. life_expec and childe_mort has negative relationship."}}