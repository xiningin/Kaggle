{"cell_type":{"3f05f246":"code","bff8bc8e":"code","cd649383":"code","963a87cc":"code","e7c73804":"code","1ca8babe":"code","cd3c2848":"code","20e2f5de":"code","0c699a5e":"code","46f35374":"code","0d206d65":"code","0a115f6a":"code","eac50f6b":"code","7d94fdd2":"code","13058493":"code","219674dc":"code","262e814e":"code","2a50599a":"code","579a4235":"code","7853ef39":"code","d559770c":"code","c8a9661d":"code","1247b265":"code","5cadcd31":"code","c2465259":"code","61542762":"code","c2c512a5":"code","e4a226f3":"code","f131bacc":"code","782c97e5":"code","0632dedd":"code","181c3aac":"code","4154fd94":"code","054e6679":"code","8bafe022":"code","d5f96417":"code","b036f167":"code","4ad95b49":"code","8bad8400":"code","3a64396c":"code","3dcb3090":"code","8db22f07":"code","0c382ef4":"code","d75ed31b":"code","9907eb12":"code","3035e51a":"code","38683bc6":"code","591b2f91":"code","33dde2e9":"code","f7a588a4":"code","a76057b2":"code","d2a0e7ff":"code","37ed4830":"code","3422a1ff":"code","13378c3e":"code","c4d22480":"code","01aa6dbe":"code","e57f3f82":"code","c0142620":"code","3a8b3b34":"code","f5a124f0":"code","e5f53338":"code","0081f7dc":"code","cd1acbee":"code","65074b2c":"code","12c47eae":"code","b0f9ea5c":"code","a3b82c78":"code","2db5621c":"code","226eb97d":"code","63484da5":"code","603e678a":"code","c9f8df2d":"code","9ace69b8":"code","8db9d937":"code","d98f67b2":"code","eded3d7f":"code","713d583e":"code","6a314ea7":"code","edb061bb":"code","e1c65213":"code","01156182":"code","6a6dd122":"code","4f1eb973":"code","327b6937":"code","2a901ffb":"code","29e5aabe":"code","b9efad5f":"code","bfb502be":"code","0f22399d":"code","59228846":"code","2e01b0bb":"code","d4850831":"code","41a0a5db":"markdown","3564bbe1":"markdown","d255a43b":"markdown","5b052879":"markdown","9666e7df":"markdown","119e9cb6":"markdown","471671c8":"markdown","b4b39a8e":"markdown","150eb99b":"markdown","25afbdb1":"markdown","b31ffccb":"markdown","0fbf0e79":"markdown","8bc29195":"markdown","86218ab3":"markdown","4ebeee61":"markdown","62012d73":"markdown"},"source":{"3f05f246":"#Import all necessery libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy import stats\n\n# Statistics\nfrom scipy.stats import norm\nfrom scipy import stats\n\n\n# Preprocessing\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import LabelEncoder\n\n#ML\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","bff8bc8e":"# read dataset\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n","cd649383":"# Description File\nwith open('..\/input\/house-prices-advanced-regression-techniques\/data_description.txt', encoding='utf8') as f:\n    for line in f:\n        print(line.strip())","963a87cc":"train.head()","e7c73804":"train.shape","1ca8babe":"train.info()","cd3c2848":"# How many unique variables does each column contain?\npd.options.display.max_rows = 100\ntrain.nunique().sort_values(ascending = False)","20e2f5de":"plt.figure(figsize=(15,8))\nsns.heatmap(train.isnull(), cbar = False, cmap=\"gray\")","0c699a5e":"missing_val= pd.DataFrame(train.isnull().sum()[train.isnull().sum()!=0]\\\n                          .sort_values(ascending = False)).rename(columns = {0:'num_miss'})\nmissing_val['missing_perc'] = (missing_val\/train.shape[0]*100).round(1)\nmissing_val = missing_val.query('missing_perc > 40')","46f35374":"missing_val","0d206d65":"drop_cols = missing_val.index.to_list()\ndrop_cols","0a115f6a":"train.drop(['Id'],axis=1,inplace=True)\ntrain.drop(columns=drop_cols,axis=1,inplace=True)","eac50f6b":"train.shape","7d94fdd2":"num_cols = train.select_dtypes(include=['number'])\ncat_cols = train.select_dtypes(include=['object'])\n\nprint(f'The dataset contains {len(num_cols.columns.tolist())} numerical columns \\\nand {len(cat_cols.columns.tolist())} categorical columns')","13058493":"num_cols.head()","219674dc":"num_cols.describe()","262e814e":"cat_cols.head()","2a50599a":"cat_cols.describe()","579a4235":"num_corr_price = num_cols.corr()['SalePrice'][:-1]","7853ef39":"#correlation with the target variable\nnum_corr_price","d559770c":"best_features = num_corr_price[abs(num_corr_price) > 0.35].sort_values(ascending=False)\nprint(\"There are {} strongly correlated numerical features with SalePrice:\\n{}\"\\\n      .format(len(best_features), best_features))","c8a9661d":"for feature in best_features.index:\n    num_corr_price.drop(feature,inplace = True)","1247b265":"for feature in num_corr_price.index:\n    train.drop(feature,axis = 1,inplace = True)\n    num_cols.drop(feature,axis = 1,inplace = True)","5cadcd31":"train.shape","c2465259":"num_corr = num_cols.corr()\ncorr_triu = num_corr.where(np.triu(np.ones(num_corr.shape), k=1).astype(np.bool))\n\nplt.figure(figsize=(10,10))\nsns.heatmap(num_corr,annot=True, square=True, fmt='.2f',\\\n            annot_kws={'size':9}, mask = np.triu(corr_triu), cmap= \"coolwarm\")","61542762":"corr_triu_collinear = corr_triu.iloc[:-1,:-1]","c2c512a5":"collinear_features = [column for column in corr_triu_collinear.columns if any(corr_triu_collinear[column] > 0.60)]\ntrain.drop(columns = collinear_features,inplace=True)\nnum_cols.drop(columns = collinear_features,inplace=True)","e4a226f3":"train.shape","f131bacc":"plt.figure(figsize=(10,10))\nsns.heatmap(num_cols.corr(),annot=True, square=True, fmt='.2f',\\\n            annot_kws={'size':9}, mask = np.triu(num_cols.corr()), cmap= \"coolwarm\")","782c97e5":"num_cols.isna().sum()","0632dedd":"num_cols['LotFrontage'].hist(bins = 40)","181c3aac":"num_cols['LotFrontage'].describe()","4154fd94":"train['LotFrontage'].fillna(np.random.randint(59,80), inplace = True)\ntrain['LotFrontage'].isna().sum()","054e6679":"#MasVnrArea: Masonry veneer area in square feet --> let's fill missing values with median (0)\nnum_cols.MasVnrArea.hist(bins = 50)","8bafe022":"num_cols.MasVnrArea.fillna(0, inplace = True)","d5f96417":"print('Number of features left in numerical features:',len(num_cols.columns))\nprint('Numerical Features left:')\nprint(num_cols.columns.values)","b036f167":"for i in range(0, len(num_cols.columns), 5):\n    plt.figure(figsize=(15,15))\n    sns.pairplot(data=num_cols, x_vars=num_cols.columns[i:i+5], y_vars=['SalePrice'])","4ad95b49":"train = train.drop(train.LotFrontage.sort_values(ascending = False)[:2].index) \ntrain = train.drop(train.BsmtFinSF1.sort_values(ascending = False)[:1].index)\ntrain = train.drop(train.MasVnrArea.sort_values(ascending = False)[:1].index)\ntrain = train.drop(train.TotalBsmtSF.sort_values(ascending = False)[:1].index)\ntrain = train.drop(train.GrLivArea.sort_values(ascending = False)[:2].index)","8bad8400":"train.reset_index(drop=True,inplace=True)","3a64396c":"train.shape","3dcb3090":"# plt.title(f'Untransformed SalePrice, Skew: {stats.skew(train.SalePrice):.3f}')\n# sns.distplot(train.SalePrice,fit=norm)\n# plt.axvline(train.SalePrice.mode().to_numpy(), linestyle='--', color='green', label='mode')\n# plt.axvline(train.SalePrice.median(), linestyle='--', color='blue', label='median')\n# plt.axvline(train.SalePrice.mean(), linestyle='--', color='red', label='mean')\n# plt.grid(alpha = 0.3)\n# plt.legend()","8db22f07":"# train.SalePrice.describe()","0c382ef4":"# train['SalePrice'] = np.log(train['SalePrice'])","d75ed31b":"# plt.title(f'Transformed SalePrice, Skew: {stats.skew(train.SalePrice):.3f}')\n# sns.distplot(train.SalePrice,fit=norm)\n# plt.axvline(train.SalePrice.mode().to_numpy(), linestyle='--', color='green', label='mode')\n# plt.axvline(train.SalePrice.median(), linestyle='--', color='blue', label='median')\n# plt.axvline(train.SalePrice.mean(), linestyle='--', color='red', label='mean')\n# plt.grid(alpha = 0.3)\n# plt.legend()","9907eb12":"cat_cols_missing = cat_cols.columns[cat_cols.isnull().any()]\ncat_cols_missing","3035e51a":"imputer = SimpleImputer(missing_values = np.NaN,strategy = 'most_frequent')\nfor feature in cat_cols_missing:\n     cat_cols[feature] = imputer.fit_transform(cat_cols[feature].values.reshape(-1,1))\n     train[feature] = imputer.fit_transform(train[feature].values.reshape(-1,1))","38683bc6":"cat_cols.nunique().sort_values(ascending = False)","591b2f91":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\nfor feature in cat_cols.columns:\n    cat_cols[feature]=le.fit_transform(cat_cols[feature])\n    train[feature]=le.fit_transform(train[feature])","33dde2e9":"plt.figure(figsize=(15,15))\nsns.heatmap(cat_cols.corr(), square=True, mask = np.triu(cat_cols.corr()), cmap= \"coolwarm\")","f7a588a4":"cat_corr = cat_cols.corr()\ncat_corr_triu = cat_corr.where(np.triu(np.ones(cat_corr.shape), k=1).astype(np.bool))\n\ncat_collinear_features = [column for column in cat_corr_triu.columns if any(cat_corr_triu[column] > 0.60)]\ntrain.drop(columns = cat_collinear_features,inplace=True)\ncat_cols.drop(columns = cat_collinear_features,inplace=True)","a76057b2":"train.head()","d2a0e7ff":"train.replace([np.inf, -np.inf], np.nan)\ntrain.isna().sum().sort_values(ascending = False)","37ed4830":"train.MasVnrArea.describe()","3422a1ff":"train.MasVnrArea.fillna(0, inplace = True)","13378c3e":"y = train['SalePrice']\nX = train.iloc[:,:-1]","c4d22480":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","01aa6dbe":"linreg = LinearRegression()\nlinreg.fit(X_train, y_train)","e57f3f82":"print(\"Training set score: {:.2f}\".format(linreg.score(X_train, y_train))) \nprint(\"Test set score: {:.2f}\".format(linreg.score(X_test, y_test)))","c0142620":"# print the intercept\nprint(linreg.intercept_)","3a8b3b34":"coeff_df = pd.DataFrame(linreg.coef_, X.columns, columns=['Coefficient'])\ncoeff_df","f5a124f0":"pred = linreg.predict(X_test)","e5f53338":"!pip install hvplot\nimport hvplot.pandas\n\npd.DataFrame({'True Values': y_test, 'Predicted Values': pred}).hvplot.scatter(x='True Values', y='Predicted Values')","0081f7dc":"pd.DataFrame({'Error Values': (y_test - pred)}).hvplot.kde()","cd1acbee":"from sklearn import metrics\n\ndef print_evaluate(true, predicted):  \n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    print('MAE:', mae)\n    print('MSE:', mse)\n    print('RMSE:', rmse)\n    print('R2 Square', r2_square)\n    print('__________________________________')","65074b2c":"test_pred = linreg.predict(X_test)\ntrain_pred = linreg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","12c47eae":"#Robust Regression\n\nfrom sklearn.linear_model import RANSACRegressor\n\nmodel = RANSACRegressor(base_estimator=LinearRegression(), max_trials=100)\nmodel.fit(X_train, y_train)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","b0f9ea5c":"#Elastic Net\n\nfrom sklearn.linear_model import ElasticNet\n\nmodel = ElasticNet(alpha=0.1, l1_ratio=0.9, selection='random', random_state=42)\nmodel.fit(X_train, y_train)\n\ntest_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","a3b82c78":"# Stochastic Gradient Descent\n\nfrom sklearn.linear_model import SGDRegressor\n\nsgd_reg = SGDRegressor(n_iter_no_change=250, penalty=None, eta0=0.0001, max_iter=100000)\nsgd_reg.fit(X_train, y_train)\n\ntest_pred = sgd_reg.predict(X_test)\ntrain_pred = sgd_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\nprint('====================================')\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","2db5621c":"# Artficial Neural Network\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Dense, Activation, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\nX_train = np.array(X_train)\nX_test = np.array(X_test)\ny_train = np.array(y_train)\ny_test = np.array(y_test)\n\nmodel = Sequential()\n\nmodel.add(Dense(X_train.shape[1], activation='relu'))\nmodel.add(Dense(32, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(64, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(128, activation='relu'))\n# model.add(Dropout(0.2))\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1))\n\nmodel.compile(optimizer=Adam(0.00001), loss='mse')\n\nr = model.fit(X_train, y_train,\n              validation_data=(X_test,y_test),\n              batch_size=1,\n              epochs=100)","226eb97d":"pd.DataFrame(r.history).hvplot.line(y=['loss', 'val_loss'])","63484da5":"test_pred = model.predict(X_test)\ntrain_pred = model.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","603e678a":"# Random Forest\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf_reg = RandomForestRegressor(n_estimators=1000)\nrf_reg.fit(X_train, y_train)\n\ntest_pred = rf_reg.predict(X_test)\ntrain_pred = rf_reg.predict(X_train)\n\nprint('Test set evaluation:\\n_____________________________________')\nprint_evaluate(y_test, test_pred)\n\nprint('Train set evaluation:\\n_____________________________________')\nprint_evaluate(y_train, train_pred)","c9f8df2d":"test.shape","9ace69b8":"train.shape","8db9d937":"for col in test.columns:\n    if col not in train.columns:\n        test.drop(col, axis = 1, inplace = True)","d98f67b2":"test.shape","eded3d7f":"test.isna().sum().sort_values(ascending = False)","713d583e":"num_cols_test = test.select_dtypes(include=['number'])\ncat_cols_test = test.select_dtypes(include=['object'])","6a314ea7":"num_cols_test.isna().sum()","edb061bb":"num_cols_test['LotFrontage'].describe()","e1c65213":"test['LotFrontage'].fillna(np.random.randint(58,80), inplace = True)\ntest['LotFrontage'].isna().sum()","01156182":"test['MasVnrArea'].fillna(test.MasVnrArea.median(), inplace = True)\ntest['BsmtFinSF1'].fillna(test.BsmtFinSF1.median(), inplace = True)\ntest['TotalBsmtSF'].fillna(test.TotalBsmtSF.median(), inplace = True)","6a6dd122":"cat_cols_missing_test = cat_cols_test.columns[cat_cols_test.isnull().any()]","4f1eb973":"imputer = SimpleImputer(missing_values = np.NaN,strategy = 'most_frequent')\nfor feature in cat_cols_missing_test:\n     cat_cols_test[feature] = imputer.fit_transform(cat_cols_test[feature].values.reshape(-1,1))\n     test[feature] = imputer.fit_transform(test[feature].values.reshape(-1,1))","327b6937":"for feature in cat_cols_test.columns:\n    cat_cols_test[feature]=le.fit_transform(cat_cols_test[feature])\n    test[feature]=le.fit_transform(test[feature])","2a901ffb":"test.isna().sum().sort_values()","29e5aabe":"pred_y = linreg.predict(test)","b9efad5f":"pred_y","bfb502be":"sample = pd.DataFrame()","0f22399d":"sample['Id'] = range(1461,2920)","59228846":"sample['SalePrice'] = pred_y","2e01b0bb":"sample","d4850831":"sample.to_csv('my_subs.csv')","41a0a5db":"# Please, if you have any questions or remarks, comment this code!:) I'll be glad to answer your questions!)","3564bbe1":"# Getting submissions","d255a43b":"For our model we will choose only those variables, which correlation to the target variable is **more than 0.35**","5b052879":"# Hi! :)\nThis Notebook was created by a beginner in ML, inspired by the codes presented here that were created by more proficient programmers:)\n\n**Ok, let's start!**","9666e7df":"So, \n* 25%        59.000000\n* 75%        80.000000\n\nTherefore, we will fill the gaps with random int between 59 and 80","119e9cb6":"* Next let's explore features more precisely. We need to divide the dataset according to the types of variables into two groups: **numerical features** and **categorical features**. We will explore them *separately*.","471671c8":"We should fill the gaps in \"LotFrontage\"","b4b39a8e":"* As you see, the **target variable 'SalePrice' is in the last column**.\n* In general there are **81 features** and **1460 samples**.\n* There a **3 types of variables**: *float64(3), int64(35), object(43)* in the dataset.\n\n\nNow look at **missing values** in each column\n* Let's work with **NaNs** in the dataset.\n* We have to get rid of columns with **more than 40%** of missing values.\n* If a column contain less than 40% of missing values, let's fill missing cells with mean values.\n","150eb99b":"# Correlation","25afbdb1":"Outliers may influence the output of the model. We should vanish them.","b31ffccb":"* The target variable SalePrice is **right-skewed**.\n* The **mean is biased towards a higher price than the median**.\n* Therefore, we have to transform the target variable by using **log function**.","0fbf0e79":"Ok, we should drop columns **['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu']**\n\nBesides, the column **ID** is meaningless for our model. We'll drop it too.","8bc29195":"Much better!\nNow, let's turn to categorical features.","86218ab3":"How is the target variable distributed?","4ebeee61":"Besides, we should get rid of correlated variables because they may worsen the output of the model. We will get rid of one of the correlated variables  in a pair where correlation > 0.80","62012d73":"# Linear Regression"}}