{"cell_type":{"b93c7f0a":"code","3df3b962":"code","b3957c90":"code","e58d4c2b":"code","f776cb22":"code","8513f7a7":"code","70d37cef":"code","6e484d33":"code","40bbc810":"code","e89be3f9":"code","097f6114":"code","99867fd2":"code","969aaa5a":"code","5214b07d":"code","b7e18e05":"code","8394f77c":"code","2f403102":"code","23b9137c":"markdown","5bf2a3b9":"markdown","bc66f2b8":"markdown","8bd5a948":"markdown","c31c21b5":"markdown","f2e61c37":"markdown","c1cd8e8a":"markdown","83108fc6":"markdown","03c16b50":"markdown"},"source":{"b93c7f0a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\npath = \"..\/input\/\"\nprint(os.listdir(path))","3df3b962":"from sklearn.metrics import roc_auc_score\nimport os\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import KMeans","b3957c90":"%time\nTrain_df = pd.read_csv(path + 'train.csv')\nTrain_df.head()","e58d4c2b":"%time\nTest_df = pd.read_csv(path + 'test.csv')\nTest_df.head()","f776cb22":"n_clusters = 5\ntest_size = 0.3","8513f7a7":"y_df = Train_df['target']\nTrain_df.drop(columns=['target'], inplace=True)","70d37cef":"X_train, X_test, y_train, y_test = train_test_split(Train_df, y_df, test_size=test_size, random_state=40)","6e484d33":"len(X_train), len(y_train)","40bbc810":"columns = [i for i in X_train.columns if i not in ['ID_code']]","e89be3f9":"def augment(x,y,t=2):\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xs.append(x1)\n\n    for i in range(t\/\/2):\n        mask = y==0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y","097f6114":"kmeans = KMeans(n_clusters=n_clusters, max_iter=1000).fit(X_train[columns])\nX_train[\"clusters\"] = kmeans.labels_","99867fd2":"cluster_idxs = [X_train[\"clusters\"] == i for i in range(5)]","969aaa5a":"import lightgbm as lgb\nfrom sklearn.model_selection import KFold, StratifiedKFold","5214b07d":"param = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.1,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.01,\n    'max_depth': -1,  \n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 4,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': 1,\n    'max_bin': 50,\n}","b7e18e05":"predictors = []\n\nfor i in range(n_clusters):\n    \n    print(\"TRAINING MODEL FOR CLUSTER: {}\".format(i))\n    x_i = X_train[cluster_idxs[i]]\n    y_i = y_train[cluster_idxs[i]]\n\n    num_folds = 3\n    features = [c for c in x_i.columns if c not in ['ID_code', 'clusters', 'target']]\n    folds = KFold(n_splits=num_folds, random_state=44000)\n\n    x_i = x_i[features]\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(x_i.values, y_i.values)):\n\n        X_trai, y_trai = x_i.iloc[trn_idx][features], y_i.iloc[trn_idx]\n        X_val, y_val = x_i.iloc[val_idx][features], y_i.iloc[val_idx]\n\n        X_trai, y_trai = augment(X_trai.values, y_trai.values)\n        X_trai = pd.DataFrame(X_trai)\n\n        print(\"Fold idx:{}\".format(fold_ + 1))\n        trn_data = lgb.Dataset(X_trai, label=y_trai)\n        val_data = lgb.Dataset(X_val, label=y_val)\n\n        clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=5000, early_stopping_rounds = 3000)\n        predictors.append(clf)","8394f77c":"def get_predictions(kmeans, X_test, n_clusters, get_score=True):\n    features = [c for c in X_test.columns if c not in ['ID_code', 'target']]\n    X_test['cluster'] = kmeans.predict(X_test[features])\n    test_idxs = [X_test['cluster'] == i for i in range(n_clusters)]\n    X_test.drop(columns=['cluster'], inplace=True)\n    preds = []\n    true = []\n\n    for i in range(n_clusters):\n        x_te = X_test[test_idxs[i]]\n        pred = predictors[i].predict(x_te[features])\n        preds.append(pred)\n        if get_score:\n            y_te = y_test[test_idxs[i]]\n            true.append(y_te.values)\n    x = []\n    y = []\n    for i in preds:\n        x = x + list(i)\n    if get_score:\n        for i in true:\n            y = y + list(i)\n        print(roc_auc_score( np.array(y), np.array(x)))\n    \n    return np.array(x)\n    ","2f403102":"get_predictions(kmeans, X_test, n_clusters = 5, get_score=True)","23b9137c":"## To enhance the pipeline:\n- Train the model on the entire dataset, skipping the test set\n- Use different clustering algorithms, (eg: kernel k-means)\n- Try different numeber of clusters\n- Try other models for each cluster","5bf2a3b9":"## Kmeans clustering of the data into n_clusters. Data is augmented with a column called 'clusters'.\n\nTo get cluster of a new data point x, use cluster = kmeans.predict(x).","bc66f2b8":"### AUC score on our sampled test set","8bd5a948":"###  Predictors[i] contains model for cluster i","c31c21b5":"## In this Kernel, I explore preprocessing the data by splitting it into clusters. \nThe approach is to split the data into K clusters and train a LGB model on each. While testing we first assign the test data point to the nearest cluster, then predict based on the model of the cluster. This kernel depicts the basic pipeline. We employ k-means clustering. Advanced clustering techniques  (like kernel k-means) can be tried to get a higher score.","f2e61c37":"### We augment the dataset to oversample the positive examples to deal with class imbalance.\n\nhttps:\/\/www.kaggle.com\/jesucristo\/santander-magic-lgb-0-901\n\nThe difference here is we augnent each cluster rather than the entire dataset.\n","c1cd8e8a":"### Get the predictions on the Test sets:","83108fc6":"## cluster_idxs is a list containing indices of data belonging to a cluster.\n\nFor eg: for getting data of cluster i use X_train[cluster_idxs[i]]","03c16b50":"### Splitting train_df into train and test sets (X_train, X_test, Y_train, Y_test)"}}