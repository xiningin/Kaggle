{"cell_type":{"95910f81":"code","e6b46077":"code","7c5db885":"code","7d395da9":"code","f854cb24":"code","9e7d15b7":"code","29917f66":"code","40d49d56":"code","2a65e4e1":"code","8bdf6a4e":"code","9eb44d26":"code","5b469c09":"code","6529db13":"code","7b5ff430":"code","edbf4b2f":"code","3c4ca3a4":"code","4bd69ab8":"code","6d78e0f4":"code","f5cdf24a":"code","eeb904e5":"code","6c8b144e":"code","c02b5db7":"markdown","548b47c6":"markdown","05892a12":"markdown","76b6549a":"markdown","2763d838":"markdown","db63ac85":"markdown","ff691355":"markdown","722b4d9d":"markdown","e4881b3d":"markdown","e0f0cb3f":"markdown","72359f2f":"markdown","51ea4d38":"markdown","70d4d594":"markdown","fededc06":"markdown","349739ad":"markdown","1f37f169":"markdown","ef1fd75d":"markdown","8595213c":"markdown","4bafa965":"markdown","3e60e07a":"markdown","c0c1ecd1":"markdown","1a453170":"markdown","1ce353e8":"markdown","297e7692":"markdown","a146377c":"markdown","52090cd1":"markdown"},"source":{"95910f81":"import pandas as pd\nimport os\ndata = pd.read_csv(\"..\/input\/predicting-a-pulsar-star\/pulsar_stars.csv\")\nprint(data.shape)\ndata.head()","e6b46077":"import seaborn as sns\n\nsns.pairplot(data, hue='target_class')\n","7c5db885":"sns.heatmap(data.corr(), annot=True)","7d395da9":"y = data.target_class.values\nx_data = data.drop(['target_class'], axis=1)","f854cb24":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x_data, y, test_size=0.15, random_state=42)\n\nfrom sklearn.preprocessing import StandardScaler  \nscaler = StandardScaler()  \nscaler.fit(x_train)  \nX_train = scaler.transform(x_train)  \nX_test = scaler.transform(x_test)\nprint(X_train)","9e7d15b7":"from sklearn import linear_model #import the model library\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter= 500,solver='lbfgs') # sitting model parameters\nprint(\"test accuracy: {} \".format(logreg.fit(x_train, y_train).score(x_test, y_test))) # printing the results of fitting the model over the testing set\nprint(\"train accuracy: {} \".format(logreg.fit(x_train, y_train).score(x_train, y_train))) # printing the results of fitting the model over the training set","29917f66":"from sklearn.neighbors import KNeighborsClassifier #import the model library\nneigh = KNeighborsClassifier(n_neighbors=3) # sitting model parameters\nprint(\"test accuracy: {} \".format(neigh.fit(x_train, y_train).score(x_test, y_test))) # printing the results of fitting the model over the testing set\nprint(\"train accuracy: {} \".format(neigh.fit(x_train, y_train).score(x_train, y_train))) # printing the results of fitting the model over the training set","40d49d56":"from sklearn import tree #import the model library\ndt = tree.DecisionTreeClassifier() # sitting model\nprint(\"test accuracy: {} \".format(dt.fit(x_train, y_train).score(x_test, y_test))) # printing the results of fitting the model over the testing set\nprint(\"train accuracy: {} \".format(dt.fit(x_train, y_train).score(x_train, y_train))) # printing the results of fitting the model over the training set","2a65e4e1":"from sklearn import svm #import the model library\nsvm = svm.SVC(gamma='scale') # sitting model parameters\nprint(\"test accuracy: {} \".format(svm.fit(x_train, y_train).score(x_test, y_test))) # printing the results of fitting the model over the testing set\nprint(\"train accuracy: {} \".format(svm.fit(x_train, y_train).score(x_train, y_train))) # printing the results of fitting the model over the training set","8bdf6a4e":"from sklearn.neural_network import MLPClassifier #import the model library\nsnn = MLPClassifier(solver='lbfgs', alpha=1e-2,hidden_layer_sizes=(8, 8), random_state=1) # sitting model parameters\nprint(\"test accuracy: {} \".format(snn.fit(x_train, y_train).score(x_test, y_test))) # printing the results of fitting the model over the testing set\nprint(\"train accuracy: {} \".format(snn.fit(x_train, y_train).score(x_train, y_train))) # printing the results of fitting the model over the training set","9eb44d26":"from sklearn.naive_bayes import GaussianNB #import the model library\ngnb = GaussianNB() # sitting model\nprint(\"test accuracy: {} \".format(gnb.fit(x_train, y_train).score(x_test, y_test))) # printing the results of fitting the model over the testing set\nprint(\"train accuracy: {} \".format(gnb.fit(x_train, y_train).score(x_train, y_train))) # printing the results of fitting the model over the training set","5b469c09":"from sklearn.ensemble import BaggingClassifier #import the model library\nfrom sklearn.neighbors import KNeighborsClassifier #import the model library\nbagging = BaggingClassifier(KNeighborsClassifier(),max_samples=0.5, max_features=0.5) # sitting model parameters\nprint(\"test accuracy: {} \".format(bagging.fit(x_train, y_train).score(x_test, y_test))) # printing the results of fitting the model over the testing set\nprint(\"train accuracy: {} \".format(bagging.fit(x_train, y_train).score(x_train, y_train))) # printing the results of fitting the model over the training set","6529db13":"from sklearn.ensemble import RandomForestClassifier #import the model library\nrf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0) # sitting model parameters\nprint(\"test accuracy: {} \".format(rf.fit(x_train, y_train).score(x_test, y_test))) # printing the results of fitting the model over the testing set\nprint(\"train accuracy: {} \".format(rf.fit(x_train, y_train).score(x_train, y_train))) # printing the results of fitting the model over the training set","7b5ff430":"from sklearn.ensemble import AdaBoostClassifier #import the model library\nadab = AdaBoostClassifier(n_estimators=100) # sitting model parameters\nprint(\"test accuracy: {} \".format(adab.fit(x_train, y_train).score(x_test, y_test))) # printing the results of fitting the model over the testing set\nprint(\"train accuracy: {} \".format(adab.fit(x_train, y_train).score(x_train, y_train))) # printing the results of fitting the model over the training set","edbf4b2f":"from sklearn.ensemble import GradientBoostingClassifier #import the model library\ngbc = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0) # sitting model parameters\nprint(\"test accuracy: {} \".format(gbc.fit(x_train, y_train).score(x_test, y_test))) # printing the results of fitting the model over the testing set\nprint(\"train accuracy: {} \".format(gbc.fit(x_train, y_train).score(x_train, y_train))) # printing the results of fitting the model over the training set","3c4ca3a4":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\n\nclf1 = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state=1,max_iter= 500)\nclf2 = RandomForestClassifier(n_estimators=50, random_state=1)\nclf3 = GaussianNB()\n\neclf = VotingClassifier(estimators=[('lr', clf1), ('rf', clf2), ('gnb', clf3)], voting='hard')\nfor clf, label in zip([clf1, clf2, clf3, eclf], ['Logistic Regression', 'Random Forest', 'naive Bayes', 'Ensemble']):\n    scores = cross_val_score(clf, x_train, y_train, cv=5, scoring='accuracy')\n    print(\"Accuracy: %0.2f (+\/- %0.2f) [%s]\" % (scores.mean(), scores.std(), label))","4bd69ab8":"from sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom vecstack import stacking\n\nmodels = [KNeighborsClassifier(n_neighbors=5,n_jobs=-1),\n          RandomForestClassifier(random_state=0, n_jobs=-1,n_estimators=100, max_depth=3),\n          XGBClassifier(random_state=0, n_jobs=-1, learning_rate=0.1,n_estimators=100, max_depth=3)\n]\nS_train, S_test = stacking(models,                   \n                           X_train, y_train, X_test,   \n                           regression=False, \n                           mode='oof_pred_bag', \n                           needs_proba=False,\n                           save_dir=None,\n                           metric=accuracy_score, \n                           n_folds=4, \n                           stratified=True,\n                           shuffle=True,  \n                           random_state=0, \n                           verbose=2)\nmodel = XGBClassifier(random_state=0, n_jobs=-1, learning_rate=0.1, \n                      n_estimators=100, max_depth=3)\nprint(\"test accuracy: {} \".format(model.fit(S_train, y_train).score(S_test, y_test)))\nprint(\"train accuracy: {} \".format(model.fit(S_train, y_train).score(S_train, y_train)))","6d78e0f4":"from keras import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.layers import Flatten\nclassifier = Sequential()\nclassifier.add(Dense(8, activation='relu', kernel_initializer='random_normal',input_dim=8))\nclassifier.add(Dropout(0.2))\nclassifier.add(Dense(16, activation='relu', kernel_initializer='random_normal'))\nclassifier.add(Dense(16, activation='relu', kernel_initializer='random_normal'))\nclassifier.add(Dense(16, activation='relu', kernel_initializer='random_normal'))\nclassifier.add(Dense(16, activation='relu', kernel_initializer='random_normal'))\nclassifier.add(Dropout(0.2))\nclassifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n\nclassifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\nclassifier.fit(X_train,y_train, batch_size=100, epochs=10)\ntest_loss, test_acc = model.evaluate(X_test, y_test)\n","f5cdf24a":"import tensorflow as tf\nfrom tensorflow import keras\n\nmodel = keras.Sequential([\n    keras.layers.Flatten(input_shape=(8,)),\n    keras.layers.Dense(16, activation=tf.nn.relu),\n\tkeras.layers.Dense(16, activation=tf.nn.relu),\n    keras.layers.Dense(1, activation=tf.nn.sigmoid),\n])\n\nmodel.compile(optimizer='SGD',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(X_train, y_train, epochs=10, batch_size=100)\n\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint('Test accuracy:', test_acc)","eeb904e5":"\n\ny_train= y_train.reshape((15213, 1))\ny_test= y_test.reshape((2685, 1))\nprint(x_train.T.shape)\nprint(y_train.T.shape)\nx_train=x_train.T\ny_train=y_train.T","6c8b144e":"import numpy as np\nimport h5py\nimport matplotlib.pyplot as plt\n%matplotlib inline\nlayers_dims =np.array([8,16,16,1])\n\ndef initialize_parameters(layer_dims):\n    parameters = {}\n    L = len(layer_dims)           \n    for l in range(1, L):\n        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n    return parameters  \n\ndef linear_forward(A, W, b):\n    Z = np.dot(W,A)+b\n    cache = (A, W, b)\n    return Z, cache\n\ndef sigmoid(z):\n    A = 1\/(1 + np.exp(-z))\n    activation_cache = A.copy()\n    return A, activation_cache\n    \ndef relu(z):\n    A = z*(z > 0)\n    activation_cache = z\n    return A, activation_cache\n\n    \ndef linear_activation_forward(A_prev, W, b, activation):\n    if activation == \"sigmoid\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = sigmoid(Z)\n    elif activation == \"relu\":\n        Z, linear_cache = linear_forward(A_prev, W, b)\n        A, activation_cache = relu(Z)\n    cache = (linear_cache, activation_cache)\n    return A, cache\n\ndef L_model_forward(X, parameters):\n    caches = []\n    A = X\n    L = len(parameters) \/\/ 2                  \n    for l in range(1, L):\n        A_prev = A \n        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n        caches.append(cache)\n    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n    caches.append(cache)\n    return AL, caches\n\ndef compute_cost(AL, Y):\n    m = Y.shape[1]\n    cost = (-1\/m)*np.sum(np.multiply(Y,np.log(AL))+np.multiply((1-Y),np.log(1-AL)))\n    cost = np.squeeze(cost)      \n    return cost\n\ndef linear_backward(dZ, cache):\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n    dW = (1\/m)*np.dot(dZ,A_prev.T)\n    db = (1\/m)*np.sum(dZ, axis=1, keepdims = True)\n    dA_prev = np.dot(W.T,dZ)\n    return dA_prev, dW, db\n\ndef sigmoid_backward(dA, activation_cache):\n    return dA*(activation_cache*(1-activation_cache))\n\ndef relu_backward(dA, activation_cache):\n    return dA*(activation_cache > 0)\n    \ndef linear_activation_backward(dA, cache, activation):\n    linear_cache, activation_cache = cache\n    if activation == \"relu\":\n        dZ = relu_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)        \n    elif activation == \"sigmoid\":\n        dZ = sigmoid_backward(dA, activation_cache)\n        dA_prev, dW, db = linear_backward(dZ, linear_cache)    \n    return dA_prev, dW, db\n\ndef L_model_backward(AL, Y, caches):\n    grads = {}\n    L = len(caches) \n    m = AL.shape[1]\n    Y = Y.reshape(AL.shape)\n    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n    current_cache = caches[L-1]\n    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n    for l in reversed(range(L-1)):\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, \"relu\")\n        grads[\"dA\" + str(l)] = dA_prev_temp\n        grads[\"dW\" + str(l + 1)] = dW_temp\n        grads[\"db\" + str(l + 1)] = db_temp\n    return grads\n\ndef update_parameters(parameters, grads, learning_rate):\n    L = len(parameters) \/\/ 2 \n    for l in range(L):\n        parameters[\"W\" + str(l+1)] = parameters[\"W\"+str(l+1)]-learning_rate*grads[\"dW\"+str(l+1)]\n        parameters[\"b\" + str(l+1)] = parameters[\"b\"+str(l+1)]-learning_rate*grads[\"db\"+str(l+1)]\n    return parameters\n\ndef L_layer_model(X, Y, layers_dims, learning_rate = 0.01, num_iterations = 3000, print_cost=False):\n    costs = []                         \n    parameters = initialize_parameters(layers_dims)\n    for i in range(0, num_iterations):\n        AL, caches = L_model_forward(X, parameters)\n        cost = compute_cost(AL, Y)\n        grads = L_model_backward(AL, Y, caches)\n        parameters = update_parameters(parameters, grads, learning_rate)\n        if print_cost and i % 100 == 0:\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per hundreds)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters\n\ndef predict(X, y, parameters): \n    m = X.shape[1]\n    p = np.zeros((1,m), dtype = np.int)\n    a3, caches = L_model_forward(X, parameters)\n    for i in range(0, a3.shape[1]):\n        if a3[0,i] > 0.5:\n            p[0,i] = 1\n        else:\n            p[0,i] = 0\n    print(str(np.mean((p[0,:] == y[0,:]))))\n    return p\n\nparameters = L_layer_model(x_train, y_train, layers_dims, num_iterations = 2500, print_cost = True)\npred_train = predict(x_train, y_train, parameters)\nprint(pred_train)\npred_test = predict(x_test.T, y_test.T, parameters)\nprint(pred_test)","c02b5db7":"**Gradient boosting**\n\n\nis a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.","548b47c6":"**importing dataset using pandas library**","05892a12":"**ensemble methods**\n\n\nits use multiple learning algorithms to obtain better predictive performance than could be obtained from any of the constituent learning algorithms alone. Unlike a statistical ensemble in statistical mechanics, which is usually infinite, a machine learning ensemble consists of only a concrete finite set of alternative models, but typically allows for much more flexible structure to exist among those alternatives.\n\n![](https:\/\/cdn-images-1.medium.com\/max\/1000\/0*c0Eg6-UArkslgviw.png)","76b6549a":"**A multilayer perceptron (MLP) **\n\n\nis a class of feedforward artificial neural network. An MLP consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training.[1][2] Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable.\n\n![](https:\/\/1.bp.blogspot.com\/-Xal8aZ5MDL8\/WlJm8dh1J9I\/AAAAAAAAAo4\/uCj6tt4T3T0HHUY4uexNuq2BXTUwcChqACLcBGAs\/s1600\/Multilayer-Perceptron.jpg)","2763d838":"**Decision tree learning** \n\n\nuses a decision tree (as a predictive model) to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. \n\n![](https:\/\/46gyn61z4i0t1u1pnq2bbk2e-wpengine.netdna-ssl.com\/wp-content\/uploads\/2018\/07\/what-is-a-decision-tree.png)","db63ac85":"**support-vector machines**\n\n\n(SVMs, also support-vector networks) are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier\n\n\n\n![](https:\/\/miro.medium.com\/max\/1200\/1*06GSco3ItM3gwW2scY6Tmg.png)","ff691355":"**Bagging**\n\nBootstrap aggregating, also called bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.\n\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/6b\/Bagging.png)","722b4d9d":"![](https:\/\/qph.fs.quoracdn.net\/main-qimg-d7f6c824d566e3f626fea25d01d39d13)","e4881b3d":"![](https:\/\/cdn.qubole.com\/wp-content\/uploads\/2018\/08\/1-400x387.png)","e0f0cb3f":"![](https:\/\/scikit-learn.org\/stable\/_static\/ml_map.png)","72359f2f":"* Artificial Intelligence (AI) -the broad discipline of creating intelligent machines\n* Machine Learning (ML) -refers to systems that can learn from experience\n* Deep Learning (DL) -refers to systems that learn from experience on large data sets","51ea4d38":"**DNN Build from scratch **","70d4d594":"**Logistic Regression**\n\n*Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression).*\n\n![](https:\/\/miro.medium.com\/max\/1200\/0*gKOV65tvGfY8SMem.png)","fededc06":"**here we will seperate data into X features and Y classes **","349739ad":"![](https:\/\/chrisalbon.com\/images\/machine_learning_flashcards\/AdaBoost_print.png)","1f37f169":"**Here by is this Kernal, I will show the most common Machine Learning approaches, the implementation will be established using mainly scikit learn library for Classical Machine Learning approaches and TensorFlow\/Keras for Deep Learning  **\n\n**FOR ANY QUESTIONS OR NEEDED EXPLANATIONS JUST COMMENT AND I WILL REPLY ASAP**","ef1fd75d":"\n\n**We plot the heatmap by using the correlation for the dataset. This helps us eliminate any features that may not help with prediction**\n\n","8595213c":"**Build it all using TENSORFLOW**","4bafa965":"**Stacking**\n\n\nis an ensemble learning technique to combine multiple classification models via a meta-classifier. The StackingCVClassifier extends the standard stacking algorithm (implemented as StackingClassifier ) using cross-validation to prepare the input data for the level-2 classifier.\n\n![](https:\/\/miro.medium.com\/max\/2044\/1*5O5_Men2op_sZsK6TTjD9g.png)","3e60e07a":"**scikit-learn**\n*Machine Learning Library in Python*\n* Simple and efficient tools for data mining and data analysis\n* Accessible to everybody, and reusable in various contexts\n* Built on NumPy, SciPy, and matplotlib\n* Open source, commercially usable - BSD license","c0c1ecd1":"**We will visualize the data for a better understanding**","1a453170":"**k-nearest neighbors algorithm (k-NN) \n**\n\nis a non-parametric method used for classification and regression. In both cases, the input consists of the k closest training examples in the feature space. The output depends on whether k-NN is used for classification or regression:\n\n* In k-NN classification, the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor.\n* In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.\n\n![](http:\/\/res.cloudinary.com\/dyd911kmh\/image\/upload\/f_auto,q_auto:best\/v1531424125\/KNN_final_a1mrv9.png)","1ce353e8":"**Example of Voting Classifier**","297e7692":"**Another way via TENSORFLOW**","a146377c":"**Random Forest**\n\n\nRandom forests or random decision forests are an ensemble learning method for classification, regression and other tasks that operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.Random decision forests correct for decision trees' habit of overfitting to their training set.\n\n![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*rLYBTFztVFOpAy7m3OtElg.jpeg)","52090cd1":"**na\u00efve Bayes classifiers**\n\n\nare a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (na\u00efve) independence assumptions between the features.\n\n\n![](https:\/\/miro.medium.com\/max\/1200\/1*ZW1icngckaSkivS0hXduIQ.jpeg)"}}