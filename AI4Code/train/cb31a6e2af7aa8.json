{"cell_type":{"48fd11dd":"code","0d1372be":"code","4ec96a1f":"code","1face6eb":"code","1590aeb6":"code","83afe0da":"code","67d7a31a":"code","0450690e":"code","7d78db45":"code","db310193":"code","5887d6b9":"code","a60f7f36":"code","c89aa22a":"code","9d44d727":"code","8e629e30":"code","19b1af0b":"code","54d03d79":"code","c18ae977":"code","32bfbd6e":"code","652a2f9b":"code","e0c3e985":"code","935d477b":"code","306db1b3":"code","810e1a6a":"code","51bafeee":"code","da361883":"code","464948d1":"code","6c26d0d8":"code","80534fa8":"code","847b1263":"code","4ee4e0ca":"code","a28b6d08":"code","db9b6f4d":"code","b09cfd8f":"code","0eba0450":"code","f2db0e0a":"code","a0494b6a":"code","d1d4dab6":"code","688ed406":"code","fe15522c":"code","aa3b867b":"code","704b2e12":"code","d2660823":"code","e5104fcc":"code","b05d5253":"code","c5cc309d":"code","b5f95430":"code","e1e0cc4f":"code","d2c07c4e":"code","d4c7ec1a":"code","66b2b727":"code","d18206ac":"code","a8b57b67":"code","1221abce":"code","8da16bf8":"code","9208f9b1":"code","dd76c04b":"code","d837bd41":"code","27dc07cb":"code","833221ff":"code","4efc95aa":"code","19013ba4":"code","3a6aca8d":"code","94a8ea8e":"code","d260286a":"code","4b48cd48":"code","f7cb44c8":"code","94205c8e":"code","d2b04904":"code","78f025f5":"code","6855c063":"code","3c867d40":"code","8b5688a5":"code","30476028":"code","2d762137":"code","c6584ac5":"code","0972bfbd":"code","332b0332":"code","c6124531":"code","01822767":"code","24076f72":"code","062823bc":"code","212aedf3":"code","27f5f078":"code","7a3911a6":"code","9a38efd6":"code","de58ddb2":"code","8be3b186":"code","2b20454d":"code","68224797":"code","0c5f6209":"code","a6767e66":"code","66808552":"markdown","1b7d8b93":"markdown","d55e42c1":"markdown","3ac86097":"markdown","cd756573":"markdown","10ee95fc":"markdown","ecc6b574":"markdown","b80c8da6":"markdown","112d314d":"markdown","0561f71a":"markdown","f03c420e":"markdown","1216eb0b":"markdown","1091007b":"markdown","ddfb9501":"markdown","5eed74fe":"markdown","5b0df5a4":"markdown","6a6f2b9c":"markdown","1cfebb81":"markdown","273162b9":"markdown","3a32a3de":"markdown","32e861e0":"markdown","61e6ea24":"markdown","6310732e":"markdown","cc0c3de0":"markdown","1b19ef47":"markdown","8750bd2b":"markdown","6accc180":"markdown","1c14b77e":"markdown","04c64e9f":"markdown","10568a32":"markdown","1668799e":"markdown","e5d48c8d":"markdown"},"source":{"48fd11dd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0d1372be":"from bs4 import BeautifulSoup\nfrom collections import Counter,defaultdict\nfrom gensim.models import Word2Vec,KeyedVectors\nimport numpy as np\nimport pandas as pd\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow import keras\nfrom keras.preprocessing.text import Tokenizer\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport matplotlib\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nimport plotly.graph_objs as go\nimport plotly.offline as py\nfrom plotly import tools\npy.init_notebook_mode(connected=True)\nimport re\nimport gc\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import PCA, TruncatedSVD,SparsePCA\nfrom sklearn.manifold import TSNE\nimport string\nfrom unidecode import unidecode\nfrom wordcloud import WordCloud, STOPWORDS\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\n%matplotlib inline","4ec96a1f":"train_df= pd.read_json(\"\/kaggle\/input\/github-bugs-prediction\/embold_train.json\").reset_index(drop=True)\ntrain_df.head()","1face6eb":"test_df= pd.read_json(\"\/kaggle\/input\/github-bugs-prediction\/embold_test.json\").reset_index(drop=True)\ntest_df.head()","1590aeb6":"train_ex_df= pd.read_json(\"\/kaggle\/input\/github-bugs-prediction\/embold_train_extra.json\").reset_index(drop=True)\ntrain_ex_df.head()","83afe0da":"def length_check(df):\n    print(len(df),df.index.shape[-1])\n                 \nlength_check(train_df)\nlength_check(train_ex_df)\nlength_check(test_df)","67d7a31a":"mas_train_df = pd.concat([train_df,train_ex_df],ignore_index=True)\nmas_train_df.head()","0450690e":"length_check(mas_train_df)","7d78db45":"mas_train_df.info()","db310193":"mas_train_df.drop_duplicates(keep='first').count()","5887d6b9":"mas_train_df.label.unique()","a60f7f36":"label_0=train_df[mas_train_df['label']==0]['title']\nlabel_1=train_df[mas_train_df['label']==1]['title']\nlabel_2=train_df[mas_train_df['label']==2]['title']\nprint(\"First 10 samples of label 1 title\\n\".format(),label_0[:10])\nprint(\"First 10 samples of label 1 title\\n\".format(),label_1[:10])\nprint(\"First 10 samples of label 1 title\\n\".format(),label_2[:10])","c89aa22a":"print('Total Counts of label column: \\n'.format(),mas_train_df['label'].value_counts())\n\nsns.countplot(x='label', data=mas_train_df)","9d44d727":"def fx(x):\n    return x['title'] + \" \" + x['body']   \nmas_train_df['text']= mas_train_df.apply(lambda x : fx(x),axis=1)\ntest_df['text']= test_df.apply(lambda x : fx(x),axis=1)","8e629e30":"mas_train_df.head()","19b1af0b":"def cal_len(data):\n    return len(data)\n\n#Create generic plotter with Seaborn\ndef plot_count(count_zeros,count_ones,count_second,title_0,title_1,title_2,subtitle):\n    fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,5))\n    sns.distplot(count_zeros,ax=ax1,color='Blue')\n    ax1.set_title(title_0)\n    sns.distplot(count_ones,ax=ax2,color='Red')\n    ax2.set_title(title_1)\n    sns.distplot(count_second,ax=ax3,color='Green')\n    ax3.set_title(title_2)\n    fig.suptitle(subtitle)\n    plt.show()    \n\n\nlabel_0_words=mas_train_df[mas_train_df['label']==0]['text'].str.split().apply(lambda z:cal_len(z))\nlabel_1_words=mas_train_df[mas_train_df['label']==1]['text'].str.split().apply(lambda z:cal_len(z))\nlabel_2_words=mas_train_df[mas_train_df['label']==2]['text'].str.split().apply(lambda z:cal_len(z))\nprint(\"Label 0 Words:\" + str(label_0_words))\nprint(\"Label 1 Words:\" + str(label_1_words))\nprint(\"Label 2 Words:\" + str(label_2_words))\nplot_count(label_0_words,label_1_words,label_2_words,\"Label 0 text\",\"Label 1 text\",\"Label 2 text\",\"Reviews Word Analysis\")","54d03d79":"label_0_df = mas_train_df[mas_train_df['label']==0]\nlabel_1_df = mas_train_df[mas_train_df['label']==1]\nlabel_2_df = mas_train_df[mas_train_df['label']==2]","c18ae977":"#Count Punctuations\/Stopwords\/Codes and other semantic datatypes\n#We will be using the \"generic_plotter\" function.\n\ncount_0_punct=label_0_df['text'].apply(lambda z: len([c for c in str(z) if c in string.punctuation]))\ncount_1_punct=label_1_df['text'].apply(lambda z:len([c for c in str(z) if c in string.punctuation]))\ncount_2_punct=label_2_df['text'].apply(lambda z:len([c for c in str(z) if c in string.punctuation]))\nplot_count(count_0_punct,count_1_punct,count_2_punct,\"Label 0 Punctuations\",\"Label 1 Punctuations\",\"Label 2 Punctuation\", \"Punctuation Analysis\")","32bfbd6e":"del count_0_punct\ngc.collect()\ndel count_1_punct\ngc.collect()\ndel count_2_punct\ngc.collect()","652a2f9b":"stops=set(stopwords.words('english'))\ncount_0_stops=label_0_df['text'].apply(lambda z : np.mean([len(z) for w in str(z).split()]))\ncount_1_stops=label_1_df['text'].apply(lambda z : np.mean([len(z) for w in str(z).split()]))\ncount_2_stops=label_2_df['text'].apply(lambda z : np.mean([len(z) for w in str(z).split()]))\nplot_count(count_0_stops,count_1_stops,count_2_stops,\"Label 0 Stopwords\",\"Label 1 Stopwords\",'Label 2 Stopwords',\"Text Stopwords Analysis\")","e0c3e985":"def draws_Word_Cloud(data):\n    wordCloud=WordCloud(width=1000,height=700,stopwords = STOPWORDS).generate(' '.join(data))\n    plt.figure(figsize = (15,8))\n    plt.imshow(wordCloud)\n    plt.axis('off')\n    plt.show()","935d477b":"draws_Word_Cloud(label_0_df['text'])","306db1b3":"draws_Word_Cloud(label_1_df['text'])","810e1a6a":"draws_Word_Cloud(label_2_df['text'])","51bafeee":"def word_count_plot(df):\n    c1= Counter()    \n    for x in df['text']:\n        c1.update(x.split())\n    most=c1.most_common()\n    x=[]\n    y=[]\n    for word,count in most[:50]:\n        if (word not in stops) :\n            x.append(word)\n            y.append(count)\n    sns.barplot(x=y,y=x)\n    plt.show()","da361883":"word_count_plot(label_0_df)","464948d1":"word_count_plot(label_1_df)","6c26d0d8":"word_count_plot(label_2_df)","80534fa8":"extra_punctuations = ['','.', '``', '...', '\\'s', '--', '-', 'n\\'t', '_', '\u2013','&']\nstopword_list = stopwords.words('english') + list(string.punctuation)+ extra_punctuations + ['the','us','say','that','he','me','she','get','rt','it','mt','via','not','and','let','so','say','dont','use','you']","847b1263":"wordnet_lemmatizer = WordNetLemmatizer()\nstemmer = PorterStemmer() \ntokenizer=TweetTokenizer()","4ee4e0ca":"def gram_analysis(data,gram):\n    token= tokenizer.tokenize(data.lower()) \n    token = [tok for tok in token if len(tok) > 2 if tok not in stopword_list and not tok.isdigit()]\n    ngrams=zip(*[token[i:] for i in range(gram)])\n    final_tokens=[\" \".join(z) for z in ngrams]\n    return final_tokens","a28b6d08":"def create_dict(data,grams):\n    freq_dict=defaultdict(int)\n    for sentence in data:\n        for tokens in gram_analysis(sentence,grams):\n            freq_dict[tokens]+=1\n    return freq_dict","db9b6f4d":"def horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"n_gram_words\"].values[::-1],\n        x=df[\"n_gram_frequency\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace","b09cfd8f":"def create_new_df(freq_dict,):\n    freq_df=pd.DataFrame(sorted(freq_dict.items(),key=lambda z:z[1])[::-1])\n    freq_df.columns=['n_gram_words','n_gram_frequency']\n    trace=horizontal_bar_chart(freq_df[:20],'orange')\n    return trace","0eba0450":"def plot_grams(df1,df2,df3):\n    fig = tools.make_subplots(rows=1, cols=3, vertical_spacing=0.1,\n                          subplot_titles=[\"Frequent words of lable 0\", \n                                          \"Frequent words of lable 1\",\n                                          \"Frequent words of lable 2\"])\n    fig.append_trace(df1, 1, 1)\n    fig.append_trace(df2, 1, 2)\n    fig.append_trace(df3, 1, 3)\n    fig['layout'].update(height=800, width=1000, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n    py.iplot(fig, filename='word-plots')","f2db0e0a":"for i in [2,3,5]:\n    if(i==2):\n        print(\"Bi-gram analysis\")\n    elif(i==3):\n        print(\"Tri-gram analysis\")\n    else:\n        print(\"Penta-gram analysis\")\n    freq_label_0_zero=create_dict(label_0_df['text'][:400],i)\n    trace_zero=create_new_df(freq_label_0_zero)\n    freq_label_1_ones=create_dict(label_1_df['text'][:400],i)\n    trace_ones=create_new_df(freq_label_1_ones)\n    freq_label_2_ones=create_dict(label_2_df['text'][:400],i)\n    trace_secs=create_new_df(freq_label_2_ones)\n    plot_grams(trace_zero,trace_ones,trace_secs)","a0494b6a":"del label_0_df\ngc.collect()\ndel label_1_df\ngc.collect()\ndel label_2_df\ngc.collect()","d1d4dab6":"cList = {\n            \"i'm\": \"i am\",\n            \"you're\": \"you are\",\n            \"it's\": \"it is\",\n            \"we're\": \"we are\",\n            \"we'll\": \"we will\",\n            \"That's\":\"that is\",\n            \"haven't\":\"have not\",\n            \"let's\":\"let us\",\n            \"ain't\": \"am not \/ are not \/ is not \/ has not \/ have not\",\n            \"aren't\": \"are not \/ am not\",\n            \"can't\": \"cannot\",\n            \"can't've\": \"cannot have\",\n            \"'cause\": \"because\",\n            \"could've\": \"could have\",\n            \"couldn't\": \"could not\",\n            \"couldn't've\": \"could not have\",\n            \"didn't\": \"did not\",\n            \"doesn't\": \"does not\",\n            \"don't\": \"do not\",\n            \"hadn't\": \"had not\",\n            \"hadn't've\": \"had not have\",\n            \"hasn't\": \"has not\",\n            \"haven't\": \"have not\",\n            \"he'd\": \"he had \/ he would\",\n            \"he'd've\": \"he would have\",\n            \"he'll\": \"he shall \/ he will\",\n            \"he'll've\": \"he shall have \/ he will have\",\n            \"he's\": \"he has \/ he is\",\n            \"how'd\": \"how did\",\n            \"how'd'y\": \"how do you\",\n            \"how'll\": \"how will\",\n            \"how's\": \"how has \/ how is \/ how does\",\n            \"I'd\": \"I had \/ I would\",\n            \"I'd've\": \"I would have\",\n            \"I'll\": \"I shall \/ I will\",\n            \"I'll've\": \"I shall have \/ I will have\",\n            \"I'm\": \"I am\",\n            \"I've\": \"I have\",\n            \"isn't\": \"is not\",\n            \"it'd\": \"it had \/ it would\",\n            \"it'd've\": \"it would have\",\n            \"it'll\": \"it shall \/ it will\",\n            \"it'll've\": \"it shall have \/ it will have\",\n            \"it's\": \"it has \/ it is\",\n            \"let's\": \"let us\",\n            \"ma'am\": \"madam\",\n            \"mayn't\": \"may not\",\n            \"might've\": \"might have\",\n            \"mightn't\": \"might not\",\n            \"mightn't've\": \"might not have\",\n            \"must've\": \"must have\",\n            \"mustn't\": \"must not\",\n            \"mustn't've\": \"must not have\",\n            \"needn't\": \"need not\",\n            \"needn't've\": \"need not have\",\n            \"o'clock\": \"of the clock\",\n            \"oughtn't\": \"ought not\",\n            \"oughtn't've\": \"ought not have\",\n            \"shan't\": \"shall not\",\n            \"sha'n't\": \"shall not\",\n            \"shan't've\": \"shall not have\",\n            \"she'd\": \"she had \/ she would\",\n            \"she'd've\": \"she would have\",\n            \"she'll\": \"she shall \/ she will\",\n            \"she'll've\": \"she shall have \/ she will have\",\n            \"she's\": \"she has \/ she is\",\n            \"should've\": \"should have\",\n            \"shouldn't\": \"should not\",\n            \"shouldn't've\": \"should not have\",\n            \"so've\": \"so have\",\n            \"so's\": \"so as \/ so is\",\n            \"that'd\": \"that would \/ that had\",\n            \"that'd've\": \"that would have\",\n            \"that's\": \"that has \/ that is\",\n            \"there'd\": \"there had \/ there would\",\n            \"there'd've\": \"there would have\",\n            \"there's\": \"there has \/ there is\",\n            \"they'd\": \"they had \/ they would\",\n            \"they'd've\": \"they would have\",\n            \"they'll\": \"they shall \/ they will\",\n            \"they'll've\": \"they shall have \/ they will have\",\n            \"they're\": \"they are\",\n            \"they've\": \"they have\",\n            \"to've\": \"to have\",\n            \"wasn't\": \"was not\",\n            \"we'd\": \"we had \/ we would\",\n            \"we'd've\": \"we would have\",\n            \"we'll\": \"we will\",\n            \"we'll've\": \"we will have\",\n            \"we're\": \"we are\",\n            \"we've\": \"we have\",\n            \"weren't\": \"were not\",\n            \"what'll\": \"what shall \/ what will\",\n            \"what'll've\": \"what shall have \/ what will have\",\n            \"what're\": \"what are\",\n            \"what's\": \"what has \/ what is\",\n            \"what've\": \"what have\",\n            \"when's\": \"when has \/ when is\",\n            \"when've\": \"when have\",\n            \"where'd\": \"where did\",\n            \"where's\": \"where has \/ where is\",\n            \"where've\": \"where have\",\n            \"who'll\": \"who shall \/ who will\",\n            \"who'll've\": \"who shall have \/ who will have\",\n            \"who's\": \"who has \/ who is\",\n            \"who've\": \"who have\",\n            \"why's\": \"why has \/ why is\",\n            \"why've\": \"why have\",\n            \"will've\": \"will have\",\n            \"won't\": \"will not\",\n            \"won't've\": \"will not have\",\n            \"would've\": \"would have\",\n            \"wouldn't\": \"would not\",\n            \"wouldn't've\": \"would not have\",\n            \"y'all\": \"you all\",\n            \"y'all'd\": \"you all would\",\n            \"y'all'd've\": \"you all would have\",\n            \"y'all're\": \"you all are\",\n            \"y'all've\": \"you all have\",\n            \"you'd\": \"you had \/ you would\",\n            \"you'd've\": \"you would have\",\n            \"you'll\": \"you shall \/ you will\",\n            \"you'll've\": \"you shall have \/ you will have\",\n            \"you're\": \"you are\",\n            \"you've\": \"you have\"\n           }","688ed406":"c_re = re.compile('(%s)' % '|'.join(cList.keys()))","fe15522c":"def expandContractions(text, c_re=c_re):\n    def replace(match):\n        return cList[match.group(0)]\n    return c_re.sub(replace, text)","aa3b867b":"def remove_emoji(string):\n        emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n        return emoji_pattern.sub(r'', string) ","704b2e12":"def remove_punctuations(data):\n    punct_tag=re.compile(r'[^\\w\\s]')\n    data=punct_tag.sub(r'',data)\n    return data","d2660823":"def removeSpecialChars(data):\n    '''\n    Removes special characters which are specifically found in tweets.\n    '''\n    #Converts HTML tags to the characters they represent\n    soup = BeautifulSoup(data, \"html.parser\")\n    data = soup.get_text()\n\n    #Convert www.* or https?:\/\/* to empty strings\n    data = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))','',data)\n\n    #Convert @username to empty strings\n    data = re.sub('@[^\\s]+','',data)\n    \n    #remove org.apache. like texts\n    data =  re.sub('(\\w+\\.){2,}','',data)\n\n    #Remove additional white spaces\n    data = re.sub('[\\s]+', ' ', data)\n    \n    data = re.sub('\\.(?!$)', '', data)\n\n    #Replace #word with word\n    data = re.sub(r'#([^\\s]+)', r'\\1', data)\n\n    return data ","e5104fcc":"def remove_nonenglish_charac(string):\n    return re.sub('\\W+','', string )","b05d5253":"def clean_text(data):\n    data = unidecode(data)\n    data = expandContractions(data)\n    tokens = tokenizer.tokenize(data)\n    data = ' '.join([tok for tok in tokens if len(tok) > 2 if tok not in stopword_list and not tok.isdigit()])\n    data = re.sub('\\b\\w{,2}\\b', '', data)\n    data = re.sub(' +', ' ', data)\n    data = removeSpecialChars(data)\n    data = remove_emoji(data)\n    data= [stemmer.stem(w) for w in data.split()]\n    return ' '.join([wordnet_lemmatizer.lemmatize(word) for word in data])","c5cc309d":"mas_train_df['text'].head()","b5f95430":"mas_train_df['text']=mas_train_df['text'].apply(lambda x: clean_text(x))","e1e0cc4f":"mas_train_df['text'].head()","d2c07c4e":"train_df_zero=mas_train_df[mas_train_df['label']==0]['text']\ntrain_df_ones=mas_train_df[mas_train_df['label']==1]['text']\ntrain_df_twos=mas_train_df[mas_train_df['label']==2]['text']","d4c7ec1a":"for i in [2,3,5]:\n    if(i==2):\n        print(\"Bi-gram analysis\")\n    elif(i==3):\n        print(\"Tri-gram analysis\")\n    else:\n        print(\"Penta-gram analysis\")\n    freq_label_0_zero=create_dict(train_df_zero[:400],i)\n    trace_zero=create_new_df(freq_label_0_zero)\n    freq_label_1_ones=create_dict(train_df_ones[:400],i)\n    trace_ones=create_new_df(freq_label_1_ones)\n    freq_label_2_ones=create_dict(train_df_twos[:400],i)\n    trace_secs=create_new_df(freq_label_2_ones)\n    plot_grams(trace_zero,trace_ones,trace_secs)","66b2b727":"draws_Word_Cloud(train_df_zero)","d18206ac":"draws_Word_Cloud(train_df_ones)","a8b57b67":"\ndraws_Word_Cloud(train_df_twos)","1221abce":"tfidf_vect=TfidfVectorizer(stop_words='english',ngram_range=(1,3))\ntrain_tfidf=tfidf_vect.fit_transform(mas_train_df['text'].values.tolist())\ntrain_tfidf.shape","8da16bf8":"def count_vectorize(data):\n    cv=CountVectorizer()\n    fit_data_cv=cv.fit_transform(data)\n    return fit_data_cv,cv","9208f9b1":"#Tfidf vectorization from sklearn\ndef tfidf(data):\n    tfidfv=TfidfVectorizer()\n    fit_data_tfidf=tfidfv.fit_transform(data)\n    return fit_data_tfidf,tfidfv","dd76c04b":"def dimen_reduc_plot(test_data,test_label,option):\n    tsvd= TruncatedSVD(n_components=2,algorithm=\"randomized\",random_state=42)\n    tsne=TSNE(n_components=2,random_state=42) #not recommended instead use PCA\n    pca=SparsePCA(n_components=2,random_state=42)\n    if(option==1):\n        tsvd_result=tsvd.fit_transform(test_data)\n        plt.figure(figsize=(10,8))\n        colors=['orange','red','blue']\n        \n        sns.scatterplot(x=tsvd_result[:,0],y=tsvd_result[:,1],hue=test_label,\n                        palette=['green','brown','dodgerblue'])\n        \n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(tsvd_result[:,0],tsvd_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='red',label='Label 0')\n        color_orange=mpatches.Patch(color='green',label='Label 1')\n        color_blue=mpatches.Patch(color='blue',label='Label 2')\n        plt.legend(handles=[color_orange,color_red,color_blue])\n        plt.title(\"TSVD\")\n        plt.show()\n    if(option==2):\n        tsne_result=tsne.fit_transform(test_data)\n        plt.figure(figsize=(10,8))\n        colors=['orange','red','blue']\n        sns.scatterplot(x=tsne_result[:,0],y=tsne_result[:,1],hue=test_label)\n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(x=tsne_result[:,0],y=tsne_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='red',label='Label 0')\n        color_orange=mpatches.Patch(color='green',label='Label 1')\n        color_blue=mpatches.Patch(color='blue',label='Label 2')\n        plt.legend(handles=[color_orange,color_red,color_blue])\n        plt.title(\"PCA\")\n        plt.show() \n    if(option==3):\n        pca_result=pca.fit_transform(test_data.toarray())\n        plt.figure(figsize=(10,8))\n        colors=['orange','red','blue']\n        sns.scatterplot(x=pca_result[:,0],y=pca_result[:,1],hue=test_label)\n        plt.show()\n        plt.figure(figsize=(10,10))\n        plt.scatter(x=pca_result[:,0],y=pca_result[:,1],c=test_label,cmap=matplotlib.colors.ListedColormap(colors))\n        color_red=mpatches.Patch(color='red',label='Label 0')\n        color_orange=mpatches.Patch(color='green',label='Label 1')\n        color_blue=mpatches.Patch(color='blue',label='Label 2')\n        plt.legend(handles=[color_orange,color_red,color_blue])\n        plt.title(\"TSNE\")\n        plt.show()","d837bd41":"text_vect=mas_train_df['text'].values\ntarget_vect=mas_train_df['label'].values\ntrain_data_cv,cv= tfidf(text_vect)","27dc07cb":"dimen_reduc_plot(train_data_cv,target_vect,1)","833221ff":"del_bj_list=[]","4efc95aa":"#TSNE visualization on first 1000 samples\ntrain_data=mas_train_df[:1000]\ndata_vect=train_data['text'].values\ntarget_vect=train_data['label'].values\ntrain_data_cv,cv= tfidf(data_vect)\ndimen_reduc_plot(train_data_cv,target_vect,3)","19013ba4":"def multiple_appends(list1, *element):\n    list1.extend(element)","3a6aca8d":"multiple_appends(del_bj_list,train_data,data_vect,target_vect,train_data_cv,cv,text_vect)","94a8ea8e":"for obj in del_bj_list:\n    del obj\n    gc.collect()","d260286a":"check_df=list(mas_train_df['text'].str.split())","4b48cd48":"import multiprocessing\n# Count the number of cores in a computer\ncores = multiprocessing.cpu_count() ","f7cb44c8":"model=Word2Vec(check_df,window=2,workers=cores-1,sg=0)\nword_li=list(model.wv.vocab)\nprint(word_li[:10])","94205c8e":"model_skpgra=Word2Vec(check_df,window=2,workers=cores-1,sg=1)\nword_li_skp_Gram=list(model_skpgra.wv.vocab)\nprint(word_li_skp_Gram[:20])","d2b04904":"plt.plot(model['piano'])\nplt.plot(model_skpgra['piano'])\nplt.show()","78f025f5":"del model_skpgra\ngc.collect()","6855c063":"model.save('word2vec_model.bin')\nloaded_model=KeyedVectors.load('word2vec_model.bin')","3c867d40":"def TSNE_SCatterPlot(word_model):\n    tsne = TSNE(n_components=2)\n    transformation_model=word_model[word_model.wv.vocab]\n    result = tsne.fit_transform(transformation_model[:50])\n    df = pd.DataFrame(result, index=word_li[:50], columns=['x', 'y'])\n    plt.scatter(df['x'], df['y'])\n    for word, pos in df.iterrows(): \n        plt.annotate(word, pos)\n    plt.figure(figsize=(20,10))\n    plt.show()","8b5688a5":"TSNE_SCatterPlot(model)","30476028":"distance=model.similarity('state','alarm')\nprint(distance)","2d762137":"google_news_embed='..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin'\ngoogle_loaded_model=KeyedVectors.load_word2vec_format(google_news_embed,binary=True)","c6584ac5":"plt.plot(google_loaded_model['action'])\nplt.show()","0972bfbd":"TSNE_SCatterPlot(google_loaded_model)","332b0332":"del google_loaded_model\ngc.collect()\ndel check_df\ngc.collect()\ndel model\ngc.collect()","c6124531":"from gensim.scripts.glove2word2vec import glove2word2vec\nglove_file='..\/input\/glove6b50dtxt\/glove.6B.50d.txt'\nword2vec_output_file = 'glove.6B.100d.txt.word2vec'\nglove_loaded=glove2word2vec(glove_file, word2vec_output_file)\nprint(glove_loaded)","01822767":"glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\nplt.plot(glove_model['action'])\nplt.plot(glove_model['alarm'])\nplt.show()","24076f72":"TSNE_SCatterPlot(glove_model)","062823bc":"del glove_model\ngc.collect()","212aedf3":"fasttext_file=\"..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec\"\nfasttext_model = KeyedVectors.load_word2vec_format(fasttext_file, binary=False)\nplt.plot(fasttext_model['action'])\nplt.plot(fasttext_model['alarm'])\nplt.show()","27f5f078":"TSNE_SCatterPlot(fasttext_model)","7a3911a6":"del fasttext_model\ngc.collect()","9a38efd6":"maxlen=1000\nmax_features=5000\nembed_size=300\ntext_df = mas_train_df['text']","de58ddb2":"def getMaxLengthSentence(corp):\n    maxlen=-1\n    for doc in corp:\n        tokens=nltk.word_tokenize(doc)        \n        if(maxlen<len(tokens)):\n            maxlen=len(tokens)\n    print(\"The maximum number of words in any document is : \",maxlen)","8be3b186":"#getMaxLengthSentence(text_df)","2b20454d":"tokenizer=Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(text_df))\ntrain_sample=tokenizer.texts_to_sequences(text_df)\n\n#Pad the sequence- To allow same length for all vectorized words\ntrain_sample=pad_sequences(train_sample,maxlen=maxlen)","68224797":"# Create the dictionary of pretrained embedding\nEMBEDDING_FILE = '..\/input\/wikinews300d1msubwordvec\/wiki-news-300d-1M-subword.vec'\ndef get_embedding_dict(word,*arr): \n    return word, np.asarray(arr, dtype='float32')\n\nembeddings_index = dict(get_embedding_dict(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\n# prepared multidimentional dictionary\nall_embs = np.stack(embeddings_index.values())","0c5f6209":"# prepare a gausiian distribution\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))","a6767e66":"for word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\nplt.plot(embedding_matrix[20])\nplt.show()","66808552":"## Matrix Embdedding using deep learning algorithm","1b7d8b93":"## PADDING THE DOCS (to make very doc of same length)\n- The Keras Embedding layer requires all individual documents to be of same length. Hence we wil pad the shorter documents with 0 for now. Therefore now in Keras Embedding layer the 'input_length' will be equal to the length (ie no of words) of the document with maximum length or maximum number of words.\n- to create embeddings all of our docs need to be of same length. hence we can pad the docs with zeros.","d55e42c1":"As we can see, all rows have a title and body. let's combine title and body because both seems to be containing relative text related to label","3ac86097":"### Count plot analysis\nLable 0 and 1 seems to be equal proportion. <br>\nLable 2 seems to be in less proportion","cd756573":"## Text Cleaning\nlet us clean the dataset and remove the redundancies.This includes\n\n1. HTML codes\n1. URLs\n1. Emojis\n1. Stopwords\n1. Punctuations\n1. Expanding Abbreviations","10ee95fc":"# Embdedding work flow\n- Create the dictionary of pretrained embedding with respective decimal array values if length of embedded word is greater thatn zero\n- using numpy stack function prepared multidimentional dictionary\n- using word embedding creating distribution of any type like gaussian\/bernouli\/poisson for increasing model accuracy\n-","ecc6b574":"**Insite From above Analysis**\n* The dataset is balanced looks to be balanced for lable 0 and lable 1. lable 2 are comparatively less in number.<br>\n* The dataset contains redundant words and html syntaxes.\n* lot of Punctuations\/stopwords are present in the dataset.","b80c8da6":"- Lable 0 and Lable 2 contains many of the puntuation marks","112d314d":"Looks like both the train and train extra dataframe contains the same content so let's combine them using concatenation command of pandas on index axis","0561f71a":"> ## Word Embedding\n* Word embeddings are a type of word representation that allows words with similar meaning to have a similar representation.\n* Word embeddings are in fact a class of techniques where individual words are represented as real-valued vectors in a predefined vector space. Each word is mapped to one vector and the vector values are learned in a way that resembles a neural network","f03c420e":"## Glove Embeddings","1216eb0b":"## EDA(Explodatory Data Analysis)","1091007b":"## Summary\n- text cleaning is completed after removing punctuatuon, html tag, urls, stop words and alphanumeric characters.\n- as more of the noisy data has been removed we can peform other semantics analysis","ddfb9501":"### Word Cloud\n- Word cloud is \u201can image composed of words used in a particular text or subject, in which the size of each word indicates its frequency or importance.\u201d So, the more often a specific words appears in your text, the bigger and bolder it appears in your word cloud\n- A word cloud is a collection, or cluster, of words depicted in different sizes. The bigger and bolder the word appears, the more often it\u2019s mentioned within a given text and the more important it is.<br>","5eed74fe":"## Fast text Embedding\n- FastText as a library for efficient learning of word representations and sentence classification. It is written in C++ and supports multiprocessing during training. The model allows one to create an unsupervised learning or supervised learning algorithm for obtaining vector representations for words\n- FastText supports training continuous bag of words (CBOW) or Skip-gram models using negative sampling, softmax or hierarchical softmax loss functions. \n- FastText is able to achieve really good performance for word representations and sentence classification, specially in the case of rare words by making use of character level information.word2vec treats every single word as the smallest unit whose vector representation is to be found but FastText assumes a word to be formed by a n-grams of character\n- Each word is represented as a bag of character n-grams in addition to the word itself, so for example, for the word matter, with n = 3, the fastText representations for the character n-grams is <ma, mat, att, tte, ter, er>. < and > are added as boundary symbols to distinguish the ngram of a word from a word itself\nPros and Cons of FastText\nLike every library in development, it has its pros and cons. Let us state them explicitly.\n\n### Pros\n1. The library is surprisingly very fast in comparison to other methods for achieving the same accuracy. Here is the result published by the Facebook research team in support of the argument.Comparison of FastText with other Word Representation models\n![image.png](attachment:image.png)\n1. Sentence Vectors(supervised) can be easily computed.\n1. fastText works better on small datasets in comparison to gensim.\n1. fastText performs superior to gensim in terms of syntactic performance and fairs equally well in case of semantic performance. <br>\n\n### Cons\n1. This is not a standalone library for NLP since it will require another library for the pre-processing steps.\n1. Though, this library has a python implementation. It is not officially supported.","5b0df5a4":"Now all the documents are of same length (after padding). And so now we are ready to create and use the embeddings.","6a6f2b9c":"## The parameters:\nThere are many parameters on this constructor; a few noteworthy arguments you may wish to configure are:\n\n* size: (default 100) The number of dimensions of the embedding, e.g. the length of the dense vector to represent each token (word).<br>\n* window: (default 5) The maximum distance between a target word and words around the target word.<br>\n* min_count: (default 5) The minimum count of words to consider when training the model; words with an occurrence less than this count will be ignored.\n* workers: (default 3) The number of threads to use while training.<br>\n* sg: (default 0 or CBOW) The training algorithm, either CBOW (0) or skip gram (1).<br>","1cfebb81":"Let's check the length of each dataframe","273162b9":"## Visualizing the Vector Space\n\nThe curse of dimensionality occurs because the sample density decreases exponentially with the increase of the dimension. When we keep adding features without increasing the number of training samples as well, the dimensionality of the feature space grows and becomes sparser and sparser. Due to this sparsity, it becomes much easier to find a \u201cperfect\u201d solution for the machine learning model which highly likely leads to overfitting.\nDimensionality reduction is the process of reducing the dimensionality of the feature space with consideration by obtaining a set of principal features. Dimensionality reduction can be further broken into feature selection and feature extraction.\nFeature selection tries to select a subset of the original features for use in the machine learning model. In this way, we could remove redundant and irrelevant features without incurring much loss of information.\n\nNow, we have to reduce the dimensions ,else the kernel will run out of memory. For this we wmploy 3 different decomposition techniques:\n\nPCA <br>\nSVD <br>\nTSNE <br>\n\nThese algorithms rely on Eigen vector decomposition and Eigen matrices for creating smaller matrices. These reduced matrices are well-fitted to perform any numerical approximation tasks from differentiation to higher order non linear dynamics. PCA in general is a well known method and forms the base of all decomposition techniques. Pictorially it operates as follows with the help of orthogonal Eigen vectors:\n![image.png](attachment:image.png)\n\nTSNE is a more sophisticated method which uses a non convex optimization along with gradient descent. This is different than Eigen Vector (convex optimization) method of PCA and hence different results may be obtained in different iterations. It is a memory intensive method and is often powerful at the expense of longer execution time.\n![image.png](attachment:image.png)","3a32a3de":"### Word Count Analysis\n- Analyse the count of words in each label type segment\n- Majority of the document\/sentence contains only total word around 100-150\n- Few of sentence word count is exceeded by 200 may be because of stopword, puntuation mark, url links etc.\n- Which implies that lot of text cleaning needs to performed","32e861e0":"The CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary.An encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document.","61e6ea24":"* Gram analysis on Training set- Bigram and Trigram","6310732e":"## Types of Word2Vec algorithms\nThese variants include :\n\n*FastText <br>\n*[Glove](https:\/\/nlp.stanford.edu\/projects\/glove\/)<br>\n*[Google News Vectors](https:\/\/code.google.com\/archive\/p\/word2vec\/)","cc0c3de0":"# Word2Vec\n* Word2Vec is a statistical method for efficiently learning a standalone word embedding from a text corpus.Word2Vec is not a singular algorithm, rather, it is a family of model architectures and optimizations that can be used to learn word embeddings from large datasets.\n\n* Two different learning models were introduced that can be used as part of the word2vec approach to learn the word embedding; they are:\n\n1. Continuous Bag-of-Words, or CBOW model.\n1. Continuous Skip-Gram Model.\nThe CBOW model learns the embedding by predicting the current word based on its context. The continuous skip-gram model learns by predicting the surrounding words given a current word.\n![image.png](attachment:image.png)\n\nThe size of the sliding window has a strong effect on the resulting vector similarities. Large windows tend to produce more topical similarities [\u2026], while smaller windows tend to produce more functional and syntactic similarities.","1b19ef47":"One issue with simple counts is that some words like \u201cthe\u201d will appear many times and their large counts will not be very meaningful in the encoded vectors.\n\nAn alternative is to calculate word frequencies, and by far the most popular method is called TF-IDF. This is an acronym than stands for \u201cTerm Frequency \u2013 Inverse Document\u201d Frequency which are the components of the resulting scores assigned to each word.TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents.","8750bd2b":"### Inference from ngram analysis <br>\n\n* In this section, we have analysed based on lable features of words in a sentence. The Gram analysis,particularly the tirgram and pentagram analysis provides an idea set of words occur more often in the corpus.\n\n* The above plot give a idea related to the frequency of the conjuction of words which are occuring at the highest frequency. Another important aspect is that,there is a presence of html tags, urls, dates and punctuations which should removed as these are adding noise to the lable corpus. This will be nect step.","6accc180":"## Importing required library for notebook","1c14b77e":"## Gram statistics after data cleaning\n- Many of the url's links other than http or www are also removed after cleaning the text\n- also many of the existing contraction has been expanded and exists in stop word those are also removed\n- many non menaingful digits are also removed","04c64e9f":"## Vectorization and Embeddings\nIn this context, we will be vectorizing our dataset. This would allow us to convert our data to higher dimensional containers (matrices). These vectorization strategies allow the word corpus to be properly suitable for advanced semantic analysis.\n\nHere there are 2 variants of transforming the textual corpus to a numerical vector:\n\nVectorize without semantics\nRetain Semantic Importance\nIn the first case, vectorization strategy is used to provide a co-occurence probabilistic distribution for vectorization. Methods like TF-IDF,Count vectorization\/One hot vectorization, falls under this criteria.These methods leverage statistical co-occurence probabilities and log likelihoods for determining the frequently occuring sentences or group of words in a corpus.\n\nThe second case, relies on applying vectors with respect to semantic importance. Embeddings fall under this category. Embeddings are largely of 2 kinds\n\nStatic Embeddings: Word2Vec, Glove, Fasttext, Paragram\nDynamic Embeddings: ELMO, BERT & its variants, XLNet\/Transformer-XL\nAll of these embeddings rely on pretrained word vectors where a probabilistic score is attributed to each word in the corpus. These probabilities are plotted in a low dimensional plane and the \"meaning\" of the words are inferred from these vectors. Generally speaking cosine distance is taken as the major metric of similarity measurement between word and sentence vectors to infer similarity.\n\n\nWe will move ahead with TFIDF and Count vectorization strategies and will be going in further sections.\n\nTF-IDF Vectorization: This works by applying a logarithmic term to inverse document frequency (IDF) part other than determining the \"TF\" or term freqency part. The formulation can be shown as follows:\n![image.png](attachment:image.png)\n\n\nCount Vectorization: This is a simpler vectorization technique which relies on frequency of occurence of a particular term in a document or corpus.\n![image.png](attachment:image.png)","10568a32":"Attribute Description:\n1. Title - the title of the GitHub bug <br> \n1. Body - the body of the GitHub bug <br> \n1. Label - Represents various classes of Labels <br> \n - Bug - 0 <br> \n - Feature - 1 <br> \n - Question - 2 <br> ","1668799e":"Let's check with CBOW or skip-gram which gives better result","e5d48c8d":"Let's drop duplicate if exists no duplicates exists <br>\nNo duplicates present in training dataset after combining"}}