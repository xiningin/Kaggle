{"cell_type":{"57f8d72a":"code","61c0b796":"code","16c43421":"code","ec47be6b":"code","cacfbd85":"code","de86cccb":"code","085961b8":"code","d0d30b45":"code","7ef3cb63":"code","fb96d7d5":"code","803b63d9":"code","58639e42":"code","50fe0ea5":"code","603daffb":"code","a91f334d":"code","a486d93d":"code","bb6a57b5":"code","f34e627e":"code","1aced05b":"code","bf397e05":"code","cb5f6fe5":"code","67732944":"code","3da01f37":"code","2ded6f2b":"code","fd2a573b":"code","8e09537d":"code","0c8c2fe1":"code","ec66bb4b":"code","60be6d29":"code","cf32911d":"code","452aa4ea":"code","c1535358":"code","c233b04b":"code","f35e8d13":"code","7f3f86ed":"code","5b8daf32":"markdown","107b83a7":"markdown","c1332412":"markdown"},"source":{"57f8d72a":"# importing the path\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# importing the preprocessing libraries\nimport numpy as np\nimport pandas as pd\n\n# importing the visualization\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = 14,7\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# importing the Ml libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import confusion_matrix , classification_report\n","61c0b796":"# importing the dataset\ndataset = pd.read_csv(\"..\/input\/carsdata\/cars.csv\" , na_values=\" \")\ndataset.head()","16c43421":"# checking the shape of the dataset\ndataset.shape","ec47be6b":"# checking for any missing values\ndataset.isnull().sum()","cacfbd85":"# replacing the names of the columns\ndataset.columns = [\"mpg\" , \"cylinders\" , \"cubicinches\" , \"hp\" , \"weightlbs\" , \"time-to-60\", \"year\" , \"brand\"]\ndataset.head()","de86cccb":"# Descriptive Stats\ndataset.describe()","085961b8":"# descriptive Stats\ndataset.info()","d0d30b45":"# replacing and filling the missing values\ndataset[\"cubicinches\"] = dataset[\"cubicinches\"].replace(np.nan)\ndataset[\"weightlbs\"] = dataset[\"weightlbs\"].replace(np.nan)\n\nmean_cubicinches = dataset[\"cubicinches\"].mean()\nmean_weightlbs = dataset[\"weightlbs\"].mean()\n\ndataset[\"cubicinches\"] = dataset[\"cubicinches\"].replace(np.nan , mean_cubicinches)\ndataset[\"weightlbs\"] = dataset[\"weightlbs\"].replace(np.nan , mean_weightlbs)\n","7ef3cb63":"# looking at the data\ndataset.info()\n\n\"\"\"thus missing values is filled with mean values\"\"\"","fb96d7d5":"# Changing int64 to float64\ndataset[\"cylinders\"] = dataset[\"cylinders\"].astype(\"float64\")\ndataset[\"hp\"] = dataset[\"hp\"].astype(\"float64\")\ndataset[\"time-to-60\"] = dataset[\"time-to-60\"].astype(\"float64\")","803b63d9":"dataset.info()","58639e42":"dataset.head()","50fe0ea5":"# Checking for any outliers\nsns.boxplot(dataset[[\"mpg\"]])","603daffb":"sns.boxplot(dataset[[\"cylinders\"]])","a91f334d":"sns.boxplot(dataset[[\"cubicinches\"]])\n","a486d93d":"sns.boxplot(dataset[[\"hp\"]])","bb6a57b5":"sns.boxplot(dataset[\"weightlbs\"])\n","f34e627e":"sns.boxplot(dataset[\"time-to-60\"])","1aced05b":"# Removing outliers in time-to-60 column by IQR method\nQ1 = dataset[\"time-to-60\"].quantile(0.25)\nQ3 = dataset[\"time-to-60\"].quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = IQR - (1.5 * Q1)\nupper_bound = IQR + (1.5 * Q3)\nprint(\"Lower Bound of Boxplot: \" , lower_bound)\nprint(\"Upper Bound of Boxplot: \" , upper_bound)\n\n# Removing the outliers\ndataset[\"time-to-60\"] = dataset[(dataset[\"time-to-60\"] > lower_bound) & (dataset[\"time-to-60\"] < upper_bound)]\n","bf397e05":"\nsns.boxplot(dataset[\"time-to-60\"])","cb5f6fe5":"# making the count plot to understand the target variables\nsns.countplot(dataset[\"brand\"])","67732944":"# understanding the correration between different features\ncorr_data = dataset.corr()\nsns.heatmap(data = corr_data , annot = True)","3da01f37":"# changing year column\nprint(dataset[\"year\"].max())\ndataset[\"year\"].min()","2ded6f2b":"# making a new column in a dataset\ndataset[\"old\"] = dataset[\"year\"].max() - dataset[\"year\"]\n\n# Dropping the year column\ndataset.drop([\"year\"] , inplace = True , axis = 1)","fd2a573b":"dataset.head()","8e09537d":"# checking the correration of the data\nnew_corr = dataset.corr()\nsns.heatmap(data = new_corr , annot = True)","0c8c2fe1":"Target = dataset[\"brand\"]\ndataset.drop([\"brand\"] , axis = 1 , inplace = True)\ndataset[\"Target\"] = Target\ndataset.head()","ec66bb4b":"dataset[\"old\"] = dataset[\"old\"].astype(\"float64\")\ndataset.head()","60be6d29":"# looking at the dataset\ndataset.head()","cf32911d":"# splitting the dataset into independent(x) and dependent(y) variables\nx = dataset.iloc[: ,: -1].values\ny = dataset.iloc[: , -1].values\nprint(x)\nprint(y)","452aa4ea":"from sklearn.preprocessing import LabelEncoder\nencoder = LabelEncoder()\ny = encoder.fit_transform(y)\nprint(y)","c1535358":"# Normalizing the x\nx_data = x \/x.max()  \nx_data","c233b04b":"# Now Splitting the dataset into training and testing dataset\nx_train , x_test , y_train , y_test = train_test_split(x_data , y , test_size = .20 , random_state = None)","f35e8d13":"# applying knearest classifier\nfrom sklearn.ensemble import RandomForestClassifier\nforest_classifier = RandomForestClassifier(random_state = None)\nforest_classifier.fit(x_train , y_train)\n\ny_preg = forest_classifier.predict(x_test) ","7f3f86ed":"# Making the Confusion Matrix and Classification Report\ncm = confusion_matrix(y_test, y_preg)\nsns.heatmap(data =  cm , annot = True , cmap = \"Blues\" , linewidths= 1 , linecolor= \"black\")\n\nreport = classification_report(y_test , y_preg)\nprint(report)","5b8daf32":"#### Thus we can see that there is a very high relationship between different features","107b83a7":"#### Here brand is our target Variable","c1332412":"## Now the dataset is perfect for Model Building"}}