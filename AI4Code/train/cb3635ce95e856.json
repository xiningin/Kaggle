{"cell_type":{"ccc4c6d1":"code","7d074e3d":"code","aee43540":"code","2db003d6":"code","b5b93ad6":"code","cce07e94":"code","fb7829cf":"code","bc32073c":"code","cf2509f8":"code","7be7d8fe":"code","f76d6528":"code","e28d717d":"code","511474fe":"code","e49e0f91":"code","daed83e8":"code","fba4e946":"code","ffe04d9f":"code","ff47a546":"code","634eaf54":"code","ee7f6de4":"code","78939e55":"code","710635e4":"code","48f4eac1":"code","515d234b":"code","b27bd49e":"code","b14c21da":"code","db7b436a":"code","67268bb4":"code","84a5ad9a":"code","4ff8ae28":"code","6cce5797":"code","c9bb62bf":"code","62580945":"code","54e5381c":"code","99af3e77":"code","d0b53d23":"code","93b2db53":"code","1ddd895a":"code","364d18fc":"code","5e473a02":"code","4991d703":"code","45ab16f7":"code","659b2604":"code","59da099f":"code","28f49f50":"code","cba15c30":"code","621db098":"code","93c3a092":"code","730e1639":"code","c692c355":"code","4883b585":"code","bb064cf1":"code","12840ad0":"code","f6a0b736":"code","fd989e47":"code","73ab0be2":"code","7b30ee12":"code","65ea54df":"code","dcc0bc1c":"code","4696a0c1":"code","0bb87509":"code","1bbcbe18":"code","16d62172":"code","12162ef4":"code","e5f554c8":"code","050b455e":"code","80577e01":"code","761af994":"code","f6cc0374":"code","499cb8bb":"code","a23a494a":"code","a5de56b6":"code","b8011ef3":"code","15e84b00":"code","867d3ab2":"code","bf84df34":"code","087462fe":"code","954c445c":"code","6af7babe":"code","2405bc45":"code","ca664b0f":"code","c6ea82c6":"code","eb4a3ce3":"code","081ec03b":"code","e2fad994":"code","50fa8c6a":"code","cdca62da":"code","d3c6cd71":"code","b2e49c5d":"code","b535096f":"code","03867fc3":"code","a1cb6751":"code","7e2caf33":"code","49cc675f":"code","dd8f12f5":"code","a51a9132":"code","fc98c730":"code","db6f42bc":"code","a86f5435":"code","497952ba":"code","bce0a756":"code","71d55d4b":"code","5e40f639":"code","417e90df":"code","0effc49a":"code","297da6b0":"code","42a30a22":"code","c694eb5b":"code","a2122833":"code","871ad47e":"code","47b77133":"code","ed900497":"code","b868ebb4":"code","d319df2e":"code","602d8a14":"code","82a7d4e8":"code","64b3049e":"code","3cd9685d":"code","18b177b3":"code","89c94566":"code","7bafddc8":"code","4230b76d":"code","1490d079":"code","8c6f359c":"code","9429723d":"code","bcce7dd2":"code","586d5f51":"code","0605f60f":"code","cfcbbc6d":"code","e4c24917":"code","5450de89":"code","95529de8":"code","fc7eae71":"code","d581e80e":"code","ec039615":"code","80991898":"code","c14b040a":"code","720138e2":"code","4497a32f":"code","10104a45":"code","e180b575":"code","66351207":"code","ac7fcc48":"code","261735da":"code","e2909e5c":"code","728f312c":"code","eb46af72":"code","22ff8396":"code","a2b1fdbc":"code","5d6d4960":"code","3d607980":"code","5b3b903a":"code","e4d6ddd3":"code","2e78b569":"markdown","a7455ceb":"markdown","aefb514f":"markdown","6dac4bb1":"markdown","dd799697":"markdown","6c69d594":"markdown","a6e83809":"markdown","43d382a8":"markdown","dfb158a8":"markdown","02754f13":"markdown","c0b00720":"markdown","71429a67":"markdown","862f2dc2":"markdown","6dee2e65":"markdown","0f43e796":"markdown","93d813cf":"markdown","a18b838a":"markdown","e9351f9b":"markdown","06b27de6":"markdown","cab8686b":"markdown","0df9f9b0":"markdown","afea3e19":"markdown","3295fde8":"markdown","beca322b":"markdown","43eca946":"markdown","a2c83c1f":"markdown","4372f2e2":"markdown","656d78ad":"markdown","f4f1e4aa":"markdown","cddaa878":"markdown","10e14fc6":"markdown","5a852806":"markdown","f9b36d65":"markdown","76d6c528":"markdown","c23f906d":"markdown","99211991":"markdown","89c569ea":"markdown","edf9ae8e":"markdown","0a7403b9":"markdown","e8b82991":"markdown","c7a3cef0":"markdown","48a7eb4f":"markdown","2de07c46":"markdown","30b7f3df":"markdown","0b675d5a":"markdown","f1a1794f":"markdown","01d6b89f":"markdown","0994be49":"markdown"},"source":{"ccc4c6d1":"# import packages\nimport os\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport missingno as msno\nfrom collections import Counter\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n%matplotlib inline\n\n# list all files\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7d074e3d":"def readfile(filevar): \n    try: \n        df = pd.read_csv(filevar)\n        return df\n    except:\n        print('File loading - Failed!')","aee43540":"# LOAD THE TRAINING SET\ndf_train = readfile('\/kaggle\/input\/home-credit-default-risk\/application_train.csv')\ndf_train.info(verbose=True, null_counts=True)","2db003d6":"# LOAD THE TEST SET\ndf_test = readfile('\/kaggle\/input\/home-credit-default-risk\/application_test.csv')\ndf_test.info(verbose=True, null_counts=True)","b5b93ad6":"# Print first 5 rows of the training file\ndf_train.head()","cce07e94":"# Show target distribution\nsns.set_style('darkgrid')\nprint(df_train.TARGET.value_counts())\ndf_train.TARGET.plot.hist(color='mediumseagreen').set_xlabel('Target value: 0 or 1');","fb7829cf":"# COLUMNS TO BE DROPPED\nlist_col_drop = ['SK_ID_CURR']","bc32073c":"tempY=df_train[df_train.FLAG_OWN_CAR=='Y']\ntempN=df_train[df_train.FLAG_OWN_CAR=='N']\ntempY_targ1=tempY[tempY.TARGET==1]\ntempN_targ1=tempN[tempN.TARGET==1]","cf2509f8":"print('People count who owns a car:',len(tempY),'(',round(len(tempY)\/len(df_train.index)*100,2),'%)')\nprint('People count who DOES NOT own a car:',len(tempN),'(',round(len(tempN)\/len(df_train.index)*100,2),'%)')\nprint('Percentage of people who defaulted (with cars):',round(len(tempY_targ1)\/len(tempY)*100,2),'%')\nprint('Percentage of people who defaulted (no cars):',round(len(tempN_targ1)\/len(tempN)*100,2),'%')","7be7d8fe":"tempY=df_train[df_train.FLAG_OWN_REALTY=='Y']\ntempN=df_train[df_train.FLAG_OWN_REALTY=='N']\ntempY_targ1=tempY[tempY.TARGET==1]\ntempN_targ1=tempN[tempN.TARGET==1]","f76d6528":"print('People count who owns realty:',len(tempY),'(',round(len(tempY)\/len(df_train.index)*100,2),'%)')\nprint('People count who DOES NOT own realty:',len(tempN),'(',round(len(tempN)\/len(df_train.index)*100,2),'%)')\nprint('Percentage of people who defaulted (with realty):',round(len(tempY_targ1)\/len(tempY)*100,2),'%')\nprint('Percentage of people who defaulted (no realty):',round(len(tempN_targ1)\/len(tempN)*100,2),'%')","e28d717d":"# CREATE NEW COLUMN : 0 - none, 1 - with car no realty, 2 - no car with realty, 3 - with car with realty\nlist_col_new_asset = ['FLAG_OWN_CAR','FLAG_OWN_REALTY'] ","511474fe":"sns.pairplot(df_train[['NAME_CONTRACT_TYPE','AMT_CREDIT','AMT_ANNUITY','AMT_GOODS_PRICE']],hue='NAME_CONTRACT_TYPE');","e49e0f91":"corr1=round(df_train.AMT_CREDIT.corr(df_train.AMT_GOODS_PRICE),2)\ncorr2=round(df_train.AMT_ANNUITY.corr(df_train.AMT_CREDIT),2)\ncorr3=round(df_train.AMT_ANNUITY.corr(df_train.AMT_GOODS_PRICE),2)","daed83e8":"print('Correlation of Credit amount vs Price of goods:',corr1)\nprint('Correlation of Annuity amount vs Credit amount:',corr2)\nprint('Correlation of Annuity amount vs Price of goods:',corr3)","fba4e946":"cash=df_train[df_train.NAME_CONTRACT_TYPE == 'Cash loans']\nrev=df_train[df_train.NAME_CONTRACT_TYPE == 'Revolving loans']\ndef_cash=cash[cash.TARGET==1]\ndef_rev=rev[rev.TARGET==1]","ffe04d9f":"print('Percentage of defaulted cash loan:',round(len(def_cash)\/len(cash)*100,2,),'%')\nprint('Percentage of defaulted revolving loan:',round(len(def_rev)\/len(rev)*100,2),'%')\nsns.catplot(data=df_train,x='NAME_CONTRACT_TYPE',hue='TARGET',kind='count');","ff47a546":"# COLUMNS TO BE DROPPED\nlist_col_drop.extend(['NAME_CONTRACT_TYPE','AMT_GOODS_PRICE'])\n\n# CREATE NEW COLUMN : PERCENT_ANNUITY_INCOME\nlist_col_new_annuity = ['AMT_ANNUITY','AMT_INCOME_TOTAL']\n\n# CREATE NEW COLUMN : CREDIT_ANNUITY_INCOME\nlist_col_new_credit = ['AMT_CREDIT','AMT_INCOME_TOTAL']","634eaf54":"sns.catplot(data=df_train,x='NAME_FAMILY_STATUS',hue='TARGET',kind='count');\nplt.xticks(rotation=90);","ee7f6de4":"df_train[['CNT_CHILDREN','NAME_FAMILY_STATUS','CNT_FAM_MEMBERS']][df_train.NAME_FAMILY_STATUS=='Married'].tail()","78939e55":"# COLUMNS TO BE DROPPED\nlist_col_drop.extend(['CNT_CHILDREN', 'NAME_FAMILY_STATUS'])\n\n# COMPLETE COLUMN : \nlist_col_fill_fam = ['CNT_FAM_MEMBERS']","710635e4":"corr1=round(df_train.OWN_CAR_AGE.corr(df_train.TARGET),2)\ncorr2=round(df_train.REG_REGION_NOT_WORK_REGION.corr(df_train.TARGET),2)\ncorr3=round(df_train.REG_CITY_NOT_WORK_CITY.corr(df_train.TARGET),2)\nno_car,yes_car = df_train.FLAG_OWN_CAR.value_counts()","48f4eac1":"print('Correlation of Age of Car vs Target:',corr1)\nprint('Correlation of Registered Region aint Work Region vs Target:',corr2)\nprint('Correlation of Registered City aint Work City vs Target:',corr3)","515d234b":"df_train[['OWN_CAR_AGE','REG_REGION_NOT_WORK_REGION','REG_CITY_NOT_WORK_CITY']].describe()","b27bd49e":"print('How many customers own a car? :',yes_car)\nprint('How many customers do NOT own a car? :',no_car)\nprint('How many missing values on OWN_CAR_AGE? :',df_train.OWN_CAR_AGE.isnull().sum())","b14c21da":"# CREATE NEW COLUMN : EXPENDITURE_CAR : 0 - no car, +=1 per age band\nlist_col_new_car = ['OWN_CAR_AGE']\n\n# COLUMNS TO BE DROPPED\nlist_col_drop.extend(['REG_REGION_NOT_WORK_REGION','REG_CITY_NOT_WORK_CITY'])","db7b436a":"temp = ['NAME_HOUSING_TYPE',\n        'APARTMENTS_AVG',\n        'BASEMENTAREA_AVG',\n        'YEARS_BEGINEXPLUATATION_AVG',\n        'YEARS_BUILD_AVG',\n        'COMMONAREA_AVG',\n        'ELEVATORS_AVG',\n        'ENTRANCES_AVG',\n        'FLOORSMAX_AVG',\n        'FLOORSMIN_AVG',\n        'LANDAREA_AVG',\n        'LIVINGAPARTMENTS_AVG',\n        'LIVINGAREA_AVG',\n        'NONLIVINGAPARTMENTS_AVG',\n        'NONLIVINGAREA_AVG',\n        'APARTMENTS_MODE',\n        'BASEMENTAREA_MODE',\n        'YEARS_BEGINEXPLUATATION_MODE',\n        'YEARS_BUILD_MODE',\n        'COMMONAREA_MODE',\n        'ELEVATORS_MODE',\n        'ENTRANCES_MODE',\n        'FLOORSMAX_MODE',\n        'FLOORSMIN_MODE',\n        'LANDAREA_MODE',\n        'LIVINGAPARTMENTS_MODE',\n        'LIVINGAREA_MODE',\n        'NONLIVINGAPARTMENTS_MODE',\n        'NONLIVINGAREA_MODE',\n        'APARTMENTS_MEDI',\n        'BASEMENTAREA_MEDI',\n        'YEARS_BEGINEXPLUATATION_MEDI',\n        'YEARS_BUILD_MEDI',\n        'COMMONAREA_MEDI',\n        'ELEVATORS_MEDI',\n        'ENTRANCES_MEDI',\n        'FLOORSMAX_MEDI',\n        'FLOORSMIN_MEDI',\n        'LANDAREA_MEDI',\n        'LIVINGAPARTMENTS_MEDI',\n        'LIVINGAREA_MEDI',\n        'NONLIVINGAPARTMENTS_MEDI',\n        'NONLIVINGAREA_MEDI',\n        'FONDKAPREMONT_MODE',\n        'HOUSETYPE_MODE',\n        'TOTALAREA_MODE',\n        'WALLSMATERIAL_MODE',\n        'EMERGENCYSTATE_MODE']","67268bb4":"# Nullity by column\nmsno.bar(df_train[temp],figsize=(20,5));","84a5ad9a":"msno.heatmap(df_train[temp],fontsize=12);","4ff8ae28":"for i in ['APARTMENTS_AVG','LANDAREA_AVG','LIVINGAPARTMENTS_AVG','NONLIVINGAREA_MEDI']:\n    temp=df_train[['NAME_HOUSING_TYPE']][df_train[i].isnull()]\n    sns.catplot(data=temp,x='NAME_HOUSING_TYPE',kind='count',palette=\"rocket\")\n    plt.xticks(rotation=20)\n    title = 'Housing Types with null values on ' + i\n    plt.title(title)","6cce5797":"# CREATE NEW COLUMN : EXPENDITURE_HOUSE \nlist_col_new_house = ['NAME_HOUSING_TYPE',\n                    'APARTMENTS_AVG',\n                    'BASEMENTAREA_AVG',\n                    'YEARS_BEGINEXPLUATATION_AVG',\n                    'YEARS_BUILD_AVG',\n                    'COMMONAREA_AVG',\n                    'ELEVATORS_AVG',\n                    'ENTRANCES_AVG',\n                    'FLOORSMAX_AVG',\n                    'FLOORSMIN_AVG',\n                    'LANDAREA_AVG',\n                    'LIVINGAPARTMENTS_AVG',\n                    'LIVINGAREA_AVG',\n                    'NONLIVINGAPARTMENTS_AVG',\n                    'NONLIVINGAREA_AVG',\n                    'APARTMENTS_MODE',\n                    'BASEMENTAREA_MODE',\n                    'YEARS_BEGINEXPLUATATION_MODE',\n                    'YEARS_BUILD_MODE',\n                    'COMMONAREA_MODE',\n                    'ELEVATORS_MODE',\n                    'ENTRANCES_MODE',\n                    'FLOORSMAX_MODE',\n                    'FLOORSMIN_MODE',\n                    'LANDAREA_MODE',\n                    'LIVINGAPARTMENTS_MODE',\n                    'LIVINGAREA_MODE',\n                    'NONLIVINGAPARTMENTS_MODE',\n                    'NONLIVINGAREA_MODE',\n                    'APARTMENTS_MEDI',\n                    'BASEMENTAREA_MEDI',\n                    'YEARS_BEGINEXPLUATATION_MEDI',\n                    'YEARS_BUILD_MEDI',\n                    'COMMONAREA_MEDI',\n                    'ELEVATORS_MEDI',\n                    'ENTRANCES_MEDI',\n                    'FLOORSMAX_MEDI',\n                    'FLOORSMIN_MEDI',\n                    'LANDAREA_MEDI',\n                    'LIVINGAPARTMENTS_MEDI',\n                    'LIVINGAREA_MEDI',\n                    'NONLIVINGAPARTMENTS_MEDI',\n                    'NONLIVINGAREA_MEDI',\n                    'FONDKAPREMONT_MODE',\n                    'HOUSETYPE_MODE',\n                    'TOTALAREA_MODE',\n                    'WALLSMATERIAL_MODE',\n                    'EMERGENCYSTATE_MODE']","c9bb62bf":"fig,ax = plt.subplots(figsize=(10,7))\nsns.countplot(data=df_train,x='REGION_POPULATION_RELATIVE',hue='TARGET',ax=ax);\nplt.xticks(rotation=90)\nplt.show()","62580945":"sns.catplot(data=df_train,x='WEEKDAY_APPR_PROCESS_START',hue='TARGET',kind='count');\nplt.xticks(rotation=90);","54e5381c":"fig,ax = plt.subplots(figsize=(10,3))\nsns.countplot(data=df_train,x='HOUR_APPR_PROCESS_START',hue='TARGET',ax=ax);\nplt.xticks(rotation=90)\nplt.show()","99af3e77":"# COLUMNS TO BE DROPPED\nlist_col_drop.extend(['WEEKDAY_APPR_PROCESS_START','HOUR_APPR_PROCESS_START'])","d0b53d23":"temp=df_train[['TARGET','EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']]\nprint(temp.info())\nsns.pairplot(temp,hue='TARGET');","93b2db53":"# COMPLETE COLUMNS : Base on mean EXT_SOURCE\nlist_col_fill_ext = ['EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3']","1ddd895a":"df_train[['AMT_INCOME_TOTAL','NAME_INCOME_TYPE','DAYS_EMPLOYED','OCCUPATION_TYPE','ORGANIZATION_TYPE']].describe(include='all')","364d18fc":"def get_thresh(df,field):\n    \"\"\" Outliers are usually > 3 standard deviations away from the mean. \"\"\"\n    ave=np.mean(df[field])\n    sdev=np.std(df[field])\n    threshold=round(ave+(3*sdev),2)\n    print('Threshold for',field,':',threshold)\n    return threshold","5e473a02":"thresh_income = get_thresh(df_train,'AMT_INCOME_TOTAL')\nanomaly_emp = int(df_train['DAYS_EMPLOYED'][df_train['DAYS_EMPLOYED']>0].unique())\ntemp_orig=df_train[['AMT_INCOME_TOTAL','DAYS_EMPLOYED']]\ntemp_no_outliers=df_train[['AMT_INCOME_TOTAL','DAYS_EMPLOYED']][(df_train.AMT_INCOME_TOTAL<thresh_income)&(df_train['DAYS_EMPLOYED']<=0)]\nprint('Anomalous data for DAYS_EMPLOYED :',anomaly_emp)","4991d703":"def plotdist(df,f1,f2):\n    f,axes = plt.subplots(1,2,figsize=(10,3))\n    sns.distplot(df[[f1]],ax=axes[0]).set_title(f1)\n    plt.xticks(rotation=75)\n\n    sns.distplot(df[[f2]],ax=axes[1]).set_title(f2)\n    plt.xticks(rotation=75)\n    plt.tight_layout()","45ab16f7":"# AMT_INCOME_TOTAL, DAYS_EMPLOYED --> ORIGINAL VALUE WITH OUTLIERS\nplotdist(temp_orig,'AMT_INCOME_TOTAL','DAYS_EMPLOYED')","659b2604":"# AMT_INCOME_TOTAL, DAYS_EMPLOYED --> OUTLIERS REMOVED\nplotdist(temp_no_outliers,'AMT_INCOME_TOTAL','DAYS_EMPLOYED')","59da099f":"# NUMBER OF INDIVIDUALS HAVING THE DAYS EMPLOYED ANOMALOUS DATA\nlen(df_train[df_train.DAYS_EMPLOYED==anomaly_emp])","28f49f50":"unpaid=df_train[df_train.TARGET==1]\nsns.catplot(data=unpaid,x='NAME_INCOME_TYPE',kind='count');\nplt.xticks(rotation=90);\nplt.title('Income Stream of DEFAULTED LOANS');","cba15c30":"fig,ax = plt.subplots(figsize=(15,5))\nsns.countplot(data=unpaid,x='ORGANIZATION_TYPE',ax=ax);\nplt.xticks(rotation=90)\nplt.title('Organization of people with DEFAULTED LOANS')\nplt.show()","621db098":"def plotstrip(df,xval,yval,hueval,yfig):\n    fig,ax = plt.subplots(figsize=(15,yfig))\n    sns.stripplot(x=xval,y=yval,hue=hueval,data=df,alpha=0.5,jitter=0.8,dodge=True,ax=ax).set_title(yval);\n    plt.legend(bbox_to_anchor=(1.05, 1))\n    plt.show()","93c3a092":"temp=df_train[df_train.OCCUPATION_TYPE.isnull()]\nprint('Individuals that left OCCUPATION_TYPE field blank:')\nprint(temp.NAME_INCOME_TYPE.value_counts())","730e1639":"print('Individuals that left ORGANIZATION_TYPE field blank (Top 10 only):')\nprint(temp.ORGANIZATION_TYPE.value_counts().head(10))","c692c355":"plotstrip(df_train[df_train.AMT_INCOME_TOTAL<thresh_income],'AMT_INCOME_TOTAL','NAME_INCOME_TYPE','TARGET',5)","4883b585":"plotstrip(df_train[df_train.AMT_INCOME_TOTAL<thresh_income],'AMT_INCOME_TOTAL','OCCUPATION_TYPE','TARGET',10)","bb064cf1":"plotstrip(df_train[df_train.AMT_INCOME_TOTAL<thresh_income],'AMT_INCOME_TOTAL','ORGANIZATION_TYPE','TARGET',20)","12840ad0":"# COLUMNS TO BE DROPPED\nlist_col_drop.extend(['OCCUPATION_TYPE','NAME_INCOME_TYPE'])\n\n# CREATE NEW COLUMN : INCOME_BAND\nlist_col_new_income = ['AMT_INCOME_TOTAL'] \n\n# CONVERT COLUMN : 365243 to -29200 \nlist_col_conv_daysemp = ['DAYS_EMPLOYED']\n\n# CREATE NEW COLUMN : YEARS_EMPLOYED\nlist_col_new_yrsemp = ['DAYS_EMPLOYED']\n\n# CONVERT COLUMN : \nlist_col_conv_org = ['ORGANIZATION_TYPE'] ","f6a0b736":"df_train[['CODE_GENDER','NAME_TYPE_SUITE','NAME_EDUCATION_TYPE','DAYS_REGISTRATION','DAYS_ID_PUBLISH']].describe(include='all')","fd989e47":"g = sns.FacetGrid(df_train,row='CODE_GENDER',col='NAME_EDUCATION_TYPE',hue='TARGET',height=4) # nominal\ng.map(plt.scatter,'DAYS_ID_PUBLISH','DAYS_REGISTRATION',alpha=0.5,edgecolor='k',linewidth=0.5) # continuous\n\nfig = g.fig \nfig.set_size_inches(25,10)\nfig.subplots_adjust(top=0.85,wspace=0.3)\nfig.suptitle('Gender - Educational Attainment - Registration Change - ID Change - Credit Ranking',fontsize=20)\n\nl = g.add_legend(title='Credit Score')","73ab0be2":"paid = df_train[df_train.TARGET==0]\nunpaid = df_train[df_train.TARGET==1]\n\nf,axes = plt.subplots(1,2,figsize=(10,3))\nsns.kdeplot(paid['DAYS_ID_PUBLISH'],paid['DAYS_REGISTRATION'],cmap=\"Blues\",shade=True,shade_lowest=False,ax=axes[0]).set_title('Paid');\nsns.kdeplot(unpaid['DAYS_ID_PUBLISH'],unpaid['DAYS_REGISTRATION'],cmap=\"Reds\",shade=True,shade_lowest=False,ax=axes[1]).set_title('Unpaid');\nsns.set_style('whitegrid')\nplt.tight_layout()","7b30ee12":"df_train.NAME_EDUCATION_TYPE.value_counts()","65ea54df":"f,axes = plt.subplots(1,2,figsize=(10,5),sharex=True)\nsns.distplot(df_train[['DAYS_BIRTH']][df_train.TARGET==0],hist=False,color=\"b\",kde_kws={\"shade\":True},ax=axes[0]).set_title('Target == 0 (Paid)');\nsns.distplot(df_train[['DAYS_BIRTH']][df_train.TARGET==1],hist=False,color=\"r\",kde_kws={\"shade\":True},ax=axes[1]).set_title('Target == 1 (Unpaid)');","dcc0bc1c":"# COLUMNS TO BE DROPPED\nlist_col_drop.extend(['NAME_TYPE_SUITE','DAYS_REGISTRATION','DAYS_ID_PUBLISH'])\n\n# CONVERT COLUMN : \nlist_col_conv_gender = ['CODE_GENDER']\n\n# CONVERT COLUMN :\nlist_col_conv_edu = ['NAME_EDUCATION_TYPE']\n\n# CREATE NEW COLUMN : AGE\nlist_col_new_age = ['DAYS_BIRTH']\n","4696a0c1":"temp1=['FLAG_MOBIL',\n'FLAG_EMP_PHONE',\n'FLAG_WORK_PHONE',\n'FLAG_CONT_MOBILE',\n'FLAG_PHONE',\n'FLAG_EMAIL']\n\ntemp2=['FLAG_DOCUMENT_2',\n'FLAG_DOCUMENT_3',\n'FLAG_DOCUMENT_4',\n'FLAG_DOCUMENT_5',\n'FLAG_DOCUMENT_6',\n'FLAG_DOCUMENT_7',\n'FLAG_DOCUMENT_8',\n'FLAG_DOCUMENT_9',\n'FLAG_DOCUMENT_10',\n'FLAG_DOCUMENT_11',\n'FLAG_DOCUMENT_12',\n'FLAG_DOCUMENT_13',\n'FLAG_DOCUMENT_14',\n'FLAG_DOCUMENT_15',\n'FLAG_DOCUMENT_16',\n'FLAG_DOCUMENT_17',\n'FLAG_DOCUMENT_18',\n'FLAG_DOCUMENT_19',\n'FLAG_DOCUMENT_20',\n'FLAG_DOCUMENT_21']","0bb87509":"df_train[temp1+temp2].describe()","1bbcbe18":"def featsum(cols,newcol):\n    \"\"\" Sums up items per row across all columns.\n        Returns df with new sum column and catplot.\n    \"\"\"\n    sample_count=df_train[cols].sum(axis=1)\n    sample = df_train.copy()\n    sample[newcol]=sample_count\n    sns.catplot(data=sample,x=newcol,hue='TARGET',kind='count');","16d62172":"featsum(temp1,'FlagContact')","12162ef4":"featsum(temp2,'FlagDocu')","e5f554c8":"# CREATE NEW COLUMN : FLAG_CONTACTS\nlist_col_new_flagCont = [ \n    'FLAG_MOBIL',\n    'FLAG_EMP_PHONE',\n    'FLAG_WORK_PHONE',\n    'FLAG_CONT_MOBILE',\n    'FLAG_PHONE',\n    'FLAG_EMAIL']\n\n# CREATE NEW COLUMN : FLAG_DOCS\nlist_col_new_flagDoc = [ \n    'FLAG_DOCUMENT_2',\n    'FLAG_DOCUMENT_3',\n    'FLAG_DOCUMENT_4',\n    'FLAG_DOCUMENT_5',\n    'FLAG_DOCUMENT_6',\n    'FLAG_DOCUMENT_7',\n    'FLAG_DOCUMENT_8',\n    'FLAG_DOCUMENT_9',\n    'FLAG_DOCUMENT_10',\n    'FLAG_DOCUMENT_11',\n    'FLAG_DOCUMENT_12',\n    'FLAG_DOCUMENT_13',\n    'FLAG_DOCUMENT_14',\n    'FLAG_DOCUMENT_15',\n    'FLAG_DOCUMENT_16',\n    'FLAG_DOCUMENT_17',\n    'FLAG_DOCUMENT_18',\n    'FLAG_DOCUMENT_19',\n    'FLAG_DOCUMENT_20',\n    'FLAG_DOCUMENT_21']","050b455e":"def plotcat(x,r):\n    sns.catplot(data=df_train,x=x,hue='TARGET',kind='count');\n    plt.xticks(rotation=r);","80577e01":"plotcat('REGION_RATING_CLIENT',0)","761af994":"plotcat('REGION_RATING_CLIENT_W_CITY',0)","f6cc0374":"print('Correlation:',round(df_train['REGION_RATING_CLIENT_W_CITY'].corr(df_train['REGION_RATING_CLIENT']),2))","499cb8bb":"temp=[\n    'REG_REGION_NOT_LIVE_REGION',\n    'LIVE_REGION_NOT_WORK_REGION',\n    'REG_CITY_NOT_LIVE_CITY',\n    'LIVE_CITY_NOT_WORK_CITY'\n]","a23a494a":"featsum(temp,'FlagAddr')","a5de56b6":"# COLUMNS TO BE DROPPED\nlist_col_drop.extend(['REGION_RATING_CLIENT_W_CITY'])\n\n# CREATE NEW COLUMN : FLAG_ADDR\nlist_col_new_flagAddr = ['REG_REGION_NOT_LIVE_REGION','LIVE_REGION_NOT_WORK_REGION','REG_CITY_NOT_LIVE_CITY','LIVE_CITY_NOT_WORK_CITY']","b8011ef3":"df_train[['OBS_30_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE','DEF_60_CNT_SOCIAL_CIRCLE','DAYS_LAST_PHONE_CHANGE']].describe()","15e84b00":"def featsumviolin(df,cols1,cols2,newcol1,newcol2):\n    \"\"\" Sums up items per row across all columns.\n        Returns df with new sum column and violinplot.\n    \"\"\"\n    sample_count1=df[cols1].sum(axis=1)\n    sample_count2=df[cols2].sum(axis=1)\n    sample = df_train.copy()\n    sample[newcol1]=sample_count1\n    sample[newcol2]=sample_count2\n    fig,ax = plt.subplots(figsize=(10,5))\n    sns.violinplot(data=sample,hue='TARGET',x=newcol1,y=newcol2,split=True,inner='quart',linewidth=1.3,palette={1:\"#FF9999\", 0:\"white\"});","867d3ab2":"featsumviolin(df_train[(df_train.OBS_30_CNT_SOCIAL_CIRCLE<348)&(df_train.OBS_60_CNT_SOCIAL_CIRCLE<344)],\n              ['DEF_30_CNT_SOCIAL_CIRCLE','DEF_60_CNT_SOCIAL_CIRCLE'],\n              ['OBS_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE'],\n              'DEF_NEW','OBS_NEW')","bf84df34":"sns.set(palette=\"muted\",color_codes=True)\nf,axes = plt.subplots(2,2,figsize=(10,5),sharex=True)\n\nsns.distplot(df_train[['DAYS_LAST_PHONE_CHANGE']][df_train.TARGET==0],kde=False,color=\"b\",ax=axes[0,0]).set_title('Target == 0 (Paid)')\nsns.distplot(df_train[['DAYS_LAST_PHONE_CHANGE']][df_train.TARGET==0],hist=False,color=\"g\",kde_kws={\"shade\":True},ax=axes[0,1]).set_title('Target == 0 (Paid)')\n\nsns.distplot(df_train[['DAYS_LAST_PHONE_CHANGE']][df_train.TARGET==1],kde=False,color=\"r\",ax=axes[1,0]).set_title('Target == 1 (Unpaid)')\nsns.distplot(df_train[['DAYS_LAST_PHONE_CHANGE']][df_train.TARGET==1],hist=False,color=\"m\",kde_kws={\"shade\":True},ax=axes[1,1]).set_title('Target == 1 (Unpaid)')\n\nplt.tight_layout()","087462fe":"# COLUMNS TO BE DROPPED\nlist_col_drop.extend(['OBS_30_CNT_SOCIAL_CIRCLE','DEF_30_CNT_SOCIAL_CIRCLE','OBS_60_CNT_SOCIAL_CIRCLE','DEF_60_CNT_SOCIAL_CIRCLE','DAYS_LAST_PHONE_CHANGE'])","954c445c":"df_train[['AMT_REQ_CREDIT_BUREAU_HOUR','AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_QRT','AMT_REQ_CREDIT_BUREAU_YEAR']].describe()","6af7babe":"for i in ['AMT_REQ_CREDIT_BUREAU_HOUR','AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_WEEK']:\n    for j in ['AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_QRT','AMT_REQ_CREDIT_BUREAU_YEAR']:\n        fig,ax = plt.subplots(figsize=(10,3))\n        sns.stripplot(data=df_train[df_train.AMT_REQ_CREDIT_BUREAU_QRT<261],x=i,y=j,hue='TARGET',alpha=0.5,jitter=0.3,dodge=True,ax=ax)\n        plt.show()","2405bc45":"# COLUMNS TO BE DROPPED\nlist_col_drop.extend(['AMT_REQ_CREDIT_BUREAU_HOUR','AMT_REQ_CREDIT_BUREAU_DAY','AMT_REQ_CREDIT_BUREAU_WEEK','AMT_REQ_CREDIT_BUREAU_MON','AMT_REQ_CREDIT_BUREAU_QRT','AMT_REQ_CREDIT_BUREAU_YEAR'])","ca664b0f":"def getmean(df,ls_cols):\n    list_mean = []\n    for i in ls_cols:\n        mean_val = df[i].mean()\n        list_mean.append(mean_val)\n    return list_mean\n\ndef fill_ave_ext(df,ls_cols):  \n    list_mean = getmean(df,ls_cols) # mean of EXT_SOURCE_*\n    ctr=0\n    for i in ls_cols:\n        df[i] = df[i].fillna(list_mean[ctr])\n        ctr+=1\n    return df\n        \n# Fill in the training set\nfill_ave_ext(df_train,list_col_fill_ext);\n\n# Fill in the testing set\nfill_ave_ext(df_test,list_col_fill_ext);","c6ea82c6":"# NO MORE NULL VALUES FOR 'EXT_SOURCE_1','EXT_SOURCE_2','EXT_SOURCE_3'\nprint(df_train[list_col_fill_ext].info())\nprint(df_test[list_col_fill_ext].info())","eb4a3ce3":"def fill_0_fam(df,ls_cols):\n    df[ls_cols] = df[ls_cols].fillna(0)\n    return df\n    \n    \nfill_0_fam(df_train,'CNT_FAM_MEMBERS');\nfill_0_fam(df_test,'CNT_FAM_MEMBERS');","081ec03b":"# NO MORE NULL VALUES FOR 'CNT_FAM_MEMBERS'\nprint(df_train[list_col_fill_fam].info())\nprint(df_test[list_col_fill_fam].info())","e2fad994":"# ANOMALY DATA COUNT BEFORE CONVERSION\nprint('Train set :',len(df_train[df_train.DAYS_EMPLOYED==365243]))\nprint('Test set  :',len(df_test[df_test.DAYS_EMPLOYED==365243]))","50fa8c6a":"def conv_daysemp(df,ls_cols):\n    df[ls_cols[0]].replace(to_replace=365243,value=-29200,inplace=True)\n    return df","cdca62da":"conv_daysemp(df_train,list_col_conv_daysemp);\nconv_daysemp(df_test,list_col_conv_daysemp);","d3c6cd71":"# ANOMALY DATA COUNT AFTER CONVERSION\nprint('Train set :',len(df_train[df_train.DAYS_EMPLOYED==365243]))\nprint('Test set  :',len(df_test[df_test.DAYS_EMPLOYED==365243]))","b2e49c5d":"# BEFORE CONVERSION\nprint(df_train[['CODE_GENDER','NAME_EDUCATION_TYPE','ORGANIZATION_TYPE']].head(3))\nprint(df_test[['CODE_GENDER','NAME_EDUCATION_TYPE','ORGANIZATION_TYPE']].head(3))","b535096f":"def conv_gender(df,ls_cols):\n    df[ls_cols[0]] = df[ls_cols[0]].map({'XNA':0,'M':1,'F':2}).astype(int)\n    return df\n\ndef conv_education(df,ls_cols):\n    temp_dict = {\n        'Lower secondary':1,\n        'Secondary \/ secondary special':2,\n        'Incomplete higher':3,\n        'Higher education':4,\n        'Academic degree':5\n    }\n    df[ls_cols[0]] = df[ls_cols[0]].map(temp_dict).astype(int)\n    return df\n\ndef conv_org(df,ls_cols):\n    ls_ctr=[i for i in np.arange(1,len(ls_cols)+1)]\n    temp_dict = dict(zip(ls_cols,ls_ctr))\n    df['ORGANIZATION_TYPE'] = df['ORGANIZATION_TYPE'].map(temp_dict).astype(int)\n    return df","03867fc3":"conv_gender(df_train,list_col_conv_gender);\nconv_gender(df_test,list_col_conv_gender);\n\nconv_education(df_train,list_col_conv_edu);\nconv_education(df_test,list_col_conv_edu);","a1cb6751":"orgtype = sorted(df_train.ORGANIZATION_TYPE.unique())\nconv_org(df_train,orgtype);\nconv_org(df_test,orgtype);","7e2caf33":"# AFTER CONVERSION\nprint(df_train[['CODE_GENDER','NAME_EDUCATION_TYPE','ORGANIZATION_TYPE']].head(3))\nprint(df_test[['CODE_GENDER','NAME_EDUCATION_TYPE','ORGANIZATION_TYPE']].head(3))","49cc675f":"# CREATE NEW COLUMN : 0 - none, 1 - with car no realty, 2 - no car with realty, 3 - with car with realty\nlist_col_new_asset","dd8f12f5":"def create_asset(df):\n    df['FLAG_ASSET'] = np.nan\n    filter_0 = (df.FLAG_OWN_CAR=='N')&(df.FLAG_OWN_REALTY=='N')\n    filter_1 = (df.FLAG_OWN_CAR=='Y')&(df.FLAG_OWN_REALTY=='N')\n    filter_2 = (df.FLAG_OWN_CAR=='N')&(df.FLAG_OWN_REALTY=='Y')\n    filter_3 = (df.FLAG_OWN_CAR=='Y')&(df.FLAG_OWN_REALTY=='Y')\n    \n    df['FLAG_ASSET'][filter_0] = 0\n    df['FLAG_ASSET'][filter_1] = 1\n    df['FLAG_ASSET'][filter_2] = 2\n    df['FLAG_ASSET'][filter_3] = 3\n    return df\n\n    \ncreate_asset(df_train);\ncreate_asset(df_test);","a51a9132":"# SAMPLE OUTPUT\ndf_test[['FLAG_OWN_CAR','FLAG_OWN_REALTY','FLAG_ASSET']].head()","fc98c730":"# SINCE WE NOW HAVE AN ASSET INDICATOR, WE CAN NOW REMOVE list_col_new_asset \nlist_col_drop.extend(list_col_new_asset)","db6f42bc":"# CREATE NEW COLUMN : FLAG_CONTACTS\nlist_col_new_flagCont","a86f5435":"def create_sum_cols(df,ls_cols,newcol):\n    \"\"\" Sums up items per row across all columns.\n        Returns df with new sum column.\n    \"\"\"\n    df[newcol] = np.nan\n    ls_sum_value = df[ls_cols].sum(axis=1)\n    df[newcol] = ls_sum_value\n    return df","497952ba":"create_sum_cols(df_train,list_col_new_flagCont,'FLAG_CONTACTS');\ncreate_sum_cols(df_test,list_col_new_flagCont,'FLAG_CONTACTS');","bce0a756":"df_test[['FLAG_CONTACTS']].head()","71d55d4b":"# CREATE NEW COLUMN : FLAG_DOCS\nlist_col_new_flagDoc","5e40f639":"create_sum_cols(df_train,list_col_new_flagDoc,'FLAG_DOCS');\ncreate_sum_cols(df_test,list_col_new_flagDoc,'FLAG_DOCS');","417e90df":"df_test[['FLAG_DOCS']].head()","0effc49a":"# CREATE NEW COLUMN : FLAG_ADDR\nlist_col_new_flagAddr","297da6b0":"create_sum_cols(df_train,list_col_new_flagAddr,'FLAG_ADDR');\ncreate_sum_cols(df_test,list_col_new_flagAddr,'FLAG_ADDR');","42a30a22":"df_test[['FLAG_ADDR']].head()","c694eb5b":"# SINCE WE NOW HAVE FLAG COMPLIANCE FIELDS, WE CAN NOW REMOVE EXISTING COLUMNS \nlist_col_drop.extend(list_col_new_flagCont)\nlist_col_drop.extend(list_col_new_flagDoc)\nlist_col_drop.extend(list_col_new_flagAddr)","a2122833":"# CREATE NEW COLUMN : AGE\nlist_col_new_age ","871ad47e":"def create_day_to_year(df,ls_cols,newcol):\n    df[newcol] = round(np.abs(df[ls_cols[0]]\/365))\n    return df","47b77133":"create_day_to_year(df_train,list_col_new_age,'AGE');\ncreate_day_to_year(df_test,list_col_new_age,'AGE');","ed900497":"f,ax=plt.subplots(figsize=(10,5))\nsns.countplot(data=df_train[df_train.TARGET==1],x='AGE',hue='TARGET').set_title('Age of Individuals with default loans')\nplt.xticks(rotation=90);","b868ebb4":"# CREATE NEW COLUMN : YEARS_EMPLOYED\nlist_col_new_yrsemp","d319df2e":"create_day_to_year(df_train,list_col_new_yrsemp,'YEARS_EMPLOYED');\ncreate_day_to_year(df_test,list_col_new_yrsemp,'YEARS_EMPLOYED');","602d8a14":"df_test[['YEARS_EMPLOYED']].head()","82a7d4e8":"# SINCE WE NOW HAVE NEW FEATURES, WE CAN NOW REMOVE EXISTING COLUMNS \nlist_col_drop.extend(list_col_new_age)\nlist_col_drop.extend(list_col_new_yrsemp)","64b3049e":"# CREATE NEW COLUMN : INCOME_BAND\nlist_col_new_income","3cd9685d":"# Create INCOME_BAND to group individuals per income range\ndef create_income_band(df):\n    df.loc[(df.AMT_INCOME_TOTAL < 30000),'INCOME_BAND'] = 1\n    df.loc[(df.AMT_INCOME_TOTAL >= 30000)&(df.AMT_INCOME_TOTAL < 65000),'INCOME_BAND'] = 2\n    df.loc[(df.AMT_INCOME_TOTAL >= 65000)&(df.AMT_INCOME_TOTAL < 95000),'INCOME_BAND'] = 3\n    df.loc[(df.AMT_INCOME_TOTAL >= 95000)&(df.AMT_INCOME_TOTAL < 130000),'INCOME_BAND'] = 4\n    df.loc[(df.AMT_INCOME_TOTAL >= 130000)&(df.AMT_INCOME_TOTAL < 160000),'INCOME_BAND'] = 5\n    df.loc[(df.AMT_INCOME_TOTAL >= 160000)&(df.AMT_INCOME_TOTAL < 190000),'INCOME_BAND'] = 6\n    df.loc[(df.AMT_INCOME_TOTAL >= 190000)&(df.AMT_INCOME_TOTAL < 220000),'INCOME_BAND'] = 7\n    df.loc[(df.AMT_INCOME_TOTAL >= 220000)&(df.AMT_INCOME_TOTAL < 275000),'INCOME_BAND'] = 8\n    df.loc[(df.AMT_INCOME_TOTAL >= 275000)&(df.AMT_INCOME_TOTAL < 325000),'INCOME_BAND'] = 9\n    df.loc[(df.AMT_INCOME_TOTAL >= 325000),'INCOME_BAND'] = 10\n    return df","18b177b3":"create_income_band(df_train);\ncreate_income_band(df_test);","89c94566":"sns.countplot(data=df_train[df_train.TARGET==1],x='INCOME_BAND',hue='TARGET').set_title('Income of Individuals with default loans');","7bafddc8":"# CREATE NEW COLUMN : PERCENT_ANNUITY_INCOME\nlist_col_new_annuity","4230b76d":"def create_perc(df,col1,col2,newcol):\n    df[newcol] = round((df[col1]\/df[col2])*100,2)\n    df[newcol] = df[newcol].fillna(0)\n    return df","1490d079":"create_perc(df_train,'AMT_ANNUITY','AMT_INCOME_TOTAL','PERCENT_ANNUITY_INCOME');\ncreate_perc(df_test,'AMT_ANNUITY','AMT_INCOME_TOTAL','PERCENT_ANNUITY_INCOME');","8c6f359c":"df_test[['AMT_INCOME_TOTAL','AMT_ANNUITY','PERCENT_ANNUITY_INCOME']].head()","9429723d":"# CREATE NEW COLUMN : PERCENT_CREDIT_INCOME\nlist_col_new_credit","bcce7dd2":"create_perc(df_train,'AMT_INCOME_TOTAL','AMT_CREDIT','PERCENT_CREDIT_INCOME');\ncreate_perc(df_test,'AMT_INCOME_TOTAL','AMT_CREDIT','PERCENT_CREDIT_INCOME');","586d5f51":"df_test[['AMT_INCOME_TOTAL','AMT_CREDIT','PERCENT_CREDIT_INCOME']].head()","0605f60f":"# CREATE NEW COLUMN : EXP_CAR \nlist_col_new_car","cfcbbc6d":"# Create EXP_CAR to group car age\ndef create_car_band(df):\n    df.loc[(df.OWN_CAR_AGE == 0 ),'EXP_CAR'] = 1\n    df.loc[(df.OWN_CAR_AGE >= 1)&(df.OWN_CAR_AGE < 4),'EXP_CAR'] = 2\n    df.loc[(df.OWN_CAR_AGE >= 4)&(df.OWN_CAR_AGE < 8),'EXP_CAR'] = 3\n    df.loc[(df.OWN_CAR_AGE >= 8)&(df.OWN_CAR_AGE < 11),'EXP_CAR'] = 4\n    df.loc[(df.OWN_CAR_AGE >= 11)&(df.OWN_CAR_AGE < 15),'EXP_CAR'] = 5\n    df.loc[(df.OWN_CAR_AGE >= 15)&(df.OWN_CAR_AGE < 20),'EXP_CAR'] = 6\n    df.loc[(df.OWN_CAR_AGE >= 20 ),'EXP_CAR'] = 7\n    df['EXP_CAR'] = df['EXP_CAR'].fillna(0)\n    return df","e4c24917":"create_car_band(df_train);\ncreate_car_band(df_test);","5450de89":"sns.countplot(data=df_train[df_train.TARGET==1],x='EXP_CAR',hue='TARGET').set_title('Car Age band of Individuals with default loans');","95529de8":"# CREATE NEW COLUMN : EXP_HOUSE \nlist_col_new_house;","fc7eae71":"df_train[list_col_new_house].describe()","d581e80e":"list_col_new_house_copy = list_col_new_house.copy()\nlist_col_new_house_copy.extend(['TARGET'])\nplt.subplots(figsize=(10,8))\nsns.heatmap(df_train[list_col_new_house_copy].corr(),vmin=-1,vmax=1,cmap='BrBG');","ec039615":"# DROP THE MEDIAN AND MODE APARTMENT MEASUREMENTS SINCE ALL ARE HIGHLY CORRELATED. CREATE ONE FEATURE FOR THE REMAINING AVG MEASUREMENT.\nlist_col_new_house_avg = list_col_new_house[0:14].copy()","80991898":"create_sum_cols(df_train,list_col_new_house_avg,'EXP_HOUSE');\ncreate_sum_cols(df_test,list_col_new_house_avg,'EXP_HOUSE');","c14b040a":"df_test[['EXP_HOUSE']].head()","720138e2":"# SINCE WE NOW HAVE NEW FEATURES, WE CAN NOW REMOVE EXSITING ONES \nlist_col_drop.extend(list_col_new_income)  \nlist_col_drop.extend(list_col_new_annuity)\nlist_col_drop.extend(list_col_new_credit) \nlist_col_drop.extend(list_col_new_car) \nlist_col_drop.extend(list_col_new_house)  ","4497a32f":"def remove_dups(x):\n  return list(dict.fromkeys(x))","10104a45":"print('Original column list count to drop:',len(list_col_drop)) #114","e180b575":"list_col_drop_new = remove_dups(list_col_drop)","66351207":"print('Column list count to drop (duplicates removed):',len(list_col_drop_new)) #112","ac7fcc48":"# UPDATED DATAFRAME\ndf_train.drop(list_col_drop_new,axis=1,inplace=True)\ndf_test.drop(list_col_drop_new,axis=1,inplace=True)\n\n# PRINT UPDATED DATAFRAME\ndf_train","261735da":"df_train.describe()","e2909e5c":"X = df_train.drop(columns='TARGET',axis=1)\ny = df_train.TARGET\n\nX_pred = df_test","728f312c":"X","eb46af72":"X_pred","22ff8396":"import time\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold","a2b1fdbc":"folds = StratifiedKFold(n_splits=5)\noof_preds = np.zeros(X.shape[0])\nsub_preds = np.zeros(X_pred.shape[0])","5d6d4960":"start = time.time()\nvalid_score = 0\nfor n_fold, (trn_idx, val_idx) in enumerate(folds.split(X, y)):\n    trn_x, trn_y = X.iloc[trn_idx], y[trn_idx]\n    val_x, val_y = X.iloc[val_idx], y[val_idx]    \n    \n    train_data = lgb.Dataset(data=trn_x, label=trn_y)\n    valid_data = lgb.Dataset(data=val_x, label=val_y)\n    \n    param = {'application':'binary','num_iterations':4000, 'learning_rate':0.05, 'num_leaves':24, 'feature_fraction':0.8, 'bagging_fraction':0.9,\n             'lambda_l1':0.1, 'lambda_l2':0.1, 'min_split_gain':0.01, 'early_stopping_round':100, 'max_depth':7, 'min_child_weight':40, 'metric':'auc'}\n    lgb_es_model = lgb.train(param, train_data, valid_sets=[train_data, valid_data], verbose_eval=100) \n    \n    oof_preds[val_idx] = lgb_es_model.predict(val_x, num_iteration=lgb_es_model.best_iteration)\n    sub_preds += lgb_es_model.predict(X_pred, num_iteration=lgb_es_model.best_iteration) \/ folds.n_splits\n    print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(val_y, oof_preds[val_idx])))\n    valid_score += roc_auc_score(val_y, oof_preds[val_idx])\n\nprint('valid score:', str(round(valid_score\/folds.n_splits,4)))\n\nend = time.time()\nprint('training time:', str(round((end - start)\/60)), 'mins')","3d607980":"lgb.plot_importance(lgb_es_model, height=0.5, max_num_features=20, ignore_zero = False, figsize = (12,6), importance_type ='gain')","5b3b903a":"# HOW TO REMOVE FILE\n\"\"\"\nif os.path.isfile('\/kaggle\/working\/application_pred.csv'):\n    os.remove('\/kaggle\/working\/application_pred.csv')\n    print(\"success\")\nelse:    \n    print(\"File doesn't exists!\")\n\"\"\";","e4d6ddd3":"# PREDICT!\napplication_test= pd.read_csv('\/kaggle\/input\/home-credit-default-risk\/application_test.csv')\noutput = pd.DataFrame({'SK_ID_CURR':application_test.SK_ID_CURR, 'TARGET': sub_preds})\noutput.to_csv('application_pred.csv', index=False)","2e78b569":"Observations: \n* These 3 fields are external data source score fields.\n* Base on the plot above, those who were able to pay and did not pay can have scores fairly distributed on EXT_SOURCE fields, but it is quite evident that on the lower end of the normalized score mark (0.0-0.5), customers who paid (target=0, blue color) are much less prominent than those who didn't (target=1, orange color)... and vice versa.\n* All 3 fields have missing values.\n\nDecisions:\n* Use EXT_SOURCE_1, EXT_SOURCE_2, EXT_SOURCE_3 as model features. \n* Complete the fields by filling up the null values base on their mean EXT_SOURCE.","a7455ceb":"<a id='eda'><\/a>\n# Exploratory Data Analysis\n\nWe'll be taking note of the things below during our field inspection:\n\n1. **Completing** - any missing values to fill in?\n2. **Correlation** - which features contribute significantly to our solution goal?\n3. **Correcting** - any outlier that skews our data terribly? Do we have some unusual error seen on the dataset? We may need to correct or exclude inaccurate features.\n4. **Conversion** - most of the time, the text features need to be converted to numerical values for our model training.\n5. **Creating** - maybe we can create a new feature out of the existing set of highly correlated features?","aefb514f":"Expenditure-related: TRAVEL","6dac4bb1":"![image.png](attachment:image.png)","dd799697":"Observations:\n* There are 2 variables related to 30 days past due (30 DPD), and another 2 for 60 days past due (60 DPD). I have combined them and plotted using a violinplot as seen above. From what I understand here, those people that defaulted, and even those who didn't, have equal possibilities of missing a payment on their due date. Though surprisingly, majority of those who paid on a much later date got their loans cleared off.\n> Note: A payment status of 30-days late means that payment is between 30-59 days past the payment due date (30 DPD). A payment status of 60-days late means that payment is between 60-89 days past the payment due date (60 DPD).\n* I plotted separately the days in which a customer last changed their phone (relative to their application) for those having good and bad loan status records. The result is pretty much expected, as most people tend to change phones in a span of 2-3 years only (hence the distribution is skewed to the right). We can see that the people count gets lesser and lesser as the usage of their phone gets longer than the average.\n\nDecisions:\n* Drop the DPD variables.\n* Drop DAYS_LAST_PHONE_CHANGE.","6c69d594":"Observations:\n* AMT_INCOME_TOTAL has an outlier. As per the data: the 75th percentile is equal to 202,500, while the maximum value is equal to a whopping 117,000,000; It is quite a big difference as it means that **75% of our customers** are already making a total income of **202,500 or below** - the remaining 25% has a total income higher than 202,500.\n* AMT_INCOME_TOTAL has a wide range of values, from min val 25,650 to max val 117,000,000.\n* DAYS_EMPLOYED is having inconsistent data. It contains negative values (since it is in the form of 'days' relative to the application), then suddenly there is one very big positive number (365243 days - which equates to 1000 years!) with a significant number of individuals having it.\n* NAME_INCOME_TYPE: The 'working' category is the most dense in terms of low wage high default customers. We also have very few samples on 'unemployed', 'student', 'maternity leave' and 'businessman'. Interesting to see that the 'businessman' category has an above average total income and has a high chance that they will maintain good credit scoring.\n* NAME_INCOME_TYPE 'Pensioner' is almost equal to the DAYS_EMPLOYED anomaly data count! And it is also almost equal to the ORGANIZATION_TYPE value 'XNA' ! It just means that the '1000 years' employment duration was made for retired people.\n* OCCUPATION_TYPE has 31% of its values missing. Majority of the null OCCUPATION_TYPE are from pensioners (which totally makes sense), then other big chunks are from working people, commercial associate, state servant, etc. My take here is that the choices on occupation type field is limited which led these number of working people to leaving it blank.\n* The ORGANIZATION_TYPE is pretty diverse regarding where do these customers work. But base on the histogram, the category where the defaulting individuals are dominant are those in Business Entity Type 3, self-employed, and XNA.\n\nDecisions:\n* AMT_INCOME_TOTAL: Create new feature 'IncomeBand' to convert into ordinal income band category.\n* DAYS_EMPLOYED: Convert anomalous data 365243 days to -29200 days (equal to 80 yrs). I think it's safe to say that an individual is retired by then after working for 80 yrs. Create new feature 'years employed' to convert number of employed 'days' to 'years' for easier reading.\n* OCCUPATION_TYPE: to drop. We can leverage on how much is their total stream of income and how they acquire it.\n* NAME_INCOME_TYPE: to drop. We can usually identify if someone is working, or a student, or already retired, etc., base on the age and ORGANIZATION_TYPE.\n* ORGANIZATION_TYPE: Convert categorical text to numerical. Use feature for modeling.","a6e83809":"Observations:\n* What I can see here is that, majority of the sample don't really have queries (0.0) regardless what time period it is, and regardless of their repayment status. The most common enquiry count ranges from 0 to 2, but then the behavior is similar to both TARGET==0 and TARGET==1. Do take note that we only have a few individuals (about 8%) in our sample population that defaulted (TARGET==1).\n\nDecisions:\n* Drop the credit bureau enquiry fields.","43d382a8":"Observations:\n* REGION_POPULATION_RELATIVE has min val = 0.00029 and max val = 0.072508. Normalized values usually range from 0 to 1, but for this sample, the maximum value is way too far from 1. Whatever values this may serve, there is a noticeable number of defaults as the value gets higher. \n* Majority of the customers apply during weekdays, with a few on weekends. The trend on customers who weren't able to repay the loan is similar with that of those who did. \n* Suspiciously, there are people applying for a loan account as early as 3am, and it gets denser throughout the day. Do note that those who defaulted on their loan has a similar pattern with those having good records.\n\nDecisions:\n* Use REGION_POPULATION_RELATIVE as a feature model. The larger the population on a certain region, the more chances of getting a customer with a bad record.\n* Drop WEEKDAY_APPR_PROCESS_START. I can't say that there's little chance of defaulting if the customer opens an account during Sundays, because the account opening overall on that day is relatively low compared to other days.\n* Drop HOUR_APPR_PROCESS_START. Similar reasoning with above.","dfb158a8":"And for the final set of variables, I tagged it as 'trustworthiness' to account for trusthworthy credentials, or environmental\/ unconsious factors that may shape an individual's behavior. It is quite a lot, so let's break it down again per function. (Note: the excel sheet above is incomplete as this notebook keeps crashing whenever I paste the remaining fields)","02754f13":"Expenditure-related: THE LOAN ITSELF.","c0b00720":"<a id=\"dataprep\"><\/a>\n# Data Preparation\n","71429a67":"Observations:\n* AMT_CREDIT and AMT_GOODS_PRICE are highly correlated (scoring 0.99!), and has a positive linear slope - which makes sense because as the price of goods for which the loan is given gets higher, the credit amount of the loan (ofcourse) gets higher too.\n* AMT_ANNUITY is also highly correlated to AMT_CREDIT and AMT_GOODS_PRICE with a positive linearity. It's because the annuity is the monthly due amount.\n* NAME_CONTRACT_TYPE: Accounting for those who defaulted is much bigger in terms of cash loan than those with revolving loan, however, we must note that cash loan is *significantly* more popular to our sample consumers than the other. \n\nDecisions:\n* AMT_GOODS_PRICE - to drop as it is quite redundant. We already have the AMT_CREDIT field to account for the loan amount.\n* AMT_ANNUITY - create new feature PERCENT_ANNUITY_INCOME to indicate the loan annuity amount relative to the person's total income.\n* AMT_CREDIT - create new feature PERCENT_CREDIT_INCOME to indicate the loan credit amount relative to the person's total income.\n* NAME_CONTRACT_TYPE - to drop since the defaulting rate for both loan type has less than 3% difference. We can focus more on the person's percentage of annuity or credit amount relative to their income.","862f2dc2":"Convert the anomaly data in 'DAYS_EMPLOYED'.","6dee2e65":"Observations:\n* We have a low correlation between these travel-related fields and the target variable.\n* The number of missing values on 'OWN_CAR_AGE' is almost == number of customers who do NOT own a car -- which CAUSES the field to have missing values.\n\nDecisions:\n* Drop REG_REGION_NOT_WORK_REGION and REG_CITY_NOT_WORK_CITY as there is low correlation wrt TARGET variable.\n* Create new ordinal category 'Car Expenditure' from a continuous numerical feature 'OWN_CAR_AGE'. Perhaps the older the car, the higher the maintenance expenditure.","0f43e796":"Observations:\n* All 26 'FLAG-' (flag for contacts and documents) variables have nominal categorical values: 1='YES', 0='NO'. \n\nDecisions:\n* Create new feature 'FlagContact' to account for the total flag-contact variable recorded per individual. \n* Create new feature 'FlagDocu' to account for the total flag-document variable recorded per individual.","93d813cf":"Observations:\n* REGION_RATING_CLIENT & REGION_RATING_CLIENT_W_CITY is highly correlated, scoring 0.95. This is Home Credit's rating of the region where client lives (values = 1,2,3). Majority of the sample obtains a value of '2', and the defaulting count is visible \n* The 4 fields related to flags if the customer's certain (registered) address does not match another given address, the values being: 0='SAME', 1='DIFFERENT'. Most of the customers registered their addresses accurately (scoring 0). And since '0' has a very high frequency, there is a greater chance of encountering different kinds of people that may default on their loan. \n\nDecisions:\n* Use REGION_RATING_CLIENT as a feature model.\n* Drop REGION_RATING_CLIENT_W_CITY since it is highly correlated to region rating alone.\n* Create new ordinal feature 'flag address' to indicate the sum of all 4 flag-address-related fields. The higher the score, the more inaccurate address declarations were made by the individual.\n","a18b838a":"<a id='implementation'><\/a>\n# Implementation","e9351f9b":"Complete the null values of the following features:\n* 'EXT_SOURCE_1'\n* 'EXT_SOURCE_2'\n* 'EXT_SOURCE_3'","06b27de6":"And finally, for the last set of variables to be explored: Number of enquiries to Credit Bureau about the customer.","cab8686b":"* For the training file, we have a total of **307511 observations** and **122 features** to consider -- with integer, float and object datatypes, and 67 features having null values.\n\n* The test file is almost similar: having **48744 observations**, **121 features** (minus the predictor variable 'TARGET'), and 64 features having null values.\n\n\nNote on the words being used: \n\n* **observations** == samples == rows\n* **features** == fields == columns\n* **defaulted** == failed to meet the legal obligations of the loan","0df9f9b0":"Since our model is to classify the 'target' column, we can plot it to visualize the current sample distribution.","afea3e19":"Observations:\n* We have 3 types of CODE_GENDER in our sample: Male, female and XNA. Male and female customers are pretty dense, both having same chances of defaulting relative to their total count. We have a very small set of XNA (non-binary?) people, and base on our sample, 100% of them have good credit record.\n* For NAME_EDUCATION_TYPE, majority of our customers - whether male, female, or XNA - are having 'secondary special' or 'higher education', and most of the defaulting customers came from this educational background. 'Incomplete higher' and 'Lower secondary' are lesser with also less defaulting customers. And the least count are those with 'Academic degree' which got the least number of unpaid customers.\n* The number of days in which the customer changed his\/her identity document and\/or application registration seemed to have the same pattern for those having good credit and bad credit. \n* The DAYS_BIRTH of a customer (or rather the 'age') seem as expected. We can note that the x-axis values are negative - because it is recorded relative prior to the loan application - and very high (because it is in 'days' format rather than in 'years'). As per the graph, younger people tend to default more. As people gets older, they seem to be able to have a higher chance of paying off the loan.\n\nDecisions:\n* Use CODE_GENDER as a model feature. Convert the categorical text to numeric.\n* Use NAME_EDUCATION_TYPE as a model feature. Convert the categorical text to numeric.\n* Drop DAYS_REGISTRATION, DAYS_ID_PUBLISH, NAME_TYPE_SUITE. I generated a KDE plot and I hardly see the difference.\n* Create new feature 'age' from DAYS_BIRTH. Compute for the age in years rather than using the day count.","3295fde8":"The expenditure-related set is quite a lot. We can break it down base on the fields' relatedness.","beca322b":"Observations:\n* It's a small difference but it looks like people with no car and\/or realty tend to default more than those who have. \n\nDecisions:\n* I'll be converting these 2 categorical fields into one nominal field called 'assets'.","43eca946":"Decisions:\n* 'SK_ID_CURR' is just the loan ID which is unique for every individual. It will not contribute to the prediction algorithm. --> DROP\n* The 'TARGET' variable is the one that we are trying to predict, so we will NOT use it as a feature.","a2c83c1f":"Complete the null values for 'CNT_FAM_MEMBERS'.","4372f2e2":"We can now finally remove all the other unecessary columns.","656d78ad":"<a id='algorithm'><\/a>\n# Model Training and Evaluation","f4f1e4aa":"Base on the above information, the dataset is **imbalanced**. Only around 8% of the training set aren't repaid.\n\n\n\nJUST TO BE CLEAR, OUR INDICATOR ON WHETHER A CURRENT\/ PREVIOUS CUSTOMER DEFAULTED ON A LOAN CAN BE SEEN AT 'TARGET' COLUMN:\n\n**TARGET == 0 --> individuals who paid their loan**\n\n**TARGET == 1 --> individuals who did NOT repay their loan**\n\nNow, let's take a closer look on the fields.","cddaa878":"![image.png](attachment:image.png)","10e14fc6":"![image.png](attachment:image.png)","5a852806":"Before anything else, it's important to understand why these fields are recorded in the first place. And below is my understanding on how do we know if an applicant is a good candidate for a loan application. \n![home_credit.jpg](attachment:home_credit.jpg)\n\n(I'm not claiming that this is right, but for me, this makes sense.)\n\nDuring field inspection, I'll try to comprehend whether a certain field falls on any of these questions. It can serve as a guide on how we deal with a certain feature. Below are excel sheet snippets of the initial sorting that I made on all the fields (please see TAG column).","f9b36d65":"<a id='engineering'><\/a>\n# Feature Engineering\n\nOnly few variables were processed at the data wrangling phase, mainly because most of it will be *engineered* to create new features from existing ones. Hopefully, this will improve the model performance. Let's start.\n\n\n***On a side note, I've dropped some of the features that I previously considered due to my limited experience in feature engineering. I'd probably revisit this notebook in the future to further improve.***","76d6c528":"![image.png](attachment:image.png)","c23f906d":"![image.png](attachment:image.png)","99211991":"This heatmap above shows the correlation of those fields with null values: how strongly the presence or absence of one variable affects the presence of another.\n\n* 1 : positive correlation (blue) >> if a variable appears, the other variable definitely does too.\n* 0 : zero correlation (white) >> variables appearing or not appearing have no effect on one another.\n* -1 : negative correlation (red) >> if a variable appears, the other variable definitely does NOT.\n\n(BTW, so thankful I stumbled upon this awesome package for missing values! Check it out [here](http:\/\/github.com\/ResidentMario\/missingno)!)","89c569ea":"Observations:\n* Every unit\/size-related field starting from 'APARTMENTS_AVG' till 'WALLSMATERIAL_MODE' are highly correlated with each other, with no value less than 0.7.\n* I've plotted 4 sample features of these normalized information on where the customer lives, and it seems that the missing values are mostly coming from 'houses\/apartments' -- take note, the 'rented' apartment is a different type (with it having a low count of missing values). Come to think of it, if we are living in a house for so long, we usually don't bother knowing its floor size or maybe we've forgotten about it - which is very much understandable. (unless you are going to renovate, rent-out or sell the place!) \n* The housing type that scored second highest with null values is 'with parents', and again it is common that someone living with parents won't actually bother knowing the land and floor area of their current residence. (from where I come from, this is usually the case)\n\nDecisions:\n* Since these are all residence-related sizes, I will consolidate them into one field to account for 'house expenditures'.","edf9ae8e":"# Table of Contents\n1. [Introduction](#introduction)\n2. [Data Preparation](#dataprep)\n3. [Exploratory Data Analysis](#eda)\n4. [Data Wrangling](#wrangling)\n5. [Feature Engineering](#engineering)\n6. [Model training and Evaluation](#algorithm)\n7. [Implementation](#implementation)","0a7403b9":"That's all folks for this Home Credit Risk Classification project. Do leave a note for feedback and improvement. Thank you for reading and spending some time on this project. :-)","e8b82991":"![image.png](attachment:image.png)","c7a3cef0":"Expenditure-related: SIZE OF HOUSEHOLD","48a7eb4f":"![image.png](attachment:image.png)","2de07c46":"Expenditure-related: RESIDENCE","30b7f3df":"<a id='introduction'><\/a>\n# Introduction\nHola fellow Kagglers! This notebook is related to Home Credit company where they held a competition to enhance their credit checking algorithm during loan application process. However, I'm 2 years late for the competition. . . so this is intended for my personal practice project only. (with the benefit of being graded publicly)\n\n* Objective: **Predict whether a loan applicant is capable of repaying the intended borrowed amount.**\n\nThis is a standard binary classification problem since we'll be predicting a category for an applicant.\n\n* Learning Model: **Supervised Learning Classification model**\n","0b675d5a":"There are 10 files in our library: 7 of them are data sources, and the remaining 3 are the train, test, and sample submission files. We'll check on the main file first (application_train.csv) and see what we can make out of it, then use the test set (application_test.csv) to make our submission. \n\n*Note: It's probably best to consider all the source files, but since it is quite overwhelming for me, I'll first start small and make another notebook (or update this notebook) for future improvements.*\n\n*File reference can be found [here](http:\/\/www.kaggle.com\/c\/home-credit-default-risk\/data?select=HomeCredit_columns_description.csv).","f1a1794f":"<a id='wrangling'><\/a>\n# Data Wrangling\n\nAfter (finally) checking all our fields, it is time to proceed with data wrangling - also known as the data cleaning process.","01d6b89f":"Observations:\n* We have a large number of married customers in our sample population. The married set also contains the most frequent defaulting individuals.\n* 'CNT_CHILDREN', 'NAME_FAMILY_STATUS', 'CNT_FAM_MEMBERS' are all related to family size. And if we only want to consider the family size for approximation of expenditures, we already have 'CNT_FAM_MEMBERS' wherein it accounts for the customer itself, plus spouse (if any), plus number of children (if any).\n\nDecisions:\n* Use 'CNT_FAM_MEMBER' as a feature model. Fill in missing values.\n* Drop 'CNT_CHILDREN' and 'NAME_FAMILY_STATUS'.","0994be49":"Convert the categorical text columns to numerical ones for:\n* CODE_GENDER\n* NAME_EDUCATION_TYPE\n* ORGANIZATION_TYPE"}}