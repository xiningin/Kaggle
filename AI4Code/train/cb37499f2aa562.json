{"cell_type":{"dfe79cde":"code","43af1919":"code","c8b687c5":"code","93824ba5":"code","8c3b0b82":"code","18c32308":"code","e4c0bf51":"code","48b52b8d":"code","700ea271":"code","e27d2083":"code","618becdd":"code","3b146cd3":"code","2cc29e7e":"code","4e7d0e0f":"code","ffbf75bf":"code","8918d516":"code","4fcac9a3":"code","abe61f63":"code","9eca8f6e":"markdown","0d52d552":"markdown","27fd0e24":"markdown"},"source":{"dfe79cde":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn\nfrom  torch.utils.data import Dataset, DataLoader\n\nimport pickle\n\n\nfrom tqdm.notebook import tqdm\n\nfrom transformers import AutoTokenizer, AutoConfig, AutoModelForTokenClassification, AutoModelForSequenceClassification","43af1919":"MAX_LENGTH = 300\nNUM_TARGETS = 1\n\nSEED = 321\n\nMODEL_NAME = \"roberta-base\"","c8b687c5":"TEST_BATCH_SIZE = 32\nTEST_NUM_WORKERS = 2\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nprint(\"Device:\", DEVICE)","93824ba5":"\ndf = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\n\nprint(df.shape)\ndf.head()","8c3b0b82":"with open(\"..\/input\/crp-roberta-model-kkiller-public-ds\/crp-roberta-model-kkiller-private\/roberta-base-tokenizer.pkl\", \"rb\") as f:\n    TOKENIZER = pickle.load(f)\n    \nwith open(\"..\/input\/crp-roberta-model-kkiller-public-ds\/crp-roberta-model-kkiller-private\/roberta-base-config.pkl\", \"rb\") as f:\n    CONFIG = pickle.load(f)","18c32308":"class CRPDataset(Dataset):\n    def __init__(self, df, tokenizer=None):\n        self.df = df\n        \n        self.tokenizer = TOKENIZER if tokenizer is None else tokenizer\n        \n        self.tokenizer_kwargs = dict(\n            add_special_tokens=True,\n            return_tensors=\"pt\",\n            max_length=MAX_LENGTH,\n            padding=\"max_length\",\n            truncation=True,\n        )\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def tokenize(self, txt):\n        return self.tokenizer(txt, **self.tokenizer_kwargs)\n    \n    def __getitem__(self, idx):\n        d = self.tokenize(df.excerpt.iloc[idx])\n        input_ids, masks =  d[\"input_ids\"].squeeze(0), d[\"attention_mask\"].squeeze(0)\n        return input_ids, masks","e4c0bf51":"ds = CRPDataset(df)\nlen(ds)","48b52b8d":"x, x_mask = ds[0]\nx.shape, x_mask.shape","700ea271":"def get_model(model_name=None, task=\"token_classification\", num_targets=NUM_TARGETS, config=None):\n    task = task.lower()\n        \n    if \"token\" in task:\n        model_instance = AutoModelForTokenClassification\n    elif \"sequence\" in task:\n        model_instance = AutoModelForSequenceClassification\n        \n    if config:\n        model = model_instance.from_config(config)\n        tokenizer = None\n    else:\n        model = model_instance.from_pretrained(model_name)\n        tokenizer = AutoTokenizer.from_pretrained(model_name)\n        config = AutoConfig.from_pretrained(model_name)\n    \n    if hasattr(model, \"classifier\"):\n        model.classifier = nn.Linear(model.classifier.in_features, NUM_TARGETS)\n        \n    return config,tokenizer, model","e27d2083":"class AttentionBlock(nn.Module):\n  def __init__(self, in_features, middle_features, out_features):\n    super().__init__()\n    self.in_features = in_features\n    self.middle_features = middle_features\n    self.out_features = out_features\n\n    self.W = nn.Linear(in_features, middle_features)\n    self.V = nn.Linear(middle_features, out_features)\n\n  def forward(self, features):\n    att = torch.tanh(self.W(features))\n\n    score = self.V(att)\n\n    attention_weights = torch.softmax(score, dim=1)\n\n    context_vector = attention_weights * features\n    context_vector = torch.sum(context_vector, dim=1)\n\n    return context_vector","618becdd":"class CRPTokenModel(nn.Module):\n    def __init__(self, config, num_targets=NUM_TARGETS):\n        super().__init__()\n        self.num_targets = num_targets\n        \n        config,tokenizer, model = get_model(config=config, task=\"token_classification\", num_targets=1)\n        \n        self.in_features =  model.classifier.in_features\n        model.classifier = nn.Identity()\n        \n        self.config = config\n        self.tokenizer = tokenizer\n        self.model = model\n        \n        self.att = AttentionBlock(self.in_features, self.in_features, 1)\n        self.fc = nn.Linear(self.in_features, self.num_targets)\n        \n    def forward(self, *args, **kwargs):\n        \n        x = self.model(*args, **kwargs)[\"logits\"]\n        x = self.att(x)\n        \n        x = self.fc(x)\n        return x","3b146cd3":"def load_net(checkpoint_path=None, num_targets=NUM_TARGETS, config=None):\n    config = CONFIG if config is None else config\n\n    net = CRPTokenModel(config)\n    net = net.to(DEVICE)\n    if checkpoint_path is not None:\n        net.load_state_dict(torch.load(checkpoint_path, map_location=DEVICE))\n    net = net.eval()\n    return net","2cc29e7e":"@torch.no_grad()\ndef _predict(nets, xb):\n    pred = 0\n    for net in nets:\n        pred += net(input_ids=xb[0], attention_mask=xb[1])\n\n    pred \/= len(nets)\n\n    return pred\n\n@torch.no_grad()\ndef predict(nets, test_data):\n    preds = []\n    for xb in  test_data:\n        xb = (xb[0].to(DEVICE), xb[1].to(DEVICE))\n        \n        preds.append(_predict(nets, xb).cpu().numpy())\n\n    preds = np.concatenate(preds)\n    return preds","4e7d0e0f":"test_data = CRPDataset(df)\ntest_loader = DataLoader(test_data, batch_size=TEST_BATCH_SIZE, num_workers=TEST_NUM_WORKERS, shuffle=False)\nlen(test_data), len(test_loader)","ffbf75bf":"# checkpoint_paths = list(Path(\"..\/input\/crp-roberta-model-kkiller-public-ds\/crp-roberta-model-kkiller-private\/roberta-base_maxlen300_seed666\").glob(\"*.pth\"))\n\n\ncheckpoint_paths = [\n    '..\/input\/crp-roberta-model-kkiller-public-ds\/crp-roberta-model-kkiller-private\/roberta-base_maxlen300_seed666\/crp_roberta-base_fold0_epoch_00_rmse_val_-0.5064_20210504223412.pth',\n    '..\/input\/crp-roberta-model-kkiller-public-ds\/crp-roberta-model-kkiller-private\/roberta-base_maxlen300_seed666\/crp_roberta-base_fold1_epoch_03_rmse_val_-0.5388_20210504225149.pth',\n    '..\/input\/crp-roberta-model-kkiller-public-ds\/crp-roberta-model-kkiller-private\/roberta-base_maxlen300_seed666\/crp_roberta-base_fold2_epoch_05_rmse_val_-0.5622_20210504230806.pth',\n    '..\/input\/crp-roberta-model-kkiller-public-ds\/crp-roberta-model-kkiller-private\/roberta-base_maxlen300_seed666\/crp_roberta-base_fold3_epoch_00_rmse_val_-0.5852_20210504231457.pth',\n    '..\/input\/crp-roberta-model-kkiller-public-ds\/crp-roberta-model-kkiller-private\/roberta-base_maxlen300_seed666\/crp_roberta-base_fold4_epoch_02_rmse_val_-0.5352_20210504233113.pth',\n]\n\nnets = [\n    load_net(str(ckpt)) for ckpt in checkpoint_paths\n]\n\nprint(len(nets))","8918d516":"preds = predict(nets, test_loader)\nprint(preds.shape)\npreds[:10]","4fcac9a3":"sub = df[[\"id\"]].copy()\nsub[\"target\"] = preds\n\nprint(sub.shape)\nsub.head()","abe61f63":"sub.to_csv(\"submission.csv\", index=False)","9eca8f6e":"This kernel if only for inference, the training one is on its road.  \nThe experimental model is **roBERTa**. But, as we're using the **huggingface**'s **AutoModel** interface, you can easily choose whatever you want.","0d52d552":"# Inference","27fd0e24":"# Notes"}}