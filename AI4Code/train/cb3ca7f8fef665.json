{"cell_type":{"264a6e3c":"code","91219a77":"code","a52c9871":"code","2a866fe8":"code","52f403c0":"code","48b5354e":"code","faa030df":"code","5db3e3df":"code","c4eae4c2":"code","7772dd38":"code","6cbc3504":"code","c9b826d7":"code","affa18e5":"code","8730aa04":"code","4353d5c6":"code","0cc82ca1":"code","4d5094f1":"code","7173dcc5":"code","f0ec74ff":"code","6aa81ba9":"code","97a4ac57":"markdown","b939c4dd":"markdown","1124e51c":"markdown","2fb1746c":"markdown","023553ff":"markdown","2ceceb4d":"markdown"},"source":{"264a6e3c":"import numpy as np  # linear algebra\nimport pandas as pd  # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt  # visualization\n\nimport os\nprint(os.listdir(\"..\/input\"))","91219a77":"data = pd.read_csv(\"..\/input\/voice.csv\")","a52c9871":"data.label = [1 if each == \"male\" else 0 for each in data.label]\ndata.label.values","2a866fe8":"data.head()","52f403c0":"data.info()","48b5354e":"data.describe()","faa030df":"y = data.label.values\nx = data.drop(['label'],axis=1)\nx = (x-np.min(x))\/(np.max(x)-np.min(x)).values  # Normalize\n\nprint(\"y: \", y.shape)\nprint(\"x: \", x.shape)","5db3e3df":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)\n\nfeatures = x_train.T\nlabels = y_train.T\ntest_features = x_test.T\ntest_labels = y_test.T\n\nprint(\"features: \", features.shape)\nprint(\"labels: \", labels.shape)\nprint(\"test_features: \", test_features.shape)\nprint(\"test_labels: \", test_labels.shape)","c4eae4c2":"def init_weights_and_bias(dim):\n    '''\n    Shape of weights:  (20, 1)\n    Shape of bias:  (20, 1)\n    '''\n    weights = np.full((dim, 1),0.01)\n    bias = np.zeros(dim).reshape(-1, 1)\n    return weights, bias\n\nweights, bias = init_weights_and_bias(20)\nprint(\"Shape of weights: \", weights.shape)\nprint(\"Shape of bias: \", bias.shape)","7772dd38":"def sigmoid(Z):\n    yHat = 1 \/ (1 + np.exp(-Z))\n    return yHat","6cbc3504":"def feedforward_back_prop(weights, bias, features, labels):\n    '''\n    features:  (20, 2534)\n    weights: (20, 1)\n    weights.T: (1, 20)\n    bias: (20, 1)\n    yHat: (1, 2534)\n    '''\n    # Feed Forward Propagation\n    Z = np.dot(weights.T, features ) + bias\n    yHat = sigmoid(Z)\n    # Cost Function\n    loss = -labels*np.log(yHat)-(1-labels)*np.log(1-yHat)\n    cost = (np.sum(loss))\/features.shape[1]\n    # Backward Propagation\n    dW = (np.dot(features, ((yHat-labels).T)))\/features.shape[1]\n    dB = np.sum(yHat-labels)\/features.shape[1]\n    grads = {\"dW\": dW, \"dB\": dB}\n    return cost, grads\n\ncost, grads = feedforward_back_prop(weights, bias, features, labels)","c9b826d7":"def update(weights, bias, features, labels, lr, reiter):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    # Updating (learning) parameters is number_of_iterations times\n    for i in range(reiter):\n        \n        cost, grads = feedforward_back_prop(weights, bias, features, labels)\n        #cost = cost_function(features, labels, weights, bias)\n        cost_list.append(cost)\n        #Let's update\n        weights = weights - lr * grads[\"dW\"]\n        bias = bias - lr * grads[\"dB\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print(\"Cost after iterations %i: %f\" %(i, cost))\n            \n    # We update (learn) parameters weights and bias\n    parameters = {\"weights\": weights, \"bias\": bias}\n    plt.plot(index, cost_list2)\n    plt.xticks(index, rotation = \"vertical\")\n    plt.xlabel(\"Number of iterations\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, grads, cost_list","affa18e5":"def predict(weights, bias, test_features):\n    # test_features are a input for feed forward propagation\n    Z = sigmoid(np.dot(weights.T, test_features) + bias)\n    prediction = np.zeros((1, test_features.shape[1]))\n    for i in range(Z.shape[1]):\n        if Z[0, i] <= 0.5:\n            prediction[0, i] = 0\n        else:\n            prediction[0, i] = 1  \n    return prediction","8730aa04":"def logistic_regression(features, labels, test_features, test_labels, lr ,  reiter):\n    # Initialize\n    # lr: learning rate\n    \n    dim =  features.shape[0]\n    # dim =  features.shape[0]: 20 for our case\n    weights, bias = init_weights_and_bias(dim)\n    # Shape of weights:  (20, 1)\n    # Shape of bias:  (20, 1)\n    parameters, grads, cost_list = update(weights, bias, features, labels, lr, reiter)\n    \n    prediction_test = predict(parameters[\"weights\"], parameters[\"bias\"],test_features)\n\n    # Print test Errors\n    print(\"Model A test accuracy: {} %\".format(100 - np.mean(np.abs(prediction_test - test_labels)) * 100))","4353d5c6":"logistic_regression(features, labels, test_features, test_labels, lr = 0.1 ,  reiter= 1250) ","0cc82ca1":"from sklearn.linear_model import LogisticRegression\nModel_B = LogisticRegression()\nModel_B.fit(x_train,y_train)\nprint(\"Model B test accuracy: {}\".format(Model_B.score(x_test,y_test)))","4d5094f1":"labels = y_train.reshape(y_train[0], -1)\ntest_labels = y_test.reshape(y_test[1], -1)\n\nprint(\"features: \", features.shape)\nprint(\"labels: \", labels.shape)\nprint(\"test_features: \", test_features.shape)\nprint(\"test_labels: \", test_labels.shape)","7173dcc5":"class ArtificialNeuralNetwork(object):\n    \n    def __init__(self, xTrain, xTest, yTrain, yTest):\n        # Define train and test data\n        self.xTrain = xTrain\n        self.xTest = xTest\n        self.yTrain = yTrain.reshape(yTrain.shape[0],-1)\n        self.yTest = yTest.reshape(yTest.shape[0],-1)\n\n        # Define hyperparameters\n        self.inputLayerSize = self.xTrain.shape[0] # nx <-> Number of features\n        self.hiddenLayerSize = 4\n        self.outputLayerSize = self.yTrain.shape[0]\n        \n    def initializeWeightsAndBias(self): #, inputLayerSize, hiddenLayerSize, outputLayerSize):\n        \"\"\"\n        This function creates a vector of zeros of shape (inputLayerSize, 1) for w and initializes b to 0.\n\n        Argument:\n        inputLayerSize -- size of the input layer\n        hiddenLayerSize -- size of the hidden layer\n        outputLayerSize -- size of the output layer\n\n        Returns:\n        params -- python dictionary containing your parameters:\n                        W1 -- weight matrix of shape (hiddenLayerSize, inputLayerSize)\n                        b1 -- bias vector of shape (hiddenLayerSize, 1)\n                        W2 -- weight matrix of shape (outputLayerSize, hiddenLayerSize)\n                        b2 -- bias vector of shape (outputLayerSize, 1)\n        \"\"\"\n        np.random.seed(23) # We set up a seed so that your output matches ours \n                           # although the initialization is random.\n        \n        W1 = np.random.randn(self.inputLayerSize, \n                             self.hiddenLayerSize) * 0.01\n        b1 = np.zeros(shape=(self.hiddenLayerSize, 1))\n        W2 = np.random.randn(self.hiddenLayerSize,\n                             self.outputLayerSize) * 0.01\n        b2 = np.zeros(shape=(self.outputLayerSize, 1))\n        \n        # assert(isinstance(B1, float) or isinstance(B1, int))\n        \n        assert (W1.shape == (self.inputLayerSize, self.hiddenLayerSize)), \"[W1] -> Unsuitable matrix size\"\n        assert (b1.shape == (self.hiddenLayerSize, 1))\n        assert (W2.shape == (self.hiddenLayerSize, self.outputLayerSize)), \"[W2] -> Unsuitable matrix size\"\n        assert (b2.shape == (self.outputLayerSize, 1))\n        \n        parameters = {\"W1\": W1,\n                      \"b1\": b1,\n                      \"W2\": W2,\n                      \"b2\": b2}   \n        \n        return parameters\n    \n    def sigmoid(self, Z):\n        \"\"\" Apply and compute sigmoid activation function to scalar, vector, or matrix (Z)\n\n        Arguments:\n        Z -- A scalar or numpy array of any size.\n\n        Return:\n        s -- sigmoid(z)\n        \"\"\"\n        return 1\/(1+np.exp(-Z))\n    \n    def forwardPropagation(self, X, parameters):\n        \"\"\" Propogate inputs though network\n        \n        Argument:\n        X -- input data of size (inputLayerSize, m)\n        parameters -- python dictionary containing your parameters (output of initialization function)\n\n        Returns:\n        A2 -- The sigmoid output of the second activation\n        cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n        \"\"\"\n        # Retrieve each parameter from the dictionary \"parameters\"\n        W1 = parameters['W1']\n        b1 = parameters['b1']\n        W2 = parameters['W2']\n        b2 = parameters['b2']\n\n        # Implement Forward Propagation to calculate A2 (probabilities)\n        Z1 = np.dot(W1.T, X) + b1\n        A1 = sigmoid(Z1)\n        Z2 = np.dot(W2.T, A1) + b2\n        yHat = self.sigmoid(Z2) # A2\n\n        assert(yHat.shape == (1, X.shape[1]))\n    \n        cache = {\"Z1\": Z1,\n                 \"A1\": A1,\n                 \"Z2\": Z2,\n                 \"yHat\": yHat}    # A2\n    \n        return yHat, cache\n    \n    def computeCost(self, yHat, Y, parameters):\n        \"\"\" Compute cost for given X,Y, use weights already stored in class \n\n        Arguments:\n        yHat -- The sigmoid output of the second activation, of shape (1, number of examples)\n        Y -- \"true\" labels vector of shape (1, number of examples)\n        parameters -- python dictionary containing your parameters W1, b1, W2 and b2\n\n        Returns:\n        cost -- cross-entropy cost given equation (13)\n        \"\"\"\n        m = Y.shape[1] # number of example\n                      \n        # Retrieve W1 and W2 from parameters\n        W1 = parameters['W1']\n        W2 = parameters['W2']   \n                    \n        # Loss\n        logprobs = np.multiply(np.log(yHat), Y) + np.multiply((1 - Y), np.log(1 - yHat))\n        # Cost\n        cost = - (np.sum(logprobs)) \/ m     # m =  yTrain.shape[1]  is for scaling\n        \n        cost = np.squeeze(cost)     # makes sure cost is the dimension we expect. \n                                    # E.g., turns [[17]] into 17 \n        assert(isinstance(cost, float))\n                      \n        return cost\n\n    def backwardPropagation(self,parameters, cache,  X, Y):\n        \"\"\" Compute the gradients of parameters by implementing the backward propagation\n\n        Arguments:\n        parameters -- python dictionary containing our parameters \n        cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n        X -- input data of shape (2, number of examples)\n        Y -- \"true\" labels vector of shape (1, number of examples)\n\n        Returns:\n        grads -- python dictionary containing your gradients with respect to different parameters\n        \"\"\"\n        m = X.shape[1]   \n                      \n        # First, retrieve W1 and W2 from the dictionary \"parameters\".       \n        W1 = parameters['W1']\n        W2 = parameters['W2']\n                      \n        # Retrieve also A1 and A2 from dictionary \"cache\".\n        A1 = cache['A1']\n        yHat = cache['yHat']                    \n                      \n        # Backward propagation: calculate dW1, db1, dW2, db2.                     \n        dZ2 = yHat - Y\n        dW2 = (1 \/ m) * np.dot(A1, dZ2.T)\n        db2 = (1 \/ m) * np.sum(dZ2, axis=1, keepdims=True)\n        dZ1 = np.multiply(np.dot(W2, dZ2), 1 - np.power(A1, 2))\n        dW1 = (1 \/ m) * np.dot(X, dZ1.T)#(1 \/ m) * np.dot(dZ1, self.xTrain.T) # MATRIS BOYUTLARINA BAK dW1 ve dW2 ICIN\n        db1 = (1 \/ m) * np.sum(dZ1, axis=1, keepdims=True)   # m is for scaling \n\n        gradients = {\"dW1\": dW1,\n                     \"db1\": db1,\n                     \"dW2\": dW2,\n                     \"db2\": db2}\n                      \n        return gradients\n    \n    def updateParameters(self, parameters, gradients, learning_rate = 0.15):\n        \"\"\"\n        Updates parameters using the gradient descent update rule given above\n\n        Arguments:\n        parameters -- python dictionary containing your parameters \n        grads -- python dictionary containing your gradients \n\n        Returns:\n        parameters -- python dictionary containing your updated parameters \n        \"\"\"\n        # Retrieve each parameter from the dictionary \"parameters\"\n        W1 = parameters['W1']\n        b1 = parameters['b1']\n        W2 = parameters['W2']\n        b2 = parameters['b2']\n\n        # Retrieve each gradient from the dictionary \"grads\"\n        dW1 = gradients['dW1']\n        db1 = gradients['db1']\n        dW2 = gradients['dW2']\n        db2 = gradients['db2']\n        \n        # Update rule for each parameter\n        W1 = W1 - learning_rate * dW1\n        b1 = b1 - learning_rate * db1\n        W2 = W2 - learning_rate * dW2\n        b2 = b2 - learning_rate * db2\n\n        parameters = {\"W1\": W1,\n                      \"b1\": b1,\n                      \"W2\": W2,\n                      \"b2\": b2}\n\n        return parameters\n                      \n    def model(self, X, Y, num_iterations=10000, print_cost=False):\n        \"\"\"\n        Arguments:\n        X -- dataset of shape (2, number of examples)\n        Y -- labels of shape (1, number of examples)\n        n_h -- size of the hidden layer\n        num_iterations -- Number of iterations in gradient descent loop\n        print_cost -- if True, print the cost every 1000 iterations\n\n        Returns:\n        parameters -- parameters learnt by the model. They can then be used to predict.\n        \"\"\"\n        np.random.seed(3)\n        \n        costStr = []\n        indexStr = []\n        \n        # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n        parameters = self.initializeWeightsAndBias()\n\n        W1 = parameters['W1']\n        b1 = parameters['b1']\n        W2 = parameters['W2']\n        b2 = parameters['b2']\n \n        # Loop (gradient descent)\n        for i in range(0, num_iterations):\n                      \n            # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n            yHat, cache = self.forwardPropagation(X, parameters)\n\n            # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n            cost = self.computeCost(yHat, Y, parameters)\n            \n            # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n            gradients = self.backwardPropagation(parameters, cache, X, Y)\n\n            # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n            parameters = self.updateParameters(parameters, gradients)\n\n            # Print the cost every 1000 iterations\n            if print_cost and i % 1000 == 0:\n                costStr.append(cost)\n                indexStr.append(i)\n                print (\"Cost after iteration %i: %f\" % (i, cost))\n            \"\"\"\n            # Plot Cost Function\n            plt.plot(indexStr,costStr)\n            plt.xticks(indexStr,rotation='vertical')\n            plt.xlabel(\"Number of Iterarion\")\n            plt.ylabel(\"Cost\")\n            plt.show()\n            \"\"\"\n        return parameters\n\n    def predict(self, parameters, X):\n        \"\"\"\n        Using the learned parameters, predicts a class for each example in X\n\n        Arguments:\n        parameters -- python dictionary containing your parameters \n        X -- input data of size (n_x, m)\n\n        Returns\n        predictions -- vector of predictions of our model (red: 0 \/ blue: 1)\n        \"\"\"\n        # Computes probabilities using forward propagation, and classifies to 0\/1 using 0.5 as the threshold.\n        yHat, cache = self.forwardPropagation(X, parameters)\n        predictions = np.round(yHat)\n\n        \n        return predictions","f0ec74ff":"ANN = ArtificialNeuralNetwork(features, test_features, labels, test_labels)\nparameters = ANN.model(features, labels, num_iterations = 12000, print_cost=True)\npredictions = ANN.predict(parameters, features)\nprint('Train Accuracy: %d' % float((np.dot(labels, predictions.T) + np.dot(1 - labels, 1 - predictions.T)) \/ float(labels.size) * 100) + '%')","6aa81ba9":"parameters = ANN.model(test_features, test_labels, num_iterations = 12000, print_cost=True)\npredictions = ANN.predict(parameters, test_features)\nprint(\"Test Accuracy: %d\" % float((np.dot(test_labels, predictions.T) + np.dot(1 - test_labels, 1 - predictions.T)) \/ float(test_labels.size) * 100) + '%')","97a4ac57":"## 2 Layer ANN","b939c4dd":"# Gender Prediction with Logistic Regression and ANN","1124e51c":"> * Data Preparation\n> * Logistic Regression\n> * Logistic Regression with Scikit Learn\n> * 2 Layer ANN\n> * 3 Layer ANN with Keras","2fb1746c":"## Logistic Regression","023553ff":"## Data Exploration and Preparation","2ceceb4d":"## Logistic Regression with ScikitLearn"}}