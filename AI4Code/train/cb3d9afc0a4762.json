{"cell_type":{"835a4bf3":"code","89a92f2b":"code","284006e0":"code","04996074":"code","1eea554c":"code","7ee23ddf":"code","5599ca8c":"code","03583370":"code","2df6da26":"code","aea67f62":"code","36c2037a":"code","7639aefb":"code","6163efbe":"code","4161639d":"code","ecb3b9b1":"code","023c811e":"code","cc27d8cd":"code","b9fd0950":"code","abb19d60":"code","6b1116ab":"code","6ebc3f84":"code","1a29a43d":"code","8361b253":"code","93e5c026":"code","1c2a40a0":"code","6712b639":"code","34604c6c":"code","1b6a9588":"code","f856057e":"code","2b70cb78":"code","0897db9d":"code","a02425ab":"code","7ea3d084":"code","1dbefae4":"code","efcdcabc":"code","709d46e6":"code","2cc1b5ce":"markdown","908eba3c":"markdown","89568049":"markdown","7ec01b84":"markdown"},"source":{"835a4bf3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","89a92f2b":"df = pd.read_csv(\"\/kaggle\/input\/predict-test-scores-of-students\/test_scores.csv\")","284006e0":"df.head()","04996074":"df.info()","1eea554c":"df.dropna().shape # -> there is no empty items in df","7ee23ddf":"df.nunique().plot(kind = \"bar\")# ","5599ca8c":"tm = df[\"teaching_method\"]\ndf.nunique()","03583370":"tm.value_counts()","2df6da26":"df[\"school\"].value_counts()","aea67f62":"df[\"lunch\"].value_counts()","36c2037a":"df2 = pd.get_dummies(df,columns=[\"school_setting\"],prefix=[\"school_setting\"])\ndf2","7639aefb":"list1 = [\"school_setting_Rural\",\"school_setting_Suburban\",\"school_setting_Urban\"]","6163efbe":"for item in list1:\n    print(df2[item].corr(df2[\"posttest\"])) \n","4161639d":"# max results on tests\npretest = df[\"pretest\"]\nposttest = df[\"posttest\"]\nnp.max(pretest), np.max(posttest)","ecb3b9b1":"avg_pre = pretest.mean()\navg_post = posttest.mean()","023c811e":"np.sum(pretest>avg_pre),np.sum(posttest > avg_post)","cc27d8cd":"np.sum(pretest == 93) ,np.sum(posttest == 100)","b9fd0950":"df.iloc[np.argmax(pretest), :] # best pretest student","abb19d60":"# best posttest students\nindexnames = df[df[\"posttest\"] != 100].index \ndf.drop(indexnames) ","6b1116ab":"df.describe()","6ebc3f84":"c = 0\nlist2 = [df.school_setting,df.school_type,df.teaching_method,df.gender]\nfig, axes = plt.pyplot.subplots(2, 2, figsize=(18, 10))\nfor i in range(2):\n    for j in range(2):\n        sns.countplot(ax=axes[i,j],x = list2[c])\n        c+=1\n","1a29a43d":"fig,(ax1,ax2) = plt.pyplot.subplots(1,2,figsize=(12,6))\nsns.set_style(\"darkgrid\",{\"axes.facecolor\": \".9\"})\nsns.boxplot(x=df.pretest,y = df.gender,ax =ax1)\nsns.histplot(x = df.pretest, ax =ax2,kde = True)","8361b253":"fig,(ax1,ax2) = plt.pyplot.subplots(1,2,figsize=(12,6))\nsns.boxplot(x=df.posttest,y = df.gender,ax =ax1)\nsns.histplot(x = df.posttest, ax =ax2,kde = True)","93e5c026":"# seems, that pretest scores strongly correlate with posttest scores \ndf[[\"pretest\",\"posttest\"]].plot(figsize = (35,15))","1c2a40a0":"df[\"pretest\"].corr(df[\"posttest\"]) # we were right","6712b639":"# quick look at data again\ndf.head()","34604c6c":"# Split the data into X and y\nX = df.drop(['posttest','classroom','student_id'], axis = 1)\ny = df['posttest']\n\n# Convert categorical values to numbers\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategories = ['school', 'school_setting', 'school_type', 'teaching_method', 'gender', 'lunch']\n\none_hot = OneHotEncoder()\ntransformer = ColumnTransformer([('one_hot', one_hot, categories)],\n                                remainder = 'passthrough')\n\nX_transformed = transformer.fit_transform(X)\n\n# Split the transformed data to training and test sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size = 0.2)\n\n# Import the Random Forest regressor\nfrom sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\n\n# Score the model\nmodel.score(X_test, y_test)","1b6a9588":"from sklearn.model_selection import train_test_split","f856057e":"X_train,X_test,y_train,y_test = train_test_split(X_transformed,y,test_size = 0.18)","2b70cb78":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score\nmodel = RandomForestRegressor()\nmodel.fit(X_train,y_train)\nmodel.score(X_test,y_test)","0897db9d":"cv_score = cross_val_score(model,X_transformed,y)\nnp.mean(cv_score)","a02425ab":"from sklearn.model_selection import RandomizedSearchCV\nX = df.drop(['posttest','classroom','student_id'], axis = 1)\ny = df['posttest']\n\n# Convert categorical values to numbers\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\ncategories = ['school', 'school_setting', 'school_type', 'teaching_method', 'gender', 'lunch']\n\none_hot = OneHotEncoder()\ntransformer = ColumnTransformer([('one_hot', one_hot, categories)],\n                                remainder = 'passthrough')\n\nX_transformed = transformer.fit_transform(X)\n\n# Split the transformed data to training and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_transformed, y, test_size = 0.2)\n\ngrid = {\"n_estimators\" : [1,10,50,100,200,500,800,1200,3000],'max_depth' : [None,5,10,20,30],'max_features' : ['auto', 'sqrt']}\n\ntuned_model = RandomizedSearchCV(estimator = model,param_distributions = grid,n_iter=15,cv=7,verbose=2 )\ntuned_model.fit(X_train,y_train)","7ea3d084":"tuned_model.best_params_","1dbefae4":"y_pred = tuned_model.predict(X_test)","efcdcabc":"from sklearn.metrics import explained_variance_score\nfrom sklearn.metrics import r2_score, mean_squared_error\nr2_score(y_test,y_pred)","709d46e6":"mse = mean_squared_error(y_test, y_pred)\nmse","2cc1b5ce":"# Tuning","908eba3c":"We got some categorial features, so have to encode them to continue ","89568049":"\n# **Visualisation**","7ec01b84":"# Machine learning part"}}