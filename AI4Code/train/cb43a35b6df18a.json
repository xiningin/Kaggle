{"cell_type":{"5e154dcf":"code","359226b7":"code","6c1d9a85":"code","f48a093d":"code","40e6d9cc":"code","e3c763b5":"code","037295ea":"code","c56496a2":"code","b0e493e4":"code","ebf3f7a0":"code","6d2c14f8":"code","3d261677":"code","e0814237":"code","6d6b2556":"code","79b28420":"code","4f7e068d":"code","fe388556":"code","80db46c9":"code","c25555b9":"code","0888760f":"code","18b504ed":"code","68fb4e73":"markdown","c0e18e1a":"markdown","5bc66471":"markdown","c4c08a7e":"markdown","aff291ea":"markdown","14267349":"markdown","8e444452":"markdown","193098ec":"markdown","af2f3040":"markdown","cf7c140c":"markdown","1e6c7bdf":"markdown","6d28ccb2":"markdown","b96dd2da":"markdown","9bf6f285":"markdown","dd06ca9e":"markdown","787121fd":"markdown"},"source":{"5e154dcf":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv('\/kaggle\/input\/kaggle-survey-2020\/kaggle_survey_2020_responses.csv')","359226b7":"# Remove first column\ndf = df.iloc[:,1:]","6c1d9a85":"# Remove first row \ndf = df.iloc[1:]\ndf = df.reset_index(drop = True)","f48a093d":"# Drop rows where question 5 is empty\nq5_drop_count = 0\nfor i in range(len(df)):\n    if pd.isna(df.at[i, 'Q5']):\n        q5_drop_count += 1\n        df = df.drop(i)\n        \n# Reset index\ndf = df.reset_index(drop = True)\n\n# Display rows dropped\nprint('rows dropped because q5 was empty: ' + str(q5_drop_count))","40e6d9cc":"pip install kmodes","e3c763b5":"pip install yellowbrick","037295ea":"label_df = df[['Q1', 'Q2', 'Q3', 'Q4', 'Q5', 'Q6', \n               'Q7_Part_1', 'Q7_Part_2', 'Q7_Part_3', 'Q7_Part_4', 'Q7_Part_5', 'Q7_Part_6',\n               'Q7_Part_7', 'Q7_Part_8', 'Q7_Part_9', 'Q7_Part_10', 'Q7_Part_11', 'Q7_Part_12' ,'Q7_OTHER',\n               'Q9_Part_1', 'Q9_Part_2', 'Q9_Part_3', 'Q9_Part_4', 'Q9_Part_5', 'Q9_Part_6', 'Q9_Part_7',\n               'Q9_Part_8', 'Q9_Part_9', 'Q9_Part_10', 'Q9_Part_11', 'Q9_OTHER', \n               'Q10_Part_1', 'Q10_Part_2', 'Q10_Part_3', 'Q10_Part_4', 'Q10_Part_5', 'Q10_Part_6', 'Q10_Part_7',\n               'Q10_Part_8', 'Q10_Part_9', 'Q10_Part_10', 'Q10_Part_11', 'Q10_Part_12', 'Q10_Part_13', 'Q10_OTHER',\n               'Q11', 'Q12_Part_1', 'Q12_Part_2', 'Q12_Part_3', 'Q12_OTHER', 'Q13',\n               'Q14_Part_1', 'Q14_Part_2', 'Q14_Part_3', 'Q14_Part_4', 'Q14_Part_5', 'Q14_Part_6', 'Q14_Part_7',\n               'Q14_Part_8', 'Q14_Part_9', 'Q14_Part_10', 'Q14_Part_11', 'Q14_OTHER', 'Q15']]\nlabel_df = label_df.fillna('0')\n","c56496a2":"# Encode categorical data for machine learning\nfrom sklearn import preprocessing\nle = preprocessing.LabelEncoder()\nlabel_df_enc = label_df.apply(le.fit_transform)\npd.options.display.max_columns = None\nlabel_df_enc.head(5)","b0e493e4":"# Elbow Method for K means\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom kmodes.kmodes import KModes\n\nmodel = KModes( init = 'Cao', n_init = 1, verbose=1)\n# k is range of number of clusters.\nvisualizer = KElbowVisualizer(model, k=(2,7), timings= True)\nvisualizer.fit(label_df_enc)        # Fit data to visualizer\nvisualizer.show()        # Finalize and render figure","ebf3f7a0":"from kmodes.kmodes import KModes\n\nkm_cao = KModes(n_clusters=3, init = 'Cao', n_init = 1, verbose=1)\nfitClusters_cao = km_cao.fit_predict(label_df) # predict cluster\n\nclusterCentroidsDf = pd.DataFrame(km_cao.cluster_centroids_)\nclusterCentroidsDf.columns = label_df.columns\npd.options.display.max_columns = None\n\nclusterCentroidsDf","6d2c14f8":"# Combine df and predicted cluter to one df\npred_df = label_df.reset_index()\nclustersDf = pd.DataFrame(fitClusters_cao)\nclustersDf.columns = ['cluster_predicted']\ncombinedDf = pd.concat([pred_df, clustersDf], axis = 1).reset_index()\ncombinedDf = combinedDf.drop(['index', 'level_0'], axis = 1)\n\n# Percentage plot\npercentage_group = combinedDf.groupby(['cluster_predicted'])['Q1'].count()\npercentage_group = percentage_group \/ sum(percentage_group)\npercentage_group = percentage_group.reset_index().rename(columns = { 'Q1': 'Proportion'})\n\n# settings\nplt.figure(figsize = (5,5))\nsns.set_style(\"whitegrid\")\n\n# plot\nplot2 = sns.barplot(x = 'cluster_predicted', y = 'Proportion', data = percentage_group, palette = 'viridis')\nplot2.set(ylim=(0, .75))\n\n# annotate bar plots\nfor p in plot2.patches:\n    plot2.annotate(format(p.get_height(), '.2f'), \n                   (p.get_x() + p.get_width() \/ 2., p.get_height()), \n                   ha = 'center', va = 'center', \n                   xytext = (0, 9), \n                   textcoords = 'offset points')\n    \n# title\nplt.title('Proportion of Predicted Clusters', fontsize = 20) # title with fontsize 20\n\nplot2","3d261677":"# df : the dataframe that contains predicted clusters (null values should be string '0')\n# columns : takes list of column names (columns should be in df)\n# cluster_col : takes one string of cluster column name (this should be in df) (values should be integers)\n\ndef probability_heatmap(df,columns, cluster_col, optional_size = (15,10), title = ''):\n    intialize_df = pd.DataFrame(columns = [cluster_col, ' ', 'probability'])\n    clusters_list = sorted(df[cluster_col].unique())\n    \n    for j in columns:\n        data_crosstab = pd.crosstab(df[cluster_col], df[j], margins = True)\n                        \n        column_value = list(df[j].unique())\n        column_value.remove('0')\n        column_value = column_value[0]\n        \n        for i in clusters_list:\n            prob = data_crosstab.at[i, column_value] \/ data_crosstab.at[i, 'All']\n            \n            intialize_df = intialize_df.append({cluster_col : i, ' ' : column_value, 'probability':prob}, ignore_index=True)\n            \n    #pivot map for seaborn heatmap\n    pivot_df = intialize_df.pivot(index = cluster_col, columns = ' ', values = 'probability')\n\n    #set correlation graph size\n    sns.set(font_scale=1)\n    plt.figure(figsize = (optional_size))\n\n    #correlation graph settings\n    ax = sns.heatmap(pivot_df, vmin=0., vmax=1,cmap='viridis', fmt='.2f', annot = True, square = True,\n                     linewidths = .5, cbar = False)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n\n    # set title\n    plt.title(title, fontsize = 20) # title with fontsize 20\n    # show\n    plt.show()","e0814237":"# Q7: What programming languages do you use on a regular basis?\nQ7_cols = ['Q7_Part_1', 'Q7_Part_2', 'Q7_Part_3', 'Q7_Part_4', 'Q7_Part_5', 'Q7_Part_6',\n           'Q7_Part_7', 'Q7_Part_8', 'Q7_Part_9', 'Q7_Part_10', 'Q7_Part_11', 'Q7_Part_12' ,'Q7_OTHER']\n\nprobability_heatmap(combinedDf, columns = Q7_cols, cluster_col = 'cluster_predicted',\n                   title = 'What programming languages do you use on a regular basis?')","6d6b2556":"Q9_cols = ['Q9_Part_1', 'Q9_Part_2', 'Q9_Part_3', 'Q9_Part_4', 'Q9_Part_5', 'Q9_Part_6', 'Q9_Part_7',\n          'Q9_Part_8', 'Q9_Part_9', 'Q9_Part_10', 'Q9_Part_11', 'Q9_OTHER']\n\nprobability_heatmap(combinedDf, columns = Q9_cols, cluster_col = 'cluster_predicted',\n                   title = 'Which of the following integrated development environments (IDE\\'s) do you use on a regular basis?')","79b28420":"Q10_cols = ['Q10_Part_1', 'Q10_Part_2', 'Q10_Part_3', 'Q10_Part_4', 'Q10_Part_5', 'Q10_Part_6', 'Q10_Part_7',\n               'Q10_Part_8', 'Q10_Part_9', 'Q10_Part_10', 'Q10_Part_11', 'Q10_Part_12', 'Q10_Part_13', 'Q10_OTHER']\n\nprobability_heatmap(combinedDf, columns = Q10_cols, cluster_col = 'cluster_predicted',\n                   title = 'Which of the following hosted notebook products do you use on a regular basis?  ')","4f7e068d":"# df : the dataframe that contains predicted clusters (null values should be string '0')\n# column : takes one string of column name (column should be in df)\n# cluster_col : takes one string of cluster column name (this should be in df) (values should be integers)\n\ndef probability_heatmap_single_col(df,column, cluster_col, optional_size = (15,10), title = ''):\n    intialize_df = pd.DataFrame(columns = [cluster_col, ' ', 'probability'])\n    clusters_list = sorted(df[cluster_col].unique())\n    column_values = sorted(df[column].unique()) \n    column_values.remove('0')\n    \n    data_crosstab = pd.crosstab(df[cluster_col], df[column], margins = True)\n    #data_crosstab = data_crosstab.drop('0', axis = 1)\n\n    for j in column_values:   \n        for i in clusters_list:\n            prob = data_crosstab.at[i, j] \/ data_crosstab.at[i, 'All']\n            intialize_df = intialize_df.append({cluster_col : i, ' ' : j, 'probability':prob}, \n                                               ignore_index=True)\n            \n    #pivot map for seaborn heatmap\n    pivot_df = intialize_df.pivot(index = cluster_col, columns = ' ', values = 'probability')\n\n    #set correlation graph size\n    sns.set(font_scale=1)\n    plt.figure(figsize = (optional_size))\n\n    #correlation graph settings\n    ax = sns.heatmap(pivot_df, vmin=0., vmax=1,cmap='viridis', fmt='.2f', annot = True, square = True,\n                     linewidths = .5, cbar = False)\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n\n    # set title\n    plt.title(title, fontsize = 20) # title with fontsize 20\n    # show\n    plt.show()","fe388556":"probability_heatmap_single_col(df = combinedDf, column = 'Q11' , cluster_col = 'cluster_predicted', optional_size = (7,7), \n                               title = 'What type of computing platform do you use most often for your data science projects?')","80db46c9":"Q12_cols = ['Q12_Part_1', 'Q12_Part_2', 'Q12_Part_3', 'Q12_OTHER']\n\nprobability_heatmap(combinedDf, columns = Q12_cols, cluster_col = 'cluster_predicted', optional_size = (5,5),\n                   title = 'Which types of specialized hardware do you use on a regular basis?')","c25555b9":"probability_heatmap_single_col(df = combinedDf, column = 'Q13' , cluster_col = 'cluster_predicted', optional_size = (7,7), \n                               title = 'Approximately how many times have you used a TPU (tensor processing unit)?')","0888760f":"Q14_cols = ['Q14_Part_1', 'Q14_Part_2', 'Q14_Part_3', 'Q14_Part_4', 'Q14_Part_5', 'Q14_Part_6', 'Q14_Part_7',\n               'Q14_Part_8', 'Q14_Part_9', 'Q14_Part_10', 'Q14_Part_11', 'Q14_OTHER']\nprobability_heatmap(combinedDf, columns = Q14_cols, cluster_col = 'cluster_predicted',\n                   title = 'What data visualization libraries or tools do you use on a regular basis?')","18b504ed":"probability_heatmap_single_col(df = combinedDf, column = 'Q15' , cluster_col = 'cluster_predicted', optional_size = (10,10), \n                               title = 'For how many years have you used machine learning methods?')","68fb4e73":"## Clustering Users <a id = 'users'><\/a>","c0e18e1a":"    Cluster 0\nLooking at the centroids above, we describe cluster 0 as having the main dominant features of being 18-21 years old, man, from India, having or pursuing a Bachelor's degree, and having 1-2 years of programming. In other terms, this group of people are just starting out exploring data science, they are still in the process of learning, don't have experience with a wide variety of data tools, and more likely to try new ideas. We can infer that this group is using Kaggle as learning experience or something to put on the resume.\n\n    Cluster 1\nLooking at the qualities of the centroids, cluster 1 seems to be mostly being 25-29 years old, men, from India, having or pursuing a Master's degree, are data scientists, and have 3-5 years of programming experience. In other terms, this group has just started their data science career, are generally of entry level experience, and are generally very educated. \n\n    Cluster 2\nFor cluster 2, the main qualiies are being 35-39 years old, men, from the United States of America, have a Master's degree, are data scientists, and have 3-5 years of programming experience. This group is generally older than the previous two clusters, should have more job experiene, more set in their data science careers, and have used a variety of data tools. Since this group is already established in their careers, they could be using Kaggle for intrinsic values such as for fun or learning about new data methods.  \n\n    Percentage Plot\nPlotting the counts of the predicted cluster values in the below graph, we can see that most participants fall under cluster 0. We can, see that most of Kaggle's user base is young, in school, most likely pursuing a Bachelors, don't have an extensive amount of coding experience, and are using Kaggle as a means to expore the world of data science.","5bc66471":"## Exploratory Data Analysis <a id = 'eda'><\/a>","c4c08a7e":"## Describing Clusters <a id = 'desc'><\/a>","aff291ea":"We will need to clean the data.\nCriteria:\nFirst column will be removed, it is not necessary for our studies. Also uses alot of memory since it is a mixed data type.\nFirst row will be removed since it is just states the full question.\nPeople who did not answer Q5 will be removed because the answer is paramount to describe the demographics.\n","14267349":"A basic EDA of the dataset can be found in the link below:\n\nhttps:\/\/www.kaggle.com\/paultimothymooney\/2020-kaggle-data-science-machine-learning-survey","8e444452":"This notebook aims identify the different subgroups of users of Kaggle. Clustering analysis, K-Modes, will be used to separate users into several distinct buckets of users. Then, these types of users will be described based on the survey questions. ","193098ec":"Now we will do a deep dive into the clusters and their characteristics. ","af2f3040":"From the elbow plot, we can see that the optimal number of clusters is 3, the first major inflection point. \n\nNow, lets take a look at the clusters and see their characteristics.","cf7c140c":"We will use K-modes to identify clusters from our data set or particular types of users. Since we do not have an intial label, it is up to us to describe these clusters.\n\nSci-kit learn does not have a built in method for clustering categorical data. We will be using the library ***kmodes*** for this.\n\nChoosing the number of clusters is an essential task for a clustering problem. There are numerous ways to evaluate the number of clusters. We will be using the elbow method to determine the optimal number of clusters. We want the most distinction and the least amout of error. The library, ***yellowbrick***, provides an easy way to create the elbow plot. \n\nThere are more than 300 columns in our dataset. The high dimensionality could create noise in our analysis, o we need to choose the questions that would create the most distinction. Choosing the questions to analyze can be subjective but I chose Q1, Q2, Q3, Q4, Q5, Q6, Q7, Q9, Q10, Q11, Q12, Q13, Q14, and Q15. \n","1e6c7bdf":"Key takeaways from analyzing the demographics (questions 1 through 5) are:\n1. Age: Most participants are in the age groups of 18-21, 22-24, and 25-29.\n2. Gender: Participants are predominantly male. \n3. Country: Most participants are either from India or United States of America. Indian users outnumber US users by more than 2 times.\n4. Degree: Most participants have either a Master's or Bachelor's degree.\n5. Current Role: Most participants are Students with the second most popular role as Data Scientists.","6d28ccb2":"## Data Clean & Prep <a id = 'clean'><\/a>","b96dd2da":"## Objective <a id = 'obj'><\/a>","9bf6f285":"1. [Objective](#obj)\n2. [Data Clean and Prep](#clean)\n3. [Exploratory Data Analysis](#eda)\n4. [Clustering Users](#users)\n5. [Decribing Clusters](#desc)","dd06ca9e":"## Table of Contents","787121fd":"# Clustering Kaggle's User-base"}}