{"cell_type":{"12fd3d8d":"code","758144f2":"code","5e27a0d3":"code","40c62e94":"code","411ae91f":"code","e01c0435":"code","55cce2fa":"code","56090187":"code","2a7df958":"code","c228a07e":"code","99077a6d":"code","a490d485":"code","79dca7ee":"code","5713992d":"code","6a25b6b6":"code","01ccf8d3":"code","22b9b819":"code","14414d71":"code","1c0870c9":"code","c05e9b89":"code","27146f6b":"code","39dfdde2":"code","212edb16":"code","aaf8d579":"code","c2026cc4":"code","20899501":"code","ad1376cf":"code","9d15e159":"code","418db55f":"code","baae5e56":"code","d459ad52":"code","4bec72ce":"code","8125dfe0":"code","61bdd15b":"code","a3ba435b":"code","2bb9aaf4":"code","1389b250":"code","f15b93b5":"code","0ac53949":"code","f12bfdcd":"code","07015d49":"code","0f6305c0":"code","62d6a2e2":"code","6952e68f":"code","b9298e84":"code","bb6d65c5":"code","ffaf985a":"code","ecabfab8":"code","7625dc05":"code","c2aae425":"code","2722205e":"code","94eaa78b":"code","1fc95bfe":"code","95ae266f":"code","c7060da1":"code","bc43b5b5":"code","3a461a93":"code","e1cd7410":"code","abf15ed7":"code","bfad8da3":"code","08549c66":"code","db52c684":"code","62ff6232":"code","a8dacf0d":"code","f1dd7de4":"code","e943bd37":"code","f51084c5":"code","c2e65715":"code","e5ade98d":"code","bcb55cf7":"code","97ab438e":"code","bfa8e891":"code","35a05616":"code","92b2f3d7":"code","ccfcaba5":"code","0ddba51f":"code","35990984":"code","2ad282e8":"code","9cafb3f0":"code","8d634aaf":"code","6fd48be5":"code","a907daf6":"code","c7e6bc25":"code","1caf8e75":"code","4e421d45":"code","68e91ff6":"code","4a241830":"code","f05bb6d2":"code","6970689d":"code","1abfe539":"code","f5b7ca3e":"code","19319c58":"code","865c8fb6":"code","acb52892":"code","3722b1cb":"code","4664dee3":"code","b6945ce7":"code","0416e878":"code","5fc3c09a":"code","aaafb24f":"code","b5296d0f":"code","bf2b85d8":"code","3ec0c1ab":"code","6906da3f":"code","a40459cb":"code","46dc4f4b":"code","c0454c53":"code","06271cd9":"code","7cdb0934":"code","a2ee0690":"code","14536a7f":"code","aa242ae1":"code","6e24cfed":"code","d6b4af92":"code","3a6311f8":"code","4e53633d":"code","e1c4df85":"code","3b1567d7":"code","5f99af5d":"code","97972784":"markdown","b0b8ca82":"markdown","4c7e284a":"markdown","34f6c92e":"markdown","e9b94f5f":"markdown","7d65a00f":"markdown","254b422f":"markdown","748a553a":"markdown","e8aeff32":"markdown","061060f8":"markdown","962a41d1":"markdown","5b515379":"markdown","10463e85":"markdown","af1e0483":"markdown","cb276795":"markdown","669208fa":"markdown","bdc295fd":"markdown","457e6b00":"markdown","c0ecd0f6":"markdown","227ca6cd":"markdown","93544891":"markdown","69a0b170":"markdown","10b6b277":"markdown","55d4cdc5":"markdown","5cfa5cec":"markdown","72ae208a":"markdown","af766441":"markdown","2bf9b89d":"markdown","51f93bb9":"markdown","b1ec039e":"markdown","b052824f":"markdown","4390d251":"markdown","3a0d2596":"markdown","a0af80e4":"markdown","ad735293":"markdown","027f950c":"markdown","80ea2b7c":"markdown","15343d02":"markdown","522fdacd":"markdown","72439314":"markdown","6e879efb":"markdown","20188b89":"markdown","ebfe3621":"markdown","b40205c8":"markdown","d80f8b4b":"markdown","35555889":"markdown","ef4f2086":"markdown","3bbabc99":"markdown","026881ad":"markdown","f21a13d8":"markdown","4cd1972f":"markdown","dca69671":"markdown","eb266137":"markdown","f269756d":"markdown","0435e3b1":"markdown","f5429eb8":"markdown","973410db":"markdown","c47cf020":"markdown","62e41d92":"markdown","13c2e181":"markdown","92e0dc19":"markdown"},"source":{"12fd3d8d":"#common\nimport numpy as np\nimport pandas as pd \nimport IPython\nfrom IPython.display import display\nimport warnings\nwarnings.simplefilter('ignore')\n\n#visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#metrics and preprocessing\nfrom sklearn.metrics import SCORERS\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, precision_recall_curve\nfrom sklearn.model_selection import train_test_split, cross_validate, cross_val_score, GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV, KFold, validation_curve\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OrdinalEncoder, LabelEncoder, OneHotEncoder\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.utils import shuffle, resample\nfrom sklearn.decomposition import PCA, IncrementalPCA\n\n#classifiers\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n\n#tensorflow\nimport keras\nfrom keras import initializers\n\nfrom keras.utils import to_categorical\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam, RMSprop, SGD\nfrom keras import utils\nfrom keras.layers import Activation\nfrom keras.regularizers import l2\n\nfrom keras.callbacks import LearningRateScheduler\nfrom keras.callbacks import History\n\nfrom keras import losses\nfrom sklearn.utils import shuffle","758144f2":"subm = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\ntrain_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","5e27a0d3":"train_df.head()","40c62e94":"RND_ST = 42","411ae91f":"train_ind = train_df['PassengerId'].index\ntest_ind = test_df['PassengerId']","e01c0435":"test_ind = test_ind.tolist()\ntest_ind.append(891)\ntest_ind = np.sort(test_ind)\ntest_ind[:5]","55cce2fa":"full_df = pd.concat([train_df] + [test_df])","56090187":"full_df.head()","2a7df958":"train_df.info()","c228a07e":"test_df.info()","99077a6d":"total = full_df.isnull().sum().sort_values(ascending = False)\npercent = round(full_df.isnull().sum().sort_values(ascending = False)\/len(full_df)*100,2)\npd.concat([total, percent], axis=1, keys=['Total_missing','Percent_missing']).style.background_gradient(cmap='Reds')","a490d485":"full_df['Cabin'].unique()","79dca7ee":"print('Number of females in a train set is {}'.format(len(train_df.loc[train_df['Sex'] == 'female'])))\nprint('Number of males in a train set is {}'.format(len(train_df.loc[train_df['Sex'] == 'male'])))","5713992d":"temp_sex = train_df[['Survived', 'Sex']].groupby(['Sex'], as_index=False).mean()\n\nsns.set_style('whitegrid')\nplt.figure(figsize=(8,6))\nsns.barplot(x='Sex', y='Survived', data=temp_sex)\nplt.title('Difference in survived between males and women', size=15);","6a25b6b6":"plt.figure(figsize=(10,6))\n\nsns.distplot(full_df['Age'], color='darkgreen', bins=100)\n\nplt.title('Distribution of passengers age', size=15);","01ccf8d3":"plt.figure(figsize=(10,6))\n\n\nsns.kdeplot(full_df.loc[full_df['Survived']==1,'Age'], label='Survived', shade=True)\nsns.kdeplot(full_df.loc[full_df['Survived']==0,'Age'], label='Died', shade=True)\n\nplt.title('Comparison of sirvived and died ages', size=15);","22b9b819":"plt.figure(figsize=(10,8))\nsns.scatterplot(x=train_df['Fare'], y=train_df['Age'], hue=train_df['Survived'], style=train_df['Sex'])\nplt.title('Correlations between sex, ticket fare and age', size=15);","14414d71":"temp_pclass = train_df[['Pclass', 'Survived', 'Sex']].groupby(['Pclass', 'Sex'], as_index=False).mean()\n\nplt.figure(figsize=(8,6))\nsns.barplot(x='Pclass', y='Survived', hue='Sex', data=temp_pclass)\nplt.title('Difference in survival probability between Pclasses', size=15);","1c0870c9":"temp_pclass = train_df[['Pclass', 'Survived', 'Sex']].groupby(['Pclass', 'Sex'], as_index=False).mean()\n\nsns.set_style('whitegrid')\nplt.figure(figsize=(8,6))\nsns.barplot(x='Pclass', y='Survived', hue='Sex', data=temp_pclass)\nplt.title('Difference in survival probability between classes', size=15);","c05e9b89":"temp_pclass = train_df[['Parch', 'Survived']].groupby(['Parch'], as_index=False).mean()\n\nplt.figure(figsize=(8,5))\nsns.barplot(x='Parch', y='Survived', data=temp_pclass, color='darkgreen')\n\nplt.xlabel('Number of parents\/children')\nplt.ylabel('Surviving probability')\nplt.title('Survival probability of families with childer', size=15);","27146f6b":"temp_sibsp = train_df[['SibSp', 'Survived']].groupby(['SibSp'], as_index=False).mean()\n\nplt.figure(figsize=(8,5))\nsns.barplot(x='SibSp', y='Survived', data=temp_sibsp, color='darkgreen')\n\nplt.xlabel('Number of siblings \/ spouses')\nplt.ylabel('Surviving probability')\nplt.title('Survival probability in different family` sizes', size=15);","39dfdde2":"full_df.loc[full_df['Sex'] == 'male', 'Sex'] = 1\nfull_df.loc[full_df['Sex'] == 'female', 'Sex'] = 0\n\nfull_df['Sex'] = full_df['Sex'].astype('int')","212edb16":"import re","aaf8d579":"def title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\n\nfull_df['Title'] = full_df['Name'].apply(title)","c2026cc4":"full_df['Title'].value_counts()","20899501":"full_df['Title'] = full_df['Title'].replace(['Don', 'Rev', 'Dr','Major', 'Col', 'Capt', 'Countess','Jonkheer', 'Dona'], 'untitled')","ad1376cf":"full_df['Title'].unique()","9d15e159":"full_df.loc[full_df['Title']=='Master', 'Title'] = 'Mr'\nfull_df.loc[full_df['Title']=='Mlle', 'Title'] = 'Miss'\nfull_df.loc[full_df['Title']=='Mme', 'Title'] = 'Miss'\nfull_df.loc[full_df['Title']=='Ms', 'Title'] = 'Miss'\nfull_df.loc[full_df['Title']=='Sir', 'Title'] = 'Mr'\nfull_df.loc[full_df['Title']=='Lady', 'Title'] = 'Miss'","418db55f":"full_df['Title'].value_counts()","baae5e56":"temp_title = full_df[['Title', 'Survived']].groupby('Title', as_index=False).mean()\ntemp_title","d459ad52":"title_dict = dict(Mrs = 0, Miss = 1, untitled = 2, Mr = 3)\n\nfull_df['Title'] = full_df['Title'].replace(title_dict)\n\nfull_df = full_df.drop(['Name', 'Ticket', 'Cabin'], axis=1)","4bec72ce":"full_df.head()","8125dfe0":"full_df.query('Fare == \"NaN\"')","61bdd15b":"full_df['Fare'] = full_df.groupby('Pclass')['Fare'].transform(lambda x: x.fillna(value=x.median()))","a3ba435b":"plt.figure(figsize=(12,6))\n\nsns.distplot(full_df['Fare'], kde=False, bins=200, color='darkgreen')\nplt.xlim(-1,100)\nplt.title('Fare distribution in range 0-100 dollars', size=15);","2bb9aaf4":"def fare_into_bins(row):\n    \n    fare = row['Fare']\n    \n    if 0 <= fare < 15:\n        return 'cheap'\n    elif 15 <= fare < 42:\n        return 'moderate'\n    elif 42 <= fare < 65:\n        return 'expensive'\n    elif 65 <= fare < 100:\n        return 'business'\n    else:\n        return 'luxury'\n    \nfull_df['Fare_bins'] = full_df.apply(fare_into_bins, axis=1)","1389b250":"full_df[['Fare_bins', 'Survived']].groupby('Fare_bins',as_index=False).mean().sort_values(by='Survived', ascending=False)","f15b93b5":"full_df.loc[full_df['Embarked'].isna()]","0ac53949":"full_df[['Fare_bins','Embarked','Survived']].groupby(['Embarked','Fare_bins'], as_index=False).mean().query('Fare_bins == \"business\"')","f12bfdcd":"full_df['Embarked'] = full_df['Embarked'].fillna('C')","07015d49":"full_df_non_dum = full_df.copy()\nfull_df_non_dum = full_df_non_dum.drop('Fare', axis=1)\nfull_df_non_dum.head()","0f6305c0":"full_df = pd.get_dummies(full_df, drop_first=True)\n\nfull_df = full_df.drop('Fare', axis=1)","62d6a2e2":"full_df.head()","6952e68f":"full_df.loc[full_df['Age'] < 1, 'Age'] = 1","b9298e84":"full_df['Age'] = full_df.groupby(['Sex','Pclass'])['Age'].transform(lambda x: x.fillna(value=x.median()))\nfull_df['Age'] = full_df['Age'].astype('int')","bb6d65c5":"full_df.head()","ffaf985a":"full_df['Fam_size'] = full_df['SibSp'] + full_df['Parch'] + 1","ecabfab8":"full_df[['Fam_size', 'Survived']].groupby('Fam_size', as_index=False).mean()","7625dc05":"full_df['Fam_size'] = full_df['Fam_size'].replace([5,6,7,8,11],1)","c2aae425":"full_df[['Fam_size', 'Survived']].groupby('Fam_size', as_index=False).mean()","2722205e":"def alone(row):\n    \n    size=row['Fam_size']\n    \n    if size == 1:\n        return 1\n    else:\n        return 0\n    \nfull_df['Lon_trav'] = full_df.apply(alone, axis=1)","94eaa78b":"full_df[['Lon_trav','Survived']].groupby('Lon_trav', as_index=False).mean()","1fc95bfe":"full_df[['Pclass', 'Title', 'Survived']].groupby(['Pclass', 'Title'], as_index=False).mean().sort_values(by='Survived', ascending=False)","95ae266f":"pf = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)","c7060da1":"res = pf.fit_transform(full_df[['Pclass', 'Title']]).astype('int')\npoly_features = pd.DataFrame(res, columns=['Pclass^2', 'Title^2', 'Pc_T'])\n\n","bc43b5b5":"full_df = full_df.reset_index(drop=True)\nfull_df = full_df.join(poly_features)","3a461a93":"full_df['Pclass^2'] = full_df['Pclass^2']**2\nfull_df['Title^2'] = full_df['Title^2']**2","e1cd7410":"full_df.head()","abf15ed7":"full_df = full_df.drop(['SibSp','Parch'], axis=1)","bfad8da3":"full_df.isna().sum()","08549c66":"y_train = full_df.loc[:len(train_ind)-1, 'Survived'].astype('int')\n\nX_train = full_df.query('index in @train_ind').drop(['PassengerId', 'Survived'], axis=1)\nX_test = full_df.query('index in @test_ind').drop(['PassengerId', 'Survived'], axis=1)","db52c684":"X_train_aged = X_train.drop('Age', axis=1)\nX_test_aged = X_train.drop('Age', axis=1)","62ff6232":"MinMax = MinMaxScaler()\n\nMinMax.fit(X_train)\n\nX_train_scaled = MinMax.transform(X_train)\nX_test_scaled = MinMax.transform(X_test)\n\nMinMax.fit(X_train_aged)\nX_train_aged_sc = MinMax.transform(X_train_aged)\nX_test_aged_sc = MinMax.transform(X_test_aged)","a8dacf0d":"#### Logistic Regression\n\nlr = LogisticRegression(n_jobs=-1)\n\n\n### Random Forest\n\nrfc = RandomForestClassifier(random_state=RND_ST, n_jobs=-1)\n\nparams_rfc = dict(\n                  n_estimators=range(150,600,25),\n                  max_features=range(2,8),\n                  max_depth=range(3,7), \n                  min_samples_split=[2,3,4])\n\n\n### Nearest Neighbors\n\nknn = KNeighborsClassifier(n_jobs=-1)\n\nparams_knn = dict(\n                  metric = ['manhattan', 'minkowski'],\n                  n_neighbors = range(5,15),\n                  leaf_size=range(30,50,2))\n\n\n### Ada Boost\n\nada = AdaBoostClassifier(random_state=RND_ST)\n\nparams_ada = dict(\n                  n_estimators=range(100,500,25))\n\n\n### Support Vector\n\nsvc = SVC(random_state=RND_ST)\n\nparams_svc = dict(\n                  gamma = np.logspace(-6, -1, 5),\n                  C = [0.1,1,10,100,1000],\n                  tol=[1e-3, 1e-4, 1e-5])\n\n### Linear Support Vector\n\nlsv = LinearSVC(random_state=RND_ST)\n\nparams_lsv = dict(\n                  C = [0.1, 1, 10, 100], \n                  penalty = ['l1', 'l2'],\n                  max_iter = [1000,1500,2000])\n\n\n### Gradient Boosting\n\ngbc = GradientBoostingClassifier(random_state=RND_ST, max_depth=3)\n\nparams_gbc = dict(\n                  n_estimators=range(10,500,10),\n                  learning_rate=[0.01, 0.1, 1, 5, 10],\n                  max_leaf_nodes=range(1,6))\n\n### Stochaic Gradient Descent\n\nsgd = SGDClassifier(random_state=RND_ST)\n\nparams_sgd = dict(\n                  max_iter=[500,1000,1500],\n                  tol=[1e-3, 1e-4, 1e-5])","f1dd7de4":"clf = [rfc, lsv, knn, ada, svc, gbc, sgd]\nclf_names = ['random_forest', 'linear_vector', 'near_neighbours', 'ada_boost', 'support_vector', 'gradient_boosting', 'stochaic_gradient_descent']\nparams = [params_rfc, params_lsv, params_knn, params_ada, params_svc, params_gbc, params_sgd]","e943bd37":"def rs(model, params, feat, targ):\n    \n    search = RandomizedSearchCV(model, params, cv=7, scoring='f1', n_jobs=-1)\n    search.fit(feat, targ)\n\n    print(search.best_score_)\n    print(search.best_params_)","f51084c5":"def gs(model, params, feat, targ):\n    \n    gs = GridSearchCV(model, params, cv=7, scoring='f1', n_jobs=-1)\n    \n    gs.fit(feat, targ)\n\n    print(gs.best_score_)\n    print(gs.best_params_)","c2e65715":"for clf_, name, param in zip(clf, clf_names, params):\n    \n    print(name)\n    rs(clf_, param, X_train, y_train)\n    print()","e5ade98d":"for clf_, name, param in zip(clf, clf_names, params):\n    \n    print(name)\n    rs(clf_, param, X_train_aged, y_train)\n    print()","bcb55cf7":"gs(knn, params_knn, X_train_scaled, y_train)","97ab438e":"gs(svc, params_svc, X_train, y_train)","bfa8e891":"gs(lsv, params_lsv, X_train_scaled, y_train)","35a05616":"gs(lsv, params_lsv, X_train, y_train)","92b2f3d7":"gbc = GradientBoostingClassifier(random_state=RND_ST, max_depth=3)\n\nparams_gbc_estim = dict(\n                  n_estimators=range(10,500,5))\n\nparams_gbc_learn_rate = dict(\n                  learning_rate=[0.01, 0.1, 0.5, 1, 10],\n                  min_samples_split=[2,3,4],\n                  min_samples_leaf=[1,2,3],\n                  )","ccfcaba5":"gbc_1 = GradientBoostingClassifier(random_state=RND_ST, max_depth=3, n_estimators=85)","0ddba51f":"gs(gbc_1, params_gbc_learn_rate, X_train, y_train)\n","35990984":"# 0.7803240907336154","2ad282e8":"model_gbc_new = GradientBoostingClassifier(random_state=RND_ST, max_depth=3, n_estimators=85, learning_rate=0.5, min_samples_leaf=3)","9cafb3f0":"model_lsv = LinearSVC(C=0.1, max_iter=2000, penalty='l2', random_state=RND_ST)","8d634aaf":"model_lsv_1 = LinearSVC(C=0.1, max_iter=1000, penalty='l2', random_state=RND_ST)","6fd48be5":"model_svc = SVC(tol=0.0001, gamma=0.1, random_state=RND_ST)","a907daf6":"model_gdc_1 = GradientBoostingClassifier(\n    n_estimators=275, min_samples_split=2, min_samples_leaf=4, max_depth=4, learning_rate=0.1, random_state=RND_ST)","c7e6bc25":"model_gdc_1 = GradientBoostingClassifier(\n    n_estimators=130, max_depth=3, learning_rate=0.1, random_state=RND_ST)","1caf8e75":"model_svc_1 = SVC(C=1, gamma=0.1, tol=0.001, random_state=RND_ST)","4e421d45":"def plot_feature_importances(model, feat, targ):\n    \n    model.fit(feat, targ)\n    n_features = feat.shape[1]\n    plt.barh(range(n_features), model.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), feat.columns)\n    plt.xlabel(\"Features importance\")\n    plt.ylabel(\"Features\")\n","68e91ff6":"plot_feature_importances(model_gdc_1, X_train, y_train)","4a241830":"from catboost import CatBoostClassifier, Pool, cv\nimport lightgbm as lgb","f05bb6d2":"full_df_cat = (\n    full_df[['PassengerId', 'Survived','Pclass','Sex','Age','Title','Fam_size','Lon_trav','Pclass^2','Title^2','Pc_T']]\n    .merge(full_df_non_dum[['PassengerId','Fare_bins']], on='PassengerId')\n)","6970689d":"full_df_cat.head(3)","1abfe539":"X_train_cat = full_df_cat.query('index in @train_ind').drop(['PassengerId', 'Survived'], axis=1)\nX_test_cat = full_df_cat.query('index in @test_ind').drop(['PassengerId', 'Survived'], axis=1)\n\nX_train_cat_age = full_df_cat.query('index in @train_ind').drop(['PassengerId', 'Survived', 'Age'], axis=1)\nX_test_cat_age = full_df_cat.query('index in @test_ind').drop(['PassengerId', 'Survived', 'Age'], axis=1)\n\ny_train_cat = full_df_cat.loc[:len(train_ind)-1, 'Survived'].astype('int')","f5b7ca3e":"cat_features = ['Pclass', 'Sex', 'Title', 'Fam_size', 'Lon_trav', 'Pclass^2',\n       'Title^2', 'Pc_T', 'Fare_bins']","19319c58":"train_pool = Pool(X_train_cat,\n                 y_train_cat,\n                 cat_features=cat_features)\n\ntest_pool = Pool(X_test_cat,\n                 cat_features=cat_features)\n\ntrain_pool_age = Pool(X_train_cat_age,\n                 y_train_cat,\n                 cat_features=cat_features)\n\ntest_pool_age = Pool(X_test_cat_age,\n                 cat_features=cat_features)","865c8fb6":"cbr = CatBoostClassifier(iterations=2500,\n                         depth=2,\n                         learning_rate=1,\n                         loss_function='Logloss',\n                         random_seed=RND_ST,\n                         verbose=200)\n","acb52892":"cbr.fit(train_pool)\n\ny_pred = cbr.predict(X_train_cat)\n\nf1_score(y_train_cat, y_pred)","3722b1cb":"cbr.fit(train_pool_age)\n\ny_pred_ = cbr.predict(X_train_cat_age)\n\nf1_score(y_train_cat, y_pred)","4664dee3":"len(X_train_cat_age)","b6945ce7":"y_proba = cbr.predict_proba(train_pool)[:,1]\nroc_auc_score(y_train, y_proba)","0416e878":"y_proba = cbr.predict_proba(train_pool_age)[:,1]\nroc_auc_score(y_train, y_proba)","5fc3c09a":"pred_final = pd.DataFrame(cbr.predict(test_pool_age), columns=['Survived'])\nsubmission = pd.DataFrame(test_df['PassengerId'])\nsubmission = submission.join(pred_final)\nsubmission.to_csv('\/kaggle\/working\/titanic_sub_catboost_age.csv', index=False)\nsubmission.head()","aaafb24f":"full_df.columns","b5296d0f":"feat_1 = full_df[['Pclass', 'Sex', 'Age', 'Title',\n       'Embarked_Q', 'Embarked_S', 'Fare_bins_cheap', 'Fare_bins_expensive',\n       'Fare_bins_luxury', 'Fare_bins_moderate', 'Fam_size', 'Lon_trav']]\n\nX_train_1 = feat_1.query('index in @train_ind')\nX_test_1 = feat_1.query('index in @test_ind')","bf2b85d8":"ss = StandardScaler()\nss.fit(X_train_1)\n\nX_train_1_ss = ss.transform(X_train_1)\nX_test_1_ss = ss.transform(X_test_1)","3ec0c1ab":"ss.fit(X_train)\n\nX_train_ss = ss.transform(X_train)\nX_test_ss = ss.transform(X_test)","6906da3f":"def plot_hist(history):\n\n    plt.plot(history.history['acc'])\n    plt.plot(history.history['val_acc'])\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.grid()\n    plt.show()\n\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('model loss')\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train', 'test'], loc='upper left')\n    plt.grid()\n    plt.show()","a40459cb":"optimizer = RMSprop(learning_rate=0.0001,\n    rho=0.9,\n    epsilon=1e-07)","46dc4f4b":"optimizer = Adam(lr=0.0001)","c0454c53":"kernel_initializer='lecun_uniform'","06271cd9":"kernel_initializer='RandomNormal'","7cdb0934":"X_train_ss.shape","a2ee0690":"try:\n    del model3\n    print('refined')\nexcept:\n    print('next')\n\noptimizer = optimizer\n\nmodel3 = Sequential()\n\nmodel3.add(Dense(200, input_dim=15, activation='relu', kernel_initializer=kernel_initializer))\nmodel3.add(Dense(100, activation='relu', kernel_initializer=kernel_initializer))\nmodel3.add(Dense(50, activation='relu', kernel_initializer=kernel_initializer))\n\nmodel3.add(Dense(1, activation='sigmoid', kernel_initializer=kernel_initializer))\n\nmodel3.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['acc'])","14536a7f":"history = model3.fit(X_train_ss, y_train, epochs=1000, validation_split=0.1, batch_size=100, verbose=0)","aa242ae1":"plot_hist(history)","6e24cfed":"pred = model3.predict(X_train_ss).round()\nroc_auc_score(y_train, pred).round(4)","d6b4af92":"model3.summary()","3a6311f8":"def prediction(model, feat_tr, feat_test, targ):\n    \n    model.fit(feat_tr, targ)\n    \n\n    pred_final = pd.DataFrame(model.predict(feat_test), columns=['Survived'])\n \n    return pred_final","4e53633d":"pred = prediction(model_gbc_new, X_train, X_test, y_train)","e1c4df85":"submission = pd.DataFrame(test_df['PassengerId'])\n\nsubmission = submission.join(pred)\n\nsubmission.to_csv('\/kaggle\/working\/gbc_new.csv', index=False)","3b1567d7":"pred_final = model3.predict(X_test_ss).round().astype('int')\n\nsubmission = pd.DataFrame(test_df['PassengerId'])\n\nsubmission['Survived'] = pred_final\n\nsubmission.to_csv('\/kaggle\/working\/keras09.csv', index=False)","5f99af5d":"submission","97972784":"Let's try to find the best parameters for Gradient Boosting in two steps. First - define the number of estimators, then search for the best learning rate.","b0b8ca82":"gs(gbc, params_gbc_estim, X_train, y_train)","4c7e284a":"Create and fit the final model","34f6c92e":"Hallo, kagglers!\nMy very first participation in Kaggle competitions. \n\n#### Let's start with import libs and datasets.","e9b94f5f":"So, the same situation: women had much more chances to survive. We also can see a dramatically drop of surviving probability between Second and Third class women passengers.","7d65a00f":"Combine families with low chances to survive together.","254b422f":"### Model selection\n\n#### Featuers scaling  \n\nWe need this step for some kinds of classifiers.","748a553a":"If a passenger had from one to three family members on board, there was a high probability for one of them to survive. Also, if a passenger travelled alone, he or she had a chance to survive is approx  35.\nAs I mentioned above, we will use these columns and correlation to create some new features below.  ","e8aeff32":"Make train and test pools. Initialize the neural model.","061060f8":"### Submission  \n\n#### Final Prediction","962a41d1":"Test set has also a lot of missin cabin values and 86 missing age values.","5b515379":"There were twice more males on Titanic board in a train set, but only 19% of them survived.  ","10463e85":"Fitting the classifier with train data.","af1e0483":"We combined train and test datasets to make preprocessing much easier.  \n\nLet's explore the main information about data.","cb276795":"#### Polynomial features","669208fa":"#### Import data","bdc295fd":"#### Divide full set to train and test sets again\n","457e6b00":"#### Set a random state","c0ecd0f6":"I think it's not a crucial mistake to remove the 'Cabin' column because 77% of data is missing and we can't restore or create any important features using this column.  \n\nI'll also try to predict missing age values and fill missing Embarked cells bellow.  \n\nSeems like Parch and SibSp columns are unnecessary. But later we try to make some new features from it.\n","227ca6cd":"Let's drop age from training and test datasets.","93544891":"#### Lonely travellers","69a0b170":"Let's create some polynoms from Pclass and Title columns","10b6b277":"#### Setup some models","55d4cdc5":"#### Fill missing Embarked values","5cfa5cec":"### Visualization  \n\n#### Distribution of important features  \n\nThe most common question - was there any difference in chance to survive betveen males and women.","72ae208a":"Let's assume that these passengers came on board in Cherbourg","af766441":"Time to find the best model with f1 score (features without scaling).","2bf9b89d":"Split the updated dataset into train and test split.","51f93bb9":"Fill mising age values according to ticket class.","b1ec039e":"As we can see, women and children were at priority during evacuation and had much more chances to survive. Unfortunately, males were in huge trouble.  \nAlso, the more you paid for a ticket, the more chances to survive you had.  \n\nWhat about Pclass?","b052824f":"Most of children were resqued, and most of 20-years-old died.\n\nLet's see how fare, sex and age were affected to Survive.","4390d251":"We have some missing age values, and a lot of cabin' information. Also 2 missing port of embarkation are in training dataset.","3a0d2596":"#### Round fare values and divide it into some bins.","a0af80e4":"#### Finally, drop meaningless columns and check the full set","ad735293":"### Preprocessing and Feature enginereeng\n\n#### Turn Sex into numeric values","027f950c":"If you travelled alone, take my deep regrets.","80ea2b7c":"#### Encode Embarked and Fare_bin columns  \n\nAslo don't forget to make a non encoding copy of dataset.","15343d02":"---\n\nBest score :  \n0.79186 - model3, X_train_ss  \n0.77551 - LinearSVC (model_lsv, X_train, X_test)  \n0.77551 - CatBoost (cbr, train_pool_age, test_pool_age)","522fdacd":"Using CatBoost, we don't need to encode any 'str' categotrial feature! This method allows us to save a lot of time.  \n\nReturn all features in full_df to a categorial values.","72439314":"Finally, convert the titles into categorial feature and drop all unnecessary columns(Name, Ticket and Cabin)","6e879efb":"What is the most importance features for the final model?","20188b89":"0.77 - this is the best f1 value between all searches. Try to apply upgrated model to the test features.","ebfe3621":"#### Create a full dataset","b40205c8":"Create functions for CrossValidation and Randomized searches to find the best hyperparameters.","d80f8b4b":"Initialize categorial features.","35555889":"Most fare values lie in between 10-15 and 15-40 dollars per cabin. We will split the fare values into 5 bins: cheap, moderate, expensive, business, luxury.","ef4f2086":"Let's make a submission.","3bbabc99":"#### Export submission","026881ad":"Okay, seems like age is an important feature.","f21a13d8":"The more you pay, the more chances to survive you have.","4cd1972f":"#### Turn Name into Titles ","dca69671":"#### Family size  \n\nLets count a number of relatives on board\n\t\n","eb266137":"## Have yourself a little neural network\n\nupd 07\/09\/2020","f269756d":"Keras prediction","0435e3b1":"Combine rares values into one group 'untitled'","f5429eb8":"#### Fill missing Fare  \n\nWe have only one empty 'Fare' cell in full data.  \nFill it with median fare in 3rd class","973410db":"The most numbers of passengers were in age between 20 and 45. There were some babies and childern on board, and also some elder passengers.","c47cf020":"0.8647  \n\nloss ~0.4  ","62e41d92":"#### Fill missing Age\n\nRound babies' age to 1","13c2e181":"### Gradient boosting classifiers.  \n\nOk, what if we try to apply some modern boosting helpers to our classification task?  Let's check xgboost,LightGBM and CatBoost.","92e0dc19":"What about passenger' age"}}