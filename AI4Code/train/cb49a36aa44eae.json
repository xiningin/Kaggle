{"cell_type":{"8d9c852a":"code","244d690e":"code","e825f12c":"code","1f5c13d4":"code","6eb37776":"code","d0784581":"code","dc5fcb41":"code","0658c645":"code","b2350df2":"code","922c6b79":"code","329919e3":"code","59d29bda":"code","4db0bfeb":"code","0408c377":"code","23e5cc91":"code","75ad4aed":"code","7a379b2b":"code","1de9d2c3":"code","a03dafb2":"markdown","7cd63aea":"markdown","02cc677d":"markdown","50834622":"markdown","9f111e7f":"markdown","f5a5dfa0":"markdown","76698d23":"markdown"},"source":{"8d9c852a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","244d690e":"import os\nimport nltk\nimport math\nimport pandas as pd\nimport numpy as np\nimport keras\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom itertools import chain\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom keras.models import Sequential, Model\nfrom keras.layers import Dense, Activation, Embedding, Dropout, BatchNormalization, Activation, Input, \\\n    Conv1D, MaxPool1D, Flatten, Concatenate, Add\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\nstopwords = set(stopwords.words(\"english\"))\nps = PorterStemmer()\n\nfrom numpy.random import seed","e825f12c":"df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nprint(df)","1f5c13d4":"def load_data(file_name):\n\n    df = pd.read_csv(file_name)\n\n    return df[\"id\"], df[\"text\"], df[\"target\"]\ndef load_test_file(file_name):\n\n    df = pd.read_csv(file_name)\n\n    return df[\"id\"], df[\"text\"]\n\ndef load_labels(file_name):\n    \"\"\"\n    :param file_name: a file name, type: str\n    return a list of labels\n    \"\"\"\n    return pd.read_csv(file_name)[\"target\"]\n\ndef write_predictions(file_name, pred):\n    df = pd.DataFrame(zip(range(len(pred)), pred))\n    df.columns = [\"id\", \"target\"]\n    df.to_csv(file_name, index=False)","6eb37776":"def tokenize(text):\n    \"\"\"\n    :param text: a doc with multiple sentences, type: str\n    return a word list, type: list\n    e.g.\n    Input: 'Text mining is to identify useful information.'\n    Output: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n    \"\"\"\n    return nltk.word_tokenize(text)\n\ndef stem(tokens):\n    \"\"\"\n    :param tokens: a list of tokens, type: list\n    return a list of stemmed words, type: list\n    e.g.\n    Input: ['Text', 'mining', 'is', 'to', 'identify', 'useful', 'information', '.']\n    Output: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n    \"\"\"\n    ### equivalent code\n    # results = list()\n    # for token in tokens:\n    #     results.append(ps.stem(token))\n    # return results\n\n    return [ps.stem(token) for token in tokens]\n    \ndef filter_stopwords(tokens):\n    \"\"\"\n    :param tokens: a list of tokens, type: list\n    return a list of filtered tokens, type: list\n    e.g.\n    Input: ['text', 'mine', 'is', 'to', 'identifi', 'use', 'inform', '.']\n    Output: ['text', 'mine', 'identifi', 'use', 'inform', '.']\n    \"\"\"\n    ### equivalent code\n    # results = list()\n    # for token in tokens:\n    #     if token not in stopwords and not token.isnumeric():\n    #         results.append(token)\n    # return results\n\n    return [token for token in tokens if token not in stopwords and not token.isnumeric()]\n","d0784581":"def get_feats_dict(feats, min_freq=-1, max_freq=-1, max_size=-1):\n    \"\"\"\n    :param data: a list of features, type: list(list)\n    :param min_freq: the lowest fequency that the fequency of a feature smaller than it will be filtered out, type: int\n    :param max_freq: the highest fequency that the fequency of a feature larger than it will be filtered out, type: int\n    :param max_size: the max size of feature dict, type: int\n    return a feature dict that maps features to indices, sorted by frequencies\n    # Counter document: https:\/\/docs.python.org\/3.6\/library\/collections.html#collections.Counter\n    \"\"\"\n    # count all features\n    feat_cnt = Counter(feats) # [\"text\", \"text\", \"mine\"] --> {\"text\": 2, \"mine\": 1}\n    if max_size > 0 and min_freq == -1 and max_freq == -1:\n        valid_feats = [\"<pad>\", \"<unk>\"] + [f for f, cnt in feat_cnt.most_common(max_size-2)]\n    else:\n        valid_feats = [\"<pad>\", \"<unk>\"]\n        for f, cnt in feat_cnt.most_common():\n            if (min_freq == -1 or cnt >= min_freq) and \\\n                (max_freq == -1 or cnt <= max_freq):\n                valid_feats.append(f)\n    if max_size > 0 and len(valid_feats) > max_size:\n        valid_feats = valid_feats[:max_size]\n    print(\"Size of features:\", len(valid_feats))\n    \n    # build a mapping from features to indices\n    feats_dict = dict(zip(valid_feats, range(len(valid_feats))))\n    return feats_dict\n\ndef get_index_vector(feats, feats_dict, max_len):\n    \"\"\"\n    :param feats: a list of features, type: list\n    :param feats_dict: a dict from features to indices, type: dict\n    :param feats: a list of features, type: list\n    return a feature vector,\n    \"\"\"\n    # initialize the vector as all zeros\n    vector = np.zeros(max_len, dtype=np.int64)\n    for i, f in enumerate(feats):\n        if i == max_len:\n            break\n        # get the feature index, return 1 (<unk>) if the feature is not existed\n        f_idx = feats_dict.get(f, 1)\n        vector[i] = f_idx\n    return vector","dc5fcb41":"def build_CNN(input_length, vocab_size, embedding_size,\n              hidden_size, output_size,\n              kernel_sizes, num_filters, num_mlp_layers,\n              padding=\"valid\",\n              strides=1,\n              activation=\"relu\",\n              dropout_rate=0.0,\n              batch_norm=False,\n              l2_reg=0.0,\n              loss=\"categorical_crossentropy\",\n              optimizer=\"SGD\",\n              learning_rate=0.1,\n              metric=\"accuracy\"):\n    x = Input(shape=(input_length,))\n    \n    ################################\n    ###### Word Representation #####\n    ################################\n    # word representation layer\n    emb = Embedding(input_dim=vocab_size,\n                    output_dim=embedding_size,\n                    input_length=input_length,\n                    embeddings_initializer=keras.initializers.TruncatedNormal(mean=0.0, stddev=0.1, seed=0))(x)\n    \n    ################################\n    ########### Conv-Pool ##########\n    ################################\n    # convolutional and pooling layers\n    cnn_results = list()\n    for kernel_size in kernel_sizes:\n        # add convolutional layer\n        conv = Conv1D(filters=num_filters,\n                      kernel_size=(kernel_size,),\n                      padding=padding,\n                      strides=strides)(emb)\n        # add batch normalization layer\n        if batch_norm:\n            conv = BatchNormalization()(conv)\n        # add activation\n        conv = Activation(activation)(conv)\n        # add max-pooling\n        maxpool = MaxPool1D(pool_size=(input_length-kernel_size)\/\/strides+1)(conv)\n        cnn_results.append(Flatten()(maxpool))\n    \n    ################################\n    ##### Fully Connected Layer ####\n    ################################\n    h = Concatenate()(cnn_results) if len(kernel_sizes) > 1 else cnn_results[0]\n    h = Dropout(dropout_rate, seed=0)(h)\n    # multi-layer perceptron\n    for i in range(num_mlp_layers-1):\n        new_h = Dense(hidden_size,\n                      kernel_regularizer=keras.regularizers.l2(l2_reg))(h)\n        # add batch normalization layer\n        if batch_norm:\n            new_h = BatchNormalization()(new_h)\n        # add skip connection\n        if i == 0:\n            h = new_h\n        else:\n            h = Add()([h, new_h])\n        # add activation\n        h = Activation(activation)(h)\n    y = Dense(output_size,\n              activation=\"softmax\")(h)\n    \n    # set the loss, the optimizer, and the metric\n    if optimizer == \"SGD\":\n        optimizer = keras.optimizers.SGD(lr=learning_rate)\n    elif optimizer == \"RMSprop\":\n        optmizer = keras.optimizers.RMSprop(learning_rate=learning_rate)\n    elif optimizer == \"Adam\":\n        optmizer = keras.optimizers.Adam(learning_rate=learning_rate)\n    else:\n        raise NotImplementedError\n    model = Model(x, y)\n    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n    \n    return model","0658c645":"train_file = \"\/kaggle\/input\/nlp-getting-started\/train.csv\"\ntest = \"\/kaggle\/input\/nlp-getting-started\/test.csv\"\n# load data\ntrain_ids, train_texts, train_labels = load_data(train_file)\nresult_ids, result_texts = load_test_file(train_file)\nprint(result_texts)","b2350df2":"import numpy as np\nfrom sklearn.model_selection import train_test_split\ntrain_texts,test_texts, train_labels, test_labels = train_test_split(train_texts, train_labels , test_size=0.33, random_state=42)","922c6b79":"train_texts.shape","329919e3":"\n# extract features\ntrain_tokens = [tokenize(text) for text in train_texts]\ntest_tokens = [tokenize(text) for text in test_texts]\nresult_tokens = [tokenize(text) for text in result_texts]\n\ntrain_stemmed = [stem(tokens) for tokens in train_tokens]\ntest_stemmed = [stem(tokens) for tokens in test_tokens]\nresult_stemmed = [stem(tokens) for tokens in result_tokens]\n\ntrain_feats = [filter_stopwords(tokens) for tokens in train_stemmed]\ntest_feats = [filter_stopwords(tokens) for tokens in test_stemmed]\nresult_feats = [filter_stopwords(tokens) for tokens in result_stemmed]\n\n# build a mapping from features to indices\nfeats_dict = get_feats_dict(\n    chain.from_iterable(train_feats),\n    min_freq=min_freq)","59d29bda":"plt.figure(figsize=(10,4))\nlen_cnt = Counter([len(feats) for feats in train_feats])\nlen_key = sorted(len_cnt)\nplt.subplot(1,2,1)\nplt.plot(range(1, len(len_key)+1),\n         [len_cnt[l] for l in len_key])\nplt.xlabel(\"Sentence Length\")\nplt.ylabel(\"Length Frequency\")\n\nplt.subplot(1,2,2)\nplt.plot(range(1, len(len_key)+1),\n         np.cumsum([len_cnt[l] for l in len_key]))\nplt.xlabel(\"Sentence Length\")\nplt.ylabel(\"Cumulative Numbers\")","4db0bfeb":"max_len = 30\n\n# build the feats_matrix\n# convert each example to a index vector, and then stack vectors as a matrix\ntrain_feats_matrix = np.vstack(\n    [get_index_vector(f, feats_dict, max_len) for f in train_feats])\ntest_feats_matrix = np.vstack(\n    [get_index_vector(f, feats_dict, max_len) for f in test_feats])\nresult_feats_matrix = np.vstack(\n    [get_index_vector(f, feats_dict, max_len) for f in result_feats])\n\n# convert labels to label_matrix\nnum_classes = max(train_labels)\n# convert each label to a ont-hot vector, and then stack vectors as a matrix\ntrain_label_matrix = keras.utils.to_categorical(train_labels-1, num_classes=num_classes)\ntest_label_matrix = keras.utils.to_categorical(test_labels-1, num_classes=num_classes)","0408c377":"os.makedirs(\"models\", exist_ok=True)\n\nseed(0)\ntf.random.set_seed(0)\n\nmodel = build_CNN(input_length=max_len, vocab_size=len(feats_dict),\n                  embedding_size=100, hidden_size=100, output_size=num_classes,\n                  kernel_sizes=[1,2,3,4], num_filters=100, num_mlp_layers=3,\n                  activation=\"relu\")\ncheckpointer = keras.callbacks.ModelCheckpoint(\n    filepath=os.path.join(\"models\", \"cnn1_weights.hdf5\"),\n    monitor=\"val_accuracy\",\n    verbose=0,\n    save_best_only=True)\n\n\ncnn_history = model.fit(train_feats_matrix, train_label_matrix,\n                    validation_split=0.1,\n                    epochs=100, batch_size=100, verbose=0,\n                    callbacks=[checkpointer])\nmodel = keras.models.load_model(os.path.join(\"models\", \"cnn1_weights.hdf5\"))\n\ntrain_score = model.evaluate(train_feats_matrix, train_label_matrix,\n                             batch_size=100)\ntest_score = model.evaluate(test_feats_matrix, test_label_matrix,\n                            batch_size=100)\nprint(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\nprint(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])","23e5cc91":"result_label = model.predict(result_feats_matrix)\nprint(result_label)\nresult_label.shape","75ad4aed":"plt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.plot(cnn_history.history[\"loss\"], label=\"training\", color=\"blue\", linestyle=\"dashed\")\nplt.plot(cnn_history.history[\"val_loss\"], label=\"validation\", color=\"orange\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.subplot(1,2,2)\nplt.plot(cnn_history.history[\"accuracy\"], label=\"training\", color=\"blue\", linestyle=\"dashed\")\nplt.plot(cnn_history.history[\"val_accuracy\"], label=\"validation\", color=\"orange\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","7a379b2b":"seed(0)\ntf.random.set_seed(0)\n\nmodel = build_CNN(input_length=max_len, vocab_size=len(feats_dict),\n                  embedding_size=100, hidden_size=100, output_size=num_classes,\n                  kernel_sizes=[1,2,3,4], num_filters=100, num_mlp_layers=3,\n                  activation=\"relu\",\n                  dropout_rate=0.3, l2_reg=0.05, batch_norm=True)\ncheckpointer = keras.callbacks.ModelCheckpoint(\n    filepath=os.path.join(\"models\", \"cnn2_weights.hdf5\"),\n    monitor=\"val_accuracy\",\n    verbose=0,\n    save_best_only=True)\n\n\nhistory = model.fit(train_feats_matrix, train_label_matrix,\n                    validation_split=0.1,\n                    epochs=100, batch_size=100, verbose=0,\n                    callbacks=[checkpointer])\nmodel = keras.models.load_model(os.path.join(\"models\", \"cnn2_weights.hdf5\"))\n\ntrain_score = model.evaluate(train_feats_matrix, train_label_matrix,\n                             batch_size=100)\ntest_score = model.evaluate(test_feats_matrix, test_label_matrix,\n                            batch_size=100)\nprint(\"training loss:\", train_score[0], \"training accuracy\", train_score[1])\nprint(\"test loss:\", test_score[0], \"test accuracy\", test_score[1])","1de9d2c3":"plt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.plot(cnn_history.history[\"loss\"], label=\"training\", color=\"blue\", linestyle=\"dashed\")\nplt.plot(cnn_history.history[\"val_loss\"], label=\"validation\", color=\"orange\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.subplot(1,2,2)\nplt.plot(cnn_history.history[\"accuracy\"], label=\"training\", color=\"blue\", linestyle=\"dashed\")\nplt.plot(cnn_history.history[\"val_accuracy\"], label=\"validation\", color=\"orange\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\nplt.show()","a03dafb2":"## Try CNN for Text\nWe only select the stemmed features whose frequencies are no less than 3.\n","7cd63aea":"## Classifier: Convolutional Neural Network","02cc677d":"We then need to determine the maximum sequence length.","50834622":"## Feature Extractor","9f111e7f":"We can find that half of sentences are less than 20 words and most are less than 30 words. We can set the max_len as 30.","f5a5dfa0":"Split into train set and test set with test_size = 0.3","76698d23":"# Pipeline\n## Data Loader"}}