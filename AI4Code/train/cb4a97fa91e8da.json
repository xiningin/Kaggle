{"cell_type":{"f7549e6b":"code","870eca77":"code","b8bba80b":"code","1bc7de6b":"code","c327288a":"code","0de07e18":"code","22112a71":"markdown","733c090e":"markdown","45f06af8":"markdown","512e7451":"markdown","b315d55b":"markdown","bb1eb60d":"markdown"},"source":{"f7549e6b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","870eca77":"# Scikit-Learn kullanarak Ridge Regresyon uygulamas\u0131:\n\n# \u00d6nce rasggele veriler olu\u015ftural\u0131m:\nimport numpy as np\nimport numpy.random as rnd\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nm = 100\nX = 6 * np.random.rand(m, 1) - 3\ny = 0.5 * X**2 + X + 2 + np.random.randn(m, 1)\n\n# Ridge Regresyon (kapal\u0131-form \u00e7\u00f6z\u00fcm kullanarak)\nfrom sklearn.linear_model import Ridge\nridge_reg = Ridge(alpha=1, solver=\"cholesky\")\nridge_reg.fit(X, y)\nridge_reg.predict([[1.5]])","b8bba80b":"# Ridge Regresyon (Stochastic Gradient Descent kullanarak):\n\nfrom sklearn.linear_model import SGDRegressor\nsgd_reg_ridge = SGDRegressor(penalty=\"l2\")\nsgd_reg_ridge.fit(X, y.ravel())\nsgd_reg_ridge.predict([[1.5]])","1bc7de6b":"# Lasso s\u0131n\u0131f\u0131n\u0131 kullanan k\u00fc\u00e7\u00fck bir Scikit-Learn \u00f6rne\u011fi:\n\nfrom sklearn.linear_model import Lasso\nlasso_reg = Lasso(alpha=0.1)\nlasso_reg.fit(X,y)\nlasso_reg.predict([[1.5]])","c327288a":"# Lasso Regresyon (Stochastic Gradient Descent kullanarak):\n\nfrom sklearn.linear_model import SGDRegressor\nsgd_reg_lasso = SGDRegressor(penalty=\"l1\")\nsgd_reg_lasso.fit(X, y.ravel())\nsgd_reg_lasso.predict([[1.5]])","0de07e18":"# Scikit-Learn\u2019\u00fcn Elastik Net s\u0131n\u0131f\u0131n\u0131 kullanan k\u0131sa bir \u00f6rnek (l1_ratio, r kar\u0131\u015f\u0131m oran\u0131na kar\u015f\u0131l\u0131k gelir):\n\nfrom sklearn.linear_model import ElasticNet\nelastic_net=ElasticNet(l1_ratio=0.5,alpha=0.1)\nelastic_net.fit(X,y)\nelastic_net.predict([[1.5]])","22112a71":"# Ridge Regresyon\n\nRidge Regresyon(Tikhonov regularization da denir), Do\u011frusal Regresyonun d\u00fczenlile\u015ftirilmi\u015f bir versiyonudur: $\\alpha \\sum_{i=1}^{n} \\theta_{i}^{2}$ denklemine e\u015fit olan d\u00fczenlile\u015ftirme terimi, maliyet fonksiyonuna eklenir. Bu,\u00f6\u011frenme algoritmas\u0131n\u0131 hem verilere uymaya ,hemde modelin a\u011f\u0131rl\u0131klar\u0131n\u0131 m\u00fcmk\u00fcn oldu\u011funca k\u00fc\u00e7\u00fck tutmaya zorlar. Dikkat edilmesi gereken bir nokta: d\u00fczenlile\u015ftirme terimi,sadece e\u011fitim s\u0131ras\u0131nda maliyet fonksiyonuna eklenmelidir. Modeli e\u011fittikten sonra,modeli de\u011ferlendirmek i\u00e7in d\u00fczenle\u015ftirilmemi\u015f performans \u00f6l\u00e7\u00fctleri kullanabilirsiniz.\n\n$\\alpha$ hiperparametresi,modeli ne kadar d\u00fczenlile\u015ftirmek istedi\u011finizi kontrol eder. E\u011fer $\\alpha=0$ ise, Ridge Regresyon bir Do\u011frusal Regresyon olur. E\u011fer $\\theta$ \u00e7ok b\u00fcy\u00fck ise t\u00fcm a\u011f\u0131rl\u0131klar s\u0131f\u0131ra \u00e7ok yak\u0131n olacakt\u0131r.\nRidge Regresyon maliyet fonksiyonu:\n\n$$RSS_{RIDGE} = \\sum_{i=1}^{m}(h_{\\theta}(x_{i})-y_{i})^{2} + \\alpha \\sum_{j=1}^{n}\\theta^{2}_{j}$$\n\nBias terimi $\\theta_{0}$'\u0131n d\u00fczenlile\u015ftirilmedi\u011fine dikkat edin(toplama i=1'den ba\u015fl\u0131yor.).\n\n---------------\n\n# Uyar\u0131\n\nGirdi \u00f6zelliklerinin \u00f6l\u00e7e\u011fine duyarl\u0131 oldu\u011fu i\u00e7in Ridge Regresyon uygulamadan \u00f6nce verileri \u00f6l\u00e7eklendirmek (\u00f6rne\u011fin,StandartScaller kullanarak) \u00f6nemlidir. Bu,\u00e7o\u011fu do\u011frusal model i\u00e7in ge\u00e7erlidir.\n\n---------------\n\nDo\u011frusal Regresyon'da oldu\u011fu gibi,Ridge Regresyonu uygulamak i\u00e7in kapal\u0131-form bir denklem(do\u011frusal regresyondaki Normal Denklem gibi) veya Gradient Descent kullanabiliriz.\n\nRidge Regresyon kapal\u0131-form \u00e7\u00f6z\u00fcm\u00fc:\n\n$$\\hat{\\theta} = (X^{T}X+\\alpha A)^{-1}X^{T}y$$\n\nNOT: Denklemdeki A:(n+1)x(n+1)'lik birim matristir.","733c090e":"Peki ne zaman d\u00fcz Do\u011frusal Regresyon(yani herhangi bir d\u00fczenlile\u015ftirme olmadan),Ridge,Lasso veya Elastic Net kullanmal\u0131s\u0131n\u0131z? En az\u0131ndan biraz d\u00fczenlile\u015ftirme olmas\u0131 neredeyse her zaman tercih edilir bu sebeple d\u00fcz Do\u011frusal Regresyondan ka\u00e7\u0131nmal\u0131s\u0131n\u0131z. Ridge iyi bir varsay\u0131land\u0131r ancak e\u011fer sadece birka\u00e7 \u00f6zelli\u011fin faydal\u0131,i\u015fe yarar oldu\u011funu d\u00fc\u015f\u00fcn\u00fcyorsan\u0131z Lasso veya Elastic Net tercih etmelisiniz \u00e7\u00fcnk\u00fc Lasso ve Elastic Net faydas\u0131z,i\u015fe yaramaz \u00f6zelliklerin a\u011f\u0131rl\u0131klar\u0131n\u0131 s\u0131f\u0131r yapma e\u011filimindedirler. Genel olarak Lasso'ya kar\u015f\u0131 Elastic Net tercih edilir \u00e7\u00fcnk\u00fc Lasso, \u00f6zellik say\u0131s\u0131 e\u011fitim \u00f6rne\u011fi say\u0131s\u0131ndan daha fazla oldu\u011funda veya birka\u00e7 \u00f6zellik g\u00fc\u00e7l\u00fc bir \u015fekilde ili\u015fkili(correlated) oldu\u011funda d\u00fczensiz davranabilir.","45f06af8":"A\u015f\u0131r\u0131 \u00f6\u011frenmeyi(overfitting) azaltman\u0131n bit yolu modeli d\u00fczenlile\u015ftirmektir(yani modeli k\u0131s\u0131tlamakt\u0131r.). \u00d6rne\u011fin bir polinom modeli k\u0131s\u0131tlaman\u0131n basit bir yolu polinom derecelerinin say\u0131s\u0131n\u0131 azaltmakt\u0131r.\n\nDo\u011frusal bir model i\u00e7in d\u00fczenlile\u015ftirme,genellikle modelin katsay\u0131lar\u0131n\u0131(a\u011f\u0131rl\u0131klar\u0131n\u0131 - weights) k\u0131s\u0131tlayarak ger\u00e7ekle\u015ftirilir. Modelin katsay\u0131lar\u0131n\u0131 k\u0131s\u0131tlamak i\u00e7in 3 farkl\u0131 yol uygulayan Ridge Regression, Lasso Regression, ve Elastic Net'e bakaca\u011f\u0131z:","512e7451":"penalty hiperparametresi kullan\u0131lacak d\u00fczenlile\u015ftirme teriminin tipini belirler. \"l2\" olarak belirtirsek,maliyet fonksiyonuna model a\u011f\u0131rl\u0131klar\u0131n\u0131n karesi eklenir.Buna \"l2\" norm da denir ve maliyet fonksiyonuna model a\u011f\u0131rl\u0131klar\u0131n\u0131 l2 normunun eklenmesi basit\u00e7e Ridge Regresyondur.","b315d55b":"# Lasso Regresyon\n\nLeast Absolute Shrinkage and Selection Operator Regression(k\u0131saca Lasso Regression), Do\u011frusal Regresyonun d\u00fczenlile\u015ftirilmi\u015f ba\u015fka bir versiyonudur: Ridge Regresyon'da oldu\u011fu gibi, maliyet fonksiyonuna bir d\u00fczenlile\u015ftirme terimi eklenir. Ancak Lasso Regresyon a\u011f\u0131rl\u0131k vekt\u00f6r\u00fcn\u00fcn \"l1\" normunu kullan\u0131r.\n\nLasso Regresyon Maliyet Fonksiyonu:\n\n$$RSS_{LASSO} = \\sum_{i=1}^{m}(h_{\\theta}(x_{i})-y_{i})^{2} + \\alpha \\sum_{j=1}^{n}|\\theta_{j}|$$\n\nLasso Regresyonun \u00f6nemli bir \u00f6zelli\u011fi,en az \u00f6neme sahip \u00f6zellikleri s\u0131f\u0131ra e\u015fitleyerek onlar\u0131 eleme e\u011filiminde olmas\u0131d\u0131r.Ba\u015fka bir deyi\u015fle, Lasso Regresyon otomatik olarak \u00f6zellik se\u00e7imi ger\u00e7ekle\u015ftirir.","bb1eb60d":"# Elastic Net\n\nElastic Net, Ridge Regresyon ile Lasso Regresyonun ortas\u0131ndad\u0131r. Elastic Net'in maliyet fonksiyonu Ridge Regresyonun maliyet fonksiyonu ile Lasso Regresyonun maliyet fonksiyonunun kar\u0131\u015f\u0131m\u0131d\u0131r ve bu kar\u0131\u015f\u0131m\u0131,r hiperparametresini kullanarak kontrol edebilirsiniz. r=0 oldu\u011funda Elastic Net, Ridge Regresyona denktir ve r=1 oldu\u011funda Elastic Net, Lasso Regresyona denktir.\n\nElastic Net Maliyet Fonksiyonu:\n\n$$J(\\theta)=MSE(\\theta)+r \\alpha \\sum_{i=1}^n |\\theta_{i}|+\\frac{1-r}{2} \\alpha \\sum_{i=1}^{n} \\theta_{i}^{2}$$"}}