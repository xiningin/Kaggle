{"cell_type":{"1d0e9acb":"code","81f2c1ba":"code","782be3bb":"code","5f415646":"code","46093c31":"code","c21b958d":"code","d373eb54":"code","81ab4535":"code","b038292d":"code","54ae01a1":"code","e56efa48":"code","77a1f67a":"code","c82f6c78":"code","1e2d1ac6":"code","208678e3":"code","56473823":"code","2cda4772":"code","d49d8fc4":"code","be195afa":"code","74ab3ead":"code","3fc1f3d2":"code","14973006":"code","dca2ec93":"code","f9a4545b":"code","6581402b":"code","91ca76db":"code","ef95b528":"code","28f32f37":"code","8a24aa2e":"code","42c6903a":"code","449c1172":"code","be11d263":"code","678effad":"code","de4e5f80":"code","6f0f3ce0":"code","00ef13f8":"code","01db4050":"code","b83d8a44":"code","1b31189e":"code","950c1449":"code","a21eac2e":"code","0a0cf00f":"code","6c26a4da":"code","9ce7e2af":"code","b975c99d":"code","267b75dc":"code","05fb2f85":"code","0ce41367":"code","ad94f91e":"code","af20ea49":"code","e27f096f":"code","f14ba45f":"code","389155e5":"code","08116471":"code","ae4772b5":"code","c40c0adf":"code","cde7a8b0":"code","867f0451":"code","10516044":"code","fafebaf9":"code","ace41ebd":"code","d4ae7f3f":"code","48c1c125":"code","9015d1ef":"code","7b58520b":"code","c6c90cd7":"markdown","6fd0296d":"markdown","e16ba84d":"markdown","f50043af":"markdown","39bbeac2":"markdown","6f8f9089":"markdown","85d8febd":"markdown","7f881a3d":"markdown","0711fdc0":"markdown","9b876018":"markdown","b1dfe046":"markdown","8cbdc6f5":"markdown","10cd47c2":"markdown","8e878e55":"markdown","6e6db3c3":"markdown","b5dd325c":"markdown","9313e21f":"markdown","e4fca39e":"markdown","4d59289d":"markdown","595ecea3":"markdown","5bd2d9de":"markdown","1dce04b8":"markdown","fab4b8be":"markdown","df1c3d6a":"markdown","27ecea33":"markdown","fc0ea767":"markdown","1244d954":"markdown"},"source":{"1d0e9acb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\n\n# plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\n\nfrom sklearn.model_selection import train_test_split","81f2c1ba":"def fxn():\n    warnings.warn(\"deprecated\", DeprecationWarning)\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    fxn()","782be3bb":"train_df = pd.read_csv('\/kaggle\/input\/eurecom-aml-2021-challenge-3\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/eurecom-aml-2021-challenge-3\/test.csv')\n\ntest_df[\"target\"] = 1","5f415646":"target_conversion = {\n    'negative': 0,\n    'neutral': 1,\n    'positive': 2,\n}\n\nclass_names = ['negative', 'neutral', 'positive']","46093c31":"train_df['target'] = train_df['sentiment'].map(target_conversion)","c21b958d":"!pip install -qq transformers\n!pip install watermark\n\n%reload_ext watermark\n%watermark -v -p numpy,pandas,torch,transformers\n\nimport transformers\nfrom transformers import BertModel, BertConfig, BertForSequenceClassification, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nfrom transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\nfrom transformers import AutoModel, AutoTokenizer \nimport torch\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom pylab import rcParams\nimport matplotlib.pyplot as plt\nfrom matplotlib import rc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom collections import defaultdict\nfrom textwrap import wrap\nfrom torch import nn, optim\nfrom torch.utils.data import Dataset, DataLoader\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\n\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\nrcParams['figure.figsize'] = 12, 8\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\ntorch.manual_seed(RANDOM_SEED)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","d373eb54":"ax = sns.countplot(train_df.target)\nplt.xlabel('review sentiment')\nplt.title('Class Imbalance Diagram')\nax.set_xticklabels(class_names);","81ab4535":"PuBu_palette = sns.color_palette(\"PuBu\", 10)\nYlGnBu_palette = sns.color_palette(\"YlGnBu\", 10)\nsns.palplot(PuBu_palette)\nsns.palplot(YlGnBu_palette)","b038292d":"three_PuBu_palette = list()\nthree_PuBu_palette.append(PuBu_palette[2])\nthree_PuBu_palette.append(PuBu_palette[6])\nthree_PuBu_palette.append(PuBu_palette[4])\nthree_PuBu_palette","54ae01a1":"def get_length_alphabets(text):\n    text = str(text)\n    return len(text)","e56efa48":"def get_length_words(text):\n    text = str(text)\n    return len(text.split(' '))","77a1f67a":"train_df['length_alphabets'] = train_df['text'].apply(lambda x: get_length_alphabets(x))\ntrain_df['length_words'] = train_df['text'].apply(lambda x: get_length_words(x))\ntrain_df.head()","c82f6c78":"fig = plt.figure(figsize=(12, 8))\ngs = fig.add_gridspec(2,1)\n\naxes = list()\n\nfor index, data in zip(range(2), train_df):\n    axes.append(fig.add_subplot(gs[index, 0]))\n    \n    \n    if index==0:\n        sns.kdeplot(x='length_alphabets', data=train_df, \n                        fill=True, ax=axes[index], cut=0, bw_method=0.20, \n                        lw=1.4 , hue='sentiment', palette=three_PuBu_palette,\n                         alpha=0.3)\n    else:\n        sns.kdeplot(x='length_words', data=train_df, \n                    fill=True, ax=axes[index], cut=0, bw_method=0.20, \n                    lw=1.4 , hue='sentiment',palette=three_PuBu_palette,\n                     alpha=0.3) \n\n    axes[index].set_yticks([])\n    if index != 1 : axes[index].set_xticks([])\n    axes[index].set_ylabel('')\n    axes[index].set_xlabel('')\n    axes[index].spines[[\"top\",\"right\",\"left\",\"bottom\"]].set_visible(False)\n    \n    \n    if index == 0:\n        axes[index].text(-0.2,0,\"length_alphabets\",fontweight=\"light\", fontfamily='serif', fontsize=13,ha=\"right\")\n    else:\n        axes[index].text(-0.2,0,\"length_words\",fontweight=\"light\", fontfamily='serif', fontsize=13,ha=\"right\")\n        \n        \n    axes[index].patch.set_alpha(0)\n    if index != 0 : axes[index].get_legend().remove()\n        \nfig.text(0.05,0.91,\"Count distribution by length in Tweets\", fontweight=\"bold\", fontfamily='serif', fontsize=20)\nplt.show()","1e2d1ac6":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","208678e3":"fig, axes = plt.subplots(1,3, figsize=(12, 10), constrained_layout=True)\n\nsentiment_list = list(np.unique(train_df['sentiment']))\n\nfor i, sentiment in zip(range(3), sentiment_list):\n    top_tweet_bigrams = get_top_tweet_bigrams(train_df[train_df['sentiment']==sentiment]['text'].fillna(\" \"))[:10]\n    x,y = map(list,zip(*top_tweet_bigrams))\n    sns.barplot(x=y, y=x, ax=axes[i], palette=PuBu_palette[::-1])\n    axes[i].text(0,-0.7, sentiment, fontweight=\"bold\", fontfamily='serif', fontsize=13,ha=\"right\")\n    axes[i].patch.set_alpha(0)\n\nfig.text(0,1.01,\"Bi-gram by {}texts in Tweets\".format(sentiment), fontweight=\"bold\", fontfamily='serif', fontsize=18)\nplt.show()","56473823":"from PIL import Image\n\nmask_dir = np.array(Image.open('..\/input\/twittermask\/twitter_mask3.jpg'))","2cda4772":"from wordcloud import WordCloud\n\nfig, axes = plt.subplots(1,3, figsize=(24,12))\nsentiment_list = np.unique(train_df['sentiment'])\n\nfor i, sentiment in zip(range(3), sentiment_list):\n    wc = WordCloud(background_color=\"white\", max_words = 2000, width = 1600, height = 800, mask=mask_dir, colormap=\"Blues\").generate(\" \".join(train_df[train_df['sentiment']==sentiment]['text']))\n    \n    axes[i].text(0.5,1, \"{} text\".format(sentiment), fontweight=\"bold\", fontfamily='serif', fontsize=17)\n    axes[i].patch.set_alpha(0)\n    axes[i].axis('off')\n    axes[i].imshow(wc)\n\nfig.text(0.1,0.8,\"WordCloud by sentiment per selected text in Tweets\", fontweight=\"bold\", fontfamily='serif', fontsize=20)\nplt.show()","d49d8fc4":"import re\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize \nfrom wordcloud import WordCloud, STOPWORDS\nimport spacy","be195afa":"nlp = spacy.load('en', disable=['parser', 'ner'])","74ab3ead":"stop = stopwords.words('english')\n\n#train_df['text'] = train_df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))\n#test_df['text'] = test_df['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in stop))","3fc1f3d2":"def space(comment):\n    doc = nlp(comment)\n    return \" \".join([token.lemma_ for token in doc])\n\n#train_df['text']= train_df['text'].apply(space)\n#test_df['text']= test_df['text'].apply(space)","14973006":"# Function for url's\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\n#train_df['text']= train_df['text'].apply(remove_urls)\n#test_df['text']= test_df['text'].apply(remove_urls)","dca2ec93":"!pip install emot\nfrom emot.emo_unicode import UNICODE_EMO, EMOTICONS","f9a4545b":"# Converting emojis to words\ndef convert_emojis(text):\n    for emot in UNICODE_EMO:\n        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n    return text\n\n# Converting emoticons to words    \ndef convert_emoticons(text):\n    for emot in EMOTICONS:\n        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n    return text\n\n# Example\ntext = \"Hello :x\"\nprint(convert_emoticons(text))\n\ntext1 = \"Hilarious \ud83d\ude02\"\nprint(convert_emojis(text1))\n\n#train_df['text']= train_df['text'].apply(convert_emojis)\n#train_df['text']= train_df['text'].apply(convert_emoticons)\n\n#test_df['text']= test_df['text'].apply(convert_emojis)\n#test_df['text']= test_df['text'].apply(convert_emoticons)","6581402b":"!pip install pyspellchecker\nfrom spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n\ntext = \"speling correctin\"\nprint(correct_spellings(text))\ntext = \"thnks for readin the notebook\"\nprint(correct_spellings(text))\n\n#train_df['text']= train_df['text'].apply(correct_spellings)\n#test_df['text']= test_df['text'].apply(correct_spellings)","91ca76db":"def repetition(text):\n    return re.sub(\"(.)\\\\1{3,}\", \"\\\\1\", text)\n\n#train_df['text']= train_df['text'].apply(repetition)\n#test_df['text']= test_df['text'].apply(repetition)","ef95b528":"chat_words_str = \"\"\"\nAFAIK=As Far As I Know\nAFK=Away From Keyboard\nASAP=As Soon As Possible\nATK=At The Keyboard\nATM=At The Moment\nA3=Anytime, Anywhere, Anyplace\nBAK=Back At Keyboard\nBBL=Be Back Later\nBBS=Be Back Soon\nBFN=Bye For Now\nB4N=Bye For Now\nBRB=Be Right Back\nBRT=Be Right There\nBTW=By The Way\nB4=Before\nB4N=Bye For Now\nCU=See You\nCUL8R=See You Later\nCYA=See You\nFAQ=Frequently Asked Questions\nFC=Fingers Crossed\nFWIW=For What It's Worth\nFYI=For Your Information\nGAL=Get A Life\nGG=Good Game\nGN=Good Night\nGMTA=Great Minds Think Alike\nGR8=Great!\nG9=Genius\nIC=I See\nICQ=I Seek you (also a chat program)\nILU=ILU: I Love You\nIMHO=In My Honest\/Humble Opinion\nIMO=In My Opinion\nIOW=In Other Words\nIRL=In Real Life\nKISS=Keep It Simple, Stupid\nLDR=Long Distance Relationship\nLMAO=Laugh My fuck Off\nLOL=Laughing Out Loud\nLTNS=Long Time No See\nL8R=Later\nMTE=My Thoughts Exactly\nM8=Mate\nNRN=No Reply Necessary\nOIC=Oh I See\nPITA=Pain In The fuck\nPRT=Party\nPRW=Parents Are Watching\nROFL=Rolling On The Floor Laughing\nROFLOL=Rolling On The Floor Laughing Out Loud\nROTFLMAO=Rolling On The Floor Laughing My fuck Off\nSK8=Skate\nSTATS=Your sex and age\nASL=Age, Sex, Location\nTHX=Thank You\nTTFN=Ta-Ta For Now!\nTTYL=Talk To You Later\nU=You\nU2=You Too\nU4E=Yours For Ever\nWB=Welcome Back\nWTF=What The fuck\nWTG=Way To Go!\nWUF=Where Are You From?\nWKDN=Week-End\nW8=Wait...\n7K=Sick Laugher\n\"\"\"\n\nchat_words_map_dict = {}\nchat_words_list = []\nfor line in chat_words_str.split(\"\\n\"):\n    if line != \"\":\n        cw = line.split(\"=\")[0]\n        cw_expanded = line.split(\"=\")[1]\n        chat_words_list.append(cw)\n        chat_words_map_dict[cw] = cw_expanded\nchat_words_list = set(chat_words_list)\n\ndef chat_words_conversion(text):\n    new_text = []\n    for w in text.split():\n        if w.upper() in chat_words_list:\n            new_text.append(chat_words_map_dict[w.upper()])\n        else:\n            new_text.append(w)\n    return \" \".join(new_text)\n\nprint(chat_words_conversion(\"one minute BRB\"))\n\n#train_df['text']= train_df['text'].apply(chat_words_conversion)\n#test_df['text']= test_df['text'].apply(chat_words_conversion)","28f32f37":"#train_df['text'] = train_df['text'].apply(lambda x: re.sub(r'@mention', '', x))\n#test_df['text'] = test_df['text'].apply(lambda x: re.sub(r'@mention', '', x))","8a24aa2e":"def remove_numbers(text):\n    return re.sub('[0\u20139]','',text)\n\n#train_df[\"text\"] = train_df[\"text\"].apply(remove_numbers)\n#test_df[\"text\"] = test_df[\"text\"].apply(remove_numbers)","42c6903a":"def remove_mul_white_spaces(text):\n    return re.sub(' +', ' ',text).strip()\n\n#train_df[\"text\"] = train_df[\"text\"].apply(remove_mul_white_spaces)\n#test_df[\"text\"] = test_df[\"text\"].apply(remove_mul_white_spaces)","449c1172":"import string\n\nPUNCT_TO_REMOVE = string.punctuation\n\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n\n#train_df[\"text\"] = train_df[\"text\"].apply(remove_punctuation)\n#test_df[\"text\"] = test_df[\"text\"].apply(remove_punctuation)","be11d263":"PRE_TRAINED_MODEL_NAME = 'bert-base-cased'","678effad":"tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)","de4e5f80":"token_lens = []\n\n#for txt in train_df.text:\nfor txt in train_df.text:\n  tokens = tokenizer.encode(txt, max_length=512)\n  token_lens.append(len(tokens))","6f0f3ce0":"sns.distplot(token_lens)\nplt.xlim([0, 256]);\nplt.xlabel('Token count');\nplt.title('Tokenization max length choice')\nplt.show()","00ef13f8":"MAX_LEN = 120","01db4050":"class GPReviewDataset(Dataset):\n  def __init__(self, reviews, targets, tokenizer, max_len):\n    self.reviews = reviews\n    self.targets = targets\n    self.tokenizer = tokenizer\n    self.max_len = max_len\n  def __len__(self):\n    return len(self.reviews)\n  def __getitem__(self, item):\n    review = str(self.reviews[item])\n    target = self.targets[item]\n    encoding = self.tokenizer.encode_plus(\n      review,\n      add_special_tokens=True,\n      max_length=self.max_len,\n      return_token_type_ids=False,\n      pad_to_max_length=True,\n      return_attention_mask=True,\n      return_tensors='pt',\n    )\n    return {\n      'review_text': review,\n      'input_ids': encoding['input_ids'].flatten(),\n      'attention_mask': encoding['attention_mask'].flatten(),\n      'targets': torch.tensor(target, dtype=torch.long)\n    }","b83d8a44":"df_train, df_test = train_test_split(\n  train_df,\n  test_size=0.05,\n  random_state=RANDOM_SEED\n)\ndf_val, df_test = train_test_split(\n  df_test,\n  test_size=0.5,\n  random_state=RANDOM_SEED\n)\n\nprint(df_train.shape, df_test.shape, df_val.shape)","1b31189e":"def create_data_loader(df, tokenizer, max_len, batch_size):\n  ds = GPReviewDataset(\n    #reviews=df.text.to_numpy(),\n    reviews=df.text.to_numpy(),\n    targets=df.target.to_numpy(),\n    tokenizer=tokenizer,\n    max_len=max_len\n  )\n  return DataLoader(\n    ds,\n    batch_size=batch_size,\n    num_workers=4\n  )","950c1449":"BATCH_SIZE = 16 # originally 16\ntrain_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)\nval_data_loader = create_data_loader(df_val, tokenizer, MAX_LEN, BATCH_SIZE)\ntest_data_loader = create_data_loader(df_test, tokenizer, MAX_LEN, BATCH_SIZE)\npred_data_loader = create_data_loader(test_df, tokenizer, MAX_LEN, BATCH_SIZE)","a21eac2e":"data = next(iter(train_data_loader))\ndata.keys()","0a0cf00f":"print(data['input_ids'].shape)\nprint(data['attention_mask'].shape)\nprint(data['targets'].shape)","6c26a4da":"bert_model = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)","9ce7e2af":"#bert_model.resize_token_embeddings(MAX_LEN)","b975c99d":"class SentimentClassifier(nn.Module):\n  def __init__(self, n_classes):\n    super(SentimentClassifier, self).__init__()\n    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n    self.drop = nn.Dropout(torch.tensor(0.3)) #originally 0.3\n    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n  def forward(self, input_ids, attention_mask):\n    _, pooled_output = self.bert(\n      input_ids=input_ids,\n      attention_mask=attention_mask,\n      return_dict=False\n    )\n    output = self.drop(pooled_output)\n    return self.out(output)","267b75dc":"model = SentimentClassifier(len(class_names))\nmodel = model.to(device)","05fb2f85":"input_ids = data['input_ids'].to(device)\nattention_mask = data['attention_mask'].to(device)\nprint(input_ids.shape) # batch size x seq length\nprint(attention_mask.shape) # batch size x seq length","0ce41367":"print(data['input_ids'])\nprint(data['attention_mask'])","ad94f91e":"torch.nn.functional.softmax(model(input_ids, attention_mask), dim=1)","af20ea49":"EPOCHS = 3\noptimizer = AdamW(model.parameters(), lr=2e-5, correct_bias=False) #2e-5 False\ntotal_steps = len(train_data_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(\n  optimizer,\n  num_warmup_steps=0,\n  num_training_steps=total_steps\n)\nloss_fn = nn.CrossEntropyLoss().to(device)","e27f096f":"def train_epoch(\n  model,\n  data_loader,\n  loss_fn,\n  optimizer,\n  device,\n  scheduler,\n  n_examples\n):\n  model = model.train()\n  losses = []\n  correct_predictions = 0\n  for d in data_loader:\n    input_ids = d[\"input_ids\"].to(device)\n    attention_mask = d[\"attention_mask\"].to(device)\n    targets = d[\"targets\"].to(device)\n    outputs = model(\n      input_ids=input_ids,\n      attention_mask=attention_mask\n    )\n    _, preds = torch.max(outputs, dim=1)\n    loss = loss_fn(outputs, targets)\n    correct_predictions += torch.sum(preds == targets)\n    losses.append(loss.item())\n    loss.backward()\n    nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n    optimizer.step()\n    scheduler.step()\n    optimizer.zero_grad()\n  return correct_predictions.double() \/ n_examples, np.mean(losses)","f14ba45f":"def eval_model(model, data_loader, loss_fn, device, n_examples):\n  model = model.eval()\n  losses = []\n  correct_predictions = 0\n  with torch.no_grad():\n    for d in data_loader:\n      input_ids = d[\"input_ids\"].to(device)\n      attention_mask = d[\"attention_mask\"].to(device)\n      targets = d[\"targets\"].to(device)\n      outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n      )\n      _, preds = torch.max(outputs, dim=1)\n      loss = loss_fn(outputs, targets)\n      correct_predictions += torch.sum(preds == targets)\n      losses.append(loss.item())\n  return correct_predictions.double() \/ n_examples, np.mean(losses)","389155e5":"%%time\nhistory = defaultdict(list)\nbest_accuracy = 0\nfor epoch in range(EPOCHS):\n  print(f'Epoch {epoch + 1}\/{EPOCHS}')\n  print('-' * 10)\n  train_acc, train_loss = train_epoch(\n    model,\n    train_data_loader,\n    loss_fn,\n    optimizer,\n    device,\n    scheduler,\n    len(df_train)\n  )\n  print(f'Train loss {train_loss} accuracy {train_acc}')\n  val_acc, val_loss = eval_model(\n    model,\n    val_data_loader,\n    loss_fn,\n    device,\n    len(df_val)\n  )\n  print(f'Val   loss {val_loss} accuracy {val_acc}')\n  print()\n  history['train_acc'].append(train_acc)\n  history['train_loss'].append(train_loss)\n  history['val_acc'].append(val_acc)\n  history['val_loss'].append(val_loss)\n  if val_acc > best_accuracy:\n    torch.save(model.state_dict(), 'best_model_state.bin')\n    best_accuracy = val_acc","08116471":"plt.plot(history['train_acc'], label='train accuracy')\nplt.plot(history['val_acc'], label='validation accuracy')\nplt.title('Training history')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend()\nplt.ylim([0, 1]);","ae4772b5":"test_acc, _ = eval_model(\n  model,\n  test_data_loader,\n  loss_fn,\n  device,\n  len(df_test)\n)\ntest_acc.item()","c40c0adf":"def get_predictions(model, data_loader):\n  model = model.eval()\n  review_texts = []\n  predictions = []\n  prediction_probs = []\n  real_values = []\n  with torch.no_grad():\n    for d in data_loader:\n      texts = d[\"review_text\"]\n      input_ids = d[\"input_ids\"].to(device)\n      attention_mask = d[\"attention_mask\"].to(device)\n      targets = d[\"targets\"].to(device)\n      outputs = model(\n        input_ids=input_ids,\n        attention_mask=attention_mask\n      )\n      _, preds = torch.max(outputs, dim=1)\n      review_texts.extend(texts)\n      predictions.extend(preds)\n      prediction_probs.extend(outputs)\n      real_values.extend(targets)\n  predictions = torch.stack(predictions).cpu()\n  prediction_probs = torch.stack(prediction_probs).cpu()\n  real_values = torch.stack(real_values).cpu()\n  return review_texts, predictions, prediction_probs, real_values","cde7a8b0":"y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n  model,\n  test_data_loader\n)","867f0451":"print(classification_report(y_test, y_pred, target_names=class_names))","10516044":"def show_confusion_matrix(confusion_matrix):\n  hmap = sns.heatmap(confusion_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n  hmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\n  hmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\n  plt.ylabel('True sentiment')\n  plt.xlabel('Predicted sentiment');\n  plt.title('Confusion Matrix')\ncm = confusion_matrix(y_test, y_pred)\ndf_cm = pd.DataFrame(cm, index=class_names, columns=class_names)\nshow_confusion_matrix(df_cm)","fafebaf9":"#for i in range(len(y_pred)):\n#   if(y_test[i] !=  y_pred[i]):\n#        print(y_review_texts[i])","ace41ebd":"idx = 16\nreview_text = y_review_texts[idx]\ntrue_sentiment = y_test[idx]\npred_df = pd.DataFrame({\n  'class_names': class_names,\n  'values': y_pred_probs[idx]\n})\n\nsns.barplot(x='values', y='class_names', data=pred_df, orient='h')\nplt.ylabel('sentiment')\nplt.xlabel('probability')\nplt.xlim([0, 1]);\nplt.title(\"Prediction confidence for tweet \" + str(test_df.iloc[idx - 1].textID))","d4ae7f3f":"y_review_texts, y_pred, y_pred_probs, y_test = get_predictions(\n  model,\n  pred_data_loader\n)","48c1c125":"# from 0,1,2 model for softmax function to -1, 0, 1\n\ny_pred -= 1","9015d1ef":"submission_df = pd.DataFrame()\nsubmission_df['textID'] = test_df['textID']\nsubmission_df['sentiment'] = y_pred.detach().numpy()\nsubmission_df.to_csv('BERT.csv', index=False)","7b58520b":"def jaccard(str1, str2): \n    a = set(str1.lower().split()) \n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","c6c90cd7":"# Challenge 3 : Team 45, BERT Base Model\n\n## Imports","6fd0296d":"### Removing numerical data","e16ba84d":"We keep the lower and upper cases because it can have an impact on the sentiment.\nRemoving the ponctuation was also not efficient.\n\nAfter some transformation, the reviews are much cleaner, but we still have some words that we should remove, namely the stopwords. Stopwords are commonly used words (i.e. \u201cthe\u201d, \u201ca\u201d, \u201can\u201d) that do not add meaning to a sentence and can be ignored without having a drastic effect on the meaning of the sentence.\n","f50043af":"# Data Loading","39bbeac2":"### Tweets where we make errors\n\nHere are the tweets which our model is not predicting correctly.","6f8f9089":"### Chat Words Conversion","85d8febd":"# Data Exploration","7f881a3d":"Lastly, we will implement lemmatization using Spacy so that we can count the appearance of each word. Lemmatization removes the grammar tense and transforms each word into its original form. Another way of converting words to its original form is called stemming. While stemming takes the linguistic root of a word, lemmatization is taking a word into its original lemma. For example, if we performed stemming on the word \u201capples\u201d, the result would be \u201cappl\u201d, whereas lemmatization would give us \u201capple\u201d. Therefore I prefer lemmatization over stemming, as its much easier to interpret.","0711fdc0":"### Spell Checker","9b876018":"# Evaluation","b1dfe046":"# Training","8cbdc6f5":"### Confidence in each sentiment","10cd47c2":"## Plot results","8e878e55":"### Twitter handlers deletion","6e6db3c3":"# Data Preprocessing\n\n## Pretrained models list : https:\/\/huggingface.co\/transformers\/pretrained_models.html\n\nYou can use a cased and uncased version of BERT and tokenizer. I\u2019ve experimented with both. The cased version works better. Intuitively, that makes sense, since \u201cBAD\u201d might convey more sentiment than \u201cbad\u201d.","b5dd325c":"# Setup","9313e21f":"# Submission","e4fca39e":"### Remove punctuation after having taken care of emoticons","4d59289d":"# Cleaning text","595ecea3":"# Bonus Task\n\n## Sentiment Keyword Extraction\n\nFor the bonus task, we can estimate the overlap between our prediction, and the selected words ground truth using the Jaccard coefficient, which measures the ratio of intersection to union of predicted and label sets.\n\n## Jaccard coefficient","5bd2d9de":"### Converting emojis and emoticons into words","1dce04b8":"### Handling words with multiple repetition letters","fab4b8be":"#### Removing multiple white spaces + white space at the beginning and at the end of the tweets","df1c3d6a":"## Other visualizations","27ecea33":"## Disable Warnings","fc0ea767":"### URLs removal","1244d954":"### Choosing Sequence Length"}}