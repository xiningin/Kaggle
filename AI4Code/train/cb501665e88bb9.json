{"cell_type":{"e303f932":"code","a8d19247":"code","f66a0fe6":"code","762e40b0":"code","6ee8e243":"code","90a57b60":"code","04683409":"code","ed9cea19":"code","3ebd5436":"code","69334443":"code","8306bd53":"code","92fde6fd":"code","ba8ee783":"code","bfa4b5b1":"code","7d893c34":"code","944b0968":"code","616b3577":"code","b1ffd0fd":"code","a7b903e2":"code","bf6e45cd":"code","1cc7e8c1":"code","e2fcc1d6":"code","ce245070":"code","40a96499":"code","8b44fcd4":"code","a3928d83":"code","e4ea0e4f":"code","a5c38c99":"code","8d639ff8":"code","8b8e8e57":"code","fa839cb3":"code","70ae8bf4":"code","d9f93299":"code","32d23bc9":"code","4ea270f4":"code","8089d284":"code","2d9197fe":"code","74cabb89":"code","182affc2":"markdown","9cfbc2e8":"markdown","c6690990":"markdown","85192714":"markdown","fc0b1ee9":"markdown","addb492f":"markdown","06c5a72a":"markdown","565f9c75":"markdown","68e9f0b3":"markdown","c83e2f69":"markdown","b5bfd9a6":"markdown","71fab033":"markdown","0cebc052":"markdown","b3957832":"markdown","b94f7596":"markdown","b0e8e197":"markdown","ea0666bd":"markdown","c6259a84":"markdown","4cc76a2e":"markdown","134918af":"markdown","b9210333":"markdown"},"source":{"e303f932":"%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score, recall_score, precision_score, classification_report\nfrom sklearn.preprocessing import OneHotEncoder, RobustScaler, LabelEncoder\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy.stats import randint, uniform\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, BaggingClassifier","a8d19247":"import os\nprint(os.listdir(\"..\/input\/\"))","f66a0fe6":"# load the dataset into a pandas dataframe\ndataset = pd.read_csv('..\/input\/nasa-asteroids-classification\/nasa.csv')\ndataset.head()","762e40b0":"# Removal of irrelevant features\ndataset = dataset.drop(['Neo Reference ID', 'Name', 'Close Approach Date',\n                        'Epoch Date Close Approach', 'Orbit Determination Date'], axis=1)","6ee8e243":"print(dataset['Orbiting Body'].unique())\nprint(dataset['Equinox'].unique())","90a57b60":"# Removal of features with no discriminative value\ndataset = dataset.drop(['Orbiting Body', 'Equinox'], axis=1)","04683409":"# heatplot\nf, ax = plt.subplots(figsize=(20, 20))\ncorr = dataset.corr(\"pearson\")\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax,annot=True)","ed9cea19":"#calculate the relative difference between the two features\ndiff = (dataset['Est Dia in M(max)'] - dataset['Est Dia in M(min)']) \/ dataset['Est Dia in M(min)']\nprint(\"maximum value is: {0}\".format(diff.max()))\nprint(\"minimum value is: {0}\".format(diff.min()))","3ebd5436":"#removal of correlated (correlation equals 1) columns\ndataset = dataset.drop(['Est Dia in KM(max)', 'Est Dia in M(min)',\n                        'Est Dia in M(max)', 'Est Dia in Miles(min)','Est Dia in Miles(max)','Est Dia in Feet(min)','Est Dia in Feet(max)'], axis=1)\ndataset = dataset.drop(['Relative Velocity km per hr','Miles per hour'], axis=1)\ndataset = dataset.drop(['Miss Dist.(lunar)','Miss Dist.(kilometers)','Miss Dist.(miles)'], axis=1)","69334443":"# heatplot\nf, ax = plt.subplots(figsize=(20, 20))\ncorr = dataset.corr(\"pearson\")\nsns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),\n            square=True, ax=ax,annot=True)","8306bd53":"#check for missing values\nprint(dataset.isnull().sum())","92fde6fd":"#check dataset for duplicate samples\ndataset.duplicated().value_counts()","ba8ee783":"#print the statistical metrics\ndataset.describe()","bfa4b5b1":"#plot the boxplots of all features\nplt.tight_layout(pad=0.9)\nplt.figure(figsize=(35,30)) \nplt.subplots_adjust(wspace = 0.2  )\nnbr_columns = 4 \nnbr_graphs = len(dataset.columns) \nnbr_rows = int(np.ceil(nbr_graphs\/nbr_columns)) \ncolumns = list(dataset.columns.values) \nfor i in range(0,len(columns)-1): \n    plt.subplot(nbr_rows,nbr_columns,i+1) \n    ax1=sns.boxplot(x= columns[i], data= dataset, orient=\"h\") \n\nplt.show() ","7d893c34":"#countplot of labels\nprint(\"label balance:\")\nprint(dataset.Hazardous.value_counts())\n\nax1=sns.countplot(dataset.Hazardous,color=\"navy\")","944b0968":"# one-hot encoding of 'Orbit ID'\ndataset = pd.concat([dataset,pd.get_dummies(dataset['Orbit ID'], prefix='Orbit_ID')],axis=1)\n\ndataset.drop(['Orbit ID'],axis=1, inplace=True)\ndataset.head()","616b3577":"# Make labels in column \"Hazardous\" numerical: False \/ True -> 0 \/ 1\ndataset['Hazardous'] = pd.factorize(dataset['Hazardous'], sort=True)[0]\nprint(dataset.Hazardous[0:5])","b1ffd0fd":"# separating the classlabels from the features  \ny = dataset.Hazardous.values\nX = dataset.drop(['Hazardous'],axis=1)","a7b903e2":"# split the featureset into the numerical and one hot encoded columns\nX_num = X.loc[:,'Absolute Magnitude':'Mean Motion']\nX_One_Hot = X.loc[:,'Orbit_ID_1':].values\n\n# Standardize the numerical features with the Robust Scaler\nfrom sklearn.preprocessing import RobustScaler\nRbS = RobustScaler().fit(X_num)\nX_num = RbS.transform(X_num)\n\n# merge all features back together in one numpy array\nX = np.concatenate((X_num,X_One_Hot),axis=1)","bf6e45cd":"# Split up the dataset into a training and a test set with a 1000 waarden in test set en random_state = 0. Normaliseer de features.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n\n\n#Function to train a model and log the metrics\ndef train_model(model,X_train, y_train, X_test,y_test):\n    \n    start_time = time.time()\n    model.fit(X_train, y_train)\n    \n    delta_time = (time.time() - start_time)\n    y_predict = model.predict(X_test)\n    acc_model = accuracy_score(y_test, y_predict)\n    prec_model = precision_score(y_test, y_predict,average= None)\n    recall_model = recall_score(y_test, y_predict,average= None)\n    log = np.array([[acc_model,prec_model[0],prec_model[1],recall_model[0],recall_model[1],delta_time]])\n    \n    print(\"training time: {0}\".format(delta_time))\n    print(\"accuracy: {0}\".format(acc_model))\n    print(\"\\nconfusion matrix: \")\n    print(\"-----------------------\")\n    print(confusion_matrix(y_test, y_predict))\n    target_names = ['Not hazardous', 'Hazardous']\n    print(\"\\nclassification report:\")\n    print(\"-----------------------\")\n    print(classification_report(y_test, y_predict,target_names=target_names))\n       \n    return model, log","1cc7e8c1":"# Train and test the logistic regression classifier\nLog_reg_model = linear_model.LogisticRegression(C=0.001, solver='lbfgs', multi_class='auto')\nLog_reg_model, model_log = (train_model(Log_reg_model,X_train, y_train, X_test,y_test))","e2fcc1d6":"# Train a logistic regression model through cross-validation en GridSearch \n#--------------------------------------------------------------------------\n\nparameters =  {'C' : [0.1, 1, 10, 100], \n              'class_weight': [None,'balanced'],\n              'solver': ['newton-cg', 'lbfgs', 'liblinear']}\n             \nGrid_Log_model = GridSearchCV(estimator = LogisticRegression(), \n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 6,\n                           n_jobs = -1)\n\nGrid_Log_model,log = train_model(Grid_Log_model,X_train, y_train, X_test,y_test)\nmodel_log= np.append(model_log,log,axis=0)","ce245070":"# Train a Support Vector Machine\nfrom sklearn.svm import SVC\n\nparameters = {'C' : [8,9, 10, 11, 12,13, 14], \n              'kernel' : ['linear', 'rbf'],\n              'degree': [1],\n              'gamma': (0.01, 0.1,1)}\n             \n#SVC_model = GridSearchCV(estimator = SVC(), \n#                           param_grid = parameters,\n#                           scoring = 'accuracy',\n#                           cv = 6,\n#                           n_jobs = -1)\n\nSVC_model= SVC(C=20, degree=3, gamma='auto', kernel='rbf')\n\nSVC_model,log = train_model(SVC_model,X_train, y_train, X_test,y_test)\nmodel_log= np.append(model_log,log,axis=0)","40a96499":"# Train a Random Forest Classifier\nnumber_of_trees = 1000\nmax_number_of_features = 15\n\nRFC_model = RandomForestClassifier(n_estimators=number_of_trees, max_features=max_number_of_features)\n\nRFC_model,log = train_model(RFC_model,X_train, y_train, X_test,y_test)\nmodel_log= np.append(model_log,log,axis=0)","8b44fcd4":"# Boosting\n# Boosting met logistic regression\n\nimport warnings\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.filterwarnings(action='ignore', category=ConvergenceWarning)\n\nnumber_of_estimators = 200\ncomplexity = 10\ncart = cart = LogisticRegression(solver='lbfgs', C=complexity)\nADA_model = BaggingClassifier(base_estimator=cart, n_estimators=number_of_estimators)\n\nADA_model,log = train_model(ADA_model,X_train, y_train, X_test,y_test)\nmodel_log= np.append(model_log,log,axis=0)","a3928d83":"model_names = ('Log reg', 'Log reg gridsearch','SVC','Rand Forest', 'Adaboost')\ncolumn_names = ('accuracy','precision Not hazardous','precision hazardous','recall Not hazardous','recall hazardous','training time')\n\nplt.figure(figsize=(16,16)) \ncolors=[1000,500,1000,900]\ncolors = [x \/ max(colors) for x in colors]\nmy_cmap = plt.cm.get_cmap('GnBu')\ncolors = my_cmap(colors)\nfor i in range(0,6): \n    plt.subplot(3,2,i+1) \n    ax=plt.bar(x=model_names,height=model_log[:,i],color=colors)\n    plt.title(column_names[i])\nplt.show()","e4ea0e4f":"pd.DataFrame(data=model_log,index=model_names,columns=column_names)","a5c38c99":"importances = RFC_model.feature_importances_\nindices = np.argsort(importances)[::-1]\nplt.figure(figsize=(20,7))\nplt.title(\"Feature importances\")\nax = plt.bar(range(X.shape[1]), importances[indices],\n       color=\"r\", align=\"center\")\n\nplt.show()","8d639ff8":"#removal of all but 10 most relevant features\nleast_important_feat_10=importances.argsort()[:-10]\nX_red_10 = np.delete(X,least_important_feat_10,1)\nprint(X_red_10.shape)","8b8e8e57":"# Split up the dataset into a training and a test set\nX_train_red_10, X_test_red_10, y_train_red_10, y_test_red_10 = train_test_split(X_red_10, y, test_size=0.25, random_state=0)","fa839cb3":"# Train a Random Forest Classifier\nnumber_of_trees = 1000\nmax_number_of_features = 9\n\nRFC_model_red = RandomForestClassifier(n_estimators=number_of_trees, max_features=max_number_of_features)\n\nRFC_model_red,log = train_model(RFC_model_red,X_train_red_10, y_train_red_10, X_test_red_10,y_test_red_10)\nmodel_log= np.append(model_log,log,axis=0)","70ae8bf4":"#visualise the pairplot of the ten most significant features\nds = pd.DataFrame(X_test_red_10)\nds['label']= y_test_red_10\nsns.pairplot(ds, hue=\"label\")","d9f93299":"#removal of all but the 2 most relevant features\nleast_important_feat_2=importances.argsort()[:-2]\nX_red_2 = np.delete(X,least_important_feat_2,1)\nprint(X_red_2.shape)","32d23bc9":"# Split up the dataset into a training and a test set\nX_train_red_2, X_test_red_2, y_train_red_2, y_test_red_2 = train_test_split(X_red_2, y, test_size=0.25, random_state=0)","4ea270f4":"# Train a Random Forest Classifier\nnumber_of_trees = 1000\nmax_number_of_features = 2\n\nRFC_model_red_2 = RandomForestClassifier(n_estimators=number_of_trees, max_features=max_number_of_features)\n\nRFC_model_red_2,log = train_model(RFC_model_red_2,X_train_red_2, y_train_red_2, X_test_red_2,y_test_red_2)\nmodel_log= np.append(model_log,log,axis=0)","8089d284":"#plotting the two most signifcant features for the testsamples including the RFC boundaries\n\ndef plot_2D_boundary(model,X_test):\n    h = .01\n    plt.figure(figsize=(18,10))\n\n    x_min, x_max = X_test_red_2[:, 0].min() - 0.5, X_test_red_2[:, 0].max() + 0.5\n    y_min, y_max = X_test_red_2[:, 1].min() - 0.5, X_test_red_2[:, 1].max() + 0.5\n    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),np.arange(y_min, y_max, h))\n\n    # Plot the decision boundary. For that, we will assign a color to each\n    # point in the mesh [x_min, m_max]x[y_min, y_max].\n    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n\n    # Put the result into a color plot\n    Z = Z.reshape(xx.shape)\n    plt.contour(xx, yy, Z, cmap=plt.cm.Paired)\n    plt.scatter(X_test_red_2[:, 0], X_test_red_2[:, 1], c=y_test_red_2, cmap=plt.cm.Paired)\n\nplot_2D_boundary(RFC_model_red_2,X_test_red_2)","2d9197fe":"#Training of a decision tree using the two most significant features\nfrom sklearn import tree\ndetr_model = tree.DecisionTreeClassifier()\ndetr_model,log = train_model(detr_model,X_train_red_2, y_train_red_2, X_test_red_2,y_test_red_2)\nmodel_log= np.append(model_log,log,axis=0)","74cabb89":"plot_2D_boundary(detr_model,X_test_red_2)","182affc2":"By reducing the number of features to only the ten most significant ones, we are able to match the model metrics of our previous RFC. \nBecause we now only have 10 features left, we can use a pairplot to visualise the relationships between the various features.   ","9cfbc2e8":"# Concluding\n\nThe above test clearly shows that a simple decision tree produces exactly the same result as the previously trained RFC. It is therefore clear that in this case it is not because of the use of the ensemble techniques that we achieve such a high result, but that by reducing the number of features to these 2, we force the model to focus on these 2 features. The question that naturally arises is why the first RFC model itself failed to focus on these 2 features.\n \nAs already mentioned, with an RFC each tree will only have a subset of the features available to determine the optimum split in each node (in Scikit Learn this is the square root of the total number of features). When the number of features becomes bigger (as in our case 145 features) a large amount of trees in the RFC have no access to the most significant features and therefore cannot make optimum splits. So when we only offer the best features, they will be used more often and the model will score better.\n","c6690990":"The boxplots confirm our previous assumption. Features like \"Orbital Period\" and \"Perihelion Time\" show a significant amount of outliers. Because it is not clear whether these outliers are meaningful or not we will not remove them from the dataset just yet.\n\n(Note: The feature orbit id contains categorical or label values and should therefore not be taken into account when evaluating the above boxplots.)\n\nFinally we will take a look at the distribution of de sample labels. The countplot below shows that the distribution of dataset labels is unbalanced with nearly 4000 non hazardous and only 755 hazardous asteroids. We will need to take this into account when we evaluate our models because a model that, for example, labels every asteroid as \"non hazardous\" will already achieve an accuracy score of 3932\/4687 = 84%. It is therefore necessary to pay extra attention to other model metrics like the models Precision, Recall and Confusion Matrix.","85192714":"Using only 2 features results in an even better performance of our classifying model. In this case we only get 5 false negatives and no false positives. The scatterplot clearly shows how the RFC boundary divides the samples almost perfectly. \nHowever, using a Random Forest with 1000 decision trees where all the decision trees can only use the two given features, is not very efficient. After all, all decision trees in the RFC will use the same boundaries to divide the samples so using a single decision tree should lead to the same result at a much lower computational cost. To prove this we will train a simple decision tree with only the same 2 most significant features.","fc0b1ee9":"As suspected, the columns regarding the \"Miss Dist\", the velocity and the estimated diameter of the meteorite all have a correlation of 1. We can therefore conclude that these features merely contain the same values in a different unit of measurement and that we can discard all columns but one for every physical quantity mentioned. \nIt is however still unclear why columns like \"Est Dia in M(max)\" and \"Est Dia in M(min)\" have a correlation of 1. \n        A closer inspection of the data as shown below, shows that there is indeed a linear relation between the two features and that we only need to maintain one of the features to conserve all the information in the dataset.","addb492f":"Looking at the statistical overview, we notice that for some features the difference between the 75th percentile and the maximum value is remarkable. The \"Orbital Period\", for instance, has a mean of 635 and a 75th percentile of 794 while the maximumvalue has a value of 4172. This leads to the assumption that some of the features contain one or more outliers. ","06c5a72a":"As expected, our simple logistic regression model is not capable of distuingishing the hazardous from the non hazardous asteroids. It simply classifies every asteroid as being non hazardous. The dataset is, as mentioned before, too unbalanced for this model to classify the samples correctly.\n      \nIn order to try and find a better model, we will try out the following kinds of machine learning algorithms:  \n\n* Logistic regression with gridsearch\n* SVM with gridsearch\n* Random Forest Classifier\n* Logistic Regression with AdaBoost","565f9c75":"The above shows clearly that both columns 'Orbiting Body' and 'Equinox' only contain one value and do not have any discriminative value. We will therefore discard both columns.","68e9f0b3":"Looking at the results of the different models, we notice that by using gridsearch, we were able to drastically inmprove the performance of the logistic regression model. \nWhere the original model was not capable of detecting the hazardous asteroids, our new Logistic Regression model is able to detect aprox 167 out of 177 hazardous asteroids.\nThe gridsearch however requires a lot of training time. This is due to the fact that for each of the possible combinations of our hyperparameters, the model gets retrained to find the specific combination of hyperparameters that gives us the best performing model.\n\nOverall we can conclude that our Random Forest classifier is able to achieve the best results compared to the other models. Let's see if we can find a way to tune the RFC so we get an even better performing model.  \n","c83e2f69":"Inspection of the above printout shows that some columns contain dates, names and\/or ID's. Because these features do not give us any relevant quantative information for identifying hazardous meteorites, we will remove the following columns from the dataset: \n* 'Neo Reference ID'\n* 'Name'\n* 'Close Approach Date'\n* 'Epoch Date Close Approach'\n* 'Orbit Determination Date' ","b5bfd9a6":"The new heatmap reveals  additional columns which have a very high correlation. Because the titles reveal no real justification to believe that there is a direct relationship between the features, we will not remove the features from the dataset.\n\nNow we have a dataset with all the relevant features, we can:\n* check the dataset for missing values and remove the corresponding samples when found\n* check the dataset for duplicate values and remove the corresponding samples when found","71fab033":"## Summary\nThe Nasa dataset contains data and measurements from observed meteorites. Furthermore every meteorite is classified\/labelled as being hazardous or not. The goal of the notebook will be to perform some data preparation and  correctly predict the classification of a meteorite as hazardous or not hazardous given the set of features in the dataset.\n","0cebc052":"A quick look at the pairplot shows that for some of the feature pairs, there is no clear boundary between the two categories. So therefore we will just try to reduce the amount of features even further and just keep the two most significant ones. Because we will only use 2 features we will also be able to visualise the RFC boundary between the two classes.","b3957832":"The chart clearly shows that out of our 207 features, most of them contain little or no relevant information for classifying the testsamples. The fact that we include all these irrelevant features in our data will have a negative impact on the training and performance of our model. After all, a Random Forest Classifier is a model containing multiple decision trees which are trained over a random extraction of the samples and a random extraction of the features. This means that not every tree sees all the features or all the observations. If one or more of these decision trees is only exposed to irrelevant features, it will not be able to make a meaningful contribution to the classification of the samples.\n\nSo lets see if we can improve the models performance by only using the 10 most relevant features to retrain our model.","b94f7596":"# Data Preparation\n### One hot encoding of categorical features\n\nAs mentioned before, the feature \"Orbit ID\" is a categorical variable. Because our models cannot operate on categorical data directly, we will have to convert them into a numerical equivalent through the use of one hot encoding. This will create an extra column for every category and fill all columns with zero values except for the index of the category, which is marked with a 1.\n\n### Make labels numerical\n\nBesides the one hot encoding of the categorical features we still have a label column which contains the labels \"true\" and \"false\". These labels will have to be converted into a binary numerical value for the models to interpret them correctly. ","b0e8e197":"### Standardising the dataset and splitting up labels and features\n\nBefore we can begin with training the model, we still need to separate the labels from the features.  \n\nFurthermore, we know that many machine learning algorithms perform better when our features are on a relatively similar scale and\/or close to normally distributed, so it is a good idea to standardize our feature data. Because of the fact that we previously discovered the presence of outliers we will use the scikit learn robust scaler to standardize our data. However, we want to preserve the one hot encoded categories of the \"Orbit ID\", so we will have to temporarily remove these columns from the featureset. Finally, we will merge all the features back together into one numpy array. ","ea0666bd":"# Tuning the Random Forest Classifier\nWe will start by determining which features are being perceived as the most significant by our RFC-model. We can simply do this by extracting the feature importances from our model and plot their importance in sorted order. ","c6259a84":"As shown above, the dataset contains no missing values or duplicate samples so in this case no extra action is required.\n\nFor the second stage of our data exploration we will try to get a better idea of how our data is distributed.","4cc76a2e":"Finally the overview shows multiple columns which contain the \"Miss Dist.\", \"Relative Velocity\", etc. These columns seem to contain identical values only expressed in a different unit. If this is the case, these columns are interdependant and should have an correlation of 1. We will plot a heatmap of the correlation between the features to detect any interdependant columns.\n","134918af":"The columns 'Orbiting Body' and 'Equinox' seem to contain only one sample value. If this is the case, the features will not add any discriminative value to the dataset and the features can be discarded. To be certain we will extract the unique values out of both feature columns. ","b9210333":"# Training the classifiers\n\n### Logistic regression\nWe start by splitting up the dataset in a 3 to 1 ratio, training and test set and train a simple logistic regression model that we can use as a starting point for finding a good functioning model. We can then try other machine learning algorithms to find the model which achieves the best results.    \n\nFor all models we will log the training time needed, the accuracy, precision and recall of the model so we can get an overview of which model performs best. \n"}}