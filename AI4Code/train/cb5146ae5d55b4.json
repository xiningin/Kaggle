{"cell_type":{"91a317f9":"code","5773bdc0":"code","cf545dfc":"code","b42b9077":"code","7a00b864":"code","c9afe7d7":"code","02f11cb5":"code","76b9fb19":"code","581cbaf8":"code","53dd7d1d":"code","702c6e72":"code","761bc3b5":"code","8e25f586":"code","19ea2aad":"code","f42d0f53":"code","95deb40e":"code","cbc1c0bb":"code","2d440812":"code","2516b9ce":"code","fe9db70b":"code","88e49abe":"code","face0568":"code","cc831531":"code","603adbd7":"code","35318e9f":"code","f6857eb1":"code","6f418601":"code","b91a0afd":"code","ae5574e2":"code","2dc204fe":"code","80a8d64a":"code","a8d06708":"code","e95c37c3":"code","81be9b18":"code","6e88f8b8":"code","2f079647":"code","07d2c3b2":"code","edc44cd3":"code","def61c1a":"code","40c3cb6d":"code","f077cff8":"code","7fc164d0":"code","5d537705":"code","6857d810":"code","96e79172":"code","12083243":"code","e90d5aee":"code","57468740":"code","d221132d":"code","420b33d8":"code","c4c9c99d":"code","f62b032e":"code","b79b181b":"code","91f06a2c":"code","203907c7":"code","d4a8d612":"code","a74e543f":"code","d2566ea3":"code","3041f2ca":"code","038b1856":"code","4b3ed85c":"code","d24e1bd1":"code","a570c17c":"code","52ba47fd":"code","3c29821f":"code","6d49ceb9":"code","67b1f2d6":"code","2054f702":"code","8d700064":"code","4c57f6a2":"code","1d7a95d4":"code","818be7f9":"code","a4d9cb8d":"code","709b816f":"code","5fa0d8ea":"code","09e3bb27":"code","94005f8c":"code","b2678a83":"code","6592c4d3":"code","b5cae971":"code","a10bd38d":"code","9b238cb8":"code","5aa804e8":"code","b9df92d2":"code","96a98d57":"code","bb7130fb":"code","88103b11":"code","9c1a6ed3":"code","8588d2a3":"code","b42a044b":"code","be8e7224":"code","13fbab9e":"code","db22858d":"code","7e5c8aff":"code","ae442d5a":"code","4bb82fef":"code","1e93a0c8":"code","0dbfeb8d":"code","c3f1d79b":"code","eb978658":"code","3b088c86":"code","4d7d3302":"code","887ae88b":"code","47f5218d":"code","10ff959b":"code","624d0dd1":"code","dcc581ac":"code","9dab2ac4":"code","03a66ad2":"code","a75af3c8":"code","e82cfed4":"code","a95c914f":"code","2665b01c":"code","98637373":"code","52084ab3":"code","9c7380c0":"code","7a7baa9a":"code","7c204146":"markdown","86a6d109":"markdown","2870e369":"markdown","b12327f8":"markdown","6fb2d09a":"markdown","a07c49b9":"markdown","0f6a72e3":"markdown","30d499aa":"markdown","3df18a58":"markdown","5856e070":"markdown","75511054":"markdown","e2b7d1eb":"markdown","4db296c5":"markdown","29229906":"markdown","039038eb":"markdown","3dcede96":"markdown","94083764":"markdown","8ef3de1f":"markdown","0645f451":"markdown","d82706ba":"markdown","b73076d0":"markdown","b3eb1512":"markdown","c4879a71":"markdown","313d5e66":"markdown","de520df9":"markdown","66c46c32":"markdown","771ae705":"markdown","22688ff0":"markdown","df615dc3":"markdown","87bfd0d1":"markdown","4330ecf7":"markdown","3cd1a31a":"markdown","a55013b5":"markdown","53e1d2a3":"markdown","cc477d44":"markdown","cc7d891f":"markdown","a1e5daf6":"markdown","d87503d5":"markdown","f6d2be3a":"markdown","e4bc6567":"markdown","390b991c":"markdown","bd3ffe84":"markdown","527ef894":"markdown","b60189c8":"markdown","326b7602":"markdown","db1340d3":"markdown","1c956abc":"markdown","8c4ad386":"markdown","6e169a8c":"markdown","1b01bff6":"markdown","71a29694":"markdown","3cfc8606":"markdown","837b55b6":"markdown","3964e115":"markdown","a1c7eeb5":"markdown","159c6ea1":"markdown","71dd97a0":"markdown","8ecb014a":"markdown","7abbba67":"markdown","4fb1161b":"markdown","a07dac39":"markdown","4cdd2b0f":"markdown","3dd4840f":"markdown","3c77caad":"markdown","b88ab4d1":"markdown","350748b2":"markdown","f63f86f0":"markdown","9f0e52f4":"markdown","8644256f":"markdown","3ebf4950":"markdown","0a4e0362":"markdown"},"source":{"91a317f9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5773bdc0":"df_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\nprint(f\"Train Data Shape: {df_train.shape}\")\nprint(f\"Train Data Shape: {df_test.shape}\")","cf545dfc":"df_train.info()","b42b9077":"df_train.describe()","7a00b864":"plt.figure(figsize=(40,40))","c9afe7d7":"sns.histplot(df_train[\"Survived\"])","02f11cb5":"sns.histplot(df_train[\"Pclass\"])","76b9fb19":"sns.histplot(data=df_train, x=\"Pclass\", hue=\"Survived\", multiple=\"dodge\")","581cbaf8":"sns.histplot(data=df_train, x=\"Sex\")","53dd7d1d":"sns.histplot(data=df_train, x=\"Sex\", hue=\"Survived\", multiple=\"dodge\")","702c6e72":"sns.catplot(data=df_train, x=\"Sex\", col=\"Pclass\", col_wrap=3, kind=\"count\", hue=\"Survived\")","761bc3b5":"grp_by_sex_pclass = df_train.groupby([\"Sex\",\"Pclass\",\"Survived\"]).count()\ngrp_by_sex_pclass","8e25f586":"grpby_pclass_fare = df_train.groupby([\"Pclass\"]).describe().loc[:,\"Fare\"]\ngrpby_pclass_fare","19ea2aad":"g = sns.displot(data=df_train, x=\"Fare\", col=\"Survived\", col_wrap=2)\ng.set_xlabels(\"Fare Values\")\ng.set_ylabels(\"Count\")","f42d0f53":"sns.kdeplot(df_train[\"Fare\"])","95deb40e":"df_train.groupby([\"Cabin\",\"Pclass\"]).count().head(n=10)","cbc1c0bb":"df_train.groupby([\"Cabin\",\"Pclass\"]).count().tail(n=10)","2d440812":"passengers_without_cabins = df_train[(df_train.Cabin.isna())]\nsns.displot(data=passengers_without_cabins, x=\"Survived\")","2516b9ce":"sns.countplot(data=df_train, x=\"Embarked\", hue=\"Survived\")","fe9db70b":"df_train.loc[:,\"Embarked\"].value_counts()","88e49abe":"sns.countplot(data=df_train, x=\"Parch\", hue=\"Survived\")","face0568":"# plot the probability\nsns.factorplot(data=df_train, x=\"Parch\", y=\"Survived\",kind=\"bar\")","cc831531":"sns.countplot(data=df_train, x=\"SibSp\", hue=\"Survived\")","603adbd7":"# plot the probability\nsns.factorplot(data=df_train, x=\"SibSp\", y=\"Survived\", kind=\"bar\")","35318e9f":"df_train.Age.describe()","f6857eb1":"sns.kdeplot(data=df_train,x=\"Age\")","6f418601":"df_train.info()","b91a0afd":"df = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\ndf","ae5574e2":"from sklearn.impute import KNNImputer\n\nknn_imputer = KNNImputer(n_neighbors=5)\n\nage_imputed_train = knn_imputer.fit_transform(df.loc[:,\"Age\"].values.reshape(-1,1))\ndf.loc[:,\"Age\"] = age_imputed_train.ravel()","2dc204fe":"df_test.info()","80a8d64a":"df_train.info()","a8d06708":"def change_cabin(cabin):\n    return cabin[0]\n\ndf.loc[:,\"Cabin\"] = df.loc[:,\"Cabin\"].fillna(\"NONE\")\ndf.loc[:,\"Cabin\"] = df.Cabin.apply(change_cabin)","e95c37c3":"df_train.Cabin.value_counts()","81be9b18":"df_test.Cabin.value_counts()","6e88f8b8":"df_train.Embarked.value_counts()","2f079647":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"most_frequent\")\ndf.loc[:,\"Embarked\"] = imputer.fit_transform(df.loc[:,\"Embarked\"].values.reshape(-1,1)).ravel()","07d2c3b2":"df_train.Embarked.value_counts()","edc44cd3":"df_test.Embarked.value_counts()","def61c1a":"df_train.info()","40c3cb6d":"df_test.info()","f077cff8":"from sklearn.impute import SimpleImputer\nfare_imputer = SimpleImputer(strategy=\"mean\")\n\ndf.loc[:,\"Fare\"] = fare_imputer.fit_transform(df.loc[:,\"Fare\"].values.reshape(-1,1)).ravel()\ndf.info()","7fc164d0":"df.loc[:,\"Sex_Pclass\"] = df.loc[:,\"Sex\"] + \"_\" + df.loc[:,\"Pclass\"].astype(str)\n","5d537705":"plt.figure(figsize=(20,10))\nsns.histplot(multiple=\"dodge\",data=df,  x=\"Sex_Pclass\", hue=\"Survived\", )","6857d810":"df.Name.head()","96e79172":"import re\nresult_train = []\nfor name in df.loc[:,\"Name\"]:\n    a = re.split(\" \", name)\n    result_train.append(a[1][:-1])\n\ndf.loc[:,\"Name\"] = result_train\n\n","12083243":"df.Name.value_counts()","e90d5aee":"non_rare_names = [\n    \"Mr\",\"Mrs\",\"Miss\",\"Master\"\n]\ndef change_name(name):\n    if name in non_rare_names:\n        return name\n    return \"Rare\"\ndf.loc[:,\"Name\"] = df.Name.apply(change_name)\ndf_train.Name.value_counts()","57468740":"df.Name.value_counts()","d221132d":"plt.figure(figsize=(20,10))\nsns.histplot(multiple=\"dodge\",data=df,  x=\"Name\", hue=\"Survived\")","420b33d8":"df.loc[:,\"Sex_Name\"] = df.loc[:,\"Sex\"] + \"_\" + df.loc[:,\"Name\"].astype(str)","c4c9c99d":"plt.figure(figsize=(50,10))\nsns.histplot(multiple=\"dodge\",data=df,  x=\"Sex_Name\", hue=\"Survived\", )\n","f62b032e":"def change_age(age):\n    if age > 65.0:\n        return \"OLD\"\n    elif 40.0 <= age <= 65.0:\n        return \"ADULT\"\n    elif 39.0 < age <= 20.0:\n        return \"YOUTH\"\n    elif 15.0 <= age <= 19.0:\n        return \"TEENAGERS\"\n    else:\n        return \"CHILD\"\ndf.loc[:,\"Age_Bin\"] = df.Age.apply(change_age)","b79b181b":"df.Age.value_counts()","91f06a2c":"df.Age.value_counts()","203907c7":"plt.figure(figsize=(20,10))\nsns.histplot(multiple=\"dodge\",data=df,  x=\"Age_Bin\", hue=\"Survived\" )","d4a8d612":"df.loc[:,\"Age_Sex\"] = df.loc[:,\"Age_Bin\"]+\"_\"+df.loc[:,\"Sex\"]","a74e543f":"df.Age_Sex.value_counts()","d2566ea3":"plt.figure(figsize=(20,10))\nsns.histplot(multiple=\"dodge\",data=df,  x=\"Age_Sex\", hue=\"Survived\" )","3041f2ca":"def change_fare(fare):\n    if 0<= fare <= 7:\n        return \"LOW\"\n    elif 7 < fare <= 30:\n        return \"MEDIUM\"\n    else:\n        return \"HIGH\"\ndf.loc[:,\"Fare\"] = df.Fare.apply(change_fare)","038b1856":"df.Fare.value_counts()","4b3ed85c":"plt.figure(figsize=(20,10))\nsns.histplot(multiple=\"dodge\",data=df,  x=\"Fare\", hue=\"Survived\" )","d24e1bd1":"df.loc[:,\"Fare_Pclass\"] = df.loc[:,\"Fare\"]+\"_\"+df.loc[:,\"Pclass\"].astype(str)","a570c17c":"plt.figure(figsize=(20,10))\nsns.histplot(multiple=\"dodge\",data=df,  x=\"Fare_Pclass\", hue=\"Survived\" )","52ba47fd":"df.loc[:,\"Age_Pclass_Sex\"] = df.Age_Bin +\"_\" + df.Pclass.astype(str)+\"_\"+df.Sex","3c29821f":"plt.figure(figsize=(40,10))\nsns.histplot(multiple=\"dodge\",data=df,  x=\"Age_Pclass_Sex\", hue=\"Survived\" )","6d49ceb9":"df.loc[:,\"Pclass_Cabin\"]=df.Pclass.astype(str)+\"_\"+df.Cabin","67b1f2d6":"plt.figure(figsize=(20,10))\nsns.histplot(multiple=\"dodge\",data=df,  x=\"Pclass_Cabin\", hue=\"Survived\" )","2054f702":"df['Family_Size'] = df['Parch'] + df['SibSp'] + 1\n\ndf['Fsize_Cat'] = df['Family_Size'].map(lambda val: 'Alone' if val <= 1 else ('Small' if val < 4 else 'Big'))\n","8d700064":"plt.figure(figsize=(20,10))\nsns.histplot(multiple=\"dodge\",data=df,  x=\"Family_Size\", hue=\"Survived\" )","4c57f6a2":"plt.figure(figsize=(20,10))\nsns.histplot(multiple=\"dodge\",data=df,  x=\"Fsize_Cat\", hue=\"Survived\" )","1d7a95d4":"def change_fsize_cat(cat):\n    if cat == \"Small\":\n        return 1\n    elif cat == \"Big\":\n        return 2\n    else:\n        return 0\n\ndf.loc[:,\"Fsize_Cat\"] = df.Fsize_Cat.apply(change_fsize_cat)","818be7f9":"df.head()","a4d9cb8d":"import re\ndef clean_ticket(ticket):\n    prefix = re.sub(r'[^a-zA-Z]', '', ticket)\n    if(prefix):\n        return prefix\n    else:\n        return \"NUM\"\n\ndf[\"Ticket_Clean\"] = df.Ticket.apply(clean_ticket)","709b816f":"df.head()","5fa0d8ea":"df[\"Ticket_Cabin\"] = df[\"Ticket_Clean\"]+\"_\"+df[\"Cabin\"]","09e3bb27":"plt.figure(figsize=(50,10))\nsns.histplot(multiple=\"dodge\",data=df,  x=\"Ticket_Cabin\", hue=\"Survived\" )","94005f8c":"from sklearn.preprocessing import LabelEncoder , OneHotEncoder\n# columns for label encoding\ncol_lbl = [\n    \"Age_Bin\",\n    \"Sex\",\n    \"Fare\"\n]\n\nfor col in col_lbl:\n    lbl = LabelEncoder()\n    df.loc[:,col+\"_enc\"] = lbl.fit_transform(df.loc[:,col].values)","b2678a83":"df.head()","6592c4d3":"col_ohe = [\n    \"Name\",\n    \"Cabin\",\n    \"Embarked\",\n    \"Sex_Pclass\",\n    \"Sex_Name\",\n    \"Age_Sex\",\n    \"Fare_Pclass\",\n    \"Age_Pclass_Sex\",\n    \"Pclass_Cabin\"\n]\nfor col in col_ohe:\n    ohe = OneHotEncoder(sparse=True, handle_unknown = 'ignore')\n    column_names = []\n    ohe.fit(df.loc[:,col].values.reshape(-1,1))\n\n    data = ohe.transform(df.loc[:,col].values.reshape(-1,1)).toarray()\n    for i in range(len(data[0])):\n        column_names.append(col+\"_\"+str(i))\n\n    enc_df = pd.DataFrame(ohe.transform(df.loc[:,col].values.reshape(-1,1)).toarray(), columns=column_names)\n\n    df = df.join(enc_df)\n","b5cae971":"df.head()","a10bd38d":"num_col = [\n    \"SibSp\",\n    \"Parch\",\n    \"Age\"\n]\nfrom sklearn.preprocessing import PolynomialFeatures\nfor col in num_col:\n    poly = PolynomialFeatures(3)\n    poly.fit(df.loc[:,col].values.reshape(-1,1))\n\n    poly_df= pd.DataFrame(poly.transform(df.loc[:,col].values.reshape(-1,1)), columns=[col+\"_poly0\", col+\"_poly1\", col+\"_poly2\",col+\"_poly3\"])\n\n    df = df.join(poly_df)","9b238cb8":"df_train.head()","5aa804e8":"from sklearn.preprocessing import StandardScaler\ncols_to_scale = [\n    \"SibSp_poly0\",\"SibSp_poly1\",\"SibSp_poly2\",\"SibSp_poly3\",\n    \"Parch_poly0\",\"Parch_poly1\",\"Parch_poly2\",\"Parch_poly3\",\n    \"Age_poly0\",\"Age_poly1\",\"Age_poly2\",\"Age_poly3\"\n]\nfor col in cols_to_scale:\n    sc = StandardScaler()\n    df.loc[:,col] = sc.fit_transform(df.loc[:,col].values.reshape(-1,1))","b9df92d2":"df_train.head()","96a98d57":"col_to_drop = [\n    \"Name\",\n    \"Cabin\",\n    \"Embarked\",\n    \"Sex_Pclass\",\n    \"Sex_Name\",\n    \"Age_Sex\",\n    \"Fare_Pclass\",\n    \"Age_Pclass_Sex\",\n    \"Pclass_Cabin\",\n    \"SibSp\",\n    \"Parch\",\n    \"PassengerId\",\n    \"Age\",\n    \"Ticket\",\n    \"Sex\",\n    \"Fare\",\n    \"Age_Bin\",\"Ticket_Clean\",\"Ticket_Cabin\"\n]\ndf = df.drop(col_to_drop, axis=1)","bb7130fb":"df.columns","88103b11":"df.shape","9c1a6ed3":"df_train = df[:len(df_train)]\ndf_test = df[len(df_train):]","8588d2a3":"df_train.shape","b42a044b":"df_test.shape","be8e7224":"from sklearn.model_selection import StratifiedKFold\n\ndf_train[\"kfold\"] = -1\ndf_train = df_train.sample(frac=1).reset_index(drop=True)\ny = df_train.Survived.values\n\nskf = StratifiedKFold(n_splits=3)\nfor f , (t_,v_) in enumerate(skf.split(X=df_train, y=y)):\n    df_train.loc[v_,\"kfold\"] = f","13fbab9e":"df_train.kfold.value_counts()","db22858d":"from sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB, MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, make_scorer\nfrom sklearn import metrics\ndef run(fold, model):\n\n    features = list(df_train.columns[1:])\n\n    df_t = df_train[df_train.kfold != fold].reset_index(drop=True)\n    df_v = df_train[df_train.kfold == fold].reset_index(drop=True)\n\n    x_train = df_t[features[:-1]].values\n    x_valid = df_v[features[:-1]].values\n\n    model.fit(x_train, df_t.Survived.values)\n    valid_preds = model.predict(x_valid)\n\n    print(f\"FOLD:{fold}\")\n    print(roc_auc_score(df_v.Survived.values, valid_preds))\n    print()\n    print()\n    return roc_auc_score(df_v.Survived.values, valid_preds)\n","7e5c8aff":"model = RandomForestClassifier(n_jobs=-1)\nfor fold_ in range(3):\n    run(fold_, model)","ae442d5a":"model = SVC()\nfor fold_ in range(3):\n    run(fold_, model)","4bb82fef":"model = KNeighborsClassifier()\nfor fold_ in range(3):\n    run(fold_, model)","1e93a0c8":"model_logreg = LogisticRegression()\nfor fold_ in range(3):\n    run(fold_, model)","0dbfeb8d":"model = GaussianNB()\nfor fold_ in range(3):\n    run(fold_, model)","c3f1d79b":"model = XGBClassifier()\nfor fold_ in range(3):\n    run(fold_, model)","eb978658":"from sklearn.model_selection import GridSearchCV\nfrom sklearn import decomposition","3b088c86":"roc_auc_scorer = make_scorer(roc_auc_score, greater_is_better=True,\n                             needs_threshold=True)","4d7d3302":"param_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}\n# Create a based model\nrf_grid = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf_grid, param_grid = param_grid,\n                          n_jobs = -1, verbose = 2, scoring=roc_auc_scorer)\n\n# grid_search.fit(x, df_train.Survived.values)\n#\n#\n# print('Raw AUC score:', grid_search.best_score_)\n# print('params', grid_search.best_params_)","887ae88b":"\nresults = []\nfor fold_ in range(3):\n    results.append(run(fold_, grid_search))\n\nprint(f\"MEAN: {np.mean(results)}\")","47f5218d":"\nclf = LogisticRegression()\ngrid_values = {'penalty': ['l1', 'l2'],'C':[0.001,.009,0.01,.09,1,5,10,25]}\ngrid_clf_acc = GridSearchCV(clf, param_grid = grid_values, scoring=roc_auc_scorer)\n\n# grid_clf_acc.fit(x, df_train.Survived.values)\n#\n#\n# print('Raw AUC score:', grid_clf_acc.best_score_)\n# print('params', grid_clf_acc.best_params_)","10ff959b":"\nresults = []\nfor fold_ in range(3):\n    results.append(run(fold_, grid_clf_acc))\n\nprint(f\"MEAN: {np.mean(results)}\")","624d0dd1":"#List Hyperparameters that we want to tune.\nleaf_size = list(range(1,50))\nn_neighbors = list(range(1,30))\np=[1,2]\n#Convert to dictionary\nhyperparameters = dict(leaf_size=leaf_size, n_neighbors=n_neighbors, p=p)\n#Create new KNN object\nknn_2 = KNeighborsClassifier()\n#Use GridSearch\nclf = GridSearchCV(knn_2, hyperparameters, scoring=roc_auc_scorer)\n\n# clf.fit(x, df_train.Survived.values)\n#\n#\n# print('Raw AUC score:', clf.best_score_)\n# print('params', clf.best_params_)","dcc581ac":"results = []\nfor fold_ in range(3):\n    results.append(run(fold_, clf))\n\nprint(f\"MEAN: {np.mean(results)}\")\n","9dab2ac4":"param_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\ngrid_svc = GridSearchCV(SVC(probability=True),param_grid,refit=True,verbose=2,scoring=roc_auc_scorer)\n# grid_svc.fit(x, df_train.Survived.values)\n#\n# print('Raw AUC score:', grid_svc.best_score_)\n# print('params', grid_svc.best_params_)","03a66ad2":"results = []\nfor fold_ in range(3):\n    results.append(run(fold_, grid_svc))\n\nprint(f\"MEAN: {np.mean(results)}\")\n","a75af3c8":"xgb_model = XGBClassifier()\n\n#brute force scan for all parameters, here are the tricks\n#usually max_depth is 6,7,8\n#learning rate is around 0.05, but small changes may make big diff\n#tuning min_child_weight subsample colsample_bytree can have\n#much fun of fighting against overfit\n#n_estimators is how many round of boosting\n#finally, ensemble xgboost with multiple seeds may reduce variance\nparameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['binary:logistic'],\n              'learning_rate': [0.05], #so called `eta` value\n              'max_depth': [6],\n              'min_child_weight': [11],\n              'silent': [1],\n              'subsample': [0.8],\n              'colsample_bytree': [0.7],\n              'n_estimators': [5], #number of trees, change it to 1000 for better results\n              'missing':[-999],\n              'seed': [1337]}\n\n\nclf_xgb = GridSearchCV(xgb_model, parameters, n_jobs=5,\n                   scoring=roc_auc_scorer,\n                   verbose=2, refit=True)\n\n# clf_xgb.fit(x, df_train.Survived.values)\n#\n#\n# print('Raw AUC score:', clf_xgb.best_score_)\n# print('params', clf_xgb.best_params_)","e82cfed4":"results = []\nfor fold_ in range(3):\n    results.append(run(fold_, clf_xgb))\n\nprint(f\"MEAN: {np.mean(results)}\")\n","a95c914f":"# clf_cat = CatBoostClassifier(task_type=\"GPU\")\n# params = {'iterations': [500],\n#           'depth': [4, 5, 6],\n#           'loss_function': ['Logloss', 'CrossEntropy'],\n#           'l2_leaf_reg': np.logspace(-20, -19, 3),\n#           'leaf_estimation_iterations': [10],\n# #           'eval_metric': ['Accuracy'],\n# #            'use_best_model': ['True'],\n#           'logging_level':['Silent'],\n#           'random_seed': [42]\n#          }\n# clf_grid_cat = GridSearchCV(estimator=clf_cat, param_grid=params, scoring=roc_auc_scorer, cv=5)\n# clf_grid_cat.fit(x, df_train.Survived.values)\n#\n#\n# print('Raw AUC score:', clf_grid_cat.best_score_)\n# print('params', clf_grid_cat.best_params_)\n","2665b01c":"results = []\nfor fold_ in range(3):\n    results.append(run(fold_, clf_xgb))\n\nprint(f\"MEAN: {np.mean(results)}\")","98637373":"def run_ann(fold, model):\n    features = list(df_train.columns[1:-1])\n\n    df_t = df_train[df_train.kfold != fold].reset_index(drop=True)\n    df_v = df_train[df_train.kfold == fold].reset_index(drop=True)\n\n    x_train = df_t[features].values\n    x_valid = df_v[features].values\n\n\n    model.fit(x_train, df_t.Survived.values, batch_size=10, epochs=15)\n    valid_preds = model.predict(x_valid)\n\n    print(f\"FOLD:{fold}\")\n    print(roc_auc_score(df_v.Survived.values, valid_preds))\n    print()\n    print()\n    return roc_auc_score(df_v.Survived.values, valid_preds)","52084ab3":"import keras\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras import metrics\nfrom sklearn import decomposition\n\nclassifier = Sequential()\n\n# add the first hidden layer and the input layer\nclassifier.add(Dense(30, activation=\"relu\", kernel_initializer='uniform'))\n# add the second hidden layer\nclassifier.add(Dense(30, activation=\"relu\", kernel_initializer='uniform'))\n\nclassifier.add(Dense(30, activation=\"relu\", kernel_initializer='uniform'))\n\n\n# add the output layer\nclassifier.add(Dense(1, activation=\"sigmoid\", kernel_initializer='uniform'))\n\nclassifier.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[metrics.AUC(), metrics.BinaryCrossentropy(), metrics.Recall()])\n\n\nresults = []\nfor fold_ in range(3):\n    results.append(run_ann(fold_, classifier))\n\nprint(f\"MEAN: {np.mean(results)}\")","9c7380c0":"test_features = [f for f in df_test.columns if f not in (\"Survived\")]\nx_test = df_test[test_features].values\n\n# x_test = svd.transform(x_test)\npreds = classifier.predict_classes(x_test).ravel()\npreds","7a7baa9a":"\npassenger_id_submit = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\").PassengerId\noutput = pd.DataFrame({'PassengerId': passenger_id_submit, 'Survived': preds})\noutput.to_csv('.\/submission.csv', index=False)","7c204146":"**XGBoost Classifier**","86a6d109":"**Categorical Data Analysis**","2870e369":"PS: I found this on a discussion on kaggle, sorry i forgot the link, i'll update the notebook when i'll find the discussion","b12327f8":"**For the cabin the missed values will be treated as new categories and for every cabin we will return the first letter as it indicates where the cabin is placed which is beneficial to track dead passengers**","6fb2d09a":"**We could see that we have some missing data for features: Age, Cabin, Embarked. We should take care of them in the part of \ndata cleaning**","a07c49b9":"**Combine Pclass and Fare**","0f6a72e3":"**From these charts, we could say that having a Parent\/Children make the passenger a bit safer than being alone**","30d499aa":"So we have age, Cabin, and Embarked features with missing values.\nSo we have many strategies to fill missing values for both\ncategorical and numerical features.<br><br>\n***.Age: We will use KNN Imputer<br><br>\n.Cabin: We will treat NaN values as new category.<br><br>\n.Embarked: We have only 2 missing values so we will fill with mode values (most frequent). <br><br>***","3df18a58":"The majority of people are from the 3rd class","5856e070":"**We will start with features which will be label encoded.**","75511054":"**KNN**","e2b7d1eb":"**Now we should one hot encode other categorical features**","4db296c5":"# Artificial Neural Networks","29229906":"**Support Vector Classifier**","039038eb":"**We will see if there is a correlation between\nFare and Pclass because it's logic that passengers from 1st class has more Fare than others from 2nd or 3rd class**","3dcede96":"**SibSp\/Parch Feature engineering**","94083764":"**As the name indicates: when a name has a \"Mr\" it's always a man and when a name has a \"Mrs\" or \"Miss\" it will be always a female so we could combine Sex and Name**","8ef3de1f":"The age has a normal distribution","0645f451":"***Sex And Pclass***","d82706ba":"**So far our basic models give us a good result because they don't have any hyperparameter optimization. So, we will try now to optimize them in the next section**","b73076d0":"**One more step is to standardize the numerical features**","b3eb1512":"So in Feature engineering part we will transform the cabin columns\nbecause as shown in this link: <a>https:\/\/www.ultimatetitanic.com\/interior-fittings<\/a>\n**the 1st class has all Cabins with A,B etc... and other classes\nhad the other Cabins. It should be noted that the majority of the\npassengers does not have a cabin.<br>\n*So we should see if someone didn't have a cabin\nhas it a more ratio to be dead ?***","c4879a71":"**As i've mentioned: each class has an ensemble of cabins reserved for this class. So it would be an important information if we combine cabins and classes**","313d5e66":"# Variable encoding and standardization","de520df9":"# Hyperparameter Optimization","66c46c32":"**Now we should process numerical variables Polynomial transformation**","771ae705":"# Predicting On Our Test Data ( Final Step )","22688ff0":"**Like we discussed before there is a correlation between class and fare. For example: 1st class has a higher fare**","df615dc3":"# Cross Validation Training And Metrics","87bfd0d1":"**The majority of passengers are males**","4330ecf7":"It's obvious from results that **being a man from 3rd class and a female from first class is different.**<br>\nIn fact being in a lower class means that you have a higher chance to be dead.\n\n**We could combine later (in the part of feature engineering) classes and Sex.**","3cd1a31a":"***Important note: When a passenger is from 3rd class and has a low fare he's likely to be dead. It's the contrary for a passenger from the 1st class and has a high fare.***","a55013b5":"**We'll start with a little sure steps in order to achieve our final model for that we will build our models without any parameters**","53e1d2a3":"as we can see:\n<br>\n<br>\n.for class1: the number of survivors is greater than the number of non survivors\n<br>\n<br>\n.for class2: the number of survivors is almost equal to the number of non survivors\n<br>\n<br>\n.for class3: the number of survivors is lower than the number of non survivors\n<br>\n<br>\n**====> the class of the passenger has a high influence on the surviving rate**\n","cc477d44":"**As we said previously in the notebook, taking into account that our target value is skewed and our problem is a binary classification, so we will use roc_auc_score as a metric for our model performance**","cc7d891f":"As we said the average (mean) of fare for the first class is so much higher than the second class's fare<br>\n\n**The idea here is to group Fare values. In fact we can see Fare in 3 types:<br><br>\n.LOW: interval = [0, 7]<br><br>\n.MEDIUM: interval = ]7, 30]<br><br>\n.HIGH: ]30, +oo[**","a1e5daf6":"**Logistic Regression**","d87503d5":"**Data binning for age**","f6d2be3a":"Same notes as Sib\/sp","e4bc6567":"![](https:\/\/images.squarespace-cdn.com\/content\/v1\/5006453fe4b09ef2252ba068\/1351392390422-DAVQH210UU91AO8BTFA3\/ke17ZwdGBToddI8pDm48kJXcIO88toyD_XBci6E7S48UqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKck8ck5G76t7AON3_TWSn1dTV9fvdvXBmN40Kbuy7yKNrHpTvt0Jykiu5cqBsa-cLZ\/Titanic_Blueprints_Design+%286%29.JPG?format=1500w)","390b991c":"**So being a passenger from 3rd class without a cabin decrease his survival ratio**","bd3ffe84":"**Gaussian Naive Bayes**","527ef894":"**KNN**","b60189c8":"# Exploratory Data Analysis:","326b7602":"# Feature engineering","db1340d3":"**Random forest classifier**","1c956abc":"**Support Vector Classifier ( SVC )**","8c4ad386":"It's clear from the first observations that the ratio of surviving could depend on the fare value. In fact, when a passenger have a low fare he's likely to be dead","6e169a8c":"We will encode categorical features and standardize categorical features","1b01bff6":"**We could combine Age and Sex and see the results**","71a29694":"**I think it's in some way clear that being Embarked in S make the death ratio for the passenger higher**","3cfc8606":"# Data Cleaning","837b55b6":"**Random Forest Classifier**","3964e115":"> **Important information here: Our target is skewed ( they don't have equal probabilities ), so we'll use roc_auc_score as a metric and StratifiedKFold in the cross validation settings**","a1c7eeb5":"So for any problem there are many steps that we should follow in order to solve it.\n\n***1. Exploratory Data Analysis:<br>***\nGetting insight into the dataset is one of the most important things. First step should be to look at the properties of data which gives us clarity. It can give us a meaningful understanding and relationship of data with the target.<br><br>\n***2. Baseline Submission:<br>***\nThe second step should be to see that you clearly understood the problem and solution requirements. This includes building a simple classifier\/regressor trained on raw features and submitting the test data. We can then further improve on this pipeline and reiterate to make our model more complex over time.<br><br>\n***3. Cross-Validation setting:<br>***\nThe third step is to properly train a model which does not overfit on public data. This is significantly important as many people build models that rely on public LB score gains and consequently loose rank in Private LB due to overfitting.<br><br>\n***4. Feature Engineering:<br>***\nThe feature insights from EDA can help us with engineering new features which can be helpful from the model point of view. Sometimes finding a \"magic\" feature draws a line between low-rank submissions and high-rank submissions.<br><br>\n***5. Hyperparameter Optimization:<br>***\nTuning algorithm and model parameters contribute to significant improvement in the score. Make sure to keep a log of things describing which works for you and which didn't.<br><br>\n***6. Ensembling:<br>***\nThe last step is all about creating a strong model from a large number of weak models. Find the right coefficients and combinations which works for you. Make sure you don't overfit the results.<br><br>\n\nNote: These steps could not be linear, in fact we will see in this notebook later that cross-validation setting is made after feature engineering","159c6ea1":"**We successfully finishing our data cleaning we can now process some feature engineering**","71dd97a0":"So before solving the problem we should take some really important notes.<br>\n***To begin with it's clearly a binary classification problem. In fact given some informations about passengers we should predict either they'll be dead or not.***","8ecb014a":"Note: You should be surprised why i didn't an analysis for the age, it's because i'll show it in feature engineering.","7abbba67":"**So The age is imputed successfully for train and test data**","4fb1161b":"We will try to extract that part of the name like (Mr., Mrs., etc...). We will see if there are other types","a07dac39":"**Logistic Regression**","4cdd2b0f":"***An important remark is that when a male called master, he has a higher chance to survive***","3dd4840f":"**Another important feature here is that Sex has a high influence on surviving rate**<br>\nwe should explore some relations between Sex, Pclass, and Surviving","3c77caad":"**Fare feature engineering**","b88ab4d1":"**Numerical Features**","350748b2":"**It's clear that passengers without cabins are more likely to be dead\nand it's logic in some ways**","f63f86f0":"**So we will build our previous models but we will tune the parameters for each model, using GridSearchCV, in order to reach the best params for each one.**","9f0e52f4":"First of all, we should read the data and print the shape of both, train and test data","8644256f":"**XGBoost**","3ebf4950":"**Here we printed some descriptions about the numerical features**","0a4e0362":"# Basic Models Training"}}