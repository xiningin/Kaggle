{"cell_type":{"8504fd9a":"code","54fb4b60":"code","c70d08a5":"code","a58b41d8":"code","6a7fa415":"code","d4f7912d":"code","e8eb8e18":"code","b2283b5f":"code","efef5c84":"code","f867e9b4":"code","77b2df2c":"code","b3a60e50":"code","e5ee00c0":"code","81254d95":"code","ee0d5363":"code","df862d08":"code","23b236f4":"code","3b270b6a":"code","66bcde40":"code","2222d135":"code","4ba4bc2e":"code","a4052a27":"code","bb1129ff":"code","83f4a255":"code","cb95e059":"code","d1845b6e":"code","786e16f1":"code","895a745f":"code","ec602fbe":"code","fc073afb":"code","293c70cc":"code","eefd079d":"code","6e9ad237":"code","964218f0":"code","6c3cfb15":"code","b2ee378e":"code","a7653da3":"code","04400490":"code","ac3781d6":"code","7f10a252":"code","a9da422c":"code","140b2e5f":"code","c7562b11":"code","154c8ee1":"code","13a2a9d0":"code","fb8c4562":"code","e00c1f72":"code","0c227586":"code","4aa03c04":"code","06b566c6":"code","9c2ae15c":"code","15dd83a1":"code","4991a2e6":"code","de9b851b":"code","b72c6a26":"code","46958f20":"code","82c8dad2":"code","33afddd6":"code","9dd68649":"code","4003a8aa":"code","3cb7ad31":"code","2a37309f":"code","e9a580fd":"code","232b3ce5":"code","93b79670":"code","208fd08a":"code","5c755b87":"code","8f5e0e9a":"code","b857eb5d":"code","63700d75":"code","2fb793ca":"code","e0ed01be":"code","8b0aa2ce":"code","a07e461f":"code","4003ad6c":"code","222f139c":"code","f33d5e3f":"code","ab88e170":"code","b252b19a":"code","1572e4b7":"code","8b43745f":"code","8ee67974":"code","0daf5ec5":"code","e7c25245":"code","23d8c28d":"code","8d8b0c17":"code","60a7a6e5":"code","f60284b4":"code","15a03683":"code","442fb1ba":"code","e9c3bd76":"code","ef0e8672":"code","638308df":"code","8d1e3370":"markdown","813262a9":"markdown","5a13a321":"markdown","300640ea":"markdown","b599feca":"markdown","6f6e023d":"markdown","9e2ac795":"markdown","93f757ba":"markdown","dd18191b":"markdown","c5573014":"markdown","8005d844":"markdown","13be3faa":"markdown","d729a535":"markdown","ee89e028":"markdown","39c14645":"markdown","f914cfc2":"markdown","62151512":"markdown","03f3eda7":"markdown","391e4538":"markdown","4ad1dc53":"markdown","e14261ec":"markdown","18512ebf":"markdown","b6ee96ce":"markdown","1697bbd0":"markdown","13ffc496":"markdown","cbb14ba6":"markdown","cd9f64f1":"markdown","430fa615":"markdown","527b2df5":"markdown","1143b186":"markdown","97b32cc1":"markdown","f2d8acf7":"markdown","0cf1f5e5":"markdown","7588fcca":"markdown","e25d7d0a":"markdown","e444946f":"markdown","486cc101":"markdown","d0f6937c":"markdown","9ab216e5":"markdown","d2880046":"markdown","41149125":"markdown","0c015924":"markdown","ee10599e":"markdown","3abea939":"markdown"},"source":{"8504fd9a":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\nfrom random import sample\nfrom sklearn.utils import resample\nfrom imblearn import under_sampling,over_sampling\nfrom sklearn.preprocessing import StandardScaler,LabelEncoder\nfrom sklearn.model_selection import train_test_split,learning_curve,StratifiedKFold\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\nfrom sklearn.dummy import DummyClassifier\nfrom statsmodels.stats import stattools\nimport statsmodels.graphics.tsaplots as smgt\nfrom sklearn.metrics import roc_curve, auc, accuracy_score, confusion_matrix, classification_report, precision_recall_curve, average_precision_score, roc_auc_score\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","54fb4b60":"def plot_categoric_feature(feature_name):\n    dftemp = dfdata.groupby([feature_name,'churn'],as_index = False).agg({'customerid':'count','monthlycharges':'mean'})\n    dftemp['feature_rate'] = dftemp.apply(lambda row: row['customerid'] \/ dftemp[dftemp[feature_name] == row[feature_name]]['customerid'].sum() ,axis = 1)\n    dftemp['rate'] = dftemp['customerid'] \/ dftemp['customerid'].sum()\n    fig,ax = plt.subplots(1,3,figsize=(20,3))\n    sns.barplot(y=feature_name, x = 'feature_rate', color = 'darkorange',data = dftemp[dftemp['churn'] == 'Yes'],ax = ax[0])\n    ax[0].set(title = 'churn rate in feature ' + feature_name, xlabel = '')\n    sns.barplot(y=feature_name, x = 'rate', hue='churn',data = dftemp,ax = ax[1])\n    ax[1].set(title = feature_name + ' rate in whole dataset',xlabel = '')\n    sns.barplot(y=feature_name, x = 'monthlycharges', hue='churn',data = dftemp,ax = ax[2])\n    ax[2].set(title = feature_name + ' monthlycharges',xlabel = '')\n    plt.show()","c70d08a5":"def plot_learning_curve(estimator, X_train, y_train):\n    kfold = StratifiedKFold(n_splits = 5)\n    train_size,train_scores,test_scores = learning_curve(estimator,X_train,y_train,train_sizes = np.linspace(0.05,1,20),cv = kfold)\n    train_scores_mean = np.mean(train_scores,axis=1)\n    train_scores_std = np.std(train_scores,axis=1)\n    test_scores_mean = np.mean(test_scores,axis=1)\n    test_scores_std = np.std(test_scores,axis=1)\n\n    sns.lineplot(x=train_size, y=train_scores_mean, c='r', label='train')\n    plt.fill_between(x=train_size, y1=train_scores_mean+train_scores_std, y2=train_scores_mean-train_scores_std, alpha=0.1, color='r')\n    sns.lineplot(x=train_size,y=test_scores_mean,c='b',label='test')\n    plt.fill_between(x=train_size, y1=test_scores_mean+test_scores_std, y2=test_scores_mean-test_scores_std, alpha=0.1, color='b')\n    plt.legend(loc='best')\n    plt.title(\"Learning Curve\")","a58b41d8":"def plot_confusion_matrix(estimator,y,y_pred):\n    cm = confusion_matrix(y, y_pred, labels = [0,1] )\n    sns.heatmap(cm, annot=True,  fmt='.0f', xticklabels = [\"No\", \"Yes\"] , yticklabels = [\"No\", \"Yes\"],cbar = False)\n    plt.title(\"Confusion Matrix\")\n    plt.ylabel('Actual')\n    plt.xlabel('Prediction')","6a7fa415":"def plot_empty_confusion_matrix():\n    plt.text(0.45, .6, \"TN\", size=100, horizontalalignment='right')\n    plt.text(0.45, .1, \"FN\", size=100, horizontalalignment='right')\n    plt.text(.95, .6, \"FP\", size=100, horizontalalignment='right')\n    plt.text(.95, 0.1, \"TP\", size=100, horizontalalignment='right')\n    plt.xticks([.25, .75], [\"predicted negative\", \"predicted positive\"], size=15)\n    plt.yticks([.25, .75], [\"positive class\", \"negative class\"], size=15)\n    plt.plot([.5, .5], [0, 1], '--', c='k')\n    plt.plot([0, 1], [.5, .5], '--', c='k')\n    plt.xlim(0, 1)\n    plt.ylim(0, 1)","d4f7912d":"def print_clf_result(estimator,X_train,y_train,X_test = None, y_test = None):\n    y_pred = estimator.predict(X_train)\n    acc = round(accuracy_score(y_train, y_pred),2)\n    print(\"Train Accuracy : \", acc)\n    if X_test is None:\n        y_pred_test = estimator.predict(X_train)\n        plt.figure(figsize=(18,5))\n        plt.subplot(121)\n        plot_learning_curve(estimator,X_train,y_train)\n        plt.subplot(122)\n        plot_confusion_matrix(estimator,y_train,y_pred)   \n    else:\n        test_acc = round(estimator.score(X_test,y_test),2)\n        print(\"Classification Accuracy :\", test_acc)\n        y_pred_test = estimator.predict(X_test)\n        plt.figure(figsize=(18,5))\n        plt.subplot(121)\n        plot_learning_curve(estimator,X_train,y_train)\n        plt.subplot(122)\n        plot_confusion_matrix(estimator,y_test,y_pred_test)\n    plt.show()","e8eb8e18":"def make_clf_data(n_points, n_centers = 2, random_state = 42):\n    n_features=2\n    rnd_gen = np.random.RandomState(random_state)\n    feature_names = ['feature' + str(x+1) for x in range(n_features)]\n    X = pd.DataFrame(columns = feature_names)\n    for center in range(n_centers):\n        X = X.append(pd.DataFrame(rnd_gen.normal(loc=5*center, size=(n_points, n_features)), columns=feature_names),ignore_index=True)\n    X['target'] = (X['feature1'] > 2)\n    n_changes = int(n_points * 0.2)\n    list_true = sample(X[X['target'] == True].index.to_list(), n_changes)\n    X.loc[list_true,'target'] = False\n    n_changes = int(n_points * 0.1)\n    list_false = sample(X[X['target'] == False].index.to_list(),n_changes)\n    X.loc[list_false,'target'] = True\n    X = X.sample(frac=1).reset_index(drop=True)\n    return X","b2283b5f":"def plot_decision_boundary(estimator,X,title = None):\n    xx = np.linspace(-3, 9, 100)\n    yy = np.linspace(-3, 9, 100)\n    X1, X2 = np.meshgrid(xx, yy)\n    X_grid = np.c_[X1.ravel(), X2.ravel()]\n    decision_values = estimator.decision_function(X_grid)\n    sns.scatterplot(x='feature1', y='feature2', hue='target', data = X)\n    plt.contour(X1, X2, decision_values.reshape(X1.shape), colors=\"black\",levels = 0)\n    if title is not None:\n        plt.title(title)\n","efef5c84":"dfdata = pd.read_csv(\"..\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\ndfdata.columns = dfdata.columns.str.lower()\ndfdata.shape","f867e9b4":"dfdata.describe()","77b2df2c":"dfdata.duplicated().sum()","b3a60e50":"dfdata.isnull().sum()","e5ee00c0":"dfdata.info()","81254d95":"dfdata.nunique()","ee0d5363":"dfdata['seniorcitizen'] = dfdata['seniorcitizen'].map({0:'No',1:'Yes'})","df862d08":"dfdata['totalcharges_new'] = pd.to_numeric(dfdata['totalcharges'],errors = 'coerce')\ndfdata[dfdata['totalcharges_new'].isnull() == True][['totalcharges','totalcharges_new']]","23b236f4":"dfdata['totalcharges_new'].fillna(0,inplace = True)\ndfdata['totalcharges'] = dfdata['totalcharges_new']\ndfdata.drop('totalcharges_new',axis=1,inplace=True)","3b270b6a":"dfdata.head()","66bcde40":"print(round(100 * dfdata['churn'].value_counts() \/ dfdata.shape[0],0))\nplt.figure(figsize=(10,2))\nsns.countplot(y='churn',data = dfdata)\nplt.show()","2222d135":"numeric_features = dfdata.columns[dfdata.dtypes != 'object'].values.tolist()\ncategoric_features = dfdata.columns[dfdata.dtypes == 'object'].values.tolist()\ncategoric_features.remove('customerid')\ncategoric_features.remove('churn')\nprint(\"Categoric features : \",categoric_features)\nprint(\"Numeric features : \",numeric_features)","4ba4bc2e":"fig,ax = plt.subplots(1,3,figsize=(18,5))\n\nsns.distplot(dfdata[dfdata['churn'] == 'No']['tenure'],label = 'No',ax=ax[0])\nsns.distplot(dfdata[dfdata['churn'] == 'Yes']['tenure'],label = 'Yes',ax=ax[0])\nax[0].set_title('Tenure')\nax[0].legend()\n\nsns.distplot(dfdata[dfdata['churn'] == 'No']['monthlycharges'],label = 'No',ax=ax[1])\nsns.distplot(dfdata[dfdata['churn'] == 'Yes']['monthlycharges'],label = 'Yes',ax=ax[1])\nax[1].set_title('Monthly Charges')\nax[1].legend()\n\nsns.distplot(dfdata[dfdata['churn'] == 'No']['totalcharges'],label = 'No',ax=ax[2])\nsns.distplot(dfdata[dfdata['churn'] == 'Yes']['totalcharges'],label = 'Yes',ax=ax[2])\nax[2].set_title('Total Charges')\nax[2].legend()\n\nplt.show()","a4052a27":"sns.pairplot(dfdata[numeric_features + ['churn']],hue = 'churn', diag_kind = 'kde')\nplt.show()","bb1129ff":"bins = range(12,75,12)\ndfdata['tenure_bin'] = np.digitize(dfdata['tenure'],bins,right = True)\ndfdata['tenure_bin'] = dfdata['tenure_bin'].astype('category')\ncategoric_features.append('tenure_bin')\nplot_categoric_feature('tenure_bin')","83f4a255":"dfdata['meancharges'] = dfdata['totalcharges'] \/ dfdata['tenure']\nnumeric_features.append('meancharges')\nfig,ax = plt.subplots(1,2,figsize=(18,4))\nsns.scatterplot(x='monthlycharges',y='meancharges',data = dfdata,ax = ax[0])\nsns.boxplot(x = 'tenure_bin',y = 'monthlycharges',data= dfdata,ax = ax[1])\nplt.show()","cb95e059":"dfdata['comparemean'] = dfdata['meancharges'] > dfdata['monthlycharges']\ndfdata['comparemean'] = np.where(dfdata['meancharges'] == dfdata['monthlycharges'],'Equal',dfdata['comparemean'])\ndfdata['comparemean'] = dfdata['comparemean'].astype('category')\nplot_categoric_feature('comparemean')","d1845b6e":"dfdata[categoric_features].nunique()","786e16f1":"plot_categoric_feature('gender')","895a745f":"plot_categoric_feature('seniorcitizen')","ec602fbe":"plot_categoric_feature('partner')","fc073afb":"plot_categoric_feature('dependents')","293c70cc":"plot_categoric_feature('phoneservice')","eefd079d":"pd.crosstab(dfdata['phoneservice'],dfdata['multiplelines'])","6e9ad237":"plot_categoric_feature('multiplelines')","964218f0":"plot_categoric_feature('internetservice')","6c3cfb15":"plot_categoric_feature('onlinesecurity')","b2ee378e":"plot_categoric_feature('onlinebackup')","a7653da3":"plot_categoric_feature('deviceprotection')","04400490":"plot_categoric_feature('techsupport')","ac3781d6":"plot_categoric_feature('streamingtv')","7f10a252":"plot_categoric_feature('streamingmovies')","a9da422c":"plot_categoric_feature('contract')","140b2e5f":"plot_categoric_feature('paperlessbilling')","c7562b11":"plot_categoric_feature('paymentmethod')","154c8ee1":"dfdata['internet_fiber'] = np.where(dfdata['internetservice'] == 'Fiber optic','Yes','No')\ndfdata['monthly_contract'] = np.where(dfdata['contract'] == 'Month-to-month','Yes','No')\ndfdata['electronic_payment'] = np.where(dfdata['paymentmethod'] == 'Electronic check','Yes','No')\ncategoric_features.extend(['internet_fiber','monthly_contract','electronic_payment'])","13a2a9d0":"dfdata['internet'] = np.where(dfdata['internetservice'] == 'No','No','Yes')\ndfdata['num_services'] = (dfdata[['internet','onlinesecurity','onlinebackup','deviceprotection','techsupport','streamingtv','streamingmovies']] == 'Yes').sum(axis=1)\ndfdata['num_services'] = dfdata['num_services'].astype('category')\nplot_categoric_feature('num_services')\ncategoric_features.append('internet')\ndfdata['num_services'] = dfdata['num_services'].astype('int')\nnumeric_features.append('num_services')\n","fb8c4562":"dfdata['monthly_mean_diff'] = (dfdata['monthlycharges'] - dfdata['monthlycharges'].mean()) \/ dfdata['monthlycharges'].mean()\nnumeric_features.append('monthly_mean_diff')\nservices_list = ['internetservice','onlinesecurity', 'onlinebackup', 'deviceprotection', 'techsupport','streamingtv', 'streamingmovies']\nfor service in services_list:\n    colname = service + '_mean_diff'\n    dfdata[colname] = dfdata['monthlycharges'] \/ dfdata.groupby(service)['monthlycharges'].transform('mean')\n    numeric_features.append(colname)","e00c1f72":"le = LabelEncoder()\nencoded_features = []\nfor feature in categoric_features:\n    colname = 'le_' + feature\n    dfdata[colname] = le.fit_transform(dfdata[feature])\n    encoded_features.append(colname)","0c227586":"plt.figure(figsize=(12,6))\nsns.heatmap(dfdata[numeric_features].corr(),annot = True,fmt  ='.1g')\nplt.show()","4aa03c04":"drop_list = ['meancharges','totalcharges','monthly_mean_diff','onlinebackup_mean_diff','deviceprotection_mean_diff','techsupport_mean_diff','streamingmovies_mean_diff']","06b566c6":"plt.figure(figsize=(12,6))\nsns.heatmap(dfdata[numeric_features].drop(drop_list,axis=1).corr(),annot = True,fmt  ='.1g')\nplt.show()","9c2ae15c":"numeric_features = [x for x in numeric_features if x not in drop_list]","15dd83a1":"dfdata.columns","4991a2e6":"plt.figure(figsize=(18,12))\nsns.heatmap(dfdata[encoded_features].corr(),annot = True,fmt  ='.1g')\nplt.show()","de9b851b":"drop_list = ['le_phoneservice','le_contract','le_internet','le_tenure_bin']","b72c6a26":"plt.figure(figsize=(18,12))\nsns.heatmap(dfdata[encoded_features].drop(drop_list, axis=1).corr(),annot = True,fmt  ='.1g')\nplt.show()","46958f20":"categoric_features = [x for x in encoded_features if x not in drop_list and categoric_features]","82c8dad2":"[categoric_features + numeric_features]","33afddd6":"dftemp = make_clf_data(100)\nX = dftemp.drop('target',axis=1)\ny = dftemp['target']\nlog_reg = LogisticRegression()\nlog_reg.fit(X, y)\nprint(np.round(log_reg.intercept_, 2), np.round(log_reg.coef_, 2))\nprint_clf_result(log_reg, X, y)","9dd68649":"plot_decision_boundary(log_reg,dftemp)","4003a8aa":"c_list = [0.01,0.1,1,10]\nplt.figure(figsize=(18,5))\nfor index, c_value in enumerate(c_list):\n    title = 'C : ' + str(c_value)\n    log_reg = LogisticRegression(C = c_value).fit(X,y)\n    plt.subplot(1,len(c_list),index+1)\n    plot_decision_boundary(log_reg, dftemp, title)\nplt.show()","3cb7ad31":"X = dfdata[numeric_features + categoric_features]\ny = dfdata['churn'].map({'No':0,'Yes':1})\nX_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y)","2a37309f":"scaler = StandardScaler().fit(X_train)\nX_train_scaled = pd.DataFrame(scaler.transform(X_train), columns = X_train.columns)\nX_test_scaled = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)","e9a580fd":"log_reg = LogisticRegression()\nlog_reg.fit(X_train_scaled, y_train)\nprint(np.round(log_reg.intercept_, 2), np.round(log_reg.coef_, 2))\nprint_clf_result(log_reg, X_train_scaled, y_train, X_test_scaled, y_test)","232b3ce5":"c_list = [0.001,0.01,0.1,1,10,100,1000]\naccuracy_list = []\ncoef_list = []\nfig,ax = plt.subplots(1,2,figsize=(18,5))\nfor index, c_value in enumerate(c_list):\n    log_reg = LogisticRegression(C = c_value).fit(X_train_scaled,y_train)\n    y_pred = log_reg.predict(X_train_scaled)\n    accuracy_list.append(log_reg.score(X_train_scaled,y_train))\n    ax[0].plot(np.array(log_reg.coef_).ravel(),label = 'c:'+str(c_value))\n    ax[0].legend()\n    ax[0].set_title('Coefficients')\nax[1].plot(accuracy_list)\nax[1].set_title('Accuracy')\nplt.xticks(range(len(c_list)),c_list)\nplt.show()","93b79670":"c_list = [0.001,0.01,0.05,0.1,1,10]\nscore = []\nn_zero_coefs = []\nfor c_value in c_list:\n    log_reg = LogisticRegression(C=c_value, penalty='l1', solver='liblinear').fit(X_train,y_train)\n    coef = np.round(log_reg.coef_,4)\n    n_zero_coefs.append(len(coef[coef == 0]))\n    score.append(round(log_reg.score(X_train, y_train),2))\n    \ndftemp = pd.DataFrame(zip(c_list, n_zero_coefs, score), columns = ['alpha','zero_coef','score'])\ndftemp","208fd08a":"estimator = LogisticRegression(C=0.05, penalty='l1', solver='liblinear')\nlog_reg = estimator.fit(X_train_scaled, y_train)\nrfe = RFE(estimator,n_features_to_select=3).fit(X_train_scaled, y_train)\ndftemp = pd.DataFrame(zip(X_train_scaled.columns.values,rfe.ranking_,log_reg.coef_.ravel()),columns = ['feature','rank','coef'])\ndftemp.sort_values('rank', ascending=True, inplace=True)\ndftemp.reset_index(inplace=True,drop=True)\ndftemp.tail(10)","5c755b87":"X_temp = X_train_scaled.drop(dftemp.iloc[-8:,0].values,axis = 1)\nlog_reg = LogisticRegression()\nlog_reg.fit(X_temp, y_train)\nprint(np.round(log_reg.intercept_, 2), np.round(log_reg.coef_, 2))\nprint_clf_result(log_reg, X_temp, y_train)","8f5e0e9a":"X.drop(dftemp.iloc[-8:,0].values,axis = 1,inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(X,y)","b857eb5d":"scaler = StandardScaler().fit(X_train)\nX_train_scaled = pd.DataFrame(scaler.transform(X_train), columns = X_train.columns)\nX_test_scaled = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)","63700d75":"svc = LinearSVC(C=0.05)\nsvc.fit(X_train_scaled,y_train)\nprint(np.round(svc.intercept_, 2), np.round(svc.coef_, 2))\nprint_clf_result(svc, X_train_scaled, y_train,X_test_scaled,y_test)","2fb793ca":"svc = SVC(C=1, gamma=0.1, kernel = 'rbf')\nsvc.fit(X_train_scaled,y_train)\nprint_clf_result(svc, X_train_scaled, y_train,X_test_scaled,y_test)","e0ed01be":"sgd = SGDClassifier(loss='hinge', alpha=0.05, eta0=0.01)\nsgd.fit(X_train_scaled, y_train)\nprint_clf_result(sgd, X_train_scaled, y_train, X_test_scaled, y_test)","8b0aa2ce":"gnb = GaussianNB()\ngnb.fit(X_train, y_train)\nprint_clf_result(gnb, X_train, y_train, X_test, y_test)","a07e461f":"mnb = MultinomialNB(alpha=0.05)\nmnb.fit(X_train, y_train)\nprint_clf_result(gnb, X_train, y_train, X_test, y_test)","4003ad6c":"bnb = BernoulliNB(alpha=0.05)\nbnb.fit(X_train, y_train)\nprint_clf_result(bnb, X_train, y_train, X_test, y_test)","222f139c":"dt = DecisionTreeClassifier(max_depth=5, random_state=12)\ndt.fit(X_train_scaled, y_train)\nprint_clf_result(dt, X_train_scaled, y_train, X_test_scaled, y_test)","f33d5e3f":"dftemp = pd.DataFrame(zip(X.columns.values,dt.feature_importances_), columns = ['feature','importance'])\nplt.figure(figsize=(18,5))\nsns.barplot(x='importance', y='feature', data=dftemp)\nplt.show()","ab88e170":"dftemp = pd.DataFrame(np.round(100 * log_reg.predict_proba(X_test_scaled),0),columns = ['prob0','prob1'])\ndftemp['y'] = y_test.values\ndftemp['y_pred'] = log_reg.predict(X_test_scaled)\ndftemp['decision'] = np.round(log_reg.decision_function(X_test_scaled), 2)\ndftemp['error'] = np.abs(dftemp['y'] - dftemp['y_pred'])\nprint(\"Min and max decision function values : \",round(np.min(log_reg.decision_function(X_test_scaled)),2),round(np.max(log_reg.decision_function(X_test_scaled)),2))\ndftemp.head()\n","b252b19a":"bins = range(0,100,5)\ndftemp['prob1bin'] = np.digitize(dftemp['prob1'],bins,right=True)\nsns.barplot(x='prob1bin',y='error',data=dftemp)\nplt.show()","1572e4b7":"dummy_majority = DummyClassifier(strategy='most_frequent').fit(X_train_scaled, y_train)\npred_most_frequent = dummy_majority.predict(X_test_scaled)\nprint(\"Test score: {:.2f}\".format(dummy_majority.score(X_test_scaled, y_test)))","8b43745f":"round(100 * dfdata['churn'].value_counts() \/ dfdata.shape[0], 0)","8ee67974":"rus = under_sampling.RandomUnderSampler()\nX_rus, y_rus = rus.fit_sample(X_train_scaled,y_train)","0daf5ec5":"log_reg = LogisticRegression()\nlog_reg.fit(X_rus, y_rus)\nprint(np.round(log_reg.intercept_, 2), np.round(log_reg.coef_, 2))\nprint_clf_result(log_reg, X_rus, y_rus,X_test_scaled,y_test)","e7c25245":"smote = over_sampling.SMOTE(sampling_strategy='minority')\nX_sm, y_sm = smote.fit_sample(X_train_scaled, y_train)","23d8c28d":"log_reg = LogisticRegression()\nlog_reg.fit(X_sm, y_sm)\nprint(np.round(log_reg.intercept_, 2), np.round(log_reg.coef_, 2))\nprint_clf_result(log_reg, X_sm, y_sm,X_test_scaled,y_test)","8d8b0c17":"svc = SVC(C=1, gamma=0.1, kernel = 'rbf', class_weight='balanced', probability=True)\nsvc.fit(X_train_scaled, y_train)\nprint_clf_result(svc, X_train_scaled, y_train, X_test_scaled, y_test)","60a7a6e5":"plot_empty_confusion_matrix()","f60284b4":"y_pred = log_reg.predict(X_test_scaled)\nplt.figure(figsize=(2,2))\nplot_confusion_matrix(log_reg, y_test, y_pred)\nplt.show()\nprint(classification_report(y_test,y_pred))","15a03683":"y_pred_threshold = log_reg.decision_function(X_test_scaled) > -0.2\nplt.figure(figsize=(2,2))\nplot_confusion_matrix(log_reg, y_test, y_pred_threshold)\nplt.show()\nprint(classification_report(y_test,y_pred_threshold))","442fb1ba":"y_pred_threshold = log_reg.predict_proba(X_test_scaled)[:,1] > 0.35\nplt.figure(figsize=(2,2))\nplot_confusion_matrix(log_reg, y_test, y_pred_threshold)\nplt.show()\nprint(classification_report(y_test,y_pred_threshold))","e9c3bd76":"aps_logreg = round(average_precision_score(y_test, log_reg.predict_proba(X_test_scaled)[:, 1]),2)\naps_svc = round(average_precision_score(y_test, svc.decision_function(X_test_scaled)),2)\nprint(\"Average Precision Scores (log reg and svc) : \", aps_logreg, aps_svc)\nprecision_lr, recall_lr, thresholds_lr = precision_recall_curve(y_test, log_reg.decision_function(X_test_scaled))\nclose_zero_lr = np.argmin(np.abs(thresholds_lr))\nplt.plot(precision_lr[close_zero_lr], recall_lr[close_zero_lr], 'o', markersize=10, label=\"threshold zero logreg\", fillstyle=\"none\", c='k', mew=2)\nplt.plot(precision_lr, recall_lr, label=\"log reg\")\n\nprecision_svc, recall_svc, thresholds_svc = precision_recall_curve(y_test, svc.decision_function(X_test_scaled))\nclose_zero_svc = np.argmin(np.abs(thresholds_svc))\nplt.plot(precision_svc[close_zero_svc], recall_svc[close_zero_svc], 'v', markersize=10, label=\"threshold zero svc\", fillstyle=\"none\", c='k', mew=2)\nplt.plot(precision_svc, recall_svc, label=\"svc\")\nplt.legend()\nplt.title('Precision Recall Curve')\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")\nplt.show()","ef0e8672":"plot_empty_confusion_matrix()","638308df":"auc_logreg = roc_auc_score(y_test, log_reg.predict_proba(X_test_scaled)[:, 1])\nauc_svc = roc_auc_score(y_test, svc.decision_function(X_test_scaled))\nprint(\"AUC scores (logreg and svc) : \", round(auc_logreg,2), round(auc_svc,2))\n\nfpr_lr, tpr_lr, thresholds_lr = roc_curve(y_test, log_reg.decision_function(X_test_scaled))\nplt.plot(fpr_lr, tpr_lr, label=\"ROC LogReg\")\nclose_zero_lr = np.argmin(np.abs(thresholds_lr))\nplt.plot(fpr_lr[close_zero_lr], tpr_lr[close_zero_lr], 'o', markersize=10,label=\"threshold zero\", fillstyle=\"none\", c='k', mew=2)\n\nfpr_svc, tpr_svc, thresholds_svc = roc_curve(y_test, svc.decision_function(X_test_scaled))\nplt.plot(fpr_svc, tpr_svc, label=\"ROC SVC\")\nclose_zero_svc = np.argmin(np.abs(thresholds_svc))\nplt.plot(fpr_svc[close_zero_svc], tpr_svc[close_zero_svc], 'v', markersize=10, label=\"threshold zero\", fillstyle=\"none\", c='k', mew=2)\n\nplt.title('ROC Curve')\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR (recall)\")\nplt.legend(loc=4)\nplt.show()","8d1e3370":"- As tenure reaches to 3 years, customer gets extra discounts and average monthly charges decrease.\n- At first two years, average monthly charges increase with tenure.","813262a9":"# SVM\n- Linear and non linear support vector machines\n- Split data via linear vectors\n- Kernel trick : transformation of data to split data via linear vectors\n- Uses hinge loss function to determine classes\n- Hinge loss : Higher accuracy, worse probability analysis\n- High performance on low feature, low volume data","5a13a321":"## Confusion Matrix","300640ea":"# Uncertainty Estimates\n- Decision function\n- Probability Predictions\n- Not supported all models\n- Can be used via changing thresholds","b599feca":"- Customers who have no dependents tend to churn. Nearly 30% of customers have dependents.","6f6e023d":"- New customers until 20 months tend to churn more\n- Customers monthly charges higher than 70 tend to churn more\n- Total charges dont depend on churn. Seems to be non important","9e2ac795":"## ROC Curve\n- Tradeoff between recall and precision\n- Find best threshold to optimize both\n- Use in GridSearchCV, model, cross_val_score scoring parameter or test via different model hyperparameters","93f757ba":"- Using streamingmovies or streamingtv has little effect on churn rate.\n- Prices seem to be nearly same for both services.","dd18191b":"# Feature Selection","c5573014":"## Target Variable","8005d844":"# Naive Bayes Classifier\n- Based on Bayes probability theorem\n- Can be used for determination of baseline accuracy\n- Fast training but worse generalization performance\n- Best in low volume data\n    - GaussianNB is used for continuos features\n    - BernoulliNB is used for binary features\n    - MultinomialNB is used for multi class features","13be3faa":"## Data Preparation Methods","d729a535":"\\begin{equation}\n\\text{TPR} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n\\end{equation}\n\n\\begin{equation}\n\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\n\\end{equation}\n","ee89e028":"- Gender, PhoneService, MultipleLines dont have a clear difference in distribution of churn rates.","39c14645":"- 90% of customers use phone services but churn rate seems to be same among customers.\n- Using phone services means higher monthly charges but it doesnt change churn rate so much.","f914cfc2":"# EDA","62151512":"# Categorical Features","03f3eda7":"## Numerical Features","391e4538":"- low C parameter means, high regularization and high bias.\n- high C parameter means, high risk of overfitting.","4ad1dc53":"- 50% of customers dont use online security and they tend to churn more.\n- Online security service seems to be free or so cheap for customers using internet service.","e14261ec":"- Customers are generally young, old customers tend to churn and paying higher monthly charges","18512ebf":"- Customers working without partner tend to churn. Nearly half of the customers work with partner","b6ee96ce":"- 20% of customers use mailed check, paying less and their churn rate is lower.\n- Monthly charges dont change much with other payment methods.\n- 30% of customers use electronic check and they tend to churn more.","1697bbd0":"- Positive Class : Geri D\u00f6nen M\u00fc\u015fteri\n- D\u00f6nmeyecek m\u00fc\u015fteriyi d\u00f6ner olarak tahmin etmenin (FP) maliyeti =\n- Geri d\u00f6nen m\u00fc\u015fteriyi d\u00f6nmez olarak tahmin etmenin (FN) maliyeti = ","13ffc496":"# Functions","cbb14ba6":"# Performance Metrics","cd9f64f1":"# SGD Classifier\n- Gradient Descent Method\n- User defined loss functions can be used in addition to Hinge and Log loss functions\n- Efficient on high volume data","430fa615":"- 45% of customers dont use device protection and they tend to churn more.\n- Device protection service seems to be so cheap for customers using internet service.","527b2df5":"## SVC","1143b186":"## Balancing via Classifier","97b32cc1":"- Most of the customers use fiberoptic and they tend to churn more.\n- Fiberoptic internet service is more expensive than DSL as expected.","f2d8acf7":"- 60% of customers use paperlessbilling, they tend to pay more charges and tend to churn more.","0cf1f5e5":"<img src='https:\/\/ai-master.gitbooks.io\/logistic-regression\/assets\/sigmoid_function.png' width='50%' height = '50%'>","7588fcca":"# Logistic Regression\n- Normal regression formula into sigmoid function\n- Result is the probability of true, 1 if positive, 0 if negative\n- Linear split of data space\n- Selection of decision boundary is important\n- Regression Assumptions\n    - Handling of outlier data points\n    - No perfect multicollinearity between the predictors (via VIF Factor or correlation)\n- Regularization type and magnitude are important parameters.\n- Uses logarithmic loss function to determine classes","e25d7d0a":"- 50% of customers dont use online backup and they tend to churn more.\n- Online backup service seems to be free or so cheap for customers using internet service.","e444946f":"# Missing Values","486cc101":"- 50% of customers dont use device protection and they tend to churn more","d0f6937c":"- Gender seems to have no effect on churn rate and monthly charges","9ab216e5":"- Monthly contracts pay less and tend to churn more.","d2880046":"# Decision Tree Classifier","41149125":"- Phone services info is given in multiple lines feature. Churn rate seems to be same among customers using multiplelines","0c015924":"# Get Data","ee10599e":"# Imbalanced Data\n- Balance data via data preparation methods, use of suitable classification algorithms or performance metrics\n- Random Under Sampling\n    - Discard majority class data\n    - Information loss\n- Random Over Sampling\n    - Random generation of minority class data\n    - Risk of overfit\n- Cluster Based Over Sampling\n    - Cluster minority and majority class data independently.\n    - Random generation of data for each cluster.\n    - Risk of overfit\n- SMOTE (Synthetic Minority OverSampling Technique)\n    - Random sub sample of minority class data\n    - Generate synthetic data from random selected data via KNN\n    - Not good performance on high volume data\n- Imbalanced Data Classifiers\n   - Ensemble Classifiers\n   - Cost Censitive Classifiers\n- Imbalanced Data Performance Metrics\n    - F1 score\n    - F2 score\n    - ROC AUC - PR AUC\n    - Precision and Recall\n    - Accuracy and G-Mean","3abea939":"## Common Metrics\n\\begin{equation}\n\\text{Accuracy} = \\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}\n\\end{equation}\n\n\\begin{equation}\n\\text{Precision (Positive Prediction Value)} = \\frac{\\text{TP}}{\\text{TP} + \\text{FP}}\n\\end{equation}\n\n\\begin{equation}\n\\text{Recall (Sensitivity, TPR)} = \\frac{\\text{TP}}{\\text{TP} + \\text{FN}}\n\\end{equation}\n\n\\begin{equation}\n\\text{FPR} = \\frac{\\text{FP}}{\\text{FP} + \\text{TN}}\n\\end{equation}\n\n\\begin{equation}\n\\text{Specificity} = \\frac{\\text{TN}}{\\text{FP} + \\text{TN}}\n\\end{equation}\n\n\\begin{equation}\n\\text{G-Mean} = (\\text{Sensitivity} * \\text{Specificity})\n\\end{equation}\n\n\\begin{equation}\n\\text{F1-Score} = 2 \\cdot \\frac{\\text{precision} \\cdot \\text{recall}}{\\text{precision} + \\text{recall}}\n\\end{equation}"}}