{"cell_type":{"4397e913":"code","65585136":"code","30d59bd9":"code","6818df7b":"code","b83e73ac":"code","4d158485":"code","73a39dab":"code","17583797":"code","d522865d":"code","80fda03d":"code","a2597ce1":"code","8040f94a":"code","b4bf931a":"code","586490f9":"code","268b90ad":"code","91359583":"code","74fa630f":"code","1d8c859c":"code","0c196a06":"code","0e9b20b8":"code","6d4f8e2c":"code","ab3a06f9":"code","6e57cdfa":"code","3b6a9102":"code","4cc45234":"code","51fc3070":"code","da8901c6":"code","709fb9f9":"code","8fb12dab":"code","fccb5ba9":"code","8f9591bf":"code","8653e88d":"code","4f20f8c1":"markdown","a42b06de":"markdown","12d6d73f":"markdown","7e7f2b2c":"markdown","9a66876a":"markdown","0f44a771":"markdown","8d0d114d":"markdown","aa7f5343":"markdown","69c26a07":"markdown","994fdff7":"markdown","ee33009b":"markdown","10d9364f":"markdown","4f933429":"markdown","239fa333":"markdown","35a6dd00":"markdown","d60086f6":"markdown","37d36d76":"markdown","5c482d74":"markdown","61517386":"markdown","67db0536":"markdown","80292b03":"markdown","d0bdec88":"markdown","daad9fd2":"markdown","92ac581f":"markdown"},"source":{"4397e913":"# import libraries \nimport pandas as pd # Import Pandas for data manipulation using dataframes\nimport numpy as np # Import Numpy for data statistical analysis \nimport matplotlib.pyplot as plt # Import matplotlib for data visualisation\nimport seaborn as sns\nimport random\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","65585136":"# dataframes creation for both training and testing datasets \nfashion_train_df = pd.read_csv('..\/input\/fashion-mnist_train.csv',sep=',')\nfashion_test_df = pd.read_csv('..\/input\/fashion-mnist_test.csv', sep = ',')","30d59bd9":"# Let's view the head of the training dataset\n# 784 indicates 28x28 pixels and 1 coloumn for the label\n\nfashion_train_df.head()","6818df7b":"# Let's view the last elements in the training dataset\nfashion_train_df.tail()","b83e73ac":"#Let's check the number of rows and number of columns\nfashion_train_df.shape","4d158485":"fashion_train_df.isnull().any().describe()","73a39dab":"fashion_test_df.isnull().any().describe()","17583797":"# Create training and testing arrays before fitting into CNN model\ntraining = np.array(fashion_train_df, dtype = 'float32')\ntesting = np.array(fashion_test_df, dtype='float32')","d522865d":"training.shape","80fda03d":"testing.shape","a2597ce1":"# Let's view some images!\ni = random.randint(1,60000) # select any random index from 1 to 60,000\nplt.imshow( training[i,1:].reshape((28,28)) ) # reshape and plot the image\n\nplt.imshow( training[i,1:].reshape((28,28)) , cmap = 'gray') # reshape and plot the image\n\n\n# Remember the 10 classes decoding is as follows:\n# 0 => T-shirt\/top\n# 1 => Trouser\n# 2 => Pullover\n# 3 => Dress\n# 4 => Coat\n# 5 => Sandal\n# 6 => Shirt\n# 7 => Sneaker\n# 8 => Bag\n# 9 => Ankle boot","8040f94a":"label = training[i,0]\nlabel","b4bf931a":"# Let's view more images in a grid format\n# Define the dimensions of the plot grid \nW_grid = 15\nL_grid = 15\n\n# fig, axes = plt.subplots(L_grid, W_grid)\n# subplot return the figure object and axes object\n# we can use the axes object to plot specific figures at various locations\n\nfig, axes = plt.subplots(L_grid, W_grid, figsize = (17,17))\n\naxes = axes.ravel() # flaten the 15 x 15 matrix into 225 array\n\nn_training = len(training) # get the length of the training dataset\n\n# Select a random number from 0 to n_training\nfor i in np.arange(0, W_grid * L_grid): # create evenly spaces variables \n\n    # Select a random number\n    index = np.random.randint(0, n_training)\n    # read and display an image with the selected index    \n    axes[i].imshow( training[index,1:].reshape((28,28)) )\n    axes[i].set_title(training[index,0], fontsize = 8)\n    axes[i].axis('off')\n\nplt.subplots_adjust(hspace=0.4)\n\n# Remember the 10 classes decoding is as follows:\n# 0 => T-shirt\/top\n# 1 => Trouser\n# 2 => Pullover\n# 3 => Dress\n# 4 => Coat\n# 5 => Sandal\n# 6 => Shirt\n# 7 => Sneaker\n# 8 => Bag\n# 9 => Ankle boot\n        ","586490f9":"# Prepare the training and testing dataset \nX_train = training[:,1:]\/255\ny_train = training[:,0]\n\nX_test = testing[:,1:]\/255\ny_test = testing[:,0]","268b90ad":"from sklearn.model_selection import train_test_split\n\nX_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size = 0.1, random_state = 12345)","91359583":"X_train.shape","74fa630f":"y_train.shape","1d8c859c":"# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\nX_train = X_train.reshape(X_train.shape[0], *(28, 28, 1))\nX_test = X_test.reshape(X_test.shape[0], *(28, 28, 1))\nX_validate = X_validate.reshape(X_validate.shape[0], *(28, 28, 1))","0c196a06":"X_train.shape","0e9b20b8":"X_test.shape","6d4f8e2c":"X_validate.shape","ab3a06f9":"# Import train_test_split from scikit library\n# Import Keras\nimport pandas as pd # Import Pandas for data manipulation using dataframes\nimport numpy as np # Import Numpy for data statistical analysis \nimport matplotlib.pyplot as plt # Import matplotlib for data visualisation\nimport seaborn as sns\nimport random\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\n# Import train_test_split from scikit library\n# Import Keras\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\nfrom keras.optimizers import Adam\nfrom keras.callbacks import TensorBoard\n","6e57cdfa":"# my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\ncnn_model = Sequential()\n\n# Try 32 fliters first then 64\ncnn_model.add(Conv2D(32,(5, 5), input_shape = (28,28,1), activation='relu'))\ncnn_model.add(Conv2D(32,(5, 5), activation='relu'))\ncnn_model.add(MaxPooling2D(pool_size = (2, 2)))\n\ncnn_model.add(Dropout(0.25))\n\ncnn_model.add(Conv2D(64,(3, 3), activation='relu'))\ncnn_model.add(Conv2D(64,(3, 3), activation='relu'))\ncnn_model.add(MaxPooling2D(pool_size = (2, 2)))\n\ncnn_model.add(Dropout(0.25))\n# cnn_model.add(Conv2D(32,3, 3, activation='relu'))\n# cnn_model.add(MaxPooling2D(pool_size = (2, 2)))\n\ncnn_model.add(Flatten())\ncnn_model.add(Dense(activation = 'relu', units=32))\ncnn_model.add(Dense(activation = 'softmax', units=10))\ncnn_model.summary()\n","3b6a9102":"cnn_model.compile(loss ='sparse_categorical_crossentropy', optimizer=Adam(lr=0.001),metrics =['accuracy'])","4cc45234":"# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","51fc3070":"epochs = 50\nbatch_size=64\nhistory = cnn_model.fit(X_train,\n                        y_train,\n                        batch_size = batch_size,\n                        epochs = epochs,\n                        verbose = 2,\n                        validation_data = (X_validate, y_validate), callbacks=[learning_rate_reduction])\n","da8901c6":"evaluation = cnn_model.evaluate(X_test, y_test)\nprint('Test Accuracy : {:.3f}'.format(evaluation[1]))","709fb9f9":"# get the predictions for the test data\npredicted_classes = cnn_model.predict_classes(X_test)\n\n","8fb12dab":"L = 5\nW = 5\nfig, axes = plt.subplots(L, W, figsize = (12,12))\naxes = axes.ravel() # \n\nfor i in np.arange(0, L * W):  \n    axes[i].imshow(X_test[i].reshape(28,28))\n    axes[i].set_title(\"Prediction Class = {:0.1f}\\n True Class = {:0.1f}\".format(predicted_classes[i], y_test[i]))\n    axes[i].axis('off')\n\nplt.subplots_adjust(wspace=0.5)\n","fccb5ba9":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, predicted_classes)\nplt.figure(figsize = (14,10))\nsns.heatmap(cm, annot=True)\n# Sum the diagonal element to get the total true correct values","8f9591bf":"from sklearn.metrics import classification_report\n\nnum_classes = 10\ntarget_names = [\"Class {}\".format(i) for i in range(num_classes)]\n\nprint(classification_report(y_test, predicted_classes, target_names = target_names))","8653e88d":"def plot_accuracy_and_loss(history):\n    hist = history.history\n    acc = hist['acc']\n    val_acc = hist['val_acc']\n    loss = hist['loss']\n    val_loss = hist['val_loss']\n    epochs = range(len(acc))\n    f, ax = plt.subplots(1,2, figsize=(14,6))\n    ax[0].plot(epochs, acc, 'g', label='Training accuracy')\n    ax[0].plot(epochs, val_acc, 'r', label='Validation accuracy')\n    ax[0].set_title('Training and validation accuracy')\n    ax[0].legend()\n    ax[1].plot(epochs, loss, 'g', label='Training loss')\n    ax[1].plot(epochs, val_loss, 'r', label='Validation loss')\n    ax[1].set_title('Training and validation loss')\n    ax[1].legend()\n    plt.show()\nplot_accuracy_and_loss(history)","4f20f8c1":"> **Please Upvote I like my kernel, your upvotes will motivate me a lot.**","a42b06de":"# Splitting Training and Validation Set\n\nI choosed to split the train set in two parts : a small fraction (10%) became the validation set which the model is evaluated and the rest (90%) is used to train the model and also choosed some randomness.","12d6d73f":"# PROBLEM STATEMENT AND BUSINESS CASE","7e7f2b2c":"I choosed batch size to be **64**\n\nBatch size is mainly a constraint on your own computer. The larger the batch size, the more data your chunking into your memory that your model will train on. The small the batch size, the less data, but your computation will be slower.\n\nIt's a tradeoff between speed and memory.","9a66876a":"# STEP #1: IMPORTING LIBRARIES AND  DATA","0f44a771":"**Why is this of interest for the scientific community? **\n\nThe original MNIST dataset contains a lot of handwritten digits. People from AI\/ML\/Data Science community love this dataset and use it as a benchmark to validate their algorithms. In fact, MNIST is often the first dataset they would try on. \u201cIf it doesn\u2019t work on MNIST, it won\u2019t work at all\u201d, they said. \u201cWell, if it does work on MNIST, it may still fail on others.\u201d\n\nFashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset to benchmark machine learning algorithms, as it shares the same image size and the structure of training and testing splits.","8d0d114d":"# Conclusion\nIt looks like diversity of the similar patterns present on multiple classes effect the performance of the classifier although CNN is a robust architechture. A jacket, a shirt, and a long-sleeve blouse has similar patterns: long sleeves (or not!), buttons (or not!), and so on.\nFurther we can improve the model by decreasing batch size to train the model more efficiently and increase the epochs.\nGave a try and comment me your result.","aa7f5343":"We can get a better sense for one of these examples by visualising the image and looking at the label.","69c26a07":"Lets view some more sample images with label in grid format to get the better understanding of the data","994fdff7":"# STEP #5: EVALUATING THE MODEL","ee33009b":"# Annealing : Learning Rate Reduction\nIn order to make the optimizer converge faster and closest to the global minimum of the loss function, i used an annealing method of the learning rate (LR).\n\nThe LR is the step by which the optimizer walks through the 'loss landscape'. The higher LR, the bigger are the steps and the quicker is the convergence. However the sampling is very poor with an high LR and the optimizer could probably fall into a local minima.\n\nIts better to have a decreasing learning rate during the training to reach efficiently the global minimum of the loss function.\n\nTo keep the advantage of the fast computation time with a high LR, i decreased the LR dynamically every X steps (epochs) depending if it is necessary (when accuracy is not improved).\n\nWith the ReduceLROnPlateau function from Keras.callbacks, i choose to reduce the LR by half if the accuracy is not improved after 3 epochs.","10d9364f":"# Convolutional Neural Network\n\n**Define Model**\n\nI used the Keras Sequential API, where you have just to add one layer at a time, starting from the input.\n\nThe first is the convolutional (Conv2D) layer. It is like a set of learnable filters. I choosed to set 32 filters for the two firsts conv2D layers and 64 filters for the two last ones. Each filter transforms a part of the image (defined by the kernel size) using the kernel filter. The kernel filter matrix is applied on the whole image. Filters can be seen as a transformation of the image.\n\nThe CNN can isolate features that are useful everywhere from these transformed images (feature maps).\n\nThe second important layer in CNN is the pooling (MaxPool2D) layer. This layer simply acts as a downsampling filter. It looks at the 2 neighboring pixels and picks the maximal value. These are used to reduce computational cost, and to some extent also reduce overfitting. We have to choose the pooling size (i.e the area size pooled each time) more the pooling dimension is high, more the downsampling is important.\n\nCombining convolutional and pooling layers, CNN are able to combine local features and learn more global features of the image.\n\nDropout is a regularization method, where a proportion of nodes in the layer are randomly ignored (setting their wieghts to zero) for each training sample. This drops randomly a propotion of the network and forces the network to learn features in a distributed way. This technique also improves generalization and reduces the overfitting.\n\n'relu' is the rectifier (activation function max(0,x). The rectifier activation function is used to add non linearity to the network.\n\nThe Flatten layer is use to convert the final feature maps into a one single 1D vector. This flattening step is needed so that you can make use of fully connected layers after some convolutional\/maxpool layers. It combines all the found local features of the previous convolutional layers.\n\nIn the end i used the features in two fully-connected (Dense) layers which is just artificial an neural networks (ANN) classifier. In the last layer(Dense(10,activation=\"softmax\")) the net outputs distribution of probability of each class.","4f933429":"# STEP #4: TRAINING THE MODEL","239fa333":"# Checking for Null and Missing Values","35a6dd00":"# STEP #3: Visualization & Preprocessing of Data ","d60086f6":"It's apparent that our model is underperforming for class 6 in terms of precision,recall and f1 score. For class 0, classifier is slightly lacking precision whereas it is slightly lacking precision for class 4.","37d36d76":"# What is Keras?","5c482d74":"We perform a grayscale normalization to reduce the effect of illumination's differences.\n\nMoreover the CNN converg faster on [0,1] data than on [0 to 255].","61517386":"Now let's see some sample prediction","67db0536":"# Step #2: Loading data","80292b03":"Fashion training set consists of 70,000 images divided into 60,000 training and 10,000 testing samples. Dataset sample consists of 28x28 grayscale image, associated with a label from 10 classes. \n\nThe 10 classes are as follows:  <br>\n\n**0 => T-shirt\/top<br>\n1 => Trouser<br>\n2 => Pullover<br>\n3 => Dress<br>\n4 => Coat<br>\n5 => Sandal<br>\n6 => Shirt<br>\n7 => Sneaker<br>\n8 => Bag<br>\n9 => Ankle boot **\n\nEach image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. \n","d0bdec88":"# Optimizer\nI choosed Adam optimizer.\n\nAdam is a popular algorithm in the field of deep learning because it achieves good results fast.\n\nAdam as combines the advantages of two other extensions of stochastic gradient descent. Specifically:\n\nAdaptive Gradient Algorithm (AdaGrad) that maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems).\n\nRoot Mean Square Propagation (RMSProp) that also maintains per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing). This means the algorithm does well on online and non-stationary problems (e.g. noisy). Adam realizes the benefits of both AdaGrad and RMSProp.\n\nInstead of adapting the parameter learning rates based on the average first moment (the mean) as in RMSProp, Adam also makes use of the average of the second moments of the gradients (the uncentered variance).\n\nSpecifically, the algorithm calculates an exponential moving average of the gradient and the squared gradient, and the parameters beta1 and beta2 control the decay rates of these moving averages.\n\nAdam is a popular algorithm in the field of deep learning because it achieves good results fast.","daad9fd2":"# About Fashion MNIST Dataset\nFashion-MNIST is a dataset of Zalando\u2019s article images consisting of a training set of 60,000 examples and a test set of 10,000 examples. Each example is a 28\u00d728 grayscale image, associated with a label from 10 classes. Fashion-MNIST is intended to serve as a direct drop-in replacement of the original MNIST dataset for benchmarking machine learning algorithms.\n<img src=\"https:\/\/s3-eu-central-1.amazonaws.com\/zalando-wp-zalando-research-production\/2017\/08\/fashion-mnist-sprite.png\">","92ac581f":"Keras is a minimalist Python library for deep learning that can run on top of Theano or TensorFlow.\n\nIt was developed to make implementing deep learning models as fast and easy as possible for research and development.\n\nIt runs on Python 2.7 or 3.5 and can seamlessly execute on GPUs and CPUs given the underlying frameworks. It is released under the permissive MIT license.\n\nKeras was developed and maintained by Fran\u00e7ois Chollet, a Google engineer using four guiding principles:\n\n**1. Modularity:**  A model can be understood as a sequence or a graph alone. All the concerns of a deep learning model are discrete components that can be combined in arbitrary ways.<br>\n**2. Minimalism:** The library provides just enough to achieve an outcome, no frills and maximizing readability.<br>\n**3. Extensibility:** New components are intentionally easy to add and use within the framework, intended for researchers to trial and explore new ideas.<br>\n**4. Python:** No separate model files with custom file formats. Everything is native Python."}}