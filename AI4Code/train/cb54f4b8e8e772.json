{"cell_type":{"cab67302":"code","f9284bd3":"code","654caa44":"code","bd855363":"code","5afea943":"code","28185b06":"code","0c76133a":"code","5a8933d9":"code","d4faa95b":"code","089325b4":"code","1e8288ad":"code","bcef602a":"code","86259c31":"code","ea7bb22c":"code","4cebdee1":"code","80c83f46":"code","03f273ea":"code","edaeaaf1":"markdown","fb7cdec9":"markdown","056a1c01":"markdown","439fdfa1":"markdown","a0872259":"markdown","0069e923":"markdown","a9f7abb5":"markdown"},"source":{"cab67302":"import numpy as np\nimport pandas\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.preprocessing import LabelEncoder","f9284bd3":"np.random.seed(31)\ndata = pandas.read_csv('..\/input\/titanic\/train.csv')\n\n#It's going to be easier dealing with the data if the target is at the last column of the dataframe\nsurvived = data[\"Survived\"]\ndata.drop(columns=[\"Survived\"], inplace=True)\ndata[\"Survived\"] = survived\ndel survived\n\n#I'm not a big fan of names that aren't explicit, neither should you. \ndata = data.rename(columns={\"SibSp\" : \"NumSiblingSpouse\", \"Parch\" : \"NumParentChildren\"})\ndata[\"Embarked\"].replace({\"S\": \"Southampton\", \"C\" : \"Cherbourg\", \"Q\" : \"Queenstown\"}, inplace=True)","654caa44":"data_observed = data.copy()\ndata_observed[\"Embarked\"].fillna(\"Unavailable\", inplace=True)\ndata_observed[\"EmbarkedEncoded\"] = LabelEncoder().fit_transform(data_observed[\"Embarked\"].values)\ndata_observed[\"SexEncoded\"] = LabelEncoder().fit_transform(data_observed[\"Sex\"].values)","bd855363":"data_observed.head(5)","5afea943":"print(\"Survived values :\\n\" + str(data_observed[\"Survived\"].value_counts()) + \"\\n---------------------------\")\nprint(\"NumParentChildren values :\\n\" + str(data_observed[\"NumParentChildren\"].value_counts()) + \"\\n---------------------------\")\nprint(\"Embarked values :\\n\" + str(data_observed[\"Embarked\"].value_counts()) + \"\\n---------------------------\")\nprint(\"NumSiblingSpouse values :\\n\" + str(data_observed[\"NumSiblingSpouse\"].value_counts()) + \"\\n---------------------------\")\nprint(\"Pclass values :\\n\" + str(data_observed[\"Pclass\"].value_counts()) + \"\\n---------------------------\")\nprint(\"Sex values :\\n\" + str(data_observed[\"SexEncoded\"].value_counts()) + \"\\n---------------------------\")","28185b06":"plt.plot(range(0,len(data_observed)), data_observed[\"Fare\"].sort_values())\nplt.title(\"Sorted 'Fare' feature values\")\nplt.ylabel(\"Price\")\nplt.show()\n\nplt.plot(range(0,len(data_observed)), data_observed[\"Age\"].sort_values())\nplt.title(\"Sorted 'Age' feature values\")\nplt.ylabel(\"Age\")\nplt.show()","0c76133a":"data_observed.info()\ndata_observed.corr()","5a8933d9":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, roc_curve\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\n\ndata_copy = data_observed.copy()\nsurvived = data_copy[\"Survived\"]\ndata_copy.drop(columns=[\"PassengerId\", \"Survived\", \"Name\", \"Sex\", \"Ticket\", \"Cabin\", \"Embarked\"], inplace=True)\ndata_copy[\"Age\"].fillna(data_copy[\"Age\"].mean(), inplace=True)\nX_train, X_val, y_train, y_val = train_test_split(data_copy, survived, test_size=0.4, random_state=31)\n\ndt = DecisionTreeClassifier(max_depth=4, criterion=\"entropy\", random_state=31)\ndt.fit(X_train, y_train)\nprint(\"Accuracy score : \" + str(dt.score(X_val, y_val)))","d4faa95b":"data_observed[\"Title\"] = data_observed[\"Name\"].str.extract(r'\\,\\s(.*)?\\w*\\.').replace({'Ms' : 'Miss'})\ndata_observed[\"TitleEncoded\"] = LabelEncoder().fit_transform(data_observed[\"Title\"])","089325b4":"import pandas as pd\nfrom IPython.core.display import HTML\n\ndisplay(HTML(data_observed[[\"Ticket\", \"Survived\"]].sort_values(by=\"Survived\", ascending=False).to_html()))","1e8288ad":"from sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import FunctionTransformer, OrdinalEncoder\nimport re\n\nextract_title = lambda x : x.str.extract(r'\\,\\s(.*)?\\w*\\.').replace({'Ms' : 'Miss'})\n\ndata_preparation_pipeline = make_column_transformer(\n    (Pipeline([\n        (\"transform_name\", FunctionTransformer(extract_title, validate=False)), \n        (\"encode_title\", OrdinalEncoder())]), \"Name\"),\n    (SimpleImputer(strategy=\"mean\"), [\"Age\"]),\n    (Pipeline([\n        (\"impute_embarked\", SimpleImputer(strategy=\"constant\", fill_value=\"Unavailable\")), \n        (\"encode_embarked\", OrdinalEncoder())]), [\"Embarked\"]),\n    (OrdinalEncoder(), [\"Sex\"]),\n    (\"drop\", [\"PassengerId\", \"Ticket\", \"Cabin\"])\n, remainder=\"passthrough\")","bcef602a":"survived = data[\"Survived\"]\ntraining_pipeline = Pipeline([(\"data_preparation\",data_preparation_pipeline), (\"drop_survived\",make_column_transformer((\"drop\", [-1]), remainder=\"passthrough\"))])\n\nprepared_x = training_pipeline.fit_transform(data)\n\nX_train, X_val, y_train, y_val = train_test_split(prepared_x, survived, test_size=0.4, random_state=31)","86259c31":"accuracy_by_depth = [0,0]\nf1_by_depth = [0,0]\nfor i in range(2, len(data.columns)):\n    dt = DecisionTreeClassifier(max_depth=i, min_samples_split=0.01, criterion=\"entropy\", random_state=31)\n    \n    dt.fit(X_train, y_train)\n    accuracy_by_depth.append(dt.score(X_val, y_val))\n    f1_by_depth.append(f1_score(y_val, dt.predict(X_val)))\n    \nplt.plot(range(len(accuracy_by_depth)), accuracy_by_depth)\nplt.plot(range(len(accuracy_by_depth)), f1_by_depth)\nplt.xlim(2, len(accuracy_by_depth))\nplt.legend([\"Accuracy\", \"F1 Score\"])\nplt.ylabel(\"Score\")\nplt.xlabel(\"Tree depth\")\nplt.ylim(0.6, 1)\nplt.title(\"Scores by DecisionTree depth\")\nplt.show()","ea7bb22c":"from matplotlib.pyplot import figure\nfigure(num=None, figsize=(8, 6), dpi=150, facecolor='w', edgecolor='k')\n\nfeature_names = [\"Title\", \"Age\", \"Embarked\", \"Sex\", \"PClass\", \"# sibling\/spouse\", \"# children\/parent\", \"Fare\"]\n\ndecision_tree = DecisionTreeClassifier(max_depth=3, criterion=\"entropy\", random_state=31)\ndecision_tree.fit(X_train, y_train)\nplt.show(plot_tree(decision_tree, feature_names=feature_names, class_names=[\"Died\", \"Survived\"]))\n\nprint(dict(zip(feature_names, decision_tree.feature_importances_)))\nprint(\"Accuracy : \" + str(decision_tree.score(X_val, y_val)))\nprint(\"F1 Score : \" + str(f1_score(y_val, decision_tree.predict(X_val))))","4cebdee1":"import joblib\n\ndef extract_title(x):\n    return x.str.extract(r'\\,\\s(.*)?\\w*\\.').replace(\"Ms\", \"Miss\")\n\ndef map_title(x):\n    x[~x.isin([\"Mr\", \"Miss\", \"Mrs\"])] = \"Other\"\n    return x\n\ndata_preparation_pipeline = make_column_transformer(\n    (Pipeline([\n        (\"transform_name\",  FunctionTransformer(extract_title, validate=False)), \n        (\"map_name\",  FunctionTransformer(map_title, validate=False)),\n        (\"encode_title\", OrdinalEncoder())]), \"Name\"),\n    (SimpleImputer(strategy=\"most_frequent\"), [\"Fare\"]),\n    (SimpleImputer(strategy=\"most_frequent\"), [\"NumSiblingSpouse\"]),\n    (SimpleImputer(strategy=\"most_frequent\"), [\"Pclass\"]),\n    (OrdinalEncoder(), [\"Sex\"]))\n\nsurvived = data[\"Survived\"]\n\n#drop the Survived column\ndata_without_survived = data.drop(columns=[\"Survived\"])\n\nprediction_pipeline = Pipeline([(\"preparation\", data_preparation_pipeline), (\"prediction\", DecisionTreeClassifier(max_depth=3, criterion=\"entropy\", random_state=31))])\nprediction_pipeline.fit(data_without_survived, survived)\n\njoblib.dump(prediction_pipeline, \"prediction_pipeline.joblib\")","80c83f46":"test_data = pandas.read_csv('..\/input\/titanic\/test.csv')\ntest_data = test_data.rename(columns={\"SibSp\" : \"NumSiblingSpouse\", \"Parch\" : \"NumParentChildren\"})\n\npassengerIds = test_data[\"PassengerId\"]\npredictions = prediction_pipeline.predict(test_data)\n\nmy_submission = pd.DataFrame({'PassengerId': passengerIds, 'Survived': predictions})\n# you could use any filename. We choose submission here\nmy_submission.to_csv('submission.csv', index=False)","03f273ea":"pandas.DataFrame(data_preparation_pipeline.fit_transform(test_data), columns=[\"Title\",\"Sex\",\"Pclass\",\"Fare\", \"NumSiblingSpouse\"]).info()","edaeaaf1":"### Pipeline creation and model tuning\n\nWhat we've done so far is use some pretty simple feature engineering to clean our data and discretize it.\n\nIt might be profitable to go deeper with more complicated features and analysis, but the truth is that we have very little data with very little features. And considering our data is mostly categorical, a model like a decision tree takes good advantage of this and yields robust and explanable results. \n\nWe'll build our pipeline next. The `Pipeline` class of sklearn has some nice properties. First of all it makes it possible to define a series of operation on our data in a sequential and pretty readable way. But most importantly, it gives us the possibility to fit our data to our training set and transform\/estimate on our testing set, using the fitted values from the training set, which is an important property of a transformation pipeline.","fb7cdec9":"Looking at our plot above, we see that our decision tree performs particularly well with a depth of *three*. This surely means some of our features aren't significant and they create noise in our classifier. Looking at the decision tree plot and the `feature_importances_`, we can see that the Title, Embarked and number of children\/parents are not significant. We'll fix our pipeline to discard those features and keep only the important ones.\n\nFinally, we'll train our DecisionTree on the whole data and we'll output predictions for the test sample.","056a1c01":"### Data observation\n\nBefore throwing data at a model, we'll look at how our features are correlated, if we have missing data and potentially imbalance in class.\n\nI'll need to spoil you here, the features 'Name', 'Sex', 'Ticket', 'Cabin and 'Embarked' won't show up when we're looking at the correlation values because they are of the `object` type. Some of these values are really easy to discretize ('Sex' and 'Embarked' because they pretty much already are). We'll do it in a \"dumb\" fashion right now, just to analyze the data. Later we'll have a real pipeline where new informatin would be fitted accordingly.\n\n","439fdfa1":"A couple things to note : \n- There are 758 entries (duh?)\n- The \"Cabin\" feature has 891-204=687 null values, that's a lot. Considering this, we should consider dropping this feature from the data we'll use to build our model. An unverified intuition is also that the cabin number would somehow be correlated to the Fare and the PClass features, which aren't as sparse. Dropping this column probably won't hurt considering this.\n- The \"Age\" column also has a bit of NaN values. We'll have to get back to this.\n- Looking at the \"Ticket\" column, it's a bit of a mess. It would be worth it to take a deeper look at it to see if there's a pattern that could be correlated to our 'Survived' target.\n- We can see that the \"Fare\" feature looks a lot like an exponential function, meaning that there are few people who paid a very high fare for their ticket. If we were to bin these values in a categorical variable, we should take this in consideration.\n- We can see sort of the same thing when it comes to 'Age' feature.\n- Looking at the survival rate, we have our first baseline. The `ZeroR` classifier, which consists of always predicting \"Did not survive\". This classifier offers an accuracy of 0.608. Obviously, this isn't a classifier we'd use in a real environment.\n\nCorrelation-wise :\n- We really want to look at the correlation with the 'Survived' feature. However, it's also important to look at the other ones, simply because if we have very  strongly correlated variables, it might not be useful to keep both of them as they wouldn't be very discriminating regarding our target 'Survived' feature.\n- What are our strongest (linear) correlations (I'll cheat a bit when it comes to the 'Survived' feature, because that's our target)?\n    - Corr(Sex, Survived)\n    - Corr(PClass, Survived)\n    - Corr(Fare, Survived)\n    - Corr(PClass, Age)\n    - Corr(PClass, Fare)\n    - Corr(Age, NumSiblingSpouse)\n    - Corr(NumSiblingSpouse, NumParentChildren)\n\nOur next objective is going to be to explore these correlations and try to figure out if we can expose more significant features from these. For example, knowing the number of sibling\/spouse and children\/parent, we might be able to figure out who's a single parent, who's a couple without children, with children, etc. Maybe these features are linearly correlated with our target.\n\nBut first, let's try something. It'll be interesting to see what's our baseline. Say we remove the `object` columns and train a simple `DecisionTree` on the dataset, what's going to be our score? We'll test it on our validation set!\n\n\\* You might think a `test_size=0.4` is big. What I noticed while testing is that when I had a small test size, the `random_state` had a **big** influence on the accuracy score (going from 0.72 to 0.89 accuracy score). By having a bigger test_size, we can have a higher confidence that our model would generalize properly.","a0872259":"So, Ticket is a \"no go\". The same will apply for \"Cabin\", as there's way too many missing values to take out anything out of it..\n","0069e923":"## Feature engineering\n\nSo, we've seen that 79.9% accuracy is a baseline we can reach by throwing barely cleaned up data to a simple classifier.\n\nSome features were not exploited before : Name, Ticket, Cabin. Can we maybe use something from these features to help us out? Turns out we can!\n\nI'm not the one who though about it, but someone noticed that in the `Name` feature it's possible to extract a `Title` feature, which may have an impact, so let's check that out.\n\nWe'll also look at the values of `Ticket` in relation to `Survived` to see if we can get something out of it!","a9f7abb5":"Alright, so by dropping \"complicated\" columns and encoding the simple values, the decision tree with a pretty shallow depth of 4 has an accuracy of 0.79; not bad at all."}}