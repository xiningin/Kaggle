{"cell_type":{"a1a0c023":"code","2f6e16a3":"code","bc8fa3aa":"code","3fbf5498":"code","60857800":"code","1ccf18fa":"code","a8733efb":"code","6aa08685":"code","e36e76e1":"code","10748eba":"code","c8cbcf25":"code","1ba7c4af":"code","5036073e":"code","f83f8c88":"code","d8073a25":"code","20cd1e92":"code","14bfdcdc":"code","b11d2325":"code","76519f2d":"code","14c30656":"code","09665789":"code","62aa10b4":"code","7009e432":"code","ecce02ac":"code","be1e54ff":"code","c6e3df43":"code","b643348e":"code","fa01833c":"code","b588da63":"code","c0fb521b":"code","8e5e9e0d":"code","c755e0cb":"code","6d8dab61":"code","41666521":"code","63f75fb5":"markdown","a3b792bb":"markdown","f42a5c8e":"markdown","513873cc":"markdown","a5bc8b0a":"markdown","70e51e94":"markdown","27715c3d":"markdown","6c5ebda0":"markdown","e4425f5c":"markdown","c8ca51ad":"markdown","d3c11e97":"markdown","f7dd2ef1":"markdown","85e55a6f":"markdown","dffa6266":"markdown","0f1829b8":"markdown","99983ee3":"markdown","42244516":"markdown","686fc326":"markdown","a2767d0c":"markdown","1603bde7":"markdown","9a87cfff":"markdown","f81d43c7":"markdown","d3fcf453":"markdown","a46608e4":"markdown","6b9f80d3":"markdown","c196c483":"markdown","f0a9110e":"markdown","12d2b690":"markdown","7f9fecfb":"markdown","d2725c30":"markdown","9fef3689":"markdown","e7f65c76":"markdown","dd20c8ad":"markdown","412a479f":"markdown","abafdd9f":"markdown","c389c601":"markdown","fecd5765":"markdown","223f944b":"markdown","e7ea3f56":"markdown","800010e0":"markdown","3e214cc7":"markdown","22a4b148":"markdown","798100de":"markdown","8ca7ec30":"markdown","15191dd0":"markdown","c2697521":"markdown","594925c5":"markdown","f3bd4b28":"markdown","deee70b2":"markdown","a8518450":"markdown","06d2e82f":"markdown","364c2a42":"markdown","618a3bea":"markdown","c0a45765":"markdown","c3858f27":"markdown","8d1f0069":"markdown","6fbb582b":"markdown","e1a9d207":"markdown","9b0e21a4":"markdown","4618ce04":"markdown","768c08f4":"markdown","f5b53193":"markdown"},"source":{"a1a0c023":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport seaborn as sns\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.cluster import AgglomerativeClustering, KMeans","2f6e16a3":"data = pd.read_csv('..\/input\/iris.csv')\ndata.head(10) # show the first 10 lines","bc8fa3aa":"data.describe()","3fbf5498":"categorical_summaries = [data[c].value_counts() for c in data.columns if data[c].dtype == 'object']\n\nfor i in categorical_summaries:\n    display(pd.DataFrame(i))","60857800":"def numerical_distribution (colnumber, plot_type='histogram', data=data):\n    \"\"\"\n    function for plotting histogram of the column number corresponding to the numerical variable selected\n    colnumber is the column number corresponding to the numerical variable selected,\n    a boxplot of the numerical variable depending on the categorical variable\n    \"\"\"\n    \n    if plot_type=='histogram':\n        \n        # x label for the histogram\n        plt.xlabel(data.columns[colnumber])\n    \n        # y label for the histogram (-1 points to the last column of the set)\n        plt.ylabel('Frequency')\n    \n        # title for the histogram\n        plt.title(data.columns[colnumber] + ' distribution')\n    \n        # histogram\n        data.iloc[:,colnumber].plot.hist()\n        \n    elif plot_type=='boxplot':\n    \n        # setting type of plot\n        sns.set(style=\"ticks\", color_codes=True)\n    \n        # setting what values we plot\n        sns.catplot(x=data.columns[-1], y=data.columns[colnumber], kind='box',data=data);\n    \n        # title\n        plt.title(data.columns[colnumber] + ' distribution depending on ' + data.columns[-1])","1ccf18fa":"numerical_distribution(0,'histogram')","a8733efb":"numerical_distribution(0,'boxplot')","6aa08685":"numerical_distribution(1,'histogram')","e36e76e1":"numerical_distribution(1,'boxplot')","10748eba":"numerical_distribution(2,'histogram')","c8cbcf25":"numerical_distribution(2,'boxplot')","1ba7c4af":"numerical_distribution(3,'histogram')","5036073e":"numerical_distribution(3,'boxplot')","f83f8c88":"g = sns.PairGrid(data, hue=\"species\")\ng.map_diag(plt.hist, alpha=0.5)\ng.map_upper(plt.scatter, alpha=0.5, marker='x')\ng.map_lower(sns.kdeplot, shade=True, shade_lowest=False, alpha=0.4)\ng.add_legend()","d8073a25":"X = data.iloc[:,:-1]\ny = data.iloc[:,-1]","20cd1e92":"# splitting compulsory\n# X_train, X_test, y_train, y_test = train_test_split(data.iloc[:,:-1], data.iloc[:,-1], test_size=0.2)\n# print(X_train.shape, y_train.shape)\n# print(X_test.shape, y_test.shape)","14bfdcdc":"wcss = []\nfor i in range(1, 10):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_)","b11d2325":"plt.plot(range(1, 10), wcss)\nplt.title('Elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS') # within cluster sum of squares\nplt.show()","76519f2d":"# creating the k-means classifier with n_clusters being k, the number of clusters for the model\nkmeans = KMeans(n_clusters = 3 , \n                init = 'k-means++', \n                max_iter = 300, \n                n_init = 10, \n                random_state = 0)\n# fitting the model\ny_kmeans = kmeans.fit_predict(X)","14c30656":"def plot_data_cluster_output(method, y_clusters, features,\n                             output, k_clusters, x_axis, y_axis):\n    \"\"\"\n    function plotting the data labeled by clusters and by output\n    \"\"\"\n    # plotting the points labeled by the cluster\n    # legend\n    fig, (ax1,ax2) = plt.subplots(1, 2)\n    fig.set_size_inches(18, 8)\n    colors = iter(cm.rainbow(np.linspace(0, 1, k_clusters)))\n    \n    for i in range(0,k_clusters):\n        ax1.scatter(features.iloc[y_clusters == i, x_axis], features.iloc[y_clusters == i, y_axis], s = 30, color= next(colors), label = 'cluster ' + str(i))\n    ax1.legend()\n    \n    # plotting the centroid\n    if method=='kmeans':\n        # plotting the centroids of the clusters\n        ax1.scatter(kmeans.cluster_centers_[:, x_axis], kmeans.cluster_centers_[:,y_axis], s = 30, c = 'yellow', label = 'centroids')\n    \n    # title\n    ax1.title.set_text(method + \" clustering on \" + features.columns[x_axis] + \" and \" + features.columns[y_axis])\n    # x label\n    ax1.set_xlabel(features.columns[x_axis])\n    # y label\n    ax1.set_ylabel(features.columns[y_axis])\n    \n    # preparing legend to get the same colour if the number of clusters is equal to the number of labels\n    if len(output.unique())==k_clusters:\n        # initialize labels vector\n        labels=[None]*3\n        \n        # reordering labels\n        for i in range(0,k_clusters):\n            index_label=np.where(np.amax(pd.crosstab(y_kmeans, output).iloc[:,i].values)==pd.crosstab(y_kmeans, output).iloc[:,i].values)[0]\n            labels[index_label[0]]=output.unique()[i]\n    else:\n        labels=output.unique()\n    \n    plt.figure(2)\n    colors = iter(cm.rainbow(np.linspace(0, 1, len(output.unique()))))\n    \n    # plotting the points labeled by the label\n    for i in labels:\n        ax2.scatter(features.iloc[output.values == i, x_axis], features.iloc[output.values == i, y_axis], s = 30, color = next(colors), label = i)   \n    ax2.legend()\n    # title\n    ax2.title.set_text(\"Data labeled on species depending on \" + features.columns[x_axis] + \" and \" + features.columns[y_axis])\n    # x label\n    ax2.set_xlabel(features.columns[x_axis])\n    # y label\n    ax2.set_ylabel(features.columns[y_axis])","09665789":"plot_data_cluster_output('kmeans', y_kmeans, X, y, 3, 0, 1)","62aa10b4":"plot_data_cluster_output('kmeans', y_kmeans, X, y, 3, 0, 2)","7009e432":"pd.crosstab(y_kmeans, y)","ecce02ac":"print(\"Percentage of non-corresponding clusters:\" + str(16*100\/150) +\"%\")","be1e54ff":"def silhouette(y_clusters, k_clusters, features=X):\n    \"\"\"\n    Function plotting the clustering configuration silhouette and average value\n    arguments: y_cluster= clustering output\n               k_clusters= number of clusters\n               features=features used for clustering\n    \"\"\"\n    fig, ax1 = plt.subplots(1, 1)\n    fig.set_size_inches(10, 10)\n\n    # The silhouette coefficient can range from -1, 1\n    ax1.set_xlim([-1, 1])\n\n    # The (k_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to separate them clearly.\n    ax1.set_ylim([0, len(features) + (k_clusters + 1) * 10])\n\n    # silhouette_score gives the average value for all the samples.\n    silhouette_avg = silhouette_score(features, y_clusters)\n    print(\"For n_clusters =\", k_clusters ,\n          \"the average silhouette_score is:\", silhouette_avg)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(features, y_clusters)\n\n    y_lower = 10\n    for i in range(k_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[y_clusters == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) \/ k_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                            0, ith_cluster_silhouette_values,\n                            facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next cluster plot\n        y_lower = y_upper + 10 \n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # no yaxis labels \/ ticks\n    ax1.set_xticks([-1,-0.5,-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    plt.suptitle((\"Silhouette analysis for clustering on data \"\n                      \"with n_clusters = %d\" % k_clusters),\n                     fontsize=14, fontweight='bold')\n\n    plt.show()","c6e3df43":"silhouette(y_kmeans,3,X)","b643348e":"# creating the k-means classifier with n_clusters being k, the number of clusters for the model\nk_5_means = KMeans(n_clusters = 5 , \n                init = 'k-means++', \n                max_iter = 300, \n                n_init = 10, \n                random_state = 0)\n# fitting the model\ny_5_kmeans = k_5_means.fit_predict(X)\nsilhouette(y_5_kmeans,5,X)","fa01833c":"# generate the linkage matrix\nZ = linkage(X, 'ward')\n\n# we set the cut off, the level where we decide to cluster the data, we select the distance level where the line does not intercept any node.\n# try different levels to understand this concept\nmax_d = 8               \n\nplt.figure(figsize=(20, 10))\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('Samples')\nplt.ylabel('Distance')\ndendrogram(\n    Z,\n    truncate_mode='lastp', \n    p=150,                  # indicates how many final leafs you want to have ( ideally the number of samples)\n    leaf_rotation=90.,      # rotates the x axis labels\n    leaf_font_size=8.,      # font size for the x axis labels\n)\nplt.axhline(y=max_d, c='k')\nplt.show()","b588da63":"# Creating the hierarchical clustering model with n_clusters declaring the number of clusters\ncluster = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')  \ny_hier=cluster.fit_predict(X)","c0fb521b":"silhouette(y_hier,3,X)","8e5e9e0d":"plot_data_cluster_output('HCA',y_hier,X, y, 3, 0, 1)","c755e0cb":"plot_data_cluster_output('HCA',y_hier,X, y, 3, 0, 2)","6d8dab61":"pd.crosstab(y_hier, y)","41666521":"print(\"percentage of non-corresponding clusters:\" + str(16*100\/150) +\"%\")","63f75fb5":"You'll notice that the algorithm relies on the user stating how many clusters should be found (e.g k=3 in the example above). How should you select this number?","a3b792bb":"### Sepal width","f42a5c8e":"# 0. Library imports","513873cc":"### iv. Evaluating the clusters","a5bc8b0a":"**Based on the dendogram, it appears that a good number of clusters to pick is 3.**","70e51e94":"## c. Extracting the features\nWe select the 4 features used for clustering (`sepal_length`, `sepal_width`, `petal_length`, `petal_width`).\n\n**NOTE 1: WE EXTRACT THE SPECIES LABELS BUT WE NEVER USE IT FOR TRAINING, WE ARE DOING UNSUPERVISED LEARNING!**","27715c3d":"We define the following function to visualize and compare **any clustering** against the **labeled data** with the species on different features.","6c5ebda0":"First, we import all of the libraries we will use throughout this notebook:","e4425f5c":"A **dendogram** is a diagram that shows the relation between observations. The **height** at which two observations are joined is **related to the distance** between the observations. In the example below, we can see that E and F are most similar, as the height of the link that joins them is the smallest. \n\nWhich cluster an observation is finally placed into is determined by **drawing a horizontal line through the dendrogram**. Observations that are joined together below the line are in clusters. In the example below, we have two clusters. One cluster combines A and B, and a second cluster combining C, D, E, and F.\n\n<img src=\"dendogram.png\" alt=\"Hierarchical Clustering Process\" class=\"center\">\n","c8ca51ad":"To summarise, our data has 150 observation and 5 variables:\n- **4 numerical variables**: `sepal_length`, `sepal_width`, `petal_length` and `petal_width`\n- **1 categorical variable** (this is the label or target variable): \n    - `species` with 3 levels: `setosa`, `versicolor`, `virginica`","d3c11e97":"These results match our earlier observations during the visualization. We saw that setosa flowers could be easily separated from virginica and versicolor flowers but was harder to discriminate the virginica and the versicolor. \nOur k-means algorithm experienced the same behavior. It could easily cluster the setosa flowers and had a bit more of a problems separating the virginica and versicolor:\n- **k-means cluster 0** corresponds almost exactly to the virginica flowers. Only 2\/38 are classified into cluster 0 but correspond to versicolor flowers.\n- **k-means cluster 1** corresponds exactly to the setosa flowers.\n- **k-means cluster 2** corresponds mainly to the versicolor flowers. Only 14\/62 are classified into cluster 2 but correspond to virginica flowers.","f7dd2ef1":"### Petal length","85e55a6f":"An additional method for evaluating the quality of our clusters is examining **silhouette** values. The silhouette method measures consistency within clusters of data \u2013 the technique provides a succinct graphical representation of how well each data point has been classified.\n\nThe silhouette value is a measure of **cohesion (how similar a data point is to points in the same cluster, versus other clusters)**. The values range from \u22121 to +1, where a **high positive values** indicates that the data point matches its own cluster well and matches other clusters poorly. If most observations have a high value, then the clustering model is appropriate. If many points have a low or negative value, then the clustering configuration may have too many or too few clusters.\n\nWe define the following function to plot the silhouette and print its value, so we can double check that the number of clusters is appropriate:","dffa6266":"**All coefficient values are positive and the average is close to +1. We can continue with this number of clusters.**\n\n**Hereafter we try a k-means model with another number of clusters to observe a silhouette meaning a bad choice of clusters ( feel free to change the number of clusters)**","0f1829b8":"**The quality of the clustering is similar to the k-means clustering result.** We can again compute the cross table:","99983ee3":"**K-means clustering** is a popular clustering algorithm that aims to **partition n observations into k clusters** by iteratively estimating **cluster centroids**. At each step, each data point is placed into the cluster with the **nearest centroid** (cluster center), and all cluster centroids are recalculated based on which data points have been placed in the cluster. \n\nThis iterative process continues until the clusters **converge** (i.e. stop changing much). This can be visualised below, with the algorithm set to find k=3 clusters and the clusters are coloured <font color=\"red\">**RED (centroid=+)**<\/font>, <font color=\"gold\">**YELLOW (centroid=X)**<\/font>, and <font color=\"blue\">**BLEU (centroid=O)**<\/font>:\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/ea\/K-means_convergence.gif\" alt=\"Clustering Process\" class=\"center\">\n\n","42244516":"Let's view the distributions of all of our numerical variables:","686fc326":"- The `setosa` flowers are characterized by the lowest `petal_length.`\n- The `versicolor` flowers are characterized by medium `petal_length.`\n- The `virginica` flowers are characterized by the highest `petal_length.`\n\nThe `petal_length` feature values seem to be quite different depending on `species`.","a2767d0c":"We can then visualize the score associated with each value of k (number of clusters):","1603bde7":"We print the **data summary** to observe the range of the numerical values and see if we have any defective cells.\nWe can use the `describe()` command to observe the counts, ranges, means, and quartiles of the numerical values. Using this information, we can also quickly see if we have any defective cells (like missing values):","9a87cfff":"Now, we can apply clustering methods to identify natural clusters within our dataset. We will be using the following 2 popular clustering algorithms:\n1. **K-means clustering**\n2. **Hierarchical clustering**\n\nThese will be described in turn.","f81d43c7":"We can remake the plots but with `petal_length` as the y axis:","d3fcf453":"### Petal width","a46608e4":"### ii. Creating and training the model","6b9f80d3":"### i. Selecting the number of clusters","c196c483":"### Sepal Length","f0a9110e":"### ii. Creating the model","12d2b690":"In code, this looks like:","7f9fecfb":"- The `setosa` flowers are characterized by the lowest `petal_width`.\n- The `virginica` flowers are characterized by the highest `petal_width`.\n- The `versicolor` flowers are characterized by medium `petal_width`.\n\nThe `petal_width` feature values seem to be quite different depending on `species`.","d2725c30":"### iii. Visualization of the resulting clusters\n\nWe can again use the **`plot_data_cluster_output()` function** to visualize and compare the **clustering results** against the **labeled data** from the original `species` column:","9fef3689":"The WCSS scores will always go down at the number of clusters k increases. But there will often be a **diminishing returns** effect happening, where adding more clusters doesn't change WCSS much. We don't want to choose a value of k that is \"too big\" for the data. \n\nIf the line chart is an arm, then the **\u201celbow\u201d (the point of inflection on the curve)** is a good indication that the underlying model fits best at that point. The elbow corresponds to a good trade-off point where we have increased k to the point where we see the largest drops in WCSS, but k isn't so large that we've hit diminishing returns. \n\n**In our case, it seems that 3 is the ideal number of clusters.** A case could also be made for k=2 being optimal.\n\nNow that we know how many clusters we want to find in the data, we can pass this as a parameter to the model and fit it to the dataset.","e7f65c76":"`petal_length` has a pseudonormal distribution","dd20c8ad":"**NOTE 2: In unsupervised learning, usually there is no training or test data split because the accuracy cannot be evaluated against a label. In this case, we do have a label but very few observations (150) so we avoid splitting the data.**","412a479f":"Below, the left figure is the the **data points coloured by the predictions of the clustering model**. \n\nThe right figure is the **\"true\" labels from the original column `species`** in the dataset.","abafdd9f":"We check that the number of clusters is reasonable with the silhouette.","c389c601":"`sepal_width` has a normal distribution.","fecd5765":"# 4. Summary","223f944b":"**Almost all coefficient values are positive and the average is close to +1. We can continue with this number of clusters.**","e7ea3f56":"### iii. Examining the clustering results","800010e0":"# 2. Data visualization and preparation\nWe visualize all the variables and do some transformations if necessary.","3e214cc7":"**Recall that a clustering problem is UNSUPERVISED**. That means we will **NOT** be using the `species` column when making our clusters \u2013 the clustering algorithm will only use the features and not the label during training.\n","22a4b148":"**In order to decide the number of clusters we build and examine the dendogram:**","798100de":"`petal_width` has a pseudonormal distribution.","8ca7ec30":"**We observe negative values for some observations in cluster 2 and 4. It means that the observations match poorly its cluster.**","15191dd0":"- The `setosa` flowers are characterized by the lowest `sepal_length.`\n- The `versicolor` flowers are characterized by medium `sepal_length.`\n- The `virginica` flowers are characterized by the highest `sepal_length`.\n\nThe `sepal_length` feature values seem to be quite different depending on `species`.","c2697521":"## a. K-means clustering\n","594925c5":"`sepal_length` has a normal distribution.","f3bd4b28":"In data mining and statistics, **hierarchical clustering** (also called hierarchical cluster analysis or HCA) is a method of cluster analysis which seeks to build a **hierarchy of clusters**. Here we will implement an agglomerative HCA. This is a \"bottom-up\" approach: **each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.** This means that the user doesn't have to specify the number of clusters ahead of time.\n\nThe process can be visualised on this example of clustering 7 datapoints:\n\n<img src=\"https:\/\/dashee87.github.io\/images\/hierarch.gif\" alt=\"Hierarchical Clustering Process\" class=\"center\">\n\n","deee70b2":"## b. Hierarchical clustering","a8518450":"# 1. Data collection\n\nNext, we load the iris dataset. Our goal is to cluster the flowers by their descriptive variables (sepal\/petal features), and see if our clusters match the `species` label. \n\nLet's see what the dataset looks like:","06d2e82f":"## a. Visualizing Distributions","364c2a42":"And that's all we need to do to fit the model! We can apply the clustering model to our test set and observe the predictions, namely which cluster the algorithm would put each test datapoint into based on its features:","618a3bea":"# 3. Model selection and training","c0a45765":"We can see that the three species do have differences in `sepal_width` distributions, but that these are not as extreme as for other features. ","c3858f27":"# Clustering Notebook\n\n## Introduction to clustering\n\n**This notebook runs through the clustering process using the iris dataset as an example**. The iris dataset is a popular dataset commonly used to illustrate machine learning principles. \n\n**Clustering** is an unsupervised learning method. An **unsupervised learning method** is a method in which **we do not have any labels (output variables)** but we are still interested in learning about the **patterns** present in the data. \n\nThe **goal of clustering is dividing the data points into a number of groups** such that data points in the same groups are more similar to each other, and more dissimilar to data points in other groups.\n\nClustering is used in very different real world problems.\n\n### Spam filter\n\nSpam emails are annoying and can be dangerous for your personal data. To avoid getting and opening these dangerous emails in your inbox, companies use clustering algorithms. The purpose of the algorithm is to flag an email as spam correctly or not. How does your mailbox decide if an incoming mail is a spam or not?\n\n<img src=\"anti-spam-filtering-techniques-copy.jpg\" alt=\"Spam filter\" class=\"center\" width=\"500\">\n\nThe way that it works is by looking at the different sections of the email (header, sender, and content). The data is then grouped together. \nThese groups can then be classified to identify which are spam.\n\n### Fake news\n\nFake news is being created and spread at a rapid rate due to technology innovations such as social media. It is important to automatically identify these type of news so the information can be filtered.\nClustering can identify which news are fake by taking the content of news articles, the corpus, examining the words used and then clustering them. These clusters are what helps the algorithm determine which pieces are genuine and which are fake news.\n\n### Customized playlists\n\nPlatforms like Spotify create different playlists based on the similarity between songs based on musical features ( Acousticness ,Liveness, Popularity, Valence, Energy, Loudness, Danceability). They extract a pattern from the available variables to do clustering so that we get homogeneous playlists. Each time a new song is uploaded it can be clustered and therefore, added to some playlist to allow users to discover new songs based on their taste.\n\n<img src=\"spotify.png\" alt=\"Spotify\" class=\"center\" width=\"500\">\n\n# Iris clustering\n\nIn our dataset, we have a sample of flowers from three species:\n\n![Flowers](https:\/\/i.imgur.com\/PQqYGaW.png)\n\nFor each flower, we have the measurements`sepal_length`,`sepal_width`,`petal_length`, and `petal_width` as well as the label `species`. These quantities are measurements of the following parts of the plants: \n\n\n<img src=\"https:\/\/ars.els-cdn.com\/content\/image\/3-s2.0-B9780128147610000034-f03-01-9780128147610.jpg\" alt=\"Measurements\" class=\"center\">\n\nOur goal in this notebook is to **cluster the flowers based on the descriptive variables (features)** without looking at the `species` label.\n\nThe process of clustering can be summarized with this diagram:\n\n<img src=\"clustering_process.svg\" alt=\"Clustering Process\" class=\"center\" width=\"700\">\n\nThe rest of this notebook takes you through this process step-by-step. \n\n**Note**: This notebook can be used with other datasets if you place the label in the last column.","8d1f0069":"### i. Selecting the number of clusters","6fbb582b":"**We observe similarity between our cluster predictions and the true `species` labels.** We can print a cross table to summarise the similarity:","e1a9d207":"We applied a **K-means** clustering and a **hierarchical clustering** approach on the iris dataset. As unsupervised algorithms don't learn from labels, the biggest challenge is to determine a suitable number of clusters so that we can arrive at a meaningful clustering result. We examined several methods to find good values for the number of clusters **(elbow method, silhouette, dendogram)**.\n\nThe results for K-means and hierarchical clustering were similar and high-quality (the cluster labels closely corresponded to the true `species` labels). Note that this dataset is idealized and \"easy\", and real-world clustering tends to be messier. \n\nIn this notebook, we examined the iris data as an unsupervised problem and trained clustering algorithms to identify sub-groups in the dataset. However, if you have a dataset with a known label column of interest (like `species`), it is also worth considering using a supervised classification algorithm to predict the label column. ","9b0e21a4":"One established method for selecting the optimal number of clusters k is the **elbow method**. \n\nThis method works by fitting the model with a range of values for k clusters and observing a score for each value of k called the **within-cluster sum of squared distances (WCSS)**. This is simply the sum of squared distances between each point and the centroid. Lower values are better, because it means that points tend to be closer to the centroid. \n\nFor each model we compute the **sum of squared distances** of samples to their closest cluster center:\n\n![fig](WCSS.png)","4618ce04":"## b. Visualizing Relationships Between Pairs of Variables\n\nNext, we can make use of a **subplot grid** to examine **pairwise relationships** in a dataset. The grid maps each variable in a dataset onto a column and row in a grid of multiple axes. \n\nValues of the `species` variable are visualised using colour, by setting `hue=\"species\"`, which plots each point in the grid with a colour linked with each possible value for `species`:","768c08f4":"The results are similar to the ones from our k-means algorithm:\n\n- **hierarchical cluster 1** corresponds exactly to the **setosa** flowers. \n- **hierarchical cluster 2** corresponds almost exactly to the **virginica** flowers. Only, 1\/36 are classified in the cluster 2 but correspond to versicolor flowers.\n- **hierarchical cluster 0** corresponds **mainly to the versicolor** flowers. Only, 15\/64 are classified in the cluster 2 but correspond to virginica flowers.","f5b53193":"As we could have guessed from the previous observations, the `setosa` points appear easily separable from the other `species`. However, the `versicolor` and `virginica` are harder to separate although we can observe some patterns as before (e.g. `versicolor` tends to have smaller `petal_width`)."}}