{"cell_type":{"bfcc4eb2":"code","1c52a53c":"code","21ccde47":"code","295b29ea":"code","e6660f4b":"code","bcd5b4a5":"code","6e7f62c3":"code","fa1559d6":"code","4744a013":"code","dcc6518f":"code","930f421f":"code","0752e2cc":"code","76bc1952":"code","c2e3372d":"code","48fe20e5":"code","f285b554":"code","be4550c6":"code","31646c0d":"code","722ce9f6":"markdown","087461f6":"markdown","f00f8566":"markdown","929bcc5a":"markdown","448a5f56":"markdown"},"source":{"bfcc4eb2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport statsmodels.api as sm\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib.pyplot as plt\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1c52a53c":"dataset=pd.read_csv(\"..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","21ccde47":"dataset.quality.unique()\ndataset.isnull().sum()","295b29ea":"dataset.describe()\n\nsns.distplot(dataset['density'])\n\n\n\nsns.distplot(dataset['sulphates']) #positive skewed\nq=dataset['sulphates'].quantile(0.99)\ndataset_a=dataset[dataset['sulphates']<q]\nsns.distplot(dataset_a[\"sulphates\"])\n\nsns.distplot(dataset['pH'])#fine,no outliers\n\n\nsns.distplot(dataset['alcohol'])\nq=dataset_a['alcohol'].quantile(0.99)\ndataset_b=dataset_a[dataset['alcohol']<q]\nsns.distplot(dataset_b[\"alcohol\"])\n\nsns.distplot(dataset['quality'])\n\n\nplt.scatter(dataset['sulphates'],dataset['quality'])\nplt.scatter(dataset['alcohol'],dataset['quality'])","e6660f4b":"sns.distplot(dataset['fixed acidity'])\nq=dataset_b['fixed acidity'].quantile(0.99)\ndataset_c=dataset_b[dataset['alcohol']<q]\nsns.distplot(dataset_c[\"alcohol\"])\n\n\nsns.distplot(dataset['volatile acidity'])\nq=dataset_c['volatile acidity'].quantile(0.99)\ndataset_d=dataset_c[dataset['volatile acidity']<q]\nsns.distplot(dataset_d[\"volatile acidity\"])\n\n\n\nsns.distplot(dataset['citric acid'])\nq=dataset_d['citric acid'].quantile(0.99)\ndataset_e=dataset_d[dataset['citric acid']<q]\nsns.distplot(dataset_e[\"citric acid\"])\n\nsns.distplot(dataset['residual sugar'])\nq=dataset_e['residual sugar'].quantile(0.95)\ndataset_f=dataset_e[dataset['residual sugar']<q]\nsns.distplot(dataset_f[\"residual sugar\"])\n\nsns.distplot(dataset['chlorides'])\nq=dataset_f['chlorides'].quantile(0.95)\ndataset_g=dataset_f[dataset['chlorides']<q]\nsns.distplot(dataset_g[\"chlorides\"])\n\n\nsns.distplot(dataset['free sulfur dioxide'])\nq=dataset_g['free sulfur dioxide'].quantile(0.95)\ndataset_h=dataset_g[dataset['free sulfur dioxide']<q]\nsns.distplot(dataset_h[\"free sulfur dioxide\"])\n\nsns.distplot(dataset['total sulfur dioxide'])\nq=dataset_h['total sulfur dioxide'].quantile(0.95)\ndataset_i=dataset_h[dataset['total sulfur dioxide']<q]\nsns.distplot(dataset_i[\"total sulfur dioxide\"])\n\n\n","bcd5b4a5":"#check if the training data is balanced\n\nsns.countplot(dataset_i['quality'])\n\n#try converting the dependent variable into 2 classes ,1 for high quality and 0 for low quality\n#new_data=dataset_i\ndataset_i=dataset_i.reset_index()\ndataset_i=dataset_i.drop('index',axis=1)\n#Converting \"Quality variable into 1 or 0 based on quality\"\nt=0\nfor values in dataset_i['quality']:\n    if values>5:\n         dataset_i['quality'][t]=1\n       # print(new_data[X])\n         t=t+1\n    elif values>=0 and values<=5:\n         dataset_i['quality'][t]=0\n         t=t+1\n    #elif values>=6 and values<=10:\n         #new_data['quality'][t]=2\n        # t=t+1\nsns.countplot(dataset_i['quality'])","6e7f62c3":"dataset_i.corr()\n#dropping columns coz either they are correlated with eachother or are less correlated with output variable\n\ndataset_i=dataset_i.drop('fixed acidity',axis=1)\ndataset_i=dataset_i.drop('free sulfur dioxide',axis=1)\ndataset_i=dataset_i.drop('residual sugar',axis=1)\n","fa1559d6":"Y=dataset_i['quality'].values\nY=pd.DataFrame(Y)\nY=Y.reset_index()\nY=Y.drop('index',axis=1)\n","4744a013":"X=dataset_i.drop('quality',axis=1)\n\nX=X.reset_index()\nX=X.drop('index',axis=1)","dcc6518f":"#dividing into test and train data(Optional coz u can directly test on the given data X and Y)\nfrom sklearn.model_selection import train_test_split\n\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=1)","930f421f":"X_train=X_train.reset_index()\nX_train=X_train.drop('index',axis=1)\n\nY_train=Y_train.reset_index()\nY_train=Y_train.drop('index',axis=1)","0752e2cc":"from sklearn.preprocessing import StandardScaler\n\nscaler=StandardScaler()\nX=scaler.fit_transform(X)\n\nX=pd.DataFrame(X)","76bc1952":"from sklearn.linear_model import LogisticRegression\nlogreg=LogisticRegression()\nlogreg.fit(X,Y)\n\nlog_pred=logreg.predict(X)\n","c2e3372d":"from sklearn.metrics import accuracy_score\naccuracy_score(Y,log_pred)","48fe20e5":"from sklearn.ensemble import RandomForestClassifier \nrfc = RandomForestClassifier()\nrfc.fit(X, Y)\n#Prediction\npredictions = rfc.predict(X)\npredictions=pd.DataFrame(predictions)\n","f285b554":"from sklearn.metrics import accuracy_score\naccuracy_score(Y, predictions)","be4550c6":"from sklearn.metrics import confusion_matrix\n\nconfusion_matrix(Y,predictions)","31646c0d":"from sklearn.metrics import precision_score, recall_score\nprecision_score(Y,predictions,average='micro')\n","722ce9f6":"Logistic regression using sklearn","087461f6":"Classification Using Random Forest Classifier","f00f8566":"Seperating Independent and Dependent variables","929bcc5a":"Getting Accuracy Score","448a5f56":"Correcting the indices of the X_train n Y_train"}}