{"cell_type":{"08898db6":"code","aad67ac4":"code","704e7a18":"code","9a965924":"code","ec4598d4":"code","d4ec0842":"code","3244f4b0":"code","44cb1461":"code","d2541f2c":"code","121843ec":"code","2cbf2e86":"code","4a54c58c":"code","35fd32df":"code","a7807724":"code","8a7002b8":"code","65dffb31":"code","7bdfe567":"code","e0eda650":"code","013b8098":"code","825a84d7":"code","3f5de303":"code","e13f3f3a":"code","5e053a3b":"code","add71b55":"code","7c0d9050":"code","ab3d3a44":"code","3bcfda23":"code","1360e757":"code","e6eb2754":"code","50ac3b1f":"code","b9bcde2a":"code","3cdee207":"code","6ced2cdf":"code","c74594f3":"code","5fa9f923":"code","872aa08c":"markdown","32b4f8a4":"markdown","6f112e3a":"markdown","3c13371d":"markdown","50052cee":"markdown","d63b050f":"markdown","b7de7dd5":"markdown","79f5a2f1":"markdown","65d32cd3":"markdown","ce473053":"markdown","fef247d0":"markdown","4de5b7da":"markdown","e797ab7a":"markdown","c44a9013":"markdown","3b7d0d76":"markdown","0c4513af":"markdown","32d6719a":"markdown","b5f1bf89":"markdown","174675f5":"markdown","1d14e119":"markdown","0efce626":"markdown"},"source":{"08898db6":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","aad67ac4":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntrain.head()","704e7a18":"test = pd.read_csv('..\/input\/titanic\/test.csv')\ntest.head()","9a965924":"train.shape # shape of the training data","ec4598d4":"test.shape # shape of the test data","d4ec0842":"train.isnull().sum() # check the count of missing values in the train data","3244f4b0":"train.isnull().sum() # check the count of missing values in the test data","44cb1461":"import missingno as msno \nmsno.bar(train, color = 'steelblue')                  #Plot bar for train data","d2541f2c":"# Check the column which has more the 30% missing observation in training data\ncol = train.isnull().sum() > int(0.30 * train.shape[0]) \ncol","121843ec":"# Check the column which has more the 30% missing observation in test data\ncol = train.isnull().sum() > int(0.30 * test.shape[0]) \ncol","2cbf2e86":"col_drop = ['Cabin']\n# Drop the columns 'cabin' from training and test data as it contains more than 30% missing values\ntrain.drop(col_drop, axis = 1, inplace = True)\ntest.drop(col_drop, axis = 1, inplace = True)","4a54c58c":"sns.set_style('dark')\nsns.distplot(train['Age'].dropna(),hist = True, kde = True, bins = 50)","35fd32df":"df = pd.DataFrame(train['Age'])\ndf.describe()","a7807724":"skew = (3 *(29.699 - 28))\/14.5264  # Skewness = 3(mean - median)\/SD\nskew","8a7002b8":"train[\"Age\"].fillna(train[\"Age\"].median(), inplace = True)\ntest[\"Age\"].fillna(test[\"Age\"].median(), inplace = True)\ntrain[\"Embarked\"].fillna(train[\"Embarked\"].mode()[0], inplace = True)  # 0 index to get mode the column\ntest[\"Embarked\"].fillna(test[\"Embarked\"].mode(), inplace = True)      # Data is categorical\ntest[\"Fare\"].fillna(test[\"Fare\"].mean(), inplace = True) # Only few values are missing which can be replaced with mean\n","65dffb31":"sns.heatmap(train.isnull(),cmap = 'magma' )","7bdfe567":"sns.set_style('darkgrid')\nsns.countplot(x = 'Survived',data = train, palette = 'deep')","e0eda650":"sns.set_style('darkgrid')\nsns.countplot(x = 'Survived', hue = 'Sex' ,data = train, palette = 'deep')","013b8098":"sns.set_style('darkgrid')\nsns.countplot(x = 'Survived', hue = 'Pclass' ,data = train, palette = 'cubehelix')","825a84d7":"cor = train.corr()\nsns.heatmap(cor, annot = True)","3f5de303":"train.info()","e13f3f3a":"from sklearn.preprocessing import LabelEncoder","5e053a3b":"le = LabelEncoder()","add71b55":"train['Sex1'] = le.fit_transform(train.Sex)","7c0d9050":"train['Embarked1'] = le.fit_transform(train.Embarked)","ab3d3a44":"train.drop(['Name','Sex','Ticket','Embarked'],axis = 1,inplace = True)\n","3bcfda23":"train.rename(columns={\"Sex1\": \"Sex\", \"Embarked1\": \"Embarked\"},inplace = True)","1360e757":"train.head(5)","e6eb2754":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","50ac3b1f":"train.drop('Survived',axis=1).head()","b9bcde2a":"X_train, X_test, y_train, y_test = train_test_split(train.drop('Survived', axis = 1), train['Survived'], test_size = 0.30, random_state = 100)","3cdee207":"logisticModel = LogisticRegression()\nlogisticModel.fit(X_train,y_train)\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)  #To suppress warning ","6ced2cdf":"predict = logisticModel.predict(X_test)","c74594f3":"confusionmatrix = confusion_matrix(y_test,predict)\nconfusionmatrix","5fa9f923":"accuracy=accuracy_score(y_test,predict)\naccuracy","872aa08c":"### *Splitting the data into train and test*","32b4f8a4":"Exploratory Data Analysis is a way to understand the characteristics of dataset, identify the pattern, relationship between the variables and visualize. ","6f112e3a":"The skewness is positive. Hence we fill missing values of Age with median","3c13371d":"### Data Prepocessing ","50052cee":"### Imorting Necessary Libraries","d63b050f":"**Titanic: Machine Learning from Disaster**","b7de7dd5":"**Dealing with missing values**\n1. Plot the train data in order to chech the columns that contains missing value\n3. Replacing the missing values with the appropriate method ","79f5a2f1":"**2. Correlation matrix to depict relationship between the factors**","65d32cd3":"**1. Prediction By Using Logistic Regression**\n\nIn order to predict the survival which is in the form of binary regression, Logistic Regression is the suitable technique to predict the Survival in the sinking of Titanic","ce473053":"**Importing Necessary Libraries**\n","fef247d0":"The sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n","4de5b7da":"![img.jpg](attachment:img.jpg)","e797ab7a":"## Conclusion\nThe Accuracy prediction is 78.7%. \n\nThe best suitable technique to fit a survival prediction model is Linear classification such as logistic regression, Support vector machine, Naive bayes etc. The accuracy can be further improved by using appropriate feature engineering method. The relevant features in the model plays an important role to improve the accuracy of the model. \n\nThis was my first kaggle competition. Thank you for reading my notebook.\nI really enjoyed this competition  and looking forward to climb up the leaderboard and showcase my statistical knowledge in different competitions. \n\n### Please upvote ","c44a9013":"**Reading the dataset**","3b7d0d76":"**1. Countplots of features included in the dataset**","0c4513af":"It is observed that Sex, Embarked and Ticket are object dtpe. So we convert these variables into numeric by using LabelEncoder.","32d6719a":"First we need to convert the categorical variables into numeric type","b5f1bf89":"**Table of Content**\n\n1. Importing libraries \n2. Reading dataset\n3. Data Preprocessing\n      * Finding columns with missing values\n      * Relacing missing value with suitable statistical method\n4. Exploratory Data Analysis \n      * Countplots\n      * Correlation matrix\n5. Survival Prediction\n6. Conclusion ","174675f5":"**Exploratory Data Analysis**","1d14e119":"From above heatmap, it is clear that there is no missing value in the dataset. We can perform EDA now.","0efce626":"**Survival prediction by using various classification techniques**"}}