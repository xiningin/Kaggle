{"cell_type":{"18745f3a":"code","f8111b07":"code","93dcd5a1":"code","8a3fd4f3":"code","830e5f21":"code","0f3a0f62":"code","353bc9fb":"code","b4eeb7d2":"code","a04668e0":"code","50cc1937":"code","01b4b3d6":"code","370c208d":"code","ea7eb345":"code","5f5edac2":"code","0b75d443":"code","059f70fb":"code","b6e5b3d3":"code","c660c48a":"code","74c82ba4":"code","344ff1b6":"code","c7c20b0e":"code","8074a62c":"code","d46b7de6":"code","e8fa2c93":"code","23d8ba16":"code","969c1aa9":"code","2ae1ee2a":"code","a90da75a":"code","d56d7c34":"code","4587cfa5":"code","5a7b2d61":"code","d45227d1":"code","2accca72":"code","9be35dde":"code","daffe3e7":"code","6a383231":"code","8f440c9f":"code","64861e29":"code","22fa0765":"code","5bb325e2":"code","fef961c8":"code","be8110cd":"code","0d499ea2":"code","4e03ef6b":"code","194794f0":"code","ce083fe8":"code","75f7c8c1":"code","a2fc831c":"code","7cc9d4a8":"code","cc8cb90c":"code","a700595b":"code","949b2f46":"code","7b3abffb":"code","63336181":"code","8932319a":"code","84497d79":"code","67779d39":"code","f5acfebc":"markdown","cc86835c":"markdown","b0bbb36b":"markdown","dc6eb4a3":"markdown","8c9c2f3d":"markdown","dbbed582":"markdown","169a5581":"markdown","04db035e":"markdown","5cfc9c9a":"markdown","b1db74ce":"markdown","20b04c8c":"markdown","5480bfdf":"markdown","ffff59fe":"markdown","5db47239":"markdown","b293bcf6":"markdown","63dee43f":"markdown","8dd461a0":"markdown","517ab220":"markdown","a8ad10b3":"markdown","36892282":"markdown","54d3f8c6":"markdown","93ba333b":"markdown","af976577":"markdown","e20d0dba":"markdown","fb6a4697":"markdown","2d1c0ed6":"markdown","64d5a2e1":"markdown","60511814":"markdown","07484c5a":"markdown","dc0bc0ba":"markdown","e9950f3c":"markdown","d8f921c1":"markdown","a54afdb9":"markdown","fce9c82b":"markdown","9ce6c19a":"markdown","1f320a44":"markdown","2f9184ad":"markdown","e23fc6ec":"markdown","b5362b6d":"markdown","8ffba18d":"markdown","a033cbd9":"markdown","772a8d2f":"markdown","635d1e20":"markdown","82d7a47e":"markdown","cc02695e":"markdown","0b32796d":"markdown","47d45006":"markdown","1e609dee":"markdown"},"source":{"18745f3a":"# for data manipulation\nimport pandas as pd\nimport numpy as np\n\n# for plotting\/visualising the distibution of data\nimport matplotlib.pyplot as plt \nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nfrom plotly import tools\n\nimport random\nimport re\n\n# for pre-processing of the data\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\n\nimport warnings","f8111b07":"# load the data\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")","93dcd5a1":"train_df.head()","8a3fd4f3":"# Get the distribution of the data\ntrain_df.describe(include='all')","830e5f21":"train_df['Survived'].astype(int).plot.hist();","0f3a0f62":"# Function to calculate missing values by column\ndef missing_values_table(df):\n        # Total missing values\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # Make a table with the results\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns","353bc9fb":"# Missing values statistics\nmissing_values = missing_values_table(train_df)\nmissing_values","b4eeb7d2":"train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace = True)","a04668e0":"# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n# Create a new feature Title, containing the titles of passenger names\ntrain_df['Title'] = train_df['Name'].apply(get_title)\ntest_df['Title'] = test_df['Name'].apply(get_title)","50cc1937":"train_df['Title'].value_counts()","01b4b3d6":"test_df['Title'].value_counts()","370c208d":"dict1 = {'Dr':'Others', 'Rev':'Others', 'Col':'Others', 'Mlle':'Others', 'Major':'Others', 'Capt':'Others', 'Ms':'Others', \n         'Don':'Others', 'Lady':'Others', 'Countess':'Others', 'Jonkheer':'Others', 'Mme':'Others', 'Sir':'Others'}\ntrain_df['Title'] = train_df['Title'].replace(dict1)\n\ndict2 = {'Col':'Others', 'Rev':'Others', 'Ms':'Others', 'Dona':'Others', 'Dr':'Others'}\ntest_df['Title'] = test_df['Title'].replace(dict2)","ea7eb345":"train_df['Title'].value_counts()","5f5edac2":"# Checking the incorrect entry when age is less than 13 for male and the title is Mr.\ndf = train_df.loc[train_df['Title']=='Mr']\ndf = df.loc[df['Age']<13]\ndf.head()","0b75d443":"# Correcting the entry\ntrain_df.loc[[731],['Title']] = 'Master'","059f70fb":"# Function to plot the classes of the variables\ndef random_color_generator(number_of_colors):\n    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n                 for i in range(number_of_colors)]\n    return color\n\ndef get_percent(df, temp_col, width=800, height=500):\n    cnt_srs = df[[temp_col, 'Survived']].groupby([temp_col], as_index=False).mean().sort_values(by=temp_col)\n\n    trace = go.Bar(\n        x = cnt_srs[temp_col].values[::-1],\n        y = cnt_srs['Survived'].values[::-1],\n        text = cnt_srs.values[::-1],\n        textposition = 'auto',\n        name = \"Percent\",\n        textfont = dict(\n            size=12,\n            color='rgb(0, 0, 0)'\n        ),\n        orientation = 'v',\n            marker = dict(\n                color = random_color_generator(100),\n                line=dict(color='rgb(8,48,107)',\n                  width=1.5,)\n            ),\n            opacity = 0.7,\n    )    \n    return trace\n\ndef get_count(df, temp_col, width=800, height=500):\n    cnt_srs = df[temp_col].value_counts().sort_index()\n\n    trace = go.Bar(\n        x = cnt_srs.index[::-1],\n        y = cnt_srs.values[::-1],\n        text = cnt_srs.values[::-1],\n        textposition = 'auto',\n        textfont = dict(\n            size=12,\n            color='rgb(0, 0, 0)'\n        ),\n        name = 'Count',\n        orientation = 'v',\n            marker = dict(\n                color = random_color_generator(100),\n                line=dict(color='rgb(8,48,107)',\n                  width=1.5,)\n            ),\n            opacity = 0.7,\n    )    \n    return trace\n\ndef plot_count_percent_for_object(df, temp_col, height=400):\n    trace1 = get_count(df, temp_col)\n    trace2 = get_percent(df, temp_col)\n\n    fig = tools.make_subplots(rows=1, cols=2, subplot_titles=('Count', 'Percent'), print_grid=False)\n    fig.append_trace(trace1, 1, 1)\n    fig.append_trace(trace2, 1, 2)\n\n    fig['layout']['yaxis1'].update(title='Count')\n    fig['layout']['yaxis2'].update(range=[0, 1], title='% Survived')\n    fig['layout'].update(title = temp_col, margin=dict(l=100), width=800, height=height, showlegend=False)\n\n    py.iplot(fig)","b6e5b3d3":"# observe the distribution of title\nwarnings.simplefilter('ignore')\ntemp_col = train_df.columns.values[12]\nplot_count_percent_for_object(train_df, temp_col)","c660c48a":"# observe the distribution of Sex\ntemp_col = train_df.columns.values[4]\nplot_count_percent_for_object(train_df, temp_col)","74c82ba4":"# Making a new variable\/feature 'family'\ntrain_df['family'] = train_df['SibSp'] + train_df['Parch'] + 1\ntrain_df.drop(['SibSp', 'Parch'], axis=1, inplace=True)\n\ntest_df['family'] = test_df['SibSp'] + test_df['Parch'] + 1\ntest_df.drop(['SibSp', 'Parch'], axis=1, inplace=True)","344ff1b6":"# observe the distribution of family\ntemp_col = train_df.columns.values[11]\nplot_count_percent_for_object(train_df, temp_col)","c7c20b0e":"# Making a new variable\/feature 'family_status' from the variable 'family' \ntrain_df['family_status'] = train_df['family']\ntest_df['family_status'] = test_df['family']\ndict2 = {1:'Alone', 2:'NotAlone', 3:'NotAlone', 4:'NotAlone', 5:'NotAlone', 6:'NotAlone', 7:'NotAlone', 8:'NotAlone', \n         9:'NotAlone', 10:'NotAlone', 11:'NotAlone'}\ntrain_df['family_status'] = train_df['family_status'].replace(dict2)\ntest_df['family_status'] = test_df['family_status'].replace(dict2)","8074a62c":"train_df['family_status'].dtype","d46b7de6":"# observe the distribution of family_status\ntemp_col = train_df.columns.values[12]\nplot_count_percent_for_object(train_df, temp_col)","e8fa2c93":"plt.figure(figsize = (10, 8))\n\ndf = train_df[['Survived', 'Age']]\ndf = df.dropna()\n\n# KDE plot of passengers who did not survive \nsns.kdeplot(df.loc[df['Survived'] == 0, 'Age'], label = 'survived == 0')\n\n# KDE plot of passengers who survived\nsns.kdeplot(df.loc[df['Survived'] == 1, 'Age'], label = 'survived == 1')\n\n# Labeling of plot\nplt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Age');","23d8ba16":"# dividing the age variable into different classes\ntrain_df['Age'] = train_df['Age'].fillna(200) # this is just indicating the missing values\ntrain_df['Age'] = pd.cut(train_df['Age'], bins=[0,12,40,80, 250], labels = ['Child', 'Young', 'Old', 'Missing'])","969c1aa9":"train_df['Age'] = train_df['Age'].astype('O')","2ae1ee2a":"temp_col = train_df.columns.values[5]\nplot_count_percent_for_object(train_df, temp_col)","a90da75a":"age_train = pd.read_csv('..\/input\/train.csv')\nage_test = pd.read_csv('..\/input\/test.csv')\ntrain_df['Age'] = age_train['Age']\ntest_df['Age'] = age_test['Age']\n\n# mean of the age variable\nage_avg_train = train_df['Age'].mean()\nage_avg_test = test_df['Age'].mean()\n# standard deviation of the age variable \nage_std_train = train_df['Age'].std()\nage_std_test = test_df['Age'].std()\n\nage_null_count_train = train_df['Age'].isnull().sum()\nage_null_count_test = test_df['Age'].isnull().sum()\n\n# list of the random age values to be filled based on the distribution of the original age variable  \nage_null_random_list_train = np.random.randint(age_avg_train - age_std_train, age_avg_train + age_std_train, size=age_null_count_train)\nage_null_random_list_test = np.random.randint(age_avg_test - age_std_test, age_avg_test + age_std_test, size=age_null_count_test)\n\ntrain_df['Age'][np.isnan(train_df['Age'])] = age_null_random_list_train\ntest_df['Age'][np.isnan(test_df['Age'])] = age_null_random_list_test","d56d7c34":"# dividing the age variable into different classes\ntrain_df['Age'] = pd.cut(train_df['Age'], bins=[0,12,40,80], labels = ['Child', 'Young', 'Old'])\ntest_df['Age'] = pd.cut(test_df['Age'], bins=[0,12,40,80], labels = ['Child', 'Young', 'Old'])","4587cfa5":"train_df['Age'] = train_df['Age'].astype('O')","5a7b2d61":"temp_col = train_df.columns.values[5]\nplot_count_percent_for_object(train_df, temp_col)","d45227d1":"train_df.head()","2accca72":"drop = ['Name', 'PassengerId', 'Ticket', 'Cabin']\ntrain_df = train_df.drop(drop, axis=1)\ntest_df = test_df.drop(drop, axis=1)","9be35dde":"# Create a label encoder object\nencoder = LabelEncoder()\nencoder_count = 0\n\n# Iterate through the columns\nfor col in train_df:\n    if train_df[col].dtype == 'object':\n        # If 2 unique classes\n        if len(list(train_df[col].unique())) <= 2:\n            encoder.fit(train_df[col])\n            train_df[col] = encoder.transform(train_df[col])\n            test_df[col] = encoder.transform(test_df[col])\n            # Keep track of how many columns were label encoded\n            encoder_count += 1\n            \nprint('%d columns were label encoded.' % encoder_count)","daffe3e7":"dict1 = {'Child':1, 'Young':2, 'Old':3}\ntrain_df['Age'] = train_df['Age'].replace(dict1)\ntest_df['Age'] = test_df['Age'].replace(dict1)","6a383231":"# one-hot encoding of categorical variables\ntrain_df = pd.get_dummies(train_df)\ntest_df = pd.get_dummies(test_df)","8f440c9f":"train_df.head()","64861e29":"test_df.head()","22fa0765":"train = train_df\ntest = test_df\n\nprint(train.shape)\nprint(test.shape)","5bb325e2":"y = train['Survived']\nx = train.drop('Survived', axis=1)","fef961c8":"from sklearn.model_selection import cross_validate, ShuffleSplit, GridSearchCV\nfrom sklearn.linear_model import LogisticRegressionCV, SGDClassifier\nfrom sklearn import ensemble, naive_bayes, svm, tree, discriminant_analysis, neighbors, feature_selection","be8110cd":"MLA = [    \n        # Generalized Linear Models\n        LogisticRegressionCV(),\n    \n        # SVM\n        svm.SVC(probability = True),\n        svm.LinearSVC(),\n    \n        # KNN\n        neighbors.KNeighborsClassifier(weights='distance'),\n    \n        #Discriminant Analysis\n        discriminant_analysis.LinearDiscriminantAnalysis(),\n        discriminant_analysis.QuadraticDiscriminantAnalysis(),\n     \n        # Naive Bayes\n        naive_bayes.BernoulliNB(),\n        naive_bayes.GaussianNB(),\n    \n        #Trees    \n        tree.DecisionTreeClassifier(),\n    \n        # Ensemble Methods\n        ensemble.AdaBoostClassifier(),\n        ensemble.BaggingClassifier(),\n        ensemble.ExtraTreesClassifier(),\n        ensemble.GradientBoostingClassifier(),\n        ensemble.RandomForestClassifier()\n     \n    ]\n\ncv_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .7, random_state = 0)\nMLA_columns = ['MLA Name','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean','MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\nrow_index = 0\nfor alg in MLA:\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    cv_results = cross_validate(alg, x,y, cv  = cv_split, return_train_score=True)\n    \n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n        \n    row_index+=1\n   \n\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n","0d499ea2":"MLA_compare","4e03ef6b":"# grid search for svm\nclassifier = svm.SVC()\nbase_results = cross_validate(classifier, x, y, cv  = cv_split, return_train_score=True)\nclassifier.fit(x, y)\n\nepoch=0\nfor train_score,test_score in zip(base_results['train_score'], base_results['test_score']):\n        epoch +=1       \n        print(\"epoch:\",epoch,\"train_score:\",train_score, \"test_score:\",test_score)\nprint('-'*10)\n\nprint('BEFORE Tuning Parameters: ', classifier.get_params())\nprint(\"BEFORE Tuning Training w\/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE Tuning Test w\/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint('-'*10)\n\nparam_grid = {'C':[0.5,1.0,2.0, 3.0],  # penalty parameter C of the error term\n              'kernel':['linear', 'rbf'], # specifies the kernel type to be used in the algorithm  \n              'gamma':[0.02, 0.08,0.2,1.0] # kernel coefficient for 'rbf'\n             }\n\n# Grid Search\ntune_model = GridSearchCV(svm.SVC(), param_grid=param_grid, scoring = 'accuracy', cv = cv_split, return_train_score=True)\ntune_model.fit(x, y)\n\nfor i in range(10):\n    print(\"epoch:\",i,\"train_score:\",tune_model.cv_results_['split'+str(i)+'_train_score'][tune_model.best_index_],\n    \"test_score:\",tune_model.cv_results_['split'+str(i)+'_test_score'][tune_model.best_index_])\n\nprint('-'*10)    \n\nprint('AFTER Tuning Parameters: ', tune_model.best_params_)\nprint(\"AFTER Tuning Training w\/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100))\nprint(\"AFTER Tuning Test w\/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint('-'*10)\n","194794f0":"# grid search for decision trees\ndtree = tree.DecisionTreeClassifier(random_state = 0)\nbase_results_dtree = cross_validate(dtree, x, y, cv  = cv_split, return_train_score=True)\ndtree.fit(x, y)\n\nepoch=0\nfor train_score,test_score in zip(base_results_dtree['train_score'], base_results_dtree['test_score']):\n        epoch +=1       \n        print(\"epoch:\",epoch,\"train_score:\",train_score, \"test_score:\",test_score)\nprint('-'*10)\n\nprint('BEFORE Tuning Parameters: ', dtree.get_params())\nprint(\"BEFORE Tuning Training w\/bin score mean: {:.2f}\". format(base_results_dtree['train_score'].mean()*100)) \nprint(\"BEFORE Tuning Test w\/bin score mean: {:.2f}\". format(base_results_dtree['test_score'].mean()*100))\nprint('-'*10)\n\nparam_grid = {'criterion': ['gini','entropy'], \n              'splitter': ['best', 'random'], \n              'max_depth': [2,4,6,8,10,None], \n              #'min_samples_split': [2,5,7,10,12], \n              #'min_samples_leaf': [1,3,5,7, 10], \n              'random_state': [0] \n             }\n\n\ntune_model = GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'accuracy', cv = cv_split, return_train_score=True)\ntune_model.fit(x, y)\n\nfor i in range(10):\n    print(\"epoch:\",i,\"train_score:\",tune_model.cv_results_['split'+str(i)+'_train_score'][tune_model.best_index_],\n    \"test_score:\",tune_model.cv_results_['split'+str(i)+'_test_score'][tune_model.best_index_])\n\nprint('-'*10)    \n\nprint('AFTER Tuning Parameters: ', tune_model.best_params_)\nprint(\"AFTER Tuning Training w\/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100))\nprint(\"AFTER Tuning Test w\/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint('-'*10)","ce083fe8":"# train the model using tuned decision tree parameters\ndtree = tree.DecisionTreeClassifier(criterion= 'entropy', max_depth= 4, random_state= 0, splitter= 'random')\nbase_results = cross_validate(dtree, x, y, cv  = cv_split, return_train_score=True)\ndtree.fit(x, y)","75f7c8c1":"def plot_feature_importances(df):\n    \n    # Sort features according to importance\n    df = df.sort_values('importance', ascending = False).reset_index()\n    \n    # Normalize the feature importances to add up to one\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n\n    # Make a horizontal bar chart of feature importances\n    plt.figure(figsize = (10, 6))\n    ax = plt.subplot()\n    \n    # Need to reverse the index to plot most important on top\n    ax.barh(list(reversed(list(df.index[:15]))), \n            df['importance_normalized'].head(15), \n            align = 'center', edgecolor = 'k')\n    \n    # Set the yticks and labels\n    ax.set_yticks(list(reversed(list(df.index[:15]))))\n    ax.set_yticklabels(df['feature'].head(15))\n    \n    # Plot labeling\n    plt.xlabel('Normalized Importance'); plt.title('Feature Importances')\n    plt.show()\n    \n    return df","a2fc831c":"importance = dtree.feature_importances_\nfeature = x.columns\nfi = pd.DataFrame()\nfi['importance'] = importance\nfi['feature'] = feature\nfi_sorted = plot_feature_importances(fi)","7cc9d4a8":"# grid search for bagging classifier\nclassifier = ensemble.BaggingClassifier()\nbase_results = cross_validate(classifier, x, y, cv  = cv_split, return_train_score=True)\nclassifier.fit(x, y)\n\ncl1 = LogisticRegressionCV()\ncl2 = tree.DecisionTreeClassifier()\ncl3 = svm.LinearSVC()\ncl4 = discriminant_analysis.LinearDiscriminantAnalysis()\ncl5 = discriminant_analysis.QuadraticDiscriminantAnalysis()\nparam_grid = {'base_estimator':[cl1, cl2, cl3, cl4, cl5],\n              'n_estimators':[10,13,17],\n              #'warm_start':[False, True]\n             }\n\n\ntune_model = GridSearchCV(ensemble.BaggingClassifier(), param_grid=param_grid, scoring = 'accuracy', cv = cv_split, return_train_score=True)\ntune_model.fit(x, y)\n\n","cc8cb90c":"# printing the results of bagging before and after tuning\nepoch=0\nfor train_score,test_score in zip(base_results['train_score'], base_results['test_score']):\n        epoch +=1       \n        print(\"epoch:\",epoch,\"train_score:\",train_score, \"test_score:\",test_score)\nprint('-'*10)\n\nprint('BEFORE Tuning Parameters: ', classifier.get_params())\nprint(\"BEFORE Tuning Training w\/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE Tuning Test w\/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint('-'*10)\n\nfor i in range(10):\n    print(\"epoch:\",i,\"train_score:\",tune_model.cv_results_['split'+str(i)+'_train_score'][tune_model.best_index_],\n    \"test_score:\",tune_model.cv_results_['split'+str(i)+'_test_score'][tune_model.best_index_])\n\nprint('-'*10)    \n\n\nprint('AFTER Tuning Parameters: ', tune_model.best_params_)\nprint(\"AFTER Tuning Training w\/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100))\nprint(\"AFTER Tuning Test w\/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint('-'*10)","a700595b":"classifier = ensemble.AdaBoostClassifier()\nbase_results = cross_validate(classifier, x, y, cv  = cv_split, return_train_score=True)\nclassifier.fit(x, y)\n\nepoch=0\nfor train_score,test_score in zip(base_results['train_score'], base_results['test_score']):\n        epoch +=1       \n        print(\"epoch:\",epoch,\"train_score:\",train_score, \"test_score:\",test_score)\nprint('-'*10)\n\nprint('BEFORE Tuning Parameters: ', classifier.get_params())\nprint(\"BEFORE Tuning Training w\/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE Tuning Test w\/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint('-'*10)\n\ncl1 = LogisticRegressionCV()\ncl2 = tree.DecisionTreeClassifier()\ncl3 = naive_bayes.GaussianNB()\nparam_grid = {'base_estimator':[cl1, cl2, cl3]\n             }\n\n\ntune_model = GridSearchCV(ensemble.AdaBoostClassifier(), param_grid=param_grid, scoring = 'accuracy', cv = cv_split, return_train_score=True)\ntune_model.fit(x, y)\n\nfor i in range(10):\n    print(\"epoch:\",i,\"train_score:\",tune_model.cv_results_['split'+str(i)+'_train_score'][tune_model.best_index_],\n    \"test_score:\",tune_model.cv_results_['split'+str(i)+'_test_score'][tune_model.best_index_])\n\nprint('-'*10)    \n\n\nprint('AFTER Tuning Parameters: ', tune_model.best_params_)\nprint(\"AFTER Tuning Training w\/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100))\nprint(\"AFTER Tuning Test w\/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint('-'*10)","949b2f46":"classifier = ensemble.RandomForestClassifier()\nbase_results = cross_validate(classifier, x, y, cv  = cv_split, return_train_score=True)\nclassifier.fit(x, y)\n\nepoch=0\nfor train_score,test_score in zip(base_results['train_score'], base_results['test_score']):\n        epoch +=1       \n        print(\"epoch:\",epoch,\"train_score:\",train_score, \"test_score:\",test_score)\nprint('-'*10)\n\nprint('BEFORE Tuning Parameters: ', classifier.get_params())\nprint(\"BEFORE Tuning Training w\/bin score mean: {:.2f}\". format(base_results['train_score'].mean()*100)) \nprint(\"BEFORE Tuning Test w\/bin score mean: {:.2f}\". format(base_results['test_score'].mean()*100))\nprint('-'*10)\n\nparam_grid = {'n_estimators': [15,25,30,35],\n              'criterion': ['gini','entropy'],  #scoring methodology; two supported formulas for calculating information gain - default is gini\n              'max_depth': [2,4,6,None], #max depth tree can grow; default is none\n              'min_samples_split': [2,5,7,10,12], #minimum subset size BEFORE new split (fraction is % of total); default is 2\n              #'min_samples_leaf': [1,3,5], #minimum subset size AFTER new split split (fraction is % of total); default is 1\n              'max_features': [2,3,'auto'], #max features to consider when performing split; default none or all\n              'random_state': [0] #seed or control random number generator: https:\/\/www.quora.com\/What-is-seed-in-random-number-generation\n             }\n\n\ntune_model = GridSearchCV(ensemble.RandomForestClassifier(), param_grid=param_grid, scoring = 'accuracy', cv = cv_split, return_train_score=True)\ntune_model.fit(x, y)\n\nfor i in range(10):\n    print(\"epoch:\",i,\"train_score:\",tune_model.cv_results_['split'+str(i)+'_train_score'][tune_model.best_index_],\n    \"test_score:\",tune_model.cv_results_['split'+str(i)+'_test_score'][tune_model.best_index_])\n\nprint('-'*10)    \n\n\nprint('AFTER Tuning Parameters: ', tune_model.best_params_)\nprint(\"AFTER Tuning Training w\/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100))\nprint(\"AFTER Tuning Test w\/bin score mean: {:.2f}\". format(tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint('-'*10)","7b3abffb":"# train the model using tuned random forest parameters\nrandom_forest = ensemble.RandomForestClassifier(criterion= 'entropy', max_depth= None, random_state= 0, min_samples_split= 10, n_estimators=25)\nbase_results = cross_validate(random_forest, x, y, cv  = cv_split, return_train_score=True)\nrandom_forest.fit(x, y)","63336181":"importance = random_forest.feature_importances_\nfeature = x.columns\nfi = pd.DataFrame()\nfi['importance'] = importance\nfi['feature'] = feature\nfi_sorted = plot_feature_importances(fi)","8932319a":"MLA = [    \n        # Generalized Linear Models\n        LogisticRegressionCV(),\n    \n        # SVM\n        svm.SVC(probability=True, C=1.0, gamma=0.02, kernel='linear'),\n        svm.LinearSVC(),\n    \n        # KNN\n        neighbors.KNeighborsClassifier(weights='distance'),\n    \n        #Discriminant Analysis\n        discriminant_analysis.LinearDiscriminantAnalysis(),\n        discriminant_analysis.QuadraticDiscriminantAnalysis(),\n     \n        # Naive Bayes\n        naive_bayes.BernoulliNB(),\n        naive_bayes.GaussianNB(),\n    \n        #Trees    \n        tree.DecisionTreeClassifier(criterion= 'entropy', max_depth= 4, random_state= 0, splitter= 'random'),\n    \n        # Ensemble Methods\n        ensemble.AdaBoostClassifier(base_estimator = LogisticRegressionCV()),\n        ensemble.BaggingClassifier(base_estimator=LogisticRegressionCV(), n_estimators=10),\n        ensemble.ExtraTreesClassifier(),\n        ensemble.GradientBoostingClassifier(),\n        ensemble.RandomForestClassifier(criterion='entropy', min_samples_split=10, n_estimators=25, random_state=0, max_features=3)\n     \n    ]\n\ncv_split = ShuffleSplit(n_splits = 10, test_size = .3, train_size = .7, random_state = 0)\nMLA_columns = ['MLA Name','MLA Train Accuracy Mean', 'MLA Test Accuracy Mean','MLA Time']\nMLA_compare = pd.DataFrame(columns = MLA_columns)\n\nrow_index = 0\nfor alg in MLA:\n    MLA_name = alg.__class__.__name__\n    MLA_compare.loc[row_index, 'MLA Name'] = MLA_name\n    cv_results = cross_validate(alg, x,y, cv  = cv_split, return_train_score=True)\n    \n    MLA_compare.loc[row_index, 'MLA Time'] = cv_results['fit_time'].mean()\n    MLA_compare.loc[row_index, 'MLA Train Accuracy Mean'] = cv_results['train_score'].mean()\n    MLA_compare.loc[row_index, 'MLA Test Accuracy Mean'] = cv_results['test_score'].mean()   \n         \n    row_index+=1\n   \n\nMLA_compare.sort_values(by = ['MLA Test Accuracy Mean'], ascending = False, inplace = True)\n","84497d79":"MLA_compare","67779d39":"print('BEFORE RFE Training Shape Old: ', x.shape) \nprint('BEFORE RFE Training Columns Old: ', x.columns.values)\n\nprint(\"BEFORE RFE Training w\/bin score mean: {:.2f}\". format(base_results_dtree['train_score'].mean()*100)) \nprint(\"BEFORE RFE Test w\/bin score mean: {:.2f}\". format(base_results_dtree['test_score'].mean()*100))\nprint('-'*10)\n\n#feature selection\ndtree_rfe = feature_selection.RFECV(tree.DecisionTreeClassifier(), step = 1, scoring = 'accuracy', cv = cv_split)\ndtree_rfe.fit(x, y)\n\n#transform x&y to reduced features and fit new model\nX_rfe = x.columns.values[dtree_rfe.get_support()]\nrfe_results = cross_validate(dtree, x[X_rfe], y, cv  = cv_split)\n\nprint('AFTER RFE Training Shape New: ', x[X_rfe].shape) \nprint('AFTER RFE Training Columns New: ', X_rfe)\n\nprint(\"AFTER RFE Training w\/bin score mean: {:.2f}\". format(rfe_results['train_score'].mean()*100)) \nprint(\"AFTER RFE Test w\/bin score mean: {:.2f}\". format(rfe_results['test_score'].mean()*100))\nprint('-'*10)\n\nparam_grid = {'criterion': ['gini','entropy'], \n              'splitter': ['best', 'random'], \n              'max_depth': [2,4,6,8,10,None], \n              #'min_samples_split': [2,5,7,10,12], \n              #'min_samples_leaf': [1,3,5,7, 10], \n              'random_state': [0] \n             }\n\n#tune rfe model\nrfe_tune_model = GridSearchCV(tree.DecisionTreeClassifier(), param_grid=param_grid, scoring = 'accuracy', cv = cv_split)\nrfe_tune_model.fit(x[X_rfe], y)\n\nprint('AFTER RFE Tuned Parameters: ', rfe_tune_model.best_params_)\nprint(\"AFTER RFE Tuned Training w\/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_train_score'][tune_model.best_index_]*100)) \nprint(\"AFTER RFE Tuned Test w\/bin score mean: {:.2f}\". format(rfe_tune_model.cv_results_['mean_test_score'][tune_model.best_index_]*100))\nprint('-'*10)","f5acfebc":"## Imports","cc86835c":"Passenger Name is a categorical variable and it is obviously unique for each passenger. So we cannot use the Name variable directly in our model but we can extract some useful information from this variable like a new feature called 'Title' which can guide our model. ","b0bbb36b":"Since, Embarked column has very few number of missing values, only 2, we can impute it with the mode of the column. It will not affect the overall distribution of the column. ","dc6eb4a3":"## Credits\nI have learnt a lot of things from the following people, that helped me in building this notebook. I have also borrowed some of the code in this notebook from them. So, I want to give credit, where the credit is due.","8c9c2f3d":"# Explorartory Data Analysis\nExploratory Data Analysis (EDA) is an open-ended process where we calculate statistics and make figures to find trends, anomalies, patterns, or relationships within the data. The goal of EDA is to learn what our data can tell us. It generally starts out with a high level overview, then narrows in to specific areas as we find intriguing areas of the data. The findings may be interesting in their own right, or they can be used to inform our modeling choices, such as by helping us decide which features to use.","dbbed582":"## Examine the Distribution of the 'Survived' Column\n\nThe survival is what we are asked to predict: either a 0 for the person who will not survive, or a 1 indicating the person will survive the tragedy. We can first examine the number of passengers falling into each category.","169a5581":"From this information, we see that some titles are very rare, so we will combine these titles under a single name - 'Others'","04db035e":"Although this graph is stating that children with age between 0-10 have more survival to non-survival ratio, this graph is not helping much in getting some useful information. We will examine this variable by dividing into different age groups- child, young and old. And for the null values in the age variable, we will make 'missing' class. ","5cfc9c9a":"## Conclusion\nFrom the above table, it can be infered that the ensemble methods are not a good fit for our dataset. Although they are among the highest performers in terms of mean test accuracy, but they are facing the overfitting problem. \n\nEnsemble methods mainly works in two ways:\n\n- either they sample the data multiple times and make many datasets (like bootstrapping), and on each dataset a classifier is being trained and then the combined result of each classifier is presented as output\n- or on each dataset produced, a classifier is being trained with a randomly chosen subset of the features set, and then the combined result of each classifier is presented as output. \n\nIn our case the dataset as well as the feature set, both are very small. So, while building the ensemble model, there is high probability that the different datasets produced have almost the same type of samples or since there are very few features, the different classifiers are considering the same features again and again. Due to these two reasons, the different classifiers in an ensemble model maybe having high correlations with each other. Hence, they can create the problem of overfitting, when the results of all these classifiers are combined.     \n\nNow, since decision tree algorithm is among the top performers, we can further try to improve the performance of this algorithm with sklearn's feature selection method by exploiting the feature_importance attribute of decision trees.  ","b1db74ce":"When it comes to building our machine learning models, we will have to fill in these missing values (known as imputation). Also, there are some models available such as LightGBM, XGBoost that can [handle missing values with no need for imputation](https:\/\/stats.stackexchange.com\/questions\/235489\/xgboost-can-handle-missing-data-in-the-forecasting-phase). Another option would be to drop columns with a high percentage of missing values, which do not have high feature importance or high correlation with the labels.","20b04c8c":"### Check the Feature Importances returned by the Random Forest","5480bfdf":"First, run the algorithms with the default parameters to get an idea about their performances on our data and then we would tune the parameters of better performing algorithms to improve their performances.","ffff59fe":"# Introduction: Titanic- Machine Learning from Disaster\n\nThis notebook is intended for beginners who want to take a tour of how to approach a machine learning\/data science problem, starting from observing and analyzing the data, infering some not so directly visible information in the data, correcting the given data, exporting and extracting out some new and useful information from the data (feature engineering), dealing with missing values in the data, pre-processing the data to make it fit for feeding into the ML algorithms, building the ML models, interpreting the role of various parameters in the ML algorithms, improving the performance of the ML models by tuning the parameters and applying the feature selection techniques. Finally, this notebook will analyse, why some of the better performing algorithms are not a good fit for this problem\/dataset.      \n\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class. The challenge is to complete the analysis of what sorts of people were likely to survive. \n\n This is a standard supervised classification task:\n- __Supervised:__ The labels are included in the training data and the goal is to train a model to learn to predict the labels from the features\n- __Classification:__ The label is a binary variable, 0 (not survived the tragedy), 1 (survived the tragedy)\n\nThe __evaluation metric__ for this problem is 'accuracy', i.e. how many of our predictions are correct.\n\n__Note__: This is my first ever notebook in terms of a machine learning project. So, I am learning with this work, and if anybody has any suggestions or feedback with respect to this notebook, then please comment. And, if you like the work, then please upvote.  \n\n","5db47239":"### Grid Search for Adaboost\nThe idea behind boosting is at every stage, try to look at previous stage and see which are the data points that were misclassified and try to get them correct in this stage. It is fine to make mistakes in this stage on the data points that have been correctly classified upto the previous stage because the previous classifiers can possibly adjust for that. Adaboost classifier is a boosting technique which is based on exponential loss function.\n\nIn the case of bagging, we create multiple datasets and train a weak classifier on each of those sets parallely. So, Bagging classifier is inherently parallel and that is why it is fast as well. \n\nIn the case of boosting, first we assign weights to the data points. The data points that were misclassified in the previous stage will get higher weights in the current stage and those which were correctly classified will get lower weights. Then sample from those data points according to their weights. We create a training set by sampling from the data points given to us according to their weights. So, points for which weights are higher will get sampled more often and points for which weights are very low might not even appear in the dataset. So, if the point appears multiple times in the dataset and then we are trying to minimize the training error, we are likely to get that point correct. \n\nSince we are assigning weights to the data points at every stage according to the errors made in the previous stage, this makes boosting algorithm inherently serial, unlike bagging and that is why boosting is slow as well, in general.   \n\nSo, while tuning the adaboost classifier, we will tune the base estimators. Since adaboost requires sample weighting, those estimators which do not support sample_weights method cannot be used as base estimators, like LDA.  ","b293bcf6":"## Examine Missing Values\n\nNext we can look at the number and percentage of missing values in each column. ","63dee43f":"From the above results, it can be seen that __Gradient Boosting Classifier__ has the highest test accuracy but it has a lot of __overfitting__ since its training accuracy is around 90%, while its test accuracy is only around 83.13%. So, this algorithm cannot be trusted as it is for our model in terms of its generalization ability. So, we will tune some of the better performing algorithms by searching for the best parameters for them.   ","8dd461a0":"## Encode the categorical variable\nA machine learning model unfortunately cannot deal with categorical variables, except for some models such as LightGBM. Hence, we need to find a way to encode these variables as numbers before feeding them to our model. There are many types of encoding techniques but the two main techniques are:\n\n- Label Encoding: assign each unique class in a categorical variable with an integer, randomly. No new columns are created\n- One Hot Encoding: create a new column for each unique class in a categorical variable. Each entry recieves a 1 in the column for its corresponding class and a 0 in all other new columns\n\nThe drawback of the label encoding is that it assigns arbitrary ordering to the classes of the categorical variables which do not reflect the inherent properties\/information of that variable and label encoding may be misleading in this case. When we have only 2 unique classes (like male\/female) for a categorical variable or if we have more than 2 unique classes and we already know the relative ordering of the classes (like, low\/medium\/high), then we can go with label encoding but here also, we need to ensure that the integer values assigned to these classes align with the relative order of the classes (like, low=0, medium=1 and high=2), otherwise one-hot encoding is the safe option.\n\nThe drawback of the one-hot encoding is that the number of new columns\/features (dimensions of the data) can explode with the categorical variable having many unique classes because there will be a new column for each unique class in the variable. To deal with the feature exploding problem, we can use dimensionality reduction techniques like, Principal Component Analysis (PCA) after one-hot encoding.   ","517ab220":"From this information, we can observe that Age (Mr vs Master), Family\/relationship status (Mrs vs Miss), Gender\/Sex (Mr vs Mrs) variables play reasonable role in the survival of the passenger.","a8ad10b3":"### One-Hot Encoding ","36892282":"## Tuning the Model with Hyper-Parameters\nWe will tune our model using Parameter Grid and Grid Search CV for different algorithms.","54d3f8c6":"From this information, we can consider this problem as a balanced class problem.","93ba333b":"From this information, we can see that there are many passengers who do not have any family member onboard. This directs us to make another variable called 'family_status' which will tell us whether the passenger was alone on the ship or with some family member (not alone).  ","af976577":"## Visualise the variables by plotting the classes on a graph","e20d0dba":"### Grid Search for Bagging Classifier\nThe idea behind bagging is to create multiple training sets from the given dataset by bootstrapping, train a weak classifier on each of those sets abd then combine the output of each classifier either by majority vote (in case of 0\/1) or by weighted average of probability. While tuning the parameters for bagging classifier, we will tune the base_estimator or the weak classifier that we want to bag and the number of such weak classifiers in the bag.  ","fb6a4697":"# Build the Machine Learning Model\nThe problem that we are solving is a binary classification problem and since we are given the target variable or the labels, it is a supervised learning. There is No Free Lunch (NFL) theorem in ,achine learning which states that no algorithm is the best for the generic case and all special cases. So, to build an efficient model, we need to compare the performance of various machine learning classification algorithms. But since we have identified our solution as a supervised learning classification algorithm, we can narrow down our list of choices.\n\n__Machine Learning Classificaton Algorithms:__\n- Generalized Linear Models (GLM)\n- Support Vector Machines (SVM)\n- Discriminant Analysis\n- Nearest Neighbors\n- Naive Bayes\n- Decision Trees\n- Ensemble Methods","2d1c0ed6":"## Data\nThe data is taken from the kaggle's titanic competition website. The data contains features and labels-\n\n- __Features:__ Passenger Id, Passenger Class, Passenger Name, Passenger Gender, Passenger Age, No. of Siblings, spouse, parent and children related to passenger on-board, Passenger Ticket, Ticket's Fare, Passenger Cabin, Embarked (location on the ship)\n- __Label:__ Survived\n","64d5a2e1":"## Imports","60511814":"Passenger Id and Ticket are just random variables and they do not give any intuition for their relation with the chances of survival of a passenger. So, we would like to drop these variables. Cabin variable has more than 75% missing data, so we would want to drop this variable. Also, we have already extracted out the useful information from the Name variable, so we would drop this variable as well.  ","07484c5a":"## Correction in the data\nIf there is any male with age less than 13 and having a title of Mr, then that is incorrect entry and we need to correct the title, changing it to Master. Although, there is only one incorrect entry in our data of this type and changing it will not make much impact, but Data correction is an important step in building a machine learning model.  ","dc0bc0ba":"There are 2 features in our dataset- 'SibSp' gives the information about the sibling or spouse of the passenger onboard and 'Parch' gives information about the parents and children of the passenger onboard. But both these variables basically indicate the family information of passenger onboard. So we will combine these 2 variables into one variable 'family'.  ","e9950f3c":"## Tune the Decision Tree Model with Feature Selection\nWe will use Recursive Feature Elimination (RFE) method which selects features by recursively considering smaller and smaller sets of features. First, the estimator is trained on the initial set of features and the importance of each feature is obtained through a feature_importances_ attribute. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set until the desired number of features to select is eventually reached. ","d8f921c1":"### Grid Search for Decision Trees\nThe parameters important for tuning are:\n- 'criterion': __gini or entropy__ -  measure of impurity (can be treated as a loss function)\n- 'splitter': __best or random__ - methodology to select the next feature to split on\n- 'max_depth': maximum height to restrict the growth of the tree beyond certain point (to avoid overfitting sometimes)\n- 'min_samples_split': minimum number of samples at a node required to split \n\nDecision tree grows by splitting the data based on the selected feature\/variable (nodes in terms of trees), but a big question arises while implementing the trees that when should we stop. One answer to this question is Early Stopping. The tree grows in such a manner so as to minimize the loss function or the error by recursively splitting the variables, but at some point, the improvement in the error is not significant, then we can stop. This stopping is called as early stopping. \n\nEarly stopping might not be a good idea in some situations. For example, let's say there are two variables to split on - x1 and x2. Consider all the possible ways where we can split x1. Suppose, we are not getting any improvement in the error by splitting on x1. Now, consider all the possible ways where we can split x2. Again we are not getting any error improvement. At this point, early stopping may stop this recursive algorithm and give the final results. But what if we can get error improvement by first splitting on x1 and then on x2. Early stopping may miss out on these interactions of variables such as in this case- first split on x1 and then on x2.\n\nTo avoid the problem created by early stopping, there is a second strategy to stop the tree algorithm, which is stop the splitting when the leaves are small. This means that when the number of data points in the region defined by the leaf of the tree are very small, then stop the further splitting of that region. This small number we can define by ourselves with the help of __min_samples_leaf__ parameter and we can also tune this parameter. ","a54afdb9":"### Grid Search for Random Forest\nRandom forest is an improved version of bagging when we use decision trees as a base estimator. The goal of random forest is to reduce the correlation among trees. The idea is to start doing bagging as we normally do (create multiple training sets by bootstrapping). Then, start building trees on these datasets. So, in random forest what we do is at every node, randomly sample 't' features from 'p' feature set (t<p). Find out which is the best split variable among these 't' features alone and split on that. Generally, t=sqrt(p) or log(p) but we can select any value with the max_features parameter. \n\nIf we had just done bagging, at the root level, it is highly likely that each one of the bagged tree would have picked the same attribute. So, at the higher levels of the tree, it will look very similar. But in random forest, we are getting rid of that. This leads to significant reduction in variance in the bagged estimate, especially when we have small feature set. \n\nTuning random forest is just like tuning decision trees, except here, since we are bagging lot of decision trees, we need to tune the no. of estimators parameter. With small dataset and small feature set, large number of estimators can sometimes lead to overfitting.   ","fce9c82b":"### Label Encoding \nIn this notebook, we will implement label encoding for any categorical variable which have 2 unique classes","9ce6c19a":"## Feature Engineering\n  ","1f320a44":"### Check the Feature Importances returned by the Tuned Decision Tree","2f9184ad":"We may infer from this plot that since there were limited lifeboats, they preferred children and old people in sending them on the lifeboats.","e23fc6ec":"## Compare the performances of the tuned algorithms on our dataset","b5362b6d":"- [Will Koehrsen - Start Here: A Gentle Introduction](https:\/\/www.kaggle.com\/willkoehrsen\/start-here-a-gentle-introduction)\n\n- [LD Freeman - A Data Science Framework: To Achieve 99% Accuracy](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy)\n","8ffba18d":"Since, sex and family_status are 2 categorical variables with 2 unique classes, they are label encoded. We can also implement label encoding with the Age variable even if it has more than 2 unique classes, because we know the relative ordering of the classes, age_old > age_young > age_child ","a033cbd9":"From this information of titles, we can see that there are 5 classes. Leaving out the others, 'Miss' and 'Mrs' titles are used for females and the only information that we can get from these 2 titles is that there are 125 females (Mrs) who are married and 182 females (Miss) are not married. 'Mr' and 'Master' titles are used for males. Master is used for children (people with age less than 13). This means that there are 40 male children (age less than 13) and 517 males with age greater than 13.    ","772a8d2f":"Now, since we have imputed the missing age values, again divide the age variable into three groups- child, young and old.","635d1e20":"## Closing Comments\nAfter tuning the decision tree model with respect to hyper-parameters as well as feature selection, we are able to reduce the overfitting but still we are getting the mean test accuracy of around 83%. Since, our dataset is very small, the decision tree model will likely have more variance. So, instead we can use either Linear Discriminant Analysis model or Logistic Regression model as both of them have similar accuracy, closer to decision trees. \n\nEnsemble models like Adaboost and Bagging can also be a good fit, but at the end, if we are able to achieve accuracy level close to that of complex models with a simple model, then we prefer a simple model for our problem. So, that is why we would like to prefer Logistic Regression model here.     ","82d7a47e":"### Grid Search for SVM\nThere are 3 important parameters when it comes to tuning with svm - __C__, __kernel__, and __gamma__. \n\nC represents the penalty parameter, larger the value of C, the more we are penalising the violations\/misclassifications. So, there is a tradeoff with respect to C, the larger we make C, the smaller will the margin be but we will be getting more of the training data correct. So, if we make C very large to get most of the training data correct, then we may compromise with the generalisation property (robustness) of the model. In some cases, even when the data is truly linearly separable, we would like to tradeoff a small C for greater margins to make our model robust, especially in case of noisy data.\n\nWhen our data is not linear in the given dimensions, then to make our classifier more powerful, we do basis transformation. Kernel trick does the same thing for us, it takes the data in the given dimensions and transform it to some higher dimensions to make it linear and then applies a linear classification. This facility is provided by the kernel parameter in the SVM algorithm. \n\nGamma is a parameter which is associated with rbf or poly kernel and deals with the measure of complexity of the model. Small gamma means less complexity and large gamma means more complexity and very large gamma may eventually lead to overfitting.","cc02695e":"Now let's observe the distribution of Age. Age is a continuous variable with float data type. So, let's first plot the KDE (kernel distribution estimation) plot for age.   ","0b32796d":"## Read in data","47d45006":"Now this plot gives us some useful information such as children are more likely to survive, followed by young and old. Maybe because there were limited lifeboats and they tried to save all the children first by sending them on the lifeboats with some adults. But Age variable has many missing entries (177 to be precise) and we need to impute them.   ","1e609dee":"Data preprocessing is complete. Here x represents features and y represents labels."}}