{"cell_type":{"2d173243":"code","80e40d5b":"code","e8151111":"code","8ef13d5a":"code","fbf72054":"code","1954a10f":"code","30de79aa":"code","b4cec8bc":"code","bcc38aba":"code","b3efac4e":"code","d8cecaed":"code","bec6b6f9":"code","e712aabd":"code","d9d96069":"code","5c0d26af":"code","71bbb5fc":"code","f0797b79":"code","5b7d6210":"code","765a58cb":"code","9653466f":"code","a33aa38b":"code","ccaefe9c":"code","0a52801e":"code","38f05fbd":"code","25625a32":"code","6e9ec86d":"code","a90688b1":"code","61d92c8a":"code","dfed6915":"code","654de97d":"code","207e940e":"code","f0126ea1":"code","0fbfd00a":"code","0a24c85f":"code","a5f3afe3":"code","2e962f22":"markdown","5c9dd560":"markdown","4c0cb6ed":"markdown","662a7eec":"markdown","eeff1640":"markdown","1dbc848d":"markdown","a6cedef1":"markdown","426ca3d4":"markdown","e104e7b1":"markdown","563ee64d":"markdown","68f1976c":"markdown","408c317d":"markdown","0f36a8a6":"markdown","ec0d2c5c":"markdown","137d7ee3":"markdown","78f8452d":"markdown","dd344de9":"markdown","03813c42":"markdown","bc80b739":"markdown","fabbb873":"markdown","bbd3bfe8":"markdown","2186aa35":"markdown","593f348b":"markdown"},"source":{"2d173243":"!pip install --user torch==1.9.0 torchvision==0.10.0 torchaudio==0.9.0 torchtext==0.10.0\n!git clone https:\/\/github.com\/openai\/CLIP\n# !pip install taming-transformers\n!git clone https:\/\/github.com\/CompVis\/taming-transformers.git\n!pip install ftfy regex tqdm omegaconf pytorch-lightning\n!pip install kornia\n!pip install imageio-ffmpeg\n!pip install einops\n!mkdir steps","80e40d5b":"import os\nimport torch\ntorch.hub.download_url_to_file('https:\/\/heibox.uni-heidelberg.de\/d\/a7530b09fed84f80a887\/files\/?p=%2Fconfigs%2Fmodel.yaml&dl=1', \n                               'vqgan_imagenet_f16_16384.yaml')\ntorch.hub.download_url_to_file('https:\/\/heibox.uni-heidelberg.de\/d\/a7530b09fed84f80a887\/files\/?p=%2Fckpts%2Flast.ckpt&dl=1', \n                               'vqgan_imagenet_f16_16384.ckpt')\nimport argparse\nimport math\nfrom pathlib import Path\nimport sys\nsys.path.insert(1, '.\/taming-transformers')\n\n# from IPython import display\nfrom base64 import b64encode\nfrom omegaconf import OmegaConf\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom taming.models import cond_transformer, vqgan\nimport taming.modules \nfrom torch import nn, optim\nfrom torch.nn import functional as F\nfrom torchvision import transforms\nfrom torchvision.transforms import functional as TF\nfrom tqdm.notebook import tqdm\nfrom CLIP import clip\nimport kornia.augmentation as K\nimport numpy as np\nimport imageio\nfrom PIL import ImageFile, Image\nfrom urllib.request import urlopen\n\nImageFile.LOAD_TRUNCATED_IMAGES = True\nfrom pynvml.smi import nvmlInit, nvmlDeviceGetHandleByIndex, nvmlDeviceGetUtilizationRates\nnvmlInit()\nhandle = nvmlDeviceGetHandleByIndex(0)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e8151111":"torch.hub.download_url_to_file('https:\/\/images.pexels.com\/photos\/158028\/bellingrath-gardens-alabama-landscape-scenic-158028.jpeg', \n                               'garden.jpeg')\ntorch.hub.download_url_to_file('https:\/\/images.pexels.com\/photos\/803975\/pexels-photo-803975.jpeg', \n                               'cabin.jpeg')","8ef13d5a":"def sinc(x):\n    return torch.where(x != 0, torch.sin(math.pi * x) \/ (math.pi * x), x.new_ones([]))\n\ndef lanczos(x, a):\n    cond = torch.logical_and(-a < x, x < a)\n    out = torch.where(cond, sinc(x) * sinc(x\/a), x.new_zeros([]))\n    return out \/ out.sum()\n\ndef ramp(ratio, width):\n    n = math.ceil(width \/ ratio + 1)\n    out = torch.empty([n])\n    cur = 0\n    for i in range(out.shape[0]):\n        out[i] = cur\n        cur += ratio\n    return torch.cat([-out[1:].flip([0]), out])[1:-1]\n\ndef resample(input, size, align_corners=True):\n    n, c, h, w = input.shape\n    dh, dw = size\n    input = input.view([n * c, 1, h, w])\n    if dh < h:\n        kernel_h = lanczos(ramp(dh \/ h, 2), 2).to(input.device, input.dtype)\n        pad_h = (kernel_h.shape[0] - 1) \/\/ 2\n        input = F.pad(input, (0, 0, pad_h, pad_h), 'reflect')\n        input = F.conv2d(input, kernel_h[None, None, :, None])\n    \n    if dw < w:\n        kernel_w = lanczos(ramp(dw \/ w, 2), 2).to(input.device, input.dtype)\n        pad_w = (kernel_w.shape[0] - 1) \/\/ 2\n        input = F.pad(input, (pad_w, pad_w, 0, 0), 'reflect')\n        input = F.conv2d(input, kernel_w[None, None, None, :])\n    input = input.view([n, c, h, w])\n    return F.interpolate(input, size, mode='bicubic', align_corners=align_corners)","fbf72054":"class ReplaceGrad(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, x_forward, x_backward):\n        ctx.shape = x_backward.shape\n        return x_forward\n    @staticmethod\n    def backward(ctx, grad_in):\n        return None, grad_in.sum_to_size(ctx.shape)\nreplace_grad = ReplaceGrad.apply\n\nclass ClampWithGrad(torch.autograd.Function):\n    @staticmethod\n    def forward(ctx, input, min, max):\n        ctx.min = min\n        ctx.max = max\n        ctx.save_for_backward(input)\n        return input.clamp(min, max)\n    @staticmethod\n    def backward(ctx, grad_in):\n        input, = ctx.saved_tensors\n        return grad_in * (grad_in * (input - input.clamp(ctx.min, ctx.max)) >= 0), None, None\nclamp_with_grad = ClampWithGrad.apply","1954a10f":"def vector_quantize(x, codebook):\n    d = x.pow(2).sum(dim=-1, keepdim=True) + codebook.pow(2).sum(dim=1) - 2 * x @ codebook.T\n    indices = d.argmin(-1)\n    x_q = F.one_hot(indices, codebook.shape[0]).to(d.dtype) @ codebook\n    return replace_grad(x_q, x)\n\nclass Prompt(nn.Module):\n    def __init__(self, embed, weight=1., stop=float('-inf')):\n        super().__init__()\n        self.register_buffer('embed', embed)\n        self.register_buffer('weight', torch.as_tensor(weight))\n        self.register_buffer('stop', torch.as_tensor(stop))\n    def forward(self, input):\n        input_normed = F.normalize(input.unsqueeze(1), dim=2)\n        embed_normed = F.normalize(self.embed.unsqueeze(0), dim=2)\n        dists = input_normed.sub(embed_normed).norm(dim=2).div(2).arcsin().pow(2).mul(2)\n        dists = dists * self.weight.sign()\n        return self.weight.abs() * replace_grad(dists, torch.maximum(dists, self.stop)).mean()\n\ndef parse_prompt(prompt):\n    vals = prompt.rsplit(':', 2)\n    vals = vals + ['', '1', '-inf'][len(vals):]\n    return vals[0], float(vals[1]), float(vals[2])","30de79aa":"class MakeCutouts(nn.Module):\n    def __init__(self, cut_size, cutn, cut_pow=1):\n        super().__init__()\n        self.cut_size = cut_size\n        self.cutn = cutn\n        self.cut_pow = cut_pow\n        self.augs = nn.Sequential(\n            K.RandomAffine(degrees=15, translate=0.1, p=0.7, padding_mode='border'),\n            K.RandomPerspective(0.7,p=0.7),\n            K.ColorJitter(hue=0.1, saturation=0.1, p=0.7),\n            K.RandomErasing((.1, .4), (.3, 1\/.3), same_on_batch=True, p=0.7),\n        )\n        self.noise_fac = 0.1\n        self.av_pool = nn.AdaptiveAvgPool2d((self.cut_size, self.cut_size))\n        self.max_pool = nn.AdaptiveMaxPool2d((self.cut_size, self.cut_size))\n\n    def forward(self, input):\n        slideY, slideX = input.shape[2:4]\n        max_size = min(slideX, slideY)\n        min_size = min(slideX, slideY, self.cut_size)\n        cutouts = []\n\n        for _ in range(self.cutn):\n            cutout = (self.av_pool(input) + self.max_pool(input))\/2\n            cutouts.append(cutout)\n\n        batch = self.augs(torch.cat(cutouts, dim=0))\n        if self.noise_fac:\n            facs = batch.new_empty([self.cutn, 1, 1, 1]).uniform_(0, self.noise_fac)\n            batch = batch + facs * torch.randn_like(batch)\n        return batch","b4cec8bc":"def load_vqgan_model(config_path, checkpoint_path):\n    config = OmegaConf.load(config_path)\n    if config.model.target == 'taming.models.vqgan.VQModel':\n        model = vqgan.VQModel(**config.model.params)\n        model.eval().requires_grad_(False)\n        model.init_from_ckpt(checkpoint_path)\n\n    elif config.model.target == 'taming.models.vqgan.GumbelVQ':\n        model = vqgan.GumbelVQ(**config.model.params)\n        model.eval().requires_grad_(False)\n        model.init_from_ckpt(checkpoint_path)\n\n    elif config.model.target == 'taming.models.cond_transformer.Net2NetTransformer':\n        parent_model = cond_transformer.Net2NetTransformer(**config.model.params)\n        parent_model.eval().requires_grad_(False)\n        parent_model.init_from_ckpt(checkpoint_path)\n        model = parent_model.first_stage_model\n    else:\n        raise ValueError(f'unknown model type: {config.model.target}')\n    del model.loss\n    return model","bcc38aba":"def resize_image(image, out_size):\n    ratio = image.size[0] \/ image.size[1]\n    area = min(image.size[0] * image.size[1], out_size[0] * out_size[1])\n    size = round((area * ratio)**0.5), round((area \/ ratio)**0.5)\n    return image.resize(size, Image.LANCZOS)","b3efac4e":"model_name = \"vqgan_imagenet_f16_16384\" \nimages_interval =  50\nwidth =  512\nheight = 512\ninit_image = \"\"\nseed = 42\nBASE_PATH = '..\/input\/flickr-image-dataset\/flickr30k_images\/flickr30k_images\/'\nargs = argparse.Namespace(\n    noise_prompt_seeds=[],\n    noise_prompt_weights=[],\n    size=[width, height],\n    init_image=init_image,\n    init_weight=0.,\n    clip_model='ViT-B\/32',\n    vqgan_config=f'{model_name}.yaml',\n    vqgan_checkpoint=f'{model_name}.ckpt',\n    step_size=0.13,\n    cutn=32,\n    cut_pow=1.,\n    display_freq=images_interval,\n    seed=seed,\n)\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nmodel = load_vqgan_model(args.vqgan_config, args.vqgan_checkpoint).to(device)\nperceptor = clip.load(args.clip_model, jit=False)[0].eval().requires_grad_(False).to(device)","d8cecaed":"def inference(text, \n              seed, \n              step_size,\n              max_iterations,\n              width, \n              height,\n              init_image, \n              init_weight, \n              target_images, \n              cutn, \n              cut_pow,\n              video_file\n             ):\n    all_frames = []\n    size=[width, height]\n    texts = text\n    init_weight=init_weight\n\n    if init_image:\n        init_image = init_image\n    else:\n        init_image = \"\"\n    if target_images:\n        target_images = target_images\n    else:\n        target_images = \"\"\n    max_iterations = max_iterations\n    model_names={\"vqgan_imagenet_f16_16384\": 'ImageNet 16384',\n                 \"vqgan_imagenet_f16_1024\":\"ImageNet 1024\", \n                 'vqgan_openimages_f16_8192':'OpenImages 8912',\n                 \"wikiart_1024\":\"WikiArt 1024\", \n                 \"wikiart_16384\":\"WikiArt 16384\", \n                 \"coco\":\"COCO-Stuff\",\n                 \"faceshq\":\"FacesHQ\",\n                 \"sflckr\":\"S-FLCKR\"}\n    name_model = model_names[model_name]\n    if target_images == \"None\" or not target_images:\n        target_images = []\n    else:\n        target_images = target_images.split(\"|\")\n        target_images = [image.strip() for image in target_images]\n\n    texts = [phrase.strip() for phrase in texts.split(\"|\")]\n    if texts == ['']:\n        texts = []\n    if texts:\n        print('Using texts:', texts)\n    if target_images:\n        print('Using image prompts:', target_images)\n    if seed is None or seed == -1:\n        seed = torch.seed()\n    else:\n        seed = seed\n    torch.manual_seed(seed)\n    print('Using seed:', seed)\n\n    cut_size = perceptor.visual.input_resolution\n    f = 2**(model.decoder.num_resolutions - 1)\n    make_cutouts = MakeCutouts(cut_size, cutn, cut_pow=cut_pow)\n    toksX, toksY = size[0] \/\/ f, size[1] \/\/ f\n    sideX, sideY = toksX * f, toksY * f\n\n    if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n        e_dim = 256\n        n_toks = model.quantize.n_embed\n        z_min = model.quantize.embed.weight.min(dim=0).values[None, :, None, None]\n        z_max = model.quantize.embed.weight.max(dim=0).values[None, :, None, None]\n    else:\n        e_dim = model.quantize.e_dim\n        n_toks = model.quantize.n_e\n        z_min = model.quantize.embedding.weight.min(dim=0).values[None, :, None, None]\n        z_max = model.quantize.embedding.weight.max(dim=0).values[None, :, None, None]\n\n    if init_image:\n        if 'http' in init_image:\n            img = Image.open(urlopen(init_image))\n        else:\n            img = Image.open(init_image)\n        pil_image = img.convert('RGB')\n        pil_image = pil_image.resize((sideX, sideY), Image.LANCZOS)\n        pil_tensor = TF.to_tensor(pil_image)\n        z, *_ = model.encode(pil_tensor.to(device).unsqueeze(0) * 2 - 1)\n    else:\n        one_hot = F.one_hot(torch.randint(n_toks, [toksY * toksX], device=device), n_toks).float()\n        # z = one_hot @ model.quantize.embedding.weight\n        if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n            z = one_hot @ model.quantize.embed.weight\n        else:\n            z = one_hot @ model.quantize.embedding.weight\n        z = z.view([-1, toksY, toksX, e_dim]).permute(0, 3, 1, 2) \n        z = torch.rand_like(z)*2\n    z_orig = z.clone()\n    z.requires_grad_(True)\n    opt = optim.Adam([z], lr=step_size)\n    normalize = transforms.Normalize(mean=[0.48145466, 0.4578275, 0.40821073],\n                                    std=[0.26862954, 0.26130258, 0.27577711])\n    pMs = []\n    for prompt in texts:\n        txt, weight, stop = parse_prompt(prompt)\n        embed = perceptor.encode_text(clip.tokenize(txt).to(device)).float()\n        pMs.append(Prompt(embed, weight, stop).to(device))\n    for prompt in target_images:\n        path, weight, stop = parse_prompt(prompt)\n        img = Image.open(path)\n        pil_image = img.convert('RGB')\n        img = resize_image(pil_image, (sideX, sideY))\n        batch = make_cutouts(TF.to_tensor(img).unsqueeze(0).to(device))\n        embed = perceptor.encode_image(normalize(batch)).float()\n        pMs.append(Prompt(embed, weight, stop).to(device))\n    for seed, weight in zip(args.noise_prompt_seeds, args.noise_prompt_weights):\n        gen = torch.Generator().manual_seed(seed)\n        embed = torch.empty([1, perceptor.visual.output_dim]).normal_(generator=gen)\n        pMs.append(Prompt(embed, weight).to(device))\n\n    def synth(z):\n        if args.vqgan_checkpoint == 'vqgan_openimages_f16_8192.ckpt':\n            z_q = vector_quantize(z.movedim(1, 3), model.quantize.embed.weight).movedim(3, 1)\n        else:\n            z_q = vector_quantize(z.movedim(1, 3), model.quantize.embedding.weight).movedim(3, 1)\n        return clamp_with_grad(model.decode(z_q).add(1).div(2), 0, 1)\n    @torch.no_grad()\n    def checkin(i, losses):\n        losses_str = ', '.join(f'{loss.item():g}' for loss in losses)\n        tqdm.write(f'i: {i}, loss: {sum(losses).item():g}, losses: {losses_str}')\n        out = synth(z)\n        # TF.to_pil_image(out[0].cpu()).save('progress.png')\n        # display.display(display.Image('progress.png'))\n        res = nvmlDeviceGetUtilizationRates(handle)\n        print(f'gpu: {res.gpu}%, gpu-mem: {res.memory}%')\n    def ascend_txt():\n        # global i\n        out = synth(z)\n        iii = perceptor.encode_image(normalize(make_cutouts(out))).float()\n        \n        result = []\n        if init_weight:\n            result.append(F.mse_loss(z, z_orig) * init_weight \/ 2)\n            #result.append(F.mse_loss(z, torch.zeros_like(z_orig)) * ((1\/torch.tensor(i*2 + 1))*init_weight) \/ 2)\n        for prompt in pMs:\n            result.append(prompt(iii))\n        img = np.array(out.mul(255).clamp(0, 255)[0].cpu().detach().numpy().astype(np.uint8))[:,:,:]\n        img = np.transpose(img, (1, 2, 0))\n        # imageio.imwrite('.\/steps\/' + str(i) + '.png', np.array(img))\n        img = Image.fromarray(img).convert('RGB')\n        all_frames.append(img)\n        return result, np.array(img)\n    def train(i):\n        opt.zero_grad()\n        lossAll, image = ascend_txt()\n        if i % args.display_freq == 0:\n            checkin(i, lossAll)\n        \n        loss = sum(lossAll)\n        loss.backward()\n        opt.step()\n        with torch.no_grad():\n            z.copy_(z.maximum(z_min).minimum(z_max))\n        return image\n    i = 0\n    try:\n        with tqdm() as pbar:\n            while True:\n                image = train(i)\n                if i == max_iterations:\n                    break\n                i += 1\n                pbar.update()\n    except KeyboardInterrupt:\n        pass\n    writer = imageio.get_writer(video_file + '.mp4', fps=20)\n    for im in all_frames:\n        writer.append_data(np.array(im))\n    writer.close()\n    # all_frames[0].save('out.gif',\n              # save_all=True, append_images=all_frames[1:], optimize=False, duration=80, loop=0)\n    return image","bec6b6f9":"def load_image( infilename ) :\n    img = Image.open( infilename )\n    img.load()\n    data = np.asarray( img, dtype=\"int32\" )\n    return data","e712aabd":"def display_result(img) :\n    plt.figure(figsize=(9,9))\n    plt.imshow(img)\n    plt.axis('off')","d9d96069":"img = inference(\n    text = 'singing monster', \n    seed = 2468219,\n    step_size = 0.14,\n    max_iterations = 300,\n    width = 512,\n    height = 512,\n    init_image = '',\n    init_weight = 0.004, \n    target_images = '', \n    cutn = 64,\n    cut_pow = 1.0,\n    video_file = \"test1\"\n)\ndisplay_result(img)","5c0d26af":"from IPython.display import HTML\nfrom base64 import b64encode\nmp4 = open('test1.mp4','rb').read()\ndata_url = \"data:video\/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=500 loop=\"true\" autoplay=\"autoplay\" controls muted>\n      <source src=\"%s\" type=\"video\/mp4\">\n<\/video>\n\"\"\" % data_url)","71bbb5fc":"img = inference(\n    text = 'cosmic crystals of jelly and fire flowers', \n    seed = 1643894,\n    step_size = 0.14,\n    max_iterations = 300,\n    width = 512,\n    height = 512,\n    init_image = '',\n    init_weight = 0.04,\n    target_images = '', \n    cutn = 64,\n    cut_pow = 1.0,\n    video_file = \"test2\"\n)\ndisplay_result(img)","f0797b79":"mp4 = open('test2.mp4','rb').read()\ndata_url = \"data:video\/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=500 loop=\"true\" autoplay=\"autoplay\" controls muted>\n      <source src=\"%s\" type=\"video\/mp4\">\n<\/video>\n\"\"\" % data_url)","5b7d6210":"img = inference(\n    text = 'cyberpunk city',\n    seed = 967,\n    step_size = 0.14,\n    max_iterations = 700,\n    width = 512,\n    height = 512,\n    init_image = '', #BASE_PATH + '1024613706.jpg',\n    init_weight = 0.0,\n    target_images = '', #BASE_PATH + '101559400.jpg',\n    cutn = 64,\n    cut_pow = 1.0,\n    video_file = \"test3\"\n)\ndisplay_result(img)","765a58cb":"mp4 = open('test3.mp4','rb').read()\ndata_url = \"data:video\/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=500 loop=\"true\" autoplay=\"autoplay\" controls muted>\n      <source src=\"%s\" type=\"video\/mp4\">\n<\/video>\n\"\"\" % data_url)","9653466f":"img = inference(\n    text = 'a home in space and time', \n    seed = 27560,\n    step_size = 0.14,\n    max_iterations = 700,\n    width = 512,\n    height = 512,\n    init_image = '',\n    init_weight = 0.0,\n    target_images = '', \n    cutn = 3,\n    cut_pow = 1.0,\n    video_file = \"test4\"\n)\ndisplay_result(img)","a33aa38b":"mp4 = open('test4.mp4','rb').read()\ndata_url = \"data:video\/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=500 loop=\"true\" autoplay=\"autoplay\" controls muted>\n      <source src=\"%s\" type=\"video\/mp4\">\n<\/video>\n\"\"\" % data_url)","ccaefe9c":"img = inference(\n    text = 'castle made of honey',\n    seed = 100,\n    step_size = 0.14,\n    max_iterations = 700,\n    width = 512,\n    height = 512,\n    init_image = '',\n    init_weight = 0.0, \n    target_images = 'cabin.jpeg',\n    cutn = 32,\n    cut_pow = 1.0,\n    video_file = \"test5\"\n)\ndisplay_result(img)","0a52801e":"mp4 = open('test5.mp4','rb').read()\ndata_url = \"data:video\/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=500 loop=\"true\" autoplay=\"autoplay\" controls muted>\n      <source src=\"%s\" type=\"video\/mp4\">\n<\/video>\n\"\"\" % data_url)","38f05fbd":"img = inference(\n    text = 'flood in the streets of neon',\n    seed =  6163297225029052913,\n    step_size = 0.14,\n    max_iterations = 700,\n    width = 512,\n    height = 512,\n    init_image = '',\n    init_weight = 0.0,\n    target_images = '',\n    cutn = 32,\n    cut_pow = 1.0,\n    video_file = \"test6\"\n)\ndisplay_result(img)","25625a32":"mp4 = open('test6.mp4','rb').read()\ndata_url = \"data:video\/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=500 loop=\"true\" autoplay=\"autoplay\" controls muted>\n      <source src=\"%s\" type=\"video\/mp4\">\n<\/video>\n\"\"\" % data_url)","6e9ec86d":"img = inference(\n    text = 'want to go winter', \n    seed = 191,\n    step_size = 0.14,\n    max_iterations = 700,\n    width = 512,\n    height = 512,\n    init_image = '',\n    init_weight = 0.0,\n    target_images = '',\n    cutn = 32,\n    cut_pow = 1.0,\n    video_file = \"test7\"\n)\ndisplay_result(img)","a90688b1":"mp4 = open('test7.mp4','rb').read()\ndata_url = \"data:video\/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=500 loop=\"true\" autoplay=\"autoplay\" controls muted>\n      <source src=\"%s\" type=\"video\/mp4\">\n<\/video>\n\"\"\" % data_url)","61d92c8a":"img = inference(\n    text = 'Fireflies in the Garden', \n    seed = 201,\n    step_size = 0.12,\n    max_iterations = 400,\n    width = 512,\n    height = 512,\n    init_image = '',\n    init_weight = 0.0,\n    target_images = '',\n    cutn = 64,\n    cut_pow = 1.0,\n    video_file = \"test8\"\n)\ndisplay_result(img)","dfed6915":"mp4 = open('test8.mp4','rb').read()\ndata_url = \"data:video\/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=500 loop=\"true\" autoplay=\"autoplay\" controls muted>\n      <source src=\"%s\" type=\"video\/mp4\">\n<\/video>\n\"\"\" % data_url)","654de97d":"img = inference(\n    text = 'Angels of the Universe',\n    seed = 1011, \n    step_size = 0.12,\n    max_iterations = 700,\n    width = 512,\n    height = 512,\n    init_image = BASE_PATH +'1067180831.jpg',\n    init_weight = 0.0,\n    target_images = '',\n    cutn = 32,\n    cut_pow = 1.0,\n    video_file = \"test9\"\n)\ndisplay_result(img)","207e940e":"mp4 = open('test9.mp4','rb').read()\ndata_url = \"data:video\/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=500 loop=\"true\" autoplay=\"autoplay\" controls muted>\n      <source src=\"%s\" type=\"video\/mp4\">\n<\/video>\n\"\"\" % data_url)","f0126ea1":"img = inference(\n    text = 'rainy horror shore',\n    seed = 50122,\n    step_size = 0.12,\n    max_iterations = 700,\n    width = 512,\n    height = 512,\n    init_image = '',\n    init_weight = 0.0,\n    target_images = '',\n    cutn = 64,\n    cut_pow = 1.0,\n    video_file = \"test10\"\n)\ndisplay_result(img)","0fbfd00a":"mp4 = open('test10.mp4','rb').read()\ndata_url = \"data:video\/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=500 loop=\"true\" autoplay=\"autoplay\" controls muted>\n      <source src=\"%s\" type=\"video\/mp4\">\n<\/video>\n\"\"\" % data_url)","0a24c85f":"img = inference(\n    text = 'mutation tree and flower',\n    seed =  79472135470,\n    step_size = 0.12,\n    max_iterations = 300,\n    width = 512,\n    height = 512,\n    init_image = '',\n    init_weight = 0.0,\n    target_images = '',\n    cutn = 32,\n    cut_pow = 1.0,\n    video_file = \"test11\"\n)\ndisplay_result(img)","a5f3afe3":"mp4 = open('test11.mp4','rb').read()\ndata_url = \"data:video\/mp4;base64,\" + b64encode(mp4).decode()\nHTML(\"\"\"\n<video width=500 loop=\"true\" autoplay=\"autoplay\" controls muted>\n      <source src=\"%s\" type=\"video\/mp4\">\n<\/video>\n\"\"\" % data_url)","2e962f22":"<h3 style='background:black; border:5; color:#F2EA02; border-radius:40px 40px; font-size:200%'><center>Example - 4<\/center><\/h3>","5c9dd560":"<h3 style='background:black; border:5; color:#F2EA02; border-radius:40px 40px; font-size:200%'><center>Example - 5<\/center><\/h3>","4c0cb6ed":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: black; color :#fe346e; border-radius: 100px 100px; text-align:center\">References<\/h1><\/span>\n\n* [VQGAN_CLIP( hHugging Face )](https:\/\/huggingface.co\/spaces\/akhaliq\/VQGAN_CLIP)\n* [The Illustrated VQGAN](https:\/\/ljvmiranda921.github.io\/notebook\/2021\/08\/08\/clip-vqgan\/)\n* [VQGAN+CLIP \u2014 How does it work?](https:\/\/alexasteinbruck.medium.com\/vqgan-clip-how-does-it-work-210a5dca5e52)\n* [Image Generation Based on Abstract Concepts Using CLIP + BigGAN](https:\/\/wandb.ai\/gudgud96\/big-sleep-test\/reports\/Image-Generation-Based-on-Abstract-Concepts-Using-CLIP-BigGAN--Vmlldzo1MjA2MTE)\n","662a7eec":"<h3 style='background:black; border:5; color:#F2EA02; border-radius:40px 40px; font-size:200%'><center>Example - 3<\/center><\/h3>","eeff1640":"<h3 style='background:black; border:5; color:#F2EA02; border-radius:40px 40px; font-size:200%'><center>Example - 9<\/center><\/h3>","1dbc848d":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: black; color :#fe346e; border-radius: 100px 100px; text-align:center\">Introduction<\/h1><\/span>\n\n**VQGAN** stands for **Vector Quantized Generative Adversarial Network**, while **CLIP** stands for **Contrastive Image-Language Pretraining**. Whenever we say VQGAN-CLIP1, we refer to the interaction between these two networks. They\u2019re separate models that work in tandem. The way they work is that VQGAN generates the images, while CLIP judges how well an image matches our text prompt. This interaction guides our generator to produce more accurate images.\n___","a6cedef1":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: black; color :#fe346e; border-radius: 100px 100px; text-align:center\">Playing with VQGAN + CLIP<\/h1><\/span>\n\n![intro](https:\/\/i.imgur.com\/xRnuGGO.png)\n\n---","426ca3d4":"<h3 style='background:black; border:5; color:#F2EA02; border-radius:40px 40px; font-size:200%'><center>Example - 10<\/center><\/h3>","e104e7b1":"<h3 style='background:black; border:5; color:#F2EA02; border-radius:40px 40px; font-size:200%'><center>Example - 1<\/center><\/h3>","563ee64d":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: black; color :#fe346e; border-radius: 100px 100px; text-align:center\">How do VQGAN and CLIP work together<\/h1><\/span>\n\nCLIP guides VQGAN towards an image that is the best match to a given text. CLIP is the \u201cPerceptor\u201d and VQGAN is the \u201cGenerator\u201d. VQGAN like all GANs VQGAN takes in a noise vector, and outputs a (realistic) image. CLIP on the other hand takes in an image and text, and outputs the image features and text features respectively. The similarity between image and text can be represented by the cosine similarity of the learnt feature vectors.\n\nBy leveraging CLIPs capacities as a \u201csteering wheel\u201d, we can use CLIP to guide a search through VQGAN\u2019s latent space to find images that match a text prompt very well according to CLIP.\n\nsource - [VQGAN+CLIP \u2014 How does it work?](https:\/\/alexasteinbruck.medium.com\/vqgan-clip-how-does-it-work-210a5dca5e52)\n\n![VQGAN+CLIP \u2014 How does it work?](https:\/\/ljvmiranda921.github.io\/assets\/png\/vqgan\/clip_vqgan_with_image.png)\n\nImage source - [The Illustrated VQGAN](https:\/\/ljvmiranda921.github.io\/assets\/png\/vqgan\/clip_vqgan_with_image.png)\n___","68f1976c":"<h3 style='background:black; border:5; color:#F2EA02; border-radius:40px 40px; font-size:200%'><center>Example - 2<\/center><\/h3>","408c317d":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: black; color :#fe346e; border-radius: 100px 100px; text-align:center\">Run<\/h1><\/span>","0f36a8a6":"<h3 style='background:black; border:5; color:#F2EA02; border-radius:40px 40px; font-size:200%'><center>Example - 7<\/center><\/h3>","ec0d2c5c":"<h3 style='background:black; border:5; color:#F2EA02; border-radius:40px 40px; font-size:200%'><center>Example - 11<\/center><\/h3>","137d7ee3":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: black; color :#fe346e; border-radius: 100px 100px; text-align:center\">Thank you<\/h1><\/span>\n\n<p style=\"font-size:28px;\">Please do consider <span style=\"color:red\">Upvoting<\/span> and <span style=\"color:red\">Sharing<\/span> this Notebook if you find it informative and insightful!<\/p>\n<p style=\"font-size:28px;\">Also, let me know your favourite artwork in the comments.<\/p>","78f8452d":"<h3 style='background:black; border:5; color:#F2EA02; border-radius:40px 40px; font-size:200%'><center>Example - 6<\/center><\/h3>","dd344de9":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: black; color :#fe346e; border-radius: 100px 100px; text-align:center\">Helper functions<\/h1><\/span>","03813c42":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: black; color :#fe346e; border-radius: 100px 100px; text-align:center\">CLIP<\/h1><\/span>\n\n**CLIP( Contrastive Language\u2013Image Pre-training )** a model trained to determine which caption from a set of captions best fits with a given image.\n\nOpenAI's CLIP model aims to learn generic visual concept with natural language supervision. This is because sandard computer vision model only work well on specific task, and require significant effort to adapt to a new task, hence have weak generalization capabilities. CLIP bridges the gap via learning directly from raw text about images at a web scale level.\nCLIP does not directly optimize for the performance of a benchmark task (e.g. CIFAR), so as to keep its \"zero-shot\" capabilities for generalization. More interestingly, CLIP shows that scaling a simple pre-training task - which is to learn \"which text matches with which image\", is sufficient to achieve competitive zero-shot performance on many image classification datasets. \n\n![clipa](https:\/\/openaiassets.blob.core.windows.net\/$web\/clip\/draft\/20210104b\/overview-a.svg)\n\n![clipb](https:\/\/openaiassets.blob.core.windows.net\/$web\/clip\/draft\/20210104b\/overview-b.svg)\n\nCLIP trains a text encoder (Bag-of-Words or Text Transformer) and an image encoder (ResNet or Image Transformer) which learns feature representations of a given pair of text and image. The scaled cosine similarity matrix of the image and text feature is computed, and the diagonal values are minimized to force the image feature match its corresponding text feature.\n\n___","bc80b739":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: black; color :#fe346e; border-radius: 100px 100px; text-align:center\">Download models<\/h1><\/span>","fabbb873":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: black; color :#fe346e; border-radius: 100px 100px; text-align:center\">VQGAN<\/h1><\/span>\n\n**VQGAN** employs  two-stage structure by learning an intermediary representation before feeding it to a transformer. However, instead of downsampling the image, VQGAN uses a codebook to represent visual parts. The authors did not model the image from a pixel-level directly, but instead from the codewords of the learned codebook.\n\n![vqgan](https:\/\/compvis.github.io\/taming-transformers\/paper\/teaser.png)\n\n\nVQGAN was able to solve Transformer\u2019s scaling problem by using an intermediate representation known as a codebook. This codebook serves as the bridge for the two-stage approach found in most image transformer techniques. The VQGAN learns a codebook of context-rich visual parts, whose composition is then modeled with an autoregressive transformer.\n\nThe codebook is generated through a process called vector quantization (VQ), i.e., the \u201cVQ\u201d part of \u201cVQGAN.\u201d Vector quantization is a signal processing technique for encoding vectors. It represents all visual parts found in the convolutional step in a quantized form, making it less computationally expensive once passed to a transformer network.\n\nOne can think of vector quantization as a process of dividing vectors into groups that have approximately the same number of points closest to them. Each group is then represented by a centroid (codeword), usually obtained via k-means or any other clustering algorithm. In the end, one learns a dictionary of centroids (codebook) and their corresponding members.\n___","bbd3bfe8":"<h3 style='background:black; border:5; color:#F2EA02; border-radius:40px 40px; font-size:200%'><center>Example - 8<\/center><\/h3>","2186aa35":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: black; color :#fe346e; border-radius: 100px 100px; text-align:center\">Inference<\/h1><\/span>","593f348b":"# <span><h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: black; color :#fe346e; border-radius: 100px 100px; text-align:center\">Import Libraries<\/h1><\/span>"}}