{"cell_type":{"7ff26b1c":"code","3f256000":"code","b056b59d":"code","f07959a9":"code","0440a877":"code","97b7427b":"code","ad92c333":"code","a9ad2d7c":"code","d997d658":"code","72499a8c":"code","47e0a768":"code","51631a7b":"code","4dfeeee1":"code","b9e622f8":"code","7514d30d":"code","152b04df":"code","395e08ed":"code","ef96adcb":"code","518e8424":"code","e259d591":"code","15ea9a3e":"code","c7d5e103":"code","23a11cba":"code","52047f05":"code","21a090f7":"code","047043ed":"code","dda598cf":"code","600e3cc3":"code","7d073799":"code","933af9ef":"code","d2e58e4f":"code","ae6fc34e":"code","ac8d219b":"code","eee0918e":"code","1da90b72":"code","243790da":"code","27d7f9d3":"code","51f8c26f":"code","3ca6dc36":"code","51c5e00e":"code","4bd9ce60":"code","ad05f90e":"code","c09378ff":"code","18afdccf":"code","14a8d241":"code","c102791a":"code","93671638":"code","81052db1":"code","75d3be94":"code","5d684a18":"code","bd22be6a":"code","f2d04626":"code","dce1ca8b":"code","26de99de":"code","146527d3":"code","e66a1115":"code","a6a52f99":"code","7dfc80af":"code","002ddc9e":"code","4562a595":"code","07a7c7dc":"code","fe9a4137":"code","d37751a9":"code","9e256632":"code","b4883ea8":"code","c0fac635":"code","3a8d6128":"code","70db22fe":"code","70bb33fd":"code","9a9357d0":"code","8a84eaab":"code","90fde122":"code","bdc74266":"code","2e98377e":"code","3e093f6b":"code","ab404781":"code","706750d6":"code","7a6cb3c7":"code","f57d0da0":"code","98a456d5":"code","91725515":"code","dfd1892a":"code","0f7ce5c4":"code","0af0bf3d":"code","37ab7b7c":"code","bd597223":"code","3901f1db":"code","5e0b6f60":"code","04ae6609":"code","578a161b":"code","0bb2c3fd":"code","863f0f28":"code","92138a0b":"code","f8f3c2db":"code","ae378819":"code","50e3a1e5":"code","3d5a59c8":"code","28bc069b":"code","e2cbc7ca":"code","dfcac619":"code","1d7db9b7":"code","74e66c9c":"code","dc9f6328":"code","0ef65190":"code","025d4c20":"code","09e714b3":"code","ee08ded5":"code","fd6c113c":"code","3a07b3a0":"code","e53d084b":"code","aa9864be":"code","653144ad":"code","bdaab6aa":"code","ddefd477":"code","7c49d2b0":"code","8dd1eaec":"code","3b33407e":"code","952db55d":"code","895fd396":"code","5561c0df":"code","d4e07b65":"code","e8eb49f3":"code","c76ad181":"code","5e69de13":"code","00782f22":"code","ebfe609d":"code","feadda07":"code","ed6ec89b":"code","51e77a62":"code","d25456c0":"code","cf6dcadf":"code","eef6a6fc":"code","33e03b7b":"code","38866373":"code","b563017a":"markdown","7efd9356":"markdown","56642914":"markdown","94173377":"markdown","be24e7c3":"markdown","417e556a":"markdown","ddc1e5bd":"markdown","49c7e091":"markdown","200ac12e":"markdown","c56d643f":"markdown","04b88656":"markdown","fc3e7c83":"markdown","db6a8a70":"markdown","02de021d":"markdown","5036b11b":"markdown","eea164f4":"markdown","4c864d54":"markdown","d9e2c522":"markdown","dc44c818":"markdown","f99bf146":"markdown","a5e75c08":"markdown","68f50d19":"markdown","dc8a66c4":"markdown","5d5d8efb":"markdown","5dc16c3e":"markdown","9780dac5":"markdown","7650a5b3":"markdown","ab4e581a":"markdown","f785b1c5":"markdown","ba01996b":"markdown","8a34a50e":"markdown","43efa55f":"markdown","bab4b515":"markdown","82d7dc3a":"markdown","4bc20c2b":"markdown","886ca21e":"markdown","260ef884":"markdown","e8a53f65":"markdown","1bb16b06":"markdown","e934c306":"markdown","cd29e750":"markdown","0354e172":"markdown","c4c5db83":"markdown","4cf51b66":"markdown","e2f58eae":"markdown","ef56c339":"markdown","5cf16f51":"markdown","6a51b4b5":"markdown","e8b1e817":"markdown","b9487e3a":"markdown","72c6e670":"markdown","df9df701":"markdown","34cf1ea2":"markdown","4ed70fd1":"markdown","2fabdb86":"markdown","1e637b69":"markdown","7f7b17ec":"markdown","5cea8bd5":"markdown","26f2ce8a":"markdown","6541ad2c":"markdown","5ffec460":"markdown","e085bbc3":"markdown","4fc7311f":"markdown","6faa5c33":"markdown","a712b20f":"markdown","09042dc3":"markdown","b5eaa601":"markdown","9caec59c":"markdown","36929dfc":"markdown","011765db":"markdown","0cb4b142":"markdown","8f6e4e48":"markdown","11bbafd9":"markdown"},"source":{"7ff26b1c":"# Python Imports\n!pip install catboost\n!pip install xgboost\n\nimport pandas as pd\nimport numpy as np # Linear Algebra\nimport random, time, datetime\n\n# Data visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Data Preprocessing\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import RobustScaler, MinMaxScaler, StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.decomposition import PCA\n\n# Machine Learning\nfrom sklearn import model_selection, tree, preprocessing, metrics\nfrom sklearn import linear_model\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor, LGBMClassifier\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom catboost import Pool, CatBoostRegressor\nfrom sklearn.svm import LinearSVC\nfrom sklearn import svm\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost.sklearn import XGBClassifier\nimport xgboost as xgb\n\n# Validation & Scoring\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold\nfrom sklearn import svm\n\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score, mean_squared_error, make_scorer, mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nsns.set_style(\"whitegrid\")\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","3f256000":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b056b59d":"american = pd.read_csv(\"\/kaggle\/input\/american-bank-data\/Bank_of_America_data.csv\")","f07959a9":"american.shape","0440a877":"american.head()","97b7427b":"american.tail()","ad92c333":"american.describe()","a9ad2d7c":"american.isna().sum()","d997d658":"american.boxplot(figsize=(10,7));","72499a8c":"#sns.pairplot(american, hue = \"BAD\");","47e0a768":"correlation = abs(american.corr())\n# Generate a mask for the upper triangle\nmask = np.zeros_like(correlation, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(correlation, mask=mask, cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True);","51631a7b":"with sns.axes_style('white'):\n    sns.jointplot(\"MORTDUE\", \"VALUE\", american, kind='reg', color=\"purple\")","4dfeeee1":"#american.groupby(by=\"JOB\").mean()[\"DEBTINC\"].plot.bar();","b9e622f8":"sns.barplot(x=\"JOB\", y=\"DEBTINC\", data=american);","7514d30d":"#american.groupby(by=\"JOB\").sum()[\"DEBTINC\"].plot.bar();","152b04df":"sns.barplot(x=\"JOB\", y=\"DEBTINC\", data=american, estimator=sum);","395e08ed":"#american.groupby(by=\"JOB\").mean()[\"LOAN\"].plot.bar();","ef96adcb":"sns.barplot(x=\"JOB\", y=\"LOAN\", data=american);","518e8424":"#american.groupby(by=\"JOB\").mean()[\"VALUE\"].plot.bar();","e259d591":"#american.groupby(by=\"JOB\").mean()[\"LOAN\"].plot.bar();\nsns.barplot(x=\"JOB\", y=\"VALUE\", data=american);","15ea9a3e":"american.groupby(by=\"JOB\").median()[\"LOAN\"].plot.bar();\n#sns.barplot(x=\"JOB\", y=\"VALUE\", data=american, estimator=sum);","c7d5e103":"effectifs = american[\"JOB\"].value_counts()\nmodalites = effectifs.index # The index of the effectifs contains the modalities\n\ntab = pd.DataFrame(modalites, columns = [\"JOB\"]) # Creating an array with the modalities\ntab[\"Effectif\"] = effectifs.values\n#tab[\"f\"] = tab[\"n\"] \/ len(american) # len(data) returns the size of the sample\ntab[\"Mean_loan\"] = american.groupby(by=\"JOB\").mean()[\"LOAN\"].values\ntab[\"Mean_value\"] = american.groupby(by=\"JOB\").mean()[\"VALUE\"].values\ntab[\"Difference\"] = tab[\"Mean_value\"] - tab[\"Mean_loan\"]\ntab","23a11cba":"sns.lineplot(x=\"Mean_value\", y=\"Mean_loan\", data=tab);","52047f05":"sns.barplot(x=\"REASON\", y=\"DEBTINC\", data=american);","21a090f7":"sns.barplot(x=\"REASON\", y=\"VALUE\", data=american);","047043ed":"sns.barplot(x=\"REASON\", y=\"LOAN\", data=american);","dda598cf":"american[american[\"REASON\"] == \"DebtCon\"][\"JOB\"].value_counts(normalize=True).plot(kind='pie');","600e3cc3":"american[american[\"REASON\"] == \"HomeImp\"][\"JOB\"].value_counts(normalize=True).plot(kind='pie');","7d073799":"sns.boxplot(data=american, x=\"JOB\", y=\"VALUE\");","933af9ef":"american[\"BAD\"].unique()","d2e58e4f":"american[\"BAD\"].value_counts()","ae6fc34e":"american[\"BAD\"].hist();","ac8d219b":"american[\"BAD\"].value_counts(normalize=True).plot(kind='pie');","eee0918e":"american[[\"LOAN\"]].boxplot();","1da90b72":"american[[\"LOAN\"]].hist();","243790da":"american[\"LOAN\"].describe()","27d7f9d3":"american[\"MORTDUE\"].isna().sum()","51f8c26f":"american[[\"MORTDUE\"]].boxplot();","3ca6dc36":"american[\"MORTDUE\"].hist();","51c5e00e":"american[\"VALUE\"].isna().sum()","4bd9ce60":"american[[\"VALUE\"]].boxplot();","ad05f90e":"american[\"VALUE\"].hist();","c09378ff":"american[\"REASON\"].isna().sum()","18afdccf":"american[\"REASON\"].unique()","14a8d241":"american.groupby(\"REASON\").size().plot(kind='bar');","c102791a":"american[\"REASON\"].value_counts()","93671638":"sns.countplot(data=american, x=\"REASON\");","81052db1":"american[\"REASON\"].value_counts(normalize=True).plot(kind='pie');","75d3be94":"american[\"JOB\"].isna().sum()","5d684a18":"american[\"JOB\"].unique()","bd22be6a":"american.groupby(\"JOB\").size().plot(kind='bar');","f2d04626":"american[\"JOB\"].value_counts()","dce1ca8b":"sns.countplot(data=american, x=\"JOB\");","26de99de":"american[\"JOB\"].value_counts(normalize=True).plot(kind='pie');","146527d3":"american[\"YOJ\"].isna().sum()","e66a1115":"american[[\"YOJ\"]].boxplot();","a6a52f99":"american[[\"YOJ\"]].hist();","7dfc80af":"american[\"DEROG\"].isna().sum()","002ddc9e":"american[[\"DEROG\"]].boxplot();","4562a595":"american[[\"DEROG\"]].hist(); # This expplains the outlier values !","07a7c7dc":"american[\"DELINQ\"].isna().sum()","fe9a4137":"american[[\"DELINQ\"]].boxplot()","d37751a9":"american[[\"DELINQ\"]].hist()","9e256632":"american[\"DELINQ\"].value_counts()","b4883ea8":"american[\"CLAGE\"].isna().sum()","c0fac635":"american[[\"CLAGE\"]].boxplot();","3a8d6128":"american[[\"CLAGE\"]].hist();","70db22fe":"american[[\"NINQ\"]].boxplot();","70bb33fd":"american[[\"NINQ\"]].hist();","9a9357d0":"american[\"NINQ\"].value_counts()","8a84eaab":"american[[\"NINQ\"]].boxplot();","90fde122":"american[[\"NINQ\"]].hist();","bdc74266":"american[\"NINQ\"].value_counts()","2e98377e":"american[[\"DEBTINC\"]].boxplot();","3e093f6b":"american[[\"DEBTINC\"]].hist();","ab404781":"with_median = make_pipeline(\n    SimpleImputer(missing_values=np.nan, strategy='median')\n)\nmedian_vars = [\"LOAN\", \"MORTDUE\", \"VALUE\", \"DEBTINC\", \"CLAGE\"]\namerican[median_vars] = with_median.fit_transform(american[median_vars])","706750d6":"# Converting Discrete columns to type object\ndiscrete_columns = [\"REASON\", \"JOB\", \"DEROG\", \"DELINQ\", \"NINQ\", \"CLNO\", \"YOJ\"]\namerican[discrete_columns] = american[discrete_columns].astype(object)","7a6cb3c7":"american.dtypes","f57d0da0":"discrete_columns","98a456d5":"most_frequent = make_pipeline(\n    SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n)\nmost_frquent_value_vars = discrete_columns\namerican[discrete_columns] = most_frequent.fit_transform(american[discrete_columns])","91725515":"american.head()","dfd1892a":"american.isna().sum()","0f7ce5c4":"std_scaling = make_pipeline(\n    StandardScaler()\n)\nstd_vars = [\"LOAN\", \"MORTDUE\", \"VALUE\", \"CLAGE\", \"DEBTINC\"]\namerican[std_vars] = std_scaling.fit_transform(american[std_vars])\n\nminmax_scaling = make_pipeline(\n    MinMaxScaler()\n)\nminmax_vars = [\"YOJ\"]\namerican[minmax_vars] = minmax_scaling.fit_transform(american[minmax_vars])","0af0bf3d":"american.head()","37ab7b7c":"american2 = pd.read_csv(\"\/kaggle\/input\/american-bank-data\/Bank_of_America_data.csv\")\nwith_median = make_pipeline(\n    SimpleImputer(missing_values=np.nan, strategy='median')\n)\nmedian_vars = [\"LOAN\", \"MORTDUE\", \"VALUE\", \"DEBTINC\", \"CLAGE\"]\namerican2[median_vars] = with_median.fit_transform(american2[median_vars])\n# Converting Discrete columns to type object\ndiscrete_columns = [\"REASON\", \"JOB\", \"DEROG\", \"DELINQ\", \"NINQ\", \"CLNO\", \"YOJ\"]\namerican2[discrete_columns] = american2[discrete_columns].astype(object)\nmost_frequent = make_pipeline(\n    SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n)\nmost_frquent_value_vars = discrete_columns\namerican2[discrete_columns] = most_frequent.fit_transform(american2[discrete_columns])\nstd_scaling = make_pipeline(\n    RobustScaler()\n)\nstd_vars = [\"LOAN\", \"MORTDUE\", \"VALUE\", \"CLAGE\", \"DEBTINC\"]\namerican2[std_vars] = std_scaling.fit_transform(american2[std_vars])\n\nminmax_scaling = make_pipeline(\n    MinMaxScaler()\n)\nminmax_vars = [\"YOJ\"]\namerican2[minmax_vars] = minmax_scaling.fit_transform(american2[minmax_vars])","bd597223":"american3 = pd.read_csv(\"\/kaggle\/input\/american-bank-data\/Bank_of_America_data.csv\")\nwith_median = make_pipeline(\n    SimpleImputer(missing_values=np.nan, strategy='mean')\n)\nmedian_vars = [\"LOAN\", \"MORTDUE\", \"VALUE\", \"DEBTINC\", \"CLAGE\"]\namerican3[median_vars] = with_median.fit_transform(american3[median_vars])\n# Converting Discrete columns to type object\ndiscrete_columns = [\"REASON\", \"JOB\", \"DEROG\", \"DELINQ\", \"NINQ\", \"CLNO\", \"YOJ\"]\namerican3[discrete_columns] = american3[discrete_columns].astype(object)\nmost_frequent = make_pipeline(\n    SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n)\nmost_frquent_value_vars = discrete_columns\namerican3[discrete_columns] = most_frequent.fit_transform(american3[discrete_columns])\nstd_scaling = make_pipeline(\n    StandardScaler()\n)\nstd_vars = [\"LOAN\", \"MORTDUE\", \"VALUE\", \"CLAGE\", \"DEBTINC\"]\namerican3[std_vars] = std_scaling.fit_transform(american3[std_vars])\n\nminmax_scaling = make_pipeline(\n    MinMaxScaler()\n)\nminmax_vars = [\"YOJ\"]\namerican3[minmax_vars] = minmax_scaling.fit_transform(american3[minmax_vars])","3901f1db":"american4 = pd.read_csv(\"\/kaggle\/input\/american-bank-data\/Bank_of_America_data.csv\")\nwith_median = make_pipeline(\n    SimpleImputer(missing_values=np.nan, strategy='mean')\n)\nmedian_vars = [\"LOAN\", \"MORTDUE\", \"VALUE\", \"DEBTINC\", \"CLAGE\"]\namerican4[median_vars] = with_median.fit_transform(american4[median_vars])\n# Converting Discrete columns to type object\ndiscrete_columns = [\"REASON\", \"JOB\", \"DEROG\", \"DELINQ\", \"NINQ\", \"CLNO\", \"YOJ\"]\namerican4[discrete_columns] = american4[discrete_columns].astype(object)\nmost_frequent = make_pipeline(\n    SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n)\nmost_frquent_value_vars = discrete_columns\namerican4[discrete_columns] = most_frequent.fit_transform(american4[discrete_columns])\nstd_scaling = make_pipeline(\n    RobustScaler()\n)\nstd_vars = [\"LOAN\", \"MORTDUE\", \"VALUE\", \"CLAGE\", \"DEBTINC\"]\namerican4[std_vars] = std_scaling.fit_transform(american4[std_vars])\n\nminmax_scaling = make_pipeline(\n    MinMaxScaler()\n)\nminmax_vars = [\"YOJ\"]\namerican4[minmax_vars] = minmax_scaling.fit_transform(american4[minmax_vars])","5e0b6f60":"correlation = abs(american.corr())\nmask = np.zeros_like(correlation, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(correlation, mask=mask, cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True);","04ae6609":"correlation2 = abs(american2.corr())\nmask = np.zeros_like(correlation2, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(correlation2, mask=mask, cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True);","578a161b":"correlation3 = abs(american3.corr())\nmask = np.zeros_like(correlation3, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(correlation3, mask=mask, cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True);","0bb2c3fd":"correlation4 = abs(american4.corr())\nmask = np.zeros_like(correlation4, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(correlation4, mask=mask, cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True);","863f0f28":"american = pd.read_csv(\"\/kaggle\/input\/american-bank-data\/Bank_of_America_data.csv\")\nwith_median = make_pipeline(\n    SimpleImputer(missing_values=np.nan, strategy='median')\n)\nmedian_vars = [\"LOAN\", \"MORTDUE\", \"VALUE\", \"CLAGE\"]\namerican[median_vars] = with_median.fit_transform(american[median_vars])\ndiscrete_columns = [\"REASON\", \"JOB\", \"DEROG\", \"DELINQ\", \"NINQ\", \"CLNO\", \"YOJ\"]\namerican[discrete_columns] = american[discrete_columns].astype(object)\nmost_frequent = make_pipeline(\n    SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n)\nmost_frquent_value_vars = discrete_columns\namerican[discrete_columns] = most_frequent.fit_transform(american[discrete_columns])\nstd_scaling = make_pipeline(\n    StandardScaler()\n)\nstd_vars = [\"LOAN\", \"MORTDUE\", \"VALUE\", \"CLAGE\"]\namerican[std_vars] = std_scaling.fit_transform(american[std_vars])\n\nminmax_scaling = make_pipeline(\n    MinMaxScaler()\n)\nminmax_vars = [\"YOJ\"]\namerican[minmax_vars] = minmax_scaling.fit_transform(american[minmax_vars])\n\n# DEBTINC\ndf = american.copy()\n#df.drop(\"BAD\", axis=1, inplace=True)\ndf.dropna(subset=[\"DEBTINC\"], inplace=True)\ny_reg = df[\"DEBTINC\"]\ndf.drop(\"DEBTINC\", axis=1, inplace=True)\nX_reg = df.copy()\n# Converting Discrete columns to type str\ndiscrete_columns = [\"REASON\", \"JOB\", \"DEROG\", \"DELINQ\", \"NINQ\", \"CLNO\"]\nX_reg[discrete_columns] = X_reg[discrete_columns].astype(str)\nX_reg = pd.get_dummies(X_reg, drop_first=True)\n#PCA\npca = PCA(n_components=102)\nss = StandardScaler()\nss.fit(X_reg)\nX_scaled_reg = ss.transform(X_reg)\nX_transformed_pca_reg = pca.fit_transform(X_scaled_reg)\n\ndef evaluate(train,target_train):\n    results={}\n    def test_model(clf):\n        cv = KFold(n_splits=5,shuffle=True,random_state=45)\n        mse = make_scorer(mean_squared_error)\n        mse_val_score = cross_val_score(clf, train, target_train, cv=cv,scoring=mse)\n        mse_val_score = np.sqrt(mse_val_score)\n        scores=[mse_val_score.mean()]\n        return scores\n    clf = linear_model.LinearRegression()\n    results[\"Linear\"]=test_model(clf)\n    clf = linear_model.Ridge()\n    results[\"Ridge\"]=test_model(clf)\n    clf = linear_model.BayesianRidge()\n    results[\"Bayesian Ridge\"]=test_model(clf)\n    clf = RandomForestRegressor()\n    results[\"RandomForest\"]=test_model(clf)\n    clf = AdaBoostRegressor()\n    results[\"AdaBoost\"]=test_model(clf)\n    clf = xgb.XGBRegressor()\n    results[\"XGB\"]=test_model(clf)\n    clf = LGBMRegressor()\n    results[\"LBGMRegressor\"]=test_model(clf)\n    results = pd.DataFrame.from_dict(results,orient='index')\n    results.columns=[\"RMSE\"] \n    results.sort_values('RMSE', inplace=True)\n    #results=results.sort(columns=[\"R Square Score\"],ascending=False)\n    results.plot(kind=\"bar\",title=\"Model Scores\")\n    axes = plt.gca()\n    axes.set_ylim([0.5,20])\n    return results\n\nevaluate(X_reg,y_reg)","92138a0b":"american5 = pd.read_csv(\"\/kaggle\/input\/american-bank-data\/Bank_of_America_data.csv\")\nwith_median = make_pipeline(\n    SimpleImputer(missing_values=np.nan, strategy='median')\n)\nmedian_vars = [\"LOAN\", \"MORTDUE\", \"VALUE\", \"CLAGE\"]\namerican5[median_vars] = with_median.fit_transform(american5[median_vars])\ndiscrete_columns = [\"DEROG\", \"DELINQ\", \"NINQ\", \"CLNO\", \"YOJ\"]\namerican5[discrete_columns] = american5[discrete_columns].astype(object)\nmost_frequent = make_pipeline(\n    SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n)\nmost_frquent_value_vars = discrete_columns\namerican5[discrete_columns] = most_frequent.fit_transform(american5[discrete_columns])\nstd_scaling = make_pipeline(\n    StandardScaler()\n)\nstd_vars = [\"LOAN\", \"MORTDUE\", \"VALUE\", \"CLAGE\"]\namerican5[std_vars] = std_scaling.fit_transform(american5[std_vars])\n\nminmax_scaling = make_pipeline(\n    MinMaxScaler()\n)\nminmax_vars = [\"YOJ\"]\namerican5[minmax_vars] = minmax_scaling.fit_transform(american5[minmax_vars])\n# Encoding for regression model only\n  # Job\namerican5[\"JOB\"].fillna(\"Unspecified_Job\", inplace=True)\n  # Reason\namerican5[\"REASON\"].fillna(\"Unspecified_Reason\", inplace=True)\namerican5 = pd.get_dummies(american5).copy()","f8f3c2db":"american5.head()","ae378819":"df = american5.copy()\ndf.drop(\"BAD\", axis=1, inplace=True)\ndf.dropna(subset=[\"DEBTINC\"], inplace=True)\ny_debt = df[\"DEBTINC\"]\ndf.drop(\"DEBTINC\", axis=1, inplace=True)\nX_debt = df.copy()","50e3a1e5":"pd.set_option('display.max_columns', 50)\nX_debt.head()","3d5a59c8":"reg = RandomForestRegressor()\nreg.fit(X_debt, y_debt);","28bc069b":"for _,row in american5.iterrows():\n    if np.isnan(row[\"DEBTINC\"]):\n        data  = pd.Series(np.array([row[\"LOAN\"],row[\"MORTDUE\"],row[\"VALUE\"],row[\"YOJ\"],row[\"DEROG\"],row[\"DELINQ\"],\n                                   row[\"CLAGE\"],row[\"NINQ\"],row[\"CLNO\"],row[\"REASON_DebtCon\"],row[\"REASON_HomeImp\"],row[\"REASON_Unspecified_Reason\"],\n                                   row[\"JOB_Mgr\"],row[\"JOB_Office\"],row[\"JOB_Other\"],\n                                   row[\"JOB_ProfExe\"],row[\"JOB_Sales\"],row[\"JOB_Self\"], row[\"JOB_Unspecified_Job\"]]))\n        american5.loc[_,'DEBTINC'] = reg.predict([data])[0]","e2cbc7ca":"american5.isna().sum()","dfcac619":"correlation5 = american5.corr()","1d7db9b7":"matrices = [correlation, correlation2, correlation3, correlation4, correlation5]\nfor i in matrices:\n    mat = np.matrix(i)\n    print(str(sum(mat[:,1])))","74e66c9c":"data = american4.copy()","dc9f6328":"data_corr = abs(data.corr())\nmask = np.zeros_like(data_corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(data_corr, mask=mask, cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True);","0ef65190":"data.drop([\"YOJ\", \"VALUE\", \"LOAN\", \"MORTDUE\", \"CLNO\"], axis=1, inplace=True)","025d4c20":"data_corr = abs(data.corr())\nmask = np.zeros_like(data_corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(11, 9))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(data_corr, mask=mask, cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True);","09e714b3":"data.dtypes","ee08ded5":"discrete_columns = [\"DEROG\", \"DELINQ\", \"NINQ\", \"REASON\", \"JOB\"]\ndata[discrete_columns] = data[discrete_columns].astype(object)\ndata.dtypes","fd6c113c":"data = pd.get_dummies(data, drop_first=True)\ndata.head()","3a07b3a0":"data.shape","e53d084b":"X = data.drop(\"BAD\", axis=1)\ny = data[\"BAD\"].copy()\nX.head()","aa9864be":"pca = PCA(n_components=42)\nss = StandardScaler()\nss.fit(X)\nX_scaled = ss.transform(X)\nX_transformed_pca = pca.fit_transform(X_scaled)\npca_variance = pca.explained_variance_\nexplained_variance = pca.explained_variance_ratio_\nsum(explained_variance.tolist()[0:42])","653144ad":"plt.figure(figsize=(8, 6))\nplt.bar(range(42), pca_variance, alpha=0.5, align='center', label='Individual variance')\nplt.legend()\nplt.ylabel('Variance ratio')\nplt.xlabel('Principal components')\nplt.show()","bdaab6aa":"plt.figure(figsize=(8,6))\nplt.scatter(X_transformed_pca[:,0], X_transformed_pca[:,41], c=data['BAD'], cmap='viridis')\nplt.show()","ddefd477":"def evaluate(algo, X_train, Y_train, cv):\n    # One Pass\n    model = algo.fit(X_train, Y_train)\n    acc = round(model.score(X_train, Y_train) * 100, 2)\n    \n    # Cross Validation \n    train_pred = model_selection.cross_val_predict(algo, \n                                                  X_train, \n                                                  Y_train, \n                                                  cv=cv, \n                                                  n_jobs = -1)\n    # Cross-validation accuracy metric\n    acc_cv = round(metrics.accuracy_score(Y_train, train_pred) * 100, 2)\n    recall_cv = round(metrics.recall_score(Y_train, train_pred) * 100, 2)\n    precision_cv = round(metrics.precision_score(Y_train, train_pred) * 100, 2)\n    f1_cv = round(metrics.f1_score(Y_train, train_pred) * 100, 2)\n    \n    return train_pred, acc, acc_cv, recall_cv, precision_cv, f1_cv\n\ndef fit_ml_algo(algo, X_train, y_train, cv):\n    # One Pass\n    model = algo.fit(X_train, y_train)\n    acc = round(model.score(X_train, y_train) * 100, 2)\n    \n    # Cross Validation \n    train_pred = model_selection.cross_val_predict(algo, \n                                                  X_train, \n                                                  Y_train, \n                                                  cv=cv, \n                                                  n_jobs = -1)\n    # Cross-validation accuracy metric\n    acc_cv = round(metrics.accuracy_score(Y_train, train_pred) * 100, 2)\n    recall_cv = round(metrics.recall_score(Y_train, train_pred) * 100, 2)\n    precision_cv = round(metrics.precision_score(Y_train, train_pred) * 100, 2)\n    f1_cv = round(metrics.f1_score(Y_train, train_pred) * 100, 2)\n    \n    print(\"Model used :\", algo.best_estimator_)\n    return train_pred, acc, acc_cv, recall_cv, precision_cv, f1_cv\n\ndef fit_ml_algo_ne(algo, X_train, Y_train, cv):\n    # One Pass\n    model = algo.fit(X_train, Y_train)\n    acc = round(model.score(X_train, Y_train) * 100, 2)\n    \n    # Cross Validation \n    train_pred = model_selection.cross_val_predict(algo, \n                                                  X_train, \n                                                  Y_train, \n                                                  cv=cv, \n                                                  n_jobs = -1)\n    # Cross-validation accuracy metric\n    acc_cv = round(metrics.accuracy_score(Y_train, train_pred) * 100, 2)\n    recall_cv = round(metrics.recall_score(Y_train, train_pred) * 100, 2)\n    precision_cv = round(metrics.precision_score(Y_train, train_pred) * 100, 2)\n    f1_cv = round(metrics.f1_score(Y_train, train_pred) * 100, 2)\n    \n    return train_pred, acc, acc_cv, recall_cv, precision_cv, f1_cv","7c49d2b0":"X_train = X_transformed_pca\ny_train = y","8dd1eaec":"# Logistic Regression\nstart_time = time.time()\ntrain_pred_log, acc_log, acc_cv_log, recall_cv_log, precision_cv_log, f1_cv_log = fit_ml_algo_ne(LogisticRegression(), \n                                                               X_train, \n                                                               y_train, \n                                                                    10)\nlog_time = (time.time() - start_time)\nprint(\"Accuracy: (1 Fold) %s\" % acc_log)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_log)\nprint(\"Recall CV 10-Fold: %s\" % recall_cv_log)\nprint(\"Precision CV 10-Fold: %s\" % precision_cv_log)\nprint(\"F1 CV 10-Fold: %s\" % f1_cv_log)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=log_time))","3b33407e":"start_time = time.time()\nknn_params = {'n_neighbors':list(range(1,15)), 'weights': ['distance', 'uniform']}\nknn = KNeighborsClassifier()\n\ngrid_search = GridSearchCV(knn, knn_params, cv=5)\ngrid_search.fit(X_train, y_train)\n\ntrain_pred_knn, acc_knn, acc_cv_knn, recall_cv_knn, precision_cv_knn, f1_cv_knn = fit_ml_algo_ne(grid_search.best_estimator_, X_train,y_train,10)\nknn_time = (time.time() - start_time)\nprint(\"Accuracy: (1 Fold) %s\" % acc_knn)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_knn)\nprint(\"Recall CV 10-Fold: %s\" % recall_cv_knn)\nprint(\"Precision CV 10-Fold: %s\" % precision_cv_knn)\nprint(\"F1 CV 10-Fold: %s\" % f1_cv_knn)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=knn_time))","952db55d":"start_time = time.time()\ntrain_pred_dt, acc_dt, acc_cv_dt, recall_cv_dt, precision_cv_dt, f1_cv_dt = fit_ml_algo_ne(DecisionTreeClassifier(), \n                                                               X_train, \n                                                               y_train, \n                                                                    10)\ndt_time = (time.time() - start_time)\nprint(\"Accuracy: (1 Fold) %s\" % acc_dt)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_dt)\nprint(\"Recall CV 10-Fold: %s\" % recall_cv_dt)\nprint(\"Precision CV 10-Fold: %s\" % precision_cv_dt)\nprint(\"F1 CV 10-Fold: %s\" % f1_cv_dt)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=dt_time))","895fd396":"rf_params = {'n_estimators': list(range(1,15))\n            }\n\ngrid_search = GridSearchCV(RandomForestClassifier(), rf_params, cv=10)\ngrid_search.fit(X_train, y_train)\n\nprint(\"Best params : \",grid_search.best_params_)\nstart_time = time.time()\n\ntrain_pred_rf, acc_rf, acc_cv_rf, recall_cv_rf, precision_cv_rf, f1_cv_rf = fit_ml_algo_ne(grid_search.best_estimator_,X_train,y_train,10)\n\nrf_time = (time.time() - start_time)\nprint(\"Accuracy: (1 Fold) %s\" % acc_rf)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_rf)\nprint(\"Recall CV 10-Fold: %s\" % recall_cv_rf)\nprint(\"Precision CV 10-Fold: %s\" % precision_cv_rf)\nprint(\"F1 CV 10-Fold: %s\" % f1_cv_rf)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=rf_time))","5561c0df":"sgdc_params = {\"loss\": [\"hinge\", \"log\"], \"penalty\": [\"l1\", \"l2\"], \"max_iter\": [1,2,3,4,5]}\n\nstart_time = time.time()\n\ngrid_search = GridSearchCV(SGDClassifier(), sgdc_params, cv=5)\ngrid_search.fit(X_train, y_train)\n\nprint(\"Best params : \",grid_search.best_params_)\n\ntrain_pred_sgdc, acc_sgdc, acc_cv_sgdc, recall_cv_sgdc, precision_cv_sgdc, f1_cv_sgdc  = fit_ml_algo_ne(grid_search.best_estimator_,X_train,y_train,10)\n\nsgdc_time = (time.time() - start_time)\nprint(\"Accuracy: (1 Fold) %s\" % acc_sgdc)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_sgdc)\nprint(\"Recall CV 10-Fold: %s\" % recall_cv_sgdc)\nprint(\"Precision CV 10-Fold: %s\" % precision_cv_sgdc)\nprint(\"F1 CV 10-Fold: %s\" % f1_cv_sgdc)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=sgdc_time))","d4e07b65":"train_pred_nb, acc_gnb, acc_cv_gnb, recall_cv_gnb, precision_cv_gnb, f1_cv_gnb  = fit_ml_algo_ne(GaussianNB(),X_train,y_train,10)\ngnb_time = (time.time() - start_time)\nprint(\"Accuracy: (1 Fold) %s\" % acc_gnb)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_gnb)\nprint(\"Recall CV 10-Fold: %s\" % recall_cv_gnb)\nprint(\"Precision CV 10-Fold: %s\" % precision_cv_gnb)\nprint(\"F1 CV 10-Fold: %s\" % f1_cv_gnb)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=gnb_time))","e8eb49f3":"\"\"\"gbc_params = {\"loss\": [\"deviance\", \"exponential\"],\n              \"learning_rate\": [1,0.6 ,0.5,0.4,0.3, 0.25, 0.1, 0.05, 0.01],\n              \"n_estimators\": [10,50,100]\n            }\n\ngrid_search = GridSearchCV(GradientBoostingClassifier(), gbc_params, cv=5)\ngrid_search.fit(X_train, y_train)\nprint(\"Best params : \",grid_search.best_params_)\ntrain_pred_gbc, acc_gbc, acc_cv_gbc, recall_cv_gbc, precision_cv_gbc, f1_cv_gbc = fit_ml_algo_ne(grid_search.best_estimator_,X_train,y_train,10)\ngbc_time = (time.time() - start_time)\nprint(\"Accuracy: (1 Fold) %s\" % acc_gbc)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_gbc)\nprint(\"Recall CV 10-Fold: %s\" % recall_cv_gbc)\nprint(\"Precision CV 10-Fold: %s\" % precision_cv_gbc)\nprint(\"F1 CV 10-Fold: %s\" % f1_cv_gbc)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=gbc_time))\"\"\"","c76ad181":"# Linear SVC\nlsvc_params = {\"penalty\": [\"l2\"],\n                \"loss\": [\"hinge\", \"squared_hinge\"],\n               \"dual\": [True],\n               \"C\": [0.001,0.01,0.1,1,10]\n            }\n\ngrid_search = GridSearchCV(LinearSVC(), lsvc_params, cv=5)\ngrid_search.fit(X_train, y_train)\nprint(\"Best params : \",grid_search.best_params_)\n\ntrain_pred_lsvc, acc_lsvc, acc_cv_lsvc, recall_cv_lsvc, precision_cv_lsvc, f1_cv_lsvc = fit_ml_algo_ne(grid_search.best_estimator_,X_train,y_train,10)\nlsvc_time = (time.time() - start_time)\nprint(\"Accuracy: (1 Fold) %s\" % acc_lsvc)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_lsvc)\nprint(\"Recall CV 10-Fold: %s\" % recall_cv_lsvc)\nprint(\"Precision CV 10-Fold: %s\" % precision_cv_lsvc)\nprint(\"F1 CV 10-Fold: %s\" % f1_cv_lsvc)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=lsvc_time))","5e69de13":"# XGBoost Classifier\nstart_time = time.time()\nxgb_params = {\"early_stopping_rounds\": [1,2,5],\n                \"n_estimators\": [5,10,15],\n               \"learning_rate\": [0.001,0.03,0.05],\n               \"n_jobs\": [0,1,2,5]\n            }\n\ngrid_search = GridSearchCV(XGBClassifier(), xgb_params, cv=5)\ngrid_search.fit(X_train, y_train)\nprint(\"Best params : \",grid_search.best_params_)\n\ntrain_pred_xgb, acc_xgb, acc_cv_xgb, recall_cv_xgb, precision_cv_xgb, f1_cv_xgb = fit_ml_algo_ne(grid_search.best_estimator_,X_train, y_train,10)\nxgb_time = (time.time() - start_time)\nprint(\"Accuracy: (1 Fold) %s\" % acc_xgb)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_xgb)\nprint(\"Recall CV 10-Fold: %s\" % recall_cv_xgb)\nprint(\"Precision CV 10-Fold: %s\" % precision_cv_xgb)\nprint(\"F1 CV 10-Fold: %s\" % f1_cv_xgb)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=xgb_time))","00782f22":"from catboost import CatBoostClassifier, Pool, cv\n\ntrain_pool = Pool(X_train, \n                  y_train)\ncatboost_model = CatBoostClassifier(iterations=2500,\n                                    custom_loss=['F1', 'Precision', 'Recall', 'Accuracy'],\n                                    loss_function='Logloss',\n                                    #task_type=\"GPU\",\n                                    learning_rate=0.1,\n                                    devices='0:1')\n                                \ncatboost_model.fit(train_pool,plot=True)\ncatboost_model.best_score_\n\n# Print out the CatBoost model metrics\nprint(\"---CatBoost Metrics---\")\nprint(str(format(catboost_model.best_score_)))","ebfe609d":"acc_catboost = catboost_model.best_score_[\"learn\"][\"Accuracy\"] * 100\nrecall_catboost = catboost_model.best_score_[\"learn\"][\"Recall\"] * 100\nprecision_catboost = catboost_model.best_score_[\"learn\"][\"Precision\"] * 100\nf1_catboost = catboost_model.best_score_[\"learn\"][\"F1\"] * 100","feadda07":"my_score_cv = cross_val_score(catboost_model, X_train, y_train, \n                         cv = 5, \n                         scoring = 'recall')","ed6ec89b":"my_score_cv","51e77a62":"# Cross Validation \ntrain_pred = model_selection.cross_val_predict(catboost_model, \n                                                  X_train, \n                                                  y_train, \n                                                  cv=5, \n                                                  n_jobs = -1)","d25456c0":"    # Cross-validation accuracy metric\nacc_cv_catboost = round(metrics.accuracy_score(y_train, train_pred) * 100, 2)\nrecall_cv_catboost = round(metrics.recall_score(y_train, train_pred) * 100, 2)\nprecision_cv_catboost = round(metrics.precision_score(y_train, train_pred) * 100, 2)\nf1_cv_catboost = round(metrics.f1_score(y_train, train_pred) * 100, 2)","cf6dcadf":"print(\"Accuracy CV 10-Fold: %s\" % acc_cv_catboost)\nprint(\"Recall CV 10-Fold: %s\" % recall_cv_catboost)\nprint(\"Precision CV 10-Fold: %s\" % precision_cv_catboost)\nprint(\"F1 CV 10-Fold: %s\" % f1_cv_catboost)","eef6a6fc":"# Light GBM  Classifier\nstart_time = time.time()\nlgbm_params = {\n    'n_estimators': [100, 200, 300],\n    \"\"\"'colsample_bytree': [0.7, 0.8],\n    'max_depth': [15,20,25],\n    'num_leaves': [50, 100, 200],\n    'reg_alpha': [1.1, 1.2, 1.3],\n    'reg_lambda': [1.1, 1.2, 1.3],\n    'min_split_gain': [0.3, 0.4],\"\"\"\n    'subsample': [0.7, 0.8, 0.9],\n    'subsample_freq': [20]\n}\n\ngrid_search = GridSearchCV(LGBMClassifier(), lgbm_params, cv=5)\ngrid_search.fit(X_train, y_train)\nprint(\"Best params : \",grid_search.best_params_)\n\ntrain_pred_lgbm, acc_lgbm, acc_cv_lgbm, recall_cv_lgbm, precision_cv_lgbm, f1_cv_lgbm  = fit_ml_algo_ne(grid_search.best_estimator_,X_train, y_train,10)\nlgbm_time = (time.time() - start_time)\nprint(\"Accuracy: (1 Fold) %s\" % acc_lgbm)\nprint(\"Accuracy CV 10-Fold: %s\" % acc_cv_lgbm)\nprint(\"Recall CV 10-Fold: %s\" % recall_cv_lgbm)\nprint(\"Precision CV 10-Fold: %s\" % precision_cv_lgbm)\nprint(\"F1 CV 10-Fold: %s\" % f1_cv_lgbm)\nprint(\"Running Time: %s\" % datetime.timedelta(seconds=lgbm_time))","33e03b7b":"cv_models = pd.DataFrame({\n    'Model': ['KNN', \n              'Logistic Regression',\n              'Gaussian Naive Bayes',\n              'Stochastic Gradient Decent',\n              'Linear SVC', \n              'Decision Tree',\n              #'Gradient Boosting Trees',\n              'XGBoost',\n              'Random Forest',\n              \"LGBM\",\n              \"Catboost\",\n              \"Catboost_1F\"\n              ],\n    'Accuracy': [\n        acc_cv_knn,\n        acc_cv_log,\n        acc_cv_gnb,\n        acc_cv_sgdc,\n        acc_cv_lsvc,\n        acc_cv_dt,\n        #acc_cv_gbc,\n        acc_cv_xgb,\n        acc_cv_rf,\n        acc_cv_lgbm,\n        acc_cv_catboost,\n        acc_catboost\n    ], 'Precision': [\n        precision_cv_knn,\n        precision_cv_log,\n        precision_cv_gnb,\n        precision_cv_sgdc,\n        precision_cv_lsvc,\n        precision_cv_dt,\n        #precision_cv_gbc,\n        precision_cv_xgb,\n        precision_cv_rf,\n        precision_cv_lgbm,\n        precision_cv_catboost,\n        precision_catboost\n    ], 'Recall': [\n        recall_cv_knn,\n        recall_cv_log,\n        recall_cv_gnb,\n        recall_cv_sgdc,\n        recall_cv_lsvc,\n        recall_cv_dt,\n        #recall_cv_gbc,\n        recall_cv_xgb,\n        recall_cv_rf,\n        recall_cv_lgbm,\n        recall_cv_catboost,\n        recall_catboost\n    ], 'F1-Score' : [\n        f1_cv_knn,\n        f1_cv_log,\n        f1_cv_gnb,\n        f1_cv_sgdc,\n        f1_cv_lsvc,\n        f1_cv_dt,\n        #f1_cv_gbc,\n        f1_cv_xgb,\n        f1_cv_rf,\n        f1_cv_lgbm,\n        f1_cv_catboost,\n        f1_catboost\n    ]})","38866373":"print('---Cross-validation Scores---')\ncv_models.sort_values(by='Recall', ascending=False)","b563017a":"According to the two plot above, people who are independent are the richest out of the available job, which is logical since they usually have businesses.","7efd9356":"**MORTDUE** & **VALUE** have the highest correlation index of 0.87. This can be useful during the feature engineering phase.","56642914":"12. CLNO","94173377":"The target value's modalities are not balanced, this will help us during the choice of our evaluation metric.","be24e7c3":"### Method 2","417e556a":"###  Xgboost","ddc1e5bd":"### 1. Continuous","49c7e091":"1. **REASON** & **DEBTINC**","200ac12e":"## Machine Learning","c56d643f":"## Plotting Qualitative ~ Qualitative","04b88656":"Do the jobs count vary according to the reason of loan ?","fc3e7c83":"People take higher loan values for debt consolidation than for Home Improvement reasons.","db6a8a70":"9. DELINQ","02de021d":"### Linear SVC","5036b11b":"7. YOJ (Years at current job)","eea164f4":"## Numerical Values","4c864d54":"### GBC","d9e2c522":"3. **JOB** & **VALUE**","dc44c818":"10. CLAGE","f99bf146":"2. **JOB** & **LOAN**","a5e75c08":"3. **REASON** & **LOAN**","68f50d19":"It appears that the best regression model (lowest RMSE Score) we can use to replace the missing values for **DEBTINC** is : **Random Forest**.\nLet's apply it right now :","dc8a66c4":"### GaussianNB","5d5d8efb":"### 2. Discrete","5dc16c3e":"5. REASON","9780dac5":"According to that graph, the variable **JOB** doesn't have an influence on **DEBTINC** which is logical because customers want to get a loan that they can pay back.","7650a5b3":"### Method 3","ab4e581a":"4. VALUE","f785b1c5":"## Method 3 (Mean, Most Frequent, StandardScaler)","ba01996b":"## Correlations","8a34a50e":"1. BAD","43efa55f":"## Method 4 (Mean, Most Frequent, RobustScaler)","bab4b515":"13. DEBTINC","82d7dc3a":"By Sum :","4bc20c2b":"## PCA","886ca21e":"### Logistic Regression","260ef884":"## Method 1 (Median, Most Frequent, StandardScaler)","e8a53f65":"Now we are going to get an idea about the relation between some Categorical and numerical features.","1bb16b06":"### KNN","e934c306":"# Data Preprocessing","cd29e750":"Let us see the correlation index between each quantitative features :","0354e172":"By Mean :","c4c5db83":"1. **JOB** & **DEBTINC**","4cf51b66":"We can already see that **MORTDUE** & **VALUE** have somewhat of a positive linear correlation.","e2f58eae":"How many null values are present in the dataset ?","ef56c339":"Customers who are independent have the largest mean value of loan demands.","5cf16f51":"It looks like the best method was  **Method 4**.","6a51b4b5":"### SGDC","e8b1e817":"Now we are going to look at the distributions of each feature to get more insight on what to do during data cleaning :","b9487e3a":"2. **REASON** & **VALUE**","72c6e670":"### Scoring","df9df701":"### Method 5","34cf1ea2":"### Catboost","4ed70fd1":"## Features Engineering","2fabdb86":"### Method 1","1e637b69":"### LGBM Classifier","7f7b17ec":"6. JOB","5cea8bd5":"## Plotting Categorical ~ Numerical","26f2ce8a":"8. DEROG","6541ad2c":"### Method 4","5ffec460":"### Decision Tree","e085bbc3":"#### DEBTINC Feature Prediction\nFor this, we will make a seperate machine learning regression model to fill the missing values. So we will create a new dataframe setting the target variable as **DEBTINC**.","4fc7311f":"These are the current correlations that we have :","6faa5c33":"2. LOAN","a712b20f":"## Numerical Features","09042dc3":"3. MORTDUE","b5eaa601":"## Encoding","9caec59c":"We can see that there are outlier values in the dataset. We will look at each feature independently to get more insight later.","36929dfc":"No, the proportions are almost similar, so the job count does not change for the reason of loan.","011765db":"11. NINQ","0cb4b142":"## Method 2 (Median, Most Frequent, RobustScaler)","8f6e4e48":"# Exploratory Data Analysis","11bbafd9":"### Random Forest"}}