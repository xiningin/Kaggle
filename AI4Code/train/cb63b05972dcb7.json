{"cell_type":{"3899a28b":"code","22f340d7":"code","f877ab6b":"code","cd1b0401":"code","4d2c9047":"code","39e8b50b":"code","6fa2c764":"code","196551b7":"code","2279ca37":"code","209b7cb4":"code","3c3bf9fe":"code","d5596469":"code","a4e830c8":"code","e9759372":"code","658b5ab7":"code","0abfe349":"code","f1b45e90":"code","d2809142":"code","50f8831c":"code","1cbbc440":"code","16ba9d08":"code","6e33d8ce":"code","572c9b24":"code","0c02ce06":"code","73b766c9":"code","b8d410b4":"code","b8b677e3":"code","2a370082":"code","e54bf500":"code","cbaadf2f":"code","5352c3eb":"code","fa31db1a":"code","1652a3c5":"code","8d7c9ee7":"code","78b26a79":"code","058f8269":"code","1897eb49":"code","60959a7d":"code","eccc7a6f":"code","e2141e7f":"code","dc5d73d2":"markdown","ce5c8053":"markdown","75a45e2d":"markdown","525c498a":"markdown","11aceae6":"markdown","c736e617":"markdown","aa26f376":"markdown","a8d708f1":"markdown","433150cb":"markdown","44f90542":"markdown","13b01e27":"markdown","339408c2":"markdown","efc8b59c":"markdown","159408b4":"markdown","926d5d44":"markdown","957049fb":"markdown","5a46f97b":"markdown","de9b8d13":"markdown","d8abf23f":"markdown"},"source":{"3899a28b":"import numpy as np\nfrom tensorflow import keras\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D, BatchNormalization\nfrom matplotlib import pyplot as plt","22f340d7":"from numpy.random import seed\nseed(1)","f877ab6b":"# Image params\nimg_rows, img_cols = 28, 28\n\n# Data params\nletter_file = \"..\/input\/emnist\/emnist-letters-train.csv\"\ntest_file = \"..\/input\/emnist\/emnist-letters-test.csv\"\nnum_classes = 37\nclasses = 'ABCDEFGHIJKLMNOPQRSTUVWXYZabdefghnqrt'","cd1b0401":"## Prepare input data\ndef prep_data(raw):\n    y = raw[:, 0]\n    out_y = keras.utils.to_categorical(y, num_classes)\n\n    x = raw[:, 1:]\n    num_images = raw.shape[0]\n    out_x = x.reshape(num_images, img_rows, img_cols, 1)\n    out_x = out_x \/ 255\n    return out_x, out_y","4d2c9047":"## Convert One-Hot-Encoded values back to real values\ndef decode_label(binary_encoded_label):\n    return np.argmax(binary_encoded_label)-1","39e8b50b":"## Plot an image with it's correct value\ndef show_img(img,label):\n    img_flip = np.transpose(img, axes=[1,0])\n    plt.title('Label: ' + str(classes[decode_label(label)]))\n    plt.imshow(img_flip, cmap='Greys_r')","6fa2c764":"## Evaluate model with the test dataset\ndef eval_model(model,test_x,test_y):\n    result = model.evaluate(test_x, test_y)\n    print(\"The accuracy of the model is: \",result[1])\n    return result","196551b7":"## Plot the training history\ndef plot_history(history):\n    acc = history.history['acc']\n    val_acc = history.history['val_acc']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(1, len(acc) + 1)\n\n    # \"bo\" is for \"blue dot\"\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, acc, 'b', label='Training accuracy')\n    # b is for \"solid blue line\"\n    plt.plot(epochs, val_loss, 'ro', label='Validation loss')\n    plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n    plt.title('Training and validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    plt.show()","2279ca37":"letter_data = np.loadtxt(letter_file, skiprows=1, delimiter=',')\nx, y = prep_data(letter_data)","209b7cb4":"print(x.shape)\nprint(y.shape)","3c3bf9fe":"fig = plt.figure(figsize=(17,4.5))\nfor idx in range(30):\n    fig.add_subplot(3,10,idx+1)\n    plt.axis('off')\n    show_img(np.squeeze(x[idx]),y[idx])\nplt.subplots_adjust(wspace=0.3, hspace=0.3)","d5596469":"test_data = np.loadtxt(test_file, skiprows=1, delimiter=',')\ntest_x, test_y = prep_data(test_data)","a4e830c8":"print(test_x.shape)\nprint(test_y.shape)","e9759372":"# Create a basic short CNN model\n# As a first trial for the project\ndef create_basic_model():\n    batch_size = 64\n\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size=(3, 3), strides=1,activation='relu'))\n    model.add(Conv2D(32, (3, 3), activation='relu', strides=1))\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n\n    model.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    \n    return model","658b5ab7":"batch_size = 128\n\nbasic_model = create_basic_model()\nbasic_history = basic_model.fit(x, y,\n          batch_size = batch_size,\n          epochs = 10,\n          validation_split = 0.2)","0abfe349":"plot_history(basic_history)","f1b45e90":"eval_model(basic_model,test_x,test_y)","d2809142":"# Create a basic short CNN model with regularization\n# As a second trial for the project\ndef create_basic_model_with_reg():\n    batch_size = 64\n\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size=(3, 3), strides=1,activation='relu', input_shape = (28, 28, 1)))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, (3, 3), activation='relu', strides=1))\n    model.add(BatchNormalization())\n    model.add(Flatten())\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n\n\n    model.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    \n    return model","50f8831c":"batch_size = 128\n\nbasic_model_reg = create_basic_model_with_reg()\nbasic_reg_history = basic_model_reg.fit(x, y,\n          batch_size = batch_size,\n          epochs = 10,\n          validation_split = 0.2)","1cbbc440":"plot_history(basic_reg_history)","16ba9d08":"eval_model(basic_model_reg,test_x,test_y)","6e33d8ce":"# Create a basic short CNN model with regularization\n# As a second trial for the project\ndef create_basic_model_with_reg2():\n    batch_size = 64\n\n    model = Sequential()\n    model.add(Conv2D(32, kernel_size=(3, 3), strides=1,activation='relu', input_shape = (28, 28, 1)))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, (3, 3), activation='relu', strides=1))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n    model.add(Flatten())\n    model.add(Dropout(0.4))\n    model.add(Dense(128, activation='relu'))\n    model.add(Dense(num_classes, activation='softmax'))\n\n\n    model.compile(loss=keras.losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n    \n    return model","572c9b24":"batch_size = 128\n\nbasic_model_reg2 = create_basic_model_with_reg2()\nbasic_reg2_history = basic_model_reg2.fit(x, y,\n          batch_size = batch_size,\n          epochs = 10,\n          validation_split = 0.2)","0c02ce06":"plot_history(basic_history)\nplot_history(basic_reg2_history)","73b766c9":"eval_model(basic_model_reg2,test_x,test_y)","b8d410b4":"# Create a more complex model\n# As the architectura decision for the project\ndef create_complex_model(input_size,output_size):\n    model = Sequential()\n\n    model.add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = (input_size[0], input_size[1], input_size[2])))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, kernel_size = 3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n\n    model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size = 3, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Dropout(0.4))\n\n    model.add(Conv2D(128, kernel_size = 4, activation='relu'))\n    model.add(BatchNormalization())\n    model.add(Flatten())\n    model.add(Dropout(0.4))\n    model.add(Dense(output_size, activation='softmax'))\n\n    model.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer='adam',\n              metrics=['accuracy'])\n    \n    return model","b8b677e3":"batch_size = 64\n\ncomplex_model = create_complex_model([img_rows, img_cols,1],len(classes))\ncomplex_history = complex_model.fit(x, y,\n          batch_size = batch_size,\n          epochs = 15,\n          validation_split = 0.1)","2a370082":"plot_history(complex_history)","e54bf500":"eval_model(complex_model,test_x,test_y)","cbaadf2f":"data_generator_with_aug = keras.preprocessing.image.ImageDataGenerator(validation_split=.2,\n                                            width_shift_range=.1, \n                                            height_shift_range=.1,\n                                            rotation_range=10, \n                                            zoom_range=.1)\ntrain_generator = data_generator_with_aug.flow(x, y, subset='training')\nvalidation_data_generator = data_generator_with_aug.flow(x, y, subset='validation')\n\nmodel_with_aug = create_complex_model([img_rows, img_cols,1],len(classes))\n\nhistory_with_aug = model_with_aug.fit_generator(train_generator, \n                              steps_per_epoch=20000, epochs=15, # can change epochs to 10\n                              validation_data=validation_data_generator)\n","5352c3eb":"plot_history(history_with_aug)","fa31db1a":"eval_model(model_with_aug,test_x,test_y)","1652a3c5":"batch_size = 32\ntengwar_test = \"..\/input\/handwritten-tengwar-letters\/tengwar\/tengwar\/test\/\"\ntengwar_train = \"..\/input\/handwritten-tengwar-letters\/tengwar\/tengwar\/train\/\"\ninput_size = [64,64,3]\noutput_classes = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'","8d7c9ee7":"data_generator = keras.preprocessing.image.ImageDataGenerator(validation_split=.2,\n                                            width_shift_range=.1, \n                                            height_shift_range=.1,\n                                            rotation_range=10, \n                                            zoom_range=.1)\n\ntrain_generator = data_generator.flow_from_directory(\n        tengwar_train,\n        target_size=(input_size[0], input_size[1]),\n        batch_size=batch_size,\n        class_mode='categorical')\n\nvalidation_generator = data_generator.flow_from_directory(\n        tengwar_test,\n        target_size=(input_size[0], input_size[1]),\n        class_mode='categorical')","78b26a79":"tengwar_model = create_complex_model(input_size,len(output_classes))\ntengwar_history = tengwar_model.fit_generator(train_generator, \n                              steps_per_epoch=1000, epochs=10,\n                              validation_data=validation_generator)\n","058f8269":"plot_history(tengwar_history)","1897eb49":"import glob\nimport cv2\ntengwar_final = \"..\/input\/handwritten-tengwar-letters\/tengwar\/tengwar\/output\/*\"\nimg_names = [img for img in sorted(glob.glob(tengwar_final+\"*.png\"))]\nprint(img_names)\nx = np.array([cv2.imread(img) for img in img_names])","60959a7d":"plt.imshow(x[5])\nplt.show()","eccc7a6f":"result = tengwar_model.predict(x)","e2141e7f":"out_res = ''\nfor res in result:\n    out_res += output_classes[np.argmax(res)]\nprint(out_res.lower())\n","dc5d73d2":"The testing result has more variance in it, so not yet good. So adding dropout layers might help.","ce5c8053":"## **Building and fitting the models**\n### **Basic solution**\nFirstly create a small network.","75a45e2d":"The model can\u2019t generalize on the training dataset well, because the result on the validation data is not similar. Let\u2019s try a few modification. Firstly, i added batch normalization after each convolutions. Fun fact: if you don\u2019t add input_shape to the first layer of your model, your model will fail to converge.","525c498a":"### **Complex solution**","11aceae6":"### **Read train data**\nIn the following few line of code, we load the training data, then prepare it.","c736e617":"### **Imported Python packages**","aa26f376":"### **Parameters**","a8d708f1":"### **Preknown parameters**\nAll the data in the MNIST dataset is structured as the following:\n-  785 columns\n-  First column = class_label\n-  Each column after represents one pixel value (784 total for a 28 x 28 image)","433150cb":"We print a few images to 'get a feeling' how the data looks like. ","44f90542":"# Handwritten Letter recognition with Elves language decoding :D\n\nThis is a solution for the Hello World of machine learning, the EMNIST letter recognition. Then english letters will be encoded with tengwar characters, and they will be recognized.","13b01e27":"### **Read test data**\nThe final test data has to be loaded similarly. ","339408c2":"It has the best result so far, so try to add dataaugmentation to help the model to generalize better.","efc8b59c":"## Decoding tengwar letters into english letters","159408b4":"### **Set seed for random number generator**\nSetting the seed number will result in reproducable output. Therefore, the effects of changing anything but the model can be compared.","926d5d44":"It gives a similar result as the first, but with better validation, and test results. The problem might be the too shallow network. Therefore, i tried a model, that is used to have 99,75% accuracy on the MNIST dataset, and gave it a try to learn the characters. The description of the model can be found [here](https:\/\/www.kaggle.com\/cdeotte\/25-million-images-0-99757-mnist).","957049fb":"### **Complex solution with data augmentation**","5a46f97b":"### **Data augmentation**","de9b8d13":"Then we check the size of the data, that can help in the future to decide how do we want to split it into train and test data.","d8abf23f":"The accuracy on the test dataset is a little bit better, therefore, it generalizes better."}}