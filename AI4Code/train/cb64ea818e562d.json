{"cell_type":{"521f2e60":"code","5e52ed86":"code","df48a4e6":"code","88ce2f3f":"code","382edfa7":"code","81b5d40d":"code","26cee9ea":"code","50752293":"code","f7c6f2b6":"code","c10a2e3b":"code","ee8e640b":"code","790e6037":"code","408cd2f4":"code","12a59c7c":"code","cb000dd1":"code","d47cd870":"code","f1448d84":"code","8b4bc97b":"code","e5047229":"code","e15e23fd":"code","ac5eef2b":"code","c01a1b7e":"code","b58f2f48":"code","43537055":"code","1975ba46":"code","e5bcdb57":"code","0571f917":"code","d2de229e":"code","0729945b":"code","bc17af31":"markdown","9ae1922b":"markdown"},"source":{"521f2e60":"# install ctcdecode\n# get the code\n!git clone --recursive https:\/\/github.com\/parlance\/ctcdecode.git\n!cd ctcdecode && pip install .","5e52ed86":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch\nimport torchaudio\nimport torchtext\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\nimport glob\nimport concurrent.futures as cf\nfrom IPython.display import Audio\nimport time\nimport pickle\nfrom typing import Tuple\nfrom torch import Tensor\nfrom pathlib import Path\nfrom ctcdecode import CTCBeamDecoder\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n        \ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nroot_path = os.getcwd()\nprint(root_path)\n        \n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","df48a4e6":"torch.autograd.set_detect_anomaly(True)","88ce2f3f":"# config max_length\nsample_rate = 16000\n\nmax_seconds = 25\nmax_length = sample_rate * max_seconds # 25 seconds\nprint(\"max_length:\", max_length)","382edfa7":"def transform_audio(audio_pack):\n    waveform, sample_rate, transcript = audio_pack\n    # pad to equal size\n    wave = torch.cat([waveform, waveform.new_zeros((1, max_length - waveform.size(1)))], dim=1)\n    spectrogram = torchaudio.transforms.Spectrogram()(wave)\n    \n    # (channel, feature, timestep) -> (channel, timestep, feature)\n    spectrogram = spectrogram.permute(0, 2, 1)\n    return spectrogram, sample_rate, transcript","81b5d40d":"def load_librispeech_item(fileid: str,\n                          path: str,\n                          ext_audio: str,\n                          ext_txt: str) -> Tuple[Tensor, int, str, int, int, int]:\n    speaker_id, chapter_id, utterance_id = fileid.split(\"-\")\n\n    file_text = speaker_id + \"-\" + chapter_id + ext_txt\n    file_text = os.path.join(path, speaker_id, chapter_id, file_text)\n\n    fileid_audio = speaker_id + \"-\" + chapter_id + \"-\" + utterance_id\n    file_audio = fileid_audio + ext_audio\n    file_audio = os.path.join(path, speaker_id, chapter_id, file_audio)\n\n    # Load audio\n    waveform, sample_rate = torchaudio.load(file_audio)\n\n    # Load text\n    with open(file_text) as ft:\n        for line in ft:\n            fileid_text, transcript = line.strip().split(\" \", 1)\n            if fileid_audio == fileid_text:\n                break\n        else:\n            # Translation not found\n            raise FileNotFoundError(\"Translation not found for \" + fileid_audio)\n\n    return transform_audio((waveform, sample_rate, transcript))\n\nclass LibriSpeechDataset(torch.utils.data.Dataset):\n    \n    _ext_txt = \".trans.txt\"\n    _ext_audio = \".flac\"\n    \n    def __init__(self, train=True):\n        self.list_url = ['train-clean-100', 'train-clean-360']\n        if not train:\n            self.list_url = ['test-clean']\n        \n        self._walker = []\n        for url in self.list_url:\n            path = '..\/input\/librispeech-clean\/LibriSpeech\/' + url\n            walker = [(str(p.stem), path) for p in Path(path).glob('*\/*\/*' + self._ext_audio)]\n            self._walker.extend(walker)\n        self._walker = sorted(self._walker)\n    \n    def __len__(self):\n        return len(self._walker)\n\n    def __getitem__(self, n):\n        fileid, path = self._walker[n]\n        return load_librispeech_item(fileid, path, self._ext_audio, self._ext_txt)","26cee9ea":"train_set = LibriSpeechDataset(True)\ntest_set = LibriSpeechDataset(False)","50752293":"n_feature = train_set[0][0].size(-1)\nprint(\"n_feature:\", n_feature)","f7c6f2b6":"nrows, ncols = 1, 4\n\nfig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 5))\nax = ax.flatten()\n\nfor i in range(nrows * ncols):\n    data = train_set[i][0]\n    shape = data.shape\n    ax[i].imshow(data.squeeze().log())\n    ax[i].set_title(f'{shape[1]}, {shape[2]}')\nfig.tight_layout()\nplt.show()","c10a2e3b":"batch_size = 16\ntrain_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=True)","ee8e640b":"alphabet = pickle.load(open(\"..\/input\/librispeech-alphabet\/librispeech_alphabet.pkl\", \"rb\"))\nprint(\"Alphabet:\", alphabet)","790e6037":"vocab = torchtext.vocab.vocab(alphabet)\nvocab.insert_token('_', 0)\ndefault_index = -1\nvocab.set_default_index(default_index)\nprint(vocab.get_stoi())\nn_class = len(vocab)\nprint(\"n_class:\", n_class)\n\ndef encode_string(pack):\n    if type(pack) != tuple:\n        pack = (pack,)\n    return [torch.Tensor([vocab[i] for i in s]) for s in pack]\n\ndef decode_string(a):\n    a = a.view(-1) # flatten\n    return ''.join([vocab.lookup_token(i) for i in a if (i > 0 and i < n_class)])","408cd2f4":"class FullyConnected(torch.nn.Module):\n    \"\"\"\n    Args:\n        n_feature: Number of input features\n        n_hidden: Internal hidden unit size.\n    \"\"\"\n\n    def __init__(self,\n                 n_feature: int,\n                 n_hidden: int,\n                 dropout: float,\n                 relu_max_clip: int = 20) -> None:\n        super(FullyConnected, self).__init__()\n        self.fc = torch.nn.Linear(n_feature, n_hidden, bias=True)\n        self.relu_max_clip = relu_max_clip\n        self.dropout = dropout\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.fc(x)\n        x = torch.nn.functional.relu(x)\n        x = torch.nn.functional.hardtanh(x, 0, self.relu_max_clip)\n        if self.dropout:\n            x = torch.nn.functional.dropout(x, self.dropout, self.training)\n        return x\n\n\nclass DeepSpeech(torch.nn.Module):\n    \"\"\"\n    DeepSpeech model architecture from *Deep Speech: Scaling up end-to-end speech recognition*\n    [:footcite:`hannun2014deep`].\n\n    Args:\n        n_feature: Number of input features\n        n_hidden: Internal hidden unit size.\n        n_class: Number of output classes\n    \"\"\"\n\n    def __init__(\n        self,\n        n_feature: int,\n        n_hidden: int = 2048,\n        n_class: int = 40,\n        dropout: float = 0.0,\n    ) -> None:\n        super(DeepSpeech, self).__init__()\n        self.n_hidden = n_hidden\n        self.fc1 = FullyConnected(n_feature, n_hidden, dropout)\n        self.fc2 = FullyConnected(n_hidden, n_hidden, dropout)\n        self.fc3 = FullyConnected(n_hidden, n_hidden, dropout)\n        self.bi_rnn = torch.nn.RNN(\n            n_hidden, n_hidden, num_layers=1, nonlinearity='relu', bidirectional=True\n        )\n        self.fc4 = FullyConnected(n_hidden, n_hidden, dropout)\n        self.out = torch.nn.Linear(n_hidden, n_class)\n      \n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Args:\n            x (torch.Tensor): Tensor of dimension (batch, channel, time, feature).\n        Returns:\n            Tensor: Predictor tensor of dimension (batch, time, class).\n        \"\"\"\n        # N x C x T x F\n        x = self.fc1(x)\n        # N x C x T x H\n        x = self.fc2(x)\n        # N x C x T x H\n        x = self.fc3(x)\n        # N x C x T x H\n        x = x.squeeze(1)\n        # N x T x H\n        x = x.transpose(0, 1)\n        # T x N x H\n        x, _ = self.bi_rnn(x)\n        # The fifth (non-recurrent) layer takes both the forward and backward units as inputs\n        x = x[:, :, :self.n_hidden] + x[:, :, self.n_hidden:]\n        # T x N x H\n        x = self.fc4(x)\n        # T x N x H\n        x = self.out(x)\n        # T x N x n_class\n        x = x.permute(1, 0, 2)\n        # N x T x n_class\n        x = torch.nn.functional.log_softmax(x, dim=2)\n        # T x N x n_class\n        x = x.permute(1, 0, 2) # for ctc loss\n        return x","12a59c7c":"# checkpoint = torch.load('..\/input\/deepspeech-model\/deepspeech.state')","cb000dd1":"model = DeepSpeech(n_feature, n_hidden=4096, n_class=n_class, dropout=0.2).to(device)\n# model.load_state_dict(checkpoint['model_state_dict'])\nmodel","d47cd870":"optimizer = torch.optim.SGD(model.parameters(), lr=0.00001, momentum=0.99, nesterov=True)\n# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nloss_fn = torch.nn.CTCLoss().to(device)","f1448d84":"def make_target(transcript):\n    encoded = encode_string(transcript)\n    target_length = torch.Tensor.long(torch.Tensor([i.size(0) for i in encoded]))\n    target = torch.nn.utils.rnn.pad_sequence(encoded)\n    return target.permute(1, 0), target_length","8b4bc97b":"input_size = train_set[0][0].size(1)\ninput_size","e5047229":"def train_epoch(dataloader, model, loss_fn, optimizer, epoch):\n    size = len(dataloader.dataset)\n    model.train()\n    start_time = time.perf_counter()\n    for batch, (feature, sample_rate, transcript) in enumerate(dataloader):\n        target, target_length = make_target(transcript)\n        input_length = torch.full(size=(feature.size(0),), fill_value=input_size, dtype=torch.long).to(device)\n        feature, target, target_length = feature.to(device), target.to(device), target_length.to(device)\n        \n        predict = model(feature)\n        loss = loss_fn(predict, target, input_length, target_length)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        if batch % 100 == 0:\n            loss, current = loss.item(), batch * len(feature)\n            print(f\"[Epoch: {epoch + 1}] = loss: {loss:>7f} [{current:>5d}\/{size:>5d} ({round(current\/size * 100)}%)] [{round(time.perf_counter() - start_time, 2):>6}s]\")\n\n# def eval_epoch(dataloader, model, loss_fn):\n#     size = len(dataloader.dataset)\n#     model.eval()\n#     test_loss = 0","e15e23fd":"torch.autograd.set_detect_anomaly(True)","ac5eef2b":"n_iters = 100\n# last_epoch = checkpoint['epoch']\nlast_epoch = -1\n\nfor epoch in range(last_epoch + 1, n_iters):\n    print(f\"Epoch {epoch + 1}\\n---------------------\")\n    try:\n        train_epoch(train_loader, model, loss_fn, optimizer, epoch)\n    except:\n        print(f\"Epoch {epoch + 1} has nan value\")\n        break\n    # save each epochs\n    torch.save(dict(\n        epoch=epoch,\n        model_state_dict=model.state_dict(),\n        optimizer_state_dict=optimizer.state_dict(),\n    ), root_path + '\/deepspeech.state')","c01a1b7e":"feature, rate, transcript = train_set[0]\nout = model(feature.to(device))\nout.size()","b58f2f48":"lm_path = '..\/input\/librispeech-4gram-language-model\/4-gram.arpa'","43537055":"labels = vocab.get_itos()\nprint(labels)","1975ba46":"decoder = CTCBeamDecoder(\n    labels,\n    model_path=lm_path,\n    alpha=10,\n    beta=2,\n    cutoff_top_n=40,\n    cutoff_prob=1.0,\n    beam_width=100,\n    num_processes=4,\n    blank_id=0,\n    log_probs_input=True\n)","e5bcdb57":"beam_results, beam_scores, timesteps, out_lens = decoder.decode(out)","0571f917":"def argmax_decode(predict):\n    data = []\n    for i in range(predict.size(0) - 1):\n        if predict[i] == predict[i + 1]:\n            continue\n        else:\n            data.append(predict[i])\n    data.append(predict[-1])\n    data = torch.LongTensor(data)\n    return decode_string(data)","d2de229e":"transcript","0729945b":"for i in range(beam_results.size(1)):\n    try:\n        decoded = beam_results[:, i, out_lens[0][0]]\n        print(argmax_decode(decoded))\n    except Exception as e:\n        print(str(e))\n    print('-' * 100)","bc17af31":"## Load state dict & train model","9ae1922b":"# Decode"}}