{"cell_type":{"3af6bf5b":"code","4c0a5c33":"code","3b814409":"code","75c2f368":"code","b6f7e6fa":"code","4023f1e4":"code","c8d7f467":"code","e92fec34":"code","75aba9c7":"code","cd2805ff":"code","0914d3e0":"code","85cd174c":"code","d3ac5a98":"code","6b19c1fb":"code","7a1ecfe6":"code","a02928be":"code","a0ebb924":"code","43e2f2ac":"code","c61c1b77":"code","adb1b570":"code","76066735":"code","b9533fa4":"code","966b57ca":"code","222d06c2":"code","ee3f74a9":"code","bc6562e7":"code","5d1a7924":"code","2adf2f0d":"code","cfe15c42":"code","e47be68f":"markdown","e79be191":"markdown","1ea0ca2b":"markdown","cfa8b97e":"markdown","285c0eac":"markdown","9d6c09f2":"markdown","fa5b5dec":"markdown","7ffbb345":"markdown"},"source":{"3af6bf5b":"import pandas as pd\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image","4c0a5c33":"all_data = pd.read_csv(\"..\/input\/shakespeare-plays\/Shakespeare_data.csv\", sep=\",\")","3b814409":"wrdcld = pd.Series(all_data['PlayerLine'].tolist()).astype(str)","75c2f368":"mask = np.array(Image.open(\"..\/input\/shakespeare-plays\/william-shakespeare-black-silhouette.jpg\"))","b6f7e6fa":"from wordcloud import WordCloud\n\ncloud = WordCloud(mask=mask, margin=0, max_font_size=125).generate(' '.join(wrdcld.astype(str)))\nplt.figure(figsize=(20,15))\nplt.imshow(cloud)\nplt.axis('off')\nplt.show()","4023f1e4":"all_data['Player'].replace(np.nan, 'Other', inplace=True)\n\nall_data.head(10)","c8d7f467":"len(all_data)","e92fec34":"all_data.groupby('Play').count()","75aba9c7":"play_data = all_data.groupby('Play').count().sort_values(by='PlayerLine', ascending=False)['PlayerLine']\nplay_data\n","cd2805ff":"play_data = play_data.to_frame()\nplay_data","0914d3e0":"play_data.index = np.arange(0, len(play_data))","85cd174c":"play_data = all_data.groupby('Play').count().sort_values(by='PlayerLine',ascending=False)['PlayerLine']\nplay_data = play_data.to_frame()\nplay_data['Play'] = play_data.index.tolist()\nplay_data.index = np.arange(0,len(play_data)) #changing the index from plays to numbers\nplay_data.columns =['Lines','Play']","d3ac5a98":"number_players = all_data.groupby(['Play'])['Player'].nunique().sort_values(ascending=False).to_frame()","6b19c1fb":"number_players['Play'] = number_players.index.tolist()\nnumber_players.columns = ['NumPlayers', 'Play']\n\nnumber_players.index= np.arange(0, len(number_players))\n\nnumber_players","7a1ecfe6":"plt.figure(figsize=(10,10))\nax = sns.barplot(x= 'NumPlayers',y='Play',data=number_players )\nax.set(xlabel=\"NUmber of players\", ylabel='Play Name')\n#play_data.index= np.arange(0,len(numberPlayers)))\nplt.show()","a02928be":"full_text = \"\\n\".join(all_data.PlayerLine)\nprint(len(full_text))","a0ebb924":"import collections\nimport re","43e2f2ac":"counter = 1\nfor i in all_data.PlayerLine:\n    print(i)\n    print(re.sub('[^A-Za-z]+', ' ', i).strip().lower())\n    if counter>5:\n        break\n    counter+=1","c61c1b77":"# ignoring punctuation and captalisation\n\ndef ignore_punctuation(text_array):\n    return [re.sub('[^A-Za-z]+', ' ', line ).strip().lower() for line in text_array]\n\nlines = ignore_punctuation(all_data.PlayerLine)","adb1b570":"print(lines[0])\nprint(lines[1])","76066735":"def tokenize(lines, token='word'):\n    if token=='word':\n        return [line.split(' ') for line in lines]\n    \n    elif token=='char':\n        return [list(line) for line in lines]\n    \n    else :\n        print('ERROR: unknown type: ' + token)\n\ntokens = tokenize(lines)","b9533fa4":"for i in range(5):\n    print(tokens[i])","966b57ca":"def count_corpus(tokens):\n    token_dict = {}\n    \n    for token in tokens:\n        for i in token:\n            if i not in token_dict.keys():\n                token_dict[i] = 1\n            else:\n                token_dict[i] += 1\n    \n    #print(token_dict)\n    \n    if len(tokens)==0 or isinstance(tokens[0], list):\n        print(\"tokens needs flattening\")\n        tokens = [token for line in tokens for token in line ]\n    new_dict = collections.Counter(tokens)\n    \n    #print(new_dict)\n    return new_dict","222d06c2":"count_corpus(tokens[:5])","ee3f74a9":"class Vocab:\n    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):\n        if tokens==None:\n            tokens=[]\n        if reserved_tokens==None:\n            reserved_tokens=[]\n        \n        counter = count_corpus(tokens)\n        \n        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=1)\n        \n        self.idx_to_token = ['<unk>'] + reserved_tokens\n        \n        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}\n        \n        for token, freq in self._token_freqs:\n            if freq < min_freq:\n                break\n            if token not in self.token_to_idx:\n                self.idx_to_token.append(token)\n                self.token_to_idx[token] = len(self.idx_to_token) -1 \n        \n    \n    def __len__(self):\n        return len(self.idx_to_token)\n    \n    def __getitem__(self, tokens):\n        if not isinstance(tokens, (list, tuple)):\n            return self.token_to_idx.get(tokens, self.unk)\n        return [self.__getitem(token) for token in tokens]\n    \n    def to_tokens(self, indices):\n        if not isinstance(indices, (list, tuple)):\n            return self.idx_to_token[indices]\n        return [self.idx_to_token[index] for index in indices]\n    \n    def unk(self):\n        return 0\n    \n    def token_freqs(self):\n        return self.token_freqs","bc6562e7":"vocab = Vocab(tokens)","5d1a7924":"print(list(vocab.token_to_idx.items())[:10])","2adf2f0d":"corpus = [vocab[token] for line in tokens for token in line]","cfe15c42":"print(\"Total length of corpus: \", len(corpus))\nprint(\"Vocab size of shakespeare: \", len(vocab))","e47be68f":"wordcloud from : https:\/\/www.kaggle.com\/kingburrito666\/data-wordcloud-shakespeares-most-sentimental","e79be191":"### Creating Vocabulary\n\nThe string type of the token is inconvenient to be used by models, which take numerical inputs.\nNow let us build a dictionary, often called vocabulary as well, to map string tokens into numerical\nindices starting from 0. To do so, we first count the unique tokens in all the documents from the\ntraining set, namely a corpus, and then assign a numerical index to each unique token according to\nits frequency. Rarely appeared tokens are often removed to reduce the complexity. Any token that\ndoes not exist in the corpus or has been removed is mapped into a special unknown token < unk >.\nWe optionally add a list of reserved tokens, such as < pad > for padding, < bos > to present the\nbeginning for a sequence, and < eos > for the end of a sequence.\n","1ea0ca2b":"### TEXT preprocessing on entire data","cfa8b97e":"### Creating a wordcloud","285c0eac":"### Reading the dataset","9d6c09f2":"### Replacing None in Player with Other","fa5b5dec":"first lets convert everything to small case and ignore punctuations.","7ffbb345":"### Tokenization \n\nEach text sequence is split into tokens"}}