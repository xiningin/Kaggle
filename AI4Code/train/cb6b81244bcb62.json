{"cell_type":{"6c978ff7":"code","55e57d23":"code","02ec25fc":"code","21effe27":"code","2912f419":"code","3aafc0f2":"code","969bda29":"code","4e9efc41":"code","fa28a531":"code","77134e52":"code","12dfef21":"code","621f4ac1":"code","2b905cff":"code","4491fbaf":"code","a6f45fa5":"code","571d2422":"code","bfd7cf8a":"code","9c0cc33e":"code","5c9b515f":"code","d8add90e":"code","5a157f94":"code","e6cb57f9":"code","193b14f7":"code","c7c66236":"code","ab4d4175":"code","65b93017":"code","5f60f879":"code","29c18523":"code","b23a00f5":"code","16882825":"code","f1a7d11e":"code","36e75fdb":"code","40f60e2a":"code","00d253a9":"code","e1cd5f35":"code","998a4cf7":"code","6807ed22":"code","508d6e70":"code","255302eb":"markdown","515421f5":"markdown","9d717734":"markdown"},"source":{"6c978ff7":"#Import numpy,pandas and nltk\n#Download necessary nltk Components \n\nimport numpy as np\nimport pandas as pd\nimport nltk\ntrain_data=pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest_data=pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\n","55e57d23":"#Check the number of records and number of features\ntrain_data.shape,test_data.shape","02ec25fc":"#Checkout the train dataframe\ntrain_data.head()","21effe27":"#Checkout the test dataframe\ntest_data.head()","2912f419":"#Check for NULL values in Train data\ntrain_data.isnull().sum()","3aafc0f2":"#Check for NULL values in test data\ntest_data.isnull().sum()","969bda29":"#Lets explore keyword column now\ntrain_data.keyword.unique()","4e9efc41":"#we will be considering the text columns only \ntrain_text=train_data.text\ntest_text=test_data.text","fa28a531":"#extract dependent variable\ny=train_data.target","77134e52":"import re","12dfef21":"#Write some basic function to clean our texts\ndef clean_text(text):\n    text=text.lower()\n    text=re.sub('#','',text)\n    text=re.sub('[^a-zA-Z ]','',text)\n    return text","621f4ac1":"#check train data before cleaning\ntrain_text.head()","2b905cff":"#apply that cleaning function to our data\ntrain_text=train_text.apply(clean_text)\ntest_text=test_text.apply(clean_text)","4491fbaf":"#Check data after cleaning\ntrain_text.head()","a6f45fa5":"#import stopwords and lemmatizer for lemmatization and remove some unnecessary keywords \nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nlemmatizer=WordNetLemmatizer()","571d2422":"#lemmatize the words in each text,remove some unnecessary keywords like this,the,an etc and append the final sentence in a list called train_sequence\ntrain_sequence=[]\nfor i in range(len(train_text)):\n    words=nltk.word_tokenize(train_text.iloc[i])\n    words=[lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n    sent=' '.join(words)\n    train_sequence.append(sent)","bfd7cf8a":"#Check length of train_sequence list\nlen(train_sequence)","9c0cc33e":"#do the same procedure for test data\ntest_sequence=[]\nfor i in range(len(test_text)):\n    words=nltk.word_tokenize(test_text.iloc[i])\n    words=[lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n    sent=' '.join(words)\n    test_sequence.append(sent)","5c9b515f":"len(test_sequence)","d8add90e":"#Import tfidf vectorizer from sklearn library\nfrom sklearn.feature_extraction.text import TfidfVectorizer","5a157f94":"#create tfidf object which produces 10000 features\ntfidf=TfidfVectorizer(min_df=2,ngram_range=(1,3),max_features=10000)","e6cb57f9":"# fit and transform train data\nvectorized_train=tfidf.fit_transform(train_sequence)","193b14f7":"vectorized_train.shape","c7c66236":"#transform test data\nvectorized_test=tfidf.transform(test_sequence)","ab4d4175":"vectorized_test.shape","65b93017":"#convert vectorized sparse matrix into an array\nvectorized_train=vectorized_train.toarray()\nvectorized_test=vectorized_test.toarray()","5f60f879":"vectorized_train[0]","29c18523":"#import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression","b23a00f5":"#split our train data into train and test set\nfrom sklearn.model_selection import train_test_split","16882825":"#creating 20% of test data from our dataset\nX_train, X_test, y_train, y_test = train_test_split(vectorized_train,y,test_size=0.2,random_state=0)","f1a7d11e":"classifier=LogisticRegression(C=3)","36e75fdb":"#fit training data \nclassifier.fit(X_train,y_train)","40f60e2a":"#evaluating our model\nclassifier.score(X_test,y_test)","00d253a9":"#prediction of out test data\ny_pred=classifier.predict(vectorized_test)","e1cd5f35":"id = test_data.id","998a4cf7":"output_df=pd.DataFrame({'id':id,'target':y_pred})","6807ed22":"output_df","508d6e70":"#create submissoin file\noutput_df.to_csv(\"submission2.csv\",index=False)","255302eb":"For many columns, there are same keywords hence we are droping it right now\nlater on if needed we will consider keeping them in our data","515421f5":"missing values in location column in both train and test dataset are comparatively higher, keeping total number of records in mind.\nso we are going to ignore the location column","9d717734":"Hello Everyone. So here we are going to perform some basic EDA and will try to predict the given scenerio using TF-IDF Vectorizer technique.\n\n\"Suggestions and Corrections will be Appriciated!!\" :)"}}