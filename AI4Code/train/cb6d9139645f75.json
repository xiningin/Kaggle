{"cell_type":{"09f11477":"code","c336465c":"code","e60e7887":"code","a1dfeac7":"code","b4b6ce33":"code","64a531c9":"code","1659d928":"code","ce8c73a7":"code","beb72551":"code","0097212d":"code","dccde7be":"code","33ac4eda":"code","cc5e8de1":"code","24da8391":"code","955bb184":"code","75015388":"code","0fb78c4a":"code","ba0ddfe2":"code","dadf8a1d":"code","3878319b":"code","d925a53f":"code","6b506fae":"code","ab87fea8":"code","3511d658":"code","f76327d2":"code","87c9d153":"code","81f662ee":"code","1dcbe69c":"code","b33fff7b":"code","5ca9eee6":"code","f2de8726":"code","70894a59":"code","b7300031":"code","6f4410fe":"markdown","5c29812a":"markdown","0d0c37e9":"markdown","f33f7c15":"markdown","75d173f8":"markdown","953b48ea":"markdown","c6f132ec":"markdown","48247944":"markdown","06827b56":"markdown","bd919e03":"markdown","81e681e0":"markdown"},"source":{"09f11477":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport dask.dataframe as dd # asynchronous load and subset which is useful for large dataset sampling\nfrom dask.distributed import Client\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"whitegrid\", context='paper')\nsns.set(rc={'figure.figsize':(18,5)})\n\nfrom functools import wraps # a ditty decorator\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nimport random # random integers\nimport os # i\/o read files\nimport time # time my work\nimport gc  # clear ram\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir())","c336465c":"def time_this(func): \n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        end = time.time()\n        print(\"{} takes {} minutes!\".format(func.__name__, round((end-start)\/60, 2)))\n        gc.collect()\n        return result \n    return wrapper","e60e7887":"wd = '..\/input\/netflix-prize-data\/' # working dir\ndata_files = [wd + 'combined_data_{}.txt'.format(i) for i in range(1,5)]\nprint('Data files to be combined and pre-processed: ')\nprint(data_files)","a1dfeac7":"get_movie_id = lambda line: int(line.replace(':', '').replace('\\n', ''))\nget_rating = lambda line: [x.replace('\\n', '') for x in line.split(',')]\ndf_it = lambda row: pd.DataFrame(row, index=['cust_id', 'rating', 'date']).T\n\ndef append_to_csv(data, fp):\n    print('Writing {} rows to {}'.format(data.shape[0], fp))\n    if os.path.exists(fp):\n        data.to_csv(fp, mode='a', index=False, header=False)\n    else:\n        data.to_csv(fp, index=False)\n\n@time_this\ndef get_ratings(fp):\n    \"\"\"\n    Parse the text files that have movie id and customer ratings into a usable dataframe.\n    @fp: file path (str)\n    \"\"\"\n    print('Getting ratings from file: {}'.format(fp))\n    agg_data = []\n    with open(fp, 'r') as file_:\n        for line_number, line in enumerate(file_):\n            if (line_number % 10**7) == 0: print('{} million rows..'.format(line_number \/ (10**6)))\n            if ':' in line:\n                movie_id = get_movie_id(line)                      \n            else:\n                rating_row = get_rating(line)\n                rating = {str(col): val for col,val in enumerate(rating_row)}\n                rating['movie_id'] = movie_id\n                agg_data.append(rating)            \n    agg_data = pd.DataFrame(agg_data)\n    agg_data.rename(columns={'0': 'cust_id', '1': 'rating', '2': 'date'}, inplace=True)\n    print('Finished getting ratings from file: {}'.format(fp))\n    return agg_data","b4b6ce33":"#for fp in data_files:\ndf = get_ratings(data_files[0])\nappend_to_csv(df, 'ratings.csv')\ndel df\ngc.collect()","64a531c9":"os.listdir()","1659d928":"data = dd.read_csv('ratings.csv')\nprint(data.shape)\nprint(data.head())","ce8c73a7":"n = random.randint(0, 30)\ndata = data[data.cust_id % n == 0]\nclient = Client()   # initialize the cluster\ndata = client.persist(data)\ndata = data.compute()\ndata['date'] = pd.to_datetime(data['date'])\ndata['year'] = data['date'].dt.year\nprint(data.shape)","beb72551":"movie_titles = pd.read_csv(wd + 'movie_titles.csv',\n                           encoding = 'ISO-8859-1', # some weird encoding issue\n                           header = None, names = ['movie_id', 'year', 'name'])\nmovie_titles.drop_duplicates(subset=['name', 'year'], inplace=True)\nmovie_meta = pd.read_csv('..\/input\/the-movies-dataset\/movies_metadata.csv', \n                         low_memory=False,\n                         usecols=['adult', 'budget', 'original_title',\n                                 'release_date', 'popularity', 'revenue', 'genres'])\nmovie_meta.dropna(how='any', inplace=True)\nmovie_meta['release_date'] = pd.to_datetime(movie_meta['release_date'])\nmovie_meta['meta_year'] = movie_meta['release_date'].dt.year\nmovie_meta.drop_duplicates(subset=['original_title', 'meta_year'], inplace=True)\nmovie_titles.sample(5)","0097212d":"movie_meta = movie_titles.merge(movie_meta, how='left', left_on=['name', 'year'], right_on=['original_title', 'meta_year'],\n                  validate='1:1', indicator=True).sort_values('_merge', ascending=False).query(\"_merge == 'both'\")","dccde7be":"sns.set(rc={'figure.figsize':(15,4)})\nsns.set_style('whitegrid')\ngrouped = movie_titles.groupby('year', as_index=False)['movie_id'].count()\nax = sns.lineplot(x='year', y='movie_id', data=grouped, color='red')\nax.set_title('Movies by Release Date')\nax.set_xlabel('Year')\nax.set_ylabel('Movie Count')","33ac4eda":"data = data.merge(movie_titles, how='left', on='movie_id',validate='m:1', suffixes=['', '_of_release'])\ndata.sample(5)","cc5e8de1":"agg = {\n    'cust_id': 'nunique',\n    'rating': {'min', 'max', 'mean', 'count'}\n}\nratings = data.groupby(['name', 'year_of_release'], as_index=False).agg(agg)\nratings.columns = ['_'.join(col).strip('_') for col in ratings.columns.values]\nratings.rename(columns={'cust_id_cust_id': 'unique_customers'}, inplace=True)\nratings.sample(5)","24da8391":"ax = ratings.query('rating_count > 300').plot(kind='scatter', x='year_of_release', y='rating_mean', \n                                              color='red', s=4)\nax.set_ylabel('Average Rating')\nax.set_xlabel('Year of Movie Release')\nplt.annotate('* Subset to Movies with >300 Ratings', (.75,0), (0,-40), xycoords='axes fraction', \n             textcoords='offset points', va='top', fontsize=10)\nax.set_title('Movie Ratings by Year of Release')","955bb184":"top = ratings.query('rating_count > 300').sort_values('rating_mean').tail(10)[['name', 'rating_mean']]\nbottom = ratings.query('rating_count > 300').sort_values('rating_mean').head(10)[['name', 'rating_mean']]\nfig, axes = plt.subplots(2, figsize=(15,7), sharex=True)\nax1 = plt.subplot(2, 1, 1)\ntop.set_index('name').plot(kind='barh', color='red', ax=ax1, legend=False)\nax1.set_ylabel('')\nax1.set_xlim(0,5)\nax1.xaxis.set_visible(False)\nax2 = plt.subplot(2, 1, 2)\nbottom.set_index('name').plot(kind='barh', color='blue', ax=ax2, legend=False, position=1)\nax2.set_xlabel('User Rating')\nax2.set_ylabel('')\nax2.set_xlim(0,5)\nplt.annotate('* Subset to Movies with >300 Ratings', (.75,0), (0,-40), xycoords='axes fraction', \n             textcoords='offset points', va='top', fontsize=10)\nfig.suptitle(\"Highest and Lowest Rated Movies\", fontsize=16)","75015388":"fig, axes = plt.subplots(2, figsize=(20,7))\nfig.suptitle(\"User Ratings & Volume Over Time\", fontsize=16)\nax = plt.subplot(1, 2, 1)\ngrouped = data.groupby('year', as_index=False).agg(\n    {'rating': 'mean', 'name': 'nunique', 'cust_id': 'count'})\\\n.rename(columns={'name': 'number of unique movies', 'cust_id': 'volume of reviews'})\ngrouped.plot(x='year', y=['rating','volume of reviews'], \n             secondary_y=['volume of reviews'], color=['red', 'blue'], ax=ax, grid=True)\nax.set_xlabel('')\nax2 = plt.subplot(1, 2, 2)\ngrouped.plot(x='year', y='number of unique movies', ax=ax2, color='red')\nax2.set_xlabel('')","0fb78c4a":"min_movie = 1000   # movie has to have been rated over 1000 times\nmin_user = 200   # user has to have rated at least 200 times\nusers = data.groupby('cust_id')['rating'].count()\nusers = users.loc[users > min_user].index.values\nmovies = data.groupby('movie_id')['rating'].count()\nmovies = movies.loc[movies > min_movie].index.values\nfiltered = data.loc[data.cust_id.isin(users) & data.movie_id.isin(movies)]\nprint('Unfiltered: ', data.shape[0])\nprint('Filtered: ', filtered.shape[0])\nprint('Kept {}% of data'.format(round(filtered.shape[0]\/data.shape[0], 2)*100))","ba0ddfe2":"filtered.sample(5)","dadf8a1d":"mat = filtered.pivot_table(index='cust_id', columns='movie_id', values='rating')\nprint('The User-Movie Matrix')\nmat.sample(10)","3878319b":"means = filtered.groupby('movie_id')['rating'].mean().to_dict()   # get a lookup table of movie to mean rating\ntopNrecs = mat.copy(deep=True)\nfor col in topNrecs:                    # for each movie\n    already_rated = topNrecs[col].notnull()    # make note of which ones theyve already rated\n    topNrecs[col].fillna(means[col], inplace=True)    # fill out the mean rating for each movie\n    topNrecs.loc[already_rated, col] = np.nan       # remove the information we already have\nprint('Average User Rating Imputed onto Users Matrix')\ntopNrecs.sample(10)","d925a53f":"recommendations = topNrecs.stack()\\\n.reset_index()\\\n.rename(columns={0: 'imputed_rating'})\\\n.groupby('cust_id')\\\n.apply(lambda x: x.nlargest(5, columns='imputed_rating'))\\\n.reset_index(drop=True)\\\n.sort_values(by=['cust_id', 'imputed_rating'], ascending=[True, False])\\\n.merge(movie_titles, how='left', on='movie_id', validate='m:1')\\\n.rename(columns={'name': 'recommended_movie_name',\n                 'year': 'year_of_release'})\nprint('For each user, pick the top 5 movies that they have seen (to be used to merge in).')\nrecommendations.head(10)","6b506fae":"agg_rec = recommendations.groupby(['recommended_movie_name', 'year_of_release'])['cust_id'].nunique()\\\n                        .sort_values(ascending=True)\nrb_palette = [(x\/10.0, x\/100.0, x\/40.0) for x in range(len(agg_rec.tail(10)))] \n# <-- gradient rgb   (x\/10.0, x\/20.0, 0.75)\nax = agg_rec.tail(10).plot(kind='barh', x='cust_id', color=rb_palette)\nax.set_title('Most recommended movies to Users')\nax.set_ylabel('')\nax.set_xlabel('Number of times movie made it to Users Top 5 Recommendation')\nprint('Top Movie Recommendations')","ab87fea8":"userSim = mat.copy(deep=True)\ncorr = userSim.T.corr(min_periods=50)    # pairwise pearson correlation coefficient of columns\ncorr.head(5)","3511d658":"threshold = 0.10\nprint('Set the threshold similarity between users to be .1 given the distribution of corrs.')\npd.Series(np.triu(corr.values).flatten()).dropna().describe(percentiles=[x*.1 for x in range(10)]).round(2)","f76327d2":"nearest_users = corr.stack()\\\n.reset_index(level=1)\\\n.rename(columns={\n    'cust_id': 'cust_id_2',\n    0: 'similarity_score'})\\\n.reset_index()\\\n.query('similarity_score > {}'.format(threshold))\\\n.query('cust_id != cust_id_2')\\\n.groupby('cust_id')\\\n.apply(lambda x: x.nlargest(5, columns='similarity_score'))\\\n.reset_index(drop=True)\\\n.sort_values(by=['cust_id', 'similarity_score'], ascending=[True, False])\nprint('For each user, get the nearest 5 users (not themselves) that are above the threshold similarity.')\nnearest_users.head(10)","87c9d153":"top5perUser = mat.stack()\\\n.reset_index()\\\n.rename(columns={0: 'rating'})\\\n.groupby('cust_id')\\\n.apply(lambda x: x.nlargest(5, columns='rating'))\\\n.reset_index(drop=True)\\\n.sort_values(by=['cust_id', 'rating'], ascending=[True, False])\\\n.merge(movie_titles, how='left', on='movie_id', validate='m:1')\\\n.rename(columns={'name': 'recommended_movie',\n                 'year': 'year_of_release'})\ntop5perUser.head(10)","81f662ee":"top5perUser['rank'] = top5perUser.assign(count=1).groupby('cust_id')['count'].transform('cumsum')\ntop5recs = top5perUser.drop(['movie_id', 'rating', 'year_of_release'], axis=1)\\\n.set_index(['cust_id', 'rank'])\\\n.unstack().reset_index()\ntop5recs.columns = ['_'.join([str(x) for x in col]).strip('_') for col in top5recs.columns.values]\nprint('For each user, get the top 5 recommended movies.')\ntop5recs.sample(5)","1dcbe69c":"userUserRecs = nearest_users.merge(top5recs, how='left', left_on='cust_id_2', right_on='cust_id', suffixes=['', '_'])\\\n.drop('cust_id_', axis=1)\nprint('For each customer, merge in the similar users recommended movies')\nuserUserRecs.head(10)","b33fff7b":"seenMovie = mat.stack()\\\n.reset_index()\\\n.rename(columns={0: 'user_rating'})\\\n.assign(customer_seen_movie_flag = 1)\nseenMovie.head()","5ca9eee6":"threshold = .6\nweightedRatings = mat.stack()\\\n.reset_index()\\\n.rename(columns={0: 'rating'})\\\n.merge(nearest_users, how='right', left_on='cust_id', right_on='cust_id_2', suffixes=['_', ''])\\\n.drop('cust_id_', axis=1)\\\n.query('similarity_score > {}'.format(threshold))\\\n.query('cust_id != cust_id_2')\\\n.assign(user_rating_weighted_by_similarity = lambda x: (x.rating * x.similarity_score))\\\n.groupby(['cust_id', 'movie_id'], as_index=False)[['user_rating_weighted_by_similarity', 'similarity_score']].sum()\\\n.assign(prediction = lambda x: (x.user_rating_weighted_by_similarity \/ x.similarity_score))\\\n.sort_values(by=['cust_id', 'prediction'], ascending=[True, False])\\\n.merge(movie_titles, how='left', on='movie_id', validate='m:1')\\\n.rename(columns={'name': 'recommended_movie',\n                 'year': 'year_of_release'})\\\n.merge(seenMovie, how='left', on=['cust_id', 'movie_id'])\nweightedRatings.sample(10)","f2de8726":"print('Threshold pearsons score: {}.'.format(threshold))\nprint('Increasing the threshold will decrease the number of customers we can provide \\n recommendations for but increases the quality of the recommendation.')\nprint('------------')\nprint('Recommendations available for {} out of {} users.'.format(weightedRatings.cust_id.nunique(), mat.shape[0]))\nweightedRatings.groupby('cust_id', as_index=False).agg({'recommended_movie': 'nunique', 'similarity_score': {'min', 'mean', 'max'}})\\\n.rename(columns={'recommended_movie': 'number_of_recommendations'}).sample(10)","70894a59":"print('Subset to movies customers havent seen.')\nweightedRatings.loc[(weightedRatings.customer_seen_movie_flag != 1) & (weightedRatings.prediction > 4)].sample(10)","b7300031":"sub = weightedRatings.loc[(weightedRatings.customer_seen_movie_flag == 1)]\nsample_size = round(.2 * sub.shape[0])\nsample = sub.sample(sample_size)\nfrom scipy.stats.stats import pearsonr\nstats = pearsonr(sample.user_rating, sample.prediction)\nax = sns.stripplot(x=\"user_rating\", y=\"prediction\", data=sample, jitter=True, color='red', size=3)\nax.set_title('User Ratings to Predicted Rating')\nax.set_ylabel('Prediction')\nax.set_xlabel('Current User Rating')\nprint('Lets use the data where the customer has seen the movie to evaluate these results.')\nprint('---------------')\nprint('User ratings seem to be overall trending positively with predicted value.')\nprint('Pearsons Correlation: {}, P-value: {}'.format(stats[0], stats[1]))","6f4410fe":"#### **Top N Movies**\nThe simplest form of recommendation here would be the **top N movies**. This recommendation is made to all users regardless of their preference. i.e. *\"These are the top rated movies on Netflix right now.\"*\n\nThis is fairly straightforward to do: we want to get the top rated movies (in a separate dataframe) and just impute them onto this matrix. The final output would be a matrix of (Ncustomers, 5) shape where 1-5 columns are the top 5 ranked movies they have not seen yet.","5c29812a":"# **Netflix User Recommendations**\nHow do we suggest movies to a given user based on their watch and like history as well as other things we might know about these movies? I am using this kernel to explore my thoughts. I'm learning so I'm drawing things I find from other sources and hopefully stitching them together in a way that makes me feel confident I understand how to build a recommendation engine.","0d0c37e9":"#### **User-User Similarity**\nAnother slightly more useful method would be to identify users that are similar and source recommendations from other movies similar users have rated highly. The high level questions I'm trying to answer are the following:\n* Which users are similar to each other?               *---->   Correlate users*\n* How do I determine a cutoff for similarity?           *---->   Establish a threshold*\n* What movies have the similar users seen that I can recommend to the user in question?           *---->   Pull in relevant recommendations*\n\nIn this example, I use the **Pearson's correlation coefficient** to tell me how similar two independent customers are based on their movie ratings. Pandas allows us to take the **user movie matrix** that we created before and **transpose** it so that **each column represents a different user's movie ratings** for N movie rows. Then we compute the **pairwise correlation coefficient of each user (column) to each other** , creating a **User-User Matrix**. I use the min periods option to specify that a minimum of 50 overlapping movie ratings are required for a user to be correlated with one another.\n\n**Note**: This is an incredibly inefficient process and gets poorer in efficiency as the number of customers outnumber the number of movies. I explain this to myself as, each user vector has so many movie dimensions and each user vector has to be compared against another user vector along those dimensions so its N users * N users worth of computations. There are better alternate ways but I wanted to show how to do this anyway since I'm sure there are situations in which this is useful.","f33f7c15":"**Dask** dataframe employs a **lazy parallel read** operation which will allow me to interact with the massive CSV I've created and filter it. Since **Dask is more limited than Pandas in its API**, I will only really do this to create the **random sample**; then I'll port to pandas directly and work from there. I randomly select customers by using a modulo with a random integer between 1 and 50. This should result in 26k customers sampled. \n\nI'd like to **sample by customer so that I can retain the entirety of that customer's history of ratings**. When predicting what a customer will like, I would like to have their history available for information.","75d173f8":"I'll also be reading in the movies dataset provided to us for additional information. I want to take a quick look at this dataset before we continue. Most of the movies in Netflix's armoire seem to have been **released in the last 25 years.** I will also fuzzy merge the movies metadata CSV onto this dataset so that I can get some additional information for use later. A couple of notes about this movies dataset:\n* There are 17,700 movies in the Netflix dataset.\n* Only 6,300 of those movies have a match in the metadata file (since this is useless I'll just subset down to the movies that do have metadata).\n","953b48ea":"### Recommendation \"Engines\"\nThe first thing we need to do is create a user movie matrix. Each row in this matrix will represent a customer and each column will represent a movie of the 17,700 movies in this dataset. We should expect to see a lot of nulls (a sparse matrix) here because it would be crazy if a person had managed to watch all 17,700. This will become useful in the following ways:\n* We can try to find similar users (rows) to segment the population.\n* We can use this matrix to identify movies a user has not seen, and [fill in potential ratings](http:\/\/https:\/\/www.kaggle.com\/morrisb\/how-to-recommend-anything-deep-recommender\/notebook.)","c6f132ec":"#### **As Netflix gets better at predicting what users want, overall user ratings will increase over time.**","48247944":"### Netflix's Movie Selections\nAfter merging in the movies datasets with the ratings dataset, we can aggregate up to get some **statistics by movie**. Movies that were **released before the 80s** all universally seem to have **higher ratings; as we get closer to more recent years, ratings become noisy**. ","06827b56":"An **alternate way** to do this would be to **generate predictions of what the user would rate a movie** based on what similar users rated the movie and **use those predictions to generate recommendations**. This will **not limit** us to the **top 5 movies** since we will be able to rank them instead. \n1. We will still have to use a threshold here to reduce the chance that dissimilar users sway the recommendation. \n2. Then we give a prediction (weighted rating) by using the formula P(user,item) = \u2211(other user ratings of the item multiplied by similarity to user) \/ \u2211(similarity scores to user). \n3. Then we'll the merge in the movie titles for each movie.\n4. Then filter to movies the user hasn't seen yet.","bd919e03":"## Data Load & Processing\nEach combined dataset of user ratings for a movie is some 26mm lines and **needs to be wrangled** get it into tabular format. It will be useful to create a large csv massive dataset and persist it on disk for us to **sample** from. This will also remove any biases that might come from selecting just the first file to load into memory. I'll time my process to make sure I've done it in the most efficient way. This part of the kernel takes <10 minutes to run. Have a coffee, come back for some fun.","81e681e0":"I came across a **good guide for recommendations engine** [here](http:\/\/https:\/\/www.analyticsvidhya.com\/blog\/2018\/06\/comprehensive-guide-recommendation-engine-python\/) and I'll try to select the excerpts that helped me understand what to do. I relied on this to understand the differences between recommendation engines and the math that works behind it so I would read this before looking on for how I acted on that information.\n>  Consider the example of Netflix. They save all the information related to each user in a vector form. This vector contains the past behavior of the user, i.e. the movies liked\/disliked by the user and the ratings given by them. This vector is known as the profile vector. All the information related to movies is stored in another vector called the item vector. Item vector contains the details of each movie, like genre, cast, director, etc.\n\nThis is like the matrix we just created. Each row is the behavior of the user (the profile vector) and each movie is the item vector (which at this point contains just the ratings of other users).\n\n>The content-based filtering algorithm finds the cosine of the angle between the profile vector and item vector, i.e. cosine similarity. Suppose A is the profile vector and B is the item vector, then the similarity between them can be calculated. Based on the cosine value, which ranges between -1 to 1, the movies are arranged in descending order and one of the two below approaches is used for recommendations:\n> * Top-n approach: where the top n movies are recommended (Here n can be decided by the business)\n> * Rating scale approach: Where a threshold is set and all the movies above that threshold are recommended\n\nIn the following sections I will create recommendations using the following methodologies:\n1. Top N Movies across all users\n2. User-User Similarity\n3. Movie-Movie Similarity"}}