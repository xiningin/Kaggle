{"cell_type":{"1b622372":"code","0dac3ad4":"code","5232dc8b":"code","4ba68210":"code","1e3c4047":"code","b3a4785f":"code","575ce7c2":"code","dadbb393":"code","08264917":"code","78cc2bd0":"code","7c19ac94":"code","eeba18bd":"code","05eb9dc5":"code","14a9eeef":"code","83dd0f5b":"code","0a5c0480":"code","d3dece27":"code","01e0d86a":"code","8fa9d03a":"code","d90355d0":"code","8a61494f":"markdown","e370cd61":"markdown","bb9c93b3":"markdown","7ff26a53":"markdown","2931be0d":"markdown","f0f78ab1":"markdown","cd4d3a93":"markdown","fd4af2c0":"markdown","3ea125ca":"markdown","761b4b5c":"markdown","9181f30b":"markdown","195fa6b4":"markdown","32ec7ae0":"markdown"},"source":{"1b622372":"from io import StringIO\n\nimport numpy as n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, BatchNormalization, Activation, Dropout\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import SGD\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error as MSE\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0dac3ad4":"data = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-3\/train.csv', index_col='id')\nsubmission = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-3\/eval.csv', index_col='id')","5232dc8b":"data.columns","4ba68210":"data['Eat'].describe()","1e3c4047":"len(data) - len(data['pubchem_id'].unique())","b3a4785f":"data.isna().any().any()","575ce7c2":"print(set(data.columns) - set(submission.columns))\nprint(set(submission.columns) - set(data.columns))","dadbb393":"def get_neural_network_generator(input_shape):\n    '''\n    Returns a lambda expression that generates a neural network.\n    '''\n    def generate_model(first=100, layers=10, neurons=4, dropout=True):\n        '''\n        Build a neural network.\n        '''\n        model = Sequential()\n        \n        model.add(Dense(first, activation='linear', kernel_initializer='he_normal', input_shape=input_shape))\n        model.add(BatchNormalization())\n        model.add(Activation('elu'))\n        for _ in range(layers):\n            model.add(Dense(neurons, activation='linear', kernel_initializer='he_normal'))\n            model.add(BatchNormalization())\n            model.add(Activation('elu'))\n        if dropout:\n            model.add(Dropout(rate=.2))\n        model.add(Dense(1, activation='linear'))\n\n        optimizer = SGD(learning_rate=0.1, momentum=0.9, nesterov=True)\n        model.compile(optimizer='adam', loss='mean_squared_error')\n        return model\n    \n    return generate_model","08264917":"def split_X_y(df):\n    '''\n    Split a dataframe into:\n    - A dataframe of the features.\n    - (If applicable) a dataframe of the target.\n    '''\n    if 'Eat' in df.columns:\n        X = df.drop(['pubchem_id', 'Eat'], axis=1)\n        y = df['Eat']\n    else:\n        X = df.drop(['pubchem_id'], axis=1)\n        y = None\n    \n    return X, y","78cc2bd0":"X, y = split_X_y(data)\nX_shape = (X.shape[1],)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)","7c19ac94":"def hyperparameter_search(layers_list, neurons_list, dropout_list, first_list):\n    '''\n    Ad-hoc hyperparameter search.\n    '''\n    Model = get_neural_network_generator(input_shape=X_shape)\n\n    print('first,layers,neurons,dropout,train,val')\n    for first in first_list:\n        for layers in layers_list:\n            for neurons in neurons_list:\n                for dropout in dropout_list:\n                    model = Model(first=first, layers=layers, neurons=neurons, dropout=dropout)\n                    model.fit(X_train, y_train,\n                              validation_split=.3,\n                              epochs=200,\n                              batch_size=32,\n                              verbose=0)\n                    train_score = MSE(y_train, model.predict(X_train)) ** .5\n                    val_score = MSE(y_test, model.predict(X_test)) ** .5\n                    print(f'{first},{layers},{neurons},{dropout},{train_score},{val_score}')\n    print('done')","eeba18bd":"def visualize_hyperparameter_search(results_df, indep_vars):\n    '''\n    Keep indep_vars 2d.\n    '''\n    for indep_var in indep_vars:\n        sns.lineplot(x=indep_var, y='train', hue='dropout', data=results_df)\n        sns.lineplot(x=indep_var, y='val', hue='dropout', data=results_df)\n        plt.title(f'{indep_var} vs score.')\n        plt.ylabel('score')\n        plt.show()\n    \n    # Heatmap help spot lower scores + visualize the distribution of val scores on the hyperparameter space.\n    # Note that we experimentally found 'dropout=True' to be strictly worse.\n    sns.heatmap(results_df[results_df['dropout'] == False].drop('first', axis=1).pivot('layers', 'neurons', 'val'))\n    plt.title('Val scores where dropout=False', )\n    plt.show()\n\n    return results_df.set_index(['first', 'layers', 'neurons', 'dropout'])","05eb9dc5":"# This is the result of:\n# hyperparameter_search(layers_list=range(8, 16+1, 2),\n#                       neurons_list=[3, 4, 5, 6],\n#                       dropout_list=[True, False],\n#                       first_list=[95])\nhyperparameter_search_output = '''\nfirst,layers,neurons,dropout,train,val\n95,8,3,True,0.5623976423637111,0.5557699721460119\n95,8,3,False,0.5780023994520088,0.5738695688328241\n95,8,4,True,0.5408253136639795,0.5271540267853173\n95,8,4,False,0.2751148070853048,0.28378792239564027\n95,8,5,True,0.5161783065187452,0.5169447034988781\n95,8,5,False,0.45228558390646534,0.45572521616871053\n95,8,6,True,0.4391209043349728,0.4300606185146309\n95,8,6,False,0.24050367099852896,0.2466730417559886\n95,10,3,True,0.8599342989319689,0.8587045499014221\n95,10,3,False,0.7109176044972854,0.709293292868897\n95,10,4,True,0.4639043742054948,0.452693583230689\n95,10,4,False,0.4359385547108613,0.4228864658420832\n95,10,5,True,0.5263724467900753,0.5322734972241973\n95,10,5,False,0.20487950438088914,0.2158966043158773\n95,10,6,True,0.4994336438827635,0.4973376504405244\n95,10,6,False,0.4250304174802989,0.41113348737961847\n95,12,3,True,0.7798166403424623,0.7706664089756521\n95,12,3,False,0.6422230490865043,0.6266235492169684\n95,12,4,True,0.7376228792354479,0.7247873725560269\n95,12,4,False,0.26670370422319545,0.26706149272808805\n95,12,5,True,0.602156213301103,0.5910129354909169\n95,12,5,False,0.27636174363071625,0.28386681838564964\n95,12,6,True,0.48730287250315224,0.47398086610557383\n95,12,6,False,0.2951070848491038,0.3099767032474081\n95,14,3,True,0.561834667190648,0.5478375645459298\n95,14,3,False,0.48433523880338747,0.481145952472405\n95,14,4,True,0.4934440307718421,0.4661793621519685\n95,14,4,False,0.2621580708731682,0.27219447108901346\n95,14,5,True,0.5344052501005522,0.5322407590885889\n95,14,5,False,0.4303381402796072,0.4393857639142209\n95,14,6,True,0.6662266532445641,0.6530106375400986\n95,14,6,False,0.28966319173656346,0.2967395286694352\n95,16,3,True,1.0195642061891874,1.0152393880222073\n95,16,3,False,1.0521825178716058,1.0554571427514308\n95,16,4,True,0.46798797428055733,0.45873946899566614\n95,16,4,False,0.4699847202825809,0.46709828787618185\n95,16,5,True,0.5821682814760226,0.5635869437526648\n95,16,5,False,0.2130831428096727,0.2349984816491459\n95,16,6,True,0.6933037864243308,0.688182514323461\n95,16,6,False,0.29483598876250733,0.2946889460830955\n'''\nhyperparameter_search_results = pd.read_csv(StringIO(hyperparameter_search_output))\n\nvisualize_hyperparameter_search(hyperparameter_search_results, indep_vars=['layers', 'neurons'])","14a9eeef":"# This is the result of:\n# hyperparameter_search(layers_list=[6, 8, 10, 12, 14, 16, 18],\n#                       neurons_list=[5, 6, 7, 8],\n#                       dropout_list=[False],\n#                       first_list=[95])\nhyperparameter_search_output = '''\nfirst,layers,neurons,dropout,train,val\n95,6,5,False,0.271504709349373,0.28631659737634507\n95,6,6,False,0.33713919154391514,0.329173075711912\n95,6,7,False,0.22881057542194091,0.2423339963190349\n95,6,8,False,0.31979863779000306,0.32049821633227915\n95,8,5,False,0.3471037425485091,0.35491287842985314\n95,8,6,False,0.39492561048653313,0.4055714560927945\n95,8,7,False,0.345905127679544,0.34634756581602394\n95,8,8,False,0.4353487298183549,0.2418236831204888\n95,10,5,False,0.2700569858946342,0.2667316731280619\n95,10,6,False,0.3731037650474255,0.3806527993441702\n95,10,7,False,0.19904758148731214,0.21263046483235515\n95,10,8,False,0.1903483748293882,0.20810783985414108\n95,12,5,False,0.3609313351920665,0.3706532792048498\n95,12,6,False,0.26569060345634843,0.29268957544619595\n95,12,7,False,0.2705750720758427,0.27501864422425787\n95,12,8,False,0.42524631205305663,0.4246566318919037\n95,14,5,False,0.33369550038912366,0.3503967653491459\n95,14,6,False,0.37146649700577966,0.3863205789584235\n95,14,7,False,0.27331226568557004,0.2729612259344516\n95,14,8,False,0.23882702743310932,0.22760634841890948\n95,16,5,False,0.4197791990805318,0.4219306688215339\n95,16,6,False,0.21091569537829843,0.2178550194600869\n95,16,7,False,0.2719185917143293,0.2773364558519683\n95,16,8,False,0.1995178628319916,0.19613222916254694\n95,18,5,False,0.4377472293981297,0.4363366700950816\n95,18,6,False,0.2903235198063755,0.29351151203120585\n95,18,7,False,0.19974061646987684,0.2332245318395733\n95,18,8,False,0.3164985112952696,0.29056817940599927\n'''\nhyperparameter_search_results = pd.read_csv(StringIO(hyperparameter_search_output))\n\nvisualize_hyperparameter_search(hyperparameter_search_results, indep_vars=['layers', 'neurons'])","83dd0f5b":"def evaluate_model(Model, X, y, trials=10):\n    '''\n    Create a distribution of scores for a given model.\n    \n    param Model    Class or lambda expression that returns an instance.\n    '''\n    \n    test_scores = []\n    val_scores = []\n    \n    for i in range(trials):\n        early_stopping = EarlyStopping(monitor='val_loss', patience=25)\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=i)\n\n        model = Model()\n        model.fit(X_train, y_train,\n                  validation_split=.3,\n                  epochs=200,\n                  batch_size=32,\n                  callbacks=[early_stopping],\n                  verbose=0)\n        \n        test_scores.append(MSE(y_train, model.predict(X_train)) ** .5)\n        val_scores.append(MSE(y_test, model.predict(X_test)) ** .5)\n        \n        \n    return pd.DataFrame({\n        'test_scores': test_scores,\n        'val_scores': val_scores,\n    })","0a5c0480":"Model = get_neural_network_generator(input_shape=X_shape)\n\nmodels = [\n    {\n        'first': 95,\n        'layers': 8,\n        'neurons': 6,\n        'dropout': False,\n    },\n    {\n        'first': 95,\n        'layers': 10,\n        'neurons': 8,\n        'dropout': False,\n    },\n    {\n        'first': 95,\n        'layers': 16,\n        'neurons': 8,\n        'dropout': False,\n    },\n]\n\nfor hyperparams in models:\n    print('=' * 80)\n    print(f'Model has params first={hyperparams[\"first\"]}, layers={hyperparams[\"layers\"]}, neurons={hyperparams[\"neurons\"]}, dropout={hyperparams[\"dropout\"]}.')\n    \n    result = evaluate_model(lambda: Model(**hyperparams), X_train, y_train)\n    \n    sns.swarmplot(y='val_scores', data=result)\n    plt.show()\n    sns.violinplot(y='val_scores', data=result)\n    plt.show()\n    print(result.describe())\n    \n    print()","d3dece27":"early_stopping = EarlyStopping(monitor='val_loss', patience=25)\n\nlayers = 10\nneurons = 8\ndropout = False\nfirst = 95\n\nModel = get_neural_network_generator(input_shape=X_shape)\nmodel = Model(first=first, layers=layers, neurons=neurons, dropout=dropout)\nmodel.fit(X_train, y_train,\n          validation_split=.3,\n          epochs=200,\n          batch_size=32,\n          callbacks=[early_stopping],\n          verbose=0)","01e0d86a":"MSE(y_test, model.predict(X_test)) ** .5","8fa9d03a":"MSE(y_train, model.predict(X_train)) ** .5","d90355d0":"X_sub, _ = split_X_y(submission)\n\nprediction_raw = model.predict(X_sub.to_numpy())\n\nprediction_df = pd.DataFrame({'id': X_sub.index,\n                              'Eat': prediction_raw.ravel()})\nprediction_df.to_csv('submission.csv', index=False)\nprint(prediction_df.describe())\n\nprint(prediction_df.to_string())","8a61494f":"# Generate Submission","e370cd61":"# Evaluate Model","bb9c93b3":"### Hyperparam Searching\n\nTo speed things up, I removed the call to the actual search and just hardcoded the results in.","7ff26a53":"# Imports + Load CSV","2931be0d":"### Extraneous Features\n\nFeature `'pubchem_id'` is a categorical feature where every single observation is unique. Useless!","f0f78ab1":"#### JK\n\nWe're gonna let the neural network do it for us.","cd4d3a93":"### Missing Data\n\nThere is no missing data!","fd4af2c0":"# EDA","3ea125ca":"#### Target\n\nIt appears the target vector, the atomization energy `'Eat'`, is always negative. We thus have the option of post-processing our predictions.","761b4b5c":"# Model","9181f30b":"# Choosing Model\n\nUltimately, score on the leaderboard is what matters most. Thus, I went with the best scoring model. It slightly overfits, but it should be good nonetheless. Other options are available to decrease overfitting and to increase speed, but I felt this is one of the best models to choose.","195fa6b4":"# Feature Engineering","32ec7ae0":"### Split Data"}}