{"cell_type":{"268f28ef":"code","948d9c80":"code","31fdbcf8":"code","407f0780":"code","6684f8dc":"code","6d8d7113":"code","cdf9ef54":"code","4d79a74e":"code","075cb6be":"code","5ecc1fa6":"code","93ddcda0":"code","2dd2e30e":"code","0354e5d2":"code","971a0fe8":"code","b8f86eee":"code","cfa82789":"code","bfdbb6bc":"code","5f01a79b":"code","82a83812":"code","237046ef":"code","d5b445e8":"code","37a1c220":"code","20e7bb29":"code","a374e983":"code","940b70b6":"markdown","18757bfe":"markdown","06d6e547":"markdown","8b510c18":"markdown","8816bfd5":"markdown","4ecd0c9b":"markdown","f4647d51":"markdown","f61e19b7":"markdown","b79a7ae6":"markdown","cdec9cd1":"markdown","8f71d8e5":"markdown","6f3bf1c0":"markdown"},"source":{"268f28ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","948d9c80":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","31fdbcf8":"test = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/eval.csv')\ntrain = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/train.csv')\nsub = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/sample_submission.csv')\nprint(train)","407f0780":"#Observing the Data that we are working with\nprint(train.head())","6684f8dc":"#No Null values\nprint(\"Training Null values\")\nprint(train.isnull().sum())\nprint('-----------------------------------------------')\nprint(\"Testing Null values\")\nprint(test.isnull().sum())","6d8d7113":"#These features don't effect esrb rating meaning we are filtering out useless information for our model\ntrain_data = train\ntest_data = test\n\ntrain_data = train_data.drop('title', 1)\ntrain_data = train_data.drop('console', 1)\ntrain_data = train_data.drop('id', 1)\n\n#test_data = test.drop('title', 1)\ntest_data = test_data.drop('console', 1)\ntest_data = test_data.drop('id', 1)\n\nprint(train_data.shape)\nprint(test_data.shape)\nprint(train_data)","cdf9ef54":"#Allows model to accept them as integers and not characters\/strings\n#since these are extremly important we need to convert them and not drop them\nprint(train_data['esrb_rating'].unique())\n\ntrain_data['esrb_rating'] = train['esrb_rating'].map({\n    'E' : 0,\n    'ET' : 1,\n    'T' : 2,\n    'M' : 3\n}).astype(int)\n\n\n\nprint(train_data['esrb_rating'])","4d79a74e":"#Finding out all the E ratings for each category\nprint(train_data[(train_data['esrb_rating'] == 0)].sum())\n\n#values 0-32 represent from alocohol_reference --> violence\nE_rating = train_data[(train_data['esrb_rating'] == 0)].sum()\nli = list(range(0, 32))\n\nplt.xlabel('Labels')\nplt.ylabel('E rating amount')\nplt.plot(li,E_rating)","075cb6be":"#Finding out all the ET ratings for each category\nprint(train_data[(train_data['esrb_rating'] == 1)].sum())\n\n#values 0-32 represent from alocohol_reference --> violence\nET_rating = train_data[(train_data['esrb_rating'] == 1)].sum()\nli = list(range(0, 32))\n\nplt.xlabel('Labels')\nplt.ylabel('ET rating amount')\nplt.plot(li,ET_rating)","5ecc1fa6":"#Finding out all the T ratings for each category\nprint(train_data[(train_data['esrb_rating'] == 2)].sum())\n\n#values 0-32 represent from alocohol_reference --> violence\nT_rating = train_data[(train_data['esrb_rating'] == 2)].sum()\nli = list(range(0, 32))\n\nplt.xlabel('Labels')\nplt.ylabel('T rating amount')\nplt.plot(li,T_rating)","93ddcda0":"#Finding out all the M ratings for each category\nprint(train_data[(train_data['esrb_rating'] == 3)].sum())\n\n#values 0-32 represent from alocohol_reference --> violence\nM_rating = train_data[(train_data['esrb_rating'] == 3)].sum()\nli = list(range(0, 32))\n\nplt.xlabel('Labels')\nplt.ylabel('M rating amount')\nplt.plot(li,M_rating)","2dd2e30e":"X_train = train_data.drop('esrb_rating', axis = 1)\ny_train = train_data['esrb_rating']\nX_test = test_data","0354e5d2":"import sklearn\nimport math\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import TimeSeriesSplit\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import RFE\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.pipeline import make_pipeline","971a0fe8":"#Logistic Regression\n# log = LogisticRegression(random_state=0).fit(X_train, y_train)\n# print(log.score(X_train, y_train))\n\ntuned_parameters = {'C': [0.1, 0.5, 1, 5, 10, 50, 100]}\nlog = GridSearchCV(LogisticRegression(solver='liblinear'), tuned_parameters, cv=5, scoring=\"accuracy\")\nlog.fit(X_train, y_train)\n\nprint(log.score(X_train, y_train))\nprint(log.best_params_)","b8f86eee":"log_res = cross_val_score(log, X_train, y_train, cv=10, n_jobs=-1, scoring='accuracy')\nCV_Res_Log = pd.DataFrame(log_res).describe()\nprint(\"LOG Results\", CV_Res_Log)","cfa82789":"#SVM\nfrom sklearn.svm import SVC\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\n\n\nsvm = SVC().fit(X_train, y_train)\nestimator = SVR(kernel=\"linear\")\nrfe = RFE(estimator = SVC(), n_features_to_select=31)\nsvm.fit(X_train, y_train)\nprint(svm.score(X_train, y_train))","bfdbb6bc":"svm_res = cross_val_score(svm, X_train, y_train, cv=10, n_jobs=-1, scoring='accuracy')\nCV_Res_Svm = pd.DataFrame(svm_res).describe()\nprint(\"SVM Results\", CV_Res_Svm)","5f01a79b":"#Decision Tree\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.model_selection import GridSearchCV\n\ntree_para = {'criterion':['gini','entropy'],'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}\ntre = GridSearchCV(DecisionTreeClassifier(), tree_para, cv=5)\ntre.fit(X_train, y_train)\n\nprint(tre.score(X_train, y_train))\nprint(tre.best_params_)","82a83812":"tre_res = cross_val_score(tre, X_train, y_train, cv=10, n_jobs=-1, scoring='accuracy')\nCV_Res_Tre = pd.DataFrame(tre_res).describe()\nprint(\"DT Results\", CV_Res_Tre)","237046ef":"#Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True) \n\nparam_grid = { \n    'n_estimators': [200, 700],\n    'max_features': ['auto', 'sqrt', 'log2']\n}\n\nCV_rfc = GridSearchCV(estimator=clf, param_grid=param_grid, cv= 5)\nCV_rfc.fit(X_train, y_train)\nprint (CV_rfc.best_params_)","d5b445e8":"rf_res = cross_val_score(CV_rfc, X_train, y_train, cv=10, n_jobs=-1, scoring='accuracy')\nCV_Res_Rf = pd.DataFrame(rf_res).describe()\nprint(\"RF Results\", CV_Res_Rf)","37a1c220":"#KNN\nfrom sklearn.neighbors import KNeighborsClassifier\n# neigh = KNeighborsClassifier(n_neighbors=3).fit(X_train, y_train)\n# neigh.score(X_train, y_train)\n\npipe = Pipeline([\n        ('sc', StandardScaler()),     \n        ('knn', KNeighborsClassifier(algorithm='brute')) \n    ])\nparams = {\n        'knn__n_neighbors': [3, 5, 7, 9, 11]\n    }\nneigh = GridSearchCV(estimator=pipe,           \n                      param_grid=params, \n                      cv=5,\n                      return_train_score=True)\nneigh.fit(X_train, y_train)\nprint(neigh.score(X_train, y_train))\nprint(neigh.best_params_)","20e7bb29":"kn_res = cross_val_score(neigh, X_train, y_train, cv=10, n_jobs=-1, scoring='accuracy')\nCV_Res_Kn = pd.DataFrame(kn_res).describe()\nprint(\"KNN Results\", CV_Res_Kn)","a374e983":"#Decision Tree\npredictions=svm.predict(X_test)\n\nans = []\nfor i in range(len(predictions)):\n    if predictions[i] == 0:\n        ans.append('E')\n        \n    elif predictions[i] == 1:\n        ans.append('ET')\n        \n    elif predictions[i] == 2:\n        ans.append('T')\n        \n    elif predictions[i] == 3:\n        ans.append('M')\n\n\n\n#This must be converted back to strings since we originally converted the esrb ratings into ints\nprint(ans)\n\ntesting = pd.DataFrame({'id' : test.id, 'esrb_rating': ans})\ntesting.to_csv('esrb.csv', index = False)\n\nfrom IPython.display import FileLink\nFileLink(r'esrb.csv')","940b70b6":"# Models","18757bfe":"## Submission","06d6e547":"### Data Import","8b510c18":"## Library","8816bfd5":"### Converting esrb to ints","4ecd0c9b":"# Summary\n\nWhen Working on this assignment I looked into various approaches\n* ### Data Manipulation\n  * I tried having if statements that would see if certain columns were set to 1, it would automatically be at least T\n      * This didnt work since there wasn't much consistency with the ratings, assuming that multiple people rate them\n* ### Dropping columns\n    * I found out that dropping ID, console, and title helped filter out some un-needed data\n* ### Looking for patterns\n    * Tying into with data manipulation, looking for set patterns didn't work for me since many people have varying opinions\n* ### Best Model\n    * My best model was SVM, I believe that random forest and decision tree over fitted to the data which made it not work as well as the SVM.","f4647d51":"## Outliers","f61e19b7":"#### Even though Decision Tree and Random Forest had the best result for one iteration, SVM over 10 iterations had a better overall result","b79a7ae6":"### NaN values","cdec9cd1":"For handling Outliers, I can observe where all the columns give us the value 1\nI won't be dropping any outliers since they're all necesarry to see the rating of the various esrb ratings","8f71d8e5":"### Data Setup","6f3bf1c0":"### "}}