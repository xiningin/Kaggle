{"cell_type":{"de7e0437":"code","fe2f582f":"code","7d8e64ef":"code","5726f348":"code","56d56b7a":"code","93d367bd":"code","04650782":"code","23b88cbd":"code","950b47f5":"code","bdbc3fa0":"code","f5bd073a":"code","9416a0cb":"code","0d6b7092":"code","fb833004":"code","9432ed4a":"code","84079f11":"code","24b0fb6f":"code","9adebb2c":"code","c85d3add":"code","252803c5":"code","9d38d1c3":"code","bada657c":"code","32408244":"code","3234511d":"code","3f3e988b":"code","13abb502":"code","98e5c3f1":"code","666b2ba1":"code","8ddd1fba":"code","7e98dbd4":"code","b8b38b2b":"code","098efdc2":"code","7f657019":"code","ceada2f2":"code","46c82bd1":"code","3d83343d":"code","8eeaaa7a":"code","4f3452bf":"code","19b7d6d5":"code","fc043613":"code","9469006d":"code","a191a387":"code","966cb762":"code","d3e7e596":"markdown","c2224a85":"markdown","feb5496a":"markdown","54713ea1":"markdown","bd3b501e":"markdown","93e3069b":"markdown","54c9007d":"markdown","082e4657":"markdown","5b14837f":"markdown","02037855":"markdown","f5842776":"markdown","825dd23e":"markdown"},"source":{"de7e0437":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport time\n\n#Algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n\n#from sklearn.svm import SVC,NuSVC\n#from sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier,GradientBoostingClassifier,VotingClassifier,BaggingClassifier\n#from sklearn.tree import DecisionTreeClassifier\n#from sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\n#import xgboost as xgb\n#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.layers import Embedding\n#rom keras.layers import Conv1D, GlobalMaxPooling1D\n#import keras\n#from keras.datasets import imdb\n\n# Text Processing\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport spacy\nimport string\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom textblob import TextBlob\nimport nltk\nfrom sklearn.preprocessing import LabelEncoder\nfrom nltk import FreqDist\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\n\n\n### Sentiment Analysis\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\n### Dimensionality Reduction\nfrom sklearn.decomposition import PCA\nfrom sklearn import random_projection\nfrom sklearn.decomposition import FastICA\nfrom sklearn.feature_selection import VarianceThreshold\n\n### Scaling\nfrom sklearn.preprocessing import MinMaxScaler,StandardScaler,normalize\n\n### Dataset Split\nfrom sklearn.model_selection import train_test_split\n\n# Evalution Metric\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n#from keras import metrics\n\nimport os\nimport re\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fe2f582f":"# LOADING DATASET\ntraining_set = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntesting_set = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsample_submission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\n\n# LoADING STOPWORDS\nstop_words = stopwords.words('english')\n\n# LOADING SPACY ENGLISH LANGUAGE MODEL\nnlp = spacy.load(\"en\")\n\n# LOADING WORD LIST FROM NLTK\nwords = set(nltk.corpus.words.words())\n\n# DECLARING LEMMATIZER OBJECT AND SENTIMENT OBJECT\nlemmatizer = WordNetLemmatizer()\nanalyser = SentimentIntensityAnalyzer()","7d8e64ef":"#DECLARE PUNCTUATION STRING\nstring.punctuation = '!#\"$%&\\'()*+,-.\/:;<=>?@[\\\\]^_`{|}~'","5726f348":"def freq_words(df):    \n    text_list = []\n    less_frequent_word = []\n    for rec in df[\"text\"]:\n        for words in rec.split():\n            text_list.append(words)\n    freqDist = FreqDist(text_list)\n    words = list(freqDist.keys())\n    for wrd in words:\n        if freqDist[wrd] <= 3:\n            less_frequent_word.append(wrd)\n    return less_frequent_word","56d56b7a":"\ncol_to_drop_train = []\ncol_to_drop_test = []\nless_freq_wrd_train = freq_words(training_set)\nless_freq_wrd_test = freq_words(testing_set)\nless_freq_wrd = less_freq_wrd_test + less_freq_wrd_train\nonce_present_word = list(set(less_freq_wrd))\nwrds_ignore = once_present_word+stop_words","93d367bd":"def text_preprocessing(text):\n    sentiment_value = 0\n    sentence = \"\"\n    text = text.lower()\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    for wrd in text.split():\n        if wrd not in wrds_ignore:\n            #if wrd in words:\n            if wrd.isalpha():\n                wrd = lemmatizer.lemmatize(wrd)\n                if wrd not in sentence:\n                    sentence += \" \".join(wrd.split())+\" \"\n\n    sentence = sentence.strip()\n    return sentence","04650782":"training_set[\"text\"] = training_set[\"text\"].apply(text_preprocessing)\ntesting_set[\"text\"] = testing_set[\"text\"].apply(text_preprocessing)","23b88cbd":"dictionary = []\nfor dataframe in [training_set[\"text\"],testing_set[\"text\"]]:\n    for sentence in dataframe:\n        for words in sentence.split():\n            dictionary.append(words)\n            \nunique_dictionary = list(set(dictionary))","950b47f5":"# for text,iD in zip(training_set[training_set.keyword.isnull()][\"text\"],training_set[training_set.keyword.isnull()][\"text\"].index):\n#     for wrd in text.split():\n#         if wrd.startswith(\"#\"):\n#             training_set.loc[iD,\"keyword\"] = wrd[1:]","bdbc3fa0":"# for text,iD in zip(testing_set[testing_set.keyword.isnull()][\"text\"],testing_set[testing_set.keyword.isnull()][\"text\"].index):\n#     for wrd in text.split():\n#         if wrd.startswith(\"#\"):\n#             testing_set.loc[iD,\"keyword\"] = wrd[1:]","f5bd073a":"#training_set[training_set[\"target\"]==0][\"text\"]","9416a0cb":"for text,iD in zip(testing_set[testing_set.keyword.isnull()][\"text\"],testing_set[testing_set.keyword.isnull()][\"text\"].index):\n    doc = nlp(text)\n    wrds_pos = []\n    for ent in doc.ents: \n        if ent.label_ == \"GPE\":\n            #testing_set.loc[iD,\"keyword\"] = ent.text\n            #testing_set.loc[iD,\"location\"] = ent.text\n            wrds_pos.append(ent.text)\n        elif ent.label_ == \"NORP\":\n            #testing_set.loc[iD,\"keyword\"] = ent.text\n            wrds_pos.append(ent.text)\n        elif ent.label_ == \"LOC\":\n            #testing_set.loc[iD,\"keyword\"] = ent.text\n            wrds_pos.append(ent.text)\n        elif ent.label_ == \"ORG\":\n            #testing_set.loc[iD,\"keyword\"] = ent.text\n            wrds_pos.append(ent.text)\n        elif ent.label_ ==\"EVENT\":\n            #testing_set.loc[iD,\"keyword\"] = ent.text\n            wrds_pos.append(ent.text)\n        elif ent.label_ == \"FAC\":\n            #testing_set.loc[iD,\"keyword\"] = ent.text\n            wrds_pos.append(ent.text)\n        elif ent.label_ == \"LANGUAGE\":\n            #testing_set.loc[iD,\"keyword\"] = ent.text\n            wrds_pos.append(ent.text)\n        elif ent.label_ == \"PRODUCT\":\n            #testing_set.loc[iD,\"keyword\"] = ent.text\n            wrds_pos.append(ent.text)\n#         elif ent.label_ == 'WORK_OF_ART':\n#             testing_set.loc[iD,\"keyword\"] = ent.text\n#             wrds_pos.append(ent.text)\n        else:\n            wrds_pos.append(\"\")\n    for token in nlp(text):\n        if token.pos_ == \"NOUN\":\n            #testing_set.loc[iD,\"keyword\"] = token.text\n            wrds_pos.append(token.text)\n        elif token.pos_ == \"VERB\":\n            #testing_set.loc[iD,\"keyword\"] = token.text\n            wrds_pos.append(token.text)\n#         elif token.pos_ == \"PROPN\":\n#             testing_set.loc[iD,\"keyword\"] = token.text\n#             wrds_pos.append(token.text)\n#         elif token.pos_ == \"ADJ\":\n#             testing_set.loc[iD,\"keyword\"] = token.text\n#             wrds_pos.append(token.text)\n#         if token.pos_ == \"ADV\":\n#             testing_set.loc[iD,\"keyword\"] = token.text\n#             wrds_pos.append(token.text)\n    sent = \" \".join(wrd for wrd in wrds_pos)\n    testing_set.loc[iD,\"text\"] = sent","0d6b7092":"for text,iD in zip(training_set[training_set.keyword.isnull()][\"text\"],training_set[training_set.keyword.isnull()][\"text\"].index):\n    doc = nlp(text)\n    wrds_pos = []\n    for ent in doc.ents: \n        if ent.label_ == \"GPE\":\n            #training_set.loc[iD,\"keyword\"] = ent.text\n            #training_set.loc[iD,\"location\"] = ent.text\n            wrds_pos.append(ent.text)\n        elif ent.label_ == \"NORP\":\n            #training_set.loc[iD,\"keyword\"] = ent.text\n            wrds_pos.append(ent.text)\n        elif ent.label_ == \"ORG\":\n            #training_set.loc[iD,\"keyword\"] = ent.text\n            wrds_pos.append(ent.text)\n        elif ent.label_ == \"LOC\":\n            #training_set.loc[iD,\"keyword\"] = ent.text\n            wrds_pos.append(ent.text)\n        elif ent.label_ ==\"EVENT\":\n            #training_set.loc[iD,\"keyword\"] = ent.text\n            wrds_pos.append(ent.text)\n        elif ent.label_ == \"FAC\":\n            #training_set.loc[iD,\"keyword\"] = ent.text\n            wrds_pos.append(ent.text)\n        elif ent.label_ == \"LANGUAGE\":\n            #training_set.loc[iD,\"keyword\"] = ent.text\n            wrds_pos.append(ent.text)\n        elif ent.label_ == \"PRODUCT\":\n            #training_set.loc[iD,\"keyword\"] = ent.text\n            wrds_pos.append(ent.text)\n#         elif ent.label_ == 'WORK_OF_ART':\n#             training_set.loc[iD,\"keyword\"] = ent.text\n#             wrds_pos.append(ent.text)\n#         elif ent.label_ == 'PERSON':\n#             training_set.loc[iD,\"keyword\"] = ent.text\n#             wrds_pos.append(ent.text)\n        else:\n            wrds_pos.append(\"\")\n    for token in nlp(text):\n        if token.pos_ == \"NOUN\":\n            #training_set.loc[iD,\"keyword\"] = token.text\n            wrds_pos.append(token.text)\n        elif token.pos_ == \"VERB\":\n            #training_set.loc[iD,\"keyword\"] = token.text\n            wrds_pos.append(token.text)\n#         elif token.pos_ == \"PROPN\":\n#             training_set.loc[iD,\"keyword\"] = token.text\n#             wrds_pos.append(token.text)\n#         elif token.pos_ == \"ADJ\":\n#             training_set.loc[iD,\"keyword\"] = token.text\n#             wrds_pos.append(token.text)\n#         if token.pos_ == \"ADV\":\n#             training_set.loc[iD,\"keyword\"] = token.text\n#             wrds_pos.append(token.text)\n    sent = \" \".join(wrd for wrd in wrds_pos)\n    training_set.loc[iD,\"text\"] = sent","fb833004":"# creating label\ny = training_set[\"target\"]","9432ed4a":"# training_set[\"Sentiment\"] = pd.Series()\n# testing_set[\"Sentiment\"] = pd.Series()\n# training_set[\"Subjectivity\"] = pd.Series()\n# testing_set[\"Subjectivity\"] = pd.Series()","84079f11":"# def calculate_sentiment(sentence):\n#     text = TextBlob(sentence)\n#     #print(text)\n#     sentiment_value = text.sentiment.polarity\n#     return sentiment_value","24b0fb6f":"# def calculate_subjectivity(sentence):\n#     text = TextBlob(sentence)\n#     #print(text)\n#     subject_value = text.sentiment.subjectivity\n#     return subject_value","9adebb2c":"# training_set[\"Sentiment\"] =  training_set[\"text\"].apply(calculate_sentiment)\n# training_set[\"Subjectivity\"] =  training_set[\"text\"].apply(calculate_subjectivity)","c85d3add":"# testing_set[\"Sentiment\"] =  testing_set[\"text\"].apply(calculate_sentiment)\n# testing_set[\"Subjectivity\"] =  testing_set[\"text\"].apply(calculate_subjectivity)","252803c5":"# print(training_set[training_set[\"target\"]==1][\"Sentiment\"].sum())\n# print(training_set[training_set[\"target\"]==0][\"Sentiment\"].sum())\n# print(training_set[training_set[\"target\"]==1][\"Subjectivity\"].sum())\n# print(training_set[training_set[\"target\"]==0][\"Subjectivity\"].sum())","9d38d1c3":"tfidf_vector = TfidfVectorizer(ngram_range=(1,2))","bada657c":"# training_set[\"keyword\"] = training_set[\"keyword\"].str.replace(\"%20\",\" \")\n# testing_set[\"keyword\"] = testing_set[\"keyword\"].str.replace(\"%20\",\" \")","32408244":"#training_set.loc[training_set[\"target\"]==0,\"keyword\"] = \"unknown\"","3234511d":"# train_kw = pd.DataFrame(training_set[\"keyword\"])\n# test_kw = pd.DataFrame(testing_set[\"keyword\"])","3f3e988b":"# training_set[\"text\"] = training_set[\"text\"]+\" \"+train_kw[\"keyword\"]\n# testing_set[\"text\"] = testing_set[\"text\"]+\" \"+test_kw[\"keyword\"]","13abb502":"testing_set[\"text\"]","98e5c3f1":"text_vector_train = pd.DataFrame(tfidf_vector.fit_transform(training_set[\"text\"].apply(lambda x: np.str_(x))).toarray(),columns = tfidf_vector.get_feature_names())","666b2ba1":"text_vector_test = pd.DataFrame(tfidf_vector.transform(testing_set[\"text\"].apply(lambda x: np.str_(x))).toarray(),columns = tfidf_vector.get_feature_names())","8ddd1fba":"# train_sent = pd.DataFrame(training_set,columns=[\"Sentiment\",\"Subjectivity\"])\n# train_sent.loc[train_sent[\"Sentiment\"]<= 0.5,\"Sentiment\"] = 0","7e98dbd4":"#train_sent.loc[train_sent[\"Sentiment\"]> 0.5,\"Sentiment\"] = 1","b8b38b2b":"#train_sent[\"Sentiment\"].value_counts()","098efdc2":"#test_sent = pd.DataFrame(testing_set,columns = [\"Sentiment\",\"Subjectivity\"])","7f657019":"#train_sent.corr(method ='pearson') ","ceada2f2":"#train_sent[\"Senti_Subj\"] = train_sent[\"Sentiment\"] + train_sent[\"Subjectivity\"]","46c82bd1":"#train_sent.drop(columns=[\"Sentiment\",\"Subjectivity\"],inplace=True)","3d83343d":"#X = pd.concat([text_vector_train, train_sent], axis=1)","8eeaaa7a":"#X_test = pd.concat([text_vector_test,test_sent],axis=1)","4f3452bf":"X_train, X_valid, y_train, y_valid = train_test_split(text_vector_train, y,random_state=42, test_size=0.30)\n\nprint('Number of rows in the total set: {}'.format(text_vector_train.shape[0]))\nprint('Number of rows in the training set: {}'.format(X_train.shape[0]))\nprint('Number of rows in the test set: {}'.format(X_valid.shape[0]))","19b7d6d5":"# Tried out - Models with worst performance\n# default_classifiers = {'Gradient Boosting Classifier':GradientBoostingClassifier(),'Adaptive Boosting Classifier':AdaBoostClassifier(),'RadiusNN':RadiusNeighborsClassifier(radius=40.0),\n#                'Linear Discriminant Analysis':LinearDiscriminantAnalysis(), 'GaussianNB': GaussianNB(), 'BerNB': BernoulliNB(), 'KNN': KNeighborsClassifier(),\n#                'Random Forest Classifier': RandomForestClassifier(min_samples_leaf=10,min_samples_split=20,max_depth=4),'Decision Tree Classifier': DecisionTreeClassifier(),'Logistic Regression':LogisticRegression(), \"XGBoost\": xgb.XGBClassifier()}","fc043613":"#Dictionary of models\nclassifiers = {'BerNB': BernoulliNB(),'Logistic Regression':LogisticRegression()}","9469006d":"#Iterating through the dataset with all model declared in the dataset.\nbase_accuracy = 0\nfor Name,classify in classifiers.items():\n    classify.fit(X_train,y_train)\n    y_predictng = classify.predict(X_valid)\n    print('Accuracy Score of '+str(Name) + \" : \" +str(accuracy_score(y_valid,y_predictng)))\n    ","a191a387":"#X_test_normalized= normalize(X_test, norm='l2')\npID = sample_submission[\"id\"]\npredicted_test = classify.predict(text_vector_test)\npredicted_test_value = pd.DataFrame({ 'id': pID,\n                        'target': predicted_test })\npredicted_test_value.to_csv(\"PredictedTestScore.csv\", index=False)","966cb762":"# predicted_test = []\n# for x in model.predict_classes(X_test_ica):\n#     predicted_test.append(x[:][0])\n# predicted_test_value = pd.DataFrame({ 'PassengerId': pID,\n#                         'Survived': predicted_test })\n# predicted_test_value.to_csv(\"PredictedTestScore.csv\", index=False)","d3e7e596":"## Loading Dataset\n\n**Along with loading dataset, we are loading the stopwords from the nltk, english language model for Spacy, valid word list from nltk, Lemmatization object, Sentiment object.**","c2224a85":"> **Calculate Sentiment - Using Textblob, we are finding the sentiment score of each text in training and testing set, the value range from -1 to 1.**\n> **Calculate Subjectivity - Using Textblob, we are finding the Subjectivity score of each text in training and testing set, the value range from 0 to 1. If value close to 0, then the text is objective, while closer to 1 means the text is subjective.**","feb5496a":"### Text extraction using NER - terms like earthquake, cyclone and other such calamities are mapped as events in NER (Name Entity Recognition). So using Spacy NER, we can extract the keywords which is mentioned as event is extracted for both training and testing set.","54713ea1":"**Creating a dictionary, it is a list of words present in training and testing data**","bd3b501e":"**Deep learning for Text Classification** - https:\/\/www.kaggle.com\/jacklinggu\/keras-mlp-cnn-test-for-text-classification","93e3069b":"## Natural Language Processing\nIt is a tool to understand the textual data. Recently many pre-trained models like BERT, OpenGPT 2 etc, have made huge progress in the field on ***NLG*** and ***NLU***. This notebook present some of the basic, but vital nlp techniques implemented across all the NLP tasks are as follows\n\n* Tokenization\n* Lemmatization\n* Stopwords\n* Frequency Distribution of words\n* Vectorization\n* Sentiment","54c9007d":"**Combining all the less frequent words from both test & train and removing the duplicate words. Removing less frequent words also reduces the dimension of the vector for the model to run on.**","082e4657":"### text_preprocessing \n* **Convert text to lower,**\n* **removing the punctuation,** \n* **tokenzing the sentence,**\n* **remove words which are present in stopwords or present in frequency,**\n* **drop words if not alphabets,** \n* **lemmatize the words**\n* **combine again as sentence.**","5b14837f":"### Using .apply(), preprocess the text from training and testing data.","02037855":"### freq_words() - It outputs a list of words which is present only once throughout the dataset turning it into less frequent word, here words which are present 3 or less are made termed as less freqent words.","f5842776":"**train test split the dataset for training and validating the model**","825dd23e":"**Creating TFIDF Vectorizer, with uni-grams and bi-grams.**"}}