{"cell_type":{"f39c086b":"code","87437b39":"code","fbbb3530":"code","daafc738":"code","2887304d":"code","ab573829":"code","c6431819":"code","2098a530":"code","e93d1d8c":"code","06c1e5d2":"code","d9c04642":"code","dbd83776":"code","7235e242":"code","c54f1fd3":"code","dbb00ed2":"code","971368e4":"code","0f3f3f50":"code","2492c139":"code","52341702":"code","ec147c9a":"code","d06d7a5d":"code","ed25fa77":"code","44a4e2c1":"code","e392aa38":"code","68e926e9":"code","4bf806fb":"code","6628c3e4":"code","63f47092":"code","657f24ee":"code","f74a9947":"code","265c21a0":"code","78c31d3c":"code","2a5be165":"code","d36e6201":"code","2c78fd2e":"code","872a8579":"code","c5d0270e":"code","de23f675":"code","bb157a59":"code","024febe1":"code","d8e93321":"code","ce2ff6db":"code","471283b4":"code","aae4a9e5":"code","32d6d30d":"code","bd6a6aeb":"code","66917ae3":"code","1b313c87":"code","5e2dffdd":"code","b6af26a1":"code","077ea5c9":"code","9b5d4c99":"code","c76da22b":"code","e7c0daa1":"code","bb1be66b":"code","03de7ce3":"code","d551921e":"code","b2712df4":"code","1fd8a413":"code","28ef01ad":"code","bd92f533":"code","289a62f0":"code","208b4680":"code","3166bbc7":"code","fa71b46c":"code","70b0a82e":"code","47908bd6":"markdown","ad11f673":"markdown","c2867734":"markdown","34705960":"markdown","6a695e59":"markdown","66bee721":"markdown","5ce1dd3c":"markdown","33c8e3d0":"markdown","92d13c67":"markdown","a198c754":"markdown","ae4e5b7d":"markdown","01c5efd7":"markdown","de7822e2":"markdown","0b8ab78f":"markdown","28a8e1c4":"markdown","813d1f9b":"markdown","a259f553":"markdown","eba4e173":"markdown","40ef99ea":"markdown","79f017ee":"markdown","086d8b35":"markdown","cadec674":"markdown","cc189024":"markdown","2a9991a8":"markdown","e0a655bb":"markdown","f73d9c71":"markdown","64c4a837":"markdown","1c03dda7":"markdown","08f35d49":"markdown","41316ba8":"markdown","b949f8c9":"markdown","ba698e70":"markdown","5bf0d399":"markdown","ad5ee25e":"markdown","9e9cfbc0":"markdown","c2240fb3":"markdown","185fad86":"markdown","bc319425":"markdown","82c8e75e":"markdown","826bbd96":"markdown","e24f8ad9":"markdown","9e24c11a":"markdown","9c724342":"markdown","ba11f6ff":"markdown","1dcfaef0":"markdown","a05c87fe":"markdown","28cac018":"markdown","47a543fd":"markdown","d7324f6e":"markdown","ce0796e0":"markdown","b4f8316e":"markdown","b5175aa8":"markdown","b1b18632":"markdown","6703f1fc":"markdown","445a287a":"markdown","950191b0":"markdown","ab740f37":"markdown","46244b2c":"markdown","87ba80ae":"markdown","b6473cde":"markdown","7e6c3886":"markdown","094fcb48":"markdown","60b8fa1f":"markdown","11ae7bbc":"markdown","0288622a":"markdown","f76f5a51":"markdown","d69cfd05":"markdown","d3ffca35":"markdown","b2e86517":"markdown","06ca2970":"markdown","e6ebd6ec":"markdown","85cd5e5e":"markdown","5a19972d":"markdown","2bc1cb9b":"markdown","6a1fdb8c":"markdown","b41f8731":"markdown","62cd8b12":"markdown","645a749e":"markdown","ba66ab76":"markdown","d5d5c897":"markdown","79160abd":"markdown","d826b45f":"markdown","cc87c2b1":"markdown","de1d1901":"markdown","8ffb7ad0":"markdown","9a2c33d0":"markdown","735d8701":"markdown","8dcc3fb3":"markdown","3d8d3748":"markdown","56ba706b":"markdown","6948e90f":"markdown","8fe48c56":"markdown","a07f30fe":"markdown","e4f5c1c2":"markdown","4a6d5c18":"markdown","63373d96":"markdown","0a66ea89":"markdown"},"source":{"f39c086b":"import warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import rcParams\n\nrcParams[\"xtick.labelsize\"] = 15\nrcParams[\"ytick.labelsize\"] = 15\nrcParams[\"legend.fontsize\"] = \"small\"\n\npd.set_option(\"precision\", 2)\nwarnings.filterwarnings(\"ignore\")","87437b39":"# Import Apple\/Google stock prices\naapl_googl = pd.read_csv(\n    \"https:\/\/raw.githubusercontent.com\/BexTuychiev\/medium_stories\/master\/2021\/july\/3_time_series_manipulation\/data\/apple_google.csv\",\n    parse_dates=[\"Date\"],\n    index_col=\"Date\",\n).dropna()","fbbb3530":"aapl_googl.head()","daafc738":"# Import S&P500 stock prices\nsp500 = pd.read_csv(\n    \"https:\/\/raw.githubusercontent.com\/BexTuychiev\/medium_stories\/master\/2021\/july\/3_time_series_manipulation\/data\/sp500.csv\",\n    parse_dates=[\"date\"],\n    index_col=\"date\",\n)","2887304d":"sp500.head()","ab573829":"# Import the data with unknown date column\nsp500 = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/BexTuychiev\/medium_stories\/master\/2021\/july\/3_time_series_manipulation\/data\/sp500.csv\")\n\n# Inspect the dtypes\nsp500.dtypes","c6431819":"sp500.head()","2098a530":"sp500[\"date\"] = pd.to_datetime(sp500[\"date\"], format=\"%Y-%m-%d\", errors=\"coerce\")\n\n# Check if the conversion is successful\nassert sp500[\"date\"].dtype == \"datetime64[ns]\"","e93d1d8c":"sp500.set_index(\"date\", inplace=True)","06c1e5d2":"stamp = pd.Timestamp(\"2020\/12\/26\")  # You can pass any date-like string\nstamp","d9c04642":"from datetime import datetime\n\nstamp = pd.Timestamp(\n    datetime(year=2021, month=10, day=5, hour=13, minute=59, second=59)\n)\nstamp","dbd83776":"attributes = [\n    \".year\",\n    \".month\",\n    \".quarter\",\n    \".day\",\n    \".hour\",\n    \".minute\",\n    \".second\",\n    \".weekday()\",\n    \".dayofweek\",\n    \".weekofyear\",\n    \".dayofyear\",\n]\n\npd.DataFrame(\n    {\n        \"Attribute\": attributes,\n        \"'2021-10-05 13:59:59'\": [\n            eval(f\"stamp{attribute}\") for attribute in attributes\n        ],\n    }\n)","7235e242":"index = pd.date_range(start=\"2010-10-10\", end=\"2020-10-10\", freq=\"M\")\nindex","c54f1fd3":"index[0]","dbb00ed2":"pd.date_range(start=\"2020-01-01\", periods=5, freq=\"Y\")","971368e4":"aliases = [\"B\", \"D\", \"W\", \"M\", \"BM\", \"MS\", \"Q\", \"H\", \"A, Y\"]\nvalues = [\n    \"Business days\",\n    \"Calendar days\",\n    \"Weekly\",\n    \"Month end frequency\",\n    \"Business month end frequency\",\n    \"Month start frequency\",\n    \"Quarterly\",\n    \"Hourly\",\n    \"Year end\",\n]\n\npd.DataFrame({\"Frequency Alias\": aliases, \"Definition\": values})","0f3f3f50":"aapl_googl[\"2010\":\"2015\"].sample(5)  # All rows within 2010 and 2015","2492c139":"aapl_googl[\"2012-4\":\"2012-12\"].sample(5)  # rows within April and December of 2012","52341702":"aapl_googl.loc[\"2012-10-10\":\"2012-12-10\", \"GOOG\"].head()","ec147c9a":"# Choose 200 random\nrandom_indices = np.random.choice([_ for _ in range(len(aapl_googl))], size=200)\n\n# Mark the indices as missing\nclone = aapl_googl.copy(deep=True).drop(\"AAPL\", axis=1)\nclone.iloc[random_indices, 0] = np.nan","d06d7a5d":"def compare_dists(original_dist, imputed_dists: dict):\n    \"\"\"\n    Plot original_dist and imputed_dists on top of each other\n    to see the difference in distributions.\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(12, 7), dpi=140)\n    # Plot the original\n    sns.kdeplot(\n        original_dist, linewidth=5, ax=ax, color=\"black\", label=\"Original dist.\"\n    )\n    for key, value in imputed_dists.items():\n        sns.kdeplot(value, linewidth=3, label=key, ax=ax)\n\n    plt.legend()\n    plt.show();","ed25fa77":"from sklearn.impute import SimpleImputer\n\nfor method in [\"mean\", \"median\", \"most_frequent\"]:\n    clone[method] = SimpleImputer(strategy=method).fit_transform(\n        clone[\"GOOG\"].values.reshape(-1, 1)\n    )","44a4e2c1":"compare_dists(\n    clone[\"GOOG\"],\n    {\"mean\": clone[\"mean\"], \"median\": clone[\"median\"], \"mode\": clone[\"most_frequent\"]},\n)","e392aa38":"clone.drop([\"mean\", \"median\", \"most_frequent\"], axis=1, inplace=True)","68e926e9":"sample = pd.Series([np.nan, 2, 3, np.nan, 4, np.nan, np.nan, 5, 12, np.nan]).to_frame(\n    name=\"original\"\n)\nsample","4bf806fb":"sample[\"ffill\"] = sample[\"original\"].ffill()\nsample[\"bfill\"] = sample[\"original\"].bfill()\n\nsample","6628c3e4":"air_q = pd.read_csv(\n    \"https:\/\/raw.githubusercontent.com\/BexTuychiev\/medium_stories\/master\/2021\/july\/3_time_series_manipulation\/data\/station_day.csv\",\n    usecols=[\"Date\", \"NO2\"],\n    parse_dates=[\"Date\"],\n    index_col=\"Date\",\n)\n\nfor method in [\"ffill\", \"bfill\"]:\n    air_q[method] = eval(f\"air_q['NO2'].{method}()\")\n\ncompare_dists(air_q[\"NO2\"], {\"ffill\": air_q[\"ffill\"], \"bfill\": air_q[\"bfill\"]})","63f47092":"sample = pd.Series([1] + [np.nan] * 6 + [10]).to_frame(name=\"original\")\nsample","657f24ee":"sample[\"linear\"] = sample.original.interpolate(method=\"linear\")\nsample[\"nearest\"] = sample.original.interpolate(method=\"nearest\")\n\nsample","f74a9947":"from sklearn.impute import KNNImputer\n\nn_neighbors = [2, 3, 5, 7, 9]\n\nfor k in n_neighbors:\n    imp = KNNImputer(n_neighbors=k)\n    clone[f\"k={k}\"] = imp.fit_transform(clone[\"GOOG\"].values.reshape(-1, 1))\n\ncompare_dists(clone[\"GOOG\"], {f\"k={k}\": clone[f\"k={k}\"] for k in n_neighbors})","265c21a0":"sp500 = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/BexTuychiev\/medium_stories\/master\/2021\/july\/3_time_series_manipulation\/data\/sp500.csv\", parse_dates=[\"date\"], index_col=\"date\")\n\nsp500.head()","78c31d3c":"sp500[\"shifted_1\"] = sp500[\"SP500\"].shift(periods=1)  # the default\nsp500[\"shifted_2\"] = sp500[\"SP500\"].shift(periods=2)\n\nsp500.head(6)","2a5be165":"sp500.drop([\"shifted_1\", \"shifted_2\"], axis=1, inplace=True)\n\nsp500[\"lagged_1\"] = sp500[\"SP500\"].shift(periods=-1)\nsp500[\"lagged_2\"] = sp500[\"SP500\"].shift(periods=-2)\n\nsp500.tail(6)","d36e6201":"sp500.drop(\"lagged_2\", axis=1, inplace=True)\n\nsp500[\"diff_lag\"] = sp500[\"lagged_1\"] - sp500[\"SP500\"]\nsp500.head()","2c78fd2e":"sp500[\"diff_lag\"].plot(figsize=(16, 4));","872a8579":"sp500.drop([\"lagged_1\", \"diff_lag\"], axis=1, inplace=True)\n\nsp500[\"shifted_diff_1\"] = sp500[\"SP500\"].diff(periods=1)\nsp500[\"shifted_diff_3\"] = sp500[\"SP500\"].diff(periods=3)\nsp500[\"shifted_lagg_1\"] = sp500[\"SP500\"].diff(periods=-1)\nsp500[\"shifted_lagg_3\"] = sp500[\"SP500\"].diff(periods=3)\n\nsp500.drop(\"SP500\", axis=1).plot(figsize=(16, 8), subplots=True);","c5d0270e":"sp500.drop(\n    [\"shifted_diff_1\", \"shifted_diff_3\", \"shifted_lagg_1\", \"shifted_lagg_3\"],\n    axis=1,\n    inplace=True,\n)","de23f675":"sp500[\"shifted\"] = sp500[\"SP500\"].shift(1)\nsp500[\"change\"] = sp500[\"SP500\"].div(sp500[\"shifted\"]).sub(1).mul(100)\n\nsp500.head()","bb157a59":"sp500[\"pct_change\"] = sp500[\"SP500\"].pct_change().mul(100)\n\nsp500.head()","024febe1":"sp500.drop([\"shifted\", \"change\", \"pct_change\"], axis=1, inplace=True)","d8e93321":"sp500.head()","ce2ff6db":"sp500.asfreq(\"D\").head(7)","471283b4":"# 5-hour frequency\nsp500.asfreq(\"5h\").head(7)  # This makes the dataset very large","aae4a9e5":"# 10 day frequency\nsp500.asfreq(\"10d\", method=\"ffill\").head(7)  # This makes the dataset smaller","32d6d30d":"# 10 month frequency\nsp500.asfreq(\"10M\", method=\"bfill\").head(7)","bd6a6aeb":"aapl_googl.resample(\"M\")","66917ae3":"aapl_googl.resample(\"M\").mean().tail()","1b313c87":"# Resample with business-month frequency\n# and return the first record of each group\naapl_googl.resample(\"BM\").first().tail()","5e2dffdd":"# Opposite of first()\naapl_googl.resample(\"Y\").last().tail()  # Year-end frequency","b6af26a1":"aapl_googl.resample(\"Y\").agg([\"mean\", \"median\", \"std\"]).head()","077ea5c9":"# Resample with business day freq and forward-fill\naapl_googl.resample(\"B\").ffill().tail()","9b5d4c99":"# Resample with 20-hour frequency and back-fill\naapl_googl.resample(\"20h\").bfill().sample(5)","c76da22b":"quarter_google = aapl_googl.resample(\"Q\")[\"GOOG\"].mean()\nyearly_google = aapl_googl.resample(\"Y\")[\"GOOG\"].mean()\n\nquarter_apple = aapl_googl.resample(\"Q\")[\"AAPL\"].mean()\nyearly_apple = aapl_googl.resample(\"Y\")[\"AAPL\"].mean()","e7c0daa1":"# Plot Apple's downsampled stocks\naapl_googl[\"AAPL\"].plot(figsize=(16, 5), label=\"Original\")\nquarter_apple.plot(label=\"Quarterly\")\nyearly_apple.plot(label=\"Yearly\")\nplt.legend(fontsize=\"x-large\");","bb1be66b":"# Plot Google's downsampled stocks\naapl_googl[\"GOOG\"].plot(figsize=(16, 5), label=\"Original\")\nquarter_google.plot(label=\"Quarterly\")\nyearly_google.plot(label=\"Yearly\")\nplt.legend(fontsize=\"x-large\");","03de7ce3":"aapl_googl.mean()","d551921e":"aapl_googl.dropna(inplace=True)\n\n# The first rows will contain ones because\n# they are being divided by themselvs\naapl_googl.div(aapl_googl.iloc[0]).head(10)","b2712df4":"# Normalize\nnormalized_aapl_goog = aapl_googl.div(aapl_googl.iloc[0])\n\nnormalized_aapl_goog.plot(figsize=(16, 5))\nplt.legend(fontsize=\"xx-large\");","1fd8a413":"# Normalize SP500 dataset\nnormalized_sp500 = sp500.div(sp500.iloc[0])\n\n# PLot\nfig, ax = plt.subplots(figsize=(16, 5))\n\nnormalized_aapl_goog.plot(ax=ax)\nnormalized_sp500[\"2011\":].plot(label=\"S&P500\", ax=ax)\n\nplt.legend(fontsize=\"xx-large\");","28ef01ad":"aapl_googl.rolling(window=5)","bd92f533":"aapl_googl[\"GOOG_5d_roll\"] = aapl_googl[\"GOOG\"].rolling(window=5).sum()\n\naapl_googl.head(10)","289a62f0":"aapl_googl[\"90D_roll_mean\"] = aapl_googl[\"GOOG\"].rolling(window=\"90D\").mean()\naapl_googl[\"360D_roll_mean\"] = aapl_googl[\"GOOG\"].rolling(window=\"360D\").mean()","208b4680":"fig, ax = plt.subplots(figsize=(16, 5))\n\naapl_googl[[\"90D_roll_mean\", \"360D_roll_mean\", \"GOOG\"]].plot(ax=ax)\n\nplt.legend(fontsize=\"xx-large\");","3166bbc7":"aapl_googl.drop(\n    [\"GOOG_5d_roll\", \"90D_roll_mean\", \"360D_roll_mean\"], axis=1, inplace=True\n)","fa71b46c":"aapl_googl[\"expanding_cumsum\"] = aapl_googl[\"GOOG\"].expanding(min_periods=1).sum()\n# The same operation with cumsum() func\naapl_googl[\"cumsum_function\"] = aapl_googl[\"GOOG\"].cumsum()\n\naapl_googl.head()","70b0a82e":"sp500[\"running_min\"] = sp500[\"SP500\"].expanding().min()  # same as cummin()\nsp500[\"running_max\"] = sp500[\"SP500\"].expanding().max()\n\nfig, ax = plt.subplots(figsize=(10, 8))\n\nsp500.plot(ax=ax)\nplt.legend(fontsize=\"xx-large\");","47908bd6":"[Back to top\ud83d\udd1d](#toc)\n\nIn time series lingo, making the frequency of a `DateTime` less granular is called downsampling. The examples are changing the frequency from hourly to daily, from daily to weekly, etc.\n\nWe saw how to downsample with `asfreq`. A more powerful alternative is `resample` which behaves like `pd.groupby`. Just like `groupby` groups the data based on categorical values, `resample` groups the data by date frequencies.\n\nLet's downsample the Apple\/Google stock prices by month-end frequency:","ad11f673":"[Back to top\ud83d\udd1d](#toc)\n\nConsider this small distribution:","c2867734":"We will also create a function that plots the original distribution before and after an imputation(s) is performed:","34705960":"## 6. Window functions <small id='6'><\/small>","6a695e59":"There is also a `reindex` function that operates similarly and supports additional missing value filling logic. We won't discuss it here as there are better options we will consider.","66bee721":"## 2. Missing data imputation or interpolation <small id='2'><\/small>","5ce1dd3c":"### 4.3 Upsampling with `resample` and interpolating <small id='4.3'><\/small>","33c8e3d0":"It is also possible to pass custom frequencies such as \"1h30min\", \"5D\", \"2W\", etc. Again, check out [this link](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/timeseries.html#timeseries-offset-aliases) for the full info.","92d13c67":"Just like `groupby` and `resample`, you can calculate multiple metrics with the `agg` function for each window.","a198c754":"We will use both forward and backward filling and assign them back to the DataFrame as separate columns:","ae4e5b7d":"A full timestamp has useful attributes such as these:","01c5efd7":"[Back to top\ud83d\udd1d](#toc)\n\nSlicing time series data can be very intuitive if the index is a `DateTimeIndex`. You can use something called partial slicing:","de7822e2":"[Back to top\ud83d\udd1d](#toc)\n\nResampling isn't going to give much if you don't plot its results.\n\nIn most cases, you will see new trends and patterns when you downsample. This is because downsampling reduces the granularity, thus eliminating noise:","0b8ab78f":"Neat, huh? The linear method considers the distance between any two non-missing points as linearly spaced and finds a linear line that connects them (like `np.linspace`). 'Nearest' method should be understandable from its name and the above output.","28a8e1c4":"## Setup","813d1f9b":"[Back to top\ud83d\udd1d](#toc)\n\nThe basic date data structure in Pandas is a timestamp:","a259f553":"### 6.2 Expanding window functions <small id='6.2'><\/small>","eba4e173":"You can even go down to hours, minutes, or seconds levels if the DateTime is granular enough.\n\nNote that pandas slices dates in closed intervals. For example, using \"2010\": \"2013\" returns rows for all 4 years\u200a-\u200ait does not exclude the end of the period like integer slicing.\n\nThis date slicing logic applies to other operations like choosing a specific column after the slice:","40ef99ea":"### 2.1 Mean, median and mode imputation <small id='2.1'><\/small>","79f017ee":"As you can see, Apple and Google have much higher growth than other top 500 companies in the US.","086d8b35":"## Introduction to this project on Time Series Forecasting","cadec674":"### 4.1 Changing the frequency with `asfreq` <small id='4.1'><\/small>","cc189024":"Even though very basic, forward and backward filling actually works pretty well on climate and stocks data since the differences between nearby data points are small.","2a9991a8":"Shifting backward enables us to see the difference between the current data point and the one that comes one or more periods later.\n\nA common operation after shifting or lagging is finding the difference and plotting it:","e0a655bb":"Here is how we create rolling windows in pandas:","f73d9c71":"[Back to top\ud83d\udd1d](#toc)\n\nRolling window functions will have the same length. As they slide through the data, their coverage (number of rows don't change). Here is an example window of 5 periods sliding through the data:","64c4a837":"### 2.4 Model based imputation with KNN <small id='2.4'><\/small>","1c03dda7":"![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*AsTSxTsolMRce59M3dw-KA.png)","08f35d49":"Google's stock prices are way higher than Apple's. Plotting the stocks together would probably squish Apple's to a flat line. In other words, the two stocks have different scales.\n\nTo fix this, statisticians use normalization. The most common variation is choosing the first recorded value and dividing the rest of the samples by that amount. This shows how each record changes compared to the first.\n\nHere is an example:","41316ba8":"[Back to top\ud83d\udd1d](#toc)\n\nLet's start with the basics. We will randomly select data points in Apple\/Google stock dataset and convert them to NaN:","b949f8c9":"## Table of Contents <small id='toc'><\/small>","ba698e70":"It should be fairly obvious how these methods work once you examine the above output. \n\nNow, let's perform these methods on the Airquality in India dataset:","5bf0d399":"## 1. Basic date and time functions in Pandas","ad5ee25e":"### 1.4 Slicing <small id='1.4'><\/small>","9e9cfbc0":"Unlike `asfreq`, using resample only returns the data in the resampled state. To see each group, we need to use some type of function, similar to how we use `groupby`.\n\nSince downsampling decreases the number of data points, we need an aggregation function like mean, median, or mode:","c2240fb3":"[Back to top\ud83d\udd1d](#toc)\n\nAnother type of window function deals with expanding windows. Each new window will contain all the records up to the current date:\n\n![](https:\/\/cdn-images-1.medium.com\/max\/800\/1*lqNZULHaEUHJDcaevMz1cA.png)","185fad86":"## 3. Basic time series calculations <small id='3'><\/small>","bc319425":"You can make even more granular timestamps using the right format or, better yet, using the `datetime` module:","82c8e75e":"We will start trying out techniques with `SimpleImputer` from Sklearn:","826bbd96":"### 1.3 Sequence of dates (timestamps) <small id='1.3'><\/small>","e24f8ad9":"### You might also be interested...","9e24c11a":"## 5. Comparing the growth of multiple time series <small id='5'><\/small>","9c724342":"### 3.2 Percentage changes <small id='3.2'><\/small>","ba11f6ff":"There are also functions that return the first or last record of a group:","1dcfaef0":"It is also possible to use multiple aggregating functions using `agg`:","a05c87fe":"[Back to top\ud83d\udd1d](#toc)\n\nOften, you may want to increase or decrease the granularity of time series to generate new insights. These operations are called resampling or changing the frequency of time series, and we will discuss the Pandas functions related to them in this section.","28cac018":"To calculate day-to-day percentage change, shift one period forward and divide the original distribution by the shifted one and subtract 1. The resulting values are given as proportions of what they were the day before.\n\nSince it is a common operation, Pandas implements it with the `pct_change` function:","47a543fd":"The SP500 stocks data does not have a fixed date frequency, i.e., the period difference between each date is not the same:","d7324f6e":"[Back to top\ud83d\udd1d](#toc)\n\nThe opposite of downsampling is making the `DateTime` more granular. This is called upsampling and includes operations like changing the frequency from daily to hourly, hourly to seconds, etc.\n\nWhen upsampling, you introduce new dates leading to more missing values. This means you need to use some type of imputation:","ce0796e0":"[Back to top\ud83d\udd1d](#toc)\n\nAnother common metric that can be derived from time-series data is day-to-day percentage change:","b4f8316e":"We just made the frequency of the date in SP500 more granular. As a result, new dates were added, leading to more missing values. You can now interpolate them using any of the techniques we discussed earlier.\n\nYou can see the list of built-in frequency aliases from [here](https:\/\/medium.com\/r\/?url=https%3A%2F%2Fpandas.pydata.org%2Fpandas-docs%2Fstable%2Fuser_guide%2Ftimeseries.html%23offset-aliases). A more interesting scenario would be using custom frequencies:","b5175aa8":"Just like `resample`, it is in a read-only state - to use each window, we should chain some type of function. For example, let's create a cumulative sum for every past 5 periods:","b1b18632":"After conversion, set the DateTime column as index (a strict requirement for best time series analysis):","6703f1fc":"Let's fix this by giving it a calendar day frequency (daily):","445a287a":"#### [**1. Basic date and time functions**](#1)\n  * [1.1 Importing time series data](#1.1)\n  * [1.2 Pandas TimeStamp](#1.2)\n  * [1.3 Sequence of dates (timestamps)](#1.3)\n  * [1.4 Slicing](#1.4)\n\n#### [**2. Missing data imputation\/interpolation in time series**](#2)\n  * [2.1 Mean, median and mode imputation](#2.1)\n  * [2.2 Forward and backward filling](#2.2)\n  * [2.3 Using pd.interpolate](#2.3)\n  * [2.4 Model based imputation with KNN](#2.4)\n\n#### [**3. Basic time series calculations and metrics**](#3)\n  * [3.1 Shifts and lags](#3.1)\n  * [3.2 Percentage changes](#3.2)\n\n#### [**4. Resampling - upsample and downsample**](#4)\n  * [4.1 Changing the frequency with `asfreq`](#4.1)\n  * [4.2 Downsampling with resample and aggregating](#4.2)\n  * [4.3 Upsampling with resample and interpolating](#4.3)\n  * [4.4 Plotting the resampled data](#4.4)\n\n#### [**5. Comparing the growth of multiple time series**](#5)\n\n#### [**6. Window functions**](#6)\n  * [6.1 Rolling window functions](#6.1)\n  * [6.2 Expanding window functions](#6.2)\n\n#### [**7. Summary**](#7)","950191b0":"- [Matplotlib vs. Plotly: Let\u2019s Decide Once and for All](https:\/\/towardsdatascience.com\/matplotlib-vs-plotly-lets-decide-once-and-for-all-ad25a5e43322?source=your_stories_page-------------------------------------)\n- [6 Things I Do to Consistently Improve My Machine Learning Models](https:\/\/medium.com\/me\/stories\/public#:~:text=6%20Things%20I%20Do%20to%20Consistently%20Improve%20My%20Machine%20Learning%20Models)\n- [5 Super Productive Things To Do While Training Machine Learning Models](https:\/\/towardsdatascience.com\/5-short-but-super-productive-things-to-do-during-model-training-b02e2d7f0d06?source=your_stories_page-------------------------------------)","ab740f37":"[Back to top\ud83d\udd1d](#toc)\n\nPandas provides a whole suite of other statistical imputation techniques in `pd.interpolate` function. Its `method` parameter accepts the name of the technique as a string.\n\nThe most popular ones are 'linear' and 'nearest,' but you can see the full list from the function's documentation. Here, we will only discuss those two.\n\nConsider this small distribution:","46244b2c":"[Back to top\ud83d\udd1d](#toc)\n\nA common operation in time series is to move all data points one or more periods backward or forward to compare past and future values. You can do these operations using `shift` function of pandas. Let's see how to move the data points 1 and 2 periods into the future:","87ba80ae":"### 2.2 Forward and backward filling <small id='2.2'><\/small>","b6473cde":"Shifting forward enables you to compare the current data point to those recorded one or more periods before.\n\nYou can also shift backward. This operation is also called \"lagging\":","7e6c3886":"`parse_dates` converts date-like strings to DateTime objects and `index_col` sets the passed column as the index. This operation is the basis for all time-series manipulation you will do with Pandas.\n\nWhen you don't know which column contains dates upon importing, you can perform the date conversion using `pd.to_datetime` function afterward:","094fcb48":"### 3.1 Shifts and lags <small id='3.1'><\/small>","60b8fa1f":"## 4. Resampling <small id='4'><\/small>","11ae7bbc":"It is hard to say which lines most closely resembles the black line, but I will go with the blue.","0288622a":"Expanding windows are useful for calculating 'running' metrics-for example, running sum, mean, min and max, running rate of return, etc.\n\nBelow, you will see how to calculate the cumulative sum. The cumulative sum is actually an expanding window function with a window size of 1:","f76f5a51":"Another way to create date ranges is passing the start date and telling how many periods you want, and specifying the frequency:","d69cfd05":"### 4.2 Downsampling with `resample` and aggregating <small id='4.2'><\/small>","d3ffca35":"[Back to top\ud83d\udd1d](#toc)\n\nWhen using the `pd.read_csv` function to import time series, there are 2 arguments you should always use - `parse_dates` and `index_col`:","b2e86517":"[Back to top\ud83d\udd1d](#toc)\n\nIt is common to compare two or more numeric values that change over time. For example, we might want to see the growth rate of Google and Apple's stock prices. But here is the problem:","06ca2970":"### 4.4 Plotting the resampled data <small id='4.4'><\/small>","e6ebd6ec":"Both Apple's and Google's achieved over 300% growth from 2011 to 2017. This plot may be even more interesting if we compare their growth to other 500 Fortune Companies:","85cd5e5e":"Recently, the Optiver Realized Volatility Prediction competition has been launched on Kaggle. As the name suggests, it is a time series forecasting challenge.\n\nI wanted to participate, but it turns out my knowledge in time series couldn't even begin to suffice to participate in a competition of such a magnitude. So, I accepted this as the 'kick in the pants' I needed to start paying serious attention to this large sphere of ML.\n\nAs the first step, I wanted to learn and teach every single Pandas function you can use to manipulate time-series data. These functions are the basic requirements for dealing with any time series data you encounter in the wild.\n\nI have got rather cool and interesting articles\/notebooks planned on this topic, and today, you will be reading the first taste of what is to come. Enjoy!","5a19972d":"Obviously, the first 4 rows will be NaNs. Any other row will contain the sum of the previous 4 rows and the current one.\n\nPay attention to the window argument. If you pass an integer, the window size will be determined by that number of rows. If you pass a frequency alias such as months, years, 5 hours, or 7 weeks, the window size will be whatever number of rows that includes the single unit of the passed frequency. In other words, a 5-period window might have a different size than a 5-day frequency window.\n\nAs an example, let's plot 90 and 360-day moving averages for Google stock prices and plot them:","2bc1cb9b":"Now, let's plot the running min and max of S&P500 stocks:","6a1fdb8c":"It is in the format \"%Y-%m-%d\" (full list of datetime format strings can be found [here](https:\/\/docs.python.org\/3\/library\/datetime.html#strftime-and-strptime-behavior)). Pass this to `pd.to_datetime`:","b41f8731":"[Back to top\ud83d\udd1d](#toc)\n\nThe last method we will see is the K-Nearest-Neighbors algorithm. I won't detail how the algorithm works but only show how you can use it with Sklearn. If you want the details, I have a separate article [here](https:\/\/medium.com\/r\/?url=https%3A%2F%2Ftowardsdatascience.com%2Fgoing-beyond-the-simpleimputer-for-missing-data-imputation-dd8ba168d505%3Fsource%3Dyour_stories_page-------------------------------------).\n\nThe most important parameter of KNN is `k` - the number of neighbors. We will apply the technique to Apple\/Google data with several values of `k` and find the best one the same way as we did in the previous sections:","62cd8b12":"### 1.1 Importing time series data <small id='1.1'><\/small>","645a749e":"[Back to top\ud83d\udd1d](#toc)\n\nA `DateTime` column\/index in pandas is represented as a series of `TimeStamp` objects.\n\n`pd.date_range` returns a special `DateTimeIndex` object that is a collection of `TimeStamps` with a custom frequency over a given range:","ba66ab76":"Now, inspect the datetime format string:","d5d5c897":"Since we set the frequency to years, `date_range` with 5 periods returns 5 years\/timestamp objects. The [list of frequency aliases](https:\/\/medium.com\/r\/?url=https%3A%2F%2Fpandas.pydata.org%2Fpandas-docs%2Fstable%2Fuser_guide%2Ftimeseries.html%23timeseries-offset-aliases) that can be passed to `freq` is large, so I will only mention the most important ones here:","79160abd":"[Back to top\ud83d\udd1d](#toc)\n\nMissing data is ubiquitous no matter the type of the dataset. This section is all about imputing it in the context of time series. \n\n> You may also hear it called **interpolation** of missing data in time series lingo.\n\nBesides the basic mean, median and mode imputation, some of the most common techniques include:\n\n1. Forward filling\n2. Backward filling\n3. Intermediate imputations with `pd.interpolate`\n\nWe will also discuss model-based imputation such as KNN imputing. Moreover, we will explore visual methods of comparing the efficiency of the techniques and choose the one that best fits the underlying distribution.","d826b45f":"Since this operation is so common, Pandas has the `diff` function that computes the differences based on the period:","cc87c2b1":"[Back to top\ud83d\udd1d](#toc)\n\nThere is another type of function that helps you analyze time-series data in novel ways. These are called window functions, and they help you aggregate over a custom number of rows called 'windows.'\n\nFor example, I can create a 30-day window over my [Medium subscribers](https:\/\/medium.com\/@ibexorigin) data to see the total number of subscribers for the past 30 days on any given day. Or a restaurant owner might create a weekly window to see average sales of the past week. Examples are endless as you can create a window of any size over your data.\n\nLet's explore these in more detail.","de1d1901":"After specifying the date range (from October 10, 2010, to the same date in 2020), we are telling pandas to generate `TimeStamps` on a monthly-basis with `freq='M'`:","8ffb7ad0":"Passing a format string to `pd.to_datetime` significantly speeds up the conversion for large datasets. Set `errors` to \"coerce\" to mark invalid dates as `NaT` (not a date, i.e. - missing).","9a2c33d0":"Let's plot the original GOOG distribution against the 3 imputed features we just created:","735d8701":"### 6.1 Rolling window functions <small id='6.1'><\/small>","8dcc3fb3":"The above output shows that for the first 3 dates, Apple stocks didn't change. Then, it increased by 1% of what it was on the first date ('2010\u201312\u201316'). Google's prices are more volatile, fluctuating between 1 and 2% increases during the first 10 dates.\n\nNow, let's plot them to compare growth:","3d8d3748":"Once again, we apply the methods and assign their results back:","56ba706b":"[Back to top\ud83d\udd1d](#toc)\n\nI think congratulations are in order!\n\nNow, you know every single Pandas function you can use to manipulate time-series data. It has been an excruciatingly long post, but it was definitely worth it since now, you can tackle any time series data thrown at you.\n\nThis post was mainly focused on data manipulation. The next posts in the series will be about more in-depth time series analyses, similar posts on every single plot you can create on time series, and dedicated articles on forecasting. Stay tuned!","6948e90f":"# Every Function You Can (Should) Use in Pandas to Manipulate Time Series\n## From basic time series metrics to window functions\n![](https:\/\/cdn-images-1.medium.com\/max\/1200\/1*goDYbZULUkLiheRkrJpmJQ.jpeg)\n<figcaption style=\"text-align: center;\">\n    <strong>\n        Photo by \n        <a href='https:\/\/www.pexels.com\/@bentonphotocinema?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Jordan Benton<\/a>\n        on \n        <a href='https:\/\/www.pexels.com\/photo\/shallow-focus-of-clear-hourglass-1095601\/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>Pexels<\/a>\n    <\/strong>\n<\/figcaption>","8fe48c56":"Plotting the upsampled distribution is only going to introduce more noise, so we won't do it here.","a07f30fe":"### 2.3 Using `pd.interpolate` <small id='2.3'><\/small>","e4f5c1c2":"`expanding` function has a `min_periods` parameter that determines the initial window size.","4a6d5c18":"### 1.2 Pandas TimeStamp <small id='1.2'><\/small>","63373d96":"[Back to top\ud83d\udd1d](#toc)\n\nPandas offers basic functions to calculate the most common time series calculations. These are called shifts, lags, and something called a percentage change.","0a66ea89":"## Summary <small id='7'><\/small>"}}