{"cell_type":{"64230db2":"code","038d4441":"code","8ba1c05f":"code","be6feff4":"code","f7eb9da2":"code","c5bd671f":"code","a18e58e0":"code","cf12bc24":"code","f4b47921":"code","2f3b5b21":"code","824f7030":"code","608115e4":"code","2d6d421f":"code","1d4b7339":"code","b6da6230":"code","5b6fb617":"code","f4aa3f72":"code","a30d3b04":"code","99734d8d":"markdown","57563294":"markdown","cdd242f3":"markdown","61c1b3c0":"markdown","d07507d7":"markdown","e5a154b6":"markdown","2720452f":"markdown"},"source":{"64230db2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.offline as py\nimport plotly.express as px\n\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom graphviz import Source\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=Warning)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","038d4441":"df = pd.read_csv('..\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv', encoding='utf8')\ndf.head()","8ba1c05f":"#Code by Parul Pandey  https:\/\/www.kaggle.com\/parulpandey\/a-guide-to-handling-missing-values-in-python\n\n\nfrom sklearn.impute import SimpleImputer\ndf_most_frequent = df.copy()\n#setting strategy to 'mean' to impute by the mean\nmean_imputer = SimpleImputer(strategy='most_frequent')# strategy can also be mean or median \ndf_most_frequent.iloc[:,:] = mean_imputer.fit_transform(df_most_frequent)","be6feff4":"df_most_frequent.isnull().sum()","f7eb9da2":"from sklearn.preprocessing import LabelEncoder\n\n#fill in mean for floats\nfor c in df_most_frequent.columns:\n    if df_most_frequent[c].dtype=='float16' or  df_most_frequent[c].dtype=='float32' or  df_most_frequent[c].dtype=='float64':\n        df_most_frequent[c].fillna(df[c].mean())\n\n#fill in -999 for categoricals\ndf_most_frequent = df_most_frequent.fillna(-999)\n# Label Encoding\nfor f in df_most_frequent.columns:\n    if df_most_frequent[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(df_most_frequent[f].values))\n        df_most_frequent[f] = lbl.transform(list(df_most_frequent[f].values))\n        \nprint('Labelling done.')","c5bd671f":"df_most_frequent.head()","a18e58e0":"import seaborn as sbn\n\ncorrelation=df_most_frequent.corr()\nplt.figure(figsize=(15,15))\nsbn.heatmap(correlation,annot=True,cmap=plt.cm.summer);","cf12bc24":"from sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom graphviz import Source\n\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.model_selection import train_test_split\n\n\nfrom scipy.stats import skew\nplt.style.use('ggplot')","f4b47921":"x = df_most_frequent.drop(['locale', 'pct_black\/hispanic'], axis=1)\nx.fillna(999999, inplace=True)\ny = df_most_frequent['pct_black\/hispanic']","2f3b5b21":"dt = DecisionTreeClassifier(max_depth=3)","824f7030":"dt.fit(x, y)","608115e4":"dt_feat = pd.DataFrame(dt.feature_importances_, index=x.columns, columns=['feat_importance'])\ndt_feat.sort_values('feat_importance').tail(8).plot.barh(figsize=(14, 6), color='green')\nplt.show()","2d6d421f":"from IPython.display import SVG\nos.environ[\"PATH\"] += os.pathsep + 'C:\/Program Files (x86)\/Graphviz2.38\/bin\/'\n\ngraph = Source(export_graphviz(dt, out_file=None, feature_names=x.columns, filled = True))\ndisplay(SVG(graph.pipe(format='svg')))","1d4b7339":"x = df_most_frequent.drop(['pp_total_raw', 'pct_free\/reduced'], axis=1)\nx.fillna(999999, inplace=True)\ny = df_most_frequent['pct_free\/reduced']","b6da6230":"dt = DecisionTreeClassifier(max_depth=3)","5b6fb617":"dt.fit(x, y)","f4aa3f72":"dt_feat = pd.DataFrame(dt.feature_importances_, index=x.columns, columns=['feat_importance'])\ndt_feat.sort_values('feat_importance').tail(8).plot.barh(figsize=(14, 6), color='red')\nplt.show()","a30d3b04":"from IPython.display import SVG\nos.environ[\"PATH\"] += os.pathsep + 'C:\/Program Files (x86)\/Graphviz2.38\/bin\/'\n\ngraph = Source(export_graphviz(dt, out_file=None, feature_names=x.columns, filled = True))\ndisplay(SVG(graph.pipe(format='svg')))","99734d8d":"![](https:\/\/datascience.foundation\/img\/pdf_images\/understanding_decision_trees_with_python_decision_tree.png)datascience.foundation","57563294":"<h1><span class=\"label label-default\" style=\"background-color:#CD5C5C;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:white; padding:10px\">A Second Decision Tree<\/span><\/h1><br>","cdd242f3":"<center style=\"font-family:verdana;\"><h1 style=\"font-size:200%; padding: 10px; background: #CD5C5C;\"><b style=\"color:white;\">Decision Tree<\/b><\/h1><\/center>\n\n\n\"Explaining Decision Trees for Machine Learning\" By z_ai\n\n\"In the Machine Learning world, Decision Trees are a kind of non parametric models, that can be used for both classification and regression.\"\n\n\"This means that Decision trees are flexible models that don\u2019t increase their number of parameters as we add more features (if we build them correctly), and they can either output a categorical prediction or a numerical prediction.\" \n\n\"They are constructed using two kinds of elements: nodes and branches. At each node, one of the features of our data is evaluated in order to split the observations in the training process or to make an specific data point follow a certain path when making a prediction.\"\n\nhttps:\/\/towardsdatascience.com\/decision-trees-explained-3ec41632ceb6","61c1b3c0":"#Heatmap with Brazil's colors","d07507d7":"#Label Encoding","e5a154b6":"#Handling Missing Values","2720452f":"In this figure we can observe three kinds of nodes:\n\n\"The Root Node: Is the node that starts the graph. In a normal decision tree it evaluates the variable that best splits the data.\"\n\n\"Intermediate nodes: These are nodes where variables are evaluated but which are not the final nodes where predictions are made.\"\n\n\"Leaf nodes: These are the final nodes of the tree, where the predictions of a category or a numerical value are made.\"\n\n<h1><span class=\"label label-default\" style=\"background-color:#CD5C5C;border-radius:100px 100px; font-weight: bold; font-family:Garamond; font-size:20px; color:white; padding:10px\">Making Predictions<\/span><\/h1><br>\n\n\"All we have to do is start at the root node, look at the value of the feature that it evaluates, and depending on that value go to the left or right children node.\"\n\n\"This process is repeated until we reach a leaf node. When this happens, depending on whether we are facing a classification or a regression problem two things can happen:\n\n\"If we are facing a classification problem, the predicted category would be the mode of the categories on that leaf node\"\n\n\"For a regression tree, (In our digital learning case above) the prediction we make at the end is the mean of the values for the target variable at such leaf node. If a leaf node had 4 samples with values 20, 18, 22, and 24, then the predicted value at that node would be 21, the mean of the 4 training examples that end there.\n\nhttps:\/\/towardsdatascience.com\/decision-trees-explained-3ec41632ceb6"}}