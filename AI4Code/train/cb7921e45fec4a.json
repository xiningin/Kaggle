{"cell_type":{"d4ecc396":"code","08d51bcd":"code","a60008ad":"code","7407f760":"code","1ef2fc3c":"code","b02c2b30":"code","d741e7e5":"code","3f432f99":"code","fb143d0c":"code","2d6a4f84":"code","dc3fec41":"code","ffcaac25":"code","88963658":"code","e14d4c0c":"code","f558fae8":"code","72e5c559":"code","00a87f2a":"code","844c53c2":"code","8a6fbf0f":"code","1c1ea9ea":"code","7723b149":"code","0b4b0d3c":"code","5f36cbf3":"code","38b644b0":"code","34a36526":"code","5fba6f8d":"code","d3eedce4":"code","c91e7c34":"code","c51a8eda":"code","fecbe06d":"code","bc5e66ff":"code","eae687b8":"code","49edd933":"code","3a57cbc6":"markdown","31f54a49":"markdown","05f67281":"markdown","10317466":"markdown","9a3f6111":"markdown","667321e6":"markdown","1dffd172":"markdown","4b996dad":"markdown","460decc2":"markdown","a1d7dbf2":"markdown","00fe2d68":"markdown","8093f24d":"markdown","a90e80f7":"markdown","b36e3097":"markdown","39ba8062":"markdown","6839470a":"markdown","63c0cd57":"markdown","96873046":"markdown","518abfeb":"markdown","36f11b84":"markdown"},"source":{"d4ecc396":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import Layer\nimport matplotlib.pyplot as plt\nimport unicodedata\nimport re\nimport time","08d51bcd":"# Converts the unicode file to ascii\ndef unicode_to_ascii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s)\n                 if unicodedata.category(c) != 'Mn')\n\n\ndef preprocess_sentence(w):\n    w = unicode_to_ascii(w.lower().strip())\n\n    w = re.sub(r\"([?.!,\u00bf])\", r\" \\1 \", w)\n    w = re.sub(r'[\" \"]+', \" \", w)\n    w = re.sub(r\"[^a-zA-Z?.!,\u00bf]+\", \" \", w)\n    w = w.strip()\n    return w","a60008ad":"NUM_EXAMPLES = 100000\ndata_examples = []\nwith open('\/kaggle\/input\/engpor-sentence-pairs\/eng-por.txt', 'r', encoding='utf8') as f:\n    for line in f.readlines():\n        if len(data_examples) < NUM_EXAMPLES:\n            data_examples.append(line)\n        else:\n            break\n\ndf = pd.DataFrame(data_examples, columns=['all'])\ndf = df['all'].str.split('\\t', expand=True)\n\ndf.columns = columns=['english', 'portuguese', 'rest']\n\ndf = df[['english', 'portuguese']]\n\ndf['portuguese'] = df.apply(lambda row: \"<start> \" + preprocess_sentence(row['portuguese']) + \" <end>\", axis=1)\ndf['english'] = df.apply(lambda row: preprocess_sentence(row['english']), axis=1)\n\ntokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n\ntokenizer.fit_on_texts(df['portuguese'].to_list())\n\ndf['portuguese_tokens'] = df.apply(lambda row: \n                        tokenizer.texts_to_sequences([row['portuguese']])[0], axis=1)\n\n# Print 5 examples from each language\n\nidx = np.random.choice(df.shape[0],5)\n\nfor i in idx:\n    print('English')\n    print(df['english'][i])\n    \n    print('\\nportuguese')\n    print(df['portuguese'][i])\n    print(df['portuguese_tokens'][i])\n    print('\\n----\\n')\n\nportuguese_tokens = tf.keras.preprocessing.sequence.pad_sequences(df['portuguese_tokens'].to_list(),\n                                                 padding='post',\n                                                 value=0)\n\nportuguese_tokens.shape","7407f760":"# Load embedding module from Tensorflow Hub\n\nembedding_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/google\/tf2-preview\/nnlm-en-dim128\/1\", \n                                 output_shape=[128], input_shape=[], dtype=tf.string)\n\n# Test the layer\n\nembedding_layer(tf.constant([\"these\", \"aren't\", \"the\", \"droids\", \"you're\", \"looking\", \"for\"])).shape","1ef2fc3c":"maxlen = 13\n\ndef split_english(dataset):\n    dataset = dataset.map(lambda x, y: (tf.strings.split(x, sep=' '), y))\n    return dataset\n\ndef filter_func(x,y):\n    return (tf.math.less_equal(len(x),maxlen)) \n\ndef map_maxlen(x, y):\n    paddings = [[0, maxlen - tf.shape(x)[0]], [0, 0]]\n    x = tf.pad(x, paddings, 'CONSTANT', constant_values=0)\n    return (x, y)\n\ndef embed_english(dataset):\n    dataset = dataset.map(lambda x, y: (embedding_layer(x), y))\n    return dataset","b02c2b30":"english_strings = df['english'].to_numpy()\nenglish_string_train, english_string_valid, portuguese_token_train, portuguese_token_valid = train_test_split(english_strings, portuguese_tokens, train_size=0.8)\n\ndataset_train = (embed_english(\n                        split_english(\n                            tf.data.Dataset.from_tensor_slices((english_string_train, portuguese_token_train)))\n                                                                .filter(filter_func))\n                                                                .map(map_maxlen)\n                                                                .batch(16))\ndataset_valid = (embed_english(\n                        split_english(\n                            tf.data.Dataset.from_tensor_slices((english_string_valid, portuguese_token_valid)))\n                                                                .filter(filter_func))\n                                                                .map(map_maxlen)\n                                                                .batch(16))\n\ndataset_train.element_spec","d741e7e5":"# shape of the English data example from the training Dataset\nlist(dataset_train.take(1).as_numpy_iterator())[0][0].shape","3f432f99":"# shape of the portuguese data example Tensor from the validation Dataset\nlist(dataset_valid.take(1).as_numpy_iterator())[0][1].shape","fb143d0c":"class EndTokenLayer(Layer):\n\n    def __init__(self, embedding_dim=128, **kwargs):\n        super(EndTokenLayer, self).__init__(**kwargs)\n        self.end_token_embedding = tf.Variable(initial_value=tf.random.uniform(shape=(embedding_dim,)), trainable=True)\n\n    def call(self, inputs):\n        end_token = tf.tile(tf.reshape(self.end_token_embedding, shape=(1, 1, self.end_token_embedding.shape[0])), [tf.shape(inputs)[0],1,1])\n        return tf.keras.layers.concatenate([inputs, end_token], axis=1)","2d6a4f84":"end_token_layer = EndTokenLayer()","dc3fec41":"inputs = tf.keras.Input(shape=(maxlen, 128))\nx = end_token_layer(inputs)\nx = tf.keras.layers.Masking(mask_value=0)(x)\nwhole_seq_output, final_memory_state, final_carry_state = tf.keras.layers.LSTM(512, return_sequences=True, return_state=True)(x)\noutputs = (final_memory_state, final_carry_state)","ffcaac25":"encoder_model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"encoder_model\")\nencoder_model.summary()","88963658":"inputs_eng = list(dataset_train.take(1).as_numpy_iterator())[0][0]","e14d4c0c":"memory_state, carry_state = encoder_model(inputs_eng)","f558fae8":"class Attention(tf.keras.layers.Layer):\n    def __init__(self, units):\n        super(Attention, self).__init__()\n        self.W1 = tf.keras.layers.Dense(units)\n        self.W2 = tf.keras.layers.Dense(units)\n        self.V = tf.keras.layers.Dense(1)\n\n    def call(self, hidden_states, cell_states):\n        hidden_states_with_time = tf.expand_dims(hidden_states, 1)\n\n        score = self.V(tf.nn.tanh(\n            self.W1(hidden_states_with_time) + self.W2(cell_states)))\n\n        attention_weights = tf.nn.softmax(score, axis=1)\n\n        context_vector = attention_weights * cell_states\n        context_vector = tf.reduce_sum(context_vector, axis=1)\n\n        return context_vector, attention_weights","72e5c559":"import json\nword_index = json.loads(tokenizer.get_config()['word_index'])\n\nmax_index_value = max(word_index.values())","00a87f2a":"class decoder_RNN(tf.keras.Model):\n    def __init__(self, **kwargs):\n        super(decoder_RNN, self).__init__()\n        self.embed = tf.keras.layers.Embedding(input_dim=max_index_value+1, output_dim=128, mask_zero=True)\n        self.lstm_1 = tf.keras.layers.LSTM(1024, return_sequences=True, return_state=True)\n        self.dense_1 = tf.keras.layers.Dense(max_index_value+1)\n        self.attention = Attention(1024)\n\n    def call(self, inputs, training=False, hidden_state=None, cell_state=None):\n        context_vector, attention_weights = self.attention(hidden_state, cell_state)\n        x = self.embed(inputs)\n        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n\n        x, hidden_state, cell_state = self.lstm_1(x)\n        x = self.dense_1(x)\n        return x","844c53c2":"decoder_model = decoder_RNN()","8a6fbf0f":"decoder_model(inputs = tf.random.uniform((16, 1)), hidden_state = memory_state, cell_state = carry_state)\ndecoder_model.summary()","1c1ea9ea":"def portuguese_input_output(data):\n    return (tf.cast(data[:,0:-1], tf.float32), tf.cast(data[:,1:], tf.float32))","7723b149":"optimizer_obj = tf.keras.optimizers.Adam(learning_rate=0.001)\nloss_obj = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)","0b4b0d3c":"@tf.function\ndef grad(encoder_model, decoder_model, english_input, portuguese_input, portuguese_output, loss):\n    with tf.GradientTape() as tape:\n        loss_value = 0\n        hidden_state, cell_state = encoder_model(english_input)\n        dec_input = tf.expand_dims([word_index['<start>']] * 16, 1)\n\n        # Teacher forcing - feeding the target as the next input\n        for t in range(1, portuguese_output.shape[1]):\n            predictions = decoder_model(dec_input, hidden_state = hidden_state, cell_state = cell_state)\n\n            loss_value += loss(portuguese_output[:, t], predictions)\n\n            # using teacher forcing\n            dec_input = tf.expand_dims(portuguese_output[:, t], 1)\n\n        grads = tape.gradient(loss_value, encoder_model.trainable_variables + decoder_model.trainable_variables)\n    return loss_value, grads","5f36cbf3":"def train_model(encoder_model, decoder_model, num_epochs, train_dataset, valid_dataset, optimizer, loss, grad_fn):\n    inputs = (14,)\n    train_loss_results = []\n    train_loss_results_valid = []\n    for epoch in range(num_epochs):\n        start = time.time()\n\n        epoch_loss_avg = tf.keras.metrics.Mean()\n        \n        for x, y in train_dataset:\n            dec_inp, dec_out = portuguese_input_output(y)\n            loss_value, grads = grad_fn(encoder_model, decoder_model, x, dec_inp, dec_out, loss)\n            optimizer.apply_gradients(zip(grads, encoder_model.trainable_variables + decoder_model.trainable_variables))\n            epoch_loss_avg(loss_value)\n\n        train_loss_results.append(epoch_loss_avg.result())\n        \n        print(\"Epoch {:03d}: Loss: {:.3f}\".format(epoch,\n                                                  epoch_loss_avg.result()))\n        print(f'Time taken for 1 epoch {time.time()-start:.2f} sec\\n')\n    return train_loss_results","38b644b0":"num_epochs=15\ntrain_loss_results = train_model(encoder_model, decoder_model, num_epochs, dataset_train, dataset_valid, optimizer_obj, loss_obj, grad)","34a36526":"plt.plot(np.arange(num_epochs), train_loss_results)\nplt.ylabel('loss')\nplt.xlabel('epoch');","5fba6f8d":"english_test = ['that is not safe .',\n                 'this is my life .',\n                 'are you still at home ?',\n                 'it is very cold here .']","d3eedce4":"english_test_emb = embed_english(split_english(tf.data.Dataset.from_tensor_slices((english_test, portuguese_token_train[:len(english_test),:])))).map(map_maxlen).batch(1)","c91e7c34":"total_output=[]\n\nfor x, y in english_test_emb:\n    hidden_state, cell_state = encoder_model(x)\n    output_decoder = decoder_model(inputs = np.tile(word_index['<start>'], (1, 1)), hidden_state = hidden_state, cell_state = cell_state)\n    output=[]\n    output.append(output_decoder)\n    for i in range(14):\n        output_decoder = decoder_model(inputs = tf.math.argmax(output_decoder, axis=2), hidden_state = hidden_state, cell_state = cell_state)\n        if tf.math.argmax(output_decoder, axis=2).numpy()[0][0] == 2: # <end> character\n            break\n        output.append(output_decoder)\n    total_output.append(output)","c51a8eda":"total_output_trans = []\nfor j in range(len(english_test)):\n    output_trans = []\n    for i in range(len(total_output[j])):\n        output_trans.append(tf.math.argmax(total_output[j][i], axis=2).numpy()[0][0])\n    total_output_trans.append(output_trans)","fecbe06d":"output_trans = np.array([np.array(xi) for xi in total_output_trans], dtype=object)","bc5e66ff":"portuguese_trans_batch=[]\ninv_word_index = {v: k for k, v in word_index.items()}\nfor i in range(output_trans.shape[0]):\n    portuguese_trans_batch.append(' '.join(list(np.vectorize(inv_word_index.get)(output_trans[i]))))","eae687b8":"list(english_test)","49edd933":"portuguese_trans_batch","3a57cbc6":"For both the encoder and decoder RNNs, we need to define embedding layers to turn our indices of words into dense vectors of fixed size. For the decoder RNN, we trained our own embedding. For the encoder RNN, we used a pre-trained English word embedding from Tensorflow Hub. It is a token-based text embedding trained on the English Google News 200B corpus. It allows us to leverage a word representation trained on a very large corpus, following the principle of Transfer Learning (see [[6]](https:\/\/towardsdatascience.com\/transfer-learning-and-data-augmentation-applied-to-the-simpsons-image-dataset-e292716fbd43) for an extended definition and a Transfer Learning application to computer vision). We padded the English sentences before feeding them into the RNN.","31f54a49":"# 3. Enconder Recurrent Neural Network","05f67281":"# 1. Introduction","10317466":"[[1]](https:\/\/arxiv.org\/abs\/1609.08144) [Wu et al., 2016] Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun,M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., \u0141ukaszKaiser, Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., Wang,W., Young, C., Smith, J., Riesa, J., Rudnick, A., Vinyals, O., Corrado, G., Hughes, M., andDean, J. (2016). Google\u2019s neural machine translation system: Bridging the gap between humanand machine translation.\n\n[[2]](https:\/\/arxiv.org\/abs\/1412.2007) [Jean et al., 2015] Jean, S., Cho, K., Memisevic, R., and Bengio, Y. (2015). On using very largetarget vocabulary for neural machine translation.\n\n[[3]](https:\/\/towardsdatascience.com\/generating-text-with-recurrent-neural-networks-based-on-the-work-of-f-pessoa-1e804d88692d) https:\/\/towardsdatascience.com\/generating-text-with-recurrent-neural-networks-based-on-the-work-of-f-pessoa-1e804d88692d\n\n[[4]](https:\/\/tfhub.dev\/google\/tf2-preview\/nnlm-en-dim128) https:\/\/tfhub.dev\/google\/tf2-preview\/nnlm-en-dim128\n\n[[5]](https:\/\/www.kaggle.com\/luisroque\/engpor-sentence-pairs) https:\/\/www.kaggle.com\/luisroque\/engpor-sentence-pairs\n\n[[6]](https:\/\/towardsdatascience.com\/transfer-learning-and-data-augmentation-applied-to-the-simpsons-image-dataset-e292716fbd43) https:\/\/towardsdatascience.com\/transfer-learning-and-data-augmentation-applied-to-the-simpsons-image-dataset-e292716fbd43\n\n[[7]](https:\/\/arxiv.org\/pdf\/1409.0473.pdf) [Bahdanau et al., 2016] Bahdanau, D., Cho, K., and Bengio, Y. (2016). Neural machine translationby jointly learning to align and translate.\n\n[[8]](https:\/\/arxiv.org\/pdf\/1508.04025v5.pdf) [Luong et al., 2015] Luong, M.-T., Pham, H., and Manning, C. D. (2015). Effective approaches to attention-based neural machine translation.","9a3f6111":"The architecture of the NMT model is quite challenging to work with and requires a significant amount of customization, for instance, in its training procedure. We used the principle of Transfer Learning when embedding the English sequences using a pre-trained embedding in a very large corpus. On the other side, we developed our own embedding for the Portuguese language used as input for the decoder network. The encoder and decoder RNNs were kept as simple as possible, as the model is computationally expensive to train.\n\nWe generated translations from English text to Portuguese without providing anything more than sentence pairs in English and Portuguese to train our model. The model understood affirmation and negation, important syntax distinctions, as in building an interrogative type of clause, and was able to interpret grammatical rules such as subject\u2013auxiliary inversion, often used in English, that do not translate directly to Portuguese.\n\nThis approach can be extended by increasing the depth of the model with more recurrent layers and the number of units in each layer. Hyperparameters such as the batch size can be tuned to increase accuracy. A wider range of attention strategies can also be tested.","667321e6":"The decoder is trained to predict the next word $y_t$ given the context vector $c$ and all the previously predicted words ${y_1,...,y_{t-1}}$, such that:\n\n$$p(\\textbf{y})=\\prod^T_{t=1}p(y_t|{y_1,...,y_{t-1}},c)$$\n\nwhere $\\textbf{y}=(y_1,...,y_T)$. For this we use an RNN, which means that each conditional probability is modeled as\n\n$$p(y_t|{y_1,...,y_{t-1}},c)=g(y_{t-1},s_t,c),$$\n\nwhere $g$ is a nonlinear function and $s_t$ is the hidden state of the RNN.\n\nFor the decoder RNN, we defined an embedding layer with the vocabulary size set to the number of unique Portuguese tokens. An LSTM layer followed this embedding layer with 1024 units and a Dense layer with a number of units equal to the number of unique Portuguese tokens and no activation function. Notice that despite the considerably shallow network, as we used only one recurrent layer, we end up with more than 18M parameters to train.","1dffd172":"# 7. Results","4b996dad":"# 2. Preprocessing","460decc2":"# 6. Training","a1d7dbf2":"Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation [[1]](https:\/\/arxiv.org\/abs\/1609.08144). Its strength comes from the fact that it learns the mapping directly from input text to associated output text. It has been proven to be more effective than traditional phrase-based machine translation, which requires much more effort to design the model. On the other hand, NMT models are costly to train, especially on large-scale translation datasets. They are also significantly slower at inference time due to the large number of parameters used. Other limitations are its robustness when translating rare words and failing to translate all parts of the input sentence. To overcome these problems, there are already some solutions, such as using the attention mechanism to copy rare words [[2]](https:\/\/arxiv.org\/abs\/1412.2007).\n\nTypically, NMT models follow the common sequence-to-sequence learning architecture. It consists of an encoder and a decoder Recurrent Neural Networks (RNN) (for a simpler example on how to set up an RNN, see [[3]](https:\/\/towardsdatascience.com\/generating-text-with-recurrent-neural-networks-based-on-the-work-of-f-pessoa-1e804d88692d)). The encoder transforms the input sentence into a list of vectors, one vector per input. Given this list, the decoder produces one output at a time until the special end-of-sentence token is produced.\n\nOur task is to produce translations in Portuguese for input sentences in English, using a medium-size corpus of example pairs. We build our NMT model using a sequence to sequence architecture. For the encoder RNN, we use a pre-trained embedding, a token-based text embedding trained on English Google News 200B corpus [[4]](https:\/\/tfhub.dev\/google\/tf2-preview\/nnlm-en-dim128). On the other hand, we train our own embedding in the decoder RNN, with a vocabulary size set to the number of unique Portuguese words in the corpus. Due to the complex architecture of the model, we implement a custom training loop to train our model.\n\nThis article uses a dataset that consists of 170,305 sentence pairs in English and Portuguese [[5]](https:\/\/www.kaggle.com\/luisroque\/engpor-sentence-pairs). The data comes from Tatoeba.org, a large database of example sentences translated into many languages by volunteers.","00fe2d68":"We start by adding two special tokens to every sentence in Portuguese, a `<start>` and `<end>` tokens. They are used to signal the beginning and end of a sentence to the decode RNN. Next, we tokenize the Portuguese sentences and pad the end of the sentences with zeros.","8093f24d":"To train a model with a sequence to sequence architecture, we needed to define a custom training loop. First, we defined a function that split the Portuguese corpus into the input and output tensors fed to the decoder model. Secondly, we created the forward and backward passes of the complete model. We passed the English input into the encoder to get the hidden and cell states of the encoder LSTM. These hidden and cell states are then passed into the decoder, along with the Portuguese inputs. We defined the loss function, calculated between the decoder outputs and the Portuguese output previously split, and the computation of the gradients with respect to the encoder and decoder trainable variables. Finally, we ran the training loop for a defined number of epochs. It iterated through the training dataset, creating the decoder inputs and outputs from the Portuguese sequences. It then computed the gradients and updated the parameters of the model accordingly.\n\nThe model is quite slow to train, even using GPU. Recall that we did not even stack layers in any of the RNNs, which would reduce our loss but, at the same time, make our model even harder to train. We can see from the plots below that both the training and validation reduced steadily over time.","a90e80f7":"The attention mechanism that we will be using was proposed by [[7]](https:\/\/arxiv.org\/pdf\/1409.0473.pdf). The main difference of using an attention mechanism is that we increase the expressiveness of the model, especially the encoder component. It no longer needs to encode all the information in the source sentence into a fixed-length vector. The context vector $c_i$ is then computed as:\n\n$$c_i = \\sum^{T_x}_{j=1}\\alpha_{ij}h_j$$.\n\nThe weights $\\alpha_{ij}$ are calculated as \n\n$$\\alpha_{ij}=\\frac{\\exp(e_{ij})}{\\sum^{T}_{k=1}\\exp(e_{ik})},$$\n\nwhere $e_{ij}=a(s_{i-1},h_j)$ is the score of how well the inputs around position $j$ and the output at position $i$ match.","b36e3097":"## 2.1 Pre-Trained Embedding Layer","39ba8062":"# 5. Decoder Recurrent Neural Network","6839470a":"To test our model, we define a set of sentences in English. To translate the sentences, we first preprocessed and embedded the sentences the same way we did with the training and validation sets. Next, we passed the embedded sentences through the encoder RNN to get the hidden and cell states. Starting with the special `<start>` token, we used this token and the encoder network's final hidden and cell states to get the one-step prediction and updated hidden and cell states from the decoder. Afterward, we created a loop to get the next step prediction and updated hidden and cell states from the decoder, using the most recent hidden and cell states. The loop is terminated when the `<end>` token is emitted or when the sentence has reached a defined maximum length. Finally, we decoded the output token sequence into Portuguese text.\n\nThe translations that were obtained are reasonably good. An interesting and more complex example was the input \u2018are you still at home?\u2019. A question has very clear syntax rules and some of them are language-specific. The returned translation \u2018ainda esta em casa?\u2019 showed that the model was able to capture those specificities.","63c0cd57":"# 9. References","96873046":"The encoder network is an RNN whose job is to read a sequence of vectors $\\textbf{x}=(x_1,...,x_T)$ into a vector $c$, such that:\n\n$$\\begin{aligned}h_t = f(x_t, h_{t-1}) \\\\\nc = q({h_1,...,h_T}),\n\\end{aligned}$$\n\nwhere $h_t \\in \\mathbb{R}^n$ is the hidden state at time $t$, $c$ is a vector generated from the sequence of the hidden states and $f$ and $q$ are nonlinear functions.\n\nBefore defining our encoder network, we introduced a layer that learns the 128-dimensional representation (the size of the embedding space) of the end token for the English corpus. Therefore, the input dimension to the RNN was increased by 1. The RNN consists of a Long Short-Term Memory (LSTM) layer with 1024 units. Padded values are masked in the RNN, so they are simply ignored. The encoder is a multi-output model: it outputs the hidden state and cell states of the LSTM layer. The output of the LSTM layer is not used in a sequence to sequence architecture.","518abfeb":"# 4. Attention","36f11b84":"# 8. Conclusion"}}