{"cell_type":{"b6b7df90":"code","2af45273":"code","e55279c7":"code","95068ca1":"code","bdb34004":"code","a5efb9e6":"code","d06624fa":"code","0f0457b5":"code","2b519e51":"code","b2ea3064":"code","ff324cb6":"code","b19fbea5":"code","b644b591":"code","180eb1af":"code","37e21ea1":"code","9156a1df":"code","534a1e21":"code","7e108e6d":"code","b3bc3a68":"code","b97cafdc":"code","064a6894":"code","39ca0a7e":"code","5165704d":"code","b8d902b8":"code","2bbb5672":"code","eb1ed1e7":"code","f0b19e76":"code","4d22771a":"code","e10bb417":"code","6068e6cc":"code","ba942bbc":"code","00b52b08":"code","bb3167af":"code","e9ccef72":"code","50ba6396":"code","a2b84db9":"code","7ba3b844":"code","56f055b1":"code","c66cf5fe":"code","cfc80fd5":"code","977c2192":"code","7ca13eb6":"code","2d3280aa":"code","4039b6ef":"code","fb405ccf":"code","634e4227":"code","4bab1e82":"code","2720a23c":"code","7573ae2c":"code","6d6bb20f":"code","a4f2db3f":"code","9d82ce93":"code","77b0699e":"code","f4ce21a1":"code","615b69c4":"code","32824421":"code","c5bf44aa":"markdown","c54716af":"markdown","8c1df173":"markdown","114edb5f":"markdown","785630ed":"markdown","2fc9ff46":"markdown","57230965":"markdown","406b78d7":"markdown","25413810":"markdown","fef5355b":"markdown","bf09920c":"markdown","a609136c":"markdown","f55d0306":"markdown","0804f4ef":"markdown","1d31beca":"markdown","a757fd49":"markdown","82928f47":"markdown","58e9bade":"markdown","e1fb4623":"markdown","2c59090b":"markdown","f93e283a":"markdown","5ed4aee0":"markdown","98a9f1ba":"markdown","fb718f1a":"markdown","5d531aff":"markdown","5989130c":"markdown","ef2dadb0":"markdown","0ea0be68":"markdown","0de93201":"markdown","378aa8d5":"markdown","110d2e40":"markdown","9cb30e03":"markdown","6217261d":"markdown","98a8cd70":"markdown","4de052ad":"markdown","1917bbf4":"markdown","0e0881b8":"markdown","1f02913d":"markdown","3c314cb5":"markdown","988f50b3":"markdown","df73b075":"markdown","8cfcc490":"markdown","b44f5157":"markdown","ee406a76":"markdown","7a9abd74":"markdown","e86c0dd3":"markdown","a7d0d4b5":"markdown","5a3ae374":"markdown","7b4bfe44":"markdown","4ab744ef":"markdown","10f88fdc":"markdown","15119571":"markdown","ceb4596d":"markdown","df276993":"markdown","220ade4c":"markdown","c25d3c35":"markdown","156b7003":"markdown","acd52c74":"markdown","23b1da4e":"markdown","7f385ff0":"markdown","e3392d89":"markdown","abfd93aa":"markdown","105bac3f":"markdown","87fdac5b":"markdown","548f84c4":"markdown"},"source":{"b6b7df90":"from IPython.display import Image\nImage('..\/input\/illustrations\/preproc.jpg')","2af45273":"!pip install hdbscan","e55279c7":"import os\nimport numpy as np\nimport pydicom\nimport pandas as pd\nfrom random import sample\nfrom tqdm import tqdm_notebook as tqdm\nimport hdbscan\nfrom scipy.spatial.distance import jensenshannon\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import f1_score\nimport pickle\nimport random\nrandom.seed(1)\nfrom numpy.random import seed\nseed(1)\nimport matplotlib.pyplot as plt\n%matplotlib inline","95068ca1":"train_folder = '..\/input\/rsna-intracranial-hemorrhage-detection\/stage_1_train_images\/'","bdb34004":"train_df = pd.read_csv('..\/input\/rsna-intracranial-hemorrhage-detection\/stage_1_train.csv')\ntrain_df.head()","a5efb9e6":"img_labels_df = train_df[train_df['Label'] == 1].copy()\nimg_labels_df['Label_String'] = img_labels_df['ID'].map(lambda x: x.split('_')[-1])\nimg_labels_df['Image'] = img_labels_df['ID'].map(lambda x: 'ID_' + x.split('_')[1])\nimg_2_labels = img_labels_df.groupby('Image')['Label_String'].agg(list)","d06624fa":"plt.hist(img_2_labels.map(len))\nplt.xlabel('Number of conditions per positive image')\nplt.ylabel('Count')\nplt.title('Counts of labels per positive image')","0f0457b5":"print(f\"Label any is present in {100*sum(img_2_labels.map(lambda x: 'any' in x))\/len(img_2_labels):.2f}% of positive images.\")","2b519e51":"img_2_labels = img_2_labels.map(lambda x: [i for i in x if i != 'any'])\nimg_2_labels = img_2_labels.map(lambda x: ', '.join(x))\nimg_2_labels.head()","b2ea3064":"train_img_names = os.listdir('..\/input\/rsna-intracranial-hemorrhage-detection\/stage_1_train_images\/')\nprint(f'There are {len(train_img_names)} train images.')","ff324cb6":"train_img_names_subsample = sample(train_img_names, 1000)","b19fbea5":"def get_img(img_name, folder=train_folder):\n    pydicom_filedataset = pydicom.read_file(os.path.join(folder, img_name))\n    return pydicom_filedataset.pixel_array\n\ndef vizualize_tuple(imgs, img_names, grid_size=3):\n    fig, axes = plt.subplots(grid_size, grid_size, figsize=(grid_size+4, grid_size+4))\n    fig.tight_layout()\n    for img_i, img in enumerate(imgs):\n        ax = axes[img_i\/\/grid_size, img_i%grid_size]\n        ax.imshow(img, cmap='bone')\n        ax.axis('off')\n        ax.set_title(f\"{img_names[img_i]}:\\n{img_2_labels.get(img_names[img_i].replace('.dcm', ''), '')}\")","b644b591":"img_names_subsample = sample(train_img_names_subsample, 9)\nvizualize_tuple([get_img(img_name) for img_name in img_names_subsample], img_names_subsample)","180eb1af":"imgs_to_compare = ['ID_df5ae8f49.dcm', 'ID_8cb74b318.dcm']\n\ndef plot_img_pair(imgs, suptitle='', img_names=imgs_to_compare):\n    fig, axes = plt.subplots(1, 2, figsize=(7, 6))\n    fig.tight_layout()\n    for img_i, img in enumerate(imgs):\n        ax = axes[img_i]\n        ax.imshow(img, cmap='bone')\n        ax.axis('off')\n        ax.set_title(f\"{img_names[img_i]}:\\n{img_2_labels.get(img_names[img_i].replace('.dcm', ''), '')}\")\n    plt.suptitle(suptitle, fontsize=15)\n        \nplot_img_pair([get_img(img_name) for img_name in imgs_to_compare])","37e21ea1":"def plot_intensity_hists(imgs, bins=None, img_names=imgs_to_compare):\n    min_intesity = min([np.min(img) for img in imgs])\n    max_intesity = max([np.max(img) for img in imgs])\n    for img_i, img in enumerate(imgs):\n        plt.figure(figsize=(4, 4))\n        plt.hist(img.flatten(), bins=bins)\n        plt.xlabel('Image intensities', fontsize=11)\n        plt.ylabel('Pixels count', fontsize=11)\n        plt.xlim((min_intesity, max_intesity))\n        plt.title(f\"{img_names[img_i]}:\\n{img_2_labels.get(img_names[img_i].replace('.dcm', ''), '')}\", fontsize=14)\n        \nplot_intensity_hists([get_img(img_name) for img_name in imgs_to_compare])","9156a1df":"plot_img_pair([get_img(img_name) for img_name in imgs_to_compare], 'Initial Images')\n\ndef clip_negatives(img, threshold=0):\n    img[img < threshold] = threshold\n    return img\n\nplot_img_pair([clip_negatives(get_img(img_name)) for img_name in imgs_to_compare], 'Without Negative Intensities')","534a1e21":"plot_intensity_hists([clip_negatives(get_img(img_name)) for img_name in imgs_to_compare], bins=50)","7e108e6d":"plot_img_pair([get_img(img_name) for img_name in imgs_to_compare], 'Initial Images')\ndef clip_positives(img, threshold=1500):\n    img[img > threshold] = threshold\n    return img\n\nlb = 700\nub = 1500\nclipped_imgs = [clip_positives(clip_negatives(get_img(img_name), lb), ub) for img_name in imgs_to_compare]\nplot_img_pair(clipped_imgs, f'Intensities between {lb} and {ub}')","b3bc3a68":"plot_intensity_hists(clipped_imgs, bins=50)","b97cafdc":"plot_img_pair([get_img(img_name) for img_name in imgs_to_compare], 'Initial Images')\n\nlb = 900\nub = 1200\nclipped_imgs = [clip_positives(clip_negatives(get_img(img_name), lb), ub) for img_name in imgs_to_compare]\nplot_img_pair(clipped_imgs, f'Intensities between {lb} and {ub}')","064a6894":"img_names_diverse = ['ID_c96b7ba2a.dcm', 'ID_8cb74b318.dcm']\nimgs_diverse = [get_img(img_name) for img_name in img_names_diverse]\nplot_img_pair(imgs_diverse, img_names=img_names_diverse)\nplot_intensity_hists(imgs_diverse, img_names=img_names_diverse)","39ca0a7e":"def generate_hist_vec(img_name, bins=list(range(-50, 1500, 25))):\n    img = get_img(img_name)\n    return np.histogram(img[(img > -50) & (img < 1500)].flatten(), bins=bins)[0]\n\nhistogram_vectors = []\nfor img_name in tqdm(train_img_names_subsample, desc='Generation histogram vectors..'):\n    histogram_vectors.append(generate_hist_vec(img_name))","5165704d":"histograms_df = pd.DataFrame(np.array(histogram_vectors), columns=[f'bin_{i}' for i in range(len(histogram_vectors[0]))])","b8d902b8":"# clusterer = DBSCAN(eps=0.1, min_samples=3, metric=jensenshannon)\nclusterer = hdbscan.HDBSCAN(metric=jensenshannon)\nclusterer.fit(histograms_df)","2bbb5672":"print(f'There are {max(clusterer.labels_) + 1} clusters.')","eb1ed1e7":"print(f\"\"\"Sizes of the clusters:\n{pd.Series(clusterer.labels_).value_counts()}.\"\"\")","f0b19e76":"def get_cluster_sample(cluster_i, sample_size=9):\n    cluster_img_names = [img for img, is_in in zip(train_img_names_subsample, clusterer.labels_==cluster_i) \n                               if is_in]\n    cluster_sample = sample(cluster_img_names, min(sample_size, len(cluster_img_names)))\n    return cluster_sample\n\n                \ndef check_cluster_vizually(cluster_sample, lb=-4000, ub=5000, hist=True): \n    clipped_imgs = [clip_positives(clip_negatives(get_img(img_name), lb), ub) for img_name in cluster_sample]\n                    \n    def vizualize_nine_hists(imgs):\n        min_intesity = min([np.min(img) for img in imgs])\n        max_intesity = max([np.max(img) for img in imgs])\n        fig, axes = plt.subplots(3, 3, figsize=(7, 7))\n        for img_i, img in enumerate(imgs):\n            ax = axes[img_i\/\/3, img_i%3]\n            ax.hist(img.flatten(), bins=20)\n            ax.set_xlim((min_intesity, max_intesity))\n            ax.set_yticks([], [])\n        fig.suptitle('Histograms of pixel intensities')\n        \n    vizualize_tuple(clipped_imgs, cluster_sample)\n    if hist:\n        vizualize_nine_hists(clipped_imgs)\n    \n    \ncluster_samples = [get_cluster_sample(cluster_i) for cluster_i in range(max(clusterer.labels_) + 1)]\nfor cluster_i, cluster_sample in enumerate(cluster_samples):\n    plt.figure(figsize=(9, 1))\n    plt.plot(np.arange(20), np.ones(20))\n    plt.title(f'Cluster {cluster_i}')\n    check_cluster_vizually(cluster_sample, hist=False)","4d22771a":"cluster_2_intensity_limits = []","e10bb417":"check_cluster_vizually(cluster_samples[0], lb=930, ub=1150)","6068e6cc":"check_cluster_vizually(cluster_samples[0], lb=830, ub=930, hist=False)\ncheck_cluster_vizually(cluster_samples[0], lb=1150, ub=1250, hist=False)","ba942bbc":"cluster_2_intensity_limits.append((930, 1150))","00b52b08":"check_cluster_vizually(cluster_samples[1], lb=920, ub=1200)","bb3167af":"check_cluster_vizually(cluster_samples[1], lb=820, ub=920, hist=False)\ncheck_cluster_vizually(cluster_samples[1], lb=1200, ub=1300, hist=False)","e9ccef72":"cluster_2_intensity_limits.append((920, 1200))","50ba6396":"check_cluster_vizually(cluster_samples[2], lb=5, ub=300)","a2b84db9":"check_cluster_vizually(cluster_samples[2], lb=-95, ub=5, hist=False)\ncheck_cluster_vizually(cluster_samples[2], lb=300, ub=400, hist=False)","7ba3b844":"cluster_2_intensity_limits.append((5, 300))","56f055b1":"check_cluster_vizually(cluster_samples[3], lb=1500, ub=2000)","c66cf5fe":"check_cluster_vizually(cluster_samples[3], lb=970, ub=1300)","cfc80fd5":"check_cluster_vizually(cluster_samples[3], lb=870, ub=970, hist=False)\ncheck_cluster_vizually(cluster_samples[3], lb=1300, ub=1400, hist=False)","977c2192":"cluster_2_intensity_limits.append((970, 1300))","7ca13eb6":"check_cluster_vizually(cluster_samples[4], lb=1400, ub=2100)","2d3280aa":"check_cluster_vizually(cluster_samples[4], lb=900, ub=1300)","4039b6ef":"check_cluster_vizually(cluster_samples[4], lb=800, ub=900, hist=False)\ncheck_cluster_vizually(cluster_samples[4], lb=1300, ub=1400, hist=False)","fb405ccf":"cluster_2_intensity_limits.append((900, 1300))","634e4227":"non_noise_bool_index = clusterer.labels_ != -1\nhistograms_df = histograms_df[non_noise_bool_index]\nlabels = clusterer.labels_[non_noise_bool_index]","4bab1e82":"scaler = StandardScaler()\nhistograms_df_scaled = scaler.fit_transform(histograms_df)","2720a23c":"train_df, test_df, train_labels, test_labels = train_test_split(histograms_df_scaled, labels, test_size=0.2, stratify=labels)","7573ae2c":"svm = LinearSVC(multi_class='crammer_singer')\nsvm.fit(train_df, train_labels)","6d6bb20f":"test_predictions = svm.predict(test_df)","a4f2db3f":"print(f\"Average weighted F1 score on test data is: {f1_score(test_labels, test_predictions, average='weighted')}.\")","9d82ce93":"with open('scaler.pkl', 'wb') as f:\n    pickle.dump(scaler, f)\n    \nwith open('svm_cluster_type.pkl', 'wb') as f:\n    pickle.dump(svm, f)","77b0699e":"used_set = set(train_img_names_subsample)\nunseen_img_names = [img for img in train_img_names if img not in used_set]\nunseen_img_sample = sample(unseen_img_names, 9)\n\nhistogram_vectors = [generate_hist_vec(img) for img in unseen_img_sample]","f4ce21a1":"used_set = set(train_img_names_subsample)\nunseen_img_names = [img for img in train_img_names if img not in used_set]\nunseen_img_sample = sample(unseen_img_names, 16)\n\nhistogram_vectors = scaler.transform([generate_hist_vec(img) for img in unseen_img_sample])\nimg_cluster_classes = svm.predict(histogram_vectors)\nclipping_limits = [cluster_2_intensity_limits[class_i] for class_i in img_cluster_classes]\nclipped_imgs = [clip_positives(clip_negatives(get_img(img_name), lb), ub) for img_name, (lb, ub) in zip(unseen_img_sample,\n                                                                                                       clipping_limits)]\nvizualize_tuple(clipped_imgs, unseen_img_sample, 4)","615b69c4":"def plot_pie_per_cluster_type(cluster_class_i):\n    all_known_cluster_imgs = get_cluster_sample(cluster_class_i, sample_size=float('inf'))\n    all_deceaseas = []\n    for img in all_known_cluster_imgs:\n        all_deceaseas.extend(img_2_labels.get(img.replace('.dcm', ''), 'nothing').split(', '))\n    all_deceaseas_counts = pd.Series(all_deceaseas).value_counts()\n    \n    fig, ax = plt.subplots(figsize=(7, 7))\n    # credits: https:\/\/stackoverflow.com\/questions\/6170246\/how-do-i-use-matplotlib-autopct\n    def make_autopct(values):\n        def my_autopct(pct):\n            total = sum(values)\n            val = int(round(pct*total\/100.0))\n            return '{p:.0f}%  ({v:d})'.format(p=pct,v=val)\n        return my_autopct\n    ax.pie(all_deceaseas_counts, labels=all_deceaseas_counts.index, autopct=make_autopct(all_deceaseas_counts), shadow=True, startangle=90)\n    ax.axis('equal')\n    plt.title(f'Distribution of deceases per class {cluster_class_i}', fontsize=15)","32824421":"for cluster_class_i in range(max(clusterer.labels_)+1):\n    plot_pie_per_cluster_type(cluster_class_i)","c5bf44aa":"# Cluster-type Classifier","c54716af":"### Scaling data","8c1df173":"## Cluster-3 image type","114edb5f":"### With range clipping","785630ed":"Let's leave values between 700 and 1500.","2fc9ff46":"## Images for analysis","57230965":"Let's first clip away negative intensities.","406b78d7":"No positive image has a single label. Let's check if the label ```any``` is redundant.","25413810":"# Next steps\n  1. Generation of the pre-processed dataset,\n  2. Comparison of deep net training convergence with and without the preprocessing,\n  3. Improvements of the pre-processing if needed.","fef5355b":"## Intensities range","bf09920c":"# Data","a609136c":"We can see, the ranges of intensities are very wide: 5000 instead of standard 255. As a result, approximately 20 different values of raw intensities become effectively indistinguishable from each other to a human eye. And for a deep net the relevant information is hidden in a pile of less relevant data.\n\nLet's manually select intensities range with the most important information for the competition task.","f55d0306":"### With range clipping","0804f4ef":"### Again Skull?","1d31beca":"## Cluster-4 image type","a757fd49":"#### Checking that nothing very informative was thrown away\nThe lost information would be gray, brain-tissue-like (between zero and one)","82928f47":"Random subsample","58e9bade":"# Investigating lack of brain tissue details","e1fb4623":"## Cluster-0 image type","2c59090b":"#### Checking that nothing very informative was thrown away\nThe lost information would be gray\/brain-tissue-like (between zero and one)","f93e283a":"## Summary on raw images","5ed4aee0":"We got rid of the redundant background and we can already start seeing some brain-tissue details. Let's see new intensities ranges.","98a9f1ba":"We observe higher HU values.\n\n> Dense materials such as bone have density values approaching +1000 HU.\n([source](https:\/\/www.sciencedirect.com\/topics\/medicine-and-dentistry\/hounsfield-scale))\n\nTherefore, I'd consider this cluster to correspond to low brain visibility and strong skull pattern. However, the skull itself is of low interest (hopefully I'm not mistaken). So I'd clip its intensities.","fb718f1a":"I'll focus on a pair of images which are look-alike to me, even though one has ```Intraventricular``` and another one has ```Intraparenchymal``` in it.","5d531aff":"# Clustering images","5989130c":"It seems this cluster is very similar to the previous one. Perhaps, a tiny a \"halo\" around the skull might be the main difference.","ef2dadb0":"# Check: preprocessing of new images","0ea0be68":"## Generating intensity histograms","0de93201":"Let's cluster images based on histograms of intensities. We'll use [HDBSCAN](https:\/\/hdbscan.readthedocs.io) for clustering.","378aa8d5":"#### Checking that nothing very informative was thrown away\nThe lost information would be gray\/brain-tissue-like (between zero and one)","110d2e40":"# Raw images visualization","9cb30e03":"## Cluster-2 image type","6217261d":"## Organizing labels","98a8cd70":"# Plan\n1. [Libraries](#Libraries)\n2. [Data](#Data)\n3. [Raw images visualization](#Raw-images-visualization)\n  * [Comparison with expected CT images](#Comparison-with-expected-CT-images)\n4. [Investigating lack of brain tissue details](#Investigating-lack-of-brain-tissue-details)\n  * [Intensities range](#Intensities-range)\n    * [Challenge with fine-grained intensities](#Challenge-with-fine-grained-intensities)\n    * [Manual range reduction](#Manual-range-reduction)\n5. [Dataset diversity](#Dataset-diversity)\n6. [Clustering images](#Clustering-images)\n  * [Generating intensity histograms](#Generating-intensity-histograms)\n  * [HDBSCAN](#HDBSCAN)\n  * [Visual check of the clusters](#Visual-check-of-the-clusters)\n7. [Range selection for each cluster](#Range-selection-for-each-cluster)\n  * [Cluster-0 image type](#Cluster-0-image-type)\n  * [Cluster-1 image type](#Cluster-1-image-type)\n  * [Cluster-2 image type](#Cluster-2-image-type)\n  * [Cluster-3 image type](#Cluster-3-image-type)\n  * [Cluster-4 image type](#Cluster-4-image-type)\n8. [Cluster-type Classifier](#Cluster-type-Classifier)\n  * [Data preprocessing](#Data-preprocessing)\n  * [Stratified train\/test split](#Stratified-train\/test-split)\n  * [SVM](#SVM)\n  * [Performance check](#Performance-check)\n9. [Check: preprocessing of new images](#Check:-preprocessing-of-new-images)\n10. [Distribution of hemorrhages per image-type](#Distribution-of-hemorrhages-per-image-type)\n11. [Next steps](#Next-steps)","4de052ad":"As we are clustering histograms, we'd like to use metric capturing similarity of two distributions. Let's use [Jensen\u2013Shannon divergence](https:\/\/en.wikipedia.org\/wiki\/Jensen%E2%80%93Shannon_divergence). One of HDBSCAN advantages is its ability to work with arbitrary similarity measures.","1917bbf4":"## Stratified train\/test split","0e0881b8":"## SVM\nLinear SVM for it to be fast.","1f02913d":"## Visual check of the clusters","3c314cb5":"### With range clipping","988f50b3":"To sum up, based on the allowed-to-use intensities we've arrived at cluster classes which in addition to the rules-complient preprocessing provide us with some info about final labels.","df73b075":"### With range clipping","8cfcc490":"Hence, naive intensity range selection is not an option.","b44f5157":"### Bones? ","ee406a76":"# Intro\n## Problem being solved\n  * when displaying pixel data, CT scans don't look like one would expect.","7a9abd74":"#### Checking that nothing very informative was thrown away\nThe lost information would be gray, brain-tissue-like (between zero and one)","e86c0dd3":"# Distribution of hemorrhages per image-type","a7d0d4b5":"# Libraries","5a3ae374":"To sum up,\nraw images don't have detalization of example images (e.g. in the figure with the explanations). Due to the lack of details, pictures of different hemorrhages then look alike. Even though the required info likely is somewhere in the image, pulling it to the surface before feeding to the network would be desirable. ","7b4bfe44":"## Data preprocessing\n### Removing the noise cluster","4ab744ef":"## Performance check","10f88fdc":"Based on the [initial sample of images](#Random-subsample), we saw that there are quite diverse images in the dataset:","15119571":"## HDBSCAN","ceb4596d":"### Manual range reduction","df276993":"Let's remove the redundant label.","220ade4c":"P.S. Metadata in the DICOM files cannot be used per competition rules. That's why I rushed into the detective work you can find below :)\n\nOtherwise, with metadata it'd be possible to extract the most relevant info (so that the scans would look like ones in the right picture). More details can be found [here](https:\/\/www.kaggle.com\/c\/rsna-intracranial-hemorrhage-detection\/discussion\/109328). It must be said that in the end metadata might be allowed, I'd recommend to check [the *Welcome!* thread](https:\/\/www.kaggle.com\/c\/rsna-intracranial-hemorrhage-detection\/discussion\/109258).\n\nP.P.S. In theory deep nets are able to learn preprocessing, however in practice lack of preprocessing unnecessarily complicates training\n\nP.P.P.S. :D \nI've checked 100.000 randomly sampled training images, and `RescaleSlope` was always equal to 1. I believe, it's reasonable to assume that pixel data are in Hounsfield units, and there's no need of intensity rescaling. ","c25d3c35":"# Dataset diversity","156b7003":"#### Checking that nothing very informative was thrown away\nThe lost information would be gray, brain-tissue-like (between zero and one)","acd52c74":"## Range selection for each cluster\n\nLet's visualization of pixel data together with pixel intensity histograms.\nApproach: gradually clip values in a search for main class mode of the histograms, then try to center the mode (between range bounds). Then check pixels around boarders (which are clipped).","23b1da4e":"## Cluster-1 image type","7f385ff0":"Let's tighten the intensities range.","e3392d89":"We can see the visualizations we get don't look like ones on the explanation page:\n![](https:\/\/cdn.discordapp.com\/attachments\/507208726864855060\/623862702955036702\/RSNA_Intracranial_Hemorrhage_Detection_Kaggle.png)","abfd93aa":"## Comparison with expected CT images","105bac3f":"### Challenge with fine-grained intensities","87fdac5b":"### With chosen range clipping","548f84c4":"It's exciting to start seeing brain tissue."}}