{"cell_type":{"562b0c17":"code","3f5c8246":"code","ad642733":"code","dbf6d312":"code","0f2680db":"code","c8c1c91c":"code","dff0d861":"code","997a8370":"code","01784c32":"code","a20f283d":"code","571b2da7":"code","5b22ddc7":"code","f583341a":"code","720b3ebf":"code","25d64135":"code","03e52b55":"code","a5b52a90":"code","16b49425":"code","dff77076":"markdown","c7b3dd6c":"markdown","b0b28ccc":"markdown","6959cc6b":"markdown","3069767a":"markdown","9e61b78b":"markdown","b2a5c146":"markdown","f50b931e":"markdown","8e1b6942":"markdown","03e1ca39":"markdown","08bb6de6":"markdown","77db62ab":"markdown","20b8732d":"markdown","61c98c2b":"markdown"},"source":{"562b0c17":"import numpy as np, pandas as pd, matplotlib.pyplot as plt\nimport joblib\nimport optuna\nimport sklearn \n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import VotingClassifier","3f5c8246":"# load data\ntrain = pd.read_csv('..\/input\/forest-cover-type-prediction\/train.csv')\n# view data\ntrain.head()","ad642733":"# remove ID column from set\ntrain = train.iloc[:, 1:]\ntrain.head()","dbf6d312":"# add new features from feature engineering\ntrain['Elev_to_Horizontal_Hyd'] = train.Elevation - 0.2 * train.Horizontal_Distance_To_Hydrology \ntrain['Elev_to_Horizontal_Road'] = train.Elevation - 0.05 * train.Horizontal_Distance_To_Roadways  \ntrain['Elev_to_Verticle_Hyd'] = train.Elevation - train.Vertical_Distance_To_Hydrology \ntrain['Mean_Horizontal_Dist'] = (train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology + \n                                 train.Horizontal_Distance_To_Roadways)\/3 \ntrain['Mean_Fire_Hydro'] = (train.Horizontal_Distance_To_Fire_Points + train.Horizontal_Distance_To_Hydrology)\/2","0f2680db":"# move target to first column\nfirst_column = train.pop('Cover_Type')\n  \n# insert column using insert(position,column_name,first_column) function\ntrain.insert(0, 'Cover_Type', first_column)\n  \n# view\ntrain.head()","c8c1c91c":"# create cat, num, and y\nX_cat = train.iloc[:, 11:55].values\nB = train.iloc[:, 55:60]\nA = train.iloc[:, 1:11]\nX_num = pd.concat([A, B], axis = 1).values\ny = train.iloc[:, 0].values","dff0d861":"# scale\/standardizing numerical columns\n# scaler object\nscaler = StandardScaler()\n# fit to training data\nscaler.fit(X_num)\n# scale num columns\nX_num = scaler.transform(X_num)\n\n# shape\nprint(f'Categorical Shape: {X_cat.shape}')\nprint(f'Numerical Shape: {X_num.shape}')\nprint(f'Label Shape: {y.shape}')","997a8370":"# combine num and cat\nX = np.hstack((X_num, X_cat))\nprint(X.shape)","01784c32":"# best model \n\nlr_model = LogisticRegression(random_state = 1, \n                              penalty = 'none', \n                              max_iter = 500, \n                              solver = 'saga')\nlr_model.fit(X, y)\nprint(lr_model.score(X, y))","a20f283d":"# dt best model\ndt_model = DecisionTreeClassifier(random_state = 1, \n                                  max_depth = 19, \n                                  min_samples_leaf = 1)\ndt_model.fit(X, y)","571b2da7":"# best model\nrf_model = RandomForestClassifier(random_state = 1, \n                                  n_estimators = 130, \n                                  max_depth = 50, \n                                  min_samples_leaf = 1)\n\nrf_model.fit(X, y)","5b22ddc7":"# best model\ntree_model = ExtraTreesClassifier(random_state = 1, \n                                  n_estimators = 1000, \n                                  max_depth = 33, \n                                  min_samples_leaf = 1)\n\ntree_model.fit(X, y)","f583341a":"%%time\n# best model\ngradb_model = GradientBoostingClassifier(random_state = 0,\n                                         max_depth = 19, \n                                         min_samples_leaf = 16)\n\ngradb_model.fit(X, y)","720b3ebf":"%%time\n# best model\nxgb_model = XGBClassifier(random_state = 0,\n                           max_depth = 14,\n                           tree_method = 'gpu_hist')\n\nxgb_model.fit(X, y)\nxgb_model.score(X, y)","25d64135":"%%time\n# best model\nlgbm_model = LGBMClassifier(random_state = 0,\n                            max_depth = 16,\n                            num_leaves = 10)\n\nlgbm_model.fit(X, y)","03e52b55":"%%time \n# create ensemble classifier \nensemble_model = VotingClassifier(\n    estimators = [('tree', tree_model), \n                  ('rf', rf_model), \n                  ('gradb', gradb_model), \n                  ('xgb', xgb_model)],\n    voting = 'hard'\n)\n\n# fit\nensemble_model.fit(X, y)\n\n# print training accuracy\nprint('Logistic Regression Accuracy', lr_model.score(X, y))\nprint('Decision Tree Accuracy', dt_model.score(X, y))\nprint('Random Forest Accuracy', rf_model.score(X, y))\nprint('Extra Trees Accuracy', tree_model.score(X, y))\nprint('Gradient Boosting Accuracy', gradb_model.score(X, y))\nprint('Extra Gradient Boosting Accuracy', xgb_model.score(X, y))\nprint('LightGBM Accuracy', lgbm_model.score(X, y))\nprint('Ensemble Accuracy:', ensemble_model.score(X, y))","a5b52a90":"# save scaler\njoblib.dump(scaler, 'forest_cover_scaler_final.joblib')","16b49425":"joblib.dump(rf_model, 'rf_model_final.joblib')\njoblib.dump(tree_model, 'tree_model_final.joblib')\njoblib.dump(gradb_model, 'gradb_model_final.joblib')\njoblib.dump(xgb_model, 'xgb_model_final.joblib')\njoblib.dump(lgbm_model, 'lgbm_model_final.joblib')\njoblib.dump(ensemble_model, 'ensemble_model_final.joblib')\nprint('Model written to file.')","dff77076":"# Various Models\nThe following sections include various models we built to predict the Forest Cover Type. We used GridSearchCV and Optuna to identify the optimal paramaters for each model. \n\nOur hyperparameter tuning can be found in the notebook: https:\/\/www.kaggle.com\/emknowles\/g2-forestcovertype-modelparams-notebook\/","c7b3dd6c":"# Extreme Gradient Boosting Classifier\nThe best parameters from Optuna were {'max_depth': 14}","b0b28ccc":"# Model Selection ","6959cc6b":"# Extra Tree Classifier\nThe best parameters from Optuna were {'max_depth': 33, 'min_samples_leaf': 1}","3069767a":"# Decision Tree\nThe best parameters from Optuna were {'max_depth': 19, 'min_samples_leaf': 1}","9e61b78b":"# Gradient Boosting Classifier\nThe best parameters from Optuna were {'max_depth': 19, 'min_samples_leaf': 16}","b2a5c146":"# Light Gradient Boosting Classifier\nThe best parameters from Optuna were {'num_leaves': 10, 'max_depth': 16}","f50b931e":"# Import Statements","8e1b6942":"# Save Preprocessor and Models","03e1ca39":"# Load Data","08bb6de6":"# Logistic Regression\nLogistic Regression did not yield high accuracy, even after GridSearchCV and Optuna were used to find optimal parameters.","77db62ab":"# Random Forest\nThe best parameters from Optuna were {'n_estimators': 130, 'max_depth': 50, 'min_samples_leaf': 1}","20b8732d":"# Preprocessing","61c98c2b":"# Add Features from Feature Engineering"}}