{"cell_type":{"d573339f":"code","fb3a97b0":"code","7fc603f0":"code","e6edddc1":"code","32f6bb8b":"code","d0ec050c":"code","62fe1556":"code","6eee5d0f":"code","cc5d851e":"code","ad709a5d":"code","4eba4f64":"code","2ca7c81c":"code","50432dbf":"code","1c05b48b":"code","b8f540d1":"code","23a269af":"code","b13dcfc6":"code","b163e9fa":"code","cb660ae0":"code","ad8ea5fd":"code","779d6cbb":"code","758e3b63":"code","08c40564":"code","875547f5":"code","8ab8a93f":"code","c8d2b897":"code","4c9e6421":"code","0cca79b1":"code","dae37625":"code","f1d1030a":"code","9af4972e":"markdown","25d64b86":"markdown","f8400676":"markdown","bb4fcf46":"markdown","c3c9ee10":"markdown","d3483307":"markdown","dbe283a2":"markdown","28e5a2a1":"markdown","2fe750ba":"markdown","67b14c4f":"markdown","45ecc7ef":"markdown","ee25aea1":"markdown","f4babba1":"markdown","42ed7cb0":"markdown","8228c48d":"markdown","4a49aed2":"markdown","a06138a1":"markdown","bd9c573f":"markdown","f1250e23":"markdown","98c0ce1e":"markdown","3ce6d0ce":"markdown","1778e4c8":"markdown","4261a56b":"markdown"},"source":{"d573339f":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import KFold, train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, matthews_corrcoef, plot_confusion_matrix\nfrom sklearn.ensemble import GradientBoostingClassifier as gbc","fb3a97b0":"matplotlib.rcParams['figure.figsize'] = (20.0, 10.0) # I like big figures!","7fc603f0":"def get_ds_infos():\n    \"\"\"\n    Read the file includes data subject information.\n    \n    Data Columns:\n    0: code [1-24]\n    1: weight [kg]\n    2: height [cm]\n    3: age [years]\n    4: gender [0:Female, 1:Male]\n    \n    Returns:\n        A pandas DataFrame that contains inforamtion about data subjects' attributes \n    \"\"\" \n\n    dss = pd.read_csv(\"data_subjects_info.csv\")\n    print(\"[INFO] -- Data subjects' information is imported.\")\n    \n    return dss\n\ndef set_data_types(data_types=[\"userAcceleration\"]):\n    \"\"\"\n    Select the sensors and the mode to shape the final dataset.\n    \n    Args:\n        data_types: A list of sensor data type from this list: [attitude, gravity, rotationRate, userAcceleration] \n\n    Returns:\n        It returns a list of columns to use for creating time-series from files.\n    \"\"\"\n    dt_list = []\n    for t in data_types:\n        if t != \"attitude\":\n            dt_list.append([t+\".x\",t+\".y\",t+\".z\"])\n        else:\n            dt_list.append([t+\".roll\", t+\".pitch\", t+\".yaw\"])\n\n    return dt_list\n\n\ndef creat_time_series(dt_list, act_labels, trial_codes, mode=\"mag\", labeled=True):\n    \"\"\"\n    Args:\n        dt_list: A list of columns that shows the type of data we want.\n        act_labels: list of activites\n        trial_codes: list of trials\n        mode: It can be \"raw\" which means you want raw data\n        for every dimention of each data type,\n        [attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)].\n        or it can be \"mag\" which means you only want the magnitude for each data type: (x^2+y^2+z^2)^(1\/2)\n        labeled: True, if we want a labeld dataset. False, if we only want sensor values.\n\n    Returns:\n        It returns a time-series of sensor data.\n    \n    \"\"\"\n    num_data_cols = len(dt_list) if mode == \"mag\" else len(dt_list*3)\n\n    if labeled:\n        dataset = np.zeros((0,num_data_cols+7)) # \"7\" --> [act, code, weight, height, age, gender, trial] \n    else:\n        dataset = np.zeros((0,num_data_cols))\n        \n    ds_list = get_ds_infos()\n    \n    print(\"[INFO] -- Creating Time-Series\")\n    for sub_id in ds_list[\"code\"]:\n        for act_id, act in enumerate(act_labels):\n            for trial in trial_codes[act_id]:\n                fname = 'A_DeviceMotion_data\/A_DeviceMotion_data\/'+act+'_'+str(trial)+'\/sub_'+str(int(sub_id))+'.csv'\n                raw_data = pd.read_csv(fname)\n                raw_data = raw_data.drop(['Unnamed: 0'], axis=1)\n                vals = np.zeros((len(raw_data), num_data_cols))\n                for x_id, axes in enumerate(dt_list):\n                    if mode == \"mag\":\n                        vals[:,x_id] = (raw_data[axes]**2).sum(axis=1)**0.5        \n                    else:\n                        vals[:,x_id*3:(x_id+1)*3] = raw_data[axes].values\n                    vals = vals[:,:num_data_cols]\n                if labeled:\n                    lbls = np.array([[act_id,\n                            sub_id-1,\n                            ds_list[\"weight\"][sub_id-1],\n                            ds_list[\"height\"][sub_id-1],\n                            ds_list[\"age\"][sub_id-1],\n                            ds_list[\"gender\"][sub_id-1],\n                            trial          \n                           ]]*len(raw_data))\n                    vals = np.concatenate((vals, lbls), axis=1)\n                dataset = np.append(dataset,vals, axis=0)\n    cols = []\n    for axes in dt_list:\n        if mode == \"raw\":\n            cols += axes\n        else:\n            cols += [str(axes[0][:-2])]\n            \n    if labeled:\n        cols += [\"act\", \"id\", \"weight\", \"height\", \"age\", \"gender\", \"trial\"]\n    \n    dataset = pd.DataFrame(data=dataset, columns=cols)\n    return dataset\n","e6edddc1":"ACT_LABELS = [\"dws\",\"ups\", \"wlk\", \"jog\", \"std\", \"sit\"]\nTRIAL_CODES = {\n    ACT_LABELS[0]:[1,2,11],\n    ACT_LABELS[1]:[3,4,12],\n    ACT_LABELS[2]:[7,8,15],\n    ACT_LABELS[3]:[9,16],\n    ACT_LABELS[4]:[6,14],\n    ACT_LABELS[5]:[5,13]\n}\n\n## Here we set parameter to build labeld time-series from dataset of \"(A)DeviceMotion_data\"\n## attitude(roll, pitch, yaw); gravity(x, y, z); rotationRate(x, y, z); userAcceleration(x,y,z)\nsdt = [\"attitude\", \"userAcceleration\"]\nprint(\"[INFO] -- Selected sensor data types: \"+str(sdt))    \nact_labels = ACT_LABELS [0:6]\nprint(\"[INFO] -- Selected activites: \"+str(act_labels))    \ntrial_codes = [TRIAL_CODES[act] for act in act_labels]\ndt_list = set_data_types(sdt)","32f6bb8b":"os.chdir('\/kaggle\/input\/motionsense-dataset\/')\ndataset = creat_time_series(dt_list, act_labels, trial_codes, mode=\"raw\", labeled=True)\nprint(\"[INFO] -- Shape of time-Series dataset:\"+str(dataset.shape))    \ndataset.head()","d0ec050c":"act_dict = {0: 'dws',\n            1: 'ups',\n            2: 'wlk',\n            3: 'jog',\n            4: 'std',\n            5: 'sit'}\ndataset['label'] =  dataset.act.apply(lambda act: act_dict[act])","62fe1556":"dataset.isna().sum()","6eee5d0f":"dataset.label.value_counts()","cc5d851e":"metrics = ['attitude.roll', 'attitude.pitch', 'attitude.yaw', 'userAcceleration.x', 'userAcceleration.y', \n            'userAcceleration.z']","ad709a5d":"def plot_data(start_position, number_of_frames, metrics=metrics, fourier=False):\n    print(f\"Looking at {metrics} for {number_of_frames} from {start_position}\")\n    for label in dataset.label.unique():\n        mini_df = dataset[dataset['label']==label].iloc[start_position:start_position+number_of_frames].reset_index()\n        for metric in metrics:\n            mini_df[metric].plot(title=label, legend=True)\n        plt.show()","4eba4f64":"plot_data(np.random.randint(1,130000), 300)","2ca7c81c":"plot_data(np.random.randint(1,130000), 300)","50432dbf":"acceleration_metrics = [metric for metric in metrics if metric.startswith('userAcceler')]\nacceleration_metrics","1c05b48b":"for metric in acceleration_metrics:\n    dataset[f'{metric}_ra'] = dataset[metric].rolling(20).median()","b8f540d1":"avg_acc_metrics = [metric for metric in dataset.columns if metric.endswith('_ra')]\navg_acc_metrics","23a269af":"dataset.columns","b13dcfc6":"df = dataset.fillna(0, axis=0) # the rolling window made a bunch of nans and it makes our fft unhappy.","b163e9fa":"metrics = ['userAcceleration.x_ra', 'userAcceleration.y_ra', 'userAcceleration.z_ra', \n           'attitude.roll', 'attitude.pitch', 'attitude.yaw']\n\ndef build_features(data=df, number_of_frames=300, metrics=metrics):\n    instances = pd.DataFrame() # this is where our features and labels will end up.\n    instance = 0\n    data.fillna(method='bfill')\n    for label in data.label.unique():\n        print(f\"Building features for {label}...\")\n        start_position=0\n        label_df = data[data['label']==label]\n        while len(label_df) > start_position+number_of_frames:\n            for metric in metrics:\n                instance_df = label_df.iloc[start_position:start_position+number_of_frames].reset_index()\n                instances.loc[instance, 'label'] = label\n                instances.loc[instance, f'median_{metric}'] = instance_df[metric].median()\n                instances.loc[instance, f'std_{metric}'] = instance_df[metric].std()\n                fourier = np.fft.rfft(instance_df[metric])[1:]\n                amplitude = max(np.abs(fourier))\n                frequency = np.where(np.abs(fourier)==amplitude)[0][0]\n                instances.loc[instance, f'amplitude_{metric}'] = amplitude\n                instances.loc[instance, f'frequency_{metric}'] = frequency\n                instances.loc[instance, f'phase_{metric}'] = np.angle(fourier)[frequency]\n            instance = instance + 1\n            start_position = start_position + number_of_frames\n    return instances","cb660ae0":"instances = build_features()","ad8ea5fd":"instances.head()","779d6cbb":"len(instances)","758e3b63":"instances.label.value_counts()","08c40564":"for label in instances.label.unique():\n    for feature in ['median_attitude.pitch']:\n        instances[instances.label==label][feature].hist(alpha=0.3, label=label, bins=20)\nplt.legend()\nplt.show()","875547f5":"frequencies = [feature for feature in instances.columns if feature.startswith('frequency')]\nfrequencies","8ab8a93f":"instances['med_freq'] = instances[frequencies].median(axis=1)","c8d2b897":"for label in instances.label.unique():\n    instances[instances.label==label]['med_freq'].hist(alpha=0.3, label=label, bins=20)\nplt.legend()\nplt.show()","4c9e6421":"def plot_feature_importance(model, feature_names):\n    feature_importances = model.feature_importances_\n    idxSorted = np.argsort(feature_importances)[-10:]\n    barPos = np.arange(idxSorted.shape[0]) + .5\n    plt.barh(barPos, feature_importances[idxSorted], align='center')\n    plt.yticks(barPos, feature_names[idxSorted])\n    plt.xlabel('Feature Importance')\n    plt.subplots_adjust(left=0.2, right=0.9, top=0.9, bottom=0.1)\n    plt.show()","0cca79b1":"y = instances.label\nX = instances.drop('label', axis=1)\nkf = KFold(n_splits=5, shuffle=True)\ngbc_model = gbc()\nMMC = []\nfor train_index, test_index in kf.split(X):\n    X_train, X_test = X.loc[train_index], X.loc[test_index]\n    y_train, y_test = y.loc[train_index], y.loc[test_index]\n    gbc_model.fit(X_train, y_train)\n    y_pred = gbc_model.predict(X_test)\n    print(f\"MMC: {matthews_corrcoef(y_test, y_pred):.3f}\")\n    MMC.append(matthews_corrcoef(y_test, y_pred))\nprint(f\"Mean MMC: {np.mean(MMC):.3f}\")\nprint(f\"Std of MMCs: {np.std(MMC):.4f}\")\nprint(\"These are the plots of the last test so we could get an idea of what it looks like:\")\nplot_confusion_matrix(gbc_model, X=X_test, y_true=y_test, labels=gbc_model.classes_, cmap='Blues')\nplt.show()\nplot_feature_importance(gbc_model, X_test.columns)","dae37625":"no_fft_features = [feature for feature in instances.columns  if feature.startswith('median') or feature.startswith('std')]\nno_fft_features","f1d1030a":"y = instances.label\nfeatures = no_fft_features\nX = instances[features]\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\ngbc_model = gbc()\ngbc_model.fit(X_train, y_train)\ny_pred = gbc_model.predict(X_test)\nprint(f\"MMC: {matthews_corrcoef(y_test, y_pred)}\")\nplot_confusion_matrix(gbc_model, X=X_test, y_true=y_test, labels=gbc_model.classes_, cmap='Blues')\nplt.show()\nplot_feature_importance(gbc_model, X_test.columns)","9af4972e":"#### Create labels so we don't get confused.","25d64b86":"So the Fourier transform made a difference but even without it the algorithm works surprisingly well...","f8400676":"#### We also expect frequency to be a way of setting the joggers apart:","bb4fcf46":"#### We expect median pitch to be a way of differentiating sitting:","c3c9ee10":"### Building the dataset:","d3483307":"## General statistics:","dbe283a2":"## What can be done to improve predictions:\nLots! <br>\n- Hyperparameter optimization - I just took the default values in the first tree classifier I could think of...\n- Different prediction model - Another tree or a different classifier.\n- More features - for instance - Phase!\n- More preprocessing so a better moving avg or aligning the data somehow.\n- Cut the data into smaller pieces (less frames per instance) might improve prediction but, we would need to think hard about how to measure the improvement.","28e5a2a1":"#### Yay!","2fe750ba":"### I ran a bunch of these at random points just to see what we are dealing with.","67b14c4f":"## Feature Egineering\n### Each metric will get 5 features:\n - ### median\n - ### std\n - ### amplitude\n - ### frequency\n - ### phase","45ecc7ef":"### Bam!\nResults are pretty good, very stable and comparable with different notebooks here using DL techniques.","ee25aea1":"### I spent quite a bit of time debugging that fourier function up there.. so I wonder how much did it help for the overall score?","f4babba1":"#### To get a better feel for the data we'll take a deeper look and try to guess which metrics will be helpful to us.","42ed7cb0":"### Some quick conclusions:\n - 300 frames should be enough to get a frequency and amplitude.\n - Acceleration data is noisy!\n - As we would expect, sitting & standing are differet because of their low acceleration amplitude and can be told apart based on pitch.\n - As expected, jogging would be easy to differentiate using frequency.","8228c48d":"### Sanity checks:","4a49aed2":"## Let's take a deeper look at our data!","a06138a1":"#### Tada!","bd9c573f":"### Build the dataset as recommended in the [github repository's](https:\/\/github.com\/mmalekzadeh\/motion-sense) README:","f1250e23":"## Let's just shove it in a decision tree and see what happens:","98c0ce1e":"### Denoising acceleration data:","3ce6d0ce":"### The next block uses loops inside a dataframe which, generally, is a big nono as it is very inefficicent. <br> I chose to not refactor it into something faster (using groupby for instance) because I think it makes things much clearer and more readable and that is our priority in this pedagogical notebook.","1778e4c8":"## This notebook as a short experiment with fourier transformation.\n","4261a56b":"### Looks like ups, jog & dws are underrepresented but I think we have enough data to get a decent prediction on all labels."}}