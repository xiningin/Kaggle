{"cell_type":{"2825f992":"code","8d5654ce":"code","81bf2dea":"code","e5a1f9fc":"code","b2819abf":"code","42edd5f1":"code","f6772b6d":"code","33abb882":"code","ea903b05":"code","3daf1390":"code","a031f516":"code","7fb3d9b2":"code","4759f5f1":"code","3ff74cf8":"code","80849d69":"markdown","479ad3f8":"markdown","bab85c70":"markdown","2b45dc38":"markdown","6e8064a8":"markdown","cfaba69f":"markdown","48ebd423":"markdown","92553571":"markdown","6e864692":"markdown","3352b2e5":"markdown","5bb1b8cb":"markdown","f92eac31":"markdown"},"source":{"2825f992":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\nimport tensorflow as tf\nfrom keras.layers import Dense, Input, Activation\nfrom keras.layers import BatchNormalization,Add\nfrom keras.optimizers import Adam\nfrom keras.models import Model, load_model\nfrom keras import callbacks\nfrom keras import backend as K\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(action=\"ignore\",category=DeprecationWarning)\nwarnings.filterwarnings(action=\"ignore\",category=FutureWarning)\n","8d5654ce":"df_train=pd.read_csv('..\/input\/train.csv')\ndf_test=pd.read_csv('..\/input\/test.csv')\ndf_struct=pd.read_csv('..\/input\/structures.csv')\n\n#df_train_sub_potential=pd.read_csv('..\/input\/potential_energy.csv')\n#df_train_sub_moment=pd.read_csv('..\/input\/dipole_moments.csv')\ndf_train_sub_charge=pd.read_csv('..\/input\/mulliken_charges.csv')\ndf_train_sub_tensor=pd.read_csv('..\/input\/magnetic_shielding_tensors.csv')\n","81bf2dea":"def map_atom_info(df_1,df_2, atom_idx):\n    df = pd.merge(df_1, df_2, how = 'left',\n                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n                  right_on = ['molecule_name',  'atom_index'])\n    df = df.drop('atom_index', axis=1)\n\n    return df\n\n\n\nfor atom_idx in [0,1]:\n    df_train = map_atom_info(df_train,df_struct, atom_idx)\n    df_train = map_atom_info(df_train,df_train_sub_charge, atom_idx)\n    df_train = map_atom_info(df_train,df_train_sub_tensor, atom_idx)\n    df_train = df_train.rename(columns={'atom': f'atom_{atom_idx}',\n                                        'x': f'x_{atom_idx}',\n                                        'y': f'y_{atom_idx}',\n                                        'z': f'z_{atom_idx}',\n                                        'mulliken_charge': f'charge_{atom_idx}',\n                                        'XX': f'XX_{atom_idx}',\n                                        'YX': f'YX_{atom_idx}',\n                                        'ZX': f'ZX_{atom_idx}',\n                                        'XY': f'XY_{atom_idx}',\n                                        'YY': f'YY_{atom_idx}',\n                                        'ZY': f'ZY_{atom_idx}',\n                                        'XZ': f'XZ_{atom_idx}',\n                                        'YZ': f'YZ_{atom_idx}',\n                                        'ZZ': f'ZZ_{atom_idx}',})\n    df_test = map_atom_info(df_test,df_struct, atom_idx)\n    df_test = df_test.rename(columns={'atom': f'atom_{atom_idx}',\n                                'x': f'x_{atom_idx}',\n                                'y': f'y_{atom_idx}',\n                                'z': f'z_{atom_idx}'})\n    #add some features\n    df_struct['c_x']=df_struct.groupby('molecule_name')['x'].transform('mean')\n    df_struct['c_y']=df_struct.groupby('molecule_name')['y'].transform('mean')\n    df_struct['c_z']=df_struct.groupby('molecule_name')['z'].transform('mean')\n    df_struct['atom_n']=df_struct.groupby('molecule_name')['atom_index'].transform('max')\n\n","e5a1f9fc":"def make_features(df):\n    df['dx']=df['x_1']-df['x_0']\n    df['dy']=df['y_1']-df['y_0']\n    df['dz']=df['z_1']-df['z_0']\n    df['distance']=(df['dx']**2+df['dy']**2+df['dz']**2)**(1\/2)\n    return df\ndf_train=make_features(df_train)\ndf_test=make_features(df_test) ","b2819abf":"Cosine angles also works.\nhttps:\/\/www.kaggle.com\/kmat2019\/effective-feature","42edd5f1":"def get_closest_farthest(df):\n    df_temp=df.loc[:,[\"molecule_name\",\"atom_index_0\",\"atom_index_1\",\"distance\",\"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\"]].copy()\n    df_temp_=df_temp.copy()\n    df_temp_= df_temp_.rename(columns={'atom_index_0': 'atom_index_1',\n                                       'atom_index_1': 'atom_index_0',\n                                       'x_0': 'x_1',\n                                       'y_0': 'y_1',\n                                       'z_0': 'z_1',\n                                       'x_1': 'x_0',\n                                       'y_1': 'y_0',\n                                       'z_1': 'z_0'})\n    df_temp_all=pd.concat((df_temp,df_temp_),axis=0)\n\n    df_temp_all[\"min_distance\"]=df_temp_all.groupby(['molecule_name', 'atom_index_0'])['distance'].transform('min')\n    df_temp_all[\"max_distance\"]=df_temp_all.groupby(['molecule_name', 'atom_index_0'])['distance'].transform('max')\n    \n    df_temp= df_temp_all[df_temp_all[\"min_distance\"]==df_temp_all[\"distance\"]].copy()\n    df_temp=df_temp.drop(['x_0','y_0','z_0','min_distance'], axis=1)\n    df_temp= df_temp.rename(columns={'atom_index_0': 'atom_index',\n                                         'atom_index_1': 'atom_index_closest',\n                                         'distance': 'distance_closest',\n                                         'x_1': 'x_closest',\n                                         'y_1': 'y_closest',\n                                         'z_1': 'z_closest'})\n\n    for atom_idx in [0,1]:\n        df = map_atom_info(df,df_temp, atom_idx)\n        df = df.rename(columns={'atom_index_closest': f'atom_index_closest_{atom_idx}',\n                                        'distance_closest': f'distance_closest_{atom_idx}',\n                                        'x_closest': f'x_closest_{atom_idx}',\n                                        'y_closest': f'y_closest_{atom_idx}',\n                                        'z_closest': f'z_closest_{atom_idx}'})\n\n    df_temp= df_temp_all[df_temp_all[\"max_distance\"]==df_temp_all[\"distance\"]].copy()\n    df_temp=df_temp.drop(['x_0','y_0','z_0','max_distance'], axis=1)\n    df_temp= df_temp.rename(columns={'atom_index_0': 'atom_index',\n                                         'atom_index_1': 'atom_index_farthest',\n                                         'distance': 'distance_farthest',\n                                         'x_1': 'x_farthest',\n                                         'y_1': 'y_farthest',\n                                         'z_1': 'z_farthest'})\n\n    for atom_idx in [0,1]:\n        df = map_atom_info(df,df_temp, atom_idx)\n        df = df.rename(columns={'atom_index_farthest': f'atom_index_farthest_{atom_idx}',\n                                        'distance_farthest': f'distance_farthest_{atom_idx}',\n                                        'x_farthest': f'x_farthest_{atom_idx}',\n                                        'y_farthest': f'y_farthest_{atom_idx}',\n                                        'z_farthest': f'z_farthest_{atom_idx}'})\n    return df\n    \ndf_train=get_closest_farthest(df_train)    \ndf_test=get_closest_farthest(df_test)    ","f6772b6d":"def add_cos_features(df):\n    df[\"distance_center0\"]=((df['x_0']-df['c_x'])**2+(df['y_0']-df['c_y'])**2+(df['z_0']-df['c_z'])**2)**(1\/2)\n    df[\"distance_center1\"]=((df['x_0']-df['c_x'])**2+(df['y_0']-df['c_y'])**2+(df['z_0']-df['c_z'])**2)**(1\/2)\n    df[\"distance_c0\"]=((df['x_0']-df['x_closest_0'])**2+(df['y_0']-df['y_closest_0'])**2+(df['z_0']-df['z_closest_0'])**2)**(1\/2)\n    df[\"distance_c1\"]=((df['x_1']-df['x_closest_1'])**2+(df['y_1']-df['y_closest_1'])**2+(df['z_1']-df['z_closest_1'])**2)**(1\/2)\n    df[\"distance_f0\"]=((df['x_0']-df['x_farthest_0'])**2+(df['y_0']-df['y_farthest_0'])**2+(df['z_0']-df['z_farthest_0'])**2)**(1\/2)\n    df[\"distance_f1\"]=((df['x_1']-df['x_farthest_1'])**2+(df['y_1']-df['y_farthest_1'])**2+(df['z_1']-df['z_farthest_1'])**2)**(1\/2)\n    df[\"vec_center0_x\"]=(df['x_0']-df['c_x'])\/(df[\"distance_center0\"]+1e-10)\n    df[\"vec_center0_y\"]=(df['y_0']-df['c_y'])\/(df[\"distance_center0\"]+1e-10)\n    df[\"vec_center0_z\"]=(df['z_0']-df['c_z'])\/(df[\"distance_center0\"]+1e-10)\n    df[\"vec_center1_x\"]=(df['x_1']-df['c_x'])\/(df[\"distance_center1\"]+1e-10)\n    df[\"vec_center1_y\"]=(df['y_1']-df['c_y'])\/(df[\"distance_center1\"]+1e-10)\n    df[\"vec_center1_z\"]=(df['z_1']-df['c_z'])\/(df[\"distance_center1\"]+1e-10)\n    df[\"vec_c0_x\"]=(df['x_0']-df['x_closest_0'])\/(df[\"distance_c0\"]+1e-10)\n    df[\"vec_c0_y\"]=(df['y_0']-df['y_closest_0'])\/(df[\"distance_c0\"]+1e-10)\n    df[\"vec_c0_z\"]=(df['z_0']-df['z_closest_0'])\/(df[\"distance_c0\"]+1e-10)\n    df[\"vec_c1_x\"]=(df['x_1']-df['x_closest_1'])\/(df[\"distance_c1\"]+1e-10)\n    df[\"vec_c1_y\"]=(df['y_1']-df['y_closest_1'])\/(df[\"distance_c1\"]+1e-10)\n    df[\"vec_c1_z\"]=(df['z_1']-df['z_closest_1'])\/(df[\"distance_c1\"]+1e-10)\n    df[\"vec_f0_x\"]=(df['x_0']-df['x_farthest_0'])\/(df[\"distance_f0\"]+1e-10)\n    df[\"vec_f0_y\"]=(df['y_0']-df['y_farthest_0'])\/(df[\"distance_f0\"]+1e-10)\n    df[\"vec_f0_z\"]=(df['z_0']-df['z_farthest_0'])\/(df[\"distance_f0\"]+1e-10)\n    df[\"vec_f1_x\"]=(df['x_1']-df['x_farthest_1'])\/(df[\"distance_f1\"]+1e-10)\n    df[\"vec_f1_y\"]=(df['y_1']-df['y_farthest_1'])\/(df[\"distance_f1\"]+1e-10)\n    df[\"vec_f1_z\"]=(df['z_1']-df['z_farthest_1'])\/(df[\"distance_f1\"]+1e-10)\n    df[\"vec_x\"]=(df['x_1']-df['x_0'])\/df[\"distance\"]\n    df[\"vec_y\"]=(df['y_1']-df['y_0'])\/df[\"distance\"]\n    df[\"vec_z\"]=(df['z_1']-df['z_0'])\/df[\"distance\"]\n    df[\"cos_c0_c1\"]=df[\"vec_c0_x\"]*df[\"vec_c1_x\"]+df[\"vec_c0_y\"]*df[\"vec_c1_y\"]+df[\"vec_c0_z\"]*df[\"vec_c1_z\"]\n    df[\"cos_f0_f1\"]=df[\"vec_f0_x\"]*df[\"vec_f1_x\"]+df[\"vec_f0_y\"]*df[\"vec_f1_y\"]+df[\"vec_f0_z\"]*df[\"vec_f1_z\"]\n    df[\"cos_center0_center1\"]=df[\"vec_center0_x\"]*df[\"vec_center1_x\"]+df[\"vec_center0_y\"]*df[\"vec_center1_y\"]+df[\"vec_center0_z\"]*df[\"vec_center1_z\"]\n    df[\"cos_c0\"]=df[\"vec_c0_x\"]*df[\"vec_x\"]+df[\"vec_c0_y\"]*df[\"vec_y\"]+df[\"vec_c0_z\"]*df[\"vec_z\"]\n    df[\"cos_c1\"]=df[\"vec_c1_x\"]*df[\"vec_x\"]+df[\"vec_c1_y\"]*df[\"vec_y\"]+df[\"vec_c1_z\"]*df[\"vec_z\"]\n    df[\"cos_f0\"]=df[\"vec_f0_x\"]*df[\"vec_x\"]+df[\"vec_f0_y\"]*df[\"vec_y\"]+df[\"vec_f0_z\"]*df[\"vec_z\"]\n    df[\"cos_f1\"]=df[\"vec_f1_x\"]*df[\"vec_x\"]+df[\"vec_f1_y\"]*df[\"vec_y\"]+df[\"vec_f1_z\"]*df[\"vec_z\"]\n    df[\"cos_center0\"]=df[\"vec_center0_x\"]*df[\"vec_x\"]+df[\"vec_center0_y\"]*df[\"vec_y\"]+df[\"vec_center0_z\"]*df[\"vec_z\"]\n    df[\"cos_center1\"]=df[\"vec_center1_x\"]*df[\"vec_x\"]+df[\"vec_center1_y\"]*df[\"vec_y\"]+df[\"vec_center1_z\"]*df[\"vec_z\"]\n    df=df.drop(['vec_c0_x','vec_c0_y','vec_c0_z','vec_c1_x','vec_c1_y','vec_c1_z',\n                'vec_f0_x','vec_f0_y','vec_f0_z','vec_f1_x','vec_f1_y','vec_f1_z',\n                'vec_center0_x','vec_center0_y','vec_center0_z','vec_center1_x','vec_center1_y','vec_center1_z',\n                'vec_x','vec_y','vec_z'], axis=1)\n    return df\n    \ndf_train=add_cos_features(df_train)\ndf_test=add_cos_features(df_test)","33abb882":"df_train.head()","ea903b05":"def create_nn_model(input_shape):\n    inp = Input(shape=(input_shape,))\n    x = Dense(256, activation=\"relu\")(inp)\n    x = BatchNormalization()(x)\n    x = Dense(256, activation=\"relu\")(x)\n    x = BatchNormalization()(x)\n    x = Dense(256, activation=\"relu\")(x)\n    x = BatchNormalization()(x)\n    x = Dense(128, activation=\"relu\")(x)\n    x = BatchNormalization()(x)\n    out1 = Dense(2, activation=\"linear\")(x)#mulliken charge 2\n    out2 = Dense(6, activation=\"linear\")(x)#tensor 6(xx,yy,zz)\n    out3 = Dense(12, activation=\"linear\")(x)#tensor 12(others) \n    x = Dense(64, activation=\"relu\")(x)\n    x = BatchNormalization()(x)\n    x = Dense(64, activation=\"relu\")(x)\n    x = BatchNormalization()(x)\n    out = Dense(1, activation=\"linear\")(x)#scalar_coupling_constant    \n    model = Model(inputs=inp, outputs=[out,out1,out2,out3])\n    return model","3daf1390":"mol_types=df_train[\"type\"].unique()\ncv_score=[]\ncv_score_total=0\ntest_prediction=np.zeros(len(df_test))\n\nfor mol_type in mol_types:\n    df_train_=df_train[df_train[\"type\"]==mol_type]\n    df_test_=df_test[df_test[\"type\"]==mol_type]\n    input_features=[\"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\",\"c_x\",\"c_y\",\"c_z\",\n                    'x_closest_0','y_closest_0','z_closest_0','x_closest_1','y_closest_1','z_closest_1',\n                    \"distance\",\"distance_center0\",\"distance_center1\",\"distance_c0\",\"distance_c1\",\"distance_f0\",\"distance_f1\",\n                    \"cos_c0_c1\",\"cos_f0_f1\",\"cos_center0_center1\",\"cos_c0\",\"cos_c1\",\"cos_f0\",\"cos_f1\",\"cos_center0\",\"cos_center1\",\n                    \"atom_n\"\n                   ]\n    input_data=StandardScaler().fit_transform(pd.concat([df_train_.loc[:,input_features],df_test_.loc[:,input_features]]))\n    target_data=df_train_.loc[:,\"scalar_coupling_constant\"].values\n    target_data_1=df_train_.loc[:,[\"charge_0\",\"charge_1\"]]\n    target_data_2=df_train_.loc[:,[\"XX_0\",\"YY_0\",\"ZZ_0\",\"XX_1\",\"YY_1\",\"ZZ_1\"]]\n    target_data_3=df_train_.loc[:,[\"YX_0\",\"ZX_0\",\"XY_0\",\"ZY_0\",\"XZ_0\",\"YZ_0\",\"YX_1\",\"ZX_1\",\"XY_1\",\"ZY_1\",\"XZ_1\",\"YZ_1\"]]\n    \n    #following parameters should be adjusted to control the loss function\n    #if all parameters are zero, attractors do not work. (-> simple neural network)\n    m1=1\n    m2=4\n    m3=1\n    target_data_1=m1*(StandardScaler().fit_transform(target_data_1))\n    target_data_2=m2*(StandardScaler().fit_transform(target_data_2))\n    target_data_3=m3*(StandardScaler().fit_transform(target_data_3))\n    \n    train_index, cv_index = train_test_split(np.arange(len(df_train_)),random_state=111, test_size=0.2)\n    \n    train_input=input_data[train_index]\n    cv_input=input_data[cv_index]\n    train_target=target_data[train_index]\n    cv_target=target_data[cv_index]\n    train_target_1=target_data_1[train_index]\n    cv_target_1=target_data_1[cv_index]\n    train_target_2=target_data_2[train_index]\n    cv_target_2=target_data_2[cv_index]\n    train_target_3=target_data_3[train_index]\n    cv_target_3=target_data_3[cv_index]\n    test_input=input_data[len(df_train_):,:]\n\n    epoch_n=12#as trial. Longer epoch improve the score\n    verbose=1\n    nn_model=create_nn_model(train_input.shape[1])\n    nn_model.compile(loss='mae', optimizer=Adam(lr=1e-3))#, metrics=[auc])\n    es = callbacks.EarlyStopping(monitor='val_loss', min_delta=0., patience=12,#val_auc\n                                 verbose=verbose, mode='min', baseline=None, restore_best_weights=True)\n    rlr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3,\n                                      patience=4, min_lr=2e-6, mode='min', verbose=verbose)\n    nn_model.fit(train_input,[train_target,train_target_1,train_target_2,train_target_3], \n            validation_data=(cv_input,[cv_target,cv_target_1,cv_target_2,cv_target_3]), \n            callbacks=[es, rlr], epochs=epoch_n, batch_size=64, verbose=verbose)\n    cv_predict=nn_model.predict(cv_input)\n    \n    accuracy=np.mean(np.abs(cv_target-cv_predict[0][:,0]))\n    cv_score.append(np.log(accuracy))\n    cv_score_total+=np.log(accuracy)\n    #print(mol_type,\": log score is \",np.log(accuracy))    \n    test_predict=nn_model.predict(test_input)\n    test_prediction[df_test[\"type\"]==mol_type]=test_predict[0][:,0]\n    K.clear_session()\n\ncv_score_total\/=len(mol_types)\n#print(\"total score is\",cv_score_total)","a031f516":"i=0\nfor mol_type in mol_types: \n    print(mol_type,\": cv score is \",cv_score[i])\n    i+=1\nprint(\"total cv score is\",cv_score_total)","7fb3d9b2":"def submit(predictions):\n    submit = pd.read_csv('..\/input\/sample_submission.csv')\n    submit[\"scalar_coupling_constant\"] = predictions\n    submit.to_csv(\"submission.csv\", index=False)\n\nsubmit(test_prediction)","4759f5f1":"mol_types=df_train[\"type\"].unique()\ncv_score=[]\ncv_score_total=0\n#test_prediction=np.zeros(len(df_test))\n\nfor mol_type in mol_types:\n    df_train_=df_train[df_train[\"type\"]==mol_type]\n    df_test_=df_test[df_test[\"type\"]==mol_type]\n    input_features=[\"x_0\",\"y_0\",\"z_0\",\"x_1\",\"y_1\",\"z_1\",\"c_x\",\"c_y\",\"c_z\",\n                    'x_closest_0','y_closest_0','z_closest_0','x_closest_1','y_closest_1','z_closest_1',\n                    \"distance\",\"distance_center0\",\"distance_center1\",\"distance_c0\",\"distance_c1\",\"distance_f0\",\"distance_f1\",\n                    \"cos_c0_c1\",\"cos_f0_f1\",\"cos_center0_center1\",\"cos_c0\",\"cos_c1\",\"cos_f0\",\"cos_f1\",\"cos_center0\",\"cos_center1\",\n                    \"atom_n\"\n                   ]\n    input_data=StandardScaler().fit_transform(pd.concat([df_train_.loc[:,input_features],df_test_.loc[:,input_features]]))\n    target_data=df_train_.loc[:,\"scalar_coupling_constant\"].values\n    target_data_1=df_train_.loc[:,[\"charge_0\",\"charge_1\"]]\n    target_data_2=df_train_.loc[:,[\"XX_0\",\"YY_0\",\"ZZ_0\",\"XX_1\",\"YY_1\",\"ZZ_1\"]]\n    target_data_3=df_train_.loc[:,[\"YX_0\",\"ZX_0\",\"XY_0\",\"ZY_0\",\"XZ_0\",\"YZ_0\",\"YX_1\",\"ZX_1\",\"XY_1\",\"ZY_1\",\"XZ_1\",\"YZ_1\"]]\n    \n    #following parameters should be adjusted to control the loss function\n    #if all parameters are zero, attractors do not work. (-> simple neural network)\n    m1=0\n    m2=0\n    m3=0\n    target_data_1=m1*(StandardScaler().fit_transform(target_data_1))\n    target_data_2=m2*(StandardScaler().fit_transform(target_data_2))\n    target_data_3=m3*(StandardScaler().fit_transform(target_data_3))\n    \n    train_index, cv_index = train_test_split(np.arange(len(df_train_)),random_state=111, test_size=0.2)\n    \n    train_input=input_data[train_index]\n    cv_input=input_data[cv_index]\n    train_target=target_data[train_index]\n    cv_target=target_data[cv_index]\n    train_target_1=target_data_1[train_index]\n    cv_target_1=target_data_1[cv_index]\n    train_target_2=target_data_2[train_index]\n    cv_target_2=target_data_2[cv_index]\n    train_target_3=target_data_3[train_index]\n    cv_target_3=target_data_3[cv_index]\n    test_input=input_data[len(df_train_):,:]\n\n    epoch_n=12#for trial. Longer epoch improve the score\n    verbose=1\n    nn_model=create_nn_model(train_input.shape[1])\n    nn_model.compile(loss='mae', optimizer=Adam(lr=1e-3))#, metrics=[auc])\n    es = callbacks.EarlyStopping(monitor='val_loss', min_delta=0., patience=12,#val_auc\n                                 verbose=verbose, mode='min', baseline=None, restore_best_weights=True)\n    rlr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.3,\n                                      patience=4, min_lr=2e-6, mode='min', verbose=verbose)\n    nn_model.fit(train_input,[train_target,train_target_1,train_target_2,train_target_3], \n            validation_data=(cv_input,[cv_target,cv_target_1,cv_target_2,cv_target_3]), \n            callbacks=[es, rlr], epochs=epoch_n, batch_size=64, verbose=verbose)\n    cv_predict=nn_model.predict(cv_input)\n    \n    accuracy=np.mean(np.abs(cv_target-cv_predict[0][:,0]))\n    cv_score.append(np.log(accuracy))\n    cv_score_total+=np.log(accuracy)\n    #print(mol_type,\": log score is \",np.log(accuracy))    \n    #test_predict=nn_model.predict(test_input)\n    #test_prediction[df_test[\"type\"]==mol_type]=test_predict[0][:,0]\n    K.clear_session()\n\ncv_score_total\/=len(mol_types)\n#print(\"total score is\",cv_score_total)","3ff74cf8":"i=0\nfor mol_type in mol_types: \n    print(mol_type,\": cv score is \",cv_score[i])\n    i+=1\nprint(\"total cv score is\",cv_score_total)","80849d69":"## Create Neural Network Model\nI use the additional data (mulliken charge and tensor) as the output of the middle of neural network model. I am too lazy to make the pipeline model. I hope this works as the kind of attractors.*(I don't know how to call this, so let it call as attractors in this kernel)*","479ad3f8":"I use this great kernel. \nhttps:\/\/www.kaggle.com\/seriousran\/just-speed-up-calculate-distance-from-benchmark","bab85c70":"If you need higher score, you can increase the number of epochs and adjust the hyper parameters.","2b45dc38":"## Import libraries","6e8064a8":"## Load files\nload the additional data as well.","cfaba69f":"## Check the effect of middle outputs\nNow, let's compare with the model without attractors by changing parameter m1,m2,m3 to zero.","48ebd423":"This is my first kernel and I am a beginner of both data science and programming. Any comments and advices are welcome!","92553571":"Distance is very effective feature.","6e864692":"I guess most people prefer gradient boosting models to neural networks in this conpetition. But I don't have enough domain knowledge, I relied on the neural network model first. In my model, the additional data(mulliken charge and tensors) are used as the output of the middle of neural network model. It slightly improved the score.","3352b2e5":"create neural networks for each molecule type. (calculate 8 models in total) ","5bb1b8cb":"## Feature Engineering\nLet's merge all data and create features. The additional features are only used for training session.It will come up later.","f92eac31":"The score with attractors is slightly better than without attractors."}}