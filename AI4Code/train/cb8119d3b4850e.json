{"cell_type":{"83bbebdb":"code","076230af":"code","3f1fd4af":"code","93374643":"code","f8f255dd":"code","ca161dd7":"code","dc53872f":"code","f1b67915":"code","738c5ed4":"code","a7e5edbf":"code","b20aa538":"code","d96394a7":"code","8ad887e1":"code","6d47684d":"code","4c1a292b":"code","ba643753":"code","16cc021a":"code","7b97a013":"code","bceeeff0":"markdown","a6c41da7":"markdown","7af70fe7":"markdown","3a9a784d":"markdown","4c875132":"markdown","44e70070":"markdown","33b0499c":"markdown","0970f478":"markdown","cd84b6f2":"markdown"},"source":{"83bbebdb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","076230af":"df = pd.read_csv('\/kaggle\/input\/random-tweets-pakistan\/Random Tweets from Pakistan- Cleaned.csv')\ndf.head()","3f1fd4af":"import regex as re\ndf['full_text']=df['full_text'].apply(str)\ndef cleantxt(text):\n    text = re.sub(r'@[A-Za-z0-9]+', '',text)\n    text = re.sub(r'#', '',text)\n    text = re.sub(r'RT[\\s]+', '',text)\n    text = re.sub(r'https?:\\\/\\\/\\S+', '',text)\n    \n    return text\n\ndf['full_text'] = df['full_text'].apply(cleantxt)\n\ndf['full_text']","93374643":"df.describe(include='all')","f8f255dd":"df['full_text'].describe()","ca161dd7":"text = df['full_text']\ntarget = df['location']\n\ntest_text = df['full_text']\n\n# Print random samples from the training text \nfor i in np.random.randint(500, size=5):\n    print(f'Tweet #{i}: ', text[i], '=> Target: ', target[i], end='\\n' * 2)","dc53872f":"df.isnull().sum()","f1b67915":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.subplot(221)\n\ndf['location'].value_counts().plot(kind='bar', title='Tweet Location', figsize=(16,9))\nplt.xticks(rotation=0)","738c5ed4":"df.boxplot()","a7e5edbf":"import nltk                             \nfrom nltk.corpus import twitter_samples   \nimport matplotlib.pyplot as plt           \nimport random \nnltk.download('twitter_samples')","b20aa538":"all_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')\n\nprint('Number of positive tweets: ', len(all_positive_tweets))\nprint('Number of negative tweets: ', len(all_negative_tweets))\n\nprint('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\nprint('The type of a tweet entry is: ', type(all_negative_tweets[0]))","d96394a7":"total_positive_words = []\nfor sentence in all_positive_tweets:\n    total_positive_words.append(sentence.count(' '))\n    \ntotal_negative_words = []\nfor sentence in all_negative_tweets:\n    total_negative_words.append(sentence.count(' '))\n    \nimport plotly.graph_objects as go\nimport numpy as np\n\nx0 = np.array(total_positive_words)\nx1 = np.array(total_negative_words)\n\nfig = go.Figure()\nfig.add_trace(go.Histogram(x=x1, name = 'Negative'))\nfig.add_trace(go.Histogram(x=x0, name = 'Positive'))\n\n# Overlay both histograms\nfig.update_layout(barmode='overlay')\n# Reduce opacity to see both histograms\nfig.update_traces(opacity=0.75)\nfig.show()","8ad887e1":"tweet = all_positive_tweets[1455]","6d47684d":"nltk.download('stopwords')\n\nimport re                                  \nimport string                             \nfrom nltk.corpus import stopwords \nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer ","4c1a292b":"print('Original Tweet: ')\nprint(tweet)\n\n# it will remove the old style retweet text \"RT\"\ntweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n\n# it will remove hyperlinks\ntweet2 = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet2)\n\n# it will remove hashtags. We have to be careful here not to remove \n# the whole hashtag because text of hashtags contains huge information. \n# only removing the hash # sign from the word\ntweet2 = re.sub(r'#', '', tweet2)\n\n# it will remove single numeric terms in the tweet. \ntweet2 = re.sub(r'[0-9]', '', tweet2)\nprint('\\nAfter removing old style tweet, hyperlinks and # sign')\nprint(tweet2)","ba643753":"import nltk\nfrom nltk.stem.porter import PorterStemmer\nporter_stemmer  = PorterStemmer()\ntext = \"Hello World, You are Greater\"\ntokenization = nltk.word_tokenize(text)\nfor w in tokenization:\n    print(\"Stemming for {} is {}\".format(w,porter_stemmer.stem(w)))","16cc021a":"import nltk\nfrom nltk.stem import \tWordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\ntext = \"Horse horses faster\"\ntokenization = nltk.word_tokenize(text)\nfor w in tokenization:\n\tprint(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))  ","7b97a013":"print('Before Tokenizing: ')\nprint(tweet2)\n\n# instantiate the tokenizer class\ntokenizer = TweetTokenizer(preserve_case=False, \n                           strip_handles=True,\n                           reduce_len=True)\n\n# tokenize the tweets\ntweet_tokens = tokenizer.tokenize(tweet2)\n\nprint('\\nTokenized string:')\nprint(tweet_tokens)\n","bceeeff0":"Stemming","a6c41da7":"Data Cleaning","7af70fe7":"Stop Words Downloading","3a9a784d":"Lematization","4c875132":"Remove Hyperlinks and styles","44e70070":"String Tokenization","33b0499c":"Positive and Negative DataSet Below Segregation","0970f478":"NLP Pre Processing","cd84b6f2":"EDA"}}