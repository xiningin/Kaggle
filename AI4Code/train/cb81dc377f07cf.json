{"cell_type":{"199211d0":"code","4ae8a85b":"code","f6b57a72":"code","24f06580":"code","fbef409f":"code","9b36eb39":"code","bdbce025":"code","d1e32874":"code","4c8f26ea":"code","3c517488":"code","59169a46":"code","45bb8606":"code","5a3ca9ce":"code","3447978c":"code","70124d5d":"markdown","8e1cf48e":"markdown","79519617":"markdown","348ad679":"markdown","a3460f9e":"markdown","5f75b17c":"markdown","39b93a27":"markdown","33f8f5df":"markdown","a85993a0":"markdown","5563d647":"markdown","8423160d":"markdown","fc34e0c6":"markdown","c287a5a3":"markdown","f84d7ad1":"markdown","286e56b3":"markdown","f774ef92":"markdown"},"source":{"199211d0":"## importing packages\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error,roc_auc_score\nfrom google.cloud import bigquery\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom datetime import date\nfrom datetime import timedelta\nimport gc\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","4ae8a85b":"def MyLabelEncodeSingle(col):\n    levels=col.unique().tolist()\n    for l in levels:\n        if l is np.nan:\n            levels.remove(np.nan)\n    levelmap={e:i for i,e in enumerate(levels)}\n    return col.map(levelmap)","f6b57a72":"#https:\/\/www.kaggle.com\/jasonbenner\/world-bank-datasets#World_Happiness_Index.csv\nworld_happiness = pd.read_csv(\"..\/input\/world-bank-datasets\/World_Happiness_Index.csv\")\nworld_happiness=world_happiness.iloc[:,:19]\nworld_happiness.columns=[c.replace('(','').replace(')','').replace('(','').replace(',','').replace('-','_').replace('\/','_').replace(' ','_') \n                               for c in world_happiness.columns]\naverage_year={}\ntemp_matrix=world_happiness.iloc[:,2:]\nfor y in world_happiness.Year.unique():\n    average_year[y]=temp_matrix.loc[world_happiness.Year==y,:].mean()\ndel temp_matrix\ngc.collect()\ndistance=0\nwhile world_happiness.isna().sum().sum()!=0:\n    for y in world_happiness.Year.unique():\n        yhat=y-distance\n        if yhat>2018:\n            yhat=2018\n        elif yhat<2005:\n            yhat=2005\n        for c in world_happiness.columns[2:]:\n            world_happiness.loc[world_happiness.Year==y,c]=world_happiness.loc[world_happiness.Year==y,c].fillna(average_year[yhat][c])\n        yhat=y+distance\n        if yhat>2018:\n            yhat=2018\n        elif yhat<2005:\n            yhat=2005\n        for c in world_happiness.columns[2:]:\n            world_happiness.loc[world_happiness.Year==y,c]=world_happiness.loc[world_happiness.Year==y,c].fillna(average_year[yhat][c])\n        distance += 1\nworld_happiness_latest = world_happiness.groupby('Country_name').nth(-1)\nworld_happiness_first = world_happiness.groupby('Country_name').agg('first')\nworld_happiness_last = world_happiness.groupby('Country_name').agg('last')\nworld_happiness_count = world_happiness.groupby('Country_name').count()\nworld_happiness_range=(world_happiness_last-world_happiness_first)\/world_happiness_count\nworld_happiness_range.drop(\"Year\", axis=1, inplace=True)\nworld_happiness_latest.drop(\"Year\", axis=1, inplace=True)\nworld_happiness_range.columns=[c+'_range' for c in world_happiness_range.columns]\nworld_happiness_latest.columns=[c+'_latest' for c in world_happiness_latest.columns]\nworld_happiness_grouped=pd.concat((world_happiness_latest,world_happiness_range),axis=1).reset_index()\nworld_happiness_grouped[\"geography\"] = world_happiness_grouped.Country_name.astype(str)\nworld_happiness_grouped.drop(\"Country_name\", axis=1, inplace=True)\n#X_train = pd.merge(left=X_train, right=world_happiness_grouped, how='left', left_on='Country_Region', right_on='Country_name')\n#X_test = pd.merge(left=X_test, right=world_happiness_grouped, how='left', left_on='Country_Region', right_on='Country_name')\n#X_train.drop(\"Country_name\", axis=1, inplace=True)\n#X_test.drop(\"Country_name\", axis=1, inplace=True)\nmalaria_world_health = pd.read_csv(\"..\/input\/world-bank-datasets\/Malaria_World_Health_Organization.csv\")\nmalaria_world_health.columns=[c.replace(' ','_') for c in malaria_world_health.columns]\nmalaria_world_health[\"geography\"] = malaria_world_health.Country.astype(str)\nmalaria_world_health.drop(\"Country\", axis=1, inplace=True)\n#X_train = pd.merge(left=X_train, right=malaria_world_health, how='left', left_on='Country_Region', right_on='Country')\n#X_test = pd.merge(left=X_test, right=malaria_world_health, how='left', left_on='Country_Region', right_on='Country')\n#X_train.drop(\"Country\", axis=1, inplace=True)\n#X_test.drop(\"Country\", axis=1, inplace=True)\nhuman_development = pd.read_csv(\"..\/input\/world-bank-datasets\/Human_Development_Index.csv\")\nhuman_development.columns=[c.replace(')','').replace('(','').replace(' ','_') for c in human_development.columns]\nhuman_development['Gross_national_income_GNI_per_capita_2018']= human_development['Gross_national_income_GNI_per_capita_2018'].apply(lambda x: x if x!=x else x.replace(',','')).astype(float)\nhuman_development[\"geography\"] = human_development.Country.astype(str)\nhuman_development.drop(\"Country\", axis=1, inplace=True)\n#X_train = pd.merge(left=X_train, right=human_development_index, how='left', left_on='Country_Region', right_on='Country')\n#X_test = pd.merge(left=X_test, right=human_development_index, how='left', left_on='Country_Region', right_on='Country')\n#X_train.drop(\"Country\", axis=1, inplace=True)\n#X_test.drop(\"Country\", axis=1, inplace=True)\n#https:\/\/www.kaggle.com\/nightranger77\/covid19-demographic-predictors\nnight_ranger = pd.read_csv(\"..\/input\/covid19-demographic-predictors\/covid19_by_country.csv\")\nnight_ranger.columns=[c.replace(' ','_') for c in night_ranger.columns]\nnight_ranger = night_ranger[night_ranger.Country != \"Georgia\"]\nnight_ranger=night_ranger[['Country','Median_Age','GDP_2018','Crime_Index','Population_2020','Smoking_2016','Females_2018']]\nnight_ranger[\"geography\"] = night_ranger.Country.astype(str)\nnight_ranger.drop(\"Country\", axis=1, inplace=True)\n#X_train = pd.merge(left=X_train, right=night_ranger_predictors, how='left', left_on='Country_Region', right_on='Country')\n#X_test = pd.merge(left=X_test, right=night_ranger_predictors, how='left', left_on='Country_Region', right_on='Country')\n#X_train.drop(\"Country\", axis=1, inplace=True)\n#X_test.drop(\"Country\", axis=1, inplace=True)\n#https:\/\/www.kaggle.com\/hbfree\/covid19formattedweatherjan22march24\nweather_df = pd.read_csv(\"..\/input\/covid19formattedweatherjan22march24\/covid_dataset.csv\")\nweather_df=weather_df[['Province\/State',\n'Country\/Region',\n'lat',\n'long',\n'day',\n'pop',\n'urbanpop',\n'density',\n'medianage',\n'smokers',\n'health_exp_pc',\n'hospibed',\n'temperature',\n'humidity']]\nweather_df[\"geography\"] = weather_df['Country\/Region'].astype(str) + \": \" + weather_df['Province\/State'].astype(str)\nweather_df.loc[weather_df['Province\/State'].isna(), \"geography\"] = weather_df[weather_df['Province\/State'].isna()]['Country\/Region']\nweather_df.drop(['Country\/Region','Province\/State'], axis=1, inplace=True)\nweather_df=weather_df.replace(-999,np.nan)\n#weather_df['Province\/State']=weather_df['Province\/State'].fillna('Unknown')\nweather_df['day']=pd.to_datetime('2020-01-22')+weather_df['day'].apply(lambda x: timedelta(days=x))\nweather_df['month']=weather_df['day'].dt.month\nweather_df.drop('day',axis=1,inplace=True)\nweather_df=weather_df.groupby(['geography','month']).mean().reset_index()\nweather_df_latest = weather_df.groupby(['geography']).nth(-1).reset_index()\nweather_df_latest['month']=4\nweather_df=pd.concat((weather_df,weather_df_latest),sort=True,axis=0,ignore_index=True)\n\n#X_train = pd.merge(left=X_train, right=weather_df, how='left', left_on=['Country_Region','Province_State','month'], right_on=['Country\/Region','Province\/State','month'])\n#X_test = pd.merge(left=X_test, right=weather_df, how='left', left_on=['Country_Region','Province_State','month'], right_on=['Country\/Region','Province\/State','month'])\n#X_train.drop(['Country\/Region','Province\/State'], axis=1, inplace=True)\n#X_test.drop(['Country\/Region','Province\/State'], axis=1, inplace=True)\n#https:\/\/www.kaggle.com\/londeen\/world-happiness-report-2020\nhappiness_df = pd.read_csv(\"..\/input\/world-happiness-report-2020\/WHR20_DataForFigure2.1.csv\")\nhappiness_df.columns=[c.replace(':','').replace('+','').replace(' ','_') for c in happiness_df.columns]\nhappiness_df['Regional_indicator']=MyLabelEncodeSingle(happiness_df['Regional_indicator'])\nhappiness_df[\"geography\"] = happiness_df.Country_name.astype(str)\nhappiness_df.drop(\"Country_name\", axis=1, inplace=True)\n#X_train = pd.merge(left=X_train, right=happiness_df, how='left', left_on='Country_Region', right_on='Country_name')\n#X_test = pd.merge(left=X_test, right=happiness_df, how='left', left_on='Country_Region', right_on='Country_name')\n#X_train.drop('Country_name', axis=1, inplace=True)\n#X_test.drop('Country_name', axis=1, inplace=True)\n#https:\/\/www.kaggle.com\/alizahidraja\/world-population-by-age-group-2020\nage_df = pd.read_csv(\"..\/input\/world-population-by-age-group-2020\/WorldPopulationByAge2020.csv\")\nage_df['AgeGrp']=MyLabelEncodeSingle(age_df['AgeGrp'])\ndef processAge(df):\n    ageindex=df['AgeGrp']\n    sexsum=df[['PopMale', 'PopFemale', 'PopTotal']].sum()\n    mp=sexsum['PopMale']\/sexsum['PopTotal']\n    fp=sexsum['PopFemale']\/sexsum['PopTotal']\n    p0=df.loc[ageindex==0,'PopTotal'].values[0]\/sexsum['PopTotal']\n    p1=df.loc[ageindex==1,'PopTotal'].values[0]\/sexsum['PopTotal']\n    p2=df.loc[ageindex==2,'PopTotal'].values[0]\/sexsum['PopTotal']\n    p3=df.loc[ageindex==3,'PopTotal'].values[0]\/sexsum['PopTotal']\n    m0=df.loc[ageindex==0,'PopMale'].values[0]\/sexsum['PopMale']\n    m1=df.loc[ageindex==1,'PopMale'].values[0]\/sexsum['PopMale']\n    m2=df.loc[ageindex==2,'PopMale'].values[0]\/sexsum['PopMale']\n    m3=df.loc[ageindex==3,'PopMale'].values[0]\/sexsum['PopMale']\n    f0=df.loc[ageindex==0,'PopFemale'].values[0]\/sexsum['PopFemale']\n    f1=df.loc[ageindex==1,'PopFemale'].values[0]\/sexsum['PopFemale']\n    f2=df.loc[ageindex==2,'PopFemale'].values[0]\/sexsum['PopFemale']\n    f3=df.loc[ageindex==3,'PopFemale'].values[0]\/sexsum['PopFemale']\n    return pd.DataFrame({'MaleP':mp,'MaleP_0':m0,'MaleP_1':m1,'MaleP_2':m2,'MaleP_3':m3,'FemaleP':fp,\n                         'FemaleP_0':f0,'FemaleP_1':f1,'FemaleP_2':f2,'FemaleP_3':f3,'PopTotal':sexsum['PopTotal'],\n                         'Pop_0':p0,'Pop_1':p1,'Pop_2':p2,'Pop_3':p3},index=[0])\nage_df=age_df.groupby('Location').apply(processAge).reset_index().drop('level_1',axis=1)\nage_df[\"geography\"] = age_df.Location.astype(str)\nage_df.drop(\"Location\", axis=1, inplace=True)\n#X_train = pd.merge(left=X_train, right=age_df, how='left', left_on='Country_Region', right_on='Location')\n#X_test = pd.merge(left=X_test, right=age_df, how='left', left_on='Country_Region', right_on='Location')\n#X_train.drop('Location', axis=1, inplace=True)\n#X_test.drop('Location', axis=1, inplace=True)\n#https:\/\/www.kaggle.com\/danevans\/world-bank-wdi-212-health-systems\nhealthsys_df = pd.read_csv(\"..\/input\/world-bank-wdi-212-health-systems\/2.12_Health_systems.csv\")\nhealthsys_df.columns=[c.replace('-','_') for c in healthsys_df.columns]\nhealthsys_df.drop('World_Bank_Name',axis=1,inplace=True)\nnan_country=healthsys_df[['Country_Region', 'Province_State']].isna().all(axis=1)\nhealthsys_df=healthsys_df.loc[nan_country==False,:].reset_index(drop=True)\n#healthsys_df['Province_State']=healthsys_df['Province_State'].fillna('Unknown')\nhealthsys_df[\"geography\"] = healthsys_df['Country_Region'].astype(str) + \": \" + healthsys_df['Province_State'].astype(str)\nhealthsys_df.loc[healthsys_df['Province_State'].isna(), \"geography\"] = healthsys_df[healthsys_df['Province_State'].isna()]['Country_Region']\nhealthsys_df.drop(['Country_Region','Province_State'], axis=1, inplace=True)\n#X_train = pd.merge(left=X_train, right=healthsys_df, how='left', left_on=['Country_Region','Province_State'], right_on=['Country_Region', 'Province_State'])\n#X_test = pd.merge(left=X_test, right=healthsys_df, how='left', left_on=['Country_Region','Province_State'], right_on=['Country_Region', 'Province_State'])\n#https:\/\/www.kaggle.com\/tanuprabhu\/population-by-country-2020\npop_df = pd.read_csv(\"..\/input\/population-by-country-2020\/population_by_country_2020.csv\")\npop_df.columns=[c.replace('.',' ').split(' ')[0]+'_pop2020' for c in pop_df.columns]\npercent_col=['Yearly_pop2020','Urban_pop2020', 'World_pop2020']\ndef depercent(x):\n    if x=='N.A.':\n        return np.nan \n    else:\n        return float(x.replace('%',''))\nfor c in percent_col:\n    pop_df[c]=pop_df[c].apply(lambda x: depercent(x))\npop_df=pop_df.replace('N.A.',np.nan)\npop_df[['Population_pop2020', 'Yearly_pop2020',\n       'Net_pop2020', 'Density_pop2020', 'Land_pop2020', 'Migrants_pop2020',\n       'Fert_pop2020', 'Med_pop2020', 'Urban_pop2020', 'World_pop2020']]=pop_df[['Population_pop2020', 'Yearly_pop2020',\n       'Net_pop2020', 'Density_pop2020', 'Land_pop2020', 'Migrants_pop2020',\n       'Fert_pop2020', 'Med_pop2020', 'Urban_pop2020', 'World_pop2020']].astype(float)\npop_df[\"geography\"] = pop_df.Country_pop2020.astype(str)\npop_df.drop(\"Country_pop2020\", axis=1, inplace=True)\n#X_train = pd.merge(left=X_train, right=pop_df, how='left', left_on='Country_Region', right_on='Country_pop2020')\n#X_test = pd.merge(left=X_test, right=pop_df, how='left', left_on='Country_Region', right_on='Country_pop2020')\n#X_train.drop('Country_pop2020', axis=1, inplace=True)\n#X_test.drop('Country_pop2020', axis=1, inplace=True)\n#https:\/\/www.kaggle.com\/koryto\/countryinfo\ncompre_df = pd.read_csv(\"..\/input\/countryinfo\/covid19countryinfo.csv\")\n#compre_df['region']=compre_df['region'].fillna('Unknown')\nkeepcol=['region', 'country', 'tests',\n       'testpop', 'density', 'medianage', 'urbanpop', 'quarantine', 'schools',\n       'publicplace', 'gatheringlimit', 'gathering', 'nonessential',\n       'hospibed', 'smokers', 'sex0', 'sex14', 'sex25', 'sex54', 'sex64',\n       'sex65plus', 'sexratio', 'lung', 'femalelung', 'malelung', 'gdp2019',\n       'healthexp', 'healthperpop', 'fertility', 'firstcase']\ndef tempfun(x):\n    if x is np.nan:\n        return x\n    else:\n        return float(x.replace(',',''))\nfor c in ['gdp2019','healthexp']:\n    compre_df[c]=compre_df[c].apply(lambda x: tempfun(x) )\n    todate_col=['quarantine', 'schools','publicplace', 'gathering', 'nonessential','firstcase']\nfor c in todate_col:\n    compre_df[c]= (pd.to_datetime(date.today())-pd.to_datetime(compre_df[c])).dt.days.astype(float)\ncompre_df=compre_df[keepcol]\ncompre_df[\"geography\"] = compre_df['country'].astype(str) + \": \" + compre_df['region'].astype(str)\ncompre_df.loc[compre_df['region'].isna(), \"geography\"] = compre_df[compre_df['region'].isna()]['country']\ncompre_df.drop(['country','region'], axis=1, inplace=True)\n#X_train = pd.merge(left=X_train, right=compre_df, how='left', left_on=['Country_Region','Province_State'], right_on=['country','region'])\n#X_test = pd.merge(left=X_test, right=compre_df, how='left', left_on=['Country_Region','Province_State'], right_on=['country','region'])\n#X_train.drop(['country','region'], axis=1, inplace=True)\n#X_test.drop(['country','region'], axis=1, inplace=True)\n#https:\/\/www.kaggle.com\/imdevskp\/sars-outbreak-2003-complete-dataset\nsars_df = pd.read_csv(\"..\/input\/sars-outbreak-2003-complete-dataset\/sars_2003_complete_dataset_clean.csv\")\ndef getProvince(x):\n    x_seg=x.split(',')\n    if len(x_seg)==2:\n        if 'SAR' in x_seg[0]:\n            return x_seg[0][:-4]\n        else:\n            return x_seg[0]\n    else:\n        return 'Unknown'\nsars_df['Province']=sars_df['Country'].apply(lambda x: getProvince(x))\nsars_df['Country']=sars_df['Country'].apply(lambda x: x.split(',')[-1])\nsars_df['Country']=sars_df['Country'].replace('Viet Nam','Vietnam')\ndef getSlope(ses,segs):\n    segsize=np.floor(len(ses)\/segs)\n    slope=[]\n    for i in range(segs):\n        if i==segs-1:\n            slope.append((ses[-1]-ses[int(i*segsize)])\/(len(ses)-1-i*segsize))\n        else:\n            slope.append((ses[int((i+1)*segsize-1)]-ses[int(i*segsize)])\/(segsize-1))\n    return slope   \ndef aggSARS(df):\n    case=df['Cumulative number of case(s)']\n    death=df['Number of deaths'].cumsum()\n    recover=df['Number recovered'].cumsum()\n    Sars_dict={}\n    Sars_dict['CaseMax']=case.max()\n    Sars_dict['DeathMax']=death.max()\n    Sars_dict['RecoverMax']=recover.max()\n    segs=df['Date'].apply(lambda x: x.split('-')[1]).nunique()\n    for i,s in enumerate(getSlope(case.values,segs)):\n        Sars_dict['Case_'+str(i)]=s\n    for i,s in enumerate(getSlope(death.values,segs)):\n        Sars_dict['Death_'+str(i)]=s\n    for i,s in enumerate(getSlope(recover.values,segs)):\n        Sars_dict['Recover_'+str(i)]=s\n    return pd.DataFrame(Sars_dict,index=[0])\nsars_df_grouped=sars_df.groupby(['Country','Province']).apply(aggSARS).reset_index().drop('level_2',axis=1)\nsars_df_grouped[\"geography\"] = sars_df_grouped['Country'].astype(str) + \": \" + sars_df_grouped['Province'].astype(str)\nsars_df_grouped.loc[sars_df_grouped['Province'].isna(), \"geography\"] = sars_df_grouped[sars_df_grouped['Province'].isna()]['Country']\nsars_df_grouped.drop(['Country','Province'], axis=1, inplace=True)\n#X_train = pd.merge(left=X_train, right=sars_df_grouped, how='left', left_on=['Country_Region','Province_State'], right_on=['Country','Province'])\n#X_test = pd.merge(left=X_test, right=sars_df_grouped, how='left', left_on=['Country_Region','Province_State'], right_on=['Country','Province'])\n#X_train.drop(['Country','Province'], axis=1, inplace=True)\n#X_test.drop(['Country','Province'], axis=1, inplace=True)\n\n\n#f_cat=['Country_Region','Province_State','Regional_indicator']\n","24f06580":"## defining constants\nVAL_DAYS = 7\nMAD_FACTOR = 0.5\nDAYS_SINCE_CASES = [1, 10, 50, 100, 500, 1000, 5000, 10000]\n\nSEED = 1990","fbef409f":"## reading data\ntrain = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-3\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/covid19-global-forecasting-week-3\/test.csv')\n#https:\/\/www.kaggle.com\/rohanrao\/covid19-forecasting-metadata\nregion_metadata = pd.read_csv('\/kaggle\/input\/covid19-forecasting-metadata\/region_metadata.csv')\nregion_date_metadata = pd.read_csv('\/kaggle\/input\/covid19-forecasting-metadata\/region_date_metadata.csv')\n","9b36eb39":"## preparing data\ntrain = train.merge(test[[\"ForecastId\", \"Province_State\", \"Country_Region\", \"Date\"]], on = [\"Province_State\", \"Country_Region\", \"Date\"], how = \"left\")\ntest = test[~test.Date.isin(train.Date.unique())]\n\ndf_panel = pd.concat([train, test], sort = False)\n\n# combining state and country into 'geography'\ndf_panel[\"geography\"] = df_panel.Country_Region.astype(str) + \": \" + df_panel.Province_State.astype(str)\ndf_panel.loc[df_panel.Province_State.isna(), \"geography\"] = df_panel[df_panel.Province_State.isna()].Country_Region\n\n# fixing data issues with cummax\ndf_panel.ConfirmedCases = df_panel.groupby(\"geography\")[\"ConfirmedCases\"].cummax()\ndf_panel.Fatalities = df_panel.groupby(\"geography\")[\"Fatalities\"].cummax()\n\n# merging external metadata\ndf_panel = df_panel.merge(region_metadata, on = [\"Country_Region\", \"Province_State\"], how = \"left\")\ndf_panel = df_panel.merge(region_date_metadata, on = [\"Country_Region\", \"Province_State\", \"Date\"], how = \"left\")\n\n# label encoding continent\ndf_panel.continent = MyLabelEncodeSingle(df_panel.continent)\ndf_panel.Date = pd.to_datetime(df_panel.Date, format = \"%Y-%m-%d\")\n\ndf_panel.sort_values([\"geography\", \"Date\"], inplace = True)\n","bdbce025":"## feature engineering\nmin_date_train = np.min(df_panel[~df_panel.Id.isna()].Date)\nmax_date_train = np.max(df_panel[~df_panel.Id.isna()].Date)\n\nmin_date_test = np.min(df_panel[~df_panel.ForecastId.isna()].Date)\nmax_date_test = np.max(df_panel[~df_panel.ForecastId.isna()].Date)\n\nn_dates_test = len(df_panel[~df_panel.ForecastId.isna()].Date.unique())\n\nprint(\"Train date range:\", str(min_date_train), \" - \", str(max_date_train))\nprint(\"Test date range:\", str(min_date_test), \" - \", str(max_date_test))\n\n# creating lag features\nfor lag in range(1, 41):\n    df_panel[f\"lag_{lag}_cc\"] = df_panel.groupby(\"geography\")[\"ConfirmedCases\"].shift(lag)\n    df_panel[f\"lag_{lag}_ft\"] = df_panel.groupby(\"geography\")[\"Fatalities\"].shift(lag)\n    df_panel[f\"lag_{lag}_rc\"] = df_panel.groupby(\"geography\")[\"Recoveries\"].shift(lag)\n\nfor case in DAYS_SINCE_CASES:\n    df_panel = df_panel.merge(df_panel[df_panel.ConfirmedCases >= case].groupby(\"geography\")[\"Date\"].min().reset_index().rename(columns = {\"Date\": f\"case_{case}_date\"}), on = \"geography\", how = \"left\")\n","d1e32874":"#all extra features calculated above\ndef extrafeatures(df):\n    #print('before: {}'.format(len(df)))\n    has_col=df.columns.tolist()\n    df['UpToNow']=(pd.to_datetime(date.today())-pd.to_datetime(df['Date'])).dt.days.astype(float)\n   # print('after UpToNow: {}'.format(len(df)))\n    df = df.merge(world_happiness_grouped, how='left', on='geography')\n    #print('after world_happiness: {}'.format(len(df)))\n    df = df.merge(malaria_world_health, how='left', on='geography')\n   # print('after malaria: {}'.format(len(df)))\n    df = df.merge(human_development, how='left', on='geography')\n   # print('after human: {}'.format(len(df)))\n    df = df.merge(night_ranger, how='left', on='geography')\n    #print('after night: {}'.format(len(df)))\n    df['month']=df['Date'].dt.month\n    df = df.merge(weather_df, how='left', on=['geography','month']).drop('month',axis=1)#month\n    #print('after weather: {}'.format(len(df)))\n    df = df.merge(happiness_df, how='left', on='geography')\n    #print('after happiness: {}'.format(len(df)))\n    df = df.merge(age_df, how='left', on='geography')\n    #print('after age: {}'.format(len(df)))\n    df = df.merge(healthsys_df, how='left', on='geography')\n    #print('after healthsys: {}'.format(len(df)))\n    df = df.merge(pop_df, how='left', on='geography')\n    #print('after pop: {}'.format(len(df)))\n    df = df.merge(compre_df, how='left', on='geography')\n    #print('after compre: {}'.format(len(df)))\n    df = df.merge(sars_df_grouped, how='left', on='geography')\n    #print('after sars: {}'.format(len(df)))\n    extra_col=[c for c in df.columns.tolist() if c not in has_col]\n    return df,extra_col","4c8f26ea":"## function for preparing features\ndef prepare_features(df, gap):\n    \n    df[\"perc_1_ac\"] = (df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap}_rc\"]) \/ df[f\"lag_{gap}_cc\"]\n    df[\"perc_1_cc\"] = df[f\"lag_{gap}_cc\"] \/ df.population\n    \n    df[\"diff_1_cc\"] = df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap + 1}_cc\"]\n    df[\"diff_2_cc\"] = df[f\"lag_{gap + 1}_cc\"] - df[f\"lag_{gap + 2}_cc\"]\n    df[\"diff_3_cc\"] = df[f\"lag_{gap + 2}_cc\"] - df[f\"lag_{gap + 3}_cc\"]\n    \n    df[\"diff_1_ft\"] = df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap + 1}_ft\"]\n    df[\"diff_2_ft\"] = df[f\"lag_{gap + 1}_ft\"] - df[f\"lag_{gap + 2}_ft\"]\n    df[\"diff_3_ft\"] = df[f\"lag_{gap + 2}_ft\"] - df[f\"lag_{gap + 3}_ft\"]\n    \n    df[\"diff_123_cc\"] = (df[f\"lag_{gap}_cc\"] - df[f\"lag_{gap + 3}_cc\"]) \/ 3\n    df[\"diff_123_ft\"] = (df[f\"lag_{gap}_ft\"] - df[f\"lag_{gap + 3}_ft\"]) \/ 3\n\n    df[\"diff_change_1_cc\"] = df.diff_1_cc \/ df.diff_2_cc\n    df[\"diff_change_2_cc\"] = df.diff_2_cc \/ df.diff_3_cc\n    \n    df[\"diff_change_1_ft\"] = df.diff_1_ft \/ df.diff_2_ft\n    df[\"diff_change_2_ft\"] = df.diff_2_ft \/ df.diff_3_ft\n\n    df[\"diff_change_12_cc\"] = (df.diff_change_1_cc + df.diff_change_2_cc) \/ 2\n    df[\"diff_change_12_ft\"] = (df.diff_change_1_ft + df.diff_change_2_ft) \/ 2\n    \n    df[\"change_1_cc\"] = df[f\"lag_{gap}_cc\"] \/ df[f\"lag_{gap + 1}_cc\"]\n    df[\"change_2_cc\"] = df[f\"lag_{gap + 1}_cc\"] \/ df[f\"lag_{gap + 2}_cc\"]\n    df[\"change_3_cc\"] = df[f\"lag_{gap + 2}_cc\"] \/ df[f\"lag_{gap + 3}_cc\"]\n\n    df[\"change_1_ft\"] = df[f\"lag_{gap}_ft\"] \/ df[f\"lag_{gap + 1}_ft\"]\n    df[\"change_2_ft\"] = df[f\"lag_{gap + 1}_ft\"] \/ df[f\"lag_{gap + 2}_ft\"]\n    df[\"change_3_ft\"] = df[f\"lag_{gap + 2}_ft\"] \/ df[f\"lag_{gap + 3}_ft\"]\n\n    df[\"change_123_cc\"] = df[f\"lag_{gap}_cc\"] \/ df[f\"lag_{gap + 3}_cc\"]\n    df[\"change_123_ft\"] = df[f\"lag_{gap}_ft\"] \/ df[f\"lag_{gap + 3}_ft\"]\n    \n    for case in DAYS_SINCE_CASES:\n        df[f\"days_since_{case}_case\"] = (df[f\"case_{case}_date\"] - df.Date).astype(\"timedelta64[D]\")\n        df.loc[df[f\"days_since_{case}_case\"] < gap, f\"days_since_{case}_case\"] = np.nan\n\n    df[\"country_flag\"] = df.Province_State.isna().astype(int)\n    df[\"density\"] = df.population \/ df.area\n    \n    # target variable is log of change from last known value\n    df[\"target_cc\"] = np.log1p(df.ConfirmedCases) - np.log1p(df[f\"lag_{gap}_cc\"])\n    df[\"target_ft\"] = np.log1p(df.Fatalities) - np.log1p(df[f\"lag_{gap}_ft\"])\n    \n    df,extra_col=extrafeatures(df.copy())\n    \n    features = [\n        f\"lag_{gap}_cc\",\n        f\"lag_{gap}_ft\",\n        f\"lag_{gap}_rc\",\n        \"perc_1_ac\",\n        \"perc_1_cc\",\n        \"diff_1_cc\",\n        \"diff_2_cc\",\n        \"diff_3_cc\",\n        \"diff_1_ft\",\n        \"diff_2_ft\",\n        \"diff_3_ft\",\n        \"diff_123_cc\",\n        \"diff_123_ft\",\n        \"diff_change_1_cc\",\n        \"diff_change_2_cc\",\n        \"diff_change_1_ft\",\n        \"diff_change_2_ft\",\n        \"diff_change_12_cc\",\n        \"diff_change_12_ft\",\n        \"change_1_cc\",\n        \"change_2_cc\",\n        \"change_3_cc\",\n        \"change_1_ft\",\n        \"change_2_ft\",\n        \"change_3_ft\",\n        \"change_123_cc\",\n        \"change_123_ft\",\n        \"days_since_1_case\",\n        \"days_since_10_case\",\n        \"days_since_50_case\",\n        \"days_since_100_case\",\n        \"days_since_500_case\",\n        \"days_since_1000_case\",\n        \"days_since_5000_case\",\n        \"days_since_10000_case\",\n        \"country_flag\",\n        #\"lat\",\n        #\"lon\",\n        \"continent\",\n        #\"population\",\n        \"area\",\n        \"density\",\n        \"target_cc\",\n        \"target_ft\"\n    ]+extra_col\n    \n    return df[features]\n","3c517488":"## function for building and predicting using LGBM model\ndef build_predict_lgbm(df_train, df_test, gap):\n    LGB_PARAMS_C = {\"objective\": \"regression\",\n              \"num_leaves\": 7,\n              \"learning_rate\": 0.1,\n              \"bagging_fraction\": 0.91,\n              \"feature_fraction\": 0.81,\n              \"reg_alpha\": 0.05,\n              \"reg_lambda\": 0.13,\n              \"metric\": \"rmse\",\n              \"seed\": SEED\n             }\n    \n    LGB_PARAMS_F = {\"objective\": \"regression\",\n              \"num_leaves\": 9,\n              \"learning_rate\": 0.1,#0.013,\n              \"bagging_fraction\": 0.91,\n              \"feature_fraction\": 0.25,#0.81,\n              #\"min_data_in_leaf\" : 50,\n              \"reg_alpha\": 0.13,\n              \"reg_lambda\": 0.13,\n              \"metric\": \"rmse\",\n              \"seed\": SEED\n             }\n    \n    df_train.dropna(subset = [\"target_cc\", \"target_ft\", f\"lag_{gap}_cc\", f\"lag_{gap}_ft\"], inplace = True)\n    \n    target_cc = df_train.target_cc\n    target_ft = df_train.target_ft\n    \n    test_lag_cc = df_test[f\"lag_{gap}_cc\"].values\n    test_lag_ft = df_test[f\"lag_{gap}_ft\"].values\n    \n    df_train.drop([\"target_cc\", \"target_ft\"], axis = 1, inplace = True)\n    df_test.drop([\"target_cc\", \"target_ft\"], axis = 1, inplace = True)\n    \n    categorical_features = ['continent','Regional_indicator']\n    \n    dtrain_cc = lgb.Dataset(df_train, label = target_cc, categorical_feature = categorical_features)\n    dtrain_ft = lgb.Dataset(df_train, label = target_ft, categorical_feature = categorical_features)\n\n    model_cc = lgb.train(LGB_PARAMS_C, train_set = dtrain_cc, num_boost_round = 200)\n    model_ft = lgb.train(LGB_PARAMS_F, train_set = dtrain_ft, num_boost_round = 1000)\n    \n    # inverse transform from log of change from last known value\n    y_pred_cc = np.expm1(model_cc.predict(df_test, num_boost_round = 200) + np.log1p(test_lag_cc))\n    y_pred_ft = np.expm1(model_ft.predict(df_test, num_boost_round = 1000) + np.log1p(test_lag_ft))\n    \n    return y_pred_cc, y_pred_ft, model_cc, model_ft\n","59169a46":"## building lag x-days models\ndf_train = df_panel[~df_panel.Id.isna()]\ndf_test_full = df_panel[~df_panel.ForecastId.isna()]\n\ndf_preds_val = []\ndf_preds_test = []\n\nfor pdate in df_test_full.Date.unique():\n    \n    print(\"Processing date:\", pdate)\n    \n    # ignore date already present in train data\n    if pdate in df_train.Date.values:\n        df_pred_test = df_test_full.loc[df_test_full.Date == pdate, [\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]].rename(columns = {\"ConfirmedCases\": \"ConfirmedCases_test\", \"Fatalities\": \"Fatalities_test\"})\n        \n        # multiplying predictions by 41 to not look cool on public LB\n        df_pred_test.ConfirmedCases_test = df_pred_test.ConfirmedCases_test * 41\n        df_pred_test.Fatalities_test = df_pred_test.Fatalities_test * 41\n    else:\n        df_test = df_test_full[df_test_full.Date == pdate]\n        \n        gap = (pd.Timestamp(pdate) - max_date_train).days\n        \n        if gap <= VAL_DAYS:\n            val_date = max_date_train - pd.Timedelta(VAL_DAYS, \"D\") + pd.Timedelta(gap, \"D\")\n\n            df_build = df_train[df_train.Date < val_date]\n            df_val = df_train[df_train.Date == val_date]\n            \n            X_build = prepare_features(df_build, gap)\n            X_val = prepare_features(df_val, gap)\n            \n            #print('len of df_val{}, len of X_val{}'.format(len(df_val),len(X_val)) )\n            y_val_cc_lgb, y_val_ft_lgb, _, _ = build_predict_lgbm(X_build, X_val, gap)\n            #y_val_cc_mad, y_val_ft_mad = predict_mad(df_val, gap, val = True)\n            #print('{}_{}_{}'.format(len(df_val.Id.values),len(y_val_cc_lgb),len(y_val_ft_lgb)))\n            df_pred_val = pd.DataFrame({\"Id\": df_val.Id.values,\n                                        \"ConfirmedCases_val_lgb\": y_val_cc_lgb,\n                                        \"Fatalities_val_lgb\": y_val_ft_lgb,\n                                       # \"ConfirmedCases_val_mad\": y_val_cc_mad,\n                                      #  \"Fatalities_val_mad\": y_val_ft_mad,\n                                       })\n\n            df_preds_val.append(df_pred_val)\n\n        X_train = prepare_features(df_train, gap)\n        X_test = prepare_features(df_test, gap)\n\n        y_test_cc_lgb, y_test_ft_lgb, model_cc, model_ft = build_predict_lgbm(X_train, X_test, gap)\n       # y_test_cc_mad, y_test_ft_mad = predict_mad(df_test, gap)\n        \n        if gap == 1:\n            model_1_cc = model_cc\n            model_1_ft = model_ft\n            features_1 = X_train.columns.values\n        elif gap == 14:\n            model_14_cc = model_cc\n            model_14_ft = model_ft\n            features_14 = X_train.columns.values\n        elif gap == 28:\n            model_28_cc = model_cc\n            model_28_ft = model_ft\n            features_28 = X_train.columns.values\n        \n        df_pred_test = pd.DataFrame({\"ForecastId\": df_test.ForecastId.values,\n                                     \"ConfirmedCases_test_lgb\": y_test_cc_lgb,\n                                     \"Fatalities_test_lgb\": y_test_ft_lgb,\n                                   #  \"ConfirmedCases_test_mad\": y_test_cc_mad,\n                                  #   \"Fatalities_test_mad\": y_test_ft_mad,\n                                    })\n    \n    df_preds_test.append(df_pred_test)\nprint(len(X_val.columns))","45bb8606":"## validation score\ndf_panel = df_panel.merge(pd.concat(df_preds_val, sort = False), on = \"Id\", how = \"left\")\ndf_panel = df_panel.merge(pd.concat(df_preds_test, sort = False), on = \"ForecastId\", how = \"left\")\n\nrmsle_cc_lgb = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.ConfirmedCases_val_lgb.isna()].ConfirmedCases), np.log1p(df_panel[~df_panel.ConfirmedCases_val_lgb.isna()].ConfirmedCases_val_lgb)))\nrmsle_ft_lgb = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.Fatalities_val_lgb.isna()].Fatalities), np.log1p(df_panel[~df_panel.Fatalities_val_lgb.isna()].Fatalities_val_lgb)))\n\n#rmsle_cc_mad = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.ConfirmedCases_val_mad.isna()].ConfirmedCases), np.log1p(df_panel[~df_panel.ConfirmedCases_val_mad.isna()].ConfirmedCases_val_mad)))\n#rmsle_ft_mad = np.sqrt(mean_squared_error(np.log1p(df_panel[~df_panel.Fatalities_val_mad.isna()].Fatalities), np.log1p(df_panel[~df_panel.Fatalities_val_mad.isna()].Fatalities_val_mad)))\n\nprint(\"LGB CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_lgb, 2))\nprint(\"LGB FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_lgb, 2))\nprint(\"LGB Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_lgb + rmsle_ft_lgb) \/ 2, 2))\n#print(\"\\n\")\n#print(\"MAD CC RMSLE Val of\", VAL_DAYS, \"days for CC:\", round(rmsle_cc_mad, 2))\n#print(\"MAD FT RMSLE Val of\", VAL_DAYS, \"days for FT:\", round(rmsle_ft_mad, 2))\n#print(\"MAD Overall RMSLE Val of\", VAL_DAYS, \"days:\", round((rmsle_cc_mad + rmsle_ft_mad) \/ 2, 2))\n","5a3ca9ce":"## feature importance\nfrom bokeh.io import output_notebook, show\nfrom bokeh.layouts import column\nfrom bokeh.palettes import Spectral3\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\ndf_fimp_1_cc = pd.DataFrame({\"feature\": features_1, \"importance\": model_1_cc.feature_importance(), \"model\": \"m01\"})\ndf_fimp_14_cc = pd.DataFrame({\"feature\": features_14, \"importance\": model_14_cc.feature_importance(), \"model\": \"m14\"})\ndf_fimp_28_cc = pd.DataFrame({\"feature\": features_28, \"importance\": model_28_cc.feature_importance(), \"model\": \"m28\"})\n\ndf_fimp_1_cc.sort_values(\"importance\", ascending = False, inplace = True)\ndf_fimp_14_cc.sort_values(\"importance\", ascending = False, inplace = True)\ndf_fimp_28_cc.sort_values(\"importance\", ascending = False, inplace = True)\n\nv1 = figure(plot_width = 800, plot_height = 400, x_range = df_fimp_1_cc.feature[:25], title = \"Feature Importance of LGB Model 1\")\nv1.vbar(x = df_fimp_1_cc.feature[:25], top = df_fimp_1_cc.importance[:25], width = 1)\nv1.xaxis.major_label_orientation = 1.3\n\nv14 = figure(plot_width = 800, plot_height = 400, x_range = df_fimp_14_cc.feature[:25], title = \"Feature Importance of LGB Model 14\")\nv14.vbar(x = df_fimp_14_cc.feature[:25], top = df_fimp_14_cc.importance[:25], width = 1)\nv14.xaxis.major_label_orientation = 1.3\n\nv28 = figure(plot_width = 800, plot_height = 400, x_range = df_fimp_28_cc.feature[:25], title = \"Feature Importance of LGB Model 28\")\nv28.vbar(x = df_fimp_28_cc.feature[:25], top = df_fimp_28_cc.importance[:25], width = 1)\nv28.xaxis.major_label_orientation = 1.3\n\nv = column(v1, v14, v28)\n\nshow(v)\n","3447978c":"df_test = df_panel.loc[~df_panel.ForecastId.isna(), [\"ForecastId\", \"Country_Region\", \"Province_State\", \"Date\",\n                                                     \"ConfirmedCases_test\", \"ConfirmedCases_test_lgb\",\n                                                     \"Fatalities_test\", \"Fatalities_test_lgb\"]].reset_index()\ndf_test[\"ConfirmedCases\"] = df_test.ConfirmedCases_test_lgb\ndf_test[\"Fatalities\"] = df_test.Fatalities_test_lgb\ndf_test.loc[df_test.Date.isin(df_train.Date.values), \"ConfirmedCases\"] = df_test[df_test.Date.isin(df_train.Date.values)].ConfirmedCases_test.values\ndf_test.loc[df_test.Date.isin(df_train.Date.values), \"Fatalities\"] = df_test[df_test.Date.isin(df_train.Date.values)].Fatalities_test.values\n\ndf_submission = df_test[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]]\ndf_submission.ForecastId = df_submission.ForecastId.astype(int)\ndf_submission.to_csv('submission.csv', index = False)","70124d5d":"## visualizing Fatalities\nfrom bokeh.models import Panel, Tabs\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\ntab_list = []\n\nfor geography in df_panel.geography.unique():\n    df_geography = df_panel[df_panel.geography == geography]\n    v = figure(plot_width = 800, plot_height = 400, x_axis_type = \"datetime\", title = \"Covid-19 Fatalities over time\")\n    v.line(df_geography.Date, df_geography.Fatalities, color = \"green\", legend_label = \"FT (Train)\")\n    v.line(df_geography.Date, df_geography.Fatalities_val_lgb, color = \"blue\", legend_label = \"FT LGB (Val)\")\n    v.line(df_geography.Date, df_geography.Fatalities_val_mad, color = \"purple\", legend_label = \"FT MAD (Val)\")\n    v.line(df_geography.Date[df_geography.Date > max_date_train], df_geography.Fatalities_test_lgb[df_geography.Date > max_date_train], color = \"red\", legend_label = \"FT LGB (Test)\")\n    v.line(df_geography.Date[df_geography.Date > max_date_train], df_geography.Fatalities_test_mad[df_geography.Date > max_date_train], color = \"orange\", legend_label = \"FT MAD (Test)\")\n    v.legend.location = \"top_left\"\n    tab = Panel(child = v, title = geography)\n    tab_list.append(tab)\n\ntabs = Tabs(tabs=tab_list)\nshow(tabs)\n","8e1cf48e":"## writing final submission and complete output\ndf_submission.to_csv(PATH_SUBMISSION, index = False)\ndf_test.to_csv(PATH_OUTPUT, index = False)\n","79519617":"## External dataset of my choise, data cleaning and feature engineering","348ad679":"### Function to merge my added data to main dataframe","a3460f9e":"## MAD Model","5f75b17c":"## preparing submission file\ndf_test = df_panel.loc[~df_panel.ForecastId.isna(), [\"ForecastId\", \"Country_Region\", \"Province_State\", \"Date\",\n                                                     \"ConfirmedCases_test\", \"ConfirmedCases_test_lgb\", \"ConfirmedCases_test_mad\",\n                                                     \"Fatalities_test\", \"Fatalities_test_lgb\", \"Fatalities_test_mad\"]].reset_index()\n\ndf_test[\"ConfirmedCases\"] = 0.41 * df_test.ConfirmedCases_test_lgb + 0.59 * df_test.ConfirmedCases_test_mad\ndf_test[\"Fatalities\"] = 0.05 * df_test.Fatalities_test_lgb + 0.95 * df_test.Fatalities_test_mad\n\n# Since LGB models don't predict these countries well\ndf_test.loc[df_test.Country_Region.isin([\"China\", \"US\", \"Diamond Princess\"]), \"ConfirmedCases\"] = df_test[df_test.Country_Region.isin([\"China\", \"US\", \"Diamond Princess\"])].ConfirmedCases_test_mad.values\ndf_test.loc[df_test.Country_Region.isin([\"China\", \"US\", \"Diamond Princess\"]), \"Fatalities\"] = df_test[df_test.Country_Region.isin([\"China\", \"US\", \"Diamond Princess\"])].Fatalities_test_mad.values\n\ndf_test.loc[df_test.Date.isin(df_train.Date.values), \"ConfirmedCases\"] = df_test[df_test.Date.isin(df_train.Date.values)].ConfirmedCases_test.values\ndf_test.loc[df_test.Date.isin(df_train.Date.values), \"Fatalities\"] = df_test[df_test.Date.isin(df_train.Date.values)].Fatalities_test.values\n\ndf_submission = df_test[[\"ForecastId\", \"ConfirmedCases\", \"Fatalities\"]]\ndf_submission.ForecastId = df_submission.ForecastId.astype(int)\n\ndf_submission\n","39b93a27":"I borrowed the frame in [Vopani's work in week 2](https:\/\/www.kaggle.com\/rohanrao\/covid-19-w2-lgb-mad) to save some time. He did a excellent work there. I didn't use his model on moving average though.","33f8f5df":"## Covid-19 Week 3 light gbm model\n\nweek 3 of COVID-19 reseasrch using light gbm as base learner.\n\nPlease upvote:) and enjoy\n\n","a85993a0":"## function for predicting moving average decay model\ndef predict_mad(df_test, gap, val = False):\n    \n    df_test[\"avg_diff_cc\"] = (df_test[f\"lag_{gap}_cc\"] - df_test[f\"lag_{gap + 3}_cc\"]) \/ 3\n    df_test[\"avg_diff_ft\"] = (df_test[f\"lag_{gap}_ft\"] - df_test[f\"lag_{gap + 3}_ft\"]) \/ 3\n\n    if val:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test.avg_diff_cc - (1 - MAD_FACTOR) * df_test.avg_diff_cc * np.sum([x for x in range(gap)]) \/ VAL_DAYS\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test.avg_diff_ft - (1 - MAD_FACTOR) * df_test.avg_diff_ft * np.sum([x for x in range(gap)]) \/ VAL_DAYS\n    else:\n        y_pred_cc = df_test[f\"lag_{gap}_cc\"] + gap * df_test.avg_diff_cc - (1 - MAD_FACTOR) * df_test.avg_diff_cc * np.sum([x for x in range(gap)]) \/ n_dates_test\n        y_pred_ft = df_test[f\"lag_{gap}_ft\"] + gap * df_test.avg_diff_ft - (1 - MAD_FACTOR) * df_test.avg_diff_ft * np.sum([x for x in range(gap)]) \/ n_dates_test\n\n    return y_pred_cc, y_pred_ft\n","5563d647":"## Modelling","8423160d":"## visualizing ConfirmedCases\nfrom bokeh.models import Panel, Tabs\nfrom bokeh.io import output_notebook, show\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\ntab_list = []\n\nfor geography in df_panel.geography.unique():\n    df_geography = df_panel[df_panel.geography == geography]\n    v = figure(plot_width = 800, plot_height = 400, x_axis_type = \"datetime\", title = \"Covid-19 ConfirmedCases over time\")\n    v.line(df_geography.Date, df_geography.ConfirmedCases, color = \"green\", legend_label = \"CC (Train)\")\n    v.line(df_geography.Date, df_geography.ConfirmedCases_val_lgb, color = \"blue\", legend_label = \"CC LGB (Val)\")\n    v.line(df_geography.Date, df_geography.ConfirmedCases_val_mad, color = \"purple\", legend_label = \"CC MAD (Val)\")\n    v.line(df_geography.Date[df_geography.Date > max_date_train], df_geography.ConfirmedCases_test_lgb[df_geography.Date > max_date_train], color = \"red\", legend_label = \"CC LGB (Test)\")\n    v.line(df_geography.Date[df_geography.Date > max_date_train], df_geography.ConfirmedCases_test_mad[df_geography.Date > max_date_train], color = \"orange\", legend_label = \"CC MAD (Test)\")\n    v.legend.location = \"top_left\"\n    tab = Panel(child = v, title = geography)\n    tab_list.append(tab)\n\ntabs = Tabs(tabs=tab_list)\nshow(tabs)\n","fc34e0c6":"## Validation","c287a5a3":"## Submission","f84d7ad1":"## LGB Model","286e56b3":"## Visualizing Predictions\n* Viewing the actual, validation and test values together for each geography for ConfirmedCases as well as Fatalities.","f774ef92":"## Customized label encoding that leaves NaN value as NaN"}}