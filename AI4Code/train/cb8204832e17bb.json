{"cell_type":{"0d630d6c":"code","6ccf8551":"code","f4ba5f13":"code","d4449493":"code","4fa39f54":"code","b3a1e6fc":"code","642036ce":"code","6e3f4298":"code","b861d67d":"code","5d8d9b39":"code","a2c902d2":"code","339001d0":"code","4970902d":"code","f78029c5":"code","b9c0dae4":"code","90ded135":"code","fb36d065":"code","04149b16":"code","2b0b3731":"code","33f8a7bc":"code","df19d98d":"code","0b0de1e0":"code","bb4b805a":"code","cc8c507f":"code","f29f15df":"code","e0dabc1b":"code","c0789d3c":"code","795d572c":"code","6e2f2fd7":"code","0c696813":"code","54ead9ff":"code","8c0b6e4f":"code","f1d5353b":"code","54ac91b9":"code","21be983a":"code","04a2c8ea":"code","225ac347":"code","f2f890db":"code","fef682ab":"code","b0f51235":"code","7c27ff2f":"code","cf19c3ad":"code","588ea6c8":"code","0b90ddd0":"code","57a20783":"code","bdaf989a":"code","3d221c1a":"code","178f2b99":"code","afb29e7d":"code","f529c4c3":"code","22957e2d":"code","7dc6422e":"code","898070cd":"code","70090de8":"code","329935d0":"code","61a90678":"code","820e507d":"code","08d9e3d5":"markdown","7c1fe401":"markdown","68a5c710":"markdown","667abeb6":"markdown","20ff124a":"markdown","f8d423f6":"markdown","0d7d9df1":"markdown","86f0b411":"markdown"},"source":{"0d630d6c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import StandardScaler, MaxAbsScaler, Binarizer,FunctionTransformer, OneHotEncoder,LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression , SGDRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error as MSE\nfrom pandas.tools.plotting import scatter_matrix\nfrom sklearn.metrics import f1_score\nfrom sklearn.cluster import KMeans\nfrom sklearn.pipeline import Pipeline, FeatureUnion\nfrom sklearn.mixture import GaussianMixture\nsns.set(style=\"whitegrid\", color_codes=True)\n%matplotlib inline\nimport matplotlib.pyplot as plt \nplt.rcParams[\"figure.figsize\"] = [16, 12]\n\nprint(os.listdir(\"..\/input\"))","6ccf8551":"nRowsRead = None\ndf = pd.read_csv('..\/input\/bagrut-israel\/israel_bagrut_averages.csv', delimiter=',', nrows = nRowsRead)\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')","f4ba5f13":"df = df.dropna()","d4449493":"nRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} columns')","4fa39f54":"# GET CITIES DATA SET\nplaces = pd.read_csv('..\/input\/places\/cities.csv', delimiter=',', nrows = nRowsRead)\nnRow, nCol = places.shape\nprint(f'There are {nRow} rows and {nCol} columns')","b3a1e6fc":"places.head()","642036ce":"print(df['grade'].describe())\nprint(df.grade.plot.hist(figsize=(16,12)))","6e3f4298":"# 1. REMOVE EMPTY VALUES\ndf = df.where(df['grade'] >=0).dropna()\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows\\n')\n\nyear,yNames = len(df['year'].unique()),df['year'].unique()\nprint(f'There are {year} unique years {yNames}\\n')\n\nstudyunits,uNames = len(df['studyunits'].unique()),df['studyunits'].unique()\nprint(f'There are {studyunits} unique study units {uNames}\\n')\n\nciteis_count = df['city'].unique().size\nprint(f'There are {citeis_count} unique cities')\nsubject = df['subject'].unique().size\nprint(f'There are {subject} unique subjects')\n","b861d67d":"# GOOGLE MAPS FUNCTION\n# import csv\n# def write_to_file(city):\n# #     r = requests.get(\"https:\/\/maps.googleapis.com\/maps\/api\/geocode\/json?address=\" + city + \"&key=\" + apiKey)\n#     d = json.loads(r.content)\n#     results_len = len(d['results'])\n#     if(results_len > 0 ):\n#         lat = d['results'][0]['geometry']['location']['lat']\n#         lng = d['results'][0]['geometry']['location']['lng']\n#         # print(lat, lng)\n#         row = [city, lat, lng]\n#         with open('cities.csv', 'a') as csvFile:\n#             writer = csv.writer(csvFile)\n#             writer.writerow(row)\n\n#         csvFile.close()\n\n# for city in cities:\n#     city = city.strip()\n#     write_to_file(city)","5d8d9b39":"#MERGE DATAFRAMES\ndf['city'] = df['city'].apply(lambda x: x.strip())\ndf = pd.merge(df, places, on='city')\ndf.head()","a2c902d2":"def show_loca(tmp_df):\n    data = pd.DataFrame({'x': tmp_df['lng'],'y':  tmp_df['lat']},columns=['x', 'y'])\n#     print(tmp_df.groupby(['lat','lng','city']).agg({'takers':'sum', 'grade':'mean'}).reset_index().sort_values(by=['lng'], ascending=True).head())\n    data.plot('x', 'y', kind='scatter', s=100 , figsize=(20,12))","339001d0":"# PLOT LOCATIONS\nshow_loca(df)","4970902d":"# REMOVE OUTLYERS\ndf = df[(df['lng'] > 25) & (df['lat'] < 34)]","f78029c5":"show_loca(df)","b9c0dae4":"new_df = df.groupby(['lat', 'lng'], as_index=False).mean()\nnew_df = new_df.drop(['takers', 'studyunits','year','semel'], axis=1) \nprint(new_df.head())\ncolor_theme = np.array(['blue','red','green'])\nplt.figure(figsize=(20, 12), dpi=80)\nplt.scatter(x=df['lng'], y = df['lat'],c=df['grade'],s=100)","90ded135":"# PLOT BY REGION\nfrom ipyleaflet import Map, Heatmap\n# from random import uniform\naccdf = df\naccdf=accdf.groupby(['lat','lng']).agg({'grade':'mean'}).reset_index()\naccdf['grade'].fillna((accdf['grade'].mean()), inplace=True)\nlats=accdf[['lat','lng','grade']].values.tolist()\nm = Map(center=(accdf['lat'].mean(), accdf['lng'].mean()), zoom=8)\nheat = Heatmap(locations=lats, radius=20, blur=20)\nm.add_layer(heat)\n\n# # Change some attributes of the heatmap\n# heat.radius = 10\n# heat.blur = 10\n# heat.max = 0.5\n# heat.gradient = {0.7: 'red', 0.8: 'cyan', 1.0: 'blue'}\n# m.layout.width = '100%'\nm.layout.height = '800px'\n# m","fb36d065":"# PLOT TOP SCORES BY REGION\n\naccdf_small = accdf[accdf['grade']>83]\naccdf_small=accdf_small.groupby(['lat','lng']).agg({'grade':'mean'}).reset_index()\naccdf_small['grade'].fillna((accdf_small['grade'].mean()), inplace=True)\nlats=accdf_small[['lat','lng','grade']].values.tolist()\nm = Map(center=(accdf_small['lat'].mean(), accdf_small['lng'].mean()), zoom=8)\nheat = Heatmap(locations=lats, radius=20, blur=20)\nm.add_layer(heat)\nm.layout.height = '500px'\n# m","04149b16":"k = 6\nmodel = GaussianMixture(n_components=6)\nmodel.fit_predict(df[['lng', 'lat']])\n\ndf = df.dropna()\n\ncolors = {0: 'red', 1: 'lightgreen', 2: 'blue', \n          3:'yellow', 4: 'orange', 5: 'purple', \n          -1: 'black'}\n\ncluster = np.array(df[['lng', 'lat']])\ndf['cluster'] = model.predict(cluster)\nc = df['cluster'].apply(lambda x: colors[x])\ndf.plot('lng', 'lat', kind='scatter', c=c, s=20, figsize=(20,12))\n","2b0b3731":"#Shuffle DATA\ndf.sample(frac=1).head(10)","33f8a7bc":"#HOW MANY AVR NOT PASSED\ndf_small = df[df['grade']<=55]\nsSmall = len(df_small)\nprint(f'There are {sSmall} rows\\n')\nprint(#WHERE MOST OF SCHOOLLS DIDNT PASS\ndf_small.groupby('city').size().sort_values(ascending=False).head())\nprint(df_small.grade.plot.hist(figsize=(16,12)))","df19d98d":"sns.set(rc={'figure.figsize':(16,12)})\nsns.boxplot(x=\"year\", y=\"grade\", data=df, linewidth=1)","0b0de1e0":"sns.boxplot(x=\"year\", y=\"grade\", hue='studyunits', data=df, linewidth=1)","bb4b805a":"# See DIFF between random cities to check if years is relevant\nimport random\nstore_data = pd.DataFrame()\ndef reverse_city(value):\n    return value[::-1]\ndf['reverse_city'] =  df['city'].apply(reverse_city)\nunique_cities = df['city'].unique()\n\ndef gen_chart():\n    random_city = random.choices(unique_cities,k=1)[0]\n    city_data = df[df['city'] == random_city].groupby(['year']).mean().reset_index()\n    print(city_data.plot.line(x='year', y='grade',ylim=(0,100),title=random_city[::-1],figsize=(6,4)))\n    return df[df['city'] == random_city]\n\n\nfor _ in range(10):\n    city_data = gen_chart()\n    store_data = store_data.append(city_data)\n# store_data.head()","cc8c507f":"# MOST TAKErS by 'subject','studyunits'\ndf_exploration  = df.groupby(['subject','studyunits']).agg({'takers':'sum', 'grade':'mean'}).reset_index()\ndf_exploration.sort_values(by=['takers'], ascending=False).head(10)","f29f15df":"# LEAST TAKErS by 'subject','studyunits'\ndf_exploration.sort_values(by=['takers'], ascending=True).head(10)","e0dabc1b":"#BOTTOM BY GRADES by 'subject','studyunits'\ndf_exploration.sort_values(by=['grade'], ascending=True).head(10)","c0789d3c":"#TOP BY GRADES by 'subject','studyunits'\ndf_exploration.sort_values(by=['grade'], ascending=False).head(10)","795d572c":"def trim(row):\n    return row['subject'].strip()","6e2f2fd7":"#DATA ABOUT TOP GRRADES by 'subject','studyunits'\ndf['subject'] = df.apply(trim,axis=1)\ndf[df['subject']=='\u05d0\u05d5\u05de\u05e0\u05d5\u05ea'].groupby(['school','city','studyunits']).agg({'grade':'mean','takers':'sum'}).reset_index()","0c696813":"df_mm = df[df['subject']=='\u05ea\u05db\u05e0\u05d5\u05df \u05d5\u05ea\u05db\u05e0\u05d5\u05ea \u05de\u05e2\u05e8\u05db\u05d5\u05ea'].groupby(['school','city','studyunits']).agg({'grade':'mean','takers':'sum'}).reset_index()\nprint(len(df_mm))\ndf_mm.head(10)","54ead9ff":"#MOST TAKERS BY CITY & SUBJECT\ndf_exploration_city  = df.groupby(['city','subject']).agg({'takers':'sum', 'grade':'mean'}).reset_index()\ndf_exploration_city.sort_values(by=['takers'], ascending=False).head(10)","8c0b6e4f":"#TOP GRADES BY BY CITY & SUBJECT\ndf_exploration_city.sort_values(by=['grade'], ascending=False).head(10)","f1d5353b":"def f(row):\n    return row['takers'] * 100 \/ df_exploration['takers'].sum()\n\ndf_exploration['precent'] = df_exploration.apply(f,axis=1)\ndf_exploration.sort_values(by=['precent'], ascending=False).head(20)","54ac91b9":"# GET FIRST WORD \nfrom collections import Counter\nunique_school = df['school'].unique()\nunique_school_list = []\n\nfor sc in unique_school:\n    new_sc = sc.split()[0]\n    unique_school_list.append(new_sc)\n\nx = Counter(unique_school_list)  \ntups = x.most_common()\ntups[:40]","21be983a":"def get_first_word(school):\n    return school.split()[0]\n\ndf['school_word'] = df['school'].apply(get_first_word)\ndf.head()","04a2c8ea":"df.groupby(['school','city']).agg({'takers':'sum', 'grade':'mean'}).reset_index().sort_values(by=['grade'], ascending=False).head(10)","225ac347":"#MOST TAKERS BY SCHOOL WORD\ndf_by_grade = df.groupby(['school_word']).agg({'city':'size', 'grade':'mean'}).reset_index().sort_values(by=['grade'], ascending=False).rename(columns={'grade':'avg grade'})\ndf_by_count = df.groupby(['school_word']).agg({'takers':'sum', 'grade':'mean'}).reset_index()\nmost_takers_df = df_by_count.sort_values(by=['takers'], ascending=False).head(30)\nmost_takers_df.head(30)","f2f890db":"#TOP GRADES BY SCHOOL WORD\nmost_takers_df.sort_values(by=['grade'], ascending=False).head(10)","fef682ab":"# df[df['school_word'] == '\u05e0\u05d6\u05d9\u05e8\u05d5\u05ea'].groupby(['school','subject']).mean()\ndf[df['school_word'] == '\u05e0\u05d6\u05d9\u05e8\u05d5\u05ea'].groupby('school').mean()","b0f51235":"#BOTTOM GRADES BY SCHOOL WORD\nmost_takers_df.sort_values(by=['grade'], ascending=True).head(10)","7c27ff2f":"df[df['school_word'] == '\u05d1\u05e8\u05e0\u05e7\u05d5'].groupby('school').mean()","cf19c3ad":"df.hist(bins=100, layout=(2, 8), figsize=(20,12))","588ea6c8":"X = df.drop('grade', axis=1)\ny = df['grade']\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=0)","0b90ddd0":"X_train.head()","57a20783":"def get_cols_4_ss(df):\n    return df[['grade']]\n\ndef get_cols_4_mas(df):\n    return df[['semel','year','studyunits']]\n\ndef get_cols_4_bins(df):\n    return df[['studyunits']]\n\ndef get_takers(df):\n    return df[['takers']]\n\ndef count_vectorize_school(df):\n    corpus = df['school']\n    vectorizer = CountVectorizer(encoding=\"utf-8\",max_features=880)\n    X = vectorizer.fit_transform(corpus)\n#     print(vectorizer.get_feature_names())\n    return X.toarray()\n\ndef count_vectorize_subject(df):\n    corpus = df['subject']\n    vectorizer = CountVectorizer(encoding=\"utf-8\",max_features=159)\n    X = vectorizer.fit_transform(corpus)\n#     print(vectorizer.get_feature_names())\n    return X.toarray()\n\ndef count_vectorize_school_word(df):\n    corpus = df['school_word']\n    vectorizer = CountVectorizer(encoding=\"utf-8\",max_features=189)\n    X = vectorizer.fit_transform(corpus)\n#     print(vectorizer.get_feature_names())\n    return X.toarray()\n\ndef count_vectorize_studyunits(df):\n    corpus = df['studyunits']\n    lb = LabelEncoder()\n    y = lb.fit_transform(corpus)\n    return y\n\n\n# def count_vectorize_cluster(df):\n#     corpus = df['cluster']\n#     lb = LabelEncoder()\n#     y = lb.fit_transform(corpus)\n#     return y\n\ndef count_vectorize_cluster(df):\n    return pd.get_dummies(df['cluster'],prefix=('cluster'))\n\nss_selector = FunctionTransformer(func=get_cols_4_ss, validate=False)\nmas_selector = FunctionTransformer(func=get_cols_4_mas, validate=False)\nstudyunits_selector = FunctionTransformer(func=get_cols_4_bins, validate=False)\ntakers_selector = FunctionTransformer(func=get_takers, validate=False)\n\n# OneHotEncoderTransformer = OneHotEncoder(sparse = False, categories='auto')\ncount_vectorize_school = FunctionTransformer(func=count_vectorize_school, validate=False)\ncount_vectorize_subject = FunctionTransformer(func=count_vectorize_subject, validate=False)\ncount_vectorize_school_word = FunctionTransformer(func=count_vectorize_school_word, validate=False)\ncount_vectorize_studyunits = FunctionTransformer(func=count_vectorize_studyunits, validate=False)\ncount_vectorize_cluster = FunctionTransformer(func=count_vectorize_cluster, validate=False)\n\n# get_subject_dummies = FunctionTransformer(func=get_subject_dummies, validate=False)\n# get_studyunits_dummies = FunctionTransformer(func=get_studyunits_dummies, validate=False)\n\n# def get_subject_dummies(df):\n#     df_subject = df[['subject']].copy()\n#     return pd.get_dummies(df_subject, columns=['subject'], drop_first=True)\n\n# def get_studyunits_dummies(df):\n#     df_subject = df[['studyunits']].copy()\n#     return pd.get_dummies(df_subject, columns=['studyunits'], drop_first=True)","bdaf989a":"xx = df.groupby(['subject']).agg({'takers':'count', 'grade':'mean'}).reset_index()\nxx.sort_values(by=['takers'], ascending=False).head()","3d221c1a":"ss_pipeline = Pipeline([('ss_selector', ss_selector), \n                        ('ss', StandardScaler())])\n\nmas_pipeline = Pipeline([('mas_selector', mas_selector), \n                         ('mas', MaxAbsScaler())])\n\ntakers_pipeline = Pipeline([('takers_selector', takers_selector), \n                           ('takers_bin', Binarizer())])\n\ncount_vectorize_school = Pipeline([('count_vectorize_school'\n                                    , count_vectorize_school)])\n\ncount_vectorize_subject = Pipeline([('count_vectorize_subject'\n                                    , count_vectorize_subject)])\n\ncount_vectorize_school_word = Pipeline([('count_vectorize_school_word'\n                                    , count_vectorize_school_word)])\n\ncount_vectorize_studyunits = Pipeline([('count_vectorize_studyunits'\n                                    , count_vectorize_studyunits)])\n\ncount_vectorize_cluster = Pipeline([('count_vectorize_cluster'\n                                    , count_vectorize_cluster)])\n","178f2b99":"print('school')\nprint(count_vectorize_school.fit_transform(X_train).shape)\nprint(count_vectorize_school.fit_transform(X_test).shape)\nprint('subject')\nprint(count_vectorize_subject.fit_transform(X_train).shape)\nprint(count_vectorize_subject.fit_transform(X_test).shape)\nprint('count_vectorize_school_word')\nprint(count_vectorize_school_word.fit_transform(X_train).shape)\nprint(count_vectorize_school_word.fit_transform(X_test).shape)\nprint('count_vectorize_cluster')\nprint(count_vectorize_cluster.fit_transform(X_train).shape)\nprint(count_vectorize_cluster.fit_transform(X_test).shape)\n","afb29e7d":"trans_pipeline = FeatureUnion([\n                                ('mas_pipeline', mas_pipeline), \n                                ('count_vectorize_school', count_vectorize_school), \n                                ('count_vectorize_subject', count_vectorize_subject),\n                                ('count_vectorize_cluster', count_vectorize_cluster),    \n                               ('takers_pipeline', takers_pipeline)\n                            ])","f529c4c3":"# prepared_train = trans_pipeline.fit_transform(X_train)\n# prepared_train","22957e2d":"lr = LinearRegression()\ndt = DecisionTreeRegressor(max_depth=5)\nrf = RandomForestRegressor(max_depth=20, n_estimators=10, n_jobs=-1)","7dc6422e":"full_pipeline = Pipeline([('trans_pipeline', trans_pipeline), ('lr', lr)])\nfull_pipeline.fit(X_train, y_train)\ny_train_pred = full_pipeline.predict(X_train)\ny_test_pred = full_pipeline.predict(X_test)\nrmse = np.sqrt(MSE(y_test, y_test_pred))\nprint(rmse)","898070cd":"plt.plot(y_train, y_train_pred, '.', label='Data')\nplt.plot([0, 100], [0, 100], label='Ideal')\nplt.axes().set_aspect('equal')\nplt.legend()","70090de8":"full_pipeline = Pipeline([('trans_pipeline', trans_pipeline), ('dt', dt)])\nfull_pipeline.fit(X_train, y_train)\ny_train_pred = full_pipeline.predict(X_train)\ny_test_pred = full_pipeline.predict(X_test)\nrmse = np.sqrt(MSE(y_test, y_test_pred))\nprint(rmse)","329935d0":"plt.plot(y_test, y_test_pred, '.', label='Data')\nplt.plot([0, 100], [0, 100], label='Ideal')\nplt.axes().set_aspect('equal')\nplt.legend()","61a90678":"full_pipeline = Pipeline([('trans_pipeline', trans_pipeline), ('rf', rf)])\nfull_pipeline.fit(X_train, y_train)\ny_train_pred = full_pipeline.predict(X_train)\ny_test_pred = full_pipeline.predict(X_test)\nrmse = np.sqrt(MSE(y_test, y_test_pred))\nprint(rmse)","820e507d":"plt.plot(y_test, y_test_pred, '.', label='Data')\nplt.plot([0, 100], [0, 100], label='Ideal')\nplt.axes().set_aspect('equal')\nplt.legend()","08d9e3d5":"**Random Forest Regressor**","7c1fe401":"**Modeling**","68a5c710":"**Transformers**","667abeb6":"4.png![image.png](attachment:image.png)","20ff124a":"3.png![image.png](attachment:image.png)","f8d423f6":"**transformers**","0d7d9df1":"**Linear Regression**","86f0b411":"**Decision Tree Regressor**"}}