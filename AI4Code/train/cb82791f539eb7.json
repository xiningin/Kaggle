{"cell_type":{"1d12261c":"code","d4a76cf5":"code","79890eda":"code","30fc5b6b":"code","c2e0d03f":"code","7a60031f":"code","5df1c249":"code","044646a2":"code","ec0e5421":"code","d78218d2":"code","8950448b":"code","ef58b185":"code","eee9a7b8":"code","c31f6cbb":"code","73ce5f4a":"code","18a2cc38":"code","3ef2571e":"code","0c0f54f6":"code","30024d4c":"code","0b27196c":"code","55ac35d1":"markdown","35b86545":"markdown","02097607":"markdown","e9a9e1c9":"markdown","c32c26be":"markdown","d6f30883":"markdown","198c8113":"markdown","26d079b4":"markdown","9ecd5396":"markdown","52e01803":"markdown","dc714c5e":"markdown","cfa87608":"markdown","c7fd7096":"markdown","c5602fe4":"markdown"},"source":{"1d12261c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn.feature_selection as feat_sel\nimport sklearn.ensemble as ensemble\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#print(os.listdir(\"..\/input\"))","d4a76cf5":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","79890eda":"# N\u00ba of rows and colums\nprint('Train: Rows - '+str(len(df_train)) + ' Columns - ' + str(len(df_train.columns)))\nprint('Test: Rows - '+str(len(df_test)) + ' Columns - ' + str(len(df_test.columns)))\n\n# Type of columns\ntrain_col_types = df_train.dtypes\ntest_col_types = df_train.dtypes\nprint('-'*60)\nprint('Train: Type of columns')\nprint('-'*60)\nprint(train_col_types.groupby(train_col_types).count())\nprint('-'*60)\nprint('Test: Type of columns')\nprint('-'*60)\nprint(test_col_types.groupby(test_col_types).count())\n\n# Missing values?\nprint('-'*60)\nlist = []\ncounts = []\nfor i in df_train.columns:\n    list.append(i)\n    counts.append(sum(df_train[i].isnull()))\nprint('Train: N\u00ba of columns with missing values')\nprint('-'*60)\nprint(sum(counts))\nprint('-'*60)\nlist = []\ncounts = []\nfor i in df_test.columns:\n    list.append(i)\n    counts.append(sum(df_test[i].isnull()))\nprint('Test: N\u00ba of columns with missing values')\nprint('-'*60)\nprint(sum(counts))","30fc5b6b":"# Columns with all rows zero\ncolumns_train_sum = pd.DataFrame(df_train.sum(),columns=['Sum of Row'])\nprint('Train: N\u00ba of columns with all rows zero train: ' + str(columns_train_sum[columns_train_sum==0].count()))","c2e0d03f":"# Is any ID on the test dataset?\nfor i in df_train.ID.values:\n    c = 0\n    if i in df_test.ID.values:\n        c = c + 1\nprint('N\u00ba of ID''s on the test dataset: ' + str(c))","7a60031f":"# There's any visibla outlier on the target?\n#plt.scatter(range(df_train.shape[0]), np.sort(df_train['target'].values))\nplt.plot(df_train.ID, np.sort(df_train.target.values))\nplt.xlabel('ID')\nplt.ylabel('Target')\nplt.title('ID vs Target')\nplt.show()","5df1c249":"# How is the target distribuition\ndf_train.target.hist()\nplt.xlabel('Target')\nplt.ylabel('N\u00ba of ID''s')\nplt.title('Histogram of target')\nplt.show()","044646a2":"# How is the log target distribuition\ndf_train['target_log'] = np.log(df_train.target)\ndf_train.target_log.hist()\nplt.xlabel('Target')\nplt.ylabel('N\u00ba of ID''s')\nplt.title('Histogram of log target')\nplt.show()","ec0e5421":"## Drop columns will all zero values\nlist_columns_train_drop=[]\nfor i in columns_train_sum[columns_train_sum['Sum of Row']==0].index:\n    list_columns_train_drop.append(i)\ndf_train = df_train.drop(columns=list_columns_train_drop)\nlen(df_train.columns)","d78218d2":"## Verify the correlatin between the target and the variables\ncorr_train_target_values = []\ncorr_train_target_column = []\nfor i in df_train.columns:\n    if i in ['ID','target']:\n        None\n    else:\n        corr = df_train[['target',i]].corr(method='spearman')\n        corr_train_target_values.append(corr.target[1])\n        corr_train_target_column.append(i)\n\ncorr_train_target = pd.DataFrame(corr_train_target_values,index=corr_train_target_column)\ncorr_train_target.describe()","8950448b":"X = df_train.drop(columns=['target','target_log','ID'])\nvariable_mean = X.mean()\nvariable_std = X.std()\nvariable_name = X.columns\nhigh_indices = np.argsort(variable_mean)[::-1][:50]\nlow_indices = np.argsort(variable_mean)[:50]\nplt.bar(range(len(variable_mean[high_indices])),variable_mean[high_indices],yerr=variable_std[high_indices])\n#plt.xticks(range(len(variable_mean[high_indices])),variable_name[high_indices],rotation='vertical')\nplt.show()\nplt.bar(range(len(variable_mean[low_indices])),variable_mean[low_indices],yerr=variable_std[low_indices])\n#plt.xticks(range(len(variable_mean[low_indices])),variable_name[low_indices],rotation='vertical')\nplt.show()","ef58b185":"# Implementing a PCA to reduze the amount of variables and standardize data\n\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\n\nX = df_train.drop(columns=['target','ID','target_log'])\nX = preprocessing.scale(X)\nlist_n_comp=[]\nlist_var_ratio=[]\nn_comp = 100\nmax_list_var_ratio = 0.0\nwhile max_list_var_ratio<0.8: #n_comp <= 1000:\n    print(n_comp)\n    pca = PCA(n_components=n_comp)\n    pca.fit(X)\n    list_n_comp.append(n_comp)\n    list_var_ratio.append(sum(pca.explained_variance_ratio_))\n    max_list_var_ratio = max(list_var_ratio)\n    print(max_list_var_ratio)\n    n_comp = n_comp + 100\n#list_n_comp,list_var_ratio\n\nplt.plot(list_n_comp, list_var_ratio)\nplt.xlabel('Number of components')\nplt.ylabel('Variance Ratio')\nplt.title('PCA')\nplt.ylim([0,1])\nplt.axhline(0.8,color='r')\nplt.show()","eee9a7b8":"# PCA with 80% explained ratio is with 900 components\nfrom sklearn import preprocessing\nfrom sklearn.decomposition import PCA\n\nX = df_train.drop(columns=['target','ID','target_log'])\nX = pd.DataFrame(preprocessing.scale(X),columns = X.columns)\npca = PCA(n_components=900)\nX_pca = pd.DataFrame(pca.fit_transform(X))\n\ndf_train_pca = df_train[['ID','target','target_log']]\ndf_train_pca[X_pca.columns.values]= X_pca\n\nX = df_test.drop(columns=['ID'])\nX = pd.DataFrame(preprocessing.scale(X),columns = X.columns)\npca = PCA(n_components=900)\nX_pca = pd.DataFrame(pca.fit_transform(X))\n\ndf_test_pca = pd.DataFrame(df_test['ID'],columns=['ID'])\ndf_test_pca[X_pca.columns.values]= X_pca","c31f6cbb":"# Split train data into test and train\nX = df_train_pca.drop(columns=['target','ID','target_log'])\ny = df_train_pca.target_log\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# Finding the best model out of the box\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error\nfrom sklearn.linear_model import Ridge\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.metrics import make_scorer\n\ndef score(y_true,y_pred):\n    score = np.sqrt(mean_squared_log_error(np.exp(y_true), np.exp(y_pred)))\n    return score\nmy_score = make_scorer(score,greater_is_better=False)\n\nlist_model = [Ridge(),SVR(),GradientBoostingRegressor(),RandomForestRegressor(),AdaBoostRegressor(),MLPRegressor()]\nfor i in list_model:\n    print(i)\n    print(cross_val_score(i, X_train, y_train, cv=5, scoring = my_score))\n    print('-'*60)","73ce5f4a":"# Finding best hyperparameters\n\nparameters = {'n_estimators':[50,100,150],'max_depth':[3,5]}\nmodel = GradientBoostingRegressor()\ngrid = GridSearchCV(model, parameters,scoring=my_score)\ngrid_result = grid.fit(X_train,y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","18a2cc38":"# Using best hyperparameters and evaluate test set\n\nmodel = GradientBoostingRegressor(n_estimators=100,max_depth=3)\nmodel.fit(X_train,y_train)\ny_pred_train = model.predict(X_train)\ny_pred_test = model.predict(X_test)\nresults = {'rmse train': np.sqrt(mean_squared_log_error(np.exp(y_train), np.exp(y_pred_train))),\n           'rmse test': np.sqrt(mean_squared_log_error(np.exp(y_test), np.exp(y_pred_test))),\n           'std y_train': np.exp(y_train).std(),\n           'mean y_train': np.exp(y_train).mean(),\n           'std y_test': np.exp(y_test).std(),\n           'mean y_test': np.exp(y_test).mean(),\n           'std y_pred_train': np.exp(y_pred_train).std(),\n           'mean y_pred_train': np.exp(y_pred_train).mean(),\n           'std y_pred_test': np.exp(y_pred_test).std(),\n          }\nresults","3ef2571e":"# make prediction with gradient boost\nX = df_test_pca.drop(columns=['ID'])\ndf_test_pca['target'] = np.exp(model.predict(X))\ndf_test_pca[['ID','target']].to_csv('subsmission_gb.csv',index=False,sep=',')","0c0f54f6":"from keras.models import Sequential\nfrom keras.layers import Dense,Dropout\nfrom keras import backend as K\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n# Split train data into test and train\nX = df_train_pca.drop(columns=['target','ID','target_log'])\ny = df_train_pca.target_log\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n\n# Finding best hyperparameters\n\nfrom sklearn.metrics import make_scorer\ndef score(y_true,y_pred):\n    score = np.sqrt(mean_squared_log_error(np.exp(y_true), np.exp(y_pred)))\n    return score\nmy_score = make_scorer(score,greater_is_better=False)\n\ndef root_mean_squared_error(y_true, y_pred):\n    return K.sqrt(K.mean(K.square(y_pred - y_true))) \n\ndef create_model():\n    model = Sequential()\n    model.add(Dense(2,input_dim=900,activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(1,activation='relu'))\n    model.compile(loss=root_mean_squared_error,optimizer='SGD',metrics=['accuracy'])\n    return model\n\nmodel = KerasClassifier(build_fn=create_model,epochs=10,batch_size=3)\n\nepochs = [5,10,20]\nbatch_size = [3,5,7]\nverbose = [0]\nparameters = dict(epochs=epochs,batch_size=batch_size,verbose=verbose)\n\ngrid = GridSearchCV(model, parameters,scoring=my_score)\ngrid_result = grid.fit(X_train,y_train)\n\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","30024d4c":"# Using best hyperparameters and evaluate test set\n\nmodel = Sequential()\nmodel.add(Dense(2,input_dim=900,activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1,activation='relu'))\nmodel.compile(loss=root_mean_squared_error,optimizer='SGD')\n\nmodel.fit(X_train, y_train, epochs=10, batch_size=7,verbose=1)\ny_pred_train = model.predict(X_train, batch_size=32)\ny_pred_test = model.predict(X_test, batch_size=32)\n\nresults = {'rmse train': np.sqrt(mean_squared_log_error(np.exp(y_train), np.exp(y_pred_train))),\n           'rmse test': np.sqrt(mean_squared_log_error(np.exp(y_test), np.exp(y_pred_test))),\n           'std y_train': np.exp(y_train).std(),\n           'mean y_train': np.exp(y_train).mean(),\n           'std y_test': np.exp(y_test).std(),\n           'mean y_test': np.exp(y_test).mean(),\n           'std y_pred_train': np.exp(y_pred_train).std(),\n           'mean y_pred_train': np.exp(y_pred_train).mean(),\n           'std y_pred_test': np.exp(y_pred_test).std(),\n          }\nresults","0b27196c":"# make prediction with nueral network\nX = df_test_pca.drop(columns=['ID','target'])\ndf_test_pca['target'] = np.exp(model.predict(X))\ndf_test_pca[['ID','target']].to_csv('subsmission_nn.csv',index=False,sep=',')","55ac35d1":"#### There a big difference between each variable in terms off the mean and std. Best to standardize the data and reduce the number of variables for predictions","35b86545":"1. The train dataset has 256 columns with all values zero ","02097607":"## Importing Files and Libraries","e9a9e1c9":"1. The train set has 4.459 rows and 4.993 columns\n1. The test set has 49.342 rows and 4.992 columns\n1. All the columns are numbers and looks like they area anonized","c32c26be":"> #### Aflter Gradient Boost lets see how a neural network on Keras\/Tensorflow performes","d6f30883":"#### With the log of the target the distribuiton looks more distribuite, more like a normal distribuition","198c8113":"### Analysing columns and distribuitions","26d079b4":"#### As expeted theres no ID form the train set ont the test set","9ecd5396":"#### GradientBoost looks the best performer. Lets see what's the best hyperparameters","52e01803":"#### Looks like the distribuition is not normal so lets try the log of the target","dc714c5e":"#### There is a low correlation of the variables with the target. Let's see how's the distribuiton off eah variable","cfa87608":"#### After scale and reduze the variables let's see what's the best model using cross validation","c7fd7096":"#### PCA with 900 componente explains more than 80% of the variability","c5602fe4":"#### There isn't visible outliers on the train set"}}