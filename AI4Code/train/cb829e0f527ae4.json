{"cell_type":{"6de870ef":"code","53026d6e":"code","794be716":"code","82f78e8e":"code","e10cf0ff":"code","3aad7220":"code","509b35ee":"code","6e24d92a":"code","691ac4e9":"code","65e1dd14":"code","56772f1c":"code","e3701bc1":"code","ea67c362":"code","1711166d":"code","55970edb":"code","e4b23305":"code","f5c3fb6a":"code","2180714c":"code","66331cdd":"code","846d342d":"code","145680de":"markdown","499c676a":"markdown","adcded36":"markdown","e97e85c6":"markdown","cc8b742f":"markdown","9d6d9666":"markdown","b40d4836":"markdown","5f11a829":"markdown","1b2bfd09":"markdown","4487a179":"markdown","f72ca5c7":"markdown","6c68701b":"markdown","7c582f5f":"markdown","f5439d7c":"markdown","dabba896":"markdown","4b2e8115":"markdown","4e361cb3":"markdown"},"source":{"6de870ef":"import random, os\nimport numpy as np\nfrom copy import deepcopy\n\nimport torch\nfrom torch.utils.data.dataset import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.sampler import SequentialSampler\n\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import TrainingArguments, Trainer, DataCollatorWithPadding","53026d6e":"model_checkpoint = \"distilbert-base-uncased\"\nmax_length = 512\nbatch_size = 16\nepochs = 1\nlr = 2e-5","794be716":"# load pretrained Tokenizer\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)","82f78e8e":"train, test = load_dataset('imdb', split=['train', 'test'])","e10cf0ff":"# create a tokenizer function with static padding\ntoknize_with_padding = lambda x: tokenizer(x['text'], truncation=True, padding=True)\n\n# prepare datasets\ntrain_ds = train.map(toknize_with_padding, batched=True, batch_size=128, remove_columns=['text']).shuffle()\ntest_ds = test.map(toknize_with_padding, batched=True, batch_size=128, remove_columns=['text']).shuffle()","3aad7220":"def torch_seed(seed=0):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    torch.cuda.manual_seed_all(seed)\n\ntorch_seed()","509b35ee":"model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint)\nweights = deepcopy(model.state_dict())","6e24d92a":"args = TrainingArguments(\n    f\"result\",\n    evaluation_strategy='epoch',\n    learning_rate=lr,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=epochs,\n    report_to='none'\n    )","691ac4e9":"trainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_ds,\n    eval_dataset=test_ds,\n    )\n\ntrainer.train()","65e1dd14":"# don't pad your dataset\ntoknize_wo_padding = lambda x: tokenizer(x['text'], truncation=True, padding=False)\n\n# prepare datasets\ntrain_ds = train.map(toknize_wo_padding, batched=True, batch_size=128, remove_columns=['text'])\ntest_ds = test.map(toknize_wo_padding, batched=True, batch_size=128, remove_columns=['text'])","56772f1c":"collator = DataCollatorWithPadding(tokenizer=tokenizer, max_length=512)","e3701bc1":"# load the weights of model\nmodel.load_state_dict(weights)","ea67c362":"trainer = Trainer(\n    model=model,\n    args=args,\n    train_dataset=train_ds,\n    eval_dataset=test_ds,\n    data_collator=collator\n    )\n\ntrainer.train()","1711166d":"class SortedDataset(Dataset):\n    def __init__(self, examples):\n        self.examples = sorted(examples, key=lambda t: len(t['input_ids']))\n\n    def __getitem__(self, i):\n        return self.examples[i]\n\n    def __len__(self):\n        return len(self.examples)\n\nsorted_train_ds = SortedDataset(train_ds.shuffle())\nsorted_test_ds = SortedDataset(test_ds.shuffle())","55970edb":"# new Trainer class with SequentialSampler\nclass SortedTrainer(Trainer):\n    def get_train_dataloader(self):    \n        return DataLoader(\n            self.train_dataset,\n            batch_size=self.args.train_batch_size,\n            # don't shuffle on train_ds\n            sampler=SequentialSampler(self.train_dataset), \n            collate_fn=self.data_collator\n        )","e4b23305":"# load the original weights\nmodel.load_state_dict(weights)","f5c3fb6a":"# using SortedTrainer instead of Trainer\ntrainer = SortedTrainer( \n    model=model,\n    args=args,\n    train_dataset=sorted_train_ds,\n    eval_dataset=sorted_test_ds,\n    data_collator=collator\n)\n\ntrainer.train()","2180714c":"# this isn't the best way for doing this but you get the idea\nclass SortishDataset(Dataset):\n    def __init__(self, examples, batch_size=16, orderish=True, mega_size=30):\n        n = len(examples)\n        if orderish:  step = batch_size*mega_size\n        else: step = int(batch_size*(1\/batch_size))\n\n        self.sortish_examples = []\n        for mega_batch in range(0, n, step):\n            if mega_batch+step>n:\n                self.sortish_examples += sorted(examples.select(range(mega_batch, n)), key=lambda t: len(t['input_ids']))\n                break\n            self.sortish_examples += sorted(examples.select(range(mega_batch, mega_batch+step)), key=lambda t: len(t['input_ids']))\n\n    def __getitem__(self, i):\n        return self.sortish_examples[i]\n\n    def __len__(self):\n        return len(self.sortish_examples)\n\n\nsortish_train_ds = SortishDataset(train_ds.shuffle(), orderish=True, batch_size=batch_size)\nsortish_test_ds = SortishDataset(test_ds, orderish=False, batch_size=batch_size)","66331cdd":"model.load_state_dict(weights)","846d342d":"trainer = SortedTrainer(\n    model=model,\n    args=args,\n    train_dataset=sortish_train_ds,\n    eval_dataset=sortish_test_ds,\n    data_collator=collator)\n\ntrainer.train()","145680de":"\n\n![dynamic padding at one glance](https:\/\/uupload.ir\/files\/79ki_dynamic-pad.png)\n<center>dynamic padding at one glance<\/center>","499c676a":"By using just dynamic padding, we do not see a significant improvement\n- it depends on the variety of sequence lengths in the dataset\n- By just using Dynamic Padding for SQuADv2 I was able to reduce my time by 1.5 times","adcded36":"Hyperparameters that we need","e97e85c6":"seed everything for reproducibility","cc8b742f":"## IMBD sentiment classfication as an example","9d6d9666":"#  Faster \ud83e\udd17 Transformers Training\n\nwith dynamic padding and order batches\n","b40d4836":"it takes about 12 min","5f11a829":"## Normal (Static) Padding\ntrain with padding until the biggest example in the dataset (or set a max_len)","1b2bfd09":"During training, \ud83e\udd17 DataCollatorWithPadding will be used for padding","4487a179":"## Dynamic Padding With Sorted dataset\nTo create bathes with similar token lengths, we will sort the dataset based on token length\n- Cons: this affects the randomness of SGD (Sortish will address this)","f72ca5c7":"## Dynamic Padding\nwe don't need padding in the tokenization stage; we'll do it during training","6c68701b":"load model and save its weights for reproducibility","7c582f5f":"## Dynamic Padding with Sortish","f5439d7c":"6 mins without any effects on training, not bad ha??\n- Note: You may need to run the Sortish process again if you run more than 3 or 4 epochs to prevent learning the sequence of batches\n\n<center>Thanks for reading, I hope it was helpful<\/center>","dabba896":"By using a sorted dataset, we are able to train 2 times faster\n- But we have not the advantage of stochastic training (we got 0.256 valid_loss which is worse than normal training)\n- The simplest solution is Sortish Training","4b2e8115":"Sortish means sorting each mega batch instead of the entire dataset\n- mega batch: batch_size * mega_size \n- To maintain the advantage of randomness along with dynamic padding","4e361cb3":"`Transformers.Trainer`'s default sampler is RandomSampler\n  - After Sorting, we don't want to shuffle the data\n  - Override the Trainer class and create a new Data Loader with Sequential Sampler"}}