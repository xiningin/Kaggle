{"cell_type":{"49ff6719":"code","ce6d017d":"code","2428065a":"code","1fa8864d":"code","91dd005b":"code","096ef912":"code","9ba3104c":"code","020dd473":"code","a6342a43":"code","0c8a3eba":"code","78d7b577":"code","e1d2afb2":"code","0d3b5336":"code","2fcb0a52":"code","c14a2c4a":"code","91b580d0":"code","a9082667":"code","bd70c395":"markdown","02d3f23c":"markdown","9e286128":"markdown","2ed716c5":"markdown","e8069762":"markdown","d4a17cc4":"markdown","e3dde17d":"markdown","781c74a0":"markdown","65aab3bb":"markdown","1193f42c":"markdown","ccca2e71":"markdown","931a0d96":"markdown","4e095307":"markdown","fa2541d8":"markdown","2783fa23":"markdown","6bb48771":"markdown","0ed75bfa":"markdown","5c19de19":"markdown","aa7755dc":"markdown","5bf5619e":"markdown","c9ab8857":"markdown"},"source":{"49ff6719":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ce6d017d":"from sklearn.decomposition import PCA\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn import datasets\ncolors = ['royalblue','red','deeppink', 'maroon', 'mediumorchid', 'tan', 'forestgreen', 'olive', 'goldenrod', 'lightcyan', 'navy']\nvectorizer = np.vectorize(lambda x: colors[x % len(colors)])","2428065a":"rng = np.random.RandomState(1)\nX = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\nplt.scatter(X[:, 0], X[:, 1])","1fa8864d":"pca = PCA(n_components=2)\npca.fit(X)","91dd005b":"print(pca.components_)","096ef912":"print(pca.explained_variance_ratio_)","9ba3104c":"def draw_vector(v0, v1, ax=None):\n    ax = ax or plt.gca()\n    arrowprops=dict(arrowstyle='->',\n                    linewidth=2,\n                    shrinkA=0, shrinkB=0)\n    ax.annotate('', v1, v0, arrowprops=arrowprops)\n\n# plot data\nplt.scatter(X[:, 0], X[:, 1], alpha=0.2)\nfor length, vector in zip(pca.explained_variance_, pca.components_):\n    v = vector * 3 * np.sqrt(length)\n    draw_vector(pca.mean_, pca.mean_ + v)\nplt.axis('equal');","020dd473":"iris = datasets.load_iris()\nX = iris.data\ny = iris.target","a6342a43":"pca = PCA(n_components=2)\nX_r = pca.fit(X).transform(X)","0c8a3eba":"plt.scatter(X_r[:,0],X_r[:,1],c=vectorizer(y))","78d7b577":"from sklearn.datasets import fetch_openml\nmnist = fetch_openml('mnist_784', version=1)\nmnist.target = mnist.target.astype(np.int64)\nfrom sklearn.model_selection import train_test_split\n\nX = mnist[\"data\"]\ny = mnist[\"target\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y)","e1d2afb2":"some_digit = X[10]\nsome_digit_image = some_digit.reshape(28, 28)\nplt.imshow(some_digit_image, cmap=\"binary\")\nplt.axis(\"off\")\nplt.show()","0d3b5336":"pca = PCA().fit(X_train)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance');","2fcb0a52":"pca = PCA(n_components=0.90)\nX_reduced = pca.fit_transform(X_train)","c14a2c4a":"pca.n_components_","91b580d0":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\ntree_clf = DecisionTreeClassifier(random_state=42)\ntree_clf.fit(X_train, y_train)\ny_pred_tree = tree_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred_tree))","a9082667":"X_r_train = pca.transform(X_train)\nX_r_test = pca.transform(X_test)\ntree_clf.fit(X_r_train, y_train)\ny_pred_tree = tree_clf.predict(X_r_test)\nprint(accuracy_score(y_test, y_pred_tree))","bd70c395":"* **Randomized PCA:** It is a faster method. At, a high level an approximate matrix of the original matrix is found and it's components are found out. The scikit learn automatically changes the randomized if the data is large.\n* **Incrmental PCA:** This can be used when the dataset is too large to fit in memory, the principal components are founf using mini batches\n* **Sparse PCA:** The normal version of PCA uses linear combinaton of the original features. The coefficents of the features are referred as **loading**. In the normal variant most of the loadings are nonzero, hence are not interpretable may be pc1= 0.2 * NoofRooms + 0.5 * DistanceFromschool + 0.3 * FloorArea. So, this 0.2,0.5,0.3 are called loading and in Sparse PCA, an attempt is made so that loadings are Zero, similiar to LASSO. As, a result the models are more interpretable.\n\n","02d3f23c":"# Question 4: PCA on MNIST","9e286128":"# Question 3: PCA on Iris data <a id=3> <\/a>","2ed716c5":"### Importing MNSIT Data","e8069762":"![image.png](attachment:image.png)","d4a17cc4":"### On original data","e3dde17d":"### Finding number of components","781c74a0":"* [<font size=4>Question 1: What are some of the inputs and outputs of the PCA<\/font>](#1)\n* [<font size=4>Question 2: How to apply PCA on some linearly correlated data?<\/font>](#2)   \n* [<font size=4>Question 3: PCA on Iris data<\/font>](#3)   \n* [<font size=4>Question 4:PCA on MNIST <\/font>](#4)  \n* [<font size=4>Question 5:Classification on original data on transfromed data<\/font>](#5) \n* [<font size=4>Question 6:What are diffrent variants of PCA<\/font>](#6) ","65aab3bb":"### On Projected data","1193f42c":"* n_components int, float, None or str Number of components to keep. if n_components is not set all components are kept:\n* svd_solverstr {\u2018auto\u2019, \u2018full\u2019, \u2018arpack\u2019, \u2018randomized\u2019}\n* components_array, shape (n_components, n_features) Principal axes in feature space, representing the directions of maximum variance in the data. The components are sorted by explained_variance_.\n* explained_variance_ratio array, shape (n_components,)","ccca2e71":"### Plotting the components","931a0d96":"### Loading the data","4e095307":"### Plotting a random image","fa2541d8":"### Creating the data ","2783fa23":"# Question 6: What are other variants of PCA?<a id=1.6><\/a>","6bb48771":"### Fitting the PCA","0ed75bfa":"# Question 2: How to apply PCA on some linearly correlated data? <a id=2> <\/a>","5c19de19":"### Inspecting the PCA","aa7755dc":"# Quesion 5: How much this effects classification if components are used?","5bf5619e":"# Question 1: What are some of the inputs and outputs of the PCA? <a id=1> <\/a>","c9ab8857":"* **Kernel PCA:** Same kernet trick as SVM, if the variablity are explained by non linear componets, Kernel PCA can achieve the same"}}