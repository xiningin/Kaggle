{"cell_type":{"49c81c8c":"code","59df33a2":"code","f44c00fd":"code","e7e89806":"code","4441cae1":"code","a355d878":"code","ef383266":"code","8eec4613":"code","8670963c":"code","4aaf6393":"code","e5ab9e15":"code","537d2835":"code","ea0efcbe":"code","100fce3c":"code","efa94ad8":"code","d440560d":"code","9bdf3126":"code","1e18c613":"code","8c4513df":"code","582174a7":"code","2c1b18fa":"code","0e069257":"code","d7344f11":"code","bd06d7e9":"code","d092afbb":"code","3f17030c":"code","dd0db524":"code","a9bee774":"code","21a1713e":"code","e32058a7":"code","c9976707":"code","b27044eb":"code","67e80ab3":"code","789fb811":"code","76a34fc2":"code","6bbfba16":"code","2ed7e055":"code","1b44c36b":"code","137db852":"code","152c44df":"code","2330f21b":"code","d94c5532":"code","d0169284":"code","09e3586b":"code","f7456a37":"code","e4bf7e3a":"code","d4e87db8":"code","b7aee443":"code","fca8f480":"code","4e59e2c1":"code","b6470980":"code","cc110a3c":"code","e7a9da7d":"code","536ebeb4":"code","45a77c75":"code","ba221a35":"code","82031d42":"markdown","04598356":"markdown","4f5cb5d2":"markdown","54047049":"markdown","f5e2b2bf":"markdown","e1aa659f":"markdown","2bc4c7ec":"markdown","2579e9fe":"markdown","50e4c665":"markdown","d89fb00f":"markdown","213c2595":"markdown","6bc3c73b":"markdown","0e662b91":"markdown","efa18ca2":"markdown","6dddca2b":"markdown","f9e5cfb4":"markdown","122318bd":"markdown","f0aeb2e7":"markdown","73ceb507":"markdown","939b02ec":"markdown","d34760b4":"markdown","1a9d2831":"markdown","43e1c704":"markdown","cb0d4fb5":"markdown","e6dc5cb0":"markdown","afb32aeb":"markdown","93f1228f":"markdown","381acca2":"markdown","cc55dea7":"markdown","e761cde9":"markdown","f1f8749c":"markdown","958e106e":"markdown","20128583":"markdown","ca7b80b3":"markdown","3db134b8":"markdown","2feab806":"markdown","968f9a8e":"markdown","865bb4ab":"markdown","15a60eae":"markdown","88eb2f56":"markdown","98abcd6e":"markdown","73ad4b69":"markdown"},"source":{"49c81c8c":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, validation_curve\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn import metrics\nimport itertools\nfrom mlxtend.evaluate import confusion_matrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport folium\nfrom folium.plugins import MarkerCluster\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_rows', 194673)\npd.set_option('display.max_columns', 37)\n","59df33a2":"df = pd.read_csv('..\/input\/seattle-sdot-collisions-data\/Collisions.csv')","f44c00fd":"df.columns","e7e89806":"df.rename(columns={'SEVERITYCODE': 'severity_code', 'X':'longitude', 'Y': 'latitude',\n                   'ADDRTYPE':'addr_type', 'LOCATION': 'location','SEVERITYDESC':'severity_desc', 'COLLISIONTYPE':'collision_type',\n                   'PERSONCOUNT':'person_count', 'PEDCOUNT': 'ped_count', 'PEDCYLCOUNT': 'ped_cycle_count', 'VEHCOUNT': 'veh_count',\n                   'INCDTTM': 'inc_dt', 'JUNCTIONTYPE': 'junc_type', 'SDOT_COLCODE': 'case_code', 'SDOT_COLDESC': 'case_desc',\n                   'UNDERINFL':'under_infl', 'WEATHER': 'weather', 'ROADCOND': 'roadcond', 'LIGHTCOND': 'light_cond',\n                   'ST_COLCODE': 'st_code', 'ST_COLDESC': 'st_desc', 'HITPARKEDCAR':'hit_parked_car', 'SPEEDING':'speeding', \n                   'FATALITIES':'fatalities', 'INJURIES':'injuries', 'SERIOUSINJURIES':'serious_injuries'}, inplace=True)","4441cae1":"df.shape","a355d878":"df.head()","ef383266":"df.info()","8eec4613":"map = folium.Map(location=[47.606209, -122.332069], zoom_start=10)\nmap_clust = MarkerCluster().add_to(map)\nlocation = df[['latitude', 'longitude']][df['longitude'].notnull()][:5000]\nloc = location.values.tolist()\nfor i in range(len(loc)):\n  folium.Marker(loc[i]).add_to(map_clust)\nmap","8670963c":"df['severity_code'].value_counts().to_frame('counts')","4aaf6393":"df['severity_desc'].value_counts().to_frame('counts')","e5ab9e15":"df['collision_type'].value_counts().to_frame('counts')","537d2835":"df['addr_type'].value_counts().to_frame('counts')","ea0efcbe":"df['junc_type'].value_counts().to_frame('counts')","100fce3c":"df['weather'].value_counts().to_frame('counts')","efa94ad8":"df['roadcond'].value_counts().to_frame()","d440560d":"df['light_cond'].value_counts().to_frame('counts')","9bdf3126":"df[['person_count', 'ped_count', 'ped_cycle_count', 'veh_count']].describe()","1e18c613":"df = df[['longitude', 'latitude','location','severity_code',\n        'severity_desc','collision_type', 'person_count', 'ped_count', 'ped_cycle_count',\n       'veh_count','inc_dt','addr_type', 'junc_type', 'case_code', 'case_desc','under_infl',\n       'speeding', 'weather', 'roadcond', 'light_cond','st_code', 'st_desc',\n       'hit_parked_car', 'injuries', 'serious_injuries', 'fatalities']]","8c4513df":"df.isnull().sum()","582174a7":"df1 = df[['latitude', 'longitude', 'severity_code', 'weather', 'roadcond', 'light_cond', \n          'speeding', 'under_infl', 'person_count', 'ped_count', 'ped_cycle_count', 'veh_count', \n          'injuries', 'serious_injuries', 'severity_desc', 'fatalities']]","2c1b18fa":"df1['speeding'].replace(np.nan,0,inplace=True)\ndf1['speeding'].replace('Y', 1, inplace=True)\ndf1['speeding'].value_counts().to_frame('counts')","0e069257":"df1.replace(to_replace={'Unknown': np.nan, \n                        'Other':np.nan}, inplace=True)","d7344f11":"df1.dropna(inplace=True)","bd06d7e9":"df1.isnull().sum()","d092afbb":"df1['under_infl'].replace(to_replace={'Y':1, 'N':0, '1':1, '0':0}, inplace=True)","3f17030c":"df1['under_infl'].value_counts().to_frame('counts')","dd0db524":"df1['severity_code'].replace(to_replace={'2b':'4'}, inplace=True)","a9bee774":"plt.style.use('ggplot')\nax = sns.countplot(df1['weather'])\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nplt.show()","21a1713e":"plt.style.use('seaborn')\nax = sns.countplot(df1['severity_desc'])\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nplt.xlabel('severity')\nplt.show()","e32058a7":"ax = sns.countplot(df1['roadcond'])\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nplt.show()","c9976707":"ax = sns.countplot(df1['light_cond'])\nax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\nplt.show()","b27044eb":"plt.figure(figsize=(6, 4))\nsns.countplot(df1['under_infl'])","67e80ab3":"ax = plt.scatter(df1['veh_count'], df1['person_count'])\nplt.xlabel('vehicle_count')\nplt.ylabel('person_couont')\nplt.show()","789fb811":"plt.scatter(df1['ped_count'], df1['person_count'])\nplt.xlabel('pedestrian_count')\nplt.ylabel('person_count')\nplt.show()","76a34fc2":"plt.scatter(df1['veh_count'], df1['injuries'])\nplt.xlabel('vehicle count')\nplt.ylabel('injuries count')\nplt.show()","6bbfba16":"sns.heatmap(df1.corr(), cmap='YlGnBu_r')\nplt.show()","2ed7e055":"df2 = pd.concat([df1.drop(['weather', 'roadcond', 'light_cond','severity_desc'], axis=1),\n                 pd.get_dummies(df1['weather']),\n                 pd.get_dummies(df1['roadcond']),\n                 pd.get_dummies(df1['light_cond'])], axis=1)\ndf2.reset_index(drop=True, inplace=True)","1b44c36b":"df2.head().T","137db852":"sns.heatmap(df2.corr(), cmap='YlGnBu_r')\nplt.show()","152c44df":"x = df2.drop('severity_code', axis=1).values\ny = df2['severity_code'].values\nX = StandardScaler().fit(x).transform(x)\ny = y.astype(int)","2330f21b":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","d94c5532":"def cnf_mx(preds):\n    cm = confusion_matrix(y_target=y_test, \n                          y_predicted=preds, \n                          binary=False)\n    fig, ax = plot_confusion_matrix(conf_mat=cm)\n    plt.show()","d0169284":"\ndef validate_models(model):\n    kfold = KFold(n_splits=10, random_state=42)\n    results_1 = cross_val_score(model, X, y, cv=kfold)\n    print(\"kfold cross_val_score: %.2f%%\" % (results_1.mean()*100.0))\n    \n    skfold = StratifiedKFold(n_splits=3, random_state=100)\n    results_2  = cross_val_score(model, X, y, cv=skfold)\n    print(\"stratified kfold cross_val _score: %.2f%%\" % (results_2.mean()*100.0))","09e3586b":"tree_model = DecisionTreeClassifier(criterion='entropy', max_depth=5)\ntree_model.fit(x_train, y_train)\nprint(tree_model)\nyhat1 = tree_model.predict(x_test)\nprint('The accuracy of the decision tree classifier is {} with a max_depth of 5'.format(accuracy_score(y_test, yhat1)))","f7456a37":"print(classification_report(y_test, yhat1))","e4bf7e3a":"cnf_mx(preds=yhat1)","d4e87db8":"validate_models(DecisionTreeClassifier(criterion='entropy', max_depth=5))","b7aee443":"forest_model = RandomForestClassifier(n_estimators=75)\nforest_model.fit(x_train, y_train)\nprint(forest_model)\nyhat2 = forest_model.predict(x_test)\nprint('the accuracy score for Random Forest Classifier is {}'.format(accuracy_score(y_test, yhat2)))","fca8f480":"print(classification_report(y_test, yhat2))","4e59e2c1":"cnf_mx(preds=yhat2)","b6470980":"validate_models(RandomForestClassifier(n_estimators=75))","cc110a3c":"log_reg_model = LogisticRegression(C=0.06)\nlog_reg_model.fit(x_train, y_train)\nprint(log_reg_model)\nyhat3 = log_reg_model.predict(x_test)\nprint('The accuracy score for logistic regression is {}'.format(accuracy_score(y_test, yhat3)))","e7a9da7d":"print(classification_report(y_test, yhat3))","536ebeb4":"cnf_mx(yhat3)","45a77c75":"validate_models(LogisticRegression())","ba221a35":"plt.bar(['DecisionTreeClassifier', 'RandomForestClassifier', 'LogisticRegression'], [1.,1.,1.])\nplt.ylabel('accuracy')\nplt.xlabel('machine learning models')\nplt.show()","82031d42":"##### As said before the dataset has some useless data like unknown and others which won't fall under any of the category and they are a kind of outliers which ruin the dataset .hence they are all converted to nan.Hence paving way to group them under the category of missing values","04598356":"#### Logistic Regression is a classifier that estimates discrete values (binary values like 0\/1, yes\/no, true\/false) based on a given set of an independent variables. It basically predicts the probability of occurrence of an event by fitting data to a logistic function. Hence it is also known as logistic regression. The values obtained would always lie within 0 and 1 since it predicts the probability.\n\n#### The chosen dataset has more than two target categories in terms of the accident severity code assigned, one-vs-one (OvO) strategy is employed.\n****","4f5cb5d2":"#### The above represented is the map of seattle and the locations where collisions took place .","54047049":"### **Data Cleaning & Pre-Processing**","f5e2b2bf":"### **Modelling and Evaluation**","e1aa659f":"**addr_type**\n#### Collision address type:\n\u2022 Alley\n\u2022 Block\n\u2022 Intersection","2bc4c7ec":"**weather**\n#### A description of the weather conditions during the time of the collision.","2579e9fe":"**collison_type**\n#### The type of collision","50e4c665":"![image.png](attachment:image.png)","d89fb00f":"#### This dataset is obtained from an external source and usually any dataset obtained will not be in a clean format ,it will always contain some missing values and some irrelevant data which are just trash to the dataset. This dataset too consist of a lot of missing values and useless datas .so before further processing the data it is important to clean the dataset","213c2595":"### **visualization**","6bc3c73b":"#### Importing the libraries which will be used in this notebook","0e662b91":"####  Seattle, a city on Puget Sound in the Pacific Northwest.  Washington State\u2019s largest city, it\u2019s home to a large tech industry, with Microsoft and Amazon headquartered in its metropolitan area. The city is of high socio-economic value and a fast moving city .The city is  in a surge to improve the lifestyle of the people living over there .one of the  most important problem they are facing is traffic .The provided dataset consist of a large amount of  collision data recorded over many years in the seattle city .This dataset also provides various information which are some kind of related to the cause of the collision ","efa18ca2":"##### dropping off all the missing data from the dataset","6dddca2b":"#### Random Forest Classifier is an ensemble (algorithms which combines more than one algorithms of same or different kind for classifying objects) tree-based learning algorithm. RFC is a set of decision trees from randomly selected subset of training set. It aggregates the votes from different decision trees to decide the final class of the test object. Used for both classification and regression.\n\n#### Similar to DTC, RFT requires an input that specifies a measure that is to be used for classification, along with that a value for the number of estimators (number of decision trees) is required. A hyper parameter RFT was used to determine the best choices for the above mentioned parameters. RFT with 75 DT\u2019s using entropy as the measure gave the best accuracy when trained and tested on pre-processed accident severity dataset.\n","f9e5cfb4":"### **Data**","122318bd":"## **Business Undertanding**","f0aeb2e7":"#### It's really huge, A dataset consisting of 40 columns and 221266 rows ","73ceb507":"##### The under the influece of alcohol has a lot of variables which are just the duplicate of themselves .hence changing them to a unique numeric data which will be good for processing the data","939b02ec":"**light_cond**\n#### The light conditions during the collision","d34760b4":"#### Just for my convenience ,  i have changed the column names","1a9d2831":"# *Seattle Collision Severity Prediction*","43e1c704":"#### STATUS, INTKEY,OBJECTID, INCKEY, COLDETKEY,\tREPORTNO, EXCEPTRSNCODE,\tEXCEPTRSNDESC, INCDATE, INATTENTIONIND, PEDROWNOTGRNT, SDOTCOLNUM, SEGLANEKEY, CROSSWALKKEY has a lot of missing values and they are not useful and irrelevant for this dataset. so they are too dropped","cb0d4fb5":"**K-fold Cross-Validation**\n\n#### In k-fold cross-validation, the data is divided into k folds. The model is trained on k-1 folds with one fold held back for testing. This process gets repeated to ensure each fold of the dataset gets the chance to be the held back set. Once the process is completed, we can summarize the evaluation metric using the mean or\/and the standard deviation. \n\n**Stratified K-fold Cross-Validation**\n\n#### Stratified K-Fold approach is a variation of k-fold cross-validation that returns stratified folds, i.e., each set containing approximately the same ratio of target labels as the complete data.","e6dc5cb0":"#### this column is similar to the junc_type","afb32aeb":"##### The next step is to divide the dataset into data values and labels that means to x and y .Then normalizing the data for feature scaling, passing the data to the train_test_split function.here ,we split the data into training data and testing data as x_train, x_test, y_train, y_test by choosing a split ratio of 0.2 ,means 20% of data used for testing","93f1228f":"#### The obtained dataset has a lot of missing values. lets first eliminate the unwanted parts of the dataset","381acca2":"### **Conclusion**\n\n#### Initially, The classifiers had an prediction accuracy of 66%-71%, however, upon going back to the data preparation and taking additional fields in the dataset improved the overall accuracy of all models.\n\n#### The accuracy of the classifiers is excellent, i.e. 100%. This means that the model has trained well and fits the training data and performs well on the testing set as well as the training set. We can conclude that this model can accurately predict the severity of car accidents in Seattle.","cc55dea7":"#### The accuracies of all the  models is 100% which means we can accurately predict the severity of an accident. A bar plot is plotted below with the bars representing the accuracy of each model.","e761cde9":"##### The speeding column had a lot of missing values and consists of only 'y' means who are speedy ,converting Y->1 and nan->0 ,thus the speeding column's looking good","f1f8749c":"### **Results**","958e106e":"##### using one hot encoding to change the categorical variables. Its good to use pd.get_dummies ,its another way for one hot encoding as the column names of category comes along with it and its good to see.","20128583":"**severity_code**\n#### A code that corresponds to the severity of the collision","ca7b80b3":"#### 1 - Property damage only collision(collison causing damages only to properties)\n#### 2 - injury collision(collison which caused injuries to the pedestrians or passenger involved in the collision)\n#### 0 - unknown\n#### 2b - the collision caused serious injuries to pedestrians or passengers\n#### 3 - the collision lead to some death","3db134b8":"#### lets take a look at the dataset","2feab806":"**roadcond**\n#### The condition of the road during the collision","968f9a8e":"#### The process of modeling means training a machine learning algorithm to predict the labels from the features, tuning it for the business need, and validating it on holdout data.The output from modeling is a trained model that can be used for inference, making predictions on new data points.\n#### Model Evaluation is an integral part of the model development process. It helps to find the best model that represents our data and how well the chosen model will work in the future. To avoid overfitting, both methods use a test set (not seen by the model) to evaluate model performance\n\n#### Decision Tree makes decision with tree-like model. It splits the sample into two or more homogenous sets (leaves) based on the most significant differentiators in the input variables. To choose a differentiator (predictor), the algorithm considers all features and does a binary split on them (for categorical data, split by category; for continuous, pick a cut-off threshold). It will then choose the one with the least cost (i.e. highest accuracy), and repeats recursively, until it successfully splits the data in all leaves (or reaches the maximum depth).\n\n#### Information gain for a decision tree classifier can be calculated either using the Gini Index measure or the Entropy measure, whichever gives a greater gain. A hyper parameter Decision Tree Classifier was used to decide which tree to use, DTC using entropy had greater information gain; hence it was used for this classification problem.\n","865bb4ab":"##### Correlation is a statistical technique that can show whether and how strongly pairs of variables are related. Finding the correlation among the features of the dataset helps understand the data better. For example, in the heatmap shown below, it can be observed that some features have a strong positive \/ negative correlation while most of them have weak \/ no correlation.","15a60eae":"**severity_desc**\nA detailed description of the severity of the collision","88eb2f56":"#### below the seattle collision dataset is loaded using pandas","98abcd6e":"### **Data Understanding**","73ad4b69":"**Confusion Matrix**\n\n#### The confusion matrix (or error matrix) is one way to summarize the performance of a classifier for binary classification tasks. This square matrix consists of columns and rows that list the number of instances as absolute or relative \"actual class\" vs. \"predicted class\" ratios.\n\n"}}