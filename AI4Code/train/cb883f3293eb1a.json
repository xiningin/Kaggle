{"cell_type":{"d1e739d3":"code","1f5ca89f":"code","c5391af6":"code","6138db41":"code","60dfe438":"code","7d89568d":"code","2e29bbe7":"code","282575f3":"code","2b459801":"code","981311f2":"code","8ce28710":"code","2f908356":"code","346c4864":"code","f1337386":"code","d395e583":"code","fe8dfcff":"code","d686d484":"code","23546a8c":"code","6c1ed991":"code","fe5160ed":"code","cab7b7ad":"code","dfb24836":"code","ccc8965c":"code","0cebc5a9":"code","47e53220":"code","4aca8059":"code","780703a3":"code","4dc6908f":"code","b700fe59":"code","a19fdf4f":"code","2eb70428":"code","e330a652":"code","68827fb1":"code","a3d559ba":"code","43ba1c7e":"code","cc4d1f85":"code","5c01dc26":"code","12602a2d":"code","290ec0e1":"code","6ff97339":"code","ac259c2a":"code","25f6e478":"code","b2509480":"code","76950389":"code","7e0164fe":"markdown","3f09e7b3":"markdown","722d861e":"markdown","f7971c03":"markdown","76002974":"markdown","0f47c048":"markdown","ad561900":"markdown","7e6d9eda":"markdown","280e1077":"markdown","1273ea3e":"markdown","5b8222fc":"markdown","1d8c99e6":"markdown","987bfb1d":"markdown","dd0f4b2c":"markdown","878a8ba0":"markdown","f469b089":"markdown","0d7a0e70":"markdown","4c50783f":"markdown","89196367":"markdown","734e0481":"markdown","d192d762":"markdown","55b82007":"markdown","955c356e":"markdown","c0c3a182":"markdown","67d7b8d4":"markdown","54dc2a24":"markdown","f8faeffc":"markdown","c05d53f1":"markdown","c97ddcaa":"markdown","3ac82361":"markdown","61ca8e12":"markdown","2de6e42c":"markdown","12f05ff3":"markdown","d49bcc82":"markdown"},"source":{"d1e739d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn import svm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import f1_score\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1f5ca89f":"ds_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\nds_output = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","c5391af6":"ds_train","6138db41":"ds_output","60dfe438":"sns.catplot(x=\"Sex\", y=\"Survived\", data=ds_train, kind=\"bar\", height = 4)","7d89568d":"sns.catplot(x=\"Embarked\", y =\"Survived\", data=ds_train, kind=\"bar\", height=4)","2e29bbe7":"sns.catplot(x=\"Pclass\", y=\"Age\", hue=\"Survived\", data=ds_train, height = 4)","282575f3":"sns.catplot(x=\"Sex\", y=\"Age\", hue=\"Survived\", data=ds_train, height=4)","2b459801":"sns.heatmap(ds_train[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\",\"Fare\",\"Embarked\", \"Survived\"]].corr(), annot = True)","981311f2":"X_train = ds_train.drop(['PassengerId','Name','Ticket'],axis=1)\nX_output = ds_output.drop(['PassengerId','Name','Ticket'],axis=1)","8ce28710":"X_train['Cabin'].isna().sum()\/ X_train.shape[0]","2f908356":"X_train = X_train.drop(['Cabin'],axis=1)\nX_output = X_output.drop(['Cabin'],axis=1)","346c4864":"print(f\"Count : {X_train['Embarked'].isna().sum()} \/ {X_train.shape[0]} = {X_train['Embarked'].isna().sum()\/X_train.shape[0]}\")\nprint(f\"Most common letter : {np.squeeze(X_train['Embarked'].mode())}\")","f1337386":"X_train['Embarked'].fillna(np.squeeze(X_train['Embarked'].mode()), inplace=True)\nX_output['Embarked'].fillna(np.squeeze(X_output['Embarked'].mode()), inplace=True)","d395e583":"#numerical data\nprint(f\"Median age : {X_train['Age'].median()}\")\nX_train['Age'].fillna(X_train['Age'].median(), inplace=True)\nX_output['Age'].fillna(X_output['Age'].median(), inplace=True)\nX_train['Fare'].fillna(X_train['Fare'].median(), inplace=True)\nX_output['Fare'].fillna(X_output['Fare'].median(), inplace=True)","fe8dfcff":"letter_dictionnary = X_train['Embarked'].astype('category').cat.categories #will be used later for prettier columns","d686d484":"X_train['Sex']=X_train['Sex'].astype('category').cat.codes\nX_output[\"Sex\"]=X_output[\"Sex\"].astype('category').cat.codes","23546a8c":"X_train['Embarked']=X_train['Embarked'].astype('category').cat.codes\nX_output['Embarked']=X_output[\"Embarked\"].astype('category').cat.codes","6c1ed991":"X_train = pd.concat([X_train, pd.get_dummies(X_train.Pclass)], axis=1)\nX_output = pd.concat([X_output, pd.get_dummies(X_output.Pclass)], axis=1)\nX_train.rename(columns={1: \"Pclass1\", 2: \"Pclass2\",3:\"Pclass3\"}, inplace=True)\nX_output.rename(columns={1: \"Pclass1\", 2: \"Pclass2\",3:\"Pclass3\"}, inplace=True)\nX_train.drop(['Pclass'],inplace=True,axis=1)\nX_output.drop(['Pclass'],inplace=True,axis=1)","fe5160ed":"X_train = pd.concat([X_train, pd.get_dummies(X_train.Embarked)], axis=1)\nX_output = pd.concat([X_output, pd.get_dummies(X_output.Embarked)], axis=1)\nX_train.rename(columns={0: f\"Embarked_{letter_dictionnary[0]}\", 1: f\"Embarked_{letter_dictionnary[1]}\",2: f\"Embarked_{letter_dictionnary[2]}\"}, inplace=True)\nX_output.rename(columns={0: f\"Embarked_{letter_dictionnary[0]}\", 1: f\"Embarked_{letter_dictionnary[1]}\",2: f\"Embarked_{letter_dictionnary[2]}\"}, inplace=True)\nX_train.drop(['Embarked'],inplace=True,axis=1)\nX_output.drop(['Embarked'],inplace=True,axis=1)","cab7b7ad":"def normalization(column):\n  return (column-column.mean())\/column.std()","dfb24836":"X_train['Age'],X_train['Fare'] = normalization(X_train['Age']),normalization(X_train['Fare'])","ccc8965c":"X_output['Age'],X_output['Fare'] = normalization(X_output['Age']),normalization(X_output['Fare'])","0cebc5a9":"y_train = X_train['Survived']\nX_train = X_train.drop(['Survived'], axis=1)","47e53220":"X_train.reset_index(drop=True, inplace=True)\ny_train.reset_index(drop=True, inplace=True)\nX_output.reset_index(drop=True, inplace=True)","4aca8059":"X_train,X_test,y_train,y_test = train_test_split(X_train,y_train)","780703a3":"X_train","4dc6908f":"hyperparameters = {'C': np.arange(0.5,4,0.2), 'kernel':['linear', 'rbf', 'sigmoid'], 'gamma':np.linspace(10**-3,1,50)}\ngrid = GridSearchCV(svm.SVC(), hyperparameters, cv=5)\ngrid.fit(X_train,y_train)\nC, kernel, gamma = grid.best_params_['C'],grid.best_params_['kernel'],grid.best_params_['gamma']    \nprint(f\"Best score : {grid.best_score_} for hyperparameters : {grid.best_params_}\")","b700fe59":"inertia=[]\nn_clusters_list=[]\nfor n_clusters in range(1,200):\n    KM = KMeans(n_clusters=n_clusters)\n    KM.fit(X_train,y_train)\n    inertia.append(KM.inertia_)\n    n_clusters_list.append(n_clusters)\nplt.plot(n_clusters_list,inertia)","a19fdf4f":"from sklearn.metrics.cluster import completeness_score\nKM = KMeans(n_clusters=30)\nKM.fit(X_train,y_train)\nprint(f\"Best score : {completeness_score(KM.predict(X_test),y_test)} for hyperparameters : n_clusters = 15\")","2eb70428":"hyperparameters = {'n_neighbors': np.arange(1,50), 'metric': ['euclidean','manhattan']}\ngrid = GridSearchCV(KNeighborsClassifier(), hyperparameters, cv=5)\ngrid.fit(X_train,y_train)\nn_neighbors,metric = grid.best_params_['n_neighbors'],grid.best_params_['metric']\nprint(f\"Best score : {grid.best_score_} for hyperparameters : {grid.best_params_}\")","e330a652":"hyperparameters = {'n_estimators': [1500,2000,2500,3000,4000],'max_features':['auto','sqrt','log2'], 'max_depth':[10,20,30]}\ngrid = GridSearchCV(RandomForestClassifier(), hyperparameters, cv=5)\ngrid.fit(X_train,y_train)\nn_estimators,max_features,max_depth = grid.best_params_['n_estimators'],grid.best_params_['max_features'],grid.best_params_['max_depth']\nprint(f\"Best score : {grid.best_score_} for hyperparameters : {grid.best_params_}\")","68827fb1":"hyperparameters = {'n_estimators': [500,700,800,900,1000],'max_features':['auto','sqrt','log2'], 'max_depth':[10,20,30]}\n\ngrid = GridSearchCV(ExtraTreesClassifier(), hyperparameters, cv=5)\ngrid.fit(X_train,y_train)\nn_e,max_f,max_d= grid.best_params_['n_estimators'],grid.best_params_['max_features'],grid.best_params_['max_depth']\nprint(f\"Best score : {grid.best_score_} for hyperparameters : {grid.best_params_}\")","a3d559ba":"best_mean_score = 0","43ba1c7e":"sv = svm.SVC(C=C, kernel=kernel, gamma=gamma)\nsv.fit(X_train,y_train)\nsv_mean_score, sv_std_score = cross_val_score(sv,X_test, y_test, cv = 5).mean(),cross_val_score(sv,X_test, y_test, cv = 5).std()\nprint(sv_mean_score, sv_std_score)\nif sv_mean_score>best_mean_score:\n    best_mean_score = sv_mean_score\n    model = \"sv\"","cc4d1f85":"knn = KNeighborsClassifier(n_neighbors, metric=metric)\nknn.fit(X_train,y_train)\nknn_mean_score, knn_std_score = cross_val_score(knn,X_test, y_test, cv = 5).mean(),cross_val_score(knn,X_test, y_test, cv = 5).std()\nprint(knn_mean_score, knn_std_score)\nif knn_mean_score>best_mean_score:\n    best_mean_score = knn_mean_score\n    model = \"knn\"","5c01dc26":"rdf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features)\nrdf.fit(X_train,y_train)\nrdf_mean_score, rdf_std_score = cross_val_score(rdf, X_test, y_test, cv = 5).mean(),cross_val_score(rdf, X_test, y_test, cv = 5).std()\nprint(rdf_mean_score, rdf_std_score)\nif rdf_mean_score>best_mean_score:\n    best_mean_score = rdf_mean_score\n    model = \"rdf\"","12602a2d":"clf = ExtraTreesClassifier(n_estimators=n_e, max_depth=max_d,max_features=max_f )\nclf.fit(X_train,y_train)\nclf_mean_score, clf_std_score = cross_val_score(clf, X_test, y_test, cv = 5).mean(),cross_val_score(clf, X_test, y_test, cv = 5).std()\nprint(clf_mean_score, clf_std_score)\nif clf_mean_score>best_mean_score:\n    best_mean_score = clf_mean_score\n    model = \"clf\"","290ec0e1":"if model==\"svm\":\n    y_output = sv.predict(X_output)\nelif model==\"knn\":\n    y_output = knn.predict(X_output)\nelif model==\"rdf\":\n    y_output = rdf.predict(X_output)\nelse:\n    y_output = clf.predict(X_output)\nprint(\"Chosen model :\", model)","6ff97339":"submission = ds_output[\"PassengerId\"]","ac259c2a":"submission = pd.DataFrame(submission)","25f6e478":"submission['Survived']=y_output","b2509480":"submission.set_index('PassengerId',drop=True, inplace=True)","76950389":"submission.to_csv(r'\/kaggle\/working\/submission.csv')","7e0164fe":"In order to choose our model for the submission, we will use the cross validation.","3f09e7b3":"## 4.5. Fith model : ExtraTreesClassifier from sklearn","722d861e":"We can see that the best value of *n_clusters* is around 30, the end of the elbow.","f7971c03":"Here we can see that 77% of the column *Cabin* is NaN. As I am hesitant of the importance of such feature, the fact of having so little information makes me decide to get rid of this column. ","76002974":"## 1.1. Imports","0f47c048":"# 4. Models","ad561900":"We achieve to find 80% good predictions. I would also add that changing categorical data to binary columns didn't seem to change the results in a relevant way.","7e6d9eda":"We are going to remove useless data for prediction which are *PassengerId*, *Ticket* or *Name*, and categorize the string data. Then, we have to handle NaN values. We don't want to lose the whole raw for a null age, so we are going to replace NaN value by median values of their column. Finally we will normalize numerical data. ","280e1077":"# 2. Data Visualization","1273ea3e":"A final look at the dataset before creating the model.","5b8222fc":"We have a terrible result. However, even if this is not the good model to fit our data, it would not be relevant to rise the numbers of clusters even if it increases our score, as it is overfitting so our model will lose the ability to generalize.","1d8c99e6":"This is my first try on a competition, trying on the famous titanic dataset. The goal of this notebook is to find a correlation between particular attributes of passengers and the fact that they survived or not the sinking of the titanic. The submission is a csv containing the prediction of the survival of the 418 passengers in the test dataset.","987bfb1d":"# 5. Find best model","dd0f4b2c":"## 4.4. Fourth model : RandomForestClassifier from sklearn \n","878a8ba0":"## 4.3. Third Model : K Neighbors from sklearn","f469b089":"# I. Introduction","0d7a0e70":"We are going to change strings into categories and then categories to binary columns.","4c50783f":"This is the data on which we have to make a prediction to submit to the competition.","89196367":"## 3.3. Categorization","734e0481":"# 6. Submitting the results","d192d762":"I noticed that the *Embarked* column has not many NaN, so I decided to replace them by the most frequent letter of the column.","55b82007":"We find around **81%** of good predictions.","955c356e":"We see on the heatmap that *Fare* has a more important correlation with the survival passengers.","c0c3a182":"# 3. Data Preprocessing","67d7b8d4":"This model also finds around **81%** of good predictions.","54dc2a24":"## 3.2. Handling NaN elements","f8faeffc":"## 4.1. First Model : SVM from sklearn","c05d53f1":"We find around **81%** of good predictions like the first model.","c97ddcaa":"Normalization is relevant as we certainly don't have outliers in our dataset. ","3ac82361":"## 3.4. Normalization","61ca8e12":"For each model, we will search its best parameters and then we will compare the models. However, we need to separate our dataset in three parts, the train set which will be used to train our model, the validation set which will be used to find the best parameters (using cross validation), and finally, the test set which will be used to generate scores in order to choose our best model. ","2de6e42c":"Instead of using GridSearchCV, another way to find the best *n_clusters* with Kmeans is to plot intertia (the opposite of the score in this case) against the numbers of neighbors. We will see an elbow in the curve, this is the best values in order to generalize and avoid overfitting. This is called the **elbow method**.","12f05ff3":"## 3.1. Deleting columns","d49bcc82":"## 4.2. Second Model : KMeans from sklearn"}}