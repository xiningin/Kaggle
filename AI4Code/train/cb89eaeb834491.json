{"cell_type":{"78c03712":"code","2db246de":"code","d7d20662":"code","3c8fba5d":"code","69b9c1c5":"code","af44268e":"code","127c47e3":"code","7de76ee9":"code","e3340b70":"code","262eefa8":"code","a49456b0":"code","c707c786":"code","da91cb36":"code","09f687cf":"code","af4a629b":"code","a026d055":"code","90e4a15b":"code","137061f8":"markdown","18fa7aad":"markdown","e9b85c2b":"markdown","82840af9":"markdown","390cba78":"markdown","b978601c":"markdown","18ae5284":"markdown","c81e06ae":"markdown"},"source":{"78c03712":"# start with loading lib\n#load packages\nimport numpy as np \nimport pandas as pd \n\n# keras for NN\nfrom keras.layers import Dense\nfrom keras.models import Sequential\n\n#Common Model Helpers\nfrom sklearn import model_selection\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import confusion_matrix,precision_score\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\n\n#Configure Visualization Defaults\n%matplotlib inline \nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","2db246de":"#import data\n# train is used to optimize a model, as the column survived is present (so it will be split at some point in train and test)\n# validation does not have survived and is used for kaggle competition\n\ntrain_data = pd.read_csv('..\/input\/train.csv')\nvalidation_data  = pd.read_csv('..\/input\/test.csv')\n\n# can clean both datasets at once\ndata_cleaner = [train_data, validation_data]\n\ntrain_data.sample(5)","d7d20662":"train_data.describe(include = 'all')","3c8fba5d":"print('Train columns with null values:\\n', train_data.isnull().sum())\nprint(\"-\"*10)\n\nprint('Test\/Validation columns with null values:\\n', validation_data.isnull().sum())\nprint(\"-\"*10)","69b9c1c5":"# complete or delete missing values in train and test\/validation dataset\nfor dataset in data_cleaner:    \n    #complete missing age with median\n    dataset['Age'].fillna(dataset['Age'].median(), inplace = True)\n\n    #complete embarked with mode\n    dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace = True)\n\n    #complete missing fare with median\n    dataset['Fare'].fillna(dataset['Fare'].median(), inplace = True)\n    \n    #delete column Cabin and Ticket in dataset\n    drop_column = ['Cabin', 'Ticket']\n    dataset.drop(drop_column, axis=1, inplace = True)\n    \n#delete column PassengerId in train_data\ndrop_column = ['PassengerId']\ntrain_data.drop(drop_column, axis=1, inplace = True)\n\nprint(train_data.isnull().sum())\nprint(\"-\"*10)\nprint(validation_data.isnull().sum())","af44268e":"###CREATE: Feature Engineering for train and test\/validation dataset\nfor dataset in data_cleaner:    \n    #Discrete variables\n    dataset['FamilySize'] = dataset ['SibSp'] + dataset['Parch'] + 1\n\n    dataset['IsAlone'] = 1 #initialize to yes\/1 is alone\n    dataset['IsAlone'].loc[dataset['FamilySize'] > 1] = 0 # now update to no\/0 if family size is greater than 1\n\n    #quick and dirty code split title from name: http:\/\/www.pythonforbeginners.com\/dictionary\/python-split\n    dataset['Title'] = dataset['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]\n\n\n    #Continuous variable bins; qcut vs cut: https:\/\/stackoverflow.com\/questions\/30211923\/what-is-the-difference-between-pandas-qcut-and-pandas-cut\n    #Fare Bins\/Buckets \n    dataset['FareBin'] = pd.qcut(dataset['Fare'], 4)\n\n    #Age Bins\/Buckets \n    dataset['AgeBin'] = pd.cut(dataset['Age'].astype(int), 5)\n\n    #cleanup rare title names\n    stat_min = 10 # common minimum in statistics: http:\/\/nicholasjjackson.com\/2012\/03\/08\/sample-size-is-10-a-magic-number\/\n    title_names = (dataset['Title'].value_counts() < stat_min) #this will create a true false series with title name as index\n\n    #apply and lambda functions are quick and dirty code to find and replace with fewer lines of code: https:\/\/community.modeanalytics.com\/python\/tutorial\/pandas-groupby-and-python-lambda-functions\/\n    dataset['Title'] = dataset['Title'].apply(lambda x: 'Misc' if title_names.loc[x] == True else x)\n\ntrain_data.sample(3)","127c47e3":"#CONVERT: convert objects to category using Label Encoder for train and test\/validation dataset\n\n#code categorical data\nlabel = LabelEncoder()\nfor dataset in data_cleaner:    \n    dataset['Sex_Code'] = label.fit_transform(dataset['Sex'])\n    dataset['Embarked_Code'] = label.fit_transform(dataset['Embarked'])\n    dataset['Title_Code'] = label.fit_transform(dataset['Title'])\n    dataset['AgeBin_Code'] = label.fit_transform(dataset['AgeBin'])\n    dataset['FareBin_Code'] = label.fit_transform(dataset['FareBin'])\n\n    \n# Now we can drop some column \n#delete column Cabin and Ticket in test dataset\nfor dataset in data_cleaner:\n    drop_column = ['Name', 'Sex','Embarked','Title','FareBin','AgeBin']\n    dataset.drop(drop_column, axis=1, inplace = True)\n","7de76ee9":"train_data.sample(3)","e3340b70":"validation_data.sample(3)","262eefa8":"#split train to train and test data with function defaults (as validation is only for kaggle compet.. )\nseed = 4\n\nxt = train_data.drop('Survived', axis=1)\nyt = train_data['Survived']\nx_train, x_test, y_train, y_test = model_selection.train_test_split(xt, yt, random_state = seed)\nprint(x_train.shape)\nx_train.sample(3)","a49456b0":"#NN using keras\nclass NN_keras():\n    def __init__(self,nbneuron, indim):\n        # create model\n        self.model = Sequential()\n        self.model.add(Dense(nbneuron, input_dim=indim, kernel_initializer='normal', activation='relu'))\n        self.model.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n        # Compile model\n        self.model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n    \n    def __call__(self):\n        return self.model","c707c786":"model = NN_keras(nbneuron=24,indim=12).__call__()\nmodel.summary()\n\nmodel_output = model.fit(x_train,y_train,epochs = 200,batch_size = 10,validation_data=(x_test,y_test), verbose=0)","da91cb36":"# show model accuracy\nplt.subplot(211)\nacc_data = pd.DataFrame({'train_acc': model_output.history[\"acc\"], 'test_acc': model_output.history[\"val_acc\"]})\nsns.lineplot(data = acc_data, palette=\"tab10\", linewidth=2.5)\n\n# show model loss\nplt.subplot(212)\nloss_data = pd.DataFrame({'train_loss': model_output.history[\"loss\"], 'test_loss': model_output.history[\"val_loss\"]})\nsns.lineplot(data = loss_data, palette=\"tab10\", linewidth=2.5)\n\nprint('Training Accuracy : ', np.mean(model_output.history[\"acc\"]))\nprint('Validation Accuracy : ', np.mean(model_output.history[\"val_acc\"]))","09f687cf":"y_pred = model.predict(x_test)\nrounded = [round(x[0]) for x in y_pred]\ny_pred1 = np.array(rounded,dtype='int64')\nconfusion_matrix(y_test,y_pred1)","af4a629b":"precision_score(y_test,y_pred1)","a026d055":"x_val = validation_data.drop(\"PassengerId\", axis=1)\ny_val = model.predict(x_val)","90e4a15b":"#submission = pd.DataFrame({\n#        \"PassengerId\": validation_data[\"PassengerId\"],\n#        \"Survived\": y_val.round().astype(int).flatten()})\n\n#submission.to_csv('..\/output\/titanic_keras.csv', index=False)\n#submission.sample(10)","137061f8":"# Load lib and data \nfrom dataset in Kaggle: \"Titanic: Machine Learning from Disaster\"","18fa7aad":"# Preparing Data - Cleaning\n## Correcting aberrant values and outliers","e9b85c2b":"## Create new data ","82840af9":"This notebook is based on : https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy\n\n# Problem statement\nSinking of the Titanic in 1912, kill 1502 out of 2224 passengers.\nFrom passengers informations, was it possible to predict which passengers would survive the tragedy?","390cba78":"\n#  Model with Keras\n","b978601c":"From the output above, it doesnt seems there is any aberrant value or outliers\n\n## Completing missing value","18ae5284":"We can observe that there is some missing data in age and cabin and Embarked, and 1 missing fare in validation data","c81e06ae":"## Converting data for analysis"}}