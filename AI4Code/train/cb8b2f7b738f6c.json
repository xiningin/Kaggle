{"cell_type":{"cbc4c75e":"code","887fc413":"code","318b4033":"code","25bb59c2":"code","b15fc29a":"code","48313f5d":"code","606268cc":"code","8c29ec74":"code","f9fa0fdf":"code","c44b7f83":"code","2d04b971":"code","1f21e443":"code","8f918a78":"code","f6af6037":"code","cd92f84e":"code","f8b25d33":"code","9c73a9a6":"code","2e0e9247":"markdown","092bc3b0":"markdown","3879de54":"markdown","577b957c":"markdown","4c6794cb":"markdown","3c3e111b":"markdown","d5b55c41":"markdown","1388f945":"markdown"},"source":{"cbc4c75e":"import matplotlib\nimport matplotlib.pyplot as plt\nimport torch\nimport pandas as pd\nimport numpy as np\nprint(f'matplotlib: {matplotlib.__version__}')\nprint(f'pytorch   : {torch.__version__}')\nprint(f'pandas    : {pd.__version__}')\nprint(f'numpy     : {np.__version__}')","887fc413":"# Load the data\ndf = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf.describe()","318b4033":"df.head(10)","25bb59c2":"from sklearn import preprocessing\n\ndef format_feats(in_feats):\n    x = in_feats.values #returns a numpy array\n    min_max_scaler = preprocessing.MinMaxScaler()\n    x_scaled = min_max_scaler.fit_transform(x)\n    return pd.DataFrame(x_scaled, columns=in_feats.columns)\n\n# Apply some data formatting\ndef format_data(data):\n    # One-hot encode 'Embarked' column\n    data = pd.get_dummies(data, columns=['Sex','Embarked'])\n    # Drop columns that require additional processing\n    data = data.drop(['Name','Ticket','Cabin'], axis=1)\n    # Fill null values with the mean of the column\n    data.fillna(data.mean(), inplace=True)\n    if 'Survived' in data.columns:\n        data_y = data['Survived']\n        data_x = data.drop(['Survived'], axis=1)\n        data_x = format_feats(data_x)\n        return data_x, data_y\n    else:\n        return format_feats(data)\n\n# This should split the data into our features and our labels\nfeats, labels = format_data(df)\nfeats.describe()","b15fc29a":"# Split the data into training and testing samples\n# The training sample should consist of ~80% of our data\nmask  = np.random.rand(len(feats)) < 0.8\ntrain_X = feats[mask]\ntrain_y = labels[mask]\ntest_X  = feats[~mask]\ntest_y  = labels[~mask]\n\n# Look at the training sample\ntrain_X.describe()\nprint(train_X.describe(), test_y.describe())","48313f5d":"# Format the data into PyTorch tensors\ntrn_X = torch.Tensor(train_X.to_numpy())\ntrn_y = torch.Tensor(train_y.to_numpy()).type(torch.LongTensor)\ntst_X = torch.Tensor(test_X.to_numpy())\ntst_y = torch.Tensor(test_y.to_numpy()).type(torch.LongTensor)\n\n# Get the number of inputs\ndrpout = 0.2","606268cc":"# Generate the model\nfrom torch import nn\n\n# Set Dropout rate\ndrpout = 0.1\n# Define number of inputs\ninputs = len(trn_X[0])\n\n# Method for initializing weights and biases\ndef set_weight_bias(layer):\n    layer.bias.data.fill_(0)\n    layer.weight.data.normal_(std=0.01)\n\n# Create a function for model construction\n# This will help \ndef model_construct(inputs, n=[16], outputs=2,\n                    activ=nn.ReLU):\n    # Add the outputs to the list of nodes\n    n.append(outputs)\n    \n    # Input layer\n    layers = []\n    layers.append(nn.Linear(inputs, n[0]))\n    set_weight_bias(layers[-1])\n    layers.append( nn.Dropout(p=drpout) )\n    layers.append(activ())\n    \n    # Loop over the hidden layers\n    for i in range(len(n)-1):\n        layers.append(nn.Linear(n[i], n[i+1]))\n        set_weight_bias(layers[-1])\n        layers.append( nn.Dropout(p=drpout) )\n        layers.append(activ())\n        \n    # Remove the last dropout layer\n    layers.pop()\n    layers.pop()\n    # Change final activation function\n    #layers[-1] = nn.Softmax(dim=1)\n    \n    # Put it all together\n    return nn.Sequential(*layers)","8c29ec74":"# Write another function for training and testing the model\nfrom torch import optim\nfrom sklearn.utils import shuffle\nfrom torch.autograd import Variable\n\ndef train_model(model, train_data, test_data, epochs=5, verbose=False):\n    \n    # Setup\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.01)\n    \n    # Loop over the epochs\n    train_losses, test_losses = [0]*epochs, [0]*epochs\n    accuracy = [0]*epochs\n    for e in range(epochs):\n        \n        # Iterate the model, note we are passing in the\n        # entire training set as a single batch\n        optimizer.zero_grad()\n        ps = model(train_data[0])\n        loss = criterion(ps, train_data[1])\n        loss.backward()\n        optimizer.step()\n        train_losses[e] = loss.item()\n\n        # Compute the test stats\n        with torch.no_grad():\n            # Turn on all the nodes\n            model.eval()\n            \n            # Comput test loss\n            ps = model(test_data[0])\n            loss = criterion(ps, test_data[1])\n            test_losses[e] = loss.item()\n            \n            # Compute accuracy\n            top_p, top_class = ps.topk(1, dim=1)\n            equals      = (top_class == test_data[1].view(*top_class.shape))\n            accuracy[e] = torch.mean(equals.type(torch.FloatTensor))\n            \n        model.train()\n        \n    # Print the final information\n    print(f'   Accuracy  : {100*accuracy[-1].item():0.2f}%')\n    print(f'   Train loss: {train_losses[-1]}')\n    print(f'   Test loss : {test_losses[-1]}')\n        \n    # Plot the results\n    plt.subplot(211)\n    plt.ylabel('Accuracy')\n    plt.plot(accuracy)\n    plt.subplot(212)\n    plt.ylabel('Loss')\n    plt.plot(train_losses, label='train')\n    plt.plot(test_losses, label='test')\n    plt.legend();\n    return","f9fa0fdf":"# Give it a try\nprint(\"Test 1:\")\nmodel = model_construct(inputs, n=[256])\nprint(model)\ntrain_model(model, epochs=100,\n            train_data=(trn_X,trn_y), test_data=(tst_X,tst_y))","c44b7f83":"print(\"Test 2:\")\nmodel = model_construct(inputs, n=[256, 64])\nprint(model)\ntrain_model(model, epochs=200,\n            train_data=(trn_X,trn_y), test_data=(tst_X,tst_y))","2d04b971":"print(\"Test 3:\")\nmodel = model_construct(inputs, n=[16])\nprint(model)\ntrain_model(model, epochs=1000,\n            train_data=(trn_X,trn_y), test_data=(tst_X,tst_y))","1f21e443":"# Assign the training data to the full training set\ntrn_X = torch.Tensor(feats.to_numpy())\ntrn_y = torch.Tensor(labels.to_numpy()).type(torch.LongTensor)\n\n# Construct and fit the model\nmodel = model_construct(inputs, n=[16])\ntrain_model(model, epochs=100,\n            train_data=(trn_X,trn_y), test_data=(tst_X,tst_y))","8f918a78":"def gen_model(scale):\n    \"\"\"\n    Generate and fit a model\n\n    Parameters\n    ----------\n    scale : int\n        Number of models that are being trained\n    \n    Returns\n    -------\n    Model generated and trained.\n    \"\"\"\n    # Generate a model\n    mod = model_construct(inputs, n=[32,16])\n    \n    # Update the data subset\n    mask  = np.random.rand(len(feats)) < 1.0\/(scale\/2.0)\n    train_dat = (torch.Tensor(feats[mask].to_numpy()),\n                 torch.Tensor(labels[mask].to_numpy()).type(torch.LongTensor))\n    test_dat = (torch.Tensor(feats[~mask].to_numpy()),\n                torch.Tensor(labels[~mask].to_numpy()).type(torch.LongTensor))\n    \n    # Train the model\n    train_model(mod, epochs=100,\n                train_data=train_dat, test_data=test_dat)\n    \n    # Return the trained model\n    return mod","f6af6037":"# Function to combine the probabilities from all models\ndef combined_pred(models, data):\n    # Loop through models and get the probabilities\n    prob = np.array([[0.0]*2]*len(data))\n    with torch.no_grad():\n        for mod in models:\n            mod.eval()\n            prob += torch.exp(mod(data)).numpy()\n            mod.train()\n    return prob\/len(models)","cd92f84e":"# Generate a fit a group of models\nmodels = []\nmod_size = 4\nfor i in range(mod_size):\n    models.append(gen_model(mod_size))\npreds = combined_pred(models, tst_X)\ntop_class = np.argmax(preds, axis=1)\nequals = (top_class == tst_y.numpy())\nprint(100*np.mean(equals))","f8b25d33":"# Load and process the testing data\ntest_df    = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest_feats = format_data(test_df)\ntest_feats = torch.Tensor(test_feats.to_numpy())\n\n# Compute the results\n#results          = model(test_feats)\n#top_p, top_class = results.topk(1, dim=1)\nresults = combined_pred(models, test_feats)\ntop_class = np.argmax(results, axis=1)\n\n# Load it all into a dataframe\nsubmission_df = pd.DataFrame({'PassengerId': test_df['PassengerId'], \n                              'Survived'   : top_class})\nsubmission_df.describe()","9c73a9a6":"submission_df.to_csv('submission.csv', index=False)","2e0e9247":"And for training\/testing the model...","092bc3b0":"The formatting that we will apply includes the following:\n* **One-hot encode**: 'Sex', 'Embarked'\n* **Remove**: 'Name', 'Ticket', 'Cabin'\n* **Fill *null* values** with the mean of the associated column.","3879de54":"# Titanic: Machine Learning From Disaster\nIn this notebook I'm going to expand on my previous attempt that used scikit-learn random forests and try to use pytorch as the learning framework this time.\n\nStep 1: Load the modules and see what versions we have installed.","577b957c":"Well, I guess simple wins the day, so we'll go with the 16 node, single hidden layer model. Also, it looks like anything with more than 100 training epochs tends to overfit, so we'll use only 100 epochs.\n\nThe next thing to do is re-train the model using the full training set","4c6794cb":"## Building the model\nNow we need to build a model that is capable of being trained and generating predictions. For this attempt I will be using PyTorch.\n\nNote that we will create a function for generating our model from a list of nodes per layer. This will help us to more easily tune these parameters as we search for the best model.","3c3e111b":"## Submit the Result\nNow generate the test results and save them to a file.","d5b55c41":"## Format the data\nNext step is to format the data so that we can use it to actually train and test our data.","1388f945":"## An ensemble of Networks\nI'm curious if we could get better performance if we did an ensemble of networks, each trained on a sub-sample of the data. For example, we'll train 4 small networks, each on a sample of 50% of the data (randomly sampled)."}}