{"cell_type":{"9e68c6de":"code","964cef86":"code","8103ce53":"code","b6225a6d":"code","873fa57e":"code","292a058b":"code","f31366d3":"code","45074c47":"code","1ffcb88d":"code","017288ac":"code","9364f4a8":"code","87ddebe7":"code","8ca2d8d4":"code","05b72795":"code","9bd8370d":"code","79e72a0c":"code","a4761b50":"code","84a5f596":"code","04446a8b":"code","560eb7ad":"code","1ac82606":"code","620eceec":"code","3a8b386f":"code","ccc147b6":"code","1e181b78":"code","3f91145e":"code","28d88932":"code","518b5dac":"code","bd768d81":"code","cbd6a1fd":"code","ac308b75":"code","fd5f3f7c":"code","62be6b4d":"code","4f92a24d":"code","000c803c":"code","b9591e06":"code","51487cd5":"code","588610bd":"code","068ed9b4":"code","5e85f486":"code","94a42974":"code","0fd1011a":"code","d8213c63":"code","441ef6f0":"code","97b28b20":"code","3ad45efb":"code","c0d6cafd":"code","e1001c3e":"code","228984a1":"code","61097cc2":"code","0b515177":"code","2b5e9369":"code","9bdd82c9":"markdown","b716f307":"markdown","cec0281b":"markdown","726bb865":"markdown","33f35c2c":"markdown","2882b8a4":"markdown","2f9bcdd0":"markdown","0b73c8dd":"markdown","c2aaf38a":"markdown","b3d91e60":"markdown","73dc644e":"markdown","f5947976":"markdown","d9076c36":"markdown","0dc23751":"markdown","b4767b34":"markdown","7b2c79fc":"markdown","b6083714":"markdown","a383dc99":"markdown","c50acd6d":"markdown","25df9230":"markdown","b129cbad":"markdown","1c3d8465":"markdown","e932fc71":"markdown","7060bd58":"markdown","d1dad12d":"markdown","a3ae058a":"markdown","00d2d0aa":"markdown","2bcbe365":"markdown","3567ba0d":"markdown","4872a48e":"markdown","83d45ccc":"markdown","17fec359":"markdown","2d880d0a":"markdown","b0c166da":"markdown","6265779b":"markdown","cce98ee4":"markdown","6a262085":"markdown","5f78fd9e":"markdown","d5b37662":"markdown","a728710a":"markdown","f2845f06":"markdown","03780ae1":"markdown","ab4c1b03":"markdown","d5463b86":"markdown","40bd201f":"markdown","5d239dbe":"markdown","ef5b50e0":"markdown","7f674574":"markdown","60deb1b1":"markdown","4ba6da17":"markdown","6ae2191d":"markdown","c5a2a92e":"markdown","6e39a5cd":"markdown","3cd0c6ad":"markdown","2044533f":"markdown","eac5f4f9":"markdown","1d08f1fd":"markdown","bf2c159d":"markdown"},"source":{"9e68c6de":"!pip install bubbly\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as mtick\nimport matplotlib.gridspec as grid_spec\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.offline as po\nfrom bubbly.bubbly import bubbleplot\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected = True)\n\ndata1 = pd.read_csv('..\/input\/world-happiness-report-with-terrorism\/WorldHappinessReportwithTerrorism-2015.csv')\ndata2 = pd.read_csv('..\/input\/world-happiness-report-with-terrorism\/WorldHappinessReportwithTerrorism-2016.csv')\ndata3 = pd.read_csv('..\/input\/world-happiness-report-with-terrorism\/WorldHappinessReportwithTerrorism-2017.csv')\ndata4 = pd.read_csv('..\/input\/world-happiness\/2018.csv')\ndata5 = pd.read_csv('..\/input\/world-happiness\/2019.csv')\ndata6 = pd.read_csv('..\/input\/world-happiness-report\/2020.csv')\ndata7 = pd.read_csv('..\/input\/global-terrorism-report-for-world-happiness-report\/GlobalTerrorismReport-2015.csv')\ndata8 = pd.read_csv('..\/input\/global-terrorism-report-for-world-happiness-report\/GlobalTerrorismReport-2016.csv')\ndata9 = pd.read_csv('..\/input\/global-terrorism-report-for-world-happiness-report\/GlobalTerrorismReport-2017.csv')","964cef86":"print(data1.columns)\n\nprint(data1.info())\n\nprint(data1.describe())\n\ndf1 = data1[\"country\"]\ndff1 = df1.value_counts()","8103ce53":"x1 = data1.iloc[:,5:].values\ny1 = data1.iloc[:,3:4].values","b6225a6d":"from sklearn.model_selection import train_test_split\nx_train1, x_test1, y_train1, y_test1 = train_test_split(x1, y1, test_size=0.33, random_state=0)","873fa57e":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train1 = sc.fit_transform(x_train1)\nX_test1 = sc.fit_transform(x_test1)\nY_train1 = sc.fit_transform(y_train1)\nY_test1 = sc.fit_transform(y_test1)","292a058b":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x_train1, y_train1)\nprint(\"b0: \", lr.intercept_)\nprint(\"other b: \", lr.coef_)","f31366d3":"y_pred1 = lr.predict(x_test1)\nprediction1 = lr.predict(np.array([[1.16492,0.87717,0.64718,0.23889,0.12348,0.04707,2.29074,542]]))\nprint(\"Prediction is \", prediction1)","45074c47":"y_pred1 = lr.predict(x_test1)\nprediction1 = lr.predict(np.array([[1.198274,1.337753,0.637606,0.300741,0.099672,0.046693,1.879278,181]]))\nprint(\"Prediction is \", prediction1)","1ffcb88d":"low_c = '#dd4124'\nhigh_c = '#009473'\nbackground_color = '#fbfbfb'\nfig = plt.figure(figsize=(12, 10), dpi=150,facecolor=background_color)\ngs = fig.add_gridspec(3, 3)\ngs.update(wspace=0.2, hspace=0.5)\n\nnewdata1 = data1.iloc[:,4:]\ncategorical = [var for var in newdata1.columns if newdata1[var].dtype=='O']\ncontinuous = [var for var in newdata1.columns if newdata1[var].dtype!='O']\n\nhappiness_mean = data1['happinessscore'].mean()\n\ndata1['lower_happy'] = data1['happinessscore'].apply(lambda x: 0 if x < happiness_mean else 1)\n\nplot = 0\nfor row in range(0, 3):\n    for col in range(0, 3):\n        locals()[\"ax\"+str(plot)] = fig.add_subplot(gs[row, col])\n        locals()[\"ax\"+str(plot)].set_facecolor(background_color)\n        locals()[\"ax\"+str(plot)].tick_params(axis='y', left=False)\n        locals()[\"ax\"+str(plot)].get_yaxis().set_visible(False)\n        locals()[\"ax\"+str(plot)].set_axisbelow(True)\n        for s in [\"top\",\"right\",\"left\"]:\n            locals()[\"ax\"+str(plot)].spines[s].set_visible(False)\n        plot += 1\n\nplot = 0\n\nYes = data1[data1['lower_happy'] == 1]\nNo = data1[data1['lower_happy'] == 0]\n\nfor variable in continuous:\n        sns.kdeplot(Yes[variable],ax=locals()[\"ax\"+str(plot)], color=high_c,ec='black', shade=True, linewidth=1.5, alpha=0.9, zorder=3, legend=False)\n        sns.kdeplot(No[variable],ax=locals()[\"ax\"+str(plot)], color=low_c, shade=True, ec='black',linewidth=1.5, alpha=0.9, zorder=3, legend=False)\n        locals()[\"ax\"+str(plot)].grid(which='major', axis='x', zorder=0, color='gray', linestyle=':', dashes=(1,5))\n        locals()[\"ax\"+str(plot)].set_xlabel(variable, fontfamily='monospace')\n        plot += 1\n        \nXstart, Xend = ax0.get_xlim()\nYstart, Yend = ax0.get_ylim()\n\nax0.text(Xstart, Yend+(Yend*0.5), 'Differences Between Happy & Unhappy Countries', fontsize=15, fontweight='bold', fontfamily='sansserif',color='#323232')\nax0.text(Xstart, Yend+(Yend*0.25), 'There are large differences, with GDP & Social Support being clear perhaps more interesting though,unhappy\\ncountries appear to be more generous.', fontsize=10, fontweight='light', fontfamily='monospace',color='gray')\n\nplt.show()","017288ac":"import statsmodels.regression.linear_model as sm\nX1 = np.append(arr = np.ones((158,1)).astype(int), values=x1, axis=1)\nr_ols1 = sm.OLS(endog = y1, exog = X1)\nr1 = r_ols1.fit()\nprint(r1.summary())","9364f4a8":"print(data2.columns)\n\nprint(data2.info())\n\nprint(data2.describe())\n\ndf2 = data2[\"country\"]\ndff2 = df2.value_counts()","87ddebe7":"x2 = data2.iloc[:,6:].values\ny2 = data2.iloc[:,3:4].values","8ca2d8d4":"from sklearn.model_selection import train_test_split\nx_train2, x_test2, y_train2, y_test2 = train_test_split(x2, y2, test_size=0.33, random_state=0)","05b72795":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train2 = sc.fit_transform(x_train2)\nX_test2 = sc.fit_transform(x_test2)\nY_train2 = sc.fit_transform(y_train2)\nY_test2 = sc.fit_transform(y_test2)","9bd8370d":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x_train2, y_train2)\nprint(\"b0: \", lr.intercept_)\nprint(\"other b: \", lr.coef_)","79e72a0c":"y_pred2 = lr.predict(x_test2)\nprediction2 = lr.predict(np.array([[1.06098,0.94632,0.73172,0.22815,0.15746,0.12253,2.08528,422]]))\nprint(\"Prediction is \", prediction2)","a4761b50":"y_pred2 = lr.predict(x_test2)\nprediction2 = lr.predict(np.array([[1.198274,1.337753,0.637606,0.300741,0.099672,0.046693,1.879278,181]]))\nprint(\"Prediction is \", prediction2)","84a5f596":"figure = bubbleplot(dataset = data2, x_column = 'happinessscore', y_column = 'healthlifeexpectancy', \n    bubble_column = 'country', size_column = 'economysituation', color_column = 'region', \n    x_title = \"Happiness Score\", y_title = \"Health Life Expectancy\", title = 'Happiness vs Health Life Expectancy vs Economy',\n    x_logscale = False, scale_bubble = 1, height = 650)\n\npo.iplot(figure)","04446a8b":"import statsmodels.regression.linear_model as sm\nX2 = np.append(arr = np.ones((157,1)).astype(int), values=x2, axis=1)\nr_ols2 = sm.OLS(endog = y2, exog = X2)\nr2 = r_ols2.fit()\nprint(r2.summary())","560eb7ad":"print(data3.columns)\n\nprint(data3.info())\n\nprint(data3.describe())\n\ndf3 = data3[\"country\"]\ndff3 = df3.value_counts()","1ac82606":"x3 = data3.iloc[:,5:].values\ny3 = data3.iloc[:,2:3].values","620eceec":"from sklearn.model_selection import train_test_split\nx_train3, x_test3, y_train3, y_test3 = train_test_split(x3, y3, test_size=0.33, random_state=0)","3a8b386f":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train3 = sc.fit_transform(x_train3)\nX_test3 = sc.fit_transform(x_test3)\nY_train3 = sc.fit_transform(y_train3)\nY_test3 = sc.fit_transform(y_test3)","ccc147b6":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x_train3, y_train3)\nprint(\"b0: \", lr.intercept_)\nprint(\"other b: \", lr.coef_)","1e181b78":"y_pred3 = lr.predict(x_test3)\nprediction3 = lr.predict(np.array([[1.06098,0.94632,0.73172,0.22815,0.15746,0.12253,2.08528,422]]))\nprint(\"Prediction is \", prediction3)","3f91145e":"y_pred3 = lr.predict(x_test3)\nprediction3 = lr.predict(np.array([[1.16492,0.87717,0.64718,0.23889,0.12348,0.04707,2.29074,542]]))\nprint(\"Prediction is \", prediction3)","28d88932":"trace1 = [go.Choropleth(\n               colorscale = 'Electric',\n               locationmode = 'country names',\n               locations = data3['country'],\n               text = data3['country'], \n               z = data3['happinessrank'],\n               )]\n\nlayout = dict(title = 'Happiness Rank',\n                  geo = dict(\n                      showframe = True,\n                      showocean = True,\n                      showlakes = True,\n                      showcoastlines = True,\n                      projection = dict(\n                          type = 'hammer'\n        )))\n\n\nprojections = [ \"equirectangular\", \"mercator\", \"orthographic\", \"natural earth\",\"kavrayskiy7\", \n               \"miller\", \"robinson\", \"eckert4\", \"azimuthal equal area\",\"azimuthal equidistant\", \n               \"conic equal area\", \"conic conformal\", \"conic equidistant\", \"gnomonic\", \"stereographic\", \n               \"mollweide\", \"hammer\", \"transverse mercator\", \"albers usa\", \"winkel tripel\" ]\n\nbuttons = [dict(args = ['geo.projection.type', y],\n           label = y, method = 'relayout') for y in projections]\n\nannot = list([ dict( x=0.1, y=0.8, text='Projection', yanchor='bottom', \n                    xref='paper', xanchor='right', showarrow=False )])\n\n\n# Update Layout Object\nlayout[ 'updatemenus' ] = list([ dict( x=0.1, y=0.8, buttons=buttons, yanchor='top' )])\nlayout[ 'annotations' ] = annot\n\n\nfig = go.Figure(data = trace1, layout = layout)\npo.iplot(fig)","518b5dac":"import statsmodels.regression.linear_model as sm\nX3 = np.append(arr = np.ones((155,1)).astype(int), values=x3, axis=1)\nr_ols3 = sm.OLS(endog = y3, exog = X3)\nr3 = r_ols3.fit()\nprint(r3.summary())","bd768d81":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value= 0)\nimpdata = data4.iloc[:,2:9].values\nimputer = imputer.fit(impdata[:,2:9])\nimpdata[:,2:9] = imputer.transform(impdata[:,2:9])\ndata4.iloc[:,2:9] = impdata[:,:]","cbd6a1fd":"print(data4.columns)\n\nprint(data4.info())\n\nprint(data4.describe())\n\ndf4 = data4[\"Country or region\"]\ndff4 = data4.value_counts()","ac308b75":"x4 = data4.iloc[:,3:].values\ny4 = data4.iloc[:,2:3].values","fd5f3f7c":"from sklearn.model_selection import train_test_split\nx_train4, x_test4, y_train4, y_test4 = train_test_split(x4, y4, test_size=0.33, random_state=0)","62be6b4d":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train4 = sc.fit_transform(x_train4)\nX_test4 = sc.fit_transform(x_test4)\nY_train4 = sc.fit_transform(y_train4)\nY_test4 = sc.fit_transform(y_test4)","4f92a24d":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x_train4, y_train4)\nprint(\"b0: \", lr.intercept_)\nprint(\"other b: \", lr.coef_)","000c803c":"import statsmodels.regression.linear_model as sm\nX4 = np.append(arr = np.ones((156,1)).astype(int), values=x4, axis=1)\nr_ols4 = sm.OLS(endog = y4, exog = X4)\nr4 = r_ols4.fit()\nprint(r4.summary())","b9591e06":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value= 0)\nimpdata = data5.iloc[:,3:9].values\nimputer = imputer.fit(impdata[:,3:9])\nimpdata[:,3:9] = imputer.transform(impdata[:,3:9])\ndata5.iloc[:,3:9] = impdata[:,:]","51487cd5":"print(data5.columns)\n\nprint(data5.info())\n\nprint(data5.describe())\n\ndf5 = data5[\"Country or region\"]\ndff5 = data5.value_counts()","588610bd":"x5 = data5.iloc[:,3:].values\ny5 = data5.iloc[:,2:3].values","068ed9b4":"from sklearn.model_selection import train_test_split\nx_train5, x_test5, y_train5, y_test5 = train_test_split(x5, y5, test_size=0.33, random_state=0)","5e85f486":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train5 = sc.fit_transform(x_train5)\nX_test5 = sc.fit_transform(x_test5)\nY_train5 = sc.fit_transform(y_train5)\nY_test5 = sc.fit_transform(y_test5)","94a42974":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x_train5, y_train5)\nprint(\"b0: \", lr.intercept_)\nprint(\"other b: \", lr.coef_)","0fd1011a":"import statsmodels.regression.linear_model as sm\nX5 = np.append(arr = np.ones((156,1)).astype(int), values=x5, axis=1)\nr_ols5 = sm.OLS(endog = y5, exog = X5)\nr5 = r_ols5.fit()\nprint(r5.summary())","d8213c63":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value= 0)\nimpdata = data6.iloc[:,4:20].values\nimputer = imputer.fit(impdata[:,4:20])\nimpdata[:,4:20] = imputer.transform(impdata[:,4:20])\ndata6.iloc[:,4:20] = impdata[:,:]","441ef6f0":"print(data6.columns)\n\nprint(data6.info())\n\nprint(data6.describe())\n\ndf6 = data6[\"Country name\"]\ndff6 = data6.value_counts()","97b28b20":"x6 = data6.iloc[:,4:].values\ny6 = data6.iloc[:,2:3].values","3ad45efb":"from sklearn.model_selection import train_test_split\nx_train6, x_test6, y_train6, y_test6 = train_test_split(x6, y6, test_size=0.33, random_state=0)","c0d6cafd":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train6 = sc.fit_transform(x_train6)\nX_test6 = sc.fit_transform(x_test6)\nY_train6 = sc.fit_transform(y_train6)\nY_test6 = sc.fit_transform(y_test6)","e1001c3e":"from sklearn.linear_model import LinearRegression\nlr = LinearRegression()\nlr.fit(x_train6, y_train6)\nprint(\"b0: \", lr.intercept_)\nprint(\"other b: \", lr.coef_)","228984a1":"import statsmodels.regression.linear_model as sm\nX6 = np.append(arr = np.ones((153,1)).astype(int), values=x6, axis=1)\nr_ols6 = sm.OLS(endog = y6, exog = X6)\nr6 = r_ols6.fit()\nprint(r6.summary())","61097cc2":"print(data7.columns)\n\nprint(data7.info())\n\nprint(data7.describe())\n\ndf7 = data7[\"country\"]\ndff7 = df7.value_counts()\nprint(dff7)","0b515177":"print(data8.columns)\n\nprint(data8.info())\n\nprint(data8.describe())\n\ndf8 = data8[\"country\"]\ndff8 = df8.value_counts()\nprint(dff8)","2b5e9369":"print(data9.columns)\n\nprint(data9.info())\n\nprint(data9.describe())\n\ndf9 = data9[\"country\"]\ndff9 = df9.value_counts()\nprint(dff9)","9bdd82c9":"We understand that our 2015 Terrorism Report data are countries with a year and a non-zero value of 14963.\n\nFor example, countries are ranked according to the number of terrorism. All values have been added as a new feature to our 2015 report model.","b716f307":"# **Note: It will be added when 2021 is announced.**","cec0281b":"We find that our 2016 data includes 157 countries, 13 features with non-blank values, and all other statistical information from the code output.","726bb865":"As a code output, the missing data is filled as 0.","33f35c2c":"Test and training sets are created as code output.","2882b8a4":"# **Note: Updates will continue.**","2f9bcdd0":"They are assigned to the variable x and y as the output of the code, x as the input columns affecting the happiness score, and y as the resultant happiness score column.","0b73c8dd":"Normalization is done as a code output.","c2aaf38a":"In the 2015, 2016 and 2017 World Happiness Reports, we used the Global Terrorism Report to track which factor affects the World Happiness Report and how much, and the impact of terrorist incidents on this ranking. At the end of the analysis, we estimate the happiness score with logical random values using Machine Learning and found that they really work. We found the Multilinear Regression Machine Learning Method to be a good option for our model. We found that the values of terrorist incidents were not affected by happiness scores, but the effect increased in 2016. Since there is no Terrorism data for 2018, 2019 and 2020 data, only their own data were analyzed.","b3d91e60":"We find that our 2019 data includes 156 countries, 7 features with non-blank values, and all other statistical information from the code output.","73dc644e":"Normalization is done as a code output.","f5947976":"# **World Happiness Rank 2019**","d9076c36":"# **Global Terrorism Report 2017**","0dc23751":"Normalization is done as a code output.","b4767b34":"R-squared and Adj. R-squared values (1.000 and 1.000) show the Success of Machine Learning Method in World Happiness Ranking 2017 Data. We understand that our model is very successful if these values are 1 or close to 1. Therefore, we are not changing our selected Machine Learning method.\n\nP>|t| value indicates the Success of the Variables on the World Happiness Ranking 2017 Data. We understand that the fixed value we added as 1 and the p values of our added terrorist event values are more than the 5% or 1% threshold values. This means that the values of terrorism events are not appropriate and affectable for our model.","7b2c79fc":"They are assigned to the variable x and y as the output of the code, x as the input columns affecting the happiness score, and y as the resultant happiness score column.","b6083714":"They are assigned to the variable x and y as the output of the code, x as the input columns affecting the happiness score, and y as the resultant happiness score column.","a383dc99":"R-squared and Adj. R-squared values (1.000 and 1.000) show the Success of Machine Learning Method in World Happiness Ranking 2016 Data. We understand that our model is very successful if these values are 1 or close to 1. Therefore, we are not changing our selected Machine Learning method.\n\nP>|t| value indicates the Success of the Variables on the World Happiness Ranking 2016 Data. We understand that the fixed value we added as 1 and the p values of our added terrorist event values are more than the 5% or 1% threshold values. This means that the values of terrorism events are not appropriate and affectable for our model. However, we see that these p-values decrease, which means that their effect on our model increases.","c50acd6d":"# **World Happiness Rank 2016**","25df9230":"# **7. REFERENCES**","b129cbad":"We understand that our Terrorism Report 2017 data are countries with year and 10900 non-zero values.\n\nFor example, countries are ranked according to the number of terrorists. All values have been added as a new feature to our 2017 report model.","1c3d8465":"Normalization is done as a code output.","e932fc71":"A machine learning model is created as a code output and weight factor values are calculated according to the model.","7060bd58":"# **World Happiness Rank 2020**","d1dad12d":"Normalization is done as a code output.","a3ae058a":"# **Global Terrorism Report 2016**","00d2d0aa":"They are assigned to the variable x and y as the output of the code, x as the input columns affecting the happiness score, and y as the resultant happiness score column.","2bcbe365":"A machine learning model is created as a code output and weight factor values are calculated according to the model.","3567ba0d":"# **1. INTRODUCTION**\nEverybody desires to be happy in life and interestingly the requirements to be happy vary from person to person. We all invest different meanings in the concept of happiness. People give happiness different values at different part of our lives. However, there are some vital factors that are commonly regarded as the main ingredients to be happy in life. Physical and psychological sturdiness is very important to be happy and people can truly realize that when they get sick. Economic freedom - ability to fulfil the needs in life, is the another most important factor to be happy, in my opinion. It is always said that being happy with an empty stomach is not possible. Individual birth \u2013 right freedom is also considered a great influence to be happy in life. Someone without birth - right freedom cannot feel the happiness.\n\nMain analysis will consist correlations (regression; output=happiness, variables=economy, health, freedom, etc.), creating a Machine-Learning algorithm (clustering countries according to their happiness rank and score). For example, a country where the happiness score is more than 7.00 is a developed country, a country where the happiness score is between 5.00 and 7.00 is a developing country, and a country where the happiness score is less than 5.00 is undeveloped country.\n\nThis analysis will evaluate a connection between people\u2019s happiness and their countries. In my opinion, the location where we live is a huge factor of our happiness proportion. The World Happiness Report, produced by the United Nations, ranks about 155 countries by how happy their citizens see themselves to be. It\u2019s based on factors including economic wealth, life expectancy, social support, freedom to make life choices and levels of government corruption. The happiness scores and rankings use data from the Gallup World Poll. The scores are based on answers to the main life evaluation question asked in the poll.\n\nLouise Millard do data mining and analysis of Global Happiness with Machine Learning Methods in 2011. This report including 123 countries with happiness values. This report is approached economics, health, climate data. [1]\n\nNatasha Jaques et al do analysis of Predicting Students\u2019 Happiness from Physiology, Phone, Mobility, and Behavioral Data with Machine Learning Methods in \u2026 . This report includes physiological data with electrodermal activity (a measure of physiological stress), and 3-axis accelerometer (a measure of steps and physical activity); survey data with questions related to academic activity, sleep, drug and alcohol use, and exercise; Phone data with phone call, SMS, and usage patterns; location data with coordinates logged throughout the day. In this paper they analysis a machine learning algorithm to distinguish between happy and unhappy college students, assess which measures provide the most information about happiness, and evaluate the relationship between different components of wellbeing including happiness, health, energy, alertness and stress. [2]\n\n# **2. ABOUT THE DATA**\nThe first used data that is the World Happiness Report is a landmark survey of the state of global happiness. The report that ranks about 155 countries according to happiness levels continues to gain global recognition as governments, organizations and civil society increasingly use happiness indicators to inform their policy-making decisions. Leading experts across fields economics, psychology, survey analysis, national statistics, health, public policy etc. describe how measurements of well-being can be used effectively to assess the progress of nations.\n\nThe Second used data that is the Global Terrorism Database (GTD) is an open-source database including information on terrorist attacks around the world from 1970 through 2017. The GTD includes systematic data on domestic as well as international terrorist incidents that have occurred during this time period and now includes more than 180,000 attacks. \n\n# **3. PROJECT DEFINITION**\nThe study examines effects of Happiness Score. People\u2019s happiness level can be affected by some situations. Machine Learning with CRISP-DM method measures at least to the most effects. Values say the most important things for happiness. The aim of the project provides tricks to governments, organizations and civil society. Multiple Linear Regression model is selected to set up.\n\nOn the other hand, these datasets and all outputs will be analyzed again with global terrorist attacks. My plan is creating a new independent variable that does not exist on \u201cWorld Happiness Reports\u201d. The main goal of this purpose is to see whether the geography is a trigger of terrorist attacks and these attacks has a role on citizens\u2019 happiness or not.\n\nThis project main aim is approaching sociology and politics with machine learning and statistics. It will be very interesting because this is a good example for understanding the usefulness of data science on our lives. No matter data contains numerical or verbal values, we can make analysis and inferences to help building better world.  \n\n     \n# **4. METHODOLOGY**\nWe use Python programming language and Multiple Linear Regression Machine Learning Method for data analysis. We take suitable data from The Global Terrorism Report as country, weapon attack etc. And these columns are added to our World Happiness Report. This data frame splits as test (1\/3) and train (2\/3) randomly. And multiple linear regression method works on this data frame. And we find which feature affects more than others to happiness score. If any problem for method, we will change to other methods and compare their output values. \n\n# **4.1. CRISP-DM Method**\nCRISP-DM (Cross Industry Standard Process for Data Mining) method that a model to define standard processes for data mining are used for data mining projects to become more effective, faster, safer, less costly. This method begins with understanding problem and what we need to do and continues with data preparation, modelling and evaluation for Machine Learning. \n\n![image.png](attachment:12535ccd-3cba-4364-b401-8a2dab6e53f3.png)\n\n **Figure-1:** Structure of CRISP-DM Method\n\n# **4.2. Understanding Data**\nWorld Happiness Report includes countries with happiness scores, economy situation (GDP per Capita), health situation (Life Expectancy), freedom situation, trusting situation (Government Corruption), generosity situation. We understand to find relation happiness scores with these values. \n\nGlobal Terrorism Database report includes years with countries of terrorist events, attack types, weapon types etc. We understand to take and add new features to our data frame. Years and Countries are enough to take.\n\n# **4.3. Data Preprocessing**\nData preprocessing part is important for processing data. This part includes libraries and data importing, data cleaning, suitable data editing to use, data adding to data frame to pass to Machine Learning part. We need to preprocesses for Machine Learning part.\n\n# **4.3.1. Libraries and Data Importing**\nWe decide to which libraries are used according to processes. In Python, Pandas library is used for data and data frame processes, NumPy library is used for calculates, Matplotlib and Seaborn library is used for plotting, and Sci-Kit Learn library is used for Machine Learning process using \u201cimport\u201d command.\n\nUsing Data can be csv (comma separated values), excel, html etc. and is taken \u201cread_csv\/excel(\u201cfile_name\u201d)\u201d command. Data file must be working directory. If it is not in working directory, you can write index of file for file_name. And this defines as any variable.","4872a48e":"We find that our 2015 data includes 158 countries, 12 features with non-blank values, and all other statistical information from the code output.","83d45ccc":"A machine learning model is created as a code output and weight factor values are calculated according to the model.","17fec359":"We see from the code output that the real value for Turkey in the 2016 report will be 5,389 happiness points, our model estimates it as 5.33218602 from the 2015 report and 5.49986729 from the 2017 report. We estimate that the reason for the 2017 value and others' differences is that the characteristics and the impact of terrorism.","2d880d0a":"R-squared and Adj. R-squared values (1.000 and 1.000) show the Success of Machine Learning Method in World Happiness Ranking 2015 Data. We understand that our model is very successful if these values are 1 or close to 1. Therefore, we are not changing our selected Machine Learning method.\n\nP>|t| value indicates the Success of the Variables on the World Happiness Ranking 2015 Data. We understand that the fixed value we added as 1 and the p values of our added terrorist event values are more than the 5% or 1% threshold values. This means that the values of terrorism events are not appropriate and affectable for our model.","b0c166da":"They are assigned to the variable x and y as the output of the code, x as the input columns affecting the happiness score, and y as the resultant happiness score column.","6265779b":"Test and training sets are created as code output.","cce98ee4":"# **World Happiness Rank 2018**","6a262085":"Test and training sets are created as code output.","5f78fd9e":"As a code output, the missing data is filled as 0.","d5b37662":"As a code output, the missing data is filled as 0.","a728710a":"As code outputs, we see from the report that the real value for Turkey in the 2015 report will be 5,332 happiness points, our model estimates it to be 5.38965305 from the 2016 report and 5.50012402 from the 2017 report. We estimate that the reason for the 2017 value and others' differences is that the characteristics and the impact of terrorism have changed.","f2845f06":"**[1]** Louise Millard (2011) Data Mining and Analysis of Global Happiness: A Machine Learning Approach\n\n**[2]** Natasha Jaques, Sara Taylor, Asaph Azaria, Asma Ghandeharioun, Akane Sano, Rosalind Picard (2015) Predicting Students\u2019 Happiness from Physiology, Phone, Mobility, and Behavioral Data","03780ae1":"# **Global Terrorism Report 2015**","ab4c1b03":"They are assigned to the variable x and y as the output of the code, x as the input columns affecting the happiness score, and y as the resultant happiness score column.","d5463b86":"A machine learning model is created as a code output and weight factor values are calculated according to the model.","40bd201f":"# **World Happiness Rank 2017**","5d239dbe":"A machine learning model is created as a code output and weight factor values are calculated according to the model.","ef5b50e0":"Test and training sets are created as code output.","7f674574":"Normalization is done as a code output.","60deb1b1":"Test and training sets are created as code output.","4ba6da17":"A machine learning model is created as a code output and weight factor values are calculated according to the model.","6ae2191d":"As a code output, we see from the report that the real value for Turkey in the 2017 report will be 5.5 happiness points, our model estimates it to be 5.33249758 from the 2015 report and 5.38949664 from the 2016 report. If you want to use this model, you can use the 2015 and 2016 reports because of its equivalence.","c5a2a92e":"We find that our 2020 data includes 153 countries, 16 features with non-blank values, and all other statistical information from the code output.","6e39a5cd":"We find that our 2017 data includes 155 countries, 12 features with non-blank values, and all other statistical information from the code output.","3cd0c6ad":"# **4.3.2. Data Cleaning**\nSome Machine Learning algorithms cannot run data with missing values. It must be fixed for processes. Missing values can be seen NaN (not a number), \u201c?\u201d, blank etc. You can impute as for numerical missing values can be fixed statistical methods (mean etc.), putting specific number or deleting rows. For our study, it is not used because data is without missing value and zeros are not affect our value.\n\nWe need to diagnose data before exploring, because column name inconsistency like upper-lower case letter or space between words or different languages. Upper-lower case letter or space problem is fixed. We have to change column names, if our data\u2019s columns names have upper-lower letter or space. You can change columns names with coding on IDE or typing on Excel. \n     \nTerrorism report has a lot of unneeded knowledges for our report. So, we edit this report according to our report as deleting unneeded columns. And we keep only terrorist attacks in 2015, 2016 and 2017.\n\n# **4.3.3. Suitable Data Editing to Use**\nData can be categoric (nominal or ordinal) or numerical (ration or interval). If we have only numerical data, we don\u2019t have to edit for suitable data. If we have not, we have to edit data. For example, in our report we have 2 categoric data. First is index number, second is countries. If these things affect happiness score, we have to edit as we change these data to categoric as number. But index number and countries don\u2019t affect happiness score. So, we ignore these for Machine Learning process. \n\nWe want to see effect of terrorism to world happiness report. So, we have to take suitable data from Terrorism Report. For suitable data, we take only countries with how many terrorist attacks are in same year. We ignore other features and countries that does not exist in World Happiness Report. We need only affectable for happiness report. We take terrorism event counts from edited Global Terrorism Report with only years and countries separately as years.\n      \n# **4.3.4. Data Adding to Data Frame**\nNow we have 2 data frames: World Happiness data frame and Terrorism data frame with only countries and counts. We have to add terrorism feature to World Happiness Data Frame. We add these values at same year and same countries. We add terrorism event counts to our World Happiness Report.\n\n# **4.4. Exploratory Analysis**\nWe want to understand and think about our data. So firstly, we take summary and info from data, like statistical knowledges, how many rows and columns data frames are and etc. And exploratory analysis is need for diagnosing data.\n\n# **4.5. Deciding Multiple Linear Regression Method**\nMultiple Linear Regression is to use output is affected by multiple independent values in problem. Happiness scores are affected by multiple independent values. So, I began to try this method for this problem.\n\n![image.png](attachment:301487f4-74ca-4a27-b656-4ed4bc7c8f24.png)\n\n**Figure-2:** Formula of Multiple Linear Regression Method\n\nIn the formula, \u03b20 is constant number, \u03b21,2,3,4\u2026 is multiplier of variables, and \u03f5 is error rate. From this formula, we understand to building straight for problem. And this means that \u03f5 (error) can be and is need to add calculation. Multiple Linear Regression can be more than 3 dimensions in space. For \u03920, we add a column with only 1 in each data.\n\nThe difference between y and flat y(\u03b3i) values is known as residual. It should be built on flat, minimum square error (MSE). \"N\" is the sample number.\n\n![image.png](attachment:814872b5-63e9-4428-aa64-bc0f0802b43d.png)\n\n**Figure-3:** Formula of Minimum Square Error\n\n# **4.6. Data Splitting as Test and Train Cluster**\nData splitting as test and train cluster is done for measuring success Machine Learning Algorithm, aim is developing evaluation. Generally, for test 1\/3 and for train 2\/3 is used randomly, known as percentage split. Machine learning algorithm run on train data and predict on test data, prediction and test data\u2019s happiness score are compared. And success is seen, and we can decide good or not Machine Learning Method. According to success we can change Machine Learning Method is used.\n\n# **4.7. Feature Scaling**\nFeature Scaling process is important for Machine Learning. Different columns have different data movements and statistical features (mean, min\/max values, standard deviation etc.). So, effects of columns are different and this is problem. We have to fix this problem using feature scaling. Feature Scaling is done with two classic methods: Standardization and Normalization. We use normalization method. Numerical data are scaled between 0 and 1.\n\n![image.png](attachment:399256b1-e4e5-4729-97eb-58b1020af811.png)\n\n**Figure-4:** Formulas of Standardization and Normalization\n\n# **4.8. Be Careful to Dummy Variable Trap**\nDummy variable is situation that data frame has variables have same values. Some Machine Learning Algorithm can be affected from dummy variable. If any situation in data frame, we have to cancel one. For example, man and woman values can be encoded 0 and 1 to transform string value to numeral value. And if we use these two encoded columns, we could have problem. Output affects two times from situation that can be man or woman. Only one column is enough and second one is canceled. For our data frame, it is not needed.\n\n# **4.9. Fine Tuning**\nWe look at the success of the data and the Machine Learning method chosen. If we have any problems, we will change the method. We look at P-Values for data success and R^2 and Adjusted R^2 for method success.\n\n# **4.9.1. Look at the P-Values**\nA p-value indicates the significance of a result, representing the probability the result would occur by chance. A low value means the result is unlikely to occur randomly and hence is statistically significant. Threshold values of 5% and 1% are commonly used, below which a result can be stated as significant. Statistical tests can be relative to test parameters such as the size of the dataset, and a p-value provides a comparable value that takes into account such aspects of the data. [1]\n     \n# **4.9.2. Comparison of Method\u2019s Success with R^2 and Adjusted R^2**\nR^2  is also known as the coefficient of determination, and is an extension of the R value. It represents the proportion of variation in the label that can be accounted for by the regression model. However, R is relative to the number of variables used in the model and therefore is not comparable when this differs. Adjusted R^2 takes into consideration the number of variables used in the regression. [1]\n\n![image.png](attachment:81504f8c-7377-4a53-8bd0-9c47ea5d57d7.png)\n\n**Figure-5:** Formulas of R^2 and Adjusted R^2\n\nThe closer the R^2 and Adjusted R^2 values are to 1, the better the case for the chosen method. The Multiple Linear Regression Method is good for our analysis. We analyze the effects of happiness scores. All values in the dataset are important because all values affect happiness scores. Therefore, we do not cancel or ignore any features.\n\n# **5. DATA ANALYSIS**\n\n# **World Happiness Rank 2015**","2044533f":"Test and training sets are created as code output.","eac5f4f9":"# **6. RESULTS**","1d08f1fd":"We find that our 2018 data includes 156 countries, 7 features with non-blank values, and all other statistical information from the code output.","bf2c159d":"We understand that our 2016 Terrorism Report data is the year and countries with a non-zero value of 13587.\n\nFor example, countries are ranked according to the number of terrorists. All values have been added as a new feature to our 2016 report model."}}