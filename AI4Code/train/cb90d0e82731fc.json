{"cell_type":{"7b441ae8":"code","34e0a6aa":"code","9b94dcea":"code","1d39c6a0":"code","227c902b":"code","09b76532":"code","3b1ba558":"code","0942f5b5":"code","031ce275":"code","6c7005e7":"code","6bbd36e7":"code","c2583728":"code","41ffc90c":"code","21622de7":"code","0e95cbfd":"code","1aabb407":"code","9d309d36":"code","6a63729a":"code","e4f9fbe3":"code","961ada86":"code","32488959":"code","c1b5e146":"code","f3516b7c":"code","fc5db167":"code","971591b7":"code","0e27e5c3":"code","ff67fabb":"code","3c9f30d7":"code","65f5be4f":"code","90daa5f4":"code","b2503bc2":"code","222e2f68":"code","dbb89d7e":"code","747e70b5":"code","11b3b298":"code","e68f26b7":"code","4240e446":"code","7505a1d8":"code","99dbd901":"code","8aabdba2":"markdown","55fa715d":"markdown","ca099486":"markdown","2dac4b7d":"markdown","eab7419f":"markdown","41c8c908":"markdown","e7047f81":"markdown","a9386d4f":"markdown","5ef2f49a":"markdown","4acb77f4":"markdown","ab7d184a":"markdown"},"source":{"7b441ae8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport glob","34e0a6aa":"data_dir = '..\/input\/optiver-realized-volatility-prediction\/'","9b94dcea":"def calc_wap(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])\/(df['bid_size1'] + df['ask_size1'])\n    return wap\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])\/(df['bid_size2'] + df['ask_size2'])\n    return wap\ndef calc_wap3(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) \/ (df['bid_size2']+ df['ask_size2'])\n    return wap\ndef calc_wap4(df):\n    wap = (df['bid_price1'] * df['bid_size1'] + df['ask_price1'] * df['ask_size1']) \/ (df['bid_size1']+ df['ask_size1'])\n    return wap\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    return len(np.unique(series))","1d39c6a0":"book_train = pd.read_parquet(data_dir + \"book_train.parquet\/stock_id=15\")\nbook_train.head()","227c902b":"def preprocessor_book(file_path):\n    df = pd.read_parquet(file_path)\n    #calculate return etc\n    df['wap'] = calc_wap(df)\n    df['log_return'] = df.groupby('time_id')['wap'].apply(log_return)\n    \n    df['wap2'] = calc_wap2(df)\n    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n    \n    df['wap3'] = calc_wap3(df)\n    df['log_return3'] = df.groupby('time_id')['wap3'].apply(log_return)\n    \n    df['wap_balance'] = abs(df['wap'] - df['wap2'])\n    \n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1'])\/2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n\n    #dict for aggregate\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'log_return2':[realized_volatility],\n        'log_return3':[realized_volatility],\n        'wap_balance':[np.mean],\n        'price_spread':[np.mean],\n        'bid_spread':[np.mean],\n        'ask_spread':[np.mean],\n        'volume_imbalance':[np.mean],\n        'total_volume':[np.mean],\n        'wap':[np.mean],\n            }\n\n    #####groupby \/ all seconds\n    df_feature = pd.DataFrame(df.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n    \n    df_feature.columns = ['_'.join(col) for col in df_feature.columns] #time_id is changed to time_id_\n        \n    ######groupby \/ last XX seconds\n    last_seconds = [600]\n    \n    for second in last_seconds:\n        second = 1200 - second \n    \n        df_feature_sec = pd.DataFrame(df.query(f'seconds_in_bucket >= {second}').groupby(['time_id']).agg(create_feature_dict)).reset_index()\n\n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns] #time_id is changed to time_id_\n     \n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n\n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n    \n    #create row_id\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['time_id_'],axis=1)\n    \n    return df_feature","09b76532":"%%time\nfile_path = data_dir + \"book_train.parquet\/stock_id=0\"\npreprocessor_book(file_path)","3b1ba558":"trade_train = pd.read_parquet(data_dir + \"trade_train.parquet\/stock_id=0\")\ntrade_train.head(15)","0942f5b5":"def preprocessor_trade(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    \n    aggregate_dictionary = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    df_feature = df.groupby('time_id').agg(aggregate_dictionary)\n    \n    df_feature = df_feature.reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n\n    \n    ######groupby \/ last XX seconds\n    last_seconds = [600]\n    \n    for second in last_seconds:\n        second = 1200 - second\n    \n        df_feature_sec = df.query(f'seconds_in_bucket >= {second}').groupby('time_id').agg(aggregate_dictionary)\n        df_feature_sec = df_feature_sec.reset_index()\n        \n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns]\n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n        \n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['trade_time_id_'],axis=1)\n    \n    return df_feature","031ce275":"%%time\nfile_path = data_dir + \"trade_train.parquet\/stock_id=0\"\npreprocessor_trade(file_path)","6c7005e7":"def preprocessor(list_stock_ids, is_train = True):\n    from joblib import Parallel, delayed # parallel computing to save time\n    df = pd.DataFrame()\n    \n    def for_joblib(stock_id):\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n            \n        df_tmp = pd.merge(preprocessor_book(file_path_book),preprocessor_trade(file_path_trade),on='row_id',how='left')\n     \n        return pd.concat([df,df_tmp])\n    \n    df = Parallel(n_jobs=-1, verbose=1)(\n        delayed(for_joblib)(stock_id) for stock_id in list_stock_ids\n        )\n\n    df =  pd.concat(df,ignore_index = True)\n    return df\n","6bbd36e7":"list_stock_ids = [0,1]\npreprocessor(list_stock_ids, is_train = True)","c2583728":"train = pd.read_csv(data_dir + 'train.csv')","41ffc90c":"train_ids = train.stock_id.unique()","21622de7":"%%time\ndf_train = preprocessor(list_stock_ids= train_ids, is_train = True)","0e95cbfd":"train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntrain = train[['row_id','target']]\ndf_train = train.merge(df_train, on = ['row_id'], how = 'left')","1aabb407":"df_train.head()","9d309d36":"test = pd.read_csv(data_dir + 'test.csv')","6a63729a":"test_ids = test.stock_id.unique()","e4f9fbe3":"%%time\ndf_test = preprocessor(list_stock_ids= test_ids, is_train = False)","961ada86":"df_test = test.merge(df_test, on = ['row_id'], how = 'left')","32488959":"df_test.head()","c1b5e146":"from sklearn.model_selection import KFold\n#stock_id target encoding\ndf_train['stock_id'] = df_train['row_id'].apply(lambda x:x.split('-')[0])\ndf_test['stock_id'] = df_test['row_id'].apply(lambda x:x.split('-')[0])\n\nstock_id_target_mean = df_train.groupby('stock_id')['target'].mean() \ndf_test['stock_id_target_enc'] = df_test['stock_id'].map(stock_id_target_mean) # test_set\n\n#training\ntmp = np.repeat(np.nan, df_train.shape[0])\nkf = KFold(n_splits = 10, shuffle=True,random_state = 19911109)\nfor idx_1, idx_2 in kf.split(df_train):\n    target_mean = df_train.iloc[idx_1].groupby('stock_id')['target'].mean()\n\n    tmp[idx_2] = df_train['stock_id'].iloc[idx_2].map(target_mean)\ndf_train['stock_id_target_enc'] = tmp","f3516b7c":"df_train.head()","fc5db167":"df_test.head()","971591b7":"import lightgbm as lgbm","0e27e5c3":"df_train['stock_id'] = df_train['stock_id'].astype(int)\ndf_test['stock_id'] = df_test['stock_id'].astype(int)","ff67fabb":"X = df_train.drop(['row_id','target'],axis=1)\ny = df_train['target']","3c9f30d7":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\ndef feval_RMSPE(preds, lgbm_train):\n    labels = lgbm_train.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n\nparams = {\n      \"objective\": \"rmse\", \n      \"metric\": \"rmse\", \n      \"boosting_type\": \"gbdt\",\n      'early_stopping_rounds': 20,\n      'learning_rate': 0.01,\n      'lambda_l1': 1,\n      'lambda_l2': 1,\n      'feature_fraction': 0.85,\n      'bagging_fraction': 0.85,\n  }","65f5be4f":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=5, random_state=19901028, shuffle=True)\noof = pd.DataFrame()                 # out-of-fold result\nmodels = []                          # models\nscores = 0.0                         # validation score","90daa5f4":"%%time\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n\n    print(\"Fold :\", fold+1)\n    \n    # create dataset\n    X_train, y_train = X.loc[trn_idx], y[trn_idx]\n    X_valid, y_valid = X.loc[val_idx], y[val_idx]\n    \n    #RMSPE weight\n    weights = 1\/np.square(y_train)\n    lgbm_train = lgbm.Dataset(X_train,y_train,weight = weights)\n\n    weights = 1\/np.square(y_valid)\n    lgbm_valid = lgbm.Dataset(X_valid,y_valid,reference = lgbm_train,weight = weights)\n    \n    # model \n    model = lgbm.train(params=params,\n                      train_set=lgbm_train,\n                      valid_sets=[lgbm_train, lgbm_valid],\n                      num_boost_round=5000,         \n                      feval=feval_RMSPE,\n                      verbose_eval=100,\n                      categorical_feature = ['stock_id']                \n                     )\n    \n    # validation \n    y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n\n    RMSPE = round(rmspe(y_true = y_valid, y_pred = y_pred),3)\n    print(f'Performance of the\u3000prediction: , RMSPE: {RMSPE}')\n\n    #keep scores and models\n    scores += RMSPE \/ 5\n    models.append(model)\n    print(\"*\" * 100)","b2503bc2":"scores","222e2f68":"df_test.columns","dbb89d7e":"df_train.columns","747e70b5":"y_pred = df_test[['row_id']]\nX_test = df_test.drop(['time_id', 'row_id'], axis = 1)","11b3b298":"X_test","e68f26b7":"target = np.zeros(len(X_test))\n\n#light gbm models\nfor model in models:\n    pred = model.predict(X_test[X_valid.columns], num_iteration=model.best_iteration)\n    target += pred \/ len(models)","4240e446":"y_pred = y_pred.assign(target = target)","7505a1d8":"y_pred","99dbd901":"y_pred.to_csv('submission.csv',index = False)","8aabdba2":"# Test set","55fa715d":"## Main function for preprocessing book data","ca099486":"## Functions for preprocess","2dac4b7d":"## Model Building","eab7419f":"## Combined preprocessor function","41c8c908":"### Cross Validation","e7047f81":"## Training set","a9386d4f":"## LightGBM","5ef2f49a":"## Main function for preprocessing trade data","4acb77f4":"## Target encoding by stock_id","ab7d184a":"## Test set"}}