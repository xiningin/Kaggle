{"cell_type":{"7f801eaa":"code","e9d78809":"code","4dfb0b93":"code","a3dd3d11":"code","87c02dca":"code","1c517ee6":"code","5a3ab39f":"code","d640988b":"code","cae54ca4":"code","67922e51":"code","3077a8c9":"code","319313ed":"code","ede9586a":"code","5515118f":"code","ff378583":"code","095ee9a6":"code","c3a2b4d6":"code","b6cd5e0c":"code","22a8e0a8":"code","4dee2814":"code","d101a004":"code","e5dc9209":"markdown","8c33786f":"markdown","b1131e93":"markdown","94c703c1":"markdown","db704af6":"markdown","cb12e091":"markdown","5f5b9bd9":"markdown","5532da6b":"markdown","27f0fee8":"markdown","002984ed":"markdown","a27270e7":"markdown","d5bd0050":"markdown","2a768b5b":"markdown","0c6c0d30":"markdown","73f31e92":"markdown","ddd7a7d5":"markdown","4c8a1006":"markdown","c044bce6":"markdown","480208d5":"markdown","bb14a813":"markdown","7f949d11":"markdown","4d2bf3f2":"markdown"},"source":{"7f801eaa":"# Generic\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, warnings, gc\nwarnings.filterwarnings(\"ignore\")\n\n# SKLearn (feature selection)\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif, mutual_info_classif\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.feature_selection import SelectFpr, SelectFdr, SelectFwe\nfrom sklearn.feature_selection import GenericUnivariateSelect\nfrom sklearn.feature_selection import SelectFromModel\n\n# SKLearn (feature extraction)\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n\n\n\n# SkLearn Classification Algorithm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\n#Sklearn Metrics, Model Selection , Preprocessing & Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.pipeline import Pipeline\n\n# Tabulate\nfrom tabulate import tabulate\n\n# PLot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","e9d78809":"# Load Data\nurl = '..\/input\/all-datasets-for-practicing-ml\/Class\/Class_Abalone.csv'\ndata = pd.read_csv(url, header='infer')","4dfb0b93":"# Label Encoding\nencoder = LabelEncoder()\ndata['Sex']= encoder.fit_transform(data['Sex']) \n\n# Seperate Features & Target\ncolumns = data.columns\ntarget = ['Sex']\nfeatures = columns[1:]\n\nX = data[features]\ny = data[target]","a3dd3d11":"# Inspect\nX.head()","87c02dca":"'''Classification Model Evaluation Using the Above Data'''\n'''Classifier Model = Random Forest '''\n\n# Dataset Split\n''' Training = 90% & Validation = 10%  '''\ntest_size = 0.1\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=test_size, random_state=0, shuffle=True) \n\n\n'''Creating Pipeline [Feature Scaling & Classification]'''\npipe = Pipeline([\n         (\"Feature_Scaling\", StandardScaler()),\n         (\"RandomForest_Classifier\", RandomForestClassifier(random_state=1, verbose=0)) ])\n\npipe.fit(X_train, y_train)\n\n\n# Accuracy\nprint(\"Random Forest Accuracy (without feature selection): \", '{:.2%}'.format(pipe.score(X_val,y_val)))","1c517ee6":"'''Applying SelectKBest with Scoring Function = Chi2 to Abalone dataset to retrieve the **4** best features.'''\n\nX_new = pd.DataFrame(SelectKBest(chi2, k=4).fit_transform(X,y))\n\n#Inspect\nX_new.head()","5a3ab39f":"#Rename\nX_new.rename(columns={0: \"Whole_Weight\", 1: \"Shucked_Weight\", \n                      2:\"Shell_Weight\", 3: \"Rings\"}, inplace=True)\n\n#Inspect\nX_new.head()","d640988b":"'''Classification Model Evaluation Using the Newly Selected Feature Data'''\n\n# Dataset Split\n''' Training = 90% & Validation = 10%  '''\ntest_size = 0.1\nX_train, X_val, y_train, y_val = train_test_split(X_new, y, test_size=test_size, random_state=0, shuffle=True) \n\n# Feature Scale & Train Model\npipe.fit(X_train, y_train)\n\n\n# Tabulate Data Empty List\ntab_data = []\n\n# Accuracy\n#print(\"Random Forest Accuracy (with feature selection): \", '{:.2%}'.format(pipe.score(X_val,y_val)))\ntab_data.append(['Chi2', '{:.2%}'.format(pipe.score(X_val,y_val))])\n","cae54ca4":"'''Applying SelectKBest with Scoring Function = f_classif to Abalone dataset to retrieve the **4** best features.'''\n\nX_new = pd.DataFrame(SelectKBest(f_classif, k=4).fit_transform(X,y))\n\n# Dataset Split\n''' Training = 90% & Validation = 10%  '''\ntest_size = 0.1\nX_train, X_val, y_train, y_val = train_test_split(X_new, y, test_size=test_size, random_state=0, shuffle=True) \n\n# Feature Scale & Train Model\npipe.fit(X_train, y_train)\n\n# Accuracy\n#print(\"Random Forest Accuracy (with feature selection): \", '{:.2%}'.format(pipe.score(X_val,y_val)))\ntab_data.append(['f_classif', '{:.2%}'.format(pipe.score(X_val,y_val))])","67922e51":"'''Applying SelectKBest with Scoring Function = mutual_info_classif to Abalone dataset to retrieve the **4** best features.'''\n\nX_new = pd.DataFrame(SelectKBest(mutual_info_classif, k=4).fit_transform(X,y))\n\n# Dataset Split\n''' Training = 90% & Validation = 10%  '''\ntest_size = 0.1\nX_train, X_val, y_train, y_val = train_test_split(X_new, y, test_size=test_size, random_state=0, shuffle=True) \n\n# Feature Scale & Train Model\npipe.fit(X_train, y_train)\n\n# Accuracy\n#print(\"Random Forest Accuracy (with feature selection): \", '{:.2%}'.format(pipe.score(X_val,y_val)))\ntab_data.append(['mutual_info_classif', '{:.2%}'.format(pipe.score(X_val,y_val))])","3077a8c9":"# Tabulate Data\nprint(\"Random Forest Accuracy (with SelectKBest feature selection):\\n\\n\", tabulate(tab_data, headers=['Scoring_Func', 'Accuracy']))","319313ed":"'''Applying SelectPercentile with Scoring Function = Chi2 to Abalone dataset to retrieve the **4** best features.'''\n''' Percentile = Percent of features to keep'''\n'''For this example, we're going to keep 70% of the features'''\n\nX_new = pd.DataFrame(SelectPercentile(chi2, percentile=70).fit_transform(X,y))\n\n#Inspect\nX_new.shape","ede9586a":"'''Classification Model Evaluation Using the Newly Selected Feature Data'''\n\n# Dataset Split\n''' Training = 90% & Validation = 10%  '''\ntest_size = 0.1\nX_train, X_val, y_train, y_val = train_test_split(X_new, y, test_size=test_size, random_state=0, shuffle=True) \n\n# Feature Scale & Train Model\npipe.fit(X_train, y_train)\n\n# Accuracy\nprint(\"Random Forest Accuracy (with SelectPercentile{Chi2} feature selection): \", '{:.2%}'.format(pipe.score(X_val,y_val)))","5515118f":"'''Applying FPR with Scoring Function = Chi2 to Abalone dataset to retrieve the best features.'''\n\nX_new = pd.DataFrame(SelectFpr(chi2).fit_transform(X,y))\n\n#Inspect\nX_new.shape","ff378583":"'''Applying FDR with Scoring Function = Chi2 to Abalone dataset to retrieve the best features.'''\n\nX_new = pd.DataFrame(SelectFdr(chi2).fit_transform(X,y))\n\n#Inspect\nX_new.shape","095ee9a6":"'''Applying FWE with Scoring Function = Chi2 to Abalone dataset to retrieve the best features.'''\n\nX_new = pd.DataFrame(SelectFwe(chi2).fit_transform(X,y))\n\n#Inspect\nX_new.shape","c3a2b4d6":"'''Applying GenericUnivariateSelect with Scoring Function = Chi2 & Mode = k_best to Abalone dataset to retrieve the best features.'''\n\nX_new = pd.DataFrame(GenericUnivariateSelect(chi2, mode='k_best', param=4).fit_transform(X,y))\n\n#Inspect\nX_new.shape","b6cd5e0c":"# Dict\nstaff = [{'name': 'John Oxboro', 'age': 23, 'role':'Manager'},\n         {'name': 'Regina Smith', 'age': 10, 'role':'Lead'},\n         {'name': 'Ollie Dyson', 'age': 28, 'role':'Architect'},\n        {'name': 'Ian McGrath', 'age': 48, 'role':'Engineer'}]\n\n# Convert Dictionary To Feature Matrix\ndv = DictVectorizer()\ndv.fit_transform(staff).toarray()\n\n#View Feature Names\ndv.get_feature_names()","22a8e0a8":"# List of Texts\ntext = ['The quick brown fox jumped over the lazy dog']\n\n# CountVectorizer\ncv = CountVectorizer()\n\n# Tokenize & build vocab\ncv.fit(text)\n\n# Summarize\nprint(\"Vocab Summary: \", cv.vocabulary_)\n\n# Encode Text\nvec = cv.transform(text)\n\n# summarize encoded vector\nprint(\"Vector Shape: \",vec.shape)\nprint(\"Vector Type: \",type(vec))\nprint(\"Vector Array\",vec.toarray())\n","4dee2814":"# encode another document\ntext2 = [\"every dog must have his day\"]\nvector = cv.transform(text2)\nprint(vector.toarray())","d101a004":"# Define text\ntext = [\"The quick brown fox jumped over the lazy dog\", \"The dog\", \"The fox\"]\n\n# create the \nvectorizer = TfidfVectorizer()\n\n# tokenize and build vocab\nvectorizer.fit(text)\n\n# summarize\nprint(\"Vocabulary Summary: \", vectorizer.vocabulary_)\nprint(\"Vector IDF:\", vectorizer.idf_)\n\n# encode document\nvector = vectorizer.transform([text[0]])\n\n# summarize encoded vector\nprint(\"Vector Shape: \",vector.shape)\nprint(\"Vector Array: \",vector.toarray())","e5dc9209":"# Feature Extraction\n\nModule used for extracting features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image\n\nFollowing are some of the Feature Extraction methods:\n\n1. Loading features from dicts\n2. Text feature extraction\n3. Image feature extraction  [not covered in this notebook]","8c33786f":"#### As we can observe that the Generic Univariate Select feature_selection method with mode = k_best has selected the 4 best features. The accuracy for this would be same as the SelectKBest with Scoring Function = Chi2","b1131e93":"# Setup","94c703c1":"The same vectorizer can be used on documents that contain words not included in the vocabulary. These words are ignored and no count is given in the resulting vector.\n\nFor example, below is an example of using the vectorizer above to encode a document with one word (dog) in the vocab and the rest words that are not.","db704af6":"## I hope this notebook was helpful in understanding the difference between Feature Selection & Feature Extraction.","cb12e091":"#### Note: Classifier training & evaluation wasn't performed because this feature_selection method has selected all the features & we know that the accuracy will be ~ 58.8%","5f5b9bd9":"# Feature Selection & Feature Extraction\n\nIn this notebook, we're going to learn about 2 SKLearn modules. They're\n\n1. Feature Selection\n2. Feature Extraction\n\n\n**Feature Selection**:- This module is used for feature selection\/dimensionality reduction on given datasets. This is done either to improve estimators\u2019 accuracy scores or to boost their performance on very high-dimensional datasets.\n\n**Feature Extraction**:- This module is used to extract features in a format supported by machine learning algorithms from the given datasets consisting of formats such as text and image.\n\n**The main difference**:-  Feature Extraction transforms an arbitrary data, such as text or images, into numerical features that is understood by machine learning algorithms. Feature Selection on the other hand is a machine learning technique applied on these (numerical) features.\n\n\nLet's try to understand the different classes under these 2 modules with the help of the [Abalone](https:\/\/archive.ics.uci.edu\/ml\/datasets\/abalone) dataset. \n\n\nAs always, I'll keep this notebook well commented & organized for easy understanding. Please do consider it to UPVOTE if its helpful.","5532da6b":"### False Positive Rate (fpr), False Discovery Rate(fdr) & Family-wise error rate(fwe)\n\nfpr = Select the pvalues below alpha based on a FPR test.It controls the total amount of false detections\n\nfdr = Select the p-values for an estimated false discovery rate. alpha is an upper bound on the expected false discovery rate\n\nfwe = Select the p-values corresponding to Family-wise error rate","27f0fee8":"### SelecKBest ","002984ed":"On comparison, we can observe that the Feature Selection method with Chi2 scoring function has selected:\n\n1. Whole_Weight\n2. Shucked_Weight\n3. Shell_Weight\n4. Rings\n\nLet's rename the columns of new feature dataset","a27270e7":"### The Bag of Words Representation\n\nText Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n\nIn order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:\n\n* **tokenizing** strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators.\n\n* **counting** the occurrences of tokens in each document.\n\n* **normalizing** and weighting with diminishing importance tokens that occur in the majority of samples \/ documents.\n\n\nIn this scheme, features and samples are defined as follows:\n\n* each individual token occurrence frequency (normalized or not) is treated as a feature.\n\n* the vector of all the token frequencies for a given document is considered a multivariate sample.\n\n\nA corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus. \n\n\nWe call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the Bag of Words or \u201cBag of n-grams\u201d representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document.\n\n\n","d5bd0050":"## Univariate Feature Selection\n\nUnivariate feature selection works by selecting the best features based on univariate statistical tests. It can be seen as a preprocessing step to an estimator.\n\nFollowing are the selection methods:\n\n1. **SelectKBest** [removes all but the  highest scoring features]\n2. **SelectPercentile** [removes all but a user-specified highest scoring percentage of features \n3. false positive rate **SelectFpr**, false discovery rate **SelectFdr**, or family wise error **SelectFwe**.\n4. **GenericUnivariateSelect** [allows to perform univariate feature selection with a configurable strategy. This allows to select the best univariate selection strategy with hyper-parameter search estimator.]\n\n\n**Note**: Since we're using a classification dataset, we will be using a the following scoring function.\n\n1. [chi2](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.chi2.html#sklearn.feature_selection.chi2)\n2. [f_classif](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif)\n3. [mutual_info_classif](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_selection.mutual_info_classif.html#sklearn.feature_selection.mutual_info_classif)\n\nIn case of regression dataset, following are the scoring functions:\n\n1. f_regression\n2. mutual_info_regression\n\nThe methods based on F-test estimate the degree of linear dependency between two random variables. On the other hand, mutual information methods can capture any kind of statistical dependency, but being nonparametric, they require more samples for accurate estimation.\n\nIf the dataset that is being used has sparse data (data represented as sparse matrices) then, **chi2**, **mutual_info_regression**, **mutual_info_classif** will deal with the data without making it dense.\n\n\n#### Statutory Warning: Do not use a regression scoring function with a classification problem, the results will be  useless.","2a768b5b":"We observe the encoded sparse vector showing one occurrence of the one word in the vocab (dog) and the other words that are not in the vocab completely ignored. This encoded vectors can then be used directly with a machine learning algorithm.","0c6c0d30":"### Count Vectorizer\n\nThe **CountVectorizer** provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary. ","73f31e92":"### Select Percentile\n\nSelect features according to a percentile of the highest scores.","ddd7a7d5":"# Feature Selection","4c8a1006":"## Loading features from dicts\n\nThe class DictVectorizer can be used to convert feature arrays represented as lists of standard Python dict objects to the NumPy\/SciPy representation used by scikit-learn estimators. \n\nDictVectorizer implements what is called one-of-K or \u201cone-hot\u201d coding for categorical (aka nominal, discrete) features. Categorical features are \u201cattribute-value\u201d pairs where the value is restricted to a list of discrete of possibilities without ordering\n\nIn short, this transformer turns lists of mappings (dict-like objects) of feature names to feature values into Numpy arrays or scipy.sparse matrices for use with scikit-learn estimators.","c044bce6":"### Word Frequencies with TfidfVectorizer\n\nAn alternative is to calculate word frequencies, and by far the most popular method is called TF-IDF. This is an acronym than stands for \u201cTerm Frequency \u2013 Inverse Document\u201d Frequency which are the components of the resulting scores assigned to each word.\n\n* Term Frequency: This summarizes how often a given word appears within a document.\n* Inverse Document Frequency: This downscales words that appear a lot across documents.\n\nTF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents.\n\nThe TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. Alternately, if you already have a learned CountVectorizer, you can use it with a TfidfTransformer to just calculate the inverse document frequencies and start encoding documents.\n\nThe same create, fit, and transform process is used as with the CountVectorizer.","480208d5":"## Text Feature Extraction","bb14a813":"#### Conclusion - As we can observe, the classifier accuracy with feature selection is less than the original. This goes to show that the feature_selection for dimension reductionality does not work well for features < 10. \n\n#### However, we shall continue using the same dataset since this notebook is more about learning the feature selection techniques than accuracy comparison","7f949d11":"### Generic Univariate Select\n\nUnivariate feature selector with configurable strategy. In this we can select the feature selection mode from one of the following:\n\n1. percentile\n2. k_best\n3. fpr\n4. fdr\n5. fwe\n","4d2bf3f2":"In the above example,\n\nA vocabulary of 8 words is learned from the documents and each word is assigned a unique integer index in the output vector. \n\nThe inverse document frequencies are calculated for each word in the vocabulary, assigning the lowest score of 1.0 to the most frequently observed word: \u201cthe\u201d at index 7.\n\nFinally, the first document is encoded as an 8-element sparse array and we can review the final scorings of each word with different values for \u201cthe\u201c, \u201cfox\u201c, and \u201cdog\u201d from the other words in the vocabulary.\n\nThe scores are normalized to values between 0 and 1 and the encoded document vectors can then be used directly with most machine learning algorithms."}}