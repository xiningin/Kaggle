{"cell_type":{"71e8dce4":"code","3829f2f6":"code","2eb8cfbf":"code","b2c4ad32":"code","35683881":"code","50364380":"markdown","271dffcf":"markdown","63cf060e":"markdown","081b8b89":"markdown","78a1c106":"markdown"},"source":{"71e8dce4":"import pandas as pd\n\ndef prepX(xs):\n    result = xs.copy()\n    result['Sex'] = (result['Sex'] == 'male').astype(int)\n    result['No cabin'] = result['Cabin'].isna().astype(int)\n    result['Cabin'] = [ord(x[0]) - ord('A') + 1 for x in result['Cabin'].fillna(chr(ord('A') - 1))]\n    result['Alone'] = ((result['SibSp'] == 0) & (result['Parch'] == 0)).astype(int)\n    \n    # When NaN -- assume Southampton:\n    result['Embarked'] = result['Embarked'].map({'S': 0, 'C': 1, 'Q' : 2}).fillna(0)\n    return result\n\ndef prepAgeClassMedian(xs):\n    medians = xs.groupby('Pclass')['Age'].median()\n    xs['Age'] = xs['Age'].fillna(\n        pd.Series(data=medians[xs['Pclass']].values, index=xs.index)) \/\/ 10 * 10\n    return xs\n\ndef loadXY(trainOrTest, rm = [], prepX = lambda x: x):\n    df = pd.read_csv(f'\/kaggle\/input\/titanic\/{trainOrTest}.csv').drop(columns=rm)\n    return (prepX(df[[x for x in df.columns if x != 'Survived']]), \n            df['Survived'] if 'Survived' in df.columns else None)\n\nxs, ys = loadXY('train', rm = ['PassengerId', 'Name', 'Ticket', 'Fare'], \n                prepX = lambda x: prepAgeClassMedian(prepX(x)))\nxs","3829f2f6":"(xs['Sex'] != ys).sum() \/ 891","2eb8cfbf":"from sklearn.ensemble import RandomForestClassifier \nfrom sklearn.metrics import accuracy_score, make_scorer \nfrom sklearn.model_selection import GridSearchCV\n\nparams = { \n    'bootstrap': [False], \n    'class_weight' : [None, 'balanced'], \n    'max_depth': [3, 4, 5], \n    'max_features' : [1.0], \n    'min_samples_leaf' : [1, 2, 3, 4], \n    'n_estimators': [2, 4, 8]}\n\ngrid_search = GridSearchCV(\n    n_jobs = 2, \n    estimator = RandomForestClassifier(), \n    scoring = make_scorer(accuracy_score), \n    param_grid = params, \n    cv = 4, verbose = 1) \ngrid_search.fit(xs, ys)","b2c4ad32":"print(grid_search.best_params_)\nprint(grid_search.best_score_)","35683881":"xTest, yTest = loadXY('test', rm = ['Name', 'Ticket', 'Fare'], \n                      prepX = lambda x: prepAgeClassMedian(prepX(x)))\n\npd.DataFrame({\n    'PassengerId' : xTest['PassengerId'],\n    'Survived' : RandomForestClassifier(**grid_search.best_params_)\n                     .fit(xs,ys).predict(xTest.drop(columns = 'PassengerId'))}\n).to_csv('submission.csv', index=False)","50364380":"Generate the prediction:","271dffcf":"# Titanic competition - attempt with minimal manual feature engineering\nKey takeaways:\n- Sex the most important feature of the dataset. It is worth to ensure that decision tree-based classifiers start splitting with it. Hence, 'max_features' : [1.0],\nin the grid search.\n- For decision tree-based classifiers, features with moderate amount of values work best \n(see how the Age is treated for example).\n\nImprovement directions:\n- accuracy might not be the best metric for the binary classification problems.\n- more features?","63cf060e":"Take a look at the best parameters and test set accuracy.","081b8b89":"## Grid search for the optimal parameters","78a1c106":"Baseline for the model accuracy is the \"prediction\" that all male passengers die:"}}