{"cell_type":{"452c0493":"code","9cffd122":"code","dc0cc536":"code","1fad0c43":"code","c4be9082":"code","489d6414":"code","b207f387":"code","6a70e659":"code","b908c0a7":"code","1df62306":"code","984fc842":"code","93ddb415":"code","8114d147":"code","c72b2f3a":"code","b3cb823c":"code","dd8f73e5":"code","3a21955b":"code","56792d87":"code","6ff87a7b":"code","909f8ffc":"code","60c4f512":"code","a19bc274":"code","3919b421":"code","91940ae6":"code","e94f98f3":"markdown","85ef0bae":"markdown","99fef4d2":"markdown","79f1e2ee":"markdown"},"source":{"452c0493":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9cffd122":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv('\/kaggle\/input\/online-shoppers-intention\/online_shoppers_intention.csv')\ndf","dc0cc536":"df.isna().sum()","1fad0c43":"df.info()","c4be9082":"# We have enough data, we don't have to impute the missing values,\n# we can just drop them.\ndf.dropna(inplace=True)","489d6414":"df.isna().sum()","b207f387":"df.nunique()","6a70e659":"from sklearn.model_selection import train_test_split\nX_train, X_test = train_test_split(df, test_size=0.2)\ny_train, y_test = X_train.pop('Revenue'), X_test.pop('Revenue')","b908c0a7":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.linear_model import LogisticRegression\n\ncategorical_features = ['SpecialDay', 'Month', 'OperatingSystems', 'Browser', 'Region', 'TrafficType', 'VisitorType',\n                        'Weekend']\nohe = OneHotEncoder(handle_unknown='ignore')\ntransformer = make_column_transformer((ohe, categorical_features))\nclf = LogisticRegression()\nbasic_pipe = make_pipeline(transformer, clf)\nbasic_pipe.fit(X_train, y_train)\nbasic_pipe.score(X_test, y_test)","1df62306":"# Let's checkout correlation between features and the label\ndf.corr()['Revenue']","984fc842":"# It seems llike there is high corr between PageValues and the Revenue","93ddb415":"# From my experience RFC works very well for classification puposes, let's check it out\n# I also use GridSearch for hyperparameters optimization","8114d147":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\n\n\ncategorical_features = ['SpecialDay', 'Month', 'OperatingSystems', 'Browser', 'Region', 'TrafficType',\n                        'VisitorType', 'Weekend']\nnumerical_features = ['Administrative','Administrative_Duration','Informational','Informational_Duration',\n                      'ProductRelated','ProductRelated_Duration','BounceRates','ExitRates',\n                      'PageValues','SpecialDay']\nohe = OneHotEncoder(handle_unknown='ignore')\nss = StandardScaler()\ntransformer = make_column_transformer((ohe, categorical_features),\n                                      (ss, numerical_features),\n                                      remainder='passthrough')\nclf = RandomForestClassifier()\nrfc_pipe = Pipeline([('transformer', transformer), \n                    ('rf', clf)])\n\n# Lets optimize some hyperparams\nparams = {'rf__n_estimators':[100,150,200],\n          'rf__max_depth':[None, 30,50]}\ngrid_pipe = GridSearchCV(rfc_pipe, param_grid=params, cv=5)\ngrid_pipe.fit(X_train, y_train)\nprint(grid_pipe.best_params_)\nprint(grid_pipe.best_score_)","c72b2f3a":"encoded_X_train = grid_pipe.best_estimator_.get_params()['transformer'].transform(X_train)\nencoded_X_train","b3cb823c":"# Let's visualize the feature importance of the forest\n\nbest_forest = grid_pipe.best_estimator_.get_params()['rf']\nimportances = best_forest.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in best_forest],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(encoded_X_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n    \n# Plot the feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(encoded_X_train.shape[1]), importances[indices],\n       color=\"r\", yerr=std[indices], align=\"center\")\n# Let's plot only 17 features\nplt.xticks(range(X_train.shape[1]), indices)\nplt.xlim([-1, X_train.shape[1]])\nplt.show()","dd8f73e5":"encoded_X_train = grid_pipe.best_estimator_.get_params()['transformer'].transform(X_train)\nencoded_X_train","3a21955b":"encoded_X_test = grid_pipe.best_estimator_.get_params()['transformer'].transform(X_test)","56792d87":"# Let's try to use only those best features for training a new RFC\nbest_features_list = []\nscore_list = [0]\n# Let's find the ideal amount of feature between the first 30\nfor i in range(1, 30):\n    amount_of_selected_features = i\n    for f in range(amount_of_selected_features):\n        best_features_list.append(indices[f])\n\n    rfc = RandomForestClassifier(max_depth=30, n_estimators=100)\n    rfc.fit(encoded_X_train[:, best_features_list], y_train)\n\n    score_list.append(rfc.score(encoded_X_test[:, best_features_list],\n                                y_test))","6ff87a7b":"# A comment, this is very unstable search because of the nature of RFC.\n# You can't really relay on one iteration but for simplicity let's do it for one.\nprint(f'best amount of features: {np.argmax(score_list)}.'\n      f'score: {np.max(score_list).round(5)}')","909f8ffc":"# Using all of the features","60c4f512":"from catboost import CatBoostClassifier, Pool\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\nendoded_train_label = le.fit_transform(y_train)\nendoded_test_label = le.fit_transform(y_test)\n\ntransformer = make_column_transformer((ohe, categorical_features),\n                                      (ss, numerical_features),\n                                      remainder='passthrough')\n\nmodel = CatBoostClassifier(iterations=20,\n                           depth=2,\n                           learning_rate=1,\n                           loss_function='CrossEntropy',\n                           verbose=False)\n\ncatboost_pipe = Pipeline([('transformer', transformer), \n                         ('cb', model)])\n\n\n# Lets optimize some hyperparams\nparams = {'cb__iterations':[10,20],\n          'cb__depth':[2, 3, 4],\n         'cb__learning_rate': np.linspace(0.75, 1, 3)}\n\ncb_pipe = GridSearchCV(catboost_pipe, param_grid=params, cv=5)\n\n#train the model\ncb_pipe.fit(X_train, endoded_train_label)\n\nprint(cb_pipe.best_params_)\nprint(cb_pipe.best_score_)","a19bc274":"# Select Best features according to f_classif test, \n# with the best amount of features from the previous step.","3919b421":"from catboost import CatBoostClassifier, Pool\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_selection import SelectKBest, f_classif\n\nle = LabelEncoder()\nendoded_train_label = le.fit_transform(y_train)\nendoded_test_label = le.transform(y_test)\n\ntransformer = make_column_transformer((ohe, categorical_features),\n                                      (ss, numerical_features),\n                                      remainder='passthrough')\n\nmodel = CatBoostClassifier(iterations=10,\n                           depth=3,\n                           learning_rate=0.75,\n                           loss_function='CrossEntropy',\n                           verbose=False)\nbestk = SelectKBest(score_func=f_classif, k=np.argmax(score_list))\n\ncatboost_pipe = Pipeline([('transformer', transformer), \n                          ('bestk', bestk),\n                         ('cb', model)])\n\n#train the model\ncatboost_pipe.fit(X_train, endoded_train_label)\n\nprint(catboost_pipe.score(X_test, endoded_test_label))","91940ae6":"# Conclusions:\n# It's looks like taking the best amount of features from RFC,\n# and then select by f_classif this amount of features gave us the best score","e94f98f3":"# Base model - LogisticRegression","85ef0bae":"# Boosting - CatBoost","99fef4d2":"# RandomForestClassifier","79f1e2ee":"# EDA"}}