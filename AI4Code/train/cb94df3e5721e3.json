{"cell_type":{"8c977c9b":"code","853ad977":"code","ea9155de":"code","5f923c81":"code","fe3715b0":"code","0eb0e2cd":"code","30ebe623":"code","8e819921":"code","251d74f8":"code","1aeb9858":"code","0b82e33c":"code","629c14b5":"code","17c4cd8f":"code","7235c242":"code","800b54cd":"code","2f1983d5":"code","e1d78c68":"code","044244cb":"code","e6273fd9":"code","21726cfc":"code","f6a1d41a":"code","0cb0d019":"code","84fbb595":"code","406f7a4c":"code","cf4baef7":"code","0bfa551e":"code","097de27b":"code","5e81121b":"code","d1c6ace2":"code","96f2d1ae":"code","771eccea":"code","b993d9db":"code","d02dfe31":"code","33950445":"code","9df697a9":"code","a9d7f68e":"code","8418aadc":"code","c4d423cc":"code","6724deb3":"code","ba59a18f":"code","ced42893":"code","4ba9342c":"code","f60e6cf1":"code","35a25422":"code","befe4c74":"code","52a58a74":"code","708c4b09":"code","9297242a":"code","f636a161":"code","ade6fd66":"code","c76ac1be":"code","d37dbdf9":"code","2aa4d689":"code","ecdebaed":"code","80a7bb96":"code","e7b140e9":"code","5eee5168":"code","94a1f358":"code","e29d4274":"code","3978afe4":"code","62c35af2":"code","c8173968":"code","4fe97400":"code","05cd3f3c":"markdown","b7f0eaf8":"markdown","ba5e491d":"markdown","9d8b30ee":"markdown","d502ee4b":"markdown","5e91b1ae":"markdown","3535e555":"markdown","57837dcd":"markdown","cae29e61":"markdown","01f3d899":"markdown","04759d10":"markdown","b0fee9fd":"markdown","0c2cf8ba":"markdown","8b859f67":"markdown","63ffb85b":"markdown","6add5bee":"markdown","b328111a":"markdown","52a34453":"markdown","6c4349c2":"markdown","6eaea15a":"markdown","0db4392a":"markdown","011b52b3":"markdown","729a4adf":"markdown","a0c82d93":"markdown"},"source":{"8c977c9b":"import pandas as pd \nimport plotly.express as px \nimport numpy as np \nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","853ad977":"df = pd.read_csv('..\/input\/game-data\/game_data\/video_games.csv')","ea9155de":"df.isnull().sum()","5f923c81":"df = df.dropna(subset = ['name'])","fe3715b0":"bin_labels = ['low','medium','high','very_high']\ndf['achievements'] = pd.qcut(df['achievements'],q = 4, labels = bin_labels).astype(str).str.replace('nan','missing')\ndf['achievements'] = df['achievements'].fillna('missing')","0eb0e2cd":"color_palette = sns.color_palette(\"vlag\", as_cmap=True)\n\ndf['achievements'].value_counts().to_frame().style.background_gradient()","30ebe623":"df['release_date'] = df['release_date'].str.replace(r'[a-zA-Z\\,\\\/]+','')#.str.extract('\\d{4}')\ndf['release_date'] = df['release_date'].str.replace(r'[^\\w\\s]','')#.str.extract('\\d{4}')\ndf['release_date'] = df['release_date'].str.replace(r'[^\\x00-\\x7F]+','')#.str.extract('\\d{4}')","8e819921":"years = df['release_date'].str.split().str[-1]","251d74f8":"years = years.fillna('1500')\nyears[~years.str.contains('\\d{4}')] = '1500'\nyears = years.str[-4:]","1aeb9858":"years = years.astype(int)\ndf['release_date'] = pd.cut(years, bins = [0,1500,2005,2010,2012,2015,2017,2018,2050], labels = ['missing','oldest','very_old','kinda_old','old','new','newer','newest'])\ndf['release_date'] = df['release_date'].astype(str).replace('nan','missing')\ndf['release_date'].value_counts().to_frame().style.background_gradient()","0b82e33c":"df['original_price'] = df['original_price'].fillna('missing')","629c14b5":"df['original_price'] = df['original_price'].str.replace(r'$','').str.lower()","17c4cd8f":"df['original_price'][df['original_price'].str.contains('free')] = 'free'","7235c242":"df['original_price'][df['original_price'].str.contains('demo')] = 'trial'","800b54cd":"df['original_price'][df['original_price'].str.contains('trial|try')] = 'trial'","2f1983d5":"df['original_price'][df['original_price'].str.contains('[^\\x00-\\x7F]+')] = 'trial'","e1d78c68":"df['original_price'][(df['original_price'].str.contains('[a-zA-Z]+')) & (~df['original_price'].str.contains('missing|demo|free|trial'))] = 'trial'","044244cb":"df['original_price'][~df['original_price'].str.contains('missing|free|trial')] = (df['original_price'][~df['original_price'].str.contains('missing|free|trial')].astype(float) + 1).round().astype(str)","e6273fd9":"df['original_price'][~df['original_price'].str.contains('missing|demo|free|trial')].astype(float).describe()","21726cfc":"import re \nprice_series = df['original_price']\nfor price in df['original_price'].iteritems():\n    if re.search('\\d+',price[1]):\n        int_price = float(price[1])\n        if int_price < 5:\n            price_series.loc[price[0]] = 'very cheap'\n        elif 5 < int_price < 15: \n            price_series.loc[price[0]] = 'cheap'\n        elif 15 < int_price < 50: \n            price_series.loc[price[0]] = 'affordable'\n        elif 50 < int_price < 150: \n            price_series.loc[price[0]] = 'costly'\n        elif int_price > 150: \n            price_series.loc[price[0]] = 'very_costly'\n        else:\n            price_series.loc[price[0]] = 'missing'\n            \n            \n    ","f6a1d41a":"price_series.value_counts().to_frame().style.background_gradient()","0cb0d019":"df['original_price'] = price_series","84fbb595":"df['developer'] = df['developer'].fillna('missing')\ndf['developer'][df['developer'].isin(df['developer'].value_counts()[df['developer'].value_counts() <= 2].index)] = 'other'","406f7a4c":"df['num_reviews'] = pd.qcut(df['num_reviews'],q = 5, labels = ['very_less','less','ok','good','very good'])","cf4baef7":"df['num_reviews'] = df['num_reviews'].astype(str).replace('nan','missing')\ndf['num_reviews'] = df['num_reviews'].fillna('missing')","0bfa551e":"df['num_reviews'].value_counts().to_frame().style.background_gradient()","097de27b":"df['percent_positive'] = pd.cut(df['percent_positive'].str[:-1].astype(float),5, labels = ['very_bad','bad','ok','good','very_good']).astype(str).replace('nan','missing')\ndf['percent_positive'] = df['percent_positive'].fillna('missing')","5e81121b":"df['percent_positive'].value_counts().to_frame().style.background_gradient()","d1c6ace2":"df['multiplayer'] = df['multiplayer'].astype(str).fillna('missing')\ndf['multiplayer'] = df['multiplayer'].fillna('missing')","96f2d1ae":"df = df.dropna()\n","771eccea":"df = df.drop(columns = 'name')","b993d9db":"df = pd.read_csv('..\/input\/game-data\/game_data\/filtered_and_processed_games.csv')\ndf = df.drop(columns = 'Unnamed: 0').dropna()","d02dfe31":"from sklearn.preprocessing import OrdinalEncoder \nX = df\noe = OrdinalEncoder()\nX_categorical = oe.fit_transform(X)","33950445":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_categorical_scaled = sc.fit_transform(X_categorical)","9df697a9":"from sklearn.cluster import KMeans, MiniBatchKMeans\nfrom sklearn.metrics import silhouette_score\nfrom tqdm.notebook import tqdm\nrange_n_clusters = [3,4,5] \nsill_scores = []\ninteria = []\nfor n_cluster in tqdm(range_n_clusters): \n    algo = MiniBatchKMeans(n_clusters=n_cluster, batch_size = 256 * 12)\n    cluster_labels = algo.fit_predict(X_categorical_scaled)\n    interia.append(algo.inertia_)\n    sill_ave = silhouette_score(X_categorical_scaled,cluster_labels,) \n    sill_scores.append(sill_ave)\n    \n    ","a9d7f68e":"import matplotlib.pyplot as plt \nplt.style.use('seaborn')\nplt.plot(range_n_clusters, sill_scores)","8418aadc":"plt.plot(range_n_clusters, interia)","c4d423cc":"import tensorflow as tf \nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras import layers \nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.utils import plot_model","6724deb3":"# expeted architecure \n# 8 --> 6 --> 4 --> [3] --> 4 --> 6 --> 8  \ninputs = tf.keras.Input(shape = 8) \nencoder_six = layers.Dense(units = 6, activation = 'relu')(inputs) \nencoder_four = layers.Dense(units = 4, activation = 'relu')(encoder_six) \n\nmiddle_three = layers.Dense(units = 3, activation = 'relu')(encoder_four)\n\ndecoder_four = layers.Dense(units = 4, activation = 'relu')(middle_three) \n\ndecoder_six = layers.Dense(units = 6, activation = 'relu')(decoder_four) \n\noutputs = layers.Dense(8, activation = 'relu')(decoder_six) \n\nautoencoder = tf.keras.Model(inputs = inputs, outputs = outputs, name = 'encoder-decoder')\n\n\nautoencoder.summary()","ba59a18f":"encoder = tf.keras.Model(inputs, outputs, name = 'encoder')","ced42893":"from sklearn.preprocessing import MinMaxScaler \nscaler = MinMaxScaler()\nX_categorical_minmax = scaler.fit_transform(X_categorical) \n","4ba9342c":"import tensorflow as tf \nautoencoder.compile(loss=\"mse\" ,optimizer=SGD(lr=1.3))\nautoencoder.fit(X_categorical_minmax,X_categorical_minmax, epochs = 9)","f60e6cf1":"X_categorical_minmax_pred_3dim = encoder.predict(X_categorical_minmax)","35a25422":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom tqdm.notebook import tqdm\n\ndef perform_kmeans(features):\n    \"\"\"Performs MiniBatchKmeans and returns respective scores\n    Params:\n    =====\n    features: np.array \n        Featurs to cluser.\n    Returns: tuple(list,list,list)\n        returns number of clusters and respective sillouet score and inertia.\n    \n    \"\"\"\n    range_n_clusters = [3,4,5] \n    sill_scores = []\n    interia = []\n    for n_cluster in tqdm(range_n_clusters): \n        algo = MiniBatchKMeans(n_clusters=n_cluster, batch_size=256)\n        cluster_labels = algo.fit_predict(features)\n        interia.append(algo.inertia_)\n        sill_ave = silhouette_score(features,cluster_labels) \n        sill_scores.append(sill_ave)\n    return range_n_clusters, sill_scores, interia \n    \n    ","befe4c74":"range_n_clusters_3dim, sill_scores_3dim, interia_3dim = perform_kmeans(X_categorical_minmax_pred_3dim)","52a58a74":"plt.figure(dpi = 100)\nplt.plot(range_n_clusters, sill_scores, label = 'kmeans')\nplt.plot(range_n_clusters_3dim, sill_scores_3dim, label = 'auto_encoder3d + kmeans')\nplt.title('sill score')\nplt.legend()","708c4b09":"plt.figure(dpi = 100)\nplt.plot(range_n_clusters, interia,label = 'kmeans')\nplt.plot(range_n_clusters, interia_3dim,label = 'auto_encoder3d + kmeans')\nplt.legend()","9297242a":"# 8 --> 4 --> 4 --> 2 --> 4 --> 4 --> 8\nencoder_2dim = Sequential()\nencoder_2dim.add(Dense(units=4,activation='relu',input_shape=[8]))\nencoder_2dim.add(Dense(units=4,activation='relu',input_shape=[4]))\nencoder_2dim.add(Dense(units=2,activation='relu',input_shape=[4]))\n\ndecoder_2dim = Sequential()\ndecoder_2dim.add(Dense(units=4,activation='relu',input_shape=[2]))\ndecoder_2dim.add(Dense(units=4,activation='relu',input_shape=[4]))\ndecoder_2dim.add(Dense(units=8,activation='relu',input_shape=[4]))\n\n\nautoencoder_2dim = Sequential([encoder_2dim,decoder_2dim])\n\n\nautoencoder_2dim.compile(loss=\"mse\" ,optimizer=SGD(lr = 1))\nautoencoder_2dim.fit(X_categorical_minmax,X_categorical_minmax, epochs = 9)\nX_categorical_minmax_pred_2dim = encoder_2dim.predict(X_categorical_minmax)\nrange_n_clusters_2dim, sill_scores_2dim, interia_2dim = perform_kmeans(X_categorical_minmax_pred_2dim)\n\n\nplt.figure(dpi = 100)\nplt.plot(range_n_clusters, sill_scores, label = 'kmeans')\nplt.plot(range_n_clusters_3dim, sill_scores_3dim, label = 'auto_encoder3d + kmeans')\nplt.plot(range_n_clusters_2dim, sill_scores_2dim, label = 'auto_encoder2d + kmeans')\nplt.title('sill score')\nplt.legend()","f636a161":"plt.figure(dpi = 100)\nplt.plot(range_n_clusters, interia,label = 'kmeans')\nplt.plot(range_n_clusters, interia_3dim,label = 'auto_encoder3d + kmeans')\nplt.plot(range_n_clusters, interia_2dim,label = 'auto_encoder2d + kmeans')\nplt.title('inertia')\nplt.legend()","ade6fd66":"kmeans = MiniBatchKMeans(n_clusters=4)\ncluster_labels = kmeans.fit_predict(X_categorical_minmax)","c76ac1be":"from sklearn.decomposition import PCA\npca_kmeans = PCA(n_components=3)\npca_kmeans_components = pca_kmeans.fit_transform(X_categorical_minmax)","d37dbdf9":"kmeans_dataframe = pd.DataFrame(pca_kmeans_components)\nkmeans_dataframe['labels'] = cluster_labels","2aa4d689":"fig = px.scatter_3d(kmeans_dataframe, x=0, y=1, z=2,\n                    color='labels', template = 'plotly', color_discrete_sequence=['Set2'])\nfig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nfig.show()","ecdebaed":"from sklearn.decomposition import PCA\npca_kmeans = PCA(n_components=2)\npca_kmeans_components = pca_kmeans.fit_transform(X_categorical_minmax)\nkmeans_dataframe = pd.DataFrame(pca_kmeans_components)\nkmeans_dataframe['labels'] = cluster_labels\nfig = px.scatter(kmeans_dataframe, x=0, y=1,\n                    color='labels', template = 'seaborn', color_discrete_sequence=['Set2'], height = 500)\n#fig.update_layout(margin=dict(l=0, r=0, b=0, t=0))\nfig.show()","80a7bb96":"auto_enc_kmeans = KMeans(n_clusters=4, random_state=1)\nauto_enc_cluster_labels = auto_enc_kmeans.fit_predict(X_categorical_minmax_pred_3dim)\n\nauto_enc_pca_kmeans = PCA(n_components=3)\nauto_enc_pca_kmeans_components = auto_enc_pca_kmeans.fit_transform(X_categorical_minmax)","e7b140e9":"auto_enc_kmeans_dataframe = pd.DataFrame(auto_enc_pca_kmeans_components)\nauto_enc_kmeans_dataframe['labels'] = auto_enc_cluster_labels","5eee5168":"fig = px.scatter_3d(auto_enc_kmeans_dataframe, x=0, y=1, z=2,\n                    color='labels')\nfig.update_layout(margin=dict(l=7, r=1, b=1, t=7))\nfig.show()","94a1f358":"auto_enc_pca_kmeans = PCA(n_components=2)\nauto_enc_pca_kmeans_components = auto_enc_pca_kmeans.fit_transform(X_categorical_minmax)\nauto_enc_kmeans_dataframe = pd.DataFrame(auto_enc_pca_kmeans_components)\nauto_enc_kmeans_dataframe['labels'] = auto_enc_cluster_labels\nfig = px.scatter(auto_enc_kmeans_dataframe, x=0, y=1,\n                    color='labels')\n#fig.update_layout(margin=dict(l=7, r=1, b=1, t=7))\nfig.show()","e29d4274":"df['clusters'] = cluster_labels","3978afe4":"#df = pd.read_csv('..\/input\/game-data\/game_data\/processed_data.csv')","62c35af2":"px.histogram(data_frame=df, x = 'release_date', color ='clusters', barmode = 'group', template = 'seaborn', height = 500,histnorm='percent')","c8173968":"px.histogram(data_frame=df, x = 'achievements', color ='clusters', barmode = 'group', template = 'seaborn', height = 500,histnorm='percent')","4fe97400":"df['achievements'].value_counts()","05cd3f3c":"# 4.2.2 Auto Encoder Sequential API","b7f0eaf8":"# 5.  Making sense of the clusters","ba5e491d":"looking at achievements column I observed there are many missing values so I decided to convert it into categorical variables. and the decision behind creation of \ncategories was based on percentile. \n1. if less than .25 low achievements \n2. between .25 and .50 medium achievements\n3. between .50 and .75 high achievements \n4. more than .75 very high achievements \n5. NaN: no achievements \n\n\n![image.png](attachment:c9667890-ed06-4df1-bfd6-1b83c0c48622.png)","9d8b30ee":"\n# 4.2 Using auto encoders neural networks to find number of clusters \n1. Autoencoder is an unsupervised artificial neural network that learns how to efficiently compress and encode data then learns how to reconstruct the data back from the reduced encoded representation to a representation that is as close to the original input as possible. Autoencoder, by design, reduces data dimensions by learning how to ignore the noise in the data.\n2. I have coded out two examples of auto-encoders the first one concerns with functional API ans second one with Sequential API. ","d502ee4b":"# 2.2 Handling Achievements\nAfter looking at the features we find that there are many missing values and we need to take care of these missing values. We cannot drop any missing values because doing do will seriously undermine profits of the company.","5e91b1ae":"## Looking at the analysis cluster 4 of auto encoded 3 dim which as highest sill score and good inertia is choice","3535e555":"## Conclusion of Clustering \nEven though K-Means with Auto encoders gave a better score but when visualized the results were sub optimal so we stick with normal kmeans","57837dcd":"# 3.2 Scaling Variables. \nThe goal is to convert these games into 5 categories lets perform K means clustering to find out how these featured do. Moreover k means is a distance based alogrithm so lets scale these features using Standard scalar to bring all features to a single scale. \n","cae29e61":"# 2.3 Handling Date\n- I thought date will be very important parameter for clustering \n- After observing at date column; I observed that date format is not very consistent. and only 50% of the dateset follows a convention \n- I thought instead of keeping date values I will convert them into categorical variable which tells about release of game. \n- Logic:\n    - Filling NaN with 1500 \n    - Handling Other inconsistent dates. \n    - Removing ASCII values. \n\n","01f3d899":"# 2.4 Handling original_price \n1. if contains free make price replace with free \n2. if contains demo replace with demo \n3. missing or any other replace with missing \n4. and remaining value make make categories \n5. Convert costs into categories:\n    1. Very Costly\n    2. Costly.\n    3. Affordable \n    4. Cheap \n6. Categories:\n```\n    1.less than 5: very cheap \n    2.5-15: cheap \n    3.15-50: affordable \n    4.50-150: costly \n    5.150 and above : very costly \n    6.free \n    7.trial\n    8.missing \n```\n\n","04759d10":"# 4 clusters have least inertia and highest sillhoutte scores hence the number of clusters I am taking into consideration is 4","b0fee9fd":"# 2.1 Dropping rows which do not have any names","0c2cf8ba":"# 1. Importing and Loading data. \n## *If you are interested in feature engineering and how I developed features for this dataset, start from section 1. otherwise you can skip to section 3, because I took a lot of time handelling null values and discretizing the features*","8b859f67":"# 2.3 Handling release_Date\nAfter looking at the distribution of date I created following categories \n1. greater than 2018: Newest \n2. 2017-2018: Newer \n3. 2012-2015: New\n4. 2010-2012: Old \n5. 2005-2010: Very Old \n6. less than 2005 oldest ","63ffb85b":"# 3. Data Processing","6add5bee":"# 2.6 Handling num_reviews\nConverting reviews into categorical variable\n1. 9.999, 19.0] : very less\n2. (19.0, 40.0] : less\n3. (40.0, 108.0] : ok\n4. (108.0, 446.0] : good\n5. (446.0, 836608.0] : very good \n6. NaN: missing ","b328111a":"# 2.5 Handling Developers. \nif number of games developed by developer is less than 2 make that developer other and fill the missing values with missing.  ","52a34453":"# 2. Handling Nulls and Feature Engineering","6c4349c2":"# Filling missing multiplayer with missing variable ","6eaea15a":"# 3.1 Encoding Categorical Variables","0db4392a":"# Handling Percent Positive. \nConverting percent_positive into categorical features \n1. (-0.1, 20.0] : very bad \n2. (20.0, 40.0] : bad \n3. (40.0, 60.0] : ok \n4. (60.0, 80.0] : good \n5. (80.0, 100.0] : very good \n6. NaN: missing ","011b52b3":"# 4.2.1 Autoencoder Functional API","729a4adf":"## We can group the games based on the pre-defined categroies below if the user know what he or she wants, But the problem with this basic approach is there are too many features for a user to look at so we will go for unsupervised learning methods for clustering the games and later make sense of those features.\n\n\n\n![image.png](attachment:8901c2d0-ddf6-4b9e-9566-c114b8922f86.png)","a0c82d93":"# 4.1 Kmeans \n1. Silhouette score for a set of sample data points is used to measure how dense and well-separated the clusters are.\n2. Silhouette score takes into consideration the intra-cluster distance between the sample and other data points within the same cluster (a) and inter-cluster distance between the sample and the next nearest cluster (b).\n3. The silhouette score falls within the range [-1, 1].\n4. The silhouette score of 1 means that the clusters are very dense and nicely separated. The score of 0 means that clusters are overlapping. The score of less than 0 means that data belonging to clusters may be wrong\/incorrect.\n5. The silhouette plots can be used to select the most optimal value of the K (no. of cluster) in K-means clustering."}}