{"cell_type":{"991d616b":"code","be40e820":"code","fd5558ad":"code","c3874a80":"code","c7680334":"code","9aabaab8":"code","a00778a3":"code","2d172c57":"code","62b5a254":"code","dfec324e":"code","e5a50eb2":"code","2fa06cb6":"code","51933798":"code","0be73810":"code","0820e6b7":"code","96bd9de2":"code","5123d71b":"code","6465c5bd":"code","dc4a3977":"code","5eaf6ed3":"code","b5cb6c22":"code","521dbcee":"code","be4e88cf":"code","1518a9cc":"code","4470dcae":"code","4832e9c1":"code","d013eb63":"code","8506caab":"code","d42638fc":"code","83676a88":"code","7e0b5380":"code","f78ac3bf":"code","7b11dca5":"code","2e5147cb":"code","4b2d98cc":"code","b1d9d1a0":"code","b2ae40d3":"code","5f6d76f7":"code","9b4903db":"code","1ac41e8f":"code","e93a6ba9":"code","079137ea":"code","f3467912":"code","208b745e":"code","ff3d9746":"code","d30f4b5d":"code","ce11a32c":"code","bdd6f7d1":"code","134e93fe":"code","3d45c7ed":"code","15e3834f":"code","b306f315":"code","cc47f0fb":"code","30c05b56":"code","cc82347a":"code","7fd2960c":"code","97bbd997":"code","2a1cc1ac":"code","4d654afd":"markdown","40c24df0":"markdown","2760d0b4":"markdown","52cef04a":"markdown","572acd23":"markdown","0e208ced":"markdown","a80cf2dc":"markdown","a66219a8":"markdown","34b8f038":"markdown","d850103e":"markdown","36e437ea":"markdown","24a95f3a":"markdown","25d79153":"markdown","7327c0e0":"markdown","39d27cb1":"markdown","9d2d74e5":"markdown","9ab5ede4":"markdown","30bc6df5":"markdown","c555d88c":"markdown","4b4bd74f":"markdown","10a3dddd":"markdown","1664c3b3":"markdown","4763dcce":"markdown","90894b79":"markdown","a291c9db":"markdown","7da20068":"markdown","0b96ec42":"markdown","6577c27c":"markdown","0bacabf5":"markdown","7c4804d0":"markdown","59cae345":"markdown","d55c07d7":"markdown"},"source":{"991d616b":"# libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'figure.max_open_warning': 0})\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\", category=DeprecationWarning)\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nfrom sklearn.model_selection import train_test_split","be40e820":"# read training data\ndf = pd.read_csv('\/kaggle\/input\/actuarial-loss-estimation\/train.csv')\nprint('Number of rows and columns:', df.shape)","fd5558ad":"# drop duplicates (if any)\ndf.drop_duplicates().shape","c3874a80":"# display first rows (transposed)\ndf.head(6).T","c7680334":"# basis statistics: nominal features \ndf.describe(include=['object']).T","9aabaab8":"# basic statistics: numerical features \ndf.describe().T","a00778a3":"# Count missing values for each column\ndf.isnull().sum()","2d172c57":"# MaritalStatus: Replace NaN by 'S' (group with lowest costs, see version 1)  and calculate statistics by group\ndf = df.fillna('S')\ndf['UltimateIncurredClaimCost'].groupby(df['MaritalStatus']).agg(['count','median','mean'])","62b5a254":"# replace rare attributes:\ndf['Gender'] = df['Gender'].replace(('U'),('S'))\n\n# wage per hour (if hours worked between 5 and 100) \ndf['HourlyWages'] = np.where((df['HoursWorkedPerWeek'] >= 5.0) & (df['HoursWorkedPerWeek'] <= 100.0) , df['WeeklyWages']\/df['HoursWorkedPerWeek'],-1.)\n\n# Claim number seems to rise with time. Extract number  \ndf['ClaimNumberInt'] = df['ClaimNumber'].str[2:].astype(int)\n\n# Some date features\ndf['YearOfAccident']  = pd.DatetimeIndex(df['DateTimeOfAccident']).year\ndf['MonthOfAccident']  = pd.DatetimeIndex(df['DateTimeOfAccident']).month\ndf['DayOfAccident']  = pd.DatetimeIndex(df['DateTimeOfAccident']).day\ndf['WeekdayOfAccident']  = pd.DatetimeIndex(df['DateTimeOfAccident']).day_name()\ndf['HourOfAccident']  = pd.DatetimeIndex(df['DateTimeOfAccident']).hour\ndf['YearReported']  = pd.DatetimeIndex(df['DateReported']).year\n\n# Reporting delay in weeks \ndf['DaysReportDelay'] = pd.DatetimeIndex(df['DateReported']).date - pd.DatetimeIndex(df['DateTimeOfAccident']).date\ndf['DaysReportDelay'] = (df['DaysReportDelay']  \/ np.timedelta64(1, 'D')).astype(int)\ndf['WeeksReportDelay'] = np.floor(df['DaysReportDelay'] \/ 7.).astype(int)\ndf['WeeksReportDelay'] = np.clip(df['WeeksReportDelay'], a_max=55, a_min=None)\n\n# drop unneccessary columns\ndf.drop(['ClaimNumber','DateTimeOfAccident','DaysReportDelay','DateReported'],axis=1,inplace=True)\ndf.shape","dfec324e":"# Skewness of the numerical feature distributions\nprint(df.skew())","e5a50eb2":"# The log1p function applies log(1+x) to all elements of the column\ndf[\"LogUltimateIncurredClaimCost\"] = np.log1p(df[\"UltimateIncurredClaimCost\"])\ndf[\"LogInitialIncurredCalimsCost\"] = np.log1p(df[\"InitialIncurredCalimsCost\"])\n\n# plot distribution: claim costs (log)\nplt.subplots(figsize=(10, 6))\nsns.distplot(df.LogUltimateIncurredClaimCost, kde=False, label='Ultimate',bins=100)\nsns.distplot(df.LogInitialIncurredCalimsCost, kde=False, label='Initial', bins=100)\nplt.xlabel('claim costs (log)')\nplt.legend()\nplt.show()","2fa06cb6":"# search for frequent initial costs and calculate some statistics for ultimate costs (mean)\ndf['UltimateIncurredClaimCost'].groupby(df['InitialIncurredCalimsCost']).agg(['mean','median','min','count']).query('count >= 2000')","51933798":"# Average cost factor Ultimate \/ Initial\ndf.UltimateIncurredClaimCost.sum() \/ df.InitialIncurredCalimsCost.sum()","0be73810":"# Scatter plot: claim costs (log)\nplt.subplots(figsize=(10, 7))\nsns.scatterplot(data=df, x=\"LogInitialIncurredCalimsCost\",y=\"LogUltimateIncurredClaimCost\")\nplt.show()","0820e6b7":"# Scatter plot: zoom into claim costs (log)\nplt.subplots(figsize=(10, 7))\nsns.scatterplot(data=df.query('LogInitialIncurredCalimsCost > 6 and LogInitialIncurredCalimsCost < 12 and LogUltimateIncurredClaimCost < 12'), x=\"LogInitialIncurredCalimsCost\",y=\"LogUltimateIncurredClaimCost\")\nplt.show()","96bd9de2":"# Generate a list of numerical variables, remove claim cost variables\nnum_list = [c for c in df.columns if((df[c].dtype != np.object) and not \"Cost\" in c)] \n# plot histograms\nfor name in num_list:\n    f, ax = plt.subplots(figsize=(10, 5))\n    nbins = min(df[name].value_counts().count(),70)\n    plt.hist(data=df, x=name, bins=nbins)\n    plt.xlabel(name)\n    plt.show()","5123d71b":"# List of features with to many different values\nnum_list_bins =['HoursWorkedPerWeek','WeeklyWages','HourlyWages','ClaimNumberInt']\n\n# plot binned plot boxplots for 'LogUltimateIncurredClaimCost'\nfor name in num_list_bins:\n    f, ax = plt.subplots(figsize=(14, 5))\n    df['bin_'] = pd.cut(df[name], 8)\n    sns.boxplot(x='bin_', y='LogUltimateIncurredClaimCost', data=df)\n    plt.xlabel(name)\n    plt.show()\n\ndf.drop(['bin_'],axis=1,inplace=True)","6465c5bd":"# Generate a list of the remaining numerical variables (without binned or cost variables)\nnum_list_nobins = [c for c in num_list if(c not in num_list_bins)] \n\n# plot unbinned boxplots for 'LogUltimateIncurredClaimCost'\nfor name in num_list_nobins:\n    f, ax = plt.subplots(figsize=(14, 5))\n    sns.boxplot(x=name, y='LogUltimateIncurredClaimCost', data=df)\n    plt.xlabel(name)\n    plt.show()","dc4a3977":"# calcuate correlation matrix\ncorrmat = df.corr()\n\n# Draw the heatmap \nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, annot=True, square=True, cmap='rainbow')\nplt.show()","5eaf6ed3":"# Generate a list of categorical variables and remove those with too many different values (e.g. 'ClaimDescription')\ncat_list = [c for c in df.columns if( (df[c].dtype == np.object) & (df[c].value_counts().count() <= 25) )] \n\n# plot distributions\nfor name in cat_list:\n    f, ax = plt.subplots(figsize=(10, 5))\n    sns.countplot(x=name,data=df)\nplt.show()","b5cb6c22":"#  plot boxplots for 'LogUltimateIncurredClaimCost'\nfor name in cat_list:\n    f, ax = plt.subplots(figsize=(10, 5))\n    sns.boxplot(x=name, y=\"LogUltimateIncurredClaimCost\", data=df)\nplt.show()","521dbcee":"# dispay claim description of a) the most severe claims ...\nvars = ['UltimateIncurredClaimCost','ClaimDescription']\ndf[vars].sort_values(by='UltimateIncurredClaimCost', ascending=False).head(8)","be4e88cf":"# ... and b) the least severe claims\ndf[vars].sort_values(by='UltimateIncurredClaimCost', ascending=True).head(8)","1518a9cc":"# search for some words and create new features\ntext = ['NECK','BACK','KNEE','FINGER','EYE','STRUCK','HAMMER','LADDER','STAIR','FELT','TRAUMA']\nfor name in text:\n    df['CD_' + name] = np.where( (df['ClaimDescription'].str.find(name) < 0), 0, 1)\n\n# some two or tree word features\ndf['CD_FOREIGN_BODY'] = np.where( (df['ClaimDescription'].str.find('FOREIGN BODY') < 0), 0, 1)\ndf['CD_BACK_STRAIN']  = np.where( (df['ClaimDescription'].str.find('BACK STRAIN') < 0), 0, 1)\ndf['CD_SOFT_TISSUE_'] = np.where( (df['ClaimDescription'].str.find('SOFT TISSUE INJURY') < 0), 0, 1)\ndf['CD_WORKPLACE_STRESS'] = np.where( (df['ClaimDescription'].str.find('WORKPLACE STRESS') < 0), 0, 1)\ndf['CD_LOWER_BACK_STRAIN'] = np.where( (df['ClaimDescription'].str.find('LOWER BACK STRAIN') < 0), 0, 1)\n\n# body side, lacerated\/laceration:\ndf['CD_LEFT_RIGHT'] = np.where( ((df['ClaimDescription'].str.find('LEFT') < 0) & (df['ClaimDescription'].str.find('RIGHT') < 0)), 0, 1)\ndf['CD_LACERAT_'] = np.where( (df['ClaimDescription'].str.find('LACERAT') < 0), 0, 1)\ndf['UltimateIncurredClaimCost'].groupby(df['CD_LACERAT_']).agg(['count','median','mean'])","4470dcae":"# Print the most expensive claim amounts\nvars = ['UltimateIncurredClaimCost']\nprint(df[vars].sort_values(by='UltimateIncurredClaimCost', ascending=False).head(8))","4832e9c1":"# Since the four million outliner may affect samples we limit it to one million. (The influence of this step needs further investigation)\ndf['UltimateIncurredClaimCost'] = np.where(df['UltimateIncurredClaimCost'] > 1000000, 1000000., df['UltimateIncurredClaimCost']) * 1.000\ndf['UltimateIncurredClaimCost'].mean() ","d013eb63":"# To avoid confusion (e.g. log or untransformed), we give the target a new name: loss\ndf['loss'] = df[\"UltimateIncurredClaimCost\"]\n# drop unneccessary columns\ndf.drop(['ClaimDescription','UltimateIncurredClaimCost','LogUltimateIncurredClaimCost','LogInitialIncurredCalimsCost'],axis=1,inplace=True)","8506caab":"# read test data\ndf_test = pd.read_csv('..\/input\/actuarial-loss-estimation\/test.csv')\nprint('Number of rows and columns:', df_test.shape)","d42638fc":"# MaritalStatus: Replace NaN by 'S'\ndf_test = df_test.fillna('S')\n\n# Basic feature enineering:\n\n# group\/cut rare attributes:\ndf_test['Gender'] = df_test['Gender'].replace(('U'),('S'))\n\n# wage per hour (if hours worked between 5 and 100) \ndf_test['HourlyWages'] = np.where((df_test['HoursWorkedPerWeek'] >= 5.0) & (df_test['HoursWorkedPerWeek'] <= 100.0) , df_test['WeeklyWages']\/df_test['HoursWorkedPerWeek'],-1.)\n\n# Claim number seems to rise with time. Extract number  \ndf_test['ClaimNumberInt'] = df_test['ClaimNumber'].str[2:].astype(int)\n\n# Some date features\ndf_test['YearOfAccident']  = pd.DatetimeIndex(df_test['DateTimeOfAccident']).year\ndf_test['MonthOfAccident']  = pd.DatetimeIndex(df_test['DateTimeOfAccident']).month\ndf_test['DayOfAccident']  = pd.DatetimeIndex(df_test['DateTimeOfAccident']).day\ndf_test['WeekdayOfAccident']  = pd.DatetimeIndex(df_test['DateTimeOfAccident']).day_name()\ndf_test['HourOfAccident']  = pd.DatetimeIndex(df_test['DateTimeOfAccident']).hour\ndf_test['YearReported']  = pd.DatetimeIndex(df_test['DateReported']).year\n\n# Reporting delay in weeks \ndf_test['DaysReportDelay'] = pd.DatetimeIndex(df_test['DateReported']).date - pd.DatetimeIndex(df_test['DateTimeOfAccident']).date\ndf_test['DaysReportDelay'] = (df_test['DaysReportDelay']  \/ np.timedelta64(1, 'D')).astype(int)\ndf_test['WeeksReportDelay'] = np.floor(df_test['DaysReportDelay'] \/ 7.).astype(int)\ndf_test['WeeksReportDelay'] = np.clip(df_test['WeeksReportDelay'], a_max=55, a_min=None)\n\n#Save the ClaimNumbers for submission file\nID = df_test['ClaimNumber']\n\n\n# The log1p function applies log(1+x) to all elements of the column\n#df_test[\"LogInitialIncurredCalimsCost\"] = np.log1p(df_test[\"InitialIncurredCalimsCost\"])\n\n# drop unneccessary columns\ndf_test.drop(['ClaimNumber','DateTimeOfAccident','DaysReportDelay','DateReported'],axis=1,inplace=True)\n\n","83676a88":"# Very basis text processing: claim description features\n\n# create new features for some \"cheap\" or \"expensive\" words \nfor name in text:\n    df_test['CD_' + name] = np.where( (df_test['ClaimDescription'].str.find(name) < 0), 0, 1)\n\n# some two or tree word features\ndf_test['CD_FOREIGN_BODY'] = np.where( (df_test['ClaimDescription'].str.find('FOREIGN BODY') < 0), 0, 1)\ndf_test['CD_BACK_STRAIN']  = np.where( (df_test['ClaimDescription'].str.find('BACK STRAIN') < 0), 0, 1)\ndf_test['CD_SOFT_TISSUE_'] = np.where( (df_test['ClaimDescription'].str.find('SOFT TISSUE INJURY') < 0), 0, 1)\ndf_test['CD_WORKPLACE_STRESS'] = np.where( (df_test['ClaimDescription'].str.find('WORKPLACE STRESS') < 0), 0, 1)\ndf_test['CD_LOWER_BACK_STRAIN'] = np.where( (df_test['ClaimDescription'].str.find('LOWER BACK STRAIN') < 0), 0, 1)\n\n# body side, lacerated\/laceration:\ndf_test['CD_LEFT_RIGHT'] = np.where( ((df_test['ClaimDescription'].str.find('LEFT') < 0) & (df_test['ClaimDescription'].str.find('RIGHT') < 0)), 0, 1)\ndf_test['CD_LACERAT_'] = np.where( (df_test['ClaimDescription'].str.find('LACERAT') < 0), 0, 1)","7e0b5380":"# combine, drop claim description and get dummies\ndf_all = pd.concat([df.assign(role=\"train\"), df_test.assign(role=\"test\")])\ndf_all.drop(['ClaimDescription'],axis=1,inplace=True)\ndf_all = pd.get_dummies(df_all)\n\n# seperate\ndf_test_dummies, df_dummies = df_all[df_all[\"role_test\"].eq(1)], df_all[df_all[\"role_train\"].eq(1)]\ndf_dummies.drop(['role_test','role_train'],axis=1,inplace=True)\ndf_test_dummies.drop(['role_test','role_train','loss'],axis=1,inplace=True)\ndf_dummies.shape","f78ac3bf":"seed = 123\n\n# split data in feature matrix X and label y for training and validation\nX = df_dummies.drop(['loss'], axis=1)\ny = df_dummies['loss']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25, random_state=seed)\nX_test = df_test_dummies","7b11dca5":"# libraries\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nimport time\nimport shap ","2e5147cb":"# CatBoostRegressor (Default values)\ntic = time.time()\nCGB = CatBoostRegressor(logging_level='Silent')    \nCGB.fit(X_train, y_train)\nprint(\"time (sec):\" + \"%6.0f\" % (time.time() - tic))\n\n# Validation MSE\nresult = mean_squared_error(y_val, CGB.predict(X_val))\n#result = mean_squared_error(np.expm1(y_val), np.expm1(CGB.predict(X_val)))\nprint(\"MSE:\" + \"%6.2f\" % result)","4b2d98cc":"# CatBoost: Plot feature importance\n(pd.Series(CGB.feature_importances_, index=X.columns).nlargest(20).plot(kind='barh'))  \nplt.show()","b1d9d1a0":"# LGBMRegressor: Hyperparameter tuning with RandomizedSearchCV\ntic = time.time()\nparam_grid ={'learning_rate': [0.02,0.025], 'n_estimators': [500], 'num_leaves': [30,40,50],'feature_fraction': [0.7]} \nLGB_random_search = RandomizedSearchCV(LGBMRegressor(),param_grid, scoring='neg_mean_squared_error', cv=4,  n_iter=5, random_state=seed)\nLGB_random_search.fit(X_train,y_train)\nprint(\"Best parameters:\",LGB_random_search.best_params_)\nLGB = LGBMRegressor(**LGB_random_search.best_params_)    \nLGB.fit(X_train, y_train)\nprint(\"time (sec):\" + \"%6.0f\" % (time.time() - tic))\n\n# Validation MSE\nresult = mean_squared_error(y_val, LGB.predict(X_val))\n#result = mean_squared_error(np.expm1(y_val), np.expm1(LGB.predict(X_val)))\nprint(\"MSE:\" + \"%6.2f\" % result)","b2ae40d3":"# lightGBM: Plot feature importance\n(pd.Series(LGB.feature_importances_, index=X.columns).nlargest(20).plot(kind='barh'))  \nplt.show()","5f6d76f7":"# XGBRegressor: Hyperparameter tuning with RandomizedSearchCV\ntic = time.time()\nparam_grid ={'learning_rate': [0.02,0.025], 'max_depth': [5,6,7],'n_estimators': [500],'colsample_bytree': [0.9], 'subsample': [0.7], 'tree_method': [\"hist\"] } \nXGB_random_search = RandomizedSearchCV(XGBRegressor(),param_grid, scoring='neg_mean_squared_error', cv=4,  n_iter=5, random_state=seed)\nXGB_random_search.fit(X_train,y_train)\nprint(\"Best parameters:\",XGB_random_search.best_params_)\nXGB = XGBRegressor(**XGB_random_search.best_params_)    \nXGB.fit(X_train, y_train)\nprint(\"time (sec):\" + \"%6.0f\" % (time.time() - tic))\n\n# Validation MSE\nresult = mean_squared_error(y_val, XGB.predict(X_val))\n#result = mean_squared_error(np.expm1(y_val), np.expm1(XGB.predict(X_val)))\nprint(\"MSE:\" + \"%6.2f\" % result)","9b4903db":"# XGBoost: Plot feature importance\n(pd.Series(XGB.feature_importances_, index=X.columns).nlargest(20).plot(kind='barh'))  \nplt.show()","1ac41e8f":"# Validation MSE (Blend of LightGBM and XGBoost)\npredictions_val = 0.5*(LGB.predict(X_val)+XGB.predict(X_val))\nresult = mean_squared_error(y_val, predictions_val) \nprint(\"MSE:\" + \"%6.2f\" % result)","e93a6ba9":"# Scoring: Make predictions on test data and write submission file: LGB + XGB\npredictions = 0.5*(LGB.predict(X_test)+XGB.predict(X_test))\ndf_test_pred = pd.DataFrame({'ClaimNumber':ID,'UltimateIncurredClaimCost':predictions})\ndf_test_pred.to_csv('subm-v3-blend-LX.csv',index=False)\ndf_test_pred.head()","079137ea":"import shap\nshap.initjs()\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(LGB)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(X_test)","f3467912":"# first 3 observations in validation set: ultimate costs (log-scale) \nprint(LGB.predict(X_test)[:4])","208b745e":"# explaining the ultimate costs (log-scale): 1. Dataset \nshap.force_plot(explainer.expected_value, shap_values[1:2], X_test[0:1])","ff3d9746":"# explaining the ultimate costs (log-scale): 3. Dataset \nshap.force_plot(explainer.expected_value, shap_values[2:3], X_test[2:3])","d30f4b5d":"# explaining the ultimate costs (log-scale): 4. Dataset \nshap.force_plot(explainer.expected_value, shap_values[3:4], X_test[3:4])","ce11a32c":"# Make Shap summary plot. Model: LGB\nshap.summary_plot(shap_values, X_test)","bdd6f7d1":"import fastai\nfastai.__version__","134e93fe":"from fastai.tabular.all import *\n\n# combine train and val data set from train_test_split to preserve the same samples\ndf_ai = pd.concat([X_train.assign(role=\"train\"), X_val.assign(role=\"val\")])\n\n# define dependent variable, log-transform to improve convergence\ndf_ai['y'] = np.log1p(np.concatenate((y_train, y_val)))\n\n# create lists of train_idx and valid_idx (\"splits\" for TabularPandas)\ncond = (df_ai.role == 'train')\ntrain_idx = np.where( cond)[0]\nvalid_idx = np.where(~cond)[0]\nsplits = (list(train_idx),list(valid_idx))\ndf_ai.drop(['role'],axis=1,inplace=True)","3d45c7ed":"#  change data types for \"no embedding\"-variables (in preparation for cont_cat_split)\nnum_list = [c for c in df_ai.columns if(df_ai[c].dtype != np.object)] \nemb_list = ['DaysWorkedPerWeek','MonthOfAccident','DayOfAccident','HourOfAccident'] \ncont_list = [c for c in num_list if(c not in emb_list)]\ndf_ai[cont_list] = df_ai[cont_list].astype(np.float64)","15e3834f":"# define continuous and categorical columns\ncont_nn,cat_nn = cont_cat_split(df_ai, max_card=9000, dep_var='y')\n# let's look at cardinality\ndf_ai[cat_nn].nunique()","b306f315":"# pre-process data \nprocs_nn = [Categorify, FillMissing, Normalize]\n# create TabularPandas object\nto_nn = TabularPandas(df_ai, procs_nn, cat_nn, cont_nn, splits=splits, y_names='y')\nlen(to_nn.train),len(to_nn.valid)","cc47f0fb":"# data loader (batch size) \ndls = to_nn.dataloaders(512)","30c05b56":"y=to_nn.train.y\ny.min(),y.max()","cc82347a":"# tabular_learner with cat-embeddings (default), two hidden layers and small drouout rates\nlearn = tabular_learner(dls, y_range=(5,14), layers=[50,25], n_out=1, loss_func=F.mse_loss)\n# show architecture\nlearn.model","7fd2960c":"# Suggest learning rate (lr)\nlearn.lr_find()","97bbd997":"# fit a few cycles with lr=0.1\nlearn.fit_one_cycle(5,0.05)","2a1cc1ac":"# Validation MSE\npreds,targs = learn.get_preds()\nmse(np.expm1(preds),np.expm1(targs))","4d654afd":"<a id=\"ch4\"><\/a>\n## 4. Explaining Predictions: Shap Values\n\nThis chapter is not really necessary for a prediction competition, but how to understand and explain your own predictions will likely be of interest to other actuaries.\n\nFor more information about Shap values the book \"Interpretable Machine Learning\" by Christoph Molnar is highly recommended:  https:\/\/christophm.github.io\/interpretable-ml-book\/ , Chapter 5.\n\nLet's explain the predictions of the LGB model for test data:","40c24df0":"This could lead to an additional \"lower limit\" modeling requirement and is not considered further in this notebook.","2760d0b4":"There are some spikes in the initial cost distribution. Lets' see if there is interesting information in it:","52cef04a":"<a id=\"ch3\"><\/a>\n## 3. Boosting Models, Validation, Scoring\n\nWe restrict ourselves here to gradient boosted regression tree models. They do not require scaling and are known to be well suited for contests with tabular data. When treating 'ClaimDescription' with NLP methods, this may change towards artificial neural networks.","572acd23":"### 1.5 Visualize numerical features","0e208ced":"<a id=\"ch5\"><\/a>\n## 5. Deep Learning with fastai\n\nfastai's \"tabular_learner\" is a high-level API to PyTorch (like Keras to TensorFlow) and thus a simple and powerful approach to Deep Learning for tabular data. The following part is based on chapter 9 of \"Deep Learning for Coders with fastai & PyTorch\" by Jeremy Howard & Sylvain Gugger, see the following notebook and video.\n* https:\/\/colab.research.google.com\/github\/fastai\/fastbook\/blob\/master\/09_tabular.ipynb\n* https:\/\/course.fast.ai\/videos\/?lesson=7\n\nNote: There are syntax changes from fastai version 1 to version 2, so codes from notebooks older than 2020 may not work properly (if you are looking for them). ","a80cf2dc":"The resulting MSE varies between quite good and bad. So fight against overfitting :-)","a66219a8":"The average claim cost (ultimate) is 11000. The most expensive claim costs 4 million. We will soon take care of the cost distributions, but first deal with missing values and basic feature engineering.","34b8f038":"### 1.2 Count and replace missing values","d850103e":"### 3.2 lightGBM: Hyperparametertuning","36e437ea":"The claim descriptions seem to be predictive and should be continued using modern natural language processing techniques (NLP).","24a95f3a":"### 1.3 Basic feature engineering","25d79153":"### 1.7 Visualize categorical features","7327c0e0":"### 3.5 Model blend and scoring","39d27cb1":"### 3.1 CatBoost (with default settings)","9d2d74e5":"<a id=\"ch2\"><\/a>\n## 2.  Data preperation for modeling","9ab5ede4":"January 16th, 2021","30bc6df5":"### 1.4 Skewness and claim costs (transform, analyze)","c555d88c":"## 1.  Training Data: Explorative Data Analysis (EDA)","4b4bd74f":"### 1.1 Read data and get a first impression","10a3dddd":"# Actuarial Loss Prediction\n\nStarter notebook for exploring, engineering, modeling, predicting and explaining Workers' Compensation claims. Submissions based on this notebook are currently ranked Top35% on public leaderboard. Feel free to copy and improve.\n\nVersion 3: Focusing on boosting (without log-transforms). Deep learning with fastai added.\n\n### Table of Contents\n1. [Training Data: Explorative Data Analysis (EDA)](#ch1)\n2. [Data Preperation for Modeling](#ch2)\n3. [Boosting Models, Validation and Scoring](#ch3)\n4. [Explaining Predictions: Shap Values](#ch4)\n5. [Appendix: Deep Learning with fastai](#ch5)\n","1664c3b3":"### 2.2 One-hot-Encoding (nominal features to dummies) ","4763dcce":"### 1.6 Correlations","90894b79":"### 1.9 Treatment of extreme outliers","a291c9db":"### 2.1 Feature engineering test data\n\nTo make the EDA less confusing, the feature engineering (Chapter 1.3 and 1.8) was done for training data only. Since test data needs the same features, we have to do this now.","7da20068":"### 1.10 Define target: UltimateIncurredClaimCost","0b96ec42":"'HoursWorkedPerWeek' is probably affected by wrong values (>168hours) and 'DependentsOther' might be influenced by outliers. \n\nAs expected claim costs are strongly right-skewed. Thus a first basic log-transform:","6577c27c":"#### Data Description \nThe dataset includes 90,000 realistic, synthetically generated worker compensation insurance policies, all of which have had an accident. For each record there is demographic and worker related information, as well as a text description of the accident.\n\n#### Evaluation metric\nSubmissions are scored on the mean squared error. ","0bacabf5":"### 3.3 XGBoost: Hyperparametertuning","7c4804d0":"### 2.3  Create modeling data sets","59cae345":"The average ultimate costs of frequent low initial costs (500,1000,1500,3500) are by a factor of 2 to 3 higher and thus remarkably higher than average (1.40). \n\nLet's see if there is a pattern in the scatter plot inital vs. ultimate costs:","d55c07d7":"### 1.8 Very basic text processing\n\nIs there predictive power in claim descriptions? Let's get a first impression."}}