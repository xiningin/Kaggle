{"cell_type":{"613e6245":"code","7fd01426":"code","1035e633":"code","2544fa6f":"code","bbed372c":"code","e6dde243":"code","4b0cece6":"code","717e4912":"code","39ad526f":"code","29c1d4db":"code","93895ccb":"code","9b237cc5":"code","d1f208f3":"code","7cd0a156":"code","cd07ec63":"code","5f00fd22":"code","3cbd0a8f":"code","36d0b5b8":"code","454dffc6":"code","2754dc28":"code","dee1cd92":"code","7c7f9a32":"code","eda214d3":"markdown","a2de43af":"markdown","8eb80943":"markdown","7c605c4e":"markdown","b9e63400":"markdown","5d1cdacb":"markdown","b73c324e":"markdown","7aa75c7a":"markdown"},"source":{"613e6245":"import os\nimport gc\nimport time\nimport glob\nimport numpy as np \nimport pandas as pd \nimport xgboost as xgb\nimport lightgbm as lgb\nimport soundfile as sf\n\nfrom tqdm.notebook import tqdm\nfrom joblib import Parallel, delayed\n\nfrom scipy.stats import rankdata\nfrom scipy.interpolate import interp1d\n\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, label_ranking_average_precision_score\n\nimport librosa\nimport librosa.display\nimport IPython.display as ipd\nimport matplotlib.pyplot as plt\n\nimport cuml as cm\nimport cupy as cp\n","7fd01426":"trainfiles = glob.glob( '..\/input\/rfcx-species-audio-detection\/train\/*.flac' )\ntestfiles = glob.glob( '..\/input\/rfcx-species-audio-detection\/test\/*.flac' )\n\nlen(trainfiles), len(testfiles), trainfiles[0]\n","1035e633":"traint = pd.read_csv( '..\/input\/rfcx-species-audio-detection\/train_tp.csv' )\ntraint['t_dif'] = traint['t_max'] - traint['t_min']\ntraint['f_dif'] = traint['f_max'] - traint['f_min']\n\ntrainf = pd.read_csv( '..\/input\/rfcx-species-audio-detection\/train_fp.csv' )\ntrainf['t_dif'] = trainf['t_max'] - trainf['t_min']\ntrainf['f_dif'] = trainf['f_max'] - trainf['f_min']\n\ntraint.shape, trainf.shape\n","2544fa6f":"traint.head()\n","bbed372c":"trainf.head()\n","e6dde243":"trainf.f_dif.unique()\n","4b0cece6":"traint.nunique()\n","717e4912":"trainf.nunique()\n","39ad526f":"traint.describe()\n","29c1d4db":"trainf.describe()\n","93895ccb":"def extract_fft(fn):\n    data, samplerate = sf.read(fn)\n    data = cp.array(data)\n\n    varfft = cp.abs( cp.fft.fft(data)[:(len(data)\/\/2)] )\n    \n    return cp.asnumpy( varfft.reshape( (1000,1440) ).mean(axis=1) )\n","9b237cc5":"FT = []\nfor fn in tqdm(traint.recording_id.values):\n    FT.append( extract_fft( '..\/input\/rfcx-species-audio-detection\/train\/'+fn+'.flac' ) )\nFT = np.stack(FT)\ngc.collect()\n\nFT.shape\n","d1f208f3":"FF = []\nfor fn in tqdm(trainf.recording_id.values):\n    FF.append( extract_fft( '..\/input\/rfcx-species-audio-detection\/train\/'+fn+'.flac' ) )\nFF = np.stack(FF)\ngc.collect()\n\nFF.shape\n","7cd0a156":"#Combine True Positives and False Positives\nTRAIN = np.vstack( (FT, FF) )\n\ndel FT, FF\ngc.collect()\nTRAIN.shape\n","cd07ec63":"TEST = []\nfor fn in tqdm(testfiles):\n    TEST.append( extract_fft(fn) )\nTEST = np.stack(TEST)\ngc.collect()\n\nTEST.shape\n","5f00fd22":"tt = traint[['recording_id','species_id']].copy()\ntf = trainf[['recording_id','species_id']].copy()\ntf['species_id'] = -1\n\nTRAIN_TAB = pd.concat( (tt, tf) )\n\nfor i in range(24):\n    TRAIN_TAB['s'+str(i)] = 0\n    TRAIN_TAB.loc[TRAIN_TAB.species_id==i,'s'+str(i)] = 1\n\nTRAIN_TAB.shape\n","3cbd0a8f":"TRAIN_TAB.head()\n","36d0b5b8":"std = StandardScaler()\nstd.fit( np.vstack((TRAIN,TEST)) )\n\nTRAIN = std.transform(TRAIN)\nTEST  = std.transform(TEST)\ngc.collect()\n","454dffc6":"sub = pd.DataFrame({'recording_id': [f.split('\/')[-1].split('.')[0] for f in testfiles] })\ngkf = GroupKFold(5)\n\nSCORE = []\ngroups = TRAIN_TAB['recording_id'].values\n\nfor tgt in range(0,24):\n    starttime = time.time()\n    target = TRAIN_TAB['s'+str(tgt)].values\n\n    ytrain = np.zeros(TRAIN.shape[0])\n    ytest = np.zeros(TEST.shape[0])\n    \n    for ind_train, ind_valid in gkf.split( TRAIN, target, groups ):\n        \n        # Define 4 models\n        model1 = xgb.XGBClassifier(n_estimators=1000,\n                                   max_depth=4,\n                                   learning_rate=0.09,\n                                   verbosity=0,\n                                   objective='binary:logistic',\n                                   subsample=0.95,\n                                   colsample_bytree=0.95,\n                                   random_state=2021,\n                                   tree_method='gpu_hist',\n                                   predictor='gpu_predictor',\n                                   n_jobs=2,\n                                   scale_pos_weight = np.sum(target==0) \/ np.sum(target==1),\n                                  )\n        model2 = cm.linear_model.LogisticRegression( C=1, max_iter=5000 )\n        \n        model3 = cm.svm.SVC(C=1.0, class_weight='balanced', probability=True, kernel='rbf', gamma='auto')\n        \n        model4 = cm.neighbors.KNeighborsClassifier(n_neighbors=55)\n        \n        # Train using GPUs\n        model1.fit( X=TRAIN[ind_train], y=target[ind_train], eval_set=[(TRAIN[ind_valid], target[ind_valid])], eval_metric='auc', early_stopping_rounds=60, verbose=False )\n        model2.fit( TRAIN[ind_train], target[ind_train] )\n        model3.fit( TRAIN[ind_train], target[ind_train] )\n        model4.fit( TRAIN[ind_train], target[ind_train] )\n        \n        # Predict valid and test sets\n        yvalid1 = model1.predict_proba(TRAIN[ind_valid])[:,1]\n        yvalid2 = model2.predict_proba(TRAIN[ind_valid])[:,1]\n        yvalid3 = model3.predict_proba(TRAIN[ind_valid])[:,1]\n        yvalid4 = model4.predict_proba(TRAIN[ind_valid])[:,1]\n        \n        ytest1 = model1.predict_proba(TEST)[:,1]\n        ytest2 = model2.predict_proba(TEST)[:,1]\n        ytest3 = model3.predict_proba(TEST)[:,1]\n        ytest4 = model4.predict_proba(TEST)[:,1]\n        \n        #Rank predictions\n        SZ = len(ind_valid) + len(ytest1)\n        yvalid1 = rankdata( np.concatenate((yvalid1,ytest1)) )[:len(ind_valid)] \/ SZ\n        yvalid2 = rankdata( np.concatenate((yvalid2,ytest2)) )[:len(ind_valid)] \/ SZ\n        yvalid3 = rankdata( np.concatenate((yvalid3,ytest3)) )[:len(ind_valid)] \/ SZ\n        yvalid4 = rankdata( np.concatenate((yvalid4,ytest4)) )[:len(ind_valid)] \/ SZ\n        \n        ytest1 = rankdata( np.concatenate((yvalid1,ytest1)) )[len(ind_valid):] \/ SZ\n        ytest2 = rankdata( np.concatenate((yvalid2,ytest2)) )[len(ind_valid):] \/ SZ\n        ytest3 = rankdata( np.concatenate((yvalid3,ytest3)) )[len(ind_valid):] \/ SZ\n        ytest4 = rankdata( np.concatenate((yvalid4,ytest4)) )[len(ind_valid):] \/ SZ\n        \n        #Weighted average models\n        ytrain[ind_valid] = ((0.85*yvalid1) + (0.15*yvalid2) + (0.00*yvalid3) + (0.00*yvalid4)) \/ 4.\n        ytest += ((0.85*ytest1) + (0.15*ytest2) + (0.00*ytest3) + (0.00*ytest4)) \/ (4.*5)\n\n    score = roc_auc_score(target, ytrain)\n    print( 'Target AUC', tgt, score, time.time()-starttime )\n    SCORE.append(score)\n    \n    TRAIN_TAB['y'+str(tgt)] = ytrain\n    sub['s'+str(tgt)] = ytest\n\nprint('Overall Score:', np.mean(SCORE) )\n","2754dc28":"sub.head()\n","dee1cd92":"sub.to_csv('submission.csv', index=False)\n","7c7f9a32":"!ls\n","eda214d3":"<div class=\"alert alert-success\">  \n<\/div>","a2de43af":"<div class=\"alert alert-success\">  \n<\/div>","8eb80943":"<div class=\"alert alert-success\">  \n<\/div>","7c605c4e":"<div class=\"alert alert-success\">  \n<\/div>","b9e63400":"**The codes of this address have been used:**\n\nhttps:\/\/www.kaggle.com\/titericz\/0-525-tabular-xgboost-gpu-fft-gpu-cuml-fast\n\nYou can also find the full description there. Thanks to Mr. [Giba](https:\/\/www.kaggle.com\/titericz) & https:\/\/rapids.ai","5d1cdacb":"<div class=\"alert alert-success\">  \n<\/div>","b73c324e":"<div class=\"alert alert-success\">  \n<\/div>","7aa75c7a":"<div>\n    <h1 align=\"center\">\"Rainforest Connection Species Audio Detection\"<\/h1><\/h1>\n    <h4 align=\"center\">By: Somayyeh Gholami & Mehran Kazeminia<\/h4>\n<\/div>"}}