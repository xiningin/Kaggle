{"cell_type":{"a97ea27b":"code","ce9730b2":"code","696fcbf3":"code","ca47f092":"code","dd4b6e53":"code","24969f30":"code","07e50fa6":"code","4cb305ab":"code","4608f8f2":"code","243f8d05":"code","220414bc":"code","6dfb4b87":"code","c629c900":"code","e0965880":"code","742326da":"code","3ee32f5f":"code","96473264":"code","72cbb785":"code","8fd118b2":"code","c1a867ea":"code","a282e1ba":"code","159670c6":"code","95d098d2":"code","e2d62aa3":"code","79c47584":"code","439aaccf":"code","48597739":"code","2e0771fb":"code","d47643b1":"code","b312206f":"code","176125e7":"code","5020874c":"code","17f595d7":"code","937ae4ad":"code","47d614e9":"code","9b31952f":"code","219bcb13":"code","3d6cb9e3":"code","6119c338":"code","14a317da":"code","c842a500":"code","f431a678":"code","69cb59c3":"code","4d5474fe":"markdown","13af0ab1":"markdown","8eca4d0c":"markdown","06f96285":"markdown"},"source":{"a97ea27b":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.express as px\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n\ncnf = '#393e46' # confirmed - grey\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n#dataset name coronavirus-2019ncov\ndf = pd.read_csv(\"..\/input\/coronavirus-2019ncov\/covid-19-all.csv\")\ndf[\"Date\"] = pd.to_datetime(df[\"Date\"])\n\ndf_temp=pd.read_csv('\/kaggle\/input\/covid19-global-weather-data\/temperature_dataframe.csv')\ndf_temp[\"date\"] = pd.to_datetime(df_temp[\"date\"])\ndf_pop=pd.read_csv('\/kaggle\/input\/population-by-country-2020\/population_by_country_2020.csv')","ce9730b2":"df.head().T","696fcbf3":"print(df.shape[0])\n\n# Sutun Adlari\nprint(df.columns.tolist())\n\n# Veri Tipleri\nprint(df.dtypes)\n","ca47f092":"print(df.describe())","dd4b6e53":"import matplotlib.pyplot as plt\n%matplotlib inline\n# Matplotlib ile basit bir dagilim grafigi\nax = plt.axes()\n\nax.scatter(df.Deaths, df.Recovered)\n\n# Eksenleri isimlendirme\nax.set(xlabel='\u00d6len Ki\u015fi',\n       ylabel='\u0130yile\u015fen Hasta',\n       title='\u00d6len Ki\u015fi vs \u0130yile\u015fen Hasta');","24969f30":"\nplt.axes().set(xlabel='Hasta Say\u0131s\u0131',\n       ylabel='Confirmed',\n       title='D\u00fcnya Genelinde Hastalar');\n# Histogram\n# bins = number of bar in figure\ndf.Confirmed.plot(kind = 'hist',bins = 20,figsize = (10,5))\n\nplt.show()","07e50fa6":"import plotly.graph_objects as go\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.offline as ply\nply.init_notebook_mode(connected=True)\nimport plotly.express as px\ndf.sort_values(by=[\"Confirmed\"], ascending=False, inplace=True)\nfig = px.pie(\n    df.head(50),\n    values = \"Confirmed\",\n    names = \"Country\/Region\",\n    title = \"En Y\u00fcksek Vaka Say\u0131s\u0131na Sahip 5 \u00dclke\"\n)\nfig.update_traces(textposition=\"inside\", textinfo=\"percent+label\")\nfig.show()","4cb305ab":"df_pop.rename(columns={'Country (or dependency)': 'country',\n                             'Population (2020)' : 'population',\n                             'Density (P\/Km\u00b2)' : 'density',\n                             'Fert. Rate' : 'fertility',\n                             'Med. Age' : \"age\",\n                             'Urban Pop %' : 'urban_percentage'}, inplace=True)\ndf.rename(columns={'Country\/Region': 'country'}, inplace=True)\ndf_temp.rename(columns={'date': 'Date'}, inplace=True)\ndf_temp['country'] = df_temp['country'].replace('USA', 'US')\ndf_pop['country'] = df_pop['country'].replace('United States', 'US')\ndf['country'] = df['country'].replace('Mainland China', 'China')\ndf_pop = df_pop[[\"country\", \"population\", \"density\", \"fertility\", \"age\", \"urban_percentage\"]]\ndf = df.merge(df_pop, on=['country'], how='left')\ndf_temp.drop_duplicates(subset =[\"Date\",'country'], \n                     keep = 'first', inplace = True)\ndf = df.merge(df_temp, on=['Date','country'], how='left')","4608f8f2":"plt.axes().set(xlabel='Hasta Say\u0131s\u0131',\n       ylabel='population',\n       title='D\u00fcnya Genelinde Hastalar');\n# Histogram\n# bins = number of bar in figure\ndf_pop.population.plot(kind = 'hist',bins = 50,figsize = (9,4))\n\nplt.show()\n","243f8d05":"tarih=df['Date'].max()\nguncel=df[df['Date']==tarih]\nolum=guncel['Deaths'].sum()\niyilesme=guncel['Recovered'].sum()\nvaka=guncel['Confirmed'].sum()\nturkiye=guncel[guncel['country']=='Turkey']\nturkiye_vaka=turkiye['Confirmed'].sum()\nturkiye_olum=turkiye['Deaths'].sum()\nturkiyeOlum_orani=(turkiye_olum\/turkiye_vaka)*100\nturkiye_iyilesme=turkiye['Recovered'].sum()\nprint ('Bilgilerin Son G\u00fcncellenme Tarihi: {}'.format(tarih))\nprint ('T\u00fcrkiye Vaka: {:,.0f}'.format(turkiye_vaka))\nprint ('T\u00fcrkiye \u00d6l\u00fcm: {:,.0f}'.format(turkiye_olum))\nprint ('T\u00fcrkiye \u0130yile\u015fme: {:,.0f}'.format(turkiye_iyilesme))\nprint ('T\u00fcrkiye \u00d6l\u00fcm Oran\u0131: {:,.1f}%'.format(turkiyeOlum_orani))\nprint ('Toplam \u00d6l\u00fcm: {:,.0f}'.format(olum))\nprint ('Toplam \u0130yile\u015fme: {:,.0f}'.format(iyilesme))\nprint ('Toplam Vaka: {:,.0f}'.format(vaka))","220414bc":"df['Active']=df['Confirmed']-df['Deaths']-df['Recovered']\ntemp = df.groupby('Date')['Recovered', 'Deaths', 'Active'].sum().reset_index()\ntemp = temp.melt(id_vars=\"Date\", value_vars=['Recovered', 'Deaths', 'Active'],\n                 var_name='Case', value_name='Count')\n\n\nfig = px.area(temp, x=\"Date\", y=\"Count\", color='Case',\n             title='Yay\u0131lma H\u0131z\u0131', color_discrete_sequence = ['#21bf73', '#ff2e63', '#fe9801'])\nfig.show()","6dfb4b87":"df1 = pd.read_csv('..\/input\/covid19-coronavirus\/2019_nCoV_data.csv')\ndf1.head()","c629c900":"df1 = df1.astype({\"Confirmed\": int, \"Deaths\": int, \"Recovered\" : int})\ndf1 = df1.filter([\"Date\", \"Province\/State\", \"Country\", \"Last Update\", \"Confirmed\", \"Deaths\", \"Recovered\"])\ndf1.head()","e0965880":"#Convert the date (remove time stamp)\n\ndf1['Date1'] = pd.to_datetime(df1['Date'])\ndf1['Date'] = df1['Date1'].dt.date\ndf1['Last Update1'] = pd.to_datetime(df1['Last Update'])\ndf1['Last Update'] = df1['Last Update1'].dt.date\ndf1 = df1.filter([\"Date\", \"Province\/State\", \"Country\", \"Last Update\", \"Confirmed\", \"Deaths\", \"Recovered\"])\ndf1.head()","742326da":"#Combine country and province to location and sum values pertaining to it\n\ndf1['Location'] = df1['Country'] + ', ' + df1['Province\/State'].fillna('N\/A')\n\ndaily = pd.DataFrame(columns=df1.columns)\n\nfor item in df1['Location'].unique():\n    a = df1[df1['Location']==item].set_index('Date')\n    a = a.rename_axis('Date').reset_index()\n    daily = daily.append(a, sort=False, ignore_index=True)\n\ndf1_daily = daily.sort_values(['Date','Country','Province\/State'])\ndf1_daily = df1_daily.reset_index()\ndf1_daily = df1_daily.filter([\"Date\", \"Province\/State\", \"Country\", \"Last Update\", \"Confirmed\", \"Deaths\", \"Recovered\", \"Location\"])\ndf1_daily.head()","3ee32f5f":"\ndf1_date = df1_daily.filter([\"Date\",  \"Confirmed\", \"Deaths\", \"Recovered\"])\ndf1_date = df1_date.groupby(df1_date[\"Date\"]).sum()\ndf1_date.head()","96473264":"plt.figure(figsize=(11,6))\nplt.plot(df1_date, marker='o')\nplt.title('Tarihe G\u00f6re Toplam Coronavir\u00fcs Vaka Say\u0131s\u0131')\nplt.legend(df1_date.columns)\nplt.xticks(rotation=75)\nplt.show()","72cbb785":"#For graph - Confirmed cases by country:\n\nfig = px.scatter(df1_daily, x='Date', y='Confirmed', hover_data=['Province\/State', 'Deaths', 'Recovered'], color='Country')\nannotations = []\nannotations.append(dict(xref='paper', yref='paper', x=0.0, y=1.05, xanchor='left', yanchor='bottom',\n                              text='\u00dclkeye G\u00f6re Vakalar Say\u0131s\u0131',\n                              font=dict(family='Calibri', size=20, color='rgb(37,37,37)'),\n                              showarrow=False))\nfig.update_layout(annotations = annotations)\nfig.show()","8fd118b2":"df1_date = df1_date.reset_index()\ndf1_date","c1a867ea":"#setting index as date\ndf1_date['Date'] = pd.to_datetime(df1_date.Date,format='%Y-%m-%d')\ndf1_date.index = df1_date['Date']\n\n#plot\nplt.figure(figsize=(16,8))\nplt.plot(df1_date['Confirmed'], label='Vakalar')","a282e1ba":"#setting index as date values\ndf1_date['Date'] = pd.to_datetime(df1_date.Date,format='%Y-%m-%d')\ndf1_date.index = df1_date['Date']\n\n#sorting\ndata = df1_date.sort_index(ascending=True, axis=0)\n\n#creating a separate dataset\nnew_data = pd.DataFrame(index=range(0,len(df1_date)),columns=['Date', 'Confirmed'])\n\nfor i in range(0,len(data)):\n    new_data['Date'][i] = data['Date'][i]\n    new_data['Confirmed'][i] = data['Confirmed'][i]\nnew_data","159670c6":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(new_data['Date'], new_data['Confirmed'], random_state = 0)\nX_train = pd.DataFrame(X_train)\nX_test = pd.DataFrame(X_test)","95d098d2":"#create features\nfrom fastai.tabular import add_datepart\nadd_datepart(X_train, 'Date')\nX_train.drop('Elapsed', axis=1, inplace=True)  #elapsed will be the time stamp\nX_train = X_train.filter([ \"Year\", \"Month\", \"Day\"])\nX_train\n\nadd_datepart(X_test, 'Date')\nX_test.drop('Elapsed', axis=1, inplace=True)  #elapsed will be the time stamp\nX_test = X_test.filter([ \"Year\", \"Month\", \"Day\"])\nX_test","e2d62aa3":"#implement linear regression\nfrom sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train,y_train)","79c47584":"#make predictions and find the rmse\npreds = model.predict(X_test)\nrms=np.sqrt(np.mean(np.power((np.array(y_test)-np.array(preds)),2)))\nrms","439aaccf":"preds","48597739":"new_data","2e0771fb":"# object (string) sutunlarini secme\nmask = df1.dtypes == np.object\ncategorical_cols = df1.columns[mask]","d47643b1":"# Kac tane ekstra sutun olusturulacagini belirleme\nnum_ohc_cols = (df1[categorical_cols]\n                .apply(lambda x: x.nunique())\n                .sort_values(ascending=False))\n\n\n# Yalnizca bir deger varsa kodlamaya gerek yoktur\nsmall_num_ohc_cols = num_ohc_cols.loc[num_ohc_cols>1]\n\n# one-hot sutun satisi, kategori sayisindan bir azdir. \nsmall_num_ohc_cols -= 1\n\n# Bu, orjinal sutunlarin cikarildigi varsayilan 215 sutundur.\n\nsmall_num_ohc_cols.sum()","b312206f":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\n# Verilerin kopyasi\ndata_ohc = df1.copy()\n\n# Kodlayicilar\nle = LabelEncoder()\nohc = OneHotEncoder()\n\nfor col in num_ohc_cols.index:\n    \n    # orjinal sutunu dataframeden kaldirma\n    data_ohc = data_ohc.drop(col, axis=1)\n\n    # one-hot kod verileri-- bir aralikli array dondurur\n    new_dat = ohc.fit_transform(dat.reshape(-1,1))\n\n    # Benzersiz sutun adlari olusturma\n    n_cols = new_dat.shape[1]\n    col_names = ['_'.join([col, str(x)]) for x in range(n_cols)]\n\n    # Yeni dataframe olusturma\n    new_df = pd.DataFrame(new_dat.toarray(), \n                          index=data_ohc.index, \n                          columns=col_names)\n\n    # Yeni verileri dataframe'e ekleme\n    data_ohc = pd.concat([data_ohc, new_df], axis=1)","176125e7":"# Sutun farki yukarida hesaplandigi gibidir\ndata_ohc.shape[1] - df1.shape[1]","5020874c":"print(df1.shape[1])\n\n# dataframe'den string sutunlarin kaldirilmasi\ndata = df1.drop(num_ohc_cols.index, axis=1)\n\nprint(data.shape[1])","17f595d7":"from sklearn.model_selection import train_test_split\n\ny_col = 'Confirmed'\n\n# one-hot kodlanmamis verileri bolme\nfeature_cols = [x for x in data.columns if x != y_col]\nX_data = data[feature_cols]\ny_data = data[y_col]\n\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, \n                                                    test_size=0.3, random_state=42)\n# one-hot kodlanmis verileri bolme\nfeature_cols = [x for x in data_ohc.columns if x != y_col]\nX_data_ohc = data_ohc[feature_cols]\ny_data_ohc = data_ohc[y_col]\n\nX_train_ohc, X_test_ohc, y_train_ohc, y_test_ohc = train_test_split(X_data_ohc, y_data_ohc, \n                                                    test_size=0.3, random_state=42)","937ae4ad":"# Kopyalama uyarilariyla ayari sessize alma\npd.options.mode.chained_assignment = None","47d614e9":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nLR = LinearRegression()\n\n# Hata degerleri icin depolama\nerror_df = list()\n\n# one-hot kodlanmamis veriler\nLR = LR.fit(X_train, y_train)\ny_train_pred = LR.predict(X_train)\ny_test_pred = LR.predict(X_test)\n\nerror_df.append(pd.Series({'train': mean_squared_error(y_train, y_train_pred),\n                           'test' : mean_squared_error(y_test,  y_test_pred)},\n                           name='no enc'))\n\n# one-hot kodlanmis veriler\nLR = LR.fit(X_train_ohc, y_train_ohc)\ny_train_ohc_pred = LR.predict(X_train_ohc)\ny_test_ohc_pred = LR.predict(X_test_ohc)\n\nerror_df.append(pd.Series({'train': mean_squared_error(y_train_ohc, y_train_ohc_pred),\n                           'test' : mean_squared_error(y_test_ohc,  y_test_ohc_pred)},\n                          name='one-hot enc'))\n\n# Sonuclari bir araya getirin\nerror_df = pd.concat(error_df, axis=1)\nerror_df","9b31952f":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n\n\nscalers = {'standard': StandardScaler(),\n           'minmax': MinMaxScaler(),\n           'maxabs': MaxAbsScaler()}\n\ntraining_test_sets = {\n    'not_encoded': (X_train, y_train, X_test, y_test),\n    'one_hot_encoded': (X_train_ohc, y_train_ohc, X_test_ohc, y_test_ohc)}\n\n\n# Onceden olceklendirdigimiz bir seyi olceklendirmemek icin \n# float sutunlarin listesini ve float verilerini alin \n# Orijinal verileri her seferinde \u00f6lceklememiz gerekiyor\nmask = X_train.dtypes == np.float\nfloat_columns = X_train.columns[mask]\n\n# initialize model\nLR = LinearRegression()\n\n# tum olas\u0131 kombinasyonlari tekrarlayin ve hatalari alin\nerrors = {}\nfor encoding_label, (_X_train, _y_train, _X_test, _y_test) in training_test_sets.items():\n    for scaler_label, scaler in scalers.items():\n        trainingset = _X_train.copy()  # kopyalayin cunku bunu bir kereden fazla olceklemek istemiyoruz.\n        testset = _X_test.copy()\n       # trainingset[float_columns] = scaler.fit_transform(trainingset[float_columns])\n       # testset[float_columns] = scaler.transform(testset[float_columns])\n        LR.fit(trainingset, _y_train)\n        predictions = LR.predict(testset)\n        key = encoding_label + ' - ' + scaler_label + 'scaling'\n        errors[key] = mean_squared_error(_y_test, predictions)\n\nerrors = pd.Series(errors)\nprint(errors.to_string())\nprint('-' * 80)\nfor key, error_val in errors.items():\n    print(key, error_val)","219bcb13":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\nsns.set_context('talk')\nsns.set_style('ticks')\nsns.set_palette('dark')\n\nax = plt.axes()\n#  y_test, y_test_pred kullanilacak\nax.scatter(y_test, y_test_pred, alpha=.5)\n\nax.set(xlabel='Temel Do\u011fruluk Verileri', \n       ylabel='Tahminler',\n       title='Linear Regression \u0130le Model Tahmini');","3d6cb9e3":"df.iloc[:, :-1].min().value_counts()\ndf.iloc[:, :-1].max().value_counts()","6119c338":"from sklearn.preprocessing import LabelEncoder\n\nle = LabelEncoder()\ndf['country'] = le.fit_transform(df.country)\ndf['country'].sample(5)\n\n","14a317da":"from sklearn.preprocessing import LabelEncoder\n\ndf['province'] = pd.to_numeric(df['province'], errors='coerce')\nle2= LabelEncoder()\ndf['province'] = le2.fit_transform(df.province)\ndf['province'].sample(5)","c842a500":"# Korelasyon de\u011ferlerini hesaplama\nfeature_cols = df.columns[:-1]\ncorr_values = df[feature_cols].corr()\n\n# K\u00f6\u015fegen alt\u0131ndaki t\u00fcm verileri bo\u015faltarak basitle\u015ftirin\ntril_index = np.tril_indices_from(corr_values)\n\n# Kullan\u0131lmayan de\u011ferleri NaN yap\u0131n\nfor coord in zip(*tril_index):\n    corr_values.iloc[coord[0], coord[1]] = np.NaN\n    \n# Verileri istifleyin ve dataframe'e d\u00f6n\u00fc\u015ft\u00fcr\u00fcn\ncorr_values = (corr_values.stack().to_frame().reset_index().rename(columns={'level_0':'feature1','level_1':'feature2',0:'correlation'}))\n\n# S\u0131ralama i\u00e7in mutlak de\u011ferleri al\u0131n\ncorr_values['abs_correlation'] = corr_values.correlation.abs()","f431a678":"sns.set_context('talk')\nsns.set_style('white')\nsns.set_palette('dark')\n\nax = corr_values.abs_correlation.hist(bins=50)\n\nax.set(xlabel='Absolute Correlation', ylabel='Frequency');","69cb59c3":"# En y\u00fcksek korelasyon de\u011ferleri\ncorr_values.sort_values('correlation', ascending=False).query('abs_correlation>0.8')","4d5474fe":"Korelasyon de\u011ferleri hesaplama","13af0ab1":"LINEAR REGRESSION","8eca4d0c":"Yeni Data Set \u0130ncelemesi\n","06f96285":"Egitim Test Bolumleme, Dogrusal Regresyon,StandardScaler, MinMaxScaler, MaxAbsScaler"}}