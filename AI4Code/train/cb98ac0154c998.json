{"cell_type":{"bc33f5fb":"code","6930abee":"code","a7872c4e":"code","4839f525":"code","f59c1267":"code","705fd1e5":"code","adec0c37":"code","be1f7ab6":"code","c3ae27aa":"code","0b72e412":"code","d099ffe9":"code","ca7c45a4":"code","8772b040":"code","3dac7ee9":"code","8d29d035":"code","efbb25c6":"code","0f7212b7":"code","07c382ee":"code","406e4643":"code","68d50f94":"code","ca1e2ae1":"code","4fcd6efd":"code","40eac64a":"code","1d612a10":"code","cc484a74":"code","026b3827":"code","066a7a19":"code","08544b4c":"code","8c36ef72":"code","95e604dd":"code","39aaf8e4":"code","232d0f5d":"code","ec8e4ff5":"code","d4a7da8a":"code","e6b1304a":"code","d9aed483":"code","aab0641e":"code","954bb57b":"code","0f3a90e2":"code","9c05cad7":"code","0328895f":"code","1605a2a5":"code","54f54f75":"code","761970c9":"code","dd71f17a":"code","442bfdff":"code","8aad0363":"code","db5b8ae1":"code","12aa7ff7":"code","b01cdc72":"code","0b82d83f":"code","218070e0":"code","b145837a":"code","f8f4f94c":"code","22af4b89":"code","e0d041ed":"code","16f98536":"code","b0dd9338":"code","7a3f9a91":"code","16888332":"code","2f645fc2":"code","bc4e5ee0":"code","21852d53":"code","56329ab0":"code","f8cdd605":"code","b73ee694":"code","ed429084":"code","cf7c2082":"code","2e17e8c2":"code","2dd03910":"code","9e8286d3":"code","f9e71963":"code","341377af":"markdown","93044722":"markdown","b5979fe0":"markdown","d6435ae7":"markdown","ceb7c36f":"markdown","da44a243":"markdown","2ebf6d8d":"markdown","8ef85c64":"markdown","236ce59c":"markdown","8ad9df7f":"markdown","92627fb0":"markdown","9103ee5a":"markdown","ee9a3f59":"markdown","e3270b1b":"markdown","a3326af4":"markdown","52bfa973":"markdown","904ae040":"markdown","0e704400":"markdown","2e9e7310":"markdown","d9ef5c1c":"markdown","84a4b141":"markdown","05079037":"markdown","87f0e2a9":"markdown","333ccc0c":"markdown","8e436b7b":"markdown","cfa74c52":"markdown","1abc0310":"markdown","ca41cd5a":"markdown","a55ac3eb":"markdown","376cc136":"markdown","05f58240":"markdown","57333df6":"markdown"},"source":{"bc33f5fb":"import numpy as np\nimport pandas as pd\nfrom scipy import stats\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.figure_factory as ff","6930abee":"plt.style.use('ggplot')\n\nSEED = 42\nnp.random.seed(SEED)\n\nTARGET = \"SalePrice\"\nTRAIN_SET = \"..\/input\/house-prices-advanced-regression-techniques\/train.csv\"\nTEST_SET = '..\/input\/house-prices-advanced-regression-techniques\/test.csv'\n\nTIME_VARIATIONS = ['year', 'yr']","a7872c4e":"df_train = pd.read_csv(TRAIN_SET)\ndf_test = pd.read_csv(TEST_SET)\ndf_train.head()","4839f525":"df_train.info()","f59c1267":"numerical_variables = [var for var in df_train.columns if df_train[var].dtypes != 'O']\nnumerical_variables.pop(numerical_variables.index(TARGET))\nnumerical_variables_num = len(numerical_variables)\nprint('Number of numerical variables: ', numerical_variables_num)\nfor var in numerical_variables:\n    print(var, end=' - ')","705fd1e5":"time_vars = [var for var in numerical_variables for var2 in TIME_VARIATIONS if var2 in var.lower()]\nprint('Number of time variables: ', len(time_vars))\nfor var in time_vars:\n    print(var, end=' - ')","adec0c37":"discrete_vars = [var for var in numerical_variables if len(df_train[var].unique()) < 20 and var not in time_vars]\ndiscrete_vars_num = len(discrete_vars)\nprint('Number of discrete variables: ', discrete_vars_num)\nfor var in discrete_vars:\n    print(var, end=' - ')","be1f7ab6":"# removing 'PoolArea' since its a continuous variable even if it has few values\ndiscrete_vars.pop(discrete_vars.index(\"PoolArea\"));\ndiscrete_vars_num -= 1","c3ae27aa":"continuous_vars = [var for var in numerical_variables if var not in time_vars+discrete_vars]\nprint('Number of continuous variables: ', len(continuous_vars))\nfor var in continuous_vars:\n    print(var, end=' - ')","0b72e412":"categorical_variables = [var for var in df_train.columns if df_train[var].dtypes == 'O']\ncategorical_variables_num = len(categorical_variables)\nprint('Number of categorical variables: ', categorical_variables_num)\nfor var in categorical_variables:\n    print(var, end=' - ')","d099ffe9":"print(\"Number of complete duplicates: \", df_train.duplicated().sum())","ca7c45a4":"print(f\"Number of nulls in features: {df_train.isnull().sum().sum()}\")","8772b040":"null_variables_in_numerics = [var for var in numerical_variables if df_train[var].isnull().sum() > 0]\nprint(f\"Number of nulls in numerical features: {df_train[null_variables_in_numerics].isnull().sum().sum()}\")\nnum_nulls = df_train[numerical_variables].isnull().mean()\nnum_nulls.sort_values(ascending=False).head(numerical_variables_num)","3dac7ee9":"null_variables_in_categoric = [var for var in categorical_variables if df_train[var].isnull().sum() > 0]\nprint(f\"Number of nulls in categorical features: {df_train[null_variables_in_categoric].isnull().sum().sum()}\")\nnum_nulls = df_train[categorical_variables].isnull().mean()\nnum_nulls.sort_values(ascending=False).head(categorical_variables_num)","8d29d035":"df_train[TARGET].describe()","efbb25c6":"plt.figure(figsize=(10, 5))\nsns.histplot(df_train[TARGET], kde=True)\nplt.title(\"Sale Price Distribution\");","0f7212b7":"plt.figure(figsize=(10, 5))\nstats.probplot(df_train[TARGET], plot=plt);","07c382ee":"sale_price_log_transformed = np.log(df_train[TARGET])\nplt.figure(figsize=(15, 5))\n\nplt.subplot(1 , 2, 1)\nsns.histplot(sale_price_log_transformed, kde=True)\nplt.title(\"Sale Price Distribution in Log\");\n\nplt.subplot(1 , 2, 2)\nstats.probplot(sale_price_log_transformed, plot=plt);","406e4643":"# df_train[TARGET] = np.log(df_train[TARGET])","68d50f94":"for var in continuous_vars:\n    sns.scatterplot(data=df_train, x=var, y=TARGET)\n    plt.show();","ca1e2ae1":"m, b = np.polyfit(df_train['GrLivArea'], df_train[TARGET], 1)\n\nplt.figure(figsize=(15, 5));\nsns.scatterplot(data=df_train, x='GrLivArea', y=TARGET)\nsns.lineplot(x=df_train['GrLivArea'], y=m*df_train['GrLivArea']+b, color='blue')\nplt.annotate(\"outlier\", xy=(4800, 200000), xytext=(5000, 250000), arrowprops={'facecolor':'grey', 'width': 3}, backgroundcolor = 'white')\nplt.annotate(\"outlier\", xy=(5700, 200000), xytext=(5850, 250000), arrowprops={'facecolor':'grey', 'width': 3}, backgroundcolor = 'white');","4fcd6efd":"sns.pairplot(data=df_train, y_vars=TARGET, x_vars=time_vars);\nplt.figure(figsize=(30,20));","40eac64a":"plt.figure(figsize=(15,5));\ndf_train.groupby('YrSold')[TARGET].median().plot();\nplt.ylabel(\"Price\");","1d612a10":"# fig, axes = plt.subplots(1, discrete_vars_num, figsize=(65, 5), sharey=False)\nfor i, var in enumerate(discrete_vars):\n    sns.boxplot(data=df_train, x=var, y=TARGET)\n    plt.show();","cc484a74":"# fig, axes = plt.subplots(1, categorical_variables_num, figsize=(350, 5), sharey=False)\nfor i, var in enumerate(categorical_variables):\n    sns.boxplot(data=df_train, x=var, y=TARGET)\n    plt.show();","026b3827":"# for better visualisation\nneed_more_visualisation = ['Neighborhood', 'RoofMatl', 'Exterior1st', 'Exterior2nd']\nfor var in need_more_visualisation:\n    plt.figure(figsize=(35, 5))\n    sns.boxplot(data=df_train, x=var, y=TARGET);","066a7a19":"corr_matrix = df_train.corr()\nplt.figure(figsize=(20, 15))\nsns.heatmap(corr_matrix, annot=False, fmt='.1f', cmap='PuBu');","08544b4c":"#saleprice correlation matrix\nk = 10 \ncols = corr_matrix.nlargest(k, TARGET)[TARGET].index\ncm = np.corrcoef(df_train[cols].values.T)\nplt.figure(figsize=(10, 10))\nsns.heatmap(cm, yticklabels=cols.values, xticklabels=cols.values, annot=True, fmt='.2f', cmap='PuBu');","8c36ef72":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import OrdinalEncoder, MinMaxScaler, StandardScaler\n\nfrom scipy.stats import norm, skew\nfrom scipy.special import boxcox1p","95e604dd":"SEED = 42\nnp.random.seed(SEED)","39aaf8e4":"df_train.set_index('Id', inplace=True)\ndf_test.set_index('Id', inplace=True)","232d0f5d":"numerical_variables = [var for var in df_train.columns if df_train[var].dtypes != 'O']\n# removing target \nnumerical_variables.pop(numerical_variables.index(TARGET))\nnumerical_variables_num = len(numerical_variables)\n\ntime_vars = [var for var in numerical_variables for var2 in TIME_VARIATIONS if var2 in var.lower()]\n\ndiscrete_vars = [var for var in numerical_variables if len(df_train[var].unique()) < 20 and var not in time_vars]\ndiscrete_vars_num = len(discrete_vars)\ndiscrete_vars.pop(discrete_vars.index(\"PoolArea\"));\ndiscrete_vars_num -= 1\n\ncontinuous_vars = [var for var in numerical_variables if var not in time_vars+discrete_vars]\ncontinuous_vars_num = len(continuous_vars)\n\ncategorical_variables = [var for var in df_train.columns if df_train[var].dtypes == 'O']\ncategorical_variables_num = len(categorical_variables)","ec8e4ff5":"print(f\"Number of nulls in features: {df_train.isnull().sum().sum()}\")","d4a7da8a":"null_variables_in_numerics = [var for var in numerical_variables if df_train[var].isnull().sum() > 0]\nprint(f\"Number of null features: {len(null_variables_in_numerics)}\")\nprint(f\"Number of nulls in numerical features: {df_train[null_variables_in_numerics].isnull().sum().sum()}\")\nnum_nulls = df_train[numerical_variables].isnull().sum().sort_values(ascending=False)\nfor row in num_nulls.iteritems():\n    print(f\"{row[0]}  \\t{row[1]} nulls \\t{row[1]\/df_train.shape[0]:.3f}%\")","e6b1304a":"null_variables_in_categoric = [var for var in categorical_variables if df_train[var].isnull().sum() > 0]\nprint(f\"Number of null features: {len(null_variables_in_categoric)}\")\nprint(f\"Number of nulls in categorical features: {df_train[null_variables_in_categoric].isnull().sum().sum()}\")\nnum_nulls = df_train[categorical_variables].isnull().sum().sort_values(ascending=False)\nfor row in num_nulls.iteritems():\n    print(f\"{row[0]}   \\t{row[1]} nulls \\t{row[1]\/df_train.shape[0]:.3f}%\")","d9aed483":"#Deleting outliers\ndf_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train[TARGET]<300000)].index)\ndf_train = df_train.drop(df_train[(df_train['TotalBsmtSF']>5000) & (df_train[TARGET]<200000)].index)\ndf_train = df_train.drop(df_train[(df_train['1stFlrSF']>4000) & (df_train[TARGET]<200000)].index)","aab0641e":"null_variables_in_categoric","954bb57b":"df_train[null_variables_in_categoric] = df_train[null_variables_in_categoric].fillna('NONE')\ndf_test[null_variables_in_categoric] = df_test[null_variables_in_categoric].fillna('NONE')\n\n\nnum_nulls = df_train[categorical_variables].isnull().sum().sort_values(ascending=False)\nnum_nulls","0f3a90e2":"df_test[categorical_variables].isnull().sum().sort_values(ascending=False)","9c05cad7":"categorical_features_nulls_filled_with_mode = ['MSZoning', 'Utilities', 'Exterior1st', 'Exterior2nd', 'Electrical', 'KitchenQual', 'Functional', 'SaleType']\nfor var in categorical_features_nulls_filled_with_mode:\n    mode = df_test[var].mode()[0]\n    df_test[var] = df_test[var].fillna(mode)","0328895f":"null_variables_in_numerics","1605a2a5":"numerical_features_nulls_filled_with_zero = ['GarageYrBlt']\n\ndf_train[numerical_features_nulls_filled_with_zero] = df_train[numerical_features_nulls_filled_with_zero].fillna(0)\ndf_test[numerical_features_nulls_filled_with_zero] = df_test[numerical_features_nulls_filled_with_zero].fillna(0)\n\n\nnumerical_features_nulls_filled_with_mode = ['LotFrontage', 'MasVnrArea']\nfor var in numerical_features_nulls_filled_with_mode:\n    mode = df_train[var].mode()[0]\n    df_train[var+'_na'] = df_train[var].isnull()\n    df_train[var] = df_train[var].fillna(mode)\n    \n    df_test[var+'_na'] = df_test[var].isnull()\n    df_test[var] = df_test[var].fillna(mode)\n    \n    \nnum_nulls = df_train[numerical_variables].isnull().sum().sort_values(ascending=False)\nnum_nulls   ","54f54f75":"df_test[numerical_variables].isnull().sum().sort_values(ascending=False)","761970c9":"numerical_features_nulls_filled_with_zero_in_test_set = ['GarageCars', 'GarageArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']\ndf_test[numerical_features_nulls_filled_with_zero_in_test_set] = df_test[numerical_features_nulls_filled_with_zero_in_test_set].fillna(0)","dd71f17a":"df_train['TimeSold'] = df_train['YrSold'] - df_train['YearBuilt']\ndf_test['TimeSold'] = df_test['YrSold'] - df_test['YearBuilt']","442bfdff":"def to_ordinal_encoding(train, test, var):\n    train = train.copy()\n    test = test.copy()\n\n    orderd_labels = train.groupby(var)[TARGET].mean().sort_values().index.tolist()\n    ordinal_encoder = OrdinalEncoder(categories=[orderd_labels], handle_unknown='ignore')\n    \n    train[var] = ordinal_encoder.fit_transform(train[var].values.reshape(-1, 1))\n    test[var] = ordinal_encoder.transform(test[var].values.reshape(-1, 1))\n    \n    return train, test","8aad0363":"for var in categorical_variables:\n    df_train, df_test = to_ordinal_encoding(df_train, df_test, var)","db5b8ae1":"# Reference\n# https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset\ntrain = df_train\ntest = df_test\n\ntrain[\"OverallGrade\"] = train[\"OverallQual\"] * train[\"OverallCond\"]\n# Overall quality of the garage\ntrain[\"GarageGrade\"] = train[\"GarageQual\"] * train[\"GarageCond\"]\n# Overall quality of the exterior\ntrain[\"ExterGrade\"] = train[\"ExterQual\"] * train[\"ExterCond\"]\n# Overall kitchen score\ntrain[\"KitchenScore\"] = train[\"KitchenAbvGr\"] * train[\"KitchenQual\"]\n# Overall fireplace score\ntrain[\"FireplaceScore\"] = train[\"Fireplaces\"] * train[\"FireplaceQu\"]\n# Overall garage score\ntrain[\"GarageScore\"] = train[\"GarageArea\"] * train[\"GarageQual\"]\n# Overall pool score\ntrain[\"PoolScore\"] = train[\"PoolArea\"] * train[\"PoolQC\"]\n# Total number of bathrooms\ntrain[\"TotalBath\"] = train[\"BsmtFullBath\"] + (0.5 * train[\"BsmtHalfBath\"]) + \\\ntrain[\"FullBath\"] + (0.5 * train[\"HalfBath\"])\n# Total SF for house (incl. basement)\ntrain[\"AllSF\"] = train[\"GrLivArea\"] + train[\"TotalBsmtSF\"]\n# Total SF for 1st + 2nd floors\ntrain[\"AllFlrsSF\"] = train[\"1stFlrSF\"] + train[\"2ndFlrSF\"]\n# Total SF for porch\ntrain[\"AllPorchSF\"] = train[\"OpenPorchSF\"] + train[\"EnclosedPorch\"] + \\\ntrain[\"3SsnPorch\"] + train[\"ScreenPorch\"]\n\ntest[\"OverallGrade\"] = test[\"OverallQual\"] * test[\"OverallCond\"]\n# Overall quality of the garage\ntest[\"GarageGrade\"] = test[\"GarageQual\"] * test[\"GarageCond\"]\n# Overall quality of the exterior\ntest[\"ExterGrade\"] = test[\"ExterQual\"] * test[\"ExterCond\"]\n# Overall kitchen score\ntest[\"KitchenScore\"] = test[\"KitchenAbvGr\"] * test[\"KitchenQual\"]\n# Overall fireplace score\ntest[\"FireplaceScore\"] = test[\"Fireplaces\"] * test[\"FireplaceQu\"]\n# Overall garage score\ntest[\"GarageScore\"] = test[\"GarageArea\"] * test[\"GarageQual\"]\n# Overall pool score\ntest[\"PoolScore\"] = test[\"PoolArea\"] * test[\"PoolQC\"]\n# Total number of bathrooms\ntest[\"TotalBath\"] = test[\"BsmtFullBath\"] + (0.5 * test[\"BsmtHalfBath\"]) + \\\ntest[\"FullBath\"] + (0.5 * test[\"HalfBath\"])\n# Total SF for house (incl. basement)\ntest[\"AllSF\"] = test[\"GrLivArea\"] + test[\"TotalBsmtSF\"]\n# Total SF for 1st + 2nd floors\ntest[\"AllFlrsSF\"] = test[\"1stFlrSF\"] + test[\"2ndFlrSF\"]\n# Total SF for porch\ntest[\"AllPorchSF\"] = test[\"OpenPorchSF\"] + test[\"EnclosedPorch\"] + \\\ntest[\"3SsnPorch\"] + test[\"ScreenPorch\"]\n\ndf_train = train\ndf_test = test","12aa7ff7":"def transform_with_log(train, test, features):\n    train = train.copy()\n    test = test.copy()\n    \n    for var in features:\n        train[var] = np.log(train[var])\n        test[var] = np.log(test[var]) \n        \n    return train, test","b01cdc72":"features_transformed_with_log = ['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea']\ndf_train, df_test = transform_with_log(df_train, df_test, features_transformed_with_log)\n\ndf_train[TARGET] = np.log(df_train[TARGET])","0b82d83f":"features = [var for var in df_train.columns if var != 'SalePrice']","218070e0":"def transform_with_MinMax(train, test, features):\n    train = train.copy()\n    test = test.copy()\n    \n    scaler = MinMaxScaler()\n    scaler.fit(train[features])\n    \n    train[features] = scaler.transform(train[features])\n    test[features] = scaler.transform(test[features])\n    \n    return train, test\n\ndef transform_with_Standard(train, test, features):\n    train = train.copy()\n    test = test.copy()\n    \n    scaler = StandardScaler()\n    scaler.fit(train[features])\n    \n    train[features] = scaler.transform(train[features])\n    test[features] = scaler.transform(test[features])\n    \n    return train, test","b145837a":"df_train, df_test = transform_with_MinMax(df_train, df_test, features)","f8f4f94c":"def get_skewed_features(train, numeric_feats, threshold=0.5):\n    df_train = train.copy()\n\n    skewed_feats = df_train[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\n    skewness = pd.DataFrame({'Skew' :skewed_feats})\n\n    skewness = skewness[abs(skewness) > threshold]\n    print(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n    \n    return skewness.index\n\ndef transform_with_box_cox(data, skewed_features, lambda_=0.15):\n    data = data.copy()\n    lambda_ = lambda_\n    \n    for feat in skewed_features:\n        data[feat] = boxcox1p(data[feat], lambda_)\n        \n    return data","22af4b89":"skewed_features = get_skewed_features(df_train, features)\n\ndf_train = transform_with_box_cox(df_train, skewed_features)\ndf_test = transform_with_box_cox(df_test, skewed_features)","e0d041ed":"# df_train.to_csv('processed_train.csv', index=False)\n# df_test.to_csv('processed_test.csv', index=True)\n\ndf_test.reset_index(inplace=True)","16f98536":"import numpy as np\nimport pandas as pd\nimport math\n\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import cross_val_score, KFold, train_test_split, GridSearchCV\nfrom sklearn.metrics import make_scorer, mean_squared_error\nfrom sklearn.linear_model import Lasso, ElasticNet\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nimport xgboost as xgb","b0dd9338":"SEED = 42\nnp.random.seed(SEED)","7a3f9a91":"X_test = df_test.drop(['Id'], axis=1)\ntest_Id = df_test['Id']","16888332":"X_train = df_train.drop([TARGET], axis=1)\ny_train = df_train[TARGET]","2f645fc2":"#Validation function\nn_folds =  12\n# n_folds =  X_train.shape[0]\n\ndef scorer_rmse(y_true, y_pred):\n    score = math.sqrt(mean_squared_error(y_true, y_pred))\n    return score\n\nscorer = make_scorer(scorer_rmse, greater_is_better=False)\n\ndef cv_rsme(model, X_train, y_train):\n    kf = KFold(n_folds, shuffle=True, random_state=SEED).get_n_splits(X_train)\n    rmse= -cross_val_score(model, X_train.values, y_train.values, scoring=scorer, cv=kf, n_jobs=-1)\n    return rmse","bc4e5ee0":"df_scores = pd.DataFrame(columns=['rsme', 'Summary'])\ndf_scores.index.name = 'Model'","21852d53":"%%time\n\nparam_grid = {'alpha': [0.0005, 0.005, 0.05, 0.01, 0.1, 1],\n             }\n\nlasso = Lasso(random_state=SEED)\ngrid_search = GridSearchCV(lasso, param_grid=param_grid, cv=n_folds, scoring=scorer, return_train_score=True, n_jobs=-1)\ngrid_search.fit(X_train, y_train)\nbest_params_lasso = grid_search.best_params_\n\nlasso = Lasso(**best_params_lasso, random_state=SEED)\nlasso_score = cv_rsme(lasso, X_train, y_train)\nprint(f\"lasso rsme mean: {lasso_score.mean()} std: {lasso_score.std()}\")\n\ndf_scores.loc['Lasso', 'Summary'] = (lasso.get_params().keys(), lasso.get_params().values())                   \ndf_scores.loc['Lasso', 'rsme'] = lasso_score.mean()","56329ab0":"en = ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)\nen_score = cv_rsme(en, X_train, y_train)\nprint(f\"lasso rsme mean: {en_score.mean()} std: {en_score.std()}\")\n\ndf_scores.loc['Elastic', 'Summary'] = (en.get_params().keys(), en.get_params().values())                   \ndf_scores.loc['Elastic', 'rsme'] = en_score.mean()","f8cdd605":"krr = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nkrr_score = cv_rsme(krr, X_train, y_train)\nprint(f\"lasso rsme mean: {krr_score.mean()} std: {krr_score.std()}\")\n\ndf_scores.loc['KRR', 'Summary'] = (krr.get_params().keys(), krr.get_params().values())                   \ndf_scores.loc['KRR', 'rsme'] = krr_score.mean()","b73ee694":"%%time\n\n# param_grid = {'C': [1, 5, 10, 20],\n#              'kernel': ['poly', 'rbf', 'sigmoid'],\n#               'degree': [3, 5, 10],\n#              'gamma': ['scale', 'auto'],\n#              }\n\n# svr = SVR()\n# grid_search = GridSearchCV(svr, param_grid=param_grid, cv=n_folds, scoring=scorer, return_train_score=True, n_jobs=-1)\n# grid_search.fit(X_train, y_train)\n# best_params_svr = grid_search.best_params_\n\nbest_params_svr = {'C': 1, 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf'}\nsvr = SVR(**best_params_svr)\nsvr_score = cv_rsme(svr, X_train, y_train)\nprint(f\"svr rsme mean: {svr_score.mean()} std: {svr_score.std()}\")\n\ndf_scores.loc['SVR', 'Summary'] = (svr.get_params().keys(), svr.get_params().values())                   \ndf_scores.loc['SVR', 'rsme'] = svr_score.mean()","ed429084":"%%time\n\nGBoost = GradientBoostingRegressor(n_estimators=2000, learning_rate=0.05,\n                                   max_features='sqrt', max_depth=4,\n                                   loss='huber', random_state=SEED)\nGBoost_score = cv_rsme(GBoost, X_train, y_train)\nprint(f\"GBoost rsme mean: {GBoost_score.mean()} std: {GBoost_score.std()}\")\n\n\ndf_scores.loc['Gboost', 'rsme'] = GBoost_score.mean()\ndf_scores.loc['Gboost', 'Summary'] = (GBoost.get_params().keys(), GBoost.get_params().values()) ","cf7c2082":"%%time \n\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nmodel_xgb_score = cv_rsme(model_xgb, X_train, y_train)\nprint(f\"RandomForest rsme mean: {model_xgb_score.mean()} std: {model_xgb_score.std()}\")\n\ndf_scores.loc['Xgb', 'rsme'] = model_xgb_score.mean()\ndf_scores.loc['Xgb', 'Summary'] = (model_xgb.get_params().keys(), model_xgb.get_params().values()) ","2e17e8c2":"# Reference\n# https:\/\/www.analyticsvidhya.com\/blog\/2020\/12\/improve-predictive-model-score-stacking-regressor\/\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=10):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=SEED)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","2dd03910":"%%time\n\nstacked_averaged_models = StackingAveragedModels(base_models=(svr, en, GBoost),\n                                                 meta_model=lasso, n_folds=n_folds)\n\nstacking_score = cv_rsme(stacked_averaged_models, X_train, y_train)\nprint(f\"Stacking rsme mean: {stacking_score.mean()} std: {stacking_score.std()}\")\n\n\ndf_scores.loc['Stacking', 'Summary'] = (\"base_models = (svr, en, GBoost),\\\n                                                 meta_model = lasso, n_folds=n_folds\")                   \ndf_scores.loc['Stacking', 'rsme'] = stacking_score.mean()","9e8286d3":"stacked_averaged_models.fit(X_train.values, y_train.values)\npreds = np.expm1(stacked_averaged_models.predict(X_test.values))","f9e71963":"sub = pd.DataFrame()\nsub['Id'] = test_Id\nsub[TARGET] = preds\n# sub.to_csv('submission.csv',index=False)","341377af":"## Submission","93044722":"# Modeling","b5979fe0":"## Correlations","d6435ae7":"### Target vs All Summary\n\n* numerical variables\n> * 'GrLivArea' and 'TotalBsmtSF' seem to be linearly related with 'SalePrice' which means that as one variable increases, the other also increases. In the case of 'TotalBsmtSF', the slope towrds y ('SalePrice') is very high.\n> * 'OverallQual', 'TotRmsAbvGrd' and 'YearBuilt' also seem to be related with 'SalePrice'. The relationship seems to be stronger in the case of 'OverallQual', where the box plot shows how sales prices increase with the overall quality.\n* categorical variables\n> * All variables that are ordinals goes towards the main concept of ordinality (e.g. 'PoolQC' (Pool quality) if 'Ex' (Excellent) the 'SalePrice' is higher than 'Gd' (Good)).\n> * Other variables depends on its value for example 'CentralAir' if it was 'Y' (yes) the higher the sale price . ","ceb7c36f":"### First Intuition Summary\n* 1460 records. \n* 80 feature + target.\n* 37 numerical feature include (4 time features + target).\n* 43 categorical features.\n* 0 complete duplicates.\n* 6965 null value in all features.\n* 348 nulls in numerical features.\n* 6617 nulls in categorical features.","da44a243":"## References\n> https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard <br>\n> https:\/\/github.com\/trainindata\/dmlm-research-and-production\/blob\/master\/Section-2-Machine-Learning-Pipeline-Overview\/Machine-Learning-Pipeline-Step2-Feature-Engineering.ipynb","2ebf6d8d":"## First Intuition","8ef85c64":"- **TimeSold** : the difference between the year the house was sold and the year the house was built.\n<!-- - **TimeSold** : the difference between the year the house was sold and the year the house was built. -->","236ce59c":"**Elastic regression**","8ad9df7f":"## Missing values","92627fb0":"**Categorical variables**","9103ee5a":"## References\n> https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python <br>\n> https:\/\/github.com\/trainindata\/dmlm-research-and-production\/blob\/master\/Section-2-Machine-Learning-Pipeline-Overview\/Machine-Learning-Pipeline-Step1-DataAnalysis.ipynb","ee9a3f59":"# Feature Engineering","e3270b1b":"## Adding Features (again)","a3326af4":"### In Summary\n\n* 'GarageCars' and 'GarageArea' are the same, the number of cars that fit into the garage is a consequence of the garage area they are like twin brothers. Therefore, we just need one of these variables in our analysis (we can keep 'GarageCars' since its correlation with 'SalePrice' is higher).\n* 'TotalBsmtSF' and '1stFloor' also the same twin brothers. We can keep 'TotalBsmtSF'.\n* 'TotRmsAbvGrd' and 'GrLivArea', twin brothers again. \n* 'YearBuilt' is slightly correlated with 'SalePrice'.","52bfa973":"- **LotFrontage** : add a binary missing value indicator variable then fill with the \"mode\".\n- **MasVnrArea** : add a binary missing value indicator variable then fill with the \"mode\".\n- **GarageYrBlt, GarageCars & GarageArea** : since there is no grage then fill with \"0\".\n- **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath & BsmtHalfBath** : since there is no basement then fill with \"0\".","904ae040":"**Gradient boost regressor**","0e704400":"## Removing the outliers","2e9e7310":"# EDA","d9ef5c1c":"**Xgboost**","84a4b141":"- **Alley** : data description says NA means \"no alley access\".\n- **MasVnrType** : data description says None means \"None\".\n- **BsmtQual** : data description says NA means \"No Basement\".\n- **BsmtCond** : data description says NA means \"No Basement\".\n- **BsmtExposure** : data description says NA means \"No Basement\".\n- **BsmtFinType1** : data description says NA means \"No Basement\".\n- **BsmtFinType2** : data description says NA means \"No Basement\".\n- **FireplaceQu** : data description says NA means \"no fireplace\"\n- **GarageType** : NA means there is \"no garage\".\n- **GarageFinish** : NA means there is \"no garage\".\n- **GarageQual** : NA means there is \"no garage\".\n- **GarageCond** : NA means there is \"no garage\".\n- **PoolQC** : data description says NA means \"No  Pool\". \n- **Fence** : data description says NA means \"no fence\".\n- **MiscFeature** : data description says NA means \"None\".\n- **Electrical** : only 1 values missing so fill it with its mode won't affect as much.","05079037":"**Lasso regression**","87f0e2a9":"**Kernel ridge regression**","333ccc0c":"- **MSZoning** : only 4 values missing so fill it with its mode won't affect as much.\n- **Utilities** : only 2 values missing so fill it with its mode won't affect as much.\n- **Exterior1st & Exterior2nd** : only 1 values missing so fill them with there mode won't affect as much.\n- **KitchenQual** : only 1 values missing so fill it with its mode won't affect as much.\n- **Functional** : only 2 values missing so fill it with its mode won't affect as much.\n- **SaleType** : only 1 values missing so fill it with its mode won't affect as much.","8e436b7b":"## Target vs All","cfa74c52":"**Support vector regressor**","1abc0310":"## Feature Transformation & Scaling","ca41cd5a":"### Numerical Variables","a55ac3eb":"There has been a drop in the value of the houses. That is unusual, in real life, house prices typically go up as years go by maybe because 2008 financial crisis.","376cc136":"## Categorical Encoding","05f58240":"## Adding Features","57333df6":"**Numerical variables**"}}