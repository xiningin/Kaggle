{"cell_type":{"66047b31":"code","802f124b":"code","b067f420":"code","1d02784b":"code","ab87f099":"code","4706f346":"code","9fcb64d3":"code","bc6df3d9":"code","a6387080":"code","ab277258":"code","c8b34105":"code","70296332":"code","7faa2cd2":"code","5cd1c8d3":"code","96a6ea40":"code","d8fa2bfc":"code","dc34a678":"code","3c9c25e3":"code","90f1f6bd":"code","22bf1850":"code","4483f06f":"code","f1ac3f11":"code","0cbe26ec":"code","b3f40354":"code","6e7aa741":"markdown","f7fca2bb":"markdown","b6f457a4":"markdown","e690c7be":"markdown","7b4e45ba":"markdown","60dd6ba2":"markdown","cfa70611":"markdown","1f8791ec":"markdown","5f1ad6c5":"markdown","27bdefd8":"markdown"},"source":{"66047b31":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","802f124b":"import pandas as pd\nirisr2 = pd.read_csv(\"..\/input\/irisr2.csv\")","b067f420":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt","1d02784b":"irisr2.sample(10)","ab87f099":"irisr2.isnull().sum()","4706f346":"irisr2.info()","9fcb64d3":"irisr2.isna()","bc6df3d9":"irisr2['SepalLengthCm'].fillna(irisr2['SepalLengthCm'].median(),inplace=True)\nirisr2['SepalWidthCm'].fillna(irisr2['SepalWidthCm'].median(),inplace = True)\nirisr2['PetalLengthCm'].fillna(irisr2['PetalLengthCm'].median(),inplace = True)\nirisr2['PetalWidthCm'].fillna(irisr2['PetalWidthCm'].median(),inplace = True)","a6387080":"# We are checking whether the data has been refined\n\nirisr2.isnull().sum()","ab277258":"### Dealing with categorical data\n\n##Change all the classes to numericals (0 to 2)\n\n###Hint: use **LabelEncoder()**\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nirisr2['Species'] = le.fit_transform(irisr2['Species'])","c8b34105":"irisr2.loc[:,~irisr2.corr()['Species'].between(-0.1,0.1,inclusive=True)].head()","70296332":"irisr2.loc[:,irisr2.var()>0.1].head()","7faa2cd2":"sns.pairplot(irisr2,hue='Species')","5cd1c8d3":"from sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom scipy.stats import zscore\n\n\n\nfeature_columns = ['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm','PetalWidthCm']\nX = irisr2[feature_columns].values\ny = irisr2['Species'].values\n\n\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)","96a6ea40":"# Split X and y into training validation and testing data set\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n","d8fa2bfc":"## Calculating the Z Score befor instatiating the model\n\nX_train_z = zscore(X_train)  \n\nX_test_z = zscore(X_test)","dc34a678":"# Instantiating the model \n# when value of K = 3\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.model_selection import cross_val_score\nclassifier = KNeighborsClassifier(n_neighbors=3)\n\n#Fitting the model\nclassifier.fit(X_train, y_train)\n\n# Predicting the Test set results\ny_pred = classifier.predict(X_test)","3c9c25e3":"## Building Confusion matrix\n\ncm = confusion_matrix(y_test, y_pred)\ncm","90f1f6bd":"## Calculating the accuracy model\n\n\n\naccuracy = accuracy_score(y_test, y_pred)*100\nprint('Accuracy of our model is equal ' + str(round(accuracy, 2)) + ' %.')","22bf1850":"# creating list of K for KNN\n\nk_list = list(range(1,50,2))\n# creating list of cv scores\ncv_scores = []\n\n# perform 10-fold cross validation\n\nfor k in k_list:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, X_train, y_train, cv=10, scoring='accuracy')\n    cv_scores.append(scores.mean())\n","4483f06f":"## Find optimal value of K\n\n##- Run the KNN with no of neighbours to be 1, 3, 5 ... 19\n##- Find the **optimal number of neighbours** from the above list\n\n# changing to misclassification error\nMSE = [1 - x for x in cv_scores]\n\nplt.figure()\nplt.figure(figsize=(15,10))\nplt.title('The optimal number of neighbors', fontsize=20, fontweight='bold')\nplt.xlabel('Number of Neighbors K', fontsize=15)\nplt.ylabel('Misclassification Error', fontsize=15)\nsns.set_style(\"whitegrid\")\nplt.plot(k_list, MSE)\n\nplt.show()","f1ac3f11":"## FINDING THE BEST VALUE OF K ( OPTIMAL VALUE OF K)\n\nbest_k = k_list[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d.\" % best_k)","0cbe26ec":"## Plot accuracy\n\n##Plot accuracy score vs k (with k value on X-axis) using matplotlib.","b3f40354":"neighbors = np.arange(1, 9)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\n# Loop over different values of k\nfor i, k in enumerate(neighbors):\n    # Setup a k-NN Classifier with k neighbors: knn\n    knn = KNeighborsClassifier(n_neighbors=k)\n\n    # Fit the classifier to the training data\n    knn.fit(X_train, y_train)\n    \n    #Compute accuracy on the training set\n    train_accuracy[i] = knn.score(X_train, y_train)\n\n    #Compute accuracy on the testing set\n    test_accuracy[i] = knn.score(X_test, y_test)\n\n# Generate plot\nplt.title('k-NN: Varying Number of Neighbors')\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy')\nplt.plot(neighbors, train_accuracy, label = 'Training Accuracy')\nplt.legend()\nplt.xlabel('Number of Neighbors')\nplt.ylabel('Accuracy')\nplt.show()","6e7aa741":"# K nearest neighbors\n\nKNN falls in the supervised learning family of algorithms. Informally, this means that we are given a labelled dataset consiting of training observations (x, y) and would like to capture the relationship between x and y. More formally, our goal is to learn a function h: X\u2192Y so that given an unseen observation x, h(x) can confidently predict the corresponding output y.\n\nIn this module we will explore the inner workings of KNN, choosing the optimal K values and using KNN from scikit-learn.\n\n## Overview\n\n1. Read the problem statement.\n\n2. Get the dataset.\n\n3. Explore the dataset.\n\n4. Pre-processing of dataset.\n\n5. Visualization\n\n6. Transform the dataset for building machine learning model.\n\n7. Split data into train, test set.\n\n8. Build Model.\n\n9. Apply the model.\n\n10. Evaluate the model.\n\n11. Finding Optimal K value\n\n12. Repeat 7, 8, 9 steps.","f7fca2bb":"### Dataset\n\nThe data set we\u2019ll be using is the Iris Flower Dataset which was first introduced in 1936 by the famous statistician Ronald Fisher and consists of 50 observations from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals.\n\n**Download the dataset here:**\n- https:\/\/www.kaggle.com\/uciml\/iris\n\n**Train the KNN algorithm to be able to distinguish the species from one another given the measurements of the 4 features.**","b6f457a4":"Observe the independent variables variance and drop such variables having no variance or almost zero variance (variance < 0.1). They will be having almost no influence on the classification\n\nHint: use **var()**","e690c7be":"Plot the scatter matrix for all the variables.\n\nHint: use **pandas.plotting.scatter_matrix()**\n\nyou can also use pairplot()","7b4e45ba":"## Load data\n\nImport the data set and print 10 random rows from the data set\n\nHint: use **sample()** function to get random rows","60dd6ba2":"## Data Pre-processing\n\n### Question 2 - Estimating missing values\n\nIts not good to remove the records having missing values all the time. We may end up loosing some data points. So, we will have to see how to replace those missing values with some estimated values (median)\n\nCalculate the number of missing values per column\n- don't use loops","cfa70611":"## Split the dataset into training and test sets\n\nSplit the dataset into training and test sets with 80-20 ratio\nHint: use **train_test_split()**","1f8791ec":"Observe the association of each independent variable with target variable and drop variables from feature set having correlation in range -0.1 to 0.1 with target variable.\n\nHint: use **corr()**","5f1ad6c5":"**Build Model**\n\nBuild the model and train and test on training and test sets respectively using **scikit-learn**.\n\nPrint the Accuracy of the model with different values of **k = 3, 5, 9**\n\nHint: For accuracy you can check **accuracy_score()** in scikit-learn## Build Model\n\nBuild the model and train and test on training and test sets respectively using **scikit-learn**.\n\nPrint the Accuracy of the model with different values of **k = 3, 5, 9**\n\nHint: For accuracy you can check **accuracy_score()** in scikit-learn","27bdefd8":"Fill missing values with median of that particular column"}}