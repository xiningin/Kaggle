{"cell_type":{"942f08b1":"code","5115a248":"code","0b5f1cf5":"code","e76075a0":"code","9b3eafe9":"code","1023b405":"code","4326279b":"code","82a1dc45":"code","b4ed5006":"code","a3c4e27a":"code","839caddc":"code","7391c318":"code","6cec8635":"code","af509874":"code","29f1d5f4":"code","8aad7670":"code","e1c5004c":"code","fbda46fa":"markdown","dd97c588":"markdown"},"source":{"942f08b1":"import os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom keras.callbacks import LearningRateScheduler, EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import optimizers\nfrom keras.models import load_model","5115a248":"df_train = pd.read_csv('..\/input\/train.csv').astype('float32')\ndf_test = pd.read_csv('..\/input\/test.csv')","0b5f1cf5":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    #start_mem = df.memory_usage().sum() \/ 1024**2\n    #print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n\n    for col in df.columns:\n        col_type = df[col].dtype\n\n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n\n    #end_mem = df.memory_usage().sum() \/ 1024**2\n    #print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    #print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n\n    return df","e76075a0":"df_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)","9b3eafe9":"# https:\/\/www.kaggle.com\/sidjhanji\/pubg-kill-them-all\n# Some Feature Engineering\ndf_train[\"distance\"] = df_train[\"rideDistance\"]+df_train[\"walkDistance\"]+df_train[\"swimDistance\"]\n# df_train[\"healthpack\"] = df_train[\"boosts\"] + df_train[\"heals\"]\ndf_train[\"skill\"] = df_train[\"headshotKills\"]+df_train[\"roadKills\"]\ndf_test[\"distance\"] = df_test[\"rideDistance\"]+df_test[\"walkDistance\"]+df_test[\"swimDistance\"]\n# df_test[\"healthpack\"] = df_test[\"boosts\"] + df_test[\"heals\"]\ndf_test[\"skill\"] = df_test[\"headshotKills\"]+df_test[\"roadKills\"]","1023b405":"\"\"\"\nit is a team game, scores within the same group is same, so let's get the feature of each group\n\"\"\"\ndf_train_size = df_train.groupby(['matchId','groupId']).size().reset_index(name='group_size')\ndf_test_size = df_test.groupby(['matchId','groupId']).size().reset_index(name='group_size')\n\ndf_train_mean = df_train.groupby(['matchId','groupId']).mean().reset_index()\ndf_test_mean = df_test.groupby(['matchId','groupId']).mean().reset_index()\n\ndf_train_max = df_train.groupby(['matchId','groupId']).max().reset_index()\ndf_test_max = df_test.groupby(['matchId','groupId']).max().reset_index()\n\ndf_train_min = df_train.groupby(['matchId','groupId']).min().reset_index()\ndf_test_min = df_test.groupby(['matchId','groupId']).min().reset_index()\n","4326279b":"\"\"\"\nalthough you are a good game player, \nbut if other players of other groups in the same match is better than you, you will still get little score\nso let's add the feature of each match\n\"\"\"\ndf_train_match_mean = df_train.groupby(['matchId']).mean().reset_index()\ndf_test_match_mean = df_test.groupby(['matchId']).mean().reset_index()\n\ndf_train = pd.merge(df_train, df_train_mean, suffixes=[\"\", \"_mean\"], how='left', on=['matchId', 'groupId'])\ndf_test = pd.merge(df_test, df_test_mean, suffixes=[\"\", \"_mean\"], how='left', on=['matchId', 'groupId'])\ndel df_train_mean\ndel df_test_mean\n\ndf_train = pd.merge(df_train, df_train_max, suffixes=[\"\", \"_max\"], how='left', on=['matchId', 'groupId'])\ndf_test = pd.merge(df_test, df_test_max, suffixes=[\"\", \"_max\"], how='left', on=['matchId', 'groupId'])\ndel df_train_max\ndel df_test_max\n\ndf_train = pd.merge(df_train, df_train_min, suffixes=[\"\", \"_min\"], how='left', on=['matchId', 'groupId'])\ndf_test = pd.merge(df_test, df_test_min, suffixes=[\"\", \"_min\"], how='left', on=['matchId', 'groupId'])\ndel df_train_min\ndel df_test_min\n\ndf_train = pd.merge(df_train, df_train_match_mean, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\ndf_test = pd.merge(df_test, df_test_match_mean, suffixes=[\"\", \"_match_mean\"], how='left', on=['matchId'])\ndel df_train_match_mean\ndel df_test_match_mean\n\ndf_train = pd.merge(df_train, df_train_size, how='left', on=['matchId', 'groupId'])\ndf_test = pd.merge(df_test, df_test_size, how='left', on=['matchId', 'groupId'])\ndel df_train_size\ndel df_test_size\n\ntarget = 'winPlacePerc'\ntrain_columns = list(df_test.columns)","82a1dc45":"\"\"\" remove some columns \"\"\"\ntrain_columns.remove(\"Id\")\ntrain_columns.remove(\"matchId\")\ntrain_columns.remove(\"groupId\")\ntrain_columns.remove(\"Id_mean\")\ntrain_columns.remove(\"Id_max\")\ntrain_columns.remove(\"Id_min\")\ntrain_columns.remove(\"Id_match_mean\")","b4ed5006":"\"\"\"\nin this game, team skill level is more important than personal skill level \nmaybe you are a good player, but if your teammates is bad, you will still lose\nso let's remove the features of each player, just select the features of group and match\n\"\"\"\ntrain_columns_new = []\nfor name in train_columns:\n    if '_' in name:\n        train_columns_new.append(name)\ntrain_columns = train_columns_new    \nprint(train_columns)","a3c4e27a":"# train_columns = ['assists', 'boosts', 'damageDealt', 'DBNOs', 'headshotKills', \n#                 'heals', 'killPlace', 'killPoints', 'kills', 'killStreaks', \n#                 'longestKill', 'maxPlace', 'numGroups', 'revives','rideDistance', \n#                 'roadKills', 'swimDistance', 'teamKills', 'vehicleDestroys', 'walkDistance', \n#                 'weaponsAcquired', 'winPoints']\n\nX = df_train[train_columns]\nY = df_test[train_columns]\nT = df_train[target]\n\ndel df_train\n\n# X = np.array(X, dtype=np.float32)\n# Y = np.array(Y, dtype=np.float32)\n# T = np.array(T, dtype=np.float32)","839caddc":"x_train, x_test, t_train, t_test = train_test_split(X, T, test_size = 0.2, random_state = 1234)\n\n# scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1)).fit(x_train)\nscaler = preprocessing.QuantileTransformer().fit(x_train)\n\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)\nY = scaler.transform(Y)\n\nprint(\"x_train\", x_train.shape, x_train.min(), x_train.max())\nprint(\"x_test\", x_test.shape, x_test.min(), x_test.max())\nprint(\"Y\", Y.shape, Y.min(), Y.max())","7391c318":"model = Sequential()\nmodel.add(Dense(512, kernel_initializer='he_normal', input_dim=x_train.shape[1], activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(256, kernel_initializer='he_normal', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(128, kernel_initializer='he_normal', activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.1))\nmodel.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n\n# model.summary()","6cec8635":"# optimizer = optimizers.SGD(lr=0.01, momentum=0.9, decay=1e-4, nesterov=True)\noptimizer = optimizers.Adam(lr=0.01, epsilon=1e-8, decay=1e-4, amsgrad=False)\n\nmodel.compile(optimizer=optimizer, loss='mse', metrics=['mae'])","af509874":"def step_decay_schedule(initial_lr=1e-3, decay_factor=0.75, step_size=10, verbose=0):\n    '''\n    Wrapper function to create a LearningRateScheduler with step decay schedule.\n    '''\n    def schedule(epoch):\n        return initial_lr * (decay_factor ** np.floor(epoch\/step_size))\n    \n    return LearningRateScheduler(schedule, verbose)\n\nlr_sched = step_decay_schedule(initial_lr=0.1, decay_factor=0.9, step_size=1, verbose=1)\nearly_stopping = EarlyStopping(monitor='val_mean_absolute_error', mode = 'min', patience=4, verbose=1)\n# model_checkpoint = ModelCheckpoint(filepath='best_model.h5', monitor='val_mean_absolute_error', mode = 'min', save_best_only=True, verbose=1)\n# reduce_lr = ReduceLROnPlateau(monitor='val_mean_absolute_error', mode = 'min',factor=0.5, patience=3, min_lr=0.0001, verbose=1)","29f1d5f4":"history = model.fit(x_train, t_train, \n                 validation_data=(x_test, t_test),\n                 epochs=30,\n                 batch_size=32768,\n                 callbacks=[lr_sched,early_stopping], \n                 verbose=1)","8aad7670":"# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\n# Plot training & validation mae values\nplt.plot(history.history['mean_absolute_error'])\nplt.plot(history.history['val_mean_absolute_error'])\nplt.title('Mean Abosulte Error')\nplt.ylabel('Mean absolute error')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","e1c5004c":"pred = model.predict(Y)\npred = pred.ravel()\n\n# pred = (pred + 1) \/ 2\ndf_test['winPlacePercPred'] = np.clip(pred, a_min=0, a_max=1)\n\n\naux = df_test.groupby(['matchId','groupId'])['winPlacePercPred'].agg('mean').groupby('matchId').rank(pct=True).reset_index()\naux.columns = ['matchId','groupId','winPlacePerc']\ndf_test = df_test.merge(aux, how='left', on=['matchId','groupId'])\n    \nsubmission = df_test[['Id', 'winPlacePerc']]\n\nsubmission.to_csv('submission.csv', index=False)","fbda46fa":"Code adapted from : [Simple NN Baseline](https:\/\/www.kaggle.com\/amoeba3215\/simple-nn-baseline) by [@song](https:\/\/www.kaggle.com\/anycode)  \nThank you!","dd97c588":"save memory ! Thank you [@Hyun woo kim](https:\/\/www.kaggle.com\/chocozzz)!  \nhttps:\/\/www.kaggle.com\/chocozzz\/how-to-save-time-and-memory-with-big-datasets"}}