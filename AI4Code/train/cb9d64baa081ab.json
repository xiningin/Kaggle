{"cell_type":{"be566502":"code","722ea237":"code","7c774a93":"code","e0e5594c":"code","ea0d4df0":"code","3287a374":"code","97e5b526":"code","fab1821f":"code","29ba2c09":"code","ec92258e":"code","973a7674":"code","dc8b0466":"code","05fdeb55":"code","772cba81":"code","a96d60f1":"code","99877b28":"code","1e942edb":"code","7904616f":"code","9a883eb6":"code","f85fdba7":"code","95b82ca3":"code","70103442":"markdown","8d7bc5db":"markdown","1a266653":"markdown","bd5e8577":"markdown","4b936204":"markdown","42b30a1b":"markdown","61a185c8":"markdown","9a27ec29":"markdown","d86e2573":"markdown","2f01e47e":"markdown"},"source":{"be566502":"#Load libraries and the dataset\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndf = pd.read_csv('..\/input\/insurance.csv')\ndf.head()","722ea237":"df.dtypes #check datatype","7c774a93":"df.mean()","e0e5594c":"df.corr()","ea0d4df0":"df.count()","3287a374":"df.max()","97e5b526":"df.std()","fab1821f":"df.isnull().sum(axis=0) #check null","29ba2c09":"#Crosstab - we can validate some basic hypothesis using PANDAS crosstab\npd.crosstab(df[\"sex\"],df[\"region\"],margins=True)","ec92258e":"def percConvert(ser):\n    return ser\/float(ser[-1])\npd.crosstab(df[\"sex\"],df[\"region\"],margins=True).apply(percConvert, axis=1)","973a7674":"df.loc[(df[\"sex\"]==\"male\") & (df[\"smoker\"]==\"yes\") & (df[\"region\"]==\"southwest\"), [\"sex\",\"smoker\",\"region\"]].head(10)","dc8b0466":"df_sorted = df.sort_values(['smoker','region'], ascending=False)\ndf_sorted[['smoker','region']].head(10)","05fdeb55":"# Pivot\nimpute_grps = df.pivot_table(values=[\"charges\"], index=[\"sex\",\"smoker\"], aggfunc=np.mean)\nprint (impute_grps)","772cba81":"df.boxplot(column=\"charges\",by=\"region\", figsize=(18, 8))","a96d60f1":"df.boxplot(column=\"charges\",by=\"smoker\", figsize=(18, 8))","99877b28":"df.boxplot(column=\"charges\",by=\"children\", figsize=(18, 8))","1e942edb":"df.hist(column=\"charges\",by=\"sex\",figsize=(18, 8), bins=30)","7904616f":"#Scatter \ndf.plot.scatter(x='charges', y='bmi', figsize=(18, 8))","9a883eb6":"# Area Plot\ndf_new = df.drop(columns = 'charges') #dropping charges for the plot\ndf_new.plot.area(figsize=(18, 8))","f85fdba7":"# Kernel Density Estimation plot (KDE)\ndf['charges'].plot(kind='kde')","95b82ca3":"from IPython.display import YouTubeVideo\nYouTubeVideo(\"B42n3Pc-N2A\")","70103442":"- <a href='#1'>1. Introduction<\/a>  \n- <a href='#2'>2. Data Cleaning<\/a>\n- <a href='#3'>3. Exploratory Data analysis<\/a>\n- <a href='#4'>4.  Data Visualization<\/a>\n\n\n\n\n\n# <a id='1'>1. Introduction<\/a>\n\n\nPANDAS stands for \u201cPython Data Analysis Library\u201d,  the name is derived from the term \u201cpanel data\u201d.  It is a super useful  open source, free to use library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n\nGreat thing about Pandas is that it takes data (CSV or a SQL database) and creates a Python object with rows and columns called data frame that looks very similar to table in a statistical software (Excel for example) \n\nObjective of this Kernel is to teach and refresh PANDAS  skills\n\n## Background\n\nPandas is the most popular Python library for doing data analysis. While it does offer quite a lot of functionality, it is also regarded as a fairly difficult library to learn well. Some reasons for this include:\n\n1. There are often multiple ways to complete common tasks\n2. There are over 240 DataFrame attributes and methods\n3. There are several methods that are aliases (reference the same exact underlying code) of each other\n4. There are several methods that have nearly identical functionality\n5. There are many tutorials written by different people that show different ways to do the same thing\n6. There is no official document with guidelines on how to idiomatically complete common tasks\n7. The official documentation, itself contains non-idiomatic code","8d7bc5db":"We have no null values in our dataset.\n\n# <a id='3'>3. Exploratory data analysis<\/a>\n\nIn this section, we will do a bit of exploratory data analysis (EDA) using PANDAS. \n\n### Crosstabs","1a266653":"###  Sorting dataframes\n\n Pandas allow easy sorting based on multiple columns. This can be done as:","bd5e8577":"# <a id='4'>4. Data Visualization<\/a>\n\nData visualization is a general term that describes any effort to help people understand the significance of data by placing it in a visual context. Patterns, trends and correlations that might go undetected in text-based data can be exposed and recognized easier with data visualization software.\n\nWe can use PANDAS and do data visualization. However, please note that there are better libraries like seaborn which are more appealing.\n\n","4b936204":"### Boolean Indexing\n\nWhat do you do, if you want to filter values of a column based on conditions from another set of columns? For instance, we want a list of all males who smoke and are from the region = 'southwest'.\n\nBoolean indexing can help here. You can use the following code:","42b30a1b":"Thanks for reading the Kernel. I will **continue updating** this. Please **leave a comment for any suggestions**. \n\nIf you have time, you can learn more in below YouTube video :) ","61a185c8":"![](http:\/\/i.imgur.com\/c7Kdkuu.png)\n","9a27ec29":"### Pivot Table\n\nPandas can be used to create MS Excel style pivot tables. For instance, in this case, a key column is \u201ccharges\u201d . As an example, we can pivot it using mean amount of each 'sex' and 'smoker' group. ","d86e2573":"**Columns from the dataset - **\n\n* age: age of primary beneficiary\n* sex: gender\n* bmi: Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg \/ m ^ 2) using the ratio of height to weight, ideally 18.5 to 24.9\n* children: Number of children covered by health insurance \/ Number of dependents\n* smoker: Smoking\n* region: the beneficiary's residential area in the US, northeast, southeast, southwest, northwest.\n* charges: Individual medical costs billed by health insurance\n\n\n# <a id='2'>2. Data Cleaning<\/a>\n\n\n\nData cleaning is one of most critical step in data analysis. It is the process of detecting and correcting (or removing) corrupt or inaccurate records from a record set, table, or database and refers to identifying incomplete, incorrect, inaccurate or irrelevant parts of the data and then replacing, modifying, or deleting the dirty or coarse data. \n\nHere are several key benefits that come out of the data cleaning process:\n\n* It removes major errors and inconsistencies that are inevitable when multiple sources of data are getting pulled into one dataset.\n* Using tools to cleanup data will make everyone more efficient since they\u2019ll be able to quickly get what they need from the data.\n* Fewer errors means happier customers and fewer frustrated employees.\n* The ability to map the different functions and what your data is intended to do and where it is coming from your data.\n\n**Inspecting the data**\n\nTo start data cleaning, take a look at the data. If it is a large dataset, take a look at the top 20 rows, the bottom 20 rows, and a 20 row random sample by inspecting the data. You can inspect the data using pandas as mentioned below: \n\n* df.mean() - returns the mean of all columns\n* df.corr() - returns the correlation between columns in a data frame\n* df.count() - returns the number of non-null values in each data frame column\n* df.max() - returns the highest value in each column\n* df.min() - returns the lowest value in each column\n* df.median() - returns the median of each column\n* df.std() - returns the standard deviation of each column\n\n\n**Missing values:** We always check for missing values in the data by running pd.isnull() which checks for null values, and returns a boolean array (an array of true for missing values and false for non-missing values). In order to get a sum of null\/missing values, run pd.isnull().sum(). pd.notnull() is the opposite of pd.isnull(). After you get a list of missing values you can get rid of them, or drop them by using df.dropna() to drop the rows or df.dropna(axis=1) to drop the columns. A different approach would be to fill the missing values with other values by using df.fillna(x) which fills the missing values with x (you can put there whatever you want) or s.fillna(s.mean()) to replace all null values with the mean (mean can be replaced with almost any function from the statistics section).\n\n**Replace values:** It is sometimes necessary to *replace values* with different values. For example, s.replace(1,'one') would replace all values equal to 1 with 'one'. It\u2019s possible to do it for multiple values: s.replace([1,3],['one','three']) would replace all 1 with 'one' and 3 with 'three'. You can also rename specific columns by running: df.rename(columns={'old_name': 'new_ name'}) or use df.set_index('column_one') to change the index of the data frame.\n\nNow, that we know some of the basic things PANDAS can do, let's apply the knowledge in our dataset df","2f01e47e":"As seen above, we can use Crosstab to validate some basic hypothesis. For instance, in this case,  we wanted to see the distribution of male vs female in different regions. These are absolute numbers. But, percentages can be more intuitive in making some quick insights. We can do this using the apply function as shown below:"}}