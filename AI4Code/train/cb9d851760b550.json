{"cell_type":{"d617a6a5":"code","fdc01823":"code","50de27b4":"code","47861ebd":"code","4b75b99d":"code","bfd23737":"code","b0bfa358":"code","9bdd9c99":"code","75ff2ca4":"code","efc5ae41":"markdown","9c3f9808":"markdown","6bca39ec":"markdown","f4689b39":"markdown","cc06a759":"markdown","7f70e7ea":"markdown","ae3ff9bd":"markdown","e694c506":"markdown"},"source":{"d617a6a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","fdc01823":"reviews = pd.read_csv(\"..\/input\/reviews_dec18.csv\", header = 0)\nprint(reviews.shape)","50de27b4":"import re\n\ndef clean_text(string_in):\n    string_in = re.sub(\"[^a-zA-Z]\", \" \", str(string_in))  # Replace all non-letters with spaces\n    string_in = string_in.lower()                         # Tranform to lower case    \n    \n    return string_in.strip()\n\nreviews[\"comments_cleaned\"] = reviews.comments.apply(clean_text)","47861ebd":"from nltk.tokenize import RegexpTokenizer\npreprocessed = [\" \".join(RegexpTokenizer(r'\\w+').tokenize(reviews.comments_cleaned[idx])) for idx in reviews.index]","4b75b99d":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction import text \n\ncustom_stop_words = []\nmy_stop_words = text.ENGLISH_STOP_WORDS.union(custom_stop_words)\n\nvectorizer = TfidfVectorizer(min_df = 1, ngram_range = (1,1), \n                             stop_words = my_stop_words)\n\ntfidf = vectorizer.fit_transform(preprocessed)\nprint(\"Created document-term matrix of size %d x %d\" % (tfidf.shape[0],tfidf.shape[1]))","bfd23737":"from sklearn import decomposition\nimport numpy as np\nnmf = decomposition.NMF(init = 'nndsvd', n_components = 5, max_iter = 200)\nW = nmf.fit_transform(tfidf)\nH = nmf.components_\nprint(\"Generated W(document-topic)) matrix of size %s and H (topic-word) matrix of size %s\" % ( str(W.shape), str(H.shape)))\n\nfeature_names = vectorizer.get_feature_names()\nn_top_words = 10\n\n# Print top words in each topic\nfor topic_idx, topic in enumerate(H):\n    print(\"Topic #%d:\" % topic_idx)\n    print(\" \".join([feature_names[i]\n                    for i in topic.argsort()[:-n_top_words - 1:-1]]))\n    print()","b0bfa358":"# Create pandas dataframe for the topics\nmydf = pd.DataFrame({\"feature_name\": feature_names})\n\nfor topic_idx, topic in enumerate(H):\n    mydf[\"topic_\" + str(topic_idx)] = topic\n\nmylist = list(mydf.itertuples())\n\nreviews_topic1 = []\nreviews_topic2 = []\nreviews_topic3 = []\nreviews_topic4 = []\nreviews_topic5 = []\n\nfor order_id, key, num1, num2, num3, num4, num5 in mylist:\n    reviews_topic1.append((key, num1))\n    reviews_topic2.append((key, num2))\n    reviews_topic3.append((key, num3))\n    reviews_topic4.append((key, num4))\n    reviews_topic5.append((key, num5))\n\nreviews_topic1 = sorted(reviews_topic1, key=lambda myword: myword[1], reverse=True)\nreviews_topic2 = sorted(reviews_topic2, key=lambda myword: myword[1], reverse=True)\nreviews_topic3 = sorted(reviews_topic3, key=lambda myword: myword[1], reverse=True)\nreviews_topic4 = sorted(reviews_topic4, key=lambda myword: myword[1], reverse=True)\nreviews_topic5 = sorted(reviews_topic5, key=lambda myword: myword[1], reverse=True)","9bdd9c99":"from wordcloud import WordCloud \nimport matplotlib.pyplot as plt\n\n%matplotlib inline\n\ndef draw_wordcloud(dict, topic_number):\n    wc = WordCloud(max_words=1000)    \n    wordcloud = WordCloud().generate_from_frequencies(dict)\n    \n    plt.title('Topic %s' %str(topic_number), size = 16)\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")        \n    plt.show()\n\ndraw_wordcloud(dict(reviews_topic1), topic_number=1)\ndraw_wordcloud(dict(reviews_topic2), topic_number=2)\ndraw_wordcloud(dict(reviews_topic3), topic_number=3)\ndraw_wordcloud(dict(reviews_topic4), topic_number=4)\ndraw_wordcloud(dict(reviews_topic5), topic_number=5)","75ff2ca4":"# Prediction example\ntext_new = preprocessed[0:5]\ntfidf_new = vectorizer.transform(text_new)\nDT_new = nmf.transform(tfidf_new)","efc5ae41":"Remove non-letters and change to lower case:","9c3f9808":"## Visualize topics with a word cloud\n\nTo better communicate the topics found, I used the [wordcloud](https:\/\/github.com\/amueller\/word_cloud) package.\n\nSeparate the topics into separate sorted lists:","6bca39ec":"Generate the word cloud for the 5 topics identified from NMF. Larger fonts indicate higher weights of the words in a topic and the colors are randomly assigned. Compared with a simple list, from a word cloud we can better understand the relative frequency of all the words:","f4689b39":"# NMF analysis\n\n\nIf we use **V<span>** to represent a **document-word** matrix with tf-idf values, non-negative matrix factorization factorizes the matrix into two matrices **W<span>** and **H<span>**, representing **document-topic** and **topic-word** matrices, respectively, as shown in the following figure:\n\n![NMF](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/f\/f9\/NMF.png)\n\nThe **document-topic** matrix or **W<span>** matrix allows us to compare texts and determine which ones are similar. The **topic-word** matrix, or **H<span>** matrix tells us the top words in any given topic and we can use visualization to better undertand the topics.\n\n\nNow I can use NMF on the derived tf-idf matrix. We specify the number of topics to be 5.","cc06a759":"## Identify topics\n\n### Calculate tf-idf matrix\n\nIn order to use non-negative matrix factorization, I first calculated the *tf-idf* (term frequency-inverse document frequency) matrix. The value of tf-idf reflects the number times a word appears in the *document* after adjusting for the frequency of the word in the *corpus*. \n\nWhen calculating the tf-idf matrix, I filtered out words like \"being\" and \"for\" which are called *stop words*. I used the stop words as defined by the scikit-learn package. I can expand the list of stop words by adding to the `custom_stop_words` variable.","7f70e7ea":"Tokenize the text data","ae3ff9bd":"## Making predictions\n\nFor any new reviews, I can calculate its tf-idf matrix and then calculate its **Document-Topic** matrix. This will then allow us to assign the new reviews to a segment and predict its sentiment (for example, positive sentiment vs. negative sentiment on a product).\n\nFor example: ","e694c506":"# Discover sentiment in Airbnb reviews\n\nSentiment analysis is commonly used in marketing and customer service to answer questions such as \"Is a customer review positive or negative?\" and \"How are customers responding to the service?\"\n\n*Topic modeling* discovers the abstract topics in a corpus of texts. The results from topic modeling analysis can then be used in sentiment analysis. For example, they can be used to split texts into different subsets and allow us to train a separate sentiment model for each of the subsets. Training separate sentiment models for different subsets can lead to more accurate predictions than using a single model for all the texts. \n\nThis notebook discovers and visualizes topics from Airbnb reviews."}}