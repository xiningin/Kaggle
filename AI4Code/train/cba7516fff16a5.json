{"cell_type":{"61e916cd":"code","a26116af":"code","c18325c2":"code","7e096347":"code","698ffea3":"code","e8d041d4":"code","54160bcf":"code","fee23060":"code","ccdafe69":"code","cea3f024":"code","54410ead":"code","bc8dba9d":"code","e4a25de3":"code","eff3990c":"code","fa7f2410":"code","5bd10e23":"code","0536773a":"code","2d6d44d7":"code","8af5d828":"code","0e124526":"code","d91ca2c3":"code","be94454f":"code","8acbc69b":"code","4c80ddb7":"code","fbd0041c":"code","ecda547f":"code","2c1e9b55":"code","03c5689b":"code","1aa98d4d":"code","71d8461f":"code","2e462805":"code","70a695a0":"code","94f45657":"code","4f9d209f":"code","b47e2cd0":"code","1ff1e6aa":"code","179f1592":"code","4b285c58":"code","64815aad":"code","bb34e883":"code","865d0987":"markdown","e3d23763":"markdown","a28b585c":"markdown","1bd8373a":"markdown","1445de50":"markdown","710326bb":"markdown","be9ee049":"markdown","06223d70":"markdown","855d6545":"markdown","e9e7b60e":"markdown"},"source":{"61e916cd":"import nltk\nimport pandas as pd\nimport re\nimport string\nimport seaborn as sns\nimport numpy as np","a26116af":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV\n\nfrom matplotlib import pyplot as plt","c18325c2":"from sklearn import set_config\nset_config(print_changed_only = False)\n\n%matplotlib inline","7e096347":"train_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntrain_df.head()","698ffea3":"test_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntest_df.head()","e8d041d4":"train_df.shape","54160bcf":"train_df.info()","fee23060":"train_df.isnull().sum()","ccdafe69":"hashtag = r\"#[a-zA-Z]\\w+\"\ntrain_df['hashtag_count'] = train_df['text'].apply(lambda txt: len(re.findall(hashtag, txt)))\ntrain_df.head()","cea3f024":"test_df['hashtag_count'] = test_df['text'].apply(lambda txt: len(re.findall(hashtag, txt)))\ntest_df.head()","54410ead":"train_df['hashtag_count'].describe()","bc8dba9d":"plt.figure(figsize=(12,6))\nsns.countplot(x= 'hashtag_count', hue='target', data = train_df)","e4a25de3":"train_df['keyword'].fillna(value = 'NaN', inplace = True)\ntest_df['keyword'].fillna(value = 'NaN', inplace = True)\n\nprint('Null value count for keywords in training dataset: ', train_df['keyword'].isnull().sum())\nprint('Null value count for keywords in testing dataset: ', test_df['keyword'].isnull().sum())","eff3990c":"kw_le = LabelEncoder()\nkw_le.fit(train_df['keyword'])","fa7f2410":"train_df['keyword'] = kw_le.transform(train_df['keyword'])\ntrain_df.head()","5bd10e23":"test_df['keyword'] = kw_le.transform(test_df['keyword'])\ntest_df.head()","0536773a":"train_df['text_len'] = train_df['text'].apply(lambda x: len(x) - x.count(\" \"))\ntrain_df.head()","2d6d44d7":"test_df['text_len'] = test_df['text'].apply(lambda x: len(x) - x.count(\" \"))\ntest_df.head()","8af5d828":"train_df['text_len'].describe()","0e124526":"bins = np.linspace(0, 150, 16)\n\nplt.figure(figsize = (12, 6))\nplt.hist(train_df[train_df['target'] == 1]['text_len'], bins, alpha = 0.5, density = True, label = 'Real')\nplt.hist(train_df[train_df['target'] == 0]['text_len'], bins, alpha = 0.5, density = True, label = 'Not Real')\nplt.legend()\nplt.show()","d91ca2c3":"wn = nltk.WordNetLemmatizer()\nstopwords = nltk.corpus.stopwords.words('english')","be94454f":"def tokenize_text(text):\n    text = text.lower()\n    text = \"\".join([word for word in text if word not in string.punctuation])\n    tokens = re.split('[^a-z]+', text)\n    return tokens\n\ndef clean_text(tokens):\n    text = [wn.lemmatize(word) for word in tokens if not (word in stopwords or str.isspace(word) or len(word)==0)]\n    return text ","8acbc69b":"train_df['text'] = train_df['text'].apply(lambda txt: tokenize_text(txt))\ntrain_df['word_count'] = train_df['text'].apply(lambda tokens: len(tokens))\ntrain_df.head()","4c80ddb7":"test_df['text'] = test_df['text'].apply(lambda txt: tokenize_text(txt))\ntest_df['word_count'] = test_df['text'].apply(lambda tokens: len(tokens))\ntest_df.head()","fbd0041c":"train_df['word_count'].describe()","ecda547f":"bins = np.linspace(0, 32, 17)\n\nplt.figure(figsize = (12, 6))\nplt.hist(train_df[train_df['target'] == 1]['word_count'], bins, alpha = 0.5, density = True, label = 'Real')\nplt.hist(train_df[train_df['target'] == 0]['word_count'], bins, alpha = 0.5, density = True, label = 'Not Real')\nplt.legend()\nplt.show()","2c1e9b55":"tfidf_vect = TfidfVectorizer(analyzer= clean_text)\ntfidf_vect_fit = tfidf_vect.fit(train_df['text'])","03c5689b":"tfidf_vect_columns = ['tf_' + colname for colname in tfidf_vect.get_feature_names()]\ntfidf_vect_columns[::1000]","1aa98d4d":"train_tf_df = pd.DataFrame(tfidf_vect_fit.transform(train_df['text']).toarray(), columns = tfidf_vect_columns)\ntrain_tf_df.head()","71d8461f":"test_tf_df = pd.DataFrame(tfidf_vect_fit.transform(test_df['text']).toarray(), columns = tfidf_vect_columns)\ntest_tf_df.head()","2e462805":"X_features = pd.concat([train_df[['keyword', 'hashtag_count', 'text_len', 'word_count']], train_tf_df], axis = 1)\nX_features.shape","70a695a0":"Y_label = train_df['target']\nY_label.shape","94f45657":"rf = RandomForestClassifier()\nrf","4f9d209f":"param = {'n_estimators': [50, 100, 150],\n        'max_depth': [30, 60, 90, None]}\n\nk_fold = KFold(n_splits = 5)\n\ngs = GridSearchCV(rf, param, cv = k_fold)\ngs_fit = gs.fit(X_features, Y_label)\npd.DataFrame(gs_fit.cv_results_).sort_values('mean_test_score', ascending = False).head()","b47e2cd0":"gs_fit.best_params_","1ff1e6aa":"X_test_features = pd.concat([test_df[['keyword', 'hashtag_count', 'text_len', 'word_count']], test_tf_df], axis = 1)\nX_test_features.shape","179f1592":"y_hat = gs_fit.predict(X_test_features)\ny_hat.shape","4b285c58":"test_df = pd.concat([test_df, pd.DataFrame(y_hat, columns=['target'])], axis = 1)\ntest_df.head()","64815aad":"submission_df = test_df[['id', 'target']]\nsubmission_df.head()","bb34e883":"submission_df.to_csv('disaster_tweet_nlp_mayur.csv', index=False)","865d0987":"### Hyperparameter tuning using GridSearchCV","e3d23763":"### Keywords","a28b585c":"## Feature Engineering","1bd8373a":"### TF-IDF","1445de50":"## Loading Data and EDA","710326bb":"### Length of text","be9ee049":"### Hashtag Count","06223d70":"## Random Forest Classifier","855d6545":"## Imports","e9e7b60e":"### Word count"}}