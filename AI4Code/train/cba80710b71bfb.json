{"cell_type":{"6b761646":"code","03a363d4":"code","712b67f0":"code","c05685a8":"code","d0579385":"code","2c042917":"code","0e96eef0":"code","d22ec023":"code","39cb9caf":"code","7aa442f2":"code","82753fa3":"code","4ec13da4":"code","6cdc424d":"code","3bef3bca":"code","e186aec1":"markdown","327860ff":"markdown","38c4efb0":"markdown","2c7457d6":"markdown","6e93bea9":"markdown","9e042da1":"markdown","597016aa":"markdown","bc5adca9":"markdown","6e2b9437":"markdown","4bddc9e2":"markdown","3e8cb864":"markdown","f07eb33a":"markdown"},"source":{"6b761646":"\n\n# Loading the required libraries \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport warnings\nimport itertools\nwarnings.filterwarnings('ignore')\nreview =pd.read_csv(\"..\/input\/Womens Clothing E-Commerce Reviews.csv\")","03a363d4":"review.head()","712b67f0":"text = review[['Review Text','Rating']]\ntext.shape","c05685a8":"text['Review Text'][0]\ntext[text['Review Text']==\"\"]=np.NaN\ntext['Review Text'].fillna(\"No Review\",inplace=True)","d0579385":"# Split into train and test data:\nsplit = np.random.randn(len(text)) <0.8\ntrain = text[split]\ntest = text[~split]\nprint(\"Total rows in train:\",len(train),\"and test:\",len(test))\nytrain=train['Rating']\nytest=test['Rating']","2c042917":"lens=train['Review Text'].str.len()\nprint(\"Mean Length:\",lens.mean(),\"Standard Deviation\",lens.std(),\"Maximum Length\",lens.max())\n\n","0e96eef0":"lens.hist()","d22ec023":"plt.figure(figsize=(8,8))\ntext['Length']=lens\nfx=sns.boxplot(x='Rating',y='Length',data=text)\nplt.title(\"Distribution of length with respect to rating\")\nplt.xlabel(\"Rating\")\nplt.ylabel(\"Length\")","39cb9caf":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import log_loss,confusion_matrix,classification_report, accuracy_score\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import RegexpTokenizer\nimport re\n","7aa442f2":"count_vect = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english',max_features=5000)\ncount_vect.fit(list(train['Review Text'].values.astype('U'))+list(test['Review Text'].values.astype('U')))\nxtrain=count_vect.transform(train['Review Text'].values.astype('U'))\nxtest=count_vect.transform(test['Review Text'].values.astype('U'))\n","82753fa3":"## Applying naive bayes:\n\nmodel = MultinomialNB()\nmodel.fit(xtrain, ytrain)\npredictions = model.predict(xtest)\n","4ec13da4":"### Lets check the accuracy score.\nprint(accuracy_score(ytest, predictions))","6cdc424d":"conf_matrix=confusion_matrix(ytest,predictions)","3bef3bca":"### Print confusion matrix:\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    print(cm)\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nplt.figure(figsize=(8,8))\nplot_confusion_matrix(conf_matrix, classes=['1', '2','3','4','5'],\n                      title='Confusion matrix')\nplt.show()","e186aec1":"### Metrics:","327860ff":"# E Commerce Reviews ","38c4efb0":"**Thanks for reading my kernel.**","2c7457d6":"The model has an accuracy of 62 %.","6e93bea9":"### Objective:","9e042da1":"### Inspiration:\n[Tutorials on Bag of words](https:\/\/www.kaggle.com\/rochachan\/bag-of-words-meets-bags-of-popcorn)\n\n[Abhishek's Kernel on NLP](https:\/\/www.kaggle.com\/abhishek\/approaching-almost-any-nlp-problem-on-kaggle)\n\n[Jeremy Howard's kernel on Naive Bayes](https:\/\/www.kaggle.com\/jhoward\/nb-svm-strong-linear-baseline)\n\n[sban's advanced kernel on LSTM](https:\/\/www.kaggle.com\/shivamb\/beginners-guide-to-text-generation-using-lstms)\n\nMany more kernels dealing with NLP.","597016aa":"This is my first extensive kernel dealing with an text classification problem.Therefore I have tried my hands on whatever i have learnt so far on NLP.I try to do a data analysis and visualisation before creating a simple naive bayes model to predict review scores using the reviews.Thanks for reading through.Also check out my other kernel on [whiskey classification using reviews](https:\/\/www.kaggle.com\/gsdeepakkumar\/classy-whisky-approach-through-nlp)","bc5adca9":"There seems to be a slight difference in the length of the reviews for different rating.","6e2b9437":"Now we train naive bayes model on the data and look at the log loss value.","4bddc9e2":"We will now convert the text files into numerical vectors through **Bag of words** model.For this , first we will clean the reviews - remove stopwords as a baseline model.We can also look at removing punctuations,numbers.","3e8cb864":"### Examine the length of the comments:\n","f07eb33a":"We find that the length of the text varies.Let us see how the length is distributed for every rating ."}}