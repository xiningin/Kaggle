{"cell_type":{"9288e745":"code","78883334":"code","f7a56258":"code","ba327005":"code","4dfae6ba":"code","c7777cd1":"code","510877b1":"code","aaa14df3":"code","f4a2f1fe":"code","29b42541":"code","4a8a747a":"code","a5d79b86":"code","570bfac0":"code","a327b645":"code","1dc895f7":"code","ec633098":"code","17811588":"code","cb9a56ce":"code","ff3420e1":"code","80430447":"code","0034d7c4":"code","4bb26625":"code","52c8e5c5":"code","cf9f598d":"code","cbd45607":"code","7784403e":"code","7ce12401":"code","976bb3f9":"code","ee6e4d90":"code","94a4cbdb":"code","d0220064":"code","3dacfda0":"code","0c5de892":"code","4daf985a":"code","24faafbd":"code","614f3dfe":"markdown","6900c491":"markdown","20e6e3b3":"markdown","6b9fd3f7":"markdown","79434896":"markdown","acac625e":"markdown","7dff5df1":"markdown","fd382f31":"markdown","2945c4ea":"markdown","5c2bef67":"markdown","db07d63c":"markdown","0eaea0f3":"markdown","e0a6a2ae":"markdown","80ce5fc8":"markdown","69e1d059":"markdown","ac76f4dd":"markdown","1bab063f":"markdown","39c03acf":"markdown","b7ca799d":"markdown"},"source":{"9288e745":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# plotly\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","78883334":"data = pd.read_csv(\"..\/input\/Skyserver_SQL2_27_2018 6_51_39 PM.csv\")","f7a56258":"data.head()","ba327005":"data.info()","4dfae6ba":"data.columns","c7777cd1":"data.describe()","510877b1":"# We don't need objid, specobjid, rerun for classification\ndata.drop([\"objid\", \"specobjid\", \"rerun\"], axis = 1, inplace = True)","aaa14df3":"# QSO data deleting for binary classification. We just need star and galacy classes\ndata = data[data[\"class\"] != \"QSO\"]","f4a2f1fe":"sns.countplot(x= \"class\", data = data)\ndata[\"class\"].value_counts()","29b42541":"sns.pairplot(data.loc[:,[\"u\", \"g\", \"r\", \"i\", \"z\", \"class\"]], hue = \"class\")\nplt.show()","4a8a747a":"# Galaxy = 1 and Star = 0\ndata['class_binary'] = [1 if i == 'GALAXY' else 0 for i in data.loc[:,'class']]","a5d79b86":"# Convert STAR and GALAXY classes to int. For binary classification\ndata[\"class\"] = [1 if each == \"GALAXY\" else 0 for each in data[\"class\"]] \n# After converting operation. We call Star as 0 and Galaxy as 1","570bfac0":"# data after preparation - formatting operations\ndata.head()","a327b645":"# value selection and normalization\ny = data[\"class\"].values\nx_data = data.drop([\"class\"], axis = 1)\nx = (x_data - np.min(x_data))\/(np.max(x_data) - np.min(x_data))","1dc895f7":"# after normalization\nx.head()","ec633098":"# data separation for train - test operations\nfrom sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42)","17811588":"# Dictionary for score results of classification models\nalgorithmPerfomanceDict = {}\n#algorithmPerfomanceDict = {'ClassificationModel': 1, 'Accuracy': 2}","cb9a56ce":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train, y_train)\nlogisticRegressionScore = lr.score(x_test, y_test)\nprint(\"Score of Logistic Regression : {0}\".format(logisticRegressionScore))\nalgorithmPerfomanceDict['LogisticRegression'] = logisticRegressionScore","ff3420e1":"# ROC Curve with logistic regression\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nx,y = data.loc[:,(data.columns != 'class') & (data.columns != 'class_binary')], data.loc[:,'class_binary']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)\nlogreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred_prob = logreg.predict_proba(x_test)[:,1]\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n\n# Plot ROC curve\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr, tpr)\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC')\nplt.show()","80430447":"data.drop([\"class_binary\"], axis = 1, inplace = True)","0034d7c4":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 1) # Number of neighbors to consider.\nknn.fit(x_train, y_train)\nknnScore = knn.score(x_test, y_test)\nprint(\"Score of KNN Regression : {0}\".format(knnScore))\nalgorithmPerfomanceDict['KNeighborsClassifier'] = knnScore","4bb26625":"#Lets find best K value\nscoreList = []\nfor each in range(1, 20):\n    optimumKnn = KNeighborsClassifier(n_neighbors = each)\n    optimumKnn.fit(x_train, y_train)\n    scoreList.append(optimumKnn.score(x_test, y_test))\n    \nplt.plot(range(1, 20), scoreList)\nplt.xlabel(\"K value\")\nplt.ylabel(\"Score - Accuracy\")\nplt.show();","52c8e5c5":"from sklearn.model_selection import GridSearchCV\ngrid = {'n_neighbors': np.arange(1, 50)}\nknn = KNeighborsClassifier()\nknn_cv = GridSearchCV(knn, grid, cv = 3)\nknn_cv.fit(x, y)\nprint(\"Tuned hyperparameter k: {}\".format(knn_cv.best_params_)) \nprint(\"Best score: {}\".format(knn_cv.best_score_))","cf9f598d":"from sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LinearRegression\nreg = LinearRegression()\nk = 5\ncv_result = cross_val_score(reg, x, y, cv = k) # uses R^2 as score \nprint('CV Scores : ',cv_result)\nprint('CV Average Score : ',np.sum(cv_result) \/ k)","cbd45607":"# According to plot best value for KNN algorithm is 1. It has highest accuracy percentage.","7784403e":"from sklearn.svm import SVC\nsvm = SVC(random_state = 42)\nsvm.fit(x_train, y_train)\nsvmScore = svm.score(x_test, y_test)\nprint(\"Accuracy of Support Vector Machine is : \", svmScore)\nalgorithmPerfomanceDict['SVM'] = svmScore","7ce12401":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(x_train, y_train)\nnb.score(x_test, y_test)\nnaiveBayesScore = nb.score(x_test, y_test)\nprint(\"Accuracy of Naive Bayes Classifier is : \", naiveBayesScore)\nalgorithmPerfomanceDict['NaiveBayesClassifier'] = naiveBayesScore","976bb3f9":"from sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train, y_train)\ndecisionTreeScore = dt.score(x_test, y_test)\nprint(\"Accuracy of Decision Tree Classifier is : \", decisionTreeScore)\nalgorithmPerfomanceDict['DecisionTreeClassifier'] = decisionTreeScore","ee6e4d90":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 100, random_state = 42) #Number of trees in forest.\nrf.fit(x_train, y_train)\nrandomForestScore = rf.score(x_test, y_test)\nprint(\"Accuracy of Random Forest Classifier is : \", randomForestScore)\nalgorithmPerfomanceDict['RandomForestClassifier'] = randomForestScore","94a4cbdb":"algorithmPerfomanceDict","d0220064":"comparisonData = pd.DataFrame.from_dict(algorithmPerfomanceDict, orient = 'index', columns = [\"Accuracy\"])\ncomparisonData.head(10)","3dacfda0":"plt.figure(figsize = (20, 7))\nsns.barplot(x = comparisonData.index, y = comparisonData.Accuracy)\nplt.ylabel('Accuracy')\nplt.xlabel('Classification Model')\nplt.title('Accuracy Values of Classification Models', color = 'blue', fontsize = 15)\nplt.show()","0c5de892":"from sklearn.metrics import confusion_matrix\ny_pred = rf.predict(x_test)\ny_actual = y_test\ncm = confusion_matrix(y_actual, y_pred)","4daf985a":"f, ax = plt.subplots(figsize = (8, 8))\nsns.heatmap(cm, annot = True, linewidths = 0.5, linecolor = \"red\", fmt = \".0f\", ax = ax)\nplt.xlabel(\"y_pred -> STAR = 0, GALAXY = 1\")\nplt.ylabel(\"y_actual -> STAR = 0, GALAXY = 1\")\nplt.show()","24faafbd":"from sklearn.metrics import classification_report\ny_pred = rf.predict(x_test)\ncm = confusion_matrix(y_test,y_pred)\nprint('Confusion matrix: \\n',cm)\nprint('Classification report: \\n',classification_report(y_test, y_pred))","614f3dfe":"<a id=\"11\"><\/a> \n***Cross Validation***\n* Classification algorithms can produce different results at the different random state. Below is the cross validation measures its R^2 score and gives us an average score.","6900c491":"<a id=\"12\"><\/a> \n**Hyperparameter tuning**\n* Another way to find best k value","20e6e3b3":"**Confusion Matrix For Random Forest Classification**","6b9fd3f7":"<a id=\"1\"><\/a> \n**Logistic Regression**<br\/>\n* Binary Classification Model (0-1)\n* It has two results 0 and 1.\n* Simplest neural network.\n* Weights ara calculated with forward and backward propogation processes.\n","79434896":"<a id=\"3\"><\/a> \n***SVM (Support Vector Machines)***<br\/>\nIt finds best decision boundry between given training data. A support vector machine makes the margin value max. Margin is the distance of two lines which divide the data. The dashed line of below chart is SVM.\n\n![SVM.png](http:\/\/i65.tinypic.com\/2qn0dc9.png)\n","acac625e":"According to [Comparison of Classifiers](#8) section DecisionTreeClassifier is the best option for this test data.  \n\n*If you have a suggestion, I'd be happy to read it.*","7dff5df1":"<a id=\"6\"><\/a> \n***Random Forest Classification***<br\/>\n\nRandom Forest Classifier is an ensamble learning model.It creates decision tree sets from randomly selected subsets of train dataset and it determines results of the decision trees for the final prediction of test object.\n\n![RandomForest.PNG](http:\/\/i66.tinypic.com\/29zd9n4.png)","fd382f31":"<a id=\"5\"><\/a> \n***Decision Tree Classification***<br\/>\n\nDecision Tree splits data for classification. This process is performed until each class becomes pure. Pure means each class has own area that contains onlu 1 type of class.\n\n![DecisionTree.PNG](http:\/\/i65.tinypic.com\/2jbs29j.png)","2945c4ea":"<a id=\"4\"><\/a> \n***Naive Bayes Classification***<br\/>\n\nIt calculates probability of factors and create similarity ranges than selects the result with the highest probability. \n\n![NaiveBayes.PNG](http:\/\/i64.tinypic.com\/xc68oj.png)\n\n**P(A|B)** : Probability of A given B<br\/>\n**P(B|A)** : Probability of B given A<br\/>\n**P(A)** : Probability of A<br\/>\n**P(B)** : Probability of B<br\/>\n\n","5c2bef67":"![BinaryClassification.png](http:\/\/i65.tinypic.com\/28moies.png)","db07d63c":"<a id=\"7\"><\/a> \n***Data Exploration (EDA)***\n* Data Preparation and Formatting","0eaea0f3":"<a id=\"8\"><\/a> \n***Comparison of Classifiers***","e0a6a2ae":"<a id=\"9\"><\/a> \n***Confusion Matrix - Classification of Model Evaluation***<br\/>\n\nConfusion matrix shows us performance of a classification model. That means details of accuracy. For Example : \n\n![confusionMatrix.PNG](http:\/\/i68.tinypic.com\/23tqrlg.png)\n\n**True Positive** : We predicted stars and we have 1248 stars<br\/>\n**True Negative** : We predicted galaxies and we have 1489 galaxies<br\/>\n**False Positive** : We predicted star, but it is not a star. It is a galaxy (Also known as a \"Type I error.)<br\/>\n**False Negative** : We predicted galaxy, but it is not a galaxy. It is a star (Also known as a \"Type II error.)<br\/>\n**Precision** = tp \/ (tp+fp))<br\/>\n**Recall** = tp \/ (tp+fn))<br\/>\n**f1** = 2 precision recall \/ ( precision + recall))<br\/>","80ce5fc8":"<a id=\"10\"><\/a> \n# Conclusion","69e1d059":" # Star - Galaxy Classification\n \n We are going to classify \"SkyServer\" data for separation of star or galaxy. For this classification we'll use different classification algorithms for performance measurement of prediction percentage. The following algorithms to be used are :\n \n* [Data Exploration (EDA)](#7)\n    * [Data Visualization](#13)\n* [Logistic Regression](#1)\n* [KNN (K-Nearest Neighbors Algorithm)](#2)\n    * [Hyperparameter tuning](#12)\n* [Cross Validation](#11)\n* [SVN (Support Vector Machines)](#3)\n* [Naive Bayes Classification](#4)\n* [Decision Tree Classification](#5)\n* [Random Forest Classification](#6)\n* [Confusion Matrix - Classification of Model Evaluation](#9)<br\/>\n    * In addition to these, the prediction results of above algorithms will be evaluate by using confusion matrix.\n* [Conclusion](#10)","ac76f4dd":"We have 4998 Galaxies and 4152 stars. Lets look their astronomic magnitude values by classes.","1bab063f":"<a id=\"2\"><\/a> \n***KNN (K-Nearest Neighbors Algorithm)***<br\/>\n* An object is classified by a majority vote of its neighbors. \n* Choose s \"k\" value.\n* Find the closest point to \"k\" value.  Euclidean distance is used to find the closest point to k.\n\n![EuclideanDistance.png](http:\/\/i67.tinypic.com\/sv1mcj.png)","39c03acf":"<a id=\"13\"><\/a> \n**Data Visualization**\n\n* u = better of DeV\/Exp magnitude fit\n* g = better of DeV\/Exp magnitude fit\n* r = better of DeV\/Exp magnitude fit\n* i = better of DeV\/Exp magnitude fit\n* z = better of DeV\/Exp magnitude fit\n\nThe Thuan-Gunn astronomic magnitude system. u, g, r, i, z represent the response of the 5 bands of the telescope.","b7ca799d":"* Calculate the neighbor class count\n* Calculate mojority vote of its neighbors.\n\n![KNN_NearestNeighbor.png](http:\/\/i68.tinypic.com\/2itllk6.png)"}}