{"cell_type":{"b4e3abce":"code","5d4bc4ee":"code","f4b19334":"code","b48fb848":"code","2b97d78a":"code","019d8027":"code","7a38beb7":"code","d0c7c2e3":"code","dfd4a917":"code","ee68a18f":"code","3be66182":"code","b91b35db":"code","9f135cc9":"code","0a7b2e15":"code","b0f29e7c":"code","f73d155d":"code","50552d2f":"code","7ded559c":"code","f12d6249":"code","4fb6793a":"code","4164ed19":"code","87f3c46d":"code","315a2b41":"code","4847ad67":"code","d51e0bfb":"code","60e8e13f":"code","c8c92b23":"code","749f9949":"code","9c36c8ab":"code","45822d64":"code","dbc0e688":"code","0303a01f":"code","b4f972be":"code","f5df649f":"code","23885031":"code","aed7ea79":"code","c79a51ca":"code","c67e9fef":"code","838418f5":"code","348dbc97":"code","548972bc":"code","4b1c626d":"code","8df0ac87":"code","41772d69":"code","92f8044f":"code","f7d73053":"code","9faa4a89":"code","5cf99437":"code","6b32d4fc":"code","ed1eb6d3":"code","cf38da95":"code","1748eab8":"code","dad3cca8":"code","6f38c74c":"code","d38edde6":"code","75881e4f":"markdown","e60dccec":"markdown","b5cf056e":"markdown","41d902fa":"markdown","d252b816":"markdown","2d854f69":"markdown","43f0e665":"markdown","b60e8826":"markdown","8a3e7d83":"markdown","cc851a7e":"markdown","ec44ab94":"markdown","0f548758":"markdown","adeebe3b":"markdown","48b9a793":"markdown","accef33e":"markdown","a1b32872":"markdown","09bde65b":"markdown","77e13d22":"markdown","7b10efc0":"markdown","8eb6d0e4":"markdown","0e79301c":"markdown","43c24def":"markdown","a3b1b68f":"markdown","84a2781c":"markdown","a58bb420":"markdown","fc03f3e4":"markdown","31843838":"markdown","88141b2b":"markdown","d325251c":"markdown","56283974":"markdown","29f41bfd":"markdown","acd4a5b6":"markdown","4b9200f8":"markdown","30b5c0a5":"markdown","d824c794":"markdown","49b7143c":"markdown","50ea53a9":"markdown","0222dc35":"markdown","6b5fe7bc":"markdown","688cc1a8":"markdown","83730869":"markdown","48950296":"markdown","40823bea":"markdown","a532a3bc":"markdown","894a2234":"markdown","0eec6edd":"markdown","0ca25937":"markdown","bbb6a040":"markdown","fa9f7602":"markdown"},"source":{"b4e3abce":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nimport warnings\nwarnings.filterwarnings('ignore')","5d4bc4ee":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","f4b19334":"train.head()","b48fb848":"plt.subplots(figsize=(8, 6))\nsns.distplot(train['SalePrice'], kde = False, fit = stats.norm)","2b97d78a":"prob = stats.probplot(train['SalePrice'], plot=plt)","019d8027":"print('Skewness: %f' % train['SalePrice'].skew())\nprint('Kurtosis: %f' % train['SalePrice'].kurt())","7a38beb7":"train['SalePrice'] = np.log1p(train['SalePrice'])","d0c7c2e3":"plt.subplots(figsize=(8, 4))\nsns.distplot(train['SalePrice'], kde = False, fit = stats.norm)\nplt.figure()\nprob = stats.probplot(train['SalePrice'], plot=plt)","dfd4a917":"print('Skewness: %f' % train['SalePrice'].skew())\nprint('Kurtosis: %f' % train['SalePrice'].kurt())","ee68a18f":"corrmat = train.corr(method='spearman')\nplt.subplots(figsize=(12, 9))\n\nk = 15 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 8}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","3be66182":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'YearBuilt', 'GarageArea', 'FullBath', 'TotalBsmtSF']\nsns.pairplot(train[cols], size = 2.5)\nplt.show();","b91b35db":"data = pd.concat([train['SalePrice'], train['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x='OverallQual', y=\"SalePrice\", data=data)","9f135cc9":"data = pd.concat([train['SalePrice'], train['GarageCars']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x='GarageCars', y=\"SalePrice\", data=data)","0a7b2e15":"data = pd.concat([train['SalePrice'], train['FullBath']], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x='FullBath', y=\"SalePrice\", data=data)","b0f29e7c":"all_data = pd.concat((train.iloc[:, 1:-1], test.iloc[:, 1:]))","f73d155d":"missing = all_data.isnull().sum()\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar()\nmissing_data = pd.DataFrame({'Total': missing})\nmissing_data.sort_values(by='Total',ascending=False)","50552d2f":"all_data.shape","7ded559c":"all_data = all_data.drop((missing_data[missing_data['Total'] > 100]).index,1)","f12d6249":"from sklearn.preprocessing import OneHotEncoder\nall_data = pd.get_dummies(all_data)","4fb6793a":"all_data = all_data.fillna(all_data.mean())","4164ed19":"all_data.shape","87f3c46d":"all_data.isnull().sum().max()","315a2b41":"quan_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = all_data[quan_feats].apply(lambda x: stats.skew(x))\nskewed_feats = skewed_feats[skewed_feats > 0.5].index\n\nall_data[skewed_feats] = np.log1p(all_data[skewed_feats])","4847ad67":"X_train = all_data[:train.shape[0]]\nX_test = all_data[train.shape[0]:]\ny = train['SalePrice']","d51e0bfb":"from sklearn.linear_model import Ridge, Lasso\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import RobustScaler\n\n# additionally I produce robust scaling to increase model accuracy\nscaler = RobustScaler().fit_transform(X_train) \n\n#creating the cross validation function for ridge and lasso\ndef rmse_cv(model):\n    rmse= np.sqrt(-cross_val_score(model, scaler, y, scoring=\"neg_mean_squared_error\", cv = 5))\n    return(rmse)","60e8e13f":"alphas_r = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\n\nval_errors_r = []\nfor alpha in alphas_r:\n    ridge = Ridge(alpha = alpha)\n    errors_r = rmse_cv(ridge).mean()\n    val_errors_r.append(errors_r)\n","c8c92b23":"plt.plot(alphas_r, val_errors_r)\nplt.title('Ridge')\nplt.xlabel('lambda')\nplt.ylabel('rmse')","749f9949":"print('best alpha: {}'.format(alphas_r[np.argmin(val_errors_r)]))\nprint('Min RMSE: {}'.format(min(val_errors_r)))","9c36c8ab":"alphas_l = [0.00005, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005,\n           0.0006, 0.0007, 0.0008, 1e-3, 5e-3]\n\nval_errors_l = []\nfor alpha in alphas_l:\n    lasso = Lasso(alpha = alpha)\n    errors_l = rmse_cv(lasso).mean()\n    val_errors_l.append(errors_l)","45822d64":"plt.plot(alphas_l, val_errors_l)\nplt.title('Lasso')\nplt.xlabel('alpha')\nplt.ylabel('rmse')","dbc0e688":"print('best alpha: {}'.format(alphas_l[np.argmin(val_errors_l)]))\nprint('Min RMSE: {}'.format(min(val_errors_l)))","0303a01f":"import xgboost as xgb","b4f972be":"#Create a train and test matrix for xgb\ndtrain = xgb.DMatrix(data = X_train, label = y)\ndtest = xgb.DMatrix(X_test)","f5df649f":"untuned_params = {'objective':'reg:linear'}\nuntuned_cv = xgb.cv(dtrain = dtrain, params = untuned_params, nfold = 4, metrics='rmse', as_pandas=True, seed = 5)","23885031":"print('Untuned rmse: %f' % (untuned_cv[\"test-rmse-mean\"].tail(1).values[0]))","aed7ea79":"gbm_param_grid = {\n    'colsample_bytree': [0.3],\n#    'subsample': [0.3,0.5, 0.7, 1],\n    'n_estimators': [400, 450, 500],\n    'max_depth': [3],\n    'learning_rate' : [0.1]\n}","c79a51ca":"gbm = xgb.XGBRegressor()","c67e9fef":"from sklearn.model_selection import GridSearchCV\ngrid_mse = GridSearchCV(param_grid = gbm_param_grid, estimator = gbm, scoring=\"neg_mean_squared_error\", cv = 4)","838418f5":"grid_mse.fit(X_train,y)","348dbc97":"print(\"Best parameters found: \", grid_mse.best_params_)\nprint(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))","548972bc":"tuned_params = {'objective':'reg:linear', 'n_estimators': 450, 'learning_rate': 0.1, 'max_depth': 3, 'colsample_bytree': 0.3, 'subsample': 1}\ntuned_cv = xgb.cv(dtrain = dtrain, params = tuned_params, nfold = 4, num_boost_round = 500, metrics='rmse', as_pandas=True, seed = 5)","4b1c626d":"print('Tuned rmse: %f' % (tuned_cv[\"test-rmse-mean\"].tail(1).values[0]))","8df0ac87":"l2_params = [0.05, 0.1, 0.3, 1, 3, 5, 10, 15, 30, 50, 75]\ntuned_params = {'objective':'reg:linear', 'n_estimators': 450, 'learning_rate': 0.1, 'max_depth': 3, 'colsample_bytree': 0.3, 'subsample': 1}","41772d69":"rmses_l2 = []","92f8044f":"for reg in l2_params:\n    tuned_params['lambda'] = reg\n    cv_results_rmse = xgb.cv(tuned_params,dtrain, num_boost_round=500, early_stopping_rounds=100, nfold=4, metrics ='rmse', as_pandas=True, seed =123)\n    rmses_l2.append(cv_results_rmse['test-rmse-mean'].tail(1).values[0])","f7d73053":"print(\"Best rmse as a function of l2:\")\nprint(pd.DataFrame(list(zip(l2_params, rmses_l2)), columns=[\"l2\", \"rmse\"]), '\\n')\nprint('Min L2 Tuned rmse: %f' % (min(rmses_l2)))\nprint('Min lambda: %f' % (min(l2_params)))","9faa4a89":"l1_params = [0.00005, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008, 1e-3, 5e-3]\ntuned_params = {'objective':'reg:linear', 'n_estimators': 450, 'learning_rate': 0.1, 'max_depth': 3, 'colsample_bytree': 0.3, 'subsample': 1}","5cf99437":"rmses_l1 = []","6b32d4fc":"for reg in l1_params:\n    tuned_params['alpha'] = reg\n    cv_results_rmse = xgb.cv(tuned_params,dtrain, num_boost_round=500, early_stopping_rounds=100, nfold=4, metrics ='rmse', as_pandas=True, seed =123)\n    rmses_l1.append(cv_results_rmse['test-rmse-mean'].tail(1).values[0])","ed1eb6d3":"print(\"Best rmse as a function of l2:\")\nprint(pd.DataFrame(list(zip(l1_params, rmses_l1)), columns=[\"l1\", \"rmse\"]), '\\n')\nprint('Min L1 Tuned rmse: %f' % (min(rmses_l1)))\nprint('Min alpha: %f' % (min(l1_params)))","cf38da95":"model_xgb = xgb.XGBRegressor(objective= 'reg:linear',reg_alpha = 0.00005, n_estimators=500, learning_rate=0.1, max_depth=3, colsample_bytree=0.3, subsample=1) \nmodel_xgb.fit(X_train, y)","1748eab8":"xgb_preds = np.expm1(model_xgb.predict(X_test))","dad3cca8":"lasso.fit(X_train, y)\nlass_pred = np.expm1(lasso.predict(X_test))","6f38c74c":"preds = xgb_preds","d38edde6":"solution = pd.DataFrame({\"id\":test.Id, \"SalePrice\":preds})\nsolution.to_csv(\"sol5.csv\", index = False)","75881e4f":"# Grid Search","e60dccec":"Coding quality variables","b5cf056e":"# XGboost tuned with L1","41d902fa":"Counting Total Missing Values","d252b816":"Building xgb with tuning parameters","2d854f69":"First we use xgb without tuning parameters and calculate the rmse using cross validation","43f0e665":"# Preprocessing","b60e8826":"As can be seen in the following graphs, after the conversion, the data distribution is close to normal.","8a3e7d83":"Since variables containing missing data more than 100 do not have a strong effect on the target variable or have highly correlated variables without missing data, I consider it possible to delete these variables with missing values. In other cases, apply averaging.","cc851a7e":"Perform selection on training and test set","ec44ab94":"Numerical measurement of data deviation from the normal distribution. With normal data distribution Skewness is 0, Kurtosis is = 3.","0f548758":"Next, I construct a correlation matrix. The spearman method was used because spearman's coefficient looks at the relative order of values for each variable. This makes it appropriate to use with both continuous and discrete data.","adeebe3b":"## Lasso Regularization","48b9a793":" The lasso performs even better.","accef33e":"# XGboost","a1b32872":"implement the ridge with different lambda and find rmse with cross validation","09bde65b":"I also build a boxplot for some feature.","77e13d22":"Compound train and test set to find missing data with the exception of the Id and SalePrice fields","7b10efc0":"Although it seemed that obvious outliers were identified, in practice their exclusion leads to an increase in the rmse model. Therefore, it was decided not to exclude them.","8eb6d0e4":"This confirms the probability plot, which shows that the SalePrice has a'peakedness', a positive skewness and does not follow the diagonal line.","0e79301c":"# Visualization ","43c24def":"# XGboost tuned with L2","a3b1b68f":"Here you can see that there seems to be a connection between the data. Since as the feature increases, SalePrice also increases, this is especially noticeable by OverallQual and GrLivArea. You may also notice observations that may be outliers on the graphs GrLivArea and GarageArea. Further attention should be paid to this.","84a2781c":"I will do a slightly different approach here and use the built in Lasso to figure out the best alpha.","a58bb420":"## Outliers","fc03f3e4":"## Normal distribution","31843838":"Use Grid Search to find the optimal parameters. Since Grid Search requires more computer resources, the selection of parameters was performed by gbm_param_grid was implemented several times with fewer parameters.","88141b2b":"rmse is better than the evaluation of the untuned model","d325251c":"# Tuned Model","56283974":"Boxplot also indicates the relationship of independent variables with target variable.","29f41bfd":"Also we can use lasso regression for prediction","acd4a5b6":"# Normalization","4b9200f8":"# Prediction","30b5c0a5":"You can use the average of the prediction of the two models, but unfortunately the improvement in the accuracy of the model in this case did not affect. So i used only xgb model for submission.","d824c794":"# Untuned Model","49b7143c":"# Modeling","50ea53a9":"The graph shows that rmse decreases first, but then with increasing lambda the error increases as well. The larger the lambda, the less model prones to overfit, but this reduces the model's ability to generalize the data","0222dc35":"With the help of the matrix you can see the most important features.For SalePrice this is 'OverallQual', 'GrLivArea', 'GarageCars', 'YearBuilt', 'GarageArea', 'FullBath', 'TotalBsmtSF'. Consider them in more detail.","6b5fe7bc":"## Ridge Regularization","688cc1a8":"At the end, the following parameters were obtained 'n_estimators': 450, 'learning_rate': 0.1, 'max_depth': 3, 'colsample_bytree': 0.3, 'subsample': 1","83730869":"compute logarithm of quantitative independent variables in which Skewness is greater than 0.5. In order to achieve a normal distribution of data, as in the case of SalePrice","48950296":"## Missing data","40823bea":"## Correlation matrix","a532a3bc":"For the prediction, we will use the XGboost model with L1 regularization, because this variant has the lowest estimate rmse","894a2234":"Apply averaging","0eec6edd":"further regularization L2 and L1 were made on tuned model","0ca25937":"Normal data distribution is important because several statistics tests rely on this (e.g. t-statistics) and it helps to improve model accuracy. Therefore, you should convert the data. This can be done using the logarithmic function.","bbb6a040":"Consider the independent variable SalePrice. The graph below shows that the distribution of data differs from the normal distribution.","fa9f7602":"Check for missing values"}}