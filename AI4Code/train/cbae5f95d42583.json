{"cell_type":{"ad5641ac":"code","b13ecaaa":"code","4985a164":"code","7779cbcb":"code","5c64eaa9":"code","bd1dc59b":"code","10262972":"code","136d227c":"code","0bfb0781":"code","96f9e544":"code","dca26294":"code","9d40010a":"code","a343a86c":"code","c5c7ba8a":"code","524406ff":"code","6cf05d70":"code","17d7f29f":"code","b451a13c":"code","8542facf":"code","f3da5997":"code","e4259a0d":"code","07ad83f4":"code","9eef9201":"code","9a092106":"code","75aa99db":"code","8d03213d":"code","719f95d5":"code","c72a7f47":"code","84399fca":"code","b7657ee9":"code","acaea9e3":"code","1e4435fb":"code","66a4e4a1":"code","9ff91d24":"code","5a1af69a":"code","3c97ca60":"code","00666157":"code","ea8ce444":"code","fcce6331":"code","ea73c25d":"code","809c187a":"code","b1b300ca":"code","4c5b8a59":"code","0bab6dbb":"code","2481a252":"code","6eb6f589":"code","42290ae8":"code","b8e3992f":"code","37e19298":"code","08e15261":"code","631498e0":"code","b75408f7":"code","57c77547":"markdown","bdbd9429":"markdown","8631dbcb":"markdown","44a180b1":"markdown","2c838914":"markdown","1bcf9647":"markdown","31e593e8":"markdown","7ff131b7":"markdown","c2d5cd52":"markdown","4376c974":"markdown","77638351":"markdown","a9ffa889":"markdown","06cd9bc5":"markdown","7c7ca92d":"markdown","a994298c":"markdown","e7d2ee5d":"markdown","6d55bae0":"markdown","392001d5":"markdown","cc5d7d34":"markdown","d259920b":"markdown","eb9aaa88":"markdown","3c04c714":"markdown","fe7a24d4":"markdown","932c761d":"markdown","d28cfb09":"markdown","e218fb7e":"markdown","148d6062":"markdown","08e97800":"markdown","2fe89605":"markdown","5315b109":"markdown","358f2baa":"markdown","6a7966ff":"markdown","46212a40":"markdown","3a954977":"markdown","b4b59e85":"markdown","4433c56a":"markdown","919b61f1":"markdown","e548949e":"markdown","e536d23f":"markdown","d7d0d9b4":"markdown","e4f41133":"markdown","ae1624a1":"markdown","5ab692d0":"markdown","d2a6f389":"markdown","a90a9606":"markdown","7b14bd32":"markdown","ed567ac6":"markdown","e0f00c1a":"markdown","6dd613a9":"markdown","f0c68b12":"markdown","101648cc":"markdown","76f73cfe":"markdown","d15aaae6":"markdown","4461e1f0":"markdown","4adc01ba":"markdown","1d7d4b3d":"markdown","9b2faca5":"markdown","22291260":"markdown","8998bce0":"markdown","e6ea88f2":"markdown"},"source":{"ad5641ac":"# -*- coding: utf-8 -*-\n\"\"\"\nCreated on Wed Jun 10 08:59:51 2020","b13ecaaa":"@author: jaket\n\"\"\"\n#j2p\n### Exercise pattern classification","4985a164":"import math\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport warnings \nimport seaborn as sns\nimport sklearn\nfrom datetime import datetime\nimport calendar\n%matplotlib inline\npd.set_option('display.max_rows', 1000)\nimport warnings\nwarnings.filterwarnings('ignore')","7779cbcb":"train_df = pd.read_csv('..\/input\/exercisepatternpredict\/pml-training.csv', error_bad_lines=False, index_col=False).drop('Unnamed: 0', axis=1)\ntest_df = pd.read_csv('..\/input\/exercisepatternpredict\/pml-testing.csv', error_bad_lines=False, index_col=False).drop('Unnamed: 0', axis=1)","5c64eaa9":"train_df = train_df.sample(frac=1).reset_index(drop=True)","bd1dc59b":"print(train_df.columns.values)\nprint(train_df.isna().sum()) ","10262972":"sns.heatmap(train_df.isnull(), cbar=False) # Heatmap to visualise NAs","136d227c":"train_df.describe()","0bfb0781":"train_df['classe']=train_df['classe'].astype('category')","96f9e544":"freq_plot1=train_df.filter(items=['user_name', 'classe'])\nfreq_plot1=freq_plot1.groupby(['user_name'])['classe'].agg(counts='value_counts').reset_index()","dca26294":"sns.barplot(data = freq_plot1, x = 'counts', y = 'user_name', hue = 'classe', ci = None)","9d40010a":"            \npairplot1=train_df.filter(items=['num_window', 'roll_belt', 'pitch_belt', 'yaw_belt', 'total_accel_belt', 'classe'])\nsns.pairplot(pairplot1, hue='classe',  plot_kws = {'alpha': 0.6, 'edgecolor': 'k'},size = 4)","a343a86c":"pairplot2=train_df.filter(items=['num_window', 'gyros_belt_x', 'gyros_belt_y', 'accel_belt_x', 'accel_belt_y',  'magnet_belt_x','magnet_belt_y', 'classe'])\nsns.pairplot(pairplot2, hue='classe',  plot_kws = {'alpha': 0.6,  'edgecolor': 'k'},size = 4)","c5c7ba8a":"pairplot3=train_df.filter(items=['num_window', 'gyros_belt_z', 'accel_belt_z', 'magnet_belt_z', 'classe'])\nsns.pairplot(pairplot3, hue='classe',  plot_kws = {'alpha': 0.6, 'edgecolor': 'k'},size = 4)","524406ff":"pairplot4=train_df.filter(items=['roll_arm', 'pitch_arm', 'yaw_arm', 'total_accel_arm', 'classe'])\nsns.pairplot(pairplot4, hue='classe',  plot_kws = {'alpha': 0.6, 'edgecolor': 'k'},size = 4)","6cf05d70":"pairplot5=train_df.filter(items=['num_window', 'gyros_arm_x', 'gyros_arm_y', 'accel_arm_x', 'accel_arm_y',  'magnet_arm_x','magnet_arm_y', 'classe'])\nsns.pairplot(pairplot5, hue='classe',  plot_kws = {'alpha': 0.6,  'edgecolor': 'k'},size = 4)","17d7f29f":"pairplot6=train_df.filter(items=['num_window', 'gyros_arm_z', 'accel_arm_z', 'magnet_arm_z', 'classe'])\nsns.pairplot(pairplot6, hue='classe',  plot_kws = {'alpha': 0.6, 'edgecolor': 'k'},size = 4)","b451a13c":"pairplot7=train_df.filter(items=['skewness_roll_belt', 'max_roll_belt', 'max_picth_belt', \n                                 'var_total_accel_belt', 'stdev_roll_belt',\n                                 'avg_yaw_belt', 'classe'])\nsns.pairplot(pairplot7, hue='classe',  plot_kws = {'alpha': 0.6, 'edgecolor': 'k'},size = 4)","8542facf":"print(train_df.isna().sum()) \ntrain_df = train_df.loc[:, train_df.isnull().mean() < .8] #remove cols with <80% completeness.\ntest_df = test_df.loc[:, test_df.isnull().mean() < .8] #remove cols with <80% completeness.","f3da5997":"train_df = train_df.drop(['raw_timestamp_part_1', 'raw_timestamp_part_2' ,'cvtd_timestamp', 'new_window','num_window'], axis=1)\ntest_df = test_df.drop(['raw_timestamp_part_1', 'raw_timestamp_part_2' ,'cvtd_timestamp', 'new_window','num_window', 'problem_id'], axis=1)","e4259a0d":"def zeros_to_ones(x):\n    x = np.where(x==0, 1, x)\n    return(x)","07ad83f4":"def feat_eng (df):\n    df['x_axis_feat']=df[df.columns[df.columns.to_series().str.contains('_x')]].apply(zeros_to_ones).apply(np.prod, axis=1)\n    df['y_axis_feat']=df[df.columns[df.columns.to_series().str.contains('_y')]].apply(zeros_to_ones).apply(np.prod, axis=1)\n    df['z_axis_feat']=df[df.columns[df.columns.to_series().str.contains('_z')]].apply(zeros_to_ones).apply(np.prod, axis=1)\n    \n    # Lets interact all belt, arm, dumbell and forearm variables\n    \n    df['belt_feat']=df[df.columns[df.columns.to_series().str.contains('_belt')]].apply(zeros_to_ones).apply(np.prod, axis=1)\n    df['arm_feat']=df[df.columns[df.columns.to_series().str.contains('_arm')]].apply(zeros_to_ones).apply(np.prod, axis=1)\n    df['forearm_feat']=df[df.columns[df.columns.to_series().str.contains('_forearm')]].apply(zeros_to_ones).apply(np.prod, axis=1)\n    \n    # Let's interact all magnet, accel and gyros variables\n    \n    df['accel_feat']=df[df.columns[df.columns.to_series().str.contains('accel_')]].apply(zeros_to_ones).apply(np.prod, axis=1)\n    df['magnet_feat']=df[df.columns[df.columns.to_series().str.contains('magnet_')]].apply(zeros_to_ones).apply(np.prod, axis=1)\n    df['gyros_feat']=df[df.columns[df.columns.to_series().str.contains('gyros_')]].apply(zeros_to_ones).apply(np.prod, axis=1)\n    \n    return(df)","9eef9201":"train_df=feat_eng(train_df)\ntest_df=feat_eng(test_df)","9a092106":"def Encode_fn(df):\n    users=pd.get_dummies(df['user_name']) #OneHot encode username\n    df=pd.concat([df, users], axis=1).reset_index(drop=True) #Join to modelling df\n    df=df.drop('user_name', axis=1) #Drop original username var\n    return(df)","75aa99db":"train_df=Encode_fn(train_df)\ntest_df=Encode_fn(test_df)","8d03213d":"train_df['classe']=train_df['classe'].astype('category') # Ensure the target is cat\ntrain_df['target']=train_df['classe'].cat.codes # Label encoding\ntrain_df['target']=train_df['target'].astype('category') # Ensure the target is cat\ntrain_df=train_df.drop('classe', axis=1)","719f95d5":"from sklearn.model_selection import train_test_split","c72a7f47":"X=train_df.drop('target', axis=1).reset_index(drop=True)\ny=train_df['target']","84399fca":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","b7657ee9":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis,  QuadraticDiscriminantAnalysis\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, log_loss, precision_score, recall_score, f1_score","acaea9e3":"classifiers = [\n    KNeighborsClassifier(3),\n    SVC(kernel=\"rbf\", C=0.025, probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis()]","1e4435fb":"log_cols=[\"Classifier\", \"Accuracy\", \"Log Loss\"]\nlog = pd.DataFrame(columns=log_cols)","66a4e4a1":"for clf in classifiers:\n    clf.fit(X_train, y_train)\n    name = clf.__class__.__name__\n    \n    print(\"=\"*30)\n    print(name)\n    \n    print('****Results****')\n    train_predictions = clf.predict(X_test)\n    acc = accuracy_score(y_test, train_predictions)\n    \n    # calculate score\n    precision = precision_score(y_test, train_predictions, average = 'macro') \n    recall = recall_score(y_test, train_predictions, average = 'macro') \n    f_score = f1_score(y_test, train_predictions, average = 'macro')\n    \n    \n    print(\"Precision: {:.4%}\".format(precision))\n    print(\"Recall: {:.4%}\".format(recall))\n    print(\"F-score: {:.4%}\".format(recall))\n    print(\"Accuracy: {:.4%}\".format(acc))\n    \n    train_predictions = clf.predict_proba(X_test)\n    ll = log_loss(y_test, train_predictions)\n    print(\"Log Loss: {}\".format(ll))\n    \n    log_entry = pd.DataFrame([[name, acc*100, ll]], columns=log_cols)\n    log = log.append(log_entry)\n    \nprint(\"=\"*30)","9ff91d24":"sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")","5a1af69a":"sns.set_color_codes(\"muted\")\nsns.barplot(x='Log Loss', y='Classifier', data=log, color=\"g\")","3c97ca60":"rf = RandomForestClassifier(n_estimators=500, random_state = 42)\nrf.fit(X_train, y_train);\nfeat_importances = pd.Series(rf.feature_importances_, index=X_train.columns)\nfeat_importances.nlargest(25).plot(kind='barh')","00666157":"from sklearn.model_selection import RandomizedSearchCV","ea8ce444":"n_estimators = [int(x) for x in np.linspace(start = 10, stop = 20, num = 10)]","fcce6331":"max_features = ['auto', 'sqrt']","ea73c25d":"max_depth = [int(x) for x in np.linspace(10, 1000, num = 10)]\nmax_depth.append(None)","809c187a":"min_samples_split = [2, 5, 10]","b1b300ca":"min_samples_leaf = [2, 4, 10, 100]","4c5b8a59":"bootstrap = [True, False]","0bab6dbb":"random_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","2481a252":"rf_random = RandomizedSearchCV(estimator = rf, \n                               param_distributions = random_grid, \n                               n_iter = 100, cv = 3, verbose=2, \n                               random_state=42, n_jobs = -1)\n# Fit the random search model\nrf_random.fit(X_train, y_train)","6eb6f589":"print(rf_random.best_params_)","42290ae8":"best_params_rf = rf_random.best_estimator_\nbest_params_rf.fit(X_train,y_train)","b8e3992f":"y_pred_rf = best_params_rf.predict(X_test)","37e19298":"precision = precision_score(y_test, y_pred_rf, average = 'macro') \nrecall = recall_score(y_test, y_pred_rf, average = 'macro') \nf_score = f1_score(y_test, y_pred_rf, average = 'macro')\n    \n    \nprint(\"Precision: {:.4%}\".format(precision))\nprint(\"Recall: {:.4%}\".format(recall))\nprint(\"F-score: {:.4%}\".format(recall))\n","08e15261":"final_predictions = best_params_rf.predict(test_df)","631498e0":"print(final_predictions)","b75408f7":"! p2j Exercise classification.py","57c77547":"Number of features to consider at every split","bdbd9429":"This data was collected from activity tracking device(s) (unspecified). There is a training data set with a known class ranging<br>\nFrom A-E, which should be preticted in the test data set. Many of the features are not defined, limiting the domain-specific<br>\npreprocessing and EDA we can do.","8631dbcb":"# Parameter tuning<br>\nWe'll tune the RF using a random search grid. We could use a grid search but it is computationally expensive and given that we're<br>\nOn >99% accuracy and only need to predict a data set size of 20, I think we can manage without.","44a180b1":"Randomise the rows, it is currently very structured and improve training predictability","2c838914":"No engineered features seem that important according to the RF, likely because theyre interactions (\/derivatives). It would make sense<br>\nto go back and remove these to test the RF accuracy without engineering, but i'll leave that for now.","1bcf9647":"efine train and test","31e593e8":"# Initial classification testing","7ff131b7":"Np.prod will give us the product (multiple) of all columns for a given row, creating an interaction variable on the axis.    ","c2d5cd52":"Fit the tuned model","4376c974":"Before we move to modelling we should consider feature removal, feature engineering and categorical encoding amongst other things.","77638351":"The differences here are again subtle. The distribution of A seems to take a distinctly different pattern from the others.","a9ffa889":"Label encode target","06cd9bc5":"Minimum number of samples required at each leaf node","7c7ca92d":"Now lets plot some pairplots...","a994298c":"And on the Z:","e7d2ee5d":"We could continue to generate more features by interacting newly engineered features, or in new combinations, and this may give<br>\nus some additional model performance however due to time and computation restraints we'll leave it here.","6d55bae0":"We can do similar plots for the forearm variables if we wanted but i'll skip it for now. Lastly, lets take a look at some of the<br>\nVariables where nearly all data is missing. If this is uninformative, it makes sense for us to remove it, however it may give<br>\naway a certain class clearly. Since there are many of these I shall just pick a few out.","392001d5":"There aren't any large differences in these relationships regarding class. However, the distribution does seem to be slightly different<br>\nAcross different classes. Lets look at the X and Y axes of the gyros, accel and magnet belt.","cc5d7d34":"Method of selecting samples for training each tree","d259920b":"Number of trees in random forest","eb9aaa88":"Again the data is closely grouped, though we can see that movement D has some determinable features not shared by the others on<br>\nThese axes. Can the Z-axes provide any more information?","3c04c714":"Now we see some extremely distinctive features of class D on the Z axis. This means these variables will be important in classification.","fe7a24d4":"Run algo loop","932c761d":"Evaluate","d28cfb09":"Create the random grid","e218fb7e":"# EDA<br>\nFirst, it would make sense to run some pairplots using class as the Hue, so we can begin to determine which variables are related<br>\nTo the target. We can only do this in the training data set. Lets start by examining the frequency of classes and participants","148d6062":"Maximum number of levels in tree","08e97800":"# Preprocessing","2fe89605":"Define features and labels","5315b109":"Explore","358f2baa":"Use the random grid to search for best hyperparameters<br>\nRandom search of parameters, using 3 fold cross validation, <br>\nSearch across 100 different combinations, and use all available cores","6a7966ff":"The timestamps are not going to be useful for predicting on the other data sets. Also, they would not be informative in real-life<br>\nactivity prediction. It make be that correctly used these timestampts could give away the entire answer if they align with the<br>\nTest data, which will be the case if the test data is a random sample of train. We'll lose these for now. to make it more realistic.<br>\nFor the same reasons, it makes sense to also lose num window and new window","46212a40":"First, drop cols with high % NA","3a954977":"Accuracy","b4b59e85":"Log Loss.","4433c56a":"Generate a fn to turn 0s into 1s as it not ruin the interaction variables","919b61f1":"The arm data shows significantly different patterns from the belt data. However, there is not a considerable difference between classes.<br>\nAgain, lets look at them on the x\/y and then the Z axis.","e548949e":"Minimum number of samples required to split a node","e536d23f":"It's clear we have a large amount of missing data in some columns. It's likely that where this data is available is specific to<br>\na certain exercise type\/class. Due to the amount, imputation will not be effective. In all other columns there is no missing data<br>\nIt could be appropriate to create a seperate df in the cases which have this additional data.","d7d0d9b4":"# Feature Removal","e4f41133":"Only 2 encoding processes need to be done. (1) to one hot encode the user and (2) to label encode the outcome.","ae1624a1":"Load packages","5ab692d0":"As we dont have an excessive number of features, nor a considerable amount of categorical variables to expand our X-cols, we<br>\nCan consider generating some interaction features. For example, lets combine all x y and z","d2a6f389":"Predict test data","a90a9606":"Log results for performance vis","7b14bd32":"# Final Predictions","ed567ac6":"# Set up<br>\nImport, set, read, initial exploration","e0f00c1a":"# Plot results of algo testing","6dd613a9":"Select classification algos","f0c68b12":"These dont show any clear associations with class. Given the small (<1%) fraction of the data available, it makes sense that we remove these.","101648cc":"Convert Py to Notebook","76f73cfe":"Import packages","d15aaae6":"# Encoding","4461e1f0":"Its clear that the frequency of class A is much greater than each other class. The classes are not divided equally between users.<br>\nFor example, Jeremy has around 2x as many A's as other classes. This is important as user may be an important predictive<br>\nVariable when predicting the test data, and we will later need to OH Encode user.","4adc01ba":"# Splitting","1d7d4b3d":"# Feature Engineering","9b2faca5":"Set wd and read in data","22291260":"It is clear that random forest does an extremely good job of classifying these. Usually I would opt to tune multiple algos but based<br>\nOn the accuracy of the RF i'll just do some brief tuning.","8998bce0":"Run randomized search","e6ea88f2":"First, lets consider variable importance"}}