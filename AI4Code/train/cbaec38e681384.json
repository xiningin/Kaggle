{"cell_type":{"71757c50":"code","cb8031c0":"code","6fa6ebe8":"code","a3612dd9":"code","ab3fdb94":"code","ff2913d7":"code","3e6f90c0":"markdown","5e2533f9":"markdown","2888c55c":"markdown","04d34687":"markdown","df55c250":"markdown"},"source":{"71757c50":"import re\nimport os\nimport numpy as np \nimport pandas as pd\nfrom tqdm import tqdm\n\n# Drawing the embeddings\nimport matplotlib.pyplot as plt\n\n# Deep learning: \nimport tensorflow as tf\nfrom scipy import sparse","cb8031c0":"texts = [\n    'The future king is the prince',\n    'Daughter is the princess',\n    'Son is the prince',\n    'Only a man can be a king',\n    'Only a woman can be a queen',\n    'The princess will be a queen',\n    'Queen and king rule the realm',\n    'The prince is a strong man',\n    'The princess is a beautiful woman',\n    'The royal family is the king and queen and their children',\n    'Prince is only a boy now',\n    'A boy will be a man',\n]","6fa6ebe8":"def create_unique_word_dict(text:list) -> dict:\n    \"\"\"\n    A method that creates a dictionary where the keys are unique words\n    and key values are indices\n    \"\"\"\n    # Getting all the unique words from our text and sorting them alphabetically\n    words = list(set(text))\n    words.sort()\n\n    # Creating the dictionary for the unique words\n    unique_word_dict = {}\n    for i, word in enumerate(words):\n        unique_word_dict.update({\n            word: i\n        })\n\n    return unique_word_dict    \n\ndef text_preprocessing(\n    text:list,\n    punctuations = r'''!()-[]{};:'\"\\,<>.\/?@#$%^&*_\u201c~''',\n    stop_words=['and', 'a', 'is', 'the', 'in', 'be', 'will']\n    )->list:\n    \"\"\"\n    A method to preproces text\n    \"\"\"\n    for x in text.lower(): \n        if x in punctuations: \n            text = text.replace(x, \"\")\n\n    # Removing words that have numbers in them\n    text = re.sub(r'\\w*\\d\\w*', '', text)\n\n    # Removing digits\n    text = re.sub(r'[0-9]+', '', text)\n\n    # Cleaning the whitespaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    # Setting every word to lower\n    text = text.lower()\n\n    # Converting all our text to a list \n    text = text.split(' ')\n\n    # Droping empty strings\n    text = [x for x in text if x!='']\n\n    # Droping stop words\n    text = [x for x in text if x not in stop_words]\n\n    return text\n\n# Functions to find the most similar word \ndef euclidean(vec1:np.array, vec2:np.array) -> float:\n    \"\"\"\n    A function to calculate the euclidean distance between two vectors\n    \"\"\"\n    return np.sqrt(np.sum((vec1 - vec2)**2))\n\ndef find_similar(word:str, embedding_dict:dict, top_n=10)->list:\n    \"\"\"\n    A method to find the most similar word based on the learnt embeddings\n    \"\"\"\n    dist_dict = {}\n    word_vector = embedding_dict.get(word, [])\n    if len(word_vector) > 0:\n        for key, value in embedding_dict.items():\n            if key!=word:\n                dist = euclidean(word_vector, value)\n                dist_dict.update({\n                    key: dist\n                })\n\n        return sorted(dist_dict.items(), key=lambda x: x[1])[0:top_n]     ","a3612dd9":"# Reading the text from the input folder\ntexts = [x for x in texts]\n\n# Defining the window for context\nwindow = 2\n\n# Creating a placeholder for the scanning of the word list\nword_lists = []\nall_text = []\n\nfor text in texts:\n\n    # Cleaning the text\n    text = text_preprocessing(text)\n\n    # Appending to the all text list\n    all_text += text \n\n    # Creating a context dictionary\n    for i, word in enumerate(text):\n        for w in range(window):\n            # Getting the context that is ahead by *window* words\n            if i + 1 + w < len(text): \n                word_lists.append([word] + [text[(i + 1 + w)]])\n            # Getting the context that is behind by *window* words    \n            if i - w - 1 >= 0:\n                word_lists.append([word] + [text[(i - w - 1)]])\n\nunique_word_dict = create_unique_word_dict(all_text)\n\n# Defining the number of features (unique words)\nn_words = len(unique_word_dict)\n\n# Getting all the unique words \nwords = list(unique_word_dict.keys())\n\n# Creating the X and Y matrices using one hot encoding\nX = []\nY = []\n\nfor i, word_list in tqdm(enumerate(word_lists)):\n    # Getting the indices\n    main_word_index = unique_word_dict.get(word_list[0])\n    context_word_index = unique_word_dict.get(word_list[1])\n\n    # Creating the placeholders   \n    X_row = np.zeros(n_words)\n    Y_row = np.zeros(n_words)\n\n    # One hot encoding the main word\n    X_row[main_word_index] = 1\n\n    # One hot encoding the Y matrix words \n    Y_row[context_word_index] = 1\n\n    # Appending to the main matrices\n    X.append(X_row)\n    Y.append(Y_row)\n\n# Converting the matrices into a sparse format because the vast majority of the data are 0s\n# X = sparse.csr_matrix(X)\n# Y = sparse.csr_matrix(Y)\nX = np.array(X)\nY = np.array(Y)","ab3fdb94":"# Defining the size of the embedding\nembed_size = 2\n\n# Defining the neural network\nmodel = tf.keras.Sequential([\n    tf.keras.layers.InputLayer(input_shape=(X.shape[1],)),\n    tf.keras.layers.Dense(units=embed_size, activation='linear'),\n    tf.keras.layers.Dense(units=Y.shape[1], activation='softmax')\n])\nmodel.compile(loss='categorical_crossentropy', optimizer='nadam')\n\n# Optimizing the network weights\nmodel.fit(x=X, y=Y, batch_size=16, epochs=500)\n\n# Obtaining the weights from the neural network. \n# These are the so called word embeddings\n\n# The input layer \nweights = model.get_weights()[0]\n\n# Creating a dictionary to store the embeddings in. The key is a unique word and \n# the value is the numeric vector\nembedding_dict = {}\nfor word in words: \n    embedding_dict.update({\n        word: weights[unique_word_dict.get(word)]\n        })","ff2913d7":"# Ploting the embeddings\nplt.figure(figsize=(10, 10))\nfor word in list(unique_word_dict.keys()):\n    coord = embedding_dict.get(word)\n    plt.scatter(coord[0], coord[1])\n    plt.annotate(word, (coord[0], coord[1]))       ","3e6f90c0":"# Visualize the embeddings","5e2533f9":"# Create or Load Dataset","2888c55c":"# Create Embeddings","04d34687":"# Preprocess Data","df55c250":"# Utility functions"}}