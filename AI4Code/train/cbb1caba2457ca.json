{"cell_type":{"589aa9f1":"code","6cc2b007":"code","5c694df4":"code","e955885b":"code","a9f9ae61":"code","57e9d9b8":"code","181b4707":"code","db850520":"code","37b1421f":"code","6ad08e03":"code","5e7deac8":"code","e9b2cae9":"code","074edd52":"code","0507bc60":"code","8113160b":"code","0d0b6076":"code","477a252e":"code","d2316cb1":"code","bbdcdae8":"code","a71b8fef":"code","4ccfc6d7":"code","260a6800":"code","2ea9debe":"code","c739f0f6":"code","43d3304d":"code","62eb3c65":"code","f308cc41":"code","12973ac4":"code","b7b5f9b5":"code","753969b6":"code","9adad109":"code","9cf14b2f":"code","d6aacef9":"code","d7c02cd7":"code","646ef255":"code","3b7e6def":"code","afdda343":"code","5e3e96b9":"code","c455f032":"code","ce476a64":"code","1534a3fe":"code","3d3bca34":"code","8b0432a5":"code","0c7bb086":"code","b968d285":"code","ef5a2d00":"code","c44cc956":"code","0490024b":"code","43789ff6":"code","3d9959c7":"code","8211b3f5":"markdown","cdf49490":"markdown","7f65713f":"markdown","a8401ca8":"markdown","8c9f45b0":"markdown","840d832b":"markdown","02ea1a38":"markdown","cb53e1cb":"markdown","adc6323d":"markdown","80ea85b7":"markdown","8f0f32b3":"markdown","f68be0c5":"markdown","bdbc2957":"markdown","570b152c":"markdown","11fb05ba":"markdown","aa70dd26":"markdown","8926be87":"markdown","a7695b1d":"markdown","88726b00":"markdown","baad75b8":"markdown","bd666d8e":"markdown","6a2e1529":"markdown","c759f99d":"markdown","280484a8":"markdown","3631982d":"markdown","234bd2c5":"markdown","8ae42e67":"markdown"},"source":{"589aa9f1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport cv2\nimport glob\nimport keras\nfrom keras.utils import np_utils\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D,MaxPooling2D,Dense,Flatten,Dropout\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD, Adam, RMSprop, Adagrad, Nadam, Adadelta, Adamax\nfrom keras.utils import to_categorical\nfrom matplotlib import pyplot\n\n\n\nimport os\nprint(os.listdir(\"..\/input\/cell_images\/cell_images\/\"))\n\n# Any results you write to the current directory are saved as output.","6cc2b007":"images = []\nlabels = []\n\npath1 = \"..\/input\/cell_images\/cell_images\/Parasitized\/\"\npath2 = \"..\/input\/cell_images\/cell_images\/Uninfected\/\"\n\n\nfor i in glob.glob(os.path.join(path1,'*png')):\n    img = cv2.imread(i)\n    #img  = cv2.cvtColor(img , cv2.COLOR_BGR2GRAY) # convert to grayscale\n    img = cv2.resize(img,(50,50))  #resize\n    images.append(np.array(img))\n    labels.append(0)\n    \nfor j in glob.glob(os.path.join(path2,'*png')):\n    img = cv2.imread(j)\n    #img  = cv2.cvtColor(img , cv2.COLOR_BGR2GRAY) # convert to grayscale\n    img = cv2.resize(img,(50,50))  #resize\n    images.append(np.array(img))\n    labels.append(1)    \n    \n\n\n","5c694df4":"   \n    \ncells , labels = np.array(images), np.array(labels)    \nnp.save(\"cells\",cells)\nnp.save(\"labels\",labels)\nthe_cells, the_labels = np.load(\"cells.npy\") , np.load(\"labels.npy\")\n\n","e955885b":"\nfig=plt.figure(figsize=(10, 8))\nn = 16\nfor i in range(n):\n    img = np.random.randint(0, the_cells.shape[0] , 1)\n    fig.add_subplot(n**(.5), n**(.5), i+1)\n    plt.imshow(the_cells[img[0]])\n    plt.title('{} : {}'.format('Unifected' if the_labels[img[0]] == 1 else 'Parasitized' ,\n                                the_labels[img[0]]) )\n    plt.xticks([]) , plt.yticks([])\n        \nplt.show()","a9f9ae61":"    X_train, X_test, Y_train, Y_test = train_test_split(the_cells, the_labels, test_size=0.2)\n    \n    #converting to float and normalizing\n    X_train = X_train.astype('float32')\/255 \n    X_test = X_test.astype('float32')\/255\n    \n    #getting the numbr of unique classes in the labels\n    num_classes=len(np.unique(the_labels))\n\n    #One hot encoding as classifier since we  has multiple classes\n    Y_train=keras.utils.to_categorical(Y_train,num_classes)\n    Y_test=keras.utils.to_categorical(Y_test,num_classes)","57e9d9b8":"# import regularizer\nfrom keras.regularizers import l1\n# instantiate regularizer\nreg = l1(0.001)","181b4707":"from keras.layers import Dense, Activation\n\n\nnnmodel = Sequential()\nnnmodel.add(Dense(32, input_shape=(50,50,3)))\nnnmodel.add(Activation('relu'))\nnnmodel.add(Flatten())\nnnmodel.add(Dense(2,activation=\"softmax\"))#, activity_regularizer=l1(0.001))) \nnnmodel.summary()","db850520":"nnmodel.compile(loss='categorical_crossentropy',optimizer=SGD(lr=0.001),metrics=['mae', 'acc'])\n\nnnhistory =nnmodel.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100, verbose=1)\n","37b1421f":"# summarize history for accuracy\nplt.plot(nnhistory.history['acc'])\nplt.plot(nnhistory.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test' ], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(nnhistory.history['loss'])\nplt.plot(nnhistory.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show() ","6ad08e03":"plt.plot(nnhistory.history['mean_absolute_error'])\nplt.title('Model training mean_absolute_error')\nplt.ylabel('mean_absolute_error')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()\n\nplt.plot(nnhistory.history['val_mean_absolute_error'])\nplt.title('Model validation mean_absolute_error')\nplt.ylabel('mean_absolute_error')\nplt.xlabel('epoch')\nplt.legend(['test' ], loc='upper left')\nplt.show()","5e7deac8":"model = Sequential()\nmodel.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\nmodel.add(MaxPooling2D(pool_size=2))\nmodel.add(Flatten())\nmodel.add(Dense(50,activation=\"relu\"))\nmodel.add(Dense(2,activation=\"softmax\",activity_regularizer=l1(0.001)))#2 represent output layer neurons \nmodel.summary()","e9b2cae9":"model.compile(loss='categorical_crossentropy',optimizer='sgd',metrics=['mae', 'acc'])\n\nhistory = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, verbose=1)","074edd52":"# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show() ","0507bc60":"plt.plot(history.history['mean_absolute_error'])\nplt.title('Model training mean_absolute_error')\nplt.ylabel('mean_absolute_error')\nplt.xlabel('epoch')\nplt.legend(['train'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['val_mean_absolute_error'])\nplt.title('Model validation mean_absolute_error')\nplt.ylabel('mean_absolute_error')\nplt.xlabel('epoch')\nplt.legend(['test' ], loc='upper left')\nplt.show()","8113160b":"# split data set \ndef loadingdata():\n    #splitting\n    X_train, X_test, Y_train, Y_test = train_test_split(the_cells, the_labels, test_size=0.2)\n    \n    #converting to float and normalizing\n    X_train = X_train.astype('float32')\/255 \n    X_test = X_test.astype('float32')\/255\n    \n    #getting the numbr of unique classes in the labels\n    num_classes=len(np.unique(the_labels))\n\n    #One hot encoding as classifier since we  has multiple classes\n    Y_train=keras.utils.to_categorical(Y_train,num_classes)\n    Y_test=keras.utils.to_categorical(Y_test,num_classes)\n\n    return X_train, Y_train, X_test, Y_test","0d0b6076":"# here the model is initialized and fitted, accuracy and losses as well as varying learnig curves are plotted \ndef MyCNNmodel(X_train, Y_train, X_test, Y_test, lrate):\n\n    model = Sequential()\n    model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n    model.add(MaxPooling2D(pool_size=2))\n    model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n    model.add(MaxPooling2D(pool_size=2))\n    model.add(Flatten())\n    #model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))    \n    model.add(Dense(50,activation=\"relu\"))\n    model.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n    #model.summary()\n    ## compiling the model\n    opt = SGD(lr=lrate)\n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n    #model.compile(loss='mean_squared_error',optimizer='sgd',metrics=['mae', 'acc'])\n    # fit model\n    history = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, verbose=1)\n    #history = model.fit(X_train, Y_train, validation_split=0.33, epochs=20, batch_size=10, verbose=1)\n    #print(history.history.keys())\n    \n#     # summarize history for accuracy\n#     plt.plot(history.history['acc'])\n#     plt.plot(history.history['val_acc'])\n#     plt.title('model accuracy')\n#     plt.ylabel('accuracy')\n#     plt.xlabel('epoch')\n#     plt.legend(['train', 'test'], loc='upper left')\n#     plt.show()\n#     # summarize history for loss\n#     plt.plot(history.history['loss'])\n#     plt.plot(history.history['val_loss'])\n#     plt.title('model loss')\n#     plt.ylabel('loss')\n#     plt.xlabel('epoch')\n#     plt.legend(['train', 'test'], loc='upper left')\n#     #plt.show()\n    \n    # plot learning curves\n    plt.plot(history.history['acc'], label='train')\n    plt.plot(history.history['val_acc'], label='val')\n    plt.title('lrate='+str(lrate), pad=-50)\n\n     ","477a252e":"      \n# prepare dataset\nX_train, Y_train, X_test, Y_test = loadingdata()\n# create learning curves for different learning rates\nlearning_rates = [1E-0, 1E-1, 1E-2, 1E-3, 1E-4, 1E-5, 1E-6, 1E-7]\nfor i in range(len(learning_rates)):\n    # determine the plot number\n    plot_no = 420 + (i+1)\n    plt.subplot(plot_no)\n    # fit model and plot learning curves for a learning rate\n    MyCNNmodel(X_train, Y_train, X_test, Y_test, learning_rates[i])\n# show learning curves\nplt.show()","d2316cb1":"from sklearn.metrics import classification_report\n\ny_pred=model.predict(X_test) \ny_pred=np.argmax(y_pred, axis=1) \ny_true = Y_test\ny_true=np.argmax(y_true, axis=1) \n\n\nprint(classification_report(y_true, y_pred))\n\n","bbdcdae8":"#Making confusion matrix that checks accuracy of the model\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true, y_pred)\ncm","a71b8fef":"import scikitplot \nimport matplotlib.pyplot as plt\n\n#y_true = # ground truth labels\n#y_probas = # predicted probabilities generated by sklearn classifier\ny_probas = model.predict(X_test)\nscikitplot.metrics.plot_roc(y_true, y_probas)\nplt.show()","4ccfc6d7":"from keras.callbacks import TensorBoard, LearningRateScheduler, ReduceLROnPlateau\n\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-5)","260a6800":"# import regularizer\nfrom keras.regularizers import l1,l2\n# instantiate regularizer\nreg = l1(0.001)\nreg2 = l2(0.001)","2ea9debe":"nn1model = Sequential()\nnn1model.add(Dense(32, input_shape=(50,50,3)))\nnn1model.add(Activation('relu'))\nnn1model.add(Flatten())\nnn1model.add(Dense(2,activation=\"softmax\"))#, activity_regularizer=l1(0.001))) \n#nn1model.summary()\n\nnn1model.compile(loss='categorical_crossentropy',optimizer=SGD(lr=0.01),metrics=['accuracy'])\n\nnn1history =nn1model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=300, verbose=1)","c739f0f6":"nn1model.summary()\n","43d3304d":"# summarize history for accuracy\nplt.plot(nn1history.history['acc'])\nplt.plot(nn1history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test' ], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(nn1history.history['loss'])\nplt.plot(nn1history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","62eb3c65":"score = nn1model.evaluate(X_test, Y_test, verbose=1)\nprint('\\n', 'Test_Loss:-', score[0])\nprint('\\n', 'Test_Accuracy:-', score[1])","f308cc41":"from keras.layers import  Dropout, BatchNormalization","12973ac4":"cnn1model = Sequential()\ncnn1model.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\n#cnn1model.add(BatchNormalization())\ncnn1model.add(MaxPooling2D(pool_size=2))\ncnn1model.add(Dropout(0.25))      \ncnn1model.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\n#cnn1model.add(BatchNormalization())\ncnn1model.add(MaxPooling2D(pool_size=2))\ncnn1model.add(Dropout(0.25))      \n# cnn1model.add(Conv2D(filters=64,kernel_size=2,padding=\"same\",activation=\"relu\"))\n# cnn1model.add(MaxPooling2D(pool_size=2))\n# cnn1model.add(BatchNormalization())\n#cnn1model.add(Dropout(0.25))      \ncnn1model.add(Flatten())\ncnn1model.add(Dense(50,activation=\"relu\"))\ncnn1model.add(Dense(2,activation=\"softmax\", kernel_regularizer=l2(0.001)))#activity_regularizer=l1(0.001)))\n#cnn1model.summary()\ncnn1model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\ncnn1history = cnn1model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=300, verbose=1)\n","b7b5f9b5":"cnn1model.summary()\n","753969b6":"score1 = cnn1model.evaluate(X_test, Y_test, verbose=1)\nprint('\\n', 'Test_Loss:-', score1[0])\nprint('\\n', 'Test_Accuracy:-', score1[1])","9adad109":"# summarize history for accuracy\nplt.plot(cnn1history.history['acc'])\nplt.plot(cnn1history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test' ], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(cnn1history.history['loss'])\nplt.plot(cnn1history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show() ","9cf14b2f":"cnnmodel_rmsprop = Sequential()\ncnnmodel_rmsprop.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\ncnnmodel_rmsprop.add(MaxPooling2D(pool_size=2))\ncnnmodel_rmsprop.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnnmodel_rmsprop.add(MaxPooling2D(pool_size=2))\ncnnmodel_rmsprop.add(Flatten())\ncnnmodel_rmsprop.add(Dense(50,activation=\"relu\"))\ncnnmodel_rmsprop.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n#cnnmodel_rmsprop.summary()\ncnnmodel_rmsprop.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\nhist_rmsprop = cnnmodel_rmsprop.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, verbose=1, callbacks=[reduce_lr])\n","d6aacef9":"cnnmodel_adam = Sequential()\ncnnmodel_adam.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\ncnnmodel_adam.add(MaxPooling2D(pool_size=2))\ncnnmodel_adam.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnnmodel_adam.add(MaxPooling2D(pool_size=2))\ncnnmodel_adam.add(Flatten())\ncnnmodel_adam.add(Dense(50,activation=\"relu\"))\ncnnmodel_adam.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \ncnnmodel_adam.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\nhist_adam = cnnmodel_adam.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, verbose=1, callbacks=[reduce_lr])\n","d7c02cd7":"cnnmodel_nadam = Sequential()\ncnnmodel_nadam.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\ncnnmodel_nadam.add(MaxPooling2D(pool_size=2))\ncnnmodel_nadam.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnnmodel_nadam.add(MaxPooling2D(pool_size=2))\ncnnmodel_nadam.add(Flatten())\ncnnmodel_nadam.add(Dense(50,activation=\"relu\"))\ncnnmodel_nadam.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \ncnnmodel_nadam.compile(optimizer=Nadam(), loss='binary_crossentropy', metrics=['accuracy'])\nhist_nadam = cnnmodel_nadam.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, verbose=1, callbacks=[reduce_lr])\n","646ef255":"cnnmodel_sgd= Sequential()\ncnnmodel_sgd.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\ncnnmodel_sgd.add(MaxPooling2D(pool_size=2))\ncnnmodel_sgd.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnnmodel_sgd.add(MaxPooling2D(pool_size=2))\ncnnmodel_sgd.add(Flatten())\ncnnmodel_sgd.add(Dense(50,activation=\"relu\"))\ncnnmodel_sgd.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n#cnnmodel_rmsprop.summary()\ncnnmodel_sgd.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n#hist_rmsprop = cnnmodel_rmsprop.fit(X_train, Y_train, batch_size=batch_size*2,  nb_epoch=0, validation_data=(X_test,Y_test), callbacks=[reduce_lr])\nhist_sgd = cnnmodel_sgd.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, verbose=1, callbacks=[reduce_lr])\n\n","3b7e6def":"cnnmodel_sgdnesterov = Sequential()\ncnnmodel_sgdnesterov.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\ncnnmodel_sgdnesterov.add(MaxPooling2D(pool_size=2))\ncnnmodel_sgdnesterov.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnnmodel_sgdnesterov.add(MaxPooling2D(pool_size=2))\ncnnmodel_sgdnesterov.add(Flatten())\ncnnmodel_sgdnesterov.add(Dense(50,activation=\"relu\"))\ncnnmodel_sgdnesterov.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n#cnnmodel_rmsprop.summary()\ncnnmodel_sgdnesterov.compile(optimizer=SGD(nesterov=True), loss='binary_crossentropy', metrics=['accuracy'])\n#hist_rmsprop = cnnmodel_rmsprop.fit(X_train, Y_train, batch_size=batch_size*2,  nb_epoch=0, validation_data=(X_test,Y_test), callbacks=[reduce_lr])\nhist_sgdnesterov = cnnmodel_sgdnesterov.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, verbose=1, callbacks=[reduce_lr])\n","afdda343":"cnnmodel_sgdmomentum = Sequential()\ncnnmodel_sgdmomentum.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\ncnnmodel_sgdmomentum.add(MaxPooling2D(pool_size=2))\ncnnmodel_sgdmomentum.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnnmodel_sgdmomentum.add(MaxPooling2D(pool_size=2))\ncnnmodel_sgdmomentum.add(Flatten())\ncnnmodel_sgdmomentum.add(Dense(50,activation=\"relu\"))\ncnnmodel_sgdmomentum.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n#cnnmodel_rmsprop.summary()\ncnnmodel_sgdmomentum.compile(optimizer=SGD(momentum=0.9), loss='binary_crossentropy', metrics=['accuracy'])\n#hist_rmsprop = cnnmodel_rmsprop.fit(X_train, Y_train, batch_size=batch_size*2,  nb_epoch=0, validation_data=(X_test,Y_test), callbacks=[reduce_lr])\nhist_sgdmomentum = cnnmodel_sgdmomentum.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, verbose=1, callbacks=[reduce_lr])\n\n","5e3e96b9":"cnnmodel_sgdnestmomentum = Sequential()\ncnnmodel_sgdnestmomentum.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\ncnnmodel_sgdnestmomentum.add(MaxPooling2D(pool_size=2))\ncnnmodel_sgdnestmomentum.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnnmodel_sgdnestmomentum.add(MaxPooling2D(pool_size=2))\ncnnmodel_sgdnestmomentum.add(Flatten())\ncnnmodel_sgdnestmomentum.add(Dense(50,activation=\"relu\"))\ncnnmodel_sgdnestmomentum.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n#cnnmodel_rmsprop.summary()\ncnnmodel_sgdnestmomentum.compile(optimizer=SGD(momentum=0.9, nesterov=True), loss='binary_crossentropy', metrics=['accuracy'])\n#hist_rmsprop = cnnmodel_rmsprop.fit(X_train, Y_train, batch_size=batch_size*2,  nb_epoch=0, validation_data=(X_test,Y_test), callbacks=[reduce_lr])\nhist_sgdnestmomentum = cnnmodel_sgdnestmomentum.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, verbose=1, callbacks=[reduce_lr])\n","c455f032":"cnnmodel_adagrad = Sequential()\ncnnmodel_adagrad.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\ncnnmodel_adagrad.add(MaxPooling2D(pool_size=2))\ncnnmodel_adagrad.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnnmodel_adagrad.add(MaxPooling2D(pool_size=2))\ncnnmodel_adagrad.add(Flatten())\ncnnmodel_adagrad.add(Dense(50,activation=\"relu\"))\ncnnmodel_adagrad.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n#cnnmodel_rmsprop.summary()\ncnnmodel_adagrad.compile(optimizer='adagrad', loss='binary_crossentropy', metrics=['accuracy'])\n#hist_rmsprop = cnnmodel_rmsprop.fit(X_train, Y_train, batch_size=batch_size*2,  nb_epoch=0, validation_data=(X_test,Y_test), callbacks=[reduce_lr])\nhist_adagrad = cnnmodel_adagrad.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, verbose=1, callbacks=[reduce_lr])\n\n","ce476a64":"cnnmodel_adadelta = Sequential()\ncnnmodel_adadelta.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\ncnnmodel_adadelta.add(MaxPooling2D(pool_size=2))\ncnnmodel_adadelta.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnnmodel_adadelta.add(MaxPooling2D(pool_size=2))\ncnnmodel_adadelta.add(Flatten())\ncnnmodel_adadelta.add(Dense(50,activation=\"relu\"))\ncnnmodel_adadelta.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n#cnnmodel_rmsprop.summary()\ncnnmodel_adadelta.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])\n#hist_rmsprop = cnnmodel_rmsprop.fit(X_train, Y_train, batch_size=batch_size*2,  nb_epoch=0, validation_data=(X_test,Y_test), callbacks=[reduce_lr])\nhist_adadelta = cnnmodel_adadelta.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, verbose=1, callbacks=[reduce_lr])\n","1534a3fe":"cnnmodel_adamax = Sequential()\ncnnmodel_adamax.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3)))\ncnnmodel_adamax.add(MaxPooling2D(pool_size=2))\ncnnmodel_adamax.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnnmodel_adamax.add(MaxPooling2D(pool_size=2))\ncnnmodel_adamax.add(Flatten())\ncnnmodel_adamax.add(Dense(50,activation=\"relu\"))\ncnnmodel_adamax.add(Dense(2,activation=\"softmax\"))#2 represent output layer neurons \n#cnnmodel_rmsprop.summary()\ncnnmodel_adamax.compile(optimizer='adamax', loss='binary_crossentropy', metrics=['accuracy'])\n#hist_rmsprop = cnnmodel_rmsprop.fit(X_train, Y_train, batch_size=batch_size*2,  nb_epoch=0, validation_data=(X_test,Y_test), callbacks=[reduce_lr])\nhist_adamax = cnnmodel_adamax.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=10, verbose=1, callbacks=[reduce_lr])\n\n","3d3bca34":"plt.figure(figsize=(10, 6))  \n#plt.axis((-1,14,0.4, 1))\n\nplt.plot(hist_rmsprop.history['val_acc'])\nplt.plot(hist_adam.history['val_acc'])\nplt.plot(hist_nadam.history['val_acc'])\nplt.plot(hist_sgd.history['val_acc'])\nplt.plot(hist_sgdnesterov.history['val_acc'])\nplt.plot(hist_sgdmomentum.history['val_acc'])\nplt.plot(hist_sgdnestmomentum.history['val_acc'])\nplt.plot(hist_adagrad.history['val_acc'])\nplt.plot(hist_adadelta.history['val_acc'])\nplt.plot(hist_adamax.history['val_acc'])\nplt.title('val accuracy')  \nplt.ylabel('accuracy')  \nplt.xlabel('epoch')  \nplt.legend(['rmsprop', 'adam', 'nadam', 'sgd', 'sgd_w\/nesterov', 'sgd_w\/momentum', 'sgd_w\/nesterov+momentum', 'adagrad', 'adadelta', 'adamax'], loc='lower right',fontsize = 'x-small')  \n\nplt.show()","8b0432a5":"plt.figure(figsize=(10, 6))  \n#plt.axis((-1,14,0.4, 1))\n\n\nplt.plot(hist_rmsprop.history['acc'])  \nplt.plot(hist_adam.history['acc'])  \nplt.plot(hist_nadam.history['acc']) \nplt.plot(hist_sgd.history['acc']) \nplt.plot(hist_sgdnesterov.history['acc']) \nplt.plot(hist_sgdmomentum.history['acc'])\nplt.plot(hist_sgdnestmomentum.history['acc'])\nplt.plot(hist_adagrad.history['acc'])\nplt.plot(hist_adadelta.history['acc'])\nplt.plot(hist_adamax.history['acc'])\nplt.title('train accuracy')  \nplt.ylabel('accuracy')  \nplt.xlabel('epoch')  \nplt.legend(['rmsprop', 'adam', 'nadam', 'sgd', 'sgd_w\/nesterov', 'sgd_w\/momentum', 'sgd_w\/nesterov+momentum', 'adagrad', 'adadelta', 'adamax'], loc='lower right',fontsize = 'x-small')  \n\nplt.show()","0c7bb086":"plt.figure(figsize=(10, 6))  \n#plt.axis((-1,14,0, 1))\n\n\nplt.plot(hist_rmsprop.history['val_loss'])  \nplt.plot(hist_adam.history['val_loss'])  \nplt.plot(hist_nadam.history['val_loss']) \nplt.plot(hist_sgd.history['val_loss']) \nplt.plot(hist_sgdnesterov.history['val_loss']) \nplt.plot(hist_sgdmomentum.history['val_loss'])\nplt.plot(hist_sgdnestmomentum.history['val_loss'])\nplt.plot(hist_adagrad.history['val_loss'])\nplt.plot(hist_adadelta.history['val_loss'])\nplt.plot(hist_adamax.history['val_loss'])\nplt.title('val loss')  \nplt.ylabel('loss')  \nplt.xlabel('epoch')  \nplt.legend(['rmsprop', 'adam', 'nadam', 'sgd', 'sgd_w\/nesterov', 'sgd_w\/momentum', 'sgd_w\/nesterov+momentum', 'adagrad', 'adadelta', 'adamax'], loc='upper right',fontsize = 'x-small')  \n\nplt.show()","b968d285":"plt.figure(figsize=(10, 6))  \n#plt.axis((-1,14,0, 1))\n\n\nplt.plot(hist_rmsprop.history['loss'])  \nplt.plot(hist_adam.history['loss'])  \nplt.plot(hist_nadam.history['loss']) \nplt.plot(hist_sgd.history['loss']) \nplt.plot(hist_sgdnesterov.history['loss']) \nplt.plot(hist_sgdmomentum.history['loss'])\nplt.plot(hist_sgdnestmomentum.history['loss'])\nplt.plot(hist_adagrad.history['loss'])\nplt.plot(hist_adadelta.history['loss'])\nplt.plot(hist_adamax.history['loss'])\nplt.title('train loss')  \nplt.ylabel('loss')  \nplt.xlabel('epoch')  \nplt.legend(['rmsprop', 'adam', 'nadam', 'sgd', 'sgd_w\/nesterov', 'sgd_w\/momentum', 'sgd_w\/nesterov+momentum', 'adagrad', 'adadelta', 'adamax'], loc='upper right',fontsize = 'x-small')  \n\nplt.show()","ef5a2d00":"plt.figure(figsize=(10, 6))  \n\n#plt.axis((-1,14,0.4, 1))\n\n\nplt.plot(hist_nadam.history['val_acc'])\nplt.plot(hist_sgd.history['val_acc'])\nplt.plot(hist_sgdmomentum.history['val_acc'])\n\nplt.plot(hist_adamax.history['val_acc'])\nplt.title('test accuracy')  \nplt.ylabel('accuracy')  \nplt.xlabel('epoch')  \nplt.legend(['nadam', 'sgd', 'sgd_w\/momentum', 'adamax'], loc='lower right',fontsize = 'x-small')  \n\nplt.show()","c44cc956":"plt.figure(figsize=(10, 6))  \n\n#plt.axis((-1,14,0.4, 1))\n  \nplt.plot(hist_nadam.history['acc']) \nplt.plot(hist_sgd.history['acc']) \nplt.plot(hist_sgdmomentum.history['acc'])\n\nplt.plot(hist_adamax.history['acc'])\nplt.title('train accuracy')  \nplt.ylabel('accuracy')  \nplt.xlabel('epoch')  \nplt.legend([ 'nadam', 'sgd', 'sgd_w\/momentum', 'adamax'], loc='lower right',fontsize = 'x-small')  \n\nplt.show()","0490024b":"plt.figure(figsize=(10, 6))  \n\n\n \nplt.plot(hist_nadam.history['val_loss']) \nplt.plot(hist_sgd.history['val_loss']) \nplt.plot(hist_sgdmomentum.history['val_loss'])\nplt.plot(hist_adamax.history['val_loss'])\nplt.title('test loss')  \nplt.ylabel('loss')  \nplt.xlabel('epoch')  \nplt.legend([ 'nadam', 'sgd', 'sgd_w\/momentum', 'adamax'], loc='upper right',fontsize = 'x-small')  \n\nplt.show()","43789ff6":"plt.figure(figsize=(10, 6))  \n#plt.axis((-1,14,0, 1))\n \nplt.plot(hist_nadam.history['loss']) \nplt.plot(hist_sgd.history['loss']) \nplt.plot(hist_sgdmomentum.history['loss'])\nplt.plot(hist_adamax.history['loss'])\nplt.title('train loss')  \nplt.ylabel('loss')  \nplt.xlabel('epoch')  \nplt.legend([ 'nadam', 'sgd', 'sgd_w\/momentum', 'adamax'], loc='upper right',fontsize = 'x-small')  \n\nplt.show()","3d9959c7":"from keras.callbacks import TensorBoard, LearningRateScheduler, ReduceLROnPlateau\n\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-5)\n\n\ncnndeepmodel = Sequential()\n# first convolution layer\ncnndeepmodel.add(Conv2D(filters=16,kernel_size=2,padding=\"same\",activation=\"relu\",input_shape=(50,50,3))) \ncnndeepmodel.add(BatchNormalization())\ncnndeepmodel.add(MaxPooling2D(pool_size=2))\ncnndeepmodel.add(Dropout(0.25))\n\n#second convolution  layer\ncnndeepmodel.add(Conv2D(filters=32,kernel_size=2,padding=\"same\",activation=\"relu\"))\ncnndeepmodel.add(BatchNormalization())\ncnndeepmodel.add(MaxPooling2D(pool_size=2))\ncnndeepmodel.add(Dropout(0.5))\n\n#Third convolution  layer\ncnndeepmodel.add(Conv2D(64, kernel_size=2 ,padding=\"same\",activation='relu'))\ncnndeepmodel.add(BatchNormalization())\ncnndeepmodel.add(MaxPooling2D(pool_size=2))\ncnndeepmodel.add(Dropout(0.5))\n\n#first Fully connected layer\ncnndeepmodel.add(Flatten()) \ncnndeepmodel.add(Dense(256,kernel_regularizer=l2(0.001)))#activity_regularizer=l1(0.001)))\ncnndeepmodel.add(BatchNormalization())\ncnndeepmodel.add(Activation('relu')) \ncnndeepmodel.add(Dropout(0.5))      \n\n#Final Fully connected layer\ncnndeepmodel.add(Dense(2)) #8\ncnndeepmodel.add(Activation('softmax')) \n\ncnndeepmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])\n\ncnndeepmodel.summary()\n\ncnndeephistory = cnndeepmodel.fit(X_train, Y_train, epochs=300,verbose=1,validation_data=(X_test, Y_test),\n                                  shuffle=True,callbacks=[reduce_lr])\n\ncnnscore = cnndeepmodel.evaluate(X_test, Y_test, verbose=0)\n\n#loss and accuracy\nprint('Test loss:', cnnscore[0])\nprint('Test accuracy:', cnnscore[1])","8211b3f5":"## Optimizer 5: SGD + Nesterov","cdf49490":"# Splitting and Normalizing the data","7f65713f":"## Convolutional Neural Network","a8401ca8":"## Optimizer 2: Adam","8c9f45b0":"# Showing just the best and the worst performing after some observations","840d832b":"## Optimizer 6: SGD with momentum=0.9","02ea1a38":"# Organizing the Model","cb53e1cb":"view how the models loss and accuracy and loss changes\n1. fcn2layer, lr = 0.01, beta = 0,reg=0, epochs = 10\n2. fcn2layer+reg, lr = 0.01, beta = 0,reg=0.001, epochs = 10\n3. fcn2layer+lr1, lr = 0.001, beta = 0,reg=0, epochs = 10\n4. fcn2layer+lr1+reg, lr = 0.001, beta = 0,reg=0.001, epochs = 10\n5. fcn3layer+lr1+reg, lr = 0.001, beta = 0,reg=0.001, epochs = 10\n6. fcn3layer+lr1+reg, lr = 0.001, beta = 0,reg=0.001, epochs = 10\n7. fcn4layer+lr1+reg, lr = 0.001, beta = 0,reg=0.001, epochs = 10\n\nand also when the nodes changes\n1. fcn2layer, nodes = 32\n2. fcn2layer, nodes = 64\n3. fcn2layer, nodes = 128\n\n\nwhere: lr = learning rate, reg = regularization\n\n\n","adc6323d":"## fully connected neural network","80ea85b7":"# Comparing Several Keras Optimizers","8f0f32b3":"# Classification Report and Confusion Matrix","f68be0c5":"## Optimizer 9: Adadelta","bdbc2957":"# Plotting some images of the labelled dataset","570b152c":"## Optimizer 4: SGD","11fb05ba":"# Now a deeper model with better hyperparamters","aa70dd26":"## Optimizer 10: Adamax","8926be87":"## Optimizer 7: SGD + Nesterov with momentum=0.9","a7695b1d":"# Plots of the optimizers","88726b00":"## Optimizer 1: RMSprop","baad75b8":"# Plotting the Loss and Accuracy","bd666d8e":"## Optimizer 3: Nadam","6a2e1529":"# Still in progress, I am a beginner so incase of any correction","c759f99d":"# Plotting the Learning Rates","280484a8":"## Optimizer 8:Adagrad","3631982d":"# Variations of the models for Hyperparameter Optimization","234bd2c5":"# Loading and Preprocessing the Data","8ae42e67":"# Defining a simple model"}}