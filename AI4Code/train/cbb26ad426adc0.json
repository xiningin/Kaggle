{"cell_type":{"fde7caab":"code","f745131b":"code","2a27d98b":"code","50cb1357":"code","951626fb":"code","c5e6564b":"code","f86d8e3f":"code","748a6050":"code","5a52772f":"code","e14f65a3":"code","4fb4c9b2":"code","5726c7cf":"code","c07405b0":"code","798a5157":"code","5d382d35":"code","15b4d433":"code","a59fffe3":"code","0d05e752":"markdown","4e041ce6":"markdown","6b6189c7":"markdown","d981ae3b":"markdown","543d951e":"markdown","a087d8cc":"markdown","f0414caf":"markdown","8e6877c3":"markdown","da006755":"markdown","c543f7e1":"markdown","d8de140c":"markdown","80b07927":"markdown","3b85bf92":"markdown","287b9fef":"markdown","e0caf1fe":"markdown","f540a7df":"markdown","d56506d0":"markdown","b9e1268f":"markdown","72532b1e":"markdown"},"source":{"fde7caab":"%%time\n!python ..\/input\/roberta-large-code\/infer.py","f745131b":"%%time\n!python ..\/input\/xlnet-base\/infer.py","2a27d98b":"%%time\n!python ..\/input\/roberta-base\/infer.py","50cb1357":"%%time\n!python ..\/input\/distil-roberta\/infer.py","951626fb":"%%time\n!python ..\/input\/tweet-inference-scripts\/inference_distilbert.py","c5e6564b":"%%time\n!python ..\/input\/tweet-inference-scripts\/inference_bert_base.py","f86d8e3f":"%%time\n!python ..\/input\/tweet-inference-scripts\/inference_bert_wwm.py","748a6050":"%%time\n!python ..\/input\/tweet-inference-scripts\/inference_albert.py","5a52772f":"%%time\n\n!pip install \/kaggle\/input\/bertweet-libs\/sacrebleu-1.4.10-py3-none-any.whl\n!cp -R \/kaggle\/input\/bertweet-libs\/fairseq-0.9.0\/fairseq-0.9.0 \/kaggle\/working\n!cp -R \/kaggle\/input\/bertweet-libs\/fastBPE-0.1.0\/fastBPE-0.1.0\/ \/kaggle\/working\n\n!pip install \/kaggle\/working\/fairseq-0.9.0\/\n!pip install \/kaggle\/working\/fastBPE-0.1.0\/\n\n!python ..\/input\/tweet-inference-scripts\/inference_bertweet.py","e14f65a3":"%%time\n!python ..\/input\/tweet-inference-scripts\/inference_roberta_anton.py","4fb4c9b2":"%%time\n!python ..\/input\/tweet-inference-scripts\/inference_roberta_large_hiki.py","5726c7cf":"%%time\n!python ..\/input\/tweet-inference-scripts\/inference_roberta_hiki.py","c07405b0":"!python ..\/input\/tweet-inference-scripts-lvl-2\/inference_cnn_0_2.py","798a5157":"!python ..\/input\/tweet-inference-scripts-lvl-2\/inference_cnn_1_2.py","5d382d35":"!python ..\/input\/tweet-inference-scripts-lvl-2\/inference_wavenet_2.py","15b4d433":"!python ..\/input\/tweet-inference-scripts-lvl-2\/inference_wavenet_6_2.py","a59fffe3":"import pickle\n\nimport numpy as np\nimport pandas as pd\n\n\ndef string_from_preds_char_level(texts, preds):\n    selected_texts = []\n    n_models = len(preds)\n\n    for idx in range(len(texts)):\n        data = texts[idx]\n\n        start_probas = np.mean(\n            [preds[i][0][idx] for i in range(n_models)], 0)\n        end_probas = np.mean(\n            [preds[i][1][idx] for i in range(n_models)], 0)\n\n        start_idx = np.argmax(start_probas)\n        end_idx = np.argmax(end_probas)\n\n        if end_idx < start_idx:\n            selected_text = data\n        else:\n            selected_text = data[start_idx: end_idx]\n\n        selected_texts.append(selected_text.strip())\n\n    return selected_texts\n\n\ndf_test = pd.read_csv(\n    '..\/input\/tweet-sentiment-extraction\/test.csv').fillna('')\ndf_test['selected_text'] = ''\nsub = pd.read_csv(\n    '..\/input\/tweet-sentiment-extraction\/sample_submission.csv')\n\n\npreds_1 = np.load('preds_char_test_cnn_0_2.npy')\npreds_2 = np.load('preds_char_test_cnn_1_2.npy')\npreds_3 = np.load('preds_char_test_wavenet_2.npy')\npreds_4 = np.load('preds_char_test_wavenet_6_2.npy')\n\ntest_preds = (preds_1 + preds_2 + preds_3 + preds_4) \/ 4\n\nselected_texts = string_from_preds_char_level(\n    df_test['text'].values, test_preds)\n\nsub['selected_text'] = selected_texts\ndf_test['selected_text'] = selected_texts\nsub.to_csv('submission.csv', index=False)","0d05e752":"## Hiki","4e041ce6":"### Bert base","6b6189c7":"### Bertweet","d981ae3b":"### Distilbert","543d951e":"### Roberta","a087d8cc":"### Albert large","f0414caf":"### Roberta large","8e6877c3":"## Theo","da006755":"### Roberta","c543f7e1":"## Hearkilla","d8de140c":"### Bert large wwm","80b07927":"### Xlnet","3b85bf92":"### Distilroberta","287b9fef":"## Anton","e0caf1fe":"### Roberta","f540a7df":"# Level 2 models","d56506d0":"# Stage 1 Models","b9e1268f":"### Roberta large","72532b1e":"# Ensemble"}}