{"cell_type":{"38bec5ea":"code","0c8dea41":"code","02f1b154":"code","9e902b0f":"code","d83a90fd":"code","c9523b17":"code","bdd6a6aa":"code","ddff1f66":"code","5eda0192":"code","4eaff4b4":"code","598585b7":"code","fc18471b":"code","6b8bb09f":"code","d42b0304":"code","c5e493e7":"code","7d4e5a14":"code","649063ba":"code","27fc7bd6":"code","40d2c52b":"code","420c945e":"code","813a75b3":"code","51af70eb":"code","987133d4":"code","4ce55799":"code","f724071d":"code","38acc5f1":"code","cae5d74e":"code","4bd38d70":"code","c29b229a":"code","ad8195f2":"code","f4534da6":"code","25aaf986":"code","ff24a13e":"code","99b9e1ba":"code","7dde70af":"code","dcd72210":"code","cde1ee7f":"code","d07f8fb0":"code","291ef060":"code","44da4b15":"code","7fdda3dd":"code","355b7470":"code","9c19d8aa":"code","4ba4c1a4":"code","8b7b8664":"code","d2d4cad4":"code","01617722":"code","424edc69":"code","8f86cdc2":"code","f448eb76":"code","09a3d1fe":"code","2367e339":"code","bb7a012a":"code","bac912e0":"code","2556e004":"code","ff1d39e5":"code","5dda5986":"code","9108db9d":"code","ec414d86":"code","10643904":"code","c2bc6eb9":"code","98c9049d":"code","9bab260f":"code","d4fce1b7":"code","ebb3e2d2":"code","b703835d":"code","383fffc0":"code","b747b13c":"code","0cabc83c":"code","42fac4ca":"code","1f2fdb4b":"code","f566d906":"code","1005a047":"code","698ac26a":"markdown"},"source":{"38bec5ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0c8dea41":"#importing the training and test data sets\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","02f1b154":"#importing other necessary libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","9e902b0f":"#print top 5 values\ndf_train.head()","d83a90fd":"#let's summarize the dataset\ndf_train.describe()","c9523b17":"#check the shape of the data set\ndf_train.shape","bdd6a6aa":"df_test.shape","ddff1f66":"#assigning id column to this variable\ntest_ID = df_test['Id']","5eda0192":"#delete the id column from datasets\ndel df_train['Id']\ndel df_test['Id']","4eaff4b4":"#exploring outliers\nfig, ax = plt.subplots()\nax.scatter(x = df_train['GrLivArea'], y = df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","598585b7":"#Deleting outliers\ndf_train = df_train.drop(df_train[(df_train['GrLivArea']>4000) & (df_train['SalePrice']<300000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(df_train['GrLivArea'], df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","fc18471b":"#scatter plot totalbsmtsf\/saleprice\nvar = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice');","6b8bb09f":"#Deleting outliers\ndf_train = df_train.drop(df_train[(df_train['TotalBsmtSF']>2800) & (df_train['SalePrice']<600000)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(df_train['TotalBsmtSF'], df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('TotalBsmtSF', fontsize=13)\nplt.show()","d42b0304":"#scatter plot GarageArea\/SalePrice\nfig, ax = plt.subplots()\nax.scatter(x = df_train['GarageArea'], y = df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GarageArea', fontsize=13)\nplt.show()","c5e493e7":"#Deleting outliers\ndf_train = df_train.drop(df_train[(df_train['GarageArea']>1200) & (df_train['SalePrice']<13.0)].index)\n\n#Check the graphic again\nfig, ax = plt.subplots()\nax.scatter(df_train['TotalBsmtSF'], df_train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('TotalBsmtSF', fontsize=13)\nplt.show()","7d4e5a14":"#box plot overallqual\/saleprice\nvar = 'OverallQual'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","649063ba":"#relationship of SalePrice with YearBuilt\nvar = 'YearBuilt'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","27fc7bd6":"#correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","40d2c52b":"#check the distribution of target variable \nprint (\"Skew is:\", df_train.SalePrice.skew())\nplt.hist(df_train.SalePrice, color='blue')\nplt.show()","420c945e":"#as the distribution is positively skewed, we will perform log transformation on the target variable to make it normally distributed\n\ndf_train['SalePrice'] = np.log(df_train.SalePrice)\nprint (\"Skew is:\", df_train['SalePrice'].skew())\nplt.hist(df_train['SalePrice'], color='blue')\nplt.show()","813a75b3":"#Check the distribution \nsns.distplot(df_train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\nplt.show()","51af70eb":"#concat the train and test dataset into all_data\nall_data = pd.concat((df_train, df_test)).reset_index(drop=True)","987133d4":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = all_data.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)","4ce55799":"# least correlated features\ncorrmat = all_data.corr()\nleast_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])<0.30]\nplt.figure(figsize=(10,10))\ng = sns.heatmap(all_data[least_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","f724071d":"least_corr_features","38acc5f1":"#we will remove the least correlated features from the dataset to make faster and more accurate predictions\nall_data.drop(columns={'3SsnPorch','BedroomAbvGr','BsmtFinSF2','BsmtFullBath','BsmtHalfBath','BsmtUnfSF','EnclosedPorch','KitchenAbvGr','LotArea','LowQualFinSF','MSSubClass','MiscVal','MoSold','OverallCond','PoolArea','ScreenPorch','YrSold'},inplace=True)","cae5d74e":"# most correlated features\ncorrmat = all_data.corr()\nmost_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.5]\nplt.figure(figsize=(10,10))\ng = sns.heatmap(all_data[most_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","4bd38d70":"#check for missing values\nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(20)","c29b229a":"#it is advisable to remove the columns having high null values (considering the correlation of that variable)\nall_data.drop(columns={'PoolQC','MiscFeature','Alley','Fence','FireplaceQu'},inplace=True)","ad8195f2":"#check for the most and least correlated features\ncorr = all_data.corr()\n\nprint (corr['SalePrice'].sort_values(ascending=False)[:5], '\\n')\nprint (corr['SalePrice'].sort_values(ascending=False)[-5:])","f4534da6":"#delete the 'SalePrice' column\nntrain = df_train.shape[0]\nntest = df_test.shape[0]\ny_train = df_train.SalePrice.values\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","25aaf986":"#there may be no garage, so filling NaN with 0\nall_data['GarageYrBlt']=all_data['GarageYrBlt'].fillna(0)","ff24a13e":"#no masonry veneer for some houses\nall_data['MasVnrType']=all_data['MasVnrType'].fillna('None')\nall_data['MasVnrArea']=all_data['MasVnrArea'].fillna(0)","99b9e1ba":"#NaN means there is no basement, so filling the null values with 'None'\nfor col in('BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2'):\n    all_data[col]=all_data.fillna('None')","7dde70af":"#there is only one missing value, so filling it with mode\nall_data['Electrical']=all_data['Electrical'].fillna(all_data['Electrical'].mode()[0])","dcd72210":"#no basement, so filling it with zero\nfor col in ('BsmtFinSF1','TotalBsmtSF'):\n    all_data[col] = all_data[col].fillna(0)","cde1ee7f":"#replacing null values with 'None' as there may be no garage in the house\nfor col in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[col] = all_data[col].fillna('None')","d07f8fb0":"#filling with the most occurring value\nall_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","291ef060":"#mostly the dataset consists of same value in 'Utilities', so droping this feature\nall_data = all_data.drop(['Utilities'], axis=1)","44da4b15":"#NA for 'Functional' variable means typical (as per data description)\nall_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","7fdda3dd":"#filling it with most frequent value\nall_data['KitchenQual'] = all_data['KitchenQual'].fillna(all_data['KitchenQual'].mode()[0])","355b7470":"#as there is only one missing value, we will impute it with the most occurring value\nall_data['Exterior1st'] = all_data['Exterior1st'].fillna(all_data['Exterior1st'].mode()[0])\nall_data['Exterior2nd'] = all_data['Exterior2nd'].fillna(all_data['Exterior2nd'].mode()[0])","9c19d8aa":"#imputing with mode\nall_data['SaleType'] = all_data['SaleType'].fillna(all_data['SaleType'].mode()[0])","4ba4c1a4":"#there is no garage, so fill it with 0\nfor col in ('GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","8b7b8664":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))","d2d4cad4":"#checking if still the data set has null values or not \nnull_columns=all_data.columns[all_data.isnull().any()]\nall_data[null_columns].isnull().sum()","01617722":"#performing label encoding for the categorical variables\nfrom sklearn.preprocessing import LabelEncoder\ncols = ( 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC',  'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'CentralAir')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","424edc69":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","8f86cdc2":"#let's check for the skewed features\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew \nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","f448eb76":"#performing BoxCox transformation for the highly skewed features\n\nskewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)","09a3d1fe":"#dummy categorical features\nall_data = pd.get_dummies(all_data)\nprint(all_data.shape)","2367e339":"#get the new train and test dataset\ntrain = all_data[:ntrain]\ntest = all_data[ntrain:]","bb7a012a":"train.head()","bac912e0":"test.head()","2556e004":"#importing necessary libraries for the modelling part\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","ff1d39e5":"#splitting the data for train and test \ny = df_train['SalePrice']\ny_train = y\nX_train_sparse, X_test_sparse, y_train_sparse, y_test_sparse = train_test_split(\n                                     train, y_train,\n                                     test_size=0.25,\n                                     random_state=42\n                                     )","5dda5986":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","9108db9d":"#defining regression function\nimport time\nfrom sklearn import metrics\n\ndef regression(regr,X_test_sparse,y_test_sparse):\n    start = time.time()\n    regr.fit(X_train_sparse,y_train_sparse)\n    end = time.time()\n    rf_model_time=(end-start)\/60.0\n    print(\"Time taken to model: \", rf_model_time , \" minutes\" ) \n    \ndef regressionPlot(regr,X_test_sparse,y_test_sparse,title):\n    predictions=regr.predict(X_test_sparse)\n    plt.figure(figsize=(10,6))\n    plt.scatter(predictions,y_test_sparse,cmap='plasma')\n    plt.title(title)\n    plt.show()\n    \n    print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(np.log1p(y_test_sparse), np.log1p(predictions))))","ec414d86":"#Lasso regression\nlasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n\nregression(lasso,X_test_sparse,y_test_sparse)\nregressionPlot(lasso,X_test_sparse,y_test_sparse,\"Lasso Model\")","10643904":"#ElasticNet regression\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nscore = rmsle_cv(ENet)\n\nregression(ENet,X_test_sparse,y_test_sparse)\nregressionPlot(ENet,X_test_sparse,y_test_sparse,\"Elastic Net Regression\")","c2bc6eb9":"#KernelRidge Regression\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nscore = rmsle_cv(KRR)\n\nregression(KRR,X_test_sparse,y_test_sparse)\nregressionPlot(KRR,X_test_sparse,y_test_sparse,\"Gradient Boosting Regression\")","98c9049d":"#Gradient Boosting Regression\nGBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\n\nscore = rmsle_cv(GBoost)\n\nregression(GBoost,X_test_sparse,y_test_sparse)\nregressionPlot(GBoost,X_test_sparse,y_test_sparse,\"Gradient Boosting Regression\")","9bab260f":"#XGBoost\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\n\nscore = rmsle_cv(model_xgb)\n\nregression(model_xgb,X_test_sparse,y_test_sparse)\nregressionPlot(model_xgb,X_test_sparse,y_test_sparse,\"XGBoost\")","d4fce1b7":"#Light GBM\nmodel_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nscore = rmsle_cv(model_lgb)\n\nregression(model_lgb,X_test_sparse,y_test_sparse)\nregressionPlot(model_lgb,X_test_sparse,y_test_sparse,\"LightGBM\")","ebb3e2d2":"#averaging base models\nclass AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)   ","b703835d":"#calculating the average score of all the models\naveraged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\n\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","383fffc0":"#we will define a function to evalute the rmsle value\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","b747b13c":"#final training and prediction\n#stacked regressor\naveraged_models.fit(train.values, y_train)\nstacked_train_pred = averaged_models.predict(train.values)\nstacked_pred = np.expm1(averaged_models.predict(test.values))\nprint(rmsle(y_train, stacked_train_pred))","0cabc83c":"#XGBoost\nmodel_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\nprint(rmsle(y_train, xgb_train_pred))","42fac4ca":"#LightGBM\nmodel_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test.values))\nprint(rmsle(y_train, lgb_train_pred))","1f2fdb4b":"'''RMSE on the entire Train data when averaging'''\n\nprint('Average RMSLE score:')\nprint(rmsle(y_train,stacked_train_pred*0.70 +\n               xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","f566d906":"ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","1005a047":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = ensemble\nsub.to_csv('submission.csv',index=False)","698ac26a":"Handling missing values"}}