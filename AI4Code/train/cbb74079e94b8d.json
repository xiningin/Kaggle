{"cell_type":{"f883fcc9":"code","d94251ea":"code","ee904431":"code","aad783ba":"code","23390f7e":"code","40a4eae9":"code","49a4445b":"code","3b7472e5":"code","961dfe61":"code","96c43197":"code","7f49ed62":"code","659db02e":"code","af2b5961":"code","6ab46357":"code","63860aa3":"code","0d869e89":"code","4c50800f":"code","2617d4ff":"code","bac6cb68":"code","875c7e8f":"code","ddab3e3e":"markdown","6a53a9ea":"markdown","f4a4f101":"markdown"},"source":{"f883fcc9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d94251ea":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport string\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn import metrics","ee904431":"train = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","aad783ba":"train","23390f7e":"test","40a4eae9":"#dropping features url_legal and license \ntrain.drop(['url_legal','license'],axis=1,inplace=True)\ntest.drop(['url_legal','license'],axis=1,inplace=True)","49a4445b":"sns.set_theme(context='notebook',style='darkgrid',palette='coolwarm')\n\nsns.lmplot(x='target',y='standard_error',data=train)","3b7472e5":"plt.figure(figsize=(10,7))\nsns.jointplot(x='target',y='standard_error',data=train,kind='hex',palette='rainbow')","961dfe61":"def process_text(text):\n    \"\"\"\n    process the text by removing extra words, punctuations, \n    numbers and tokenize the word vector\n    \"\"\"\n    words = '' \n    for val in text:     \n        #remove numbers\n        rm_num = ''.join(char for char in val if not char.isdigit())\n        #remove puntuations\n        rm_pun = ''.join(char for char in rm_num if char not in string.punctuation)\n        # split the value \n        tokens = rm_pun.split() \n        # Converts each token into lowercase \n        for i in range(len(tokens)):\n            tokens[i] = tokens[i].lower() \n    \n        #exclude the stopwords\n        words += \" \".join(word for word in tokens if word not in stopwords.words('english'))+\" \"\n    return words","96c43197":"words = process_text(train['excerpt'])","7f49ed62":"#wordcloud\nstw = set(STOPWORDS)\nwordcloud = WordCloud(width = 1200, height = 1000, \n                background_color ='grey', colormap = 'rainbow',\n                stopwords = stw, \n                min_font_size = 10).generate(words)\nplt.figure(figsize = (12,10), facecolor = 'green') \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","659db02e":"X = train['excerpt']\ny = train['target']\n","af2b5961":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=100)","6ab46357":"gbr = GradientBoostingRegressor()\ngrid={'n_estimators':[500,1000],'learning_rate':[.001,0.01],'max_depth':[1,2],'subsample':[.5,.75],'random_state':[1]}\n\nsearch = GridSearchCV(estimator=gbr,param_grid=grid,scoring='neg_mean_squared_error',n_jobs=1)\n\npipeline = Pipeline([\n    ('cv', CountVectorizer(analyzer=process_text)),  \n    ('tfidf', TfidfTransformer()), \n    ('regressor',search),  \n])\n\n","63860aa3":"pipeline.fit(X_train,y_train)\ny_pred = pipeline.predict(X_test)\nprint('MAE:', metrics.mean_absolute_error(y_test, y_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, y_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nprint('R Square:', metrics.explained_variance_score(y_test, y_pred))","0d869e89":"pipeline.fit(X,y)","4c50800f":"sub = pipeline.predict(test['excerpt'])","2617d4ff":"sub","bac6cb68":"submission = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\nsubmission['id'] = test['id']\nsubmission['target'] = sub","875c7e8f":"submission.to_csv(\"submission.csv\", index=False)","ddab3e3e":"# Text Preprocessing","6a53a9ea":"# EDA","f4a4f101":"# Gradient Boosting Regressor"}}