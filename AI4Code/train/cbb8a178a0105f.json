{"cell_type":{"986e097d":"code","6cb9acdc":"code","c8c9ca4b":"code","611f1561":"code","6ac8d560":"code","95c21e3f":"code","0c44d9fb":"code","9e3650db":"code","818799ec":"code","774da5a7":"code","fa0dec49":"code","74a9c97e":"code","87484e27":"code","6a94b2d4":"code","ddcad451":"code","5a891db8":"markdown","4ce37d0f":"markdown","706a26cf":"markdown","bd17339a":"markdown"},"source":{"986e097d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\nimport matplotlib.pyplot as plt\n\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.model_selection import train_test_split\n\nimport nltk\nfrom nltk import wordpunct_tokenize\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'","6cb9acdc":"train_df = pd.read_json(\"..\/input\/train.json\").set_index(\"id\")\ntrain_df.cuisine = train_df.cuisine.astype(\"category\")\ntrain_df.head()","c8c9ca4b":"train_df.ingredients.apply(len).describe().astype(int)","611f1561":"train_df.cuisine.describe()","6ac8d560":"train_df.cuisine.value_counts().plot(kind=\"barh\", color=\"steelblue\");","95c21e3f":"from collections import Counter\n\ncuisines = train_df.cuisine.cat.categories.values.tolist()\n\ntexts = []\nlabels = []\n\nlabel2index = { cuisine: i for i, cuisine in enumerate(cuisines)}\nfor i, row in train_df.iterrows():\n    texts.append(\"\\n\".join(row.ingredients))\n    labels.append(label2index[row.cuisine])\n\nlabels = to_categorical(np.asarray(labels, dtype=np.int32))\n\nword_count = Counter()\n\ndef lemmatize(texts):\n    global word_count\n    wnl = nltk.WordNetLemmatizer()\n    for text in texts:\n        tokens_recipe = []\n        for sentence in text.split(\"\\n\"):\n            tokens_ingredient = [ wnl.lemmatize(w) for w in wordpunct_tokenize(sentence.lower()) if w.isalpha() ]\n            word_count.update(tokens_ingredient)\n            tokens_recipe.append(\" \".join(tokens_ingredient))\n        yield \" \".join(tokens_recipe)\n\ndef preprocess(texts):\n    processed_texts = list(lemmatize(texts))\n    black_list = [ word for word, count in word_count.items() if count < 5 ]\n    return [[ word for word in sentence.split() if word not in black_list] for sentence in processed_texts ]\n    \ntokenizer = Tokenizer(oov_token=\"<UNK>\")\nprocessed_texts = preprocess(texts)\ntokenizer.fit_on_texts(processed_texts)\n# sequences = tokenizer.texts_to_sequences(processed_texts)\n# padded_sequences = pad_sequences(sequences, maxlen=100)\nfeature_matrix = tokenizer.texts_to_matrix(processed_texts)\nword2index = tokenizer.word_index\n\nprint(\"Unique tokens: {}\".format(len(word2index)))\nfeature_matrix.shape","0c44d9fb":"from keras.layers import Dense, Dropout\nfrom keras.models import Sequential\n\ndef build_model(hidden_units, dropout):\n    model = Sequential()\n    model.add(Dense(hidden_units, input_shape=[1722,], activation=\"relu\", name=\"hidden\"))\n    model.add(Dropout(dropout, name=\"dropout\"))\n    model.add(Dense(20, name=\"output\"))\n    \n    model.compile(\"adam\", \"categorical_hinge\", metrics=[\"accuracy\"])\n    return model","9e3650db":"## do a grid search for hyper parameters\n# from keras.wrappers.scikit_learn import KerasClassifier\n# from sklearn.model_selection import GridSearchCV\n\n# np.random.seed(7)\n\n# model = KerasClassifier(build_fn=build_model, epochs=10, verbose=0)\n\n# param_grid = dict(\n#     batch_size = [128,],  # (32, 64, 128, 256, 512,),\n#     hidden_units = [2048,] , # [32, 64, 256, 512, 1024, 2048,],\n#     dropout = [0.8,], # [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8,],\n#     )\n\n# cv = [train_test_split( np.arange(39774), test_size=0.2), train_test_split( np.arange(39774), test_size=0.2), ]\n# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, verbose=10, cv=cv)\n# grid_result = grid.fit(feature_matrix, labels)\n# print(\"best: {} using {}\".format(grid_result.best_score_, grid_result.best_params_))","818799ec":"X_train, X_val, y_train, y_val = train_test_split(feature_matrix, labels, test_size=0.2, random_state=42)","774da5a7":"test_df = pd.read_json(\"..\/input\/test.json\")\ntest_texts = []\n\nfor i, row in test_df.iterrows():\n    test_texts.append(\"\\n\".join(row.ingredients))\n\nprocessed_texts = preprocess(test_texts)\ntest_feature_matrix = tokenizer.texts_to_matrix(processed_texts)\n\ntest_feature_matrix.shape","fa0dec49":"from keras.callbacks import EarlyStopping, ModelCheckpoint\n\nfrom sklearn.metrics import hinge_loss, accuracy_score\nfrom sklearn.model_selection import KFold\n\nkf = KFold(n_splits=5, shuffle=True)\n\nmodels = []\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n\noos_y = []\noos_pred = []\n\nfor i, (train_index, test_index) in enumerate(kf.split(feature_matrix), start=1):\n    X_train, y_train = feature_matrix[train_index], labels[train_index]\n    X_val, y_val = feature_matrix[test_index], labels[test_index]\n    model = build_model(2048, 0.8)\n    chk_point = ModelCheckpoint(\"best-model-{}.h5\".format(i), monitor='val_loss', save_best_only=True, save_weights_only=True)\n    model.fit(X_train, y_train, \n        validation_data=(X_val, y_val),\n        epochs=50,\n        callbacks=[early_stopping, chk_point],\n        verbose=0,\n        batch_size=128)\n\n    # use the best model to predict\n    model.load_weights(\"best-model-{}.h5\".format(i))\n    y_pred = model.predict(X_val)\n    loss = hinge_loss(y_val.argmax(axis=1), y_pred)\n    accuracy = accuracy_score(y_val.argmax(axis=1), y_pred.argmax(axis=1))\n    \n    oos_y.append(y_val)\n    oos_pred.append(y_pred)\n    models.append(model)\n        \n    print(\"Fold {}, loss: {}, accuracy: {:.2%}\".format(i, loss, accuracy))\n    \ny_true = np.vstack(oos_y)\ny_pred = np.vstack(oos_pred)\n\nloss = hinge_loss(y_true.argmax(axis=1), y_pred)\naccuracy = accuracy_score(y_true.argmax(axis=1), y_pred.argmax(axis=1))\n\nprint(\"loss: {}, accuracy: {:.2%}\".format(loss, accuracy))","74a9c97e":"summary = np.zeros((20, 20), dtype=np.int32)\nfor y_true_i, y_pred_i in zip(y_true.argmax(axis=1), y_pred.argmax(axis=1)):\n    summary[y_true_i, y_pred_i] += 1\n\nsummary_df = pd.DataFrame(summary, \n                          columns=cuisines, \n                          index=cuisines)\n\nsummary_df","87484e27":"import seaborn as sns \n\nsummary_norm = ( summary \/ y_true.sum(axis=0) )\nsns.heatmap( summary_norm, \n            vmin=0, vmax=1, center=0.5, \n            xticklabels=cuisines,\n            yticklabels=cuisines);","6a94b2d4":"test_pred = np.zeros((9944, 5), dtype=np.int32)\n\nfor i, model in enumerate(models):\n    y_pred = model.predict(test_feature_matrix)\n    test_pred[:, i] = y_pred.argmax(axis=1)\n    \ndef voting(arr):\n    return np.bincount(arr).argmax()\n\npredictions = np.apply_along_axis(voting, 1, test_pred)","ddcad451":"result = pd.Series( pd.Categorical.from_codes(predictions, cuisines), test_df.id, name=\"cuisine\")\nresult.to_csv(\"submission.csv\", header=True)\nresult.value_counts()","5a891db8":"There are a total of 6714 unique ingredients with salt, onion, olive oil, water and garlic being most commonly used.","4ce37d0f":"Dishes have wide number of ingredients, with some having only one ingredient to as high as 65.","706a26cf":"K Fold Cross Validation approach for Deep Learning by Jeff Heaton [1] .\n\n[1] https:\/\/www.youtube.com\/watch?v=SIyMm5DFwQ8","bd17339a":"We have total of 20 cuisines with the distribution shown below."}}