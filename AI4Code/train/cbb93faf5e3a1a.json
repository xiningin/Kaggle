{"cell_type":{"c4c3638e":"code","0814d190":"code","677c0e1c":"code","0530e2d5":"code","48127b45":"code","fa7acb5e":"code","34d0901d":"code","7e2f17ed":"code","81f43a07":"code","de7281ec":"code","8d9aa910":"code","1f074532":"code","05823a5d":"code","46c8ff0c":"code","dfb8c95e":"code","899bb98d":"code","081c21c8":"code","efcf531a":"code","d9d852fe":"code","767c07ec":"code","99a259f3":"code","f523b9ff":"markdown","98cfe8a2":"markdown","103ab80b":"markdown","a19c6f7d":"markdown","25d7dc33":"markdown","591f983e":"markdown"},"source":{"c4c3638e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sys\nimport os\nimport time\nimport datetime\nfrom nlp_utilities import *\nfiles = []\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n            for filename in filenames:\n                files.append(open(os.path.join(dirname, filename),'rb').read().decode(encoding='utf-8'))\n\ntext = getText(files).strip()\nwords = list(text.split(' '))\nvocab = uniqueCharacters(text)\n\nchar2idx = {u:i for i, u in enumerate(vocab)}\nidx2char = np.array(vocab)\ntext_as_int = np.array([char2idx[c] for c in text])","0814d190":"import tensorflow as tf\nfrom tensorflow.data import Dataset\n\nseq_length = 100\nexamples_per_epoch = len(text)\/\/(seq_length+1)\n\n# Create training examples \/ targets\nchar_dataset = Dataset.from_tensor_slices(text_as_int)","677c0e1c":"sequences = char_dataset.batch(seq_length+1, drop_remainder=True)","0530e2d5":"def split_input_target(chunk):\n    input_text = chunk[:-1]\n    target_text = chunk[1:]\n    return input_text, target_text\n\ndataset = sequences.map(split_input_target)\nfor input_example, target_example in  dataset.take(2):\n    print('Input data: ', repr(''.join(idx2char[input_example.numpy()])))\n    print('Target data:', repr(''.join(idx2char[target_example.numpy()])))\n    print()","48127b45":"for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n    print(\"Step {:4d}\".format(i))\n    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))\n    print()\n","fa7acb5e":"# Batch size\nBATCH_SIZE = 64\n\n# Buffer size to shuffle the dataset\n# (TF data is designed to work with possibly infinite sequences,\n# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n# it maintains a buffer in which it shuffles elements).\nBUFFER_SIZE = 10000\n\ndataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)","34d0901d":"# Length of the vocabulary in chars\nvocab_size = len(vocab)\n\n# The embedding dimension\nembedding_dim = 256\n\n# Number of RNN units\nLSTM_units = 1024","7e2f17ed":"!rm -rf .\/logs\/\n!mkdir .\/logs\/","81f43a07":"# Directory where the checkpoints will be saved\ncheckpoint_dir = '.\/training_checkpoints'\n# Name of the checkpoint files\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n\ncheckpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n    filepath=checkpoint_prefix,\n    save_weights_only=True)","de7281ec":"def build_model(vocab_size, embedding_dim, LSTM_units, batch_size):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Embedding(vocab_size, embedding_dim,\n                                  batch_input_shape=[batch_size, None]),\n        \n        tf.keras.layers.LSTM(LSTM_units,\n                            return_sequences=True,\n                            stateful=True,\n                            recurrent_initializer='glorot_uniform'),\n        \n        tf.keras.layers.LSTM(LSTM_units,\n                            return_sequences=True,\n                            stateful=True,\n                            recurrent_initializer='glorot_uniform'),\n        \n        tf.keras.layers.Dense(vocab_size)\n        \n    ])\n    return model\n\ndef loss(labels, logits):\n    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n","8d9aa910":"model = build_model(\n    vocab_size=len(vocab),\n    embedding_dim=embedding_dim,\n    LSTM_units=LSTM_units,\n    batch_size=BATCH_SIZE)\nmodel.summary()","1f074532":"model.compile(optimizer='adam', loss=loss)","05823a5d":"for input_example_batch, target_example_batch in dataset.take(1):\n    example_batch_predictions = model(input_example_batch)\n    print(example_batch_predictions.shape, \"= (batch_size, sequence_length, vocab_size)\")","46c8ff0c":"sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\nsampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()","dfb8c95e":"example_batch_loss = loss(target_example_batch, example_batch_predictions)\nprint(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\nprint(\"scalar_loss:      \", example_batch_loss.numpy().mean())","899bb98d":"EPOCHS = 35","081c21c8":"history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])","efcf531a":"import plotly.graph_objects as go\nloss = history.history['loss']\nfig = go.Figure(data=go.Line(x=[i for i in range(len(list(loss)))], y= loss))\nfig.update_layout(title_text='Training Loss')\nfig.show()","d9d852fe":"tf.train.latest_checkpoint(checkpoint_dir)\nmodel = build_model(vocab_size, embedding_dim, LSTM_units, batch_size=1)\nmodel.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\nmodel.build(tf.TensorShape([1, None]))","767c07ec":"def generate_text(model, start_string= None, temperature = 0.5):\n    # Evaluation step (generating text using the learned model)\n    \n    if start_string == None:\n        rand_int = np.random.randint(0,len(words)-1)\n        start_string = words[rand_int]\n        while len(start_string)<=1:\n            rand_int = np.random.randint(0,len(words)-1)\n            start_string = words[rand_int]\n            \n\n    # Number of characters to generate        \n    num_generate = 1000\n\n    # Converting our start string to numbers (vectorizing)\n    input_eval = [char2idx[s] for s in start_string]\n    input_eval = tf.expand_dims(input_eval, 0)\n\n    # Empty string to store our results\n    text_generated = []\n\n    # Here batch size == 1\n    model.reset_states()\n    for i in range(num_generate):\n        predictions = model(input_eval)\n        # remove the batch dimension\n        predictions = tf.squeeze(predictions, 0)\n\n        # using a categorical distribution to predict the character returned by the model\n        predictions = predictions \/ temperature\n        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n\n        # Pass the predicted character as the next input to the model\n        # along with the previous hidden state\n        input_eval = tf.expand_dims([predicted_id], 0)\n\n        text_generated.append(idx2char[predicted_id])\n\n    return (start_string + ''.join(text_generated))","99a259f3":"speech = generate_text(model)\nprint(speech)","f523b9ff":"## Model Architecture","98cfe8a2":"# Methodology\n\n1. Import the files, and turn them into a single text string.\n\n2. Use this string to find the set of all unique characters used (defined as  `vocab` here).\n\n3. Map each character in the text to it's respective position in the vocab, and it's inverse map, and transform the text string into a list of integers(using the *character to integer* map that we just defined.\n\n4. Create a training dataset, and target labels. (we are teaching the model to predict the character which comes after another character. \n\n5. Shuffle, Batch, and all the usual.\n\n6. Define model hyperparameters `BATCH_SIZE`, `embedding_dimension`, and `LSTM_units` (which are 64, 256, and 1024 in the model used in this notebook, respectively).\n\n7. Define, compile, and train a sequential model with an embedding layer, two LSTM layers, and one Dense Layer for a given number of epochs (35 in this notebook).\n\n8. Generate some Trump speech! \ud83d\udc4c\ud83d\udc4c\ud83d\udc4c\ud83d\udc4c\ud83d\udc4c\ud83d\udc4c","103ab80b":"## Training Loss","a19c6f7d":"# Generate some text!","25d7dc33":"## In this notebook, we'll be using Tensorflow to create a presidential speech generator, of sorts. \n\n![](https:\/\/i.imgflip.com\/1igyrc.jpg)","591f983e":"![](https:\/\/media.giphy.com\/media\/bot1fOwSZbs7C\/source.gif)"}}