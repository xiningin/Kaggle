{"cell_type":{"695dd3b5":"code","f0cfcd81":"code","a3ac8157":"code","6b450698":"code","fc37d71f":"code","cebbe33b":"code","889a8da0":"code","f7a882ac":"code","b6d0851e":"code","247d0196":"code","00194269":"code","cbcb71bb":"code","57b45589":"code","b6201774":"markdown","aeafa110":"markdown","db06fc00":"markdown","5283f7fe":"markdown","8f4f9f6d":"markdown","9516e640":"markdown","120148f4":"markdown","a836cc6a":"markdown","c431f08d":"markdown","1e1334f2":"markdown","e5d8f5c1":"markdown","0e5ab1c7":"markdown"},"source":{"695dd3b5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\n\n!pip install tensorflow_decision_forests   #instaling the tensorflow decison forests API\nimport tensorflow_decision_forests as tfdf  #importing the tensorflow decison forests API\nimport tensorflow as tf\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f0cfcd81":"#The data is stored in Google servers under the following URLs\n\nTRAIN_DATA_URL = \"https:\/\/storage.googleapis.com\/tf-datasets\/titanic\/train.csv\"\nTEST_DATA_URL = \"https:\/\/storage.googleapis.com\/tf-datasets\/titanic\/eval.csv\"\n\n# Downloads a file from a URL if it not already in the cache using `tf.keras.utils.get_file()`.\ntrain_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\ntest_file_path = tf.keras.utils.get_file(\"eval.csv\", TEST_DATA_URL)","a3ac8157":"#Next we'll be reading the CSV file and create a tensorflow.data Dataset from it using the make_csv_dataset function\ntrain_data = tf.data.experimental.make_csv_dataset(\n    train_file_path,       # train csv file\n    label_name='survived', # We specify the label column name in our csv file\n    batch_size = 5,        # This has to do with the batches we'll be using to train the model\n    na_value = \"?\",        #How to flag missing values\n    num_epochs=1,          \n    ignore_errors=True)\n\ntest_data = tf.data.experimental.make_csv_dataset(\n    test_file_path,       # test csv file\n    label_name='survived',# We specify the label column name in our csv file\n    batch_size = 5,       # This has to do with the batches we'll be using to train the model\n    na_value = \"?\",       #How to flag missing values\n    num_epochs=1,\n    ignore_errors=True)","6b450698":"def show_batch(dataset):\n    for batch,label in dataset.take(1): # we look into the first batch\n        for key, value in batch.items(): # we extract the keys and the values in that first batch\n            print (f\"{key:20s}:   {value.numpy()}\")","fc37d71f":"# let's have a look at the first batch of the train data\n\nshow_batch(train_data)","cebbe33b":"NUMERIC_FEATURES = ['survived','age','n_siblings_spouses','parch', 'fare']\nDEFAULTS = [0, 0.0, 0.0, 0.0, 0.0]\ntrain_data_num = tf.data.experimental.make_csv_dataset(\n    train_file_path,       # train csv file\n    label_name='survived', # We specify the label column name in our csv file\n    batch_size = 5,        # This has to do with the batches we'll be using to train the model\n    na_value = \"?\",        #How to flag missing values\n    num_epochs=1,          \n    ignore_errors=True,\n    select_columns = NUMERIC_FEATURES,\n    column_defaults = DEFAULTS)\n\ntest_data_num = tf.data.experimental.make_csv_dataset(\n    test_file_path,       # train csv file\n    label_name='survived', # We specify the label column name in our csv file\n    batch_size = 5,        # This has to do with the batches we'll be using to train the model\n    na_value = \"?\",        #How to flag missing values\n    num_epochs=1,          \n    ignore_errors=True,\n    select_columns = NUMERIC_FEATURES,\n    column_defaults = DEFAULTS)\n\nshow_batch(train_data_num)","889a8da0":"#Define a function that will pack the dataset columns\ndef pack(features, label):  #here the inputs are the elements of the dataset that will be used in the map operation\n         return tf.stack(list(features.values()), axis = 1), label #stack will pack a list of tensors of rank R into a single R+1 tensor","f7a882ac":"packed_train = train_data_num.map(pack)\npacked_test = test_data_num.map(pack)","b6d0851e":"for features , labels in packed_train.take(1):\n    print(features.numpy() ,'\\n')  #transform the tenssor to a numpy element and print it\n    print(labels.numpy())","247d0196":"#We first define the model and instantiate it\n\nrandom_forest = tfdf.keras.RandomForestModel()\n\n#Then we fit our model on the training data\n\nrandom_forest.fit(packed_train)","00194269":"#let's define the evalution metric for the test data\n\nrandom_forest.compile(metrics = ['accuracy'])\n\nprint(random_forest.evaluate(packed_test))","cbcb71bb":"# recall that our Model is random forest that consists of multiple trees. So let's have a look at the first tree!\n\ntfdf.model_plotter.plot_model_in_colab(random_forest, tree_idx = 0)","57b45589":"# Next, let's look at some of the model propreties\n\nrandom_forest.summary()","b6201774":"The Tensorflow API is called **Tensorflow Decision Forests** and we'll refrence to it using TFDF down the road.\n\nBefore we get into any of the code examples, we shall maybe try to remember and discover for those who have not dealt with Decision forests before.\n\nSkip to the next section if you're familiar with decision trees already.\n\nThe simple Decision forest algorithm is the decision tree. **You can think of it as a forest with a single tree**. Park this idea for the moment and we'll get back to it in a while.\n\nSo a decision tree is a simple tree structure where each time we make a split in the tree based on a criteria. Say for example you want to decide on whether a particular person is a pizza lover based on some information you collected on him like ( Age, Sex, wheight). You can start saying that younger people are more prone to like pizza ( and Fast foods in general :)) and decide your first split takes the person age and classify it left if he's young ( say below 25 ) or to the right else. Then next we might consider sex if there is any indication that for instance men like pizza more than women and finaly if the weight is over say 100Kg then this person is a pizza lover. That is more less the logic behind it and you can find better explanations than mine on the topic.\n\n**Random forests** are a more generalized form of decison trees where we will train miltiple decison trees and in each iteration have a different tree by altering the dataset or the features it can use to make splits. After we train our trees say 100, we'll proceed with a voting mechanism in case of classification problem and an average if case of regression problem.\n\nThe last type of decision forests is **gradient boosting** and its explanation is beyond this notebook. On a high level, we train consecutive trees and each time we focus on the errors the previous tree has\/have made to make a better guess this time. Worth noting that those are the best performing decision trees so far and multiple Kaggle competition winners.\n\nNow that we have a bit of understanding of what decison trees are let's jump into the codes!","aeafa110":"As you can see, we have now a batch of vectors as we used .numpy() and each vector has 4 entries for 'age','n_siblings_spouses','parch', 'fare'. The last vector represents the labels i.e. whether each person survived or not. ","db06fc00":"Before we move further, let's step back and understand what we got in hands as a dataset.\nThe make_csv_dataset function creates a tuple of two entities: features and labels.\neach entry in this tuple is first a batch and then its corresponding label.\n\nEach Batch consists of a dictionary where the key is the feature name ( sex, cabin etc...) and the value is the feature value ( male for sex feature).\n\nSo if we look into the first entry in our dataset, we see something like this ( 5 entries for each batch):\n\n{ ((Key = sex, value = male ),...,(Key = Age, value = 27), ...)  , ( survived = 0)}\n                 .\n                 .\n                 .\n{ ((Key = sex, value = male ),...,(Key = Age, value = 87), ...) , ( survived = 1)} \n\nThe different elements of the dictionary are tf.Tensors and the labels are an array data types.\n\nPretty cool, so let's have a look at some of our data!","5283f7fe":"Et voil\u00e0! The model has learnt how to make predictions. let's check the results on the test data.","8f4f9f6d":"Now, we finaly can start training the model using the TFDF API. It is no more than two line of codes!","9516e640":"For the remaining of this notebook, we'll onmy consider numeric features in our dataset for the sake of simplicity. Those features are: **NUMERIC_FEATURES = 'age',  'n_siblings_spouses',  'parch' and 'fare'**","120148f4":"The next task will be to load the datasets both train and test datasets. We could have used pandas for that, but the whole idea here is maybe to use a pure tensorflow environement. So instead, we'll be using tf.data API to load the datasets and do some feature engineering using tf.feature_column. The advantage of doing so if that the preprocessing will be incorporated into your model after you finish training it and saving it and don't need to specifi it in production once you start getting raw data. pretty cool and practical to me.\n\nAs you might have guessed, w'll be using our famous Titanic dataset, the hello world dataset in machine learning community :)","a836cc6a":"So this looks very promising and as a former R user, I see a smooth transition to this API with a lot of cleverly designed similarities.\n\nHope you enjoyed this notebook and looking forward for feedback and ideas to enhance it.","c431f08d":"Next, we define a function to stack the elements of the dataset together using the .map() on the dataset columns. The function we pass to map() will stack the column features in the dictionary and add the label column to it. Let's see how it works!","1e1334f2":"One of the main advantages of Tree based models is its interpretability.\nLet's explore some aspects of the model we just trained and see how easy it is to do so.","e5d8f5c1":"\n\nI have been reading through the news yesterday that Tensorflow now incorporates a new API for playing with Decision trees algorithms and since I kinda love decision trees for their simplicity and performance, I decided to share with you some code examples using the new TFDT API; so let's kick it off!","0e5ab1c7":"Now, since we have a dataset with stacked column, the show_batch function will not work on it anymore. Thus we will access the dataset items in the few next line of codes"}}