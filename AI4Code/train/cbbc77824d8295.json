{"cell_type":{"04ca72a7":"code","da5ad39f":"code","1f2468ff":"code","99415775":"code","3b21415f":"code","70bf387e":"code","8897dcbc":"code","ebc59b29":"code","f941f34e":"code","46063e63":"code","1799d06f":"code","ad76423c":"code","83f96c4c":"code","d1f01039":"code","973676a5":"code","b4ad2b80":"code","8f056a35":"code","e24751c3":"code","283f190f":"code","55bb2c9c":"code","0b962dd2":"code","5805e04a":"code","d59bb998":"code","ee2ef3b0":"code","c5d21895":"code","689571fe":"markdown","a26ab464":"markdown","a95bffdf":"markdown","f1dec4d9":"markdown","23fe8df9":"markdown","3a2823f2":"markdown","f6de4284":"markdown","4c3826ba":"markdown","d21b94f5":"markdown","141dea22":"markdown"},"source":{"04ca72a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy.misc import imread\nfrom sklearn.metrics import accuracy_score\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom keras.models import Sequential , Model\nfrom keras.layers import Input, Dense , Flatten , Dropout ,Activation , Conv2D , MaxPooling2D\nimport keras \nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","da5ad39f":"rn = np.random.RandomState(128)","1f2468ff":"directory = \"..\/input\"","99415775":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","3b21415f":"train_y = keras.utils.np_utils.to_categorical(train.label.values)\ntrain.drop('label', axis=1 , inplace=True)","70bf387e":"image_data = train.values\ntrain_x = np.array([image_data[i].reshape(28,28,1) for i in range(image_data.shape[0])])\ntest_image = test.values\ntest_x = np.array([test_image[i].reshape(28,28,1) for i in range(test_image.shape[0])])","8897dcbc":"print(train_y.shape)\nprint(train_x.shape)","ebc59b29":"# Normalising images\n\ntrain_x = train_x\/255.0\ntest_x = test_x\/255.0\n#train_y = pd.get_dummies(train_y)","f941f34e":"split = int(train_x.shape[0]*0.8)","46063e63":"train_x, val_x = train_x[:split], train_x[split:]\ntrain_y, val_y = train_y[:split], train_y[split:]","1799d06f":"epochs =5 \nbatch_size = 128\n\nmodel = Sequential()\nmodel.add(Conv2D(32 , kernel_size=(3,3) , activation='relu' , input_shape=(28,28,1)))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax', name='preds'))","ad76423c":"model.compile(loss='categorical_crossentropy' , optimizer='adam', metrics=['accuracy'])\ntrained_model = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, validation_data=(val_x, val_y))\npred = model.predict_classes(test_x)","83f96c4c":"model.summary()","d1f01039":"from keras.utils import plot_model\nplot_model(model, to_file='model.jpg')","973676a5":"from IPython.display import SVG\nfrom keras.utils import model_to_dot\n\nSVG(model_to_dot(model).create(prog='dot', format='svg'))","b4ad2b80":"top_layer = model.layers[0]\nimport matplotlib.pyplot as plt\nplt.imshow(top_layer.get_weights()[0][:, :, :, 0].squeeze(), cmap='gray')","8f056a35":"!pip uninstall keras-vis\n!pip install git+https:\/\/github.com\/raghakot\/keras-vis.git -U","e24751c3":"from vis.visualization import visualize_activation\nfrom vis.utils import utils\nfrom keras import activations","283f190f":"%matplotlib inline\nplt.rcParams['figure.figsize'] = (18, 6)\n\n# Utility to search for layer index by name.\n# Alternatively we can specify this as -1 since it corresponds to the last layer.\nlayer_idx = utils.find_layer_idx(model, 'preds')\n\n# Swap softmax with linear\nmodel.layers[layer_idx].activation = activations.linear\nmodel = utils.apply_modifications(model)\n\n# This is the output node we want to maximize.\nfilter_idx = 0\nimg = visualize_activation(model, layer_idx, filter_indices=filter_idx)\nplt.imshow(img[..., 0])","55bb2c9c":"for output_idx in np.arange(10):\n   # Lets turn off verbose output this time to avoid clutter and just see the output.\n   img = visualize_activation(model, layer_idx, filter_indices=output_idx, input_range=(0., 1.))\n   plt.figure()\n   plt.title('Networks perception of {}'.format(output_idx))\n   plt.imshow(img[..., 0])","0b962dd2":"def iter_occlusion(image, size=8):\n    # taken from https:\/\/www.kaggle.com\/blargl\/simple-occlusion-and-saliency-maps\n\n   occlusion = np.full((size * 5, size * 5, 1), [0.5], np.float32)\n   occlusion_center = np.full((size, size, 1), [0.5], np.float32)\n   occlusion_padding = size * 2\n\n   # print('padding...')\n   image_padded = np.pad(image, ((occlusion_padding, occlusion_padding), (occlusion_padding, occlusion_padding), (0, 0) ), 'constant', constant_values = 0.0)\n\n   for y in range(occlusion_padding, image.shape[0] + occlusion_padding, size):\n\n       for x in range(occlusion_padding, image.shape[1] + occlusion_padding, size):\n           tmp = image_padded.copy()\n\n           tmp[y - occlusion_padding:y + occlusion_center.shape[0] + occlusion_padding, \\\n             x - occlusion_padding:x + occlusion_center.shape[1] + occlusion_padding] \\\n             = occlusion\n\n           tmp[y:y + occlusion_center.shape[0], x:x + occlusion_center.shape[1]] = occlusion_center\n\n           yield x - occlusion_padding, y - occlusion_padding, \\\n             tmp[occlusion_padding:tmp.shape[0] - occlusion_padding, occlusion_padding:tmp.shape[1] - occlusion_padding]\n\n\n\n","5805e04a":"train_y[1]","d59bb998":"i = 23  # for example\ndata = train_x[i]\ncorrect_class = np.argmax(train_y[4])\n\n# input tensor for model.predict\ninp = data.reshape(1, 28, 28, 1)\n\n# image data for matplotlib's imshow\nimg = data.reshape(28, 28)\n\n# occlusion\nimg_size = img.shape[0]\nocclusion_size = 4\n\n# preview\n_ = plt.imshow(img)","ee2ef3b0":"print('occluding...')\n\nheatmap = np.zeros((img_size, img_size), np.float32)\nclass_pixels = np.zeros((img_size, img_size), np.int16)\n\nfrom collections import defaultdict\ncounters = defaultdict(int)\n\nfor n, (x, y, img_float) in enumerate(iter_occlusion(data, size=occlusion_size)):\n\n    X = img_float.reshape(1, 28, 28, 1)\n    out = model.predict(X)\n    #print('#{}: {} @ {} (correct class: {})'.format(n, np.argmax(out), np.amax(out), out[0][correct_class]))\n    #print('x {} - {} | y {} - {}'.format(x, x + occlusion_size, y, y + occlusion_size))\n\n    heatmap[y:y + occlusion_size, x:x + occlusion_size] = out[0][correct_class]\n    class_pixels[y:y + occlusion_size, x:x + occlusion_size] = np.argmax(out)\n    counters[np.argmax(out)] += 1","c5d21895":"pred = model.predict(inp)\nprint('Correct class: {}'.format(correct_class))\nprint('Predicted class: {} (prob: {})'.format(np.argmax(pred), np.amax(out)))\n\nprint('Predictions:')\nfor class_id, count in counters.items():\n    print('{}: {}'.format(class_id, count))","689571fe":"## Preprocessing Data","a26ab464":"## Splitting dataset\n\n","a95bffdf":"**Note: Before going to next part be sure to install keras-vis package. `!pip install keras-vis` would work** \n","f1dec4d9":"### loading data ","23fe8df9":"# Creating Model","3a2823f2":"## Activation Maps\n### Maximal Activations","f6de4284":"**This kernel is follow up of post on analytics vidhya  [Link](https:\/\/www.analyticsvidhya.com\/blog\/2018\/03\/essentials-of-deep-learning-visualizing-convolutional-neural-networks\/)**","4c3826ba":"## Visualize filters","d21b94f5":"## Preliminary Methods\n### Plotting model architecture","141dea22":"## Image Occlusion\nIn an image classification problem, a natural question is if the model is truly identifying the location of the object in the image, or just using the surrounding context. We took a brief look at this in gradient based methods above. Occlusion based methods attempt to answer this question by systematically occluding different portions of the input image with a grey square, and monitoring the output of the classifier. "}}