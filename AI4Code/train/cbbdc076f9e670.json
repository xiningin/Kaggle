{"cell_type":{"51f10f2b":"code","fee7a0be":"code","a9cf588e":"code","f0aa048f":"code","5705dbfd":"code","329e5680":"code","2dc605f0":"code","82797c26":"code","96263200":"code","c5c658c9":"code","f3d053ac":"code","0d365a93":"code","6f137097":"code","e3ebb0ef":"code","b6c9919b":"code","ea036d9d":"code","01035a48":"code","b1d7035b":"code","ea73640e":"code","9a7c5ee6":"code","5ea7f224":"code","06ae769e":"code","1b9adf31":"code","d064a619":"code","41e38757":"code","fe6a99dd":"code","65a3eb68":"code","f84bf87d":"code","af028364":"code","24065fe7":"code","4aeb41f2":"code","c124942f":"code","5ea50196":"code","139f60e6":"code","e7d79085":"code","355dadf7":"code","8477f471":"code","4da87980":"code","75418ff0":"code","32289b7e":"code","53aee1bb":"code","350473ac":"code","3fc9754c":"code","12b5c040":"code","d5077861":"code","6b871020":"code","460353c7":"code","bb377b92":"code","a4ba65c5":"code","5a053d2c":"code","8c17c027":"code","dd9a7f1c":"code","337a5648":"code","04e7677c":"code","82afdbf8":"code","b49b8e18":"code","3643f2b3":"code","5fe243b0":"code","c3ec57ad":"code","1720cd2f":"code","b5fd7005":"code","1cb9ca52":"code","7e4ff801":"code","6dcd1628":"code","858a6fff":"markdown","7e856483":"markdown","279c4a1f":"markdown","4c65464d":"markdown","912f8920":"markdown","86464ad5":"markdown","dd740196":"markdown","678e6d7c":"markdown","61e75059":"markdown","7e81b1ea":"markdown","c39643f7":"markdown","c4f66ddc":"markdown","47eea6b7":"markdown","0e0fa8b1":"markdown","d4928be5":"markdown","539ec4d4":"markdown","822c6802":"markdown","a3934b51":"markdown","4ad103c4":"markdown","c56204aa":"markdown"},"source":{"51f10f2b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fee7a0be":"data  =pd.read_csv('..\/input\/pimaindiansdiabetes\/pima-indians-diabetes.data')","a9cf588e":"data.head()","f0aa048f":"display(data.info(),data.head())","5705dbfd":"####  Find the features of the dataset\nfeature_names = [\n   \"pregnant\",\n   \"glucose\",\n   \"Diastolic blood pressure\" ,\n  \"Triceps skin\",\n   \"2-Hour serum insulin\",\n   \"Body mass index\",\n   \"Diabetes\" ,\n   \"Age\" ,\n   \"Outcome\"]","329e5680":"# Python libraries\n# Classic,data manipulation and linear algebra\nimport pandas as pd\nimport numpy as np\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport plotly.offline as py\nimport plotly.graph_objs as go\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\npy.init_notebook_mode(connected=True)\nimport squarify\n\n# Data processing, metrics and modeling\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix,  roc_curve, precision_recall_curve, accuracy_score, roc_auc_score\nimport lightgbm as lgbm\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_curve,auc\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_predict\nfrom yellowbrick.classifier import DiscriminationThreshold\n\nimport scipy.stats as ss\nfrom scipy import interp\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\n# Time\nfrom contextlib import contextmanager\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n\n#ignore warning messages \nimport warnings\nwarnings.filterwarnings('ignore') \n\n","2dc605f0":"data =pd.read_csv(\"..\/input\/pimaindiansdiabetes\/pima-indians-diabetes.data\",header=None,names=feature_names)","82797c26":"data.head()","96263200":"data.shape","c5c658c9":"data.columns","f3d053ac":"D = data[(data['Outcome'] != 0)]\nH = data[(data['Outcome'] == 0)]\n\n#------------COUNT-----------------------\ndef target_count():\n    trace = go.Bar( x = data['Outcome'].value_counts().values.tolist(), \n                    y = ['healthy','diabetic' ], \n                    orientation = 'h', \n                    text=data['Outcome'].value_counts().values.tolist(), \n                    textfont=dict(size=15),\n                    textposition = 'auto',\n                    opacity = 0.8,marker=dict(\n                    color=['lightskyblue', 'gold'],\n                    line=dict(color='#000000',width=1.5)))\n\n    layout = dict(title =  'Count of Outcome variable')\n\n    fig = dict(data = [trace], layout=layout)\n    py.iplot(fig)\n\n#------------PERCENTAGE-------------------\ndef target_percent():\n    trace = go.Pie(labels = ['healthy','diabetic'], values = data['Outcome'].value_counts(), \n                   textfont=dict(size=15), opacity = 0.8,\n                   marker=dict(colors=['lightskyblue', 'gold'], \n                               line=dict(color='#000000', width=1.5)))\n\n\n    layout = dict(title =  'Distribution of Outcome variable')\n\n    fig = dict(data = [trace], layout=layout)\n    py.iplot(fig)","0d365a93":"target_count()\ntarget_percent()\n","6f137097":"data.head()","e3ebb0ef":"data[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = data[['glucose','Diastolic blood pressure','Triceps skin','2-Hour serum insulin','Body mass index']].replace(0,np.NaN)","b6c9919b":"data.head()\ndata =data.drop(columns=['glucose','Diastolic blood pressure','Triceps skin','2-Hour serum insulin','Body mass index'])","ea036d9d":"# Define missing plot to detect all missing values in dataset\ndef missing_plot(dataset, key) :\n    null_feat = pd.DataFrame(len(dataset[key]) - dataset.isnull().sum(), columns = ['Count'])\n    percentage_null = pd.DataFrame((len(dataset[key]) - (len(dataset[key]) - dataset.isnull().sum()))\/len(dataset[key])*100, columns = ['Count'])\n    percentage_null = percentage_null.round(2)\n\n    trace = go.Bar(x = null_feat.index, y = null_feat['Count'] ,opacity = 0.8, text = percentage_null['Count'],  textposition = 'auto',marker=dict(color = '#7EC0EE',\n            line=dict(color='#000000',width=1.5)))\n\n    layout = dict(title =  \"Missing Values (count & %)\")\n\n    fig = dict(data = [trace], layout=layout)\n    py.iplot(fig)\nmissing_plot(data, 'Outcome')\n","01035a48":"data.isnull().sum()","b1d7035b":"def correlation_plot():\n    #correlation\n    correlation = data.corr()\n    #tick labels\n    matrix_cols = correlation.columns.tolist()\n    #convert to array\n    corr_array  = np.array(correlation)\n    trace = go.Heatmap(z = corr_array,\n                       x = matrix_cols,\n                       y = matrix_cols,\n                       colorscale='Viridis',\n                       colorbar   = dict() ,\n                      )\n    layout = go.Layout(dict(title = 'Correlation Matrix for variables',\n                            #autosize = False,\n                            #height  = 1400,\n                            #width   = 1600,\n                            margin  = dict(r = 0 ,l = 100,\n                                           t = 0,b = 100,\n                                         ),\n                            yaxis   = dict(tickfont = dict(size = 9)),\n                            xaxis   = dict(tickfont = dict(size = 9)),\n                           )\n                     )\n    fig = go.Figure(data = [trace],layout = layout)\n    py.iplot(fig)\n    \ncorrelation_plot()\n","ea73640e":"def median_target(var):   \n    temp = data[data[var].notnull()]\n    temp = temp[[var, 'Outcome']].groupby(['Outcome'])[[var]].median().reset_index()\n    return temp","9a7c5ee6":"def plot_distribution(data_select, size_bin) :  \n    # 2 datasets\n    tmp1 = D[data_select]\n    tmp2 = H[data_select]\n    hist_data = [tmp1, tmp2]\n    \n    group_labels = ['diabetic', 'healthy']\n    colors = ['#FFD700', '#7EC0EE']\n\n    fig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = size_bin, curve_type='kde')\n    \n    fig['layout'].update(title = data_select)\n\n    py.iplot(fig, filename = 'Density plot')","5ea7f224":"median_target('Insulin')\n","06ae769e":"data.loc[(data['Outcome'] == 0 ) & (data['Insulin'].isnull()), 'Insulin'] = 102.5\ndata.loc[(data['Outcome'] == 1 ) & (data['Insulin'].isnull()), 'Insulin'] = 169.5","1b9adf31":"median_target('Glucose')\n","d064a619":"data.loc[(data['Outcome'] == 0 ) & (data['Glucose'].isnull()), 'Glucose'] = 107\ndata.loc[(data['Outcome'] == 1 ) & (data['Glucose'].isnull()), 'Glucose'] = 140\n","41e38757":"data.describe()","fe6a99dd":"median_target('SkinThickness')\n","65a3eb68":"data.loc[(data['Outcome'] == 0 ) & (data['SkinThickness'].isnull()), 'SkinThickness'] = 27\ndata.loc[(data['Outcome'] == 1 ) & (data['SkinThickness'].isnull()), 'SkinThickness'] = 32","f84bf87d":"median_target('BloodPressure')\n","af028364":"data.loc[(data['Outcome'] == 0 ) & (data['BloodPressure'].isnull()), 'BloodPressure'] = 70\ndata.loc[(data['Outcome'] == 1 ) & (data['BloodPressure'].isnull()), 'BloodPressure'] = 74.5","24065fe7":"median_target('BMI')\n","4aeb41f2":"data.loc[(data['Outcome'] == 0 ) & (data['BMI'].isnull()), 'BMI'] = 30.1\ndata.loc[(data['Outcome'] == 1 ) & (data['BMI'].isnull()), 'BMI'] = 34.3\n","c124942f":"plot_distribution('Age', 0)\n","5ea50196":"missing_plot(data, 'Outcome')\n","139f60e6":"def plot_feat1_feat2(feat1, feat2) :  \n    D = data[(data['Outcome'] != 0)]\n    H = data[(data['Outcome'] == 0)]\n    trace0 = go.Scatter(\n        x = D[feat1],\n        y = D[feat2],\n        name = 'diabetic',\n        mode = 'markers', \n        marker = dict(color = '#FFD700',\n            line = dict(\n                width = 1)))\n\n    trace1 = go.Scatter(\n        x = H[feat1],\n        y = H[feat2],\n        name = 'healthy',\n        mode = 'markers',\n        marker = dict(color = '#7EC0EE',\n            line = dict(\n                width = 1)))\n\n    layout = dict(title = feat1 +\" \"+\"vs\"+\" \"+ feat2,\n                  yaxis = dict(title = feat2,zeroline = False),\n                  xaxis = dict(title = feat1, zeroline = False)\n                 )\n\n    plots = [trace0, trace1]\n\n    fig = dict(data = plots, layout=layout)\n    py.iplot(fig)","e7d79085":"def barplot(var_select, sub) :\n    tmp1 = data[(data['Outcome'] != 0)]\n    tmp2 = data[(data['Outcome'] == 0)]\n    tmp3 = pd.DataFrame(pd.crosstab(data[var_select],data['Outcome']), )\n    tmp3['% diabetic'] = tmp3[1] \/ (tmp3[1] + tmp3[0]) * 100\n\n    color=['lightskyblue','gold' ]\n    trace1 = go.Bar(\n        x=tmp1[var_select].value_counts().keys().tolist(),\n        y=tmp1[var_select].value_counts().values.tolist(),\n        text=tmp1[var_select].value_counts().values.tolist(),\n        textposition = 'auto',\n        name='diabetic',opacity = 0.8, marker=dict(\n        color='gold',\n        line=dict(color='#000000',width=1)))\n\n    \n    trace2 = go.Bar(\n        x=tmp2[var_select].value_counts().keys().tolist(),\n        y=tmp2[var_select].value_counts().values.tolist(),\n        text=tmp2[var_select].value_counts().values.tolist(),\n        textposition = 'auto',\n        name='healthy', opacity = 0.8, marker=dict(\n        color='lightskyblue',\n        line=dict(color='#000000',width=1)))\n    \n    trace3 =  go.Scatter(   \n        x=tmp3.index,\n        y=tmp3['% diabetic'],\n        yaxis = 'y2',\n        name='% diabetic', opacity = 0.6, marker=dict(\n        color='black',\n        line=dict(color='#000000',width=0.5\n        )))\n\n    layout = dict(title =  str(var_select)+' '+(sub),\n              xaxis=dict(), \n              yaxis=dict(title= 'Count'), \n              yaxis2=dict(range= [-0, 75],\n                          overlaying= 'y', \n                          anchor= 'x', \n                          side= 'right',\n                          zeroline=False,\n                          showgrid= False, \n                          title= '% diabetic'\n                         ))\n\n    fig = go.Figure(data=[trace1, trace2, trace3], layout=layout)\n    py.iplot(fig)","355dadf7":"def plot_pie(var_select, sub) :\n    D = data[(data['Outcome'] != 0)]\n    H = data[(data['Outcome'] == 0)]\n    \n    col =['Silver', 'mediumturquoise','#CF5C36','lightblue','magenta', '#FF5D73','#F2D7EE','mediumturquoise']\n    \n    trace1 = go.Pie(values  = D[var_select].value_counts().values.tolist(),\n                    labels  = D[var_select].value_counts().keys().tolist(),\n                    textfont=dict(size=15), opacity = 0.8,\n                    hole = 0.5, \n                    hoverinfo = \"label+percent+name\",\n                    domain  = dict(x = [.0,.48]),\n                    name    = \"Diabetic\",\n                    marker  = dict(colors = col, line = dict(width = 1.5)))\n    trace2 = go.Pie(values  = H[var_select].value_counts().values.tolist(),\n                    labels  = H[var_select].value_counts().keys().tolist(),\n                    textfont=dict(size=15), opacity = 0.8,\n                    hole = 0.5,\n                    hoverinfo = \"label+percent+name\",\n                    marker  = dict(line = dict(width = 1.5)),\n                    domain  = dict(x = [.52,1]),\n                    name    = \"Healthy\" )\n\n    layout = go.Layout(dict(title = var_select + \" distribution by target <br>\"+(sub),\n                            annotations = [ dict(text = \"Diabetic\"+\" : \"+\"268\",\n                                                font = dict(size = 13),\n                                                showarrow = False,\n                                                x = .22, y = -0.1),\n                                            dict(text = \"Healthy\"+\" : \"+\"500\",\n                                                font = dict(size = 13),\n                                                showarrow = False,\n                                                x = .8,y = -.1)]))\n                                          \n\n    fig  = go.Figure(data = [trace1,trace2],layout = layout)\n    py.iplot(fig)","8477f471":"plot_feat1_feat2('Glucose','Age')\n","4da87980":"palette ={0 : 'lightblue', 1 : 'gold'}\nedgecolor = 'black'\n\nfig = plt.figure(figsize=(12,8))\n\nax1 = sns.scatterplot(x = data['Glucose'], y = data['Age'], hue = \"Outcome\",\n                    data = data, palette = palette, edgecolor=edgecolor)\n\nplt.annotate('N1', size=25, color='black', xy=(80, 30), xytext=(60, 35),\n            arrowprops=dict(facecolor='black', shrink=0.05),\n            )\nplt.plot([50, 120], [30, 30], linewidth=2, color = 'red')\nplt.plot([120, 120], [20, 30], linewidth=2, color = 'red')\nplt.plot([50, 120], [20, 20], linewidth=2, color = 'red')\nplt.plot([50, 50], [20, 30], linewidth=2, color = 'red')\nplt.title('Glucose vs Age')\nplt.show()","75418ff0":"data.loc[:,'N1']=0\ndata.loc[(data['Age']<=30) & (data['Glucose']<=120),'N1']=1\n","32289b7e":"barplot('N1', ':Glucose <= 120 and Age <= 30')\n","53aee1bb":"plot_pie('N1', '(Glucose <= 120 and Age <= 30)')\n","350473ac":"data.loc[:,'N2']=0\ndata.loc[(data['BMI']<=30),'N2']=1\n","3fc9754c":"barplot('N2', ': BMI <= 30')\n","12b5c040":"plot_pie('N2', 'BMI <= 30')\n","d5077861":"barplot('N11', ': Pregnancies > 0 and < 4')\n","6b871020":"plot_pie('N11', 'Pregnancies > 0 and < 4')\n","460353c7":"D = data[(data['Outcome'] != 0)]\nH = data[(data['Outcome'] == 0)]\n","bb377b92":"plot_distribution('N0', 0)\n","a4ba65c5":"data.loc[:,'N15']=0\ndata.loc[(data['N0']<1034) ,'N15']=1\n","5a053d2c":"barplot('N15', ': N0 < 1034')\n","8c17c027":"plot_pie('N15', 'N0 < 1034')\n","dd9a7f1c":"target_col = [\"Outcome\"]\ncat_cols   = data.nunique()[data.nunique() < 12].keys().tolist()\ncat_cols   = [x for x in cat_cols ]\n#numerical columns\nnum_cols   = [x for x in data.columns if x not in cat_cols + target_col]\n#Binary columns with 2 values\nbin_cols   = data.nunique()[data.nunique() == 2].keys().tolist()\n#Columns more than 2 values\nmulti_cols = [i for i in cat_cols if i not in bin_cols]\n\n#Label encoding Binary columns\nle = LabelEncoder()\nfor i in bin_cols :\n    data[i] = le.fit_transform(data[i])\n    \n#Duplicating columns for multi value columns\ndata = pd.get_dummies(data = data,columns = multi_cols )\n\n#Scaling Numerical columns\nstd = StandardScaler()\nscaled = std.fit_transform(data[num_cols])\nscaled = pd.DataFrame(scaled,columns=num_cols)\n\n#dropping original values merging scaled values for numerical columns\ndf_data_og = data.copy()\ndata = data.drop(columns = num_cols,axis = 1)\ndata = data.merge(scaled,left_index=True,right_index=True,how = \"left\")","337a5648":"\ndef correlation_plot():\n    #correlation\n    correlation = data.corr()\n    #tick labels\n    matrix_cols = correlation.columns.tolist()\n    #convert to array\n    corr_array  = np.array(correlation)\n    trace = go.Heatmap(z = corr_array,\n                       x = matrix_cols,\n                       y = matrix_cols,\n                       colorscale='Viridis',\n                       colorbar   = dict() ,\n                      )\n    layout = go.Layout(dict(title = 'Correlation Matrix for variables',\n                            #autosize = False,\n                            #height  = 1400,\n                            #width   = 1600,\n                            margin  = dict(r = 0 ,l = 100,\n                                           t = 0,b = 100,\n                                         ),\n                            yaxis   = dict(tickfont = dict(size = 9)),\n                            xaxis   = dict(tickfont = dict(size = 9)),\n                           )\n                      )\n    fig = go.Figure(data = [trace],layout = layout)\n    py.iplot(fig)","04e7677c":"correlation_plot()\n","82afdbf8":"X = data.drop('Outcome', 1)\ny = data['Outcome']","b49b8e18":"def model_performance(model, subtitle) :   \n    #Kfold\n    cv = KFold(n_splits=5,shuffle=False, random_state = 42)\n    y_real = []\n    y_proba = []\n    tprs = []\n    aucs = []\n    mean_fpr = np.linspace(0,1,100)\n    i = 1\n    for train,test in cv.split(X,y):\n        model.fit(X.iloc[train], y.iloc[train])\n        pred_proba = model.predict_proba(X.iloc[test])\n        precision, recall, _ = precision_recall_curve(y.iloc[test], pred_proba[:,1])\n        y_real.append(y.iloc[test])\n        y_proba.append(pred_proba[:,1])\n        fpr, tpr, t = roc_curve(y[test], pred_proba[:, 1])\n        tprs.append(interp(mean_fpr, fpr, tpr))\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc) \n    \n    # Confusion matrix\n    y_pred = cross_val_predict(model, X, y, cv=5)\n    conf_matrix = confusion_matrix(y, y_pred)\n    trace1 = go.Heatmap(z = conf_matrix  ,x = [\"0 (pred)\",\"1 (pred)\"],\n                        y = [\"0 (true)\",\"1 (true)\"],xgap = 2, ygap = 2, \n                        colorscale = 'Viridis', showscale  = False)\n#Show metrics\n    tp = conf_matrix[1,1]\n    fn = conf_matrix[1,0]\n    fp = conf_matrix[0,1]\n    tn = conf_matrix[0,0]\n    Accuracy  =  ((tp+tn)\/(tp+tn+fp+fn))\n    Precision =  (tp\/(tp+fp))\n    Recall    =  (tp\/(tp+fn))\n    F1_score  =  (2*(((tp\/(tp+fp))*(tp\/(tp+fn)))\/((tp\/(tp+fp))+(tp\/(tp+fn)))))\n\n    show_metrics = pd.DataFrame(data=[[Accuracy , Precision, Recall, F1_score]])\n    show_metrics = show_metrics.T\n\n    colors = ['gold', 'lightgreen', 'lightcoral', 'lightskyblue']\n    trace2 = go.Bar(x = (show_metrics[0].values), \n                    y = ['Accuracy', 'Precision', 'Recall', 'F1_score'], text = np.round_(show_metrics[0].values,4),\n                    textposition = 'auto', textfont=dict(color='black'),\n                    orientation = 'h', opacity = 1, marker=dict(\n            color=colors,\n            line=dict(color='#000000',width=1.5)))\n#Roc curve\n    mean_tpr = np.mean(tprs, axis=0)\n    mean_auc = auc(mean_fpr, mean_tpr)\n\n    trace3 = go.Scatter(x=mean_fpr, y=mean_tpr,\n                        name = \"Roc : \" ,\n                        line = dict(color = ('rgb(22, 96, 167)'),width = 2), fill='tozeroy')\n    trace4 = go.Scatter(x = [0,1],y = [0,1],\n                        line = dict(color = ('black'),width = 1.5,\n                        dash = 'dot'))\n    \n    #Precision - recall curve\n    y_real = y\n    y_proba = np.concatenate(y_proba)\n    precision, recall, _ = precision_recall_curve(y_real, y_proba)\n\n    trace5 = go.Scatter(x = recall, y = precision,\n                        name = \"Precision\" + str(precision),\n                        line = dict(color = ('lightcoral'),width = 2), fill='tozeroy')\n    \n    mean_auc=round(mean_auc,3)\n    \n    #Subplots\n    fig = tls.make_subplots(rows=2, cols=2, print_grid=False,\n                          specs=[[{}, {}], \n                                 [{}, {}]],\n                          subplot_titles=('Confusion Matrix',\n                                          'Metrics',\n                                          'ROC curve'+\" \"+ '('+ str(mean_auc)+')',\n                                          'Precision - Recall curve',\n                                          ))\n    #Trace and layout\n    fig.append_trace(trace1,1,1)\n    fig.append_trace(trace2,1,2)\n    fig.append_trace(trace3,2,1)\n    fig.append_trace(trace4,2,1)\n    fig.append_trace(trace5,2,2)\n    \n    fig['layout'].update(showlegend = False, title = '<b>Model performance report (5 folds)<\/b><br>'+subtitle,\n                        autosize = False, height = 830, width = 830,\n                        plot_bgcolor = 'black',\n                        paper_bgcolor = 'black',\n                        margin = dict(b = 195), font=dict(color='white'))\n    fig[\"layout\"][\"xaxis1\"].update(color = 'white')\n    fig[\"layout\"][\"yaxis1\"].update(color = 'white')\n    fig[\"layout\"][\"xaxis2\"].update((dict(range=[0, 1], color = 'white')))\n    fig[\"layout\"][\"yaxis2\"].update(color = 'white')\n    fig[\"layout\"][\"xaxis3\"].update(dict(title = \"false positive rate\"), color = 'white')\n    fig[\"layout\"][\"yaxis3\"].update(dict(title = \"true positive rate\"),color = 'white')\n    fig[\"layout\"][\"xaxis4\"].update(dict(title = \"recall\"), range = [0,1.05],color = 'white')\n    fig[\"layout\"][\"yaxis4\"].update(dict(title = \"precision\"), range = [0,1.05],color = 'white')\n    for i in fig['layout']['annotations']:\n        i['font'] = titlefont=dict(color='white', size = 14)\n    py.iplot(fig)","3643f2b3":"def scores_table(model, subtitle):\n    scores = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n    res = []\n    for sc in scores:\n        scores = cross_val_score(model, X, y, cv = 5, scoring = sc)\n        res.append(scores)\n    df = pd.DataFrame(res).T\n    df.loc['mean'] = df.mean()\n    df.loc['std'] = df.std()\n    df= df.rename(columns={0: 'accuracy', 1:'precision', 2:'recall',3:'f1',4:'roc_auc'})\n\n    trace = go.Table(\n        header=dict(values=['<b>Fold', '<b>Accuracy', '<b>Precision', '<b>Recall', '<b>F1 score', '<b>Roc auc'],\n                    line = dict(color='#7D7F80'),\n                    fill = dict(color='#a1c3d1'),\n                    align = ['center'],\n                    font = dict(size = 15)),\n        cells=dict(values=[('1','2','3','4','5','mean', 'std'),\n                           np.round(df['accuracy'],3),\n                           np.round(df['precision'],3),\n                           np.round(df['recall'],3),\n                           np.round(df['f1'],3),\n                           np.round(df['roc_auc'],3)],\n                   line = dict(color='#7D7F80'),\n                   fill = dict(color='#EDFAFF'),\n                   align = ['center'], font = dict(size = 15)))\n\n    layout = dict(width=800, height=400, title = '<b>Cross Validation - 5 folds<\/b><br>'+subtitle, font = dict(size = 15))\n    fig = dict(data=[trace], layout=layout)\n\n    py.iplot(fig, filename = 'styled_table')","5fe243b0":"random_state=42\n\nfit_params = {\"early_stopping_rounds\" : 100, \n             \"eval_metric\" : 'auc', \n             \"eval_set\" : [(X,y)],\n             'eval_names': ['valid'],\n             'verbose': 0,\n             'categorical_feature': 'auto'}\n\nparam_test = {'learning_rate' : [0.01, 0.02, 0.03, 0.04, 0.05, 0.08, 0.1, 0.2, 0.3, 0.4],\n              'n_estimators' : [100, 200, 300, 400, 500, 600, 800, 1000, 1500, 2000],\n              'num_leaves': sp_randint(6, 50), \n              'min_child_samples': sp_randint(100, 500), \n              'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n              'subsample': sp_uniform(loc=0.2, scale=0.8), \n              'max_depth': [-1, 1, 2, 3, 4, 5, 6, 7],\n              'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n              'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n              'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\nn_iter = 300\n\n#intialize lgbm and lunch the search\nlgbm_clf = lgbm.LGBMClassifier(random_state=random_state, silent=True, metric='None', n_jobs=4)\ngrid_search = RandomizedSearchCV(\n    estimator=lgbm_clf, param_distributions=param_test, \n    n_iter=n_iter,\n    scoring='accuracy',\n    cv=5,\n    refit=True,\n    random_state=random_state,\n    verbose=True)\n\ngrid_search.fit(X, y, **fit_params)\nopt_parameters =  grid_search.best_params_\nlgbm_clf = lgbm.LGBMClassifier(**opt_parameters)\n","c3ec57ad":"model_performance(lgbm_clf, 'LightGBM')\nscores_table(lgbm_clf, 'LightGBM')\n","1720cd2f":"visualizer = DiscriminationThreshold(lgbm_clf)\n\nvisualizer.fit(X, y)  \nvisualizer.poof() ","b5fd7005":"knn_clf = KNeighborsClassifier()\n\nvoting_clf = VotingClassifier(estimators=[ \n    ('lgbm_clf', lgbm_clf),\n    ('knn', KNeighborsClassifier())], voting='soft', weights = [1,1])\n\nparams = {\n      'knn__n_neighbors': np.arange(1,30)\n      }\n      \ngrid = GridSearchCV(estimator=voting_clf, param_grid=params, cv=5)\n\ngrid.fit(X,y)\n\nprint(\"Best Score:\" + str(grid.best_score_))\nprint(\"Best Parameters: \" + str(grid.best_params_))","1cb9ca52":"knn_clf = KNeighborsClassifier(n_neighbors = 25)\n\nvoting_clf = VotingClassifier (\n        estimators = [('knn', knn_clf), ('lgbm', lgbm_clf)],\n                     voting='soft', weights = [1,1])\n","7e4ff801":"model_performance(voting_clf, 'LightGBM & KNN')\nscores_table(voting_clf, 'LightGBM & KNN')","6dcd1628":"visualizer = DiscriminationThreshold(voting_clf)\n\nvisualizer.fit(X, y)  \nvisualizer.poof() ","858a6fff":"Discrimination Threshold : A visualization of precision, recall, f1 score, and queue rate with respect to the discrimination threshold of a binary classifier. The discrimination threshold is the probability or score at which the positive class is chosen over the negative class","7e856483":"RandomSearch + LightGBM - Accuracy = 89.8%\u00b6\n","279c4a1f":"LightGBM & KNN - Discrimination Threshold\n \n","4c65464d":"# The datasets consist of several medical predictor (independent) variables and one target (dependent) variable, Outcome. Independent variables include the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.","912f8920":"#  Missing Values \n\nWe saw on data.head() that some features contain 0, it doesn't make sense here and this indicates missing value Below we replace 0 value by NaN :**","86464ad5":"A correlation matrix is a table showing correlation coefficients between sets of variables. Each random variable (Xi) in the table is correlated with each of the other values in the table (Xj). This allows you to see which pairs have the highest correlation.","dd740196":"GridSearch + LightGBM & KNN- Accuracy = 90.6%\nWe obtain a really good result but we can beat 90% with adding a KNeighborsClassifier to LightGBM (Voting Classifier)\n\nKNeighborsClassifier : KNeighborsClassifier implements learning based on the k nearest neighbors of each query point, where k is an integer value specified by the user.\n\nVotingClassifier : VotingClassifier is a meta-classifier for combining similar or conceptually different machine learning classifiers for classification via majority or plurality voting\n\nWith GridSearch CV we search the best \"n_neighbors\" to optimize accuracy of Voting Classifier","678e6d7c":"To measure the performance of a model, we need several elements :\n\nThis part is essential\n\nConfusion matrix : also known as the error matrix, allows visualization of the performance of an algorithm :\n\ntrue positive (TP) : Diabetic correctly identified as diabetic\ntrue negative (TN) : Healthy correctly identified as healthy\nfalse positive (FP) : Healthy incorrectly identified as diabetic\nfalse negative (FN) : Diabetic incorrectly identified as healthy\n\n\nMetrics :\n\nAccuracy : (TP +TN) \/ (TP + TN + FP +FN)\nPrecision : TP \/ (TP + FP)\nRecall : TP \/ (TP + FN)\nF1 score : 2 x ((Precision x Recall) \/ (Precision + Recall))\nRoc Curve : The ROC curve is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.\n\n\n\nPrecision Recall Curve : shows the tradeoff between precision and recall for different threshold\n","61e75059":"To find the best hyperparameters, we'll use Random Search CV.\n\nRandom search is a technique where random combinations of the hyperparameters are used to find the best solution for the built model. Generally RS is more faster and accurate than GridSearchCV who calculate all possible combinations. With Random Grid we specify the number of combinations that we want\n\nLightGBM : Hyperparameters :\n\nlearning_rate : This determines the impact of each tree on the final outcome. GBM works by starting with an initial estimate which is updated using the output of each tree. The learning parameter controls the magnitude of this change in the estimates\nn_estimators : number of trees (or rounds)\nnum_leaves : number of leaves in full tree, default: 31\nmin_child_samples : minimal number of data in one leaf. Can be used to deal with over-fitting\nmin_child_weight : minimal sum hessian in one leaf.\nsubsample : randomly select part of data without resampling\nmax_depth : It describes the maximum depth of tree. This parameter is used to handle model overfitting.\ncolsample_bytree : LightGBM will randomly select part of features on each iteration if colsample_bytree smaller than 1.0. For example, if you set it to 0.8, LightGBM will select 80% of features before training each tree\nreg_alpha : regularization\nreg_lambda : regularization\nearly_stopping_rounds : This parameter can help you speed up your analysis. Model will stop training if one metric of one validation data doesn\u2019t improve in last early_stopping_round rounds. This will reduce excessive iterations\n","7e81b1ea":"To train and test our algorithm we'll use cross validation K-Fold\n\n\n\nIn k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k \u2212 1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data. The k results can then be averaged to produce a single estimation. The advantage of this method over repeated random sub-sampling is that all observations are used for both training and validation, and each observation is used for validation exactly once.\n\nBellow we define a stylized report with Plotly","c39643f7":"Thank you all :)\n","c4f66ddc":"# **What is diabetes ?**","47eea6b7":"LightGBM is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with the following advantages:\n\nFaster training speed and higher efficiency.\nLower memory usage.\nBetter accuracy.\nSupport of parallel and GPU learning.\nCapable of handling large-scale data.","0e0fa8b1":"According to wikipedia \"The body mass index (BMI) or Quetelet index is a value derived from the mass (weight) and height of an individual. The BMI is defined as the body mass divided by the square of the body height, and is universally expressed in units of kg\/m2, resulting from mass in kilograms and height in metres.\"\n\n30 kg\/m\u00b2 is the limit to obesity\n\n","d4928be5":"With n_neighbors = 25, the accuracy increase to 90.625 ! Bellow the model performance report\n\n","539ec4d4":"Credits :\n\nhttps:\/\/medium.com\/@pushkarmandot\/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\nhttps:\/\/en.wikipedia.org\/wiki\/Body_mass_index\nhttp:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/EnsembleVoteClassifier\/\nhttps:\/\/www.news-medical.net\/health\/What-is-Diabetes.aspx\nhttps:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters-Tuning.html\nhttp:\/\/ogrisel.github.io\/scikit-learn.org\/sklearn-tutorial\/modules\/generated\/sklearn.neighbors.KNeighborsClassifier.html\nhttps:\/\/www.scikit-yb.org\/en\/latest\/api\/classifier\/threshold.html","822c6802":"We can complete model performance report with a table contain all results by fold\n\n","a3934b51":"StandardScaler :\nStandardize features by removing the mean and scaling to unit variance :\n\n\n\nCentering and scaling happen independently on each feature by computing the relevant statistics on the samples in the set. Mean and standard deviation are then stored to be used on later data using the transform method.\n\nStandardization of a dataset is a common requirement for many machine learning estimators: they might behave badly if the individual features do not more or less look like standard normally distributed data (e.g. Gaussian with 0 mean and unit variance).\n\nLabelEncoder : Encode labels with value between 0 and n_classes-1.\nBellow we encode the data to feed properly to our algorithm\n\n","4ad103c4":"Acccording to NIH, \"Diabetes is a disease that occurs when your blood glucose, also called blood sugar, is too high. Blood glucose is your main source of energy and comes from the food you eat. Insulin, a hormone made by the pancreas, helps glucose from food get into your cells to be used for energy. Sometimes your body doesn\u2019t make enough\u2014or any\u2014insulin or doesn\u2019t use insulin well. Glucose then stays in your blood and doesn\u2019t reach your cells.\n\nOver time, having too much glucose in your blood can cause health problems. Although diabetes has no cure, you can take steps to manage your diabetes and stay healthy.\n\nSometimes people call diabetes \u201ca touch of sugar\u201d or \u201cborderline diabetes.\u201d These terms suggest that someone doesn\u2019t really have diabetes or has a less serious case, but every case of diabetes is serious.\n\nWhat are the different types of diabetes? The most common types of diabetes are type 1, type 2, and gestational diabetes.\n\nType 1 diabetes If you have type 1 diabetes, your body does not make insulin. Your immune system attacks and destroys the cells in your pancreas that make insulin. Type 1 diabetes is usually diagnosed in children and young adults, although it can appear at any age. People with type 1 diabetes need to take insulin every day to stay alive.\n\nType 2 diabetes If you have type 2 diabetes, your body does not make or use insulin well. You can develop type 2 diabetes at any age, even during childhood. However, this type of diabetes occurs most often in middle-aged and older people. Type 2 is the most common type of diabetes.\n\nGestational diabetes Gestational diabetes develops in some women when they are pregnant. Most of the time, this type of diabetes goes away after the baby is born. However, if you\u2019ve had gestational diabetes, you have a greater chance of developing type 2 diabetes later in life. Sometimes diabetes diagnosed during pregnancy is actually type 2 diabetes.\n\nOther types of diabetes Less common types include monogenic diabetes, which is an inherited form of diabetes, and cystic fibrosis-relat","c56204aa":"# Replace missing values and EDA\u00b6\n"}}