{"cell_type":{"1f86f315":"code","75713082":"code","c1431698":"code","f2c6885d":"code","2102c32c":"code","58c2d924":"code","0260227a":"code","f001d1be":"code","2472037a":"code","d9765a3f":"code","2090b64d":"code","9c3a5672":"code","e1f0a2de":"code","5581dc7a":"code","02450591":"code","4a5b1609":"code","09b96fd3":"code","26f92add":"code","8cd88b67":"code","c80fe810":"code","afb78163":"code","0bbaf5b9":"code","c20c023f":"code","52cb0bd0":"code","510b7aa6":"code","aded0420":"code","59132828":"code","57ec997a":"code","1d4fa018":"code","443d119a":"code","34e92c81":"code","0fc297bf":"code","e89114c8":"code","caa0bd45":"code","544d454a":"code","fb0c1472":"code","441ac945":"code","95695cef":"code","21077a41":"code","108b9c1f":"code","fc5d6578":"code","363058a8":"code","5485171a":"code","14807dd1":"code","881691ef":"code","60676035":"code","17f99ab1":"code","17ee2c32":"code","7e5c924b":"code","38aae923":"code","e8e4de1d":"code","bf246812":"code","69278b06":"code","04533431":"code","01f361aa":"code","243ea579":"code","d2ab4ce2":"code","943d4da9":"markdown","28c85b75":"markdown","60e2dfca":"markdown","ec22f731":"markdown","fc65f774":"markdown","c5159799":"markdown","05045164":"markdown","e337ff8e":"markdown","d83d6176":"markdown","cf75a472":"markdown","1f2cd6cd":"markdown","ab482243":"markdown","cf5ed6b9":"markdown","00f53fea":"markdown","5bfc61f2":"markdown","306c6f54":"markdown","58a32714":"markdown","d8416697":"markdown","5d3879ca":"markdown","3f0748d4":"markdown","1031bd8f":"markdown"},"source":{"1f86f315":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os\nfrom glob import glob\nimport seaborn as sns\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nimport itertools\nimport keras\nfrom keras.utils.np_utils import to_categorical\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, MaxPooling2D\nfrom keras import backend as K\nimport itertools\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.utils.np_utils import to_categorical \nfrom keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","75713082":"base_dir = os.path.join('..', 'input\/skin-cancer-mnist-ham10000')\nimageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in glob(os.path.join(base_dir, '*', '*.jpg'))}\n\n\ntype_dict = {\n    'nv': 'Melanocytic nevi',\n    'mel': 'Melanoma',\n    'bkl': 'Benign keratosis-like lesions ',\n    'bcc': 'Basal cell carcinoma',\n    'akiec': 'Actinic keratoses',\n    'vasc': 'Vascular lesions',\n    'df': 'Dermatofibroma'\n}\n","c1431698":"metadata = pd.read_csv(\"\/kaggle\/input\/skin-cancer-mnist-ham10000\/HAM10000_metadata.csv\")\nmetadata.head()","f2c6885d":"metadata['cell'] = metadata['dx'].map(type_dict.get) \nmetadata['cell_id'] = pd.Categorical(metadata['cell']).codes\nmetadata['path'] = metadata['image_id'].map(imageid_path_dict.get)","2102c32c":"metadata.head()","58c2d924":"metadata.tail()","0260227a":"metadata.info()","f001d1be":"metadata.isnull().sum()","2472037a":"metadata = metadata.dropna(how='any')\nmetadata.isnull().sum()","d9765a3f":"data = metadata.cell\nplt.subplots(figsize=(30,15))\nwordcloud = WordCloud(\n                          background_color='white',\n                          width=1024,\n                          height=512\n                         ).generate(\" \".join(data))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.savefig('graph_cell.png')\n\nplt.show()","2090b64d":"fig, ax1 = plt.subplots(1, 1, figsize= (10, 5))\nmetadata['cell'].value_counts().plot(kind='bar', ax=ax1 , color=\"red\" )\nplt.savefig('bar1.png')\nplt.show()\n\nfig, ax1 = plt.subplots(1, 1, figsize= (10, 5))\nmetadata['dx_type'].value_counts().plot(kind='bar' , ax=ax1 , color=\"green\")\nplt.savefig('bar2.png')\nplt.show()\n\nfig, ax1 = plt.subplots(1, 1, figsize= (10, 5))\nmetadata['localization'].value_counts().plot(kind='bar' , ax=ax1 , color=\"blue\")\nplt.savefig('bar3.png')\nplt.show()\n","9c3a5672":"import plotly.express as px\nfig = px.scatter_3d(metadata, x='localization', y='cell', z='sex', color='cell')\nfig.show()\n","e1f0a2de":"import plotly.express as px\nfig = px.scatter_3d(metadata, x='localization', y='cell', z='age', color='cell')\nfig.show()\n","5581dc7a":"fig, ax1 = plt.subplots(2, 2, figsize= (50, 50) )\npercent = metadata['sex'].value_counts() \/ 100\nlabels = \"male\" , \"female\" , \"unknown\"\nax1[0,0].pie(percent , labels=labels , startangle=180 , autopct='%1.1f%%' ,textprops={ 'fontsize': 19 , 'rotation':0}, shadow=True, radius=1.25)\npercent = metadata['cell'].value_counts() \/ 100\nlabels = \"Melanocytic nevi\" , \"Melanoma \" , \"Benign keratosis-like lesions \" , \"Basal cell carcinoma\" , \"Actinic keratoses\" , \"Vascular lesions\"  ,\"Dermatofibroma\"\nax1[0,1].pie(percent , labels=labels , startangle=180 , autopct='%1.1f%%'  ,textprops={'fontsize': 19 , 'rotation':0}, shadow=True, radius=1.25)\npercent = metadata['dx_type'].value_counts() \/ 100\nlabels = \"histo\" , \"follow_up\" , \"consensus\" , \"confocal\"\nax1[1,0].pie(percent , labels=labels , startangle=180 , autopct='%1.1f%%'  ,textprops={'fontsize': 19 , 'rotation':0}, shadow=True, radius=1.25)\npercent = metadata['localization'].value_counts() \/ 100\nlabels = \"back\" , \"lower extremity\" , \"trunk\" , \"upper extremity \" ,\"abdomen\" , \"face\" ,\"chest\" , \"foot\" , \"unknown\" , \"neck\" , \"scalp\" , \"hand\" , \"ear\" , \"genital\" , \"acral\"\nax1[1,1].pie(percent , labels=labels , startangle=180, autopct='%1.1f%%' ,textprops={'fontsize': 19 , 'rotation':0}, shadow=True, radius=1.25)\n\nfig.savefig('pie.png', dpi=300)\nplt.show()","02450591":"metadata['image'] = metadata['path'].map(lambda x: np.asarray(Image.open(x).resize((100,100))))","4a5b1609":"metadata.head()","09b96fd3":"n_samples = 7\nfig, m_axs = plt.subplots(7, n_samples, figsize = (7*n_samples, 7*7))\nfor n_axs, (type_name, type_rows) in zip(m_axs,metadata.sort_values(['cell']).groupby('cell')):\n    n_axs[0].set_title(type_name)\n    for c_ax, (_, c_row) in zip(n_axs, type_rows.sample(n_samples, random_state=77).iterrows()):\n        c_ax.imshow(c_row['image'])\nfig.savefig('category_samples.png', dpi=300)","26f92add":"X_train = metadata.drop(columns=['cell_id'],axis=1)\nY_train = metadata['cell_id']\n\nx_orjinal_train , x_orjinal_test, y_orjinal_train, y_orjinal_test = train_test_split(X_train, Y_train, test_size=0.2,random_state=77)\n\n\ny_train = to_categorical(y_orjinal_train, num_classes = 7)\ny_test = to_categorical(y_orjinal_test, num_classes = 7)\n\nx_train = np.asarray(x_orjinal_train['image'].tolist())\nx_test = np.asarray(x_orjinal_test['image'].tolist())\n\nx_train_mean = np.mean(x_train)\nx_train_std = np.std(x_train)\n\nx_test_mean = np.mean(x_test)\nx_test_std = np.std(x_test)\n\nx_train = (x_train - x_train_mean)\/x_train_std\nx_test = (x_test - x_test_mean)\/x_test_std\n\n\nx_train, x_validate, y_train, y_validate = train_test_split(x_train, y_train, test_size = 0.1, random_state = 2)\n\nx_train = x_train.reshape(x_train.shape[0], *(100, 100, 3))\nx_test = x_test.reshape(x_test.shape[0], *(100, 100, 3))\nx_validate = x_validate.reshape(x_validate.shape[0], *(100, 100, 3))","8cd88b67":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\ndatagen.fit(x_train)\n","c80fe810":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","afb78163":"from keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\ndef swish(x):\n    return (K.sigmoid(x) * x)\n\nget_custom_objects().update({'swish': Activation(swish)})\n\nclassifier = Sequential()\n\nclassifier.add(Conv2D(32, kernel_size= (3,3), activation= 'swish', padding= 'Same', input_shape = (100, 100, 3)))\nclassifier.add(Conv2D(32, kernel_size= (3,3), activation= 'swish', padding= 'Same'))\nclassifier.add(MaxPooling2D(pool_size=(2,2)))\nclassifier.add(Dropout(0.25))\n\nclassifier.add(Conv2D(64, kernel_size= (3,3), activation= 'swish', padding= 'Same'))\nclassifier.add(Conv2D(64, kernel_size= (3,3), activation= 'swish', padding= 'Same'))\nclassifier.add(MaxPooling2D(pool_size=(2,2)))\nclassifier.add(Dropout(0.25))\n\n\nclassifier.add(Flatten())\nclassifier.add(Dense(128, activation= 'swish'))\nclassifier.add(Dropout(0.5))\n\nclassifier.add(Dense(7, activation= 'softmax'))\nclassifier.summary()\n\nclassifier.compile(optimizer= keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False), loss='categorical_crossentropy', metrics = ['accuracy'])\n","0bbaf5b9":"epochs = 100\nbatch_size = 32\nhistory = classifier.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (x_validate,y_validate),\n                              verbose = 1, steps_per_epoch=x_train.shape[0] \/\/ batch_size, \n                              callbacks=[learning_rate_reduction])","c20c023f":"loss, accuracy = classifier.evaluate(x_test, y_test, verbose=1)\nloss_v, accuracy_v = classifier.evaluate(x_validate, y_validate, verbose=1)\nprint(\"Validation: accuracy = %f  ;  loss_v = %f\" % (accuracy_v, loss_v))\nprint(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n","52cb0bd0":"fig, ax1 = plt.subplots(figsize= (15, 10) )\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.savefig(\"acc_swish_adam.png\")\nplt.show()\n\nfig, ax1 = plt.subplots(figsize= (15, 10) )\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.savefig(\"loss_swish_adam.png\")\nplt.show()","510b7aa6":"Y_pred = classifier.predict(x_validate)\nY_pred_classes = np.argmax(Y_pred,axis = 1) \nY_true = np.argmax(y_validate,axis = 1) \ncm = confusion_matrix(Y_true, Y_pred_classes)\n\nlabel = pd.unique(metadata['cell'])\nf,ax = plt.subplots(figsize=(18, 18))\nax = sns.heatmap(cm, xticklabels=label, yticklabels=label, linewidths=.5 , cbar=False , fmt=\"d\" , annot=True)\nplt.savefig(\"corr_cnn_swish_adam.png\")\nplt.show()","aded0420":"label_error = 1 - np.diag(cm) \/ np.sum(cm, axis=1)\nplt.figure(figsize=(25,10))\nplt.bar(label,label_error)\nplt.xlabel('True Label')\nplt.ylabel('Classified incorrectly')\nplt.savefig(\"cnn_bar_swish_adam.png\")","59132828":"classifier = Sequential()\n\nclassifier.add(Conv2D(32, kernel_size= (3,3), activation= 'relu', padding= 'Same', input_shape = (100, 100, 3)))\nclassifier.add(Conv2D(32, kernel_size= (3,3), activation= 'relu', padding= 'Same'))\nclassifier.add(MaxPooling2D(pool_size=(2,2)))\nclassifier.add(Dropout(0.25))\n\nclassifier.add(Conv2D(64, kernel_size= (3,3), activation= 'relu', padding= 'Same'))\nclassifier.add(Conv2D(64, kernel_size= (3,3), activation= 'relu', padding= 'Same'))\nclassifier.add(MaxPooling2D(pool_size=(2,2)))\nclassifier.add(Dropout(0.25))\n\n\nclassifier.add(Flatten())\nclassifier.add(Dense(128, activation= 'relu'))\nclassifier.add(Dropout(0.5))\n\nclassifier.add(Dense(7, activation= 'softmax'))\nclassifier.summary()\n\nclassifier.compile(optimizer= keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False), loss='categorical_crossentropy', metrics = ['accuracy'])\n","57ec997a":"epochs = 100\nbatch_size = 32\nhistory = classifier.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (x_validate,y_validate),\n                              verbose = 1, steps_per_epoch=x_train.shape[0] \/\/ batch_size, \n                              callbacks=[learning_rate_reduction])","1d4fa018":"loss, accuracy = classifier.evaluate(x_test, y_test, verbose=1)\nloss_v, accuracy_v = classifier.evaluate(x_validate, y_validate, verbose=1)\nprint(\"Validation: accuracy = %f  ;  loss_v = %f\" % (accuracy_v, loss_v))\nprint(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n","443d119a":"fig, ax1 = plt.subplots(figsize= (15, 10) )\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.savefig(\"acc_relu_adam.png\")\nplt.show()\n\nfig, ax1 = plt.subplots(figsize= (15, 10) )\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.savefig(\"loss_relu_adam.png\")\nplt.show()","34e92c81":"Y_pred = classifier.predict(x_validate)\nY_pred_classes = np.argmax(Y_pred,axis = 1) \nY_true = np.argmax(y_validate,axis = 1) \ncm = confusion_matrix(Y_true, Y_pred_classes)\n\nlabel = pd.unique(metadata['cell'])\nf,ax = plt.subplots(figsize=(18, 18))\nax = sns.heatmap(cm, xticklabels=label, yticklabels=label, linewidths=.5 , cbar=False , fmt=\"d\" , annot=True)\nplt.savefig(\"corr_cnn_relu_adam.png\")\nplt.show()","0fc297bf":"label_error = 1 - np.diag(cm) \/ np.sum(cm, axis=1)\nplt.figure(figsize=(25,10))\nplt.bar(label,label_error)\nplt.xlabel('True Label')\nplt.ylabel('Classified incorrectly')\nplt.savefig(\"cnn_bar_relu_adam.png\")","e89114c8":"classifier = Sequential()\n\nclassifier.add(Conv2D(32, kernel_size= (3,3), activation= 'tanh', padding= 'Same', input_shape = (100, 100, 3)))\nclassifier.add(Conv2D(32, kernel_size= (3,3), activation= 'tanh', padding= 'Same'))\nclassifier.add(MaxPooling2D(pool_size=(2,2)))\nclassifier.add(Dropout(0.25))\n\nclassifier.add(Conv2D(64, kernel_size= (3,3), activation= 'tanh', padding= 'Same'))\nclassifier.add(Conv2D(64, kernel_size= (3,3), activation= 'tanh', padding= 'Same'))\nclassifier.add(MaxPooling2D(pool_size=(2,2)))\nclassifier.add(Dropout(0.25))\n\n\nclassifier.add(Flatten())\nclassifier.add(Dense(128, activation= 'tanh'))\nclassifier.add(Dropout(0.5))\n\nclassifier.add(Dense(7, activation= 'softmax'))\nclassifier.summary()\n\nclassifier.compile(optimizer= keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False), loss='categorical_crossentropy', metrics = ['accuracy'])\n\n","caa0bd45":"epochs = 100\nbatch_size = 32\nhistory = classifier.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (x_validate,y_validate),\n                              verbose = 1, steps_per_epoch=x_train.shape[0] \/\/ batch_size, \n                              callbacks=[learning_rate_reduction])","544d454a":"\nloss, accuracy = classifier.evaluate(x_test, y_test, verbose=1)\nloss_v, accuracy_v = classifier.evaluate(x_validate, y_validate, verbose=1)\nprint(\"Validation: accuracy = %f  ;  loss_v = %f\" % (accuracy_v, loss_v))\nprint(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))\n","fb0c1472":"fig, ax1 = plt.subplots(figsize= (15, 10) )\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.savefig(\"acc_tanh_adam.png\")\nplt.show()\n\nfig, ax1 = plt.subplots(figsize= (15, 10) )\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.savefig(\"loss_tanh_adam.png\")\nplt.show()","441ac945":"Y_pred = classifier.predict(x_validate)\nY_pred_classes = np.argmax(Y_pred,axis = 1) \nY_true = np.argmax(y_validate,axis = 1) \ncm = confusion_matrix(Y_true, Y_pred_classes)\n\nlabel = pd.unique(metadata['cell'])\nf,ax = plt.subplots(figsize=(18, 18))\nax = sns.heatmap(cm, xticklabels=label, yticklabels=label, linewidths=.5 , cbar=False , fmt=\"d\" , annot=True)\nplt.savefig(\"corr_cnn_tanh_adam.png\")\nplt.show()","95695cef":"label_error = 1 - np.diag(cm) \/ np.sum(cm, axis=1)\nplt.figure(figsize=(25,10))\nplt.bar(label,label_error)\nplt.xlabel('True Label')\nplt.ylabel('Classified incorrectly')\nplt.savefig(\"cnn_bar_tanh_adam.png\")","21077a41":"from keras.utils.generic_utils import get_custom_objects\nfrom keras.layers import Activation\ndef swish(x):\n    return (K.sigmoid(x) * x)\n\nget_custom_objects().update({'swish': Activation(swish)})\n\nclassifier = Sequential()\n\nclassifier.add(Conv2D(32, kernel_size= (3,3), activation= 'swish', padding= 'Same', input_shape = (100, 100, 3)))\nclassifier.add(Conv2D(32, kernel_size= (3,3), activation= 'swish', padding= 'Same'))\nclassifier.add(MaxPooling2D(pool_size=(2,2)))\nclassifier.add(Dropout(0.25))\n\nclassifier.add(Conv2D(64, kernel_size= (3,3), activation= 'swish', padding= 'Same'))\nclassifier.add(Conv2D(64, kernel_size= (3,3), activation= 'swish', padding= 'Same'))\nclassifier.add(MaxPooling2D(pool_size=(2,2)))\nclassifier.add(Dropout(0.25))\n\n\nclassifier.add(Flatten())\nclassifier.add(Dense(128, activation= 'swish'))\nclassifier.add(Dropout(0.5))\n\nclassifier.add(Dense(7, activation= 'softmax'))\nclassifier.summary()\n\nclassifier.compile(optimizer= keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6), loss='categorical_crossentropy', metrics = ['accuracy'])\n\n","108b9c1f":"epochs = 100\nbatch_size = 32\nhistory = classifier.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (x_validate,y_validate),\n                              verbose = 1, steps_per_epoch=x_train.shape[0] \/\/ batch_size, \n                              callbacks=[learning_rate_reduction])","fc5d6578":"loss, accuracy = classifier.evaluate(x_test, y_test, verbose=1)\nloss_v, accuracy_v = classifier.evaluate(x_validate, y_validate, verbose=1)\nprint(\"Validation: accuracy = %f  ;  loss_v = %f\" % (accuracy_v, loss_v))\nprint(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))","363058a8":"fig, ax1 = plt.subplots(figsize= (15, 10) )\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.savefig(\"acc_swish_rms.png\")\nplt.show()\n\nfig, ax1 = plt.subplots(figsize= (15, 10) )\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.savefig(\"loss_swish_rms.png\")\nplt.show()\n","5485171a":"Y_pred = classifier.predict(x_validate)\nY_pred_classes = np.argmax(Y_pred,axis = 1) \nY_true = np.argmax(y_validate,axis = 1) \ncm = confusion_matrix(Y_true, Y_pred_classes)\n\nlabel = pd.unique(metadata['cell'])\nf,ax = plt.subplots(figsize=(18, 18))\nax = sns.heatmap(cm, xticklabels=label, yticklabels=label, linewidths=.5 , cbar=False , fmt=\"d\" , annot=True)\nplt.savefig(\"corr_cnn_swish_rms.png\")\nplt.show()\n","14807dd1":"label_error = 1 - np.diag(cm) \/ np.sum(cm, axis=1)\nplt.figure(figsize=(25,10))\nplt.bar(label,label_error)\nplt.xlabel('True Label')\nplt.ylabel('Classified incorrectly')\nplt.savefig(\"cnn_bar_swish_rms.png\")","881691ef":"classifier = Sequential()\n\nclassifier.add(Conv2D(32, kernel_size= (3,3), activation= 'relu', padding= 'Same', input_shape = (100, 100, 3)))\nclassifier.add(Conv2D(32, kernel_size= (3,3), activation= 'relu', padding= 'Same'))\nclassifier.add(MaxPooling2D(pool_size=(2,2)))\nclassifier.add(Dropout(0.25))\n\nclassifier.add(Conv2D(64, kernel_size= (3,3), activation= 'relu', padding= 'Same'))\nclassifier.add(Conv2D(64, kernel_size= (3,3), activation= 'relu', padding= 'Same'))\nclassifier.add(MaxPooling2D(pool_size=(2,2)))\nclassifier.add(Dropout(0.25))\n\n\nclassifier.add(Flatten())\nclassifier.add(Dense(128, activation= 'relu'))\nclassifier.add(Dropout(0.5))\n\nclassifier.add(Dense(7, activation= 'softmax'))\nclassifier.summary()\n\nclassifier.compile(optimizer= keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6), loss='categorical_crossentropy', metrics = ['accuracy'])\n\n","60676035":"epochs = 100\nbatch_size = 32\nhistory = classifier.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (x_validate,y_validate),\n                              verbose = 1, steps_per_epoch=x_train.shape[0] \/\/ batch_size, \n                              callbacks=[learning_rate_reduction])","17f99ab1":"loss, accuracy = classifier.evaluate(x_test, y_test, verbose=1)\nloss_v, accuracy_v = classifier.evaluate(x_validate, y_validate, verbose=1)\nprint(\"Validation: accuracy = %f  ;  loss_v = %f\" % (accuracy_v, loss_v))\nprint(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))","17ee2c32":"fig, ax1 = plt.subplots(figsize= (15, 10) )\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.savefig(\"acc_relu_rms.png\")\nplt.show()\n\nfig, ax1 = plt.subplots(figsize= (15, 10) )\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.savefig(\"loss_relu_rms.png\")\nplt.show()","7e5c924b":"Y_pred = classifier.predict(x_validate)\nY_pred_classes = np.argmax(Y_pred,axis = 1) \nY_true = np.argmax(y_validate,axis = 1) \ncm = confusion_matrix(Y_true, Y_pred_classes)\n\nlabel = pd.unique(metadata['cell'])\nf,ax = plt.subplots(figsize=(18, 18))\nax = sns.heatmap(cm, xticklabels=label, yticklabels=label, linewidths=.5 , cbar=False , fmt=\"d\" , annot=True)\nplt.savefig(\"corr_cnn_relu_rms.png\")\nplt.show()","38aae923":"label_error = 1 - np.diag(cm) \/ np.sum(cm, axis=1)\nplt.figure(figsize=(25,10))\nplt.bar(label,label_error)\nplt.xlabel('True Label')\nplt.ylabel('Classified incorrectly')\nplt.savefig(\"cnn_bar_relu_rms.png\")","e8e4de1d":"classifier = Sequential()\n\nclassifier.add(Conv2D(32, kernel_size= (3,3), activation= 'tanh', padding= 'Same', input_shape = (100, 100, 3)))\nclassifier.add(Conv2D(32, kernel_size= (3,3), activation= 'tanh', padding= 'Same'))\nclassifier.add(MaxPooling2D(pool_size=(2,2)))\nclassifier.add(Dropout(0.25))\n\nclassifier.add(Conv2D(64, kernel_size= (3,3), activation= 'tanh', padding= 'Same'))\nclassifier.add(Conv2D(64, kernel_size= (3,3), activation= 'tanh', padding= 'Same'))\nclassifier.add(MaxPooling2D(pool_size=(2,2)))\nclassifier.add(Dropout(0.25))\n\n\nclassifier.add(Flatten())\nclassifier.add(Dense(128, activation= 'tanh'))\nclassifier.add(Dropout(0.5))\n\nclassifier.add(Dense(7, activation= 'softmax'))\nclassifier.summary()\n\nclassifier.compile(optimizer= keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6), loss='categorical_crossentropy', metrics = ['accuracy'])\n","bf246812":"epochs = 100\nbatch_size = 32\nhistory = classifier.fit_generator(datagen.flow(x_train,y_train, batch_size=batch_size),\n                              epochs = epochs, validation_data = (x_validate,y_validate),\n                              verbose = 1, steps_per_epoch=x_train.shape[0] \/\/ batch_size, \n                              callbacks=[learning_rate_reduction])","69278b06":"loss, accuracy = classifier.evaluate(x_test, y_test, verbose=1)\nloss_v, accuracy_v = classifier.evaluate(x_validate, y_validate, verbose=1)\nprint(\"Validation: accuracy = %f  ;  loss_v = %f\" % (accuracy_v, loss_v))\nprint(\"Test: accuracy = %f  ;  loss = %f\" % (accuracy, loss))","04533431":"fig, ax1 = plt.subplots(figsize= (15, 10) )\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.savefig(\"acc_tanh_rms.png\")\nplt.show()\n\nfig, ax1 = plt.subplots(figsize= (15, 10) )\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.savefig(\"loss_tanh_rms.png\")\nplt.show()\n\n","01f361aa":"Y_pred = classifier.predict(x_validate)\nY_pred_classes = np.argmax(Y_pred,axis = 1) \nY_true = np.argmax(y_validate,axis = 1) \ncm = confusion_matrix(Y_true, Y_pred_classes)\n\nlabel = pd.unique(metadata['cell'])\nf,ax = plt.subplots(figsize=(18, 18))\nax = sns.heatmap(cm, xticklabels=label, yticklabels=label, linewidths=.5 , cbar=False , fmt=\"d\" , annot=True)\nplt.savefig(\"corr_cnn_tanh_rms.png\")\nplt.show()\n","243ea579":"label_error = 1 - np.diag(cm) \/ np.sum(cm, axis=1)\nplt.figure(figsize=(25,10))\nplt.bar(label,label_error)\nplt.xlabel('True Label')\nplt.ylabel('Classified incorrectly')\nplt.savefig(\"cnn_bar_tanh_rms.png\")","d2ab4ce2":"from keras.utils import plot_model\nplot_model(classifier)","943d4da9":"--------------------------------------------","28c85b75":"## Tanh + RMSprop","60e2dfca":"## Swish + RMSprop","ec22f731":"-----------------","fc65f774":"# Data Read","c5159799":"--------------------------","05045164":"# Conclusion","e337ff8e":"## Relu + RMSprop","d83d6176":"# Convolutional Neural Network","cf75a472":"# Introduction \n\nHello, welcome to our kernel. \nOur goal in this kernel is to predict the type of cancer using the deep learning algorithm. The algorithm we use is convolution neural network.","1f2cd6cd":"![image.png](attachment:image.png)","ab482243":"-----------------","cf5ed6b9":"## Swish + Adam","00f53fea":"## Relu + Adam","5bfc61f2":"## Tanh + Adam","306c6f54":"# Data Preprocessing","58a32714":"-----------------------------------","d8416697":"# Data Visualization","5d3879ca":"-------------------------------","3f0748d4":"\n-------------------------","1031bd8f":"We used 2 different optimizations and 3 different activations to train our model. Our results are given in the table."}}