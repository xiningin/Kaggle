{"cell_type":{"4961266d":"code","de117d10":"code","d409a955":"code","8960d4bf":"code","dc602baa":"code","bde47934":"code","be726bf9":"code","76a56637":"code","33babd94":"code","c92b1dbb":"code","831f860c":"code","39964c91":"code","16c8a0af":"code","2529e4c3":"code","ff37ddf8":"code","2402aabb":"code","c58d9119":"code","9d13a869":"code","51a36677":"code","d4ccbb14":"code","37facd52":"code","fc874895":"code","279fc236":"code","d7c57604":"code","74ee57ea":"code","4eae928a":"code","d9d3068a":"code","e3e0160a":"code","a527e495":"code","0f9e5584":"code","efd2b884":"code","27558e13":"code","f5db5f8a":"code","aa0aa743":"code","df7406b1":"code","5a6e8b25":"code","88ad214d":"code","a191ddac":"code","ac5aeb7d":"code","de0555dc":"code","324bbb02":"code","67a8b49c":"code","70ddc73b":"code","253768d7":"code","0fd23e75":"code","9e974dd4":"code","6c5aa57a":"code","8d5a8c20":"code","0fa34384":"code","40e4c316":"code","e69ecb86":"code","711ebd95":"code","36f0e4ea":"code","68ab1939":"code","2322386a":"code","0a87d7be":"code","a3a4bb43":"code","9610499a":"code","132cea01":"code","9f9cbc5c":"code","a3cd6917":"code","d7071daf":"code","79ee9968":"code","3d9c75b0":"code","0b65ea49":"code","5b5ecf1d":"code","03114419":"code","62ac3076":"code","d4748684":"code","42e49b41":"code","0322f161":"code","21d9835d":"code","e1f23789":"code","7937eabd":"markdown","425185dc":"markdown","55658530":"markdown","54380e23":"markdown","c98d853c":"markdown","d9bcf835":"markdown","db606d74":"markdown","9bb7ee1f":"markdown","82a06b51":"markdown","3e536250":"markdown","d70b3676":"markdown","cf1c8ece":"markdown","5dfb1166":"markdown","d106215f":"markdown","b449d3c7":"markdown","10efb648":"markdown","a4c93d96":"markdown","e29a0aba":"markdown","63fa0f0c":"markdown","bb7f6dd8":"markdown","b514211b":"markdown","c5baa57a":"markdown","341f3e8a":"markdown","4f5b0efd":"markdown","d0a3072a":"markdown","0d39eb8e":"markdown","ad998e33":"markdown","a1e958bf":"markdown","a8cc1846":"markdown","5f23a5c6":"markdown","0e53f760":"markdown","322e8201":"markdown","ceaa5ae3":"markdown","bcdbd0a7":"markdown","a6a5e710":"markdown","0b90ae02":"markdown"},"source":{"4961266d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","de117d10":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, KFold, GridSearchCV\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score\nfrom gensim.corpora.dictionary import Dictionary\nfrom gensim.models import LdaModel, LsiModel, TfidfModel, Word2Vec, FastText, Doc2Vec\nimport re\nimport string\nimport nltk\nnltk.download('wordnet')\nnltk.download('punkt')\nfrom nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords\n\nfrom sklearn.decomposition import PCA, KernelPCA, IncrementalPCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d409a955":"df = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ndf_test  = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ndf.sample(10)","8960d4bf":"df_test.sample(10)","dc602baa":"print(\"Training :\")\nprint(\"Length of the data :\", len(df))\nprint(df.isnull().sum())","bde47934":"print(\"Test :\")\nprint(\"Length of the data :\", len(df_test))\nprint(df_test.isnull().sum())","be726bf9":"target_values = df['target'].value_counts()\nsns.barplot(target_values.index, target_values)\nplt.gca().set_ylabel('samples')","76a56637":"df['target_mean'] = df.groupby('keyword')['target'].transform('mean')\n\nfig = plt.figure(figsize=(8, 72), dpi=100)\n\nsns.countplot(y=df.sort_values(by='target_mean', ascending=False)['keyword'],\n              hue=df.sort_values(by='target_mean', ascending=False)['target'])\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()\n\ndf.drop(columns=['target_mean'], inplace=True)","33babd94":"tokens = word_tokenize(df[\"text\"][0])\ntokens = [word.lower() for word in tokens]\nprint(df[\"text\"][0])\nprint(tokens)","c92b1dbb":"words = [word for word in tokens if word.isalpha()]\nprint(words)","831f860c":"stop_words = set(stopwords.words(\"english\"))\nwords = [word for word in words if not word in stop_words]\nprint(words)","39964c91":"porter = PorterStemmer()\nstemmed = [porter.stem(word) for word in words]\nprint(stemmed)","16c8a0af":"lemmatizer = WordNetLemmatizer()\nlemmatized = [lemmatizer.lemmatize(word) for word in words]\nprint(lemmatized)","2529e4c3":"# \u0443\u0431\u0440\u0430\u0442\u044c \u0441\u0441\u044b\u043b\u043a\u0438\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\n# \u0443\u0431\u0440\u0430\u0442\u044c HTML \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n\n# \u0443\u0431\u0440\u0430\u0442\u044c \u044d\u043c\u043e\u0434\u0437\u0438\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n# \u043d\u0430\u043f\u0438\u0448\u0435\u043c \u043d\u0430\u0448\u0443 \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u0438, \u0441 \u0443\u0447\u0435\u0442\u043e\u043c \u0432\u0441\u0435\u0433\u043e \u0440\u0430\u0441\u0441\u043c\u043e\u0442\u0440\u0435\u043d\u043d\u043e\u0433\u043e\nstop_words = set(stopwords.words(\"english\"))\ndef mytokenize(text, normalize=None, stop_words=stop_words):\n    text = remove_html(text)\n    text = remove_URL(text)\n    text = remove_emoji(text)\n    text = text.lower()\n    text = word_tokenize(text)\n    text = [word for word in text if word.isalpha()]\n    text = [word for word in text if not word in stop_words]\n    if normalize == \"s\":\n        porter = PorterStemmer()\n        text = [porter.stem(word) for word in text]\n    if normalize == \"l\":\n        lemmatizer = WordNetLemmatizer()\n        text = [lemmatizer.lemmatize(word) for word in text]\n    return text\n\nfrom functools import partial\n \nmytokenize_s = partial(mytokenize, normalize='s')\nmytokenize_l = partial(mytokenize, normalize='l')\n\n# \u0443\u0431\u0440\u0430\u0442\u044c \u043f\u0443\u043d\u043a\u0442\u0443\u0430\u0446\u0438\u044e. \u0412 \u0446\u0435\u043b\u043e\u043c \u043f\u0440\u0438 \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0430\u0446\u0438\u0438 \u0438 \u043e\u0447\u0438\u0442\u0441\u043a\u0435 \u043e\u0442 \"\u043d\u0435\u0441\u043b\u043e\u0432\" \u0432\u044b \u0438 \u0442\u0430\u043a \u0443\u0436\u0435 \u043e\u0442 \u043d\u0435\u0435 \u0438\u0437\u0431\u0430\u0432\u0438\u0442\u0435\u0441\u044c\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\ndf['tokens'] = df['text'].apply(mytokenize_l)\ndf_test['tokens'] = df_test['text'].apply(mytokenize_l)","ff37ddf8":"df['tokens']","2402aabb":"def join_list(tab):\n    return \" \".join(tab)\ndf[\"text_preprocessed\"] = df[\"tokens\"].apply(join_list)\ndf_test[\"text_preprocessed\"] = df_test[\"tokens\"].apply(join_list)\n\ndef transform_keyword(word) :\n    # Split \u043f\u043e %20 (\u043f\u0440\u043e\u0431\u0435\u043b)\n    return word.split('%20')\n\n# Transform NaN value to empty string\ndf[\"keyword\"] = df.keyword.fillna(\" \")\ndf_test[\"keyword\"] = df_test.keyword.fillna(\" \")\n\ndf[\"keyword\"] = df[\"keyword\"].apply(transform_keyword).apply(join_list)\ndf_test[\"keyword\"] = df_test[\"keyword\"].apply(transform_keyword).apply(join_list)\n\n# Concant keyword to the phrases\ndf[\"text_preprocessed\"] = df[\"keyword\"] + \" \" + df[\"text_preprocessed\"] \ndf_test[\"text_preprocessed\"] = df_test[\"keyword\"] + \" \" + df_test[\"text_preprocessed\"] \n\ndf[\"tokens\"] = df[\"text_preprocessed\"].apply(str.split)\ndf_test[\"tokens\"] = df_test[\"text_preprocessed\"].apply(str.split)\n\ndf.sample()","c58d9119":"df[\"text_preprocessed\"][:2]","9d13a869":"X_all = pd.concat([df[\"text_preprocessed\"], df_test[\"text_preprocessed\"]])\n\nsk_doc2bow = CountVectorizer()\nsk_doc2bow.fit(X_all)\ndel X_all\n# \u0440\u0430\u0431\u043e\u0442\u0430\u0435\u043c \u0441 \u043d\u0435 \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c \u0442\u0435\u043a\u0441\u0442\u043e\u043c\nX = sk_doc2bow.transform(df[\"text_preprocessed\"])\nX_test = sk_doc2bow.transform(df_test[\"text_preprocessed\"])\n\nX_train, X_val, y_train, y_val = train_test_split(X, df[\"target\"], test_size=0.1, random_state=42)","51a36677":"print(len(X_train[0].toarray()[0]))\nrow_d2b = X_train[0].toarray()[0]\nprint(row_d2b)\nrow_d2b[row_d2b != 0]","d4ccbb14":"df[\"tokens\"][:1].values","37facd52":"model = SVC(C=2, gamma=0.4, kernel='rbf').fit(X_train, y_train)\ny_val_pred = model.predict(X_val)\nprint(accuracy_score(y_val, y_val_pred), f1_score(y_val, y_val_pred))\nprint(confusion_matrix(y_val, y_val_pred))\nmodel = SVC(C=2, gamma=0.4, kernel='rbf').fit(X, df.target)\ny_test_pred = model.predict(X_test)\nsub_df = pd.read_csv(os.path.join('..\/input\/nlp-getting-started\/', 'sample_submission.csv'))\nsub_df[\"target\"] = y_test_pred\nsub_df.to_csv(\"submission_d2b.csv\",index=False)","fc874895":"X_all = pd.concat([df[\"text_preprocessed\"], df_test[\"text_preprocessed\"]])\n\ntfidf = TfidfVectorizer(stop_words = 'english')\ntfidf.fit(X_all)\ndel X_all\nX = tfidf.transform(df[\"text_preprocessed\"])\nX_test = tfidf.transform(df_test[\"text_preprocessed\"])\n\nX_train, X_val, y_train, y_val = train_test_split(X, df[\"target\"], test_size=0.1, random_state=42)","279fc236":"row_tf = X_train[0].toarray()[0]\nprint(row_tf.shape)\nprint(row_tf)\nrow_tf[row_tf != 0]","d7c57604":"model = SVC(C=2, gamma=0.4, kernel='rbf').fit(X_train, y_train)\ny_val_pred = model.predict(X_val)\nprint(accuracy_score(y_val, y_val_pred), f1_score(y_val, y_val_pred))\nprint(confusion_matrix(y_val, y_val_pred))\nmodel = SVC(C=2, gamma=0.4, kernel='rbf').fit(X, df.target)\ny_test_pred = model.predict(X_test)\nsub_df = pd.read_csv(os.path.join('..\/input\/nlp-getting-started\/', 'sample_submission.csv'))\nsub_df[\"target\"] = y_test_pred\nsub_df.to_csv(\"submission_tf.csv\",index=False)","74ee57ea":"X_all = pd.concat([df[\"text_preprocessed\"], df_test[\"text_preprocessed\"]])\n\ntfidf = TfidfVectorizer(stop_words = 'english', min_df=10)\ntfidf.fit(X_all)\ndel X_all\nX = tfidf.transform(df[\"text_preprocessed\"])\nX_test = tfidf.transform(df_test[\"text_preprocessed\"])\n\nX_train, X_val, y_train, y_val = train_test_split(X, df[\"target\"], test_size=0.1, random_state=42)","4eae928a":"row_tf = X_train[0].toarray()[0]\nprint(row_tf.shape)\nprint(row_tf)\nrow_tf[row_tf != 0]","d9d3068a":"model = SVC(C=2, gamma=0.4, kernel='rbf').fit(X_train, y_train)\ny_val_pred = model.predict(X_val)\nprint(accuracy_score(y_val, y_val_pred), f1_score(y_val, y_val_pred))\nprint(confusion_matrix(y_val, y_val_pred))\nmodel = SVC(C=2, gamma=0.4, kernel='rbf').fit(X, df.target)\ny_test_pred = model.predict(X_test)\nsub_df = pd.read_csv(os.path.join('..\/input\/nlp-getting-started\/', 'sample_submission.csv'))\nsub_df[\"target\"] = y_test_pred\nsub_df.to_csv(\"submission_tf.csv\",index=False)","e3e0160a":"X_all = pd.concat([df[\"text_preprocessed\"], df_test[\"text_preprocessed\"]])\ntfidf = TfidfVectorizer(stop_words = 'english', min_df=5, ngram_range=(1, 3))\ntfidf.fit(X_all)\ndel X_all\nX = tfidf.transform(df[\"text_preprocessed\"])\nX_test = tfidf.transform(df_test[\"text_preprocessed\"])\n\nX_train, X_val, y_train, y_val = train_test_split(X, df[\"target\"], test_size=0.1, random_state=42)\nrow_tf = X_train[0].toarray()[0]\nprint(row_tf.shape)\nprint(row_tf)\nrow_tf[row_tf != 0]","a527e495":"model = SVC(C=2, gamma=0.4, kernel='rbf').fit(X_train, y_train)\ny_val_pred = model.predict(X_val)\nprint(accuracy_score(y_val, y_val_pred), f1_score(y_val, y_val_pred))\nprint(confusion_matrix(y_val, y_val_pred))\nmodel = SVC(C=2, gamma=0.4, kernel='rbf').fit(X, df.target)\ny_test_pred = model.predict(X_test)\nsub_df = pd.read_csv(os.path.join('..\/input\/nlp-getting-started\/', 'sample_submission.csv'))\nsub_df[\"target\"] = y_test_pred\nsub_df.to_csv(\"submission_tf.csv\",index=False)","0f9e5584":"# \u043e\u0431\u0440\u0430\u0442\u0438\u0442\u0435 \u0432\u043d\u0438\u043c\u0430\u043d\u0438\u0435, \u0447\u0442\u043e \u0442\u0443\u0442 \u0443\u0436\u0435 \u0440\u0430\u0431\u043e\u0442\u0430\u0435 \u043c \u0442\u043e\u043a\u0435\u043d\u0430\u043c\u0438\nX_all = pd.concat([df[\"tokens\"], df_test[\"tokens\"]]).reset_index(drop=True)\nprint(len(X_all))\nmydict = Dictionary(X_all)\n# \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u043a\u043e\u0440\u043f\u0443\u0441 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 doc2bow \u043c\u043e\u0434\u0435\u043b\u0438\ncorpus = [mydict.doc2bow(text) for text in X_all]\n# \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u043a\u043e\u0440\u043f\u0443\u0441 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 tf_idf \u043c\u043e\u0434\u0435\u043b\u0438\ntf_model = TfidfModel(corpus)  # \u0442\u0443\u0442, \u043f\u0440\u0438 \u043f\u0435\u0440\u0435\u0434\u0430\u0447\u0435 \u043a\u043e\u0440\u043f\u0443\u0441\u0430 \u0441\u0440\u0430\u0437\u0443 \u043f\u0440\u043e\u0438\u0441\u0445\u043e\u0434\u0438\u0442 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438\ncorpus_tf = tf_model[corpus]  # \u0441\u043e\u0437\u0434\u0430\u0435\u043c \u043a\u043e\u0440\u043f\u0443\u0441 tf_idf\nprint(len(corpus_tf))\nlsi_model = LsiModel(corpus_tf, id2word=mydict, num_topics=200) # \u043e\u0431\u0443\u0447\u0430\u0435\u043c LSI \u043c\u043e\u0434\u0435\u043b\u044c","efd2b884":"mydict.num_docs","27558e13":"X_all[0]","f5db5f8a":"for i, word in enumerate(mydict.items()):\n    print(word)\n    if i > 9:\n        break","aa0aa743":"corpus[50]","df7406b1":"corpus_tf[50]","5a6e8b25":"lsi_model.num_topics","88ad214d":"lsi_model.show_topics(num_topics=5, num_words=10, formatted=False)","a191ddac":"lsi_model[corpus_tf[10]]","ac5aeb7d":"def transform(df):\n    corpus = [mydict.doc2bow(text) for text in df]\n    corpus = tf_model[corpus]\n    corpus = lsi_model[corpus]\n    return corpus\nprint(len(df[\"tokens\"]))\nX = transform(df[\"tokens\"])\nprint(len(X))\nX_test = transform(df_test[\"tokens\"])\nX_train, X_val, y_train, y_val = train_test_split(X, df[\"target\"], test_size=0.1, random_state=42)","de0555dc":"# model = SVC(C=2, gamma=0.4, kernel='rbf').fit(X_train, y_train)\n# y_val_pred = model.predict(X_val)\n# print(accuracy_score(y_val, y_val_pred), f1_score(y_val, y_val_pred))\n# print(confusion_matrix(y_val, y_val_pred))\n# model = SVC(C=2, gamma=0.4, kernel='rbf').fit(X, df.target)\n# y_test_pred = model.predict(X_test)\n# sub_df = pd.read_csv(os.path.join('..\/input\/nlp-getting-started\/', 'sample_submission.csv'))\n# sub_df[\"target\"] = y_test_pred\n# sub_df.to_csv(\"submission_lsi.csv\",index=False)","324bbb02":"X_train[0]","67a8b49c":"def make_vec(X, num_top):\n    matrix = np.zeros((len(X), num_top))\n    for i, row in enumerate(X):\n        matrix[i, list(map(lambda tup: tup[0], row))] = list(map(lambda tup: tup[1], row))\n    return matrix\nmake_vec(X_train, lsi_model.num_topics).shape","70ddc73b":"def transform(df, tf_model, model):\n    corpus = [mydict.doc2bow(text) for text in df]\n    corpus = tf_model[corpus]\n    corpus = model[corpus]\n    corpus = make_vec(corpus, model.num_topics)\n    return corpus\nprint(len(df[\"tokens\"]))\nX = transform(df[\"tokens\"], tf_model, lsi_model)\nprint(len(X))\nX_test = transform(df_test[\"tokens\"], tf_model, lsi_model)\n\nX_train, X_val, y_train, y_val = train_test_split(X, df[\"target\"], test_size=0.1, random_state=42)","253768d7":"model = SVC(C=2, gamma=0.4, kernel='rbf').fit(X_train, y_train)\ny_val_pred = model.predict(X_val)\nprint(accuracy_score(y_val, y_val_pred), f1_score(y_val, y_val_pred))\nprint(confusion_matrix(y_val, y_val_pred))\nmodel = SVC(C=2, gamma=0.4, kernel='rbf').fit(X, df.target)\ny_test_pred = model.predict(X_test)\nsub_df = pd.read_csv(os.path.join('..\/input\/nlp-getting-started\/', 'sample_submission.csv'))\nsub_df[\"target\"] = y_test_pred\nsub_df.to_csv(\"submission_lsi.csv\",index=False)","0fd23e75":"pca = PCA()\nX_pca = pca.fit_transform(X)\nplt.figure(figsize=(8, 8))\nsns.scatterplot(X_pca[:, 0], X_pca[:, 1], hue=df.target);\nplt.xlabel(\"\u041f\u0435\u0440\u0432\u0430\u044f \u0433\u043b\u0430\u0432\u043d\u0430\u044f \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u0430\");\nplt.ylabel(\"\u0412\u0442\u043e\u0440\u0430\u044f \u0433\u043b\u0430\u0432\u043d\u0430\u044f \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u0430\");","9e974dd4":"plt.figure(figsize=(8, 8))\nsns.scatterplot(X_pca[df.target == 0, 0], X_pca[df.target == 0, 1], hue=df[df.target==0].target);\nplt.xlabel(\"\u041f\u0435\u0440\u0432\u0430\u044f \u0433\u043b\u0430\u0432\u043d\u0430\u044f \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u0430\");\nplt.ylabel(\"\u0412\u0442\u043e\u0440\u0430\u044f \u0433\u043b\u0430\u0432\u043d\u0430\u044f \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u0430\");\n\nplt.figure(figsize=(8, 8))\nsns.scatterplot(X_pca[df.target == 1, 0], X_pca[df.target == 1, 1], hue=df[df.target==1].target);\nplt.xlabel(\"\u041f\u0435\u0440\u0432\u0430\u044f \u0433\u043b\u0430\u0432\u043d\u0430\u044f \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u0430\");\nplt.ylabel(\"\u0412\u0442\u043e\u0440\u0430\u044f \u0433\u043b\u0430\u0432\u043d\u0430\u044f \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u0430\");","6c5aa57a":"pca = TSNE()\nX_pca = pca.fit_transform(X)\nplt.figure(figsize=(8, 8))\nsns.scatterplot(X_pca[:, 0], X_pca[:, 1], hue=df.target);\nplt.xlabel(\"\u041f\u0435\u0440\u0432\u0430\u044f \u0433\u043b\u0430\u0432\u043d\u0430\u044f \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u0430\");\nplt.ylabel(\"\u0412\u0442\u043e\u0440\u0430\u044f \u0433\u043b\u0430\u0432\u043d\u0430\u044f \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u0430\");","8d5a8c20":"# pca = TSNE(perplexity=5)\n# X_pca = pca.fit_transform(X)\n# plt.figure(figsize=(8, 8))\n# sns.scatterplot(X_pca[:, 0], X_pca[:, 1], hue=df.target);\n# plt.xlabel(\"\u041f\u0435\u0440\u0432\u0430\u044f \u0433\u043b\u0430\u0432\u043d\u0430\u044f \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u0430\");\n# plt.ylabel(\"\u0412\u0442\u043e\u0440\u0430\u044f \u0433\u043b\u0430\u0432\u043d\u0430\u044f \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u0430\");","0fa34384":"# pca = TSNE(perplexity=50)\n# X_pca = pca.fit_transform(X)\n# plt.figure(figsize=(8, 8))\n# sns.scatterplot(X_pca[:, 0], X_pca[:, 1], hue=df.target);\n# plt.xlabel(\"\u041f\u0435\u0440\u0432\u0430\u044f \u0433\u043b\u0430\u0432\u043d\u0430\u044f \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u0430\");\n# plt.ylabel(\"\u0412\u0442\u043e\u0440\u0430\u044f \u0433\u043b\u0430\u0432\u043d\u0430\u044f \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u0430\");","40e4c316":"model = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42, n_jobs=-1).fit(X_train, y_train)\ny_val_pred = model.predict(X_val)\nprint(accuracy_score(y_val, y_val_pred), f1_score(y_val, y_val_pred))\nprint(confusion_matrix(y_val, y_val_pred))\nmodel = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42, n_jobs=-1).fit(X, df.target)\ny_test_pred = model.predict(X_test)\nsub_df = pd.read_csv(os.path.join('..\/input\/nlp-getting-started\/', 'sample_submission.csv'))\nsub_df[\"target\"] = y_test_pred\nsub_df.to_csv(\"submission_lsi_rf.csv\",index=False)","e69ecb86":"lda_model = LdaModel(corpus_tf, id2word=mydict, num_topics=100, dtype=np.float64) # \u043e\u0431\u0443\u0447\u0430\u0435\u043c LDA \u043c\u043e\u0434\u0435\u043b\u044c","711ebd95":"lda_model.num_topics","36f0e4ea":"lda_model.show_topics(num_topics=5, num_words=10, formatted=False)","68ab1939":"lda_model[corpus_tf[10]]","2322386a":"print(len(df[\"tokens\"]))\nX = transform(df[\"tokens\"], tf_model, lda_model)\nprint(len(X))\nX_test = transform(df_test[\"tokens\"], tf_model, lda_model)\n\nX_train, X_val, y_train, y_val = train_test_split(X, df[\"target\"], test_size=0.1, random_state=42)","0a87d7be":"model = SVC(C=2, gamma=0.4, kernel='rbf').fit(X_train, y_train)\ny_val_pred = model.predict(X_val)\nprint(accuracy_score(y_val, y_val_pred), f1_score(y_val, y_val_pred))\nprint(confusion_matrix(y_val, y_val_pred))\nmodel = SVC(C=2, gamma=0.4, kernel='rbf').fit(X, df.target)\ny_test_pred = model.predict(X_test)\nsub_df = pd.read_csv(os.path.join('..\/input\/nlp-getting-started\/', 'sample_submission.csv'))\nsub_df[\"target\"] = y_test_pred\nsub_df.to_csv(\"submission_lda.csv\",index=False)","a3a4bb43":"model = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42, n_jobs=-1).fit(X_train, y_train)\ny_val_pred = model.predict(X_val)\nprint(accuracy_score(y_val, y_val_pred), f1_score(y_val, y_val_pred))\nprint(confusion_matrix(y_val, y_val_pred))\nmodel = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42, n_jobs=-1).fit(X, df.target)\ny_test_pred = model.predict(X_test)\nsub_df = pd.read_csv(os.path.join('..\/input\/nlp-getting-started\/', 'sample_submission.csv'))\nsub_df[\"target\"] = y_test_pred\nsub_df.to_csv(\"submission_lda_rf.csv\",index=False)","9610499a":"pca = PCA()\nX_pca = pca.fit_transform(X)\nplt.figure(figsize=(8, 8))\nsns.scatterplot(X_pca[:, 0], X_pca[:, 1], hue=df.target);\nplt.xlabel(\"\u041f\u0435\u0440\u0432\u0430\u044f \u0433\u043b\u0430\u0432\u043d\u0430\u044f \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u0430\");\nplt.ylabel(\"\u0412\u0442\u043e\u0440\u0430\u044f \u0433\u043b\u0430\u0432\u043d\u0430\u044f \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u0430\");","132cea01":"print(len(df[\"tokens\"]))\nX_1 = transform(df[\"tokens\"], tf_model, lda_model)\nX_2 = transform(df[\"tokens\"], tf_model, lsi_model)\nX = np.hstack((X_1, X_2))\nprint(len(X))\nX_test = transform(df_test[\"tokens\"], tf_model, lda_model)\nX_1 = transform(df_test[\"tokens\"], tf_model, lda_model)\nX_2 = transform(df_test[\"tokens\"], tf_model, lsi_model)\nX_test = np.hstack((X_1, X_2))\nX_train, X_val, y_train, y_val = train_test_split(X, df[\"target\"], test_size=0.1, random_state=42)\nX_train.shape","9f9cbc5c":"model = SVC(C=2, gamma=0.4, kernel='rbf').fit(X_train, y_train)\ny_val_pred = model.predict(X_val)\nprint(accuracy_score(y_val, y_val_pred), f1_score(y_val, y_val_pred))\nprint(confusion_matrix(y_val, y_val_pred))\nmodel = SVC(C=2, gamma=0.4, kernel='rbf').fit(X, df.target)\ny_test_pred = model.predict(X_test)\nsub_df = pd.read_csv(os.path.join('..\/input\/nlp-getting-started\/', 'sample_submission.csv'))\nsub_df[\"target\"] = y_test_pred\nsub_df.to_csv(\"submission_lda_lsi.csv\",index=False)","a3cd6917":"model = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42, n_jobs=-1).fit(X_train, y_train)\ny_val_pred = model.predict(X_val)\nprint(accuracy_score(y_val, y_val_pred), f1_score(y_val, y_val_pred))\nprint(confusion_matrix(y_val, y_val_pred))\nmodel = RandomForestClassifier(n_estimators=200, max_depth=None, random_state=42, n_jobs=-1).fit(X, df.target)\ny_test_pred = model.predict(X_test)\nsub_df = pd.read_csv(os.path.join('..\/input\/nlp-getting-started\/', 'sample_submission.csv'))\nsub_df[\"target\"] = y_test_pred\nsub_df.to_csv(\"submission_lda_lsi_rf.csv\",index=False)","d7071daf":"pca = PCA()\nX_pca = pca.fit_transform(X)\nplt.figure(figsize=(8, 8))\nsns.scatterplot(X_pca[:, 0], X_pca[:, 1], hue=df.target);\nplt.xlabel(\"\u041f\u0435\u0440\u0432\u0430\u044f \u0433\u043b\u0430\u0432\u043d\u0430\u044f \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u0430\");\nplt.ylabel(\"\u0412\u0442\u043e\u0440\u0430\u044f \u0433\u043b\u0430\u0432\u043d\u0430\u044f \u043a\u043e\u043c\u043f\u043e\u043d\u0435\u043d\u0442\u0430\");","79ee9968":"from gensim.models import Word2Vec\nX_all = pd.concat([df[\"tokens\"], df_test[\"tokens\"]]).reset_index(drop=True)\nmodel_w2v = Word2Vec(sentences=X_all, size=50, window=3, min_count=1, workers=-1)\ndel X_all","3d9c75b0":"model_w2v.wv.most_similar('armageddon', topn=10)","0b65ea49":"vector = model_w2v.wv['armageddon']\nprint(vector)","5b5ecf1d":"print(len(df[\"tokens\"]))\nX = [np.mean([model_w2v.wv[text] for text in texts], axis=0) for texts in df[\"tokens\"]]\nX = np.array(X)\nprint(len(X))\nX_test = [np.mean([model_w2v.wv[text] for text in texts], axis=0) if len(texts) != 0 else np.zeros(50) for texts in df_test[\"tokens\"]]\nX_test = np.array(X_test)\nX_test.shape","03114419":"X_train, X_val, y_train, y_val = train_test_split(X, df[\"target\"], test_size=0.1, random_state=42)\nprint(X_train.shape)\nmodel = SVC(C=2, gamma=0.4, kernel='rbf').fit(X_train, y_train)\ny_val_pred = model.predict(X_val)\nprint(accuracy_score(y_val, y_val_pred), f1_score(y_val, y_val_pred))\nprint(confusion_matrix(y_val, y_val_pred))\nmodel = SVC(C=2, gamma=0.4, kernel='rbf').fit(X, df.target)\ny_test_pred = model.predict(X_test)\nsub_df = pd.read_csv(os.path.join('..\/input\/nlp-getting-started\/', 'sample_submission.csv'))\nsub_df[\"target\"] = y_test_pred\nsub_df.to_csv(\"submission_w2v.csv\",index=False)","62ac3076":"import gensim.downloader\nprint(list(gensim.downloader.info()['models'].keys()))","d4748684":"w2v_model = gensim.downloader.load(\"word2vec-google-news-300\")\ntype(w2v_model)","42e49b41":"w2v_model.wv.most_similar('armageddon', topn=10)","0322f161":"from gensim.models.doc2vec import Doc2Vec, TaggedDocument\nX_all = pd.concat([df[\"tokens\"], df_test[\"tokens\"]]).reset_index(drop=True)\ndocuments = [TaggedDocument(doc, [i]) for i, doc in enumerate(X_all)]\ndel X_all\nmodel_d2v = Doc2Vec(documents, vector_size=500, window=2, min_count=1, workers=-1)","21d9835d":"print(len(df[\"tokens\"]))\nX = [model_d2v.infer_vector(texts) for texts in df[\"tokens\"]]\nX = np.array(X)\nprint(len(X))\nX_test = [model_d2v.infer_vector(texts) for texts in df_test[\"tokens\"]]\nX_test = np.array(X_test)","e1f23789":"X_train, X_val, y_train, y_val = train_test_split(X, df[\"target\"], test_size=0.1, random_state=42)\nprint(X_train.shape)\nmodel = SVC(C=2, gamma=0.4, kernel='rbf').fit(X_train, y_train)\ny_val_pred = model.predict(X_val)\nprint(accuracy_score(y_val, y_val_pred), f1_score(y_val, y_val_pred))\nprint(confusion_matrix(y_val, y_val_pred))\nmodel = SVC(C=2, gamma=0.4, kernel='rbf').fit(X, df.target)\ny_test_pred = model.predict(X_test)\nsub_df = pd.read_csv(os.path.join('..\/input\/nlp-getting-started\/', 'sample_submission.csv'))\nsub_df[\"target\"] = y_test_pred\nsub_df.to_csv(\"submission_lda_lsi.csv\",index=False)","7937eabd":"# doc2bow - \u043c\u0435\u0448\u043e\u043a \u0441\u043b\u043e\u0432 (\u0432 sklearn)","425185dc":"\u0427\u0442\u043e\u0431\u044b \u043e\u043f\u0438\u0441\u0430\u0442\u044c \u043d\u0430\u0448 \u0442\u0432\u0438\u0442 \u0441\u043b\u043e\u0432\u043e\u043c, \u043d\u0430\u043c \u043d\u0443\u0436\u043d\u043e \u0443\u0434\u0430\u043b\u0438\u0442\u044c \"\u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430\". \u042d\u0442\u043e \u043e\u0437\u043d\u0430\u0447\u0430\u0435\u0442 \u0432\u0441\u0435 \u043e\u0441\u043d\u043e\u0432\u043d\u044b\u0435 \u0441\u043b\u043e\u0432\u0430, \u0442\u0430\u043a\u0438\u0435 \u043a\u0430\u043a \"\u0442\u043e\u0442\", \"\u0442\u044b\"... \u042d\u0442\u0438 \u0441\u043b\u043e\u0432\u0430 \u043f\u0440\u0438\u0441\u0443\u0442\u0441\u0442\u0432\u0443\u044e\u0442 \u0432\u043e \u0432\u0441\u0435\u0445 \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u0430\u0445, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 \u0438\u0445 \u043d\u0435\u043b\u044c\u0437\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0434\u043b\u044f \u0440\u0430\u0437\u043b\u0438\u0447\u0435\u043d\u0438\u044f \u0442\u0432\u0438\u0442\u043e\u0432.","55658530":"# LDA - \u0431\u043e\u043b\u0435\u0435 \u0436\u0435\u0441\u0442\u043a\u0430\u044f \u043a\u043b\u0430\u0441\u0442\u0435\u0440\u0438\u0437\u0430\u0446\u0438\u044f \u043d\u0430 \u0442\u0435\u043c\u044b","54380e23":"# \u0418\u0441\u0441\u043b\u0435\u0434\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435","c98d853c":"\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0435 \u0432\u0435\u043a\u0442\u043e\u0440\u0430","d9bcf835":"## \u041e\u0431\u044a\u0435\u0434\u0435\u043d\u0438\u043c \u0434\u0432\u0435 \u043c\u043e\u0434\u0435\u043b\u0438","db606d74":"\u041c\u043e\u0436\u043d\u043e \u0443\u0432\u0438\u0434\u0435\u0442\u044c, \u0447\u0442\u043e \u043a\u043e\u0440\u043f\u0443 doc2bow genswim \u043e\u0442\u043b\u0438\u0447\u0430\u0435\u0442\u0441\u044f \u043f\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u044e \u0434\u0430\u043d\u043d\u044b\u0445\n\n`(\u043d\u043e\u043c\u0435\u0440 \u0441\u043b\u043e\u0432\u0430 \u0432 \u0441\u043b\u043e\u0432\u0430\u0440\u0435, \u0447\u0430\u0441\u0442\u043e\u0442\u0430 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0435\u043c\u043e\u0441\u0442\u0438 \u0441\u043b\u043e\u0432\u0430 \u0432 \u0434\u0430\u043d\u043d\u043e\u043c \u0442\u0435\u043a\u0441\u0442\u0435)`","9bb7ee1f":"**\u041d\u0435\u043c\u043d\u043e\u0433\u043e \u043f\u043e\u0434\u0440\u043e\u0431\u043d\u0435\u0435 \u043e tf-idf**","82a06b51":"\u041f\u043e\u043d\u0438\u0436\u0435\u043d\u0438\u0435 \u0440\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u0438 \u0438 \u043e\u0442\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0435 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430","3e536250":"# \u041f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0442\u0435\u043a\u0441\u0442\u0430 - \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0430\u0446\u0438\u044f\n\n\u0412 \u044d\u0442\u043e\u0439 \u0447\u0430\u0441\u0442\u0438 \u043c\u044b \u0441\u043e\u0431\u0438\u0440\u0430\u0435\u043c\u0441\u044f \u043d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u0442\u044c \u043d\u0430\u0448\u0438 \u0434\u0430\u043d\u043d\u044b\u0435. \u041f\u0440\u0435\u0436\u0434\u0435 \u0432\u0441\u0435\u0433\u043e, \u043c\u044b \u0441\u043e\u0431\u0438\u0440\u0430\u0435\u043c\u0441\u044f\n* \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u0442\u0435\u043a\u0441\u0442 \u0432 \u043d\u0438\u0436\u043d\u0438\u0439 \u0440\u0435\u0433\u0438\u0441\u0442\u0440\n* \u043e\u0447\u0438\u0441\u0442\u0438\u0442\u044c \u043e\u0442 \u043f\u0443\u043d\u043a\u0442\u0443\u0430\u0446\u0438\u0438\n* \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0442\u0435\u043a\u0441\u0442","d70b3676":"\u041f\u043e\u0441\u043e\u043c\u0442\u0440\u0438\u043c, \u043a\u0430\u043a \u0432\u044b\u0433\u043b\u044f\u0434\u0438\u0442 \u0432\u0435\u043a\u0442\u043e\u0440 \u043e\u0434\u043d\u043e\u0433\u043e \u0442\u0435\u043a\u0441\u0442\u0430","cf1c8ece":"\u0421\u043b\u043e\u0432\u0430 \u043c\u043e\u0433\u0443\u0442 \u043d\u0435\u0441\u0442\u0438 \u043c\u043d\u043e\u0433\u043e \u043b\u0438\u0448\u043d\u0435\u0433\u043e \u0440\u0430\u0437\u043d\u043e\u043e\u0431\u0440\u0430\u0437\u0438\u044f \u0437\u0430 \u0441\u0447\u0435\u0442: \u043f\u0430\u0434\u0435\u0436\u0438, \u0440\u0430\u0437\u043d\u044b\u0439 \u0440\u043e\u0434, \u0440\u0430\u0437\u043d\u043e\u0435 \u0432\u0440\u0435\u043c\u044f, \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0435 \u0447\u0438\u0441\u043b\u043e\n\u0415\u0441\u0442\u044c \u0434\u0432\u0430 \u043f\u043e\u0434\u0445\u043e\u0434\u0430 \u043a \u0443\u043c\u0435\u043d\u044c\u0448\u0435\u043d\u0438\u044e \u0440\u0430\u0437\u043d\u043e\u043e\u0431\u0440\u0430\u0437\u0438\u044f: \u0441\u0442\u0435\u043c\u043c\u0438\u043d\u0433 \u0438 \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044f","5dfb1166":"## LSI","d106215f":"\u0414\u043e\u0431\u0430\u0432\u0438\u043c \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044e \u043e \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u0445 \u0441\u043b\u043e\u0432\u0430\u0445","b449d3c7":"\u041f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u043e\u0448\u0438\u0431\u043a\u0443","10efb648":"![image.png](attachment:7e725386-9a94-4ca7-b89c-fca40ae4d0c6.png)","a4c93d96":"\u041f\u0440\u0438\u0432\u043e\u0434\u0438\u043c \u0441\u043b\u043e\u0432\u043e \u043a \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u0435","e29a0aba":"\u0421\u043c\u043e\u0440\u0438\u043c \u0441\u043b\u043e\u0432\u0435\u0441\u043d\u043e\u0435 \u044f\u0434\u0440\u043e \u0442\u0435\u043c","63fa0f0c":"\u0423\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u0441\u043b\u043e\u0432\u043e\u0441\u043e\u0447\u0435\u0442\u0430\u043d\u0438\u044f","bb7f6dd8":"# Real or Not? NLP with Disaster Tweets\n\n## Competition \n\n\u0412 \u044d\u0442\u043e\u043c \u043a\u043e\u043d\u043a\u0443\u0440\u0441\u0435 \u043c\u044b \u0445\u043e\u0442\u0438\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u0442\u044c, \u043a\u0430\u043a\u0438\u0435 \u0442\u0432\u0438\u0442\u044b \u043e \u0440\u0435\u0430\u043b\u044c\u043d\u044b\u0445 \u043a\u0430\u0442\u0430\u0441\u0442\u0440\u043e\u0444\u0430\u0445, \u0430 \u043a\u0430\u043a\u0438\u0435 \u043d\u0435\u0442. \u0417\u0430\u0442\u0435\u043c \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u0442\u0432\u0438\u0442\u0430 (\u0432\u0445\u043e\u0434\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435) \u043c\u044b \u0445\u043e\u0442\u0438\u043c \u0437\u043d\u0430\u0442\u044c, \u043e\u0442\u043d\u043e\u0441\u0438\u0442\u0441\u044f \u043b\u0438 \u043e\u043d \u043a \u0441\u043e\u0431\u044b\u0442\u0438\u044e \u043a\u0430\u0442\u0430\u0441\u0442\u0440\u043e\u0444\u044b (\u0446\u0435\u043b\u0435\u0432\u0430\u044f \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0430\u044f). \u0412\u044b\u0432\u043e\u0434 \u043d\u0430\u0448\u0435\u0439 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u0431\u0443\u0434\u0435\u0442 \u043b\u043e\u0433\u0438\u0447\u0435\u0441\u043a\u0438\u043c (true \u0438\u043b\u0438 false).\n\n- \u0418\u0441\u0441\u043b\u0435\u0434\u0443\u0435\u043c \u043d\u0430\u0448\u0438 \u0434\u0430\u043d\u043d\u044b\u0435.\n- \u0412\u044b\u043f\u043e\u043b\u043d\u0438\u043c \u043f\u0440\u0435\u0434\u0432\u0430\u0440\u0438\u0442\u0435\u043b\u044c\u043d\u0443\u044e \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0443\n- doc2bow + SVM \u043f\u043e\u0434\u0445\u043e\u0434\n- TF-IDF + SVM \u043f\u043e\u0434\u0445\u043e\u0434\n- \n- \u0412\u044b\u0431\u0438\u0440\u0430\u0435\u043c \u043b\u0443\u0447\u0448\u0443\u044e \u0432\u0435\u043a\u0442\u043e\u0440\u043d\u0443\u044e \u0438 \u043c\u043e\u0434\u0435\u043b\u044c \u0438 \u0432\u044b\u043f\u043e\u043b\u043d\u044f\u0435\u043c \u043f\u043e\u043b\u043d\u043e\u0435 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u0435 \u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u0430.","b514211b":"\u0421\u043c\u043e\u0442\u0440\u0438\u043c, \u043a\u0430\u043a \u0432\u044b\u0433\u043b\u044f\u0434\u0438\u0442 \u0441\u043b\u043e\u0432\u0430\u0440\u044c","c5baa57a":"\u041e\u0431\u0440\u0435\u0437\u0430\u0435\u043c \u043e\u043a\u043e\u043d\u0447\u0430\u043d\u0438\u044f","341f3e8a":"\u041f\u0440\u0438\u043c\u0435\u043d\u044f\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c","4f5b0efd":"\u0427\u0442\u043e \u0435\u0449\u0435 \u043b\u0438\u0448\u043d\u0435\u0433\u043e \u043c\u044b \u043c\u043e\u0436\u0435\u043c \u0443\u0434\u0430\u043b\u0438\u0442\u044c \u0438\u0437 \u0442\u0435\u043a\u0441\u0442\u0430?","d0a3072a":"# word2vec","0d39eb8e":"\u0410\u043d\u0430\u043b\u0438\u0437 \u043a\u043b\u044e\u0447\u0435\u0432\u044b\u0445 \u0441\u043b\u043e\u0432","ad998e33":"\u0421\u043e\u0437\u0434\u0430\u0435\u043c \u0444\u0443\u043d\u043a\u0446\u0438\u044e \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043a \u043d\u0443\u0436\u043d\u043e\u043c\u0443 \u0444\u043e\u0440\u043c\u0430\u0442\u0443","a1e958bf":"**\u041f\u0440\u0438\u043c\u0435\u0440 \u043d\u0430 \u043e\u0434\u043d\u043e\u043c \u0442\u0432\u0438\u0442\u0435**","a8cc1846":"\u0423\u0434\u0430\u043b\u0438\u043c \u0432\u0441\u0451, \u0447\u0442\u043e \u043d\u0435 \u044f\u0432\u043b\u044f\u0435\u0442\u0441\u044f \u0441\u043b\u043e\u0432\u0430\u043c\u0438","5f23a5c6":"\u0421\u043c\u043e\u0442\u0440\u0438\u043c \u0447\u0438\u0441\u043b\u043e \u0442\u0435\u043c","0e53f760":"\u0421\u043c\u043e\u0442\u0440\u0438\u043c \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0434\u043e\u043a\u0443\u043c\u0435\u043d\u0442\u043e\u0432, \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0441\u043e\u0437\u0434\u0430\u0432\u0430\u043b\u0441\u044f \u0441\u043b\u043e\u0432\u0430\u0440\u044c","322e8201":"\u0427\u0438\u0441\u0442\u0438\u043c \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u0440\u0435\u0434\u043a\u0438\u0435 \u0441\u043b\u043e\u0432\u0430","ceaa5ae3":"# doc2vec","bcdbd0a7":"# TF-IDF sklearn","a6a5e710":"\u0421\u043c\u043e\u0442\u0440\u0438\u043c \u0442\u0435\u043a\u0441\u0442 \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u0442\u0432\u0438\u0442\u0430","0b90ae02":"# \u0422\u0435\u043c\u0430\u0442\u0438\u0447\u0435\u0441\u043a\u0438\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 gensim"}}