{"cell_type":{"85aada42":"code","125ee7c5":"code","d41c300e":"code","c80a3ce1":"code","4047ba55":"code","554d872c":"code","19da88a2":"code","a144089a":"code","ded69a99":"code","01b7854f":"code","b41fa721":"code","cc3f2acf":"code","87c70a67":"code","10fe91a3":"code","61186918":"code","d56818ab":"code","3f2f2bb9":"code","5a75a060":"code","6c5be655":"code","f6e3e4d2":"code","87faafe9":"code","ecad75c8":"code","d6cc0c61":"markdown","f7a2dc59":"markdown","9e6ebb65":"markdown","2cdfb69e":"markdown","ab3be11f":"markdown","bbf86b45":"markdown","cc4c3e83":"markdown","5157473f":"markdown","6e03919f":"markdown","42b86f1e":"markdown","48759115":"markdown"},"source":{"85aada42":"import sys\nsys.path.append('\/kaggle\/input\/efficientnet-keras-dataset\/efficientnet_kaggle')","125ee7c5":"from kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import KFold\nimport efficientnet.tfkeras as efn\nimport matplotlib.pyplot as plt\nimport tensorflow_addons as tfa\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport os","d41c300e":"print('Using tensorflow %s' % tf.__version__)\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print('Running on TPUv3-8')\nexcept:\n    tpu = None\n    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n    strategy = tf.distribute.get_strategy()\n    print('Running on GPU with mixed precision')\n\nbatch_size = 16 * strategy.num_replicas_in_sync\n\nprint('Number of replicas:', strategy.num_replicas_in_sync)\nprint('Batch size: %.i' % batch_size)","c80a3ce1":"class CFG():\n    \n    '''\n    keep these\n    '''\n    strategy = strategy\n    batch_size = batch_size\n    \n    img_size = 600\n    classes = [\n        'complex', \n        'frog_eye_leaf_spot', \n        'powdery_mildew', \n        'rust', \n        'scab']\n    \n    gcs_path_raw = KaggleDatasets().get_gcs_path('pp2021-kfold-tfrecords-0')\n    \n    gcs_path_aug = [\n        KaggleDatasets().get_gcs_path('pp2021-kfold-tfrecords'),\n        KaggleDatasets().get_gcs_path('pp2021-kfold-tfrecords-1'),\n        KaggleDatasets().get_gcs_path('pp2021-kfold-tfrecords-2'),\n        KaggleDatasets().get_gcs_path('pp2021-kfold-tfrecords-3')]\n    \n    '''\n    tweak these\n    '''\n    seed = 42 # random seed we use for each operation\n    epochs = 100 # maximum number of epochs <-- keep this large as we use EarlyStopping\n    patience = [5, 2] # patience[0] is for EarlyStopping, patience[1] is for ReduceLROnPlateau\n    factor = .1 # new_lr =  lr * factor if patience_count > patience[1]\n    min_lr = 1e-8 # minimum optimizer lr\n    \n    verbose = 2 # set this to 1 to see live progress bar or to 2 when commiting\n    \n    folds = 5 # number of KFold folds\n    used_folds = [0, 1, 2, 3, 4] # number of used folds <-- here we use only the first one","4047ba55":"def count_data_items(filenames):\n    return np.sum([int(x[:-6].split('-')[-1]) for x in filenames])\n\n\ndef decode_image(image_data):\n    image = tf.image.decode_jpeg(image_data, channels=3)\n    image = tf.reshape(image, [CFG.img_size, CFG.img_size, 3])\n    image = tf.cast(image, tf.float32) \/ 255.\n    return image\n\n\nfeature_map = {\n    'image': tf.io.FixedLenFeature([], tf.string),\n    'image_name': tf.io.FixedLenFeature([], tf.string),\n    'complex': tf.io.FixedLenFeature([], tf.int64),\n    'frog_eye_leaf_spot': tf.io.FixedLenFeature([], tf.int64),\n    'powdery_mildew': tf.io.FixedLenFeature([], tf.int64),\n    'rust': tf.io.FixedLenFeature([], tf.int64),\n    'scab': tf.io.FixedLenFeature([], tf.int64),\n    'healthy': tf.io.FixedLenFeature([], tf.int64)}\n\n\ndef read_tfrecord(example, labeled=True):\n    example = tf.io.parse_single_example(example, feature_map)\n    image = decode_image(example['image'])\n    if labeled:\n        label = [tf.cast(example[x], tf.float32) for x in CFG.classes]\n    else:\n        label = example['image_name']\n    return image, label\n\n\ndef get_dataset(filenames, labeled=True, ordered=True, shuffled=False, \n                repeated=False, cached=False, distributed=True):\n    auto = tf.data.experimental.AUTOTUNE\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=auto)\n    if not ordered:\n        ignore_order = tf.data.Options()\n        ignore_order.experimental_deterministic = False\n        dataset = dataset.with_options(ignore_order)\n    dataset = dataset.map(\n        lambda x: read_tfrecord(x, labeled=labeled), \n        num_parallel_calls=auto)\n    if shuffled:\n        dataset = dataset.shuffle(2048, seed=CFG.seed)\n    if repeated:\n        dataset = dataset.repeat()\n    dataset = dataset.batch(CFG.batch_size)\n    if cached:\n        dataset = dataset.cache()\n    dataset = dataset.prefetch(auto)\n    if distributed:\n        dataset = CFG.strategy.experimental_distribute_dataset(dataset)\n    return dataset\n\n\ndef get_model():\n    model = tf.keras.models.Sequential(name='EfficientNetB4')\n    \n    model.add(efn.EfficientNetB4(\n        include_top=False,\n        input_shape=(CFG.img_size, CFG.img_size, 3),\n        weights='noisy-student',\n        pooling='avg'))\n    \n    model.add(tf.keras.layers.Dense(len(CFG.classes), \n        kernel_initializer=tf.keras.initializers.RandomUniform(seed=CFG.seed),\n        bias_initializer=tf.keras.initializers.Zeros(), name='dense_top'))\n    model.add(tf.keras.layers.Activation('sigmoid', dtype='float32'))\n    \n    return model","554d872c":"filenames = tf.io.gfile.glob(os.path.join(CFG.gcs_path_aug[0], 'fold_0\/*.tfrec'))[:1]\n\ndataset = get_dataset(filenames, ordered=False, distributed=False)\n\nplt.figure(figsize=[15, 15])\n\nfor i, sample in enumerate(dataset.unbatch().take(25).as_numpy_iterator()):\n    plt.subplot(5, 5, i + 1)\n    plt.imshow(sample[0])\n    plt.axis('off')\n    \nplt.show()","19da88a2":"model = get_model()\nmodel.summary()","a144089a":"histories = []\nscores = []\nimage_names = np.empty((0,))\npredicts = np.empty((0, len(CFG.classes)))\n\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(\n        monitor='val_f1_score', mode='max', \n        patience=CFG.patience[0], restore_best_weights=True),\n    tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_f1_score', mode='max',\n        patience=CFG.patience[1], min_lr=CFG.min_lr, verbose=2)]\n\nkfold = KFold(n_splits=CFG.folds, shuffle=True, random_state=CFG.seed)\nfolds = ['fold_0', 'fold_1', 'fold_2', 'fold_3', 'fold_4']\n\n'''\nrun training loop\n'''\nfor i, (train_index, val_index) in enumerate(kfold.split(folds)):\n    \n    '''\n    run only selected folds\n    '''\n    if i in CFG.used_folds:\n        \n        print('=' * 74)\n        print(f'Fold {i}') \n        print('=' * 74)\n        \n        '''\n        reinitialize the system\n        '''\n        if tpu is not None: \n            tf.tpu.experimental.initialize_tpu_system(tpu)\n        \n        '''\n        model setup\n        '''\n        with CFG.strategy.scope():\n            model = get_model()\n            \n            model.compile(\n                loss=tf.keras.losses.BinaryCrossentropy(),\n                optimizer='adam',\n                metrics=[\n                    tf.keras.metrics.BinaryAccuracy(name='acc'), \n                    tfa.metrics.F1Score(\n                        num_classes=len(CFG.classes), \n                        average='macro')])\n            \n        '''\n        data setup\n        '''\n        train_filenames = []\n        for j in train_index:\n            train_filenames += tf.io.gfile.glob(os.path.join(CFG.gcs_path_aug[0], folds[j], '*.tfrec'))\n            train_filenames += tf.io.gfile.glob(os.path.join(CFG.gcs_path_aug[1], folds[j], '*.tfrec'))\n            train_filenames += tf.io.gfile.glob(os.path.join(CFG.gcs_path_aug[2], folds[j], '*.tfrec'))\n            train_filenames += tf.io.gfile.glob(os.path.join(CFG.gcs_path_aug[3], folds[j], '*.tfrec'))\n        np.random.shuffle(train_filenames)\n            \n        val_filenames = []\n        for j in val_index:\n            val_filenames += tf.io.gfile.glob(os.path.join(CFG.gcs_path_raw, folds[j], '*.tfrec'))\n\n        train_dataset = get_dataset(\n            train_filenames, \n            ordered=False, shuffled=True, repeated=True)\n        \n        val_dataset = get_dataset(\n            val_filenames, \n            cached=True)\n\n        steps_per_epoch = count_data_items(train_filenames) \/\/ (20 * CFG.batch_size)\n        validation_steps = count_data_items(val_filenames) \/\/ CFG.batch_size\n        \n        '''\n        fit\n        '''\n        history = model.fit(\n            train_dataset,\n            steps_per_epoch=steps_per_epoch,\n            validation_data=val_dataset,\n            validation_steps=validation_steps,\n            callbacks=callbacks,\n            epochs=CFG.epochs,\n            verbose=CFG.verbose).history\n        \n        '''\n        write out-of-fold predictions\n        '''\n        size = count_data_items(val_filenames)\n        steps = size \/\/ CFG.batch_size + 1\n        \n        val_dataset = get_dataset(val_filenames, labeled=False, distributed=False)\n        val_predicts = model.predict(\n            val_dataset.map(lambda x, y: x), \n            steps=steps, \n            verbose=CFG.verbose)[:size]\n        val_image_names = [x.decode() for x in val_dataset.map(lambda x, y: y).unbatch().take(size).as_numpy_iterator()]\n        \n        image_names = np.concatenate((image_names, val_image_names))\n        predicts = np.concatenate((predicts, val_predicts))\n        \n        '''\n        finalize\n        '''\n        model.save_weights(f'model_{i}.h5')\n        histories.append(pd.DataFrame(history))\n        scores.append(histories[-1]['val_f1_score'].max())\n        \n    else:\n        pass","ded69a99":"scores_df = pd.DataFrame({\n    'fold': np.arange(len(scores)),\n    'f1': np.round(scores, 4)})\n\nwith pd.option_context('display.max_rows', None):\n    display(scores_df)\n\nprint('CV %.4f' % scores_df['f1'].mean())","01b7854f":"figure, axes = plt.subplots(1, 5, figsize=[20, 5])\n\nfor i in range(CFG.folds):\n    \n    try:\n        axes[i].plot(histories[i].loc[:, 'f1_score'], label='train')\n        axes[i].plot(histories[i].loc[:, 'val_f1_score'], label='val')\n        axes[i].legend()\n    except IndexError:\n        pass\n    \n    axes[i].set_title(f'fold {i}')\n    axes[i].set_xlabel('epochs')\n    \nplt.show()","b41fa721":"predicts_df = pd.DataFrame(\n    columns=CFG.classes, \n    data=predicts, \n    index=pd.Index(data=image_names, name='image'))\n\npredicts_df.to_csv('oof_predicts.csv')\ndisplay(predicts_df.head())","cc3f2acf":"def get_model():\n    model = tf.keras.models.Sequential(name='EfficientNetB5')\n    \n    model.add(efn.EfficientNetB5(\n        include_top=False,\n        input_shape=(CFG.img_size, CFG.img_size, 3),\n        weights='noisy-student',\n        pooling='avg'))\n    \n    model.add(tf.keras.layers.Dense(len(CFG.classes), \n        kernel_initializer=tf.keras.initializers.RandomUniform(seed=CFG.seed),\n        bias_initializer=tf.keras.initializers.Zeros(), name='dense_top'))\n    model.add(tf.keras.layers.Activation('sigmoid', dtype='float32'))\n    \n    return model","87c70a67":"model = get_model()\nmodel.summary()","10fe91a3":"histories = []\nscores = []\nimage_names = np.empty((0,))\npredicts = np.empty((0, len(CFG.classes)))\n\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(\n        monitor='val_f1_score', mode='max', \n        patience=CFG.patience[0], restore_best_weights=True),\n    tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_f1_score', mode='max',\n        patience=CFG.patience[1], min_lr=CFG.min_lr, verbose=2)]\n\nkfold = KFold(n_splits=CFG.folds, shuffle=True, random_state=CFG.seed)\nfolds = ['fold_0', 'fold_1', 'fold_2', 'fold_3', 'fold_4']\n\n'''\nrun training loop\n'''\nfor i, (train_index, val_index) in enumerate(kfold.split(folds)):\n    \n    '''\n    run only selected folds\n    '''\n    if i in CFG.used_folds:\n        \n        print('=' * 74)\n        print(f'Fold {i}') \n        print('=' * 74)\n        \n        '''\n        reinitialize the system\n        '''\n        if tpu is not None: \n            tf.tpu.experimental.initialize_tpu_system(tpu)\n        \n        '''\n        model setup\n        '''\n        with CFG.strategy.scope():\n            model = get_model()\n            \n            model.compile(\n                loss=tf.keras.losses.BinaryCrossentropy(),\n                optimizer='adam',\n                metrics=[\n                    tf.keras.metrics.BinaryAccuracy(name='acc'), \n                    tfa.metrics.F1Score(\n                        num_classes=len(CFG.classes), \n                        average='macro')])\n            \n        '''\n        data setup\n        '''\n        train_filenames = []\n        for j in train_index:\n            train_filenames += tf.io.gfile.glob(os.path.join(CFG.gcs_path_aug[0], folds[j], '*.tfrec'))\n            train_filenames += tf.io.gfile.glob(os.path.join(CFG.gcs_path_aug[1], folds[j], '*.tfrec'))\n            train_filenames += tf.io.gfile.glob(os.path.join(CFG.gcs_path_aug[2], folds[j], '*.tfrec'))\n            train_filenames += tf.io.gfile.glob(os.path.join(CFG.gcs_path_aug[3], folds[j], '*.tfrec'))\n        np.random.shuffle(train_filenames)\n            \n        val_filenames = []\n        for j in val_index:\n            val_filenames += tf.io.gfile.glob(os.path.join(CFG.gcs_path_raw, folds[j], '*.tfrec'))\n\n        train_dataset = get_dataset(\n            train_filenames, \n            ordered=False, shuffled=True, repeated=True)\n        \n        val_dataset = get_dataset(\n            val_filenames, \n            cached=True)\n\n        steps_per_epoch = count_data_items(train_filenames) \/\/ (20 * CFG.batch_size)\n        validation_steps = count_data_items(val_filenames) \/\/ CFG.batch_size\n        \n        '''\n        fit\n        '''\n        history = model.fit(\n            train_dataset,\n            steps_per_epoch=steps_per_epoch,\n            validation_data=val_dataset,\n            validation_steps=validation_steps,\n            callbacks=callbacks,\n            epochs=CFG.epochs,\n            verbose=CFG.verbose).history\n        \n        '''\n        write out-of-fold predictions\n        '''\n        size = count_data_items(val_filenames)\n        steps = size \/\/ CFG.batch_size + 1\n        \n        val_dataset = get_dataset(val_filenames, labeled=False, distributed=False)\n        val_predicts = model.predict(\n            val_dataset.map(lambda x, y: x), \n            steps=steps, \n            verbose=CFG.verbose)[:size]\n        val_image_names = [x.decode() for x in val_dataset.map(lambda x, y: y).unbatch().take(size).as_numpy_iterator()]\n        \n        image_names = np.concatenate((image_names, val_image_names))\n        predicts = np.concatenate((predicts, val_predicts))\n        \n        '''\n        finalize\n        '''\n        model.save_weights(f'model_{i}.h5')\n        histories.append(pd.DataFrame(history))\n        scores.append(histories[-1]['val_f1_score'].max())\n        \n    else:\n        pass","61186918":"scores_df = pd.DataFrame({\n    'fold': np.arange(len(scores)),\n    'f1': np.round(scores, 4)})\n\nwith pd.option_context('display.max_rows', None):\n    display(scores_df)\n\nprint('CV %.4f' % scores_df['f1'].mean())","d56818ab":"figure, axes = plt.subplots(1, 5, figsize=[20, 5])\n\nfor i in range(CFG.folds):\n    \n    try:\n        axes[i].plot(histories[i].loc[:, 'f1_score'], label='train')\n        axes[i].plot(histories[i].loc[:, 'val_f1_score'], label='val')\n        axes[i].legend()\n    except IndexError:\n        pass\n    \n    axes[i].set_title(f'fold {i}')\n    axes[i].set_xlabel('epochs')\n    \nplt.show()","3f2f2bb9":"predicts_df = pd.DataFrame(\n    columns=CFG.classes, \n    data=predicts, \n    index=pd.Index(data=image_names, name='image'))\n\npredicts_df.to_csv('oof_predicts_B5.csv')\ndisplay(predicts_df.head())","5a75a060":"def get_model():\n    model = tf.keras.models.Sequential(name='EfficientNetB7')\n    \n    model.add(efn.EfficientNetB7(\n        include_top=False,\n        input_shape=(CFG.img_size, CFG.img_size, 3),\n        weights='noisy-student',\n        pooling='avg'))\n    \n    model.add(tf.keras.layers.Dense(len(CFG.classes), \n        kernel_initializer=tf.keras.initializers.RandomUniform(seed=CFG.seed),\n        bias_initializer=tf.keras.initializers.Zeros(), name='dense_top'))\n    model.add(tf.keras.layers.Activation('sigmoid', dtype='float32'))\n    \n    return model","6c5be655":"histories = []\nscores = []\nimage_names = np.empty((0,))\npredicts = np.empty((0, len(CFG.classes)))\n\ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(\n        monitor='val_f1_score', mode='max', \n        patience=CFG.patience[0], restore_best_weights=True),\n    tf.keras.callbacks.ReduceLROnPlateau(\n        monitor='val_f1_score', mode='max',\n        patience=CFG.patience[1], min_lr=CFG.min_lr, verbose=2)]\n\nkfold = KFold(n_splits=CFG.folds, shuffle=True, random_state=CFG.seed)\nfolds = ['fold_0', 'fold_1', 'fold_2', 'fold_3', 'fold_4']\n\n'''\nrun training loop\n'''\nfor i, (train_index, val_index) in enumerate(kfold.split(folds)):\n    \n    '''\n    run only selected folds\n    '''\n    if i in CFG.used_folds:\n        \n        print('=' * 74)\n        print(f'Fold {i}') \n        print('=' * 74)\n        \n        '''\n        reinitialize the system\n        '''\n        if tpu is not None: \n            tf.tpu.experimental.initialize_tpu_system(tpu)\n        \n        '''\n        model setup\n        '''\n        with CFG.strategy.scope():\n            model = get_model()\n            \n            model.compile(\n                loss=tf.keras.losses.BinaryCrossentropy(),\n                optimizer='adam',\n                metrics=[\n                    tf.keras.metrics.BinaryAccuracy(name='acc'), \n                    tfa.metrics.F1Score(\n                        num_classes=len(CFG.classes), \n                        average='macro')])\n            \n        '''\n        data setup\n        '''\n        train_filenames = []\n        for j in train_index:\n            train_filenames += tf.io.gfile.glob(os.path.join(CFG.gcs_path_aug[0], folds[j], '*.tfrec'))\n            train_filenames += tf.io.gfile.glob(os.path.join(CFG.gcs_path_aug[1], folds[j], '*.tfrec'))\n            train_filenames += tf.io.gfile.glob(os.path.join(CFG.gcs_path_aug[2], folds[j], '*.tfrec'))\n            train_filenames += tf.io.gfile.glob(os.path.join(CFG.gcs_path_aug[3], folds[j], '*.tfrec'))\n        np.random.shuffle(train_filenames)\n            \n        val_filenames = []\n        for j in val_index:\n            val_filenames += tf.io.gfile.glob(os.path.join(CFG.gcs_path_raw, folds[j], '*.tfrec'))\n\n        train_dataset = get_dataset(\n            train_filenames, \n            ordered=False, shuffled=True, repeated=True)\n        \n        val_dataset = get_dataset(\n            val_filenames, \n            cached=True)\n\n        steps_per_epoch = count_data_items(train_filenames) \/\/ (20 * CFG.batch_size)\n        validation_steps = count_data_items(val_filenames) \/\/ CFG.batch_size\n        \n        '''\n        fit\n        '''\n        history = model.fit(\n            train_dataset,\n            steps_per_epoch=steps_per_epoch,\n            validation_data=val_dataset,\n            validation_steps=validation_steps,\n            callbacks=callbacks,\n            epochs=CFG.epochs,\n            verbose=CFG.verbose).history\n        \n        '''\n        write out-of-fold predictions\n        '''\n        size = count_data_items(val_filenames)\n        steps = size \/\/ CFG.batch_size + 1\n        \n        val_dataset = get_dataset(val_filenames, labeled=False, distributed=False)\n        val_predicts = model.predict(\n            val_dataset.map(lambda x, y: x), \n            steps=steps, \n            verbose=CFG.verbose)[:size]\n        val_image_names = [x.decode() for x in val_dataset.map(lambda x, y: y).unbatch().take(size).as_numpy_iterator()]\n        \n        image_names = np.concatenate((image_names, val_image_names))\n        predicts = np.concatenate((predicts, val_predicts))\n        \n        '''\n        finalize\n        '''\n        model.save_weights(f'model_{i}.h5')\n        histories.append(pd.DataFrame(history))\n        scores.append(histories[-1]['val_f1_score'].max())\n        \n    else:\n        pass","f6e3e4d2":"scores_df = pd.DataFrame({\n    'fold': np.arange(len(scores)),\n    'f1': np.round(scores, 4)})\n\nwith pd.option_context('display.max_rows', None):\n    display(scores_df)\n\nprint('CV %.4f' % scores_df['f1'].mean())","87faafe9":"figure, axes = plt.subplots(1, 5, figsize=[20, 5])\n\nfor i in range(CFG.folds):\n    \n    try:\n        axes[i].plot(histories[i].loc[:, 'f1_score'], label='train')\n        axes[i].plot(histories[i].loc[:, 'val_f1_score'], label='val')\n        axes[i].legend()\n    except IndexError:\n        pass\n    \n    axes[i].set_title(f'fold {i}')\n    axes[i].set_xlabel('epochs')\n    \nplt.show()","ecad75c8":"predicts_df = pd.DataFrame(\n    columns=CFG.classes, \n    data=predicts, \n    index=pd.Index(data=image_names, name='image'))\n\npredicts_df.to_csv('oof_predicts.csv')\ndisplay(predicts_df.head())","d6cc0c61":"## Results\nDisplay out-of-fold scores.","f7a2dc59":"### Inspect augmented images","9e6ebb65":"# **Running with B5**","2cdfb69e":"### Helper functions","ab3be11f":"### Inspect model","bbf86b45":"### Hardware configuration","cc4c3e83":"### Configurations\nTweak `used folds`, `patience`, `epochs` and other hyperparameters","5157473f":"## Train loop (5 folds CV)","6e03919f":"Write out-of-fold predictions to `oof_predicts.csv` (we will need them later).","42b86f1e":" # **B7**","48759115":"## Summary\n\nThis is **TensorFlow** and **Keras** implementation of training loop for **Plant Pathology 2021** competition optimized for achieving maximum speed.\n* Preprocessing: deleted 77 duplicates detected with `image_hash` library, stratified labels\n* Training strategy: 5 folds CV, unweighted ensemble\n* Backbone: EfficientNetB4, `noisy-student` weights, single dense layer on top\n* Optimizer: Adam, learning rate of 1e-3, ReduceLROnPlateau\n* Image size: 600x600\n* Augmentations: heavy augmentations with `albumentations` library, pre-recorded\n\nTraining takes just 15 minutes per model.\n\n### Also check out:\n1. The previous step of **[Ultimate Preprocessing Notebook](https:\/\/www.kaggle.com\/nickuzmenkov\/pp2021-ultimate-preprocessing)** where we remove duplicates, make stratified folds and tfrecords.\n2. The **[Inference Notebook](https:\/\/www.kaggle.com\/nickuzmenkov\/pp2021-tpu-tf-inference)**.\n\n### Imports"}}