{"cell_type":{"655d2221":"code","3633d8a9":"code","71f9a64a":"markdown"},"source":{"655d2221":"#!\/usr\/bin\/env python3\n# coding: utf-8\n\n# This is a fixed version of [@felipebihaiek](https:\/\/www.kaggle.com\/felipebihaiek) [episode scraper](https:\/\/www.kaggle.com\/felipebihaiek\/google-football-episode-scraper-quick-fix), which returned a TypeError for None values.\n# The downloaded files are zipped at the end of the notebook. Make sure that you do not excess the memory limit by downloading too much files. You can change the first selected team by modifying the top_teams[i] i indice at the top of the second to last cell\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport requests\nimport json\nimport datetime\nimport time\nimport itertools\n\nMIN_FINAL_RATING = 850 # top submission in a match must have reached this score\nglobal num_api_calls_today\nnum_api_calls_today = 0\nDIR = 'episode'\nMAXAPICALLS = 3600 # this is the maximum per day permitted by the API\nBUFFER = 1\n\nbase_url = \"https:\/\/www.kaggle.com\/requests\/EpisodeService\/\"\nget_url = base_url + \"GetEpisodeReplay\"\nlist_url = base_url + \"ListEpisodes\"\n\n# inital team list\nr = requests.post(list_url, json = {\"teamId\": 5786854}) # arbitrary ID, change to leading ID during challenge\nrj = r.json()\nteams_df = pd.DataFrame(rj['result']['teams'])\n\nteams_df.sort_values('publicLeaderboardRank', inplace = True)\n# Selecting the top 50 teams\nteams_df = teams_df.iloc[:50,:]\nteams_df\n\ndef getTeamEpisodes(team_id,teams_df):\n\n    r = requests.post(list_url, json = {\"teamId\":  int(team_id)})\n    rj = r.json()\n\n    # update teams list\n    # global teams_df\n    teams_df_new = pd.DataFrame(rj['result']['teams'])\n\n    if len(teams_df.columns) == len(teams_df_new.columns) and (teams_df.columns == teams_df_new.columns).all():\n        teams_df = pd.concat( (teams_df, teams_df_new.loc[[c for c in teams_df_new.index if c not in teams_df.index]] ) )\n        teams_df.sort_values('publicLeaderboardRank', inplace = True)\n    else:\n        print('teams dataframe did not match')\n\n    # make df\n    team_episodes = pd.DataFrame(rj['result']['episodes'])\n    team_episodes['avg_score'] = -1;\n\n    agents = []\n    for i in range(len(team_episodes)):\n        agents.append(team_episodes['agents'].loc[i])\n    not_none_agents = []\n    for i in range(len(agents)):\n        l = [a['updatedScore'] for a in agents[i] if a['updatedScore'] is not None]\n        if l:\n            not_none_agents.append(agents[i])\n            \n    for i in range(len(not_none_agents)):\n        agents = not_none_agents[i]\n        agent_scores = [a['updatedScore'] for a in agents if a['updatedScore'] is not None]\n        team_episodes.loc[i, 'submissionId'] = max([a['submissionId'] for a in agents])\n        team_episodes.loc[i, 'updatedScore'] = max([a['updatedScore'] for a in agents]) \n    \n    team_episodes['final_score'] = team_episodes['updatedScore']\n    team_episodes.sort_values('final_score', ascending = False, inplace=True)\n    \n    return rj, team_episodes\n\n\ndef saveEpisode(epid, rj):\n    # request\n    re = requests.post(get_url, json = {\"EpisodeId\": int(epid)})\n\n    # save replay\n    with open(DIR + '\/{}.json'.format(epid), 'w') as f:\n        f.write(re.json()['result']['replay'])\n   \n\npulled_teams = {}\n# populate pulled_episodes\n## pulled_episodes = []\npulled_episodes = [ os.path.splitext(file)[0] for file in itertools.chain(os.listdir(DIR), os.listdir('invalid')) ]\n\nstart_time = datetime.datetime.now()\nr = BUFFER;\n\nwhile num_api_calls_today < MAXAPICALLS:\n    # pull team\n    top_teams = [i for i in teams_df.id if i not in pulled_teams]\n    if len(top_teams) > 0:\n        team_id = top_teams[0]\n    else:\n        break;\n        \n    # get team data\n    team_json, team_df = getTeamEpisodes(team_id,teams_df); r+=1;\n    \n    num_api_calls_today+=1\n    \n    print('{} calls and getting: {}'.format(num_api_calls_today, teams_df.loc[teams_df.id == team_id].iloc[0].teamName))\n    \n    team_df = team_df[  (MIN_FINAL_RATING is None or (team_df.final_score > MIN_FINAL_RATING))]\n    \n    print('   {} in score range from {} submissions'.format(len(team_df), len(team_df.submissionId.unique() ) ) )\n    \n    team_df = team_df[~team_df.id.isin(pulled_episodes)]        \n    print('      {} remain to be downloaded'.format(len(team_df)))\n        \n    # pull games\n    target_team_games = max(0, MAXAPICALLS - num_api_calls_today)\n     \n    pulled_teams[team_id] = 0\n    \n    i = 0\n    while i < len(team_df) and pulled_teams[team_id] < target_team_games:\n        epid = team_df.id.iloc[i]\n        if not (epid in pulled_episodes):\n            try:\n                saveEpisode(epid, team_json); r+=1;\n                num_api_calls_today+=1\n            except:\n                time.sleep(20)\n                i+=1;\n                continue;\n                \n            pulled_episodes.append(epid)\n            pulled_teams[team_id] += 1\n\n            if r > (datetime.datetime.now() - start_time).seconds:\n                time.sleep( r - (datetime.datetime.now() - start_time).seconds)\n\n        i+=1;","3633d8a9":"#!\/bin\/bash -e                                                                                                                                                               \n\ncd $HOME\/rockPaperScissors\n\n.\/episode-scraper.py\n\n# tidy invalid responses                                                                                                                                                     \nfor file in `egrep -l INVALID episode\/*json` ; do\n  mv $file invalid\ndone\n\nrm -rf upload\nmkdir upload\n\nNEPISODE=`ls episode\/*json | wc -l`\n\ncat << EOF > upload\/dataset-metadata.json                                                                                                                                    \n{                                                                                                                                                                            \n  \"title\": \"Rock, Paper, Scissors: Episode archive\",                                                                                                                         \n  \"subtitle\": \"$NEPISODE episodes from top leaderboard teams\",                                                                                                               \n  \"id\": \"tonyrobinson\/rps-episode\",                                                                                                                                          \n  \"licenses\": [{\"name\": \"CC0-1.0\"}]                                                                                                                                          \n}                                                                                                                                                                            \nEOF                                                                                                                                                                          \n\ntar --create --auto-compress --file upload\/rps-episode.tar.gz --directory=episode .\n\n# kaggle datasets create --path upload --public                                                                                                                              \nkaggle datasets version --path upload --message \"Updated to $NEPISODE episodes\"\n\nexit 0\n","71f9a64a":"This is the code that I've used to scrape the rps-episodes (Python wrapped in bash).  For CREDITS see the top of the Python, this isn't my code.\n\nThere are two known bugs:\n1. If you set MIN_FINAL_RATING too high then it doens't use all of the 3600 API calls\n2. It can fail when data is sparse\n\nThere are several requests for extensions, but I don't have time so that's why this code is here - take it and enjoy!"}}