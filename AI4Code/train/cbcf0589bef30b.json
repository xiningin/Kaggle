{"cell_type":{"a87a3386":"code","04dab6e0":"code","6ceee088":"code","9c3ee12e":"code","724c3b89":"code","56f618e1":"code","8b100245":"code","b02560bd":"code","fb7ba470":"code","327f858b":"code","fd9c37cf":"code","1a91919e":"code","702447c7":"code","27111029":"code","7ebd6eef":"code","c4aff5bd":"code","a6c395eb":"code","a7c36a68":"code","20f9e88f":"code","7380385d":"code","50aabfd0":"code","65e501cf":"code","fe383755":"code","84e71cac":"code","1eee1e7e":"code","a6c96b7f":"code","73fdbe2f":"code","904d99f4":"markdown","402ddbad":"markdown","99dee6d8":"markdown","b0b90612":"markdown","eac90fa4":"markdown","2c603727":"markdown","82e5bb9c":"markdown","a78d80f7":"markdown","3dcdcf92":"markdown","85bae7ea":"markdown","d8da3ef1":"markdown","5aa8c932":"markdown","8482bd64":"markdown","7b66dca2":"markdown","78f2276b":"markdown","c2f2a2ee":"markdown","bcacd04f":"markdown","213995fc":"markdown","73c52b2d":"markdown","cb464d4d":"markdown"},"source":{"a87a3386":"# install an older version of tensorflow \n# (the implementation may not work with the most recent TensorFlow release)\n\n!pip install keras==2.3.1\n!pip install tensorflow==2.1.0","04dab6e0":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\n\nimport glob\nimport os\n\nfrom keras import Input\nfrom keras.applications import VGG19\nfrom keras.callbacks import TensorBoard\nfrom keras.layers import BatchNormalization, Activation, LeakyReLU, Add, Dense\nfrom keras.layers import Conv2D, UpSampling2D\nfrom keras.models import Model\nfrom keras.optimizers import Adam\n\n\nimport random\nfrom numpy import asarray\nfrom itertools import repeat\n\nimport imageio\nfrom imageio import imread\nfrom PIL import Image\nfrom skimage.transform import resize as imresize\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint(\"Tensorflow version \" + tf.__version__)\nprint(\"Keras version \" + tf.keras.__version__)","6ceee088":"# data path\nTRAIN_PATH = r'..\/input\/kermany2018\/OCT2017 \/train\/'\nVAL_PATH = r'..\/input\/kermany2018\/OCT2017 \/val\/'\nTEST_PATH = r'..\/input\/kermany2018\/OCT2017 \/test\/'\ndata_path = TRAIN_PATH\n\nepochs = 5001\n\n# batch size equals to 8 (due to RAM limits)\nbatch_size = 8\n\n# define the shape of low resolution image (LR) \nlow_resolution_shape = (64, 64, 3)\n\n# define the shape of high resolution image (HR) \nhigh_resolution_shape = (256, 256, 3)\n\n# optimizer for discriminator, generator \ncommon_optimizer = Adam(0.0002, 0.5)\n\n# use seed for reproducible results\nSEED = 2020 \ntf.random.set_seed(SEED)","9c3ee12e":"def get_train_images(data_path):\n\n    CLASSES = ['CNV', 'DME', 'DRUSEN', 'NORMAL']\n    image_list = []\n\n    for class_type in CLASSES:\n        image_list.extend(glob.glob(data_path + class_type + '\/*'))\n    \n    return image_list    ","724c3b89":"def find_img_dims(image_list):\n    \n    min_size = []\n    max_size = []\n    \n    for i in range(len(image_list)):\n        im = Image.open(image_list[i])\n        min_size.append(min(im.size))\n        max_size.append(max(im.size))\n    \n    return min(min_size), max(max_size)","56f618e1":"# get min\/max image sizes\n\nimage_list = get_train_images(data_path)\nmin_size, max_size = find_img_dims(image_list)\nprint('The min and max image dims are {} and {} respectively.'\n      .format(min_size, max_size))","8b100245":"def compute_psnr(original_image, generated_image):\n    \n    original_image = tf.convert_to_tensor(original_image, dtype=tf.float32)\n    generated_image = tf.convert_to_tensor(generated_image, dtype=tf.float32)\n    psnr = tf.image.psnr(original_image, generated_image, max_val=1.0)\n\n    return tf.math.reduce_mean(psnr, axis=None, keepdims=False, name=None)","b02560bd":"def plot_psnr(psnr):\n    \n    psnr_means = psnr['psnr_quality']\n    plt.figure(figsize=(10,8))\n    plt.plot(psnr_means)    \n    plt.xlabel('Epochs')\n    plt.ylabel('PSNR') \n    plt.title('PSNR')","fb7ba470":"def compute_ssim(original_image, generated_image):\n    \n    original_image = tf.convert_to_tensor(original_image, dtype=tf.float32)\n    generated_image = tf.convert_to_tensor(generated_image, dtype=tf.float32)\n    ssim = tf.image.ssim(original_image, generated_image, max_val=1.0, filter_size=11,\n                          filter_sigma=1.5, k1=0.01, k2=0.03)\n\n    return tf.math.reduce_mean(ssim, axis=None, keepdims=False, name=None)","327f858b":"def plot_ssim(ssim):\n    \n    ssim_means = ssim['ssim_quality']\n\n    plt.figure(figsize=(10,8))\n    plt.plot(ssim_means)\n    plt.xlabel('Epochs')\n    plt.ylabel('SSIM')\n    plt.title('SSIM')","fd9c37cf":"def plot_loss(losses):\n\n    d_loss = losses['d_history']\n    g_loss = losses['g_history']\n    \n   \n    plt.figure(figsize=(10,8))\n    plt.plot(d_loss, label=\"Discriminator loss\")\n    plt.plot(g_loss, label=\"Generator loss\")\n    \n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.title(\"Loss\")    \n    plt.legend()","1a91919e":"def sample_images(image_list, batch_size, high_resolution_shape, low_resolution_shape):\n    \n    \"\"\"\n    Pre-process a batch of training images\n    \"\"\"\n    \n    # image_list is the list of all images\n    # ransom sample a batch of images\n    images_batch = np.random.choice(image_list, size=batch_size)\n    \n    lr_images = []\n    hr_images = []\n    \n\n    for img in images_batch:\n  \n        img1 = imread(img, as_gray=False, pilmode='RGB')\n        #img1 = imread(img, pilmode='RGB')\n        img1 = img1.astype(np.float32)\n        \n        # change the size     \n        img1_high_resolution = imresize(img1, high_resolution_shape)\n        img1_low_resolution = imresize(img1, low_resolution_shape)\n                \n\n        # do a random horizontal flip\n        if np.random.random() < 0.5:\n            img1_high_resolution = np.fliplr(img1_high_resolution)\n            img1_low_resolution = np.fliplr(img1_low_resolution)\n       \n        hr_images.append(img1_high_resolution)\n        lr_images.append(img1_low_resolution)\n        \n   \n    # convert lists into numpy ndarrays\n    return np.array(hr_images), np.array(lr_images)    ","702447c7":"def save_images(original_image, lr_image, sr_image, path):\n    \n    \"\"\"\n    Save LR, HR (original) and generated SR\n    images in one panel \n    \"\"\"\n    \n    fig, ax = plt.subplots(1,3, figsize=(10, 6))\n\n    images = [original_image, lr_image, sr_image]\n    titles = ['HR', 'LR','SR - generated']\n\n    for idx,img in enumerate(images):\n        # (X + 1)\/2 to scale back from [-1,1] to [0,1]\n        ax[idx].imshow((img + 1)\/2.0, cmap='gray')\n        ax[idx].axis(\"off\")\n    for idx, title in enumerate(titles):    \n        ax[idx].set_title('{}'.format(title))\n        \n    plt.savefig(path)    ","27111029":"def residual_block(x):\n\n    filters = [64, 64]\n    kernel_size = 3\n    strides = 1\n    padding = \"same\"\n    momentum = 0.8\n    activation = \"relu\"\n\n    res = Conv2D(filters=filters[0], kernel_size=kernel_size, strides=strides, padding=padding)(x)\n    res = Activation(activation=activation)(res)\n    res = BatchNormalization(momentum=momentum)(res)\n\n    res = Conv2D(filters=filters[1], kernel_size=kernel_size, strides=strides, padding=padding)(res)\n    res = BatchNormalization(momentum=momentum)(res)\n\n    res = Add()([res, x])\n    \n    return res","7ebd6eef":"def build_generator():\n    \n    # use 16 residual blocks in generator\n    residual_blocks = 16\n    momentum = 0.8\n    \n    # input LR dimension: 4x downsample of HR\n    input_shape = (64, 64, 3)\n    \n    # input for the generator\n    input_layer = Input(shape=input_shape)\n    \n    # pre-residual block: conv layer before residual blocks \n    gen1 = Conv2D(filters=64, kernel_size=9, strides=1, padding='same', activation='relu')(input_layer)\n    \n    # add 16 residual blocks\n    res = residual_block(gen1)\n    for i in range(residual_blocks - 1):\n        res = residual_block(res)\n    \n    # post-residual block: conv and batch-norm layer after residual blocks\n    gen2 = Conv2D(filters=64, kernel_size=3, strides=1, padding='same')(res)\n    gen2 = BatchNormalization(momentum=momentum)(gen2)\n    \n    # take the sum of pre-residual block(gen1) and post-residual block(gen2)\n    gen3 = Add()([gen2, gen1])\n    \n    # upsampling\n    gen4 = UpSampling2D(size=2)(gen3)\n    gen4 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same')(gen4)\n    gen4 = Activation('relu')(gen4)\n    \n    # upsampling\n    gen5 = UpSampling2D(size=2)(gen4)\n    gen5 = Conv2D(filters=256, kernel_size=3, strides=1, padding='same')(gen5)\n    gen5 = Activation('relu')(gen5)\n    \n    # conv layer at the output\n    gen6 = Conv2D(filters=3, kernel_size=9, strides=1, padding='same')(gen5)\n    output = Activation('tanh')(gen6)\n    \n    # model \n    model = Model(inputs=[input_layer], outputs=[output], name='generator')\n\n    return model","c4aff5bd":"generator = build_generator()","a6c395eb":"def build_discriminator():\n    \n    # define hyperparameters\n    leakyrelu_alpha = 0.2\n    momentum = 0.8\n    \n    # the input is the HR shape\n    input_shape = (256, 256, 3)\n    \n    # input layer for discriminator\n    input_layer = Input(shape=input_shape)\n    \n    # 8 convolutional layers with batch normalization  \n    dis1 = Conv2D(filters=64, kernel_size=3, strides=1, padding='same')(input_layer)\n    dis1 = LeakyReLU(alpha=leakyrelu_alpha)(dis1)\n\n    dis2 = Conv2D(filters=64, kernel_size=3, strides=2, padding='same')(dis1)\n    dis2 = LeakyReLU(alpha=leakyrelu_alpha)(dis2)\n    dis2 = BatchNormalization(momentum=momentum)(dis2)\n\n    dis3 = Conv2D(filters=128, kernel_size=3, strides=1, padding='same')(dis2)\n    dis3 = LeakyReLU(alpha=leakyrelu_alpha)(dis3)\n    dis3 = BatchNormalization(momentum=momentum)(dis3)\n\n    dis4 = Conv2D(filters=128, kernel_size=3, strides=2, padding='same')(dis3)\n    dis4 = LeakyReLU(alpha=leakyrelu_alpha)(dis4)\n    dis4 = BatchNormalization(momentum=0.8)(dis4)\n\n    dis5 = Conv2D(256, kernel_size=3, strides=1, padding='same')(dis4)\n    dis5 = LeakyReLU(alpha=leakyrelu_alpha)(dis5)\n    dis5 = BatchNormalization(momentum=momentum)(dis5)\n\n    dis6 = Conv2D(filters=256, kernel_size=3, strides=2, padding='same')(dis5)\n    dis6 = LeakyReLU(alpha=leakyrelu_alpha)(dis6)\n    dis6 = BatchNormalization(momentum=momentum)(dis6)\n\n    dis7 = Conv2D(filters=512, kernel_size=3, strides=1, padding='same')(dis6)\n    dis7 = LeakyReLU(alpha=leakyrelu_alpha)(dis7)\n    dis7 = BatchNormalization(momentum=momentum)(dis7)\n\n    dis8 = Conv2D(filters=512, kernel_size=3, strides=2, padding='same')(dis7)\n    dis8 = LeakyReLU(alpha=leakyrelu_alpha)(dis8)\n    dis8 = BatchNormalization(momentum=momentum)(dis8)\n    \n    # fully connected layer \n    dis9 = Dense(units=1024)(dis8)\n    dis9 = LeakyReLU(alpha=0.2)(dis9)\n    \n    # last fully connected layer - for classification \n    output = Dense(units=1, activation='sigmoid')(dis9)   \n    \n    model = Model(inputs=[input_layer], outputs=[output], name='discriminator')\n    \n    return model","a7c36a68":"discriminator = build_discriminator()\ndiscriminator.trainable = True\ndiscriminator.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])","20f9e88f":"VGG19_base = VGG19(weights=\"imagenet\")","7380385d":"def build_VGG19():\n    \n    input_shape = (256, 256, 3)\n    VGG19_base.outputs = [VGG19_base.get_layer('block5_conv2').output]\n    input_layer = Input(shape=input_shape)\n    features = VGG19_base(input_layer)\n    model = Model(inputs=[input_layer], outputs=[features])\n    \n    return model","50aabfd0":"fe_model = build_VGG19()\nfe_model.trainable = False\nfe_model.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])","65e501cf":"def build_adversarial_model(generator, discriminator, feature_extractor):\n    \n    # input layer for high-resolution images\n    input_high_resolution = Input(shape=high_resolution_shape)\n\n    # input layer for low-resolution images\n    input_low_resolution = Input(shape=low_resolution_shape)\n\n    # generate high-resolution images from low-resolution images\n    generated_high_resolution_images = generator(input_low_resolution)\n\n    # extract feature maps from generated images\n    features = feature_extractor(generated_high_resolution_images)\n    \n    # make a discriminator non-trainable \n    discriminator.trainable = False\n    discriminator.compile(loss='mse', optimizer=common_optimizer, metrics=['accuracy'])\n\n    # discriminator will give us a probability estimation for the generated high-resolution images\n    probs = discriminator(generated_high_resolution_images)\n\n    # create and compile \n    adversarial_model = Model([input_low_resolution, input_high_resolution], [probs, features])\n    adversarial_model.compile(loss=['binary_crossentropy', 'mse'], loss_weights=[1e-3, 1], optimizer=common_optimizer)\n    \n    return adversarial_model\n","fe383755":"adversarial_model = build_adversarial_model(generator, discriminator, fe_model)","84e71cac":"# initialize \n\nlosses = {\"d_history\":[], \"g_history\":[]}\npsnr = {'psnr_quality': []}\nssim = {'ssim_quality': []}","1eee1e7e":"# training loop\n\nfor epoch in range(epochs):\n\n    d_history = []\n    g_history = []\n    \n    image_list = get_train_images(data_path)\n    \n    \"\"\"\n    Train the discriminator network\n    \"\"\"\n    \n    hr_images, lr_images = sample_images(image_list, \n                                         batch_size=batch_size,\n                                         low_resolution_shape=low_resolution_shape,\n                                         high_resolution_shape=high_resolution_shape)\n    \n    \n    # normalize the images\n    hr_images = hr_images \/ 127.5 - 1.\n    lr_images = lr_images \/ 127.5 - 1.\n    \n    # generate high-resolution images from low-resolution images\n    generated_high_resolution_images = generator.predict(lr_images)\n    \n    # generate a batch of true and fake labels \n    real_labels = np.ones((batch_size, 16, 16, 1))\n    fake_labels = np.zeros((batch_size, 16, 16, 1))\n    \n \n    d_loss_real = discriminator.train_on_batch(hr_images, real_labels)\n    d_loss_real =  np.mean(d_loss_real)\n    d_loss_fake = discriminator.train_on_batch(generated_high_resolution_images, fake_labels)\n    d_loss_fake =  np.mean(d_loss_fake)\n    \n    # calculate total loss of discriminator as average loss on true and fake labels\n    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n    losses['d_history'].append(d_loss)\n   \n\n    \"\"\"\n        Train the generator network\n    \"\"\"\n      \n    # sample a batch of images    \n    hr_images, lr_images = sample_images(image_list, \n                                         batch_size=batch_size,\n                                         low_resolution_shape=low_resolution_shape,\n                                         high_resolution_shape=high_resolution_shape)\n    \n    \n    # normalize the images\n    hr_images = hr_images \/ 127.5 - 1.\n    lr_images = lr_images \/ 127.5 - 1.\n    \n    \n    \n    # extract feature maps for true high-resolution images\n    image_features = fe_model.predict(hr_images)\n\n\n    \n    # train the generator\n    g_loss = adversarial_model.train_on_batch([lr_images, hr_images],\n                                               [real_labels, image_features])\n    \n    losses['g_history'].append(0.5 * (g_loss[1]))\n    \n    \n    \n    # calculate the psnr  \n    ps = compute_psnr(hr_images, generated_high_resolution_images) \n    psnr['psnr_quality'].append(ps)\n            \n    # calculate the ssim \n    ss = compute_ssim(hr_images, generated_high_resolution_images)   \n    ssim['ssim_quality'].append(ss)\n\n    \n  \n    \"\"\"\n        save and print image samples\n    \"\"\"\n    \n    if epoch % 500 == 0:\n        \n        hr_images, lr_images = sample_images(image_list, \n                                             batch_size=batch_size,\n                                             low_resolution_shape=low_resolution_shape,\n                                             high_resolution_shape=high_resolution_shape)\n    \n    \n        # normalize the images\n        hr_images = hr_images \/ 127.5 - 1.\n        lr_images = lr_images \/ 127.5 - 1.\n    \n    \n        generated_images = generator.predict_on_batch(lr_images)\n    \n        for index, img in enumerate(generated_images):\n            if index < 3:   # comment this line to display all the images\n                save_images(hr_images[index], lr_images[index], img,\n                            path=\"\/kaggle\/working\/img_{}_{}\".format(epoch, index))  ","a6c96b7f":"# plots - post training\n\nplot_loss(losses)\nplot_psnr(psnr)\nplot_ssim(ssim)","73fdbe2f":"# save model weights\n\ngenerator.save_weights(\"\/kaggle\/working\/srgan_generator.h5\")\ndiscriminator.save_weights(\"\/kaggle\/working\/srgan_discriminator.h5\")","904d99f4":"Optical coherence tomography (OCT) scans of the retina are used to produce high resolution cross-sectional images of the retina. OCT imaging is the now the standard of care in opthalmology\nand there is interest in applying AI methods to enhance conventional OCT images from commercial systems. \n\nThe computational enhancement of image resolution, is known as *super-resolution*. Generative Adversarial Networks (GANs) <a href=\"#ref1\">[1]<\/a> are one of newer methods that have been applied to super resolution and in this notebook we use a  Super-Resolution GAN (SRGAN) <a href=\"#ref2\">[2]<\/a> to enhance subsampled OCT scans.\n\nThe SRGAN, introduced in 2016, addressed the issue of reconstructing high resolution (HR) images from low resolution (LR) images such that fine texture detail in the reconstructed super resolution (SR) images was not lost. Here the authors used a perceptual loss instead of a pixel-wise Mean Squared Error (MSE) loss. MSE loss approaches give a high Peak Signal-to-Noise (PSNR) value, but they also tend to produce overly-smooth images with insufficient high-frequency details. The perceptual loss by contrast has a content loss component that computes pixel-wise differences in feature space (not pixel space) and this results in an SR image that is closer to the subjective evaluation of human observers.\n\nThe SRGAN model uses a deep neural network (built with residual blocks) and optimized using perceptual loss in a GAN framework. A VGG-19 network is used for feature extraction; this allows us to compute the feature distance between the original and generated images sent through the feature extractor.\n\nIn this exercise we use an SRGAN design which is faithful to the original SRGAN <a href=\"#ref2\">[2]<\/a>. This architecture used a pre-trained VGG-19 feature extractor and gave photorealistic results on large (4x) upsampled low resolution images. It has been applied to the DIV2K, CelebA and other natural image datasets and here we want to see how it performs on OCT data. This network will serve as a baseline for further experiments with upscaling, choice of feature extractor etcetera.\n\nThe SRGAN is implemented as follows:\n\n**Training**\n1. We downsample HR OCT images by 4x to synthetically create LR training data. This gives us pairs of HR and LR images for the training data set.\n2. The Generator upsamples LR images by 4x and will be trained to generate SR images.\n3. The discriminator will be trained to distinguish between HR\/SR images; the GAN loss is backpropagated to the discriminator and the generator.\n\n**Evaluation**  \nThe  visual quality of generated images will be observed. In addition standard quantitative metrics, Peak Signal-to-Noise Ratio and Structural Similarity Index (PSNR, SSIM), will be used to assess the results.\n\n**Code**  \nThe code is based on the SRGAN  paper <a href=\"#ref2\">[2]<\/a> and Kailash Ahirwar's code <a href=\"#ref3\">[3]<\/a>.","402ddbad":"#### Plot loss function","99dee6d8":"The SRGAN has the following code components:\n 1. Generator network\n 2.  Discriminator network\n 3. Feature extractor using the VGG19 network\n 4. Adversarial framework","b0b90612":"## Enhancing OCT image resolution with SRGANs\n\n\n#### Keywords: unsupervised super-resolution, SRGAN-VGG19","eac90fa4":"## V. SRGAN-VGG19","2c603727":"### IV C. Sampling, saving images","82e5bb9c":"## IV. Utility functions\n\nQuantitative metrics for image quality  \nLoss functions  \nPlots  \nImage processing: sampling and saving images","a78d80f7":"## I. Introduction","3dcdcf92":"### V 3. VGG19 Feature Extractor ","85bae7ea":"### IV A. Metrics","d8da3ef1":"## II. Imports and globals","5aa8c932":"### V 1. Generator","8482bd64":"## VII. References and further reading\n\n\n<a name=\"ref1\"><\/a>[1] [Goodfellow et al. 'Generative Adversarial Nets'](https:\/\/arxiv.org\/pdf\/1406.2661.pdf)\n\n<a name=\"ref2\"><\/a>[2] [Ledig et al. 'Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network'](https:\/\/arxiv.org\/abs\/1609.04802)\n\n<a name=\"ref3\"><\/a>[3] [Kailash Ahirwar. 'Generative Adversarial Networks Projects'](https:\/\/github.com\/PacktPublishing\/Generative-Adversarial-Networks-Projects)\n\n<a name=\"ref4\"><\/a>[4] [Saeed Anwar et al. 'A Deep Journey into Super-resolution: A Survey'](https:\/\/arxiv.org\/pdf\/1904.07523.pdf)\n\n<a name=\"ref5\"><\/a>[5] [Xintao Wang et al. 'ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks'](https:\/\/arxiv.org\/pdf\/1809.00219.pdf)","7b66dca2":"There are 16 residual blocks and 2 upsampling blocks. The generator follows the architecture outlined in [2]. ","78f2276b":"### IV B. Loss Functions\n\nThe most important contribution of the SRGAN paper was the use of a *perceptual loss* function. \n\n\n**Perceptual Loss**  is a weighted sum of the *content loss* and *adversarial loss*.\n\n\n$${ l^{SR} = l_X^{SR} + 10^{-3}l_{Gen}^{SR}}$$\n\n$l^{SR}$ - perceptual loss   \n$l_X^{SR}$ - content loss   \n$l_{Gen}^{SR}$ - adversarial loss \n\n\n****************************\n\n**1. Content Loss**   \nThe SRGAN replaced the *MSE loss* with a *VGG loss*. Both losses are defined below: \n\n         \n**Pixel-wise MSE loss** is the mean squared error between each pixel in the original HR image and a the corresponding pixel in the generated SR image.\n\n\n**VGG loss** is the euclidean distance between the feature maps of the generated SR image and the original HR  image. The feature maps are the activation layers of the pre-trained  VGG 19 network. \n\n$${ l_{{VGG}\/{i,j}}^{SR} = {1 \\over {W_{i,j}H_{i,j}}} \\sum\\limits_{x=1}^{W_{i,j}} \\sum\\limits_{y=1}^{H_{i,j}}  ({\\phi}_{i,j}(I^{HR})_{x,y} - {\\phi}_{i,j} (G_{{\\theta}_G} (I^{LR}))_{x,y})^2}$$\n\n\n$ l_{{VGG}\/{i,j}}^{SR} $  -  VGG loss    \n$ {\\phi}_{i,j} $  -   the feature map obtained by the j-th convolution (after activation) before the i-th maxpooling layer within the VGG19 network\n\n\n\n**2. Adversarial Loss**  \nThis is calculated based on probabilities provided by Discriminator.\n\n$${ l_{Gen}^{SR} = \\sum\\limits_{n=1}^{N} - \\log{D_{{\\theta}_D}} (G_{{\\theta}_G} (I^{LR}))}$$\n\n$ l_{Gen}^{SR} $  -  generative loss  \n$ D $  -  discriminator function    \n$ D_{{\\theta}_D} $  -  discriminator function parametrized with $ {\\theta}_D $   \n$ {D_{{\\theta}_D}} (G_{{\\theta}_G} (I^{LR})) $   -  probability that the reconstructed image $ \n$ G_{{\\theta}_G} (I^{LR}) $  is a natural HR image","c2f2a2ee":"#### 2. SSIM - Structural Similarity Index\n\n\nSSIM measures the perceptual difference between two similar images [(see Wikipedia)](https:\/\/en.wikipedia.org\/wiki\/Structural_similarity).\n\n$${ SSIM(x, y) = {(2 \\mu_x \\mu_y + c_1) (2 \\sigma_{xy} + c_2) \\over (\\mu_x^2 + \\mu_y^2 + c_1) ( \\sigma_x^2 + \\sigma_y^2 +c_2)}  }$$\n\n\n$ \\mu_x, \\mu_y$       - average value for image $x, y$    \n$ \\sigma_x, \\sigma_y$ - standard deviation for image $x, y$     \n$ \\sigma_{xy}$        - covariance  of $x$ and $y$      \n$ c_1, c_2 $          - coefficients ","bcacd04f":"## VI. Training \n","213995fc":"### V 2. Discriminator","73c52b2d":"#### 1. PSNR - Peak Signal-to-Noise ratio\n\n\nPSNR is the ratio between maximum possible power of signal and power of corrupting noise (Wikipedia). \n\n\n$${ PSNR = 10  \\log_{10}  \\left( {MAX_I^2 \\over MSE} \\right) }$$\n\n$ MAX_I $  -  maximum possible power of a signal of image I  \n$ MSE $  -  mean squared error pixel by pixel ","cb464d4d":"## III. Data\n\nLoad data, process data, EDA"}}