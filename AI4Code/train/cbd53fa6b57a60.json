{"cell_type":{"804463f2":"code","f487fc7f":"code","17750f53":"code","98a98f43":"code","d0fe1db5":"code","7de72cdb":"code","cfda3f22":"code","00a627e3":"code","98f2ba22":"code","31ce1bfc":"code","bc9e455d":"code","f9db8234":"markdown","0e6a2342":"markdown","ca638468":"markdown","e9df6e97":"markdown","060868ba":"markdown","ede7ba76":"markdown","6cf8c3f8":"markdown","26dbed6c":"markdown","66902998":"markdown","a509e07e":"markdown","6b6a236c":"markdown"},"source":{"804463f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nfrom wordcloud import WordCloud\nfrom keras.optimizers import Adam\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.layers import Dense, Dropout, LSTM, Embedding\nfrom sklearn.preprocessing import Normalizer, LabelEncoder\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f487fc7f":"train = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv', encoding='latin1')\ntest = pd.read_csv('..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv', encoding='latin1')","17750f53":"train","98a98f43":"test","d0fe1db5":"le = LabelEncoder()\nX_train = train['OriginalTweet']\ny_train = le.fit_transform(train['Sentiment'])\n\nX_test = test['OriginalTweet']\ny_test = le.transform(test['Sentiment'])","7de72cdb":"BoW = CountVectorizer()\ntfidf = TfidfTransformer()\ntok = Tokenizer(num_words=50, split=\" \")","cfda3f22":"X_tr_bow = BoW.fit_transform(X_train)\nX_te_bow = BoW.transform(X_test)\n\nX_tr_tfidf = tfidf.fit_transform(X_tr_bow)\nX_te_tfidf = tfidf.transform(X_te_bow)\n\ntok.fit_on_texts(X_train)\ntok_train = tok.texts_to_sequences(X_train)\nX_tr_tok = sequence.pad_sequences(tok_train, maxlen=11, dtype='float32')\n\ntok_test = tok.texts_to_sequences(X_test)\nX_te_tok = sequence.pad_sequences(tok_test, maxlen=11, dtype='float32')","00a627e3":"wordcloud = WordCloud(background_color='white').generate(\" \".join(X))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","98f2ba22":"month = pd.to_datetime(train['TweetAt']).dt.month\nday = pd.to_datetime(train['TweetAt']).dt.day\n\ncount = Counter(day)\nplt.bar(count.keys(), count.values(), color='blue')\nplt.xlabel('Days')\nplt.ylabel('Occurence')\nplt.title('Distribution of the tweets per day')\nplt.show()\n\ncount = Counter(month)\nplt.bar(count.keys(), count.values(), color='red')\nplt.xlabel('Month')\nplt.ylabel('Occurence')\nplt.title('Distribution of the tweets per month')\nplt.show()","31ce1bfc":"bow_clf = SGDClassifier(eta0=0.01, learning_rate='optimal', penalty='l1', max_iter=100)\nbow_clf.fit(X_tr_bow, y_train)\nbow_score = bow_clf.score(X_te_bow, y_test)\nprint('Bag of Words score: ' + str(bow_score))\n\ntfidf_clf = SGDClassifier(eta0=0.0001, loss='modified_huber', penalty='l1', learning_rate='optimal')\ntfidf_clf.fit(X_tr_tfidf, y_train)\ntfidf_score = tfidf_clf.score(X_te_tfidf, y_test)\nprint('TFIDF score:        ' + str(tfidf_score))\n\ntok_clf = SGDClassifier()\ntok_clf.fit(X_tr_tok, y_train)\ntok_score = tok_clf.score(X_te_tok, y_test)\nprint('Tokenizer score:    ' + str(tok_score))","bc9e455d":"pred = bow_clf.predict(X_te_bow)\noutput = pd.DataFrame({'Real': y_test, 'Prediction': pred})\noutput.to_csv('submission.csv', index=False)","f9db8234":"## Thank you for reading my notebook. \n## If you enjoyed it and found it helpful, please upvote it as it would help me make more of these.","0e6a2342":"The final piece of visualisation is two bar charts which plot out how many tweets happened each day of the month and every month of the year. The month of March is when the most tweets happened, presumably because that was when lockdown started.","ca638468":"As seen above, the SGD classifier which has had its data engineered with a Bag of Words method has the highest accuracy, with a score of 60%. Meanwhile, the TFIDF score is closely behind with 59% and the Tokenizer is far back with 18%.","e9df6e97":"Now we create three models: Bag of Words, TFIDF and Tokenizer in order to be able to convert the text data (X) into a usable matrix of numbers. All models are created in order to test them out and see which one is the best for use.","060868ba":"At last, we create different transformations of the Bag of Word, TFIDF and Tokenizer methods. All three methods are types of NLP (Natural Language Processing) approaches that work in different ways to help AIs get better at reading and understanding text.","ede7ba76":"## Feature engineering\n\nFirstly we will gather the data. As seen below, we have acquired train and test sets, both consisting of UserName, ScreenName, Location, TweetAt, OriginalTweet and Sentiment columns.","6cf8c3f8":"# Data visualisation\n\nSubsequently, we will now visualise the data that has been engineered. This is useful as it can help us understand the trend of which our data follows.","26dbed6c":"Now, at the climax of our notebook, we shall create three SGD classifiers which each use the NLP approaches to create predictions as to whether a tweet is positive, negative or neutral.","66902998":"The 'OriginalTweet' feature from will be used as X and the 'Sentiment' as the target. Then, we encode the target (y) feature with a label encoder in order for it to be able to be inputted to the classifier.","a509e07e":"Below is a word cloud that shows which words are the most common. 'https' and 'co' are the most common due to people linking their websites in tweets. Following are the words 'COVID' and a misspelled version of 'coronavirus'. After that are words such as 'grocery' and 'supermarket', which isn't surprising due to people's concerns about not being able to go out and buy things.","6b6a236c":"# Coronavirus Tweet Classification\n\nWelcome to the Coronavirus Tweet Classification where today, we will be analysing 40,000 tweets about the Covid-19 pandemic and predict whether they are positive, negative or neutral. Using these predictions, we can figure out how people are dealing with the pandemic and we can take a step further in figuring out how to make the world a calmer place."}}