{"cell_type":{"997ff499":"code","7a1b2206":"code","1074d7cc":"code","167c52e6":"code","22e0564e":"code","c208d5b9":"code","68a6f682":"code","98843843":"code","f9162105":"code","8537c3d8":"code","7c02fd16":"code","1283953a":"code","a1ed4abc":"code","cdc700ae":"code","856e90e1":"markdown","fe216a64":"markdown","ec4ab4a1":"markdown","04eee74f":"markdown","709156d1":"markdown","287c6ed7":"markdown","070cd238":"markdown","f997ea5c":"markdown","e8709a2c":"markdown","134eef8c":"markdown","5476b911":"markdown","36a6ee68":"markdown"},"source":{"997ff499":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # plotting\nimport matplotlib.pyplot as plt # plots handling\nimport os # file management\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7a1b2206":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv', index_col=0)\ntest  = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv', index_col=0)","1074d7cc":"def basic_insights(frame, label=None):\n    space = 20\n    if label is not None:\n        print(f\"{'Name'.rjust(space)} : {label}\")\n    print(f\"{'Dimensions'.rjust(space)} : {frame.shape} <=> Total = {frame.shape[0] * frame.shape[1]}\")\n    print(f\"{'Missing values'.rjust(space)} : {frame.isna().sum().sum()} <=> {frame.isna().sum().sum() \/ (frame.shape[0] * frame.shape[1]) * 100}%\")","167c52e6":"basic_insights(train, label='Train')","22e0564e":"basic_insights(test, label='Test')","c208d5b9":"train.isna().sum()[ train.isna().sum() > 0 ] \/ train.shape[0] * 100","68a6f682":"#\u00a0We will delete the columns with the most missing values\ncols = [\"Alley\", \"FireplaceQu\", \"PoolQC\", \"Fence\", \"MiscFeature\"]\n\ntrain = train.drop(columns=cols)\ntest  = test.drop(columns=cols)","98843843":"convert_1 = {\n    'Po':0,\n    'Fa':1,\n    'TA':2,\n    'Gd':3,\n    'Ex':4\n}\n\nconvert_2 = {\n    'Unf':0,\n    'LwQ':1,\n    'Rec':2,\n    'BLQ':3,\n    'ALQ':4,\n    'GLQ':5\n}\n\nconvert_3 = {\n    'Y':1,\n    'N':0\n}\n\nmap_ordinal = {\n    'ExterQual': convert_1,\n    'ExterCond': convert_1,\n    'BsmtQual': convert_1,\n    'BsmtCond': convert_1,\n    'BsmtExposure': {'No': 0, 'Mn':1, 'Av':2, 'Gd':3},\n    'BsmtFinType1':convert_2,\n    'BsmtFinType2':convert_2,\n    'HeatingQC':convert_1,\n    'CentralAir':convert_3,\n    'KitchenQual':convert_1,\n    'FireplaceQu':convert_1,\n    'GarageFinish':{'Unf':0, 'RFn':1, 'Fin':2},\n    'GarageQual':convert_1,\n    'GarageCond':convert_1,\n    'PoolQC':convert_1,\n    'PavedDrive': {'Y':2, 'P':1, 'N':0}\n}","f9162105":"from sklearn.model_selection import train_test_split\nfrom pandas.api.types import CategoricalDtype\n\n#\u00a0This code generate list of all possible values for categorical variables\nall_data = pd.concat([train.copy(), test.copy()], axis=0)\nall_data = all_data.replace(map_ordinal)\ncategories = dict()\nfor cat in all_data.dtypes[all_data.dtypes == 'O'].index:\n    categories[cat] = all_data[cat].dropna().unique().tolist()\ndel all_data\n\ndef preprocessing(frame, scaler, is_train=False, target=None, submission=False):\n    # Log scaling the target if train\n    if is_train:\n        frame[target] = frame[target].apply(np.log)\n        \n    # Ordinal & Categorical variables handling\n    frame = frame.replace(map_ordinal)\n    \n    #\u00a0Get numericals and categorical variables\n    numericals   = frame.dtypes[frame.dtypes != 'O'].index.tolist()\n    categoricals = frame.dtypes[frame.dtypes == 'O'].index.tolist()\n    \n    if is_train:\n        numericals.remove(target)\n    \n    # Impute values by grouping with Neighborhood median\n    frame[numericals] = frame[numericals].fillna(frame[numericals].median())\n    \n    # Standardize numericals values\n    frame[numericals] = pd.DataFrame(scaler.fit_transform(frame[numericals]), columns=numericals) if is_train else pd.DataFrame(scaler.transform(frame[numericals]), columns=numericals)\n    \n    # Ont-Hot Encoding\n    for key in categories.keys():\n        categorical_type = CategoricalDtype(categories=categories[key], ordered=False)\n        frame[key] = frame[key].astype(categorical_type)\n    frame = pd.concat([frame.drop(columns=categoricals), pd.get_dummies(frame[categoricals])], axis=1)\n    #frame = frame.drop(columns=categoricals)\n    \n    #\u00a0In case of train\n    if is_train:\n        \n        X = frame.drop(columns=target)\n        y = frame[target]\n        \n        #\u00a0Case of submission : Train on all data\n        if submission:\n            return X, y\n        \n        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.33)\n        return X_train, X_test, y_train, y_test\n    \n    else:\n        X = frame\n        return X","8537c3d8":"from sklearn.preprocessing import StandardScaler\n\n#\u00a0The standardizer\nscaler = StandardScaler()\n\n#\u00a0Prepare the training data\nX_train, X_test, y_train, y_test = preprocessing(train.copy(), scaler, is_train=True, target='SalePrice')","7c02fd16":"from sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\n\n# Hyperparameters\nparams = {\n    'objective': 'reg:squarederror', #\u00a0MSE as loss function\n    'eval_metric': 'rmse', # RMSE as metric\n    'eta': 0.3, # Learning rate\n}\n\n#\u00a0Init model\nmodel = XGBRegressor(**params)\n\n# Fit the model to the data\nmodel.fit(X_train, y_train)\n\n# Predictions\ny_train_preds = model.predict(X_train)\ny_test_preds = model.predict(X_test)\n\n# Display results\nprint(f\"Train RSME : {mean_squared_error(y_train, y_train_preds, squared=False)}\")\nprint(f\"Test  RSME : {mean_squared_error(y_test, y_test_preds, squared=False)}\")","1283953a":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nplt.figure(figsize=(20, 6))\nsns.distplot(y_train, label=\"Real\")\nsns.distplot(y_train_preds, label=\"Predictions\")\nplt.title(\"Train data |\u00a0Real vs Predictions\")\nplt.show()\n\nplt.figure(figsize=(20, 6))\nsns.distplot(y_test, label=\"Real\")\nsns.distplot(y_test_preds, label=\"Predictions\")\nplt.title(\"Test data |\u00a0Real vs Predictions\")\nplt.show()","a1ed4abc":"plt.figure(figsize=(20, 6))\nsns.distplot(np.exp(y_train), label=\"Real\")\nsns.distplot(np.exp(y_train_preds), label=\"Predictions\")\nplt.title(\"Train data |\u00a0Real vs Predictions\")\nplt.show()\n\nplt.figure(figsize=(20, 6))\nsns.distplot(np.exp(y_test), label=\"Real\")\nsns.distplot(np.exp(y_test_preds), label=\"Predictions\")\nplt.title(\"Test data |\u00a0Real vs Predictions\")\nplt.show()","cdc700ae":"#\u00a0Re-prepare the data for the training\nX_train, y_train = preprocessing(train.copy(), scaler, is_train=True, target='SalePrice', submission=True)\n\n#\u00a0Init model\nmodel = XGBRegressor(**params)\n\n# Fit the model to the data\nmodel.fit(X_train, y_train)\n\n# Prepare the data for submission\nX = preprocessing(test.copy(), scaler, is_train=False, target=None, submission=False)\n\n#\u00a0Make predictions : Don't forget the log scale to cancel\nX['SalePrice'] = np.exp(model.predict(X))\n\n#\u00a0Save to output\nX['SalePrice'].to_csv('submission.csv', index=True)","856e90e1":"___\n# Pre-processing\nIn this section we will :\n- Handle target variable\n- Handle ordinal variables\n- Impute missing data\n- One-hot encode categorical variables","fe216a64":"## Acknowledgments\nThe Ames Housing dataset was compiled by Dean De Cock for use in data science education. It's an incredible alternative for data scientists looking for a modernized and expanded version of the often cited Boston Housing dataset. ","ec4ab4a1":"## Ordinal variables","04eee74f":"Do not forget to upvote if it helped you :)","709156d1":"___\n#\u00a0Submission","287c6ed7":"## Load the data","070cd238":"# Getting started with XGBoost | House Prices\n\n<div style=\"align:center\">\n    <img src=\"https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/5407\/media\/housesbanner.png\">\n<\/div>\n\n## Introduction\n\n> Ask a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence. <br><br> With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home. <br><br> The goals of this competition is to **engineer features** and **use advanced regression techniques like random forests and gradient boosting**.\n\n## Table of contents\n- Setup\n- Pre-processing\n- Modelling\n- Submission\n\n___\n# Setup\nIn this section we will :\n- Import the main packages\n- Load the csv file\n- Take a look at some basic insights\n\n## Imports","f997ea5c":"___\n# Modelling","e8709a2c":"> Pretty much okay :)","134eef8c":"## Pre-processing","5476b911":"> Good result in log scale, let's see this in normal scale.","36a6ee68":"## Basic insights"}}