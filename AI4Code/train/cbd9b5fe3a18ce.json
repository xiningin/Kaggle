{"cell_type":{"93433866":"code","51bb053c":"code","b857cba1":"code","40065f79":"code","01b502e8":"code","0ecc0f00":"code","bbd2f21b":"code","0c9f3318":"code","a06c7f43":"code","fafac562":"code","c67dd49c":"code","88b5a67c":"code","bde7be8e":"code","1ea4453f":"code","c717c4da":"code","14ad6afa":"code","ffcaade0":"code","2d0e3c4e":"code","ed922506":"code","31a9f0f1":"markdown","19baf2a0":"markdown","65b3fb57":"markdown","a69c4641":"markdown"},"source":{"93433866":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom catboost import CatBoostRegressor\nimport itertools\nimport time","51bb053c":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv')","b857cba1":"train_df.describe()","40065f79":"test_df.describe()","01b502e8":"# Correlation matrix\ntrain_df.corr()","0ecc0f00":"# A little bit more beautiful correlation matrix\nsns.heatmap(train_df.corr(), cmap='coolwarm')","bbd2f21b":"train_df = train_df.drop('id', axis=1)\nX = train_df.drop('target', axis=1)\nY = train_df.target","0c9f3318":"X_test = test_df.drop('id', axis=1)\ntest_df['target'] = np.nan","a06c7f43":"float_cols_train = [c for c in X if X[c].dtype == \"float32\"]\nint_cols_train =   [c for c in X if X[c].dtype == \"int32\"]\n    \n# Upcast to avoid some problems with the number of digits after the point\nX[float_cols_train] = X[float_cols_train].astype(np.float64)\nX[int_cols_train] = X[int_cols_train].astype(np.int64)","fafac562":"float_cols_test = [c for c in X if X[c].dtype == \"float32\"]\nint_cols_test =   [c for c in X if X[c].dtype == \"int32\"]\n    \n# Upcast to avoid some problems with the number of digits after the point\nX_test[float_cols_test] = X_test[float_cols_train].astype(np.float64)\nX_test[int_cols_test] = X_test[int_cols_train].astype(np.int64)","c67dd49c":"# Feature generation for train. It's a little bit straightforward but why not.\ns = 1\nf = 14\nfor i in range(s, f+1):\n    X['cont' + str(i) + '_p2'] = X['cont' + str(i)] ** 2\n    for j in range(i+1, f+1):\n        X[str(i) + '_' + str(j)] = X['cont' + str(i)] * X['cont' + str(j)]\nX.describe()","88b5a67c":"# Feature generation for test. It's a little bit straightforward but why not.\nfor i in range(s, f+1):\n    X_test['cont' + str(i) + '_p2'] = X_test['cont' + str(i)] ** 2\n    for j in range(i+1, f+1):\n        X_test[str(i) + '_' + str(j)] = X_test['cont' + str(i)] * X_test['cont' + str(j)]\nX_test.describe()","bde7be8e":"# Data split\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.1)","1ea4453f":"# Sanity check\nY_train.head()","c717c4da":"# Catboost implementation. Do not forget to kill your previous snapshot.\nreg = CatBoostRegressor(\n    iterations=20000,\n    learning_rate=0.001,\n    max_depth=8,\n    od_type='Iter',\n    od_wait=1000,\n    eval_metric = 'RMSE',\n    save_snapshot=True,\n    snapshot_file='snapshot.bkp'\n)\nreg.fit(\n    X_train, Y_train,\n    # cat_features=cat_features,\n    eval_set=(X_val, Y_val),\n    logging_level='Silent',\n    plot=True\n)","14ad6afa":"test_df['target'] = reg.predict(X_test)","ffcaade0":"test_df.head()","2d0e3c4e":"res = test_df[['id', 'target']]","ed922506":"res.to_csv('submussion.csv', index=None)","31a9f0f1":"# Libraries import","19baf2a0":"# Training","65b3fb57":"In this notebook I'm using catboost with that weird data, which was created in the EDA. As we usually say in Russia: \"Not great, not terrible\". But it works. Hyperparameters for catboost are not final.","a69c4641":"# Exploratory data analysis"}}