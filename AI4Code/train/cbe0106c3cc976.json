{"cell_type":{"61e66ba2":"code","4f168af0":"code","008d5655":"code","994f39ec":"code","96c2bb3c":"code","da515be0":"code","0b2f9142":"code","d215505e":"code","321025dd":"code","6b2d94b9":"code","13f9b8ee":"code","594f2ec0":"code","4766ed46":"code","8d1b229a":"code","cf653230":"code","63d9ec57":"code","cd6f436b":"markdown","cfb2b449":"markdown","39e8cb0a":"markdown","f2951ea7":"markdown","0ef0da77":"markdown","94cd8e8a":"markdown","62fe437a":"markdown","04184501":"markdown","e5cd52e0":"markdown","e7ff7fda":"markdown","ec85fbaf":"markdown","f795c5cd":"markdown","5655c446":"markdown","abdc2ee6":"markdown","f3fe463c":"markdown","6164586d":"markdown","7f344778":"markdown","996a0af8":"markdown","3d3f6585":"markdown","70c794e1":"markdown","dfc23b81":"markdown","495ea0c2":"markdown","d20cdf38":"markdown","c80121e2":"markdown"},"source":{"61e66ba2":"# min_temp data\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import Image\nimport os\n%matplotlib inline\n\ntemp_data = pd.read_csv(\"..\/input\/min-temp\/min_temp.csv\")\ntemp_data.head().append(temp_data.tail())","4f168af0":"#ts data\nyears = pd.date_range('2012-01', periods=72, freq=\"M\")\nindex = pd.DatetimeIndex(years)\n\nnp.random.seed(3456)\nsales= np.random.randint(-4, high=4, size=72)\nbigger = np.array([0,0,0,0,0,0,0,0,0,1,1,1,1,1,1,1,1,1,1,1,3,3,3,3,\n                   3,3,3,3,3,3,3,3,7,7,7,7,7,7,7,7,7,7,7,\n                   11,11,11,11,11,11,11,11,11,11,18,18,18,\n                   18,18,18,18,18,18,26,26,26,26,26,36,36,36,36,36])\ndata = pd.Series(sales+bigger+6, index=index)\nts=data\nts","008d5655":"#Replace the index by Date column\ntemp_data.Date = pd.to_datetime(temp_data.Date, format='%d\/%m\/%y') #convert to pandas timestamp type\ntemp_data.set_index('Date', inplace=True) \n\ntemp_data.head(2)","994f39ec":"#Group by month and find the average of each month\n\ntemp_monthly = temp_data.resample('MS') #Since it reduces the data, this called down sampling\nmonth_mean = temp_monthly.mean()\nmonth_mean.head(5)","96c2bb3c":"#Adding addition row for each day and fill it with previous value\ntemp_bidaily= temp_data.resample('12H').asfreq()#Since it increase the data, this called up sampling\n\nprint(temp_bidaily.isnull().sum()) #There are some null here\n\n#Fill data behind it with the following one\ntemp_bidaily_fill= temp_data.resample('12H').ffill() #forward filling, backward filling (bfill())\ntemp_bidaily_fill.head()","da515be0":"#Selecting and slicing time series data\n#Retrieve data after 1985\ntemp_1985_onwards = temp_data['1985':]\ntemp_1985_onwards.head(2).append(temp_1985_onwards.tail(2))","0b2f9142":"temp_data.plot()\nplt.show()","d215505e":"#Dot plots can prove to be very helpful in identifying outliers and very small patterns \n#which may not be so obvious otherwise\n\ntemp_data.plot(style=\".b\")\nplt.show()","321025dd":"Image(\"..\/input\/pictures\/stationary.png\")","6b2d94b9":"rolmean = ts.rolling(window = 10, center = False).mean()\nrolstd = ts.rolling(window = 10, center = False).std()\n#Note that it lost a little bit in the beginning since the window use the previous info to check the future\n\nfig = plt.figure(figsize=(12,7))\norig = plt.plot(ts, color='blue',label='Original')\nmean = plt.plot(rolmean, color='red', label='Rolling Mean')\nstd = plt.plot(rolstd, color='black', label = 'Rolling Std')\nplt.legend(loc='best')\nplt.title('Rolling Mean & Standard Deviation')\nplt.show(block=False)","13f9b8ee":"# Use Pandas ewma() to calculate Weighted Moving Average of ts_log\nexp_rolmean = data.ewm(halflife = 3).mean() #Here, 3 is 3 month period. Halflife means the decativity rate\nexp_rolstd = data.ewm(halflife = 3).std()\n# Plot the original data with exp weighted average\nfig = plt.figure(figsize=(12,7))\nplt.plot(data, color='blue',label='Original')\nplt.plot(exp_rolmean, color='red', label='Exponentially Weighted Rolling Mean')\nplt.plot(exp_rolstd, color='black', label='Exponentially Weighted Rolling STD')\nplt.legend()\nplt.title('Exponentially Weighted Rolling Mean & Standard Deviation')\nplt.show()","594f2ec0":"from statsmodels.tsa.stattools import adfuller\n\ndftest = adfuller(ts)\n\ndfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n\nprint ('Results of Dickey-Fuller Test:')\nprint(dfoutput)","4766ed46":"fig, axs = plt.subplots(4,sharex=True,figsize=(11,7),gridspec_kw={'hspace': 0})\n\n#Original data\naxs[0].plot(ts, color='blue')\n\n#Log transform\naxs[1].plot(np.log(ts),color='red')\n\n#Subtracting the rolling mean\nrolmean = ts.rolling(window = 4).mean()\ndata_minus_rolmean1 = ts - rolmean #How we define \"Subtracting the rolling mean\"\naxs[2].plot(data_minus_rolmean1,color='green')\n\n#Subtracting the weighted rolling mean\nexp_rolmean = data.ewm(halflife = 2).mean()\ndata_minus_rolmean2 = ts - exp_rolmean\naxs[3].plot(data_minus_rolmean2,color='purple')","8d1b229a":"data_diff = data.diff(periods=1)\ndata_diff.head(10)\n\nfig = plt.figure(figsize=(11,3))\nplt.plot(data_diff, color='blue',label='Sales - rolling mean')\nplt.legend(loc='best')\nplt.title('Differenced sales series')\nplt.show()","cf653230":"fig, axs = plt.subplots(2,sharex=True,figsize=(11,7),gridspec_kw={'hspace': 0})\n\n#Original data temp data\naxs[0].plot(temp_data, color='blue', linewidth=1)\n\n#1-period lag\ndata_diff = temp_data.diff(periods=365)\naxs[1].plot(data_diff, color='red', linewidth=1)","63d9ec57":"from statsmodels.tsa.seasonal import seasonal_decompose\n\ndecomposition = seasonal_decompose(ts) #model=\"additive\" by default\n\n# Gather the trend, seasonality and noise of decomposed object\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\n# Plot gathered statistics\nfig, axs = plt.subplots(4,sharex=True,figsize=(11,7),gridspec_kw={'hspace': 0})\n\naxs[0].plot(ts, label='Original', color=\"blue\") #Original data\naxs[1].plot(trend, label='Trend', color=\"red\") #Trend\naxs[2].plot(seasonal,label='Seasonality', color=\"green\") #Season\naxs[3].plot(residual, label='Residuals', color=\"brown\") #Residual\n\nplt.show()","cd6f436b":"This is the end of the first part of this sequel. I hope that you will be able to use these ideas to implement your time series model!","cfb2b449":"[More trick](https:\/\/towardsdatascience.com\/basic-time-series-manipulation-with-pandas-4432afee64ea)","39e8cb0a":"### Code\nThe `statsmodels` library provides an implementation of the naive, or classical, decomposition method in a function called `seasonal_decompose()`. It requires that you specify whether the model is additive or multiplicative. By default, it is additive.\n\n\n**Recommendation**: This is just for showing what happens when we decompose a time series. The Naive method does not work on complex time series, so I recommend to take a look at something like [Loess or STL decomposition](https:\/\/otexts.com\/fpp2\/stl.html)","f2951ea7":"The red and black lines represent the rolling mean and rolling standard deviations. You can see that the mean is not constant over time, so we can reconfirm our conclusion that the time series is not stationary based on rolling mean and rolling standard error.","0ef0da77":"### Eliminating the trend\n\nRecap:\n\nThe reason to eliminate the trend is that the stationarity assumption is required in major time series modeling techniques but few practical time series are stationary.\n\n\n>**Trend**: Varying means over time. \n\n>**Seasonality**: Certain variations at specific time-frames.\n\nThe underlying principle is to model or estimate the trend and seasonality in the series and remove those from the series to get a stationary series. Statistical modeling techniques can then be implemented in these series. The final step would be to convert the modeled values into the original scale by applying trend and seasonality constraints back.\n\nThere are 3 important keys to eliminate trends:\n- Taking the log transformation (Alternatively, square root, cube root,...)\n- Subtracting the rolling mean\n- Differencing","94cd8e8a":"A time series has 4 components: level, trend, seasonality, and noise where\n\nLevel: The average value in the series.\nTrend: The increasing or decreasing value in the series.\nSeasonality: The repeating short-term cycle in the series. For example, a website might receive more visits during weekends; this would produce data with seasonality of 7 days.\nNoise: The random variation in the series.\n\nThe Naive decomposition will help us decompose these components. Note that because it is naive, it can only deal with simple time series. The temperature data set is too complicated for naive","62fe437a":"Before moving to the code lets break down two types of time series problems: \n\n> 1) Additive problems:\n$$y(t) = \\text{ Level } + \\text{ Trend } + \\text{ Seasonality } + \\text{ Noise }$$\nExample: For monthly data, an additive model assumes that the difference between the January and July values is approximately the same each year. In other words, the amplitude of the seasonal effect is the same each year.\n\nThe model similarly assumes that the residuals are roughly the same size throughout the series -- they are a random component that adds on to the other components in the same way at all parts of the series.\n\n> 2) Multiplicative problems: \n$$y(t) = \\text{ Level } \\cdot \\text{ Trend } \\cdot \\text{ Seasonality } \\cdot \\text{ Noise }$$\nIn many time series involving quantities (e.g. money, wheat production, ...), the absolute differences in the values are of less interest and importance than the percentage changes. For example, in seasonal data, it might be more useful to model that the July value is the same proportion higher than the January value in each year, rather than assuming that their difference is constant. Assuming that the seasonal and other effects act proportionally on the series is equivalent to a multiplicative model.\n\nMore about it [here](http:\/\/www-ist.massey.ac.nz\/dstirlin\/CAST\/CAST\/Hmultiplicative\/multiplicative1.html).","04184501":"This seems to work pretty well if you want to make the series stationary!\n\nDifferencing is a very popular tool to remove seasonal trends from time series as well. Look at the plot below. Here, we differenced our temperature data by taking differences of exactly one year, which removes the cyclical seasonality from the time series data! Pretty magical!","e5cd52e0":"# 0. Introduction\n\nThis note gives you a brief introduction of how to deal with time series. I try to write in a succinct and informative language. The material should not be too hard if you decided to click on this notebook (and you did). Have fun and feel free to comment. ","e7ff7fda":"### Changing the index and resampling the data","ec85fbaf":"# III. Naive Decomposition","f795c5cd":"### Differencing\n\nOne of the most common methods of dealing with both trend and seasonality is differencing. In this technique, we take the difference of observation at a particular time instant with that at the previous instant (i.e. a co-called 1-period \"lag\"). \n\nThis mostly works pretty well in improving stationarity. First-order differencing can be done in Pandas using the `.diff()` function with periods = 1 (denoting a 1-period lag). Details on `.diff()` can be found [here](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.diff.html).","5655c446":"We are going to use two data sets:\n\nThe **min_temp data** has the minimum temperature of every day for 10 years. (1981-1990)\n\nThe **ts data** set has 72 random numbers with an increasing trend and some noises.","abdc2ee6":"# II. Time series trend\n### Concept\nA given time series is thought to consist of three systematic components including level, trend, seasonality, and one non-systematic component called noise:\n\n>**Seasonal**: Patterns that repeat within a fixed period. For example, a website might receive more visits during weekends; this would produce data with the seasonality of 7 days.\n\n>**Trend**: The underlying trend of the metrics. A website increasing in popularity should show a general trend that goes up.\n\n>**Level**: The average value in the series.\n\n>**Noise\/Residual**: The random variation in the series. This is what left after removing seasonal and trend\n\nThere are two types of time series:\n\n* **Stationary**: the oscillation does not go up or down. Same mean over time (**homoscedasticity**)\n\n* **Non-stationary**: Having some tendency (trend) of going up or down. Different means over time. The tendency it non-stationary time series may have are: linear, exponential, periodic, oscillation get bigger over time","f3fe463c":"The null-hypothesis here is there is no trend. Since p-value = 1,  we reject the null-hypothesis.","6164586d":"### Trend or not trend \nIt is important to know the trend because the model often works better with non-trend time series. (Just like how model work better with normal distribution for different types of data)\n\n**1) Rolling statistics**\n\nFor any specific time $t$ we can use a window of length $m$ to capture the values of the time series right before $t$. After getting these values from the window, we can compute the average or variance, depends on the purpose, and estimate the value at $t$. \n\nThe graph of this method is much smoother than the original graph. This will give us a better indication of the trend of the graph. \n\nPandas has a built-in function called [rolling()](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.rolling.html), which can be called along with `.mean()` and `.std()` to calculate these rolling statistics. Let's take a window size of 8 for this example. \n\n(Test different window sizes and see how it affect the rolling graph) ","7f344778":"**1) Log transformation vs Subtracting the rolling mean**\n\nBy apply the log transformation, we reduce the volatile of the graph. \n\nBy subtracting the rolling mean, we straighten the trend line and move it toward the horizontal line. \n\nIn this example, we will compare the original graph with log transformation, subtracting the rolling mean, and subtracting the weighted rolling mean.","996a0af8":"**3) The Dickey-Fuller Test** \n\nThe Dickey-Fuller Test is a statistical test for testing stationarity. The Null-hypothesis for the test is that the time series is not stationary. So if the test statistic is less than the critical value, we reject the null\nhypothesis and say that the series is stationary. The Dickey-Fuller test is available in stat tools from the StatsModels module. More details on this can be viewed [here](http:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.tsa.stattools.adfuller.html).","3d3f6585":"Observe that log transformation bend the trend line just a little bit. The subtracting weight method seems to work very effectively. Also, notice that the purple graph did not lose the tip like the green graph thanks to the math behind weight rolling mean!","70c794e1":"### Plot","dfc23b81":"Observe that the data is increasing with respect to time but the variance is at a stable level. Thus, this is an additive model. The trend line is linearly increasing. The third graph so that there is some seasonal pattern in the `ts` data set, and there is some noise in the data. You can scroll to the top to see how the `ts` data set was created.","495ea0c2":"# I. Timestamp manipulation","d20cdf38":"# IV. References\n\nA big thanks to [learn.co author](https:\/\/github.com\/learn-co-students) and [Jason Brownlee](http:\/\/www-ist.massey.ac.nz\/dstirlin\/CAST\/CAST\/Hmultiplicative\/multiplicative1.html) who provided me the idea and sources to write this blog.","c80121e2":"#### 2) The weighted rolling mean\n\nA drawback of the rolling mean approach is that the window has to be strictly defined. In this case, we can take yearly averages but in complex situations like forecasting a stock price, it may be difficult to come up with an exact number. So we take a \"weighted rolling mean\" (or weighted moving average, WMA for short) where **more recent values are given a higher weight**. There are several techniques for assigning weights. A popular one is **Exponentially Weighted Moving Average** where weights are assigned to all the previous values with an exponential decay factor. This can be implemented in Pandas with `DataFrame.ewm()` method. Details can be found [here](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.ewm.html)."}}