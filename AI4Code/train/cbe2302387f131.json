{"cell_type":{"3f3441fb":"code","dbe1cc1c":"code","86f643f1":"code","d671a30a":"code","08ef6dff":"code","77975201":"code","75390e58":"code","2f180210":"code","186c51c6":"code","a95435fb":"code","ccdde447":"code","1bc1edd9":"code","f5a52ab6":"code","5049874e":"code","dfea05e5":"code","6128f7d2":"code","a367a1bf":"code","312e1818":"code","dae781ac":"markdown","48c63152":"markdown","38ced60b":"markdown","f5b29be9":"markdown","d81ff160":"markdown","17c92d98":"markdown","9164f62e":"markdown","c535ae59":"markdown","8be6e4f5":"markdown","d969523b":"markdown","37cd93b4":"markdown","b6cc3b9a":"markdown","a02d70b6":"markdown"},"source":{"3f3441fb":"# imports\nimport numpy as np\nimport torch\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nimport torch.nn.functional as F\n\nimport matplotlib.pyplot as plt","dbe1cc1c":"!ls ..\/input","86f643f1":"train = torch.load('..\/input\/wustl-math450-spring-2021\/train.pt')\ntest = torch.load('..\/input\/wustl-math450-spring-2021\/test.pt')\n\ntrain_data, train_targets = train['data'], train['label']\ntest_data = test['data']","d671a30a":"# verifying the shapes\nprint(f\"The shapes of train data, train targets, test data are\\n {train_data.size()}, {train_targets.size()}, {test_data.size()}.\")","08ef6dff":"fig, axes = plt.subplots(4,8, figsize=(20, 12))\naxes = axes.reshape(-1)\nnp.random.seed(1)\nidx = np.random.choice(len(train_data), size=32)\n\nfor i, ix in enumerate(idx):\n    axes[i].axis('off') # hide the axes ticks\n    axes[i].imshow(train_data[ix], cmap = 'gray')\n    axes[i].set_title(str(int(train_targets[ix])), color= 'black', fontsize=25)\nplt.show()","77975201":"from torchvision.models.resnet import ResNet, BasicBlock, Bottleneck\n\nclass MyResNet(ResNet):\n    def __init__(self,\n                 block=BasicBlock, \n                 layers=[2, 2, 2, 2],\n                 num_classes=10):\n        super(MyResNet, self).__init__(block, \n                                       layers, \n                                       num_classes=num_classes) \n        self.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=1, padding=3,bias=False)\n        \n        '''\n        you can modify the layers in this initialization of ResNet if you like\n        '''\n        \n# choose 1        \nresnet18 = {\n    \"block\": BasicBlock,\n    \"layers\": [2, 2, 2, 2]\n}\n\nresnet34 = {\n    \"block\": BasicBlock,\n    \"layers\": [3, 4, 6, 3]\n}\n\nresnet50 = {\n    \"block\": Bottleneck,\n    \"layers\": [3, 4, 6, 3]\n}\n\n\n\nmodel = MyResNet(**resnet18)","75390e58":"!pip install torchsummary -q\nfrom torchsummary import summary","2f180210":"summary(model, (1,28,28), batch_size=1024, device='cpu') # (color_channel, 28, 28)","186c51c6":"batch_size = 1024\n\ntrain = TensorDataset(train_data[:,None,:,:].float(),train_targets.long())\ntrain_loader = DataLoader(train, batch_size = batch_size, shuffle = True)","a95435fb":"sample = next(iter(train_loader))\nprint(sample[0].size(), sample[1].size())","ccdde447":"with torch.no_grad():\n    sample_output = model(sample[0]) # verifying the model is okay\n\nprint(sample_output.size()) # should be (n_batch, n_classes)","1bc1edd9":"from torch.optim import Optimizer","f5a52ab6":"class SGD(Optimizer):\n    \"\"\"\n    Implements the vanilla SGD simplified from the torch official one\n    for Math 450 WashU\n    \n    Args:\n        params (iterable): iterable of parameters to optimize or dicts defining\n            parameter groups\n        lr (float): learning rate\n        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)\n        \n    Example:\n        >>> optimizer = SGD(model.parameters(), lr=1e-2)\n        >>> optimizer.zero_grad()\n        >>> loss_fn(model(input), target).backward()\n        >>> optimizer.step()\n    \"\"\"\n\n    def __init__(self, params, \n                      lr=1e-3, \n                      weight_decay=0,):\n        defaults = dict(lr=lr, \n                        weight_decay=weight_decay)\n        super(SGD, self).__init__(params, defaults)\n\n    def step(self, closure=None):\n        \n        for group in self.param_groups:\n            weight_decay = group['weight_decay']\n\n            for param in group['params']:\n                if param.grad is None:\n                    continue\n                grad_param = param.grad.data\n                \n                if weight_decay != 0:\n                    grad_param += weight_decay*param.data\n\n                param.data -= group['lr']*grad_param\n\n        return loss","5049874e":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device) # should be cuda","dfea05e5":"from tqdm.auto import tqdm\n\nlearning_rate = 1e-3\noptimizer = SGD(model.parameters(), lr=learning_rate)\nloss_func = nn.CrossEntropyLoss()\n\nmodel.to(device)","6128f7d2":"epochs = 50\n\nfor epoch in range(epochs):\n    \n    model.train()\n    \n    loss_vals = []\n    \n    with tqdm(total=len(train_loader)) as pbar:\n        for x, targets in train_loader:\n            \n            x, targets = x.to(device), targets.to(device)\n            \n            # forward pass\n            outputs = model(x)\n            \n            # loss function\n            loss = loss_func(outputs, targets)\n            \n            # record loss function values\n            loss_vals.append(loss.item())\n            \n            # clean the gradient from last iteration\n            optimizer.zero_grad()\n            \n            # backprop\n            loss.backward()\n            \n            # gradient descent\n            optimizer.step()\n            \n            desc = f\"epoch: [{epoch+1}\/{epochs}] loss: {np.mean(loss_vals)}\"\n            pbar.set_description(desc)\n            pbar.update()\n                ","a367a1bf":"model.eval() # this cannot be omitted due to dropout\n\ntest = TensorDataset(test_data[:,None,:,:].float())\ntest_loader = DataLoader(test, batch_size = batch_size, shuffle = False, drop_last=False)\n\ny_preds = []\nfor x in tqdm(test_loader):\n    with torch.no_grad():\n        x = x[0].to(device)\n        y_pred = model(x)\n        y_preds.append(y_pred.cpu().detach().numpy())\n        \ny_preds = np.concatenate(y_preds)\npreds = y_preds.argmax(axis=-1).astype(int)\nprint(preds.shape) # should be 30,000","312e1818":"len_test = test_data.size(0)\nsolutions = np.zeros((test_data.size(0), 2))\nsolutions[:,0] = np.arange(1,len_test+1)\nsolutions[:,1] = preds\nsolutions = solutions.astype(int)\nnp.savetxt(\"solutions-yournames.csv\", solutions, \n           fmt='%s', header = 'Id,Category', delimiter = ',', comments='')","dae781ac":"# WashU Math 450 Sp21 Final project starter code\n\nThis is the starter code for the final project. Please walk it through.\nYou can download this starter as a notebook and upload it to Colab, or directly run it on Kaggle.\nDo not attempt to run the code locally unless there is a GPU supporting CUDA.\nAll the given models are fairly large (> 10 million parameters) and we only about 40 GPU hours on Kaggle per week.\nSo please plan accordingly and do not start the final project in the last two weeks.\nThe due date for submitting the final project report is May 15, 2021.\n\n## Q&A:\nFor questions you can post in the Piazza discussion board: [https:\/\/piazza.com\/class\/kkcyi2zgosr3wh](https:\/\/piazza.com\/class\/kkcyi2zgosr3wh)","48c63152":"# Team member(s):\n\nUpon submitting your code on Gradescope, please type the names of the team members here:","38ced60b":"# Training\n\nMake sure the `cuda` is on by checking the setting on the right.","f5b29be9":"# Predict the solution on the test set\n\nEvaluate the trained model and generate the `preds` variable, which is the index of the maximum entry of the 10 outputs for each sample.","d81ff160":"The train set has data and targets given, yet test set has no targets. We have to train models based on the train data to achieve a high score on the leaderboard on the test set.","17c92d98":"![title](https:\/\/sites.wustl.edu\/scao\/files\/2021\/03\/math450-kaggle.png)","9164f62e":"# Final project: write our own optimizer\n\nIn the following cells, we use the `torch.optim` interface to build our own optimizer.\nThe following code is simplified from [http:\/\/pytorch.org\/docs\/master\/_modules\/torch\/optim\/sgd.html#SGD](http:\/\/pytorch.org\/docs\/master\/_modules\/torch\/optim\/sgd.html#SGD)\n\nPlease refer to the final project page to see requirements of the implementation of a customized optimizer.","c535ae59":"## Load the data from Kaggle\nThe data file in `.pt` format (a pickle format for PyTorch) from [our class's Kaggle competition website](https:\/\/www.kaggle.com\/c\/wustl-math450-spring-2021), put them in the same folder with this notebook. The following cells will load the file as torch tensors.\n\nIf you have already registered on Kaggle, then directly running this Kaggle kernel from cloud is better. Please click the **COPY and EDIT** button on the upper right corner.\n![](https:\/\/sites.wustl.edu\/scao\/files\/2020\/10\/Screen-Shot-2020-10-25-at-1.09.19-PM.png)\n\nThe dataset is already added to the path accessible to your notebook, and it is in locally `..\/input\/wustl-math450-spring-2021` directory. Using the following bash command hack will reveal. On the right there is GPU toggle by clicking the Accelerator button. Kaggle has a Tesla P100 GPU which is better than Colab, but we have only 40 hours of GPU computing time reseting per week.\n![](https:\/\/sites.wustl.edu\/scao\/files\/2021\/03\/Screenshot-from-2021-03-23-21-10-01.png)","8be6e4f5":"# Final Project: write an optimizer\n\nWe will have 3 given CNN models as follows, and we will write our own optimizer(s) using `torch.optim` interface to optimize the given 3 models. You are free to use any techniques listed in the final project page to optimize the model. The goal is to optimize the model such that the model(s) trained performs good in the test set with unknown targets.\n\n## Models:\nThe 3 fixed models we can use are the famous first state-of-art image recognition CNN ResNet: ResNet 18, ResNet 34, and ResNet 509\\ [https:\/\/pytorch.org\/hub\/pytorch_vision_resnet\/](https:\/\/pytorch.org\/hub\/pytorch_vision_resnet\/). Their settings are in the next cell. \nThe `__init__()` has two parameters: \n- `block`: the first type is the `BasicBlock` used for building ResNet, defined in the file (`torchvision.models.resnet`). The other type is `Bottleneck` block, which has a more agressive dimension reduction mechanics.\n- `[2, 2, 2, 2]`: Each number denotes the number of `Bottleneck` or `BasicBlock` modules in a \"stage\". It depends on how deep you want the networks to be. For example, for ResNet18, it is `[2, 2, 2, 2]`; for ResNet34, it is `[3, 4, 6, 3]`.\n\nThe full code of ResNet is here: [https:\/\/github.com\/pytorch\/vision\/blob\/master\/torchvision\/models\/resnet.py](https:\/\/github.com\/pytorch\/vision\/blob\/master\/torchvision\/models\/resnet.py). We are just gonna import the `torchvision` implementation, and change the first convolutional layer to accept single channel input, and the stride of the first convolutional layer from 2 to 1.\n\n\n## Cross-validation:\nIn the starter code, no cross-validation technique is used. However for the final project, you are expected to use the cross-validation technique featured in class to choose your best-performanced models.\n\nSince you do not have access to the target of the test set, to cross-validate your model, you should split the training samples to two sets using the techniques learned in class: one is training dataset, the other is your validation dataset (so that you have access to the targets of this set). This routine is usually used for Kaggle competitions and in the real world model deployment to ensure the trained model has generalizability to the unseen data.\n\n## Hyperparameter tuning:\nAside from writing our own optimizer, there are quite a few hyperparameter for us to tune to achieve better performance:\n- batch size\n- learning rate and\/or with a scheduler\n- dropout rate\n- early stop\n- other validation strategy based tuning\nIt is okay to use autoML package (H2O, Rapids, Optuna, fast.ai) if you know how to write a wrapper of them using the given model.","d969523b":"A batch size of 1024 of float precision training needs about 4GB of RAM for the GPU.","37cd93b4":"# Export the solutions and submit to Kaggle\n\nThe result predicted by your model can be named to `preds`, and be exported to a `.csv` file using the following cell. The `y_pred` should be of a dimension `(30000,)` numpy array. \n\nRename the resulting `solutions-yournames.csv` by replacing `yournames` by your team members' initials connected by hyphen and then save the notebook on Kaggle.\n\nThen click the submit button below, the solution can be submitted directly from the output tab of a committed notebook (click `Save Version` blue button in the interactive kernel window).","b6cc3b9a":"## Visualizing samples\nNotice `train_data` and `test_data` are torch tensors with dimensions `(40000, 28, 28)` and `(30000, 28, 28)`, so that `train_data[i]` and `test_data[i]` represent images for training and testing, respectively. We can plot by randomly choosing 32 samples from the `train_data` (a 28x28 grayscale image), and we make the title as their label (which category they belong) as follows: for example, in the plotted images, the first image in the first row is of category 9.\n\nReference:\n\n| ID | Category |\n|---|-------|\n| 0 | \u304a |\n| 1 |  \u304d |\n| 2 | \u3059      |\n| 3 |   \u3064    |\n| 4 |    \u306a    |\n| 5 |  \u306f     |\n| 6 |   \u307e     |\n| 7 |   \u3084     |\n| 8 |  \u308c     |\n| 9 |  \u3092    |","a02d70b6":"# Prepare the train loader"}}