{"cell_type":{"4ebdd710":"code","ecb773ee":"code","5c6fb9fd":"code","e50105f0":"code","c3b8ba61":"code","42f8024c":"code","f20a6fd2":"code","28c48777":"code","5ac47b10":"code","846f1e6a":"code","4794c522":"code","7d8ac72b":"code","8a9f242f":"code","b9cfdf03":"code","f9e1f509":"code","8137dd08":"code","512e7efb":"code","e3c40294":"code","b56c2ac2":"code","3e5a32e9":"code","67b267ea":"code","fa371b5a":"code","05d69e3f":"code","0cdafe51":"code","cb09c861":"code","5f805415":"code","2a24cd70":"code","7858534e":"code","3ac5d80e":"code","b4221089":"code","a680334a":"code","ab4f943c":"code","49e6bce0":"code","e63a0590":"code","880dfc44":"code","168dc51e":"code","38f946f0":"code","18d4a5e3":"code","7434c452":"code","83889116":"code","d777d980":"code","737c6147":"code","3fa12fa8":"code","b9b8a309":"code","b1dabf34":"code","a3e6807d":"code","11d315e3":"code","bc013738":"code","9a2e920d":"code","7f1fa8d2":"code","be932bfa":"code","8fd2e2e0":"code","035d2808":"code","9850e0b0":"code","0d57722e":"code","aa95c1b8":"code","f4b57935":"code","a2368601":"code","00c9d49c":"code","69738d64":"code","d41a5c99":"markdown","ee6c3838":"markdown","2175b41a":"markdown","3e227220":"markdown","7a71400c":"markdown","b0822afd":"markdown","7e314b56":"markdown","7ba36500":"markdown","f23a9ea3":"markdown","b8dee8bc":"markdown","0ef3b448":"markdown","dc89b41c":"markdown","a0f318f0":"markdown","2b617ae4":"markdown","e26e34ab":"markdown","5ab30b1d":"markdown","a4375f1a":"markdown","9a31e651":"markdown","345cb6cd":"markdown","e3854275":"markdown","8323b560":"markdown","b5b142ef":"markdown","f16be214":"markdown","68b2fd21":"markdown","78ba66cd":"markdown","5a686d18":"markdown","d3ecc12e":"markdown","c16a80d2":"markdown","2f6d81c6":"markdown","e671266b":"markdown","c44a2c8e":"markdown","5791cac9":"markdown","8f51890d":"markdown","38b88296":"markdown","f4efce31":"markdown","42ba380b":"markdown","e113e550":"markdown","6582cd86":"markdown","851b56bc":"markdown","7639c010":"markdown","e90b2f4e":"markdown","fde7abf5":"markdown","8c8d2edb":"markdown","48fbe524":"markdown","43d4a9df":"markdown","cf5e6d33":"markdown","995444ca":"markdown"},"source":{"4ebdd710":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport scipy.stats\n\nimport warnings\nimport os\nwarnings.simplefilter(\"ignore\")\nos.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n","ecb773ee":"# Data set documentation\nhouse_data_description = open('..\/input\/house-prices-advanced-regression-techniques\/data_description.txt')\ntext = house_data_description.read()\nprint(text)","5c6fb9fd":"#a brief look at the dataset\nhouses_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nhouses_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\noriginal_numerical_features = houses_test.select_dtypes(include=np.number).columns.to_list()\noriginal_categorical_features = houses_test.select_dtypes(exclude=np.number).columns.to_list()\n\n\nprint(f\"shape of test: {houses_train.shape}\")\nprint(f\"shape of test: {houses_test.shape}\")\nhouses_train.head()","e50105f0":"#splitting features to categorical and numerical\ncategorical = houses_train.dtypes[(houses_train.dtypes==object)].index\nnumerical = houses_train.dtypes[(houses_train.dtypes!=object)].index[1:-1] #omitting id and SalePrice (first and last dtypes respectively)\n\nprint(f\"Numerical features: {numerical.size}\")\nprint(f\"Categorical features: {categorical.size}\")","c3b8ba61":"print(houses_train['SalePrice'].describe())\nprint(f\"Skewness: {houses_train['SalePrice'].skew()}\")\nprint(f\"Kurtosis: {houses_train['SalePrice'].kurt()}\")\nhouses_train['SalePrice'].hist(bins=50, density=True, alpha=0.5);","42f8024c":"mu = np.mean(houses_train['SalePrice'])\nsigma = np.std(houses_train['SalePrice'])\nnorm = np.random.normal(mu, sigma, houses_train['SalePrice'].shape)\n\nplt.figure(figsize=(6, 3))\nsns.distplot(houses_train['SalePrice'],kde=False,fit=scipy.stats.norm)\nplt.title(\"normal\")\nprint(\"Skewness_norm:\", scipy.stats.skew(norm))\nprint(\"Kurtosis_norm:\", scipy.stats.kurtosis(norm))\n\n\nlog_sp = np.log(houses_train['SalePrice'])\nplt.figure(figsize=(6, 3),)\nsns.distplot(houses_train['SalePrice'], kde=False, fit=scipy.stats.lognorm)\nplt.title(\"Log norm\")\nprint(\"Skewness_Log norm:\", scipy.stats.skew(log_sp))\nprint(\"Kurtosis_Log norm:\", scipy.stats.kurtosis(log_sp))","f20a6fd2":"houses_train[\"SalePrice_log\"] = log_sp","28c48777":"plt.figure(figsize=(8, 4))\nplt.subplot(1,2,1)\nscipy.stats.probplot(houses_train['SalePrice'], dist=scipy.stats.norm, sparams=(houses_train['SalePrice'].mean()), plot=plt)\nplt.title('Q-Q plot, Sales price vs normal')\nplt.subplot(1,2,2)\nscipy.stats.probplot(log_sp, dist=scipy.stats.norm, sparams=(log_sp.mean()), plot=plt,)\nplt.title('Q-Q plot, log norm (Sales Price) vs normal');\nplt.tight_layout()","5ac47b10":"\nfor col in list(numerical):\n    mean_col = houses_train[col].mean()\n    std_col = houses_train[col].std()\n    lower_limit = mean_col - 3*std_col\n    upper_limit = mean_col + 3*std_col\n    houses_train[col] = pd.Series([min(max(a,lower_limit),upper_limit) for a in houses_train[col]])\n    houses_test[col] = pd.Series([min(max(a,lower_limit),upper_limit) for a in houses_test[col]])\n","846f1e6a":"#for train\nmissing_train = houses_train.isnull().sum()*100\/len(houses_train)\nmissing_train = missing_train[missing_train > 0]\nmissing_train.sort_values(inplace=True)\nmissing_train.plot.bar()\nplt.xlabel(\"Feature\")\nplt.ylabel(\"percent missing\")\nplt.show();\n#for test\nmissing_test = houses_test.isnull().sum()*100\/len(houses_train)\nmissing_test = missing_test[missing_test > 0]\nmissing_test.sort_values(inplace=True)","4794c522":"missing = set(missing_train.index).union(set(missing_test.index))","7d8ac72b":"# columns where NaN values have meaning e.g. no pool etc.\ncols_fill_with_none = [col for col in missing if col in categorical]\ncols_fill_with_average = [col for col in missing if col in numerical]\n# replace 'NaN' with 'None' in these columns\nfor col in cols_fill_with_none:\n    houses_train[col].fillna(\"None\",inplace=True)\n    houses_test[col].fillna(\"None\",inplace=True)\n    \nfor col in cols_fill_with_average:\n    houses_train[col].fillna(houses_train[col].mean(),inplace=True)\n    houses_test[col].fillna(houses_test[col].mean(),inplace=True)","8a9f242f":"cols = list(numerical) + [\"SalePrice_log\"]\n\ncorrs = houses_train[cols].corr()[\"SalePrice_log\"]\nfig, ax = plt.subplots(figsize=(12, 8))\ng = sns.barplot(\n    x=corrs[:-1].index,\n    y=corrs[:-1].values, palette=\"crest\")\nplt.xticks(rotation=90)\nplt.show();","b9cfdf03":"correlations = houses_train[numerical].corr()\nmask = np.zeros_like(correlations, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nfig, ax = plt.subplots(figsize=(14, 10))\nsns.heatmap(correlations, mask=mask, center=0,\n            square=True, linewidths=.5,cmap=\"YlGnBu\")        \nplt.show()","f9e1f509":"normal_data = pd.concat([pd.Series(houses_train[numerical].kurt(),name=\"Kurtosis\"),pd.Series(houses_train[numerical].skew(),name=\"Skewness\")],axis=1)\nnormal_data","8137dd08":"log_house_train = np.log(houses_train[numerical]+0.001)\nlog_normal_data = pd.concat([pd.Series(log_house_train[numerical].kurt(),name=\"Kurtosis\"),pd.Series(log_house_train[numerical].skew(),name=\"Skewness\")],axis=1)\nlog_normal_data","512e7efb":"diff = abs(normal_data)-abs(log_normal_data)\ndiff[(diff.Kurtosis>1) & (diff.Skewness>0)]","e3c40294":"features_to_log = diff[(diff.Kurtosis>1) & (diff.Skewness>0)].index\nfor feature in features_to_log:\n    houses_train[feature+\"_log\"]=np.log(houses_train[feature]+0.0001) # adding a small constant to avoid numerical instability\n    houses_test[feature+\"_log\"]=np.log(houses_test[feature]+0.0001) # adding a small constant to avoid numerical instability","b56c2ac2":"n_rows = 4\nn_cols = 9\n\nfig, axs = plt.subplots(n_rows, n_cols, figsize=(n_cols*3.5,n_rows*3))\nli_num_feats = list(numerical)\n\nfor r in range(0, n_rows):\n    for c in range(0, n_cols):  \n        i = r * n_cols + c\n        if i < len(li_num_feats):\n            \n            feature_to_plot = li_num_feats[i]\n            if feature_to_plot+\"_log\" in houses_train.columns:\n                \n                feature_to_plot = feature_to_plot+\"_log\"\n            sns.regplot(x=houses_train[feature_to_plot], y=houses_train[\"SalePrice_log\"], ax = axs[r][c],  scatter_kws={\"alpha\": 0.1}, line_kws={\"color\": \"y\", \"alpha\": 0.5}\n)\n            stp = scipy.stats.pearsonr(houses_train[feature_to_plot], houses_train[\"SalePrice_log\"])\n            \n            str_title = \"r = \" + \"{0:.2f}\".format(stp[0]) + \"  \" \"p = \" + \"{0:.2f}\".format(stp[1])\n            axs[r][c].set_title(str_title,fontsize=11)\n            \nplt.tight_layout()    \nplt.show() \n","3e5a32e9":"li_cat_feats = list(categorical)\nnr_rows = 10\nnr_cols = 4\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*5,nr_rows*4))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_cat_feats):\n            sns.boxplot(data=houses_train, x=li_cat_feats[i], y=\"SalePrice_log\", ax = axs[r][c], palette=\"crest\")\n            plt.xticks(rotation=90)\n    \nplt.tight_layout()    \nplt.show()   ","67b267ea":"import sklearn\nfrom sklearn import preprocessing\nfrom scipy import stats\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\n\n\n# from sklearn.exceptions import ConvergenceWarning\n# with warnings.catch_warnings():\n#     warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)","fa371b5a":"houses_train_modified = houses_train.copy()\nhouses_test_modified = houses_test.copy()","05d69e3f":"for col in houses_train_modified[categorical]:\n    grouped_target = houses_train_modified.groupby(col)[\"SalePrice\"].mean().sort_values(ascending=True)\n    factors = pd.factorize(grouped_target.index)[0]\n    grouped_target = grouped_target.reset_index()\n    grouped_target.drop(\"SalePrice\",axis=1,inplace=True)\n    grouped_target[col+\"_fact\"] = factors \n    houses_train_modified = pd.merge(left = houses_train_modified, right=grouped_target, on=col, how=\"left\")\n    houses_test_modified = pd.merge(left = houses_test_modified, right=grouped_target, on=col, how=\"left\")\ncategorical_fact = categorical+'_fact'","0cdafe51":"numerical=houses_train_modified.select_dtypes(include=np.number).columns.to_list()\n[numerical.remove(x) for x in ['Id','SalePrice_log', 'SalePrice']]","cb09c861":"houses_test_modified[numerical]=houses_test_modified[numerical].fillna(-1) #handling categories abcsent from train\nhouses_train_modified = houses_train_modified[numerical] #without original categorical columns but with columns we created\nhouses_test_modified = houses_test_modified[numerical] #without original categorical columns but with columns we created","5f805415":"area_list = [\"LotArea\",\"TotalBsmtSF\",\"1stFlrSF\",\"2ndFlrSF\",\"GrLivArea\"]\narea_features_df = houses_train_modified[area_list]","2a24cd70":"k_to_exmine = np.arange(1,int(np.sqrt(len(area_features_df)\/2)))\n\ndistortions = []\nfor k in k_to_exmine:\n    kmeanModel = KMeans(n_clusters=k)\n    kmeanModel.fit(area_features_df)\n    distortions.append(kmeanModel.inertia_)\n    \nplt.figure(figsize=(8,8))\nplt.plot(k_to_exmine, distortions, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.show()","7858534e":"km = KMeans(n_clusters=5)\nhouses_train_modified['area_label'] = km.fit_predict(area_features_df)\nhouses_test_modified['area_label'] =km.predict(houses_test_modified[area_list])","3ac5d80e":"train_full = pd.merge(left=houses_train_modified,right=houses_train[[\"SalePrice\",\"SalePrice_log\"]],\n                      left_index=True,right_index=True) #adding columns \"SalePrice\",\"SalePrice_log\"\ngrouped_target = train_full.groupby('area_label')[\"SalePrice\"].mean().sort_values(ascending=True)\n\nfactors = pd.factorize(grouped_target.index)[0]\ngrouped_target = grouped_target.reset_index()\ngrouped_target.drop(\"SalePrice\",axis=1,inplace=True)\ngrouped_target['area_label'+\"_fact\"] = factors \nhouses_train_modified = pd.merge(left = houses_train_modified, right=grouped_target, on=\"area_label\", how=\"left\")\nhouses_test_modified = pd.merge(left = houses_test_modified, right=grouped_target, on=\"area_label\", how=\"left\")\n\nhouses_train_modified.drop(\"area_label\",inplace=True,axis=1)\nhouses_test_modified.drop(\"area_label\",inplace=True,axis=1)\n\n#updating the numerical features with are new feature\nnumerical = numerical +[\"area_label_fact\"]\n","b4221089":"cols_for_pca = numerical","a680334a":"# Create a scaler object\nsc = StandardScaler()\n# Fit the scaler to the features and transform\nX_std = sc.fit_transform(houses_train_modified[cols_for_pca])","ab4f943c":"pca = PCA()\nnew_features = pca.fit_transform(X_std) #return all the data points after projecting them on new vector span (which builded by the components) the compo) \npca_col_names = [\"PC\"+str(x) for x in list(range(1,new_features.shape[1]+1))]\npca_df = pd.DataFrame(data = new_features, columns = pca_col_names)\nhouses_train_modified = pd.merge(left=houses_train_modified,right=pca_df, left_index=True, right_index=True)","49e6bce0":"X_std_test = sc.transform(houses_test_modified[cols_for_pca])\nnew_features_test = pca.transform(X_std_test)\npca_df_test = pd.DataFrame(data = new_features_test, columns = [\"PC\"+str(x) for x in list(range(1,new_features.shape[1]+1))])\nhouses_test_modified = pd.merge(left=houses_test_modified,right=pca_df_test, left_index=True, right_index=True)\n","e63a0590":"pca_df","880dfc44":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_log_error","168dc51e":"basic_f = list(categorical_fact)+original_numerical_features\nbasic_f.remove(\"Id\")\nbasic_f_with_log = basic_f+[f+\"_log\" for f in features_to_log]\nkmean_f_with_log = basic_f_with_log + [\"area_label_fact\"]\nkmeans_pca = kmean_f_with_log + [\"PC\"+str(x) for x in list(range(1,new_features.shape[1]+1))]\nkmeans_pca_only = pca_col_names+[\"area_label_fact\"]\n\nfeature_to_examin = {\"basic\":basic_f,\n                     \"basic_withlog\":basic_f_with_log,\n                     \"kmeans\":kmean_f_with_log,\n                     \"kmeans_and_pca\":kmeans_pca,\n                     \"kmeans_pca_only\":kmeans_pca_only}\n\n\ndef ceck_basemdl_scores(X_train, X_test, y_train, y_test, feature_to_examin):\n    \"\"\"\n    For each set of features in the feature_to_examin dictionary, fit a RFR\n    and output a dataframe containing scores and mean log square error for train and test\n    \"\"\"\n    data = []\n    \n    for k,col_to_use in feature_to_examin.items():\n        print(f\"Calculating for {k}\")    \n        X_train_ = X_train[col_to_use] \n        X_test_ = X_test[col_to_use] \n    \n        rf_clf = RandomForestRegressor(oob_score=True)\n        rf_clf.fit(X_train_,y_train)\n        prediction = rf_clf.predict(X_test_)\n        \n        train_score = rf_clf.score(X_train_,y_train)\n        test_score = rf_clf.score(X_test_,y_test)\n        \n        try:\n            oob_score = rf_clf.oob_score_\n        except AttributeError:\n            oob_score = \"-\"\n            \n        pred_train = rf_clf.predict(X_train_)\n        pred_test = rf_clf.predict(X_test_)\n        \n        msle_train = mean_squared_log_error(pred_train,y_train)\n        msle_test = mean_squared_log_error(pred_test,y_test)\n        \n        data.append((train_score,test_score,oob_score,msle_train,msle_test))\n    df = pd.DataFrame(data = data, index = list(feature_to_examin.keys()), columns = (\"train_scr\",\"test_scr\",\"oob_scr\",\"train_msle\",\"test_msle\"))    \n    return df\n    \n    \nX_train, X_test, y_train, y_test = train_test_split(houses_train_modified, houses_train[\"SalePrice\"].values, test_size=0.2, random_state=42)\nresults = ceck_basemdl_scores(X_train, X_test, y_train, y_test, feature_to_examin)","38f946f0":"results","18d4a5e3":"rf_clf = RandomForestRegressor(oob_score=True)\nrf_clf.fit(houses_train_modified[feature_to_examin[\"kmeans_and_pca\"]],houses_train[\"SalePrice\"].values)\ntrain_score = rf_clf.score(houses_train_modified[feature_to_examin[\"kmeans_and_pca\"]],houses_train[\"SalePrice\"].values)\n\nprint(f\"full train data score: {train_score}\")\nprint(f\"OOB score: {rf_clf.oob_score_}\")\n\nfeature_importances = pd.Series(data=rf_clf.feature_importances_, index=feature_to_examin[\"kmeans_and_pca\"])\nfeature_importances.sort_values(ascending = False).head(20).plot.bar();\n","7434c452":"#predictions using the RFR classifier\nrfr_test_prediction = rf_clf.predict(houses_test_modified[feature_to_examin[\"kmeans_and_pca\"]])","83889116":"from sklearn.linear_model import LinearRegression, ElasticNet, Lasso, Ridge\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom datetime import date\nfrom sklearn.model_selection import GridSearchCV","d777d980":"features_for_lr = sorted(list(zip(pca.components_[0],cols_for_pca)),key=lambda x: -abs(x[0]))[:10]\nfeatures_name_set_one = [f[1] for f in features_for_lr]","737c6147":"houses_train_modified_copy = houses_train_modified.copy()","3fa12fa8":"#Compute VIF data for each independent variable\nvif = pd.DataFrame()\nvif[\"features\"] = houses_train_modified_copy[features_name_set_one].columns\nvif[\"vif_Factor\"] = [variance_inflation_factor(houses_train_modified_copy[features_name_set_one].values, i) for i in range(houses_train_modified_copy[features_name_set_one].shape[1])]\nvif","b9b8a309":"# creating the date object of today's date\ntodays_date = date.today()\nhouses_train_modified_copy['house_age'] = todays_date.year - houses_train_modified_copy['YearBuilt'] ","b1dabf34":"features_name_set_one.remove('YearBuilt')\nfeatures_name_set_one.append('house_age')","a3e6807d":"vif = pd.DataFrame()\nvif[\"features\"] = houses_train_modified_copy[features_name_set_one].columns\nvif[\"vif_Factor\"] = [variance_inflation_factor(houses_train_modified_copy[features_name_set_one].values, i) for i in range(houses_train_modified_copy[features_name_set_one].shape[1])]\nvif","11d315e3":"features_name_second_set = list(feature_importances.sort_values(ascending = False).index[:10])","bc013738":"vif = pd.DataFrame()\nvif[\"features\"] = houses_train_modified_copy[features_name_second_set].columns\nvif[\"vif_Factor\"] = [variance_inflation_factor(houses_train_modified_copy[features_name_second_set].values, i) for i in range(houses_train_modified_copy[features_name_second_set].shape[1])]\nvif","9a2e920d":"houses_train_modified_copy['Garage_age'] = todays_date.year - houses_train_modified_copy['GarageYrBlt'] \nfeatures_name_second_set.remove('GarageYrBlt')\nfeatures_name_second_set.append('Garage_age')","7f1fa8d2":"vif = pd.DataFrame()\nvif[\"features\"] = houses_train_modified_copy[features_name_second_set].columns\nvif[\"vif_Factor\"] = [variance_inflation_factor(houses_train_modified_copy[features_name_second_set].values, i) for i in range(houses_train_modified_copy[features_name_second_set].shape[1])]\nvif","be932bfa":"features_third_set = ['OverallQual','GrLivArea','TotalBsmtSF','GarageArea',\n                      'LotArea_log','YearBuilt','TotRmsAbvGrd','FullBath',\n                      'MSZoning_fact','Street_fact','Neighborhood_fact','Condition1_fact',\n                      'HouseStyle_fact','CentralAir_fact','KitchenQual_fact']","8fd2e2e0":"X_train, X_test, y_train, y_test = train_test_split(houses_train_modified[features_third_set], houses_train[\"SalePrice_log\"].values, test_size=0.2, random_state=42)\nparametersGrid = {\"max_iter\": [1, 5, 10],\n                  \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n                  \"l1_ratio\": np.arange(0.0, 1.0, 0.1)}\n\neNet = ElasticNet()\ngrid = GridSearchCV(eNet, parametersGrid, cv=10)\ngrid.fit(X_train, y_train)","035d2808":"df = pd.DataFrame(grid.cv_results_)\ndf[['param_alpha','param_l1_ratio','param_max_iter','mean_test_score','rank_test_score']].sort_values(by = 'rank_test_score')","9850e0b0":"ENclf = ElasticNet(max_iter = 10 ,alpha = 0.01, l1_ratio = 0)\nENclf.fit(X_train ,y_train)\ntrain_score = ENclf.score(X_train,y_train)\nprint(f\"the score of the train set is {train_score}\")\ntest_score = ENclf.score(X_test,y_test)\nprint(f\"the score of the test set is {test_score}\")","0d57722e":"from catboost import CatBoostRegressor\nfrom sklearn.metrics import mean_squared_error","aa95c1b8":"data = []\nfor k,v in feature_to_examin.items():\n    \n#     print(f\"trying with {k} features\")\n    \n    X_train, X_test, y_train, y_test = train_test_split(houses_train_modified[v], houses_train[\"SalePrice\"].values, test_size=0.2, random_state=42)\n    # parametersGrid = {\"max_iter\": [1, 5, 10],\n    #                   \"alpha\": [0.0001, 0.001, 0.01, 0.1, 1, 10, 100],\n    #                   \"l1_ratio\": np.arange(0.0, 1.0, 0.1)}\n\n    CBclf = CatBoostRegressor()\n    # grid = GridSearchCV(eNet, parametersGrid, cv=10)\n    CBclf.fit(X_train, y_train,verbose=False)\n#     pred = CBclf.predic|t(X_test)\n    train_score = CBclf.score(X_train, y_train)\n    test_score = CBclf.score(X_test, y_test)\n    data.append((train_score,test_score))\n\nCB_results_per_feature =  pd.DataFrame(data = data, index = list(feature_to_examin.keys()), columns = (\"train_scr\",\"test_scr\"))","f4b57935":"CB_results_per_feature","a2368601":"CBclf = CatBoostRegressor()\nCBclf.fit(houses_train_modified[feature_to_examin[\"kmeans\"]],houses_train[\"SalePrice\"].values, verbose=False)","00c9d49c":"CB_test_predictions = CBclf.predict(houses_test_modified[feature_to_examin[\"kmeans\"]])","69738d64":"test_prediction_to_upload = pd.DataFrame(data = list(zip(houses_test.Id.values,CB_test_predictions)), columns = [\"Id\",\"SalePrice\"])\ntest_prediction_to_upload.to_csv(\".\/houses_SalePrice_predictions.csv\",sep=\",\",index=False)","d41a5c99":"The basic data with the feature generated with k-means seems to work best. lets use this","ee6c3838":"First, we want to factorize the categorical features so some basic techniques could handle them as numerical","2175b41a":"Now Lets find that features for which taking log indeed improve","3e227220":"Lets try Catboost with the different set of features that we have created and tried with the RFR baseline model","7a71400c":"# Baseline model(s)","b0822afd":" removing 'YearBuilt' from features_name list and appending 'house_age'","7e314b56":"We can see that some features are highly correlated. We shall consider remove some of features in training to avoid multicolinearity","7ba36500":"Now Lets explore our categorical fearues..\nWe can boxplot the target variable for each group in each categorical feature","f23a9ea3":"##### outliers\nA common method to define outliers is the \u201c3 times the standard deviation\u201d rule. For now, we will use this method and replace all the exceptional values with lower_limit\/upper_limits defined by 3std with respect to the feature mean.\nWe also apply the same approuch on the test using the mean and std calculated from the train.","b8dee8bc":"Now lets keep the log of these features (and also keep in mind that some may not be relevant at all. e.g. MiscVal)","0ef3b448":"We can see that the Random forest regressor with the features produced by kmeans and PCA together yield slightly better results than the same model with other more lin sets of feature. Thus, we will use this to produce the Baseline model results   ","dc89b41c":"# Part 2","a0f318f0":"We can try to cluster the datapoints and use the labels of the clusters as features. We may find it usefull later to also convert the arbitrary clusters' labels to factors corresponding to ranks oF the cluster mean SalePrice value as we did with existing categorical features. it is reasonable to combine different features that together could indicate the property total area to create a new feature that (intuitively) should be highly correlated with the target variable.","2b617ae4":"Again, we still have multicolinerity. \nLets just use the features which are highly correlated with the target variable and make sure we do not include pairs of features which are correlated with each other (see plots above for inter-features correlations and correlations with target variable). ","e26e34ab":"First, we would like to understand the distribution describing the target variable. We aim to work with normaly distributed variables (target and features) because then, methods that are non-invariant to standartization such as LASSO or ridge regression should work more properly.\nAlso, for sake of interpertability, validity of measures of association (such as correlations), and insensitivity to the features scales, the different variables should be normalized.  ","5ab30b1d":"# House Prices With Advanced Feature Engineering\n","a4375f1a":"These above plots help a lot when determining the dependancy of the target variable in the different quantitative features.\nWe can look at the estimated slopes of the different fits. fits with small slopes values correspond to features in which the target variable is somewhant independant (e.g. MoSold, YrSold etc.). However, some features are clearly correlated with the target variable and may contribute later on when we try to build our LR model (e.g OverallQual, TotRmsAbvGrd). \n","9a31e651":"First, we will examine the correlation between the independent variables and the dependent variable (our target) and among themselves. we will test quantitative features first.","345cb6cd":"First lets try the following intuition: we take the most dominant features comining the most principal component generated using PCA:","e3854275":"### Random forest regressor","8323b560":"# Correlations","b5b142ef":"Generally, when VIF is higher than 10, there is significant multicollinearity that needs to be corrected.\nin order to tackel this problem we will try to transform the 'YearBuilt' variable to make it less correlated but still maintain the information it brings to the model:","f16be214":"### Writing results to required format","68b2fd21":"# Improving results with Catboost","78ba66cd":"based on elbow method we should use k=5.","5a686d18":"Seems more like a log normal distribution\n","d3ecc12e":"Now lets move on to examine the features. First. how many NAs do we have?","c16a80d2":"Creating a (SalePrice) sorted label feature:","2f6d81c6":"Looking at the correlations between SalePrice and numerical features, we can see that setting a threshold of absolute value of $|R|=0.4$ will maintain about half of the features. ","e671266b":"Before we do anything, we should determine the best possible k to cluster the data.\nWe can use elbow method to decide on that:","c44a2c8e":"We first want to find important features and also make sure to chose features that are somewhat independant of each other (avoiding multicolinearity).","5791cac9":"## Feature engineering\n\n### K-means","8f51890d":"### PCA\n\nWe can also use PCA to create combinations of features that explain most of the variance of the data","38b88296":"### Linear regression","f4efce31":"As we can see, we succeeded in reducing the VIF, however it is not enough :(.\nmeaning, we can't just use the features combining the most principal component as the are highly correlated.  \nInstead, we will try to use the most important features from the Random Forest Regresson we have tried above.","42ba380b":"Again, lets try to reduce VIF by tranforming year built to age","e113e550":"# Overview","6582cd86":"Our results does not improve significantly with respect to the RFR. let try to use tree boosting to improve even further (recall that in decision trees, we do not care about multicolineraity and correlations)","851b56bc":"Lets also print 2 box plots of critical numerical features","7639c010":"# SalePrice statistic","e90b2f4e":"By examining the kurtosis and skewness, we can find variables which their original distribution is far from normal. We would hope to normalize those. Lets try the lognormal (and specificaly focus on variables that are right now far from being normal such as LotArea, 3SsnPorch, PoolArea etc.).","fde7abf5":"Some features have large portions of NAs, But these are not actually missing values as they reprecent the absence of a house characraristic such as not having a pool (PoolQC, Alley etc.). However the MiscFeature which describe the present of some special charactaristics (such as 2nd garage, elevator etc.) could be discarded entirely as those few special features are not comparable...  \n\n\nFor numerical data we can later fill in the missing values with columns averages. For categorical, we can change to `None`\n\nWe should note that, when filling in the missing values with averages, we are not affecting the average, but are affecting the general distribution, and that should be taken into consideration.","8c8d2edb":"One can see that for some features the target variable distributions in different groups differ from each other quit a lot. Those will be good candidates for our model as the capture some of the variance in the target variable","48fbe524":"We can see that (unsurpisingly) by far the most important feature is the first PC created using PCA ","43d4a9df":"As mentioned above, we shall also try to work with normaly distributed features","cf5e6d33":"Lets try the normal and log-normal for the target:","995444ca":"# Features"}}