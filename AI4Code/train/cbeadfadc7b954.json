{"cell_type":{"31c83e56":"code","3637f20c":"code","cea43159":"code","2e6f5786":"code","153ab87f":"code","819fa29f":"code","2b8e28e3":"code","4c50ba43":"code","0f248ee2":"code","fc4f3348":"code","dba771a3":"code","f19a4d0c":"code","1dcf13c1":"code","c5bfb85b":"code","e2783c02":"code","1e1db831":"code","042c271a":"code","645be64a":"code","119be160":"code","80cafe2a":"code","3bf1896c":"code","53266fab":"code","4692042b":"code","4e2954d0":"code","e4c4cc6f":"code","ac7beb57":"code","f8ac0f1f":"code","8ec70096":"code","26bedc67":"code","68ce881d":"code","0fbaf1b4":"code","bbe8c3c3":"code","1275637b":"code","7ec7daa3":"code","722a8eb8":"code","aae315f0":"code","90cdf943":"code","0cd54131":"code","b8702d16":"code","c3936803":"code","7835c2b9":"code","cab5433a":"code","551360ca":"code","1239f745":"code","a58114e0":"code","82848c91":"code","98776316":"code","f774e409":"code","8da9dfc0":"code","5c64b279":"code","55c908e4":"code","16eebf34":"code","d94eba07":"code","1926900d":"code","2cd67f20":"code","fe9ca659":"code","f2bbfd5d":"code","b1d63e54":"code","28e67e50":"code","37151851":"code","5f464a51":"code","5f771f4c":"code","656ad345":"code","07f14965":"code","3257b661":"code","20f0e8d0":"code","d11c7582":"code","fc97bcdd":"code","64d8125f":"code","37702bd7":"code","bc422c02":"code","f6a0d2ef":"code","79cca839":"code","31f9f5e9":"code","3b0dc0a7":"code","8f0b77a9":"code","adb033a4":"code","0e8a4a3d":"code","d4100975":"code","67024220":"code","6da88111":"code","4efcd3fc":"code","50718900":"code","b0acd7b7":"code","5189bad4":"code","ddc6c971":"code","b35f60f1":"code","322172ad":"code","36d570dd":"code","f38861ed":"code","16098cd4":"code","67f74f03":"code","8d029ed0":"markdown","0042c85d":"markdown","c9b95f02":"markdown","397e3ef7":"markdown","bcc1c097":"markdown","bb8ac00a":"markdown","c90f7754":"markdown","1e61a211":"markdown","1cf0b170":"markdown","50569578":"markdown","6b772929":"markdown","1fa25d68":"markdown","f42c5fb2":"markdown","7ca2b3b8":"markdown","da4a32cb":"markdown","3517e0ff":"markdown","6f7d4f4b":"markdown","cc60d0c3":"markdown","23cf412e":"markdown","4ceba2e8":"markdown","c7c884ef":"markdown","daefcf6f":"markdown","0fb2b183":"markdown","59d96f81":"markdown","67be064b":"markdown","cfa3640f":"markdown","6a0d0cc8":"markdown","2049bc58":"markdown","09bd2df8":"markdown"},"source":{"31c83e56":"# Importing Libraries\nimport pandas as pd\nimport numpy as np\nfrom scipy.stats import mode\nimport seaborn as sns\nfrom matplotlib import pyplot as plt","3637f20c":"# Reading the Train and Test CSV files\ntrain = pd.read_csv(\"..\/input\/bigmart-dataset\/Train.csv\")\ntest = pd.read_csv(\"..\/input\/bigmart-dataset\/Test.csv\")","cea43159":"train.head()","2e6f5786":"train.describe()","153ab87f":"train.info()","819fa29f":"# Join both the train and test dataset\ntrain['source']='train'\ntest['source']='test'\n\ndataset = pd.concat([train,test], ignore_index = True)\nprint(\"Train dataset shape:\",train.shape)\nprint(\"Test dataset shape:\",test.shape)\nprint(\"Concatenated dataset shape:\",dataset.shape)","2b8e28e3":"dataset.info()","4c50ba43":"dataset.head()","0f248ee2":"dataset.isnull().sum()","fc4f3348":"#To find percentage of test set in the dataset\nprint(dataset[\"Item_Outlet_Sales\"].isnull().sum()\/dataset.shape[0]*100,\"%\")","dba771a3":"# pivot_table() allows us to create a table that contains the mean values of identifiers\navg = pd.pivot_table(dataset,values='Item_Weight', index='Item_Identifier',aggfunc='mean')\navg","f19a4d0c":"# We find that all examples containing the same Item_Identifier value have same Item_Weight.\n# This proves the fact that the mean is same as their value.\n# For example the mean value for the value \"DRA12\" in the Item_Identifier is same as the Item_Weight for \n# individual examples in the Item_Weight column.\n\ndf=dataset[dataset['Item_Identifier'].str.contains(\"DRA12\")]\ndf","1dcf13c1":"dataset[:][dataset['Item_Identifier'] == 'DRI11']\ndef impute(cols):\n    Weight = cols[1]\n    Identifier = cols[0]\n    \n    if pd.isnull(Weight):\n        return avg['Item_Weight'][avg.index == Identifier]\n    else:\n        return Weight\nprint ('Orignal Number of missing values in Item_Weight:',sum(dataset['Item_Weight'].isnull()))\n\n# Applying the impute() function to impute null values of Item_Weight\ndataset['Item_Weight'] = dataset[['Item_Identifier','Item_Weight']].apply(impute,axis=1).astype(float)\n\nprint ('Number of missing values in Item_Weight after imputation: ',sum(dataset['Item_Weight'].isnull()))","c5bfb85b":"# Finding unique Outlet Types\ndataset.Outlet_Type.unique()","e2783c02":"# pivot_table() allows us to create a table that contains the mode of identifiers\nmode = pd.pivot_table(dataset, values='Outlet_Size', columns='Outlet_Type',aggfunc=lambda x:x.mode())\nmode","1e1db831":"# Imputing Outlet_Size missing values with their mode\ndef impute_mode(cols):\n    size = cols[1]\n    Type = cols[0]\n    \n    if pd.isnull(size):\n        return mode.loc['Outlet_Size'][mode.columns == Type][0]\n    else:\n        return size\nprint ('Orignal Number of missing values in Outlet_Size:',sum(dataset['Outlet_Size'].isnull()))\n\n# Applying the impute() function to impute null values of Item_Weight\ndataset['Outlet_Size'] = dataset[['Outlet_Type','Outlet_Size']].apply(impute_mode,axis=1)\n\nprint ('Number of missing values in Outlet_Size after imputation: ',sum(dataset['Outlet_Size'].isnull()))","042c271a":"dataset.isnull().sum()","645be64a":"dataset.head()","119be160":"dataset.Item_Fat_Content.unique()","80cafe2a":"dataset['Item_Fat_Content'] = dataset['Item_Fat_Content'].replace({'low fat':'Low Fat','reg':'Regular','LF':'Low Fat'})\ndataset.Item_Fat_Content.unique()","3bf1896c":"dataset['Outlet_Year'] = 2020 - dataset['Outlet_Establishment_Year']\ndataset.drop(['Outlet_Establishment_Year'],axis=1,inplace=True)\ndataset.Outlet_Year.unique()","53266fab":"dataset.to_csv('insight.csv')\ndataset.head()","4692042b":"vmean = dataset.pivot_table(index = \"Item_Identifier\",  values = \"Item_Visibility\")","4e2954d0":"dataset.loc[(dataset[\"Item_Visibility\"] == 0.0), \"Item_Visibility\"] = dataset.loc[(dataset[\"Item_Visibility\"] == 0.0), \"Item_Identifier\"].apply(lambda x : vmean.at[x, \"Item_Visibility\"])","e4c4cc6f":"# Turning all categorical variables into numerical values can be done by mapping each categorical value with  \n# respective FREQUENCY of the values in the column\n\ncat_var = ['Item_Fat_Content','Outlet_Location_Type','Outlet_Size','Outlet_Type','Item_Type','Item_Identifier','Outlet_Identifier']\nfor i in cat_var:\n    p  = dataset[i].value_counts().to_dict()\n    dataset[i] = dataset[i].map(p)","ac7beb57":"dataset.head()","f8ac0f1f":"#Divide into test and train:\ntrain = dataset.loc[dataset['source']==\"train\"]\ntest = dataset.loc[dataset['source']==\"test\"]\n#Drop unnecessary columns:\ntest.drop(['source'],axis=1,inplace=True)\ntrain.drop(['source'],axis=1,inplace=True)","8ec70096":"train.head()","26bedc67":"test.head()","68ce881d":"corr_matrix=train.corr()\ncorr_matrix['Item_Outlet_Sales']","0fbaf1b4":"sns.pairplot(data=train,y_vars=['Item_Outlet_Sales'],x_vars=['Item_Identifier','Item_Weight', 'Item_Fat_Content',\n       'Item_Visibility', 'Item_Type', 'Item_MRP','Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type',\n       'Outlet_Type', 'Outlet_Year'])\n","bbe8c3c3":"plt.figure(figsize = (10,5))\nsns.heatmap(corr_matrix, cmap = \"RdYlGn\", annot = True)","1275637b":"train= train.drop(['Item_Identifier','Item_Weight', 'Item_Fat_Content', 'Item_Type', 'Outlet_Location_Type'],axis=1)","7ec7daa3":"train.skew()","722a8eb8":"# Before Scaling\nfig, ax = plt.subplots(2,3,figsize = (15,15))\nsns.distplot(train[\"Item_Visibility\"], kde =True, ax=ax[0,0], color = \"red\")\nsns.distplot(train[\"Item_MRP\"], kde =True, ax=ax[0,1], color = \"blue\")\nsns.distplot(train[\"Outlet_Identifier\"], kde =True, ax=ax[0,2], color = \"orange\")\nsns.distplot(train[\"Outlet_Size\"], kde =True, ax=ax[1,0], color = \"magenta\")\nsns.distplot(train[\"Outlet_Type\"], kde =True, ax=ax[1,1], color = \"black\")\nsns.distplot(train[\"Outlet_Year\"], kde =True, ax=ax[1,2])","aae315f0":"for i in train.columns:\n    train[i] =np.log(train[i])","90cdf943":"train.head()","0cd54131":"#After Scaling the variables\nfig, ax = plt.subplots(2,3,figsize = (15,15))\nsns.distplot(train[\"Item_Visibility\"], kde =True, ax=ax[0,0], color = \"red\")\nsns.distplot(train[\"Item_MRP\"], kde =True, ax=ax[0,1], color = \"blue\")\nsns.distplot(train[\"Outlet_Identifier\"], kde =True, ax=ax[0,2], color = \"orange\")\nsns.distplot(train[\"Outlet_Size\"], kde =True, ax=ax[1,0], color = \"magenta\")\nsns.distplot(train[\"Outlet_Type\"], kde =True, ax=ax[1,1], color = \"black\")\nsns.distplot(train[\"Outlet_Year\"], kde =True, ax=ax[1,2])","b8702d16":"y=train[\"Item_Outlet_Sales\"]\nx= train.drop([\"Item_Outlet_Sales\"],axis=1)\nx.head()","c3936803":"from matplotlib import pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import r2_score","7835c2b9":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20, random_state=0)","cab5433a":"reg = LinearRegression()\nreg = reg.fit(x_train,y_train)","551360ca":"reg.coef_","1239f745":"reg.intercept_","a58114e0":"y_pred = reg.predict(x_test)","82848c91":"rmse = np.sqrt(metrics.mean_squared_error(y_test, y_pred))\nr2_score = r2_score(y_test, y_pred)\nprint('Root Mean Squared Error:',rmse )\nprint('R2_Score:',r2_score*100,\"%\" )","98776316":"residue_lreg = y_test - y_pred\n#Plotting Residual Plot\nplt.scatter(y_test,residue_lreg, c = \"blue\")\nplt.xlabel(\"Residual Plot for Linear Regression\")\nplt.ylabel(\"y_test\")\nplt.axhline(y = 0)","f774e409":"from sklearn.linear_model import Lasso, Ridge\nls = Lasso(alpha = 0.009)\nls = ls.fit(x_train, y_train)","8da9dfc0":"ls.coef_","5c64b279":"ls.intercept_","55c908e4":"ls_pred = ls.predict(x_test)","16eebf34":"rmse_LS = np.sqrt(metrics.mean_squared_error(y_test, ls_pred))\nprint('Root Mean Squared Error:',rmse_LS )","d94eba07":"from sklearn.metrics import r2_score\nr2_score_LS = r2_score(y_test, ls_pred)\nprint('R2_Score:',r2_score_LS*100,\"%\" )","1926900d":"#RESIDUE VALUE AFTER LASSO REGRESSION\nresidue_lasso = y_test - ls_pred\n#Plotting Residual Plot\nplt.scatter(y_test,residue_lasso, c = \"blue\")\nplt.xlabel(\"Residual Plot for Lasso Regression\")\nplt.ylabel(\"y_test\")\nplt.axhline(y = 0)","2cd67f20":"#Ridge Regression\nrr = Ridge(alpha = 0.009)\nrr.fit(x_train, y_train)","fe9ca659":"#Prediction AFTER Ridge regression\nrr_pred = rr.predict(x_test)","f2bbfd5d":"#Accuracy score check\nr2_score_RR = r2_score(y_test, y_pred)\nprint('R2_Score:',r2_score_RR*100,\"%\" )","b1d63e54":"#RMSE\nrmse_ridge = np.sqrt(metrics.mean_squared_error(y_test, rr_pred))\nrmse_ridge","28e67e50":"#residue after ridge\nresidue_rr = y_test-rr_pred\n#Plotting Residual Plot\nplt.scatter(y_test,residue_rr, c = \"blue\")\nplt.xlabel(\"Residual Plot for Ridgeo Regression\")\nplt.ylabel(\"y_test\")\nplt.axhline(y = 0)","37151851":"#Linear Regression\nLreg_coef = pd.Series(reg.coef_,index =x.columns)\nLreg_coef.plot(kind=\"bar\", title= \"Linear\")","5f464a51":"# Lasso Regression\nlasso_coef = pd.Series(ls.coef_,index =x.columns)\nlasso_coef.plot(kind=\"bar\", title= \"Lasso\")","5f771f4c":"# Ridge Regression\nridge_coef = pd.Series(rr.coef_,index =x.columns)\nridge_coef.plot(kind=\"bar\", title= \"Ridge\")","656ad345":"df1 = pd.DataFrame(columns=[\"Linear Regression\", \"Ridge Regression\",\"Lasso Regression\"])\nfor i in range(len(rr.coef_)):\n    df1=df1.append({\"Linear Regression\":reg.coef_[i],\"Ridge Regression\":rr.coef_[i], \"Lasso Regression\":ls.coef_[i]}, ignore_index = True)\ndf1","07f14965":"test.head()","3257b661":"test= test.drop([\"Item_Outlet_Sales\"],axis=1)\ntest_= test.drop(['Item_Identifier','Item_Weight', 'Item_Fat_Content', 'Item_Type', 'Outlet_Location_Type'],axis=1)","20f0e8d0":"test_.head()","d11c7582":"test_.skew()","fc97bcdd":"for i in test_.columns:\n    test_[i] =np.log(test_[i])","64d8125f":"fig, ax = plt.subplots(2,3,figsize = (15,15))\nsns.distplot(test_[\"Item_Visibility\"], kde =True, ax=ax[0,0], color = \"red\")\nsns.distplot(test_[\"Item_MRP\"], kde =True, ax=ax[0,1], color = \"blue\")\nsns.distplot(test_[\"Outlet_Identifier\"], kde =True, ax=ax[0,2], color = \"orange\")\nsns.distplot(test_[\"Outlet_Size\"], kde =True, ax=ax[1,0], color = \"magenta\")\nsns.distplot(test_[\"Outlet_Type\"], kde =True, ax=ax[1,1], color = \"black\")\nsns.distplot(test_[\"Outlet_Year\"], kde =True, ax=ax[1,2])","37702bd7":"test_.head()","bc422c02":"item_outsale_pred = reg.predict(test_)","f6a0d2ef":"item_outsale_pred","79cca839":"#Performing inverse transformation\nactual_item_outsale = np.exp(item_outsale_pred+1)","31f9f5e9":"actual_item_outsale","3b0dc0a7":"#ADDING THE PREDICTED ITEM_OUTLET_SALE COLUMNS TO TEST DATA\ntest = pd.read_csv(\"..\/input\/bigmart-dataset\/Test.csv\")\ntest[\"Item_Outlet_Sales\"] = actual_item_outsale","8f0b77a9":"test","adb033a4":"test.to_csv('testLR.csv')","0e8a4a3d":"vis = pd.read_csv(\"insight.csv\")\nvis.head()","d4100975":"#FINDING FREQUENCY COUNT OF OUTLET TYPE\nsns.countplot(data = vis, x = \"Outlet_Type\",hue = \"Outlet_Size\")\nplt.xticks(rotation =90)","67024220":"# Relation between Outlet_Identifier and Item_Outlet_Sales\nimport matplotlib.pyplot as plt\nplt.figure(figsize = (10,5))\nsns.barplot(data = vis, x = \"Outlet_Identifier\", y= \"Item_Outlet_Sales\")","6da88111":"plt.figure(figsize = (10,5))\nsns.barplot(data = vis, x = \"Outlet_Size\", y= \"Item_Outlet_Sales\")","4efcd3fc":"import matplotlib.pyplot as plt\nplt.figure(figsize = (10,5))\nsns.barplot(data = vis, x = \"Outlet_Type\", y= \"Item_Outlet_Sales\")","50718900":"plt.figure(figsize = (10,5))\nsns.barplot(data = vis, x = \"Outlet_Location_Type\", y= \"Item_Outlet_Sales\")","b0acd7b7":"vis.groupby(\"Outlet_Year\")[\"Item_Outlet_Sales\"].mean().plot.bar()\nplt.ylabel(\"Mean of Item outlet sales\")","5189bad4":"#Understanding to item_type per year with respective to mean of each respective year item outlet sales\nvis.groupby(\"Item_Type\")[\"Item_Outlet_Sales\"].mean().plot.bar()\nplt.ylabel(\"Mean of Item outlet sales\")","ddc6c971":"#Understanding to outlet_type per year with respective to mean of each respective year item outlet sales\nvis.groupby(\"Outlet_Type\")[\"Item_Outlet_Sales\"].mean().plot.bar()\nplt.ylabel(\"Mean of Item outlet sales\")","b35f60f1":"sns.countplot(vis[\"Item_Fat_Content\"]).set_title=\"Item_Fat_Content\"\nprint(vis[\"Item_Fat_Content\"].value_counts(normalize=True))\nplt.xticks(rotation=90)\nplt.show()","322172ad":"sns.countplot(vis[\"Outlet_Size\"]).set_title=\"Outlet_Size\"\nprint(vis[\"Outlet_Size\"].value_counts(normalize=True))\nplt.xticks(rotation=90)\nplt.show()","36d570dd":"sns.countplot(vis[\"Outlet_Location_Type\"]).set_title=\"Outlet_Location_Type\"\nprint(vis[\"Outlet_Location_Type\"].value_counts(normalize=True))\nplt.xticks(rotation=90)\nplt.show()","f38861ed":"sns.countplot(vis[\"Item_Type\"]).set_title=\"Item_Type\"\nprint(vis[\"Item_Type\"].value_counts(normalize=True))\nplt.xticks(rotation=90)\nplt.show()","16098cd4":"sns.jointplot(train[\"Item_Outlet_Sales\"],train[\"Item_MRP\"],kind=\"reg\")","67f74f03":"sns.jointplot(train[\"Item_Outlet_Sales\"],train[\"Item_Visibility\"],kind=\"reg\")","8d029ed0":"# Normalization\n<b>We find that the predictors are all not following normal distribution. Hence we need to apply normalization. I am using Log Scaling for this purpose.<\/b>","0042c85d":"<b>Now, in this concatenated dataset we have 3 features with missing NaN values. Rows from 8524 to 14204 (Test dataset) have NaN values in the 'Item_Outlet_Sales' column. This is our Target Variable. <\/b> ","c9b95f02":"# STEP 3:Converting the categorical data into numerical data appropriately\n<b> scikit-learn only accepts numerical variables. Hence, we need to convert all categorical variables into numeric types.<\/b>","397e3ef7":"# Analysing the Dataset\n<b>\nItem_Identifier: Code given to items\n        \nItem_Weight: Weight of an item\n    \nItem_Fat_Content: Fat content of the item like low, high etc\n    \nItem_Visibility: A continuous value indicating the visibility of the item for a customer\n    \nItem_Type: Type of the item like dairy, vegetables etc\n    \nItem_MRP: Maximum retail price of the item\n    \nOutlet_Identifier: Code given to outlets\n    \nOutlet_Establishment_Year : Year of establishment of an outlet\n    \nOutlet_Size: Size of an outlet like medium, high etc.\n    \nOutlet_Location_Type: Location of the outlet in a city like Tier1, 2 etc.\n    \nOutlet_Type: Type of the outlet like supermarket, grocery shop etc.\n    \nItem_Outlet_Sales: Sale value of an item in outlet. It is a target variable.\n    \n\n# MY ANALYSIS:\n\nThe heatmap described that Item_Identifier, Item_Weight, item_fat_content, Item_Type, Outlet_Location_Type have less correlation with the Item_Outlet_Sales (Target Variable). Therefore, We removed these predictors from the model.\n\nThe Predictor variables (X):'Item_Visibility', 'Item_MRP', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Type','Outlet_Year'.\n    \nThe Target variavle (Y)    : Item_Outlet_Sales <\/b>\n\n# Lets plot various graphs to get an insight on the dataset","bcc1c097":"# Fitting Linear Regression Model:","bb8ac00a":"# b) replacing missing data with substituted values\n<b>Out of the 3 features with missing NaN values, the missing values in 'Item_Outlet_Sales' are the Values that need to be predicted by our model. Hence, we need to impute the NaN values in the Item_Weight and Outlet_Size columns.\nItem_Weight missing values can be replaced by their mean and \nOutlet_Size missing values can be replaces by their mode<\/b>","c90f7754":"# STEP 4: Finding Significant Predictor Variables","1e61a211":"# Train\/Test Split","1cf0b170":"# Importing necessary Libraries","50569578":"# Applying the Linear Regression Model to the test dataset","6b772929":"# Regularization:\n\n# 1) Lasso Regression (L1 Regularization)","1fa25d68":"# Deciding Best Fit Model\n<u><b>RMSE:<\/b><\/u>\n\n<b>Linear Regression:<\/b> 0.5375914591271844\n\n<b>Lasso Regression:<\/b> 0.5517663802701465\n\n<b>Ridge Regression:<\/b> 0.5375929508668702\n\n<u><b>r2_score:<\/b><\/u>\n\n<b>Linear Regression:<\/b> 72.95010719621588 %\n\n<b>Lasso Regression:<\/b> 71.50482703170576 %\n\n<b>Ridge Regression:<\/b> 72.95010719621588 %\n\nA model is said to be best fit if r2_score is closer to 100% and has a better RMSE score.\n\n1) Lasso regression has lower accuracy than Linear and Ridge Regression.\n\n2) Ridge regression and Linear regression have similar R2_Score values. But Linear Regression model has better RMSE value. \n\nTherefore, the best fit model for the given dataset is: <b>LINEAR REGRESSION<\/b>","f42c5fb2":"# Saving the predicted values to the test dataset as testLR.csv","7ca2b3b8":"# STEP 5: Building Model and identifying the best fit model\n\n# Defining the Tatget and Predictor Variables\n<b>Next, we need to define the Target and Predictor variables.<\/b>","da4a32cb":"# Normalizing the Test dataset values","3517e0ff":"# STEP 1: Data Pre- Processing\n\n# a) Finding the Missing Value Features","6f7d4f4b":"# Plotting the coeffecients of each model","cc60d0c3":"# STEP 6: Predicting the values of sales for the data given in the file Test.csv","23cf412e":"# 2) Ridge Regression (L2 Regularization)","4ceba2e8":"<b>Here, there are typos line case sensitive words and short cuts. The actual unique values must be [\"Low Fat\", \"Regular\"]. This modification has to be done before applying the model.<\/b>","c7c884ef":"<b>Also, the Outlet_Establishment_Year contains year in which the outlet was established. If we can get information on how many years the outlet has been working, then taht data would be more useful.<\/b>","daefcf6f":"# Conclusions:\n<b>After plotting vatious graphs and plotting of heatmap:\n\nThe most significant predictors is: Item_MRP\n\nThe other significant predictors are: Outlet_Type, Outlet_Size, Outlet_Year.\n\n1 . Stores located in Tier 2 cities should have higher sales.\n\n2 . Stores with higher population will have more sales.\n\n3 . Stores located within popular marketplaces should have higher sales because of better access to customers. \n\n4 . The most common food that people buy is Fruits nad Vegetables, Snack items and Frozen foods.\n\n5 . Though the number of small size shops are more, Medium and large shops get better sales as they can accomodate more items.\n\nSales increase with respect to bigger size of outlet, lower prices of items, populat locations, variety of items available, number of years of establishment.<\/b>\n","0fb2b183":"# Removing the less correlated features\n<b>From the above plot, we can visualize that Item_Identifier, Item_Weight, item_fat_content,  Item_Type, Outlet_Location_Type have less correlation with the Outlet_Location_Type (Target Variable). We can remove these predictors from the model.<\/b>","59d96f81":"<b>From train.info(), it is clear that the Item_Weight and Outlet_Size have NaN values (Null values). \nAlso, we can see that the dataset has 5 Numerical features(Item_Weight, Item_Visibility, Item_MRP, Outlet_Establishment_Year and Item_Outlet_Sales) and 7 Categorical features(Item_Identifier, Item_Fat_Content, Item_Type, Outlet_Identifier, Outlet_Size, Outlet_Location_Type and Outlet_Type).<\/b>","67be064b":"<b>First, we need to find the correlation between each predictor and the target variable. This can be done using the corr() method.<\/b>","cfa3640f":"<b>Finding the skewness gives helps us identify if scaling of the features is required.<\/b>","6a0d0cc8":"# ML ASSIGNMENT ON REGRESSION","2049bc58":"# c) Modifying the features in a more intuitive manner","09bd2df8":"# Visualizing the correlation gives a better understanding."}}