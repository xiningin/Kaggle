{"cell_type":{"920a6ea9":"code","42961344":"code","a1788a77":"code","3ef80bd0":"code","9fca8448":"code","bf48aced":"code","e00fa93b":"code","2e3dd8b7":"code","a6ec6861":"code","a31ae503":"code","86d39dc2":"code","d51173b1":"code","dbaf8ff8":"code","aa9eed74":"code","64989e05":"code","61193b83":"code","77cc5438":"code","9aba8e7d":"code","21bc60da":"code","0a470108":"code","4844284b":"code","f83f734b":"code","691ce02f":"code","6f1a86e3":"code","df74b60e":"markdown","9824b235":"markdown","5f001350":"markdown","9d106baf":"markdown"},"source":{"920a6ea9":"%reset -sf","42961344":"import os, collections, random, itertools\n\nimport tqdm\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","a1788a77":"# for dirname, _, filenames in os.walk('\/kaggle\/input'): \n#     for filename in filenames: print(os.path.join(dirname, filename))","3ef80bd0":"# load data\n\ndf = pd.read_csv(\"\/kaggle\/input\/quora-question-pairs\/train.csv.zip\", dtype={'question1': str, 'question2': str})\ndf_hidden = pd.read_csv(\"\/kaggle\/input\/quora-question-pairs\/test.csv\", dtype={'question1': str, 'question2': str})\n\ndf[\"question1\"] = df[\"question1\"].astype(str)  # resolve nan\ndf[\"question2\"] = df[\"question2\"].astype(str)\ndf_hidden[\"question1\"] = df_hidden[\"question1\"].astype(str)  # resolve nan\ndf_hidden[\"question2\"] = df_hidden[\"question2\"].astype(str)\ndf[\"qid1\"] = df[\"qid1\"] - 1\ndf[\"qid2\"] = df[\"qid2\"] - 1\nmaxidx = max(max(df[\"qid1\"]), max(df[\"qid2\"])) + 1","9fca8448":"df_tfidf = pd.read_csv(\"\/kaggle\/input\/quora-question-pair-competition-tfidf\/train_tfidf.csv\")\ndf_hidden_tfidf = pd.read_csv(\"\/kaggle\/input\/quora-question-pair-competition-tfidf\/test_tfidf.csv\")\n\ndf[\"word_match\"] = df_tfidf[\"word_match\"]\ndf[\"tfidf_word_match\"] = df_tfidf[\"tfidf_word_match\"]\ndf_hidden[\"word_match\"] = df_hidden_tfidf[\"word_match\"]\ndf_hidden[\"tfidf_word_match\"] = df_hidden_tfidf[\"tfidf_word_match\"]","bf48aced":"df.sample(10)","e00fa93b":"## for internal test\n# df = df[:1234]\n# df_hidden = df_hidden[:5678]","2e3dd8b7":"from fuzzywuzzy import fuzz","a6ec6861":"def process_fuzz(df):\n    df[\"ratio\"] = [fuzz.ratio(question1,question2)\n        for question1, question2 in tqdm.tqdm(zip(df[\"question1\"], df[\"question2\"]))]\n    df[\"partial_ratio\"] = [fuzz.partial_ratio(question1,question2)\n        for question1, question2 in tqdm.tqdm(zip(df[\"question1\"], df[\"question2\"]))]\n    df[\"token_sort_ratio\"] = [fuzz.token_sort_ratio(question1,question2)\n        for question1, question2 in tqdm.tqdm(zip(df[\"question1\"], df[\"question2\"]))]\n    df[\"token_set_ratio\"] = [fuzz.token_set_ratio(question1,question2)\n        for question1, question2 in tqdm.tqdm(zip(df[\"question1\"], df[\"question2\"]))]\n    return df\n\ndf = process_fuzz(df)\ndf_hidden = process_fuzz(df_hidden)","a31ae503":"def process_text(df):\n    df[\"q1_length\"] = df[\"question1\"].str.len()\n    df[\"q2_length\"] = df[\"question2\"].str.len()\n    df[\"q1_spaces\"] = df[\"question1\"].str.count(\" \")  # words\n    df[\"q2_spaces\"] = df[\"question2\"].str.count(\" \")\n    df[\"q1_upper\"] = df['question1'].str.count(r'[A-Z]')\n    df[\"q2_upper\"] = df['question2'].str.count(r'[A-Z]')\n\n    return df.drop([\"question1\", \"question2\"], axis=1)\n\ndf = process_text(df)\ndf_hidden = process_text(df_hidden)","86d39dc2":"model_name = \"bert-base-nli-stsb-mean-tokens\"\nsentence_vectors = np.load(f\"..\/input\/quora-question-pairs-bert-sentence-vectors-hidden\/sentence_vectors_{model_name}.npy\")\n\nsentence_vectors_question1 = np.load(f\"..\/input\/quora-question-pairs-bert-sentence-vectors-hidden\/sentence_vectors_question1.npy\")\nsentence_vectors_question2 = np.load(f\"..\/input\/quora-question-pairs-bert-sentence-vectors-hidden\/sentence_vectors_question2.npy\")","d51173b1":"# %%time\n# from sklearn.decomposition import PCA\n\n# import pickle\n# with open('..\/input\/quora-question-pairs-bert-sentence-vectors-hidden\/pca.pkl', 'rb') as pickle_file:\n#     pca = pickle.load(pickle_file) \n\n# pca = PCA(n_components=2)\n# sentence_vectors = pca.fit_transform(sentence_vectors)\n# sentence_vectors_question1 = pca.transform(sentence_vectors_question1)\n# sentence_vectors_question2 = pca.transform(sentence_vectors_question2)\n# pca.explained_variance_ratio_","dbaf8ff8":"# q1_vec = [sentence_vectors[qid] for qid in df[\"qid1\"]]\n# q1_vec = np.transpose(np.array(q1_vec))\n# for i,v in enumerate(q1_vec):\n#     df[f\"q1v{i}\"] = v\n\n# q2_vec = [sentence_vectors[qid] for qid in df[\"qid2\"]]\n# q2_vec = np.transpose(np.array(q2_vec))\n# for i,v in enumerate(q2_vec):\n#     df[f\"q2v{i}\"] = v\n    \n# del q1_vec, q2_vec, sentence_vectors","aa9eed74":"# q1_vec = sentence_vectors_question1\n# q1_vec = np.transpose(np.array(q1_vec))\n# for i,v in enumerate(q1_vec):\n#     df_hidden[f\"q1v{i}\"] = v\n\n# q2_vec = sentence_vectors_question2\n# q2_vec = np.transpose(np.array(q2_vec))\n# for i,v in enumerate(q2_vec):\n#     df_hidden[f\"q2v{i}\"] = v\n    \n# del q1_vec, q2_vec\n# df_hidden","64989e05":"from scipy.spatial import distance\nq1_vecs = [sentence_vectors[qid] for qid in df[\"qid1\"]]\nq2_vecs = [sentence_vectors[qid] for qid in df[\"qid2\"]]\nq_sim = [distance.cosine(q1_vec, q2_vec) for q1_vec, q2_vec in tqdm.tqdm(zip(q1_vecs, q2_vecs))]\ndf[\"q_sim\"] = q_sim\n\nq1_vecs = sentence_vectors_question1\nq2_vecs = sentence_vectors_question2\nq_sim = [distance.cosine(q1_vec, q2_vec) for q1_vec, q2_vec in tqdm.tqdm(zip(q1_vecs, q2_vecs))]\ndf_hidden[\"q_sim\"] = q_sim","61193b83":"df_train = df.drop([\"id\", \"qid1\",\"qid2\", \"is_duplicate\"], axis=1)\ntarget_train = df[\"is_duplicate\"]","77cc5438":"import lightgbm as lgb\n\neval_set = np.array([True if i < len(df_train)*0.2 else False for i in range(len(df_train))])\nlgb_train = lgb.Dataset(df_train[~eval_set], target_train[~eval_set])\nlgb_eval = lgb.Dataset(df_train[eval_set], target_train[eval_set], reference=lgb_train)\nlgb_all = lgb.Dataset(df_train, target_train)","9aba8e7d":"params = {\n#     'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'scale_pos_weight': 0.360,\n#     'metric': {'auc'},\n#     'num_leaves': 15,\n#     'learning_rate': 0.05,\n#     'feature_fraction': 0.9,\n#     'bagging_fraction': 0.8,\n#     'bagging_freq': 5,\n    'verbose': -1,\n}\n\n\ngbm = lgb.train(params,\n                lgb_train,\n                num_boost_round=10000,\n                valid_sets=lgb_eval,\n                verbose_eval=200,\n                early_stopping_rounds=10)","21bc60da":"pd.DataFrame({\"feature\": df_train.columns, \"importance\": gbm.feature_importance(importance_type=\"gain\")})[:20]","0a470108":"pred_test = gbm.predict(df_hidden.drop([\"test_id\"], axis=1),                     \n    num_iteration=gbm.best_iteration)","4844284b":"plt.hist(pred_test)\nplt.show()","f83f734b":"df_submission = pd.DataFrame({\"test_id\": df_hidden[\"test_id\"], \"is_duplicate\":pred_test})\ndf_submission.to_csv(\"submission.csv\", index=False)\ndf_submission.shape","691ce02f":"fig, ax = plt.subplots(figsize=(14,4))\nsc = ax.scatter(df_hidden[\"tfidf_word_match\"], df_hidden[\"word_match\"], alpha=1, c=df_submission[\"is_duplicate\"])\nfig.colorbar(sc, ax=ax)\nplt.ylabel(\"word_match\")\nplt.xlabel(\"tfidf_word_match\")\nplt.show()","6f1a86e3":"!head submission.csv","df74b60e":"### Training","9824b235":"### Compute Augmented Features","5f001350":"### Include Sentence Vectors","9d106baf":"# Competition Attempt"}}