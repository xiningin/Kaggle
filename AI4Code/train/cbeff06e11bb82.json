{"cell_type":{"c2ea7af3":"code","43aaf145":"code","08645b92":"code","921d48f5":"code","83c5eee6":"code","6897aefa":"code","55fa2e7b":"code","d25ac4b8":"code","0bee6b5c":"code","e90a1e4c":"code","be8df182":"code","6912f1e1":"code","94a4739e":"code","3414fea6":"code","80ebac52":"code","aa3abdb6":"code","e3b0638e":"code","df2eabb2":"code","bb6b307f":"code","b5dbf0f2":"code","9e8b2601":"code","74df1a7c":"code","58ab8233":"code","4222ce4a":"code","cf698d5e":"code","e7fcb0d5":"markdown","40c86923":"markdown","c3cf53e9":"markdown","170d809e":"markdown","bc873e37":"markdown","89dd3f63":"markdown","24893387":"markdown","b040712f":"markdown","504441f3":"markdown","745815d4":"markdown","60bc4fd7":"markdown"},"source":{"c2ea7af3":"import pandas as pd \nimport numpy as np \n\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n%matplotlib inline \n\nimport tensorflow as tf ","43aaf145":"# taking all columns of training set only for data exploration\ndf_train=pd.read_csv('\/kaggle\/input\/stumbleupon\/train.tsv',sep='\\t')\n# taking boilerplate column as an input for the model beacuse only this column contain lot of high quality text data useful for our nlp task\ndf_test=pd.read_csv('\/kaggle\/input\/stumbleupon\/test.tsv',sep='\\t',usecols=['urlid','boilerplate'])\n","08645b92":"df_train.head()\n","921d48f5":"df_train.columns","83c5eee6":"df_train['alchemy_category'].value_counts()","6897aefa":"plt.figure(figsize=(15,10))\nsns.countplot(x=df_train['alchemy_category'],hue=df_train['label']);\nplt.xlabel('Category');\nplt.xticks(rotation=90);","55fa2e7b":"sns.countplot(x=df_train['label'])\n# This is a balanced dataset ","d25ac4b8":"df_train['boilerplate'].replace(to_replace=r'\"title\":', value=\"\",inplace=True,regex=True)\ndf_train['boilerplate'].replace(to_replace=r'\"url\":',value=\"\",inplace=True,regex=True)\n\ndf_train['boilerplate'].replace(to_replace=r'{|}',value=\"\",inplace=True,regex=True)\ndf_train['boilerplate']=df_train['boilerplate'].str.lower()\n\n\n#Cleaning the test dataframe \n\ndf_test['boilerplate'].replace(to_replace=r'\"title\":', value=\"\",inplace=True,regex=True)\ndf_test['boilerplate'].replace(to_replace=r'\"url\":',value=\"\",inplace=True,regex=True)\n\ndf_test['boilerplate'].replace(to_replace=r'{|}',value=\"\",inplace=True,regex=True)\ndf_test['boilerplate']=df_test['boilerplate'].str.lower()","0bee6b5c":"from transformers import AutoTokenizer\n\n\n#Downloading the tokenizer and the Albert model for fine tuning\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')","e90a1e4c":"#ADD all the variable for the Transformer model \n# because bert base uncased Model can only handle upto 512 tokens at a time\nSEQ_length=512\n\n#Lets create the X and Y matrix from the Df train set \n\nXids=np.zeros((df_train.shape[0],SEQ_length))\nXmask=np.zeros((df_train.shape[0],SEQ_length))\ny=np.zeros((df_train.shape[0],1))\n\n#Preparing the test dataframe\n\nXids_test=np.zeros((df_test.shape[0],SEQ_length))\nXmask_test=np.zeros((df_test.shape[0],SEQ_length))\nXids","be8df182":"for i,sequence in enumerate(df_train['boilerplate']):\n    tokens=tokenizer.encode_plus(sequence,max_length=SEQ_length,padding='max_length',add_special_tokens=True,\n                           truncation=True,return_token_type_ids=False,return_attention_mask=True,\n                           return_tensors='tf')\n    \n    Xids[i,:],Xmask[i,:],y[i,0]=tokens['input_ids'],tokens['attention_mask'],df_train.loc[i,'label']\n    \n\nfor i,sequence in enumerate(df_test['boilerplate']):\n    tokens=tokenizer.encode_plus(sequence,max_length=SEQ_length,padding='max_length',add_special_tokens=True,\n                           truncation=True,return_token_type_ids=False,return_attention_mask=True,\n                           return_tensors='tf')\n    \n    Xids_test[i,:],Xmask_test[i,:]=tokens['input_ids'],tokens['attention_mask']","6912f1e1":"Xids.shape","94a4739e":"#Check if the GPU is avalaible\ntf.config.get_visible_devices()","3414fea6":"dataset=tf.data.Dataset.from_tensor_slices((Xids,Xmask,y))\n\ndef map_func(input_ids,mask,labels):\n    return {'input_ids':input_ids,'attention_mask':mask},labels\n\ndataset=dataset.map(map_func)\ndataset=dataset.shuffle(100000).batch(32).prefetch(1000)\n\nDS_size=len(list(dataset))\n\ntrain=dataset.take(round(DS_size*0.85))\nval=dataset.skip(round(DS_size*0.85))","80ebac52":"#Preparing the test dataset\n\ndataset_test=tf.data.Dataset.from_tensor_slices((Xids_test,Xmask_test))\n\ndef map_func(input_ids,mask):\n    return {'input_ids':input_ids,'attention_mask':mask}\n\ndataset_test=dataset_test.map(map_func)\ndataset_test=dataset_test.batch(32).prefetch(1000)","aa3abdb6":"from transformers import TFDistilBertModel, DistilBertConfig\ndistil_bert = 'distilbert-base-uncased'\n\nconfig = DistilBertConfig(dropout=0.2, attention_dropout=0.2)\nconfig.output_hidden_states = False\ntransformer_model = TFDistilBertModel.from_pretrained(distil_bert, config = config)\n\ninput_ids_in = tf.keras.layers.Input(shape=(SEQ_length,), name='input_ids', dtype='int32')\ninput_masks_in = tf.keras.layers.Input(shape=(SEQ_length,), name='attention_mask', dtype='int32') \n\nembedding_layer = transformer_model(input_ids_in, attention_mask=input_masks_in)[0]\nX = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(50, return_sequences=True, dropout=0.1, recurrent_dropout=0.1))(embedding_layer)\nX = tf.keras.layers.GlobalMaxPool1D()(X)\nX = tf.keras.layers.Dense(50, activation='relu')(X)\nX = tf.keras.layers.Dropout(0.2)(X)\nX = tf.keras.layers.Dense(1, activation='sigmoid')(X)\nmodel = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs = X)\n\nfor layer in model.layers[:3]:\n  layer.trainable = False","e3b0638e":"model.summary()","df2eabb2":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n              optimizer='adam',metrics=[tf.keras.metrics.AUC(),tf.keras.metrics.Precision(),tf.keras.metrics.Recall()\n])","bb6b307f":"history=model.fit(train,validation_data=val,epochs=3)","b5dbf0f2":"predictions=model.predict(dataset_test)\ndf_test['label']=predictions\n\ndf_test.to_csv('submission.csv',columns=['urlid','label'],index=False)","9e8b2601":"input_x=tf.data.Dataset.from_tensor_slices((Xids,Xmask,y))\n\ndef map_func(input_ids,mask,labels):\n    return {'input_ids':input_ids,'attention_mask':mask}\n\ninput_x=input_x.map(map_func)\ninput_x=input_x.shuffle(100000).batch(32).prefetch(1000)\n\ny_true = y","74df1a7c":"y_true","58ab8233":"y_pred=model.predict(dataset)\ny_pred","4222ce4a":"y_pred = np.round(y_pred)\ny_pred","cf698d5e":"from sklearn import metrics\nprint(metrics.classification_report(y_true, y_pred))","e7fcb0d5":"# below is the Precision and Recall with loss and AUc","40c86923":"# Load the data ","c3cf53e9":"plz upvote the sourve notebook\nsource notebook of Stumble Upon Challenge AUC Private LB 0.85 from Sumeet Sawant  ","170d809e":"## Cleaning the boilerplate text\n\nLets remove the title and url word from each description . We will also lower case the words as we are planing to used a uncased version of Transformer model","bc873e37":"# Data Explorations \n\nThe dataset containes 27 columns and the end goal is predicting if the article is evergreen or non-evergreen. <br>\n","89dd3f63":"Decode the test data and see if urlid and text matches ","24893387":"# Precision and recall for each classes","b040712f":"# Build the model ","504441f3":"Alchemy catergory does have a role in determining the label for the article \n\nWe see that business, Recreation and health are more likley to be evergreen <br>\n\nWhere as sports computer_internet and arts and entertainment are more like to be non-evergreen. <br>","745815d4":"# Prediction ","60bc4fd7":"# Model Download from Hugging Face "}}