{"cell_type":{"47d6c111":"code","7fcc3100":"code","58b7181b":"code","6cab97a8":"code","ce06892e":"code","5419eaa2":"code","25987fd3":"code","7efb6068":"code","2217d759":"code","c60919a7":"code","6ba393cf":"code","7d0a4993":"code","a8127867":"code","3703073a":"code","5250900c":"code","b1b50acd":"code","60e098d4":"code","03788665":"code","1426e30f":"code","d0f4e93e":"code","358b126b":"code","be782b11":"code","d60ab889":"code","cd209feb":"code","b2973f7e":"code","d99b66d4":"code","e9a2273a":"code","04f43fd1":"code","2171f7ea":"code","7a3ed361":"code","7c80ab69":"code","4112152d":"code","c1814e71":"code","de85181f":"code","a9ae7c92":"code","9b1bf7f5":"code","0890f57a":"code","7b0bcb3c":"code","b1c09168":"code","4fb8f146":"code","0a3e3e31":"code","6a7f2221":"code","86cc2866":"code","b8779d92":"code","bd58a4b7":"code","6446a96e":"code","4f92865e":"code","23fe7723":"code","7865bafe":"code","e8670137":"code","e793013a":"code","83c6b8cc":"code","0b8a2b7e":"code","2265b324":"code","cb99173e":"code","e3bd0a30":"code","ab453485":"code","d091b7d2":"code","ee724ff6":"code","f065dace":"code","1a6d72c8":"code","86a8f997":"code","f918e2dd":"code","3f6881dc":"code","3ec167b0":"code","432efd9a":"code","85f3c30b":"code","af7953df":"code","6de43091":"code","0b34521d":"code","48f3310b":"code","e6aa5f8f":"code","a9ec8b1d":"code","a84dc540":"code","2976fcd9":"code","705e284d":"code","5d4c96e5":"code","3b1704b1":"code","be0f03fe":"code","525653f9":"code","781cd409":"code","5c8dab6b":"code","17a308be":"code","8f2ca7c3":"code","46d0753c":"code","410d304f":"code","60f0eb17":"code","9f6ce081":"code","c2f542dc":"code","7e05cd28":"code","3c40f202":"code","c528907e":"code","38283145":"code","c508db13":"markdown","c2e7dc55":"markdown","255932ea":"markdown","a86d6e5d":"markdown","3cbcb91b":"markdown","27177685":"markdown","7990d4a8":"markdown","09c1fd97":"markdown","64e8f009":"markdown","4bb33953":"markdown","27fdf173":"markdown","163eed46":"markdown","a761858c":"markdown","28bcf8e4":"markdown","8ca33988":"markdown","78125b7d":"markdown","d0111af6":"markdown","264ce09a":"markdown","31ff4dcb":"markdown","99f66fbe":"markdown","01ce452c":"markdown","0b1e9e90":"markdown","1ea35355":"markdown","f56b4587":"markdown","c9f14643":"markdown","186c3b63":"markdown","cc94ab55":"markdown","a4449e7c":"markdown","63d76e57":"markdown","e56c3df6":"markdown","479668a4":"markdown","f4b25261":"markdown","77e4a7ea":"markdown","45596774":"markdown","b730b3b0":"markdown","42a30012":"markdown","8d2599e0":"markdown","812c97e6":"markdown","cb419561":"markdown","b2372c85":"markdown","8452ceb9":"markdown","e3bf5c2d":"markdown","51d53bc9":"markdown","c2447eaa":"markdown","8a64f896":"markdown","77696da2":"markdown","ed54c0d2":"markdown","db878e3f":"markdown","b3e68708":"markdown","edc31fd8":"markdown","0506c342":"markdown","9914806c":"markdown","7c4b36dc":"markdown","265b6770":"markdown","0e155008":"markdown","07439ae5":"markdown","54561006":"markdown","2091f144":"markdown","1f0d8b07":"markdown","38367a65":"markdown","7dbc3cb1":"markdown"},"source":{"47d6c111":"import numpy as np \nimport pandas as pd \nimport os\nfrom time import time\nfrom sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom matplotlib import rcParams\n%matplotlib inline\nle = preprocessing.LabelEncoder()\nfrom numba import jit\nimport itertools\nfrom seaborn import countplot,lineplot, barplot\nfrom numba import jit\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn import preprocessing\nfrom scipy.stats import randint as sp_randint\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import LeaveOneGroupOut\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nimport matplotlib.style as style \nstyle.use('ggplot')\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport gc\ngc.enable()\n\n!ls ..\/input\/\n!ls ..\/input\/robots-best-submission\nprint (\"Ready !\")","7fcc3100":"data = pd.read_csv('..\/input\/career-con-2019\/X_train.csv')\ntr = pd.read_csv('..\/input\/career-con-2019\/X_train.csv')\nsub = pd.read_csv('..\/input\/career-con-2019\/sample_submission.csv')\ntest = pd.read_csv('..\/input\/career-con-2019\/X_test.csv')\ntarget = pd.read_csv('..\/input\/career-con-2019\/y_train.csv')\nprint (\"Data is ready !!\")","58b7181b":"data.head()","6cab97a8":"test.head()","ce06892e":"target.head()","5419eaa2":"len(data.measurement_number.value_counts())","25987fd3":"data.describe()","7efb6068":"test.describe()","2217d759":"target.describe()","c60919a7":"totalt = data.isnull().sum().sort_values(ascending=False)\npercent = (data.isnull().sum()\/data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([totalt, percent], axis=1, keys=['Total', 'Percent'])\nprint (\"Missing Data at Training\")\nmissing_data.tail()","6ba393cf":"totalt = test.isnull().sum().sort_values(ascending=False)\npercent = (test.isnull().sum()\/data.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([totalt, percent], axis=1, keys=['Total', 'Percent'])\nprint (\"Missing Data at Test\")\nmissing_data.tail()","7d0a4993":"print (\"Test has \", (test.shape[0]-data.shape[0])\/128, \"series more than Train (later I will prove it) = 768 registers\")\ndif = test.shape[0]-data.shape[0]\nprint (\"Let's check this extra 6 series\")\ntest.tail(768).describe()","a8127867":"target.groupby('group_id').surface.nunique().max()","3703073a":"target['group_id'].nunique()","5250900c":"sns.set(style='darkgrid')\nsns.countplot(y = 'surface',\n              data = target,\n              order = target['surface'].value_counts().index)\nplt.show()","b1b50acd":"fig, ax = plt.subplots(1,1,figsize=(26,8))\ntmp = pd.DataFrame(target.groupby(['group_id', 'surface'])['series_id'].count().reset_index())\nm = tmp.pivot(index='surface', columns='group_id', values='series_id')\ns = sns.heatmap(m, linewidths=.1, linecolor='black', annot=True, cmap=\"YlGnBu\")\ns.set_title('Number of surface category per group_id', size=16)\nplt.show()","60e098d4":"plt.figure(figsize=(23,5)) \nsns.set(style=\"darkgrid\")\ncountplot(x=\"group_id\", data=target, order = target['group_id'].value_counts().index)\nplt.show()","03788665":"serie1 = tr.head(128)\nserie1.head()","1426e30f":"serie1.describe()","d0f4e93e":"plt.figure(figsize=(26, 16))\nfor i, col in enumerate(serie1.columns[3:]):\n    plt.subplot(3, 4, i + 1)\n    plt.plot(serie1[col])\n    plt.title(col)","358b126b":"target.head(1)","be782b11":"del serie1\ngc.collect()","d60ab889":"series_dict = {}\nfor series in (data['series_id'].unique()):\n    series_dict[series] = data[data['series_id'] == series]  ","cd209feb":"def plotSeries(series_id):\n    style.use('ggplot')\n    plt.figure(figsize=(28, 16))\n    print(target[target['series_id'] == series_id]['surface'].values[0].title())\n    for i, col in enumerate(series_dict[series_id].columns[3:]):\n        if col.startswith(\"o\"):\n            color = 'red'\n        elif col.startswith(\"a\"):\n            color = 'green'\n        else:\n            color = 'blue'\n        if i >= 7:\n            i+=1\n        plt.subplot(3, 4, i + 1)\n        plt.plot(series_dict[series_id][col], color=color, linewidth=3)\n        plt.title(col)","b2973f7e":"id_series = 15\nplotSeries(id_series)","d99b66d4":"del series_dict\ngc.collect()","e9a2273a":"f,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(tr.iloc[:,3:].corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","04f43fd1":"f,ax = plt.subplots(figsize=(8, 8))\nsns.heatmap(test.iloc[:,3:].corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","2171f7ea":"!ls ..\/input","7a3ed361":"train_x = pd.read_csv('..\/input\/career-con-2019\/X_train.csv')\ntrain_y = pd.read_csv('..\/input\/career-con-2019\/y_train.csv')","7c80ab69":"import math\n\ndef prepare_data(t):\n    def f(d):\n        d=d.sort_values(by=['measurement_number'])\n        return pd.DataFrame({\n         'lx':[ d['linear_acceleration_X'].values ],\n         'ly':[ d['linear_acceleration_Y'].values ],\n         'lz':[ d['linear_acceleration_Z'].values ],\n         'ax':[ d['angular_velocity_X'].values ],\n         'ay':[ d['angular_velocity_Y'].values ],\n         'az':[ d['angular_velocity_Z'].values ],\n        })\n\n    t= t.groupby('series_id').apply(f)\n\n    def mfft(x):\n        return [ x\/math.sqrt(128.0) for x in np.absolute(np.fft.fft(x)) ][1:65]\n\n    t['lx_f']=[ mfft(x) for x in t['lx'].values ]\n    t['ly_f']=[ mfft(x) for x in t['ly'].values ]\n    t['lz_f']=[ mfft(x) for x in t['lz'].values ]\n    t['ax_f']=[ mfft(x) for x in t['ax'].values ]\n    t['ay_f']=[ mfft(x) for x in t['ay'].values ]\n    t['az_f']=[ mfft(x) for x in t['az'].values ]\n    return t","4112152d":"t=prepare_data(train_x)\nt=pd.merge(t,train_y[['series_id','surface','group_id']],on='series_id')\nt=t.rename(columns={\"surface\": \"y\"})","c1814e71":"def aggf(d, feature):\n    va= np.array(d[feature].tolist())\n    mean= sum(va)\/va.shape[0]\n    var= sum([ (va[i,:]-mean)**2 for i in range(va.shape[0]) ])\/va.shape[0]\n    dev= [ math.sqrt(x) for x in var ]\n    return pd.DataFrame({\n        'mean': [ mean ],\n        'dev' : [ dev ],\n    })\n\ndisplay={\n'hard_tiles_large_space':'r-.',\n'concrete':'g-.',\n'tiled':'b-.',\n\n'fine_concrete':'r-',\n'wood':'g-',\n'carpet':'b-',\n'soft_pvc':'y-',\n\n'hard_tiles':'r--',\n'soft_tiles':'g--',\n}","de85181f":"import matplotlib.pyplot as plt\nplt.figure(figsize=(14, 8*7))\n#plt.margins(x=0.0, y=0.0)\n#plt.tight_layout()\n# plt.figure()\n\nfeatures=['lx_f','ly_f','lz_f','ax_f','ay_f','az_f']\ncount=0\n\nfor feature in features:\n    stat= t.groupby('y').apply(aggf,feature)\n    stat.index= stat.index.droplevel(-1)\n    b=[*range(len(stat.at['carpet','mean']))]\n\n    count+=1\n    plt.subplot(len(features)+1,1,count)\n    for i,(k,v) in enumerate(display.items()):\n        plt.plot(b, stat.at[k,'mean'], v, label=k)\n        # plt.errorbar(b, stat.at[k,'mean'], yerr=stat.at[k,'dev'], fmt=v)\n   \n    leg = plt.legend(loc='best', ncol=3, mode=\"expand\", shadow=True, fancybox=True)\n    plt.title(\"sensor: \" + feature)\n    plt.xlabel(\"frequency component\")\n    plt.ylabel(\"amplitude\")\n\ncount+=1\nplt.subplot(len(features)+1,1,count)\nk='concrete'\nv=display[k]\nfeature='lz_f'\nstat= t.groupby('y').apply(aggf,feature)\nstat.index= stat.index.droplevel(-1)\nb=[*range(len(stat.at['carpet','mean']))]\n\nplt.errorbar(b, stat.at[k,'mean'], yerr=stat.at[k,'dev'], fmt=v)\nplt.title(\"sample for error bars (lz_f, surface concrete)\")\nplt.xlabel(\"frequency component\")\nplt.ylabel(\"amplitude\")\n\nplt.show()","a9ae7c92":"del train_x, train_y\ngc.collect()","9b1bf7f5":"def plot_feature_distribution(df1, df2, label1, label2, features,a=2,b=5):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(a,b,figsize=(17,9))\n\n    for feature in features:\n        i += 1\n        plt.subplot(a,b,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","0890f57a":"features = data.columns.values[3:]\nplot_feature_distribution(data, test, 'train', 'test', features)","7b0bcb3c":"def plot_feature_class_distribution(classes,tt, features,a=5,b=2):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(a,b,figsize=(16,24))\n\n    for feature in features:\n        i += 1\n        plt.subplot(a,b,i)\n        for clas in classes:\n            ttc = tt[tt['surface']==clas]\n            sns.kdeplot(ttc[feature], bw=0.5,label=clas)\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","b1c09168":"classes = (target['surface'].value_counts()).index\naux = data.merge(target, on='series_id', how='inner')\nplot_feature_class_distribution(classes, aux, features)","4fb8f146":"plt.figure(figsize=(26, 16))\nfor i,col in enumerate(aux.columns[3:13]):\n    ax = plt.subplot(3,4,i+1)\n    ax = plt.title(col)\n    for surface in classes:\n        surface_feature = aux[aux['surface'] == surface]\n        sns.kdeplot(surface_feature[col], label = surface)","0a3e3e31":"plt.figure(figsize=(26, 16))\nfor i, col in enumerate(data.columns[3:]):\n    ax = plt.subplot(3, 4, i + 1)\n    sns.distplot(data[col], bins=100, label='train')\n    sns.distplot(test[col], bins=100, label='test')\n    ax.legend()   ","6a7f2221":"# https:\/\/stackoverflow.com\/questions\/53033620\/how-to-convert-euler-angles-to-quaternions-and-get-the-same-euler-angles-back-fr?rq=1\ndef quaternion_to_euler(x, y, z, w):\n    import math\n    t0 = +2.0 * (w * x + y * z)\n    t1 = +1.0 - 2.0 * (x * x + y * y)\n    X = math.atan2(t0, t1)\n\n    t2 = +2.0 * (w * y - z * x)\n    t2 = +1.0 if t2 > +1.0 else t2\n    t2 = -1.0 if t2 < -1.0 else t2\n    Y = math.asin(t2)\n\n    t3 = +2.0 * (w * z + x * y)\n    t4 = +1.0 - 2.0 * (y * y + z * z)\n    Z = math.atan2(t3, t4)\n\n    return X, Y, Z","86cc2866":"def fe_step0 (actual):\n    \n    # https:\/\/www.mathworks.com\/help\/aeroblks\/quaternionnorm.html\n    # https:\/\/www.mathworks.com\/help\/aeroblks\/quaternionmodulus.html\n    # https:\/\/www.mathworks.com\/help\/aeroblks\/quaternionnormalize.html\n    \n    # Spoiler: you don't need this ;)\n    \n    actual['norm_quat'] = (actual['orientation_X']**2 + actual['orientation_Y']**2 + actual['orientation_Z']**2 + actual['orientation_W']**2)\n    actual['mod_quat'] = (actual['norm_quat'])**0.5\n    actual['norm_X'] = actual['orientation_X'] \/ actual['mod_quat']\n    actual['norm_Y'] = actual['orientation_Y'] \/ actual['mod_quat']\n    actual['norm_Z'] = actual['orientation_Z'] \/ actual['mod_quat']\n    actual['norm_W'] = actual['orientation_W'] \/ actual['mod_quat']\n    \n    return actual","b8779d92":"data = fe_step0(data)\ntest = fe_step0(test)\nprint(data.shape)\ndata.head()","bd58a4b7":"fig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols=4, figsize=(18, 5))\n\nax1.set_title('quaternion X')\nsns.kdeplot(data['norm_X'], ax=ax1, label=\"train\")\nsns.kdeplot(test['norm_X'], ax=ax1, label=\"test\")\n\nax2.set_title('quaternion Y')\nsns.kdeplot(data['norm_Y'], ax=ax2, label=\"train\")\nsns.kdeplot(test['norm_Y'], ax=ax2, label=\"test\")\n\nax3.set_title('quaternion Z')\nsns.kdeplot(data['norm_Z'], ax=ax3, label=\"train\")\nsns.kdeplot(test['norm_Z'], ax=ax3, label=\"test\")\n\nax4.set_title('quaternion W')\nsns.kdeplot(data['norm_W'], ax=ax4, label=\"train\")\nsns.kdeplot(test['norm_W'], ax=ax4, label=\"test\")\n\nplt.show()","6446a96e":"def fe_step1 (actual):\n    \"\"\"Quaternions to Euler Angles\"\"\"\n    \n    x, y, z, w = actual['norm_X'].tolist(), actual['norm_Y'].tolist(), actual['norm_Z'].tolist(), actual['norm_W'].tolist()\n    nx, ny, nz = [], [], []\n    for i in range(len(x)):\n        xx, yy, zz = quaternion_to_euler(x[i], y[i], z[i], w[i])\n        nx.append(xx)\n        ny.append(yy)\n        nz.append(zz)\n    \n    actual['euler_x'] = nx\n    actual['euler_y'] = ny\n    actual['euler_z'] = nz\n    return actual","4f92865e":"data = fe_step1(data)\ntest = fe_step1(test)\nprint (data.shape)\ndata.head()","23fe7723":"fig, (ax1, ax2, ax3) = plt.subplots(ncols=3, figsize=(15, 5))\n\nax1.set_title('Roll')\nsns.kdeplot(data['euler_x'], ax=ax1, label=\"train\")\nsns.kdeplot(test['euler_x'], ax=ax1, label=\"test\")\n\nax2.set_title('Pitch')\nsns.kdeplot(data['euler_y'], ax=ax2, label=\"train\")\nsns.kdeplot(test['euler_y'], ax=ax2, label=\"test\")\n\nax3.set_title('Yaw')\nsns.kdeplot(data['euler_z'], ax=ax3, label=\"train\")\nsns.kdeplot(test['euler_z'], ax=ax3, label=\"test\")\n\nplt.show()","7865bafe":"data.head()","e8670137":"def feat_eng(data):\n    \n    df = pd.DataFrame()\n    data['totl_anglr_vel'] = (data['angular_velocity_X']**2 + data['angular_velocity_Y']**2 + data['angular_velocity_Z']**2)** 0.5\n    data['totl_linr_acc'] = (data['linear_acceleration_X']**2 + data['linear_acceleration_Y']**2 + data['linear_acceleration_Z']**2)**0.5\n    data['totl_xyz'] = (data['orientation_X']**2 + data['orientation_Y']**2 + data['orientation_Z']**2)**0.5\n    data['acc_vs_vel'] = data['totl_linr_acc'] \/ data['totl_anglr_vel']\n    \n    def mean_change_of_abs_change(x):\n        return np.mean(np.diff(np.abs(np.diff(x))))\n    \n    for col in data.columns:\n        if col in ['row_id','series_id','measurement_number']:\n            continue\n        df[col + '_mean'] = data.groupby(['series_id'])[col].mean()\n        df[col + '_median'] = data.groupby(['series_id'])[col].median()\n        df[col + '_max'] = data.groupby(['series_id'])[col].max()\n        df[col + '_min'] = data.groupby(['series_id'])[col].min()\n        df[col + '_std'] = data.groupby(['series_id'])[col].std()\n        df[col + '_range'] = df[col + '_max'] - df[col + '_min']\n        df[col + '_maxtoMin'] = df[col + '_max'] \/ df[col + '_min']\n        df[col + '_mean_abs_chg'] = data.groupby(['series_id'])[col].apply(lambda x: np.mean(np.abs(np.diff(x))))\n        df[col + '_mean_change_of_abs_change'] = data.groupby('series_id')[col].apply(mean_change_of_abs_change)\n        df[col + '_abs_max'] = data.groupby(['series_id'])[col].apply(lambda x: np.max(np.abs(x)))\n        df[col + '_abs_min'] = data.groupby(['series_id'])[col].apply(lambda x: np.min(np.abs(x)))\n        df[col + '_abs_avg'] = (df[col + '_abs_min'] + df[col + '_abs_max'])\/2\n    return df\n    ","e793013a":"%%time\ndata = feat_eng(data)\ntest = feat_eng(test)\nprint (\"New features: \",data.shape)","83c6b8cc":"data.head()","0b8a2b7e":"from scipy.stats import kurtosis\nfrom scipy.stats import skew\n\ndef _kurtosis(x):\n    return kurtosis(x)\n\ndef CPT5(x):\n    den = len(x)*np.exp(np.std(x))\n    return sum(np.exp(x))\/den\n\ndef skewness(x):\n    return skew(x)\n\ndef SSC(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1 \n    xn_i1 = x[0:len(x)-2]  # xn-1\n    ans = np.heaviside((xn-xn_i1)*(xn-xn_i2),0)\n    return sum(ans[1:]) \n\ndef wave_length(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1 \n    return sum(abs(xn_i2-xn))\n    \ndef norm_entropy(x):\n    tresh = 3\n    return sum(np.power(abs(x),tresh))\n\ndef SRAV(x):    \n    SRA = sum(np.sqrt(abs(x)))\n    return np.power(SRA\/len(x),2)\n\ndef mean_abs(x):\n    return sum(abs(x))\/len(x)\n\ndef zero_crossing(x):\n    x = np.array(x)\n    x = np.append(x[-1], x)\n    x = np.append(x,x[1])\n    xn = x[1:len(x)-1]\n    xn_i2 = x[2:len(x)]    # xn+1\n    return sum(np.heaviside(-xn*xn_i2,0))","2265b324":"def fe_advanced_stats(data):\n    \n    df = pd.DataFrame()\n    \n    for col in data.columns:\n        if col in ['row_id','series_id','measurement_number']:\n            continue\n        if 'orientation' in col:\n            continue\n            \n        print (\"FE on column \", col, \"...\")\n        \n        df[col + '_skew'] = data.groupby(['series_id'])[col].skew()\n        df[col + '_mad'] = data.groupby(['series_id'])[col].mad()\n        df[col + '_q25'] = data.groupby(['series_id'])[col].quantile(0.25)\n        df[col + '_q75'] = data.groupby(['series_id'])[col].quantile(0.75)\n        df[col + '_q95'] = data.groupby(['series_id'])[col].quantile(0.95)\n        df[col + '_iqr'] = df[col + '_q75'] - df[col + '_q25']\n        df[col + '_CPT5'] = data.groupby(['series_id'])[col].apply(CPT5) \n        df[col + '_SSC'] = data.groupby(['series_id'])[col].apply(SSC) \n        df[col + '_skewness'] = data.groupby(['series_id'])[col].apply(skewness)\n        df[col + '_wave_lenght'] = data.groupby(['series_id'])[col].apply(wave_length)\n        df[col + '_norm_entropy'] = data.groupby(['series_id'])[col].apply(norm_entropy)\n        df[col + '_SRAV'] = data.groupby(['series_id'])[col].apply(SRAV)\n        df[col + '_kurtosis'] = data.groupby(['series_id'])[col].apply(_kurtosis) \n        df[col + '_zero_crossing'] = data.groupby(['series_id'])[col].apply(zero_crossing) \n        \n    return df","cb99173e":"basic_fe = ['linear_acceleration_X','linear_acceleration_Y','linear_acceleration_Z',\n           'angular_velocity_X','angular_velocity_Y','angular_velocity_Z']","e3bd0a30":"def fe_plus (data):\n    \n    aux = pd.DataFrame()\n    \n    for serie in data.index:\n        #if serie%500 == 0: print (\"> Serie = \",serie)\n        \n        aux = X_train[X_train['series_id']==serie]\n        \n        for col in basic_fe:\n            data.loc[serie,col + '_unq'] = aux[col].round(3).nunique()\n            data.loc[serie,col + 'ratio_unq'] = aux[col].round(3).nunique()\/18\n            try:\n                data.loc[serie,col + '_freq'] = aux[col].value_counts().idxmax()\n            except:\n                data.loc[serie,col + '_freq'] = 0\n            \n            data.loc[serie,col + '_max_freq'] = aux[aux[col] == aux[col].max()].shape[0]\n            data.loc[serie,col + '_min_freq'] = aux[aux[col] == aux[col].min()].shape[0]\n            data.loc[serie,col + '_pos_freq'] = aux[aux[col] >= 0].shape[0]\n            data.loc[serie,col + '_neg_freq'] = aux[aux[col] < 0].shape[0]\n            data.loc[serie,col + '_nzeros'] = (aux[col]==0).sum(axis=0)","ab453485":"#https:\/\/stackoverflow.com\/questions\/17778394\/list-highest-correlation-pairs-from-a-large-correlation-matrix-in-pandas\ncorr_matrix = data.corr().abs()\nraw_corr = data.corr()\n\nsol = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n                 .stack()\n                 .sort_values(ascending=False))\ntop_corr = pd.DataFrame(sol).reset_index()\ntop_corr.columns = [\"var1\", \"var2\", \"abs corr\"]\n# with .abs() we lost the sign, and it's very important.\nfor x in range(len(top_corr)):\n    var1 = top_corr.iloc[x][\"var1\"]\n    var2 = top_corr.iloc[x][\"var2\"]\n    corr = raw_corr[var1][var2]\n    top_corr.at[x, \"raw corr\"] = corr","d091b7d2":"top_corr.head(15)","ee724ff6":"data.fillna(0,inplace=True)\ntest.fillna(0,inplace=True)\ndata.replace(-np.inf,0,inplace=True)\ndata.replace(np.inf,0,inplace=True)\ntest.replace(-np.inf,0,inplace=True)\ntest.replace(np.inf,0,inplace=True)","f065dace":"target.head()","1a6d72c8":"target['surface'] = le.fit_transform(target['surface'])","86a8f997":"target['surface'].value_counts()","f918e2dd":"target.head()","3f6881dc":"folds = StratifiedKFold(n_splits=10, shuffle=True, random_state=59)","3ec167b0":"predicted = np.zeros((test.shape[0],9))\nmeasured= np.zeros((data.shape[0]))\nscore = 0","432efd9a":"for times, (trn_idx, val_idx) in enumerate(folds.split(data.values,target['surface'].values)):\n    model = RandomForestClassifier(n_estimators=500, n_jobs = -1)\n    #model = RandomForestClassifier(n_estimators=500, max_depth=10, min_samples_split=5, n_jobs=-1)\n    model.fit(data.iloc[trn_idx],target['surface'][trn_idx])\n    measured[val_idx] = model.predict(data.iloc[val_idx])\n    predicted += model.predict_proba(test)\/folds.n_splits\n    score += model.score(data.iloc[val_idx],target['surface'][val_idx])\n    print(\"Fold: {} score: {}\".format(times,model.score(data.iloc[val_idx],target['surface'][val_idx])))\n\n    importances = model.feature_importances_\n    indices = np.argsort(importances)\n    features = data.columns\n    \n    if model.score(data.iloc[val_idx],target['surface'][val_idx]) > 0.92000:\n        hm = 30\n        plt.figure(figsize=(7, 10))\n        plt.title('Feature Importances')\n        plt.barh(range(len(indices[:hm])), importances[indices][:hm], color='b', align='center')\n        plt.yticks(range(len(indices[:hm])), [features[i] for i in indices])\n        plt.xlabel('Relative Importance')\n        plt.show()\n\n    gc.collect()","85f3c30b":"print('Avg Accuracy RF', score \/ folds.n_splits)","af7953df":"confusion_matrix(measured,target['surface'])","6de43091":"# https:\/\/www.kaggle.com\/artgor\/where-do-the-robots-drive\n\ndef plot_confusion_matrix(truth, pred, classes, normalize=False, title=''):\n    cm = confusion_matrix(truth, pred)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n    plt.title('Confusion matrix', size=15)\n    plt.colorbar(fraction=0.046, pad=0.04)\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.grid(False)\n    plt.tight_layout()","0b34521d":"plot_confusion_matrix(target['surface'], measured, le.classes_)","48f3310b":"sub['surface'] = le.inverse_transform(predicted.argmax(axis=1))\nsub.to_csv('submission.csv', index=False)\nsub.head()","e6aa5f8f":"best_sub = pd.read_csv('..\/input\/robots-best-submission\/final_submission.csv')\nbest_sub.to_csv('best_submission.csv', index=False)\nbest_sub.head(10)","a9ec8b1d":"sub073 = pd.read_csv('..\/input\/robots-best-submission\/mybest0.73.csv')\nsub072 = pd.read_csv('..\/input\/robots-best-submission\/sub_0.72.csv')\nsub072_2 = pd.read_csv('..\/input\/robots-best-submission\/sub_0.72_2.csv')\nsub071 = pd.read_csv('..\/input\/robots-best-submission\/sub_0.71.csv')\nsub06 = pd.read_csv('..\/input\/robots-best-submission\/sub_0.6.csv')\n\nsub073 = sub073.rename(columns = {'surface':'surface073'})\nsub072 = sub072.rename(columns = {'surface':'surface072'})\nsub072_2 = sub072_2.rename(columns = {'surface':'surface072_2'})\nsub071 = sub071.rename(columns = {'surface':'surface071'})\nsub06 = sub06.rename(columns = {'surface':'surface06'})\nprint (\"Submission data is ready\")","a84dc540":"sub073.head()","2976fcd9":"subtest = pd.concat([sub073['series_id'], sub073['surface073'], sub072['surface072'], sub071['surface071'], sub06['surface06']], axis=1)\nsubtest.head()","705e284d":"differents = []\nfor i in range (0,subtest.shape[0]): \n    labels = list(subtest.iloc[i,1:])\n    result = len(set(labels))>1\n    if result:\n        differents.append((i, str(labels)))\n        \ndifferents = pd.DataFrame(differents, columns=['idx','group']) \ndifferents.head()","5d4c96e5":"differents['group'].nunique()","3b1704b1":"differents['count'] = differents.groupby('group')['group'].transform('count')\ndifferents = differents.sort_values(by=['count'], ascending=False)\ndifferents = differents.drop(['idx'],axis=1)\ndifferents = differents.drop_duplicates()","be0f03fe":"differents.head(10)","525653f9":"differents.tail(10)","781cd409":"from numpy.fft import *\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style \nstyle.use('ggplot')","5c8dab6b":"X_train = pd.read_csv('..\/input\/career-con-2019\/X_train.csv')\nX_test = pd.read_csv('..\/input\/career-con-2019\/X_test.csv')\ntarget = pd.read_csv('..\/input\/career-con-2019\/y_train.csv')","17a308be":"series_dict = {}\nfor series in (X_train['series_id'].unique()):\n    series_dict[series] = X_train[X_train['series_id'] == series] ","8f2ca7c3":"# From: Code Snippet For Visualizing Series Id by @shaz13\ndef plotSeries(series_id):\n    style.use('ggplot')\n    plt.figure(figsize=(28, 16))\n    print(target[target['series_id'] == series_id]['surface'].values[0].title())\n    for i, col in enumerate(series_dict[series_id].columns[3:]):\n        if col.startswith(\"o\"):\n            color = 'red'\n        elif col.startswith(\"a\"):\n            color = 'green'\n        else:\n            color = 'blue'\n        if i >= 7:\n            i+=1\n        plt.subplot(3, 4, i + 1)\n        plt.plot(series_dict[series_id][col], color=color, linewidth=3)\n        plt.title(col)","46d0753c":"plotSeries(1)","410d304f":"# from @theoviel at https:\/\/www.kaggle.com\/theoviel\/fast-fourier-transform-denoising\ndef filter_signal(signal, threshold=1e3):\n    fourier = rfft(signal)\n    frequencies = rfftfreq(signal.size, d=20e-3\/signal.size)\n    fourier[frequencies > threshold] = 0\n    return irfft(fourier)","60f0eb17":"X_train_denoised = X_train.copy()\nX_test_denoised = X_test.copy()\n\n# train\nfor col in X_train.columns:\n    if col[0:3] == 'ang' or col[0:3] == 'lin':\n        # Apply filter_signal function to the data in each series\n        denoised_data = X_train.groupby(['series_id'])[col].apply(lambda x: filter_signal(x))\n        \n        # Assign the denoised data back to X_train\n        list_denoised_data = []\n        for arr in denoised_data:\n            for val in arr:\n                list_denoised_data.append(val)\n                \n        X_train_denoised[col] = list_denoised_data\n        \n# test\nfor col in X_test.columns:\n    if col[0:3] == 'ang' or col[0:3] == 'lin':\n        # Apply filter_signal function to the data in each series\n        denoised_data = X_test.groupby(['series_id'])[col].apply(lambda x: filter_signal(x))\n        \n        # Assign the denoised data back to X_train\n        list_denoised_data = []\n        for arr in denoised_data:\n            for val in arr:\n                list_denoised_data.append(val)\n                \n        X_test_denoised[col] = list_denoised_data","9f6ce081":"series_dict = {}\nfor series in (X_train_denoised['series_id'].unique()):\n    series_dict[series] = X_train_denoised[X_train_denoised['series_id'] == series] ","c2f542dc":"plotSeries(1)","7e05cd28":"plt.figure(figsize=(24, 8))\nplt.title('linear_acceleration_X')\nplt.plot(X_train.angular_velocity_Z[128:256], label=\"original\");\nplt.plot(X_train_denoised.angular_velocity_Z[128:256], label=\"denoised\");\nplt.legend()\nplt.show()","3c40f202":"X_train_denoised.head()","c528907e":"X_test_denoised.head()","38283145":"X_train_denoised.to_csv('test_denoised.csv', index=False)\nX_test_denoised.to_csv('train_denoised.csv', index=False)","c508db13":"### Best Submission","c2e7dc55":"**use random_state at Random Forest**\n\nif you don't use random_state you will get a different solution everytime, sometimes you will be lucky, but other times you will lose your time comparing.","255932ea":"**So, we have 3810 train series, and 3816 test series.\nLet's engineer some features!**\n\n## Example: Series 1\n\nLet's have a look at the values of features in a single time-series, for example series 1  ```series_id=0```\n\nClick to see all measurements of the **first series** ","a86d6e5d":"## New advanced features","3cbcb91b":"**I will analyze my best submissions in order to find something interesting.**\n\nPlease, feel free to optimize this code.","27177685":"**73 groups**\n**Each group_id is a unique recording session and has only one surface type **","7990d4a8":"**Correlations test (click \"code\")** ","09c1fd97":"**Euler angles** are really important, and we have a problem with Z.\n\n### Why Orientation_Z (euler angle Z) is so important?\n\nWe have a robot moving around, imagine a robot moving straight through different surfaces (each with different features), for example concrete and hard tile floor. Our robot can can **bounce** or **balance** itself a little bit on if the surface is not flat and smooth, that's why we need to work with quaternions and take care of orientation_Z.\n\n![](https:\/\/lifeboat.com\/blog.images\/robot-car-find-share-on-giphy.gif.gif)","64e8f009":"**Normal distribution**\n\nThere are obviously differences between *surfaces* and that's good, we will focus on that in order to classify them better.\n\nKnowing this differences and that variables follow a normal distribution (in most of the cases) we need to add new features like: ```mean, std, median, range ...``` (for each variable).\n\nHowever, I will try to fix *orientation_X* and *orientation_Y* as I explained before, scaling and normalizing data.\n\n---\n\n### Now with a new scale (more more precision)","4bb33953":"### There is missing data in test and train data","27fdf173":"### Target feature - surface and group_id distribution\nLet's show now the distribution of target feature - surface and group_id.\nby @gpreda.","163eed46":"![](https:\/\/d2gne97vdumgn3.cloudfront.net\/api\/file\/UMYT4v0TyIgtyGm8ZXDQ)","a761858c":"**by [@ninoko](https:\/\/www.kaggle.com\/ninoko)**\n\nI've probed the public leaderboard and this is what I got\nThere are much less surfaces like wood or tiled, and much more soft and hard tiles in public leaderboard. This can be issue, why CV and LB results differ strangely.\n\n![graph](https:\/\/i.imgur.com\/DoFc3mW.png)","28bcf8e4":"- Frequency of the max value\n- Frequency of the min value\n- Count Positive values\n- Count Negative values\n- Count zeros","8ca33988":"# Run Model","78125b7d":"Each series has 128 measurements. \n\n**1 serie = 128 measurements**. \n\nFor example, serie with series_id=0 has a surface = *fin_concrete* and 128 measurements.","d0111af6":"<br>\n### Correlations (Part I)","264ce09a":"**X_[train\/test].csv** - the input data, covering 10 sensor channels and 128 measurements per time series plus three ID columns:\n\n- ```row_id```: The ID for this row.\n\n- ```series_id: ID``` number for the measurement series. Foreign key to y_train\/sample_submission.\n\n- ```measurement_number```: Measurement number within the series.\n\nThe orientation channels encode the current angles how the robot is oriented as a quaternion (see Wikipedia). Angular velocity describes the angle and speed of motion, and linear acceleration components describe how the speed is changing at different times. The 10 sensor channels are:\n\n```\norientation_X\n\norientation_Y\n\norientation_Z\n\norientation_W\n\nangular_velocity_X\n\nangular_velocity_Y\n\nangular_velocity_Z\n\nlinear_acceleration_X\n\nlinear_acceleration_Y\n\nlinear_acceleration_Z\n```\n\n**y_train.csv** - the surfaces for training set.\n\n- ```series_id```: ID number for the measurement series.\n\n- ```group_id```: ID number for all of the measurements taken in a recording session. Provided for the training set only, to enable more cross validation strategies.\n\n- ```surface```: the target for this competition.\n\n**sample_submission.csv** - a sample submission file in the correct format.","31ff4dcb":"Well, this is immportant, there is a **strong correlation** between:\n- angular_velocity_Z and angular_velocity_Y\n- orientation_X and orientation_Y\n- orientation_Y and orientation_Z\n\nMoreover, test has different correlations than training, for example:\n\n- angular_velocity_Z and orientation_X: -0.1(training) and 0.1(test). Anyway, is too small in both cases, it should not be a problem.","99f66fbe":"**Validation Strategy: Stratified KFold**","01ce452c":"### describe (basic stats)","0b1e9e90":"### Confusion Matrix Plot","1ea35355":"### Load data","f56b4587":"**Useful functions**","c9f14643":"### Load packages","186c3b63":"Orientation - quaternion coordinates\nYou could notice that there are 4 coordinates: X, Y, Z, W.\n\nUsually we have X, Y, Z - Euler Angles. But Euler Angles are limited by a phenomenon called \"gimbal lock,\" which prevents them from measuring orientation when the pitch angle approaches +\/- 90 degrees. Quaternions provide an alternative measurement technique that does not suffer from gimbal lock. Quaternions are less intuitive than Euler Angles and the math can be a little more complicated.\n\nHere are some articles about it:\n\nhttp:\/\/www.chrobotics.com\/library\/understanding-quaternions\n\nhttp:\/\/www.tobynorris.com\/work\/prog\/csharp\/quatview\/help\/orientations_and_quaternions.htm\n\nBasically 3D coordinates are converted to 4D vectors.","cc94ab55":"## Next Step ??\n\n- I will create a test dataset with those special cases and then I will ad a new CV stage where I will try to classify those surfaces correctly.\n- I will look for surfaces distinctive features.","a4449e7c":"# Data exploration","63d76e57":"## Is it an Humanoid Robot instead of a car?\n\n![](https:\/\/media1.giphy.com\/media\/on7ipUR0rFjRS\/giphy.gif)\n\n**Acceleration**\n- X (mean at 0)\n- Y axis is centered at a value wich shows us the movement (straight ).\n- Z axis is centered at 10 (+- 9.8) wich is the gravity !! , you can see how the robot bounds.\n\nAngular velocity (X,Y,Z) has mean (0,0,0) so there is no lineal movement on those axis (measured with an encoder or potentiometer)\n\n**Fourier**\n\nWe can see: with a frequency 3 Hz we can see an acceleration, I think that acceleration represents one step.\nMaybe ee can suppose that every step is caused by many different movements, that's why there are different accelerations at different frequencies.\n\nAngular velocity represents spins. \nEvery time the engine\/servo spins, the robot does an step - relation between acc y vel.","e56c3df6":"\n> *Are there any reasons to not automatically normalize a quaternion? And if there are, what quaternion operations do result in non-normalized quaternions?*\n\nAny operation that produces a quaternion will need to be normalized because floating-point precession errors will cause it to not be unit length.\nI would advise against standard routines performing normalization automatically for performance reasons. \nAny competent programmer should be aware of the precision issues and be able to normalize the quantities when necessary - and it is not always necessary to have a unit length quaternion.\nThe same is true for vector operations.\n\nsource: https:\/\/stackoverflow.com\/questions\/11667783\/quaternion-and-normalization","479668a4":"## Label encoding","f4b25261":"If we look at the features: orientation, angular velocity and linear acceleration, we can see big differences between **max** and **min** from entire test vs 6 extra test's series (see **linear_acceleration_Z**).\n\nObviously we are comparing 3810 series vs 6 series so this is not a big deal.","77e4a7ea":"Let's denoise train and test angular_velocity and linear_acceleration data","45596774":"## Step 1: (x, y, z, w) -> (x,y,z)   quaternions to euler angles","b730b3b0":"In this example, we can see a quite interesting performance:\n1. Orientation X increases\n2. Orientation Y decreases\n3. We don't see any kind of pattern except for linear_acceleration_Y\n\nAnd we know that in this series, the robot moved throuh \"fine_concrete\".","42a30012":"### Filling missing NAs and infinite data \u221e  by zeroes 0","8d2599e0":"For example the serie with **series_id = 2** has the following predicition:\n\n```\n['tiled', 'tiled', 'tiled', 'fine_concrete']\n```\n\nThis means that my best submissions (*0.73, 0.72 and  0.71 LB* ) predicted the same: **tiled**, but a worst submission (*0.6 LB*) would have predicted **fine_concrete**.\n\n---\n\n### So... Why is this interesting?\n\nIn order to improve our classification, LB is indicating us wich kind of surfaces are confused with others.\nIn that example, ```tiled``` and ```fine_concrete``` are being **confused** (maybe because the two surfaces are **alike**)\n\n---\n\nAs you can see bellow, we have **177 cases of confusion**\nI'm going to plot the tp 10% and see what happens.","812c97e6":"We can see that **wood** and **fine_concrete** are really hard to guess.","cb419561":"### Important !\nAs you can see in this kernel https:\/\/www.kaggle.com\/anjum48\/leakage-within-the-train-dataset\n\nAs discussed in the discussion forums (https:\/\/www.kaggle.com\/c\/career-con-2019\/discussion\/87239#latest-508136) it looks as if each series is part of longer aquisition periods that have been cut up into chunks with 128 samples.\n\nThis means that each series is not truely independent and there is leakage between them via the orientation data. Therefore if you have any features that use orientation, you will get a very high CV score due to this leakage in the train set.\n\n[This kernel](https:\/\/www.kaggle.com\/anjum48\/leakage-within-the-train-dataset) will show you how it is possible to get a CV score of 0.992 using only the **orientation data**.\n\n---\n\n**So I recommend not to use orientation information**","b2372c85":"## Step 2: + Basic features","8452ceb9":"This kernel [distribution hack](https:\/\/www.kaggle.com\/donkeys\/distribution-hack) by [@donkeys](https:\/\/www.kaggle.com\/donkeys) simply produces 9 output files, one for each target category. \nI submitted each of these to the competition to see how much of each target type exists in the test set distribution. Results:\n\n- carpet 0.06\n- concrete 0.16\n- fine concrete 0.09\n- hard tiles 0.06\n- hard tiles large space 0.10\n- soft pvc 0.17\n- soft tiles 0.23\n- tiled 0.03\n- wood 0.06\n\nAlso posted a discussion [thread](https:\/\/www.kaggle.com\/c\/career-con-2019\/discussion\/85204)\n\n","e3bf5c2d":"This advanced features based on robust statistics.","51d53bc9":"### Histogram for main features","c2447eaa":"## Fourier Analysis\n\nMy hope was, that different surface types yield (visible) differences in the frequency spectrum of the sensor measurements.\n\nMachine learning techniques might learn frequency filters on their own, but why don't give the machine a little head start? So I computed the the cyclic FFT for the angular velocity and linear acceleration sensors and plotted mean and standard deviation of the absolute values of the frequency components per training surface category (leaving out the frequency 0 (i.e. constants like sensor bias, earth gravity, ...).\n\nThe sensors show some different frequency characterists (see plots below), but unfortunately the surface categories have all similar (to the human eye) shapes, varying mostly in total power, and the standard deviations are high (compared to differences in the means). So there are no nice strong characteristic peaks for surface types. But that does not mean, that there is nothing detectable by more sophisticated statistical methods.\n\nThis article http:\/\/www.kaggle.com\/christoffer\/establishing-sampling-frequency makes a convincing case, that the sampling frequency is around 400Hz, so according to that you would see the frequency range to 3-200 Hz in the diagrams (and aliased higher frequencies).\n\nby [@trohwer64](https:\/\/www.kaggle.com\/trohwer64)","8a64f896":"**Generate new denoised train and test**","77696da2":"### Submission (Part I)","ed54c0d2":"# ABOUT Submissions & Leaderboard","db878e3f":"## Correlations (Part II)","b3e68708":"### Maybe this is the most interesting part, the difference between a 0.73 and 0.72 submission.","edc31fd8":"---\n\n# Feature Engineering","0506c342":"## Generate a new train and test: Fast Fourier Transform Denoising","9914806c":"## References\n\n[1] https:\/\/www.kaggle.com\/vanshjatana\/help-humanity-by-helping-robots-4e306b\n\n[2] https:\/\/www.kaggle.com\/artgor\/where-do-the-robots-drive\n\n[3] https:\/\/www.kaggle.com\/gpreda\/robots-need-help\n\n[4] https:\/\/www.kaggle.com\/vanshjatana\/help-humanity-by-helping-robots-4e306b by [@vanshjatana](https:\/\/www.kaggle.com\/vanshjatana)","7c4b36dc":"## Step 0 : quaternions","265b6770":"We need to classify on which surface our robot is standing.\n\nMulti-class Multi-output\n\n9 classes (suface)","0e155008":"**Now, Let's see code for series 15 ( is an example, try what you want)**","07439ae5":"### goup_id will be important !!","54561006":"## Visualizing Series\n\nBefore, I showed you as an example the series 1.\n\n**This code allows you to visualize any series.**\n\nFrom: *Code Snippet For Visualizing Series Id by @shaz13*","2091f144":"Godd news, our basic features have the **same distribution (Normal) on test and training**. There are some differences between *orientation_X* , *orientation_Y* and *linear_acceleration_Y*.\n\nI willl try **StandardScaler** to fix this, and remember: orientation , angular velocity and linear acceleration are measured with different units, scaling might be a good choice.","1f0d8b07":"Remember the order at the array is [0.73LB, 0.72LB, 0.71LB, 06LB].\nSeries with ```series_id```= 575, 1024, 911, 723, 148, 338 are really interesting because they show important differences between surfaces that often are being confused.","38367a65":"# CareerCon 2019 - Help Navigate Robots\n## Robots are smart\u2026 by design !!\n\n![](https:\/\/www.lextronic.fr\/imageslib\/4D\/0J7589.320.gif)\n\n---\n\nRobots are smart\u2026 by design. To fully understand and properly navigate a task, however, they need input about their environment.\n\nIn this competition, you\u2019ll help robots recognize the floor surface they\u2019re standing on using data collected from Inertial Measurement Units (IMU sensors).\n\nWe\u2019ve collected IMU sensor data while driving a small mobile robot over different floor surfaces on the university premises. The task is to predict which one of the nine floor types (carpet, tiles, concrete) the robot is on using sensor data such as acceleration and velocity. Succeed and you'll help improve the navigation of robots without assistance across many different surfaces, so they won\u2019t fall down on the job.\n\n###  Its a golden chance to help humanity, by helping Robots !\n\n<br>\n<img src=\"https:\/\/media2.giphy.com\/media\/EizPK3InQbrNK\/giphy.gif\" border=\"1\" width=\"400\" height=\"300\">","7dbc3cb1":"<br>\n# DATA"}}