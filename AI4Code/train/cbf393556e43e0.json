{"cell_type":{"e084bac3":"code","72a6a857":"code","14e63827":"code","1acf5804":"code","a2494c68":"code","3347b951":"code","0021ef5c":"code","fc470d90":"code","1758de75":"code","445d8c41":"code","6d6a486d":"code","a3644c1d":"code","b58b2a57":"code","d5ae0ca4":"code","b4c0b80d":"code","69401323":"code","fad962c6":"code","29704fc9":"code","93cfe353":"code","6291fb39":"code","7ac6f5b9":"code","3d96d419":"code","a36cc5f9":"code","d693ea0d":"code","a6f11b4f":"code","8837dd0f":"code","8a1c672c":"code","12131efb":"code","8b840be5":"markdown","6d2fc9ae":"markdown","67eb5c2a":"markdown","e8b07467":"markdown","5947cd60":"markdown","f9aa7e5b":"markdown","c2815d14":"markdown","9d48eb46":"markdown","1cd6fea3":"markdown","343a21c5":"markdown","b7fd5394":"markdown","129c48c4":"markdown","6aa595ec":"markdown","e6ec3e63":"markdown","d1cdcedc":"markdown","ced0eb5a":"markdown","1fd05ee2":"markdown","680f24e2":"markdown","d9f72632":"markdown","34e4aaa0":"markdown","625d4c59":"markdown","12415499":"markdown","1749355c":"markdown","7b3731e1":"markdown","592864c7":"markdown","7daa255e":"markdown","8f10386c":"markdown","dc33c041":"markdown","27391df5":"markdown","dde43f5a":"markdown","953f70c2":"markdown","57dc0d14":"markdown","1fc0c709":"markdown","96bec513":"markdown","5b4ed1a5":"markdown","e04c8c8f":"markdown"},"source":{"e084bac3":"import pandas as pd\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass \nwarnings.warn = ignore_warn #ignore warning from sklearn and seaborn\n\n#pd.set_option('display.float_format', lambda x: '{:.2f}'.format(x)) #Limiting floats output to 2 decimal points\n","72a6a857":"# Path of the file to read. \ntrain_set = '..\/input\/train.csv'\ntrain = pd.read_csv(train_set)\n\ntrain.head()\n","14e63827":"#Save the Primary Key IDcolumn in case we need it later\ntrain_ID = train['Id']\n\n#Drop Primary Key - \ntrain.drop(\"Id\", axis = 1, inplace = True)","1acf5804":"#correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, square=True);\n","a2494c68":"#scatterplot correlated variables.\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols], size = 2.5)\nplt.show()","3347b951":"#plot scatter to identify any outliers\nvar='GrLivArea'\n\nfig, ax = plt.subplots()\nax.scatter(x = train[var], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=10)\nplt.xlabel(var, fontsize=10)\nplt.show()","0021ef5c":"#Delete outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\n#Check the graph again\nvari = 'GrLivArea'\nfig, ax = plt.subplots()\nax.scatter(train[vari], train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel(vari, fontsize=13)\nplt.show()","fc470d90":"#Plot the distribution of the Target column Y.\nsns.distplot(train['SalePrice'])\n\n","1758de75":"#Sort out the distribution by using Log1p\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'])","445d8c41":"#Show % of nulls per column\ntrain_na_percent = (train.isnull().sum() \/ len(train)).sort_values(ascending=False)\ntrain_na_total = train.isnull().sum().sort_values(ascending=False)\nmissing_data = pd.concat([train_na_percent, train_na_total], axis=1, keys=['%', 'Total'])[:30]\nmissing_data.head()","6d6a486d":"#Graph it \nf, ax = plt.subplots(figsize=(6, 4))\nplt.xticks(rotation='90')\nsns.barplot(x=missing_data.index, y=missing_data['%'])\nplt.xlabel('Feature')\nplt.ylabel('Percent of missing values')\n","a3644c1d":"#Drop Columns with poor data\n\n#train = train.drop(['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'], axis=1)\n\n\n","b58b2a57":"##FillNA with String\nfor col in ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu','GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','MSSubClass', 'MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    train[col] = train[col].fillna('None')\n     \n#FillNA with Median\ntrain[\"LotFrontage\"] = train.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n\n#fillNA with Zero    \nfor col in ('GarageYrBlt', 'GarageArea', 'GarageCars','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath', 'MasVnrArea'):\n    train[col] = train[col].fillna(0)\n\n#fillNA with Mode    \nfor col in ('MSZoning', 'Electrical', 'Exterior1st', 'Exterior2nd', 'KitchenQual', 'SaleType'):\n    train[col] = train[col].fillna(train[col].mode()[0])\n\n#Drop\ntrain = train.drop(['Utilities'], axis=1)\n\n#Functional replace NA with Typ as per data descritpion - thanks @Sergine\ntrain[\"Functional\"] = train[\"Functional\"].fillna(\"Typ\")\n\n\n","d5ae0ca4":"#Check remaining missing values if any \ntrain_na = (train.isnull().sum() \/ len(train)) * 100\ntrain_na = train_na.drop(train_na[train_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame(train_na)\nmissing_data.head()","b4c0b80d":"#MSSubClass needs to be str\ntrain['MSSubClass'] = train['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\ntrain['OverallCond'] = train['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\ntrain['YrSold'] = train['YrSold'].astype(str)\ntrain['MoSold'] = train['MoSold'].astype(str)","69401323":"#label encoding for all of the categorical varibles that are stored as object - also added in the remaining \"object\" type features.\n\n\ncols = ('PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold', 'MSZoning','LandContour', 'LotConfig', 'Neighborhood', 'Condition1', \n        'Condition2',  'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', \n        'MasVnrType', 'Foundation', 'Heating', 'Electrical', 'GarageType', 'SaleType', 'SaleCondition')\n\n\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(train[c].values)) \n    train[c] = lbl.transform(list(train[c].values))\n\n","fad962c6":"#Check if there are any \"object\" types left. (used previously for randomForest)\ntrain.info()","29704fc9":"# Adding total sqfootage feature \ntrain['TotalSF'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']","93cfe353":"train.head()","6291fb39":"# Split Data set into X and Y - X is the features, Y is the varible we want to predict.\nX = train\nX = train.drop(['SalePrice'], axis=1) #remove when using test data\ny = train['SalePrice'] #remove when using test data\n\n","7ac6f5b9":"X.shape\n","3d96d419":"# Select an alpha - Trail and Error or Cross Validation - will expand on this.\nbest_alpha = 0.00099\n\n#Train the Model\nregr = Lasso(alpha=best_alpha, max_iter=50000)\nregr.fit(X, y)\n","a36cc5f9":"#Predict using X as our parameters\nlasso_pred = regr.predict(X)","d693ea0d":"print(lasso_pred)\n","a6f11b4f":"#Add the targets and predictiions back into the data set also use expm1 to change them back to their original numbers\n\n#Add the score to X\nX['lassoScore'] = lasso_pred\n\n#Add the Actual Score to X\nX['lassoScoreAct']= np.expm1(X['lassoScore'])\n\n#Create a column named Targets containing the original SalesPrice feature \nX['Targets'] = y\nX.head()\n\n","8837dd0f":"#Graph Targets vs Prediction to see correllation\nsns.scatterplot(x=X['lassoScore'], y=X['Targets'])\nplt.xlabel('Target')\nplt.ylabel('Prediction')\nplt.show()","8a1c672c":"from sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_squared_log_error\nfrom math import sqrt\n\n\nrf_val_sq_error2 = sqrt(mean_squared_error(X['Targets'], X['lassoScore'])) #correct\nprint(rf_val_sq_error2)\n\n\n\n","12131efb":"#for submission of test data set\n#submission = X[['Id', 'SalePrice']]\n#submission.to_csv('submission.csv', index=False)\n\n\n","8b840be5":"# Machine Learning - Random Forest Regression for House Prices","6d2fc9ae":"First we need to pick a model, for this is have used a Lasso from SciKit.  Once we have selected the model we must fit the previously created X and y variables to it.   This is just for training data, you will not re-run this when it comes to submitting your test data. ","67eb5c2a":"## Distribution \/ Skewedness of Values","e8b07467":"## Outliers","5947cd60":"Once we have made the changes to our dataset, we run a check to see if there are any nulls left ","f9aa7e5b":"## Correlation of Numerical Features","c2815d14":"Judging from the amount of missing data, its reasonable to remove PoolQC, MiscFeature, Alley, Fence and FireplaceQu.  Essentially I made the decision that these are not important because they are not things we consider when buying a house.  Ideally we would do more analysis on these features, but in the interest of keeping this simple I wont bother.  Lets just drop them altoghter. \n\nNote:  Model is actually more accurate with these in.  Will dig deeper and update further, for now the columns are included in the model.\n\n","9d48eb46":"Next we check the data description to see if we need to update the datatypes on any of our features.\nIf a numeric variable is actually a category, we need to convert this to a string, so the RandomForest will treat is as a category and not a numeric variable.   A good example of this is years,  we dont want 2019 to be treated as a greater value than 2001, we want the algorithm to understand that the years are a category, not a number.","1cd6fea3":"## Split into X (Features) and Y (Target)","343a21c5":"The distribution is left Skewed.  We can run Log1p to sort out the distribution of the data.   Log1p bascially applies Log(1+x) to the target column.","b7fd5394":"## Train the Model (Train Data Only)","129c48c4":"As per Competition Rules, we validate the data using log mean squared error.  Which is the sqrt of mean_squared_error.","6aa595ec":"We are mostly interested in the target variable 'SalePrice'.  Judging from the heatmap, there is a strong correlation between SalePrice and two other numeric variables.  These are: GrLivArea and OverallCond. \n\nScatterPlot the different 'highly correlated' variables against eachother to investigate the linear relationship and identify any outliers. (Be patient, this may take some time to visualize. ","e6ec3e63":"Now we should check that our list of predictions make sense, remember they wont look like house prices because we applied log1p to the values, we can undo this afterwards. \n","d1cdcedc":"## Scoring the Results","ced0eb5a":"## Predict Using Trained Model","1fd05ee2":"You can see from the second plot that the outliers are now removed.   Lets move forwards and take a look at the distribution of the SalesPrice values.","680f24e2":"## Submitting to Kaggle","d9f72632":"There are different methods we will use here to replace nulls with something dependent upon the type of feature we are dealing with.  We use the lines of code below to replace the nulls with either: \n1.  a string stating the word \"None\" - if the data descritpion defines this.\n2.  the median value of the feature (if its a numberic variable)\n3.  The modal class of the feature (if it is a categorical feature)\n4.  A zero - if the data description defines this.\n","34e4aaa0":"## Understanding the dataset - Initial Data Analysis","625d4c59":"The ID column is useless for analysis, first we save the ID column from the data set - then we drop it so we are left with just the data we want to analyze.","12415499":"This last box will save the columns required to submit to Kaggle and output a document named \"submission.csv\" to the root of your filesystem.","1749355c":"## Dealing with Missing Data","7b3731e1":"## Feature Engineering","592864c7":"Now we have a clean, encoded dataset that is ready to be used to train a model.  We need to split down our dataset into:\n\nX  - This contains all of the features \n\ny  - This is the variable that we will use to train the model","7daa255e":"Add the targets and predictions back into the data set also use expm1 to change them back to their original numbers","8f10386c":"Now we run train.head() to check that our data is encoded. i.e. check that there are no text values in there, and everything is represented by a number.","dc33c041":"Notice when we plot GrLivArea against Saleprice there are a couple of particularly significant outliers on the bottom right of the diagram.  Lets run some code to remove these outliers so they dont reduce the accuracy of our model.","27391df5":"First lets take a look at the percentage of nulls per column.   We sort this descending because we are only bothered about features with a high number of nulls","dde43f5a":"Are there any linear relationships to 'SalePrice' that we feel we need to dig into further?   Looks like we have linear relationships between SalesPrice and GrLivArea, and TotBsmtSF.   Let create a scattergraph we can use to zoom in on these and pick put the outliers.","953f70c2":"One of the most useful ways in which we can visualize the correlation between the different numeric features of our dataset is by producing a correlation matrix, then overlaying it with a heatmap.  Generally lighter colours will signify a high correlation.   ","57dc0d14":"The purpose of this notebook is to expand on the basic training provided by Kaggle and add some more in depth data analyis and preparation activities, in an attempt to apply a \"lite data science process\" to the stated problem.","1fc0c709":"Now we have a nice bell-shaped distribution of values for our data, let look for our other ","96bec513":"We also should check if there are any \"object\" type categorical features left in our dataset.  Most ML algorithms need (or prefer) to accept encoded information.  Encoded inforomation is essentially changing values into numbers.  I.e. \"Street\" may contain values labelled \"King Street\" or \"West 25th Street\", an encoder will assign a number to the value and populate it throughout the dataset.  \n\nThe main two types of encoding you will come across will be Label and OneHot Encoders.  Both have separate use cases. [This link ](https:\/\/medium.com\/@contactsunny\/label-encoder-vs-one-hot-encoder-in-machine-learning-3fc273365621) is useful if you want to learn more about encoding. \n\n","5b4ed1a5":"Are there any additional features that we can create from our current dataset that we believe could be useful in identifying the value of the house?  These will generally always be a numeric.","e04c8c8f":"Now we have a trained model, we need to use it to create some predictions on our training (or test) data.  We simply create a variable which references the model we trained and pass in X, which is our list of features."}}