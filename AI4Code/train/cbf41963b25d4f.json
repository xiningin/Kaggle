{"cell_type":{"86bb07e2":"code","b4534f4d":"code","e936f382":"code","250b3fb2":"code","721ac3be":"code","92026bbd":"code","859b08bd":"code","ac55c4e9":"code","f333ba52":"code","9fc86fa4":"code","28665f51":"code","07353dab":"code","702fbf1d":"code","8b9860c2":"code","5985d836":"code","a15f243f":"code","ba563b78":"markdown","a72158e1":"markdown","906484f4":"markdown","24347422":"markdown","44c27187":"markdown","69af1cfb":"markdown","9a786dde":"markdown"},"source":{"86bb07e2":"import numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import metrics \nfrom lightgbm import LGBMRegressor \n        \ninput_path = Path('\/kaggle\/input\/tabular-playground-series-feb-2021\/')","b4534f4d":"train = pd.read_csv(input_path \/ 'train.csv', index_col='id')\ndisplay(train.head())","e936f382":"test = pd.read_csv(input_path \/ 'test.csv', index_col='id')\ndisplay(test.head())","250b3fb2":"submission = pd.read_csv(input_path \/ 'sample_submission.csv', index_col='id')\ndisplay(submission.head())","721ac3be":"for c in train.columns:\n    if train[c].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(train[c].values) + list(test[c].values))\n        train[c] = lbl.transform(train[c].values)\n        test[c] = lbl.transform(test[c].values)\n        \ndisplay(train.head())","92026bbd":"target = train.pop('target')","859b08bd":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\ntrain = scaler.fit_transform(train)\ntest = scaler.transform(test)","ac55c4e9":"from pandas import DataFrame\ntrain = DataFrame(train)\ntest = DataFrame(test)","f333ba52":"X_train, X_test, y_train, y_test = train_test_split(train, target, train_size=0.90)","9fc86fa4":"from sklearn import ensemble\nrf = ensemble.RandomForestRegressor()\nrf.fit(X_train,y_train)\ny_preds = rf.predict (X_test)\nprint(mean_squared_error(y_test,y_preds))","28665f51":"lgbm = LGBMRegressor()\nlgbm.fit(X_train,y_train)\ny_pred = lgbm.predict(X_test)\nmse_l = mean_squared_error(y_test,y_pred)\nprint(mse_l)","07353dab":"import xgboost as xg\nxgr = xg.XGBRegressor()\nxgr.fit(X_train,y_train)\ny_preds = xgr.predict(X_test)\nprint(mean_squared_error(y_test,y_preds))","702fbf1d":"from sklearn.ensemble import VotingRegressor\n#create a dictionary of our models\nestimators=[('RandomForest', rf), ('LightGBM',lgbm), ('xgboost', xgr)]\n#create our voting classifier, inputting our models\nensemble = VotingRegressor(estimators)","8b9860c2":"ensemble.fit(X_train,y_train)\ny_preds = ensemble.predict(X_test)\nprint(mean_squared_error(y_test,y_preds))","5985d836":"\nlgbm.fit(train, target)\nsubmission['target'] = lgbm.predict(test)\nsubmission.to_csv('lgbm.csv')\n","a15f243f":"submission","ba563b78":"Splitting the data into training set and testing set. ","a72158e1":"## Introduction\n\nIn this notebook, We will be creating a model where we will ensemble the results of three regressors namely Random Forest, Xgboost and LGBM \ud83d\ude0e. I have not done any Exploratory data analysis and have included only the very basic feature engineering. However, I have plans to improve on that to see if the performance and can be improved or not. Hope the notebook helps you. Thank you and a upvote would mean a lot. \ud83d\ude09","906484f4":"# Read in the data files","24347422":"Standardizing the data, though Decision Tree and therefore Random Forest do not need standardization. ","44c27187":"## Feature Engineering ","69af1cfb":"## Model Creation\n\nSo we will be creating a ensemble model that will combine the results of Xgboost, RandomForest and LightGBM. ","9a786dde":"#### So lets use label encoding because I will be making mostly tree based models. \n\nFor reference, when to go for label encoding and OHE ( One Hot Encoding ) - \n1. Basically tree based models know how to handle categorical variables so label encoding is preferred over categorical. For non tree based models genrally OHE is better. \n2. If the categorical values are not ordinal then OHE should be preferred over label encoding. \n3. OHE has a problem that it gives rises to the curse of dimensionality when the ordanlity is high. So for good result, one needs to do PCA after OHE. \n\nSome more reasons are there as well but these three are generally my main guideline. \n"}}