{"cell_type":{"64e1ddca":"code","c9dfbb6f":"code","422154e6":"code","226cd480":"code","5519f899":"code","9aae7b22":"code","99081668":"code","59f35e51":"code","df3bd779":"code","d2c0357e":"code","f20a2225":"code","16df6ff0":"code","aca09446":"code","ed51940d":"code","fffe5362":"code","114eb4df":"code","c61ae3e2":"code","e10813b6":"code","5350cbc6":"code","33ecca44":"code","6a685895":"code","4fef77a4":"code","f27bb814":"code","ae717c89":"code","81ecaa86":"code","c2be30b9":"code","a66f24ad":"code","2b8e13e0":"code","a3158af4":"code","084d92cd":"code","c789bdf4":"code","fad02863":"code","69f99eb7":"code","a63535ca":"code","2ac181b7":"code","464d87e8":"code","b8414633":"code","f91672a6":"code","4003b4c9":"code","fff12a3c":"code","67a554a2":"code","c9c60621":"code","aa5facee":"code","c40bb5ae":"code","533e5112":"markdown","6ec729c6":"markdown","6b720409":"markdown","7b07f5d4":"markdown","878be861":"markdown","a0569f7a":"markdown","82a15473":"markdown","95e6d897":"markdown","ca0d18d6":"markdown","36c31292":"markdown","0130ad75":"markdown","4c669a24":"markdown","dd83bd37":"markdown","c0c7bf7c":"markdown","50c33d9f":"markdown","ab2e278f":"markdown","73514608":"markdown","68c2df19":"markdown","0f97ecbb":"markdown","50a1a037":"markdown","23229267":"markdown","2ede72ab":"markdown","37922919":"markdown","f048b952":"markdown","bc5cbb62":"markdown","ecf5df74":"markdown"},"source":{"64e1ddca":"import math, time, random, datetime\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns\nimport missingno\nimport pickle","c9dfbb6f":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ntrain = pd.read_csv('\/kaggle\/input\/unsw-nb15\/UNSW_NB15_training-set.csv')\ntest = pd.read_csv('\/kaggle\/input\/unsw-nb15\/UNSW_NB15_testing-set.csv')\ntest.head()\n\n","422154e6":"train.head()","226cd480":"#X_train = train.drop(['label'], axis=1)\n#Y_train = train['label']\n#X_test = test.drop(['label'], axis=1)\n#Y_test = test['label']\n#","5519f899":"data = pd.concat([train,test]).reset_index(drop=True)\ncols_cat = data.select_dtypes('object').columns # To be explained later\ncols_numeric = data._get_numeric_data().columns # To be explained later","9aae7b22":"data.head()","99081668":"data.describe()","59f35e51":"print(data.isnull().sum())","df3bd779":"missingno.matrix(data)","d2c0357e":"data['proto'].unique() #This is definitely a categorical feature.","f20a2225":"data['service'].unique() #Here, we'll deal with the type of service that is '-'\ndata['service']= np.where(data['service'] == '-', 'None', data['service'])\nprint(data['service'].unique())","16df6ff0":"data['state'].unique() #Keep it.","aca09446":"def Remove_dump_values(data, cols):\n    for col in cols:\n        data[col] = np.where(data[col] == '-', 'None', data[col])\n    return data","ed51940d":"cols = data.columns\ndata_bin = Remove_dump_values(data, cols)","fffe5362":"data_bin = data_bin.drop(['id'], axis=1) #Remove Unnecessary features","114eb4df":"data_bin.drop(['attack_cat'], axis=1, inplace=True)","c61ae3e2":"cols_cat = cols_cat.drop(['attack_cat'])","e10813b6":"data_bin_hot = pd.get_dummies(data_bin,columns=cols_cat)","5350cbc6":"data_bin_hot.shape","33ecca44":"cols_numeric = list(cols_numeric)\ncols_numeric.remove('label')\ncols_numeric.remove('id')","6a685895":"data_bin_hot[cols_numeric] = data_bin_hot[cols_numeric].astype('float') ","4fef77a4":"data_bin_hot[cols_numeric] = (data_bin_hot[cols_numeric] - np.min(data_bin_hot[cols_numeric])) \/ np.std(data_bin_hot[cols_numeric])","f27bb814":"data_bin_hot.head()","ae717c89":"#from sklearn.model_selection import train_test_split\n#X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3, random_state = 42)","81ecaa86":"from sklearn import model_selection\nfrom sklearn import metrics\nX = data_bin_hot.drop('label', axis=1)\nY = data_bin_hot['label']","c2be30b9":"global X #To include in upcoming functions.","a66f24ad":"def fit_algo(algo, x, y, cv):\n    #Fit the model\n    model = algo.fit(x, y)\n    \n    #Check its score\n    acc = round(model.score(x, y) *100, 2)\n    y_pred = model_selection.cross_val_predict(algo, x, y, cv=cv, n_jobs = -1)\n    \n    acc_cv = round(metrics.accuracy_score(Y,y_pred)*100, 2)\n    \n    return y_pred, acc, acc_cv, model","2b8e13e0":"from sklearn.linear_model import LogisticRegression\nstart_time = time.time()\npred_now, acc_lr, acc_cv_lr, lr = fit_algo(LogisticRegression(C=0.1)\n                                        , X, Y, 10)\n\nlr_time = (time.time() - start_time)\n\nprint(\"Accuracy: %s\" % acc_lr)\nprint(\"Accuracy of CV: %s\" % acc_cv_lr)\nprint(\"Execution time: %s\" % lr_time)","a3158af4":"def feature_plot(imp):\n    global X\n    fimp = pd.DataFrame({'Feature': X.columns, 'Importance' : np.round(imp)})\n    fimp =fimp.sort_values(by='Importance', ascending=False)\n    plt.figure(figsize=(10,10))\n    plt.plot(fimp['Feature'], fimp['Importance'])\n    plt.xticks(rotation=90);","084d92cd":"feature_plot(lr.coef_[0])","c789bdf4":"fimp_lr = pd.DataFrame({'Feature': X.columns, 'Importance' : np.round(lr.coef_[0])})\nfimp_lr =fimp_lr.sort_values(by='Importance', ascending=False)\nfimp_lr","fad02863":"#from sklearn.neighbors import KNeighborsClassifier\n#start_time = time.time()\n#pred_now, acc_knn, acc_cv_knn, knn = fit_algo(KNeighborsClassifier(n_neighbors = 3)\n #                                       , X, Y, 10)\n#knn_time = (time.time() - start_time)\n\n#print(\"Accuracy: %s\" % acc_knn)\n#print(\"Accuracy of CV: %s\" % acc_cv_knn)\n#print(\"Execution time: %s\" % knn_time)\n","69f99eb7":"from sklearn.tree import DecisionTreeClassifier\nstart_time = time.time()\npred_now, acc_dt, acc_cv_dt, dt = fit_algo(DecisionTreeClassifier(random_state = 1)\n                                        , X, Y, 10)\n\ndt_time = (time.time() - start_time)\n\nprint(\"Accuracy: %s\" % acc_dt)\nprint(\"Accuracy of CV: %s\" % acc_cv_dt)\nprint(\"Execution time: %s\" % dt_time)","a63535ca":"from sklearn.ensemble import RandomForestClassifier\nstart_time = time.time()\npred_now, acc_rf, acc_cv_rf, rf = fit_algo(RandomForestClassifier(n_estimators = 100)\n                                        , X, Y, 10)\n\nrf_time = (time.time() - start_time)\n\nprint(\"Accuracy: %s\" % acc_rf)\nprint(\"Accuracy of CV: %s\" % acc_cv_rf)\nprint(\"Execution time: %s\" % rf_time)\n","2ac181b7":"from sklearn.ensemble import RandomForestClassifier\nstart_time = time.time()\npred_now, acc_rf2, acc_cv_rf2, rf2 = fit_algo(RandomForestClassifier(n_estimators = 100, criterion='entropy')\n                                        , X, Y, 10)\n\nrf2_time = (time.time() - start_time)\n\nprint(\"Accuracy: %s\" % acc_rf2)\nprint(\"Accuracy of CV: %s\" % acc_cv_rf2)\nprint(\"Execution time: %s\" % rf2_time)","464d87e8":"from sklearn.neural_network import MLPClassifier\n\nstart_time = time.time()\npred_now, acc_nn, acc_cv_nn, nn = fit_algo(MLPClassifier(hidden_layer_sizes = (20,), activation='relu', solver='adam')\n                                        , X, Y, 5)\n\nnn_time = (time.time() - start_time)\n\nprint(\"Accuracy: %s\" % acc_nn)\nprint(\"Accuracy of CV: %s\" % acc_cv_nn)\nprint(\"Execution time: %s\" % nn_time)","b8414633":"from sklearn.naive_bayes import GaussianNB\nstart_time = time.time()\n\npred_now, acc_gnb, acc_cv_gnb, gnb= fit_algo(GaussianNB()\n                                        ,X,Y,5)\n\ngnb_time = (time.time() - start_time)\n\nprint(\"Accuracy: %s\" % acc_gnb)\nprint(\"Accuracy of CV: %s\" % acc_cv_gnb)\nprint(\"Execution time: %s\" % gnb_time)","f91672a6":"from sklearn.ensemble import GradientBoostingClassifier\nstart_time = time.time()\n\npred_now, acc_gbt, acc_cv_gbt, gbt= fit_algo(GradientBoostingClassifier()\n                                        , X, Y, 10)\n\ngbt_time = (time.time() - start_time)\n\nprint(\"Accuracy: %s\" % acc_gbt)\nprint(\"Accuracy of CV: %s\" % acc_cv_gbt)\nprint(\"Execution time: %s\" % gbt_time)","4003b4c9":"from sklearn.svm import LinearSVC\nstart_time = time.time()\n\npred_now, acc_svc, acc_cv_svc, svc= fit_algo(LinearSVC()\n                                        ,X,Y,10)\n\nsvc_time = (time.time() - start_time)\n\nprint(\"Accuracy: %s\" % acc_svc)\nprint(\"Accuracy of CV: %s\" % acc_cv_svc)\nprint(\"Execution time: %s\" % svc_time)","fff12a3c":"algo_name = ['Log. Reg.', 'Decision Tree', 'RandomForest Gini', 'RandomForest IG', 'Neural Network', 'Gaussian NB', 'GBC', 'SVM']\nacc_df = pd.DataFrame({'Algorithm' : algo_name, 'Accuracy %' : [acc_cv_lr, acc_cv_dt, acc_cv_rf, acc_cv_rf2, acc_cv_nn, acc_cv_gnb, acc_cv_gbt, acc_cv_svc] })\nacc_df = acc_df.sort_values(by='Accuracy %', ascending = False)\nacc_df = acc_df.reset_index(drop=True)\nacc_df","67a554a2":"fimp_rf = pd.DataFrame({'Feature' : X.columns, 'Importance' : (rf.feature_importances_).astype(float)})\nfimp_rf = fimp_rf.sort_values(by='Importance', ascending=False)\nfimp_rf","c9c60621":"feature_plot(rf.feature_importances_*100)","aa5facee":"filename = 'RandomForest_IG_IDS.sav'\npickle.dump(rf2, open(filename, 'wb'))","c40bb5ae":"# load the model from disk\nloaded_model = pickle.load(open(filename, 'rb'))\nresult = loaded_model.score(X, Y)\nprint(result)","533e5112":"# Train and Test set splitting:\n","6ec729c6":"##### Categorical Features:\nOne Hot Encoding using cols_cat","6b720409":"# Gradient Boosting","7b07f5d4":"# Problem statement\nWe have a clean dataset that is generated from an IDS, also labeled denoting '0' for No-Attack and '1' for Attack.\nIn this dataset, we will go through different pre-processing well-known to drive this notebook to understandability.\nThe data is clean, yet we need to ensure that every feature matters to the model via 'Feature Importance' that comes out with the generated model.\n\nSource: http:\/\/www.secrepo.com\/\n","878be861":"# Decision Tree","a0569f7a":"# Random Forest (Gini)","82a15473":"# Cross-Validation Accuracy Comparison:","95e6d897":"# Neural Networks","ca0d18d6":"# Missing Values:\n\n   <ul>\n        <li>\n        Check for missing values.\n        <\/li>\n         <li>\n        Replace those missing values.\n        <\/li>\n    <\/ul>","36c31292":"##### Now, let's try to automate this process.","0130ad75":"#### Read the training and test set.","4c669a24":"# Machine Learning Models:\n<ul>\n    <li>\n        This is a Classification problem where we want to detect whether there is an attack or not.\n    <\/li>\n    <li>\n        We will use simple Logistic Regression.\n    <\/li>\n        <li>\n            K-Nearest Neighbour (Lazy Algorithm)\n    <\/li>\n        <li>\n        Decision Trees\n    <\/li>\n        <li>\n        Random Forest (gini)\n    <\/li>\n        <li>\n        Random Forest (Entropy or Information-gain)\n    <\/li>\n    <\/ul>","dd83bd37":"# Gaussian Naive Bayes","c0c7bf7c":"# SVM","50c33d9f":"# Save the best accuracy model","ab2e278f":"# Feature Importance","73514608":"# Loading the model (for future use)","68c2df19":"# Random Forest (Information Gain)","0f97ecbb":"We won't split the Data.. We do care about precision in our case!","50a1a037":"**Do one-hot encoding**","23229267":"##### Normalization:\nNormalize all the values in the dataset.\n","2ede72ab":"Data is clean and there are no missing values. ","37922919":"# Logistic Regression","f048b952":"**Removing unnecessary features:**","bc5cbb62":"We can do per below but we'd like to concatenate both the training set and the test set to avoid doing the preprocessing twice.","ecf5df74":"# Insights and steps: \n   <ul>\n        <li>\n        Data is clean.\n        <\/li>\n         <li>\n        Data still needs furthur processing in terms of One-hot-encoding for categorical data.\n                     E.g.: 'service' consists of different types, we have ftp, http, and '-' denoting (not available or None), So we will need to treat it as a missing value as we will change it from '-' to 'None' instead of dropping the whole column.\n        <\/li>\n            <li>\n        Removing unnecessary features like 'id'.\n        <\/li>\n    <\/ul>"}}