{"cell_type":{"32020efc":"code","985d8544":"code","f383cbe6":"code","55de916d":"code","3ab2ae57":"code","dfb02fd2":"code","4ce3d268":"code","fa8cb277":"code","2b051945":"code","32bb0f3d":"code","c0a05785":"code","4503f849":"code","5561e648":"markdown","9a9efbe1":"markdown","4ebd3da0":"markdown","bd549ec8":"markdown","76aa993c":"markdown","f388998b":"markdown","a6e3d029":"markdown","2ec239a4":"markdown"},"source":{"32020efc":"# *- encoding: utf-8 -*-\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\nimport numpy as np\nimport pandas as pd\n\nimport os\nimport json\nfrom pathlib import Path\nfrom xgboost import XGBClassifier\nimport matplotlib.pyplot as plt\nfrom matplotlib import colors\nimport numpy as np\nfrom sklearn.model_selection import GridSearchCV\n\nimport pdb","985d8544":"data_path = Path('\/kaggle\/input\/abstraction-and-reasoning-challenge\/')\ntraining_path = data_path \/ 'training'\nevaluation_path = data_path \/ 'evaluation'\ntest_path = data_path \/ 'test'","f383cbe6":"def plot_result(test_input, test_prediction,\n                input_shape):\n    \"\"\"\n    Plots the first train and test pairs of a specified task,\n    using same color scheme as the ARC app\n    \"\"\"\n    cmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n    norm = colors.Normalize(vmin=0, vmax=9)\n    fig, axs = plt.subplots(1, 2, figsize=(15,15))\n    test_input = test_input.reshape(input_shape[0],input_shape[1])\n    axs[0].imshow(test_input, cmap=cmap, norm=norm)\n    axs[0].axis('off')\n    axs[0].set_title('Actual Target')\n    test_prediction = test_prediction.reshape(input_shape[0],input_shape[1])\n    axs[1].imshow(test_prediction, cmap=cmap, norm=norm)\n    axs[1].axis('off')\n    axs[1].set_title('Model Prediction')\n    plt.tight_layout()\n    plt.show()\n    \ndef plot_test(test_prediction, task_name):\n    \"\"\"\n    Plots the first train and test pairs of a specified task,\n    using same color scheme as the ARC app\n    \"\"\"\n    cmap = colors.ListedColormap(\n        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n         '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n    norm = colors.Normalize(vmin=0, vmax=9)\n    fig, axs = plt.subplots(1, 1, figsize=(15,15))\n    axs.imshow(test_prediction, cmap=cmap, norm=norm)\n    axs.axis('off')\n    axs.set_title(f'Test Prediction {task_name}')\n    plt.tight_layout()\n    plt.show()","55de916d":"# https:\/\/www.kaggle.com\/inversion\/abstraction-and-reasoning-starter-notebook\ndef flattener(pred):\n    str_pred = str([row for row in pred])\n    str_pred = str_pred.replace(', ', '')\n    str_pred = str_pred.replace('[[', '|')\n    str_pred = str_pred.replace('][', '|')\n    str_pred = str_pred.replace(']]', '|')\n    return str_pred","3ab2ae57":"sample_sub = pd.read_csv(data_path\/'sample_submission.csv')\nsample_sub = sample_sub.set_index('output_id')\nsample_sub.head()","dfb02fd2":"def get_moore_neighbours(color, cur_row, cur_col, nrows, ncols):\n\n    if cur_row<=0: top = -1\n    else: top = color[cur_row-1][cur_col]\n        \n    if cur_row>=nrows-1: bottom = -1\n    else: bottom = color[cur_row+1][cur_col]\n        \n    if cur_col<=0: left = -1\n    else: left = color[cur_row][cur_col-1]\n        \n    if cur_col>=ncols-1: right = -1\n    else: right = color[cur_row][cur_col+1]\n        \n    return top, bottom, left, right\n\ndef get_tl_tr(color, cur_row, cur_col, nrows, ncols):\n        \n    if cur_row==0:\n        top_left = -1\n        top_right = -1\n    else:\n        if cur_col==0: top_left=-1\n        else: top_left = color[cur_row-1][cur_col-1]\n        if cur_col==ncols-1: top_right=-1\n        else: top_right = color[cur_row-1][cur_col+1]   \n        \n    return top_left, top_right","4ce3d268":"def features(task, mode='train'):\n    cur_idx = 0\n    num_train_pairs = len(task[mode])\n    total_inputs = sum([len(task[mode][i]['input'])*len(task[mode][i]['input'][0]) for i in range(num_train_pairs)])\n    feat = np.zeros((total_inputs,nfeat))\n    target = np.zeros((total_inputs,), dtype=np.int)\n    \n    global local_neighb\n    for task_num in range(num_train_pairs):\n        input_color = np.array(task[mode][task_num]['input'])\n        target_color = task[mode][task_num]['output']\n        nrows, ncols = len(task[mode][task_num]['input']), len(task[mode][task_num]['input'][0])\n\n        target_rows, target_cols = len(task[mode][task_num]['output']), len(task[mode][task_num]['output'][0])\n        \n        if (target_rows!=nrows) or (target_cols!=ncols):\n            print('Number of input rows:',nrows,'cols:',ncols)\n            print('Number of target rows:',target_rows,'cols:',target_cols)\n            not_valid=1\n            return None, None, 1\n\n        for i in range(nrows):\n            for j in range(ncols):\n                feat[cur_idx,0] = i\n                feat[cur_idx,1] = j\n                feat[cur_idx,2] = input_color[i][j]\n                feat[cur_idx,3:7] = get_moore_neighbours(input_color, i, j, nrows, ncols)\n                feat[cur_idx,7:9] = get_tl_tr(input_color, i, j, nrows, ncols)\n                feat[cur_idx,9] = len(np.unique(input_color[i,:]))\n                feat[cur_idx,10] = len(np.unique(input_color[:,j]))\n                feat[cur_idx,11] = (i+j)\n                feat[cur_idx,12] = len(np.unique(input_color[i-local_neighb:i+local_neighb,\n                                                             j-local_neighb:j+local_neighb]))\n        \n                target[cur_idx] = target_color[i][j]\n                cur_idx += 1\n            \n    return feat, target, 0","fa8cb277":"param_grid = {\n    \"xgb__n_estimators\": [10],\n    \"xgb__learning_rate\": [0.1],\n    \"xgb__early_stopping_rounds\": np.array((50, 1000))\n}","2b051945":"all_task_ids = sorted(os.listdir(test_path))\n\nnfeat = 13\nlocal_neighb = 5\nvalid_scores = {}\nfor task_id in all_task_ids:\n\n    task_file = str(test_path \/ task_id)\n    with open(task_file, 'r') as f:\n        task = json.load(f)\n\n    feat, target, not_valid = features(task)\n    if not_valid:\n        print('ignoring task', task_file)\n        print()\n        not_valid = 0\n        continue\n\n    nrows, ncols = len(task['train'][-1]['input']\n                       ), len(task['train'][-1]['input'][0])\n    # use the last train sample for validation\n    val_idx = len(feat) - nrows*ncols\n\n    train_feat = feat[:val_idx]\n    val_feat = feat[val_idx:, :]\n\n    train_target = target[:val_idx]\n    val_target = target[val_idx:]\n\n    #     check if validation set has a new color\n    #     if so make the mapping color independant\n    if len(set(val_target) - set(train_target)):\n        print('set(val_target)', set(val_target))\n        print('set(train_target)', set(train_target))\n        print('Number of colors are not same')\n        print('cant handle new colors. skipping')\n        continue\n\n    xgb = XGBClassifier(n_estimators=100, n_jobs=-1)\n   # hgb_pipe = make_pipeline([('xgb', xgb)])\n\n\n    skf = StratifiedKFold(n_splits=10, shuffle = True, random_state = 1001)\n    hgb_grid = GridSearchCV(xgb, param_grid, n_jobs=8, \n         cv=skf, verbose=2, refit=True)\n    hgb_grid.fit(feat, target)\n#     training on input pairs is done.\n#     test predictions begins here\n\n    num_test_pairs = len(task['test'])\n    for task_num in range(num_test_pairs):\n        cur_idx = 0\n        input_color = np.array(task['test'][task_num]['input'])\n        nrows, ncols = len(task['test'][task_num]['input']), len(\n            task['test'][task_num]['input'][0])\n        feat = np.zeros((nrows*ncols, nfeat))\n        unique_col = {col: i for i, col in enumerate(\n            sorted(np.unique(input_color)))}\n\n        for i in range(nrows):\n            for j in range(ncols):\n                feat[cur_idx, 0] = i\n                feat[cur_idx, 1] = j\n                feat[cur_idx, 2] = input_color[i][j]\n                feat[cur_idx, 3:7] = get_moore_neighbours(\n                    input_color, i, j, nrows, ncols)\n                feat[cur_idx, 7:9] = get_tl_tr(\n                    input_color, i, j, nrows, ncols)\n                feat[cur_idx, 9] = len(np.unique(input_color[i, :]))\n                feat[cur_idx, 10] = len(np.unique(input_color[:, j]))\n                feat[cur_idx, 11] = (i+j)\n                feat[cur_idx, 12] = len(np.unique(input_color[i-local_neighb:i+local_neighb,\n                                                              j-local_neighb:j+local_neighb]))\n\n                cur_idx += 1\n\n        print('Made predictions for ', task_id[:-5])\n        preds = hgb_grid.predict(feat).reshape(nrows, ncols)\n        preds = preds.astype(int).tolist()\n        plot_test(preds, task_id)\n        sample_sub.loc[f'{task_id[:-5]}_{task_num}',\n                       'output'] = flattener(preds)\n","32bb0f3d":"print('\\n Best hyperparameters:')\nprint(hgb_grid.best_params_)","c0a05785":"sample_sub.head()","4503f849":"sample_sub.to_csv('submission.csv')","5561e648":"# Extract neighbourhood Features","9a9efbe1":"# Set Paths","4ebd3da0":"# Loading Libraries","bd549ec8":"# Make features for each train sample","76aa993c":"# Training and Prediction","f388998b":"# Plotting functions","a6e3d029":"# Credit to Siddharth.","2ec239a4":"# For flattening 2D numpy arrays"}}