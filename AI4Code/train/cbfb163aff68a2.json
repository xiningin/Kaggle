{"cell_type":{"5edf494a":"code","1496864e":"code","380c006a":"code","aa8067ad":"code","61b217ba":"code","fdb8f895":"code","d354d722":"code","ed8c4c6f":"code","84e4ff9d":"code","f88a09ca":"code","bbf45ec0":"code","e7bc7a85":"code","b54bac18":"code","11f79d61":"code","e7f1850c":"code","512698e0":"code","c44c0f14":"code","ee3ac27c":"code","e244c94c":"code","33cffe09":"code","4ae54883":"code","0532c181":"code","2056aed8":"code","7070d8c9":"code","e54ed1a2":"code","4dabaf10":"markdown","d995e841":"markdown","b7433789":"markdown","db0ac61b":"markdown","2dce2e2e":"markdown","3fa48d38":"markdown","7c4635c2":"markdown","d4253906":"markdown","0a492d75":"markdown","c806603f":"markdown","0384a363":"markdown","efceb0ab":"markdown","741fd340":"markdown","39514e3c":"markdown","252c1b6a":"markdown","5b6338e5":"markdown","7c0814de":"markdown","6976531e":"markdown","9a4e4534":"markdown","6fd064bc":"markdown","763db33e":"markdown","da8ec2d7":"markdown","5c7cd452":"markdown","f67b18ba":"markdown","6f909536":"markdown","c8300683":"markdown","fe6ba9db":"markdown","624333a3":"markdown","f2716b00":"markdown","4a41ea61":"markdown","0ccbad02":"markdown","7890c154":"markdown","1883d881":"markdown","b28c3a26":"markdown","e73b410c":"markdown","36f85049":"markdown","2e002544":"markdown","1986ff10":"markdown"},"source":{"5edf494a":"import pandas as pd\n\ntrainData = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_train.csv')       \ntestData = pd.read_csv('\/kaggle\/input\/fashionmnist\/fashion-mnist_test.csv')  ","1496864e":"print('\\t Filas,  Columnas', )\nprint('Train:\\t', trainData.shape)\nprint('Test:\\t', testData.shape)","380c006a":"def cehck_nulls(data):\n    if data.isnull().any().any() == False:\n        return print('los datos NO conetienen valores Null')\n    else:\n        return print('los datos SI conetienen valores Null')\n\ncehck_nulls(trainData)\ncehck_nulls(testData)","aa8067ad":"trainData.head(4).append(trainData.tail(3))","61b217ba":"labels = {  0: \"Camiseta \/ Top\",\n            1: \"Pantal\u00f3n\",\n            2: \"Jersey\",\n            3: \"Vestido\",\n            4: \"Abrigo\",\n            5: \"Sandalia\",\n            6: \"Camisa\",\n            7: \"Zapatilla de deporte\",\n            8: \"Bolsa\",\n            9: \"Botines\"\n         }\n\nn_cat = len(labels)\n\ndef add_column_from_dict(data, col, new_col, dict_):\n    data[new_col] = data[col].map(dict_)\n    return data\n\nadd_column_from_dict(trainData, 'label', 'labelName', labels)\nadd_column_from_dict(testData, 'label', 'labelName', labels)","fdb8f895":"import matplotlib.pyplot as plt\n\ndef pie_plot(data, plotTitle):\n    \n    aux = data['labelName'].value_counts().to_frame('Freq')\n    aux['labelName'] = aux.index \n    valores = aux['Freq']\n    \n    def pct_abs(values):\n        def funct(pct):\n            total = sum(values)\n            val = int(round(pct * total \/ 100.0))\n            return '{p:.2f}%\\n({v:d} it'.format(p = pct,v = val)\n        return funct\n\n\n    plt.figure(figsize = (16,8))\n\n    ax1 = plt.subplot(121, aspect = 'equal')\n    aux.plot(kind = 'pie', \n             y = 'Freq', \n             ax = ax1,\n             autopct = pct_abs(valores), \n             labels = aux['labelName'], \n             legend = False,\n             title = plotTitle,\n             fontsize = 10)\n\n    # plot table\n    ax2 = plt.subplot(122)\n    plt.axis('off')\n    plt.show()\n    \n    \nplot1 = pie_plot(trainData,'Distribuci\u00f3n de la ropa para el conjunto de datos TRAIN')\nplot2 = pie_plot(testData, 'Distribuci\u00f3n de la ropa para el conjunto de datos TEST')\nplt.show()","d354d722":"import numpy as np\n\ndef plot_image_sample(data, label_number, DataSetType, pf, pc):\n    \n    type_data = ('TRAIN' if DataSetType.lower().find(\"train\") == label_number else 'TEST')\n    \n    # Obtenemos la etiqueta (diccionario)\n    etiqueta = labels[label_number]\n    # Eliminamos la primera columna (codigo etiqueta) y la \u00faltima (nombre etiqueta)\n    aux = data[data[\"label\"] == label_number].sample(1)\n    aux2 = aux.iloc[:, 1:-1]\n    img = np.array(aux2).reshape(pf, pc)\n\n    plt.imshow(img, cmap = 'gray')\n    plt.grid(True)\n    plot = plt.title('Ropa: ' + str(etiqueta) + '\\nDatos: ' + str(type_data))\n    \n\ndef matrix_image_sample(data, label_number, pf ,pc):\n    \n    pd.options.display.max_columns = None\n    aux = data[data[\"label\"] == label_number].sample(1)\n    aux2 = aux.iloc[:, 1:-1]\n    img = pd.DataFrame(np.array(aux2).reshape(pf, pc))\n\n    return img ","ed8c4c6f":"pf = 28\npc = 28\n\nplot_image_sample(trainData, 9, 'train', pf, pc)\nmatrix_image_sample(trainData, 9, pf, pc)","84e4ff9d":"plot_image_sample(testData, 3, 'Test', pf, pc)\nmatrix_image_sample(testData, 3, pf, pc)","f88a09ca":"import keras\n\ndef preprocesamiento(data, pf, pc):\n    \n    out_Y = keras.utils.to_categorical(data.label, len(labels))\n    x_vect = data.values[:,1:-1]  #transformamos el dataFrame en un ndarray, seleccionando solo los p\u00edxeles\n    x_scaled = x_vect \/ 255 # Dividimos por 255 por literatura (convergencia del gradiente, evita le colapso)\n    n_img = data.shape[0]\n    out_X = x_scaled.reshape(n_img, pf, pc, 1) # redimensionamos el vector a (1,784) a (28, 28, 1)  \n    \n    out_X = out_X.astype(float)\n    out_Y = out_Y.astype(float)\n    \n    return out_X, out_Y","bbf45ec0":"x_train, y_train = preprocesamiento(trainData, pf, pc)\nx_test, y_test = preprocesamiento(testData, pf, pc)","e7bc7a85":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, Y_train, Y_val = train_test_split(x_train, y_train, test_size = 0.3, random_state = 42)","b54bac18":"def proc_data_to_plot(data):\n\n    freq = []\n    for i in range(len(data)):\n        freq.append(np.argmax(data[i]))\n        \n    return pd.DataFrame(freq, columns = ['Label'])\n    \n    \n    \nTrain_labels_to_plot = proc_data_to_plot(Y_train) \nVal_labels_to_plot = proc_data_to_plot(Y_val) \n\nTrain_labels_to_plot = add_column_from_dict(Train_labels_to_plot, 'Label', 'labelName', labels)\nVal_labels_to_plot = add_column_from_dict(Val_labels_to_plot, 'Label', 'labelName', labels)\n\n\nplot1 = pie_plot(Train_labels_to_plot,'Distribuci\u00f3n de la ropa para el conjunto de datos TRAIN')\nplot2 = pie_plot(Val_labels_to_plot, 'Distribuci\u00f3n de la ropa para el conjunto de datos de VALIDACION')\nplt.show()","11f79d61":"import tensorflow as tf\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, Conv2D, Dropout, MaxPooling2D\n\n\n\n#Parte 1 del modelo\nmodel = Sequential()\n\nLeakyReLU = lambda x: tf.keras.activations.relu(x, alpha=0.1)\nmodel.add(Conv2D(32, \n                 kernel_size = (3, 3),\n                 activation = LeakyReLU,\n                 padding=\"same\",\n                 input_shape=(pf, pc, 1)))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Dropout(0.3))\n\n\n#Parte 2 del modelo\nmodel.add(Conv2D(64, \n                 kernel_size = (3, 3), \n                 activation = LeakyReLU,\n                 padding=\"same\"))\nmodel.add(MaxPooling2D(pool_size = (2, 2)))\nmodel.add(Dropout(0.5))\n\n\n#Parte 3 del modelo\nmodel.add(Conv2D(128, (3, 3), activation = LeakyReLU))\nmodel.add(Flatten())                               # Flatemos el tensor de pixeles:\nmodel.add(Dense(128, activation = LeakyReLU))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(n_cat, activation = 'softmax'))    #\u00a0La ultima capa debe ser el n\u00ba de lables a predecir\n\n","e7f1850c":"model.compile(loss = keras.losses.categorical_crossentropy,\n              optimizer = 'adam',\n              metrics = ['accuracy'])","512698e0":"model.summary()","c44c0f14":"from keras.utils.vis_utils import model_to_dot\nfrom IPython.display import SVG\n\n\nSVG(model_to_dot(model).create(prog='dot', format='svg'))","ee3ac27c":"batch = 70\nepocas = 50\n \ntrain_model = model.fit(X_train, Y_train,\n                        batch_size = batch,\n                        epochs = epocas,\n                        verbose = 1,\n                        validation_data = (X_val, Y_val))","e244c94c":"score = model.evaluate(x_test, y_test, verbose = 0)\nprint('Perdida\/Loss Test:', score[0])\nprint('Precision\/Accuracy Test:', score[1])","33cffe09":"import plotly.graph_objs as go\n\ndef interpolation_tracer(x, y, text, mode):\n    fig.add_trace(go.Scatter(x = x, \n                             y = y, \n                             name = text,\n                             mode = mode))\n    fig.update_yaxes(range=[0,1])\n    fig.update_xaxes(title_text = '\u00c9pocas')\n    fig.update_yaxes(title_text = 'Loss & Accuracy')\n    \ndef layout_plot(Titulo):\n    fig.update_layout(title = {'text': Titulo},\n                      xaxis_title = \"Accuracy\",\n                      yaxis_title = \"\u00c9pocas\",\n                      legend_title = \"Leyenda\",\n                      font = dict(family = \"Courier New, monospace\",\n                                  size = 18,\n                                  color = \"RebeccaPurple\"))\nhist = train_model.history\nacc = hist['accuracy']\nval_acc = hist['val_accuracy']\nloss = hist['loss']\nval_loss = hist['val_loss']\nepochs = list(range(1, len(acc) + 1))\n    \nfig = go.Figure()\ninterpolation_tracer(epochs, acc, 'Training accuracy', 'lines+markers')\ninterpolation_tracer(epochs, val_acc, 'Validation accuracy', 'lines+markers')\nlayout_plot('<b>Accuracy<\/b> entrenamiento y validaci\u00f3n')\nfig.show()\n\nfig = go.Figure()\ninterpolation_tracer(epochs,loss,'Training loss', 'lines+markers')\ninterpolation_tracer(epochs,val_loss,'Validation loss', 'lines+markers')\nlayout_plot('<b>Loss<\/b> entrenamiento y validaci\u00f3n')\nfig.show()\n\n","4ae54883":"pred = model.predict_classes(x_test)","0532c181":"y_true = testData.iloc[:,0].to_numpy()","2056aed8":"n = len(pred[:10000])\n\nGoodPred = np.where((pred[:10000] == y_true[:10000]) == True)[0]\nBadPred  = np.where((pred[:10000] == y_true[:10000]) == False)[0]\n\nprint('Se han predicho correctamente ' + str(GoodPred.shape[0]) + \n      ' clases de ' + str(n) + '.\\tAcc: ' + str(round((GoodPred.shape[0]\/n)*100, 2)) + '%')\n\nprint('Se han predicho err\u00f3neamente ' + str(BadPred.shape[0]) +\n      ' clases de ' + str(n) + '.\\tAcc: ' + str(round((BadPred.shape[0]\/n)*100, 2)) + '%')","7070d8c9":"from sklearn.metrics import confusion_matrix\nimport itertools\n\ndef Matriz_de_confusion(cm, clases,  normalize = False, title = 'Matriz de confusi\u00f3n', cmap = plt.cm.Oranges):\n    \n    plt.figure(figsize=(10 , 10) , dpi= 70)\n    plt.imshow(cm , \n               interpolation = 'nearest' , \n               cmap = cmap ) \n    plt.suptitle(title, fontsize=20)\n    tick_marks = np.arange(len(clases))\n    plt.xticks(tick_marks, \n               clases,\n               rotation = 45 )\n    plt.yticks(tick_marks, \n               clases)\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max()\/2.\n    for i, j in itertools.product(range(cm.shape[0]) , range(cm.shape[1]) ):\n        plt.text(j, i, format(cm[i, j] , fmt), \n        horizontalalignment = \"center\" ,\n        color=\"white\" if cm[ i, j] > thresh else \"black\" )\n        \n    plt.ylabel('Etiquetas reales')\n    plt.xlabel('Etiquetas predichas') ","e54ed1a2":"np.set_printoptions(precision = 2)\nsetLabels = [str(key) + str(': ') + labels[key] for key in labels]\n\n\nMatriz_de_confusion(confusion_matrix(y_true, pred), \n                    clases = setLabels )","4dabaf10":"Como podemos observar, la primera columna `label` (no confindir con el \u00edndice) es la que nos referencia que tipo de prenda es, por ejemplo, la primera fila el valor de `label` es 2, por lo que nosotros podemos afirmar que ese conjunto de p\u00edxeles conforman la prenda __jersey__. Crearemos una nueva columna llamada `labelName` (qualitativa) que ser\u00e1 la transcripcion de `label` (quantitativa):\n","d995e841":"<a id=\"5\"><\/a>\n# 5. PREPROCESAMIENTO DE LOS DATOS\n\n<a id=\"5.1\"><\/a>\n\n##\u00a05.1. Feature Engineering\n\n<br>\n\nLa __feature engineering__ es simplemente un proceso ed exploracion de datos, por ejemplo, saber si nuestros datos se deben __escalar__, __estandarizar__, __normalizar__ o __transformar__ (algunas redundantes). \n<br>\n\nRealmente, para la __feature engineering__ no existe una regla f\u00e1cil o com\u00fan que debamos utilizar, pero con experiencia, se puede llegar a ciertas conclusiones. Por ejemplo, se sabe que dividir por 255 una matriz de datos cuando \u00e9sta representa valores asociados a escalas de grises (de 0 a 255), produce una mejora en la convergencia de algunas funciones como puede ser la sigmoide, la cual trabaja con valores $x \\in$ $[0, 1]$. Adem\u00e1s, esto puede causar una explosi\u00f3n del gradiente (no entraremos ahora en esto). Sin afirmar nada pero como regla general, yo escalaria los datos primero y luego entrenaria el modelo.\n<br>\n\nPor otro lado, si los datos est\u00e1n normalizados o centrados en cero, los valores de los p\u00edxeles son peque\u00f1os (a\u00fan siendo valores peque\u00f1os, estos no pierden la representatividad de la imagen original) y, por lo tanto, el c\u00e1lculo requerido y el tiempo para la convergencia del modelo se reducen significativamente. \n\n\nPensad que durante la __forward propagation__ (propagaci\u00f3n hacia adelante), se realizan productos sobre los valores de los p\u00edxeles con la matriz de peso para esa capa en particular. Ahora, multiplicar esos grandes valores de p\u00edxeles requiere muchos recursos computacionales y de tiempo. Y, por lo tanto, el modelo converge muy lentamente.\n\n\n","b7433789":"<a id=\"4.2\"><\/a>\n## 4.2. Visualizando la matriz como im\u00e1genes\n\n<br>\n\nProcedemos a cear una funci\u00f3n que nos graficar\u00e1 la prenda seleccionadndo una fila de la matriz (1 fila = 1 prenda). Para ello, redimensionamos la fila de p\u00edxeles (una fila = una imagen, es decir, un vector $V = (v_1, v_2, ..., v_n)$ donde $n$ = 784 ) a una matriz de I $\\in$ M$_{m,n}$, donde $m, n = 28$.\n","db0ac61b":"Existe una funci\u00f3n en __sklearn__ que realiza dicho split del dataset automaticamente. Seg\u00fan la literatura (concretamente lo menciona __Aur\u00e9lien G\u00e9ron__ en el manual __Hand on machine learning with scikit-learn and tensorflow pdf__) el valor de la semilla (si se deseja fijar y mantener su reproducibilidad) se fija normalmente en el valor __42__.","2dce2e2e":"Seguidamente, debemos comprobar si existen datos faltantes:","3fa48d38":"# <a id='0'>Content<\/a>\n\n- <a href='#1'>1. INTRODUCCI\u00d3N<\/a>  \n- <a href='#2'>2. IMPORTACI\u00d3N DE DATOS<\/a>  \n- <a href='#3'>3. CONJUNTO DE DATOS<\/a>  \n    - <a href='#3.1'>3.1. Categorias de los datos<\/a>\n    - <a href='#3.2'>3.2. \u00bfC\u00f3mo son las im\u00e1genes?<\/a>\n- <a href='#4'>4. QUICK VIEW DE LOS DATOS<\/a>  \n    - <a href='#4.1'>4.1. Distribuci\u00f3n por clases<\/a>\n    - <a href='#4.2'>4.2. Visualizando la matriz como im\u00e1genes<\/a>\n        - <a href='#4.2.1'>4.2.1. Muestra train<\/a>\n        - <a href='#4.2.2'>4.2.1. Muestra test<\/a>\n- <a href='#5'>5. PREPROCESAMIENTO DE LOS DATOS<\/a>  \n    - <a href='#5.1'>5.1. Feature Engineering<\/a>\n    - <a href='#5.2'>5.2. Split del datset train para el entrenamiento del modelo<\/a>\n- <a href='#6'>6. MODELO<\/a>  \n    - <a href='#6.1'>6.1. Partes del Modelo<\/a>\n        - <a href='#6.1.1'>6.1.1. Modelo Parte 1<\/a>\n        - <a href='#6.1.2'>6.1.2. Modelo Parte 2<\/a>\n        - <a href='#6.1.3'>6.1.3. Modelo Parte 3<\/a>\n    - <a href='#6.2'>6.2. Evaluaci\u00f3n del modelo<\/a>\n    - <a href='#6.3'>6.3. Predicciones en base al modelo<\/a>\n    - <a href='#6.4'>6.4. Matriz de Confusi\u00f3n: Evaluaci\u00f3n de las Predicciones<\/a>\n- <a href='#7'>7. CONCLUSIONES<\/a>  \n- <a href='#8'>8. REFERENCIAS<\/a>  ","7c4635c2":"Ahora, por ejemplo, las distribuci\u00f3n de las prendas de ropa es la siguiente:","d4253906":"No menos importante es visualizar la matriz de datos, pues nos es imprescindible para manejarlos y saber con cual de las columnas asociar las etiquetas. Para ello, mostramos las 3 primeras y \u00faltimas filas del data set Train (idem por el test):","0a492d75":"[1] Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow, 2nd Editio, Aur\u00e9lien G\u00e9roN. [https:\/\/www.oreilly.com](https:\/\/www.oreilly.com\/library\/view\/hands-on-machine-learning\/9781492032632\/)\n\n[2] How to use Learning Curves to Diagnose Machine Learning Model Performance, Jason Brownlee. [https:\/\/machinelearningmastery.com](https:\/\/machinelearningmastery.com\/learning-curves-for-diagnosing-machine-learning-model-performance\/)\n\n[3] Activation Functions : Sigmoid, ReLU, Leaky ReLU and Softmax basics for Neural Networks and Deep Learning \n[https:\/\/medium.com](https:\/\/medium.com\/@himanshuxd\/activation-functions-sigmoid-relu-leaky-relu-and-softmax-basics-for-neural-networks-and-deep-8d9c70eed91e)","c806603f":"<a id=\"3\"><\/a>\n# 3. CONJUNTO DE DATOS\n\n<br>\n    \nNuestro conjunto de datos esta formado por 70.000 im\u00e1genes, el cual est\u00e1 dividido en dos subconjuntos:  __train__ y __test__. Los conjuntos de datos tienen las siguientes caracter\u00edsticas:\n * __Train:__ Se utilizar\u00e1 para entrenar el modelo. Este set contiene el 85% de todas las imagenes, es decir, un total de 60.000 im\u00e1genes donde cada una esta formada por 784 p\u00edxeles.\n * __Test:__ Se utilizar\u00e1 para probar el modelo. Este set contiene el 15% de todas las imagenes, es decir, un total de 10.000 im\u00e1genes donde cada una esta formada por 784 p\u00edxeles.\n<br>\n\n\n## 3.1. Categorias de los datos\n\n |   \u00cdndice    |        Categoria          |\n |-------------|---------------------------|\n |      0      |      Camiseta \/ top       |    \n |      1      |        Pantal\u00f3n           |\n |      2      |         Jersey            |\n |      3      |        Vestido            |\n |      4      |        Abrigo             |\n |      5      |       Sandalia            |\n |      6      |        Camisa             |\n |      7      |  Zapatilla de deporte     |\n |      8      |        Bolsa              |\n |      9      |        Bot\u00edn              |\n\n\n<br>\n\n<a id=\"3.2\"><\/a>\n## 3.2. \u00bfC\u00f3mo son las im\u00e1genes? \n\nCada imagen es una matriz cuadrada I$_{i}$ $\\in$ M$_{m,n}$ d\u00f3nde m,n = 28 p\u00edxeles. Cada uno de los p\u00edxeles que describen la matriz representa el brillo (o color) de dicho p\u00edxel. En el caso m\u00e1s sencillo de im\u00e1genes binarias, el valor del p\u00edxel es un n\u00famero de un bit que indica el primer plano o el fondo del imagen. Un ejemplo sencillo es el s\u00edmbolo del _Yin y el  Yang_. Pero nosotros estamos en una situaci\u00f3n diferente, donde cada imagen tiene tonalidades diferentes ya que estamos en un caso donde el color es una escala de grises. \n<br>\n\nEn este caso, estamos en un formato de imagen tipo byte (el n\u00famero se almacena como un entero de 8 bits, es decir, un byte) lo que nos da un rango de posibles valores que fluct\u00faan entre 0 y 255 donde, normalmente, 255 es el blanco y 0 el negro.\n\n<br>\n<a id=\"3.3\"><\/a>\n\n## 3.3. \u00bfSe puede utilizar Deep Learning para este conjunt de datos? \nEn este escenario, tenemos que cada imagen tiene un total de 785 p\u00edxeles (28x28), es decir, tenemos un total aproximado de 55 millones de valores para todo el conjunto de datos, por lo tanto, es una cifra aceptable para un proyecto de DL\n","0384a363":"<a id=\"4.2.2\"><\/a>\n## 4.2.2. Muestra test\n","efceb0ab":"<u>__\u00bfQu\u00e9 es lo que hace \u00e9sta funci\u00f3n?__<\/u>\n\n\nSi somos un poco curosos, observamos que:\n\n 1. Separamos la variable quantitativa `label` y la asociamos a la variable`out_Y`, la cual devolver\u00e1 la _respuesta_. Destacar que la respuesta es ahora un vector, por ejemplo, si:\n \n     * `label = 0` $\\rightarrow$ `(1, 0, 0, 0, 0, 0, 0, 0, 0, 0)`\n     * `label = 1` $\\rightarrow$ `(0, 1, 0, 0, 0, 0, 0, 0, 0, 0)`\n     * $\\vdots$              \n     * `label = 9` $\\rightarrow$ `(0, 0, 0, 0, 0, 0, 0, 0, 0, 1)`\n     \n 2. Seleccionamos solo las columnas referente a los p\u00edxeles, es decir, quitamos las varibles referentes a las etiquetas (`label` y `labelName`). El resultado lo asociamos a la variable `x_vect`.\n 3. Procedemos a realizar el reescalado de los datos, cogiendo todo el vector y dividi\u00e9ndola por 255. El resultado lo asociamos a la variable `x_scaled`.\n 4. Redimensionamos cada una de las im\u00e1genes a (28, 28, 1), el formato ideal para introducirlo al modelo\n\n\n\n","741fd340":"<a id=\"4.1\"><\/a>\n## 4.1. Distribuci\u00f3n por clases\n","39514e3c":"<a id=\"4.2.1\"><\/a>\n## 4.2.1. Muestra train\n\nComo ya hemos hablado antes, mostramos la imagen con una dimension de 28x28. Para ello, definimos dos parametros _**pf**_ y _**pc**_:\n\n   * **pf** $\\rightarrow$ 28 ( _p\u00edxeles fila_ )\n   * **pc** $\\rightarrow$ 28 ( _p\u00edxeles columna_ )\n \nProcedemos a visualizar para una prenda de ropa su matrix de datos y su apariencia real: ","252c1b6a":"<a id=\"6.3\"><\/a>\n\n## 6.3 Predicciones en base al modelo\n\nAhora viene lo divertido, probar el modelo! Para ello, antens nos hemos reservado en conjunto de datos test. Con la funci\u00f3n `predict_classes()` llevaremos a cabo esta tarea.\n\nVamos a realizar las predicciones:","5b6338e5":"Al entrenar un modelo de machine learning, una de las principales cosas que desea evitar ser\u00eda el overfitting (sobreajuste). \nEl overfitting _aparece_ cuando el __modelo se ajusta bien a los datos de entrenamiento, pero no puede generalizar y hacer predicciones precisas de datos que no ha visto antes__. \n\n\nLas m\u00e9tricas del conjunto de entrenamiento nos permitem ver c\u00f3mo progresa nuestro modelo en t\u00e9rminos del entrenamiento, pero son las m\u00e9tricas del conjunto de validaci\u00f3n las que nos permitiran obtener una medida de la calidad del modelo: qu\u00e9 tan bien es capaz de hacer nuevas predicciones basadas en datos que no ha visto antes.\n\n\nUn gr\u00e1fico de curvas de aprendizaje muestra sobreajuste si:\n* El evolutivo de la __training loss__ contin\u00faa disminuyendo con la experiencia.\n* El evolutivo de la __training loss__ disminuye hasta llegar a un punto de inflexi\u00f3n y comienza a aumentar nuevamente.\n\n\nEl punto de inflexi\u00f3n en la p\u00e9rdida de validaci\u00f3n puede ser el punto en el que el entrenamiento podr\u00eda detenerse ya que la experiencia despu\u00e9s de ese punto muestra la din\u00e1mica del overfitting.\n\nComo nos podemos imagniar, un buen ajuste es el objetivo de nuestro modelo. Un buen ajuste se identifica por una p\u00e9rdida de entrenamiento y validaci\u00f3n que disminuye hasta llegar a un punto de estabilidad con una brecha m\u00ednima entre los dos valores de p\u00e9rdida final.\n\nLa p\u00e9rdida del modelo casi siempre ser\u00e1 menor en el conjunto de datos de entrenamiento que en el conjunto de datos de validaci\u00f3n. Esto significa que debemos esperar cierta brecha entre ambas curvas, esta brecha se conoce como la __brecha de generalizaci\u00f3n__.\n\nSabiendo todo esto, podemos decir que nuestro modelo en general no presneta overfitting (o muy poquito) y, por este motivo, nuestras curvas de aprendizaje nos dan a entender que nuestro modelo tiene un buen ajuste. En otras palabas:\n\n* El evolutivo de la __training loss__ disminuye hasta llegar a un punto de estabilidad.\n* El evolutivo de la __training loss__ disminuye hasta llegar a un punto de estabilidad y presentna una peque\u00f1a brecha respecto la __training loss__.\n\n__NOTA:__ El entrenamiento continuo de un buen ajuste probablemente conducir\u00e1 a un sobreajuste.\n","7c0814de":"Ahora que hemos definido como sera la red neuronal ahora, debemos elegir la __funci\u00f3n de coste__, un __optimizador__ y las __m\u00e9tricas de rendimiento__, es decir, la __compilaci\u00f3n__ del modelo.\n\nEn nuestero caso eligiremos lo siguiente:\n\n* __FUNCI\u00d3N DE COSTE__ --> `categorical_crossentrop`: Para un problema de clasificaci\u00f3n como el nestro que tiene 10 clases posibles etiquetas, necesitamos usar la funci\u00f3n de p\u00e9rdida llamada `categ\u00f3rica_crossentropy`. \n* __OPTIMIZADOR__ --> `adam`: Una de las partes m\u00e1s importantes del modelo es la elecci\u00f3n del m\u00e9todo de optimizaci\u00f3n. La elecci\u00f3n del algoritmo de optimitzaci\u00f323 marca la diferencia entre buenas y malas predicciones. En nuestro caso, hemos seleccionado el algoritmo de optimizaci\u00f3n [Adam](https:\/\/machinelearningmastery.com\/adam-optimization-algorithm-for-deep-learning\/) (existen otros como  el stochastic gradiente descent (SGD), Mini-batch gradiente descent (MBGD), ...), el cual es extensi\u00f3n del SGD. Adam, seg\u00fan los autorses, es computacionalmente eficiente, necesita pocos requisitos de memoria y, adem\u00e1s, es adecuado para grandes cantidades de datos.\n\n* __M\u00c9TRICA DE RENDIMIENTO__ --> `Accuracy`. Nos ayudar\u00e1 a validar el modelo ","6976531e":"En python exisen varias maneras de implementar un modelo, en nuestro caso, usearemos el __secuencial__ (`model = Sequential()`). Este funciona a\u00f1adiendo capas de c\u00f3digo como se puede observar en el sigueinte chunk.\n\n\n<a href='#6.1'><\/a>\n\n## 6.1 Partes del Modelo\n\n<a id=\"6.1.1\"><\/a>\n### 6.1.1 Modelo Parte 1\n    \n* __LeakyReLU__:  Definimos la funcion __LeakyReLU__ como funcion de activaci\u00f3n. Esta funci\u00f3n es m\u00e1s eficaz que la ReLU comunmente conocida.  \n* __Capa convolucional 2D (Conv2D)__:\n    * __Filtros__: Numero de filtros (kernels) utilizados en esta capa son 32\n    * __kernel_size__: Dimmensi\u00f3n del Kernel: (3 x 3)\n    * __activation__: Utilitzamos la funci\u00f3n `LeakyReLU`\n    * __kernel_initializer__: Funci\u00f3n utilizada para inicializarel kernel: `he_normal`. Solo se utiliza en la primera capa. Esta [funcion](http:\/\/man.hubwiz.com\/docset\/TensorFlow.docset\/Contents\/Resources\/Documents\/api_docs\/python\/tf\/keras\/initializers\/he_normal.html) se basa en muestras de una distribuci\u00f3n normal truncada centrada en $0$ con $sd = \\sqrt(\\frac{2}{fan_{in}})$ donde $fan_{in}$ es el n\u00famero de unidades de entrada en el tensor (\"vector\").\n    * __input_shape__: Dimensi\u00f3n de la imagen presentada a la CNN: en nuestro caso es una imagen de 28 x 28. La entrada y salida del Conv2D es un tensor 4D.\n* __MaxPooling2D__: La capa de reducci\u00f3n o pooling se coloca generalmente despu\u00e9s de la capa convolucional. La funci\u00f3n principal radica en la reducci\u00f3n de las dimensiones (anchura y altura) de entrada para la siguiente capa convolucional. Esto est\u00e1 muy bien, pero la reducci\u00f3n del volumen de datos conlleva intr\u00ednsecamente la p\u00e9rdida de informaci\u00f3n, sin embargo, la reducci\u00f3n de la informaci\u00f3n puede ser algo banefici\u00f3s para la red por tres razones:\n    \n     * Reduce la sobrecarga de c\u00e1lculos para las pr\u00f3ximas capas de la red \n     * Reduce el overfitting (o el sobreajustmanet)\n     * Favorece una computaci\u00f3n ligera\n  \n  Sin alargarme m\u00e1s, en nuestro caso aplicamos un reduccion de (2, 2), reducimos 2 en $y$ y 2 en $x$\n         \n* __Dropout__: En redes neuronales profundas, tener una gran cantidad de par\u00e1metros hace que el overfitting tome un rol importante en las predicciones. El overfitting es un problema frecuente que requiere de t\u00e9cnicas para su regulaci\u00f3n. As\u00ed pues, la t\u00e9cnica de regularizaci\u00f3n m\u00e1s popular para redes neuronales profundas es, sin duda, el __dropout__. La idea clave es que, en cada uno de los pasos del entrenamiento, desactive aleatoriamente neuronas (incluyendo las neuronas de entrada, pero excluyendo las neuronas de salida). Concretamente, cada neurona est\u00e1 determinada por una probabilidad $p$ de ser temporalmente abandonadas, lo que se llama en ingl\u00e9s neuronas en estado _dropped-out_. Esto significa que, las neuronas que pertenezcan en este estado ser\u00e1n totalmente ignoradas durante el entrenamiento. El hiperparam\u00e8metre $p$ se denomina __tasa de abandono__ o __dropout rate__ y normalmente se sit\u00faa en el 50%, es decir, $p$ = 0.5. Este valor $p$ es totalmente fluctuable y no se rigue por reglas concretas. \n\n  Pues en nuestro caso fijaremos este parametro en $p$ = 0.3\n\n_Code:_\n\n`model = Sequential()\nLeakyReLU = lambda x: tf.keras.activations.relu(x, alpha=0.1)\nmodel.add(Conv2D(32, \n                 kernel_size = (3, 3),\n                 activation = LeakyReLU,\n                 padding=\"same\",\n                 input_shape=(pf, pc, 1)))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Dropout(0.3)`\n\n                  \n\n\n\n<a id=\"6.1.2\"><\/a>\n### 6.1.2 Modelo Parte 2\n    \n\n  \n* __Capa convolucional 2D__:\n    * __Filtros__: 64\n    * __kernel_size__: (3 x 3)\n    * __activation__: Utilitzamos la funci\u00f3n `LeakyReLU`\n    * __input_shape__: 28 x 28\n* __MaxPooling2D__: (2, 2)\n* __Dropout__: 0.5\n\n_Code:_\n\n`model.add(Conv2D(64, \n                  kernel_size = (3, 3),\n                  activation = LeakyReLU,\n                  input_shape=(pf, pc, 1)))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Dropout(0.5))`\n\n\n\n\n\n<a id=\"6.1.3\"><\/a>\n### 6.1.3 Modelo Parte 3\n    \n\n\n* __Capa convolucional 2D__:\n    * __Filtros__: 128\n    * __kernel_size__: (3 x 3)\n    * __activation__: Utilitzamos la funci\u00f3n `LeakyReLU`\n* __Flatten__: Esta capa aplana la entrada y Se usa sin parametros \n* __Dense__:\n    * __unidades__: 128 (debe ser positivo)\n    * __activation__: Utilitzamos la funci\u00f3n `LeakyReLU`\n.\n* __Dropout__: 0.3\n* __Dense - Fully Connected__: Esta es la capa final (completamente conectada). \n    * __unidades__: Numero de categorias a predecir, en nuestro caso, 10.\n    * __activation__: `softmax` (est\u00e1ndar para la clasificaci\u00f3n multiclase)\n    \n\n\n_Code:_\n\n`model.add(Conv2D(128, (3, 3), activation = LeakyReLU))\nmodel.add(Flatten())                               \nmodel.add(Dense(128, activation = LeakyReLU))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(n_cat, activation = 'softmax'))`               \n","9a4e4534":"<a id=\"7\"><\/a>\n# 7. CONCLUSIONES\n\nPara resolver este complejo problema, hemos aplicado t\u00e9cnicas del Deep Learning para la predicci\u00f3n de art\u00edculos de ropa con im\u00e1genes. Hemos observado que lesprediccions hechas han sido bastante buenas, con un total de __9272__ predicciones correctas respecto __728__ err\u00f3neas. \n\nConcluimos que no ha existido overfitting ya que hemos aplicado t\u00e9cnicas de reducci\u00f3n de la dimensionalidad (MaxPooling2D), capas de regularizaci\u00f3n (Dropouts), 50 epocas, un batch size de 70 (no se ha estudiado). \n\n\nFinalmente, con el modelo entrenado, hemos comprobado que predice bastante bien dentro lo que cabe para la resoluci\u00f3n de las im\u00e1genes. Para confirmar que nuestro modelo puede generalizar, hemos introducido datos nuevos al modelo y este los ha predicho bien. \n\nConfirmamos que el modelo es bueno obteniendo una precisi\u00f3n de ~ 0.927 para los datos test.","6fd064bc":"<center><font size=\"6\">FASHION MNIST: EASY WAY TO IMPLEMENT A CONVOLUTIONAL NEURAL NETWORK (CNN)<\/font><\/center>\n\n<br>\n\n<img src=\"https:\/\/raw.githubusercontent.com\/OriolGilabertLopez\/MachineLearning\/master\/Projects\/MachineLearning\/FashionMNIST\/Auxiliars\/Images\/Clothes.JPG\" width=\"900px\">","763db33e":"Unificamos el datatype del vector a un `float`","da8ec2d7":"<a id=\"4\"><\/a>\n# 4. QUICK VIEW DE LOS DATOS\n\n<br>\nLo primero que debemos hacer es visualizar las dimensi\u00f3n del dataset:","5c7cd452":"<a id=\"8\"><\/a>\n# 8. REFERANCIAS","f67b18ba":"__INTERPRETACI\u00d3N__:\n\n\n__Ejes:__\n* En el __eje _x___ tenemos las etiquetas (o prendas) que el modelo ha predicho \n* En el __eje _y___ tenemos las etiquetas (o prendas) reales del dataset test\n\n\n\n__Valores:__\n* Caso de la etiqueta __0: Camiseta\/Top__: \n\n    1. El modelo ha predicho __895__ prendas de ropa como __Camiseta\/Top__ que realmente eran __Camiseta\/Top__\n    2. El modelo ha predicho __1__ prendas de ropa como __Pantal\u00f3n__ cuando realmente eran __Camiseta\/Top__\n    3. El modelo ha predicho __25__ prendas de ropa como __Jerseys__ cuando realmente eran __Camiseta\/Top__\n    4. El modelo ha predicho __15__ prendas de ropa como __Vestido__ cuando realmente eran __Camiseta\/Top__\n    5. El modelo ha predicho __2__ prendas de ropa como __Abrigo__ cuando realmente eran __Camiseta\/Top__\n    6. El modelo __no ha predicho ninguna Sandalia__ siendo en realida un __Camiseta\/Top__. El modelo distingue bien entre __Sandalias__ y __Camiseta\/Top__\n    7. Etc\u00e9tera\n    \n    As\u00ed pues, nos fijamos que la diagonal representa el n\u00famero de predicciones correctas y fuera de ella el n\u00famero de predicciones err\u00f3neas. \n\n\n__Valores Marginales:__\n* __Marginal de Y _(suma fila i-\u00e9ssima)___: N\u00famero de etiquetas reales. \n    1. El n\u00famero de etiquetas reales coinciden con el la suma de los marginales fila\n    2. Ex: __9: Botines__: 2 + 26 + 972 = __1000__ prendas\/etiquetas reales\n    3. Etc\u00e9tera\n \n \n* __Marginal de X _(suma columna i-\u00e9ssima)___:  N\u00famero de etiquetas predichas.\n    1. La suma de una columa no tiene por que ser el igual al n\u00famero de etiquetas reales para esa prenda, solo seria as\u00ed si el modelo fuese 100% eficaz.\n    2. Ex: __8: Bolsa__: 4 + 1 + 3 + 987 = __995__ Bolsas predichas, de las cuales  987 lo ha hecho satisfactoriamente.\n    3. Ex: __9: Botines__: 3 + 35 + 972 = __1013__ Botines predichos, de las cuales  972 lo ha hecho satisfactoriamente.\n    4. Etc\u00e9tera","6f909536":"<a id=\"1\"><\/a>\n# 1. INTRODUCCI\u00d3N\n\nActualmente, la clasificaci\u00f3n de cualquier objeto por imagen es una realidad gracias el Machine Learning. Una herramienta tan potente como \u00e9sta ha dado vida a abrir nuevas investigaciones en el mundo de la estad\u00edstica, biolog\u00eda y cualquier rama que use las ciencias computacionales. \n\n\u00a1Vamos!\n\n<br>\n\n__\u00bfQu\u00e9 encontraras en este notebook?__\n * __An\u00e1lisis Descriptivo:__ Estudiaremos con una descriptiva b\u00e1sica los datos. Hay que conocerlos antes de actuar\n * __Selecci\u00f3n del Modelo:__ Seleccionaremos una __R__ed __N__ueronal __C__onvolucional  como modelo para la clasificaci\u00f3n de las im\u00e1genes\n * __Construcci\u00f3n del Modelo:__ Estudiaremos el overfitting para que el modelo generalice las predicciones (no s\u00f3lo lo haga para sus datos de entrenamiento). Se pondr\u00e1n caps de regularizaci\u00f3n, filtros, se aplicara reducci\u00f3n de la dimensionalidad... \n * __Selecci\u00f3n del Algortimo de Optimizaci\u00f3n:__ Seleccionaremos un algoritmo adecuado para la optimizaci\u00f3n del modelo en cuanto a su convergencia que, seg\u00fan pruebas realizadas en algunos estudios, marcan la diferencia entre los buenos resultados y los malos. En nuestro caso, creemos adecuado aplicar el algoritmo de optimizaci\u00f3n __Adam__, una extensi\u00f3n del algoritmo Stochastic Gradiente Descent (SGD)\n * __Valorac\u00f3n del Modelo:__ Valoraremos la __precisi\u00f3n\/accuracy__ y la __p\u00e9rdida\/loss__ del conjuntos de datos de validaci\u00f3n. \n * __Predicciones:__ Haremos las predicciones con los datos test\n * __Valoraci\u00f3n de las Predicciones:__ Sacaremos una matriz de confusion para evaluar la calidad de las predicciones","c8300683":"Comparamos las etiquetas predecidas con las reales (`pred[:10000] == y_true[:10000]`). Luego, sacamos aquellas que hayan hecho match, es decir, que has sido predecidas correctamente (` == True`). Con la funci\u00f3n `np.where()` sacamos el valor de indice (posici\u00f3n) de la etiqueta para saber a que n\u00famero se refiere y, finalmente, con `[0]` convertimos el resultado de `tupla` a `numpy.ndarray`.   ","fe6ba9db":"\n<img src=\"https:\/\/raw.githubusercontent.com\/OriolGilabertLopez\/MachineLearning\/master\/Projects\/MachineLearning\/FashionMNIST\/Auxiliars\/Images\/SplitTrain_with_Train%26Val.png\" width=\"400px\">\n\n\n<br>","624333a3":"<a id=\"6.2\"><\/a>\n## 6.2 Evaluaci\u00f3n del modelo\n\n","f2716b00":"Segidamente, crearemos una funci\u00f3n llamada `pie_plot()` que nos graficar\u00e1 la distribuci\u00f3n de prendas de ropa seg\u00fan el conjuto de datos introducidos:","4a41ea61":"<a id=\"6\"><\/a>\n# 6. MODELO\n","0ccbad02":"Escogemos solo el set train (azul) y lo dividimos en 80% train y 20 test (o validaci\u00f3n) de la siguiente manera:","7890c154":"<a id=\"2\"><\/a>\n# 2. IMPORTACI\u00d3N DE DATOS\n\nEl primer paso que debemos realizar es importar los datos, visualizarlos y tener una idea general de como son y como se distribuyen. Nuestra ruta para importar las im\u00e1genes es la siguiente: _`\/kaggle\/input\/fashionmnist\/...`_\n\nPara ello, debemos cargar antes los datos al notebook (`+ Add data`) y seguidamente leemos los paths con la ayuda de la libreria `import pandas`. El c\u00f3digo es el siguiente: ","1883d881":"<a id=\"5.2\"><\/a>\n## 5.2. Split del datset train para el entrenamiento del modelo\n\n","b28c3a26":"paro... realmente que es lo que deberia haber predicho?. Para saber que es lo que deberia haber predicho sacamos las etiquetas reales del conjunto de entrenamiento:","e73b410c":"\n<img src=\"https:\/\/raw.githubusercontent.com\/OriolGilabertLopez\/MachineLearning\/master\/Projects\/MachineLearning\/FashionMNIST\/Auxiliars\/Images\/FullData_Tran%26Val.png\" width=\"400px\">\n<br>","36f85049":"Normalmente lo que se hace es coger los datos train y dividir dicho set en dos, una parte para entrenar el modelo y otra parte para validarlo. Luego, se predice con los datos test original (no del train)\n\nEs decir, del conjunto de datos total:","2e002544":"<a id=\"6.4\"><\/a>\n\n## 6.4 Matriz de Confusi\u00f3n: Evaluaci\u00f3n de las Predicciones\n\n\nUna buena herramienta para visualizar sobre que objetos hemos predicho mal, es la matriz de confusi\u00f3n (o clasificaci\u00f3n). Esta matriz muestra como ha clasificado el modelo cada cinjunto de prendas. ","1986ff10":"Observamos que cada prenda de ropa se distribuye por igal para cada uno de los conjuntos de datos (10 calses donde cada clase representa un 10% del total)."}}