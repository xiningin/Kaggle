{"cell_type":{"120f15ef":"code","cdd6cdaa":"code","018c0c5e":"code","d2f56e21":"code","f81c5ea5":"code","1bed56ea":"code","68c491a5":"code","ace3f6d7":"code","e2e1609f":"code","abad832b":"code","95cfc3ec":"code","83c3801a":"code","777fad29":"code","3cc9f664":"code","6c0d0676":"code","74ee5095":"code","7de055ef":"code","14e16f14":"code","10f2adf7":"code","2e793048":"code","b18b7647":"code","57296410":"code","274a96e2":"code","e1824a29":"code","8ebfabc9":"code","6f856120":"code","70f71cad":"code","fde1f4d1":"code","5e579a8f":"code","295bca2c":"code","e033b58f":"code","cf19be08":"code","2025a662":"code","588bd170":"code","4ec04ed3":"code","126a1cc4":"code","ea6ac009":"code","59142d1f":"code","f692fdbd":"code","26573033":"code","850f418a":"code","3979c775":"code","7c5ee598":"code","a67af701":"code","ad15800a":"code","7a9e6980":"code","408a227e":"code","43090dda":"code","1b9e02c7":"code","dddf2e43":"code","5c8d766e":"code","1ded2f20":"code","b0f35cfb":"code","b53b9a03":"code","3e1f60af":"code","3e4b7cf0":"code","0c5384df":"code","5d82e435":"code","790a7743":"code","44ee3d35":"code","bd330026":"code","190af715":"code","e5863130":"code","f9bf4326":"code","320f1afd":"code","292924b1":"code","f0b48f4d":"code","0411bca4":"code","7d2c26ee":"code","b9d7171f":"code","0bd97fdd":"code","4c05082e":"code","be9b5444":"code","c2f8ec4f":"code","9ec40c52":"code","e7e8a27b":"code","ca660742":"code","a82b6bd5":"code","f9c286b6":"code","c827cf72":"markdown","56bc02e1":"markdown","5a2b2fb4":"markdown","9d041b0b":"markdown","492f8a54":"markdown","d6f78d75":"markdown","eab51382":"markdown","c973b825":"markdown","c2803b4d":"markdown","f7dd10dc":"markdown","33eb8239":"markdown","47fdcf70":"markdown","01a9ecbd":"markdown","aef9283f":"markdown","31739290":"markdown"},"source":{"120f15ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cdd6cdaa":"df=pd.read_excel(\"\/kaggle\/input\/bank-loan-modelling\/Bank_Personal_Loan_Modelling.xlsx\",'Data')","018c0c5e":"df.head(5)","d2f56e21":"df=df.drop(['ID'],axis=1)","f81c5ea5":"df.head(5)","1bed56ea":"#Finding if their are any null values in the data set\ndf.isnull().sum()","68c491a5":"#Finding if any duplicate values\ndf.duplicated().sum()","ace3f6d7":"#Since all the features we have are of integer type we can describe the data\ndf.describe().transpose()","e2e1609f":"import seaborn as sns\nimport matplotlib.pyplot as plt","abad832b":"col=['Age', 'Experience', 'Income','CCAvg','Mortgage']\nj=0\ni=3\nplt.figure(figsize=(14,12))\nfor k in col :\n    plt.subplot(i,i,i*(j+1)\/\/i)\n    sns.distplot(df[k])\n    j=j+1\nplt.show()","95cfc3ec":"# Replacing negative experience with median value of the Experience column\nnegexp=df[df['Experience']<0]","83c3801a":"negexp['Experience'].value_counts()","777fad29":"negval=[-3,-2,-1]\n\nfor i in negval:\n    df['Experience']=df['Experience'].replace(negval,np.median(df['Experience']))","3cc9f664":"df['Experience'].describe()","6c0d0676":"#Finding corelation between features\ncor=df.corr()","74ee5095":"plt.figure(figsize=(10,8))\nplt.title(\"Correlation plot\")\nsns.heatmap(cor,annot=True)\nplt.show()","7de055ef":"plt.figure(figsize=(10,8))\nplt.title(\"Scatter plot for Experience and Age\")\nsns.scatterplot(x='Age',y='Experience',hue='Personal Loan',data=df)\nplt.show()","14e16f14":"#Dropping Experience from the dataset\ndf=df.drop(['Experience'],axis=1)","10f2adf7":"df.columns","2e793048":"#Plotting scatterplot for multivariate variables\ncol=['Income','CCAvg','Mortgage']\nplt.figure(figsize=(14,12))\nj=3\nk=0\nfor i in col :\n    plt.subplot(1,j,j*(k+1)\/\/j)\n    sns.scatterplot(x='Age',y=i,hue='Personal Loan',data=df)\n    k=k+1\nplt.show()","b18b7647":"#Plotting countplot for Categorical variables\ncol=['Securities Account', 'CD Account', 'Online',\n       'CreditCard']\nplt.figure(figsize=(14,12))\nj=2\nk=0\nfor i in col :\n    plt.subplot(2,j,j*(k+1)\/\/j)\n    sns.countplot(x=i,hue='Personal Loan',data=df)\n    k=k+1\n    plt.grid(True)\nplt.show()","57296410":"df.columns","274a96e2":"plt.figure(figsize=(8,6))\nsns.boxplot(x='Family',y='Income',hue='Personal Loan',data=df)\nplt.show()","e1824a29":"plt.figure(figsize=(10,8))\nsns.boxplot(x='Education',y='CCAvg',hue='Personal Loan',data=df)\nplt.show()","8ebfabc9":"df.columns","6f856120":"df=df.drop(['ZIP Code'],axis=1)","70f71cad":"df.head(5)","fde1f4d1":"#checking class balance for Personal loan\ndf['Personal Loan'].value_counts()\n","5e579a8f":"df1=df","295bca2c":"df1['Personal Loan'].value_counts()","e033b58f":"#Class label is having imbalance data so we will re-balance the class variable using upsample method\n#splitting major and minor class data frames\ndf_majority=df[df['Personal Loan']==0]\ndf_minority=df[df['Personal Loan']==1]\n","cf19be08":"print(\"Majority class shape {}\".format(df_majority.shape))\nprint(\"Minority class shape {}\".format(df_minority.shape))","2025a662":"from sklearn.utils import resample","588bd170":"#Upsampling\ndf_minority_upsample=resample(df_minority,n_samples=4520)","4ec04ed3":"#Joining both dataframes\ndf=pd.concat([df_majority,df_minority_upsample])","126a1cc4":"df['Personal Loan'].value_counts()\n","ea6ac009":"#Seperating x and y variables\nx=df.drop(['Personal Loan'],axis=1)\ny=df['Personal Loan']","59142d1f":"from sklearn.model_selection import train_test_split","f692fdbd":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)","26573033":"y_train.head(5)","850f418a":"from sklearn.tree import DecisionTreeClassifier\n","3979c775":"dt=DecisionTreeClassifier(criterion=\"gini\")","7c5ee598":"#using entropy\ndt_en=DecisionTreeClassifier(criterion=\"entropy\")","a67af701":"dt.fit(x_train,y_train)","ad15800a":"dt_en.fit(x_train,y_train)","7a9e6980":"#Predicting using gini criteria\ny_pred_dt_gini=dt.predict(x_test)","408a227e":"#Predicting using entropy criteria\ny_pred_dt_en=dt_en.predict(x_test)","43090dda":"#Checking accuracy of the model\nfrom sklearn.metrics import accuracy_score ,confusion_matrix,classification_report","1b9e02c7":"acc=accuracy_score(y_test,y_pred_dt_gini)\nprint(acc)","dddf2e43":"#Classification error\nprint(1-acc)","5c8d766e":"acc_en=accuracy_score(y_test,y_pred_dt_en)\nprint(acc_en)","1ded2f20":"#Model validaton for Gini\nconfusion_matrix(y_test,y_pred_dt_gini)\n","b0f35cfb":"#Model validaton for Entropy\nconfusion_matrix(y_test,y_pred_dt_en)","b53b9a03":"print(classification_report(y_test,y_pred_dt_gini))","3e1f60af":"#Hypertuing Decision Tree\nfrom sklearn.model_selection import GridSearchCV\n","3e4b7cf0":"dt_base=DecisionTreeClassifier()","0c5384df":"parameters={'criterion': ['gini','entropy'],'max_depth' : np.arange(1,50),'min_samples_leaf': [1,2,5,10,13,15]}","5d82e435":"grid=GridSearchCV(dt_base,parameters)","790a7743":"grid.fit(x_train,y_train)","44ee3d35":"best_dt=grid.best_params_\nprint(best_dt)","bd330026":"grid.best_score_","190af715":"model=grid.best_estimator_","e5863130":"model.fit(x_train,y_train)","f9bf4326":"y_pred_best=model.predict(x_test)","320f1afd":"#Cross validation using cross_val_score\nfrom sklearn.model_selection import cross_val_score\na=cross_val_score(model, x, y, cv=10)\nprint(a)","292924b1":"accur = np.mean(a)\nprint(accur)","f0b48f4d":"#Plotting tree\nfrom sklearn import tree\nplt.figure(figsize=(20,14))\ntree.plot_tree(model)\nplt.show()","0411bca4":"#For imbalance dataset\nx_imb=df1.drop(['Personal Loan'],axis=1)\ny_imb=df1['Personal Loan']\n","7d2c26ee":"x_train_imb,x_test_imb,y_train_imb,y_test_imb=train_test_split(x_imb,y_imb,test_size=0.3)","b9d7171f":"model.fit(x_train_imb,y_train_imb)","0bd97fdd":"y_pred_imb=model.predict(x_test_imb)","4c05082e":"accuracy_score(y_test_imb,y_pred_imb)","be9b5444":"#Predict proabilities\nprobs = model.predict_proba(x_test)","c2f8ec4f":"probs = probs[:, 1]\nprint(probs)","9ec40c52":"from sklearn import metrics\nauc = metrics.roc_auc_score(y_test, probs)\nprint(auc)\n","e7e8a27b":"from sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve(y_test, probs)\n","ca660742":"print(fpr,tpr)","a82b6bd5":"def plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","f9c286b6":"plot_roc_curve(fpr, tpr)","c827cf72":"Both Entropy and gini Models are showing 99% accuracy","56bc02e1":"Any of the columns do not have null values","5a2b2fb4":"**Age Vs Income** : People with more Income(greater than 100$) have opted for Personal loan. \n\n**Age Vs CCAvg** : Also ppl with high CCAvg have also opted for Personal loan\n\n**Age Vs Mortgage** : People who have opted for personal loan does not relate much with higher mortgage value but people with Mortgage greater than 400$ tend to take Personal Loan","9d041b0b":"Experience and Age are highly co-related and we can drop Experience feature","492f8a54":"**Age** : Mean value of Age is 45.33 and distribution is even across mean. Hence normally distibuted\n\n**Experience**: Even distribution across mean hence normally distributed\n\n**Income** : Positively skewed i.e Median is lesser than mean value\n\n**CCAvg** : Positively skewed i.e Median is lesser than mean value\n\n**Mortgage** : Positively skewed i.e Median is lesser than mean value\n","d6f78d75":"\n\n**Accuracy** =(TP+TN)\/(TP+TN+FP+FN)\n\n**Classification error** =(FP+FN)\/(TP+TN+FP+FN) or 1-Accuracy\n\n**Sensitivity: When the actual value is positive, how often is the prediction correct?**\n\n**Sensitivity** = TP \/(FN + TP)\n\n**Specificity: When the actual value is negative, how often is the prediction correct?**\n\n**Specificity** = TN \/ (TN + FP)\n\n**Precision: When a positive value is predicted, how often is the prediction correct?**\n\n**Precision** = TP \/ TP + FP\n\n","eab51382":"Replacing all the negative experience with median value","c973b825":"Experience and Age looks like highly positively co-related with each other.We can remove one of the variable  to avoide multi-collinearity problem.\n\n","c2803b4d":"People with more Online accounts tends to take more personal loan,whereas very minimal number of people who have Securities account have opted for Personal loan\nMost of the people who have Credit card do not seems to take Personal loan. And people with no CD account are taking more loans than those who have CD account","f7dd10dc":"People with high income irrespective family size have opted for laon","33eb8239":"**Personal loan** : This is our Target variable, where we have data which describe that who have opted for Persnonal Loan(0) and who have not opted for Personal Loan(1).\n\nSince ID column do not contribute much to this data set we will remove ID column\n\n","47fdcf70":"Irrespective of education people who have Credit card avg greater than 2.5 have opted for Personal Loan","01a9ecbd":"**Experience** : We can see minimum of experience is -3, which is junk value, we need to rectify columns which incorrect information.\n\nBefore correcting Experience info let's plot the distribution","aef9283f":"No duplicate value is avaliable in the dataset","31739290":"The clear sign of a machine learning overfitting is if its error on testing set is much greater than the error on training set.\n\nTo prevent overfitting, you need to add regularisation in case of Linear and SVM models. Similarly, in decision tree models you we can reduce the maximum depth. While in neural networks, we can introduce dropout layer to reduce overfitting."}}