{"cell_type":{"6081aad6":"code","f6891c65":"code","fc318137":"code","c94b1379":"code","8f4f10f7":"code","8985d367":"code","4f84553d":"code","f0ef5877":"code","3e206169":"code","2a34dd94":"markdown","609b2be2":"markdown","4eae498c":"markdown","ab40d72a":"markdown","39ff455d":"markdown","212fee0d":"markdown","99010c62":"markdown","a55180a2":"markdown","c9846d37":"markdown"},"source":{"6081aad6":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport gc\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras import *\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom sklearn.preprocessing import RobustScaler, normalize\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import mean_absolute_error\nfrom pickle import load\n!cp ..\/input\/ventilator-feature-engineering\/VFE.py .","f6891c65":"from VFE import add_features\n\ntrain_ori = pd.read_csv('..\/input\/ventilator-pressure-prediction\/train.csv')\ntargets = train_ori['pressure'].to_numpy().reshape(-1, 80)\ntrain_ori.drop(labels='pressure', axis=1, inplace=True)\ntrain = add_features(train_ori)\n# normalise the dataset\nRS = RobustScaler()\ntrain = RS.fit_transform(train)\n\n# Reshape to group 80 timesteps for each breath ID\ntrain = train.reshape(-1, 80, train.shape[-1])","fc318137":"test_ori = pd.read_csv('..\/input\/ventilator-pressure-prediction\/test.csv')\ntest = add_features(test_ori)\ntest = RS.transform(test)\ntest = test.reshape(-1, 80, test.shape[-1])","c94b1379":"# model creation\ndef create_lstm_model():\n\n    x0 = tf.keras.layers.Input(shape=(train.shape[-2], train.shape[-1]))  \n\n    lstm_layers = 4 # number of LSTM layers\n    lstm_units = [320, 305, 304, 229]\n    lstm = Bidirectional(keras.layers.LSTM(lstm_units[0], return_sequences=True))(x0)\n    for i in range(lstm_layers-1):\n        lstm = Bidirectional(keras.layers.LSTM(lstm_units[i+1], return_sequences=True))(lstm)    \n    lstm = Dropout(0.001)(lstm)\n    lstm = Dense(100, activation='relu')(lstm)\n    lstm = Dense(1)(lstm)\n\n    model = keras.Model(inputs=x0, outputs=lstm)\n    model.compile(optimizer=\"adam\", loss=\"mae\")\n    \n    return model","8f4f10f7":"# Function to get hardware strategy\ndef get_hardware_strategy():\n    try:\n        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n        # set: this is always the case on Kaggle.\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())\n    except ValueError:\n        tpu = None\n\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        tf.config.optimizer.set_jit(True)\n    else:\n        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n        strategy = tf.distribute.get_strategy()\n\n    return tpu, strategy\n\ntpu, strategy = get_hardware_strategy()","8985d367":"EPOCH = 350\nBATCH_SIZE = 512\nNFOLDS = 5\n\nwith strategy.scope():\n    kf = KFold(n_splits=NFOLDS, shuffle=True, random_state=2021)\n    history = []\n    test_preds = []\n    for fold, (train_idx, test_idx) in enumerate(kf.split(train, targets)):\n        print('-'*15, '>', f'Fold {fold+1}', '<', '-'*15)\n        X_train, X_valid = train[train_idx], train[test_idx]\n        y_train, y_valid = targets[train_idx], targets[test_idx]\n        model = create_lstm_model()\n        model.compile(optimizer=\"adam\", loss=\"mae\", metrics=[tf.keras.metrics.MeanAbsolutePercentageError()])\n\n        scheduler = ExponentialDecay(1e-3, 400*((len(train)*0.8)\/BATCH_SIZE), 1e-5)\n        lr = LearningRateScheduler(scheduler, verbose=0)\n\n        history.append(model.fit(X_train, y_train, \n                                 validation_data=(X_valid, y_valid), \n                                 epochs=EPOCH, batch_size=BATCH_SIZE, callbacks=[lr]))\n        test_pred = model.predict(test).squeeze().reshape(-1, 1).squeeze()\n        test_preds.append(test_pred)    \n        \n        # save model\n        #model.save(\"lstm_model_fold_{}\".format(fold))\n        \n        del X_train, X_valid, y_train, y_valid, model\n        gc.collect()","4f84553d":"colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red', 'tab:purple']\nplt.figure(figsize=(16,16))\nfor i in range(NFOLDS):\n    plt.plot(history[i].history['loss'], linestyle='-', color=colors[i], label='Train, fold #{}'.format(str(i)))\nfor i in range(NFOLDS):\n    plt.plot(history[i].history['val_loss'], linestyle='--', color=colors[i], label='Validation, fold #{}'.format(str(i)))\nplt.ylim(top=1)\nplt.title('Model Loss')\nplt.ylabel('MAE')\nplt.xlabel('Epoch')\nplt.legend()\nplt.grid(which='major', axis='both')\nplt.show();","f0ef5877":"def addlabels(x,y):\n    for i in range(len(x)):\n        plt.text(i,y[i],y[i], ha = 'center')\n\nfold_mae = np.zeros(NFOLDS, dtype=np.float)\nfor i in range(1):\n    fold_mae[i] = history[i].history['val_loss'][-1]\nplt.figure(figsize = (10, 5))\nnames = ['Fold #0', 'Fold #1', 'Fold #2', 'Fold #3', 'Fold #4']\nplt.bar(names, fold_mae, color ='royalblue', width = 0.4)\naddlabels(names, np.round(fold_mae, 3))\nplt.ylabel(\"MAE\")\nplt.title(\"Fold scores\")\nplt.show();","3e206169":"submission = pd.read_csv('..\/input\/ventilator-pressure-prediction\/sample_submission.csv')\nsubmission[\"pressure\"] = sum(test_preds)\/5\nsubmission.to_csv('submission.csv', index=False)","2a34dd94":"# Dataset creation\nTraining dataset is prepared with functions in the [feature engineering notebook](https:\/\/www.kaggle.com\/mistag\/ventilator-feature-engineering), which is based on [Improvement base on Tensor Bidirect LSTM](https:\/\/www.kaggle.com\/kensit\/improvement-base-on-tensor-bidirect-lstm-0-173\/notebook) by [Ken Sit](https:\/\/www.kaggle.com\/kensit).","609b2be2":"# Submission","4eae498c":"# Model creation\nModel parameters are from [Keras model tuning with Optuna](https:\/\/www.kaggle.com\/mistag\/keras-model-tuning-with-optuna). (The \"optimal parameters\" will not be exactly the same every time the optimization study is run, so the parameters used below might differ from the model tuning notebook).","ab40d72a":"Also look at the MAE for the different folds:","39ff455d":"The test set is created below, using the feature engineering function from the above mentioned notebook:","212fee0d":"This notebook implements the optimized model from [Keras model tuning with Optuna](https:\/\/www.kaggle.com\/mistag\/keras-model-tuning-with-optuna).","99010c62":"![logo](https:\/\/cdn.freelogovectors.net\/wp-content\/uploads\/2018\/07\/tensorflow-logo.png)","a55180a2":"Plot the learning curves:","c9846d37":"# Training"}}