{"cell_type":{"799ccfdb":"code","78ea2853":"code","618d71a3":"code","7beff8e2":"code","cb9cd5eb":"code","e012ad15":"code","f056b052":"code","a79f3cd8":"code","25156ef1":"code","fa9843d1":"code","b04beec1":"code","2923e199":"code","ae9d479e":"code","21cf2ae8":"code","a382b7ae":"code","2f00c741":"code","5f805c79":"code","c8165ed9":"code","09580d1b":"code","8db80c2f":"code","6cbd2220":"code","1c4569b2":"code","69e4648c":"code","c51e94cb":"code","50347f5c":"code","e098c1b5":"code","09e25cab":"code","23d9e4f4":"code","2660ed1b":"code","5ba92b14":"code","c48f89d0":"code","e381a948":"code","3861ed47":"code","0307b20c":"code","86f5abc5":"code","b2870b9e":"code","81c84ef9":"code","b9ecbe66":"code","753fea29":"code","cc7c1629":"code","7dfb2dfb":"code","03cd3910":"code","4ad8d3f7":"code","e7b82eb8":"code","2a6356ca":"code","bbd5ec16":"code","864b49cb":"code","107fa036":"code","477d5aa2":"code","70d89c08":"code","6ca79cb3":"markdown","a7e17c69":"markdown","a698d92c":"markdown","d3b44263":"markdown","7e880e3b":"markdown","218de769":"markdown","e717d426":"markdown","236e45a6":"markdown","158da3ca":"markdown","f96e801e":"markdown","84425ed5":"markdown","d8439eac":"markdown","6eb8cd70":"markdown","93eb311b":"markdown","f3c81291":"markdown","a5dbfeba":"markdown","d4709e8f":"markdown","44a1f3e8":"markdown","0cea7131":"markdown","1139750a":"markdown","7b642185":"markdown","c329f390":"markdown","3d32f0d3":"markdown","efd474ef":"markdown","dbce37b1":"markdown","fea0a58b":"markdown","61e7181e":"markdown","ce8f55b7":"markdown","75605a2f":"markdown","db951c93":"markdown","62726835":"markdown","1b3630d4":"markdown","8990c7a1":"markdown","45bed480":"markdown","a54699b0":"markdown","cb5812db":"markdown","d96bcb86":"markdown","e4bd5a4d":"markdown","2589f46e":"markdown","7c8b6f99":"markdown"},"source":{"799ccfdb":"# Load Libraries:\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport plotly.offline as pyo \nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom matplotlib.colors import ListedColormap\n#\nfrom sklearn.metrics import classification_report \nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import accuracy_score \n#\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\n#\nfrom sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis, LocalOutlierFactor\nfrom sklearn.decomposition import PCA\n#\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n#\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","78ea2853":"data = pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")\ndata.head()","618d71a3":"# Drop Unnecessary columns\ndata.drop([\"Unnamed: 32\",\"id\"],axis=1,inplace=True)\ndata.head()","7beff8e2":"# data shape:\nrow, columns = data.shape\nprint(\"Data Row:\", row)\nprint(\"Data Columns:\", columns)\n# column names:\ndata.columns\n# descriptions \ndisplay(data.describe().T)\n# class distribution \nprint(\"Data is  balanced:\",data.groupby('diagnosis').size())","cb9cd5eb":"# correlation:\ncorr_matrix = data.corr()\nsns.clustermap(corr_matrix,annot=True,fmt=\".2f\",figsize=(20,14))\nplt.title(\"Correlation Between Features\")","e012ad15":"data.info()","f056b052":"data_m = data[data.diagnosis == \"M\"]\ndata_b = data[data.diagnosis == \"B\"]","a79f3cd8":"trace = [go.Bar(x=data.diagnosis.unique(), y=(len(data_m),len(data_b)),\n               marker=dict(color=[\"blue\",\"brown\"]))]\n               \nlayout = go.Layout(title=\"Count of M = malignant, B = benign \")# \u00fcst \u00fcste gelecek \u015fekilde..\nfig = go.Figure(data=trace,layout=layout)   \npyo.iplot(fig)","25156ef1":"labels = [\"M\",\"B\"]\nvalues = [len(data_m),len(data_b)]\ntrace = [go.Pie(labels=labels, values=values,\n               marker=dict(colors=[\"blue\",\"brown\"]))]\nlayout = go.Layout(title=\"Percentage of M = malignant, B = benign \")\nfig = go.Figure(data=trace,layout=layout)\npyo.iplot(fig)","fa9843d1":"def dist_plot(data_feature): \n    hist_data = [data_m[data_feature], data_b[data_feature]]\n    \n    group_labels = ['malignant', 'benign']\n    colors=[\"blue\",\"brown\"]\n    \n    fig = ff.create_distplot(hist_data, group_labels, colors = colors)\n    fig['layout'].update(title = data_feature)\n    return pyo.iplot(fig)","b04beec1":"dist_plot('radius_mean')\ndist_plot('texture_mean')","2923e199":"# Change object to integer:\ndata[\"diagnosis\"] = [1 if item == \"M\" else 0  for item in data[\"diagnosis\"]]","ae9d479e":"y = data[\"diagnosis\"]\nx = data.drop([\"diagnosis\"],axis=1)","21cf2ae8":"columns = x.columns.tolist()","a382b7ae":"clf = LocalOutlierFactor()\ny_pred = clf.fit_predict(x)","2f00c741":"y_pred[:10]","5f805c79":"X_score = clf.negative_outlier_factor_\noutlier_score = pd.DataFrame()\noutlier_score[\"score\"] = X_score","c8165ed9":"outlier_score.head()","09580d1b":"# So make threshold: we decide about max and min of \"outlier_score\"\nthreshold = -2\nfiltre = outlier_score[\"score\"] < threshold\noutlier_index = outlier_score[filtre].index.tolist()","8db80c2f":"# Radius for our outliers\nradius = (X_score.max()-X_score)\/(X_score.max()-X_score.min())","6cbd2220":"trace0 = go.Scatter(x=x.iloc[outlier_index,0], y=x.iloc[outlier_index,1],\n                   mode=\"markers\",\n                   marker=dict(size=10,color=\"brown\"),\n                   name=\"outliers\"\n                   )\n\ntrace1 = go.Scatter(x=x.iloc[:,0], y=x.iloc[:,1],\n                   mode=\"markers\",\n                   marker=dict(size=50*radius,color=\"gold\"),\n                   name=\"real points\"\n                   )\n \nlayout = go.Layout(title=\"Outliers (Depends on Threshold Value)\",hovermode=\"closest\")\nfig = go.Figure(data=[trace0,trace1],layout=layout)\npyo.iplot(fig)","1c4569b2":"x = x.drop(outlier_index)\ny = y.drop(outlier_index)","69e4648c":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)","c51e94cb":"sc = StandardScaler()\nX_train = sc.fit_transform(x_train)\nX_test  = sc.transform(x_test) ","50347f5c":"knn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)","e098c1b5":"knn_cm = confusion_matrix(y_test,y_pred)\nknn_acc = metrics.accuracy_score(y_test, y_pred)\nprint(knn_cm)\nprint(knn_acc)","09e25cab":"# Tuning Decision Tree Model\nn_neighbors = [5,7,9,11,13,15,17,19,21]\nweights = [\"uniform\",\"distance\"]\nmetric = [\"euclidean\",\"manhattan\",\"minkowski\"]\nparam_grid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)","23d9e4f4":"knn = KNeighborsClassifier()\ngs = GridSearchCV(estimator=knn,param_grid=param_grid,scoring=\"accuracy\", cv=10)\ngrid_search = gs.fit(x_train,y_train)\nbest_score = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"Best Score:\",best_score)\nprint(\"Best Parameters:\",best_parameters)","2660ed1b":"knn = KNeighborsClassifier(metric='manhattan',n_neighbors=9,weights='distance')\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)","5ba92b14":"knn_cm = confusion_matrix(y_test,y_pred)\nknn_acc = metrics.accuracy_score(y_test, y_pred)\nprint(knn_cm)\nprint(knn_acc)","c48f89d0":"data = pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")\n# Drop Unnecessary columns\ndata.drop([\"Unnamed: 32\",\"id\"],axis=1,inplace=True)\n# Change object to integer:\ndata[\"diagnosis\"] = [1 if item == \"M\" else 0  for item in data[\"diagnosis\"]]\ny = data[\"diagnosis\"]\nx = data.drop([\"diagnosis\"],axis=1)","e381a948":"# PCA needs scaled data\nscaler = StandardScaler()\nx_scaled = scaler.fit_transform(x)","3861ed47":"# Build PCA\npca = PCA(n_components = 2)\npca.fit(x_scaled)\nX_reduced_pca = pca.transform(x_scaled)","0307b20c":"pca_data = pd.DataFrame(X_reduced_pca,columns=[\"p1\",\"p2\"])\npca_data[\"diagnosis\"] = y","86f5abc5":"hue =pca_data[\"diagnosis\"]\ndata = [go.Scatter(x = pca_data.p1,\n                   y = pca_data.p2,\n                   mode = 'markers',\n                   marker=dict(\n                           size=12,\n                           color=hue,\n                           symbol=\"pentagon\",\n                           line=dict(width=2) #\u00e7evre \u00e7izgileri\n                           ))]  \n                            \nlayout = go.Layout(title=\"PCA\",\n                   xaxis=dict(title=\"p1\"),\n                   yaxis=dict(title=\"p2\"),\n                   hovermode=\"closest\")\nfig = go.Figure(data=data,layout=layout)   \npyo.iplot(fig)                ","b2870b9e":"pca_data.head()","81c84ef9":"y_pca = pca_data.diagnosis\nx_pca = pca_data.drop([\"diagnosis\"],axis=1)","b9ecbe66":"x_train_pca, x_test_pca, y_train_pca, y_test_pca = train_test_split(x_pca, y_pca, test_size=0.33, random_state=42)","753fea29":"knn_pca = KNeighborsClassifier()\nknn_pca.fit(x_train_pca, y_train_pca)\ny_pred_pca = knn_pca.predict(x_test_pca)","cc7c1629":"knn_cm_pca = confusion_matrix(y_test_pca,y_pred_pca)\nknn_acc_pca = metrics.accuracy_score(y_test_pca, y_pred_pca)\nprint(knn_cm_pca)\nprint(knn_acc_pca)","7dfb2dfb":"# visualize \ncmap_light = ListedColormap(['orange',  'cornflowerblue'])\ncmap_bold = ListedColormap(['darkorange', 'darkblue'])\n\nh = .05 # step size in the mesh\nX = x_pca\nx_min, x_max = (X.iloc[:, 0].min() - 1), (X.iloc[:, 0].max() + 1)\ny_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = knn_pca.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(figsize=(20, 10), dpi=80)\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot also the training points\nplt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())","03cd3910":"nca = NeighborhoodComponentsAnalysis(n_components = 2, random_state = 42)\nnca.fit(x_scaled, y)\nx_nca = nca.transform(x_scaled)\nnca_data = pd.DataFrame(x_nca, columns = [\"p1\",\"p2\"])\nnca_data[\"diagnosis\"] = y","4ad8d3f7":"hue =nca_data[\"diagnosis\"]\ndata_nca = [go.Scatter(x = nca_data.p1,\n                   y = nca_data.p2,\n                   mode = 'markers',\n                   marker=dict(\n                           size=7,\n                           color=hue,\n                           symbol=\"circle\",\n                           line=dict(width=2) \n                           ))]  \n                            \nlayout = go.Layout(title=\"NCA\",\n                   xaxis=dict(title=\"p1\"),\n                   yaxis=dict(title=\"p2\"),\n                   hovermode=\"closest\")\nfig = go.Figure(data=data_nca,layout=layout)   \npyo.iplot(fig) ","e7b82eb8":"y_nca = nca_data.diagnosis\nx_nca = nca_data.drop([\"diagnosis\"],axis=1)","2a6356ca":"x_train_nca, x_test_nca, y_train_nca, y_test_nca = train_test_split(x_nca, y_nca, test_size=0.33, random_state=42)","bbd5ec16":"knn_nca = KNeighborsClassifier()\nknn_nca.fit(x_train_nca, y_train_nca)\ny_pred_nca = knn_nca.predict(x_test_nca)","864b49cb":"knn_cm_nca = confusion_matrix(y_test_nca,y_pred_nca)\nknn_acc_nca = metrics.accuracy_score(y_test_nca, y_pred_nca)\nprint(knn_cm_nca)\nprint(knn_acc_nca)","107fa036":"# visualize \ncmap_light = ListedColormap(['orange',  'cornflowerblue'])\ncmap_bold = ListedColormap(['darkorange', 'darkblue'])\n\nh = .3 # step size in the mesh\nX = x_nca\nx_min, x_max = (X.iloc[:, 0].min() - 1), (X.iloc[:, 0].max() + 1)\ny_min, y_max = X.iloc[:, 1].min() - 1, X.iloc[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = knn_nca.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure(figsize=(20, 10), dpi=80)\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot also the training points\nplt.scatter(X.iloc[:, 0], X.iloc[:, 1], c=y, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())","477d5aa2":"models = [\"Default\",\"PCA\",\"NCA\"]\nvalues = [0.946,0.952,0.984]","70d89c08":"# Compare Model's Acc\nf,ax = plt.subplots(figsize = (10,7))\nsns.barplot(x=models, y=values,palette=\"viridis\");\nplt.title(\"Compare Accuracies\",fontsize = 20,color='blue')\nplt.xlabel('Analysis',fontsize = 15,color='blue')\nplt.ylabel('Accuracies',fontsize = 15,color='blue')","6ca79cb3":"## Let's Which Points are in the correct area ","a7e17c69":"## Let's Which Points are in the correct area ","a698d92c":"<a id = \"20\"><\/a><br>\n## Visualize Of New Dataframe","d3b44263":"<a id = \"5\"><\/a><br>\n## EDA","7e880e3b":"### KNN Model via NCA Features","218de769":"* Sentisitive for outliers\n* It is problem on big data\n* Curse of Dimensionality\n* Feature Scaling\n* It is problem on imbalance data\n* Depends on K, model will check K nearst neighbour","e717d426":"<a id = \"18\"><\/a><br>\n## Make Prediction After Tuning","236e45a6":"<a id = \"13\"><\/a><br>\n## Drop Outliers","158da3ca":"# Introduction\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\nn the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server:\nftp ftp.cs.wisc.edu\ncd math-prog\/cpo-dataset\/machine-learn\/WDBC\/\n\nAlso can be found on UCI Machine Learning Repository: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29\n<br>\n<br>\n<font color = 'blue'>\n<b>Content: <\/b>\n\n1. [Prepare Problems](#1)\n    * [Load Libraries](#2)\n    * [Load Dataset](#3)    \n1. [Descriptive Analysis](#4)\n1. [EDA](#5)\n1. [Missing Values](#6)\n1. [Data Visualization](#7)\n    * [Count Plot](#8)\n    * [Pie Chart](#9)\n    * [Distribution Plot](#10)\n   \n1. [Outlier Detection](#11)\n    * [Let's The Outliers via Bubble Chart](#12)\n1. [Drop Outliers](#13)\n1. [Create Train and Test Dataset](#14)\n1. [Standardization](#15)\n1. [KNN Model](#16)\n    * [KNN Tuning](#17)\n    * [Make Prediction After Tuning](#18)\n\n1. [Principal Component Analysis (PCA)](#19)\n    * [Visualize Of New Dataframe](#20)\n    * [Classification After PCA](#21)\n1. [Neighborhood Components Analysis (NCA)](#22)\n    * [Visualize Of New Dataframe](#23)\n    * [Classification After NCA](#24)\n1. [Compare Accuracies](#25)","f96e801e":"<a id = \"9\"><\/a><br>\n## Pie Chart","84425ed5":"<a id = \"15\"><\/a><br>\n## Standardization","d8439eac":"<a id = \"2\"><\/a><br>\n## Load Libraries","6eb8cd70":"### You can make more lots via dist_plot()","93eb311b":"To get : https:\/\/scikit-learn.org\/stable\/auto_examples\/neighbors\/plot_lof_outlier_detection.html","f3c81291":"<a id = \"16\"><\/a><br>\n## KNN Model","a5dbfeba":"<a id = \"8\"><\/a><br>\n## Count Plot","d4709e8f":"Attribute Information:\n\n- 1) ID number\n* 2) Diagnosis (M = malignant, B = benign)\n3-32)\n\nTen real-valued features are computed for each cell nucleus:\n\n*  radius (mean of distances from center to points on the perimeter)\n*  texture (standard deviation of gray-scale values)\n*  perimeter\n*  area\n*  smoothness (local variation in radius lengths)\n*  compactness (perimeter^2 \/ area - 1.0)\n*  concavity (severity of concave portions of the contour)\n*  concave points (number of concave portions of the contour)\n*  symmetry\n*  fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three\nlargest values) of these features were computed for each image,\nresulting in 30 features. For instance, field 3 is Mean Radius, field\n13 is Radius SE, field 23 is Worst Radius.\n\nAll feature values are recoded with four significant digits.\n\nMissing attribute values: none\n\nClass distribution: 357 benign, 212 malignant","44a1f3e8":"<a id = \"14\"><\/a><br>\n## Create Train and Test Dataset","0cea7131":"<a id = \"7\"><\/a><br>\n## Data Visualization","1139750a":"### Outlier detection with Local Outlier Factor (LOF)","7b642185":"<a id = \"22\"><\/a><br>\n## Neighborhood Components Analysis (NCA)","c329f390":"### Prepare X and Y","3d32f0d3":"* property fit_predict :\n* is_inlierarray, shape (n_samples,)\n* Returns -1 for anomalies\/outliers and 1 for inliers.","efd474ef":"<a id = \"24\"><\/a><br>\n## Classification After NCA","dbce37b1":"<a id = \"25\"><\/a><br>\n## Compare Accuracies","fea0a58b":"<a id = \"4\"><\/a><br>\n## Descriptive Analysis","61e7181e":"<a id = \"10\"><\/a><br>\n## Distribution Plot","ce8f55b7":"<a id = \"19\"><\/a><br>\n## Principal Component Analysis (PCA)","75605a2f":"<a id = \"17\"><\/a><br>\n## KNN Tuning","db951c93":"<font color = 'red'>\n<a id = \"1\"><\/a><br>\n<h2>Prepare Problems<h2>\n<font color = 'blue'>\n      Predict whether the cancer is benign or malignant","62726835":"<a id = \"6\"><\/a><br>\n## Missing Values","1b3630d4":"<font color = 'green'>\n<h1>Breast Cancer Wisconsin (Diagnostic)<h1>","8990c7a1":"The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method which computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. This example shows how to use LOF for outlier detection which is the default use case of this estimator in scikit-learn. Note that when LOF is used for outlier detection it has no predict, decision_function and score_samples methods. See User Guide: for details on the difference between outlier detection and novelty detection and how to use LOF for novelty detection.\n\nThe number of neighbors considered (parameter n_neighbors) is typically set 1) greater than the minimum number of samples a cluster has to contain, so that other samples can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by samples that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general.","45bed480":"![](https:\/\/scikit-learn.org\/stable\/_images\/sphx_glr_plot_lof_outlier_detection_001.png)","a54699b0":"<a id = \"23\"><\/a><br>\n## Visualize Of New Dataframe","cb5812db":"<a id = \"12\"><\/a><br>\n## Let's The Outliers via Bubble Chart","d96bcb86":"### KNN Model via PCA Features","e4bd5a4d":"<a id = \"21\"><\/a><br>\n## Classification After PCA","2589f46e":"<a id = \"11\"><\/a><br>\n## Outlier Detection","7c8b6f99":"<a id = \"3\"><\/a><br>\n## Load Dataset"}}