{"cell_type":{"c0c473e2":"code","4c10fbcd":"code","1e7789e0":"code","07a3908e":"code","a6e30898":"code","95bb3951":"code","596d5056":"code","9b0658c0":"code","eaf0d4ec":"code","3600c96d":"code","a0fef756":"code","de694b78":"code","3ff2e76a":"code","a811f99d":"code","93f94cb1":"code","14e73360":"code","a1eab1ac":"code","502d4665":"code","dacf28b3":"code","05ee1f22":"code","16f7ba56":"code","51d2aad0":"code","72f54454":"code","a2d38d81":"code","e40ab99c":"code","cc9ec3c4":"code","41312bf1":"markdown","2476e69d":"markdown","3fca8e06":"markdown","d637486c":"markdown","be0e6e2a":"markdown","1b5be6b3":"markdown","6d6fc884":"markdown","66bb3071":"markdown","72aa6d48":"markdown","322ee81c":"markdown","415794e2":"markdown","a1016ec7":"markdown","57553dba":"markdown","3525fec3":"markdown","baf2d082":"markdown","a9081b9a":"markdown","00f965a4":"markdown","60513d75":"markdown","7eaf1cd6":"markdown","1434adc2":"markdown","420d7be3":"markdown","615e9809":"markdown"},"source":{"c0c473e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.offline as py#visualization\npy.init_notebook_mode(connected=True)#visualization\nimport plotly.graph_objs as go#visualization\nimport plotly.tools as tls#visualization\nimport plotly.figure_factory as ff#visualization\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\nimport matplotlib.pyplot as plt#visualization\n%matplotlib inline\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4c10fbcd":"import pandas as pd\ntweets = pd.read_csv(\"..\/input\/clinton-trump-tweets\/tweets.csv\")\ntweets = tweets[[ 'handle', 'text', 'is_retweet', 'original_author', \n                 'time', 'lang', 'retweet_count', 'favorite_count']]","1e7789e0":"tweets.head()","07a3908e":"tweets['lang'].value_counts()","a6e30898":"def language(df) :\n    if df[\"lang\"] == \"en\" :\n        return \"English\"\n    elif df[\"lang\"] == \"es\" :\n        return \"Spanish\"\n    else :\n        return \"Other\"\n\ntweets[\"lang\"] = tweets.apply(lambda tweets:language(tweets),axis = 1)\n\n\n# datetime convert\nfrom datetime import datetime\ndate_format = \"%Y-%m-%dT%H:%M:%S\" \ntweets[\"time\"]   = pd.to_datetime(tweets[\"time\"],format = date_format)\ntweets[\"hour\"]   = pd.DatetimeIndex(tweets[\"time\"]).hour\ntweets[\"month\"]  = pd.DatetimeIndex(tweets[\"time\"]).month\ntweets[\"day\"]    = pd.DatetimeIndex(tweets[\"time\"]).day\ntweets[\"month_f\"]  = tweets[\"month\"].map({1:\"JAN\",2:\"FEB\",3:\"MAR\",\n                                        4:\"APR\",5:\"MAY\",6:\"JUN\",\n                                        7:\"JUL\",8:\"AUG\",9:\"SEP\"})","95bb3951":"tweets['lang'].value_counts()","596d5056":"data=pd.concat([tweets.handle, tweets.text], axis=1)       # data icinde handle ve text kisimini ayirip concate edip DF yaptik. cunku diger kisimlara ihtiyacimiz olmayacak","9b0658c0":"data.dropna(axis=0, inplace=True)                  # bos gozlem yerlerinin satirlarini sildik","eaf0d4ec":"data.handle.value_counts()  ","3600c96d":"#Total number of tweets by both of the twitter handles\nsns.countplot(x='handle', data = tweets)","a0fef756":"#Number of tweets by the months\nmonthly_tweets = tweets.groupby(['month', 'handle']).size().unstack()\nmonthly_tweets.plot(title='Monthly Tweet Counts', colormap='copper')","de694b78":"#trump tweets without retweets\ntweets_trump   = (tweets[(tweets[\"handle\"] == \"realDonaldTrump\") &\n                         (tweets[\"is_retweet\"] == False)].reset_index()\n                  .drop(columns = [\"index\"],axis = 1))\n\n#trump tweets with retweets\ntweets_trump_retweets   = (tweets[(tweets[\"handle\"] == \"realDonaldTrump\") &\n                                  (tweets[\"is_retweet\"] == True)].reset_index()\n                                  .drop(columns = [\"index\"],axis = 1))\n\n#hillary tweets without retweets\ntweets_hillary  = (tweets[(tweets[\"handle\"] == \"HillaryClinton\") &\n                            (tweets[\"is_retweet\"] == False)].reset_index()\n                              .drop(columns = [\"index\"],axis = 1))\n\n#hillary tweets with retweets\ntweets_hillary_retweets  = (tweets[(tweets[\"handle\"] == \"HillaryClinton\") &\n                            (tweets[\"is_retweet\"] == True)].reset_index()\n                              .drop(columns = [\"index\"],axis = 1))","3ff2e76a":"plt.style.use('ggplot')\n\nplt.figure(figsize = (13,6))\nplt.subplot(121)\ntweets[tweets[\"handle\"] ==\n       \"realDonaldTrump\"][\"is_retweet\"].value_counts().plot.pie(autopct = \"%1.0f%%\",\n                                                                wedgeprops = {\"linewidth\" : 1,\n                                                                              \"edgecolor\" : \"k\"},\n                                                                shadow = True,fontsize = 13,\n                                                                explode = [.1,0.09],\n                                                                startangle = 20,\n                                                                colors = [\"#ff0026\",\"#fbff00\"]\n                                                               )\nplt.ylabel(\"\")\nplt.title(\"Percentage of Trump retweets\")\n\n\nplt.subplot(122)\ntweets[tweets[\"handle\"] ==\n       \"HillaryClinton\"][\"is_retweet\"].value_counts().plot.pie(autopct = \"%1.0f%%\",\n                                                                wedgeprops = {\"linewidth\" : 1,\n                                                                              \"edgecolor\" : \"k\"},\n                                                                shadow = True,fontsize = 13,\n                                                                explode = [.09,0],\n                                                                startangle = 60,\n                                                                colors = [\"#0095ff\",\"#fbff00\"]\n                                                               )\nplt.ylabel(\"\")\nplt.title(\"Percentage of Hillary retweets\")\nplt.show()","a811f99d":"# regular expression yapalim. Burada gulucuk ve benzeri ifadeleri silmek icin\n\nimport re\n\nfirst_text = data.text[5]                   # butun datadan once 5.siradaki datanin temizlenmesini yapalim bakalim temizlenmis ise butun dataya uygulamaya calisacagiz\ntext = re.sub(\"[^a-zA-Z]\",\" \", first_text)\ntext = text.lower()                        # ilk ve son haline gore description icindeki elemanlari kucultmesini istedik ve yazdirirsak kuculdugunu gorebiliriz","93f94cb1":"text","14e73360":"import nltk         # nltk nin download kiti bulunmaktadir. Bisey ifade etmeyen kelimeleri(Stop words ler ornegin 'an' veya 'the') cikarmamiz gerekiyor. Eger cikarmazsak 'an' veya 'the' kelimeleri enfazla kullanilanlar olarak gorunmus olacak bunlar bizim icin bisey ifade etmiyor\nnltk.download(\"stopwords\")          # hazir olarak bulunan stopwordslari indirip uygulayabiliriz\nnltk.download(\"punkt\")\n\nfrom nltk.corpus import stopwords","a1eab1ac":"text = nltk.word_tokenize(text)       # tokenize ettik yani kelimeleri birbirinden ayirdik. ayirmak icin split kullandik cunku mesela \"doesn't\" vy \"shouldn't\" kelimesini \"does\" ve \"not\" olarak ayiramiyor bunun icin tokenize kullandik\n","502d4665":"text = [ word for word in text if not word in set(stopwords.words(\"english\"))]","dacf28b3":"text","05ee1f22":"# kelimeleri sade haline indirgemek icin\n\nimport nltk as nlp\nnltk.download('wordnet')\n\nlemma = nlp.WordNetLemmatizer()\ntext = [lemma.lemmatize(word) for word in text]\n\ntext = \" \".join(text)","16f7ba56":"# simdi BUTUN VERI dekilere bunu uygulamak icin for dongusu olusturalim:\n\ntext_list = []\nfor text in data.text:\n    text = re.sub(\"[^a-zA-Z]\",\" \", text)\n    text= text.lower()\n    text= nltk.word_tokenize(text)\n    text = [ word for word in text if not word in set(stopwords.words(\"english\"))]\n    lemma = nlp.WordNetLemmatizer()\n    text = [lemma.lemmatize(word) for word in text]\n    text = \" \".join(text)\n    text_list.append(text)","51d2aad0":"text_list","72f54454":"# bag of words: kac kelime kullanmak istiyorsak kendimiz belirliyoruz. Duygu kelimelerini verip analizini yapacagiz\n\nfrom sklearn.feature_extraction.text import CountVectorizer        # bag of words olusturmak icin \n\nmax_features = 5000               # max ... kadar kelimeye baksin\n\n# Simdi modelimizi olusturalim\ncount_vectorizer = CountVectorizer(max_features=max_features, stop_words=\"english\")\nsparce_matrix = count_vectorizer.fit_transform(text_list).toarray()                     # modelimizi fitleyip array donusturduk\n","a2d38d81":"print(f\"{max_features} most used words:\\n\\n{count_vectorizer.get_feature_names()}\")","e40ab99c":"plt.figure(figsize = (12,8))\nsns.countplot(x = \"month_f\",hue = \"handle\",palette = [\"#ff0026\",\"#0095ff\"],\n              data = tweets.sort_values(by = \"month\",ascending = True),\n             linewidth = 1,edgecolor = \"k\"*tweets_trump[\"month\"].nunique())\nplt.grid(True)\nplt.title(\"Tweets month by month in 2016 election\")\nplt.show()","cc9ec3c4":"from textblob import TextBlob\n\nbloblist_desc = list()                                  # butun tweet ler listeleniyor\n\ndf_tweet_descr_str=tweets['text'].astype(str)           # text ler ayiklaniyor\n\nfor row in df_tweet_descr_str:\n    blob = TextBlob(row)\n    bloblist_desc.append((row,blob.sentiment.polarity, blob.sentiment.subjectivity))\n    df_tweet_polarity_desc = pd.DataFrame(bloblist_desc, columns = ['sentence','sentiment','polarity'])\n \ndef f(df_tweet_polarity_desc):\n    if df_tweet_polarity_desc['sentiment'] > 0:\n        val = \"Positive\"\n    elif df_tweet_polarity_desc['sentiment'] == 0:\n        val = \"Neutral\"\n    else:\n        val = \"Negative\"\n    return val\n\ndf_tweet_polarity_desc['Sentiment_Type'] = df_tweet_polarity_desc.apply(f, axis=1)\n\nplt.figure(figsize=(10,10))\nsns.set_style(\"whitegrid\")\nax = sns.countplot(x=\"Sentiment_Type\", data=df_tweet_polarity_desc)","41312bf1":"### Let's check what we have in data.","2476e69d":"## Tweets month by month","3fca8e06":"### Pure version of words.","d637486c":"### Now, lets download nltk for using 'stopwords' and 'punkt'. These are extracting some words like 'an', 'the',...","be0e6e2a":"### Using Regular expression for delete or cleaning of some expression. We are cleaning just for 5.row. If it is cleaned then we will use all data.","1b5be6b3":"## Categorize the text column into Positive and Negative sentiments using TextBlob\n\n### Sentiment Analysis on the entiere dataset ","6d6fc884":"## Read Data","66bb3071":"## Time Analysis of the number of tweets","72aa6d48":"### Most used words determining number of tweets.","322ee81c":"### We cleaned 5.row in data. Now, we are doing for all data and add a list.","415794e2":"Let's check...","a1016ec7":"### Overall Tweets and Retweets visualization","57553dba":"### Splitting words with tokenize modul from nltk.","3525fec3":"## Data manipulation\n\n### Editing language names. As you see we have 3 main language but \"und\" is Undetermined then lets collect them as called \"Other\". And convert to date format and extract hour.","baf2d082":"### Let's have a look as general view.","a9081b9a":"### If you want to how much words to determine in the words list, we are using CountVectorizer(for bag of words).","00f965a4":"### Let's see every words in the list.","60513d75":"### Null values of rows are throwing up.","7eaf1cd6":"### Cleaning all useless words with stopwords.","1434adc2":"### Lets have a look","420d7be3":"# Conclusion\n\n* If this tutorial is not enough you can check NLP for Beginners prepared by \n    - https:\/\/www.kaggle.com\/mogady\/kickstarter-s-nlp-anlaysis\n* After this tutorial, my aim is to prepare 'kernel' which is connected to Recommendation System 'The Movies Dataset' data.\n* If you have any suggestions, please could you write for me? I wil be happy for comment and critics!\n* Thank you for your suggestion and votes ;)","615e9809":"### Seperate 'handle' and 'text' values then make DataFrame using concate."}}