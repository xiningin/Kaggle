{"cell_type":{"4341bb46":"code","c6f1c72f":"code","fb9fdc75":"code","bf5a8197":"code","392f7bed":"code","7ef4c9bd":"code","a6ed8b46":"code","4b64d269":"code","7a455a0a":"code","6895daf7":"code","1a2dbe5b":"code","61202af6":"code","e6ff418f":"code","730326ef":"code","477d11fa":"code","94b29e17":"code","d6cbd7d1":"code","e5fd73d5":"code","871e1f9a":"code","4dff65a7":"code","f3e300cc":"code","dc184c19":"code","d32e7cbe":"code","e18eed96":"code","1882dad1":"code","0cfb28f0":"code","d021b4a5":"code","5599a8b3":"code","dac09cce":"code","8af204c1":"code","92eec80d":"code","c1becc66":"code","21d6656f":"code","eb5daa8f":"code","84cc354b":"code","46e01941":"code","5ad1b435":"code","3563def2":"code","aa001705":"code","08b4d788":"code","9a4f583e":"code","594196df":"code","1c16a08a":"code","f0327e76":"code","aabad8d5":"code","dd42d3b3":"code","f715009f":"code","b752f07a":"code","e5d27144":"code","6266ef4a":"code","c0f33c69":"code","955480e3":"code","629ef008":"code","fe3a6961":"code","ce619284":"code","e2747ce2":"code","6ce743fb":"code","7033ac12":"code","929d34ec":"code","10852c1f":"code","d9c95625":"code","05b9e0c6":"code","c1339842":"code","12dfc349":"code","ad70740a":"code","7faa9907":"code","94f854cf":"code","bde3473d":"code","025bef9a":"code","731ff056":"code","967a1405":"code","dff9df05":"code","e7e87ddf":"code","44b4cad9":"code","0b161126":"code","338a8b39":"code","e05584b0":"code","593ee1ff":"code","af5bb286":"code","e590e048":"code","af044d9e":"code","05e3573b":"code","b4458797":"code","6fb56031":"code","a99e5d3c":"code","685b8bc1":"code","f91844f5":"code","e5c7a1f8":"code","5b2db81c":"code","cd799902":"code","29b0a55c":"code","23d0f706":"code","8cbf6277":"code","942ff2f0":"code","b964bbd1":"code","219aefea":"code","80162bc5":"code","6188d5e3":"code","e673e94c":"code","709d7af9":"code","65bc1475":"code","60a2108c":"code","544cb531":"code","dc3581fd":"code","dcfe8734":"code","9a6e019b":"code","5878edc6":"code","93c8dd1b":"code","54c9c344":"code","5ad9754a":"code","0f038af7":"code","04a2de91":"code","66427036":"code","b69b365b":"code","f290ee15":"code","147db327":"code","168a801b":"code","592bc413":"code","53b34a02":"code","aa370a62":"code","0d55526c":"code","2516f372":"code","80c1fa49":"code","724f9f8d":"code","a8877eea":"code","4dbede70":"code","7c4f4c6e":"code","84681e08":"markdown","3f1b98a8":"markdown","49a0e7a4":"markdown","4eec6199":"markdown","bb82a7c6":"markdown","7784323a":"markdown","88ed40a4":"markdown","039cd241":"markdown","4137c4db":"markdown","99eef19e":"markdown","ec2327fc":"markdown","cc60d3bb":"markdown","1e4b7867":"markdown","2cd60a02":"markdown","d6e70ddf":"markdown","3d1a613f":"markdown","dc822574":"markdown","bfaf3c02":"markdown","5700cbb4":"markdown","5ecc5098":"markdown","f4e904b8":"markdown","5f9c530c":"markdown","ea645df4":"markdown","9a357b69":"markdown","6f3be5f4":"markdown","78189ced":"markdown","2df7594c":"markdown","8e64c38d":"markdown","d6de4c7d":"markdown","bc80fd15":"markdown","914f43cd":"markdown","2e72a227":"markdown","ba5bf0b7":"markdown","b55f8948":"markdown","f41f860e":"markdown","706b2875":"markdown","6837bdb0":"markdown"},"source":{"4341bb46":"!pip install graphviz","c6f1c72f":"import pandas as pd \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nfrom datetime import datetime\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.model_selection import train_test_split\nimport lightgbm as lgbm\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, roc_auc_score, roc_curve\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\n\nfrom sklearn.tree import export_graphviz\nimport graphviz\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","fb9fdc75":"DIR_PATH = '\/Users\/carlosperezricardo\/Documents\/data\/'\nDIR_PATH = '..\/input\/easymoneygrupo5\/'\n\ncommercial_activity = pd.read_csv(DIR_PATH+'commercial_activity_df.csv', encoding='utf-8')\ncommercial_activity.drop(columns=['Unnamed: 0'], inplace=True)\n\nproducts = pd.read_csv(DIR_PATH+'products_df.csv', encoding='utf-8')\nproducts.drop(columns=['Unnamed: 0'], inplace=True)\n\nsociodemographic = pd.read_csv(DIR_PATH+'sociodemographic_df.csv', encoding='utf-8')\nsociodemographic.drop(columns=['Unnamed: 0'], inplace=True)","bf5a8197":"df_whole = pd.merge( commercial_activity, products, on = ['pk_cid','pk_partition'] )\ndf_whole = pd.merge( df_whole, sociodemographic, on=['pk_cid','pk_partition'] )","392f7bed":"boolean_cols = [\"short_term_deposit\", \"loans\", \"mortgage\", \"funds\", \"securities\",\"long_term_deposit\", \"em_account_pp\", \"credit_card\", \"payroll_account\", \"emc_account\", \"debit_card\", \"em_account_p\", \"em_acount\", \"payroll\", \"pension_plan\"] \n\nfor x in boolean_cols:\n    df_whole[x] = df_whole[x].astype(bool)","7ef4c9bd":"df_whole.head(3).T","a6ed8b46":"del commercial_activity, products, sociodemographic","4b64d269":"partitions = ['2018-01-28','2018-02-28','2018-03-28','2018-04-28','2018-05-28','2018-06-28', \\\n    '2018-07-28','2018-08-28','2018-09-28','2018-10-28','2018-11-28','2018-12-28','2019-01-28', \\\n        '2019-02-28','2019-03-28','2019-04-28','2019-05-28']\n\nlist_products = ['short_term_deposit','loans','mortgage','funds','securities',\n    'long_term_deposit','em_account_pp','credit_card','pension_plan',\n    'payroll_account','emc_account','debit_card','em_account_p','em_acount']\n\nproducts_dict = {\"short_term_deposit\":\"ahorro e inversi\u00f3n\", \"loans\":\"financiaci\u00f3n\", \"mortgage\":\"financiaci\u00f3n\", \n    \"funds\":\"ahorro e inversi\u00f3n\", \"securities\":\"ahorro e inversi\u00f3n\", \"long_term_deposit\":\"ahorro e inversi\u00f3n\", \n    \"em_account_pp\":\"cuenta\", \"credit_card\":\"financiaci\u00f3n\", \"payroll_account\":\"cuenta\", \"pension_plan\":\"ahorro e inversi\u00f3n\", \n    \"emc_account\":\"cuenta\", \"debit_card\":\"financiaci\u00f3n\", \"em_account_p\":\"cuenta\", \"em_acount\":\"cuenta\"}\n\ncost_product = {'cuenta':10, 'ahorro e inversi\u00f3n':40, 'financiaci\u00f3n':60}","7a455a0a":"def determinar_altas(data):\n    data.columns=['pk_partition','pk_cid','product']\n    data['prev'] = data.groupby('pk_cid')['product'].shift(1)\n    data['diff'] = data['product'] - data['prev']\n\n    # Solo queremos 1 \n    data['diff'] = np.where( (data['product']==1) & (data['diff'].isna()) & (data['pk_partition']!='2018-01-28'), 1, data['diff'] )\n    data['diff'].fillna(0,inplace=True)\n    data['diff'] = np.where( data['diff'] == -1, 0, data['diff'] )\n\n    return data['diff']","6895daf7":"df = df_whole.copy(deep=True)","1a2dbe5b":"for prod in list_products:\n    df[prod] = determinar_altas( df_whole[['pk_partition','pk_cid',prod]] )\n    print(prod)\n","61202af6":"# Total altas\nproducts_dict2 = {\"short_term_deposit\":\"ahorros\", \"loans\":\"financiacion\", \"mortgage\":\"financiacion\", \n    \"funds\":\"ahorros\", \"securities\":\"ahorros\", \"long_term_deposit\":\"ahorros\", \n    \"em_account_pp\":\"cuenta\", \"credit_card\":\"financiacion\", \"payroll_account\":\"cuenta\", \"pension_plan\":\"ahorros\", \n    \"emc_account\":\"cuenta\", \"debit_card\":\"financiacion\", \"em_account_p\":\"cuenta\", \"em_acount\":\"cuenta\"}\n\ndf['total_cuenta'] = 0\ndf['total_ahorros'] = 0\ndf['total_financiacion'] = 0\n\nfor key, value in products_dict2.items():\n    df['total_'+value] += df[key] \n\ndf['total'] = df[list_products].sum(axis=1)","e6ff418f":"df = df.sort_values(['pk_cid','pk_partition'])\ndf.head(3).T","730326ef":"le = LabelEncoder()\nle.fit(df['entry_channel'])\ndf['entry_channel'] = le.transform(df['entry_channel'])","477d11fa":"le = LabelEncoder()\nle.fit(df['gender'])\ndf['gender'] = le.transform(df['gender'])","94b29e17":"del le","d6cbd7d1":"data = df[['pk_cid','pk_partition','total']]","e5fd73d5":"data['pk_partition'] = data['pk_partition'].astype(object)\naltas = data[ data['total'] >= 1]\n\n\naltas_pt = pd.pivot_table(altas, values='total', index=['pk_cid'],\n                    columns=['pk_partition'], aggfunc=np.max)\n    \n    #altas_pt.fillna( '2001-01-01', inplace=True)\n    #print(altas_pt)\n    #print(altas_pt.isnull().sum())\n    \ndata = pd.merge( data, altas_pt, how='left', on='pk_cid' )\n    \ncols = ['pk_cid','pk_partition']\ndata.columns = cols + partitions # pone las particiones como datetime\n\nfor part, i in zip(partitions, range(len(partitions))):\n    data[part] = np.where(data[part] >= 1, 1, data[part])\n\n    # No han comprado nada ese mes\n    data[part] = np.where(data[part] == 0, -1, data[part])\n    # Han comprado 1 producto o mas\n    data[part] = np.where(data[part] == 1, i, data[part])\n    # No aparece info de ellos en esa partition NaN o no compraron nada\n    data[part] = np.where(data[part].isna(), -1, data[part])\n        \n    data[part] = data[part].replace( {i:part, -1:'2001-01-01'})\n#   print( data[part].value_counts() )\n    data[part] = pd.to_datetime(data[part])\n\n","871e1f9a":"data['pk_partition'] = pd.to_datetime(data['pk_partition'])\n    #data[partitions].fillna('2021-01-01', inplace=True)\n    #print(data)\n\n## CALCULAMOS TIEMPO DESDE ULTIMA COMPRA\ndata['last_compra'] = datetime(2001,1,1)\n\nfor part in partitions:\n    data['last_compra'] = np.where( (data[part] > data['last_compra']) & (part <= data['pk_partition']), data[part], data['last_compra'])\n\ndata['tiempo_ult_compra'] = round(((data['pk_partition'] - data['last_compra'])\/np.timedelta64(1, 'M')))\n\ndata['prev'] = data.groupby('pk_cid')['tiempo_ult_compra'].shift(1)\n\ndata['tiempo_ult_compra'] = np.where( data['tiempo_ult_compra']==0, data['prev']+1, data['tiempo_ult_compra'])","4dff65a7":"data[ data['pk_cid'] == 17457]","f3e300cc":"data['tiempo_ult_compra'] = np.where( data['tiempo_ult_compra'] < 0, 20, data['tiempo_ult_compra'] )\ndata['tiempo_ult_compra'] = np.where( data['tiempo_ult_compra'] > 100, 20, data['tiempo_ult_compra'] )\ndata['tiempo_ult_compra'] = np.where( data['tiempo_ult_compra'].isna(), 20, data['tiempo_ult_compra'] )","dc184c19":"data_to_join = data[['pk_cid','pk_partition','tiempo_ult_compra']]\n\n# Anado si ha comprado antes\ndata_to_join['compra_antes'] = np.where(data_to_join['tiempo_ult_compra'] == 20, 0, 1)\n#data_to_join['pk_partition'] = data_to_join['pk_partition']","d32e7cbe":"df['pk_partition'] = pd.to_datetime(df['pk_partition'])","e18eed96":"df = pd.merge(df, data_to_join, on=['pk_cid','pk_partition'], how='left')","1882dad1":"del data, data_to_join, altas_pt, altas","0cfb28f0":"df = df[df['tiempo_ult_compra'] != 20]","d021b4a5":"#\n#cols_to_melt = []\n\n#for x in range(1,max_):\n#    df['prod'+str(x)] = 0\n#    cols_to_melt.append('prod'+str(x))","5599a8b3":"df.head(3).T","dac09cce":"#list_ = list_products.copy()\n#list_.append('pk_partition')\n#list_.append('pk_cid')\n\n#for number in range(1,max_):\n#    df[df['total'] == number][list_]\n#    for key, row in df[df['total'] == number][list_].iterrows():\n#        #print(key, row)\n#        #clients = row[row != 0].index.to_list()\n#        #print(row['pk_partition'], row['pk_cid'])\n#        client = row['pk_cid']\n#        partition = row['pk_partition']\n#        prods = row.T[row.T != 0].index.to_list()\n        \n#        for num in range(1,number+1):\n#            df['prod'+str(num)] = np.where((df['pk_cid']==client)&(df['pk_partition']==partition), cost_product[products_dict[prods[num-1]]], df['prod'+str(num)] )\n#    \n#   print(number)\n#    #print(cost_product[products_dict[key]])\n    \n    #df['prod1'] = np.where((df['pk_cid'].isin(clients)) & (df['prod1']==0), cost_product[products_dict[key]], df['prod1'])\n    #df['prod2'] = np.where((df['pk_cid'].isin(clients)) & (df['prod1']!=0) & (df['prod2']==0), cost_product[products_dict[key]], df['prod2'])\n    #df['prod3'] = np.where((df['pk_cid'].isin(clients)) & (df['prod1']!=0) & (df['prod2']!=0) & (df['prod3']==0), cost_product[products_dict[key]], df['prod3'])\n    #df['prod4'] = np.where((df['pk_cid'].isin(clients)) & (df['prod1']!=0) & (df['prod2']!=0) & (df['prod3']!=0) & (df['prod4']==0), cost_product[products_dict[key]], df['prod4'])\n    #df['prod5'] = np.where((df['pk_cid'].isin(clients)) & (df['prod1']!=0) & (df['prod2']!=0) & (df['prod3']!=0) & (df['prod4']!=0) & (df['prod5']==0), cost_product[products_dict[key]], df['prod5'])","8af204c1":"#for x in range(2,max_):\n#    df['prod'+str(x)] = np.where(df['prod'+str(x)]==0,np.nan,df['prod'+str(x)])","92eec80d":"df.shape","c1becc66":"client_segmentation = pd.read_csv(DIR_PATH+'client_segmentation.csv')\ntry:\n    client_segmentation.drop('Unnamed: 0',axis=1, inplace=True)\nexcept:\n    pass\n\nif client_segmentation[client_segmentation['cluster'] == 0].shape[0] > 0:\n    client_segmentation['cluster'] += 1\n\ndf = pd.merge(df, client_segmentation, how='left', on='pk_cid')","21d6656f":"del client_segmentation","eb5daa8f":"df.info()","84cc354b":"boolean_cols = [\"short_term_deposit\", \"loans\", \"mortgage\", \"funds\", \"securities\",\"long_term_deposit\", \"em_account_pp\", \"credit_card\", \"payroll_account\", \"emc_account\", \"debit_card\", \"em_account_p\", \"em_acount\", \"payroll\", \"pension_plan\"] \n\nfor x in boolean_cols:\n    df[x] = df[x].astype('int8')","46e01941":"df.info()","5ad1b435":"def rolling_sum_shift(df, col, shifts):\n    rolling = df.groupby('pk_cid')[[col]].rolling(window = 17, min_periods=1).sum()\n    rolling.reset_index(inplace=True)\n    for shift in shifts:\n        rolling['shift_'+str(shift)] = rolling.groupby('pk_cid')[[col]].shift(shift)\n        rolling['shift_'+str(shift)].fillna(0,inplace=True)\n        rolling['shift_'+str(shift)] = rolling['shift_'+str(shift)].astype('int')\n        \n    return rolling","3563def2":"total_cols = ['total_cuenta','total_ahorros','total_financiacion','total']\nshifts = [1,2,3,4,5,6]\n\nfor col in total_cols:\n    new_shift_cols=[ col+'_shift_'+str(x) for x in shifts ]\n    shift_cols=[ 'shift_'+str(x) for x in shifts ]\n    \n    add = rolling_sum_shift(df, col, shifts)\n    \n    for x in add.columns.to_list():\n        add[x] = add[x].astype('int8')\n        \n    #for new_col in shift_cols:\n    df[new_shift_cols] = add[shift_cols]\n    \n    print(col)","aa001705":"del add","08b4d788":"df.head(3).T","9a4f583e":"# Calculamos nuevas altas y lo gastado en cada mes pasado\nmeses = [1,2,3,4]\nfor mes in meses:  \n    df['active_customer_'+str(mes)] = df['active_customer'].shift(mes)\n    \n    df['gastado_shift_'+str(mes)] = df['total_cuenta_shift_'+str(mes)]*10 + df['total_ahorros_shift_'+str(mes)]*40 \\\n        + df['total_financiacion_shift_'+str(mes)]*60\n    \n    df['altas_shift_'+str(mes)] = df['total_shift_'+str(mes)] - df['total_shift_'+str(mes+1)]\n    df['altas_cuenta_shift_'+str(mes)] = df['total_cuenta_shift_'+str(mes)] - df['total_cuenta_shift_'+str(mes+1)]\n    df['altas_ahorros_shift_'+str(mes)] = df['total_ahorros_shift_'+str(mes)] - df['total_ahorros_shift_'+str(mes+1)]\n    df['altas_financiacion_shift_'+str(mes)] = df['total_financiacion_shift_'+str(mes)] - df['total_financiacion_shift_'+str(mes+1)]\n\ndf.fillna(0,inplace=True)","594196df":"target_ = 'altas_shift_'+str(1)\ndata = df[df[target_] != 0]\ngb_df = data.groupby(['pk_partition','active_customer'])[target_].agg(['sum','mean'])","1c16a08a":"def gb_feature_generator(df_, meses, gb_list, target, calculations, filter_positives=False):\n\n    #meses = [1,2,3]\n    #gb_list = ['pk_partition','cluster']\n    #target = ['altas_shift_1']\n    #calculations = ['mean','sum']\n\n    for mes in meses: \n        #print(mes)   \n        target_ = target+str(mes)\n        \n        if filter_positives and df_[df_[target_] != 0].shape[0] != 0:\n            data = df_[df_[target_] != 0]\n            gb_df = data.groupby(gb_list)[target_].agg(calculations)\n        else:\n            gb_df = df_.groupby(gb_list)[target_].agg(calculations)\n\n        new_cols = []\n        for cal in calculations:\n            new_cols.append(target_+'_'+gb_list[1]+'_'+cal)\n        #print(new_cols)    \n        gb_df.columns = new_cols\n        gb_df.reset_index(inplace=True)\n        \n        #print(gb_df.head())\n        df_ = pd.merge(df_, gb_df, how='left', on=gb_list)\n    \n    return df_\n#altas_partition_cluster_gb = df.groupby(['pk_partition','cluster'])['altas_shift_1'].agg(['mean','sum'])\n#altas_partition_cluster_gb.reset_index()","f0327e76":"print('Initial shape: ',df.shape)\n\n# Per cluster\n\ngb_list = ['pk_partition','cluster']\nfeature_list = {'altas_shift_':True,'altas_cuenta_shift_':True,'altas_ahorros_shift_':True,'altas_financiacion_shift_':True,\\\n               'total_shift_':False,'total_cuenta_shift_':False,'total_ahorros_shift_':False,'total_financiacion_shift_':False}\nmeses = [1,2,3]\n\nfor key, value in feature_list.items():\n    df = gb_feature_generator(df, meses, gb_list, key, ['mean','sum','median'], filter_positives=value)\n\nprint(gb_list)\n\n# Per active_customer\n\ngb_list = ['pk_partition','active_customer']\nfeature_list = {'altas_shift_':True,'altas_cuenta_shift_':True,'altas_ahorros_shift_':True,'altas_financiacion_shift_':True}\nmeses = [1,2]\n\nfor key, value in feature_list.items():\n    df = gb_feature_generator(df, meses, gb_list, key, ['mean','sum'], filter_positives=value)\n\nprint(gb_list)\n\nprint('Final shape: ',df.shape)","aabad8d5":"max_ = int(max(df['total'].value_counts().index)+1)\nlist_ = list_products.copy()\ndf['pk_partition'] = df['pk_partition'].astype('object')\nlist_.append('pk_partition')\nlist_.append('pk_cid')\n\ncompras_todas = {}\n\nfor number in range(1,max_):\n    part_df = df[df['total'] == number][list_].set_index(['pk_cid','pk_partition'])\n    \n    #print(part_df[list_products] != 0)\n    for key, row in part_df.iterrows():\n        #print(row[row != 0].index.to_list())\n        compras_todas[key] = row[row != 0].index.to_list()     \n    \n    print(number)\n    #for num in range(1,number+1):\n    #    df['prod'+str(num)] = np.where( , cost_product[products_dict[prods[num-1]]], df['prod'+str(num)] )","dd42d3b3":"import pickle\nf = open(\"compras_todas.pkl\",\"wb\")\npickle.dump(compras_todas,f)\nf.close()","f715009f":"compras_df = pd.DataFrame.from_dict(compras_todas, orient='index')\ncompras_melted = compras_df.melt(ignore_index = False)\ncompras_melted['tipo_producto'] = compras_melted['value'].replace(products_dict)\ncompras_melted['precio'] = compras_melted['tipo_producto'].replace(cost_product)\ncompras_melted = compras_melted.reset_index()\ncompras_melted['pk_cid'] = compras_melted['index'].apply( lambda x: x[0] )\ncompras_melted['pk_partition'] = compras_melted['index'].apply( lambda x: x[1] )\ncompras_melted[['pk_cid','pk_partition','tipo_producto','precio']]","b752f07a":"compras_melted.info()\ncompras_melted['pk_partition'] = compras_melted['pk_partition'].apply(lambda x: str(x)[:11])\ndf['pk_partition'] = df['pk_partition'].apply(lambda x: str(x)[:11])","e5d27144":"del compras_todas, compras_df, part_df","6266ef4a":"df = pd.merge(df, compras_melted[['pk_cid','pk_partition','value','tipo_producto','precio']], on=['pk_cid','pk_partition'], how='left')","c0f33c69":"del compras_melted","955480e3":"df['precio'].value_counts(dropna=False)","629ef008":"df['precio'] = np.where(df['precio'].isna(),0,df['precio'])\n#df['value'] = np.where(df['value'].isna(),0,df['value'])","fe3a6961":"df['value'].value_counts(dropna=False)","ce619284":"df.isnull().sum()[df.isnull().sum()!=0]","e2747ce2":"#df.fillna(0,inplace=True)","6ce743fb":"df['salary'].fillna(-9999, inplace=True)","7033ac12":"df = df[ (df['deceased'] != 'S') & (df['age'] >= 18) & (df['age'] <= 90)]\n#df = df[ (df['age'] >= 18) & (df['age'] <= 90)]","929d34ec":"df['tipo_producto'].value_counts()","10852c1f":"df['compra'] = df['value']\ndf['compra_cuenta'] = np.where(df['tipo_producto']=='cuenta',1,0)\ndf['compra_ahorros'] = np.where(df['tipo_producto']=='ahorro e inversi\u00f3n',1,0)\ndf['compra_financiacion'] = np.where(df['tipo_producto']=='financiaci\u00f3n',1,0)","d9c95625":"cols_to_drop = ['segment','entry_date','payroll','deceased','value','tipo_producto']\ndf.drop(cols_to_drop, axis=1, inplace=True)\ndf.drop(list_products, axis=1, inplace=True)","05b9e0c6":"df['region_code'].fillna(-99, inplace=True)\n\n# Frequency encoding de region_code\n#region_vc = df['region_code'].value_counts()\n#df['region_code'] = df['region_code'].replace(region_vc)\ndf['region_code'].value_counts()","c1339842":"df['pk_partition'] = pd.to_datetime(df['pk_partition'])\ndf['year'] = df['pk_partition'].dt.year\ndf['month'] = df['pk_partition'].dt.month","12dfc349":"df.info()","ad70740a":"print(df['compra'].value_counts(dropna=False))\ndf['compra'] = np.where(df['compra'].isna(),0,1)\nprint(df['compra'].value_counts(dropna=False))","7faa9907":"df['total'].value_counts(normalize=True)*100","94f854cf":"obtain = False\n\nif obtain == True:\n    pred_df = df.copy(deep=True)\n\n    list1 = ['total','total_cuenta','total_ahorros','total_financiacion']\n    list2 = ['altas','altas_cuenta','altas_ahorros','altas_financiacion']\n\n    for x, y in zip(list1,list2):\n        print(x,y)\n        pred_df[y] = df[x]\n\n    update_cols = ['total_cuenta','total_ahorros','total_financiacion','total','active_customer','altas',\\\n               'altas_cuenta','altas_ahorros','altas_financiacion']\n\n    for i in range(1,5):\n        for col in update_cols:\n            if i == 1:\n                pred_df[col+'_shift_1'] = pred_df[col]\n            else:\n                pred_df[col+'_shift_'+str(i)] = pred_df[col+'_shift_'+str(i-1)]\n\n    # Feature Generators\n    gb_list = ['pk_partition','cluster']\n    feature_list = {'altas_shift_':True,'altas_cuenta_shift_':True,'altas_ahorros_shift_':True,'altas_financiacion_shift_':True,\\\n                'total_shift_':False,'total_cuenta_shift_':False,'total_ahorros_shift_':False,'total_financiacion_shift_':False}\n    meses = [1,2,3]\n    calculations = ['mean','sum','median']\n\n    for key in feature_list.keys():\n        for mes in meses:\n            for cal in calculations:\n                name_ = key+str(mes)+'_'+gb_list[1]+'_'+cal\n                pred_df.drop( name_, axis=1, inplace=True)\n\n    for key, value in feature_list.items():\n        pred_df = gb_feature_generator(pred_df, meses, gb_list, key, calculations, filter_positives=value)\n\n    print(gb_list)\n\n    gb_list = ['pk_partition','active_customer']\n    feature_list = {'altas_shift_':True,'altas_cuenta_shift_':True,'altas_ahorros_shift_':True,'altas_financiacion_shift_':True}\n    meses = [1,2]\n    calculations = ['mean','sum']\n\n    for key in feature_list.keys():\n        for mes in meses:\n            for cal in calculations:\n                name_ = key+str(mes)+'_'+gb_list[1]+'_'+cal\n                pred_df.drop( name_, axis=1, inplace=True)\n\n    for key, value in feature_list.items():\n        pred_df = gb_feature_generator(pred_df, meses, gb_list, key, calculations, filter_positives=value)\n\n    print(gb_list)\n\n    prediction_df = pred_df[(pred_df['year']==2019)&(pred_df['month']==5)]\n    prediction_df = pred_df.groupby('pk_cid').last()\n\n    prediction_df['tiempo_ult_compra'] += 1\n    prediction_df['tiempo_ult_compra'] = np.where(prediction_df['total']>1, 1, prediction_df['tiempo_ult_compra'])\n\n    prediction_df['month'] += 1\n\n    pred_features = prediction_df.columns.to_list()\n    totales = ['total','total_cuenta','total_financiacion','total_ahorros','compra','compra_cuenta','compra_ahorros','compra_financiacion']\n\n    for item in totales:\n        pred_features.remove(item)\n\n    prediction_df.to_csv(DIR_PATH+'clients_prediction.csv')\n\n    del pred_df, prediction_df\n    ","bde3473d":"df.drop('pk_partition', axis=1, inplace=True)","025bef9a":"def balancear_dataset(df, target, dist, totales, tipo, times=1):\n    totales_ = totales.copy()\n    \n    compras = ['compra_ahorros','compra_cuenta','compra_financiacion']\n    boolean = False\n    if target in compras:\n        compras.remove(target)\n        boolean = True\n    \n    subset_positives = df[ (df[target]==True) ]\n    df_ = subset_positives.copy(deep=True)\n    clusters = list(df_['cluster'].unique())\n    \n    if boolean:\n        df = df[(df[compras[0]]==False) & (df[compras[1]]==False)]\n    \n    if tipo == 'undersampling':\n        subset_negatives = df[ (df[target] == 0) & (df['cluster'].isin(clusters)) ].sample( df_.shape[0]*dist )\n    elif tipo == 'oversampling':\n        for time in range(times-1):\n            df_ = df_.append( subset_positives, ignore_index=True )\n        if dist == 0:\n            subset_negatives = df[ (df[target] == 0) & (df['cluster'].isin(clusters)) ]\n        else:\n            subset_negatives = df[ (df[target] == 0) & (df['cluster'].isin(clusters)) ].sample( df_.shape[0]*dist )\n\n    df_final = subset_negatives.copy(deep=True)\n    df_final = df_final.append( df_, ignore_index=True )\n    \n    totales_.remove(target)\n    #print(totales_)\n        \n    df_final.drop(totales_, axis='columns', inplace=True)\n    \n    del totales_\n    \n    return df_final","731ff056":"totales = ['total','total_cuenta','total_financiacion','total_ahorros','compra','compra_cuenta','compra_ahorros','compra_financiacion']","967a1405":"#from sklearn.model_selection import TimeSeriesSplit\n\ndef dataset_split(df_final, target, balanceo, totales, dist, times=1):\n    features = df_final.columns.to_list()  \n    X = df_final[ features ]\n    y = df_final[ target ]\n\n    X_validation = X[(X['year']==2019) & (X['month']==5)]\n    y_validation = y.loc[X_validation.index]\n    \n    #X_train_test = X[(X['year']!=2019) & (X['month']!=5)]\n    #y_train_test = y.loc[X_train_test.index]\n    \n    # Balancear dataset \n    df_balanceo = balancear_dataset(df_final, target, dist, totales, \"oversampling\", times)\n    \n    features = df_balanceo.columns.to_list()\n    remove_cols = ['country_id','compra_antes',target]\n    for x in remove_cols:\n        try:\n            features.remove(x)\n        except:\n            pass\n    \n    X_train_test = df_balanceo[ features ]\n    y_train_test = df_balanceo[ target ]\n    X_validation = X_validation[ features ]\n    \n    # Train and Test\n    #X_train, X_test, y_train, y_test = TimeSeriesSplit(X_train_test, y_train_test)\n    X_train, X_test, y_train, y_test =  train_test_split(X_train_test, y_train_test, test_size=0.2, random_state=42)\n    \n    return X_train, y_train, X_test, y_test, X_validation, y_validation ","dff9df05":"def metricas(y_test, y_pred, y_test_score, y_train, y_train_pred, y_train_score):\n    print(\"----------- TEST -----------\")\n    print(\"Accuracy:\",accuracy_score(y_test, y_pred))\n    print(\"Precision:\",precision_score(y_test, y_pred))\n    print(\"Recall:\",recall_score(y_test, y_pred))\n    print(\"F1-score:\",f1_score(y_test, y_pred))\n    print(\"ROC AUC score:\",roc_auc_score(y_test, y_test_score))\n    print(\"----------- TRAIN -----------\")\n    print(\"Accuracy:\",accuracy_score(y_train, y_train_pred))\n    print(\"Precision:\",precision_score(y_train, y_train_pred))\n    print(\"Recall:\",recall_score(y_train, y_train_pred))\n    print(\"F1-score:\",f1_score(y_train, y_train_pred))\n    print(\"ROC AUC score:\",roc_auc_score(y_train, y_train_score))","e7e87ddf":"def confusion_matrix_figure(y_test, y_pred):\n    fig, ax = plt.subplots(figsize=(8,8))\n\n    cf_matrix = confusion_matrix(y_test, y_pred)\n    cf_matrix_ = cf_matrix.copy()\n    cf_matrix_[1,1] = cf_matrix[0,0]\n    cf_matrix_[0,1] = cf_matrix[1,0]\n    cf_matrix_[0,0] = cf_matrix[1,1]\n    cf_matrix_[1,0] = cf_matrix[0,1]\n    cf_matrix_\n\n    group_names = ['True Pos','False Neg','False Pos','True Neg']\n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                    cf_matrix_.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in\n                         cf_matrix_.flatten()\/np.sum(cf_matrix)]\n    labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n              zip(group_names,group_counts,group_percentages)]\n\n    labels = np.asarray(labels).reshape(2,2)\n    ax = sns.heatmap(cf_matrix_, annot=labels, fmt='', cmap='Greens', annot_kws={\"size\": 20},\n                    yticklabels=['Positive','Negative'], xticklabels=['Positive','Negative'])\n    ax.set_xlabel('Predicted',fontsize=23)\n    ax.set_ylabel('Real',fontsize=23)\n    ax.tick_params(labelsize=20)\n    \n    return fig","44b4cad9":"df[df['compra']!= 0]['cluster'].value_counts()","0b161126":"selected_clusters = [3,4,5,6]","338a8b39":"df_selected = df[df['cluster'].isin(selected_clusters)]","e05584b0":"df_selected['id'] = range(df_selected.shape[0])","593ee1ff":"cost_10 = set(list(df_selected['id'].sample(int(df_selected.shape[0]*0.35))))\ncost_60 = set(list(df_selected['id'].sample(int(df_selected.shape[0]*0.35))))\n\nprint(len(cost_10))\nprint(len(cost_60))","af5bb286":"df_selected['precio'].fillna(0,inplace=True)\ndf_selected['precio'] = np.where( (df_selected['precio'] == 0) & (df_selected['id'].isin(cost_10)), 10, df_selected['precio'])\ndf_selected['precio'] = np.where( (df_selected['precio'] == 0) & (df_selected['id'].isin(cost_60)), 60, df_selected['precio'])\ndf_selected['precio'] = np.where( df_selected['precio'] == 0, 40, df_selected['precio'])","e590e048":"print(df_selected['precio'].value_counts(dropna=False))\ndf_selected.drop('id',axis=1,inplace=True)","af044d9e":"df_selected[['compra','precio']].value_counts()","05e3573b":"df_selected.columns.to_list()","b4458797":"df_selected.isnull().sum()[df_selected.isnull().sum()!=0]","6fb56031":"df_selected.fillna(0,inplace=True)","a99e5d3c":"df_selected.isnull().sum()[df_selected.isnull().sum()!=0]","685b8bc1":"df_selected['compra'].value_counts()","f91844f5":"X_train, y_train, X_test, y_test, X_validation, y_validation = \\\n    dataset_split(df_selected, 'compra', \"oversampling\", totales, dist=0, times=3)\n\nfeatures = X_train.columns.to_list()\nfeatures.remove('pk_cid')\n#features.remove('cluster')","e5c7a1f8":"print('TRAIN: ',y_train.shape[0]) \nprint(y_train.value_counts(normalize=True),'\\n')\n\nprint('TEST: ', y_test.shape[0])\nprint(y_test.value_counts(normalize=True),'\\n')\n\nprint('VALIDATION: ',y_validation.shape[0])\nprint(y_validation.value_counts(normalize=True))","5b2db81c":"#model = lgbm.LGBMClassifier(n_estimators=500, max_depth=20, random_state=42, min_child_samples=250)\nmodel = DecisionTreeClassifier( max_depth=50, min_samples_leaf=500, random_state=42 )\n\nmodel.fit(X_train[features],y_train)\n\ny_train_pred = model.predict(X_train[features])\ny_train_score = model.predict_proba(X_train[features])[:,1]\n\n#Predict the response for test dataset\ny_pred = model.predict(X_test[features])\ny_test_score = model.predict_proba(X_test[features])[:,1]","cd799902":"metricas(y_test, y_pred, y_test_score, y_train, y_train_pred, y_train_score)","29b0a55c":"y_pred_valida = model.predict(X_validation[features])\ny_valida_score = model.predict_proba(X_validation[features])[:,1]\n\nprint(\"----------- VALIDATION -----------\")\nprint(\"Accuracy:\",accuracy_score(y_validation, y_pred_valida))\nprint(\"Precision:\",precision_score(y_validation, y_pred_valida))\nprint(\"Recall:\",recall_score(y_validation, y_pred_valida))\nprint(\"F1-score:\",f1_score(y_validation, y_pred_valida))\nprint(\"ROC AUC score:\",roc_auc_score(y_validation, y_valida_score))","23d0f706":"#fig = confusion_matrix_figure(y_train, y_train_pred)\nfig = confusion_matrix_figure(y_validation, y_pred_valida)\nfig.show()","8cbf6277":"pd.Series( model.feature_importances_, index=features ).sort_values(ascending=False).head(30)","942ff2f0":"fig = confusion_matrix_figure(y_train, y_train_pred)\nfig.show()","b964bbd1":"tree_data = export_graphviz(\n    decision_tree = model,\n    out_file=None,\n    #*,\n    max_depth=3,\n    feature_names= features,\n    class_names=['No Compra','Compra'],\n    #label='all',\n    filled=True,\n    #leaves_parallel=False,\n    impurity=True,\n    #node_ids=False,\n    proportion=True,\n    rotate=True,\n    rounded=True,\n    #special_characters=False,\n    precision=3,\n)\n\n#graph = graphviz.Source(tree_data, format='png')\n#graph","219aefea":"fig = confusion_matrix_figure(y_train, y_train_pred)\nfig.show()","80162bc5":"df_selected['compra_cuenta'].value_counts()","6188d5e3":"X_train, y_train, X_test, y_test, X_validation, y_validation = \\\n    dataset_split(df_selected, 'compra_cuenta', \"oversampling\", totales, dist=0, times=6)\n\nfeatures = X_train.columns.to_list()\nfeatures.remove('pk_cid')\nfeatures.remove('precio')\n#features.remove('cluster')","e673e94c":"print('TRAIN: ',y_train.shape[0]) \nprint(y_train.value_counts(normalize=True),'\\n')\n\nprint('TEST: ', y_test.shape[0])\nprint(y_test.value_counts(normalize=True),'\\n')\n\nprint('VALIDATION: ',y_validation.shape[0])\nprint(y_validation.value_counts(normalize=True))","709d7af9":"#model = lgbm.LGBMClassifier(n_estimators=650, max_depth=20, random_state=42, min_child_samples=150)\nmodel = DecisionTreeClassifier( max_depth=60, min_samples_leaf=200, random_state=42 )\n\nmodel.fit(X_train[features],y_train)\n\ny_train_pred = model.predict(X_train[features])\ny_train_score = model.predict_proba(X_train[features])[:,1]\n\n#Predict the response for test dataset\ny_pred = model.predict(X_test[features])\ny_test_score = model.predict_proba(X_test[features])[:,1]","65bc1475":"metricas(y_test, y_pred, y_test_score, y_train, y_train_pred, y_train_score)","60a2108c":"y_pred_valida = model.predict(X_validation[features])\ny_valida_score = model.predict_proba(X_validation[features])[:,1]\n\nprint(\"----------- VALIDATION -----------\")\nprint(\"Accuracy:\",accuracy_score(y_validation, y_pred_valida))\nprint(\"Precision:\",precision_score(y_validation, y_pred_valida))\nprint(\"Recall:\",recall_score(y_validation, y_pred_valida))\nprint(\"F1-score:\",f1_score(y_validation, y_pred_valida))\nprint(\"ROC AUC score:\",roc_auc_score(y_validation, y_valida_score))\n\nfig = confusion_matrix_figure(y_validation, y_pred_valida)\nfig.show()","544cb531":"pd.Series( model.feature_importances_, index=features ).sort_values(ascending=False).head(30)","dc3581fd":"tree_data = export_graphviz(\n    decision_tree = model,\n    out_file=None,\n    #*,\n    max_depth=3,\n    feature_names= features,\n    class_names=['No Compra','Compra'],\n    #label='all',\n    filled=True,\n    #leaves_parallel=False,\n    impurity=True,\n    #node_ids=False,\n    proportion=True,\n    rotate=True,\n    rounded=True,\n    #special_characters=False,\n    precision=3,\n)\n\ngraph = graphviz.Source(tree_data, format='png')\ngraph","dcfe8734":"fig = confusion_matrix_figure(y_train, y_train_pred)\nfig.show()","9a6e019b":"df_selected['compra_ahorros'].value_counts()","5878edc6":"X_train, y_train, X_test, y_test, X_validation, y_validation = \\\n    dataset_split(df_selected, 'compra_ahorros', \"oversampling\", totales, dist=0, times=4)\n\nfeatures = X_train.columns.to_list()\nfeatures.remove('pk_cid')\nfeatures.remove('precio')\n#features.remove('cluster')","93c8dd1b":"print('TRAIN: ',y_train.shape[0]) \nprint(y_train.value_counts(normalize=True),'\\n')\n\nprint('TEST: ', y_test.shape[0])\nprint(y_test.value_counts(normalize=True),'\\n')\n\nprint('VALIDATION: ',y_validation.shape[0])\nprint(y_validation.value_counts(normalize=True))","54c9c344":"#model = lgbm.LGBMClassifier(n_estimators=650, max_depth=20, random_state=42, min_child_samples=150)\nmodel = DecisionTreeClassifier( max_depth=20, min_samples_leaf=350, random_state=42 )\n\nmodel.fit(X_train[features],y_train)\n\ny_train_pred = model.predict(X_train[features])\ny_train_score = model.predict_proba(X_train[features])[:,1]\n\n#Predict the response for test dataset\ny_pred = model.predict(X_test[features])\ny_test_score = model.predict_proba(X_test[features])[:,1]","5ad9754a":"metricas(y_test, y_pred, y_test_score, y_train, y_train_pred, y_train_score)","0f038af7":"pd.Series( model.feature_importances_, index=features ).sort_values(ascending=False).head(30)","04a2de91":"y_pred_valida = model.predict(X_validation[features])\ny_valida_score = model.predict_proba(X_validation[features])[:,1]\n\nprint(\"----------- VALIDATION -----------\")\nprint(\"Accuracy:\",accuracy_score(y_validation, y_pred_valida))\nprint(\"Precision:\",precision_score(y_validation, y_pred_valida))\nprint(\"Recall:\",recall_score(y_validation, y_pred_valida))\nprint(\"F1-score:\",f1_score(y_validation, y_pred_valida))\nprint(\"ROC AUC score:\",roc_auc_score(y_validation, y_valida_score))\n\n#fig = confusion_matrix_figure(y_test, y_pred)\nfig = confusion_matrix_figure(y_validation, y_pred_valida)\nfig.show()","66427036":"df_selected['compra_financiacion'].value_counts()","b69b365b":"X_train, y_train, X_test, y_test, X_validation, y_validation = \\\n    dataset_split(df_selected, 'compra_financiacion', \"oversampling\", totales, dist=6, times=2)\n\nfeatures = X_train.columns.to_list()\nfeatures.remove('pk_cid')\nfeatures.remove('precio')\n#features.remove('cluster')","f290ee15":"print('TRAIN: ',y_train.shape[0]) \nprint(y_train.value_counts(normalize=True),'\\n')\n\nprint('TEST: ', y_test.shape[0])\nprint(y_test.value_counts(normalize=True),'\\n')\n\nprint('VALIDATION: ',y_validation.shape[0])\nprint(y_validation.value_counts(normalize=True))","147db327":"model = lgbm.LGBMClassifier(n_estimators=450, max_depth=20, random_state=42, min_child_samples=150)\n#model = DecisionTreeClassifier( max_depth=40, min_samples_leaf=150, random_state=42 )\nmodel.fit(X_train[features],y_train)\n\ny_train_pred = model.predict(X_train[features])\ny_train_score = model.predict_proba(X_train[features])[:,1]\n\n#Predict the response for test dataset\ny_pred = model.predict(X_test[features])\ny_test_score = model.predict_proba(X_test[features])[:,1]","168a801b":"metricas(y_test, y_pred, y_test_score, y_train, y_train_pred, y_train_score)","592bc413":"pd.Series( model.feature_importances_, index=features ).sort_values(ascending=False).head(30)","53b34a02":"y_pred_valida = model.predict(X_validation[features])\ny_valida_score = model.predict_proba(X_validation[features])[:,1]\n\nprint(\"----------- VALIDATION -----------\")\nprint(\"Accuracy:\",accuracy_score(y_validation, y_pred_valida))\nprint(\"Precision:\",precision_score(y_validation, y_pred_valida))\nprint(\"Recall:\",recall_score(y_validation, y_pred_valida))\nprint(\"F1-score:\",f1_score(y_validation, y_pred_valida))\nprint(\"ROC AUC score:\",roc_auc_score(y_validation, y_valida_score))","aa370a62":"#fig = confusion_matrix_figure(y_test, y_pred)\n#fig = confusion_matrix_figure(y_train, y_train_pred)\nfig = confusion_matrix_figure(y_validation, y_pred_valida)\nfig.show()","0d55526c":"X_train, y_train, X_test, y_test, X_validation, y_validation = \\\n    dataset_split(df_selected, 'compra', \"oversampling\", totales, dist=0, times=1)\n\nfeatures = X_train.columns.to_list()\nfeatures.remove('pk_cid')\n#features.remove('cluster')","2516f372":"model = lgbm.LGBMClassifier(n_estimators=550, max_depth=50, random_state=42, min_child_samples=250)\n#model = DecisionTreeClassifier( max_depth=50, min_samples_leaf=500, random_state=42 )\n\nmodel.fit(X_train[features],y_train)\n\ny_train_pred = model.predict(X_train[features])\ny_train_score = model.predict_proba(X_train[features])[:,1]\n\n#Predict the response for test dataset\ny_pred = model.predict(X_test[features])\ny_test_score = model.predict_proba(X_test[features])[:,1]","80c1fa49":"y_pred_valida = model.predict(X_validation[features])\ny_valida_score = model.predict_proba(X_validation[features])[:,1]\n\nprint(\"----------- VALIDATION -----------\")\nprint(\"Accuracy:\",accuracy_score(y_validation, y_pred_valida))\nprint(\"Precision:\",precision_score(y_validation, y_pred_valida))\nprint(\"Recall:\",recall_score(y_validation, y_pred_valida))\nprint(\"F1-score:\",f1_score(y_validation, y_pred_valida))\nprint(\"ROC AUC score:\",roc_auc_score(y_validation, y_valida_score))","724f9f8d":"fig = confusion_matrix_figure(y_validation, y_pred_valida)\nfig.show()","a8877eea":"import pickle \n\nfilename = 'modelo_compra_general.sav'\npickle.dump(model, open(filename, 'wb'))","4dbede70":"summary = X_train.copy(deep=True)\nsummary['real'] = y_train\nsummary['predicted'] = y_train_pred\nsummary['proba'] = y_train_score\nsummary['diff'] = summary['predicted']-summary['real']\n\nsummary[summary['diff'] == 1].describe()","7c4f4c6e":"summary[summary['diff'] == 1].sort_values('proba',ascending=False).head(30)","84681e08":"A estas variables se les realiza un desfase (shifts) de X meses y se a\u00f1aden al dataframe origen.","3f1b98a8":"# Tarea 3: Recomendaci\u00f3n (Modelo para predicci\u00f3n de compra)\n\nParte de la soluci\u00f3n de la tarea 3 est\u00e1 en crear un modelo que permita obtener una probabilidad de cu\u00e1les recomendaciones de la parte anterior \\PONER LINK tienen m\u00e1s probabilidad de \u00e9xito con el fin de poder escoger entre la larga lista de recomendaciones reduciendo as\u00ed el riesgo y optando por las opciones de las que estamos m\u00e1s seguros. \n\nEste modelo se trata de un modelo de Machine Learning supervisado de clasificaci\u00f3n binaria, donde 1 o True significa que ha comprado y 0 que no ha comprado. Se desarrollan 3 modelos \nDurante el desarrollo de este notebook se ir\u00e1n preparando y limpiando los datos del modelo para finalmente obtener un modelo muy aceptable con un ROC AUC score de 0.86698. \n\nPara simplificar el problema se considera como compra si un cliente se da de alta en ese producto. Sin embargo, como se comenta en tareas anteriores este no es el caso y el producto no se cobra hasta pasados 3 meses de permanencia. Tener en cuenta la permanencia para el modelo hubiera sido otro problema muy distinto y bastante m\u00e1s complejo. \n\nPuede probar el modelo en <a href=\"http:\/\/carlosperez1697.pythonanywhere.com\/compra\">Modelo Compra<\/a>, y predecir la probabilidad de compra de un usuario dadas sus caracter\u00edsticas.\n\n## Tabla de Contenidos <a class=\"anchor\" id=\"0\"><\/a>\n\n1. [Data Preparation](#1) <br>\n    1.1 [Determinar altas](#11) <br>\n    1.2 [Tiempo desde la \u00faltima compra](#12) <br>\n    1.3.[Precio del producto a comprar](#13) <br>\n    1.4.[Precio del producto a comprar](#14) <br>\n2. [Data Cleaning](#2) <br>\n    2.1. [Resampling](#21) <br>\n    2.2. [Precio de productos no comprados](#22) <br>\n    2.3. [Tiempo desde \u00faltima compra no existente](#23) <br>\n3. [Model Construction](#3) <br>\n    3.1. [Modelo General](#31) <br>\n    3.2. [Modelo Cuentas](#32) <br>\n    3.3. [Modelo Ahorros](#33) <br>\n    3.4. [Modelo Financiaci\u00f3n](#34) <br>\n    3.5. [Selecci\u00f3n del modelo](#35) <br>\n4. [Preparaci\u00f3n de las predicciones](#4) <br>","49a0e7a4":"## Modelo Financiaci\u00f3n <a class=\"anchor\" id=\"34\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEl target es *compra_financiacion* y ocurre exactamente lo mismo que con los modelos anteriores, dataset desbalanceado, pocos datos y el modelo no es capaz de encontrar diferencias entre compras y no compras. ","4eec6199":"Y finalmente las siguientes columnas son eliminadas del dataset y no ser\u00e1n introducidas al modelo. ","bb82a7c6":"Tambi\u00e9n se eval\u00faa la confusion matrix pero en este caso en el dataset de train, dataset con el que se entren\u00f3. Como se puede observar ocurre un comportamiento similar al de validaci\u00f3n.","7784323a":"## Tiempo desde la \u00faltima compra <a class=\"anchor\" id=\"12\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nComo es de esperar un mismo cliente no tiene el mismo comportamiento cada mes, es decir no realiza una compra cada mes sino que realiza una compra (se da de alta) un mes y lo vuelve a hacer dentro de 3 meses, por ejemplo. Una m\u00e9trica interesante a obtener es el tiempo que ha transcurrido desde su \u00faltima compra. \n\nEl siguiente c\u00f3digo tiene en cuenta este hecho. Un problema que plantea esta m\u00e9trica y que afectar\u00e1 al futuro modelo es qu\u00e9 pasa con los clientes que realizan su primera compra o clientes que no han comprado nada, qu\u00e9 tiempo desde la \u00faltima compra se supone. \n\nEn este apartado tambi\u00e9n se genera una nueva m\u00e9trica inter\u00e9s muy relacionada al tiempo desde la \u00faltima compra, conocer si el cliente ha comprado antes o no. ","88ed40a4":"## Modelo Ahorros <a class=\"anchor\" id=\"33\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEn este caso el caso a predecir es *compra_ahorros*, al igual que en el caso anterior, el dataset se balancea en este caso con un oversampling. Y ocurre m\u00e1s de lo mismo, al modelo le cuesta detectar las compras y cuando lo hace se equivoca demasiado. \n\nSi se reduce el n\u00famero de casos positivos (compras) para que el modelo aprenda que las compras son poco frecuentes, entonces predice siempre que no es compra. A\u00fan as\u00ed mantiene una distribuci\u00f3n constante entre el n\u00famero de predicciones de compra acertadas y erradas.","039cd241":"La siguiente funci\u00f3n realiza el split del dataset. Para la validaci\u00f3n se coge el \u00faltimo mes, mientras que el resto de datos se toman para el train-test, estos datos se balancean con la funci\u00f3n anterior y se realiza un train_test_split donde el test_size ser\u00e1 del 20%.","4137c4db":"## Selecci\u00f3n del modelo <a class=\"anchor\" id=\"35\"><\/a>\n\n[Tabla de Contenidos](#0) \n\nFinalmente, se opta por el [Modelo General](#31) que es el que mejor resultados obtiene, pese a que estos est\u00e9n alog lejos de lo que podr\u00eda considerarse un resultado aceptable. \n\nEl valor de Recall es bajo tanto en test, train como en validaci\u00f3n. Y deber\u00e1 prestarse mucha antenci\u00f3n con los errores de Tipo II (hip\u00f3tesis aceptada cuando era falsa - condenar a un inocente). Es decir por la tem\u00e1tica del problema donde se cada email tiene un coste, se prefiere reducir el n\u00famero de emails a clientes que no ten\u00edan ning\u00fan deseo a comprar. \n\nFinalmente, guardamos el modelo que ser\u00e1 utilizado en la Tarea de Recomendaci\u00f3n. ","99eef19e":"Y seguidamente se decide que hacer con la variable precio del producto a comprar para aquellos registros en los que no se ha comprado nada. Se decide por rellenarlos de manera aleatoria con una distribuci\u00f3n equitativa. \n\nSe comprob\u00f3 que si por ejemplo la distribuci\u00f3n se altera y hay menos casos de precios de 60 euros, el modelo cuando ve un producto de 60 euros, al tener pocos datos se atreve a predecir que se compra, cuando la realidad dista mucho de esta predicci\u00f3n. Es por ello que se opta por tener una distribuci\u00f3n equitativa entre los diferentes precios de productos. \n\nEsta variable se utilizar\u00e1 para el modelo general de compra, pero pierde su sentido para los modelos particulares de cada tipo de producto. ","ec2327fc":"Se utiliza un DecisionTreeClassifier para as\u00ed poder visualizar el \u00e1rbol de decisi\u00f3n. Tambi\u00e9n se prob\u00f3 un Gradient Boosting en este caso un Light Gradient Boosting Machine pero sorprendentemente el score tampoco mejoraba, lo que indica que el problema tampoco es muy f\u00e1cil de predecir. Normalmente un Gradient Boosting suele mejorar el score de un simple Decision Tree.","cc60d3bb":"Se a\u00f1ade una columna con la informaci\u00f3n del cluster al que pertenece cada cliente, esta informaci\u00f3n proviene de la Tarea 2 de Segmentaci\u00f3n. PONERLINK\n\nY a continuaci\u00f3n se crea una funci\u00f3n que permitir\u00e1 balancear el dataset. El balanceo puede realizarse de dos maneras:\n- **Undersampling**. En este caso se aumenta el n\u00famero de registros de la clase minoritaria.\n- **Oversampling** En este caso se reduce el n\u00famero de registros de la clase mayoritaria, de manera que la distribuci\u00f3n no sea tan desproporcionada. ","1e4b7867":"## Modelo Cuentas <a class=\"anchor\" id=\"32\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEn este modelo para la predicci\u00f3n de compras en cuentas el target a predecir es *compra_cuenta*. Al reducir el n\u00famero de compras el dataset est\u00e1 desbalanceado por lo que se balancea. Si el dataset no se balancea, el modelo determinar\u00e1 no compra en todos o casi todos los casos, dado que los casos de compra son minoritarios. \n\nLa idea es aumentar los datos de compra suficientemente ya sea con undersampling o oversampling, de manera que el modelo vea m\u00e1s casos de compra y se \"atreva\" a predecir compra. \n\nSin embargo, los resultados obtenidos son bastante desastrosos y esto se debe a los pocos datos y la poca variedad de compras para el tipo de producto cuenta. ","2cd60a02":"Vamos a evaluar las caracter\u00edsticas de los errores de tipo II que suele cometer el modelo. Como se puede observar la mayor\u00eda de estos errores los comete con los clusters 3 y 4. Aunque un hecho positivo es que la probabilidad que determina el modelo con el predict_proba tampoco es muy alta, es decir que el modelo no est\u00e1 del todo convencido que el cliente va a comprar. ","d6e70ddf":"**Importaci\u00f3n de librer\u00edas y dataset**\n\n","3d1a613f":"Los siguientes resultados no son los esperados. Destaca el baj\u00edsimo recall y en la confusion matrix evaluada en validaci\u00f3n, se observa que pese a que el modelo se atreva a predecir compra, se equivoca m\u00e1s que acierta y a\u00fan as\u00ed se le escapan muchas compras. Se trata de un modelo poco fiable y que no ofrece mucho valor al negocio. ","dc822574":"A continuaci\u00f3n se realiza un value_counts del n\u00famero de compras en el dataset. C\u00f3mo puede observarse en la gran mayor\u00eda de registros no se produce una compra y el dataset est\u00e1 totalmente desbalanceado.","bfaf3c02":"Nos quedamos \u00fanicamente con registros de clientes que ya han comprado anteriormente, as\u00ed que se construye un modelo que sea capaz de predecir la nueva compra de clientes recurrentes. Y nos quedamos con los registros de los clusters que m\u00e1s inter\u00e9s tienen para le negocio.","5700cbb4":"# Data Preparation <a class=\"anchor\" id=\"1\"><\/a>\n## Determinar altas <a class=\"anchor\" id=\"11\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEn esta subtarea es importante detectar las altas de productos. La funci\u00f3n implementada es la misma que para la Tarea 2. PONER LINK. Las altas ser\u00e1n nuestro target a predecir. Dado que queremos predecir cuando un usuario se da de alta o no, se contabiliza el estado de todos los productos en esa partici\u00f3n por cada cliente. Finalmente, la columna total indica 1 o mayor que 1, si un cliente se dio de alta en cualquier producto durante ese mes, o 0 si no se dio de alta. ","5ecc5098":"En este caso se seleccionan los registros de los clusters de los que nos gustar\u00eda predecir su comportamiento ante la acci\u00f3n de compra. Por ejemplo, a\u00f1adir registros del cluster 1 (que no est\u00e1 presente) a\u00f1adir\u00eda ruido al modelo dado que no han realizado ninguna compra.","f4e904b8":"Guardamos el modelo en un pickle para as\u00ed poder utilizarlo en otro script o notebook.","5f9c530c":"# Preparaci\u00f3n de las predicciones <a class=\"anchor\" id=\"4\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nFinalmente, se preparan las predicciones para el modelo, se toma la informaci\u00f3n de los clientes en el pr\u00f3ximo mes. Es importante destacar que se calcula el tiempo desde la \u00faltima compra. Si el cliente no compro nada en el \u00faltimo mes, se a\u00f1ade 1 al tiempo desde la \u00faltima compra y si hubo compra se renueva el contador a 1. \n\nSe guardan las predicciones en un csv que ser\u00e1 le\u00eddo en \\PONER LINK.","ea645df4":"No nos quedamos con los clientes que han fallecido, ni con los clientes menores de 18 a\u00f1os y mayores de 90. ","9a357b69":"A continuaci\u00f3n pasamos a valor num\u00e9rico los campos de entry_channel y gender con un LabelEncoder.","6f3be5f4":"## Precio del producto a comprar <a class=\"anchor\" id=\"14\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nExisten 14 productos diferentes, \u00e9stos pueden ser agrupados en diferentes tipos de productos; y cada tipo tiene un precio. En este apartado se crea una columna nueva que indica la inversi\u00f3n o precio del producto en el que el cliente se dio de alta. ","78189ced":"# Model construction <a class=\"anchor\" id=\"3\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEn este apartado se construyen 4 modelos diferentes. \n- [Modelo General](#31)\n- [Modelo Cuentas](#32)\n- [Modelo Ahorros](#33)\n- [Modelo Financiaci\u00f3n](#34)\n\nSe construye un modelo general donde se tienen en cuenta todas las compras, y tambi\u00e9n se construyen otros 3 modelos dependiendo del tipo de producto (cuentas, ahorros e inversi\u00f3n o financiaci\u00f3n) con el objetivo de ver si dividiendo las compras es posible mejorar el modelo general. \n\nM\u00e1s adelante se ver\u00e1 que debido a que el dataset est\u00e1 desbalanceado y que el problema es un problema dif\u00edcil de predecir, los resultados no son tan buenos como se espera. ","2df7594c":"# Data Cleaning <a class=\"anchor\" id=\"2\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEl siguiente paso es limpiar el dataset de nulos y quedarse \u00fanicamente con las columnas relevantes para el modelo. Muchas de estas columnas no ser\u00e1n introducidas al modelo, pero ser\u00eda interesante encontrar nuevas m\u00e9tricas y tenerlas en cuenta para as\u00ed realizar un modelo m\u00e1s complejo y que tenga en cuenta m\u00e1s diversidad de casos. ","8e64c38d":"# N\u00famero de productos contratados y  anteriormente <a class=\"anchor\" id=\"13\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEn esta secci\u00f3n se calcula el n\u00famero de productos totales y por tipo de producto, como tambi\u00e9n el dinero \"gastado\". Estas variables ser\u00e1n introducidas al modelo hace 1 mes, 2 meses o 3 meses, con shifts. \n\nPrimero se obtienen estos totales con la siguiente funci\u00f3n. Esta funci\u00f3n utiliza el m\u00e9todo rolling que permite obtener la suma, media, etc. de una variable teniendo los valores anteriores.","d6de4c7d":"Se obtienen el a\u00f1o y mes de cada partici\u00f3n. ","bc80fd15":"Y finalmente se eval\u00faa el modelo en validaci\u00f3n y se observa la confusion matrix. El ROC AUC score es m\u00e1s o menos alto, sin embargo esto se debe al balanceo de los datos, donde falla este modelo es en el baj\u00edsimo Recall y la baja precision que tiene y es que al modelo le cuesta predecir positivo y se le escapan muchos casos positivos, y cuando lo hace se equivoca bastante. ","914f43cd":"Finalmente unimos los tiempos desde la \u00faltima compra con el dataset original. La uni\u00f3n se realiza por cliente y por partici\u00f3n. ","2e72a227":"# PREDICTION","ba5bf0b7":"Tambi\u00e9n se construyen algunas funciones para visualizar los resultados ya sea las m\u00e9tricas o la confusion_matrix.","b55f8948":"A continuaci\u00f3n se visualiza la importancia de las variables m\u00e1s importantes (es decir las que m\u00e1s ayudan a reducir la entrop\u00eda o gini index del modelo) y se visualiza el \u00e1rbol de decisi\u00f3n y las preguntas que ha ido realizando. ","f41f860e":"## Modelo General <a class=\"anchor\" id=\"31\"><\/a>\n\n[Tabla de Contenidos](#0)\n\nEn este primer modelo, el target a predecir es *compra*, este incluye cualquier compra de cualquier producto o tipo de producto, por lo que el dataset es m\u00e1s rico y est\u00e1 m\u00e1s balanceado (por lo que no se balancea). El \u00fanico inconveniente que presenta este m\u00e9todo es la aletoriedad de la variable precio que es la variable que nos permite discernir entre que tipo de producto se compra o no. \n\nAl tener que rellenar la variable precio en los registros sin compra, la calidad del modelo flaquea. Sin embargo, pese a este hecho este modelo es el que mejor resultados da. ","706b2875":"A los clientes sin provincia se les asigna un valor extremo para la region_code. Y se deja con la codificaci\u00f3n que ya tienen, aunque podr\u00eda realizarse un Frequency Encoding. ","6837bdb0":"Para el caso del salario se establece un valor extremo."}}