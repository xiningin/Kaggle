{"cell_type":{"7d8f4c09":"code","1f373b80":"code","ea4d9f1d":"code","4fa4274f":"code","c80982f5":"code","35b9902c":"code","e072c882":"code","f1be84b1":"code","b765de5a":"code","91d7dd93":"code","e95d0182":"code","7dae0dad":"code","3fb03c5c":"code","f4509662":"code","bd20ec0b":"code","75e55ad4":"code","7fb26c07":"code","6920c4fa":"code","72ba5026":"code","d7c3fcfa":"code","b86e84fd":"code","be07d935":"code","2e6a9b6f":"code","f20ed3e2":"code","e8380f6a":"code","5bc9ca51":"code","35955249":"code","42b28ef3":"code","4f36a6b4":"code","7100f121":"code","a8271ad7":"code","341523a1":"code","09bce637":"code","12b7eabb":"code","b8e7627e":"code","657af63a":"code","84984ed2":"code","f58d10e5":"code","d4aa566a":"code","5ba4499c":"code","3702e31b":"code","fabb26f6":"code","2f581c70":"code","55138088":"code","4194c9bd":"code","a27f04b9":"markdown","1b7b6e95":"markdown","2d9349a3":"markdown","ed59aeb4":"markdown","830a0977":"markdown","d912dfe0":"markdown","86543277":"markdown","dfe9f3f9":"markdown","d60b804e":"markdown","e15d85d0":"markdown","49ea6cee":"markdown","28b63b5e":"markdown","41b75918":"markdown","ada54251":"markdown","e7fdae75":"markdown","e63330ca":"markdown","04599586":"markdown","ae5754c8":"markdown","f97b0bb7":"markdown","6caa46f6":"markdown","c367c04d":"markdown","54a8b4a2":"markdown","4678dfa4":"markdown","13b0b527":"markdown","f604548e":"markdown","6ac9fe50":"markdown","399ad124":"markdown","f121b023":"markdown","3364f869":"markdown","24936b41":"markdown","81421e24":"markdown","beebeeeb":"markdown","64bd9d12":"markdown","c10bfe3c":"markdown"},"source":{"7d8f4c09":"#Credits goes to https:\/\/www.kaggle.com\/demidova\/titanic-eda-tutorial\/ -- somewhat used as a reference\n#Please comment if you have any suggestions on what to do better :)\n\n#Updates:\n#1. Added CatBoostClassifier\n\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\npassengerIds = test.PassengerId","1f373b80":"test.head()","ea4d9f1d":"test.tail()","4fa4274f":"train = train.drop(['PassengerId'], axis = 1)","c80982f5":"train.isnull().sum()","35b9902c":"test.nunique()","e072c882":"sns.countplot(x = 'Survived', data = train)\ntrain['Survived'].value_counts(normalize = True)","f1be84b1":"sns.countplot(x = 'Sex', hue = 'Survived', data = train)","b765de5a":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\n\n\ntrain['Sex'] = label_encoder.fit_transform(train['Sex'].values)","91d7dd93":"train.describe()","e95d0182":"plt.figure(figsize= (10,5))\nsns.histplot(train['Age'])\nplt.show()","7dae0dad":"plt.figure(figsize = (20, 8))\nsns.violinplot(x = 'Survived', y='Age', data = train)\nplt.show()","3fb03c5c":"train[train[\"Survived\"] == 0][\"Age\"].plot.hist( bins = 20, edgecolor = \"black\", color = \"red\")\n","f4509662":"train[train[\"Survived\"] == 1][\"Age\"].plot.hist(bins = 20, edgecolor = \"black\", color = \"blue\")","bd20ec0b":"train['Age'].fillna((train['Age'].mean()), inplace = True)\n\nbin = np.array([0,16,30,48, 63,75,200])\ntrain['AgeGroup'] = bin.searchsorted(train['Age'])\n\n","75e55ad4":"sns.countplot(x = 'Pclass', hue = 'Survived', data = train)","7fb26c07":"sns.heatmap(train.corr(), annot = True, cmap = 'coolwarm')","6920c4fa":"train['FamilySize'] = train['SibSp'] + train['Parch'] + 1\n\nplt.figure(figsize= (20,10))\nsns.countplot(x = train['FamilySize'], hue = train['Survived'])\nplt.show()","72ba5026":"train['FamilySizeGrouped'] = train['FamilySize'].map(lambda x: 'single' if x == 1 \n                                                            else ('normal' if 5 > x >= 2 \n                                                                  else ('big' if 8 > x >= 5 \n                                                                       else 'large' )\n                                                                 ))\nfamily_dict = {'single':1 , 'normal':2, 'big':3 , 'large':4}\ntrain = train.replace({'FamilySizeGrouped':family_dict})","d7c3fcfa":"train['Title'] = train['Name'].str.split(',', expand = True)[1].str.split('.', expand = True)[0].str.strip(' ')\ntrain = train.drop(['Name'], axis = 1)\ntrain.head()","b86e84fd":"plt.figure(figsize= (20,10))\nsns.countplot(x = train['Title'], hue = train['Survived'])\nplt.show()","be07d935":"titles = {  'Mr':     'Mr',\n                'Mrs':    'Mrs',\n                'Miss':   'Miss',\n                'Master': 'Master',\n            \n                'Ms':     'Miss',\n                'Mme':    'Mrs',\n                'Mlle':   'Miss',\n\n                'Capt':   'military',\n                'Col':    'military',\n                'Major':  'military',\n\n                'Dr':     'Dr',\n                'Rev':    'Rev',\n                  \n                'Sir':    'honor',\n                'the Countess': 'honor',\n                'Lady':   'honor',\n                'Jonkheer': 'honor',\n                'Don':    'honor',\n                'Dona':   'honor' }\n\ntrain['Title'] = train['Title'].map(titles)\ntrain['Title'] = label_encoder.fit_transform(train['Title'].values)\ntrain.head()","2e6a9b6f":"test.describe()","f20ed3e2":"plt.figure(figsize= (20,10))\nsns.histplot(train['Fare'])\nplt.show()","e8380f6a":"plt.figure(figsize = (20, 20))\nsns.violinplot(x = 'Survived', y='Fare', data = train)\nplt.show()","5bc9ca51":"sns.countplot(x = train['Fare'].apply(lambda x: x > 40), hue = train['Survived'])","35955249":"train['Fare'].fillna((train['Fare'].mean()), inplace = True)\ntrain['FareGroup'] = train['Fare']\n\ntrain.loc[(train['FareGroup'] < 40), 'FareGroup'] = 1 \ntrain.loc[(train['FareGroup'] >= 40) & (train['FareGroup'] < 90), 'FareGroup'] = 2\ntrain.loc[(train['FareGroup'] >= 90) & (train['FareGroup'] < 180), 'FareGroup'] = 3\ntrain.loc[(train['FareGroup'] >= 180) & (train['FareGroup'] < 270), 'FareGroup'] = 4\ntrain.loc[(train['FareGroup'] >= 270), 'FareGroup'] = 5\n\n\n","42b28ef3":"import re \ntrain.Cabin = train.Cabin.fillna('U')\n\ntrain['Cabin'] = train.Cabin.map(lambda x:re.compile(\"([a-zA-Z])\").search(x).group())\ntrain.groupby('Cabin').size()\ncabin_dictionary = {'A':1 , 'B':2, 'C':3 , 'D':4 , 'E':5 , 'F':6 , 'G':7 , 'T':8 , 'U':9}\ntrain = train.replace({'Cabin':cabin_dictionary})","4f36a6b4":"plt.figure(figsize= (20,10))\nsns.countplot(x = train['Embarked'], hue = train['Survived'])\nplt.show()","7100f121":"\ndummies = pd.get_dummies(train.Embarked)\ntrain = pd.concat([train, dummies], axis = 'columns')\ntrain.drop(['Embarked'], axis = 'columns', inplace = True)","a8271ad7":"train.head()","341523a1":"#Let's just drop ticket for now\ntrain = train.drop(['Ticket'], axis = 1)","09bce637":"y_trained = train['Survived']\nX_trained = train.drop(['Survived'], axis = 1)","12b7eabb":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X_trained, y_trained, test_size = 0.2, random_state = 0)\n","b8e7627e":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.multioutput import MultiOutputClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\n\n\nlr_clf = LogisticRegression(max_iter = 10000)\nxgb_clf = XGBClassifier(use_label_encoder =False, eval_metric='mlogloss')\nlgbm_clf = LGBMClassifier()\ngbf_clf = GradientBoostingClassifier()\nmnb_clf = MultinomialNB()\ndtc_clf = DecisionTreeClassifier()\nrnd_clf = RandomForestClassifier()\ngnb_clf = GaussianNB()\nvoting_clf = VotingClassifier(\n        estimators=[('lr', lr_clf), ('xgb', xgb_clf), ('lgbm', lgbm_clf),('gbf', gbf_clf),('mnb', mnb_clf), ('dtc', dtc_clf), ('rf', rnd_clf),('gnb', gnb_clf)],\n        voting='hard')\nvoting_clf.fit(X_train, y_train)","657af63a":"for clf in (lr_clf, xgb_clf ,lgbm_clf, gbf_clf, dtc_clf, rnd_clf, gnb_clf, voting_clf):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))","84984ed2":"#Grid search on xgb:\nfrom sklearn.model_selection import GridSearchCV\n'''estimator = XGBClassifier(\n    nthread=4,\n    seed=0\n)\nparameters = {\n    'max_depth': range (2, 20, 1),\n    'n_estimators': range(40, 400, 40),\n    'learning_rate': [0.3, 0.01, 0.05]\n}\n\ngrid_search = GridSearchCV(\n    estimator=estimator,\n    param_grid=parameters,\n    scoring = 'roc_auc',\n    n_jobs = 10,\n    cv = 10,\n    verbose = 6\n)\n\nxgb_model = grid_search.fit(X_train, y_train)\n\ny_pred = xgb_model.predict(X_test)\nprint(xgb_model.__class__.__name__, accuracy_score(y_test, y_pred))\nxgb_model.best_params_\n'''\n#gives {'learning_rate': 0.5, 'max_depth': 2, 'n_estimators': 160} -> gives 0.8659217877094972 -> increase lr as it is max\n","f58d10e5":"xgb_model = XGBClassifier( nthread=4, seed=0, learning_rate = 0.3, max_depth = 2, n_estimators = 160, use_label_encoder = False)\nxgb_model.fit(X_train, y_train)\ny_pred = xgb_model.predict(X_test)\nprint(xgb_model.__class__.__name__, accuracy_score(y_test, y_pred))","d4aa566a":"#Grid search on random forests\n'''estimator = RandomForestClassifier(\n    random_state = 1\n)\nrf_params = {\n    'n_estimators': range(40, 400, 40),\n    'max_features': ['sqrt', 0.25, 0.5, 0.75, 1.0],\n    'min_samples_split': [2, 4, 6]\n}\n\n\ngrid_search = GridSearchCV(\n    estimator=estimator,\n    param_grid= rf_params,\n    n_jobs = 10,\n    cv = 10,\n    verbose = 6\n)\n\nrf_model = grid_search.fit(X_train, y_train)\n\ny_pred = rf_model.predict(X_test)\nprint(rf_model.__class__.__name__, accuracy_score(y_test, y_pred))\n''' #'max_features': 0.5, 'min_samples_split': 6, 'n_estimators': 40} gave 0.8491620111731844","5ba4499c":"rf_model = RandomForestClassifier(max_features = 0.5, min_samples_split = 6, n_estimators = 40)\n\nrf_model.fit(X_train, y_train)\n\ny_pred = rf_model.predict(X_test)\nprint(rf_model.__class__.__name__, accuracy_score(y_test, y_pred))","3702e31b":"from catboost import CatBoostClassifier\n\n#simple model\nparams = {'loss_function': 'Logloss',\n          'random_seed': 0,\n          'eval_metric': 'AUC',\n          'verbose': 200\n         }\n\ncat_model = CatBoostClassifier(**params)\ncat_model.fit(X_train, y_train, eval_set = (X_test, y_test), use_best_model = True, plot = True);\n","fabb26f6":"test = test.drop(['PassengerId'], axis = 1)\ntest['Age'].fillna((test['Age'].mean()), inplace = True)\nbin = np.array([0,16,30,48, 63,75,200])\ntest['AgeGroup'] = bin.searchsorted(test['Age'])\n\ntest['FamilySize'] = test['SibSp'] + test['Parch'] + 1\n\ntest['FamilySizeGrouped'] = test['FamilySize'].map(lambda x: 'single' if x == 1 \n                                                            else ('normal' if 5 > x >= 2 \n                                                                  else ('big' if 8 > x >= 5 \n                                                                       else 'large' )\n                                                                 ))\nfamily_dict = {'single':1 , 'normal':2, 'big':3 , 'large':4}\ntest = test.replace({'FamilySizeGrouped':family_dict})\n\ntest['Sex'] = label_encoder.fit_transform(test['Sex'].values)\n\ntest['Title'] = test['Name'].str.split(',', expand = True)[1].str.split('.', expand = True)[0].str.strip(' ')\ntest = test.drop(['Name'], axis = 1)\n\ntitles = {  'Mr':     'Mr',\n                'Mrs':    'Mrs',\n                'Miss':   'Miss',\n                'Master': 'Master',\n            \n                'Ms':     'Miss',\n                'Mme':    'Mrs',\n                'Mlle':   'Miss',\n\n                'Capt':   'military',\n                'Col':    'military',\n                'Major':  'military',\n\n                'Dr':     'Dr',\n                'Rev':    'Rev',\n                  \n                'Sir':    'honor',\n                'the Countess': 'honor',\n                'Lady':   'honor',\n                'Jonkheer': 'honor',\n                'Don':    'honor',\n                'Dona':   'honor' }\n\ntest['Title'] = test['Title'].map(titles)\ntest['Title'] = label_encoder.fit_transform(test['Title'].values)\n\n\ntest['Fare'].fillna((test['Fare'].mean()), inplace = True)\ntest['FareGroup'] = test['Fare']\n\ntest.loc[(test['FareGroup'] < 40), 'FareGroup'] = 1 \ntest.loc[(test['FareGroup'] >= 40) & (test['FareGroup'] < 90), 'FareGroup'] = 2\ntest.loc[(test['FareGroup'] >= 90) & (test['FareGroup'] < 180), 'FareGroup'] = 3\ntest.loc[(test['FareGroup'] >= 180) & (test['FareGroup'] < 270), 'FareGroup'] = 4\ntest.loc[(test['FareGroup'] >= 270), 'FareGroup'] = 5\n\ntest.Cabin = test.Cabin.fillna('U')\n\ntest['Cabin'] = test.Cabin.map(lambda x:re.compile(\"([a-zA-Z])\").search(x).group())\ntest.groupby('Cabin').size()\ncabin_dictionary = {'A':1 , 'B':2, 'C':3 , 'D':4 , 'E':5 , 'F':6 , 'G':7 , 'T':8 , 'U':9}\ntest = test.replace({'Cabin':cabin_dictionary})\n\n\ndummies = pd.get_dummies(test.Embarked)\ntest = pd.concat([test, dummies], axis = 'columns')\ntest.drop(['Embarked'], axis = 'columns', inplace = True)\n\ntest = test.drop(['Ticket'], axis = 1)\n","2f581c70":"prediction_submission = rf_model.predict(test)\n\nsubmission_df = pd.DataFrame({'PassengerId': passengerIds.values,\n                             \"Survived\": prediction_submission,\n                             })\nsubmission_df.to_csv('submission_rf.csv', index = False)","55138088":"prediction_submission = xgb_model.predict(test)\n\nsubmission_df = pd.DataFrame({'PassengerId': passengerIds.values,\n                             \"Survived\": prediction_submission,\n                             })\nsubmission_df.to_csv('submission_xgb.csv', index = False)","4194c9bd":"prediction_submission = cat_model.predict(test)\n\nsubmission_df = pd.DataFrame({'PassengerId': passengerIds.values,\n                             \"Survived\": prediction_submission,\n                             })\nsubmission_df.to_csv('submission_ctb.csv', index = False)","a27f04b9":"**Survived**","1b7b6e95":"In general, people were more likely to die than survive","2d9349a3":"Might not be such a good idea to add Cabin due to how many missing values there are","ed59aeb4":"**3.4 CatBoost**","830a0977":"Big difference between S and C,Q. Anyways, let's use dummies to encode them:","d912dfe0":"**3. Modeling**","86543277":"Parch and SibSp has a high correlation (0.41). Let's make a new feature, FamilySize:","dfe9f3f9":"**Age**","d60b804e":"**Cabin**","e15d85d0":"**Sex**","49ea6cee":"**5. Save output**","28b63b5e":"**3.2 Grid Search XGB Classifier**","41b75918":"Pclass members most likely to die. Potential to combine with other features.","ada54251":"When the fare is 0-40, most likely of ending up dead, > 40 (somewhat) more likely to survive - > group\nLet's fill in missing values with mean and group:\n[0-40], [40, 90], [90,180], [180,270], [270, inf]","e7fdae75":"**Embarked**","e63330ca":"**Name**","04599586":"**Pclass**","ae5754c8":"Make another group to represent the ones who are single, in a normal sized family, large one etc.:","f97b0bb7":"isnull: 177 missing values for Age, 687 for Cabin and 2 Embarked -> has to be filled with proxy or deleted ","6caa46f6":"**Unique values\n**\n\nPclass: numerical, int between 1-3 -> keep\n\nName: categorical -> clean\n\nSex: categorical, 'male' or 'female' -> label 1 or 0\n\nAge: numerical, 88 different ages -> dummies\n\nSibSp: numerical, 0-7, -> figure out later\n\nParch: numerical, 0-7 -> figure out later\n\nTicket: categorical and numerical -> figure out later\n\nFare: numerical -> use mean (?) (figure out later)\n\nCabin: categorical -> figure out later","c367c04d":"Encode Sex:","54a8b4a2":"**2. Analyze data**","4678dfa4":"As Fare has low mean and generally low distributions (25-50-75), but a high max; - let's take a closer look:","13b0b527":"First plot from the ones who died, the second one from the ones who survived.\n[0,16],[16,30],[30,48],[48,63],[63,75],[75, inf] fits the graphs (ish).\n\nFill in the missing values and make new age groups:","f604548e":"* Delete passengerID (uninformative)\n* Pclass might be informative\n* Survived is target (drop)\n* Different titles in Name can be valuable\n* Sex and Age can be informative\n* SibSp (Siblings\/Spouses) and parch (parents\/children) can be informative \n* Ticket: on first glance, it does not say a lot, should investigate further\n* Fare might be informative \n* Cabin: needs further investigation\n* Embarked: Needs further investigation","6ac9fe50":"Mr -> likely of dying; Mrs and miss -> likely to survive\n\nWe can change titles, like sir, countess, lady, jonkheer, don and dona to 'honor', and capt, col and major to military.\n","399ad124":"**Fare**","f121b023":"**3.1 Voting Classifier**","3364f869":"**SibSp and Parch**","24936b41":"Right skewed, might be interesting to group Fare in light of survived. Let's look closer","81421e24":"**1. Import data**","beebeeeb":"**4. Process Test data**","64bd9d12":"**3.3 Grid Search Random Forest Classifier**","c10bfe3c":"Cabins are grouped by first letter: A, B, C, D, E, F, G, T, U, where each letter corresponds to 1,2,..,9 respectively\n(T not quite sure, and let U be for unknown)."}}