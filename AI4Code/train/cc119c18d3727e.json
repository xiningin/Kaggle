{"cell_type":{"68e8ddb3":"code","b25c88ad":"code","9663819c":"code","45dd5cd0":"code","ae685816":"code","778303bd":"code","79b1afe4":"code","2ac4435e":"code","00eb5a4d":"code","829b1e32":"code","0d84f93d":"code","5a0f1891":"code","476616ca":"code","68cde715":"code","02c2987c":"code","0acf3e02":"code","599b35a3":"code","b1cbc747":"code","508ffcb7":"code","db6a4c2c":"code","624d3800":"code","f266b1db":"code","7971e091":"code","2c270c9e":"code","ff0092c4":"code","d76c5b64":"code","46923fe6":"code","5887873f":"code","d3afda4c":"code","d3c47064":"code","fe690468":"code","8bc6bd3b":"code","269dbd86":"code","643ae197":"code","f7ce8481":"code","19155de8":"code","9ed43c7e":"code","6e1b4838":"code","67fdc9a0":"code","8ea8fb3e":"code","8b008b7e":"code","9bb113ce":"markdown"},"source":{"68e8ddb3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b25c88ad":"!pip install text-hammer","9663819c":"import text_hammer as th\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom tensorflow.keras import layers, Input\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom keras.losses import BinaryCrossentropy\nfrom keras.layers import Dense, LSTM, Dropout, Flatten\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence\nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n\nfrom sklearn.model_selection import train_test_split\n\nfrom  matplotlib import pyplot as plt\nimport matplotlib.image as mpimg\nimport random\n%matplotlib inline","45dd5cd0":"# Loading pretrained glove word embeddings\n!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip -q glove.6B.zip","ae685816":"# Loading the word embeddings\ndef read_glove_vecs():\n    path_to_glove_file = os.path.join(\".\/glove.6B.50d.txt\")\n\n    embeddings_index = {}\n    word_to_index = {}\n    index_to_word = {}\n    with open(path_to_glove_file) as f:\n        for line in f:\n            word, coefs = line.split(maxsplit=1)\n            coefs = np.fromstring(coefs, \"f\", sep=\" \")\n            embeddings_index[word] = coefs\n    \n    words_list = list(embeddings_index.keys())\n    \n    for i in range(len(words_list)):\n        word_to_index[words_list[i]] = i\n        index_to_word[i] = words_list[i]            \n    \n    print(\"Found %s word vectors.\" % len(embeddings_index))\n    return word_to_index, index_to_word, embeddings_index","778303bd":"word_to_index, index_to_word, embeddings_matrix = read_glove_vecs()","79b1afe4":"TRAIN_PATH = \"..\/input\/nlp-getting-started\/train.csv\"\nTEST_PATH = \"..\/input\/nlp-getting-started\/test.csv\"\nSAMPLE_PATH = \"..\/input\/nlp-getting-started\/sample_submission.csv\"\nMAX_LENGTH = 36","2ac4435e":"train_df = pd.read_csv(TRAIN_PATH, usecols=[\"id\", \"text\", \"target\"])\ntest_df = pd.read_csv(TEST_PATH, usecols=[\"id\", \"text\"])\nsample_df = pd.read_csv(SAMPLE_PATH, usecols=[\"id\", \"target\"])\nprint(train_df.head())\nprint(test_df.head())","00eb5a4d":"def clean_text_data(clean_df, colname):\n    clean_df[colname] = clean_df[colname].progress_apply(lambda x: str(x).lower())\n#     clean_df[colname] = clean_df[colname].progress_apply(lambda x: th.cont_expt(x))\n#     clean_df[colname] = clean_df[colname].progress_apply(lambda x: th.remove_emails(x))\n    clean_df[colname] = clean_df[colname].progress_apply(lambda x: th.remove_html_tags(x))\n#     clean_df[colname] = clean_df[colname].progress_apply(lambda x: th.remove_stopwords(x))\n    clean_df[colname] = clean_df[colname].progress_apply(lambda x: th.remove_special_chars(x))\n    clean_df[colname] = clean_df[colname].progress_apply(lambda x: th.remove_accented_chars(x))\n#     clean_df[colname] = clean_df[colname].progress_apply(lambda x: th.make_base(x))\n    return clean_df","829b1e32":"train_clean_df = clean_text_data(train_df, \"text\")\ntrain_clean_df.head()","0d84f93d":"train_clean_df[train_clean_df[\"target\"] == 1]","5a0f1891":"train_clean_df[train_clean_df[\"target\"] == 0]","476616ca":"print(\"max len of tweets\",max([len(x.split()) for x in train_clean_df.text]))","68cde715":"test_clean_df = clean_text_data(test_df, \"text\")\ntest_clean_df.head()","02c2987c":"X_train, X_test, y_train, y_test = train_test_split(np.array(train_clean_df[\"text\"]), np.array(train_clean_df[\"target\"]), test_size=0.2, shuffle=True)\nprint(\"X_train shape: \" + str(X_train.shape))\nprint(\"X_test shape: \" + str(X_test.shape))\nprint(\"y_train shape: \" + str(y_train.shape))\nprint(\"y_test shape: \" + str(y_test.shape))","0acf3e02":"def sentences_to_indices(X, word_to_index, max_len):\n    m = X.shape[0]\n    X_indices = np.zeros((m, max_len))\n    for i in range(m):\n        sentence_words = X[i].lower().split()\n        j = 0\n        for  w in sentence_words:\n            try:\n                X_indices[i, j] = word_to_index[w]\n            except:\n                X_indices[i, j] = 0\n            j += 1\n    X_indices = np.array(X_indices).astype(np.int64)\n    return X_indices","599b35a3":"X_train_ind = sentences_to_indices(X_train, word_to_index, max_len = MAX_LENGTH)\nX_test_ind = sentences_to_indices(X_test, word_to_index, max_len = MAX_LENGTH)\nprint(X_train_ind[:5, : ])\nprint(X_test_ind[:5, : ])","b1cbc747":"# X_train_ind = sequence.pad_sequences(X_train_ind, maxlen = MAX_LENGTH, dtype=\"object\", padding=\"post\", truncating=\"post\")\n# X_test_ind = sequence.pad_sequences(X_test_ind, maxlen = MAX_LENGTH, dtype=\"object\")","508ffcb7":"def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n    vocab_len = len(word_to_index) + 1\n    emb_dim = word_to_vec_map[\"the\"].shape[0]\n    emb_matrix = np.zeros((vocab_len, emb_dim))\n    \n    for word, index in word_to_index.items():\n        emb_matrix[index, :] = word_to_vec_map[word]\n    \n    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n    embedding_layer.build((None,))\n    embedding_layer.set_weights([emb_matrix])\n    \n    return embedding_layer","db6a4c2c":"def create_model(input_shape, word_to_vec_map, word_to_index):\n    model = Sequential()\n    \n    model.add(Input(shape=input_shape))\n    \n    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n    model.add(embedding_layer)\n    \n    model.add(LSTM(128, return_sequences=True))\n    model.add(Dropout(0.8))\n    model.add(LSTM(80))\n    model.add(Dropout(0.8))\n    model.add(Dense(1, \"sigmoid\"))\n    \n    return model","624d3800":"model = create_model(MAX_LENGTH, embeddings_matrix, word_to_index)","f266b1db":"model.summary()","7971e091":"optimizer = Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False, name=\"Adam\")\nloss_fn = BinaryCrossentropy(from_logits=True)\nmodel.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'])","2c270c9e":"X_train_tensor = tf.convert_to_tensor(X_train_ind, np.int64)\nX_train_tensor.shape","ff0092c4":"EPOCH = 20","d76c5b64":"history = model.fit(x=X_train_tensor, y=y_train, epochs=EPOCH)","46923fe6":"X_test_tensor = tf.convert_to_tensor(X_test_ind, np.int64)\nX_test_tensor.shape","5887873f":"loss = history.history[\"loss\"]\nacc = history.history[\"accuracy\"]","d3afda4c":"epoch = np.arange(EPOCH)\nplt.plot(epoch, loss)\n# plt.plot(epoch, val_loss)\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training Loss')\nplt.legend(['train', 'val'])","d3c47064":"epoch = np.arange(EPOCH)\nplt.plot(epoch, acc)\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.title('Training Accuracy');","fe690468":"eval_score = model.evaluate(X_test_tensor, y_test)\nprint(\"Test loss:\", eval_score[0])\nprint(\"Test accuracy:\", eval_score[1])","8bc6bd3b":"model.save(\"DisasterTweetv9\")","269dbd86":"# Creating a zip of the model folder \n!tar -zcvf DisasterTweetv9.tar.gz \/kaggle\/working\/DisasterTweetv6","643ae197":"# Creating submision file from here\ntest_clean_df.head()","f7ce8481":"x_pred = sentences_to_indices(np.array(test_clean_df[\"text\"]), word_to_index, max_len = MAX_LENGTH)\nx_pred.shape","19155de8":"predicted = model.predict(x_pred)\npredicted","9ed43c7e":"y_predicted = np.where(predicted>0.5, 1, 0)\ny_predicted","6e1b4838":"y_predicted = y_predicted.reshape((1, len(y_predicted)))[0]\ny_predicted","67fdc9a0":"sample_df.head()","8ea8fb3e":"sample_df[\"id\"] = test_clean_df[\"id\"]\nsample_df[\"target\"] = y_predicted\nsample_df.head()\nsample_df.shape","8b008b7e":"sample_df.to_csv(\"submission3.csv\", index=False)","9bb113ce":"# **Notice that as you train the model multiple times, it somehow retains the previous train data and give better accuracy from the start**"}}