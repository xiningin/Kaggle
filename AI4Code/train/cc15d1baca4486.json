{"cell_type":{"3f7fd560":"code","3a6754e1":"code","a6233781":"code","23a24846":"code","fe00821d":"code","43ff21e6":"code","65d8a14c":"code","3d910640":"code","385ca253":"code","fe37ea50":"code","5ade7527":"code","b53dd130":"code","849df8ca":"code","99dd6f80":"code","765e24f2":"code","1318ef6b":"code","6f000015":"code","d55d539c":"markdown","3779bde0":"markdown","f286d640":"markdown","103e3c00":"markdown","7bfc1121":"markdown","bd022a3b":"markdown","3a530542":"markdown","ee9372fc":"markdown","6d0ebcb8":"markdown","c512d7cb":"markdown","e9175864":"markdown","5b631453":"markdown","f52a8356":"markdown","0249745c":"markdown","eeae438e":"markdown","a174707f":"markdown","f47c6860":"markdown","2b3dfc00":"markdown","3d3084c0":"markdown","982ed6a9":"markdown","fdc93d6a":"markdown","9785ad5b":"markdown","372e8f04":"markdown","ae4f2d21":"markdown","0d9242e6":"markdown","d40941b3":"markdown","8317f664":"markdown","84b93de6":"markdown"},"source":{"3f7fd560":"import logging\nimport time\nimport warnings\n\nimport catboost as cb\nimport datatable as dt\nimport joblib\nimport lightgbm as lgbm\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport optuna\nimport pandas as pd\nimport seaborn as sns\nimport xgboost as xgb\nfrom optuna.samplers import TPESampler\nfrom sklearn.compose import (\n    ColumnTransformer,\n    make_column_selector,\n    make_column_transformer,\n)\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.metrics import log_loss, mean_squared_error\nfrom sklearn.model_selection import (\n    KFold,\n    StratifiedKFold,\n    cross_validate,\n    train_test_split,\n)\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\nlogging.basicConfig(\n    format=\"%(asctime)s - %(message)s\", datefmt=\"%d-%b-%y %H:%M:%S\", level=logging.INFO\n)\noptuna.logging.set_verbosity(optuna.logging.WARNING)\nwarnings.filterwarnings(\"ignore\")\npd.set_option(\"float_format\", \"{:.5f}\".format)","3a6754e1":"tps = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\").drop(\"id\", axis=1)\ntps.head().sample(10, axis=1)","a6233781":"tps.shape","23a24846":"from sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\n\n# Sample the data - 100k\ntps_sample = tps.sample(100000)\nX, y = tps_sample.drop(\"claim\", axis=1), tps_sample[[\"claim\"]].values.flatten()\n\n# Preprocess\npipe = make_pipeline(SimpleImputer(strategy=\"mean\"))\nX = pipe.fit_transform(X.copy())","fe00821d":"%%time\n\nimport umap  # pip install umap-learn\n\nmanifold = umap.UMAP().fit(X, y)\nX_reduced = manifold.transform(X)","43ff21e6":"X_reduced.shape","65d8a14c":"plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=y, s=0.5);","3d910640":"%%time\n\nfrom sklearn.preprocessing import QuantileTransformer\n\n# Preprocess again\npipe = make_pipeline(SimpleImputer(strategy=\"mean\"), QuantileTransformer())\nX = pipe.fit_transform(X.copy())\n\n# Fit UMAP to processed data\nmanifold = umap.UMAP().fit(X, y)\nX_reduced_2 = manifold.transform(X)","385ca253":"# Plot the results\nplt.scatter(X_reduced_2[:, 0], X_reduced_2[:, 1], c=y, s=0.5);","fe37ea50":"tps_june = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/train.csv\")\n\nX, y = tps_june.drop(\"target\", axis=1), tps_june[[\"target\"]].values.flatten()\n\nX.head().sample(10, axis=1)","5ade7527":"X.shape","b53dd130":"np.unique(y)","849df8ca":"import umap\nfrom sklearn.preprocessing import PowerTransformer\n\n# Scale\npipe = make_pipeline(PowerTransformer())\nX = pipe.fit_transform(X.copy())\n\n# Encode the target to numeric\ny_encoded = pd.factorize(y)[0]","99dd6f80":"%%time\n\nmanifold = umap.UMAP().fit(X, y_encoded)","765e24f2":"import umap.plot  # pip install umap-learn[plot]\n\numap.plot.points(manifold, labels=y, theme=\"fire\");","1318ef6b":"n_neighbors = [15, 100, 250, 1000]\n\ntps_june = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/train.csv\").sample(15000)\n\nX, y = tps_june.drop(\"target\", axis=1), tps_june[[\"target\"]].values.flatten()\n\n# Scale\npipe = make_pipeline(PowerTransformer())\nX = pipe.fit_transform(X.copy())\n\n# Encode the target to numeric\ny_encoded = pd.factorize(y)[0]\n\nfig, ax = plt.subplots(2, 2, figsize=(20, 20))\nfor ax, n in zip(ax.flatten(), n_neighbors):\n    manifold = umap.UMAP(n_neighbors=n, random_state=1121218)\n    manifold.fit(X, y_encoded)\n    umap.plot.points(manifold, labels=y, ax=ax, theme=\"fire\")\n    ax.set_title(f\"UMAP with n_neighbors={n}\", fontsize=\"x-large\")\n\n\nfig.savefig(\"multiple_neighbors.png\", dpi=300);","6f000015":"import umap.plot  # pip install umap-learn[plot]\nfrom sklearn.preprocessing import PowerTransformer\ndistances = [0.1, 0.3, 0.7, 0.9]\n\ntps_june = pd.read_csv(\"..\/input\/tabular-playground-series-jun-2021\/train.csv\").sample(15000)\n\nX, y = tps_june.drop(\"target\", axis=1), tps_june[[\"target\"]].values.flatten()\n\n# Scale\npipe = make_pipeline(PowerTransformer())\nX = pipe.fit_transform(X.copy())\n\n# Encode the target to numeric\ny_encoded = pd.factorize(y)[0]\n\nfig, ax = plt.subplots(2, 2, figsize=(20, 20))\nfor ax, n in zip(ax.flatten(), distances):\n    manifold = umap.UMAP(min_dist=n, random_state=1121218)\n    manifold.fit(X, y_encoded)\n    umap.plot.points(manifold, labels=y, ax=ax, theme=\"fire\")\n    ax.set_title(f\"UMAP with min_dist={n}\", fontsize=\"x-large\")\n\n\nfig.savefig(\"multiple_distances.png\", dpi=300);","d55d539c":"# Introduction","3779bde0":"The Kaggle TPS September dataset contains ~1M rows and ~120 features with a binary target. They are all numerical, and we are pretty helpless in performing proper EDA on this dataset. Our options are limited to only printing summary statistics and plotting histograms of each feature.\n\nLet's see what UMAP can do for us. Before using it, we will sample the dataset to avoid overplotting and fill in the missing values:","f286d640":"\n`metric` represents the formula to calculate the distance between points. The default is `euclidean` but you can choose among many others, including `manhattan`, `minkowski` and `chebyshev`.","103e3c00":"# Summary","7bfc1121":"# Setup","bd022a3b":"Even though it is fun to look at, the plot does not show any clear patterns. It is because we didn't scale the features before fitting UMAP. The algorithm uses distance metrics to group similar data points, and features with higher scales bias such calculations.\n\nSo, we will choose Quantile Transformer to scale the features based on their quantiles and median. This scaling method suits the dataset better since it contains many skewed and bimodal features:","3a530542":"The target contains nine classes.\n\nAs before, we will scale all features, but this time with a straightforward log transform. Then, we fit the UMAP manifold:","ee9372fc":"By default, UMAP projects the data into two components (2D). Let's create a scatterplot colored by the target class:","6d0ebcb8":"# What is UMAP?","c512d7cb":"# How to Analyze 100-Dimensional Data with UMAP in Breathtakingly Beautiful Ways\n## Reduce dimensionality and \u201csee\u201d your data\n![](https:\/\/miro.medium.com\/max\/2000\/1*whWabNszg0B9EcGdkUQifw.jpeg)\n<figcaption style=\"text-align: center;\">\n    <strong>\n        Photo by \n        <a href='https:\/\/www.pexels.com\/@arthousestudio?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'>ArtHouse Studio<\/a>\n        on \n        <a href='https:\/\/www.pexels.com\/photo\/wood-man-people-art-4905089\/?utm_content=attributionCopyText&utm_medium=referral&utm_source=pexels'><\/a>\n    <\/strong>\n<\/figcaption>","e9175864":"# Better visualization with UMAP","5b631453":"First consideration while using UMAP is the RAM consumption. Under the hood, UMAP consumes a lot of memory, especially during fitting and creating diagrams like connectivity plots. I suggest running UMAP on a machine with at least 16GB of RAM.\n\nFor example, even the 200k-row dataset in the plotting section consumed ~18GB of RAM when creating the connectivity plot. The documentation suggests setting `low_memory` to True as a possible fix. Additionally, I recommend reducing the memory usage of the dataset by casting each column to the smallest subtype possible using NumPy. I have discussed dealing with out-of-memory issues at length in a previous article ([link](https:\/\/towardsdatascience.com\/how-to-work-with-million-row-datasets-like-a-pro-76fb5c381cdd?source=your_stories_page-------------------------------------)).\n\nAlso, don't forget to transform\/scale numerical features as they tend to have different scales by default. I suggest QuantileTransformer for crazy distributions like bimodals, trimodals, etc. PowerTransformer works best for skewed features. Whichever transformer you choose, the goal is always to make features as normally distributed as possible.","f52a8356":"After the fit is done, we will import the `umap.plot` package (installed separately) and plot a point cloud:","0249745c":"# Most important parameters of UMAP","eeae438e":"Doesn't it resemble a nebula from space? We can distinctly see that Class 8 dominates the space and is clustered around the center. Class 6 is also clearly distinguished from the rest. We see half a circle of mixed data points around class 8. Regarding the singleton data points, they may be classified as outliers.\n\n> A note on visualization above - we are simply passing the fitted manifold (not transformed data!) to the `points` function and specifying the labels for color encoding. I have also chosen the `fire` as a dark theme.","a174707f":"Today, we have covered the basics of UMAP and learned only a thin slice of what it can do. For further reading, I suggest checking out the documentation of the package. There, you will see different use cases of UMAP and how it is used by tech giants like Google in various projects.\nThere are also dedicated sections for comparing UMAP to other dimensionality reduction algorithms. For a math enthusiast, you can also read how UMAP works and proofs of its formulas. Thank you for reading!\n\n![](https:\/\/miro.medium.com\/max\/1400\/1*KeMS7gxVGsgx8KC36rSTcg.gif)","f47c6860":"Another critical parameter is `min_dist` which controls the literal distance between data points. You can tweak the default value of 0.1 to control the tightness of distinct point clouds. Lower values will result in clumpier embeddings, allowing you to see individual clusters more easily. This may be useful during clustering. In contrast, values close to 1 give points more breathing room and enable you to see the broader topological structure.","2b3dfc00":"![](https:\/\/cdn-images-1.medium.com\/max\/900\/1*m0-c5e45bQH7bigxoSOnyQ.gif)","3d3084c0":"You can learn more about UMAP visualizations from [this section](https:\/\/umap-learn.readthedocs.io\/en\/latest\/plotting.html) of the documentation.","982ed6a9":"Now, we are talking! UMAP managed to capture the hidden distinction between the target classes perfectly. We can also see some outliers (the dot around the yellow blob). The dataset was not so challenging after all.\n\nBut, this plot is in no way close to what I have shown you. It is still overplotted to see structural patterns within each cluster. To take this to the next level, we will use the default UMAP visualization package with many more features. And we will need a better dataset.","fdc93d6a":"This section will analyze the Kaggle TPS May competition data that categorizes ~200k eCommerce listings based on ~75 numeric qualities. Let's import it and take a quick look:","9785ad5b":"<p float=\"left\">\n  <img src=\"https:\/\/miro.medium.com\/max\/1250\/1*OMXhwgFxgwn5fLEkGrV_Pw.png\" width=\"300\" height=\"300\"\/>\n  <img src=\"https:\/\/miro.medium.com\/max\/1250\/1*rlfn-CugxKhmZ8G7sLJQJQ.png\" width=\"300\" height=\"300\"\/> \n<\/p>","372e8f04":"No matter how powerful machine learning models are, they can't quite beat the feeling you get during those \"Aha!\" moments of exploring data through rich visuals. But, there are so many histograms, scatterplots, heatmaps you can create before you say, \"this is really getting old.\"\n\nIn those moments, you need something that reminds you of how amazing and mesmerizing data can be. You need to get inspiration from masterpiece visuals like at FlowingData or relevant subreddits, but you don't have to go that far. Recently, I have been fortunate to come across UMAP \u2014 a Python package to visualize and cluster high-dimensional data in breathtakingly beautiful ways. It was just what I needed to remember why I got into learning data science two years ago.\n\nToday, we will learn how to analyze multi-dimensional datasets by projecting them to 2D using Uniform Manifold Approximation & Projection (UMAP) package through visuals like below:\n\n<p float=\"left\">\n  <img src=\"https:\/\/miro.medium.com\/max\/1250\/1*OMXhwgFxgwn5fLEkGrV_Pw.png\" width=\"300\" height=\"300\"\/>\n  <img src=\"https:\/\/miro.medium.com\/max\/3750\/1*GEeKKJET7WzzrGcWhL3H-A.png\" width=\"300\" height=\"300\"\/>\n  <img src=\"https:\/\/miro.medium.com\/max\/1250\/1*rlfn-CugxKhmZ8G7sLJQJQ.png\" width=\"300\" height=\"300\"\/> \n<\/p>","ae4f2d21":"You can also create connectivity plots with `umap.plot.connectivity` for diagnostics purposes and to better understand the manifold structure. Do note that creating these plots is time-consuming and computation\/memory-heavy.","0d9242e6":"# Best practices of using UMAP","d40941b3":"UMAP is a [dimensionality reduction](https:\/\/en.wikipedia.org\/wiki\/Dimensionality_reduction) algorithm and a powerful data analysis tool.\n\nIt is similar to PCA (Principal Component Analysis) in terms of speed and resembles tSNE to reduce dimensionality while preserving as much information of the dataset as possible. Before the UMAP algorithm was introduced in 2018, PCA and tSNE had two most significant flaws:\n\n1) PCA was very fast at the cost of losing finer details of the data after the reduction\n2) tSNE was extremely slow even though it preserves the underlying structure of the data.\n\nWe will talk more about these differences later. Now, it is time to get the first taste of UMAP, and we will immediately start with a challenging dataset:","8317f664":"Here, the target represents whether or not a client claims their insurance.\n\nAfter [installing](https:\/\/umap-learn.readthedocs.io\/en\/latest\/#:~:text=install%20umap-learn-,User%20Guide%20\/%20Tutorial%3A,-How%20to%20Use) and importing UMAP, we initialize the manifold algorithm and fit it to `X`, `y` in the familiar Sklearn `fit\/transform` pattern:","84b93de6":"The underlying reduction algorithm has many parameters that can significantly impact the manifold and hence, the visuals. The four most important ones are:\n- `n_components`\n- `n_neighbors`\n- `min_dist`\n- `metric`\n\nAs you might have guessed, `n_components` controls the number of dimensions after the projection. The default is 2 for its ease of visualization. However, for datasets with more than 100 features, 2D may not be enough to preserve the underlying topological structure of the data fully. I recommend trying values between 2-20 at steps of 5 and evaluate different baseline models to see the change in accuracy.\n\nNext, we have `n_neighbors`. It controls the area of the local neighborhood UMAP looks at for each sample when building the manifold. Smaller values narrow the focus to local structure, taking into account peculiarities and small patterns, potentially losing the big picture.\n\nHigher values to `n_neighbors` give more flexibility and allows UMAP to focus on a broader \"view\" of the data in the corresponding dimension. This, of course, comes at the cost of losing the finer details of the structure. The default value for this parameter is 15."}}