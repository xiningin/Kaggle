{"cell_type":{"a6d52c62":"code","717576c7":"code","4daac87f":"code","d0030a60":"code","da1280db":"code","d42569df":"code","2912acb3":"code","8edeedbf":"code","3cc0d025":"code","ada064d9":"code","f8fbf604":"code","2c08c040":"code","6474c351":"code","204aae75":"code","c5b4d743":"code","23d29873":"code","5c14e9a1":"code","db963690":"code","d67e7153":"code","2a6ffb35":"code","db6b4eb6":"code","b3e445a5":"code","7ae4cd3f":"code","e96c04ac":"code","cdbb5900":"code","95abfc37":"code","fefe4adf":"code","c26ab890":"code","436f189c":"code","c5e6d4b1":"code","7ee3fb89":"code","cd109c91":"code","73f53a36":"code","5cf060a4":"code","7c165cba":"code","a57173a4":"code","704a1a87":"code","cb926ad4":"code","b5b87756":"code","4d9077cf":"code","81926c4f":"code","82e4f6ec":"code","5c497644":"code","3f5e86e2":"code","9f0657e0":"code","59857688":"code","1b4ba487":"code","e4783f61":"code","0732e87f":"code","7223f75e":"markdown","5d2e3a1e":"markdown","f1dc2ff6":"markdown"},"source":{"a6d52c62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","717576c7":"\nimport pandas as pd\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import datasets, ensemble\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nimport datetime as dt\nfrom sklearn import  metrics \nfrom sklearn.metrics import  make_scorer \nimport xgboost as xgb\nfrom xgboost.sklearn import XGBClassifier\nfrom bayes_opt import BayesianOptimization\nfrom skopt  import BayesSearchCV \nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nimport time\nimport sys\nfrom catboost import CatBoostClassifier","4daac87f":"train= pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/train.csv')\ntest= pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/test.csv')\n","d0030a60":"train['Response'].value_counts()\n","da1280db":"\ntrain.head()\n","d42569df":"bins = [18, 30, 40, 50, 60, 70, 80, 90, 100]\nlab = ['2', '3', '4', '5', '6', '7','8','9']\ntrain['agerange'] = pd.cut(train['Age'], bins, labels = lab,include_lowest = True)\ntest['agerange'] = pd.cut(test['Age'], bins, labels = lab,include_lowest = True)\n\ntrain['agerange']=train['agerange'].apply(np.int64)\ntest['agerange']=test['agerange'].apply(np.int64)\n\n","2912acb3":"train['agerange']=train['agerange'].astype(int)\ntest['agerange']=test['agerange'].astype(int)","8edeedbf":"train['Vehicle_Damage']=train['Vehicle_Damage'].map({'Yes': 10,'No': 1}).astype(int)\ntest['Vehicle_Damage']=test['Vehicle_Damage'].map({'Yes': 10,'No': 1}).astype(int)\n\ntrain['Vehicle_Age']=train['Vehicle_Age'].map({'< 1 Year' : 0,'1-2 Year': 1,'> 2 Years': 2}).astype(int)\ntest['Vehicle_Age']=test['Vehicle_Age'].map({'< 1 Year' : 0,'1-2 Year': 1,'> 2 Years': 2}).astype(int)\n\ntrain['Vehicle_Age'].apply(np.int64)\ntest['Vehicle_Age'].apply(np.int64)\n\ntrain['Vehicle_Damage'].apply(np.int64)\ntest['Vehicle_Damage'].apply(np.int64)\n\ntrain['Vehicle_Damage_Age']= train['Vehicle_Damage']*train['Vehicle_Age']\ntest['Vehicle_Damage_Age']= test['Vehicle_Damage']*test['Vehicle_Age']\n\ntrain['mean_annual_premium_policy_sales_channel']=train.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('mean')\ntest['mean_annual_premium_policy_sales_channel']=test.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('mean')\n\ntrain['max_annual_premium_policy_sales_channel']=train.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('max')\ntest['max_annual_premium_policy_sales_channel']=test.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('max')\n\ntrain['sum_annual_premium_policy_sales_channel']=train.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('sum')\ntest['sum_annual_premium_policy_sales_channel']=test.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('sum')\n\n\ntrain['count_annual_premium_policy_sales_channel']=train.groupby(['Policy_Sales_Channel'])['id'].transform('count')\ntest['count_annual_premium_policy_sales_channel']=test.groupby(['Policy_Sales_Channel'])['id'].transform('count')\n\ntrain['min_annual_premium_policy_sales_channel']=train.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('min')\ntest['min_annual_premium_policy_sales_channel']=test.groupby(['Policy_Sales_Channel'])['Annual_Premium'].transform('min')\n\ntrain['Annual_Premium_Vintage']=train['Annual_Premium']* train['Vintage']\ntest['Annual_Premium_Vintage']=test['Annual_Premium']* test['Vintage']\n\n","3cc0d025":"train['Gender']=train['Gender'].map({'Male' : 0,'Female': 1}).astype(int)\ntest['Gender']=test['Gender'].map({'Male' : 0,'Female': 1}).astype(int)\n","ada064d9":"train.columns","f8fbf604":"train.head()","2c08c040":"train.dtypes","6474c351":"from sklearn.utils.class_weight import compute_class_weight\n\nc= np.unique(train['Response'])\n\nweigh = compute_class_weight(class_weight='balanced',classes= c,  y=train['Response'])\n#weigh= pd.Series(weigh)","204aae75":"#46710","c5b4d743":"train[train['Response']==0].count()","23d29873":"y=train['Response']","5c14e9a1":"predictors = [x for x in train.columns if x not in ['Response', 'id']]","db963690":"def modelfit(alg, dtrain, predictors,useTrainCV=True, cv_folds=5, early_stopping_rounds=50):\n    \n    if useTrainCV:\n        xgb_param = alg.get_xgb_params()\n        xgtrain = xgb.DMatrix(dtrain[predictors].values, label=y)\n        cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=alg.get_params()['n_estimators'], nfold=cv_folds,\n            metrics='auc', early_stopping_rounds=early_stopping_rounds, verbose_eval=None)\n        alg.set_params(n_estimators=cvresult.shape[0])\n    \n    #Fit the algorithm on the data\n    alg.fit(dtrain[predictors], y,eval_metric='auc')\n        \n    #Predict training set:\n    dtrain_predictions = alg.predict(dtrain[predictors])\n    dtrain_predprob = alg.predict_proba(dtrain[predictors])[:,1]\n        \n    #Print model report:\n    print (\"\\nModel Report\")\n    print (\"Accuracy : %.4g\" % metrics.accuracy_score(y, dtrain_predictions))\n    print (\"AUC Score (Train): %f\" % metrics.roc_auc_score(y, dtrain_predprob))\n                    \n    feat_imp = pd.Series(alg.booster().get_fscore()).sort_values(ascending=False)\n    feat_imp.plot(kind='bar', title='Feature Importances')\n    plt.ylabel('Feature Importance Score')","d67e7153":"predictors = [x for x in train.columns if x not in ['Response', 'id']]\nxgb1 = XGBClassifier(learning_rate =0.1, n_estimators=1000, max_depth=4, min_child_weight=4, \n                     gamma=0.45, subsample=0.8, colsample_bytree=0.8, objective= 'binary:logistic', \n                     nthread=-1, seed=27, scale_pos_weight= 7.16)\nmodelfit(xgb1, train, predictors)","2a6ffb35":"print(\"Best Iteration: {}\".format(xgb1.get_booster().best_iteration))","db6b4eb6":"param_test1 = {\n    'reg_lambda': [25,30]\n}\n\ngsearch1 = GridSearchCV(estimator = XGBClassifier( learning_rate =0.1, n_estimators=243, max_depth=4,\n                                                  min_child_weight=4, gamma=0.45, subsample=0.8, colsample_bytree=0.8, reg_alpha= 0.01,\n                                                  reg_lambda= 25,\n                                                  objective= 'binary:logistic', nthread=4, scale_pos_weight=7.16, seed=27), \n                        param_grid = param_test1, scoring='roc_auc',n_jobs=4,iid=False, cv=5)\ngsearch1.fit(train[predictors],y)\ngsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_","b3e445a5":"mod= XGBClassifier( learning_rate =0.01, n_estimators=5000, max_depth=4,\n                   min_child_weight=4, gamma=0.45, subsample=0.8, colsample_bytree=0.8, reg_alpha= 0.01,\n                   reg_lambda= 25,objective= 'binary:logistic', nthread=4, scale_pos_weight=7.16, seed=27)\nmod.fit(train[predictors],y)\npreds= mod.predict_proba(test[predictors])[:, 1]","7ae4cd3f":"pre= pd.Series(preds)\npre.value_counts()","e96c04ac":"tid=test['id']","cdbb5900":"sub = pd.concat([tid, pre], axis=1)\nsub","95abfc37":"sub.rename({0: 'Response'},axis=1, inplace= True)","fefe4adf":"sub.to_csv('submissionY.csv', index=False)","c26ab890":"params = {#'depth':[3,1,2,6,4,5,7,8,9,10],\n          'iterations':[250,100,500,1000],\n          'learning_rate':[0.03,0.001,0.01,0.1,0.2,0.3], \n          #'l2_leaf_reg':[3,1,5,10,100],\n          #'border_count':[32,5,10,20,50,100,200],\n          #'ctr_border_count':[50,5,10,20,100,200],\n          #'thread_count':-1\n}","436f189c":"categorical_features = ['Gender', 'Driving_License', 'Previously_Insured', 'Vehicle_Age','agerange',\n                        'Region_Code', 'Policy_Sales_Channel','Vehicle_Damage', \n                        'count_annual_premium_policy_sales_channel',\n                        #'count_annual_premium_region_code'\n                       ]","c5e6d4b1":"train['count_annual_premium_policy_sales_channel'].value_counts()","7ee3fb89":"\ntrain['Region_Code']=train['Region_Code'].apply(np.int64)\ntest['Region_Code']=test['Region_Code'].apply(np.int64)\n\ntrain['Policy_Sales_Channel']=train['Policy_Sales_Channel'].apply(np.int64)\ntest['Policy_Sales_Channel']=test['Policy_Sales_Channel'].apply(np.int64)","cd109c91":"x.columns","73f53a36":"y = x['Response']\nx.drop(['id','Response'],axis=1, inplace= True)\n#test.drop(['id','Response'],axis=1, inplace= True)\ntest_case_id= test['id']\ntest.drop(['id'],axis=1, inplace= True)","5cf060a4":"train, test, y_train, y_test = train_test_split(train, y,\n                                                random_state=10, test_size=0.25,\n                                                stratify=y)","7c165cba":"train.columns","a57173a4":"import catboost as cb\n#cat_features_index = [0,1,2,3,4,5,6]\n\ndef auc(m, train, test): \n    return (metrics.roc_auc_score(y_train,m.predict_proba(train)[:,1]),\n                            metrics.roc_auc_score(y_test,m.predict_proba(test)[:,1]))\n\nparams = {\n    'n_estimators' : [250],\n    'depth': [6],\n    'objective' : ['Logloss'],\n    'learning_rate' : [0.1],\n    'l2_leaf_reg': [9],\n    'bootstrap_type' : ['Bayesian'],\n    'colsample_bylevel' : [0.9],\n    'bagging_temperature' : [1],\n    'border_count':[50],\n    'num_leaves' : []\n    #'iterations': [300]\n}\ncb = cb.CatBoostClassifier(scale_pos_weight = 7.16)\ncb_model = GridSearchCV(cb, params, scoring=\"roc_auc\", cv = 3)\ncb_model.fit(train, y_train,cat_features= categorical_features)\ncb_model.cv_results_, cb_model.best_params_, cb_model.best_score_\n\n","704a1a87":"x= train.copy()","cb926ad4":"from sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.model_selection import StratifiedKFold,KFold\nfrom lightgbm  import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\n#rf = RandomForestClassifier(n_estimators=50)\n\n#categorical_features = ['Gender', 'Driving_License', 'Previously_Insured','Region_Code', 'Vehicle_Age','agerange', 'Vehicle_Damage']\n\nrf_1 = LGBMClassifier(learning_rate=0.02, boosting_type='gbdt', max_depth=6,num_leaves = 32,   objective='binary', \n                      random_state=100, n_estimators=3000 ,reg_alpha=2, reg_lambda=3, n_jobs=-1, \n                      is_unbalance= True,\n                     # class_weights =weigh,\n                      categorical_feature = categorical_features)\n\nrf_2 = CatBoostClassifier(learning_rate=0.01,\n                          #subsample=0.085, \n                          depth=6, verbose=400,\n                          bootstrap_type=\"Bayesian\",border_count = 50, colsample_bylevel = 0.9, l2_leaf_reg = 9, \n                          cat_features = categorical_features,iterations=3000,\n                          #class_weights= weigh, \n                          eval_metric='AUC',loss_function= 'Logloss',\n                          bagging_temperature =  1, scale_pos_weight = 7.16, thread_count=-1\n                          #random_seed': 42\n                         )\n\nrf_3= XGBClassifier( learning_rate =0.01, n_estimators=5000, max_depth=4,\n                    min_child_weight=4, gamma=0.45, subsample=0.8, colsample_bytree=0.8, reg_alpha= 0.01,\n                    reg_lambda= 25,objective= 'binary:logistic', nthread=-1, scale_pos_weight=7.16, seed=27)\n\n\nrf = VotingClassifier(estimators=[('CatBoost_Best', rf_2),('XGBoost_Best', rf_3),], voting='soft',weights= [6,5])\n","b5b87756":"from sklearn.model_selection import StratifiedKFold,KFold\nkf = StratifiedKFold(n_splits=5,shuffle=True,random_state=10)\nacc = []\n\nfor fold,(t_id,v_id) in enumerate(kf.split(x,y)):\n    tx = x.iloc[t_id]; ty = y.iloc[t_id]\n    vx = x.iloc[v_id]; vy = y.iloc[v_id]\n    rf.fit(tx,ty #weights = weights,\n          )\n           \n    val_y = rf.predict_proba(vx)[:,1]\n    auc_score = roc_auc_score(vy,val_y)\n    acc.append(auc_score)\n    print(f\"fold {fold} accuracy {auc_score}\")\n\nprint(f\"Mean accuracy score {np.mean(auc_score)}\")","4d9077cf":"pred= rf.predict_proba(test)[:,1]\n","81926c4f":"pre= pd.Series(pred)\nsub = pd.concat([test_case_id, pre], axis=1)\nsub.rename({0: 'Response'},axis=1, inplace= True)\nsub.head()","82e4f6ec":"sub.to_csv('latesub.csv', index=False)","5c497644":"#With Categorical features\nclf = cb.CatBoostClassifier(eval_metric=\"AUC\", depth=10, iterations= 500, l2_leaf_reg= 9, learning_rate= 0.15)\nclf.fit(train,y_train)\nauc(clf, train, test,cat_features= categorical_features)\n\n","3f5e86e2":"#With Categorical features\nclf = cb.CatBoostClassifier(eval_metric=\"AUC\",one_hot_max_size=31, \n                            depth=10, iterations= 500, l2_leaf_reg= 9, learning_rate= 0.15)\nclf.fit(train,y_train, cat_features= categorical_features)\nauc(clf, train, test)","9f0657e0":"from catboost import CatBoostClassifier\nimport numpy as np\n\nmodel = CatBoostClassifier(loss_function='Logloss',scale_pos_weight=7.16)\n\ngrid = {'learning_rate': [0.001,0.01,0.1,1],\n        #'iterations': [1000,2000],\n        #'l2_leaf_reg': [1, 3, 5, 7, 9]\n       }\n\ngrid_search_result = GridSearchCV(estimator=model, param_grid = grid, scoring= 'roc_auc')\n\ngrid_search_result.fit(train, y)\n\ngrid_search_result.cv_results_, grid_search_result.best_params_, grid_search_result.best_score_\n","59857688":"train.isnull().sum()","1b4ba487":"clf = CatBoostClassifier()\n\ncat_dims = [train.columns.get_loc(i) for i in categorical_features[:-1]] \nclf.fit(train, np.ravel(y), cat_features=categorical_features)\nres = clf.predict(test)\nprint('error:',1-np.mean(res==np.ravel(test_label)))\n","e4783f61":"def catboost_param_tune(params,train_set,train_label,cat_dims=None,n_splits=3):\n    ps = paramsearch(params)\n    # search 'border_count', 'l2_leaf_reg' etc. individually \n    #   but 'iterations','learning_rate' together\n    for prms in chain(ps.grid_search(['border_count']),\n                      ps.grid_search(['ctr_border_count']),\n                      ps.grid_search(['l2_leaf_reg']),\n                      ps.grid_search(['iterations','learning_rate']),\n                      ps.grid_search(['depth'])):\n        res = crossvaltest(prms,train_set,train_label,cat_dims,n_splits)\n        # save the crossvalidation result so that future iterations can reuse the best parameters\n        ps.register_result(res,prms)\n        print(res,prms,'best:',ps.bestscore(),ps.bestparam())\n    return ps.bestparam()\n","0732e87f":"def crossvaltest(params,train_set,train_label,cat_dims,n_splits=5):\n    kf = KFold(n_splits=n_splits,shuffle=True) \n    res = []\n    for train_index, test_index in kf.split(train_set):\n        train = train_set.iloc[train_index,:]\n        test = train_set.iloc[test_index,:]\n\n        labels = train_label.ix[train_index]\n        test_labels = train_label.ix[test_index]\n\n        clf = cb.CatBoostClassifier(**params)\n        clf.fit(train, np.ravel(labels), cat_features=cat_dims)\n\n        res.append(np.mean(clf.predict(test)==np.ravel(test_labels)))\n    return np.mean(res)","7223f75e":"CATBOOST PARAM","5d2e3a1e":"XGBOOST PARAM","f1dc2ff6":"**VOTING CLASSIFIER**"}}