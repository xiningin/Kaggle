{"cell_type":{"ad25d479":"code","e0fd6884":"code","47acc785":"code","6bb2724b":"code","ad74397d":"code","cf406b0b":"code","503c897e":"code","6a7dd10c":"code","bc70c294":"code","afa04018":"code","86be3686":"code","305489d7":"code","f67951d8":"code","ed987eb8":"code","13427a86":"code","d1c21931":"code","4ac8c107":"code","7fc9469a":"code","b1ab5196":"code","3d700d2d":"code","1300f09e":"code","b60b36ed":"code","e7c1c48c":"code","1bc0ba2e":"code","d7c6844a":"code","786053c1":"code","71991fb3":"code","71de64c2":"code","76022844":"code","1ecdf537":"code","91ca6d06":"code","2f6a990b":"code","3010ea49":"code","33359da8":"code","f1464308":"code","3b907039":"code","cd5eec77":"code","7307552c":"code","7708a8fe":"code","38c86b2b":"code","0373f4eb":"code","22d3f936":"code","4ed7dfa1":"code","fcac5b43":"code","2daf2d02":"code","5d14d65e":"markdown","241a1384":"markdown","ed667339":"markdown","f3ac51b6":"markdown","e8033315":"markdown","cdd6f35b":"markdown","d8ef6be5":"markdown","14af7477":"markdown","8350275b":"markdown","a7734dce":"markdown","3f3a3177":"markdown","71a2d2f0":"markdown","2dfaf027":"markdown","7a7e51b2":"markdown","e4cbcbbc":"markdown","5ce27628":"markdown","336f06c0":"markdown","cefacd2e":"markdown","4737ab6c":"markdown","b9d50dbe":"markdown","cf7556bc":"markdown","2ae04223":"markdown","ebdaf59e":"markdown","93e19053":"markdown","667668e1":"markdown","1e63d406":"markdown","e41ca822":"markdown","9131d7fe":"markdown","b65a2291":"markdown","12d6d93c":"markdown","906dea34":"markdown","8e01ebe1":"markdown","ef615210":"markdown","62c4f12b":"markdown","7bfc296a":"markdown","5996899e":"markdown","27371834":"markdown","e9b74e90":"markdown"},"source":{"ad25d479":"import numpy as np\nimport pandas as pd\nimport warnings\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport seaborn as sns\nimport gc\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')","e0fd6884":"import missingno as msno\n%matplotlib inline","47acc785":"train = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv').drop('id', axis=1)\ntest  = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv').drop('id', axis=1)\nss    = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')","6bb2724b":"train.head(10)","ad74397d":"train.shape, test.shape","cf406b0b":"msno.matrix(train)","503c897e":"train.loc[:, 'f0':'f99'].describe().T.style.bar(subset=['mean'], color='#206ff2')\\\n                            .background_gradient(subset=['std'], cmap='Reds')\\\n                            .background_gradient(subset=['50%'], cmap='coolwarm')\n","6a7dd10c":"sns.countplot(train['target'], palette='Set3')","bc70c294":"features = train.columns.values[0:100]\ni = 0\nsns.set_style('whitegrid')\nplt.figure()\nfig, ax = plt.subplots(10,10,figsize=(18,22))\n\nfor feature in features:\n    i += 1\n    plt.subplot(10,10,i)\n    sns.distplot(train[feature], hist=False,label='train')\n    sns.distplot(test[feature], hist=False,label='test')\n    plt.xlabel(feature, fontsize=9)\n    locs, labels = plt.xticks()\n    plt.tick_params(axis='x', which='major', labelsize=6, pad=-6)\n    plt.tick_params(axis='y', which='major', labelsize=6)\nplt.show();","afa04018":"plt.figure(figsize=(16,6))\nfeatures = train.columns.values[0:100]\nplt.title(\"Distribution of mean values per row in the train and test set\")\nsns.distplot(train[features].mean(axis=1),color=\"green\", kde=True,bins=120, label='train')\nsns.distplot(test[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","86be3686":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train and test set\")\nsns.distplot(train[features].mean(axis=0),color=\"magenta\",kde=True,bins=120, label='train')\nsns.distplot(test[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","305489d7":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per row in the train and test set\")\nsns.distplot(train[features].std(axis=1),color=\"black\", kde=True,bins=120, label='train')\nsns.distplot(test[features].std(axis=1),color=\"red\", kde=True,bins=120, label='test')\nplt.legend();plt.show()","f67951d8":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per column in the train and test set\")\nsns.distplot(train[features].std(axis=0),color=\"blue\",kde=True,bins=120, label='train')\nsns.distplot(test[features].std(axis=0),color=\"green\", kde=True,bins=120, label='test')\nplt.legend(); plt.show()","ed987eb8":"t0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per row in the train set\")\nsns.distplot(t0[features].mean(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","13427a86":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train set\")\nsns.distplot(t0[features].mean(axis=0),color=\"green\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","d1c21931":"plt.figure(figsize=(16,6))\nfeatures = train.columns.values[0:100]\nplt.title(\"Distribution of min values per row in the train and test set\")\nsns.distplot(train[features].min(axis=1),color=\"red\", kde=True,bins=120, label='train')\nsns.distplot(test[features].min(axis=1),color=\"orange\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","4ac8c107":"plt.figure(figsize=(16,6))\nfeatures = train.columns.values[0:100]\nplt.title(\"Distribution of min values per column in the train and test set\")\nsns.distplot(train[features].min(axis=0),color=\"magenta\", kde=True,bins=120, label='train')\nsns.distplot(test[features].min(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","7fc9469a":"plt.figure(figsize=(16,6))\nfeatures = train.columns.values[0:100]\nplt.title(\"Distribution of max values per row in the train and test set\")\nsns.distplot(train[features].max(axis=1),color=\"brown\", kde=True,bins=120, label='train')\nsns.distplot(test[features].max(axis=1),color=\"yellow\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","b1ab5196":"plt.figure(figsize=(16,6))\nfeatures = train.columns.values[0:100]\nplt.title(\"Distribution of max values per column in the train and test set\")\nsns.distplot(train[features].max(axis=0),color=\"blue\", kde=True,bins=120, label='train')\nsns.distplot(test[features].max(axis=0),color=\"red\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","3d700d2d":"t0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of min values per row in the train set\")\nsns.distplot(t0[features].min(axis=1),color=\"orange\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].min(axis=1),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","1300f09e":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of min values per column in the train set\")\nsns.distplot(t0[features].min(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].min(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","b60b36ed":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of max values per row in the train set\")\nsns.distplot(t0[features].max(axis=1),color=\"gold\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].max(axis=1),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","e7c1c48c":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of max values per column in the train set\")\nsns.distplot(t0[features].max(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].max(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","1bc0ba2e":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew per row in the train and test set\")\nsns.distplot(train[features].skew(axis=1),color=\"red\", kde=True,bins=120, label='train')\nsns.distplot(test[features].skew(axis=1),color=\"orange\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","d7c6844a":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew per column in the train and test set\")\nsns.distplot(train[features].skew(axis=0),color=\"magenta\", kde=True,bins=120, label='train')\nsns.distplot(test[features].skew(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","786053c1":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis per row in the train and test set\")\nsns.distplot(train[features].kurtosis(axis=1),color=\"darkblue\", kde=True,bins=120, label='train')\nsns.distplot(test[features].kurtosis(axis=1),color=\"yellow\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","71991fb3":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis per column in the train and test set\")\nsns.distplot(train[features].kurtosis(axis=0),color=\"magenta\", kde=True,bins=120, label='train')\nsns.distplot(test[features].kurtosis(axis=0),color=\"green\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","71de64c2":"t0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew values per row in the train set\")\nsns.distplot(t0[features].skew(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].skew(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","76022844":"t0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of skew values per column in the train set\")\nsns.distplot(t0[features].skew(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].skew(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","1ecdf537":"t0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis values per row in the train set\")\nsns.distplot(t0[features].kurtosis(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].kurtosis(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","91ca6d06":"t0 = train.loc[train['target'] == 0]\nt1 = train.loc[train['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of kurtosis values per column in the train set\")\nsns.distplot(t0[features].kurtosis(axis=0),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].kurtosis(axis=0),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","2f6a990b":"columns = train.columns[0:25].to_list()\ncolumns.append('target')\n\ncorr = train[columns].corr()\nf, ax = plt.subplots(figsize=(20,10))\nmask = np.triu(np.ones_like(corr, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, annot=True, mask = mask, cmap=cmap)","3010ea49":"columns = train.columns[25:50].to_list()\ncolumns.append('target')\n\ncorr = train[columns].corr()\nf, ax = plt.subplots(figsize=(20,10))\nmask = np.triu(np.ones_like(corr, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, annot=True, mask = mask, cmap=cmap)","33359da8":"columns = train.columns[50:75].to_list()\ncolumns.append('target')\n\ncorr = train[columns].corr()\nf, ax = plt.subplots(figsize=(20,10))\nmask = np.triu(np.ones_like(corr, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, annot=True, mask = mask, cmap=cmap)","f1464308":"columns = train.columns[75:100].to_list()\ncolumns.append('target')\n\ncorr = train[columns].corr()\nf, ax = plt.subplots(figsize=(20,10))\nmask = np.triu(np.ones_like(corr, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(corr, annot=True, mask = mask, cmap=cmap)","3b907039":"X = train.drop('target', axis=1).copy()\ny = train['target']\nX_test = test.copy()\n\ndel train\ngc.collect\ndel test\ngc.collect","cd5eec77":"def feature_eng(df):\n    df['sum'] = df.sum(axis=1)  \n    df['min'] = df.min(axis=1)\n    df['max'] = df.max(axis=1)\n    df['mean'] = df.mean(axis=1)\n    df['std'] = df.std(axis=1)\n    df['skew'] = df.skew(axis=1)\n    df['kurt'] = df.kurtosis(axis=1)\n    return df\n\nX = feature_eng(X)\nX_test = feature_eng(X_test)","7307552c":"display(X.head())\ndisplay(X_test.head())","7708a8fe":"def plot_new_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(2,4,figsize=(18,8))\n\n    for feature in features:\n        i += 1\n        plt.subplot(2,4,i)\n        sns.kdeplot(df1[feature], bw=0.5,label=label1)\n        sns.kdeplot(df2[feature], bw=0.5,label=label2)\n        plt.xlabel(feature, fontsize=11)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=8)\n        plt.tick_params(axis='y', which='major', labelsize=8)\n    plt.show();","38c86b2b":"features = X.columns.values[100:107]\nplot_new_feature_distribution(X, X_test, 'train', 'test', features)","0373f4eb":"from xgboost import XGBClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc","22d3f936":"params = { \n          'objective': 'binary:logistic', \n          'gpu_id': 0, \n          'n_estimators': 10000, \n          'learning_rate': 0.01, \n          'gamma': 0.25, \n          'max_depth': 4, \n          'min_child_weight': 366, \n          'subsample': 0.64, \n          'colsample_bytree': 0.78, \n          'colsample_bylevel': 0.86, \n          'reg_lambda': 0, \n          'reg_alpha': 10\n          }","4ed7dfa1":"%%time\nkf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\npreds = []\nscores = []\nfeature_importance_df = pd.DataFrame()\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X, y)):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n    \n    model = XGBClassifier(**params,\n                            booster= 'gbtree',\n                            eval_metric = 'auc',\n                            tree_method= 'gpu_hist',\n                            predictor=\"gpu_predictor\",\n                            use_label_encoder=False)\n    \n    model.fit(X_train,y_train,\n              eval_set=[(X_valid,y_valid)],\n              early_stopping_rounds=100,\n              verbose=False)\n    \n    pred_valid = model.predict_proba(X_valid)[:,1]\n    fpr, tpr, _ = roc_curve(y_valid, pred_valid)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = X.columns\n    fold_importance_df[\"importance\"] = model.feature_importances_\n    fold_importance_df[\"fold\"] = fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('||'*40)\n    \n    test_preds = model.predict_proba(X_test)[:,1]\n    preds.append(test_preds)\n    \nprint(f\"Overall Validation Score: {np.mean(scores)}\")","fcac5b43":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:107].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,28))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('Features importance (averaged\/folds)')\nplt.tight_layout()\nplt.savefig('FI.png')","2daf2d02":"predictions = np.mean(np.column_stack(preds),axis=1)\n\nss['target'] = predictions\nss.to_csv('.\/xgb.csv', index=False)\nss.head()","5d14d65e":"Distribution of **max** values per **rows** for train and test set.","241a1384":"It seems like there is common correlation coefficient(**0.11**) of some features with target variable.","ed667339":"This plot shows that there is almost equal distribution of **Target** variable.","f3ac51b6":"# Correlation\n\nBelow is a heatmap plot of first 25 features(f0-f24) and target variable.","e8033315":"Distribution of **min** values per **columns** in train set, grouped by **target**.","cdd6f35b":"Lets check if there is any missing values or not.","d8ef6be5":"# Distribution of min and max\n\nLet's check the distribution of min per row in the train and test set.","14af7477":"Distribution of **max** values per **columns** in the train set, grouped by **target**.","8350275b":"# Density plots of features\n\nHere we represent distribution of **train** and **test** in different color.","a7734dce":"Distribution of the **mean** values per **columns** in the train and test set.","3f3a3177":"Train and test seems to have almost same distribution.","71a2d2f0":"Now showing the max distribution on columns for train and test set.","2dfaf027":"Distribution of **max** values per **row** in the train set, grouped by **target**.","7a7e51b2":"**f50-f74** and **target** variable.","e4cbcbbc":"Distribution of the **mean** value per **column** in the train dataset, grouped by value of **target**.","5ce27628":"Distribution of **kurtosis** calculated per **rows** in train and test sets.","336f06c0":"Distribution of **skewness** values per **row** in the train set, grouped by **target**.","cefacd2e":"# Distribution of skew and kurtosis\n\nDistribution of **skewness** calculated per **rows** in train and test sets.","4737ab6c":"Distribution of the **standard deviation** of values per **columns** in the train and test datasets.","b9d50dbe":"Distribution of **kurtosis** calculated per **columns** in train and test sets.","cf7556bc":"**f75-f99** and **target** variable.","2ae04223":"# Distribution of mean and std\n\nLet's check the distribution of the mean values per row in the train and test set.","ebdaf59e":"Distribution of **skewness** calculated per **columns** in train and test sets.","93e19053":"Let's check the distribution of **target** in training dataset.","667668e1":"Distribution of **min** per **column** in the train and test set.","1e63d406":"Distribution of **skewness** values per **column** in the train set, grouped by **target**.","e41ca822":"Distribution of the **mean** value per **row** in the train dataset, grouped by value of **target**.","9131d7fe":"Let's check the distribution of these new engineered features.","b65a2291":"Distribution of **standard deviation** of values per **row** for train and test datasets.","12d6d93c":"Distribution of **kurtosis** values per **column** in the train set, grouped by **target**.","906dea34":"* We can clearly see that there is no missing values in the dataset.","8e01ebe1":"**f25-f49** and **target** variable.","ef615210":"Distributions of **min** values per **row** in train set, grouped by value of **target**.","62c4f12b":"# Feature Engineering","7bfc296a":"# Reference\n* https:\/\/www.kaggle.com\/gpreda\/santander-eda-and-prediction\n* https:\/\/www.kaggle.com\/subinium\/tps-oct-simple-eda","5996899e":"# Model","27371834":"Distribution of **kurtosis** values per **row** in the train set, grouped by **target**.","e9b74e90":"# Submission"}}