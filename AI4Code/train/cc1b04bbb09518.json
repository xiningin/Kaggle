{"cell_type":{"2bb03a9d":"code","9a4fc639":"code","f29659c3":"code","62221193":"code","3493f877":"code","63bc2141":"code","bc2317f6":"code","056d29ba":"code","9c6e3bec":"code","bb966f80":"code","3384d7a7":"code","d8747972":"code","3855dd80":"code","3d372d45":"code","74f8a7fa":"code","c27fd3bb":"code","96ea079d":"code","73beef44":"code","6ae8ec71":"code","24490b1c":"code","681dfd39":"code","896d830c":"code","36191f90":"code","3b7af0fa":"code","926473db":"code","a3bfcee9":"code","7d344fb8":"code","b41e0e1e":"code","ab85cecc":"code","a40fce94":"code","7cea130e":"code","68abc7cb":"code","40e6550d":"code","34909bfa":"code","2dc46f90":"code","f0630584":"code","f2a6d2e7":"code","2400caf6":"code","6dd65794":"code","9d047116":"code","e46b0898":"code","10b92108":"code","f5fc0db5":"code","10bdfa22":"code","c3e00b08":"code","7e88e05a":"code","8b6d9fb2":"code","64d064ac":"code","5b1f26fa":"code","d50f7349":"code","d910ef1c":"code","9cd22745":"code","02b81db7":"code","eecabf8f":"code","0354fd03":"code","301b61b7":"code","d1cd971f":"code","7b462bd9":"code","82997296":"code","0a3c04fd":"code","4cde8bd1":"code","d644643b":"markdown","dd609ca6":"markdown","35ebfaf5":"markdown","9f34d04d":"markdown","2a7bb0af":"markdown","8bf9c1c9":"markdown","9f0a9eb3":"markdown","8cc8864c":"markdown","9a7cf470":"markdown"},"source":{"2bb03a9d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9a4fc639":"# Importing necessary libraries used the file\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","f29659c3":"# importing files to view\nblack = pd.read_csv('..\/input\/black-friday-sales-prediction\/train.csv')\ntest_file = pd.read_csv('..\/input\/black-friday-sales-prediction\/test.csv')","62221193":"# Looking at the first 5 rows of black df\nblack.head()","3493f877":"# looking at the rows of the test file\ntest_file.head()","63bc2141":"# Looking at column null values and their data types\nblack.info()","bc2317f6":"# grouping by the product id's purchase amount(mean purchase)\nblack.groupby(['Product_ID'])['Purchase'].mean().sort_values(ascending=False)","056d29ba":"# grouping by marital status and their purchase\nblack.groupby('Marital_Status')['Purchase'].mean()","9c6e3bec":"# grouping by product category 1 and their purchase\nblack.groupby('Product_Category_1')['Purchase'].mean().sort_values(ascending=False)","bb966f80":"# grouping by product category 2 and their purchase\n\nblack.groupby('Product_Category_2')['Purchase'].mean().sort_values(ascending=False)","3384d7a7":"# grouping by product category 3 and their purchase\nblack.groupby('Product_Category_3')['Purchase'].mean().sort_values(ascending=False)","d8747972":"# grouping by age and look their purchase\nblack.groupby('Age')['Purchase'].mean().sort_values(ascending=False)","3855dd80":"# group by their city category and look their purchase\nblack.groupby('City_Category')['Purchase'].mean().sort_values(ascending=False)","3d372d45":"# grouping by gender and looking at purchase\nblack.groupby('Gender')['Purchase'].mean().sort_values(ascending=False)","74f8a7fa":"# grouping by occupation and looking at purchase\nblack.groupby('Occupation')['Purchase'].mean().sort_values(ascending=False)","c27fd3bb":"# Grouping by stay in city and looking at purchase\nblack.groupby('Stay_In_Current_City_Years')['Purchase'].mean().sort_values(ascending=False)","96ea079d":"# Counting frequency of product id\nblack.groupby(['Product_ID'])['Product_ID'].count()","73beef44":"# Creating a feature showing count of product id frequency\nblack['Prod_ID_Freq'] = black.groupby('Product_ID')['Product_ID'].transform('count')\nblack.head()","6ae8ec71":"plt.figure(figsize=(15,8))\nsns.boxplot(x='Product_Category_1', y='Purchase', data=black)\nplt.title(\"Product Category 1 Vs Purchase\")","24490b1c":"plt.figure(figsize=(15,8))\nsns.boxplot(x='Product_Category_2', y='Purchase', data=black)\nplt.title(\"Product Category 2 Vs Purchase\")","681dfd39":"plt.figure(figsize=(15,8))\nsns.boxplot(x='Product_Category_3', y='Purchase', data=black)\nplt.title(\"Product Category 3 Vs Purchase\")","896d830c":"plt.figure(figsize=(15,8))\nsns.boxplot(x='Age', y='Purchase', data=black)\nplt.title(\"Age Wise Purchase\")","36191f90":"plt.figure(figsize=(15,8))\nsns.boxplot(x='Marital_Status', y='Purchase', data=black)\nplt.title('Marital Status VS Purchase')","3b7af0fa":"plt.figure(figsize=(15,8))\nsns.boxplot(x='City_Category', y='Purchase', data=black)\nplt.title('City Category VS Purchase')","926473db":"plt.figure(figsize=(15,8))\nsns.boxplot(x='Gender', y='Purchase', data=black)\nplt.title('Gender VS Purchase')","a3bfcee9":"plt.figure(figsize=(15,8))\nsns.barplot(x='Age', y='Purchase', data=black)","7d344fb8":"plt.figure(figsize=(15,8))\nsns.barplot(x='Occupation', y='Purchase', data=black)\nplt.title('Occupation VS Purchase')","b41e0e1e":"plt.figure(figsize=(15,8))\nsns.barplot(x='Stay_In_Current_City_Years', y='Purchase', data=black)\nplt.title('Stay_In_Current_City_Years     VS    Purchase')","ab85cecc":"# plotting each feature against others using pairplot\nsns.pairplot(black)","a40fce94":"# Check the correlation and produce heatmap\ncorr = black.corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(corr, annot=True)","7cea130e":"black['Age'] = black['Age'].map({'0-17': 1, '18-25': 2, '26-35': 3, '36-45': 4, '46-50': 5, '51-55': 6, '55+': 7})\nblack['Gender'] = black['Gender'].map({'F': 0, 'M': 1})","68abc7cb":"city = pd.get_dummies(black['City_Category'],drop_first=True)\ncity.rename(columns = {'B': 'cityB', 'C': 'cityC'}, inplace=True)\nblack = pd.concat([black, city], axis=1)","40e6550d":"black['Stay_In_Current_City_Years'] = black['Stay_In_Current_City_Years'].map({'0': 0, '1': 1, '2': 2, '3': 3, '4+': 4})\nblack = black.drop('City_Category', axis=1)\nblack.head()","34909bfa":"black.info()","2dc46f90":"# Checking % of missing values\nprint((black.isnull().sum()\/len(black.index))*100)\n","f0630584":"# imputing with value 0, it won't affect the model \n# Model will ignore these zeros while fitting\n\nblack['Product_Category_3'] = black['Product_Category_3'].fillna(0)\nblack['Product_Category_2'] = black['Product_Category_2'].fillna(0)","f2a6d2e7":"black.info()","2400caf6":"#  Dropping unnecessary columns\nblack = black.drop(['User_ID', 'Product_ID'], axis=1)","6dd65794":"# dividing it into train and test\nblack_train, black_test = train_test_split(black, test_size=0.3, random_state=100)","9d047116":"# Looking at their shapes\nblack_train.shape, black_test.shape","e46b0898":"black_train.head()","10b92108":"# Using minmaxscaler to bring values in range 0-1(Chosen numeric columns excluding traget column)\nfrom sklearn.preprocessing import MinMaxScaler\nscale = MinMaxScaler()\nnum_variable = ['Age', 'Occupation','Stay_In_Current_City_Years', 'Product_Category_1', 'Product_Category_2', 'Product_Category_3', 'Prod_ID_Freq']\nblack_train[num_variable]= scale.fit_transform(black_train[num_variable])","f5fc0db5":"black_train.head()","10bdfa22":"# dividing further into Xtrain and ytrain\ny_train = black_train['Purchase']\nX_train = black_train.drop('Purchase', axis=1)","c3e00b08":"# Adding constant 1 as a column for statsmodel\nX_train_sm = sm.add_constant(X_train)\nLR = sm.OLS(y_train, X_train_sm)\nLR_model = LR.fit()","7e88e05a":"LR_model.summary()","8b6d9fb2":"# Checking VIF score so that if any feature score is > 5, we can drop it to improve our model\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif.sort_values(by='VIF',ascending=False)","64d064ac":"# predicting on Xtrain and printing first 10 score\ny_pred_train = LR_model.predict(X_train_sm)\ny_pred_train[:10]","5b1f26fa":"# residual plot for train set\nres = y_train - y_pred_train\nsns.displot(res, kde=True)","d50f7349":"# scaling test data 30%\n# Scaling test dataset(Only transform it as you have done fit on train already)\nblack_test[num_variable]= scale.transform(black_test[num_variable])","d910ef1c":"# applying on 30% test set\ny_test = black_test['Purchase']\nX_test = black_test.drop('Purchase', axis=1)","9cd22745":"# Adding constant for Xtest and predicting\nX_test_sm = sm.add_constant(X_test)\ny_test_pred = LR_model.predict(X_test_sm)","02b81db7":"# Residual plot for test data\nres = y_test - y_test_pred\nsns.displot(res, kde=True)","eecabf8f":"# Checking r2_score and using mean squared error for root mean squared error\nfrom sklearn.metrics import r2_score, mean_squared_error\nr2_score(y_true=y_test, y_pred=y_test_pred)","0354fd03":"# Root mean squared error on test data\nrmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\nrmse","301b61b7":"from sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor(max_depth=3, random_state=100)\nrfr.fit(X_train, y_train)","d1cd971f":"# root mean squared error in case of train data\nrmse_rfr = np.sqrt(mean_squared_error(y_train, rfr.predict(X_train)))\nrmse_rfr","7b462bd9":"# rmse in case of test data 30%\nrmse_rfr = np.sqrt(mean_squared_error(y_test, rfr.predict(X_test)))\nrmse_rfr","82997296":"import xgboost\nmodel = xgboost.XGBRegressor()\nmodel.fit(X_train, y_train)","0a3c04fd":"# rmse on train data\nrmse_xg = np.sqrt(mean_squared_error(y_train, model.predict(X_train)))\nrmse_xg","4cde8bd1":"# rmse on test data\nrmse_xg = np.sqrt(mean_squared_error(y_test, model.predict(X_test)))\nrmse_xg","d644643b":"# Using Random Forest","dd609ca6":"# looking and analysing the data using pandas","35ebfaf5":"<h3>We got fine result with xgboost method. We can keep this model for final test data.<\/h3>","9f34d04d":"# Model selection","2a7bb0af":"# EDA","8bf9c1c9":"# preprocessing","9f0a9eb3":"# Using XG Boost","8cc8864c":"# Here you go, we finish with train data given for black sales regression problem. You can do same methods to prepare final test data given for this problem. You can do that as a task. I'm adding link to download dataset from analytics vidya.\n<a href=\"https:\/\/datahack.analyticsvidhya.com\/contest\/black-friday\/#DiscussTab\">Click here to register and get dataset<\/a>\n","9a7cf470":"# Using Statsmodel"}}