{"cell_type":{"7e4b787b":"code","6d436a97":"code","036da518":"code","89b6ab8c":"code","7c62383e":"code","024d3efa":"code","2f5b45c6":"code","c1b361cf":"code","3a67232c":"code","d7cb0751":"code","0054f64f":"code","ff19142c":"code","b7e84684":"code","2582ef4d":"code","861c52a2":"markdown","14fe407a":"markdown","c976eb42":"markdown","c33a7292":"markdown","b205eed6":"markdown","62df3f89":"markdown","bceb06ce":"markdown","482da6ec":"markdown","c73083d7":"markdown","ae835764":"markdown","5a5e338f":"markdown","7683b46b":"markdown"},"source":{"7e4b787b":"from __future__ import unicode_literals, print_function\n\nimport numpy as np \nimport pandas as pd \nimport json\nimport glob\nimport itertools\nimport logging\nimport re\n\nimport plac\nimport random\nfrom pathlib import Path\nimport spacy\nfrom spacy.util import minibatch, compounding\nfrom tqdm import tqdm, tqdm_notebook\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\npd.options.mode.chained_assignment = None  # default='warn'","6d436a97":"# We will use provided dataset for training and testing our ner model.\n# It is easily possible to replace current testing data with search \n# engine articles for each question and get summary tables. \n\n\npath = r'..\/input\/CORD-19-research-challenge\/Kaggle\/target_tables\/1_population\/'\nall_files = glob.glob(path + \"\/*.csv\")\n\ntemp_df = []\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, header=0)\n    temp_df.append(df)\n\ndf_all_provided_summary_tables = pd.concat(temp_df, axis=0, ignore_index=True)\n# this df_all_provided_summary_tables dataframe will be using to train and test our NER model. \n# this dataframe contains all the pre summary tables curated by experts ","036da518":"# import metadata file where all the covid19 research paper metadata stored\ndf_metadata = pd.read_csv('..\/input\/CORD-19-research-challenge\/metadata.csv',\n                          low_memory=False)\n# there are some rows which contains multiple entry for location of articles\n# we will remove those and keep first\ndf_metadata.pdf_json_files = df_metadata.pdf_json_files.apply(\n    lambda x: x.split(';')[0] if pd.notnull(x) else x)\ndf_metadata.pmc_json_files = df_metadata.pmc_json_files.apply(\n    lambda x: x.split(';')[0] if pd.notnull(x) else x)","89b6ab8c":"def convert_dataturks_to_spacy(dataturks_JSON_FilePath):\n    \"\"\"\n    we have annotated our text dataset for ner model.\n    we use dataturks.com for our annotation tools and spacy \n    for ner model library. \n    this function converts the annoationa json files to\n    spacy trainable input format.\n    source: https:\/\/dataturks.com\/help\/dataturks-ner-json-to-spacy-train.php\n    \"\"\"\n    \n    try:\n        training_data = []\n        lines=[]\n        with open(dataturks_JSON_FilePath, 'r') as f:\n            lines = f.readlines()\n\n        for line in lines:\n            data = json.loads(line)\n            text = data['content']\n            entities = []\n            for annotation in data['annotation']:\n                #only a single point in text annotation.\n                point = annotation['points'][0]\n                labels = annotation['label']\n                # handle both list of labels or a single label.\n                if not isinstance(labels, list):\n                    labels = [labels]\n\n                for label in labels:\n                    #dataturks indices are both inclusive [start, end] but spacy is not [start, end)\n                    entities.append((point['start'], point['end'] + 1 ,label))\n\n\n            training_data.append((text, {\"entities\" : entities}))\n\n        return training_data\n    except Exception as e:\n        print(str(e))\n        logging.exception(\"Unable to process \" + dataturks_JSON_FilePath + \"\\n\" + \"error = \" + str(e))\n        return None","7c62383e":"def get_raw_articles_by_title(provided_titles, metadata,\n                articles_base_location='..\/input\/CORD-19-research-challenge\/'):\n    '''\n    get raw articles by title\n    \n    this method filter metadata dataframe by provided titles.\n    after that it access the json files and read them and extract\n    methods, results section and entire body text of articles.\n    then it returns static columns from metadata dataframe and abstract,\n    body_text, methods and results from articles json files\n    \n    \n    \n    provided_titles: string\n    metadata: dataframe\n    \n    return: datframe\n        'title', 'doi', 'publish_time', 'journal', 'url', 'abstract', 'body_text'\n    '''\n    methods = ['methods','method','statistical methods','materials',\n               'materials and methods','data collection','the study',\n               'study design','experimental design','objective',\n               'objectives','procedures','data collection and analysis',\n               'methodology','material and methods','the model',\n               'experimental procedures','main text']\n    \n    metadata_filtered = metadata.loc[metadata.title.isin(provided_titles)]\n    \n    # replace empty pdf_json_files column with pmc_json_files column value\n    metadata_filtered['pdf_json_files'] = \\\n    metadata_filtered.pdf_json_files.fillna(metadata_filtered.pmc_json_files)\n    # drop those rows that doesn't have location of articles \n    metadata_filtered = metadata_filtered.dropna(subset=['pdf_json_files'])\n    # create articles location for reading articles\n    metadata_filtered['articles_location'] = articles_base_location \\\n    + metadata_filtered['pdf_json_files']\n    \n    metadata_filtered['body_text'] = '' # create a column for articles body text\n    metadata_filtered['methods'] = ''\n    metadata_filtered['results'] = ''\n    # fill body_text column\n    for index, row in metadata_filtered.iterrows():\n        temp_body_text = ''\n        temp_methods = ''\n        temp_results = ''\n        with open(row['articles_location']) as file:\n            content = json.load(file)\n            for entry in content['body_text']:\n                temp_body_text = temp_body_text + entry['text']\n            # Methods\n            for entry in content['body_text']:\n                section_title = ''.join(\n                    x.lower() for x in entry['section'] \\\n                    if x.isalpha()) #remove numbers and spaces\n                if any(m in section_title for m in [''.join(\n                    x.lower() for x in m \\\n                    if x.isalpha()) for m in methods]) : \n                    temp_methods = temp_methods + entry['text']\n            # Results\n            results_synonyms = ['result', 'results']\n            for entry in content['body_text']:\n                section_title = ''.join(x.lower() for x in entry['section'] \\\n                                        if x.isalpha())\n                if any(r in section_title for r in results_synonyms) :\n                    temp_results = temp_results + entry['text']\n                    \n        metadata_filtered.at[index, 'body_text'] = temp_body_text\n        metadata_filtered.at[index, 'methods'] = temp_methods\n        metadata_filtered.at[index, 'results'] = temp_results\n        \n    metadata_filtered = metadata_filtered.rename(\n        columns={'title': 'Study', 'publish_time': 'Date'})\n    return metadata_filtered[['Study', 'doi', 'Date',\n                              'journal', 'url', 'abstract',\n                              'methods', 'results', 'body_text']]\n\n\n\n\ndef preprocess_articles(raw_articles_dataframe):\n    '''\n    clean abstract, body text for performance\n    \n    this function clean articles abstract, methods, results and body_text section\n    after that it combine articles abstract, methods and results section into one \n    column. sometimes articles doesn't have abstract other than the title\n    we will be using first 1500 letters from body text. \n    \n    \n    \n    raw_articles_dataframe: dataframe\n        this dataframe should contain articles abstract,\n        methods, results, body_text.\n        ideal dataframe is the return of \n        get_raw_articles_by_title() function\n    \n    '''\n    raw_articles_dataframe['abstract'] = \\\n    raw_articles_dataframe['abstract']\\\n    .fillna(raw_articles_dataframe.body_text.str[:1500])\n    \n    raw_articles_dataframe['shorten_full_article_text'] = \\\n    raw_articles_dataframe['Study'] \\\n    + \"\\n\\n\" + raw_articles_dataframe['abstract'] \\\n    + \"\\n\\n\" + raw_articles_dataframe['methods'] \\\n    + \"\\n\\n\" + raw_articles_dataframe['results']\n    \n    \n    # remove (), [] and all text between baraces and normalize whitespace\n    raw_articles_dataframe['shorten_full_article_text'] = \\\n    raw_articles_dataframe['shorten_full_article_text']\\\n    .str.replace(r\"\\s*([\\(\\[]).*?([\\)\\]])\",\"\").str.strip()\n    \n    # remove all urls from text\n    raw_articles_dataframe['shorten_full_article_text'] = \\\n    raw_articles_dataframe['shorten_full_article_text']\\\n    .str.replace(r\"http\\S+|www.\\S+\",\"\").str.strip()\n    \n    # remove all single digit number\n    raw_articles_dataframe['shorten_full_article_text'] = \\\n    raw_articles_dataframe['shorten_full_article_text']\\\n    .str.replace(r\"(?<!\\d)[1-7]\\b\",\"\").str.strip()\n    \n    \n    \n    \n\n    \n    return raw_articles_dataframe\n\n\ndef generate_articles_for_annotation(processed_articles_dataframe):\n    '''\n    this function generate text for annotation\n    input is the dataframe that contains process full articles text\n    this function just make .txt file for each row of processed\n    dataframe in preprocess_articles() function\n    \n    note: hold some data from processed_articles_dataframe before providing\n    to this function and you can use it in later for testing the model\n    \n    '''\n    temp = pd.DataFrame(columns=['articles'])\n    temp['articles'] = processed_articles_dataframe['shorten_full_article_text']\n    temp = temp.dropna()\n    \n    temp = temp.reset_index(drop=True)\n    \n    file = '.\/{}.txt'\n    for i, row in temp.iterrows():\n        with open(file.format(str(i)), 'w') as f:\n            f.write(str(row['articles']))\n            \n    return \"TEXT SAVE TO WORKING DIRECTORY\"\n\ndef training_ner_model(training_data, model=None, output_dir='.\/', n_iter=100):\n    \n    '''\n    this function build and train a spacy ner model\n    '''\n    \n    TRAIN_DATA = training_data.copy()\n    print('Training started...')\n    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n    if model is not None:\n        nlp = spacy.load(model)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n\n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n\n    # add labels\n    for _, annotations in TRAIN_DATA:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        # reset and initialize the weights randomly \u2013 but only if we're\n        # training a new model\n        if model is None:\n            nlp.begin_training()\n        for itn in tqdm_notebook(range(n_iter)):\n            random.shuffle(TRAIN_DATA)\n            losses = {}\n            # batch up the examples using spaCy's minibatch\n            batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(\n                    texts,  # batch of texts\n                    annotations,  # batch of annotations\n                    drop=0.5,  # dropout - make it harder to memorise data\n                    losses=losses,\n                )\n    print('Training completed.')\n    return nlp\n\n\n# function for extracting data field value\ndef _ner_apply(text):\n    '''\n    this function is the inside function of pandas apply\n    this function apply trained ner model to pandas sereis and \n    extract labels \n    '''\n    # pass our text to spacy\n    # it will return us doc (spacy doc)\n    doc = ner_model(text)\n    # return list of tuples look like \n    # this [('four-year', 'EDUCATION_YEARS'), ('college or university', 'SCHOOL_TYPE')]\n    return [(ent.text, ent.label_) for ent in doc.ents]\n\ndef ner_extraction(model, processed_articles):\n    \n    \"\"\"\n    return full extracted dataset.\n    \n    ner model extract in a list of dict.\n    in this function we will extract labels and process store each label\n    to new column and output a total fresh of summary tables \n    \"\"\"\n    temp = processed_articles.copy()\n    temp = temp.reset_index(drop=True)\n    # apply the function and store the result in a new column \n    temp['temp_entity'] = temp['shorten_full_article_text'].apply(lambda x: _ner_apply(x))\n\n\n    # process our data field column and seperate each column and store their value in their column\n    flatter = sorted([list(x) + [idx] for idx, y in enumerate(temp['temp_entity']) \n                      for x in y], key = lambda x: x[1]) \n\n    # Find all of the values that will eventually go in each F column                \n    for key, group in itertools.groupby(flatter, lambda x: x[1]):\n        list_of_vals = [(val, idx) for val, _, idx in group]\n\n        # Add each value at the appropriate index and F column\n        for val, idx in list_of_vals:\n            temp.loc[idx, key] = val\n    return temp","024d3efa":"# below is keyword list for each of the study design \n# we will use these keywords to indentify study design\n\n\nsystematic_review_and_meta_analysis = [\"systematic review\",\n                                       \"meta-analysis\", \"electronic search\",\n \"pooled odds ratio\", \"Cohen\\'s d\", \"difference in means\",\n \"difference between means\", \"d-pooled\", \"pooled adjusted odds ratio\",\n \"pooled OR\", \"pooled AOR\", \"pooled risk ratio\", \"pooled RR\",\n \"pooled relative risk\", \"Cochrane review\", \"PRISMA\", \"protocol\",\n \"registry\", \"search string\", \"search criteria\", \"search strategy\",\n \"eligibility criteria\", \"inclusion criteria\", \"exclusion criteria\",\n \"interrater reliability\", \"cohen\\'s kappa\", \"databases searched\",\n \"risk of bias\", \"heterogeneity\", \"i2\", \"publication bias\"]\n\nprospective_observational_study = [\"prospective\", \"followed up\",\n                                   \"baseline characteristics\",\n                                   \"lost to follow-up\", \n                                   \"number of patients potentially\",\n                                   \"eligible\", \"examined for eligibility\",\n                                   \"confirmed eligible\", \"included in the study\",\n                                   \"completing follow-up\", \"and analysed\"]\n\nretrospective_observational_study = [\"retrospective\", \"medical records review\",\n                                     \"chart review\", \"case control\",\n                                     \"data collection instrument\",\n                                     \"data abstraction forms\"]\n\ncross_sectional_study = [\"prevalence survey\", \"survey instrument\" ,\n                         \"syndromic surveillance\", \"surveillance\",\n                         \"registry data\", \"frequency\", \"response rate\",\n                         \"questionnaire development\",\n                         \"psychometric evaluation of instrument\",\n                         \"non-response bias\"]\n\ncase_series = [\"case study\", \"case series\", \"case report\",\n\"clinical findings\", \"symptoms\", \"diagnosis\", \n\"interventions\", \"outcomes\", \"dosage\",\n\"strength\", \"duration\", \"follow-up\",\n\"adherence\", \"tolerability\"]\n\nexpert_review = ['expert review', 'literature review', 'expert', 'expert conceses', 'expert reviewers']\neditorial = ['dear editorial', 'editorial', 'editorials']\necological_regression = ['regression']\n\nsimulation = [\"computer model\", \"forecast\", \"mathematical model\",\n\"statistical model\",\"stochastic model\", \"simulation\",\n\"synthetic data\", \"monte carlo\", \"bootstrap\",\n\"machine learning\", \"deep learning\",\n\"AUC\", \"area under the curve\", \"receiver-operator curve\",\n\"ROC\", \"model fit\", \"AIC\", \"Akaike Information Criterion\"]\n\n\n\n\n\nsystematic_review_and_meta_analysis = [re.escape(m) for m in systematic_review_and_meta_analysis]\nprospective_observational_study = [re.escape(m) for m in prospective_observational_study]\nretrospective_observational_study = [re.escape(m) for m in retrospective_observational_study]\ncross_sectional_study = [re.escape(m) for m in cross_sectional_study]\ncase_series = [re.escape(m) for m in case_series]\nexpert_review = [re.escape(m) for m in expert_review]\neditorial = [re.escape(m) for m in editorial]\necological_regression = [re.escape(m) for m in ecological_regression]\nsimulation = [re.escape(m) for m in simulation]\n\n\n\n\n# build a dict with key as study design name and value of keywords list\nstudy_designs = {'systematic_review_and_meta_analysis': systematic_review_and_meta_analysis, \n                'prospective_observational_study': prospective_observational_study,\n                'retrospective_observational_study': retrospective_observational_study,\n                 'cross_sectional_study' : cross_sectional_study,\n                 'case_series' : case_series,\n                 'expert_review' : expert_review,\n                 'editorial' : editorial,\n                 'ecological_regression' : ecological_regression,\n                 'simulation' : simulation\n                }","2f5b45c6":"def extract_study_design(dataframe, study_designs):\n    \n    '''\n    this function extract study design by looking for predefined \n    keywords \n    \n    source: https:\/\/www.kaggle.com\/danielwolffram\/cord-19-create-dataframe-june-9\n    '''\n    \n    df = dataframe.copy()\n    df['study_abstract'] = [set() for _ in range(len(df))]\n    df['study_methods'] = [set() for _ in range(len(df))]\n    df['study_results'] = [set() for _ in range(len(df))]\n\n    for tag in study_designs.keys():\n        for synonym in study_designs[tag]:\n            df[df.abstract.str.contains(synonym, case=False, na=False) | df.Study.str.contains(synonym, case=False, na=False)].study_abstract.apply(lambda x: x.add(tag))\n            df[df.methods.str.contains(synonym, case=False, na=False)].study_methods.apply(lambda x: x.add(tag))\n            df[df.results.str.contains(synonym, case=False, na=False)].study_results.apply(lambda x: x.add(tag))\n    \n    df['Study Design'] = df.apply(lambda x: list(x.study_abstract.union(x.study_methods).union(x.study_results)), axis=1)\n    df.study_abstract = df.study_abstract.apply(lambda x: list(x))\n    df.study_methods = df.study_abstract.apply(lambda x: list(x))\n    df.study_results = df.study_results.apply(lambda x: list(x))\n    \n    df = df.drop(['study_abstract', 'study_methods', 'study_results'], axis=1)\n    \n    return df","c1b361cf":"# temp = get_raw_articles_by_title(df_all_provided_summary_tables.Study.tolist(), df_metadata)\n# temp = preprocess_articles(temp)\n# train = temp[:50]\n# test = temp[50:]\n\n# generate_articles_for_annotation(train)","3a67232c":"# uncomment the below two line code for model training \n\n# annotation_for_training = convert_dataturks_to_spacy('..\/input\/covid19-annotation\/task_1_population_annotation.json')\n# ner_model = training_ner_model(annotation_for_training)","d7cb0751":"# load the trained model \n# model training notebook https:\/\/www.kaggle.com\/niyamatalmass\/task-1-training-ner-model\nner_model = spacy.load(\"..\/input\/task-1-training-ner-model\/task_1_ner_model\")","0054f64f":"def create_summary_table_multiple_article(model, titles, metadata_dataframe):\n    '''\n    this function provides easy to use for making summary of articles.\n    it take the trained ner model, titles of articles and our provided metadata df.\n    it output articles summary table\n    '''\n    \n    # get the articles raw text by matching provided titles\n    temp = get_raw_articles_by_title(titles, metadata_dataframe)\n    # processed those articles \n    temp = preprocess_articles(temp)\n    # extract study design from articles \n    temp = extract_study_design(temp, study_designs)\n    # extract labels using ner model\n    temp = ner_extraction(model, temp)\n    # drop unnecessary column\n    temp = temp.drop(['abstract', 'methods', 'results', 'body_text', 'shorten_full_article_text', 'temp_entity'], axis=1)\n    return temp","ff19142c":"lower_social_search_engine = pd.read_csv('..\/input\/population-articles-by-davidmezzetti\/Management of patients who are underhoused or otherwise lower social economic status.csv')\ntitles_lower_social = lower_social_search_engine.Study.tolist()\n\ncreate_summary_table_multiple_article(ner_model, titles_lower_social, df_metadata)","b7e84684":"path = r'..\/input\/population-articles-by-davidmezzetti\/'\nall_files = glob.glob(path + \"\/*.csv\")\n\ntemp_df = []\nfor filename in all_files:\n    if filename.split('\/')[3] == 'population.csv':\n        continue\n    titles = pd.read_csv(filename)['Study'].tolist()\n    summary_table = create_summary_table_multiple_article(ner_model, titles, df_metadata)\n    summary_table_name = filename.split('\/')[3]\n    summary_table.to_csv('.\/' + summary_table_name, index=False)","2582ef4d":"!ls","861c52a2":"### Ready our necessary datasets\nThis section imports necessary libraries, provided datasets and done some inital filtering. We will combine our provided hand labeled dataset and import metadata dataframe. ","14fe407a":"## Pros\n* Not based on hard-coded rules or regex. Because of the ner model learns from NLP machine learning, it is easily generalizable and accurate for future new articles \n\n* The notebook provides all the necessary function to easily make summary tables of articles by providing only articles title. \n\n* With more annotation data we can make the NER model more robust. \n\n* This model can be applied to all the tasks and any articles with custom annotations. \n\n\n## Cons\n* The NER model has trained only small subset of hand-labelled articles provided by kaggle. More annotation data will help to make the NER model more robust.","c976eb42":"# Conclusion\nThanks for reading! Hope this notebook will help the researcher to find important knowledge and help to fight COVID-19 pandemic. Thanks!","c33a7292":"# Problem\nEntire world affected by Covid-19. Experts are trying there best for solving this global crisis. Every single day thousands of research articles are published. Keeping up with these huge pools of articles is impossible. Often great articles failed to draw attention. For this reason, Kaggle as a global data science and machine learning community hosts this challenge to solve this problem. In round 2, we have to summarize articles in a tabular format. So that researcher and expert can easily get a bird's eye view of articles and more importantly filter these huge pools of articles. ","b205eed6":"# Model testing\nNow we have built our model. Let's test our model. We have trained our model using some portion of hand labelled summary table data. Now we will be using @davidmezzetti search engine data for each table. @davidmezzetti has notebooks for each task and output relative papers for each question. We will be using that to make a summary table of each question in a task and save them as CSV file. \n\nWe will make a dataset from the davidmezzetti's search engine output and use the search engine results articles titles to produce summary tables of those articles. We can also give titles of kaggle provided hand labeled data set and it will also work. ","62df3f89":"# Solution\nResearch papers have thousands of variation. There are no single patters that can be used to summarize them into a tabular format. Regex is useful but because of this large diversity, the regex usefulness is very limited. Often time it will break and if we build any regex engine it will be not very efficient. For this, we have to move to more forward technology. We will use NLP ( Natural Language Processing ) for those labels that are not possible to handle with regex. There are also some static columns like Date, Study title, Journal, their data is already available in metadata dataset. We will use some pre-hand coded keywords for extracting study design. \n\nOne of the important division of NLP is Named Entity Recognition ( NER ). It is very widely used in the NLP industry. \n> Named Entity Recognition, also known as entity extraction classifies named entities that are present in a text into pre-defined categories like \u201cindividuals\u201d, \u201ccompanies\u201d, \u201cplaces\u201d, \u201corganization\u201d, \u201ccities\u201d, \u201cdates\u201d, \u201cproduct terminologies\u201d etc. It adds a wealth of semantic knowledge to your content and helps you to promptly understand the subject of any given text.\n\n![](http:\/\/imanage.com\/wp-content\/uploads\/2014\/10\/NER1.png)\n\nThat's exactly what we need. We need to extract data from raw text (articles) to pre-defined categories like Address ```Population```, ```Challenge```, ```Solutions```. So for this project, I have build custom NER models for each task that successfully extract the necessary information for given labels and store it to a table. This is a de-facto solution for this type of problem. Because if you use regex then for new articles it may not match. But NER is a machine learning model. It can identify context and can perform very good on any new research articles. ","bceb06ce":"# Custom NER model building for articles summary\nThis is the main section of this notebook. In this section, we will be extracting all the necessary labels for summary tables for this task. This section is divided into different subsections. Each subsection has a detailed explanation. First, we will import all the provided hand-labelled target table datasets and combined them into one dataframe. Then we will get information on static columns from metadata dataset. After that, we will use a portion of that dataframe to build a custom annotation dataset. And then we will train our NER model using spacy NLP library. And we will use that model to extract labels of our target tables.","482da6ec":"# Introduction\n\n* Problem\n* Solution\n* NER extraction model\n    * Ready dataset for annotation\n    * Building and training model\n    * Testing the model\n* Real NER extraction by extracting search engine results for each question and output the results\n* Conclusion\n    ","c73083d7":"### Model training\nIn this section, we will be training our model. Because we already build our model and stored in easy to use as a function. We just here use that function to easily train our model. Because the model takes some time to train we move the training part to [different notebook](https:\/\/www.kaggle.com\/niyamatalmass\/task-1-training-ner-model) and import the model in here for reuse purpose. But we provided the code, just uncomment them and you can train the model. ","ae835764":"We are seeing a summary table of question 'Management of patients who are under-housed or otherwise lower social-economic status'. @david search engine results for this question is used for testing. We are seeing very good results. Our NER model has picked up correctly for most of the summary of the article. Next, we will be building summary tables for all question in this task and saving them as they are named. ","5a5e338f":"### Prepare training datasets\nThis section prepares the dataset for training. All this does is first get articles texts by titles provided by hand labelled data and preprocess them and combine them for annotation and save them. We have commented on this section. Because we already build our annotation dataset but you can easily uncomment them when you need to build a training dataset for annotations. ","7683b46b":"### Define all necessary functions \nThis is a very important section. In here we are defining all our necessary functions for this notebook. This function will help us to efficiently build our solution. Necessary documentation is available inside the function. Some function may be intentionally hidden to make this notebook more readable. Feel free to unhide them. "}}