{"cell_type":{"c1f90a95":"code","b94dc5e3":"code","5e74210f":"code","8f5257cb":"code","ab736434":"code","c507ac11":"code","0047100d":"code","74fb7731":"code","5b52b885":"code","22c05283":"code","621a936d":"code","1640248c":"code","937cfcba":"code","c360e855":"code","fa9851b8":"code","0fec7fdc":"code","4324aa24":"code","b014ae67":"code","fffb023b":"code","220aa1dd":"markdown","090f8e15":"markdown","ac7a20dc":"markdown","07eebc27":"markdown","04d30280":"markdown","526c2a45":"markdown","55c8f7b0":"markdown","38c65942":"markdown","600d0b05":"markdown","874df292":"markdown","2d0c1f9f":"markdown","d6faf479":"markdown","9f19897b":"markdown"},"source":{"c1f90a95":"# for some basic operations\nimport numpy as np \nimport pandas as pd\n\n# for visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# for interactive visualizations\nimport plotly.offline as py\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\n\n\nimport seaborn as sns\nimport plotly.express as px","b94dc5e3":"cyber_csv = pd.read_csv(\"\/kaggle\/input\/cybercrime\/cyber.csv\")","5e74210f":"#Top 5 Datas\ncyber_csv.head()","8f5257cb":"# Correlations between Data\n\nplt.rcParams['figure.figsize'] = (15, 12)\nsns.heatmap(cyber_csv.corr(), cmap='gray', annot=True)\nplt.show()","ab736434":"import plotly.graph_objects as go\n# Set notebook mode to work in offline\nimport plotly.offline as pyo\npyo.init_notebook_mode()\n\nfig = go.Figure()\nfig.add_trace(go.Box(y = cyber_csv['Personal'], name='Personal'))\nfig.add_trace(go.Box(y = cyber_csv['Social'], name='Social'))\nfig.add_trace(go.Box(y = cyber_csv['Technical'], name='Technical'))\nfig.add_trace(go.Box(y = cyber_csv['Motivation'], name='Motivation'))\n\nfig.show()","c507ac11":"sns.pairplot(cyber_csv)","0047100d":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=2)\n\npca.fit(cyber_csv)\nvalues = pca.transform(cyber_csv)\n\nx = values[0:,:1]\ny = values[0:,1:2]","74fb7731":"def plot_scatter(x1,x2):\n    plt.scatter(x1, x2)\n\n    plt.xlabel('component 1')\n    plt.ylabel('component 2')\n    \n    plt.show()","5b52b885":"plt.rcParams[\"figure.figsize\"] = [10, 5]\nplot_scatter(x,y)","22c05283":"X = values","621a936d":"from sklearn.cluster import KMeans\n\n# Fnding the right number of K values\n\nsum_squred_distance = []\nK = range(1,15)\nfor k in K:\n    kmeans = KMeans(n_clusters=k, random_state=0).fit(X)\n    sum_squred_distance.append(kmeans.inertia_)","1640248c":"plt.plot(K, sum_squred_distance, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","937cfcba":"kmeans = KMeans(n_clusters=3, random_state=0).fit(X)","c360e855":"labels_ = kmeans.labels_","fa9851b8":"colors = {}\n\ncolors[0] = 'b'\ncolors[1] = 'r'\ncolors[2] = 'm'\n\n#Build the color vaectore for each data frame point\ncvec = [colors[label] for label in labels_]\n\nplt.xlabel('Component 1')\nplt.ylabel('Component 2')\n\n#for construction of the legend of the plot\nr = plt.scatter(x,y, color='r');\nb = plt.scatter(x,y, color='b');\nm = plt.scatter(x,y, color='m');\n\n#Plotting DATE in X-axis and no_of_activities in Y-axis\nplt.figure(figsize =(10, 5))\nplt.scatter(x,y,c = cvec)\n\n#Building the legend\nplt.legend((r,b,m),('label 0','label 1', 'label 2'))\nplt.xticks(rotation=90)\n\nplt.xlabel('Component 1')\nplt.ylabel('Component 2')\n    \nplt.show()","0fec7fdc":"X_df = pd.DataFrame()\nX_df['component1'] = values[0:,:1].tolist()\nX_df['component2'] = values[0:,1:2].tolist()\n\nX_df['Target'] = 0\n\nfor v in X_df[kmeans.labels_==1].index:\n    X_df['Target'][v] = 1","4324aa24":"X_df.head()","b014ae67":"# Making Feature and target\n\nX = values\nY = X_df['Target']\n\nfrom sklearn.model_selection import train_test_split\n\n#make the x for train and test (also called validation data) \nxtrain,xtest,ytrain,ytest = train_test_split(X,Y,train_size=0.80,random_state=42)","fffb023b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n\nclf = LogisticRegression()\nclf.fit(xtrain, ytrain)\n\ny_predict = clf.predict(xtest)\n\naccuracy_score(ytest, y_predict)","220aa1dd":"# BoxPlot Charts","090f8e15":"# UnSupervised Learning","ac7a20dc":"## Cyber Data Analysis\n\nThe data is a dummy data. Data generated using website. \n\nlink: https:\/\/www.generatedata.com\/","07eebc27":"* Since the features are in the same range of from 0-10. There is no need to do normalization","04d30280":"# Visualizing KMeans clustering","526c2a45":"Gained Informations:\n\n1. The median value for each feature is different\n2. The max, min values are the same for all parameters\n3. More peoples seems to be more technical, motivated and deep personal traits\n4. They are less social\n\n# Pair Plot","55c8f7b0":"# Perform Supervised Learning in Labeled Data","38c65942":"## EDA\n\n* Since the data is generated. There is no error in the data regarding its values. We don't need to clean the data too much.","600d0b05":"The features are not correlated to each other . Its good. WHY?\nThe conversation will clear this.\nlink: https:\/\/datascience.stackexchange.com\/questions\/24452\/in-supervised-learning-why-is-it-bad-to-have-correlated-features","874df292":"# Using PCA to reduce the dimension of data","2d0c1f9f":"* Now me have three set of people\n* Lets add the second cluster to suspect","d6faf479":"* In the plot above the elbow is at k=3 indicating the optimal k for this dataset is 3","9f19897b":"# More Analysis Coming"}}