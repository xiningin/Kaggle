{"cell_type":{"bb2bf629":"code","717a6657":"code","c6a5c695":"code","b7c2e747":"code","c4130018":"code","baa0499b":"code","6c51fde6":"code","a96cc5cf":"code","6cb39ae0":"code","30a990e5":"code","37d47491":"code","0db9ce0c":"code","71f283d2":"code","57641ba5":"code","557992ae":"code","3010f94b":"code","e110cf54":"code","51897e0e":"code","da279a52":"code","17ae39ee":"code","e1ede3f1":"code","40282d09":"code","6c0315a6":"code","56a2ffc9":"code","b762a841":"code","290075ec":"code","f11cb693":"code","d8a5758e":"markdown","fdf959e5":"markdown","0579389f":"markdown","e02f7d24":"markdown","dba489da":"markdown","181a8ee9":"markdown","d8c2eb72":"markdown","a9a51634":"markdown"},"source":{"bb2bf629":"import numpy as np\nimport pandas as pd\nimport os\nimport json\nimport glob\nimport sys\nsys.path.insert(0, \"..\/\")\n\n#root_path = '\/kaggle\/input\/CORD-19-research-challenge\/2020-03-13'\nroot_path = '\/kaggle\/input\/CORD-19-research-challenge'\n\n# Get all the files saved into a list and then iterate over them like below to extract relevant information\n# hold this information in a dataframe and then move forward from there. \n\n# Just set up a quick blank dataframe to hold all these medical papers. \n\ncorona_features = {\"doc_id\": [None], \"source\": [None], \"title\": [None],\n                  \"abstract\": [None], \"text_body\": [None]}\ncorona_df = pd.DataFrame.from_dict(corona_features)\n\n# Cool so dataframe now set up, lets grab all the json file names. \n\n# For this we can use the very handy glob library\n\njson_filenames = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\n\n# Now we just iterate over the files and populate the data frame. \n\ndef return_corona_df(json_filenames, df, source):\n\n    for file_name in json_filenames:\n\n        row = {\"doc_id\": None, \"source\": None, \"title\": None,\n              \"abstract\": None, \"text_body\": None}\n\n        with open(file_name) as json_data:\n            data = json.load(json_data)\n\n            row['doc_id'] = data['paper_id']\n            row['title'] = data['metadata']['title']\n\n            # Now need all of abstract. Put it all in \n            # a list then use str.join() to split it\n            # into paragraphs. \n\n            abstract_list = [data['abstract'][x]['text'] for x in range(len(data['abstract']) - 1)]\n            abstract = \"\\n \".join(abstract_list)\n\n            row['abstract'] = abstract\n\n            # And lastly the body of the text. For some reason I am getting an index error\n            # In one of the Json files, so rather than have it wrapped in a lovely list\n            # comprehension I've had to use a for loop like a neanderthal. \n            \n            # Needless to say this bug will be revisited and conquered. \n            \n            body_list = []\n            for _ in range(len(data['body_text'])):\n                try:\n                    body_list.append(data['body_text'][_]['text'])\n                except:\n                    pass\n\n            body = \"\\n \".join(body_list)\n            \n            row['text_body'] = body\n            \n            # Now just add to the dataframe. \n            \n            if source == 'b':\n                row['source'] = \"BIORXIV\"\n            elif source == \"c\":\n                row['source'] = \"COMMON_USE_SUB\"\n            elif source == \"n\":\n                row['source'] = \"NON_COMMON_USE\"\n            elif source == \"p\":\n                row['source'] = \"PMC_CUSTOM_LICENSE\"\n            \n            df = df.append(row, ignore_index=True)\n    \n    return df\n\ncorona_df = return_corona_df(json_filenames, corona_df, 'b')\ncorona_out = corona_df.to_csv('kaggle_covid-19_open_csv_format.csv')","717a6657":"import pandas as pd\ncorona_df = pd.read_csv('kaggle_covid-19_open_csv_format.csv')","c6a5c695":"!ls ","b7c2e747":"corona_df.head()","c4130018":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans","baa0499b":"abstract = corona_df['abstract']\nabstract.fillna(\"\",inplace=True)\n","6c51fde6":"tfidf = TfidfVectorizer(max_features=20000, stop_words='english')\nX = tfidf.fit_transform(abstract)\n\nclustered = KMeans(n_clusters=6, random_state=0).fit_predict(X)\n\n#with_clusters=pd.DataFrame(clusters)\ncorona_df['cluster_abstract']=clustered\n\ngrouped=corona_df.groupby('cluster_abstract')\nfor gp_name, gp in grouped:\n    display(gp)\n\ngrouped.describe()","a96cc5cf":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation","6cb39ae0":"tfidf = TfidfVectorizer(max_features=1000, stop_words='english')\nX_tfidf = tfidf.fit_transform(abstract)\ntfidf_feature_names = tfidf.get_feature_names()\n\nvectorizer = CountVectorizer(stop_words='english', max_features=1000)\nX_tf = vectorizer.fit_transform(abstract)\ntf_feature_names = vectorizer.get_feature_names()","30a990e5":"no_topics = 15\n\n# Run NMF\nnmf = NMF(n_components=no_topics).fit(X_tfidf)\n\n# Run LDA\nlda = LatentDirichletAllocation(n_components=no_topics).fit(X_tf)\n","37d47491":"#extract topics\ndef display_topics(model, feature_names, no_top_words):\n    topics=[]\n    for topic_idx, topic in enumerate(model.components_):\n        #rint (\"Topic %d:\" % (topic_idx))\n        topic_words=\" \".join([feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]])\n        #rint(topic_words)\n        topics.append(topic_words)\n    return topics\n\nno_top_words = 5\n#rint(\"NMF: \")\ntopics_nmf=display_topics(nmf, tfidf_feature_names, no_top_words)\n#rint(\"\\nLDA: \")\ntopics_lda=display_topics(lda, tf_feature_names, no_top_words)\n\n#rint(topics_nmf)\n#rint(topics_lda)\n\npred_lda=lda.transform(X_tf)\npred_nmf=nmf.transform(X_tfidf)\n\nres_lda=[topics_lda[np.argmax(r)] for r in pred_lda]\nres_nmf=[topics_nmf[np.argmax(r)] for r in pred_nmf]\n\ncorona_df['topic_lda']=res_lda\ncorona_df['topic_nmf']=res_nmf","0db9ce0c":"grouped=corona_df.groupby('topic_lda')\nfor gp_name, gp in grouped:\n    display(gp)","71f283d2":"grouped.describe()","57641ba5":"grouped_nmf=corona_df.groupby('topic_nmf')\nfor gp_name, gp in grouped_nmf:\n    display(gp)","557992ae":"grouped_nmf.describe()","3010f94b":"\ncorona_df.drop(corona_df.index[0], inplace=True)\ncorona_df.dropna(subset=['title'],inplace=True)","e110cf54":"from transformers import *\nimport tensorflow as tf","51897e0e":"model_version = 'allenai\/scibert_scivocab_uncased'\ndo_lower_case = True\nmodel = TFBertModel.from_pretrained(model_version, from_pt=True)\ntokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)","da279a52":"model.summary()","17ae39ee":"list_titles = corona_df['title'].values\ntitles_encoded=[tokenizer.encode(l ,return_tensors='tf',max_length=200, pad_to_max_length=True) for l in list_titles]\n\nbatch_size=100\nbatched=[]\nfor i in range(int(len(titles_encoded)\/batch_size)+1):\n  b = titles_encoded[i*batch_size:(i+1)*batch_size]\n  batched.append(tf.concat(b,axis=0))","e1ede3f1":"from tqdm import tqdm\nembed =[]\nfor i in tqdm(range(len(batched))):\n#for i in range(200):\n  #print(i)\n  e = model(batched[i])[0]\n  mean_e = tf.reduce_mean(e, axis=1)\n  embed.append(mean_e)","40282d09":"import numpy as np\nembed_titles = np.concatenate(embed)","6c0315a6":"#!pip install umap-learn\nimport umap\nreducer = umap.UMAP()","56a2ffc9":"from bokeh.plotting import figure, show, output_notebook\nfrom bokeh.models import HoverTool, ColumnDataSource, CategoricalColorMapper\nfrom bokeh.palettes import Spectral10, Category20c\nfrom bokeh.palettes import magma\nimport pandas as pd\noutput_notebook()","b762a841":"red = reducer.fit_transform(embed_titles[:200])","290075ec":"def make_plot(red, title_list, number, color = True):\n    digits_df = pd.DataFrame(red, columns=('x', 'y'))\n    digits_df['digit'] = title_list\n    datasource = ColumnDataSource(digits_df)\n    plot_figure = figure(\n    title='UMAP projection of the article title embeddings',\n    plot_width=890,\n    plot_height=600,\n    tools=('pan, wheel_zoom, reset')\n    )\n\n    plot_figure.add_tools(HoverTool(tooltips=\"\"\"\n    <div>\n    <div>\n        <img src='@image' style='float: left; margin: 5px 5px 5px 5px'\/>\n    <\/div>\n    <div>\n        <span style='font-size: 10px; color: #224499'><\/span>\n        <span style='font-size: 10px'>@digit<\/span>\n    <\/div>\n    <\/div>\n    \"\"\"))\n    if color:\n        color_mapping = CategoricalColorMapper(factors=title_list, palette=magma(number))\n        plot_figure.circle(\n            'x',\n            'y',\n            source=datasource,\n            color=dict(field='digit', transform=color_mapping),\n            line_alpha=0.6,\n            fill_alpha=0.6,\n            size=7\n        )\n        show(plot_figure)\n    else:\n        \n        plot_figure.circle(\n            'x',\n            'y',\n            source=datasource,\n            color=dict(field='digit'),\n            line_alpha=0.6,\n            fill_alpha=0.6,\n            size=7\n        )\n        show(plot_figure)","f11cb693":"make_plot(red, list_titles[:200], 200)","d8a5758e":"Code from: https:\/\/www.kaggle.com\/cogitae\/create-corona-csv-file to create single csv file with data","fdf959e5":"# clustering","0579389f":"# Visualization of embeddings (https:\/\/www.kaggle.com\/isaacmg\/scibert-embeddings)","e02f7d24":"# LDA, NMF","dba489da":"# Load data","181a8ee9":"# SciBERT embeddings","d8c2eb72":"based on https:\/\/www.kaggle.com\/isaacmg\/scibert-embeddings\n\nTranslated to Tensorflow and optimized with batch processing","a9a51634":"This notebook contains some experiments with topic modeling and SciBERT embeddings "}}