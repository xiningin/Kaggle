{"cell_type":{"6e9f29f7":"code","aca90933":"code","100eef50":"code","9202f7da":"code","342e397d":"code","eb42ec54":"code","6be49dd5":"code","15ef3feb":"code","8d71e1b6":"code","94bacdfe":"code","d424e215":"code","1a4c23d3":"code","27735db9":"code","c75c5c29":"code","8ea35b51":"code","2e626d4d":"code","67e25e22":"code","9723da83":"code","fbd7e7a3":"code","0a515883":"code","b481dcc8":"markdown","6d7f5c3a":"markdown","7a777369":"markdown","b995a3f4":"markdown","b94bd8d8":"markdown","9782aa7b":"markdown","d2ee0917":"markdown"},"source":{"6e9f29f7":"#import all libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n","aca90933":"fake_df = pd.read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')\nreal_df = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')","100eef50":"fake_df.head()","9202f7da":"real_df.head()","342e397d":"fake_df['class'] = 0 \nreal_df['class'] = 1","eb42ec54":"fake_df.drop(['date', 'subject'], axis=1, inplace=True)\nreal_df.drop(['date', 'subject'], axis=1, inplace=True)","6be49dd5":"df = pd.concat([fake_df, real_df], ignore_index=True, sort=False)\ndf.head()","15ef3feb":"df.tail()","8d71e1b6":"# Lets combine the title and text column\n\ndf['text'] = df['title'] + df['text']\ndf.drop('title', axis=1, inplace=True)","94bacdfe":"df","d424e215":"sentences = df['text'].astype(str).str.lower().values.tolist()\nlabels = df['class'].values.tolist()","1a4c23d3":"len(sentences)","27735db9":"training_size = 35000\n\ntraining_sentences = sentences[0:training_size]\ntesting_sentences = sentences[training_size:]\ntraining_labels = labels[0:training_size]\ntesting_labels = labels[training_size:]","c75c5c29":"#Hyper parameters\n\n\nvocab_size = 10000\nembedding_dim = 40\nmax_length = 100\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\n","8ea35b51":"tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(training_sentences)\n\nword_index = tokenizer.word_index\nprint('The total words are ', len(word_index))\n\ntraining_sequences = tokenizer.texts_to_sequences(training_sentences)\ntraining_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n\ntesting_sequences = tokenizer.texts_to_sequences(testing_sentences)\ntesting_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)","2e626d4d":"# Need this block to get it to work with TensorFlow 2.x\nimport numpy as np\ntraining_padded = np.array(training_padded)\ntraining_labels = np.array(training_labels)\ntesting_padded = np.array(testing_padded)\ntesting_labels = np.array(testing_labels)","67e25e22":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Dense, LSTM, Bidirectional\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n    #tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Bidirectional(LSTM(32, return_sequences=True)),\n    tf.keras.layers.Bidirectional(LSTM(32)),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(24, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n\nprint(model.summary())","9723da83":"num_epochs = 30\nhistory = model.fit(training_padded, training_labels, epochs=num_epochs, validation_data=(testing_padded, testing_labels), verbose=2)","fbd7e7a3":"import matplotlib.pyplot as plt\n\n\ndef plot_graphs(history, string):\n  plt.plot(history.history[string])\n  plt.plot(history.history['val_'+string])\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(string)\n  plt.legend([string, 'val_'+string])\n  plt.show()\n  \nplot_graphs(history, \"accuracy\")\nplot_graphs(history, \"loss\")","0a515883":"model.evaluate(testing_padded, testing_labels)","b481dcc8":"### data preprocessing(Tokenization)","6d7f5c3a":"### Prediction","7a777369":"### Import","b995a3f4":"Lets us create a new column 'class' which holds values as '0' for fake news, '1' for real news.\nAlso drop the unneccessary columns date and subject.\nFinally concat both the datasets into one.","b94bd8d8":"Accuracy of 99.94% has been achieved.\n","9782aa7b":"### Modelling","d2ee0917":"### Load data"}}