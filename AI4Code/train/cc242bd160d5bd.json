{"cell_type":{"7749a1c1":"code","dd0a26be":"code","5a562391":"code","114781c3":"code","4593b386":"code","ad56d7fa":"code","b7a6cf52":"code","34423246":"code","3fad0784":"code","ae463c14":"code","65f828db":"code","6e8db0ab":"code","846ed67a":"code","c6d3ee5a":"code","6068276e":"markdown","863f0bc9":"markdown","d77a537a":"markdown","2aeca985":"markdown","50c3ea50":"markdown","892a8b5b":"markdown","b9a5d3b9":"markdown","6a3da5b4":"markdown","19aafdc5":"markdown","7ba5070f":"markdown","395e9256":"markdown","3c020b85":"markdown","f52ce0a8":"markdown","a45ad94c":"markdown","1c25b60b":"markdown"},"source":{"7749a1c1":"#Import libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#import datasets\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","dd0a26be":"#Separate columns into groups\nID_target_comment_text = ['id', 'target', 'comment_text']\n\nmain_indicators = ['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'sexual_explicit']\n\nidentity_columns = ['asian', 'atheist', 'bisexual', 'black', 'buddhist', 'christian',\n                    'female', 'heterosexual', 'hindu', 'homosexual_gay_or_lesbian',\n                    'intellectual_or_learning_disability', 'jewish', 'latino', 'male',\n                    'muslim', 'other_disability', 'other_gender', 'other_race_or_ethnicity',\n                    'other_religion', 'other_sexual_orientation', 'physical_disability',\n                    'psychiatric_or_mental_illness', 'transgender', 'white']\n\n# Only identities with more than 500 examples in the test set (combined public and private)\n# will be included in the evaluation calculation. \nmain_identities = ['male', 'female', 'homosexual_gay_or_lesbian',\n                    'christian', 'jewish', 'muslim', 'white', 'black',\n                    'psychiatric_or_mental_illness']\n\ncomment_properties = ['created_date', 'publication_id', 'parent_id', 'article_id']\n\nreactions = ['funny', 'wow', 'sad', 'likes', 'disagree', 'rating']\n\nannotators = ['identity_annotator_count', 'toxicity_annotator_count']","5a562391":"train.head(10)","114781c3":"#Find missing values\nnan_dict = dict()\nfor column in train.columns:\n    nan_dict[column] = train[column].isna().sum()\/len(train[column])\n\n#Find unique values\nunique_values_dict = dict()\nfor column in train.columns:\n    unique_values_dict[column] = len(train[column].unique())\/train[column].count()\n\ncolumns = list(unique_values_dict.keys())\ny_pos = np.arange(len(columns))\nunique_percentage = list(unique_values_dict.values())\nnan_percentage = list(nan_dict.values())\n\n#fig, ax = plt.subplots(figsize=(8, 10))\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 10), sharey=True)\nax1.barh(y_pos, nan_percentage, color='green', ecolor='black')\nax2.barh(y_pos, unique_percentage, color='green', ecolor='black')\n\nax1.set_title('NaN percentage in train dataset')\nax2.set_title('Unique values percentage in train dataset')\n\nax1.set_xlabel('nan_percentage percentage')\nax2.set_xlabel('unique_values percentage')\n\nax1.set_yticks(y_pos)\nax1.set_yticklabels(columns)\nax1.invert_yaxis()\n\n\nplt.show()","4593b386":"for column in main_indicators + main_identities:\n    print('-'*5, column, '-'*5, '\\n')\n    comment, target, column_value = train[['comment_text', 'target', column]][train[column] == train[column].max()].iloc[0]\n    print('target:', target)\n    print(str(column)+':', column_value)\n    print(comment, '\\n')","ad56d7fa":"test.head()","b7a6cf52":"#BIGGER IDENTITY GROUPS\n#This is how I would classify the columns. You might desagree. Feel free to change.\n#Physical disability and other disability have a single category, so it doesn't need to be grouped.\nreligion_columns = ['atheist', 'buddhist', 'christian', 'hindu', 'jewish', 'muslim', 'other_religion']\ngender_columns = ['male', 'female']\nsexuality_columns = ['heterosexual', 'homosexual_gay_or_lesbian', 'other_gender', 'other_sexual_orientation', 'transgender']\nethinicity_columns = ['black', 'latino', 'white', 'asian', 'other_race_or_ethnicity']\nmental_disability_columns = ['intellectual_or_learning_disability', 'psychiatric_or_mental_illness']\n\ntrain['is_religion_related'] = (train[religion_columns] > 0).sum(axis=1)\ntrain['is_gender_related'] = (train[gender_columns] > 0).sum(axis=1)\ntrain['is_sexuality_related'] = (train[sexuality_columns] > 0).sum(axis=1)\ntrain['is_ethinicity_related'] = (train[ethinicity_columns] > 0).sum(axis=1)\ntrain['is_mental_disability_related'] = (train[mental_disability_columns] > 0).sum(axis=1)\n\n#LIKES RATIO\npd.options.mode.chained_assignment = None  # desible copy warning - default='warn'\ntrain['disagree_to_likes'] = 0\ntrain['funny_to_likes'] = 0\ntrain['wow_to_likes'] = 0\ntrain['sad_to_likes'] = 0\ntrain['all_to_likes'] = 0\ntrain['disagree_to_likes'][train['likes'] > 0] = train['disagree'][train['likes'] > 0] \/ train['likes'][train['likes'] > 0]\ntrain['funny_to_likes'][train['likes'] > 0] = train['funny'][train['likes'] > 0] \/ train['likes'][train['likes'] > 0]\ntrain['wow_to_likes'][train['likes'] > 0] = train['wow'][train['likes'] > 0] \/ train['likes'][train['likes'] > 0]\ntrain['sad_to_likes'][train['likes'] > 0] = train['sad'][train['likes'] > 0] \/train['likes'][train['likes'] > 0]\ntrain['all_to_likes'][train['likes'] > 0] = train[['disagree', 'funny', 'wow', 'sad']][train['likes'] > 0].sum(axis = 1) \/ train['likes'][train['likes'] > 0]\n\n#COMMENTS PROPERTIES\n#rating\ntrain['rating'] = train['rating'].apply(lambda x: 1 if x =='approved' else 0)\n\n#has_parent_id\ntrain['has_parent_id'] = train['parent_id'].apply(lambda x: 1 if x > 0 else 0)\n\n#date\ntrain['created_date'] = pd.to_datetime(train['created_date'])\nearliest_date = train['created_date'].min()\ntrain['created_date'] = train['created_date'].apply(lambda x: (x - earliest_date).days)\n\n#TOXICITY RELATED TO IDENTITY\n\n#identity_degree\ntrain['identity_degree'] = (train[identity_columns] > 0).sum(axis=1)\n\n#identity_weight\ntrain['identity_weight'] = train[identity_columns].sum(axis=1)\n\n#is_identity_related\ntrain['is_identity_related'] = train['identity_degree'].apply(lambda x: 1 if x>0 else 0)\n\n#is_main_identity_related\ntrain['in_main_identity_related'] = 0\nfor identity in main_identities:\n    train['in_main_identity_related'] += train[identity].apply(lambda x: 1 if x>0 else 0)\n\ntrain['in_main_identity_related'] = train['in_main_identity_related'].apply(lambda x: 1 if x >0 else 0)\n\n#FILLING NaN\ntrain.fillna(0, inplace = True)","34423246":"#CORRELATION HEAT MAP\ncolumns = [column for column in train.columns if column not in ['id', 'comment_text', 'created_date']]\n\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train[columns].astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=False)","3fad0784":"#If you want to see the correlation matrix, uncomment the code below\ncorrelation_matrix = train[columns].corr()\n#correlation_matrix","ae463c14":"#Plots target correlation\ntarget_correlation = correlation_matrix['target'].copy()\ntarget_correlation = target_correlation.sort_values(ascending = False)\n\nplt.rcdefaults()\nfig, ax = plt.subplots(figsize=(4, 10))\n\n# Example data\ncolumns = list(target_correlation.index)\ny_pos = np.arange(len(columns))\nnan_percentage = list(target_correlation.values)\n\nax.barh(y_pos, nan_percentage, color='green', ecolor='black')\nax.set_yticks(y_pos)\nax.set_yticklabels(columns, fontsize = 9)\nax.invert_yaxis()  # labels read top-to-bottom\nax.set_xlabel('Correlation')\nax.set_title('Target correlation')\nplt.show()","65f828db":"columns_to_make_binary = ['target'] + main_indicators\n\ntrain_binary = train[columns_to_make_binary].copy()\n\nbinary_threshold = 0.5\nfor column in columns_to_make_binary:\n    train_binary[column] = train_binary[column].apply(lambda x: 1 if x >= binary_threshold else 0)\n\nfig, ax = plt.subplots(figsize=(10, 4))\nfor column in main_indicators:\n    ax.bar(column, (train_binary[column] == 1).sum())\n\nax.set_title('Main indicators occurrencies')\nplt.tight_layout()\nplt.show()","6e8db0ab":"#FIRST GRAPH\ntrain_binary = train[columns_to_make_binary].copy()\n\ntrain_binary['target'] = train_binary['target'].apply(lambda x: 1 if x >= 0.5 else 0)\n\nall_positives = (train_binary['target'] == 1).sum()\n\ntrue_positive_dict = dict()\nfalse_negative_dict = dict()\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 10))\n\nfor column in main_indicators:\n    true_positive_dict[column] = []\n    false_negative_dict[column] = []\n\n    for binary_threshold in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n        train_binary[column] = train[column].apply(lambda x: 1 if x >= binary_threshold else 0)\n        all_negatives = (train_binary['target'][train[column] > 0] == 0).sum()\n        \n        true_positives = (train_binary[column][train_binary['target'] == 1] == 1).sum()\n        false_positive = (train_binary[column][(train_binary['target'] == 0) & (train[column] > 0)] == 1).sum()\n        \n        true_positive_dict[column].append(true_positives\/all_positives)\n        false_negative_dict[column].append(false_positive\/all_negatives)\n    \n        ax1.annotate(binary_threshold, (false_positive\/all_negatives, true_positives\/all_positives))\n    \n    ax1.plot(false_negative_dict[column], true_positive_dict[column], label = column)\n    \nax1.set_ylabel('True Positive Rate', fontsize=12)\nax1.set_xlabel('False Positive Rate', fontsize=12)\n    \n\n#SECOND GRAPH\ntrain_temp = train[columns_to_make_binary][train['in_main_identity_related'] > 0].copy()\n\ntrain_binary = train_temp.copy()\ntrain_binary['target'] = train_binary['target'].apply(lambda x: 1 if x >= 0.5 else 0)\n\nall_positives = (train_binary['target'] == 1).sum()\n\ntrue_positive_dict = dict()\nfalse_negative_dict = dict()\n\nfor column in main_indicators:\n    true_positive_dict[column] = []\n    false_negative_dict[column] = []\n\n    for binary_threshold in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n        train_binary[column] = train[column].apply(lambda x: 1 if x >= binary_threshold else 0)\n        all_negatives = (train_binary['target'][train_temp[column] > 0] == 0).sum()\n        \n        true_positives = (train_binary[column][train_binary['target'] == 1] == 1).sum()\n        false_positive = (train_binary[column][(train_binary['target'] == 0) & (train_temp[column] > 0)] == 1).sum()\n\n        true_positive_dict[column].append(true_positives\/all_positives)\n        false_negative_dict[column].append(false_positive\/all_negatives)\n        ax2.annotate(binary_threshold, (false_positive\/all_negatives, true_positives\/all_positives))\n            \n    ax2.plot(false_negative_dict[column], true_positive_dict[column], label = column)\n\n\nax1.set_title('Main indicators ROC-AUC all comments')\nax2.set_title('Main Indicators ROC-AUC comments with main identities')\nax2.set_xlabel('False Positive Rate', fontsize=12)\nax2.legend(bbox_to_anchor=(1.10, 1), loc=2, borderaxespad=0.)    \nplt.show()","846ed67a":"train_binned = train.copy()\ntrain_binned['target_binned'] = pd.cut(train_binned['target'], bins = 10)\ntrain_binned = train_binned.groupby('target_binned').mean().copy()\n\nidentity_big_groups = ['is_identity_related', 'is_religion_related', 'is_gender_related', 'is_sexuality_related',\n            'is_ethinicity_related', 'is_mental_disability_related', 'identity_degree',\n           'identity_weight']\n\nlikes_ratio = ['disagree_to_likes', 'funny_to_likes', 'wow_to_likes', 'sad_to_likes', 'all_to_likes', 'rating']\ntitles = ['Identity Big Groups', 'Main Identities', 'Reactions', 'Likes Ratio', 'Annotators']\nfor columns, title in zip([identity_big_groups, main_identities, reactions, likes_ratio, annotators], titles):\n    fig, ax1 = plt.subplots()\n    ax2 = ax1.twinx()\n\n    ax1.hist(train['target'], color = 'grey', bins = np.array(range(11))\/10, label = 'Target Histogram')\n    ax1.set_ylabel('Target Frequency', fontsize=12)\n    for i in range(len(columns)):\n        ax2.plot(train_binned['target'], train_binned[columns[i]], label = columns[i])\n    ax1.legend(bbox_to_anchor=(1.15, 1), loc=2, borderaxespad=0.)\n    ax2.legend(bbox_to_anchor=(1.15, 0.9), loc=2, borderaxespad=0.)\n    ax2.set_ylabel('Feature Value', fontsize=12)\n    ax1.set_xlabel('Target', fontsize=12)\n    fig.suptitle(title, fontsize=20)\n    plt.show()","c6d3ee5a":"train_binary = train.copy()\n\ncolumns_to_make_binary = ['target'] + main_indicators + main_identities\n\nfor column in columns_to_make_binary:\n    train_binary[column] = train_binary[column].apply(lambda x: 1 if x>= 0.5 else 0)\n    \nmain_col=\"target\"\ncorr_mats=[]\nfor other_col in columns_to_make_binary:\n    if other_col == 'target':\n        continue\n    confusion_matrix = pd.crosstab(train_binary[main_col], train_binary[other_col])\n    corr_mats.append(confusion_matrix)\nout = pd.concat(corr_mats,axis=1,keys=columns_to_make_binary[1:])\n\n#cell highlighting\nout = out.style.highlight_min(axis=0)\nout.columns.names = [None, 'Classification']\nout","6068276e":"### **3.3. IDENTITY, REACTIONS AND ANNOTATORS**\n\nBelow we will average the values of these features for every 10% bin of the target. Let's see if these graphs tell us anything.","863f0bc9":"#### **3.1.2. CORRELATION MATRIX**","d77a537a":"From the graphs above we can clearly see that some features might be useful to predict the toxicity of a comment. Let's dig deeper.\n\n### **3.2. MAIN INDICATORS**\n\nBelow we will do some experiments with which threshold value for the main indicators best capture a toxic comment. For example, we could assume that any value of insult > 0.5 indicates that the comment is toxic. But what if 0.6 is a better threshold?","2aeca985":"### **3. FEATURE ENGINEERING AND PRE-PROCESSING**\n\nSome of the original and engineered features will hardly help us to predict the toxicity bias, not because the data is useless, but because it isn't available on the test dataset. For example, it would be a very difficult task to estimate the date of the comments just by its content. It would also be hard to estimate how many likes a comment would receive. However, in this kernel we will not drop those columns in case someone find a way to use them.\n\n#### **New features and data pre-processing**:\n**Bigger identity groups**:<br>\n- For example, 'atheist', 'buddhist', 'christian', 'hindu', 'jewish', 'muslim', 'other_religion' can all be classified under religion toxicity. Let create new bigger groups: is_religion_related, is_gender_related, is_sexuality_related, is_ethinicity_related, is_mental_disability_related\n\n**Likes ratio**:<br>\n- Maybe the absolute value of disagreement doesn't mean much, but the ratio between disagreement\/likes do. We will calculate those ratios for all the reactions.\n\n**Comments properties**:<br>\n- date_created: We will express this in terms of days past since the earliest comment;<br>\n- has_parent_id: Column indicating if a variable has parent_id, since almost 50% of the data don't have it;<br>\n- rating: rejected and approved substituted for 0 and 1;<br>\n\n**Toxicity related to identity**:<br>\n- is_identity_related: If any of the identity_columns have a value > 0<br>\n- in_main_identity_related: If any of the main (as described in the instructions of this competition) identity_columns have a value. <br>\n- identity_degree: How many identity_columns have a value > 0<br>\n- identity_weight: The sum of values in identity_columns.<br>\n\n**Filling NaN**:\n- All NaN will be filled with 0. It will affect the identity groups, that are classified as NaN if no identity is mentioned in the comment, and the parent_id. All comments without parent_id will belong to the same parent_id = 0. The data as provided doesn't have a parent_id = 0.","50c3ea50":"Insult is not only a good indicator of toxicity, but it is also the most frequent one among the main indicators. The graph shows that using 0.5 as a rule of thumb to classify something as toxic or not seems to be a good strategy, since the ROC-AUC curves have their inflection point around there.","892a8b5b":"*This EDA in being developed. Please, keep coming for updates and feel free to comment feedbacks and suggestions. If it helps you, please, consider upvoting :)*\n\n### **1. INTRODUCTION**\nA year ago, a simmilar competition was hosted by Jigsaw at kaggle with the goal of classifying toxic comments. This year, a new challenge was added to that task: How to remove bias from that classification?\n\nTo illustrate this problem, compare the comments: `You are so gay.` and `I am gay`. Both use the word gay, but most of the people would consider only one of them as toxic, depending on the context.\n\n**Why is this competition so important for the real world and so hard to deal with?** <br>\nThe outcome of our analysis is the type of algorithm that companies will use to define what is free speech and what shouldn't be tolerated in a discussion. This challenge actually starts with how the training dataset was produced: Multiple people (annotators) read thousands of comments and defined if those comments were offensive or not. Where is the trick? They disagreed in many of them. The solution? Average the answer and whatever is above 0.5 is considered offensive. \n\nLet's see a real example from the dataset. Someone said: `Is there any such thing as a 'gay trans woman '?  Technically?  Theoretically, for example, a female, once you become your born sex, anatomically, you will be\/continue to be attracted to women and not necessarily gay women`\n\nNow, is this a toxic comment? Well, it's target value was 0.39, which means it was almost labeled as toxic.\n\nIf not even humans are sure about it, then how can we properly train an algorithm to deal with this kind of situation? I hope you find out and I encourage you in your journey to analyse the data, not only from a statistical point of view, but also from a social one.\n\nIn this kernel we will understand the data, have some ideas about how to approach the problem and how to create new features. ","b9a5d3b9":"**Missing values:** <br>\n- Identity columns will have missing values when a comment didn't mention any identity. We can replace them with 0.<br>\n- Parent_id will probably have missing values when it is an original topic. We can also replace them with 0.<br>\n\n**Unique values**:\n- Some of the comments are repeated. Let's check them later and see if they are spam;<br>\n- There are very few article_ids, probably because the comments were extracted from a limited set of articles;<br>\n- Every comment has an unique ID;<br>\n- Date also constains milliseconds, so it is expected that we will see almost all comment_dates being unique.\n\n","6a3da5b4":"### **3.3.1. AN INTERESTING SURPRISE**\n\n**Identity**: <br>\nThe data shows such an interesting behavior. The peak of the identity average values are usualy when target = 0.5. Here is my hypothesis about it: <br>\n- A lot of comments are not toxic (target = 0) and a few of them have an identity mentioned (identity_degree < 0.3)\n- Very few comments are toxic (target = 1) and a few of them have an identity mentioned (identity_degree < 0.3).\n- But there are some comments, where identity is mentioned more often (identity_degree > 0.5), and it is NOT clear at all if they are toxic or not (target = 0.5). Here is where the challenge seems to be.\n\nWhat is also interesting about identity_degree having a higher value in the target = 0.5 region is that it indicates that situations where there are two identities together, like \"white male\", are more complex to classify as toxic.\n\n**Likes ratio and rating**: <br>\nReactions and like ratio don't seem to be good indicators of a comment being toxic. The comment rating, however, seems to matter. \n\n**Annotators**: <br>\nIt seems that identity annotators are way less frequent than toxicity annotator. However, on toxicity, some comments were evaluated by hundreds of them. It means that some targets are more trustable than others and should have more weight in our model. <br>\n\n### **3.4. FEATURES SUMMARY:** <br>\n\nThose graphs were great to show how big of a challenge this competition offers. It appears to be relatively easy to classify as toxic a comment containing words related to insults, but extremely dificult for humans to say if a comment is toxic solely based on identity.","19aafdc5":"### **3.1. CORRELATIONS**\n\nBelow we will see if there is any strong correlations between our features and target. In sections ahead we will analyse more in deepth the correlations between target and characteristcs of the comments, like comment length and words frequency. For now, we will limit ourselves to the existing features.\n\n#### **3.1.1. HEAT MAP**","7ba5070f":"Lets check the percentage of missing values and unique values in our columns.","395e9256":"#### **2.3. THE TEST DATASET**\n\nThe test and train datasets are very different. In the test dataset we have access only to the comments. It seems that one of the strategies that can be used is to train a model to categorize the data, and then train these categories to predict the toxicity.","3c020b85":"If this kernel gave you any new ideas, please, consider upvoting. Thanks in advance! :)\n\n# TO BE CONTINUED...","f52ce0a8":"#### **3.1.3. TARGET CORRELATION PLOT**","a45ad94c":"### **2.2. EXAMPLES OF COMMENTS**\n\n**WARNING**: Please, consider that this is a competition about finding toxicity and these comments might be offensive to you or someone.","1c25b60b":"### **2. DIFFERENCES BETWEEN TRAIN AND TEST DATASETS**\n\n#### **2.1) THE TRAIN DATASET**\n\nThere are 45 columns for each row. Let's understand them better.\n\n**ID, target and comment_text**: <br>\n'id', 'target', 'comment_text'\n\n**Main indicators of toxicity (6):** <br>\nmain_indicators = ['severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat', 'sexual_explicit']\n\n**Identity columns (24):**<br>\nidentity_columns = ['asian', 'atheist', 'bisexual', 'black', 'buddhist', 'christian',\n                    'female', 'heterosexual', 'hindu', 'homosexual_gay_or_lesbian',\n                    'intellectual_or_learning_disability', 'jewish', 'latino', 'male',\n                    'muslim', 'other_disability', 'other_gender', 'other_race_or_ethnicity',\n                    'other_religion', 'other_sexual_orientation', 'physical_disability',\n                    'psychiatric_or_mental_illness', 'transgender', 'white']<br>\n\n*Notice: The final evaluation will only have data about identities with more than 500 examples in the test + train dataset, which are: ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish', 'muslim', 'white', 'black', 'psychiatric_or_mental_illness']<br>*\n\n**Comment's properties (5)**':<br>\ncomment_properties = ['created_date', 'publication_id', 'parent_id', 'article_id', 'rating']\n\n**Reactions (5)**:<br>\nreactions = ['funny', 'wow', 'sad', 'likes', 'disagree']\n\n**Annotators (2)**:<br>\nannotators = ['identity_annotator_count', 'toxicity_annotator_count']\n\nLet's take a look at the first 10 rows"}}