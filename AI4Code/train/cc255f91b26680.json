{"cell_type":{"526440eb":"code","d3aa4bf0":"code","31ed6269":"code","478b230c":"code","08175616":"code","faffeb6a":"code","93b60f86":"code","61395f31":"code","e6ae9cd2":"code","4b107d95":"code","35a36d99":"code","265df8e7":"code","88286c17":"code","39302124":"code","d368cc9d":"code","b058219c":"code","a414dfff":"code","a7af17ef":"markdown"},"source":{"526440eb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n\nfrom matplotlib import pyplot as plt\nplt.style.use('fivethirtyeight')\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom sklearn.metrics import f1_score, log_loss, precision_score, confusion_matrix, classification_report\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d3aa4bf0":"test_stage_1 = pd.read_csv(\"..\/input\/test_stage_1.tsv\", sep=\"\\t\")","31ed6269":"test_stage_1[0:5]","478b230c":"# assigning the GAP dev data as test data\ntest_df = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/google-research-datasets\/gap-coreference\/master\/gap-development.tsv\", delimiter='\\t')\n# assigning the GAP test data as train data\ntrain_df = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/google-research-datasets\/gap-coreference\/master\/gap-test.tsv\", delimiter='\\t')\nvalid_df = pd.read_csv(\"https:\/\/raw.githubusercontent.com\/google-research-datasets\/gap-coreference\/master\/gap-validation.tsv\", delimiter='\\t')","08175616":"# using the full set of training and validation data\ntrain_df = pd.concat([train_df,valid_df])","faffeb6a":"train_df.head()","93b60f86":"def scrape_url(url):\n    '''\n    get the title of the wikipedia page and replace \"_\" with white space\n    '''\n    return url[29:].lower().replace(\"_\",\" \")\n\ndef check_name_in_string(name,string):\n    '''\n    check whether the name string is a substring of another string (i.e. wikipedia title)\n    '''\n\n    return name.lower() in string\n\n\n\ndef predict_coref(df):\n    pred =[]\n    for index, row in df.iterrows():\n        wiki_title = scrape_url(row[\"URL\"])\n        if (check_name_in_string(row[\"A\"],wiki_title)):\n            pred.append(\"A\")\n        else:\n            if (check_name_in_string(row[\"B\"],wiki_title)):\n                pred.append(\"B\")\n            else:\n                pred.append(\"NEITHER\")\n    return pred\n\ntrain_pred = predict_coref(train_df)\ntest_pred = predict_coref(test_df)","61395f31":"train_len = len(train_df)\nA_prior = len(train_df[train_df[\"A-coref\"] == True])\/train_len\nB_prior = len(train_df[train_df[\"B-coref\"] == True])\/train_len\nNeither_prior = len(train_df[(train_df[\"A-coref\"] == False) & (train_df[\"B-coref\"] == False)])\/train_len\n\nprint(\"A prior: \"+str(A_prior))\n\nprint(\"B prior: \"+str(B_prior))\n\nprint(\"NEITHER prior: \"+str(Neither_prior))","e6ae9cd2":"gold_train = []\nfor index, row in train_df.iterrows():\n    if (row[\"A-coref\"]):\n        gold_train.append(\"A\") \n    else:\n        if (row[\"B-coref\"]):\n            gold_train.append(\"B\") \n        else:\n            gold_train.append(\"NEITHER\")\n            \ngold_test = []\nfor index, row in test_df.iterrows():\n    if (row[\"A-coref\"]):\n        gold_test.append(\"A\") \n    else:\n        if (row[\"B-coref\"]):\n            gold_test.append(\"B\") \n        else:\n            gold_test.append(\"NEITHER\")\n","4b107d95":"\nprint(f1_score( gold_train, train_pred, average='micro'))\nprint(classification_report( gold_train, train_pred))\nprint(confusion_matrix(gold_train, train_pred))","35a36d99":"def prec_prob(gold, pred, test):\n    '''\n    Using the training set to determine the precision by class\n    and assigning it to the test data set\n    '''\n    scores = []\n    precision = precision_score(gold, pred,  average=None,\n                                labels=['A','B','NEITHER'])\n    A_prec = precision[0]\n    B_prec = precision[1]\n    Neither_prec = precision[2]\n    for ante in test:\n        if (ante == 'A'):\n            scores.append([A_prec, B_prec*B_prior, Neither_prec*Neither_prior])\n        else:\n            if (ante =='B'):\n                scores.append([A_prec*A_prior, B_prec, Neither_prec*Neither_prior])\n            else:\n                scores.append([A_prec*A_prior,B_prec*B_prior,Neither_prec])\n    return scores","265df8e7":"\nscores_train = prec_prob(gold_train, train_pred, train_pred)\nlog_loss(gold_train,scores_train)","88286c17":"\nscores_test = prec_prob(gold_train, train_pred, test_pred)\nlog_loss(gold_test,scores_test)","39302124":"sample_submission = pd.read_csv(\"..\/input\/sample_submission_stage_1.csv\")","d368cc9d":"sample_submission[['A','B','NEITHER']] = scores_test","b058219c":"sample_submission.head()","a414dfff":"sample_submission.to_csv(\"submission.csv\", index=False)","a7af17ef":"## Wiki title baseline\n\nThis is a simple baseline only using the wiki title supplied from the URL in the data set.\nSimilar to the best baseline described in the article describing the corpus, pronouns refer to the entity that is a substring of the title (e.g, ```\"Dehner\" in \"Jeremy Dehner\"```. This is known as a strong coreference baseline, i.e., taking the main protoganist in the entire article. This seems to work to a certain extend here as well, but note that the creators of the corpus created a balanced selection of what they call Page Entity:\n> Page Entity. Pronouns in a Wikipedia page\n> often refer to the entity the page is about. We\n> include such examples in our dataset but balance them 1:1 against examples that do not\n> include mentions of the page entity.\n\nRunning this baseline on the training data (=```gap_test```) shows a precision for A ab B of about 0.78 (but lower recall).\n\nThe baseline also takes into account the prior and the precision values for A, B, and NEITHER. The probability for a value is set to the precision vallue calculated on the training data and the probabilities for the other two values are set to the precision * prior of the respective value.\n\nhttps:\/\/arxiv.org\/abs\/1810.05201\n\nBaldridge, J., Webster, K., Recasens, M., and Axelrod, V. Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns. 2018.\nTransactions of the Association of Computational Linguistics\n\n\n\n"}}