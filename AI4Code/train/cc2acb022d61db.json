{"cell_type":{"7616eed5":"code","248bf614":"code","50416bc4":"code","c0bfddc2":"code","48148e19":"code","75389015":"code","a4014728":"code","960cc267":"code","91214866":"code","3903ceec":"code","02a1155d":"code","a4a09db2":"code","063ba253":"code","6442c568":"code","d3548779":"code","335bfed4":"code","81ffc751":"code","a33389b4":"code","98948418":"code","e5708204":"markdown","6d5c7455":"markdown","958fe9f8":"markdown","8412a3e9":"markdown","65b570ab":"markdown","f0f07a54":"markdown","4d30cc3b":"markdown","d26e6550":"markdown","045ee0db":"markdown","15127815":"markdown"},"source":{"7616eed5":"# Global variables for testing changes to this notebook quickly\nRANDOM_SEED = 0\nNUM_FOLDS = 7\n\n# Don't change these\nNUM_CORES = 8\nNUM_CLASSES = 10\nINPUT_SIZE = 28,28,1","248bf614":"# General Imports\nimport numpy as np\nimport pandas as pd\nimport time\nimport os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n\n# Logging\/Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Tensorflow\/Keras\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Evaluation\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Plotting\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Load data\nsubmission = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv')\ntrain = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n\nfeatures = [x for x in train.columns if x != 'label']\n\nprint(\"Train Shape:\", train.shape)\nprint(\"Test Shape:\", test.shape)","50416bc4":"# Model 1 Parameters\nBATCH_SIZE = 32 * NUM_CORES\nEPOCHS = 50\nEARLY_STOP = 3\nVERBOSE = 0","c0bfddc2":"def train_islr_model():\n    \n    # Store the holdout predictions\n    oof_preds = np.zeros((train.shape[0],))\n    test_preds = np.zeros((test.shape[0], 10))\n    scores = np.zeros(NUM_FOLDS)\n    \n    # Stratified k-fold cross-validation\n    skf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['label'])):\n        \n        # Training and Validation Sets\n        X_train, y_train = train[features].iloc[train_idx], train['label'].iloc[train_idx]\n        X_valid, y_valid = train[features].iloc[valid_idx], train['label'].iloc[valid_idx]\n        \n        # Scale pixel values to [0,1]\n        scaler = MinMaxScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_valid = scaler.transform(X_valid)\n        X_test = scaler.transform(test)\n        \n        # Convert training examples to shape (28,28,1)\n        X_train = np.reshape(X_train, (X_train.shape[0], *INPUT_SIZE))\n        X_valid = np.reshape(X_valid, (X_valid.shape[0], *INPUT_SIZE))\n        X_test = np.reshape(X_test, (X_test.shape[0], *INPUT_SIZE))\n        \n        # Convert target vectors for keras input\n        Y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\n        Y_valid = keras.utils.to_categorical(y_valid, NUM_CLASSES)\n        \n        # Define Model\n        model = keras.Sequential(\n            [\n                keras.Input(shape = INPUT_SIZE),\n                layers.Flatten(),\n                layers.Dense(256, activation=\"relu\"),\n                layers.Dropout(0.4),\n                layers.Dense(128, activation=\"relu\"),\n                layers.Dropout(0.3),\n                layers.Dense(NUM_CLASSES, activation=\"softmax\"),\n            ]\n        )\n\n        model.compile(\n            loss = tf.keras.losses.CategoricalCrossentropy(), \n            optimizer = tf.keras.optimizers.RMSprop(), \n            metrics=[tf.keras.metrics.CategoricalAccuracy()],\n        )\n            \n        start = time.time()\n\n        model.fit(\n            X_train, Y_train, \n            validation_data = (X_valid, Y_valid),\n            batch_size = BATCH_SIZE,\n            epochs = EPOCHS,\n            callbacks = [\n                EarlyStopping(\n                    monitor = \"val_categorical_accuracy\",\n                    patience = EARLY_STOP, \n                    restore_best_weights = True)\n            ],\n            verbose = VERBOSE,\n        )\n        \n        end = time.time()\n        \n        # predict class probabilities\n        test_preds += model.predict(X_test)\n        valid_prob = model.predict(X_valid)\n        valid_preds = valid_prob.argmax(axis=-1)\n        scores[fold] = accuracy_score(y_valid, valid_preds)\n        oof_preds[valid_idx] = valid_preds\n        \n        time.sleep(0.5)\n        \n        print(f'Fold {fold}: {round(scores[fold], 6)} (Accuracy) in {round(end-start, 2)}s.')\n    \n    return scores.mean(), test_preds.argmax(axis=-1), oof_preds","48148e19":"islr_scores, islr_preds, islr_oof = train_islr_model()\n\nsubmission['Label'] = islr_preds\nsubmission.to_csv('islr_submission.csv', index=False)","75389015":"# Model 2 Parameters\nBATCH_SIZE = 32 * NUM_CORES\nEPOCHS = 50\nEARLY_STOP = 3\nVERBOSE = 0","a4014728":"def train_ebook_model():\n    \n    # Store the holdout predictions\n    oof_preds = np.zeros((train.shape[0],))\n    test_preds = np.zeros((test.shape[0], 10))\n    scores = np.zeros(NUM_FOLDS)\n    \n    # Stratified k-fold cross-validation\n    skf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['label'])):\n        \n        # Training and Validation Sets\n        X_train, y_train = train[features].iloc[train_idx], train['label'].iloc[train_idx]\n        X_valid, y_valid = train[features].iloc[valid_idx], train['label'].iloc[valid_idx]\n        \n        # Scale pixel values to [0,1]\n        scaler = MinMaxScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_valid = scaler.transform(X_valid)\n        X_test = scaler.transform(test)\n        \n        # Convert training examples to shape (28,28,1)\n        X_train = np.reshape(X_train, (X_train.shape[0], *INPUT_SIZE))\n        X_valid = np.reshape(X_valid, (X_valid.shape[0], *INPUT_SIZE))\n        X_test = np.reshape(X_test, (X_test.shape[0], *INPUT_SIZE))\n        \n        # Convert target vectors for keras input\n        Y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\n        Y_valid = keras.utils.to_categorical(y_valid, NUM_CLASSES)\n        \n        # Define Model\n        model = keras.Sequential(\n            [\n                keras.Input(shape = INPUT_SIZE),\n                layers.Flatten(),\n                layers.Dense(512, activation=\"relu\"),\n                layers.Dropout(0.2),\n                layers.Dense(NUM_CLASSES, activation=\"softmax\"),\n            ]\n        )\n\n        model.compile(\n            loss = tf.keras.losses.CategoricalCrossentropy(), \n            optimizer = tf.keras.optimizers.Adam(), \n            metrics=[tf.keras.metrics.CategoricalAccuracy()],\n        )\n            \n        start = time.time()\n\n        model.fit(\n            X_train, Y_train, \n            validation_data = (X_valid, Y_valid),\n            batch_size = BATCH_SIZE,\n            epochs = EPOCHS,\n            callbacks = [\n                EarlyStopping(\n                    monitor = \"val_categorical_accuracy\",\n                    patience = EARLY_STOP, \n                    restore_best_weights = True)\n            ],\n            verbose = VERBOSE,\n        )\n        \n        end = time.time()\n        \n        # predict class probabilities\n        test_preds += model.predict(X_test)\n        valid_prob = model.predict(X_valid)\n        valid_preds = valid_prob.argmax(axis=-1)\n        scores[fold] = accuracy_score(y_valid, valid_preds)\n        oof_preds[valid_idx] = valid_preds\n        \n        \n        time.sleep(0.5)\n        \n        print(f'Fold {fold}: {round(scores[fold], 6)} (Accuracy) in {round(end-start, 2)}s.')\n    \n    return scores.mean(), test_preds.argmax(axis=-1), oof_preds","960cc267":"ebook_scores, ebook_preds, ebook_oof = train_ebook_model()\n\nsubmission['Label'] = ebook_preds\nsubmission.to_csv('ebook_nn_submission.csv', index=False)","91214866":"# Model 3 Parameters\nBATCH_SIZE = 32 * NUM_CORES\nEPOCHS = 200\nEARLY_STOP = 10\nVERBOSE = 0","3903ceec":"def train_ebook_cnn():\n    \n    # Store the holdout predictions\n    oof_preds = np.zeros((train.shape[0],))\n    test_preds = np.zeros((test.shape[0], 10))\n    scores = np.zeros(NUM_FOLDS)\n    \n    # Stratified k-fold cross-validation\n    skf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['label'])):\n        \n        # Training and Validation Sets\n        X_train, y_train = train[features].iloc[train_idx], train['label'].iloc[train_idx]\n        X_valid, y_valid = train[features].iloc[valid_idx], train['label'].iloc[valid_idx]\n        \n        # Scale pixel values to [0,1]\n        scaler = MinMaxScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_valid = scaler.transform(X_valid)\n        X_test = scaler.transform(test)\n        \n        # Convert training examples to shape (28,28,1)\n        X_train = np.reshape(X_train, (X_train.shape[0], *INPUT_SIZE))\n        X_valid = np.reshape(X_valid, (X_valid.shape[0], *INPUT_SIZE))\n        X_test = np.reshape(X_test, (X_test.shape[0], *INPUT_SIZE))\n        \n        # Convert target vectors for keras input\n        Y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\n        Y_valid = keras.utils.to_categorical(y_valid, NUM_CLASSES)\n        \n        # Define Model\n        model = keras.Sequential(\n            [\n                keras.Input(shape = INPUT_SIZE),\n                layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n                layers.MaxPooling2D(pool_size=(2, 2)),\n                layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n                layers.MaxPooling2D(pool_size=(2, 2)),\n                layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n                layers.MaxPooling2D(pool_size=(2, 2)),\n                layers.Flatten(),\n                layers.Dense(64, activation=\"relu\"),\n                layers.Dense(NUM_CLASSES, activation=\"softmax\"),\n            ]\n        )\n\n        model.compile(\n            loss = tf.keras.losses.CategoricalCrossentropy(), \n            optimizer = tf.keras.optimizers.Adam(), \n            metrics=[tf.keras.metrics.CategoricalAccuracy()],\n        )\n\n        start = time.time()\n        \n        model.fit(\n            X_train, Y_train, \n            validation_data = (X_valid, Y_valid),\n            batch_size = BATCH_SIZE,\n            epochs = EPOCHS,\n            callbacks = [\n                EarlyStopping(\n                    monitor = \"val_categorical_accuracy\",\n                    patience = EARLY_STOP, \n                    restore_best_weights = True)\n            ],\n            verbose = VERBOSE,\n        )\n        \n        end = time.time()\n        \n        # predict class probabilities\n        test_preds += model.predict(X_test)\n        valid_prob = model.predict(X_valid)\n        valid_preds = valid_prob.argmax(axis=-1)\n        scores[fold] = accuracy_score(y_valid, valid_preds)\n        oof_preds[valid_idx] = valid_preds\n        \n        \n        time.sleep(0.5)\n        \n        print(f'Fold {fold}: {round(scores[fold], 6)} (Accuracy) in {round(end-start, 2)}s.')\n    \n    return scores.mean(), test_preds.argmax(axis=-1), oof_preds","02a1155d":"ebook_cnn_scores, ebook_cnn_preds, ebook_cnn_oof = train_ebook_cnn()\n\nsubmission['Label'] = ebook_cnn_preds\nsubmission.to_csv('ebook_cnn_submission.csv', index=False)","a4a09db2":"# Model 4 Parameters\nBATCH_SIZE = 128 * NUM_CORES\nEPOCHS = 200\nEARLY_STOP = 10\nVERBOSE = 0","063ba253":"def train_keras_cnn():\n    \n    # Store the holdout predictions\n    oof_preds = np.zeros((train.shape[0],))\n    test_preds = np.zeros((test.shape[0], 10))\n    scores = np.zeros(NUM_FOLDS)\n    \n    # Stratified k-fold cross-validation\n    skf = StratifiedKFold(n_splits = NUM_FOLDS, shuffle = True, random_state = RANDOM_SEED)\n    for fold, (train_idx, valid_idx) in enumerate(skf.split(train, train['label'])):\n        \n        # Training and Validation Sets\n        X_train, y_train = train[features].iloc[train_idx], train['label'].iloc[train_idx]\n        X_valid, y_valid = train[features].iloc[valid_idx], train['label'].iloc[valid_idx]\n        \n        # Scale pixel values to [0,1]\n        scaler = MinMaxScaler()\n        X_train = scaler.fit_transform(X_train)\n        X_valid = scaler.transform(X_valid)\n        X_test = scaler.transform(test)\n        \n        # Convert training examples to shape (28,28,1)\n        X_train = np.reshape(X_train, (X_train.shape[0], *INPUT_SIZE))\n        X_valid = np.reshape(X_valid, (X_valid.shape[0], *INPUT_SIZE))\n        X_test = np.reshape(X_test, (X_test.shape[0], *INPUT_SIZE))\n        \n        # Convert target vectors for keras input\n        Y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\n        Y_valid = keras.utils.to_categorical(y_valid, NUM_CLASSES)\n        \n        # Define Model\n        model = keras.Sequential(\n            [\n                keras.Input(shape = INPUT_SIZE),\n                layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n                layers.MaxPooling2D(pool_size=(2, 2)),\n                layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n                layers.MaxPooling2D(pool_size=(2, 2)),\n                layers.Flatten(),\n                layers.Dropout(0.5),\n                layers.Dense(NUM_CLASSES, activation=\"softmax\"),\n            ]\n        )\n\n        model.compile(\n            loss = tf.keras.losses.CategoricalCrossentropy(), \n            optimizer = tf.keras.optimizers.Adam(), \n            metrics=[tf.keras.metrics.CategoricalAccuracy()],\n        )\n\n        \n        start = time.time()\n        \n        model.fit(\n            X_train, Y_train, \n            validation_data = (X_valid, Y_valid),\n            batch_size = BATCH_SIZE,\n            epochs = EPOCHS,\n            callbacks = [\n                EarlyStopping(\n                    monitor = \"val_categorical_accuracy\",\n                    patience = EARLY_STOP, \n                    restore_best_weights = True)\n            ],\n            verbose = VERBOSE,\n        )\n        \n        end = time.time()\n        \n        # predict class probabilities\n        test_preds += model.predict(X_test)\n        valid_prob = model.predict(X_valid)\n        valid_preds = valid_prob.argmax(axis=-1)\n        scores[fold] = accuracy_score(y_valid, valid_preds)\n        oof_preds[valid_idx] = valid_preds\n        \n        print(f'Fold {fold}: {round(scores[fold], 6)} (Accuracy) in {round(end-start, 2)}s.')\n    \n    return scores.mean(), test_preds.argmax(axis=-1), oof_preds","6442c568":"keras_scores, keras_preds, keras_oof = train_keras_cnn()\n\nsubmission['Label'] = keras_preds\nsubmission.to_csv('keras_doc_submission.csv', index=False)","d3548779":"# Helper function to plot 6 misclassified examples\ndef plot_misclassified(preds):\n    train['guess'] = preds\n    samples = train[train['label'] != preds].sample(n = 6, random_state = RANDOM_SEED)\n    y_true = samples['label'].to_numpy()\n    y_pred = samples['guess'].to_numpy()\n    samples = samples[features].to_numpy()\n    fig, ax = plt.subplots(2, 3, sharex=True, sharey=True, figsize=(12, 9))\n    for row in range(2):\n        for col in range(3):\n            idx = 3*row + col\n            ax[row,col].imshow(samples[idx].reshape(28, 28), cmap=plt.get_cmap('gray'))\n            ax[row,col].set_title(f'Actual - {y_true[idx]}\\n Predicted - {int(y_pred[idx])}')","335bfed4":"plot_misclassified(islr_oof)","81ffc751":"plot_misclassified(ebook_oof)","a33389b4":"plot_misclassified(ebook_cnn_oof)","98948418":"plot_misclassified(keras_oof)","e5708204":"# Model 2: 4-Layer Neural Network (AMD)\n\nThe second model we consider has the following structure:\n\n* A Flatten layer to reshape the input\n* A Dense layer with 512 units and a ReLu activation function\n* A Dropout layer with a dropout rate of 0.5\n* A Dense layer with 10 units an a softmax activation","6d5c7455":"# Model 4: Convolutional Neural Network with Dropout\n\nThe final model we consider is the convolutional neural network from the Keras documentation. This model utilizes two convolution layers and dropout regularization.","958fe9f8":"## Model 3: Convolutional Neural Network","8412a3e9":"## Model 1: Simple Neural Network","65b570ab":"# Misclassification \u932f\u8aa4\u5206\u985e\n\nWe take a look at a few misclassified training examples using the out-of-fold predictions to get an idea of the examples which our neural networks failed to classify correctly.","f0f07a54":"## Model 2: 4-Layer Neural Network","4d30cc3b":"# Model 1: Simple Neural Network (ISL)\n\nThe first model we consider has the following structure:\n\n* A Flatten Layer to reshape our input\n* Dense Layer with 256 units and ReLu activation function\n* Dropout Layer with 0.4 dropout rate\n* Dense Layer with 128 unit and ReLu activation function\n* Dropout Layer with 0.3 dropout rate\n* Dense Layer with 10 units and softmax activation","d26e6550":"# Simple Keras Models\n\nIn this notebook, we adapt example code from various sources for use with this competition. In particular, we look at the following examples:\n\n* A 5-layer NN from the book [Introduction to Statistical Learning (2nd Edition)](https:\/\/www.statlearning.com\/).\n* A 4-layer NN from the ebook [Artificial Intelligence, Machine Learning and Deep Learning](http:\/\/www.merclearning.com\/titles\/Artificial%20Intelligence-Machine-Learning-and-Deep-Learning.html).\n* A CNN from the same ebook.\n* The CNN from the [Keras documentation](https:\/\/keras.io\/examples\/vision\/mnist_convnet\/). \n\n\nWe make the following adjustments from the code given at the original sources, namely:\n\n* Use pandas and scikit-learn for loading\/processing the data.\n* Enabling GPU to speedup training.\n* Vary the number of epochs and enable [early stopping](https:\/\/keras.io\/api\/callbacks\/early_stopping\/).\n* Use cross-validation to ensemble test predictions and average validation accuracy.","045ee0db":"# Model 3: Convolutional Neural Network\n\nThe third model we consider is a convolutional neural network with 3 convolution layers (each followed by max pooling) and two dense layers.","15127815":"## Model 4: Convolutional Neural Network with Dropout"}}