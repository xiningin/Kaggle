{"cell_type":{"ede9022f":"code","2dfdce81":"code","c9c2fc26":"code","2ffb4661":"code","cfe28d9f":"code","19fd8d01":"code","52d19e89":"code","0c05217f":"code","060761b8":"code","b58a1dce":"code","44f3cb18":"code","ec55dc5b":"code","2e653f40":"code","049d36d1":"code","89906d9c":"code","b884377d":"code","2ee6a5f9":"code","b48b79bc":"code","a811407b":"code","c2fcde23":"code","ef6baf72":"code","21c6e720":"code","44258cda":"code","8a50074f":"code","52d90b36":"code","60604bdf":"code","22fcf3f7":"code","fd1857f1":"code","a01325ec":"code","a040c7ba":"code","d9b329b4":"markdown","eb19a9a3":"markdown","27edf215":"markdown","a417ee6e":"markdown","233c8bf3":"markdown","452147fd":"markdown","50c089b7":"markdown","c4b26226":"markdown","c7c95d72":"markdown","cc73b986":"markdown","d03c55b8":"markdown","53cdfe21":"markdown","0ee69df1":"markdown","ef85cb6f":"markdown","d5a074f7":"markdown","ef3ec93d":"markdown","9fca709d":"markdown","a5eec684":"markdown","497d48f6":"markdown","984c51e5":"markdown"},"source":{"ede9022f":"import pandas as pd\nimport numpy as np\n\nimport os\nimport random\nimport re\nimport math\nimport time\n\n# modules forxgboost modeling\nimport xgboost as xgb\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score, cross_validate\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\n# ignore warnings for clearner ouputs\nimport warnings\nwarnings.filterwarnings('ignore')","2dfdce81":"seed = 42\nrandom.seed(42)\nnp.random.seed(42)","c9c2fc26":"# setting path, change path here for other melanoma dataset\n\nbase_path = '\/kaggle\/input\/siim-isic-melanoma-classification'\ntrain_img_path = '\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/train\/'\ntest_img_path = '\/kaggle\/input\/siim-isic-melanoma-classification\/jpeg\/test\/'\nimg_stats_path = '\/kaggle\/input\/melanoma2020imgtabular'","2ffb4661":"train = pd.read_csv(os.path.join(base_path, 'train.csv'))\ntest = pd.read_csv(os.path.join(base_path, 'test.csv'))\nsample = pd.read_csv(os.path.join(base_path, 'sample_submission.csv'))","cfe28d9f":"f'train features: {train.columns.tolist()}', f'test_featuers: {test.columns.tolist()};'","19fd8d01":"# do some simple renaming for complecatec feature name\n\ntrain.columns = [\n    'img_name', 'id', 'sex', 'age', 'location', 'diagnosis',\n    'benign_malignant', 'target'\n]\ntest.columns = ['img_name', 'id', 'sex', 'age', 'location']","52d19e89":"train.head()","0c05217f":"test.tail()","060761b8":"# now I'm using my own code for simplicity\ndef fill_missing(df):\n    '''The strategy here is:\n    1. For cat features we fill 'unknown' if catgories are many else mode\n    2. For con features we fill the median\n    '''\n    df = df.copy()\n    # fill nan values\n    df.location.fillna(value='unknown', inplace=True)\n    df.sex.fillna(value=train.sex.mode()[0], inplace=True)\n    df.age.fillna(value=df.age.median(), inplace=True)\n\n    return df","b58a1dce":"train = fill_missing(train)\ntest = fill_missing(test)","44f3cb18":"train.isnull().any(), test.isnull().any()","ec55dc5b":"# Loading lanscape data\n\ntrain40 = pd.read_csv('..\/input\/melanoma2020imgtabular\/train40Features.csv')\ntest40 = pd.read_csv('..\/input\/melanoma2020imgtabular\/test40Features.csv')\n\ntrainmet = pd.read_csv('..\/input\/melanoma2020imgtabular\/trainMetrics.csv')\ntestmet = pd.read_csv('..\/input\/melanoma2020imgtabular\/testMetrics.csv')","2e653f40":"# drop duplicate data from landscape dataset\ntrain40.drop(['sex', 'age_approx', 'anatom_site_general_challenge'],\n             axis=1,\n             inplace=True)\ntest40.drop(['sex', 'age_approx', 'anatom_site_general_challenge'],\n            axis=1,\n            inplace=True)\n\n# merging both datasets\ntrain = pd.concat([train, train40, trainmet], axis=1)\ntest = pd.concat([test, test40, testmet], axis=1)","049d36d1":"train.head()","89906d9c":"# def label_encoding(df):\n#     df = df.copy()\n#     # encode labels\n#     location = LabelEncoder()\n#     sex = LabelEncoder()\n#     location_data = location.fit_transform(df.location)\n#     sex_data = sex.fit_transform(df.sex)\n#     df.location = location_data\n#     df.sex = sex_data\n    \n#     return df\n\n# train = label_encoding(train)\n# test = label_encoding(test)","b884377d":"def dummy_encoding(df):\n    df = df.copy()\n    \n    # dummy encoding for label sex and location\n    sex_dummies = pd.get_dummies(df.sex, prefix='sex')\n    location_dummies = pd.get_dummies(df.location, prefix='location')\n    df = pd.concat([df, sex_dummies], axis=1)\n    df = pd.concat([df, location_dummies], axis=1)\n    \n    return df","2ee6a5f9":"train = dummy_encoding(train)\ntest = dummy_encoding(test)","b48b79bc":"train.head()","a811407b":"train.drop(['sex','img_name','id','diagnosis','benign_malignant', 'location'], axis=1, inplace=True)\ntest.drop(['sex','img_name','id', 'location'], axis=1, inplace=True)","c2fcde23":"train.head()","ef6baf72":"# input and output for modelling\n\nX = train.drop('target', axis=1)\ny = train.target","21c6e720":"# 5 stratified KFold with holdout validating\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.2,\n                                                    stratify=y,\n                                                    random_state=seed)\ncv = StratifiedKFold(5, shuffle=True, random_state=seed)","44258cda":"X_train.head()","8a50074f":"# drop features for overfitting\n# X_train.drop(['n_images', 'image_size','width','height','total_pixels','reds','blues','greens','mean_colors', 'age_min', 'age_max'], axis=1, inplace=True)\n# test.drop(['n_images', 'image_size','width','height','total_pixels','reds','blues','greens','mean_colors', 'age_min', 'age_max'], axis=1, inplace=True)","52d90b36":"xg = xgb.XGBClassifier(\n    n_estimators=750,\n    min_child_weight=0.81,\n    learning_rate=0.025,\n    max_depth=2,\n    subsample=0.80,\n    colsample_bytree=0.42,\n    gamma=0.10,\n    random_state=42,\n    n_jobs=-1,\n)","60604bdf":"estimators = [xg]","22fcf3f7":"def model_check(X_train, y_train, estimators, cv):\n    model_table = pd.DataFrame()\n    row_index = 0\n    \n    for est in estimators:\n        MLA_name = est.__class__.__name__\n        model_table.loc[row_index, 'Model Name'] = MLA_name\n        \n        cv_results = cross_validate(est,\n                                    X_train,\n                                    y_train,\n                                    cv=cv,\n                                    scoring='roc_auc',\n                                    return_train_score=True,\n                                    n_jobs=-1)\n        \n        model_table.loc[row_index,\n                        'Train roc Mean'] = cv_results['train_score'].mean()\n        model_table.loc[row_index,\n                        'Test roc Mean'] = cv_results['test_score'].mean()\n        model_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        model_table.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n        \n        row_index += 1\n    \n    model_table.sort_values(by=['Test roc Mean'],\n                           ascending=False,\n                           inplace=True)\n    \n    return model_table","fd1857f1":"raw_models = model_check(X_train, y_train, [xg], cv)\ndisplay(raw_models)","a01325ec":"# xgboost predict\n\nxg.fit(X_train, y_train)\npredictions = xg.predict_proba(test)[:, 1]","a040c7ba":"# create submission file with two meta features required\n\nmeta_df = pd.DataFrame(columns=['image_name', 'target'])\nmeta_df['image_name'] = sample['image_name']\nmeta_df['target'] = predictions\nmeta_df.to_csv('xgboost_meta_simplified.csv', header=True, index=False)","d9b329b4":"## process well-preprocessed tabular data for modelling","eb19a9a3":"Please refer to the original notebook if you want more data analysis and visualization.","27edf215":"## first things we need to do...","a417ee6e":"## Step4: feature cleaning - drop uneccesary features which are not helpful after all feature engineering","233c8bf3":"same as the original code ","452147fd":"The reason to write this is that I've already tuned my deep learning models for both tensorflow(wihout tabular data) and pytorch(with tabualr data). When I want more essence from tabular data I see this greate notebook from Ertu\u011frul Demir (Thanks again!). So I extract the part of XGBoost from the notebook and do some refactor for fine tuning on my local machine (works faster than kaggle if you have a good CPU). I wish this could also be helpful and bring some convenience for others.","50c089b7":"# data preprocessing","c4b26226":"Great code also from the original notebook, thanks!","c7c95d72":"Thanks to this great dataset by Marcelo Kittlein [here](https:\/\/www.kaggle.com\/kittlein\/landscape)","cc73b986":"## Step2: feature engineering - concat landscape attributes from images","d03c55b8":"## cross validation scheme","53cdfe21":"## Step3: label encoding","0ee69df1":"## set up model hyperparameters for fine-tuning","ef85cb6f":"## make predictions","d5a074f7":"## load data and get an impression","ef3ec93d":"Here I only use the simplified meta prediction from the original notebook as I already know the problem of overfitting and the need of more randomness. Feeling really not neccessary to the analyze and do the testing again.\n\nAlso I found the dropping process is no longer needed here as the dataset of landscape has changed  so the train\/test split is no longer that obvious for our model.","9fca709d":"looks like we've got what we want","a5eec684":"# XGBoost modelling","497d48f6":"## Step1: fill missing value","984c51e5":"This notebook is innovated from [Analysis of Melanoma Metadata and EffNet Ensemble\n](https:\/\/www.kaggle.com\/datafan07\/analysis-of-melanoma-metadata-and-effnet-ensemble\/data?), greate work and thanks!"}}