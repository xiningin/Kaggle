{"cell_type":{"94b98fd8":"code","aadc6303":"code","0e1719a7":"code","eb3b6126":"code","4617c57b":"code","1db8edaf":"code","8df5fc6a":"code","111f6850":"code","f325d0f4":"code","5916fb88":"code","81f8ce12":"code","2e078d50":"code","0fa040ab":"code","d548a51d":"code","8f9340b1":"code","ee73f281":"code","c9f49d02":"code","731732be":"code","c1ae8222":"code","476fc7bf":"code","806bf242":"code","bbc394b8":"code","1f758c76":"code","7a1f733b":"code","8daacf5b":"code","08c230be":"code","fb7d76a3":"markdown","70d968a4":"markdown","52d02f74":"markdown","7f1c476d":"markdown","39a82b29":"markdown","4a93e292":"markdown","3a0abb13":"markdown","c0867cb4":"markdown","fe94be4f":"markdown","33ac1833":"markdown","d1615269":"markdown","a4494a17":"markdown","92403c6d":"markdown","8ec070e3":"markdown","2f110ac2":"markdown","1d468030":"markdown","e9cfa8dc":"markdown","0e391971":"markdown","066ed081":"markdown","29f18f19":"markdown","69072ba9":"markdown","993d13a8":"markdown","ddf43a1a":"markdown","00f578b5":"markdown"},"source":{"94b98fd8":"# imports + loading dataset\n\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix\nfrom sklearn import neighbors\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nimport time\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nimport optuna\nfrom optuna.samplers import TPESampler\n\n\ntrain = pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/health-insurance-cross-sell-prediction\/test.csv')\n\nprint(train.shape)\nprint(test.shape)\n\ntrain.head()\ntest.head()","aadc6303":"train = train.drop(['id'], axis=1)\ntest = test.drop(['id'], axis=1)","0e1719a7":"train.info()","eb3b6126":"train.describe()","4617c57b":"train['Response'].value_counts()\/len(train)","1db8edaf":"from sklearn.preprocessing import OrdinalEncoder\n\nC = (train.dtypes == 'object')\nCategoricalVariables = list(C[C].index)\n\nprint(CategoricalVariables)","8df5fc6a":"enc = OrdinalEncoder()\ntrain[[\"Gender\",\"Vehicle_Damage\"]] = enc.fit_transform(train[[\"Gender\",\"Vehicle_Damage\"]])\ntest[[\"Gender\",\"Vehicle_Damage\"]] = enc.fit_transform(test[[\"Gender\",\"Vehicle_Damage\"]])\n\ntrain.loc[train['Vehicle_Age'] == '> 2 Years', 'Vehicle_Age'] = 2\ntrain.loc[train['Vehicle_Age'] == '1-2 Year', 'Vehicle_Age'] = 1\ntrain.loc[train['Vehicle_Age'] == '< 1 Year', 'Vehicle_Age'] = 0\ntest.loc[test['Vehicle_Age'] == '> 2 Years', 'Vehicle_Age'] = 2\ntest.loc[test['Vehicle_Age'] == '1-2 Year', 'Vehicle_Age'] = 1\ntest.loc[test['Vehicle_Age'] == '< 1 Year', 'Vehicle_Age'] = 0\n\ntrain['Vehicle_Age']=train['Vehicle_Age'].astype(float)\ntest['Vehicle_Age']=test['Vehicle_Age'].astype(float)\n\n\ntest.head()","111f6850":"# draw boxplots to visualize outliers\nhas_outliers = ['Age', 'Annual_Premium', 'Vintage']\n\nplt.figure(figsize=(10,7.5))\n\nfor i,col in enumerate(has_outliers):\n    plt.subplot(1, 3, i+1)\n    fig = train.boxplot(column=col)\n    fig.set_title('')\n    fig.set_ylabel(col)","f325d0f4":"has_outliers.remove('Age')\nhas_outliers.remove('Vintage')\nup_outliers = []\nlow_outliers = []\n\ndef max_value(df, variable, top):\n    return np.where(df[variable]>top, top, df[variable])\n\n\nfor col in has_outliers:\n    IQR = train[col].quantile(0.75) - train[col].quantile(0.25)\n    Lower_fence = train[col].quantile(0.25) - (IQR * 3)\n    Upper_fence = train[col].quantile(0.75) + (IQR * 3)\n    low_outliers.append(Lower_fence)\n    up_outliers.append(Upper_fence)\n    print(f'{col} outliers are values < {Lower_fence} or > {Upper_fence}')\n\nfor col, outlier in zip(has_outliers, up_outliers):\n    train[col] = max_value(train, col, outlier)\n\n# Tamb\u00e9 per el Test set\n\nfor col in has_outliers:\n    IQR = test[col].quantile(0.75) - test[col].quantile(0.25)\n    Lower_fence = test[col].quantile(0.25) - (IQR * 3)\n    Upper_fence = test[col].quantile(0.75) + (IQR * 3)\n    low_outliers.append(Lower_fence)\n    up_outliers.append(Upper_fence)\n    print(f'{col} outliers are values < {Lower_fence} or > {Upper_fence}')\n\nfor col, outlier in zip(has_outliers, up_outliers):\n    test[col] = max_value(test, col, outlier)\n\ntest.head()","5916fb88":"plt.figure(figsize=(16,12))\nax = sns.heatmap(train.corr(), annot=True, fmt='.2f')\nax.set_title('Correlations Insurance Sell')\n","81f8ce12":"X = train.loc[:, train.columns != 'Response']\nY = train.loc[:, train.columns == 'Response']\nY = Y.to_numpy().ravel()","2e078d50":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler()\ncols = X.columns\nX = scaler.fit_transform(X)\nX = pd.DataFrame(X, columns=[cols])\nX.describe()\nX = X.to_numpy()\n\n\ncols = test.columns\ntest = scaler.fit_transform(X)\ntest = pd.DataFrame(test, columns=[cols])\ntest.describe()\ntest = test.to_numpy()","0fa040ab":"list_acc=np.zeros((5,4))\nlist_f1=np.zeros((5,4))\nlist_time=np.zeros((5,4))\nfor i in range(5):\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)\n\n    logreg = LogisticRegression(max_iter=100000)\n    nn = neighbors.KNeighborsClassifier() #neighbours=5 by default\n    dt = tree.DecisionTreeClassifier()\n    clf = RandomForestClassifier()\n    \n    \n    t0 = time.time()\n    logreg.fit(X_train,Y_train)\n    t1 = time.time()\n    nn.fit(X_train,Y_train)\n    t2 = time.time()\n    dt.fit(X_train,Y_train)\n    t3 = time.time()\n    clf.fit(X_train, Y_train)\n    t4 = time.time()\n\n    \n    Y_logreg=logreg.predict(X_test)\n    Y_nn=nn.predict(X_test)\n    Y_dt=dt.predict(X_test)\n    Y_clf=clf.predict(X_test)\n    \n    list_acc[i][0] = metrics.accuracy_score(Y_test, Y_logreg)\n    list_acc[i][1] = metrics.accuracy_score(Y_test, Y_nn)\n    list_acc[i][2] = metrics.accuracy_score(Y_test, Y_dt)\n    list_acc[i][3] = metrics.accuracy_score(Y_test, Y_clf)\n\n    list_f1[i][0] = metrics.f1_score(Y_test, Y_logreg)\n    list_f1[i][1] = metrics.f1_score(Y_test, Y_nn)\n    list_f1[i][2] = metrics.f1_score(Y_test, Y_dt)\n    list_f1[i][3] = metrics.f1_score(Y_test, Y_clf)\n\n    list_time[i][0] = t1-t0\n    list_time[i][1] = t2-t1\n    list_time[i][2] = t3-t2\n    list_time[i][3] = t4-t3","d548a51d":"plt.boxplot(list_acc);\nfor i in range(4):\n    xderiv = (i+1)*np.ones(list_acc[:,i].shape)+(np.random.rand(5,)-0.5)*0.1\n    plt.plot(xderiv,list_acc[:,i],'ro',alpha=0.3)\n    \nax = plt.gca()\nax.set_xticklabels(['Logistic regression','NN','Tree','Forest'])\nplt.ylabel('Accuracy')","8f9340b1":"plt.boxplot(list_f1);\nfor i in range(4):\n    xderiv = (i+1)*np.ones(list_f1[:,i].shape)+(np.random.rand(5,)-0.5)*0.1\n    plt.plot(xderiv,list_f1[:,i],'ro',alpha=0.3)\n    \nax = plt.gca()\nax.set_xticklabels(['Logistic regression','NN','Tree','Forest'])\nplt.ylabel('F1 Score')","ee73f281":"plt.boxplot(list_time);\nfor i in range(4):\n    xderiv = (i+1)*np.ones(list_time[:,i].shape)+(np.random.rand(5,)-0.5)*0.1\n    plt.plot(xderiv,list_time[:,i],'ro',alpha=0.3)\n    \nax = plt.gca()\nax.set_xticklabels(['Logistic regression','NN','Tree','Forest'])\nplt.ylabel('Time')","c9f49d02":"from sklearn.metrics import roc_curve,roc_auc_score\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)\nclf = RandomForestClassifier()\nclf.fit(X_train, Y_train)\nlr_probs = clf.predict_proba(X_test)\nlr_probs = lr_probs[:, 1]\n\nlr_auc = roc_auc_score(Y_test, lr_probs)\n# keep probabilities for the positive outcome only\n\n# summarize scores\nprint('ROC AUC =', lr_auc)\n# calculate roc curves\nlr_fpr, lr_tpr, _ = roc_curve(Y_test, lr_probs)\n\n# plot the roc curve for the model\nplt.plot(lr_fpr, lr_tpr, marker='.')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","731732be":"def plot_confusion_matrix(y_real, y_pred):\n    cm = confusion_matrix(y_real, y_pred)\n\n    ax= plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax, fmt='g')\n\n    ax.set_xlabel('Predicted labels')\n    ax.set_ylabel('True labels')\n\npreds= clf.predict(X_test)\nplot_confusion_matrix(Y_test, preds)","c1ae8222":"from sklearn.metrics import roc_curve,roc_auc_score\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)\nclf = LogisticRegression(max_iter=100000)\nclf.fit(X_train, Y_train)\nlr_probs = clf.predict_proba(X_test)\nlr_probs = lr_probs[:, 1]\n\nlr_auc = roc_auc_score(Y_test, lr_probs)\n# keep probabilities for the positive outcome only\n\n# summarize scores\nprint('ROC AUC =', lr_auc)\n# calculate roc curves\nlr_fpr, lr_tpr, _ = roc_curve(Y_test, lr_probs)\n\n# plot the roc curve for the model\nplt.plot(lr_fpr, lr_tpr, marker='.')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","476fc7bf":"preds= clf.predict(X_test)\nplot_confusion_matrix(Y_test, preds)","806bf242":"from sklearn.metrics import roc_curve,roc_auc_score\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2)\nclf = tree.DecisionTreeClassifier() \nclf.fit(X_train, Y_train)\n\nlr_probs = clf.predict_proba(X_test)\nlr_probs = lr_probs[:, 1]\n\nlr_auc = roc_auc_score(Y_test, lr_probs)\n# keep probabilities for the positive outcome only\n\n# summarize scores\nprint('ROC AUC =', lr_auc)\n# calculate roc curves\nlr_fpr, lr_tpr, _ = roc_curve(Y_test, lr_probs)\n\n# plot the roc curve for the model\nplt.plot(lr_fpr, lr_tpr, marker='.')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","bbc394b8":"preds= clf.predict(X_test)\nplot_confusion_matrix(Y_test, preds)","1f758c76":"np.random.seed(777)\nsampler = TPESampler(seed=0)\n\ndef create_model(trial):\n    max_depth = trial.suggest_int(\"max_depth\", 2, 20)\n    n_estimators = trial.suggest_int(\"n_estimators\", 1, 400)\n    learning_rate = trial.suggest_uniform('learning_rate', 0.0000001, 0.2)\n    gamma = trial.suggest_uniform('gamma', 0.0000001, 1)\n    scale_pos_weight = trial.suggest_int(\"scale_pos_weight\", 1, 20)\n    model = XGBClassifier(\n        learning_rate=learning_rate, \n        n_estimators=n_estimators, \n        max_depth=max_depth, \n        gamma=gamma, \n        scale_pos_weight=scale_pos_weight, \n        random_state=0,\n        eval_metric= 'error' # we fix this because is a binary classifier\n    )\n    return model\n\ndef objective(trial):\n    model = create_model(trial)\n    model.fit(X_train, Y_train)\n    preds = model.predict(X_test)\n    score = f1_score(Y_test, preds) # we fix the f1_score as evaluating metric\n    return score\n\n\"\"\"\nstudy = optuna.create_study(direction=\"maximize\", sampler=sampler)\nstudy.optimize(objective, n_trials=300) # Random parameter optimizer, for n_trials\n\nxgb_params = study.best_params\n\"\"\"\n\n","7a1f733b":"# After 300 trials, those are the best parameters that optuna could find. I will use them, instead of re-executing the hyperparameter search, for obvius reasons\nxgb_params = {\n    'max_depth': 2, \n    'n_estimators': 384, \n    'learning_rate': 0.13878057972985153, \n    'gamma': 0.0256366368300565, \n    'scale_pos_weight': 2,\n    'eval_metric':'error',\n    'random_state':0\n}\n\nxgb = XGBClassifier(**xgb_params)\nxgb.fit(X_train, Y_train)\npreds = xgb.predict(X_test)\nprint('Optimized XGBClassifier accuracy: ', accuracy_score(Y_test, preds))\nprint('Optimized XGBClassifier f1-score', f1_score(Y_test, preds))\n\nlr_probs = xgb.predict_proba(X_test)\nlr_probs = lr_probs[:, 1]\n\nlr_auc = roc_auc_score(Y_test, lr_probs)\n# keep probabilities for the positive outcome only\n\n# summarize scores\nprint('ROC AUC =', lr_auc)\n# calculate roc curves\nlr_fpr, lr_tpr, _ = roc_curve(Y_test, lr_probs)\n\n# plot the roc curve for the model\nplt.plot(lr_fpr, lr_tpr, marker='.')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","8daacf5b":"submit = xgb.predict(test)\nsubmitdf= pd.DataFrame(data=submit, columns=['Response'])\nsubmitdf","08c230be":"submitdf.to_csv('output_submission.csv',index=False)","fb7d76a3":"## **Preprocessing**\n\n### **Encoding****\n","70d968a4":"## Introduction\n\nThis DB is provided by an insurance company. It's objective its to predict which clients are going to subscribe to the insurance offer. Basically, this is a **binary classification problem**.\n\nThis dataset was used in a hackaton. The test and train were already defined and they wanted to look for the better prediccion evaluating the ROC AUC.\n\n## State-of-the-art\n\nMy notebook has influence from those two notebooks\n\n- [Jakub's notebook.](https:\/\/www.kaggle.com\/jjmewtw\/actuarial-study-eda-pca-cluster-estimation-0-88)\n- [Kostiantyn's notebook.](https:\/\/www.kaggle.com\/isaienkov\/insurance-prediction-eda-and-modeling-acc-88)\n\n\nBoth of them did a great work on the EDA so I wont pay that much attention on analysing each an every feature. The difference between them is the **approach** they take on solving the problem.\n\nThe model has a non-balanced distribution at the objective feature. Jakub did a clustering treatment plus oversampling so he enrichened the data. In the other hand, Kostiantyn looks for a good hiperparameter search, without oversampling. \n\n## Exploratory Data Analysis\n\nThen, our Target is the binary feature 'Response'. ","52d02f74":"### Decission Tree","7f1c476d":"To conclude the notebook, we will prepare the answer on the test, like if we were submitting it for the hackaton :)","39a82b29":"Some features may contain outlayers, like \"Annual_Premium\", \"Age\" or \"Vintage\".\n\nWith the next line we confirm the non-balance on the objective feature.","4a93e292":"## Conclusions\n\nIn this notebook I learned a lot about the binary classification methods currently used in Kaggle, XGBClassifier and LightGBM. In addition, I also learned about how optuna works, which is a current parameter optimizer.\n\nAnother aspect to comment on is that I have also expanded my vision by seeing another type of approach taken by [Jakub](https:\/\/www.kaggle.com\/jjmewtw\/actuarial-study-eda-pca-cluster-estimation-0-88) on his notebook. I find it very interesting to improve the quality of the data (enrich it) for better results.\n\nThanks you all if you reached this point. **Sending a virtual hug from Barcelona!**","3a0abb13":"## Model selection\n\nFor model selection we will consider the following:\n\n- LogisticRegression\n- KNN\n- DecissionTree\n- Random Forest\n\nWe do not consider SVMs for their temporary cost.\n\nWe also consider interesting other types of classifiers such as:\n- XGBClassifier: Boosting method based on Decision Tree, iteratively corrects errors during training.\n- LGBMClassifier: Boosting method based on XGBClassifier, made by microsoft and whose difference is in the speed with respect to the XGBClassifier because it makes a growth of the tree vertically, instead of horizontally. He loses some precision with respect to his father.\n\n### Candidate classifiers","c0867cb4":"### Logistic Regressor","fe94be4f":"After all these models we can come to the conclusion that by this dataset, the models using Decission Trees are the way to go, so we will try to use the current models used throughout Kaggle for their good results.\nSpecifically the ** XGBClassifier **.\n\n## XGBClassifier & Hyperparameter Search\n\nWe will directly apply the search for the best parameters on the **XGBClassifier** model as this is where we can find an improvement in a guaranteed way.","33ac1833":"We make sure there aren't null values.","d1615269":"Therefore, of our candidates, only \"Annual_Premium\" contains outliers.","a4494a17":"### Normalitzation\nWe will apply Feature Scaling, specifically MinMaxScaler.\n\nWith this we get all the values to be between 0 and 1, improving the speed of the gradient descents and the accuracy of the classifiers.","92403c6d":"Using the heatmap we find the following correlations between attributes:\n\nRegarding the objective attribute \"Response\" and the rest we find that there is a weak relationship between whether the car has been damaged (Vehicle_Damage) and a negative correlation with respect to whether the car was previously insured (Previusly_Insured)\n\n- The age of the person is strongly correlated with the age of the car because young people often drive old cars.\n- The age with the sales channel because young people usually hire online and the elderly through other channels.\n- Previously insured with age and age of the vehicle, this is because young people tend to change insurers frequently.\n- Previously insured with vehicle damage. Due to the indirect correlation of the person's age with the age of the car and the age with the chosen sales channel.","8ec070e3":"Observing this we can say that we will have to transform those *object* types later on.\n\nUsing train.describe() we can see some stats about our dataset. It only shows the numeric variables though.","2f110ac2":"### Heatmap","1d468030":"We note that only Decisson Tree-based models achieve a minimally \"acceptable\" F1 Score.","e9cfa8dc":"Aleshores, Gender y Vehicle_Damage volem codificar-ne els valors a 0 i 1.\n\nThen, we want to codify Gender and Vehicle_Damage to binary.\n\nFor Vehicle_Age we will apply a different type of transformation. So that the coding is ascend-depending on the longevity of the car.","0e391971":"We note that all models have significant accuracy.","066ed081":"Despite not getting a good ROC AUC score, we reached a good level in the F1 Score, observing the confusion matrix","29f18f19":"We drop the 'id' feature because it doesnt give info about the target.","69072ba9":"We observe that the KNN takes a long time compared to other methods and therefore, we directly discard it as a model.\n\n### Random forest","993d13a8":"### Outliers","ddf43a1a":"Because of this, we will consider the F1 Score more important than other metrics, like the Accuracy.","00f578b5":"From this execution we obtain the following results with respect to the Accuracy, the F1 Score and the execution time."}}