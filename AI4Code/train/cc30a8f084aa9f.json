{"cell_type":{"77b2348a":"code","6f4c9dec":"code","c120d29e":"code","f706d495":"code","cb513ce0":"code","131e2686":"code","ccc631e2":"code","912adf04":"code","c5bde0f3":"code","6a0a7fa6":"code","7aff3620":"code","33f44cd3":"code","fece779c":"code","34a5b032":"code","9800aa66":"code","e0b925f7":"code","b3d62c83":"code","fa965436":"code","714b420f":"code","deb36c15":"code","eaeee8d3":"code","75bed305":"code","ab49c4fc":"code","29636422":"markdown","96bcd0e5":"markdown","eef033d0":"markdown","54f42cb8":"markdown","0f6878bc":"markdown","85c5fd4d":"markdown","19692909":"markdown","4419e1e7":"markdown","0e4c84e1":"markdown","a8973f76":"markdown","2a49e714":"markdown","62ec7c20":"markdown","f0ab57c0":"markdown","2af514af":"markdown","0f1c777e":"markdown","8f7786db":"markdown","0125a508":"markdown","efaba2d6":"markdown","06730733":"markdown","594b7271":"markdown","f05d25c3":"markdown","8c2917de":"markdown","ab2415cb":"markdown","e1289786":"markdown","4e63c120":"markdown","1409e898":"markdown"},"source":{"77b2348a":"import numpy as np\nimport pandas as pd","6f4c9dec":"dataset = pd.read_csv(\"\/kaggle\/input\/air-passengers\/AirPassengers.csv\")","c120d29e":"dataset.isnull().sum()","f706d495":"dataset.shape","cb513ce0":"dataset.head()","131e2686":"data = np.array(dataset[[\"#Passengers\"]])","ccc631e2":"print(data[:5])","912adf04":"data.shape","c5bde0f3":"from sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(-1, 1))\ndata = scaler.fit_transform(data)","6a0a7fa6":"X_train = []\ny_train = []\nfor i in range(60, 144):\n    X_train.append(data[i-60:i, 0])\n    y_train.append(data[i, 0])\nX_train, y_train = np.array(X_train), np.array(y_train)","7aff3620":"X_train.shape","33f44cd3":"X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))","fece779c":"X_train.shape","34a5b032":"from keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.layers import Dense","9800aa66":"regressor = Sequential()\nregressor.add(LSTM(units = 50,return_sequences = True,input_shape = (X_train.shape[1],1)))\nregressor.add(Dropout(0.2))\nregressor.add(LSTM(units = 50,return_sequences = True))\nregressor.add(Dropout(0.2))\nregressor.add(LSTM(units = 50,return_sequences = True))\nregressor.add(Dropout(0.2))\nregressor.add(LSTM(units = 50))\nregressor.add(Dropout(0.2))\nregressor.add(Dense(units = 1))","e0b925f7":"regressor.summary()","b3d62c83":"regressor.compile(optimizer = 'adam',loss = 'mean_squared_error')","fa965436":"regressor.fit(X_train,y_train,epochs = 100, batch_size = 32)","714b420f":"X_train = np.reshape(X_train, (X_train.shape[0], 1, X_train.shape[1]))","deb36c15":"X_train.shape","eaeee8d3":"import torch.nn as nn\nimport torch\nfrom torch.autograd import Variable","75bed305":"INPUT_SIZE = 60\nHIDDEN_SIZE = 64\nNUM_LAYERS = 2\nOUTPUT_SIZE = 1","ab49c4fc":"class LSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers, output_size):\n        super(LSTM, self).__init__()\n\n        self.LSTM = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=num_layers\n        )\n        self.out = nn.Linear(hidden_size, output_size)\n\n    def forward(self, x, h_state):\n        r_out, hidden_state = self.LSTM(x, h_state)\n        \n        hidden_size = hidden_state[-1].size(-1)\n        r_out = r_out.view(-1, hidden_size)\n        outs = self.out(r_out)\n\n        return outs, hidden_state\n\nLSTM = LSTM(INPUT_SIZE, HIDDEN_SIZE, NUM_LAYERS, OUTPUT_SIZE)\n\noptimiser = torch.optim.Adam(LSTM.parameters(), lr=0.01)\ncriterion = nn.MSELoss()\n\nhidden_state = None\n\nfor epoch in range(100):\n    inputs = Variable(torch.from_numpy(X_train).float())\n    labels = Variable(torch.from_numpy(y_train).float())\n\n    output, hidden_state = LSTM(inputs, hidden_state) \n\n    loss = criterion(output.view(-1), labels)\n    optimiser.zero_grad()\n    loss.backward(retain_graph=True)                     # back propagation\n    optimiser.step()                                     # update the parameters\n    \n    print('epoch {}, loss {}'.format(epoch,loss.item()))","29636422":"Then we add a **dropout** in order to prevent overfitting.[](http:\/\/)","96bcd0e5":"Check dataset shape","eef033d0":"Then we add the last LSTM layer. Since this is last layer we do not want to take into consideration the last state of the cell, so we skip it as the default is FALSE. ","54f42cb8":"Now we get the summary of the model that we just built.","0f6878bc":"Checking the shape of the data","85c5fd4d":"1. We first create an instance of the **Sequential** class, which is the class of Keras, the Sequential model is a linear stack of layers.","19692909":"# **LSTM implentation in Keras**","4419e1e7":"Now we need to take the number of passengers and store it into an array","0e4c84e1":"Viewing the dataset","a8973f76":"Viewing the data","2a49e714":"Now we train or fit the model. We would train for 100 epochs, with a batch size of 32.","62ec7c20":"Now we start building our model. ","f0ab57c0":"But the LSTM does not accept this type of shape. So we need to reshape this so that it is suitable for LSTM.","2af514af":"Now we scale the data into a range of -1 and 1 using the scikit-learn library","0f1c777e":"We check the shape of X_train","8f7786db":"So this is the new shape.","0125a508":"Now we start the building our model.\nWe start by importing the necessary libraries.","efaba2d6":"Checking if there are any null values","06730733":"Next we add a Dropout layer and finally add a Dense Layer to get the output.","594b7271":"Now we convert the data into sequences. This is done as the LSTM layer only accepts in this form.\nWe first take 60 passenger sample into X_train and the next passenger sample is stored in y_train. We repeatedly performed this and store them into a numpy arrays X_train and y_train. \n","f05d25c3":"We start by oading necessary data pre-processing and computation libraries.","8c2917de":"Now we will compile the model. We used the adam optimizer and the loss function is mean_squared_error as we have a regression problem.","ab2415cb":"We stack up these layers.","e1289786":"Loading the dataset","4e63c120":"We are given the number of passengers travelling from a flight for 144 months. We want to predict the number of passengers. This is a typical time-series prediction problem. ","1409e898":"Then we add to the model which named as regressor an LSTM layer. This layer has 50 units or cells of LSTM, we set **return_sequences\nTRUE** to **return_sequences\nTRUE** to tell that the LSTM cell need to return the last state, so that it can be used in the next cell. Then we tell the shape of thee input sequence that we would be giving to the layer."}}