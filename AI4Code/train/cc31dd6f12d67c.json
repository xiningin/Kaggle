{"cell_type":{"c6f86712":"code","06126844":"code","5573309b":"code","9a93d737":"code","90150f96":"code","b6a3a18f":"code","6cad640c":"code","c94fdf0f":"code","058a1132":"code","5d345d66":"code","fd32247f":"code","9ca2013c":"code","efbc7ee4":"code","cee5da89":"markdown","0655a6b9":"markdown","9066b5d8":"markdown","3c13bd20":"markdown","1c83a6f4":"markdown","f93ecbba":"markdown","177bb6a6":"markdown","ccf7ab29":"markdown","048a4c24":"markdown","d5c3c953":"markdown","ee0b55cc":"markdown","77332b80":"markdown","523d771e":"markdown","bd565166":"markdown","c208b3f3":"markdown","d12fdb2f":"markdown"},"source":{"c6f86712":"#import packages\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #visualization\n\n#load dataframe\ncrimes=pd.read_table('..\/input\/crimes\/CommViolPredUnnormalizedData.txt',sep=\",\",header=None)\nprint(crimes.head())\nprint(crimes.info())","06126844":"crimes=crimes.replace('?', np.NaN)\ncrimes.iloc[:,4:]=crimes.iloc[:,4:].apply(pd.to_numeric)\nprint(crimes.head())\nprint(crimes.info())\n","5573309b":"with open ('..\/input\/feature-description\/featureDescription.txt', \"r\") as myfile:\n    description=myfile.read()\nprint(\"This text file has been read in as %s\" % type(description))\nprint(\"Below is a sample of the text file:\")\nprint(description[0:235])\n","9a93d737":"import re\nfoundlist = re.findall('(?<=--\\s)(\\w+)(?=:)' , description )\nprint(\"First seven entries: %s\" % foundlist[0:7])\nprint(\"length of list: %d\" % len(foundlist))\n\ncrimes.columns=foundlist","90150f96":"print(crimes.head())\nprint(crimes.info())","b6a3a18f":"crimes.describe()\n","6cad640c":"import seaborn as sn\nf= plt.subplots(figsize=(21,21))\nsn.heatmap(crimes.iloc[:,-18:].corr(),annot=True,fmt='.1f',cmap=\"Reds\") ","c94fdf0f":"targetVarsPerPop = ['murdPerPop','rapesPerPop','robbbPerPop','assaultPerPop','burglPerPop','larcPerPop','autoTheftPerPop','arsonsPerPop','ViolentCrimesPerPop','nonViolPerPop']\ng= plt.subplots(figsize=(12,12))\nsn.heatmap(crimes[targetVarsPerPop].corr(),annot=True,fmt='.1f',cmap=\"Reds\")","058a1132":"pd.plotting.scatter_matrix(crimes[targetVarsPerPop],alpha=0.5,figsize=[21,21],diagonal='hist',s=200,marker='.',edgecolor='black',color=\"red\")\nplt.show()","5d345d66":"import numpy as np\nfrom sklearn.cluster import KMeans\n\ndef kmeans_fill(data, n_clusters,iterations):\n    \"\"\"\n    SPECS\n    Cluster data, apply kmeans to missing values of each cluster, rinse and repeat.\n\n    Args:\n      data: data to cluster.\n      n_clusters: number of clusters \n      iterations: number of iterations\n\n    Returns:\n      labels: contains cluster labels (as integers).\n      centroids: contains cluster centroids.\n      data_new: Updated data with the missing values filled in.\n    \"\"\"\n\n    # Initialize missing values to their column means\n    missing = ~np.isfinite(data)\n    mu = np.nanmean(data, 0, keepdims=1)\n    data_new = np.where(missing, mu, data)\n\n    for i in range(iterations):\n        if i > 0:\n            # kmeans with the previous set of centroids. \n            cluster = KMeans(n_clusters, init=prev_centroids)\n        else:\n            # randomly init\n            cluster = KMeans(n_clusters)\n\n        # perform clustering on the filled-in data\n        labels = cluster.fit_predict(data_new)\n        centroids = cluster.cluster_centers_\n\n        # fill in the missing values based on their cluster centroids\n        data_new[missing] = centroids[labels][missing]\n\n        prev_labels = labels\n        prev_centroids = cluster.cluster_centers_\n\n    return(labels, centroids, data_new)\ncrimeLabels,crimeCentroids,crimes_new = kmeans_fill(crimes.iloc[:,4:].values,15,10)\nprint(crimes_new.as)","fd32247f":"crimes=crimes.fillna(crimes.mean())","9ca2013c":"from sklearn.decomposition import PCA\n\n\n#ViolentCrimes Split\nX_Viol, y_Viol = crimes.iloc[:,5:129],  crimes.iloc[:,145:146]\n\n#nonViolCrimes Split\nX_nonViol, y_nonViol = crimes.iloc[:,5:129],  crimes.iloc[:,146:147]\n#standardizing\nfrom sklearn.preprocessing import StandardScaler\n\nscalerViol = StandardScaler()\nscalerViol.fit(X_Viol)\n\nscalernonViol = StandardScaler()\nscalernonViol.fit(X_nonViol)\n\nX_Viol = scalerViol.transform(X_Viol)\nX_nonViol = scalernonViol.transform(X_nonViol)\n\n#cumulative variance plot\nplt.subplot(1, 2, 1)\npca_violent = PCA().fit(X_Viol)\nplt.plot(np.cumsum(pca_violent.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title(\"Violent Component Variance\")\n\nplt.subplot(1, 2, 2)\npca_nonviolent = PCA().fit(X_nonViol)\nplt.plot(np.cumsum(pca_nonviolent.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title(\"NonViolent Component Variance\")\nplt.tight_layout()\nplt.show()\n\n","efbc7ee4":"#select 30 features\npca_violent=PCA(30)\npca_nonviolent=PCA(30)\n\n\nX_PCA_Viol = pca_violent.fit_transform(X_Viol)\nX_PCA_nonViol = pca_violent.fit_transform(X_nonViol)\n#L2 loss function for regularization\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\n\n\n\nridgeReg_violent = Ridge(alpha=0.5,random_state=30)\nridgeReg_nonviolent = Ridge(alpha=0.5,random_state=30 )\n\nscoresViolent = cross_val_score(ridgeReg_violent,X_PCA_Viol,y_Viol,scoring='r2',cv=10)\nscoresnonViolent = cross_val_score(ridgeReg_nonviolent,X_PCA_nonViol,y_nonViol,scoring='r2',cv=10)\n#test_predictViol = ridgeReg_violent.predict(X_testViol)\nprint(\"Rsquared values for violent predictions after kfold CV with k=10: %s\" % scoresViolent)\nprint(\"Rsquared values for nonviolent predictions after kfold CV with k=10: %s\" % scoresnonViolent)","cee5da89":"Our R-squared values typically hovered around 0.5 when testing for model performance using kfolds cross validation. Perhaps we could have further refined the model by more intelligently selecting alpha. Another further drawback of this method is that it can be quite difficult to retrieve the original features that were used in determining the components found in PCA (i.e. not as intuitive as some other methods).","0655a6b9":"**DATA CLEANING**","9066b5d8":"**Exploratory Data Analysis**","3c13bd20":"**Dimensionality Reduction using PCA**\n\nFor this endeavor, we'll treat our target variables as the violentCrimesPerPop and nonViolPerPop, and try to predict those quantities with the other 125 predictor variables. \n\nWe first assign our violent\/nonviolent variables, then standardize our features as a prerequisite for PCA.\n\nThen we investigate the cumulative explained variance","1c83a6f4":"In the above scatterplot, we see an extremely strong correlation between the perPop metrics of (ViolentCrimes, assault) and (nonViolentCrimes, larcenies). This seems somewhat plausible given their [definitions](https:\/\/blogs.findlaw.com\/blotter\/2015\/01\/whats-the-difference-between-burglary-robbery-and-theft.html) ","f93ecbba":"Now we observe that all of our columns are untitled. While this technically does not affect our analysis, it would be nice to label them.  To do this, we obtain the [text descriptions](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Communities+and+Crime+Unnormalized#) of the columns in a text file and read in the descriptions:","177bb6a6":"Below we take a quick look at our numerical features - nothing looks too out of the ordinary. However, with features like \"agePct12t21\" and \"agePcT12t29\" (percentage of a community that is made up of 12 to 21 and 12 to 29 year olds, respectively), we can already start to see how some of our predictors may not be independent.","ccf7ab29":"Now our dataframe has appropriately labeled columns, typechecked values, and missing values that have been appropriately taken care of. Nice!","048a4c24":"First we note that the \"?\" character has been used as an indicator for missing values, and we cast the numerical columns as type float (as some were initialized as string objects)","d5c3c953":"Interesting! It seems like all the raw counts for our crimes are extremely highly correlated- Of course, this seems reasonable - if an area has high crime rates, then we'd probably expect lots of different crimes being committed- not just one specific. \n\nFurthermore, when we match the raw count of a specific crime to its perPop (per 100,00) counterpart, there tends to be a low correlation - this also makes sense, as those two metrics have completely different scales. \n\nLet's examine a heatmap for the perPop target variables only:","ee0b55cc":"What about our predictor variables? This dataset has 18 different crime metrics we could possibly predict for, so perhaps we should start there first.","77332b80":"Now we can use regex to obtain column names:","523d771e":"**Investigating Crime Rates**\n\n**Andrew Liu**\n\n**Dec 2, 2018**\n\nOur goal is to investigate the crimes dataset provided by the UCI machine learning repo [here](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Communities+and+Crime+Unnormalized#).\n\nThis dataset contains socioeconomic and demographic information from the 1990's on various communities in the US, along with various crime rates(burglaries, theft, murder, etc.) as reported by each community's police department. Refer back to the [documentation](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Communities+and+Crime+Unnormalized#) for a brief description of the columns\n","bd565166":"**Clustering Observations, Filling NA's**\n\nWe haven't clearly handled our missing values, so for now we'll naively just apply column means to fill in NA values. In the future perhaps we may wish to try an iterative process of clustering the data, then filling each missing value with its cluster's median.","c208b3f3":"Upon examination our dataframe has 147 columns, and this list has length 147 (only the first seven are shown above), and the list entries look reasonably similar to the features in the documentation, so we can be reasonably sure that we've captured the column names as intended.","d12fdb2f":"Nice! It looks like we can about 30 feature variables to explain at least 90% of the explained variance."}}