{"cell_type":{"be40d617":"code","cba715bb":"code","d20b4e38":"code","17e1c9a7":"code","d71c0cbc":"code","c860fac2":"code","2b92efd9":"code","7c89b41e":"code","2481f74b":"code","ea912131":"code","c81d2c80":"code","d3b433e4":"code","adee55b0":"code","6b1666a4":"code","6a5a8f42":"code","97db1435":"code","56727e4a":"code","26023b45":"code","8a2c1c6c":"code","605e52ee":"code","04aebcb7":"code","2083737e":"code","342b79c9":"code","cbbee766":"code","749aef76":"code","022b46f0":"code","07e0799f":"code","fbcba25d":"code","f1b219ec":"code","4cc5fb25":"code","2140065c":"code","12ca8932":"code","8092d56a":"code","2d212660":"markdown","42f2890d":"markdown","df91da70":"markdown","155f7e8e":"markdown"},"source":{"be40d617":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tqdm\nimport gc\nimport sys\nimport warnings\nwarnings.filterwarnings(\"ignore\")","cba715bb":"date = pd.read_csv('..\/input\/bosch-production-line-performance\/train_date.csv.zip', nrows=10000)\nnumeric = pd.read_csv('..\/input\/bosch-production-line-performance\/train_numeric.csv.zip', nrows=10000)\ncategory = pd.read_csv('..\/input\/bosch-production-line-performance\/train_categorical.csv.zip', nrows=10000)","d20b4e38":"date","17e1c9a7":"numeric","d71c0cbc":"category","c860fac2":"num_feats = ['Id',\n       'L3_S30_F3514', 'L0_S9_F200', 'L3_S29_F3430', 'L0_S11_F314',\n       'L0_S0_F18', 'L3_S35_F3896', 'L0_S12_F350', 'L3_S36_F3918',\n       'L0_S0_F20', 'L3_S30_F3684', 'L1_S24_F1632', 'L0_S2_F48',\n       'L3_S29_F3345', 'L0_S18_F449', 'L0_S21_F497', 'L3_S29_F3433',\n       'L3_S30_F3764', 'L0_S1_F24', 'L3_S30_F3554', 'L0_S11_F322',\n       'L3_S30_F3564', 'L3_S29_F3327', 'L0_S2_F36', 'L0_S9_F180',\n       'L3_S33_F3855', 'L0_S0_F4', 'L0_S21_F477', 'L0_S5_F114',\n       'L0_S6_F122', 'L1_S24_F1122', 'L0_S9_F165', 'L0_S18_F439',\n       'L1_S24_F1490', 'L0_S6_F132', 'L3_S29_F3379', 'L3_S29_F3336',\n       'L0_S3_F80', 'L3_S30_F3749', 'L1_S24_F1763', 'L0_S10_F219',\n 'Response']","2b92efd9":"length = date.drop('Id', axis=1).count()\ndate_cols = length.reset_index().sort_values(by=0, ascending=False)\nstations = sorted(date_cols['index'].str.split('_',expand=True)[1].unique().tolist())\ndate_cols['station'] = date_cols['index'].str.split('_',expand=True)[1]\ndate_cols = date_cols.drop_duplicates('station', keep='first')['index'].tolist()","7c89b41e":"data = None\nfor chunk in pd.read_csv('..\/input\/bosch-production-line-performance\/train_date.csv.zip',usecols=['Id'] + date_cols,chunksize=50000,low_memory=False):\n\n    chunk.columns = ['Id'] + stations\n    chunk['start_station'] = -1\n    chunk['end_station'] = -1\n    \n    for s in stations:\n        chunk[s] = 1 * (chunk[s] >= 0)\n        id_not_null = chunk[chunk[s] == 1].Id\n        chunk.loc[(chunk['start_station']== -1) & (chunk.Id.isin(id_not_null)),'start_station'] = int(s[1:])\n        chunk.loc[chunk.Id.isin(id_not_null),'end_station'] = int(s[1:])   \n    data = pd.concat([data, chunk])","2481f74b":"for chunk in pd.read_csv('..\/input\/bosch-production-line-performance\/test_date.csv.zip',usecols=['Id'] + date_cols,chunksize=50000,low_memory=False):\n    \n    chunk.columns = ['Id'] + stations\n    chunk['start_station'] = -1\n    chunk['end_station'] = -1\n    for s in stations:\n        chunk[s] = 1 * (chunk[s] >= 0)\n        id_not_null = chunk[chunk[s] == 1].Id\n        chunk.loc[(chunk['start_station']== -1) & (chunk.Id.isin(id_not_null)),'start_station'] = int(s[1:])\n        chunk.loc[chunk.Id.isin(id_not_null),'end_station'] = int(s[1:])   \n    data = pd.concat([data, chunk])\ndel chunk\ngc.collect()   ","ea912131":"data = data[['Id','start_station','end_station']]\nusefuldatefeatures = ['Id']+date_cols","c81d2c80":"minmaxfeatures = None\nfor chunk in pd.read_csv('..\/input\/bosch-production-line-performance\/train_date.csv.zip',usecols=usefuldatefeatures,chunksize=50000,low_memory=False):\n    features = chunk.columns.values.tolist()\n    features.remove('Id')\n    df_mindate_chunk = chunk[['Id']].copy()\n    df_mindate_chunk['mindate'] = chunk[features].min(axis=1).values\n    df_mindate_chunk['maxdate'] = chunk[features].max(axis=1).values\n    df_mindate_chunk['min_time_station'] =  chunk[features].idxmin(axis = 1).apply(lambda s: int(s.split('_')[1][1:]) if s is not np.nan else -1)\n    df_mindate_chunk['max_time_station'] =  chunk[features].idxmax(axis = 1).apply(lambda s: int(s.split('_')[1][1:]) if s is not np.nan else -1)\n    minmaxfeatures = pd.concat([minmaxfeatures, df_mindate_chunk])\n\ndel chunk\ngc.collect()","d3b433e4":"for chunk in pd.read_csv('..\/input\/bosch-production-line-performance\/test_date.csv.zip',usecols=usefuldatefeatures,chunksize=50000,low_memory=False):\n    features = chunk.columns.values.tolist()\n    features.remove('Id')\n    df_mindate_chunk = chunk[['Id']].copy()\n    df_mindate_chunk['mindate'] = chunk[features].min(axis=1).values\n    df_mindate_chunk['maxdate'] = chunk[features].max(axis=1).values\n    df_mindate_chunk['min_time_station'] =  chunk[features].idxmin(axis = 1).apply(lambda s: int(s.split('_')[1][1:]) if s is not np.nan else -1)\n    df_mindate_chunk['max_time_station'] =  chunk[features].idxmax(axis = 1).apply(lambda s: int(s.split('_')[1][1:]) if s is not np.nan else -1)\n    minmaxfeatures = pd.concat([minmaxfeatures, df_mindate_chunk])\n\ndel chunk\ngc.collect()","adee55b0":"minmaxfeatures.sort_values(by=['mindate', 'Id'], inplace=True)\nminmaxfeatures['min_Id_rev'] = -minmaxfeatures.Id.diff().shift(-1)\nminmaxfeatures['min_Id'] = minmaxfeatures.Id.diff()","6b1666a4":"cols = [['Id']+date_cols,num_feats]","6a5a8f42":"traindata = None\ntestdata = None","97db1435":"trainfiles = ['train_date.csv.zip','train_numeric.csv.zip']\ntestfiles = ['test_date.csv.zip','test_numeric.csv.zip']","56727e4a":"for i,f in enumerate(trainfiles):\n    \n    subset = None\n    \n    for chunk in pd.read_csv('..\/input\/bosch-production-line-performance\/' + f,usecols=cols[i],chunksize=100000,low_memory=False):\n        subset = pd.concat([subset, chunk])\n    \n    if traindata is None:\n        traindata = subset.copy()\n    else:\n        traindata = pd.merge(traindata, subset.copy(), on=\"Id\")\n        \ndel subset,chunk\ngc.collect()\ndel cols[1][-1]","26023b45":"for i, f in enumerate(testfiles):\n    subset = None\n    \n    for chunk in pd.read_csv('..\/input\/bosch-production-line-performance\/' + f,usecols=cols[i],chunksize=100000,low_memory=False):\n        subset = pd.concat([subset, chunk])\n        \n    if testdata is None:\n        testdata = subset.copy()\n    else:\n        testdata = pd.merge(testdata, subset.copy(), on=\"Id\")\n    \ndel subset,chunk\ngc.collect()","8a2c1c6c":"traindata = traindata.merge(minmaxfeatures, on='Id')\ntraindata = traindata.merge(data, on='Id')\ntestdata = testdata.merge(minmaxfeatures, on='Id')\ntestdata = testdata.merge(data, on='Id')","605e52ee":"del minmaxfeatures,data\ngc.collect()","04aebcb7":"traindata","2083737e":"testdata","342b79c9":"traindata.fillna(value=0,inplace=True)\ntestdata.fillna(value=0,inplace=True)","cbbee766":"np.set_printoptions(suppress=True)","749aef76":"import gc","022b46f0":"total = traindata[traindata['Response']==0].sample(frac=1).head(400000)\ntotal = pd.concat([total,traindata[traindata['Response']==1]]).sample(frac=1)","07e0799f":"del traindata\ngc.collect()","fbcba25d":"from sklearn.ensemble import RandomForestClassifier","f1b219ec":"model = RandomForestClassifier(n_estimators=500,n_jobs=-1,verbose=1,random_state=11)\nmodel.fit(total.drop(['Response','Id'],axis=1),total['Response'])","4cc5fb25":"test = model.predict(testdata.drop(['Id'],axis=1))","2140065c":"testdata['Response'] = test\ntestdata[['Id','Response']].to_csv(\"submit.csv\",index=False)","12ca8932":"!gzip submit.csv","8092d56a":"total","2d212660":"### The list of numeric features is selected based on the other XGBOOST classifier check the numericclassifier notebook","42f2890d":"# MODELLING","df91da70":"# Note: The dataset is too large in terms of features we will be using the dataset in form of chunks in the entire solution","155f7e8e":"# FEATURE ENGINEERING"}}