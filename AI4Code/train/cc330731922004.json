{"cell_type":{"31b0f462":"code","608dba4c":"code","4f4373d1":"code","14ff841d":"code","58f48cce":"code","3782684a":"code","11b93505":"code","f1fd450c":"code","03d88241":"code","1fe915e9":"code","3500ace4":"code","843ee7a8":"code","db9a995d":"markdown","c52e1d23":"markdown"},"source":{"31b0f462":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","608dba4c":"!pip install transformers","4f4373d1":"import torch\nfrom transformers import BertForQuestionAnswering\nfrom transformers import BertTokenizer","14ff841d":"model= BertForQuestionAnswering.from_pretrained('bert-base-uncased')\ntokenizer= BertTokenizer.from_pretrained('bert-base-uncased')","58f48cce":"question = \"How many parameters does BERT-large have?\"\nanswer_text = \"BERT-large is really big... it has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\"","3782684a":"input_ids= tokenizer.encode(question, answer_text)\nprint('The input has total {:} tokens'.format(len(input_ids)))","11b93505":"sep_index= input_ids.index(tokenizer.sep_token_id)\n\nnum_seg_a= sep_index + 1\n\nnum_seg_b= len(input_ids) - num_seg_a\n\nsegment_ids= [0]*num_seg_a + [1]*num_seg_b\n\n","f1fd450c":"start_scores, end_scores= model(torch.tensor([input_ids]), token_type_ids= torch.tensor([segment_ids]))","03d88241":"tokens= tokenizer.convert_ids_to_tokens(input_ids)","1fe915e9":"print(input_ids)   ##Hence proved that BERT takes only token ids as input and not the words","3500ace4":"print(tokens)","843ee7a8":"answer_start= torch.argmax(start_scores)\nanswer_end= torch.argmax(end_scores)\n\nassert answer_start < answer_end  ## Think yourself, its common sense\nanswer= ' '.join(tokens[answer_start: answer_end +1])\nprint(answer)","db9a995d":"***This notebook is written with the purpose of getting an essence of how the BERT Q\/A works. I've tried to see the various tensors and input formats that BERT requires for further processing. The code snippets are inspired from Chris McCormick's BERT blog***","c52e1d23":"# **I am getting different answer due to usage of \"Bert base\" instead of \"Bert large\" model**"}}