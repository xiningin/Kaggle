{"cell_type":{"cba9ebbf":"code","a296fe55":"code","a994e251":"code","f390139c":"code","104945c0":"code","a4a42ef0":"code","1c385c49":"code","0ce48819":"code","155fd3fd":"code","766f0527":"code","1ef86ec0":"code","28b24a4e":"code","070a28d8":"code","b7cefa99":"code","bbc201b1":"code","793d5389":"code","eb1afb3c":"code","fe325464":"code","a493ee46":"code","0e4486b2":"code","62bf17ee":"code","04bcbc0a":"code","8a356427":"code","61a59f89":"code","5eaa39b2":"code","eda071db":"code","2ea255f0":"code","087bb37c":"code","91c83ed4":"code","ba3e99f9":"code","24a7cbbc":"code","0591c483":"code","22bea7b2":"code","f0e738bc":"code","1b1c6b4e":"code","549f3422":"code","3bc4cd05":"code","9ebeb8c9":"code","446fc1a2":"code","bdd813ee":"code","e06c04c7":"code","119aa637":"code","1190516d":"code","1ad1feff":"code","92722df1":"code","fdb1449d":"code","3b8b12d6":"markdown","e52b5813":"markdown","7c3a8267":"markdown","4b56b558":"markdown","a7ae18aa":"markdown","6faed1ef":"markdown","837ad198":"markdown","93069579":"markdown","f2ba0cfc":"markdown","e88e510a":"markdown","8970e510":"markdown","5894e6e2":"markdown","98b925e6":"markdown","99c0e882":"markdown","46822631":"markdown","de54bf3f":"markdown","d1aaa850":"markdown","73ec4003":"markdown","49833708":"markdown","781355ed":"markdown","117ec47d":"markdown","7d5e3880":"markdown","e20a2db5":"markdown","b582cac7":"markdown","1c2a6a7c":"markdown","14d8d203":"markdown","19192817":"markdown","1a0517c6":"markdown","9a52ab28":"markdown","fd2114ed":"markdown","3b6cdf85":"markdown","a47efe4b":"markdown","b0845a82":"markdown"},"source":{"cba9ebbf":"!pip install nltk\n!pip install pyLDAvis\n!pip install gensim\n!pip3 install bertopic","a296fe55":"!pip install git+https:\/\/github.com\/goolig\/dsClass.git","a994e251":"from dsClass.path_helper import *","f390139c":"import warnings\nwarnings.filterwarnings('ignore')  # To ignore all warnings that arise here to enhance clarit\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\nimport nltk\n#nltk.download('stopwords')\nfrom nltk.stem import WordNetLemmatizer as Lemmatizer\nfrom nltk.corpus import stopwords\nimport pyLDAvis.gensim\nfrom gensim.models import ldamulticore \nfrom gensim.corpora.dictionary import Dictionary\nfrom nltk.stem.porter import PorterStemmer\nimport numpy as np\nimport pickle\nfrom bertopic import BERTopic\nimport time","104945c0":"text = '''The history of NLP generally started in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence.\n\nThe Georgetown experiment in 1954 involved fully automatic translation of more than sixty Russian sentences into English. The authors claimed that within three or five years, machine translation would be a solved problem.[2] However, real progress was much slower, and after the ALPAC report in 1966, which found that ten-year-long research had failed to fulfill the expectations, funding for machine translation was dramatically reduced. Little further research in machine translation was conducted until the late 1980s, when the first statistical machine translation systems were developed.\n\nSome notably successful NLP systems developed in the 1960s were SHRDLU, a natural language system working in restricted \"blocks worlds\" with restricted vocabularies, and ELIZA, a simulation of a Rogerian psychotherapist, written by Joseph Weizenbaum between 1964 and 1966. Using almost no information about human thought or emotion, ELIZA sometimes provided a startlingly human-like interaction. When the \"patient\" exceeded the very small knowledge base, ELIZA might provide a generic response, for example, responding to \"My head hurts\" with \"Why do you say your head hurts?\".\n\nDuring the 1970s, many programmers began to write \"conceptual ontologies\", which structured real-world information into computer-understandable data. Examples are MARGIE (Schank, 1975), SAM (Cullingford, 1978), PAM (Wilensky, 1978), TaleSpin (Meehan, 1976), QUALM (Lehnert, 1977), Politics (Carbonell, 1979), and Plot Units (Lehnert 1981). During this time, many chatterbots were written including PARRY, Racter, and Jabberwacky.\n\nUp to the 1980s, most NLP systems were based on complex sets of hand-written rules. Starting in the late 1980s, however, there was a revolution in NLP with the introduction of machine learning algorithms for language processing. This was due to both the steady increase in computational power (see Moore's law) and the gradual lessening of the dominance of Chomskyan theories of linguistics (e.g. transformational grammar), whose theoretical underpinnings discouraged the sort of corpus linguistics that underlies the machine-learning approach to language processing.[3] Some of the earliest-used machine learning algorithms, such as decision trees, produced systems of hard if-then rules similar to existing hand-written rules. However, part-of-speech tagging introduced the use of hidden Markov models to NLP, and increasingly, research has focused on statistical models, which make soft, probabilistic decisions based on attaching real-valued weights to the features making up the input data. The cache language models upon which many speech recognition systems now rely are examples of such statistical models. Such models are generally more robust when given unfamiliar input, especially input that contains errors (as is very common for real-world data), and produce more reliable results when integrated into a larger system comprising multiple subtasks.\n\nMany of the notable early successes occurred in the field of machine translation, due especially to work at IBM Research, where successively more complicated statistical models were developed. These systems were able to take advantage of existing multilingual textual corpora that had been produced by the Parliament of Canada and the European Union as a result of laws calling for the translation of all governmental proceedings into all official languages of the corresponding systems of government. However, most other systems depended on corpora specifically developed for the tasks implemented by these systems, which was (and often continues to be) a major limitation in the success of these systems. As a result, a great deal of research has gone into methods of more effectively learning from limited amounts of data.\n\nRecent research has increasingly focused on unsupervised and semi-supervised learning algorithms. Such algorithms are able to learn from data that has not been hand-annotated with the desired answers, or using a combination of annotated and non-annotated data. Generally, this task is much more difficult than supervised learning, and typically produces less accurate results for a given amount of input data. However, there is an enormous amount of non-annotated data available (including, among other things, the entire content of the World Wide Web), which can often make up for the inferior results.\n\nIn recent years, there has been a flurry of results showing deep learning techniques[4][5] achieving state-of-the-art results in many natural language tasks, for example in language modeling,[6] parsing,[7][8] and many others.'''\n\ncorpus = [line for line in text.splitlines() if line != '']","a4a42ef0":"corpus[0]","1c385c49":"cv = CountVectorizer(lowercase=False)\ntermMatrix = cv.fit_transform(corpus)\ndf = pd.DataFrame(data=termMatrix.toarray(),columns=cv.get_feature_names())\ndf","0ce48819":"print (corpus[0])\ndf.iloc[0][df.iloc[0]>0].to_frame().T","155fd3fd":"stopwords.words('english')","766f0527":"cv = CountVectorizer(analyzer=\"word\",token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b',stop_words=set(stopwords.words('english')))\ntermMatrix = cv.fit_transform(corpus)\ndf = pd.DataFrame(data=termMatrix.toarray(),columns=cv.get_feature_names())\ndf","1ef86ec0":"print (corpus[0])\ndf.iloc[0][df.iloc[0]>0].to_frame().T","28b24a4e":"cv = CountVectorizer(analyzer=\"word\",token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b',stop_words=set(stopwords.words('english')),ngram_range=(1,2))\ntermMatrix = cv.fit_transform(corpus)\ndf = pd.DataFrame(data=termMatrix.toarray(),columns=cv.get_feature_names())\ndf","070a28d8":"print (corpus[0])\ndf.iloc[0][df.iloc[0]>0].to_frame().T","b7cefa99":"testText = '''The program STUDENT, written in 1964 by Daniel Bobrow for his PhD dissertation at MIT is one of the earliest known attempts at natural-language understanding by a computer.[6][7][8][9][10] Eight years after John McCarthy coined the term artificial intelligence, Bobrow's dissertation (titled Natural Language Input for a Computer Problem Solving System) showed how a computer could understand simple natural language input to solve algebra word problems.\n\nA year later, in 1965, Joseph Weizenbaum at MIT wrote ELIZA, an interactive program that carried on a dialogue in English on any topic, the most popular being psychotherapy. ELIZA worked by simple parsing and substitution of key words into canned phrases and Weizenbaum sidestepped the problem of giving the program a database of real-world knowledge or a rich lexicon. Yet ELIZA gained surprising popularity as a toy project and can be seen as a very early precursor to current commercial systems such as those used by Ask.com.[11]\n\nIn 1969 Roger Schank at Stanford University introduced the conceptual dependency theory for natural-language understanding.[12] This model, partially influenced by the work of Sydney Lamb, was extensively used by Schank's students at Yale University, such as Robert Wilensky, Wendy Lehnert, and Janet Kolodner.\n\nIn 1970, William A. Woods introduced the augmented transition network (ATN) to represent natural language input.[13] Instead of phrase structure rules ATNs used an equivalent set of finite state automata that were called recursively. ATNs and their more general format called \"generalized ATNs\" continued to be used for a number of years.\n\nIn 1971 Terry Winograd finished writing SHRDLU for his PhD thesis at MIT. SHRDLU could understand simple English sentences in a restricted world of children's blocks to direct a robotic arm to move items. The successful demonstration of SHRDLU provided significant momentum for continued research in the field.[14][15] Winograd continued to be a major influence in the field with the publication of his book Language as a Cognitive Process.[16] At Stanford, Winograd would later be the adviser for Larry Page, who co-founded Google.\n\nIn the 1970s and 1980s the natural language processing group at SRI International continued research and development in the field. A number of commercial efforts based on the research were undertaken, e.g., in 1982 Gary Hendrix formed Symantec Corporation originally as a company for developing a natural language interface for database queries on personal computers. However, with the advent of mouse driven, graphic user interfaces Symantec changed direction. A number of other commercial efforts were started around the same time, e.g., Larry R. Harris at the Artificial Intelligence Corporation and Roger Schank and his students at Cognitive Systems corp.[17][18] In 1983, Michael Dyer developed the BORIS system at Yale which bore similarities to the work of Roger Schank and W. G. Lehnart.[19]\n\nThe third millennium saw the introduction of systems using machine learning for text classification, such as the IBM Watson. However, it is debated how much \"understanding\" such systems demonstrate, e.g. according to John Searle, Watson did not even understand the questions.[20]\n\nJohn Ball, cognitive scientist and inventor of Patom Theory supports this assessment. Natural language processing has made inroads for applications to support human productivity in service and ecommerce but this has largely been made possible by narrowing the scope of the application. There are thousands of ways to request something in a human language which still defies conventional natural language processing. \"To have a meaningful conversation with machines is only possible when we match every word to the correct meaning based on the meanings of the other words in the sentence \u2013 just like a 3-year-old does without guesswork\" Patom Theory'''\n\ntestCorpus = [line for line in testText.splitlines() if line != '']","bbc201b1":"testTermMatrix = cv.transform(testCorpus)\ntestDf = pd.DataFrame(data=testTermMatrix.toarray(),columns=cv.get_feature_names())","793d5389":"print (testCorpus[0])\ntestDf.iloc[0][testDf.iloc[0]>0].to_frame().T","eb1afb3c":"lemmatizer = Lemmatizer()\nprint (lemmatizer.lemmatize('ate',pos='v'))\nprint(lemmatizer.lemmatize('leaves'))","fe325464":"stemmer = PorterStemmer()\nprint (stemmer.stem('ate'))\nprint(stemmer.stem('leaves'))","a493ee46":"# Load some categories \ncategories = [\n    'sci.space',\n    'alt.atheism',\n    'comp.graphics',\n    'rec.sport.baseball'\n]\n\nprint(\"Loading 20 newsgroups dataset for categories:\")\nprint(categories)\ntrainDataset = fetch_20newsgroups(categories=categories,subset='train',shuffle=True, remove=('headers', 'footers', 'quotes'))\ntestDataset = fetch_20newsgroups(categories=categories,subset='test',shuffle=True, remove=('headers', 'footers', 'quotes'))\n# with open('testDataset.pickle', 'rb') as handle:\n#     trainDataset = pickle.load(handle)\n# with open('testDataset.pickle', 'rb') as handle:\n#     testDataset = pickle.load(handle)\n\nprint(\"%d documents for training\" % len(trainDataset.data))\nprint(\"%d documents for testing\" % len(testDataset.data))\nprint(\"%d categories\" % len(trainDataset.target_names))","0e4486b2":"def filterSmallDocs(docs,targets):\n    indices = [i for i in range(0,len(docs)) if len(docs[i].split())>20]\n    filteredDocs = [docs[i] for i in indices]\n    filteredTarget = [targets[i] for i in indices]\n    return filteredDocs,filteredTarget","62bf17ee":"trainDocs, trainTarget = filterSmallDocs(trainDataset.data,trainDataset.target)\ntestDocs, testTarget = filterSmallDocs(testDataset.data,testDataset.target)","04bcbc0a":"pd.Series(trainTarget).value_counts()","8a356427":"pd.Series(testTarget).value_counts()","61a59f89":"cv = CountVectorizer(analyzer=\"word\",token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b',stop_words='english',min_df=50, max_df=0.8, ngram_range=(1,2))\ndata = cv.fit_transform(trainDocs)\ndata = pd.DataFrame(data.toarray(), columns=cv.get_feature_names())\ndata","5eaa39b2":"cols = data.columns\nbt = data.apply(lambda x: x > 0)\nbt = bt.apply(lambda x: list(cols[x.values]), axis=1)\nbt","eda071db":"dictionary = Dictionary(bt)\n# convert tokenized documents into a document-term matrix\ncorpusTopicModeling = [dictionary.doc2bow(text) for text in bt]\ncorpusTopicModeling[0:3]","2ea255f0":"dictionary[0]","087bb37c":"print(\"The model has %d features\" % (len(dictionary)))","91c83ed4":"#This is a sample code for training the model. It takes a while, so we already trained it for you \n\n#ldaModel = ldamulticore.LdaMulticore(corpus=corpus, id2word=dictionary, passes=150, num_topics=4)\n#with open('ldaModel.pickle', 'wb') as handle:\n#    pickle.dump(ldaModel, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","ba3e99f9":"with open(get_file_path('ldaModel.pickle'), 'rb') as handle:\n    ldaModel = pickle.load(handle)","24a7cbbc":"import warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \npyLDAvis.enable_notebook()\npyLDAvis.gensim.prepare(ldaModel, corpusTopicModeling,dictionary,R=20,mds=\"tsne\")","0591c483":" ldaModel.show_topics(formatted=False)","22bea7b2":"topic_model = BERTopic(n_gram_range=(1,2),nr_topics=4)\ntopics, probs = topic_model.fit_transform(trainDocs)","f0e738bc":"topic_model.get_topic_info()","1b1c6b4e":"bt","549f3422":"cv = CountVectorizer(analyzer=\"word\",token_pattern='(?u)\\\\b[a-zA-Z][a-zA-Z]+\\\\b',stop_words='english',min_df=50, max_df=0.8, ngram_range=(1,2))\ndata = cv.fit_transform(trainDocs)\ndata = pd.DataFrame(data.toarray(), columns=cv.get_feature_names())\ncols = data.columns\nbt = data.apply(lambda x: x > 0)\nbt = bt.apply(lambda x: \" \".join(cols[x.values]), axis=1)","3bc4cd05":"bt","9ebeb8c9":"start = time.time()\ntopic_model = BERTopic(language=\"english\",nr_topics=4)\ntopics, probs = topic_model.fit_transform(bt)\nend = time.time()\nprint(\"{} sec\".format(int(end - start)))\ntopic_model.get_topic_info()","446fc1a2":"topic_model.get_topic(2)","bdd813ee":"topic_model.visualize_topics()","e06c04c7":"# Prepare data and classes\nclasses = [trainDataset.target_names[i] for i in trainTarget] \ntopics_per_class = topic_model.topics_per_class(bt, topics, classes=classes)\ntopic_model.visualize_topics_per_class(topics_per_class)","119aa637":"# sci.space\n# alt.atheism\n# comp.graphics\n# rec.sport.baseball","1190516d":"#Q2\ntopicAnnotation = ['alt.atheism',\n                   'comp.graphics',\n                   'sci.space',\n                   'rec.sport.baseball',\n                   ]\n#replace letters with the names you choose\n# the last was hard to choose(computer-graphics)","1ad1feff":"#TODO: continue from here ! merge Naor code.","92722df1":"#Q3 Train set","fdb1449d":"#Q3 Test set","3b8b12d6":"You answer geos here (Q6)","e52b5813":"## Lemmatization","7c3a8267":"# After runing this cell **restart the notebook** and continue from here.","4b56b558":"## Q6: Can [overfitting](https:\/\/en.wikipedia.org\/wiki\/Overfitting) happen in an unsupervised learning task? Explain.","a7ae18aa":"## Q8: Guy claims that we have had a very easy life in modelling the topics in our data and that in data collected from the real world it will be harder. What is the main reason for that?\nThere are pleanty of reasons for that of course, but there is **ONE** major reason for that.","6faed1ef":"<b>Documantaion:<\/b> https:\/\/github.com\/MaartenGr\/BERTopic\n<\/br>\n<b>Remark<\/b>: Due to the stochastisch nature of UMAP, the results from BERTopic might differ and the quality can degrade. ","837ad198":"## Convert the term matrix into term lists","93069579":"### As opposed to stemming","f2ba0cfc":"## Filter out small docs","e88e510a":"## Q7. Is there a difference between the results of the train and test sets? Explain the reasons behind it.\n","8970e510":"# Data preparation ","5894e6e2":"<b>Remark:<\/b> The topic name -1 refers to all documents that did not have any topics assigned. Not all documents are forced towards a certain cluster. If no cluster could be found, then it is simply an outlier.","98b925e6":"You answer goest here (Q5)","99c0e882":"## Q5: Topic modelling is considered an unsupervised learning task. Explain why.","46822631":"Your answer goes here (Q8)","de54bf3f":"## Test set","d1aaa850":"## Turn our tokenized documents into an id term, frequancy list","73ec4003":"You answer geos here (Q7)","49833708":"# Modeling using LDA","781355ed":"# Modeling using BertTopic","117ec47d":"## How does the docs distribute over the topics","7d5e3880":"# Topic modeling","e20a2db5":"# Q1: Analyzing the graph\nTurn on all of the topics from the lenged. Hint: click on the rectangles.\n1. Which topic was best idenfitied by the model?\n1. Explain why you think so. Discuss the whole graph not only the topic you selected.","b582cac7":"![wiki_NLP.PNG](attachment:wiki_NLP.PNG)","1c2a6a7c":"## Tokenization","14d8d203":"1.Best topic identified is: baseball\n\n\n2.We belive so, because baseball got the precision(pos\/pos+neg) - orange(global topic 0 is the biggest bar, and the others bars are the smallest.\nthe \"race\" is with sci.space topic, which has more False Positive. the contrast between the colors in baseball topic is the strongest( the proportion is the most drastic, as well as global topic 0 divide mostly to baseball and only small amount go to others topics).\n","19192817":"## Bigram","1a0517c6":"# Feature selection ","9a52ab28":"Note: it takes about 5 minutes to run the first cell.","fd2114ed":"## <br> Q2. Annotate the topics. I.e., label them based on the word distrubution.\nLook at the training set, trainDataset, and on the target attribute, decide on an appropriate name for each topic the LDA model created.\n\n**You are trying to give the names from the training data to the topics numbers the *LDA* model created**.\n\n\nSo if you thing that the topic number 0 by the LDA model is rec.sport.baseball than the first element in the list will be 'rec.sport.baseball'.","3b6cdf85":"## Removing all punctuation, numbers and stop words","a47efe4b":"![wiki_NLU.JPG](attachment:wiki_NLU.JPG)","b0845a82":"## <br> Q3. Evalute the LDA model using accuracy score_method (SKLEARN) and print the classification report:\nYou are expected to read the documentation and use the function properly.\n\nFirst evaluate on the train set, and then on the test set.\n\nFunctions:\n\n* https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.accuracy_score.html\n* https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.classification_report.html\n\n**Hint**: look at the function ldaModel.get_document_topics for predicting the probabilities of each topic"}}