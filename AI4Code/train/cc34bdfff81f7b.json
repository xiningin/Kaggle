{"cell_type":{"bcfeda2e":"code","8ac74c95":"code","b8f3fda0":"code","a603b8cc":"code","e228e331":"code","ebcd9624":"code","e6dd3f7a":"code","742fa410":"code","45750234":"code","55ff61cc":"code","98799378":"code","0834926b":"code","316947bc":"code","8b282e3b":"code","213c20bd":"code","11cee947":"code","8b87fc1a":"code","2db1a0ef":"code","d24d8119":"code","80c980fd":"code","50bd3ae2":"code","1b86e3c5":"code","f8ed1082":"code","0f2d34b2":"code","bcd70378":"markdown","93467512":"markdown","a7327606":"markdown","dc28beb1":"markdown","5919908b":"markdown"},"source":{"bcfeda2e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statistics\nimport sklearn\n%matplotlib inline\nfrom sklearn.model_selection import train_test_split\nfrom pandas.plotting import scatter_matrix\nfrom scipy import stats\nimport sklearn.metrics as sm\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nimport catboost as ctb\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import Lasso\nfrom xgboost import XGBRegressor\n\n%matplotlib ipympl\n\nimport matplotlib.pyplot as plt\n\n#test_df=pd.read_csv(\"items.csv\")\nitems = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitems.head()\n\n","8ac74c95":"test = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')\ntest.head()","b8f3fda0":"categories = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\ncategories.head(20)","a603b8cc":"shops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nshops.head()","e228e331":"sale_train = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\nsale_train.head()\n#sales_train.describe()","ebcd9624":"print(\"----------Top-5- Record----------\")\nprint(sale_train.head(5))\nprint(\"-----------Information-----------\")\nprint(sale_train.info())\nprint(\"-----------Data Types-----------\")\nprint(sale_train.dtypes)\nprint(\"----------Missing value-----------\")\nprint(sale_train.isnull().sum())\nprint(\"----------Null value-----------\")\nprint(sale_train.isna().sum())\nprint(\"----------Shape of Data----------\")\nprint(sale_train.shape)","e6dd3f7a":"print('Number of duplicates:', len(sale_train[sale_train.duplicated()]))","742fa410":"sale_train.info()","45750234":"sale_train.describe()","55ff61cc":"sale_train.hist(figsize=(8,8), bins=6)\nplt.show()","98799378":"import seaborn as sns\nsns.displot(sale_train['item_cnt_day'])","0834926b":"sale_train['item_cnt_day'].describe()","316947bc":"sale_train = sale_train.drop(columns=['date','date_block_num','item_price'])\nsale_train.head()","8b282e3b":"y= sale_train['item_cnt_day']\nx= sale_train.drop(columns=['item_cnt_day'])\nx.head()","213c20bd":"X_train, X_test, Y_train, Y_test = train_test_split(x,y, test_size = 0.3, random_state = 60,shuffle=True)\nprint(len(X_train))\nprint(len(X_test))","11cee947":"linear_model = LinearRegression()\nlinear_model.fit(X_train, Y_train)\n\n#make predictions using linear regression\nprice_predict =(linear_model.predict(X_test))\n\n#Measure Performance \nlinear_mse = mean_squared_error(Y_test,price_predict)\nprint(\"Mean Squared error: \",linear_mse)\nlinear_SqMse = np.sqrt(linear_mse)\nprint(\"Root Mean Squared error: \", linear_SqMse)\n\nprint(\"Explain variance score for Linear Regression =\", round(sm.explained_variance_score(Y_test,price_predict),2)) \nprint(\"R2 score for Linear Regression =\", round(sm.r2_score(Y_test, price_predict), 2))","8b87fc1a":"lasso_model= Lasso(max_iter=5000)\n\n# Train the model with training data\nlasso_model.fit(X_train,Y_train)\n#make predictions \nprice_predict =(lasso_model.predict(X_test))\n\n\nlasso_mse = mean_squared_error(Y_test,price_predict)\nprint(\"Mean Squared Error : \" , lasso_mse)\nlasso_SqMse = np.sqrt(lasso_mse)\nprint(\"Root Mean Squared error: \", lasso_SqMse)\n\nprint(\"Explain variance score for Lasso Regression =\", round(sm.explained_variance_score(Y_test,price_predict), 2)) \nprint(\"R2 score for Lasso Regression =\", round(sm.r2_score(Y_test, price_predict), 2))","2db1a0ef":"# Create a model with 50 decision trees\nforest_model= RandomForestRegressor(n_estimators = 10, random_state = 42)\n\n# Train the model with training data\nforest_model.fit(X_train,Y_train)\n#make predictions using random forest model\nprice_predict =(forest_model.predict(X_test))\n\n\nforest_mse = mean_squared_error(Y_test,price_predict)\nprint(\"Mean Squared Error : \" , forest_mse)\nforest_SqMse = np.sqrt(forest_mse)\nprint(\"Root Mean Squared error: \", forest_SqMse)\n\nprint(\"Explain variance score for Random Forest Regression =\", round(sm.explained_variance_score(Y_test,price_predict), 2)) \nprint(\"R2 score for Random Forest Regression =\", round(sm.r2_score(Y_test, price_predict), 2))","d24d8119":"# Create a model with 50 decision trees\nxgbr=  XGBRegressor()\n\n# Train the model with training data\nxgbr.fit(X_train,Y_train)\n#make predictions using random forest model\nprice_predict =(xgbr.predict(X_test))\n\n\n\nxgbr_mse = mean_squared_error(Y_test,price_predict)\nprint(\"Mean Squared Error : \" , xgbr_mse)\nxgbr_SqMse = np.sqrt(xgbr_mse)\nprint(\"Root Mean Squared error: \", xgbr_SqMse)\n\nprint(\"Explain variance score for XGB Regression =\", round(sm.explained_variance_score(Y_test,price_predict), 2)) \nprint(\"R2 score for XGB Regression =\", round(sm.r2_score(Y_test, price_predict), 2))","80c980fd":"#using catgradient boost prediction\n\nimport catboost as ctb \nmodel_CBR = ctb.CatBoostRegressor()\nmodel_CBR.fit(X_train,Y_train)\n\nprice_predict =(model_CBR.predict(X_test))                              \n\nsvr_mse = mean_squared_error(Y_test,price_predict)\nprint(\"Mean Squared Error : \" , svr_mse)\nsvr_SqMse = np.sqrt(svr_mse)\nprint(\"Root Mean Squared error: \", xgbr_SqMse)\n\nprint(\"Explain variance score for SVM Regression =\", round(sm.explained_variance_score(Y_test,price_predict), 2)) \nprint(\"R2 score for SVM Regression =\", round(sm.r2_score(Y_test, price_predict), 2))","50bd3ae2":"test.head()","1b86e3c5":"test_id = test['ID']\ntest_x= test.drop(columns=['ID'])\nprint(len(test_x.columns))","f8ed1082":"y_prdict = model_CBR.predict(test_x)\nsubmission = pd.DataFrame(list(zip(test_id, y_prdict)),\n               columns =['ID', 'item_cnt_month'])\nsubmission.tail(20)","0f2d34b2":"submission.to_csv('submission.csv', index=False)","bcd70378":"Split Data set","93467512":"There are less number of dulicates so we can ignore these","a7327606":"DATA Exploration.","dc28beb1":"Prediction using CatBoost Algorithm. ","5919908b":"Reading TestData"}}