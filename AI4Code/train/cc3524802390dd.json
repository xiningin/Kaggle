{"cell_type":{"63f4f248":"code","01143a9f":"code","4dee8a95":"code","5b03c6cd":"code","7f5e337b":"code","4a31fd8a":"code","e7b795d0":"code","357da282":"code","1053f4b6":"code","672ef66f":"code","74f90ecf":"code","689abd29":"code","385a636b":"code","db1328db":"code","7183c646":"code","e48907b6":"code","de8db56d":"code","31d10d50":"markdown","a0e56c65":"markdown","208e2f76":"markdown","ef0e3859":"markdown","92b582b4":"markdown","bc09e068":"markdown","a404f89f":"markdown","15d857e5":"markdown","21fc3aaa":"markdown","564a0238":"markdown","2cfc702c":"markdown","81f4957f":"markdown","212195b9":"markdown"},"source":{"63f4f248":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import KFold\n\nimport numpy as np\nimport pandas as pd \n\nimport os\nimport gc\nimport psutil\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, Normalizer,MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom optuna.integration import LightGBMPruningCallback\n\n# get skewed features to impute median instead of mean\nfrom scipy.stats import skew\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import linear_model\nfrom sklearn.linear_model import Ridge,Lasso\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBRegressor, XGBRFRegressor\n\nimport itertools\nimport optuna\nfrom lightgbm import LGBMClassifier,LGBMRegressor\nimport lightgbm as lgb\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom catboost import CatBoostRegressor, CatBoostClassifier\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","01143a9f":"train_data = pd.read_csv('..\/input\/titanic-create-folds\/TITANIC_Folds.csv') # Read TITANIC_Folds as train_data\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")","4dee8a95":"train_data.drop('Cabin',axis= 1,inplace= True)\ntest_data.drop('Cabin',axis= 1,inplace= True)\ntrain_data['Age'] = train_data['Age'].fillna(int(train_data['Age'].mean()))\ntest_data['Age'] = test_data['Age'].fillna(int(test_data['Age'].mean()))\ntrain_data['Embarked'] = train_data['Embarked'].fillna(train_data['Embarked'].mode()[0])\ntest_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].median())\n\ntrain_data.drop('Ticket',axis= 1,inplace= True)\ntest_data.drop('Ticket',axis= 1,inplace= True)\ntrain_data['Family_size']=train_data.SibSp + train_data.Parch + 1\ntest_data['Family_size']=test_data.SibSp + test_data.Parch + 1\ntrain_data['IsAlone'] = 1\ntrain_data[['IsAlone']][train_data.Family_size >1] = 0\ntest_data['IsAlone'] = 1\ntest_data[['IsAlone']][test_data.Family_size >1] = 0\n#-----------------------------------------------\ntrain_data['isTrain'] = 1\ntest_data['isTrain'] = 0\ntt = pd.concat([train_data,test_data])\ntt['Title']= tt.Name.apply(lambda x: x.split(',')[1].split('.')[0])\nstat_min = 10\ntitle_names = (tt['Title'].value_counts() >= stat_min)\nt=title_names.reset_index()# most common titles\nmost_freq_titles = list(t[t.Title == True]['index'])\ntt['Title']= tt.Title.apply(lambda x: x if x in most_freq_titles else 'other')\ntt= pd.get_dummies(data= tt,columns=['Title'],drop_first= True)\ntt['Fare_bins']=pd.qcut(tt.Fare, 4)\ntt['Age_bins']= pd.cut(tt.Age.astype(int), 5)\n\nlabel = LabelEncoder()\nfor rows in ['Age_bins','Fare_bins']:\n    tt[rows]=label.fit_transform(tt[rows])\n\ntt.drop(['Name','Fare','Age'],axis=1,inplace=True) # we have created 'Title' , 'Fare_bins', 'Age_bins' \ntt = pd.get_dummies(tt,columns= ['Sex','Embarked'],drop_first = True)\ntrain_data = tt[tt.isTrain == 1]\ntest_data = tt[tt.isTrain == 0]\ntest_data.drop(['Survived','isTrain','fold'],axis=1,inplace = True)\ntrain_data.drop(['isTrain'],axis=1,inplace= True)\n\n# Note don't drop PassengerId column of train_data and test_data as it is used later. Instead drop it only for test and my_folds\nuseful_features = test_data.drop('PassengerId',axis=1).columns.tolist() #########################################\ntest = test_data[useful_features]\nmy_folds = train_data.copy()","5b03c6cd":"test.shape, my_folds.shape, useful_features","7f5e337b":"# specifing Level no and Round no is very important as it will be used while saving and calling back predictions.\nLevel = 1  \nRound = 2","4a31fd8a":"params1 = {'learning_rate': 0.3139879230656916,\n 'max_depth': 3,\n 'min_child_weight': 5,\n 'subsample': 0.21024707128123832,\n 'n_estimators': 1000,\n 'objective': 'reg:squarederror',\n 'tree_method': 'gpu_hist',\n 'gpu_id': 0,\n 'predictor': 'gpu_predictor'}\nparams2 = {'class_weight': None, 'penalty': 'l1', 'C': 212.3270162715133} # make C capital manually\nparams3 = {'alpha': 4.0, 'solver': 'lsqr'}\nparams4 = {'alpha': 0.1, 'selection': 'cyclic'}\nparams5 = {'device_type': 'gpu',\n 'n_estimators': 10000,\n 'learning_rate': 0.16378896855537373,\n 'num_leaves': 1560,\n 'max_depth': 9,\n 'min_data_in_leaf': 200,\n 'lambda_l1': 10,\n 'lambda_l2': 75,\n 'min_gain_to_split': 14.788213772600205,\n 'bagging_fraction': 0.9,\n 'bagging_freq': 1,\n 'feature_fraction': 0.4}\nparams6 = {'iterations': 710,\n 'objective': 'CrossEntropy',\n 'bootstrap_type': 'Bernoulli',\n 'od_wait': 1320,\n 'learning_rate': 0.18237937201181403,\n 'reg_lambda': 66.99624458505492,\n 'random_strength': 38.72245442533837,\n 'depth': 5,\n 'min_data_in_leaf': 5,\n 'leaf_estimation_iterations': 1,\n 'subsample': 0.563747965911564}\nparams7 = {'learning_rate': 'adaptive',\n 'hidden_layer_sizes': (10, 20),\n 'alpha': 0.3,\n 'activation': 'tanh'}\nparams8 = {'leaf_size': 30, 'n_neighbors': 9, 'algorithm': 'auto', 'weights': 'uniform'}\nparams9 = {'class_weight': {1: 1, 0: 1.6080586080586081},\n 'criterion': 'entropy',\n 'max_depth': 70,\n 'min_samples_leaf': 10,\n 'min_samples_split': 2}\nparams10 = {'n_estimators': 500,\n 'learning_rate': 0.29819090106977675,\n 'algorithm': 'SAMME'}\nparams11 = {'n_estimators': 200,\n 'learning_rate': 0.24523612474099096,\n 'max_depth': 7,\n 'loss': 'exponential',\n 'criterion': 'mse',\n 'max_features': 'auto',\n 'min_samples_split': 0.14461927390807197,\n 'subsample': 0.5}  #min_sample_split to min_samples_split\nparams12 = {'l2_regularization': 3.471059177214683e-08,\n 'early_stopping': 'False',\n 'learning_rate': 0.028851213824838177,\n 'max_iter': 10000,\n 'max_depth': 16,\n 'max_bins': 229,\n 'min_samples_leaf': 46,\n 'max_leaf_nodes': 23}\nparams13 = {'device_type': 'gpu',\n 'n_estimators': 10000,\n 'learning_rate': 0.18405319172732204,\n 'num_leaves': 1880,\n 'max_depth': 10,\n 'min_data_in_leaf': 200,\n 'lambda_l1': 55,\n 'lambda_l2': 85,\n 'min_gain_to_split': 0.06526179648709107,\n 'bagging_fraction': 0.7,\n 'bagging_freq': 1,\n 'feature_fraction': 0.9}","e7b795d0":"Algo1 = XGBRegressor(**params1, random_state=141)\nAlgo2 = LogisticRegression(**params2, random_state= 141,fit_intercept=True,solver='liblinear') # make c capital in params\nAlgo3 = Ridge(**params3, random_state=141,fit_intercept=True)\nAlgo4 = Lasso(**params4, random_state = 141,fit_intercept=True)\nAlgo5 = LGBMRegressor(**params5, random_state = 141,objective='regression')\n################################################\nAlgo6 = CatBoostClassifier(**params6, random_state=141)\nAlgo7 = MLPClassifier(**params7, random_state = 141)\nAlgo8 = KNeighborsClassifier(**params8, )\nAlgo9 = DecisionTreeClassifier(**params9, random_state= 141)\nAlgo10 = AdaBoostClassifier(**params10, random_state=141)\nAlgo11 = GradientBoostingClassifier(**params11, random_state = 141)\nAlgo12 = HistGradientBoostingClassifier(**params12, random_state = 141)\nAlgo13 = LGBMClassifier(**params13, random_state = 141)\n#####################################################\n\n\nAlgos = [Algo1, Algo2, Algo3,  Algo4, Algo5, Algo6, Algo7, Algo8, Algo9, Algo10, Algo11, Algo12, Algo13]","357da282":"my_folds1 = my_folds.copy()          \ntest1  = test.copy()\n\nfinal_test_predictions = [[],[],[],[],[],[],[],[],[],[],[],[],[]]\nfinal_valid_predictions = [dict(),dict(),dict(), dict(),dict(),dict(), dict(),dict(),dict(),dict(), dict(),dict(),dict()]\nscores = [[],[],[],[],[],[],[],[],[],[],[],[],[]]\n\nfor fold in range(5):\n    xtrain = my_folds[my_folds1.fold != fold].reset_index(drop=True)\n    xvalid = my_folds[my_folds1.fold == fold].reset_index(drop=True)\n    xtest = test1.copy()\n    \n    valid_ids = xvalid.PassengerId.values.tolist() # we require Id column so just keep PassengerId in training set don't drop it\n    # note we train of useful_features so PassengerId has no role there\n    \n    ytrain = xtrain.Survived\n    yvalid = xvalid.Survived\n    \n    xtrain = xtrain[useful_features]\n    xvalid = xvalid[useful_features]\n    \n    ## preprocess\n    si = SimpleImputer(strategy='median')\n    xtrain = si.fit_transform(xtrain)\n    xvalid = si.transform(xvalid)\n    xtest = si.transform(xtest)\n    \n    # scale\n    ss = MinMaxScaler()\n    xtrain = ss.fit_transform(xtrain)\n    xvalid = ss.transform(xvalid)\n    xtest = ss.transform(xtest)\n    \n    xtrain = pd.DataFrame(xtrain, columns=useful_features)\n    xvalid = pd.DataFrame(xvalid, columns=useful_features)\n    xtest = pd.DataFrame(xtest, columns=useful_features)\n    \n    \n    \n#     for col in useful_features:\n#         xtrain[col] = np.log1p(xtrain[col])\n#         xvalid[col] = np.log1p(xvalid[col])\n#         xtest[col] = np.log1p(xtest[col]) \n        \n    for i,clf in enumerate(Algos):\n        clf.fit(xtrain, ytrain)\n        if i in [0,1,2,3,4]:\n            #XGBoostRegressor\n            valid_preds = clf.predict(xvalid)\n            test_preds = clf.predict(xtest)\n        else:\n            valid_preds = clf.predict_proba(xvalid)[:,1]\n            test_preds = clf.predict_proba(xtest)[:,1]\n        score = roc_auc_score(yvalid, valid_preds)\n        final_test_predictions[i].append(test_preds)\n        final_valid_predictions[i].update(dict(zip(valid_ids, valid_preds)))\n        scores[i].append(score)","1053f4b6":"for i,clf in enumerate(Algos):\n    print(Algos[i])\n    print(\"score:- \")\n    print(scores[i])\n    print(\"score mean and std:- \")\n    print(np.mean(scores[i]), np.std(scores[i]))\n    temp_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions[i], orient='index').reset_index()\n    temp_valid_predictions.columns = ['PassengerId', f'Level{Level}_Round{Round}_pred_{1+i}']\n    temp_valid_predictions.to_csv(f\"Level{Level}_Round{Round}_valid_pred_{1+i}.csv\", index=False)\n    \n    sample = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\n    sample.Survived = np.mean(np.column_stack(final_test_predictions[i]), axis=1)\n    sample.columns = ['PassengerId', f'Level{Level}_Round{Round}_pred_{1+i}']\n    sample.to_csv(f\"Level{Level}_Round{Round}_test_pred_{1+i}.csv\", index=False)\n    print(\"=\"*40)\n######################################################","672ef66f":"Logs =pd.DataFrame(zip([type(i).__name__ for i in Algos],scores))\nLogs.columns = ['Algos','Scores']\nLogs['mean_score'] = Logs.Scores.apply(lambda x: np.mean(x))\nLogs['std_dvn'] = Logs.Scores.apply(lambda x: np.std(x))\nLogs['Level'] = Level\nLogs['Round'] = Round\ncol_names = list(Logs.columns)\nLogs = Logs.reset_index()\ncol_names =['model_no'] + col_names\nLogs.columns = col_names\nLogs.model_no = list(range(1,Logs.shape[0]+1))\nLogs.to_csv(f\"Logs_Level{Level}_Round{Round}.csv\",index=False)\nLogs","74f90ecf":"df = train_data.copy()\ndf_test = test_data.copy()\n\nfor i,algo in enumerate(Algos):\n    df1 =pd.read_csv(f\"Level{Level}_Round{Round}_valid_pred_{1+i}.csv\")\n    df= df.merge(df1, on='PassengerId', how='left')\n    df_test1 = pd.read_csv(f\"Level{Level}_Round{Round}_test_pred_{1+i}.csv\")\n    df_test=df_test.merge(df_test1, on='PassengerId', how='left')","689abd29":"df.columns","385a636b":"df_test.columns","db1328db":"# Find the column name of added data\nfav_col=[f\"Level{Level}_Round{Round}_pred_{1+i}\" for i in range(len(Algos))]\nfav_col","7183c646":"fig, ax = plt.subplots(1,2,figsize=(20,8))         # Sample figsize in inches\nax[0].title.set_text(\"OOF\")\nax[1].title.set_text(\"Test\")\nsns.heatmap(df[fav_col+ ['Survived']].corr(), annot=True, linewidths=.5, ax=ax[0])\nsns.heatmap(df_test[fav_col].corr(), annot=True, linewidths=.5, ax=ax[1])","e48907b6":"#import matplotlib as plt\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\nhist_data = [df[str(i)] for i in fav_col] +[df.Survived] \ngroup_labels = fav_col +['True Label'] #['top1', 'top2', 'top3']\nfig = ff.create_distplot(hist_data, group_labels, bin_size=0.3, show_hist=False, show_rug=False)\nfig.show()","de8db56d":"#import matplotlib as plt\nimport plotly.figure_factory as ff\nimport plotly.express as px\n\nhist_data = [df_test[str(i)] for i in fav_col] #[sub4.target, sub4.target2, sub4.target3]\ngroup_labels = fav_col #['top1', 'top2', 'top3']\nfig = ff.create_distplot(hist_data, group_labels, bin_size=0.3, show_hist=False, show_rug=False)\nfig.show()","31d10d50":"\n<a id=\"0\"><\/a>\n# <p style=\"background-color:#FFCC70;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:5px 5px;\">LEVEL1 ROUND2<br><p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">INTRODUCTION<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">This is a part of the notebook series <i>\"My_Complete_Pipeline_for_any_ML_Competition\"<\/i> where we are building complete pipeline.<\/p> \n\n\ud83d\udcccLink of first notebook of the series <a href=\"https:\/\/www.kaggle.com\/raj401\/my-complete-pipeline-for-any-ml-competition\">https:\/\/www.kaggle.com\/raj401\/my-complete-pipeline-for-any-ml-competition<\/a><br>\n\ud83d\udcccLink of notebook where we have created folds <a href=\"https:\/\/www.kaggle.com\/raj401\/titanic-create-folds\">https:\/\/www.kaggle.com\/raj401\/titanic-create-folds<\/a><br>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">\n    If you like my effort please do <b><span style=\"color:crimson; font-size:20px\">UPVOTE\ud83d\udc4d<\/span><\/b>, it really keeps me motivated. <\/p>\n\n\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:50%;text-align:center;border-radius:20px 60px;\">\"\"<\/p> \n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">This is the Last notebook of ROUND2. So far we have found optimal hyperparameters of several models in ROUND2. Now this is the time we use those models along with their hyperparameter to make predictions on test set.\n<br>In the <b>TITANIC_Create_Folds<\/b> notebook we have modified our training set by adding new column named 'fold' and then saved it as <i>TITANIC_folds.csv<\/i>. In this notebook we will use this modified training set instead of original training set. I am providing the link of <i>TITANIC_folds.csv<\/i> you can just add it to your notebook and you are good to go.<b><br>[Make sure you have added it before moving further. If you have TITANIC_Create_Folds notebook you can also add that notebook instead.]<br><\/b>\n\ud83d\udcccLink of Dataset containing <i>TITANIC_folds.csv<\/i> <a href=\"https:\/\/www.kaggle.com\/raj401\/my-complete-pipeline-for-any-ml-competition\">https:\/\/www.kaggle.com\/raj401\/my-complete-pipeline-for-any-ml-competition<\/a><br><\/p> \n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:50%;text-align:center;border-radius:20px 60px;\">\"\"<\/p> \n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">\n    <br><b>So how does this work? [STACKING and BLENDING]<\/b>\n    <br>While training of model we will run a for loop for range(0,5) i.e in each loop we will pick one of the fold out of [0,1,2,3,4] folds. Like let say we picked 3. Then we will make it as validation set and train our model on folds 0,1,2,4. Once our model is trained we will make predictions on that validation set and find roc_auc_score. This will represent our current model. We will make predictions on our whole test set. Then we will store this test_prediction as well as our validation prediction which was only on a part of training set.\n<br>In next loop we will pick another fold let say fold4 then fold 4 will be our validataion set and fold 0,1,2,3 will be our training set. We will repeat the same process. \n<br> This way when we have covered all folds we will have 5 different predictions of test set and one completed prediction of whole training set i.e in each loop on fold was predicted. Also we will have 5 roc_auc_score. We will find it's variance. We will store all this information in a DataFrame called Logs. So basically for each model we will make predictions on test set and train set(called as OOF out of fold predictions because we made prediction only on that fold which we didn't trained current model, this we do to avoid data leakage because training on x dataset x and then predicting on same dataset x can't tell how our model is. It may be badly overfitting that is why we make OOF prediction)<br><b>Now one may ask why are we making prediction on train set as we needed to make prediction on test set.<\/b>\n<br>The answer is: Because we are again going to make predictions on these test prediction using new models called as meta models. So prediction of predicted test set. This will boost our score. But now for initial test set we had traininig set on which we trainied our model and predicted on test set. But now if we train on same initial training set then our answer will be wrong because now our new test set is not of same format(Initial test set and train set contained features like Age, Gender, Embarked etc but now our new test set contains columns like test_pred_from_algo1, test_pred_from_algo2.. etc.)\n<br> So here we will use that OOF predictions to train our meta models and then make predictions on this new test set.\n    <br><b> [That completes our explanation]<\/b>\n<br> Note this method works because we are making a very strong validation set and thus we will not be overfitting. Also it gives a correct way of measuring actual performance of our models. As each model has 5 roc_auc_score for each fold. Now since we did Stratified KFold mean distribution of labels in each fold is same so model score should be on same in each fold.But if there is too much variance in the score of each fold. It means our model is probably overfitting thus we can't trust on it. On the other hand if score in each fold are quite close mean our model is quite <b>STABLE<\/b> and we have created a very strong validation set. Our model will perform better on test set.\n<br>Now Let's start coding...<\/p>\n\n\n\n<a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">TABLE OF CONTENTS<\/p>   \n    \n* [1. IMPORTING LIBRARIES](#1)\n    \n* [2. READ DATASETS](#2)\n\n* [3. PREPROCESSING](#3)\n    \n    \n* [4. ALL MODELS](#4)\n    \n* [5. TRAIN ALL MODELS](#5)\n    \n    \n* [6. SAVE TEST PREDICTION and OOF PREDICTION](#6)\n    \n* [7. CREATE LOGS](#7)\n    \n* [8. VISUALIZE](#8)\n    * [8.1. OOF PREDICTIONS vs TARGET](#8.1)\n    * [8.2. TEST SET PREDICTIONS](#8.2)\n    \n    \n* [9. CONCLUSION](#9)\n    \n* [10. END](#10)\n\n<a id=\"1\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">IMPORTING LIBRARIES<\/p>","a0e56c65":"<a id=\"6\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">SAVE TEST PREDICTION and OOF PREDICTION<\/p>","208e2f76":"<a id=\"8\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">VISUALIZE<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Now we will merge these new predictions of TEST set and OOF set to test_data and train_data respectively<\/p>","ef0e3859":"<a id=\"3\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">PREPROCESSING<\/p>\n","92b582b4":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Either you can add the dataset whoose link I have given above or if you have TITANIC_Create_Folds notebook you can add it's output from <code>Add data<\/code> option. Both contains TITANIC_Folds.csv (modified train set). <br>\nNow read it as train_data.<\/p>","bc09e068":"<a id=\"2\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">READ DATASETS<\/p>","a404f89f":"<a id=\"4\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">ALL MODELS<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">We will copy and paste the hyperparameter values of all the models which we have optimized previously using OPTUNA<\/p> ","15d857e5":"<a id=\"9\"><\/a>\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">CONCLUSION<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">We will use these TEST predictions and OOF predictions while training meta models. In next notebook we will start with LEVEL2. \n<br> That is all for now, If you have any doubt feel free to ask me in the comment. <br>\n    If you appreciate my effort please do <b>UPVOTE\ud83d\udc4d<\/b> and I will see you in the next Notebook\ud83d\udcd2. <\/p>\n\n**<span style=\"color:#444160;\"> Thanks!<\/span>**\n<a id=\"10\"><\/a>\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">END<\/p>\n    <a href=\"#top\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","21fc3aaa":"<a id=\"7\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">CREATE LOGS<\/p>","564a0238":"<a id=\"8.2\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">TEST SET PREDICTIONS<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Here we plot all the TEST SET predictions to see similarity between predictions of each model.<\/p>","2cfc702c":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">We draw correlation plot of  (OOF SET + Target) and TEST SET.<\/p>","81f4957f":"<a id=\"5\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">TRAIN ALL MODELS<\/p>","212195b9":"<a id=\"8.1\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">OOF PREDICTIONS vs TARGET<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Here we plot OOF predictions of each model and Survived column on same plot to compare their distribution.<\/p>"}}