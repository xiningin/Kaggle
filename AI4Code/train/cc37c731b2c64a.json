{"cell_type":{"c450bf05":"code","6ea1c0b5":"code","ffe99e95":"code","b8148763":"code","93d8b0ec":"code","c9aef54f":"code","2170dad5":"code","c4cf0302":"code","cee8a883":"code","102fb13f":"code","bf4cc0a3":"code","e9c9219b":"code","a6aaa46c":"markdown","3e5902c7":"markdown","256f7677":"markdown","bc2dcece":"markdown","38b108c6":"markdown","60f54954":"markdown","a1f05cc5":"markdown","0f9c9480":"markdown","1efc8d8b":"markdown"},"source":{"c450bf05":"import os\nimport os.path\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.base import BaseEstimator, ClassifierMixin\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nwarnings.simplefilter(\"ignore\")","6ea1c0b5":"dftrain = pd.read_csv('https:\/\/storage.googleapis.com\/tf-datasets\/titanic\/train.csv')\ndfeval = pd.read_csv('https:\/\/storage.googleapis.com\/tf-datasets\/titanic\/eval.csv')\ny_train = dftrain.pop('survived')\ny_eval = dfeval.pop('survived')","ffe99e95":"dftrain.head()","b8148763":"cat_col = ['sex', 'n_siblings_spouses', 'parch', 'class', 'deck',\n           'embark_town', 'alone']\nfor i in cat_col:\n    le = LabelEncoder()\n    n = str(i) + '_n'\n    \n    dftrain[n] = le.fit_transform(dftrain[i])\n    dfeval[n] = le.fit_transform(dfeval[i])\n    \n    del dfeval[i]\n    del dftrain[i]\n\ndftrain.head()","93d8b0ec":"dftrain = dftrain.values\ndfeval = dfeval.values\ny_train = y_train.values\ny_eval = y_eval.values","c9aef54f":"\"\"\" Gradient Boosted Tree\"\"\"\n\n# Author: Seyedsaman Emami\n\n# Licence: GNU Lesser General Public License v2.1 (LGPL-2.1)\n\nclass TFBT(BaseEstimator, ClassifierMixin):\n\n    def __init__(self, *,\n                 n_batches_per_layer=1,\n                 label_vocabulary=None,\n                 n_trees=100,\n                 max_depth=4,\n                 learning_rate=0.3,\n                 max_steps=None,\n                 steps=None):\n        self.n_batches_per_layer = n_batches_per_layer\n        self.label_vocabulary = label_vocabulary\n        self.n_trees = n_trees\n        self.max_depth = max_depth\n        self.learning_rate = learning_rate\n        self.max_steps = max_steps\n        self.steps = steps\n\n\n    def _dataframe(self, X, y):\n        X = pd.DataFrame(X)\n        y = pd.DataFrame(y)\n        X = X.astype(\"int64\")\n        y = y.astype(str) if (self.label_vocabulary) else y.astype(\"int64\")\n\n        self.feature = []\n        for i in range(len(X.columns)):\n            self.feature.append(str(i))\n\n        col_rename = {i: j for i, j in zip(X.columns, self.feature)}\n        X = X.rename(columns=col_rename, inplace=False)\n\n        return X, y\n\n    def _make_input_fn(self, X, y, n_epochs=None, shuffle=True):\n        def input_fn():\n            NUM_EXAMPLES = len(y)\n            dataset = tf.data.Dataset.from_tensor_slices((dict(X), y))\n            if shuffle:\n                dataset = dataset.shuffle(NUM_EXAMPLES)\n            # For training, cycle thru dataset as many times as need (n_epochs=None).\n            dataset = dataset.repeat(n_epochs)\n            # In memory training doesn't use batching.\n            dataset = dataset.batch(NUM_EXAMPLES)\n            return dataset\n        return input_fn\n\n    def _accuracy(self, evaluate):\n        item = list(evaluate.items())\n        array = np.array(item)\n        return (array[0, 1]).astype(np.float64)","2170dad5":"class BoostedTreesClassifier(TFBT):\n\n    def fit(self, X, y):\n\n        X, y = self._dataframe(X, y)\n\n        # Training and evaluation input functions.\n        train_input_fn = self._make_input_fn(X, y)\n\n        # feature selection\n        num_columns = self.feature\n        feature_columns = []\n        n_classes = len(np.unique(y))\n\n        for feature_name in num_columns:\n            feature_columns.append(tf.feature_column.numeric_column(feature_name,\n                                                                    dtype=tf.float32))\n        self.est = tf.estimator.BoostedTreesClassifier(feature_columns,\n                                                       n_batches_per_layer=self.n_batches_per_layer,\n                                                       n_classes=n_classes,\n                                                       n_trees=self.n_trees,\n                                                       max_depth=self.max_depth,\n                                                       learning_rate=self.learning_rate,\n                                                       label_vocabulary=self.label_vocabulary,\n                                                       center_bias=True\n                                                       )\n\n        self.est.train(train_input_fn, max_steps=self.max_steps,\n                       steps=self.steps)\n\n        return self\n\n    def score(self, X, y):\n        X, y = self._dataframe(X, y)\n\n        eval_input_fn = self._make_input_fn(X, y,\n                                            shuffle=False,\n                                            n_epochs=1)\n\n        accuracy = self._accuracy(self.est.evaluate\n                                  (eval_input_fn,\n                                   steps=self.steps))\n\n        return accuracy\n","c4cf0302":"tfbt = BoostedTreesClassifier(n_batches_per_layer=1,\n                              n_trees=50,\n                              max_depth=3,\n                              learning_rate=0.1,\n                              max_steps=100)\nprint(tfbt)","cee8a883":"tfbt.fit(dftrain, y_train)","102fb13f":"tfbt.score(dfeval, y_eval)","bf4cc0a3":"N = 100\nerr_tfbt = []\nfor i in range(1, N + 1):\n\n    model = BoostedTreesClassifier(n_batches_per_layer=1,\n                              n_trees=i,\n                              max_depth=3,\n                              learning_rate=0.1,\n                              max_steps=100)\n\n    model.fit(dftrain, y_train)\n    err_tfbt.append(model.score(dfeval, y_eval)) ","e9c9219b":"plt.plot(range(1, N + 1), err_tfbt)\nplt.grid(True, linewidth=0.5, color='gainsboro', linestyle='-')\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Boosting Iteration\")\nplt.title(\"Average generalization accuracy\")\nplt.text(10, 0.799, 'Dataset: Titanic')\nplt.show(block=False)","a6aaa46c":"# Import dataset","3e5902c7":"# Model evaluation","256f7677":"# Build a wrapper for the TFBT Model","bc2dcece":"# One hot encoding\nConverting categorical features and class labels","38b108c6":"# Build the model\n<h3> Build the model which I defined the wrapper already <\/h3>","60f54954":"# Import Libraries","a1f05cc5":"# Prepare the class labels for the model","0f9c9480":"# Train the model","1efc8d8b":"# Model evolution"}}