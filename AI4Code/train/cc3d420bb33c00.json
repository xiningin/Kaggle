{"cell_type":{"7416af45":"code","1d533d5a":"code","f054b9d1":"code","a43d785f":"code","883b0b07":"code","ef8412aa":"code","5b8ec403":"code","28353d9c":"code","f9ffad93":"code","35725c23":"code","d62b13f3":"code","bb4ac305":"code","8f90e9ec":"code","2f4c3135":"code","9df00bc2":"code","416d05d5":"code","4d174743":"code","099e1cef":"code","2dcd7685":"code","44aabaa2":"code","abe1258a":"code","99ed30f9":"code","4545f03e":"code","4b9660bd":"code","6a0e73d0":"code","8dd86eb8":"code","9f51fcea":"code","ed7ad382":"code","03f11bc4":"code","9cfb4b68":"code","b1044b9a":"code","0f86a69a":"code","b635d61e":"code","7e0feb47":"code","913c6977":"code","8087938b":"code","8d7ed3d1":"code","a791d4b8":"code","383dec70":"code","6e5ec2bc":"markdown","6a94a7d7":"markdown","c9745939":"markdown","8ee3305b":"markdown","717035f3":"markdown","3d8704ff":"markdown","84e64c84":"markdown","f08ff8bc":"markdown","2f34bc77":"markdown","48eae305":"markdown","eed6aae4":"markdown"},"source":{"7416af45":"#importing the required libraries\n\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n%matplotlib inline\nimport warnings\nfrom collections import Counter\nwarnings.filterwarnings('ignore')","1d533d5a":"#loading the given training dataset\n\ntrain_set = pd.read_csv('\/kaggle\/input\/equipfails\/equip_failures_training_set.csv')","f054b9d1":"#exploring first few rows of training data\n\ntrain_set.head()","a43d785f":"#reading the given test data\n\ntest_set = pd.read_csv('\/kaggle\/input\/equipfails\/equip_failures_test_set.csv')","883b0b07":"#exploring first few rows of test data\n\ntest_set.head()","ef8412aa":"# Counting the occurence of each target value - 0 and 1\ng = sns.countplot(x='target', data = train_set)\nplt.xlabel('Target')\nplt.ylabel('Number of Records')","5b8ec403":"# Concatenating train_set and test_set to clean both the sets together\n\ndata = pd.concat(objs=[train_set, test_set], axis=0).reset_index(drop=True)","28353d9c":"# Length of training data so that later we can split our training and test data\n\ntrain_len = len(train_set)","f9ffad93":"# Replacing string 'na' with NaN values\n\ndata = data.replace('na', np.NaN)","35725c23":"# Converting all measures to numerical data type\nfor col in data.columns:\n    if col not in ['id', 'target']:\n        data[col] = data[col].astype(np.float)","d62b13f3":"# Checking for unique values in each column in full dataset\n\nunique_data = data.nunique().reset_index()\nunique_data.columns = ['Name','Unique_Count']","bb4ac305":"# Checking the columns which have less than 2 unique values across the training and test data sets because columns with \n# constant values do not support our model's prediction\n\nunique_data[unique_data.Unique_Count < 2]","8f90e9ec":"# Checking for null values across each column in dataset\n\nnull_df = data.isna().sum().reset_index()\nnull_df.columns = ['Name', 'Unique_Count']","2f4c3135":"# sorting the columns having null values\n\nnull_df.sort_values('Unique_Count', ascending=False).head(5)","9df00bc2":"#Outlier detection - Finding rows with more than two outlier values in columns\n\ndef outliers(df, n, features):\n    \n    outlier_indices = []\n    \n    for col in features:\n        \n        #1st quartile\n        Q1 = np.percentile(df[col], 25)\n        \n        #3rd quartile\n        Q3 = np.percentile(df[col], 75)\n        \n        #inter quartile range\n        IQR = Q3 - Q1\n        \n        #identify index of outlier rows\n        outlierlist = df[(df[col] < (Q1 - (1.5 * IQR))) | (df[col] > (Q3 + (1.5 * IQR)))].index\n        \n        outlier_indices.extend(outlierlist)\n        \n    #selecting rows with more than two outliers\n    outlier_indices = Counter(outlier_indices)\n    multipleoutliers = list(k for k,v in outlier_indices.items() if v > n)\n    \n    return multipleoutliers\n\n#detect outliers from Age, SibSp, Fare and Parch\nfinaloutliers = outliers(data, 2, data.columns)","416d05d5":"#outlier detection\ndata.iloc[finaloutliers]","4d174743":"# Dropping sensor54_measure as it has constant values + nulls\n\ndata.drop(columns=['sensor54_measure'], axis=1, inplace=True)","099e1cef":"# Removing columns which mostly have null values - more than 75%\n\ndata.drop(columns=['sensor43_measure', 'sensor42_measure', 'sensor41_measure', 'sensor40_measure', 'sensor2_measure', \\\n                  'sensor39_measure', 'sensor38_measure', 'sensor68_measure'], axis=1, inplace = True)","2dcd7685":"# Replacing NaN with median values\n\nfor col in data.columns:\n    if col not in ['id','target']:\n        data[col] = data[col].fillna(data[col].median())","44aabaa2":"data.isnull().sum().sort_values(ascending=False).head(5)","abe1258a":"# Correlation matrix between highly correlated varaibles and target value\nfig, ax = plt.subplots(figsize = (18, 18))\ng = sns.heatmap(data[['sensor104_measure','sensor103_measure','sensor10_measure','sensor11_measure','sensor12_measure','sensor13_measure',\n     'sensor14_measure','sensor15_measure','sensor46_measure','sensor27_measure','sensor31_measure',\n     'sensor32_measure','sensor33_measure',\n      'sensor44_measure','sensor48_measure','sensor49_measure','sensor59_measure',\n     'sensor53_measure', 'sensor78_measure', 'sensor72_measure','sensor87_measure','sensor88_measure','sensor89_measure',\n     'sensor8_measure', 'sensor90_measure', 'sensor91_measure', 'sensor94_measure', 'sensor95_measure', 'target']].corr(), annot=True, ax=ax)","99ed30f9":"data['sensor_1415'] = data['sensor14_measure'] - data['sensor15_measure']\ndata['sensor_7872'] = data['sensor78_measure'] - data['sensor72_measure']\ndata['sensor3214'] = data['sensor32_measure'] - data['sensor14_measure']\ndata['sensor148'] = data['sensor14_measure'] - data['sensor8_measure']\ndata['sensor4615'] = data['sensor46_measure'] - data['sensor15_measure']\ndata['sensor815'] = data['sensor15_measure'] - data['sensor8_measure']\ndata['sensor468'] = data['sensor46_measure'] - data['sensor8_measure']\ndata['sensor278'] = data['sensor27_measure'] - data['sensor8_measure']\ndata['sensor8933'] = data['sensor89_measure'] - data['sensor33_measure']\ndata['sensor9495'] = data['sensor94_measure'] - data['sensor95_measure']\ndata['sensor1427'] = data['sensor14_measure'] - data['sensor27_measure']","4545f03e":"# Dropping sensor32_measure as it mostly duplicates sensor8_measure\ndata.drop(columns=['sensor32_measure'], axis=1, inplace=True)","4b9660bd":"data.head()","6a0e73d0":"# Dropping id feature as it does not affect model performance\ndata.drop(columns=['id'], axis=1, inplace=True)","8dd86eb8":"# train_set and test_set split\ntrain_set = data[:train_len]\ntest_set = data[train_len:]","9f51fcea":"# X and y split\nX = train_set.drop(labels=['target'], axis=1)\ny = train_set['target']","ed7ad382":"# Train and val split\n\nfrom sklearn.model_selection import GridSearchCV, train_test_split, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state=0)","03f11bc4":"# RF classifier\n\nrfr = RandomForestClassifier(n_estimators = 100, random_state=0, n_jobs=4, class_weight={0:1,1:2}, verbose=1)\nrfr.fit(X_train,y_train)","9cfb4b68":"#Predicting validation set results\ny_pred = rfr.predict(X_val)","b1044b9a":"#Checking f1 score based on validation set results\nfrom sklearn.metrics import f1_score\nf1_score(y_val, y_pred)","0f86a69a":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=5, loss='deviance', verbose=1)","b635d61e":"# Fitting with train values and prediciting for validation set\ngb.fit(X_train, y_train)\ny_pred_gb = gb.predict(X_val)","7e0feb47":"# Checking f1 score for GBDT model using validation set results\nfrom sklearn.metrics import f1_score\nf1_score(y_val, y_pred_gb)","913c6977":"# Ada Boost classifier\nfrom sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier()\nada.fit(X_train, y_train)","8087938b":"# Predicting validation set results\ny_pred_ada = ada.predict(X_val)\n\n#Checking f1 score\nf1_score(y_val, y_pred_ada)","8d7ed3d1":"test_set.drop(columns=['target'], axis=1, inplace=True)","a791d4b8":"def finalpred(testset):\n    finalpred = rfr.predict(testset)\n    \n    prediction = pd.Series(finalpred, name = 'target')\n    test_id = pd.read_csv('\/kaggle\/input\/equipfails\/equip_failures_test_set.csv')\n    submission = pd.concat([test_id['id'], prediction], axis = 1)\n    submission['target'] = submission['target'].astype(np.int)\n    submission.to_csv('finalsub.csv', index=False)","383dec70":"finalpred(test_set)","6e5ec2bc":"From all three results, we see that Random Forest classifier performs the best on our validation set. Hence we will be using this model to predict our test set results","6a94a7d7":"Here sensor54_measure seems to have a constant value across all observations. Hence it is of no use to our model. We will remove this feature. ","c9745939":"Below algorithms are being used in our modeling process.\n\n- Random Forest\n- Gradient Boosting Decision Trees\n- Ada Boost\n\nWe will be comparing the performance of the models on our validation set and use the better performing one for our test set predictions. ","8ee3305b":"## Modeling","717035f3":"#### The Challenge:\nA data set has been provided that has documented failure events that occurred on surface equipment and down-hole equipment. For each failure event, data has been collected from over 107 sensors that collect a variety of physical information both on the surface and below the ground.\n\nUsing this data, can we predict failures that occur both on the surface and below the ground? Using this information, how can we minimize costs associated with failures?\n\nThe goal of this challenge will be to predict surface and down-hole failures using the data set provided. This information can be used to send crews out to a well location to fix equipment on the surface or send a workover rig to the well to pull down-hole equipment and address the failure.","3d8704ff":"We have an imabalanced class situation in the train data. The number of observations having downhole failures seems to be quite less. Hence it is important to evaluate the model using the F1 score instead of a common metric like accuracy.","84e64c84":"Here, all columns with more than 75% missing value have been removed.","f08ff8bc":"## Exploratory Data Analysis","2f34bc77":"Since there are multiple features that are highly correlated within themselves and with the target feature as well, we will add a few interaction terms to make the relationship more explicit","48eae305":"We do not have any rows having two or more outlier values. Hence we are not removing any observations from our dataset.","eed6aae4":"Since most of the obeservations in each sensor measure is close to 0 and rest of the observations have an extremely high value, imputing missing values using mean would result in incorrect high values. Hence we choose to impute the missing values using the median of each column"}}