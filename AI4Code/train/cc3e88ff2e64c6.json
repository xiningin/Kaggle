{"cell_type":{"099a0738":"code","2e3baafc":"code","b0cd3385":"code","922dc538":"code","7e60ccca":"code","c75f88ae":"code","48afedee":"code","5dfaa836":"code","ec1aafc8":"code","6e005051":"code","30ae2081":"code","18ffcf9d":"code","e80eee77":"code","8d9b0313":"code","9dcd2be8":"code","4984d2d8":"code","5fbf197f":"code","97ec3cfb":"code","614718ea":"code","83413c3a":"code","e00f1a40":"code","b78e7c90":"code","2c87fd1b":"code","88f8e83a":"code","f80ae0be":"code","3c9e223b":"code","281ac63f":"code","0dba0bb0":"code","ec3cf735":"code","5866beb7":"code","ff7ce707":"code","ec2d80f6":"code","96d9e09a":"markdown","48502f87":"markdown","7fd8e119":"markdown","465b79da":"markdown","aa47f59b":"markdown","c53d8c65":"markdown","c696aac7":"markdown","0c92d716":"markdown","d93627ad":"markdown","0c4d7a6b":"markdown","c7f7f809":"markdown","994ecef1":"markdown","6c4572b1":"markdown","ab313f2c":"markdown","3acae99b":"markdown"},"source":{"099a0738":"import pandas as pd\nimport numpy as np\nimport multiprocessing\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport gc\nfrom time import time\nimport datetime\nfrom tqdm import tqdm_notebook\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import KFold, TimeSeriesSplit\nfrom sklearn.metrics import roc_auc_score\nwarnings.simplefilter('ignore')\nsns.set()\n%matplotlib inline\nfrom fancyimpute import KNN\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.impute import SimpleImputer","2e3baafc":"files = ['..\/input\/test_identity.csv', \n         '..\/input\/test_transaction.csv',\n         '..\/input\/train_identity.csv',\n         '..\/input\/train_transaction.csv',\n         '..\/input\/sample_submission.csv']","b0cd3385":"%%time\ndef load_data(file):\n    return pd.read_csv(file)\n\nwith multiprocessing.Pool() as pool:\n    test_id, test_tr, train_id, train_tr, sub = pool.map(load_data, files)","922dc538":"train = pd.merge(train_tr, train_id, on='TransactionID', how='left')\ntest = pd.merge(test_tr, test_id, on='TransactionID', how='left')\n\ndel test_id, test_tr, train_id, train_tr\ngc.collect()","7e60ccca":"useful_features = ['TransactionAmt', 'ProductCD',\"card1\", 'card2', 'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1',\n                   'P_emaildomain', 'R_emaildomain', 'C1', 'C2', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10', 'C11', 'C12', 'C13',\n                   'C14', 'D1', 'D2', 'D3', 'D4', 'D5', 'D6', 'D8', 'D9', 'D10', 'D11', 'D12', 'D13', 'D14', 'D15', 'M2', 'M3',\n                   'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V17',\n                   'V19', 'V20', 'V29', 'V30', 'V33', 'V34', 'V35', 'V36', 'V37', 'V38', 'V40', 'V44', 'V45', 'V46', 'V47', 'V48',\n                   'V49', 'V51', 'V52', 'V53', 'V54', 'V56', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64', 'V69', 'V70', 'V71',\n                   'V72', 'V73', 'V74', 'V75', 'V76', 'V78', 'V80', 'V81', 'V82', 'V83', 'V84', 'V85', 'V87', 'V90', 'V91', 'V92',\n                   'V93', 'V94', 'V95', 'V96', 'V97', 'V99', 'V100', 'V126', 'V127', 'V128', 'V130', 'V131', 'V138', 'V139', 'V140',\n                   'V143', 'V145', 'V146', 'V147', 'V149', 'V150', 'V151', 'V152', 'V154', 'V156', 'V158', 'V159', 'V160', 'V161',\n                   'V162', 'V163', 'V164', 'V165', 'V166', 'V167', 'V169', 'V170', 'V171', 'V172', 'V173', 'V175', 'V176', 'V177',\n                   'V178', 'V180', 'V182', 'V184', 'V187', 'V188', 'V189', 'V195', 'V197', 'V200', 'V201', 'V202', 'V203', 'V204',\n                   'V205', 'V206', 'V207', 'V208', 'V209', 'V210', 'V212', 'V213', 'V214', 'V215', 'V216', 'V217', 'V219', 'V220',\n                   'V221', 'V222', 'V223', 'V224', 'V225', 'V226', 'V227', 'V228', 'V229', 'V231', 'V233', 'V234', 'V238', 'V239',\n                   'V242', 'V243', 'V244', 'V245', 'V246', 'V247', 'V249', 'V251', 'V253', 'V256', 'V257', 'V258', 'V259', 'V261',\n                   'V262', 'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V270', 'V271', 'V272', 'V273', 'V274', 'V275', 'V276',\n                   'V277', 'V278', 'V279', 'V280', 'V282', 'V283', 'V285', 'V287', 'V288', 'V289', 'V291', 'V292', 'V294', 'V303',\n                   'V304', 'V306', 'V307', 'V308', 'V310', 'V312', 'V313', 'V314', 'V315', 'V317', 'V322', 'V323', 'V324', 'V326',\n                   'V329', 'V331', 'V332', 'V333', 'V335', 'V336', 'V338', 'id_01', 'id_02', 'id_03', 'id_05', 'id_06', 'id_09',\n                   'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_17', 'id_19', 'id_20', 'id_30', 'id_31', 'id_32', 'id_33',\n                   'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo']","c75f88ae":"cols_to_drop = [col for col in train.columns if col not in useful_features]\ncols_to_drop.remove('isFraud')\ncols_to_drop.remove('TransactionID')\ncols_to_drop.remove('TransactionDT')","48afedee":"print('{} features are going to be dropped for being useless'.format(len(cols_to_drop)))\n\ntrain = train.drop(cols_to_drop, axis=1)\ntest = test.drop(cols_to_drop, axis=1)","5dfaa836":"# New feature - decimal part of the transaction amount\ntrain['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)\n\n# Count encoding for card1 feature. \n# Explained in this kernel: https:\/\/www.kaggle.com\/nroman\/eda-for-cis-fraud-detection\ntrain['card1_count_full'] = train['card1'].map(pd.concat([train['card1'], test['card1']], ignore_index=True).value_counts(dropna=False))\ntest['card1_count_full'] = test['card1'].map(pd.concat([train['card1'], test['card1']], ignore_index=True).value_counts(dropna=False))\n\n# https:\/\/www.kaggle.com\/fchmiel\/day-and-time-powerful-predictive-feature\ntrain['Transaction_day_of_week'] = np.floor((train['TransactionDT'] \/ (3600 * 24) - 1) % 7)\ntest['Transaction_day_of_week'] = np.floor((test['TransactionDT'] \/ (3600 * 24) - 1) % 7)\ntrain['Transaction_hour'] = np.floor(train['TransactionDT'] \/ 3600) % 24\ntest['Transaction_hour'] = np.floor(test['TransactionDT'] \/ 3600) % 24\n\n# Some arbitrary features interaction\nfor feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', \n                'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1']:\n\n\n    f1, f2 = feature.split('__')\n    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n\n    le = LabelEncoder()\n    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n    train[feature] = le.transform(list(train[feature].astype(str).values))\n    test[feature] = le.transform(list(test[feature].astype(str).values))\n    \nfor feature in ['id_34', 'id_36']:\n    if feature in useful_features:\n        # Count encoded for both train and test\n        train[feature + '_count_full'] = train[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n        test[feature + '_count_full'] = test[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n        \nfor feature in ['id_01', 'id_31', 'id_33', 'id_35', 'id_36']:\n    if feature in useful_features:\n        # Count encoded separately for train and test\n        train[feature + '_count_dist'] = train[feature].map(train[feature].value_counts(dropna=False))\n        test[feature + '_count_dist'] = test[feature].map(test[feature].value_counts(dropna=False))","ec1aafc8":"for col in tqdm_notebook(train.columns):\n    if train[col].dtype == 'object':\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values))   ","6e005051":"X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT', 'TransactionID'], axis=1)\ny = train.sort_values('TransactionDT')['isFraud']\ntest = test.sort_values('TransactionDT').drop(['TransactionDT', 'TransactionID'], axis=1)","30ae2081":"# params = {'num_leaves': 491,\n#           'min_child_weight': 0.03454472573214212,\n#           'feature_fraction': 0.3797454081646243,\n#           'bagging_fraction': 0.4181193142567742,\n#           'min_data_in_leaf': 106,\n#           'objective': 'binary',\n#           'max_depth': -1,\n#           'learning_rate': 0.006883242363721497,\n#           \"boosting_type\": \"gbdt\",\n#           \"bagging_seed\": 11,\n#           \"metric\": 'auc',\n#           \"verbosity\": -1,\n#           'reg_alpha': 0.3899927210061127,\n#           'reg_lambda': 0.6485237330340494,\n#           'random_state': 47\n#          }","18ffcf9d":"# folds = TimeSeriesSplit(n_splits=10)\n\n# aucs = list()\n# feature_importances = pd.DataFrame()\n# feature_importances['feature'] = X.columns\n\n# training_start_time = time()\n# for fold, (trn_idx, test_idx) in enumerate(folds.split(X, y)):\n#     start_time = time()\n#     print('Training on fold {}'.format(fold + 1))\n    \n#     trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n#     val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n#     clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds=500)\n    \n#     feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n#     aucs.append(clf.best_score['valid_1']['auc'])\n    \n#     print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n# print('-' * 30)\n# print('Training has finished.')\n# print('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\n# print('Mean AUC:', np.mean(aucs))\n# print('-' * 30)\n# baseline_err=np.mean(aucs)","e80eee77":"# Xdel = X.dropna()\n# Ydel = y.dropna()","8d9b0313":"# folds = TimeSeriesSplit(n_splits=10)\n\n# aucs = list()\n# feature_importances = pd.DataFrame()\n# feature_importances['feature'] = X.columns\n\n# training_start_time = time()\n# for fold, (trn_idx, test_idx) in enumerate(folds.split(Xdel, Ydel)):\n#     start_time = time()\n#     print('Training on fold {}'.format(fold + 1))\n    \n#     trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n#     val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n#     clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds=500)\n    \n#     feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n#     aucs.append(clf.best_score['valid_1']['auc'])\n    \n#     print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n# print('-' * 30)\n# print('Training has finished.')\n# print('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\n# print('Mean AUC:', np.mean(aucs))\n# print('-' * 30)\n# deletion_err=np.mean(aucs)","9dcd2be8":"# imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n# imp.fit(X)\n\n# Xmean = imp.transform(X)\n# Ymean = y\n","4984d2d8":"# folds = TimeSeriesSplit(n_splits=10)\n\n# aucs = list()\n# feature_importances = pd.DataFrame()\n# feature_importances['feature'] = X.columns\n\n# training_start_time = time()\n# for fold, (trn_idx, test_idx) in enumerate(folds.split(Xmean, Ymean)):\n#     start_time = time()\n#     print('Training on fold {}'.format(fold + 1))\n    \n#     trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n#     val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n#     clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds=500)\n    \n#     feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n#     aucs.append(clf.best_score['valid_1']['auc'])\n    \n#     print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n# print('-' * 30)\n# print('Training has finished.')\n# print('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\n# print('Mean AUC:', np.mean(aucs))\n# print('-' * 30)\n# mean_err=np.mean(aucs)","5fbf197f":"# !pip install impyute","97ec3cfb":"\n#  from impyute.imputation import *","614718ea":"# X_mice=impyute.imputation.cs.mice(X)","83413c3a":"# from fancyimpute import MICE","e00f1a40":"# Xmice = MICE(n_imputations=200, impute_type='col', verbose=False).complete(X.as_matrix())\n# Ymice = y","b78e7c90":"# folds = TimeSeriesSplit(n_splits=10)\n\n# aucs = list()\n# feature_importances = pd.DataFrame()\n# feature_importances['feature'] = X.columns\n\n# training_start_time = time()\n# for fold, (trn_idx, test_idx) in enumerate(folds.split(Xmice, Ymice)):\n#     start_time = time()\n#     print('Training on fold {}'.format(fold + 1))\n    \n#     trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n#     val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n#     clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds=500)\n    \n#     feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n#     aucs.append(clf.best_score['valid_1']['auc'])\n    \n#     print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n# print('-' * 30)\n# print('Training has finished.')\n# print('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\n# print('Mean AUC:', np.mean(aucs))\n# print('-' * 30)\n# mice_err=np.mean(aucs)","2c87fd1b":"# from fancyimpute import KNN\n\n\n# def standardize(s):\n#     return s.sub(s.min()).div((s.max() - s.min()))\n\n# Xnorm = X.apply(standardize, axis=0)\n# kvals = np.linspace(1, 100, 20, dtype='int64')\n\n# knn_errs = []\n# for k in kvals:\n#     knn_err = []\n#     Xknn = KNN(k=k, verbose=False).complete(Xnorm)\n#     knn_err = cross_val_score(RandomForestClassifier(n_estimators=1000,\n#                            max_depth=None,\n#                            min_samples_split=10), Xknn, Y, cv=10, n_jobs=-1).mean()\n\n#     knn_errs.append(knn_err)\n#     print(\"[KNN] Estimated RF Test Error (n = {}, k = {}, 10-fold CV): {}\".format(len(Xknn), k, np.mean(knn_err)))","88f8e83a":"# sns.set_style(\"darkgrid\")\n# _ = plt.plot(kvals, knn_errs)\n# _ = plt.xlabel('K')\n# _ = plt.ylabel('10-fold CV Error Rate')\n\n# knn_err = max(knn_errs)\n# k_opt = kvals[knn_errs.index(knn_err)]\n\n# Xknn = KNN(k=k_opt, verbose=False).complete(Xnorm)\n# Yknn = y\n\n# print(\"[BEST KNN] Estimated RF Test Error (n = {}, k = {}, 10-fold CV): {}\".format(len(Xknn), k_opt, np.mean(knn_err)))","f80ae0be":"# folds = TimeSeriesSplit(n_splits=10)\n\n# aucs = list()\n# feature_importances = pd.DataFrame()\n# feature_importances['feature'] = X.columns\n\n# training_start_time = time()\n# for fold, (trn_idx, test_idx) in enumerate(folds.split(Xknn, Yknn)):\n#     start_time = time()\n#     print('Training on fold {}'.format(fold + 1))\n    \n#     trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n#     val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n#     clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds=500)\n    \n#     feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n#     aucs.append(clf.best_score['valid_1']['auc'])\n    \n#     print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n# print('-' * 30)\n# print('Training has finished.')\n# print('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\n# print('Mean AUC:', np.mean(aucs))\n# print('-' * 30)\n# knn_err=np.mean(aucs)","3c9e223b":"# X_em=impyute.imputation.cs.em(X, loops=50)","281ac63f":"# folds = TimeSeriesSplit(n_splits=10)\n\n# aucs = list()\n# feature_importances = pd.DataFrame()\n# feature_importances['feature'] = X.columns\n\n# training_start_time = time()\n# for fold, (trn_idx, test_idx) in enumerate(folds.split(X_em, y)):\n#     start_time = time()\n#     print('Training on fold {}'.format(fold + 1))\n    \n#     trn_data = lgb.Dataset(X.iloc[trn_idx], label=y.iloc[trn_idx])\n#     val_data = lgb.Dataset(X.iloc[test_idx], label=y.iloc[test_idx])\n#     clf = lgb.train(params, trn_data, 10000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds=500)\n    \n#     feature_importances['fold_{}'.format(fold + 1)] = clf.feature_importance()\n#     aucs.append(clf.best_score['valid_1']['auc'])\n    \n#     print('Fold {} finished in {}'.format(fold + 1, str(datetime.timedelta(seconds=time() - start_time))))\n# print('-' * 30)\n# print('Training has finished.')\n# print('Total training time is {}'.format(str(datetime.timedelta(seconds=time() - training_start_time))))\n# print('Mean AUC:', np.mean(aucs))\n# print('-' * 30)\n# em_err=np.mean(aucs)","0dba0bb0":"# impyute.imputation.ts.moving_window(X, nindex=None, wsize=50000, errors='coerce', inplace=False)\n","ec3cf735":"\n# X_barycentric=X.interpolate(method=\"barycentric\", axis=0, inplace=False)","5866beb7":"# X_linear=X.interpolate(method='linear', axis=0, limit=None, inplace=False, limit_direction=\"forward\")\n","ff7ce707":"# X_interp_spline=X.interpolate(method='spline', axis=0, limit=None, inplace=False, limit_direction=\"forward\")\n","ec2d80f6":"\nX_soft=SoftImpute().fit_transform(Xnorm)","96d9e09a":"**8. Moving window imputation**","48502f87":"**10. SoftImpute** needs normalised data as knn","7fd8e119":"Change you own arguments and test it quickly, just rely on you CV scheme ;)","465b79da":"**1. Baseline**","aa47f59b":"check the score:","c53d8c65":"**7. Expectation minimazation**","c696aac7":"# NOTE:\nI do not use this cv scheme locally!","0c92d716":"**9. [DataFrame impute with different arguments:](https:\/\/www.geeksforgeeks.org\/python-pandas-dataframe-interpolate\/) Things that I saw it works:\n**","d93627ad":"**2. Deletion**","0c4d7a6b":"**3. Mean Substitution**","c7f7f809":"**Alternative from fancy impute or sklearn *IterativeImputer*!**","994ecef1":"# Why should you care?\n\n1. It **increases** baseline score of LGBM\n\nI was suprised too. LGBM and other boosting methods do handle missing values efficently (LGBM removes them and then imputes them with values that minimize the loss.)  In my case it was KNN with optimised k that beat the baseline\n\n2. To solve imbalance problems, apply NN etc etc one should take care of missing values effectively, hence we do need to impute efficiently.\n\n\nProblems: Computational time, our data set is huuuge, 500k rows and for most of them executing the script on the server is not feasible. Do it locally, and you will see difference in CV between baseline and some methods.\n\nThings I tried:\n1. deletion\n2. mean \n3. MICE\n4. knn (different optimal k)\n5. Softimpute\n6. Expectation minimization\n7. moving window\n8. Spline, barycentric etc imputation of df columns.\n\n\n","6c4572b1":"NOTE","ab313f2c":"**4. MICE**","3acae99b":"**6. KNN- first find optimal number of k then substitute**"}}