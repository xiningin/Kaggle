{"cell_type":{"ebfb3fd9":"code","9a1cd03d":"code","bedd3afd":"code","e1c5c585":"code","580cb93b":"code","79394b27":"code","71ff2bd4":"code","38b1d4b2":"code","92ed305a":"code","77ba4140":"code","583046aa":"code","3ac2b5d6":"code","eec46df6":"code","ed78e1a3":"code","33e346e7":"code","43bcf1dc":"code","b15a04db":"code","f0083a07":"code","2e4884e3":"code","4dec1963":"code","b3e04e75":"code","7b571367":"code","06084957":"code","3421ac10":"code","a7ccf164":"code","f6fa5f9e":"code","2562bd3b":"code","9b487d11":"code","84ed371e":"code","96cc9de2":"code","0401264c":"code","dc33820f":"code","c17d5d62":"code","5e6464e7":"code","5aa13bf6":"code","32d3d8c8":"code","98860a70":"code","e7b7b0ae":"code","40604819":"code","5299e5a3":"code","6aa461b9":"code","3dbef410":"code","d52692d4":"code","d31feb95":"code","a44752c2":"code","7be7cff8":"code","ac14f8c2":"code","64472cd7":"code","2aa221a6":"code","1e4db68f":"code","488e3dd1":"code","8eca926a":"code","f610280a":"code","1d7f09cd":"code","a0e061a8":"code","f9d79b45":"code","fe2bfdca":"code","11672f98":"code","c8394f13":"code","67706c79":"code","5bd95829":"code","10a22462":"markdown","2109815c":"markdown","554b211f":"markdown","7246bbdb":"markdown","6372154b":"markdown","bdc991b2":"markdown","3213f5c8":"markdown","916abf87":"markdown","8f3212e3":"markdown","9b44b479":"markdown","f2e56a1c":"markdown","6bb66c06":"markdown","8b4f5a99":"markdown","c0eccf2c":"markdown","004bfa1f":"markdown","de384753":"markdown","15279c4d":"markdown","c095f470":"markdown","3b415c9e":"markdown","4c86d62e":"markdown","01b20c96":"markdown","631dfcc3":"markdown","9901296a":"markdown","e6f6e064":"markdown","b1c2d8f5":"markdown","d89012e5":"markdown","c1c7c408":"markdown","d5e3f235":"markdown","8586ff85":"markdown","1be5a05c":"markdown","757e3a77":"markdown","9f638d1e":"markdown","a9a5e4ee":"markdown","bd94b69e":"markdown","93f33241":"markdown","1abbb34c":"markdown","2d158665":"markdown","9be69bb6":"markdown","1dba80b0":"markdown","314c51be":"markdown"},"source":{"ebfb3fd9":"#Importing required modules\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime\nimport time\nplt.style.use('ggplot')\n\n# ignore warnings \ndef warn(*args, **kwargs):\n    pass\nimport warnings\nwarnings.warn = warn\n\n#to include matplotlib graphs within the notebook, next to the code\n%matplotlib inline  ","9a1cd03d":"#Loading the dataset\ndf = pd.read_csv('..\/input\/bank-customer-dataset\/data.csv')","bedd3afd":"df.head()","e1c5c585":"df.dtypes","580cb93b":"obj_columns = ['job', 'marital', 'education', 'housing', 'loan', 'poutcome']\ndf[obj_columns].describe()","79394b27":"plt.figure(figsize=(20,10))\nfor i, col in enumerate(obj_columns):\n    ax = plt.subplot(2,3,i+1)\n    ax = df[col].value_counts().plot(kind='bar')\n    ax.set_title(col)\nplt.subplots_adjust(hspace=1)\nplt.show()","71ff2bd4":"#Selecting binary variables from category dict, since we do not want to apply One Hot encoding on those.\n#These can directly be transformed into binary categories\nbinary_cols = df.columns[df.nunique() == 2]\nbinary_cols","38b1d4b2":"#Non-binary categorical columns for one hot encoding\ncat_columns = ['job', 'marital', 'education', 'housing', 'loan', 'poutcome']\ndf = pd.get_dummies(df, prefix_sep=\"_\",columns=cat_columns)","92ed305a":"df[binary_cols] = df[binary_cols].apply(lambda x: pd.factorize(x)[0])","77ba4140":"#First checking the month  and day_of_week columns for null values\nprint(df.month.isnull().sum())\nprint(df.day_of_week.isnull().sum())","583046aa":"#Since the conversion to datetime does not support null values. We remove the row with a null month value\ndf = df.dropna(axis=0, subset=['month'])","3ac2b5d6":"#We now convert month and day_of_week to datetime format and extract their respective numerical values and assign\n#these to the columns\n\nmonth = []\nweekday = []\n\nfor m, wd in zip(df.month, df.day_of_week):\n    \n    #Extracting month number\n    mth = datetime.strptime(m, '%b')\n    month.append(mth.strftime('%m')) #Appending to list\n    \n    #Extracting weekday number starting Monday = 0\n    wkdy = time.strptime(wd, \"%a\")\n    weekday.append(wkdy.tm_wday) #Appending to list\n","eec46df6":"df['month'] = month\ndf['day_of_week'] = weekday ","ed78e1a3":"#Converting 'month' and 'day_of_week' to int type\ndf['month'] = df['month'].astype('int')\ndf['day_of_week'] = df['day_of_week'].astype('int')","33e346e7":"#Dataset with transformed variables\ndf.head()","43bcf1dc":"#The row containing null value in 'month' column was already removed earlier\n\n#Checking for null values in other columns\ndf.isnull().sum()","b15a04db":"#Replacing missing values with median\ndf = df.replace(np.nan,df.median())","f0083a07":"df.isnull().sum()","2e4884e3":"#Continuous\/float variables\ndf.columns[df.dtypes == 'float']","4dec1963":"df['age'] = df['age'].astype('int')\ndf['pdays'] = df['pdays'].astype('int')\ndf['previous'] = df['previous'].astype('int')","b3e04e75":"#Checking again for the remaining float variables\ndf.columns[df.dtypes == 'float']","7b571367":"#Analysis of the continuous variables\ndf[['duration', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx',\n       'euribor3m', 'nr.employed']].describe()","06084957":"#Plots to verify the above impression\n\nfig, axs = plt.subplots(2,3, figsize=(20,8))\nsns.boxplot(x= 'duration', data=df, ax=axs[0,0])\nsns.boxplot(x= 'emp.var.rate', data=df, ax=axs[0,1])\nsns.boxplot(x= 'cons.price.idx', data=df, ax=axs[0,2])\nsns.boxplot(x= 'cons.conf.idx', data=df, ax=axs[1,0])\nsns.boxplot(x= 'euribor3m', data=df, ax=axs[1,1])\nsns.boxplot(x= 'nr.employed', data=df, ax=axs[1,2])\nfig.show()","3421ac10":"#Plot for 'duration'\nax = df.duration.plot(style = 'o')\nax.set_ylabel('Duration')\nplt.show()","a7ccf164":"#Deciding the Z-score threshold for outlier removal\n\nthresh_list = [2,2.5,3]\nprint(np.multiply(thresh_list, df['duration'].std()))","f6fa5f9e":"#Checking for outliers\n\nfrom scipy import stats\n\nthresh = 3\n\ndf_to_clean = df[['duration', 'emp.var.rate', 'cons.price.idx', 'cons.conf.idx',\n       'euribor3m', 'nr.employed']]\n\n# 1. Select a threshold for a Z-score to identify and remove outliers\ndf_Z = df_to_clean[(np.abs(stats.zscore(df_to_clean)) < thresh).all(axis=1)]\nix_keep = df_Z.index\n\n# 2. Subset the raw dataframe with the indexes you'd like to keep\ndf_without_outlier = df.loc[ix_keep]","2562bd3b":"print('Number of outliers removed: ',df.shape[0] - df_without_outlier.shape[0])","9b487d11":"#Descriptive stats of 'duration' after outlier removal\ndf_without_outlier[['duration']].describe()","84ed371e":"#Modules required for training a classifier\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nfrom hyperopt import hp, tpe, fmin, space_eval","96cc9de2":"X = df_without_outlier.drop(['Unnamed: 0','y'],axis=1)\ny = df_without_outlier['y']","0401264c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2018)","dc33820f":"def StratifiedKFold_dtree(X, y, params):\n    '''\n    CV for hyperopt \n    '''\n    skf = StratifiedKFold(n_splits=5, shuffle=True)\n    result = []\n    # Loop through the indices the split() method returns\n    for index, (train_indices, val_indices) in enumerate(skf.split(X, y)):\n        # Generate batches from indices\n        xtrain, xtest = X.values[train_indices], X.values[val_indices]\n        ytrain, ytest = y.values[train_indices], y.values[val_indices]\n        # Create the Decision Tree model\n        model = tree.DecisionTreeClassifier(**params)\n        model.fit(xtrain, ytrain)\n        y_hat = model.predict(xtest)\n        score = accuracy_score(ytest, y_hat)\n        result.append(score)\n    return np.mean(result)","c17d5d62":"def objective(params):\n    \"\"\"\n    Objective function to minimize\n    \"\"\"\n    return -StratifiedKFold_dtree(X,y, params) #Maximize D-Tree accuracy score","5e6464e7":"#Decision Tree Parameter Space\nfrom hyperopt.pyll.base import scope\n\ndtree_space = {\n    'max_depth': hp.quniform('max_depth', 1, 10, 1),\n    \n    'min_samples_split': hp.choice('min_samples_split', np.arange(2, 10, 1, dtype=int) ),\n    \n    'criterion': hp.choice('criterion', ('entropy',\n                                         'gini',)),\n    'random_state': 42,\n}","5aa13bf6":"dtree_best = fmin(objective, space=dtree_space, algo=tpe.suggest, max_evals=50)\ndtree_params_from_hyperopt = space_eval(dtree_space, dtree_best)\nprint('Best Decision Tree Hyperparameters:\\n', dtree_params_from_hyperopt)","32d3d8c8":"#Stratified K-Fold Cross Validation Accuracy Score\nmean_acc_dtree = StratifiedKFold_dtree(X, y, dtree_params_from_hyperopt)\nprint('Stratified K-Fold Cross Validation Mean Accuracy:\\n',mean_acc_dtree)","98860a70":"def StratifiedKFold_randFor(X, y, params):\n    '''\n    CV for hyperopt \n    '''\n    skf = StratifiedKFold(n_splits=5, shuffle=True)\n    result = []\n    # Loop through the indices the split() method returns\n    for index, (train_indices, val_indices) in enumerate(skf.split(X, y)):\n        # Generate batches from indices\n        xtrain, xtest = X.values[train_indices], X.values[val_indices]\n        ytrain, ytest = y.values[train_indices], y.values[val_indices]\n        # Create the Random Forest model\n        model = RandomForestClassifier(**params)\n        model.fit(xtrain, ytrain)\n        y_hat = model.predict(xtest)\n        score = accuracy_score(ytest, y_hat)\n        result.append(score)\n    return np.mean(result)","e7b7b0ae":"def objective(params):\n    \"\"\"\n    Objective function to minimize\n    \"\"\"\n    return -StratifiedKFold_randFor(X,y, params) #Maximize Random Forest accuracy score","40604819":"#Random Forest Parameter Space\n#All parameters except n_estimators have the same values as D-Tree parameters (for accuracy comparison)\n\nfrom hyperopt.pyll.base import scope\n\nrf_space = {\n    'max_depth': hp.quniform('max_depth', 1, 10, 1),\n    \n    'n_estimators': scope.int(hp.quniform('n_estimators', 10,20,1)), #max 20 estimators because of runtime concerns\n    \n    'min_samples_split': hp.choice('min_samples_split', np.arange(2, 10, 1, dtype=int) ),\n    \n    'criterion': hp.choice('criterion', ('entropy',\n                                         'gini',)),\n    'random_state': 42,\n}","5299e5a3":"rf_best = fmin(objective, space=rf_space, algo=tpe.suggest, max_evals=50)\nrf_params_from_hyperopt = space_eval(rf_space, rf_best)\nprint('Best Random Forest Hyperparameters:\\n', rf_params_from_hyperopt)","6aa461b9":"#Stratified K-Fold Cross Validation Accuracy Score \nmean_acc_randFor = StratifiedKFold_randFor(X, y, rf_params_from_hyperopt)\nprint('Stratified K-Fold Cross Validation Mean Accuracy:\\n',mean_acc_randFor)","3dbef410":"dtree = tree.DecisionTreeClassifier(**dtree_params_from_hyperopt)\ndtree.fit(X_train, y_train)\ny_hat_dtree = dtree.predict(X_test)","d52692d4":"dtree_test_acc =  accuracy_score(y_test,y_hat_dtree)\nprint('Accuracy of Decision Tree Model on Test Set:\\n', dtree_test_acc)","d31feb95":"rf = RandomForestClassifier(**rf_params_from_hyperopt)\nrf.fit(X_train, y_train)\ny_hat_rf = rf.predict(X_test)","a44752c2":"rf_test_acc = accuracy_score(y_test,y_hat_rf)\nprint('Accuracy of Random Forest Model on Test Set:\\n', rf_test_acc)","7be7cff8":"print('RESULT SUMMARY:\\n\\n')\nprint('1. Decision Tree Model:\\n')\nprint('Cross Validation Mean Accuracy: %f \\n'%mean_acc_dtree)\nprint('Accuracy on Test Set: %f \\n\\n'%dtree_test_acc)\nprint('2. Random Forest Model:\\n')\nprint('Cross Validation Mean Accuracy: %f\\n' %mean_acc_randFor)\nprint('Accuracy on Test Set: %f' %rf_test_acc)","ac14f8c2":"#One hot encoding of 'month' and 'day_of_week'\ndf_one_hot = df_without_outlier\ndf_one_hot[['month','day_of_week']]= df_one_hot[['month','day_of_week']].apply(lambda x: pd.factorize(x)[0])","64472cd7":"X_oh = df_one_hot.drop(['Unnamed: 0','y'],axis=1)\ny_oh = df_one_hot['y']","2aa221a6":"def objective(params):\n    \"\"\"\n    Objective function to minimize\n    \"\"\"\n    return -StratifiedKFold_randFor(X_oh,y_oh, params) \n#Same Stratified CV function as before is called with dataset with one hot endcoded 'month' and 'day_of_week' variables","1e4db68f":"rf_oh_best = fmin(objective, space=rf_space, algo=tpe.suggest, max_evals=50) #Same parameter space used as before\nrf_oh_params_from_hyperopt = space_eval(rf_space, rf_oh_best)\nprint('Best Random Forest Hyperparameters:\\n', rf_oh_params_from_hyperopt)","488e3dd1":"#Comparing CV Mean Accuracy Scores for both cases\nmean_acc_rf_one_hot = StratifiedKFold_randFor(X_oh, y_oh, rf_oh_params_from_hyperopt)\nprint('Cross Validation Mean Accuracy with one hot encoding categorical variables AND month, day_of_week: %f' %mean_acc_rf_one_hot)\nprint('Cross Validation Mean Accuracy with one hot encoding categorical variables WITHOUT month, day_of_week: %f' %mean_acc_randFor)","8eca926a":"df_standardized = df_without_outlier\ndf_standardized.loc[:, df_standardized.columns != 'y'] =  preprocessing.scale(df_standardized.loc[:, df_standardized.columns != 'y'])\n","f610280a":"X_st = df_standardized.drop(['Unnamed: 0','y'],axis=1)\ny_st = df_standardized['y']","1d7f09cd":"def objective(params):\n    \"\"\"\n    Objective function to minimize\n    \"\"\"\n    return -StratifiedKFold_randFor(X_st,y_st, params)\n#Same Stratified CV function as before is called with standardised dataset","a0e061a8":"rf_st_best = fmin(objective, space=rf_space, algo=tpe.suggest, max_evals=50) #Same parameter space used as before\nrf_st_params_from_hyperopt = space_eval(rf_space, rf_st_best)\nprint('Best Random Forest Hyperparameters:\\n', rf_st_params_from_hyperopt)","f9d79b45":"#Comparing CV Mean Accuracy Scores for both cases: standardized vs unstandardized\nmean_acc_rf_st = StratifiedKFold_randFor(X_st, y_st, rf_st_params_from_hyperopt)\nprint('Cross Validation Mean Accuracy with standardized dataset: %f' %mean_acc_rf_st)\nprint('Cross Validation Mean Accuracy with unscaled dataset: %f'%mean_acc_randFor)","fe2bfdca":"df_normalized = df_without_outlier[:]\ndf_normalized.loc[:, df_normalized.columns != 'y'] = preprocessing.normalize(df_normalized.loc[:, df_normalized.columns != 'y'])\n","11672f98":"X_norm = df_standardized.drop(['Unnamed: 0','y'],axis=1)\ny_norm = df_standardized['y']","c8394f13":"def objective(params):\n    \"\"\"\n    Objective function to minimize\n    \"\"\"\n    return -StratifiedKFold_randFor(X_norm,y_norm, params)\n#Same Stratified CV function as before is called with normalised dataset","67706c79":"rf_norm_best = fmin(objective, space=rf_space, algo=tpe.suggest, max_evals=50) #Same parameter space used as before\nrf_norm_params_from_hyperopt = space_eval(rf_space, rf_norm_best)\nprint('Best Random Forest Hyperparameters:\\n', rf_norm_params_from_hyperopt)","5bd95829":"#Comparing CV Mean Accuracy Scores for both cases: normalized vs unnormalized\nmean_acc_rf_norm = StratifiedKFold_randFor(X_norm, y_norm, rf_norm_params_from_hyperopt)\nprint('Cross Validation Mean Accuracy with normalized dataset: %f' %mean_acc_rf_norm)\nprint('Cross Validation Mean Accuracy with unscaled dataset: %f'%mean_acc_randFor)","10a22462":"***\n# Discussion\n\nIn this section we look at some more pre-processing aproaches tried on the data and their effect on the result and consequently why they were not applied to the data finally used for modelling.\n\n### 1. One Hot Encoding of 'month' , 'day_of_week'","2109815c":"### 2. Random Forests<br>\n\nIt is a meta estimator that fits a number of decision tree classifiers on various sub-samples of the dataset and uses averaging to improve the predictive accuracy and control over-fitting. The n_estimator parameter specifies the number of trees to fit. Keeping all the common hyperparameters of both classifiers same, Random Forest classifier usually delivers a higher accuracy over decision tress.","554b211f":"**As these results show, the above pre-processed approaches did not yield better results. Standardized data sometimes fared marginally better on mean accuracy. But since the improvement was only marginal, it was not considered in order to save on runtime. However, in situations where even the most marginal improvement can add value, standardization could be an option.**","7246bbdb":"## Aim\nDeveloping a machine learning model for predicting if a customer will subscribe to a term deposit. For this an adapted dataset about bank marketing statistics is used. More about the original dataset [here](https:\/\/archive.ics.uci.edu\/ml\/datasets\/bank+marketing)\n\n### Importing modules and load data","6372154b":"#### 2.2 Normalization","bdc991b2":"## Outlier Removal","3213f5c8":"### Partitioning the Dataset\n\nPartitioning the dataset into a training and test set. The test set is 25% of the original dataset","916abf87":"#### Using One-Hot encoded variables for hyperparameter selection for Random Forest Model","8f3212e3":"## Random Forest Model","9b44b479":"## Decision Tree Model","f2e56a1c":"#### Hyperparameter Selection","6bb66c06":"As mentioned in Section 2, the random forest model when applied to the dataset where the variables 'month' and 'day_of_week' are also one-hot encoded (along with other categorical variables) does not perform well compared to the previous case where only the categorical variables are one hot encoded. This could be due to the 13 extra one hot variables that get added to the dataset after one hot encoding the month and day_of_week variables. This could have caused overfitting and worsened the model performance.","8b4f5a99":"### One Hot Encoding of (Non-Binary) Categorical Text Variables","c0eccf2c":"### Observation 2\n\nThe variables 'month', 'day_of_week' can be converted to numerical values by converting these to datetime format and then getting the corresponding numerical values. This allows datetime operations or time based analysis to be performed on these variables.\n\n**Note:** The initial idea was to one hot encode the **month** and **day_of_week** variables as well along with the above categorical variables. However, when testing the performance of the classifier trained on the dataset **with** one hot encoded month and day_of_week vs that trained on the dataset **without** one hot encode the month and day_of_week, the second one performed better. This is because the first case adds 13 extra features to the dataset. Too many features can lead to overfitting. This is discussed at the end of Section 2.","004bfa1f":"**As evident from the Sratified K-Fold cross validation above, the Random Forest Classifier performed better than Decision Trees. We further verify these results, by training both the classifier models with their selected hyperparameters on the training set partitioned in Section 3.1 and apply them to the corresponding test set.**","de384753":"### Choosing a Machine Learning Model<br>\n\nIt is best to try out different classifier models on the given dataset and then select the one with best classification accuracy. Since the dataset contains both continous and categorical variables, tree-based models like Decision Tree and Random Forest are a good choice as they are robust in such cases. \n\nThe strategy applied here is to select the hyperparameters for both classifiers using Stratified K-Fold Cross Validation and Bayesian Optimization. Then training both classifiers using their selected hyperparameters on the training data and testing their accuracy on the test data to choose the classifier with the maximum accuracy.","15279c4d":"### Factorizing the Binary Text Variables","c095f470":"As mentioned earlier the variables age, pdays, previous are integers with the wrong type i.e. float. We now convert these to int type","3b415c9e":"### Z-score for Outlier Removal\nWe need to decide the Z-Score for removing the outliers. This is basically the number of standard deviations. The points lying outside this number are considered outliers and are removed from the dataset.","4c86d62e":"The above table shows that there are outliers in the 'duration' column because the difference between the inter-quartile range (values between Q1and Q3) and the maximum value (4199.0) is extremely large.\n\nThe rest of the continuous variables appear to be without outliers. However it is practical to visualize the data to verify this hypothesis.","01b20c96":"#### The first step is to assign the correct type to variables","631dfcc3":"Looking at the plot above and the different Z-score thresholds, the threshold of 3 looks reasonable.","9901296a":"## 2. Data Cleaning & Pre-processing","e6f6e064":"No more missing values now","b1c2d8f5":"#### Converting *month* and *day_of_week* to numeric values","d89012e5":"#### 2.1 Standardization","c1c7c408":"### Observation 3\n\nThe variables age, pdays, previous are currently float type. These are however integer variables and don't assume decimal values. They should be therefore converted to int type. However to do that we have to first take care of the null values in these columns in the next section (2.2). Then we can convert these variables to int type","d5e3f235":"----\n## 1. Data Visualisation","8586ff85":"### 1. Decision Trees","1be5a05c":"## Handling Missing Values","757e3a77":"----\n## 3. Training of a Machine Learning Classifier","9f638d1e":"Many columns such as 'job', 'marital', 'education', 'housing' etc. have finite string values. Let's get a better  overview of these columns.","a9a5e4ee":"#### Hyperparameter Selection","bd94b69e":"We can now look at the outliers in the remaining float variables","93f33241":"**As evident from the above results, Random Forest outperforms Decision Tree in both cases**","1abbb34c":"We can impute the missing values in rest of the colmns by replacing them with the median values of the respective column. Median values remain unaffected by the outliers in the data and hence are more reliable than using mean. \n\nAlso if a variable has many missing values then imputing these with the valriable mode will only make the data more skewed towards the mode. Hence it is best to use the median.","2d158665":"***\n### 2. Rescaling of Dataset<br>\nBoth Standardization and Normalization of the dataset lowered the random forest model accuracy in most cases in comparison to unstandardized dataset. My hypothesis for this is that after one hot encoding of the categorical variables, the continuous features get far outnumbered by the added binary features in the dataset and rescaling of these few continuous features probably skews the model even more towards the binary features.","9be69bb6":"# Predicting Subscription to Term Deposit\nNotebook by Garima Mittal","1dba80b0":"### Observation 1\n\nThe string variables 'job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'poutcome', 'y' in the given dataset take on a limited and fixed number of possible values. These can therefore be converted to categorical type. This has the following advantages:\n- Storing variables as numerical categories saves memory in comparison to storing long strings.<br>\n- It allows application of suitable statistical methods and plotting techniques.\n\nHowever, some ML algorithms interpret numerical data based on its numerical value. This might lead to misclassification in case of categorical numerical variables where the numerical values follow no order of precedence. It is therefore best to transform these variables using One Hot Encoding where each column represents a distinct value of the given variable.","314c51be":"**NOTE: Rescaling of Data**\n\nThe initial thought was to rescale the data before training the classifier. However, both Standardization and Normalization lowered the accuracy of the classifier in comparison to the unscaled dataset. This is discussed at the end of Section 3.\n"}}