{"cell_type":{"aedad969":"code","7d60bffb":"code","52ff28c5":"code","090b5e31":"code","b6ccd41e":"code","e319aa6a":"code","747c6f47":"code","42cc8831":"code","17a2a189":"code","f33a788b":"code","70a2a805":"code","9462b12c":"code","1f51b324":"code","ed9936b8":"code","265279fc":"code","d61af8b5":"code","e2332d63":"code","57318835":"code","a5f9ce20":"code","673e4a34":"code","765430f4":"code","0f151403":"code","73c00a7a":"code","0fe7b0c7":"code","f03e174c":"code","436e7c71":"code","41b3fbcb":"code","47b7a2d8":"markdown","25ab8670":"markdown","0ed7d734":"markdown","0847bb0a":"markdown","f7aa17f6":"markdown","dc12d905":"markdown","b742f68f":"markdown","7f3bce1c":"markdown","feb5332e":"markdown","f3c2b7ad":"markdown","b7ec6bc9":"markdown","2d2f0af5":"markdown","e2bc04af":"markdown","d1ea22ec":"markdown","b7c621e2":"markdown","1ad32564":"markdown","640d1331":"markdown","fef2264f":"markdown","032fc5fa":"markdown","1c53cc5b":"markdown","f823a5e4":"markdown","429742ae":"markdown","11b3db01":"markdown","ba3833e8":"markdown","8c79c9f0":"markdown","b45452cf":"markdown"},"source":{"aedad969":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.utils import shuffle\nfrom sklearn.model_selection import cross_val_score\nfrom tensorflow import keras\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping","7d60bffb":"train_data_path = '..\/input\/fashionmnist\/fashion-mnist_test.csv'\ntest_data_path = '..\/input\/fashionmnist\/fashion-mnist_test.csv'\n#Convertimos los datos a float\ntrain_data = pd.read_csv(train_data_path, dtype=np.float)\ntest_data = pd.read_csv(test_data_path, dtype=np.float)","52ff28c5":"#Separamos los diferentes conjuntos de datos\n \n# 1 Condici\u00f3n\n\nnine_train = train_data[train_data[\"label\"] !=5]\nnine_test= test_data[test_data[\"label\"] != 5]\n\neight_train = nine_train[nine_train[\"label\"] !=6]\neight_test = nine_test[nine_test[\"label\"] !=6]\n\n#Aplicaci\u00f3n TL\n\nfive_train = train_data[train_data[\"label\"]==5]\nfive_test = test_data[test_data[\"label\"]==5]\nsix_train = train_data[train_data[\"label\"]==6]\nsix_test = test_data[test_data[\"label\"]==6]\n\ntrain_frames=[five_train, six_train]\ntest_frames=[five_test, six_test]\n\ntwo_train = pd.concat(train_frames)\ntwo_test = pd.concat(test_frames)\n\n#Segunda aplicaci\u00f3n TL\nfull_train = train_data\nfull_test = test_data","090b5e31":"def info (arg):\n    return print(\"Shape y unique\",arg.shape,arg['label'].unique(),\"\\n\")\n\n#Pre\ninfo(eight_train)\ninfo(eight_test)\n\n#TL\ninfo(two_train)\ninfo(two_test)\n\n#2TL\ninfo(full_train)\ninfo(full_test)","b6ccd41e":"#Barajamos los datos y los dividimos en los diferentes sets para el modelo\n\ndef barajar (arg):\n    return shuffle(arg)\n\neight_train = barajar(eight_train)\neight_test = barajar(eight_test)\n\ntwo_train = barajar(two_train)\ntwo_test = barajar(two_test)\n\nfull_train = barajar(full_train)\nfull_test = barajar(full_test)","e319aa6a":"#Split\n\ndef split_data(x,y):\n    return x.drop('label', 1)\/255, y['label']\n\n\nX_train_8, y_train_8 = split_data(eight_train,eight_train)\nX_test_8, y_test_8 = split_data(eight_test,eight_test)\n\nX_train_2,y_train_2 =split_data(two_train,two_train)\nX_test_2, y_test_2 = split_data(two_test,two_test)\n\nX_train_full,y_train_full=split_data(full_train,full_train)\nX_test_full, y_test_full = split_data(full_test,full_test)","747c6f47":"X_train_8 = np.array([i.reshape(28,28,1) for i in X_train_8.values])\nX_test_8 = np.array([i.reshape(28,28,1) for i in X_test_8.values])\nX_train_2 = np.array([i.reshape(28,28,1) for i in X_train_2.values])\nX_test_2 = np.array([i.reshape(28,28,1) for i in X_test_2.values])\nX_train_full = np.array([i.reshape(28,28,1) for i in X_train_full.values])\nX_test_full = np.array([i.reshape(28,28,1) for i in X_test_full.values])","42cc8831":"def print_trshape (arg):\n    return print(\"Shape de tensor TRAINING: \",arg.shape)\ndef print_teshape (arg):\n    return print(\"Shape de tensor TESTING: \", arg.shape,\"\\n\")\n\nprint_trshape(X_train_8)\nprint_teshape(X_test_8)\nprint_trshape(X_train_2)\nprint_teshape(X_test_2)\nprint_trshape(X_train_full)\nprint_teshape(X_test_full)","17a2a189":"class_names = [\"T-shirt\/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\n              \"Shirt\",\"Sneaker\",\"Bag\",\"Ankle boot\"]\n\nnumero_imagenes = 10\n\ndef plot_pics(arg1, arg2):\n    fig, axs = plt.subplots(nrows=1, ncols=10, figsize=(16, 8))\n    for i in range(numero_imagenes):\n        axs[i].imshow(tf.reshape(arg1[i], [28, 28])*255,vmin=0, vmax=255,cmap='gray')\n        axs[i].title.set_text(str(class_names[int(arg2.values[i])]))\n\nplot_pics(X_train_8, y_train_8)\nplot_pics(X_train_2,y_train_2)\nplot_pics(X_train_full,y_train_full)","f33a788b":"#8 Cat\n\ndic_8={0:0, 1.0:1, 2.0:2, 3.0:3, 4.0:4, 7.0:5, 8.0:6, 9.0:7}\ndef dic_ev8(a):\n    return dic_8[a]\n\ny_train_8 = y_train_8.apply(dic_ev8)\ny_test_8 = y_test_8.apply(dic_ev8)","70a2a805":"#2 Cat\n\ndic_2={5.0:0, 6.0:1}\ndef dic_ev2(a):\n    return dic_2[a]\n\ny_train_2 = y_train_2.apply(dic_ev2)\ny_test_2 = y_test_2.apply(dic_ev2)","9462b12c":"def token_create(arg):\n    return np.array([np.eye(1,size, int(i))[0] for i in arg.values])\n\n\n#Creamos un token para cada Training y Test de Y\nsize=8\ny_train_8 =token_create(y_train_8)\ny_test_8 =token_create(y_test_8)\nsize=2\ny_train_2 =token_create(y_train_2)\ny_test_2 =token_create(y_test_2)\nsize=10\ny_train_full =token_create(y_train_full)\ny_test_full =token_create(y_test_full)","1f51b324":"def forma(arg):\n    return print(arg.shape)\n\nforma(y_train_8)\nforma(y_test_8)\nforma(y_train_2)\nforma(y_test_2)\nforma(y_train_full)\nforma(y_test_full)","ed9936b8":"print(y_train_8[5])\n\nplt.imshow(X_train_8[5])","265279fc":"#8 Categor\u00edas\n#AlexNet Architecture:\n\n#Input Layer\n\nmodel_input = Input(shape=(28,28,1))\n\n#Block 1\n\nx = Conv2D(filters=32, kernel_size=(3,3), strides=(2,2), activation='relu', padding= 'same')(model_input)\nx = Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding=\"same\")(x)\nx = BatchNormalization()(x)\nx = MaxPool2D((3,3), strides=(2,2))(x)\n\n#Block 3\n\nx = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(x)\nx = Conv2D(filters=64, kernel_size=(3,3), strides=(1,1), activation='relu', padding=\"same\")(x)\nx = BatchNormalization()(x)\nx = MaxPool2D((2,2), strides=(2,2))(x)\n\n#Block 4\n\nflat = Flatten()(x)\ny = Dense(512, activation='relu')(flat)\ny = Dropout(0.3)(y)\ny = Dense(128, activation='relu')(y)\ny = Dense(128, activation='relu')(y)\ny = Dropout(0.3)(y)\ntrainable = Dense(128, activation='relu')(y)\n\n# Output Layer\n\noutput1 = Dense(8, activation='softmax')(trainable)\n\nmodel = Model(inputs = model_input, outputs = output1)\n\nprint(model.summary())","d61af8b5":"#Compilamos el modelo\nmodel.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n\n\nprint(\"weights : \",len(model.weights))\nprint(\"trainable weights: \", len(model.trainable_weights))\nprint(\"non trainable weights: \",len(model.non_trainable_weights))","e2332d63":"#Creamos un Callback \nearly_stopping = EarlyStopping(monitor='loss',patience=2)\n\n#Entremos el modelo\nmodel.fit(X_train_8, y_train_8, epochs=10,callbacks=[early_stopping], validation_data=(X_test_8, y_test_8))","57318835":"accuracy= model.evaluate(X_test_8,y_test_8)[1]\nprint ('Model accuracy:' ,accuracy*100, '%')","a5f9ce20":"#model.save(r'C:\\Users\\pablo.cumbrera\\Documents\\Data\\Models\\cnn_ochocategorias.h5')","673e4a34":"#Congelamos el modelo para T.L.\n\nmodel.trainable=False\n\n#####\n\ninput_1 = keras.layers.Input(shape = (28,28,1))\n\nmodel_con = model(input_1)\n\nlayer_2 = keras.layers.Dense(2, activation='relu')(model_con)\n\nconcatenate = keras.layers.concatenate([model_con, layer_2])\n\noutput = keras.layers.Dense(10, activation='softmax')(concatenate)\n\ntop_model = keras.models.Model(inputs = [input_1],outputs = [output])\n\ntop_model.summary()","765430f4":"#Compilamos el modelo\nopt = tf.keras.optimizers.SGD(learning_rate=0.001)\ntop_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n\n\nprint(\"weights : \",len(top_model.weights))\nprint(\"trainable weights: \", len(top_model.trainable_weights))\nprint(\"non trainable weights: \",len(top_model.non_trainable_weights))","0f151403":"#Entremos el modelo\nhistory1 = top_model.fit(X_train_full, y_train_full, epochs=15, validation_data=(X_test_full, y_test_full))","73c00a7a":"accuracy= top_model.evaluate(X_test_full,y_test_full)[1]\nprint ('Model accuracy:' ,accuracy*100, '%')","0fe7b0c7":"# Gr\u00e1fica Precisi\u00f3n\n\nplt.plot(history1.history['accuracy'])\nplt.plot(history1.history['val_accuracy'])\nplt.title('Precision del modelo')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['Entrenamiento', 'test'], loc='upper left')\nplt.show()\n\n# Gr\u00e1fica P\u00e9rdida\n\nplt.plot(history1.history['loss'])\nplt.plot(history1.history['val_loss'])\nplt.title('Perdidas del modelo')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['Entrenamiento', 'test'], loc='upper left')\nplt.show()","f03e174c":"predictions = model.predict(X_test_8)\nfor i in range (10):\n    print(\" 0 Tshirt 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Shirt 6 Sneaker 7 Bag 8 Ankle Boot 9 Sandals\")\n    print(predictions[i])\n    print(y_test_8[i])\n    print (\" \\n \")","436e7c71":"predictions = top_model.predict(X_test_full)\nfor i in range (10):\n    print(\" 0 Tshirt 1 Trouser 2 Pullover 3 Dress 4 Coat 5 Shirt 6 Sneaker 7 Bag 8 Ankle Boot 9 Sandals\")\n    print(predictions[i])\n    print(y_test_full[i])\n    print (\" \\n \")\n","41b3fbcb":"#####################################################","47b7a2d8":"Visualizamos la curva de precisi\u00f3n y p\u00e9rdida","25ab8670":" - Llamaremos a cada subconjunto por el n\u00famero de categorias que lleva","0ed7d734":"- Preentrenamos el modelo base","0847bb0a":"### Data Cleansing\n___","f7aa17f6":" ### Dataset y Librerias","dc12d905":"- Presentaci\u00f3n de los DataFrames","b742f68f":"En el siguiente proyecto trataremos de realizar un aprendizaje sobre el conjunto de datos MNIST, dejando fuera del set la categor\u00eda 5 (Sandalias) y 6 (Shirt), para comprobar si tras el entrenamiento del modelo en las categorias con las que comparte caracter\u00edsticas es capaz, posteriormente, de clasificarla correctamente. \n\nEntre los diferentes items incluimos:\n- 0 T-shirt\/top\n- 1 Trouser\n- 2 Pullover\n- 3 Dress\n- 4 Coat\n- 5 Sandal (Categor\u00eda a la que se aplica Transfer Learning)\n- 6 Shirt (Categor\u00eda a la que se aplica Transfer Learning)\n- 7 Sneaker \n- 8 Bag\n- 9 Ankle boot\n","7f3bce1c":"El Transfer Learning es una t\u00e9cnica aplicada al Deep Learning que consiste, esencialmente, en el aprovechamiento de una gran cantidad de informaci\u00f3n (datos) que se han aplicado sobre el entrenamiento de un modelo con el objetivo de resolver un problema y reutilizarlo en otro modelo(transferencia) para resolver otro problema con el que comparte ciertas caracter\u00edsticas.","feb5332e":"___","f3c2b7ad":"- Transfer Learning through  Concat Set Up","b7ec6bc9":"### \u00cdndice\n - Introducci\u00f3n\n\n - Dataset y Librer\u00edas\n - Data Cleansing & Preprocessing\n - Exploratory Data Analysis (EDA)\n - Model Building\n - Conclusiones","2d2f0af5":"- A trav\u00e9s del aprendizaje de las ocho categor\u00edas, el modelo ha sido capaz de generalizar los resultados a dos clases con las que no hab\u00eda sido entrenada en primer lugar.","e2bc04af":"### Model Building\n___ ","d1ea22ec":"- Aunque el accuracy salga bajo, luego comprobando los resultados veremos que acierta tanto como el otro modelo","b7c621e2":"### Exploratory Data Analysis\n___","1ad32564":"![mnist prendas.png](attachment:3dd9a134-7e81-4dce-9df3-9876bd4896e9.png)","640d1331":"### Conclusiones\n___","fef2264f":"- Predicciones para el modelo:","032fc5fa":"Creamos los conjuntos de datos que vamos a entrenar a continuaci\u00f3n:","1c53cc5b":"- Visualizamos el modelo","f823a5e4":"<center>\n    \n    \n ## Transfer Learning Apply on MNIST DataSet\n    \n___","429742ae":"#### Introducci\u00f3n\n___","11b3db01":"<center>\n\n##### Pablo de la Asunci\u00f3n Cumbrera Conde","ba3833e8":"- Modelo de 8 categor\u00edas","8c79c9f0":"![transferlearning.gif](attachment:ac0b0237-8997-4d9f-926e-635be5e49935.gif)","b45452cf":"![1c9f9532-3773-40bc-8951-f53f572f12ae.png](attachment:f7b8d0a1-14a6-4fef-b6e6-4cc97a490a61.png)"}}