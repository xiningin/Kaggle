{"cell_type":{"565cc563":"code","88826593":"code","d1e67edd":"code","42082597":"code","5ca120a4":"code","61fc51f9":"code","d9a1e6b1":"code","d6f417de":"code","75e8b7a7":"code","17a1f734":"code","1d5a61da":"code","675d4092":"markdown","95683730":"markdown","a8ab82f2":"markdown","f414beb9":"markdown","bba3bffa":"markdown","f43ad198":"markdown","a4c30013":"markdown","6cb4ebd5":"markdown","7dce1a6d":"markdown","5b30d499":"markdown","254c7fa1":"markdown"},"source":{"565cc563":"import pandas as pd\nimport numpy as np\nimport pickle\nimport itertools\nimport gc\nimport math\nimport matplotlib.pyplot as plt\nimport dateutil.easter as easter\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nfrom datetime import datetime, date, timedelta\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, HuberRegressor","88826593":"original_train_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv')\noriginal_test_df = pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/test.csv')\ngdp_df = pd.read_csv('..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\n\ngdp_df.set_index('year', inplace=True)\n\n# The dates are read as strings and must be converted\nfor df in [original_train_df, original_test_df]:\n    df['date'] = pd.to_datetime(df.date)\n    df.set_index('date', inplace=True, drop=False)\noriginal_train_df.head(2)","d1e67edd":"def smape_loss(y_true, y_pred):\n    \"\"\"SMAPE Loss\"\"\"\n    return np.abs(y_true - y_pred) \/ (y_true + np.abs(y_pred)) * 200\n\n#print(smape_loss(tf.constant([1, 2]), tf.constant([3, 4]))) # should print [100, 66.6667]","42082597":"# Feature engineering\ndef engineer(df):\n    \"\"\"Return a new dataframe with the engineered features\"\"\"\n    \n    def get_gdp(row):\n        country = 'GDP_' + row.country\n        return gdp_df.loc[row.date.year, country]\n        \n    new_df = pd.DataFrame({'gdp': np.log(df.apply(get_gdp, axis=1)),\n                           'wd4': df.date.dt.weekday == 4, # Friday\n                           'wd56': df.date.dt.weekday >= 5, # Saturday and Sunday\n                          })\n\n    # One-hot encoding (no need to encode the last categories)\n    for country in ['Finland', 'Norway']:\n        new_df[country] = df.country == country\n    new_df['KaggleRama'] = df.store == 'KaggleRama'\n    for product in ['Kaggle Mug', 'Kaggle Sticker']:\n        new_df[product] = df['product'] == product\n        \n    # Seasonal variations (Fourier series)\n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 20):\n        new_df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n        new_df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n        new_df[f'mug_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Mug']\n        new_df[f'mug_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Mug']\n        new_df[f'sticker_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Sticker']\n        new_df[f'sticker_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Sticker']\n\n    return new_df\n\ntrain_df = engineer(original_train_df)\ntrain_df['date'] = original_train_df.date\ntrain_df['num_sold'] = original_train_df.num_sold.astype(np.float32)\ntest_df = engineer(original_test_df)\n\nfeatures = test_df.columns\n\nfor df in [train_df, test_df]:\n    df[features] = df[features].astype(np.float32)\nprint(list(features))","5ca120a4":"def fit_model(X_tr, X_va=None, outliers=False):\n    \"\"\"Scale the data, fit a model, plot the training history and validate the model\"\"\"\n    start_time = datetime.now()\n\n    # Preprocess the data\n    X_tr_f = X_tr[features]\n    preproc = StandardScaler()\n    X_tr_f = preproc.fit_transform(X_tr_f)\n    y_tr = X_tr.num_sold.values.reshape(-1, 1)\n    \n    # Train the model\n    #model = LinearRegression() # 5.80558\n    model = HuberRegressor(epsilon=1.20, max_iter=500, alpha = 7e-4) # 5.80143 (epsilon=1.20) *****************\n    model.fit(X_tr_f, np.log(y_tr).ravel())\n\n    if X_va is not None:\n        # Preprocess the validation data\n        X_va_f = X_va[features]\n        X_va_f = preproc.transform(X_va_f)\n        y_va = X_va.num_sold.values.reshape(-1, 1)\n\n        # Inference for validation\n        y_va_pred = np.exp(model.predict(X_va_f)).reshape(-1, 1)\n        \n        # Evaluation: Execution time and SMAPE\n        smape_before_correction = np.mean(smape_loss(y_va, y_va_pred))\n        #y_va_pred *= LOSS_CORRECTION\n        smape = np.mean(smape_loss(y_va, y_va_pred))\n        print(f\"Fold {run}.{fold} | {str(datetime.now() - start_time)[-12:-7]}\"\n              f\" | SMAPE: {smape:.5f}   (before correction: {smape_before_correction:.5f})\")\n        print(np.mean(smape_loss(y_va, y_va_pred)))\n        \n        # Plot y_true vs. y_pred\n        plt.figure(figsize=(10, 10))\n        plt.scatter(y_va, y_va_pred, s=1, color='r')\n        #plt.scatter(np.log(y_va), np.log(y_va_pred), s=1, color='g')\n        plt.plot([plt.xlim()[0], plt.xlim()[1]], [plt.xlim()[0], plt.xlim()[1]], '--', color='k')\n        plt.gca().set_aspect('equal')\n        plt.xlabel('y_true')\n        plt.ylabel('y_pred')\n        plt.title('OOF Predictions')\n        plt.show()\n        \n    return preproc, model\n\npreproc, model = fit_model(train_df)\n\n# Plot all num_sold_true and num_sold_pred (five years) for one country-store-product combination\ndef plot_five_years_combination(engineer, country='Norway', store='KaggleMart', product='Kaggle Hat'):\n    demo_df = pd.DataFrame({'row_id': 0,\n                            'date': pd.date_range('2015-01-01', '2019-12-31', freq='D'),\n                            'country': country,\n                            'store': store,\n                            'product': product})\n    demo_df.set_index('date', inplace=True, drop=False)\n    demo_df = engineer(demo_df)\n    demo_df['num_sold'] = np.exp(model.predict(preproc.transform(demo_df[features])))\n    plt.figure(figsize=(20, 6))\n    plt.plot(np.arange(len(demo_df)), demo_df.num_sold, label='prediction')\n    train_subset = train_df[(original_train_df.country == country) & (original_train_df.store == store) & (original_train_df['product'] == product)]\n    plt.scatter(np.arange(len(train_subset)), train_subset.num_sold, label='true', alpha=0.5, color='red', s=3)\n    plt.legend()\n    plt.title('Predictions and true num_sold for five years')\n    plt.show()\n\nplot_five_years_combination(engineer)","61fc51f9":"train_df['pred'] = np.exp(model.predict(preproc.transform(train_df[features])))\nby_date = train_df.groupby(level='date')\nresiduals = (by_date.pred.sum() - by_date.num_sold.sum()) \/ (by_date.pred.sum() + by_date.num_sold.sum()) * 200\n\n# Plot all residuals (four-year range, sum of all products)\ndef plot_all_residuals(residuals):\n    plt.figure(figsize=(20,6))\n    plt.scatter(residuals.index,\n                residuals,\n                s=1, color='k')\n    plt.vlines(pd.date_range('2015-01-01', '2019-01-01', freq='M'),\n               plt.ylim()[0], plt.ylim()[1], alpha=0.5)\n    plt.vlines(pd.date_range('2015-01-01', '2019-01-01', freq='Y'),\n               plt.ylim()[0], plt.ylim()[1], alpha=0.5)\n    plt.title('Residuals for four years')\n    plt.show()\n    \nplot_all_residuals(residuals)\n\n# Plot residuals for interesting intervals\ndef plot_around(residuals, m, d, w):\n    \"\"\"Plot residuals in an interval of with 2*w around month=m and day=d\"\"\"\n    plt.figure()\n    plt.title(f\"Residuals around m={m} d={d}\")\n    for y in np.arange(2015, 2020):\n        d0 = pd.Timestamp(date(y, m, d))\n        residual_range = residuals[(residuals.index > d0 - timedelta(w)) & \n                                   (residuals.index < d0 + timedelta(w))]\n        plt.plot([(r - d0).days for r in residual_range.index], residual_range, label=str(y))\n    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True)) # only integer labels\n    plt.legend()\n    plt.show()\n\nplot_around(residuals, 1, 1, 20) # end of year peak\nplot_around(residuals, 5, 1, 50) # three moveable peaks depending on Easter\n#plot_around(residuals, 5, 21, 10) # zoom-in\n#plot_around(residuals, 5, 31, 15) # zoom-in\nplot_around(residuals, 6, 10, 10) # first half of June (with overlay of Pentecost in 2017)\nplot_around(residuals, 6, 30, 10) # moveable peak end of June\nplot_around(residuals, 11, 5, 10) # moveable peak beginning of November","d9a1e6b1":"# Feature engineering\ndef engineer_more(df):\n    \"\"\"Return a new dataframe with more engineered features\"\"\"\n    new_df = engineer(df)\n\n    # End of year\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d)\n                                      for d in range(24, 32)})],\n                       axis=1)\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) \n                                      for d in range(1, 13)})],\n                       axis=1)\n    \n    # May\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) \n                                      for d in list(range(1, 10)) + list(range(17, 25))})],\n                       axis=1)\n    \n    # June\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"june{d}\":\n                                      (df.date.dt.month == 6) & (df.date.dt.day == d) \n                                      for d in list(range(6, 14))})],\n                       axis=1)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"wed_june{d}\": \n                                      (df.date - wed_june_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(-5, 6))})],\n                       axis=1)\n    \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"sun_nov{d}\": \n                                      (df.date - sun_nov_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(0, 10))})],\n                       axis=1)\n    \n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"easter{d}\": \n                                      (df.date - easter_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(0, 9)) + list(range(50, 60)) + list(range(40, 46))})],\n                       axis=1)\n    \n    # Growth is country-specific\n    #for country in ['Finland', 'Norway', 'Sweden']:\n    #    new_df[f\"{country}_year\"] = (df.country == country) * (df.date.dt.year - 2016)\n    #    new_df[f\"{country}_peak_year\"] = (df.country == country) * (new_df.dec29 | new_df.dec30 | new_df.easter_week) * (df.date.dt.year - 2016)\n        \n    return new_df.astype(np.float32)\n\ntrain_df = engineer_more(original_train_df)\ntrain_df['date'] = original_train_df.date\ntrain_df['num_sold'] = original_train_df.num_sold.astype(np.float32)\ntest_df = engineer_more(original_test_df)\ntest_df.year = 2018 # no growth patch, see https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/298318\n\nfeatures = test_df.columns\nprint(list(features))\n","d6f417de":"preproc, model = fit_model(train_df)\ntrain_df['pred'] = np.exp(model.predict(preproc.transform(train_df[features])))\nby_date = train_df.groupby(level='date')\nresiduals = (by_date.pred.sum() - by_date.num_sold.sum()) \/ (by_date.pred.sum() + by_date.num_sold.sum()) * 200\n\n# Plot all num_sold_true and num_sold_pred (five years) for one country-store-product combination\nplot_five_years_combination(engineer_more)\n\n# Plot all residuals (four-year range, sum of all products)\nplot_all_residuals(residuals)\n\n# Plot residuals for interesting intervals\nplot_around(residuals, 1, 1, 20) # end of year peak\nplot_around(residuals, 5, 1, 50) # three moveable peaks depending on Easter\n#plot_around(residuals, 5, 21, 10) # zoom-in\n#plot_around(residuals, 5, 31, 15) # zoom-in\nplot_around(residuals, 6, 10, 10) # first half of June (with overlay of Pentecost in 2017)\nplot_around(residuals, 6, 30, 10) # moveable peak end of June\nplot_around(residuals, 11, 5, 10) # moveable peak beginning of November\n","75e8b7a7":"#%%time\nRUNS = 1 # should be 1. increase the number of runs only if you want see how the result depends on the random seed\nOUTLIERS = True\nTRAIN_VAL_CUT = datetime(2018, 1, 1)\nLOSS_CORRECTION = 1 # correction factor between Huber loss and SMAPE: 1.035 ( for linear regression with MSE use 1.038)\n\n# Make the results reproducible\nnp.random.seed(202100)\n\ntotal_start_time = datetime.now()\nfor run in range(RUNS):\n    fold = 0\n    train_idx = np.arange(len(train_df))[train_df.date < TRAIN_VAL_CUT]\n    val_idx = np.arange(len(train_df))[train_df.date > TRAIN_VAL_CUT]\n    print(f\"Fold {run}.{fold}\")\n    X_tr = train_df.iloc[train_idx]\n    X_va = train_df.iloc[val_idx]\n    \n    preproc, model = fit_model(X_tr, X_va)","17a1f734":"# Fit the model on the complete training data\ntrain_idx = np.arange(len(train_df))\nX_tr = train_df.iloc[train_idx]\npreproc, model = fit_model(X_tr, None)\n\nplot_five_years_combination(engineer_more) # Quick check for debugging\n\n# Inference for test\ntest_pred_list = []\ntest_pred_list.append(np.exp(model.predict(preproc.transform(test_df[features]))) * LOSS_CORRECTION)\n\nif len(test_pred_list) > 0:\n    # Create the submission file\n    sub = original_test_df[['row_id']].copy()\n    sub['num_sold'] = sum(test_pred_list) \/ len(test_pred_list)\n    sub.to_csv('submission.csv', index=False)\n\n    # Plot the distribution of the test predictions\n    plt.figure(figsize=(16,3))\n    plt.hist(train_df['num_sold'], bins=np.linspace(0, 3000, 201),\n             density=True, label='Training')\n    plt.hist(sub['num_sold'], bins=np.linspace(0, 3000, 201),\n             density=True, rwidth=0.5, label='Test predictions')\n    plt.xlabel('num_sold')\n    plt.ylabel('Frequency')\n    plt.legend()\n    plt.show()\n","1d5a61da":"sub","675d4092":"# Pembetulan hari libur","95683730":"# Import Library","a8ab82f2":"## Rumus SMAPE (Rumus untuk menghitung kesalahan prediksi)","f414beb9":"# Inference and submission","bba3bffa":"# Train Model dengan Validasi\n\nModel dilatih dengan data tanggal 2015 hingga 2017 dan menggunakan 2018 sebagai validasi. Pada validasi, akan ditunjukkan :\n- Seberapa lama waktu eksekusi dan SMAPE (Symmetric Mean Absolute Percentage Error)\n- Scatterplot ytrue vs ypred (seharusnya semuanya mendekati garis diagonal)\n","f43ad198":"All credit due to AmbrosM, this notebook is useful that I'm using it for teaching my apprentices ^^","a4c30013":"# Simple feature engineering\n\nDari data yang ada, akan ditambahkan hal - hal berikut pada data nya :\n- Negara, Toko, Produk\n- Hari Kerja\n- Variasi musiman tiap produk sebagai seri Fourier dengan frekuensi dari 1 tahun hingga 18 hari\n- GDP per negara\n\nResidu dari model yang simpel ini akan mempermudah memahami hari libur","6cb4ebd5":"# Melatih model simpel (tanpa hari libur)\n\nModel akan dilatih pada full data training. Validasi bisa nanti.\n\nPengecekan cepat, kita membuat plot prediksi nya `country='Norway', store='KaggleMart', product='Kaggle Hat'`.\n","7dce1a6d":"# Melatih advanced model\n\nDiagram menunjukkan bahwa residu sangat lebih kecil sehingga hari libur bisa dimasukkan dengan tepat.","5b30d499":"# Residu model simpel\n\nSetelah memahami plot nya, plot ini menunjukkan bahwa hari libur ada pada :\n- Sekitar akhir tahun\n- Tiga hari libur sesuai bulan penuh\n- Setengah pertama bulan Mei, setengah kedua bulan Mei.\n- Awal Juni dan Akhir Juni\n- Awal November\n","254c7fa1":"# Mengambil Data"}}