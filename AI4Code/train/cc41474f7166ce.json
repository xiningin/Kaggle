{"cell_type":{"e384915c":"code","7b6cdf77":"code","19bce5d3":"code","68b779bb":"code","af4d53bf":"code","6e06ffde":"code","5a1db618":"code","695fd8fb":"code","f63b93f9":"code","c09de215":"code","b7df5134":"code","f4e5dd5c":"code","9fbd8e4c":"code","6b7590cd":"code","0050b7b9":"code","2bc4e476":"code","17eb04b4":"code","eb1d3dee":"code","3c5b2a89":"code","9d5a4289":"code","0203b101":"code","138d5f9b":"code","b3ec06cd":"code","7d632982":"code","9257a18b":"code","786b378b":"code","5258b123":"code","368a9ce3":"code","0ff6beb4":"code","ca1893cd":"code","ebf4c09e":"code","bb594b38":"code","90dd00c2":"code","5bea850c":"code","cc890131":"code","563bbe3a":"code","5432f341":"markdown","26ab6921":"markdown","9bbef64d":"markdown","fa601769":"markdown","64aca84f":"markdown","00d1f2cf":"markdown","523eacf8":"markdown","ba2a62e8":"markdown","084ef010":"markdown","ccdeaf86":"markdown","37c523b3":"markdown","3e67591e":"markdown","a637f975":"markdown","49b40d30":"markdown","13e016e9":"markdown","0992d812":"markdown","8d1c5fe4":"markdown","e1ae4d35":"markdown","c5a0053a":"markdown","3d0f5c51":"markdown","ed0d2d8f":"markdown","05813cab":"markdown","9c4d965e":"markdown","5e17bbb0":"markdown","6cd52d63":"markdown","99810201":"markdown","c260e8c4":"markdown","74734a8a":"markdown","08012dfa":"markdown","ea317177":"markdown","798dfaaf":"markdown"},"source":{"e384915c":"import sklearn\nfrom sklearn.metrics import roc_auc_score","7b6cdf77":"import os\nimport sys\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Dense,Input,MaxPooling1D,GlobalMaxPooling1D,Conv1D,Embedding\nfrom tensorflow.keras.models import Model\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","19bce5d3":"train=pd.read_csv(\"train.csv\",low_memory=False)\ntrain","68b779bb":"test=pd.read_csv(\"test.csv\",low_memory=False)\ntest","af4d53bf":"MAX_SEQ_LENGTH=100\nMAX_VOCAB_SIZE=20000 #This is the maximum number of unique words that will be tokenized\nEMBEDDING_DIM=100 # Each word will be represented as 100 dim vector\\\nVALIDATION_SPLIT=0.2 #Useful while training\nBATCH_SIZE=128\nEPOCHS=10","6e06ffde":"word2vec={}\nwith open(os.path.join(\"..\/large_data\/glove.6B\/glove.6B.%sd.txt\" % EMBEDDING_DIM),encoding=\"utf-8\") as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        embed=np.asarray(values[1:],dtype=\"float32\")\n        word2vec[word]=embed\nprint(\"Found \",len(word2vec),\" word vectors\")   ","5a1db618":"sentences=train[\"comment_text\"].fillna(\"DUMMY_VALUE\").values   #.values returns a numpy array\npossible_labels=[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\ntargets=train[possible_labels].values # returns a one hot encoded label vector for each example in train data\ntargets.shape","695fd8fb":"tokenizer=Tokenizer(num_words=MAX_VOCAB_SIZE)\ntokenizer.fit_on_texts(sentences)\nsequences=tokenizer.texts_to_sequences(sentences)","f63b93f9":"word2idx=tokenizer.word_index\nprint(\"Found %s unique tokens \" % len(word2idx))","c09de215":"data=pad_sequences(sequences,maxlen=MAX_SEQ_LENGTH) # padding is pre by default\nprint(\"shape of data is \",data.shape)","b7df5134":"word2idx","f4e5dd5c":"num_words=min(MAX_VOCAB_SIZE,len(word2idx)+1) # Num of words should be less than or equal to MAX_VOCAB_SIZE\n\n# The +1 term indicates that the tokenizer indexing begins from 1\n\nembedding_matrix=np.zeros((num_words,EMBEDDING_DIM))\nfor word,pos_from_start in word2idx.items():\n    if pos_from_start<MAX_VOCAB_SIZE:\n        embedding_vector=word2vec.get(word) #we use get method instead of indexing because it helps if the word is not present in the dictionary\n        if embedding_vector is not None:\n            embedding_matrix[pos_from_start]=embedding_vector","9fbd8e4c":"embedding_matrix.shape","6b7590cd":"embedding_layer=Embedding(num_words,\n                          EMBEDDING_DIM,\n                          weights=[embedding_matrix],\n                          input_length=MAX_SEQ_LENGTH,\n                          trainable=False\n                         )","0050b7b9":"input_=Input(shape=(MAX_SEQ_LENGTH,))\nx=embedding_layer(input_)\nx=Conv1D(128,3,activation=\"relu\")(x)\nx=MaxPooling1D(3)(x)\nx=Conv1D(128,3,activation=\"relu\")(x)\nx=MaxPooling1D(3)(x)\nx=Conv1D(128,3,activation=\"relu\")(x)\nx=GlobalMaxPooling1D()(x) # max from every input channel\n# it also indicates which timestep value in that sequnce was most influential for classification\nx=Dense(128,activation=\"relu\")(x)\noutput=Dense(len(possible_labels),activation=\"sigmoid\")(x)\n# we use sigmoid classifier so that each of the 6 units in the last layer act as a linear classifier(y\/n)\n","2bc4e476":"model1=Model(input_,output)\nmodel1.compile(loss=\"binary_crossentropy\",\n             optimizer=\"rmsprop\",\n             metrics=[\"accuracy\"]\n              )","17eb04b4":"history=model1.fit(\n                data,\n                targets,\n                batch_size=BATCH_SIZE,\n                epochs=EPOCHS,\n                validation_split=VALIDATION_SPLIT\n                )","eb1d3dee":"def plot_curves(history):\n    fig,(ax0,ax1)=plt.subplots(2,1,figsize=(8,8))\n    ax0.plot(history.history[\"loss\"],label=\"loss\")\n    ax0.plot(history.history[\"val_loss\"],label=\"val_loss\")\n    ax0.legend()\n    ax1.plot(history.history[\"accuracy\"],label=\"accuracy\")\n    ax1.plot(history.history[\"val_accuracy\"],label=\"val_accuracy\")\n    ax1.legend()\n    plt.show()","3c5b2a89":"plot_curves(history)","9d5a4289":"model2=Model(input_,output)\nmodel2.compile(loss=\"binary_crossentropy\",\n             optimizer=\"adam\",\n             metrics=[\"accuracy\"]\n              )\nhistory=model2.fit(\n                data,\n                targets,\n                batch_size=BATCH_SIZE,\n                epochs=EPOCHS,\n                validation_split=VALIDATION_SPLIT\n                )","0203b101":"plot_curves(history)","138d5f9b":"input_=Input(shape=(MAX_SEQ_LENGTH,))\nx=embedding_layer(input_)\nx=Conv1D(128,3,activation=\"relu\")(x)\nx=MaxPooling1D(3)(x)\nx=Conv1D(128,3,activation=\"relu\")(x)\nx=MaxPooling1D(3)(x)\nx=Conv1D(128,3,activation=\"relu\")(x)\nx=GlobalMaxPooling1D()(x) # max from every input channel\n# it also indicates which timestep value in that sequnce was most influential for classification\nx=Dense(128,activation=\"relu\")(x)\nx=tf.keras.layers.Dropout(0.3)(x)\noutput=Dense(len(possible_labels),activation=\"sigmoid\")(x)\n# we use sigmoid classifier so that each of the 6 units in the last layer act as a linear classifier(y\/n)\n\nmodel3=Model(input_,output)\nmodel3.compile(loss=\"binary_crossentropy\",\n             optimizer=\"adam\",\n             metrics=[\"accuracy\"]\n              )\nhistory=model3.fit(\n                data,\n                targets,\n                batch_size=BATCH_SIZE,\n                epochs=EPOCHS,\n                validation_split=VALIDATION_SPLIT\n                )","b3ec06cd":"plot_curves(history)","7d632982":"early_stopping=tf.keras.callbacks.EarlyStopping(patience=5,monitor=\"val_accuracy\")\nmodel_checkpoint=tf.keras.callbacks.ModelCheckpoint(\"model3.h5\",monitor=\"val_accuracy\",save_best_only=True)","9257a18b":"EPOCHS=100\nhistory=model3.fit(\n                data,\n                targets,\n                batch_size=BATCH_SIZE,\n                epochs=EPOCHS,\n                validation_split=VALIDATION_SPLIT,\n                callbacks=[early_stopping,model_checkpoint]\n                )","786b378b":"model=tf.keras.models.load_model(\"model3.h5\")\np=model.predict(data)\naucs = []\nfor j in range(6):\n    auc = roc_auc_score(targets[:,j], p[:,j])\n    aucs.append(auc)\nprint(np.mean(aucs))","5258b123":"p.shape","368a9ce3":"test_sentences=test[\"comment_text\"].fillna(\"DUMMY_VALUE\").values\ntest_sequences=tokenizer.texts_to_sequences(test_sentences)\ntest_data=pad_sequences(test_sequences,maxlen=MAX_SEQ_LENGTH)","0ff6beb4":"pred=model.predict(test_data)","ca1893cd":"pred[:,0].shape","ebf4c09e":"possible_labels","bb594b38":"submit1=pd.DataFrame(columns=[\"id\",\"toxic\",\"severe_toxic\",\"threat\",\"insult\",\"identity_hate\"])\nsubmit1[\"id\"]=test[\"id\"]\ni=0\nfor col in possible_labels:\n    submit1[col]=pred[:,i]\n    i=i+1","90dd00c2":"submit1","5bea850c":"submit1.index = submit1.index+1\nsubmit1","cc890131":"submit1.to_csv(\"submission1.csv\",index=False)","563bbe3a":"a=pd.read_csv(\"submission1.csv\")\na","5432f341":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Problem-Defintion:\" data-toc-modified-id=\"Problem-Defintion:-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Problem Defintion:<\/a><\/span><\/li><li><span><a href=\"#Data\" data-toc-modified-id=\"Data-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Data<\/a><\/span><\/li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>Evaluation<\/a><\/span><\/li><li><span><a href=\"#Dealing-with-the-data\" data-toc-modified-id=\"Dealing-with-the-data-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Dealing with the data<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Making-necessary-imports-and-installations\" data-toc-modified-id=\"Making-necessary-imports-and-installations-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Making necessary imports and installations<\/a><\/span><\/li><li><span><a href=\"#Viewing-the-data\" data-toc-modified-id=\"Viewing-the-data-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;<\/span>Viewing the data<\/a><\/span><\/li><li><span><a href=\"#Defining-some-constant-terms-that-we'll-be-using-later\" data-toc-modified-id=\"Defining-some-constant-terms-that-we'll-be-using-later-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;<\/span>Defining some constant terms that we'll be using later<\/a><\/span><\/li><li><span><a href=\"#Loading-the-pretrained-word-embeddings-into-the--notebook-as-a-dictionary\" data-toc-modified-id=\"Loading-the-pretrained-word-embeddings-into-the--notebook-as-a-dictionary-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;<\/span>Loading the pretrained word embeddings into the  notebook as a dictionary<\/a><\/span><\/li><li><span><a href=\"#Preparing-the-data-to-feed-to-model\" data-toc-modified-id=\"Preparing-the-data-to-feed-to-model-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;<\/span>Preparing the data to feed to model<\/a><\/span><\/li><li><span><a href=\"#Converting-sentences-into-numbers-(sequences)\" data-toc-modified-id=\"Converting-sentences-into-numbers-(sequences)-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;<\/span>Converting sentences into numbers (sequences)<\/a><\/span><\/li><li><span><a href=\"#Padding-the-sequences\" data-toc-modified-id=\"Padding-the-sequences-4.7\"><span class=\"toc-item-num\">4.7&nbsp;&nbsp;<\/span>Padding the sequences<\/a><\/span><\/li><li><span><a href=\"#Preparing-the-embedding-matrix-corresponding-to-our-dataset-using-the-pretrained-embeddings\" data-toc-modified-id=\"Preparing-the-embedding-matrix-corresponding-to-our-dataset-using-the-pretrained-embeddings-4.8\"><span class=\"toc-item-num\">4.8&nbsp;&nbsp;<\/span>Preparing the embedding matrix corresponding to our dataset using the pretrained embeddings<\/a><\/span><\/li><li><span><a href=\"#Loading-the-embeddings-we-obtained-into-a-keras-Embedding-Layer\" data-toc-modified-id=\"Loading-the-embeddings-we-obtained-into-a-keras-Embedding-Layer-4.9\"><span class=\"toc-item-num\">4.9&nbsp;&nbsp;<\/span>Loading the embeddings we obtained into a keras Embedding Layer<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Modelling\" data-toc-modified-id=\"Modelling-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>Modelling<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Baseline-model\" data-toc-modified-id=\"Baseline-model-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>Baseline model<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#Defining-a-1D-Convolutional-Neural-Network\" data-toc-modified-id=\"Defining-a-1D-Convolutional-Neural-Network-5.1.1\"><span class=\"toc-item-num\">5.1.1&nbsp;&nbsp;<\/span>Defining a 1D Convolutional Neural Network<\/a><\/span><\/li><li><span><a href=\"#Compiling-the-model\" data-toc-modified-id=\"Compiling-the-model-5.1.2\"><span class=\"toc-item-num\">5.1.2&nbsp;&nbsp;<\/span>Compiling the model<\/a><\/span><\/li><li><span><a href=\"#Fitting-the-model-to-0.8-split-of-total-data-and-validating-on-the-0.2-part\" data-toc-modified-id=\"Fitting-the-model-to-0.8-split-of-total-data-and-validating-on-the-0.2-part-5.1.3\"><span class=\"toc-item-num\">5.1.3&nbsp;&nbsp;<\/span>Fitting the model to 0.8 split of total data and validating on the 0.2 part<\/a><\/span><\/li><li><span><a href=\"#Reviewing-loss-and-accuracy-path-of-model\" data-toc-modified-id=\"Reviewing-loss-and-accuracy-path-of-model-5.1.4\"><span class=\"toc-item-num\">5.1.4&nbsp;&nbsp;<\/span>Reviewing loss and accuracy path of model<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Conv1D-model-with-adam-optimizer\" data-toc-modified-id=\"Conv1D-model-with-adam-optimizer-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>Conv1D model with adam optimizer<\/a><\/span><\/li><li><span><a href=\"#Introducing-Dropout\" data-toc-modified-id=\"Introducing-Dropout-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>Introducing Dropout<\/a><\/span><\/li><li><span><a href=\"#Using-callbacks-on-our-model\" data-toc-modified-id=\"Using-callbacks-on-our-model-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;<\/span>Using callbacks on our model<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#Evaluating-model-on-train-data\" data-toc-modified-id=\"Evaluating-model-on-train-data-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;<\/span>Evaluating model on train data<\/a><\/span><\/li><li><span><a href=\"#Evaluating-model-on-test-data\" data-toc-modified-id=\"Evaluating-model-on-test-data-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;<\/span>Evaluating model on test data<\/a><\/span><\/li><li><span><a href=\"#Creating-submission-file\" data-toc-modified-id=\"Creating-submission-file-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;<\/span>Creating submission file<\/a><\/span><\/li><\/ul><\/div>","26ab6921":"## Baseline model","9bbef64d":"## Using callbacks on our model","fa601769":"* The model is evaluated on the mean column-wise ROC AUC. \n* In other words, the score is the average of the individual AUCs of each predicted column.","64aca84f":"## Making necessary imports and installations","00d1f2cf":"Also we set Trainable as False for this layer as we have already loaded pretrained weights(embeddings)","523eacf8":"### Compiling the model","ba2a62e8":"# Dealing with the data","084ef010":"## Padding the sequences","ccdeaf86":"# Modelling","37c523b3":"### Defining a 1D Convolutional Neural Network","3e67591e":"# Problem Defintion:","a637f975":"## Preparing the embedding matrix corresponding to our dataset using the pretrained embeddings","49b40d30":"# Evaluating model on train data","13e016e9":"## Loading the pretrained word embeddings into the  notebook as a dictionary","0992d812":"### Reviewing loss and accuracy path of model","8d1c5fe4":"## Preparing the data to feed to model","e1ae4d35":"# Evaluation","c5a0053a":"#  Data\nThe data is available on kaggle at\n\nhttps:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/data\n\nThe data comprises of\n\n* 159571 comments for train data and\n* 153164 test comments","3d0f5c51":"* To given a negative comment in English Language, we must be able to classify its toxicity\n* This is a multilabel problem.m\n* So the output for each example should be a six dimensional vector","ed0d2d8f":"### Fitting the model to 0.8 split of total data and validating on the 0.2 part","05813cab":"# Evaluating model on test data","9c4d965e":"## Conv1D model with adam optimizer","5e17bbb0":"So we see the validation accuracy started dropping later.\n\nIt indicates that the model started overfitting the data","6cd52d63":"Note: In the embedding matrix ,vectors corresponding to words that are present in the data but not in the tokenizer are all zeros","99810201":"## Viewing the data","c260e8c4":"## Defining some constant terms that we'll be using later","74734a8a":"## Introducing Dropout","08012dfa":"## Converting sentences into numbers (sequences)","ea317177":"# Creating submission file","798dfaaf":"## Loading the embeddings we obtained into a keras Embedding Layer"}}