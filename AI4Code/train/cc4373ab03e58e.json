{"cell_type":{"ae18fa48":"code","0dcf822e":"code","3d1e599b":"code","1b79b4d3":"code","d125a783":"code","9515eac3":"code","240ab0e6":"code","cfd63b7e":"code","808cbbed":"code","5121ed68":"code","a2d4eebb":"code","e9ab8693":"code","e7708f22":"code","7b45d64a":"code","06a56791":"code","c7b28dc3":"code","1600d706":"code","ad9b1692":"code","344d5c8a":"code","945853f9":"code","6aa66340":"code","aa1ce6f6":"code","6d75c023":"code","c4e1442e":"code","cab936b0":"code","117a6d6e":"code","889b874c":"code","c73a71ef":"code","6389f60b":"code","ae863923":"code","5c641d28":"code","6712a567":"code","f1c7aa4f":"code","204b172a":"code","f088e977":"code","31664f9b":"code","9ab8857f":"code","a1caf825":"code","19fa2b53":"code","5b27318c":"code","fcb707de":"code","47adf356":"code","e4cd7c27":"code","b929dcda":"code","c648ce4d":"code","aa6dbb7f":"code","c4357a40":"code","a85e50df":"code","6d8bfbd1":"code","298e0bd8":"code","5cc57260":"code","4a2b13bc":"code","34bdc10e":"markdown","eccebcdf":"markdown","31752439":"markdown"},"source":{"ae18fa48":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn import svm\nfrom sklearn.neural_network import MLPClassifier \nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline \nimport os","0dcf822e":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","3d1e599b":"#loading datasets\nos.getcwd()\nTest_data = pd.read_csv('\/kaggle\/input\/test_KaymcHn.csv', sep=',')\nTrain_data = pd.read_csv('\/kaggle\/input\/train_jqd04QH.csv', sep=',')","1b79b4d3":"Test_data.head()","d125a783":"Train_data.head()","9515eac3":"Test_data.info()","240ab0e6":"Test_data.isnull().sum()","cfd63b7e":"Train_data.info()","808cbbed":"Train_data.isnull().sum()","5121ed68":"sns.countplot(Train_data['gender'])","a2d4eebb":"print(Train_data['city'].unique())","e9ab8693":"print(Train_data['enrolled_university'].unique())","e7708f22":"print(Train_data['city_development_index'].unique())","7b45d64a":"print(Train_data['gender'].unique())","06a56791":"print(Train_data['relevent_experience'].unique())","c7b28dc3":"print(Train_data['enrolled_university'].unique())","1600d706":"print(Train_data['education_level'].unique())","ad9b1692":"print(Train_data['major_discipline'].unique())","344d5c8a":"print(Train_data['experience'].unique())","945853f9":"print(Train_data['company_size'].unique())","6aa66340":"print(Train_data['company_type'].unique())","aa1ce6f6":"print(Test_data['last_new_job'].unique())","6d75c023":"print(Train_data['training_hours'].unique())","c4e1442e":"print(Train_data['target'].unique())","cab936b0":"#Understanding the gender break up - target wise \nsg_mode = Train_data.groupby(['target','gender'])\nsg_mode.size()","117a6d6e":"# In both the categories, target 0 or 1: It is male is dominating gender and hence will replace all missing values with 'Male'\nTrain_data['gender']=np.where((Train_data['gender'].isnull()),\"Male\",Train_data['gender']) \nTrain_data['gender']=np.where((Train_data['gender']=='Other'),\"Male\",Train_data['gender']) \nTest_data['gender']=np.where((Test_data['gender'].isnull()),\"Male\",Test_data['gender']) \nTest_data['gender']=np.where((Test_data['gender']=='Other'),\"Male\",Test_data['gender']) ","889b874c":"sg_mode = Train_data.groupby(['target','enrolled_university'])\nsg_mode.size()","c73a71ef":"# It is dominated by students without any enrollment in both categories of output - target 0 or 1. \nTrain_data['enrolled_university'].fillna('no_enrollment', inplace = True)\nTest_data['enrolled_university'].fillna('no_enrollment', inplace = True)","6389f60b":"sg_mode = Train_data.groupby(['target','education_level'])\nsg_mode.size()","ae863923":"# It is dominated by students with graduate degree in both categories of output - target 0 or 1. \nTrain_data['education_level'].fillna('Graduate', inplace = True)\nTest_data['education_level'].fillna('Graduate', inplace = True)","5c641d28":"sg_mode = Train_data.groupby(['target','major_discipline'])\nsg_mode.size()","6712a567":"# It is dominated by students with STEM graduation degree in both categories of output - target 0 or 1. \nTrain_data['major_discipline'].fillna('STEM', inplace = True)\nTest_data['major_discipline'].fillna('STEM', inplace = True)","f1c7aa4f":"sg_mode = Train_data.groupby(['target','experience'])\nsg_mode.size()","204b172a":"#around 59 entries has no fields under experience category. dropping these rows for now\nCompany_size = Train_data['company_size']\nTrain_data  = Train_data.dropna(subset =['experience','company_size','company_type','last_new_job'])","f088e977":"Company_size.fillna('50-99', inplace = True)\n\n# Train_data['company_size'].fillna('50-99', inplace = True)\n# Test_data['company_size'].fillna('50-99', inplace = True)","31664f9b":"Train_data['last_new_job'].describe()","9ab8857f":"sg_mode = Train_data.groupby(['target','education_level'])\nsg_mode.size()","a1caf825":"#Preprocessing city development index into 4 major city categories - Metro, Tier 1, Tier 2, and Tier 3\nbins = (0.25,0.5,0.75,1)\nTest_data['city'].unique()\ngroup_names = ['Tier 3','Tier 2', 'Tier 1', 'Metro']\nTrain_CDI = Train_data['city_development_index']\nTrain_data['city_development_index'] = pd.cut(Train_data['city_development_index'], 4, labels = group_names)\nTest_data['city_development_index'] = pd.cut(Test_data['city_development_index'], 4, labels = group_names)\nTest_data['city'].unique()","19fa2b53":"Train_data['city'] = Train_data['city_development_index']\nTrain_data['city_development_index'] = Train_CDI\nTest_data['city'] = Test_data['city_development_index']\nTest_data['city'].unique()","5b27318c":"group_names = ['Low','Medium', 'High']\nTrain_data['training_hours'] = pd.cut(Train_data['training_hours'], 3, labels = group_names)\nTest_data['training_hours'] = pd.cut(Test_data['training_hours'], 3, labels = group_names)","fcb707de":"Train_data['city_development_index'].value_counts()","47adf356":"sns.countplot(Test_data['city'])","e4cd7c27":"sns.countplot(Train_data['city_development_index'])","b929dcda":"Train_data.drop(['city_development_index'],axis = 1, inplace = True)\nTrain_data.drop(['experience'],axis = 1, inplace = True)\nTrain_data.drop(['last_new_job'],axis = 1, inplace = True)","c648ce4d":"Train_data.info()","aa6dbb7f":"Train_data.info()","c4357a40":"# We will now convert the nominal attributes to numbers to use it for dimensonality reduction\nlabel_class= LabelEncoder()\nTest_data ['city'] = label_class.fit_transform(Test_data['city'])\nTest_data ['gender'] = label_class.fit_transform(Test_data['gender'])\nTest_data ['relevent_experience'] = label_class.fit_transform(Test_data['relevent_experience'])\nTest_data ['enrolled_university'] = label_class.fit_transform(Test_data['enrolled_university'])\n\nTest_data ['education_level'] = label_class.fit_transform(Test_data['education_level'])\n\nTest_data ['major_discipline'] = label_class.fit_transform(Test_data['major_discipline'])\nTest_data.head()","a85e50df":"# We will now convert the nominal attributes to numbers to use it for dimensonality reduction\nlabel_class= LabelEncoder()\nTrain_data ['city'] = label_class.fit_transform(Train_data['city'])\nTrain_data ['gender'] = label_class.fit_transform(Train_data['gender'])\nTrain_data ['relevent_experience'] = label_class.fit_transform(Train_data['relevent_experience'])\nTrain_data ['enrolled_university'] = label_class.fit_transform(Train_data['enrolled_university'])\n\nTrain_data ['education_level'] = label_class.fit_transform(Train_data['education_level'])\n\nTrain_data ['major_discipline'] = label_class.fit_transform(Train_data['major_discipline'])\nTrain_data.head()","6d8bfbd1":"Y_train = Train_data['target']\nfeatures = ['city','gender','relevent_experience','enrolled_university','education_level','major_discipline','company_size','training_hours']\nX_train = pd.get_dummies(Train_data[features])\nX_test = pd.get_dummies(Test_data[features])\n","298e0bd8":"Train_data.info()","5cc57260":"Test_data.info()","4a2b13bc":"model = RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train, Y_train)\npredictions = model.predict(X_test)\n\npredict = pd.DataFrame({'enrollee_id': Test_data.enrollee_id, 'target': predictions})\npredict.to_csv('Vikash_submission.csv', index=False)\n\n#predict = pd.DataFrame(predictions, columns=['predictions']).to_csv ('predictions1.csv')\nprint(\"Your submission was successfully saved!\")\n\n","34bdc10e":"# Key Learnings from mistakes\n\n* Dropped all NA values from train data sets and reduced accuracy in prediction \n* Only tried RandomForestClassifier algorithm, no optimize done with other algorithm \n* Should have combined the train data and test data for better prediction and to reduce the mean average error in final prediction \n\n","eccebcdf":"# **![](http:\/\/)JanataHack: HR Analytics Challenge (1st Machine Learning Code)**\n\n**Problem Statement: \n**\nHR analytics is revolutionising the way human resources departments operate, leading to higher efficiency and better results overall. Human resources has been using analytics for years. However, the collection, processing and analysis of data has been largely manual, and given the nature of human resources dynamics and HR KPIs, the approach has been constraining HR. \n\nTherefore, it is surprising that HR departments woke up to the utility of machine learning so late in the game. This Janata Hack presents an opportunity to try predictive analytics in HR Domain, so gear up for another fun filled weekend\n\n\n**Approach:\n**\n\nSince I have started learning Machine learning algorithm about a week ago, I had decided to take a plunge to experiment with real time challenge in one of the competition. Based on my limited learning, here is what I did in this competition and ended up at rank 286 out of 326. ","31752439":"# Result\n\n*Received score of 0.5011361030 with rank of 286 out of 326.\n*"}}