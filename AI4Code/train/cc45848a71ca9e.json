{"cell_type":{"4ed450d1":"code","667791ae":"code","3380eb2f":"code","1e7703fb":"code","4118107e":"code","42ef87c1":"code","6f6e6547":"code","b797b99f":"code","dee04813":"code","256febeb":"code","6e6c7cbe":"code","891f8f37":"code","e8a82030":"code","499a62c5":"code","9fb5c401":"code","b4842787":"code","3ee9985a":"code","4c6bc140":"code","a4599c63":"code","b189d795":"code","fd12f43e":"code","1d374801":"code","261b8bbd":"code","dedb5586":"code","abe06691":"code","c9714587":"code","b531e85e":"code","9be50367":"code","f2d8c7cd":"code","b1f86790":"code","58f889be":"code","c259021a":"code","01417839":"code","85bd7186":"code","872bb91c":"code","b3557767":"code","ea747e3d":"code","c9c6ad6e":"code","dcb03eb7":"code","5d3e5180":"code","a8f6b69e":"code","9b77e9ad":"code","3da9c820":"code","6eb1de9e":"code","6d4fd2a6":"code","13904fe6":"code","cf865a1d":"code","8915b1af":"code","3f4b9525":"code","594691db":"code","ef396fc9":"code","c044d7e9":"code","f51f4e9b":"markdown","81be0080":"markdown","9f4493ad":"markdown","864797a0":"markdown","913bd207":"markdown","94f3361b":"markdown","67903e73":"markdown","d2f5b9d6":"markdown","112952e3":"markdown","9a8deccf":"markdown","b988c53b":"markdown","71a16332":"markdown","cb95a6b1":"markdown","7c6365cf":"markdown","4ce3fcc2":"markdown","53ec683d":"markdown","6b90ee7a":"markdown","748eb54e":"markdown","b84956e0":"markdown","ebdd8e5c":"markdown","360271dc":"markdown","17377f19":"markdown","b2b9cc2a":"markdown","dd704d50":"markdown","da8b55f3":"markdown","65794078":"markdown","02c17509":"markdown"},"source":{"4ed450d1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for visualization\n%matplotlib inline\nimport seaborn as sns # also for visualization\nfrom scipy import stats # general statistical functions\n\nimport warnings\nwarnings.filterwarnings('ignore') # ignore warnings from the different libraries\n\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\nimport os\nprint(os.listdir(\"..\/input\")) # check directory contents","667791ae":"# Import and put the train and test datasets in pandas dataframes\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","3380eb2f":"# Drop the 'Id' colum since it's unnecessary for prediction process\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","1e7703fb":"# Distribution plot for SalePrice\nfrom scipy.stats import norm\nsns.distplot(train['SalePrice'] , fit=norm)\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')","4118107e":"# Determine which kind of variables are present\n\ntrain.dtypes.unique()","42ef87c1":"# Plotting function\n\ndef explore_variables(target_name, dt):\n    for col in dt.drop(target_name, 1).columns:\n        if train.dtypes[train.columns.get_loc(col)] == 'O': # categorical variable\n            f, ax = plt.subplots()\n            fig = sns.boxplot(x=col, y=target_name, data=dt)\n            ax = sns.swarmplot(x=col, y=target_name, data=dt, color=\".25\", alpha=0.2)\n            fig.axis(ymin=0, ymax=800000)\n        else: # numerical variable\n            fig, ax = plt.subplots()\n            ax.scatter(x=dt[col], y=dt[target_name])\n            plt.ylabel(target_name, fontsize=13)\n            plt.xlabel(col, fontsize=13)\n            plt.show()","6f6e6547":"explore_variables('SalePrice', train)","b797b99f":"print(\"Original size: {}\".format(train.shape))\n\n# Drop extreme observations\nconditions = [train['LotFrontage'] > 250,\n             train['LotArea'] > 100000,\n             train['BsmtFinSF1'] > 4000,\n             train['TotalBsmtSF'] > 5000,\n             train['1stFlrSF'] > 4000,\n             np.logical_and(train['GrLivArea'] > 4000, train['SalePrice'] < 300000)]\n\nprint(\"Outliers: {}\".format(sum(np.logical_or.reduce(conditions))))","dee04813":"# drop outliers\ntrain = train[np.logical_or.reduce(conditions)==False]","256febeb":"# drop useless variables\ntrain.drop(labels=['PoolArea', \n                   'PoolQC', \n                   'Street', \n                   'Condition2', \n                   'RoofMatl', \n                   'Heating', \n                   'MiscFeature', \n                   'MiscVal', \n                   'Utilities'], axis=1, inplace=True);","6e6c7cbe":"#missing data\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","891f8f37":"train['Alley'].value_counts()","e8a82030":"train.drop('Alley', axis=1, inplace=True)","499a62c5":"train['Fence'].value_counts()","9fb5c401":"# replace NaNs\n\nvalues = {'Fence': 'NoFence'}\ntrain.fillna(value=values, inplace=True)","b4842787":"# combine other values into one\n\ntrain.loc[train['Fence'] != 'NoFence', 'Fence'] = 'Fence'","3ee9985a":"train['FireplaceQu'].value_counts()","4c6bc140":"# replace NaNs\n\nvalues = {'FireplaceQu': 'NoFireplace'}\ntrain.fillna(value=values, inplace=True)","a4599c63":"# drop 'Fireplaces'\n\ntrain.drop('Fireplaces', axis=1, inplace=True)","b189d795":"sns.distplot(train['LotFrontage'].dropna())\n\nprint(\"Mean: {}. Median: {}\".format(np.mean(train['LotFrontage']), np.median(train['LotFrontage'].dropna())))","fd12f43e":"# replace NaNs\n\nvalues = {'LotFrontage': np.median(train['LotFrontage'].dropna())}\ntrain.fillna(value=values, inplace=True)","1d374801":"# drop 'GarageYrBlt'\n\ntrain.drop('GarageYrBlt', axis=1, inplace=True)","261b8bbd":"# replace NaNs\n\nvalues = {var:'NoGarage' for var in ['GarageCond', 'GarageFinish', 'GarageQual', 'GarageType']}\ntrain.fillna(value=values, inplace=True)","dedb5586":"# replace NaNs\n\nvalues = {var:'NoBsmt' for var in ['BsmtFinType2', 'BsmtExposure', 'BsmtQual', 'BsmtCond', 'BsmtFinType1']}\ntrain.fillna(value=values, inplace=True)","abe06691":"# replace NaNs\n\nvalues = {'MasVnrType': 'None', 'MasVnrArea':0}\ntrain.fillna(value=values, inplace=True)","c9714587":"train['Electrical'].value_counts()","b531e85e":"# replace NaNs\n\nvalues = {'Electrical': 'SBrkr'}\ntrain.fillna(value=values, inplace=True)","9be50367":"# combine other values into one\n\ntrain.loc[train['Electrical'] != 'SBrkr', 'Electrical'] = 'Fusebox'","f2d8c7cd":"#missing data\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(10)","b1f86790":"explore_variables('SalePrice', train.loc[:, ['SalePrice', 'LotShape'] ])","58f889be":"# Turn irregular lot shapes into one class\n\ntrain.loc[train['LotShape'] != 'Reg', 'LotShape'] = 'Irregular'","c259021a":"explore_variables('SalePrice', train.loc[:, ['SalePrice', 'LandContour'] ])","01417839":"# Turn non leveled land contours into one class\n\ntrain.loc[train['LandContour'] != 'Lvl', 'LandContour'] = 'Unleveled'","85bd7186":"explore_variables('SalePrice', train.loc[:, ['SalePrice', 'LotConfig'] ])","872bb91c":"# Turn FR* observations into one class\n\ntrain.loc[np.logical_or(train['LotConfig'] == 'FR2', train['LotConfig'] == 'FR3'), 'LotConfig'] = 'FR'","b3557767":"explore_variables('SalePrice', train.loc[:, ['SalePrice', 'LandSlope'] ])","ea747e3d":"# Turn Severe slopes into Moderate (there were too few)\n\ntrain.loc[train['LandSlope'] == 'Sev', 'LandSlope'] = 'Mod'","c9c6ad6e":"explore_variables('SalePrice', train.loc[:, ['SalePrice', 'MasVnrType'] ])","dcb03eb7":"# Turn brick veneer types into one class (for example BrkCmn)\n\ntrain.loc[train['MasVnrType'] == 'BrkFace', 'MasVnrType'] = 'BrkCmn'","5d3e5180":"explore_variables('SalePrice', train.loc[:, ['SalePrice', 'ExterCond'] ])","a8f6b69e":"# Put poor and excellent external conditions into other conditions\n\ntrain.loc[train['ExterCond'] == 'Po', 'ExterCond'] = 'Fa'\ntrain.loc[train['ExterCond'] == 'Ex', 'ExterCond'] = 'Gd'","9b77e9ad":"# incorporate log(1+x) transformation !NOT ANYMORE!\n\n# train[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])","3da9c820":"# Transform nominal variables that were read as numeric back into nominal\n\n#MSSubClass=The building class\ntrain['MSSubClass'] = train['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\ntrain['OverallCond'] = train['OverallCond'].astype(str)\n\n\n#Month sold transformed into categorical feature.\ntrain['MoSold'] = train['MoSold'].astype(str)","6eb1de9e":"# transform year variables and drop not useful ones\n\ntrain['SoldYrAgo'] = 2019 - train['YrSold']\ntrain.drop('YrSold', axis=1, inplace=True)\n\ntrain['BuiltYrAgo'] = 2019 - train['YearBuilt']\ntrain.drop('YearBuilt', axis=1, inplace=True)\n\ntrain.drop(labels=['YearRemodAdd', 'GarageFinish'], axis=1, inplace=True)","6d4fd2a6":"#Correlation map to see how features are correlated with SalePrice\ncorrmat = train.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True);","13904fe6":"# Lets see which correlations are very large in absolute value\n\ncorrmat = train.corr()\ncorrmat[abs(corrmat) > 0.7] = 1\ncorrmat[abs(corrmat) <= 0.7] = 0\n\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True);","cf865a1d":"# drop GarageArea\ntrain.drop('GarageArea', axis=1, inplace=True)","8915b1af":"# drop TotalBsmtSF\ntrain.drop('TotalBsmtSF', axis=1, inplace=True)","3f4b9525":"# create AreaPerRoom\ntrain['AreaPerRoom'] = train['GrLivArea'] \/ train['TotRmsAbvGrd']","594691db":"# dummy coding\n\ntrain = pd.get_dummies(train)","ef396fc9":"train.shape","c044d7e9":"train.to_csv('preprocessed_training.csv', index=False)","f51f4e9b":"Strong correlations with SalePrice are actually good for a future prediction. Then we obtain the following pairs of correlated variables:\n* GarageCars - GarageArea: We can drop GarageArea, as it is proportional to the number of cars.\n* TotalBsmtSF - 1stFlrSF: Here it seems that when there is a basement, its area correlates to the ground floor area, not a big surprise. I think that the presence or ausence of the basement is more important than its area, therefore we can drop TotalBsmtSF.\n* TotalRmsAbvGrd - GrLivArea: The correlation is quite simple, more rooms translate generally to a greater living area. It may be convenient to create a new variable AreaPerRoom that may provide some aditional information.\n\nLets do this changes.","81be0080":"## Data Cleaning and Formatting\n\nNow we will correct some data format problems.","9f4493ad":"Now we will transform some of the nominal variables so that they have less categories. It will however not be exhaustive, there will be variables with inconvenient distributions left. Hopefully the models won't get confused. \n\nLets just plot and modify the variables:","864797a0":"Here the two main options are 'standard circuit breaker' and 'fuse box'. Lets impute the missing value and make the variable binary.","913bd207":"# EDA\n\nLets start by droping the \"Id\" variable and doing a distribution plot for the \"SalePrice\" variable.\n","94f3361b":"Finally lets save the modified dataset, we will use it in the next kernel for modelling.","67903e73":"We can see that the variable roughly resembles a normal distribution, but it's a bit skewed. Lets take a note about that and try to improve it in the next section.\n\nNext I will analyze the dependence of the independent variables with \"SalePrice\". I will take a different approach and plot them all at once with a single function (one plot each). The idea is to visually inspect the empirical distributions and identify useless variables and outliers.","d2f5b9d6":"Lets continue with the Bsmt* variables. In all of them NA means no basement, we can then impute the missing values. I am impressed about most buildings having a basement!","112952e3":"We are left with 'Electrical'. There is one real NA that we will impute according to the empirical distribution of the variable. Lets take a look:","9a8deccf":"Lets continue with 'LotFrontage'. In this case we have a numerical variable with missing values, and a significant amount at that. We could drop the observations, but that would lead to a significant loss of data. We will then impute the NAs with either the mean or the median:","b988c53b":"Now we will continue to encode the nominal variables. In some kernels I have seen the use of the LabelEncoder for ordinal variables. In my opinion this is a big risk, because there is no real underlying scale (e.g. we cannot say \"Fair\" * 2 = \"Good\"). I will stick to dummy coding and let the models figure out the ordinality.","71a16332":"Lets continue with 'FireplaceQu':","cb95a6b1":"Lets continue with 'Fence':","7c6365cf":"**THIS KERNEL IS A WORK IN PROGRESS**\n\n# Exploratory analysis and feature engineering\nIn this kernel I will put together code snippets and analysis taken from some other kernels, improve them when possible and also provide a bit of new insight. I will be using code coming from these kernels:\n* [Serigne - Stacked Regressions to predict House Prices](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\/notebook)\n* [Pedro Marcelino - Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python)\n\nI will focus my explanations on whatever I am doing different than the other kernels, but I will also mention what I'm doing equally and why it is being done.\n\nThe kernel will be divided in the following sections:\n1. Introduction: introducing the problem, dataset and variables.\n2. EDA: understanding the dataset.\n3. Feature Engineering: improve the statistical quality of the variables, in order to train better models.\n4. Model training: Train different models and compare their performance.\n5. Conclusions.\n\n# Introduction\n\nThe task consists on predicting the price of houses in the USA based on a series of characteristics, either concerning the edification (e.g. YearBuilt, HouseStyle), the zone\/neighborhood (e.g. Street, Neighborhood) or spatial measurements of the house and terrain (e.g. LotArea, GrLivArea). I will assume that you are familiar with the variable names and descriptions, which can be found right on the competition's homepage.\n\nFrom a machine learning perspective it is a regression problem and the performance of a model can be measured for example through the root mean square error. In general it is a bit more difficult to determine if a model is \"good\" for regression problems, as the RMSE is relative to the dependent variable's scale. We will explore whether it makes sense to measure the relative error also.\n\nBefore starting let's import the necessary Python libraries and load the datasets:","4ce3fcc2":"Lets continue with the Garage* variables. In all of them NA means no garage, we can then impute the missing values. \n\n'GarageYrBlt' seems kind of irrelevant, because it should not be different in most cases to that of the building, therefore we will drop it.","53ec683d":"Most of the observations are NAs, i.e. no alley access. It is safe to drop the variable.","6b90ee7a":"In this case NA means no fireplace. Lets fill the NAs with 'NoFireplace', we will worry about the encoding in the next section. We will also drop the 'Fireplaces' variable, because the quality variable contains the interesting information (whether a house has a fireplace and of which quality)","748eb54e":"Lets check if we still have missing values:","b84956e0":"Mean and median are very near to one another. Lets just use the median:","ebdd8e5c":"# Feature Engineering\n\nNow we will proceed to further transform variables that have a not very useful empirical distribution, trying to make them useful for statistical learning. In the process we will create new variables. I will be using the EDA plots and empirical distributions.\n\nIn other kernels I have seen that they analize the normality in the distribution of all variables, including SalePrice. However, I have to say that not all models care about this, as in the case of SVM or tree-based models. It is only important if the objective is to do inference, and even then only the residuals should be analyzed. However, in the case of the dependent variable it can be appropiate, if there is a underlying reason. In our case it is logical to assume that the different observations for a house have an porcentual effect on the base price of the house. For example, having a central heating will increase the price of a house normally costing 100k to 110k, while it will increase a house costing 50k to 55k (i.e. 10% increase). That behaviour can be modeled easier with the log of the variable.\n\nIn previous versions of this kernel there was QQ-Plot and empirical distribution analysis of the price and log(price), but as said earlier that doesn't make sense. We will try transformed and untransformed variants in the modeling part (next kernel).","360271dc":"In this case NAs actually mean no fence. We could introduce a new value 'NoFence' for the NAs and all other values could be grouped together as 'Fence', obtaining a binary variable. Lets do that.","17377f19":"Now we will see if we can drop or put together features that are strongly correlated with one another.","b2b9cc2a":"Next we will analyze how many missing values do we have in the dataset, like in the kernel from Marcelino,","dd704d50":"# Preprocessing\n\nWe can then proceed to drop the outliers and useless variables.","da8b55f3":"Then we have the following options for each of the variables:\n1. Drop the observations (can lead to a large data loss).\n2. Drop the variable.\n3. Impute missing values.\n\nWe will look at them case by case (some of them as groups). First 'Alley':","65794078":"Lets continue with MasVnr*. In these case the NAs are real missing data, but we can impute them as type 'None'\/0, which is the most frequent value.","02c17509":"Wow, that was an overwhelming amount of information. Notice that some variables were read as numerical, eventhough they are categorical in nature. However, the scatterplots obtained from these variables were fairly useful, so we will leave them like they are.\n\nI have noticed the following:\n\n#### Outliers in numerical variables\nThese variables have some points that are sitting alone and don't follow the general trend. We will probably drop these **observations** (if they are few in total):\n* LotFrontage\n* LotArea\n* BsmtFinSF1\n* TotalBsmtSF\n* 1stFlrSF\n* GrLivArea\n\n#### Useless variables\nThe vast majority of the observations of these variables belong to the same class or value. They simply don't have enough observations in different classes to be able to generalize. Better drop them:\n* PoolArea\n* PoolQC\n* Street\n* Condition2\n* RoofMatl\n* Heating\n* MiscFeature\n* MiscVal\n* Utilities\n\n#### Outliers in categorical variables\nThere are a lot of \"outliers\" (actually, extreme values would be more correct) in each of the boxplots. However, take into account that we are only plotting two variables each time, so that the extreme values could be explained by others variables. Additionally, they are important in number, so we will include them in the modeling.\n\nLets see how small gets the dataset after dropping the extreme observations:"}}