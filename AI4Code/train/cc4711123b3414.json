{"cell_type":{"45f419b4":"code","338e047b":"code","5d512474":"code","81092560":"code","80d61516":"code","539ea882":"code","b38044a8":"code","e5192bf3":"code","fd94a450":"code","5f65d0bd":"code","91cedf42":"code","04cc90da":"code","951f2ed0":"code","dc0a5b25":"code","d80fb9da":"code","818e4131":"code","c4ad865b":"code","7ff6edcf":"code","4f04ec99":"code","9f647597":"code","66d3dbec":"code","19e9e3bb":"code","b2e21d00":"code","454cb642":"code","0b655830":"code","295df3d6":"code","20ddaf6b":"code","e595412e":"code","993ead45":"code","be3dc4df":"code","ed6834d3":"code","148fafa3":"code","6d21b038":"code","2af88d56":"code","504a854e":"code","14de7e4d":"code","677f845d":"code","1bf64053":"code","38130776":"code","22fe17ad":"code","b9bc4493":"code","10ca6e3f":"code","9135cbc8":"code","98ace5c6":"code","f685d798":"code","e6e189e6":"markdown"},"source":{"45f419b4":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport plotly.offline as pyoff\nimport plotly.express as  px\nfrom xgboost.sklearn import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import LabelEncoder, KBinsDiscretizer\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import StratifiedKFold, KFold\nfrom sklearn.metrics import accuracy_score\nfrom catboost import *","338e047b":"train = pd.read_csv('\/kaggle\/input\/Coustomer_sgmentation\/Train_aBjfeNk.csv')\ntest = pd.read_csv('\/kaggle\/input\/Coustomer_sgmentation\/Test_LqhgPWU.csv')\nsub = pd.read_csv('\/kaggle\/input\/Coustomer_sgmentation\/sample_submission_wyi0h0z.csv')","5d512474":"train.head()","81092560":"data = pd.concat([train,test], axis=0)\ndata.head()","80d61516":"for i in data.columns:\n    if data[i].isnull().sum() !=0:\n        print('this columns {} contain '.format(i),data[i].isnull().sum(),' null values')","539ea882":"data.select_dtypes(include='O').columns.tolist() # object dtypes columns","b38044a8":"# Convert Numeric NaNs to -999\nfeatures = [\"Age\", \"Work_Experience\", \"Family_Size\"]\ndata[features] = data[features].fillna(-999)","e5192bf3":"    \nmarry = {'Yes':0, 'No':1}\ndata['Ever_Married'] = data['Ever_Married'].fillna(2)\n\nfor i, j in marry.items():\n    data['Ever_Married'] = data['Ever_Married'].replace(i,j)","fd94a450":"gender_mapping = {\"Male\": 0, \"Female\": 1}\ndata[\"Gender\"] = data[\"Gender\"].fillna(2)\n\nfor gender, label in gender_mapping.items():\n    data[\"Gender\"] = data[\"Gender\"].replace(gender, label)\n    ","5f65d0bd":"grad = {'Yes':0, 'No':1}\ndata['Graduated'] = data['Graduated'].fillna(1)\n\nfor i,j in grad.items():\n    data['Graduated'] = data['Graduated'].replace(i,j)","91cedf42":"# Nan in profession could mean that person is unemployed\nprof_mapping = {\"Artist\": 0, \"Doctor\": 1, \"Engineer\": 2, \"Entertainment\": 3, \"Executive\": 4, \"Healthcare\": 5, \"Homemaker\": 6, \"Lawyer\": 7, \"Marketing\": 8}\ndata[\"Profession\"] = data[\"Profession\"].fillna(9)\n\nfor prof, label in prof_mapping.items():\n    data[\"Profession\"] = data[\"Profession\"].replace(prof, label)\n","04cc90da":"ss_mapping = {\"Low\": 0, \"Average\": 1, \"High\": 2}\ndata[\"Spending_Score\"] = data[\"Spending_Score\"].fillna(ss_mapping[\"Low\"])\n\nfor ss, label in ss_mapping.items():\n    data[\"Spending_Score\"] = data[\"Spending_Score\"].replace(ss, label)\n ","951f2ed0":"# NaN in Var1 is just another category\nvar1_mapping = {\"Cat_1\": 0, \"Cat_2\": 1, \"Cat_3\": 2, \"Cat_4\": 3, \"Cat_5\": 4, \"Cat_6\": 5, \"Cat_7\": 6}\ndata[\"Var_1\"] = data[\"Var_1\"].fillna(7)\n\nfor var1, label in var1_mapping.items():\n    data[\"Var_1\"] = data[\"Var_1\"].replace(var1, label)\n ","dc0a5b25":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ndata[\"Prof+Grad\"] = data[\"Profession\"]+data[\"Graduated\"]\n# data[\"Prof+Grad\"] = le.fit_transform(data[\"Prof+Grad\"])","d80fb9da":"data['prof+var'] = data['Profession'] + data['Var_1']\ndata['spend+family'] = data['Spending_Score'] + data['Family_Size']","818e4131":"temp = data.groupby(['Age']).agg({'Spending_Score':['count','mean','sum'],\n                                   'Work_Experience':['count','sum','min','max','mean'],\n                                   'Profession':['max','count'],\n                                           'Graduated':['count'],\n                                   'Ever_Married':['count'],\n                                    'Gender':['count'], \n                                       'Family_Size':['count','sum','max'],\n                                       'Age':['count'],\n                                    'Var_1':['count','max','min']})","c4ad865b":"temp.columns = ['_'.join(x) for x in temp.columns]\ntemp","7ff6edcf":"temp.skew()","4f04ec99":"data = pd.merge(data,temp,on=['Age'],how='left')\n","9f647597":"data","66d3dbec":"sa","19e9e3bb":"Train = data.iloc[:len(train), :]\nTest = data.iloc[len(train):, :]","b2e21d00":"# Finally label encode segmentation\nseg_mapping = {\"A\": 0, \"B\": 1, \"C\": 2, \"D\": 3}\nseg_mapping_rev = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\nfor seg, label in seg_mapping.items():\n    Train[\"Segmentation\"] = Train[\"Segmentation\"].replace(seg, label)","454cb642":"# 1 -> young, 2 -> middle-aged, 3 -> old, 4 -> retired and old\nTrain[\"Age_group\"] = [1 if i<=33 else 2 if i>33 and i<65 else 3 if i>=65 and i<74 else 4 for i in Train[\"Age\"].values]\nTest[\"Age_group\"] = [1 if i<=33 else 2 if i>33 and i<65 else 3 if i>=65 and i<74 else 4 for i in Test[\"Age\"].values]","0b655830":"# Bin the ID column and add as feature\nest = KBinsDiscretizer(n_bins=10, encode='ordinal', strategy='kmeans')\nTrain[\"Binned_ID\"] = est.fit_transform(np.reshape(Train[\"ID\"].values, (-1,1)))\nTest[\"Binned_ID\"] = est.transform(np.reshape(Test[\"ID\"].values, (-1,1)))\n","295df3d6":"Train","20ddaf6b":"# feature to specify as categoreis\ncat_feats = ['Gender', 'Ever_Married', 'Graduated', \"Var_1\", 'Profession', 'Age_group', \"Binned_ID\"]\ncat_feats_inds = [Train.columns.get_loc(c) for c in cat_feats]\ncat_feats_inds","e595412e":"X = Train.drop(columns=['ID','Segmentation'])\nY = Train['Segmentation']\n","993ead45":"Test.drop(columns=['ID','Segmentation'], inplace=True)","be3dc4df":"# CV: 1\nkfold = KFold(n_splits=5, random_state=27, shuffle=True)\nscores = list()\nfor train, test in kfold.split(X):\n    x_train, x_test = X.iloc[train], X.iloc[test]\n    y_train, y_test = Y[train], Y[test]\n    \n    model = LGBMClassifier(random_state=100, max_depth=3, n_estimators=200, learning_rate=0.1)\n    model.fit(x_train, y_train, categorical_feature=cat_feats_inds)\n    preds = model.predict(x_test)\n    \n    score = accuracy_score(y_test, preds)\n    scores.append(score)\n    print(\"Score: \", score)\nfirst_fold = sum(scores)\/len(scores)\nprint(\"\\nAverage Score: \", first_fold, \"\\n\\n\")","ed6834d3":"\n# CV: 2\noof = []\nkfold = StratifiedKFold(n_splits=10, random_state=27, shuffle=True)\nscores = list()\nfor train, test in kfold.split(X, Y):\n    x_train, x_test = X.iloc[train], X.iloc[test]\n    y_train, y_test = Y[train], Y[test]\n#     model = CatBoostClassifier(random_state=27,verbose = 0)\n    model = LGBMClassifier(class_weight = 'balanced', max_depth=8, n_estimators=200, learning_rate=0.3)\n    model.fit(x_train, y_train)\n   \n    preds = model.predict(x_test)\n    \n    score = accuracy_score(y_test, preds)\n    scores.append(score)\n    print(\"Score: \", score)\n    oof.append(model.predict_proba(Test))\n    \nsecond_fold = sum(scores)\/len(scores)\nprint(\"\\nAverage Score: \", second_fold)\nprint(\"\\n\\nFinal Average: \", first_fold*0.5 + second_fold*0.5)","148fafa3":"oof_cat = oof","6d21b038":"oof_lgb = oof\n","2af88d56":"oof_lgb[3]","504a854e":"oof_lgb = np.mean(oof_lgb, 0 )\n#     oof_cat = np.mean(oof_cat, 0 )","14de7e4d":"# ensemble Model\nfinal = oof_lgb","677f845d":"preds = [np.argmax(x) for x in final]","1bf64053":"# from scipy.stats import chi2_contingency\n# target_related_cols=[]\n# for i in cat_col:\n#    if i in train.columns:\n#       cross_table=pd.crosstab( train.loc[:,i],train[\"Segmentation\"])\n#       obs=cross_table.values\n#       chi2, p, dof, ex = chi2_contingency(obs, correction=False)\n#       if p < 0.05:\n#         print(\"Null statement:\" ,i ,\" is rejected \",np.round(p,5),\" which means it has some association with the target variable\")\n#         target_related_cols.append(i)\n# target_related_cols.pop()\n# print(target_related_cols)","38130776":"# from statsmodels.stats.outliers_influence import variance_inflation_factor\n# vif=pd.DataFrame()\n# vif[\"vif\"]=[variance_inflation_factor(X.values,i) for i in range(X.shape[1])]\n# vif[\"features\"]=X.columns\n# req_col=list(vif.query(\"vif<5\")[\"features\"])\n# req_col\n","22fe17ad":"# from imblearn.over_sampling import SMOTE\n# smote = SMOTE(sampling_strategy='auto')\n# x_train, y_train = smote.fit_sample(x_train, y_train)\n# print(x_train.shape,y_train.shape)","b9bc4493":"# from sklearn.model_selection import GridSearchCV\n# param_test1 = {\n# #     'colsample_bytree' :[0.5,0.6,0.7,0.8,0.9],\n#      'gamma':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9],\n# #     'max_depth':[1,2,3,4,5,6,7,8,9],\n#     'learning_rate':[0.001,0.01,0.03,0.1,0.3]\n# }\n# gsearch1 = GridSearchCV(estimator = XGBClassifier(),param_grid = param_test1,verbose=True,n_jobs=-1, cv=5)\n# gsearch1.fit(x_train, y_train)\n# gsearch1.best_params_","10ca6e3f":"len(preds)","9135cbc8":"sub['Segmentation'] = preds","98ace5c6":"sub['Segmentation'] = sub['Segmentation'].map({0:'A',\n                                              1:'B',\n                                              2:'C',\n                                              3:'D'})","f685d798":"sub.to_csv('sub.csv')","e6e189e6":"# Data Exploration"}}