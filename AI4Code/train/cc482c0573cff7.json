{"cell_type":{"c81af2fa":"code","a21cc7fd":"code","9e2a6917":"code","5f815211":"code","4ddba934":"code","3e0c64aa":"code","a80dc452":"code","3b825c08":"code","dffefc3b":"code","679e8b89":"code","9035284d":"code","fa5991c9":"code","b8473b90":"code","726b8ed0":"code","29ea8e3d":"code","93e4ca92":"markdown","a3792272":"markdown","3cc9de2c":"markdown","a4f9cd51":"markdown","5ea8485c":"markdown","9157e2f3":"markdown","20e594da":"markdown","0436afff":"markdown","7d9cd816":"markdown","2d92a4b0":"markdown","d3d36e5b":"markdown","1673fd0b":"markdown","e6163f98":"markdown","db54e7bd":"markdown"},"source":{"c81af2fa":"#core packages\nimport os \nimport numpy as np\nimport pandas as pd\nimport warnings\n\n#visualisation\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib import ticker\nimport plotly.express as px\nimport seaborn as sns\nplt.rcParams['figure.dpi'] = 600\npd.set_option('display.max_rows', None)\npd.set_option('display.max.columns', None)\npd.set_option('float_format', '{:f}'.format)\nwarnings.filterwarnings('ignore')\n\n#reduce memory usage conversions\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int8','int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","a21cc7fd":"data = pd.read_csv('..\/input\/top-100-korean-drama-mydramalist\/top100_kdrama.csv')\nprint(data.shape)\ndata.head()","9e2a6917":"data.dtypes","5f815211":"print(f'Number of rows: {data.shape[0]};  Number of columns: {data.shape[1]}; No of missing values: {sum(data.isna().sum())}')","4ddba934":"#GENRE Encoding\ng = []\nfor genres in data['Genre']:\n    G = genres.split(',', -1)\n    for i, genre in enumerate(G):\n        if genre.strip() not in g:\n            g.append(G[i].strip())\n        else:\n            pass\ng.sort()\n\nfor genre in g:\n    data[f'Genre_{genre}'] = np.zeros((100,), dtype = int)\ncounter = 0\nfor genres in data['Genre']:\n    G = genres.split(',', -1)\n    for i, genre in enumerate(G):\n        for gen in g:\n            if G[i].strip() == gen:\n                data[f'Genre_{gen}'][counter] = 1 \n            else:\n                pass\n    counter +=1\ndata.drop(['Genre'], axis = 1, inplace = True)\n\n#TAG Encoding\nt = []\nfor tags in data['Tags']:\n    T = tags.split(',', -1)\n    for i, tag in enumerate(T):\n        if tag.strip() not in t:\n            t.append(T[i].strip())\n        else:\n            pass\nt.sort()\n\nfor tag in t:\n    data[f'Tag_{tag}'] = np.zeros((100,), dtype = int)\ncounter = 0\nfor tags in data['Tags']:\n    T = tags.split(',', -1)\n    for i, tag in enumerate(T):\n        for ta in t:\n            if T[i].strip() == ta:\n                data[f'Tag_{ta}'][counter] = 1 \n            else:\n                pass\n    counter +=1\ndata.drop(['Tags'], axis = 1, inplace = True)\n\n#CAST Encoding\nc = []\nfor cast in data['Cast']:\n    C = cast.split(',', -1)\n    for i, cas in enumerate(C):\n        if cas.strip() not in c:\n            c.append(C[i].strip())\n        else:\n            pass\nc.sort()\n\nfor actor in c:\n    data[f'{str(actor).replace(\" \", \"_\").lower()}'] = np.zeros((100,), dtype = int)\ncounter = 0\nfor actor in data['Cast']:\n    C = actor.split(',', -1)\n    for i, act in enumerate(C):\n        for A in c:\n            if C[i].strip() == A:\n                data[f'{str(A).replace(\" \", \"_\").lower()}'][counter] = 1 \n            else:\n                pass\n    counter +=1\ndata.drop(['Cast'], axis = 1, inplace = True)\n\ndata.head()\nprint(data.shape)","3e0c64aa":"data['Content Rating'].value_counts() # Yes so after looking at this I want to conver it to a Integer Encoded column. ","a80dc452":"import sklearn\nfrom sklearn.preprocessing import OrdinalEncoder\nenc = OrdinalEncoder()\nRating = np.array(data['Content Rating'])\ndata['Content Rating'] = enc.fit_transform(Rating.reshape(-1,1)) # 2 = 18+, 1 = 15+, 0 = 13+\ndata.head()","3b825c08":"def t_converter(duration):\n    duration = duration.strip(\" min.\").replace(\" hr. \",\":\")\n    # SHould now have a string of this formate'X:YY'\n    T = duration.split(':')\n    if len(T)==2:\n        hours = int(T[0])*60\n        mins = int(T[1])\n        time = hours +mins\n    else:\n        time = int(T[0]) #this only has minutes\n    return time\nfor i, run in enumerate(data['Duration']):\n    data['Duration'][i] = t_converter(run)\ndata.head()\ndata.rename(columns = {'Duration':'Duration\/min'}, inplace = True)","dffefc3b":"for i, row in enumerate(data['Network']):\n    networks = str(row)\n    networks = networks.split(',')\n    data['Network'][i] = networks[0].strip()\ndata.head()","679e8b89":"data['Network'].value_counts()","9035284d":"data['Air Day 1'] = np.zeros((100,))\ndata['Air Day 2'] = np.zeros((100,))\ndata['Air Day 3'] = np.zeros((100,))\nfor i, col in enumerate(data['Aired On']):\n    days = col.split(\", \")\n    if len(days)== 3:\n        data['Air Day 1'][i] = days[0].strip()\n        data['Air Day 2'][i] = days[1].strip()\n        data['Air Day 3'][i] = days[2].strip()\n    elif len(days) == 2:\n        data['Air Day 1'][i] = days[0].strip()\n        data['Air Day 2'][i] = days[1].strip()\n        data['Air Day 3'][i] = np.nan\n    else:\n        data['Air Day 1'][i] = days[0].strip()\n        data['Air Day 2'][i] = np.nan\n        data['Air Day 3'][i] = np.nan\ndata.drop(['Aired On'], axis = 1, inplace = True)","fa5991c9":"cols = ['Air Day 1', 'Air Day 2', 'Air Day 3']\ndummies = pd.get_dummies(data[cols])\ndata = pd.concat([data, dummies], axis = 1)\ndata.drop(cols, axis =1, inplace = True)\ndata.head()","b8473b90":"import datetime \ndata['First Aired'] = np.nan \ndata['Last Aired'] = np.nan\nfor i, row in enumerate(data['Aired Date']):\n    dates = row.split(' - ')\n    if len(dates)>1:\n        data['First Aired'][i] = datetime.datetime.strptime(dates[0], '%b %d, %Y').strftime('%d\/%m\/%y')\n        data['Last Aired'][i] = datetime.datetime.strptime(dates[1], '%b %d, %Y').strftime('%d\/%m\/%y')\n    else:\n        data['First Aired'][i] = datetime.datetime.strptime(dates[0], '%b %d, %Y').strftime('%d\/%m\/%y')\ndata.drop('Aired Date', axis =1, inplace = True)\ndata.head()","726b8ed0":"for i, rank in enumerate(data['Rank']):\n    R = rank.strip('#')\n    data['Rank'][i] = int(R)\ndata.drop('Synopsis', axis = 1, inplace = True)\ndata = reduce_mem_usage(data)\ndata.head()","29ea8e3d":"data.to_csv('top_100_k_drama_clean.csv', index = False, header = True)","93e4ca92":"<a id = '2.1.5'><\/a>\n### 2.1.5 Converting Aired On\nNeed to go through this column and; firstly - split it into multiple columns, `Air Date 1`, `Air Date 2` etc.\nAfter that One-Hot Encode all columns. Is easier than trying to Integer encode all three columns and is also easier to intepret.","a3792272":"<a id = '2.1.3'><\/a>\n### 2.1.3 Converting Duration\nNeed to convert duration from Xhr Ymin to Zmins","3cc9de2c":"Now we have a very big dataset","a4f9cd51":"[back to top](#table-of-contents)\n<a id = '2'><\/a>\n# 2. Data Preparation","5ea8485c":"Okay so that has created a cleaned up and much larger csv","9157e2f3":"<a id = '2.1.6'><\/a>\n### 2.1.6 Converting Aired Date\nNeed to convert the Aired Date column to two columns: `First Aired`, `Last Aired`. Also want to convert that column to datetime. day-month-year","20e594da":"<a id = '2.1.2'><\/a>\n### 2.1.2 Converting Content Rating\nWe gonna see the unique values for content rating and decide what to do. Maybe change it into a integer encoded column where rather than 15, 18 etc we have like 1, 2 ","0436afff":"# Table of Contents\n<a id='table-of-contents'><\/a>\n- [1 Introduction](#1)\n- [2 Data Preparation](#2)\n    - [2.1](#2.1)\n         -[2.1.1](#2.1.1)\n         -[2.1.2](#2.1.2)\n         -[2.1.3](#2.1.3)\n         -[2.1.4](#2.1.4)\n         -[2.1.5](#2.1.5)\n         -[2.1.6](#2.1.6)\n         -[2.1.7](#2.1.7)\n     ","7d9cd816":"[back to top](#table-of-contents)\n<a id='1'><\/a>\n# 1. Introduction\nIn this notebook the top 100 Kdramas csv will be cleaned and prepped so that I can perform some EDA on it and then eventually build a recommender system for it.\n## 1.1 Preloading packages","2d92a4b0":"<a id = '2.1.4'><\/a>\n### 2.1.4 Converting Network\nAm going to assume that the first network in each list is the original air network.","d3d36e5b":"<a id = '2.1.7'><\/a>\n### 2.1.7 Last Tidy\nLastly want to get rid of the number symbol in the rank column and drop the synopsis column as it won't be needed for any EDA","1673fd0b":"**Notes on data prep to be done:**\n- `Aired date` should be split into two columns: `Aired on`, `Final Episode` or something similar\n- `Network` should be split into `Network 1`, `Network 2`, etc..\n- `Duration` should be changed from X hr X min to an integer that is equal to minutes. \n- `Content Rating` just a rumber, maybe an extra column for explanation but not really necessary\n- `Cast` can be split into multiple columns\n- `Genre` split into multiple columns\n- `Tags` into multiple columns \n- Any dates changed to proper datetime format ","e6163f98":"<a id='2.1'><\/a>\n## 2.1 Data cleaning\n<a id = '2.1.1'><\/a>\n### 2.1.1 Converting Cast, Genre, and Tags","db54e7bd":"No missing values, nice."}}