{"cell_type":{"1df1e4f2":"code","a49c5a25":"code","9db30a6b":"code","53807e63":"code","e412865b":"code","1ca49dcf":"code","32d5a016":"code","4f259641":"code","01253aba":"code","16cdb8e6":"code","c85e4064":"code","25b79872":"code","284db321":"code","ae77d6c7":"code","ee0b9e81":"code","3ecca86c":"code","050a3352":"code","e4e2494a":"code","d4d804ea":"code","f01d4844":"code","0dc3dffa":"code","95d15e70":"code","7e69e723":"code","01eeccc9":"code","df9594c6":"code","e773e713":"code","98245382":"code","72b7619c":"code","c4786944":"code","0ed4c101":"code","33b5e7d1":"code","4b922eb3":"code","ef733fe0":"markdown","befb1821":"markdown","74b69632":"markdown","a762a1d5":"markdown","0ec92b1f":"markdown","3baefeff":"markdown","3374b361":"markdown","335547bf":"markdown","04f9072a":"markdown","57839718":"markdown","a88a6e5a":"markdown","5ff0edb2":"markdown","58ebc573":"markdown","178872c4":"markdown"},"source":{"1df1e4f2":"import re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n%matplotlib inline\nimport seaborn as sns\n\nfrom PIL import Image\nfrom wordcloud import WordCloud\n\nimport spacy\nimport nltk\nfrom nltk.util import ngrams\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LinearRegression\nfrom lightgbm import LGBMRegressor, LGBMClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, f1_score, classification_report, mean_absolute_error, r2_score\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\n\nimport lime\nfrom lime import lime_text\nfrom lime.lime_text import LimeTextExplainer\nfrom sklearn.pipeline import make_pipeline","a49c5a25":"data = pd.read_csv('\/kaggle\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv')\ndata.shape","9db30a6b":"data.head()","53807e63":"# Looks like there are 16 objects with no description.\ndata[data['name'].isnull()]","e412865b":"# Fill NaN with an empty string and check missing values again\ndata['name'].fillna('', inplace=True)\ndata['name'].isnull().sum()","1ca49dcf":"def remove_punct(line):\n    return re.sub('[^A-Za-z]+', ' ', line).lower()\n\ndata['clean_name'] = data['name'].apply(remove_punct)","32d5a016":"# Let's compare raw and cleaned texts.\ndata[['name', 'clean_name']]","4f259641":"nlp = spacy.load(\"en\")\nstopwords = nlp.Defaults.stop_words","01253aba":"def tokenize_no_stopwords(line):\n    tokens = nltk.tokenize.word_tokenize(line)\n    tokens_no_stop = [w for w in tokens if w not in stopwords]\n    return \" \".join(tokens_no_stop)","16cdb8e6":"data['final_name'] = data['clean_name'].apply(tokenize_no_stopwords)","c85e4064":"# Well, looks about right\ndata[['clean_name', 'final_name']]","25b79872":"# first, we need to concatenate all descriptions in one string\ntext = \"\"\nfor i in data['final_name']:\n    text += \" \" + i\n    \n# next, we tokenize it into separate words\ntokenized_text = nltk.tokenize.word_tokenize(text)\n\n# finally, create a frequency dictionary with the help of nltk\nfreq_dict = nltk.FreqDist(w for w in tokenized_text)","284db321":"def plot_most_common(dict_data ,title):\n    df = pd.DataFrame(dict_data)\n    df.columns = ['word', 'count']\n    plt.figure(figsize=(8, 8))\n    sns.set(style=\"darkgrid\")\n    sns.barplot(x=\"count\", y=\"word\", data=df, palette='twilight')\n    plt.title(title)\n    plt.show()\n    \nplot_most_common(freq_dict.most_common(20), 'Top 20 frequent words for NYC Airbnb titles')","ae77d6c7":"freq_dict_bigrams = nltk.FreqDist(nltk.bigrams(w for w in tokenized_text))\nplot_most_common(freq_dict_bigrams.most_common(20), 'Top 20 frequent bigrams for NYC Airbnb titles')","ee0b9e81":"freq_dict_trigrams = nltk.FreqDist(nltk.trigrams(w for w in tokenized_text))\nplot_most_common(freq_dict_trigrams.most_common(20), 'Top 20 frequent trigrams for NYC Airbnb titles')","3ecca86c":"# First, we are going to take a look at the price distribution\nplt.figure(figsize=(8, 8))\nsns.distplot(data['price'])\nplt.show()","050a3352":"data['price'].describe()","e4e2494a":"costly = data[data['price']>1000]\ncostly.shape","d4d804ea":"# first, we need to concatenate all descriptions in one string\ncostly_text = \"\"\nfor i in costly['final_name']:\n    costly_text += \" \" + i\n    \n# next, we tokenize it into separate words\ntokenized_costly_text = nltk.tokenize.word_tokenize(costly_text)\n\n# finally, create a frequency dictionary with the help of nltk\nfreq_dict_costly = nltk.FreqDist(w for w in tokenized_costly_text)","f01d4844":"plot_most_common(freq_dict_costly.most_common(20), \"Top 20 words in pricy apartments' titles\")","0dc3dffa":"freq_dict_bigrams_costly = nltk.FreqDist(nltk.bigrams(w for w in tokenized_costly_text))\nplot_most_common(freq_dict_bigrams_costly.most_common(20), \"Top 20 bigrams in pricy apartments' titles\")","95d15e70":"freq_dict_trigrams_costly = nltk.FreqDist(nltk.trigrams(w for w in tokenized_costly_text))\nplot_most_common(freq_dict_trigrams_costly.most_common(20), \"Top 20 trigrams in pricy apartments' titles\")","7e69e723":"def new_target(line):\n    if line > 500:\n        return 1\n    else:\n        return 0\n        \ndata['target'] = data['price'].apply(new_target)\ndata['target'].value_counts()","01eeccc9":"train, test = train_test_split(data, test_size=0.2, random_state=315, stratify=data['target'])\n\nX_train, y_train = train['final_name'], train['target']\nX_test, y_test = test['final_name'], test['target']\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape","df9594c6":"vect = TfidfVectorizer()\nX_train = vect.fit_transform(X_train)\nX_test = vect.transform(X_test)","e773e713":"ros = RandomOverSampler(sampling_strategy='minority', random_state=1)\n\nX_train_ros, y_train_ros = ros.fit_sample(X_train, y_train)\nnp.bincount(y_train_ros)","98245382":"lr = LGBMClassifier(random_state=315)\nlr.fit(X_train_ros, y_train_ros)\npreds = lr.predict(X_test)","72b7619c":"print(classification_report(y_test, preds))","c4786944":"def draw_cm(y_test, y_pred):\n  cm = confusion_matrix(y_test, y_pred)\n  cm_norm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n  df_cm = pd.DataFrame(cm_norm)\n  plt.figure(figsize = (6,4))\n  sns.heatmap(df_cm, annot=True, cmap=\"Blues\")\n  plt.xlabel(\"Predicted class\")\n  plt.ylabel(\"True class\")\n  print(\"Accuracy: {0:.3f}\".format(accuracy_score(y_test, y_pred)))\n  print(\"Recall: {0:.3f}\".format(recall_score(y_test, y_pred)))\n  plt.show()","0ed4c101":"draw_cm(y_test, preds)","33b5e7d1":"# create a pipeline and a LIME's explainer object\nc = make_pipeline(vect, lr)\nclass_names=['cheaper', 'expensive']\nexplainer = LimeTextExplainer(class_names=class_names)\n\n# use them to explain an individual prediction\n\nind = 48050\nexp = explainer.explain_instance(test.loc[ind]['final_name'], c.predict_proba, labels=[1])\nexp.show_in_notebook(text=True)\n\nind_pred = c.predict_proba([test.loc[ind]['final_name']])\n\nprint(\"True class: {}\".format(class_names[test.loc[ind]['target']]))\nprint(\"Predicted class: {}\".format(class_names[np.argmax(ind_pred)]))","4b922eb3":"ind = 19427\nexp = explainer.explain_instance(test.loc[ind]['final_name'], c.predict_proba, labels=[1])\nexp.show_in_notebook(text=True)\n\nind_pred = c.predict_proba([test.loc[ind]['final_name']])\n\nprint(\"True class: {}\".format(class_names[test.loc[ind]['target']]))\nprint(\"Predicted class: {}\".format(class_names[np.argmax(ind_pred)]))","ef733fe0":"Let's look at the columns of our data.\n\n* id - Listing ID\n* name - Listing Title\n* host_id - ID of Host\n* host_name - Name of Host\n* neighbourhood_group - Borough that contains listing\n* neighbourhood - Name of neighbourhood that listing is in\n* latitude - latitude of listing\n* longitude - longitude of listing\n* room_type - Type of public space that is being offered\n* price - price per night, USD\n* minimum_nights - minimum number of nights required to book listing\n* number_of_reviews - total number of reviews that listing has accumulated\n* last_review - date in which listing was last rented\n* reviews_per_month - total number of reviews divided by the number of months the listing is active\n* calculated_host_listings_count - amount of listing per host**\n* availability_365 - number of days per year the listing is active\n\nAs we are interested in textual descriptions of appartments, we need to preprocess data in `name` column for further use.\n\n## <a id='prep'><\/a> <font color='orange'> Preprocessing\n### <font color='orange'> 1. Check if there are missing values. If so, swap it with an empty string.\n","befb1821":"![language_crop.jpg](attachment:language_crop.jpg)\n\n*Credits*: <span>Photo by <a href=\"https:\/\/unsplash.com\/@ninjason?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Jason Leung<\/a> on <a href=\"https:\/\/unsplash.com\/s\/photos\/language?utm_source=unsplash&amp;utm_medium=referral&amp;utm_content=creditCopyText\">Unsplash<\/a><\/span>\n\nThe place of language in our lives is hard to overestimate. It helps us to convey meaning and not rarely serves as a means of persuasion.\n\nHave you ever noticed that words may be a good indicator of the cost? \"Exclusive\", \"brand-new\", \"luxurious\" and similar terms signal that the service or the product described will be rather expensive. \n\nWell, if it's in fact so, can we create a program that would predict prices by product names? Sure, we can!\n\nToday we'll use NLP to understand whether there are particular wordings that indicate high end prices for Airbnb places in New Yourk City.\n\n\n[Preprocessing](#prep)\n\n[Most frequent words](#freq_words)\n\n[Most frequent bigrams](#freq_bigrams)\n\n[Most frequent trigrams](#freq_trigrams)\n\n[But what about a price?](#price)\n\n[Most frequent wordings for expensive places](#top_costly_words)\n\n[ML model](#model)\n\n[Interpreting ML predictions](#interpret)\n\n[Wrapping up](#wrapping_up)\n\n\nFirst, let's load the data and take a quick look at it.","74b69632":"## <a id='freq_bigrams'><\/a> <font color='purple'> Most frequent bigrams to describe NYC appartments.\n\nMost of top 20 words obtained in the previous section aren't really specific for New York. Now we'll find top bigrams, i.e. the sequences of two neighbouring words, used in Airbnb titles for NYC flats.","a762a1d5":"### <font color='orange'> 2. Remove punctuation, digits and special characters.","0ec92b1f":"### <font color='orange'> 3. Get rid of stopwords, i.e. words conveying no meaning such as articles, conjunctions, etc. ","3baefeff":"Since the classes are not balanced (there are much more objects of class 0), we'll use resampling.\n\nMore details on dealing with imabalanced data can be found in [this notebook](https:\/\/www.kaggle.com\/amidala\/handling-imbalanced-data-with-resampling)\n","3374b361":"## <a id='freq_words'><\/a> <font color='purple'> Most frequent words to describe NYC appartments.\n    \nLet's take a look at what words are usually used by owners to describe their places.","335547bf":"By looking at the plots above, it's easy to identify some kind of pattern. We encounter some terms that weren't present in the list of frequently used words when we were looking at all the data.\n\nTitles for appartments with high end prices uses such words as:\n* lux, luxury\n* Manhattan\n* Townhouse\n* Chelsea Gallery\n* Super Bowl\n* park views\n\nSeems right to me. You *do* expect an appartment with a park view or near Chelsea Gallery to be a bit more expensive than a median price tag. The same for the term \"luxury\".\n\n## <a id='model'><\/a> <font color='navy'> ML model\n\nNow let's create an ML model that will try to answer the question \"Is the price of an Airbnb place in New York higher than 500 a night?\" by utilizing only the title describing an appartment.","04f9072a":"Alright, the scores aren't perfect but taking into account that the predicting model is built solely on textual descriptions of an appatment, it seems like the words in Airbnb titles actually **do** matter!\n\n\n## <a id='interpret'><\/a> <font color='purple'> Interpreting ML predictions\n\nLet's create human-friendly interpretations for our prediction using [LIME](https:\/\/github.com\/marcotcr\/lime).\n\nMore details on how to interpret ML models' predictions is in [this kernel](https:\/\/www.kaggle.com\/amidala\/explain-your-ml-model-no-more-black-boxes)","57839718":"Most apparntments can be rented with approximately \\$200. However, the right tail is quite long meaning there are places with much higher prices.\n\n## <a id='top_costly_words'><\/a> <font color='scarlett'> Most frequent wordings for expensive places\n\nLet's explore the titles of the appartments that cost more than 1k for a night.","a88a6e5a":"## <a id='price'><\/a> <font color='green'> But what about the price?\n\nCan we predict, at least approximately, how much a place will cost us just by looking at its Airbnb title?\n\nWell, let's find out.","5ff0edb2":"Well, now the plot looks much more specific for the city. We can see such bigrams as:\n* East and West Village\n* Central Park\n* New York\n* Upper East","58ebc573":"## <a id='freq_trigrams'><\/a> <font color='purple'> Most frequent trigrams to describe NYC appartments.\n\nAs long as we are here and looking at n-grams used in Airbnb titles, let's create a plot for top trigrams.\n    \nWe find a very interesting trigram indeed - hell's kitchen - the name of a popular NYC area.","178872c4":"The interpretation shows us that such terms as `bluebird`, `luxury`, `riverview` do appear in the titles of more expensive places. The word `group` is also frequently encountered in descriptions of a high price apartments as this usually indicates that the place is large and may be a better suit for several people.\n\n## <a id='wrapping_up'><\/a> <font color='green'> Wrapping up \n\nIn this notebook we have identified interesting patterns. For instance:\n* It's possible to guess the price range of an Airbnb apartment by simply looking at its title.\n* More expensive places use such words as `luxury`, `elegant`, `views`, `groups`, etc.\n* Flats which titles indicate the vicinity of city attractions also tend to cost more.\n    \nLooks like it's important what to write in the title of your Airbnb place after all!\n    \nHope you find this quick analysis helpful and funny. If so, upvote :)"}}