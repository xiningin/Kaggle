{"cell_type":{"5c5b1d42":"code","2bbc5fba":"code","26a831c5":"code","364eaf1b":"code","2b750647":"code","b8f4abca":"code","21a0fd0b":"code","18a04cb9":"code","6c652377":"code","26ca7f42":"code","1d57a7b2":"code","a108c336":"code","1f7cdbd9":"code","2cd76b5a":"code","a707c8f6":"code","aec211f6":"code","13a8f23c":"code","e4a72b1c":"code","9e4da843":"code","979fd843":"code","1c4e5290":"code","9d374417":"code","5ca24508":"code","b54c9c08":"code","363bafb2":"code","107a889e":"code","dff0d4e5":"code","3b234553":"code","eee44201":"code","b01b009c":"code","39001942":"code","48fa3309":"code","aa533c05":"code","7a517017":"markdown","a72ab56d":"markdown","083a8a3a":"markdown","ea688889":"markdown","fefe614b":"markdown","b267aae6":"markdown"},"source":{"5c5b1d42":"import numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\nimport pandas as pd \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, mean_squared_error, log_loss\n\nfrom tqdm import tqdm_notebook\nimport seaborn as sns\nimport imageio\nimport time \nfrom IPython.display import HTML\n\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.datasets import make_blobs","2bbc5fba":"my_cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [\"red\",\"yellow\",\"green\"])","26a831c5":"np.random.seed(0)","364eaf1b":"data, labels = make_blobs(n_samples=1000, centers=4, n_features=2, random_state=0)\nprint(data.shape, labels.shape)","2b750647":"plt.scatter(data[:,0], data[:,1], c=labels, cmap = my_cmap)\nplt.show()","b8f4abca":"labels_orig = labels\nlabels = np.mod(labels_orig, 2)","21a0fd0b":"plt.scatter(data[:,0], data[:,1], c=labels, cmap = my_cmap)\nplt.show()","18a04cb9":"X_train, X_val, Y_train, Y_val = train_test_split(data, labels_orig, stratify=labels_orig, random_state=0)\nprint(X_train.shape, X_val.shape, labels_orig.shape)","6c652377":"enc = OneHotEncoder()\n# 0 -> (1, 0, 0, 0), 1 -> (0, 1, 0, 0), 2 -> (0, 0, 1, 0), 3 -> (0, 0, 0, 1)\ny_OH_train = enc.fit_transform(np.expand_dims(Y_train,1)).toarray()\ny_OH_val = enc.fit_transform(np.expand_dims(Y_val,1)).toarray()\nprint(y_OH_train.shape, y_OH_val.shape)","26ca7f42":"W1 = np.random.randn(2,2)\nW2 = np.random.randn(2,4)\nprint(W1)\nprint(W2)","1d57a7b2":"class FFNetwork:\n    \n    def __init__(self, W1, W2):\n        \n        self.params={}\n        self.params[\"W1\"]=W1.copy()\n        self.params[\"W2\"]=W2.copy()\n        self.params[\"B1\"]=np.zeros((1,2))\n        self.params[\"B2\"]=np.zeros((1,4))\n        self.num_layers=2\n        self.gradients={}\n        self.update_params={}\n        self.prev_update_params={}\n        for i in range(1, self.num_layers+1):\n            self.update_params[\"v_w\"+str(i)]=0\n            self.update_params[\"v_b\"+str(i)]=0\n            self.update_params[\"m_b\"+str(i)]=0\n            self.update_params[\"m_w\"+str(i)]=0\n            self.prev_update_params[\"v_w\"+str(i)]=0\n            self.prev_update_params[\"v_b\"+str(i)]=0\n    \n    def forward_activation(self, X):\n        return 1.0\/(1.0 + np.exp(-X))\n    \n    def grad_activation(self, X):\n        return X*(1-X)\n    \n    def softmax(self, X):\n        exps = np.exp(X)\n        return exps \/ np.sum(exps, axis=1).reshape(-1,1)\n    \n    def forward_pass(self, X, params = None):\n        if params is None:\n            params = self.params\n        self.A1 = np.matmul(X, params[\"W1\"]) + params[\"B1\"] # (N, 2) * (2, 2) -> (N, 2)\n        self.H1 = self.forward_activation(self.A1) #(N, 2)\n        self.A2 = np.matmul(self.H1, params[\"W2\"]) + params[\"B2\"] # (N, 2) * (2, 4) -> (N, 4)\n        self.H2 = self.softmax(self.A2) # (N, 4)\n        return self.H2\n    \n    def grad(self, X, Y, params = None):\n        if params is None:\n            params = self.params\n        \n        self.forward_pass(X, params)\n        m = X.shape[0]\n        self.gradients[\"dA2\"] = self.H2 - Y # (N, 4) - (N, 4) -> (N, 4)\n        self.gradients[\"dW2\"] = np.matmul(self.H1.T, self.gradients[\"dA2\"]) # (2, N) * (N, 4) -> (2, 4)\n        self.gradients[\"dB2\"] = np.sum(self.gradients[\"dA2\"], axis=0).reshape(1, -1) # (N, 4) -> (1, 4)\n        self.gradients[\"dH1\"] = np.matmul(self.gradients[\"dA2\"], params[\"W2\"].T) # (N, 4) * (4, 2) -> (N, 2)\n        self.gradients[\"dA1\"] = np.multiply(self.gradients[\"dH1\"], self.grad_activation(self.H1)) # (N, 2) .* (N, 2) -> (N, 2)\n        self.gradients[\"dW1\"] = np.matmul(X.T, self.gradients[\"dA1\"]) # (2, N) * (N, 2) -> (2, 2)\n        self.gradients[\"dB1\"] = np.sum(self.gradients[\"dA1\"], axis=0).reshape(1, -1) # (N, 2) -> (1, 2)\n\n    def fit(self, X, Y, epochs=1, algo=\"GD\", display_loss=False, eta=1, mini_batch_size=100, eps=1e-8,beta=0.9, beta1=0.9, beta2=0.9, gamma=0.9):\n        \n        if display_loss:\n            loss={}\n            for num_epoch in tqdm_notebook(range(epochs), total=epochs, unit=\"epoch\"):\n                m = X.shape[0]\n                \n                if algo == \"GD\":\n                    self.grad(X,Y)\n                    for i in range (1, self.num_layers+1):\n                        self.params[\"W\"+str(i)] -= eta * (self.gradients[\"dW\"+str(i)]\/m)\n                        self.params[\"B\"+str(i)] -= eta * (self.gradients[\"dB\"+str(i)]\/m)\n                \n                elif algo == \"MiniBatch\":\n                    for k in range(0, m, mini_batch_size):\n                        self.grad(X[k:k+mini_batch_size], Y[k:k+mini_batch_size])\n                        for i in range(1, self.num_layers+1):\n                            self.params[\"W\"+str(i)] -= eta * (self.gradients[\"dW\"+str(i)]\/mini_batch_size)\n                            self.params[\"B\"+str(i)] -= eta * (self.gradients[\"dB\"+str(i)]\/mini_batch_size)\n                        \n                    \n                elif algo == \"Momentum\":\n                    self.grad(X,Y)\n                    for i in range(1, self.num_layers+1):\n                        self.update_params[\"v_w\"+str(i)] = gamma * self.update_params[\"v_w\"+str(i)] + eta * (self.gradients[\"dW\"+str(i)]\/m)\n                        self.update_params[\"v_b\"+str(i)] = gamma * self.update_params[\"v_b\"+str(i)] + eta * (self.gradients[\"dB\"+str(i)]\/m)\n                        self.params[\"W\"+str(i)] -= self.update_params[\"v_w\"+str(i)]\n                        self.params[\"B\"+str(i)] -= self.update_params[\"v_b\"+str(i)]\n                        \n                elif algo == \"NAG\":\n                    temp_params = {}\n                    for i in range(1, self.num_layers+1):\n                        self.update_params[\"v_w\"+str(i)] = gamma * self.prev_update_params[\"v_w\"+str(i)]\n                        self.update_params[\"v_b\"+str(i)] = gamma * self.prev_update_params[\"v_b\"+str(i)]\n                        temp_params[\"W\"+str(i)] = self.params[\"W\"+str(i)]-self.update_params[\"v_w\"+str(i)]\n                        temp_params[\"B\"+str(i)] = self.params[\"B\"+str(i)]-self.update_params[\"v_b\"+str(i)]\n                    self.grad(X, Y, temp_params)\n                    for i in range(1, self.num_layers+1):\n                        self.update_params[\"v_w\"+str(i)] = gamma * self.update_params[\"v_w\"+str(i)] + eta * (self.gradients[\"dW\"+str(i)]\/m)\n                        self.update_params[\"v_b\"+str(i)] = gamma * self.update_params[\"v_b\"+str(i)] + eta * (self.gradients[\"dB\"+str(i)]\/m)\n                        self.params[\"W\"+str(i)] -= eta * (self.update_params[\"v_w\"+str(i)])\n                        self.params[\"B\"+str(i)] -= eta * (self.update_params[\"v_b\"+str(i)])\n                    self.prev_update_params = self.update_params\n                \n                elif algo == \"AdaGrad\":\n                    self.grad(X, Y)\n                    for i in range(1, self.num_layers+1):\n                        self.update_params[\"v_w\"+str(i)] += (self.gradients[\"dW\"+str(i)]\/m)**2\n                        self.update_params[\"v_b\"+str(i)] += (self.gradients[\"dB\"+str(i)]\/m)**2\n                        self.params[\"W\"+str(i)] -= (eta\/(np.sqrt(self.update_params[\"v_w\"+str(i)])+eps)) * (self.gradients[\"dW\"+str(i)]\/m)\n                        self.params[\"B\"+str(i)] -= (eta\/(np.sqrt(self.update_params[\"v_b\"+str(i)])+eps)) * (self.gradients[\"dB\"+str(i)]\/m)\n                    \n            \n                elif algo == \"RMSProp\":\n                    self.grad(X, Y)\n                    for i in range(1, self.num_layers+1):\n                        self.update_params[\"v_w\"+str(i)] = beta * self.update_params[\"v_w\"+str(i)] + (1-beta) * ((self.gradients[\"dW\"+str(i)]\/m)**2)\n                        self.update_params[\"v_b\"+str(i)] = beta * self.update_params[\"v_b\"+str(i)] + (1-beta) * ((self.gradients[\"dB\"+str(i)]\/m)**2)\n                        self.params[\"W\"+str(i)] -= (eta\/(np.sqrt(self.update_params[\"v_w\"+str(i)]+eps)))*(self.gradients[\"dW\"+str(i)]\/m)\n                        self.params[\"B\"+str(i)] -= (eta\/(np.sqrt(self.update_params[\"v_b\"+str(i)]+eps)))*(self.gradients[\"dB\"+str(i)]\/m)\n                \n                elif algo == \"Adam\":\n                    self.grad(X, Y)\n                    num_updates=0\n                    for i in range(1, self.num_layers+1):\n                        num_updates+=1\n                        self.update_params[\"m_w\"+str(i)]=beta1*self.update_params[\"m_w\"+str(i)]+(1-beta1)*(self.gradients[\"dW\"+str(i)]\/m)\n                        self.update_params[\"v_w\"+str(i)]=beta2*self.update_params[\"v_w\"+str(i)]+(1-beta2)*((self.gradients[\"dW\"+str(i)]\/m)**2)\n                        m_w_hat = self.update_params[\"m_w\"+str(i)]\/(1-np.power(beta1,num_updates))\n                        v_w_hat = self.update_params[\"v_w\"+str(i)]\/(1-np.power(beta2,num_updates))\n                        self.params[\"W\"+str(i)] -= (eta\/np.sqrt(v_w_hat+eps))*m_w_hat\n                        \n                        self.update_params[\"m_b\"+str(i)]=beta1*self.update_params[\"m_b\"+str(i)]+(1-beta1)*(self.gradients[\"dB\"+str(i)]\/m)\n                        self.update_params[\"v_b\"+str(i)]=beta2*self.update_params[\"v_b\"+str(i)]+(1-beta2)*((self.gradients[\"dB\"+str(i)]\/m)**2)\n                        m_b_hat = self.update_params[\"m_b\"+str(i)]\/(1-np.power(beta1,num_updates))\n                        v_b_hat = self.update_params[\"v_b\"+str(i)]\/(1-np.power(beta2,num_updates))\n                        self.params[\"B\"+str(i)] -=(eta\/np.sqrt(v_b_hat+eps))*m_b_hat\n                    \n                if display_loss:\n                    Y_pred = self.predict(X)\n                    loss[num_epoch] = log_loss(np.argmax(Y, axis=1), Y_pred)\n            \n            if display_loss:\n                plt.plot(np.fromiter(loss.values(), dtype = float), '-o',markersize=5)\n                plt.xlabel('Epochs')\n                plt.ylabel('Log Loss')\n                plt.show()\n            \n    def predict(self, X):\n        Y_pred = self.forward_pass(X)\n        return np.array(Y_pred).squeeze()","a108c336":"def print_accuracy():    \n    Y_pred_train = model.predict(X_train)\n    Y_pred_train = np.argmax(Y_pred_train,1)\n    Y_pred_val = model.predict(X_val)\n    Y_pred_val = np.argmax(Y_pred_val,1)\n    accuracy_train = accuracy_score(Y_pred_train, Y_train)\n    accuracy_val = accuracy_score(Y_pred_val, Y_val)\n    print(\"Training accuracy\", round(accuracy_train, 4))\n    print(\"Validation accuracy\", round(accuracy_val, 4))\n    \n    if False:\n      plt.scatter(X_train[:,0], X_train[:,1], c=Y_pred_train, cmap=my_cmap, s=15*(np.abs(np.sign(Y_pred_train-Y_train))+.1))\n      plt.show()","1f7cdbd9":"%%time\nmodel = FFNetwork(W1, W2)\nmodel.fit(X_train, y_OH_train, epochs=100, eta=1, algo=\"GD\", display_loss=True)\nprint_accuracy()","2cd76b5a":"%%time\nmodel = FFNetwork(W1, W2)\nmodel.fit(X_train, y_OH_train, epochs=100, eta=1, algo=\"MiniBatch\", mini_batch_size=128, display_loss=True)\nprint_accuracy()","a707c8f6":"%%time\nmodel = FFNetwork(W1, W2)\nmodel.fit(X_train, y_OH_train, epochs=100, eta=1, algo=\"MiniBatch\", mini_batch_size=8, display_loss=True)\nprint_accuracy()","aec211f6":"%%time\nmodel = FFNetwork(W1, W2)\nmodel.fit(X_train, y_OH_train, epochs=100, eta=1, algo=\"Momentum\", gamma=0.5, display_loss=True)\nprint_accuracy()","13a8f23c":"%%time\nmodel = FFNetwork(W1, W2)\nmodel.fit(X_train, y_OH_train, epochs=100, eta=1, algo=\"Momentum\", gamma=0.9, display_loss=True)\nprint_accuracy()","e4a72b1c":"%%time\nmodel = FFNetwork(W1, W2)\nmodel.fit(X_train, y_OH_train, epochs=100, eta=1, algo=\"Momentum\", gamma=0.99, display_loss=True)\nprint_accuracy()","9e4da843":"%%time\nmodel = FFNetwork(W1, W2)\nmodel.fit(X_train, y_OH_train, epochs=100, eta=1, algo=\"NAG\", gamma=0.99, display_loss=True)\nprint_accuracy()","979fd843":"%%time\nmodel = FFNetwork(W1, W2)\nmodel.fit(X_train, y_OH_train, epochs=100, eta=1, algo=\"NAG\", gamma=0.5, display_loss=True)\nprint_accuracy()","1c4e5290":"%%time\nmodel = FFNetwork(W1, W2)\nmodel.fit(X_train, y_OH_train, epochs=100, eta=1, algo=\"NAG\", gamma=0.9, display_loss=True)\nprint_accuracy()","9d374417":"%%time\nmodel = FFNetwork(W1, W2)\nmodel.fit(X_train, y_OH_train, epochs=100, eta=1, algo=\"AdaGrad\", display_loss=True)\nprint_accuracy()","5ca24508":"%%time\nmodel = FFNetwork(W1, W2)\nmodel.fit(X_train, y_OH_train, epochs=100, eta=.1, algo=\"AdaGrad\", display_loss=True)\nprint_accuracy()","b54c9c08":"%%time\nmodel = FFNetwork(W1, W2)\nmodel.fit(X_train, y_OH_train, epochs=100, eta=.1, algo=\"RMSProp\", beta=0.9, display_loss=True)\nprint_accuracy()","363bafb2":"%%time\nmodel = FFNetwork(W1, W2)\nmodel.fit(X_train, y_OH_train, epochs=100, eta=.9, algo=\"RMSProp\", beta=0.9, display_loss=True)\nprint_accuracy()","107a889e":"%%time\nmodel = FFNetwork(W1, W2)\nmodel.fit(X_train, y_OH_train, epochs=100, eta=.9, algo=\"Adam\", beta=0.9, display_loss=True)\nprint_accuracy()","dff0d4e5":"%%time\nmodel = FFNetwork(W1, W2)\nmodel.fit(X_train, y_OH_train, epochs=100, eta=.1, algo=\"Adam\", beta=0.9, display_loss=True)\nprint_accuracy()","3b234553":"\n%%time\nmodel = FFNetwork(W1, W2)\nmodel.fit(X_train, y_OH_train, epochs=10000, eta=0.5, algo=\"GD\", display_loss=True)\nprint_accuracy()","eee44201":"%%time\nmodel = FFNetwork(W1, W2)\nmodel.fit(X_train, y_OH_train, epochs=1000, eta=0.5, algo=\"Momentum\", gamma=0.9, display_loss=True)\nprint_accuracy()","b01b009c":"%%time\nmodel = FFNetwork(W1, W2)\nmodel.fit(X_train, y_OH_train, epochs=1000, eta=0.5, algo=\"NAG\", gamma=0.9, display_loss=True)\nprint_accuracy()","39001942":"%%time\nmodel = FFNetwork(W1, W2)\nmodel.fit(X_train, y_OH_train, epochs=500, eta=1, algo=\"AdaGrad\", display_loss=True)\nprint_accuracy()","48fa3309":"%%time\nmodel = FFNetwork(W1, W2)\nmodel.fit(X_train, y_OH_train, epochs=2000, eta=.01, algo=\"RMSProp\", beta=0.9, display_loss=True)\nprint_accuracy()","aa533c05":"%%time\nmodel = FFNetwork(W1, W2)\nmodel.fit(X_train, y_OH_train, epochs=200, eta=.1, algo=\"Adam\", beta=0.9, display_loss=True)\nprint_accuracy()","7a517017":"### Generate Data","a72ab56d":"### FF Class","083a8a3a":"### Multi class classification","ea688889":"\n### Neural-Network optimizers:\ud83e\udde0\n- Gradient Descent\n- Stochastic Gradient Descent (SGD)\n- Mini Batch Stochastic Gradient Descent (MBSGD)\n- SGD with momentum\n- Nesterov Accelerated Gradient (NAG)\n- Adaptive Gradient\n- AdaDelta\n- RMSprop\n- Adam ","fefe614b":"### Good configurations with each algo","b267aae6":"### Setup"}}