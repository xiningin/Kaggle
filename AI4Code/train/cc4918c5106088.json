{"cell_type":{"43852628":"code","4829114b":"code","5a84e1e9":"code","c1b4585a":"code","01f61d6e":"code","e6bf7f7f":"code","d0299f11":"code","9361271f":"code","34106e55":"code","bc8676b8":"code","298801c5":"code","8e8a9d40":"code","cdbd1c19":"code","cd591997":"code","38618953":"code","2cd9235d":"code","9c1985a4":"code","cb97eb52":"code","945d4099":"code","69cbcdc1":"code","86ee3f0f":"code","8d34ad0c":"code","4d434c23":"code","14e8f02e":"code","7adb7f07":"code","b61a6269":"code","e12eb5d1":"code","af7e2451":"code","672d9dd2":"code","4517dc51":"code","e921b04d":"code","0d7c8519":"code","47240ff4":"markdown","6655a125":"markdown"},"source":{"43852628":"from __future__ import print_function, division\nfrom builtins import range","4829114b":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, GlobalMaxPooling1D\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Model\nfrom sklearn.metrics import roc_auc_score","5a84e1e9":"import tensorflow as tf","c1b4585a":"# some configuration\nMAX_SEQUENCE_LENGTH = 1400\nMAX_VOCAB_SIZE = 40000\nEMBEDDING_DIM = 300\nVALIDATION_SPLIT = 0.1\nBATCH_SIZE = 128\nEPOCHS = 10","01f61d6e":"# load in pre-trained word vectors\nprint('Loading word vectors...')\nword2vec = {}\nwith open(os.path.join('..\/input\/glove6b\/glove.6B.%sd.txt' % EMBEDDING_DIM)) as f:\n  # is just a space-separated text file in the format:\n  # word vec[0] vec[1] vec[2] ...\n  for line in f:\n    values = line.split()\n    word = values[0]\n    vec = np.asarray(values[1:], dtype='float32')\n    word2vec[word] = vec\nprint('Found %s word vectors.' % len(word2vec))","e6bf7f7f":"train = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")","d0299f11":"train.describe(include='all')","9361271f":"train.url_legal.value_counts()","34106e55":"train.isnull().sum()","bc8676b8":"sentences = train[\"excerpt\"].fillna(\"DUMMY_VALUE\").values\ntarget = train[\"target\"].values","298801c5":"\n# convert the sentences (strings) into integers\ntokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\ntokenizer.fit_on_texts(sentences)\nsequences = tokenizer.texts_to_sequences(sentences)\n# print(\"sequences:\", sequences); exit()","8e8a9d40":"print(\"max sequence length:\", max(len(s) for s in sequences))\nprint(\"min sequence length:\", min(len(s) for s in sequences))\n","cdbd1c19":"s = sorted(len(s) for s in sequences)\nprint(\"median sequence length:\", s[len(s) \/\/ 2])","cd591997":"print(\"max word index:\", max(max(seq) for seq in sequences if len(seq) > 0))","38618953":"# get word -> integer mapping\nword2idx = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word2idx))","2cd9235d":"# pad sequences so that we get a N x T matrix\ndata = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', data.shape)\n","9c1985a4":"len(word2idx)","cb97eb52":"# prepare embedding matrix\nprint('Filling pre-trained embeddings...')\nnum_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word2idx.items():\n  if i < MAX_VOCAB_SIZE:\n    embedding_vector = word2vec.get(word)\n    if embedding_vector is not None:\n      # words not found in embedding index will be all zeros.\n      embedding_matrix[i] = embedding_vector","945d4099":"\n# load pre-trained word embeddings into an Embedding layer\n# note that we set trainable = False so as to keep the embeddings fixed\nembedding_layer = Embedding(\n  num_words,\n  EMBEDDING_DIM,\n  weights=[embedding_matrix],\n  input_length=MAX_SEQUENCE_LENGTH,\n  trainable=False\n)","69cbcdc1":"print('Building model...')\n# train a 1D convnet with global maxpooling\ninput_ = Input(shape=(MAX_SEQUENCE_LENGTH,))\nx = embedding_layer(input_)\nx = Conv1D(512, 3, activation='relu')(x)\nx = MaxPooling1D(3)(x)\nx = Conv1D(256, 3, activation='relu')(x)\nx = MaxPooling1D(3)(x)\nx = Conv1D(128, 3, activation='relu')(x)\nx = GlobalMaxPooling1D()(x)\nx = Dense(128, activation='tanh')(x)\noutput = Dense(1, activation='linear')(x)\n\nmodel = Model(input_, output)\nmodel.compile(\n  loss='mse',\n  optimizer='adam',\n  metrics=[tf.keras.metrics.RootMeanSquaredError()]\n)","86ee3f0f":"print('Training model...')\nr = model.fit(\n  data,\n  target,\n  batch_size=BATCH_SIZE,\n  epochs=EPOCHS,\n  validation_split=VALIDATION_SPLIT\n)\n","8d34ad0c":"# plot some data\nplt.plot(r.history['loss'], label='loss')\nplt.plot(r.history['val_loss'], label='val_loss')\nplt.legend()\nplt.show()","4d434c23":"# accuracies\nplt.plot(r.history['root_mean_squared_error'], label='rmse')\nplt.plot(r.history['val_root_mean_squared_error'], label='Val_rmse')\nplt.legend()\nplt.show()","14e8f02e":"test = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")","7adb7f07":"test.head(1)","b61a6269":"test_sent = test[\"excerpt\"].fillna(\"DUMMY_VALUE\").values\n# convert the sentences (strings) into integers\ntest_tokens = tokenizer.texts_to_sequences(test_sent)","e12eb5d1":"# pad sequences so that we get a N x T matrix\ntest_data = pad_sequences(test_tokens, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', test_data.shape)","af7e2451":"# plot the mean AUC over each label\np = model.predict(test_data)","672d9dd2":"submission = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/sample_submission.csv\")","4517dc51":"submission['target'] = p","e921b04d":"submission","0d7c8519":"submission.to_csv('submission.csv', index=False)","47240ff4":"# CONV1D Model Tryout - 1","6655a125":"# # Model"}}