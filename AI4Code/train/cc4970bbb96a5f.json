{"cell_type":{"463ba6db":"code","cadcfebd":"code","19e868be":"code","e62d0b2d":"code","48e4a168":"code","21f6076c":"code","3c58af3a":"code","384a123b":"code","8b38e713":"code","5711a960":"code","e8ab7074":"code","5fa77a65":"code","767b7d57":"code","48fa4122":"code","b014c4e9":"code","285c7c0b":"code","7e455135":"code","e7fedd1c":"code","5caba904":"code","6f3944b9":"code","0cce83cb":"code","ef5d58e4":"code","8f0f6e69":"code","8c24a6df":"code","1d82ab39":"code","b4827527":"code","fcb35b4e":"code","74584c6e":"code","bc5bd9ae":"code","f0dd3c84":"code","8bcd8b53":"code","18d811a1":"code","93ffb57a":"code","06a63b74":"code","f30f87c6":"code","fefba9b5":"code","687762ff":"code","2849023c":"code","dcaa62f9":"code","51d82e52":"code","a56d2a6b":"code","78bb2307":"code","73615161":"code","6ec9edf8":"code","163f566e":"code","71039dff":"code","4d18679f":"code","c5ebcfb2":"code","030af313":"code","8b7c4313":"code","3a09a772":"code","4335a6d3":"code","f0d34eea":"code","7563dc1e":"code","50686cf1":"code","dad03608":"code","833afafe":"code","6df50f0f":"code","5fc3ba07":"code","3c5870bd":"code","067008e4":"code","860509c4":"code","fb6e8a6d":"code","07bcc7c4":"code","afbdde69":"code","10a55332":"code","8a2aab83":"code","306ad3a6":"code","b53b3118":"code","2c6b7c58":"code","0777d809":"code","325abd5f":"code","bc083657":"code","fe63b53c":"code","540e4dd6":"code","983b952d":"code","94be1f6f":"code","8a450ce7":"code","9b671f49":"code","92a7f1cb":"code","44afc1e4":"code","c989e4bb":"code","10696c7e":"code","60e9cfb9":"code","74c38143":"code","d98487ab":"code","bac4aca5":"code","7e6bb01e":"code","6dce04f4":"code","b6d3af4c":"code","d2b590c4":"code","53ff6ba8":"code","7f7dc129":"code","8d14b337":"code","a4d5db30":"code","7d74cb67":"code","07820c30":"code","93feecb6":"code","88ff0024":"code","510cc342":"code","0de6bd8e":"code","6eed0a37":"code","3ae3a7bf":"code","bfaf4744":"code","f180472c":"code","ea5efddd":"code","77e6b28a":"code","589e0ad8":"code","6598d9f9":"code","68a83aad":"code","d498ce19":"code","7b8d5c1a":"code","f52f8583":"code","b7b54913":"code","25244820":"code","ffb06ef9":"code","611089b0":"code","29a81493":"code","1d9dc1be":"code","527079a3":"code","5d7f91b5":"code","2d1458c5":"code","97fe2f79":"code","60fe8c93":"code","1200c85f":"code","03a54457":"code","2313f55a":"code","9b084bd3":"code","85c07e41":"code","710a18e8":"code","d9366802":"code","fc3dec37":"code","19241dd2":"code","1f44965a":"code","3cf7b962":"code","4a2c06ce":"code","ef38f9d0":"code","696d0eb4":"code","e7a6e268":"code","cebb655f":"code","614b8d23":"code","8f8d9b7c":"code","068393dd":"code","976d4e78":"code","9db7e2e4":"code","0781bb8e":"code","64c7b05f":"code","e6efe00b":"code","ade330f1":"code","071ab6dd":"code","96287c6e":"code","9a393095":"code","9b560532":"code","f94f6f38":"code","dcc99832":"code","3b57f6f3":"code","35018288":"code","9be1a19d":"code","5c375718":"code","1124b152":"code","f6424adc":"code","423ec708":"code","209e67eb":"code","95b983e4":"code","60842abd":"code","f9beeeae":"code","8465efbf":"code","cbad7930":"code","0bfbe6e0":"code","4a065822":"code","27ff7f20":"code","b345071c":"code","3bef5be1":"code","620a694c":"code","7eb8428f":"code","418e8673":"code","295099c9":"code","0665c4cd":"code","391d795e":"code","0ee7bba8":"code","e055a97e":"code","f5fcdb06":"code","8be1e523":"code","707699f7":"code","9ebd54aa":"code","461a7d9a":"code","b5bc72d7":"code","fbfa37a0":"code","dfc948d0":"code","93f8bfa7":"code","093d3ad3":"code","6a4b07e9":"code","601d4016":"code","e9992bff":"code","bdb03518":"code","ef2596d7":"code","16a0d6b8":"code","c5660dea":"code","d511164c":"code","05537145":"code","5cdbc063":"code","499b6886":"code","3a8339e1":"code","c903ef77":"code","ee03b3de":"code","7c9c394a":"code","d83bb606":"code","0384efbe":"code","11012458":"code","d7903274":"code","302b9254":"code","b6bb1994":"code","cfc285af":"code","cd6d4966":"code","6506e5aa":"code","99721fb1":"code","953ca22e":"code","bc96818f":"code","9c798bbd":"code","51e56f69":"code","1a07306c":"code","f447b045":"code","3ea309e8":"code","5b8f728b":"code","f457e7d3":"code","4602cc82":"code","8afa2e04":"code","ce30be06":"code","4101d66b":"code","12566f1d":"code","0a84386d":"code","6c1eb35d":"code","c73aabc5":"code","306d9a0f":"code","6cab7100":"code","86955671":"markdown","e064ccb4":"markdown","3aeb416e":"markdown","6c2846ec":"markdown","0373246f":"markdown","d1df72c9":"markdown","59fca9df":"markdown","146f9fd8":"markdown","fb239107":"markdown","cdeb6c68":"markdown","7cd9839e":"markdown","ad03d675":"markdown","8fc7ee54":"markdown","a2a585da":"markdown","e442c912":"markdown","97c5481e":"markdown","40f88359":"markdown","4bb875d6":"markdown","fa6c18bb":"markdown","da1b10ba":"markdown","a6612999":"markdown","2d9de58d":"markdown","12994483":"markdown","2dd67f0b":"markdown","a5435b20":"markdown","36cbe2b7":"markdown","f009b11b":"markdown","c05866c8":"markdown","e35517e2":"markdown","4beaff97":"markdown","1fe2ce70":"markdown","412b3012":"markdown","e42a6ad5":"markdown","a611e89c":"markdown","4510bac8":"markdown","01c367c6":"markdown","61bdb976":"markdown","125d67be":"markdown","ccbe6e80":"markdown","ace23977":"markdown","fb92edf8":"markdown","b0752d79":"markdown","6dc52969":"markdown","11d44bc7":"markdown","8c031a9c":"markdown","71e6b7ea":"markdown","2024db91":"markdown","cdd57e75":"markdown","ad73f810":"markdown","87faa7ad":"markdown","14760d5f":"markdown","45239e75":"markdown","7e58cd58":"markdown","b740c661":"markdown","9622ef12":"markdown","e20c29c0":"markdown","78fffef9":"markdown","7bf05686":"markdown","e71c1b15":"markdown","a2bb89ce":"markdown","8018642b":"markdown","3151791f":"markdown","6a52819a":"markdown","168d7ada":"markdown","54e0ff6c":"markdown","7389faf0":"markdown","99c1a019":"markdown","deb22bdc":"markdown","2678cac2":"markdown","2046fa33":"markdown","a7a25de9":"markdown","717bd099":"markdown","137aee39":"markdown","936874b3":"markdown","6db0f1ae":"markdown","a27d4c36":"markdown","ee43cef9":"markdown","d463eeca":"markdown","c501c9ff":"markdown","ac5fc95c":"markdown","fe3129f4":"markdown","1c34178b":"markdown","7f28a801":"markdown","8ff5e5b0":"markdown","da91f8b4":"markdown","0aadaac3":"markdown","75cfd700":"markdown","e337476e":"markdown","6029f16c":"markdown","19a7f6e1":"markdown","ac3112fe":"markdown","455d4276":"markdown","be36ab07":"markdown","4f40dd79":"markdown","6782fc62":"markdown","d4deadba":"markdown","24b06c3a":"markdown","3ddbe474":"markdown","82248fee":"markdown","42e53604":"markdown","c809b15f":"markdown","f43db884":"markdown","3d0b0a15":"markdown","40b006db":"markdown","0a0185af":"markdown","c65434c0":"markdown","90917909":"markdown","380c4237":"markdown","1e4d73ab":"markdown","c5238ca4":"markdown"},"source":{"463ba6db":"import pandas as pd\nimport numpy as np\nimport os\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom keras import models, layers, Model\nfrom keras.layers import Input, Dense, Concatenate, Activation #,LSTM\nfrom keras.utils.generic_utils import get_custom_objects\nimport keras.backend as K\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import load_model\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\n\nfrom IPython.display import Markdown as md\n\n# Keras custom activation functions.\ndef cuadratic(x):\n    return 0.00000001*x*x\n\nget_custom_objects().update({'cos':K.cos, 'sin':K.sin, 'cuadratic':cuadratic})\n\n# Plot options. \n#style.use('dark_background') # If you use dark mode.\nmpl.rcParams['figure.dpi']=200","cadcfebd":"# Function for visualize original and cleaned data.\n\ndef vis_orig_vs_clean(rows, columns, plot_columns, cols, yl, fs=(10, 10), xticks_rate=1):\n\n    global df_before\n    \n    mpl.rcParams['axes.titlepad'] = 13\n    fig, ax = plt.subplots(rows, columns, figsize=fs)\n\n    for i,j in zip(range(1, rows + 1), plot_columns):\n\n        if i == 1: \n            ax1 = plt.subplot(rows, columns, 2*(i-1) + 1, title=cols[j])\n            ax1.text(0.5, 1.4, 'Original', horizontalalignment='center', fontweight='bold', fontsize=20, transform=ax1.transAxes)\n        elif i == rows:\n            plt.subplot(rows, columns, 2*(i-1) + 1, title=cols[j])\n            plt.xlabel('Date', fontsize=10, labelpad=8)\n        else:\n            plt.subplot(rows, columns, 2*(i-1) + 1, title=cols[j])\n\n        plt.ylabel(yl[i-1], fontsize=10)\n        plt.plot(df_before[cols[j]], lw=0.5)\n\n        if i == 1: \n            ax2 = plt.subplot(rows, columns, 2*i, title=cols[j])\n            ax2.text(0.5, 1.4, 'Cleaned', horizontalalignment='center', fontweight='bold', fontsize=20, transform=ax2.transAxes)\n        elif i == rows:\n            plt.subplot(rows, columns, 2*i, title=cols[j])\n            plt.xlabel('Date', fontsize=10, labelpad=8)\n        else:\n            plt.subplot(rows, columns, 2*i, title=cols[j])\n\n        plt.plot(df[cols[j]], lw=0.5)\n\n    for ax in fig.axes:\n        ax.tick_params(labelrotation=0, labelsize=8)\n        ax.xaxis.set_major_locator(mpl.dates.YearLocator(base=xticks_rate))\n        ax.xaxis.set_major_formatter(mpl.dates.DateFormatter('%Y'))\n\n    plt.tight_layout(h_pad=3.0, w_pad=2.0)\n    plt.show()","19e868be":"# Function to plot final results.\n\ndef plotResults(forecast1, real1, forecast2, real2, yl=['Depth (m)', 'Depth (m)']):\n\n    mpl.rcParams['axes.titlepad'] = 13\n    fig, ax = plt.subplots(2, 1, figsize=(10,12))\n\n    plt.subplot(211)\n    plt.title('Best Model Results', fontsize=16)\n    plt.ylabel(yl[0], fontsize=10, labelpad=8)\n    plt.xlabel('Date', fontsize=10, labelpad=8)\n    plt.plot(df.loc[point_start:point_end].index, real1, label='Train + Validation + Test')\n    plt.plot(df.loc[point_start:point_end].index, forecast1, label='Forecast')\n    plt.legend()\n\n    plt.subplot(212)\n    plt.title('Best Model Results', fontsize=16)\n    plt.ylabel(yl[1], fontsize=10, labelpad=8)\n    plt.xlabel('Date', fontsize=10, labelpad=8)\n    plt.plot(df.loc[point_partition_1:point_end].index, real2, label='Validation + Test')\n    plt.plot(df.loc[point_partition_1:point_end].index, forecast2, label='Forecast')\n    plt.legend()\n    \n    plt.tight_layout(h_pad=4.0)\n    plt.show()","e62d0b2d":"# Function to plot forecasts without validation data.\n\ndef forecast(days_to_predict = 90, yl='Depth (m)'):\n    global point_end\n    global point_start\n    global train_data\n    global validation_data\n    global tes_data\n    global model\n    global scaler\n    global real1\n    dates1 = [pd.to_datetime(point_end) + pd.Timedelta(weeks=i) for i in range(int(days_to_predict\/7))]\n    x = np.array(list(range(test_data[-1].astype(int), test_data[-1].astype(int) + days_to_predict, 7))).astype(float)\n    x = x[1:]\n    validation = np.concatenate((train_data.ravel(), validation_data.ravel(), test_data.ravel(), x))\n    forecast = model.predict(validation)\n    forecast = scaler.inverse_transform(forecast)\n    \n    n0 = len(dates1)\n    n1 = len(x)\n    n2 = len(real1)\n    n3 = len(forecast)\n    \n    dates2 = [pd.to_datetime(point_start) + pd.Timedelta(weeks=i) for i in range(n2)]\n    \n    #dates2 = [pd.to_datetime(point_start) + pd.Timedelta(weeks=n1) for i in range(int(n1))]\n\n    plt.figure(figsize=(10,5))\n    plt.title('Best model results')\n    plt.ylabel(yl, fontsize=10, labelpad=8)\n    plt.xlabel('Date', fontsize=10, labelpad=8)\n    plt.plot(dates2, real1, label='Train + Validation + Test')\n    plt.plot(dates2, forecast[:n2], label='Tests Forecast')\n    plt.plot(dates1, forecast[-n0:], label='New Forecast')\n    plt.legend()\n    plt.show()","48e4a168":"# Import database.\n\ndf = pd.read_csv(\"..\/input\/aceawaterprediction\/Aquifer_Auser.csv\")\n\n# Convert 'Date' to date-time type and set it as index.\ndf['Date'] = pd.to_datetime(df['Date'], format='%d\/%m\/%Y')\ndf.set_index('Date', inplace=True)\n\ndf","21f6076c":"# Auxiliar functions and main genetic algorithm.\n\n# Create individual from his gene, return a list with the gene, the correspondent model and its score.\ndef createIndividual(gen):\n\n    epoc = 500\n    \n    il = Input(shape=1)\n    \n    l1 = Dense(gen[1], activation=K.sin)(il)\n    l2 = Dense(gen[2], activation=K.cos)(il)\n    l3 = Dense(1, activation='cuadratic')(il)\n    lc1 = Concatenate()([l1, l2, l3])\n    \n    ol = Dense(1, activation='linear')(lc1)\n    \n    model = Model(inputs=il, outputs=ol)\n    \n    callbacks_list = [ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n    \n    model.compile(optimizer='adam', loss='mae')\n    history = model.fit(train_data, train_target_data, verbose=0, epochs=epoc, batch_size=gen[0], validation_data=(validation_data, validation_target_data), callbacks=callbacks_list)\n    \n    model = load_model('..\/input\/aceawaterprediction-models\/best_model.h5')\n    os.remove('best_model.h5')\n\n    return [gen, model, min(history.history['val_loss'])]\n\n# Function for sorting genes by validation score.\ndef customOrder(elem):\n    return elem[2]\n\n# Main algorithm.\ndef geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n                     test_target_data, max_batch_size, popul_inic = 50, parents_pairs = 25, sons_per_pair = 1, \n                     prob_mutation = 0.05, num_generations = 10, max_sin = 200, max_cos = 200):\n\n    # gen: [Batch size, sin() number, cos() number].\n    # gen limits, including both extremes.\n    gen_limits = [[1, max_batch_size], [0, max_sin], [0, max_cos]]\n\n\n    # Individuals list: [gen, model, score].\n    popul = [[None, None, 0.0] for i in range(popul_inic)]\n\n    # Descendants list. Same strucre as 'popul'.\n    desc = [[None, None, 0.0] for i in range(parents_pairs*sons_per_pair)]\n\n    # Initialize population.\n    for i in range(popul_inic):\n\n        print(\"Generating initial population, individual\", i+1, '.')\n\n        batch_s = np.random.randint(max_batch_size) + 1\n        num_sin = np.random.randint(max_sin + 1)\n        num_cos = np.random.randint(max_cos + 1)\n        popul[i] = createIndividual([batch_s, num_sin, num_cos])\n        #popul[i] = crearIndividuo([batch_s, num_sin])\n\n        print(\"Gene:\", popul[i][0], \"Val. MAE:\", popul[i][2])\n\n    # Generation of descendants through generations (main part of the algorithm).\n    for i in range(num_generations):\n\n        #Print info.\n        print(\"Generating population (generation\", i,\"):\")\n        for p in range(popul_inic): print(popul[p][0], popul[p][2])\n\n        # Couples selection and descendendants generation.\n        for j in range(parents_pairs):\n\n            # Parents selection.\n            ind1 = 0; ind2 = 0\n            while ind1 == ind2:\n                ind1 = np.random.randint(popul_inic)\n                ind2 = np.random.randint(popul_inic)\n\n            # Parents decendants.\n            for k in range(sons_per_pair):\n\n                # Gene obtained from the parents.\n                gen = [0 for i in range(len(popul[0][0]))]   # Empty gene.\n                for l in range(len(gen)):\n                    # Coin toss.\n                    if np.random.randint(2):\n                        gen[l] = popul[ind1][0][l] if np.random.randint(2) else popul[ind2][0][l]\n                    else:\n                        gen[l] = int((popul[ind1][0][l] + popul[ind2][0][l])\/2)\n\n                # Mutate descendant.\n                if np.random.rand() <= prob_mutation:\n                    elem = np.random.randint(len(gen))\n                    gen[elem] = np.random.randint(gen_limits[elem][0], gen_limits[elem][1]+1)\n\n                desc[j*sons_per_pair + k] = createIndividual(gen)\n\n        # Population substitution of worst members with best descendendants, based on their scores.\n\n        popul_aux = popul + desc\n        # Print info.\n        print(\"Descendants (generation\", i,\"):\")\n        for d in range(len(desc)): print(desc[d][0], desc[d][2])\n        popul_aux.sort(key=customOrder)\n\n        popul = popul_aux[:popul_inic]\n\n    # Print final population info.\n    print(\"Final population:\")\n    for p in range(len(popul)): print(popul[p][0], popul[p][2])\n\n    # Choose the best individual based on the established metric.\n    scores = [[0.0,0] for i in range(len(popul))]    # Variable for saving scores and models index.\n    for i in range(len(popul)):\n\n        train_pred = popul[i][1].predict(train_data)\n        train_score = 0.25*mean_absolute_error(train_target_data, train_pred)\n        validation_pred = popul[i][1].predict(validation_data)\n        validation_score = 0.25*mean_absolute_error(validation_target_data, validation_pred)\n        test_pred = popul[i][1].predict(test_data)\n        test_score = 0.5*mean_absolute_error(test_target_data, test_pred)\n        score = train_score + validation_score + test_score\n\n        scores[i] = [score, i]\n\n    scores.sort()\n\n    print(f'Best score correspond to model with index {scores[0][1]}.')\n\n    # Return the best model.\n    return popul[scores[0][1]][1]","3c58af3a":"# Import database.\n\ndf = pd.read_csv(\"..\/input\/aceawaterprediction\/Aquifer_Auser.csv\")\n\n# Convert 'Date' to date-time type and set it as index.\ndf['Date'] = pd.to_datetime(df['Date'], format='%d\/%m\/%Y')\ndf.set_index('Date', inplace=True)","384a123b":"# Dictionary of integers to columns names for shorter code.\n\ncols1 = {\n    0:'Rainfall_Gallicano',\n    1:'Rainfall_Pontetetto',\n    2:'Rainfall_Monte_Serra',\n    3:'Rainfall_Orentano',\n    4:'Rainfall_Borgo_a_Mozzano',\n    5:'Rainfall_Piaggione',\n    6:'Rainfall_Calavorno',\n    7:'Rainfall_Croce_Arcana',\n    8:'Rainfall_Tereglio_Coreglia_Antelminelli',\n    9:'Rainfall_Fabbriche_di_Vallico',\n    10:'Depth_to_Groundwater_LT2',\n    11:'Depth_to_Groundwater_SAL',\n    12:'Depth_to_Groundwater_PAG',\n    13:'Depth_to_Groundwater_CoS',\n    14:'Depth_to_Groundwater_DIEC',\n    15:'Temperature_Orentano',\n    16:'Temperature_Monte_Serra',\n    17:'Temperature_Ponte_a_Moriano',\n    18:'Temperature_Lucca_Orto_Botanico',\n    19:'Volume_POL',\n    20:'Volume_CC1',\n    21:'Volume_CC2',\n    22:'Volume_CSA',\n    23:'Volume_CSAL',\n    24:'Hydrometry_Monte_S_Quirico',\n    25:'Hydrometry_Piaggione',\n}","8b38e713":"# Clean data.\n\n# Save original series.\ndf_before = df.copy()\n\n# Convert outliers to NaN and interpolate missing values in 'Depth_to_Groundwater_LT2'.\ndf.loc[df[cols1[10]] > -1, cols1[10]] = np.nan\ndf[cols1[10]] = df[cols1[10]].interpolate()\n\n# Convert outliers to NaN and interpolate missing values in 'Depth_to_Groundwater_SAL'.\ndf.loc[df[cols1[11]] > -1, cols1[11]] = np.nan\ndf[cols1[11]] = df[cols1[11]].interpolate()\n\n# Convert outliers to NaN and interpolate missing values in 'Depth_to_Groundwater_CoS'.\ndf.loc[df[cols1[13]] > -1, cols1[13]] = np.nan\ndf[cols1[13]] = df[cols1[13]].interpolate()\n\n# Include a point to get a better patern and interpolate missing values in 'Depth_to_Groundwater_DIEC'.\ndf.loc['2020-03-29', cols1[14]] = -3.9\ndf[cols1[14]] = df[cols1[14]].interpolate()\n\ndf_aux = df.copy()","5711a960":"# Visualize original and cleaned data.\ndf = df.resample(rule='W').mean()\nvis_orig_vs_clean(3, 2, [10,11,13], cols1, yl=['Depth (m)']*3, fs=(10, 10), xticks_rate=2)","e8ab7074":"# Key number of the column to predict.\ncol_target = 10\n\n# Partition points for train, validation and test.\n\npoint_start = '2015-04-16'\npoint_partition_1 = '2019-07-01'\npoint_partition_2 = '2020-01-01'\npoint_end = '2020-07-01'","5fa77a65":"# Visualize train + validation + test data for Depth to Groundwater LT2.\n\nplt.figure(figsize=(10,5))\nplt.title('Depth to Groundwater LT2')\nplt.ylabel('Depth (m)', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.loc[point_start:point_partition_1, cols1[col_target]], label='Train')\nplt.plot(df.loc[point_partition_1:point_partition_2, cols1[col_target]], label='Validation')\nplt.plot(df.loc[point_partition_2:point_end, cols1[col_target]], label='Test')\nplt.legend()\nplt.show()","767b7d57":"# Select domain to analize.\ndf = df_aux.copy()\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols1[col_target]]])\ndf[[cols1[col_target]]] = scaler.transform(df[[cols1[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols1[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols1[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols1[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","48fa4122":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","b014c4e9":"model = load_model('..\/input\/aceawaterprediction-models\/AA_DGLT2.h5')","285c7c0b":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols1[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols1[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2, yl=['Hydrometry']*2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","7e455135":"print(MAE)\nprint(RMSE)","e7fedd1c":"forecast(days_to_predict = 365)","5caba904":"# Key number of the column to predict.\ncol_target = 11\n\n# Partition points for train, validation and test.\n\npoint_start = '2013-08-02'\npoint_partition_1 = '2019-07-01'\npoint_partition_2 = '2020-01-01'\npoint_end = '2020-07-01'","6f3944b9":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Depth to Groundwater SAL')\nplt.ylabel('Depth (m)', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.loc[point_start:point_partition_1, cols1[col_target]], label='Train')\nplt.plot(df.loc[point_partition_1:point_partition_2, cols1[col_target]], label='Validation')\nplt.plot(df.loc[point_partition_2:point_end, cols1[col_target]], label='Test')\nplt.legend()\nplt.show()","0cce83cb":"# Select domain to analize.\ndf = df_aux.copy()\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols1[col_target]]])\ndf[[cols1[col_target]]] = scaler.transform(df[[cols1[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols1[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols1[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols1[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","ef5d58e4":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","8f0f6e69":"model = load_model('..\/input\/aceawaterprediction-models\/AA_DGSAL.h5')","8c24a6df":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols1[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols1[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","1d82ab39":"print(MAE)\nprint(RMSE)","b4827527":"forecast(days_to_predict = 365)","fcb35b4e":"# Key number of the column to predict.\ncol_target = 13\n\n# Partition points for train, validation and test.\n\npoint_start = '2015-10-10'\npoint_partition_1 = '2019-07-01'\npoint_partition_2 = '2020-01-01'\npoint_end = '2020-07-01'","74584c6e":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Depth to Groundwater CoS')\nplt.ylabel('Depth (m)', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.loc[point_start:point_partition_1, cols1[col_target]], label='Train')\nplt.plot(df.loc[point_partition_1:point_partition_2, cols1[col_target]], label='Validation')\nplt.plot(df.loc[point_partition_2:point_end, cols1[col_target]], label='Test')\nplt.legend()\nplt.show()","bc5bd9ae":"# Select domain to analize.\ndf = df_aux.copy()\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols1[col_target]]])\ndf[[cols1[col_target]]] = scaler.transform(df[[cols1[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols1[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols1[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols1[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","f0dd3c84":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","8bcd8b53":"model = load_model('..\/input\/aceawaterprediction-models\/AA_DGCOS.h5')","18d811a1":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols1[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols1[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","93ffb57a":"print(MAE)\nprint(RMSE)","06a63b74":"forecast(days_to_predict = 365)","f30f87c6":"# Import database.\n\ndf = pd.read_csv(\"..\/input\/aceawaterprediction\/Aquifer_Doganella.csv\")\n\n# Convert 'Date' to date-time type and set it as index.\ndf['Date'] = pd.to_datetime(df['Date'], format='%d\/%m\/%Y')\ndf.set_index('Date', inplace=True)","fefba9b5":"# Dictionary of integers to columns names for shorter code.\n\ncols2 = {\n    0:'Rainfall_Monteporzio',\n    1:'Rainfall_Velletri',\n    2:'Depth_to_Groundwater_Pozzo_1',\n    3:'Depth_to_Groundwater_Pozzo_2',\n    4:'Depth_to_Groundwater_Pozzo_3',\n    5:'Depth_to_Groundwater_Pozzo_4',\n    6:'Depth_to_Groundwater_Pozzo_5',\n    7:'Depth_to_Groundwater_Pozzo_6',\n    8:'Depth_to_Groundwater_Pozzo_7',\n    9:'Depth_to_Groundwater_Pozzo_8',\n    10:'Depth_to_Groundwater_Pozzo_9',\n    11:'Volume_Pozzo_1',\n    12:'Volume_Pozzo_2',\n    13:'Volume_Pozzo_3',\n    14:'Volume_Pozzo_4',\n    15:'Volume_Pozzo_5+6',\n    16:'Volume_Pozzo_7',\n    17:'Volume_Pozzo_8',\n    18:'Volume_Pozzo_9',\n    19:'Temperature_Monteporzio',\n    20:'Temperature_Velletri'\n}","687762ff":"# Clean data.\n\n# Save original series.\ndf_before = df.iloc[:, 2:11].copy()\n\ndf[cols2[0]] = df[cols2[0]].interpolate()\ndf[cols2[1]] = df[cols2[1]].interpolate()\ndf[cols2[2]] = df[cols2[2]].interpolate()\n\n# Convert outliers to NaN and interpolate in Depth_to_Groundwater_Pozzo_2.\ndf.loc[df[cols2[3]] < -110, cols2[3]] = np.nan\ndf[cols2[3]] = df[cols2[3]].interpolate()\n\n# Convert peaks afeter 2018 to NaN, add a point to interpolate and get a more congruent series in \n# Depth_to_Groundwater_Pozzo_3.\n#df.loc[df[cols2[4]] > -103, cols2[4]] = np.nan\ndf.loc[(df.index > '2018') & (df[cols2[4]] > -110), cols2[4]] = np.nan\ndf.loc['2019-09-19', cols2[4]] = -112.3\ndf[cols2[4]] = df[cols2[4]].interpolate()\n\ndf[cols2[5]] = df[cols2[5]].interpolate()\n\n# Remove the sharp valley near 2018 and interpolate in Depth_to_Groundwater_Pozzo_5.\ndf.loc[(df.index >= '2017-06-01') & (df.index <='2018-06-01') & (df[cols2[6]] < -104), cols2[6]] = np.nan\ndf[cols2[6]] = df[cols2[6]].interpolate()\n\n# Convert outliers to NaN and interpolate in Depth_to_Groundwater_Pozzo_6.\ndf.loc[(df.index >= '2019-01-01') & (df.index <='2020-01-01') & (df[cols2[7]] > -92), cols2[7]] = np.nan\ndf[cols2[7]] = df[cols2[7]].interpolate()\n\ndf[cols2[8]] = df[cols2[8]].interpolate()\n\n# Convert outliers to NaN and interpolate in Depth_to_Groundwater_Pozzo_8.\ndf.loc[df[cols2[9]] < -106, cols2[9]] = np.nan\ndf[cols2[9]] = df[cols2[9]].interpolate()\n\ndf[cols2[10]] = df[cols2[10]].interpolate()\n\n# Interpolate all volume series.\ndf.iloc[:, 11:19] = df.iloc[:, 11:19].interpolate()\n\ndf_aux = df.copy()","2849023c":"# Visualize original and cleaned data.\ndf = df.resample(rule='W').mean()\nvis_orig_vs_clean(9, 2, [2,3,4,5,6,7,8,9,10], cols2, yl=['Depth (m)']*9, fs=(10, 27))","dcaa62f9":"df = df_aux.copy()\n\n# Key number of the column to predict.\ncol_target = 2\n\n# Partition points for train, validation and test.\n\npoint_start = '2017-06-01'\npoint_partition_1 = '2019-07-01'\npoint_partition_2 = '2020-01-01'\npoint_end = '2020-07-01'","51d82e52":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Depth to Groundwater Pozzo 1')\nplt.ylabel('Depth (m)', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.resample(rule='W').mean().loc[point_start:point_partition_1, cols2[col_target]], label='Train')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_1:point_partition_2, cols2[col_target]], label='Validation')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_2:point_end, cols2[col_target]], label='Test')\nplt.legend()\nplt.show()","a56d2a6b":"# Select domain to analize.\ndf = df_aux.copy()\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols2[col_target]]])\ndf[[cols2[col_target]]] = scaler.transform(df[[cols2[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols2[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols2[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols2[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","78bb2307":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","73615161":"model = load_model('..\/input\/aceawaterprediction-models\/AD_DGP1.h5')","6ec9edf8":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols2[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols2[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","163f566e":"print(MAE)\nprint(RMSE)","71039dff":"forecast(days_to_predict = 365)","4d18679f":"df = df_aux.copy()\n\n# Key number of the column to predict.\ncol_target = 3\n\n# Partition points for train, validation and test.\n\npoint_start = '2015-01-01'\npoint_partition_1 = '2019-07-01'\npoint_partition_2 = '2020-01-01'\npoint_end = '2020-07-01'","c5ebcfb2":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Depth to Groundwater Pozzo 2')\nplt.ylabel('Depth (m)', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.resample(rule='W').mean().loc[point_start:point_partition_1, cols2[col_target]], label='Train')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_1:point_partition_2, cols2[col_target]], label='Validation')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_2:point_end, cols2[col_target]], label='Test')\nplt.legend()\nplt.show()","030af313":"# Select domain to analize.\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols2[col_target]]])\ndf[[cols2[col_target]]] = scaler.transform(df[[cols2[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols2[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols2[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols2[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","8b7c4313":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","3a09a772":"model = load_model('..\/input\/aceawaterprediction-models\/AD_DGP2.h5')","4335a6d3":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols2[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols2[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","f0d34eea":"print(MAE)\nprint(RMSE)","7563dc1e":"forecast(days_to_predict = 365)","50686cf1":"df = df_aux.copy()\n\n# Key number of the column to predict.\ncol_target = 4\n\n# Partition points for train, validation and test.\n\npoint_start = '2017-01-01'\npoint_partition_1 = '2020-01-01'\npoint_partition_2 = '2020-04-01'\npoint_end = '2020-07-01'","dad03608":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Depth to Groundwater Pozzo 3')\nplt.ylabel('Depth (m)', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.resample(rule='W').mean().loc[point_start:point_partition_1, cols2[col_target]], label='Train')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_1:point_partition_2, cols2[col_target]], label='Validation')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_2:point_end, cols2[col_target]], label='Test')\nplt.legend()\nplt.show()","833afafe":"# Select domain to analize.\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols2[col_target]]])\ndf[[cols2[col_target]]] = scaler.transform(df[[cols2[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols2[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols2[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols2[col_target]]\n\ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","6df50f0f":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","5fc3ba07":"model = load_model('..\/input\/aceawaterprediction-models\/AD_DGP3.h5')","3c5870bd":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols2[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols2[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","067008e4":"print(MAE)\nprint(RMSE)","860509c4":"forecast(days_to_predict = 182)","fb6e8a6d":"df = df_aux.copy()\n\n# Key number of the column to predict.\ncol_target = 5\n\n# Partition points for train, validation and test.\n\npoint_start = '2017-01-01'\npoint_partition_1 = '2020-01-01'\npoint_partition_2 = '2020-04-01'\npoint_end = '2020-07-01'","07bcc7c4":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Depth to Groundwater Pozzo 4')\nplt.ylabel('Depth (m)', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.resample(rule='W').mean().loc[point_start:point_partition_1, cols2[col_target]], label='Train')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_1:point_partition_2, cols2[col_target]], label='Validation')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_2:point_end, cols2[col_target]], label='Test')\nplt.legend()\nplt.show()","afbdde69":"# Select domain to analize.\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols2[col_target]]])\ndf[[cols2[col_target]]] = scaler.transform(df[[cols2[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols2[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols2[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols2[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","10a55332":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","8a2aab83":"model = load_model('..\/input\/aceawaterprediction-models\/AD_DGP4.h5')","306ad3a6":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols2[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols2[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","b53b3118":"print(MAE)\nprint(RMSE)","2c6b7c58":"forecast(days_to_predict = 182)","0777d809":"df = df_aux.copy()\n\n# Key number of the column to predict.\ncol_target = 6\n\n# Partition points for train, validation and test.\n\npoint_start = '2017-05-22'\npoint_partition_1 = '2019-07-01'\npoint_partition_2 = '2020-01-01'\npoint_end = '2020-07-01'","325abd5f":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Depth to Groundwater Pozzo 5')\nplt.ylabel('Depth (m)', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.resample(rule='W').mean().loc[point_start:point_partition_1, cols2[col_target]], label='Train')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_1:point_partition_2, cols2[col_target]], label='Validation')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_2:point_end, cols2[col_target]], label='Test')\nplt.legend()\nplt.show()","bc083657":"# Select domain to analize.\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols2[col_target]]])\ndf[[cols2[col_target]]] = scaler.transform(df[[cols2[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols2[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols2[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols2[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","fe63b53c":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","540e4dd6":"model = load_model('..\/input\/aceawaterprediction-models\/AD_DGP5.h5')","983b952d":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols2[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols2[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","94be1f6f":"print(MAE)\nprint(RMSE)","8a450ce7":"forecast(days_to_predict = 182)","9b671f49":"df = df_aux.copy()\n\n# Key number of the column to predict.\ncol_target = 7\n\n# Partition points for train, validation and test.\n\npoint_start = '2017-05-22'\npoint_partition_1 = '2019-07-01'\npoint_partition_2 = '2020-01-01'\npoint_end = '2020-07-01'","92a7f1cb":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Depth to Groundwater Pozzo 6')\nplt.ylabel('Depth (m)', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.resample(rule='W').mean().loc[point_start:point_partition_1, cols2[col_target]], label='Train')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_1:point_partition_2, cols2[col_target]], label='Validation')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_2:point_end, cols2[col_target]], label='Test')\nplt.legend()\nplt.show()","44afc1e4":"# Select domain to analize.\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols2[col_target]]])\ndf[[cols2[col_target]]] = scaler.transform(df[[cols2[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols2[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols2[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols2[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","c989e4bb":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","10696c7e":"model = load_model('..\/input\/aceawaterprediction-models\/AD_DGP6.h5')","60e9cfb9":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols2[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols2[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","74c38143":"print(MAE)\nprint(RMSE)","d98487ab":"forecast(days_to_predict = 182)","bac4aca5":"df = df_aux.copy()\n\n# Key number of the column to predict.\ncol_target = 8\n\n# Partition points for train, validation and test.\n\npoint_start = '2017-05-23'\npoint_partition_1 = '2019-07-01'\npoint_partition_2 = '2020-01-01'\npoint_end = '2020-07-01'","7e6bb01e":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Depth to Groundwater Pozzo 7')\nplt.ylabel('Depth (m)', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.resample(rule='W').mean().loc[point_start:point_partition_1, cols2[col_target]], label='Train')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_1:point_partition_2, cols2[col_target]], label='Validation')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_2:point_end, cols2[col_target]], label='Test')\nplt.legend()\nplt.show()","6dce04f4":"# Select domain to analize.\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols2[col_target]]])\ndf[[cols2[col_target]]] = scaler.transform(df[[cols2[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols2[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols2[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols2[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","b6d3af4c":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","d2b590c4":"model = load_model('..\/input\/aceawaterprediction-models\/AD_DGP7.h5')","53ff6ba8":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols2[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols2[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","7f7dc129":"print(MAE)\nprint(RMSE)","8d14b337":"forecast(days_to_predict = 182)","a4d5db30":"df = df_aux.copy()\n\n# Key number of the column to predict.\ncol_target = 9\n\n# Partition points for train, validation and test.\n\npoint_start = '2017-08-01'\npoint_partition_1 = '2019-07-01'\npoint_partition_2 = '2020-01-01'\npoint_end = '2020-07-01'","7d74cb67":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Depth to Groundwater Pozzo 8')\nplt.ylabel('Depth (m)', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.resample(rule='W').mean().loc[point_start:point_partition_1, cols2[col_target]], label='Train')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_1:point_partition_2, cols2[col_target]], label='Validation')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_2:point_end, cols2[col_target]], label='Test')\nplt.legend()\nplt.show()","07820c30":"# Select domain to analize.\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols2[col_target]]])\ndf[[cols2[col_target]]] = scaler.transform(df[[cols2[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols2[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols2[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols2[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","93feecb6":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","88ff0024":"model = load_model('..\/input\/aceawaterprediction-models\/AD_DGP8.h5')","510cc342":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols2[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols2[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","0de6bd8e":"print(MAE)\nprint(RMSE)","6eed0a37":"forecast(days_to_predict = 182)","3ae3a7bf":"df = df_aux.copy()\n\n# Key number of the column to predict.\ncol_target = 10\n\n# Partition points for train, validation and test.\n\npoint_start = '2018-05-20'\npoint_partition_1 = '2020-02-01'\npoint_partition_2 = '2020-04-15'\npoint_end = '2020-07-01'","bfaf4744":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Depth to Groundwater Pozzo 9')\nplt.ylabel('Depth (m)', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.resample(rule='W').mean().loc[point_start:point_partition_1, cols2[col_target]], label='Train')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_1:point_partition_2, cols2[col_target]], label='Validation')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_2:point_end, cols2[col_target]], label='Test')\nplt.legend()\nplt.show()","f180472c":"# Select domain to analize.\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols2[col_target]]])\ndf[[cols2[col_target]]] = scaler.transform(df[[cols2[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols2[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols2[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols2[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","ea5efddd":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","77e6b28a":"model = load_model('..\/input\/aceawaterprediction-models\/AD_DGP9.h5')","589e0ad8":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols2[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols2[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","6598d9f9":"print(MAE)\nprint(RMSE)","68a83aad":"forecast(days_to_predict = 120)","d498ce19":"# Import database.\n\ndf = pd.read_csv(\"..\/input\/aceawaterprediction\/Aquifer_Luco.csv\")\n\n# Convert 'Date' to date-time type and set it as index.\ndf['Date'] = pd.to_datetime(df['Date'], format='%d\/%m\/%Y')\ndf.set_index('Date', inplace=True)","7b8d5c1a":"# Dictionary of integers to columns names for shorter code.\n\ncols3 = {\n    0:'Rainfall_Simignano',\n    1:'Rainfall_Siena_Poggio_al_Vento',\n    2:'Rainfall_Mensano',\n    3:'Rainfall_Montalcinello',\n    4:'Rainfall_Monticiano_la_Pineta',\n    5:'Rainfall_Sovicille',\n    6:'Rainfall_Ponte_Orgia',\n    7:'Rainfall_Scorgiano',\n    8:'Rainfall_Pentolina',\n    9:'Rainfall_Monteroni_Arbia_Biena',\n    10:'Depth_to_Groundwater_Podere_Casetta',\n    11:'Depth_to_Groundwater_Pozzo_1',\n    12:'Depth_to_Groundwater_Pozzo_3',\n    13:'Depth_to_Groundwater_Pozzo_4',\n    14:'Temperature_Siena_Poggio_al_Vento',\n    15:'Temperature_Mensano',\n    16:'Temperature_Pentolina',\n    17:'Temperature_Monteroni_Arbia_Biena',\n    18:'Volume_Pozzo_1',\n    19:'Volume_Pozzo_3',\n    20:'Volume_Pozzo_4'\n}","f52f8583":"# Clean data.\n\n# Save original series.\ndf_before = df.iloc[:, 10:14].copy()\n\n# Replace the biggest hole of NaN's with the same period from 2011 to 2012 and interpolate in Rainfall_Simignano.\ndf.loc['2013-01-01':'2014-01-30', cols3[0]] = df.loc['2011-01-01':'2012-01-30', cols3[0]].values\ndf[cols3[0]] = df[cols3[0]].interpolate()\n\n# Interpolate all rainfalls, except Pentolia, which will be ignored.\ndf.iloc[:, 1:8] = df.iloc[:, 1:8].interpolate()\ndf[cols3[9]] = df[cols3[9]].interpolate()\n# Todos los valores de Rainfall_Pentolia a 0.\ndf[cols3[8]] = 0\n\ndf.loc[:'2019-01-13', cols3[10]] = df.loc[:'2019-01-13', cols3[10]].interpolate()\ndf[cols3[11]] = df[cols3[11]].interpolate()\n\n# Quitar outliers, llenar los huecos m\u00e1s grandes con los periodos inmediatos anteriores al rev\u00e9s (espejo) e \n# interpolar en Depth_to_Groundwater_Pozzo_3.\ndf.loc[(df.index >= '2018-12-01') & (df.index < '2019-01-31') & (df[cols3[12]] > -9), cols3[12]] = np.nan\ndf.loc['2019-08-21':'2019-11-10', cols3[12]] = df.loc['2018-10-01':'2018-12-21', cols3[12]].iloc[::-1].values + 0.8\ndf.loc['2020-04-07':'2020-04-26', cols3[12]] = df.loc['2020-03-19':'2020-04-07', cols3[12]].iloc[::-1].values\ndf[cols3[12]] = df[cols3[12]].interpolate()\n\ndf[cols3[13]] = df[cols3[13]].interpolate()\n\ndf_aux = df.copy()","b7b54913":"# Visualize original and cleaned data.\ndf = df.resample(rule='W').mean()\nvis_orig_vs_clean(1, 2, [10], cols3, yl=['Depth (m)']*4, fs=(10, 4.5))","25244820":"df = df_aux.copy()\n\n# Key number of the column to predict.\ncol_target = 10\n\n# Partition points for train, validation and test.\n\npoint_start = '2008-02-21'\npoint_partition_1 = '2018-01-01'\npoint_partition_2 = '2018-06-30'\npoint_end = '2019-01-01'","ffb06ef9":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Depth to Groundwater Podere Casetta')\nplt.ylabel('Depth (m)', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.resample(rule='W').mean().loc[point_start:point_partition_1, cols3[col_target]], label='Train')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_1:point_partition_2, cols3[col_target]], label='Validation')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_2:point_end, cols3[col_target]], label='Test')\nplt.legend()\nplt.show()","611089b0":"# Select domain to analize.\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols3[col_target]]])\ndf[[cols3[col_target]]] = scaler.transform(df[[cols3[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols3[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols3[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols3[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","29a81493":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","1d9dc1be":"model = load_model('..\/input\/aceawaterprediction-models\/AL_DGPC.h5')","527079a3":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols3[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols3[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","5d7f91b5":"print(MAE)\nprint(RMSE)","2d1458c5":"forecast(days_to_predict = 365)","97fe2f79":"# Import database.\n\ndf = pd.read_csv(\"..\/input\/aceawaterprediction\/Aquifer_Petrignano.csv\")\n\n# Convert 'Date' to date-time type and set it as index.\ndf['Date'] = pd.to_datetime(df['Date'], format='%d\/%m\/%Y')\ndf.set_index('Date', inplace=True)","60fe8c93":"cols4 = {\n    0:'Rainfall_Bastia_Umbra',\n    1:'Depth_to_Groundwater_P24',\n    2:'Depth_to_Groundwater_P25',\n    3:'Temperature_Bastia_Umbra',\n    4:'Temperature_Petrignano',\n    5:'Volume_C10_Petrignano',\n    6:'Hydrometry_Fiume_Chiascio_Petrignano'\n}","1200c85f":"# Clean data.\n\n# Save original series.\ndf_before = df.iloc[:, 1:3].copy()\n\n# Convert outliers to NaN in Volume_C10_Petrignano.\ndf.loc[df[cols4[5]] == 0, cols4[5]] = np.nan\n\n# Fill the biggest NaN hole with the immediately next period flipped (like a mirror) in \n# Hydrometry_Fiume_Chiascio_Petrignano.\ndf.loc[df[cols4[6]] == 0, cols4[6]] = df.loc['2015-09-22':'2016-02-18', cols4[6]].iloc[::-1].values\n\n# Interpolate all columns.\ndf = df.interpolate()\n\ndf_aux = df.copy()","03a54457":"# Visualize original and cleaned data.\ndf = df.resample(rule='W').mean()\nvis_orig_vs_clean(2, 2, [1, 2], cols4, yl=['Depth (m)']*2, fs=(10, 8), xticks_rate=2)","2313f55a":"df = df_aux.copy()\n\n# Key number of the column to predict.\ncol_target = 1\n\n# Partition points for train, validation and test.\n\npoint_start = '2014-04-21'\npoint_partition_1 = '2019-07-01'\npoint_partition_2 = '2020-01-01'\npoint_end = '2020-07-01'","9b084bd3":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Depth to Groundwater P24')\nplt.ylabel('Depth (m)', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.resample(rule='W').mean().loc[point_start:point_partition_1, cols4[col_target]], label='Train')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_1:point_partition_2, cols4[col_target]], label='Validation')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_2:point_end, cols4[col_target]], label='Test')\nplt.legend()\nplt.show()","85c07e41":"# Select domain to analize.\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols4[col_target]]])\ndf[[cols4[col_target]]] = scaler.transform(df[[cols4[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols4[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols4[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols4[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","710a18e8":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","d9366802":"model = load_model('..\/input\/aceawaterprediction-models\/AP_DGP24.h5')","fc3dec37":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols4[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols4[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","19241dd2":"print(MAE)\nprint(RMSE)","1f44965a":"forecast(days_to_predict = 365)","3cf7b962":"df = df_aux.copy()\n\n# Key number of the column to predict.\ncol_target = 2\n\n# Partition points for train, validation and test.\n\npoint_start = '2014-04-21'\npoint_partition_1 = '2019-07-01'\npoint_partition_2 = '2020-01-01'\npoint_end = '2020-07-01'","4a2c06ce":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Depth to Groundwater P25')\nplt.ylabel('Depth (m)', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.resample(rule='W').mean().loc[point_start:point_partition_1, cols4[col_target]], label='Train')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_1:point_partition_2, cols4[col_target]], label='Validation')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_2:point_end, cols4[col_target]], label='Test')\nplt.legend()\nplt.show()","ef38f9d0":"# Select domain to analize.\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols4[col_target]]])\ndf[[cols4[col_target]]] = scaler.transform(df[[cols4[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols4[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols4[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols4[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","696d0eb4":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","e7a6e268":"model = load_model('..\/input\/aceawaterprediction-models\/AP_DGP25.h5')","cebb655f":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols4[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols4[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","614b8d23":"print(MAE)\nprint(RMSE)","8f8d9b7c":"forecast(days_to_predict = 365)","068393dd":"# Import database.\n\ndf = pd.read_csv(\"..\/input\/aceawaterprediction\/Lake_Bilancino.csv\")\n\n# Convert 'Date' to date-time type and set it as index.\ndf['Date'] = pd.to_datetime(df['Date'], format='%d\/%m\/%Y')\ndf.set_index('Date', inplace=True)","976d4e78":"# Dictionary of integers to columns names for shorter code.\n\ncols5 = {\n    0:'Rainfall_S_Piero',\n    1:'Rainfall_Mangona',\n    2:'Rainfall_S_Agata',\n    3:'Rainfall_Cavallina',\n    4:'Rainfall_Le_Croci',\n    5:'Temperature_Le_Croci',\n    6:'Lake_Level',\n    7:'Flow_Rate'\n}","9db7e2e4":"# Clean data.\n\n# Save original series.\ndf_before = df.iloc[:, 6:8].copy()\n\ndf = df.interpolate()\n\ndf_aux = df.copy()","0781bb8e":"# Visualize original and cleaned data.\ndf = df.resample(rule='W').mean()\nvis_orig_vs_clean(2, 2, [6, 7], cols5, fs=(10, 8), yl=['Level', 'Flow rate'], xticks_rate=2)","64c7b05f":"df = df_aux.copy()\n\n# Key number of the column to predict.\ncol_target = 6\n\n# Partition points for train, validation and test.\n\npoint_start = '2013-03-18'\npoint_partition_1 = '2019-07-01'\npoint_partition_2 = '2020-01-01'\npoint_end = '2020-07-01'","e6efe00b":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Lake Level')\nplt.ylabel('Level', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.resample(rule='W').mean().loc[point_start:point_partition_1, cols5[col_target]], label='Train')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_1:point_partition_2, cols5[col_target]], label='Validation')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_2:point_end, cols5[col_target]], label='Test')\nplt.legend()\nplt.show()","ade330f1":"# Select domain to analize.\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols5[col_target]]])\ndf[[cols5[col_target]]] = scaler.transform(df[[cols5[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols5[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols5[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols5[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","071ab6dd":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","96287c6e":"model = load_model('..\/input\/aceawaterprediction-models\/LB_LL.h5')","9a393095":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols5[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols5[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2, yl=['Level']*2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","9b560532":"print(MAE)\nprint(RMSE)","f94f6f38":"forecast(days_to_predict = 365, yl='Level')","dcc99832":"df = df_aux.copy()\n\n# Key number of the column to predict.\ncol_target = 7\n\n# Partition points for train, validation and test.\n\npoint_start = '2013-08-02'\npoint_partition_1 = '2019-07-01'\npoint_partition_2 = '2020-01-01'\npoint_end = '2020-07-01'","3b57f6f3":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Flow Rate')\nplt.ylabel('Flow rate', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.resample(rule='W').mean().loc[point_start:point_partition_1, cols5[col_target]], label='Train')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_1:point_partition_2, cols5[col_target]], label='Validation')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_2:point_end, cols5[col_target]], label='Test')\nplt.legend()\nplt.show()","35018288":"# Select domain to analize.\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols5[col_target]]])\ndf[[cols5[col_target]]] = scaler.transform(df[[cols5[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols5[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols5[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols5[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","9be1a19d":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","5c375718":"model = load_model('..\/input\/aceawaterprediction-models\/LB_FR.h5')","1124b152":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols5[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols5[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2, yl=['Flow rate']*2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","f6424adc":"print(MAE)\nprint(RMSE)","423ec708":"forecast(days_to_predict = 365, yl='Flow rate')","209e67eb":"# Import database.\n\ndf = pd.read_csv(\"..\/input\/aceawaterprediction\/River_Arno.csv\")\n\n# Convert 'Date' to date-time type and set it as index.\ndf['Date'] = pd.to_datetime(df['Date'], format='%d\/%m\/%Y')\ndf.set_index('Date', inplace=True)","95b983e4":"# Dictionary of integers to columns names for shorter code.\n\ncols6 = {\n    0:'Rainfall_Le_Croci',\n    1:'Rainfall_Cavallina',\n    2:'Rainfall_S_Agata',\n    3:'Rainfall_Mangona',\n    4:'Rainfall_S_Piero',\n    5:'Rainfall_Vernio',\n    6:'Rainfall_Stia',\n    7:'Rainfall_Consuma',\n    8:'Rainfall_Incisa',\n    9:'Rainfall_Montevarchi',\n    10:'Rainfall_S_Savino',\n    11:'Rainfall_Laterina',\n    12:'Rainfall_Bibbiena',\n    13:'Rainfall_Camaldoli',\n    14:'Temperature_Firenze',\n    15:'Hydrometry_Nave_di_Rosano'\n}","60842abd":"# Clean data.\n\n# Save original series.\ndf_before = df.iloc[:, 15:16].copy()\n\n# Fill the biggest NaN hole in 2008 with a similar period from 2005, convert outliers to NaN and interpolate in \n# Hydrometry_Nave_di_Rosano.\ndf.loc['2008-07-03':'2008-12-31', cols6[15]] = df.loc['2005-06-03':'2005-12-01', cols6[15]].values\ndf.loc[df[cols6[15]] == 0, cols6[15]] = np.nan\ndf[cols6[15]] = df[cols6[15]].interpolate()\n\ndf_aux = df.copy()","f9beeeae":"# Visualize original and cleaned data.\ndf = df.resample(rule='W').mean()\nvis_orig_vs_clean(1, 2, [15], cols6, fs=(10, 4), yl=['Hydrometry'], xticks_rate=2)","8465efbf":"df = df_aux.copy()\n\n# Key number of the column to predict.\ncol_target = 15\n\n# Partition points for train, validation and test.\n\npoint_start = '2015-04-16'\npoint_partition_1 = '2019-07-01'\npoint_partition_2 = '2020-01-01'\npoint_end = '2020-07-01'","cbad7930":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Hydrometry Nave di Rosano')\nplt.ylabel('Hydrometry', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.resample(rule='W').mean().loc[point_start:point_partition_1, cols6[col_target]], label='Train')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_1:point_partition_2, cols6[col_target]], label='Validation')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_2:point_end, cols6[col_target]], label='Test')\nplt.legend()\nplt.show()","0bfbe6e0":"# Select domain to analize.\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols6[col_target]]])\ndf[[cols6[col_target]]] = scaler.transform(df[[cols6[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols6[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols6[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols6[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","4a065822":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","27ff7f20":"model = load_model('..\/input\/aceawaterprediction-models\/RA_HNR.h5')","b345071c":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols6[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols6[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2, yl=['Hydrometry']*2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","3bef5be1":"print(MAE)\nprint(RMSE)","620a694c":"forecast(days_to_predict = 365, yl='Hydrometry')","7eb8428f":"# Import database.\n\ndf = pd.read_csv(\"..\/input\/aceawaterprediction\/Water_Spring_Amiata.csv\")\n\n# Convert 'Date' to date-time type and set it as index.\ndf['Date'] = pd.to_datetime(df['Date'], format='%d\/%m\/%Y')\ndf.set_index('Date', inplace=True)","418e8673":"# Dictionary of integers to columns names for shorter code.\n\ncols7 = {\n    0:'Rainfall_Castel_del_Piano',\n    1:'Rainfall_Abbadia_S_Salvatore',\n    2:'Rainfall_S_Fiora',\n    3:'Rainfall_Laghetto_Verde',\n    4:'Rainfall_Vetta_Amiata',\n    5:'Depth_to_Groundwater_S_Fiora_8',\n    6:'Depth_to_Groundwater_S_Fiora_11bis',\n    7:'Depth_to_Groundwater_David_Lazzaretti',\n    8:'Temperature_Abbadia_S_Salvatore',\n    9:'Temperature_S_Fiora',\n    10:'Temperature_Laghetto_Verde',\n    11:'Flow_Rate_Bugnano',\n    12:'Flow_Rate_Arbure',\n    13:'Flow_Rate_Ermicciolo',\n    14:'Flow_Rate_Galleria_Alta'\n}","295099c9":"# Clean data.\n\n# Save original series.\ndf_before = df.iloc[:, 11:15].copy()\n\n# Fill the NaN holes with similar periods in Flow_Rate_Bugnano.\ndf.loc['2018-08-01':'2018-08-31', cols7[11]] = df.loc['2018-05-01':'2018-05-31', cols7[11]].values + 0.003\ndf.loc['2018-12-01':'2018-12-31', cols7[11]] = df.loc['2018-03-01':'2018-03-31', cols7[11]].values - 0.035\ndf.loc['2019-01-01':'2019-01-31', cols7[11]] = df.loc['2018-03-01':'2018-03-31', cols7[11]].values - 0.05\n\n# Fill the NaN holes with similar periods in Flow_Rate_Arbure.\ndf.loc['2018-04-01':'2018-04-30', cols7[12]] = df.loc['2017-10-01':'2017-10-30', cols7[12]].values + 0.8\ndf.loc['2018-08-01':'2018-08-31', cols7[12]] = df.loc['2017-11-01':'2017-12-01', cols7[12]].values - 0.4\ndf.loc['2018-08-31', cols7[12]] = df.loc['2018-08-30', cols7[12]]\n\ndf.loc['2018-12-01':'2018-12-31', cols7[12]] = df.loc['2018-11-01':'2018-12-01', cols7[12]].values - 0.1\ndf.loc['2018-12-31':'2019-01-31', cols7[12]] = df.loc['2018-10-30':'2018-11-30', cols7[12]].values\n\n# Fill the NaN holes with similar periods in and descend the anomaly in Flow_Rate_Ermicciolo.\ndf.loc['2016-02-01':'2016-02-29', cols7[13]] = df.loc['2016-03-01':'2016-03-29', cols7[13]].values - 0.1\ndf.loc['2016-08-01':'2016-10-31', cols7[13]] = df.loc['2016-03-01':'2016-05-31', cols7[13]].values - 0.05\ndf.loc['2019-09-01':'2019-12-31', cols7[13]] -= 2.5\n\ndf_aux = df.copy()","0665c4cd":"# Visualize original and cleaned data.\ndf = df.resample(rule='W').mean()\nvis_orig_vs_clean(4, 2, [11, 12, 13, 14], cols7, fs=(10, 13), yl=['Flow rate']*4)","391d795e":"df = df_aux.copy()\n\n# Key number of the column to predict.\ncol_target = 11\n\n# Partition points for train, validation and test.\n\npoint_start = '2018-03-01'\npoint_partition_1 = '2020-01-01'\npoint_partition_2 = '2020-04-01'\npoint_end = '2020-07-01'","0ee7bba8":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Flow Rate Bugnano')\nplt.ylabel('Flow rate', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.resample(rule='W').mean().loc[point_start:point_partition_1, cols7[col_target]], label='Train')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_1:point_partition_2, cols7[col_target]], label='Validation')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_2:point_end, cols7[col_target]], label='Test')\nplt.legend()\nplt.show()","e055a97e":"# Select domain to analize.\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols7[col_target]]])\ndf[[cols7[col_target]]] = scaler.transform(df[[cols7[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols7[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols7[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols7[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","f5fcdb06":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","8be1e523":"model = load_model('..\/input\/aceawaterprediction-models\/WSA_FRB.h5')","707699f7":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols7[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols7[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2, yl=['Flow rate']*2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","9ebd54aa":"print(MAE)\nprint(RMSE)","461a7d9a":"forecast(days_to_predict = 90, yl='Flow rate')","b5bc72d7":"df = df_aux.copy()\n\n# Key number of the column to predict.\ncol_target = 12\n\n# Partition points for train, validation and test.\n\npoint_start = '2018-06-01'\npoint_partition_1 = '2020-01-01'\npoint_partition_2 = '2020-04-01'\npoint_end = '2020-07-01'","fbfa37a0":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Flow Rate Arbure')\nplt.ylabel('Flow rate', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.resample(rule='W').mean().loc[point_start:point_partition_1, cols7[col_target]], label='Train')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_1:point_partition_2, cols7[col_target]], label='Validation')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_2:point_end, cols7[col_target]], label='Test')\nplt.legend()\nplt.show()","dfc948d0":"# Select domain to analize.\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols7[col_target]]])\ndf[[cols7[col_target]]] = scaler.transform(df[[cols7[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols7[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols7[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols7[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","93f8bfa7":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","093d3ad3":"model = load_model('..\/input\/aceawaterprediction-models\/WSA_FRA.h5')","6a4b07e9":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols7[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols7[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2, yl=['Flow rate']*2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","601d4016":"print(MAE)\nprint(RMSE)","e9992bff":"forecast(days_to_predict = 90, yl='Flow rate')","bdb03518":"df = df_aux.copy()\n\n# Key number of the column to predict.\ncol_target = 13\n\n# Partition points for train, validation and test.\n\npoint_start = '2018-04-01'\npoint_partition_1 = '2020-01-01'\npoint_partition_2 = '2020-04-01'\npoint_end = '2020-07-01'","ef2596d7":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Flow Rate Ermicciolo')\nplt.ylabel('Flow rate', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.resample(rule='W').mean().loc[point_start:point_partition_1, cols7[col_target]], label='Train')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_1:point_partition_2, cols7[col_target]], label='Validation')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_2:point_end, cols7[col_target]], label='Test')\nplt.legend()\nplt.show()","16a0d6b8":"# Select domain to analize.\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols7[col_target]]])\ndf[[cols7[col_target]]] = scaler.transform(df[[cols7[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols7[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols7[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols7[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","c5660dea":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","d511164c":"model = load_model('..\/input\/aceawaterprediction-models\/WSA_FRE.h5')","05537145":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols7[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols7[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2, yl=['Flow rate']*2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","5cdbc063":"print(MAE)\nprint(RMSE)","499b6886":"forecast(days_to_predict = 90, yl='Flow rate')","3a8339e1":"df = df_aux.copy()\n\n# Key number of the column to predict.\ncol_target = 14\n\n# Partition points for train, validation and test.\n\npoint_start = '2015-01-01'\npoint_partition_1 = '2019-07-01'\npoint_partition_2 = '2020-01-01'\npoint_end = '2020-07-01'","c903ef77":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Flow Rate Galleria Alta')\nplt.ylabel('Flow rate', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.resample(rule='W').mean().loc[point_start:point_partition_1, cols7[col_target]], label='Train')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_1:point_partition_2, cols7[col_target]], label='Validation')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_2:point_end, cols7[col_target]], label='Test')\nplt.legend()\nplt.show()","ee03b3de":"# Select domain to analize.\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols7[col_target]]])\ndf[[cols7[col_target]]] = scaler.transform(df[[cols7[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols7[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols7[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols7[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","7c9c394a":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","d83bb606":"model = load_model('..\/input\/aceawaterprediction-models\/WSA_FRGA.h5')","0384efbe":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols7[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols7[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2, yl=['Flow rate']*2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","11012458":"print(MAE)\nprint(RMSE)","d7903274":"forecast(days_to_predict = 365, yl='Flow rate')","302b9254":"# Import database.\n\ndf = pd.read_csv(\"..\/input\/aceawaterprediction\/Water_Spring_Lupa.csv\")\n\n# Convert 'Date' to date-time type and set it as index.\ndf['Date'] = pd.to_datetime(df['Date'], format='%d\/%m\/%Y')\ndf.set_index('Date', inplace=True)","b6bb1994":"# Dictionary of integers to columns names for shorter code.\n\ncols8 = {\n    0:'Rainfall_Terni',\n    1:'Flow_Rate_Lupa'\n}","cfc285af":"# Clean data.\n\n# Save original series.\ndf_before = df.iloc[:, 1:2].copy()\n\ndf = df.interpolate()\n\ndf_aux = df.copy()","cd6d4966":"# Visualize original and cleaned data.\ndf = df.resample(rule='W').mean()\nvis_orig_vs_clean(1, 2, [1], cols8, fs=(10, 4), yl=['Flow rate'])","6506e5aa":"df = df_aux.copy()\n\n# Key number of the column to predict.\ncol_target = 1\n\n# Partition points for train, validation and test.\n\npoint_start = '2020-03-05'\npoint_partition_1 = '2020-06-01'\npoint_partition_2 = '2020-06-15'\npoint_end = '2020-07-01'","99721fb1":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Flow Rate Lupa')\nplt.ylabel('Flow rate', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.resample(rule='W').mean().loc[point_start:point_partition_1, cols8[col_target]], label='Train')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_1:point_partition_2, cols8[col_target]], label='Validation')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_2:point_end, cols8[col_target]], label='Test')\nplt.legend()\nplt.show()","953ca22e":"# Select domain to analize.\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols8[col_target]]])\ndf[[cols8[col_target]]] = scaler.transform(df[[cols8[col_target]]])\n\n#scaler = MinMaxScaler()\n#scaler.fit(df[[cols8[col_target], 't']])\n#df[[cols8[col_target], 't']] = scaler.transform(df[[cols8[col_target], 't']])\n\n#scaler2 = MinMaxScaler()\n#scaler2.fit(df[['t']])\n#df[['t']] = scaler2.transform(df[['t']])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\n#train_data_aux = train_data_aux.iloc[1:, :]\n#test_data_aux = test_data_aux.iloc[:-1, :]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols8[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols8[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols8[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","bc96818f":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 3, \n#                         max_sin = 0, max_cos = 0)","9c798bbd":"#model = load_model('..\/input\/aceawaterprediction-models\/WSL_FRL.h5')","51e56f69":"forecast1 = np.array([[0.00602318],\n       [0.02730885],\n       [0.058597  ],\n       [0.09458612],\n       [0.1352762 ],\n       [0.18066731],\n       [0.23075934],\n       [0.28555238],\n       [0.34504634],\n       [0.40924132],\n       [0.4781372 ],\n       [0.55173415],\n       [0.630032  ],\n       [0.7130309 ],\n       [0.8007308 ],\n       [0.89313143],\n       [0.9902331 ]])\n\nforecast2 = np.array([[0.7130309 ],\n       [0.8007308 ],\n       [0.89313143],\n       [0.9902331 ]])\n\nforecast3 = np.array([[0.00602318],\n       [0.02730885],\n       [0.058597  ],\n       [0.09458612],\n       [0.1352762 ],\n       [0.18066731],\n       [0.23075934],\n       [0.28555238],\n       [0.34504634],\n       [0.40924132],\n       [0.4781372 ],\n       [0.55173415],\n       [0.630032  ],\n       [0.7130309 ],\n       [0.80073065],\n       [0.89313143],\n       [0.9902331 ],\n       [1.0920358 ],\n       [1.1985396 ],\n       [1.3097445 ]])\n\nforecast3 = scaler.inverse_transform(forecast3)","1a07306c":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\n#forecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols8[col_target]]])\n\n#forecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols8[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2, yl=['Flow rate']*2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","f447b045":"print(MAE)\nprint(RMSE)","3ea309e8":"n1 = 3\nn2 = len(real1)\n\ndates1 = [pd.to_datetime(point_start) + pd.Timedelta(weeks=i) for i in range(n2 + 3)]\ndates2 = [pd.to_datetime(point_start) + pd.Timedelta(weeks=i) for i in range(n2)]\n    \n\nplt.figure(figsize=(10,5))\nplt.title('Best model results')\nplt.ylabel('Flow rate', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(dates2, real1, label='Train + Validation + Test')\nplt.plot(dates2, forecast1, label='Tests Forecast')\nplt.plot(dates1[-3:], forecast3[-3:], label='New Forecast')\nplt.legend()\nplt.show()","5b8f728b":"# Import database.\n\ndf = pd.read_csv(\"..\/input\/aceawaterprediction\/Water_Spring_Madonna_di_Canneto.csv\")\n\n# Convert 'Date' to date-time type and set it as index.\ndf['Date'] = pd.to_datetime(df['Date'], format='%d\/%m\/%Y')\ndf.set_index('Date', inplace=True)","f457e7d3":"# Dictionary of integers to columns names for shorter code.\n\ncols9 = {\n    0:'Rainfall_Settefrati',\n    1:'Temperature_Settefrati',\n    2:'Flow_Rate_Madonna_di_Canneto'\n}","4602cc82":"# Clean data.\n\n# Save original series.\ndf_before = df.iloc[:, 2:3].copy()\n\n# Only interpolate Flow_Rate_Madonna_di_Canneto column.\ndf[cols9[2]] = df[cols9[2]].interpolate()\n\ndf_aux = df.copy()","8afa2e04":"# Visualize original and cleaned data.\ndf = df.resample(rule='W').mean()\nvis_orig_vs_clean(1, 2, [2], cols9, fs=(10, 4), yl=['Flow rate'])","ce30be06":"df = df_aux.copy()\n\n# Key number of the column to predict.\ncol_target = 2\n\n# Partition points for train, validation and test.\n\npoint_start = '2019-11-05'\npoint_partition_1 = '2020-06-01'\npoint_partition_2 = '2020-06-15'\npoint_end = '2020-07-01'","4101d66b":"# Visualize train + validation + test data.\n\nplt.figure(figsize=(10,5))\nplt.title('Flow Rate Madonna di Canneto')\nplt.ylabel('Flow rate', fontsize=10, labelpad=8)\nplt.xlabel('Date', fontsize=10, labelpad=8)\nplt.plot(df.resample(rule='W').mean().loc[point_start:point_partition_1, cols9[col_target]], label='Train')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_1:point_partition_2, cols9[col_target]], label='Validation')\nplt.plot(df.resample(rule='W').mean().loc[point_partition_2:point_end, cols9[col_target]], label='Test')\nplt.legend()\nplt.show()","12566f1d":"# Select domain to analize.\ndf = df.loc[point_start:point_end]\n\n# Add the integer type time column.\ndf.insert(len(df.columns), 't', np.array(range(len(df))).astype('float64'), True) \n\ndf = df.resample(rule='W').mean()\n\n# Scale the series to 0-1 domain to feed neural networks.\nscaler = MinMaxScaler()\nscaler.fit(df[[cols9[col_target]]])\ndf[[cols9[col_target]]] = scaler.transform(df[[cols9[col_target]]])\n\n# Divide series into train, validation and test.\ntrain_data_aux = df[point_start:point_partition_1]\nvalidation_data_aux = df[point_partition_1:point_partition_2]\ntest_data_aux = df[point_partition_2:point_end]\n\ntrain_data = train_data_aux['t']\ntrain_target_data =  train_data_aux[cols9[col_target]]\nvalidation_data = validation_data_aux['t']\nvalidation_target_data = validation_data_aux[cols9[col_target]]\ntest_data = test_data_aux['t']\ntest_target_data = test_data_aux[cols9[col_target]]\n              \ntrain_data = train_data.to_numpy()\ntrain_target_data = train_target_data.to_numpy()\nvalidation_data = validation_data.to_numpy()\nvalidation_target_data = validation_target_data.to_numpy()","0a84386d":"# Run genetic algorithm.\n\n#model = geneticAlgorithm(train_data, train_target_data, validation_data, validation_target_data, test_data, \n#                         test_target_data, max_batch_size = int(len(train_data)\/2), popul_inic = 50, \n#                         parents_pairs = 25, sons_per_pair = 1, prob_mutation = 0.05, num_generations = 10, \n#                         max_sin = 200, max_cos = 200)","6c1eb35d":"model = load_model('..\/input\/aceawaterprediction-models\/WSMC_FRMC.h5')","c73aabc5":"# Calculate MAE and RMSE and plot results.\n\n# Plot train + validation + test forecast,\n# and validation + test forecast.\n\nforecast1 = model.predict(np.concatenate((train_data, validation_data, test_data)))\n\nforecast1 = scaler.inverse_transform(forecast1)\nreal1 = scaler.inverse_transform(df.loc[point_start:point_end, [cols9[col_target]]])\n\nforecast2 = model.predict(np.concatenate((validation_data, test_data)))\n\nforecast2 = scaler.inverse_transform(forecast2)\nreal2 = scaler.inverse_transform(df.loc[point_partition_1:point_end, [cols9[col_target]]])\n\nplotResults(forecast1, real1, forecast2, real2, yl=['Flow rate']*2)\n\nMAE = mean_absolute_error(real2, forecast2)\nRMSE = np.sqrt(mean_squared_error(real2, forecast2))","306d9a0f":"print(MAE)\nprint(RMSE)","6cab7100":"forecast(days_to_predict = 60, yl='Flow rate')","86955671":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 0.2646**<\/font>\n\n<font size=\"6\">**RMSE = 0.3326**<\/font>","e064ccb4":"## Aquifer Doganella\n\n### Data cleaning and preprocessing\n\nThe columns to be worked with inn this waterbodie are `Depth_to_Groundwater_Pozzo_1` to `Depth_to_Groundwater_Pozzo_9`.\n\nThe changes made to each of this columns are:\n\n* `Depth_to_Groundwater_Pozzo_1`: For this column, only the `NaN` values were interpolated.\n\n* `Depth_to_Groundwater_Pozzo_2`: In this column, the outlier at the end of the series was removed (converted to `NaN`) and after this the `NaN` values were interpolated.\n\n* `Depth_to_Groundwater_Pozzo_3`: The peaks and outliers after 2018 in this series were removed. A point at 2019-09-19 is added in order to the result after interpolation would be more congruent with the series.\n\n* `Depth_to_Groundwater_Pozzo_4`: The only thing done in this series was interpolate the `NaN` values.\n\n* `Depth_to_Groundwater_Pozzo_5`: The sharp valley near 2018 was removed and then `NaN` values were interpolated.\n\n* `Depth_to_Groundwater_Pozzo_6`: The outliers between 2019 and 2020 were removed and then an interpolation was performed.\n\n* `Depth_to_Groundwater_Pozzo_7`: The only thing done in this series was interpolate the `NaN` values.\n\n* `Depth_to_Groundwater_Pozzo_8`: Outliers near the end of the series were removed and then interpolated with the rest of `NaN`'s.\n\n* `Depth_to_Groundwater_Pozzo_9`:The only thing done in this series was interpolate the `NaN` values.\n\nAfter clean all this columns, they all were subsampled to week frequency by mean. \n\nThe following plots correspond to the columns to analize before and after the data cleaning and preprocessing.","3aeb416e":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 0.4732**<\/font>\n\n<font size=\"6\">**RMSE = 0.6427**<\/font>","6c2846ec":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","0373246f":"### Depth to Groundwater Pozzo 3\n\nSame as `Depth_to_Groundwater_Pozzo_1`, there are two behaviours separated by 2017 in this series. So I took the data from 2017 to train the model.\n\nAll three series, train + validation + test are shown below:","d1df72c9":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","59fca9df":"### Lake Level\n\nThe data used to predict this series begins near 2014, since there is a regular pattern from there and through the next 6 years.\n\nAll three series, train + validation + test are shown below:","146f9fd8":"### Flow Rate Madonna di Canneto\n\nThere is a chaotic behaviour from begin to end of 2019 in this series, so this prediction will be based only on the final stable trend from to the end of the series.\n\nAll three series, train + validation + test, are shown below:","fb239107":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","cdeb6c68":"### Depth to Groundwater Pozzo 2\n\nThe recent data chosen to train the model for this series begins in 2015.\n\nAll three series, train + validation + test are shown below:","7cd9839e":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 0.3164**<\/font>\n\n<font size=\"6\">**RMSE = 0.3953**<\/font>","ad03d675":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","8fc7ee54":"After running the genetic algorithm and getting the best model, the forecasting results are the following:\n\n(I had a trouble running this model in this notebook, in my editor runs fine, so I copied the results from there)","a2a585da":"## Water Spring Lupa\n\n### Data cleaning and preprocessing\n\nThe column to predict in this waterbodie is `Flow_Rate_Lupa`.\n\nThe changes made to this column are:\n\n* `Flow_Rate_Lupa`: The `NaN` values were interpolated.\n\nAfter clean this column, it was subsampled to week frequency by mean. \n\nThe following plots correspond to the column to analize before and after the data cleaning and preprocessing.","e442c912":"### Flow Rate Bugnano\n\nThe most recent behaviour is intended to be predicted in this series, so domain of the data given to the model starts after the highest peak through the end.\n\nAll three series, train + validation + test are shown below:","97c5481e":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","40f88359":"> \n<p style=\"line-height:1.4; font-size:400%; text-align:center; font-weight: bold;\">\nWater supplies forecasting with neural networks and genetic algorithms\n<\/p>","4bb875d6":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 0.2468**<\/font>\n\n<font size=\"6\">**RMSE = 0.3905**<\/font>","fa6c18bb":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 3.5424**<\/font>\n\n<font size=\"6\">**RMSE = 5.180**<\/font>","da1b10ba":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","a6612999":"### Depth to Groundwater Pozzo 8\n\nThere is a \"sharped valleys\" behaviour in this series that begins mid 2017. I infer this behaviour is likely to continue, so I train the model with that data.\n\nAll three series, train + validation + test are shown below:","2d9de58d":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 0.2854**<\/font>\n\n<font size=\"6\">**RMSE = 0.3982**<\/font>","12994483":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","2dd67f0b":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 1.4036**<\/font>\n\n<font size=\"6\">**RMSE = 1.7816**<\/font>","a5435b20":"## Aquifer Luco\n\n### Data cleaning and preprocessing\n\nThe column to predict in this table is `Depth_to_Groundwater_Podere_Casetta`.\n\nThe changes made to column are:\n\n* `Depth_to_Groundwater_Pozzo_1`: The `NaN` values were interpolated until 2019.\n\nAfter clean the series, it was subsampled to week frequency by mean. \n\nThe series befere and after claning process are showed below:","36cbe2b7":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 0.4114**<\/font>\n\n<font size=\"6\">**RMSE = 0.6268**<\/font>","f009b11b":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","c05866c8":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 0.0047**<\/font>\n\n<font size=\"6\">**RMSE = 0.0059**<\/font>","e35517e2":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","4beaff97":"## Water Spring Madonna di Canneto\n\n### Data cleaning and preprocessing\n\nThe column to predict in this waterbodie is `Flow_Rate_Madonna_di_Canneto`.\n\nThe changes made to this column are:\n\n* `Flow_Rate_Madonna_di_Canneto`: The `NaN` values were interpolated.\n\nAfter clean this column, it was subsampled to week frequency by mean. \n\nThe following plots correspond to the column to analize before and after the data cleaning and preprocessing.","1fe2ce70":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 0.3005**<\/font>\n\n<font size=\"6\">**RMSE = 0.3873**<\/font>","412b3012":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","e42a6ad5":"# Water Springs\n\n## Water Spring Amiata\n\n### Data cleaning and preprocessing\n\nThe columns to predict in this waterbodie are `Flow_Rate_Bugnano`, `Flow_Rate_Arbure`, `Flow_Rate_Ermicciolo` and `Flow_Rate_Galleria_Alta`.\n\nThe changes made to each of this columns are:\n\n* `Flow_Rate_Bugnano`: The zero values were replaced with a more congruent period after 2018.\n\n* `Flow_Rate_Arbure`: The zero values were replaced with a more congruent period after 2018 and the peak near 2018 was lowered to be more consistent with the series behaviour.\n\n* `Flow_Rate_Ermicciolo`: The zero values were replaced with a more congruent period and the peak near 2020 was lowered to be more consistent with the series behaviour.\n\n* `Flow_Rate_Galleria_Alta`: No changes were performed on this series.\n\nAfter clean all this columns, they all were subsampled to week frequency by mean. \n\nThe following plots correspond to the columns to analize before and after the data cleaning and preprocessing.","a611e89c":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","4510bac8":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","01c367c6":"### Depth to Groundwater Pozzo 7\n\nThe data used to predict this series begins mid 2017 as well as `Depth_to_Groundwater_Pozzo_5` and `Depth_to_Groundwater_Pozzo_6`.\n\nAll three series, train + validation + test are shown below:","61bdb976":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","125d67be":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","ccbe6e80":"# Introduction\n\nIn this notebook I'll introduce the method used for predicting some characteristics of diverses classes of waterbodies which are in Italy.\n\nThis waterbodies are divided in four categories:\n\n* Aquifers\n* Lakes\n* Rivers\n* Water springs\n\nFor each category there is one or more waterbodies to analize:\n\nAquifers:\n  + Aquifer Auser\n  + Aquifer Doganella\n  + Aquifer Luco\n  + Aquifer Petrignano\n  \nLakes:\n  + Lake Bilancino\n  \nRivers:\n  + River Arno\n  \nWater springs:\n  + Water Spring Amiata\n  + Water Spring Lupa\n  + Water Spring Madonna di Canneto\n\nEach waterbodie has associated a table with numerical data, in which each column consist in a time series with information about measures of rainfall, temperature, volume, depth to ground water or hydrometry, obtained in regions related to the waterbodie.\nFor example this is how the table of Auser aquifer looks like:","ace23977":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 0.4927**<\/font>\n\n<font size=\"6\">**RMSE = 0.6685**<\/font>","fb92edf8":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","b0752d79":"# Lakes\n\n## Lake Bilancino\n\n### Data cleaning and preprocessing\n\nThe columns to predict are `Lake_Level` and `Flow_Rate`.\n\nThe changes made to each of this columns are:\n\n* `Lake_Level`: The `NaN` values were interpolated.\n\n* `Flow_Rate`: The `NaN` values were interpolated.\n\nAfter clean all this columns, they all were subsampled to week frequency by mean. \n\nThe following plots correspond to the columns to analize before and after the data cleaning and preprocessing.","6dc52969":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","11d44bc7":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 0.3610**<\/font>\n\n<font size=\"6\">**RMSE = 0.4554**<\/font>","8c031a9c":"### Flow Rate Galleria Alta\n\nAll data of this series was used to train the model and validate its results.\n\nAll three series, train + validation + test, are shown below:","71e6b7ea":"### Depth to Groundwater SAL\n\nThe used data for this series is in the domain that begins in 2013-08-02, because from this point, a more regular behaviour begins in the series.\n\nAll three series, train + validation + test are shown below:","2024db91":"### Depth to Groundwater P25\n\nThis series is very similar to `Depth_to_Groundwater_P24`, whence the domain of the data given used for train and validate the model is the same.\n\nAll three series, train + validation + test are shown below:","cdd57e75":"### Flow Rate Arbure\n\n\nLike in `Flow_Rate_Bugnano`, the data given to the model starts after the highest peak through the end.\n\nAll three series, train + validation + test are shown below:","ad73f810":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","87faa7ad":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","14760d5f":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","45239e75":"# Conclusion\n\nIn this work, a method for time series forecasting about water supplies of different waterbodies was presented. The method consists in a neural network that has a hidden layer with sine and cosine activation functions. Through backpropagation process, the amplitude, frequency and gap parameters of the periodics functions are optimized to approximate to the desired series. Through a genetic algorithm, some hyperparameters of the net, like the number of hidden nodes, are optimized.\n\n## Model advantages\n\n* The model only depends only on a time variable, so exogenous data could be complementary but in many cases it is not needed.\n\n* With the network achitecture used in this work, a mathematical model of the series with it's natural frequencies can be easily extracted from the net.\n\n* It is simplier than LSTM or another kind of recurrent or convolutional neural network.\n\n* Being a neural network, it's a flexible model which admit adding other kinds of functions in a very easy way, in order to adapt the net to the behaviour of the series, or add more complexity through more hidden layers (at the expense of explicability).\n\n* About 6 months forecasts can be made with an acceptable accuracy, but if only short predictions are needed the accuracy can increase significantly.\n\n* Can be applied to any waterbodie time series and, in general, to almost any kind of time series.\n\n## Model disadvantages\n\n* Time series explanations are in terms of its natural frequencies.\n\n* The model can hardly predict very infrequent events like an isolated sudden growth of the series values.\n\n* Requires past data about the series to predict (although, can be adapted to predict based on exogenous data).\n\nAs future work, both the neural network and genetic algorithm can be improved by several techniques that waren't applied due lack of time, like including data of other variables in the net or add more parameters to genetic algorithm genes like learned weights of previous generations models as well as prerforming smoothing and other preprocessing techniques to the time series for noise reduction.","7e58cd58":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","b740c661":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","9622ef12":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","e20c29c0":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 0.3619**<\/font>\n\n<font size=\"6\">**RMSE = 0.5126**<\/font>","78fffef9":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","7bf05686":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 1.0973**<\/font>\n\n<font size=\"6\">**RMSE = 1.7608**<\/font>","e71c1b15":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 0.1435**<\/font>\n\n<font size=\"6\">**RMSE = 0.2059**<\/font>","a2bb89ce":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","8018642b":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","3151791f":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","6a52819a":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 0.0805**<\/font>\n\n<font size=\"6\">**RMSE = 0.1026**<\/font>","168d7ada":"### Depth to Groundwater CoS\n\nIn this series, a regular behaviour can be seen from 2015 so this is the starting point for the model's trainning data.\n\nAll three series, train + validation + test are shown below:","54e0ff6c":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","7389faf0":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","99c1a019":"### Depth to Groundwater Pozzo 6\n\nSame as `Depth_to_Groundwater_Pozzo_5` the data used to feed the model was taken from mid 2017, because this is the most recent regular period in the series and some other past behaviours are unlikely to happen.\n\nAll three series, train + validation + test are shown below:","deb22bdc":"### Flow Rate Lupa\n\nIn this series there is a clear linear trend in almost all the data. It's very easy to forecast based on this trend, but due to the behaviour suddenly changes in the last part of the series and because the linear trend is trivial to predict, the forecast will be based only on the last behaviour.\n\nThis series is a special case because the beehavior seems to be not so complex, so by training the models one can expect that the net only will focus on the quadratic term, leaving the $A_s$'s and $A_c$'s coefficients equal to zero, but in practice, it's very difficult that the net sets the amplitude or frequency exactly equal to zero, which causes the periodic funtions to add noise in predictions. \nSo in this case I tell the algorithm funtion to not include the periodic functions in the net by setting the `max_sin` and `max_cos` arguments equal to zero, and in order to the learning converges faster, I also normalize the time variable and set the initial quadratic coefficient to $1$, by changing the line `return 0.00000001*x*x` for `return x*x` in the `cuadratic()` function. With this changes there is only need one generation to get good results, like those given by the pre-builded model. To get similar results it's necessary to make the described changes.\n\nAll three series, train + validation + test, are shown below:","2678cac2":"### Depth to Groundwater Pozzo 9\n\nThere is a very strange valley in this series and isn't enogh data to know its cause and accurately predict if there will be another event of this kind, so I choose to predict the more regular and recent behaviour that starts near 2018.\n\nAll three series, train + validation + test are shown below:","2046fa33":"### Depth to Groundwater LT2\n\nFor this series I decided to truncate it to the domain 2015-04-16 to 2020-07-01, because this is a more regular period with a clear pattern and contains the most recent data with an aceptable number of points.\n\nAs validation data I chose a the six month period, from 2019-07-01 to 2020-01-01, and for test data, the last six month period, from 2020-01-01 to 2020-07-01.\n\nAll three series, train + validation + test are shown below:","a7a25de9":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 0.2887**<\/font>\n\n<font size=\"6\">**RMSE = 0.3909**<\/font>","717bd099":"# Rivers\n\n## River Arno\n\n### Data cleaning and preprocessing\n\nThe column to predict in this waterbodie is `Hydrometry_Nave_di_Rosano`.\n\nThe changes made to this column are:\n\n* `Hydrometry_Nave_di_Rosano`: The hole of missing values near 2008 was filled with a similar period of 2005, then then zero values were converted to `NaN` and all `NaN` values were interpolated.\n\nAfter clean this column, it was subsampled to week frequency by mean. \n\nThe following plot corresponds to the column to analize before and after the data cleaning and preprocessing.","137aee39":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","936874b3":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","6db0f1ae":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","a27d4c36":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 0.0547**<\/font>\n\n<font size=\"6\">**RMSE = 0.0691**<\/font>","ee43cef9":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 0.2881**<\/font>\n\n<font size=\"6\">**RMSE = 0.3756**<\/font>","d463eeca":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","c501c9ff":"### Depth to Groundwater P24\n\nThere is some kind of pattern from its maximum (near 2014) to the end, so the chosen data to predict this recent behaviour it's from this maximum to the end.\n\nAll three series, train + validation + test are shown below:","ac5fc95c":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 0.0731**<\/font>\n\n<font size=\"6\">**RMSE = 0.0919**<\/font>","fe3129f4":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","1c34178b":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","7f28a801":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","8ff5e5b0":"# Model description\n\nThe methods I implement consists in a neural network, which is inspired in Fourier Analisys, and a genetic algorithm that searches for suitable hyperparameters for the net. It has the advantage of only needs the a time variable to predict new values and in spite of being a neural network it isn't a black box at all, because it only has one hidden layer with simple activations functions, therefore a mathematical model can be easily extracted from the net as will be explained below.\n\n## Neural network\n\nWith Fourier Analisys you can decompose a signal or time series $f$ into a sum of sine a cosine functions with different amplitudes and frequencies, as can be seen in equation below:\n\n$$ f(t_k) = c + \\sum_{i=1}^{n-1} a_i\\sin \\left( \\frac{2\\pi it_k}{T} \\right) + b_i\\cos \\left( \\frac{2\\pi it_k}{T} \\right) $$\n\nwhere $n$ is the number of points in $f$, $t_k$ is the discrete time variable, related to point $k$ in the series, $T = t_{\\text{max}} - t_0$ and $c$ is a bias correction constant that also can be seen as the $i=0$ term in the sum.\n\nThe coefficients $a_i$ and $b_i$ can be calculated in order to the sum of sine and cosine functions does match with every point in the original series.\n\nHowever, in spite of the perfect accuracy of the above function for known points it isn't a good model for forecasting the future, because for times grater than $t_{\\text{max}}$ it only repeats the same values from $t_0$ to $t_{\\text{max}}$ periodically. This is roughly because the sum is composed of periodic functions that doesn't admit frequencies greater than the period or domain of the original time series\n\nNevertheless, there is a clear periodicity in many of the time series to predict. So what I did was relax the rigidity of the original Fourier decomposition and let a neural network to learn the natural frequencies of the time series, including those related to arbritary long time periods, which is very useful to predict the future.\n\nFor example, a time series to train the network could encompass a time period of two months, but this isn't an obstacle for the network to learn patterns and trends that could last 4 months or one year, to say something, in addition to the patterns that are alredy present in the series.\n\nSo the network used is showed in the next picture.\n\n![Net2.png](attachment:Net2.png)\n\nwhere $t$ is the time-related value of a point in the series, so $t$ values are only a linear succesion of numbers (like [0,1,2,3 ...] or [0,2,4,6 ...], $f(t)$ is the net output, the value that is desired to match with the corresponding of the column to predict and:\n\n\\begin{align}\n    &a_i = {w_s}_it + {c_s}_i \\\\ \n    &b_i = {w_c}_it + {c_c}_i \\\\\n    &t' = (w_0t + c_0)^2\n\\end{align}\n\nAfter training the network to approximate $f(t)$ to the series to predict we get the learned coefficients $w$'s, $c$'s and $A$'s, and we can meke predictions for $t$ greater than its maximum value given in the training.\nThen, mathematicaly $f(t)$ is expressed as:\n\n$$f(t) = c + A_0{t'}^2 + \\sum_{i=1}^{n_s} {A_s}_i\\sin a_i + \\sum_{i=1}^{n_c} {A_c}_i\\cos b_i$$\n\nA quadratic term ${t'}^2$ is included in the net because a linear or a very smooth quadratic long-term trend is observed in most of the series. It could be different to quadratic, but due to the forecasts are not so far of this trend I think this approximation is enough.\n\n## Genetic algorithm\n\nOnce the net architecture is ready, we just need to set an appropiate number of sine nodes $n_s$ and cosine nodes $n_c$, and other hyperparameters like the batch size we are going to use in training, etc.\n\nFor this, I implement a genetic algorithm that searches for a suitable number of sine nodes, cosine nodes and batch size to train the network, based on the validation accuracy (MAE) that a net has for a given set of this parameters.\n\nThe main steps of the genetic algorithm are listed below:\n\n* Create a population of 50 models (neural networks) with a random number for each hyperparameter (number of sine nodes, number of cosine nodes, batch size) between a min and max value for each one; and train all models with the current series train data base.\n\n\n* Choose 25 pairs (parents) of models randomly, a model can be in more than one pair. For each pair generate a new model (descendant) based on the hyperparameters of the parents. There are two methods for this: one consists on set each hyperparameter of the descendant equal to the one of any of its parents with a coin toss; the other method consists on set each hyperparameter equal to the sum of the parents values divided by 2. The selection of the method is made with a coin toss. A component of each descendant could be randomly changed (mutated) with certain low probability.\n\n\n* Once we get the 25 descendants we train it and we evaluate the 75 models accuracy with the current series validation data base. The 50 best evaluated models survive and conform a new generation, the rest are ignored.\n\n\n* Repeat the three previous points for as many generations you want (10 in this case) and return the best model od the last generation.\n\nSince the models evaluation is based on the MAE of the validation data base, they could be biased or overfitted to perform well on this data base, but not in future data. For that reason a second partition of the series to predict is made: the test data base. This base is the final portion of the series to predict and I give more weight to the MAE of this base to choose a good model, so the best model is chosen based on the following metric:\n\n$$ E = 0.25(\\text{training MAE}) + 0.25(\\text{validation MAE}) + 0.5(\\text{test MAE}) $$\n\nThe model of the final generation with the lowest $E$ value is chosen as the best and returned by the algorithm. However I highly recommend to explore the other last generation models, specially the models with index `scores[i][1]`, for the first values of `i`, because `scores` has the best scores $E$ of the last generation sorted, in the form `[score, model index]`. This can be done just changing the last line of the algorithm function `return popul[score[0][1]][1]` to `return popul` for returning all 50 last generation models and then explore the `i`'st model: `popul[i][1]`.\n\nFinally the MAE and RMSE errors for each series are calculated in a normal way with the validation + test databases, since any of these sub-series are seen by the models during their training.\n\n<font color='yellow'>**Warning**<\/font>: As you can note, the algorithm function is commented and instead, a pre-builded model is loaded in the next cell this is because one run of this function takes about two hours in a 4-core i7 7th generation processor, 8gb RAM lap-top with no GPU card.","da91f8b4":"# Aquifers\n\n## Aquifer Auser\n\n### Data cleaning\n\nFirst of all we need to clean and preprocess the columns we are going to work with, in order to our algorithm performs the most efficiently possible.\nThese columns are `Depth_to_Groundwater_LT2`, `Depth_to_Groundwater_SAL`, `Depth_to_Groundwater_PAG`, `Depth_to_Groundwater_CoS` and `Depth_to_Groundwater_DIEC`.\n\nThe changes made to each of this columns are explained below.\n\n* `Depth_to_Groundwater_LT2`: Here we remove the two outliers at the end of the series, because they are introducing noise to the regular behaviour seen in the plot. So this two points are converted to `NaN` and then interpolated with the points immediately after and before of them.\nThe rest missing values are also interpolated.\n\n\n* `Depth_to_Groundwater_SAL`: Same case as `Depth_to_Groundwater_LT2`, the outliers are converted to `NaN` and then interpolated, the rest of missing values are also interpolated.\n\n\n* `Depth_to_Groundwater_PAG`: Here, the hole of missing values in 2014 is filled ...\n\n\n* `Depth_to_Groundwater_CoS`: Same case as `Depth_to_Groundwater_LT2` and `Depth_to_Groundwater_SAL`.\n\n\n* `Depth_to_Groundwater_DIEC`: A point is included in this series in order to get a more consistent pattern in the plot when the missing values are interpolated later.\n\nAs I metioned before, all this columns were subsampled to week frequency by mean. \n\nThis plots corresponds to the columns to analize before and after the data cleaning.","0aadaac3":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","75cfd700":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","e337476e":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 0.0472**<\/font>\n\n<font size=\"6\">**RMSE = 0.0643**<\/font>","6029f16c":"### Depth to Groundwater Podere Casetta\n\nAll points of this series were used to train the model and to validate its results.\n\nAll three series, train + validation + test are shown below:","19a7f6e1":"### Flow Rate Ermicciolo\n\nSimilar to `Flow_Rate_Bugnano` and `Flow_Rate_Arbure`, the predicted data for this series will be based on the same domain of the first.\n\nAll three series, train + validation + test, are shown below:","ac3112fe":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 0.0505**<\/font>\n\n<font size=\"6\">**RMSE = 0.0644**<\/font>","455d4276":"### Depth to Groundwater Pozzo 4\n\nSame as `Depth_to_Groundwater_Pozzo_3` and `Depth_to_Groundwater_Pozzo_1`, I think the useful data to predict this series begins about 2017.\n\nAll three series, train + validation + test are shown below:","be36ab07":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","4f40dd79":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","6782fc62":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","d4deadba":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","24b06c3a":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","3ddbe474":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","82248fee":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","42e53604":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","c809b15f":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 0.1906**<\/font>\n\n<font size=\"6\">**RMSE = 0.2692**<\/font>","f43db884":"### Flow Rate\n\nAll the data of this series was used to train the model and validate its results.\n\nAll three series, train + validation + test are shown below:","3d0b0a15":"After running the genetic algorithm and getting the best model, the forecasting results are the following:","40b006db":"## Aquifer Petrignano\n\n### Data cleaning and preprocessing\n\nThe columns to predict are `Depth_to_Groundwater_P24` to `Depth_to_Groundwater_P25`.\nThe series that were cleaned for this aquifer are showed below:\n\nThe changes made to each of this columns are:\n\n* `Depth_to_Groundwater_P24`: The `NaN` values were interpolated.\n\n* `Depth_to_Groundwater_P25`: The `NaN` values were interpolated.\n\nAfter clean all this columns, they all were subsampled to week frequency by mean. \n\nThe following plots correspond to the columns to analize before and after the data cleaning and preprocessing.","0a0185af":"### Depth to Groundwater Pozzo 1\n\nThere are two marked behaviours before and after 2017 in this series, so I chose the most recent to feed the model.\n\nAll three series, train + validation + test are shown below:","c65434c0":"Before starting with the process for each waterbody I like to point out some considerations about the data cleaning of all of them.\n\n***Note about data cleaning:*** Data cleaning of all series to predict consist mainly on removing or displacing outliers and interpolating `NaN` values. In some cases there is a group of points that exhibit an uncommon behaviour, like suddenly increase or decrease a relatively large amount of units, giving place to an anomalous peak or valley. Even if this behaviour is real, I decided to remove or replace this kind of points in the cases that this behaviours only happens few times (1 or 2), because there isn't enough information for the model to learn the cause or pattern that is behind this conduct. So it's likely that this points, instead adding useful information to the model, they could be confusing it.\n\nSo I decided to keep the main trend of the series by sacrificing a bit of real data, in order to get a better learning of this main trend and predict it more accurately.\n\nFor the same reason I also fill some large holes of `NaN` values with another period of the series that seems congruent with its main trend and thus get more training or validation data.\n\n#  How to run the code?\n\n* First run every cell above this section in order.\n\n* Then for each waterbodie subsection (like Aquifer Auser), you must first run the code inside *Data cleaning* sub-subsection in order.\n\n* Then you can run the code inside any column sub-subsection (like *Depth to Groundwater LT2*) in order, if you go to another waterbodie subsection, return to point 2.","90917909":"The error of the model forecast for validation + test data is:\n\n<font size=\"6\">**MAE = 0.1941**<\/font>\n\n<font size=\"6\">**RMSE = 0.2432**<\/font>","380c4237":"With this model, you can make predictions only based on time (very far future predictions could be less accurate).","1e4d73ab":"### Hydrometry Nave di Rosano\n\nAll the data of this waterbodie was used to feed the proposed model.\n\nAll three series, train + validation + test are shown below:","c5238ca4":"### Depth to Groundwater Pozzo 5\n\nThe last regular behaviour in this series begins mid 2017, so the data for the model is taken from this point.\n\nAll three series, train + validation + test are shown below:"}}