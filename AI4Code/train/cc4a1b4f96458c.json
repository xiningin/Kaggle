{"cell_type":{"0cd462e1":"code","13fc1102":"code","95f111cd":"code","fb8e47da":"code","326abea0":"code","b0a9e869":"code","4f40d8f0":"code","c2ac2c9f":"code","d5f822f7":"code","2aabae91":"code","587ca295":"code","119a1f49":"code","58098183":"code","a12be21a":"code","dd039fb0":"code","e7e80963":"code","b36e314a":"code","74209e91":"code","65be9086":"code","e47bea6e":"code","e9b76e34":"code","b0d66384":"code","0a506115":"code","a32f1f15":"code","a10c45ac":"code","f8dc46d1":"code","2bb54e3d":"code","fd7059ec":"code","427d0b2a":"code","6122c4fe":"code","6dccbcea":"code","f0c06d05":"code","a8f890ef":"code","d3e0b494":"code","bf37ea1d":"code","bc80a6f1":"code","bc52e43f":"code","32166d86":"code","d9a2e054":"code","cf9e7ae9":"code","2e1300bf":"code","92fefdc0":"code","e25ab3be":"code","03fd7dd6":"code","94d749b6":"code","beb59e46":"code","e52143ed":"code","71bd5c72":"code","7b25a686":"code","e8feab8a":"code","069ef41e":"code","2c1e3d6e":"code","d709ec6a":"code","a88da992":"code","e158aeba":"code","c4f43ac7":"code","554f6c6f":"code","d105f850":"code","768acd9e":"code","c99f0e2c":"code","6deee657":"code","674aff28":"code","00743ac0":"code","950c16ed":"code","b14be736":"code","6270a5e1":"code","0d07b46a":"code","a4e52d40":"markdown","5470602c":"markdown","967abd98":"markdown","cfc261aa":"markdown","7a75ad30":"markdown","4a62ddba":"markdown","82a0c5cf":"markdown","7ef5406d":"markdown","009e91bf":"markdown","a8f96167":"markdown","bde34e0b":"markdown","6cfec7d8":"markdown","7db8203d":"markdown","bd9f9ac7":"markdown","6fbb9a78":"markdown","71a40b8e":"markdown","b98cb539":"markdown","73240929":"markdown","b3e81183":"markdown","85f24a7c":"markdown","a917c3ce":"markdown","d234cc8f":"markdown","491762aa":"markdown","e3d0be16":"markdown","cf2955a0":"markdown","ab423f5f":"markdown","5d812387":"markdown","b499f59d":"markdown","c95bbd78":"markdown","0a0f0b34":"markdown","8291d67d":"markdown","d4c1c025":"markdown","5875d71c":"markdown","d89a4fe6":"markdown","91071aec":"markdown","3d007099":"markdown","33e95d33":"markdown","faec9618":"markdown","f46e7aa7":"markdown","b8b39c5b":"markdown","c3840d69":"markdown","91ef7230":"markdown","8c3cd4c5":"markdown"},"source":{"0cd462e1":"import numpy as np\nimport pandas as pd\nimport gc\nimport time\nimport datetime\nfrom contextlib import contextmanager\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import metrics, preprocessing\n\nfrom sklearn.model_selection import KFold # StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.decomposition import PCA # KernelPCA\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\n\nfrom sklearn.preprocessing import LabelEncoder\n\n\npd.set_option('display.max_rows', 999)\npd.set_option('display.max_columns',700)\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \nwarnings.filterwarnings(\"ignore\", category=FutureWarning) \nwarnings.filterwarnings(\"ignore\", category=UserWarning) \n\n#plt.style.use('ggplot')\n#color_pal = [x['color'] for x in plt.rcParams['axes.prop_cycle']]\n# pd.show_versions (as_json = False)\n\n","13fc1102":"def getCatFeatureDetail(df,cat_cols):\n    cat_detail_dict = {} \n    for col in cat_cols:\n        cat_detail_dict[col] = df[col].nunique()\n    cat_detail_df = pd.DataFrame.from_dict(cat_detail_dict, orient='index', columns=['nunique'])\n    print('There are ' + str(len(cat_cols)) + ' categorical columns.')\n    print(cat_detail_df)\n    \n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n \n    \n\n                        \ndef ploting_cnt_amt(df, col, lim=2000):\n    tmp = pd.crosstab(df[col], df['isFraud'], normalize='index') * 100\n    tmp = tmp.reset_index()\n    tmp.rename(columns={0:'NoFraud', 1:'Fraud'}, inplace=True)\n    total = len(df)\n    \n    plt.figure(figsize=(16,14))    \n    plt.suptitle(f'{col} Distributions ', fontsize=24)\n    \n    plt.subplot(211)\n    g = sns.countplot( x=col,  data=df, order=list(tmp[col].values))\n    gt = g.twinx()\n    gt = sns.pointplot(x=col, y='Fraud', data=tmp, order=list(tmp[col].values),\n                       color='black', legend=False, )\n    gt.set_ylim(0,tmp['Fraud'].max()*1.1)\n    gt.set_ylabel(\"%Fraud Transactions\", fontsize=16)\n    g.set_title(f\"Most Frequent {col} values and % Fraud Transactions\", fontsize=20)\n    g.set_xlabel(f\"{col} Category Names\", fontsize=16)\n    g.set_ylabel(\"Count\", fontsize=17)\n    g.set_xticklabels(g.get_xticklabels(),rotation=45)\n    sizes = []\n    for p in g.patches:\n        height = p.get_height()\n        sizes.append(height)\n        g.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}%'.format(height\/total*100),\n                ha=\"center\",fontsize=12) \n        \n    g.set_ylim(0,max(sizes)*1.15)\n    plt.show()\n","95f111cd":"# Load data\n\n\n\ntrain_transaction = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv\")\ntrain_identity = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv\")\n\ntest_transaction = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv\")\ntest_identity = pd.read_csv(\"\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv\")\n\n# Fix column name \nfix_col_name = {testIdCol:trainIdCol for testIdCol, trainIdCol in zip(test_identity.columns, train_identity.columns)}\ntest_identity.rename(columns=fix_col_name, inplace=True)\n    \n# Reduce memory\ntrain_transaction = reduce_mem_usage(train_transaction)\ntrain_identity = reduce_mem_usage(train_identity)\n\ntest_transaction = reduce_mem_usage(test_transaction)\ntest_identity = reduce_mem_usage(test_identity)\n    \n# Merge (transaction - identity)\ntrain = train_transaction.merge(train_identity, on='TransactionID', how='left')\ntest = test_transaction.merge(test_identity, on='TransactionID', how='left')\n\n# Merge (X_train - X_test)\ntrain_test = pd.concat([train, test], ignore_index=True)\n\nprint(f'train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\nprint(f'test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')\n\ndel train_transaction, train_identity, test_transaction, test_identity; x = gc.collect()","fb8e47da":"train_test = train_test.copy()\ntrain = train.copy()\ntest = test.copy()","326abea0":"train.head()","b0a9e869":"train.isnull().sum().sort_values(ascending =False)","4f40d8f0":"train.dtypes","c2ac2c9f":"train_fraud = train.loc[train['isFraud'] == 1]\ntrain_non_fraud = train.loc[train['isFraud'] == 0]","d5f822f7":"train['isFraud'].value_counts(normalize=True)","2aabae91":"print('  {:.2f}% of Transactions that are fraud in train_transaction '.format(train['isFraud'].mean() * 100))","587ca295":"sns.countplot(x=\"isFraud\", data=train).set_title('Distribution of Target')\nplt.show()","119a1f49":"plt.figure(figsize=(15,5))\nsns.distplot(train[\"TransactionDT\"])\nsns.distplot(test[\"TransactionDT\"])\nplt.title('train vs test TransactionDT distribution')\nplt.show()","58098183":"plt.figure(figsize=(15,5))\nsns.distplot(train_fraud[\"TransactionDT\"], color='b', label='Fraud')\nsns.distplot(train_non_fraud[\"TransactionDT\"], color='r', label ='non-Fraud')\nplt.title('Fraud vs non-Fraud TransactionDT distribution')\nplt.legend()","a12be21a":"START_DATE = '2015-04-22'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n\ntrain_test['New_Date'] = train_test['TransactionDT'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\ntrain_test['New_Date_YMD'] = train_test['New_Date'].dt.year.astype(str) + '-' + train_test['New_Date'].dt.month.astype(str) + '-' + train_test['New_Date'].dt.day.astype(str)\ntrain_test['New_Date_YearMonth'] = train_test['New_Date'].dt.year.astype(str) + '-' + train_test['New_Date'].dt.month.astype(str)\ntrain_test['New_Date_Weekday'] = train_test['New_Date'].dt.dayofweek\ntrain_test['New_Date_Hour'] = train_test['New_Date'].dt.hour\ntrain_test['New_Date_Day'] = train_test['New_Date'].dt.day\n\n\nfig,ax = plt.subplots(4, 1, figsize=(16,15))\n\ntrain_test.groupby('New_Date_Weekday')['isFraud'].mean().to_frame().plot.bar(ax=ax[0])\ntrain_test.groupby('New_Date_Hour')['isFraud'].mean().to_frame().plot.bar(ax=ax[1])\ntrain_test.groupby('New_Date_Day')['isFraud'].mean().to_frame().plot.bar(ax=ax[2])\ntrain_test.groupby('New_Date_YearMonth')['isFraud'].mean().to_frame().plot.bar(ax=ax[3])","dd039fb0":"print(pd.concat([train['TransactionAmt'].quantile([.01, .1, .25, .5, .75, .9, .99]).reset_index(),\n                 train_fraud['TransactionAmt'].quantile([.01, .1, .25, .5, .75, .9, .99]).reset_index(), \n                 train_non_fraud['TransactionAmt'].quantile([.01, .1, .25, .5, .75, .9, .99]).reset_index()],\n                   axis=1, keys=['Total','Fraud', \"No Fraud\"]))","e7e80963":"print(' Fraud TransactionAmt mean      :  '+str(train_fraud['TransactionAmt'].mean()))\nprint(' Non - Fraud TransactionAmt mean:  '+str(train_non_fraud['TransactionAmt'].mean()))","b36e314a":"plt.figure(figsize=(15,5))\nsns.distplot(train_test[\"TransactionAmt\"].apply(np.log))\nplt.title('Train - Test TransactionAmt distribution')\nplt.show()","74209e91":"plt.figure(figsize=(15,5))\nsns.distplot(train_fraud[\"TransactionAmt\"].apply(np.log), label = 'Fraud | isFraud = 1')\nsns.distplot(train_non_fraud[\"TransactionAmt\"].apply(np.log), label = 'non-Fraud | isFraud = 0')\nplt.title('Fraud vs non-Fraud TransactionAmt distribution')\nplt.legend()\nplt.show()","65be9086":"train['New_TransactionAmt_Bin'] = pd.qcut(train['TransactionAmt'],15)\ntrain.groupby('New_TransactionAmt_Bin')[['isFraud']].mean()","e47bea6e":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n\ntrain['dist1'].plot(kind='hist',bins=5000,ax=ax1,title='dist1 distribution',logx=True)\ntrain['dist2'].plot(kind='hist',bins=5000,ax=ax2,title='dist2 distribution',logx=True)\n\nplt.show()","e9b76e34":"cat_features = ['isFraud','ProductCD','addr1', 'addr2', 'P_emaildomain','R_emaildomain','DeviceType','DeviceInfo']\nall_cat_features = cat_features+ [f'card{i}' for i in range(1,7)]+ [f'M{i}' for i in range(1,10)] + [f'id_{i}' for i in range(12,39)]\n\ngetCatFeatureDetail(train_test, cat_features)","b0d66384":"ploting_cnt_amt(train, 'ProductCD')","0a506115":"train['addr1'].value_counts()","a32f1f15":"train['addr2'].value_counts()","a10c45ac":"train.loc[train['addr1'].isin(train['addr1'].value_counts()[train['addr1'].value_counts() <= 5000 ].index), 'addr1'] = \"Others\"\ntrain.loc[train['addr2'].isin(train['addr2'].value_counts()[train['addr2'].value_counts() <= 50 ].index), 'addr2'] = \"Others\"\n\ntest.loc[test['addr1'].isin(test.addr1.value_counts()[test['addr1'].value_counts() <= 5000 ].index), 'addr1'] = \"Others\"\ntest.loc[test['addr2'].isin(test.addr2.value_counts()[test['addr2'].value_counts() <= 50 ].index), 'addr2'] = \"Others\"\n\ntrain['addr1'].fillna(\"NoInf\", inplace=True)\ntest['addr1'].fillna(\"NoInf\", inplace=True)\n\ntrain['addr2'].fillna(\"NoInf\", inplace=True)\ntest['addr2'].fillna(\"NoInf\", inplace=True)","f8dc46d1":"ploting_cnt_amt(train, 'addr1')","2bb54e3d":"ploting_cnt_amt(train, 'addr2')","fd7059ec":"train['P_emaildomain'].value_counts()","427d0b2a":"train.loc[train['P_emaildomain'].isin(['gmail.com', 'gmail']),'P_emaildomain'] = 'Google'\n\ntrain.loc[train['P_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk',\n                                         'yahoo.co.jp', 'yahoo.de', 'yahoo.fr',\n                                         'yahoo.es']), 'P_emaildomain'] = 'Yahoo Mail'\ntrain.loc[train['P_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n                                         'hotmail.es','hotmail.co.uk', 'hotmail.de',\n                                         'outlook.es', 'live.com', 'live.fr',\n                                         'hotmail.fr']), 'P_emaildomain'] = 'Microsoft'\ntrain.loc[train['P_emaildomain'].isin(train['P_emaildomain']\\\n                                         .value_counts()[train.P_emaildomain.value_counts() <= 500 ]\\\n                                         .index), 'P_emaildomain'] = \"Others\"\ntrain['P_emaildomain'].fillna(\"NoInf\", inplace=True)\n","6122c4fe":"ploting_cnt_amt(train, 'P_emaildomain')","6dccbcea":"train['R_emaildomain'].value_counts()","f0c06d05":"train.loc[train['R_emaildomain'].isin(['gmail.com', 'gmail']),'R_emaildomain'] = 'Google'\n\ntrain.loc[train['R_emaildomain'].isin(['yahoo.com', 'yahoo.com.mx',  'yahoo.co.uk',\n                                             'yahoo.co.jp', 'yahoo.de', 'yahoo.fr',\n                                             'yahoo.es']), 'R_emaildomain'] = 'Yahoo Mail'\ntrain.loc[train['R_emaildomain'].isin(['hotmail.com','outlook.com','msn.com', 'live.com.mx', \n                                             'hotmail.es','hotmail.co.uk', 'hotmail.de',\n                                             'outlook.es', 'live.com', 'live.fr',\n                                             'hotmail.fr']), 'R_emaildomain'] = 'Microsoft'\ntrain.loc[train['R_emaildomain'].isin(train.R_emaildomain\\\n                                         .value_counts()[train['R_emaildomain'].value_counts() <= 300 ]\\\n                                         .index), 'R_emaildomain'] = \"Others\"\ntrain['R_emaildomain'].fillna(\"NoInf\", inplace=True)\n\n","a8f890ef":"ploting_cnt_amt(train, 'R_emaildomain')","d3e0b494":"train['DeviceType'].value_counts()","bf37ea1d":"ploting_cnt_amt(train, 'DeviceType')","bc80a6f1":"train['DeviceInfo'].value_counts()","bc52e43f":"train['DeviceInfo'].value_counts()[train['DeviceInfo'].value_counts() > 1000 ], 'DeviceInfo'] = \"Others\"","32166d86":"train['DeviceInfo'].value_counts().head(20).plot(kind='barh', figsize=(15, 5), title='Top 20 Devices in Train')\nplt.show()","d9a2e054":"train_test['DeviceInfo'] = train_test['DeviceInfo'].fillna('unknown_device').str.lower()\ntrain_test['DeviceName'] = train_test['DeviceInfo'].str.split('\/', expand=True)[0]\n\ntrain_test.loc[train_test['DeviceName'].str.contains('SM', na=False), 'DeviceName'] = 'Samsung'\ntrain_test.loc[train_test['DeviceName'].str.contains('SAMSUNG', na=False), 'DeviceName'] = 'Samsung'\ntrain_test.loc[train_test['DeviceName'].str.contains('GT-', na=False), 'DeviceName'] = 'Samsung'\ntrain_test.loc[train_test['DeviceName'].str.contains('Moto G', na=False), 'DeviceName'] = 'Motorola'\ntrain_test.loc[train_test['DeviceName'].str.contains('Moto', na=False), 'DeviceName'] = 'Motorola'\ntrain_test.loc[train_test['DeviceName'].str.contains('moto', na=False), 'DeviceName'] = 'Motorola'\ntrain_test.loc[train_test['DeviceName'].str.contains('LG-', na=False), 'DeviceName'] = 'LG'\ntrain_test.loc[train_test['DeviceName'].str.contains('rv:', na=False), 'DeviceName'] = 'RV'\ntrain_test.loc[train_test['DeviceName'].str.contains('HUAWEI', na=False), 'DeviceName'] = 'Huawei'\ntrain_test.loc[train_test['DeviceName'].str.contains('ALE-', na=False), 'DeviceName'] = 'Huawei'\ntrain_test.loc[train_test['DeviceName'].str.contains('-L', na=False), 'DeviceName'] = 'Huawei'\ntrain_test.loc[train_test['DeviceName'].str.contains('Blade', na=False), 'DeviceName'] = 'ZTE'\ntrain_test.loc[train_test['DeviceName'].str.contains('BLADE', na=False), 'DeviceName'] = 'ZTE'\ntrain_test.loc[train_test['DeviceName'].str.contains('Linux', na=False), 'DeviceName'] = 'Linux'\ntrain_test.loc[train_test['DeviceName'].str.contains('XT', na=False), 'DeviceName'] = 'Sony'\ntrain_test.loc[train_test['DeviceName'].str.contains('HTC', na=False), 'DeviceName'] = 'HTC'\ntrain_test.loc[train_test['DeviceName'].str.contains('ASUS', na=False), 'DeviceName'] = 'Asus'\n\ntrain_test.loc[train_test['DeviceName'].isin(train_test['DeviceName'].value_counts()[train_test['DeviceName'].value_counts() < 1000].index), 'DeviceName'] = \"Others\"\n ","cf9e7ae9":"ploting_cnt_amt(train_test, 'DeviceName')","2e1300bf":"card_cols = [c for c in train.columns if 'card' in c]\ntrain[card_cols].head()","92fefdc0":"train_test[card_cols].isnull().sum()","e25ab3be":"for col in card_cols:\n    print(col+'  :' + str(train[col].nunique()))","03fd7dd6":"#f = lambda x: np.nan if x.isnull().all() else x.value_counts(dropna=False).index[0]\nfor col in ['card2','card3','card4','card5','card6']:\n    train_test[col] = train_test.groupby(['card1'])[col].transform(lambda x: x.mode(dropna=False).iat[0])\n    train_test[col].fillna(train_test[col].mode()[0], inplace=True)\n    print(col+' has : '+str(train_test[col].isnull().sum())+' missing values')\n","94d749b6":"ploting_cnt_amt(train, 'card4')","beb59e46":"ploting_cnt_amt(train, 'card6')","e52143ed":"c_cols = [c for c in train if c[0] == 'C']\ntrain[c_cols].head()","71bd5c72":"train[c_cols].describe()","7b25a686":"train[c_cols].quantile([.01, .1, .25, .5, .75, .9, .99])","e8feab8a":"#train[train['C6']>118.000]['isFraud'].mean()","069ef41e":"for col in c_cols:\n    print('\\n Fraud '+col+' mean    :  '+str(train_fraud[train_fraud[col]<=37.00][col].mean()))\n    print(' Non - Fraud '+col+' mean:  '+str(train_non_fraud[train_non_fraud[col]<=37.00][col].mean()))","2c1e3d6e":"d_cols = ['D1','D2','D3','D4','D5','D6','D7','D8','D9','D10','D11','D12','D13','D14']\ntrain[d_cols].head()","d709ec6a":"train[d_cols].describe()","a88da992":"for col in d_cols:\n    plt.figure(figsize=(15,5))\n    plt.scatter(train['TransactionDT'] ,train[col])\n    plt.title(col + ' Vs TransactionDT')\n    plt.xlabel('Time')\n    plt.ylabel(col)\n    plt.show()","e158aeba":"msno.matrix(train[d_cols])","c4f43ac7":"m_cols = [c for c in train if c[0] == 'M']\nfor col in m_cols:\n    ploting_cnt_amt(train, col, lim=2500)","554f6c6f":"msno.matrix(train[m_cols]) ","d105f850":"v_cols = [c for c in train if c[0] == 'V']\ntrain[v_cols].head()","768acd9e":"train[v_cols].describe()","c99f0e2c":"v_cols = [c for c in train_test if c[0] == 'V']\nv_nan_df = train_test[v_cols].isna()\nnan_groups={}\n\nfor col in v_cols:\n    cur_group = v_nan_df[col].sum()\n    try:\n        nan_groups[cur_group].append(col)\n    except:\n        nan_groups[cur_group]=[col]\ndel v_nan_df; x=gc.collect()\n\n'''\nfor nan_cnt, v_group in nan_groups.items():\n    train_test['New_v_group_'+str(nan_cnt)+'_nulls'] = nan_cnt\n    sc = preprocessing.MinMaxScaler()\n    pca = PCA(n_components=2)\n    v_group_pca = pca.fit_transform(sc.fit_transform(train_test[v_group].fillna(-1)))\n    train_test['New_v_group_'+str(nan_cnt)+'_pca0'] = v_group_pca[:,0]\n    train_test['New_v_group_'+str(nan_cnt)+'_pca1'] = v_group_pca[:,1]\n'''","6deee657":"def plot_corr(v_cols):\n    cols = v_cols + ['TransactionDT']\n    plt.figure(figsize=(15,15))\n    sns.heatmap(train[cols].corr(),cmap='RdBu_r', annot=True, center=0.0)\n    plt.title(v_cols[0]+' - '+v_cols[-1],fontsize=14)\n    plt.show()\n    ","674aff28":"for k,v in nan_groups.items():\n    plot_corr(v)","00743ac0":"id_cols = [c for c in train_test if c[:2] == 'id']\n\nid_num_cols=id_cols[:11]\nid_cat_cols=id_cols[11:]","950c16ed":"train[id_num_cols].describe()","b14be736":"for col in id_num_cols:\n    print('\\n'+col)\n    print(' Fraud mean    :  ' + str(train_fraud[col].mean()))\n    print(' Non - Fraud mean:  ' + str(train_non_fraud[col].mean()))","6270a5e1":"getCatFeatureDetail(train,id_cat_cols)","0d07b46a":"for col in  ['id_12', 'id_15', 'id_16', 'id_23', 'id_27', 'id_28', 'id_29']:\n    ploting_cnt_amt(train, col, lim=2500)","a4e52d40":"In this section, I will examine the effect of categorical variables on fraud.\n\nAlthough some of the group features are categorical(like M, card, id_), I will examine them in a diffrent section.\n\n**Categorical Features**\n* ProductCD\n* addr1 & addr2\n* P_emaildomain & R_emaildomain\n* DeviceType\n* DeviceInfo","5470602c":"### D1-D15","967abd98":"### dist1 & dist2","cfc261aa":"### id1-id38","7a75ad30":"# Exploring Continuous Features","4a62ddba":"# Introduction","82a0c5cf":"### M1-M9","7ef5406d":"### card1-card6","009e91bf":"### DeviceInfo","a8f96167":"* Perhaps this could be the distance of the transaction vs. the card owner's home\/work address.","bde34e0b":"* **TransactionDT :** is a timedelta from a given reference datetime (not an actual timestamp).\n* TransactionDT is one of the features that can cause problems. \n* It seems as if there is a time difference between testing and train operations.","6cfec7d8":"# Exploratory Data Analysis (EDA)","7db8203d":"**P-emaildomain** \n* I will group all e-mail domains by the respective enterprises.\n* Also, I will set as \"Others\" all values with less than 500 entries.","bd9f9ac7":"### TransactionDT","6fbb9a78":"### DeviceType","71a40b8e":"* Most of the fraudulent transactions were done by the mobile device.","b98cb539":"### ProductCD","73240929":"* M1-M9 :  match, such as names on card and address, etc.\n* All of the M features are categorical.\n* Values are T F or NaN except M4.\n* M4 feature appears to be different from others.","b3e81183":"* Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n* I will group the v features that have a similar number of nan observations.","85f24a7c":"# Overview And Loading Data","a917c3ce":"* **TransactionAmt :** The ammount of transaction.\n* I apply log transform in order to better show the distribution of data. Otherwise very large transactions skew the distribution.\n* The mean of the fraud transaction amount is larger than the mean of non - fraud transaction amount.\n* And also , the lowest and highest transaction amounts seem to be more likely to be fraudulent transactions.","d234cc8f":"* W, C and R are the most frequent values.\n* 75.45% of observations belong to product W.\n* 1.97% of observations belong to product S.\n* Approximately 12% of transactions with product C are fraudulent.\n* Approximately 2% of transactions with product W are fraudulent.","491762aa":"### addr1 - addr2\n\n* The host of the competition stated that these features are categorical even if they look numerical.","e3d0be16":"**R-emaildomain**\n* I will group all e-mail domains by the respective enterprises.\n* I will set as \"Others\" all values with less than 300 entries.","cf2955a0":"There are two different datasets for both train and test datasets: transaction and identity. The identity datasets hold information about the identity of the customer and the transaction datasets hold information about then transactions.\n\nNote: Not all transactions have corresponding identity information.","ab423f5f":"# Exploring  Categorical Features","5d812387":"# Exploring Group Features (card, C, D, M, V, id )","b499f59d":"# Helper Functions ","c95bbd78":"# Table Of Contents : \n* Libraries\n* Helper Functions\n* Overwiev And Loading Data\n* Exploratory Data Analysis\n    * Exploring Target Feature - isFraud\n    * Exploring Continuous Features \n    * Exploring Categorical Features\n    * Exploring Group Features (card, C, D, M, V, id )","0a0f0b34":"# Exploring  Target Features - isFraud","8291d67d":"# IEEE Fraud Detection - EDA ","d4c1c025":"* id1-id11 are numeric features\n* id12-id38 are categorical features.","5875d71c":"### C1-C14","d89a4fe6":"### TransactionAmt","91071aec":"* The host of the competition stated that some of the features are categorical even if they look numerical like card features.\n* card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n* card4 and card6 have 4 unique values, and the others more than 100 \n* Except card1, card features have nan values \u200b\u200bso I will group them according to card1 and fill with the most common value.","3d007099":"# Libraries","33e95d33":"### P-emaildomain & R-emaildomain","faec9618":"* C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is 0masked.\n* All of the C features are continuous. ","f46e7aa7":"* We can see a very similar distribution in both email domain features.\n* We have high values in google and icloud frauds.","b8b39c5b":"* In the target feature, there is an unbalanced data problem that is frequently encountered in spam and fraud detection problems.\n* 3.5% of the transactions are fraudulent.\n* I divided the train dataset into two as fraud and non-fraud in order to see the effect of fraud transactions on other features. ","c3840d69":"### V1-V339","91ef7230":"* The D Columns are \"time deltas\" from some point in the past. ","8c3cd4c5":"In this project, I'll deal with the problem of IEEE Fraud Detection which is a Kaggle competition. My goal is to estimate whether transactions are fraudulent with as high success as possible using the data of real-world e-commerce transactions provided by the Vesta company.\n\nThis is a binary classification problem. The target variable (isFraud) has an unbalanced distribution as with all fraud and spam detection problem.\n\nFirst, I will be interested in understanding features using visualization and analysis techniques. Next, in a different kernel, I will process the data, create new features, and build a  model.\n\nTo reach the competition and the details: https:\/\/www.kaggle.com\/c\/ieee-fraud-\n\nAnd also ; \nThe following kernels helped me to create this kernel. Thanks for sharing, I learned a lot from your kernels.\n\n   Leonardo Ferreira : https:\/\/www.kaggle.com\/kabure\/extensive-eda-and-modeling-xgb-hyperopt \n   \n   Rob Mulla : https:\/\/www.kaggle.com\/robikscube\/ieee-fraud-detection-first-look-and-eda\n   \n   Chris Deotte : https:\/\/www.kaggle.com\/cdeotte\/xgb-fraud-with-magic-0-9600\n"}}