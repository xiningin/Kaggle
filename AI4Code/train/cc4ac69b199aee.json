{"cell_type":{"7f4f986d":"code","fe96e840":"code","9a38f441":"code","eb20a384":"code","49d4e52c":"code","1a21f3d5":"code","a7da32da":"code","7dc9f31a":"code","fce2c805":"code","9734b2db":"code","2f706185":"code","b0c6a81c":"code","501323b7":"code","3013117a":"code","b40323cd":"code","67d88c4c":"code","87c350a6":"code","2fd75b33":"code","256b0e1d":"code","21eb3ee6":"code","11fa32ae":"code","a942e1cb":"code","d4d8718d":"code","3124dab4":"code","b75d804a":"code","f8a98f20":"code","6fef5f73":"code","6b57232e":"markdown","22c0e4bd":"markdown","6613956a":"markdown","c6e4c6e4":"markdown","7c6bc932":"markdown","213c0fec":"markdown","9925ec3b":"markdown","f733f7d5":"markdown","2797b235":"markdown"},"source":{"7f4f986d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","fe96e840":"data = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndata.head(6)","9a38f441":"data.shape","eb20a384":"data.columns","49d4e52c":"#find percentage of missing values in columns\nperct = data.isnull().sum()\/data.shape[0] *100\nperct[perct!=0]","1a21f3d5":"# Let's drop the columns with over 70% missing values\nprint(perct[perct>70])\n\ndata.drop([\"Alley\", \"PoolQC\", \"Fence\", \"MiscFeature\"],axis = 1, inplace = True )","a7da32da":"plt.figure(figsize = (20,4))\nsns.heatmap(data.isnull(),yticklabels = False, cbar = False)","7dc9f31a":"miss_val_col = data.columns[data.isnull().any()]\nprint(data[miss_val_col].dtypes)\nmiss_cat_col = data[miss_val_col].select_dtypes(include = 'object').columns\n\n# Impute each missing value in categorical feature with most freq value\nfor each_col in miss_cat_col:\n    data[each_col] = data[each_col].fillna(data[each_col].mode()[0])","fce2c805":"cols = data.isnull().sum()\ncols[cols>0]","9734b2db":"data[\"GarageYrBlt\"] = data[\"GarageYrBlt\"].fillna(data[\"GarageYrBlt\"].mean())\ndata[\"MasVnrArea\"] = data[\"MasVnrArea\"].fillna(data[\"MasVnrArea\"].mean())\ndata[\"LotFrontage\"] = data[\"LotFrontage\"].fillna(data[\"LotFrontage\"].mean())","2f706185":"cols = data.isnull().sum()\ncols[cols>0]","b0c6a81c":"catergorical_cols = data.select_dtypes(include = 'object').columns\ncatergorical_cols","501323b7":"# Lets write a function to perform 1 hot encoding on all the categorical features\n\ndef one_hot_encode(cols):\n    data1 = big_dataset\n    i = 0\n    \n    for each_col in cols:\n        print(each_col)\n        df = pd.get_dummies(big_dataset[each_col], drop_first = True)\n        big_dataset.drop([each_col], axis = 1, inplace = True)\n        \n        if i==0:\n            data1 = df.copy()\n        else:\n            data1 = pd.concat([data1, df], axis = 1)\n        i = i + 1\n        \n    data1 = pd.concat([data1, big_dataset], axis = 1)\n    \n    return(data1)","3013117a":"# pd.get_dummies(data[\"MSZoning\"], drop_first = True).head(4)","b40323cd":"## There are several features whose categories are different in test and train dataset.\n## In order to handle this, lets combine test and train ","67d88c4c":"test_df = pd.read_csv(\".\/formulatedtest.csv\")\n\nbig_dataset = pd.concat([data, test_df], axis = 0)\nbig_dataset.head()","87c350a6":"big_dataset.shape","2fd75b33":"# Perform one hot encoding on the categorical columns\nbig_dataset = one_hot_encode(list(catergorical_cols))","256b0e1d":"big_dataset.shape \n# Observe that there are 238 columns in the latest dataset. More columns are created due to dummies","21eb3ee6":"# Lets remove the duplicate columns\nbig_dataset = big_dataset.loc[:,~big_dataset.columns.duplicated()]\nbig_dataset.shape","11fa32ae":"# split train and test data\ntrain_dataset = big_dataset[:1460]\ntest_dataset = big_dataset[1460:]\n\ntest_dataset.drop([\"SalePrice\"], axis = 1, inplace = True)\n\nprint(train_dataset.shape)\nprint(test_dataset.shape)","a942e1cb":"# To train the model, Lets choose X and y\nX_train = train_dataset.drop([\"SalePrice\"], axis = 1)\ny_train = train_dataset.SalePrice","d4d8718d":"import xgboost\n\nmodel = xgboost.XGBRegressor()\nmodel.fit(X_train, y_train)","3124dab4":"# from sklearn.ensemble import RandomForestRegressor\n\n# model = RandomForestRegressor()\n# model.fit(X_train, y_train)","b75d804a":"import pickle\n\nfilename = \"final_model.pkl\"\npickle.dump(model, open(filename,'wb'))","f8a98f20":"# Create sample submission file and submit\n\ny_pred = pd.DataFrame(model.predict(test_dataset))\n\nother_df = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\ndatasets = pd.concat([other_df['Id'], y_pred], axis = 1)\n\ndatasets.columns = ['Id', 'SalePrice']\ndatasets.to_csv(\"sample_submission.csv\", index = False)","6fef5f73":"booster = ['gbtree', 'gblinear']\nbase_score = [0.25, 0.5, 0.75, 1]\n\n\nn_estimators = [100, 500, 900, 1100, 1500]\nmax_depth = [2,3,5,10,15]\nlearning_rate = [0.05, 0.1, 0.15, 0.20]\nmin_child_weight = [1,2,3,4]","6b57232e":"## let's save the model as a pickle file, so that we dont need to repeatedly train it","22c0e4bd":"# House price prediction using advanced Regression Techniques","6613956a":"# Handling categorical features.\n### ML model needs all data to be numerical. Let's convert all catogories to numbers","c6e4c6e4":"# Handling missing values","7c6bc932":"## Model Training","213c0fec":"## Let's try to tune the Hyperparameters and check if that improves the score","9925ec3b":"### Model 2 :","f733f7d5":"### Model 1 :","2797b235":"### Model 3 :"}}