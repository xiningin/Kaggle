{"cell_type":{"732b63e8":"code","786f8dba":"code","8cc94b1c":"code","9e1a6bac":"code","5f62ab23":"code","9e7a9d19":"code","5b4361fb":"code","6c3d62cc":"code","0bf36edd":"code","bae87b73":"code","d6d0999a":"markdown","5950834d":"markdown","f4150ad1":"markdown","1866ffe9":"markdown","4ac85fd7":"markdown","9ba1080b":"markdown","9a3a25a9":"markdown","812c9c22":"markdown","66f52d1b":"markdown","7342200d":"markdown","75944af5":"markdown","97b4c0da":"markdown","29dfdc1b":"markdown","cb76df45":"markdown"},"source":{"732b63e8":"import pandas as pd\nimport numpy as np\n\nhd = pd.read_csv('..\/input\/cleveland-clinic-heart-disease-dataset\/processed_cleveland.csv', na_values = '?')\nhd['cp'].replace({1:'typical_angina', 2:'atypical_angina', 3: 'non-anginal_pain', 4: 'asymptomatic'}, inplace = True)\nhd['restecg'].replace({0:'normal', 1:' ST-T_wave_abnormality', 2:'left_ventricular_hypertrophy'}, inplace = True)\nhd['slope'].replace({1:'upsloping', 2:'flat', 3:'downsloping'}, inplace = True)\nhd['thal'].replace({3:'normal', 6:'fixed_defect', 7:'reversible_defect'}, inplace = True)\nhd['num'].replace({2:1, 3:1, 4:1}, inplace = True)\n\nhd.dropna(how = 'any', inplace = True)\n\nfeatures = hd.columns.to_list()\ncategorical_features = ['cp', 'thal', 'restecg', 'slope']\ncategorical_features = pd.get_dummies(hd[categorical_features].applymap(str))\nfeatures.remove('num')\n\nfeatures.remove('cp')\nfeatures.remove('thal')\nfeatures.remove('restecg')\nfeatures.remove('slope')\n\ny = hd['num']\ny.columns = ['target']\nX = pd.concat([hd[features],categorical_features], axis = 1)\nX.drop([92, 138, 163, 164, 251])\nX.head()\n","786f8dba":"from sklearn.model_selection import learning_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\ntrain_sizes, train_scores_lr, valid_scores_lr = learning_curve( LogisticRegression(), X_scaled, y, train_sizes=[50, 100, 150, 200, 237], cv=5)\ntrain_sizes, train_scores_svc, valid_scores_svc = learning_curve( SVC(kernel = 'linear'), X_scaled, y, train_sizes=[50, 100, 150, 200, 237], cv=5)\ntrain_sizes, train_scores_dt, valid_scores_dt = learning_curve( DecisionTreeClassifier(), X_scaled, y, train_sizes=[50, 100, 150, 200, 237], cv=5)\ntrain_sizes, train_scores_rf, valid_scores_rf = learning_curve( RandomForestClassifier(), scaler.fit_transform(X), y, train_sizes=[50, 100, 150, 200, 237], cv=5)\n\n\nplt.style.use('ggplot')\n\nfig, ((ax, ax1), (ax2, ax3)) = plt.subplots(2,2, figsize=(15, 15))\n\nax.plot(train_sizes, np.mean(train_scores_lr, axis = 1), label = 'training accuracy')\nax.fill_between(train_sizes, np.mean(train_scores_lr, axis = 1) + np.std(train_scores_lr, axis = 1)\/2, np.mean(train_scores_lr, axis = 1) - np.std(train_scores_lr, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax.plot(train_sizes, np.mean(valid_scores_lr, axis = 1), label = 'validation accuracy')\nax.fill_between(train_sizes, np.mean(valid_scores_lr, axis = 1) + np.std(valid_scores_lr, axis = 1)\/2, np.mean(valid_scores_lr, axis = 1) - np.std(valid_scores_lr, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax.set_ylabel('Accuracy')\nax.set_xlabel('Train Sizes')\nax.set_title('Logistic Regression')\nax.legend(loc = 'upper right')\n\n\nax1.plot(train_sizes, np.mean(train_scores_svc, axis = 1), label = 'training accuracy')\nax1.fill_between(train_sizes, np.mean(train_scores_svc, axis = 1) + np.std(train_scores_svc, axis = 1)\/2, np.mean(train_scores_svc, axis = 1) - np.std(train_scores_svc, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax1.plot(train_sizes, np.mean(valid_scores_svc, axis = 1), label = 'validation accuracy')\nax1.fill_between(train_sizes, np.mean(valid_scores_svc, axis = 1) + np.std(valid_scores_svc, axis = 1)\/2, np.mean(valid_scores_svc, axis = 1) - np.std(valid_scores_svc, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\n\n\nax1.set_ylabel('Accuracy')\nax1.set_xlabel('Train Sizes')\nax1.set_title('SVC')\n\nax2.plot(train_sizes, np.mean(train_scores_dt, axis = 1), label = 'training accuracy')\nax2.fill_between(train_sizes, np.mean(train_scores_dt, axis = 1) + np.std(train_scores_dt, axis = 1)\/2, np.mean(train_scores_dt, axis = 1) - np.std(train_scores_dt, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax2.plot(train_sizes, np.mean(valid_scores_dt, axis = 1), label = 'validation accuracy')\nax2.fill_between(train_sizes, np.mean(valid_scores_dt, axis = 1) + np.std(valid_scores_dt, axis = 1)\/2, np.mean(valid_scores_dt, axis = 1) - np.std(valid_scores_dt, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\n\n\nax2.set_ylabel('Accuracy')\nax2.set_xlabel('Train Sizes')\nax2.set_title('Decision Tree')\n\nax3.plot(train_sizes, np.mean(train_scores_rf, axis = 1), label = 'training accuracy')\nax3.fill_between(train_sizes, np.mean(train_scores_rf, axis = 1) + np.std(train_scores_rf, axis = 1)\/2, np.mean(train_scores_rf, axis = 1) - np.std(train_scores_rf, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax3.plot(train_sizes, np.mean(valid_scores_rf, axis = 1), label = 'validation accuracy')\nax3.fill_between(train_sizes, np.mean(valid_scores_rf, axis = 1) + np.std(valid_scores_rf, axis = 1)\/2, np.mean(valid_scores_rf, axis = 1) - np.std(valid_scores_rf, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\n\n\nax3.set_ylabel('Accuracy')\nax3.set_xlabel('Train Sizes')\nax3.set_title('Random Forest')\n\n\nplt.show()","8cc94b1c":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.feature_selection import RFE\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, stratify = y, random_state = 1)\ncorr_df = X_train.corr().abs()\nmask = np.triu((np.ones_like(corr_df, dtype = bool)))\ntri_df = corr_df.mask(mask)\n\n\nto_drop = []\n\nfor index, row in tri_df.iterrows():\n    for col in row.index:\n        if tri_df.loc[index, col]>.9:\n            to_drop.append((index, col))\n\n\nto_drop = [val[0] for val in to_drop]\nX_train = X_train.drop(to_drop, axis = 1)\nX_test = X_test.drop(to_drop, axis = 1)\nreduced_X =  X.drop(to_drop, axis = 1)\n\nscaler = StandardScaler()\nX_train_std = pd.DataFrame(scaler.fit_transform(X_train))\nX_train_std.columns = X_train.columns\n\nX_test_std = pd.DataFrame(scaler.transform(X_test))\nX_test_std.columns = X_test.columns\n\nkf = KFold(n_splits = 5, random_state = 1, shuffle = True)\n\n\nlcv = LassoCV(cv = kf)\nlcv.fit(X_train_std, y_train)\nlcv_mask = lcv.coef_ != 0\n\nrfe_rf = RFE(estimator = RandomForestClassifier(),n_features_to_select = sum(lcv_mask), step = 1)\nrfe_rf.fit(X_train_std, y_train)\nrf_mask = rfe_rf.support_\n\nrfe_gb = RFE(estimator = GradientBoostingRegressor(), n_features_to_select =sum(lcv_mask), step = 1)\nrfe_gb.fit(X_train_std, y_train)\ngb_mask = rfe_gb.support_\n\nvotes = np.sum([lcv_mask, rf_mask, gb_mask], axis = 0)\n\nmask  = votes > 2\n\n\nX_train = X_train.loc[:, mask]\nX_test = X_test.loc[:, mask]\nX_train_std = X_train_std.loc[:, mask]\nX_test_std = X_test_std.loc[:, mask]\n\n\n\nlassopd = pd.DataFrame([int(i) for i in lcv_mask ], index = reduced_X.columns, columns = ['Lasso']).loc[lcv_mask, :]\nrfpd = pd.DataFrame([int(i) for i in rf_mask ], index = reduced_X.columns, columns = ['RandomForest']).loc[rf_mask, :]\ngbpd =  pd.DataFrame([int(i) for i in gb_mask ], index = reduced_X.columns, columns = ['GradientBoosting']).loc[gb_mask, :]\n\nvotepd = pd.DataFrame(votes, index = reduced_X.columns, columns = ['tally']).loc[votes>0, :]\n\n\nreduced_X = reduced_X.loc[:, mask]\npd.concat([lassopd, rfpd, gbpd, votepd], axis = 1, sort = True).fillna(0).sort_values(by='tally', ascending = False)\n\n","9e1a6bac":"from sklearn.model_selection import validation_curve\n\n\n\n\nplt.style.use('ggplot')\n\nfig, ((ax, ax1, ax2), (ax3, ax4, ax5) ) = plt.subplots(2, 3, figsize=(20, 15))\n\n\n\nkernels = ['linear','poly', 'rbf', 'sigmoid']\ntrain_scores, valid_scores = validation_curve(SVC(), X_train_std, y_train, \"kernel\",kernels, cv=kf)\nax.plot(kernels, np.mean(train_scores, axis = 1), label = 'training accuracy')\nax.fill_between(kernels, np.mean(train_scores, axis = 1) + np.std(train_scores, axis = 1)\/2, np.mean(train_scores, axis = 1) - np.std(train_scores, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax.plot(kernels, np.mean(valid_scores, axis = 1), label = 'validation accuracy')\nax.fill_between(kernels, np.mean(valid_scores, axis = 1) + np.std(valid_scores, axis = 1)\/2, np.mean(valid_scores, axis = 1) - np.std(valid_scores, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax.legend(loc = 'upper right')\nax.set_ylabel('Accuracy')\nax.set_xlabel('kernel_type')\nax.set_title('SVC: Kernel Type')\n\na1 = np.logspace(-3, -1, 3)\na2 = np.arange(1,11,1)\n\nC_param_range = np.outer(a1, a2).flatten()\n\n\ntrain_scores, valid_scores = validation_curve(SVC(kernel = 'linear'), X_train_std, y_train, \"C\",C_param_range, cv=kf)\nax1.plot(C_param_range, np.mean(train_scores, axis = 1), label = 'training accuracy')\nax1.fill_between(C_param_range, np.mean(train_scores, axis = 1) + np.std(train_scores, axis = 1)\/2, np.mean(train_scores, axis = 1) - np.std(train_scores, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax1.plot(C_param_range, np.mean(valid_scores, axis = 1), label = 'validation accuracy')\nax1.fill_between(C_param_range, np.mean(valid_scores, axis = 1) + np.std(valid_scores, axis = 1)\/2, np.mean(valid_scores, axis = 1) - np.std(valid_scores, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax1.set_ylabel('Accuracy')\nax1.set_xlabel('C_parameter')\nax1.set_title('SVC: C parameter')\n\nindex = np.where((np.mean(valid_scores, axis = 1) == max(np.mean(valid_scores, axis = 1))))[0][0]\nc_svc = C_param_range[index]\n\na1 = np.logspace(-3, -1, 3)\na2 = np.arange(1,11,1)\ngammas = np.outer(a1, a2).flatten()\n\ntrain_scores, valid_scores = validation_curve(SVC(kernel = 'linear'), X_train_std, y_train, \"gamma\",gammas, cv = kf)\nax2.plot(gammas, np.mean(train_scores, axis = 1), label = 'training accuracy')\nax2.fill_between(gammas, np.mean(train_scores, axis = 1) + np.std(train_scores, axis = 1)\/2, np.mean(train_scores, axis = 1) - np.std(train_scores, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax2.plot(gammas, np.mean(valid_scores, axis = 1), label = 'validation accuracy')\nax2.fill_between(gammas, np.mean(valid_scores, axis = 1) + np.std(valid_scores, axis = 1)\/2, np.mean(valid_scores, axis = 1) - np.std(valid_scores, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax2.set_ylabel('Accuracy')\nax2.set_xlabel('Gamma')\nax2.set_title('SVC: Gamma')\n\nindex = np.where((np.mean(valid_scores, axis = 1) == max(np.mean(valid_scores, axis = 1))))[0][0]\ng_svc =gammas[index]\n\n\na1 = np.logspace(-3, -2, 2)\na2 = np.arange(1,11,2)\n\nC_param_range = np.outer(a1, a2).flatten()\ntrain_scores, valid_scores = validation_curve(LogisticRegression(), X_train_std, y_train, \"C\",C_param_range, cv= kf)\nax3.plot(C_param_range, np.mean(train_scores, axis = 1), label = 'training accuracy')\nax3.fill_between(C_param_range, np.mean(train_scores, axis = 1) + np.std(train_scores, axis = 1)\/2, np.mean(train_scores, axis = 1) - np.std(train_scores, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax3.plot(C_param_range, np.mean(valid_scores, axis = 1), label = 'validation accuracy')\nax3.fill_between(C_param_range, np.mean(valid_scores, axis = 1) + np.std(valid_scores, axis = 1)\/2, np.mean(valid_scores, axis = 1) - np.std(valid_scores, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax3.set_title('Logistic Regression: C Parameter')\nax3.set_ylabel('Accuracy')\nax3.set_xlabel('C Parameter')\n\nindex = np.where((np.mean(valid_scores, axis = 1) == max(np.mean(valid_scores, axis = 1))))[0][0]\nlr_c =C_param_range[index]\n\ntree = DecisionTreeClassifier()\npath = tree.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\ntrain_scores, valid_scores = validation_curve(DecisionTreeClassifier(), X_train, y_train, \"ccp_alpha\",ccp_alphas, cv=kf)\n\nax4.plot(ccp_alphas, np.mean(train_scores, axis = 1), label = 'training accuracy')\nax4.fill_between(ccp_alphas, np.mean(train_scores, axis = 1) + np.std(train_scores, axis = 1)\/2, np.mean(train_scores, axis = 1) - np.std(train_scores, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax4.plot(ccp_alphas, np.mean(valid_scores, axis = 1), label = 'validation accuracy')\nax4.fill_between(ccp_alphas, np.mean(valid_scores, axis = 1) + np.std(valid_scores, axis = 1)\/2, np.mean(valid_scores, axis = 1) - np.std(valid_scores, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax4.set_title('DecisionTree: ccp alpha')\nax4.set_ylabel('Accuracy')\nax4.set_xlabel('ccp_alpha')\n\nindex = np.where((np.mean(valid_scores, axis = 1) == max(np.mean(valid_scores, axis = 1))))[0][0]\nccp_alpha_dt = ccp_alphas[index]\n\n\ntree = DecisionTreeClassifier()\npath = tree.cost_complexity_pruning_path(X_train, y_train)\nccp_alphas, impurities = path.ccp_alphas, path.impurities\ntrain_scores, valid_scores = validation_curve(RandomForestClassifier(random_state = 10), X_train, y_train, \"ccp_alpha\",ccp_alphas, cv=kf)\n\nindex = np.where((np.mean(valid_scores, axis = 1) == max(np.mean(valid_scores, axis = 1))))[0][0]\nccp_alpha_rf = ccp_alphas[index]\n\n\nax5.plot(ccp_alphas, np.mean(train_scores, axis = 1), label = 'training accuracy')\nax5.fill_between(ccp_alphas, np.mean(train_scores, axis = 1) + np.std(train_scores, axis = 1)\/2, np.mean(train_scores, axis = 1) - np.std(train_scores, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax5.plot(ccp_alphas, np.mean(valid_scores, axis = 1), label = 'validation accuracy')\nax5.fill_between(ccp_alphas, np.mean(valid_scores, axis = 1) + np.std(valid_scores, axis = 1)\/2, np.mean(valid_scores, axis = 1) - np.std(valid_scores, axis = 1)\/2, interpolate = True, color='#888888', alpha=0.4)\nax5.set_title('RandomForest: ccp alpha')\nax5.set_ylabel('Accuracy')\nax5.set_xlabel('ccp_alpha')\nplt.show()","5f62ab23":"from sklearn.metrics import classification_report\n\n\nsvc = SVC(kernel = 'linear', C = c_svc)\nsvc.fit(X_train_std, y_train)\ny_pred = svc.predict(X_test_std)\nprint('SVC\\n')\nprint(classification_report(y_test, y_pred))\n\nlog = LogisticRegression(C = lr_c)\nlog.fit(X_train_std, y_train)\ny_pred = log.predict(X_test_std)\nprint('\\nLogiistic Regression\\n')\nprint(classification_report(y_test, y_pred))\n\ntree = DecisionTreeClassifier(ccp_alpha = ccp_alpha_dt)\ntree.fit(X_train, y_train)\ny_pred = tree.predict(X_test)\nprint('\\nDecision Tree\\n')\nprint(classification_report(y_test, y_pred))\n\n#ccp_alpha_rf\nrf = RandomForestClassifier(random_state = 10, ccp_alpha = ccp_alpha_rf)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\nprint('\\nRandom Forest\\n')\nprint(classification_report(y_test, y_pred))","9e7a9d19":"import graphviz\nfrom sklearn.tree import export_graphviz\n\n\n\n\ndot_data =export_graphviz(tree, out_file = None, feature_names =reduced_X.columns, class_names = ['not disease', 'disease'])\ngraph = graphviz.Source(dot_data)\ngraph","5b4361fb":"#dataframe with most important factors sorted by coefficient absolute value\ndf = pd.DataFrame(log.coef_.flatten(), index = X_train.columns, columns = ['coeficients'])\ndf.reindex(df.coeficients.abs().sort_values(ascending= False).index)","6c3d62cc":"df = pd.DataFrame(rf.feature_importances_.flatten(), index = X_train.columns, columns = ['importance'])\ndf.reindex(df.importance.abs().sort_values(ascending= False).index)","0bf36edd":"df = pd.DataFrame(svc.coef_.flatten(), index = X_train.columns, columns = ['coeficients'])\ndf.reindex(df.coeficients.abs().sort_values(ascending= False).index)","bae87b73":"from sklearn.neighbors.classification import KNeighborsClassifier\nfrom sklearn.manifold.t_sne import TSNE\n\nreduced_X_std = pd.concat([X_train_std,X_test_std])\ny_concat = pd.concat([y_train,y_test])\n\nresolution = 100\n\nfig, ((ax, ax1),(ax2, ax3)) = plt.subplots(2,2, figsize = (10,10))\n\nX_embedded = TSNE(n_components=2).fit_transform(reduced_X_std)\n\nlog_predicted = log.predict(reduced_X_std)\n\nX2d_xmin, X2d_xmax = np.min(X_embedded[:,0]), np.max(X_embedded[:,0])\n\nX2d_ymin, X2d_ymax = np.min(X_embedded[:,1]), np.max(X_embedded[:,1])\n\nxx, yy = np.meshgrid(np.linspace(X2d_xmin, X2d_xmax, resolution), np.linspace(X2d_ymin, X2d_ymax, resolution))\n\nbackground_model= KNeighborsClassifier(n_neighbors=1).fit(X_embedded, log_predicted) \n\nvoronoiBackground = background_model.predict(np.c_[xx.ravel(), yy.ravel()])\n\nvoronoiBackground = voronoiBackground.reshape((resolution, resolution))\n\nax.contourf(xx, yy, voronoiBackground, alpha = .5)\n\nax.scatter(X_embedded[:,0], X_embedded[:,1], c=y_concat)\nax.set_title('Logistic Regression')\nax.set_axis_off()\n\n\n\n\nsvc_predicted = svc.predict(reduced_X_std)\nbackground_model= KNeighborsClassifier(n_neighbors=1).fit(X_embedded, svc_predicted) \nvoronoiBackground = background_model.predict(np.c_[xx.ravel(), yy.ravel()])\nvoronoiBackground = voronoiBackground.reshape((resolution, resolution))\n\nax1.contourf(xx, yy, voronoiBackground, alpha = .5)\n\nax1.scatter(X_embedded[:,0], X_embedded[:,1], c=y_concat)\nax1.set_title('SVC')\nax1.set_axis_off()\n\nrf_predicted = rf.predict(reduced_X_std)\nbackground_model= KNeighborsClassifier(n_neighbors=1).fit(X_embedded, rf_predicted) \nvoronoiBackground = background_model.predict(np.c_[xx.ravel(), yy.ravel()])\nvoronoiBackground = voronoiBackground.reshape((resolution, resolution))\n\nax2.contourf(xx, yy, voronoiBackground, alpha = .5)\n\nax2.scatter(X_embedded[:,0], X_embedded[:,1], c=y_concat)\nax2.set_title('Random Forest')\nax2.set_axis_off()\n\ntree_predicted = tree.predict(reduced_X_std)\nbackground_model= KNeighborsClassifier(n_neighbors=1).fit(X_embedded, tree_predicted) \nvoronoiBackground = background_model.predict(np.c_[xx.ravel(), yy.ravel()])\nvoronoiBackground = voronoiBackground.reshape((resolution, resolution))\n\nax3.contourf(xx, yy, voronoiBackground, alpha = .5)\n\nax3.scatter(X_embedded[:,0], X_embedded[:,1], c=y_concat)\nax3.set_title('Decision Tree')\nax3.set_axis_off()\n\nplt.show()\n\n","d6d0999a":"## Comparison of Feature Importances by Model <a id = 'feature_importances'><\/a>\n\n### Decision Tree <a id = 'decision_tree'><\/a>\n\nThe Decision Tree Classifier is printed below. As can be seen from the printout, the classifier only uses three feature which results in an prediction accuracy of ~.75. First it looks at the results of the thalium stress test (thal_normal). If the results are not normal it classifiers the patient as having heart disease. If the results are normal it asks whether the patient has asyptomatic chest pain (cp_asymptomatic). If not, it classifies the patient as not having heart disease. If the patient has asymptomatic chest pain it asks whether any arteries are colored by fluoroscopy (ca). If arteries are colored by fluoroscopy which indicates arterial blockages it classifies the patient as having heart disease. If not it classifies the patient a not having heart disease. ","5950834d":"## Conclusion <a id = 'conclusion'><\/a>\n\nBy using the results of the non-invasive tests, we can train models that can predict coronary heart disease with substantial agreement with the results of invasive coronary angiography. The best peforming model was SVC which predicted unseen test data with 90% accuracy. Conservatively, maybe we could expect an accuracy of ~ 85% using this type of model. ","f4150ad1":"## Visualization of Decision Regions <a id = 'decision_regions'><\/a>\n\nHere we use t-sne to visualize approximate decision regions for the different models. As seen below, the SVC and Logistic Regression models do the best job of separating diseased and not diseased datapoints into separate decision regions. ","1866ffe9":"## Model Validation on Hold-Out Data <a id = 'validation'><\/a>\n\nModels are trained with optimal hyperparameters on the training set and scored using the unseen test set, the results of which are printed below. As seen from the validation scores, SVC, Random Forest and Logistic Regression classifiers perform the best on test data with test accuracies of between.87-.9. The Decision Tree classifier performs the worst with a test accuracty of .75. Of course these are just estimates of how these models would generalize so we should take them with a grain of salt. ","4ac85fd7":"## Context <a id = 'context'><\/a>\n\nCoronary heart disease (CHD) involves the reduction of blood flow to the heart muscle due to build-up of plaque in the arteries of the heart. It is the most common form of cardiovascular disease. Currently, invasive coronary angiography represents the gold standard for establishing the presence, location, and severity of CAD, however this diagnostic method is costly and associated with morbidity and mortality in CAD patients. Therefore, it would be beneficial to develop a non-invasive alternative to replace the current gold standard. \n\nOther less invasive diagnostics methods have been proposed in the scientific literature including exercise electrocardiogram, thallium scintigraphy and fluoroscopy of coronary calcification. However the diagnostic accuracy of these tests only ranges between 35%-75%. Therefore, it would be beneficial to develop a computer aided diagnostic tool that could utilize the combined results of these non-invasive tests in conjunction with other patient attributes to boost the diagnostic power of these non-invasive methods with the aim ultimately replacing the current invasive gold standard.\n\nIn this vein, the following dataset comprises 303 observations, 13 features and 1 target attribute. The 13 features include the results of the aforementioned non-invasive diagnostic tests along with other relevant patient information. The target variable includes the result of the invasive coronary angiogram which represents the presence or absence of coronary artery disease in the patient with 0 representing absence of CHD and labels 1-4 representing presence of CHD. Therefore, the task at hand is to predict the result of the invasive coronary angiography, the current gold standard, using the results of the aforementioned non-invasive tests and patient information. \n\nThe data was originally collected by Robert Detrano, M.D., Ph.D of the Cleveland Clinic Foundation. \n\n","9ba1080b":"## Out of the Box Model Comparisons: Logistic Regression, SVC, Decision Tree and Random Forest <a id = 'learning_curves'><\/a>\n\n\nLearning curves are plotted for four out-of-the-box models. The curves give an idea of whether the models would benefit from additional data, whether the models are overfitting or underfitting the data, and of baseline model peformance. Training accuracies are plotted in red and valdiation accuracies are plotted in blue. ","9a3a25a9":"## Table of Contents\n* [Context](#context)\n* [Read and Preprocess Data](#preprocess)\n* [Out of the Box Model Comparisons: Logistic Regression, SVC, Decision Tree and Random Forest](#learning_curves)\n* [Feature Selection](#feature_selection)\n* [Hyperparameter Tuning with Cross-Validation](#validation_curves)\n* [Model Validation on Hold-Out Data](#validation)\n* [Comparison of Feature Importances by Model](#feature_importances)\n    * [Decision Tree](#decision_tree)\n    * [Logistic Regression](#logistic_regression)\n    * [Random Forest](#random_forest)\n    * [SVC](#svc)\n* [Visualization of Decision Regions](#decision_regions)\n* [Conclusion](#conclusion)\n","812c9c22":"## Feature Selection <a id = 'feature_selection'><\/a>\n\nThe data is split into training and test sets. Lasso Regression is combined with recursive feature selection methods using Random Forest and GradientBoosting models to reduce the dataset to include only the most important features. Only features deemed important by all three methods were maintained in the reduced dataset, which reduces the number of utilized features from 22 to 15, as shown below. A random seed of 1 is in the train test split to ensure reproducibility. ","66f52d1b":"### Logistic Regression <a id = 'logistic_regression'><\/a>\n\nThe coeficients from the Logistic Regression Classifier sorted in descending order by their absolute values are shown below. Since the data is normalized prior to training, this gives an indication of which features the model found to be most important. ","7342200d":"### SVC <a id = 'svc'><\/a>\n\nFeature coeficients used by the SVC model are likewise sorted in descending order by coefficient absolute value and printed below. By comparing the different models, it is apparent that the degree of importance assigned to the different features differs between models. ","75944af5":"## Hyperparameter Tuning with Cross-Validation <a id = 'validation_curves'><\/a>\n\nValidation curves are plotted to determine the optimal hyperparameters for the different models. Optimal hyperparameters are those that optimize valitation scores while reducing the degree of model overfitting as determined by the differences between training scores (red) and validation scores (blue).   ","97b4c0da":"## Read and Preprocess Data: <a id = 'preprocess'><\/a>\n\nData is read from csv and dummy variables are create for all non-binary categorical data.","29dfdc1b":"# Predicting Coronary Heart Disease Non-Invasively","cb76df45":"### Random Forest <a id = random_forest><\/a>\n\nFeature importances calculated by the Random Forest Classifier are printed below. "}}