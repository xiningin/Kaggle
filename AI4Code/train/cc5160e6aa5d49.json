{"cell_type":{"085619d1":"code","0cc99bbe":"code","68e11adb":"code","f765cfce":"code","cd45c6a5":"code","1371f962":"code","a440662f":"code","09eb49d9":"code","20bec315":"code","aa2158ad":"code","3d62f353":"code","ba9722e6":"code","b5a4ccc0":"code","786ca8eb":"code","367218fa":"code","bf7e69ee":"code","37911e96":"code","88824106":"code","1302550e":"code","9bf40660":"code","16dd64d1":"code","0a0b4aa8":"code","a3ece021":"code","8d547c5a":"code","ae720e5d":"code","29aaa90a":"code","ea5cfc13":"code","333c9035":"code","56015d37":"code","116ebd90":"code","a78bc57d":"code","29931bf3":"code","ede71db0":"code","de559d22":"code","62c2d599":"code","74a48a97":"code","256dd739":"code","31ab899e":"code","b0e3ff63":"code","abd8046f":"code","e29b9ba1":"code","0df2f7ae":"code","2f2c27fa":"code","feff0424":"code","67a61482":"code","46937da5":"code","abccad97":"code","a5b71e3a":"code","a0340b08":"code","a592e453":"code","f031a3ed":"code","4c45652a":"code","50e8b6a6":"markdown","2de3f289":"markdown","57345a0d":"markdown","7e56d5be":"markdown","8cc762db":"markdown","7af2fc61":"markdown","d82bbb62":"markdown","5bb43716":"markdown","0e46e039":"markdown","048442f5":"markdown","518f1a32":"markdown","a7d87332":"markdown"},"source":{"085619d1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0cc99bbe":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import roc_auc_score, accuracy_score, f1_score\npd.set_option(\"display.max_columns\", None)","68e11adb":"train = pd.read_csv(\"..\/input\/summeranalytics2020\/train.csv\", index_col= \"Id\")\ntest = pd.read_csv(\"..\/input\/summeranalytics2020\/test.csv\")\ntrain.head()","f765cfce":"train.isnull().sum()","cd45c6a5":"train.duplicated().sum()","1371f962":"train.drop_duplicates(inplace= True)","a440662f":"train.dtypes","09eb49d9":"train.describe().T","20bec315":"train.nunique()","aa2158ad":"train.drop(\"Behaviour\", axis= \"columns\", inplace= True)\ntest.drop(\"Behaviour\", axis= \"columns\", inplace= True)","3d62f353":"train.shape","ba9722e6":"sns.countplot(x= \"Attrition\", data= train)","b5a4ccc0":"train.columns","786ca8eb":"nominal_variables = train.select_dtypes(include= \"object\").columns.to_list()\ncontinuous_variables = ['Age','DistanceFromHome','EmployeeNumber','MonthlyIncome','NumCompaniesWorked', 'PercentSalaryHike','TotalWorkingYears', 'TrainingTimesLastYear', 'YearsAtCompany', 'YearsInCurrentRole','YearsSinceLastPromotion','YearsWithCurrManager']\nordinal_variables = ['EnvironmentSatisfaction','JobInvolvement','JobSatisfaction','Education','CommunicationSkill','PerformanceRating','StockOptionLevel']\n\nprint (len(nominal_variables), len(continuous_variables), len(ordinal_variables))\nlen(nominal_variables + continuous_variables + ordinal_variables)","367218fa":"fig, ax = plt.subplots(4, 3, figsize= (15, 15))\nfor feature, ax in zip(continuous_variables, fig.axes):\n    sns.distplot(train[feature], ax= ax)","bf7e69ee":"plt.figure(figsize= (10, 10))\nsns.heatmap(train[continuous_variables].corr(), vmin= -1, vmax= 1, annot= True, square= True, cmap= \"viridis\")","37911e96":"def cont_cols_plot(feature):\n    fig, ax = plt.subplots(1, 3, figsize= (15, 4))\n    sns.distplot(train[train.Attrition == 0][feature], color = \"blue\", ax= ax[0])\n    sns.distplot(train[train.Attrition == 1][feature], color = \"darkred\", ax= ax[0])\n    \n    sns.boxplot(x= \"Attrition\", y= feature, data= train, ax= ax[1])\n    sns.pointplot(x= \"Attrition\", y= feature, data= train, ax= ax[2])\n    \n    plt.show()","88824106":"for i in continuous_variables:\n    cont_cols_plot(i)\n    print (\"-\"*50)\n","1302550e":"from scipy.stats import ttest_ind\n\nttest_pvalue = pd.DataFrame(index= continuous_variables, columns= [\"p_value\"])\n\nfor feature in continuous_variables:\n    ttest = ttest_ind(train[train.Attrition ==0][feature], train[train.Attrition == 1][feature], equal_var= False)\n    ttest_pvalue.loc[feature, \"p_value\"] = ttest[1]\n    \nttest_pvalue[\"p_value < 0.05\"] = ttest_pvalue.apply(lambda x: x < 0.05)\nttest_pvalue","9bf40660":"import seaborn as sns\nplt.figure(figsize= (8, 8))\nsns.heatmap(train[ordinal_variables].corr(method= \"kendall\"), vmin= -1, vmax= 1, square= True, cmap= \"viridis\", annot= True) ","16dd64d1":"train[ordinal_variables].nunique()","0a0b4aa8":"fig, ax = plt.subplots(2, 4, figsize= (18, 8))\nfor feature, ax in zip(ordinal_variables, fig.axes):\n    sns.countplot(x= feature, data= train, ax= ax)","a3ece021":"def ordinal_cols_plot(feature):\n    fig, ax = plt.subplots(1, 2, figsize = (10, 4))\n    sns.countplot(x= feature, hue= \"Attrition\", data= train, ax= ax[0])\n    \n    cross = pd.crosstab(index= train[feature], columns= train.Attrition, normalize= \"index\")\n    sns.pointplot(x= cross.index, y= cross[1]*100, ax= ax[1])\n    plt.ylabel(\"Attrition Percentage\")\n    \n    print (cross)\n    plt.show()","8d547c5a":"for i in ordinal_variables:\n    ordinal_cols_plot(i)\n    print (\"--\"*50)","ae720e5d":"train[nominal_variables].nunique()","29aaa90a":"fig, ax = plt.subplots(2, 4, figsize= (18, 8))\nfor feature, ax in zip(nominal_variables, fig.axes):\n    sns.countplot(x= feature, data= train, ax= ax, order= train[feature].value_counts().index)","ea5cfc13":"def nominal_cols_plot(feature):\n    fig, ax = plt.subplots(1, 2, figsize = (10, 4))\n    sns.countplot(x= feature, hue= \"Attrition\", data= train, ax= ax[0], order = train[feature].value_counts().index)\n    \n    cross = pd.crosstab(index= train[feature], columns= train.Attrition, normalize= \"index\")\n    sns.pointplot(x= cross.index, y= cross[1]*100, ax= ax[1], order = train[feature].value_counts().index)\n    plt.ylabel(\"Attrition Percentage\")\n    \n    print (cross)\n    plt.show()","333c9035":"for i in nominal_variables:\n    nominal_cols_plot(i)\n    print (\"-\"*50)","56015d37":"fig, ax = plt.subplots(2, 1, figsize = (10, 9))\nsns.countplot(x= \"EducationField\", hue= \"Attrition\", data= train, ax= ax[0], order= train.EducationField.value_counts().index)\n    \ncross = pd.crosstab(index= train.EducationField, columns= train.Attrition, normalize= \"index\")\nsns.pointplot(x= cross.index, y= cross[1]*100, ax= ax[1], order= train.EducationField.value_counts().index)\nplt.ylabel(\"Attrition Percentage\")\n\nprint (cross)\nplt.show()","116ebd90":"fig, ax = plt.subplots(2, 1, figsize = (18, 9))\nsns.countplot(x= \"JobRole\", hue= \"Attrition\", data= train, ax= ax[0], order = train.JobRole.value_counts().index)\n    \ncross = pd.crosstab(index= train.JobRole, columns= train.Attrition, normalize= \"index\")\nsns.pointplot(x= cross.index, y= cross[1]*100, ax= ax[1], order = train.JobRole.value_counts().index)\nplt.ylabel(\"Attrition Percentage\")\nplt.xticks(rotation= 90)\n\nprint (cross)\nplt.show()","a78bc57d":"a = ordinal_variables + nominal_variables\nfrom scipy.stats import chi2_contingency\n\nchi2_pvalue = pd.DataFrame(index= a, columns= [\"p_value\"])\nfor feature in a:\n    cross = pd.crosstab(index= train[feature], columns= train.Attrition)\n    chi2 = chi2_contingency(cross)\n    \n    chi2_pvalue.loc[feature, \"p_value\"] = chi2[1]\n\nchi2_pvalue[\"p_value < 0.05\"] = chi2_pvalue.apply(lambda x: x < 0.05)   \nchi2_pvalue    ","29931bf3":"features_to_drop = [\"EmployeeNumber\", \"NumCompaniesWorked\", \"PercentSalaryHike\", \"TotalWorkingYears\", \n                    \"YearsInCurrentRole\", \"YearsWithCurrManager\", \"YearsSinceLastPromotion\", \n                    \"Education\", \"CommunicationSkill\", \"PerformanceRating\", \"Gender\"]\nlen(features_to_drop)","ede71db0":"train_dropped = train.drop(features_to_drop, axis= \"columns\").copy()\ntest_dropped = test.drop(features_to_drop, axis= \"columns\").copy()","de559d22":"Id = test_dropped.pop(\"Id\")\ny = train_dropped.pop(\"Attrition\")","62c2d599":"train_dropped.columns","74a48a97":"nominal_cols = train_dropped.select_dtypes(include= \"object\").columns.tolist()\nordinal_cols = [i for i in ordinal_variables if i in train_dropped.columns]\nnominal_cols + ordinal_cols","256dd739":"cat_index = [train_dropped.columns.get_loc(i) for i in nominal_cols + ordinal_cols]\ncat_index","31ab899e":"train_dropped[nominal_cols].head()","b0e3ff63":"le = LabelEncoder()\n\nfor i in nominal_cols:\n    train_dropped[i] = le.fit_transform(train_dropped[i])\n    print (le.classes_)\n    test_dropped[i] = le.transform(test_dropped[i])\n    ","abd8046f":"train_dropped[nominal_cols].head()","e29b9ba1":"y.value_counts()","0df2f7ae":"from imblearn.over_sampling import SMOTENC","2f2c27fa":"sm = SMOTENC(categorical_features = np.asarray(cat_index), random_state= 5)\nX_train_res, y_train_res = sm.fit_sample(train_dropped, y)\ny_train_res.value_counts()","feff0424":"X_train_res.shape","67a61482":"skf= StratifiedKFold(n_splits=5, shuffle= False, random_state= 5)","46937da5":"def model_accuracy(model, param_grid):\n    classifier = Pipeline(steps=[\n        (\"scaler\", StandardScaler()),\n        (\"model\", model)\n    ])\n    grid= GridSearchCV(estimator= classifier, param_grid= param_grid, cv= skf, scoring= \"roc_auc\")\n    grid.fit(X_train_res, y_train_res)\n    \n    print (\"Best_score: \", grid.best_score_, \"\\n\", \"Best_params: \", grid.best_params_)\n    return (pd.DataFrame(grid.cv_results_)\n            .sort_values([\"rank_test_score\"])[[\"params\", \"mean_test_score\", \"std_test_score\"]].head(10).T)","abccad97":"model_accuracy(SVC(), param_grid= { \"model__C\": [0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100], \"model__gamma\": [0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1]})","a5b71e3a":"model_accuracy(LogisticRegression(penalty= \"l1\", solver= \"liblinear\"), param_grid= {\"model__C\": [0.01, 0.05, 0.1, 0.5, 1, 5, 10, 50, 100]})","a0340b08":"final_svm = Pipeline(steps= [\n    (\"scaler\", StandardScaler()),\n    (\"model\", SVC(C= 5, gamma= 0.1, probability= True))\n])\n\nfinal_svm.fit(X_train_res, y_train_res)","a592e453":"predictions = final_svm.predict_proba(test_dropped)[:, 1]\npredictions[:5]","f031a3ed":"output = pd.DataFrame({\"Id\": Id,\n                       \"Attrition\": predictions})\noutput.head()","4c45652a":"output.to_csv(\"submission.csv\", index= False)","50e8b6a6":"- **Okay let's Drop Behaviour.**","2de3f289":"- **We have to remove highly correlated variables. Let's remove TotalWorkingYears, YearsInCurrentRole, YearsWithCurrentManager and YearsSinceLastPromotion.**","57345a0d":"- **We will drop Education, CommunicationSkill, PerformanceRating and Gender.**","7e56d5be":"# EDA and Feature Selection","8cc762db":"- **No missing values. Great!**","7af2fc61":"## Categorical features","d82bbb62":"- **The data is highly imbalanced. We have to do oversampling before building our model to prevent skewed results.**","5bb43716":"- We will remove EmployeeNumber, NumCompaniesWorked, PercentSalaryHike, YearsSinceLastPromotion","0e46e039":"## Continuous Variables","048442f5":"- **We need to remove these duplicates else it will skew our analysis.**","518f1a32":"# Model Building","a7d87332":"- **Behaviour has 0 variance. Strange!**"}}