{"cell_type":{"5c007ce1":"code","51f7568c":"code","85b1ff9c":"code","004a5c9d":"code","117f320a":"code","d28007cc":"code","0f4f4d4c":"code","8f9f7071":"code","1a6fac6a":"code","c27fc974":"code","8a8e1af3":"code","9bed4f2c":"code","27905cf0":"code","d28a97f8":"code","ca1349d1":"code","d1516f36":"code","d48e76d8":"code","93cded0e":"code","068af7d4":"code","afc74db4":"code","f16c5025":"code","0c7f9f83":"code","e299ac93":"code","a1757419":"code","9502b765":"code","901c4eda":"code","e663bd9a":"code","5813a60e":"code","0b09b932":"code","13eece79":"code","fbdbf902":"code","710d34a6":"code","a4e2f644":"code","cbd8f02c":"code","f1183ce4":"markdown","498fd9c6":"markdown","52ddc322":"markdown","975056fb":"markdown","d5bd4bb4":"markdown","570f59c5":"markdown","06270929":"markdown","87ef823e":"markdown","67f8b6a2":"markdown","b92b0c98":"markdown","3c1fb645":"markdown","b9a82a55":"markdown","68026e0a":"markdown","f2547f50":"markdown","bb8033fe":"markdown","0a486970":"markdown","debaa441":"markdown","dafa1eed":"markdown","2eb7f295":"markdown","5da70728":"markdown","cc77655d":"markdown"},"source":{"5c007ce1":"!pip install ipython-autotime\n!pip install --upgrade transformers\n%matplotlib inline\n%load_ext autotime","51f7568c":"import os,re\nimport unicodedata\nimport gc\nimport time\nimport numpy as np \nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, BatchNormalization, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.losses import BinaryCrossentropy\n\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom transformers import T5Tokenizer, TFT5Model\nfrom transformers import TFRobertaModel, RobertaTokenizerFast, RobertaConfig\nfrom tokenizers import BertWordPieceTokenizer\nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n\nfrom tqdm.notebook import tqdm\nfrom numba import jit, cuda ","85b1ff9c":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","004a5c9d":"# the data paths\ndata_path = '\/kaggle\/input\/jigsaw-multilingual-toxic-comment-classification\/'\ntranslated_data_path = '\/kaggle\/input\/jigsaw-multilingual-toxic-test-translated\/'\n\n# loading all the train datasets\n\ntrain_data1 = pd.read_csv(data_path + 'jigsaw-toxic-comment-train.csv')\n# train_data2 = pd.read_csv(data_path + 'jigsaw-toxic-comment-train-processed-seqlen128.csv')\ntrain_data3 = pd.read_csv(data_path + 'jigsaw-unintended-bias-train.csv')\n# train_data4 = pd.read_csv(data_path + 'jigsaw-unintended-bias-train-processed-seqlen128.csv')\n\n# loading all the validation and test datasets\n\n# validation_data1 = pd.read_csv(data_path + 'validation.csv')\n# validation_data2 = pd.read_csv(data_path + 'validation-processed-seqlen128.csv')\nvalid_translated = pd.read_csv(translated_data_path + 'jigsaw_miltilingual_valid_translated.csv')\n\n# test_data1 = pd.read_csv(data_path + 'test.csv')\n# test_data2 = pd.read_csv(data_path + 'test-processed-seqlen128.csv')\ntest_translated = pd.read_csv(translated_data_path + 'jigsaw_miltilingual_test_translated.csv')","117f320a":"# Stopword list\npattern = re.compile(r'\\b('+r'|'.join(stopwords.words('english'))+r')\\b\\s*')\n\n# @cuda.jit(device=True)\ndef unicode_to_ascii(s):\n  return ''.join(c for c in unicodedata.normalize('NFD', s)\n      if unicodedata.category(c) != 'Mn')\n\n# @tf.function()\ndef clean_text(text):\n    text = unicode_to_ascii(text.lower().strip())\n    \n    #replacing email addresses with blank space\n    text = re.sub(r\"[a-zA-Z0-9_\\-\\.]+@[a-zA-Z0-9_\\-\\.]+\\.[a-zA-Z]{2,5}\",\" \",text)\n    \n    #replacing urls with blank space\n    text = re.sub(r\"\\bhttp:\\\/\\\/([^\\\/]*)\\\/([^\\s]*)|https:\\\/\\\/([^\\\/]*)\\\/([^\\s]*)\",\" \",text)\n    \n    # creating a space between a word and the punctuation following it\n    text = re.sub(r\"([?.!,\u00bf])\", r\" \\1 \", text)\n    text = re.sub(r'[\" \"]+', \" \", text)\n    \n    # replacing all the stopwords\n    text = pattern.sub('',text)\n    \n    # removes all the punctuations\n    text = re.sub(r\"[^a-zA-Z]+\", \" \", text)\n    \n    text = text.strip()\n\n    # adding a start and an end token to the sentence so that the model know when to start and stop predicting.\n#     text = '<start> ' + text + ' <end>'\n    \n    return text\n\nclean_text_vect = np.vectorize(clean_text)","d28007cc":"def chunk_clean(array,chunk_size=256):\n    cleaned_array = []\n    \n    for i in tqdm(range(0, len(array), chunk_size)):\n        text_chunk = clean_text_vect(array[i:i+chunk_size])\n        cleaned_array.extend(text_chunk)\n\n    return np.array(cleaned_array)","0f4f4d4c":"def regular_encode(texts, tokenizer, maxlen=512):\n    \n    tokenizer.pad_token = tokenizer.pad_token\n    tokenizer.unk_token = tokenizer.unk_token\n    tokenizer.eos_token = tokenizer.eos_token\n    \n    enc_di = tokenizer.batch_encode_plus(\n        list(texts), \n        return_attention_masks=False, \n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen,\n        add_special_tokens=True\n    )\n    \n    return np.array(enc_di['input_ids'])","8f9f7071":"def chunk_encode(texts,tokenizer,maxlen=512,chunk_size=256):\n    all_enc=[]\n    for i in tqdm(range(0,len(texts),chunk_size)):\n        enc = list(regular_encode(texts[i:i+chunk_size],tokenizer,maxlen=maxlen))\n        all_enc.extend(enc)\n        \n    return np.array(all_enc)","1a6fac6a":"def fast_encode(texts, tokenizer, chunk_size=256, maxlen=512):\n\n    tokenizer.enable_truncation(max_length=maxlen)\n    tokenizer.enable_padding(max_length=maxlen)\n    all_ids = []\n\n    for i in tqdm(range(0, len(texts), chunk_size)):\n        text_chunk = texts[i:i+chunk_size].tolist()\n        encs = tokenizer.encode_batch(text_chunk)\n        all_ids.extend([enc.ids for enc in encs])\n\n    return np.array(all_ids)","c27fc974":"MODEL = 'google\/electra-large-generator'\nMODEL2 = 'google\/electra-large-discriminator'\nMODEL3 = 'gpt2-medium'\nMODEL4 = 'roberta-large'","8a8e1af3":"tokenizer = RobertaTokenizerFast.from_pretrained(MODEL4)\nprint(tokenizer.save_pretrained('.'))\nprint(tokenizer)","9bed4f2c":"# bert_tokenizer = transformers.BertTokenizerFast.from_pretrained('bert-large-uncased')\n# bert_tokenizer.save_pretrained('.')\n# fast_tokenizer = BertWordPieceTokenizer('.\/vocab.json', lowercase=False)\n# fast_tokenizer","27905cf0":"train_data3.toxic = train_data3.toxic.round().astype(int)\nvalid_translated['comment_text'] = valid_translated['translated']\n\ndata = pd.concat([\n    train_data1[['comment_text', 'toxic']],\n    train_data3[['comment_text', 'toxic']].query('toxic==1'),\n    train_data3[['comment_text', 'toxic']].query('toxic==0').sample(n=200000, random_state=0),\n    valid_translated[['comment_text','toxic']]\n])\ndata.toxic = data.toxic.round().astype(int)\ndata.drop_duplicates(inplace=True)\n\nfinal_test_data = test_translated.translated.values\n\nprint('Number of toxic comments = ',list(data.toxic).count(1))\nprint('Number of non-toxic comments = ',list(data.toxic).count(0))","d28a97f8":"np.random.seed(2048)\ntrain, valid, test = np.split(data.sample(frac=1), [int(.94*len(data)), int(.97*len(data))])\n\nprint(\"Train rows = \", train.shape[0])\nprint(\"validate rows = \", valid.shape[0])\nprint(\"Test rows = \", test.shape[0])\nprint(\"\\nFinal Test Data rows = \",len(final_test_data))","ca1349d1":"del train_data1\ndel train_data3\ndel valid_translated\ndel data\ngc.collect()","d1516f36":"MAX_LEN = 128","d48e76d8":"x_train = chunk_clean(train.comment_text.values)\nx_valid = chunk_clean(valid.comment_text.values)\nx_test = chunk_clean(test.comment_text.values)\n\nfinal_test_data = chunk_clean(final_test_data)","93cded0e":"x_train = chunk_encode(x_train, tokenizer, maxlen=MAX_LEN)\nx_valid = chunk_encode(x_valid, tokenizer, maxlen=MAX_LEN)\nx_test = chunk_encode(x_test, tokenizer, maxlen=MAX_LEN)\n\nfinal_test_data = chunk_encode(final_test_data, tokenizer, maxlen=MAX_LEN)","068af7d4":"# x_train = fast_encode(x_train, fast_tokenizer, maxlen=MAX_LEN)\n# x_valid = fast_encode(x_valid, fast_tokenizer, maxlen=MAX_LEN)\n# x_test = fast_encode(x_test, fast_tokenizer, maxlen=MAX_LEN)\n\n# final_test_data = fast_encode(final_test_data, fast_tokenizer, maxlen=MAX_LEN)","afc74db4":"y_train = np.array(train.toxic.values)\ny_train.resize((len(y_train),1))\n\ny_valid = np.array(valid.toxic.values)\ny_valid.resize((len(y_valid),1))\n\ny_test = np.array(test.toxic.values)\ny_test.resize((len(y_test),1))","f16c5025":"print('New shape of comments and labels after TOKENIZATION and PROCESSING:-')\nprint('-'*50)\nprint('Data for Training and Evaluation:\\n')\nprint('x_train shape = ',x_train.shape)\nprint('x_valid shape = ',x_valid.shape)\nprint('x_test shape = ',x_test.shape)\nprint('-'*30)\nprint('Labels shapes:\\n')\nprint('y_train shape = ',y_train.shape)\nprint('y_valid shape = ',y_valid.shape)\nprint('y_test shape = ',y_test.shape)\nprint('-'*50)\nprint('The Final data for Predication:\\n')\nprint('final_test_data shape = ',final_test_data.shape)","0c7f9f83":"AUTO = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync","e299ac93":"train = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\nvalid = (\n    tf.data.Dataset\n    .from_tensor_slices((x_valid, y_valid))\n    .batch(BATCH_SIZE)\n    .cache()\n#     .repeat()\n#     .shuffle(256)\n    .prefetch(AUTO)\n)\n\ntest = (\n    tf.data.Dataset\n    .from_tensor_slices((x_test,y_test))\n    .batch(BATCH_SIZE)\n    .cache()\n#     .shuffle(256)\n    .prefetch(AUTO)\n)\n\nfinal_test_data = (\n    tf.data.Dataset\n    .from_tensor_slices(final_test_data)\n    .batch(BATCH_SIZE)\n)","a1757419":"lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n    initial_learning_rate=1e-5,\n    decay_steps=1000,\n    decay_rate=0.9)","9502b765":"def build_model(transformer, max_len=512):\n    \n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n            \n    out = Dense(64,activation=tf.nn.swish)(cls_token)\n    out = Dense(16,activation=tf.nn.swish)(out)\n    out = Dense(1, activation='sigmoid')(out)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    \n    model.compile(Adam(lr=1e-5),\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    \n    return model","901c4eda":"configs = {\n  \"architectures\": [\n    \"RobertaForMaskedLM\"\n  ],\n  \"attention_probs_dropout_prob\": 0.1,\n  \"bos_token_id\": 0,\n  \"eos_token_id\": 2,\n  \"hidden_act\": \"gelu\",\n  \"hidden_dropout_prob\": 0.2,\n  \"hidden_size\": 1024,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4096,\n  \"layer_norm_eps\": 1e-05,\n  \"max_position_embeddings\": 514,\n  \"model_type\": \"roberta\",\n  \"num_attention_heads\": 16,\n  \"num_hidden_layers\": 24,\n  \"pad_token_id\": 1,\n  \"type_vocab_size\": 1,\n  \"vocab_size\": 50265\n}\n\nconfiguration = RobertaConfig.from_dict(configs)\nconfiguration","e663bd9a":"with strategy.scope():\n    transformer_layer = transformers.TFAutoModel.from_pretrained(MODEL4)\n#     transformer_layer = TFRobertaModel(configuration)\n    model = build_model(transformer_layer, max_len=MAX_LEN)\nmodel.summary()","5813a60e":"EPOCHS = 3","0b09b932":"n_steps = x_train.shape[0] \/\/ BATCH_SIZE\ntrain_history = model.fit(\n    train,\n    steps_per_epoch=n_steps,\n    validation_data=valid,\n#     validation_steps=100,\n    epochs=EPOCHS\n)","13eece79":"epochs_range = range(EPOCHS)\n\nplt.figure(figsize=(16, 5))\n\nplt.subplot(121)\nplt.plot(epochs_range,train_history.history['accuracy'], label='accuracy')\nplt.plot(epochs_range,train_history.history['val_accuracy'], label = 'val_accuracy')\nplt.ylim(0.75,1)\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(loc='center right')\n\nplt.subplot(122)\nplt.plot(epochs_range,train_history.history['loss'], label='loss')\nplt.plot(epochs_range,train_history.history['val_loss'], label = 'val_loss')\nplt.ylim(0.1,0.35)\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(loc='center right')","fbdbf902":"loss,accuracy = model.evaluate(test,verbose=1)\nprint('Loss = ',loss*100,'%')\nprint('Accuracy = ',accuracy*100,'%')","710d34a6":"# n_steps = x_valid.shape[0] \/\/ BATCH_SIZE\n# train_history_2 = model.fit(\n#     valid_dataset.repeat(),\n#     steps_per_epoch=n_steps,\n#     epochs=EPOCHS*2\n# )","a4e2f644":"sub1 = pd.read_csv(data_path + 'sample_submission.csv')\nsub1['toxic'] = model.predict(final_test_data, verbose=1)","cbd8f02c":"sub1.to_csv('submission.csv', index=False)\nsub1.head(15)","f1183ce4":"Data splitting","498fd9c6":"# Converting to Tensorflow dataset","52ddc322":"# Importing required Libraries","975056fb":"Data cleaning Functions","d5bd4bb4":"Initializing the Tokenizers","570f59c5":"Tokenizing and Encoding Functions","06270929":"# Training the Model","87ef823e":"Creating the model","67f8b6a2":"# TPU Configuration","b92b0c98":"# Reading Data","3c1fb645":"# Helper Functions","b9a82a55":"Stage 2","68026e0a":"# Defining the Tokenizer","f2547f50":"Creating submission file","bb8033fe":"Stage 1","0a486970":"Pre-trained models","debaa441":"Cleaning & Tokenizing Input data and Preparing Labels","dafa1eed":"\n# Data Preparation","2eb7f295":"# Loading Model into TPU ","5da70728":"# THE END","cc77655d":"# Exploratory Data Analysis\n\n*Link to the notebook:*\n"}}