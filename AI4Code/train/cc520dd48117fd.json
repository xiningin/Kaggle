{"cell_type":{"f3d686a8":"code","dfae8283":"code","1942665a":"code","dacd7121":"code","b53c9062":"code","b734c652":"code","ade0298d":"markdown"},"source":{"f3d686a8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm import tqdm_notebook as tqdm\n\nimport os\nprint(os.listdir(\"..\/input\"))\n","dfae8283":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\nfeatures = [c for c in train_df.columns if c not in ['ID_code', 'target']] #basic features\ntarget = train_df['target']","1942665a":"def t_encoding(tr,va,by):\n    df = tr.groupby(by).agg({'target':['sum','count']})\n    cols = ['sum_y','count_y']\n    df.columns = cols\n    df = df.reset_index()\n    df = df.sort_values(by)\n\n    df['r'] = df['sum_y'].cumsum() \/ df['count_y'].cumsum()  # I think this is the key operation\n    df.drop(['sum_y','count_y'],axis=1,inplace=True)\n    return va.merge(df,on=by,how='left')['r'].values\n","dacd7121":"\nvar_cols = features\n\nfor col in tqdm(var_cols):\n    te_r = t_encoding(train_df,test_df,col) \n    test_df.loc[:,col+'_r'] = te_r\n\n\nfolds = StratifiedKFold(n_splits=12, shuffle=False, random_state=99999)\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n    print(\"Fold {}\".format(fold_))\n    \n    tr = train_df.loc[trn_idx,var_cols+['target']]\n    va = train_df.loc[val_idx,var_cols+['target']]\n\n    for col in tqdm(var_cols):\n        encoded = t_encoding(tr,va,col)\n        train_df.loc[val_idx,col+'_r'] = encoded\n                \n    \nfeatures = [col for col in train_df.columns if ('var' in col)]\n\ntrain_df = train_df[features].reset_index(drop=True)\nprint('data prepared: {}'.format(train_df.shape))","b53c9062":"param = {\n    'bagging_freq': 5,          \n    'bagging_fraction': 0.38,   'boost_from_average':'false',   \n    'boost': 'gbdt',             'feature_fraction': 1,     'learning_rate': 0.0085,\n    'max_depth': -1,             'metric':'auc',                'min_data_in_leaf': 80,     'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,            'num_threads': 8,              'tree_learner': 'serial',   'objective': 'binary',\n    'reg_alpha': 0.1302650970728192, 'reg_lambda': 0.3603427518866501,'verbosity': 1\n}\n","b734c652":"folds = StratifiedKFold(n_splits=12, shuffle=False, random_state=99999)\noof = np.zeros(len(train_df))\npredictions = np.zeros(len(test_df))\nfor fold_, (trn_idx, val_idx) in enumerate(folds.split(train_df.values, target.values)):\n    print(\"Fold {}\".format(fold_))\n    \n    trn_data = lgb.Dataset(train_df.iloc[trn_idx][features], label=target.iloc[trn_idx])\n    val_data = lgb.Dataset(train_df.iloc[val_idx][features], label=target.iloc[val_idx])\n    clf = lgb.train(param, trn_data, 1000000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 2000)\n    oof[val_idx] = clf.predict(train_df.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    predictions += clf.predict(test_df[features], num_iteration=clf.best_iteration) \/ folds.n_splits\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(target, oof)))\nsub = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nsub[\"target\"] = predictions\nsub.to_csv(\"submission.csv\", index=False)","ade0298d":"This is full implementation of Yimin Nie's solution as given in [discussion](https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/discussion\/88950)\n\nI wanted to get target encoding working without leaking the target variable. The above post by Yimin Nie gives the code to do that.\nI packaged it as a complete solution in a kernel using the starter kernel [30 lines Starter Solution #FAST](https:\/\/www.kaggle.com\/jesucristo\/30-lines-starter-solution-fast)\n\nHope this serves as a reference for anyone who want to know how to get target encoding working. If there is something wrong please comment.\n\nChanges: \nRemoved DF reversals as it doesn't help in target encoding narration. \nfeature_fraction needs to be set to 1.\n\nThis does not use real\/fake data split. Like they have shown in other kernels we can break 0.901 without exploiting the test data split.\n\nCaution: This kernel takes several hours to run.\n\n\nAll credits to go to [Yimin Nie](https:\/\/www.kaggle.com\/chikenfeet) for posting the code. Upvote the [original discussion](https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/discussion\/88950) if you liked this."}}