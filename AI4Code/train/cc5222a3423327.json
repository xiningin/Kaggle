{"cell_type":{"97d7656f":"code","9261f3da":"code","4964c1e8":"code","89fcfae6":"code","261ec4bd":"code","656c6ea5":"code","aefba383":"code","41c49f61":"code","5456808b":"code","9ba98661":"code","b8a45592":"code","07d0a4ab":"code","14e2224a":"code","1ccc4307":"code","2461ae12":"code","8d3ca2e7":"code","ce1a9336":"code","c2c6c621":"code","aaacfbee":"code","8e630a8a":"code","dbb5903a":"code","9b07e7b0":"code","c0989491":"code","f3828066":"code","9cab0165":"markdown","10c27708":"markdown","14a37ac8":"markdown","49434f54":"markdown","7dd8d9ce":"markdown","054ff40b":"markdown","5896bf72":"markdown","3dda64d5":"markdown","2f96b968":"markdown","2f4e7071":"markdown","60130e29":"markdown","b3232512":"markdown","c423f6e2":"markdown","92bc57e5":"markdown","758cf3f2":"markdown","bfb49b5d":"markdown","8a195b5e":"markdown"},"source":{"97d7656f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","9261f3da":"data=pd.read_csv(\"..\/input\/voice.csv\")","4964c1e8":"data.head()","89fcfae6":"data.info()","261ec4bd":"data.label.value_counts()","656c6ea5":"data.label=[  1 if i==\"male\" else 0 for i in data.label]","aefba383":"data.head()","41c49f61":"data.info()","5456808b":"y=data.label.values\nx_data=data.drop([\"label\"],axis=1)","9ba98661":"y","b8a45592":"x_data.head()","07d0a4ab":"# normalization =(a-min(a))\/(max(a)-min(a))\n\nx=(x_data-np.min(x_data))\/(np.max(x_data)-np.min(x_data)).values","14e2224a":"x.head()","1ccc4307":"# create x_train, y_train, x_test, y_test arrays\nfrom sklearn.model_selection import train_test_split\n\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=42)\n\n# our features must be row in our matrix.\n\nx_train=x_train.T\nx_test=x_test.T\ny_train=y_train.T\ny_test=y_test.T\n\nprint(\"x_train: \", x_train.shape)\nprint(\"x_test: \", x_test.shape)\nprint(\"y_train: \", y_train.shape)\nprint(\"y_test: \", y_test.shape)\n","2461ae12":"# lets initialize parameters\n# So what we need is dimension, that is, the number of features as a parameter for our initialize method(def)\n# dimension=20\n#initial weights=0.01, initial bias=0\n\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w, b\n\n#sigmoid function\n\ndef sigmoid(z):\n    \n    y_head=1\/(1+np.exp(-z))\n    return y_head\n    ","8d3ca2e7":"# forward backward propagation\n\ndef forward_backward_propagation(w,b,x_train,y_train):\n    #forward propagation\n    z=np.dot(w.T,x_train)+b\n    y_head=sigmoid(z)\n    loss=-y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost=(np.sum(loss))\/x_train.shape[1] # x_train.shape[1] is for scaling\n    \n    #backward propagation\n    # In backward propagation we will use y_head that found in forward propagation\n    derivative_weight=(np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1] is for scaling\n    derivative_bias=np.sum(y_head-y_train)\/x_train.shape[1]                   # x_train.shape[1] is for scaling\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    \n    return cost,gradients","ce1a9336":"# Updating(learning) parameters\ndef update(w, b, x_train, y_train, learning_rate,number_of_iteration):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iteration):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list","c2c6c621":"#prediction\ndef predict(w,b,x_test):\n    # x_test is an input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","aaacfbee":"#Logistic Regression\n\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 20\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n\n    # Print train\/test Errors\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \nlogistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 100)","8e630a8a":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 300)","dbb5903a":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 1, num_iterations = 500)","9b07e7b0":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 2, num_iterations = 500)","c0989491":"logistic_regression(x_train, y_train, x_test, y_test,learning_rate = 3, num_iterations = 300)","f3828066":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression()\n\nlr.fit(x_train.T,y_train.T)\nprint(\"test accuracy {}\".format(lr.score(x_test.T,y_test.T)))\n","9cab0165":"As we see, we have the best result when  learning_rate = 3, num_iterations = 300 or  learning_rate = 2, num_iterations = 500.","10c27708":"<a id=\"4\"><\/a>\n# Train Test Split\nWe want to train our data by Linear Regression. But after getting our model, we need another data to test our model. So we will use **train_test_split**\nto control the acurracy of our model.\n* train_test_split says that take 80% of data to get the model and use 20% of data to control the model.","14a37ac8":"<a id=\"1\"><\/a>\n# Read Data","49434f54":"To get an appropriate model we need to normalize the values in x_data.","7dd8d9ce":"<a id=\"7\"><\/a>\n# Logistic Regression Algorithm\nWe make prediction.  Let's define logistic_regression function with learning_rate = 1, num_iterations = 100","054ff40b":"Now, we split our data with train_test_split and we will use x_train and y_train  for Linear Regression Model. \n* We will define the functions which we'll use in Linear Regression. \n*  First, we will define initial weights, initial bias and sigmoid function.","5896bf72":"<a id=\"3\"><\/a>\n# Determine Values\nFirst of all, we will determine x and y values for Logistic Regression.","3dda64d5":"Let's check what is y and x_data.\n* y is our output\n* the values in x_data will be coefficients of weights. ","2f96b968":"When updating parameters, we need to choose wisely learning rate. Learning rate should be neither too big nor too small.\n* Here, number_of_iterations and learning_rate are called as hyperparameter. That is, we need to set the values by hand. ","2f4e7071":"# INTRODUCTION\n\nIn this kernel, we will apply Logistic Regression procedure to \"Gender Recognition by Voice Data\"\n1. [Read Data](#1)\n1. [Logistic Regression](#2)\n    1. [Determine Values](#3)\n    1. [Train Test Split](#4)\n    1. [Forward Backward Propagation](#5)\n    1. [Prediction](#6)\n    1. [Logistic Regression Algorithm](#7)\n    1. [Logistic Regression with sklearn Library](#8)\n1. [Conclusion](#9) ","60130e29":"<a id=\"8\"><\/a>\n# Logistic Regression with sklearn Library\nAlso, we can use sklearn library to make Linear Regression.","b3232512":"Let's classify male and female as male=1 and female=0. ","c423f6e2":"Now, label is a binary output, and our data is convenient for Logistic Regression.","92bc57e5":"<a id=\"2\"><\/a>\n# Logistic Regression\n\nLogistic Regression is a classification algortihm. It is the simplest deep learning (neural network). \n\nFirst of all we want to train our data. So , we will use Computation Graph. Here are the components of Computation Graph.\n* parameters: weights and bias(w and b )\n* weights: coefficents of values of  each feature \n* z = ((w)^T)*x + b  or we can write  z = b + p1*w1 + p2*w2 + ... + p20*w20 for our data\n* p1, p2,..., p20: values of each feature in data (this will be meaningful after train test split method !)\n* y_head = sigmoid(z)\n    * Sigmoid function (which is called as activation function) makes z between 0 and 1 so that is a probabilitic result. \n    * Mathematical equation of sigmoid function is   $f(x)=\\displaystyle \\frac{1}{1+\\mathbb{e}^{-x}}$.\n","758cf3f2":"<a id=\"5\"><\/a>\n# Forward Backward Propagation\n\n**Forward propagation** is the all steps from features (x_train) to cost.\n*  z = ((w)^T)*x + b  or we can write  z = b + p1*w1 + p2*w2 + ... + p20*w20\n* Then compute y_head=sigmoid(z)\n* Calculate loss(error) function= $-(1-y).\\log(1-\\widetilde{y})-y.\\log(\\widetilde{y})$; ( actually we are finding y_head for each column in x_train matrix.)\n    * We are using loss function to decide whether our prediction is correct or not.    \n* Cost function=Summation of all loss functions.\n\n**Backward propagation** means that we are updating parameters in terms of the value of Cost Funciton. So we will use y_head that we found in forward propagation.\n\n*  Updating: There is a cost function(takes weight and bias). Take derivative of cost function according to weight and bias. Then multiply it with  \u03b1 (learning rate). Then update weight. \n    * w = w - learning_rate * gradients[\"derivative_bias\"]\n* We will do the same thing for bias. \n     *     i.e.  Take derivative of bias according to weight and bias. Then multiply it with  \u03b1 (learning rate). Then update bias.\n         * b = b - learning_rate * gradients[\"derivative_bias\"]        \n\n","bfb49b5d":"<a id=\"9\"><\/a>\n# Conclusion\n\nIf we write all the functions we need, then we get the best result for accuracy as  \n* test accuracy: 97.94952681388013 % when  learning_rate = 2, num_iterations = 500.\n* But if we use sklearn libray for Linear Regression our test accuracy is 0.9810725552050473.","8a195b5e":"<a id=\"6\"><\/a>\n# Prediction\nUp to here, we do:\n* prepare our data for LR\n* parameters: weights and bias\n* initialize parameters\n* sigmoid fuction\n* loss function\n* Cost function\n* updating parameters\n* Now let's predict.  In prediction step we have x_test as input "}}