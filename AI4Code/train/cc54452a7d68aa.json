{"cell_type":{"814df9f0":"code","bda914a9":"code","0d3275d5":"code","5f77ec9c":"code","d53016bf":"code","2f7bc108":"code","34572b22":"code","9a70467a":"code","91270819":"code","5de0b4dc":"code","01a27df7":"code","85b0938b":"code","557604ad":"code","48397e2c":"code","b1bec6a6":"code","47096e81":"code","3f22d897":"code","c07391f9":"code","6ceea80c":"markdown","ebfa0e42":"markdown","22269cae":"markdown","8e63e048":"markdown","207082f2":"markdown","579b6da9":"markdown","85385849":"markdown","067febbe":"markdown"},"source":{"814df9f0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.cluster import KMeans #K-means Clustering\nfrom sklearn.preprocessing import StandardScaler # Standardization\nfrom sklearn.pipeline import make_pipeline\nfrom scipy.cluster.hierarchy import linkage,dendrogram # Hierarchical Clustering\nimport matplotlib.pyplot as plt # For graph\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bda914a9":"data = pd.read_csv(\"\/kaggle\/input\/iris\/Iris.csv\")\ndata.head()","0d3275d5":"data.info()","5f77ec9c":"data.shape","d53016bf":"data.columns","2f7bc108":"plt.scatter(data.PetalWidthCm[data.Species == \"Iris-setosa\"] , data.PetalLengthCm[data.Species == \"Iris-setosa\"],color = \"red\" , alpha = 0.5)\nplt.scatter(data.PetalWidthCm[data.Species == \"Iris-versicolor\"] , data.PetalLengthCm[data.Species == \"Iris-versicolor\"],color = \"black\" , alpha = 0.5)\nplt.scatter(data.PetalWidthCm[data.Species == \"Iris-virginica\"] , data.PetalLengthCm[data.Species == \"Iris-virginica\"],color = \"cyan\" , alpha = 0.5)\nplt.grid()\nplt.xlabel(\"PetalWidthCm\")\nplt.ylabel(\"PetalLengthCm\")\nplt.show()","34572b22":"plt.scatter(data.PetalWidthCm , data.PetalLengthCm, color = \"green\")\nplt.xlabel(\"PetalWidthCm\")\nplt.ylabel(\"PetalLengthCm\")\nplt.grid()\nplt.show()","9a70467a":"# KMeans Clustering\ndata2 = data.loc[:,['PetalWidthCm','PetalLengthCm']]\nfrom sklearn.cluster import KMeans\nkmeans = KMeans(n_clusters = 2)\nkmeans.fit(data2)\nlabels = kmeans.predict(data2)\nplt.scatter(data['PetalLengthCm'],data['PetalWidthCm'],c = labels)\nplt.xlabel('PetalLengthCm')\nplt.xlabel('PetalWidthCm')\nplt.show()","91270819":"# cross tabulation table\ndf = pd.DataFrame({'labels':labels,\"Species\":data['Species']})\nct = pd.crosstab(df['labels'],df['Species'])\nprint(ct)","5de0b4dc":"# inertia\ninertia_list = np.empty(8)\nfor i in range(1,8):\n    kmeans = KMeans(n_clusters=i)\n    kmeans.fit(data2)\n    inertia_list[i] = kmeans.inertia_\nplt.plot(range(0,8),inertia_list,'-o')\nplt.xlabel('Number of cluster')\nplt.ylabel('Inertia')\nplt.show()","01a27df7":"data = pd.read_csv('\/kaggle\/input\/iris\/Iris.csv')\ndata3 = data.drop('Species',axis = 1)","85b0938b":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nscalar = StandardScaler()\nkmeans = KMeans(n_clusters = 3)\npipe = make_pipeline(scalar,kmeans)\npipe.fit(data3)\nlabels = pipe.predict(data3)\ndf = pd.DataFrame({'labels':labels,\"Species\":data['Species']})\nct = pd.crosstab(df['labels'],df['Species'])\nprint(ct)","557604ad":"plt.scatter(data['PetalLengthCm'],data['PetalWidthCm'],c = labels)\nplt.xlabel('PetalLengthCm')\nplt.xlabel('PetalWidthCm')\nplt.show()","48397e2c":"merg = linkage(data3.iloc[0:50,:],method = 'single')\ndendrogram(merg, leaf_rotation = 90, leaf_font_size = 6)\nplt.show()","b1bec6a6":"data3.head()","47096e81":"df_new = pd.DataFrame({\"PW\" : data3.PetalWidthCm , \"PL\" :data3.PetalLengthCm })\ndf_new.head(10)","3f22d897":"from sklearn.cluster import AgglomerativeClustering\nh_c = AgglomerativeClustering(n_clusters = 3 , affinity = \"euclidean\" , linkage = \"ward\")\ncluster2 = h_c.fit_predict(df_new)\ndf_new[\"cls\"] = cluster2","c07391f9":"plt.scatter(df_new.PW[df_new.cls == 0],df_new.PL[df_new.cls == 0] , color = \"green\" ,alpha = 0.5)\nplt.scatter(df_new.PW[df_new.cls == 1],df_new.PL[df_new.cls == 1] , color = \"red\" ,alpha = 0.5)\nplt.scatter(df_new.PW[df_new.cls == 2],df_new.PL[df_new.cls == 2] , color = \"cyan\" ,alpha = 0.5)\nplt.xlabel(\"Petal Weight\")\nplt.ylabel(\"Petal Length\")\nplt.show()","6ceea80c":"## Unsupervised Learning\n* Unsupervised learning: It uses data that has unlabeled and uncover hidden patterns from unlabeled data.\n* As you know iris data is labeled (supervised) data. It has target variables. In order to work on unsupervised learning, lets drop target variables and to visualize just consider PetalLengthCm, PetalWidthCm","ebfa0e42":"## HIERARCHY\n* vertical lines are clusters\n* height on dendogram: distance between merging cluster\n* method= 'single' : closest points of clusters","22269cae":"## Import Libraries","8e63e048":"This notebook was prepared with the help of DATAI Team Machine Learning Udemy Course and DATAI Team Machine Learning Tutorial for Beginners Notebook on Kaggle.","207082f2":"The new question is that we know how many class data includes, but what if number of class is unknow in data. This is kind of like hyperparameter in KNN or regressions.\n\n* inertia: how spread out the clusters are distance from each sample\n* lower inertia means more clusters\n* What is the best number of clusters ? *There are low inertia and not too many cluster trade off so we can choose elbow","579b6da9":"## KMEANS\n* Lets try our first unsupervised method that is KMeans Cluster\n* KMeans Cluster: The algorithm works iteratively to assign each data point to one of K groups based on the features that are provided. Data points are clustered based on feature similarity\n* KMeans(n_clusters = 2): n_clusters = 2 means that create 2 cluster","85385849":"## STANDARDIZATION\n* Standardizaton is important for both supervised and unsupervised learning\n* Do not forget standardization as pre-processing\n* As we already have visualized data so you got the idea. Now we can use all features for clustering.\n* We can use pipeline like supervised learning.","067febbe":"## EVALUATING OF CLUSTERING\n* We cluster data in two groups. Okey well is that correct clustering? In order to evaluate clustering we will use cross tabulation table.\n\n* There are two clusters that are 0 and 1\n* First class 0\n* Second class 1\n* The majority of two clusters are 0 and they are Iris-virginica and Iris-versicolor."}}