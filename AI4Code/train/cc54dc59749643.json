{"cell_type":{"e3455603":"code","20eaa61e":"code","711835e6":"code","6c93e54f":"code","70a3ed34":"code","45d86396":"code","54289e00":"code","4db6dfde":"code","d6a3012a":"code","6a0a6991":"code","1e764e5a":"code","9e9844a7":"code","815d3c79":"code","a72d010b":"code","8cfb5c28":"code","711c7df1":"code","f8d082a7":"code","240b8583":"code","d080c113":"code","b51bf862":"code","88fea851":"code","b82214fc":"code","3235667d":"code","860e0e0f":"code","59507c15":"code","4c9cf6c6":"code","0dcae085":"code","f4306d6c":"markdown","7933fe99":"markdown","3225a841":"markdown","cc98abb2":"markdown","b586c4e1":"markdown","c78aa886":"markdown","a1bd70cf":"markdown","51cae54e":"markdown","7c26e2ea":"markdown","e2e76b2b":"markdown","101481fd":"markdown","8087bfa9":"markdown","eb506dc4":"markdown","8dc3e602":"markdown","5c02c395":"markdown","2ba324cb":"markdown","dbd09f29":"markdown","028b67da":"markdown","7aa57ba7":"markdown","fbbeb8f0":"markdown","35654ddc":"markdown","dae97241":"markdown"},"source":{"e3455603":"import torch\nimport time\n\nn = 1000\na = torch.ones(n)\nb = torch.ones(n)\n","20eaa61e":"# define the Timer class to record time \nclass Timer(object):\n    \"\"\"Record multiple running times.\"\"\"\n    def __init__(self):\n        self.times = []\n        self.start()\n\n    def start(self):\n        # Start the timer\n        self.start_time = time.time()\n\n    def stop(self):\n        # Stop the timer and record the time in a list\n        self.times.append(time.time() - self.start_time)\n        return self.times[-1]\n\n    def avg(self):\n        # Return the average time\n        return sum(self.times)\/len(self.times)\n\n    def sum(self):\n        # Return the sum of time\n        return sum(self.times)","711835e6":"timer = Timer()\nc = torch.zeros(n)\nfor i in range(n):\n    c[i] = a[i] + b[i]\n'%.5f sec' % timer.stop()","6c93e54f":"timer.start()\nd = a + b\n'%.5f sec' % timer.stop()","70a3ed34":"%matplotlib inline\nimport torch\nfrom IPython import display\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport random\n\nprint(torch.__version__)","45d86396":"\nnum_inputs = 2\nnum_examples = 1000\ntrue_w = [2, -3.4]\ntrue_b = 4.2\nfeatures = torch.randn(num_examples, num_inputs,\n                      dtype=torch.float32)\nlabels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\nlabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()),\n                       dtype=torch.float32)","54289e00":"def use_svg_display():\n    # display in vector graph\n    display.set_matplotlib_formats('svg')\n\ndef set_figsize(figsize=(3.5, 2.5)):\n    use_svg_display()\n    # set the size of figure\n    plt.rcParams['figure.figsize'] = figsize\n\nset_figsize()\nplt.scatter(features[:, 1].numpy(), labels.numpy(), 1);","4db6dfde":"def data_iter(batch_size, features, labels):\n    num_examples = len(features)\n    indices = list(range(num_examples))\n    random.shuffle(indices)  # random read 10 samples\n    for i in range(0, num_examples, batch_size):\n        j = torch.LongTensor(indices[i: min(i + batch_size, num_examples)]) # the last time may be not enough for a whole batch\n        yield  features.index_select(0, j), labels.index_select(0, j)","d6a3012a":"batch_size = 10\n\nfor X, y in data_iter(batch_size, features, labels):\n    print(X, '\\n', y)\n    break","6a0a6991":"w = torch.tensor(np.random.normal(0, 0.01, (num_inputs, 1)), dtype=torch.float32)\nb = torch.zeros(1, dtype=torch.float32)\n\nw.requires_grad_(requires_grad=True)\nb.requires_grad_(requires_grad=True)","1e764e5a":"def linreg(X, w, b):\n    return torch.mm(X, w) + b","9e9844a7":"def squared_loss(y_hat, y): \n    return (y_hat - y.view(y_hat.size())) ** 2 \/ 2","815d3c79":"def sgd(params, lr, batch_size): \n    for param in params:\n        param.data -= lr * param.grad \/ batch_size # ues .data to operate param without gradient track","a72d010b":"lr = 0.03\nnum_epochs = 5\nnet = linreg\nloss = squared_loss\n\nfor epoch in range(num_epochs):  # training repeats num_epochs times\n    # in each epoch, all the samples in dataset will be used once\n    # X is the feature and y is the label of a batch sample\n    for X, y in data_iter(batch_size, features, labels):\n        l = loss(net(X, w, b), y).sum()  # l is the loss of the batch sample\n        l.backward()  # calculate the gradient of batch sample loss \n        sgd([w, b], lr, batch_size)  # using small batch random gradient descent to iter model parameters\n        \n        # reset parameter gradient\n        w.grad.data.zero_()\n        b.grad.data.zero_()\n    train_l = loss(net(features, w, b), labels)\n    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().item()))","8cfb5c28":"w, true_w, b, true_b","711c7df1":"import torch\nfrom torch import nn\nimport numpy as np\ntorch.manual_seed(1)\n\nprint(torch.__version__)\ntorch.set_default_tensor_type('torch.FloatTensor')","f8d082a7":"num_inputs = 2\nnum_examples = 1000\ntrue_w = [2, -3.4]\ntrue_b = 4.2\nfeatures = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float)\nlabels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b\nlabels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)","240b8583":"import torch.utils.data as Data\n\nbatch_size = 10\n\n# combine featues and labels of dataset\ndataset = Data.TensorDataset(features, labels)\n\n# put dataset into DataLoader\ndata_iter = Data.DataLoader(\n    dataset=dataset,            # torch TensorDataset format\n    batch_size=batch_size,      # mini batch size\n    shuffle=True,               # whether shuffle the data or not\n    num_workers=2,              # read data in multithreading\n)","d080c113":"for X, y in data_iter:\n    print(X, '\\n', y)\n    break","b51bf862":"class LinearNet(nn.Module):\n    def __init__(self, n_feature):\n        super(LinearNet, self).__init__()      # call father function to init \n        self.linear = nn.Linear(n_feature, 1)  # function prototype: `torch.nn.Linear(in_features, out_features, bias=True)`\n\n    def forward(self, x):\n        y = self.linear(x)\n        return y\n    \nnet = LinearNet(num_inputs)\nprint(net)","88fea851":"\n# method one\nnet = nn.Sequential(\n    nn.Linear(num_inputs, 1)\n    # other layers can be added here\n    )\n\n# method two\nnet = nn.Sequential()\nnet.add_module('linear', nn.Linear(num_inputs, 1))\n# net.add_module ......\n\n# method three\nfrom collections import OrderedDict\nnet = nn.Sequential(OrderedDict([\n          ('linear', nn.Linear(num_inputs, 1))\n          # ......\n        ]))\n\nprint(net)\nprint(net[0])","b82214fc":"from torch.nn import init\n\ninit.normal_(net[0].weight, mean=0.0, std=0.01)\ninit.constant_(net[0].bias, val=0.0)  # or you can use `net[0].bias.data.fill_(0)` to modify it directly","3235667d":"for param in net.parameters():\n    print(param)","860e0e0f":"loss = nn.MSELoss()    # nn built-in squared loss function\n                       # function prototype: `torch.nn.MSELoss(size_average=None, reduce=None, reduction='mean')`","59507c15":"import torch.optim as optim\n\noptimizer = optim.SGD(net.parameters(), lr=0.03)   # built-in random gradient descent function\nprint(optimizer)  # function prototype: `torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)`","4c9cf6c6":"num_epochs = 3\nfor epoch in range(1, num_epochs + 1):\n    for X, y in data_iter:\n        output = net(X)\n        l = loss(output, y.view(-1, 1))\n        optimizer.zero_grad() # reset gradient, equal to net.zero_grad()\n        l.backward()\n        optimizer.step()\n    print('epoch %d, loss: %f' % (epoch, l.item()))","0dcae085":"dense = net[0]\nprint(true_w, dense.weight.data)\nprint(true_b, dense.bias.data)","f4306d6c":"### Initializing the Model Parameters","7933fe99":"### Generating the Dataset","3225a841":"# Linear Regression\ncontains:\n1. basic elements of linear regression\n2. implementation from zero\n3. concise implementation in pytorch","cc98abb2":"### Generating the Dataset","b586c4e1":"Now we can benchmark the workloads. First, we add them, one coordinate at a time, using a for loop.","c78aa886":"### Training","a1bd70cf":"## Concise Implementation in PyTorch\n","51cae54e":"### Defining the Model","7c26e2ea":"You probably noticed that the second method is dramatically faster than the first. Vectorizing code often yields order-of-magnitude speedups. ","e2e76b2b":"### Training","101481fd":"## Basic Elements of Linear Regression\n\n### Model\nTo keep things simple, we will start with running example in which we consider the problem of estimating the price of a house (e.g. in dollars) based on area (e.g. in square feet) and age (e.g. in years). More formally, the assumption of linearity suggests that our model can be expressed in the following form:\n\n$$\\mathrm{price} = w_{\\mathrm{area}} \\cdot \\mathrm{area} + w_{\\mathrm{age}} \\cdot \\mathrm{age} + b$$\n\n\n### Data Set\nThe first thing that we need is training data. Sticking with our running example, we'll need some collection of examples for which we know both the actual selling price of each house as well as their corresponding area and age. Our goal is to identify model parameters that minimize the error between the predicted price and the real price.   \nIn the terminology of machine learning, the data set is called a training data or training set, a house (often a house and its price) here comprises one sample, and its actual selling price is called a label. The two factors used to predict the label are called features or covariates.\n\n### Loss Function\nIn model training, we need to measure the error between the predicted value and the real value of the price. Usually, we will choose a non-negative number as the error. The smaller the value, the smaller the error. A common choice is the square function. For given parameters $\\mathbf{w}$ and $b$, we can express the error of our prediction on a given a sample as follows:\n\n$$l^{(i)}(\\mathbf{w}, b) = \\frac{1}{2} \\left(\\hat{y}^{(i)} - y^{(i)}\\right)^2,$$\n\n$$L(\\mathbf{w}, b) =\\frac{1}{n}\\sum_{i=1}^n l^{(i)}(\\mathbf{w}, b) =\\frac{1}{n} \\sum_{i=1}^n \\frac{1}{2}\\left(\\mathbf{w}^\\top \\mathbf{x}^{(i)} + b - y^{(i)}\\right)^2.$$\n\n### Optimization - Gradient Descent\nLinear regression happens to be an unusually simple optimization problem. Unlike nearly every other model that we will encounter in this book, linear regression can be solved easily with a simple formula, yielding a global optimum. \nEven in cases where we cannot solve the models analytically, and even when the loss surfaces are high-dimensional and nonconvex, it turns out that we can still train models effectively in practice. \nThe key technique for optimizing nearly any deep learning model, and which we will call upon throughout this book, consists of iteratively reducing the error by updating the parameters in the direction that incrementally lowers the loss function.   \n$$(\\mathbf{w},b) \\leftarrow (\\mathbf{w},b) - \\frac{\\eta}{|\\mathcal{B}|} \\sum_{i \\in \\mathcal{B}} \\partial_{(\\mathbf{w},b)} l^{(i)}(\\mathbf{w},b)$$  \nTo summarize, steps of the algorithm are the following: (i) we initialize the values of the model parameters, typically at random; (ii) we iterate over the data many times, updating the parameters in each by moving the parameters in the direction of the negative gradient, as calculated on a random minibatch of data.","8087bfa9":"### Defining the Loss Function","eb506dc4":"## Vectorization for Speed\nWhen training our models, we typically want to process whole minibatches of examples simultaneously. Doing this efficiently requires that we vectorize the calculations and leverage fast linear algebra libraries rather than writing costly for-loops in Python.\n\nTo illustrate why this matters so much, we can consider two methods for adding vectors.\n1. loop over vectors with Python for loop\n2. rely on a single call to torch","8dc3e602":"### Reading the Dataset","5c02c395":"### Initializing Model Parameters","2ba324cb":"### Defining the Optimization Alogrithm","dbd09f29":"## Implementation from zero\n\n","028b67da":"### Defining the Model","7aa57ba7":"Alternatively, we rely on torch to compute the elementwise sum:\n","fbbeb8f0":"### Reading the Dataset","35654ddc":"### Defining the Optimization Alogrithm","dae97241":"### Defining the Loss Function"}}