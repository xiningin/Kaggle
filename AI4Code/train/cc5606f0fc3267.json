{"cell_type":{"2f1b564f":"code","59dec120":"code","1773c114":"code","5d88d92d":"code","a86a4d40":"code","8afb9c8c":"code","6ac85334":"code","61f95afc":"code","111be5ad":"code","45331060":"code","ee9114f2":"code","96486755":"markdown","34878ac8":"markdown","2a48163c":"markdown","3d63831c":"markdown","9935f816":"markdown","a688455e":"markdown","2aff9316":"markdown","f64be852":"markdown","fb409022":"markdown","5e1f54dd":"markdown","c205dd92":"markdown","a20fadc4":"markdown"},"source":{"2f1b564f":"import pandas as pd\nimport numpy as np\n\n#Data Visualization\nimport matplotlib.pyplot as plt\n\n#Text Color\nfrom termcolor import colored\n\n#Train Test Split\nfrom sklearn.model_selection import train_test_split\n\n#Model Evaluation\nfrom sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score\nfrom mlxtend.plotting import plot_confusion_matrix\n\n#Deep Learning\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Input, Dropout\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow_hub import KerasLayer","59dec120":"#Load Data\ntrain = pd.read_csv('..\/input\/ag-news-classification-dataset\/train.csv')\ntest = pd.read_csv('..\/input\/ag-news-classification-dataset\/test.csv')\n\n#Set Column Names \ntrain.columns = ['ClassIndex', 'Title', 'Description']\ntest.columns = ['ClassIndex', 'Title', 'Description']\n\n#Combine Title and Description\nX_train = train['Title'] + \" \" + train['Description'] # Combine title and description (better accuracy than using them as separate features)\ny_train = train['ClassIndex'].apply(lambda x: x-1).values # Class labels need to begin from 0\n\nX_test = test['Title'] + \" \" + test['Description'] # Combine title and description (better accuracy than using them as separate features)\ny_test = test['ClassIndex'].apply(lambda x: x-1).values # Class labels need to begin from 0\n\ntrain.head()","1773c114":"X_train.shape,X_test.shape","5d88d92d":"in_layer = Input(shape=[], dtype=tf.string)\nembedding_layer = KerasLayer('..\/input\/universalsentenceencodermodels\/universal-sentence-encoder-models\/use',\n                    trainable=True)(in_layer)\nd1_layer = Dense(256,activation = 'relu',\n    kernel_regularizer=tf.keras.regularizers.L2(0.01))(embedding_layer)\ndropout = Dropout(0.2)(d1_layer)\nclassifier = Dense(4,activation = 'softmax')(dropout)\n\nmodel = Model(in_layer,classifier)\n\nmodel.summary()","a86a4d40":"callbacks = [\n    EarlyStopping(     #EarlyStopping is used to stop at the epoch where val_accuracy does not improve significantly\n        monitor='val_loss',\n        min_delta=1e-4,\n        patience=4,\n        verbose=1\n    ),\n    ModelCheckpoint(\n        filepath='weights.h5',\n        monitor='val_loss', \n        mode='min', \n        save_best_only=True,\n        save_weights_only=True,\n        verbose=1\n    )\n]","8afb9c8c":"model.compile(loss='sparse_categorical_crossentropy', #Sparse Categorical Crossentropy Loss because data is not one-hot encoded\n              optimizer='adam', \n              metrics=['accuracy']) \n\nhistory = model.fit(X_train, \n          y_train, \n          batch_size=32, \n          validation_data=(X_test, y_test), \n          epochs=20, \n          callbacks=callbacks)","6ac85334":"model.load_weights('weights.h5')\nmodel.save('model.hdf5')","61f95afc":"## Thanks to Ishan Dutta for his creativity\ndef modelDemo(news_text):\n\n  #News Labels\n  labels = ['World News', 'Sports News', 'Business News', 'Science-Technology News']\n\n  test_preds = [labels[np.argmax(i)] for i in model.predict(news_text)]\n\n  for news, label in zip(news_text, test_preds):\n      # print('{} - {}'.format(news, label))\n      print('{} - {}\\n'.format(colored(news, 'yellow'), colored(label, 'blue')))","111be5ad":"modelDemo(['New evidence of virus risks from wildlife trade',\n          'Coronavirus: Bank pumps \u00a3100bn into UK economy to aid recovery',\n          'Trump\\'s bid to end Obama-era immigration policy ruled unlawful',\n          'David Luiz\u2019s future with Arsenal to be decided this week'])","45331060":"labels = ['World News', 'Sports News', 'Business News', 'Science-Technology News']\npreds = [np.argmax(i) for i in model.predict(X_test)]\ncm  = confusion_matrix(y_test, preds)\nplt.figure()\nplot_confusion_matrix(cm, figsize=(16,12), hide_ticks=True, cmap=plt.cm.Blues)\nplt.xticks(range(4), labels, fontsize=12)\nplt.yticks(range(4), labels, fontsize=12)\nplt.show()","ee9114f2":"print(\"Recall of the model is {:.2f}\".format(recall_score(y_test, preds, average='micro')))\nprint(\"Precision of the model is {:.2f}\".format(precision_score(y_test, preds, average='micro')))\nprint(\"Accuracy of the model is {:.2f}\".format(accuracy_score(y_test, preds)))","96486755":"# Evaluation","34878ac8":"## Universal Sentence Encoder\nIf you have been working with any kind of text processing, you would be aware of word embeddings using any of the popular implementations like GloVe, word2vec, fastText. These embeddings are only useful for word level operations, sometimes we would want to explore embeddings for sentences, or generally, greater-than-word length text. In this notebook we will explore sentence encoding with universal-sentence-encoder. This module is part of tensorflow-hub. We will be using the pre-trained model to create embeddings for our sentences. The embeddings vector is 512 length, irrespective of the length of the input.\n![](https:\/\/www.gstatic.com\/aihub\/tfhub\/universal-sentence-encoder\/example-similarity.png)\nThe Universal Sentence Encoder(USE) encodes text into high dimensional vectors that can be used for text classification, semantic similarity, clustering, and other natural language tasks. It comes with two variations i.e. one trained with Transformer encoder and other trained with Deep Averaging Network (DAN). The two have a trade-off of accuracy and computational resource requirement. While the one with Transformer encoder has higher accuracy, it is computationally more intensive. The one with DAN encoding is computationally less expensive and with little lower accuracy. For a concise understanding of it's training process and use-cases, I would recommend you to read [the paper](https:\/\/arxiv.org\/pdf\/1803.11175.pdf).","2a48163c":"# Checkout the demo","3d63831c":"> This notebook is inspired by the works of [Ishan Dutta](https:\/\/www.kaggle.com\/ishandutta). \nYou might like his [notebook](https:\/\/www.kaggle.com\/ishandutta\/ag-news-classification-lstm),\nwhere he elaborated theoretical and practical information about Bi-directional LSTM. \nUpvote my notebook if you find it useful","9935f816":"# Network Architecture","a688455e":"# My Work\n![image.png](attachment:image.png)\nIn this notebook I introduce you to the transfer learning methods in NLP. In most of my research I use transfer learning methods(USE, BERT, GloVe etc.) for clustering unstructured textual data, whereas their use-cases could be many like for classification tasks and similar. Let me know if we can use them in other things in the comment.","2aff9316":"# Dataset Loading","f64be852":"![image.png](attachment:image.png)\nI imported USE as a Keras Layer using Tensorflow APIs, and instead of preprocessing text, which we don't need to do as USE converts them into a global representation, I made the input layer to take string variables. Since I don't need to train USE(it's pretrained), I add fully connected layers to the network with a final classifier of 4 nodes.","fb409022":"# Importing Packages","5e1f54dd":"## Training the Classifier\n![](https:\/\/beta.techcrunch.com\/wp-content\/uploads\/2017\/04\/neural_networks_fully_connected_layers_gumgum1.gif)\n\nWe'll train the FC layer in the network including the classifier layer, and also save network weights with best validation accuracy using Tensorflow callback.","c205dd92":"### Origin\nAG-Corpus is a collection of more than 1 million news articles from more than 2000 news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. \n\nThe dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non-commercial activity.\n\nThe AG's news topic classification dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the dataset above. It is used as a text classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\n\n### Description\nThe AG's news topic classification dataset is constructed by choosing 4 largest classes from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600.\nThe files train.csv and test.csv contain all the training samples as comma-sparated values. There are 3 columns in them, corresponding to\n- class index (1 to 4), \n- title and \n- description.\n\nThe class index ranging from 1 to 4 denotes to **World News**, **Sports News**, **Business News** and **Science-Technology News**, respectively.\nThe title and description are escaped using double quotes (\"), and any internal double quote is escaped by 2 double quotes (\"\"). ","a20fadc4":"# About the dataset\n![image.png](https:\/\/i1.adis.ws\/i\/canon\/Digital-Newspaper-Printing-hero-1280x768?w=100%&aspect=16:9&qlt=80&sm=aspect&fmt=jpg&fmt.options=interlaced&fmt=jpg&fmt.options=interlaced&bg=rgb(255,255,255))"}}