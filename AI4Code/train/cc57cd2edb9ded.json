{"cell_type":{"35c4dfb4":"code","32c937d9":"code","adc53668":"code","4afa8b4f":"code","2c4085c9":"code","d2ee0a39":"code","39081d0d":"code","89a1b261":"code","419bafd7":"code","175a39f4":"code","8c55c964":"code","ab58dfd3":"code","1c655ecc":"markdown","0c565d47":"markdown","8fc992ae":"markdown","ad6f32cc":"markdown","6f2bdc0e":"markdown","3572410d":"markdown"},"source":{"35c4dfb4":"import numpy as np \nimport matplotlib.pyplot as plt \nimport os, time, pickle, json \nfrom glob import glob \nfrom PIL import Image\nimport cv2 \nfrom typing import List, Tuple, Dict\nfrom statistics import mean \nfrom tqdm import tqdm \n\nimport torch \nimport torch.nn as nn \nfrom torchvision import transforms \nfrom torchvision.utils import save_image\nfrom torch.utils.data import DataLoader ","32c937d9":"MEAN = (0.5, 0.5, 0.5,)\nSTD = (0.5, 0.5, 0.5,)\nRESIZE = 64\n\ndef read_path(filepath) -> List[str]:\n    root_path = \"..\/input\/pix2pix-dataset\/maps\/maps\"\n    path = os.path.join(root_path, filepath)\n    dataset = []\n    for p in glob(path+\"\/\"+\"*.jpg\"):\n        dataset.append(p)\n    return dataset \n\n\nclass Transform():\n    def __init__(self, resize=RESIZE, mean=MEAN, std=STD):\n        self.data_transform = transforms.Compose([\n            transforms.Resize((resize, resize)), \n            transforms.ToTensor(),\n            transforms.Normalize(mean, std)\n        ])\n        \n    def __call__(self, img: Image.Image):\n        return self.data_transform(img)\n\n    \nclass Dataset(object):\n    def __init__(self, files: List[str]):\n        self.files = files \n        self.trasformer = Transform()\n        \n    def _separate(self, img) -> Tuple[Image.Image, Image.Image]:\n        img = np.array(img, dtype=np.uint8)\n        h, w, _ = img.shape\n        w = int(w\/2)\n        return Image.fromarray(img[:, w:, :]), Image.fromarray(img[:, :w, :])\n    \n    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n        img = Image.open(self.files[idx])\n        input, output = self._separate(img)\n        input_tensor = self.trasformer(input)\n        output_tensor = self.trasformer(output)\n        return input_tensor, output_tensor \n    \n    def __len__(self):\n        return len(self.files)\n    \ndef show_img_sample(img: torch.Tensor, img1: torch.Tensor):\n    fig, axes = plt.subplots(1, 2, figsize=(15, 8))\n    ax = axes.ravel()\n    ax[0].imshow(img.permute(1, 2, 0))\n    ax[0].set_xticks([])\n    ax[0].set_yticks([])\n    ax[0].set_title(\"input image\", c=\"g\")\n    ax[1].imshow(img1.permute(1, 2, 0))\n    ax[1].set_xticks([])\n    ax[1].set_yticks([])\n    ax[1].set_title(\"label image\", c=\"g\")\n    plt.subplots_adjust(wspace=0, hspace=0)\n    plt.show()\n        ","adc53668":"train = read_path(\"train\")\nval = read_path(\"val\")\ntrain_ds = Dataset(train)\nval_ds = Dataset(val)","4afa8b4f":"show_img_sample(train_ds.__getitem__(1)[0], train_ds.__getitem__(1)[1])","2c4085c9":"BATCH_SIZE = 16\ndevice = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\ntorch.manual_seed(0)\nnp.random.seed(0)\n\ntrain_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\nval_dl = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, drop_last=False)","d2ee0a39":"class Generator(nn.Module):\n    def __init__(self):\n        super(Generator, self).__init__()\n        self.enc1 = self.conv2Relu(3, 32, 5)\n        self.enc2 = self.conv2Relu(32, 64, pool_size=4)\n        self.enc3 = self.conv2Relu(64, 128, pool_size=2)\n        self.enc4 = self.conv2Relu(128, 256, pool_size=2)\n        \n        self.dec1 = self.deconv2Relu(256, 128, pool_size=2)\n        self.dec2 = self.deconv2Relu(128+128, 64, pool_size=2)\n        self.dec3 = self.deconv2Relu(64+64, 32, pool_size=4)\n        self.dec4 = nn.Sequential(\n            nn.Conv2d(32+32, 3, 5, padding=2), \n            nn.Tanh()\n        )\n        \n    def conv2Relu(self, in_c, out_c, kernel_size=3, pool_size=None):\n        layer = []\n        if pool_size:\n            # Down width and height\n            layer.append(nn.AvgPool2d(pool_size))\n        # Up channel size \n        layer.append(nn.Conv2d(in_c, out_c, kernel_size, padding=(kernel_size-1)\/\/2))\n        layer.append(nn.LeakyReLU(0.2, inplace=True))\n        layer.append(nn.BatchNorm2d(out_c))\n        layer.append(nn.ReLU(inplace=True))\n        return nn.Sequential(*layer)\n    \n    def deconv2Relu(self, in_c, out_c, kernel_size=3, stride=1, pool_size=None):\n        layer = []\n        if pool_size:\n            # Up width and height\n            layer.append(nn.UpsamplingNearest2d(scale_factor=pool_size))\n        # Down channel size \n        layer.append(nn.Conv2d(in_c, out_c, kernel_size, stride, padding=1))\n        layer.append(nn.BatchNorm2d(out_c))\n        layer.append(nn.ReLU(inplace=True))\n        return nn.Sequential(*layer)\n    \n    def forward(self, x):\n        x1 = self.enc1(x)\n        x2 = self.enc2(x1)\n        x3 = self.enc3(x2)\n        x4 = self.enc4(x3) # (b, 256, 4, 4)\n        \n        out = self.dec1(x4)\n        out = self.dec2(torch.cat((out, x3), dim=1)) # concat channel \n        out = self.dec3(torch.cat((out, x2), dim=1))\n        out = self.dec4(torch.cat((out, x1), dim=1)) # (b, 3, 64, 64)\n        return out ","39081d0d":"class Discriminator(nn.Module):\n    def __init__(self):\n        super(Discriminator, self).__init__()\n        self.layer1 = self.conv2relu(6, 16, 5, cnt=1)\n        self.layer2 = self.conv2relu(16, 32, pool_size=4)\n        self.layer3 = self.conv2relu(32, 64, pool_size=2)\n        self.layer4 = self.conv2relu(64, 128, pool_size=2)\n        self.layer5 = self.conv2relu(128, 256, pool_size=2)\n        self.layer6 = nn.Conv2d(256, 1, kernel_size=1)\n        \n    def conv2relu(self, in_c, out_c, kernel_size=3, pool_size=None, cnt=2):\n        layer = []\n        for i in range(cnt):\n            if i == 0 and pool_size != None:\n                # Down width and height \n                layer.append(nn.AvgPool2d(pool_size))\n            # Down channel size \n            layer.append(nn.Conv2d(in_c if i == 0 else out_c, \n                                   out_c,\n                                   kernel_size,\n                                   padding=(kernel_size-1)\/\/2))\n            layer.append(nn.BatchNorm2d(out_c))\n            layer.append(nn.LeakyReLU(0.2, inplace=True))\n        return nn.Sequential(*layer)\n        \n    def forward(self, x, x1):\n        x = torch.cat((x, x1), dim=1)\n        out = self.layer5(self.layer4(self.layer3(self.layer2(self.layer1(x)))))\n        return self.layer6(out) # (b, 1, 2, 2)\n        ","89a1b261":"def train_fn(train_dl, G, D, criterion_bce, criterion_mae, optimizer_g, optimizer_d):\n    G.train()\n    D.train()\n    LAMBDA = 100.0\n    total_loss_g, total_loss_d = [], []\n    for i, (input_img, real_img) in enumerate(tqdm(train_dl)):\n        input_img = input_img.to(device)\n        real_img = real_img.to(device)\n        \n        real_label = torch.ones(input_img.size()[0], 1, 2, 2)\n        fake_label = torch.zeros(input_img.size()[0], 1, 2, 2)\n        # Generator \n        fake_img = G(input_img)\n        fake_img_ = fake_img.detach() # commonly using \n        out_fake = D(fake_img, input_img)\n        loss_g_bce = criterion_bce(out_fake, real_label) # binaryCrossEntropy\n        loss_g_mae = criterion_mae(fake_img, real_img) # MSELoss\n        loss_g = loss_g_bce + LAMBDA * loss_g_mae \n        total_loss_g.append(loss_g.item())\n        \n        optimizer_g.zero_grad()\n        optimizer_d.zero_grad()\n        loss_g.backward(retain_graph=True)\n        optimizer_g.step()\n        # Discriminator\n        out_real = D(real_img, input_img)\n        loss_d_real = criterion_bce(out_real, real_label)\n        out_fake = D(fake_img_, input_img)\n        loss_d_fake = criterion_bce(out_fake, fake_label)\n        loss_d = loss_d_real + loss_d_fake \n        total_loss_d.append(loss_d.item())\n        \n        optimizer_g.zero_grad()\n        optimizer_d.zero_grad()\n        loss_d.backward()\n        optimizer_d.step()\n    return mean(total_loss_g), mean(total_loss_d), fake_img.detach().cpu() \n\n\ndef saving_img(fake_img, e):\n    os.makedirs(\"generated\", exist_ok=True)\n    save_image(fake_img, f\"generated\/fake{str(e)}.png\", range=(-1.0, 1.0), normalize=True)\n    \ndef saving_logs(result):\n    with open(\"train.pkl\", \"wb\") as f:\n        pickle.dump([result], f)\n        \ndef saving_model(D, G, e):\n    os.makedirs(\"weight\", exist_ok=True)\n    torch.save(G.state_dict(), f\"weight\/G{str(e+1)}.pth\")\n    torch.save(D.state_dict(), f\"weight\/D{str(e+1)}.pth\")\n        \ndef show_losses(g, d):\n    fig, axes = plt.subplots(1, 2, figsize=(14,6))\n    ax = axes.ravel()\n    ax[0].plot(np.arange(len(g)).tolist(), g)\n    ax[0].set_title(\"Generator Loss\")\n    ax[1].plot(np.arange(len(d)).tolist(), d)\n    ax[1].set_title(\"Discriminator Loss\")\n    plt.show()\n\n\ndef train_loop(train_dl, G, D, num_epoch, lr=0.0002, betas=(0.5, 0.999)):\n    G.to(device)\n    D.to(device)\n    optimizer_g = torch.optim.Adam(G.parameters(), lr=lr, betas=betas)\n    optimizer_d = torch.optim.Adam(D.parameters(), lr=lr, betas=betas)\n    criterion_mae = nn.L1Loss()\n    criterion_bce = nn.BCEWithLogitsLoss()\n    total_loss_d, total_loss_g = [], []\n    result = {}\n    \n    for e in range(num_epoch):\n        loss_g, loss_d, fake_img = train_fn(train_dl, G, D, criterion_bce, criterion_mae, optimizer_g, optimizer_d)\n        total_loss_d.append(loss_d)\n        total_loss_g.append(loss_g)\n        saving_img(fake_img, e+1)\n        \n        if e%10 == 0:\n            saving_model(D, G, e)\n    try:\n        result[\"G\"] = total_loss_d \n        result[\"D\"] = total_loss_g \n        saving_logs(result)\n        show_losses(total_loss_g, total_loss_d)\n        saving_model(D, G, e)\n        print(\"successfully save model\")\n    finally:\n        return G, D \n    ","419bafd7":"G = Generator()\nD = Discriminator()\nEPOCH = 10 \ntrained_G, trained_D = train_loop(train_dl, G, D, EPOCH)","175a39f4":"def load_model(name):\n    G = Generator()\n    G.load_state_dict(torch.load(f\"weight\/G{name}.pth\", map_location={\"cuda:0\": \"cpu\"}))\n    G.eval()\n    return G.to(device)\n\ndef train_show_img(name, G):\n#     G = load_model(name)\n    root = \"generated\"\n    fig, axes = plt.subplots(int(name), 1, figsize=(12, 18))\n    ax = axes.ravel()\n    for i in range(int(name)):\n        filename = os.path.join(root, f\"fake{str(i+1)}.png\")\n        ax[i].imshow(Image.open(filename))\n        ax[i].set_xticks([])\n        ax[i].set_yticks([])\n\ndef de_norm(img):\n    img_ = img.mul(torch.FloatTensor(STD).view(3, 1, 1))\n    img_ = img_.add(torch.FloatTensor(MEAN).view(3, 1, 1)).detach().numpy()\n    img_ = np.transpose(img_, (1, 2, 0))\n    return img_ \n\ndef evaluate(val_dl, name, G):\n    with torch.no_grad():\n        fig, axes = plt.subplots(6, 8, figsize=(12, 12))\n        ax = axes.ravel()\n#         G = load_model(name)\n        for input_img, real_img in tqdm(val_dl):\n            input_img = input_img.to(device)\n            real_img = real_img.to(device)\n            \n            fake_img = G(input_img)\n            batch_size = input_img.size()[0]\n            batch_size_2 = batch_size * 2 \n            \n            for i in range(batch_size):\n                ax[i].imshow(input_img[i].permute(1, 2, 0))\n                ax[i+batch_size].imshow(de_norm(real_img[i]))\n                ax[i+batch_size_2].imshow(de_norm(fake_img[i]))\n                ax[i].set_xticks([])\n                ax[i].set_yticks([])\n                ax[i+batch_size].set_xticks([])\n                ax[i+batch_size].set_yticks([])\n                ax[i+batch_size_2].set_xticks([])\n                ax[i+batch_size_2].set_yticks([])\n                if i == 0:\n                    ax[i].set_ylabel(\"Input Image\", c=\"g\")\n                    ax[i+batch_size].set_ylabel(\"Real Image\", c=\"g\")\n                    ax[i+batch_size_2].set_ylabel(\"Generated Image\", c=\"r\")\n            plt.subplots_adjust(wspace=0, hspace=0)\n            break   ","8c55c964":"train_show_img(5, trained_G)","ab58dfd3":"evaluate(val_dl, 5, trained_G)","1c655ecc":"### evaluate\n---","0c565d47":"## Model \n---","8fc992ae":"## preprocess\n---","ad6f32cc":"### Train\n---","6f2bdc0e":"#### Discriminator\n(batch, 3, 64, 64) *2 -> (batch, 1, 2, 2)","3572410d":"#### Generator\n(batch, 3, 64, 64) -> (batch, 3, 64, 64)"}}