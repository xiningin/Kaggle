{"cell_type":{"bc14ae8e":"code","02644ffb":"code","7c25ddcc":"code","c644118e":"code","836b302a":"code","d4ff64e9":"code","83047c40":"code","14ffc8a3":"code","ea90e0b3":"code","b5f11366":"code","c2018a0a":"code","720617ba":"code","e92ae743":"code","5292abf6":"code","d15c9833":"code","dadf0014":"code","089749b1":"code","63fa8099":"code","a2ce9ed2":"code","4ef6bf26":"code","67d9799f":"code","84ae11d5":"code","4b8fac34":"code","ec0d3ba4":"code","2452c89e":"code","f85a71fe":"code","f8c7a1b0":"code","7da78e5b":"code","6471648d":"code","2616cc5c":"code","59f10e49":"code","e4231ccc":"code","3485a28f":"code","1e824584":"code","b4fc8c10":"code","0614ebde":"code","91cc6510":"code","0b6fc927":"code","5f309077":"code","8d43d6ce":"code","dc521178":"code","4e2758e4":"code","b6dd130e":"code","b65915e7":"code","bc170801":"code","c5ae7d78":"code","46a4716a":"code","345c066a":"code","c3ce8b9b":"code","5625e965":"code","2cf69298":"code","851ab9bc":"code","c05494a8":"code","9000d472":"code","c32e54d7":"code","3e580d0e":"code","923172ce":"code","4d021494":"code","dfe409f6":"code","dc0a5ff4":"code","d51d9daa":"code","57cb5e52":"code","765659a0":"code","290b5889":"code","7fdcf276":"code","3da68a9d":"code","92715a3b":"code","3d05c6ca":"code","01a2ff89":"code","9b58bd9d":"code","de768f3a":"code","cd601bd8":"code","e75835b4":"code","6eb42b98":"code","c2a687cf":"code","aac3f3a2":"code","4c0a58c2":"code","f5ad43f8":"code","0604d327":"code","25724a65":"code","1f39600d":"code","4f42a0b0":"code","abb73f96":"code","c78e2623":"code","8b5ec3aa":"code","64c81393":"code","d07ca8b2":"code","a995fb20":"code","2481d26a":"code","12e62ee8":"code","bd046cb7":"code","690983fc":"code","91f499a9":"code","7656481f":"code","fc426229":"code","b9e3c3df":"code","26181776":"code","82d6a924":"code","738fe891":"code","eef7cc49":"code","1bfc11de":"code","949faa42":"code","bcdcc4a1":"code","6905cf4f":"markdown","9037a266":"markdown","05867c45":"markdown","0cc72887":"markdown","246d8ee6":"markdown","ea332b23":"markdown","a724cd6e":"markdown","4adc1b96":"markdown","6e3d5a3f":"markdown","3df9cd57":"markdown","b82f76a3":"markdown","ed30cd2a":"markdown","466e0f29":"markdown","262fe98a":"markdown","001e605e":"markdown","b338181f":"markdown","7b06676e":"markdown","e4f7c426":"markdown","a3c8cddc":"markdown","6dd746b3":"markdown","9693f4fe":"markdown","0b4a0500":"markdown","21d1cf34":"markdown","a441107a":"markdown","3585a1bd":"markdown","5c9a37e8":"markdown","3e093fdc":"markdown","872248e1":"markdown","55a6e755":"markdown","e60359c4":"markdown","5474ff31":"markdown","85c4dcae":"markdown","d1c43167":"markdown","a8d86d4d":"markdown","a9a37dbf":"markdown","b7353756":"markdown","d3325e32":"markdown","1fa799b1":"markdown","d7bf420d":"markdown","02902429":"markdown","c11f9610":"markdown","746dd9c0":"markdown","22bc1e56":"markdown","85bfd093":"markdown","3cea0cfc":"markdown","6cdc2a65":"markdown","22ed78d1":"markdown","ca511c37":"markdown","3ab75cdf":"markdown","491f63dd":"markdown","49a27ac6":"markdown","8a1a471b":"markdown","f1081608":"markdown","da89b8c4":"markdown","e79f5b65":"markdown","4d871142":"markdown","dd01934a":"markdown","230341b7":"markdown","fc4e833b":"markdown","626c64ce":"markdown","2b537e84":"markdown","48069475":"markdown","434a8daa":"markdown","6194bf7a":"markdown","ffa553fe":"markdown","75299f71":"markdown","34bef75f":"markdown","b7f4169b":"markdown","d210aaf0":"markdown","d4dddfaa":"markdown","94bc8281":"markdown","98665e50":"markdown","67f65060":"markdown","eb7a823c":"markdown","0707212d":"markdown","cec4b752":"markdown","9f5950ad":"markdown","850b2e69":"markdown","5df89f15":"markdown","d7623399":"markdown","0ad819d6":"markdown","71b7380b":"markdown","f289b69b":"markdown","c0dd476b":"markdown","c6b7e1d7":"markdown","d07f6062":"markdown","5d8b9f70":"markdown","fa8ada70":"markdown","785d40ee":"markdown","7158773c":"markdown","3bd513f7":"markdown","161cd5d2":"markdown","82edf71d":"markdown","478b2a3d":"markdown","25f8c390":"markdown","66d877c1":"markdown","89810dfa":"markdown","3c6af04a":"markdown","21d6bd00":"markdown","e3225dc4":"markdown","daa2a314":"markdown","af70f079":"markdown","28daa086":"markdown","5477600f":"markdown","7c79b519":"markdown","a67026c5":"markdown","bbf198fc":"markdown","d6def1e5":"markdown","f2bfaf64":"markdown","47991b2d":"markdown","fa34254f":"markdown","5855bd86":"markdown","60378ed8":"markdown","907ad4ca":"markdown","c1497bf4":"markdown","3d2af2c8":"markdown","69cd0902":"markdown","b33469ba":"markdown","7bb3da4f":"markdown","e84345a0":"markdown","6d9002da":"markdown","f0aed279":"markdown","8aef58ad":"markdown","96abf98c":"markdown","240ae226":"markdown","4e00217c":"markdown","b5b332b0":"markdown","3692cdd7":"markdown","e8eed69a":"markdown","876fe824":"markdown","0d11c323":"markdown","64531467":"markdown","e21226e9":"markdown","48288e4b":"markdown","31021b8c":"markdown","5fa58a8a":"markdown"},"source":{"bc14ae8e":"path=\"..\/input\/\"","02644ffb":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime as dt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom haversine import haversine\nimport statsmodels.formula.api as sm\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nimport warnings; warnings.simplefilter('ignore')","7c25ddcc":"#import the data from a csv file.\ndata = pd.read_csv(path+\"train.csv\")","c644118e":"data.head()","836b302a":"#Check shape of dataset\ndata.shape ","d4ff64e9":"#Check count of unique id's in the dataset\nprint(\"There are %d unique id's in Training dataset, which is equal to the number of records\"%(data.id.nunique()))","83047c40":"#Check for NaN values\ndata.isnull().sum()","14ffc8a3":"#Convert timestamp to datetime format to fetch the other details as listed below\ndata['pickup_datetime'] = pd.to_datetime(data['pickup_datetime'])\ndata['dropoff_datetime'] = pd.to_datetime(data['dropoff_datetime'])","ea90e0b3":"#Calculate and assign new columns to the dataframe such as weekday,\n#month and pickup_hour which will help us to gain more insights from the data.\ndata['weekday'] = data.pickup_datetime.dt.weekday_name\ndata['month'] = data.pickup_datetime.dt.month\ndata['weekday_num'] = data.pickup_datetime.dt.weekday\ndata['pickup_hour'] = data.pickup_datetime.dt.hour","b5f11366":"#calc_distance is a function to calculate distance between pickup and dropoff coordinates using Haversine formula.\ndef calc_distance(df):\n    pickup = (df['pickup_latitude'], df['pickup_longitude'])\n    drop = (df['dropoff_latitude'], df['dropoff_longitude'])\n    return haversine(pickup, drop)","c2018a0a":"#Calculate distance and assign new column to the dataframe.\ndata['distance'] = data.apply(lambda x: calc_distance(x), axis = 1)","720617ba":"#Calculate Speed in km\/h for further insights\ndata['speed'] = (data.distance\/(data.trip_duration\/3600))","e92ae743":"#Check the type of each variable\ndata.dtypes.reset_index()","5292abf6":"#Dummify all the categorical features like \"store_and_fwd_flag, vendor_id, month, weekday_num, pickup_hour, passenger_count\" except the label i.e. \"trip_duration\"\n\ndummy = pd.get_dummies(data.store_and_fwd_flag, prefix='flag')\ndummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\ndata = pd.concat([data,dummy], axis = 1)\n\ndummy = pd.get_dummies(data.vendor_id, prefix='vendor_id')\ndummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\ndata = pd.concat([data,dummy], axis = 1)\n\ndummy = pd.get_dummies(data.month, prefix='month')\ndummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\ndata = pd.concat([data,dummy], axis = 1)\n\ndummy = pd.get_dummies(data.weekday_num, prefix='weekday_num')\ndummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\ndata = pd.concat([data,dummy], axis = 1)\n\ndummy = pd.get_dummies(data.pickup_hour, prefix='pickup_hour')\ndummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\ndata = pd.concat([data,dummy], axis = 1)\n\ndummy = pd.get_dummies(data.passenger_count, prefix='passenger_count')\ndummy.drop(dummy.columns[0], axis=1, inplace=True) #avoid dummy trap\ndata = pd.concat([data,dummy], axis = 1)","d15c9833":"data.head()","dadf0014":"pd.options.display.float_format = '{:.2f}'.format #To suppres scientific notation.\ndata.passenger_count.value_counts()","089749b1":"plt.figure(figsize = (20,5))\nsns.boxplot(data.passenger_count)\nplt.show()","63fa8099":"data.passenger_count.describe()","a2ce9ed2":"data['passenger_count'] = data.passenger_count.map(lambda x: 1 if x == 0 else x)","4ef6bf26":"data = data[data.passenger_count <= 6]","67d9799f":"data.passenger_count.value_counts()","84ae11d5":"sns.countplot(data.passenger_count)\nplt.show()","4b8fac34":"sns.countplot(data.vendor_id)\nplt.show()","ec0d3ba4":"print(data.distance.describe())","2452c89e":"plt.figure(figsize = (20,5))\nsns.boxplot(data.distance)\nplt.show()","f85a71fe":"print(\"There are {} trip records with 0 km distance\".format(data.distance[data.distance == 0 ].count()))","f8c7a1b0":"data[data.distance == 0 ].head()","7da78e5b":"data.distance.groupby(pd.cut(data.distance, np.arange(0,100,10))).count().plot(kind='barh')\nplt.show()","6471648d":"data.trip_duration.describe()","2616cc5c":"plt.figure(figsize = (20,5))\nsns.boxplot(data.trip_duration)\nplt.show()","59f10e49":"data.trip_duration.groupby(pd.cut(data.trip_duration, np.arange(1,max(data.trip_duration),3600))).count()","e4231ccc":"data[data.trip_duration > 86400]","3485a28f":"data = data[data.trip_duration <= 86400]","1e824584":"data.trip_duration.groupby(pd.cut(data.trip_duration, np.arange(1,7200,600))).count().plot(kind='barh')\nplt.xlabel('Trip Counts')\nplt.ylabel('Trip Duration (seconds)')\nplt.show()","b4fc8c10":"data.speed.describe()","0614ebde":"plt.figure(figsize = (20,5))\nsns.boxplot(data.speed)\nplt.show()","91cc6510":"data = data[data.speed <= 104]\nplt.figure(figsize = (20,5))\nsns.boxplot(data.speed)\nplt.show()","0b6fc927":"data.speed.groupby(pd.cut(data.speed, np.arange(0,104,10))).count().plot(kind = 'barh')\nplt.xlabel('Trip count')\nplt.ylabel('Speed (Km\/H)')\nplt.show()","5f309077":"data.flag_Y.value_counts(normalize=True)","8d43d6ce":"data.flag_Y.value_counts()","dc521178":"data.vendor_id[data.flag_Y == 1].value_counts()","4e2758e4":"data[data.flag_Y == 1]","b6dd130e":"sns.countplot(data.pickup_hour)\nplt.show()","b65915e7":"plt.figure(figsize = (8,6))\nsns.countplot(data.weekday_num)\nplt.xlabel(' Month ')\nplt.ylabel('Pickup counts')\nplt.show()","bc170801":"n = sns.FacetGrid(data, col='weekday_num')\nn.map(plt.hist, 'pickup_hour')\nplt.show()","c5ae7d78":"sns.countplot(data.month)\nplt.ylabel('Trip Counts')\nplt.xlabel('Months')\nplt.show()","46a4716a":"group1 = data.groupby('pickup_hour').trip_duration.mean()\nsns.pointplot(group1.index, group1.values)\nplt.ylabel('Trip Duration (seconds)')\nplt.xlabel('Pickup Hour')\nplt.show()","345c066a":"group2 = data.groupby('weekday_num').trip_duration.mean()\nsns.pointplot(group2.index, group2.values)\nplt.ylabel('Trip Duration (seconds)')\nplt.xlabel('Weekday')\nplt.show()","c3ce8b9b":"group3 = data.groupby('month').trip_duration.mean()\nsns.pointplot(group3.index, group3.values)\nplt.ylabel('Trip Duration (seconds)')\nplt.xlabel('Month')\nplt.show()","5625e965":"group4 = data.groupby('vendor_id').trip_duration.mean()\nsns.barplot(group4.index, group4.values)\nplt.ylabel('Trip Duration (seconds)')\nplt.xlabel('Vendor')\nplt.show()","2cf69298":"plt.figure(figsize = (6,5))\nplot_dur = data.loc[(data.trip_duration < 10000)]\nsns.boxplot(x = \"flag_Y\", y = \"trip_duration\", data = plot_dur)\nplt.show()","851ab9bc":"group5 = data.groupby('pickup_hour').distance.mean()\nsns.pointplot(group5.index, group5.values)\nplt.ylabel('Distance (km)')\nplt.show()","c05494a8":"group6 = data.groupby('weekday_num').distance.mean()\nsns.pointplot(group6.index, group6.values)\nplt.ylabel('Distance (km)')\nplt.show()","9000d472":"group7 = data.groupby('month').distance.mean()\nsns.pointplot(group7.index, group7.values)\nplt.ylabel('Distance (km)')\nplt.show()","c32e54d7":"group8 = data.groupby('vendor_id').distance.mean()\nsns.barplot(group8.index, group8.values)\nplt.ylabel(\"Distance km\")\nplt.show()","3e580d0e":"plt.figure(figsize = (6,6))\nplot_dist = data.loc[(data.distance < 100)]\nsns.boxplot(x = \"flag_Y\", y = \"distance\", data = plot_dist)\nplt.ylabel('Distance (km)')\nplt.show()","923172ce":"plt.scatter(data.trip_duration, data.distance , s=1, alpha=0.5)\nplt.ylabel('Distance')\nplt.xlabel('Trip Duration')\nplt.show()","4d021494":"dur_dist = data.loc[(data.distance < 50) & (data.trip_duration < 1000), ['distance','trip_duration']]\nplt.scatter(dur_dist.trip_duration, dur_dist.distance , s=1, alpha=0.5)\nplt.ylabel('Distance')\nplt.xlabel('Trip Duration')\nplt.show()","dfe409f6":"data = data[~((data.distance == 0) & (data.trip_duration >= 60))]","dc0a5ff4":"duo = data.loc[(data['distance'] <= 1) & (data['trip_duration'] >= 3600),['distance','trip_duration']].reset_index(drop=True)","d51d9daa":"sns.regplot(duo.distance, duo.trip_duration)\nplt.show()","57cb5e52":"data = data[~((data['distance'] <= 1) & (data['trip_duration'] >= 3600))]","765659a0":"group9 = data.groupby('pickup_hour').speed.mean()\nsns.pointplot(group9.index, group9.values)\nplt.show()","290b5889":"group10 = data.groupby('weekday_num').speed.mean()\nsns.pointplot(group10.index, group10.values)\nplt.show()","7fdcf276":"group9 = data.groupby('vendor_id').passenger_count.mean()\nsns.barplot(group9.index, group9.values)\nplt.ylabel('Passenger count')\nplt.show()","3da68a9d":"data.groupby('passenger_count').vendor_id.value_counts().reset_index(name='count').pivot(\"passenger_count\",\"vendor_id\",\"count\").plot(kind='bar')\nplt.show()","92715a3b":"def map_marker(set):\n    # Import package for map\n    from mpl_toolkits.basemap import Basemap\n    plt.figure(figsize = (20,20))\n\n    # Set the limits of the map to the minimum and maximum coordinates\n    lat_min = data[\"pickup_latitude\"].min() - .2\n    lat_max = data[\"pickup_latitude\"].max() + .2\n    lon_min = data[\"pickup_longitude\"].min() - .2\n    lon_max = data[\"pickup_longitude\"].max() + .2\n\n    # Set the center of the map\n    cent_lat = (lat_min + lat_max) \/ 2\n    cent_lon = (lon_min + lon_max) \/ 2\n\n    #Create the Basemap object with required params.\n    map = Basemap(llcrnrlon=lon_min,\n                  llcrnrlat=lat_min,\n                  urcrnrlon=lon_max,\n                  urcrnrlat=lat_max,\n                  resolution='h',\n                  projection='tmerc',\n                  lat_0 = cent_lat,\n                  lon_0 = cent_lon)\n\n    #Style the map\n    map.drawmapboundary()\n    map.drawcoastlines()\n    map.fillcontinents()\n    map.drawcountries(linewidth=2)\n    map.drawstates()\n\n\n    #Fetch the long and lat in form of array\n    long = np.array(data[\"pickup_longitude\"])\n    lat = np.array(data[\"pickup_latitude\"])\n\n    x, y = map(long, lat)\n    map.plot(x, y,'ro', markersize=2, alpha=1)\n\n    plt.show()","3d05c6ca":"map_marker(data)","01a2ff89":"#CA state pickup\ndata[data.pickup_longitude == data.pickup_longitude.min()]","9b58bd9d":"data = data[data.pickup_longitude != data.pickup_longitude.min()]","de768f3a":"map_marker(data)","cd601bd8":"data[data.pickup_longitude == data.pickup_longitude.min()]","e75835b4":"data = data[data.pickup_longitude != data.pickup_longitude.min()]\nmap_marker(data)","6eb42b98":"plt.figure(figsize=(20,20))\nfrom mpl_toolkits.basemap import Basemap\n\n# Set the limits of the map to the minimum and maximum coordinates\nlat_min = 40.5\nlat_max = 40.9\nlon_min = -74.2\nlon_max = -73.7\n\n# Set the center of the map\ncent_lat = (lat_min + lat_max) \/ 2\ncent_lon = (lon_min + lon_max) \/ 2\n\n#Create the Basemap object with required params.\nmap = Basemap(llcrnrlon=lon_min,\n              llcrnrlat=lat_min,\n              urcrnrlon=lon_max,\n              urcrnrlat=lat_max,\n              resolution='h',\n              projection='tmerc',\n              lat_0 = cent_lat,\n              lon_0 = cent_lon)\n\n#Style the map\n# map.drawcounties()\n\n#Fetch the long and lat in form of array\nlong = np.array(data[\"pickup_longitude\"])\nlat = np.array(data[\"pickup_latitude\"])\n\nx, y = map(long, lat)\nmap.plot(x, y,'bo', markersize=1, alpha=1)\nplt.xticks()\nplt.show()","c2a687cf":"plt.figure(figsize=(20,20))\n\n# Set the limits of the map to the minimum and maximum coordinates\nlat_min = 40.5\nlat_max = 40.9\nlon_min = -74.2\nlon_max = -73.7\n\n# Set the center of the map\ncent_lat = (lat_min + lat_max) \/ 2\ncent_lon = (lon_min + lon_max) \/ 2\n\n#Create the Basemap object with required params.\nmap = Basemap(llcrnrlon=lon_min,\n              llcrnrlat=lat_min,\n              urcrnrlon=lon_max,\n              urcrnrlat=lat_max,\n              resolution='h',\n              projection='tmerc',\n              lat_0 = cent_lat,\n              lon_0 = cent_lon)\n\n#Style the map\n# map.drawcounties()\n\n#Fetch the long and lat in form of array\nlong = np.array(data[\"dropoff_longitude\"])\nlat = np.array(data[\"dropoff_latitude\"])\n\nx, y = map(long, lat)\nmap.plot(x, y,'go', markersize=1, alpha=1)\nplt.xticks()\nplt.show()","aac3f3a2":"#First chech the index of the features and label\nlist(zip( range(0,len(data.columns)),data.columns))","4c0a58c2":"Y = data.iloc[:,10].values\nX = data.iloc[:,range(15,61)].values","f5ad43f8":"print(\"Let's append {} rows of 1's as the first column in the X array\".format(X.shape[0]))","0604d327":"X1 = np.append(arr = np.ones((X.shape[0],1)).astype(int), values = X, axis = 1)","25724a65":"X1.shape","1f39600d":"# X_opt = X1[:,[0,1,3,4,6,7,8,9,10,11]]\n# regressor_OLS = sm.OLS(endog = Y, exog = X_opt).fit()\n# regressor_OLS.summary()","4f42a0b0":"#Select all the features in X array\nX_opt = X1[:,range(0,46)]\nregressor_OLS = sm.OLS(endog = Y, exog = X_opt).fit()\n\n#Fetch p values for each feature\np_Vals = regressor_OLS.pvalues\n\n#define significance level for accepting the feature.\nsig_Level = 0.05\n\n#Loop to iterate over features and remove the feature with p value less than the sig_level\nwhile max(p_Vals) > sig_Level:\n    print(\"Probability values of each feature \\n\")\n    print(p_Vals)\n    X_opt = np.delete(X_opt, np.argmax(p_Vals), axis = 1)\n    print(\"\\n\")\n    print(\"Feature at index {} is removed \\n\".format(str(np.argmax(p_Vals))))\n    print(str(X_opt.shape[1]-1) + \" dimensions remaining now... \\n\")\n    regressor_OLS = sm.OLS(endog = Y, exog = X_opt).fit()\n    p_Vals = regressor_OLS.pvalues\n    print(\"=================================================================\\n\")\n    \n#Print final summary\nprint(\"Final stat summary with optimal {} features\".format(str(X_opt.shape[1]-1)))\nregressor_OLS.summary()","abb73f96":"#Split raw data\nX_train, X_test, y_train, y_test = train_test_split(X,Y, random_state=4, test_size=0.2)\n\n#Split data from the feature selection group\nX_train_fs, X_test_fs, y_train_fs, y_test_fs = train_test_split(X_opt,Y, random_state=4, test_size=0.2)","c78e2623":"X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X,Y, random_state=4, test_size=0.2)","8b5ec3aa":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train_pca = scaler.fit_transform(X_train_pca)\nX_test_pca = scaler.transform(X_test_pca)","64c81393":"from sklearn.decomposition import PCA\npca = PCA().fit(X_train_pca)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel(\"number of components\")\nplt.ylabel(\"Cumulative explained variance\")\nplt.show()","d07ca8b2":"arr = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\nlist(zip(range(1,len(arr)), arr))","a995fb20":"pca_10 = PCA(n_components=40)\nX_train_pca = pca_10.fit_transform(X_train_pca)\nX_test_pca = pca_10.transform(X_test_pca)","2481d26a":"plt.figure(figsize=(15,15))\ncorr = pd.DataFrame(X_train_fs[:,1:]).corr()\ncorr.index = pd.DataFrame(X_train_fs[:,1:]).columns\nsns.heatmap(corr, cmap='RdYlGn', vmin=-1, vmax=1, square=True)\nplt.title(\"Correlation Heatmap\", fontsize=16)\nplt.show()","12e62ee8":"plt.figure(figsize=(15,15))\ncorr = pd.DataFrame(X_train_pca).corr()\ncorr.index = pd.DataFrame(X_train_pca).columns\nsns.heatmap(corr, cmap='RdYlGn', vmin=-1, vmax=1, square=True)\nplt.title(\"Correlation Heatmap\", fontsize=16)\nplt.show()","bd046cb7":"#Linear regressor for the raw data\nregressor = LinearRegression() \nregressor.fit(X_train,y_train) \n\n#Linear regressor for the Feature selection group\nregressor1 = LinearRegression() \nregressor1.fit(X_train_fs,y_train_fs) \n\n#Linear regressor for the Feature extraction group\nregressor2 = LinearRegression() \nregressor2.fit(X_train_pca,y_train_pca) ","690983fc":"#Predict from the test features of raw data\ny_pred = regressor.predict(X_test) \n\n#Predict from the test features of Feature Selection group\ny_pred = regressor1.predict(X_test_fs) \n\n#Predict from the test features of Feature Extraction group\ny_pred_pca = regressor2.predict(X_test_pca) ","91f499a9":"#Evaluate the regressor on the raw data\nprint('RMSE score for the Multiple LR raw is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test,y_pred))))\nprint('Variance score for the Multiple LR raw is : %.2f' % regressor.score(X_test, y_test))\nprint(\"\\n\")\n\n#Evaluate the regressor on the Feature selection group\nprint('RMSE score for the Multiple LR FS is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_fs,y_pred))))\nprint('Variance score for the Multiple LR FS is : %.2f' % regressor1.score(X_test_fs, y_test_fs))\nprint(\"\\n\")\n\n#Evaluate the regressor on the Feature extraction group\nprint('RMSE score for the Multiple LR PCA is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_pca,y_pred_pca))))\nprint('Variance score for the Multiple LR PCA is : %.2f' % regressor2.score(X_test_pca, y_test_pca))","7656481f":"X_train.shape","fc426229":"#Find linear correlation of each feature with the target variable\nfrom scipy.stats import pearsonr\ndf1 = pd.DataFrame(np.concatenate((X_train,y_train.reshape(len(y_train),1)),axis=1))\ndf1.columns = df1.columns.astype(str)\n\nfeatures = df1.iloc[:,:46].columns.tolist()\ntarget = df1.iloc[:,46].name\n\ncorrelations = {}\nfor f in features:\n    data_temp = df1[[f,target]]\n    x1 = data_temp[f].values\n    x2 = data_temp[target].values\n    key = f + ' vs ' + target\n    correlations[key] = pearsonr(x1,x2)[0]\n    \ndata_correlations = pd.DataFrame(correlations, index=['Value']).T\ndata_correlations.loc[data_correlations['Value'].abs().sort_values(ascending=False).index]","b9e3c3df":"#instantiate the object for the Random Forest Regressor with default params from raw data\nregressor_rfraw = RandomForestRegressor(n_jobs=-1)\n\n#instantiate the object for the Random Forest Regressor with default params for Feature Selection Group\nregressor_rf = RandomForestRegressor(n_jobs=-1)\n\n# #instantiate the object for the Random Forest Regressor with tuned hyper parameters for Feature Selection Group\n# regressor_rf1 = RandomForestRegressor(n_estimators = 26,\n#                                      max_depth = 22,\n#                                      min_samples_split = 9,\n#                                      n_jobs=-1)\n\n#instantiate the object for the Random Forest Regressor for Feature Extraction Group\nregressor_rf2 = RandomForestRegressor(n_jobs=-1)\n\n\n#Train the object with default params for raw data\nregressor_rfraw.fit(X_train,y_train)\n\n#Train the object with default params for Feature Selection Group\nregressor_rf.fit(X_train_fs,y_train_fs)\n\n# #Train the object with tuned params for Feature Selection Group\n# regressor_rf1.fit(X_train_fs,y_train_fs)\n\n# #Train the object with default params for Feature Extraction Group\nregressor_rf2.fit(X_train_pca,y_train_pca)\n\nprint(\"\\n\")","26181776":"#Predict the output with object of default params for Feature Selection Group\ny_pred_rfraw = regressor_rfraw.predict(X_test)\n\n#Predict the output with object of default params for Feature Selection Group\ny_pred_rf = regressor_rf.predict(X_test_fs)\n\n# #Predict the output with object of hyper tuned params for Feature Selection Group\n# y_pred_rf1 = regressor_rf1.predict(X_test_fs)\n\n#Predict the output with object of PCA params for Feature Extraction Group\ny_pred_rfpca = regressor_rf2.predict(X_test_pca)\n\nprint(\"\\n\")","82d6a924":"#Evaluate the model with default params for raw data\nprint('RMSE score for the RF regressor raw is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test,y_pred_rfraw))))\nprint('RMSLE score for the RF regressor raw is : {}'.format(np.sqrt(metrics.mean_squared_log_error(y_test,y_pred_rfraw))))\nprint('Variance score for the RF regressor raw is : %.2f' % regressor_rfraw.score(X_test, y_test))\n\nprint(\"\\n\")\n\n#Evaluate the model with default params for Feature Selection Group\nprint('RMSE score for the RF regressor is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_fs,y_pred_rf))))\nprint('RMSLE score for the RF regressor is : {}'.format(np.sqrt(metrics.mean_squared_log_error(y_test_fs,y_pred_rf))))\nprint('Variance score for the RF regressor is : %.2f' % regressor_rf.score(X_test_fs, y_test_fs))\n\n# print(\"\\n\")\n\n# #Evaluate the model with tuned params for Feature Selection Group\n# print('RMSE score for the RF regressor1 is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_fs,y_pred_rf1))))\n# print('RMSLE score for the RF regressor1 is : {}'.format(np.sqrt(metrics.mean_squared_log_error(y_test_fs,y_pred_rf1))))\n# print('Variance score for the RF regressor1 is : %.2f' % regressor_rf1.score(X_test_fs, y_test_fs))\n\nprint(\"\\n\")\n\n#Evaluate the model with PCA params  for Feature Extraction Group\nprint('RMSE score for the RF regressor2 is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_pca, y_pred_rfpca))))\nprint('Variance score for the RF regressor2 is : %.2f' % regressor_rf2.score(X_test_pca, y_test_pca))","738fe891":"#instantiate the object for the XGBoost Regressor with default params for raw data\nregressor_xgbraw = XGBRegressor(n_jobs=-1)\n\n#instantiate the object for the XGBoost Regressor with default params for Feature Selection Group\nregressor_xgb = XGBRegressor(n_jobs=-1)\n\n#instantiate the object for the XGBoost Regressor with tuned hyper parameters for Feature Selection Group\nregressor_xgb1 = XGBRegressor(n_estimators=300,\n                            learning_rate=0.08,\n                            gamma=0,\n                            subsample=0.75,\n                            colsample_bytree=1,\n                            max_depth=7,\n                            min_child_weight=4,\n                            silent=1,\n                           n_jobs=-1)\n\n#instantiate the object for the XGBoost Regressor for Feature Extraction Group\nregressor_xgb2 = XGBRegressor(n_jobs=-1)\n\n\n#Train the object with default params for raw data\nregressor_xgbraw.fit(X_train,y_train)\n\n#Train the object with default params for Feature Selection Group\nregressor_xgb.fit(X_train_fs,y_train_fs)\n\n#Train the object with tuned params for Feature Selection Group\nregressor_xgb1.fit(X_train_fs,y_train_fs)\n\n#Train the object with default params for Feature Extraction Group\nregressor_xgb2.fit(X_train_pca,y_train_pca)\n\nprint(\"\\n\")","eef7cc49":"#Predict the output with object of default params for raw data\ny_pred_xgbraw = regressor_xgbraw.predict(X_test)\n\n#Predict the output with object of default params for Feature Selection Group\ny_pred_xgb = regressor_xgb.predict(X_test_fs)\n\n#Predict the output with object of hyper tuned params for Feature Selection Group\ny_pred_xgb1 = regressor_xgb1.predict(X_test_fs)\n\n#Predict the output with object of PCA params for Feature Extraction Group\ny_pred_xgb_pca = regressor_xgb2.predict(X_test_pca)\n\nprint(\"\\n\")","1bfc11de":"#Evaluate the model with default params for raw data\nprint('RMSE score for the XGBoost regressor raw is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test,y_pred_xgbraw))))\n# print('RMSLE score for the XGBoost regressor is : {}'.format(np.sqrt(metrics.mean_squared_log_error(y_test,y_pred_xgb))))\nprint('Variance score for the XGBoost regressor raw is : %.2f' % regressor_xgbraw.score(X_test, y_test))\n\nprint(\"\\n\")\n\n#Evaluate the model with default params for Feature Selection Group\nprint('RMSE score for the XGBoost regressor is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_fs,y_pred_xgb))))\n# print('RMSLE score for the XGBoost regressor is : {}'.format(np.sqrt(metrics.mean_squared_log_error(y_test,y_pred_xgb))))\nprint('Variance score for the XGBoost regressor is : %.2f' % regressor_xgb.score(X_test_fs, y_test_fs))\n\nprint(\"\\n\")\n\n#Evaluate the model with Tuned params for Feature Selection Group\nprint('RMSE score for the XGBoost regressor1 is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_fs,y_pred_xgb1))))\n# print('RMSLE score for the XGBoost regressor1 is : {}'.format(np.sqrt(metrics.mean_squared_log_error(y_test_fs,y_pred_xgb1))))\nprint('Variance score for the XGBoost regressor1 is : %.2f' % regressor_xgb1.score(X_test_fs,y_test_fs))\n\nprint(\"\\n\")\n\n#Evaluate the model with PCA params  for Feature Extraction Group\nprint('RMSE score for the XGBoost regressor2 is : {}'.format(np.sqrt(metrics.mean_squared_error(y_test_pca, y_pred_xgb_pca))))\nprint('Variance score for the XGBoost regressor2 is : %.2f' % regressor_xgb2.score(X_test_pca, y_test_pca))","949faa42":"#Comparing test results for the XGBoost and RF regressor\nprint(\"Total sum of difference between the actual and the predicted values for the RF regressor is : %d\"%np.abs(np.sum(np.subtract(y_test,y_pred_rf))))\nprint(\"Total sum of difference between the actual and the predicted values for the tuned XGB regressor is : %d\"%np.abs(np.sum(np.subtract(y_test,y_pred_xgb1))))","bcdcc4a1":"#Define a function to plot learning curve.\ndef learning_curves(estimator, title, features, target, train_sizes, cv, n_jobs=-1):\n    plt.figure(figsize = (14,5))\n    train_sizes, train_scores, validation_scores = learning_curve(estimator, features, target, train_sizes = train_sizes, cv = cv, scoring = 'neg_mean_squared_error',  n_jobs=n_jobs)\n    train_scores_mean = -train_scores.mean(axis = 1)\n    validation_scores_mean = -validation_scores.mean(axis = 1)\n    \n    plt.grid()\n    \n    plt.plot(train_sizes, train_scores_mean,'o-', color=\"r\", label = 'Training error')\n    plt.plot(train_sizes, validation_scores_mean,'o-', color=\"g\", label = 'Validation error')\n\n    plt.ylabel('MSE', fontsize = 14)\n    plt.xlabel('Training set size', fontsize = 14)\n    \n    title = 'Learning curves for a ' + title + ' model'\n    plt.title(title, fontsize = 18, loc='left')\n    \n    plt.legend(loc=\"best\")\n    \n    return plt\n\n# score curves, each time with 20% data randomly selected as a validation set.\ncv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=4)\n\n# Plot learning curve for the RF Regressor\ntitle = \"Random Forest Regressor\"\n\n# Call learning curve with all dataset i.e. traininig and test combined because CV will take of data split.\nlearning_curves(regressor_rf, title, X_opt,Y, train_sizes=np.linspace(.1, 1.0, 5), cv=cv, n_jobs=-1)\n\n#Plot learning curve for the XGBoost Regressor\ntitle = \"XGBoost Regressor\"\n\n# Call learning curve on less number of estimators than the tuned estimator because it took too much time for the compilation.\nlearning_curves(XGBRegressor(n_estimators=111,\n                            learning_rate=0.08,\n                            gamma=0,\n                            subsample=0.75,\n                            colsample_bytree=1,\n                            max_depth=7,\n                            min_child_weight=4,\n                            silent=1), title, X_opt,Y, train_sizes=np.linspace(.1, 1.0, 5), cv=cv, n_jobs=-1)\n\nplt.show()","6905cf4f":"### Interesting find\n- It seems that most of the big cars are served by the Vendor 2 including minivans because other than passenger 1, vendor 2 has majority in serving more than 1 passenger count and that explains it greater share of the market.","9037a266":"### Question: \nBut what's the difference?\n - **Feature selection**: we select a subset of the original feature set based on the statistical significance of different parameters.\n > Example: Backward elimination, Forward selection, Recursive feature elimination\n - **Feature extraction**: we build a new set of features from the original feature set\n > Example: PCA, LDA, Kernel PCA","05867c45":"<a id=evaluate><\/a>\n## Model evaluation\n***\nWe will evaluate our model's accuracy through two suggested metrics for the regression models. i.e. RMSE and variance score. Where RMSE of 0 and variance of 1 is considered as the best score for a prediction model.","0cc72887":"Though both the vendors seems to have almost equal market share. But Vendor 2 is evidently more famous among the population as per the above graph.","246d8ee6":"<a id=duration><\/a>\n## Trip duration\n***\n<img src='https:\/\/i.imgur.com\/2ZaOl33.png'\/>","ea332b23":"Now our dataset is complete for the further analysis before we train our model with optimal variables.","a724cd6e":"### Interesting Find\nHere are some observations from the map:\n - One unusual pickup from the CA state.\n - There are quite a few pickup from the neighbouring state as well. Some are quite far and some very near to the NYC state.\n\nWe should check the validity of the unusual pickup here and take corrective actions if necessary. Let's take a closer look.","4adc1b96":"<a id=week_duration><\/a>\n## Trip duration per weekday\n***\nLet's now analyze the pattern of trip duration during the week.","6e3d5a3f":"### Interesting find:\n- There some trips with over 100 km distance.\n- Some of the trips distance value is 0 km.\n\n### Observations:\n- mean distance travelled is approx 3.5 kms.\n- standard deviation of 4.3 which shows that most of the trips are limited to the range of 1-10 kms.","3df9cd57":"### Interesting Find\n - We can see that most of the taxi pickups were done in the manhattan area as compared to the other areas in NYC.\n - A long trail towards the airport shows that the airport is situated quite far from the Manhattan area.\n \n### Observations\n - There must have been some long distance rides towards and from the airport.\n - Similarly the average duration for the rides picked-up to or for the airport would have been longer.\n","b82f76a3":"<a id=month_trip><\/a>\n## Total trips per month\n***\nLet's take a look at the trip distribution across the months to understand if there is any diffrence in the taxi pickups in different months","ed30cd2a":"### Observations:\n- Trips over 30 km\/h are being considered as outliers but we cannot ignore them because they are well under the highest speed limit of 104 km\/h on state controlled highways.\n- Mostly trips are done at a speed range of 10-20 km\/h with an average speed of around 14 km\/h.\n\nLet's take a look at the speed range ditribution with the help of graph.","466e0f29":"PCA is applied on the training and the test dataset. Our input features are now ready for the regression.","262fe98a":"<img src='http:\/\/www.sixthcents.net\/images\/macbook.gif'\/>","001e605e":"### Observations\n - All of the features shows **NO** correlation at all. Because feature extraction removes all collinearity.\n \nLet's move on to the Model now.","b338181f":"### PCA application\nLet's apply PCA technique on the training features to understand how many principal components should we select for our model to capture atleast 90% variance. For that we will take help of plot and cumsum function of numpy package.","7b06676e":"### Observations\nFinally we have reached the combination of optimum features with each feature having **p value < 0.05**. ","e4f7c426":"<a id=month_duration><\/a>\n## Trip duration per month\n***\nLet's take a look at the trip duration pattern with respect to the different months.","a3c8cddc":"We can see that trip duration is almost equally distributed across the week on a scale of 0-1000 minutes with minimal difference in the duration times. Also, it is observed that trip duration on thursday is longest among all days.","6dd746b3":"<a id=week_distance><\/a>\n## Distance per weekday\n***\nLet's analyze the average trip distance covered on each day of the week.","9693f4fe":"<a id=flag_distance><\/a>\n## Distance v\/s Flag\n***\nLet's visualize if there is any effect of Flag setting on the distance covered in the trips","0b4a0500":"<a id=vendor_duration><\/a>\n## Trip duration per vendor\n***\nWe can also look at the average difference between the trip duration for each vendor. However we do know that vendor 2 has larger share of the market. Let's visualize.","21d1cf34":"### Observations:\n- Trip durations scale is less for the trips where the flag is set i.e. the trip details are stored before sending it to the server.\n- Trip duration outliers are also less for the trips with flag 'Y' as compared the trips with flag 'N'.\n- Trip duration is longer for the trips where the flag is not set.\n- Inter quartile range of trip duration is more for the trips with the flag 'Y' as compared to the trips with flag 'N' but the median value is almost equal for both.","a441107a":"### Observations\n - Some combinations of features shows slight correlation but not above 0.5.\n - Some features are infact negatively correlated.\n - But most of the features shows no correlation. Which is a good thing.\n \n All in all, we have a very good pack of attributes to train our model. Let's move ahead.","3585a1bd":"Let's now plot the relationship between the features of the **Feature extraction** group.","5c9a37e8":"As per above details. Mean median and mode are all approx equal to 1. So we would replace the 0 passenger count with 1.","3e093fdc":"This will divide our dataset randomly with a ratio of 80\/20 where training set consists of more than 1 million records and test dataset with more than .35 million records. Let's train our model on the training set now.","872248e1":"<a id=duration_distance><\/a>\n## Distance v\/s Trip duration\n***\nLet's visualize the relationship between Distance covered and respective trip duration.","55a6e755":"<a id=library><\/a>\n## Import libraries","e60359c4":"### Model prediction","5474ff31":"Let's visualize the number of trips taken in slabs of 0-10, 20-30 ... minutes respectively","85c4dcae":"### Scale Data\nIt is suggested to scale the input varibles first before applying PCA to standardise the variance and avoid the bias. Lets Scale the data using StandardScaler.","d1c43167":"<a id=rf_reg><\/a>\n## Random Forest Regressor\n***\nA random forest is a meta estimator that fits a number of classifying decision trees on various sub-samples of the dataset and use averaging to improve the predictive accuracy and control over-fitting","a8d86d4d":"<a id=data><\/a>\n## Import Dataset","a9a37dbf":"### Interesting find\nWe can still see a marker in the CA state. \n\n### Idea\nWere there more than one pickups in the CA or the data is still not filtered?\n\nLet's check this again","b7353756":"### Model evaluation","d3325e32":"<a id=explore><\/a>\n## Data Exploration\nWe will explore the data and modify dataset as per the our requirment for the further analysis of the problem.","1fa799b1":"From the above observation it is evident that most of the rides are completed between 1-10 Kms with some of the rides with distances between 10-30 kms. Other slabs bar are not visible because the number of trips are very less as compared to these slabs","d7bf420d":"### Observations:\n- We can see almost similar results like the one observed in the Trip duration v\/s Flag analysis.\n- Only two major difference can be seen here.\n 1. Interquartile range of distance is almost twice for Flag 'Y' trips as compared to the Flag 'N' trips\n 2. Median value is much different in both the case as well.\n\nWhich points us to the fact that range of distance and trip duration for the Flag 'Y' trips is much more limited and confined as compared with the flag 'N' trips and this also resulted in much less number of outliers for Flag 'Y' trips.","02902429":"It is evident from this graph what we thought off earlier i.e. most of the trips were done at a speed range of 10-20 km\/H.","c11f9610":"<a id=flag><\/a>\n## Store_and_fwd_flag\n***\nThis flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - Y=store and forward; N=not a store and forward trip.","746dd9c0":"<a id=curve><\/a>\n## Learning curves\n***\nLearning curves constitute a great tool to diagnose bias and variance in any supervised learning algorithm. It shows how error changes as the training set size increases. We'll use the learning_curve() [function](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.learning_curve.html) from the scikit-learn library to generate a learning curve for the regression model. There's no need to put aside a validation set because learning_curve() will take care of that and that's why we will plot the learning curve over whole dataset.","22bc1e56":"### Observations\n - Very poor **Root mean squared** value. \n - And the low **variance score** which is also bad.\n - Both the models i.e. from the feature selection and the feature extraction group resulted quite bad in prediction.\n \n **Let's find out the reason of this behaviour:-**","85bfd093":"<a id=vendor><\/a>\n## Vendor\n***\nHere we analyze taxi data only for the 2 vendors which are listed as 1 and 2 in the datset.","3cea0cfc":"<a id=extract><\/a>\n## Feature Extraction\n***\nWe will use PCA for feature extraction i.e. Principal Component Analysis. It is a statistical procedure that uses an orthogonal transformation to convert a set of observations of possibly correlated variables into a set of values of linearly uncorrelated variables called principal components","6cdc2a65":"# NYC Taxi Trip Duration Prediction\n***\n<img src='https:\/\/i.imgur.com\/GNbWoOk.jpg' align=\"left\"\/>","22ed78d1":"### Interesting find\n- There are quite a few pickups being shown off the NYC coast i.e. in the Atlantic ocean. \n\n### Observations\n- Now most of the pickups are being shown in and around NYC area.\n- It would be cumbersome to track them and remove. We can ignore them as of now to focus more on NYC pickups.","ca511c37":"## Further Scope..\n***\nThere's always a room for the improvement and a lot more to explore, and **if this helped you** in any way, I'd like to see **One Upvote!**. Also, please **leave comments** about any further improvements to this notebook!! Your feedback or any constructive criticism is highly appreciated.","3ab75cdf":"<a id=model><\/a>\n# Model\n***\nWe need a model to train on our dataset to serve our purpose of prediciting the NYC taxi trip duration given the other features as training and test set. Since our dependent variable contains continous values so we will use regression technique to predict our output.","491f63dd":"<a id=map><\/a>\n# Map Visualization\n***\nWe shall visualize the Taxi pickup locations by placing long and lat marker on the MAP of the US. So that we can analyze below questions:\n - Are all pickups constrained to NYC and it's surrounding areas?\n - Is there any unusual location of the pickup?\n - Are the lat long constrained to the land area of the US and nowhere else?\n \n <img src='https:\/\/cdn.dribbble.com\/users\/22930\/screenshots\/1919115\/line-art-map_2.gif' align='left'\/>\n","49a27ac6":"### Split Data\n\nLets split our data first before scaling the features","8a1a471b":"## Table of Contents\n***\n\n### [Problem Statement](#problem)\n - [Set path](#path)\n - [Libraries](#library)\n - [Dataset](#data)\n - [Explore Data](#explore)\n\n### [Univariate Analysis](#univariate)\n - [Passengers](#passenger)\n - [Vendor](#vendor)\n - [Distance](#distance)\n - [Trip duration](#duration)\n - [Speed](#speed)\n - [Store and forward flag](#flag)\n - [Trips per hour](#hour_trip)\n - [Trips per weekday](#week_trip)\n - [Trips per Month](#month_trip)\n\n### [Bivariate analysis](#bivariate)\n - [Trip duration per hour](#hour_duration)\n - [Trip duration per weekday](#week_duration)\n - [Trip duration per month](#month_duration)\n - [Trip duration per vendor](#vendor_duration)\n - [Trip duration v\/s Flag](#flag_duration)\n - [Distance per hour](#hour_distance)\n - [Distance per weekday](#week_distance)\n - [Distance per month](#month_distance)\n - [Distance per vendor](#vendor_distance)\n - [Distance v\/s Flag](#flag_distance)\n - [Distance v\/s Trip duration](#duration_distance)\n - [Speed per hour](#speed_hour)\n - [Speed per weekday](#speed_weekday)\n - [Passengers per vendor](#passenger_vendor)\n\n### [Map Visualization](#map)\n - [Basemap](#basemap)\n - [Taxi pickup locations](#pickup)\n - [NYC pickup locations](#nyc_pick)\n - [NYC dropff locations](#nyc_drop)\n\n### [Feature Engineering](#feature)\n - [Feature Selection](#select)\n - [Feature Extraction](#extract)\n\n### [Correlation Analysis](#corr)\n - [Heatmap](#heatmap)\n \n### [Model](#model)\n - [Multiple Linear Regression](#lin_reg)\n - [Random Forest Regressor](#rf_reg)\n - [XGBoost Regressor](#xgboost)\n\n### [Validation](#valid)\n - [Learning Curves](#curve)","f1081608":"There we go, our feature set is now ready for the feature selection model with 1s in the first column for a0 constant.\n\nLet's fit stats model on the X array to figure out an optimal set of features by recursively checking for the highest p value and removing the feature of that index.\n\n### Note:\nHere we will take the level of significance as 0.05 i.e. 5% which means that we will reject feature from the list of array and re-run the model till p value for all the features goes below .05 to find out the optimal combination for our model.","da89b8c4":"### Interesting find\n- There is approx **200% improvement** on the RMSE score for the Random forest regressor over the Linear regressor of the feature selection group.\n- Even the variance score is approx 1 which is a good score.\n- RMSE score for the RF regressor of feature extraction group is still very bad along with the variance score.\n- RMSE score for the feature selection group is more or less same as the raw data score. Sometimes the RMSE score for the raw data is better and vice versa. It fluctuates on every iteration and this is quite weird!\n\nLet's see if we can improve this further with the most sought after algorigthm i.e. XGBoost!!","e79f5b65":"<a id=select><\/a>\n## Feature Selection\n***\n#### Intuition\n - We will use **backward elimination** technique to select the best features to train our model. \n - It displays some statistical metrics with there significance value.\n - Like, It shows the **p** values for each feature as per its significance in the whole dataset.\n - It also shows the **adjusted R squared** values to identify whether removing or selecting the feature is beneficial or not.\n - For now we will only look at the P and adjusted R squared value to decide which features to keep and which needed to be removed.\n \nLet's assign the values to X & Y array from the dataset.","4d871142":"<a id=speed><\/a>\n## Speed\n***\nSpeed is a function of distance and time. Let's visualize speed in different trips.\n\nMaximum speed limit in NYC is as follows:\n- 25 mph in urban area i.e. **40 km\/h**\n- 65 mph on controlled state highways i.e. approx **104 km\/h**\n\n<img src='https:\/\/hips.hearstapps.com\/hmg-prod.s3.amazonaws.com\/images\/281\/gettyimages-56293248-speed-limit-image-source-1518561160.jpg' align='left'\/>","dd01934a":"<a id=heatmap><\/a>\n## Heatmap\n***\nA heatmap is a graphical representation of data that uses a system of color-coding to represent statistical relationship between different values.\n\nLet's plot the relationship between the features of the **Feature selection** group first\n","230341b7":"Vendor 2 takes the crown. Average trip duration for vendor 2 is higher than vendor 1 by approx 200 seconds i.e. atleast 3 minutes per trip.","fc4e833b":"### Observations:\n - Some trips are local some cover longer distance \n - Almost each day is listed against offline trips.\n - Offline trips were taken almost at all hours as per the search result.\n - There is no month which appears to be more dominant in the results.\n - Even the trip duration covers different scales.\n\nSo all in all there doesn't seems to be any relation with either of the metric for the offline trips. Let's move ahead","626c64ce":"<a id=week_trip><\/a>\n## Total trips per weekday\n***\nLet's take a look now at the distribution of taxi pickups across the week.","2b537e84":"<a id=hour_trip><\/a>\n## Total trips Per Hour\n***\nLet's take a look at the distribution of the pickups across the 24 hour time scale.","48069475":"<a id=lin_reg><\/a>\n## Multiple Linear Regression\n***\nIt is used to explain the relationship between one continuous dependent variable and two or more independent variables. Let's proceed","434a8daa":"### Question: \nWhy few features are not assigned to the X array like features at the index 2,3,10 were missed?\n\n### Idea:\n - **duration** variable assigned to Y because that is the dependent variable.\n - features such as  **id, timestamp** and **weekday** were not assigned to X array because they are of type object. And we need an array of float data type.\n \n#### Trick for backward elimination:\nGeneral equation for multiple linear regression is like\n> Y = a0 + a1x1 + a2x2 + ... + anxn\n\nSince, we dont have x0 in our X array so the regressor won't consider the constant value of the equation i.e. **a0**.\nSo to make it count in the equation we will append the selected feature set with a contant series of 1's as a first column. To make it appear like below equation to the statsmodel.\n> y = a0x0 + a1x1 + a2x2 + ... + anxn","6194bf7a":"### Model prediction","ffa553fe":"Above result shows that around 8K trips had to store the flag and then report to the server when the connection was established. Let's check the respective distribution with the vendors for the offline trips.","75299f71":"So it's a fairly equal distribution with average distance metric verying around 3.5 km\/h with Sunday being at the top may be due to outstation trips or night trips towards the airport.","34bef75f":"Here also the distibution is almost equivalent, varying mostly around 3.5 km\/h with 5th month being the highest in the average distance and 2nd month being the lowest.","b7f4169b":"### Observations:\n- Above result shows that all the offline trips were taken by vendor 1. We already know that vendor 2 has greater market share as compared to vendor 1. So, there can be two reasons for this scenario.\n\n 1. Either vendor 1 utilizes advance technology then vendor 2 to store and forward trip details in case of temporary signal loss.\n 2. Or vendor 1 uses poor infrastructure which often suffers from the server connection instability due to which they have to store the trip info in the vehicle and send it to the server later when the server connection is back.\n\nLet's check if there is some relation to the other metrics for these trips?","d210aaf0":"<a id=univariate><\/a>\n# Univariate Analysis\n***\nUnivariate analysis is the analysis of one variable. It's major purpose is to describe patterns in the data consisting of single variable.\n\n<img src='https:\/\/cdn.dribbble.com\/users\/207178\/screenshots\/2089878\/framer_bars.gif'\/>","d4dddfaa":"This is more or less same picture with both the vendors. Nothing more to analyze in this.","94bc8281":"<a id=split><\/a>\n### Split Data\nBefore training our model on the dataset, we need to split the dataset into training and testing datasets. This is required to train our model on the major part of our dataset and test the accuracy of the model on the minor part. Let's split it","98665e50":"<a id=train><\/a>\n## Model training\n***\nWe will first try with the default instantiation of the regressor object without using any generalization parameter. We will also **not perform any scaling** of the features because linear regression model takes care of that inherently. This is a plus point to use Linear regression model. It is quite fast to train even on very large datasets. So considering the size of our dataset this seems to be the correct approach as of now. Let's see how it performs.","67f65060":"### Observations\n- Average taxi speed is higher on weekend as compared to the weekdays which is obvious when there is mostly rush of office goers and business owners.\n- Even on monday the average taxi speed is shown higher which is quite surprising when it is one of the most busiest day after the weekend. There can be several possibility for such behaviour\n 1. Lot of customers who come back from outstation in early hours of Monday before 6 AM to attend office on time.\n 2. Early morning hours customers who come from the airports after vacation to attend office\/business on time for the coming week.\n- There could be some more reasons as well which only a local must be aware of. \n- We also can't deny the anomalies in the dataset. which is quite cumbersome to spot in such a large dataset.","eb7a823c":"### Observations:\n - There are some trips with more than 24 hours of travel duration i.e. 86400 seconds. Which might have occured on weekends for the outstation travels.\n - Major chunk of trips are completed within an interval of 1 hour with some good numbers of trips duration going above 1 hour.\n \nLet's look at those trips with huge duration, these are outliers and should be removed for the data consistency.","0707212d":"### Model training\n***\nWe will train the model on the filtered features. Our data has already been split so we will not split the data further.\n\n#### Note:\nWe used **GridSearch** to tune the **hyperparameters** of XGBoost regressor to get the best possible test score.  We will compare results from the default regressor and the tuned regressor.","cec4b752":"### Interesting find:\n- There are lots of trips which covered negligible distance but clocked more than 20,000 seconds in terms of the Duration.\n- Initially there is some proper correlation between the distance covered and the trip duration in the graph. but later on it all seems uncorrelated.\n- There were few trips which covered huge distance of approx 200 kms within very less time frame, which is unlikely and should be treated as outliers.\n\nLet's focus on the graph area where distance is < 50 km and duration is < 1000 seconds.","9f5950ad":"### Observation\n- Here we can see that almost 40 variables are needed for capturing atleast 99% of the variance in the training dataset. Hence we will use the same set of variables.","850b2e69":"<a id=predict><\/a>\n## Model prediction\n***\nSo now, our model has been fitted to the training set. It's time to predict the dependent variable. Let's do that now.","5df89f15":"### Observations\n- Around 6K trip record with distance equal to 0. Below are some possible explanation for such records.\n 1. Customer changed mind and cancelled the journey just after accepting it.\n 2. Software didn't recorded dropoff location properly due to which dropoff location is the same as the pickup location.\n 3. Issue with GPS tracker while the journey is being finished.\n 4. Driver cancelled the trip just after accepting it due to some reason. So the trip couldn't start\n 5. Or some other issue with the software itself which a technical guy can explain\n\nThere is some serious inconsistencies in the data where drop off location is same as the pickup location. We can't think off imputing the distance values considering a correlation with the duration because the dropoff_location coordinates would not be inline with the distance otherwise. We will look more to it in bivariate analysis with the Trip duration.","d7623399":"<a id=feature><\/a>\n## Feature Engineering\n***\nAfter looking at the dataset from different perspectives. Let's prepare our dataset before training our model. Since our dataset do not contain very large number of dimensions. We will first try to use feature selection instead of the feature extraction technique.\n\n<img src='https:\/\/www.netsoftlab.com\/images\/full-flash.gif' align='left'\/>","0ad819d6":"<a id=hour_duration><\/a>\n## Trip Duration per hour\n***\nWe need to aggregate the total trip duration to plot it agaist the month. The aggregation measure can be anything like sum, mean, median or mode for the duration. Since we already did the outlier analysis, so we can take the mean to visualize the pattern which should not result in the bias of the general trend.\n - Lets take a look.","71b7380b":"<a id=pickup><\/a>\n## Taxi pickup locations\n***\nLet's call the map_marker function to visualize all the pickup locations in the dataset.","f289b69b":"Now, Instead of looking at each and every trip, we should approximate and try to filter those trips which covered less than 1 km distance and but clocked more than an hour.","c0dd476b":"### Observations:\n- We can observe that both the models shows somewhat similar learning rate but with visible differences in error rates. \n- RF training curve initially starts high but later on improves as the training size increases and then seems to plateaud by the end.\n- XGBoost training curve on the other hand starts quite low and further improves with the increase in the training size and it too plateau towards the end.\n- Validation curve seems to show similar trend in both the models i.e. starts very high but improves with the training size with some differences in error rate i.e. XGBoost curve learning is quite fast and more accurate as compared to the RF one.\n- Both the models seems to suffer from **high variance** since the training curve error is very less in both the models.\n- The large gap at the end also indicates that the model suffers from quite a **low bias** i.e. overfitting the training data.\n- Also, both the model's still has potential to decrease and converge towards the training curve by the end.\n\n**At this point, here are a few things we could do to improve our model:**\n\n1. Add more training instances to improve validation curve in the XGBoost model.\n2. Increase the regularization for the learning algorithm. This should decrease the variance and increase the bias towards the validation curve.\n3. Reduce the numbers of features in the training data that we currently use. The algorithm will still fit the training data very well, but due to the decreased number of features, it will build less complex models. This should increase the bias and decrease the variance.","c6b7e1d7":"### Feature details:\n- id - a unique identifier for each trip\n- vendor_id - a code indicating the provider associated with the trip record\n- pickup_datetime - date and time when the meter was engaged\n- dropoff_datetime - date and time when the meter was disengaged\n- passenger_count - the number of passengers in the vehicle (driver entered value)\n- pickup_longitude - the longitude where the meter was engaged\n- pickup_latitude - the latitude where the meter was engaged\n- dropoff_longitude - the longitude where the meter was disengaged\n- dropoff_latitude - the latitude where the meter was disengaged\n- store_and_fwd_flag - This flag indicates whether the trip record was held in vehicle memory before sending to the vendor because the vehicle did not have a connection to the server - Y=store and forward; N=not a store and forward trip.\n\n### Label details:\n- trip_duration - duration of the trip in seconds ","d07f6062":"Quite a balance across the months here. It could have been more equivalent if we wouldn't have removed the inconsistent records in our study of the univariate analysis","5d8b9f70":"<a id=bivariate><\/a>\n# Bivariate Analysis\n***\nBivariate analysis is used to find out if there is a relationship between two sets of values. It usually involves the variables X and Y.\n\n\n<img src='https:\/\/i.pinimg.com\/originals\/c8\/d4\/0e\/c8d40e9ec4ffd4f3af527eb40ba80462.gif' align='left'\/>","fa8ada70":"### Observation\n- It's inline with the general trend of taxi pickups which starts increasing from 6AM in the morning and then declines from late evening i.e. around 8 PM. There is no unusual behavior here.","785d40ee":"<a id=nyc_drop><\/a>\n## NYC dropoff locations\n***\nNow let's take a look at the taxi dropoff's","7158773c":"### Interesting find:\n - Taxi pickups increased in the late night hours over the weekend possibly due to more outstation rides or for the late night leisures nearby activities.\n - Early morning pickups i.e before 5 AM have increased over the weekend in comparison to the office hours pickups i.e. after 7 AM which have decreased due to obvious reasons.\n - Taxi pickups seems to be consistent across the week at 15 Hours i.e. at 3 PM.","3bd513f7":"<a id=basemap><\/a>\n## Basemap\n***\nWe will utilize the matplotlib basemap toolkit library for plotting the maps. It is very lightweight and easy to use. Let's define a **map_marker** function for our purpose of visualizing long lat marker on the map.","161cd5d2":"<a id=hour_distance><\/a>\n## Distance per hour\n***\nNow, let us check how the distance is distributed against different variables. We know that trip distance must be more or less proportional to the trip duration if we ignore general traffic and other stuff on the road. Let's visualize this for each hour now. \n\nSince we have already done the outlier analysis for this variable as well. We can take the mean as aggregate measure for our visualizations.","82edf71d":"<a id=problem><\/a>\n# The Problem\n***\nBuild a machine learning model to predict the duration of NYC taxi trip.","478b2a3d":"We can observe that most of the trips took 0 - 30 mins to complete i.e. approx 1800 secs. Let's move ahead to next feature.","25f8c390":"## Thank you guys... Yayyy!!!!\n\n\n<img src='https:\/\/media.agoramt.com.br\/2018\/08\/Minions.gif' align='left'\/>","66d877c1":"<a id=nyc_pick><\/a>\n## NYC pickup locations\n***\nNow, let's focus on the taxi pickups in the NYC area. And for that we have to fix the Longitude and latitude to view only the NYC area map. Let's do that now and visualize the pickup locations.","89810dfa":"<a id=speed_hour><\/a>\n## Average speed per hour\n***\nLet's look at the average speed of NYC Taxi per hour.","3c6af04a":"There is no NaN\/NULL record in the dataset, So we dont have to impute any record.","21d6bd00":"<a id=corr><\/a>\n# Correlation Analysis\n***\nCorrelation analysis is a method of statistical evaluation used to study the strength of a relationship between two or more, numerically measured, continuous variables. This analysis is useful when we need to check if there are possible connections between variables. We will utilize Heatmap for our analysis.","e3225dc4":"### Observations:\n- Trip distance is highest during early morning hours which can account for some things like:\n 1. Outstation trips taken during the weekends.\n 2. Longer trips towards the city airport which is located in the outskirts of the city.\n- Trip distance is fairly equal from morning till the evening varying around 3 - 3.5 kms.\n- It starts increasing gradually towards the late night hours starting from evening till 5 AM and decrease steeply towards morning.","daa2a314":"<a id='distance'><\/a>\n## Distance\n***\nLet's now have a look on the distribution of the distance across the different types of rides.\n\n\n<img src='https:\/\/developmentandleadership.org\/wp-content\/uploads\/2016\/06\/iStock7114610XLARGEs-1024x680.jpg' align='left'\/>","af70f079":"Clear difference between the two operators for the average passenger count in all trips. It seems that vendor 2 trips generally consist of 2 passengers as compared to the vendor 1 with 1 passenger. Let's bifurcate it further.","28daa086":"### Observations:\n- There should have been a linear relationship between the distance covered and trip duration on an average but we can see dense collection of the trips in the lower right corner which showcase many trips with the inconsistent readings.\n\n### Idea:\nWe should remove those trips which covered 0 km distance but clocked more than 1 minute to make our data more consistent for predictive model. Because if the trip was cancelled after booking, than that should not have taken more than a minute time. This is our assumption.","5477600f":"Let's check whether those have been removed from the map or not?","7c79b519":"### Interesting find:\n- Many trips were done at a speed of over 200 km\/h. Going SuperSonic..!!\n\nLet's remove them and focus on the trips which were done at less than 104 km\/h as per the speed limits","a67026c5":"### Observations:\n- Average trip duration is lowest at 6 AM when there is minimal traffic on the roads.\n- Average trip duration is generally highest around 3 PM during the busy streets.\n- Trip duration on an average is similar during early morning hours i.e. before 6 AM & late evening hours i.e. after 6 PM.","bbf198fc":"<a id=passenger_vendor><\/a>\n## Passenger count per vendor\n***\nLet's try some different metric in the series i.e. passenger count. We will plot it agaist the vendor only because it will not be much helpful to plot it against hour, weekday or month like others as the passenger count should be a whole number and not a ratio. \n\nwe will take mean as the aggregate measure because we already did the outlier analysis on this metric. So our results woudn't be affected by some extreme values. Also if we take median than it will return only 1 because majorty of the trips have been taken by single passenger. Let's take a look about it's distribution.","d6def1e5":"### Observations:\n - Though the straight line tries to show some linear relation between the two. But there seems to be negligible correlation between these two metric as seen from the scatter plot where it should have been a linear distribution.\n - It is rarely occurs that customer keep sitting in the taxi for more than an hour and it does not travel for even 1 km.\n \nThese should be removed to bring in more consistency to our results.","f2bfaf64":"So this one is a different pickup but with the same story like the previous one i.e. the trip duration is not in correlation with the distance covered.\n\nLet's remove this one as well and call up the map_marker to check the latest picture.","47991b2d":"### Observations:\n- There are some trips with 0 passenger count. \n- Few trips consisted of even 7, 8 or 9 passengers. Clear outliers and pointers to data inconsistency\n- Most of trip consist of passenger either 1 or 2.\n\n### Idea:\nPassenger count is a driver entered value. Since the trip is not possible without passengers. It is evident that the driver forgot to enter the value for the trips with 0 passenger count. Lets analyze the passenger count distribution further to make it consistent for further analysis","fa34254f":"<a id=vendor_distance><\/a>\n## Distance per vendor\n***\nLet's check how both the vendors have covered the average distance during the trips","5855bd86":"<a id=final><\/a>\n## End Notes\n***\nIn this project we covered various aspects of the Machine learning development cycle. We observed that the data exploration and variable analysis is a very important aspect of the whole cycle and should be done for thorough understanding of the data. We also cleaned the data while exploring as there were some outliers which should be treated before feature engineering. Further we did feature engineering to filter and gather only the optimal features which are more significant and covered most of the variance in the dataset. Then finally we trained the models on the optimum featureset to get the results. ","60378ed8":"<a id=month_distance><\/a>\n## Distance per month\n***\nNow we will look at the average trip distance covered per month.","907ad4ca":"<a id=xgboost><\/a>\n## XGBoost Regressor\n***\nXGBoost (Extreme Gradient Boosting) is an optimized distributed gradient boosting library. It uses gradient boosting (GBM) framework at core. It belongs to a family of boosting algorithms that convert weak learners into strong learners. A weak learner is one which is slightly better than random guessing.\n\n'Boosting' here is a sequential process; i.e., trees are grown using the information from a previously grown tree one after the other. This process slowly learns from data and tries to improve its prediction in the subsequent iterations.","c1497bf4":"### Observation\n- The average trend is totally inline with the normal circumstances.\n- Average speed tend to increase after late evening and continues to increase gradually till the late early morning hours.\n- Average taxi speed is highest at 5 AM in the morning, then it declines steeply as the office hours approaches.\n- Average taxi speed is more or less same during the office hours i.e. from 8 AM till 6PM in the evening.","3d2af2c8":"### Observation\n- Here we can see an increasing trend of taxi pickups starting from Monday till Friday. The trend starts declining from saturday till monday which is normal where some office going people likes to stay at home for rest on the weekends. \n\nLet's drill down more to see the hourwise pickup pattern across the week","69cd0902":"<a id=passenger><\/a>\n## Passengers\n***\nNew York City Taxi Passenger Limit says:\n- A maximum of 4 passengers can ride in traditional cabs, there are also 5 passenger cabs that look more like minivans.\n- A child under 7 is allowed to sit on a passenger's lap in the rear seat in addition to the passenger limit.\n\nSo, in total we can assume that maximum 6 passenger can board the new york taxi i.e. 5 adult + 1 minor\n\n<img src='https:\/\/assets.rbl.ms\/15262493\/980x.gif' align='left'\/>","b33469ba":"### Observations:\n- We can see an increasing trend in the average trip duration along with each subsequent month. \n- The duration difference between each month is not much. It has increased gradually over a period of 6 months.\n- It is lowest during february when winters starts declining.\n- There might be some seasonal parameters like wind\/rain which can be a factor of this gradual increase in trip duration over a period. Like May is generally the considered as the wettest month in NYC and which is inline with our visualization. As it generally takes longer on the roads due to traffic jams during rainy season. So natually the trip duration would increase towards April May and June.","7bb3da4f":"Now the data is consistent with respect to the passenger count. Let's take a look at the ditribution with a graph below","e84345a0":"<a id=path><\/a>\n## Set Local Path\nWe need to set the local path to read and write to the file.","6d9002da":"There are approx **1.5 million records** in our dataset.","f0aed279":"### Interesting Find:\n\nDropoff's are much more distributed around the NYC area where still most of the dropoff's were done in the Manhattan. \n\n### Question:\n\nDoes that mean there were more dropoff's than the pickup's?\n\n### Idea:\n\nThough the dropoffs seems to be larger in number than the pickups. But we know that for each pickup we have a associated dropoffs in the dataset. It's just that the pickups were majorly concentrated in the Manhattan area.","8aef58ad":"### General inference\n- XGBoost proved to be much more efficient in predicting the output. But it takes much more time to train it over the large dataset wih more complexity as compared to the RF and Linear regression model but less time then the SVR.\n- It didn't helped us much to generalize the model by tuning hyper parameters for the RF model as there is not much difference in the RMSE scores of the default model and the tuned model of the feature selection group infact both varies on every iteration and sometimes the tuned model gives poor results than the default model. Though we tried many possible alterations with GSCV but the tuning could not achieve a significant improvement over the default model which also depends on the contents of the dataset.\n- Contrast to the RF regressor, XGBoost regressor prediction results were consistent on every iteration i.e. for each param configuration the results were the same.\n- Feature extraction didn't helped in anyway to improve the RMSE score with any of the regressor models. This shows us that the feature extraction is somewhat not a good technique to preprocess the data before feeding it into the regressor models for the continous target value prediction. Whereas it also depends on the type and features of data that how it behaves with the model.","96abf98c":"### Observations:\n - These trips ran for more than 20 days, which seems unlikely by the distance travelled.\n - All the trips are taken by vendor 1 which points us to the fact that this vendor might allows much longer trip for outstations.\n - All these trips are either taken on Tuesday's in 1st month or Saturday's in 2nd month. There might be some relation with the weekday, pickup location, month and the passenger.\n - But they fail our purpose of correct prediction and bring inconsistencies in the algorithm calculation.\n \nWe should get rid of them for the sake of data consistency. Those are **black** swans !!","240ae226":"We can see that though the trip duration is approx 8 minutes still the distance travelled is just in few meters. Moreover the Lat and Lon readings are same. That is a **black** swan and should be removed for the consistency of the model. Let's remove it now.","4e00217c":"### Interesting find:\n- Some trip durations are over 100000 seconds which are clear outliers and should be removed.\n\n### Observations:\n - There are some durations with as low as 1 second. which points towards trips with 0 km distance.\n - Major trip durations took between 10-20 mins to complete.\n - Mean and mode are not same which shows that trip duration distribution is skewed towards right.\n\nLet's analyze more","b5b332b0":"### Observations\n- There is a significant **improvement** in the RMSE score for the **tuned** XGBoost regressor over the Random forest regressor when trained on the feature selection group.\n- But the performance of the **default** XGBoost regressor is quite **worse** than the default RF regressor on the same data.\n- Also, the RMSE score on the raw data and feature selected data are same, which disproves the theory that it is always better to select the relevant features which are statistically important. As the data behaves differently in different models.\n- Not to mention the fact that RMSE score for the XGBoost regressor of the feature extraction group is still bad along with the variance score. ","3692cdd7":"<a id=speed_weekday><\/a>\n## Average speed per weekday\n***\nLet's visualize that on an average what is the speed of a taxi on any given weekday.","e8eed69a":"Also, we will remove the records with passenger count > 7, 8 or 9 as they are extreme values and looks very odd to be ocupied in a taxi. ","876fe824":"It is evident that most of the trips was taken by single passenger and that is inline with our day to day observations","0d11c323":"### Model training\n***\nNow we will train the model on the filtered features. Our data has already been split so we will not split the data further.\n\n#### Note:\nWe used **GridSearch** to tune the **hyperparameters** of random forest regressor to get the best possible test score. We tried various combination of the allowed hyper params values. _But any kind of combination could not produce significantly better results than the default settings. There can be many reasons for that and it totally depends on the type of data we have in hand. Therefore we will not show tuned regressor results here._","64531467":"### Interesting find:\n - It took **approx 1 second to train the model** on dataset of more than 1 million records.\n - It is evident that Linear regression model is **extremely fast** to train on the high dimension datasets consisting of even **millions** of records.\n - Linear regression object for the feature extraction group took less time to train on the input features.","e21226e9":"### Model Evaluation","48288e4b":"### Observations\nWe can see that none of the feature is linearly correlated with the target variable **\"46\"**. That is why it is not a good model for the prediction of the trip duration. So let's move ahead and try the **random forest regressor**. We are not using decision tree regressor because the random forest will anyways consist of almost all its properties. Also, we will not use SVR because it takes too much time to train on this huge dataset even with the default settings. It seems to be not good with high dimensional dataset as well as for the huge instances.","31021b8c":"<a id=flag_duration><\/a>\n## Trip duration v\/s Flag\n***\nLet's visualize if there is any effect of flag setting on the trip duration?","5fa58a8a":"### Observations: \n- Above result shows that only about 1% of the trip details were stored in the vehicle first before sending it to the server. This might have occured because of the following reasons:\n 1. Outstation trips didn't had proper connection at the time when trip completes.\n 2. Temporary loss of signals while the trip was about to finish\n 3. Inconsistent signal reception over the trip duration.\n 4. The GPS or mobile device battery was down when the trip finished.\n\nLet's check further\n"}}