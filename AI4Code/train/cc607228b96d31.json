{"cell_type":{"221afef3":"code","8ceaeca6":"code","71f68d4d":"code","37f0cd46":"code","4092ae8a":"code","7cc76cc2":"code","59ecd9d2":"code","8b6a75b3":"code","34ec12f7":"code","341daa06":"markdown"},"source":{"221afef3":"## Ref https:\/\/youtu.be\/uZalt-weQMM\nimport sys\npt_models = \"..\/input\/pretrained-models\/pretrained-models.pytorch-master\/\"\nsys.path.insert(0,pt_models)\nimport pretrainedmodels\n\nimport glob\nimport torch\nimport albumentations\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np # linear algebra\nfrom tqdm import tqdm\nfrom PIL import Image\nimport joblib\nimport torch.nn as nn\nfrom torch.nn import functional as F","8ceaeca6":"TEST_BATCH_SIZE = 32\nMODEL_MEAN=(0.485,0.465,0.406)\nMODEL_STD=(0.229,0.224,0.225)\nIMG_HEIGHT=137\nIMG_WIDTH=236\nDEVICE=\"cuda\"","71f68d4d":"class ResNet34(nn.Module):\n    def __init__(self ,pretrained):\n        super(ResNet34,self).__init__()\n        if pretrained is True:\n            self.model = pretrainedmodels.__dict__[\"resnet34\"](pretrained=\"imagenet\")\n        else:\n            self.model = pretrainedmodels.__dict__[\"resnet34\"](pretrained=None)\n\n        # To replace the last layer of the model with these\n        self.layer0 = nn.Linear(512,168) # 168 grapheme_root\n        self.layer1 = nn.Linear(512,11) # 11 vowel_diacritic\n        self.layer2 = nn.Linear(512,7) # 7 consonant_diacritic\n\n    def forward(self,x):\n#         print(x.shape)\n        batch_size ,_,_,_ = x.shape\n        x = self.model.features(x)\n        x = F.adaptive_avg_pool2d(x,1).reshape(batch_size, -1)\n        layer0 = self.layer0(x)\n        layer1 = self.layer1(x)\n        layer2 = self.layer2(x)\n        return layer0, layer1, layer2 # grapheme_root, vowel_diacritic, consonant_diacritic\n\n","37f0cd46":"class BengaliDatasetTest:\n    def __init__(self, df, img_height, img_width, mean, std):\n        \n        self.image_ids = df.image_id.values\n        self.img_arr = df.iloc[:, 1:].values\n\n        self.aug = albumentations.Compose([\n            albumentations.Resize(img_height, img_width, always_apply=True),\n            albumentations.Normalize(mean, std, always_apply=True)\n        ])\n\n\n    def __len__(self):\n        return len(self.image_ids)\n    \n    def __getitem__(self, item):\n        image = self.img_arr[item, :]\n        img_id = self.image_ids[item]\n        \n        image = image.reshape(137, 236).astype(float)\n        image = Image.fromarray(image).convert(\"RGB\")\n        image = self.aug(image=np.array(image))[\"image\"]\n        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n        \n\n        return {\n            \"image\": torch.tensor(image, dtype=torch.float),\n            \"image_id\": img_id\n        }","4092ae8a":"def model_predict():\n    g_pred, v_pred, c_pred = [], [], []\n    img_ids_list = [] \n    \n    for file_idx in range(4):\n        df = pd.read_parquet(f\"..\/input\/bengaliai-cv19\/test_image_data_{file_idx}.parquet\")\n\n        dataset = BengaliDatasetTest(df=df,\n                                    img_height=IMG_HEIGHT,\n                                    img_width=IMG_WIDTH,\n                                    mean=MODEL_MEAN,\n                                    std=MODEL_STD)\n\n        data_loader = torch.utils.data.DataLoader(\n            dataset=dataset,\n            batch_size= TEST_BATCH_SIZE,\n            shuffle=False,\n            num_workers=4\n        )\n\n        for bi, d in enumerate(data_loader):\n            image = d[\"image\"]\n            img_id = d[\"image_id\"]\n            image = image.to(DEVICE, dtype=torch.float)\n\n            g, v, c = model(image)\n            #g = np.argmax(g.cpu().detach().numpy(), axis=1)\n            #v = np.argmax(v.cpu().detach().numpy(), axis=1)\n            #c = np.argmax(c.cpu().detach().numpy(), axis=1)\n\n            for ii, imid in enumerate(img_id):\n                g_pred.append(g[ii].cpu().detach().numpy())\n                v_pred.append(v[ii].cpu().detach().numpy())\n                c_pred.append(c[ii].cpu().detach().numpy())\n                img_ids_list.append(imid)\n        \n    return g_pred, v_pred, c_pred, img_ids_list","7cc76cc2":"model = ResNet34(pretrained=False)\nTEST_BATCH_SIZE = 32\nStart_fold=3\nfinal_g_pred = []\nfinal_v_pred = []\nfinal_c_pred = []\nfinal_img_ids = []\n\nfor i in range(Start_fold,5):\n    model.load_state_dict(torch.load(f\"..\/input\/bengali-models\/50 Epoch\/resnet34_fold{i}.bin\"))\n#     print(model)\n    model.to(DEVICE)\n    model.eval()\n    g_pred, v_pred, c_pred, img_ids_list = model_predict()\n#     print(img_ids_list)\n    final_g_pred.append(g_pred)\n    final_v_pred.append(v_pred)\n    final_c_pred.append(c_pred)\n#     print(final_c_pred)\n    if i == Start_fold:\n        final_img_ids.extend(img_ids_list)","59ecd9d2":"final_g = np.argmax(np.mean(np.array(final_g_pred), axis=0), axis=1)\nfinal_v = np.argmax(np.mean(np.array(final_v_pred), axis=0), axis=1)\nfinal_c = np.argmax(np.mean(np.array(final_c_pred), axis=0), axis=1)\n# print(final_g)\n# print(final_img_ids)\npredictions = []\nfor ii, imid in enumerate(final_img_ids):\n\n    predictions.append((f\"{imid}_grapheme_root\", final_g[ii]))\n    predictions.append((f\"{imid}_vowel_diacritic\", final_v[ii]))\n    predictions.append((f\"{imid}_consonant_diacritic\", final_c[ii]))\n","8b6a75b3":"sub = pd.DataFrame(predictions,columns=[\"row_id\",\"target\"])\nprint(sub)","34ec12f7":"sub.to_csv(\"submission.csv\",index=False)","341daa06":"This kernel is from [@Abhishek Thakur](https:\/\/www.youtube.com\/channel\/UCBPRJjIWfyNG4X-CRbnv78A) youtube channel\n\n[Bengali.AI: Handwritten Grapheme Classification Using PyTorch (Part-1)](https:\/\/www.youtube.com\/watch?v=8J5Q4mEzRtY)\n\n[Bengali.AI: Handwritten Grapheme Classification Using PyTorch (Part-2)](https:\/\/www.youtube.com\/watch?v=uZalt-weQMM&t=3970s)"}}