{"cell_type":{"2672006d":"code","30d12b97":"code","f4f853ef":"code","a797f09a":"code","7bac71a1":"code","e3be1042":"code","eecbec77":"code","90530196":"code","5f4e2379":"code","b1278bd4":"code","f52a9955":"code","490aa6d4":"code","3f261b2a":"code","f2720831":"code","0ac025e2":"code","e723cb1d":"code","5324b5b2":"code","ddde9784":"code","01fcaeb3":"code","a417c841":"code","b2ab9d0a":"markdown"},"source":{"2672006d":"import pandas as pd\nimport numpy as np\nimport gc\n\n# Gradient Boosting\nimport lightgbm as lgb\nimport xgboost as xgb\n\n# Scikit-learn\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.model_selection import StratifiedKFold \nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n\n# Graphics\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Skopt functions\nfrom skopt import BayesSearchCV\nfrom skopt import gp_minimize # Bayesian optimization using Gaussian Processes\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.utils import use_named_args # decorator to convert a list of parameters to named arguments\nfrom skopt.callbacks import DeadlineStopper # Stop the optimization before running out of a fixed budget of time.\nfrom skopt.callbacks import VerboseCallback # Callback to control the verbosity\nfrom skopt.callbacks import DeltaXStopper # Stop the optimization If the last two positions at which the objective has been evaluated are less than delta\n\n# Hyperparameters distributions\nfrom scipy.stats import randint\nfrom scipy.stats import uniform\n\n# Metrics\nfrom sklearn.metrics import average_precision_score, roc_auc_score, mean_absolute_error\n\nimport os\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn","30d12b97":"santander_data = pd.read_csv('..\/input\/train.csv')\nsantander_data_test = pd.read_csv('..\/input\/test.csv')","f4f853ef":"# Taking the labels (price)\nlabel_df = santander_data['target']","a797f09a":"santander_data.drop(['ID_code','target'], axis=1, inplace=True)\n\nsantander_data_test.drop('ID_code', axis=1, inplace=True)\nsantander_data.head(10)","7bac71a1":"santander_data_test.head(10)","e3be1042":"santander_data.describe()","eecbec77":"santander_data[santander_data.isnull().any(axis=1)]","90530196":"santander_data.select_dtypes(exclude=np.number).columns","5f4e2379":"len_train = len(santander_data)\nlen_train","b1278bd4":"#Merge test and train\nmerged = pd.concat([santander_data, santander_data_test])\n#Saving the list of original features in a new list `original_features`.\noriginal_features = merged.columns\nmerged.shape","f52a9955":"idx = features = merged.columns.values[0:200]\nfor df in [merged]:\n    df['sum'] = df[idx].sum(axis=1)  \n    df['min'] = df[idx].min(axis=1)\n    df['max'] = df[idx].max(axis=1)\n    df['mean'] = df[idx].mean(axis=1)\n    df['std'] = df[idx].std(axis=1)\n    df['skew'] = df[idx].skew(axis=1)\n    df['kurt'] = df[idx].kurtosis(axis=1)\n    df['med'] = df[idx].median(axis=1)","490aa6d4":"print(\"Total number of features: \",merged.shape[1])","3f261b2a":"train_df = merged.iloc[:len_train]\ntrain_df.head()","f2720831":"X_test = merged.iloc[len_train:]\nX_test.head()","0ac025e2":"def augment(x,y,t=2):\n    xs,xn = [],[]\n    for i in range(t):\n        mask = y>0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xs.append(x1)\n\n    for i in range(t\/\/2):\n        mask = y==0\n        x1 = x[mask].copy()\n        ids = np.arange(x1.shape[0])\n        for c in range(x1.shape[1]):\n            np.random.shuffle(ids)\n            x1[:,c] = x1[ids][:,c]\n        xn.append(x1)\n\n    xs = np.vstack(xs)\n    xn = np.vstack(xn)\n    ys = np.ones(xs.shape[0])\n    yn = np.zeros(xn.shape[0])\n    x = np.vstack([x,xs,xn])\n    y = np.concatenate([y,ys,yn])\n    return x,y","e723cb1d":"\"\"\"train_df = santander_data\nX_test = santander_data_test\"\"\"\ndel santander_data\ndel santander_data_test\ngc.collect()","5324b5b2":"skf_three= StratifiedKFold(n_splits=7, shuffle=False, random_state=2319)","ddde9784":"param = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.335,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.041,\n    'learning_rate': 0.0083,\n    'max_depth': -1,\n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 8,\n    'tree_learner': 'serial',\n    'objective': 'binary', \n    'verbosity': -1\n}","01fcaeb3":"# Create arrays and dataframes to store results\noof_preds = np.zeros(train_df.shape[0])\nsub_preds = np.zeros(len(X_test))\nfeats = [f for f in train_df.columns]\n    \nfor n_fold, (train_idx, valid_idx) in enumerate(skf_three.split(train_df[feats], label_df)):\n    X_train, y_train = train_df.iloc[train_idx][feats], label_df.iloc[train_idx]\n    X_valid, y_valid = train_df.iloc[valid_idx][feats], label_df.iloc[valid_idx]\n    \n    X_tr, y_tr = augment(X_train.values, y_train.values)\n    X_tr = pd.DataFrame(X_tr)\n    \n    print(\"Fold idx:{}\".format(n_fold + 1))\n    trn_data = lgb.Dataset(X_tr, label=y_tr)\n    val_data = lgb.Dataset(X_valid, label=y_valid)\n        \n    clf = lgb.train(param, trn_data,1000000, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 3000)\n        \n\n    oof_preds[valid_idx] = clf.predict(train_df.iloc[valid_idx][feats], num_iteration=clf.best_iteration)\n    sub_preds += clf.predict(X_test[feats], num_iteration=clf.best_iteration) \/ 7\n\n\nprint('Full AUC score %.6f' % roc_auc_score(label_df, oof_preds))\n\npred3=sub_preds","a417c841":"sample_submission = pd.read_csv('..\/input\/sample_submission.csv')\nsample_submission['target'] = pred3\nsample_submission.to_csv('submission.csv', index=False)","b2ab9d0a":"**Data Augment**\nAugmentation is a method to increase the amount of training data by randomly shuffle\/transform the features in a certain way. It improves accuracy by letting the model see more cases of both \"1\" and \"0\" samples in training so the model can generalize better to new data.\n\nThanks to Jiwei Lu for teaching this new concept . *https:\/\/www.kaggle.com\/jiweiliu\/lgb-2-leaves-augment*"}}