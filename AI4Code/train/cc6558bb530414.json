{"cell_type":{"4c06d4be":"code","baf6420c":"code","dbe7188c":"code","d8a69e87":"code","94bc9ae5":"code","dc5ff26f":"code","d1ffabc0":"code","9e3a8c1b":"code","186cadb6":"code","cccf6064":"code","62cb608f":"code","a6431042":"code","47309225":"code","d312330b":"code","7d6acf6a":"code","311d8d84":"code","96217b83":"code","2dbaf4d4":"code","1f3d07dd":"code","b8166158":"code","af67e1bd":"code","4862097f":"code","5579a01c":"code","5bd099f0":"code","7316e4b3":"code","7c31628e":"code","ae36a813":"code","bfcb18a6":"code","d8254589":"code","ed940a5b":"code","a80d2948":"code","24056017":"code","0fd85b85":"code","4cb54476":"code","6e29cd82":"markdown","b82563c3":"markdown","7c32f6ab":"markdown","d0529e57":"markdown","0e3784ab":"markdown","9e7ac859":"markdown","000eebd9":"markdown","bd35e4ee":"markdown","c599036f":"markdown","43f7bfd7":"markdown","250e7d5b":"markdown","5c90b06e":"markdown","09d34116":"markdown","2fd4a8fe":"markdown","9ca8e320":"markdown","2b73d1c2":"markdown","6653ac3c":"markdown","a53ea5c6":"markdown","9a03ce1c":"markdown","91b5ad4b":"markdown"},"source":{"4c06d4be":"import numpy as np \nimport pandas as pd \nimport os\nimport warnings\n!pip install plotly==4.1.0\nimport plotly\nfrom plotly.offline import iplot\nwarnings.filterwarnings(\"ignore\")\nprint(plotly.__version__)\nimport seaborn as sns\nimport sklearn\nfrom sklearn.impute import KNNImputer\nimport matplotlib.pyplot as plt\nimport pandas_profiling as pp\nfrom sklearn.metrics import mean_absolute_error\nimport optuna\nimport optuna.integration.lightgbm as lgb\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\nfrom sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nimport lightgbm as lgb\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom plotly.offline import plot, iplot, init_notebook_mode\ninit_notebook_mode(connected=True)\nimport optuna\nfrom optuna import Trial, visualization\nfrom optuna.samplers import TPESampler\nfrom lightgbm import LGBMClassifier","baf6420c":"path=r\"\/kaggle\/input\/water-potability\/water_potability.csv\"\ndata = pd.read_csv(path)\ndata.head()\nprint(\"Data shape is {}\".format(data.shape))","dbe7188c":"#checking an arbitrary NaN value and its type\nnumm_example=data[\"ph\"]\nprint(\"NaN value check : \", numm_example[0],\" , type of NaN value :  \", type(numm_example[0]))","d8a69e87":"data.head()","94bc9ae5":"data.dtypes","dc5ff26f":"data.describe()","d1ffabc0":"data['Potability'].value_counts()","9e3a8c1b":"cat_col = data.select_dtypes(\"object\").columns.tolist()\ncat_col","186cadb6":"pp.ProfileReport(data)","cccf6064":"data.isnull().sum()","62cb608f":"missinglist_ph = list(data[\"ph\"].isnull()) \nmissinglist_sulfate = list( data[\"Sulfate\"].isnull()) \nmissinglist_Trihalomethanes=list(data[\"Trihalomethanes\"].isnull()) \n\nindex_missing_ph = np.where(missinglist_ph)  \nindex_missing_sulfate=np.where(missinglist_sulfate) \nindex_missing_Trihalomethanes=np.where(missinglist_Trihalomethanes) ","a6431042":"df_ph = data[\"ph\"]\ndf_sulfate = data[\"Sulfate\"]\ndf_Trih = data[\"Trihalomethanes\"]\nprint(\"ph dim:\",df_ph.shape)\nprint(\"sulfate dim:\",df_sulfate.shape)\nprint(\"Trihalomethanes dim:\",df_Trih.shape)","47309225":"df_ph2 = df_ph.drop(df_ph.index[index_missing_ph])\nprint(\"ph dim: \",df_ph2.shape)\n\ndf_sulfate2 = df_sulfate.drop(df_sulfate.index[index_missing_sulfate])\nprint(\"sulfate dim:\",df_sulfate2.shape)\n\ndf_Trih2 = df_Trih.drop(df_Trih.index[index_missing_Trihalomethanes])\nprint(\"Trihalomethanes dim:\",df_Trih2.shape)","d312330b":"g=sns.distplot(df_ph2, color=\"r\",  label=\"PH w\/o nulls\")\ng.legend()\nplt.show()","7d6acf6a":"g=sns.distplot(df_sulfate2, color=\"r\",  label=\"Sulfate w\/o nulls\")\ng.legend()\nplt.show()","311d8d84":"g=sns.distplot(df_Trih2, color=\"r\",  label=\"Trihalomethanes w\/o nulls\")\ng.legend()\nplt.show()","96217b83":"data_dropped = data.dropna()\nprint(\"new shape: \",data_dropped.shape)\ndata_dropped.isnull().sum()","2dbaf4d4":"colnames=[]\nfor col in data.columns:\n    colnames.append(col)    \nprint (colnames)","1f3d07dd":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(strategy='mean')\nimp.fit(data)\ndata_impute_mean = imp.transform(data)\ndata_impute_mean = pd.DataFrame(data_impute_mean, columns = colnames)\ndata_impute_mean.isnull().sum()","b8166158":"data_impute_mean.head()","af67e1bd":"from sklearn.impute import SimpleImputer\nimp_med = SimpleImputer(strategy='median')\nimp_med.fit(data)\ndata_impute_med = imp_med.transform(data)\ndata_impute_med = pd.DataFrame(data_impute_med, columns = colnames)\ndata_impute_med.isnull().sum()","4862097f":"from sklearn.impute import KNNImputer\nimputer_knn = KNNImputer(n_neighbors=8, weights=\"uniform\")\nimputer_knn.fit(data)\ndata_impute_knn = imputer_knn.transform(data)\ndata_impute_knn = pd.DataFrame(data_impute_knn, columns = colnames)\ndata_impute_knn.isnull().sum()","5579a01c":"X = data_impute_med.iloc[:,:-1].values\ny = data_impute_med[\"Potability\"].values","5bd099f0":"sns.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\nplt.figure(figsize=(8, 5))\nplt.title(\"Potability Class Distribution Before SMOTE\")\nsns.countplot(x=\"Potability\", data=data, palette='pastel');","7316e4b3":"X_train, X_test, y_train, y_test = train_test_split( X , y, test_size=0.2)\nprint(\"X_train shape before smote: {} andn\/ y_train shape before smote: {}\".format(X_train.shape,y_train.shape))\nprint(type(X_train))","7c31628e":"y_train = y_train.astype(int)","ae36a813":"from imblearn.over_sampling import SMOTE \n\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\nprint(\"X_train shape after smote: {} andn\/ y_train shape after smote: {}\".format(X_train_smote.shape,y_train_smote.shape))","bfcb18a6":"y_dist= pd.DataFrame(data=y_train_smote, index=range(y_train_smote.shape[0]),columns=[\"Potability\"])\nsns.set_style(\"ticks\", {\"xtick.major.size\": 8, \"ytick.major.size\": 8})\nplt.figure(figsize=(8, 5))\nplt.title(\"Potability Class Distribution After SMOTE\")\nsns.countplot(x=\"Potability\", data=y_dist, palette='pastel');","d8254589":"def objective(trial):\n\n    params = {\n        'reg_alpha': trial.suggest_float('reg_alpha', 0.001, 10.0),\n        'reg_lambda': trial.suggest_float('reg_lambda', 0.001, 10.0),\n        'num_leaves': trial.suggest_int('num_leaves', 10, 502),\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 102),\n        'max_depth': trial.suggest_int('max_depth', 2, 6),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.25),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.6),\n        'cat_smooth' : trial.suggest_int('cat_smooth', 10, 102),\n        'cat_l2': trial.suggest_int('cat_l2', 1, 20),\n        'min_data_per_group': trial.suggest_int('min_data_per_group', 1, 200),\n        'n_estimators': trial.suggest_int('n_estimators', 300, 10000),\n        'random_state': 42,\n        'metric': 'binary_logloss'\n    }\n    \n    model = LGBMClassifier(**params)    \n    return cross_val_score(model, X_train_smote, y_train_smote, cv = 10, scoring = 'accuracy').mean()\noptuna.logging.set_verbosity(optuna.logging.WARNING) \nstudy = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials=150, show_progress_bar=True)\n\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)\n","ed940a5b":"optuna.visualization.plot_optimization_history(study)","a80d2948":"optuna.visualization.plot_slice(study)","24056017":"print(study.best_params)","0fd85b85":"dtrain = lgb.Dataset(X, label=y)\nbest_params = {\n    \"objective\": \"binary\",\n    \"metric\": \"binary_error\",\n    \"verbosity\": -1,\n    \"seed\": 42} \nbest_params.update(study.best_params)\nbest_params\n\n#Actual training with best parameters\nlgbfit = LGBMClassifier(**best_params,\n                   \n                   verbose_eval=False )\n\nlgbfit.fit(X,y)","4cb54476":"from sklearn.metrics import accuracy_score\n\ny_predd = lgbfit.predict(X_test)\nrecall_score(y_test, y_predd)\nacc = accuracy_score(y_test, y_predd)\nacc= acc*100\nprint(\"accuracy score of validation is {} %  .\".format(acc))","6e29cd82":"## Missing Value Handling","b82563c3":"There are some ways to handle missing values in a dataframe. One should select the suitable method according to nature of the dataset and nature of the feature which contains missing values!! Hence there should be some exploratory analysis of those features first.\n\n* Deleting Rows with missing values\n* Impute missing values with Mean\n* Impute missing values with Median\n* Prediction of missing values (w\/ ML or DL or Datawig [works fine categorical and non-numerical features.])\n","7c32f6ab":"# Conclusion\n\nAs conclusion we will investigate Water Quality dataset. We did following steps:\n\n\n* We use a strategy to impute the missing values first. We check the nature of the featuures to use right imputing technique. We select median imputing since we do not have skew but translation.\n\n* Afterwards, we split a part of data for the validation. This data is not used both in SMOTE and LightGBM training. \n\n* Then after spliting the train and validation set we balanced the data with using SMOTE technique. \n\n* For finding best parameters of LightGBM we use OPTUNA hyperparameter optimization engine. The important thing is, we have to use OPTUNA with crossvalidation!! Otherwise, our training will overfit without any doubts.\n\n* After finding the best parameters, we train the LightGBM again.\n\n* We test already splitted data with our model.","d0529e57":"### Validation set split","0e3784ab":"## $\\color{Pink}{\\text{Table of Contents}}$\n\n* [Chapter 1. Introduction](#chapter1)     \n* [Chapter 2. EDA & Missing Value Handling](#chapter2)\n* [Chapter 3. SMOTE](#chapter3)\n* [Chapter 4. OPTUNA & LightGBM Classifier](#chapter4)\n\n\n\n\n\n ##### ****$\\color{Black}{\\text{If You like my work, Please upvote!}}$****","9e7ac859":"## $\\color{Pink}{\\text{Chapter 4. OPTUNA & LightGBM Classifier}}$ <a class=\"anchor\" id=\"chapter4\"><\/a>","000eebd9":"Main Hyperparameters of LightGBM are:\n\n* num_leaves [  Value more than  2^(max_depth) this will result in overfitting. ]\n* min_data_in_leaf  \n* max_depth\n* learning_rate\n* num_iterations\n* regularization\n* max_bin\n* boosting type\n* Cat_l2\n* min_child_samples\n\nBoosting types:\n\n* \"gbdt\" : traditional Gradient Boosting Decision Tree\n\n* \"rf\" : Random Forest\n\n* \"dart\" : Dropouts meet Multiple Additive Regression Trees\n\n* \"goss\" : Gradient-based One-Side Sampling\n\nWe will use gdbt which is default parameter.","bd35e4ee":"We will check the distribution of the features without null walues, so we want to find the location (index) where missing values occur.","c599036f":" According to distrubition of the missing values, using **median imputer** will not affect the features' charactaristics so that we will use med imputer. You are free to use other imputing types.","43f7bfd7":"For Hyperparameter optimization there are many methods such grid search and random search. Even if those old school methods work fine in many cases, for large datasets and for better accuracy matching there are softwares\/libraries such as Optuna, Hyperopt , Spearmint , SMAC , Autotune , and Vizier. In this notebook i will try to use Optuna with one of on edge classifier called LightGBM.","250e7d5b":"## $\\color{Pink}{\\text{Chapter 3. SMOTE}}$ <a class=\"anchor\" id=\"chapter3\"><\/a>\n\n\nIt is nothing but a type of data augmentation for the unbalanced class and it is referred to as the **Synthetic Minority Oversampling Technique** (SMOTE). SMOTE is a better way of increasing the number of rare cases than simply duplicating existing cases. You are able to connect the SMOTE module to a dataset that is imbalanced. There are many reasons why a dataset might be imbalanced: the category you are targeting might be very rare in the population, or the data might simply be difficult to collect. Typically, you use SMOTE when the class you want to analyze is under-represented. The module returns a dataset that contains the original samples, plus an additional number of synthetic minority samples, depending on the percentage you specify.\n\nsource : Chawla, N. V., Bowyer, K. W., Hall, L. O., & Kegelmeyer, W. P. (2002). SMOTE: synthetic minority over-sampling technique. Journal of artificial intelligence research, 16, 321-357.","5c90b06e":"![kaggle (3).jpg](attachment:f59b37ec-eaa1-4b17-b7d3-d48b49db4a43.jpg)","09d34116":"# Optuna Hyperparameter Optimization with LightGBM Classifier","2fd4a8fe":"## $\\color{Pink}{\\text{Chapter 2. EDA & Missing Value Handling}}$ <a class=\"anchor\" id=\"chapter2\"><\/a>","9ca8e320":"## C. Imputing by K Nearest Neighbours Algorithm","2b73d1c2":"# Setting Best Parameters to the Model","6653ac3c":"## B. Imputing By Mean or Median","a53ea5c6":"## A. Handling with Dropping NA Values","9a03ce1c":"## $\\color{Pink}{\\text{Chapter 1. Introduction}}$ <a class=\"anchor\" id=\"chapter1\"><\/a>","91b5ad4b":"We clean the null values with droping JUST FOR EDA. We will decide to fill them or drop them according to analysis."}}