{"cell_type":{"b222cf85":"code","75a57dcf":"code","a6918fa1":"code","586e8643":"code","f0cc96d3":"code","133da81f":"code","800ebe0f":"code","e1bd99ef":"code","a8063a93":"code","4a88b7eb":"code","7140edfe":"code","db187500":"code","8b8393db":"code","475ce167":"code","e0acbccc":"code","ea13c361":"code","f3934112":"code","13ea8d66":"code","3bc7f06c":"code","8ce15634":"code","b7068e6a":"code","28821ca9":"code","c20ed30a":"code","b93ba8f2":"code","c83ccf47":"code","bf9f1007":"code","97e21856":"code","bcfb17de":"code","1da900e3":"code","3e18a57d":"code","910d0e13":"code","627e44ef":"code","ffab81b6":"code","d756b325":"code","6206250b":"code","6e8b3594":"code","786fed28":"code","85168d48":"code","1708a080":"code","53df6420":"code","56c4c5a4":"code","18d807d0":"code","09cae86b":"code","005997f8":"code","6c855722":"code","4534dbf8":"code","c3c0f198":"code","cd72accc":"code","71213666":"code","6c42e3be":"code","6b697c8a":"code","e895e153":"code","9f00217a":"code","fd3a79b3":"code","c5c057eb":"code","fc75d8e2":"code","5f6c547a":"code","67311401":"code","16a8d5ae":"code","e4f0b95d":"code","6e36ccba":"code","905e56c2":"code","44f7bccd":"code","a7b53653":"code","7df4160f":"code","6a423f5d":"code","8f7b58db":"code","bb6b9452":"code","d7ac02a9":"code","38232324":"code","69f3d322":"code","342d5461":"code","45a2e81f":"code","8cff8715":"code","75b9271d":"code","a9aec3ac":"code","e559ac28":"code","3bd6dc89":"code","04a6be10":"code","8d4f2532":"code","b2876db0":"code","dbb45b64":"code","fe1e822b":"code","e9220a8d":"code","32a95c9a":"code","72835486":"code","f95ce833":"code","e3af3319":"code","dc0f8d10":"code","b75e22f3":"code","d82c8993":"code","41ad8bd3":"code","65435a58":"code","ae0bd293":"code","25117da7":"code","0ddb1120":"code","e43d00d0":"code","97138c61":"code","08c8f396":"code","9877dde6":"code","bbb40e5a":"code","8492e783":"code","ab32fb8f":"code","7dcf6e14":"code","c51db941":"code","185807c4":"code","faab39c8":"markdown","ae44edae":"markdown","c4c342cf":"markdown","5b33887b":"markdown","42c42f00":"markdown","b8431af8":"markdown","b9f91d19":"markdown","98125f0b":"markdown","2b440e8b":"markdown","fe95d81e":"markdown","5dcdad94":"markdown","eedbbfbf":"markdown","b966284c":"markdown","7fb7af31":"markdown","773c3a16":"markdown","31c3ee08":"markdown","21305f5b":"markdown","565e7227":"markdown","62aec255":"markdown","20d9b4f6":"markdown","71382efa":"markdown","11e35285":"markdown","5e7d2085":"markdown","7d4b4b61":"markdown","fa85bf9d":"markdown","2bf6bf36":"markdown","2fdd7e8f":"markdown","3aa4824c":"markdown","d7b924a3":"markdown","0f9969c6":"markdown","ec9b42f7":"markdown","bd4c6805":"markdown","ba2e29c9":"markdown","18cfe9bd":"markdown","fddcc348":"markdown","054edcce":"markdown","d1c2b42c":"markdown","8c04df97":"markdown","b1b507ce":"markdown","293cc2cc":"markdown","474f6b94":"markdown","3d174a47":"markdown","a9b8343d":"markdown","e2f1c3eb":"markdown","502981e9":"markdown","4f6d7f30":"markdown","e6f5b4d2":"markdown"},"source":{"b222cf85":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import  LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\nfrom sklearn.model_selection import GridSearchCV, StratifiedKFold, KFold\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom mlxtend import plotting\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.decomposition import PCA\n\nimport statsmodels.api as sm\n\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials, space_eval\nfrom hyperopt.pyll.base import scope\nfrom hyperopt.pyll.stochastic import sample\nfrom xgboost import XGBClassifier\n","75a57dcf":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a6918fa1":"#\u0142adowanie datasetu o zawa\u0142ach serca\ndf = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndf","586e8643":"#usuniecie kolumny ID\ndel df[\"id\"]","f0cc96d3":"#sprawdzamy typy zmiennych\ndf.info()","133da81f":"#sprawdzamy ile jest NA\ndf.isna().sum()","800ebe0f":"#zamienimy na w kolumnie BMI, przy wykorzystaniu SimpleImputer z pakietu sklearn\nimputer = SimpleImputer()\n\npom1 = df[\"bmi\"].to_frame()\ndf[\"bmi\"]=imputer.fit_transform(pom1)","e1bd99ef":"#poznajmy troch\u0119 nasze dane:\ndf[df.columns[1:]].describe()","a8063a93":"df = pd.read_csv('..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv')\ndel df[\"id\"]","4a88b7eb":"df = df[~df['bmi'].isnull()] ","7140edfe":"# poznajmy inne dane\nzmienne_ciagle= df[df.columns[1:]].select_dtypes(include = [\"float64\",\"int64\"])\nzmienne_ciagle[zmienne_ciagle.columns[[1,2]]].boxplot()\npass","db187500":"zmienne_ciagle[zmienne_ciagle.columns[[0,3,4]]].boxplot()\npass","8b8393db":"zmienne_kategoryczne = df.select_dtypes(include = \"object\").join(zmienne_ciagle[zmienne_ciagle.columns[[1,2]]])\nfor i in zmienne_kategoryczne.columns:\n    plt.hist(df[i])\n    print(\"plot of\", i)\n    plt.show()","475ce167":"for i in zmienne_kategoryczne.columns:\n    print(f\"Struktura zmiennej {i} \\n{df[i].value_counts()\/df.shape[0]} \\n\")","e0acbccc":"# zmiana kolumn kategorycznych na typ category\nfor i in df.select_dtypes(include = 'object').join(df[[\"hypertension\",\"heart_disease\"]]):\n       df[i]=df[i].astype('category')","ea13c361":"# wyznaczenie dummy variables\ndf_dummy = pd.get_dummies(df,drop_first= True)","f3934112":"# zdefiniujmy zbi\u00f3r X i Y bez podzialu na testowy do ewentualnej cross validacji\nX = df_dummy.iloc[:,0:3].join(df_dummy.iloc[:,4:])\nY = df_dummy[\"stroke\"]","13ea8d66":"# podzielmy te\u017c na zbi\u00f3r treningowy i testowy\nX_train, X_test, Y_train,Y_test = train_test_split(X,Y, test_size = 0.2)","3bc7f06c":"# prosty model drzewa decyzyjnego, nr1\nmodel_dd1 = DecisionTreeClassifier(random_state=1).fit(X_train, Y_train)\n\nY_pred_dd1 =  model_dd1.predict(X_test)\n\n#ocena modelu na danych treningowych i testowych \nprint(\"acc train: \",round(model_dd1.score(X_train, Y_train),4), \"\\n\",\n      \"acc test: \", round(model_dd1.score(X_test, Y_test),4))","8ce15634":"#obejrzyjmy jeszcze macierz pomy\u0142ek \nmatrix_dd1 = confusion_matrix(Y_test, Y_pred_dd1)\nraport_dd1 = classification_report(Y_test, Y_pred_dd1)\nprint(raport_dd1)","b7068e6a":"# stw\u00f3rzmy najpierw zestaw parametr\u00f3w do test\u00f3w\nparams_dd2 = {\"max_depth\": list(range(2,16,3)),\n              \"max_features\" : list(range(3,16,3)),\n              \"max_leaf_nodes\" : list(range(2,53,10))}","28821ca9":"clf = DecisionTreeClassifier(random_state=1) \n\n#zdefiniowalem tez wlasny scoring, poniewaz acc byl identyczny dla kilkunastu zestawow parametrow\ngrid_search_dd2 = GridSearchCV(clf, param_grid = params_dd2, cv = 5, verbose = 1, scoring = make_scorer(f1_score, average = \"macro\")).fit(X,Y)","c20ed30a":"# wyswietle teraz najlepszy model\n\ngrid_search_dd2.best_params_","b93ba8f2":"# wdroze ten model dd2 i przetestuje jego wynik\nmodel_dd2 = DecisionTreeClassifier(max_depth=11, max_features=9, max_leaf_nodes=52,random_state=1).fit(X_train, Y_train)\n\nY_pred_dd2 =  model_dd2.predict(X_test)\n\n#ocena modelu na danych treningowych i testowych \nprint(\"acc train: \",round(model_dd2.score(X_train, Y_train),4), \"\\n\",\n      \"acc test: \", round(model_dd2.score(X_test, Y_test),4))\n\n#obejrzyjmy jeszcze macierz pomy\u0142ek \nmatrix_dd2 = confusion_matrix(Y_test, Y_pred_dd2)\nraport_dd2 = classification_report(Y_test, Y_pred_dd2)\nprint(raport_dd2)","c83ccf47":"# prosty model drzewa decyzyjnego, nr1\nmodel_ll1 = RandomForestClassifier(random_state=1).fit(X_train, Y_train)\n\nY_pred_ll1 =  model_ll1.predict(X_test)\n\n#ocena modelu na danych treningowych i testowych \nprint(\"acc train: \",round(model_ll1.score(X_train, Y_train),4), \"\\n\",\n      \"acc test: \", round(model_ll1.score(X_test, Y_test),4))\n","bf9f1007":"#obejrzyjmy jeszcze macierz pomy\u0142ek \nmatrix_ll1 = confusion_matrix(Y_test, Y_pred_ll1)\nraport_ll1 = classification_report(Y_test, Y_pred_ll1)\nprint(raport_ll1)","97e21856":"# stw\u00f3rzmy najpierw zestaw parametr\u00f3w do test\u00f3w\nparams_ll2 = {\"max_depth\": list(range(2,36,3)),\n              \"max_features\" : list(range(3,16,3)),\n              \"max_leaf_nodes\" : list(range(2,58,5)),\n              \"n_estimators\" : list(range(50,200,10))}\n\nr=  0   # policzymy ile jest mozliwych kombinacji parametrow\nfor j in params_ll2.values():\n    s=len(j)\n    r += s","bcfb17de":"# wylosujmy najlepsze kombinacje parametr\u00f3w dla tego modelu.\nclf1 = RandomForestClassifier(random_state=1) \n\nrandom  = RandomizedSearchCV(clf1, param_distributions = params_ll2, cv = 5, n_iter = round(r\/3,0), scoring = make_scorer(f1_score, average = \"macro\"), random_state = 1)\n\nrandom.fit(X,Y)","1da900e3":"# otrzymamy liste 10 najlepszych zestawow parametrow z posrod wylosowanych\nranks_ll2 = random.cv_results_['rank_test_score']\nranks_ll2.sort()\nn = 10\n\nfor i in range(n):\n    print(random.cv_results_['params'][ranks_ll2[i]])","3e18a57d":"# dodajmy do modelu wylosowane najlepsze parametry\nnew_params_ll2 = {\"max_depth\": list(range(10,21,2)),\n              \"max_features\" : [10,12],\n              \"max_leaf_nodes\" : list(range(20,31,2)),\n              \"n_estimators\" : list(range(70,100,10))}","910d0e13":"# wybierzmy teraz najlepszy zestaw parametrow z okrojonej listy parametrow\ngrid_search_ll2 = GridSearchCV(clf1, param_grid = new_params_ll2, cv = 5, verbose = 1, scoring = make_scorer(f1_score, average = \"macro\")).fit(X,Y)\n","627e44ef":"# mamy proponowany najlepszy zestaw parametrow\ngrid_search_ll2.best_params_","ffab81b6":"# sprawdzmy jak sie sprawuje nasz model ze wskazanymi parametrami\nmodel_ll2 =  RandomForestClassifier(max_depth= 14, max_features = 9,max_leaf_nodes= 22, n_estimators = 70, random_state=1).fit(X_train, Y_train)\n\nY_pred_ll2 =  model_ll2.predict(X_test)\n\n#ocena modelu na danych treningowych i testowych \nprint(\"acc train: \",round(model_ll2.score(X_train, Y_train),4), \"\\n\",\n      \"acc test: \", round(model_ll2.score(X_test, Y_test),4))\n","d756b325":"#obejrzyjmy jeszcze macierz pomy\u0142ek \nmatrix_ll2 = confusion_matrix(Y_test, Y_pred_ll2)\nraport_ll2 = classification_report(Y_test, Y_pred_ll2)\nprint(raport_ll2)","6206250b":"#zdefiniujmy najpierw zakres wag jakie chcemy pr\u00f3bowa\u0107 w modelu\nweights = np.linspace(0.1,0.90,50)\n\nparam_grid_weight = {'class_weight': [{0:x, 1:1.0-x} for x in weights]}","6e8b3594":"# wybierzmy teraz najlepszy zestaw wag dla modelu\ngrid_search_ll3 = GridSearchCV(model_ll2, param_grid = param_grid_weight, cv = 5, verbose = 1, scoring = make_scorer(f1_score, average = \"macro\")).fit(X,Y)\n","786fed28":"#w ten sposob uzyskalismy najlepszy model przy ponizszych wagach\ngrid_search_ll3.best_params_","85168d48":"# sprawdzmy jak sie sprawuje nasz model ze wskazanymi parametrami\nmodel_ll3 =  RandomForestClassifier(max_depth= 14, max_features = 9,max_leaf_nodes= 22, n_estimators = 70,\n                                   class_weight= {0: 0.1163265306122449, 1: 0.883673469387755},random_state=1).fit(X_train, Y_train)\n\nY_pred_ll3 =  model_ll3.predict(X_test)\n\n#ocena modelu na danych treningowych i testowych \nprint(\"acc train: \",round(model_ll3.score(X_train, Y_train),4), \"\\n\",\n      \"acc test: \", round(model_ll3.score(X_test, Y_test),4))\n","1708a080":"#obejrzyjmy jeszcze macierz pomy\u0142ek \nmatrix_ll3 = confusion_matrix(Y_test, Y_pred_ll3)\nraport_ll3 = classification_report(Y_test, Y_pred_ll3)\nprint(raport_ll3)","53df6420":"# sprobujmy przypisac ze decyzja o przypisaniu do klasy 1 jest podejmowana gdy prawdopodobienstwo wynosi >0.3, >0,4, >0.6 lub >0.8\ndecyzje = pd.DataFrame( model_ll3.predict_proba(X_test)[::,1], columns= [\"a\"])\n\nlista_03 = []\nlista_04 = []\nlista_06 = []\nlista_08 = []\nfor i in decyzje[\"a\"]:\n    if i >0.6:\n        lista_06.append(1)\n    else:\n        lista_06.append(0)\n    if i >0.8:\n        lista_08.append(1)\n    else:\n        lista_08.append(0)\n    if i >0.4:\n        lista_04.append(1)\n    else:\n        lista_04.append(0)\n    if i >0.3:\n        lista_03.append(1)\n    else:\n        lista_03.append(0)","56c4c5a4":"# przeanalizujmy jak w tych wypadkach wyglada nasza macierz pomylek\n\nmatrix_ll3_06 = confusion_matrix(Y_test, lista_06)\nraport_ll3_06 = classification_report(Y_test, lista_06)\nprint(raport_ll3_06)","18d807d0":"matrix_ll3_08 = confusion_matrix(Y_test, lista_08)\nraport_ll3_08 = classification_report(Y_test, lista_08)\nprint(raport_ll3_08)","09cae86b":"matrix_ll3_04 = confusion_matrix(Y_test, lista_04)\nraport_ll3_04 = classification_report(Y_test, lista_04)\nprint(raport_ll3_04)","005997f8":"matrix_ll3_03 = confusion_matrix(Y_test, lista_03)\nraport_ll3_03 = classification_report(Y_test, lista_03)\nprint(raport_ll3_03)","6c855722":"#przeskalowanie danych dla calego zbioru treningowego\nscaler = StandardScaler().fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n","4534dbf8":"# wybranie odpowiedniej ilosci komponentow PCA\n\n## najpierw max kolumn, a potem sprawdzic ile jest nam rzeczywiscie potrzebnych \nPCA_results = PCA(n_components = 16)\n\nPCA_results.fit(X_train_scaled)\nX_train_scaled_PCA = PCA_results.transform(X_train_scaled)\nX_test_scaled_PCA = PCA_results.transform(X_test_scaled) \n\nplt.plot(PCA_results.explained_variance_ratio_)","c3c0f198":"np.cumsum(PCA_results.explained_variance_ratio_)","cd72accc":"#ostateczna wersja PCA\nPCA_results = PCA(n_components = 11)\n\nPCA_results.fit(X_train_scaled)\nX_train_scaled_PCA = PCA_results.transform(X_train_scaled)\nX_test_scaled_PCA = PCA_results.transform(X_test_scaled) ","71213666":"# sprawdzmy jak sie sprawuje nasz model ze wskazanymi parametrami\nmodel_ll4 =  RandomForestClassifier(max_depth= 14, max_features = 9,max_leaf_nodes= 22, n_estimators = 70,\n                                   class_weight= {0: 0.1163265306122449, 1: 0.883673469387755}, random_state= 1).fit(X_train_scaled_PCA, Y_train)\n\nY_pred_ll4 =  model_ll4.predict(X_test_scaled_PCA)\n\n#ocena modelu na danych treningowych i testowych \nprint(\"acc train: \",round(model_ll4.score(X_train_scaled_PCA, Y_train),4), \"\\n\",\n      \"acc test: \", round(model_ll4.score(X_test_scaled_PCA, Y_test),4))\n\n","6c42e3be":"#obejrzyjmy jeszcze macierz pomy\u0142ek \nmatrix_ll4 = confusion_matrix(Y_test, Y_pred_ll4)\nraport_ll4 = classification_report(Y_test, Y_pred_ll4)\nprint(raport_ll4)","6b697c8a":"# stw\u00f3rzmy najpierw zestaw parametr\u00f3w do test\u00f3w\nparams_ll5 = {\"max_depth\": list(range(2,36,3)),\n              \"max_features\" : list(range(3,12,2)),\n              \"max_leaf_nodes\" : list(range(2,58,5)),\n              \"n_estimators\" : list(range(50,200,10))}\n\nr=  0   # policzymy ile jest mozliwych kombinacji parametrow\nfor j in params_ll2.values():\n    s=len(j)\n    r += s","e895e153":"# wylosujmy najlepsze kombinacje parametr\u00f3w dla tego modelu.\nclf5 = RandomForestClassifier(random_state=1) \n\nrandom  = RandomizedSearchCV(clf5, param_distributions = params_ll5, cv = 5, n_iter = round(r\/3,0), scoring = make_scorer(f1_score, average = \"macro\"))\n\nrandom.fit(X_train_scaled_PCA, Y_train)","9f00217a":"# otrzymamy liste 10 najlepszych zestawow parametrow z posrod wylosowanych\nranks_ll5 = random.cv_results_['rank_test_score']\nranks_ll5.sort()\nn = 10\n\nfor i in range(n):\n    print(random.cv_results_['params'][ranks_ll5[i]])\n","fd3a79b3":"# dodajmy do modelu wylosowane najlepsze parametry\nnew_params_ll5 =  {\"max_depth\": list(range(2,36,3)),\n              \"max_features\" : list(range(3,12,2)),\n              \"max_leaf_nodes\" : list(range(12,58,5)),\n              \"n_estimators\" : list(range(70,180,20))}","c5c057eb":"# wybierzmy teraz najlepszy zestaw parametrow z okrojonej listy parametrow\ngrid_search_ll5 = GridSearchCV(clf5, param_grid = new_params_ll5, cv = 5, verbose = 1, scoring = make_scorer(f1_score, average = \"macro\")).fit(X,Y)\n","fc75d8e2":"# mamy proponowany najlepszy zestaw parametrow\ngrid_search_ll5.best_params_","5f6c547a":"# sprawdzmy jak sie sprawuje nasz model ze wskazanymi parametrami\nmodel_ll5 =  RandomForestClassifier(max_depth= 29, max_features = 11,max_leaf_nodes= 52, n_estimators = 70, random_state=1).fit(X_train_scaled_PCA, Y_train)\n\nY_pred_ll5 =  model_ll5.predict(X_test_scaled_PCA)\n\n#ocena modelu na danych treningowych i testowych \nprint(\"acc train: \",round(model_ll5.score(X_train_scaled_PCA, Y_train),4), \"\\n\",\n      \"acc test: \", round(model_ll5.score(X_test_scaled_PCA, Y_test),4))","67311401":"#obejrzyjmy jeszcze macierz pomy\u0142ek \nmatrix_ll5 = confusion_matrix(Y_test, Y_pred_ll5)\nraport_ll5 = classification_report(Y_test, Y_pred_ll5)\nprint(raport_ll5)","16a8d5ae":"# wybierzmy teraz najlepszy zestaw wag dla modelu\ngrid_search_ll6 = GridSearchCV(model_ll5, param_grid = param_grid_weight, cv = 5, verbose = 1, scoring = make_scorer(f1_score, average = \"macro\")).fit(X,Y)\n","e4f0b95d":"#w ten sposob uzyskalismy najlepszy model przy ponizszych wagach\ngrid_search_ll6.best_params_","6e36ccba":"# sprawdzmy jak sie sprawuje nasz model ze wskazanymi parametrami\nmodel_ll6 =  RandomForestClassifier(max_depth= 14, max_features = 9,max_leaf_nodes= 22, n_estimators = 70,\n                                   class_weight= {0: 0.1163265306122449, 1: 0.883673469387755}, random_state= 1).fit(X_train_scaled_PCA, Y_train)\n\nY_pred_ll6 =  model_ll6.predict(X_test_scaled_PCA)\n\n#ocena modelu na danych treningowych i testowych \nprint(\"acc train: \",round(model_ll6.score(X_train_scaled_PCA, Y_train),4), \"\\n\",\n      \"acc test: \", round(model_ll6.score(X_test_scaled_PCA, Y_test),4))","905e56c2":"#obejrzyjmy jeszcze macierz pomy\u0142ek \nmatrix_ll6 = confusion_matrix(Y_test, Y_pred_ll6)\nraport_ll6 = classification_report(Y_test, Y_pred_ll6)\nprint(raport_ll6)","44f7bccd":"# sprobujmy przypisac ze decyzja o przypisaniu jest podejmowana gdy prawdopodobienstwo wynosi >0,4, >0.6, >0.8\ndecyzje2 = pd.DataFrame( model_ll6.predict_proba(X_test_scaled_PCA)[::,1], columns= [\"a\"])\n\nlista_04_2 = []\nlista_06_2 = []\n\nfor i in decyzje2[\"a\"]:\n    if i >0.6:\n        lista_06_2.append(1)\n    else:\n        lista_06_2.append(0)\n    if i >0.4:\n        lista_04_2.append(1)\n    else:\n        lista_04_2.append(0)","a7b53653":"# przeanalizujmy jak w tych wypadkach wyglada nasza macierz pomylek\nmatrix_ll6_06 = confusion_matrix(Y_test, lista_06_2)\nraport_ll6_06 = classification_report(Y_test, lista_06_2)\nprint(raport_ll6_06)","7df4160f":"# przeanalizujmy jak w tych wypadkach wyglada nasza macierz pomylek\nmatrix_ll6_04 = confusion_matrix(Y_test, lista_04_2)\nraport_ll6_04 = classification_report(Y_test, lista_04_2)\nprint(raport_ll6_04)","6a423f5d":"#powiekszmy sobie nasz zbior dla klasy 1. o 2000 losowych obserwacji\nlosowe_indeksy = np.random.randint(1,249, 2000)\nzbior1 = df_dummy[df_dummy.stroke == 1]\n\nduplikaty = pd.DataFrame()\nfor i in losowe_indeksy:\n    zbior1 = zbior1.append(zbior1.iloc[i:i+1,])\n    \nzbior1.reset_index()\n    \ndf_dummy_duplicated = df_dummy[df_dummy.stroke == 0].append(zbior1)","8f7b58db":"# zdefiniujmy zbi\u00f3r X i Y bez podzialu na testowy do cross validacji\nX_duplicated = df_dummy_duplicated.iloc[:,0:3].join(df_dummy.iloc[:,4:])\nY_duplicated  = df_dummy_duplicated [\"stroke\"]","bb6b9452":"# podzielmy te\u017c na zbi\u00f3r treningowy i testowy\nX_train_duplicated, X_test_duplicated, Y_train_duplicated,Y_test_duplicated = train_test_split(X_duplicated,Y_duplicated, test_size = 0.2, random_state=1)","d7ac02a9":"# stw\u00f3rzmy najpierw zestaw parametr\u00f3w do test\u00f3w\nparams_ll7 =  {\"max_depth\": list(range(2,36,3)),\n              \"max_features\" : list(range(3,12,2)),\n              \"max_leaf_nodes\" : list(range(2,58,5)),\n              \"n_estimators\" : list(range(70,200,20))}\n\n\nr=  0   # policzymy ile jest mozliwych kombinacji parametrow\nfor j in params_ll2.values():\n    s=len(j)\n    r += s","38232324":"# wylosujmy najlepsze kombinacje parametr\u00f3w dla tego modelu.\nclf7 = RandomForestClassifier(random_state=1) \n\nrandom  = RandomizedSearchCV(clf7, param_distributions = params_ll7, cv = 5, n_iter = round(r\/3,0), scoring = make_scorer(f1_score, average = \"macro\"))\n\nrandom.fit(X_train_duplicated, Y_train_duplicated)","69f3d322":"# otrzymamy liste 10 najlepszych zestawow parametrow z posrod wylosowanych\nranks_ll7 = random.cv_results_['rank_test_score']\nranks_ll7.sort()\nn = 10\n\nfor i in range(n):\n    print(random.cv_results_['params'][ranks_ll7[i]])","342d5461":"# sprawdzmy od razu czy jest sens dalej szukac parametrow, czyli jak sie sprawuje nasz model ze wskazanymi parametrami\nmodel_ll7 =  RandomForestClassifier(max_depth= 20, max_features = 11,max_leaf_nodes= 27, n_estimators = 110,random_state=1).fit(X_train_duplicated, Y_train_duplicated)\n\nY_pred_ll7 =  model_ll7.predict(X_test_duplicated)\n\n#ocena modelu na danych treningowych i testowych \nprint(\"acc train: \",round(model_ll7.score(X_train_duplicated, Y_train_duplicated),4), \"\\n\",\n      \"acc test: \", round(model_ll7.score(X_test_duplicated, Y_test_duplicated),4))","45a2e81f":"#obejrzyjmy jeszcze macierz pomy\u0142ek \nmatrix_ll7 = confusion_matrix(Y_test_duplicated, Y_pred_ll7)\nraport_ll7 = classification_report(Y_test_duplicated, Y_pred_ll7)\nprint(raport_ll7)","8cff8715":"model_lr3 = LogisticRegression(random_state=1)\nmodel_lr3.fit(X_train_scaled_PCA, Y_train)\nY_pred_lr3 = model_lr3.predict(X_test_scaled_PCA)","75b9271d":"#ocena modelu na danych treningowych i testowych \nprint(\"acc train: \",round(model_lr3.score(X_train_scaled_PCA, Y_train),4), \"\\n\",\n      \"acc test: \", round(model_lr3.score(X_test_scaled_PCA, Y_test),4))","a9aec3ac":"#obejrzyjmy jeszcze macierz pomy\u0142ek \nmatrix_lr3 = confusion_matrix(Y_test, Y_pred_lr3)\nraport_lr3 = classification_report(Y_test, Y_pred_lr3)\nprint(raport_lr3)","e559ac28":"model_ll3","3bd6dc89":"print(\"nasz wynik to: \\n\", raport_ll3)","04a6be10":"#wyliczmy AUC-ROC dla naszego najlepszego modelu\nY_pred_proba_ll3 = model_ll3.predict_proba(X_test)[::,1]\nFPR, TPR,_ = roc_curve(Y_test,Y_pred_proba_ll3)\nAUC = roc_auc_score(Y_test,Y_pred_proba_ll3)\n\nplt.plot(FPR, TPR, label = \"wynik \" + str(AUC))\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.show()","8d4f2532":"#Sprawdzmy jak rzeczywiscie prezentuje sie modele przy klasyfikacji od progu 0.8","b2876db0":"#przypiszmy, wi\u0119c decyzje na podstawie prawdopodobienstwo 0.2 i wyzej do klasy 1\n\ndecyzje = pd.DataFrame( model_ll3.predict_proba(X_test)[::,1], columns= [\"a\"])\n\nlista_02 = []\nfor i in decyzje[\"a\"]:\n    if i >0.2:\n        lista_02.append(1)\n    else:\n        lista_02.append(0)","dbb45b64":"matrix_ll3_02 = confusion_matrix(Y_test, lista_02)\nraport_ll3_02 = classification_report(Y_test, lista_02)\nprint(raport_ll3_02)","fe1e822b":"# prosty model xgb, nr1\n\nmodel_xgb1= XGBClassifier(objective = 'binary:logistic',random_state=1).fit(X_train, Y_train, eval_metric = \"logloss\")\n\nY_pred_xgb1 =  model_xgb1.predict(X_test)\n\n#ocena modelu na danych treningowych i testowych \nprint(\"acc train: \",round(model_xgb1.score(X_train, Y_train),4), \"\\n\",\n      \"acc test: \", round(model_xgb1.score(X_test, Y_test),4))\n\n","e9220a8d":"#obejrzyjmy jeszcze macierz pomy\u0142ek \nmatrix_xgb1 = confusion_matrix(Y_test, Y_pred_xgb1)\nraport_xgb1 = classification_report(Y_test, Y_pred_xgb1)\nprint(raport_xgb1)","32a95c9a":"#dane musza byc w odpowiednim formacie, dlatego dostosujmy df do formatu array\nX_train_array = X_train.values\nX_test_array = X_test.values\n\n#dostosujmy wartosci Y, za pomoca LabelEncoder\nle  = LabelEncoder()\nY_train_encoded = le.fit_transform(Y_train)\nY_test_encoded = le.fit_transform(Y_test)","72835486":"def run_xgb_hyperopt(X,Y,cv, num_eval):\n    def objective(parametry): #zdefiniujmy funkcje celu\n        model = XGBClassifier(**parametry, random_state = 10, objective = \"binary:logistic\",\n                              n_jobs = 2,use_label_encoder=False, scale_pos_weight = 10)\n                \n        for a, b in cv.split(X, Y):\n            eval_set = [(X[a],Y[a]),(X[b],Y[b])]\n            model.fit(X[a],Y[a],eval_set = eval_set, eval_metric = 'logloss', early_stopping_rounds = 10, verbose = 0) \n            y_pred = model.predict(X[b])\n            score =cohen_kappa_score(Y[b],y_pred)\n           # print(\"kappa score: {:.4f}\".format(score))\n            return{'loss': -score, 'status':STATUS_OK}\n        \n    parametry ={\n        'max_depth': scope.int(hp.quniform('max_depth',5,15,1)),\n        'n_estimators' : hp.choice('n_estimators', np.arange(20, 150, 2, dtype=int)),\n        'learning_rate' : hp.quniform('learning_rate', 0.001, 0.5, 0.005),\n        'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n        'subsample': hp.uniform('subsample', 0.7, 1),\n        'gamma' : hp.quniform('gamma', 0.5, 1, 0.05),\n        'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1, 0.05)\n        }  \n    \n    trials = Trials()\n    \n    best = fmin(fn=objective,\n                space = parametry,\n                algo = tpe.suggest,\n                max_evals = num_eval,\n                trials = trials)\n    hyperparams = space_eval(parametry, best)\n    \n    print(\"Najlepsze parametry: \", hyperparams)\n    return hyperparams","f95ce833":"#zdefiniujmy jeszcze rodzaj walidacji, jako wykorzystamy\nkF = StratifiedKFold(n_splits = 5, shuffle= True, random_state= 10)","e3af3319":"# odaplenie hyeropt\nbest_XGB = run_xgb_hyperopt(X_train_array, Y_train_encoded, cv=kF, num_eval=20)\n","dc0f8d10":"# uruchomie teraz nasz najlepszy model\nmodel_xgb2 = XGBClassifier(**best_XGB, objective= \"binary:logistic\",use_label_encoder=False,random_state=1).fit(X_train,Y_train_encoded, eval_metric = 'logloss')\nY_pred_xgb2 = model_xgb2.predict(X_test)","b75e22f3":"#obejrzyjmy jeszcze macierz pomy\u0142ek \nmatrix_xgb2 = confusion_matrix(Y_test_encoded, Y_pred_xgb2)\nraport_xgb2 = classification_report(Y_test_encoded, Y_pred_xgb2)\nprint(raport_xgb2)","d82c8993":"import tensorflow as tf\nfrom keras import layers\nfrom tensorflow.keras import layers, callbacks\nfrom sklearn.preprocessing import OneHotEncoder\nfrom imblearn.over_sampling import SMOTE\n","41ad8bd3":"#konstrukcja modelu\nmodel = tf.keras.models.Sequential()\nmodel.add(layers.InputLayer(input_shape = X_train_array.shape[1]))","65435a58":"# transformujemy y_test\n\nY_train = np.array(Y_train)\nY_test = np.array(Y_test)\nY_test = OneHotEncoder().fit_transform(Y_test.reshape(-1,1)).toarray()\n# zmienmy jeszcze format\nY_test = Y_test.astype(np.float32)\n\n# transformujemy y_train\nY_train = OneHotEncoder().fit_transform(Y_train.reshape(-1,1)).toarray()\n# zmienmy jeszcze format\nY_train = Y_train.astype(np.float32)","ae0bd293":"#definicja modelu\ndef model_nn(X,Y, units, drop, classNum, epoch, lr = 0.01, bn = False):\n    '''\n    X - dane treningowe w postaci array bez labeli,\n    Y - labeli dla danych treningowych\n    units - lista unitow dla poszczegolnych warstw\n    drop - poziom dropoutu dla poszczegolnej wartswy\n    classNum = ilosc klas \n    epoch - liczba epok podczas trenowania \n    lr - learning rate\n    bn - czy zastosowac batch normalization \n    '''\n    model = tf.keras.models.Sequential()\n    model.add(layers.InputLayer(input_shape = X.shape[1]))\n    for i, j in enumerate(units):\n        model.add(layers.Dense(j, activation='relu', name=\"ukryta_\" + str(i)))\n        model.add(layers.Dropout(drop))\n        if bn == True:\n            model.add(layers.BatchNormalization())\n                  \n    model.add(layers.Dense(classNum, name=\"WYJSCIE\"))\n    model.add(layers.Softmax())\n    \n    model.summary()\n    \n    optymalizator = tf.optimizers.Adam(lr)\n    \n    model.compile(optimizer=optymalizator,\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=['AUC'])\n    \n    results = model.fit(X,Y, validation_split= 0.2, epochs=epoch, verbose=1,\n              callbacks=[callbacks.EarlyStopping(monitor= 'val_auc', min_delta=0.1, \n                                                    patience=5)])\n    return model","25117da7":"model_nn1 = model_nn(X_train_array, Y_train,[512,256,64],0.3,2,30)\nY_pred_nn1 = model_nn1.predict(X_test_array).argmax(axis = 1)","0ddb1120":"# szybka ewaluacja\neval_res = model_nn1.evaluate(X_test_array, Y_test)\neval_names = model_nn1.metrics_names\nprint(f\"{eval_names[0]}: {eval_res[0]}\")\nprint(f\"{eval_names[1]}: {eval_res[1]}\")","e43d00d0":"#obejrzyjmy jeszcze macierz pomy\u0142ek \nmatrix_nn1 = confusion_matrix(Y_test.argmax(axis = 1), Y_pred_nn1)\nraport_nn1 = classification_report(Y_test.argmax(axis = 1), Y_pred_nn1)\nprint(raport_nn1)","97138c61":"model_nn2 = model_nn(X_train_array, Y_train,[256,128],0.5,2,30,0.1)\nY_pred_nn2 = model_nn2.predict(X_test_array).argmax(axis = 1)\n\n# szybka ewaluacja\neval_res = model_nn2.evaluate(X_test_array, Y_test)\neval_names = model_nn2.metrics_names\nprint(f\"{eval_names[0]}: {eval_res[0]}\")\nprint(f\"{eval_names[1]}: {eval_res[1]}\")\n\n#obejrzyjmy jeszcze macierz pomy\u0142ek \nmatrix_nn2 = confusion_matrix(Y_test.argmax(axis = 1), Y_pred_nn2)\nraport_nn2 = classification_report(Y_test.argmax(axis = 1), Y_pred_nn2)\nprint(raport_nn2)","08c8f396":"#Sprobujmy wykorzystac inne metode replikacje danych, dla poradzenia sobie z problemem niezbalansowania.\n\nsmote = SMOTE(sampling_strategy = 'minority')\nx_sm, y_sm = smote.fit_resample(X,Y)\n\n\n# podzielmy jeszcze raz zbior na zbior treningowy i testowy\nX_train1, X_test1, Y_train1,Y_test1 = train_test_split(x_sm,y_sm, test_size = 0.2)","9877dde6":"# Przetworzmy dane do formatu wejsciowego do modelu\n\nX_train_array_d = X_train1.values\nX_test_array_d = X_test1.values\n\n\n# transformujemy Y_test_duplicated i Y_test_duplicated\n\nY_train_d = np.array(Y_train1)\nY_test_d = np.array(Y_test1)\n\nY_test_d = OneHotEncoder().fit_transform(Y_test_d.reshape(-1,1)).toarray()\n# zmienmy jeszcze format\nY_test_d = Y_test_d.astype(np.float32)\n\n# transformujemy y_train\nY_train_d = OneHotEncoder().fit_transform(Y_train_d.reshape(-1,1)).toarray()\n# zmienmy jeszcze format\nY_train_d = Y_train_d.astype(np.float32)\n","bbb40e5a":"model_nn3 = model_nn(X_train_array_d, Y_train_d,[256,128],0.5,2,30,0.1, True)\nY_pred_nn3 = model_nn3.predict(X_test_array_d).argmax(axis = 1)\n\n# szybka ewaluacja\neval_res = model_nn3.evaluate(X_test_array_d, Y_test_d)\neval_names = model_nn3.metrics_names\nprint(f\"{eval_names[0]}: {eval_res[0]}\")\nprint(f\"{eval_names[1]}: {eval_res[1]}\")\n\n#obejrzyjmy jeszcze macierz pomy\u0142ek \nmatrix_nn3 = confusion_matrix(Y_test_d.argmax(axis = 1), Y_pred_nn3)\nraport_nn3 = classification_report(Y_test_d.argmax(axis = 1), Y_pred_nn3)\nprint(raport_nn3)","8492e783":"model_nn4 = model_nn(X_train_array_d, Y_train_d,[512],0.5,2,30,0.001, True)\nY_pred_nn4 = model_nn4.predict(X_test_array_d).argmax(axis = 1)\n\n# szybka ewaluacja\neval_res = model_nn4.evaluate(X_test_array_d, Y_test_d)\neval_names = model_nn4.metrics_names\nprint(f\"{eval_names[0]}: {eval_res[0]}\")\nprint(f\"{eval_names[1]}: {eval_res[1]}\")\n\n#obejrzyjmy jeszcze macierz pomy\u0142ek \nmatrix_nn4 = confusion_matrix(Y_test_d.argmax(axis = 1), Y_pred_nn4)\nraport_nn4 = classification_report(Y_test_d.argmax(axis = 1), Y_pred_nn4)\nprint(raport_nn4)","ab32fb8f":"model_nn5 = model_nn(X_train_array_d, Y_train_d,[512,128],0.2,2,30,0.0001, True)\nY_pred_nn5 = model_nn5.predict(X_test_array_d).argmax(axis = 1)\n\n# szybka ewaluacja\neval_res = model_nn5.evaluate(X_test_array_d, Y_test_d)\neval_names = model_nn5.metrics_names\nprint(f\"{eval_names[0]}: {eval_res[0]}\")\nprint(f\"{eval_names[1]}: {eval_res[1]}\")\n\n#obejrzyjmy jeszcze macierz pomy\u0142ek \nmatrix_nn5 = confusion_matrix(Y_test_d.argmax(axis = 1), Y_pred_nn5)\nraport_nn5 = classification_report(Y_test_d.argmax(axis = 1), Y_pred_nn5)\nprint(raport_nn5)","7dcf6e14":"model_nn6 = model_nn(X_train_array_d, Y_train_d,[512,256],0.1,2,30,0.0001, True)\nY_pred_nn6 = model_nn6.predict(X_test_array_d).argmax(axis = 1)\n\n# szybka ewaluacja\neval_res = model_nn6.evaluate(X_test_array_d, Y_test_d)\neval_names = model_nn6.metrics_names\nprint(f\"{eval_names[0]}: {eval_res[0]}\")\nprint(f\"{eval_names[1]}: {eval_res[1]}\")\n\n#obejrzyjmy jeszcze macierz pomy\u0142ek \nmatrix_nn6 = confusion_matrix(Y_test_d.argmax(axis = 1), Y_pred_nn6)\nraport_nn6 = classification_report(Y_test_d.argmax(axis = 1), Y_pred_nn6)\nprint(raport_nn6)","c51db941":"model_nn6 = model_nn(X_train_array_d, Y_train_d,[1024,256],0.1,2,30,0.0001, True)\nY_pred_nn6 = model_nn6.predict(X_test_array_d).argmax(axis = 1)\n\n# szybka ewaluacja\neval_res = model_nn6.evaluate(X_test_array_d, Y_test_d)\neval_names = model_nn6.metrics_names\nprint(f\"{eval_names[0]}: {eval_res[0]}\")\nprint(f\"{eval_names[1]}: {eval_res[1]}\")\n\n#obejrzyjmy jeszcze macierz pomy\u0142ek \nmatrix_nn6 = confusion_matrix(Y_test_d.argmax(axis = 1), Y_pred_nn6)\nraport_nn6 = classification_report(Y_test_d.argmax(axis = 1), Y_pred_nn6)\nprint(raport_nn6)","185807c4":"model_nn6 = model_nn(X_train_array_d, Y_train_d,[1024,512,128],0.05,2,30,0.0001, True)\nY_pred_nn6 = model_nn6.predict(X_test_array_d).argmax(axis = 1)\n\n# szybka ewaluacja\neval_res = model_nn6.evaluate(X_test_array_d, Y_test_d)\neval_names = model_nn6.metrics_names\nprint(f\"{eval_names[0]}: {eval_res[0]}\")\nprint(f\"{eval_names[1]}: {eval_res[1]}\")\n\n#obejrzyjmy jeszcze macierz pomy\u0142ek \nmatrix_nn6 = confusion_matrix(Y_test_d.argmax(axis = 1), Y_pred_nn6)\nraport_nn6 = classification_report(Y_test_d.argmax(axis = 1), Y_pred_nn6)\nprint(raport_nn6)","faab39c8":"Sprawdzmy jeszcze przypisanie do klasy decyzyjnej przy 0.4 i 0.6","ae44edae":"Model po replikacji danych osiagnal znaczna poprawe, sprobujmy jeszcze kilku eksperyment\u00f3w.","c4c342cf":"Poprawila nam sie accuracy calkowita i dla klasy 0, jednak dla klasy 1 precyzja i czulosc wciaz sa niskie.","5b33887b":"**1.wykorzystajmy w naszej analizie model XGBoost**","42c42f00":"Wynik Acc bardzo niski, jak to wyglada dalej:\n","b8431af8":"Niestety nic z tego, nie widz\u0119 potrzeby dalszych pr\u00f3b.","b9f91d19":"W zasadzie acc moglaby nam powiedziec, ze model jest bardzo dobry, ale pamietajmy, ze mamy niezbalansowany zbior 5:95","98125f0b":"Jak widzimy wartosci wskaznika BMI przy zastosowaniu Imputera zacze\u0142y nam si\u0119 kszta\u0142towa\u0107 ponad ramy dopuszczalnych wskaznik\u00f3w.\nUsu\u0144my jednak wartosci NA z tej zmiennej.","2b440e8b":"**9. Ze wzgledu na skalowanie i PCA model moze wymagac innych parametrow w takim razie poszukajmy bardziej optymalnego zestawu parametrow**","fe95d81e":"Widzimy, \u017ce prostym modelem XGB nie poprawili\u015bmy wynik\u00f3w dla 1 klasy.  \nSpr\u00f3bujmy znale\u017a\u0107 bardziej optymalny zestaw parametr\u00f3w przy u\u017cyciu optymalizatowra hyperopt","5dcdad94":"**8. Wyprobujmy nasz najlepszy dotad model na danych przeskalowanych + PCA**","eedbbfbf":"Model nie rozpoznaje w og\u00f3le klasy 2, spr\u00f3bujmy innych konfiguracji.","b966284c":"Podobna sytuacja jak w poprzednim przypadku, model ma wysok\u0105 dok\u0142adno\u015b\u0107, ze wzgl\u0119du na niezbalansowane dane. Na mniej licznej klasie uzyskuje zerowa precyzje. \n","7fb7af31":"Widzimy, \u017ce zmienne hypertension i heart_disease to tak na prawd\u0119 zmienne kategoryczne","773c3a16":"**Nasz finalny model charakteryzuje si\u0119 nisk\u0105 acc, bo 0.69, ale za to:**\n- poprawnie rozpoznaje 99% klasy 0,\n- jezeli juz przypisze kogos do klasy 1, to z 85% prawdopodobienstwem \n\n\nNiestety, przypisuje poprawnie tylko 11% objekt\u00f3w klasy 1.","31c3ee08":"# Podsumowanie ","21305f5b":"Widzimy, \u017ce mamy niemal o po\u0142owe wiecej kobiet w zbiorze i niewielki odsetek os\u00f3b nieokre\u015blonych.\nAnalizuj\u0105c zmienn\u0105 ever_married mo\u017cemy dostrzec, \u017ce by\u0142o prawie dwa razy wiecej os\u00f3b, ktore by kiedykolwiek po slubie.  \nNajwi\u0119cej obserwacji dotyczy osob z sektora prywatnego i 3x mniej, niemal po r\u00f3wno, os\u00f3b samozatrudnionych, wychowuj\u0105cych dzieci czy pracuj\u0105cych w pa\u0144stwowych instytucjach. Nigdy nie pracujacy stanowi\u0105 4 promile.  \nW przypadku os\u00f3b z nadci\u015bnieniem (hypertension) mamy 90% oznaczonych jako 0 - brak i nieca\u0142e 10% cierpi\u0105cych na te dolegliwos\u0107.\nWarto zwr\u00f3ci\u0107 uwag\u0119 na zmienna obja\u015bnian\u0105, czyli heart_disease mamy tylko w 5% przypadk\u00f3w oznaczonych jako 1, czyli gdzie wyst\u0105pi\u0142 atak serca, nale\u017cy to uwzgl\u0119dni\u0107 przy wyznaczenia setu testowego.","565e7227":"**II Zaawansowane modele**","62aec255":"**3. Na pocz\u0105tku zaczniemy od prostego modelu drzewa decyzyjnego**","20d9b4f6":"Poprzez zmiane wartosci od ktorej przypisujemy dana prognoze do jednej z klas, nie widzimy nadal znaczacej poprawy wskaznika f1.\nWidzimy jednak, ze dzi\u0119ki temu (wskaznik recall) poprawil sie i teraz jak ju\u017c klasyfikujemy kogos jako chorego to z 70% dokladnoscia. Niestety klasyfikujemy tylko 16% wszystkich przypadkow.","71382efa":"**1. Poznanie danych ze zbioru healtcare, odnosnie atak\u00f3w serca**","11e35285":"Jak widzimy bardziej zaawansowana metoda, jako jest XGBoost wraz z optymalizatorem parametr\u00f3w przy u\u017cyciu hyperopta nie poprawi\u0142a znacznie naszego najlepszego wyniku osiagnietego przez model RandomForrest.\nPokazuje nam z 98% dokladno\u015bcia przypadki braku ataku serca, ale bardzo s\u0142abo przewiduje faktyczne ataki serca","5e7d2085":"**10.Jako kolejna metode walki z niezbalansowanym zbiorem wypr\u00f3bujmy duplikacje sampli w klasie 1**","7d4b4b61":"**4. Spr\u00f3bujmy dostroi\u0107 model drzewa decyzyjnego i stworzyc kolejny model dd2**","fa85bf9d":"Sprawdzmy teraz wynik modelu na zmiennych po standaryzacji i PCA, wykorzystamy najlepszy wytrenowany dotad model ll3\n","2bf6bf36":"Niestety i ta metoda nie daje nam dobrego rozwi\u0105zania w\u0142a\u015bciwe klasyfikujacego klase \"1\"","2fdd7e8f":"  W przypadku pozosta\u0142ych zmiennych ci\u0105g\u0142ych, mamy duzo warto\u015bci odstaj\u0105cych dla avg_glucose_level i BMI. \n  Jednak nie wyklucza\u0142bym ich, poniewa\u017c cz\u0119sto przy chorobach pacjenci charakteryzuj\u0105 sie wysokimi wynikami.","3aa4824c":"**2. Wst\u0119pna obr\u00f3bka danych do modelu**","d7b924a3":"Dokladnosc naszego modelu mocno spad\u0142a, ale sprawd\u017amy czy poprawi\u0142a si\u0119 inna metryka. ","0f9969c6":"**7. Spr\u00f3bujmy popracowa\u0107 troch\u0119 z naszym zbiorem i dostroi\u0107 wagi pod niezbalansowany zbi\u00f3r**","ec9b42f7":"Jak widzimy 10 zmiennych wyjasnia zmiennosc modelu w prawie 83%, ale skorzystajmy z 11, bo wariancja jeszcze sie calkiem mocna poprawia po dodaniu jednej zmiennej i mamy wynik 88%.","bd4c6805":"Ta metoda w tym przypadku nie zda\u0142a egzaminu.\n","ba2e29c9":"**III Sie\u0107 neuronowa**","18cfe9bd":"Ju\u017c na wst\u0119pie osi\u0105gamy lepsze wyniki ni\u017c w naszym prostym modelu drzewa decyzyjnego.  \nWynik jest zbli\u017cony do zoptymalizowanego modelu drzewa a jego uzyskanie zabiera du\u017co mniej czasu.","fddcc348":"Niestety ponownie wynik dobry tylko dla 1 klasy, pozostalo nam jeszcze sprawdzic czy zmiana wag cos pomoze.","054edcce":"**5.Skonstruujmy prosty model las\u00f3w losowych**","d1c2b42c":"Nasz model uzysa\u0142 najwyzsza dok\u0142adno\u015b\u0107, ze wszystkich dotychczasowych modeli, ale to jeszcze nie znaczy, \u017ce jest dok\u0142adny.","8c04df97":"Spr\u00f3bujmy znale\u017a\u0107 najlepsze parametry dla nowego modelu, wykorzystamy model random forrest.","b1b507ce":"Widzimy, \u017ce dzi\u0119ki zmianie parametr\u00f3w modelu jeste\u015bmy w stanie znacznie poprawi\u0107 nasz\u0105 acc, ale nasz model ma nisk\u0105 precyzje dla drugiej klasy i przez to usredniony wynik wyrazony przez F1 score jest niski. ","293cc2cc":"Dzi\u0119ki odpowiedniemu wywa\u017ceniu parametr\u00f3w dla niezbalansowanej klasy poprawili\u015bmy precyzje i czulo\u015b\u0107, nie mniej jednak wska\u017anik f1 dla klasy 1 jest nadal na niskim poziomie.","474f6b94":"Nasz model po zastosowanie zaawansowanej metody replikacji danych mniejszej klasy, poprzez interpolacje i dostrojenia parametrow modelu w sieci neuronowej uzyska\u0142 ca\u0142kiem dobre wyniki na poziomie 88% dokladnosci\n","3d174a47":"Na podstawie macierzy b\u0142\u0119du, widzimy, \u017ce ta metoda zapewnia nam nisk\u0105 precyzj\u0119 dla klasy 1.   \nModel nie nadaje si\u0119 do produkcji!","a9b8343d":"**12. Podsumowanie**  \nWykorzystujac przedstawione tutaj metody, najlepszy wyniki, przedstawiony za pomoca F1 score dla obu klas, uzyskali\u015bmy stosujac model model_ll3 o parametrach jak ponizej.","e2f1c3eb":"Szukamy pkt, w kt\u00f3rym mamy najwyzsze TPR i wzglednie niskie FPR,\nmo\u017cemy uzna\u0107, \u017ce w\u0142a\u015bciwym progiem klasyfikacji dla tego modelu bedzie nawet 0.2","502981e9":"**11.Ostatni\u0105 metod\u0105 bedzie regresja logistyczna na przeskalowanych danych + PCA**","4f6d7f30":"Niestety losowane parametry nie ukaza\u0142y nam zbyt wiele prawid\u0142owo\u015bci, dlatego model zostanie wytrenowany na wszystkich parametrach. ","e6f5b4d2":"**6. Spr\u00f3bujmy dostroi\u0107 model las\u00f3w losowych i stworzyc kolejny model ll2**"}}