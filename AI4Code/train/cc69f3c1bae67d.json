{"cell_type":{"f53bcdc4":"code","5607b2c6":"code","849400de":"code","fb59340f":"code","f749beae":"code","3b7fb53a":"code","f7cc00ce":"code","a1611311":"code","00f8a3f7":"code","3f627042":"code","55909a9f":"code","728b5897":"code","4f4ead6d":"code","acd4b582":"code","43006e15":"code","5813ee9f":"code","c3ebbef5":"code","ee2a3e82":"code","fbfa2404":"code","cda57211":"code","627a815f":"code","c62e1a1d":"code","b643bd08":"code","8b9d08bb":"code","472f1e57":"code","e6d12992":"code","cafaf647":"code","87a1437a":"code","74ba2470":"code","ca2637af":"code","f35246d7":"code","f49b1511":"code","07af66af":"code","ba26075f":"code","06787019":"code","f04617e6":"code","9d65e089":"code","cd956023":"code","9448f484":"code","c63b0995":"markdown","032f54e8":"markdown","0be96b9b":"markdown","3b99e524":"markdown","9213df6e":"markdown","acacedaa":"markdown","fbdd0835":"markdown","cccc3173":"markdown","059c149f":"markdown","70f76531":"markdown","b9151774":"markdown","78b2110f":"markdown","7ff58365":"markdown","32c92c7c":"markdown","4b03dcbf":"markdown","a95c7b41":"markdown","33416e09":"markdown","555e5cc1":"markdown","06d07614":"markdown","807fbbe5":"markdown","77245939":"markdown","5576fcbf":"markdown","50ae0199":"markdown","2073321c":"markdown","be24d2ec":"markdown","54c2537f":"markdown"},"source":{"f53bcdc4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization library\npd.set_option('display.max_columns', 100) # Setting pandas to display a N number of columns\npd.set_option('display.max_rows', 10) # Setting pandas to display a N number rows\npd.set_option('display.width', 1000) # Setting pandas dataframe display width to N\nfrom scipy import stats # statistical library\nfrom statsmodels.stats.weightstats import ztest # statistical library for hypothesis testing\nimport plotly.graph_objs as go # interactive plotting library\nimport plotly.express as px # interactive plotting library\nimport matplotlib.pyplot as plt # plotting library\nimport pandas_profiling # library for automatic EDA\n%pip install autoviz # installing and importing autoviz, another library for automatic data visualization\nfrom autoviz.AutoViz_Class import AutoViz_Class\nfrom IPython.display import display # display from IPython.display\nfrom itertools import cycle # function used for cycling over values\n%pip install ppscore # installing ppscore, library used to check non-linear relationships between our variables\nimport ppscore as pps # importing ppscore\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nprint(\"\")\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5607b2c6":"# Importing the data and displaying some rows\ndf = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n\ndisplay(df.head(10))","849400de":"# The pandas profiling library is really useful on helping us understand the data we're working on.\n# It saves us some precious time on the EDA process.\nreport = pandas_profiling.ProfileReport(df)","fb59340f":"# Let's now visualize the report generated by pandas_profiling.\ndisplay(report)\n\n# Also, there is an option to generate an .HTML file containing all the information generated by the report.\n# report.to_file(output_file='report.html')","f749beae":"# Another great library for automatic EDA is AutoViz.\n# With this library, several plots are generated with only 1 line of code.\n# When combined with pandas_profiling, we obtain lots of information in a\n# matter of seconds, using less then 5 lines of code.\nAV = AutoViz_Class()\n\n# Let's now visualize the plots generated by AutoViz.\nreport_2 = AV.AutoViz(\"\/kaggle\/input\/titanic\/train.csv\")","3b7fb53a":"# Creating different datasets for survivors and non-survivors\ndf_survivors = df[df['Survived'] == 1]\ndf_nonsurvivors = df[df['Survived'] == 0]","f7cc00ce":"# Filling in the data inside the Violin Objects\nviolin_survivors = go.Violin(\n    y=df_survivors['Age'],\n    x=df_survivors['Survived'],\n    name='Survivors',\n    marker_color='forestgreen',\n    box_visible=True)\n\nviolin_nonsurvivors = go.Violin(\n    y=df_nonsurvivors['Age'],\n    x=df_nonsurvivors['Survived'],\n    name='Non-Survivors',\n    marker_color='darkred',\n    box_visible=True)\n\ndata = [violin_nonsurvivors, violin_survivors]\n\n\n# Plot's Layout (background color, title, etc.)\nlayout = go.Layout(\n    paper_bgcolor='rgba(0,0,0,0)',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title='\"Age\" of survivors vs Ages of non-survivors',\n  xaxis=dict(\n        title='Survived or not'\n    ),\n    yaxis=dict(\n        title='Age'\n    )\n)\n\nfig = go.Figure(data=data, layout=layout)\nfig.show()","a1611311":"# First distribution for the hypothesis test: Ages of survivors\ndist_a = df_survivors['Age'].dropna()\n\n# Second distribution for the hypothesis test: Ages of non-survivors\ndist_b = df_nonsurvivors['Age'].dropna()","00f8a3f7":"# Z-test: Checking if the distribution means (ages of survivors vs ages of non-survivors) are statistically different\nt_stat, p_value = ztest(dist_a, dist_b)\nprint(\"----- Z Test Results -----\")\nprint(\"T stat. = \" + str(t_stat))\nprint(\"P value = \" + str(p_value)) # P-value is less than 0.05\n\nprint(\"\")\n\n# T-test: Checking if the distribution means (ages of survivors vs ages of non-survivors) are statistically different\nt_stat_2, p_value_2 = stats.ttest_ind(dist_a, dist_b)\nprint(\"----- T Test Results -----\")\nprint(\"T stat. = \" + str(t_stat_2))\nprint(\"P value = \" + str(p_value_2)) # P-value is less than 0.05","3f627042":"# Taking the count of each Sex value inside the Survivors\ndf_survivors_sex = df_survivors['Sex'].value_counts()\ndf_survivors_sex = pd.DataFrame({'Sex':df_survivors_sex.index, 'count':df_survivors_sex.values})\n\n# Taking the count of each Sex value inside the Survivors\ndf_nonsurvivors_sex = df_nonsurvivors['Sex'].value_counts()\ndf_nonsurvivors_sex = pd.DataFrame({'Sex':df_nonsurvivors_sex.index, 'count':df_nonsurvivors_sex.values})\n\n\n# Creating the plotting objects\npie_survivors_sex = go.Pie(  \n   labels = df_survivors_sex['Sex'],\n   values = df_survivors_sex['count'],\n   domain=dict(x=[0, 0.5]),\n   name='Survivors',\n   hole = 0.5,\n   marker = dict(colors=['violet', 'cornflowerblue'], line=dict(color='#000000', width=2))\n)\n\npie_nonsurvivors_sex = go.Pie(  \n   labels = df_nonsurvivors_sex['Sex'],\n   values = df_nonsurvivors_sex['count'],\n   domain=dict(x=[0.5, 1.0]), \n   name='non-Survivors',\n   hole = 0.5,\n   marker = dict(colors=['cornflowerblue', 'violet'], line=dict(color='#000000', width=2))\n)\n\ndata = [pie_survivors_sex, pie_nonsurvivors_sex]\n\n\n# Plot's Layout (background color, title, annotations, etc.)\nlayout = go.Layout(\n    paper_bgcolor='rgba(0,0,0,0)',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title='\"Sex\" percentage from Survivors vs non-Survivors',\n    annotations=[dict(text='Survivors', x=0.18, y=0.5, font_size=15, showarrow=False),\n                 dict(text='Non-Survivors', x=0.85, y=0.5, font_size=15, showarrow=False)]\n)\n\nfig = go.Figure(data=data, layout=layout)\n\nfig.show()","55909a9f":"# Taking the count of each Pclass value inside the Survivors\ndf_survivors_pclass = df_survivors['Pclass'].value_counts()\ndf_survivors_pclass = pd.DataFrame({'Pclass':df_survivors_pclass.index, 'count':df_survivors_pclass.values})\n\n# Taking the count of each Pclass value inside the Survivors\ndf_nonsurvivors_pclass = df_nonsurvivors['Pclass'].value_counts()\ndf_nonsurvivors_pclass = pd.DataFrame({'Pclass':df_nonsurvivors_pclass.index, 'count':df_nonsurvivors_pclass.values})\n\n\n# Creating the plotting objects\npie_survivors_pclass = go.Pie(  \n   labels = df_survivors_pclass['Pclass'],\n   values = df_survivors_pclass['count'],\n   domain=dict(x=[0, 0.5]),\n   name='Survivors',\n   hole = 0.5,\n   marker = dict(colors=['#636EFA', '#EF553B', '#00CC96'], line=dict(color='#000000', width=2))\n)\n\npie_nonsurvivors_pclass = go.Pie(  \n   labels = df_nonsurvivors_pclass['Pclass'],\n   values = df_nonsurvivors_pclass['count'],\n   domain=dict(x=[0.5, 1.0]), \n   name='non-Survivors',\n   hole = 0.5,\n   marker = dict(colors=['#EF553B', '#00CC96', '#636EFA'], line=dict(color='#000000', width=2))\n)\n\ndata = [pie_survivors_pclass, pie_nonsurvivors_pclass]\n\n\n# Plot's Layout (background color, title, annotations, etc.)\nlayout = go.Layout(\n    paper_bgcolor='rgba(0,0,0,0)',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title='\"Pclass\" percentage from Survivors vs non-Survivors',\n    annotations=[dict(text='Survivors', x=0.18, y=0.5, font_size=15, showarrow=False),\n                 dict(text='Non-Survivors', x=0.85, y=0.5, font_size=15, showarrow=False)]\n)\n\nfig = go.Figure(data=data, layout=layout)\n\nfig.show()","728b5897":"# Checking out the differences between Fare distribution for survivors and non-survivors\nfare_survivors_box = go.Box(  \n   x=df_survivors['Fare'],\n   name='Survivors',\n   marker=dict(color='navy')\n)\n\nfare_nonsurvivors_box = go.Box(  \n   x=df_nonsurvivors['Fare'],\n   name='Non-Survivors',\n   marker=dict(color='steelblue')\n)\n  \ndata = [fare_nonsurvivors_box, fare_survivors_box]\n\n\n# Plot's Layout (background color, title, etc.)\nlayout = go.Layout(\n    paper_bgcolor='rgba(0,0,0,0)',\n    plot_bgcolor='rgba(0,0,0,0)',\n    title='\"Fare\" value of survivors vs \"Fare\" value of non-survivors',\n    barmode='stack',\n    xaxis=dict(\n        title='Fare distribution'\n    )\n)\n\nfig = go.Figure(data=data, layout=layout)\nfig.show()","4f4ead6d":"# Third distribution for the hypothesis test - Fares of survivors\ndist_c = df_survivors['Fare'].dropna()\n\n# Fourth distribution for the hypothesis test - Fares of non-survivors\ndist_d = df_nonsurvivors['Fare'].dropna()","acd4b582":"# Z-test: Checking if the distribution means (fares of survivors vs fares of non-survivors) are statistically different\nt_stat_3, p_value_3 = ztest(dist_c, dist_d)\nprint(\"----- Z Test Results -----\")\nprint(\"T stat. = \" + str(t_stat_3))\nprint(\"P value = \" + str(p_value_3)) # P-value is less than 0.05\n\nprint(\"\")\n\n# T-test: Checking if the distribution means (fares of survivors vs fares of non-survivors) are statistically different\nt_stat_4, p_value_4 = stats.ttest_ind(dist_c, dist_d)\nprint(\"----- T Test Results -----\")\nprint(\"T stat. = \" + str(t_stat_4))\nprint(\"P value = \" + str(p_value_4)) # P-value is less than 0.05","43006e15":"matrix_df = pps.matrix(df)[['x', 'y', 'ppscore']].pivot(columns='x', index='y', values='ppscore')\nsns.heatmap(matrix_df, vmin=0, vmax=1, cmap=\"Blues\", linewidths=0.75, annot=True)","5813ee9f":"import collections\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom imblearn.combine import SMOTEENN\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom category_encoders import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer, IterativeImputer\nfrom sklearn.model_selection import KFold, StratifiedKFold, RandomizedSearchCV, train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures\nfrom sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score, accuracy_score, make_scorer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier, StackingClassifier, VotingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom xgboost import XGBClassifier, plot_importance as plot_importance_xgb\nfrom lightgbm import LGBMClassifier, plot_importance as plot_importance_lgbm","c3ebbef5":"# Creating a categorical variable for Ages\ndf['AgeCat'] = ''\ndf['AgeCat'].loc[(df['Age'] < 18)] = 'young'\ndf['AgeCat'].loc[(df['Age'] >= 18) & (df['Age'] < 56)] = 'mature'\ndf['AgeCat'].loc[(df['Age'] >= 56)] = 'senior'\n\n\n# Creating a categorical variable for Family Sizes\ndf['FamilySize'] = ''\ndf['FamilySize'].loc[(df['SibSp'] <= 2)] = 'small'\ndf['FamilySize'].loc[(df['SibSp'] > 2) & (df['SibSp'] <= 5 )] = 'medium'\ndf['FamilySize'].loc[(df['SibSp'] > 5)] = 'large'\n\n\n# Creating a categorical variable to tell if the passenger is alone\ndf['IsAlone'] = ''\ndf['IsAlone'].loc[((df['SibSp'] + df['Parch']) > 0)] = 'no'\ndf['IsAlone'].loc[((df['SibSp'] + df['Parch']) == 0)] = 'yes'\n\n\n# Creating a categorical variable to tell if the passenger is a Young\/Mature\/Senior male or a Young\/Mature\/Senior female\ndf['SexCat'] = ''\ndf['SexCat'].loc[(df['Sex'] == 'male') & (df['Age'] <= 21)] = 'youngmale'\ndf['SexCat'].loc[(df['Sex'] == 'male') & ((df['Age'] > 21) & (df['Age']) < 50)] = 'maturemale'\ndf['SexCat'].loc[(df['Sex'] == 'male') & (df['Age'] > 50)] = 'seniormale'\ndf['SexCat'].loc[(df['Sex'] == 'female') & (df['Age'] <= 21)] = 'youngfemale'\ndf['SexCat'].loc[(df['Sex'] == 'female') & ((df['Age'] > 21) & (df['Age']) < 50)] = 'maturefemale'\ndf['SexCat'].loc[(df['Sex'] == 'female') & (df['Age'] > 50)] = 'seniorfemale'\n\n\n# Taking another look at the data\ndf.head(10)","ee2a3e82":"def get_feature_names(df):\n    # Splitting the target\n    target = df['Survived']\n\n    # Dropping unused columns from the feature set\n    df.drop(['PassengerId', 'Survived', 'Ticket', 'Name', 'Cabin'], axis=1, inplace=True)\n\n    # Splitting categorical and numerical column dataframes\n    categorical_df = df.select_dtypes(include=['object'])\n    numeric_df = df.select_dtypes(exclude=['object'])\n\n    # And then, storing the names of categorical and numerical columns.\n    categorical_columns = list(categorical_df.columns)\n    numeric_columns = list(numeric_df.columns)\n    \n    print(\"Categorical columns:\\n\", categorical_columns)\n    print(\"\\nNumeric columns:\\n\", numeric_columns)\n\n    return target, categorical_columns, numeric_columns\n\ntarget, categorical_columns, numeric_columns = get_feature_names(df)","fbfa2404":"# You can call any of the functions below, if you wish, inside the \"defineBestModelPipeline()\" function\n\ndef balancingClassesRus(x_train, y_train):\n    \n    # Using RandomUnderSampler to balance our training data points\n    rus = RandomUnderSampler(random_state=7)\n    features_balanced, target_balanced = rus.fit_resample(x_train, y_train)\n    \n    print(\"Count for each class value after RandomUnderSampler:\", collections.Counter(target_balanced))\n    \n    return features_balanced, target_balanced\n\n\ndef balancingClassesSmoteenn(x_train, y_train):\n    \n    # Using SMOTEEN to balance our training data points\n    smn = SMOTEENN(random_state=7)\n    features_balanced, target_balanced = smn.fit_resample(x_train, y_train)\n    \n    print(\"Count for each class value after SMOTEEN:\", collections.Counter(target_balanced))\n    \n    return features_balanced, target_balanced\n\n\ndef balancingClassesSmote(x_train, y_train):\n\n    # Using SMOTE to to balance our training data points\n    sm = SMOTE(random_state=7)\n    features_balanced, target_balanced = sm.fit_resample(x_train, y_train)\n\n    print(\"Count for each class value after SMOTE:\", collections.Counter(target_balanced))\n\n    return features_balanced, target_balanced","cda57211":"# Function responsible for checking our model's performance on the test data\ndef testSetResultsClassifier(classifier, x_test, y_test):\n    predictions = classifier.predict(x_test)\n    \n    results = []\n    f1 = f1_score(y_test, predictions)\n    precision = precision_score(y_test, predictions)\n    recall = recall_score(y_test, predictions)\n    roc_auc = roc_auc_score(y_test, predictions)\n    accuracy = accuracy_score(y_test, predictions)\n    \n    results.append(f1)\n    results.append(precision)\n    results.append(recall)\n    results.append(roc_auc)\n    results.append(accuracy)\n    \n    print(\"\\n\\n#---------------- Test set results (Best Classifier) ----------------#\\n\")\n    print(\"F1 score, Precision, Recall, ROC_AUC score, Accuracy:\")\n    print(results)\n    \n    return results","627a815f":"# Now, we are going to create our Pipeline, fitting several different data preprocessing, feature selection \n# and modeling techniques inside a RandomSearchCV, to check which group of techniques has better performance.\n\n# Building a Pipeline inside RandomSearchCV, responsible for finding the best model and it's parameters\ndef defineBestModelPipeline(df, target, categorical_columns, numeric_columns):\n    \n    # Splitting original data into Train and Test\n    x_train, x_test, y_train, y_test = train_test_split(df, target, test_size=0.1, random_state=42)\n    y_train = y_train.to_numpy() # Transforming training targets into numpy arrays\n    y_test = y_test.to_numpy() # Transforming test targets into numpy arrays\n    \n    \n    # # If desired, we can balance training classes using one of the functions below\n    # # Obtaining balanced data for modeling using Random Under Sampling\n    #x_train, y_train = balancingClassesRus(x_train, y_train)\n\n    # # Obtaining balanced data for modeling using SMOTEENN\n    #x_train, y_train = balancingClassesSmoteenn(x_train, y_train)\n\n    # # Obtaining balanced data for modeling using SMOTE\n    #x_train, y_train = balancingClassesSmote(x_train, y_train)\n    \n    \n    \n    # 1st -> Numeric Transformers\n    # Here, we are creating different several different data transformation pipelines \n    # to be applied in our numeric features\n    numeric_transformer_1 = Pipeline(steps=[('imp', IterativeImputer(max_iter=30, random_state=42)),\n                                            ('scaler', MinMaxScaler())])\n    \n    numeric_transformer_2 = Pipeline(steps=[('imp', IterativeImputer(max_iter=20, random_state=42)),\n                                            ('scaler', StandardScaler())])\n    \n    numeric_transformer_3 = Pipeline(steps=[('imp', SimpleImputer(strategy='mean')),\n                                            ('scaler', MinMaxScaler())])\n    \n    numeric_transformer_4 = Pipeline(steps=[('imp', SimpleImputer(strategy='median')),\n                                            ('scaler', StandardScaler())])\n    \n    \n    # 2nd -> Categorical Transformer\n    # Despite my option of not doing it, you can also choose to create different \n    # data transformation pipelines for your categorical features.\n    categorical_transformer = Pipeline(steps=[('frequent', SimpleImputer(strategy='most_frequent')),\n                                              ('onehot', OneHotEncoder(use_cat_names=True))])\n    \n    \n    # 3rd -> Combining both numerical and categorical pipelines\n    # Here, we are creating different ColumnTransformers, each one with a different numerical transformation\n    data_transformations_1 = ColumnTransformer(transformers=[('num', numeric_transformer_1, numeric_columns),\n                                                             ('cat', categorical_transformer, categorical_columns)])\n    \n    data_transformations_2 = ColumnTransformer(transformers=[('num', numeric_transformer_2, numeric_columns),\n                                                             ('cat', categorical_transformer, categorical_columns)])\n    \n    data_transformations_3 = ColumnTransformer(transformers=[('num', numeric_transformer_3, numeric_columns),\n                                                             ('cat', categorical_transformer, categorical_columns)])\n    \n    data_transformations_4 = ColumnTransformer(transformers=[('num', numeric_transformer_4, numeric_columns),\n                                                             ('cat', categorical_transformer, categorical_columns)])\n    \n    \n    \n    # And finally, we are going to apply these different data transformations to RandomSearchCV,\n    # trying to find the best imputing strategy, the best feature engineering strategy\n    # and the best model with it's respective parameters.\n    # Below, we just need to initialize a Pipeline object with any transformations we want, on each of the steps.\n    pipe = Pipeline(steps=[('data_transformations', data_transformations_1), # Initializing data transformation step by choosing any of the above\n                           ('feature_eng', PCA()), # Initializing feature engineering step by choosing any desired method\n                           ('clf', SVC())]) # Initializing modeling step of the pipeline with any model object\n                           #memory='cache_folder') -> Used to optimize memory when needed\n    \n    \n    \n    # Now, we define the grid of parameters that RandomSearchCV will use. It will randomly chose\n    # options for each step inside the dictionaries ('data transformations', 'feature_eng', 'clf'\n    # and 'clf parameters'). In the end of it's iterations, RandomSearchCV will return the best options.\n    params_grid = [\n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=1), PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [KNeighborsClassifier()],\n                     'clf__n_neighbors': stats.randint(1, 30),\n                     'clf__metric': ['minkowski', 'euclidean']},\n\n        \n\n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=1), PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [LogisticRegression()],\n                     'clf__penalty': ['l1', 'l2'],\n                     'clf__C': stats.uniform(0.01, 10)},\n\n\n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=1), PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [SVC()],\n                     'clf__C': stats.uniform(0.01, 1),\n                     'clf__gamma': stats.uniform(0.01, 1)},\n\n\n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=1), PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [DecisionTreeClassifier()],\n                     'clf__criterion': ['gini', 'entropy'],\n                     'clf__max_features': [None, \"auto\", \"log2\"],\n                     'clf__max_depth': [None, stats.randint(1, 5)]},\n\n\n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=1), PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [RandomForestClassifier()],\n                     'clf__n_estimators': stats.randint(10, 175),\n                     'clf__max_features': [None, \"auto\", \"log2\"],\n                     'clf__max_depth': [None, stats.randint(1, 5)]},\n        \n                    \n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=1), PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [ExtraTreesClassifier()],\n                     'clf__n_estimators': stats.randint(10, 150),\n                     'clf__max_features': [None, \"auto\", \"log2\"],\n                     'clf__max_depth': [None, stats.randint(1, 6)]},\n\n                    \n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=1), PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [GradientBoostingClassifier()],\n                     'clf__n_estimators': stats.randint(10, 100),\n                     'clf__learning_rate': stats.uniform(0.01, 0.7),\n                     'clf__max_depth': [None, stats.randint(1, 6)]},\n\n        \n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=1), PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [LGBMClassifier()],\n                     'clf__n_estimators': stats.randint(1, 100),\n                     'clf__learning_rate': stats.uniform(0.01, 0.7),\n                     'clf__max_depth': [None, stats.randint(1, 6)]},\n\n\n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=1), PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [XGBClassifier()],\n                     'clf__n_estimators': stats.randint(5, 125),\n                     'clf__eta': stats.uniform(0.01, 1),\n                     'clf__max_depth': [None, stats.randint(1, 6)],\n                     'clf__gamma': stats.uniform(0.01, 1)},\n\n\n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=1), PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [StackingClassifier(estimators=[('svc', SVC(C=1, gamma=1)),\n                                                            ('rf', RandomForestClassifier(max_depth=7, max_features=None, n_estimators=60, n_jobs=-1)),\n                                                            ('xgb', XGBClassifier(eta=0.6, gamma=0.7, max_depth=None, n_estimators=30))], \n                                                final_estimator=LogisticRegression(C=1))]},\n   \n   \n        \n                    {'data_transformations': [data_transformations_1, data_transformations_2, data_transformations_3, data_transformations_4],\n                     'feature_eng': [None, \n                                     PCA(n_components=round(x_train.shape[1]*0.9)),\n                                     PCA(n_components=round(x_train.shape[1]*0.8)),\n                                     PCA(n_components=round(x_train.shape[1]*0.7)),\n                                     PolynomialFeatures(degree=1), PolynomialFeatures(degree=2), PolynomialFeatures(degree=3)],\n                     'clf': [VotingClassifier(estimators=[('gbt', GradientBoostingClassifier(learning_rate=0.8, max_depth=None, n_estimators=30)),\n                                                          ('lgbm', LGBMClassifier(n_estimators=30, learning_rate=0.6, max_depth=None)),\n                                                          ('xgb', XGBClassifier(eta=0.8, gamma=0.8, max_depth=None, n_estimators=40))],\n                                              voting='soft')]}\n                ]\n    \n    \n    # Now, we fit a RandomSearchCV to search over the grid of parameters defined above\n    metrics = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\n    \n    best_model_pipeline = RandomizedSearchCV(pipe, params_grid, n_iter=500, \n                                             scoring=metrics, refit='accuracy', \n                                             n_jobs=-1, cv=5, random_state=42)\n\n    best_model_pipeline.fit(x_train, y_train)\n    \n    \n    # At last, we check the final results\n    print(\"\\n\\n#---------------- Best Data Pipeline found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[0])\n    print(\"\\n\\n#---------------- Best Feature Engineering technique found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[1])\n    print(\"\\n\\n#---------------- Best Classifier found in RandomSearchCV  ----------------#\\n\\n\", best_model_pipeline.best_estimator_[2])\n    print(\"\\n\\n#---------------- Best Estimator's average Accuracy Score on CV (validation set) ----------------#\\n\\n\", best_model_pipeline.best_score_)\n    \n    return x_train, x_test, y_train, y_test, best_model_pipeline","c62e1a1d":"# Calling the function above, returing train\/test data and best model's pipeline\nx_train, x_test, y_train, y_test, best_model_pipeline = defineBestModelPipeline(df, target, categorical_columns, numeric_columns)\n\n\n# Checking best model's performance on test data\ntest_set_results = testSetResultsClassifier(best_model_pipeline, x_test, y_test)","b643bd08":"# Visualizing all results and metrics, from all models, obtained by the RandomSearchCV steps\ndf_results = pd.DataFrame(best_model_pipeline.cv_results_)\n\ndisplay(df_results)","8b9d08bb":"# Now visualizing all results and metrics obtained only by the best classifier\ndisplay(df_results[df_results['rank_test_accuracy'] == 1])","472f1e57":"# Here, we access the categorical feature names generated by OneHotEncoder, and then concatenate them\n# with the numerical feature names, in the same order our pipeline is applying data transformations.\ncategorical_features_after_onehot = best_model_pipeline.best_estimator_.named_steps['data_transformations']\\\n                                        .transformers_[1][1].named_steps['onehot'].get_feature_names()\n\nfeature_names_in_order = numeric_columns + categorical_features_after_onehot\n\nprint(feature_names_in_order)","e6d12992":"# # Plotting feature importances of the best model, if sklearn tree-based (top 5 features)\n#print(\"\\n#---------------- Bar plot with feature importances ----------------#\")\n#feat_importances = pd.Series(best_model_pipeline.best_estimator_.named_steps['clf'].feature_importances_, index=feature_names_in_order)\n#feat_importances.nlargest(5).plot(kind='barh')\n\n\n# # Plotting feature importances of the best model, if linear regression-based (top 5 features)\n#print(\"\\n#---------------- Bar plot with feature importances ----------------#\")\n#feat_importances = pd.Series(best_model_pipeline.best_estimator_.named_steps['clf'].coef_, index=feature_names_in_order)\n#feat_importances.nlargest(5).plot(kind='barh')\n\n\n# # Plotting feature importances for XGB Model\n#plot_importance_xgb(best_model_pipeline.best_estimator_.named_steps['clf'], height=0.4, \n#title='Feature Importances for XGB Classifier', importance_type='gain')\n\n\n# # Plotting feature importances for LGBM Model\n#plot_importance_lgbm(best_model_pipeline.best_estimator_.named_steps['clf'], \n#                     figsize=(10, 4), title='Feature importances for LGBM Classifier',\n#                     importance_type='gain', max_num_features=10)","cafaf647":"# Defining a PCA Pipeline\ndef definePCAPipeline(categorical_columns, numeric_columns):\n\n    # 1st -> Numeric Transformer\n    numeric_transformer = Pipeline(steps=[('imp', SimpleImputer(strategy='median')),\n                                            ('scaler', StandardScaler())])\n    \n    \n    # 2nd -> Categorical Transformer\n    categorical_transformer = Pipeline(steps=[('frequent', SimpleImputer(strategy='most_frequent')),\n                                              ('onehot', OneHotEncoder(use_cat_names=True))])\n    \n    \n    # 3rd -> Combining both numerical and categorical pipelines\n    data_transformations = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_columns),\n                                                             ('cat', categorical_transformer, categorical_columns)])\n    \n\n    # 4th -> Final PCA Pipeline\n    pca_pipeline = Pipeline(steps=[('data_transformations', data_transformations),\n                           ('feature_eng', PCA(n_components=2, whiten=True))])\n    \n    \n    return pca_pipeline","87a1437a":"# Generating transformed data after PCA, only 2 principal components chosen\npca_pipeline = definePCAPipeline(categorical_columns, numeric_columns)\npca_arr = pca_pipeline.fit_transform(df)\n\n# How much variance does our PCA obtained components explains from the original variance in data?\ncomp1 = pca_pipeline[1].explained_variance_ratio_[0]\ncomp2 = pca_pipeline[1].explained_variance_ratio_[1]\nexp_variance_pca = comp1 + comp2\n\nprint(\"Compontent 1 explained variance ratio:\", comp1)\nprint(\"Compontent 2 explained variance ratio:\", comp2)\nprint(\"Total explained variance ratio obtained from both components:\", exp_variance_pca)","74ba2470":"# Visualizing the new 2-dimensional dataset and points' respective classes\npca_df = pd.DataFrame(pca_arr, columns=[\"PC1\", \"PC2\"])\npca_df['Survived'] = target\n\npca_df.head(10)","ca2637af":"# Creating different datasets for survivors and non-survivors\npca_df_survivors = pca_df[pca_df['Survived'] == 1]\npca_df_nonsurvivors = pca_df[pca_df['Survived'] == 0]\n\n\n# Visualizing the two-dimensional dataset\nscatter_obj_survs = go.Scatter(x=pca_df_survivors['PC1'],\n                               y=pca_df_survivors['PC2'],\n                               mode=\"markers\",\n                               name='Survivors',\n                               marker=dict(color='forestgreen'))\n\n\nscatter_obj_nonsurvs = go.Scatter(x=pca_df_nonsurvivors['PC1'],\n                                  y=pca_df_nonsurvivors['PC2'],\n                                  mode=\"markers\",\n                                  name='Non-survivors',\n                                  marker=dict(color='darkred'))\n\n\ndata = [scatter_obj_survs, scatter_obj_nonsurvs]\n\n\n# Plot's Layout (background color, title, etc.)\nlayout = go.Layout(title='2-Dimensional visualization of survivors and non-survivors',\n                   xaxis=dict(title='PC1'), yaxis=dict(title='PC2'))\n\nfig = go.Figure(data=data, layout=layout)\nfig.show()","f35246d7":"# Defining a t-SNE Pipeline\ndef defineTSNEPipeline(categorical_columns, numeric_columns):\n\n    # 1st -> Numeric Transformer\n    numeric_transformer = Pipeline(steps=[('imp', SimpleImputer(strategy='median')),\n                                            ('scaler', StandardScaler())])\n    \n    \n    # 2nd -> Categorical Transformer\n    categorical_transformer = Pipeline(steps=[('frequent', SimpleImputer(strategy='most_frequent')),\n                                              ('onehot', OneHotEncoder(use_cat_names=True))])\n    \n    \n    # 3rd -> Combining both numerical and categorical pipelines\n    data_transformations = ColumnTransformer(transformers=[('num', numeric_transformer, numeric_columns),\n                                                             ('cat', categorical_transformer, categorical_columns)])\n    \n\n    # 4th -> Final t-SNE Pipeline\n    tsne_pipeline = Pipeline(steps=[('data_transformations', data_transformations),\n                                    ('feature_eng', TSNE(n_components=2, random_state=1))])\n    \n    \n    return tsne_pipeline","f49b1511":"# Generating transformed data after t-SNE, 2-Dimensional Feature Space chosen\ntsne_pipeline = defineTSNEPipeline(categorical_columns, numeric_columns)\ntsne_arr = tsne_pipeline.fit_transform(df)","07af66af":"# Visualizing the new 2-dimensional dataset and points' respective classes\ntsne_df = pd.DataFrame(tsne_arr, columns=[\"tsne_dim1\", \"tsne_dim2\"])\ntsne_df['Survived'] = target\n\ntsne_df.head(10)","ba26075f":"# Creating different datasets for survivors and non-survivors\ntsne_df_survivors = tsne_df[tsne_df['Survived'] == 1]\ntsne_df_nonsurvivors = tsne_df[tsne_df['Survived'] == 0]\n\n\n# Visualizing the two-dimensional dataset\nscatter_obj_survs_tsne = go.Scatter(x=tsne_df_survivors['tsne_dim1'],\n                                    y=tsne_df_survivors['tsne_dim2'],\n                                    mode=\"markers\",\n                                    name='Survivors',\n                                    marker=dict(color='forestgreen'))\n\n\nscatter_obj_nonsurvs_tsne = go.Scatter(x=tsne_df_nonsurvivors['tsne_dim1'],\n                                       y=tsne_df_nonsurvivors['tsne_dim2'],\n                                       mode=\"markers\",\n                                       name='Non-survivors',\n                                       marker=dict(color='darkred'))\n\n\ndata_tsne = [scatter_obj_survs_tsne, scatter_obj_nonsurvs_tsne]\n\n\n# Plot's Layout (background color, title, etc.)\nlayout_tsne = go.Layout(title='2-Dimensional visualization of survivors and non-survivors (t-SNE algorithm)',\n                        xaxis=dict(title='tsne_dim1'), yaxis=dict(title='tsne_dim2'))\n\nfig_tsne = go.Figure(data=data_tsne, layout=layout_tsne)\nfig_tsne.show()","06787019":"# Importing data and displaying some rows\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\n# Dropping unnecessary columns\ndf_test.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace=True)","f04617e6":"# Creating a categorical variable for Ages\ndf_test['AgeCat'] = ''\ndf_test['AgeCat'].loc[(df_test['Age'] < 18)] = 'young'\ndf_test['AgeCat'].loc[(df_test['Age'] >= 18) & (df_test['Age'] < 56)] = 'mature'\ndf_test['AgeCat'].loc[(df_test['Age'] >= 56)] = 'senior'\n\n\n# Creating a categorical variable for Family Sizes\ndf_test['FamilySize'] = ''\ndf_test['FamilySize'].loc[(df_test['SibSp'] <= 2)] = 'small'\ndf_test['FamilySize'].loc[(df_test['SibSp'] > 2) & (df_test['SibSp'] <= 5 )] = 'medium'\ndf_test['FamilySize'].loc[(df_test['SibSp'] > 5)] = 'large'\n\n\n# Creating a categorical variable to tell if the passenger is alone\ndf_test['IsAlone'] = ''\ndf_test['IsAlone'].loc[((df_test['SibSp'] + df_test['Parch']) > 0)] = 'no'\ndf_test['IsAlone'].loc[((df_test['SibSp'] + df_test['Parch']) == 0)] = 'yes'\n\n\n# Creating a categorical variable to tell if the passenger is a Young\/Mature\/Senior male or a Young\/Mature\/Senior female\ndf_test['SexCat'] = ''\ndf_test['SexCat'].loc[(df_test['Sex'] == 'male') & (df_test['Age'] <= 21)] = 'youngmale'\ndf_test['SexCat'].loc[(df_test['Sex'] == 'male') & ((df_test['Age'] > 21) & (df_test['Age']) < 50)] = 'maturemale'\ndf_test['SexCat'].loc[(df_test['Sex'] == 'male') & (df_test['Age'] > 50)] = 'seniormale'\ndf_test['SexCat'].loc[(df_test['Sex'] == 'female') & (df_test['Age'] <= 21)] = 'youngfemale'\ndf_test['SexCat'].loc[(df_test['Sex'] == 'female') & ((df_test['Age'] > 21) & (df_test['Age']) < 50)] = 'maturefemale'\ndf_test['SexCat'].loc[(df_test['Sex'] == 'female') & (df_test['Age'] > 50)] = 'seniorfemale'\n\n\n# Applying best_model_pipeline:\n# Step 1 -> Transforming data the same way we did in the training set;\n# Step 2 -> making predictions using the best model obtained by RandomSearchCV.\ntest_predictions = best_model_pipeline.predict(df_test)\nprint(test_predictions)","9d65e089":"# Generating predictions file that is going to be submitted to the competition\ndf_submission = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\ndf_submission['Survived'] = test_predictions # Adding a column with predicted values\n\ndf_submission.drop(df_submission.columns.difference(['PassengerId', 'Survived']), axis=1, inplace=True) # Selecting only needed columns\n\ndf_submission.head(10)","cd956023":"# Checking if the number of rows is OK (the file is expected to have 418 rows)\ndf_submission.count()","9448f484":"# Writing submitions to CSV file\ndf_submission.to_csv('submission.csv', index=False)","c63b0995":"---","032f54e8":"### Part 2 - t-SNE (t-Distributed Stochastic Neighbour Embedding)\n\nAnother great algorithm generally used for visualizing X-dimensional representations of data is **t-SNE**. It basically calculates distances between data points in the original feature space (by creating something called a **Similarity Matrix**: a matrix representing each data point and it's distance to the other data points in the feature space), and then, it represents all original points randomly in a new dimensional-space (and creates another Similarity Matrix for this new random representation). \n\nLastly, it moves the points that were randomly presented on the new dimensional space, trying to make the second similarity matrix as close as possible to the first one (tries to represent points that are closer in the original feature space closer, and points that are far apart, farther apart).\n\n\n![tSNE.png](attachment:tSNE.png)\n\n\nIf you want to know more about t-SNE, please take a look at this article: https:\/\/en.wikipedia.org\/wiki\/T-distributed_stochastic_neighbor_embedding\n\nAlso, there is a really didatic explanation in \"StatQuest\" youtube channel, so I invite you to watch it too: https:\/\/www.youtube.com\/watch?v=NEaUSP4YerM","0be96b9b":"From both PCA and t-SNE 2-dimensional representations, we can see that there isn't a clear linear separation between survivors and non-survivors. This can be interpreted as a sign that linear separation lines (linear models) probably wouldn't work that well on a job to separate both classes. Because of that, non-linear models such as Kernelized SVMs and Decision-tree based methods should have better results, when modeling this specific problem.","3b99e524":"As we can see both from the plot and hypothesis tests showed above, there is actually a statistically significant difference between the means of both distributions (ages of survivors and non-survivors). Let's do some more exploring to see what further information we can gather from this data.","9213df6e":"### Part 3 - K-Means Clustering\n\nUnder construction.","acacedaa":"### Pipeline Construction\n\nWell, you might be wondering now: What is a Pipeline?\n\n![pipeline.jpg](attachment:pipeline.jpg)\n\nWe can understand a Pipelines as a sequence of actions you apply in data. Just like the image above, you can see that a full pipeline is made of several different small pipes. Take this to Data Science: imagine that each small pipe is a step in your modeling process. For example:\n\n#### -> Step 1: fill null values from numerical columns. \n\n#### -> Step 2: normalizing numerical features, so they will be in the same scale. \n\n#### -> Step 3: filling null values from categorical features. \n\n#### -> Step 4: OneHotEncoding categorical features.\n\n#### -> Step 5: fitting a Machine Learning model and evaluating it.\n\nInstead of doing each one of these steps separately, we can create a Pipeline object that unites all of these steps together, and then fit this object in our training data.\n\nAnd why should we do that?\n\nWell, there are a lot of advantages we get on using pipelines. Below are the ones I find most relevant for this discussion:\n\n\n## 1 - Production code gets much easier to implement\nWhen deploying a Machine Learning model into production, the main goal is to use it on data it hasn't seen before. To do that, the new data needs to be transformed the same way training data was. Instead of having several different functions for each one of the preprocessing tasks, you can use a single pipeline object to apply all of them sequentially. It means that, in 1 line of code, you can apply all needed transformations. Check an example of this in the \"Predictions\" section of this notebook.\n\n## 2 - When combined with RandomSearchCV, it is possible to test several pipeline options\nYou must have already asked yourself, when training your models: \"for this type of data, what works best? Fill in missing values with the average or the median of a column? Should I use MinMaxScaler or StandardScaler? Apply dimensionality reduction? Create more features using, for example, PolynomialFeatures?\" Using Pipelines and hyperparameter search functions (like RandomSearchCV), you can search through entire sets of pipelines, models and parameters automatically, saving up effort invested by you in the search for optimal feautre engineering methods and models\/hyperparameters.\n\nSuppose we have 4 different pipelines:\n\n#### -> Pipeline 1: fill missing values from numeric features by imputing the mean of each column - apply MinMaxScaler - apply OneHotEncoder to categorical features - fits the data into a KNN Classifier with n_neighbors = 15.\n\n#### -> Pipeline 2: fill missing values from numeric features by imputing the mean of each column - apply StandardScaler - apply OneHotEncoder to categorical features - fits the data into a KNN Classifier with n_neighbors = 30.\n\n#### -> Pipeline 3: fill missing values from numeric features by imputing the median of each column - apply MinMaxScaler - apply OneHotEncoder to categorical features - fits the data into a Random Forest Classifier with n_estimators = 100.\n\n#### -> Pipeline 4: fill missing values from numeric features by imputing the median of each column - apply StandardScaler - apply OneHotEncoder to categorical features - fits the data into a Random Forest Classifier with n_estimators = 100.\n\nInitially, you might think that, to check which pipeline is better, all you need to do is to create all of them manually, fit your data, and then evaluate the results. But what if we want to increase the range of this search, let's say, to over hundreds of different pipelines? It will be really hard to do that manually. And that's where RandomSearchCV comes into play. \n\n\n\n## 3 - No information leakage when Cross-Validating\nThis one is a bit trickier, so it would take a little longer to explain. By this reason, we won't do it in this notebook. Still, this is a really important concept. I recommend you to read articles about it. Also, as a recommendation, chapter 6 of the book \"Introduction to Machine Learning with Python\" by Andreas C. Muller & Sarah Guido (pages 306 and 307, mainly) gives a really good perspective of this problem.\n\n---\n\nFor further information about Pipelines and RandomSearchCV, check the docs out:\n\n**Pipelines** - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html\n\n**RandomizedSearchCV** - https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.RandomizedSearchCV.html","fbdd0835":"# Titanic Dataset: Automatic EDA, different Data Preprocessing & Modeling Techniques compared with Pipelines + RandomSearchCV and much more!\n\n![Titanic.jpg](attachment:Titanic.jpg)\n\nThe main goal of this notebook is to try to present a complete approach to modeling problems, that goes from Exploratory Data Analysis to applying Supervised and Unsupervised learning techniques to our data. This notebook's content is mainly directed to data scientists, data science students or people interested in how these techniques can be applied into data.\n\nThanks to Andreas C. Muller, Sarah Guido & other co-authors, for writting the book **\"Introduction to Machine Learning with Python\"**. A great source of knowledge for Data Scientists of all levels.\n\nNotebook written by **Pedro de Matos Gon\u00e7alves**","cccc3173":"\n## Model training & Evaluation functions\n\nAfter all the preprocessing, we are now ready to build and evaluate different Machine Learning models.\n\nFirst, let's create a function responsible for evaluating our classifiers on a test set we will create later.","059c149f":"### Plotting Feature Importances\n\nIf we want to, it's also possible to check the feature importances of the best model, in case they're easy to understand and explain.\n\nJust remember that, if the best pipeline found in RandomSearchCV applies dimensionality reduction or creates new features using PolynomialFeatures, it will be much harder to explain importances.\n\nIn a scenario that no transformations are applied to the features inside the pipeline, if the model is tree-based (RandomForestClassifier, for example), or linear regression-based (Logistic Regression, for example), then explaining most important features becomes much easier.","70f76531":"From the pie chart showed above, we can notice a peculiar behavior: when looking at passengers that didn't survive, ~68% of them were at \"Pclass\" 3. When looking at passengers that survived, only ~35% of them were at \"Pclass\" 3.\n\nAt the same point of view, when looking at passengers that survived, ~40% of them were at \"Pclass\" 1. At the non-survivors, only 14.6% of them were at \"Pclass\" 1.\n\nIt seems that there is some kind of relation between \"P-class\" and the fact of a passenger surviving the accident or not. Let's get into more detail.","b9151774":"To begin our analysis, lets take our first look at the dataset. To save some precious time on our Exploratory Data Analysis process, we are going to use 2 libraries: **\"pandas_profiling\"** and **\"autoviz\"**.","78b2110f":"Using the power of both automatic EDA libraries listed above, we can observe each variable's behaviour individually, with plots that goes from Histograms to Boxplots, Correlation Matrix and much more. It speeds up time and minimizes the effort spent on the initial process of our work.\n\nWe can gather some really useful information from both reports. Let's now point some of them out:\n\n* Our classes are not that much disbalanced. We have ~38% of the passengers into class \"1\" (survived) and ~62% of the passengers into class \"0\" (didn't survive).\n\n\n* The \"Pclass\" column, that informs us about the passenger's ticket class, shows us that ~55% of them are on class 3, ~24% of them are on class 2 and ~21% on class 1.\n\n\n* Most of the passengers into this dataset are male: ~35% of the passengers are female, and ~65% are male.\n\n\n* Almost 20% of the values in the \"Age\" column are missing. We can fill out these nulls with various techniques, such as filling them with the distribution's mean. The ages distribution is a little bit skewed, with it's mean being around 30 years old, and it's standard deviation being close to 15. The oldest passenger we have in this dataset is 80 years old.\n\n\n* According to the \"SibSP\" column, most of the passengers (~68%) didn't have any spouses or siblings aboard the ship. That is also applied when we check out the \"Parch\" column.\n\n\n* The distribution of Fares is much more skewed. It's mean value is around 32, with it's standard deviation being close to 50. It's minimum value is 0, and it's maximum value is 512.3292. That means that we're going to have to deal with this column carefully if we plan to use models such as SVMs.\n\n\n* When ckecking the \"Embarked\" column, it shows us that 72.3% of the passengers embarked at Southampton port, 18.9% of the passengers at Cherbourg port and 8.6% of the passengers at Queenstown port.\n\n\n* \"Fare\" values are higher for passengers with \"Pclass\" = 1, lower for passengers with \"Pclass\" = 2 and even lower for passengers with \"Pclass\" = 3. Logically, it looks like the classification of \"Pclass\" is defined by the value of the passenger's fare.\n\n","7ff58365":"**Remember:** All transformations that were done in the training dataset must be done in the test set.","32c92c7c":"---\n\n# Predictions\n\nNow that we have tried different preprocessing and modeling techniques, resulting in a final best pipeline, let's use it to predict the test data provided by kaggle.","4b03dcbf":"## *Section 1 - Data Exploration*","a95c7b41":"---","33416e09":"Looking at this PPS matrix, we can see that the best univariate predictor of the **Survived** variable is the column **Ticket**, with 0.19 pps, followed by **Sex**, with 0.13 pps. That makes sense because women were prioritized during the rescue, and ticket is closely related to **Pclass**. The best univariate predictor of the **Parch** variable is the column **Cabin**, with 0.37 pps, and so on.","555e5cc1":"## Introducing PPS (Predictive Power Score)\n\nYou may have heard about correlation matrices. Basically, correlation matrices are able to identify linear relationships between variables. Because relationships in our data may sometimes be non-linear (most of the times, actually), we can use a PPS (Predictive Power Score) matrix, to figure out **non-linear relations** between columns.\n\nIf you want to understand why PPS is important, I recommend you to read this medium article: https:\/\/towardsdatascience.com\/rip-correlation-introducing-the-predictive-power-score-3d90808b9598\n\nAlso, take a look at the Python PPS implementation used in this notebook: https:\/\/github.com\/8080labs\/ppscore","06d07614":"## Feature Engineering \nTo help us get a better performance, we can create new features based on the original features of our dataset.","807fbbe5":"## More Exploration\n\nBefore we go to the modeling part, let's take a look at a few more plots that gives us a different perspective from the ones generated above. That may give us further insights and help us understand the differences between the passengers that survived the catastrophe and the people that didn't. For these visualizations, we are going to use Plotly, a library that gives us beautiful plots and allows us to interact with them.\n\nFirst, let's take a look at the differences between the ages of both groups, using a Violin plot.","77245939":"After going through all steps in RandomSearchCV, we can check the results from it's steps using the \"cv_results_\" atrribute.","5576fcbf":"After creating new features, we can drop useless columns that we won't use in the training process.","50ae0199":"Checking out the plots and hypothesis tests over fare distributions, comparing Survivors and non-Survivors, we can again observe that there is a statistically significant difference between the means of both groups. \n\nWhen checking out the boxplots, we can see that fare values of survivors are generally higher, when compared to fare values of non-survivors. This information is probably related to the \"Pclass\" percentages we have seen before on the pie plots.","2073321c":"## Balancing Data\n\nAs we saw earlier on EDA section, our data is pretty much balanced, but we have a small number of observations in our training set. To try tackling this problem, we can take different approaches.\n\nThree common ones are  **RandomUnderSampling**, **SMOTE** and **SMOTEENN**. We can try using one of them to balance our data. \n\nWe also have the option of not balancing data, going straight to the Pipeline part.","be24d2ec":"## *Section 2 - Modeling*\n\nNow that we have some nice context about the data we are working with, let's dive into the modeling part.\n\nFirst of all, we import the libraries we're going to use.","54c2537f":"## *Extra Section - Unsupervised Learning*\n\n### Part 1 - PCA\n\nPCA is a very common technique for dimensionality reduction. It's generally used to find a new representation of your original dataset, that cointains only a percentage of it's original variance (information is lost in the process of representing higher dimensions into lower dimensions).\n\nIn a brief description, PCA tries to identify the directions where the original data varies the most (horizontal, vertical, diagonal, etc.) and sets these directions as elements called Principal Components. It means that, PC1 (Principal Component 1) is the direction to where our data varies that can better explain it's total variance, PC2 (principal component 2) is the second best direction for explaining it's total variance, and so on.\n\n\n![PCA.webp](attachment:PCA.webp)\n\n\nIf you want to have a better look at how PCA math really works, please have a look at this youtube video by Josh Stormer, on his \"StatQuest\" youtube channel: https:\/\/www.youtube.com\/watch?v=FgakZw6K1QQ"}}