{"cell_type":{"9231703b":"code","ee0e42e3":"code","8ce6dd8e":"code","ffd66d98":"code","48fea875":"code","eb4d74c1":"code","57b6a5e8":"code","ccee69bb":"code","10146c23":"code","cd6ec69b":"code","99763b46":"code","fd1a48ff":"code","67d33884":"code","95ec5ae0":"code","7cb43daa":"code","41941ff7":"code","e0b5cfe0":"code","81b908ce":"code","f944ec29":"code","9818fa8e":"code","be85bba0":"code","f5b45d2f":"code","d8c40d9c":"code","3481edd9":"code","1e250ffb":"code","e00086a4":"code","af9c5499":"code","c54ab92f":"code","bcefd1a2":"code","da2b3052":"code","78affc8b":"code","39236ca1":"code","9f8b9dfe":"code","c34a4365":"code","811ec07a":"code","7dc9571c":"code","79a9b6c1":"code","06fcd205":"code","ea755ced":"code","e138779d":"code","106430df":"code","413084f2":"code","bd061b51":"code","f428b56b":"code","4d7886e6":"code","05b0674e":"code","e64dd34e":"code","ef2f826d":"code","ef014ecb":"code","2c3294fe":"code","029a7b22":"code","449e23b7":"code","2cb082dc":"code","f00d8aa7":"code","71ba9481":"markdown","d852c8d2":"markdown","75164954":"markdown","ad8dfa9b":"markdown","370e316a":"markdown","fd466a1e":"markdown","187768ee":"markdown","e5975f4e":"markdown","4940b900":"markdown","16346746":"markdown","bb694f5c":"markdown","a2536271":"markdown","823c23bb":"markdown","d129a468":"markdown","46ed4af0":"markdown","8b33e853":"markdown","48d67616":"markdown","82dff81c":"markdown","4f26dceb":"markdown","635c015c":"markdown","4c692c18":"markdown","3ea1373f":"markdown","232395a6":"markdown","d67e36cd":"markdown","971274c9":"markdown","31411fb2":"markdown","e5fea73c":"markdown","ecfc4f0b":"markdown","b7dae059":"markdown","361e86f7":"markdown","1b3c3185":"markdown","3664bafd":"markdown","6cc59ccd":"markdown","abd8f844":"markdown","a64cbda7":"markdown","2288efd3":"markdown","a66fe5c7":"markdown","8a9a8763":"markdown","89ff4226":"markdown","a672f4b9":"markdown","eadd5562":"markdown","2cc59746":"markdown","b892cff8":"markdown","924fddd8":"markdown","f7e2727d":"markdown","33a0e31e":"markdown","b6939a86":"markdown","8c1ffbdc":"markdown","a0bb1578":"markdown","2b0a90a5":"markdown","36675e3a":"markdown","9f30d100":"markdown","b942b74e":"markdown"},"source":{"9231703b":"import pandas as pd\nimport numpy as np","ee0e42e3":"df = pd.read_csv(\"bank_marketing.csv\")\ndf.head(10)","8ce6dd8e":"df.info()","ffd66d98":"print(\"Bank marketing veri seti {rows} sat\u0131r veri i\u00e7ermektedir.\".format(rows = len(df)))","48fea875":"df.describe()","eb4d74c1":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8,8))\n\nsns.countplot(x = 'marital', hue = 'y', data=df, palette = 'inferno')\n\nplt.show()","57b6a5e8":"sns.distplot(df.age)","ccee69bb":"plt.figure(figsize=(12,8))\n\nsns.countplot(x = 'job', hue = 'y', data=df,palette = 'inferno')\n\nplt.xticks(rotation=45)\nplt.show()","10146c23":"plt.figure(figsize=(8,8))\n\nsns.countplot(x = 'contact', hue = 'y', data=df, palette = 'inferno')\n\nplt.xticks(rotation=45)\nplt.show()","cd6ec69b":"numerik_degiskenler=df.describe().columns\ndf.hist(column=numerik_degiskenler,figsize=(20,20))\nplt.show()","99763b46":"kategorik_degiskenler=df.describe(include=[object]).columns\n\nfig, axes = plt.subplots(4, 3, figsize=(20, 20))\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.7, hspace=0.3)\n\nfor i, ax in enumerate(axes.ravel()):\n    if i > 7:\n        ax.set_visible(False)\n        continue\n    sns.countplot(y = kategorik_degiskenler[i], data=df, ax=ax)\nplt.show()","fd1a48ff":"#age de\u011ferlerimizde outlier'lar var m\u0131 bakal\u0131m:\n\nplt.figure(dpi=130)\nsns.boxplot(df[\"age\"])\nplt.show()","67d33884":"missing_values = df.isnull().mean()*100\n\nmissing_values.sum()","95ec5ae0":"q1 = df[\"age\"].quantile(0.25)\nq3 = df[\"age\"].quantile(0.75)\n\niqr = q3 - q1\n\nprint(\"1st Quartile: \" + str(q1))\nprint(\"3rd Quartile: \" + str(q3))\nprint(\"Inter-quartile range: \" + str(iqr))","7cb43daa":"#age kolonu i\u00e7in alt ve \u00fcst s\u0131n\u0131rlar\u0131m\u0131z\u0131 belirlelim : bu s\u0131n\u0131rlar d\u0131\u015f\u0131ndaki veriler OUTLIER'lard\u0131r. \nlower_age_bound = q1 - 1.5*iqr\nupper_age_bound = q3 + 1.5*iqr\n\n# 1.5 genel olarak kullan\u0131lan say\u0131, fakat siz de\u011fi\u015ftirebilirsiniz. \n\nprint(\"Lower age bound: \" + str(lower_age_bound))\nprint(\"Upper age bound: \" + str(upper_age_bound))","41941ff7":"df.age.isnull().sum()","e0b5cfe0":"# kabul edilebilir range'in d\u0131\u015f\u0131ndaki de\u011ferleri (outlier'Lar\u0131) NON yapaca\u011f\u0131z.\ndf.age = df.age.map(lambda x: x if lower_age_bound < x < upper_age_bound else np.nan)\n\ndf.age.isnull().sum()\n#487 adet outlier verimiz oldu\u011funu tespit ettik. Bu sat\u0131rlar\u0131 veri setinden kald\u0131raca\u011f\u0131z.","81b908ce":"df_new = df[~df['age'].isnull()]\ndf_new","f944ec29":"print(len (df_new[df_new['pdays'] < 0] ) \/ len(df_new) * 100)\nprint(len (df_new[df_new['pdays'] > 400] ) \/ len(df_new) * 100)","9818fa8e":"def get_dummy_from_bool(row, column_name):\n    \n    return 1 if row[column_name] == 'yes' else 0\n#s\u00fctun i\u00e7eri\u011fi hay\u0131r ise 0, evet ise 1 d\u00f6nd\u00fcr\n\ndef get_correct_values(row, column_name, threshold, df):\n    \n    if row[column_name] <= threshold:\n        return row[column_name]\n    else:\n        mean = df[df[column_name] <= threshold][column_name].mean()\n        return mean\n#e\u011fer de\u011fer threshold yani s\u0131n\u0131r\u0131n \u00fczerindeyse ortalama de\u011feri ver","be85bba0":"#temizlenmi\u015f datay\u0131 haz\u0131rl\u0131yoruz\ndef clean_data(df):\n    \n    cleaned_df = df_new.copy() #ana veri setimizi kopyal\u0131yoruz\n    \n    bool_columns = ['default','housing','loan','y']\n    \n    for bool_col in bool_columns:\n        \n        cleaned_df[bool_col + '_bool'] = df.apply(lambda row: get_dummy_from_bool(row, bool_col),axis=1)\n    \n    cleaned_df = cleaned_df.drop(columns = bool_columns)\n    #evet ve hay\u0131r i\u00e7eren de\u011fi\u015fkenlere 0-1 at\u0131yoruz.\n    \n    cat_columns = ['job', 'marital', 'education', 'contact', 'month', 'poutcome']\n    \n    for col in  cat_columns:\n        \n        cleaned_df = pd.concat([cleaned_df.drop(col, axis=1),\n                                pd.get_dummies(cleaned_df[col], prefix=col, prefix_sep='_',\n                                               drop_first=True, dummy_na=False)], axis=1)\n    #kategorik veri i\u00e7eren de\u011fi\u015fkenler dummy de\u011fi\u015fkenlere d\u00f6n\u00fc\u015ft\u00fcr\u00fcld\u00fc\n    \n    cleaned_df = cleaned_df.drop(columns = ['pdays'])\n    #pdays kolonunu kald\u0131rmaya karar vermi\u015ftik\n    \n\n    return cleaned_df","f5b45d2f":"cleaned_df = clean_data(df_new)\ncleaned_df.head()","d8c40d9c":"cleaned_df.info()","3481edd9":"plt.figure(figsize=(40,40))\nsns.heatmap(cleaned_df.corr(),square=True,annot=True,cmap= 'Spectral')","1e250ffb":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split","e00086a4":"X = cleaned_df.drop('y_bool', axis = 1)\ny = cleaned_df['y_bool']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3 , random_state = 101)","af9c5499":"rfc = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n                            random_state=0, max_features= 10, \n                            max_depth= 5)\n\nrfc.fit(X_train, y_train)\n\nrfc_predict_1 = rfc.predict(X_test)","c54ab92f":"from sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix as cm\nfrom sklearn.metrics import accuracy_score\n\n\nprint(classification_report(y_test, rfc_predict_1))\n\nscore = round(accuracy_score(y_test, rfc_predict_1),3)\n\ncm1 = cm(y_test, rfc_predict_1) #confusion matrix (TP, TN , FP, FN)\n\nsns.heatmap(cm1, annot=True, fmt=\".1f\", linewidths=.3, square = True, cmap = 'PuBu')\n\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.title('Accuracy Score: {0}'.format(score), size = 20)\nplt.show()","bcefd1a2":"y_is_equal_to_0_data = cleaned_df[cleaned_df.y_bool == 0]\ny_is_equal_to_1_data = cleaned_df[cleaned_df.y_bool == 1]\n\nprint(len(y_is_equal_to_0_data), len(y_is_equal_to_1_data))","da2b3052":"#Hedef de\u011fi\u015fkenimizin 0 oldu\u011fu g\u00f6zlem say\u0131s\u0131n\u0131 azaltarak 5.000 seviyesine \u00e7ekelim\nfrom sklearn.utils import resample\ndownsampled0s = resample(y_is_equal_to_0_data, replace=False, n_samples=5000, random_state=101)\n\nprint(len(downsampled0s), len(y_is_equal_to_1_data))","78affc8b":"data_downsampled = pd.concat([y_is_equal_to_1_data , downsampled0s])\ndata_downsampled.shape","39236ca1":"X_down = data_downsampled.drop('y_bool', axis=1)\ny_down = data_downsampled['y_bool']\n\nX_train, X_test, y_train, y_test = train_test_split(X_down,y_down,test_size = 0.3 , random_state = 101)\n\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\n\nrfc = RandomForestClassifier(n_estimators=100, n_jobs=-1,\n                            random_state=0, max_features= 10, \n                            max_depth= 5)\n\nrfc.fit(X_train, y_train)\n\nrfc_predict_1 = rfc.predict(X_test)","9f8b9dfe":"print(classification_report(y_test, rfc_predict_1))\n\nscore5 = round(accuracy_score(y_test, rfc_predict_1),3)\n\ncm5 = cm(y_test, rfc_predict_1) \n\nsns.heatmap(cm5, annot=True, fmt=\".1f\", linewidths=.3, square = True, cmap = 'PuBu')\n\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.title('Accuracy Score: {0}'.format(score5), size = 20)\nplt.show()","c34a4365":"from sklearn.model_selection import cross_val_score\n\nprint(cross_val_score(RandomForestClassifier(n_estimators = 100, \n                                             n_jobs=-1, \n                                             random_state=0, \n                                             max_features= 10, \n                                             max_depth= 5), \n                       X_train, y_train, cv=5\n                      )\n     )","811ec07a":"print('Mean of cv-scores: {0}'.format(round(np.mean(cross_val_score(RandomForestClassifier(n_estimators=100, \n                                                                                           n_jobs=-1, \n                                                                                           random_state=101, \n                                                                                           max_features= 10, \n                                                                                            max_depth= 5), \n                                                                     X_train, y_train, cv=5)\n                                                    ),3)\n                                        )\n     )","7dc9571c":"from xgboost import XGBClassifier","79a9b6c1":"X = cleaned_df.drop('y_bool', axis = 1)\ny = cleaned_df['y_bool']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state = 101)","06fcd205":"xgb = XGBClassifier(n_estimators=100, subsample=0.5,colsample_bytree=1, max_depth= 5)\n\nxgb.fit(X_train,y_train.squeeze().values)\n\nxgb_predict_1 = xgb.predict(X_test)","ea755ced":"print('XGB accuracy score: %.3f' % (accuracy_score(y_test, xgb_predict_1)))","e138779d":"print(classification_report(y_test, xgb_predict_1))\n\nscore2 = round(accuracy_score(y_test, xgb_predict_1),3)\n\ncm2 = cm(y_test, xgb_predict_1) \n\nsns.heatmap(cm2, annot=True, fmt=\".1f\", linewidths=.3, square = True, cmap = 'Greens')\n\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.title('Accuracy Score: {0}'.format(score2), size = 20)\nplt.show()","106430df":"y_is_equal_to_0_data = cleaned_df[cleaned_df.y_bool == 0]\ny_is_equal_to_1_data = cleaned_df[cleaned_df.y_bool == 1]\n\nprint(len(y_is_equal_to_0_data), len(y_is_equal_to_1_data))\n\nfrom sklearn.utils import resample\ndownsampled0s = resample(y_is_equal_to_0_data, replace=False, n_samples=5000, random_state=101)\n\nprint(len(downsampled0s), len(y_is_equal_to_1_data))","413084f2":"data_downsampled = pd.concat([y_is_equal_to_1_data , downsampled0s])\ndata_downsampled.shape","bd061b51":"X_down = data_downsampled.drop('y_bool', axis=1)\ny_down = data_downsampled['y_bool']\n\nX_train, X_test, y_train, y_test = train_test_split(X_down,y_down,test_size = 0.3 , random_state = 101)\n\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\n\nxgb = XGBClassifier(n_estimators=100, subsample=0.5,colsample_bytree=1, max_depth= 5)\n\nxgb.fit(X_train,y_train.squeeze().values)\n\nxgb_predict_1 = xgb.predict(X_test)\n\nprint('XGB accuracy score: %.3f' % (accuracy_score(y_test, xgb_predict_1)))","f428b56b":"print(classification_report(y_test, xgb_predict_1))\n\nscore6 = round(accuracy_score(y_test, xgb_predict_1),3)\n\ncm6 = cm(y_test, xgb_predict_1) \n\nsns.heatmap(cm6, annot=True, fmt=\".1f\", linewidths=.3, square = True, cmap = 'Greens')\n\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.title('Accuracy Score: {0}'.format(score6), size = 20)\nplt.show()","4d7886e6":"from sklearn.model_selection import train_test_split\n\nX = cleaned_df.drop('y_bool', axis = 1)\ny = cleaned_df['y_bool']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y , test_size = 0.3, shuffle=True, stratify = y, random_state = 0)","05b0674e":"#Normalizasyon \u2013 Feature Scaling yap\u0131yoruz (Fitting)\nfrom sklearn.preprocessing import StandardScaler\n\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)","e64dd34e":"from sklearn.svm import SVC\n\nclassifier = SVC(kernel='linear', random_state = 0)\nclassifier.fit(X_train, y_train)","ef2f826d":"y_pred = classifier.predict(X_test)","ef014ecb":"print(classification_report(y_test, y_pred))\n\nscore3 = round(accuracy_score(y_test, y_pred),3)\n\ncm3 = cm(y_test, y_pred) #confuison matrix\n\nsns.heatmap(cm3, annot=True, fmt=\".1f\", linewidths=.3, square = True, cmap = 'coolwarm_r')\n\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.title('Accuracy Score: {0}'.format(score3), size = 20)\nplt.show()","2c3294fe":"y_is_equal_to_0_data = cleaned_df[cleaned_df.y_bool == 0]\ny_is_equal_to_1_data = cleaned_df[cleaned_df.y_bool == 1]\n\nprint(len(y_is_equal_to_0_data), len(y_is_equal_to_1_data))","029a7b22":"from sklearn.utils import resample\ndownsampled0s = resample(y_is_equal_to_0_data, replace=False, n_samples=5000, random_state=101)\n\nprint(len(downsampled0s), len(y_is_equal_to_1_data))","449e23b7":"data_downsampled = pd.concat([y_is_equal_to_1_data , downsampled0s])\ndata_downsampled.shape","2cb082dc":"X_down = data_downsampled.drop('y_bool', axis=1)\ny_down = data_downsampled['y_bool']\n\nX_train, X_test, y_train, y_test = train_test_split(X_down, y_down , test_size = 0.3, shuffle=True, stratify = y_down, random_state = 101)\n\nsc_X = StandardScaler()\nX_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\n\nclassifier = SVC(kernel='linear', random_state = 0)\nclassifier.fit(X_train, y_train)\ny_pred_2 = classifier.predict(X_test)","f00d8aa7":"print(classification_report(y_test, y_pred_2))\n\nscore4 = round(accuracy_score(y_test, y_pred_2),3)\n\ncm4 = cm(y_test, y_pred_2) \n\nsns.heatmap(cm4, annot=True, fmt=\".1f\", linewidths=.3, square = True, cmap = 'coolwarm_r')\n\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.title('Accuracy Score: {0}'.format(score4), size = 20)\nplt.show()","71ba9481":"<b>Ay\u0131rd\u0131\u011f\u0131m\u0131z test setimizi (X_test) kullanarak olu\u015fturdu\u011fumuz model ile tahmin yapal\u0131m  ve elde etti\u011fimiz de\u011ferler (y_pred) ile hedef de\u011fi\u015fken (y_test) test setimizi kar\u015f\u0131la\u015ft\u0131ral\u0131m","d852c8d2":"<b>Modelimizi 5 farkl\u0131 e\u011fitim ve test veri setine b\u00f6ld\u00fc\u011f\u00fcm\u00fczde ortalamas\u0131 %83.2 \u00e7\u0131kt\u0131. \n    <br>Yani RFC'da bulmu\u015f oldu\u011fumuz %84 do\u011fruluk oran\u0131 overfitting'den kaynakl\u0131 olmad\u0131\u011f\u0131n\u0131 g\u00f6rm\u00fc\u015f olduk. Modelimiz gayet iyi \u00e7al\u0131\u015fm\u0131\u015f.\n<b>","75164954":"<b> Hedef de\u011fi\u015fkenimizin 0 oldu\u011fu g\u00f6zlem say\u0131s\u0131n\u0131 azaltarak 5.000 seviyesine \u00e7ekelim","ad8dfa9b":"Kullanm\u0131\u015f oldu\u011fumuz modelleri kar\u015f\u0131la\u015ft\u0131r\u0131rken accuracy, precision ve F1 de\u011ferlerini inceliyoruz.\n<br>Accuracy en y\u00fcksek <b>%85.1<\/b> ile <i>XGBoost<\/i> algoritmas\u0131 oluyor, daha sonra <b>84.1<\/b> <i>Linear SVC<\/i> ve <b>83.8<\/b> <i>Random Forest<\/i> geliyor.\n\n<br> F1 de\u011ferlerine bakt\u0131\u011f\u0131m\u0131zda ise <b>%85<\/b> ile <i>XGBoost<\/i> yine en iyi tahminde bulunan model oluyor.","370e316a":"<b>Her sat\u0131r i\u00e7in eksik de\u011ferlerin y\u00fczdesine bakt\u0131\u011f\u0131m\u0131zda null de\u011fer olmad\u0131\u011f\u0131n\u0131 g\u00f6r\u00fcyoruz. \n   E\u011fer eksik de\u011ferler olsayd\u0131 bunlar\u0131 medyan, ortalama veya mod ile doldurmam\u0131z gerekecekti.<b>","fd466a1e":"## DownSampling","187768ee":"## ML B\u00f6l\u00fcm\u00fc - Tahminleyici Analitik Algoritmalar\u0131n\u0131n Uygulanmas\u0131","e5975f4e":"## <b>DOWNSAMPLING  \n    Veri setimizi dengelemek i\u00e7in burada downsapling y\u00f6ntemini uygulamay\u0131 uygun g\u00f6rd\u00fck","4940b900":"# Comparison ML Models","16346746":"<b>Bu g\u00f6rselde n\u00fcmerik de\u011fi\u015fkenlerimizin tamam\u0131n\u0131n histogram\u0131n\u0131 inceledik.<b>","bb694f5c":"XGboost modeli i\u00e7in train ve test veri setlerini olu\u015fturuyoruz ve parametrelerini belirliyoruz.\n<br> \nXGB model ayarlar\u0131 i\u00e7in bir \u00e7ok parametre mevcut. Bizim deneme yan\u0131lmalar sonras\u0131 en uygun bulup kullanm\u0131\u015f olduklar\u0131m\u0131z \u015fu \u015fekilde <br> \n-- n_estimators -> modelde kurulacak a\u011fa\u00e7 say\u0131s\u0131<br> \n-- max_depth -> a\u011fac\u0131n derinli\u011fini ifade eder<br> \n-- colsample_bytree -> her bir a\u011fac\u0131 olu\u015ftururken s\u00fctunlar\u0131n olu\u015fturdu\u011fu alt veri setleridir,default u 1 dir<br> \n-- subsample -> e\u011fitim \u00f6rneklerinin alt \u00f6rneklere oran\u0131. Bu de\u011feri 0.5 olarak ayarlamak XGBoost a\u011fa\u00e7lar\u0131n\u0131 b\u00fcy\u00fctmeden \u00f6nce e\u011fitim verilerinin yar\u0131s\u0131n\u0131 rastgele train edece\u011fi anlam\u0131na gelir. Bu da overfitting i \u00f6nler.","a2536271":"<b> \u00dcstteki g\u00f6rselde g\u00f6rd\u00fc\u011f\u00fcm\u00fcz \u00fczere bankada vadeli para tutma oranlar\u0131 bekar insanlarda daha y\u00fcksek <b>","823c23bb":"<b>Bu g\u00f6rselde kategorik de\u011fi\u015fkenlerimizin tamam\u0131n\u0131 \u00e7ubuk grafikte inceledik.<b>","d129a468":"<b>Sonu\u00e7 olarak Gradient Boosting XGBoost modeli, potansiyel bir m\u00fc\u015fterinin vadeli mevduat a\u00e7t\u0131r\u0131p a\u00e7t\u0131rmayaca\u011f\u0131n\u0131 tahmin etmek i\u00e7in en iyi model oldu\u011funa karar veriyoruz.","46ed4af0":"Modelimizin y\u00fcksek performans\u0131n\u0131n rastgele olup olmad\u0131\u011f\u0131n\u0131z g\u00f6rmemiz i\u00e7in cv uygulayaca\u011f\u0131z.","8b33e853":"### <b> 1- Tahminleme algoritmalar\u0131nda ilk olarak  <i>Random Forest Classifier<\/i> se\u00e7iyoruz.<b>","48d67616":"<b>Yukar\u0131daki g\u00f6rselde meslek da\u011f\u0131l\u0131mlar\u0131na g\u00f6re ki\u015filerin vadeli hesap adetlerini g\u00f6r\u00fcyoruz. Oran olarak bakt\u0131\u011f\u0131m\u0131zda \u00f6\u011frencilerin vadeli hesap adetlerinin y\u00fcksek oldu\u011fu g\u00f6zleniyor, bunun yan\u0131 s\u0131ra mavi yakal\u0131 olarak ifade edilen \u00e7al\u0131\u015fanlar\u0131n ise vadeli hesap oran\u0131n\u0131n \u00e7ok d\u00fc\u015f\u00fck oldu\u011fu g\u00f6r\u00fclmektedir. <b>","82dff81c":"<b>Scikit-Learn k\u00fct\u00fcphanesinden svm mod\u00fcl\u00fcn\u00fc import ederek classifier nesnemizi tan\u0131ml\u0131yoruz","4f26dceb":"Veri setimizin %30'unu test i\u00e7in ay\u0131rd\u0131k. \nStratify: Bu parametreye hedef de\u011fi\u015fkenimi (y) de\u011ferini giriyorum. Bu \u015fekilde b\u00f6ld\u00fc\u011f\u00fcm t\u00fcm veri gruplar\u0131nda y'nin oran\u0131n\u0131n veri setim ile ayn\u0131 olmas\u0131n\u0131 sa\u011fl\u0131yorum. \nBu stratify'\u0131 train_test_split'i kullan\u0131rken belirlemeliyim. \nHer \u00e7al\u0131\u015ft\u0131rd\u0131\u011f\u0131m\u0131zda train ve test verileri de\u011fi\u015fmesin diye random_state'e bir de\u011fer girdik.","635c015c":"pdays kolonu m\u00fc\u015fteriyle bir \u00f6nceki kampanya i\u00e7in ileti\u015fime ge\u00e7ildikten sonraki g\u00fcn say\u0131s\u0131n\u0131 g\u00f6steriyor.\u0130lgili kolonun detay\u0131na bakt\u0131\u011f\u0131m\u0131zda -1 i\u00e7eren de\u011ferin y\u00fczde 82lik b\u00fcy\u00fck bir b\u00f6l\u00fcm\u00fc olu\u015fturdu\u011funu. 400 ve \u00fczerindeki de\u011ferlerin ise %0.52 sini olu\u015fturdu\u011funu g\u00f6r\u00fcyoruz. \n-1 muhtemelen m\u00fc\u015fteriyle daha \u00f6nce ileti\u015fime ge\u00e7ilmedi\u011fi veya eksik verileri temsil etti\u011fi anlam\u0131na geliyor diye yorumlad\u0131k. Bu kolonun modelimize bir faydas\u0131 olmayaca\u011f\u0131n\u0131 d\u00fc\u015f\u00fcnd\u00fc\u011f\u00fcm\u00fcz i\u00e7in kald\u0131raca\u011f\u0131z.","4c692c18":"Hedef de\u011fi\u015fkenimizi y ye atad\u0131k ve train_test_split ile veri setimizi train(%70) ve test(%30) veri setlerine ay\u0131rd\u0131k. Daha sonras\u0131nda RF modelimizi kuraca\u011f\u0131z.","3ea1373f":"Modelimizi kurarken tree adedini 100, maksimum derinli\u011fini 5 olarak set ettik. N_jobs -1 ise t\u00fcm CPU kullan\u0131lacak anlam\u0131na geliyor.\nModelimize train olarak ay\u0131rd\u0131\u011f\u0131m\u0131z veri setini fit ettikten sonra, test verileri tahminlemeye \u00e7al\u0131\u015f\u0131yoruz.","232395a6":"<b> Problemimizin i\u00e7eri\u011fini tekrar hat\u0131rlatmak gerekirse; \n    <br>Portekiz bankas\u0131ndan al\u0131nm\u0131\u015f olan veri setinde kurumun telefon g\u00f6r\u00fc\u015fmeleri ile yapm\u0131\u015f oldu\u011fu pazarlama kampanyalar\u0131 detaylar\u0131 yer almaktad\u0131r. Bu telefon g\u00f6r\u00fc\u015fmeleri sonucunda m\u00fc\u015fterinin \"vadeli mevduat\" \u00fcr\u00fcn\u00fcne sahip olup olmayaca\u011f\u0131 (\"evet\" ya da \"hay\u0131r\") tahminlenmeye \u00e7al\u0131\u015f\u0131lm\u0131\u015ft\u0131r. Problemimizin girdi ve \u00e7\u0131kt\u0131lar\u0131 betimleyici analitik k\u0131sm\u0131nda detayl\u0131 anlat\u0131lm\u0131\u015ft\u0131r.\n    <br> Betimleyici analitik, veri temizleme ve i\u015fleme ad\u0131mlar\u0131n\u0131 tamamlad\u0131ktan sonra m\u00fc\u015fterinin \"vadeli mevduat\" \u00fcr\u00fcn\u00fcne sahip olup olmayaca\u011f\u0131n\u0131 \u00fc\u00e7 farkl\u0131 tahminleme modeli ile \u00e7al\u0131\u015ft\u0131k. \n    <br> Bu modeller; \n    <i><br> 1) Random Forest Classifier\n    <br> 2) Gradient Boosting (XGBoost Classifier)\n    <br> 3) Linear Support Vector Classifier<\/i>","d67e36cd":"<b>Portekiz bankas\u0131ndan al\u0131nm\u0131\u015f olan bu veri seti i\u00e7eri\u011finde kurumun telefon g\u00f6r\u00fc\u015fmeleri ile yapm\u0131\u015f oldu\u011fu pazarlama kampanyalar\u0131 detaylar\u0131 yer almaktad\u0131r. Bu telefon g\u00f6r\u00fc\u015fmeleri sonucunda m\u00fc\u015fterinin \"vadeli mevduat\" \u00fcr\u00fcn\u00fcne sahip olup olmayaca\u011f\u0131 (\"evet\" ya da \"hay\u0131r\") tahminlenmeye \u00e7al\u0131\u015f\u0131lm\u0131\u015ft\u0131r. \nVeri i\u00e7eri\u011finde 45211 instances ve 17 attributes bulunmaktad\u0131r.<b>","971274c9":"<b>H\u00fccresel ileti\u015fim t\u00fcr\u00fc olan m\u00fc\u015fterilerin vadeli mevduat hesap adedi en y\u00fcksek oldu\u011fu g\u00f6r\u00fcl\u00fcyor.<b>","31411fb2":"<b>interquartile range belirliyoruz <b>","e5fea73c":"<b>Classification Report'ta g\u00f6r\u00fcld\u00fc\u011f\u00fc \u00fczere 0'lar\u0131 tahminlemede biraz geriledik; ba\u015far\u0131 %85'e d\u00fc\u015fm\u00fc\u015f. Ancak 1'leri tahminlemede olduk\u00e7a ilerleme kaydetmi\u015f olduk(%85),Downsampling \u00f6ncesi %53 t\u00fc. \n  <br>Accuracy de\u011ferimiz ise %91 den %85'e geriledi fakat kabul edilebilir bir oran bu da.\n<br>Confusion Matrix'e g\u00f6re modelimiz; \n  \n  0'lar\u0131n 1.259'unu do\u011fru, 244 adedini ise hatal\u0131 tahminlemi\u015f. \n    \n  1'lerin ise; 1.314 adedini do\u011fru, 205 adedini hatal\u0131 tahminlemi\u015f.\n\n  G\u00f6r\u00fcld\u00fc\u011f\u00fc \u00fczere modelimiz art\u0131k 1'leri de daha do\u011fru tahminler durumu geldi.","ecfc4f0b":"## Betimleyici Analitik Ad\u0131mlar\u0131","b7dae059":"## Veri Temizleme ve \u0130\u015fleme Ad\u0131mlar\u0131","361e86f7":"<b>Verinin mod,medyan, ortalama gibi de\u011ferlerini inceliyoruz.<b>","1b3c3185":"<b>3)\n<img src=\"LS.png\">","3664bafd":"<b>Classification Report'ta g\u00f6r\u00fcld\u00fc\u011f\u00fc \u00fczere 0'lar\u0131 tahminlemede biraz geriledik; ba\u015far\u0131 %84'e d\u00fc\u015fm\u00fc\u015f. Ancak 1'leri tahminlemede olduk\u00e7a ilerleme kaydetmi\u015f olduk(%84),Downsampling \u00f6ncesi %25ti. \n  <br>Accuracy de\u011ferimiz ise %90 dan %84'e geriledi fakat kabul edilebilir bir oran bu da.\n<br>Confusion Matrix'e g\u00f6re modelimiz; \n  \n  0'lar\u0131n 1.247'sini do\u011fru, 256 adedini ise hatal\u0131 tahminlemi\u015f. \n    \n  1'lerin ise; 1.284 adedini do\u011fru, 235 adedini hatal\u0131 tahminlemi\u015f.\n\n  G\u00f6r\u00fcld\u00fc\u011f\u00fc \u00fczere modelimiz art\u0131k 1'leri de daha do\u011fru tahminler durumu geldi.","6cc59ccd":"<b>2)\n<img src=\"XGBoost.png\">","abd8f844":"<b> Bu g\u00f6rselde ya\u015f de\u011fi\u015fkenin outlier de\u011ferleri oldu\u011funu tespit ettik.<b>","a64cbda7":"### <b>3- Tahminleme algoritmalar\u0131nda son olarak  <i>Linear Support Vector Classifier<\/i> se\u00e7iyoruz.<b>","2288efd3":"XGboost algoritmas\u0131 gradient boosting algoritmas\u0131n\u0131n \u00e7e\u015fitli d\u00fczenler sonras\u0131 y\u00fcksek performans g\u00f6steren \u015feklidir. Bir decision tree algoritmas\u0131d\u0131r.","a66fe5c7":"##\u00a0Cross-Validation","8a9a8763":"<b>Classification Report'ta g\u00f6r\u00fcld\u00fc\u011f\u00fc \u00fczere 0'lar\u0131 tahminlemede biraz geriledik; ba\u015far\u0131 %84'e d\u00fc\u015fm\u00fc\u015f. Fakat 1'leri tahminlemede olduk\u00e7a ilerleme kaydetmi\u015f olduk(%84). Accuracy de\u011ferimiz ise %90 dan %84'e geriledi.\n<br>Confusion Matrix'e g\u00f6re modelimiz; \n  \n  0'lar\u0131n 1.258'ini do\u011fru, 242 adedini ise hatal\u0131 tahminlemi\u015f. \n    \n  1'lerin ise; 1.283 adedini do\u011fru, 239 adedini hatal\u0131 tahminlemi\u015f.\n\n  G\u00f6r\u00fcld\u00fc\u011f\u00fc \u00fczere modelimiz art\u0131k 1'leri de daha do\u011fru tahminler durumu geldi.","89ff4226":"<b>Vadeli yat\u0131rmak ile di\u011fer de\u011fi\u015fkenler aras\u0131ndaki pozitif korelasyona bakt\u0131\u011f\u0131m\u0131zda en y\u00fcksek \u00f6znitelikler \u015fu \u015fekilde;\n- \"Duration\" kolonu yani son telefon konu\u015fmas\u0131n\u0131n uzunlu\u011fu 0.39 ile en y\u00fcksek pozitif etki yapt\u0131\u011f\u0131 g\u00f6zlemleniyor. \n- \"poutcome success\" yani bir \u00f6nceki pazarlama kampanyas\u0131n\u0131n ba\u015far\u0131l\u0131 sonu\u00e7 vermesi 0.31 ile ikinci en y\u00fcksek pozitif etkiyi yapt\u0131\u011f\u0131n\u0131 g\u00f6r\u00fcyoruz.\n- Ev kredisi al\u0131nmamas\u0131 da y\u00fcksek etki edenler aras\u0131nda.\n- Ayr\u0131ca kampanya i\u00e7in aranan mart, eyl\u00fcl ve ekim aylar\u0131 en \u00e7ok pozitif etki yaratan aylar olarak g\u00f6ze \u00e7arp\u0131yor.\n    <b>","a672f4b9":"Decision tree algoritmalar\u0131nda en b\u00fcy\u00fck problem a\u015f\u0131r\u0131 \u00f6\u011frenme ve veriyi ezberlemektir. RF modeli ise bu problemi \u00e7\u00f6zmek i\u00e7in hem veri setinden hem de \u00f6znitelik setinden rassal olarak farkl\u0131 alt setler se\u00e7er ve bunlar\u0131 train eder. Farkl\u0131 veri setleri \u00fczerinde e\u011fitim ger\u00e7ekle\u015fti\u011fi i\u00e7in overfitting problemi azal\u0131r. Sonunda ise problem regresyon ise decision tree tahminlerinin ortalamas\u0131, s\u0131n\u0131fland\u0131rma ise tahminler aras\u0131nda en \u00e7ok oy alan\u0131 se\u00e7eriz.\n","eadd5562":"<b> ML k\u0131sm\u0131na ge\u00e7meden \u00f6nce veri setimizi i\u015fleme ad\u0131m\u0131n\u0131 yapmam\u0131z gerekmektedir. \n    Veri setimizi inceledi\u011fimizde hedef de\u011fi\u015fken olan \"y\" i\u00e7eri\u011findeki \"yes\", \"no\" de\u011ferlerine binary(0,1) hale getirmemiz gerekmektedir. \n    Ayn\u0131 \u015fekilde cinsiyet, e\u011fitim durumu vs i\u00e7eren kategorik s\u00fctunlar\u0131 da dummy de\u011fi\u015fkenlere d\u00f6n\u00fc\u015ft\u00fcrmemiz gerekmektedir.<b>","2cc59746":"<b>IQR sonras\u0131 10 ya\u015f alt\u0131 ve 70 ya\u015f \u00fcst\u00fc de\u011ferlerin outlier oldu\u011funu g\u00f6r\u00fcyoruz. Bu nedenle bu verileri kald\u0131raca\u011f\u0131z.<b>","b892cff8":"### <b>2- Tahminleme algoritmalar\u0131nda ikinci olarak  <i>XGBoost'u <\/i> se\u00e7iyoruz.<b>","924fddd8":"<b>Veri tiplerini inceliyoruz<b>","f7e2727d":"<b>\u015euan hedef de\u011fi\u015fkenimize g\u00f6re daha dengeli da\u011f\u0131l\u0131m g\u00f6steren bir veriseti elde etmi\u015f olduk. \u015eimdi yukar\u0131da uygulad\u0131\u011f\u0131m\u0131z ad\u0131mlar\u0131 yeniden uygulayarak tekrar Linear SVC modelini kuraca\u011f\u0131z","33a0e31e":"<b>1)\n<img src=\"RFC.png\">","b6939a86":"<b> Classification Report ve Confusion Matrix e tekrar bakal\u0131m ","8c1ffbdc":"Classification Report'a g\u00f6re; Accuracy de\u011ferimiz %89.6. F1 scorelara bakt\u0131\u011f\u0131m\u0131zda ise 0'lar\u0131 %94 ba\u015far\u0131yla tahmin ediyor. Fakat 1'leri tahmin etmekte \u00e7ok ba\u015far\u0131l\u0131 de\u011fil (%29).\nVeri setinde hedef de\u011ferlerimizde 1'ler 0'lara g\u00f6re \u00e7ok az oldu\u011fu i\u00e7in model 1'leri iyi tahminleyemiyor. \nBunu \u00e7\u00f6z\u00fcmlemek i\u00e7in veri setindeki bu dengesizli\u011fi a\u015fa\u011f\u0131daki 2 y\u00f6ntemden birini uygulayarak gidermemiz gerekiyor:\n1.downsampling: y de\u011feri 0 olan g\u00f6zlemler i\u00e7erisinden y de\u011feri 1 olan g\u00f6zlem say\u0131s\u0131 kadar g\u00f6zlem se\u00e7ilerek yeni bir veriseti olu\u015fturulur.\n2.upsampling","a0bb1578":"<img src=\"11.png\">","2b0a90a5":"<b>Modelimizin ba\u015far\u0131 metriklerine bakmak i\u00e7in confusion matrix, precision, recall ve f-1 scorelar\u0131n\u0131 hesaplad\u0131k. \n    <br>Accuracy score yani do\u011fruluk oran\u0131na bakt\u0131\u011f\u0131m\u0131zda hi\u00e7 g\u00f6rmedi\u011fimiz bir veri setinde %90 gibi bir oran yakalam\u0131\u015f\u0131z. Burada overfitting var m\u0131 yok mu onu kontrol etmemiz gerekiyor, bunun i\u00e7in bir sonraki ad\u0131mda cross-validation metodunu uygulamam\u0131z gerekmektedir. \n    <br>Precision de\u011ferinin y\u00fcksek olmas\u0131 da model se\u00e7imlerinde \u00f6nemli bir kriterdir. Burda y\u00fczde 90 ve y\u00fczde 77 ile asl\u0131nda bu modelin veri setimize uygun olabilece\u011fi d\u00fc\u015f\u00fcncesi ortaya \u00e7\u0131k\u0131yor.\n    <br>\n      \n  0'lar\u0131n 11.844'\u00fcn\u00fc do\u011fru, 67 adedini ise hatal\u0131 tahminlemi\u015f. \n    \n  1'lerin ise; 221 adedini do\u011fru, 1286 adedini hatal\u0131 tahminlemi\u015f.\n\n<br>\n   Veri setinde hedef de\u011ferlerimizde 1'ler 0'lara g\u00f6re \u00e7ok az oldu\u011fu i\u00e7in model 1'leri iyi tahminleyemiyor. \nBunu \u00e7\u00f6z\u00fcmlemek i\u00e7in veri setindeki bu dengesizli\u011fi a\u015fa\u011f\u0131daki 2 y\u00f6ntemden birini uygulayarak gidermemiz gerekiyor:\n1.downsampling: y de\u011feri 0 olan g\u00f6zlemler i\u00e7erisinden y de\u011feri 1 olan g\u00f6zlem say\u0131s\u0131 kadar g\u00f6zlem se\u00e7ilerek yeni bir veriseti olu\u015fturulur.\n2.upsampling\n","36675e3a":"<b>Bu g\u00f6rselde ya\u015f de\u011ferlerinin da\u011f\u0131l\u0131m\u0131n\u0131 inceledik. \u0130leti\u015fime ge\u00e7ilen m\u00fc\u015fterilerin ya\u015flar\u0131 30 ile 40 aras\u0131nda yo\u011funla\u015ft\u0131\u011f\u0131 g\u00f6zleniyor. <b>","9f30d100":"Not: G\u00f6rsel biraz b\u00fcy\u00fck oldu\u011fu i\u00e7in okunmas\u0131 zor ancak sadece bu veriyi analiz etmek bile kampanya hedefini belirleme a\u015famas\u0131nda b\u00fcy\u00fck rol oynayabilir.","b942b74e":"<b>Modelimizin ba\u015far\u0131 metriklerine bakmak i\u00e7in confusion matrix, precision, recall ve f-1 scorelar\u0131n\u0131 hesaplad\u0131k. \n    <br>Accuracy score yani do\u011fruluk oran\u0131na bakt\u0131\u011f\u0131m\u0131zda %91 gibi bir oran yakalam\u0131\u015f\u0131z. \n    <br>Precision de\u011ferinin y\u00fcksek olmas\u0131 da model se\u00e7imlerinde \u00f6nemli bir kriterdir. Burda vadeli mevduat almayacaklar\u0131 tahminlemede y\u00fczde 94 ile \u00e7ok iyi bir oran yakalam\u0131\u015f\u0131z ancak vadeli mevduat alacaklar\u0131 tahminlemede y\u00fczde 61 ile d\u00fc\u015f\u00fck bir oran gelmi\u015f.\n    <br>  \n  0'lar\u0131n 11.458'ini do\u011fru, 453 adedini ise hatal\u0131 tahminlemi\u015f. \n    \n  1'lerin ise; 715 adedini do\u011fru, 792 adedini hatal\u0131 tahminlemi\u015f.\n     <br> Bu modelimizde de 1 lerin oran\u0131n\u0131 artt\u0131rmak i\u00e7in downsampling deneyebiliriz.\n        "}}