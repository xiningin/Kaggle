{"cell_type":{"62af43ac":"code","faff31b2":"code","1dc0b544":"code","37f17cca":"code","63d90fc4":"code","e25d6b56":"code","0cc66ce7":"code","5d5e701b":"code","5b24c0cf":"code","3acdd82a":"code","52d80c60":"code","ed05728c":"code","dbe49ef8":"code","624361ec":"code","66e1a45b":"code","65130663":"code","ba8c3869":"code","ca68e5e0":"code","a6b345e0":"code","5ecea9ea":"code","4e0094ac":"code","b10f84f6":"code","eae72b91":"code","eaecfc33":"code","b7c634e2":"code","9fcfb3d2":"code","77f32102":"code","330174a0":"code","11781de7":"code","1d85400f":"code","dca24fbb":"code","735ea053":"code","e26edccd":"code","29f386ee":"code","70c8ab5a":"code","9778c252":"code","3f7e576a":"code","036095a7":"code","d487b3b8":"code","10141de8":"code","50ab18b8":"code","174c2a35":"code","973d0c39":"code","0c5dd914":"code","13a652a9":"code","0a5fac9d":"code","b02295d1":"code","0adff63d":"code","c8feece3":"code","8c98e185":"markdown","521ea4a3":"markdown","29cb9d69":"markdown","bc09e3f9":"markdown","2098ca0c":"markdown","e417dec1":"markdown","208c2ab3":"markdown","a90e9120":"markdown","a37e3bdf":"markdown","705cc110":"markdown","541b841a":"markdown","641a3c59":"markdown","98b92ae8":"markdown","0d89c04e":"markdown","47f39213":"markdown","53fd2428":"markdown","c1c64d8c":"markdown"},"source":{"62af43ac":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","faff31b2":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score,plot_confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom string import punctuation\nfrom nltk import pos_tag\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import wordnet, stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense","1dc0b544":"df = pd.read_csv(\"\/kaggle\/input\/dataisbeautiful\/r_dataisbeautiful_posts.csv\")","37f17cca":"df.head()","63d90fc4":"df.isna().sum()","e25d6b56":"# deleting columns.\ndel df['author_flair_text']\ndel df['removed_by']\ndel df['total_awards_received']\ndel df['awarders']\ndel df['id']\ndel df['created_utc']\ndel df['full_link']","0cc66ce7":"df.head()","5d5e701b":"df.isna().sum()","5b24c0cf":"# taking care of nan in title\ndf.title.fillna(\" \",inplace = True)","3acdd82a":"df['text'] = df['title'] + ' ' + df['author']\ndel df['title']\ndel df['author']","52d80c60":"df.head()","ed05728c":"df.over_18.replace([True, False], [1, 0], inplace=True)","dbe49ef8":"df.over_18.value_counts()","624361ec":"over18text_false = df[df.over_18 == 0.0].text\nover18text_true = df[df.over_18 == 1.0].text","66e1a45b":"train_text = df.text.values[:100000]\ntest_text = df.text.values[100000:]\ntrain_category = df.over_18[:100000]\ntest_category = df.over_18[100000:]","65130663":"plt.figure(figsize = (20,20))\nwc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(str(\" \".join(over18text_true)))\nplt.imshow(wc,interpolation = 'bilinear')","ba8c3869":"plt.figure(figsize = (20,20))\nwc = WordCloud(min_font_size = 3,  max_words = 3000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(str(\" \".join(over18text_false)))\nplt.imshow(wc,interpolation = 'bilinear')","ca68e5e0":"# Getting the most frequently used words from wordcloud\ntext_true = wc.process_text(str(\" \".join(over18text_true)))\ntext_true","a6b345e0":"text_true = sorted(text_true.items(),key = lambda kv:(kv[1], kv[0]))","5ecea9ea":"ans_true = []\nfor i in text_true:\n    ans_true.append(i[0])\nans_true [:5] ","4e0094ac":"predictions = []\nfor i in test_text:\n    x = i.split()\n    for j in x:\n        if j in ans_true:\n            predictions.append(1)\n            break\n        else:\n            predictions.append(0)\n            break\nlen(predictions)","b10f84f6":"count = 0\nfor i in range(len(predictions)):\n    test_category = list(test_category)\n    if(predictions[i] == int(test_category[i])):\n        count += 1\nprint(count)","eae72b91":"accuracy = (count\/len(predictions))*100\naccuracy","eaecfc33":"print(\"Accuracy using WordCloud is : \", accuracy , \"%\")","b7c634e2":"stop = set(stopwords.words('english'))\npunctuation = list(punctuation)\nstop.update(punctuation)","9fcfb3d2":"def get_simple_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","77f32102":"# Function to clean our text.\nlemmatizer = WordNetLemmatizer()\ndef clean_review(text):\n    clean_text = []\n    for w in word_tokenize(text):\n        if w.lower() not in stop:\n            pos = pos_tag([w])\n            new_w = lemmatizer.lemmatize(w, pos=get_simple_pos(pos[0][1]))\n            clean_text.append(new_w)\n    return clean_text\n\ndef join_text(text):\n    return \" \".join(text) ","330174a0":"df.text = df.text.apply(clean_review)\ndf.text = df.text.apply(join_text)","11781de7":"df.head()","1d85400f":"train_text, test_text, train_category, test_category = train_test_split(df.text, df.over_18, random_state=0)\ntrain_text.shape, test_text.shape, train_category.shape, test_category.shape","dca24fbb":"cv=CountVectorizer(min_df=0,max_df=1,binary=False,ngram_range=(1,2))\n#transformed train reviews\ncv_train_reviews=cv.fit_transform(train_text)\n#transformed test reviews\ncv_test_reviews=cv.transform(test_text)\n\nprint('cv_train:',cv_train_reviews.shape)\nprint('cv_test:',cv_test_reviews.shape)","735ea053":"tv=TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,2))\n#transformed train reviews\ntv_train_reviews=tv.fit_transform(train_text)\n#transformed test reviews\ntv_test_reviews=tv.transform(test_text)\nprint('Tfidf_train:',tv_train_reviews.shape)\nprint('Tfidf_test:',tv_test_reviews.shape)","e26edccd":"lr=LogisticRegression()\n\n# Fitting the model\nlr_cv=lr.fit(cv_train_reviews,train_category)\nlr_tfidf=lr.fit(tv_train_reviews,train_category)\n\n# Predicting for model\nlr_cv_predict=lr_cv.predict(cv_test_reviews)\nlr_tfidf_predict=lr_tfidf.predict(tv_test_reviews)","29f386ee":"# Getting Score\n\nlr_cv_score=accuracy_score(test_category,lr_cv_predict)\nprint(\"lr_cv_score :\",lr_cv_score)\n\nlr_tfidf_score=accuracy_score(test_category,lr_tfidf_predict)\nprint(\"lr_tfidf_score :\",lr_tfidf_score)","70c8ab5a":"print(classification_report(test_category,lr_cv_predict))\nprint(classification_report(test_category,lr_tfidf_predict))","9778c252":"plot_confusion_matrix(lr_cv, cv_test_reviews, test_category, cmap=\"Accent\", values_format = '')\nplot_confusion_matrix(lr_tfidf, tv_test_reviews, test_category, cmap=\"Accent\", values_format = '')","3f7e576a":"#training the model\nnb=MultinomialNB()\n\n# Fitting the model\nnb_cv=nb.fit(cv_train_reviews,train_category)\nnb_tfidf=nb.fit(tv_train_reviews,train_category)\n\n# Predicting for model\nnb_cv_predict=nb_cv.predict(cv_test_reviews)\nnb_tfidf_predict=nb_tfidf.predict(tv_test_reviews)","036095a7":"# Getting Score\n\nnb_cv_score=accuracy_score(test_category,nb_cv_predict)\nprint(\"nb_cv_score :\",nb_cv_score)\n\nnb_tfidf_score=accuracy_score(test_category,nb_tfidf_predict)\nprint(\"nb_tfidf_score :\",nb_tfidf_score)","d487b3b8":"print(classification_report(test_category,nb_cv_predict))\nprint(classification_report(test_category,nb_tfidf_predict))","10141de8":"plot_confusion_matrix(nb_cv, cv_test_reviews, test_category, cmap=\"Blues\", values_format = '')\nplot_confusion_matrix(nb_tfidf, tv_test_reviews, test_category, cmap=\"Blues\", values_format = '')","50ab18b8":"#svc=SVC()\n# Fitting the model\n#svc_cv=svc.fit(cv_train_reviews,train_category)\n#svc_tfidf=svc.fit(tv_train_reviews,train_category)\n\n# Predicting for model\n#svc_cv_predict=nb_cv.predict(cv_test_reviews)\n#svc_tfidf_predict=nb_tfidf.predict(tv_test_reviews)","174c2a35":"# Getting Score\n\n#svc_cv_score=accuracy_score(test_category,svc_cv_predict)\n#print(\"svc_cv_score :\", svc_cv_score)\n\n#svc_tfidf_score=accuracy_score(test_category,svc_tfidf_predict)\n#print(\"svc_tfidf_score :\", svc_tfidf_score)","973d0c39":"#print(classification_report(test_category,svc_cv_predict))\n#print(classification_report(test_category,svc_tfidf_predict))","0c5dd914":"#plot_confusion_matrix(svc_cv, cv_test_reviews, test_category, cmap=\"Blues\", values_format = '')\n#plot_confusion_matrix(svc_tfidf, tv_test_reviews, test_category, cmap=\"Blues\", values_format = '')","13a652a9":"model = Sequential()\nmodel.add(Dense(units = 100, activation = 'relu' , input_dim = cv_train_reviews.shape[1]))\nmodel.add(Dense(units = 20, activation = 'relu'))\nmodel.add(Dense(units = 1, activation = 'sigmoid'))\nmodel.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\nmodel.summary()","0a5fac9d":"history = model.fit(cv_train_reviews,train_category , epochs=1, batch_size = 512, validation_data=(cv_test_reviews,test_category))","b02295d1":"predictions = model.predict_classes(cv_test_reviews)\npredictions[:5]","0adff63d":"print(classification_report(test_category, predictions))","c8feece3":"cm = confusion_matrix(test_category,predictions)\ncm = pd.DataFrame(cm , index=['0','1'] , columns=['0','1'])\nplt.figure(figsize = (10,10))\nsns.heatmap(cm,cmap= \"Blues\", linecolor='black', annot=True, fmt='')","8c98e185":"Now lets take a look at our dataframe","521ea4a3":"4. **Creating Model**","29cb9d69":"Svc is taking so much time to fit the data so i didnt run these cells","bc09e3f9":"**Using just WordCloud, we have got an 86 accuracy! Thats cool. Now we will compare this result by testing this dataset on different classifiers.**","2098ca0c":"**Splitting the data into training and testing data**","e417dec1":"# Loading the Dataset","208c2ab3":"**WHAT IS LEMMATIZATION AND STEMMING?**\n\n- For grammatical reasons, documents are going to use different forms of a word, such as organize, organizes, and organizing. Additionally, there are families of derivationally related words with similar meanings, such as democracy, democratic, and democratization. In many situations, it seems as if it would be useful for a search for one of these words to return documents that contain another word in the set.\n- The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.\n\n**You guyz can read more here -> https:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/stemming-and-lemmatization-1.html**","a90e9120":"# Splitting Data","a37e3bdf":"# TRAINING WITH DIFFERENT CLASSIFIERS AND ANALYSIS AFTER TESTING","705cc110":"# Don't forget to upvote! It's free.\n# Any kind of suggestions is appreciated, feel free to comment below :-)","541b841a":"# Importing Libraries","641a3c59":"**Now for each word in every test data point , we will just check that if any word of that test data point is present in our dictionary ans_true which contains the most frequent 3000 words of label 1. If the word is present , then we will simply predict 1, otherwise 0.**","98b92ae8":"2. **Multinomial NaiveBayes**","0d89c04e":"1. **Logistic Regression**","47f39213":"**WHAT ARE STOPWORDS?**\n\n**Stopwords are the English words which does not add much meaning to a sentence. They can safely be ignored without sacrificing the meaning of the sentence. For example, the words like the, he, have etc. Such words are already captured this in corpus named corpus. We first download it to our python environment.**","53fd2428":"# Data Visualization and Preprocessing","c1c64d8c":"3. **Support Vector Machine**"}}