{"cell_type":{"36ceb94a":"code","ea4d5e0d":"code","e7175082":"code","c1e2112d":"code","56c5822d":"code","ac1cd00d":"code","16b453a7":"code","4eb81545":"code","9d752417":"code","04851464":"code","51fa01d7":"code","4bd4fbda":"code","866c1905":"code","258df3ed":"code","64d0b9c0":"code","90d483d9":"code","61cb29e4":"code","6b80c53b":"code","725960ed":"code","9e5f9962":"code","b2642f15":"code","b1ab5193":"markdown","9598d17a":"markdown","ba6efef9":"markdown","2b09572e":"markdown","52613682":"markdown","fd5b72b0":"markdown","73865d1d":"markdown"},"source":{"36ceb94a":"import pandas as pd\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport numpy as np","ea4d5e0d":"churndata = pd.read_csv(\"..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")","e7175082":"churndata.head()\nchurndata.columns","c1e2112d":"#Converting the Yes- No volumn to a binary column\nchurndata.Churn = churndata.Churn.map(dict(Yes=1,No=0))","56c5822d":"gender_tab = pd.crosstab(churndata.gender,churndata.Churn,normalize=True)\nprint(gender_tab)\ngender_tab.plot(kind=\"bar\",stacked=True)\n\n#This graph proves that Gender has not much impact on churn, since an equivalent number of cusotmers churn for each category ","ac1cd00d":"senior_citizen_tab = pd.crosstab(churndata.SeniorCitizen,churndata.Churn,normalize=True)\nprint(senior_citizen_tab)\nsenior_citizen_tab.plot(kind=\"bar\",stacked=True)\n\n#This Graph shows that a higher properion of customers churn in case of senior citizens","16b453a7":"fig, axes = plt.subplots(nrows=2, ncols=3)\nfig.set_figheight(15)\nfig.set_figwidth(15)\n#Similarly we can make Graph for Other Categorical Variables as well\npartner_tab = pd.crosstab(churndata.Partner,churndata.Churn,normalize=True)\npartner_tab.plot(kind=\"bar\",stacked=True,layout=(2,3),ax=axes[0,0])\n\nphoneservice_tab = pd.crosstab(churndata.PhoneService,churndata.Churn,normalize=True)\nphoneservice_tab.plot(kind=\"bar\",stacked=True,layout=(2,3),ax=axes[0,1])\n\nmultiplelines_tab = pd.crosstab(churndata.MultipleLines,churndata.Churn,normalize=True)\nmultiplelines_tab.plot(kind=\"bar\",stacked=True,layout=(2,3),ax=axes[0,2])\n\ninternetservice_tab = pd.crosstab(churndata.InternetService,churndata.Churn,normalize=True)\ninternetservice_tab.plot(kind=\"bar\",stacked=True,layout=(2,3),ax=axes[1,0])\n\nOnlineSecurity_tab = pd.crosstab(churndata.OnlineSecurity,churndata.Churn,normalize=True)\nOnlineSecurity_tab.plot(kind=\"bar\",stacked=True,layout=(2,3),ax=axes[1,1])\n\nOnlineBackup_tab = pd.crosstab(churndata.OnlineBackup,churndata.Churn,normalize=True)\nOnlineBackup_tab.plot(kind=\"bar\",stacked=True,ax=axes[1,2])","4eb81545":"fig, axes = plt.subplots(nrows=2, ncols=3)\nfig.set_figheight(15)\nfig.set_figwidth(15)\n\nDeviceProtection_tab = pd.crosstab(churndata.DeviceProtection,churndata.Churn,normalize=True)\nDeviceProtection_tab.plot(kind=\"bar\",stacked=True,ax=axes[0,0])\n\nTechSupport_tab = pd.crosstab(churndata.TechSupport,churndata.Churn,normalize=True)\nTechSupport_tab.plot(kind=\"bar\",stacked=True,ax=axes[0,1])\n\nStreamingTV_tab = pd.crosstab(churndata.StreamingTV,churndata.Churn,normalize=True)\nStreamingTV_tab.plot(kind=\"bar\",stacked=True,ax=axes[0,2])\n\nStreamingMovies_tab = pd.crosstab(churndata.StreamingMovies,churndata.Churn,normalize=True)\nStreamingMovies_tab.plot(kind=\"bar\",stacked=True,ax=axes[1,0])\n\nContract_tab = pd.crosstab(churndata.Contract,churndata.Churn,normalize=True)\nContract_tab.plot(kind=\"bar\",stacked=True,ax=axes[1,1])\n\nPaperlessBilling_tab = pd.crosstab(churndata.PaperlessBilling,churndata.Churn,normalize=True)\nPaperlessBilling_tab.plot(kind=\"bar\",stacked=True,ax=axes[1,2])\n\nPM_tab = pd.crosstab(churndata.PaymentMethod,churndata.Churn,normalize=True)\nPM_tab.plot(kind=\"bar\",stacked=True)\n","9d752417":"#Since we want to retreive dummy variables from the \npd.factorize(churndata['SeniorCitizen'])\npd.factorize(churndata['Dependents'])\npd.factorize(churndata['PhoneService'])\npd.factorize(churndata['MultipleLines'])\npd.factorize(churndata['InternetService'])\npd.factorize(churndata['OnlineSecurity'])\npd.factorize(churndata['OnlineBackup'])\npd.factorize(churndata['DeviceProtection'])\npd.factorize(churndata['TechSupport'])\npd.factorize(churndata['StreamingTV'])\npd.factorize(churndata['StreamingMovies'])\npd.factorize(churndata['Contract'])\npd.factorize(churndata['PaperlessBilling'])\npd.factorize(churndata['PaymentMethod'])","04851464":"# Next we take all the categorrical variables and convert them dummy variables\ncat_vars = ['SeniorCitizen', 'Partner', 'Dependents',\n       'PhoneService', 'MultipleLines', 'InternetService',\n       'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n       'StreamingTV', 'StreamingMovies', 'Contract', 'PaperlessBilling']\n\nfor var in cat_vars:\n    cat_list='var'+'_'+var\n    cat_list = pd.get_dummies(churndata[var], prefix=var,drop_first=True)\n    data1=churndata.join(cat_list)\n    churndata=data1\n","51fa01d7":"churndata.columns","4bd4fbda":"training_features = ['SeniorCitizen_1', 'Partner_Yes', 'Dependents_Yes', 'PhoneService_Yes',\n       'MultipleLines_No phone service', 'MultipleLines_Yes',\n       'InternetService_Fiber optic', 'InternetService_No',\n       'OnlineSecurity_No internet service', 'OnlineSecurity_Yes',\n       'OnlineBackup_No internet service', 'OnlineBackup_Yes',\n       'DeviceProtection_No internet service', 'DeviceProtection_Yes',\n       'TechSupport_No internet service', 'TechSupport_Yes',\n       'StreamingTV_No internet service', 'StreamingTV_Yes',\n       'StreamingMovies_No internet service', 'StreamingMovies_Yes',\n       'Contract_One year', 'Contract_Two year', 'PaperlessBilling_Yes','MonthlyCharges','tenure']","866c1905":"X = churndata[training_features]\ny = churndata['Churn']","258df3ed":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)","64d0b9c0":"class LogisticRegression:\n    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, verbose=False):\n        self.lr = lr\n        self.num_iter = num_iter\n        self.fit_intercept = fit_intercept\n    \n    def __add_intercept(self, X):\n        intercept = np.ones((X.shape[0], 1))\n        return np.concatenate((intercept, X), axis=1)\n    \n    def __sigmoid(self, z):\n        return 1 \/ (1 + np.exp(-z))\n    def __loss(self, h, y):\n        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n    \n    def fit(self, X, y):\n        if self.fit_intercept:\n            X = self.__add_intercept(X)\n        \n        # weights initialization\n        self.theta = np.zeros(X.shape[1])\n        \n        for i in range(self.num_iter):\n            z = np.dot(X, self.theta)\n            h = self.__sigmoid(z)\n            gradient = np.dot(X.T, (h - y)) \/ y.size\n            self.theta -= self.lr * gradient\n            \n        \n    def predict_prob(self, X):\n        if self.fit_intercept:\n            X = self.__add_intercept(X)\n    \n        return self.__sigmoid(np.dot(X, self.theta))\n    \n    def predict(self, X, threshold):\n        return self.predict_prob(X) >= threshold","90d483d9":"model = LogisticRegression(lr=0.001, num_iter=10000)\n%time model.fit(X_train,y_train)\n\nmodel.theta","61cb29e4":"#We test the model accuracy with a threshold of 0.5\npreds = model.predict(X_test,.5)\n# accuracy\n(preds == y_test).mean()\n\n#Thus we can see that the Logistic regression model has an accuracy of about 79.4%","6b80c53b":"class RegularizedLogisticRegression:\n    def __init__(self, lr=0.01, num_iter=100000, fit_intercept=True, verbose=False,reg=10):\n        self.lr = lr\n        self.num_iter = num_iter\n        self.fit_intercept = fit_intercept\n        self.reg = reg\n    \n    def __add_intercept(self, X):\n        intercept = np.ones((X.shape[0], 1))\n        return np.concatenate((intercept, X), axis=1)\n    \n    def __sigmoid(self, z):\n        return 1 \/ (1 + np.exp(-z))\n    def __loss(self, h, y):\n        return (-y * np.log(h) - (1 - y) * np.log(1 - h)).mean()\n    \n    def fit(self, X, y):\n        if self.fit_intercept:\n            X = self.__add_intercept(X)\n        \n        # weights initialization\n        self.theta = np.zeros(X.shape[1])\n        \n        for i in range(self.num_iter):\n            z = np.dot(X, self.theta)\n            h = self.__sigmoid(z)\n            gradient = np.dot(X.T, (h - y)) \/ y.size\n            self.theta -= self.lr * (gradient + self.reg*self.theta\/y.size)\n            \n        \n    def predict_prob(self, X):\n        if self.fit_intercept:\n            X = self.__add_intercept(X)\n    \n        return self.__sigmoid(np.dot(X, self.theta))\n    \n    def predict(self, X, threshold):\n        return self.predict_prob(X) >= threshold","725960ed":"model = RegularizedLogisticRegression(lr=0.001, num_iter=10000,reg=100)\n%time model.fit(X_train,y_train)\n\nmodel.theta","9e5f9962":"preds = model.predict(X_test,.5)\n# accuracy\n(preds == y_test).mean()\n\n#Thus we can see that the regularized Logistic regression model has an accuracy of about 79.2%","b2642f15":"from sklearn import model_selection\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nkfold = model_selection.KFold(n_splits=10, random_state=7)\nmodelCV = LogisticRegression()\nscoring = 'accuracy'\nresults = model_selection.cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring=scoring)\nprint(\"10-fold cross validation average accuracy: %.3f\" % (results.mean()))\n","b1ab5193":"<b> Dataset <\/b>\nWe have the customer data for a telecom company which offers many services like phone, internet, TV Streaming and Movie Streaming. The Goal is to predict whether or not a particular customer is likely to retain services. This is represented by the Churn column in dataset. Churn=Yes means customer leaves the company, whereas Churn=No implies customer is retained by the company.\n\n","9598d17a":"# Logistic Regression from scratch\n\nSource: This excellent Medium article <href> https:\/\/medium.com\/@martinpella\/logistic-regression-from-scratch-in-python-124c5636b8ac <\/href>\n\n<b>Algorithm<\/b>\nGiven a input X, we calculate the probability of customer churning. Then an appropriate threshold is decided, if the probability is higher than this threshold we consider that the customer will churn, else will not churn\n\n<b> Hypothesis function <\/b> A sigmoid hypothesis function is chosen\n<img src='https:\/\/cdn-images-1.medium.com\/max\/800\/1*HXCBO-Wx5XhuY_OwMl0Phw.png'>\n<b> Loss Function <\/b> To measure the performance of algorithm at each step, we define the following loss function.\n<img src='https:\/\/cdn-images-1.medium.com\/max\/800\/1*FdxEs8Iv_43Q8calTCjnow.png'>\n<b> Gradient Descent <\/b> At each iteration of the algorithm we take the gradient of the loss function and based on that update the weights\n<img src='https:\/\/cdn-images-1.medium.com\/max\/800\/1*gobKgGbRWDAoVFAan_HjxQ.png'>","ba6efef9":"# Comparing results with the Scikit Learn Logistic Regression Libraries\n\nAs a final part of this notebook, I try to use the k-fold cross validated Logistic Regression from scikit-learn library and compare the results with the scratch implementation.","2b09572e":"Since its a convention to do some exploratory data analysis before modeling, lets do some graph plotting","52613682":"# Regularized Logistic Regression from Scratch\n\nIn this case we add a regularization term to the loss function such that the new loss function is defined as follows\n![image.png](attachment:image.png)","fd5b72b0":"Based on the information we can say that gender is not a significant variable for churn and is also correalted with others, so we can drop it.","73865d1d":"In this Notebook I have implemented Scratch Implementations of Logistic Regression using Gradient Descent Algorithm and also Regularized Logistic Regression. The main motive for including scratch implementations but not scikit libraries were\n\n<ul>\n    <li> Understand how gradient descent minimizes the cost function to give optimal solution <\/li>\n    <li> Visualise how change in learning rate affects the training time <\/li>\n    <li> Get a better intuition of bias-variance tradeoff by changing the regularization parameter and the cutoff threshold <\/li>\n    <\/ul>\n\n\n"}}