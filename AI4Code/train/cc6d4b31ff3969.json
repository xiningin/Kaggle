{"cell_type":{"ab024d2f":"code","d39b6037":"code","8a24ce1c":"code","fbab772f":"code","853a8213":"code","e659827e":"code","5381eb4d":"code","ec214edd":"code","cf64c7d5":"code","9cf7be24":"code","e1989d42":"code","f5d5c1e3":"code","a2b07b56":"code","650d9402":"code","80ab4ea3":"code","1970a162":"code","c98b3439":"code","70204084":"code","66d510e5":"code","1c110306":"code","245ae0da":"code","cdd56286":"code","b047c70f":"code","6029b30b":"code","f0e051f2":"markdown","4996024c":"markdown","d90d7f68":"markdown","d95b2175":"markdown","2aabece0":"markdown","b1cc85cc":"markdown","6ada5d8a":"markdown","67b8f154":"markdown","3cf6f7c8":"markdown","81d2dd8f":"markdown","29884bad":"markdown","4d6dadd3":"markdown","34aea317":"markdown","8a191eb6":"markdown","98bc4e6e":"markdown","19e67753":"markdown","c08230c9":"markdown","8934da18":"markdown","852eb634":"markdown","fd102e0c":"markdown","3370b82a":"markdown","3cd1a236":"markdown","258f49de":"markdown","ce7148a5":"markdown","b55a83ee":"markdown","b28ce6ca":"markdown","b896fb83":"markdown","c65b609f":"markdown","aff14905":"markdown","7025bc24":"markdown","822079ae":"markdown","cf497a73":"markdown","f45f7a60":"markdown","c45da10c":"markdown","58332959":"markdown","382acd6b":"markdown","987cb518":"markdown","3407a5ff":"markdown"},"source":{"ab024d2f":"!pip install nb_black\n%load_ext lab_black\n\nimport numpy as np  # linear algebra\nimport pandas as pd\n\nfrom pathlib import Path\n\nbase_dir = Path(\"..\/input\/dogus-datathon-otomotiv\")","d39b6037":"submission = pd.read_csv(\"..\/input\/dogus-datathon-otomotiv\/sample_submission.csv\")\n\ncustomer = pd.read_csv(\n    \"..\/input\/dogus-datathon-otomotiv\/FINAL_CUSTOMER_DATATHON.csv\",\n    index_col=0,\n).drop(columns=[\"GENDER_ID\", \"MARITAL_STATUS_ID\"])\n\ncustomer[\"FK_ADDRESS_COMMUNICATION_CITY\"] = customer[\n    \"FK_ADDRESS_COMMUNICATION_CITY\"\n].str.lower()\n\ncustomer = customer.replace(\"\", np.nan)\n\nsales = pd.read_csv(\n    \"..\/input\/dogus-datathon-otomotiv\/FINAL_SALES_FILE_DATATHON.csv\",\n    index_col=0,\n    parse_dates=True,\n)\n\nvehicle = pd.read_csv(\n    \"..\/input\/dogus-datathon-otomotiv\/FINAL_VEHICLE_TABLE_DATATHON.csv\", index_col=0\n)\n\ncustomer = customer[customer[\"BASE_CUSTOMER_ID\"].isin(submission[\"Id\"])]","8a24ce1c":"cust_vehicle_interactions = (\n    pd.read_csv(\n        \"..\/input\/dogus-datathon-otomotiv\/FINAL_CUSTOMER_RELATED_TABLE_FOR_DATATHON.csv\",\n        index_col=0,\n        parse_dates=True,\n    )\n    .drop(columns=[\"FK_RELATION_STATUS_ID\"])\n    .merge(vehicle, on=[\"VEHICLE_ID\"])\n)\n\nfirst_hand_sales = pd.read_csv(\n    \"..\/input\/dogus-datathon-otomotiv\/FINAL_SIFIR_ARAC_ALANLAR_DATATHON.csv\",\n    index_col=0,\n)\ncust_vehicle_interactions = cust_vehicle_interactions.merge(\n    first_hand_sales.loc[\n        :,\n        ~first_hand_sales.columns.isin([\"CREATE_DATE\", \"year\", \"month\", \"week\", \"day\"]),\n    ].assign(is_first_hand=True),\n    on=[\"VEHICLE_ID\"],\n    how=\"left\",\n).assign(is_first_hand=lambda x: x[\"is_first_hand\"].fillna(False))\n\ndel first_hand_sales, vehicle\n\ncust_vehicle_interactions = cust_vehicle_interactions[\n    cust_vehicle_interactions[\"BASE_CUSTOMER_ID\"].isin(submission[\"Id\"])\n]","fbab772f":"cust_vehicle_interactions[\"START_DATE\"] = cust_vehicle_interactions[\n    \"START_DATE\"\n].astype(\"datetime64[ns]\")\n\ncust_vehicle_interactions = cust_vehicle_interactions.sort_values(\n    [\"START_DATE\", \"BASE_CUSTOMER_ID\", \"VEHICLE_ID\"], ignore_index=True\n)\n\nsales = sales[sales[\"CUSTOMER_ID\"].isin(customer[\"CUSTOMER_ID\"])]\n\nsales[\"SF_CREATE_DATE\"] = (\n    sales[\"SF_CREATE_DATE\"].astype(\"datetime64[ns]\").dt.date.astype(\"datetime64[ns]\")\n)\n\nsales = sales.sort_values(\"SF_CREATE_DATE\", ignore_index=True)\n\nsales = sales.merge(\n    customer.loc[:, [\"CUSTOMER_ID\", \"BASE_CUSTOMER_ID\"]], on=[\"CUSTOMER_ID\"]\n)","853a8213":"def preprocess_maintenance(maintenance):\n    maintenance[\"GEAR_BOX_TYPE\"] = (\n        maintenance[\"GEAR_BOX_TYPE\"]\n        .str.lower()\n        .str.replace(\"\\(dsg\\)\", \"\")\n        .str.replace(\"(dsg \u015fanzuman)\", \"\")\n        .str.replace(\" de\u011fi\u015fken\", \"\")\n        .str.replace(\"\\?automatisch\", \"otomatik\")\n        .str.replace(\"\\?handschaltung\", \"manuel\")\n        .str.replace(\"\\(\\)\", \"\")\n        .str.strip()\n        .str.replace(\"otomaik\", \"otomatik\")\n        .str.replace(\"d\u00fcz\", \"manuel\")\n        .str.replace(\"s\u00fcrekli\", \"otomatik\")\n        .str.replace(\"mekanik\", \"manuel\")\n        .value_counts()\n    )\n\n    maintenance[\"MOTOR_GAS_TYPE\"] = (\n        maintenance[\"MOTOR_GAS_TYPE\"]\n        .str.split(\"\/\")\n        .str.get(-1)\n        .str.replace(\"\\?\", \"\")\n        .str.lower()\n        .str.strip()\n        .str.replace(\"diesel\", \"dizel\")\n        .str.replace(\"kur\u015funsuz benzin\", \"benzin\")\n        .str.replace(\"hybrid\", \"hibrit\")\n    )\n\n\ndef fe_maintenance(maintenance):\n\n    vehicle_meta = maintenance.loc[\n        :,\n        [\n            \"BRAND_CODE\",\n            \"BASEMODEL_CODE\",\n            \"TOPMODEL_CODE\",\n            \"MOTOR_GAS_TYPE\",\n            \"GEAR_BOX_TYPE\",\n        ],\n    ].drop_duplicates()\n\n    for group in [\n        \"BRAND_CODE\",\n        \"BASEMODEL_CODE\",\n        \"TOPMODEL_CODE\",\n        \"MOTOR_GAS_TYPE\",\n        \"GEAR_BOX_TYPE\",\n    ]:\n        vehicle_meta = vehicle_meta.merge(\n            maintenance.query(\"IS_MAINTENANCE == 1\")\n            .groupby([group, \"VEHICLE_ID\"])[\"TOTAL_AMOUNT_TL\"]\n            .mean()\n            .groupby(level=[group])\n            .quantile(0.5)\n            .dropna()\n            .to_frame(\"maintenance_expense__\" + group),\n            on=group,\n            how=\"left\",\n        )\n\n    return vehicle_meta","e659827e":"def get_cross_validation_folds(start_date=\"2019-12-01\", end_date=\"2020-03-01\"):\n\n    folds = pd.DataFrame(\n        {\n            \"start_date\": pd.date_range(start_date, periods=7, freq=\"3MS\"),\n            \"end_date\": pd.date_range(end_date, periods=7, freq=\"3MS\"),\n        }\n    ).assign(\n        train_cond=lambda df: df.apply(\n            lambda x: \"{{col}} < '{}'\".format(  # START_DATE >= '{}' and\n                x[\"end_date\"].date()  # x[\"start_date\"].date(),\n            ),\n            axis=1,\n        )\n    )\n\n    folds[\"type\"] = np.where(folds[\"start_date\"] == \"2021-06-01\", \"test\", \"train\")\n\n    folds = folds.join(\n        folds[::-1]\n        .shift(1)\n        .dropna()\n        .assign(\n            target_cond=lambda df: df.apply(\n                lambda x: \"{{col}} >= '{}' and {{col}} < '{}'\".format(  # START_DATE >= '{}' and\n                    x[\"start_date\"].date(),\n                    x[\"end_date\"].date(),  # x[\"start_date\"].date(),\n                ),\n                axis=1,\n            )\n        )\n        .drop(columns=[\"train_cond\", \"start_date\", \"end_date\", \"type\"])\n        .sort_index()\n    ).dropna()\n\n    return folds","5381eb4d":"def preprocess_data_by_fold_idx(folds, fold_idx):\n    fold_train_idx = cust_vehicle_interactions.query(\n        folds.loc[fold_idx, \"train_cond\"].format(col=\"START_DATE\")\n    ).copy()\n\n    sales_train_idx = sales.query(\n        folds.loc[fold_idx, \"train_cond\"].format(col=\"SF_CREATE_DATE\")\n    ).copy()\n\n    fold_train_idx[\"END_DATE\"] = fold_train_idx[\"END_DATE\"].astype(\"datetime64[ns]\")\n    fold_train_idx.loc[\n        fold_train_idx[\"END_DATE\"] > folds.loc[fold_idx, \"end_date\"],\n        \"FK_RELATION_STATUS_EXPLANATION\",\n    ] = \"Aktif Ruhsat Sahibi\"\n    fold_train_idx.loc[\n        fold_train_idx[\"END_DATE\"] >= folds.loc[fold_idx, \"end_date\"], \"END_DATE\"\n    ] = np.nan\n\n    target = (\n        sales.query(folds.loc[fold_idx, \"target_cond\"].format(col=\"SF_CREATE_DATE\"))\n        .assign(target=1)\n        .loc[:, [\"BASE_CUSTOMER_ID\", \"target\"]]\n        .drop_duplicates()\n    )\n\n    target = (\n        fold_train_idx.loc[:, [\"BASE_CUSTOMER_ID\"]]\n        .merge(target, on=[\"BASE_CUSTOMER_ID\"], how=\"left\")\n        .assign(target=lambda df: df[\"target\"].fillna(0))\n        .drop_duplicates()\n    )\n\n    maintenance = pd.read_csv(\n        \"..\/input\/dogus-datathon-otomotiv\/MASK_SERVIS_BAKIM_DATATHON_FINAL.csv\",\n        index_col=0,\n    )\n\n    maintenance = maintenance.merge(\n        cust_vehicle_interactions, on=[\"VEHICLE_ID\"], how=\"left\"\n    )\n\n    preprocess_maintenance(maintenance)\n    fe_m = fe_maintenance(maintenance)\n\n    return (\n        fold_train_idx,\n        sales_train_idx,\n        maintenance.merge(\n            fe_m,\n            on=fe_m.columns[~fe_m.columns.str.contains(\"expense\")].tolist(),\n        ),\n        target,\n    )","ec214edd":"def fe_cust_vehicle_data(df, end_date=\"2021-03-01\"):\n\n    df = pd.concat(\n        [\n            df,\n            df.loc[\n                :,\n                [\n                    \"BASE_CUSTOMER_ID\",\n                    \"VEHICLE_ID\",\n                    \"START_DATE\",\n                    \"END_DATE\",\n                    \"FK_RELATION_STATUS_EXPLANATION\",\n                    \"TRAFFIC_DATE\",\n                    \"BRAND_CODE\",\n                    \"is_first_hand\",\n                ],\n            ]\n            .rename(columns=lambda x: x + \"_SHIFT1\" if x != \"BASE_CUSTOMER_ID\" else x)\n            .groupby([\"BASE_CUSTOMER_ID\"])\n            .shift(1),\n        ],\n        axis=1,\n    )\n\n    df[\"START_DATE_GROUP\"] = pd.cut(\n        (df[\"START_DATE\"].max() - df[\"START_DATE\"]).dt.days,\n        bins=[0, 30, 90, 365, np.inf],\n        include_lowest=True,\n    )\n\n    df[\"TRAFFIC_DATE\"] = df[\"TRAFFIC_DATE\"].astype(\"datetime64[ns]\")\n\n    df[\"time_spent_day_after__last_purchase\"] = (\n        df[\"START_DATE\"] - df[\"START_DATE_SHIFT1\"]\n    ).dt.days\n\n    df[\"vehichle_age_after__last_purchase\"] = (\n        df[\"TRAFFIC_DATE\"].astype(\"datetime64[ns]\")\n        - df[\"TRAFFIC_DATE_SHIFT1\"].astype(\"datetime64[ns]\")\n    ).dt.days\n\n    df[\"vehicle_age\"] = pd.cut(\n        (pd.to_datetime(end_date) - df[\"TRAFFIC_DATE\"].astype(\"datetime64[ns]\")).dt.days\n        \/\/ 365,\n        bins=[0, 1, 3, 7, 15, np.inf],\n        include_lowest=True,\n    )\n\n    return (\n        df.groupby([\"BASE_CUSTOMER_ID\"])\n        .aggregate(\n            **{\n                \"min__TRAFFIC_DATE\": (\"TRAFFIC_DATE\", \"min\"),\n                \"max__time_spent_day_after__last_purchase\": (\n                    \"time_spent_day_after__last_purchase\",\n                    \"max\",\n                ),\n                \"min__time_spent_day_after__last_purchase\": (\n                    \"time_spent_day_after__last_purchase\",\n                    \"min\",\n                ),\n                \"mean__time_spent_day_after__last_purchase\": (\n                    \"time_spent_day_after__last_purchase\",\n                    \"mean\",\n                ),\n                \"max__vehichle_age_after__last_purchase\": (\n                    \"vehichle_age_after__last_purchase\",\n                    \"max\",\n                ),\n                \"min__vehichle_age_after__last_purchase\": (\n                    \"vehichle_age_after__last_purchase\",\n                    \"min\",\n                ),\n                \"mean__vehichle_age_after__last_purchase\": (\n                    \"vehichle_age_after__last_purchase\",\n                    \"mean\",\n                ),\n                \"nunique__brand_code\": (\"BRAND_CODE\", \"nunique\"),\n                \"std_maintenance_expense__BRAND_CODE\": (\n                    \"maintenance_expense__BRAND_CODE\",\n                    \"std\",\n                ),\n                \"max_maintenance_expense__BRAND_CODE\": (\n                    \"maintenance_expense__BRAND_CODE\",\n                    \"max\",\n                ),\n                \"mean_maintenance_expense__BRAND_CODE\": (\n                    \"maintenance_expense__BRAND_CODE\",\n                    \"min\",\n                ),\n                \"max_maintenance_expense__BASEMODEL_CODE\": (\n                    \"maintenance_expense__BASEMODEL_CODE\",\n                    \"max\",\n                ),\n                \"mean_maintenance_expense__BASEMODEL_CODE\": (\n                    \"maintenance_expense__BASEMODEL_CODE\",\n                    \"min\",\n                ),\n                \"max_maintenance_expense__TOPMODEL_CODE\": (\n                    \"maintenance_expense__TOPMODEL_CODE\",\n                    \"max\",\n                ),\n                \"mean_maintenance_expense__TOPMODEL_CODE\": (\n                    \"maintenance_expense__TOPMODEL_CODE\",\n                    \"min\",\n                ),\n                \"max_maintenance_expense__MOTOR_GAS_TYPE\": (\n                    \"maintenance_expense__MOTOR_GAS_TYPE\",\n                    \"max\",\n                ),\n            }\n        )\n        .join(\n            df.groupby([\"BASE_CUSTOMER_ID\", \"FK_RELATION_STATUS_EXPLANATION\"])\n            .size()\n            .unstack(1)\n            .rename(columns=lambda x: \"count__\" + x.replace(\" \", \"_\").lower())\n        )\n        .join(\n            df.groupby([\"BASE_CUSTOMER_ID\", \"vehicle_age\"])[\"VEHICLE_ID\"]\n            .nunique()\n            .unstack(1)\n            .rename(\n                columns=lambda x: \"nunique__{}_{}_vehicle_id\".format(\n                    int(x.left), x.right\n                ).replace(\".0\", \"\")\n            )\n        )\n        .join(\n            df.groupby([\"BASE_CUSTOMER_ID\", \"is_first_hand\"])\n            .size()\n            .unstack(1)\n            .rename(columns=lambda x: \"count__first_hand_\" + str(x))\n        )\n        .join(\n            df.groupby([\"BASE_CUSTOMER_ID\", \"BRAND_CODE\"])\n            .size()\n            .unstack(1)\n            .rename(columns=lambda x: \"count__brand_code_\" + str(x))\n        )\n        .join(\n            df[df[\"is_first_hand\"]]\n            .groupby([\"BASE_CUSTOMER_ID\", \"BRAND_CODE\"])\n            .size()\n            .unstack(1)\n            .rename(columns=lambda x: \"count__first_hand_brand_code_\" + str(x))\n        )\n    )\n\n\ndef fe_sales_data(df, folds, fold_idx, end_date=\"2021-03-01\"):\n\n    df[\"SF_CREATE_DATE_GROUP\"] = pd.cut(\n        (df[\"SF_CREATE_DATE\"].max() - df[\"SF_CREATE_DATE\"]).dt.days,\n        bins=[0, 30, 90, 365, np.inf],\n        include_lowest=True,\n    )\n\n    tmp = (\n        df[\"SF_CREATE_DATE_GROUP\"]\n        .dropna()\n        .apply(lambda x: (str(x.left) + \"_\" + str(x.right)).replace(\".0\", \"\"))\n    )\n\n    df.loc[tmp.index, \"SF_CREATE_DATE_GROUP_T\"] = tmp.values\n\n    df.loc[tmp.index, \"SF_CREATE_STATUS\"] = (\n        df.loc[tmp.index, \"SF_CREATE_DATE_GROUP_T\"].astype(str)\n        + \"_\"\n        + df.loc[tmp.index, \"STATUS\"].astype(str)\n    )\n\n    last12 = (\n        datetime.datetime(*(map(int, end_date.split(\"-\"))))\n        - datetime.timedelta(days=365)\n    ).strftime(\"%Y-%m-%d\")\n\n    last6 = (\n        datetime.datetime(*(map(int, end_date.split(\"-\"))))\n        - datetime.timedelta(days=180)\n    ).strftime(\"%Y-%m-%d\")\n\n    last3 = (\n        datetime.datetime(*(map(int, end_date.split(\"-\"))))\n        - datetime.timedelta(days=90)\n    ).strftime(\"%Y-%m-%d\")\n\n    dt = pd.to_datetime(end_date)\n\n    skew_feats = (\n        df.query(f\"SF_CREATE_DATE < '{end_date}'\")\n        .assign(last_date_diff=lambda df: (dt - df[\"SF_CREATE_DATE\"]).dt.days)\n        .groupby([\"BASE_CUSTOMER_ID\"])[\"last_date_diff\"]\n        .skew()  # partial(nanskew, mean=all_mean))\n        .to_frame(\"all_v2_skew\")\n        .join(\n            df.query(f\"SF_CREATE_DATE >= '{last12}' and SF_CREATE_DATE < '{end_date}'\")\n            .assign(last_date_diff=lambda df: (dt - df[\"SF_CREATE_DATE\"]).dt.days)\n            .groupby([\"BASE_CUSTOMER_ID\"])[\"last_date_diff\"]\n            .skew()  # partial(nanskew, mean=all_mean))\n            .to_frame(\"last12_v2_skew\"),\n            how=\"outer\",\n        )\n        .join(\n            df.query(f\"SF_CREATE_DATE >= '{last6}' and SF_CREATE_DATE < '{end_date}'\")\n            .assign(last_date_diff=lambda df: (dt - df[\"SF_CREATE_DATE\"]).dt.days)\n            .groupby([\"BASE_CUSTOMER_ID\"])[\"last_date_diff\"]\n            .skew()  # partial(nanskew, mean=all_mean))\n            .to_frame(\"last6_v2_skew\"),\n            how=\"outer\",\n        )\n        .join(\n            df.query(f\"SF_CREATE_DATE >= '{last3}' and SF_CREATE_DATE < '{end_date}'\")\n            .assign(last_date_diff=lambda df: (dt - df[\"SF_CREATE_DATE\"]).dt.days)\n            .groupby([\"BASE_CUSTOMER_ID\"])[\"last_date_diff\"]\n            .skew()  # partial(nanskew, mean=all_mean))\n            .to_frame(\"last3_v2_skew\"),\n            how=\"outer\",\n        )\n    )\n\n    return (\n        df.groupby([\"BASE_CUSTOMER_ID\", \"SF_CREATE_DATE_GROUP\"])[\"SALESFILE_ID\"]\n        .nunique()\n        .unstack(1)\n        .rename(\n            columns=lambda x: \"nunique__{}_{}_salesfile_id\".format(\n                int(x.left), x.right\n            ).replace(\".0\", \"\"),\n        )\n        .join(\n            df.groupby([\"BASE_CUSTOMER_ID\"])[\n                [\"CUSTOMER_ID\", \"REQ_BRAND_CODE\", \"STATUS\"]\n            ]\n            .nunique()\n            .rename(columns=lambda x: \"nunique__\" + str(x)),\n            how=\"left\",\n        )\n        .join(\n            df.groupby([\"BASE_CUSTOMER_ID\", \"SF_CREATE_STATUS\"])[\"SALESFILE_ID\"]\n            .nunique()\n            .unstack(1)\n            .rename(columns=lambda x: \"nunique__sf_create_status_\" + str(x)),\n            how=\"left\",\n        )\n        .join(\n            df.groupby([\"BASE_CUSTOMER_ID\", \"STATUS\"])[\"SALESFILE_ID\"]\n            .nunique()\n            .unstack(1)\n            .rename(columns=lambda x: \"nunique__status_\" + str(x)),\n            how=\"left\",\n        )\n        .join(\n            (\n                df.query(\n                    folds.loc[fold_idx, \"target_cond\"]\n                    .replace(\"2014\", \"2013\")\n                    .replace(\"2015\", \"2014\")\n                    .replace(\"2016\", \"2015\")\n                    .replace(\"2017\", \"2016\")\n                    .replace(\"2018\", \"2017\")\n                    .replace(\"2019\", \"2018\")\n                    .replace(\"2020\", \"2019\")\n                    .replace(\"2021\", \"2020\")\n                    .format(col=\"SF_CREATE_DATE\")\n                )\n                .groupby([\"BASE_CUSTOMER_ID\"])[\"SALESFILE_ID\"]\n                .nunique()\n                .to_frame(\"is_last_year_sale_available\")\n            )\n        )\n        .join(skew_feats, how=\"left\")\n    )\n\n\ndef fe_maintenance_data(df, end_date=\"2021-03-01\"):\n    df[\"total_year_after_traffic_date\"] = (\n        (pd.to_datetime(end_date) - df[\"TRAFFIC_DATE\"].astype(\"datetime64[ns]\")).dt.days\n        \/\/ 365\n    ) + 1\n\n    sample = df.copy()\n\n    sample[\"year\"] = sample[\"CREATE_DATE\"].astype(\"datetime64[ns]\").dt.year\n\n    sample[\"is_last_year\"] = (\n        (\n            pd.to_datetime(end_date) - sample[\"CREATE_DATE\"].astype(\"datetime64[ns]\")\n        ).dt.days\n        \/\/ 365\n    ) < 1\n\n    sample[\"is_last_three_year\"] = (\n        (\n            pd.to_datetime(end_date) - sample[\"CREATE_DATE\"].astype(\"datetime64[ns]\")\n        ).dt.days\n        \/\/ 365\n    ) < 3\n\n    return (\n        sample.loc[\n            :,\n            [\n                \"BASE_CUSTOMER_ID\",\n                \"VEHICLE_ID\",\n                \"is_last_year\",\n                \"is_last_three_year\",\n                \"IS_MAINTENANCE\",\n                \"TOTAL_AMOUNT_TL\",\n                \"year\",\n            ],\n        ]\n        .assign(\n            vehicle_cnt=lambda x: x.groupby([\"BASE_CUSTOMER_ID\"])[\n                \"VEHICLE_ID\"\n            ].transform(\"nunique\"),\n            last_three_year_vehicle_cnt=lambda x: x[x[\"is_last_three_year\"]]\n            .groupby([\"BASE_CUSTOMER_ID\"])[\"VEHICLE_ID\"]\n            .transform(\"nunique\"),\n            last_three_year_vehicle_maintenance_cnt=lambda x: x[\n                \"last_three_year_vehicle_cnt\"\n            ]\n            \/ x[\"vehicle_cnt\"],\n            last_three_year_diff_row_year=lambda x: x[x[\"is_last_three_year\"]]\n            .groupby([\"VEHICLE_ID\"])[\"year\"]\n            .transform(\"nunique\"),\n            vehicle_maintenance_ratio=lambda x: x[x[\"is_last_three_year\"]]\n            .groupby([\"BASE_CUSTOMER_ID\"])[\"VEHICLE_ID\"]\n            .transform(\"nunique\")\n            \/ x[\"vehicle_cnt\"],\n            last_three_year_diff_damage_year=lambda x: x[x[\"is_last_three_year\"]]\n            .query(\"IS_MAINTENANCE == 0\")\n            .groupby([\"VEHICLE_ID\"])[\"year\"]\n            .transform(\"nunique\"),\n            last_three_year_diff_maintenance_year=lambda x: x[x[\"is_last_three_year\"]]\n            .query(\"IS_MAINTENANCE == 1\")\n            .groupby([\"VEHICLE_ID\"])[\"year\"]\n            .transform(\"nunique\"),\n            last_three_year_damage_maintenance=lambda x: x[x[\"is_last_three_year\"]]\n            .query(\"IS_MAINTENANCE == 0\")\n            .groupby([\"VEHICLE_ID\"])[\"TOTAL_AMOUNT_TL\"]\n            .transform(\"cumsum\"),\n            last_three_year_sum_maintenance=lambda x: x[x[\"is_last_three_year\"]]\n            .query(\"IS_MAINTENANCE == 1\")\n            .groupby([\"VEHICLE_ID\"])[\"TOTAL_AMOUNT_TL\"]\n            .transform(\"cumsum\"),\n            last_year_sum_maintenance=lambda x: x[x[\"is_last_year\"]]\n            .query(\"IS_MAINTENANCE == 1\")\n            .groupby([\"VEHICLE_ID\"])[\"TOTAL_AMOUNT_TL\"]\n            .transform(\"cumsum\"),\n            last_year_damage_maintenance=lambda x: x[x[\"is_last_year\"]]\n            .query(\"IS_MAINTENANCE == 0\")\n            .groupby([\"VEHICLE_ID\"])[\"TOTAL_AMOUNT_TL\"]\n            .transform(\"cumsum\"),\n        )\n        .fillna(method=\"pad\")\n        .sort_values(\"year\")\n        .drop_duplicates([\"VEHICLE_ID\"], keep=\"last\")\n        .groupby(\"BASE_CUSTOMER_ID\")\n        .aggregate(\n            **{\n                \"mean__last_three_year_vehicle_cnt\": (\n                    \"last_three_year_vehicle_cnt\",\n                    \"mean\",\n                ),\n                \"mean__vehicle_maintenance_ratio\": (\n                    \"vehicle_maintenance_ratio\",\n                    \"mean\",\n                ),\n                \"mean__last_three_year_diff_row_year\": (\n                    \"last_three_year_diff_row_year\",\n                    \"mean\",\n                ),\n                \"max__last_three_year_diff_row_year\": (\n                    \"last_three_year_diff_row_year\",\n                    \"max\",\n                ),\n                \"mean__last_three_year_diff_damage_year\": (\n                    \"last_three_year_diff_damage_year\",\n                    \"mean\",\n                ),\n                \"max__last_three_year_diff_damage_year\": (\n                    \"last_three_year_diff_damage_year\",\n                    \"max\",\n                ),\n                \"mean__last_last_three_year_diff_maintenance_yearr\": (\n                    \"last_three_year_diff_maintenance_year\",\n                    \"mean\",\n                ),\n                \"max__last_last_three_year_diff_maintenance_year\": (\n                    \"last_three_year_diff_maintenance_year\",\n                    \"max\",\n                ),\n                \"max__last_three_year_damage_maintenance\": (\n                    \"last_three_year_damage_maintenance\",\n                    \"max\",\n                ),\n                \"std__last_three_year_damage_maintenance\": (\n                    \"last_three_year_damage_maintenance\",\n                    \"std\",\n                ),\n                \"mean__last_three_year_damage_maintenance\": (\n                    \"last_three_year_damage_maintenance\",\n                    \"mean\",\n                ),\n                \"max__last_three_year_sum_maintenance\": (\n                    \"last_three_year_sum_maintenance\",\n                    \"max\",\n                ),\n                \"std__last_three_year_sum_maintenance\": (\n                    \"last_three_year_sum_maintenance\",\n                    \"std\",\n                ),\n                \"mean__last_three_year_sum_maintenance\": (\n                    \"last_three_year_sum_maintenance\",\n                    \"mean\",\n                ),\n                \"max__last_year_sum_maintenance\": (\n                    \"last_year_sum_maintenance\",\n                    \"max\",\n                ),\n                \"std__last_year_sum_maintenance\": (\n                    \"last_year_sum_maintenance\",\n                    \"std\",\n                ),\n                \"std__last_year_sum_maintenance\": (\n                    \"last_year_damage_maintenance\",\n                    \"std\",\n                ),\n            }\n        )\n    )","cf64c7d5":"import math\nimport types\nimport time\nimport typing as tp\nimport random\nimport tempfile\nimport zipfile\nimport warnings\nimport subprocess\nfrom pathlib import Path\nfrom functools import partial, reduce\nfrom contextlib import contextmanager\n\nimport numpy as np\nimport pandas as pd\n\nimport catboost\nfrom catboost import Pool, CatBoostRegressor\n\n# import xgboost as xgb\nimport lightgbm as lgb\n\nimport optuna\nfrom optuna.distributions import UniformDistribution\n\n\n@contextmanager\ndef timer(verbose, logger=None, format_str=\"{:.3f}[s]\", prefix=None, suffix=None):\n    if prefix:\n        format_str = str(prefix) + format_str\n    if suffix:\n        format_str = format_str + str(suffix)\n    start = time.time()\n    yield\n    d = time.time() - start\n    out_str = format_str.format(d)\n    if verbose:\n        if logger:\n            logger.info(out_str)\n        else:\n            print(out_str)\n\n\nclass TreeModel:\n    \"\"\"Wrapper for LightGBM\/XGBoost\/CATBoost\"\"\"\n\n    def __init__(self, model_type: str):\n        self.model_type = model_type\n        self.trn_data = None\n        self.val_data = None\n        self.model = None\n\n    def train(\n        self,\n        params: dict,\n        X_train: pd.DataFrame,\n        y_train: np.ndarray,\n        X_val: pd.DataFrame = None,\n        y_val: np.ndarray = None,\n        train_weight: tp.Optional[np.ndarray] = None,\n        val_weight: tp.Optional[np.ndarray] = None,\n        train_params: dict = None,\n        cat_cols: list = None,\n    ):\n\n        if self.model_type == \"lgb\":\n            self.trn_data = lgb.Dataset(X_train, label=y_train, weight=train_weight)\n            self.val_data = lgb.Dataset(X_val, label=y_val, weight=val_weight)\n\n            kwargs = dict(params=params, train_set=self.trn_data)\n            kwargs.update(train_params)\n            kwargs.update({\"valid_sets\": [self.trn_data, self.val_data]})\n\n            # if all([X_val is not None  y_val is not None]):\n            #    kwargs.update({\"valid_sets\": [self.trn_data, self.val_data]})\n\n            self.model = lgb.train(**kwargs)\n\n        elif self.model_type == \"xgb\":  # not prop working\n            self.trn_data = xgb.DMatrix(X_train, y_train, weight=train_weight)\n            self.val_data = xgb.DMatrix(X_val, y_val, weight=val_weight)\n            self.model = xgb.train(\n                params=params,\n                dtrain=self.trn_data,\n                evals=[(self.trn_data, \"train\"), (self.val_data, \"val\")],\n                **train_params,\n            )\n\n        elif self.model_type == \"cat\":  # not prop working\n            self.trn_data = Pool(\n                X_train, label=y_train, cat_features=cat_cols\n            )  # , group_id=[0] * len(X_train))\n            self.val_data = Pool(\n                X_val, label=y_val, cat_features=cat_cols\n            )  # , group_id=[0] * len(X_val))\n            self.model = CatBoostRegressor(**params)\n            self.model.fit(\n                self.trn_data,\n                eval_set=[self.val_data],\n                use_best_model=True,\n                **train_params,\n            )\n        else:\n            raise NotImplementedError\n\n    def predict(self, X: pd.DataFrame):\n        if self.model_type == \"lgb\":\n            return self.model.predict(\n                X, num_iteration=self.model.best_iteration\n            )  # type: ignore\n        elif self.model_type == \"xgb\":\n            X_DM = xgb.DMatrix(X)\n            return self.model.predict(\n                X_DM, ntree_limit=self.model.best_ntree_limit\n            )  # type: ignore\n        elif self.model_type == \"cat\":\n            return self.model.predict(X)\n        else:\n            raise NotImplementedError\n\n    @property\n    def feature_names_(self):\n        if self.model_type == \"lgb\":\n            return self.model.feature_name()\n        elif self.model_type == \"xgb\":\n            return list(self.model.get_score(importance_type=\"gain\").keys())\n        elif self.model_type == \"cat\":\n            return self.model.feature_names_\n        else:\n            raise NotImplementedError\n\n    @property\n    def feature_importances_(self):\n        if self.model_type == \"lgb\":\n            return self.model.feature_importance(importance_type=\"gain\")\n        elif self.model_type == \"xgb\":\n            return list(self.model.get_score(importance_type=\"gain\").values())\n        elif self.model_type == \"cat\":\n            return self.model.feature_importances_\n        else:\n            raise NotImplementedError\n\n\ndef run_gbdt_trainining_with_cv(\n    X,\n    y,\n    seed,\n    cv_signature,\n    cv_kwargs,\n    params=None,\n    gbdt_model_name=\"lgb\",\n    cat_cols=None,\n    scorer=False,\n    return_feature_importances=False,\n    return_out_of_fold_predictions=False,\n    verbose=False,\n):\n    cv = cv_signature(**cv_kwargs)\n    oof_prediction = np.zeros(len(X))\n    score_list = []\n    models = []\n    if not params:\n        params = {}\n    if gbdt_model_name == \"cat\":\n        params[\"random_state\"] = seed\n    else:\n        params[\"seed\"] = seed\n\n    for fold, (train_idx, validation_idx) in enumerate(cv.split(X, y)):\n        if verbose:\n            print(\"-\" * 100)\n            print(f\"Seed: {seed} - Fold: {fold}\")\n\n        X_trn = X.loc[train_idx, :].reset_index(drop=True)\n        X_val = X.loc[validation_idx, :].reset_index(drop=True)\n        y_trn = y.loc[train_idx]\n        y_val = y.loc[validation_idx]\n\n        model = TreeModel(model_type=gbdt_model_name)\n        with timer(prefix=\"Model training\", verbose=verbose):\n            model = run_gbdt_training(\n                X_trn,\n                y_trn,\n                \"lgb\",\n                params[\"model_params\"],\n                params[\"fit_params\"],\n                cat_cols,\n                X_val,\n                y_val,\n            )\n\n        if return_feature_importances:\n            feature_importance_ = pd.DataFrame()\n            feature_importance_[\"feature\"] = model.feature_names_\n            feature_importance_[\"importance\"] = model.feature_importances_\n            feature_importance_[\"fold\"] = fold\n            feature_importance_[\"seed\"] = seed\n\n        models.append(model)\n\n        with timer(prefix=\"Validation inference\", verbose=verbose):\n            val_pred = model.predict(X_val)\n            score = scorer(y_val, val_pred)\n            score_list.append([seed, fold, score])\n            oof_prediction[validation_idx] = val_pred\n\n        oof_score = scorer(y, oof_prediction)\n        score_list.append([\"avg\", \"oof\", oof_score])\n        score_df = pd.DataFrame(score_list, columns=[\"Seed\", \"Fold\", \"Metric Score\"])\n\n    if return_feature_importances:\n        return oof_prediction, score_df, models\n\n    # if return_out_of_fold_predictions:\n    #    return tmp_oof, score_df, models\n\n    return score_df, models\n\n\ndef run_gbdt_training(\n    X_train,\n    Y_train,\n    model_type,\n    model_params,\n    fit_params,\n    cat_cols=None,\n    X_val=None,\n    y_val=None,\n):\n\n    model = TreeModel(model_type=model_type)\n\n    model.train(\n        params=model_params,\n        X_train=X_train,\n        y_train=Y_train,\n        X_val=X_val,\n        y_val=y_val,\n        train_params=fit_params,\n        cat_cols=cat_cols,\n    )\n\n    return model","9cf7be24":"def get_feature_importance(model):\n    return (\n        pd.DataFrame(\n            dict(\n                zip(\n                    model.feature_name(),\n                    model.feature_importance(importance_type=\"gain\"),\n                )\n            ).items(),\n            columns=[\"feature_name\", \"feature_importance\"],\n        )\n        .sort_values(\"feature_importance\", ascending=False)\n        .assign(\n            cumsum_feature_importance=lambda df: df[\"feature_importance\"].cumsum()\n            \/ df[\"feature_importance\"].sum()\n        )\n    )","e1989d42":"def post_process_df(df_):\n\n    df = df_.copy()\n    df[\"min__TRAFFIC_DATE_rank\"] = df[\"min__TRAFFIC_DATE\"].rank(\n        ascending=False, pct=True\n    )\n    df[\"MAX_TRAFFIC_DATE_rank\"] = df[\"MAX_TRAFFIC_DATE\"].rank(ascending=False, pct=True)\n    df[\"last_seen_day_rank\"] = df[\"last_seen_day\"].rank(ascending=False, pct=True)\n    df[\"birth_date_rank\"] = df[\"birth_date\"].rank(ascending=False, pct=True)\n\n    for col in (\n        df.columns[df.columns.str.contains(\"LAST_STATUS\")]\n        .difference(df.columns[df.columns.str.contains(\"rank\")])\n        .tolist()\n    ):\n        df[col + \"_rank\"] = df[col].rank(ascending=False, pct=True)\n\n    return df","f5d5c1e3":"def get_training_data(folds, fold_idx, customer, end_date=\"2021-03-01\"):\n    # folds = get_cross_validation_folds()\n    fold_train_idx, sales_train_idx, maintenance, target = preprocess_data_by_fold_idx(\n        folds, fold_idx\n    )\n\n    preprocess_maintenance(fold_train_idx)\n    maintenance_meta = maintenance.drop_duplicates(\n        [\n            \"BRAND_CODE\",\n            \"BASEMODEL_CODE\",\n            \"TOPMODEL_CODE\",\n            \"MOTOR_GAS_TYPE\",\n            \"GEAR_BOX_TYPE\",\n        ]\n    ).loc[\n        :,\n        [\n            \"BRAND_CODE\",\n            \"BASEMODEL_CODE\",\n            \"TOPMODEL_CODE\",\n            \"MOTOR_GAS_TYPE\",\n            \"GEAR_BOX_TYPE\",\n            \"maintenance_expense__BRAND_CODE\",\n            \"maintenance_expense__BASEMODEL_CODE\",\n            \"maintenance_expense__TOPMODEL_CODE\",\n            \"maintenance_expense__MOTOR_GAS_TYPE\",\n            \"maintenance_expense__GEAR_BOX_TYPE\",\n        ],\n    ]\n\n    fold_train_idx = fold_train_idx.merge(\n        maintenance_meta,\n        on=[\n            \"BRAND_CODE\",\n            \"BASEMODEL_CODE\",\n            \"TOPMODEL_CODE\",\n            \"MOTOR_GAS_TYPE\",\n            \"GEAR_BOX_TYPE\",\n        ],\n        how=\"left\",\n    )\n\n    dt = pd.to_datetime(end_date)\n\n    sales_last_feat = (\n        sales.query(f\"SF_CREATE_DATE < '{end_date}'\")\n        .groupby([\"BASE_CUSTOMER_ID\", \"STATUS\"])[\"SF_CREATE_DATE\"]\n        .max()\n        .unstack()\n        .applymap(lambda x: (dt - x).days)\n    ).rename(columns=lambda x: \"LAST_STATUS__\" + str(x))\n\n    fold_train_idx = fold_train_idx.merge(\n        fold_train_idx.query(f\"START_DATE < '{end_date}'\")\n        .groupby([\"BASEMODEL_CODE\"])[\"BASE_CUSTOMER_ID\"]\n        .nunique()\n        .rank(ascending=True, pct=True)\n        .to_frame(\"BASEMODEL_CODE_dt\")\n        .reset_index(),\n        on=[\"BASEMODEL_CODE\"],\n        how=\"left\",\n    )\n\n    fold_train_idx = fold_train_idx.merge(\n        fold_train_idx.query(f\"START_DATE < '{end_date}'\")\n        .groupby([\"TOPMODEL_CODE\"])[\"BASE_CUSTOMER_ID\"]\n        .nunique()\n        .rank(ascending=True, pct=True)\n        .to_frame(\"TOPMODEL_CODE_dt\")\n        .reset_index(),\n        on=[\"TOPMODEL_CODE\"],\n        how=\"left\",\n    )\n\n    fold_train_idx = fold_train_idx.merge(\n        fold_train_idx.query(f\"START_DATE < '{end_date}'\")\n        .groupby([\"BRAND_CODE\"])[\"BASE_CUSTOMER_ID\"]\n        .nunique()\n        .rank(ascending=True, pct=True)\n        .to_frame(\"BRAND_CODE_dt\")\n        .reset_index(),\n        on=[\"BRAND_CODE\"],\n        how=\"left\",\n    )\n\n    customer_vehicle_brand_fe = (\n        fold_train_idx.groupby([\"BASE_CUSTOMER_ID\"])\n        .aggregate(\n            **{\n                \"BASEMODEL_CODE_dt_min\": (\"BASEMODEL_CODE_dt\", \"min\"),\n                \"TOPMODEL_CODE_dt_min\": (\"TOPMODEL_CODE_dt\", \"min\"),\n                \"BRAND_CODE_dt_min\": (\"BRAND_CODE_dt\", \"min\"),\n                \"BASEMODEL_CODE_dt_mean\": (\"BASEMODEL_CODE_dt\", \"mean\"),\n                \"TOPMODEL_CODE_dt_mean\": (\"TOPMODEL_CODE_dt\", \"mean\"),\n                \"BRAND_CODE_dt_mean\": (\"BRAND_CODE_dt\", \"mean\"),\n                \"BASEMODEL_CODE_dt_max\": (\"BASEMODEL_CODE_dt\", \"max\"),\n                \"TOPMODEL_CODE_dt_max\": (\"TOPMODEL_CODE_dt\", \"max\"),\n                \"BRAND_CODE_dt_max\": (\"BRAND_CODE_dt\", \"max\"),\n            }\n        )\n        .join(\n            (\n                (\n                    dt\n                    - fold_train_idx.query(\n                        f\"START_DATE < '{end_date}' and TRAFFIC_DATE < '{end_date}'\"\n                    )\n                    .astype({\"TRAFFIC_DATE\": \"datetime64[ns]\"})\n                    .groupby([\"BASE_CUSTOMER_ID\"])[\"TRAFFIC_DATE\"]\n                    .max()\n                )\n            ).dt.days.to_frame(\"MAX_TRAFFIC_DATE\"),\n            on=\"BASE_CUSTOMER_ID\",\n            how=\"left\",\n        )\n    )\n\n    maintenance[\"year\"] = (\n        maintenance[\"START_DATE\"].max() - maintenance[\"START_DATE\"]\n    ).dt.days \/\/ 365 + 1\n\n    cust_vehicle = (\n        maintenance.groupby(\n            [\"BASE_CUSTOMER_ID\", \"IS_MAINTENANCE\", \"CUSTOMER_ID\", \"VEHICLE_ID\"]\n        )\n        .aggregate(\n            **{\n                \"sum__TOTAL_AMOUNT_TL\": (\"TOTAL_AMOUNT_TL\", \"sum\"),\n                \"year\": (\"year\", \"max\"),\n            }\n        )\n        .assign(expense_by_year=lambda df: df[\"sum__TOTAL_AMOUNT_TL\"] \/ df[\"year\"])\n        .reset_index()\n        .astype({\"CUSTOMER_ID\": \"int64\"})\n        .merge(\n            customer.loc[\n                :,\n                [\n                    \"BIRTH_DATE\",\n                    \"GENDER\",\n                    \"CUSTOMER_ID\",\n                    \"FK_ADDRESS_COMMUNICATION_CITY\",\n                    \"OCCUPATION\",\n                ],\n            ],\n            on=[\"CUSTOMER_ID\"],\n        )\n    )\n\n    cust_vehicle[\"FK_ADDRESS_COMMUNICATION_CITY\"] = cust_vehicle[\n        \"FK_ADDRESS_COMMUNICATION_CITY\"\n    ].str.lower()\n\n    cust_fe = customer.groupby([\"BASE_CUSTOMER_ID\"]).aggregate(\n        **{\n            \"GENDER\": (\"GENDER\", lambda x: x.mode()),\n            \"MARITAL_STATUS\": (\"MARITAL_STATUS\", lambda x: x.mode()),\n            \"birth_date\": (\"BIRTH_DATE\", lambda x: x.mode()),\n            \"FK_ADDRESS_COMMUNICATION_CITY\": (\n                \"FK_ADDRESS_COMMUNICATION_CITY\",\n                lambda x: x.mode(),\n            ),\n            \"diff_occupation_cnt\": (\n                \"OCCUPATION\",\n                \"nunique\",\n            ),\n        }\n    )\n\n    cust_fe = cust_fe.reset_index().merge(\n        cust_vehicle.loc[:, [\"BASE_CUSTOMER_ID\", \"OCCUPATION\"]]\n        .drop_duplicates()\n        .merge(\n            cust_vehicle.groupby([\"CUSTOMER_ID\", \"OCCUPATION\", \"IS_MAINTENANCE\"])[\n                \"expense_by_year\"\n            ]\n            .mean()\n            .groupby(level=[\"OCCUPATION\", \"IS_MAINTENANCE\"])\n            .mean()\n            .sort_values()\n            .unstack(1)\n            .rename(\n                columns=lambda x: \"occupation_expense__\" + \"damage\"\n                if x == 0\n                else \"occupation_expense__\" + \"maintenance\"\n            )\n            .sort_values(\"occupation_expense__maintenance\")\n            .reset_index(),\n            on=[\"OCCUPATION\"],\n            how=\"left\",\n        )\n        .groupby([\"BASE_CUSTOMER_ID\"])\n        .aggregate(\n            **{\n                \"occupation_expense__damage\": (\"occupation_expense__damage\", \"mean\"),\n                \"occupation_expense__maintenance\": (\n                    \"occupation_expense__maintenance\",\n                    \"mean\",\n                ),\n            }\n        ),\n        on=[\"BASE_CUSTOMER_ID\"],\n        how=\"left\",\n    )\n\n    cust_adress_fe = (\n        cust_vehicle.groupby(\n            [\"CUSTOMER_ID\", \"FK_ADDRESS_COMMUNICATION_CITY\", \"IS_MAINTENANCE\"]\n        )[\"expense_by_year\"]\n        .mean()\n        .groupby(level=[\"FK_ADDRESS_COMMUNICATION_CITY\", \"IS_MAINTENANCE\"])\n        .mean()\n        .sort_values()\n        .unstack(1)\n        .rename(\n            columns=lambda x: \"adress_expense__\" + \"damage\"\n            if x == 0\n            else \"adress_expense__\" + \"maintenance\"\n        )\n        .reset_index()\n    )\n\n    cust_fe[\"FK_ADDRESS_COMMUNICATION_CITY\"] = cust_fe[\n        \"FK_ADDRESS_COMMUNICATION_CITY\"\n    ].astype(str)\n\n    cust_adress_fe[\"FK_ADDRESS_COMMUNICATION_CITY\"] = cust_adress_fe[\n        \"FK_ADDRESS_COMMUNICATION_CITY\"\n    ].astype(str)\n\n    cust_adress_fe = cust_adress_fe.dropna()\n\n    cust_fe = cust_fe.merge(\n        cust_adress_fe.dropna(), on=[\"FK_ADDRESS_COMMUNICATION_CITY\"], how=\"left\"\n    )\n\n    cust_fe[\"birth_date\"] = (\n        cust_fe[\"birth_date\"]\n        .apply(lambda x: np.nan if isinstance(x, np.ndarray) else x)\n        .astype(\"float\")\n    )\n\n    cust_fe[\"MARITAL_STATUS\"] = (\n        cust_fe[\"MARITAL_STATUS\"]\n        .apply(lambda x: np.nan if isinstance(x, np.ndarray) else x)\n        .astype(\"category\")\n        .cat.codes.replace(-1, np.nan)\n    )\n\n    cust_fe[\"GENDER\"] = (\n        cust_fe[\"GENDER\"]\n        .apply(lambda x: np.nan if isinstance(x, np.ndarray) else x)\n        .astype(\"category\")\n        .cat.codes.replace(-1, np.nan)\n    )\n\n    #fold_train_idx, sales_train_idx, maintenance, target\n    fk = (\n        sales_train_idx.loc[:, [\"BASE_CUSTOMER_ID\", \"SF_CREATE_DATE\"]]\n        .drop_duplicates()\n        .sort_values(\"SF_CREATE_DATE\")\n        .merge(\n            fold_train_idx.query(\n                \"FK_RELATION_STATUS_EXPLANATION == 'Pasif Ruhsat Sahibi'\"\n            )\n            .loc[:, [\"BASE_CUSTOMER_ID\", \"START_DATE\"]]\n            .drop_duplicates(),\n            on=[\"BASE_CUSTOMER_ID\"],\n        )\n    ).query(\"SF_CREATE_DATE > START_DATE\")\n\n    fk[\"rank\"] = fk.groupby([\"BASE_CUSTOMER_ID\", \"START_DATE\"])[\"SF_CREATE_DATE\"].rank(\n        method=\"first\"\n    )\n    fk = fk.query(\"rank == 1\")\n    fk[\"date_diff\"] = (fk[\"SF_CREATE_DATE\"] - fk[\"START_DATE\"]).dt.days\n\n    rt = fk.groupby([\"BASE_CUSTOMER_ID\"]).aggregate(\n        **{\n            \"min_date_diff_pasif\": (\"date_diff\", \"min\"),\n            \"median_date_diff_pasif\": (\"date_diff\", \"median\"),\n            \"q25_date_diff_pasif\": (\"date_diff\", lambda x: x.quantile(0.25)),\n        }\n    )\n\n    df = (\n        fe_cust_vehicle_data(fold_train_idx, end_date)\n        .join(fe_sales_data(sales_train_idx, folds, fold_idx, end_date), how=\"left\")\n        .join(target.set_index(\"BASE_CUSTOMER_ID\"), how=\"left\")\n        .join(fe_maintenance_data(maintenance, end_date), how=\"left\")\n        .join(customer_vehicle_brand_fe, how=\"left\")\n        .join(sales_last_feat, how=\"left\")\n        .join(rt, how=\"left\")\n    )\n\n    df.loc[:, \"is_there_purchase_any\"] = (\n        df.index.isin(sales_train_idx[\"BASE_CUSTOMER_ID\"]) * 1\n    )\n\n    df = df.reset_index(drop=False)\n\n    df = df.merge(\n        cust_fe.loc[\n            :,\n            ~cust_fe.columns.isin([\"FK_ADDRESS_COMMUNICATION_CITY\"]),\n        ],\n        on=[\"BASE_CUSTOMER_ID\"],\n        how=\"left\",\n    )\n\n    df[\"min__TRAFFIC_DATE\"] = (\n        df[\"min__TRAFFIC_DATE\"].max() - df[\"min__TRAFFIC_DATE\"]\n    ).dt.days\n\n    all_fe = pd.concat(\n        [\n            maintenance.query(f\"CREATE_DATE < '{end_date}'\")\n            .groupby(\"BASE_CUSTOMER_ID\")[\"CREATE_DATE\"]\n            .max()\n            .to_frame(\"maintenance\"),\n            sales.query(f\"SF_CREATE_DATE < '{end_date}'\")\n            .groupby(\"BASE_CUSTOMER_ID\")[\"SF_CREATE_DATE\"]\n            .max()\n            .to_frame(\"sales\"),\n            cust_vehicle_interactions.query(f\"START_DATE < '{end_date}'\")\n            .groupby(\"BASE_CUSTOMER_ID\")[\"START_DATE\"]\n            .max()\n            .to_frame(\"interaction\"),\n        ],\n        axis=1,\n    )\n\n    all_fe = all_fe.astype(\"datetime64[ns]\")\n\n    all_fe[\"last_seen_day\"] = (pd.to_datetime(end_date) - all_fe.min(axis=1)).dt.days\n\n    all_fe = pd.concat(\n        [\n            all_fe.loc[:, [\"last_seen_day\"]],\n            (all_fe.loc[:, [\"maintenance\", \"sales\", \"interaction\"]].isna() * 1).rename(\n                columns=lambda x: \"is_seen__\" + x\n            ),\n        ],\n        axis=1,\n    )\n\n    df = df.merge(all_fe.reset_index(), on=[\"BASE_CUSTOMER_ID\"], how=\"left\")\n\n    return df","a2b07b56":"def get_last_n_day_feats(end_date):\n    end = datetime.datetime(*map(int, end_date.split(\"-\")))\n    date_format = \"%Y-%m-%d\"\n    f1s, f1e = (end - datetime.timedelta(120)).strftime(date_format), (\n        end - datetime.timedelta(45)\n    ).strftime(date_format)\n    f2s, f2e = (end - datetime.timedelta(45)).strftime(date_format), (\n        end - datetime.timedelta(14)\n    ).strftime(date_format)\n    f3s, f3e = (end - datetime.timedelta(14)).strftime(date_format), end.strftime(\n        date_format\n    )\n\n    return (\n        sales.query(f\"SF_CREATE_DATE >= '{f1s}' and SF_CREATE_DATE < '{f1e}'\")\n        .assign(last_90_true=True)\n        .loc[:, [\"BASE_CUSTOMER_ID\", \"last_90_true\"]]\n        .drop_duplicates()\n        .merge(\n            sales.query(f\"SF_CREATE_DATE >= '{f2s}' and SF_CREATE_DATE < '{f2e}'\")\n            .assign(last_30_true=True)\n            .loc[:, [\"BASE_CUSTOMER_ID\", \"last_30_true\"]]\n            .drop_duplicates(),\n            on=[\"BASE_CUSTOMER_ID\"],\n            how=\"outer\",\n        )\n        .merge(\n            sales.query(f\"SF_CREATE_DATE >= '{f3s}' and SF_CREATE_DATE < '{f3e}'\")\n            .assign(last_15_true=True)\n            .loc[:, [\"BASE_CUSTOMER_ID\", \"last_15_true\"]]\n            .drop_duplicates(),\n            on=[\"BASE_CUSTOMER_ID\"],\n            how=\"outer\",\n        )\n        * 1\n    )","650d9402":"import datetime\nfrom functools import reduce\n\ndates = [2014 + i for i in range(7) if 2015 + i != 2020]\n\nfolds_mlp = pd.concat(\n    get_cross_validation_folds(x, y)\n    for x, y in [\n        (f\"{start}-12-01\", f\"{end}-03-01\")\n        for start, end in zip(dates[:-1], dates[1:])\n        if end != 2020\n    ]\n    + [(\"2019-12-01\", \"2020-03-01\")]\n)\n\ntr_folds = pd.concat(\n    [\n        folds_mlp[\n            folds_mlp[\"target_cond\"].str.contains(\"-03-01\")\n            & folds_mlp[\"target_cond\"].str.contains(\"-06-01\")\n            & ~folds_mlp[\"start_date\"].astype(str).str.contains(\"|\".join([\"--\"]))\n        ]\n        .reset_index(drop=True)\n        .drop_duplicates(),\n        folds_mlp.iloc[[-1]].replace(\"train\", \"test\"),\n    ],\n    ignore_index=True,\n)\n\ndataframes = []\nfor idx in [6]:\n    end_date = tr_folds.loc[idx, \"end_date\"].date().strftime(\"%Y-%m-%d\")\n    dataframes.append(\n        get_training_data(tr_folds, idx, customer, end_date).assign(fold=f\"fold{idx}\")\n    )\n    \npd.Index(\n    sales.query(\n        \"{col} >= '2020-03-01' and {col} < '2020-06-01'\".format(col=\"SF_CREATE_DATE\")\n    )[\"BASE_CUSTOMER_ID\"]\n    .drop_duplicates()\n    .values\n).difference(dataframes[-1].query(\"target == 1\")[\"BASE_CUSTOMER_ID\"])","80ab4ea3":"import warnings\nfrom optuna import distributions\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\nseed = 42\nn_splits = 5\ncv_signature = StratifiedKFold\ncv_kwargs = {\"n_splits\": n_splits, \"random_state\": seed, \"shuffle\": True}\n\nscorer = f1_score\n\nwarnings.filterwarnings(\"ignore\")\n\ndataframes = []\nfolds = get_cross_validation_folds()\n\ncompute_last_n_features = True\n\nfor idx in [3, 4]:\n    end_date = folds.loc[idx, \"end_date\"].date().strftime(\"%Y-%m-%d\")\n    df_ = post_process_df(\n        get_training_data(folds, idx, customer, end_date).assign(fold=f\"fold{idx}\")\n    )\n\n    if compute_last_n_features:\n        last_n = get_last_n_day_feats(end_date)\n\n        df_ = df_.merge(\n            last_n,\n            on=[\"BASE_CUSTOMER_ID\"],\n            how=\"left\",\n        )\n        df_[\"last_90_true\"] = df_[\"last_90_true\"].fillna(0)\n        df_[\"last_30_true\"] = df_[\"last_30_true\"].fillna(0)\n        df_[\"last_15_true\"] = df_[\"last_15_true\"].fillna(0)\n\n    dataframes.append(df_.assign(fold=f\"fold{idx}\"))\n\n# test_data = get_training_data(5, customer, \"2021-06-01\")\n# test_data = post_process_df(test_data)\n\n# cols = reduce(lambda x, y: x.intersection(y), [df.columns for df in dataframes])\n# df3 = pd.concat(dataframes).reset_index(drop=True).loc[:, cols]","1970a162":"# dataframes = list(map(post_process_df, dataframes))\ncols = reduce(lambda x, y: x.intersection(y), [df.columns for df in dataframes])\ndf3 = pd.concat(dataframes).reset_index(drop=True).loc[:, cols]","c98b3439":"test_data = get_training_data(folds, 5, customer, \"2021-06-01\")\ntest_data = post_process_df(test_data)\ntest_data = test_data.loc[:, cols.difference([\"fold\"])]\n\n\n\"\"\"\nif compute_last_n_features:\n    last_n = get_last_n_day_feats(\"2021-06-01\")\n\n    test_data = test_data.merge(\n        last_n,\n        on=[\"BASE_CUSTOMER_ID\"],\n        how=\"left\",\n    )\n    test_data[\"last_90_true\"] = test_data[\"last_90_true\"].fillna(0)\n    test_data[\"last_30_true\"] = test_data[\"last_30_true\"].fillna(0)\n    test_data[\"last_15_true\"] = test_data[\"last_15_true\"].fillna(0)\n    \n\"\"\"","70204084":"df = dataframes[-1]\n\nX_train, X_test = train_test_split(\n    df.index, stratify=df[\"target\"], test_size=0.15, random_state=42\n)\n\nX_test_all = df3.loc[\n    (df3[\"fold\"] == df3[\"fold\"].unique()[-1])\n    & (df3[\"BASE_CUSTOMER_ID\"].isin(df.loc[X_test, \"BASE_CUSTOMER_ID\"]))\n].index\n\nX_train_all = df3.index.difference(X_test_all)","66d510e5":"idx_vars = []\ndrop_cols = (\n    [\n        \"nunique__status_100\",\n        \"nunique__STATUS\",\n        \"count__pasif_ruhsat_sahibi\",\n        \"nunique__brand_code\",\n        \"max__vehichle_age_after__last_purchase\",\n        \"nunique__status_109\",\n        \"max__last_three_year_damage_maintenance\",\n        \"nunique__REQ_BRAND_CODE\",\n        \"count__first_hand_False\",\n        \"count__first_hand_brand_code_Y\",\n        \"mean__time_spent_day_after__last_purchase\",\n        \"mean__last_three_year_sum_maintenance\",\n        \"mean__last_three_year_damage_maintenance\",\n        \"pasif_ruhsat_cnt\",\n        \"first_hand_ratio\",\n        \"ratio_nunique__0_1_vehicle_id\",\n        \"ratio_nunique__1_3_vehicle_id\",\n        \"ratio_nunique__3_7_vehicle_id\",\n        \"ratio_nunique__7_15_vehicle_id\",\n        \"ratio_nunique__15_inf_vehicle_id\",\n        \"max_maintenance_expense__BRAND_CODE\",\n        \"max_maintenance_expense__MOTOR_GAS_TYPE\",\n        \"max_maintenance_expense__GEAR_BOX_TYPE\",\n        \"expected_proba\",\n        \"all_skew_traffic\",\n        \"last12_std\",\n        \"all_std\",\n        \"last6_std\",\n        \"last3_std\",\n        \"fold\",\n        \"all_skew\",\n        \"last12_skew\",\n        \"last6_skew\",\n        \"last3_skew\",\n        \"MAX_TRAFFIC_DATE\",\n        \"last_seen_day\",\n        \"birth_date_rank\",\n        \"expected\",\n        \"nunique__sf_create_status_365_inf_106\",\n    ]\n    + df.columns[df.columns.str.contains(\"nunique__sf_create_status\")]\n    .difference(\n        [\n            \"nunique__sf_create_status_-001_30_102\",\n            # \"nunique__sf_create_status_-001_30_101\",\n        ]\n    )\n    .tolist()\n) + df.columns[df.columns.str.contains(\"LAST_STATUS\")].difference(\n    df.columns[df.columns.str.contains(\"rank\")]\n).tolist()\n\n\ncat_cols = []\n\n\nparams = {\n    \"model_params\": {\n        \"boosting_type\": \"gbdt\",\n        \"force_row_wise\": True,\n        \"max_depth\": 10,\n        \"verbose\": -1,\n        \"learning_rate\": 0.015,\n        \"bagging_seed\": 42,\n        \"bagging_fraction\": 0.6,\n        \"feature_fraction_seed\": 42,\n        \"extra_seed\": 42,\n        \"drop_seed\": 42,\n        \"min_data_in_leaf\": 5,\n        \"lambda_l1\": 1e-4,\n        \"min_gain_to_split\": 1e-6,\n        \"feature_fraction\": 0.9,\n        \"feature_fraction_bynode\": 0.5,\n        \"top_rate\": 0.1,\n        \"num_boost_round\": 15000,\n        \"early_stopping_round\": 300,\n        \"metric\": \"auc\",\n    },\n    \"fit_params\": {\"verbose_eval\": 100},\n}\nprevious_purchase_drop_cols = [\n    \"min__TRAFFIC_DATE\",\n    \"max__time_spent_day_after__last_purchase\",\n    \"min__time_spent_day_after__last_purchase\",\n    \"min__vehichle_age_after__last_purchase\",\n    \"mean__vehichle_age_after__last_purchase\",\n    \"std_maintenance_expense__BRAND_CODE\",\n    \"mean_maintenance_expense__BRAND_CODE\",\n    \"max_maintenance_expense__BASEMODEL_CODE\",\n    \"mean_maintenance_expense__BASEMODEL_CODE\",\n    \"max_maintenance_expense__TOPMODEL_CODE\",\n    \"mean_maintenance_expense__TOPMODEL_CODE\",\n    \"nunique__0_1_vehicle_id\",\n    \"nunique__3_7_vehicle_id\",\n    \"nunique__7_15_vehicle_id\",\n    \"nunique__15_inf_vehicle_id\",\n    \"count__brand_code_T\",\n    \"count__brand_code_Y\",\n    \"std__last_three_year_damage_maintenance\",\n    \"max__last_year_sum_maintenance\",\n    \"nunique__365_inf_salesfile_id\",\n    \"BASEMODEL_CODE_dt_max\",\n    \"mean__last_last_three_year_diff_maintenance_yearr\",\n    \"all_v2_skew\",\n    ]\n\ngbdt_cols = sorted(\n    df.columns[\n        ~df.columns.isin(\n            idx_vars\n            + [\"target\"]\n            + drop_cols\n            + previous_purchase_drop_cols\n            + [\n                \"max_date_diff_pasif\",\n                \"mean_date_diff_pasif\",\n                \"q15_date_diff_pasif\",\n                \"q65_date_diff_pasif\",\n            ]\n            + [\n                \"min_maintenance_date_diff_pasif\",\n                \"max_maintenance_date_diff_pasif\",\n                \"mean_maintenance_date_diff_pasif\",\n                \"median_maintenance_date_diff_pasif\",\n                \"q15_maintenance_date_diff_pasif\",\n                \"q65_maintenance_date_diff_pasif\",\n            ]\n        )\n    ]\n)","1c110306":"res = run_gbdt_training(\n    df.loc[X_train, gbdt_cols].reset_index(drop=True),  # + [\"date_diff_mean\"],\n    df.loc[X_train, \"target\"],\n    \"lgb\",\n    params[\"model_params\"],\n    params[\"fit_params\"],\n    cat_cols=cat_cols,\n    X_val=df.loc[X_test, gbdt_cols].reset_index(drop=True),  # + [\"date_diff_mean\"],\n    y_val=df.loc[X_test, \"target\"].reset_index(drop=True),\n)","245ae0da":"import xgboost as xgb\n\nxgb_param = {\n    \"max_depth\": 0,\n    \"random_state\": 42,\n    \"eval_metric\": \"auc\",\n    \"scale_pos_weight\": 3,\n    \"max_leaves\": 412,\n    \"tree_method\": \"hist\",\n    \"grow_policy\": \"lossguide\",\n    \"colsample_bytree\": 0.8,\n    \"colsample_bylevel\": 0.8,\n    \"colsample_bynode\": 0.8,\n    \"eta\": 0.0015,\n    \"n_estimators\": 4500,\n}\n\nmodel = xgb.XGBClassifier(**xgb_param)\n\nxgb_param = {\n    \"max_depth\": 0,\n    \"random_state\": 42,\n    \"eval_metric\": \"auc\",\n    \"scale_pos_weight\": 3,\n    \"max_leaves\": 396,\n    \"subsample\": 0.85,\n    \"min_child_weight\": 0.03,\n    \"tree_method\": \"gpu_hist\",\n    \"grow_policy\": \"lossguide\",\n    \"colsample_bytree\": 0.8,\n    \"colsample_bylevel\": 0.8,\n    \"colsample_bynode\": 0.8,\n    \"eta\": 0.0015,\n    \"n_estimators\": 4500,\n}\n\nmodel.fit(\n    df3.loc[\n        X_train_all,\n        gbdt_cols,\n    ].reset_index(drop=True),\n    df3.loc[X_train_all, \"target\"],\n    eval_set=[\n        (\n            df3.loc[\n                X_test_all,\n                gbdt_cols,\n            ].reset_index(drop=True),\n            df3.loc[X_test_all, \"target\"].reset_index(drop=True),\n        )\n    ],\n    early_stopping_rounds=100,\n    verbose=50,\n)","cdd56286":"\"\"\"\nX_fit = pd.concat(\n    [\n        df.loc[\n            X_train, ~df.columns.isin(idx_vars + [\"target\"] + drop_cols)\n        ].reset_index(drop=True),\n        df_2.loc[:, ~df.columns.isin(idx_vars + [\"target\"] + drop_cols)],\n    ]\n).reset_index(drop=True)\ny_fit = pd.concat([df.loc[X_train, \"target\"], df_2.loc[:, \"target\"]]).reset_index(\n    drop=True\n)\n\"\"\"\n\n\"\"\"\nparams = {\n    \"model_params\": {\n        # \"is_unbalance\": True,\n        \"boosting_type\": \"gbdt\",\n        \"force_row_wise\": True,\n        \"max_depth\": 10,\n        # \"scale_pos_weight\": 101,\n        \"verbose\": -1,\n        \"extra_trees\": False,\n        # \"uniform_drop\": False,\n        \"learning_rate\": 0.015,\n        \"bagging_seed\": 42,\n        # \"bagging_freq\": 3,\n        # \"bagging_fraction\": 0.9,\n        # \"min_gain_to_split\": 0.05,\n        \"min_sum_hessian_in_leaf\": 0.005,\n        \"feature_fraction_seed\": 42,\n        \"extra_seed\": 42,\n        \"drop_seed\": 42,\n        # \"device\": \"gpu\",\n        # \"num_leaves\": 131,\n        \"min_data_in_leaf\": 5,\n        \"lambda_l1\": 0.0075,\n        # \"lambda_l2\": 0.03,\n        # \"min_gain_to_split\": 0.05,\n        # \"path_smooth\": 7,\n        # \"pos_bagging_fraction\": 0.7,\n        \"neg_bagging_fraction\": 0.5,\n        \"bagging_fraction\": 0.6,\n        \"feature_fraction\": 0.8,\n        \"feature_fraction_bynode\": 0.5,\n        \"top_rate\": 0.1,\n        \"num_boost_round\": 1000,\n        \"early_stopping_round\": 100,\n        \"metric\": \"auc\",\n    },\n    \"fit_params\": {\"verbose_eval\": 100},\n}\n\nres = run_gbdt_training(\n    pd.concat(\n        [\n            df.loc[\n                X_train, ~df.columns.isin(idx_vars + [\"target\"] + drop_cols)\n            ].reset_index(drop=True),\n            df_2.loc[:, ~df.columns.isin(idx_vars + [\"target\"] + drop_cols)],\n        ]\n    ).reset_index(drop=True),\n    pd.concat([df.loc[X_train, \"target\"], df_2.loc[:, \"target\"]]).reset_index(\n        drop=True\n    ),\n    \"lgb\",\n    params[\"model_params\"],\n    params[\"fit_params\"],\n    cat_cols=cat_cols,\n    X_val=df.loc[\n        X_test, ~df.columns.isin(idx_vars + [\"target\"] + drop_cols)\n    ].reset_index(drop=True),\n    y_val=df.loc[X_test, \"target\"].reset_index(drop=True),\n)\n\"\"\"","b047c70f":"mod = \"TEST\"\n\nif mod == \"TRAIN\":\n    xgb_pred = model.predict_proba(\n        df.loc[\n            X_test,\n            ~df.columns.isin(\n                idx_vars + [\"target\"] + drop_cols + previous_purchase_drop_cols\n            ),\n        ].reset_index(drop=True)\n    )[:, 1]\n\n    lgb_pred = res.model.predict(\n        df.loc[\n            X_test,\n            ~df.columns.isin(\n                idx_vars + [\"target\"] + drop_cols + previous_purchase_drop_cols\n            ),\n        ].reset_index(drop=True)\n    )\n\n    lgb_pred_focal = special.expit(focal_model.predict(X_val))\n\n    res_pred = pd.concat(\n        [\n            pd.Series(lgb_pred, name=\"lgb\").clip(0, 1),\n            pd.Series(xgb_pred, name=\"xgb\").clip(0, 1),\n            # pd.Series(lgb_pred_focal, name=\"focal\").clip(0, 1),\n            df.loc[X_test, [\"target\"]].reset_index(drop=True),\n        ],\n        axis=1,\n    )\n\n    res_pred[\"blend_pred\"] = res_pred.loc[:, [\"lgb\", \"xgb\"]].apply(\n        lambda x: x[\"lgb\"] * 0.8 + x[\"xgb\"] * 0.2, axis=1\n    )\n\n    res_pred[\"lgb_xgb_mean\"] = res_pred.loc[:, [\"lgb\", \"xgb\"]].mean(axis=1)\n    #res_pred[\"lgb_xgb_focal_mean\"] = res_pred.loc[:, [\"lgb\", \"xgb\", \"focal\"]].mean(\n    #    axis=1\n    #)\n\n\nelse:\n    lgb_pred = res.model.predict(\n        test_data.loc[\n            :,\n            ~test_data.columns.isin(\n                idx_vars + [\"target\"] + drop_cols + previous_purchase_drop_cols\n            ),\n        ]\n    )\n\n    xgb_pred = model.predict_proba(\n        test_data.loc[\n            :,\n            ~test_data.columns.isin(\n                idx_vars + [\"target\"] + drop_cols + previous_purchase_drop_cols\n            ),\n        ]\n    )[:, 1]\n\n    # lgb_pred_focal = special.expit(focal_model.predict(X_test_fit))\n\n    res_pred = pd.concat(\n        [\n            pd.Series(lgb_pred, name=\"lgb\").clip(0, 1),\n            pd.Series(xgb_pred, name=\"xgb\").clip(0, 1),\n            # pd.Series(lgb_pred_focal, name=\"focal\").clip(0, 1),\n        ],\n        axis=1,\n    )\n\n    res_pred[\"avg_pred\"] = res_pred.loc[:, [\"lgb\", \"xgb\"]].apply(\n        lambda x: x[\"lgb\"] * 0.5 + x[\"xgb\"] * 0.5, axis=1\n    )\n\n    res_pred[\"blend_pred\"] = res_pred.loc[:, [\"lgb\", \"xgb\"]].apply(\n        lambda x: x[\"lgb\"] * 0.7 + x[\"xgb\"] * 0.3, axis=1\n    )","6029b30b":"sub = (\n    test_data.reset_index()\n    .loc[:, [\"BASE_CUSTOMER_ID\"]]\n    .rename(columns={\"BASE_CUSTOMER_ID\": \"Id\"})\n)\n\n\nmethod = \"gbdt\"\n\nif method == \"focal\":\n    sub[\"Expected\"] = special.expit(\n        model.predict(\n            df_test.loc[\n                :, df.columns[~df.columns.isin(idx_vars + [\"target\"] + drop_cols)]\n            ]\n        )\n    )\n\nelse:\n    sub[\"Expected\"] = res_pred[\"lgb\"] #avg_pred, xgb, lgb\n\nsubmission = pd.read_csv(\"..\/input\/dogus-datathon-otomotiv\/sample_submission.csv\")\nsubmission = submission.drop(columns=[\"Expected\"]).merge(sub, on=[\"Id\"], how=\"left\")\nsubmission[\"Expected\"] = submission[\"Expected\"].clip(0, 1)\nsubmission = submission.sort_values(\"Id\", ignore_index=True)\nsubmission.to_csv(\"sub_lgbp_v2_20211103.csv\", index=False)","f0e051f2":"X_test_all = df3.loc[\n    (df3[\"fold\"] == \"fold4\")\n    & (df3[\"BASE_CUSTOMER_ID\"].isin(df.loc[X_test, \"BASE_CUSTOMER_ID\"]))\n].index\n\nX_train_all = df3.index.difference(X_test_all)","4996024c":"## Post-process FE ","d90d7f68":"## LGB feature importance yard\u0131mc\u0131 fonksiyonu. ","d95b2175":"print(\n    metrics.roc_auc_score(res_pred.loc[X_test, \"target\"], res_pred.loc[X_test, \"xgb\"])\n)\nprint(\n    metrics.roc_auc_score(res_pred.loc[X_test, \"target\"], res_pred.loc[X_test, \"lgb\"])\n)\nprint(\n    metrics.roc_auc_score(res_pred.loc[X_test, \"target\"], res_pred.loc[X_test, \"focal\"])\n)\nprint(\n    metrics.roc_auc_score(\n        res_pred.loc[X_test, \"target\"], res_pred.loc[X_test, \"lgb_xgb_mean\"]\n    )\n)","2aabece0":"## Imports","b1cc85cc":"## XGBOOST TRAINING ","6ada5d8a":"test_pa_index = df.loc[X_train][\"is_there_purchase_any\"][\n    df.loc[X_train][\"is_there_purchase_any\"] == 1\n].index\n\ntest_p_index = df.loc[X_test][\"is_there_purchase_any\"][\n    df.loc[X_test][\"is_there_purchase_any\"] == 1\n].index\n\nres_prev = run_gbdt_training(\n    df.loc[\n        X_train,\n        gbdt_cols,\n    ].reset_index(drop=True),\n    df.loc[X_train, \"target\"],\n    \"lgb\",\n    params[\"model_params\"],\n    params[\"fit_params\"],\n    cat_cols=cat_cols,\n    X_val=df.loc[\n        X_test.intersection(test_p_index),\n        gbdt_cols,\n    ].reset_index(drop=True),\n    y_val=df.loc[X_test.intersection(test_p_index), \"target\"].reset_index(drop=True),\n)\n\nres_pos_prev = run_gbdt_training(\n    df.loc[\n        X_train.intersection(test_pa_index),\n        gbdt_cols,\n    ].reset_index(drop=True),\n    df.loc[X_train.intersection(test_pa_index), \"target\"],\n    \"lgb\",\n    params[\"model_params\"],\n    params[\"fit_params\"],\n    cat_cols=cat_cols,\n    X_val=df.loc[\n        X_test.intersection(test_p_index),\n        gbdt_cols,\n    ].reset_index(drop=True),\n    y_val=df.loc[X_test.intersection(test_p_index), \"target\"].reset_index(drop=True),\n)\n\n\nfrom sklearn import metrics\n\nres_p = df3.loc[df3[\"fold\"] == \"fold3\", [\"is_there_purchase_any\", \"target\"]]\n\nres_p[\"all_model\"] = res.model.predict(\n    df3.query(\"fold == 'fold3'\")\n    .loc[\n        :,\n        ~df3.columns.isin(\n            idx_vars + [\"target\"] + drop_cols + previous_purchase_drop_cols\n        ),\n    ]\n    .reset_index(drop=True)\n)\n\nres_p.loc[:, \"only_pos_early\"] = res_prev.model.predict(\n    df3.query(\"fold == 'fold3' and is_there_purchase_any == [0, 1]\")\n    .loc[\n        :,\n        ~df3.columns.isin(\n            idx_vars + [\"target\"] + drop_cols + previous_purchase_drop_cols\n        ),\n    ]\n    .reset_index(drop=True)\n)\n\nres_p.loc[\n    res_p[\"is_there_purchase_any\"] == 1, \"only_pos_training\"\n] = res_pos_prev.predict(\n    df3.query(\"fold == 'fold3' and is_there_purchase_any == [1]\")\n    .loc[\n        :,\n        ~df3.columns.isin(\n            idx_vars + [\"target\"] + drop_cols + previous_purchase_drop_cols\n        ),\n    ]\n    .reset_index(drop=True)\n)\n\n\nprint(metrics.roc_auc_score(res_p[\"target\"], res_p[\"all_model\"]))\nprint(\n    metrics.roc_auc_score(\n        res_p.loc[res_p[\"is_there_purchase_any\"].isin([0, 1]), \"target\"],\n        res_p.loc[\n            res_p[\"is_there_purchase_any\"].isin([0, 1]),\n            [\"all_model\", \"only_pos_training\", \"only_pos_early\"],\n        ].max(axis=1),\n    )\n)\n","67b8f154":"## Ge\u00e7mi\u015f Veriyi de Kullanarak Tahmin","3cf6f7c8":"## Forward Selection - De\u011fi\u015fken \u00c7\u0131kar\u0131p Ekleyip Skoru Artt\u0131r\u0131p Artt\u0131rmamas\u0131na G\u00f6re De\u011fi\u015fken Se\u00e7ilimi\n","81d2dd8f":"## Son iki 3'er ayl\u0131k d\u00f6nemin haz\u0131rlanmas\u0131. ","29884bad":"def season_of_date(date_UTC):\n    year = str(date_UTC.year)\n    time = str(date_UTC.time())\n    seasons = {'spring': pd.date_range(start= year +'-03-21' + time, end=year + '-06-20' + time),\n               'summer': pd.date_range(start= year + '-06-21' + time, end= year + '-09-22' + time),\n               'autumn': pd.date_range(start= year + '-09-23' + time, end= year + '-12-20' + time)}\n    if date_UTC in seasons['spring']:\n        return 'spring'\n    if date_UTC in seasons['summer']:\n        return 'summer'\n    if date_UTC in seasons['autumn']:\n        return 'autumn'\n    else:\n        return 'winter'\n\nddt = sales.query(\"SF_CREATE_DATE < '2021-03-01'\").SF_CREATE_DATE.replace(\"\", np.nan).astype(\"datetime64[ns]\").map(season_of_date)\n\ngbdt_cols = gbdt_cols + [\"season_ord\"]\n\nsales[\"season\"] = (sales[\"SF_CREATE_DATE\"].dt.month % 12 + 3) \/\/ 3\n\nseasons = {1: \"Winter\", 2: \"Spring\", 3: \"Summer\", 4: \"Autumn\"}\n\nsales[\"season_name\"] = sales[\"season\"].map(seasons)","4d6dadd3":"## Test veri setinin haz\u0131rlanmas\u0131. ","34aea317":"## E\u011fitim veri setinin olu\u015fturulmas\u0131. ","8a191eb6":"## Ensemble Testleri ","98bc4e6e":"X_train_part = df3[df3[\"fold\"] == \"fold3\"].index\nX_test_part = df3[df3[\"fold\"] == \"fold4\"].index\n\n\nres = run_gbdt_training(\n    df3.loc[\n        X_train_part,\n        gbdt_cols,\n    ].reset_index(drop=True),\n    df3.loc[X_train_part, \"target\"],\n    \"lgb\",\n    params[\"model_params\"],\n    params[\"fit_params\"],\n    cat_cols=cat_cols,\n    X_val=df3.loc[\n        X_test_part,\n        gbdt_cols,\n    ].reset_index(drop=True),\n    y_val=df3.loc[X_test_part, \"target\"].reset_index(drop=True),\n)","19e67753":"res = run_gbdt_training(\n    df3.loc[\n        X_train_all,\n        ~df3.columns.isin(idx_vars + [\"target\"] + drop_cols),\n    ].reset_index(drop=True),\n    df3.loc[X_train_all, \"target\"],\n    \"lgb\",\n    params[\"model_params\"],\n    params[\"fit_params\"],\n    cat_cols=cat_cols,\n    X_val=df3.loc[\n        X_test_all,\n        ~df3.columns.isin(idx_vars + [\"target\"] + drop_cols),\n    ].reset_index(drop=True),\n    y_val=df3.loc[X_test_all, \"target\"].reset_index(drop=True),\n)","c08230c9":"print(metrics.roc_auc_score(res_pred[\"target\"], res_pred.loc[:, \"xgb\"]))\nprint(metrics.roc_auc_score(res_pred[\"target\"], res_pred.loc[:, \"lgb\"]))\nprint(metrics.roc_auc_score(res_pred[\"target\"], res_pred.loc[:, \"focal\"]))\nprint(metrics.roc_auc_score(res_pred[\"target\"], res_pred.loc[:, \"lgb_xgb_mean\"]))\nprint(metrics.roc_auc_score(res_pred[\"target\"], res_pred.loc[:, \"lgb_xgb_focal_mean\"]))\nprint(\n    metrics.roc_auc_score(\n        res_pred[\"target\"],\n        res_pred.loc[:, [\"lgb\", \"xgb\", \"focal\"]].apply(\n            lambda x: x[\"lgb\"] * 0.5 + x[\"focal\"] * 0.0 + x[\"xgb\"] * 0.5, axis=1\n        ),\n    )\n)\nprint(\n    metrics.roc_auc_score(\n        res_pred[\"target\"],\n        res_pred.loc[:, [\"lgb\", \"xgb\", \"focal\"]].apply(\n            lambda x: x[\"lgb\"] * 0.5 + x[\"focal\"] * 0.3 + x[\"xgb\"] * 0.2, axis=1\n        ),\n    )\n)","8934da18":"drop_cols_selected = []\nstats = []\ncache = []\n#cache = [col[0] for col in stats]\n\nall_cols = gbdt_cols\nfor col in all_cols:\n    res = run_gbdt_training(\n        df.loc[\n            X_train,\n            gbdt_cols,\n        ].reset_index(drop=True),\n        df.loc[X_train, \"target\"],\n        \"lgb\",\n        params[\"model_params\"],\n        params[\"fit_params\"],\n        cat_cols=cat_cols,\n        X_val=df.loc[\n            X_test,\n            gbdt_cols,\n        ].reset_index(drop=True),\n        y_val=df.loc[X_test, \"target\"].reset_index(\n            drop=True\n        ),\n    )\n\n    new_score = res.model.best_score[\"valid_1\"][\"auc\"]\n    if new_score > best_score:\n        drop_cols_selected.append(col)\n        print(\"*\")\n        print(col, new_score, best_score)\n        print(\"*\")\n    else:\n        print(col, new_score, best_score)\n\n    stats.append((col, new_score, best_score))","852eb634":"## Saf Veri Dosyalar\u0131n\u0131n \u0130\u015flenmesi & Test Verisindeki M\u00fc\u015fterilerin Filtrelenmesi & Basit Veri Manip\u00fclasyonu","fd102e0c":"## \u00d6ni\u015fleme i\u015flemleri ","3370b82a":"## Training yard\u0131mc\u0131l fonksiyonlar\u0131. ","3cd1a236":"## ","258f49de":"## Feature Engineering \u0130\u015flemleri (Her Kaynak i\u00e7in) ","ce7148a5":"## Feature Selection & Training ","b55a83ee":"## Forward Validation, 3 ayl\u0131k d\u00f6nemle e\u011fitilmi\u015f veri, sonraki 3 ayda ne performans veriyor?","b28ce6ca":"## Submission ","b896fb83":"## 3'er ayl\u0131k d\u00f6nemleri gezincek \u015fekilde CV foldlar\u0131n\u0131n \u00e7\u0131kar\u0131lmas\u0131. ","c65b609f":"## Son X g\u00fcnde sat\u0131\u015f ger\u00e7ekle\u015ftirdi mi de\u011fi\u015fkenlerinin t\u00fcretilmesi. ","aff14905":"print(\n    metrics.roc_auc_score(res_pred.loc[X_test, \"target\"], res_pred.loc[X_test, \"xgb\"])\n)\nprint(\n    metrics.roc_auc_score(res_pred.loc[X_test, \"target\"], res_pred.loc[X_test, \"lgb\"])\n)\nprint(\n    metrics.roc_auc_score(res_pred.loc[X_test, \"target\"], res_pred.loc[X_test, \"focal\"])\n)\nprint(\n    metrics.roc_auc_score(\n        res_pred.loc[X_test, \"target\"], res_pred.loc[X_test, \"lgb_xgb_mean\"]\n    )\n)\nprint(\n    metrics.roc_auc_score(\n        res_pred.loc[X_test, \"target\"], res_pred.loc[X_test, \"lgb_xgb_focal_mean\"]\n    )\n)\nprint(\n    metrics.roc_auc_score(\n        res_pred.loc[X_test, \"target\"],\n        res_pred.loc[X_test, [\"lgb\", \"xgb\", \"focal\"]].apply(\n            lambda x: x[\"lgb\"] * 0.8 + x[\"focal\"] * 0.0 + x[\"xgb\"] * 0.2, axis=1\n        ),\n    )\n)\nprint(\n    metrics.roc_auc_score(\n        res_pred.loc[X_test, \"target\"],\n        res_pred.loc[X_test, [\"lgb\", \"xgb\", \"focal\"]].apply(\n            lambda x: x[\"lgb\"] * 0.5 + x[\"focal\"] * 0.3 + x[\"xgb\"] * 0.2, axis=1\n        ),\n    )\n)","7025bc24":"## INSPECTION & TEST","822079ae":"## Sales Dosyas\u0131nda Bulunan & Bulunmayanlar i\u00e7in ayr\u0131 model stratejisi. (Pek i\u015fe yaramad\u0131)","cf497a73":"## LIGHTGBM TRAINING","f45f7a60":"## \u0130ki ayl\u0131k d\u00f6nemin birle\u015ftirilmesi. ","c45da10c":"## Son ay\u0131n al\u0131nmas\u0131 & training \/ test splitlerinin haz\u0131rlanmas\u0131. ","58332959":"res = run_gbdt_training(\n    df.loc[\n        X_train.intersection(df[df[\"is_there_purchase_any\"] == 1].index),\n        gbdt_cols,\n    ].reset_index(drop=True),\n    df.loc[X_train.intersection(df[df[\"is_there_purchase_any\"] == 1].index), \"target\"],\n    \"lgb\",\n    params[\"model_params\"],\n    params[\"fit_params\"],\n    cat_cols=cat_cols,\n    X_val=df.loc[\n        X_test.intersection(df[df[\"is_there_purchase_any\"] == 1].index),\n        gbdt_cols,\n    ].reset_index(drop=True),\n    y_val=df.loc[\n        X_test.intersection(df[df[\"is_there_purchase_any\"] == 1].index), \"target\"\n    ].reset_index(drop=True),\n)","382acd6b":"## Leakage yapmayacak \u015fekilde saf verilerin, belirli bir fold i\u00e7in filtrelenmesi.","987cb518":"## Sadece sales dosyas\u0131nda bulunan BASE_CUSTOMER_ID'ler aras\u0131nda model ","3407a5ff":"## Ge\u00e7mi\u015ften g\u00fcn\u00fcm\u00fcze 3 er ayl\u0131k d\u00f6nemlerin i\u015flenip, e\u011fitime haz\u0131r saklanmas\u0131. "}}