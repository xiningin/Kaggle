{"cell_type":{"ad290c3d":"code","eeeef73f":"code","22712f83":"code","74992240":"code","68a8ef6c":"code","a75f8e95":"code","65741e47":"code","442e3931":"code","4fb0d6e0":"code","8fb1dee7":"code","c71ed3e7":"code","72ddd0ed":"code","acfeda92":"code","ceac448e":"code","61b6913c":"code","16d46775":"code","54badf59":"code","70739c06":"code","8acac2a1":"code","7ea27cc5":"code","8a1fc1e9":"code","da3d32ad":"code","68e2f1b7":"code","90ab6195":"code","1787313e":"code","4a02f81e":"code","31051dda":"code","9f487e10":"code","368f3221":"code","4de7394d":"code","5b7ebd0a":"code","2529475d":"code","8521de85":"code","4b0424a1":"code","684ca504":"code","6b54b825":"code","6cb687d3":"code","af51723e":"code","d8ebaee9":"code","817ae772":"code","3c3e6f18":"code","53c9d03e":"code","46db0df6":"code","e1dac0c9":"code","c3f9149f":"code","de0708ee":"code","6a35e7e1":"code","a5280812":"code","dbefa637":"code","6fecf50e":"code","7f51a37a":"code","4caf62b2":"code","6f910497":"code","2ee09e60":"code","3daa38c3":"code","a9444d86":"code","53d33c4e":"code","4484c01d":"code","07a0cca2":"code","83ffe9cb":"code","45a815b3":"code","6858798e":"code","6c991726":"code","214ae838":"code","8fcb828d":"code","456b62a9":"code","e9179634":"code","b5b12fcc":"code","8e77500f":"code","27f2d3e5":"markdown","6570f7c2":"markdown","e72175a0":"markdown","4cee1c4e":"markdown","55a13bae":"markdown","843448f2":"markdown","88bda5fc":"markdown","9e5db544":"markdown","efb461a3":"markdown","aa53c46e":"markdown","06beedab":"markdown","d760d21f":"markdown","c6cc060b":"markdown","6920a0b9":"markdown","b27a86b9":"markdown","43bdf015":"markdown","d7c56ac5":"markdown","c0ae462c":"markdown","c95c3685":"markdown","622ac1c5":"markdown","425577df":"markdown","0e5adc1b":"markdown","eb938e77":"markdown"},"source":{"ad290c3d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport xgboost as xgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"whitegrid\")\nfrom statsmodels.distributions.empirical_distribution import ECDF\n\n# time logic:\nimport time\nimport datetime\nfrom datetime import timedelta\nfrom dateutil.relativedelta import relativedelta\n\n#sklearn\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.metrics import auc\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\nfrom sklearn.metrics import precision_recall_curve\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import preprocessing\n\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nimport pickle\n\n# Input data files are available in the \"..\/input\/\" directory.\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Standard plotly imports\nimport plotly.express as px","eeeef73f":"train_transaction = pd.read_csv('..\/input\/train_transaction.csv', index_col='TransactionID')\ntest_transaction = pd.read_csv('..\/input\/test_transaction.csv', index_col='TransactionID')\n\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv', index_col='TransactionID')\ntest_identity = pd.read_csv('..\/input\/test_identity.csv', index_col='TransactionID')\n\nsample_submission = pd.read_csv('..\/input\/sample_submission.csv', index_col='TransactionID')\n\n#merge the two separate CSV files:\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ntest = test_transaction.merge(test_identity, how='left', left_index=True, right_index=True)\n\nprint(train.shape)\nprint(test.shape)\n","22712f83":"train.head(5)","74992240":"#check how many transaction there are for fraud vs non-fraud\nfraud_non_fraud_df = train.groupby(by='isFraud').TransactionDT.count()\nfraud_non_fraud_df = pd.DataFrame(fraud_non_fraud_df)\nnum_transactions = fraud_non_fraud_df.sum()\nfraud_non_fraud_df['perc'] = fraud_non_fraud_df\/num_transactions*100\n\nprint(fraud_non_fraud_df)","68a8ef6c":"plt.figure(figsize=(16,6))\nplt.subplot(121)\nplt.pie(fraud_non_fraud_df.perc, colors=['g','r'],autopct='%.2f')\nplt.title('Number of fraud\/ non-fraud transactions')\nplt.xlabel('Fraud\/ non-fraud transaction')\nplt.ylabel('Number of transactions')","a75f8e95":"device_type_df = train.groupby(by='DeviceType').TransactionDT.count()\ndevice_type_df.plot(kind='pie',colors=['r','g'],autopct='%1.1f%%', shadow=True, startangle=140)\nplt.title('Number of transactions by device type')\nprint(device_type_df)","65741e47":"df_device_type_pivot = train.pivot_table(columns='DeviceType',index='isFraud',values='TransactionDT',aggfunc='count',margins=True)\npd.options.display.float_format = '{:.2f}'.format\ndf_device_type_pivot","442e3931":"#as percent of total number of transactions:\ndf_device_type_pivot_percent = df_device_type_pivot \/ 140810*100\n\ncm_green = sns.light_palette(\"green\", as_cmap=True)\ndf_device_type_pivot_percent.style.background_gradient(cmap=cm_green, subset=['desktop','mobile'])","4fb0d6e0":"perc_desktop_fraud = round(df_device_type_pivot['desktop'].loc[1] \/ df_device_type_pivot['desktop'].loc[0],5)\nperc_mobile_fraud  = round(df_device_type_pivot['mobile'].loc[1] \/ df_device_type_pivot['mobile'].loc[0],5)\n\nprint('% of fraud for desktop: ', perc_desktop_fraud)\nprint('% of fraud for mobile: ', perc_mobile_fraud)","8fb1dee7":"df_card_type = train[['isFraud','card4']].reset_index()\ndf_card_type.head(5)","c71ed3e7":"df_card_type_pivot = pd.pivot_table(data=df_card_type,index='card4', columns='isFraud',aggfunc='count')\ndf_card_type_pivot['perc'] = df_card_type_pivot.TransactionID[1]\/df_card_type_pivot.TransactionID[0]*100\ndf_card_type_pivot.sort_values(by='perc',inplace=True)\ndf_card_type_pivot","72ddd0ed":"plt.figure()\ndf_card_type_pivot.perc.plot(kind='bar',title='Percentage of fraudulent transactions by card type')\nplt.xticks(rotation=40)\nplt.xlabel('Card type')","acfeda92":"df_card_type_amount = train[['isFraud','card4','TransactionAmt']].reset_index(drop=True)\nprint(df_card_type_amount.head(5))\ndf_card_type_amount_pivot = pd.pivot_table(data=df_card_type_amount,index='card4', columns='isFraud',aggfunc='mean')\ndf_card_type_amount_pivot","ceac448e":"df_card_type_amount_pivot.plot(kind='bar',color=['g','r'],title='Avg.transaction amount for fraud\/non-fraud transactions')\nplt.ylabel('Avg transaction amount')\nplt.xticks(rotation=40)","61b6913c":"transaction_amts = train['TransactionAmt']\ntransaction_amts.describe()","16d46775":"print(transaction_amts.quantile(q=0.90))\nprint(transaction_amts.quantile(q=0.95))","54badf59":"transaction_amts_short = transaction_amts[transaction_amts< 275]\ntransaction_amts_short.hist(bins=50)\nplt.title('Histogram for transaction amount')","70739c06":"fig = plt.figure()\nplt.hist(transaction_amts_short, normed=True, cumulative=True, label='CDF',\n         histtype='bar', alpha=0.8, color='g',bins=50)\nplt.title('ECDF for transaction amount')\nplt.xlabel('Transaction amount')","8acac2a1":"#use plotly to make fancy graphs:\ntransaction_amts_short_df = pd.DataFrame(transaction_amts_short).reset_index(drop=True)\nprint(transaction_amts_short_df.head(5))\nfig = px.histogram(transaction_amts_short_df,x='TransactionAmt',nbins=50)\nfig.update_layout(\n    autosize=False,\n    width=500,\n    height=500,\n    paper_bgcolor=\"LightSteelBlue\",\n)\nfig.show()\nprint('Done')","7ea27cc5":"import plotly.graph_objects as go\nprint(transaction_amts_short_df.head(10))\nfig = go.Figure(data=[go.Histogram(x=transaction_amts_short_df.TransactionAmt, \n                                   cumulative_enabled=True,\n                                   histnorm='percent',\n                                  nbinsx=200)])\nfig.update_layout(\n    autosize=False,\n    width=500,\n    height=500,\n    title_text='ECDF for transaction amounts under 275 USD', # title of plot\n    xaxis_title_text='Transaction amount', # xaxis label\n    yaxis_title_text='Count', # yaxis label\n    bargap=0.2, # gap between bars of adjacent location coordinates\n    bargroupgap=0.1 # gap between bars \n)\n\nfig.show()","8a1fc1e9":"print(train.head(5))\ntrain.describe()","da3d32ad":"data_raw = train.drop(columns=['isFraud']).reset_index(drop=True)\ntarget_raw = train[['isFraud']].reset_index(drop=True)","68e2f1b7":"#list all columns to get an idea what we could work with\nall_cols = data_raw.columns.values\nprint(all_cols)","90ab6195":"data_raw.dtypes","1787313e":"#check the column \ndata_raw[[ 'V219', 'V220', 'V221', 'V222', 'V223', 'V224', 'V225', 'V226',\n       'V227', 'V228', 'V229', 'V230', 'V231', 'V232', 'V233', 'V234',\n       'V235', 'V236', 'V237', 'V238', 'V239', 'V240', 'V241', 'V242',\n       'V243', 'V244', 'V245', 'V246', 'V247', 'V248', 'V249', 'V250',\n       'V251', 'V252', 'V253', 'V254', 'V255', 'V256', 'V257', 'V258',\n       'V259', 'V260', 'V261', 'V262', 'V263', 'V264', 'V265', 'V266',\n       'V267', 'V268', 'V269', 'V270', 'V271', 'V272', 'V273', 'V274',\n       'V275', 'V276', 'V277', 'V278', 'V279', 'V280', 'V281', 'V282',\n       'V283', 'V284', 'V285', 'V286', 'V287', 'V288', 'V289', 'V290',\n       'V291', 'V292', 'V293', 'V294', 'V295', 'V296', 'V297', 'V298',\n       'V299', 'V300', 'V301', 'V302', 'V303', 'V304', 'V305', 'V306',\n       'V307', 'V308', 'V309', 'V310', 'V311', 'V312', 'V313', 'V314',\n       'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321', 'V322',\n       'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V329', 'V330',\n       'V331', 'V332', 'V333', 'V334', 'V335', 'V336', 'V337', 'V338',\n       'V339', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06',\n       'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13',\n       'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20',\n       'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27',\n       'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34',]].describe().transpose()","4a02f81e":"data_v1 = data_raw\ndata_v1.shape","31051dda":"from sklearn.preprocessing import LabelEncoder\n\ncat_cols = ['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29',\n            'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'ProductCD', 'card4', 'card6', 'M4','P_emaildomain',\n            'R_emaildomain', 'card1', 'card2', 'card3',  'card5', 'addr1', 'addr2', 'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9',\n            'P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3', 'R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']\nfor col in cat_cols:\n    if col in data_v1.columns:\n        le = LabelEncoder()\n        le.fit(list(data_v1[col].astype(str).values) + list(test[col].astype(str).values))\n        data_v1[col] = le.transform(list(data_v1[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values)) ","9f487e10":"data_v1.drop(columns=['TransactionDT'],axis=1, inplace=True)","368f3221":"data_v1_desc = data_v1.describe()\ndata_v1_desc","4de7394d":"data_v1_coeffVars = data_v1_desc.loc['std'] \/ data_v1_desc.loc['mean']\ndata_v1_coeffVars = data_v1_coeffVars.sort_values(ascending=False)\nprint(data_v1_coeffVars.head(5))\n\nplt.figure(figsize=(40,15))\ndata_v1_coeffVars.plot(kind='barh')\nplt.axvline(x=0.1, color='k', linestyle='--',label='Potential cutoff variance threshold')\nplt.legend()\nplt.title","5b7ebd0a":"coeff_var_threshold = 0.5\ndata_v1_coeffVars_more5 = data_v1_coeffVars[data_v1_coeffVars>coeff_var_threshold]\nlen(data_v1_coeffVars_more5)","2529475d":"data_v1a = data_v1[data_v1_coeffVars_more5.index]\ndata_v1a.shape","8521de85":"#check the variance of data for each column:\ndata_v1_vars = data_v1.describe().loc['std'].sort_values(ascending=False)\nplt.figure(figsize=(20,15))\ndata_v1_vars.plot(kind='barh')\nplt.axvline(x=0.1, color='k', linestyle='--',label='Potential cutoff variance threshold')\nprint(data_v1_vars.head(20))","4b0424a1":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data_v1a, target_raw, test_size=0.15, random_state=42)\nprint('X_train shape: ', X_train.shape)\nprint('X_test shape: ', X_test.shape)","684ca504":"#new model:\n#import sys\nprint(datetime.datetime.now())\nxgboost_model_v1a = XGBClassifier()\nxgboost_model_v1a.fit(X_test, y_test)\nprint(datetime.datetime.now())","6b54b825":"#set up functions to plot confusion matrix and precision recall curve\ndef print_confusion_matrix(\n    confusion_matrix, class_names, figsize=(3, 2.5), fontsize=14\n):\n    \"\"\"Prints a confusion matrix, as returned by sklearn.metrics.confusion_matrix, as a heatmap.\n    \"\"\"\n    df_cm = pd.DataFrame(confusion_matrix, index=class_names, columns=class_names)\n    fig = plt.figure(figsize=figsize)\n    try:\n        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n    except ValueError:\n        raise ValueError(\"Confusion matrix values must be integers.\")\n    heatmap.yaxis.set_ticklabels(\n        heatmap.yaxis.get_ticklabels(), rotation=0, ha=\"right\", fontsize=fontsize\n    )\n    heatmap.xaxis.set_ticklabels(\n        heatmap.xaxis.get_ticklabels(), rotation=45, ha=\"right\", fontsize=fontsize\n    )\n    plt.ylabel(\"True label\")\n    plt.xlabel(\"Predicted label\")\n    plt.title(\"Confusion matrix for classifier:\")\n    # fig.colorbar(shrink=0.8)\n    #return fig\n    \ndef plot_precision_recall_curve(\n    y_test, y_pred_proba_df, title=\"Precision\/Recall Curve\"\n):\n    \"\"\"\n    feed two arrays and output precision recall Curve\n    \"\"\"\n    precision, recall, _ = precision_recall_curve(y_test, y_pred_proba_df)\n    # In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n    #step_kwargs = ({\"step\": \"post\"} if \"step\" in signature(plt.fill_between).parameters else {})\n    plt.step(recall, precision, color=\"b\", alpha=0.5, where=\"post\")\n    #plt.fill_between(recall, precision, alpha=0.2, color=\"b\", **step_kwargs)\n\n    plt.xlabel(\"Recall\")\n    plt.ylabel(\"Precision\")\n    plt.title(title)\n    plt.show()\n    return plt","6cb687d3":"#test the xgboost model:\ny_pred_xgboost_v1 = pd.DataFrame(xgboost_model_v1a.predict_proba(X_test))\ny_pred_xgboost_v1 = y_pred_xgboost_v1[1]\nplot_precision_recall_curve(y_test=y_test,y_pred_proba_df=y_pred_xgboost_v1)","af51723e":"from sklearn.feature_selection import VarianceThreshold\n#Remove all the features with low variance:\ndef variance_threshold_selector(data, threshold=0.01):\n    \"\"\"\n    applies a variance threshold to a dataframe and returns the dataframe instead of array\n    \"\"\"\n    selector = VarianceThreshold(threshold)\n    selector.fit(data)\n    return data[data.columns[selector.get_support(indices=True)]]\n\ndef apply_std_threshold(data,std_threshold = 0.1):\n    '''removes all the columns with standard deviation under specific value'''\n    data_dropped_low_variance = variance_threshold_selector(data,threshold=std_threshold)\n    print(data_dropped_low_variance.shape)\n    #print(type(data_dropped_low_variance))\n    return data_dropped_low_variance\n\n\ndef plot_roc_curve(fpr, tpr):\n    plt.plot(fpr, tpr, color='orange', label='ROC')\n    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend()\n    plt.show()","d8ebaee9":"auc = roc_auc_score(y_test, y_pred_xgboost_v1)\nprint('AUC: %.5f' % auc)\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_xgboost_v1)\nplot_roc_curve(fpr, tpr)","817ae772":"cols_to_include = ['TransactionAmt', 'ProductCD', 'card1', 'card2',\n       'card3', 'card4', 'card5', 'card6', 'addr1', 'addr2', 'dist1',\n       'dist2', 'P_emaildomain', 'R_emaildomain','DeviceType']","3c3e6f18":"data_filtered_1 = data_raw[cols_to_include]\ndata_filtered_1.head(5)","53c9d03e":"#define which columns are to be treated as numberic and which ones as categorical\nlist_categorical_cols = ['ProductCD','card4','card6','P_emaildomain','R_emaildomain','DeviceType']\nlist_numerical_cols   = [column for column in cols_to_include if column not in list_categorical_cols]\nprint('Categorical cols: ', list_categorical_cols)\nprint('Numberic cols: ', list_numerical_cols)","46db0df6":"# TODO get this working:\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit_transform(data_filtered_1[list_numerical_cols])\ndata_filtered_1.head(5)","e1dac0c9":"print(data_filtered_1[list_numerical_cols].head(5))\nscaled_data = scaler.fit_transform(data_filtered_1[list_numerical_cols])\nscaled_data_df = pd.DataFrame(scaled_data)\nprint(scaled_data_df.head(5))","c3f9149f":"data_filtered_2 = data_filtered_1.merge(scaled_data_df,left_on=data_filtered_1.index, right_on = scaled_data_df.index)\ndata_filtered_2.drop(columns=list_numerical_cols,inplace=True)\ndata_filtered_2.drop(columns='key_0',inplace=True)\ndata_filtered_2.head(5)","de0708ee":"#make dummy columns with categorical cols:\ndata_filtered_catCols = pd.get_dummies(data_filtered_2, columns = list_categorical_cols)\ndata_filtered_catCols.head(5)","6a35e7e1":"#get rid of the NaN in the data:\ndata_filtered_catCols_woNa = data_filtered_catCols.fillna(data_filtered_catCols.mean())\ndata_filtered_catCols_woNa.head(5)","a5280812":"#check if any value in the dataframe is NaN:\nif (data_filtered_catCols_woNa.isnull().values.any()) == False:\n    print('No NaN values found in dataframe, all good.')\nelse:\n    print('You have NaN values in the dataframe that need to be filled first.')","dbefa637":"#check the variance of data for each column:\ndata_vars = data_filtered_catCols_woNa.describe().loc['std'].sort_values(ascending=False)\nplt.figure(figsize=(20,15))\ndata_vars.plot(kind='barh')\nplt.axvline(x=0.1, color='k', linestyle='--',label='Potential cutoff variance threshold')\nprint(data_vars.head(20))","6fecf50e":"actual_var_threshold = 0.02\ndata_low_var = apply_std_threshold(data_filtered_catCols_woNa,std_threshold = actual_var_threshold)\nprint(data_low_var.columns.values)","7f51a37a":"#check the data to be used in training:\ndata_filtered_catCols_woNa.head(5)","4caf62b2":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(data_filtered_catCols_woNa, target_raw, test_size=0.15, random_state=42)\nprint('X_train shape: ', X_train.shape)\nprint('X_test shape: ', X_test.shape)","6f910497":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_classification","2ee09e60":"print('Number of datapoints: ', data_filtered_catCols_woNa.shape[0])","3daa38c3":"clf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\nclf.fit(data_filtered_catCols_woNa, target_raw)  \n#print(clf.feature_importances_)","a9444d86":"#test the model performance:\ny_pred = clf.predict_proba(X_test)\nprint(y_pred[0:5])\nprint(y_pred.min())\nprint(y_pred.max())","53d33c4e":"#make a dataframe out of the results and select the first column\ny_pred_df = pd.DataFrame(y_pred)\ny_pred_series = y_pred_df[1]\nprint(y_pred_series[0:10])\nprint(y_pred_series.min())\nprint(y_pred_series.max())","4484c01d":"def round_up_or_down(x, threshold=0.08):\n    if x > threshold:\n        x = 1\n    else:\n        x =0\n    return x\ny_pred_custom = y_pred_series.apply(round_up_or_down)\ny_pred_custom[0:10]","07a0cca2":"from sklearn.metrics import confusion_matrix\nconf_matrix_1 = confusion_matrix(y_test, y_pred_custom)\nprint_confusion_matrix(conf_matrix_1,class_names = ['Non-Fraud','Fraud'])","83ffe9cb":"\n\nauc = roc_auc_score(y_test, y_pred_series)\nprint('AUC: %.5f' % auc)\nplot_precision_recall_curve(y_test=y_test,y_pred_proba_df=y_pred_series)","45a815b3":"param_grid_test = { \n    'n_estimators': [5],\n    'max_features': ['auto'],\n    'max_depth' : [2],\n    'criterion' :['gini']\n}\nparam_grid_1 = { \n    'n_estimators': [100,200],\n    'max_features': ['auto'],\n    'max_depth' : [4,5,6],\n    'criterion' :['gini']\n}\n\nparam_grid_2 = { \n    'n_estimators': [10,20,50,100,200,300,400,500],\n    'max_features': ['auto'],\n    'max_depth' : np.arange(2,20,2),\n    'criterion' :['gini']\n}","6858798e":"\n\ndef run_model_RandomForest(data, target, param_grid):\n    \"\"\"set the structure of the model and transform the target column\"\"\"\n    rfc = RandomForestClassifier(random_state=42)\n    CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=5)\n    CV_rfc.fit(data, target.values.ravel())\n    return CV_rfc\n\nprint(datetime.datetime.now())\nCV_rfc = run_model_RandomForest(data_filtered_catCols_woNa, target_raw,param_grid_test)\nprint(datetime.datetime.now())","6c991726":"#test the model:\ny_pred_randForest = pd.DataFrame(CV_rfc.predict_proba(X_test))\ny_pred_randForest = y_pred_randForest[1]","214ae838":"plot_precision_recall_curve(y_test=y_test,y_pred_proba_df=y_pred_randForest)","8fcb828d":"#check if pickled model is available\nos.listdir()\n#load the model again:\ntry:\n    model_filename = 'xgboost_v1.joblib.dat'\n    loaded_model = pickle.load(open(model_filename, \"rb\"))\n    print('Loaded model...')\nexcept Exception as e:\n    print(e)","456b62a9":"#new model:\n#import sys\nprint(datetime.datetime.now())\nxgboost_model = XGBClassifier()\nxgboost_model.fit(data_filtered_catCols_woNa, target_raw)\nprint(datetime.datetime.now())","e9179634":"#test the xgboost model:\ny_pred_xgboost = pd.DataFrame(xgboost_model.predict_proba(X_test))\ny_pred_xgboost = y_pred_xgboost[1]\nplot_precision_recall_curve(y_test=y_test,y_pred_proba_df=y_pred_xgboost)","b5b12fcc":"#save the model:\n\nmodel_filename = 'xgboost_v1.joblib.dat'\npickle.dump(xgboost_model, open(model_filename, \"wb\"))\nos.listdir()","8e77500f":"auc = roc_auc_score(y_test, y_pred_xgboost)\nprint('AUC: %.5f' % auc)\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_xgboost)\nplot_roc_curve(fpr, tpr)","27f2d3e5":"The first iteration is obviously not very good with standard parameters, so GridsearchCV will probably get better results with better hyperparameters:","6570f7c2":"### Transaction amount distribution:","e72175a0":"## Machine Learning part:","4cee1c4e":"How many transactions are made on mobile vs desktop?","55a13bae":"-> We have round 20k fraud transactions and around 570k non-fraud transactions (class imbalance.)","843448f2":"**Version 2**: Start with a few columns that are easy to interpret first and check if there is any benefit to using them already:","88bda5fc":"## EDA:","9e5db544":"Seems like 'Discover' cards have both higher average transaction amount for all transactions, but also fraudulent transactions seem to be for higher amounts.","efb461a3":"Calculate coefficient of variation (std \/ mean):","aa53c46e":"Joining all the separate CSV files into one dataframe:","06beedab":"**Which type of cards are mostly affected by fraud?**\nThe first approach is to only look at the number of transactions, the second approach is to look at the Transaction amount of the transactions.","d760d21f":"**Version 1**** of Feature Engineering: Only Labelencoding and variance selection.","c6cc060b":"Use some more feature engineering for training data:","6920a0b9":"Proportionally, there seem to be **more fraud transactions on mobile than desktop,** considering around 39.5% of transaction \nare on mobile but the percentage of fraud transactions are both around 4% for mobile and desktop.","b27a86b9":"Use Plotly to make some graphs:","43bdf015":"A coefficient of variation of ~5 (meaning columns with at least standard deviations of at least 5 times the mean):","d7c56ac5":"The problem can be classified as a binary classification problem with class imbalance. Suitable models could be Random Forest\/ XGBoost. We have categorical columns which will be 'spread out' to dummy columns and numeric columns, which can be scaled.","c0ae462c":"Try with **XGBoost model:**","c95c3685":"90% of transactions are under 275 USD, to make distribution more visible, we can filter for that transaction amount.","622ac1c5":"Now to applying the actual model.\nTry with Random Forest Classifier:","425577df":"Seems like mobile transactions are more prone to fraud than desktop. The common lack of anti-virus software on mobile devices would support this claim...","0e5adc1b":"## Description of the IEEE Fraud project:","eb938e77":"The idea of this project is to analyse the transaction history taken from the dataset of researchers from the IEEE Computational Intelligence Society (IEEE-CIS).\n\nBased on that data, the goal is then to predict wether a transaction is likely to be fraudulent or not."}}