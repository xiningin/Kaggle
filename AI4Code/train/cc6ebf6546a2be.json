{"cell_type":{"e037676d":"code","bb7a40b2":"code","f6f56d64":"code","98be31aa":"code","daf61e41":"code","f6ac6dfd":"code","4e64d04d":"code","64166881":"code","c5ac5416":"code","7db7361a":"code","cbda7e7a":"code","1a19fd46":"code","c05dcee7":"code","dceb06a9":"code","c801acec":"code","b01f5e56":"code","3562b83b":"code","a912ebd6":"code","2bf3a7ed":"code","b7a0c21b":"code","9d9e3dc9":"code","a42da403":"code","ee66bf20":"code","246d8add":"markdown","be261ee2":"markdown","7276a424":"markdown"},"source":{"e037676d":"import pandas as pd\nimport numpy as np\nimport seaborn as sn","bb7a40b2":"import numpy as np\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold\n\ndef light_gbm_try(X,y,X_test):\n    kf = KFold(n_splits=200,shuffle=True,random_state=42)\n\n    predictions_lgb=pd.DataFrame()\n    preds = pd.DataFrame()\n\n    f1=[]\n    i=1\n    importances=pd.DataFrame()\n    importances['Features']=X.columns\n    for a,b in kf.split(X,y):\n        X_tr=X.iloc[a,:]\n        X_te=X.iloc[b,:]\n        y_train=y[a]\n        y_test=y[b]\n\n        train_data=lgb.Dataset(X_tr, y_train)\n        test_data=lgb.Dataset(X_te, y_test, reference=train_data)\n\n        print('---------- Training fold N\u00ba {} ----------'.format(i))\n\n        params = {'num_leaves': 7,\n             'min_data_in_leaf': 20,\n             'objective': 'binary',\n             'max_depth': 20,\n             #'colsample_bytree':0.2,\n             'learning_rate': 0.1,\n             'boosting': 'gbdt',\n             'bagging_freq': 5,\n             'bagging_fraction': 1,\n             'feature_fraction': 1,\n             'bagging_seed': 11,\n             'random_state': 42,\n             'metric': 'binary_logloss',\n             'verbosity': -1,}\n\n        model = lgb.train(params,train_data,num_boost_round=10000,valid_sets = [test_data],\n                          verbose_eval=1000,early_stopping_rounds = 300)\n        #model.fit(X=X_train,y=y_train,eval_set=(X_test,y_test),verbose=100,early_stopping_rounds=300)\n        predictions_lgb[str(i)]=model.predict(X_test,num_iterations=model.best_iteration)\n\n        name = 'importance_'+str(i)\n        importances[name]=model.feature_importance()\n\n        f1.append(list(model.best_score.items())[0][1]['binary_logloss'])\n\n        i+=1\n        #gc.collect()\n    print('MEAN F1 LIGHTGBM: {}'.format(np.mean(f1)))\n        \n    return np.mean(f1), predictions_lgb, importances","f6f56d64":"!ls","98be31aa":"#Data Section 1 \nTourney_Results=pd.read_csv('..\/input\/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament\/WDataFiles_Stage1\/WNCAATourneyCompactResults.csv')\nsub=pd.read_csv('..\/input\/google-cloud-ncaa-march-madness-2020-division-1-womens-tournament\/WSampleSubmissionStage1_2020.csv')","daf61e41":"Tourney_Results.head()","f6ac6dfd":"winers=Tourney_Results[['WTeamID','WScore']].rename(columns={'WTeamID':'Team','WScore':'Score'})\nlosers=Tourney_Results[['LTeamID','LScore']].rename(columns={'LTeamID':'Team','LScore':'Score'})","4e64d04d":"total_scores=pd.concat([winers,losers])\ntotal_scores.head()","64166881":"aggs=['mean','max','min']","c5ac5416":"scores_agg=total_scores.groupby('Team').agg(aggs)\nscores_agg.columns = scores_agg.columns.map('_'.join)\nscores_agg=scores_agg.reset_index()\nscores_agg.head()","7db7361a":"train=Tourney_Results[['Season','WTeamID','LTeamID']].copy()\ntrain.rename(columns={'WTeamID':'Team1','LTeamID':'Team2'},inplace=True)\ntrain['Pred']=1\ntrain.head()","cbda7e7a":"train=pd.merge(train,scores_agg,how='left',left_on='Team1',right_on='Team').drop('Team',axis=1)\ntrain.rename(columns={'Score_mean':'Score_mean1','Score_max':'Score_max1','Score_min':'Score_min1'},inplace=True)\ntrain=pd.merge(train,scores_agg,how='left',left_on='Team2',right_on='Team').drop('Team',axis=1)\ntrain.rename(columns={'Score_mean':'Score_mean2','Score_max':'Score_max2','Score_min':'Score_min2'},inplace=True)\ntrain.head()","1a19fd46":"train_loser=train.copy()\ntrain_loser['Team1']=train['Team2'].copy()\ntrain_loser['Team2']=train['Team1'].copy()\ntrain_loser['Score_mean1']=train['Score_mean2'].copy()\ntrain_loser['Score_max1']=train['Score_max2'].copy()\ntrain_loser['Score_min1']=train['Score_min2'].copy()\ntrain_loser['Score_mean2']=train['Score_mean1'].copy()\ntrain_loser['Score_max2']=train['Score_max1'].copy()\ntrain_loser['Score_min2']=train['Score_min1'].copy()\ntrain_loser['Pred']=0\ntrain_loser.head()","c05dcee7":"sub = pd.concat([sub, sub['ID'].str.split('_', expand=True).rename(columns={0: 'Season', 1: 'Team1', 2: 'Team2'}).astype(np.int64)], axis=1)\nsub.head()","dceb06a9":"sub=pd.merge(sub,scores_agg,how='left',left_on='Team1',right_on='Team').drop('Team',axis=1)\nsub.rename(columns={'Score_mean':'Score_mean1','Score_max':'Score_max1','Score_min':'Score_min1'},inplace=True)\nsub=pd.merge(sub,scores_agg,how='left',left_on='Team2',right_on='Team').drop('Team',axis=1)\nsub.rename(columns={'Score_mean':'Score_mean2','Score_max':'Score_max2','Score_min':'Score_min2'},inplace=True)\nsub.head()","c801acec":"sub.Season.unique()","b01f5e56":"train.Season.unique()","3562b83b":"train=pd.concat([train,train_loser],axis=0)\ntrain=train[train['Season']<2015] \n","a912ebd6":"train.shape","2bf3a7ed":"X=train.drop('Pred',axis=1).reset_index().drop('index',axis=1)\na,b,c=light_gbm_try(X,y=train.Pred.values,X_test=sub.drop(['Pred','ID'],axis=1))","b7a0c21b":"sub['Pred']=b.mean(axis=1)\nsub.drop(['Season','Team1','Team2'],axis=1,inplace=True)\nsub.head()","9d9e3dc9":"sub.to_csv('try_taco_alone_resetdrop.csv',index=False)","a42da403":"c['importance']=c.drop('Features',axis=1).mean(axis=1)","ee66bf20":"c[['Features','importance']].sort_values(by='importance',ascending=False)","246d8add":"0.5308350596218261  0.61052\n0.4552870957695052 agg teams score","be261ee2":"# Hi, I would like to know if this approach is correct. I erased data from 2015 to 2020 in the tourney results (training data) since I think that the test data has seasons from 2015 onwards. So, if I don't do this, the training data would have data from the future.","7276a424":"Since the test data has seasons from 2015 to 2019, I consider that in the train data the seasons have to be before 2015. If not, I think \nwe are including future data in our trainset. At least, this consideration is valid in the first phase of the competition."}}