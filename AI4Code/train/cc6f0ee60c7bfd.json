{"cell_type":{"76c8f92a":"code","ef686e71":"code","89643daa":"code","d37e2d31":"code","53c25ee3":"code","2e5ec884":"code","3c383c74":"code","988289c4":"code","d3a16513":"code","bc80e2b7":"code","5ef8175e":"code","985bafe5":"code","1a0b0728":"code","5a51ef88":"code","d158ea74":"code","9bf399e0":"code","5120c113":"code","143b3d3f":"code","1ced88f7":"code","9290b44d":"markdown","e70b55d2":"markdown","e8439db0":"markdown","2cf529fe":"markdown","31cff11d":"markdown","868134ac":"markdown","8ec1694c":"markdown","5bd9939e":"markdown","722f77bc":"markdown","9943b058":"markdown","0bd37e5f":"markdown","8762c982":"markdown","e47f4b94":"markdown","b63fb841":"markdown","ebb718a3":"markdown","4586baec":"markdown","66942ac2":"markdown"},"source":{"76c8f92a":"import tensorflow as tf\ntry:\n   tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\nexcept ValueError:\n   tpu = None\nif tpu:\n   tf.config.experimental_connect_to_cluster(tpu)\n   tf.tpu.experimental.initialize_tpu_system(tpu)\n   strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n   strategy = tf.distribute.get_strategy()","ef686e71":"!pip install git+https:\/\/github.com\/ssut\/py-googletrans.git\n!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly  --apt-packages libomp5 libopenblas-dev","89643daa":"%%time\n%autosave 60\n\nimport os\nos.environ['XLA_USE_BF16'] = \"1\"\nos.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n\nimport gc\ngc.enable()\nimport time\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm \nfrom googletrans import Translator\nfrom dask import bag, diagnostics\n\nimport transformers\nfrom transformers import (AdamW, \n                          DistilBertTokenizer, \n                          DistilBertModel, \n                          DistilBertTokenizerFast,                          \n                          get_cosine_schedule_with_warmup)\nfrom tokenizers import BertWordPieceTokenizer\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.distributed import DistributedSampler\n\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.utils.serialization as xser\nimport torch_xla.version as xv\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint('PYTORCH:', xv.__torch_gitrev__)\nprint('XLA:', xv.__xla_gitrev__)","d37e2d31":"train = pd.read_csv('..\/input\/contradictory-my-dear-watson\/train.csv')\ntest = pd.read_csv('..\/input\/contradictory-my-dear-watson\/test.csv')\nsample_submission = pd.read_csv('..\/input\/contradictory-my-dear-watson\/sample_submission.csv')","53c25ee3":"def translate(words):\n    translator = Translator()\n    decoded = translator.translate(words, dest='en').text\n    return decoded\n\nother_langs = train.loc[train.lang_abv != \"en\"].copy()\n\n#TODO: use a dask dataframe instead of bags\npremise_bag = bag.from_sequence(other_langs.premise.tolist()).map(translate)\nhypo_bag =  bag.from_sequence(other_langs.hypothesis.tolist()).map(translate)\nwith diagnostics.ProgressBar():\n    premises = premise_bag.compute()\n    hypos = hypo_bag.compute()\n    \n    \nother_langs[['premise', 'hypothesis']] = list(zip(premises, hypos))\ntrain = train.append(other_langs)\ntrain.shape","2e5ec884":"class DatasetRetriever(Dataset):\n    def __init__(self, df, ids, mask):\n        self.df = df\n        self.ids = ids\n        self.mask = mask\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):   \n        ids = self.ids[index]\n        mask = self.mask[index]\n        targets = self.df.iloc[index].label\n        return {\n            'ids':torch.tensor(ids),\n            'mask':torch.tensor(mask),\n            'targets':targets\n        }","3c383c74":"class DistillBERT(nn.Module):\n    def __init__(self, num_labels, multisample):\n        super(DistillBERT, self).__init__()\n        output_hidden_states = True\n        self.num_labels = num_labels\n        self.multisample= multisample\n        self.distillbert = DistilBertModel.from_pretrained(\"distilbert-base-multilingual-cased\", \n                                                           output_hidden_states=output_hidden_states,\n                                                           num_labels=1)\n        self.layer_norm = nn.LayerNorm(768*2)\n        self.dropout = nn.Dropout(p=0.2)\n        self.high_dropout = nn.Dropout(p=0.5)        \n        self.classifier = nn.Linear(768*2, self.num_labels)\n    \n    def forward(self,\n        input_ids=None,\n        attention_mask=None,\n        head_mask=None,\n        inputs_embeds=None):\n        outputs = self.distillbert(input_ids,\n                                   attention_mask=attention_mask,\n                                   head_mask=head_mask,\n                                   inputs_embeds=inputs_embeds)\n        average_pool = torch.mean(outputs[0], 1)\n        max_pool, _ = torch.max(outputs[0], 1)\n        concatenate_layer = torch.cat((average_pool, max_pool), 1)\n        normalization = self.layer_norm(concatenate_layer)\n        if self.multisample:\n            # Multisample Dropout\n            logits = torch.mean(\n                torch.stack(\n                    [self.classifier(self.dropout(normalization)) for _ in range(5)],\n                    dim=0,\n                ),\n                dim=0,\n            )\n        else:\n            logits = self.dropout(normalization)\n            logits = self.classifier(logits)       \n        outputs = F.log_softmax(logits, dim=1)\n        return outputs  ","988289c4":"class AverageMeter(object):\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\nclass ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def display(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        print('\\t'.join(entries))\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches \/\/ 1))\n        fmt = '{:' + str(num_digits) + 'd}'\n        return '[' + fmt + '\/' + fmt.format(num_batches) + ']'\n\ndef accuracy(output, target, topk=(1,)):\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 \/ batch_size))\n        return res","d3a16513":"def get_model_optimizer(model):\n    # Differential Learning Rate\n    def is_backbone(name):\n        return \"distillbert\" in name\n    \n    optimizer_grouped_parameters = [\n       {'params': [param for name, param in model.named_parameters() if is_backbone(name)], 'lr': LR},\n       {'params': [param for name, param in model.named_parameters() if not is_backbone(name)], 'lr': 1e-3} \n    ]\n    \n    optimizer = AdamW(\n        optimizer_grouped_parameters, lr=LR, weight_decay=1e-2\n    )\n    \n    return optimizer","bc80e2b7":"def loss_fn(outputs, targets):\n    return nn.NLLLoss()(outputs, targets)","5ef8175e":"def train_loop_fn(train_loader, model, optimizer, device, scheduler, epoch=None):\n    # Train\n    batch_time = AverageMeter('Time', ':6.3f')\n    data_time = AverageMeter('Data', ':6.3f')\n    losses = AverageMeter('Loss', ':.4e')\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    progress = ProgressMeter(\n        len(train_loader),\n        [batch_time, data_time, losses, top1],\n        prefix=\"[xla:{}]Train:  Epoch: [{}]\".format(xm.get_ordinal(), epoch)\n    )\n    model.train()\n    end = time.time()\n    for i, data in enumerate(train_loader):\n        data_time.update(time.time()-end)\n        ids = data[\"ids\"]\n        mask = data[\"mask\"]\n        targets = data[\"targets\"]\n        ids = ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n        optimizer.zero_grad()\n        outputs = model(\n            input_ids = ids,\n            attention_mask = mask\n        )\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        xm.optimizer_step(optimizer)\n        loss = loss_fn(outputs, targets)\n        acc1= accuracy(outputs, targets, topk=(1,))\n        losses.update(loss.item(), ids.size(0))\n        top1.update(acc1[0].item(), ids.size(0))\n        scheduler.step()\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if i % 30 == 0:\n            progress.display(i)\n    del loss\n    del outputs\n    del ids\n    del mask\n    del targets\n    gc.collect()","985bafe5":"def eval_loop_fn(validation_loader, model, device):\n    #Validation\n    model.eval()\n    batch_time = AverageMeter('Time', ':6.3f')\n    losses = AverageMeter('Loss', ':.4e')\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    learning_rate = AverageMeter('LR',':2.8f')\n    progress = ProgressMeter(\n        len(validation_loader),\n        [batch_time, losses, top1],\n        prefix='[xla:{}]Validation: '.format(xm.get_ordinal()))\n    with torch.no_grad():\n        end = time.time()\n        for i, data in enumerate(validation_loader):\n            ids = data[\"ids\"]\n            mask = data[\"mask\"]\n            targets = data[\"targets\"]\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n            outputs = model(\n                input_ids = ids,\n                attention_mask = mask\n            )\n            loss = loss_fn(outputs, targets)\n            acc1= accuracy(outputs, targets, topk=(1,))\n            losses.update(loss.item(), ids.size(0))\n            top1.update(acc1[0].item(), ids.size(0))\n            batch_time.update(time.time() - end)\n            end = time.time()\n            if i % 10 == 0:\n                progress.display(i)\n    del loss\n    del outputs\n    del ids\n    del mask\n    del targets\n    gc.collect()","1a0b0728":"def fast_encode(df, fast_tokenizer):\n    fast_tokenizer.enable_truncation(max_length=MAX_LEN)\n    fast_tokenizer.enable_padding(max_length=MAX_LEN)\n    \n    text = list(zip(df.premise, df.hypothesis))\n    encoded = fast_tokenizer.encode_batch(\n        text\n    )\n    \n    all_ids = []\n    all_masks = []\n    all_ids.extend([enc.ids for enc in encoded])\n    all_masks.extend([enc.attention_mask for enc in encoded])\n    \n    return np.array(all_ids), np.array(all_masks)","5a51ef88":"TRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 16\nEPOCHS = 40\nMAX_LEN = 80\n# Scale learning rate to 8 TPU's\nLR = 2e-5 * xm.xrt_world_size() \nMETRICS_DEBUG = True\n\nWRAPPED_MODEL = xmp.MpModelWrapper(DistillBERT(num_labels=3, multisample=False))\ntokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-multilingual-cased')\n# Save the loaded tokenizer locally\ntokenizer.save_pretrained('.')\n# Reload it with the huggingface tokenizers library\nfast_tokenizer = BertWordPieceTokenizer('vocab.txt', lowercase=False)\n\n# Train Validation Split\nmask = np.random.rand(len(train)) < 0.95\ntrain_df = train[mask]\nvalid_df = train[~mask]\n\ntrain_ids, train_mask = fast_encode(train_df, fast_tokenizer)\nvalid_ids, valid_mask = fast_encode(valid_df, fast_tokenizer)\n\ntrain_dataset = DatasetRetriever(df=train_df, ids=train_ids, mask=train_mask)\nvalid_dataset = DatasetRetriever(df=valid_df, ids=valid_ids, mask=valid_mask)","d158ea74":"def _run():\n    xm.master_print('Starting Run ...')\n    train_sampler = DistributedSampler(\n        train_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=False\n    )\n    \n    train_data_loader = DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=0\n    )\n    xm.master_print('Train Loader Created.')\n    \n    valid_sampler = DistributedSampler(\n        valid_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=False\n    )\n    \n    valid_data_loader = DataLoader(\n        valid_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        sampler=valid_sampler,\n        drop_last=True,\n        num_workers=0\n    )\n    xm.master_print('Valid Loader Created.')\n    \n    num_train_steps = int(len(train_df) \/ TRAIN_BATCH_SIZE \/ xm.xrt_world_size())\n    device = xm.xla_device()\n    model = WRAPPED_MODEL.to(device)\n    xm.master_print('Done Model Loading.')\n    optimizer = get_model_optimizer(model)\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps = 0,\n        num_training_steps = num_train_steps * EPOCHS\n    )\n    xm.master_print(f'Num Train Steps= {num_train_steps}, XRT World Size= {xm.xrt_world_size()}.')\n    \n    for epoch in range(EPOCHS):\n        para_loader = pl.ParallelLoader(train_data_loader, [device])\n        xm.master_print('Parallel Loader Created. Training ...')\n        train_loop_fn(para_loader.per_device_loader(device),\n                      model,  \n                      optimizer, \n                      device, \n                      scheduler, \n                      epoch\n                     )\n        \n        xm.master_print(\"Finished training epoch {}\".format(epoch))\n            \n        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n        xm.master_print('Parallel Loader Created. Validating ...')\n        eval_loop_fn(para_loader.per_device_loader(device), \n                     model,  \n                     device\n                    )\n        \n        # Serialized and Memory Reduced Model Saving\n        if epoch == EPOCHS-1:\n            xm.master_print('Saving Model ..')\n            xser.save(model.state_dict(), f\"model.bin\", master_only=True)\n            xm.master_print('Model Saved.')\n            \n    if METRICS_DEBUG:\n      xm.master_print(met.metrics_report(), flush=True)","9bf399e0":"def _mp_fn(rank, flags):\n    # torch.set_default_tensor_type('torch.FloatTensor')\n    _run()\n\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","5120c113":"class TestDatasetRetriever(Dataset):\n    def __init__(self, df, ids, mask):\n        self.df = df\n        self.ids = ids\n        self.mask = mask\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):   \n        ids = self.ids[index]\n        mask = self.mask[index]\n        return {\n            'ids':torch.tensor(ids),\n            'mask':torch.tensor(mask)\n        }","143b3d3f":"TEST_BATCH_SIZE = 32\n\ntest_ids, test_mask = fast_encode(test, fast_tokenizer)\n\ntest_dataset = TestDatasetRetriever(test, test_ids, test_mask)\n\ntest_data_loader = DataLoader(\n    test_dataset, \n    batch_size=TEST_BATCH_SIZE,\n    drop_last=False,\n    num_workers=4,\n    shuffle=False\n)\n\n# Load Serialized Model\ndevice = xm.xla_device()\nmodel = WRAPPED_MODEL.to(device).eval()\nmodel.load_state_dict(xser.load(\"model.bin\"))","1ced88f7":"test_preds = []\n\nfor i, data in tqdm(enumerate(test_data_loader), total=len(test_data_loader)):\n    ids = data[\"ids\"]\n    mask = data[\"mask\"]\n    ids = ids.to(device, dtype=torch.long)\n    mask = mask.to(device, dtype=torch.long)\n    outputs = model(\n        input_ids = ids,\n        attention_mask = mask,\n    )\n    outputs_np = outputs.cpu().detach().numpy().tolist()\n    test_preds.extend(outputs_np)  \n    \ntest_preds = torch.FloatTensor(test_preds)\ntop1_prob, top1_label = torch.topk(test_preds, 1)\ny = top1_label.cpu().detach().numpy()\nsample_submission.prediction = y\nsample_submission.to_csv('submission.csv', index=False)","9290b44d":"### Evaluation","e70b55d2":"### Metrics Factory","e8439db0":"### Test","2cf529fe":"### Loss Factory","31cff11d":"### Test Config","868134ac":"### Test Prediction","8ec1694c":"### Dataset Factory","5bd9939e":"### Optimizer Factory","722f77bc":"##### References - [Xhlulu's - Jigsaw TPU: DistilBERT with Huggingface and Keras Kernel](https:\/\/www.kaggle.com\/xhlulu\/jigsaw-tpu-distilbert-with-huggingface-and-keras)","9943b058":"### Config","0bd37e5f":"### Data Files","8762c982":"### Text Augmentation\n\n##### References - [JohnM's - Agmenting Data with Translations Kernel](https:\/\/www.kaggle.com\/jpmiller\/augmenting-data-with-translations)","e47f4b94":"### Check TPU is available","b63fb841":"### Training","ebb718a3":"### Run","4586baec":"### Model Factory","66942ac2":"### Setup Dependencies"}}