{"cell_type":{"d6b7a8f5":"code","8ae5dc72":"code","99e0dad4":"code","22a4372e":"code","f824f8cf":"code","5d0f9e68":"code","67fed16b":"code","bcb152fc":"code","36fbf132":"code","4bb4d183":"code","da538335":"code","15ff08c9":"code","29e6fa6a":"code","7920f83a":"code","fe2d89a6":"code","0b29774b":"code","1272e6ca":"code","8f248358":"code","93eb31f3":"code","196b99b3":"code","8f77118f":"code","b5272d8f":"code","942d062d":"code","599823be":"code","83ba4f99":"code","7260580e":"code","532e67b4":"code","0598992a":"code","2a93828d":"code","973bc8e2":"code","e3c7ffad":"code","0602fd56":"code","7b24867d":"code","2be31851":"code","ff7b30d7":"code","8cb0299e":"code","73693701":"markdown","88083c18":"markdown","5c3160be":"markdown","d4fde250":"markdown","35d30cf9":"markdown","d5e28a7c":"markdown","be86e6d1":"markdown","5dac4660":"markdown","8aef81e0":"markdown","c89ff071":"markdown"},"source":{"d6b7a8f5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8ae5dc72":"df=pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","99e0dad4":"df","22a4372e":"df.shape","f824f8cf":"df.isnull().sum()","5d0f9e68":"df.describe()","67fed16b":"# I drop the column,Unamed:32 because this column is not suitable for analysis\ndf.drop('Unnamed: 32', axis = 1, inplace = True)","bcb152fc":"# Drop the column 'id'\ndf=df.drop('id',axis=1);","36fbf132":"# I take first 11 features for the analysis\ndf=df.drop(df.iloc[:,11:32],axis=1)","4bb4d183":"df","da538335":"print(df[\"diagnosis\"].value_counts().index)","15ff08c9":"# 'B' stands for 'Benign' and 'M' stands for 'Malignant'\nprint(df[\"diagnosis\"].value_counts())","29e6fa6a":"from matplotlib import pyplot as plt\nfrom matplotlib import style","7920f83a":"x=df[\"diagnosis\"].value_counts().index\ny=df[\"diagnosis\"].value_counts()\nplt.bar(x,y,color=[\"black\",\"blue\"],width=0.9)\nprint(df[\"diagnosis\"].value_counts())\nplt.xlabel(\"DIAGNOSIS\",color=\"black\",fontsize=15)\nplt.ylabel(\"DISTRIBUTION\",color=\"black\",fontsize=15)\nplt.title(\"DIAGNOSIS DISTRIBUTION\",color=\"black\",fontsize=20)\nplt.legend()\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(8,6)\nplt.tight_layout()\nplt.show()\n","fe2d89a6":"# Radius_Mean\nplt.hist(df[\"radius_mean\"][df[\"diagnosis\"]==\"M\"],color=\"blue\",alpha=0.5)\nplt.hist(df[\"radius_mean\"][df[\"diagnosis\"]==\"B\"],color=\"black\",alpha=0.5)\nplt.title(\"RADIUS\",fontsize=12,color=\"black\")\nplt.ylabel(\"FREQUENCY\",fontsize=10,color=\"black\")\nplt.legend([\"MALIGANT\",\"BENIGN\"])\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.show()","0b29774b":"# Texture_Mean\nplt.hist(df[\"texture_mean\"][df[\"diagnosis\"]==\"M\"],color=\"blue\",alpha=0.5)\nplt.hist(df[\"texture_mean\"][df[\"diagnosis\"]==\"B\"],color=\"black\",alpha=0.5)\nplt.title(\"TEXTURE\",fontsize=12,color=\"black\")\nplt.ylabel(\"FREQUENCY\",fontsize=10,color=\"black\")\nplt.legend([\"MALIGANT\",\"BENIGN\"])\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.show()","1272e6ca":"#Perimeter_Mean\nplt.hist(df[\"perimeter_mean\"][df[\"diagnosis\"]==\"M\"],color=\"blue\",alpha=0.5)\nplt.hist(df[\"perimeter_mean\"][df[\"diagnosis\"]==\"B\"],color=\"black\",alpha=0.5)\nplt.title(\"PERIMETER\",fontsize=12,color=\"black\")\nplt.ylabel(\"FREQUENCY\",fontsize=10,color=\"black\")\nplt.legend([\"MALIGANT\",\"BENIGN\"])\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.show()","8f248358":"# Area_Mean\nplt.hist(df[\"area_mean\"][df[\"diagnosis\"]==\"M\"],color=\"blue\",alpha=0.5)\nplt.hist(df[\"area_mean\"][df[\"diagnosis\"]==\"B\"],color=\"black\",alpha=0.5)\nplt.title(\"AREA\",fontsize=12,color=\"black\")\nplt.ylabel(\"FREQUENCY\",fontsize=10,color=\"black\")\nplt.legend([\"MALIGANT\",\"BENIGN\"])\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.show()","93eb31f3":"# Smoothness_Mean\nplt.hist(df[\"smoothness_mean\"][df[\"diagnosis\"]==\"M\"],color=\"blue\",alpha=0.5)\nplt.hist(df[\"smoothness_mean\"][df[\"diagnosis\"]==\"B\"],color=\"black\",alpha=0.5)\nplt.title(\"SMOOTHNESS\",fontsize=10,color=\"black\")\nplt.ylabel(\"FREQUENCY\",fontsize=15,color=\"black\")\nplt.legend([\"MALIGANT\",\"BENIGN\"])\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.show()","196b99b3":"#Compactness_mean\nplt.hist(df[\"compactness_mean\"][df[\"diagnosis\"]==\"M\"],color=\"blue\",alpha=0.5)\nplt.hist(df[\"compactness_mean\"][df[\"diagnosis\"]==\"B\"],color=\"black\",alpha=0.5)\nplt.title(\"COMAPCTNESS\",fontsize=10,color=\"black\")\nplt.ylabel(\"FREQUENCY\",fontsize=12,color=\"black\")\nplt.legend([\"MALIGANT\",\"BENIGN\"])\nplt.show()","8f77118f":"# Concavity_Mean\nplt.hist(df[\"concavity_mean\"][df[\"diagnosis\"]==\"M\"],color=\"blue\",alpha=0.5)\nplt.hist(df[\"concavity_mean\"][df[\"diagnosis\"]==\"B\"],color=\"black\",alpha=0.5)\nplt.title(\"CONCAVITY\",fontsize=12,color=\"black\")\nplt.ylabel(\"FREQUENCY\",fontsize=10,color=\"black\")\nplt.legend([\"MALIGANT\",\"BENIGN\"])\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.show()","b5272d8f":"# Concave points_Mean\nplt.hist(df[\"concave points_mean\"][df[\"diagnosis\"]==\"M\"],color=\"blue\",alpha=0.5)\nplt.hist(df[\"concave points_mean\"][df[\"diagnosis\"]==\"B\"],color=\"black\",alpha=0.5)\nplt.title(\"CONCAVE POINTS\",fontsize=12,color=\"black\")\nplt.ylabel(\"FREQUENCY\",fontsize=10,color=\"black\")\nplt.legend([\"MALIGANT\",\"BENIGN\"])\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.show()","942d062d":"#Symmetry_Mean\nplt.hist(df[\"symmetry_mean\"][df[\"diagnosis\"]==\"M\"],color=\"blue\",alpha=0.5)\nplt.hist(df[\"symmetry_mean\"][df[\"diagnosis\"]==\"B\"],color=\"black\",alpha=0.5)\nplt.title(\"SYMMETRY\",fontsize=12,color=\"black\")\nplt.ylabel(\"FREQUENCY\",fontsize=10,color=\"black\")\nplt.legend([\"MALIGANT\",\"BENIGN\"])\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.show()","599823be":"#'X' contain features which are used to train the model\nX = df.iloc[:,1:]\nX","83ba4f99":"# 'Y' conatin target data,which is used to train the model\nY=Y= (df[\"diagnosis\"]==\"M\").astype(np.int)\nY","7260580e":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, confusion_matrix,plot_confusion_matrix,accuracy_score\nfrom sklearn.model_selection import train_test_split","532e67b4":"X_train,X_test,Y_train,Y_test =train_test_split(X,Y,test_size=0.2, random_state=0)\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.fit_transform(X_test)\nclf=LogisticRegression()\nclf.fit(X_train,Y_train)\npred=clf.predict(X_test)\nlogg_acc=accuracy_score(Y_test,pred)\n# print(pred)\nprint(\"Accuracy of logistic model: \",logg_acc)","0598992a":"print(\"REPORT: \")\nprint(confusion_matrix(Y_test,pred))\nprint(classification_report(Y_test,pred))\nplot_confusion_matrix(clf,X_test,Y_test)\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.grid(False)\n\n","2a93828d":"from sklearn.tree import DecisionTreeClassifier","973bc8e2":"X_train,X_test,Y_train,Y_test =train_test_split(X,Y,test_size=0.2, random_state=0)\nclf = DecisionTreeClassifier(criterion=\"entropy\", random_state=0)\nclf = clf.fit(X_train,Y_train)\ny_pred = clf.predict(X_test)\ndtc_acc=accuracy_score(Y_test,y_pred)\nprint(\"REPORT:\")\nprint(\"Accuracy of Decision_tree_Classifier: \",dtc_acc)\nprint(confusion_matrix(Y_test,y_pred))\nprint(classification_report(Y_test, y_pred))\nplot_confusion_matrix(clf,X_test,Y_test)\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.grid(False)\n","e3c7ffad":"from sklearn.neighbors import KNeighborsClassifier\n\n","0602fd56":"# this helps in finding the value of 'n_neighbors' in KNN classifier\nerror_rate = []\n  \n\nfor i in range(1, 40):\n      \n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train, Y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != Y_test))\n  \nplt.figure(figsize =(10, 6))\nplt.plot(range(1, 40), error_rate, color ='blue',\n                linestyle ='dashed', marker ='o',\n         markerfacecolor ='red', markersize = 10)\n  \nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","7b24867d":"# I take value of 'n_neighbors'around 13,beacuse after this value error rate remain unchangeable \nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3)\nclf=KNeighborsClassifier(n_neighbors = 13)\nclf.fit(X_train,Y_train)\npred = clf.predict(X_test)\nprint(\"REPORT:\")\nknn_acc=accuracy_score(Y_test,pred)\nprint(\"Accuracy of knn model:\",knn_acc)\nprint(confusion_matrix(Y_test, pred))\nprint(classification_report(Y_test, pred))\nplot_confusion_matrix(clf,X_test,Y_test)\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.grid(False)","2be31851":"from sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, Y_train)\npred = clf.predict(X_test)\nprint(\"REPORT:\")\nrfc_acc=accuracy_score(Y_test,pred)\nprint(\"Accuracy of knn model:\",rfc_acc)\nprint(confusion_matrix(Y_test, pred))\nprint(classification_report(Y_test, pred))\nplot_confusion_matrix(clf,X_test,Y_test)\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\nplt.grid(False)","ff7b30d7":"import seaborn as sns","8cb0299e":"print(\"Accuracy of logistics regression classifier: \",logg_acc)\nprint(\"Accuracy of Decision tree classifier: \",dtc_acc)\nprint(\"Accuracy of Knn classifier: \",knn_acc)\nmodel_acc=[logg_acc,dtc_acc,knn_acc,rfc_acc]\nname_of_models=[\"Logistic_classifier\",\"Decision_tree_classifier\",\"KNN_classifier\",\"RandomForestClassifier\"]\nsns.barplot(x=model_acc,y=name_of_models,palette=\"bright\")\nplt.xlim([0.8,1.0])\nplt.style.use(\"fivethirtyeight\")\nplt.gcf().set_size_inches(12,6)\n","73693701":"# Data Analysis","88083c18":"# KNN CLASSIFIER","5c3160be":"# Loading of data","d4fde250":"# MODELS(Predict whether the cancer is benign or malignant)","35d30cf9":"# RandomForestClassifier","d5e28a7c":"****","be86e6d1":"# REPORT RELATED TO MODELS ","5dac4660":"# Data visualization","8aef81e0":"# Logistic Regression","c89ff071":"# DECISION TREE CLASSIFIER"}}