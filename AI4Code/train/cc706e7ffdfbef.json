{"cell_type":{"2ec08dbf":"code","a33e2879":"code","04acdd48":"code","f07fdefe":"code","b5526ab2":"code","0d4e4f75":"code","924fe31a":"code","cec0d264":"code","1bc6a4ff":"code","2bab614f":"code","ecfa6392":"code","c5a40a65":"code","af527236":"code","9bc911b7":"code","39971318":"code","f687b82d":"code","837a4517":"code","7e70c165":"code","91fae117":"code","a9371da1":"markdown","295845e7":"markdown","d42c7416":"markdown","b81d9f58":"markdown","4afb0650":"markdown","571b421e":"markdown","aa107c73":"markdown","c7e64e42":"markdown","a7afa0a1":"markdown","97ae845d":"markdown","72e04127":"markdown","6f0ab16c":"markdown","2a15728a":"markdown","c784ae83":"markdown","bb3ba59e":"markdown","2bd2e70e":"markdown"},"source":{"2ec08dbf":"import pandas as pd \nimport numpy as np \nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom random import sample\n\nnp.random.seed(0)","a33e2879":"def read_data():\n    print('Reading train.csv file....')\n    train = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train.csv')\n    print('Training.csv file have {} rows and {} columns'.format(train.shape[0], train.shape[1]))\n\n    print('Reading test.csv file....')\n    test = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/test.csv')\n    print('Test.csv file have {} rows and {} columns'.format(test.shape[0], test.shape[1]))\n\n    print('Reading train_labels.csv file....')\n    train_labels = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train_labels.csv')\n    print('Train_labels.csv file have {} rows and {} columns'.format(train_labels.shape[0], train_labels.shape[1]))\n\n    print('Reading specs.csv file....')\n    specs = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/specs.csv')\n    print('Specs.csv file have {} rows and {} columns'.format(specs.shape[0], specs.shape[1]))\n\n    print('Reading sample_submission.csv file....')\n    sample_submission = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/sample_submission.csv')\n    print('Sample_submission.csv file have {} rows and {} columns'.format(sample_submission.shape[0], sample_submission.shape[1]))\n    return train, test, train_labels, specs, sample_submission\n\ndef encode_title(train, test, train_labels):\n    # encode title\n    train['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), train['title'], train['event_code']))\n    test['title_event_code'] = list(map(lambda x, y: str(x) + '_' + str(y), test['title'], test['event_code']))\n    all_title_event_code = list(set(train[\"title_event_code\"].unique()).union(test[\"title_event_code\"].unique()))\n    # make a list with all the unique 'titles' from the train and test set\n    list_of_user_activities = list(set(train['title'].unique()).union(set(test['title'].unique())))\n    # make a list with all the unique 'event_code' from the train and test set\n    list_of_event_code = list(set(train['event_code'].unique()).union(set(test['event_code'].unique())))\n    list_of_event_id = list(set(train['event_id'].unique()).union(set(test['event_id'].unique())))\n    # make a list with all the unique worlds from the train and test set\n    list_of_worlds = list(set(train['world'].unique()).union(set(test['world'].unique())))\n    # create a dictionary numerating the titles\n    activities_map = dict(zip(list_of_user_activities, np.arange(len(list_of_user_activities))))\n    activities_labels = dict(zip(np.arange(len(list_of_user_activities)), list_of_user_activities))\n    activities_world = dict(zip(list_of_worlds, np.arange(len(list_of_worlds))))\n    assess_titles = list(set(train[train['type'] == 'Assessment']['title'].value_counts().index).union(set(test[test['type'] == 'Assessment']['title'].value_counts().index)))\n    # replace the text titles with the number titles from the dict\n    train['title'] = train['title'].map(activities_map)\n    test['title'] = test['title'].map(activities_map)\n    train['world'] = train['world'].map(activities_world)\n    test['world'] = test['world'].map(activities_world)\n    train_labels['title'] = train_labels['title'].map(activities_map)\n    win_code = dict(zip(activities_map.values(), (4100*np.ones(len(activities_map))).astype('int')))\n    # then, it set one element, the 'Bird Measurer (Assessment)' as 4110, 10 more than the rest\n    win_code[activities_map['Bird Measurer (Assessment)']] = 4110\n    # convert text into datetime\n    train['timestamp'] = pd.to_datetime(train['timestamp'])\n    test['timestamp'] = pd.to_datetime(test['timestamp'])\n    \n    \n    return train, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code\n\n# read data\ntrain, test, train_labels, specs, sample_submission = read_data()\n# get usefull dict with maping encode\ntrain, test, train_labels, win_code, list_of_user_activities, list_of_event_code, activities_labels, assess_titles, list_of_event_id, all_title_event_code = encode_title(train, test, train_labels)","04acdd48":"if 'start_installation' in train.columns:\n    train.drop(['start_installation', 'start_game_session'],\n               axis=1, inplace=True)\n\n# Add time since start\ntrain = pd.merge(\n    left=train,\n    right=train \\\n        .groupby('installation_id') \\\n        .timestamp.min() \\\n        .to_frame() \\\n        .rename(columns={'timestamp': 'start_installation'}),\n    how='inner',\n    left_on='installation_id',\n    right_on='installation_id'\n) \\\n    .assign(seconds_since_installation =  \\\n                lambda x: (x.timestamp - x.start_installation) \\\n                .apply(lambda x: x.total_seconds()))\n\n# Add time since session start\n# Assuming game_session is unique\ntrain = pd.merge(\n    left=train,\n    right=train \\\n        .groupby('game_session') \\\n        .timestamp.min() \\\n        .to_frame() \\\n        .rename(columns={'timestamp': 'start_game_session'}),\n    how='inner',\n    left_on='game_session',\n    right_on='game_session'\n) \\\n    .assign(seconds_since_game_session_start = \\\n                lambda x: (x.timestamp - x.start_game_session) \\\n                .apply(lambda x: x.total_seconds()))\n\n# Add the mean timestamp and plot heatmap\ninstallation_props = train \\\n    .query('event_count == 1') \\\n    .groupby('installation_id') \\\n    .seconds_since_installation.mean() \\\n    .to_frame() \\\n    .rename(columns={'seconds_since_installation':'seconds_since_installation_mean'})\n\ntrain = pd.merge(\n    train,\n    installation_props,\n    how='inner',\n    left_on='installation_id',\n    right_index=True)","f07fdefe":"train.head()","b5526ab2":"SESSION_CUTOFF=15*60  # seconds\n\nsession_ids = train \\\n    .groupby('installation_id') \\\n    .timestamp \\\n    .apply(lambda x: x.sort_values() \\\n               .diff() \\\n               .apply(lambda y: y.total_seconds() > SESSION_CUTOFF) \\\n               .cumsum()) \\\n    .reset_index() \\\n    .set_index('level_1') \\\n    .rename(columns={'timestamp': 'session_id'}) \\\n    .session_id\n\n# Add session_ids to train\ntrain = pd.merge(train,\n                 session_ids,\n                 how='inner',\n                 left_index=True,\n                 right_index=True)","0d4e4f75":"def duration(x):\n    return x.max()-x.min()\n\nsessions = train\\\n    .groupby(['installation_id' , 'session_id']) \\\n    .agg({'seconds_since_installation': ['min', 'max', 'count', duration],\n          'game_session': 'nunique'})\n\nsessions.head()","924fe31a":"train \\\n    .groupby(['installation_id']) \\\n    .session_id.nunique() \\\n    .value_counts(normalize=True, sort=True) \\\n    .head(10) \\\n    .apply(lambda x: x*100) \\\n    .plot(kind='bar')\nplt.xlabel('Number of Sessions')\nplt.ylabel('Percent of Installations')","cec0d264":"print('Cummulative Installations with Session Count')\ntrain \\\n    .groupby(['installation_id']) \\\n    .session_id.nunique() \\\n    .value_counts(normalize=True, sort=True) \\\n    .cumsum() \\\n    .head(10)","1bc6a4ff":"print('Game Session Counts')\nsessions.game_session['nunique'].describe()","2bab614f":"sessions.game_session['nunique'] \\\n    .plot('box', vert=False, sym='')\nplt.xlabel('Game Session Count')\nplt.title('Boxplot of Game Session Counts')","ecfa6392":"print('Sessions Description')\nsessions.seconds_since_installation.duration.describe()","c5a40a65":"sessions.seconds_since_installation.duration \\\n    .plot('box', vert=False, sym='')\nplt.xlabel('Session Duration (seconds)')\nplt.title('Boxplot of Session Durations')","af527236":"results ={}\nnum_samples = 50\nnum_days = 80\nfor installation in sample(list(train.installation_id.unique()), num_samples):\n    count, division = np.histogram(train \\\n        .query('installation_id == @installation') \\\n        .drop_duplicates(subset=['installation_id', 'session_id'], keep='first')\n        .seconds_since_installation \\\n        .apply(lambda x: x\/60\/60\/24),\n        bins = np.linspace(0, num_days, num_days + 1))\n    results[installation] = count\ndivision = list(map(int, division))\nresults = pd.DataFrame.from_dict(results,\n                                 orient='index',\n                                 columns=division[:-1])\n\nsns.heatmap(\n    pd.merge(\n        results,\n        installation_props,\n        how='inner',\n        left_index=True,\n        right_index=True\n    ) \\\n        .sort_values(by='seconds_since_installation_mean', ascending=True) \\\n        .drop(['seconds_since_installation_mean'], axis=1),\n    yticklabels=False,\n    vmax=3,\n    xticklabels=5\n)\nfig = plt.gcf()\nfig.set_size_inches(12,8)\nplt.xlabel('Days since installation')\nplt.ylabel('Installations')\nplt.title('Daily Session Counts per Installation')","9bc911b7":"results ={}\nnum_samples = 50\nnum_days = 80\nfor installation in sample(list(train.query('seconds_since_installation_mean > 3*24*60*60').installation_id.unique()), num_samples):\n    count, division = np.histogram(train \\\n        .query('installation_id == @installation') \\\n        .query('seconds_since_installation_mean > 1*24*60*60') \\\n        .drop_duplicates(subset=['installation_id', 'session_id'], keep='first')\n        .seconds_since_installation \\\n        .apply(lambda x: x\/60\/60\/24),\n        bins = np.linspace(0, num_days, num_days + 1))\n    results[installation] = count\ndivision = list(map(int, division))\nresults = pd.DataFrame.from_dict(results,\n                                 orient='index',\n                                 columns=division[:-1])","39971318":"sns.heatmap(\n    pd.merge(\n        results,\n        installation_props,\n        how='inner',\n        left_index=True,\n        right_index=True\n    ) \\\n        .sort_values(by='seconds_since_installation_mean', ascending=True) \\\n        .drop(['seconds_since_installation_mean'], axis=1),\n    yticklabels=False,\n    vmax=3,\n    xticklabels=5\n)\nfig = plt.gcf()\nfig.set_size_inches(12,8)\nplt.xlabel('Days since installation')\nplt.ylabel('Installations')","f687b82d":"sessions.head()","837a4517":"sessions.loc['0006a69f', :]['seconds_since_installation']['min'].diff()","7e70c165":"results = {}\nfor i in list(set([a[0] for a in sessions.index]))[:20000]:\n    results[i] = [sessions.loc[i, :]['seconds_since_installation']['min'].diff().mean(),\n                  sessions.loc[i, :]['seconds_since_installation']['min'].diff().median()]\nfrequencies = pd.DataFrame.from_dict(results, orient='index', columns=['mean', 'median']) \\\n    .apply(lambda x: x\/60\/60\/24)\nfrequencies[frequencies['mean'].notnull()].head()","91fae117":"frequencies.plot(kind='box', sym='')\nfig = plt.gcf()\nfig.set_size_inches(8, 5)\nplt.ylabel('Number of Days between Sessions')\nplt.title('Frequency of Installations')","a9371da1":"# Sessions EDA\n\nFor this dataset, we have Installations (~Users), Game Sessions and Events. This seems like a good start, however a Game Session is not quite representative of learning patterns. It's well known that people should take breaks while studying because learning drastically drops off after 40 minutes or so. To get a better understanding of how much learning has taken place, rather than game sessions, we'll look at sessions. This approach is also commonly used in Ecommerce.\n\nSessions will be defined as consecutive events separated by no more than 15 minutes. This is motivated by the maximum length of clips (156s) as well as the recommended time for a break of 10 minutes.","295845e7":"The sort of graphics above leads us to ask questions like how often is a user coming back? For this, we can calculate the frequency.","d42c7416":"### Frequency\nWe will calculate frequency of an installation as the average time between the start of sessions. It is undefined for users with just one session.","b81d9f58":"### Game Session Counts\nHow many games being played in each session?","4afb0650":"### Session Durations\nHow long is the average session?","571b421e":"## Data Augmentation\n\nLet's augment the data to provide some usefil timings.","aa107c73":"The biggest session is a whopping 4 hours, that's some addictive gameplay right there! Though the quality of the output may have dropped a little...\n\nWe can also visualise this if we drop the outliers.","c7e64e42":"## Creating Sessions\nTo create the sessions, we define the cutoff as mentioned above and then events together under each installation to find the '[islands](https:\/\/www.red-gate.com\/simple-talk\/sql\/t-sql-programming\/the-sql-of-gaps-and-islands-in-sequences\/)' of events. These session ids are then merged back with the original data source.","a7afa0a1":"### Sessions Over Time\nHow do we sessions appearing over time? Are they all at once, or over multiple days? Are there big gaps?\n\nLet's plot the sessions in time for a random sample of installations.","97ae845d":"We see that ignoring the users that have just one session, the number of days between sessions is ~5, though the third quartile (50%-75%) extends up to ~10 days.\n\nThis sort of frequency feature might be useful as a feature to predict performance on the games as it describes what sort of habit the user has.","72e04127":"## Visualisation\nWe can now visualise properties about these sessions.","6f0ab16c":"Again, we see that the majority of users are with few sessions, and even those with multiple sessions are seen to have them on just one day. Looking past that we do see some interesting behaviour, like users combing back every few days or even those coming back 70 days after the installation with nothing in between.\n\nWe can get a closer look at those installations with sessions over many days by doing some filtering.","2a15728a":"Now we can see some very addicted users accessing almost ever day without fail for weeks on end! This sort of behaviour seems like the kind of training that would lead to good performance, vs. those that are coming back a lot in one day or randomly. This is because spaced repetition is much more powerful than compressed learning. Also on the hourly scale, (e.g. no more than 1 hour of sessions or it becomes less valuable).","c784ae83":"### Session Counts\nHow many sessions per installation?","bb3ba59e":"Unsurprisingly, over 50% of the installations have just one session. The user installs the game and then quickly forgets about it - oh, poor developer! That said, there is still a decent number of installations with multiple sessions, which means the app is getting more regular usage. ","2bd2e70e":"## Read Data\n\nRead in all the data and process to get some convenient lists.\n\nFunctions taken from [this kernel](https:\/\/www.kaggle.com\/braquino\/890-features)."}}