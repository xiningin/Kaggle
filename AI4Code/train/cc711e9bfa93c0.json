{"cell_type":{"b722c00f":"code","87111c3e":"code","1e6c1fd8":"code","33045ef0":"code","16effbdc":"code","7c94c447":"code","9b9dcdbf":"code","0efbfeb9":"code","3911fa2f":"code","444dec81":"code","0e861755":"code","603f8c70":"code","6a05ef51":"code","4ce0d8c5":"code","25e8743e":"code","4a030c67":"code","1e8818e1":"code","4012a49a":"code","2b559a8c":"code","9f76eca9":"code","d83e3f8d":"code","a2ddf0e3":"code","17990f9a":"code","2667d073":"code","17be7898":"code","4285e26c":"code","f27a8ff0":"code","e27777c9":"code","5473ad95":"code","f58ba46f":"code","8fb67d2b":"code","347726a0":"code","c4219101":"code","6da36ae7":"markdown","c0237bf7":"markdown","6eb1e112":"markdown","39292581":"markdown","cb1e2e37":"markdown","0d02ff3d":"markdown"},"source":{"b722c00f":"#Importing the libraries required\nfrom wordcloud import WordCloud, STOPWORDS \nimport matplotlib.pyplot as plt \n%matplotlib inline\nfrom nltk.tokenize import RegexpTokenizer\nimport numpy as np # linear algebra\nimport pandas as pd #data processing\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import classification_report\nimport os\nimport re\nimport nltk\n","87111c3e":"#Loading the data\ntrain=pd.read_csv('..\/input\/fake-news\/train.csv')\ntest=pd.read_csv('..\/input\/fake-news\/test.csv')","1e6c1fd8":"train.head()#Real news =1 and Fake news =0\n","33045ef0":"test.head()","16effbdc":"print(train.shape, test.shape)","7c94c447":"#Checking for null values\nprint(train.isnull().sum())\nprint('************')\nprint(test.isnull().sum())","9b9dcdbf":"#Dropping Null Values\ntest=test.dropna()\ntrain=train.dropna()","0efbfeb9":"# No mssing values\nprint(test.isnull().sum())\nprint(test.isnull().sum())\n","3911fa2f":"print(train.shape, test.shape)","444dec81":"test['total']=test['title']+' '+test['text'] #to add text in one coloumn\ntrain['total']=train['title']+' '+train['text']","0e861755":"train['label'].value_counts()","603f8c70":"#Checking if the data is balanced or imbalanced\ntrain['label'].value_counts().plot(kind = \"bar\")#Class division\nplt.title(\"News Clasified as fake or real\")","6a05ef51":"#Using NLTK for preprocessing\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\ntokenizer=RegexpTokenizer(r'\\w+')\nstop_words = stopwords.words('english')\n","4ce0d8c5":"#Tokenization\ntrain['total']=train['total'].apply(lambda x:tokenizer.tokenize(x.lower()))\ntrain['total'].head()","25e8743e":"#Removing Stopwords\ndef remove_stopwords(text):\n    words=[w for w in text if  not w in stop_words]\n    return words\ntrain['total']=train['total'].apply(lambda x:remove_stopwords(x))\ntrain['total'].head()","4a030c67":"#Lemmatizing the text to get the root word\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer=WordNetLemmatizer()\ndef word_lemmatizer(text):\n    lem_text=' '.join([lemmatizer.lemmatize(i) for i in text])\n    return lem_text\ntrain['total']=train['total'].apply(lambda x:word_lemmatizer(x))\ntrain['total'].head()","1e8818e1":"#Analysing the most words produced in real articles\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 1000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(train[train.label == 1].total))\nplt.imshow(wc, interpolation = 'bilinear')\nplt.axis(\"off\")\nplt.title(\"The most frequent words generated in Real Articles\")","4012a49a":"#Analysing the most words produced in fake articles\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nplt.figure(figsize = (20,20)) \nwc = WordCloud(max_words = 1000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(train[train.label == 0].total))\nplt.imshow(wc, interpolation = 'bilinear')\nplt.axis(\"off\")\nplt.title(\"The most frequent words generated in Fake Articles\")","2b559a8c":"from sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer","9f76eca9":"#Defining independent and dependent variables\nX_train = train['total']\nY_train = train['label']","d83e3f8d":"#For further use\na=X_train\nb=Y_train","a2ddf0e3":"#Tf-idf is to covert the sentences into vectors so that the can be intputted into the model.\nvectorizer = TfidfVectorizer(norm='l2',ngram_range=(1,2))#max_df is just to say ignore all words that have appeared in 85% of the documents, since those may be unimportant.\nX = vectorizer.fit_transform(X_train)\nX_vect=X #Vectrozied form of the words\n","17990f9a":"#Looking at the voacb\nlist(vectorizer.vocabulary_.keys())[:20] ","2667d073":"#split in samples\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_vect, Y_train, random_state=0)","17be7898":"print(X_train.shape,X_test.shape)","4285e26c":"#Applying logistic regression\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=100, penalty= 'l2', solver='liblinear')\nlogreg.fit(X_train, y_train)\n\ny_pred = logreg.predict(X_test)\n\nprint('Classification Report:\\n', classification_report(y_test,y_pred))\nprint('Confusion Matrix:\\n', confusion_matrix(y_test, y_pred))\nprint(\"Accuracy score:\", accuracy_score(y_test, y_pred))","f27a8ff0":"from sklearn.pipeline import Pipeline\nimport joblib\nfrom sklearn import linear_model","e27777c9":"#Pieplining the process\npipeline = Pipeline([\n    ('vectorizer', TfidfVectorizer(norm='l2',ngram_range=(1,2))),\n    ('logreg', linear_model.LogisticRegression(C=100, penalty= 'l2', solver='liblinear')),\n])\n","5473ad95":"X_train","f58ba46f":"pipeline.fit(a, b)","8fb67d2b":"#Unseen Text\ntext=\"In the wake of yet another court decision that derailed Donald Trump s plan to bar Muslims from entering the United States, the New York Times published a report on Saturday morning detailing the president s frustration at not getting his way   and how far back that frustration goes.According to the article, back in June, Trump stomped into the Oval Office, furious about the state of the travel ban, which he thought would be implemented and fully in place by then. Instead, he fumed, visas had already been issued to immigrants at such a rate that his  friends were calling to say he looked like a fool  after making his broad pronouncements.It was then that Trump began reading from a document that a top advisor, noted white supremacist Stephen Miller, had handed him just before the meeting with his Cabinet. The page listed how many visas had been issued this year, and included 2,500 from Afghanistan (a country not on the travel ban), 15,000 from Haiti (also not included), and 40,000 from Nigeria (sensing a pattern yet?), and Trump expressed his dismay at each.According to witnesses in the room who spoke to the Times on condition of anonymity, and who were interviewed along with three dozen others for the article, Trump called out each country for its faults as he read: Afghanistan was a  terrorist haven,  the people of Nigeria would  never go back to their huts once they saw the glory of America, and immigrants from Haiti  all have AIDS. Despite the extensive research done by the newspaper, the White House of course denies that any such language was used.But given Trump s racist history and his advisor Stephen Miller s blatant white nationalism, it would be no surprise if a Freedom of Information Act request turned up that the document in question had the statements printed inline as commentary for the president to punctuate his anger with. It was Miller, after all, who was responsible for the  American Carnage  speech that Trump delivered at his inauguration.This racist is a menace to America, and he doesn t represent anything that this country stands for. Let s hope that more indictments from Robert Mueller are on their way as we speak.Featured image via Chris Kleponis\/Pool\/Getty Images\"","347726a0":"#Cleaning data before applying model\ntext=tokenizer.tokenize(text)\ntext=remove_stopwords(text)\ntext=word_lemmatizer(text)\ntext","c4219101":"#testing it on data\npipeline.predict_proba([text])","6da36ae7":"# Modelling","c0237bf7":" # Vectorizing ","6eb1e112":"# Word Cloud for Analysis","39292581":"# Applying Model to Unseen Data","cb1e2e37":"# Data Analysis","0d02ff3d":"# Data Cleaning"}}