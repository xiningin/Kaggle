{"cell_type":{"4231374a":"code","4a9ab543":"code","fc3ed686":"code","33378f54":"code","45d7b136":"code","a7a1b642":"code","85323b95":"code","cdfdc991":"code","ab6ae256":"code","6321108b":"code","99d4f702":"code","b7512ac5":"code","2aaaea8a":"code","b24d337a":"code","9ec02232":"code","68f93e8d":"code","9829d31b":"code","df679cf2":"code","cffa6e57":"code","6dc7ca4b":"code","304ef7d9":"code","c9d76323":"code","602fb63a":"code","dec206b2":"code","16497829":"code","7663eb99":"code","2ebe5a05":"code","23a839d1":"code","1ff1b200":"code","edb669fa":"code","3f02d4a6":"code","833782ed":"code","556c0176":"code","822da87b":"code","f18c5734":"code","29540f0d":"code","a6ad4908":"code","4b1fb6a0":"code","439859b2":"code","36484f61":"code","4b3c1a8a":"code","ba2067c5":"code","8708de58":"code","395bc283":"code","a52e0254":"code","54fc52c3":"code","4abdc0d9":"code","16f93b73":"code","7abaea7b":"code","17a3486a":"code","aed7a870":"code","e4ce137b":"code","46dc9ca2":"code","60062175":"code","adcf6192":"code","08f7d2da":"markdown","b708d9ba":"markdown","1ce97899":"markdown","c189e41b":"markdown","d8ec1c2c":"markdown","5f4072b4":"markdown","6e3f1dfd":"markdown","a2c370f3":"markdown","9e860999":"markdown","f5e2f562":"markdown","1bb920d1":"markdown","a613ec09":"markdown","6855d33b":"markdown","db5cb6ba":"markdown","024b1bb7":"markdown","43dc2ac1":"markdown","f42eb6c4":"markdown","1fb9ae93":"markdown","8440262d":"markdown","65fc91aa":"markdown","f561bae0":"markdown","f6e7a32b":"markdown","8eb37ffc":"markdown","630ba5ae":"markdown","4a472393":"markdown","848715b6":"markdown","bace5ccf":"markdown","ca5d6b01":"markdown","91a4e499":"markdown","a40b1a8b":"markdown","933d53f4":"markdown","e0af27cc":"markdown","cb56356d":"markdown","cdec30bf":"markdown","1fac7319":"markdown","d04d6920":"markdown","12a9a7c0":"markdown","cbfe6b4a":"markdown","0a5d8810":"markdown","4885a5c8":"markdown","9617f237":"markdown","da4980b6":"markdown","d3705a39":"markdown","6c6a10df":"markdown","107d9d0c":"markdown","a4605b1d":"markdown","da39e678":"markdown","340f4524":"markdown","75b84a49":"markdown","e19d5d0a":"markdown"},"source":{"4231374a":"# importing relevant libraries\nimport pandas as pd\nimport numpy as np","4a9ab543":"# loading the dataset\nrain_data = pd.read_csv('..\/input\/weather-dataset-rattle-package\/weatherAUS.csv')\nprint('The dataset has {} rows and {} columns'.format(rain_data.shape[0],rain_data.shape[1]))","fc3ed686":"rain_data.head()","33378f54":"rain_data.describe()","45d7b136":"# printing out the column names\nprint(rain_data.columns)","a7a1b642":"# Checking the categorical and non-categorical datasets\ncat = rain_data.dtypes=='object'\nnum = rain_data.dtypes=='float64'\ncat_columns = list(cat[cat].index)\nnum_columns = list(num[num].index)\nprint(\"Categorical variables are:\",cat_columns)\n\nprint(\"Numerical variables are:\",num_columns)","85323b95":"# checking the number of missing values per column\nrain_data.isnull().sum()","cdfdc991":"# visualizing missing data\nimport missingno as msno\nmsno.matrix(rain_data)","ab6ae256":"# plotting the number of rows with entries per column\nmsno.bar(rain_data)","6321108b":"msno.heatmap(rain_data)","99d4f702":"# drop rows without targets, raintoday, and rainfall entries\nrain_data_clean = rain_data.dropna(axis=0,how='any',subset=[\"RainTomorrow\", \"Rainfall\", \"RainToday\"])\n","b7512ac5":"# checking the number of missing values we now have;\nrain_data_clean.isnull().sum()","2aaaea8a":"# separating the target variables from the features\nX = rain_data_clean.drop(columns = \"RainTomorrow\")\ny = rain_data_clean.loc[:,\"RainTomorrow\"]\nprint (\"The size of X is {}\".format(X.shape))\nprint (\"The size of y is {}\".format(y.shape))","b24d337a":"# importing train_test_split\nfrom sklearn.model_selection import train_test_split","9ec02232":"# splitting the dataset, and using the \"stratify\" argument to preserve the class ratio in the train and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42, stratify=y)","68f93e8d":"def season_replace(df):\n    import datetime as dt\n#     initialize empty list of monthsmonth \n    month = []\n    for num in df['Date']:\n#         get the year, month, day per entry\n        date_obj = dt.datetime.strptime(num,\"%Y-%m-%d\")\n#         get the month only\n        date_mon = date_obj.month\n#     add month to the series of months\n        month.append(date_mon)\n#     initialise the seasons and let their indexes correspond with the month of the year\n#     i.e. Jan, Feb, Mar correspond to index 0, 1, 2 which are Summer, Summer, Autumn based on seasons\n    season_options = ['Summer', 'Summer', 'Autumn', 'Autumn', 'Autumn', 'Winter', 'Winter', 'Winter', 'Spring', 'Spring', 'Spring','Summer']\n#     intialize empty list of seasons\n    seasons= []\n    for i in month:\n#         add the season for each date entry to the seasons list\n        seasons.append(season_options[i-1])\n#     Drop the date column (it is the first column, index is 0)\n    n = df.columns[0]\n    df.drop(n, axis = 1, inplace = True)\n#     add seasons to the dataframe\n    df['Seasons'] = seasons\n#     re-order the dataframe to start with the seasons column\n    df = df[['Seasons'] +  [col for col in df.columns if col != 'Seasons']]\n    return df","9829d31b":"# replacing with the corresponding season\nX_train = season_replace(X_train)","df679cf2":"X_train.head()","cffa6e57":"features = X_train.columns\nfeatures_to_encode = X_train.select_dtypes(include=['object', 'bool']).columns\nfeatures_to_scale = X_train.select_dtypes(include=['int64', 'float64']).columns","6dc7ca4b":"# importing relevant packages\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\n\nfrom sklearn.impute import SimpleImputer","304ef7d9":"# instantiate one hot encoder to use\nencoder = OneHotEncoder(handle_unknown='error', drop='first', sparse='True')\n# setting up categorical pipeline\ncat_transformer = Pipeline(steps=[('onehot', encoder)])","c9d76323":"# instantiate imputer and scalar for numeric variables\nimputer = SimpleImputer(missing_values = np.nan, strategy=\"median\")\nscaler = RobustScaler()\n# setting up the numerical pipeline\nnum_transformer = Pipeline(steps = [\n    ('imputer', imputer),\n    ('scaler', scaler)\n])","602fb63a":"# combining both the numerical and categorical pipeline into a ColumnTransformer instance\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', num_transformer, features_to_scale),\n        ('cat', cat_transformer, features_to_encode)\n    ],remainder='passthrough')","dec206b2":"# importing Random Forest Classifie\nfrom sklearn.ensemble import RandomForestClassifier\n\n# instantiating the classifier\nrf_classifier = RandomForestClassifier(\n                      min_samples_leaf=50,\n                      n_estimators=150,\n                      bootstrap=True,\n                      oob_score=True,\n                      n_jobs=-1,\n                      random_state=42,\n                      max_features='auto')","16497829":"# Encoding the dependent variable\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny_train = le.fit_transform(y_train)","7663eb99":"pipe = make_pipeline(preprocessor, rf_classifier)\npipe.fit(X_train, y_train)","2ebe5a05":"# Replace Dates with season in test data\nX_test = season_replace(X_test)\nX_test.head()","23a839d1":"# Label encode y_test\ny_test = le.transform(y_test)","1ff1b200":"y_pred = pipe.predict(X_test)","edb669fa":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, roc_auc_score, roc_curve, f1_score\n\nimport matplotlib.pyplot as plt","3f02d4a6":"acc = accuracy_score(y_test, y_pred)","833782ed":"print(\"The accuracy of the model is {}%\".format(round(acc * 100,3)))","556c0176":"train_probs = pipe.predict_proba(X_train)[:, 1]\ntest_probs = pipe.predict_proba(X_test)[:, 1]\ntrain_pred = pipe.predict(X_train)","822da87b":"print(\"Train ROC AUC Score: {}\".format(roc_auc_score(y_train,train_probs)))\nprint(\"Test ROC AUC Score: {}\".format(roc_auc_score(y_test,test_probs)))","f18c5734":"def evaluate_model(y_pred, test_probs, train_pred, train_probs, y_train):\n    \n    baseline = {}\n    baseline['recall'] = recall_score(y_test,\n                        [1 for _ in range(len(y_test))])\n    baseline['precision'] = precision_score(y_test,\n                            [1 for _ in range(len(y_test))])\n    baseline['roc'] = 0.5\n    \n    results = {}\n    results['recall'] = recall_score(y_test, y_pred)\n    results['precision'] = precision_score(y_test, y_pred)\n    results['roc'] = roc_auc_score(y_test, test_probs)\n    \n    train_results = {}\n    train_results[\"recall\"] = recall_score(y_train, train_pred)\n    train_results['precision'] = precision_score(y_train, train_pred)\n    train_results['roc'] = roc_auc_score(y_train, train_probs)\n\n    for metric in ['recall', 'precision', 'roc']:\n        print('{} \\n Baseline: {} \\n Test: {} \\n Train: {}'.format(metric.capitalize(),round(baseline[metric], 2),round(results[metric], 2),round(train_results[metric], 2)))\n              \n#     calculate the  FPR and TPR\n    base_fpr, base_tpr, _ = roc_curve(y_test, [1 for _ in range(len(y_test))])\n    model_fpr, model_tpr, _ = roc_curve(y_test, test_probs)\n              \n    plt.figure(figsize = (8,6))\n    plt.rcParams['font.size'] = 16\n    \n#     Plot both curves\n    plt.plot(base_fpr, base_tpr, 'b', label='baseline')\n    plt.plot(model_fpr, model_tpr, 'r', label='model')\n    plt.legend();\n    \n    plt.xlabel('False Positive Rate');\n    plt.ylabel('True Positive Rate');\n    plt.title('ROC Curves');\n    plt.show()","29540f0d":"evaluate_model(y_pred, test_probs, train_pred, train_probs, y_train)","a6ad4908":"import itertools\n\ndef plot_confusion_matrix (cm, classes, normalize=False, title='Confusion Matrix', cmap = plt.cm.Blues):\n    \n    plt.figure(figsize = (10, 10))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, size = 24)\n    plt.colorbar (aspect = 4)\n    \n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45, size=14)\n    plt.yticks(tick_marks, classes, size = 14)\n    \n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    \n#     Label the plot\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                fontsize=20,\n                horizontalalignment='center',\n                color='white' if cm[i, j] > thresh else \"black\")\n        \n        plt.grid(None)\n        plt.tight_layout()\n        plt.ylabel ('True label', size = 18)\n        plt.xlabel ('Predicted label', size = 18)","4b1fb6a0":"cm = confusion_matrix(y_test, y_pred)\nplot_confusion_matrix(cm, classes=['0 - No Rain', '1 - Rain'],\n                     title = 'Rainfall Confusion Matrix')","439859b2":"from imblearn.over_sampling import SMOTE\nsm = SMOTE(random_state = 42)\n# pipe = make_pipeline(preprocessor, rf_classifier)\n# first we fit and transform the training data using the preprocessor transformer instance\n# this ensures that the categorical variables are encoded before the sampling takes place\nX_train_new = preprocessor.fit_transform(X_train)\nX_train_new, y_train_new = sm.fit_resample(X_train_new, y_train)","36484f61":"pipe_smote = make_pipeline(rf_classifier)\npipe_smote.fit(X_train_new, y_train_new)","4b3c1a8a":"X_test_new = preprocessor.fit_transform(X_test)","ba2067c5":"y_pred_new = pipe_smote.predict(X_test_new)","8708de58":"acc_smote = accuracy_score(y_test, y_pred_new)\nprint(\"The accuracy of the smote_model is {}%\".format(round(acc_smote * 100,3)))","395bc283":"train_probs_new = pipe_smote.predict_proba(X_train_new)[:, 1]\ntest_probs_new = pipe_smote.predict_proba(X_test_new)[:, 1]\ntrain_pred_new = pipe_smote.predict(X_train_new)","a52e0254":"evaluate_model(y_pred_new, test_probs_new, train_pred_new, train_probs_new, y_train_new)","54fc52c3":"cm_smote = confusion_matrix(y_test, y_pred_new)\nplot_confusion_matrix(cm_smote, classes=['0 - No Rain', '1 - Rain'],\n                     title = 'Rainfall Confusion Matrix [Smote]')","4abdc0d9":"# this package prints out data in a pretty format\nfrom pprint import pprint\n\n# let's see the current parameters in use\nprint('Parameters currently in useL\\n')\npprint(rf_classifier.get_params())","16f93b73":"from sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [int (x) for x in np.linspace(start=100, stop=700,num=50)]\n\n# number of features to consider at every split\nmax_features = ['auto', 'log2'] \n\n# maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n\n# include None in max_depth\nmax_depth.append(None)\n\n# minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n\n# minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 4, 10]\n\n# method of selecting samples for training each tree\nbootstrap = [True]\n\nmax_leaf_nodes = [None] + list(np.linspace(10, 50, 500).astype(int))\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'max_leaf_nodes': max_leaf_nodes,\n               'bootstrap': bootstrap}","7abaea7b":"rf = RandomForestClassifier(oob_score=True, n_jobs=-1)\n\n# creating a grid of hyperparameters\nrf_random = RandomizedSearchCV(\n                estimator = rf,\n                param_distributions = random_grid,\n                n_iter = 5, cv = 3,\n                verbose=1, random_state=42,\n                scoring='roc_auc')\n\n# next, we define a pipeline instance that takes fits each model gotten from the grid search\n# onto the training data\npipe_random = make_pipeline(rf_random)\npipe_random.fit(X_train_new, y_train_new)\n\n# return the hyperparameters of the best model\nrf_random.best_params_","17a3486a":"best_model = rf_random.best_estimator_\nn_nodes = []\nmax_depths = []\n\nfor ind_tree in best_model.estimators_:\n    n_nodes.append(ind_tree.tree_.node_count)\n    max_depths.append(ind_tree.tree_.max_depth)\n    \nprint ('Average number of nodes: {}'.format(int(np.mean(n_nodes))))\nprint ('Average maximum depth: {}'.format(int(np.mean(max_depths))))","aed7a870":"pipe_best = make_pipeline(best_model)\npipe_best.fit(X_train_new, y_train_new)\ny_pred_best = pipe_best.predict(X_test_new)","e4ce137b":"train_rf_probs_best = pipe_best.predict_proba(X_train_new)[:, 1]\ntest_rf_probs_best = pipe_best.predict_proba(X_test_new)[:, 1]\ntrain_rf_pred_best = pipe_best.predict(X_train_new)","46dc9ca2":"acc_best = accuracy_score(y_test, y_pred_best)\nprint(\"The accuracy of the smote_model is {}%\".format(round(acc_best * 100,3)))","60062175":"evaluate_model(y_pred_best, test_rf_probs_best, train_rf_pred_best, train_rf_probs_best, y_train_new)","adcf6192":"cm_best_model = confusion_matrix(y_test, y_pred_best)\nplot_confusion_matrix(cm_best_model, classes=['0 - No Rain', '1 - Rain'],\n                     title = 'Rainfall Confusion Matrix')","08f7d2da":"<a class=\"anchor\" id=\"8\"><\/a>\n# **8. Tuning Hyperparameters**\n\n[Table of Contents](#0.1)","b708d9ba":"<a class=\"anchor\" id=\"0.1\"><\/a>\n# **Table of Contents** \n\n1. [Background](#1)\n2. [The Data](#2)\n3. [Data Preprocessing](#3)\n4. [Feature Engineering](#4)\n5. [Training the Model](#5)\n6. [Evaluating the Model](#6)\n7. [Dealing with Class Imbalance](#7)\n8. [Tuning Hyperparameters](#8)\n9. [Conclusion](#9)\n\n\n\n\n\n","1ce97899":"<a class=\"anchor\" id=\"8\"><\/a>\n# **9. Conclusion** \n\n[Table of Contents](#0.1)","c189e41b":"We see that there are 7 categorical variables while the rest are numerical. Great.\n\nNow let's do some preprocessing of our data, and try to clean it up a bit","d8ec1c2c":"According to [TripSavvy](https:\/\/www.tripsavvy.com\/australian-seasons-1462601#:~:text=To%20break%20things%20down%20for%20you%2C%20each%20of,to%20August%2C%20and%20spring%20from%20September%20to%20November), Australia has four seasons categorised into months as follows:\n<ol>\n <li> Summer : December - February <\/li>\n <li> Autumn : March - May <\/li>\n <li> Winter : June - August <\/li>\n <li> Spring : September - November <\/li>\n<\/ol>\n \nAs such, we will replace the entries in the Date column with the corresponding season. This might help us get some insight as rainfall tends to be seasonal.","5f4072b4":"Instantiating a new pipeline to train the resampled dataset","6e3f1dfd":"#### Confusion Matrix\n\nNext, let's plot a pretty confusion matrix for some more insight into our model performance","a2c370f3":"### Missing Values\n\nLet's examine our missing value problem more squarely","9e860999":"<a class=\"anchor\" id=\"0\"><\/a>\n# **Rainfall in Australia** \n\n\n<img src=https:\/\/thumbs.dreamstime.com\/b\/trees-whatipu-point-huia-bay-auckland-new-zealand-march-two-tall-green-windswept-shoreline-under-heavy-cloudy-sky-91689726.jpg> \n\n\n\n\n","f5e2f562":"# **1. Background** <a class=\"anchor\" id=\"1\"><\/a>\n\n[Table of Contents](#0.1)\n\nOur objective is to predict whether or not rain will fall the next day in Australia. This knowledge might be relevant for several reasons, and some of them are listed below:\n\n- To help decide if you should head out with your umbrella or not\n- To know what kind of clothes would be suitable\n- To know if additional preparations are needed to ensure an outdoor date or event goes smoothly\n\nWhatever the case may be, we would try to make sense of the data we have to inform our predictions.\n\nHere we go.\n\n","1bb920d1":"## Preprocessing the test data\nFirst, we replace the dates with their corresponding season","a613ec09":"We will be using a Random Forest Classifier model for this probem","6855d33b":"<a class=\"anchor\" id=\"4\"><\/a>\n# 4. **Feature Engineering**\n\n[Table of Contents](#0.1)","db5cb6ba":"Next, we checj the average number of nodes and maximum depths in our best random forest classifier","024b1bb7":"Next, we will create a transformer object through which we will pass the encoder, and scaler","43dc2ac1":"We have been able to make predictions on whether rain will fall in Australia the next day with an accuracy of 79%. Our recall score shows was optimized over the precision score because we'd rather have a situation where we were wrong to predict rainfall than one where we were wrong to predict no rainfall. \n\nThank you.","f42eb6c4":"Next, we label the target test variable accordingly.","1fb9ae93":"Next, we train the model using the training dataset. The pipe object makes it easy to pass data through a series of processes that happene one after the other. Remember that the preprocessor object was defined for the imputing of missing values and standardization of out data while the rf_classifier is our chosen model.","8440262d":"<a class=\"anchor\" id=\"3\"><\/a>\n# 3. **Data Preprocessing** \n\n[Table of Contents](#0.1)","65fc91aa":"<a class=\"anchor\" id=\"5\"><\/a>\n# 5. **Training the Model** \n\n[Table of Contents](#0.1)","f561bae0":"Next, we will encode the data using One Hot Encoder. First, we create a list of the categorical variables to encode, and numerical variables to standardize.","f6e7a32b":"## Evaluating the Classifier","8eb37ffc":"Kindly upvote if you found it interesting or helpful. Also, I'd very much appreciate any comments and feedback!","630ba5ae":"Using SMOTE to remedy the class imbalance","4a472393":"Next, let's see our ROC and AUC performance","848715b6":"Next, let's see what our confusion matrix looks like","bace5ccf":"Next we label encode the target variable as it is currently a catrgorical data type","ca5d6b01":"Next, we make predictions using the new model","91a4e499":"Next, we evaluate the best model","a40b1a8b":"<a class=\"anchor\" id=\"7\"><\/a>\n# 7. **Dealing with Class Imbalance**\n\n[Table of Contents](#0.1)","933d53f4":"Let's see our model acccuracy","e0af27cc":"Next, we predict our target classses","cb56356d":"#### Probability Predictions","cdec30bf":"Next, we prepare the test dataset for prediction","1fac7319":"And of course, the confusion matrix","d04d6920":"From this, we can see that the model is not doing so well to correctly predict that it would rain the next day. In fact you can infer this from the low recall score we had (0.44 and 0.43 for the test and training sets respectively). The precision score, on the other hand was pretty high (0.78 and 0.81 for the test and training sets respectively.\n\nThis means that when our model predicts rainfall, it is more likely to rain than otherwise. However, we would also run issues, because there are a good number of cases where it predicts an absense of rainfall, and it actually rains. Having a low recall score in this case or a high number of false negatives is not desirable.\n\nOne reason why this is the case could be that the dataset is imbalanced, i.e. there are way more instances of the \"No rain\" class than the \"Rain\" class.\n\nLet's attempt to use SMOTE to sample the dataset and improve our model's predictive performance","12a9a7c0":"<a class = \"anchor\" id=\"2\"><\/a>\n# 2. **The Data**\n\n[Table of Contents](#0.1)\n\nLet's take a look at our data and get working\n","cbfe6b4a":"To plot the ROC curve, let's define a function that takes in all the necessary arguments and returns the ROC Curve as well as the precision and recall metrics","0a5d8810":"Next, let's evaluate our model's recall, precison, and roc_auc_score","4885a5c8":"We can observe some improvements to our model. The number of false negatives have reduced (so we can expect our recall score to improve). However, the number of false positives have also increased (so our precision has dropped). But that's fine. It is much better to be wrong about rain falling than about rain not falling. ","9617f237":"Let's see how well our does with predicting the target class for out test dataset.\n\nRemember, that we are to take the test data through all the preprocessing and feature engineering processes our training set went through.","da4980b6":"### Splitting the data into training and split sets\n\nLest we  fall victim to the silent killer called data leakage, let's split our data into training and test sets","d3705a39":"Let's see the categorical and numeriacal columns we have in our data","6c6a10df":"Next, create a grid of parameters for the model to randomly pick and train","107d9d0c":"We can see that the recall, precision, and auc score for the train and test sets are pretty close to each other. This suggests it is unlikely that our model is being overfitted","a4605b1d":"Some visualization might be useful, let's see","da39e678":"<a class=\"anchor\" id=\"6\"><\/a>\n# 6. **Evaluating the Model**\n\n[Table of Contents](#0.1)","340f4524":"### Replacing Dates with Seasons","75b84a49":"A cursory look at the first five rows of our data reveals that some columns have missing values. We will try to resolve this later, but let's do some more inspection on our dataset","e19d5d0a":"From the plot above and also the data printed out, there are columns with missing values. \"Sunshine\", \"Evaporation\", \"Cloud9am\", and \"Cloud3am\" in particular have a significant number of missing values. According to the definition of these columns, they seem to be important features.\n\nYou would observe that the target column (RainTomorrow) contains some missing values. We will drop row entries without targets. You would also observe that the correlation between RainToday and Rainfall is high, and that they both even have the same number of missing values. I'll keep both (no pressure), but drop rows with missing values. Later on, the missing values in other columns will be replaced."}}