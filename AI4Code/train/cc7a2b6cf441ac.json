{"cell_type":{"6a75c059":"code","351c7345":"code","62d80d7d":"code","8f293ab0":"code","d60a545d":"code","8050c12c":"code","cfe2bdc7":"code","d6c7e2ea":"code","605eac92":"code","b3e2f4ce":"code","d7cf6612":"code","1fbe4f1f":"code","97296d70":"code","214c2469":"code","3f1c7fac":"code","d96f9ae6":"code","b4a2ba98":"code","0b0ea7cd":"code","43276ffe":"code","74983f32":"code","b96d7a77":"code","2661289f":"code","31f0e2d9":"code","3747aa3c":"code","aa8b5db6":"code","e4f2486b":"code","1da8dd5b":"markdown","a4fc2f96":"markdown","ae65b265":"markdown","12e573d9":"markdown","1d81e605":"markdown","b88fbda4":"markdown","d0dc56c7":"markdown","9d1db724":"markdown","a43b271a":"markdown","39ec1e3b":"markdown"},"source":{"6a75c059":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","351c7345":"import pandas as pd\nimport numpy as np\nimport time\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import f1_score, roc_auc_score, roc_curve, precision_recall_curve, auc, make_scorer, recall_score, accuracy_score, precision_score, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV","62d80d7d":"df = pd.read_csv('..\/input\/creditcard.csv')\ndf.head()","8f293ab0":"df['Class'].value_counts()","d60a545d":"print(\"No Fruads\",round(df[\"Class\"].value_counts()[0]\/len(df)*100,2),'% of the dataset')\nprint(\"Fruads\",round(df[\"Class\"].value_counts()[1]\/len(df)*100,2),'% of the dataset')","8050c12c":"import pandas_profiling\npandas_profiling.ProfileReport(df)","cfe2bdc7":"df.isna().sum()","d6c7e2ea":"y=df['Class']\nX=df.drop(['Class'], axis=1)\n\nX_train, X_test, y_train, y_test=train_test_split(X, y,test_size=0.2, random_state=0)\nprint('X_train.shape:', X_train.shape)\nprint('y_train.shape:', y_train.shape)","605eac92":"X_train.head()","b3e2f4ce":"scaler=preprocessing.MinMaxScaler().fit(X_train[['Time','Amount']])\nprint(scaler.data_max_)","d7cf6612":"X_train[['Time','Amount']]=scaler.transform(X_train[['Time','Amount']])","1fbe4f1f":"print(X_train[['Time','Amount']].head())","97296d70":"X_test[['Time','Amount']]=scaler.transform(X_test[['Time','Amount']])","214c2469":"logreg=LogisticRegression()\n#fit the model with data\nlogreg.fit(X_train, y_train)\n#predict on test set\ny_pred=logreg.predict(X_test)","3f1c7fac":"cm=metrics.confusion_matrix(y_test,y_pred)\ncmDF=pd.DataFrame(cm, columns=['pred_0','pred_1'],index=['true_0','true_1'])\nprint(cmDF)\nprint('recall=', float(cm[1,1])\/(cm[1,0]+cm[1,1]))\nprint('precision=', float(cm[1,1])\/(cm[1,1]+cm[0,1]))","d96f9ae6":"# simple rf\nclassifier_RF=RandomForestClassifier(random_state=0)\nclassifier_RF.fit(X_train, y_train)\n\n# predict class labels 0\/1 for the test set\npredicted=classifier_RF.predict(X_test)\n\n#generate class probabilities\nprobs=classifier_RF.predict(X_test)\n\n# generate evaluation metics\nprint('%s: %r' % ('accuracy_score is:', accuracy_score(y_test, predicted)))\n#print('%s: %r' % ('roc_auc_score is:', roc_auc_score(y_test, probs[:, 1])))\nprint('%s: %r' % ('f1_score is:', f1_score(y_test, predicted)))\n\nprint('confusion_matrix is:')\ncm=confusion_matrix(y_test, predicted)\ncmDF=pd.DataFrame(cm, columns=['pred_0','pred_1'],index=['true_0','true_1'])\nprint(cmDF)\nprint('recall=', float(cm[1,1])\/(cm[1,0]+cm[1,1]))\nprint('precision=', float(cm[1,1])\/(cm[1,1]+cm[0,1]))","b4a2ba98":"smote=SMOTE(random_state=12)\nx_train_sm, y_train_sm=smote.fit_sample(X_train, y_train)\nunique, counts=np.unique(y_train_sm, return_counts=True)\nprint(np.asarray((unique, counts)).T)","0b0ea7cd":"# simple rf\nclassifier_RF_sm=RandomForestClassifier(random_state=0)\nclassifier_RF_sm.fit(X_train, y_train)\n\n# predict class labels 0\/1 for the test set\npredicted_sm=classifier_RF.predict(X_test)\n\n#generate class probabilities\nprobs_sm=classifier_RF.predict(X_test)\n\n# generate evaluation metics\nprint('%s: %r' % ('accuracy_score_sm is:', accuracy_score(y_test, predicted)))\n#print('%s: %r' % ('roc_auc_score_sm is:', roc_auc_score(y_test, probs[:, 1])))\nprint('%s: %r' % ('f1_score_sm is:', f1_score(y_test, predicted)))\n\nprint('confusion_matrix_sm is:')\ncm_sm=confusion_matrix(y_test, predicted_sm)\ncmDF_sm=pd.DataFrame(cm_sm, columns=['pred_0','pred_1'],index=['true_0','true_1'])\nprint(cmDF_sm)\nprint('recall or sens_sm=', float(cm_sm[1,1])\/(cm_sm[1,0]+cm_sm[1,1]))\nprint('precision_sm=', float(cm_sm[1,1])\/(cm_sm[1,1]+cm_sm[0,1]))","43276ffe":"# evaluated metrics to be calculated for each combination of parameters and cv\nscorers={\n    'precison_score':make_scorer(precision_score),\n    'recall_score':make_scorer(recall_score),\n    'f1_score':make_scorer(f1_score,pos_label=1)\n}\n\ndef grid_search_wrapper(model, parameters, refit_score='f1_score'):\n    grid_search=GridSearchCV(model, parameters, scoring=scorers, refit=refit_score,\n                            cv=3, return_train_score=True, n_jobs=-1)\n    grid_search.fit(X_train, y_train)\n    \n    #make predictions\n    y_pred=grid_search.predict(X_test)\n    y_prob=grid_search.predict_proba(X_test)[:, 1]\n    \n    print('Best params for{}'.format(refit_score))\n    print(grid_search.best_params_)\n    \n    #confusion matrixs on test data\n    print('\\nConfusion matrix of Random Forest optimized for {} on the test data:'.format(refit_score))\n    cm=confusion_matrix(y_test, y_pred)\n    cmDF=pd.DataFrame(cm, columns=['pred_0','pred_1'],index=['true_0','true_1'])\n    print(cmDF)\n    \n    print('recall=', float(cm[1,1])\/(cm[1,0]+cm[1,1]))\n    print('precision=', float(cm[1,1])\/(cm[1,1]+cm[0,1]))\n    \n    return grid_search","74983f32":"# C: inverse of regularization strength, smaller values specify strong regularization\nLRGrid={'C': np.logspace(-2,2,5),'penalty':['l1','l2']} #l1 lasso\n#param_grid=\nlogRegModel=LogisticRegression(random_state=0)\n\ngrid_search_LR_f1=grid_search_wrapper(logRegModel, LRGrid, refit_score='f1_score')","b96d7a77":"#optimize on f1_score on RF\n\"\"\"\"parameters={\n    'n_estimators': [10, 150],\n    'class_weight':[{0:1, 1:w} for w in [0.2, 1, 100]]\n}\nclf=RandomForestClassifier(random_state=0)\ngrid_search_rf_f1=grid_search_wrapper(clf, parameters, refit_score='f1_score')\"\"\"","2661289f":"\"\"\"\"best_rf_model_f1=grid_search_rf_f1.best_estimator_\nbest_rf_model_f1\"\"\"","31f0e2d9":"\"\"\"\nresults_f1=pd.DataFrame(grid_search_rf_f1.cv_results_)\nresults_sortf1=results_f1.sort_values(by='mean_test_f1_score',ascending=False)\nresults_sortf1[['mean_test_precision_score', 'mean_test_recall_score', 'mean_test_f1_score', 'mean_train_precision_score', 'mean_train_recall_score', 'mean_train_f1_score', 'param_max_depth','param_class_weight','param_n_estimators']].round(3).head()\n\"\"\"\"","3747aa3c":"#variable importance\n#pd.DataFrame(best_rf_model_f1.feature_importances_, index=X_train.columns, columns=['importance']).sort_values('importance',ascending=False)","aa8b5db6":"# Optimize recall_score on RF\n#grid_search_rf_recall=grid_search_wrapper(clf, parameters, refit_score='recall_score')","e4f2486b":"#best_RF_model_recall=grid_search_rf_recall.best_estimator_\n#best_RF_model_recall","1da8dd5b":"SMOTE SAMPLING","a4fc2f96":"RF on smoted training data","ae65b265":"Normalization","12e573d9":"Transform the training data and use them for the model training","1d81e605":"before the prediction of the test data, apply the same scaler obtained from above X_test, not fitting a brandnew scaler on test set","b88fbda4":"Simple Logistic Regression, using default parameter","d0dc56c7":"Any missing value? Outlier?","9d1db724":"train data","a43b271a":"whether imbalance? 0:non-fraud, 1:fraud","39ec1e3b":"parameter tuning by GridSearchCV"}}