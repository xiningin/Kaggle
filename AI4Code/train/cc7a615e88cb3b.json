{"cell_type":{"db300e6b":"code","1bacf19b":"code","c554dd04":"code","38775c32":"code","08ddcb4b":"code","1006390e":"code","176c2dcc":"code","502edc38":"code","51e6fa6b":"code","33074943":"code","276a9dde":"code","92444828":"code","85909f1f":"code","5e3c8e68":"code","62cdcf5c":"code","8db812a9":"code","b8d236f7":"code","61d432a6":"code","a64094a5":"code","1e017c62":"code","c89d8ad1":"code","28a32f2e":"code","e8c37473":"code","b3940841":"code","254f2ab8":"code","37a932c1":"code","3a6b62cd":"code","87d190da":"code","8c603fa4":"code","f7ea211e":"code","a4599d7e":"code","fdccf67a":"code","ddee120f":"code","e766171d":"code","12546954":"code","3d4d9532":"code","87543039":"code","3ab8abcb":"code","1c6b7761":"code","14f1d03e":"code","058d314f":"code","3a99663f":"code","b134d382":"code","f55c976e":"code","426d7ab8":"code","3b629dd9":"markdown","7d191b2a":"markdown","c1d0a455":"markdown","145e2359":"markdown","cc2e0cf7":"markdown","7d92163b":"markdown","ec69cf45":"markdown","6987a839":"markdown","fa29334c":"markdown","060891f0":"markdown","e1ad8b6a":"markdown","b1ef631b":"markdown","4c25bd21":"markdown","1cd7c583":"markdown","825ecd83":"markdown","75a38ca0":"markdown","acf7fa40":"markdown","71d3d9a4":"markdown","879976d5":"markdown","8f532c67":"markdown","036d6076":"markdown","a41e475f":"markdown","0c3aede6":"markdown","96c87888":"markdown","1370b540":"markdown","02b615af":"markdown","154b2ca8":"markdown","e801054f":"markdown","602b4b97":"markdown","f9a76d7b":"markdown","f0cfd857":"markdown","944ab375":"markdown","0479f6ca":"markdown","3ecf1ab3":"markdown","f9031945":"markdown","1fcc4136":"markdown","11185f1c":"markdown","aa7049f8":"markdown","954da752":"markdown","804b57d4":"markdown"},"source":{"db300e6b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as pltgs\nimport torch\nimport keras\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D, AveragePooling2D\nfrom keras.layers import Dense, Flatten\nfrom keras.models import Sequential\nfrom keras import backend as K\nfrom keras.utils import to_categorical","1bacf19b":"class Plot():\n    def __init__(self, width=10, height=4, cmap='magma', size=12, bot=0.1):\n        self.width = width\n        self.height = height\n        self.cmap = cmap\n        self.size = size\n        self.fig = plt.figure(figsize=(self.width, self.height))\n        self.bot = bot\n\n    def _network_dict(self, network_id):\n        c = 'Convolution\\n+ '\n        dictionary = {\n            0: [c + 'ReLU', r'Max Pooling $(2 \\times 2)$'],\n            1: [c + 'Tanh', r'Max Pooling $(2 \\times 2)$'],\n            2: [c + 'ReLU', r'Average Pooling $(2 \\times 2)$'],\n            3: [c + 'Tanh', r'Average Pooling $(2 \\times 2)$'],\n            4: [c + 'Tanh', r'Max Pooling $(2 \\times 2)$', c + 'ReLU'],\n            5: [c + 'ReLU', r'Max Pooling $(2 \\times 2)$', c + 'ReLU',\n                c + 'ReLU', r'Max Pooling $(2 \\times 2)$']\n        }\n        return dictionary[network_id]\n\n    def _no_ticks(self, ax):\n        ax.set_xticks([])\n        ax.set_yticks([])\n        return None\n\n    def _get_lims(self, ax):\n        return np.array(ax.get_xlim()), np.array(ax.get_ylim())\n\n    def _shape_label(self, x, xlim, ylim):\n        _shape = tuple([x.shape[i] for i in [1, 2, 0]])\n        plt.text(np.mean(xlim), np.max(ylim)*1.05, _shape,\n                 ha='center', va='top', size=self.size)\n        return None\n\n    def _level_locs(self, levels):\n        x1 = [1 \/ (levels * 4)]\n        x1 += [(i + 0.8) \/ levels for i in range(1, levels-1)]\n        x2 = [(i + 0.7) \/ levels for i in range(1, levels-1)]\n        x2 += [(levels - 0.25) \/ levels]\n        return np.array(x1), np.array(x2)\n\n\n    def _network_desc(self, levels, network_id):\n        x1, x2 = self._level_locs(levels)\n        labels = self._network_dict(network_id)\n        gs = pltgs.GridSpec(1, 1, left=0, right=1, top=self.bot, bottom=0)\n        ax = self.fig.add_subplot(gs[0,0])\n        for i in range(len(x1)):\n            ax.plot([x1[i], x1[i]], [0.8, 0.7], c='k')\n            ax.plot([x1[i], x2[i]], [0.7, 0.7], c='k')\n            ax.plot([x2[i], x2[i]], [0.8, 0.7], c='k')\n            ax.text((x1[i] + x2[i]) \/ 2, 0.55, labels[i],\n                     ha='center', va='top', size=self.size)\n        ax.set_ylim(0, 1)\n        ax.set_xlim(0, 1)\n        ax.axis('off')\n        return None\n\n    def _conv_desc(self, levels, activation):\n        x1, x2 = self._level_locs(levels)\n        label = 'Convolution'\n        if len(activation) > 0:\n            label += f'\\n+ {activation}'\n        gs = pltgs.GridSpec(1, 1, left=0, right=1, top=1, bottom=0)\n        ax = self.fig.add_subplot(gs[0,0])\n        for i in range(len(x1)):\n            ax.plot([0.27, 0.45], [0.50, 0.50], c='k')\n            ax.plot([0.45, 0.44], [0.50, 0.52], c='k')\n            ax.plot([0.45, 0.44], [0.50, 0.48], c='k')\n            ax.text((0.27 + 0.45) \/ 2, 0.47, label, ha='center', va='top',\n                    size=self.size)\n        ax.set_ylim(0, 1)\n        ax.set_xlim(0, 1)\n        ax.axis('off')\n        return None\n\n    def _plot_input(self, x_input, levels):\n        gs = pltgs.GridSpec(1, 2, left=0, right=1\/levels, top=1,\n                            bottom=self.bot)\n        ax = self.fig.add_subplot(gs[0,0])\n        ax.imshow(x_input[0,0], cmap=self.cmap, aspect='equal')\n        xlim, ylim = self._get_lims(ax)\n        plt.text(np.mean(xlim), -np.max(ylim)*0.05, 'Input',\n                 ha='center', va='bottom', size=self.size)\n        self._shape_label(x_input[0], xlim, ylim)\n        self._no_ticks(ax)\n        return None\n\n    def _network_layout(self, x_input, x_list):\n        input_size = x_input[0,0].shape[0]\n        xrng = np.arange(len(x_list))\n        x = [x_list[i][0,:,:,:] for i in xrng]\n        layers = [x[i].shape[0] for i in xrng]\n        size_ratio = [x[i].shape[1] \/ input_size for i in xrng]\n        ws = [(1 \/ ((layers[i] - 1) * size_ratio[i])) - 1 for i in xrng]\n        hs = [(1 \/ ((layers[i] - 1) * size_ratio[i])) - 1 for i in xrng]\n        return x, xrng, layers, ws, hs\n\n    def _plot_network(self, x_input, x_list, levels):\n        x, xrng, layers, ws, hs = self._network_layout(x_input, x_list)\n        for i in xrng:\n            gs = pltgs.GridSpec(layers[i], layers[i], left=(i+1)\/levels,\n                                right=(i+2)\/levels, top=1, bottom=self.bot,\n                                wspace=ws[i], hspace=hs[i])\n            for j in range(layers[i]):\n                ax = self.fig.add_subplot(gs[j,j])\n                ax.imshow(x[i][j], cmap=self.cmap, aspect='equal')\n                self._no_ticks(ax)\n            xlim, ylim = self._get_lims(ax)\n            self._shape_label(x[i], xlim, ylim)\n        return None\n\n    def network(self, x_input, x_list, activation='', network_id=0,\n                channels='first'):\n        if channels == 'last':\n            x_input = np.transpose(x_input, [0, 3, 1, 2])\n            x_list = [np.transpose(x, [0, 3, 1, 2]) for x in x_list]\n        levels = len(x_list) + 1\n        if levels > 2:\n            self._network_desc(levels, network_id)\n        else:\n            self.bot = 0\n            self._conv_desc(levels, activation)\n        self._plot_input(x_input, levels)\n        self._plot_network(x_input, x_list, levels)\n        return None","c554dd04":"def normalize(x):\n    return x.astype('float32') \/ 255\n\n\ndef plot_samples(x_train, y_train):\n    class_dict = np.arange(10)\n    sample_list = [x_train[y_train[:,i].astype(bool)][:12] for i in range(10)]\n    samples = np.concatenate(sample_list)\n    gs = pltgs.GridSpec(10, 12, hspace=-0.025, wspace=-0.025)\n    fig = plt.figure(figsize=(10, 8.5))\n    yloc = np.linspace(0.95, 0.05, 10)\n    for i in range(120):\n        ax = fig.add_subplot(gs[i\/\/12, i%12])\n        ax.imshow(samples[i,:,:,0], cmap='magma')\n        ax.set_xticks([])\n        ax.set_yticks([])\n    return None\n\n\ndef display_conv(x, conv, activation):\n    x1 = K.eval(conv(x))\n    Plot().network(K.eval(x), [x1], activation=activation, channels='last')\n    return None\n\n\ndef display_conv_pool(x, conv_list, network_id, height=5):\n    x_list = [conv_list[0](x)]\n    for i in range(1, len(conv_list)):\n        x_list += [conv_list[i](x_list[i-1])]\n    x_list = [K.eval(x_list[i]) for i in range(len(conv_list))]\n    Plot(height=height).network(K.eval(x), x_list, network_id=network_id,\n                                channels='last')\n    return None\n\n\ndef display_fc(model, x):\n    label = model.get_layer(index=-1).name\n    fc = K.eval(model(x))\n    plt.figure(figsize=(10, 0.5))\n    plt.imshow(fc, cmap='magma', aspect='auto')\n    plt.gca().set_xticks([])\n    plt.gca().set_yticks([])\n    xloc = np.array(plt.gca().get_xlim())\n    plt.text(xloc[0], -0.6, label, ha='left', va='bottom', size=12)\n    plt.text(np.mean(xloc), 0.7, fc.shape, ha='center', va='top', size=12)\n    return None","38775c32":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv').to_numpy()\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv').to_numpy()\n\nx_train = normalize(train[:,1:]).reshape(-1, 28, 28, 1)\nx_test = normalize(test).reshape(-1, 28, 28, 1)\ny_train = to_categorical(train[:,0])\n\nprint(f'  Train data   shape = {x_train.shape}')\nprint(f'   Test data   shape = {x_test.shape}')\nprint(f'Train labels   shape = {y_train.shape}')","08ddcb4b":"plot_samples(x_train, y_train)","1006390e":"x = x_train[39:40,:,:,:1]","176c2dcc":"x = np.transpose(x, [0, 3, 1, 2])\nx = torch.Tensor(x)\nprint(f'x = {x.shape}')","502edc38":"conv1 = torch.nn.Conv2d(in_channels=1, out_channels=3, kernel_size=3,\n                        stride=1, padding=0)","51e6fa6b":"conv2 = torch.nn.Conv2d(in_channels=1, out_channels=6, kernel_size=3,\n                        stride=1, padding=1)","33074943":"x1 = conv1(x).detach().numpy()\nx2 = conv2(x).detach().numpy()","276a9dde":"Plot().network(x, [x1], channels='first')","92444828":"Plot().network(x, [x2], channels='first')","85909f1f":"x = x.detach().numpy()\nx = np.transpose(x, [0, 2, 3, 1])\nx = K.constant(x)","5e3c8e68":"relu1 = Conv2D(filters=3, kernel_size=3, strides=1, padding='valid',\n               activation='relu')\nrelu2 = Conv2D(filters=6, kernel_size=3, strides=1, padding='same',\n               activation='relu')","62cdcf5c":"display_conv(x, relu1, activation='ReLU')","8db812a9":"display_conv(x, relu2, activation='ReLU')","b8d236f7":"softmax = Conv2D(filters=3, kernel_size=3, activation='softmax')","61d432a6":"display_conv(x, softmax, activation='Softmax')","a64094a5":"sigmoid = Conv2D(filters=3, kernel_size=3, activation='sigmoid')","1e017c62":"display_conv(x, sigmoid, activation='Sigmoid')","c89d8ad1":"tanh = Conv2D(filters=3, kernel_size=3, activation='tanh')","28a32f2e":"display_conv(x, tanh, activation='Tanh')","e8c37473":"elu = Conv2D(filters=3, kernel_size=3, activation='elu')","b3940841":"display_conv(x, elu, activation='ELU')","254f2ab8":"softplus = Conv2D(filters=3, kernel_size=3, activation='softplus')","37a932c1":"display_conv(x, softplus, activation='Softplus')","3a6b62cd":"softsign = Conv2D(filters=3, kernel_size=3, activation='softsign')","87d190da":"display_conv(x, softsign, activation='Softsign')","8c603fa4":"relu = Conv2D(filters=6, kernel_size=3, padding='same', activation='relu')\ntanh = Conv2D(filters=6, kernel_size=3, padding='same', activation='tanh')","f7ea211e":"maxpool = MaxPooling2D(pool_size=(2, 2))","a4599d7e":"display_conv_pool(x, [relu, maxpool], network_id=0)","fdccf67a":"display_conv_pool(x, [tanh, maxpool], network_id=1)","ddee120f":"avgpool = AveragePooling2D(pool_size=(2, 2))","e766171d":"display_conv_pool(x, [relu, avgpool], network_id=2)","12546954":"display_conv_pool(x, [tanh, avgpool], network_id=3)","3d4d9532":"relu = Conv2D(filters=6, kernel_size=3, padding='same', activation='relu')\ntanh = Conv2D(filters=6, kernel_size=3, padding='same', activation='tanh')","87543039":"display_conv_pool(x, [tanh, maxpool, relu], network_id=4)","3ab8abcb":"maxpool = MaxPooling2D(pool_size=(2, 2))\nrelu1 = Conv2D(filters=8, kernel_size=3, padding='same', activation='relu')\nrelu2 = Conv2D(filters=16, kernel_size=3, padding='same', activation='relu')\nrelu3 = Conv2D(filters=16, kernel_size=3, activation='relu')","1c6b7761":"display_conv_pool(x, [relu1, maxpool, relu2, relu3, maxpool],\n                  network_id=5)","14f1d03e":"model = Sequential()\nmodel.add(Conv2D(8, 3, padding='same', activation='relu',\n                 input_shape=(28, 28, 1), name='conv_1'))\nmodel.add(Conv2D(8, 3, padding='same', activation='relu', name='conv_2'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), name='maxpool_1'))\nmodel.add(Conv2D(16, 3, activation='relu', name='conv_3'))\nmodel.add(Conv2D(16, 3, activation='relu', name='conv_4'))\nmodel.add(Conv2D(16, 3, activation='relu', name='conv_5'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), name='maxpool_2'))","058d314f":"model.add(Flatten(name='flatten_1'))\ndisplay_fc(model, x)","3a99663f":"model.add(Dense(128, activation='relu', name='dense_1'))\ndisplay_fc(model, x)","b134d382":"model.add(Dense(128, activation='relu', name='dense_2'))\ndisplay_fc(model, x)","f55c976e":"model.add(Dense(10, activation='softmax', name='dense_3'))\ndisplay_fc(model, x)","426d7ab8":"model.summary()","3b629dd9":"## Stacking Layers","7d191b2a":"### Dense\nWe sequentially add to the model two ReLU-activated fully connected layers of size 128 and a Softmax-activated output layer with a length equal to the number of classes (in our case 10).","c1d0a455":"### Network Summary","145e2359":"Create class `Plot()` for notebook network plots:","cc2e0cf7":"The network can be summarized as follows:","7d92163b":"We apply the `network` function of object `Plot` to display the results of each convolution as follows:","ec69cf45":"### Sigmoid\n$$f(x) = \\sigma(x) = \\dfrac{1}{1 + e^{-x}}$$","6987a839":"**Read in data**:","fa29334c":"### Softsign\n$$f(x) = \\dfrac{x}{1 + |x|}$$","060891f0":"The first 12 samples from each class are shown below.","e1ad8b6a":"**Import modules**","b1ef631b":"# CNN Architecture\n\n![image.png](attachment:image.png)\n\nIn this notebook, we introduce various components of convolutional neural networks and how they can be used to assemble effective CNN classifiers. For more information about training CNN models, please see the following notebooks where we use ensemble CNN models to classify [digits](https:\/\/nbviewer.jupyter.org\/github\/cschupbach\/deep_learning_cnn\/blob\/master\/digits_ensemble.ipynb) and [fashion](https:\/\/nbviewer.jupyter.org\/github\/cschupbach\/deep_learning_cnn\/blob\/master\/fashion_ensemble.ipynb) items, achieving testing accuracies of 99.67% and 93.55%, respectively. Note that these examples utilize the MNIST datasets of the Keras distribution, where the `digits` dataset is composed of 60000 training samples and 10000 testing samples.","4c25bd21":"We use the `display_conv` function to display the resulting ReLU-activated convolutional layers as follows:","1cd7c583":"## Activation Functions","825ecd83":"Currently `x` is a NumPy array of shape $1 \\times 28 \\times 28 \\times 1$. For convolution using PyTorch, we need `x` to be a PyTorch tensor in *channels first* form (currently in *channels last*). We solve this requirement below.","75a38ca0":"### Average Pooling","acf7fa40":"Here we provide an example of convolution with ReLU activation using Keras. Because the Keras library uses *channels last* format (by default) for 2-dimensional convolution, we revert tensor `x` back into an array, transform to *channels last* format, and express the array as a Keras tensor. ","71d3d9a4":"# Citations\n\n1. C. Pelletier, G. Webb, and F. Petitjean, \"Temporal Convolutional Neural Network for the Classification of Satellite Image Time Series,\" *Remote Sensing*, vol. 11, no. 5, p. 523, 2019.","879976d5":"Most CNN architectures also apply an activation function to convolutional layers. We use activation functions to increase non-linearity in the network. In theory, the output layers of convolution with activation have a thresholding effect similar to that of action potentials in the biological firing of neurons.\n\n$$\\text{input} \\ \\ \\circledast \\ \\ \\text{conv}_{1} \\ + \\ \\text{activation}_1 \\ \\circledast \\ \\cdots \\ \\circledast \\ \\text{conv}_{N} \\ + \\ \\text{activation}_N \\longrightarrow \\ \\text{classification}$$","8f532c67":"The kernels of each convolution object convolve the input `x` as follows:","036d6076":"A common CNN feature learning architecture is similar to that shown above, where feature maps are constructed with ReLU-activated convolutional layers and intermittent max pooling. It's also fairly common to normalize the batch of input layers prior to convolution in the feature learning segment.","a41e475f":"### Softmax\n$$P(y=j| \\mathbf{x}) = \\dfrac{e^{\\mathbf{x}^\\mathsf{T}\\mathbf{w}_j}}{\\sum_{k = 1}^{k} e^{\\mathbf{x}^\\mathsf{T}\\mathbf{w}_k}}$$","0c3aede6":"We create two 2-dimensional convolution objects with ReLU activation, `relu1` and `relu2`, using the Keras function `Conv2D` below. The first convolution object `relu1` uses 3 kernel filters ($K=3$) and no zero padding ($P=0$) of the input border, while `relu2` uses 6 kernel filters ($K=6$) and adds one pixel of zero padding ($P=1$) to the input border. Both convolution objects have kernel size $F=3$ and stride $S=1$ (default). Note that the zero padding parameter is either `'valid'` (default), where $H_\\text{out}$ and $W_\\text{out}$ are according to equations (2) and (3) with $P=0$, or `'same'`, where $H_\\text{out} = H_\\text{in}$ and $W_\\text{out}=W_\\text{in}$.","96c87888":"### Exponential Linear Unit (ELU)\n$$f(\\alpha, x) = \\begin{cases} \\alpha (e^x - 1) & \\text{for } \\ x \\leq 0 \\\\\nx & \\text{for } \\ x \\gt 0 \\end{cases}$$","1370b540":"# Additional Layers and Functions\n\nMany additional layers and layer functions may be added to fine tune the base architecture of the CNN shown above based on the data being classified. While a few common additions are discussed below, we highly suggest reviewing documentation for the [Keras Layers API](https:\/\/keras.io\/api\/layers\/).\n\n## Batch Normalization\nWe use batch normalization to scale values of inputs to mean zero and unit variance. This helps account for a problem called covariate shift, where the distribution of data varies across training and testing batch inputs. For some datasets, batch normalization may increase efficiency by reducing the number of epochs required to effectively train the CNN. However, batch normalization may not be suitable in some instances due to an increase in the amount of time required to complete an epoch. Results typically depend on the data and where batch normalizations are added in the model. For a datasets like `digits`, where the distribution of pixel values of is likely similar across large batches of input data, batch normalization may not be worth the increase in computational complexity.\n\n## Dropout Layers\nA dropout layer is a regularization method where a proportion of input values are randomly set to zero. Doing this reduces the amount of overfitting while training and results in a fit more robust to potential outliers. We typically apply dropout layers to inputs of the fully connected layers during the classification segment of the network. \n\n## L1\/L2 Regularization\nRegularization techniques like L1 and L2 regularization can be applied to layers of the network to reduce\/prevent overfitting. The regularizers are added as penalty terms to the weight parameters of the loss function to reduce model complexity.","02b615af":"#### Example 2\n\nLet's use the 40th training image as our sample input `x`.","154b2ca8":"# Classification\n\nIn the classification segment of the network, we flatten the final feature learning layer to form fully connected (FC) layers.\n\n## Fully Connected (FC) Layers\n\nHere, we introduce FC layers by building a network of feature learning layers using Keras [Sequential](https:\/\/keras.io\/api\/models\/sequential\/) class. We initialize a sequential object and add feature learning layers to the model as follows:","e801054f":"### Hyperbolic Tangent (tanh)\n$$f(x) = \\tanh(x) = \\dfrac{e^x - e^{-x}}{e^x + e^{-x}}$$","602b4b97":"We use the `display_conv_pool` function to display the resulting layers as follows:","f9a76d7b":"# Feature Learning\n\nConvolutional neural networks (CNN) classify high-dimensional input data by extracting feature maps in an iterative process known as feature learning.\n\n$$\\mathsf{input} \\ \\ \\longrightarrow \\ \\ \\mathsf{feature \\ learning} \\ \\ \\longrightarrow \\ \\ \\mathsf{classification} \\\\ $$\n\nThe feature learning segment of the network is a set of feature maps or layers primarily constructed using convolutional kernels convolved with the input layers in a sequence.\n\n$$\\mathsf{input} \\ \\ \\circledast \\ \\ \\text{conv}_{1} \\ \\ \\circledast \\ \\cdots \\ \\circledast \\ \\text{conv}_{N} \\ \\ \\longrightarrow \\ \\mathsf{classification}$$","f0cfd857":"Below, we use the `torch.nn` function `Conv2d` to create two 2-dimensional convolution objects, `conv1` and `conv2`. The first convolution object `conv1` uses 3 kernel filters ($K=3$) and no zero padding ($P=0$) of the input border, while `conv2` uses 6 kernel filters ($K=6$) and adds one pixel of zero padding ($P=1$) to the input border. Both convolution objects have kernel size $F=3$, stride $S=1$, and one input channel, given that the input tensor is of shape\/size $1 \\times 1 \\times 28 \\times 28$.","944ab375":"## Convolutional Layers\n### Input\/Output\n\nHere we discuss the role of convolutional kernels\/filters in CNN architecture. Given a convolutional kernel function $\\Omega$, the kernel of $\\omega$ is convolved with an input layer $x_\\text{in}$, such that,\n\n$$x_\\text{out} = \\Omega(x_\\text{in})\\tag{1} \\\\ $$\n\nwhere $x_\\text{out}$ is the output layer. The shape of the output layer $x_\\text{out}$ is dependent on the shape of the input layer $x_\\text{in} \\in \\mathbb{R}^{H_\\text{in} \\times W_\\text{in} \\times D_\\text{in}}$ and the hyperparameters of the kernel function $\\Omega$: the number of kernels $K$, kernel size $F$, the stride $S$, and the amount of padding $P$ used on the input border.\n\n$$x_\\text{in} \\in \\mathbb{R}^{H_\\text{in} \\times W_\\text{in} \\times D_\\text{in}} \\quad\\overset{\\Omega}{\\longrightarrow}\\quad x_\\text{out} \\in \\mathbb{R}^{H_\\text{out} \\times W_\\text{out} \\times D_\\text{out}}$$\n\nwhere\n\n$$H_\\text{out} = \\frac{H_\\text{in} - F + 2P}{S} + 1\\tag{2} \\\\ $$\n\n$$W_\\text{out} = \\frac{W_\\text{in} - F + 2P}{S} + 1\\tag{3} \\\\ $$\n\n$$D_\\text{out} = K\\tag{4}$$\n\n\n#### Example 1\n\nLet the input layer $x_\\text{in}$ be a tensor of size ${5 \\times 5 \\times 1}$ and the hyperparameters of $\\Omega$ be $K=2$, $F=3$, $S=1$ and $P=0$, such that,\n\n$$H_\\text{out} = \\frac{H_\\text{in} - F + 2P}{S} + 1 = \\frac{5 - 3 + 2(0)}{1} + 1 = 3 \\\\ $$\n\n$$W_\\text{out} = \\frac{W_\\text{in} - F + 2P}{S} + 1 = \\frac{5 - 3 + 2(0)}{1} + 1 = 3 \\\\ $$\n\n$$D_\\text{out} = K = 2 \\\\ $$\n\nTherefore, the output $x_\\text{out} \\in \\mathbb{R}^{3 \\times 3 \\times 2}$ is a feature map with 2 layers and each layer is a $3 \\times 3$ matrix. For additional information about specific kernel functions and convolution, feel free to check out my GitHub [image processing](https:\/\/github.com\/cschupbach\/image_processing) repository.","0479f6ca":"### Others\n\n- Leaky ReLU\n\n- PReLU\n\n- RReLU\n\n- SELU\n\n- GELU","3ecf1ab3":"### Softplus\n$$f(x) = \\ln(1 + e^x)$$","f9031945":"## Pooling\n\nPooling methods are often used between certain convolutions for dimensionality reduction. The most common method for pooling input layers is *max* pooling using a $2 \\times 2$ kernel with non-overlapping strides. In certain cases with very large input, pooling with a $3 \\times 3$ kernel may be effective. Typically, *max* pooling produces better results than the less common *average* pooling.\n\n### Max Pooling","1fcc4136":"### PyTorch\nExample 2 briefly introduces the [torch.nn](https:\/\/pytorch.org\/docs\/stable\/nn.html) module as an introduction to the PyTorch library. While the remainder of the notebook uses the Keras library, both PyTorch and Keras offer similar tools for constructing convolutional neural networks.\n\nNote that PyTorch documentation specifies use of an input tensor in *channels first* format. This means the default input tensor for 2-dimensional convolution is expected to have the shape $N \\times C_\\text{in} \\times H_\\text{in} \\times W_\\text{in}$, where $N$ is the batch size, $C_\\text{in}$ is the number of input channels\/layers, $H_\\text{in}$ is the input height or the number of rows, and $W_\\text{in}$ is the width or the number of columns. Alternatively, Keras 2-dimensional convolution expects an input tensor of shape $N \\times H_\\text{in} \\times W_\\text{in} \\times C_\\text{in}$, which is known as *channels last* format.","11185f1c":"Define additional helper\/plotting functions:","aa7049f8":"![image.png](attachment:image.png)\n\n**Figure 1.** Example of fully connected layers where the flattened input layer is fed into a sequence of dense, fully connected layers. Neurons of each fully connected layer are linked to all activations in the prior layer.","954da752":"### Flatten\nThe output layers of the feature learning network above has the shape $4 \\times 4 \\times 16$. To begin the classification segment of the CNN, we flatten the output layers forming input array of length 256 similar to that of the simplified neural network shown in Figure 1 [[1](https:\/\/www.researchgate.net\/publication\/331525817_Temporal_Convolutional_Neural_Network_for_the_Classification_of_Satellite_Image_Time_Series)] and displayed the fully connected layer using the `display_fc` function.","804b57d4":"### Rectified Linear Unit  (ReLU)\n$$f(x) = \\begin{cases} 0 & \\text{for } \\ x \\leq 0 \\\\\nx & \\text{for } \\ x \\gt 0 \\end{cases}$$"}}