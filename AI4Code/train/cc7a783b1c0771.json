{"cell_type":{"80bf3409":"code","d5cf0c68":"code","f638ab18":"code","14a51b90":"code","73ba29c5":"code","087a27d1":"code","93e8a085":"code","66e76ad7":"code","95792f63":"code","40295e9b":"code","f653bd03":"code","90549261":"code","52ace77b":"code","69977da4":"code","af8f3c9e":"code","475c7b0e":"code","2fc7f9d7":"code","2c45e54d":"code","176ac2e2":"code","90551a18":"code","92cf3eda":"code","ea36e658":"code","1d38ba25":"code","4e631744":"code","5ba2f7dc":"code","30a73ad8":"code","5fb8788f":"code","8f10bdaf":"code","f734b65b":"code","99c89ab5":"code","79c29e70":"code","41f8505d":"code","6a061739":"code","457ed238":"code","1655a23b":"code","ab35bf9b":"code","667332f6":"code","33a78464":"code","d40b1a95":"code","cea29044":"code","3608ece9":"code","72e203d7":"code","ab7a90f7":"code","736ea7e2":"code","f9be221b":"code","4d6a1a3c":"code","8511aa84":"code","d46824a9":"code","ecbfa709":"code","d9e92a51":"code","5003a3ca":"code","8ea5633e":"code","b2562f7c":"code","111db42c":"code","187a5bf5":"code","b1561d37":"code","714112e3":"code","bededd02":"code","57a13437":"code","641364c2":"code","a319f620":"code","c8280f52":"code","b7a300ea":"code","4fa56e72":"code","4d8d40fd":"code","7c7e8a0e":"code","1fd04d6d":"code","95eda9ef":"code","44a933b0":"code","061da8c2":"code","d2dc77f2":"code","9bf66cc3":"code","94e7c710":"code","27af2b6d":"code","d8f7decd":"code","aed12dc6":"code","72bbe3dc":"code","c32683a0":"code","faf04e67":"code","e7eeb4d9":"code","554294f5":"markdown","61d86956":"markdown","41e7e843":"markdown","e53064bf":"markdown","1e6ef503":"markdown","ddfbe6c9":"markdown","5df0c276":"markdown","409a0661":"markdown","ebe3103e":"markdown","64f21ff4":"markdown","3db88db4":"markdown","99e4ffb8":"markdown","075c57ab":"markdown","12e5705e":"markdown","392f8749":"markdown","6c2a0eb4":"markdown","ebd3f055":"markdown","12321223":"markdown","77206f5d":"markdown","bec52c33":"markdown","cf788768":"markdown","7c68626b":"markdown","2c2ac49f":"markdown","5569b311":"markdown","4a1d2489":"markdown","52b7ca02":"markdown","fb48c25a":"markdown","a10487d8":"markdown","fd0db96d":"markdown","4542e48b":"markdown","3e654fa7":"markdown","16814e7b":"markdown","bb401560":"markdown","dc14b78c":"markdown","56765769":"markdown","c635dfce":"markdown","39d72225":"markdown","93133d27":"markdown","adf85176":"markdown"},"source":{"80bf3409":"# get data\nimport pandas as pd\n\ntrain_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\n\ndf = pd.concat([train_df, test_df], axis=0, ignore_index=True)\ndf[:3]","d5cf0c68":"# 'PassengerId' is just indexing number.\n# I don't see as any meaningful information in it so drop it.\ndf = df.drop('PassengerId', axis=1)","f638ab18":"df.info()","14a51b90":"# extract 'title' in name\ndf['Title'] = [i.split(',')[1].split('.')[0].strip() for i in df['Name']]","73ba29c5":"import numpy as np\n\ndef get_age(i):\n    global df\n    title_age = dict(df.groupby('Title')['Age'].mean())\n    title = df['Title'][i]\n    age = title_age[title]\n    return round(age, 2)\n\n# make age list\nage = []\nfor i in df.index:\n    if np.isnan(df['Age'][i]):\n        a = get_age(i)\n        age.append(a)\n        \n# make dictioanry = {index: age}  \nd = {}\nfor i, j in zip(np.where(df['Age'].isnull())[0], age):\n    d[i] = j\n\n    \n#fill the nulls in Age\nfor key in d:\n    df.at[key, 'Age'] = d[key]","087a27d1":"df.Age.isnull().sum()","93e8a085":"np.where(df['Fare'].isnull())","66e76ad7":"df.iloc[1043]","95792f63":"df.groupby(['Pclass', 'Embarked'])['Fare'].mean()","40295e9b":"df.at[1043, 'Fare'] = 14.4354","f653bd03":"np.where(df['Embarked'].isnull())","90549261":"df.iloc[61]","52ace77b":"# where do the people usually go in her age and status at that time?\n# I can refer the information of other passenger who has the similar background.\n\nrefer_columns = [df['Age']==any(range(30, 40)), df['Sex']=='female', \n                 df['Title']=='Miss', df['SibSp']==0]\ndf.groupby(refer_columns)['Embarked'].value_counts()","69977da4":"df.at[61, 'Embarked'] = 'C'","af8f3c9e":"df.iloc[829]","475c7b0e":"refer_columns = [df['Age']==any(range(50, 65)), df['Sex']=='female', \n                 df['Title']=='Mrs', df['SibSp']==0]\ndf.groupby(refer_columns)['Embarked'].value_counts()","2fc7f9d7":"df.at[829, 'Embarked'] = 'C'","2c45e54d":"df.Embarked.isnull().sum()","176ac2e2":"df.info()","90551a18":"import seaborn as sns\nsns.barplot(x='Pclass', y='Survived', data=df)","92cf3eda":"def get_ratio(feature_name):\n    \"\"\"\n    return:\n    'survival ratio dictionary' of given feature\n    \n    \"\"\"\n    global df\n    \n    # get number of survivals\n    survived = df.groupby('Survived')[feature_name].value_counts()[1]\n    #not_survived = df.groupby('Survived')[feature_name].value_counts()[0]\n\n    # get all passengers in corresponding feature values\n    n_passengers = df[feature_name][:891].value_counts().sort_index()\n\n    # get survivals ratio\n    ratio_dict = dict(survived \/ n_passengers)\n    \n    # check nulls and fill in\n    for i in df[feature_name].unique():\n        if i not in list(ratio_dict.keys()) or ratio_dict[i] != (ratio_dict[i]\/1):\n            ratio_dict[i] = 0           \n        ratio_dict[i] = round(ratio_dict[i], 5)\n            \n    return ratio_dict","ea36e658":"prob_pclass = get_ratio('Pclass')\nprob_pclass","1d38ba25":"# make new data frame for feature vectors.\nfeature_df = pd.DataFrame({'Pclass': []})\n\ndef mapping(feature_name, d):\n    global feature_df\n    feature_df[feature_name] = df[feature_name].map(d)\n    return print('Nulls in feature_df[{}] is {}!'.\\\n                 format(feature_name, np.where(feature_df[feature_name].isnull())[0]))","4e631744":"mapping('Pclass', get_ratio('Pclass'))","5ba2f7dc":"df['Name_Length'] = df['Name'].apply(lambda x: len(x.split()))","30a73ad8":"sns.barplot(x='Name_Length', y='Survived', data=df)","5fb8788f":"# if my assumption looks ok, do the mapping.\n\nmapping('Name_Length', get_ratio('Name_Length'))","8f10bdaf":"sns.barplot(x='Sex', y='Survived', data=df)","f734b65b":"# in order to calculate ratio, make the current 'string' representation numbers.\ndf['Sex'] = df['Sex'].map({'female':0, 'male':1}).astype(int)\n\nmapping('Sex', get_ratio('Sex'))","99c89ab5":"# I tried a number of bins to find out more explanable chunks.\n\nsns.histplot(df.Age, bins=20)","79c29e70":"# Mapping Age bins\ndf['Age_bin'] = df['Age']\n\ndf.loc[ df['Age'] <= 10, 'Age_bin']  = 0\ndf.loc[(df['Age'] > 10) & (df['Age'] <= 15), 'Age_bin'] = 1\ndf.loc[(df['Age'] > 15) & (df['Age'] <= 20), 'Age_bin'] = 2\ndf.loc[(df['Age'] > 20) & (df['Age'] <= 30), 'Age_bin'] = 3\ndf.loc[(df['Age'] > 30) & (df['Age'] <= 40), 'Age_bin'] = 4\ndf.loc[(df['Age'] > 40) & (df['Age'] <= 60), 'Age_bin'] = 5\ndf.loc[ df['Age'] > 60, 'Age_bin'] = 6","41f8505d":"# See if the number of bins looks alright.\n# Do they show that any relations to Survival?\nsns.barplot(x='Age_bin', y='Survived', data=df)","6a061739":"# if it looks ok, do the mapping.\n\nmapping('Age_bin', get_ratio('Age_bin'))","457ed238":"feature_df.info()","1655a23b":"sns.barplot(x='SibSp', y = 'Survived', data=df)","ab35bf9b":"mapping('SibSp', get_ratio('SibSp'))\n\nfeature_df.info()","667332f6":"sns.barplot(x='Parch', y = 'Survived', data=df)","33a78464":"mapping('Parch', get_ratio('Parch'))","d40b1a95":"sns.histplot(df.Fare, bins=20)","cea29044":"# Mapping Fare\ndf['Fare_bin'] = df['Fare']\n\ndf.loc[ df['Fare'] <= 20, 'Fare_bin']  = 0\ndf.loc[(df['Fare'] > 20) & (df['Fare'] <= 40), 'Fare_bin'] = 1\ndf.loc[(df['Fare'] > 40) & (df['Fare'] <= 60), 'Fare_bin'] = 2\ndf.loc[(df['Fare'] > 60) & (df['Fare'] <= 80), 'Fare_bin'] = 3\ndf.loc[(df['Fare'] > 80) & (df['Fare'] <= 100), 'Fare_bin'] = 4\ndf.loc[(df['Fare'] > 100), 'Fare_bin'] = 5","3608ece9":"sns.barplot(x='Fare_bin', y = 'Survived', data=df)","72e203d7":"# survival ratio is gradually growing as fare increases.\n\nmapping('Fare_bin', get_ratio('Fare_bin'))","ab7a90f7":"sns.barplot(x='Embarked', y = 'Survived', data=df)","736ea7e2":"mapping('Embarked', get_ratio('Embarked'))","f9be221b":"# if 'NaN', data type is float, else string.\ntype(df['Cabin'][1]), type(df['Cabin'][2])","4d6a1a3c":"df['Has_Cabin'] = df['Cabin'].apply(lambda x: 0 if type(x) == float else 1)\nsns.barplot(x='Has_Cabin', y = 'Survived', data=df)","8511aa84":"mapping('Has_Cabin', get_ratio('Has_Cabin'))","d46824a9":"title_list = list(df['Title'].value_counts().index)\ntitle_dict = dict((i[0],  i[1]) for i in zip(title_list, range(0, len(title_list))))\ndf['Title_code'] = df['Title'].map(title_dict)\n\nsns.barplot(x='Title_code', y = 'Survived', data=df)","ecbfa709":"mapping('Title_code', get_ratio('Title_code'))","d9e92a51":"len(df['Ticket'].unique())","5003a3ca":"# we can recategorize the tickets by its first letter comes in the Ticket.\na = []\nfor i in sorted(df.Ticket.unique()):\n    if i[0] not in a:\n        a.append(i[0])\na[:5], a[-5:], len(a)","8ea5633e":"df['Ticket_code'] = df['Ticket'].apply(lambda x: x[0]).astype('category').cat.codes\n\nsns.set(rc={'figure.figsize':(7,7)})\nsns.barplot(x='Ticket_code', y = 'Survived', data=df)","b2562f7c":"mapping('Ticket_code', get_ratio('Ticket_code'))\n\nfeature_df.info()","111db42c":"# save new data frame for feature vectors\n\n#feature_df['Survived'] = pd.read_csv('data\/train.csv')['Survived']\n#feature_df.to_csv('data\/feature_df.csv', index=False)","187a5bf5":"feature_df['Survived'] = df['Survived']\ndata = feature_df\ndata[:3]","b1561d37":"# correlations of features.\nimport matplotlib.pyplot as plt\n\ntrain_data = data[:891]\ntest_data = data[891:]\n\nf,ax = plt.subplots(figsize=(7, 7))\nsns.heatmap(train_data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","714112e3":"train_data.corr()['Survived'].sort_values(ascending=False)","bededd02":"# select top 8 features\n\nselected_features = train_data.corr()['Survived'].sort_values(ascending=False).index[1:8].tolist()\nselected_features","57a13437":"# prepare data to fit a model\n\ntrain_vectors = train_data[selected_features].values\ntest_vectors = test_data[selected_features].values\ntrain_vectors.shape, test_vectors.shape, ","641364c2":"target = data['Survived'][:891].values.reshape(-1, 1)\ntarget.shape","a319f620":"# scaling vectors \n\ntrain_scaled = train_vectors.copy()\nfor i in range(train_scaled.shape[0]):\n     train_scaled[i] = train_scaled[i] \/ train_scaled[i].sum()\n        \ntest_scaled = test_vectors.copy()\nfor i in range(test_scaled.shape[0]):\n     test_scaled[i] = test_scaled[i] \/ test_scaled[i].sum()","c8280f52":"#defining dataset class\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass dataset(Dataset):\n    def __init__(self,x,y):\n        self.x = torch.tensor(x, dtype=torch.float32)\n        self.y = torch.tensor(y, dtype=torch.float32)\n        self.length = self.x.shape[0]\n \n    def __getitem__(self,idx):\n        return self.x[idx],self.y[idx]\n\n    def __len__(self):\n        return self.length","b7a300ea":"# load data\ntrainset_conv = dataset(train_scaled, target)\ntrainloader_conv = DataLoader(trainset_conv, batch_size=11,shuffle=True)","4fa56e72":"#defining the network\nfrom torch import nn\nfrom torch.nn import functional as F\n\nclass ConvNet(nn.Module):\n    def __init__(self,input_shape):\n        super(ConvNet,self).__init__()\n        self.c1 = nn.Conv1d(in_channels=input_shape, out_channels=50, \n                           kernel_size=3, stride=1, padding=1)\n        \n        self.c2 = nn.Conv1d(in_channels=50, out_channels=2, \n                           kernel_size=3, stride=1, padding=1)\n        \n        self.outlayer = nn.Linear(6,1)\n        \n        self.flatten = nn.Flatten()\n        self.drop = nn.Dropout(p=0.1)\n        self.maxpool = nn.MaxPool1d(kernel_size=2, stride=1, padding=1, ceil_mode=False)\n        \n    def forward(self,x):\n        x = x.reshape(-1,7,1)\n        \n        x = self.c1(x)\n        x = torch.relu(x) \n        x = self.maxpool(x) \n        \n        x = self.c2(x)\n        x = torch.relu(x) \n        x = self.maxpool(x) \n\n        x = self.drop(x)\n        x = self.flatten(x)\n        \n        x = self.outlayer(x)\n        \n        return torch.sigmoid(x)","4d8d40fd":"#declare a model   \nconv_model = ConvNet(input_shape = train_scaled.shape[1])\n\n# get device\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nconv_model.to(device)\n\n\n\n#reset the params as needed\ndef weight_reset(m):\n    if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n        m.reset_parameters()\n\nconv_model.apply(weight_reset)","7c7e8a0e":"# Optimizer\nlearning_rate = 0.01\noptimizer1 = torch.optim.SGD(conv_model.parameters(),lr=learning_rate)\n\n# Loss function\ncompute_loss1 = nn.BCELoss(reduction = 'mean')\n\n#start training the model\nprint('ConvNet is now working on --------------')\nconv_model.train()\n\nepochs = 50\n\nlosses = []\naccs = []\nfor i in range(epochs):\n    for x_train,y_train in trainloader_conv:\n    \n        #calculate output\n        x_train,y_train = x_train.to(device), y_train.to(device)\n        output_batch = conv_model(x_train)\n\n        #for now, what is the accuracy of model about all data?\n        all_train_tensor = torch.tensor(train_scaled, dtype=torch.float32)\n        all_train_tensor = all_train_tensor.to(device)\n        output_all = conv_model(all_train_tensor.clone().detach().float())\n        acc = (output_all.reshape(-1).cpu().detach().numpy().round() == target).mean()\n        \n        #compute the difference between true value and output value of the current batch\n        loss = compute_loss1(output_batch, y_train)\n        \n        #backprop\n        optimizer1.zero_grad()\n        \n        loss.backward()\n        \n        optimizer1.step()\n        \n    if i%20 == 0:\n        losses.append(loss)\n        accs.append(acc)\n        print(\"epoch {}  \\t| loss : {}  \\t| accuracy :  {}\".format(i,loss,acc))\n        \nprint('Done!')","1fd04d6d":"import numpy as np\n\ntest_ts = torch.tensor(test_scaled, dtype=torch.float32)\n\npreds_conv = []\nconv_model.eval()\nwith torch.no_grad():\n    test = test_ts.to(device).float()\n    pred = conv_model(test)\n    pred = torch.round(pred)\n    preds_conv.append(pred.cpu().numpy())\n\npreds_conv = np.array(preds_conv).reshape(-1)\npreds_conv[:7]","95eda9ef":"class NN(nn.Module):\n    def __init__(self,input_shape):\n        super(NN,self).__init__()\n        self.fc1 = nn.Linear(input_shape, 81)\n        self.fc2 = nn.Linear(81, 5)\n        \n        self.outlayer = nn.Linear(5, 1)\n        \n    def forward(self,x):\n        x = torch.relu(self.fc1(x))\n        x = torch.relu(self.fc2(x))\n        \n        x = self.outlayer(x)\n\n        return torch.sigmoid(x)","44a933b0":"#declare a model   \nnn_model = NN(input_shape = train_scaled.shape[1])\n\n# get device\ndevice = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\nnn_model.to(device)\n\n\n#reset the params as needed\ndef weight_reset(m):\n    if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n        m.reset_parameters()\n\nnn_model.apply(weight_reset)","061da8c2":"# load data\ntrainset_lin = dataset(train_scaled, target)\ntrainloader_lin = DataLoader(trainset_lin,batch_size=11,shuffle=True)","d2dc77f2":"# Optimizer\nlearning_rate = 0.01\noptimizer1 = torch.optim.SGD(nn_model.parameters(),lr=learning_rate)\n\n# Loss function\ncompute_loss1 = nn.BCELoss(reduction = 'mean')\n\n#start training the model\nprint('NN is now working on --------------')\nnn_model.train()\n\nepochs = 50\n\nlosses = []\naccs = []\nfor i in range(epochs):\n    for x_train,y_train in trainloader_conv:\n    \n        #calculate output\n        x_train,y_train = x_train.to(device), y_train.to(device)\n        output_batch = nn_model(x_train)\n\n        #for now, what is the accuracy of model about all data?\n        all_train_tensor = torch.tensor(train_scaled, dtype=torch.float32)\n        all_train_tensor = all_train_tensor.to(device)\n        output_all = nn_model(all_train_tensor.clone().detach().float())\n        acc = (output_all.reshape(-1).cpu().detach().numpy().round() == target).mean()\n        \n        #compute the difference between true value and output value of the current batch\n        loss = compute_loss1(output_batch, y_train)\n        \n        #backprop\n        optimizer1.zero_grad()\n        \n        loss.backward()\n        \n        optimizer1.step()\n        \n    if i%20 == 0:\n        losses.append(loss)\n        accs.append(acc)\n        print(\"epoch {}  \\t| loss : {}  \\t| accuracy :  {}\".format(i,loss,acc))\n        \nprint('Done!')","9bf66cc3":"import numpy as np\n\ntest_ts = torch.tensor(test_scaled, dtype=torch.float32)\n\npreds_lin = []\nnn_model.eval()\nwith torch.no_grad():\n    test = test_ts.to(device).float()\n    pred = nn_model(test)\n    pred = torch.round(pred)\n    preds_lin.append(pred.cpu().numpy())\n\npreds_lin = np.array(preds_lin).reshape(-1)\npreds_lin[:7]","94e7c710":"from sklearn import svm\n\nsvk = svm. SVC(kernel = 'poly', probability=True)\nsvk.fit(train_scaled, target)\nsvk.score(train_scaled, target)","27af2b6d":"preds_svm = svk.predict(test_scaled)\npreds_svm[:5]","d8f7decd":"from sklearn.linear_model import LogisticRegression\n\nlg = LogisticRegression(solver='liblinear',\n    penalty='l1',dual=False, max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=1)\nlg.fit(train_scaled, target)\nlg.score(train_scaled, target)","aed12dc6":"preds_lg = lg.predict(test_scaled)\npreds_lg[:5]","72bbe3dc":"from sklearn.ensemble import RandomForestClassifier\n\nrm = RandomForestClassifier(max_depth=9, min_samples_leaf=4, min_samples_split=5,\n                       n_estimators=90)\nrm.fit(train_scaled, target)\nrm.score(train_scaled, target)","c32683a0":"\n\n\"\"\"\n# when you find the best params\nfrom sklearn.model_selection import GridSearchCV\n\n# create param grid object \n\nparams = dict(max_depth = [n for n in range(3, 14)], \n    min_samples_split = [n for n in range(4, 11)], \n    min_samples_leaf = [n for n in range(2, 5)],     \n    n_estimators = [n for n in range(10, 100, 10)],\n)\n\nforest_cv = GridSearchCV(estimator=rm, param_grid=params, cv=5) \nforest_cv.fit(train_scaled, target)\n\nprint(\"Best score: {}\".format(forest_cv.best_score_))\nprint(\"Optimal params: {}\".format(forest_cv.best_estimator_))\n\n\"\"\"\n","faf04e67":"preds_rm = rm.predict(test_scaled)\npreds_rm[:5]","e7eeb4d9":"PassengerId = test_df['PassengerId']\nsubmission = pd.DataFrame({'PassengerId': PassengerId, 'Survived':preds_rm}, dtype='int')\nsubmission['Survived'] = submission.Survived.astype('int')\n\nsubmission.to_csv('.\/submission1.csv', index=False)","554294f5":"get dataset","61d86956":"Title contains the relatively useful information about passenger, such as the status of marriage or occupation etc..","41e7e843":"3) Embarked.\n\nI tried to see if there is any 'the trip trand' in particular ages.","e53064bf":"There are some nulls in feature 'Age', 'Fare', and 'Cabin'. \n\n1) Age.\n\nI have names with title and there's no nulls in it. \n\nSo I used those information on guessing passenger's age.","1e6ef503":"Name","ddfbe6c9":"## 5) RandomForestClassifier","5df0c276":"Pclass","409a0661":"# 4. Modelling","ebe3103e":"2) Fare.\n\nIn 'Fare' column, there's only one null.\n\nSo I directly went into the particular passenger space.\n\nI roughly guessed his Fare value baased on Pclass and Embarked.","64f21ff4":"I want the feature vectors to represent the probabilities(score or ratio) to survive.","3db88db4":"# 2. Feature Engineering","99e4ffb8":"Embarked","075c57ab":"## 1) ConvNet","12e5705e":"there are not many data in this feature, so I marked it 'yes' or 'no'.","392f8749":"Parch","6c2a0eb4":"build a model","ebd3f055":"# 3.Explore data and select features","12321223":"Work Flow\n\n1. Clean data\nfill the nulls, drop the unnecessary columns\n\n2. Feature Vectorizing\nif there's non-numeric features, convert them into numbers,\n\n3. Explore data\nfeature correlations, looking for important features,\ntrying to get intuition\n\n4. Modelling\nbuild a model of the data\nselectig model, tunning parameters","77206f5d":"Title","bec52c33":"Some ticket gives more chance to survive.","cf788768":"Cabin","7c68626b":"There are many unique numbers in this feature.\nI made new feature named 'Ticket type'.","2c2ac49f":"It seems that we can recategorize the tickets by its first letter.","5569b311":"It's interesting.\nIt seems like, as you have longer name, you are more likely to survive.","4a1d2489":"In 'Cabin' feature, there are not many data.\n\nBut I cannot just immdediatly drop it yet,\n\nbecause I don't know if it's important feature or not.\n\nSo, let's hold it for now.","52b7ca02":"# 1. Clean data","fb48c25a":"In Pclass 3, average fare to 'S' is 14.435422.","a10487d8":"Ticket","fd0db96d":"## 3) SVM Kernel","4542e48b":"Age","3e654fa7":"## 2) Neural Net","16814e7b":"# 5. Submission","bb401560":"Sex","dc14b78c":"Test","56765769":"## This notebook is for gentle introduction on feature engineering and\n## models selections and its simple applications.","c635dfce":"## 4) Logistic Regression","39d72225":"SibSp","93133d27":"Fare","adf85176":"test"}}