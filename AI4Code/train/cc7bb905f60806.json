{"cell_type":{"a7407eaa":"code","f7e571b7":"code","f7f0955d":"code","526c1809":"code","2ec4a6a7":"code","833504a9":"code","a19e6202":"code","de693f6b":"code","6900365c":"code","7455fad6":"code","52c7c4b5":"code","6dbe4203":"code","025c773e":"code","7389ba8a":"code","ee203f66":"code","be6c363d":"code","63322648":"code","663f9c2f":"code","86314c63":"code","02a6a274":"code","7283c8b3":"code","41852ed1":"code","3c902c80":"code","1fee5887":"code","b0010cff":"code","1c0936c3":"code","649edf5a":"code","8c5f3b07":"code","9c622cc4":"code","80e8ae7e":"code","3bea2bea":"code","1c1f62ff":"code","7016e979":"code","6331a990":"code","0d951d61":"code","764e52fb":"code","3928d9d0":"code","93b32d30":"markdown","6f13e876":"markdown","851640d5":"markdown","c58b8925":"markdown","31d37bfc":"markdown","3e8b43b5":"markdown","2d8d1ec3":"markdown","cda6a8e7":"markdown","3f6bb433":"markdown","f69901c0":"markdown","8e22dc59":"markdown","d28c3996":"markdown","0a4c26fe":"markdown","0b4cfe99":"markdown","89b5037c":"markdown","d35a29fe":"markdown","b783ad4b":"markdown","2893e2d0":"markdown","266698fd":"markdown","e622fe9a":"markdown","479b7807":"markdown","5df0173c":"markdown","67dd63e2":"markdown","5a615640":"markdown"},"source":{"a7407eaa":"from datetime import datetime\nprint('Process start time :', datetime.now())\n\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f7e571b7":"import pandas as pd\nimport numpy as np\nimport seaborn as sns","f7f0955d":"df = pd.read_csv('\/kaggle\/input\/craigslist-carstrucks-data\/vehicles.csv')","526c1809":"# show all columns\npd.set_option('display.max_columns', None)","2ec4a6a7":"df.head(5)","833504a9":"# Removing region since we already have 'county'\ndf=df.drop(['region', 'region_url', 'vin','url','image_url','description','county'], axis=1)","a19e6202":"df=df.dropna()","de693f6b":"df.shape","6900365c":"from sklearn import preprocessing\nle = preprocessing.LabelEncoder()","7455fad6":"df.columns","52c7c4b5":"df.shape","6dbe4203":"# convert characters to numbers using label Encoding\ndf[['size','manufacturer', 'model', 'condition','cylinders', 'fuel', 'title_status', 'transmission','drive', 'type', 'paint_color', 'state']] = df[['size','manufacturer', 'model', 'condition','cylinders', 'fuel', 'title_status', 'transmission','drive','type', 'paint_color', 'state']].apply(le.fit_transform)","025c773e":"df","7389ba8a":"df[\"odometer\"] = np.sqrt(preprocessing.minmax_scale(df[\"odometer\"]))","ee203f66":"# Seperate Features and Outcome\nX = df.drop('price',axis=1).values\ny = df.price.values","be6c363d":"from sklearn.model_selection import StratifiedKFold\n\nskf = StratifiedKFold(n_splits=2)\nskf.get_n_splits(X, y)\n\nfor train_index, test_index in skf.split(X, y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n\n# works for classification\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","63322648":"print (X_train.shape, y_train.shape, X_test.shape, y_test.shape)","663f9c2f":"# Create a dataframe to store accuracy scores of different algorithms\naccuracy_df = pd.DataFrame(columns=('r2', 'rmse'))","86314c63":"from sklearn.tree import DecisionTreeRegressor\nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nimport math\nfrom sklearn.metrics import mean_squared_error as MSE\n\n# Fit\nmodel = DecisionTreeRegressor()\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['Decision Tree Baseline']))","02a6a274":"accuracy_df","7283c8b3":"from sklearn.model_selection import GridSearchCV\n\nscoring = metrics.make_scorer(metrics.mean_squared_error)\n\nparam_grid = {\n'criterion':['mse'] \n,'splitter':['best','random']\n,'max_depth':[4, 5, 6, 7, 8]\n,'min_samples_split':[0.8, 2]\n,'max_features':['auto','sqrt','log2']\n}\n\ng_cv = GridSearchCV(DecisionTreeRegressor(random_state=0),param_grid=param_grid,scoring=scoring, cv=5, refit=True)","41852ed1":"g_cv.fit(X_train, y_train)\ng_cv.best_params_\nresult = g_cv.cv_results_\n# print(result)\n\n# Predict\ny_pred = g_cv.best_estimator_.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['Decision Tree HyperParam Tuning']))\naccuracy_df.sort_values('rmse')","3c902c80":"from sklearn.ensemble import RandomForestRegressor\n\n# Fit\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['Random Forest Baseline']))\naccuracy_df.sort_values('rmse')","1fee5887":"from sklearn.ensemble import GradientBoostingRegressor\n\n# Fit\nmodel = GradientBoostingRegressor(random_state=0)\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['Gradient Boosting Baseline']))\naccuracy_df.sort_values('rmse')","b0010cff":"from xgboost import XGBRegressor\n\n# Fit\nmodel = XGBRegressor(random_state=0)\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['XGBoost Baseline']))\naccuracy_df.sort_values('rmse')","1c0936c3":"import xgboost as xgb\n\nmodel = xgb.XGBRegressor(\n#     gamma=1,                 \n    learning_rate=0.05,\n#     max_depth=3,\n#     n_estimators=10000,                                                                    \n#     subsample=0.8,\n    random_state=34,\n    booster='gbtree',    \n    objective='reg:squarederror',\n    eval_metric='rmse'\n) \nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['XGBoost with Parameters']))\naccuracy_df.sort_values('rmse')","649edf5a":"from sklearn.neural_network import MLPRegressor\n\nmlp = MLPRegressor()\nparam_grid = {\n#               'hidden_layer_sizes': [i for i in range(2,20)],\n              'activation': ['relu'],\n              'solver': ['adam'],\n#               'learning_rate': ['constant'],\n#               'learning_rate_init': [0.01],\n#               'power_t': [0.5],\n#               'alpha': [0.0001],\n#               'max_iter': [1000],\n              'early_stopping': [True],\n              'warm_start': [False]\n}\nmodel = GridSearchCV(mlp, param_grid=param_grid, \n                   cv=10, verbose=True, pre_dispatch='2*n_jobs')\n\nmodel.fit(X_train, y_train)\n\n# Predict\ny_pred = model.predict(X_test)\n\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['MLPRegressor with Parameter Tuning']))\naccuracy_df.sort_values('rmse')\n\n","8c5f3b07":"#Splitting the training data in to training and validation datasets for Model training\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split\n\nXtrain, Xval, Ztrain, Zval = train_test_split(X_train, y_train, test_size=0.3, random_state=0)\n\ntrain_set = lgb.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgb.Dataset(Xval, Zval, silent=False)\n\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.01,\n#         'max_depth': -1,\n#         'subsample': 0.8,\n#         'bagging_fraction' : 1,\n#         'max_bin' : 5000 ,\n#         'bagging_freq': 20,\n#         'colsample_bytree': 0.6,\n        'metric': 'rmse',\n#         'min_split_gain': 0.5,\n#         'min_child_weight': 1,\n#         'min_child_samples': 10,\n#         'scale_pos_weight':1,\n#         'zero_as_missing': False,\n#         'seed':0,        \n    }\nmodel = lgb.train(params, train_set = train_set, num_boost_round=10000,early_stopping_rounds=8000,\n                  verbose_eval=500, valid_sets=valid_set)\n\n\ny_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['LightGBM with Parameters']))\naccuracy_df.sort_values('rmse')","9c622cc4":"#Splitting the training data in to training and validation datasets for Model training\n\nimport lightgbm as lgb1\nfrom sklearn.model_selection import train_test_split\n\nXtrain, Xval, Ztrain, Zval = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n#Define categorical features, training and validation data\ncategorical_positions = []\ncat = ['manufacturer','model','condition','cylinders','fuel','odometer','title_status','transmission','drive','type','paint_color','county','state']\nfor c, col in enumerate(df.columns):\n    for x in cat:\n        if col == x:\n            categorical_positions.append(c-1)\n\n\ntrain_set = lgb1.Dataset(Xtrain, label=Ztrain, categorical_feature=categorical_positions)\nvalid_set = lgb1.Dataset(Xval, label=Zval)\n\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.01,\n#         'max_depth': -1,\n#         'subsample': 0.8,\n#         'bagging_fraction' : 1,\n#         'max_bin' : 5000 ,\n#         'bagging_freq': 20,\n#         'colsample_bytree': 0.6,\n        'metric': 'rmse',\n#         'min_split_gain': 0.5,\n#         'min_child_weight': 1,\n#         'min_child_samples': 10,\n#         'scale_pos_weight':1,\n#         'zero_as_missing': False,\n#         'seed':0,        \n    }\nmodel = lgb1.train(params, train_set = train_set, num_boost_round=10000,early_stopping_rounds=8000,\n                  verbose_eval=500, valid_sets=valid_set)\n\n\ny_pred = model.predict(X_test, num_iteration=model.best_iteration)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['LightGBM with Categories & Parameters']))\naccuracy_df.sort_values('rmse')","80e8ae7e":"from catboost import CatBoostRegressor, Pool\n    \nfrom sklearn.model_selection import train_test_split\n\nXtrain, Xval, Ztrain, Zval = train_test_split(X_train, y_train, test_size=0.3, random_state=0)\n\ntrain_set = lgb.Dataset(Xtrain, Ztrain)\nvalid_set = lgb.Dataset(Xval, Zval)\n\nmodel = CatBoostRegressor()\n\nmodel.fit(Xtrain, Ztrain, eval_set=[(Xval, Zval)], verbose=100, early_stopping_rounds=1000)\n\ny_pred = model.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['CatBoost Baseline']))\naccuracy_df.sort_values('rmse')","3bea2bea":"from catboost import CatBoostRegressor, Pool\n    \nfrom sklearn.model_selection import train_test_split\n\nXtrain, Xval, Ztrain, Zval = train_test_split(X_train, y_train, test_size=0.3, random_state=0)\n\ntrain_set = lgb.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgb.Dataset(Xval, Zval, silent=False)\n\n\n        \nmodel = CatBoostRegressor(\n                          iterations=1000, \n                          depth=8, \n                          learning_rate=0.01, \n                          loss_function='RMSE', \n                          eval_metric='RMSE', \n                          use_best_model=True)\n\nmodel.fit(Xtrain, Ztrain, eval_set=[(Xval, Zval)], verbose=100, early_stopping_rounds=1000)\n\ny_pred = model.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\n# accuracy_df = accuracy_df.drop('CatBoost Parameter Tuning')\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['CatBoost Parameter Tuning']))\naccuracy_df.sort_values('rmse')","1c1f62ff":"# Plot\nplt.figure(figsize=[25,6])\nplt.tick_params(labelsize=14)\nplt.plot(accuracy_df.index, accuracy_df['rmse'], label = 'RMSE Scores')\nplt.legend()\nplt.title('RMSE Score comparison for 10 popular models for test dataset')\nplt.xlabel('Models')\nplt.ylabel('RMSE Scores')\nplt.xticks(accuracy_df.index, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()","7016e979":"print('Process start time :', datetime.now())","6331a990":"# Rerunning MLP Neural Network to save the model\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.model_selection import GridSearchCV\n\n\nmlp = MLPRegressor()\nparam_grid = {\n#               'hidden_layer_sizes': [i for i in range(2,20)],\n              'activation': ['relu'],\n              'solver': ['adam'],\n#               'learning_rate': ['constant'],\n#               'learning_rate_init': [0.01],\n#               'power_t': [0.5],\n#               'alpha': [0.0001],\n#               'max_iter': [1000],\n              'early_stopping': [True],\n              'warm_start': [False]\n}\nmodel = GridSearchCV(mlp, param_grid=param_grid, \n                   cv=10, pre_dispatch='2*n_jobs')\n\nmodel.fit(X_train, y_train)","0d951d61":"# Save the neural network model\nfrom joblib import dump, load\n\nfilename = 'mlp_neural_network_001.joblib'\nwith open(filename, 'wb') as file:  \n    dump(model, file)","764e52fb":"# Predict\ny_pred = model.predict(X_test)\ndf1 = pd.DataFrame({\"y\":y_test,\"y_pred\":y_pred })","3928d9d0":"df1.head(50)","93b32d30":"## CatBoost","6f13e876":"## Random Forest with auto Hyper Parameter Tuning with Grid Search","851640d5":"## XGBoost with Parameters","c58b8925":"## Decision Tree with auto Hyper Parameter Tuning with Grid Search","31d37bfc":"## Random Forest","3e8b43b5":"### factorize the categorical columns\ncats = ['manufacturer','model','condition','cylinders','fuel','odometer','title_status','transmission','drive','type','paint_color','county','state']\nfor cat in cats:\n#     sorting_list=np.unique(sorted(df[cat],key=lambda x:(str.lower(x),x)))\n#     df[cat]=pd.Categorical(df[cat], sorting_list)\n    df=df.sort_values(cat)\n    df[cat]=pd.factorize(df[cat], sort=True)[0]","2d8d1ec3":"# Train Test Split","cda6a8e7":"# Machine Learning Algorithms","3f6bb433":"## Decision Tree","f69901c0":"There is no change in accuracy even after mentioning the categorical columns explicitly. This is because most of the columns in the dataset is categorical. LightGBM algorithm automatically selects the categorical columns if its not given.","8e22dc59":"## Gradient Boosting","d28c3996":"accuracy_df","0a4c26fe":"g_cv.fit(X_train, y_train)\ng_cv.best_params_\nresult = g_cv.cv_results_\n# print(result)\n\n# Predict\ny_pred = g_cv.best_estimator_.predict(X_test)\n\n# Metrics\nr2 = round(metrics.r2_score(y_test, y_pred),2)\nrmse = round(np.sqrt(metrics.mean_squared_error(y_test, y_pred)),2)\naccuracy_df = accuracy_df.append(pd.DataFrame({'r2':[r2],'rmse':[rmse]}, index = ['Random Forest HyperParam Tuning']))","0b4cfe99":"from sklearn.model_selection import GridSearchCV\n\nscoring = metrics.make_scorer(metrics.mean_squared_error)\n\nparam_grid = {\n# 'n_estimators':[50,70,100,120,130] \n# 'max_features':['auto','sqrt','log2']\n#,'oob_score':[False, True] # whether to use out-of-bag samples to estimate the R^2 on unseen data.\n# ,'bootstrap':[False, True]\n# ,'random_state':[10, None]\n# ,'warm_start':[True, False]\n'max_depth':[4, 5, 6, 7, 8]\n# ,'min_samples_split':[0.8, 2, 3]\n}\n\ng_cv = GridSearchCV(RandomForestRegressor(random_state=0),param_grid=param_grid,scoring=scoring, cv=5, refit=True)","89b5037c":"# LightGBM","d35a29fe":"## Handling continous values - Scaling down ","b783ad4b":"## MLP Regressor","2893e2d0":"# Save Model","266698fd":"## XGBoost","e622fe9a":"## Plotting the RMSE Scores","479b7807":"## CatBoost with Parameters","5df0173c":"## Accuracies of Models sorted by RMSE Scores","67dd63e2":"## LightGBM with categorical variables","5a615640":"# Importing the data"}}