{"cell_type":{"7fc9748c":"code","61b235ad":"code","e7968342":"code","a30d6159":"code","a9163269":"code","79db83ff":"code","cac89b7c":"code","302a29be":"code","7c168b9f":"code","12cdb3ba":"code","ac9c416f":"code","94c929c9":"code","256ad70f":"code","5619a60b":"code","78ed7a9d":"code","ce20596b":"code","e743ef20":"code","d1bd0047":"markdown","873419b2":"markdown","b24dfaf0":"markdown","535df0d8":"markdown","31b5b946":"markdown","d0fa9e90":"markdown","0c7dbd62":"markdown","e66dde03":"markdown","606ebf01":"markdown","05936dd3":"markdown","1e9f0436":"markdown","80e4c67c":"markdown","b48578ed":"markdown","ac731823":"markdown","f7c74074":"markdown","75b9bb9a":"markdown","bbaabe20":"markdown"},"source":{"7fc9748c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport numpy\nfrom sklearn.metrics import confusion_matrix, roc_curve, auc\n\n# For Embedding\nfrom tensorflow.python.keras.preprocessing.text import Tokenizer\nfrom tensorflow.python.keras.preprocessing.sequence import pad_sequences\n\n# For Models\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding, LSTM, GRU, Flatten, Dropout, Conv1D, MaxPooling1D, Activation\nfrom keras.layers.embeddings import Embedding\n\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.models import load_model\n\nEMBEDDING_DIM = 100    # Set EMBEDDING_DIM\nbat_size = 32          # Set batch size\npat = 2                # Set patience\ndropout_rate = 0.0     # Set dropout rate","61b235ad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e7968342":"df1 = pd.DataFrame()\ndf2 = pd.DataFrame()\ndf3 = pd.DataFrame()\n\ndf1 = pd.read_csv(\"..\/input\/all-the-news\/articles1.csv\", encoding = 'utf-8')\ndf2 = pd.read_csv(\"..\/input\/all-the-news\/articles2.csv\", encoding = 'utf-8')\ndf3 = pd.read_csv(\"..\/input\/all-the-news\/articles3.csv\", encoding = 'utf-8')\n\nframes = [df1, df2, df3]\ndata = pd.concat(frames)\n\nconditions = [\n    (data['publication'] == 'Breitbart'),\n    (data['publication'] == 'New York Post'),\n    (data['publication'] == 'NPR'),\n    (data['publication'] == 'CNN'),\n    (data['publication'] == 'Washington Post'),\n    (data['publication'] == 'Reuters'),\n    (data['publication'] == 'Guardian'),\n    (data['publication'] == 'New York Times'),\n    (data['publication'] == 'Atlantic'),\n    (data['publication'] == 'Business Insider'),\n    (data['publication'] == 'National Review'),\n    (data['publication'] == 'Talking Points Memo'),\n    (data['publication'] == 'Vox'),\n    (data['publication'] == 'Buzzfeed News'),\n    (data['publication'] == 'Fox News')]\n\n# choices = [1, 1, 0, -1, -1, 0, -1, -1, -1, 0, 1, -1, -1, -1, 1]\nchoices = [1, 1, 2, 0, 0, 2, 0, 0, 0, 2, 1, 0, 0, 0, 1]\ndata['label'] = np.select(conditions, choices)\n\narticle_lengths = []\n\n# Create a column with the number of words in the article\n\nfor i in range(0, len(data['content'])):\n    article_lengths.append(len(data['content'].iloc[i].split()))\n    \ndata['words'] = article_lengths\n\n#data_pre_trained_model = data[data['publication'].str.contains('New York Times', na=False, regex=True)].append(data[data['publication'].str.contains('National Review', na=False, regex=True)])\n#data = data[data['publication'].str.contains('CNN', na=False, regex=True)].append(data[data['publication'].str.contains('Fox News', na=False, regex=True)])\n\n# Filter the articles in the dataset\ndata = data.query(\"label != 2\")\ndata = data.query(\"words <= 300\")\n\n# Check word count in the articles after query\n\narticle_lengths = []\n\nfor i in range(0, len(data['content'])):\n    article_lengths.append(len(data['content'].iloc[i].split()))","a30d6159":"display(data.head(3))\n\nprint(np.unique(data[:][['publication']]))\nprint(\"\\n\")\nprint('Number of articles:',data.shape[0])\nprint(\"\\n\")\nprint(data.label.value_counts())\nprint(\"\\n\")\nprint(data.publication.value_counts())\n\nprint(\"Review length: \")\nprint(\"Mean %.2f words (%f)\" % (numpy.mean(article_lengths), numpy.std(article_lengths)))\n\n# plot review length\nplt.title('Total Word Distribution')\nplt.ylabel('Number of Words Per Article')\nplt.xlabel('News Articles')\nplt.boxplot(article_lengths)\nplt.show()","a9163269":"# This class allows to vectorize a text corpus, by turning each text into either a sequence of integers \n# (each integer being the index of a token in a dictionary) or into a vector where the coefficient for each token could be binary, \n# based on word count, based on tf-idf...\n\n# vectorize the text samples into a 2D integer tensor\ntokenizer_obj = Tokenizer()\ntotal_artciles = data['content'].values\ntokenizer_obj.fit_on_texts(total_artciles)\n\nfor i in range(0, len(data['content'])):\n    article_lengths.append(len(data['content'].iloc[i].split()))\n\n# Pad Sequences\nmax_length = 300\n\n# Define volcabulary size\nvolcabulary_size = len(tokenizer_obj.word_index) + 1","79db83ff":"X_train, X_test, y_train, y_test = train_test_split(data['content'], data['label'], test_size=0.33, random_state=2020)\n\nprint(\"Test Set\\n\", y_test.value_counts())\nprint(\"\\nTraining Set\\n\", y_train.value_counts())\n\n# Vectorize\n################################################################################## \n\nX_train_tokens = tokenizer_obj.texts_to_sequences(X_train)\nX_test_tokens  = tokenizer_obj.texts_to_sequences(X_test)\n\nX_train_pad = pad_sequences(X_train_tokens, maxlen = max_length, padding = 'pre')\nX_test_pad = pad_sequences(X_test_tokens, maxlen = max_length, padding = 'pre')","cac89b7c":"model = Sequential()\nmodel.add(Embedding(volcabulary_size, EMBEDDING_DIM, input_length = max_length))\n\nmodel.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Dropout(dropout_rate))\n\nmodel.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Dropout(dropout_rate))\n\nmodel.add(Conv1D(filters=128, kernel_size=3, padding='same', activation='relu'))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Dropout(dropout_rate))\n\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(model.summary())","302a29be":"# simple early stopping\nes = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=pat)\nmc = ModelCheckpoint('best_transfer_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n\nhist = model.fit(X_train_pad, y_train, batch_size = bat_size, epochs=10, validation_data=(X_test_pad, y_test), verbose=1, callbacks=[es, mc])","7c168b9f":"num_epochs = len(hist.history['accuracy'])\n\nplt.style.use('seaborn-whitegrid')\nplt.plot([x for x in range(1,num_epochs+1)], hist.history['accuracy'], \"-c\",  marker='o', label='Training Accuracy', linestyle='dashdot')\nplt.plot([x for x in range(1,num_epochs+1)], hist.history['val_accuracy'], \"-m\", marker='o', label='Validation Accuracy', linestyle='dashed')\nplt.legend(loc=\"best\")\nplt.title('Tranfer Learning NN Model - Training vs. Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.xticks([x for x in range(1,num_epochs+1)])\nplt.show()","12cdb3ba":"pred = model.predict_classes(X_test_pad)\n\ncm = confusion_matrix(y_true = y_test, y_pred = pred)\nprint(cm)","ac9c416f":"# Generate the probability values\npred = model.predict(X_test_pad)\n\n# Compute ROC curve and ROC area for each class\nfpr_transfer = dict()\ntpr_transfer = dict()\nroc_auc_transfer = dict()\n\nfpr_transfer, tpr_transfer, thresholds_transfer = roc_curve(y_test.values, pred)\nroc_auc_transfer = auc(fpr_transfer, tpr_transfer)\n\nprint('AUC:', roc_auc_transfer)","94c929c9":"model = load_model('best_transfer_model.h5')\n\npretrained_model = Sequential()\n\nfor i in range(0,14):\n    layer = model.layers[i]\n    layer.trainable = False\n    pretrained_model.add(layer)\n    \npretrained_model.summary()","256ad70f":"new_model = pretrained_model \n\nnew_model.add(Dense(units=512))\nnew_model.add(Dropout(0.5))\n\nnew_model.add(Dense(1, activation='sigmoid'))\n\nnew_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nprint(new_model.summary())","5619a60b":"# simple early stopping\nes = EarlyStopping(monitor='val_accuracy', mode='max', verbose=1, patience=pat)\nmc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\n\nhist = new_model.fit(X_train_pad, y_train, batch_size = bat_size, epochs=10, validation_data=(X_test_pad, y_test), verbose=1, callbacks=[es, mc])","78ed7a9d":"num_epochs = len(hist.history['accuracy'])\n\nplt.style.use('seaborn-whitegrid')\nplt.plot([x for x in range(1,num_epochs+1)], hist.history['accuracy'], \"-c\",  marker='o', label='Training Accuracy'\n         , linestyle='dashdot')\nplt.plot([x for x in range(1,num_epochs+1)], hist.history['val_accuracy'], \"-m\", marker='o', label='Validation Accuracy'\n         , linestyle='dashed')\nplt.legend(loc=\"best\")\nplt.title('New NN Model - Training vs. Validation Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.xticks([x for x in range(1,num_epochs+1)])\nplt.show()","ce20596b":"pred = model.predict_classes(X_test_pad)\n\ncm = confusion_matrix(y_true = y_test, y_pred = pred)\nprint(cm)","e743ef20":"# Generate the probability values\npred = model.predict(X_test_pad)\n\n# Compute ROC curve and ROC area for each class\nfpr = dict()\ntpr = dict()\nroc_auc = dict()\nfpr, tpr, thresholds = roc_curve(y_test.values, pred)\nroc_auc = auc(fpr, tpr)\n\nplt.figure()\nlw = 2\n\nplt.plot(fpr, tpr, color='cornflowerblue',\n         lw=lw, label='New Model - ROC curve (area = %0.2f)' % roc_auc)\nplt.plot(fpr_transfer, tpr_transfer, color='darkorange',\n         lw=lw, label='Transfer Model - ROC curve (area = %0.2f)' % roc_auc_transfer)\n\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic (Dropout = 0)')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n# In general, an AUC of 0.5 suggests no discrimination, \n# 0.7 to 0.8 is considered acceptable, \n# 0.8 to 0.9 is considered excellent, \n# and more than 0.9 is considered outstanding.","d1bd0047":"# Word Embedding For Entire Corpus","873419b2":"# Create Pre-Trained Model","b24dfaf0":"# ROC AUC","535df0d8":"# Imports","31b5b946":"# Split DataSet to Pre-train Weights","d0fa9e90":"# Confusion Model","0c7dbd62":"# New NN Model - Training vs. Validation Accuracy","e66dde03":"# Build Model to Pre-Train Weights","606ebf01":"# Create New Model Using Pre-Trained Model","05936dd3":"# References\n\nhttps:\/\/missinglink.ai\/guides\/neural-network-concepts\/7-types-neural-network-activation-functions-right\/\n\nhttps:\/\/machinelearningmastery.com\/use-word-embedding-layers-deep-learning-keras\/\n\nhttps:\/\/towardsdatascience.com\/machine-learning-word-embedding-sentiment-classification-using-keras-b83c28087456\n\nhttps:\/\/github.com\/jeffheaton\/t81_558_deep_learning\n\nhttps:\/\/www.quora.com\/In-recurrent-neural-networks-like-LSTMs-is-it-possible-to-do-transfer-learning-Has-there-been-any-research-in-this-area\n\nhttps:\/\/machinelearningmastery.com\/sequence-classification-lstm-recurrent-neural-networks-python-keras\/\n\nhttps:\/\/keras.io\/examples\/cifar10_cnn\/\n\nhttps:\/\/towardsdatascience.com\/character-level-cnn-with-keras-50391c3adf33\n\nhttps:\/\/machinelearningmastery.com\/how-to-stop-training-deep-neural-networks-at-the-right-time-using-early-stopping\/\n\nhttps:\/\/www.dlology.com\/blog\/simple-guide-on-how-to-generate-roc-plot-for-keras-classifier\/\n\nhttps:\/\/www.bmc.com\/blogs\/keras-neural-network-classification\/\n\nhttps:\/\/www.kdnuggets.com\/2020\/02\/intent-recognition-bert-keras-tensorflow.html","1e9f0436":"# LOAD DATA","80e4c67c":"# ROC AUC","b48578ed":"# Train New Model","ac731823":"# Train Model","f7c74074":"# Tranfer Learning NN Model - Training vs. Validation Accuracy","75b9bb9a":"# Limited EDA","bbaabe20":"# Confusion Matrix"}}