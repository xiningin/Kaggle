{"cell_type":{"cc354ba9":"code","e194f5ef":"code","9411b0d0":"code","f7c91c5e":"code","a4059a3b":"code","a7af8cfc":"code","2a1df80a":"code","5759d709":"code","bc1fdbc2":"code","16280655":"code","b41c1a58":"code","7a1cc23e":"code","b6de3d8d":"code","701f4840":"code","6835d3d5":"code","ad56281c":"code","4d76af49":"code","5996d7a0":"code","d01cbfd1":"code","5702343f":"code","8e7f51a4":"code","544f0088":"code","6893b73e":"code","4f796c86":"code","c8623084":"code","38b1cd6a":"code","aead49d4":"code","95de716f":"code","aba8622d":"markdown","a930ffb4":"markdown","f268905b":"markdown","86287a91":"markdown"},"source":{"cc354ba9":"import tensorflow as tf\ntry:\n   tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  \n   print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\nexcept ValueError:\n   tpu = None\nif tpu:\n   tf.config.experimental_connect_to_cluster(tpu)\n   tf.tpu.experimental.initialize_tpu_system(tpu)\n   strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n   strategy = tf.distribute.get_strategy()","e194f5ef":"!curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!python pytorch-xla-env-setup.py --version nightly  --apt-packages libomp5 libopenblas-dev","9411b0d0":"%%time\n%autosave 60\n\nimport os\nos.environ['XLA_USE_BF16'] = \"1\"\nos.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n\nimport gc\ngc.enable()\nimport time\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm \n\nimport transformers\nfrom transformers.modeling_bert import BertPreTrainedModel\nfrom transformers import (\n    BertTokenizer,\n    BertModel,\n    BertForSequenceClassification,\n    BertConfig,\n    AdamW,\n    get_linear_schedule_with_warmup,\n    get_cosine_schedule_with_warmup,\n)\nfrom tokenizers import BertWordPieceTokenizer\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.distributed import DistributedSampler\n\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.debug.metrics as met\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.distributed.xla_multiprocessing as xmp\nimport torch_xla.utils.serialization as xser\nimport torch_xla.version as xv\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nprint('PYTORCH:', xv.__torch_gitrev__)\nprint('XLA:', xv.__xla_gitrev__)","f7c91c5e":"train = pd.read_csv('..\/input\/contradictory-my-dear-watson\/train.csv')\ntest = pd.read_csv('..\/input\/contradictory-my-dear-watson\/test.csv')\nsample_submission = pd.read_csv('..\/input\/contradictory-my-dear-watson\/sample_submission.csv')","a4059a3b":"class DatasetRetriever(Dataset):\n    def __init__(self, df, encoded):\n        self.df = df\n        self.encoded = encoded\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):   \n        ids = self.encoded['input_ids'][index]\n        type_ids = self.encoded['token_type_ids'][index]\n        mask = self.encoded['attention_mask'][index]\n        targets = self.df.iloc[index].label\n        return {\n            'ids':torch.tensor(ids),\n            'token_type_ids':torch.tensor(type_ids),\n            'mask':torch.tensor(mask),\n            'targets':targets\n        }","a7af8cfc":"class BertUncased(BertPreTrainedModel):\n    def __init__(self, config):\n        config.output_hidden_states = True\n        super(BertUncased, self).__init__(config)\n        self.num_labels = config.num_labels\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(p=0.1)\n        self.high_dropout = nn.Dropout(p=0.5)\n\n        n_weights = config.num_hidden_layers + 1\n        weights_init = torch.zeros(n_weights).float()\n        weights_init.data[:-1] = -3\n        self.layer_weights = torch.nn.Parameter(weights_init)\n        self.init_weights()\n      \n        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n    \n    def forward(self,\n        input_ids=None,\n        attention_mask=None,\n        token_type_ids=None,\n        position_ids=None,\n        head_mask=None,\n        inputs_embeds=None):\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            position_ids=position_ids,\n                            head_mask=head_mask,\n                            inputs_embeds=inputs_embeds)\n        hidden_layers = outputs[2]\n        last_hidden = outputs[0]\n        cls_outputs = torch.stack(\n            [self.dropout(layer[:, 0, :]) for layer in hidden_layers], dim=2\n        )\n        cls_output = (torch.softmax(self.layer_weights, dim=0) * cls_outputs).sum(-1)\n        logits = self.classifier(cls_output)\n        outputs = logits\n        return outputs","2a1df80a":"class AverageMeter(object):\n    def __init__(self, name, fmt=':f'):\n        self.name = name\n        self.fmt = fmt\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n    def __str__(self):\n        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n        return fmtstr.format(**self.__dict__)\n\nclass ProgressMeter(object):\n    def __init__(self, num_batches, meters, prefix=\"\"):\n        self.batch_fmtstr = self._get_batch_fmtstr(num_batches)\n        self.meters = meters\n        self.prefix = prefix\n\n    def display(self, batch):\n        entries = [self.prefix + self.batch_fmtstr.format(batch)]\n        entries += [str(meter) for meter in self.meters]\n        print('\\t'.join(entries))\n\n    def _get_batch_fmtstr(self, num_batches):\n        num_digits = len(str(num_batches \/\/ 1))\n        fmt = '{:' + str(num_digits) + 'd}'\n        return '[' + fmt + '\/' + fmt.format(num_batches) + ']'\n\ndef accuracy(output, target, topk=(1,)):\n    with torch.no_grad():\n        maxk = max(topk)\n        batch_size = target.size(0)\n\n        _, pred = output.topk(maxk, 1, True, True)\n        pred = pred.t()\n        correct = pred.eq(target.view(1, -1).expand_as(pred))\n\n        res = []\n        for k in topk:\n            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n            res.append(correct_k.mul_(100.0 \/ batch_size))\n        return res","5759d709":"def get_model_optimizer(model):\n    # Differential Learning Rate\n    def is_backbone(name):\n        return \"bert\" in name\n    \n    optimizer_grouped_parameters = [\n       {'params': [param for name, param in model.named_parameters() if is_backbone(name)], 'lr': LR},\n       {'params': [param for name, param in model.named_parameters() if not is_backbone(name)], 'lr': 1e-3} \n    ]\n    \n    optimizer = AdamW(\n        optimizer_grouped_parameters, lr=LR, weight_decay=1e-2\n    )\n    \n    return optimizer","bc1fdbc2":"def loss_fn(outputs, targets):\n    return nn.CrossEntropyLoss()(outputs, targets)","16280655":"def train_loop_fn(train_loader, model, optimizer, device, scheduler, epoch=None):\n    # Train\n    batch_time = AverageMeter('Time', ':6.3f')\n    data_time = AverageMeter('Data', ':6.3f')\n    losses = AverageMeter('Loss', ':.4e')\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    progress = ProgressMeter(\n        len(train_loader),\n        [batch_time, data_time, losses, top1],\n        prefix=\"[xla:{}]Train:  Epoch: [{}]\".format(xm.get_ordinal(), epoch)\n    )\n    model.train()\n    end = time.time()\n    for i, data in enumerate(train_loader):\n        data_time.update(time.time()-end)\n        ids = data[\"ids\"]\n        mask = data[\"mask\"]\n        targets = data[\"targets\"]\n        ids = ids.to(device, dtype=torch.long)\n        mask = mask.to(device, dtype=torch.long)\n        targets = targets.to(device, dtype=torch.float)\n        optimizer.zero_grad()\n        outputs = model(\n            input_ids = ids,\n            attention_mask = mask\n        )\n        loss = loss_fn(outputs, targets)\n        loss.backward()\n        xm.optimizer_step(optimizer)\n        loss = loss_fn(outputs, targets)\n        acc1= accuracy(outputs, targets, topk=(1,))\n        losses.update(loss.item(), ids.size(0))\n        top1.update(acc1[0].item(), ids.size(0))\n        scheduler.step()\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if i % 20 == 0:\n            progress.display(i)\n    del loss\n    del outputs\n    del ids\n    del mask\n    del targets\n    gc.collect()","b41c1a58":"def eval_loop_fn(validation_loader, model, device):\n    #Validation\n    model.eval()\n    batch_time = AverageMeter('Time', ':6.3f')\n    losses = AverageMeter('Loss', ':.4e')\n    top1 = AverageMeter('Acc@1', ':6.2f')\n    learning_rate = AverageMeter('LR',':2.8f')\n    progress = ProgressMeter(\n        len(validation_loader),\n        [batch_time, losses, top1],\n        prefix='[xla:{}]Validation: '.format(xm.get_ordinal()))\n    with torch.no_grad():\n        end = time.time()\n        for i, data in enumerate(validation_loader):\n            ids = data[\"ids\"]\n            mask = data[\"mask\"]\n            targets = data[\"targets\"]\n            ids = ids.to(device, dtype=torch.long)\n            mask = mask.to(device, dtype=torch.long)\n            targets = targets.to(device, dtype=torch.float)\n            outputs = model(\n                input_ids = ids,\n                attention_mask = mask\n            )\n            loss = loss_fn(outputs, targets)\n            acc1= accuracy(outputs, targets, topk=(1,))\n            losses.update(loss.item(), ids.size(0))\n            top1.update(acc1[0].item(), ids.size(0))\n            batch_time.update(time.time() - end)\n            end = time.time()\n            if i % 10 == 0:\n                progress.display(i)\n    del loss\n    del outputs\n    del ids\n    del mask\n    del targets\n    gc.collect()","7a1cc23e":"TRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 16\nEPOCHS = 3\nMAX_LEN = 96\n# Scale learning rate to 8 TPU's\nLR = 2e-5 * xm.xrt_world_size() \nMETRICS_DEBUG = True\nbert_model = \"bert-base-multilingual-uncased\"\nNUM_CLASSES = 3\n\ntokenizer = BertTokenizer.from_pretrained(bert_model)","b6de3d8d":"# Train Validation Split\nmask = np.random.rand(len(train)) < 0.95\ntrain_df = train[mask]\nvalid_df = train[~mask]\n\ntrain_text = train_df[['premise', 'hypothesis']].values.tolist()\ntrain_encoded = tokenizer.batch_encode_plus(\n    train_text,\n    pad_to_max_length=True,\n    max_length=MAX_LEN\n)\n\nvalid_text = valid_df[['premise', 'hypothesis']].values.tolist()\nvalid_encoded = tokenizer.batch_encode_plus(\n    valid_text,\n    pad_to_max_length=True,\n    max_length=MAX_LEN\n)\n\ntrain_df.reset_index(drop=True, inplace=True)\nvalid_df.reset_index(drop=True, inplace=True)","701f4840":"WRAPPED_MODEL = xmp.MpModelWrapper(BertUncased.from_pretrained(bert_model, num_labels=NUM_CLASSES))\ntrain_dataset = DatasetRetriever(df=train_df, encoded=train_encoded)\nvalid_dataset = DatasetRetriever(df=valid_df, encoded=valid_encoded)","6835d3d5":"def _run():\n    xm.master_print('Starting Run ...')\n    train_sampler = DistributedSampler(\n        train_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=False\n    )\n    \n    train_data_loader = DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        sampler=train_sampler,\n        drop_last=True,\n        num_workers=0\n    )\n    xm.master_print('Train Loader Created.')\n    \n    valid_sampler = DistributedSampler(\n        valid_dataset,\n        num_replicas=xm.xrt_world_size(),\n        rank=xm.get_ordinal(),\n        shuffle=False\n    )\n    \n    valid_data_loader = DataLoader(\n        valid_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        sampler=valid_sampler,\n        drop_last=True,\n        num_workers=0\n    )\n    xm.master_print('Valid Loader Created.')\n    \n    num_train_steps = int(len(train_df) \/ TRAIN_BATCH_SIZE \/ xm.xrt_world_size())\n    device = xm.xla_device()\n    model = WRAPPED_MODEL.to(device)\n    xm.master_print('Done Model Loading.')\n    optimizer = get_model_optimizer(model)\n    scheduler = get_cosine_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps = 0,\n        num_training_steps = num_train_steps * EPOCHS\n    )\n    xm.master_print(f'Num Train Steps= {num_train_steps}, XRT World Size= {xm.xrt_world_size()}.')\n    \n    for epoch in range(EPOCHS):\n        para_loader = pl.ParallelLoader(train_data_loader, [device])\n        xm.master_print('Parallel Loader Created. Training ...')\n        train_loop_fn(para_loader.per_device_loader(device),\n                      model,  \n                      optimizer, \n                      device, \n                      scheduler, \n                      epoch\n                     )\n        \n        xm.master_print(\"Finished training epoch {}\".format(epoch))\n            \n        para_loader = pl.ParallelLoader(valid_data_loader, [device])\n        xm.master_print('Parallel Loader Created. Validating ...')\n        eval_loop_fn(para_loader.per_device_loader(device), \n                     model,  \n                     device\n                    )\n        \n        # Serialized and Memory Reduced Model Saving\n        if epoch == EPOCHS-1:\n            xm.master_print('Saving Model ..')\n            xm.save(model.state_dict(), \"model.bin\")\n            xm.master_print('Model Saved.')\n            \n    if METRICS_DEBUG:\n      xm.master_print(met.metrics_report(), flush=True)","ad56281c":"def _mp_fn(rank, flags):\n    _run()\n\nFLAGS={}\nxmp.spawn(_mp_fn, args=(FLAGS,), nprocs=8, start_method='fork')","4d76af49":"!pip install captum","5996d7a0":"import matplotlib.pyplot as plt\nimport captum\nfrom captum.attr import visualization as viz\nfrom captum.attr import IntegratedGradients, LayerConductance, LayerIntegratedGradients\nfrom captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer","d01cbfd1":"device = xm.xla_device() if tpu else torch.deice(\"cpu\")\nmodel = WRAPPED_MODEL.to(device)\nmodel.load_state_dict(torch.load(\"model.bin\"))\nmodel.eval()\nmodel.zero_grad()","5702343f":"ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\nsep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\ncls_token_id = tokenizer.cls_token_id # A token used for prepending to the concatenated question-text word sequence","8e7f51a4":"def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id):\n    text_ids = tokenizer.encode(text, add_special_tokens=False)\n    # construct input token ids\n    input_ids = [cls_token_id] + text_ids + [sep_token_id]\n    # construct reference token ids \n    ref_input_ids = [cls_token_id] + [ref_token_id] * len(text_ids) + [sep_token_id]\n    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(text_ids)\n\ndef construct_input_ref_token_type_pair(input_ids, sep_ind=0):\n    seq_len = input_ids.size(1)\n    token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(seq_len)]], device=device)\n    ref_token_type_ids = torch.zeros_like(token_type_ids, device=device)# * -1\n    return token_type_ids, ref_token_type_ids\n\ndef construct_input_ref_pos_id_pair(input_ids):\n    seq_length = input_ids.size(1)\n    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n    return position_ids, ref_position_ids\n    \ndef construct_attention_mask(input_ids):\n    return torch.ones_like(input_ids)","544f0088":"def predict(inputs, token_type_ids, attention_mask):\n    return model(inputs, attention_mask, token_type_ids)\n\ndef custom_forward_0(inputs, token_type_ids, attention_mask):\n    preds = predict(inputs, token_type_ids, attention_mask)\n    return torch.softmax(preds, dim = 1)[:, 0]\n\ndef custom_forward_1(inputs, token_type_ids, attention_mask):\n    preds = predict(inputs, token_type_ids, attention_mask)\n    return torch.softmax(preds, dim = 1)[:, 1]\n\ndef custom_forward_2(inputs, token_type_ids, attention_mask):\n    preds = predict(inputs, token_type_ids, attention_mask)\n    return torch.softmax(preds, dim = 1)[:, 2]\n\ndef save_act(module, inp, out):\n  return saved_act\n\nhook = model.bert.embeddings.register_forward_hook(save_act)\nhook.remove()","6893b73e":"def process(text, label):\n    input_ids, ref_input_ids, sep_id = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id)\n    token_type_ids, ref_token_type_ids = construct_input_ref_token_type_pair(input_ids, sep_id)\n    position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n    attention_mask = construct_attention_mask(input_ids)\n\n    indices = input_ids[0].detach().tolist()\n    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n    \n    if label == 0:\n        lig = LayerIntegratedGradients(custom_forward_0, model.bert.embeddings)\n    elif label == 1:\n        lig = LayerIntegratedGradients(custom_forward_1, model.bert.embeddings)\n    elif label == 2:\n        lig = LayerIntegratedGradients(custom_forward_2, model.bert.embeddings)\n    \n    attributions_main, delta_main = lig.attribute(inputs=input_ids,\n                                                  baselines=ref_input_ids,\n                                                  n_steps = 150,\n                                                  additional_forward_args=(token_type_ids, attention_mask),\n                                                  return_convergence_delta=True)\n    \n    score = predict(input_ids, token_type_ids, attention_mask)\n    attributions_main = attributions_main.cpu()\n    delta_main = delta_main.cpu()\n    score = score.cpu()\n    add_attributions_to_visualizer(attributions_main, delta_main, text, score, label, all_tokens)\n    \ndef add_attributions_to_visualizer(attributions, delta, text, score, label, all_tokens):\n    attributions = attributions.sum(dim=-1).squeeze(0)\n    attributions = attributions \/ torch.norm(attributions)\n    attributions = attributions.cpu()\n\n    score_vis.append(\n        viz.VisualizationDataRecord(\n            attributions,\n            torch.softmax(score, dim = 1)[0][label],\n            torch.argmax(torch.softmax(score, dim = 1)[0]),\n            label,\n            text,\n            attributions.sum(),\n            all_tokens,\n            delta\n        )\n    ) ","4f796c86":"score_vis = []\nfor i, text in enumerate(train_text[:2]):\n    # print(text)\n    text = \" \".join(text)\n    label = train_df.iloc[i].label\n    process(text, label)","c8623084":"viz.visualize_text(score_vis)","38b1cd6a":"class TestDatasetRetriever(Dataset):\n    def __init__(self, df, ids, mask):\n        self.df = df\n        self.ids = ids\n        self.mask = mask\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):   \n        ids = self.ids[index]\n        mask = self.mask[index]\n        return {\n            'ids':torch.tensor(ids),\n            'mask':torch.tensor(mask)\n        }","aead49d4":"TEST_BATCH_SIZE = 32\n\ntest_ids, test_mask = fast_encode(test, fast_tokenizer)\n\ntest_dataset = TestDatasetRetriever(test, test_ids, test_mask)\n\ntest_data_loader = DataLoader(\n    test_dataset, \n    batch_size=TEST_BATCH_SIZE,\n    drop_last=False,\n    num_workers=4,\n    shuffle=False\n)\n\n# Load Serialized Model\ndevice = xm.xla_device()\nmodel = WRAPPED_MODEL.to(device).eval()\nmodel.load_state_dict(xser.load(\"model.bin\"))","95de716f":"# test_preds = []\n\n# for i, data in tqdm(enumerate(test_data_loader), total=len(test_data_loader)):\n#     ids = data[\"ids\"]\n#     mask = data[\"mask\"]\n#     ids = ids.to(device, dtype=torch.long)\n#     mask = mask.to(device, dtype=torch.long)\n#     outputs = model(\n#         input_ids = ids,\n#         attention_mask = mask,\n#     )\n#     outputs_np = outputs.cpu().detach().numpy().tolist()\n#     test_preds.extend(outputs_np)  \n    \n# test_preds = torch.FloatTensor(test_preds)\n# top1_prob, top1_label = torch.topk(test_preds, 1)\n# y = top1_label.cpu().detach().numpy()\n# sample_submission.prediction = y\n# sample_submission.to_csv('submission.csv', index=False)","aba8622d":"### Interpretation of BERT","a930ffb4":"### Run","f268905b":"### Train Config","86287a91":"#### Integrated Gradients"}}