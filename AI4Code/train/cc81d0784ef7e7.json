{"cell_type":{"aa55965a":"code","56294761":"code","f02f04f3":"code","ff152abe":"code","86da1ec0":"code","2cabe939":"code","bbe45e88":"code","2cf5c55a":"code","327b3483":"code","65a6fb01":"code","203f6bc5":"code","b9bd8e0a":"code","6d8b2d57":"code","d83c0100":"code","6a3d2fbf":"code","30e3b531":"markdown","8a888160":"markdown","742c6198":"markdown","9702bd7a":"markdown","8fd34544":"markdown","7cea33ed":"markdown","8ea5ca2f":"markdown","9454d415":"markdown","2d7e6cf0":"markdown","39439e0e":"markdown","18306348":"markdown","31d7662f":"markdown"},"source":{"aa55965a":"from transformers import BertTokenizerFast, BertModel\nimport torch\nfrom torch import nn","56294761":"tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","f02f04f3":"tokenizer.tokenize(\"[CLS] Hello world, how are you?\")","ff152abe":"tokenizer.tokenize(\"[newtoken] Hello world, how are you?\")","86da1ec0":"tokenizer.add_tokens(['[newtoken]'])\n\ntokenizer.tokenize(\"[newtoken] Hello world, how are you?\")","2cabe939":"tokenized = tokenizer(\"[newtoken] Hello world, how are you?\", add_special_tokens=False, return_tensors=\"pt\")\nprint(tokenized['input_ids'])\n\ntkn = tokenized['input_ids'][0, 0]\nprint(\"First token:\", tkn)\nprint(\"Decoded:\", tokenizer.decode(tkn))","bbe45e88":"model = BertModel.from_pretrained('bert-base-uncased')\n\nmodel.embeddings","2cf5c55a":"try:\n    out = model(**tokenized)\n    out.last_hidden_state\nexcept Exception as e:\n    print(e)","327b3483":"weights = model.embeddings.word_embeddings.weight.data\nprint(weights.shape)","65a6fb01":"new_weights = torch.cat((weights, weights[101:102]), 0)\n\nnew_emb = nn.Embedding.from_pretrained(new_weights, padding_idx=0, freeze=False)\nnew_emb","203f6bc5":"model.embeddings.word_embeddings = new_emb\n\nmodel.embeddings","b9bd8e0a":"out = model(**tokenized)\nout.last_hidden_state","6d8b2d57":"model = BertModel.from_pretrained('bert-base-uncased')","d83c0100":"out2 = model(\n    **tokenizer(\"[CLS] Hello world, how are you?\", add_special_tokens=False, return_tensors=\"pt\")\n)","6a3d2fbf":"torch.all(out.last_hidden_state == out2.last_hidden_state)","30e3b531":"## Adding a new token to the tokenizer","8a888160":"If we use `[CLS]`, it will work:","742c6198":"## Adding a new token to bert","9702bd7a":"But `[newtoken]` will not since it's not in the tokenizer vocab:","8fd34544":"Let's see the dimensions of the embeddings weights","7cea33ed":"And try to run our tokenized text into the model again:","8ea5ca2f":"We need to add a new dimension. we will copy `[CLS]` and use it to initialize the new index:","9454d415":"Optional sanity check: let's see if it the result will be the same if we replace the token with `[CLS]`","2d7e6cf0":"Let's add that new `nn.Embedding` back to our model:","39439e0e":"This will not work, as the new token is out of index by default:","18306348":"Let's try to add it and try again:","31d7662f":"Let's see what the index is:"}}