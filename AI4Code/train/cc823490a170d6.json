{"cell_type":{"d753ea44":"code","d2af2b1e":"code","c4d6f006":"code","9891bdc2":"code","fb7c90c2":"code","e89b13bd":"code","56f695b8":"code","5ff09003":"code","57a89ca8":"code","6d579318":"code","c16ab32f":"code","b2d5442f":"code","05f3356b":"code","6f7fe68c":"markdown","6ca9a0a9":"markdown","6e1b2b85":"markdown","706b369b":"markdown","80402062":"markdown","082159d3":"markdown","0750da5d":"markdown","1c8737a3":"markdown","b6cebc6d":"markdown","0fce1d87":"markdown","1c5326c9":"markdown","bf961ad1":"markdown"},"source":{"d753ea44":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import init_notebook_mode, download_plotlyjs, iplot\nimport cufflinks as cf\ninit_notebook_mode(connected=True)\ncf.go_offline()\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\nprint()\nprint(\"The files in the dataset are:-\")\nfrom subprocess import check_output\nprint(check_output(['ls','..\/input']).decode('utf'))\n\n# Any results you write to the current directory are saved as output.","d2af2b1e":"# Importing the dataset.\nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\ndf = pd.read_csv('..\/input\/housing.csv', delim_whitespace=True, names=names)","c4d6f006":"df.head()","9891bdc2":"df.info()","fb7c90c2":"df.corr().iplot(kind='heatmap', )","e89b13bd":"# Importing of Useful libraries from sklearn library.\nfrom sklearn.preprocessing import StandardScaler   # For Scaling the dataset\nfrom sklearn.model_selection import train_test_split    # For Splitting the dataset\nfrom sklearn.linear_model import LinearRegression      # For Linear regression\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score","56f695b8":"# Let us Create Feature matrix and Target Vector.\nx_train = df.iloc[:,:-1].values\ny_train = df.iloc[:,-1].values","5ff09003":"sc_X=StandardScaler()\nx_train=sc_X.fit_transform(x_train)","57a89ca8":"from sklearn.decomposition import PCA\npca = PCA(n_components=None)\nx_train = pca.fit_transform(x_train)\n\nexplained_variance = pca.explained_variance_ratio_\nexplained_variance\n","6d579318":"print(f\"The sum of initial 5 values is \\t {0.47+0.11+0.09+0.06+0.06} , which is very good.\" )\nprint(\"So we will choose 5 number of features and reduce our training feature matrix to 5 features\/columns. \")\n","c16ab32f":"pca = PCA(n_components=5)\nx_train = pca.fit_transform(x_train)","b2d5442f":"def all_models():    \n    # Multi-linear regression Model. \n    regressor_multi = LinearRegression()\n    regressor_multi.fit(x_train,y_train)\n    # Let us check the accuray\n    accuracy = cross_val_score(estimator=regressor_multi, X=x_train, y=y_train,cv=10)\n    print(f\"The accuracy of the Multi-linear Regressor Model is \\t {accuracy.mean()}\")\n    print(f\"The deviation in the accuracy is \\t {accuracy.std()}\")\n    print()\n    \n    # Polynomial Regression\n    from sklearn.preprocessing import PolynomialFeatures\n    poly_reg=PolynomialFeatures(degree=4) #These 3 steps are to convert X matrix into X polynomial\n    x_poly=poly_reg.fit_transform(x_train) #matrix. \n    regressor_poly=LinearRegression()\n    regressor_poly.fit(x_poly,y_train)\n    # Let us check the accuray\n    accuracy = cross_val_score(estimator=regressor_poly, X=x_train, y=y_train,cv=10)\n    print(f\"The accuracy of the Polynomial Regression Model is \\t {accuracy.mean()}\")\n    print(f\"The deviation in the accuracy is \\t {accuracy.std()}\")\n    print()\n    \n    # Random Forest Model\n    regressor_random = RandomForestRegressor(n_estimators=100,)\n    regressor_random.fit(x_train,y_train)\n    # Let us check the accuray\n    accuracy = cross_val_score(estimator=regressor_random, X=x_train, y=y_train,cv=10)\n    print(f\"The accuracy of the Random Forest Model is \\t {accuracy.mean()}\")\n    print(f\"The deviation in the accuracy is \\t {accuracy.std()}\")\n    print()\n    \n    # SVR \n    regressor_svr = SVR(kernel='rbf')\n    regressor_svr.fit(x_train, y_train)\n    # Let us check the accuracy\n    accuracy = cross_val_score(estimator=regressor_svr, X=x_train, y=y_train,cv=10)\n    print(f\"The accuracy of the SVR Model is \\t {accuracy.mean()}\")\n    print(f\"The deviation in the accuracy is \\t {accuracy.std()}\")\n    print()\n    \n    # Decision Tress Model\n    regressor_deci = DecisionTreeRegressor()\n    regressor_deci.fit(x_train, y_train)\n    # Let us check the accuracy\n    accuracy = cross_val_score(estimator=regressor_deci, X=x_train, y=y_train,cv=10)\n    print(f\"The accuracy of the Decision Tree Model is \\t {accuracy.mean()}\")\n    print(f\"The deviation in the accuracy is \\t {accuracy.std()}\")\n    \n    \n\n    ","05f3356b":"# Let us run all models together. If we have large dataset then we will not run all models together.\n# Then we will run one model at a time, otherwise your processor will struck down.\nall_models()","6f7fe68c":"#### Scaling of Feature matrix.","6ca9a0a9":"#### Observation:-\n* These are correlation matrix between all variables.\n* The valriables which are highly correlated with MEDV, we need to select only those variable to make prediction.\n* But this thing we will do with the help of dimensionalty reduction algorithm (PCA). ","6e1b2b85":"* In this dataset MEDV is our Target, we have to predict the values of MEDV on the basis of all other variables.\n* MEDV: Median value of owner-occupied homes in $1000s \n* Our task is to predict the value of MEDV.","706b369b":"# REGRESSION:-","80402062":"<img src='https:\/\/drive.google.com\/uc?id=1rxqFy4bsnYH325VpZwjZVeKfgKa1b4oS' width=1000 >","082159d3":"### In this Notebook we will Learn:-\n* Basic EDA.\n* Aplly Scaling on Feature matrix.\n* Dimensionality Reduction (PCA) .\n* K-Cross validation to check accuracy.\n* Multi-linear Regression\n* Polynomial Regression\n* Support Vector Regressor (SVR)\n* Decision Tress Regressor \n* Random Forest Regressor","0750da5d":"# IF THIS KERNEL IS HELPFUL, THEN PLEASE UPVOTE.\n<img src='https:\/\/drive.google.com\/uc?id=17o_bxPmndgdL9Y6PPdcIXOBsJ0jizDNG' width=500 >","1c8737a3":"* There is no null values in the dataset and all values are in their proper format.\n* Dataset is in its proper format, so we will go to regression model.","b6cebc6d":"### Data Preprocessing = \n                     * In this we will follow 4 steps, MCSS.\n                     * M = dealing with Missing data\n                     * C = Dealing with the categorical dataset.\n                     * S = Splitting of dataset.\n                     * S = Scaling of the dataset.\n* As there is no missing values in the dataset.\n* There is no categorical values in the dataset.\n* As dataset is very small there is no need to split the dataset.\n* Before applying the dimensionalty reduction algorithm (PCA), we will follow 1 step i.e. Scaling of dataset.","0fce1d87":"#### Observation:-\n* The best model is Multi-linear Regression.\n* In multi-linear Regressio, we are getting the accuracy of 31% and deviation of 54%.\n* The accuracy we  are getting is not that much good due many factors like less quantity of dataset, data not collected properly, something  wrong at the time of web scraping.","1c5326c9":"# REGRESSION From Scratch With BOSTON HOUSE PRICE PREDICTION","bf961ad1":"#### Dimensionalty Reduction by PCA.\n* We are doing this to reduce the number of dimensions\/features in the dataset.\n* The features which have less effect on the prediction , we will remove those features.\n* It also boosts the process.\n* It saves time.\n* Here we will use Principal Component Analysis (PCA) with 'rbf' kernel."}}