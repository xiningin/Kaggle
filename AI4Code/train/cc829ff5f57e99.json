{"cell_type":{"14b0afe2":"code","88ea431f":"code","f322b346":"code","3330ed98":"code","c587fa3a":"code","9719d556":"code","61011d65":"code","d2d26942":"code","e2e99ad1":"code","1a7566cf":"code","bfaa4ce0":"code","3a79f938":"code","8a37c410":"code","b00dac2a":"code","0995c323":"code","3eea4ce5":"code","5c3831ba":"code","7d16b278":"code","d1a000c0":"code","acfd3993":"code","3962e7f5":"code","3a0dc979":"code","058a4f42":"code","87886ac5":"code","c797c390":"code","cc01a960":"code","be6ab0b2":"code","1fb1bb5a":"code","06cc8fd4":"code","4a78ce77":"code","78827725":"code","7c2a4030":"code","4ae82087":"code","d3952bc1":"code","7888898c":"code","2689a194":"code","abbe302c":"code","46c3db91":"code","86e2fd84":"code","37b729f3":"code","d293b393":"code","33d53e4c":"code","691b6898":"code","114aad6b":"code","6882564c":"code","55f4fdf0":"code","ce208ad2":"code","831e9ff5":"markdown","2ef79311":"markdown","60e51c2e":"markdown","3a446119":"markdown"},"source":{"14b0afe2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","88ea431f":"!pip install catboost","f322b346":"from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn import preprocessing\nfrom sklearn import datasets\nfrom sklearn import metrics\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.datasets import make_regression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.feature_selection import f_regression\nfrom matplotlib import pyplot\nimport tensorflow\ntensorflow.random.set_seed(1)\nfrom tensorflow.python.keras.layers import Dense\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.python.keras.models import Sequential\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nimport xgboost as xg \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import mean_squared_error as MSE \nfrom catboost import CatBoostRegressor\nimport re \nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score","3330ed98":"os.path.join(dirname,'train.csv')","c587fa3a":"df_train=pd.read_csv(os.path.join(dirname,'train.csv'))\ndf_test=pd.read_csv(os.path.join(dirname,'test.csv'))","9719d556":"df_train.head()","61011d65":"df_train.info()","d2d26942":"'cat0'+'_A'","e2e99ad1":"df_train.describe()","1a7566cf":"df_train['cat6'].unique()","bfaa4ce0":"df_train.describe()","3a79f938":"y=df_train['target'].values\ndf_train=df_train.drop(['target'],axis=1)","8a37c410":"\nfor col in df_train.columns:\n    if re.match('cat[0-9]',col):\n        df_train=pd.concat([df_train,pd.get_dummies(df_train[col],prefix=col)],axis=1)\n        df_train=df_train.drop([col,col+'_A'],axis=1)\n    ","b00dac2a":"df_train.head()","0995c323":"### KDE of continous variable:\n\nfor col in df_train.columns:\n    if re.match('cont[0-9]*',col):\n        plt.figure(figsize=(6, 4))\n        sns.kdeplot(df_train[col], shade=True)\n        plt.title(col)\n        plt.tight_layout()\nplt.show()","3eea4ce5":"df_train.columns","5c3831ba":"X=df_train.iloc[:,1:].values","7d16b278":"negative_columns=np.any(X,axis=0)\nprint(negative_columns,negative_columns.shape)","d1a000c0":"#Normalizing Input features\nnames = df_train.iloc[:,1:].columns\nscaler = preprocessing.MinMaxScaler((0,1))\nscaled_df = scaler.fit_transform(df_train.iloc[:,1:])\nscaled_df = pd.DataFrame(scaled_df, columns=names)","acfd3993":"X=scaled_df.values","3962e7f5":"X.shape\n","3a0dc979":"\nfor col in df_test.columns:\n    if re.match('cat[0-9]',col):\n        df_test=pd.concat([df_test,pd.get_dummies(df_test[col],prefix=col)],axis=1)\n        df_test=df_test.drop([col,col+'_A'],axis=1)\n    ","058a4f42":"X_test=df_test.iloc[:,1:].values\nX_test.shape","87886ac5":"id_=df_test.iloc[:,0].values","c797c390":"names_test= df_test.iloc[:,1:].columns\nscaled_df_test = scaler.transform(df_test.iloc[:,1:])\nscaled_df_test = pd.DataFrame(scaled_df_test, columns=names_test)\nX_test=scaled_df_test.values","cc01a960":"## Feature Selection ( MUTUAL INFORMATION)\n\ndef select_features(X_train, y_train, X_test):\n    fs = SelectKBest(score_func=f_regression, k=20) ##best 20 features\n    fs.fit(X_train, y_train)\n    X_train_fs = fs.transform(X_train)\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs\n","be6ab0b2":"X.shape,X_test.shape","1fb1bb5a":"X_train_fs, X_test_fs, fs = select_features(X, y, X_test)\n\nfor i in range(len(fs.scores_)):\n    print('Feature %d: %f' % (i, fs.scores_[i]))\n\npyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)\npyplot.show()","06cc8fd4":"xgb = xg.XGBRegressor(objective ='reg:linear', \n                  n_estimators = 60, seed = 123) \nkfold = KFold(n_splits=10, random_state=7)\n\n\nresults = cross_val_score(xgb,X_train_fs, y, cv=kfold)\nprint(\"Accuracy: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\nxgb.fit(X_train_fs, y) \npred = xgb.predict(X_test_fs) ","4a78ce77":"pred.shape","78827725":"csvlist=[]\nfor i in range(pred.shape[0]):\n    csvlist.append([id_[i],pred[i]])","7c2a4030":"Output=pd.DataFrame(csvlist,columns=['id','target'])\nOutput.to_csv('output_XGB.csv',index=False)","4ae82087":"model=CatBoostRegressor(iterations=100, depth=10, learning_rate=0.01, loss_function='RMSE')\n","d3952bc1":"model.fit(X_train_fs,y ,plot=True)","7888898c":"pred=model.predict(X_test_fs)","2689a194":"csvlist=[]\nfor i in range(pred.shape[0]):\n    csvlist.append([id_[i],pred[i]])","abbe302c":"Output=pd.DataFrame(csvlist,columns=['id','target'])\nOutput.to_csv('output_CATBOOST.csv',index=False)","46c3db91":"model = Sequential()\nmodel.add(Dense(60, input_dim=20, kernel_initializer='normal', activation='relu'))\nmodel.add(Dense(2670, activation='relu'))\nmodel.add(Dense(2670, activation='relu'))\nmodel.add(Dense(1345, activation='relu'))\nmodel.add(Dense(1345, activation='relu'))\nmodel.add(Dense(1, activation='linear'))\nmodel.summary()","86e2fd84":"model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\nearlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min')\nmcp_save = ModelCheckpoint('model_pca_MLP.h5', save_best_only=True, monitor='val_loss', mode='min')\nreduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.05, patience=5, verbose=1, epsilon=1e-4, mode='min')\nhistory_pl = model.fit(X_train_fs,y, batch_size=128, epochs=30, verbose=1, \n                       callbacks=[earlyStopping, mcp_save,reduce_lr_loss], validation_split=0.25 )","37b729f3":"history_pl.history\n","d293b393":"\nplt.title('MSE Plot for MLP')\nplt.xlabel('Epochs')\nplt.ylabel('MSE')\nplt.plot(list(range(0,30)),history_pl.history['mse'])\nplt.plot(list(range(0,30)),history_pl.history['val_mse'])","33d53e4c":"\nplt.title('MAE Plot for MLP')\nplt.xlabel('Epochs')\nplt.ylabel('MAE')\nplt.plot(list(range(0,30)),history_pl.history['mae'])\nplt.plot(list(range(0,30)),history_pl.history['val_mae'])","691b6898":"\nplt.title('Loss Plot for MLP')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.plot(list(range(0,30)),history_pl.history['loss'])\nplt.plot(list(range(0,30)),history_pl.history['val_loss'])","114aad6b":"from keras.models import load_model","6882564c":"model=load_model('model_pca_MLP.h5')\npredictions = model.predict(X_test_fs)","55f4fdf0":"csvlist=[]\nfor i in range(predictions.shape[0]):\n    csvlist.append([id_[i],predictions[i][0]])","ce208ad2":"Output=pd.DataFrame(csvlist,columns=['id','target'])\nOutput.to_csv('output_mlp.csv',index=False)","831e9ff5":"XGBOOST","2ef79311":"CATBOOST","60e51c2e":"MLP\n","3a446119":"# TEST DATA"}}