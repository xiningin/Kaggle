{"cell_type":{"e3118d05":"code","03af52f2":"code","a8614c3d":"code","970655fd":"code","ed978dfa":"code","69e683f7":"code","5095d26d":"code","cb5c9f63":"code","83f84763":"code","8a2d9d09":"code","cf70941f":"code","14feb85e":"markdown","588d00b3":"markdown","9ced871f":"markdown","bbac31ab":"markdown","1cc12789":"markdown","7576e906":"markdown","cb281a4d":"markdown","37b272d0":"markdown"},"source":{"e3118d05":"import json\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import ttest_ind\nfrom scipy.stats import linregress\nfrom scipy.signal import convolve2d\nfrom tqdm.notebook import tqdm\n\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error","03af52f2":"df_users = pd.read_csv('student_data\/df_users.csv')\ndf_sales_raw = pd.read_csv('student_data\/df_sales.csv')","a8614c3d":"df_sales_raw['sales'].hist(bins=100);","970655fd":"df_sales_drop_outlier = df_sales_raw[df_sales_raw['sales'] <= 5000].copy()\ndf_sales = df_sales_drop_outlier.groupby(['user_id', 'day'])[['sales']].sum().reset_index()","ed978dfa":"def get_feature_target_trends(\n    df_sales: pd.DataFrame, user_ids: np.array, days_x: np.array, days_y: np.array\n):\n    \"\"\"\u0413\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438, \u0446\u0435\u043b\u0435\u0432\u0443\u044e \u043c\u0435\u0442\u0440\u0438\u043a\u0443 \u0438 \u0442\u0440\u0435\u043d\u0434 \u0434\u043b\u044f \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \/ \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f.\n\n    df_sales - \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u043e \u043f\u0440\u043e\u0434\u0430\u0436\u0430\u0445\n    user_ids - id \u043a\u043b\u0435\u043d\u0442\u043e\u0432, \u0434\u043b\u044f \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u043d\u0443\u0436\u043d\u043e \u0441\u0433\u0435\u043d\u0435\u0440\u0438\u0442\u044c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\n    days_x - \u0434\u043d\u0438, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044b\u0435 \u0434\u043b\u044f \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432. 4 \u043d\u0435\u0434\u0435\u043b\u0438.\n    days_y - \u0434\u043d\u0438, \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c\u044b\u0435 \u0434\u043b\u044f \u0432\u044b\u0447\u0438\u0441\u043b\u0435\u043d\u0438\u044f \u0442\u0430\u0440\u0433\u0435\u0442\u0430. 1 \u043d\u0435\u0434\u0435\u043b\u044f, \u0441\u0440\u0430\u0437\u0443 \u043f\u043e\u0441\u043b\u0435 days_x.\n\n    return:\n        X - \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438,\n        Y_for_predict - \u0442\u0430\u0440\u0433\u0435\u0442 \u0441 \u0432\u044b\u0447\u0442\u0435\u043d\u043d\u044b\u043c \u0442\u0440\u0435\u043d\u0434\u043e\u043c,\n        Y_real - \u0442\u0430\u0440\u0433\u0435\u0442 \u0431\u0435\u0437 \u0432\u044b\u0447\u0442\u0435\u043d\u043d\u043e\u0433\u043e \u0442\u0440\u0435\u043d\u0434\u0430,\n        add_y_trendpd - \u0434\u043e\u0431\u0430\u0432\u043a\u0430 \u0442\u0440\u0435\u043d\u0434\u0430 \u043a \u0442\u0430\u0440\u0433\u0435\u0442\u044b\n    \"\"\"\n    days = np.hstack([days_x, days_y])\n    user_id_to_index = {user_id: idx for idx, user_id in enumerate(user_ids)}\n    day_to_index = {day: idx for idx, day in enumerate(days)}\n    \n    # \u0434\u0435\u043b\u0430\u0435\u043c \u043c\u0430\u0442\u0440\u0438\u0446\u0435 \u0441 \u043f\u0440\u043e\u0434\u0430\u0436\u0430\u043c\u0438 [user_id * day]\n    array_sales = np.zeros((len(user_ids), len(days)))\n    for user_id, day, sales in tqdm(df_sales[df_sales['day'].isin(days)][['user_id', 'day', 'sales']].values):\n        array_sales[user_id_to_index[user_id], day_to_index[day]] = sales\n        \n    # \u0441\u0433\u043b\u0430\u0436\u0438\u0432\u0430\u0435\u043c \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0434\u043b\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432\n    smooth_days = 14\n    array_sales_smooth = convolve2d(\n        array_sales[:, :len(days_x)],\n        np.ones((1, smooth_days)) \/ smooth_days,\n        'valid'\n    )\n    \n    # c\u0447\u0438\u0442\u0430\u0435\u043c \u0442\u0440\u0435\u043d\u0434\u044b\n    x_ = np.arange(array_sales_smooth.shape[1])\n    trends = np.array([linregress(x_, y)[0] for y in tqdm(array_sales_smooth)])\n    # \u0432\u044b\u0447\u0438\u0442\u0430\u0435\u043c \u0442\u0440\u0435\u043d\u0434\u044b ToDO\n    trend_matrix = np.ones((len(array_sales), 1)) * np.arange(len(days)) * trends.reshape(-1, 1)\n    array_sales_drop_trend = array_sales - trend_matrix\n    # \u0447\u0442\u043e \u043d\u0443\u0436\u043d\u043e \u0431\u0443\u0434\u0435\u0442 \u0434\u043e\u0431\u0430\u0432\u0438\u0442\u044c \u043a Y\n    add_y_trend = trend_matrix[:, -len(days_y):].sum(axis=1)\n    \n    # \u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0442\u0430\u0440\u0433\u0435\u0442\n    Y_for_predict = pd.Series(\n        array_sales_drop_trend[:, -len(days_y):].sum(axis=1),\n        index=user_ids\n    )\n    Y_real = pd.Series(\n        array_sales[:, -len(days_y):].sum(axis=1),\n        index=user_ids    \n    )\n    \n    ##########################################################\n    # \u0441\u0447\u0438\u0442\u0430\u0435\u043c \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438\n    dict_features = dict()\n    \n    # \u0441\u0440\u0435\u0434\u043d\u0438\u0435 \u043f\u0440\u043e\u0434\u0430\u0436\u0438 \u0437\u0430 \u043f\u0440\u043e\u0448\u043b\u044b\u0435 4 \u043d\u0435\u0434\u0435\u043b\u0438\n    for week in range(4):\n        dict_features[f'mean_daily_sales_{week}_week'] = (\n            array_sales_drop_trend[:, week * 7: (week + 1) * 7].mean(axis=1)\n        )\n        \n    # \u043f\u0440\u043e\u0434\u0430\u0436\u0438 \u0437\u0430 \u043f\u0440\u043e\u0448\u043b\u044b\u0435 7 \u0434\u043d\u044f\n    for last_day in range(1, 8):\n        dict_features[f'daily_sales_day_minus_{last_day}'] = (\n            array_sales_drop_trend[:, day_to_index[days_x[-last_day]]]\n        )\n        \n    # \u043a\u043e\u043b-\u0432\u043e \u0434\u043d\u0435\u0439 \u0441 \u043f\u043e\u043a\u0443\u043f\u043a\u0430\u043c\u0438\n    dict_features['count_day_with_sales'] = (\n        (array_sales[:, :len(days_x)] > 0).sum(axis=1)\n    )\n    # c\u0443\u043c\u043c\u0430 \u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u0435\u0439 \u043f\u043e\u043a\u0443\u043f\u043e\u043a\n    dict_features['mean_sales_drop_trend'] = (\n        array_sales_drop_trend[:, :len(days_x)].mean(axis=1)\n    )\n    # quantiles \u043d\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u0430\u044f \u0442\u0440\u0430\u0442\u0430\n    for quantile in [0, 0.2, 0.4, 0.6, 0.8, 1]:\n        dict_features[f'sales_quantile_{quantile}'] = np.quantile(\n            array_sales_drop_trend[:, :len(days_x)], quantile, axis=1\n        )\n    X = pd.DataFrame(dict_features, index=user_ids)\n    return X, Y_for_predict, Y_real, add_y_trend\n","69e683f7":"%%time\n\n\nuser_ids = df_users['user_id'].values\n\n\nX_fit, Y_for_predict_fit, Y_real_fit, add_y_trend_fit = get_feature_target_trends(\n    df_sales,\n    user_ids,\n    days_x=np.arange(14, 42),\n    days_y=np.arange(42, 49)\n)\n\nX_test, Y_for_predict_test, Y_real_test, add_y_trend_test = get_feature_target_trends(\n    df_sales,\n    user_ids,\n    days_x=np.arange(21, 49),\n    days_y=np.arange(49, 56)\n)","5095d26d":"val_user_ids = np.random.choice(user_ids, int(len(user_ids) \/ 5), False)\ntrain_user_ids = np.array(list(set(user_ids) - set(val_user_ids)))\n\nX_val = X_fit.loc[val_user_ids]\nX_train = X_fit.loc[train_user_ids]\n\ny_val = Y_for_predict_fit.loc[val_user_ids]\ny_train = Y_for_predict_fit.loc[train_user_ids]","cb5c9f63":"gbm = lgb.LGBMRegressor(\n    num_leaves=31,\n    learning_rate=0.1,\n    n_estimators=300\n)\ngbm.fit(\n    X_train, y_train,\n    eval_set=[(X_val, y_val)],\n    eval_metric='l1',\n    early_stopping_rounds=5,\n    verbose=False\n)","83f84763":"predict_naive = Y_real_fit\npredict_train = gbm.predict(X_fit) + add_y_trend_fit\npredict_test = gbm.predict(X_test) + add_y_trend_test\n\nscore_train = mean_squared_error(Y_real_fit, predict_train)\nscore_test = mean_squared_error(Y_real_test, predict_test)\nscore_test_naive = mean_squared_error(Y_real_test, predict_naive)\n\ncorr_predict_fact = np.corrcoef(predict_test, Y_real_test)[0, 1]\n\nprint('scores:')\nprint(f'  test naive  = {score_test_naive:0.0f}')\nprint(f'  train       = {score_train:0.0f}')\nprint(f'  test        = {score_test:0.0f}')\nprint()\nprint(f'corr(predict_test, fact_test) = {corr_predict_fact:0.3f}')","8a2d9d09":"df_metrics = pd.DataFrame(\n    {'sales': Y_real_test, 'sales_predict': predict_test},\n    index=Y_real_test.index\n)\n\n# CUPED\ncovariance = np.cov(predict_train, Y_real_fit)[0, 1]\nvariance = np.var(predict_train)\ntheta = covariance \/ variance\n\ndf_metrics['sales_cuped'] = df_metrics['sales'] - theta * df_metrics['sales_predict']\n\ndf_metrics = df_metrics.sort_index()","cf70941f":"# \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0432 \u0441\u0442\u0440\u043e\u043a\u0443, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0431\u0443\u0434\u0435\u0442 \u0447\u0438\u0442\u0430\u0442\u044c\u0441\u044f \u0438\u043d\u0442\u0435\u0440\u043f\u0440\u0438\u0442\u0430\u0442\u043e\u0440\u043e\u043c python \u043a\u0430\u043a \u0441\u043f\u0438\u0441\u043e\u043a.\n# \u041e\u043a\u0440\u0443\u0433\u043b\u0438\u043c \u0434\u043e \u0446\u0435\u043b\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439, \u0447\u0442\u043e\u0431\u044b \u0437\u0430\u043d\u0438\u043c\u0430\u043b\u043e \u043c\u0435\u043d\u044c\u0448\u0435 \u043c\u0435\u0441\u0442\u0430 \u0432 \u0444\u0430\u0439\u043b\u0435 \u0441 \u0440\u0435\u0448\u0435\u043d\u0438\u0435\u043c.\n# \u0412 \u0444\u0430\u0439\u043b\u0435 \u0441 \u0440\u0435\u0448\u0435\u043d\u0438\u0435\u043c \u0431\u0443\u0434\u0435\u0442 \u0432\u044b\u0433\u043b\u044f\u0434\u0435\u0442\u044c \u0442\u0430\u043a: \"METRIC_VALUES = [123,43,22,...]\"\nlist_metric_int = df_metrics['sales_cuped'].round().astype(int).to_list()\nstr_list_metric_int = json.dumps(list_metric_int, separators=(',', ':'))\nline_metrics = f\"METRIC_VALUES = {str_list_metric_int}\\n\"\n\n# \u041f\u0440\u043e\u0447\u0438\u0442\u0430\u0435\u043c \u0444\u0430\u0439\u043b \u0441 \u0448\u0430\u0431\u043b\u043e\u043d\u043e\u043c \u0440\u0435\u0448\u0435\u043d\u0438\u044f\nwith open('solution_.py', 'rb') as f:\n    lines_solution_ = f.readlines()\n\n# \u0417\u0430\u043c\u0435\u043d\u0438\u043c \u043f\u0435\u0440\u0432\u0443\u044e \u0441\u0442\u0440\u043e\u043a\u0443, \u0437\u0430\u043f\u0438\u0448\u0435\u043c \u043a\u043e\u0434 \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u0432 \u043d\u043e\u0432\u044b\u0439 \u0444\u0430\u0439\u043b\nwith open('solution.py', 'wb') as f:\n    f.write(line_metrics.encode())\n    for line in lines_solution_[2:]:\n        f.write(line)","14feb85e":"## 3.2. \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u043c\u043e\u0434\u0435\u043b\u044c","588d00b3":"# 2. \u0423\u0434\u0430\u043b\u0435\u043d\u0438\u0435 \u0432\u044b\u0431\u0440\u043e\u0441\u043e\u0432 \u0438 \u0430\u0433\u0440\u0435\u0433\u0430\u0446\u0438\u044f \u043f\u043e \u0434\u043d\u044f\u043c","9ced871f":"# 1. \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","bbac31ab":"## 3.1. C\u043e\u0431\u0438\u0440\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435","1cc12789":"# 3. \u041e\u0431\u0443\u0447\u0438\u043c \u043c\u043e\u0434\u0435\u043b\u044c \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u043d\u0435\u0434\u0435\u043b\u044c\u043d\u044b\u0435 \u043f\u0440\u043e\u0434\u0430\u0436\u0438\n\n\u0411\u0443\u0434\u0435\u043c \u043f\u043e \u0434\u0430\u043d\u043d\u044b\u043c \u0437\u0430 \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0438\u0435 4 \u043d\u0435\u0434\u0435\u043b\u0438 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c \u0441\u043b\u0435\u0434\u0443\u044e\u0449\u0443\u044e \u043d\u0435\u0434\u0435\u043b\u044c\u043d\u0443\u044e \u0432\u044b\u0440\u0443\u0447\u043a\u0443.","7576e906":"\u0420\u0430\u0437\u043e\u0431\u044a\u0451\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0434\u043b\u044f \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u043d\u0430 train \u0438 validation.","cb281a4d":"## 3.3. \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u0435\u043c \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0432 \u0444\u0430\u0439\u043b \u0441 \u0440\u0435\u0448\u0435\u043d\u0438\u0435\u043c","37b272d0":"\u0414\u043d\u0438 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f\n0 - 7 | 14 - 21 - 28 - 35 - 42 | 49 | 56\n\n\u0414\u043d\u0438 \u0442\u0435\u0441\u0442\u0430\n0 - 7 - 14 | 21 - 28 - 35 - 42 - 49 | 56 |"}}