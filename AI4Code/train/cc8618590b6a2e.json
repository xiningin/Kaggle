{"cell_type":{"d10b70e2":"code","68818e34":"code","303106d6":"code","9496c3ad":"code","c1f1518c":"code","a46fa9e8":"code","b09054b3":"code","29d7d282":"code","50c12f2e":"code","1e8c78a4":"code","85115b9c":"code","18725c6c":"code","e78c79d4":"code","ec654da1":"code","23cd891d":"code","02825294":"code","8b3547cf":"code","dd4af5b2":"code","fe98196c":"code","26faaf3b":"code","8918069f":"code","d867d6bb":"markdown"},"source":{"d10b70e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","68818e34":"import pandas as pd\nimport numpy as np\nimport nltk","303106d6":"dataset = pd.read_json('..\/input\/Sarcasm_Headlines_Dataset.json',lines=True)","9496c3ad":"data = dataset.iloc[:,1:]","c1f1518c":"X = []\nY = []\nfor i in range(len(dataset)):\n  X.append(dataset.iloc[i,0])\n  Y.append([dataset.iloc[i,1]])","a46fa9e8":"from nltk.corpus import stopwords\nimport string\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer","b09054b3":"stop = stopwords.words('english')\nlemmatizer = WordNetLemmatizer()","29d7d282":"tokenizer_nltk = RegexpTokenizer('[0-9]*\\.[0-9]*|\\w+')","50c12f2e":"for i in range(len(data)): \n    tokens = tokenizer_nltk.tokenize(data.iloc[i,0])\n    tokens = [lemmatizer.lemmatize(x) for x in tokens]\n    tok = [x for x in tokens if ((x not in stop) and (x not in string.punctuation))]\n    data.iloc[i,0] = ' '.join(tok)","1e8c78a4":"data","85115b9c":"X = []\nY = []\nfor i in range(len(dataset)):\n  X.append(data.iloc[i,0])\n  Y.append([data.iloc[i,1]])\ny = np.array(Y)","18725c6c":"from sklearn.model_selection import train_test_split","e78c79d4":"Xs_train, Xs_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=42,shuffle=True)","ec654da1":"import keras","23cd891d":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences","02825294":"tokenizer = Tokenizer(num_words=20000)\ntokenizer.fit_on_texts(Xs_train)\n\nX_train = tokenizer.texts_to_sequences(Xs_train)\nX_test =  tokenizer.texts_to_sequences(Xs_test)\n\nvocab_size = len(tokenizer.word_index) + 1","8b3547cf":"print(vocab_size)","dd4af5b2":"print(Xs_train[0])\nprint(X_train[0])","fe98196c":"maxlen = 32\nX_train = pad_sequences(X_train, padding='post', maxlen=maxlen)\nX_test = pad_sequences(X_test, padding='post', maxlen=maxlen)","26faaf3b":"from keras.models import Sequential\nfrom keras.layers import Embedding\nfrom keras.layers import Dense, Dropout, Flatten ,LSTM\nfrom keras.layers import Conv1D,GlobalMaxPool1D,MaxPooling1D","8918069f":"from keras.models import Sequential\nfrom keras import layers\n\nembedding_dim = 128\n\nmodel = Sequential()\nmodel.add(layers.Embedding(vocab_size, embedding_dim, input_length=maxlen))\nmodel.add(LSTM(embedding_dim, return_sequences=True))\nmodel.add(layers.Conv1D(128, 3, activation='relu'))\n#model.add(layers.MaxPooling1D(3,strides=2))\nmodel.add(layers.GlobalMaxPool1D())\nmodel.add(layers.Dense(10, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\nmodel.compile(optimizer=keras.optimizers.Nadam(),\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","d867d6bb":"We Are now going to remove all the stopwords and also Lemmatize the tokens using nltk WordNetLemmatizer.\nThe tokens are generated from sentences using RegexpTokenizer from nltk"}}