{"cell_type":{"f9dc49fb":"code","4a21a164":"code","c993ca29":"code","dfb93e0e":"code","aa13b499":"code","40840c60":"code","51088536":"code","650604cc":"code","6d7906e0":"code","8dddbda0":"code","4d8fad89":"code","ffc6ddc2":"code","b2dbd8c0":"code","126dc5b8":"code","da5ddc17":"code","4a62398c":"code","d3388dc8":"code","6e3bd43d":"code","e5b32d06":"code","a2b80116":"code","2ce487e3":"code","8ef0a1b5":"code","0dfc3f99":"code","7d8b00d0":"code","dd0815bb":"code","651c9960":"code","c40d880e":"code","2b67686a":"markdown","5916b342":"markdown","ed1cb082":"markdown","99149ae9":"markdown","3413e222":"markdown","88831ec3":"markdown","572a9bb2":"markdown","59e1edf7":"markdown","287115d8":"markdown","c38a662d":"markdown","047b3abc":"markdown","23a9c1a9":"markdown","18ef9330":"markdown","1a0f9e0b":"markdown","0e2b76db":"markdown","6b40b18c":"markdown","dcfe0e33":"markdown","844a3b13":"markdown","4d59a2c3":"markdown","602cd7b1":"markdown","f614dca0":"markdown","d25c0868":"markdown","7726df54":"markdown","abeaba5a":"markdown","81bca483":"markdown","459212ac":"markdown","472a8e42":"markdown","dc55dd24":"markdown","11ffda38":"markdown","b71e387e":"markdown","7c266094":"markdown","f0954236":"markdown","adc0b55c":"markdown","b35c27e0":"markdown","73faa9ca":"markdown","847f580e":"markdown","bcf893b9":"markdown","eb090824":"markdown","7e456a81":"markdown","047eac3e":"markdown","53ebe0b5":"markdown","cc8190df":"markdown","8c08b373":"markdown","f05d42cd":"markdown","a26387eb":"markdown","e44e4433":"markdown","5ffaf7a5":"markdown"},"source":{"f9dc49fb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf # Deep learning\nimport matplotlib.pyplot as plt # Plots\nimport math # Basic math\n%matplotlib inline\nimport seaborn as sns # Plots\n\ndata = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndata.describe()","4a21a164":"sns.countplot(x=\"Survived\", data=data)","c993ca29":"sns.countplot(x=\"Pclass\", hue=\"Survived\", data=data)","dfb93e0e":"data.Name","aa13b499":"data.Name.values[240:250]","40840c60":"sns.countplot(x=\"Sex\", hue=\"Survived\", data=data)","51088536":"sns.kdeplot(data=data.Age, shade=True)","650604cc":"sns.kdeplot(data=data.loc[data['Survived'] == 1].Age, label=\"Survived\", shade=True)\nsns.kdeplot(data=data.loc[data['Survived'] == 0].Age, label=\"Died\", shade=True)","6d7906e0":"sns.regplot(x=data['Age'], y=data['Survived'])","8dddbda0":"sns.lmplot(x=\"Age\", y=\"Survived\", hue=\"Sex\", data=data)","4d8fad89":"data.SibSp.describe()","ffc6ddc2":"data.Parch.describe()","b2dbd8c0":"sns.lmplot(x=\"Fare\", y=\"Survived\", data=data)","126dc5b8":"data.Cabin.describe()","da5ddc17":"data.Cabin.unique()","4a62398c":"columnFilter = [\"Survived\", \"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\"]\nfilteredData = data[columnFilter]\nfilteredTestData = test_data[columnFilter[1:]]\nfilteredData.head()","d3388dc8":"filteredData.head()","6e3bd43d":"def oneHotEncode(dataToEncode, column):\n    onehot = pd.get_dummies(dataToEncode[column])\n    dataToEncode = dataToEncode.join(onehot)\n    dataToEncode = dataToEncode.drop(columns=column)\n    return dataToEncode\n\nfilteredData = oneHotEncode(filteredData, \"Pclass\")\nfilteredData.head()","e5b32d06":"def sex_to_numerical(d):\n    sex = d[\"Sex\"]\n    if sex == \"male\":\n        return 1\n    return 0\n\nfilteredData['Sex'] = filteredData.apply(sex_to_numerical, axis=1)\nfilteredData.head()","a2b80116":"from sklearn.preprocessing import MinMaxScaler\n\ndef normalize_age(dataToNormalize):\n    scaler = MinMaxScaler()\n    dataToNormalize[\"Age\"] = scaler.fit_transform(dataToNormalize[\"Age\"].values.reshape(-1,1))\n    return dataToNormalize\n\nfilteredData.Age.fillna((filteredData['Age'].mean()), inplace=True)\n\nfilteredData = normalize_age(filteredData)\nfilteredData.head()","2ce487e3":"from sklearn.model_selection import train_test_split\nX = filteredData[filteredData.columns[1:]]\ny = filteredData[filteredData.columns[0]]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nX_train = X_train.to_numpy()\nX_test = X_test.to_numpy()\ny_train = y_train.to_numpy()\ny_test = y_test.to_numpy()","8ef0a1b5":"model = tf.keras.Sequential([\n    tf.keras.layers.Dense(64, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(32, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(16, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\nmodel.compile(optimizer='adam', loss=tf.keras.losses.BinaryCrossentropy(from_logits=True), metrics=['accuracy'])\nhistory = model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test))","0dfc3f99":"test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\nprint('\\nTest accuracy:', test_acc)\n\nplt.figure(figsize=[8,6])\nplt.plot(history.history['accuracy'],'r',linewidth=3.0)\nplt.plot(history.history['val_accuracy'],'b',linewidth=3.0)\nplt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\nplt.xlabel('Epochs ',fontsize=16)\nplt.ylabel('Accuracy',fontsize=16)\nplt.title('Accuracy Curves',fontsize=16)\nplt.show()","7d8b00d0":"filteredTestData = oneHotEncode(filteredTestData, \"Pclass\")\nfilteredTestData['Sex'] = filteredTestData.apply(sex_to_numerical, axis=1)\nfilteredTestData.Age.fillna((filteredTestData['Age'].mean()), inplace=True)\nfilteredTestData = normalize_age(filteredTestData)","dd0815bb":"model.fit(X, y, epochs=100)","651c9960":"predictions = model.predict(filteredTestData)\npredictions = np.where(predictions > 0.5, 1, 0)\npredictions","c40d880e":"predictions = pd.DataFrame(predictions).rename(columns={0: \"Survived\"})\npredictions.index.name =\"PassengerId\"\npredictions.index += 892\npredictions.to_csv(\"predictions.csv\")\npredictions","2b67686a":"### Correlation between age, sex and survival rate\nVery interesting, we can make two very important observations of this graph:\n* As women get older, we have an increase in survival rate whereas\n* As men get older, there is a decrease in survival rate","5916b342":"Let's at first [one hot encode](https:\/\/www.geeksforgeeks.org\/ml-one-hot-encoding-of-datasets-in-python\/) the Pclass attribute.","ed1cb082":"After looking into the data, we can see that some people were doctors. That is a really important piece of information since they could be more educated about survival methods. This is for sure something we can use in the __feature engineering__ stage so we'll keep this in mind.","99149ae9":"## Sex\nLet's compare the ratio of men and women. It would be interesting to compare the number of men and women who survived versus the ones who lost their life (unfortunately for some, we won't dig too deep into this kind of analysis because we will leave most of the work to the Deep learning model)","3413e222":"# Data preparation","88831ec3":"We can see that most people had little to no siblings or spouses aboard the Titanic. The same can be said for parents and children.","572a9bb2":"Let's finally run the prediction on the test dataset.","59e1edf7":"## Feature engineering\nUnfortunately, I can hardly see feature engineering making an impact on our predictions. For that reason, I'm not going to get into it this time. Here are some of the reasons:\n* It would be cool to extract the deck on which people were located, but we are provided with that sort of data for only a small fraction of the people\n* I don't see extracting the Dr. title from the names being beneficial to our model for the same reason (very small amount of people have a specific title which would intuitively make an impact on the chances of survival)","287115d8":"Unfortunately, ages of 177 people are not given which is a shame because intuitively this can be a really good indicator of survival ([women and children first](https:\/\/en.wikipedia.org\/wiki\/Women_and_children_first) approach). Let's compare survival rates for each age group.","c38a662d":"## Selecting the important data\nDuring the data understanding stage, we realised that some columns are in practice useless and now we are going to remove them from the dataset.","047b3abc":"At last, we are going to fill the missing age attributes with the mean values and normalize the data because [normalized data performs better on the deep learning models](https:\/\/mc.ai\/why-data-normalization-is-necessary-for-machine-learning-models\/).","23a9c1a9":"We can see that there are quite more people who lost their life comparet to the survivals. To be exact, the difference is 207 people.","18ef9330":"## Cabin number\nThis can be interesting. Let's see what's provided to us.","1a0f9e0b":"We can see that by using dropout layers we prevented it from overfitting, exactly what we've wanted.","0e2b76db":"Unfortunately we have the data for only about 23% of the passengers.","6b40b18c":"## Age and Sex\nOut of curiosity, let's see what happens when we try to combine age and sex values.","dcfe0e33":"## Survived\nThere is not much we can do with this data, so let's visualise it and see the ratio of people who survived with others.","844a3b13":"# Evaluation\nLet's check how our model did on the validation part of the dataset.","4d59a2c3":"This could later prove to be an important piece of data since the higher ticket class could have meant higher priority when it came to boarding the lifeboats.","602cd7b1":"__In the dataset we are introduced to 12 different properties, let's first explain each one of them so that we can better understand what's going on.__\n\n* __PassengerId__: _A unique id for every passenger_\n* __Survived__: _Whether the passenger survived (1 or 0)_\n* __Pclass__: _The ticket class (1, 2 or 3 -> 1st, 2nd or 3rd)_\n* __Name__\n* __Sex__\n* __Age__\n* __SibSp__: _Number of siblings \/ spouses aboard the Titanic_\n* __Parch__: _Number of parents \/ children aboard the Titanic_\n* __Ticket__: _Ticket number_\n* __Fare__: _Passenger fare_\n* __Cabin__: _Cabin number_\n* __Embarked__: _Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)_\n\nLet's first explore each property individually.","f614dca0":"Now let's train the model on the whole dataset so that we can get the most out of what we've got.","d25c0868":"### Challenge\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew. While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\nOn the first glance, we can anticipate that more women and children survived compared to men because of the [women and children first](https:\/\/en.wikipedia.org\/wiki\/Women_and_children_first) approach to loading the lifeboats. Our goal throughout this journey is to explore the available data, detect possible anomalies, feature engineer additional helpful information and at the end create a Deep learning predictive model to predict who is most likely to survive based on the provided test dataset.","7726df54":"# Data understanding\nWe will start by importing the required libraries and loading the provided csv dataset.","abeaba5a":"Finally we have to do turn our predictions into the specified output CSV format.","81bca483":"## Siblings and spouses & Parents and children\nLet's see what do we have here.","459212ac":"### RMS Titanic\nRMS Titanic was a British passenger liner operated by the White Star Line that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after striking an iceberg during her maiden voyage from Southampton to New York City. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making the sinking one of modern history's deadliest peacetime commercial marine disasters. RMS Titanic was the largest ship afloat at the time she entered service and was the second of three Olympic-class ocean liners operated by the White Star Line. She was built by the Harland and Wolff shipyard in Belfast. Thomas Andrews, chief naval architect of the shipyard at the time, died in the disaster.\n\nTitanic was under the command of Capt. Edward Smith, who also went down with the ship. The ocean liner carried some of the wealthiest people in the world, as well as hundreds of emigrants from Great Britain and Ireland, Scandinavia and elsewhere throughout Europe, who were seeking a new life in the United States. The first-class accommodation was designed to be the pinnacle of comfort and luxury, with a gymnasium, swimming pool, libraries, high-class restaurants and opulent cabins. A high-powered radiotelegraph transmitter was available for sending passenger \"marconigrams\" and for the ship's operational use. Although Titanic had advanced safety features, such as watertight compartments and remotely activated watertight doors, it only carried enough lifeboats for 1,178 people\u2014about half the number on board, and one third of her total capacity\u2014due to the maritime safety regulations of those days. The ship carried 16 lifeboat davits which could lower three lifeboats each, for a total of 48 boats. However, Titanic carried only a total of 20 lifeboats, four of which were collapsible and proved hard to launch during the sinking. (source: [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/RMS_Titanic))","472a8e42":"# Deployment\nLet's predict the testing dataset.","dc55dd24":"### First discovery\nWe can immediately see that there is a greater chance for women to survive compared to men, most of whom died in the disaster.","11ffda38":"Since my goal for this task is to create a __deep learning__ model, i wanted to create a model which has more than only 1 layer. After some tweaking, I settled for 6, combining the dense layers with dropout layers and the sigmoid activation function at the end. I am using dropout layers to [prevent the model from overfitting](https:\/\/www.tensorflow.org\/tutorials\/keras\/overfit_and_underfit). Finally I decided to go for [Adam optimizer (adaptive gradient descent)](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/optimizers\/Adam) with the [BinaryCrossentropy loss](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/losses\/BinaryCrossentropy) function (complementing the final sigmoid activation function).","b71e387e":"## Port\nThis is not a good indicator and would probably cause data leakage if we were to include it in the model training.","7c266094":"_This is one my first full Data Science analysis. Please feel free to share your feedback. Enjoy the journey!_\n# \ud83d\udc4b\ud83d\udef3\ufe0f Ahoy! The Titanic disaster Story\n_In addition to telling an interesting story, my goal is to follow the [CRISP-DM](https:\/\/www.sv-europe.com\/crisp-dm-methodology\/) methodology as much as possible._","f0954236":"## Inference","adc0b55c":"Based on the distribution we can clearly see that kids up to the age of approximately 15 had a greater chance of survival. Let's check the correlation between age and survival.","b35c27e0":"## Name\nAt first, this seems as quite unimportant, similar to PassengerId, but let's just see some names of people aboard the ship.","73faa9ca":"## Training and validation data\n","847f580e":"Based on the scatter plot with the regression line we can with high certainty assume that age was an important factor for surviving the wreck.","bcf893b9":"## PassengerId\nSince this is only showing us the unique ID of each passenger, it's pretty much useless and doesn't in any way influence the survival (IDs could be shuffled).","eb090824":"Upon closer inspection, we can see the Rev. title. Let's [investigate](https:\/\/en.wikipedia.org\/wiki\/The_Reverend). Interesting... it is used as a prefix to the names of Christian clergy and ministers. Let's dig deeper.","7e456a81":"## Data cleaning\nIn this stage we are going to do some data cleanup. At the end, we are going to have data ready for our deep learning model.","047eac3e":"Next we are going to convert the sex attribute to number since our model requires values to be integers.","53ebe0b5":"## Passenger fare\nOn the first glance, this is probably connected to the ticket class and should yield similar observations.","cc8190df":"# Modelling\n\n## Let's create a deep learning model","8c08b373":"## Ticket class","f05d42cd":"# Final words\nThe deep learning model is probably not the best fit for this kind of classification task due to the small amount of data we are left with to model. Nevertheless, this is a great exercise for deep learning and I hope that you had fun reading the analysis and learned something new! :D","a26387eb":"After a little bit of [research](https:\/\/www.encyclopedia-titanica.org\/titanic-deckplans\/) we can see that the letters correspond to different decks. This will probably correlate to the survival. We'll keep an eye out on this during the data preparation stage.","e44e4433":"## Age\nLet's further look into the demographics and see the age distribution.","5ffaf7a5":"## Preparation\nFirst we need to prepare the test data."}}