{"cell_type":{"865b0626":"code","0b92d836":"code","b10abda6":"code","bd624ba3":"code","d37dd02f":"code","9207e077":"code","4cc3cc76":"code","18ed79c0":"code","6005d834":"code","ccffb708":"markdown","384c086e":"markdown","8e5cfd0e":"markdown","e000e505":"markdown","4b245fed":"markdown","970f358a":"markdown","e26a6d8e":"markdown","5c64d1a6":"markdown"},"source":{"865b0626":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0b92d836":"from sklearn.model_selection import train_test_split\n\nfrom tensorflow.python import keras\nfrom tensorflow.python.keras.models import Sequential\nfrom tensorflow.python.keras.layers import Dense, Flatten, Conv2D, BatchNormalization, MaxPool2D, Dropout\nfrom keras.preprocessing.image import ImageDataGenerator","b10abda6":"img_rows, img_cols = 28, 28\nnum_classes = 10\n\ndef data_prep(raw):\n    out_y = keras.utils.to_categorical(raw.label, num_classes)\n\n    num_images = raw.shape[0]\n    x_as_array = raw.values[:,1:]\n    x_shaped_array = x_as_array.reshape(num_images, img_rows, img_cols, 1)\n    out_x = x_shaped_array \/ 255\n    return out_x, out_y\n\ntrain_size = 30000\ntrain_file = '..\/input\/train.csv'\ntest_file = '..\/input\/test.csv'\nraw_data = pd.read_csv(train_file)\n\nx, y = data_prep(raw_data)\n\nrandom_seed = 5\nx, x_val, y, y_val = train_test_split(x, y, test_size = 0.2, random_state=random_seed)","bd624ba3":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\n\ndatagen.fit(x)","d37dd02f":"model = Sequential()\n\nmodel.add(Conv2D(32, kernel_size=5, activation='relu', input_shape=(img_rows,img_cols,1)))\n\nmodel.add(Conv2D(32, kernel_size=5, activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(64, kernel_size=3, activation='relu'))\nmodel.add(Conv2D(64, kernel_size=3, activation='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])","9207e077":"#model.fit(x,y, batch_size=64, epochs=20, validation_split=0.2) #if we didn't use any data augmentation\n\nbatch_size = 64\nmodel.fit_generator(datagen.flow(x,y, batch_size=batch_size),\n                    validation_data= (x_val, y_val),\n                    epochs = 20,\n                    steps_per_epoch=x.shape[0] \/\/ batch_size)","4cc3cc76":"test_data = pd.read_csv(test_file)\nnum_images = test_data.shape[0]\ntest_data_asarray = test_data.values[:,:]\ntest_data_shaped_array = test_data_asarray.reshape(num_images, img_rows, img_cols, 1)\nt = test_data_shaped_array \/ 255","18ed79c0":"predictions = model.predict(t)","6005d834":"predictions_readable = [prediction.argmax() for prediction in predictions]\nsubmission = pd.DataFrame({\"ImageId\":list(range(1,len(predictions)+1)),\n              \"Label\":predictions_readable})\nsubmission.to_csv('submission.csv',index=False,header=True)","ccffb708":"**Format predictions and create submission file**\n\nIn order to find the most likely classification for a given image, all we need to do is take the index of the maximum probability for each prediction. For an numpy array this is as easy as calling the argmax function on the nparray. Let's do this for all the predictions in the predictions 2-d array. Finally, we need to convert the predictions in the format requested by the challenge statement and convert the DataFrame to a .csv file ready for submission.","384c086e":"**Fit the model**\n\nAfter having compiled the model, where you can set the type of loss and optimizer that you wish (and also the metrics you want it to show at each step of training), the last step of training is fitting the model to the training data.\n\nIn this method you should specify the size of your batch, that is the number of samples that will be propagated through the network at each training iteration (the loss is accumulated and backprop is only run after every batch); the number of epochs, which is the number of times you want your model to \"go over\" your training data; and finally, the validation split, that is the portion of your training dataset you wish to use for validation.","8e5cfd0e":"**Building the model**\n\nNext is the creation of the model with three convulutional layers, two with kernel size 2 and the last 3. These are purely arbitrary, and so is the number of filters used at each convolutional layer (the first argument in the Conv2D builder) and the activation function used for the layer.\n\nA Flatten layer follows, and finally two dense, or fully-connected, layers to implement the softmax classifier. These latter are a typical \"ending\" for ConvNets as they allow the final classifying nodes to consider all of the features learned at the last convolutional layer before returning a \"confidence\" score for each class.","e000e505":"**Data Augmentation**\n\nSnippet taken from [kernel](https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6). Using a data generator means that later on we'll have to use the fit_generator method on our model.","4b245fed":"**Imports**\n\nImport needed libraries and tools","970f358a":"**Make predictions**\n\nThe *predictions* variable will store an array of array of likelihoods for each class (number). The probability of an image being the *i-th* number is going to be stored at the *i-th* position in the array. We can't use directly this representation for the problem's solution, we only want to know the one number that has the greatest likelihood of being the number represented in the image.","e26a6d8e":"**Data Preparation**\n\nSome boilerplate code taken from Kaggle's tutorials on the Deep Learning track. What this does is setting up some variables that are going to be used later (img_rows\/cols, num_classes) and defining a function to prepare the data from a raw csv file (read with pandas). More precisely, this function separates the labels into the y variable and the images into the tensor x, which is normalized by dividing for the maximum possible grey-scale value before being returned.","5c64d1a6":"**Test Data Preparation**\n\nThis could have been done at the start together with the training data processing but it looked more clear when separate. We are doing the very same thing we have done with the traning data, except here we don't have any labels for our images to store in a variable y.  "}}