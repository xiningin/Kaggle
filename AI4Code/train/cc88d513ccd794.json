{"cell_type":{"528727f7":"code","7dc0efbc":"code","53889f4a":"code","b7aa19c5":"code","f374940d":"code","54e6ee97":"code","dfe24702":"code","99f1ba7d":"code","719c9e3b":"code","40f6a125":"code","a34964d5":"code","0c78c40b":"code","f69b4a2b":"code","27507afa":"code","dbe27a86":"markdown","5856dc6f":"markdown","0fa09366":"markdown","d02ccb6f":"markdown","4475c643":"markdown","ae28ba54":"markdown","67114c06":"markdown","c04b264e":"markdown","d1adcf4f":"markdown","816f2733":"markdown","18a7172a":"markdown"},"source":{"528727f7":"# Packages imports\nimport numpy as np\nimport pandas as pd\nimport scipy.stats as stats\nimport statsmodels.stats.api as sms\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom math import ceil\n\n%matplotlib inline\n\n# Some plot styling preferences\nplt.style.use('seaborn-whitegrid')\nfont = {'family' : 'Helvetica',\n        'weight' : 'bold',\n        'size'   : 14}\n\nmpl.rc('font', **font)\neffect_size = sms.proportion_effectsize(0.13, 0.15)    # Calculating effect size based on our expected rates\n\nrequired_n = sms.NormalIndPower().solve_power(\n    effect_size, \n    power=0.8, \n    alpha=0.05, \n    ratio=1\n    )                                                  # Calculating sample size needed\n\nrequired_n = ceil(required_n)                          # Rounding up to next whole number                          \n\nprint(required_n)","7dc0efbc":"# importing the datas sample\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","53889f4a":"# sample of what the collated data looks like\ndf = pd.read_csv('\/kaggle\/input\/ab-testing\/ab_data.csv')\n\ndf.head()","b7aa19c5":"df.info","f374940d":"# the dataset contains users that have experienced both the new and old pages.\npd.crosstab(df['group'], df['landing_page'])","54e6ee97":"session_counts = df['user_id'].value_counts(ascending=False)\nmulti_users = session_counts[session_counts > 1].count()\n\nprint(f'There are {multi_users} users that appear multiple times in the dataset')","dfe24702":"# we drop the second time the instance used the homepage\nusers_to_drop = session_counts[session_counts > 1].index\n\ndf = df[~df['user_id'].isin(users_to_drop)]\nprint(f'The updated dataset now has {df.shape[0]} entries')","99f1ba7d":"# the dataset is now cleaner as can see the user experience only one page\npd.crosstab(df['group'], df['landing_page'])","719c9e3b":"# here we collated the required n from each group as defined earlier\n\ncontrol_sample = df[df['group'] == 'control'].sample(n=required_n, random_state=22) \ntreatment_sample = df[df['group'] == 'treatment'].sample(n=required_n, random_state=22)\n\nab_test = pd.concat([control_sample, treatment_sample], axis=0)\nab_test.reset_index(drop=True, inplace=True)\nab_test","40f6a125":"ab_test.info()","a34964d5":"ab_test['group'].value_counts()","0c78c40b":"conversion_rates = ab_test.groupby('group')['converted']\n\nstd_p = lambda x: np.std(x, ddof=0)              # Std. deviation of the proportion\nse_p = lambda x: stats.sem(x, ddof=0)            # Std. error of the proportion (std \/ sqrt(n))\n\nconversion_rates = conversion_rates.agg([np.mean, std_p, se_p])\nconversion_rates.columns = ['conversion_rate', 'std_deviation', 'std_error']\n\n\nconversion_rates.style.format('{:.3f}')","f69b4a2b":"plt.figure(figsize=(8,6))\n\nsns.barplot(x=ab_test['group'], y=ab_test['converted'], ci=False)\n\nplt.ylim(0, 0.17)\nplt.title('Conversion rate by group')\nplt.xlabel('Group')\nplt.ylabel('Converted (proportion)');","27507afa":"from statsmodels.stats.proportion import proportions_ztest, proportion_confint\ncontrol_results = ab_test[ab_test['group'] == 'control']['converted']\ntreatment_results = ab_test[ab_test['group'] == 'treatment']['converted']\nn_con = control_results.count()\nn_treat = treatment_results.count()\nsuccesses = [control_results.sum(), treatment_results.sum()]\nnobs = [n_con, n_treat]\n\nz_stat, pval = proportions_ztest(successes, nobs=nobs)\n(lower_con, lower_treat), (upper_con, upper_treat) = proportion_confint(successes, nobs=nobs, alpha=0.05)\n\nprint(f'z statistic: {z_stat:.2f}')\nprint(f'p-value: {pval:.3f}')\nprint(f'ci 95% for control group: [{lower_con:.3f}, {upper_con:.3f}]')\nprint(f'ci 95% for treatment group: [{lower_treat:.3f}, {upper_treat:.3f}]')","dbe27a86":"## A\/B Testing website conversion\n\nA\/B testing on a website to test whether the experimental yields a better conversion rate for a website than the current theme\n\n\nThe motivation is to understand and control the variables within the A\/B test.\n\nNote: this is a very simple experiment which does not factor in the funnel and journey to conversion or track bounce rate etc.","5856dc6f":"### Sampling\n\nWe only sample the required amount the show from the power test.\n\nWe can use pandas' DataFrame.sample() method to do this, which will perform Simple Random Sampling for us.","0fa09366":"Guide from Renato of sample to \nhttps:\/\/medium.com\/@RenatoFillinich\/ab-testing-with-python-e5964dd66143\n","d02ccb6f":"## 4. Testing the hypothesis","4475c643":"## 1. Designing the experiement\n\nFormulating an experiment\n\nGiven we don\u2019t know if the new design will perform better or worse (or the same?) as our current design, we\u2019ll choose a two-tailed test:\n\nH\u2092: p = p\u2092\n\nH\u2090: p \u2260 p\u2092\n\nwhere p and p\u2092 stand for the conversion rate of the new and old design, respectively. We\u2019ll also set a confidence level of 95%:\n\n\n### Choosing the variables\n\nFor our test we\u2019ll need two groups:\n\nA control group - They'll be shown the old design\n\nA treatment (or experimental) group - They'll be shown the new design\n\n\nThis will be our Independent Variable. \n\nThe reason we have two groups even though we know the baseline conversion rate is that we want to control for other variables that could have an effect on our results: by having a control group we can directly compare their results to the treatment group, because the only systematic difference between the groups is the design of the landing page, and we can therefore attribute any differences in results to the designs.\n\nFor our Dependent Variable (i.e. what we are trying to measure), we are interested in capturing the conversion rate. A way we can code this is by each user session with a binary variable:\n\n0 - The user did not buy the product during this user session\n\n1 - The user bought the product during this user session","ae28ba54":"Judging by the stats above, it does look like our two designs performed very similarly, with our new design performing slightly better average conversion rate.","67114c06":"The number of people (or user sessions) we decide to capture in each group will have an effect on the precision of our estimated conversion rates: the larger the sample size, the more precise our estimates (i.e. the smaller our confidence intervals), the higher the chance to detect a difference in the two groups, if present.\n\n### Choosing a sample size\nIt is important to note that since we won\u2019t test the whole user base (our population), the conversion rates that we\u2019ll get will inevitably be only estimates of the true rates.\n\nThe number of people (or user sessions) we decide to capture in each group will have an effect on the precision of our estimated conversion rates: the larger the sample size, the more precise our estimates (i.e. the smaller our confidence intervals), the higher the chance to detect a difference in the two groups, if present.\n\nOn the other hand, the larger our sample gets, the more expensive (and impractical) our study becomes.\nSo how many people should we have in each group?","c04b264e":"## 3. Visualising the results\n\nThe first thing we can do is to calculate some basic statistics to get an idea of what our samples look like.","d1adcf4f":"## 2. Collecting and preparing the data","816f2733":"## 5. Drawing Conclusion\n\nSince our p-value is above our \u03b1=0.05 threshold, we cannot reject the Null hypothesis H\u2092, which means that our new design did not perform significantly different (let alone better) than our old one.\n\nAdditionally, if we look at the confidence interval for the treatment group we notice that:\n\nIt includes our baseline value of 13% conversion rate. It does not include our target value of 15% (the 2% uplift we were aiming for)\n\nWhat this means is that it is more likely that the true conversion rate of the new design is similar to our baseline, rather than the 15% target we had hoped for. This is further proof that our new design is not likely to be an improvement on our old design, and that unfortunately we go back to redesign.","18a7172a":"We\u2019d need at least the number of observations for each group and have a 90% chance to test it is significantly significant"}}