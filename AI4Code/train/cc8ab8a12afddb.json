{"cell_type":{"b515b8b8":"code","538a22d1":"code","92580ad9":"code","aa224e5f":"code","51ae288e":"code","ed61277b":"code","076fb34a":"code","ae60c114":"code","bcc1a2af":"code","684f3d2c":"code","ad7f0dd0":"code","1dd7c734":"code","37cda178":"code","2479b2f7":"code","662a0d7f":"code","1ec6f50e":"code","8e2729a7":"code","936313e5":"code","a298976e":"code","07683e78":"code","3ef20760":"code","42caf9aa":"code","21092100":"code","fa6b5a30":"code","7a838e12":"code","0d1d4bde":"code","a3313cdd":"markdown","afa504f4":"markdown","8c6218bc":"markdown","79642ecf":"markdown","de221474":"markdown","7f35bcec":"markdown","326d19d8":"markdown","bd61cbaf":"markdown","92722384":"markdown","d77bf8bc":"markdown","c7978ebf":"markdown","bf60eab8":"markdown","528fd169":"markdown","8d04836a":"markdown","5299f166":"markdown","c73b2f33":"markdown"},"source":{"b515b8b8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualisation and plotting\nimport matplotlib.pyplot as plt # data plotting\nimport warnings\nimport os","538a22d1":"# Seaborn default configuration\nsns.set_style(\"darkgrid\")\n\n# set the custom size for my graphs\nsns.set(rc={'figure.figsize':(8.7,6.27)})\n\n# filter all warnings\nwarnings.filterwarnings('ignore') \n\n# set max column to 999 for displaying in pandas\npd.options.display.max_columns=999 ","92580ad9":"#importing the data\ndata = pd.read_csv('..\/input\/Iris.csv', index_col=0)\n\n#renaming the columns\ndata = data.rename(columns={\"SepalLengthCm\": \"sepal_length\", \"SepalWidthCm\": \"sepal_width\", \"PetalLengthCm\": \"petal_length\", \"PetalWidthCm\": \"petal_width\", \"Species\": \"species\"})","aa224e5f":"data.head()","51ae288e":"data.info()","ed61277b":"data.describe()","076fb34a":"rows, col = data.shape\nprint(\"Rows : %s, column : %s\" % (rows, col))","ae60c114":"data['species'].value_counts()","bcc1a2af":"#make a copy of the species column\ndata['species_detailed'] = data['species']\n\n#replacing the two not setosa species with \"not Setosa\"\ndic_setosa = {'Iris-versicolor': 'not Setosa', 'Iris-virginica': 'not Setosa', 'Iris-setosa': 'Setosa'}\ndata = data.replace({\"species\": dic_setosa})\ndata['species'].value_counts()","684f3d2c":"sns.FacetGrid(data, hue=\"species\", palette=\"husl\", height=8) \\\n.map(plt.scatter, \"sepal_length\", \"sepal_width\") \\\n.add_legend() \nplt.show()\n\n\nsns.FacetGrid(data, hue=\"species\", palette=\"husl\", height=8) \\\n.map(plt.scatter, \"petal_length\", \"petal_width\") \\\n.add_legend() \nplt.show()","ad7f0dd0":"g = sns.pairplot(data, hue='species', markers='x')\ng = g.map_upper(plt.scatter)\ng = g.map_lower(sns.kdeplot)","1dd7c734":"plt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nsns.violinplot(x='sepal_length', y='species', data=data, inner='stick', palette='autumn')\nplt.subplot(2,2,2)\nsns.violinplot(x='sepal_width', y='species', data=data, inner='stick', palette='autumn')\nplt.subplot(2,2,3)\nsns.violinplot(x='petal_length', y='species', data=data, inner='stick', palette='autumn')\nplt.subplot(2,2,4)\nsns.violinplot(x='petal_width', y='species', data=data, inner='stick', palette='autumn')","37cda178":"from math import pi\n\ndata_setosa = data.loc[data['species_detailed'] == 'Iris-setosa']\ndata_virginica = data.loc[data['species_detailed'] == 'Iris-virginica']\ndata_versicolor = data.loc[data['species_detailed'] == 'Iris-versicolor']\n\nu=0     #x-position of the center\nv=0   #y-position of the center\nt = np.linspace(0, 2*pi, 100)\n\na=data_setosa['sepal_width'].mean()    #radius on the x-axis\nb=data_setosa['sepal_length'].mean()     #radius on the y-axis\nplt.legend()\nleaf_setosa = plt.plot( u+a*np.cos(t) , v+b*np.sin(t), label='Setosa'  ) \n\n\na=data_versicolor['sepal_width'].mean()    #radius on the x-axis\nb=data_versicolor['sepal_length'].mean()     #radius on the y-axis\nleaf_versicolor = plt.plot( u+a*np.cos(t) , v+b*np.sin(t), label='Versicolor'  ) \n\na=data_virginica['sepal_width'].mean()    #radius on the x-axis\nb=data_virginica['sepal_length'].mean()     #radius on the y-axis\nleaf_virginica = plt.plot( u+a*np.cos(t) , v+b*np.sin(t), label='Virginica'  ) \n\nplt.xticks([]) \nplt.yticks([]) \nplt.xlim(-10,10)\nplt.ylim(-10,10)\nplt.title('Sepal size Comparison')\nplt.legend()\nplt.show()\n\na=data_setosa['petal_width'].mean()    #radius on the x-axis\nb=data_setosa['petal_length'].mean()     #radius on the y-axis\nleaf_setosa = plt.plot( u+a*np.cos(t) , v+b*np.sin(t), label='Setosa'  ) \n\na=data_versicolor['petal_width'].mean()    #radius on the x-axis\nb=data_versicolor['petal_length'].mean()     #radius on the y-axis\nleaf_versicolor = plt.plot( u+a*np.cos(t) , v+b*np.sin(t), label='Versicolor' ) \n\na=data_virginica['petal_width'].mean()    #radius on the x-axis\nb=data_virginica['petal_length'].mean()     #radius on the y-axis\nleaf_virginica = plt.plot( u+a*np.cos(t) , v+b*np.sin(t),label='Virginica' ) \n\nplt.title('Petal size comparison')\nplt.xticks([]) \nplt.yticks([]) \nplt.legend()\nplt.xlim(-10,10)\nplt.ylim(-10,10)\nplt.show()\n\n","2479b2f7":"data.columns","662a0d7f":"#mporting a tool to split the dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer","1ec6f50e":"features = data.drop(columns=['species', 'species_detailed'])\nlabels = data['species']\n\n#use the label binarizer from sklearn\n#before ['Setosa', 'Not Setosa', 'Not Setosa']\n#after [1,0,0]\nlb = LabelBinarizer()\nlabels = lb.fit_transform(labels)\n\n#first we split the features and labels in to a Train (80%) and a Test (20%) set\nX_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n\n#then we split the train set into a Train (75%) and a Validation (25%) set,\n#because we should us a fully unseen set to test the model at the end. This will then be the test set.\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42) # 0.25 x 0.8 = 0.2\n\n#in the end we have: Train (60%), Validation (20%), Test (20%)","8e2729a7":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics","936313e5":"clf_setosa = LogisticRegression(random_state=42)\nclf_setosa.fit(X_train, y_train)","a298976e":"y_test_pred = clf_setosa.predict(X_test)\n\naccuracy = metrics.accuracy_score(y_test,y_test_pred)\n\nprint(\"\"\"\nAccuracy: {} \n\"\"\".format(accuracy))","07683e78":"#importing a tool to split the dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MultiLabelBinarizer","3ef20760":"features = data.drop(columns=['species', 'species_detailed'])\nlabels = data['species_detailed']\n\n#because we now nee some more values per categoriy I just use a Train (80%) and Test (20%) set\n\n#first we split the features and labels in to a Train (80%) and a Test (20%) set\nX_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)","42caf9aa":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics","21092100":"clf_multiclass = LogisticRegression(random_state=42, multi_class='ovr', solver='liblinear')\nclf_multiclass = clf_multiclass.fit(X_train, y_train)","fa6b5a30":"y_test_pred = clf_multiclass.predict(X_test)","7a838e12":"accuracy = metrics.accuracy_score(y_test,y_test_pred)\nbalanced_acc = metrics.balanced_accuracy_score(y_test,y_test_pred)\nf1_weighted = metrics.f1_score(y_test, y_test_pred, average='weighted')\nprint(\"\"\"\nAccuracy: {} \nBalanced Accuracy: {} \nF1 Weighted: {}\n\"\"\".format(accuracy,balanced_acc, f1_weighted))","0d1d4bde":"clf_multiclass.predict(np.array([2,3,2,1]).reshape(-1,4))","a3313cdd":"### Some styling settings, just ignore it","afa504f4":"# Multi-Class Classification with Logistic Regression","8c6218bc":"What is the meaning of this metrics? \n\nAccuracy: \n\n\n$$\\text{Accuracy} = \\frac{\\text{Number of correct predictions}}{\\text{Total number of predictions}}$$\n\nIf you need more information: [Crash Course Machine Learning](https:\/\/developers.google.com\/machine-learning\/crash-course\/classification\/accuracy#:~:text=Accuracy%20is%20one%20metric%20for,predictions%20Total%20number%20of%20predictions)","79642ecf":"### Splitting the data and prepare them for the models","de221474":"Okey that was too simple! :) Let try to seperate the other two species\nfor that I'll train a second model which seperates the other more difficult species. \nNormally you ","7f35bcec":"### Load Data","326d19d8":"Let's simplify the example","bd61cbaf":"# Binary Classification with Logistic Regression","92722384":"Just for fun I've plotted the mean shape of the leafs. But i think it helps to see why it is that easy to differenciate between Setosa and Not Setosa. ","d77bf8bc":"## **Data Visualization**","c7978ebf":"In this example I use the preimplemented models from the [Sklearn](https:\/\/scikit-learn.org\/stable\/) Libary","bf60eab8":"In the following Plot we can see, the differences between Setosa and not Setosa are very big. So we should be able to make model that can decide with a high accuracy.","528fd169":"The violin plot above, gives us the information how the data are split over the dataset. So you know in which range the majority of the data is.","8d04836a":"## The shape of a leaf","5299f166":"## **Inspect and analyse the data**","c73b2f33":"### Importing needed libaries"}}