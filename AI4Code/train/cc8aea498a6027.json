{"cell_type":{"8bde89d7":"code","e58c6fed":"code","40b78bf7":"code","b25cb1e8":"code","835916ac":"code","5a25c71d":"code","f1464499":"code","87e04c3d":"code","4cc6748a":"code","4938dc88":"code","d32fec4a":"code","7d3f1d0a":"code","083aefd4":"code","b7226914":"code","1eb09fce":"code","a0cc28dc":"code","b5d4f2b6":"code","518ba96b":"code","eb24dc69":"code","59a7ad1d":"code","53b6e7f0":"code","63d33d47":"code","815deeb8":"markdown","3364aa43":"markdown","c264ec4b":"markdown","a0311cfe":"markdown","b99bcb7d":"markdown","4a30e8bd":"markdown","36913e31":"markdown","378fec8c":"markdown","de3c0847":"markdown","d5146f93":"markdown","d8249785":"markdown","f816ae6a":"markdown"},"source":{"8bde89d7":"import random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfrom os.path import join\nimport shutil\n\nfrom tqdm import tqdm   # Progress bar\n\nimport torch\nimport torchvision\nimport torch.nn.functional as T\nfrom torchvision import transforms, models\nfrom torch.utils.data import DataLoader, Dataset\n\n\nrandom.seed(6)\nnp.random.seed(6)\ntorch.manual_seed(6)\ntorch.cuda.manual_seed(6)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\n\nroot_dir = ''\ninput_dir = '..\/input\/aerial-cactus-identification'","e58c6fed":"train_val_labels = pd.read_csv(join(input_dir, 'train.csv'))\ntrain_val_labels.head()","40b78bf7":"plt.figure(figsize=(3,3))\nplt.title('Labels distribution')\nsns.countplot(train_val_labels['has_cactus']);","b25cb1e8":"labels = ['no_cactus', 'has_cactus']\n\ntrain_dir = join(root_dir, 'train')\nval_dir = join(root_dir, 'val')\ntest_dir = join(root_dir, 'test')\n\n# Make train and val folders\nfor label in labels:\n    os.makedirs(join(train_dir, label), exist_ok=True)\n    os.makedirs(join(val_dir, label), exist_ok=True)","835916ac":"# 15000 train photos and 2500 val photos\n\nsource_dir = join(input_dir, 'train', 'train')\n\nfor i, filename in enumerate(tqdm(os.listdir(source_dir))):\n\n#     if i % 10 != 0:   # Skip 90% photos\n#         continue\n\n    is_cactus = int(train_val_labels.loc[train_val_labels['id'] == filename]['has_cactus'])\n\n    if i % 7 == 0:   #if i % 7 == 0:   \n        shutil.copy(join(source_dir, filename), join(val_dir, labels[is_cactus], filename))\n    else:\n        shutil.copy(join(source_dir, filename), join(train_dir, labels[is_cactus], filename))","5a25c71d":"def show_sample_images(dataloader, batch_size, images_from_batch=0, denormalize=False, classes=None):\n    if denormalize:\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225])\n    else:\n        mean = np.array([0., 0., 0.])\n        std = np.array([1., 1., 1.])\n    \n    if images_from_batch == 0 or images_from_batch > batch_size:\n            images_from_batch = batch_size\n        \n    for images, labels in dataloader:\n        plt.figure(figsize=(20, (batch_size \/\/ 20 + 1) * 3))\n\n        cols = 12\n        rows = batch_size \/\/ cols + 1\n        for i in range(images_from_batch):\n            image = images[i].permute(1, 2, 0).numpy() * std + mean   # \u0420\u0430\u0437\u043c\u0435\u0440\u043d\u043e\u0441\u0442\u044c RGB \u0432 \u043a\u043e\u043d\u0435\u0446\n            plt.subplot(rows, cols, i+1)\n            plt.xticks([])\n            plt.yticks([])\n            plt.grid(False)\n            plt.imshow(image.clip(0, 1))\n            if classes is not None:\n                plt.xlabel(classes[labels[i].numpy()])\n        plt.show()\n        \n        break","f1464499":"batch_size = 500\n\ntrain_dir = join(root_dir, 'train')\nval_dir = join(root_dir, 'val')\n\nclasses = ['No', 'Cactus']\n\ntrain_transforms1 = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_transforms2 = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_transforms3 = transforms.Compose([\n    transforms.RandomVerticalFlip(p=1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntrain_transforms4 = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=1),\n    transforms.RandomVerticalFlip(p=1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nval_transforms = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n#train_dataset = torchvision.datasets.ImageFolder(train_dir, train_transforms)\ntds1 = torchvision.datasets.ImageFolder(train_dir, train_transforms1)\ntds2 = torchvision.datasets.ImageFolder(train_dir, train_transforms2)\ntds3 = torchvision.datasets.ImageFolder(train_dir, train_transforms3)\ntds4 = torchvision.datasets.ImageFolder(train_dir, train_transforms4)\n\ntrain_dataset = torch.utils.data.ConcatDataset([tds1, tds2, tds3, tds4])\n\nval_dataset = torchvision.datasets.ImageFolder(val_dir, val_transforms)\n\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, num_workers=0)\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=0)\n\n\nfor images, labels in train_dataloader:\n    print(images.size())\n    print(labels.size())\n    break","87e04c3d":"show_sample_images(train_dataloader, batch_size, 72, denormalize=True)","4cc6748a":"print(f'Batch size: {batch_size}')\nprint(f'Train batches: {len(train_dataloader)}, Train samples: {len(train_dataset)}')\nprint(f'Val batches:   {len(val_dataloader)}, Val samples:    {len(val_dataset)}')","4938dc88":"train_batch_loss_history = []\ntrain_batch_accuracy_history = []\n\ntrain_loss_history = []\ntrain_accuracy_history = []\n\nval_loss_history = []\nval_accuracy_history = []\n\ndef validate(model, loss, optimizer):\n        \n    dataloader = val_dataloader\n    model.eval()   # Set model to evaluate mode\n\n    sum_loss = 0.\n    sum_accuracy = 0.\n\n    for inputs, labels in dataloader:\n        inputs = inputs.cuda()\n        labels = labels.cuda()\n\n        optimizer.zero_grad()\n\n        with torch.set_grad_enabled(False):\n            preds = model(inputs)\n            loss_value = loss(preds, labels)\n            preds_class = preds.argmax(dim=1)\n\n        sum_loss += loss_value.item()\n        sum_accuracy += (preds_class == labels.data).float().mean().cpu().numpy().item()\n\n    val_loss = sum_loss \/ len(dataloader)\n    val_accuracy = sum_accuracy \/ len(dataloader)\n\n    val_loss_history.append(val_loss)\n    val_accuracy_history.append(val_accuracy)\n    \n    print(f'Validation accuracy {val_accuracy * 100:.2f} %, loss {val_loss:.4f}')\n\n    model.train()  # \u0412\u0435\u0440\u043d\u0443\u043b\u0438 \u043a\u0430\u043a \u0431\u044b\u043b\u043e\n\n\ndef train_model(model, loss, optimizer, scheduler, num_epochs):\n        \n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}\/{num_epochs-1}: ', end='')\n\n        dataloader = train_dataloader\n        model.train()  # Set model to training mode\n\n        sum_loss = 0.\n        sum_accuracy = 0.\n\n        # \u041f\u0440\u043e\u0433\u043e\u043d \u043f\u043e \u0431\u0430\u0442\u0447\u0430\u043c\n        for inputs, labels in dataloader:   #tqdm(dataloader):\n            inputs = inputs.cuda()\n            labels = labels.cuda()\n\n            optimizer.zero_grad()\n\n            # forward and backward\n            with torch.set_grad_enabled(True):\n                preds = model(inputs)\n                loss_value = loss(preds, labels)\n                preds_class = preds.argmax(dim=1)\n\n                loss_value.backward()\n                optimizer.step()\n                # scheduler.step()\n\n            batch_loss = loss_value.item()\n            batch_accuracy = (preds_class == labels.data).float().mean().cpu().numpy().item()\n\n            sum_loss += batch_loss\n            sum_accuracy += batch_accuracy\n            \n            train_batch_loss_history.append(batch_loss)\n            train_batch_accuracy_history.append(batch_accuracy)\n            #print(f'\\r----- {phase}, batch accuracy {train_batch_accuracy * 100:.2f} %, batch loss {train_batch_loss:.4f}')        \n            #validate(model, loss, optimizer)\n            \n        epoch_loss = sum_loss \/ len(dataloader)\n        epoch_acc = sum_accuracy \/ len(dataloader)\n\n        train_loss_history.append(epoch_loss)\n        train_accuracy_history.append(epoch_acc)\n        scheduler.step()\n\n        # \u0412\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f\n        # print('\\n End epoch: ', end='')\n        validate(model, loss, optimizer)\n        \n    return model","d32fec4a":"model = models.resnet50(pretrained=True)\n#model = models.mobilenet_v2(pretrained=True)\n\n# for param in model.parameters():\n#     param.requires_grad = False\n\nmodel.fc = torch.nn.Linear(model.fc.in_features, 2)\n#model.classifier[1] = torch.nn.Linear(model.classifier[1].in_features, 2)\n\nmodel = model.cuda()\n\nloss = torch.nn.CrossEntropyLoss() #weight=torch.FloatTensor([1, 1]).cuda())\noptimizer = torch.optim.Adam(model.parameters())#, lr=1.0e-3, weight_decay=0.01, amsgrad=True)\n\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.33)\n#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[20, 40], gamma=0.1)  ","7d3f1d0a":"print(f'Batch size: {batch_size}\\nBatches: {len(train_dataloader)}\\nAll elements: {len(train_dataset)}')","083aefd4":"epochs = 7\n\ntrain_model(model, loss, optimizer, scheduler, num_epochs=epochs);","b7226914":"plt.figure(figsize=(20,10))\n    \nplt.subplot(1, 3, 1)\nplt.plot(train_batch_loss_history, label='Train Batch Loss')\nplt.plot(train_batch_accuracy_history, label='Train Batch Accuracy')\nplt.legend();\n\nplt.subplot(1, 3, 2)\nplt.plot(train_accuracy_history, label='Train accuracy')\nplt.plot(val_accuracy_history, label='Val accuracy')\nplt.legend();\n    \nplt.subplot(1, 3, 3)\nplt.plot(train_loss_history, label='Train Loss')\nplt.plot(val_loss_history, label='Val Loss')\nplt.legend();","1eb09fce":"os.makedirs(join(root_dir, 'test'), exist_ok=True)\n\ntest_dir = join(root_dir, 'test')\n\nos.makedirs(join(test_dir, 'unknown'), exist_ok=True)\n\nsource_dir = join(input_dir, 'test', 'test')\n\nfor i, filename in enumerate(tqdm(sorted(os.listdir(source_dir)))):\n    shutil.copy(join(source_dir, filename), join(test_dir, 'unknown', filename))\n    \n    if i < 10:\n        print(filename)","a0cc28dc":"test_transforms = val_transforms\n\ntest_dataset = torchvision.datasets.ImageFolder(test_dir, test_transforms)\n\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, num_workers=0)\n\nfor images, labels in test_dataloader:\n    print(images.size())\n    print(labels.size())\n    break","b5d4f2b6":"show_sample_images(test_dataloader, batch_size, 12, denormalize=True, classes=['Unknown'])","518ba96b":"model.eval()\n\ntest_predictions = []\n\ni = 1\nfor images, labels in test_dataloader:\n    images = images.cuda()\n    with torch.set_grad_enabled(False):\n        preds = model(images)\n    test_predictions.append(T.softmax(preds, dim=1)[:,1].data.cpu().numpy())\n    print(f'\\r{i}\/{len(test_dataloader)}', end='')\n    i += 1\n    \ntest_predictions = np.concatenate(test_predictions)  \ntest_predictions = (test_predictions >= 0.5).astype('int')","eb24dc69":"test_files = next(iter(os.walk(join(test_dir, 'unknown'))))[2]\n\nprint(sorted(test_files)[:10])","59a7ad1d":"submission_df = pd.DataFrame.from_dict({'id': sorted(test_files), 'has_cactus': test_predictions})\nsubmission_df.set_index('id', inplace=True)\nsubmission_df.head(25)","53b6e7f0":"submission_df.to_csv('submission.csv')","63d33d47":"!rm -rf train val test","815deeb8":"### Preparing train and validation datasets","3364aa43":"### Preparing test dataset and dataloader","c264ec4b":"### Let's train!","a0311cfe":"### Define Model, Loss and Optimizer","b99bcb7d":"### Make train and validation datasets","4a30e8bd":"### History graphs","36913e31":"### Make submission","378fec8c":"### Let's look at the samples","de3c0847":"### Load Labels","d5146f93":"### Predict on test set","d8249785":"### Plot function","f816ae6a":"### Define train_model and validate functions"}}