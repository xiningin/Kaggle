{"cell_type":{"66558d76":"code","90552351":"code","4b745a9f":"code","74fac8b8":"code","f2661f8f":"code","b27688e2":"code","eabe2bc5":"code","8d91e34d":"code","a3112c12":"code","d8f28651":"code","7d0274db":"code","57d60cbe":"code","005607c3":"code","5771248a":"code","06f272bf":"code","f640148c":"code","98de193e":"code","3e268796":"code","a72beb92":"code","38e2d538":"code","f6ecdc9b":"markdown"},"source":{"66558d76":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","90552351":"import numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer  \nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nstyle.use(\"seaborn\")","4b745a9f":"Hyper = {\"num_words\":1500 , \"oov_token\":'<OOV>' , \"maxlen\": 512 , \"truncating\":'post' , 'Embedding_dims' : 64 }","74fac8b8":"DataSet = pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")","f2661f8f":"DataSet.info()","b27688e2":"DataSet.describe()","eabe2bc5":"DataSet.head(3)","8d91e34d":"sns.countplot(DataSet[\"sentiment\"])","a3112c12":"def PreProcessing():\n    Corpus = []\n    for i in range( 0 , DataSet.shape[0]):\n        review = re.sub('[^a-zA-Z]' , ' ' , DataSet[\"review\"][i])\n        review = review.lower()\n        review = review.split()\n        review = ' '.join(review)\n        Corpus.append(review)\n    return Corpus","d8f28651":"Corpus = PreProcessing()\n#Make Catigoral Labels\nLabel =LabelBinarizer().fit_transform(DataSet['sentiment'])","7d0274db":"#Split Our Data\nX_train ,X_test , y_train , y_test = train_test_split( Corpus,Label, test_size = 0.25 , random_state = 42)","57d60cbe":"tokenizer = Tokenizer(num_words = Hyper[\"num_words\"] ,oov_token = Hyper[\"oov_token\"])\ntokenizer.fit_on_texts(X_train)\nword_index = tokenizer.word_index\nsequance = tokenizer.texts_to_sequences(X_train)\npad_training_data = pad_sequences(sequance , maxlen=Hyper[\"maxlen\"] , truncating = Hyper['truncating'])","005607c3":"sequance = tokenizer.texts_to_sequences(X_train)\npad_training_data = pad_sequences(sequance , maxlen=Hyper[\"maxlen\"], truncating =Hyper['truncating'])","5771248a":"testing_sequences = tokenizer.texts_to_sequences(X_test)\ntesting_padded = pad_sequences(testing_sequences,maxlen=Hyper['maxlen']  , truncating = Hyper['truncating'])","06f272bf":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_accuracy', patience = 2, verbose=1,factor=0.5, min_lr=0.00001)","f640148c":"def My_Model():\n    input_ = tf.keras.Input((Hyper['maxlen'],))\n    \n    x = tf.keras.layers.Embedding(len(tokenizer.word_index)+1,  Hyper['Embedding_dims'])(input_)\n    \n    x = tf.keras.layers.SpatialDropout1D(0.4)(x)\n    \n    x = tf.keras.layers.Conv1D(128 , 7 , activation = 'relu' , padding = 'same' , kernel_initializer='he_normal')(x)\n    x = tf.keras.layers.Conv1D(65 , 7 , activation = 'relu' , padding = 'same' , kernel_initializer='he_normal')(x)\n    \n    x = tf.keras.layers.MaxPooling1D()(x)\n    x = tf.keras.layers.BatchNormalization()(x)\n\n    flatten = tf.keras.layers.GlobalMaxPooling1D()(x)\n    x = tf.keras.layers.BatchNormalization()(flatten)\n    \n    x = tf.keras.layers.Dense(units = 32 , activation = 'relu' ,kernel_initializer='he_normal' )(x)\n    \n    x = tf.keras.layers.Dropout(0.3)(x)\n    \n    output_ = tf.keras.layers.Dense(units = 1 , activation = 'sigmoid')(x)\n    \n    return tf.keras.models.Model(inputs = [input_] , outputs = [output_])","98de193e":"Model = My_Model()\nModel.compile(optimizer = 'adam' , loss = 'binary_crossentropy' , metrics = ['accuracy'])\nkeras.utils.plot_model(Model)\nprint(Model.summary())","3e268796":"History= Model.fit(pad_training_data , y_train , epochs = 10 , batch_size = 128 , validation_data = (testing_padded , y_test) , callbacks = [learning_rate_reduction])","a72beb92":"Model.evaluate(testing_padded , y_test)","38e2d538":"train_acc = History.history['accuracy']\ntrain_loss = History.history['loss']\nval_acc = History.history['val_accuracy']\nval_loss = History.history['val_loss']\nplt.plot(train_acc , 'go-',label = 'Training Accuracy')\nplt.plot(val_acc , 'ro-', label = 'Testing Accuracy')\nplt.plot(train_loss , 'mo-',label = 'Training Loss')\nplt.plot(val_loss , 'co-',label = 'Testing Loss')\nplt.legend()\nplt.show()","f6ecdc9b":"# PreProcessing Our Data"}}