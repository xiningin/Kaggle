{"cell_type":{"af4b6c59":"code","7e4996fe":"code","5c850816":"code","cd183e4d":"code","1dcc5151":"code","1a907b9f":"code","8019d836":"code","355ac153":"code","6cd9462d":"code","c840c211":"code","32c6c2a3":"code","05a35dbd":"code","2b8c0c0c":"code","242f15f7":"code","fafbf36e":"code","852b1227":"code","55eb795c":"code","51ac9c22":"code","d8ce00cf":"code","fc795a98":"code","5bdfa32a":"code","4015166d":"code","24f9f7bb":"code","21a2c9ac":"code","438be713":"code","38943c4b":"code","78fb21b6":"code","00b7c642":"code","b2aa95c2":"code","df22d460":"code","a19988e9":"code","b18d661d":"code","ceb69803":"code","a32ab142":"code","67d6b855":"code","a454368d":"code","a38e98eb":"code","08788fc2":"code","c47fbeff":"code","d6f27902":"code","e3b952b7":"code","79904447":"code","26d8c77e":"code","de5bb3f6":"code","c20bce01":"code","b5bf4766":"code","fd67dbb4":"code","d323b146":"code","15005446":"code","1917065c":"code","bf2ea6b6":"code","f8fc3fbf":"code","ebd673ba":"code","88df7bac":"code","5ee60624":"code","617175ad":"code","f4aa326c":"code","bf8f4422":"code","9df2330f":"code","1b1c2553":"code","c7277c5c":"code","be4e32d9":"code","1bdd5226":"code","5aa42c66":"code","915403d3":"code","b4290092":"code","2c3d1071":"code","903ef635":"code","8643a010":"code","5d3d1bad":"code","5ac73ef8":"code","f7ab489d":"code","8952adf0":"code","e9e97d3c":"code","13081d33":"markdown","9e3b40f8":"markdown","1731aaf7":"markdown","f1cd0215":"markdown","bc36dd77":"markdown","74ed5d44":"markdown","2b73d146":"markdown","5695f6d9":"markdown","78bb12b5":"markdown","b767136d":"markdown","2c41b3f0":"markdown","3dcea486":"markdown","47e342c7":"markdown","a99c5d0c":"markdown","bce06153":"markdown","d2283953":"markdown","d8397b2b":"markdown","5f081755":"markdown","aa160dcc":"markdown","8f58e89a":"markdown","8d6452ef":"markdown","99e5e73e":"markdown","2ab88ab7":"markdown","badfc317":"markdown","a80bad1f":"markdown","5a2600e7":"markdown","9d4e6a4c":"markdown","db7b1c8b":"markdown"},"source":{"af4b6c59":"LOCATION_KAGGLE = True\nverbose_max = 1  # limit verbosity\n#\nout_dir = \".\"\nversion_str = \"v34\"\nSHOW_EDA = True\nUSE_SPLIT_AVE = True","7e4996fe":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\n\nimport os\n##print(os.listdir(\"..\/input\"))\n\nfrom time import time\nfrom time import strftime","5c850816":"# Read in the training and test data\ndf_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")\n\n# All of the initial Training columns are:\n#  ['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n#       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked']\n\n\n# Change\/Adjust\/Make-new feature columns...\n\n\n# --- PassengerId: sequential number - ignore.\n\n\n# --- Survived:  the \"target\" values\n# Add a Survived column to the test df for uniformity:\ndf_test['Survived'] = -1\n\n\n# --- Pclass:\n# Make one-hot versions\nfor iclass in [1,2,3]:\n    df_train['Pclass_'+str(iclass)] = (df_train['Pclass'] == iclass).astype(int)\n    df_test['Pclass_'+str(iclass)] = (df_test['Pclass'] == iclass).astype(int)\n\n    \n# --- Name: will be processed in next cells.\n\n\n# --- Sex: \n# from male\/female to 1\/0:\n##df_train['Sex'] = (df_train['Sex'] == \"male\").astype(int)\n##df_test['Sex'] = (df_test['Sex'] == \"male\").astype(int)\n# From male\/female to \"one hot\" Sex_M, Sex_F:\ndf_train['Sex_M'] = (df_train['Sex'] == \"male\").astype(int)\ndf_train['Sex_F'] = (df_train['Sex'] == \"female\").astype(int)\n#\ndf_test['Sex_M'] = (df_test['Sex'] == \"male\").astype(int)\ndf_test['Sex_F'] = (df_test['Sex'] == \"female\").astype(int)\n\n    \n# --- Age: initially about 20% have NaN for age.\n#\n# Do a little research:\n# Get a df of just the ones with NaN for Age\n##df_NaNage = df_train[~(df_train['Age'] == df_train['Age'])]\n# Average survival for these:\n##df_NaNage['Survived'].mean()\n# This gives survival rate of 0.2937 for the age = NaNs,\n# compared to 0.38 for all train samples.\n# Could use Name information (Mr, Mrs, Miss, Master) to assign an age...\n# Or flag the NaN ages by setting age to something like 99:\ndf_train['Age'].fillna(99.0, inplace=True)\ndf_test['Age'].fillna(99.0, inplace=True)\n# Add a new numeric column flagging that no age was given:\ndf_train['NoAge'] = 1.0*(df_train['Age'] > 95.0)\ndf_test['NoAge'] = 1.0*(df_test['Age'] > 95.0)\n# but for Logistic Regression better to set to the trains median value(?)\n##age_median = df_train['Age'].median()\n##df_train['Age'].fillna(age_median, inplace=True)\n##df_test['Age'].fillna(age_median, inplace=True)\n# Or randomly set the NoAge ones in the range 18 to 40:\nfor inoage in df_train[df_train['NoAge'] == 1.0].index:\n    df_train.loc[inoage,'Age'] = int(18.0+(40.0-18.0)*np.random.rand())\nfor inoage in df_test[df_test['NoAge'] == 1.0].index:\n    df_test.loc[inoage,'Age'] = int(18.0+(40.0-18.0)*np.random.rand())\n#\n# OK, the Age NaNs are taken care of...\n# Make a log(Age):\ndf_train['Age_log'] = np.log(1.0+df_train['Age'])\ndf_test['Age_log'] = np.log(1.0+df_test['Age'])\n# Make Age_young, Age_old: a ReLU-ish on ends of age\ndf_train['Age_young'] = (3.0 - df_train['Age_log'])\ndf_train.loc[df_train['Age_young'] < 0.0, 'Age_young'] = 0.0\ndf_train['Age_old'] = (df_train['Age_log'] - 3.8)\ndf_train.loc[df_train['Age_old'] < 0.0, 'Age_old'] = 0.0\n# and for test\ndf_test['Age_young'] = (3.0 - df_test['Age_log'])\ndf_test.loc[df_test['Age_young'] < 0.0, 'Age_young'] = 0.0\ndf_test['Age_old'] = (df_test['Age_log'] - 3.8)\ndf_test.loc[df_test['Age_old'] < 0.0, 'Age_old'] = 0.0\n\n\n# --- SibSp and Parch :\n# Try limiting these to just 0, 1, 2, 3, 4 :\ncols = ['SibSp','Parch']\nfor col in cols:\n    df_train.loc[(df_train[col] > 4), col] = 4\n\n    \n# --- Ticket: complex string, ignore.\n\n\n# --- Fare:\n#\n# One of the Test cases has NaN for the Fare:\n# 152\t1044\t3\tStorey, Mr. Thomas\t1\t60.5\t0\t0\t3701\tNaN\tooo\tS\n# Look at similar ones in test \n##df_test[(df_test['Pclass'] == 3) & (df_test['Embarked'] == 'S')].head(20)\n# Set it to 9.2250 based on another 4-digit ticket:\n# 5\t897\t3\tSvensson, Mr. Johan Cervin\t1\t14.0\t0\t0\t7538\t9.2250\tooo\tS\ndf_test.loc[152,'Fare'] = 9.2250\n#\n# Add Fare_zero flag for very low fares:\ndf_train['Fare_0'] = (df_train['Fare'] < 6.0).astype(int)\ndf_test['Fare_0'] = (df_test['Fare'] < 6.0).astype(int)\n# Set a nominal Fare for the Fare_0 ones, based on the Pclass:\nfare_class = np.exp([2.0, 2.4, 3.3])\nfor pclass in [1,2,3]:\n    df_train.loc[(df_train['Pclass'] == pclass) & \\\n                 (df_train['Fare_0'] == 1), 'Fare'] = fare_class[pclass-1]\n    df_test.loc[(df_test['Pclass'] == pclass) & \\\n                (df_test['Fare_0'] == 1), 'Fare'] = fare_class[pclass-1]\n# and make log(Fare):\ndf_train['Fare_log'] = np.log(df_train['Fare'])\ndf_test['Fare_log'] = np.log(df_test['Fare'])\n\n\n# --- Cabin: replace NaNs with \"ooo\"\n#\ndf_train['Cabin'].fillna(\"ooo\", inplace=True)\ndf_test['Cabin'].fillna(\"ooo\", inplace=True)\n# add a new numeric column flagging no cabin:\ndf_train['NoCabin'] = 1.0*(df_train['Cabin'] == \"ooo\")\ndf_test['NoCabin'] = 1.0*(df_test['Cabin'] == \"ooo\")\n\n\n# --- Embarked:\n# Two of the train rows have NaN for Embarked - set them to S:\ndf_train.loc[61,'Embarked'] = 'S'\ndf_train.loc[829,'Embarked'] = 'S'\n#\n# Create one-hot versions of Embarked:\ndf_train['Embark_C'] = (df_train['Embarked'] == \"C\").astype(int)\ndf_train['Embark_Q'] = (df_train['Embarked'] == \"Q\").astype(int)\ndf_train['Embark_S'] = (df_train['Embarked'] == \"S\").astype(int)\n#\ndf_test['Embark_C'] = (df_test['Embarked'] == \"C\").astype(int)\ndf_test['Embark_Q'] = (df_test['Embarked'] == \"Q\").astype(int)\ndf_test['Embark_S'] = (df_test['Embarked'] == \"S\").astype(int)\n\n\n# Done with basic features.","cd183e4d":"# Look at the Name column... all unique names it seems.\n##df_train['Name'].value_counts()","1dcc5151":"# Extract the last name from the Name column\ndef extract_last(row_in):\n    lastname = row_in['Name'].split(',')[0]\n    # replace \"'\", \" \", and \"-\" with nothing (for now)\n    ignores = [\"'\",\" \",\"-\"]\n    for iggy in ignores:\n        lastname = lastname.replace(iggy,\"\")\n    return lastname\n\n# Put the last names in LastName\ndf_train['LastName'] = df_train.apply(extract_last, axis=1)\ndf_test['LastName'] = df_test.apply(extract_last, axis=1)","1a907b9f":"# Look at the LastName column... 9 Andersson the most.\n##df_train['LastName'].value_counts()","8019d836":"# Extract Mr, Mrs, Miss, Master from the Name column\ndef extract_MMMM(row_in):\n    # first take from the Comma+Space on\n    prefix = row_in['Name'].split(', ')[1]\n    # then take before the \".\":\n    prefix = prefix.split(\".\")[0]\n    # modify ones that are not Mr Mrs Miss Master:\n    if not(prefix in [\"Mr\",\"Mrs\",\"Miss\",\"Master\"]):\n        if prefix in [\"Rev\", \"Col\", \"Major\", \"Sir\", \"Jonkheer\", \"Don\", \"Capt\"]:\n            prefix = \"Mr\"\n        elif prefix in [\"Ms\", \"Mme\", \"the Countess\", \"Lady\", \"Dona\"]:\n            prefix = \"Mrs\"\n        elif prefix in [\"Mlle\"]:\n            prefix = \"Miss\"\n        elif prefix in [\"Dr\"]:\n            # can be a female Dr:\n            if row_in['Sex'] == 'female':\n                prefix = \"Mrs\"\n            else:\n                prefix = \"Mr\"\n    ##print(\"prefix is -->\"+prefix+\"<--\")\n    return prefix\n\n# Put the prefix in a column\ndf_train['MMMM'] = df_train.apply(extract_MMMM, axis=1)\ndf_test['MMMM'] = df_test.apply(extract_MMMM, axis=1)","355ac153":"# Look at the MMMM counts\n# ( Without any corrections implemented in extract_MMMM,\n#   use value_counts to identify prefixs other than Mr Mrs Miss and Master:\n#   Alternatives found:\n#   Master: (no other versions)\n#      Mrs: Ms, Mme, the Countess, Lady, Dona\n#     Miss: Mlle\n#       Mr: Dr, Rev, Col, Major, Sir, Jonkheer, Don, Capt \n#   incorporate these in extract_MMMM above.)\n\nprint(\"\\nTrain:\\n\")\nprint(df_train['MMMM'].value_counts())\nprint(\"\\nTEST:\\n\")\nprint(df_test['MMMM'].value_counts())","6cd9462d":"# Look at the survival of these different prefixes:\ndf_train[['Survived','MMMM']].groupby('MMMM').mean()","c840c211":"# Create one-hot versions of the prefixes:\nfor prefix in [\"Mr\",\"Master\",\"Mrs\",\"Miss\"]:\n    df_train['Sex_'+prefix] = (df_train['MMMM'] == prefix).astype(int)\n    df_test['Sex_'+prefix] = (df_test['MMMM'] == prefix).astype(int)","32c6c2a3":"# Create some features from the LastName\n\n# Get the length of the LastName\ndef extract_ln_length(row_in):\n    lastname = row_in['LastName']\n    return len(lastname)\n\n# Count the vowels in LastName\ndef extract_ln_vowels(row_in):\n    lastname = row_in['LastName']\n    num_vs = 0\n    vowels_list = [\"a\",\"e\",\"i\",\"o\",\"u\",\"y\"]\n    for char in lastname.lower():\n        if char in vowels_list:\n            num_vs += 1\n    return num_vs\n\n# Put the last name lengths and vowels in LN_Length, LN_Vowels\ndf_train['LN_Length'] = df_train.apply(extract_ln_length, axis=1)\ndf_test['LN_Length'] = df_test.apply(extract_ln_length, axis=1)\ndf_train['LN_Vowels'] = df_train.apply(extract_ln_vowels, axis=1)\ndf_test['LN_Vowels'] = df_test.apply(extract_ln_vowels, axis=1)\n# And the ratio\ndf_train['LN_Vfrac'] = df_train['LN_Vowels']\/df_train['LN_Length']\ndf_test['LN_Vfrac'] = df_test['LN_Vowels']\/df_test['LN_Length']","05a35dbd":"# Check for NaN's in the data sets\n# Go through the columns one at a time\n# For Titanic the Age is the most NaN'ed column...\n\nn_train = len(df_train)\nn_test = len(df_test)\nprint(\"\\nChecking for NaNs:\\n\")\nall_ok = True\nfor col in df_test.columns:\n    nona_train = len(df_train[col].dropna(axis=0))\n    nanpc_train = 100.0*(n_train-nona_train)\/n_train\n    nona_test = len(df_test[col].dropna(axis=0))\n    nanpc_test = 100.0*(n_test-nona_test)\/n_test\n    # Only show it if there are NaNs:\n    if (nanpc_train + nanpc_test > 0.0):\n        print(\"{:.3f}%  {} OK out of {}\".format(nanpc_train, nona_train, n_train), \"  \"+col)\n        print(\"{:.3f}%  {} OK out of {}\".format(nanpc_test, nona_test, n_test), \"  \"+col)\n        all_ok = False\nif all_ok:\n    print(\"   All OK - no NaNs found.\\n\")","2b8c0c0c":"# Look at the data now...\ndf_train.head(6)","242f15f7":"# and the Test data\ndf_test.head(6)","fafbf36e":"# Compare Train and Test averages\n# Using a z-score with standard error based on the number of samples\ndescr_train = df_train.describe()\ndescr_test = df_test.describe()\n# Number of samples in the test set\nn_test = descr_test.loc[\"count\",\"Age\"]\nn_train = descr_train.loc[\"count\",\"Age\"]\nif SHOW_EDA:\n    print(\"     --column--    z-score      Test Mean     Train Mean\")\nfor col in descr_test.columns:\n    ave_test = descr_test.loc[\"mean\",col]\n    ave_train = descr_train.loc[\"mean\",col]\n    std_train = descr_train.loc[\"std\",col]\n    if SHOW_EDA:\n        print(col.rjust(15), \n            '{:.4f}'.format((ave_test - ave_train)\/\n                               (std_train*np.sqrt(1.0\/n_test+1.0\/n_train))).rjust(10),\n            '{:.4f}'.format(ave_test).rjust(14),\n            '{:.4f}'.format(ave_train).rjust(14))","852b1227":"df_train.plot.scatter(\"Age\",\"Fare_log\",\n                        figsize=(8,5),c='Survived',alpha=0.6,colormap=\"Set1\",colorbar=False)\nplt.title(\"Showing All  (red=Perished)\")\nplt.show()\nprefix = \"Mrs\"\ndf_train[df_train['MMMM'] == prefix].plot.scatter(\"Age\",\"Fare_log\",\n                        figsize=(8,5),c='Survived',alpha=0.6,colormap=\"Set1\",colorbar=False)\nplt.title(\"Showing only \"+prefix+\"  (red=Perished)\")\nplt.show()","55eb795c":"df_train.plot.scatter(\"LN_Length\",\"LN_Vowels\",\n                        figsize=(8,5),c='Survived',alpha=0.6,colormap=\"Set1\",colorbar=False)\nplt.title(\"Showing All  (red=Perished)\")\nplt.show()\n\ndf_train.plot.scatter(\"LN_Length\",\"LN_Vfrac\",\n                        figsize=(8,5),c='Survived',alpha=0.3,colormap=\"Set1\",colorbar=False)\nplt.title(\"Showing All  (red=Perished)\")\nplt.show()","51ac9c22":"# Survival vs Age for Females, Males\ndf_train[df_train['Sex_F'] == 1].plot.scatter('Age_log','Survived')\nplt.text(1.8, 0.5,\"Females\")\nplt.show()\ndf_train[df_train['Sex_M'] == 1].plot.scatter('Age_log','Survived')\nplt.text(1.9, 0.5,\"Males\")\nplt.show()","d8ce00cf":"# Try the violin plot - Male, Female categories\n# choose a numerical column:\nvpcol = \"Age\" #  or can use \"Fare_log\"\n\n# Simple histogram as a check\n##df_train['Age'].plot.hist(figsize=(8,5), bins=50)\n\ndf = df_train\nstatus_str = 'All'\n\nfig, axes = plt.subplots(figsize=(8,5))\n\naxes.violinplot(dataset = [df[df.Survived == 1][vpcol].values,\n                           df[df.Survived == 0][vpcol].values],\n               positions=[1.0,1.5],\n               widths=0.4,\n               showmeans=False, showmedians=True, showextrema=False,\n               points=1000,\n               bw_method=0.1,  # 'scott', 'silverman', or a scalar \n               vert=True)\n\naxes.set_title(\"Survived           Selection: \"+status_str+\"            Perished\")\naxes.yaxis.grid(True)\naxes.set_xlabel('')\naxes.set_ylabel(vpcol)\n\nplt.show()\n\n\ndf = df_train[df_train['Sex'] == 'male']\nstatus_str = 'Male'\n\nfig, axes = plt.subplots(figsize=(8,5))\n\naxes.violinplot(dataset = [df[df.Survived == 1][vpcol].values,\n                           df[df.Survived == 0][vpcol].values],\n               positions=[1.0,1.5],\n               widths=0.4,\n               showmeans=False, showmedians=True, showextrema=False,\n               points=1000,\n               bw_method=0.1,  # 'scott', 'silverman', or a scalar \n               vert=True)\n\naxes.set_title(\"Survived           Selection: \"+status_str+\"            Perished\")\naxes.yaxis.grid(True)\naxes.set_xlabel('')\naxes.set_ylabel(vpcol)\n\nplt.show()\n\n\ndf = df_train[df_train['Sex'] == 'female']\nstatus_str = 'Female'\n\nfig, axes = plt.subplots(figsize=(8,5))\n\naxes.violinplot(dataset = [df[df.Survived == 1][vpcol].values,\n                           df[df.Survived == 0][vpcol].values],\n               positions=[1.0,1.5],\n               widths=0.4,\n               showmeans=False, showmedians=True, showextrema=False,\n               points=1000,\n               bw_method=0.1,  # 'scott', 'silverman', or a scalar \n               vert=True)\n\naxes.set_title(\"Survived           Selection: \"+status_str+\"            Perished\")\naxes.yaxis.grid(True)\naxes.set_xlabel('')\naxes.set_ylabel(vpcol)\n\nplt.show()\n","fc795a98":"# How do Age and Fare depend on Pclass\n\n##df_train.plot.scatter('Age_log','Pclass')\n##plt.show()\n##df_train.plot.scatter('Fare_log','Pclass')\n##plt.show()","5bdfa32a":"# Try the violin plot - Pclass categories\n# choose a numerical column:\nvpcol = \"Age\" #\"Fare_log\" # or can use \"Age\"\n\n\n# Select survival status\ndfs = df_train[df_train['Survived'] == 1].copy()\ndfp = df_train[df_train['Survived'] == 0].copy()\n\nfig, axes = plt.subplots(figsize=(12,5))\naxes.violinplot(dataset = [dfs[dfs.Pclass == 1][vpcol].values,\n                           dfs[dfs.Pclass == 2][vpcol].values,\n                           dfs[dfs.Pclass == 3][vpcol].values,\n                        dfp[dfp.Pclass == 1][vpcol].values,\n                        dfp[dfp.Pclass == 2][vpcol].values,\n                        dfp[dfp.Pclass == 3][vpcol].values],\n               positions=[1.5,2.5,3.5,1.0,2.0,3.0],\n               widths=0.4,\n               showmeans=False, showmedians=True, showextrema=False,\n               points=1000,\n               bw_method=0.1,  # 'scott', 'silverman', or a scalar \n               vert=True)\n\naxes.set_title(\"Perish\/Survival for Pclasses, vs \"+vpcol)\naxes.yaxis.grid(False)\npersur_str = \"Perish                Survive\"\naxes.set_xlabel(persur_str + 30*\" \" + persur_str + 30*\" \" + persur_str)\naxes.set_ylabel(vpcol)\n\nplt.show()","4015166d":"# Try the violin plot - Pclass categories\n# choose a numerical column:\nvpcol = \"Fare_log\" #\"Fare_log\" # or can use \"Age\"\n\n\n# Select survival status\ndfs = df_train[df_train['Survived'] == 1].copy()\ndfp = df_train[df_train['Survived'] == 0].copy()\n\nfig, axes = plt.subplots(figsize=(12,5))\naxes.violinplot(dataset = [dfs[dfs.Pclass == 1][vpcol].values,\n                           dfs[dfs.Pclass == 2][vpcol].values,\n                           dfs[dfs.Pclass == 3][vpcol].values,\n                        dfp[dfp.Pclass == 1][vpcol].values,\n                        dfp[dfp.Pclass == 2][vpcol].values,\n                        dfp[dfp.Pclass == 3][vpcol].values],\n               positions=[1.5,2.5,3.5,1.0,2.0,3.0],\n               widths=0.4,\n               showmeans=False, showmedians=True, showextrema=False,\n               points=1000,\n               bw_method=0.1,  # 'scott', 'silverman', or a scalar \n               vert=True)\n\naxes.set_title(\"Perish\/Survival for Pclasses, vs \"+vpcol)\naxes.yaxis.grid(False)\npersur_str = \"Perish                Survive\"\naxes.set_xlabel(persur_str + 30*\" \" + persur_str + 30*\" \" + persur_str)\naxes.set_ylabel(vpcol)\n\nplt.show()","24f9f7bb":"# These two Age-based features 'signal' low and high ages,\n# probably most useful for simple Logisitic Rgression.\ndf_train.plot.scatter('Age_log','Age_young')\nplt.show()\ndf_train.plot.scatter('Age_log','Age_old')\nplt.show()\n","21a2c9ac":"# Look at the correlation between the numerical values\ncorr_df = df_train.corr()\n# In particular the correlations with Survived:\nprint(corr_df.Survived)\n\n# There are a bunch with abs(corr) at\/above 0.2:\n# [Pclass], [Fare], Pclass_1, Pclass_3, Fare_log, Sex_M,F, NoCabin\n# (ones in [ ]s are not used as features since others duplicate them.)","438be713":"# All Features... the training numeric columns:\nall_features = descr_train.columns\n\n# Remove the 'answer' column:\nall_features = all_features.drop('Survived')\n# and the PassengerId:\nall_features = all_features.drop('PassengerId')\n\n# List all of these potential features\nprint(len(all_features),\"All features:\")\nprint(all_features)","38943c4b":"# Can look at the value counts of a feature column ...\nfeatnum=0\ndf_train[all_features[featnum]].value_counts()","78fb21b6":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import make_scorer\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.svm import SVC\n\nfrom sklearn.neural_network import MLPClassifier","00b7c642":"# Use this routine to shown how the prediction is doing.\n# This routine is taken from the file chirp_roc_lib.py in the github repo at: \n#   https:\/\/github.com\/dan3dewey\/chirp-to-ROC\n# Some small modifications have been made here.\n\ndef y_yhat_plots(y, yh, title=\"y and y_score\", y_thresh=0.5,\n                     ROC=True, plots_prefix=None):\n    \"\"\"Output plots showing how y and y_hat are related:\n    the \"confusion dots\" plot is analogous to the confusion table,\n    and the standard ROC plot with its AOC value.\n    The yp=1 threshold can be changed with the y_thresh parameter.\n    y and yh are numpy arrays (not series or dataframe.)\n    \"\"\"\n    # The predicted y value with threshold = y_thresh\n    y_pred = 1.0 * (yh > y_thresh)\n\n    # Show table of actual and predicted counts\n    crosstab = pd.crosstab(y, y_pred, rownames=[\n                           'Actual'], colnames=['  Predicted'])\n    print(\"\\nConfusion matrix (y_thresh={:.3f}):\\n\\n\".format(y_thresh),\n        crosstab)\n\n    # Calculate the various metrics and rates\n    tn = crosstab[0][0]\n    fp = crosstab[1][0]\n    fn = crosstab[0][1]\n    tp = crosstab[1][1]\n\n    ##print(\" tn =\",tn)\n    ##print(\" fp =\",fp)\n    ##print(\" fn =\",fn)\n    ##print(\" tp =\",tp)\n\n    this_fpr = fp \/ (fp + tn)\n    this_fnr = fn \/ (fn + tp)\n\n    this_recall = tp \/ (tp + fn)\n    this_precision = tp \/ (tp + fp)\n    this_accur = (tp + tn) \/ (tp + fn + fp + tn)\n\n    this_posfrac = (tp + fn) \/ (tp + fn + fp + tn)\n\n    print(\"\\nResults:\\n\")\n    print(\" False Pos = \", 100.0 * this_fpr, \"%\")\n    print(\" False Neg = \", 100.0 * this_fnr, \"%\")\n    print(\"    Recall = \", 100.0 * this_recall, \"%\")\n    print(\" Precision = \", 100.0 * this_precision, \"%\")\n    print(\"\\n    Accuracy = \", 100.0 * this_accur, \"%\")\n    print(\" Pos. fract. = \", 100.0 * this_posfrac, \"%\")\n\n    # Put them in a dataframe for plots and ROC\n    # Reduce the number if very large:\n    if len(y) > 100000:\n        reduce_by = int(0.5+len(y)\/60000)\n        print(\"\\nUsing 1\/{} of the points for dots and ROC plots.\".format(reduce_by))\n        ysframe = pd.DataFrame([y[0: :reduce_by], yh[0: :reduce_by], \n                                y_pred[0: :reduce_by]], index=[\n                           'y', 'y-hat', 'y-pred']).transpose()\n        plot_alpha = 0.3\n    else:\n        ysframe = pd.DataFrame([y, yh, y_pred], index=[\n                           'y', 'y-hat', 'y-pred']).transpose()\n        plot_alpha = 0.7\n\n    # If the yh is discrete (0 and 1s only) then blur it a bit\n    # for a better visual dots plot\n    if min(abs(yh - 0.5)) > 0.49:\n        ysframe[\"y-hat\"] = (0.51 * ysframe[\"y-hat\"]\n                            + 0.49 * np.random.rand(len(yh)))\n\n    # Make a \"confusion dots\" plot\n    # Add a blurred y column\n    ysframe['y (blurred)'] = ysframe['y'] + 0.1 * np.random.randn(len(ysframe))\n\n    # Plot the real y (blurred) vs the predicted probability\n    # Note the flipped ylim values.\n    ysframe.plot.scatter('y-hat', 'y (blurred)', figsize=(12, 5),\n                         s=2, xlim=(0.0, 1.0), ylim=(1.8, -0.8), alpha=plot_alpha)\n    # show the \"correct\" locations on the plot\n    plt.plot([0.0, y_thresh], [0.0, 0.0], '-',\n        color='green', linewidth=5)\n    plt.plot([y_thresh, y_thresh], [0.0, 1.0], '-',\n        color='gray', linewidth=2)\n    plt.plot([y_thresh, 1.0], [1.0, 1.0], '-',\n        color='green', linewidth=5)\n    plt.title(\"Confusion-dots Plot: \" + title, fontsize=16)\n    # some labels\n    ythr2 = y_thresh\/2.0\n    plt.text(ythr2 - 0.03, 1.52, \"FN\", fontsize=16, color='red')\n    plt.text(ythr2 + 0.5 - 0.03, 1.52, \"TP\", fontsize=16, color='green')\n    plt.text(ythr2 - 0.03, -0.50, \"TN\", fontsize=16, color='green')\n    plt.text(ythr2 + 0.5 - 0.03, -0.50, \"FP\", fontsize=16, color='red')\n\n    if plots_prefix != None:\n        plt.savefig(plots_prefix+\"_dots.png\")\n    plt.show()\n\n    # Go on to calculate and plot the ROC?\n    if ROC == False:\n        return 0\n\n\n    # Make the ROC curve\n    #\n    # Set the y-hat as the index and sort on it\n    ysframe = ysframe.set_index('y-hat').sort_index()\n    # Put y-hat back as a column (but the sorting remains)\n    ysframe = ysframe.reset_index()\n\n    # Initialize the counts for threshold = 0\n    p_thresh = 0\n    FN = 0\n    TN = 0\n    TP = sum(ysframe['y'])\n    FP = len(ysframe) - TP\n\n    # Assemble the fpr and recall values\n    recall = []\n    fpr = []\n    # Go through each sample in y-hat order,\n    # advancing the threshold and adjusting the counts\n    for iprob in range(len(ysframe['y-hat'])):\n        p_thresh = ysframe.iloc[iprob]['y-hat']\n        if ysframe.iloc[iprob]['y'] == 0:\n            FP -= 1\n            TN += 1\n        else:\n            TP -= 1\n            FN += 1\n        # Recall and FPR:\n        recall.append(TP \/ (TP + FN))\n        fpr.append(FP \/ (FP + TN))\n\n    # Put recall and fpr in the dataframe\n    ysframe['Recall'] = recall\n    ysframe['FPR'] = fpr\n\n    # - - - ROC - - - could be separate routine\n    zoom_in = False\n\n    # Calculate the area under the ROC\n    roc_area = 0.0\n    for ifpr in range(1, len(fpr)):\n        # add on the bit of area (note sign change, going from high fpr to low)\n        roc_area += 0.5 * (recall[ifpr] + recall[ifpr - 1]\n                           ) * (fpr[ifpr - 1] - fpr[ifpr])\n\n    plt.figure(figsize=(8, 8))\n    plt.title(\"ROC: \" + title, size=16)\n    plt.plot(fpr, recall, '-b')\n    # Set the scales\n    if zoom_in:\n        plt.xlim(0.0, 0.10)\n        plt.ylim(0.0, 0.50)\n    else:\n        # full range:\n        plt.xlim(0.0, 1.0)\n        plt.ylim(0.0, 1.0)\n\n    # The reference line\n    plt.plot([0., 1.], [0., 1.], '--', color='orange')\n\n    # The point at the y_hat = y_tresh threshold\n    if True:\n        plt.plot([this_fpr], [this_recall], 'o', c='blue', markersize=15)\n        plt.xlabel('False Postive Rate', size=16)\n        plt.ylabel('Recall', size=16)\n        plt.annotate('y_hat = {:.2f}'.format(y_thresh),\n                            xy=(this_fpr+0.01 + 0.015,\n                            this_recall), size=14, color='blue')\n        plt.annotate(' Pos.Fraction = ' +\n                        '  {:.0f}%'.format(100 * this_posfrac),\n                        xy=(this_fpr + 0.03, this_recall - 0.045),\n                        size=14, color='blue')\n\n    # Show the ROC area (shows on zoomed-out plot)\n    plt.annotate('ROC Area = ' + str(roc_area)\n                 [:5], xy=(0.4, 0.1), size=16, color='blue')\n\n    # Show the plot\n    if plots_prefix != None:\n        plt.savefig(plots_prefix+\"_ROC.png\")\n    plt.show()\n\n    return roc_area  # or ysframe","b2aa95c2":"# Get X,y from dataframe\ndef get_Xy_values(df_in, features):\n    # Extract and return the features, X dataframe, and target values, y (np.array).\n\n    X = df_in[features].copy()\n    y = df_in.Survived.values\n\n    print(\"\\nThe y target has {} values.\\n\".format(len(y)))\n    return X, y","df22d460":"##all_features","a19988e9":"# Select which ones to use from all available:\n# ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'Pclass_1', 'Pclass_2',\n#       'Pclass_3', 'Sex_M', 'Sex_F', 'NoAge', 'Age_log', 'Age_young',\n#       'Age_old', 'Fare_0', 'Fare_log', 'NoCabin', 'Embark_C', 'Embark_Q',\n#       'Embark_S', 'Sex_Mr', 'Sex_Master', 'Sex_Mrs', 'Sex_Miss',\n#       'LN_Length', 'LN_Vowels', 'LN_Vfrac']\n\n# - Exclude Pclass, Fare and Age (but include other versions of them).\n# - Use Sex_Mr,etc instead of Sex_M,F.\n# - Add LN_Length,Vfrac   (Leave out LN_Vowels)\n# - Include Fare_log ? or not ?\nfeatures = [        'SibSp', 'Parch',          \\\n            'Pclass_1', 'Pclass_2', 'Pclass_3','NoCabin', \\\n            ##'NoAge', 'Age_log', 'Age_young', 'Age_old', 'Fare_0', \\\n            'NoAge', 'Age_log', 'Age_young', 'Age_old', 'Fare_0', 'Fare_log', \\\n            'Embark_C', 'Embark_Q', 'Embark_S', #'Sex_M', 'Sex_F']\n            'Sex_Mr', 'Sex_Master', 'Sex_Mrs', 'Sex_Miss',\n            'LN_Length', 'LN_Vfrac']\n","b18d661d":"# List the selected features\nprint(len(features),\"Selected features:\")\nprint(features)","ceb69803":"print(\"Training:\")\nX, y = get_Xy_values(df_train, features)\n\n# Get the Kaggle test set (Note: y_kag is not valid)\nprint(\"Kaggle Test:\")\nXkag, y_kag = get_Xy_values(df_test, features)","a32ab142":"# Offset, Scale all features so that Train features have mean 0.0 and standard deviation 1.0:\nfor col in X.columns:\n    col_mean = X[col].mean()\n    col_std = X[col].std()\n    ##print(col_mean, col_std)\n    # X\n    X[col] = (X[col] - col_mean)\/col_std\n    # Xkag\n    Xkag[col] = (Xkag[col] - col_mean)\/col_std","67d6b855":"# Select model to use:  lgr, dtc, rfc, gbc, svc, mlp, xgb\n\nmodel_name = \"xgb\"\n","a454368d":"# LogisticRegression(\n#  penalty=\u2019l2\u2019, dual=False, tol=0.0001, C=1.0,\n# fit_intercept=True, intercept_scaling=1, class_weight=None,\n# random_state=None, solver=\u2019warn\u2019, max_iter=100, multi_class=\u2019warn\u2019,\n# verbose=0, warm_start=False, n_jobs=None)\n\nlgr_params = {'tol': 0.00001,\n              'C': 0.04,\n              'solver': 'sag',\n              'max_iter': 10000,\n              'multi_class': 'ovr',\n              'verbose': 1}\n# Do each value once since the fitting is not very random\nlgr_param_grid = {'tol': [0.00001],\n                  'C': [0.005, 0.008, 0.01, 0.012, 0.015, 0.02, 0.025, 0.03, 0.035, 0.04, \\\n                        0.045, 0.05, 0.06, 0.07, 0.10]}\n","a38e98eb":"# Decision Tree Classifier\n\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html\n\n# DecisionTreeClassifier(\n# criterion=\u2019gini\u2019, splitter=\u2019best\u2019, max_depth=None, min_samples_split=2, min_samples_leaf=1,\n# min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None,\n# min_impurity_decrease=0.0, min_impurity_split=None, class_weight=None, presort=False)\n\ndtc_params = {'max_depth': 3,\n                'min_samples_leaf': 2,\n                'min_impurity_decrease': 0.001\n             }\n\n##dtc_param_grid = {'max_depth': [2, 3, 4, 5, 6, 7, 8, 9],\n##                'min_samples_leaf': [2, 3, 4, 5, 7, 9],\n##                'min_impurity_decrease': [0.001]\n##             }\n\ndtc_param_grid = {'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n                'min_samples_leaf': [2, 3, 4, 5, 6, 7, 8, 9, 10],\n                'min_impurity_decrease': [0.001]\n             }","08788fc2":"# Random Forest Classifier\n\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\n\n# RandomForestClassifier(\n# n_estimators=\u2019warn\u2019, criterion=\u2019gini\u2019, max_depth=None, min_samples_split=2, min_samples_leaf=1,\n# min_weight_fraction_leaf=0.0, max_features=\u2019auto\u2019, max_leaf_nodes=None, min_impurity_decrease=0.0,\n# min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0,\n# warm_start=False, class_weight=None)\n\n\nrfc_params = {'n_estimators': 200,\n              'max_depth': 5,\n              'min_samples_leaf': 2,\n              'min_impurity_decrease': 0.001\n             }\n\n##rfc_param_grid = {'max_depth': [4, 5, 6, 7, 8, 9, 10, 11, 12],\n##                  'min_samples_leaf': [2, 3, 4, 5, 6, 7, 8],\n##                  'min_impurity_decrease': [0.001]\n##                 }\n\n# Use max_depth = 5 and min_samples_leaf = 2;\n# Run these same parameters many times to see spread\nrfc_param_grid = [\n    {'max_depth': [5, 5],\n                  'min_samples_leaf': [2, 2],\n                  'min_impurity_decrease': [0.001]\n                 },\n    {'max_depth': [5, 5],\n                  'min_samples_leaf': [2, 2],\n                  'min_impurity_decrease': [0.001]\n                 },\n    {'max_depth': [5, 5],\n                  'min_samples_leaf': [2, 2],\n                  'min_impurity_decrease': [0.001]\n                 },\n    {'max_depth': [5, 5],\n                  'min_samples_leaf': [2, 2],\n                  'min_impurity_decrease': [0.001]\n                 }\n]\n","c47fbeff":"# Gradient Boosting Classifier\n\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html\n\n# GradientBoostingClassifier(\n#      loss=\u2019deviance\u2019,\n#      learning_rate=0.1,\n#      n_estimators=100,\n#      max_depth=3,\n#\n#      max_features=None,\n#      min_impurity_decrease=0.0,\n#      min_samples_leaf=1,\n#      min_samples_split=2,\n#      subsample=1.0,\n#\n#      n_iter_no_change=None,\n#      validation_fraction=0.1, tol=0.0001,\n#      verbose=0,\n#\n#      criterion=\u2019friedman_mse\u2019, \n#      min_weight_fraction_leaf=0.0,\n#      min_impurity_split=None, init=None, random_state=None,  \n#      max_leaf_nodes=None, warm_start=False, presort=\u2019auto\u2019, \n#      )\n\n# Parameters for this model\n#\n# possible loss functions:\n# \u2018deviance\u2019 refers to deviance (= logistic regression) for classification with probabilistic outputs. \n#\n# Other parameters for this model.   #***** are ones to focus on tuning...\n# Values here are updated to be the current 'best' ones.\ngbc_params = {\n          'learning_rate': 0.013, # Smaller better but n_estimators grows\n          'n_estimators': 400,   # Early stopping will limit this, so just set a large value.\n          #\n          'max_depth': 4,      #***** Keep small to reduce overfitting\n          #\n          'max_features': None,      #***** <1.0 reduces variance and increases bias\n          'min_impurity_decrease': 0.003,   #*****\n          'min_samples_leaf': 20,         # *****\n          'min_samples_split': 85,      #*****\n          'subsample': 0.80,            #***** less than 1.0 to reduce variance, increase bias\n          # early stopping:      \n          # allows not tuning the n_estimators parameter\n          'n_iter_no_change': 30,\n          'tol': 0.00003,\n          'validation_fraction': 0.15, 'tol': 0.0005,\n          'verbose': 0\n          }\n\n# Setup hyper-parameter grid for the model:\ngbc_param_grid = [\n    {\n              'min_impurity_decrease': [0.003],\n              'min_samples_leaf': [20, 20],\n              'min_samples_split': [85, 85],\n              'subsample': [0.80]},\n    {\n              'min_impurity_decrease': [0.003],\n              'min_samples_leaf': [20, 20],\n              'min_samples_split': [85, 85],\n              'subsample': [0.80]}\n]","d6f27902":"# Support Vector Classification - SVC\n\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html\n\n# SVC(\n# C=1.0, kernel=\u2019rbf\u2019, degree=3, gamma=\u2019auto_deprecated\u2019, coef0=0.0,\n# shrinking=True, probability=False, tol=0.001, cache_size=200,\n# class_weight=None, verbose=False, max_iter=-1, decision_function_shape=\u2019ovr\u2019, random_state=None)\n    \nsvc_params = {'kernel': 'rbf',  # one of \u2018linear\u2019, \u2018poly\u2019, \u2018rbf\u2019, \u2018sigmoid\u2019, \u2018precomputed\u2019 \n              'degree': 3,       # for 'poly' only\n              'coef0': 0.0,      # poly, sigmoid\n              'tol': 0.0003,      # stopping critereon\n              #\n              'C': 2.5,           # Penalty parameter C of the error term\n              'gamma': 0.060,     # auto gives 1\/n_features\n              #\n              'shrinking': True,  # use the shrinking heuristic\n              'probability': True, # allows proba values\n              'cache_size': 200,   # kernel cache (in MB)\n              'class_weight': 'balanced',  # the values of y to automatically adjust weights \n              'verbose': True,\n              'max_iter': 100000,\n              'decision_function_shape': 'ovr'  # same as other classifiers\n             }\n\n# Scan some values... \n# poly degree 3: C=2.5, gamma=0.030\nsvc_param_grid = {'C': [2.4, 2.5, 2.6],\n    'gamma': [0.029, 0.030, 0.031]\n    }\n# rbf            C=3.5, gamma=0.025\n##svc_param_grid = {'C': [3.4, 3.5, 3.6],\n##    'gamma': [0.024, 0.025, 0.026]\n##    }","e3b952b7":"# Neural Network Classifier\n\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPClassifier.html\n\n# MLPClassifier(\n# hidden_layer_sizes=(100, ), activation=\u2019relu\u2019, solver=\u2019adam\u2019, alpha=0.0001,\n# batch_size=\u2019auto\u2019, learning_rate=\u2019constant\u2019, learning_rate_init=0.001, power_t=0.5,\n# max_iter=200, shuffle=True, random_state=None, tol=0.0001,\n# verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True,\n# early_stopping=False, validation_fraction=0.1,\n# beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10\n              \nvalid_fraction = 0.15\n    \nmlp_params = {'hidden_layer_sizes': (14,10,8),\n              'alpha': 0.60,   # L2 regularization param\n              'learning_rate_init': 0.09,\n              # less often changed parameters:\n              'batch_size': 35,  # bit better than auto=200\n              'momentum': 0.50,  # for sgd; 0.95 more erratic\n              #\n              'activation': 'relu',\n              'solver': 'sgd',\n              'learning_rate': 'constant',\n              'max_iter': 500, # number of epochs, uses < 100 for (12,4)\n              'tol': 0.00003,\n              'n_iter_no_change': 30,  # number of epochs\n              'early_stopping': True,\n              'validation_fraction': valid_fraction,\n              #\n              'verbose': False\n             }\n\n# Scan some values:\n# ~ 0.6, 0.09, 0.5 best for (14,10,8)\nmlp_param_grid = [\n    {'alpha': [0.60, 0.61],  # L2 regularization param\n     'learning_rate_init': [0.090, 0.091],\n     'momentum': [0.50, 0.51],\n    }\n]","79904447":"# eXtreme Gradient Boost classifier\n\nfrom xgboost import XGBClassifier\n\n# Thefollowing is from 40% of the way down on the page:\n#   https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\n\n# XGBClassifier(\n# max_depth=3, learning_rate=0.1, n_estimators=100,\n# verbosity=1, objective='binary:logistic', booster='gbtree',\n# tree_method='auto', n_jobs=1, gpu_id=-1,\n# gamma=0, min_child_weight=1, max_delta_step=0,\n# subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1,\n# reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5,\n# random_state=0, missing=None)\n\n# get_params output, in alpha order:\n\n#{      'base_score': 0.50,\n# 'booster': 'gbtree',\n# 'colsample_bylevel': 1,\n# 'colsample_bynode': 1,\n# 'colsample_bytree': 1,\n#                           'gamma': 0,\n#                           'learning_rate': 0.1,    # xgb's eta\n# 'max_delta_step': 0,\n#                           'max_depth': 1,\n# 'min_child_weight': 1,\n# 'missing': None,\n#                           'n_estimators': 100,\n# 'n_jobs': 1,\n# 'nthread': None,\n#       'objective': 'binary:logistic',\n# 'random_state': 0,\n# 'reg_alpha': 0,    # xgb's alpha\n#       'reg_lambda': 1,   # xgb's lambda\n# 'scale_pos_weight': 1,\n# 'seed': None,\n# 'silent': None,\n#       'subsample': 1,\n#       'verbosity': 1}\n    \nxgb_params = {\n        \"max_depth\"        : 4,\n        \"learning_rate\"    : 0.08,\n        \"n_estimators\"     : 75,\n        \"min_child_weight\" : 4,\n        \"gamma\"            : 1.5,\n        \"colsample_bytree\" : 0.40,\n        \"subsample\"        : 1.0,\n        \"reg_lambda\"       : 1.0,\n    #\n        \"objective\": \"binary:logistic\",\n        \"base_score\" : 0.50,\n        \"verbosity\" : 1\n     }\n\n# Setup hyper-parameter grid for the model:\nxgb_param_grid = [\n    {\n        ##\"max_depth\"        : [5, 7, 9],  # fix at 8 or 4\n        \"learning_rate\"    : [0.01, 0.03, 0.08],   # <-- try different rates; v34 used 0.08\n        \"n_estimators\"     : np.array(range(5,300,10))  # <-- scan values; v34 used 75\n        ##\"min_child_weight\" : [2, 4, 6],   #\n        ##\"gamma\"            : [0.5, 1.5, 3.0, 4.5],  #\n        ##\"colsample_bytree\" : [0.14, 0.20, 0.28, 0.40] #\n        ##\"subsample\"        : [0.8, 1.0],      # keep value of 1\n        ##\"reg_lambda\"       : [0.1, 1.0, 10.0] # keep at 1\n     }\n]","26d8c77e":"# Choose the selected model\n\nif model_name == 'lgr':\n    model_base = LogisticRegression(**lgr_params)\n    param_grid = lgr_param_grid\n\nif model_name == 'dtc':\n    model_base = DecisionTreeClassifier(**dtc_params)\n    param_grid = dtc_param_grid\n    \nif model_name == 'rfc':\n    model_base = RandomForestClassifier(**rfc_params)\n    param_grid = rfc_param_grid\n    \nif model_name == 'gbc':\n    model_base = GradientBoostingClassifier(**gbc_params)\n    param_grid = gbc_param_grid\n    \nif model_name == 'svc':\n    model_base = SVC(**svc_params)\n    param_grid = svc_param_grid\n    \nif model_name == 'mlp':\n    model_base = MLPClassifier(**mlp_params)\n    param_grid = mlp_param_grid\n    \nif model_name == 'xgb':\n    model_base = XGBClassifier(**xgb_params)\n    param_grid = xgb_param_grid","de5bb3f6":"# Doing this fit here lets us skip over the Hyper-Parameter Search and continue\nif True:\n    best_fit_model = model_base.fit(X,y)\n    # Show these parameters\n    print(best_fit_model.get_params())\n    # Also define cv_folds, gscv_stats incase the following is skipped:\n    cv_folds = 10\n    gscv_stats = []","c20bce01":"# Define scoring function(s)\n\n# Demo of make_scorer with simple example:\nacc_scorer = make_scorer(accuracy_score, greater_is_better=True)\n\ngscv_scorer = acc_scorer\nscorer_name = 'ACC'","b5bf4766":"# Set the CV parameter or method:\n#\n# Use KFold with chosen number of folds: \ncv_folds = 10\ncv_param = cv_folds","fd67dbb4":"# Do the Grid Search\n\nprint(\"\\nDoing Grid Search on model: \"+model_name+\".\\n\")\n\n# Select number of CPUs for GSCV depending if on Kaggle or not:\nif LOCATION_KAGGLE:\n    number_cpus = -1\nelse:\n    number_cpus = -2\n\n#GridSearchCV(estimator, param_grid, scoring=None,\n#             fit_params=None,\n#             n_jobs=None, iid=\u2019warn\u2019, refit=True, cv=\u2019warn\u2019,\n#             verbose=0,\n#             pre_dispatch=\u20182*n_jobs\u2019, error_score=\u2019raise-deprecating\u2019,\n#             return_train_score=\u2019warn\u2019)\n\n# param_grid is:\n# Dictionary with parameters names (string) as keys and lists of parameter settings to try as values,\n# or a list of such dictionaries.\n    \ngscv = GridSearchCV(model_base, param_grid,\n            scoring=gscv_scorer,      #\n            n_jobs=number_cpus, #  -2: all CPUs but one are used\n            iid=False,  # independent identical distrib., \"agrees with standard defn of CV\"\n            cv=cv_param,\n            refit=True,\n            verbose=min(verbose_max,2),\n            return_train_score=True)\n\nt0 = time()\n\n_dummy = gscv.fit(X, y)\n","d323b146":"# Get the best model\nbest_fit_model = gscv.best_estimator_\n\n# Fit it to all the training data - This was already done by the refit=True\n##best_fit_model.fit(X, y)\n\nif (model_name == 'gbc'):\n    print(\"\\nGSCV Fitting took {:.1f} minutes. (Final fit took {} iterations.)\\n\".format(\n                                    (time()-t0)\/60.0, len(best_fit_model.train_score_)))\nelif (model_name == 'mlp'):\n    print(\"\\nGSCV Fitting took {:.1f} minutes. (Final fit took {} iterations.)\\n\".format(\n                                    (time()-t0)\/60.0, best_fit_model.n_iter_))\nelse:\n    print(\"\\nGSCV Fitting took {:.1f} minutes.\\n\".format((time()-t0)\/60.0))\n","15005446":"# The GSCV results are given by the python dictionary:\n##gscv.cv_results_\n\n# Here the TEST refers to the out-of-fold\/validation data in the CV process.\n","1917065c":"# Put the grid search results in a pandas dataframe\n# Sort by a value, e.g, the test score which is -1*MSE\ndf_gscv = pd.DataFrame.from_dict(gscv.cv_results_).sort_values(by='mean_test_score',ascending=False)\n\n# Put the original order in an \"index\" column:\ndf_gscv = df_gscv.reset_index()\n\n# form the statistics of the columns\ngscv_stats = df_gscv.describe()","bf2ea6b6":"# Look into the std values given by the GSCV output...\nif False:\n    # The average std of the test scores is given as:\n    print(\"\\nAverage std_test_score = {:.4f}\".format(gscv_stats.loc['mean','std_test_score']))\n    print(\"This is much larger than the range of the mean_test_score s (further below.)\")\n\n    # The error bars are dominated by the differences in score from split-to-split:\n    print(\"\\nThere is large (but consistent) variation between the splits:\")\n    for itest in range(3):   # len(df_gscv)):\n        for isplit in range(cv_folds):\n            print(\"split {}: score = {:.4f}\".format(isplit, df_gscv.loc[itest,'split'+str(isplit)+'_test_score']))\n        print(\"mean test score = {:.4f}\".format(df_gscv.loc[itest,'mean_test_score']))\n        print(\" - - -\")\n    print(\" etc\")\n\n    # This is much larger than the variation of the same split due to fitting\/parameter changes:\n    print(\"\\nIn contrast the variation within a single split is smaller:\")\n    for itest in range(6):   # len(df_gscv)):\n        for isplit in [0]:\n            print(\"split {}: score = {:.4f}\".format(isplit, df_gscv.loc[itest,'split'+str(isplit)+'_test_score']))\n    print(\" etc\\n\")\n    for itest in range(6):   # len(df_gscv)):\n        for isplit in [4]:\n            print(\"split {}: score = {:.4f}\".format(isplit, df_gscv.loc[itest,'split'+str(isplit)+'_test_score']))\n    print(\" etc\\n\")\n\n# Calculate the std of the each-split's scores,\n# and the average std of a split score.\n##print(\"\\nScore random variation is estimated from the std of each split:\")\nstd_split = 0.0\nfor isplit in range(cv_folds):\n    ##print(\"std split{} = {:.4f}\".format(isplit,gscv_stats.loc['std','split'+str(isplit)+'_test_score']))\n    std_split += (gscv_stats.loc['std','split'+str(isplit)+'_test_score'])**2\nstd_split = np.sqrt(std_split\/cv_folds)\n\nprint(\"The average standard deviation of test-split scores is {:.4f}\".format(std_split))\n# The sterr expected from the split std is then:\nsterr_splits = std_split\/np.sqrt(cv_folds)\nprint(\"Hence, the expected sterr of the test scores is {:.4f}\".format(sterr_splits))\n\n\n# Add a sterr_test_score column\n# not using this:\n##df_gscv['sterr_test_score'] = df_gscv['std_test_score']\/np.sqrt(cv_folds)\n# but using the sterr_splits instead:\ndf_gscv['sterr_test_score'] = sterr_splits","f8fc3fbf":"# Save the dataframe to a file (in out_dir) - or not:\n##timestr = strftime(\"%m-%d-%y_%H-%M\")\n# include the model name too\n##df_gscv.to_csv(out_dir+\"\/GSCV_\" + model_name +\n##               \"_{}_{}.csv\".format(timestr, version_str), header=True, index=True)\n\n# Show all the rows\n##df_gscv\n# Show the first some number of rows\ndf_gscv[0:min([6,len(df_gscv)])]","ebd673ba":"# Update the gscv_stats variable with the new sterr column\ngscv_stats = df_gscv.describe()\n\nprint(\"\\nMean test score Min\/50%\/75%\/Max: {:.4f}, {:.4f}, {:.4f}, {:.4f}\".format(\n                gscv_stats.loc['min','mean_test_score'],\n                gscv_stats.loc['50%','mean_test_score'], gscv_stats.loc['75%','mean_test_score'],\n                gscv_stats.loc['max','mean_test_score']))\nprint(\"\\nRange of mean_test_score = {:.4f}, sterr_test_score = {:.4f}\".format(\n                gscv_stats.loc['max','mean_test_score'] - gscv_stats.loc['min','mean_test_score'],\n                gscv_stats.loc['50%','sterr_test_score']))\n##gscv_stats","88df7bac":"# Get a list of all the parameters that were scanned, the keys from the dicts(s):\nif type(param_grid) ==  type({1:2}):\n    # it's a dictionary\n    param_keys = list(param_grid.keys())\nelse:\n    # it's a list of dictionaries\n    param_keys = []\n    for pdict in param_grid:\n        for key in pdict.keys():\n            param_keys.append(key)\n# make it a sorted, unique list\nparam_keys = list(set(param_keys))\nparam_keys.sort()\nprint(param_keys)","5ee60624":"# Show test score with error bars vs the GS original order (index)\n# Can color-code points by one of the parameters (or not.)\n# Pic a param\nthis_param = param_keys[0]\ndf_gscv.plot.scatter('index', 'mean_test_score', yerr='sterr_test_score', figsize=(15,4),\n                        title='Test Score ('+scorer_name+') vs Grid Search index',\n                        c=\"param_\"+this_param, colormap='plasma')\nplt.show()","617175ad":"# Make plots of the test values vs param values for the params\nnum_param_keys = len(param_keys)\nfig, axes = plt.subplots(1,num_param_keys,sharey=True,figsize=(15,5))\nfor iparam, this_param in enumerate(param_keys):\n    # Why is this needed to get scatter to work?!?\n    df_gscv[\"param_\"+this_param] = df_gscv[\"param_\"+this_param].astype(float)\n    if num_param_keys > 1:\n        ax = axes[iparam]\n    else:\n        ax = axes\n    # without or with the error bars\n    ##df_gscv.plot.scatter(\"param_\"+this_param,'mean_test_score',ax=ax)\n    df_gscv.plot.scatter(\"param_\"+this_param,'mean_test_score',ax=ax, yerr='sterr_test_score',\n                         # include this to get better scaling for small values...\n                         xlim=(0.000,1.1*max(df_gscv[\"param_\"+this_param])))\n    \nplt.show()","f4aa326c":"# Show the test score vs train score with color-code by parameter values (if useful)\nif True:\n    for this_param in param_keys:\n        df_gscv.plot.scatter('mean_train_score','mean_test_score',\n                         c=\"param_\"+this_param, colormap='plasma',\n                         sharex=True, figsize=(15,4), yerr='sterr_test_score')\n\n    plt.show()","bf8f4422":"if model_name in ['lgr','dtc','rfc','gbc','mlp','xgb']:\n    # Plot feature importance\n    # Get feature importance\n    if model_name == 'mlp':\n        # For mlp regressor create a quasi-importance from the weights.\n        # \"The ith element in the list represents the weight matrix corresponding to layer i.\"\n        # Input layer weights\n        ##len(best_regressor.coefs_[0])\n        # sum of abs() of input weights for each feature\n        feature_importance = np.array([sum(np.abs(wgts)) for wgts in best_fit_model.coefs_[0] ])\n    elif model_name == 'lgr':\n        # For Logisitic Regression use the coeff.s to approximate an importance\n        coeffs = best_fit_model.coef_[0]\n        feature_importance = 0.0 * coeffs\n        print(\" Feature        Import.      coeff.    max from mean\")\n        for icol, col in enumerate(X.columns):\n            col_mean = X[col].mean()\n            col_max_from_mean = np.max(np.abs(X[col] - col_mean))\n            feature_importance[icol] = abs(coeffs[icol]\/col_max_from_mean)\n            print(\"{:10}: {:10.3f}, {:10.3f}, {:10.2f}\".format(col, feature_importance[icol], coeffs[icol], col_max_from_mean))\n    else:\n        # tree models have feature importance directly available:\n        feature_importance = best_fit_model.feature_importances_\n        \n    # make importances relative to max importance\n    max_import = feature_importance.max()\n    feature_importance = 100.0 * (feature_importance \/ max_import)\n    sorted_idx = np.argsort(feature_importance)\n    pos = np.arange(sorted_idx.shape[0]) + 0.5\n\n    plt.figure(figsize=(8, 15))\n    ##plt.subplot(1, 2, 2)\n    plt.barh(pos, feature_importance[sorted_idx], align='center')\n    plt.yticks(pos, X.columns[sorted_idx])\n    plt.xlabel(model_name.upper()+' -- Relative Importance')\n    plt.title('           '+model_name.upper()+\n              ' -- Variable Importance                  max --> {:.3f} '.format(max_import))\n\n    plt.savefig(model_name.upper()+\"_importance_\"+version_str+\".png\")\n    plt.show()\n    ","9df2330f":"best_fit_model.get_params()","1b1c2553":"# Make the model probability predictions on the Training and Test (Kaggle) data\n\n# Just use the final best-fit model fit on all training data\nbest_fit_model.fit(X,y)\n\n# Fit the model on all the Training data\nprint(\"\")\nall_train_score = accuracy_score(y, best_fit_model.predict(X))\nprint(\"Nominal best-fit All-Train accuracy: {:.2f} %\\n\".format(100.0*all_train_score))\n\n# The probabilty values, 0 to 1\nyh = best_fit_model.predict_proba(X)\nyh = yh[:,1]\n\n# Make the Kaggle set predictions too\nyh_kag = best_fit_model.predict_proba(Xkag)\nyh_kag = yh_kag[:,1]\n\n# Or ...\nif USE_SPLIT_AVE:\n    # The CV fitting above was done on (k-1)\/k of the data,\n    # do that to evaluate the final model as well.\n    # Use the average of the split-predicted proba values.\n    # (Or use median? any\/much difference?)\n    print(\"Doing (k-1)\/k fits:\")\n    yh = 0.0 * yh\n    yh_kag = 0.0 * yh_kag\n    skf = StratifiedKFold(n_splits=cv_folds, shuffle=False)\n    for train_index, test_index in skf.split(X, y):\n        X_split, X_dummy = X.loc[train_index], X.loc[test_index]\n        y_split, y_dummy = y[train_index], y[test_index]\n        # fit the model on the (k-1)\/k of the data\n        best_fit_model.fit(X_split, y_split)\n        # FYI, accuracy of this model applied to whole dataset\n        print(\"   split --> {}\".format(accuracy_score(y, best_fit_model.predict(X))))\n        # accumulate and average the probabilty values:\n        yh_split = best_fit_model.predict_proba(X)\n        yh += (yh_split[:,1])\/cv_folds\n        yh_kag_split = best_fit_model.predict_proba(Xkag)\n        yh_kag += (yh_kag_split[:,1])\/cv_folds\n\n\n# yh and yh_kag are the model probability predictions.\n# Convert to discrete 0,1 using a threshold:\n#\nyh_threshold = 0.45\n#\n# Training:\nyp = 1.0*(yh > yh_threshold)\n# Test (Kaggle):\nyp_kag = 1.0*(yh_kag > yh_threshold)\n\n\nprint(\"\")\nave_train_score = accuracy_score(y, yp)\nprint(\"Split-Train accuracy: {:.2f} %\\n\".format(100.0*ave_train_score))","c7277c5c":"# Show the yh distribution by known Survival\n\n# Temporarily ... Put the model speed and ys in the X dataframe:\nX['preds'] = yh\nX['Survived'] = y\n\nX.hist('preds', by='Survived', bins=100, sharex=True, sharey=True, layout=(5,1), figsize=(14,9))\nplt.show()\n\n# Remove the added columns:\nX = X.drop(['Survived','preds'],axis=1)","be4e32d9":"# See how the prediction, yh, compares with the known y values:\nroc_area = y_yhat_plots(y, yh, title=\"y and y_score\", y_thresh=yh_threshold,\n                       plots_prefix=model_name.upper()+\"_\"+version_str)","1bdd5226":"# Look at the errors made\n\n# FP: Predicted Survival but did not survive:\nFP_indices = (yh > yh_threshold) & (y < 0.5)\ndf_FPs = df_train[FP_indices]\n\n# FN: Predicted not to Survive but did survive:\nFN_indices = (yh < yh_threshold) & (y > 0.5)\ndf_FNs = df_train[FN_indices]\n\nprint(\"\\n{}: {} FPs and {} FNs\\n\".format(model_name.upper(),len(df_FPs),len(df_FNs)))","5aa42c66":"# List FPs: predicted to Survive, but Perished:    mostly females\n#\norig_cols = df_FPs.columns\ndf_FPs[orig_cols[0:12]]","915403d3":"# List FNs: predicted to Perish, but Survived:    mostly males\n#\norig_cols = df_FNs.columns\ndf_FNs[orig_cols[0:12]]","b4290092":"# Histogram of the model's predicted values\n\nfixed_bins = 0.0 + np.array(range(50+1))\/(5*10.0)\n\nplt.figure(figsize=(15, 6))\nplt.subplot(2, 1, 1)\nplt.hist([yh]+[-1.0]+[5.0], bins=fixed_bins, histtype='stepfilled')\nplt.title(\"Train: Prediction Values\")\nplt.show()","2c3d1071":"# Summarize the fitting results\nif len(gscv_stats) > 0:\n    print(\"\\n\"+model_name.upper()+\n              \":  Train = {:.3f}, {} FPs, {} FNs, AUC={:.3f}   GSCV: Test = {:.3f}, Train = {:.3f}\\n\".format(\n            accuracy_score(y, yp), len(df_FPs), len(df_FNs), roc_area,\n            gscv_stats.loc['max','mean_test_score'],\n            gscv_stats.loc['max','mean_train_score']))\nelse:\n    print(\"\\n\"+model_name.upper()+\n              \":  Train = {:.3f}, {} FPs, {} FNs, AUC={:.3f}\\n\".format(\n            accuracy_score(y, yp), len(df_FPs), len(df_FNs), roc_area))\n\n\n# with Sex_M,F:\n#   LGR: Train = 0.827, 54 FPs, 100 FNs, AUC=0.866   GSCV:Test = 0.818, Train = 0.825\n#   MLP: Train = 0.842, 44 FPs,  97 FNs, AUC=0.885   GSCV:Test = 0.818, Train = 0.837\n#   GBC: Train = 0.862, 38 FPs,  85 FNs, AUC=0.908   GSCV:Test = 0.834, Train = 0.860\n# with Sex_Mr,etc:\n#   LGR: Train = 0.837, 59 FPs, 86 FNs, AUC=0.879   GSCV:Test = 0.828, Train = 0.838\n#   SVC: Train = 0.843, 77 FPs, 63 FNs, AUC=0.903   GSCV:Test = 0.819, Train = 0.839\n#   MLP: Train = 0.848, 37 FPs, 98 FNs, AUC=0.896   GSCV:Test = 0.830, Train = 0.849\n#        Train = 0.846, 38 FPs, 99 FNs, AUC=0.899   GSCV:Test = 0.828, Train = 0.847\n#   GBC: Train = 0.862, 42 FPs, 81 FNs, AUC=0.917   GSCV:Test = 0.835, Train = 0.862\n#        Train = 0.865, 42 FPs, 78 FNs, AUC=0.918   GSCV:Test = 0.841, Train = 0.865\n#        Train = 0.865, 40 FPs, 80 FNs, AUC=0.919   GSCV:Test = 0.844, Train = 0.865\n#\n# with LN_Length,LN_Vfrac (not LN_Vowels):\n#   LGR:  Train = 0.835, 67 FPs, 80 FNs, AUC=0.881   GSCV:Test = 0.832, Train = 0.841\n#   SVC:  Train = 0.851, 72 FPs, 61 FNs, AUC=0.920   GSCV:Test = 0.819, Train = 0.851\n#   MLP:  Train = 0.860, 37 FPs, 88 FNs, AUC=0.901   GSCV:Test = 0.837, Train = 0.855\n#   MLP:  Train = 0.852, 33 FPs, 99 FNs, AUC=0.904   GSCV:Test = 0.836, Train = 0.855\n#   GBC:  Train = 0.870, 39 FPs, 77 FNs, AUC=0.926   GSCV:Test = 0.834, Train = 0.864\n#   GBC:  Train = 0.870, 39 FPs, 77 FNs, AUC=0.926   GSCV:Test = 0.836, Train = 0.866\n#   GBC:  Train = 0.860, 43 FPs, 82 FNs, AUC=0.923   GSCV:Test = 0.840, Train = 0.867\n\n# but no Fare_log:\n#   GBC:  Train = 0.861, 35 FPs, 89 FNs, AUC=0.915   GSCV:Test = 0.827, Train = 0.860\n\n# Using XGB\n# (v32)  XGB:  Train = 0.909, 34 FPs, 47 FNs, AUC=0.958   GSCV: Test = 0.844, Train = 0.966\n# (v34)  XGB:  Train = 0.866, 46 FPs, 73 FNs, AUC=0.921   GSCV: Test = 0.828, Train = 0.872  ","903ef635":"# yh_kag, yp_kag were calculated above when yh,yp were evaluated.","8643a010":"# Kaggle prediction probs\nplt.figure(figsize=(15, 6))\nplt.subplot(2, 1, 1)\nplt.hist([yh_kag]+[-1.0]+[5.0], bins=fixed_bins, histtype='stepfilled')\nplt.title(\"Kaggle: Prediction Values\")\nplt.show()","5d3d1bad":"# Put the 0,1 predictions into the original df_test which is the Kaggle test data\ndf_test['Survived'] = yp_kag.astype(int)","5ac73ef8":"# Any -1 s remaining for answers?\nall_answered = (df_test.Survived.min() >= 0)\nprint(\"All predictions made?  {}\".format(all_answered))","f7ab489d":"# Save the result as the submission\ndf_test[['PassengerId','Survived']].to_csv(\"submission.csv\",index=False)","8952adf0":"# that's all.\n!head -10 submission.csv","e9e97d3c":"!tail -10 submission.csv","13081d33":"## <a id=\"DataProcessing\">Reading and Processing csv Data Files<\/a>\nBack to <a href=\"#Index\">Index<\/a>","9e3b40f8":"### Finished making features","1731aaf7":"### Show scatter plots of some values","f1cd0215":"## <a id=\"EvaluateBest\">Evaluate Best Model<\/a>\nBack to <a href=\"#Index\">Index<\/a>","bc36dd77":"### Some routines","74ed5d44":"## <a id=\"ConfusionDots\">Confusion Dots and ROC plots<\/a>\nBack to <a href=\"#Index\">Index<\/a>\n\nUse the y_yhat_plots() routine to shown how the prediction is doing with a Confusion Dots plot (a visual confusion matrix) and an ROC plot.<br>\nThis routine was taken from the file chirp_roc_lib.py in the github repo at: \nhttps:\/\/github.com\/dan3dewey\/chirp-to-ROC <br>\nand is included in this notebook at the beginning of the <a href=\"#MachineLearning\">Machine Learning<\/a> section.","2b73d146":"# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n### Continue with the Hyper-Parameter Search that follows\n#### Or skip the whole Hyper-Parameter section, go to <a href=\"#FeatureImportance\">Feature Importance<\/a> <br>\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -","5695f6d9":"## <a id=\"TheEnd\">The End<\/a>\nBack to <a href=\"#Index\">Index<\/a>","78bb12b5":"### Select the Features to use","b767136d":"## <a id=\"SetupData\">Features and the X,y Data<\/a>\nBack to <a href=\"#Index\">Index<\/a>","2c41b3f0":"## <a id=\"Index\">Index<\/a>\nDiary<br>\n<a href=\"#DataProcessing\">Reading and Processing csv Data Files<\/a><br>\n<a href=\"#NameFeats\">Features from Name<\/a><br>\n<a href=\"#FeatureSummary\">Summary of the Features<\/a><br>\n<br>\n<a href=\"#MachineLearning\">Machine Learning<\/a><br>\n<a href=\"#SetupData\">Select Features and get X,y Data<\/a><br>\n<br>\n<a href=\"#DefineModels\">Define the Models and Parameters<\/a><br>\n<br>\n<a href=\"#HyperSearch\">Do the Hyper-Parameter Search<\/a><br>\n<a href=\"#CVResults\">Grid Search CV Results<\/a><br>\n<br>\n<a href=\"#FeatureImportance\">Feature Importance<\/a><br>\n<a href=\"#EvaluateBest\">Evaluate Best Model<\/a><br>\n<b><a href=\"#ConfusionDots\">Confusion Dots and ROC plots<\/a><\/b><br>\n<br>\n<a href=\"#OutputKaggle\">Output Kaggle Predictions<\/a><br>\n<a href=\"#TheEnd\">The End<\/a><br>\n<br>","3dcea486":"## <a id=\"HyperSearch\">Do the Hyper-Parameter Search<\/a>\nBack to <a href=\"#Index\">Index<\/a> <br>\n\nScan various combinations, or <br>\nJust do single\/small numbers of values of the parameters.","47e342c7":"## <a id=\"OutputKaggle\">Output Kaggle Predictions<\/a>\nBack to <a href=\"#Index\">Index<\/a>","a99c5d0c":"## <a id=\"FeatureSummary\">Summary of the Features<\/a>\nBack to <a href=\"#Index\">Index<\/a>","bce06153":"### Check for any NaNs in the Train,Test data","d2283953":"## Diary\n(v1) LB=0.7655 Training=0.7867  Using the gender_submission file.<br>\n(v2) LB=0.7608 Training=0.7935  Using Logistic Regression. <br>\n(v3) LB=0.7703 Training=0.8002  LGR without Age and Fare. <br>\nCreate 'one hot' versions of Sex: Sex_M and Sex_F (LR uses similar coef.s anyway); create a Fare_0 flag and set a Pclass-based nominal Fare for these; include Fare_log back in fit. LR C=10 is over-fit direction, C=0.1 underfit.<br>\n(v4) LB=0.7512 Training=0.7901  LGR: with Fare_0, Fare_log, Sex_M\/F, no Age, C=1.0. <br>\n(v5) LB=0.7608 Training=0.7935  LGR: with Fare_0, Fare_log, Sex_M\/F, no Age, change to C=0.1. <br>\nAdd Age_log and Age_young, Age_old...<br>\n(v6) LB=0.7703 Training=0.8080  LGR: with Age_log,young,old, Fare_0, Fare_log, Sex_M\/F; C=0.07 . <br>\nAdjust the ReLU levels for Age_young,old and add one-hot values of Embarked. Kind of the end for Logistic Regression, try different C values (in C order):<br>\n(v11) LB=0.7608 Training=0.8148  LGR: with Age_log,young,old, Fare_0,log, Sex_M\/F, Embark_C,Q,S; C=2.0 .<br>\n(v7) LB=0.7608 Training=0.8137  LGR: with Age_log,young,old, Fare_0,log, Sex_M\/F, Embark_C,Q,S; C=0.30 .<br>\n(v8) LB=0.7655 Training=0.8126  LGR: with Age_log,young,old, Fare_0,log, Sex_M\/F, Embark_C,Q,S; C=0.07 .<br>\n(v9) LB=0.7655 Training=0.8159  LGR: with Age_log,young,old, Fare_0,log, Sex_M\/F, Embark_C,Q,S; C=0.02 .<br>\n(v10) LB=0.7368 Training=0.7867  LGR: with Age_log,young,old, Fare_0,log, Sex_M\/F, Embark_C,Q,S; C=0.010 .<br>\nAdd in the Decision Tree Classifier and try some parameters for it:<br>\n(v12) LB=0.7655 Training=0.8373  **DTC**: with max_depth 5, min_samples_leaf 10, min_impurity_decrease 0.001.<br>\nNow with many hyper parameters, need to do Grid Search CV'ing to find the best hyper-parameters for the chosen model...<br>\nBorrow GSCV code from [my simple PetFinder kernel](https:\/\/www.kaggle.com\/dan3dewey\/simple-scikit-models-and-stacking)\n . <br>\nFind best Logistic Regression C parameter:<br>\n(v13) LB=0.7608 Training=0.8204, max-CV=0.8138(10-fold)  **LRG**: use 10 folds and find C ~ 0.04 gives best CV. <br>\nFind the best Random Forest Classifier parameters: <br>\n(v14) LB=0.7799 Training=0.8272,8383,8462, 75%-CV=0.8208,8230,8191(10-fold)  **RFC**: use 10 folds; depth=5, min_samp=2 seems best. <br>\n18 Apr 2019: Scale X (and Xkag) so that the X features have mean=0, std=1 (shouldn't affect the previous results, doing it toward trying SVC and MLP.) Replace Pclass with Pclass_1,2,3; limit Parch and SibSp to a max of 3. Make lists of FPs and FNs. v15A.<br>\nInclude and find good parameters for the Gradient Boosting Classifier:<br>\n(v15) LB=0.7560 Training ~ 0.910, 75%-CV ~ 0.830(10-fold)  **GBC**: params 0.003-5-10-0.80 <br>\n(v16) LB=0.7847 Training ~ 0.847, 75%-CV ~ 0.820(10-fold)  **RFC**: params 5-0.001-2 <br>\nReduce max_depth to 4 for the GBC and use larger min values:<br>\n(v17) LB=0.7895 Training ~ 0.844, 75%-CV ~ 0.826(10-fold)  **GBC**: max_depth=4, 0.003-20-85-0.80 <br>\n19 Apr 2019: Remove the Fare_log feature from v17: Train reduced a bit, but GSCV Test and Train are closer: <br>\n(v18) LB=0.7895 Training ~ 0.841, 75%-CV ~ 0.830(10-fold)  **GBC**: No Fare, max_depth=4, 0.003-20-85-0.80 <br>\nUse the average of the (k-1)\/k split fit predictions:<br>\n(v19) LB=0.7847 Training ~ 0.845, 75%-CV ~ 0.826(10-fold)  **GBC**: Split-ave, No Fare, depth=4, 0.003-20-85-0.80 <br>\nPut the Fare_log back:<br>\n(v20) LB=0.7895 Training ~ 0.855, 75%-CV ~ 0.826(10-fold)  **GBC**: Split-ave, w\/Fare_log, depth=4, 0.003-20-85-0.80 <br>\nTry SVC models too<br>\n(v21) LB=0.7703 Training ~ 0.840, 75%-CV ~ 0.831(10-fold) **SVC**: Split-ave, w\/Fare_log, poly(3),2.5, 0.030<br>\n(v22) LB=0.7655 Training ~ 0.826, 75%-CV ~ 0.813(10-fold) **SVC**: Split-ave, w\/Fare_log, rbf,  C=3.5, 0.025<br>\nAnd try MLP models...<br>\n(v23) LB=0.7560 Training ~ 0.836, 75%-CV ~ 0.821(10-fold) **MLP**: no Fare, (14,10,8) alpha,rate,momen.: 0.60-0.09-0.50 <br>\nOK, roll the dice one more time...<br>\n(v24) LB=0.7799 Training ~ 0.843, 75%-CV ~ 0.826(10-fold)  **GBC**: No Fare, max_depth=4, 0.003-20-85-0.80 <br>\n(v25) LB=0.7799 Training ~ 0.848, 75%-CV ~ 0.829(10-fold)  **GBC**: No Fare, max_depth=**5**, 0.003-20-85-0.80 <br>\nOK, only thing left to do is to see if the Name field can provide any useful 'signal'... Put that on back burner ;-) <br>\n(v26) LB=0.7799 Training ~ 0.86.5, 75%-CV ~ 0.83.2(10-fold)  **GBC**: with Fare, max_depth=**5**, 0.003-20-85-0.80 <br>\nTry to re-do v20:<br>\n(v27) LB=0.7847 Training ~ 0.862, 75%-CV ~ 0.829(10-fold)  **GBC**: Split-ave, w\/Fare_log, depth=4, 0.003-20-85-0.80 <br>\n21 Apr 2019: Added \"violin plots\" for survival vs Age (or Fare_log) and Pclass; changed the NaN age from median to random in range 18 to 40 and\/but set a \"NoAge\" flag for these - very low feature importance.<br>\nSee if I can get something out of the Name field: Mr Master Mrs or Miss ?<br>\nAdded one-hot features: Sex_Mr,Mrs,Miss,Master; redo v27 with these in place of Sex_M,F:<br>\n(v28) **LB=0.7942** Training ~ 0.864, 75%-CV ~ 0.829(10-fold)  **GBC**: Split-ave, w\/Fare_log, depth=4, 0.003-20-85-0.80 <br>\nSee how the MLP is doing:<br>\n(v29) LB=0.7703 Training ~ 0.848, 75%-CV ~ 0.827(10-fold)  **MLP**: Split-ave, w\/Fare_log, (14,10,8) alpha,rate,momen.: 0.60-0.09-0.50 <br>\n24 Apr 2019: Add\/use two simple LastName-based features: the number of characters and the fraction of vowels in the last name...<br>\n(v30) LB=0.7895 Training ~ 0.861, 75%-CV ~ 0.829(10-fold)  **GBC**: Split-ave, w\/Fare_log, depth=4, 0.003-20-85-0.80 <br>\n(v31) LB=0.7416 Training ~ 0.852, 75%-CV ~ 0.828(10-fold)  **MLP**: Split-ave, w\/Fare_log, (14,10,8) alpha,rate,momen.: 0.60-0.09-0.50 <br>\nAdd XGBClassifier to the models... Similar to GBC?<br>\n(v32) LB=0.7656 Training ~ 0.909, 75%-CV ~ 0.833(10-fold)  **XGB**: Split-ave, w\/Fare_log, depth=8, lrn-rate=0.08, etc.<br> \nUse lower depth:<br>\n(v33) LB=0.7560 Training ~ 0.866, 75%-CV ~ 0.827(10-fold)  **XGB**: Split-ave, w\/Fare_log, depth=4, lrn-rate=0.08, etc.<br>\nFix params and use only n_estimators=75:<br>\n(v34) LB=0.79425 Training ~ 0.863, 75%-CV ~ 0.828(10-fold)  **XGB**: Split-ave, w\/Fare_log, depth=4, lrn-rate=0.08, etc.<br>\nThe 75 in v34 was chosen by looking at the error vs n_estimators and picking a value where the performance starts to plateau. <br>\nThere is a lot more to learn about using XGB, e.g., see: [Avoid Overfitting ... XGBoost ...](https:\/\/machinelearningmastery.com\/avoid-overfitting-by-early-stopping-with-xgboost-in-python\/) .<br>\nFor fun and a nice plot, re-run v34 with several values of learning_rate and many in n_estimators.\nSince the runs are random (the random seed is not fixed) not sure which learning_rate will be the best, let's see:<br>\n(v35) LB=0.---- Training ~ 0.864, 75%-CV ~ 0.828(10-fold)  **XGB**: Split-ave, w\/Fare_log, depth=4, lrn-rate=0.01,0.03,0.08, etc.<br>\n","d8397b2b":"## <a id=\"CVResults\">Grid Search CV Results<\/a>\nBack to <a href=\"#Index\">Index<\/a>","5f081755":"# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n###  Done with <a href=\"#HyperSearch\">Hyper-parameters<\/a> above\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -","aa160dcc":"## Titanic with the Confusion-Dots plot\n\nTry some of the SciKit classifier models on the Titanic data.<br>\nDemonstrate the **Confusion Dots plot** which is essentially a visual confusion matrix;<br>\nthe confusion-dots routine, `y_yhat_plots()`, was taken from the file chirp_roc_lib.py in the github repo at:<br> \nhttps:\/\/github.com\/dan3dewey\/chirp-to-ROC","8f58e89a":"### Get the X,y data","8d6452ef":"### Best model found by GS CV","99e5e73e":"### Fit the Model with Nominal Hyper-Parameters","2ab88ab7":"## <a id=\"DefineModels\">Define the Models and Parameters<\/a>\nBack to <a href=\"#Index\">Index<\/a>\n\nDefine the different models available and their parameters and GSCV parameters.","badfc317":"### Compare Train and Test Averages","a80bad1f":"## <a id=\"FeatureImportance\">Feature Importance<\/a>\nBack to <a href=\"#Index\">Index<\/a>","5a2600e7":"## <a id=\"MachineLearning\">Machine Learning<\/a>\nBack to <a href=\"#Index\">Index<\/a>","9d4e6a4c":"## Correlation with Survived","db7b1c8b":"## <a id=\"NameFeats\">Features from Name<\/a>\nBack to <a href=\"#Index\">Index<\/a>"}}