{"cell_type":{"01a66cf0":"code","e92f7621":"code","f79ebf09":"code","af668628":"code","757b3d63":"code","3d22c97b":"code","26355daf":"code","0990db51":"code","4646f80f":"code","a4a3cfc9":"code","19de3e1c":"code","d99af9a1":"code","edd594d5":"code","324584d7":"code","b5a99ccc":"code","f0bb8b0b":"code","34057802":"code","67764df4":"markdown","df270ea5":"markdown"},"source":{"01a66cf0":"import pandas as pd\nimport numpy as np\n\nimport gc\n\nclass Config:\n    data_dir = ''\n    seed = 42\n    \nrs = Config.seed\n\ndef rmspe(y_true, y_pred):\n    return  -(np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))","e92f7621":"train_data_set = pd.read_hdf('..\/input\/optiver-transformed-data\/new_train.hdf5')","f79ebf09":"train_data_set","af668628":"x = gc.collect()","757b3d63":"X_display = train_data_set.drop(['time_id', 'target', 'weights'], axis = 1)\nX = X_display.values.astype(np.float64)\ny = train_data_set['target'].values.astype(np.float64)\n\nX.shape, y.shape","3d22c97b":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, stratify=X[:, 0], shuffle=True)","26355daf":"from sklearn.preprocessing import RobustScaler, MinMaxScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nsc_rs = RobustScaler(with_centering=True)\nsc_mm = MinMaxScaler()\npoly = PolynomialFeatures(degree=2)\n\nsc = Pipeline([\n    #('poly',poly),\n    ('RobustScaler', sc_rs),\n    ('MinMax', sc_mm)\n])\n\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\n\n# from joblib import dump\n# dump(sc, 'Temp\/full_pipe.joblib')","0990db51":"X.shape","4646f80f":"del X, y, train_data_set\n_ = gc.collect()","a4a3cfc9":"!pip install tensorflow-addons","19de3e1c":"import tensorflow_addons as tfa\nimport tensorflow as tf\nfrom sklearn.model_selection import KFold\nimport tensorflow.keras.backend as K\n\ndef rmspe_k(y_true, y_pred):\n    pct_var = (y_true - y_pred) \/ y_true\n    return K.sqrt(K.mean(K.square(pct_var)))\n\ndef make_layer(x, units, dropout_rate):\n    t = tfa.layers.WeightNormalization(tf.keras.layers.Dense(units))(x)\n    t = tf.keras.layers.Dense(units, tf.keras.activations.swish)(x)\n    t = tf.keras.layers.BatchNormalization()(t)\n    t = tf.keras.layers.Dropout(dropout_rate)(t)\n    return t\n\ndef make_model(data, units, dropout_rates):\n    \n    inputs = tf.keras.layers.Input(shape=(data.shape[1],))\n    x = tf.keras.layers.BatchNormalization()(inputs)\n\n    for i in range(len(units)):\n        u = units[i]\n        d = dropout_rates[i]\n        x = make_layer(x, u, d)\n       \n    y = tf.keras.layers.Dense(1, 'linear', name='dense_output')(x)\n    \n    model = tf.keras.Model(inputs=inputs, outputs=y)\n    model.compile(loss='mse',\n                  optimizer='adam',\n                  metrics=[rmspe_k])\n    return model\n\ndef fit_predict(n_splits, x_train, y_train, units, dropout_rates, epochs, x_test, y_test, verbose, random_state):\n\n    histories = []\n    scores = []\n    y_preds = []\n\n    cv = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n    for train_idx, valid_idx in cv.split(x_train, y_train):\n\n        x_train_train = x_train[train_idx]\n        y_train_train = y_train[train_idx]\n        x_train_valid = x_train[valid_idx]\n        y_train_valid = y_train[valid_idx]\n                \n        K.clear_session()\n\n        estimator = make_model(x_train, units, dropout_rates)\n\n        es = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=2e-5, patience=5,\n                                              verbose=verbose, mode='min', restore_best_weights=True)\n\n        rl = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.7, patience=3,\n                                                  mode='min', verbose=verbose)\n\n        history = estimator.fit(x_train_train, y_train_train,\n                                batch_size=2048, epochs=epochs, callbacks=[es, rl],\n                                validation_data=(x_train_valid, y_train_valid), shuffle=False,\n                                verbose=verbose)\n        \n        if x_test is not None:\n            y_part = estimator.predict(x_test)\n            y_preds.append(y_part)\n\n        histories.append(history)\n        scores.append(history.history['val_rmspe_k'][-1])\n    \n    if x_test is not None:\n        y_pred = np.mean(y_preds, axis=0)\n    else:\n        y_pred = None\n\n    score = np.mean(scores)\n    \n    return y_pred, histories, score\n","d99af9a1":"import optuna\ndef objective(trial):\n    \n    n_layers = trial.suggest_int('n_layers', 1, 4)\n    \n    units = []\n    dropout_rates = []\n    \n    for i in range(n_layers):\n        u = trial.suggest_categorical('units_{}'.format(i+1), [1024, 512, 256, 128])\n        units.append(u)\n        r = trial.suggest_loguniform('dropout_rate_{}'.format(i+1), 0.1, 0.5)\n        dropout_rates.append(r)\n    \n    print('Units:', units, \"Dropout rates:\", dropout_rates, \"Layers:\", n_layers)\n    \n    _, _, score = fit_predict(10, X_train, y_train, units, dropout_rates, 50, X_test, y_test, 0, 42)\n    return score","edd594d5":"study = optuna.create_study(direction='minimize',\n                            #sampler=optuna.samplers.TPESampler(),\n                            sampler=optuna.samplers.RandomSampler(seed=rs),\n                            study_name='Optuna_NN')","324584d7":"x = gc.collect()","b5a99ccc":"%%time\nstudy.optimize(objective,\n               timeout=3600*7.5,\n               gc_after_trial=True)","f0bb8b0b":"print('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","34057802":"optuna.visualization.plot_optimization_history(study)","67764df4":"# Optuna Tuning","df270ea5":"# Load Data"}}