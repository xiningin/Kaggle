{"cell_type":{"babbeab4":"code","1b2d1fb8":"code","8f059a1f":"code","083ab692":"code","e07a5be2":"code","09044186":"code","184bed5c":"code","5447b9bf":"code","1bf813b6":"code","6eb6a54f":"code","12883dec":"code","8c130429":"code","0389feeb":"code","8b20d4fc":"code","0c510200":"code","8d9bd129":"code","38206104":"code","0038bdce":"code","8ae22bad":"code","5e066969":"code","38faf2ca":"code","87ae36ee":"code","830304c7":"code","60c49054":"code","19beb4c5":"code","7fc461b6":"code","bdc58d21":"code","8e953dad":"code","a1172125":"code","4fd46224":"code","9393e03e":"code","be91c3da":"code","775f8b10":"code","b7af3d93":"code","a4664e04":"code","7c734f8a":"code","74a13058":"code","9d7c720e":"code","182fa92f":"code","dd74359e":"code","aaf6e648":"code","45413b18":"code","ff36f33c":"code","6e57383f":"code","28f77032":"code","162bcd35":"code","313361b3":"markdown","37d92046":"markdown","30e5ed49":"markdown","432df51e":"markdown","07b70516":"markdown","7fd3bdc1":"markdown","3f7bc666":"markdown","36e1a005":"markdown","acc0da57":"markdown","4f4d2495":"markdown","4aace8c4":"markdown","ee0e41a4":"markdown","e541fd35":"markdown","19c71a8f":"markdown","2994e619":"markdown","94bc2c29":"markdown","834d49f0":"markdown","b51c168e":"markdown","6f72a2b8":"markdown","fb3e3e7a":"markdown","ea1c67e2":"markdown","c91bec00":"markdown","41bf3f58":"markdown"},"source":{"babbeab4":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport numpy as np\nfrom logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import StratifiedKFold, GroupKFold\nfrom sklearn.metrics import cohen_kappa_score\nfrom scipy.stats import mode\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import mode\n\n#modeling\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\n\n#LGB imports\nimport lightgbm as lgb","1b2d1fb8":"%%time\n\n#read input files\n#ignoring the event data column - JSON data. Also reduces the Memory usage.\nwanted_cols = ['event_id', 'game_session', 'installation_id', 'type', 'world','timestamp','event_count', 'event_code','title' ,'game_time']\ntrain_df = pd.read_csv(\"..\/input\/data-science-bowl-2019\/train.csv\", usecols = wanted_cols)\ntest_df = pd.read_csv(\"..\/input\/data-science-bowl-2019\/test.csv\", usecols = wanted_cols)\ntrain_labels_df =pd.read_csv('..\/input\/data-science-bowl-2019\/train_labels.csv')\nsubmission_df = pd.read_csv(\"..\/input\/data-science-bowl-2019\/sample_submission.csv\")\n\nspecs_df = pd.read_csv(\"..\/input\/data-science-bowl-2019\/specs.csv\") ","8f059a1f":"def get_logger(filename='log'):\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\nlogger = get_logger()\n\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: \n        logger.info('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","083ab692":"#reducing memory usage for traning and testing data\n\ntrain_df = reduce_mem_usage(train_df)\ntest_df = reduce_mem_usage(test_df)\nspecs_df = reduce_mem_usage(specs_df)","e07a5be2":"#peaking into the data\ntrain_df.head()","09044186":"test_df.head()","184bed5c":"train_labels_df.head()","5447b9bf":"#looking at the size of the data\n\ntrain_df.shape, test_df.shape, train_labels_df.shape","1bf813b6":"#checking for any missing values in training data\n\ntrain_df.isna().sum()","6eb6a54f":"#missing values in train labels data\n\ntrain_labels_df.isna().sum()","12883dec":"#set the plotting style\nplt.style.use(\"seaborn\")","8c130429":"#looking at the title from the train labels data\n\ntrain_labels_df[\"title\"].value_counts().plot(kind = \"barh\")\nplt.title(\"Type of Assessment\")\nplt.show()","0389feeb":"#looking at the labels\n\ntrain_labels_df[\"accuracy_group\"].value_counts().plot(kind = \"barh\")\nplt.ylabel(\"accuracy group\")\nplt.show()","8b20d4fc":"#we will look at the relation between title and accuracy group\n\nplt.figure(figsize=(15,10))\n\nsns.countplot(x = \"title\", hue = \"accuracy_group\", data = train_labels_df, orient = \"h\")\nplt.title(\"Accuracy_group vs Title\")\nplt.show()","0c510200":"#check the distribution of installation_id\n\nsns.distplot(train_labels_df[\"installation_id\"].value_counts().values)\nplt.show()","8d9bd129":"train_df.info()","38206104":"#looking at the Specs data\n\nspecs_df.head()","0038bdce":"#missing values in specs_df\n\nspecs_df.isna().sum()","8ae22bad":"temp_df = train_df.groupby([\"installation_id\", \"type\"]).agg({'game_session': 'count'}).reset_index()\ntemp_pivot_df = temp_df.pivot(index = \"installation_id\", columns = \"type\", values = \"game_session\").reset_index()\ntemp_pivot_df.head()","5e066969":"useful_installation_id = temp_pivot_df[~temp_pivot_df[\"Assessment\"].isna()][\"installation_id\"].values\n\n#filter out the data based on the useful_installation_id.\nreduced_train_df = train_df[train_df[\"installation_id\"].isin(useful_installation_id)]\n\n#get the unique installation id from train labels data\ntrain_labels_installation_ids = train_labels_df[\"installation_id\"].unique()\n\n#further reducing the data based on the unique installation id from train labels data.\nreduced_train_df = reduced_train_df[reduced_train_df[\"installation_id\"].isin(train_labels_installation_ids)].reset_index(drop = True)\n\nreduced_train_df.shape","38faf2ca":"reduced_train_df.head()","87ae36ee":"#extracting the month, hour, year, day of the week.\n\ndef extract_time_info(df):\n    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"]) #convert the variable to datetime format.\n    df[\"day\"] = df[\"timestamp\"].dt.day\n    df[\"month\"] = df[\"timestamp\"].dt.month\n    df[\"hour\"] = df[\"timestamp\"].dt.hour\n    df[\"minute\"] = df[\"timestamp\"].dt.minute\n    df[\"weekday_name\"] = df[\"timestamp\"].dt.weekday_name\n    \n    #If weekday is sunday then set boolean to 1 else 0.\n    df[\"is_sunday\"] = df[\"weekday_name\"].apply(lambda x: 1 if x == \"Sunday\" else 0)\n    \n    #drop the column\n    df = df.drop([\"weekday_name\", \"timestamp\"], axis = 1)\n    \n    return df","830304c7":"time_features=['month','hour','day','minute','is_sunday']\n\ndef prepare_data(df):\n    \n    #extract time features\n    df=extract_time_info(df)\n    \n    #generate dummies for event_code and perform groupby based on installation_id and game_session\n    join_one=pd.get_dummies(df[['event_code','installation_id','game_session']], columns=['event_code']).groupby(['installation_id','game_session'],\n                                                            as_index=False,sort=False).agg(sum)\n\n    #define aggregation for columns\n    agg={'event_count':sum,'game_time':['sum','mean'],'event_id':'count'}\n    \n    #group by installation_id and game_session and perform agg\n    join_two=df.drop(time_features,axis=1).groupby(['installation_id','game_session'],as_index=False,sort=False).agg(agg)\n    join_two.columns= [' '.join(col).strip() for col in join_two.columns.values]\n    \n    #fetch the first instance of the data points\n    join_three=df[['installation_id','game_session','type','world','title']].groupby(['installation_id','game_session'],as_index=False,sort=False).first()\n    \n    join_four=df[time_features+['installation_id','game_session']].groupby(['installation_id','game_session'],as_index=False,sort=False).agg(mode)[time_features].applymap(lambda x: x.mode[0])\n    \n    join_one=join_one.join(join_four)\n    \n    #final data\n    join_five=(join_one.join(join_two.drop(['installation_id','game_session'],axis=1))).join(join_three.drop(['installation_id','game_session'],axis=1))\n    \n    return join_five","60c49054":"#create aggregate train df and change the data type\n\nagg_train_df = prepare_data(reduced_train_df)\ncols=agg_train_df.columns.to_list()[2:-3]\nagg_train_df[cols]=agg_train_df[cols].astype('int16')\n\nagg_train_df.head()","19beb4c5":"#create aggregate test df and change the data type\n\nagg_test_df = prepare_data(test_df)\ncols=agg_test_df.columns.to_list()[2:-3]\nagg_test_df[cols]=agg_test_df[cols].astype('int16')\n\nagg_test_df.head()","7fc461b6":"cols=agg_test_df.columns[2:-12].to_list()\ncols.append('event_id count')\ncols.append('installation_id')","bdc58d21":"#group test data by installation_id\n\ndf=agg_test_df[['event_count sum','game_time mean','game_time sum','installation_id']].groupby('installation_id',as_index=False,sort=False).agg('mean')\n\ndf_two=agg_test_df[cols].groupby('installation_id',as_index=False,sort=False).agg('sum').drop('installation_id',axis=1)\n\ndf_three=agg_test_df[['title','type','world','installation_id']].groupby('installation_id',\n         as_index=False,sort=False).last().drop('installation_id',axis=1)\n\ndf_four=agg_test_df[time_features+['installation_id']].groupby('installation_id',as_index=False,sort=False). \\\n        agg(mode)[time_features].applymap(lambda x : x.mode[0])\n","8e953dad":"#merge with train labels for accuracy_group\n\nfinal_train_df=pd.merge(train_labels_df[['installation_id','game_session','accuracy_group']],agg_train_df,on=['installation_id','game_session'],how='left').drop(['game_session', \"installation_id\"],axis=1)\nfinal_train_df.head()","a1172125":"final_train_df.shape","4fd46224":"#creating final test df\n\nfinal_test_df = (df.join(df_two)).join(df_three.join(df_four)).drop('installation_id',axis=1)\nfinal_test_df.head()","9393e03e":"final_df = pd.concat([final_train_df,final_test_df])\nencoding=['type','world','title']\nfor col in encoding:\n    lb=LabelEncoder()\n    lb.fit(final_df[col])\n    final_df[col]=lb.transform(final_df[col])\n    \nfinal_train_df = final_df[:len(final_train_df)]\nfinal_test_df = final_df[len(final_train_df):]","be91c3da":"#drop the accuracy group\n\nfinal_test_df.drop(\"accuracy_group\", axis = 1, inplace = True)","775f8b10":"X_train=final_train_df.drop('accuracy_group',axis=1)\ny_train=final_train_df['accuracy_group']","b7af3d93":"from numba import jit \n\n@jit\ndef qwk3(a1, a2, max_rat=3):\n    assert(len(a1) == len(a2))\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n    e = e \/ a1.shape[0]\n    return 1 - o \/ e","a4664e04":"#define the parameters for lgbm.\n\nSEED = 42\nN_FOLD = 10\nparams = {\n    'min_child_weight': 10.0,\n    'objective': 'multi:softprob',\n    'max_depth': 7,\n    'max_delta_step': 1.8,\n    'colsample_bytree': 0.4,\n    'subsample': 0.5,\n    'num_class':4,\n    'learning_rate':0.05,\n    'n_estimators':2000,\n    'eta': 0.025,\n    'gamma': 0.65,\n    'eval_metric':'mlogloss'\n    }\n\nfeatures = [i for i in final_train_df.columns if i not in ['accuracy_group']]","7c734f8a":"def model(train_X,train_Y, test, params, n_splits=N_FOLD):\n    \n    #define KFold Strategy\n    folds = StratifiedKFold(n_splits=N_FOLD,shuffle=True, random_state=SEED)\n    scores = []\n    \n    #out of the fold \n    y_pre = np.zeros((len(test),4), dtype=float)\n    target = [\"accuracy_group\"]\n    #print(\"done\")\n    \n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(train_X, train_Y)):\n        print(\"------------------------ fold {} -------------------------\".format(fold_ + 1))\n        \n        X_train, X_valid = train_X.iloc[trn_idx], train_X.iloc[val_idx]\n        y_train, y_valid = train_Y.iloc[trn_idx], train_Y.iloc[val_idx]\n        \n        # Convert our data into XGBoost format\n        d_train = xgb.DMatrix(X_train, y_train)\n        d_valid = xgb.DMatrix(X_valid, y_valid)\n        \n        xgb_model = xgb.train(params,\n                      d_train,\n                      num_boost_round=1600,\n                      evals=[(d_train, 'train'), (d_valid, 'val')],\n                      verbose_eval=False,\n                      early_stopping_rounds=70\n                     )\n        \n        d_val = xgb.DMatrix(X_valid)\n        pred_val = [np.argmax(x) for x in xgb_model.predict(d_val)]\n        \n        #calculate cohen kappa score\n        score = cohen_kappa_score(pred_val,y_valid,weights='quadratic')\n        scores.append(score)\n\n        pred = xgb_model.predict(xgb.DMatrix(test))\n        #save predictions\n        y_pre += pred\n        \n        print(f'Fold: {fold_+1} quadratic weighted kappa score: {np.round(score,4)}')\n\n    pred = np.asarray([np.argmax(line) for line in pred])\n    print('Mean choen_kappa_score:',np.round(np.mean(scores),6))\n    \n    return xgb_model,pred","74a13058":"xgb_model,pred = model(X_train,y_train,final_test_df,params)","9d7c720e":"pred[1:10]","182fa92f":"sub=pd.DataFrame({'installation_id':submission_df.installation_id,'accuracy_group':pred})\nsub.to_csv('submission.csv',index=False)","dd74359e":"sub.accuracy_group.value_counts()","aaf6e648":"#Classic feature attributions\nax = xgb.plot_importance(xgb_model, title='Feature importance (Weight)', importance_type='weight')\nax.figure.set_size_inches(10,8)\n\nplt.show()","45413b18":"#Classic feature attributions\nax = xgb.plot_importance(xgb_model, title='Feature importance (Cover)', importance_type='cover')\nax.figure.set_size_inches(10,8)\n\nplt.show()","ff36f33c":"#Classic feature attributions\nax = xgb.plot_importance(xgb_model, title='Feature importance (Gain)', importance_type='gain')\nax.figure.set_size_inches(10,8)\n\nplt.show()","6e57383f":"import shap","28f77032":"explainer = shap.TreeExplainer(xgb_model)\nshap_values = explainer.shap_values(X_train.values)","162bcd35":"shap.summary_plot(shap_values, X_train)","313361b3":"> **Xgboost Feature Importance has three options**.\n\n1. **Weight**: The number of times a feature is used to split the data across all trees (default).\n2. **Cover**: The number of times a feature is used to split the data across all trees weighted by the number of training data points that go through those splits.\n3. **Gain**: The average training loss reduction gained when using a feature for splitting.","37d92046":"* we have to group dafaframe by `installation_id` to form a proper trainable dataframe","30e5ed49":"# Train Test Split","432df51e":"> Prepare the data by suitable for modeling. Kernel from [shahules](https:\/\/www.kaggle.com\/shahules\/xgboost-feature-selection-dsbowl)","07b70516":"## References: SHARP\n\n- [Interpretable Machine Learning with XGBoost](https:\/\/towardsdatascience.com\/interpretable-machine-learning-with-xgboost-9ec80d148d27)","7fd3bdc1":"# Import Libraries","3f7bc666":"The file train_labels.csv has been provided to show how these groups would be computed on the assessments in the training set. \n\nThe outcomes in this competition are grouped into 4 groups (labeled accuracy_group in the data):\n- 3: the assessment was solved on the first attempt\n- 2: the assessment was solved on the second attempt\n- 1: the assessment was solved after 3 or more attempts\n- 0: the assessment was never solved","36e1a005":"# Exploratory Data Analysis","acc0da57":"# 2019 Data Science Bowl\n - Uncover the factors to help measure how young children learn","4f4d2495":"Version 11 Update:\n\n> Intially tried LGBM but not much improvement even with KFold. So I am using XGBOOST now.","4aace8c4":"# Label Encoding","ee0e41a4":"## SHAP Summary Plot\n\n- Rather than use a typical feature importance bar chart, we use a density scatter plot of SHAP values for each feature to identify how much impact each feature has on the model output for individuals in the validation dataset.\n\n-  Features are sorted by the sum of the SHAP value magnitudes across all samples.","e541fd35":"From the description we know that:\n- Assessment attempts are captured in event_code 4100 for all assessments except for Bird Measurer, which uses event_code 4110. If the attempt was correct, it contains \"correct\":true.\n- In the training set, you are provided the full history of gameplay data.\n- In the test set, we have truncated the history after the start event of a single assessment, chosen randomly, for which you must predict the number of attempts.","19c71a8f":"# Data Preparation\n- We need to bring prepare the data such that it is suitable for model building.\n- Clean the data by removing useless data points from training data.\n- Extracting more information from `timestamp`.\n- Create the aggreagate variables by grouping data based on `installation_id`. ","2994e619":"> We will ignore the `installation_id` where there missing values in assessment column. ","94bc2c29":"> **The feature importance orderings are very different for each of the three options provided by XGBoost**. This should make us very uncomfortable about relying on these measures for reporting feature importance without knowing which method is best.","834d49f0":"> Since we are only interested in childern who tried at least one assessment throughout their game play history. so we can remove the data points related to other childern who never tried any assessment or childern data points where there are no train labels in `train_labels.csv`. Similar idea is implemented by [carlossouza](https:\/\/www.kaggle.com\/carlossouza\/cleaning-useless-data-to-load-train-csv-faster)","b51c168e":"## Reducing Memory Usuage\n- The code to reduce memory usage is taken from the kernel created by [yasufuminakama](https:\/\/www.kaggle.com\/yasufuminakama\/public-dsb2019-lgbm-regression-sample)\n- Other Resources: [Using Pandas with Large Data Sets in Python](https:\/\/www.dataquest.io\/blog\/pandas-big-data\/)","6f72a2b8":"- There are more than 11 Million data points in training data but there are only 17k data points in `train_labels` and only 1.1 Million points for testing. Each application install in training data is represented by an `installation_id`. This will typically correspond to one child, but you should expect noise from issues such as shared devices. So we need to group the data by `installation_id` for analysis.","fb3e3e7a":"# XGBoost Model","ea1c67e2":"> Specs.csv gives the specification of the various event types.","c91bec00":"# Model Explained with SHAP\n- SHAP (SHapley Additive exPlanations) is a unified approach to explain the output of any machine learning model.","41bf3f58":"In this dataset, you are provided with game analytics for the PBS KIDS Measure Up! app. In this app, children navigate a map and complete various levels, which may be activities, video clips, games, or assessments. Each assessment is designed to test a child's comprehension of a certain set of measurement-related skills. There are five assessments: Bird Measurer, Cart Balancer, Cauldron Filler, Chest Sorter, and Mushroom Sorter.\n\nThe intent of the competition is to use the gameplay data to forecast how many attempts a child will take to pass a given assessment    (an incorrect answer is counted as an attempt). Each application install is represented by an installation_id. This will typically correspond to one child, but you should expect noise from issues such as shared devices. In the training set, you are provided the full history of gameplay data. In the test set, we have truncated the history after the start event of a single assessment, chosen randomly, for which you must predict the number of attempts. Note that the training set contains many installation_ids which never took assessments, whereas every installation_id in the test set made an attempt on at least one assessment."}}