{"cell_type":{"4a5413d2":"code","46582451":"code","cb962e76":"code","337fc08f":"code","c686371d":"code","f428aba8":"code","68d68180":"code","fa4eeacd":"code","a9d98838":"code","de1e1fc0":"code","926d57b5":"code","aeb1892d":"code","2d8b409a":"code","b710d81e":"code","5d542543":"code","36c96a81":"markdown","ef837cc7":"markdown","0d835733":"markdown","476de95d":"markdown","8c8ffff6":"markdown","5384267f":"markdown","83403bc2":"markdown","60aa9881":"markdown","d3586881":"markdown","06321172":"markdown","4af6e7c2":"markdown","1ecc047a":"markdown","d9671111":"markdown","1194b1a9":"markdown","699e402f":"markdown"},"source":{"4a5413d2":"import numpy as np\nimport pandas as pd","46582451":"data_train = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\ndata_test = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')","cb962e76":"X_train = data_train.values[:, 1:]\nY_train = data_train.values[:, 0]\nX_test = data_test.values[:, 1:]\nY_test = data_test.values[:, 0]","337fc08f":"X_train, X_test = X_train.astype('float32'), X_test.astype('float32'), \nX_train \/= 255\nX_test \/= 255","c686371d":"def log_regression(X, Y, T=50000, learning_rate=0.01):\n    N = X.shape[0]\n    W = np.zeros((X.shape[1],)) \n    for i in range(T):\n        if i == 20000:\n            learning_rate = 0.005\n        elif i == 30000:\n            learning_rate = 0.001\n        rand_index = np.random.choice(N, size=1)\n        x_n = X[rand_index][0]\n        E_dev = -1 * (Y[rand_index] * x_n)\/(1 + np.exp((Y[rand_index][0] * np.dot(x_n, W))))\n        W = W - learning_rate * E_dev\n    return W","f428aba8":"def generate_D_Y(Y, k):\n    Y_copy = np.copy(Y)\n    for i, y in enumerate(Y_copy):\n        Y_copy[i] = 2 * (int(y) == k) - 1\n    return Y_copy","68d68180":"def sigmoid(x):\n    return 1\/(1+np.exp(-x))","fa4eeacd":"def calculate_prob(W, X):\n    probs = []\n    for i in range(10):\n        probs.append(sigmoid(np.dot(X, W[i])))\n    return probs","a9d98838":"def predict(p):\n    Y_pred = []\n    for i in range(len(p[0])):\n        Y_pred.append(np.argmax(p[:,i]))\n    return Y_pred","de1e1fc0":"def calculate_accuracy(Y_pred, Y):\n    correct = 0\n    for i in range(len(Y)):\n        if Y_pred[i] == Y[i]:\n            correct += 1\n    return correct\/len(Y)","926d57b5":"class logistic_regression:\n    def fit(X, Y, k):\n        W = []\n        for i in range(k):\n            W.append(log_regression(X, generate_D_Y(Y, i)))\n        return W\n\n    def score(X, Y, W):\n        probs = calculate_prob(W, X)\n        Y_pred = predict(np.array(probs))\n        return calculate_accuracy(Y_pred, Y)","aeb1892d":"classifier = logistic_regression\nW_log = classifier.fit(X_train, Y_train, 10)","2d8b409a":"print(classifier.score(X_test, Y_test, W_log))","b710d81e":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0, solver='liblinear', multi_class='auto').fit(X_train, Y_train)","5d542543":"print(clf.score(X_test, Y_test))","36c96a81":"## Logistic Regression from Scratch","ef837cc7":"### generate_D_Y(Y, k)\n- The model uses One vs All approach for multicalss classificattion \n- The generate_D_Y() function generates new labels for each class\n- Y is an original set labels\n- k is the number of classes","0d835733":"### predict(p)\n- The predict() function predicts the actual class by getting the most probable one\n- p is probabilities calculated by calculate_prob()","476de95d":"## Summary\nIt can be seen that the performance of my model is approximately the same as the performance of sklearn build-in model. Therefore it can be concluded that the implementation was done correctly.","8c8ffff6":"## Training the sklearn.linear_model.LogisticRegression","5384267f":"# Fashon-MNIST Classification from Scratch\nClassification of Fashion MNIST data set, from scratch using only numpy. And comparison of the implemented model with sklearn.linear_model.LogisticRegression","83403bc2":"## Data Preparation\n- Download the data set from https:\/\/www.kaggle.com\/zalando-research\/fashionmnist\n- Load it using pandas\n- Store the pixels values into X and labels into Y\n- Normalize the data dividing by 255\n","60aa9881":"### log_regression(X, Y, T, learning_rat)\n- The log_regression() function optimizes the weights by minimizing a Cross Entropy Error using stochastic gradient descent\n- X is a training data set\n- Y is corresponding labels\n- T is a number of iterations (default is 50000)\n- learning_rate is a learning rate of the model (default is initially 0.01 and decreasing during the training\n- Final weights are returned","d3586881":"### calculate_accuracy(Y_pred, Y)\n- The calculate-accuracy function evaluates the accuracy of the model by comparing predicted values Y_pred with actual labels Y","06321172":"## Testing the sklearn.linear_model.LogisticRegression ","4af6e7c2":"## Testing the Implemented Logisitic Regression","1ecc047a":"## Training the Implemented Logisitic Regression","d9671111":"### calculate prob(W, X)\n- The calculate_prob() function calculate the probability of each example belongs to each class\n- W is the set of weights genetared by log_regression on each class k\n- X is a test data set","1194b1a9":"### sigmoid(x)\n- The sigmoid function is used to predict the probability that x belongs to the class","699e402f":"### Logistic Regression"}}