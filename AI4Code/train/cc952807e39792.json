{"cell_type":{"7f57c6bf":"code","99ebdfc1":"code","9cb28d4b":"code","7319dba2":"code","501b9491":"code","c63e1d55":"code","9af4751d":"code","81776319":"code","d3458df3":"code","32bbb44f":"code","6022c633":"code","31e86f53":"code","5eb831e8":"code","2c797d31":"code","2a6d3afd":"code","d8ccaad0":"code","639b5981":"code","1271178e":"code","ab6c38af":"code","5e480609":"code","a78c5771":"code","6f033f6d":"code","ccdcd9be":"code","b11e5c3e":"code","a5760f51":"code","bef5a41b":"code","cf64d35a":"code","43240f24":"code","2b9dcfea":"code","d3edf398":"code","3de5a6b3":"code","1f904e22":"code","f84d5a00":"code","9f27e795":"code","63fe2e5f":"code","ab5904f5":"code","caa322e5":"code","b81e5e14":"code","b29175bb":"code","1e91d7c9":"code","36077b95":"markdown","24b623c3":"markdown","3a06bcde":"markdown","3c8fc6e6":"markdown","064a328b":"markdown","59a94e7e":"markdown","7100f32f":"markdown","fd76f53f":"markdown","00c423bc":"markdown","65196c3f":"markdown","be829efd":"markdown","07923f71":"markdown","e2ad43f8":"markdown","98b476dd":"markdown","8e26fcd6":"markdown","40b1f2ae":"markdown","9732bc1d":"markdown","5ae5c9e2":"markdown","39975b73":"markdown"},"source":{"7f57c6bf":"kernel_mode = True\n\nimport sys\nif kernel_mode:\n    sys.path.insert(0, \"..\/input\/iterative-stratification\")\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport time\nimport random\nimport math\n\nimport matplotlib\nimport matplotlib.pyplot as plt\n\npd.options.display.max_columns = None\n\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nfrom sklearn.manifold import TSNE\n\nimport seaborn as sns\nsns.set(style=\"darkgrid\")\n\nimport gc\ngc.enable()\n\nrand_seed = 1120","99ebdfc1":"dataset_folder = \"..\/input\/lish-moa\"","9cb28d4b":"train_features = pd.read_csv(\n    f\"{dataset_folder}\/train_features.csv\", engine='c')\ntrain_labels = pd.read_csv(\n    f\"{dataset_folder}\/train_targets_scored.csv\", engine='c')\n\ntrain_extra_labels = pd.read_csv(\n    f\"{dataset_folder}\/train_targets_nonscored.csv\", engine='c')\n\ntest_features = pd.read_csv(\n    f\"{dataset_folder}\/test_features.csv\", engine='c')","7319dba2":"train_features.shape, train_labels.shape, train_extra_labels.shape","501b9491":"test_features.shape","c63e1d55":"category_features = [\"cp_type\", \"cp_dose\"]\nnumeric_features = [c for c in train_features.columns if c != \"sig_id\" and c not in category_features]\nall_features = category_features + numeric_features\ngene_experssion_features = [c for c in numeric_features if c.startswith(\"g-\")]\ncell_viability_features = [c for c in numeric_features if c.startswith(\"c-\")]\nlen(numeric_features), len(gene_experssion_features), len(cell_viability_features)","9af4751d":"train_classes = [c for c in train_labels.columns if c != \"sig_id\"]\ntrain_extra_classes = [c for c in train_extra_labels.columns if c != \"sig_id\"]\nlen(train_classes), len(train_extra_classes)","81776319":"for df in [train_features, test_features]:\n    df['cp_type'] = df['cp_type'].map({'ctl_vehicle': 0, 'trt_cp': 1})\n    df['cp_dose'] = df['cp_dose'].map({'D1': 0, 'D2': 1})\n    df['cp_time'] = df['cp_time'].map({24: 0, 48: 0.5, 72: 1})","d3458df3":"cm = sns.light_palette(\"seagreen\", as_cmap=True)\ntrain_features[\"cp_type\"].value_counts(\n    dropna=False).to_frame().style.background_gradient(cmap=cm)","32bbb44f":"cm = sns.light_palette(\"blue\", as_cmap=True)\ntrain_features[\"cp_dose\"].value_counts(\n    dropna=False).to_frame().style.background_gradient(cmap=cm)","6022c633":"cm = sns.light_palette(\"red\", as_cmap=True)\ntrain_features[\"cp_time\"].value_counts(\n    dropna=False).to_frame().style.background_gradient(cmap=cm)","31e86f53":"kfolds = 10\nskf = MultilabelStratifiedKFold(n_splits=kfolds,\n                                shuffle=True,\n                                random_state=rand_seed)\n\nlabel_counts = np.sum(train_labels.drop(\"sig_id\", axis=1), axis=0)\ny_labels = label_counts.index.tolist()","5eb831e8":"train_index, val_index = list(skf.split(train_features,\n                                        train_labels[y_labels]))[0]","2c797d31":"train_all_features = train_features.loc[train_index,\n                                        all_features].copy().reset_index(\n                                            drop=True).values\nvalid_all_features = train_features.loc[val_index,\n                                        all_features].copy().reset_index(\n                                            drop=True).values\ntest_all_features = train_features[all_features].copy().reset_index(\n    drop=True).values\ntrain_all_features.shape, valid_all_features.shape, test_all_features.shape","2a6d3afd":"# Modified from DeepInsight Transform\n# https:\/\/github.com\/alok-ai-lab\/DeepInsight\/blob\/master\/pyDeepInsight\/image_transformer.py\n\n\nclass LogScaler:\n    \"\"\"Log normalize and scale data\n\n    Log normalization and scaling procedure as described as norm-2 in the\n    DeepInsight paper supplementary information.\n    \n    Note: The dimensions of input matrix is (N samples, d features)\n    \"\"\"\n    def __init__(self):\n        self._min0 = None\n        self._max = None\n\n    \"\"\"\n    Use this as a preprocessing step in inference mode.\n    \"\"\"\n    def fit(self, X, y=None):\n        # Min. of training set per feature\n        self._min0 = X.min(axis=0)\n\n        # Log normalized X by log(X + _min0 + 1)\n        X_norm = np.log(\n            X +\n            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n            1).clip(min=0, max=None)\n\n        # Global max. of training set from X_norm\n        self._max = X_norm.max()\n\n    \"\"\"\n    For training set only.\n    \"\"\"\n    def fit_transform(self, X, y=None):\n        # Min. of training set per feature\n        self._min0 = X.min(axis=0)\n\n        # Log normalized X by log(X + _min0 + 1)\n        X_norm = np.log(\n            X +\n            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n            1).clip(min=0, max=None)\n\n        # Global max. of training set from X_norm\n        self._max = X_norm.max()\n\n        # Normalized again by global max. of training set\n        return (X_norm \/ self._max).clip(0, 1)\n\n    \"\"\"\n    For validation and test set only.\n    \"\"\"\n    def transform(self, X, y=None):\n        # Adjust min. of each feature of X by _min0\n        for i in range(X.shape[1]):\n            X[:, i] = X[:, i].clip(min=self._min0[i], max=None)\n\n        # Log normalized X by log(X + _min0 + 1)\n        X_norm = np.log(\n            X +\n            np.repeat(np.abs(self._min0)[np.newaxis, :], X.shape[0], axis=0) +\n            1).clip(min=0, max=None)\n\n        # Normalized again by global max. of training set\n        return (X_norm \/ self._max).clip(0, 1)","d8ccaad0":"# Modified from DeepInsight Transform\n# https:\/\/github.com\/alok-ai-lab\/DeepInsight\/blob\/master\/pyDeepInsight\/image_transformer.py\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.manifold import TSNE\nfrom scipy.spatial import ConvexHull\nfrom matplotlib import pyplot as plt\nimport inspect\n\n\nclass DeepInsightTransformer:\n    \"\"\"Transform features to an image matrix using dimensionality reduction\n\n    This class takes in data normalized between 0 and 1 and converts it to a\n    CNN compatible 'image' matrix\n\n    \"\"\"\n    def __init__(self,\n                 feature_extractor='tsne',\n                 perplexity=30,\n                 pixels=100,\n                 random_state=None,\n                 n_jobs=None):\n        \"\"\"Generate an ImageTransformer instance\n\n        Args:\n            feature_extractor: string of value ('tsne', 'pca', 'kpca') or a\n                class instance with method `fit_transform` that returns a\n                2-dimensional array of extracted features.\n            pixels: int (square matrix) or tuple of ints (height, width) that\n                defines the size of the image matrix.\n            random_state: int or RandomState. Determines the random number\n                generator, if present, of a string defined feature_extractor.\n            n_jobs: The number of parallel jobs to run for a string defined\n                feature_extractor.\n        \"\"\"\n        self.random_state = random_state\n        self.n_jobs = n_jobs\n\n        if isinstance(feature_extractor, str):\n            fe = feature_extractor.casefold()\n            if fe == 'tsne_exact'.casefold():\n                fe = TSNE(n_components=2,\n                          metric='cosine',\n                          perplexity=perplexity,\n                          n_iter=1000,\n                          method='exact',\n                          random_state=self.random_state,\n                          n_jobs=self.n_jobs)\n            elif fe == 'tsne'.casefold():\n                fe = TSNE(n_components=2,\n                          metric='cosine',\n                          perplexity=perplexity,\n                          n_iter=1000,\n                          method='barnes_hut',\n                          random_state=self.random_state,\n                          n_jobs=self.n_jobs)\n            elif fe == 'pca'.casefold():\n                fe = PCA(n_components=2, random_state=self.random_state)\n            elif fe == 'kpca'.casefold():\n                fe = KernelPCA(n_components=2,\n                               kernel='rbf',\n                               random_state=self.random_state,\n                               n_jobs=self.n_jobs)\n            else:\n                raise ValueError((\"Feature extraction method '{}' not accepted\"\n                                  ).format(feature_extractor))\n            self._fe = fe\n        elif hasattr(feature_extractor, 'fit_transform') and \\\n                inspect.ismethod(feature_extractor.fit_transform):\n            self._fe = feature_extractor\n        else:\n            raise TypeError('Parameter feature_extractor is not a '\n                            'string nor has method \"fit_transform\"')\n\n        if isinstance(pixels, int):\n            pixels = (pixels, pixels)\n\n        # The resolution of transformed image\n        self._pixels = pixels\n        self._xrot = None\n\n    def fit(self, X, y=None, plot=False):\n        \"\"\"Train the image transformer from the training set (X)\n\n        Args:\n            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n            y: Ignored. Present for continuity with scikit-learn\n            plot: boolean of whether to produce a scatter plot showing the\n                feature reduction, hull points, and minimum bounding rectangle\n\n        Returns:\n            self: object\n        \"\"\"\n        # Transpose to get (n_features, n_samples)\n        X = X.T\n\n        # Perform dimensionality reduction\n        x_new = self._fe.fit_transform(X)\n\n        # Get the convex hull for the points\n        chvertices = ConvexHull(x_new).vertices\n        hull_points = x_new[chvertices]\n\n        # Determine the minimum bounding rectangle\n        mbr, mbr_rot = self._minimum_bounding_rectangle(hull_points)\n\n        # Rotate the matrix\n        # Save the rotated matrix in case user wants to change the pixel size\n        self._xrot = np.dot(mbr_rot, x_new.T).T\n\n        # Determine feature coordinates based on pixel dimension\n        self._calculate_coords()\n\n        # plot rotation diagram if requested\n        if plot is True:\n            # Create subplots\n            fig, ax = plt.subplots(1, 1, figsize=(10, 7), squeeze=False)\n            ax[0, 0].scatter(x_new[:, 0],\n                             x_new[:, 1],\n                             cmap=plt.cm.get_cmap(\"jet\", 10),\n                             marker=\"x\",\n                             alpha=1.0)\n            ax[0, 0].fill(x_new[chvertices, 0],\n                          x_new[chvertices, 1],\n                          edgecolor='r',\n                          fill=False)\n            ax[0, 0].fill(mbr[:, 0], mbr[:, 1], edgecolor='g', fill=False)\n            plt.gca().set_aspect('equal', adjustable='box')\n            plt.show()\n        return self\n\n    @property\n    def pixels(self):\n        \"\"\"The image matrix dimensions\n\n        Returns:\n            tuple: the image matrix dimensions (height, width)\n\n        \"\"\"\n        return self._pixels\n\n    @pixels.setter\n    def pixels(self, pixels):\n        \"\"\"Set the image matrix dimension\n\n        Args:\n            pixels: int or tuple with the dimensions (height, width)\n            of the image matrix\n\n        \"\"\"\n        if isinstance(pixels, int):\n            pixels = (pixels, pixels)\n        self._pixels = pixels\n        # recalculate coordinates if already fit\n        if hasattr(self, '_coords'):\n            self._calculate_coords()\n\n    def _calculate_coords(self):\n        \"\"\"Calculate the matrix coordinates of each feature based on the\n        pixel dimensions.\n        \"\"\"\n        ax0_coord = np.digitize(self._xrot[:, 0],\n                                bins=np.linspace(min(self._xrot[:, 0]),\n                                                 max(self._xrot[:, 0]),\n                                                 self._pixels[0])) - 1\n        ax1_coord = np.digitize(self._xrot[:, 1],\n                                bins=np.linspace(min(self._xrot[:, 1]),\n                                                 max(self._xrot[:, 1]),\n                                                 self._pixels[1])) - 1\n        self._coords = np.stack((ax0_coord, ax1_coord))\n\n    def transform(self, X, empty_value=0):\n        \"\"\"Transform the input matrix into image matrices\n\n        Args:\n            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n                where n_features matches the training set.\n            empty_value: numeric value to fill elements where no features are\n                mapped. Default = 0 (although it was 1 in the paper).\n\n        Returns:\n            A list of n_samples numpy matrices of dimensions set by\n            the pixel parameter\n        \"\"\"\n\n        # Group by location (x1, y1) of each feature\n        # Tranpose to get (n_features, n_samples)\n        img_coords = pd.DataFrame(np.vstack(\n            (self._coords, X.clip(0, 1))).T).groupby(\n                [0, 1],  # (x1, y1)\n                as_index=False).mean()\n\n        img_matrices = []\n        blank_mat = np.zeros(self._pixels)\n        if empty_value != 0:\n            blank_mat[:] = empty_value\n        for z in range(2, img_coords.shape[1]):\n            img_matrix = blank_mat.copy()\n            img_matrix[img_coords[0].astype(int),\n                       img_coords[1].astype(int)] = img_coords[z]\n            img_matrices.append(img_matrix)\n\n        return img_matrices\n\n    def fit_transform(self, X, empty_value=0):\n        \"\"\"Train the image transformer from the training set (X) and return\n        the transformed data.\n\n        Args:\n            X: {array-like, sparse matrix} of shape (n_samples, n_features)\n            empty_value: numeric value to fill elements where no features are\n                mapped. Default = 0 (although it was 1 in the paper).\n\n        Returns:\n            A list of n_samples numpy matrices of dimensions set by\n            the pixel parameter\n        \"\"\"\n        self.fit(X)\n        return self.transform(X, empty_value=empty_value)\n\n    def feature_density_matrix(self):\n        \"\"\"Generate image matrix with feature counts per pixel\n\n        Returns:\n            img_matrix (ndarray): matrix with feature counts per pixel\n        \"\"\"\n        fdmat = np.zeros(self._pixels)\n        # Group by location (x1, y1) of each feature\n        # Tranpose to get (n_features, n_samples)\n        coord_cnt = (\n            pd.DataFrame(self._coords.T).assign(count=1).groupby(\n                [0, 1],  # (x1, y1)\n                as_index=False).count())\n        fdmat[coord_cnt[0].astype(int),\n              coord_cnt[1].astype(int)] = coord_cnt['count']\n        return fdmat\n\n    @staticmethod\n    def _minimum_bounding_rectangle(hull_points):\n        \"\"\"Find the smallest bounding rectangle for a set of points.\n\n        Modified from JesseBuesking at https:\/\/stackoverflow.com\/a\/33619018\n        Returns a set of points representing the corners of the bounding box.\n\n        Args:\n            hull_points : an nx2 matrix of hull coordinates\n\n        Returns:\n            (tuple): tuple containing\n                coords (ndarray): coordinates of the corners of the rectangle\n                rotmat (ndarray): rotation matrix to align edges of rectangle\n                    to x and y\n        \"\"\"\n\n        pi2 = np.pi \/ 2.\n\n        # Calculate edge angles\n        edges = hull_points[1:] - hull_points[:-1]\n        angles = np.arctan2(edges[:, 1], edges[:, 0])\n        angles = np.abs(np.mod(angles, pi2))\n        angles = np.unique(angles)\n\n        # Find rotation matrices\n        rotations = np.vstack([\n            np.cos(angles),\n            np.cos(angles - pi2),\n            np.cos(angles + pi2),\n            np.cos(angles)\n        ]).T\n        rotations = rotations.reshape((-1, 2, 2))\n\n        # Apply rotations to the hull\n        rot_points = np.dot(rotations, hull_points.T)\n\n        # Find the bounding points\n        min_x = np.nanmin(rot_points[:, 0], axis=1)\n        max_x = np.nanmax(rot_points[:, 0], axis=1)\n        min_y = np.nanmin(rot_points[:, 1], axis=1)\n        max_y = np.nanmax(rot_points[:, 1], axis=1)\n\n        # Find the box with the best area\n        areas = (max_x - min_x) * (max_y - min_y)\n        best_idx = np.argmin(areas)\n\n        # Return the best box\n        x1 = max_x[best_idx]\n        x2 = min_x[best_idx]\n        y1 = max_y[best_idx]\n        y2 = min_y[best_idx]\n        rotmat = rotations[best_idx]\n\n        # Generate coordinates\n        coords = np.zeros((4, 2))\n        coords[0] = np.dot([x1, y2], rotmat)\n        coords[1] = np.dot([x2, y2], rotmat)\n        coords[2] = np.dot([x2, y1], rotmat)\n        coords[3] = np.dot([x1, y1], rotmat)\n\n        return coords, rotmat","639b5981":"all_scaler = LogScaler()\ntrain_all_features = all_scaler.fit_transform(train_all_features)\nvalid_all_features = all_scaler.transform(valid_all_features)\ntest_all_features = all_scaler.transform(test_all_features)","1271178e":"def plot_embed_2D(X, title=None):\n    sns.set(style=\"darkgrid\")\n\n    # Create subplots\n    fig, ax = plt.subplots(1, 1, figsize=(10, 7), squeeze=False)\n    ax[0, 0].scatter(X[:, 0],\n                     X[:, 1],\n                     cmap=plt.cm.get_cmap(\"jet\", 10),\n                     marker=\"x\",\n                     alpha=1.0)\n    plt.gca().set_aspect('equal', adjustable='box')\n\n    if title is not None:\n        ax[0, 0].set_title(title, fontsize=20)\n\n    plt.rcParams.update({'font.size': 14})\n    plt.show()\n\n\ndef tsne_transform(data, perplexity=30, plot=True):\n    # Transpose to get (n_features, n_samples)\n    data = data.T\n\n    tsne = TSNE(n_components=2,\n                metric='cosine',\n                perplexity=perplexity,\n                n_iter=1000,\n                method='exact',\n                random_state=rand_seed,\n                n_jobs=-1)\n    # Transpose to get (n_features, n_samples)\n    transformed = tsne.fit_transform(data)\n\n    if plot:\n        plot_embed_2D(\n            transformed,\n            f\"All Feature Location Matrix of Training Set (Perplexity: {perplexity})\"\n        )\n    return transformed","ab6c38af":"train_all_tsne = tsne_transform(train_all_features, perplexity=5)","5e480609":"train_all_tsne = tsne_transform(train_all_features, perplexity=15)","a78c5771":"train_all_tsne = tsne_transform(train_all_features, perplexity=30)","6f033f6d":"train_all_tsne = tsne_transform(train_all_features, perplexity=50)","ccdcd9be":"del train_all_tsne\ngc.collect()","b11e5c3e":"all_it = DeepInsightTransformer(feature_extractor='tsne_exact',\n                                pixels=100,\n                                perplexity=5,\n                                random_state=rand_seed,\n                                n_jobs=-1)\nall_it.fit(train_all_features, plot=True)","a5760f51":"del all_it\ngc.collect()","bef5a41b":"# Plot image matrix with feature counts per pixel\ndef plot_feature_density(it, pixels=100, show_grid=True, title=None):\n    # Update image size\n    it.pixels = pixels\n\n    fdm = it.feature_density_matrix()\n    fdm[fdm == 0] = np.nan\n\n    # Create subplots\n    fig, ax = plt.subplots(1, 1, figsize=(10, 7), squeeze=False)\n\n    if show_grid:\n        sns.heatmap(fdm,\n                    cmap=\"viridis\",\n                    linewidths=0.01,\n                    linecolor=\"lightgrey\",\n                    square=False,\n                    ax=ax[0, 0])\n        for _, spine in ax[0, 0].spines.items():\n            spine.set_visible(True)\n    else:\n        sns.heatmap(fdm,\n                    cmap=\"viridis\",\n                    linewidths=0,\n                    square=False,\n                    ax=ax[0, 0])\n\n    if title is not None:\n        ax[0, 0].set_title(title, fontsize=20)\n\n    plt.rcParams.update({'font.size': 14})\n    plt.show()\n\n    # Feature Overlapping Counts\n    gene_overlap = (\n        pd.DataFrame(all_it._coords.T).assign(count=1).groupby(\n            [0, 1],  # (x1, y1)\n            as_index=False).count())\n    print(gene_overlap[\"count\"].describe())\n    print(gene_overlap[\"count\"].hist())\n    plt.suptitle(\"Feauture Overlap Counts\")","cf64d35a":"resolution = 50\nall_it = DeepInsightTransformer(feature_extractor='tsne_exact',\n                                pixels=resolution,\n                                perplexity=5,\n                                random_state=rand_seed,\n                                n_jobs=-1)\nall_it.fit(train_all_features, plot=False)\nplot_feature_density(\n    all_it,\n    pixels=resolution,\n    title=\n    f\"All Feature Density Matrix of Training Set (Resolution: {resolution}x{resolution})\"\n)","43240f24":"resolution = 100\nall_it = DeepInsightTransformer(feature_extractor='tsne_exact',\n                                pixels=resolution,\n                                perplexity=5,\n                                random_state=rand_seed,\n                                n_jobs=-1)\nall_it.fit(train_all_features, plot=False)\nplot_feature_density(\n    all_it,\n    pixels=resolution,\n    title=\n    f\"All Feature Density Matrix of Training Set (Resolution: {resolution}x{resolution})\"\n)","2b9dcfea":"del all_it\ngc.collect()","d3edf398":"resolution = 50\ntop_k_classes = 6","3de5a6b3":"all_it = DeepInsightTransformer(feature_extractor='tsne_exact',\n                                pixels=resolution,\n                                perplexity=5,\n                                random_state=rand_seed,\n                                n_jobs=-1)\nall_it.fit(train_all_features, plot=False)","1f904e22":"train_all_images = all_it.transform(train_all_features, empty_value=0)\nlen(train_all_images), train_all_images[0].shape","f84d5a00":"class_counts = train_labels[train_classes].sum().to_frame(\n    name=\"count\").reset_index().rename(columns={\"index\": \"class\"})\nclass_counts = class_counts.sort_values(by=\"count\",\n                                        ascending=False).reset_index(drop=True)\nclass_counts","9f27e795":"# Plot image matrix with feature counts per pixel\ndef plot_feature_images(it, labels, images, classes, title=None, n_cols=2):\n    # Create subplots\n    fig, ax = plt.subplots(top_k_classes \/\/ n_cols, n_cols, figsize=(12, 12))\n\n    for i in range(0, top_k_classes \/\/ n_cols):\n        for j in range(n_cols):\n            class_rows = labels[labels[classes[i + j]] > 0]\n            # Select the random row of each class\n            sample_index = np.random.choice(class_rows.index.values, size=1)[0]\n            cax = sns.heatmap(\n                images[sample_index],\n                # cmap='hot',\n                cmap='jet',\n                linewidth=0.01,\n                linecolor='dimgrey',\n                square=False,\n                ax=ax[i, j],\n                cbar=True)\n            cax.axis('off')\n\n            ax[i, j].set_title(f\"{classes[i*n_cols + j]} (index: {sample_index})\",\n                               fontsize=14)\n\n    plt.rcParams.update({'font.size': 14})\n    if title is not None:\n        plt.suptitle(title, fontsize=20)\n\n    plt.tight_layout()\n    fig.subplots_adjust(top=0.9)\n    plt.show()","63fe2e5f":"sample_labels = train_labels.iloc[train_index, :].copy().reset_index(drop=True)\nsample_labels = sample_labels[\n    class_counts[\"class\"].values[:top_k_classes].tolist()]\nsample_labels = sample_labels[sample_labels.sum(axis=1) > 0]\nplot_feature_images(\n    all_it,\n    sample_labels,\n    train_all_images,\n    classes=sample_labels.columns.tolist(),\n    title=\n    f\"Sample Feature Images from Top-{top_k_classes} Classes (Resolution: {resolution}x{resolution})\"\n)","ab5904f5":"sample_labels = train_labels.iloc[train_index, :].copy().reset_index(drop=True)\nsample_labels = sample_labels[\n    class_counts[\"class\"].values[:top_k_classes].tolist()]\nsample_labels = sample_labels[sample_labels.sum(axis=1) > 0]\nplot_feature_images(\n    all_it,\n    sample_labels,\n    train_all_images,\n    classes=sample_labels.columns.tolist(),\n    title=\n    f\"Sample Feature Images from Top-{top_k_classes} Classes (Resolution: {resolution}x{resolution})\"\n)","caa322e5":"top_classes = class_counts[\"class\"].values[:top_k_classes].tolist()","b81e5e14":"# Plot image matrix with feature counts per pixel\ndef plot_class_feature_images(it,\n                              labels,\n                              images,\n                              target_class,\n                              title=None,\n                              n_rows=2,\n                              n_cols=2):\n    # Create subplots\n    fig, ax = plt.subplots(n_rows, n_cols, figsize=(15, 12))\n\n    for i in range(n_rows):\n        for j in range(n_cols):\n            class_rows = labels[labels[target_class] > 0]\n            # Select the random row of given class\n            sample_index = np.random.choice(class_rows.index.values, size=1)[0]\n            cax = sns.heatmap(\n                images[sample_index],\n                # cmap='hot',\n                cmap='jet',\n                linewidth=0.01,\n                linecolor='dimgrey',\n                square=False,\n                ax=ax[i, j],\n                cbar=True)\n            cax.axis('off')\n\n            ax[i, j].set_title(f\"Index: {sample_index}\", fontsize=14)\n\n    plt.rcParams.update({'font.size': 14})\n    if title is not None:\n        plt.suptitle(title, fontsize=24)\n\n    plt.tight_layout()\n    fig.subplots_adjust(top=0.9)\n    plt.show()","b29175bb":"class_index = 0\nplot_class_feature_images(\n    all_it,\n    sample_labels,\n    train_all_images,\n    target_class=top_classes[class_index],\n    title=\n    f\"Sample Feature Images of {top_classes[class_index]} (Resolution: {resolution}x{resolution})\"\n)","1e91d7c9":"class_index = 5\nplot_class_feature_images(\n    all_it,\n    sample_labels,\n    train_all_images,\n    target_class=top_classes[class_index],\n    title=\n    f\"Sample Feature Images of {top_classes[class_index]} (Resolution: {resolution}x{resolution})\"\n)","36077b95":"In the two runs of views to the feature map images, we can clearly see distinguishable patterns across different MoA target classes. This is a good news to the potentials of applying CNN-based models in this competition!","24b623c3":"### nfkb_inhibitor","3a06bcde":"## Norm-2 Normalization\nAgain, raw features are normalized into the value range of [0, 1].","3c8fc6e6":"# EOF\nThanks for reading through this post, I hope you gain some good ideas for the improvement and diversity of your models.","064a328b":"## Convex Hull Algorithm\n\nThe convex hull algorithm is applied to find the minimum box covering all features. The rotation will be applied by considering the gradient of two of the corner coordinates of the box.","59a94e7e":"# DeepInsight: Transforming Non-image data to Images for CNN Architectures\n***\nThis notebook introduces an interesting and useful preprocessing method for non-image data to be applicable in Convolutional Neural Networks (CNN) architectures. It was published in **_Scientific Reports_** of **Nature** in 2019, and I occasionally discovered it while searching for related deep learning materials to Mechanisms of Action.\n\nThe idea is very straightforward: Instead of doing feature extraction and selection for collected samples (<em>N<\/em> samples x <em>d<\/em> features), we would like to find a way of **arranging similar or correlated features into the neighboring regions of a 2-dimensional feature map (<em>d<\/em> features x <em>N<\/em> samples)** to ease the learning of their complex relationships and interactions. With this general approach, in theory we could transform any kind of non-image data into feature map images as a friendly representation of samples to CNNs, which provide several unique benefits compared with other neural network architectures, such as automated feature extraction from raw features and memory-footprint reduction by effective weight-sharing.\n\nThe following diagram outlines the key steps. First of all, a non-linear dimensionality reduction technique, like t-SNE or Kernel PCA, is applied to transform raw features into a 2D embeddings feature space. Secondly, the convex hull algorithm is used to find the smallest rectangle containing all features and a rotation is performed to align the feature map frame into a horizontal or vertical form. Finally, the raw feature values are mapped into the pixel coordinate locations of the feature map image. Note that the resolution of feature map image affects the ratio of feature overlaps (the features mapped to the same location are averaged), which is a trade-off between the level of lossy compression and computing resource requirements (e.g., host\/GPU memory, storage).\n\n![DeepInsight Architecture](https:\/\/storage.googleapis.com\/kaggle-markpeng\/MoA\/deepinsight_architecture.png)\n<div align=\"center\">Source: <a href=\"https:\/\/www.nature.com\/articles\/s41598-019-47765-6\" target=\"_blank\">https:\/\/www.nature.com\/articles\/s41598-019-47765-6<\/a><\/div>\n\n<br\/>\n\nThe authors had tested the method with a parallel CNN architecture on different kinds of datasets, including <a href=\"https:\/\/cancergenome.nih.gov\" target=\"_blank\">RNA-seq data<\/a> (gene expressions), <a href=\"https:\/\/jundongl.github.io\/scikit-feature\/OLD\/datasets_old.html\" target=\"_blank\">Text data<\/a> and <a href=\"https:\/\/catalog.ldc.upenn.edu\/LDC93S1\" target=\"_blank\">Speech data<\/a>, and showed positive results. Their implementation was in Matlab, but in the <a href=\"https:\/\/github.com\/alok-ai-lab\/DeepInsight\" target=\"_blank\">public Github repository<\/a> they shared it also includes a Python version for the transformation part.\n\nIn this notebook, the outlook of feature maps for all MoA features is demonstrated by t-SNE. You can also separate the transformation process for different groups of features (e.g., gene expressions, cell viabilities) There is a discrepancy between the Matlab version (aligned with the paper) and Python version regarding feature normalization, therefore some minor adjustments have been done here. The effect of perplexity is also being studied.\n\n***\nThis kind of transformation should open up the gates of transfer learning (ResNeSts, EfficientNets, DenseNets) and creative data augmentation techniques (CutMix, Mixup, etc.) for this small gene expression dataset. Also a great source of model diversity for the ensembles!\n\nPlease upvote or cite this notebook if you like it, thanks!\n\n***\n\n**Next Reading:**\n\n<a href=\"https:\/\/www.kaggle.com\/markpeng\/deepinsight-efficientnet-b3-noisystudent\" target=\"_blank\">DeepInsight EfficientNet-B3 Infernece Notebook<\/a>\n\n***\n\n**Reference:**\n\nSharma, Alok, Edwin Vans, D. Shigemizu, Keith A. Boroevich and T. Tsunoda (2019). \"_DeepInsight: A methodology to transform a non-image data to an image for convolution neural network architecture_,\" Scientific Reports, nature.com.\n","7100f32f":"### dna_inhibitor","fd76f53f":"# DeepInsight Transform - t-SNE 2D Embeddings\nBased on https:\/\/github.com\/alok-ai-lab\/DeepInsight, but with some corrections to the norm-2 normalization.\n\nMost of the credits should be given to the original authors!","00c423bc":"Let's compare more samples of the same classes. Here we randomly select four samples from the top-1 and top-5 class.","65196c3f":"# Load MoA Data","be829efd":"### Resolution: 50 x 50","07923f71":"# Create Stratified Multi-label K-folds\n\nHere only the first fold is used as training and validate sets for demonstration. The generation of feature map only relies on the training set to ensure the generalization of preprocessed data and further trained models.","e2ad43f8":"# Feature Encoding\nAs we only have three metadata features, a quick manual encoding process is done. All features are normalized into the value range of [0, 1].","98b476dd":"It seems clear that _\"the larger the perplexity, the greater the level of feature overlaps\"_. To make the features separated to some extent while still keeping their neighboring relationships, **perplexity=5** is chosen for the deepinsight transformation process.","8e26fcd6":"## Implementation\n\nCheckout <a href=\"https:\/\/static-content.springer.com\/esm\/art%3A10.1038%2Fs41598-019-47765-6\/MediaObjects\/41598_2019_47765_MOESM1_ESM.pdf\" target=\"_blank\">DeepInsight paper supplementary information<\/a> for more details.","40b1f2ae":"## Visualization of Feature Images\n\nOnce we got the feature map, the raw feature values can be mapped to the corresponding pixel locations in the image. For the ease of visualization here, the resolution is set to 50.\n\nTo see if the feature images could give a good representation of the raw features in different MoA targets, we select the **top-6 most frequent classes** in the dataset and visualize a sample image for each of them.","9732bc1d":"## Visualization of Feature Map\n\nFeature maps under different resolutions are visualized. Lower resolution will cause more features to be gathered into the same pixel location.","5ae5c9e2":"### Resolution: 100 x 100","39975b73":"## t-SNE Transformation\n\nIn the nice post of <a href=\"https:\/\/distill.pub\/2016\/misread-tsne\/\" target=\"_blank\">\"<em>How to Use t-SNE Effectively<\/em>\"<\/a>, we have seen that the choice of perplexity (a guess about the number of close neighbors each point has) in the t-SNE algorithm is tricky and its visual output in the embeddings feature space could be misleading. Here several values of perplexity are being set to see the difference (typical values are between 5 and 50). The similarity of features are estimated by cosine distance."}}