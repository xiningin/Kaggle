{"cell_type":{"b5d130b1":"code","615af184":"code","7171b26f":"code","6e9db2aa":"code","24f2f2de":"code","76967856":"code","34b1cb73":"code","8f3c65f7":"code","1d9a75e4":"code","53a33669":"code","643d6e6c":"code","36832a03":"code","a4b0a626":"code","77441b43":"code","50bfc874":"code","cf638134":"code","0a1c8c96":"code","3a54d043":"code","ace32075":"code","6376ee59":"code","e17eb5a1":"code","4a5e52ab":"code","6b8b97b7":"code","8b7a2434":"code","8812c528":"code","3f950ea2":"code","75d47250":"markdown","28438dc1":"markdown"},"source":{"b5d130b1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","615af184":"train_csv = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntrain = train_csv.loc[:, ('id', 'text', 'target')]\ntrain","7171b26f":"test_csv = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntest = test_csv.loc[:, ('id', 'text')]\ntest","6e9db2aa":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\n\nstop_words = set(stopwords.words('english'))","24f2f2de":"import re\nimport string\ndef clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower() \n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text) \n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","76967856":"mispell_dict = {\"aren't\" : \"are not\",\n\"can't\" : \"cannot\",\n\"couldn't\" : \"could not\",\n\"couldnt\" : \"could not\",\n\"didn't\" : \"did not\",\n\"doesn't\" : \"does not\",\n\"doesnt\" : \"does not\",\n\"don't\" : \"do not\",\n\"hadn't\" : \"had not\",\n\"hasn't\" : \"has not\",\n\"haven't\" : \"have not\",\n\"havent\" : \"have not\",\n\"he'd\" : \"he would\",\n\"he'll\" : \"he will\",\n\"he's\" : \"he is\",\n\"i'd\" : \"I would\",\n\"i'd\" : \"I had\",\n\"i'll\" : \"I will\",\n\"i'm\" : \"I am\",\n\"isn't\" : \"is not\",\n\"it's\" : \"it is\",\n\"it'll\":\"it will\",\n\"i've\" : \"I have\",\n\"let's\" : \"let us\",\n\"mightn't\" : \"might not\",\n\"mustn't\" : \"must not\",\n\"shan't\" : \"shall not\",\n\"she'd\" : \"she would\",\n\"she'll\" : \"she will\",\n\"she's\" : \"she is\",\n\"shouldn't\" : \"should not\",\n\"shouldnt\" : \"should not\",\n\"that's\" : \"that is\",\n\"thats\" : \"that is\",\n\"there's\" : \"there is\",\n\"theres\" : \"there is\",\n\"they'd\" : \"they would\",\n\"they'll\" : \"they will\",\n\"they're\" : \"they are\",\n\"theyre\":  \"they are\",\n\"they've\" : \"they have\",\n\"we'd\" : \"we would\",\n\"we're\" : \"we are\",\n\"weren't\" : \"were not\",\n\"we've\" : \"we have\",\n\"what'll\" : \"what will\",\n\"what're\" : \"what are\",\n\"what's\" : \"what is\",\n\"what've\" : \"what have\",\n\"where's\" : \"where is\",\n\"who'd\" : \"who would\",\n\"who'll\" : \"who will\",\n\"who're\" : \"who are\",\n\"who's\" : \"who is\",\n\"who've\" : \"who have\",\n\"won't\" : \"will not\",\n\"wouldn't\" : \"would not\",\n\"you'd\" : \"you would\",\n\"you'll\" : \"you will\",\n\"you're\" : \"you are\",\n\"you've\" : \"you have\",\n\"'re\": \" are\",\n\"wasn't\": \"was not\",\n\"we'll\":\" will\",\n\"didn't\": \"did not\"}","34b1cb73":"def replace_typical_misspell(text):\n    mispellings_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n\n    def replace(match):\n        return mispell_dict[match.group(0)]\n\n    return mispellings_re.sub(replace, text)","8f3c65f7":"def remove_stopwords(text):\n    text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n    return text","1d9a75e4":"def stemm_text(text):\n    stemmer = nltk.SnowballStemmer(\"english\")\n    text = ' '.join(stemmer.stem(word) for word in text.split(' '))\n    return text","53a33669":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\ndef prepare_data(tweet_text):\n    cleaned_text = replace_typical_misspell(tweet_text)\n    cleaned_text = clean_text(cleaned_text)\n    cleaned_text = remove_stopwords(cleaned_text)\n    cleaned_text = stemm_text(cleaned_text)\n    return cleaned_text","643d6e6c":"train['text_clean'] = train.loc[:, 'text'].apply(lambda x: prepare_data(x))\ntest['text_clean'] = test.loc[:, 'text'].apply(lambda x: prepare_data(x))","36832a03":"train_x = train['text_clean']\ntrain_y = train['target']\ntest_x = test['text_clean']","a4b0a626":"from sklearn.feature_extraction.text import CountVectorizer\n\n# instantiate the vectorizer\nvect = CountVectorizer()\nvect.fit(train_x)","77441b43":"x_train_dtm = vect.transform(train_x)\nx_test_dtm = vect.transform(test_x)","50bfc874":"from sklearn.feature_extraction.text import TfidfTransformer\n\ntfidf_transformer = TfidfTransformer()\n\ntfidf_transformer.fit(x_train_dtm)\ntrain_x_tfidf = tfidf_transformer.transform(x_train_dtm)\n\ntest_x_tfidf = tfidf_transformer.transform(x_test_dtm)","cf638134":"from keras.preprocessing.text import Tokenizer\n# Calculate the length of our vocabulary\nword_tokenizer = Tokenizer()\ntexts = train[['text_clean']].append(test[['text_clean']])['text_clean']\nword_tokenizer.fit_on_texts(texts)\n\nvocab_length = len(word_tokenizer.word_index) + 1\nvocab_length","0a1c8c96":"from keras.preprocessing.sequence import pad_sequences\nnltk.download('punkt')\n\n\ndef embed(corpus): \n    return word_tokenizer.texts_to_sequences(corpus)\n\nlongest_train = max(texts, key=lambda sentence: len(word_tokenize(sentence)))\nlength_long_sentence = len(word_tokenize(longest_train))\n\ntrain_padded_sentences = pad_sequences(\n    embed(train_x), \n    length_long_sentence, \n    padding='post'\n)\n\ntrain_padded_sentences","3a54d043":"test_padded_sentences = pad_sequences(\n    embed(test_x), \n    length_long_sentence, \n    padding='post'\n)\n\ntest_padded_sentences","ace32075":"# GloVe thing\nembeddings_dictionary = dict()\nembedding_dim = 100\nglove_file = open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt')\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = np.asarray(records[1:], dtype='float32')\n    embeddings_dictionary [word] = vector_dimensions\nglove_file.close()","6376ee59":"embedding_matrix = np.zeros((vocab_length, embedding_dim))\nfor word, index in word_tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","e17eb5a1":"# Create a Multinomial Naive Bayes model\nfrom sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\n\n# Train the model\nnb.fit(x_train_dtm, train_y)","4a5e52ab":"predictions = nb.predict(x_test_dtm)\n\noutput = pd.DataFrame({'id': test.id, 'target': predictions})\noutput.to_csv('submission.csv', index=False)","6b8b97b7":"# from keras.layers.embeddings import Embedding\n# from keras.models import Sequential\n# from keras.initializers import Constant\n# from keras.layers import (LSTM, \n#                           Embedding, \n#                           BatchNormalization,\n#                           Dense, \n#                           TimeDistributed, \n#                           Dropout, \n#                           Bidirectional,\n#                           Flatten, \n#                           GlobalMaxPool1D)\n\n# model = Sequential()\n# # Turns positive integers (indexes) into dense vectors of fixed size.\n# model.add(Embedding(input_dim=embedding_matrix.shape[0], \n#                     output_dim=embedding_matrix.shape[1], \n#                     weights = [embedding_matrix], \n#                     input_length=length_long_sentence))\n# model.add(Bidirectional(LSTM(length_long_sentence, return_sequences = True, recurrent_dropout=0.2)))\n# model.add(GlobalMaxPool1D()) # Downsamples the input representation by taking the maximum value over the time dimension. \n# model.add(BatchNormalization()) # maintains the mean output close to 0 and the output standard deviation close to 1\n# model.add(Dropout(0.5)) # drop 50% of units \n# model.add(Dense(length_long_sentence, activation = \"relu\")) # each neuron in the dense layer receives input from all neurons of its previous layer\n# model.add(Dropout(0.5)) #dropout\n# model.add(Dense(length_long_sentence, activation = \"relu\"))\n# model.add(Dropout(0.5)) # dropout\n# model.add(Dense(1, activation = 'sigmoid')) # output 1 value\n# model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])","8b7a2434":"# from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n# checkpoint = ModelCheckpoint(\n#     'model.h5', \n#     monitor = 'val_loss', \n#     verbose = 1, \n#     save_best_only = True\n# )\n# reduce_lr = ReduceLROnPlateau(\n#     monitor = 'val_loss', \n#     factor = 0.2, \n#     verbose = 1, \n#     patience = 5,                        \n#     min_lr = 0.001\n# )","8812c528":"# fitting = model.fit(\n#     train_padded_sentences, \n#     train_y, \n#     epochs = 7,\n#     batch_size = 32,\n#     verbose = 1,\n#     callbacks = [reduce_lr, checkpoint])","3f950ea2":"# predicted = model.predict(test_padded_sentences) \n# predictions = np.argmax(predicted,axis=1)\n\n# output = pd.DataFrame({'id': test.id, 'target': predictions})\n# output.to_csv('submission_3.csv', index=False)","75d47250":"**Building model and Training**","28438dc1":"**Preparing data**"}}