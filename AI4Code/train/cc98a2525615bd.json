{"cell_type":{"c2bca59e":"code","4bc05abd":"code","850b25ca":"code","69f9b06e":"code","ee22b326":"code","41c2ba36":"code","887b0f34":"code","0467605c":"code","7e1d9387":"code","526a07ef":"code","9f5ca6ec":"code","a0910c5e":"code","3b38c3a6":"code","a049ba05":"code","1ab447a7":"code","05706b49":"code","b9c978dd":"code","602c33ab":"code","7291a627":"code","6a12f29e":"code","dd52ca34":"code","79d83c73":"code","43e905d3":"code","07099154":"code","6452a5bd":"code","0dde1e59":"code","0f0570e2":"code","a44eae96":"code","d5467792":"markdown","cdd6d1b5":"markdown","5728fff0":"markdown","f4596966":"markdown","f38e3d98":"markdown","5690553b":"markdown","f5d81de9":"markdown","64b559ca":"markdown","979a0182":"markdown","52bb7b1f":"markdown","c90db776":"markdown","381fb2f0":"markdown","55debbc1":"markdown","c8b158f5":"markdown","4db864f0":"markdown","5fb25664":"markdown","44b2632c":"markdown","41b79275":"markdown","8c41681d":"markdown","5738eafc":"markdown","50ca51e8":"markdown","749f8a7c":"markdown","25c56c6d":"markdown","e48f8e92":"markdown","cdf1594d":"markdown"},"source":{"c2bca59e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import precision_score, recall_score, f1_score,\\\n                            accuracy_score, balanced_accuracy_score,classification_report,\\\n                            plot_confusion_matrix, confusion_matrix\nfrom sklearn.model_selection import KFold, GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\nimport lightgbm as lgb\nfrom tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply, Concatenate\nfrom tensorflow.keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D, LeakyReLU\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.initializers import RandomNormal\nimport tensorflow.keras.backend as K\nfrom sklearn.utils import shuffle\n\nnp.random.seed(1635848)","4bc05abd":"class cGAN():\n    \n    \"\"\"\n    Class containing 3 methods (and __init__): generator, discriminator and train.\n    Generator is trained using random noise and label as inputs. Discriminator is trained\n    using real\/fake samples and labels as inputs.\n    \"\"\"\n    \n    def __init__(self,latent_dim=32, out_shape=14):\n        \n        self.latent_dim = latent_dim\n        self.out_shape = out_shape \n        self.num_classes = 2\n        # using Adam as our optimizer\n        optimizer = Adam(0.0002, 0.5)\n        \n        # building the discriminator\n        self.discriminator = self.discriminator()\n        self.discriminator.compile(loss=['binary_crossentropy'],\n                                   optimizer=optimizer,\n                                   metrics=['accuracy'])\n\n        # building the generator\n        self.generator = self.generator()\n\n        noise = Input(shape=(self.latent_dim,))\n        label = Input(shape=(1,))\n        gen_samples = self.generator([noise, label])\n        \n        # we don't train discriminator when training generator\n        self.discriminator.trainable = False\n        valid = self.discriminator([gen_samples, label])\n\n        # combining both models\n        self.combined = Model([noise, label], valid)\n        self.combined.compile(loss=['binary_crossentropy'],\n                              optimizer=optimizer,\n                             metrics=['accuracy'])\n\n\n    def generator(self):\n        init = RandomNormal(mean=0.0, stddev=0.02)\n        model = Sequential()\n\n        model.add(Dense(128, input_dim=self.latent_dim))\n        model.add(Dropout(0.2))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n\n        model.add(Dense(256))\n        model.add(Dropout(0.2))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n\n        model.add(Dense(512))\n        model.add(Dropout(0.2))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n\n        model.add(Dense(self.out_shape, activation='tanh'))\n\n        noise = Input(shape=(self.latent_dim,))\n        label = Input(shape=(1,), dtype='int32')\n        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n        \n        model_input = multiply([noise, label_embedding])\n        gen_sample = model(model_input)\n\n        return Model([noise, label], gen_sample, name=\"Generator\")\n\n    \n    def discriminator(self):\n        init = RandomNormal(mean=0.0, stddev=0.02)\n        model = Sequential()\n\n        model.add(Dense(512, input_dim=self.out_shape, kernel_initializer=init))\n        model.add(LeakyReLU(alpha=0.2))\n        \n        model.add(Dense(256, kernel_initializer=init))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.4))\n        \n        model.add(Dense(128, kernel_initializer=init))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.4))\n        \n        model.add(Dense(1, activation='sigmoid'))\n        \n        gen_sample = Input(shape=(self.out_shape,))\n        label = Input(shape=(1,), dtype='int32')\n        label_embedding = Flatten()(Embedding(self.num_classes, self.out_shape)(label))\n\n        model_input = multiply([gen_sample, label_embedding])\n        validity = model(model_input)\n\n        return Model(inputs=[gen_sample, label], outputs=validity, name=\"Discriminator\")\n\n\n    def train(self, X_train, y_train, pos_index, neg_index, epochs, sampling=False, batch_size=32, sample_interval=100, plot=True): \n        \n        # though not recommended, defining losses as global helps as in analysing our cgan out of the class\n        global G_losses\n        global D_losses\n        \n        G_losses = []\n        D_losses = []\n        # Adversarial ground truths\n        valid = np.ones((batch_size, 1))\n        fake = np.zeros((batch_size, 1))\n\n        for epoch in range(epochs):\n            \n            # if sampling==True --> train discriminator with 8 sample from postivite class and rest with negative class\n            if sampling:\n                idx1 = np.random.choice(pos_index, 8)\n                idx0 = np.random.choice(neg_index, batch_size-8)\n                idx = np.concatenate((idx1, idx0))\n            # if sampling!=True --> train discriminator using random instances in batches of 32\n            else:\n                idx = np.random.choice(len(y_train), batch_size)\n            samples, labels = X_train[idx], y_train[idx]\n            samples, labels = shuffle(samples, labels)\n            \n            # Sample noise as generator input\n            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n            gen_samples = self.generator.predict([noise, labels])\n\n            # label smoothing\n            if epoch < epochs\/\/1.5:\n                valid_smooth = (valid+0.1)-(np.random.random(valid.shape)*0.1)\n                fake_smooth = (fake-0.1)+(np.random.random(fake.shape)*0.1)\n            else:\n                valid_smooth = valid \n                fake_smooth = fake\n                \n            # Train the discriminator\n            self.discriminator.trainable = True\n            d_loss_real = self.discriminator.train_on_batch([samples, labels], valid_smooth)\n            d_loss_fake = self.discriminator.train_on_batch([gen_samples, labels], fake_smooth)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n            # Train Generator\n            self.discriminator.trainable = False\n            sampled_labels = np.random.randint(0, 2, batch_size).reshape(-1, 1)\n            # Train the generator\n            g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n\n            if (epoch+1)%sample_interval==0:\n                print('[%d\/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f'\n                  % (epoch, epochs, d_loss[0], g_loss[0]))\n            G_losses.append(g_loss[0])\n            D_losses.append(d_loss[0])\n            if plot:\n                if epoch+1==epochs:\n                    plt.figure(figsize=(10,5))\n                    plt.title(\"Generator and Discriminator Loss\")\n                    plt.plot(G_losses,label=\"G\")\n                    plt.plot(D_losses,label=\"D\")\n                    plt.xlabel(\"iterations\")\n                    plt.ylabel(\"Loss\")\n                    plt.legend()\n                    plt.show()","850b25ca":"df = pd.read_csv('..\/input\/adult-census-income\/adult.csv')\ndf.head()","69f9b06e":"le = preprocessing.LabelEncoder()\nfor i in ['workclass','education','marital.status','occupation','relationship','race','sex','native.country','income']:\n    df[i] = le.fit_transform(df[i].astype(str))","ee22b326":"df.head()","41c2ba36":"df.income.value_counts()","887b0f34":"scaler = StandardScaler()\n\nX = scaler.fit_transform(df.drop('income', 1))\ny = df['income'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","0467605c":"lgb_1 = lgb.LGBMClassifier()\nlgb_1.fit(X_train, y_train)\n\ny_pred = lgb_1.predict(X_test)\n\n# evaluation\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(lgb_1, X_test, y_test)\nplt.show()","7e1d9387":"cgan = cGAN()","526a07ef":"y_train = y_train.reshape(-1,1)\npos_index = np.where(y_train==1)[0]\nneg_index = np.where(y_train==0)[0]\ncgan.train(X_train, y_train, pos_index, neg_index, epochs=500)","9f5ca6ec":"# we want to generate 19758 instances with class value 0 since that represents how many 0s are in the label of the real training set\nnoise = np.random.normal(0, 1, (19758, 32))\nsampled_labels = np.zeros(19758).reshape(-1, 1)\n\n\ngen_samples = cgan.generator.predict([noise, sampled_labels])\n\ngen_df = pd.DataFrame(data = gen_samples,\n                      columns = df.drop('income',1).columns)","a0910c5e":"# we want to generate 6290 instances with class value 1 since that represents how many 1s are in the label of the real training set\nnoise_2 = np.random.normal(0, 1, (6290, 32))\nsampled_labels_2 = np.ones(6290).reshape(-1, 1)\n\n\ngen_samples_2 = cgan.generator.predict([noise_2, sampled_labels_2])\n\ngen_df_2 = pd.DataFrame(data = gen_samples_2,\n                      columns = df.drop('income',1).columns)","3b38c3a6":"gen_df_2['income'] = 1\ngen_df['income']=0\n\ndf_gan = pd.concat([gen_df_2, gen_df], ignore_index=True, sort=False)\ndf_gan = df_gan.sample(frac=1).reset_index(drop=True)\n\nX_train_2 = df_gan.drop('income', 1)\ny_train_2 = df_gan['income'].values","a049ba05":"lgb_1 = lgb.LGBMClassifier()\nlgb_1.fit(X_train_2, y_train_2)\n\n\ny_pred = lgb_1.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(lgb_1, X_test, y_test)\nplt.show()","1ab447a7":"df2 = pd.read_csv('..\/input\/ucuss\/Skin_NonSkin.txt', sep='\\t', header=None, names=['B','G','R','Class'])\ndf2.Class = df2.Class.replace([1, 2], [0, 1])\ndf2.head()","05706b49":"df2.Class.value_counts()","b9c978dd":"scaler = StandardScaler()\n\nX = scaler.fit_transform(df2.drop('Class', 1))\ny = df2['Class'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","602c33ab":"lgb_1 = lgb.LGBMClassifier()\nlgb_1.fit(X_train, y_train)\n\n\ny_pred = lgb_1.predict(X_test)\n\n# evaluation\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(lgb_1, X_test, y_test)\nplt.show()","7291a627":"cgan = cGAN(out_shape=3)","6a12f29e":"y_train = y_train.reshape(-1,1)\npos_index = np.where(y_train==1)[0]\nneg_index = np.where(y_train==0)[0]\ncgan.train(X_train, y_train,pos_index, neg_index, epochs=100)","dd52ca34":"# we want to generate 40572 instances with class value 0 since that represents how many 0s are in the label of the real training set\nnoise = np.random.normal(0, 1, (40572, 32))\nsampled_labels = np.zeros(40572).reshape(-1, 1)\n\n\ngen_samples = cgan.generator.predict([noise, sampled_labels])\n\ngen_df = pd.DataFrame(data = gen_samples,\n                      columns = df2.drop('Class',1).columns)\ngen_df.head()","79d83c73":"# we want to generate 155473 instances with class value 1 since that represents how many 1s are in the real training set\nnoise_2 = np.random.normal(0, 1, (155473, 32))\nsampled_labels_2 = np.ones(155473).reshape(-1, 1)\n\n\ngen_samples_2 = cgan.generator.predict([noise_2, sampled_labels_2])\n\ngen_df_2 = pd.DataFrame(data = gen_samples_2,\n                      columns = df2.drop('Class',1).columns)\ngen_df_2.head()","43e905d3":"gen_df_2['Class'] = 1\ngen_df['Class']=0\n\ndf_gan = pd.concat([gen_df_2, gen_df], ignore_index=True, sort=False)\ndf_gan = df_gan.sample(frac=1).reset_index(drop=True)\n\nX_train_2 = df_gan.drop('Class', 1)\ny_train_2 = df_gan['Class'].values","07099154":"lgb_1 = lgb.LGBMClassifier()\nlgb_1.fit(X_train_2, y_train_2)\n\n\ny_pred = lgb_1.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(lgb_1, X_test, y_test)\nplt.show()","6452a5bd":"def generate_instances(df_new, cgan,num_instances,label_class,label='income'):\n    noise = np.random.normal(0, 1, (num_instances, 32))\n    \n    if label_class==0:\n        sampled_labels = np.zeros(num_instances).reshape(-1, 1)\n        gen_samples = cgan.generator.predict([noise, sampled_labels])\n    else:\n        sampled_labels = np.zeros(num_instances).reshape(-1, 1)\n        gen_samples = cgan.generator.predict([noise, sampled_labels])\n        \n    gen_df = pd.DataFrame(data = gen_samples,\n                          columns = df_new.drop(label,1).columns)\n    return gen_df\n","0dde1e59":"def run_experiment(df_new, pos_num_inst, neg_num_inst, num_epochs, out_sh, latent_d=32, label_f='income'):\n    \n    global bal_acc, acc, G_loss, D_loss\n    G_loss = []\n    D_loss = []\n    bal_acc = []\n    acc = []\n    \n    X = scaler.fit_transform(df_new.drop(label_f, 1))\n    y = df_new[label_f].values\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n    cgan=cGAN(latent_dim=latent_d, out_shape=out_sh)\n    y_train = y_train.reshape(-1,1)\n    pos_index = np.where(y_train==1)[0]\n    neg_index = np.where(y_train==0)[0]\n    \n    for i in range(5):\n        print(str(i+1)+\". RUN\")\n        \n        cgan.train(X_train, y_train, pos_index, neg_index, epochs=num_epochs, plot=False)\n        gen_df = generate_instances(df_new, cgan=cgan,num_instances=pos_num_inst, label_class=1, label=label_f)\n        gen_df_2 = generate_instances(df_new,cgan=cgan, num_instances=neg_num_inst, label_class=0, label=label_f)\n        \n        gen_df[label_f] = 1\n        gen_df_2[label_f] = 0\n\n        df_gan = pd.concat([gen_df_2, gen_df], ignore_index=True, sort=False)\n        df_gan = df_gan.sample(frac=1).reset_index(drop=True)\n\n        X_train_2 = df_gan.drop(label_f, 1)\n        y_train_2 = df_gan[label_f].values\n        \n        lgb_1 = lgb.LGBMClassifier()\n        lgb_1.fit(X_train_2, y_train_2)\n        \n        y_pred = lgb_1.predict(X_test)\n        \n        G_loss.append(G_losses)\n        D_loss.append(D_losses)\n        \n        bal_acc.append(balanced_accuracy_score(y_test, y_pred))\n        acc.append(accuracy_score(y_test, y_pred))\n        \n        print(\"Bal_Acc:\", bal_acc[i])\n        print(\"Acc:\", acc[i])\n       \n    Sum_G_loss = [sum(x) for x in zip(*G_loss)]\n    Sum_D_loss = [sum(x) for x in zip(*D_loss)]\n    Avg_G_loss = [x \/ 5 for x in Sum_G_loss]\n    Avg_D_loss = [x \/ 5 for x in Sum_D_loss]\n    \n    plt.figure(figsize=(10,5))\n    plt.title(\"Average Generator and Discriminator Loss\")\n    plt.plot(Avg_G_loss,label=\"G\")\n    plt.plot(Avg_D_loss,label=\"D\")\n    plt.xlabel(\"iterations\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()","0f0570e2":"%%time\nrun_experiment(df_new=df, pos_num_inst=6290, neg_num_inst=19758, num_epochs=2000, out_sh=14)","a44eae96":"%%time\nrun_experiment(df_new=df2, pos_num_inst=155289, neg_num_inst=40756, num_epochs=2000, out_sh=3,label_f='Class')","d5467792":"# Generative Adversarial Networks","cdd6d1b5":"## EXPERIMENTS","5728fff0":"# Skin Segmentation Dataset","f4596966":"### Training cGAN","f38e3d98":"Before employing any algorithms, we will first preprocess some data.","5690553b":"Conditional GAN or cGAN is a type of Generative Adversarial Network which adds the label y as an additional parameter to the generator in hope that the corresponding data will be generated. The labels are also added to the discriminator input to distinguish real data better.","f5d81de9":"### Generating new instances","64b559ca":"Skin segmentation dataset has 3 features (B, G, R) and one class (skin-nonskin). The dataset is extracted from images of peoples faces, so that each row presents a pixel and each pixel represents how much of that color is attained in it (i.e. B represents how much blue does a pixel have). Class tells us if the given row (pixel) is a skin or not. People represented in the images vary in age, race and gender.","979a0182":"### Classifying testset using generated trainset","52bb7b1f":"### Classifying using real trainset","c90db776":"### Splitting the dataframe","381fb2f0":"## Preprocessing","55debbc1":"### Classifying testset using generated trainset","c8b158f5":"### Generating new instances","4db864f0":"Generative Adversarial Networks are type of unsupervised machine learning method which try to generate new, synthetic instances of data that mimics the real data. They are extremely popular in image, video (Deepfake) and voice generation. Generating tabular data using GANs isn't as popular as generating images, but can still produce some pretty good results. This notebook tries to do just that. GANs are constructed of two neural networks: Generator and Discriminator. Generator, using some random noise as input, tries to mimic the real data and Discriminator tries to classify the data into real and fake. It could be said that they are eachothers enemies. Both Neural Networks (Generator and Discriminator) are trained separately through backpropagation with regards to their loss.","5fb25664":"# Adult Census Income Dataset","44b2632c":"### Splitting the Dataframe","41b79275":"First dataset has 15 features including one class that we will try to predict (income). Dataset contains a lot of categorical features that need some preprocessing before feeding it into the algorithm.","8c41681d":"### Training cGAN","5738eafc":"<h1><center><font size=\"30\">Generating Tabular Data using GANs<\/font><\/center><\/h1>","50ca51e8":"### Classifying using real trainset","749f8a7c":"### Combining generated instances into a dataframe","25c56c6d":"Since the goal of this notebook is to examine how good the generated synthetic data is, we won't analyse and do any feature engineering. It is also not that important that we get the best possible result with the algorithm, so that's one of the reasons why we will only use label-encoding (on some features normally one-hot encoding should be a better approach). ","e48f8e92":"### Combining generated instances into a dataframe","cdf1594d":"## Conditional GAN"}}