{"cell_type":{"e9fcc7a2":"code","cc2e4a76":"code","736bf61e":"code","15b7496d":"code","ac284588":"code","6bc7574b":"code","502e042b":"code","911f9a0e":"code","2805e7fb":"code","163e6dab":"code","08d86270":"code","b051f3fd":"code","0edfd029":"code","1b9952a0":"markdown","1d96b157":"markdown","8a751b73":"markdown"},"source":{"e9fcc7a2":"import warnings\nwarnings.filterwarnings(\"ignore\")","cc2e4a76":"from sklearn import metrics, preprocessing\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn import model_selection\nfrom sklearn import ensemble, tree\nimport pandas as pd\nimport numpy as np\nimport itertools\nimport lightgbm\nimport catboost\nimport xgboost\nimport os\nimport gc\n\ngc.enable()","736bf61e":"class config:\n    NUM_FOLDS = 10\n    SEED = 541\n    TARGET = 'AveragePrice'\n    JUNK = -1\n    BASE_PATH = '..\/input\/nmims-m2-batch-1\/' if 'kaggle' in os.environ.get('PWD') else ''","15b7496d":"def get_score(y_true, y_preds):\n    return round(metrics.mean_absolute_error(y_true, y_preds), 5)\n\ndef create_folds(data):\n    data[\"kfold\"] = -1\n    \n    num_bins = int(np.floor(1 + np.log2(len(data))))\n    data.loc[:, \"bins\"] = pd.cut(\n        data[config.TARGET], bins=num_bins, labels=False\n    )\n    kf = model_selection.StratifiedKFold(n_splits=config.NUM_FOLDS, shuffle=True, random_state=config.SEED)\n    for f, (t_, v_) in enumerate(kf.split(X=data, y=data.bins.values)):\n        data.loc[v_, 'kfold'] = f\n        \n    data = data.drop(\"bins\", axis=1)\n    \n    return data\n    ","ac284588":"def basic_preprocess(train_data, test_data):\n    test_data[config.TARGET] = config.JUNK\n    data = pd.concat([train_data, test_data])\n\n    data.rename(columns = {'Unnamed: 0.1': 'weekofyear_given'}, inplace = True)\n    cat_cols = ['type', 'region', 'weekofyear_given']\n    date_time_cols = ['Date']\n    num_cols = ['Total Volume', '4046', '4225', '4770', 'Total Bags', 'Small Bags', 'Large Bags', 'XLarge Bags']\n    drop_cols = ['id']\n    new_cat_cols = []\n    \n    # -------------  DATETIME COLUMNS  ------------- #\n    print('Handling Datetime cols...')\n    for col in date_time_cols:\n        data[col] = pd.to_datetime(data[col])\n        drop_cols.append(col)\n        \n        data.loc[:, 'year'] = data[col].dt.year\n        data.loc[:, 'weekofyear'] = data[col].dt.weekofyear\n        data.loc[:, 'month'] = data[col].dt.month        \n    \n    # -------------  CATEGORICAL COLUMNS  ------------- #\n    # encoding\n    print('Label Encoding...')\n    for col in cat_cols:\n        if data[col].dtype == 'O':\n            enc = preprocessing.LabelEncoder()\n            data[col] = enc.fit_transform(data[col])\n            \n    # concat features\n    print('Concatenate Cat Cols...')\n    combinations = list(itertools.combinations(cat_cols, 2))\n    for c1, c2 in combinations:\n        enc = preprocessing.LabelEncoder()\n        data[c1 + '_' + c2] = enc.fit_transform(data[c1].astype(str) + '_' + data[c2].astype(str))\n        # new_cat_cols.append(c1 + '_' + c2)\n    \n    # -------------  NUMERIC COLUMNS  ------------- #\n    # scaling\n    print('Scaling num cols...')\n    for col in num_cols:\n        data[col] = (data[col] - data[col].mean()) \/ data[col].std()\n        \n    # poly features\n#     print('Squaring num cols')\n#     for col in num_cols:\n#         data[col + '_2'] = data[col] ** 2\n    \n#     print('Multiplying num cols...')\n#     combinations = itertools.combinations(num_cols, 2)\n#     for c1, c2 in combinations:\n#         data[c1 + '_' + c2] = data[c1].astype(float) * data[c2].astype(float)\n    \n    data.drop(drop_cols, axis=1, inplace=True)\n    \n    train_data = data[data[config.TARGET] != config.JUNK].reset_index(drop=True)\n    test_data = data[data[config.TARGET] == config.JUNK].reset_index(drop=True)\n    test_data.drop([config.TARGET], axis=1, inplace=True)\n    \n    print('Done!!!')\n    return train_data, test_data, cat_cols + ['year', 'weekofyear', 'month']","6bc7574b":"def target_encoding(train_data, test_data, cat_cols):\n    for col in cat_cols:\n        x = train_data['AveragePrice'].groupby(train_data[col]).mean()\n        y = train_data['AveragePrice'].groupby(train_data[col]).median()\n        \n        train_data[col + '_tge'] = train_data[col].map(x)\n        test_data[col + '_tge'] = test_data[col].map(x)\n        \n    return train_data, test_data","502e042b":"xgb_params = {'eta': 0.1,  'max_depth': 15, 'subsample': 0.6}\nxgb_params_2 = {'eta': 0.07297038602727245, 'max_depth': 16, 'subsample': 0.6}\n\ncat_params = {'eta': 0.1,  'max_depth': 10, 'subsample': 0.6, 'verbose': 0}\ncat_params_2 = {'eta': 0.13835583783004574, 'max_depth': 10, 'subsample': 0.9510056651902064, 'verbose': 0}\n\n\nmodel_dispatcher = {\n    'xgb_1': xgboost.XGBRegressor(**xgb_params),\n    'xgb_2': xgboost.XGBRegressor(**xgb_params_2),\n    'catboost_1': catboost.CatBoostRegressor(**cat_params),\n    'catboost_2': catboost.CatBoostRegressor(**cat_params_2),\n}","911f9a0e":"def train_fn(df, fold, model_name, test_df):\n    data = df.copy(deep=True)\n    test_data = test_df.copy(deep=True)\n    \n    train_data = data[data['kfold'] != fold].reset_index(drop=True)\n    valid_data = data[data['kfold'] == fold].reset_index(drop=True)\n    \n    x_train = train_data.drop(['kfold', config.TARGET], axis=1)\n    y_train = train_data[config.TARGET]\n    \n    x_valid = valid_data.drop(['kfold', config.TARGET], axis=1)\n    y_valid = valid_data[config.TARGET]\n    \n    model = model_dispatcher[model_name]\n    model.fit(x_train, y_train)\n    y_train_preds = model.predict(x_train)\n    y_valid_preds = model.predict(x_valid)\n    y_test_preds = model.predict(test_data)\n    \n    x_train['pseudo'] = y_train_preds\n    x_valid['pseudo'] = y_valid_preds\n    test_data['pseudo'] = y_test_preds\n    \n    model = model_dispatcher[model_name]\n    model.fit(x_train, y_train)\n    y_valid_preds_pseudo = model.predict(x_valid)\n    \n    score_1 = get_score(y_valid, y_valid_preds)\n    score_2 = get_score(y_valid, y_valid_preds_pseudo)\n    print(f'Fold: {fold}, Score: {score_1}, Score with Pseudo Labels: {score_2}')\n    \n    y_test_preds_pseudo = model.predict(test_data)\n    \n#     if fold == config.NUM_FOLDS - 1:\n#         for col, imp in zip(x_train.columns, model.feature_importances_):\n#             print(col, imp)\n#         print()\n    \n    del model\n    del train_data, valid_data\n    del x_train, y_train, x_valid, y_valid\n    del y_train_preds, y_valid_preds\n    del y_valid_preds_pseudo\n    gc.collect()\n    \n    return score_1, score_2, y_test_preds, y_test_preds_pseudo","2805e7fb":"train = pd.read_csv(f'{config.BASE_PATH}train.csv')\ntest = pd.read_csv(f'{config.BASE_PATH}test.csv')\ntest_ids = test.id.values\n\ntrain, test, cat_cols = basic_preprocess(train, test)\ntrain, test = target_encoding(train, test, cat_cols=cat_cols)\n\ntrain_folds = create_folds(train)","163e6dab":"train_folds.head()","08d86270":"models = [\n    'xgb_1',\n    'xgb_2',\n    'catboost_1',\n    'catboost_2',\n]\n\nall_test_preds = {}\n\nfor model_name in models:\n    print(model_name)\n    score_sum = 0\n    pseudo_score_sum = 0\n    test_preds = np.zeros((len(test)))\n    test_preds_pseudo = np.zeros((len(test)))\n    \n    for fold in tqdm(range(config.NUM_FOLDS)):\n        score, pseudo_score, current_test_preds, current_test_preds_pseudo = train_fn(train_folds, fold, model_name, test)\n        \n        score_sum += score\n        pseudo_score_sum += pseudo_score\n        \n        test_preds += current_test_preds\n        test_preds_pseudo += current_test_preds_pseudo\n    \n    all_test_preds[model_name] = test_preds \/ config.NUM_FOLDS\n    all_test_preds[model_name + '_pseudo'] = test_preds_pseudo \/ config.NUM_FOLDS\n    \n    print(f'Average Score:                     {round(score_sum \/ config.NUM_FOLDS, 5)}')\n    print(f'Average Score with Pseudo Labels:  {round(pseudo_score_sum \/ config.NUM_FOLDS, 5)}')\n    print()","b051f3fd":"for model_name, predictions in all_test_preds.items():\n    sub_df = pd.DataFrame({\n        'id': test_ids,\n        config.TARGET: predictions,\n    })\n    \n    sub_df.to_csv(f'submission_{model_name}.csv', index=False)","0edfd029":"sub_df.head()","1b9952a0":"# Model Summaries\n\n### Models Used: \n* Xgboost\n* Catboost\n\n### Parameter Summary for Reference:\n| Parameter id | Parameters | \n| --- | --- | \n| xgb_1 | 'eta': 0.1,  'max_depth': 15, 'subsample': 0.6 | \n| xgb_2 | 'eta': 0.07297038602727245, 'max_depth': 16, 'subsample': 0.6 |\n| catboost_1 | 'eta': 0.1,  'max_depth': 10, 'subsample': 0.6, 'verbose': 0 |\n| catboost_2 | 'eta': 0.13835583783004574, 'max_depth': 10, 'subsample': 0.9510056651902064, 'verbose': 0 |\n\n### Final Summary \n| Model | Param id | Cross-Val Avg | Public LB | Private LB |\n| --- | --- | --- | --- | --- |\n| xgboost | xgb_1 | 0.07463 | 0.06887 | 0.06898 |\n| xgboost + pseudo | xgb_1 | 0.07421 | 0.06801 | 0.06807 |\n| xgboost | xgb_2 | 0.07363 | 0.06920 | 0.06958 |\n| xgboost + pseudo | xgb_2 | 0.07282 | 0.06807 | 0.06801 |\n| catboost | catboost_1 | 0.06748 | 0.06409 | 0.06479 |\n| catboost + pseudo | catboost_1 | **0.06571** | **0.06148** | 0.06203 |\n| catboost | catboost_2 | 0.06694 | 0.06356 | 0.06352 |\n| catboost + pseudo | catboost_2 | 0.06598 | 0.06208 | **0.06197** |","1d96b157":"# Approach\n\n### Step 1: Preprocessing \ud83d\udee0\n* Extract *year*, *weekofyear*, *month* from the *Date* column and then drop the original column.\n* Label Encode Categorical Variables of *object* type\n* Concatenate all categorical features with each other and label encode them. Use as new feature.\n\n### Step 1: Target Encoding \ud83c\udfaf\n* performed on ***year***, ***weekofyear***, ***month***, ***type***, ***region***, ***weekofyear_given***\n* Take one of the above mentioned column at a time and compute mean of *AveragePrice* for each catregory in the column.\n* Map each category in the column with it's **Mean Target Value** calculated from the above step.\n* This serves as a very basic pseudo label but tends to overfit if not utilized properly\n\nExample:\n\nIf we have the following dataframe, we will calculate target mean of each category.\n\n| category_col | target |\n| --- | --- |\n| 1 | 12 |\n| 2 | 17 |\n| 1 | 10 |\n| 2 | 18 |\n| 2 | 16 |\n| 3 | 7 |\n\n* Target Mean of category 1: (12 + 10) \/ 2 = 11\n* Target Mean of category 2: (17 + 18 + 16) \/ 17 =  17\n* Target Mean of category 3: (7) \/ 1 = 7\n\nNow the transformed dataframe should look like this:\n\n| category_col | category_target_encoded | target |\n| --- | --- | --- |\n| 1 | 11 | 12 |\n| 2 | 17 | 17 |\n| 1 | 11 | 10 |\n| 2 | 17 | 18 |\n| 2 | 17 | 16 |\n| 3 | 7  | 7 |\n\n### Step 3: Create Folds \ud83d\udcc3\n* using 10 stratified folds for cross-validation\n* each final single model is an ensemble of 10-fold models\n\n### Step 4: Pseudo-Labelling \ud83e\udd2a\n* train a model on the preprocessed data and get predictions\n* use these predictions as feature and train a model again\n* So, for each model, there will be 2 predictions that are generated: normal predictions on processed data AND predictions using 1st predictions as pseudo labels.","8a751b73":"### ML - M2\n\nName: Manan Jhaveri\n\nRoll: J025"}}