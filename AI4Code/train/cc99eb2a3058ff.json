{"cell_type":{"426aa480":"code","151e6612":"code","74d1ea7b":"code","a15000c9":"code","7fcde1c4":"code","fc1d8826":"code","acbea9bf":"code","cdc76999":"code","2b82c182":"code","14b2cb99":"code","10a5a7b2":"markdown","81294436":"markdown","a518b827":"markdown","4bafe936":"markdown","251e9f15":"markdown","10992285":"markdown","f91dca51":"markdown","a832d499":"markdown"},"source":{"426aa480":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","151e6612":"direct='3c-shared-task-influence'\n\ntrain=pd.read_csv('..\/input\/'+direct+'\/train.csv')\ntrain","74d1ea7b":"stopwords=pd.read_csv('..\/input\/smartstoplists\/SmartStoplist.txt')\nstopwords.columns=['word','njet']\nstopwords","a15000c9":"train.groupby('citation_influence_label').count()","7fcde1c4":"test=pd.read_csv('..\/input\/'+direct+'\/test.csv')\ntest\n","fc1d8826":"def XTX(total,Qlabels,Slabels):\n    from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n    from sklearn.metrics.pairwise import cosine_similarity,cosine_distances\n    from sklearn.datasets import fetch_20newsgroups\n\n    \n    #enrich with external news data...\n    placebo = fetch_20newsgroups()\n    print(\"nr placebo text\",len(placebo.data))\n\n    total['Qtxt']=''\n    for qi in Qlabels:\n        total['Qtxt']=total['Qtxt']+' '+total[qi]\n    total['Stxt']=''\n    for qi in Slabels:\n        total['Stxt']=total['Stxt']+' '+total[qi]        # error ' ' forgotten !\n    cv = TfidfVectorizer(ngram_range=(1, 1),stop_words=list(stopwords.word.values) )\n    \n    Stfidf=cv.fit_transform(total['Stxt'].append(pd.Series(placebo.data)))    \n    Qtfidf=cv.transform(total['Qtxt'].append(pd.Series(placebo.data)))  #newsdata added !\n\n    print(Qtfidf.shape)#,Stfidf.shape)\n    #words=cv.get_feature_names()\n\n    #wv=TfidfVectorizer(ngram_range=(2,3),analyzer='char_wb')\n    #wordcv=wv.fit_transform(words)\n\n    from sklearn.decomposition import PCA,TruncatedSVD,NMF,FastICA\n    #from sklearn.manifold import TSNE,Isomap,SpectralEmbedding,spectral_embedding,LocallyLinearEmbedding,MDS #limit number of records to 100000\n    #Xi=Qtfidf.dot(Stfidf.T)\n    #Xi= cosine_similarity(Qtfidf,Stfidf[:4000])\n    \n    #regress\n    Xi=cosine_similarity(Stfidf[:4000])\n    Xi=np.linalg.inv( Xi)\n    print('XTXi',Xi.shape)\n    Yi=cosine_similarity(Stfidf[:4000],Qtfidf[:4000])\n    print('Yi',Yi.shape)\n    Xi=Xi.dot(Yi)\n    print('XYi',Xi.shape)\n    # sparsity disappears with SVD\n    Ut=Xi#Ut=TruncatedSVD(n_components=750).fit_transform(Xi)\n    #Ut=np.hstack((Ut,PCA(n_components=300).fit_transform(Xi)))\n    #Ut=np.hstack((Ut,FastICA(n_components=300).fit_transform(Xi)))\n    #Ut=np.hstack((Ut,Isomap(n_components=10).fit_transform(Xi)))\n    print('compressed',Ut.shape)\n    return Ut\n\nXi=XTX(train.append(test),['citing_title','citing_author'],['cited_title','cited_author','citation_context'])","acbea9bf":"Xi=pd.DataFrame(Xi)\nXi['citation_influence_label']=train['citation_influence_label']\nXi['unique_id']=train['unique_id'].append(test['unique_id'],ignore_index=True)","cdc76999":"def kluster1(data,grbvar,label,nummercl,level):\n    from sklearn.cluster import KMeans\n    from sklearn.metrics.pairwise import cosine_similarity\n    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics import classification_report    \n    from scipy import spatial    \n    '''nummercl < ncol'''\n    submit=data[data[label].isnull()==True][[grbvar,label]]\n        \n    \n    print(label,data[label].unique())\n    simdata=data[data[label].isnull()==False].drop([label],axis=1)\n\n    #   Label encoding or remove string data\n    ytrain=data[label]\n    print(data[label].unique())    \n    #find mean per label\n    train_me=data.drop([grbvar],axis=1).groupby(label).mean()        \n    #impute NAN\n    kol=data.drop(label,axis=1).columns\n    from sklearn.experimental import enable_iterative_imputer  \n    from sklearn.impute import IterativeImputer\n    if False: #len(data.dropna())>len(data[data[label]!=np.nan]):\n        print('impute empty data')\n        data = IterativeImputer(random_state=0).fit_transform(data.drop(label,axis=1))\n    else:\n        data=data.fillna(0)\n    data = pd.DataFrame(data,columns=kol)   \n    data[label]=ytrain.values\n    \n    \n    #cosin similarity transform\n\n    print(train_me)\n    \n    simdata=data[data[label].isnull()==False].drop([grbvar,label],axis=1)\n    ytrain=data[data[label].isnull()==False][label]\n    simtest=data[data[label].isnull()==True].drop([grbvar,label],axis=1)\n    ytest=np.random.randint(0,1,size=(len(simtest), 1))  #fill data not used\n    iddata=data[grbvar]\n    #submit=data[data[label].isnull()==True][[grbvar,label]]\n    print(submit.columns,submit.describe())\n    if len(simtest)==0:   #randomsample if no empty label data\n        simtest=data.sample(int(len(simdata)*0.2))\n        ytest=simtest[label]\n        simtest=simtest.drop([grbvar,label],axis=1)\n\n    print(simdata.shape,simtest.shape,data.shape,ytrain.shape)\n    #train_se=data.groupby('label').std()\n    train_cs2=cosine_similarity(simdata,train_me)\n    test_cs2=cosine_similarity(simtest,train_me)\n    dicto={ np.round(i,1) : ytrain.unique()[i] for i in range(0, len(ytrain.unique()))} #print(clf.classes_)\n    ypred=pd.Series(np.argmax(train_cs2,axis=1)).map(dicto)\n    \n    print('cosinesimilarity direction' ,classification_report(ytrain.values, ypred)  )\n    \n    trainmu=pd.DataFrame( simdata.values-simdata.values.mean(axis=1)[:,None])\n    testmu=pd.DataFrame( simtest.values-simtest.values.mean(axis=1)[:,None])\n    \n    trainmu[label]=ytrain\n    trainme2=trainmu.groupby(label).mean()    \n    #spatial 0.79\n    def verslag(titel,yval,ypred,ypred2):\n        yval=pd.Series(yval)\n        ypred=pd.Series(ypred)\n        ypred2=pd.Series(ypred2)\n        print(len(yval.dropna()),len(yval),len(ypred),len(ypred.dropna()))\n        ypred=ypred.fillna(0)\n        ypred2=ypred2.fillna(0)\n        print(titel+'\\n', classification_report(yval,ypred )  )\n        submit[label]=[xp for xp in ypred2]\n        submit[label]=submit[label].astype('int')\n        #print(submit[[grbvar,label]])\n        #submit[grbvar]=submit[grbvar].values#.astype('int')\n        submit[[grbvar,label]].to_csv(titel+'submission.csv',index=False)\n        print(titel,submit[[grbvar,label]].groupby(label).count() )\n        return\n    \n    def adjcos_dist(size, matrix, matrixm):\n        distances = np.zeros((len(matrix),size))\n        M_u = matrix.mean(axis=1)\n        m_sub = matrix - M_u[:,None]\n        for first in range(0,len(matrix)):\n            for sec in range(0,size):\n                distance = spatial.distance.cosine(m_sub[first],matrixm[sec])\n                distances[first,sec] = distance\n        return distances\n\n    trainsp2=adjcos_dist(len(trainme2),trainmu.drop(label,axis=1).values,trainme2.values)\n    testsp2=adjcos_dist(len(trainme2),testmu.values,trainme2.values)\n    \n    print(trainsp2.shape,trainme2.shape,simdata.shape)\n    verslag('cosinesimilarity distance', ytrain, pd.Series(np.argmin(trainsp2,axis=1)).map(dicto),pd.Series(np.argmin(testsp2,axis=1)).map(dicto)  )  \n\n    return data\n\n\n\n#train2=kluster2( train.append(test,ignore_index=True),'Id','Credit Default',len(train['Credit Default'].unique() ),1)\ntrain2=kluster1(Xi[:4000],'unique_id','citation_influence_label',len(Xi['citation_influence_label'].unique() )-1,1)","2b82c182":"def kluster2(data,grbvar,label,nummercl,level):\n    '''nummercl < ncol'''\n    submit=data[data[label].isnull()==True][[grbvar,label]]\n        \n    \n    print(label,data[label].unique())\n    from sklearn.cluster import KMeans\n    from sklearn.metrics.pairwise import cosine_similarity\n    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics import classification_report    \n    from scipy import spatial\n    import time\n    import matplotlib.pyplot as plt\n    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.decomposition import PCA,TruncatedSVD,NMF,FastICA\n    from umap import UMAP  # knn lookalike of tSNE but faster, so scales up\n    from sklearn.manifold import TSNE,Isomap,SpectralEmbedding,spectral_embedding,LocallyLinearEmbedding,MDS #limit number of records to 100000\n    from sklearn.gaussian_process import GaussianProcessClassifier\n    from sklearn.gaussian_process import GaussianProcessClassifier\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.neural_network import MLPClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.svm import SVC,NuSVC\n    import xgboost as xgb\n    from lightgbm import LGBMClassifier,LGBMRegressor    \n    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n    from sklearn.linear_model import SGDClassifier\n    simdata=data[data[label].isnull()==False].drop([label],axis=1)\n\n    #   Label encoding or remove string data\n    ytrain=data[label]\n    if False: \n        from category_encoders.cat_boost import CatBoostEncoder\n        CBE_encoder = CatBoostEncoder()\n        cols=[ci for ci in data.columns if ci not in ['index',label]]\n        coltype=data.dtypes\n        featured=[ci for ci in cols]\n        ytrain=data[label]\n        CBE_encoder.fit(data[:len(simdata)].drop(label,axis=1), ytrain[:len(simdata)])\n        data=CBE_encoder.transform(data.drop(label,axis=1))\n        data[label]=ytrain\n    print(data[label].unique())    \n    #find mean per label\n    train_me=data.drop([grbvar],axis=1).groupby(label).mean()        \n    #impute NAN\n    kol=data.drop(label,axis=1).columns\n    from sklearn.experimental import enable_iterative_imputer  \n    from sklearn.impute import IterativeImputer\n    if False: #len(data.dropna())>len(data[data[label]!=np.nan]):\n        print('impute empty data')\n        data = IterativeImputer(random_state=0).fit_transform(data.drop(label,axis=1))\n    else:\n        data=data.fillna(0)\n    data = pd.DataFrame(data,columns=kol)   \n    data[label]=ytrain.values\n    \n    \n    #cosin similarity transform\n\n    print(train_me)\n    \n    simdata=data[data[label].isnull()==False].drop([grbvar,label],axis=1)\n    ytrain=data[data[label].isnull()==False][label]\n    simtest=data[data[label].isnull()==True].drop([grbvar,label],axis=1)\n    ytest=np.random.randint(0,1,size=(len(simtest), 1))  #fill data not used\n    iddata=data[grbvar]\n    #submit=data[data[label].isnull()==True][[grbvar,label]]\n    print(submit.columns,submit.describe())\n    if len(simtest)==0:   #randomsample if no empty label data\n        simtest=data.sample(int(len(simdata)*0.2))\n        ytest=simtest[label]\n        simtest=simtest.drop([grbvar,label],axis=1)\n\n    print(simdata.shape,simtest.shape,data.shape,ytrain.shape)\n    #train_se=data.groupby('label').std()\n    train_cs2=cosine_similarity(simdata,train_me)\n    test_cs2=cosine_similarity(simtest,train_me)\n    dicto={ np.round(i,1) : ytrain.unique()[i] for i in range(0, len(ytrain.unique()))} #print(clf.classes_)\n    ypred=pd.Series(np.argmax(train_cs2,axis=1)).map(dicto)\n    \n    print('cosinesimilarity direction' ,classification_report(ytrain.values, ypred)  )\n    \n    trainmu=pd.DataFrame( simdata.values-simdata.values.mean(axis=1)[:,None])\n    testmu=pd.DataFrame( simtest.values-simtest.values.mean(axis=1)[:,None])\n    \n    trainmu[label]=ytrain\n    trainme2=trainmu.groupby(label).mean()    \n    #spatial 0.79\n    def verslag(titel,yval,ypred,ypred2):\n        yval=pd.Series(yval)\n        ypred=pd.Series(ypred)\n        ypred2=pd.Series(ypred2)\n        print(len(yval.dropna()),len(yval),len(ypred),len(ypred.dropna()))\n        ypred=ypred.fillna(0)\n        ypred2=ypred2.fillna(0)\n        print(titel+'\\n', classification_report(yval,ypred )  )\n        submit[label]=[xp for xp in ypred2]\n        submit[label]=submit[label].astype('int')\n        #print(submit[[grbvar,label]])\n        #submit[grbvar]=submit[grbvar].values#.astype('int')\n        submit[[grbvar,label]].to_csv(titel+'submission.csv',index=False)\n        print(titel,submit[[grbvar,label]].groupby(label).count() )\n        return\n    \n    def adjcos_dist(size, matrix, matrixm):\n        distances = np.zeros((len(matrix),size))\n        M_u = matrix.mean(axis=1)\n        m_sub = matrix - M_u[:,None]\n        for first in range(0,len(matrix)):\n            for sec in range(0,size):\n                distance = spatial.distance.cosine(m_sub[first],matrixm[sec])\n                distances[first,sec] = distance\n        return distances\n\n    trainsp2=adjcos_dist(len(trainme2),trainmu.drop(label,axis=1).values,trainme2.values)\n    testsp2=adjcos_dist(len(trainme2),testmu.values,trainme2.values)\n    \n    print(trainsp2.shape,trainme2.shape,simdata.shape)\n    verslag('cosinesimilarity distance', ytrain, pd.Series(np.argmin(trainsp2,axis=1)).map(dicto),pd.Series(np.argmin(testsp2,axis=1)).map(dicto)  )  \n    # blended with three classifiers random Forest\n    classifier=[LGBMClassifier(),\n                RandomForestClassifier(n_jobs=4),\n                KNeighborsClassifier(n_neighbors=3),  #0.67\n                xgb.XGBClassifier(n_estimators=50, max_depth = 9, learning_rate=0.01, subsample=0.75, random_state=11,n_jobs=4), #0.59        \n                QuadraticDiscriminantAnalysis(),  #0.5        \n                AdaBoostClassifier(), #0.46        \n                PCA(n_components=nummercl*3,random_state=0,whiten=True),  #0.44\n                TruncatedSVD(n_components=nummercl*3, n_iter=7, random_state=42), #0.44\n                GaussianNB(),  #0.44\n                LogisticRegression(n_jobs=4),        #use binary classification\n                MLPClassifier(alpha=1, max_iter=1000), #0.37\n                FastICA(n_components=nummercl,random_state=0),  #0.35  use when consecutiverelationship        \n                #SVC(probability=True), #0.32        \n                #Isomap(n_components=nummercl),\n\n    ]\n    simdata2=np.hstack((train_cs2,trainsp2))\n    simtest2=np.hstack((test_cs2,testsp2))\n    kol2=['x'+str(xi) for xi in range(nummercl)]+['y'+str(xi) for xi in range(nummercl)]\n    for clf in classifier:\n        #clf = RandomForestClassifier(n_jobs=4) #GaussianProcessClassifier()#\n        print(simdata.shape,ytrain.shape,simtest.shape,data.shape,simdata2.shape,simtest2.shape)\n        #print(simtest2)\n\n        try:\n            clf.fit(simdata, ytrain)\n            train_tr=clf.predict_proba(simdata)\n            test_tr=clf.predict_proba(simtest)\n        except:\n            clf.fit(simdata.append(simtest))\n            train_tr=clf.transform(simdata)\n            test_tr=clf.transform(simtest)\n            \n        #dicto={ i : clf.classes_[i] for i in range(0, len(clf.classes_) ) } #print(clf.classes_)\n        ypred=pd.Series(np.argmax(train_tr,axis=1)).map(dicto)\n        ypred2=pd.Series(np.argmax(test_tr,axis=1)).map(dicto)\n        verslag('1'+str(clf)[:5]+'class' ,ytrain, ypred,ypred2  )\n        simdata2=np.hstack((simdata2,train_tr))\n        simtest2=np.hstack((simtest2,test_tr))\n        kol2=kol2+[str(clf)[:3]+str(xi) for xi in range(train_tr.shape[1])]\n    #concat data\n    simdata=pd.DataFrame(simdata2,columns=kol2)\n    simtest=pd.DataFrame(simtest2,columns=kol2)\n\n    #plotimg2=pd.DataFrame(train_cs2,columns=['x'+str(xi) for xi in range(nummercl)])\n    nummercl=3\n    clusters = [\n                PCA(n_components=nummercl*10,random_state=0,whiten=True),\n                TruncatedSVD(n_components=nummercl*10, n_iter=7, random_state=42),\n                FastICA(n_components=nummercl*50,random_state=0),\n                Isomap(n_components=nummercl*30),\n                #LocallyLinearEmbedding(n_components=nummercl),\n                SpectralEmbedding(n_components=nummercl),\n                #MDS(n_components=nummercl),\n                TSNE(n_components=nummercl,random_state=0),\n                UMAP(n_neighbors=3,n_components=nummercl, min_dist=0.3,metric='minkowski'),\n                #grbvarNMF(n_components=nummercl,random_state=0),                \n                ] \n    clunaam=['PCA','tSVD','ICA','Iso','Spectr','tSNE','UMAP','NMF']\n    \n    #clf = RandomForestClassifier()\n    #from sklearn.linear_model import SGDClassifier\n    #clf= SGDClassifier(max_iter=5)\n    #classifier after clustering\n    clf=xgb.XGBClassifier(n_estimators=50, max_depth = 9, learning_rate=0.01, subsample=0.75, random_state=11,n_jobs=4)\n    clf=LGBMClassifier()\n    clf.fit(simdata, ytrain)\n        \n    verslag('2xgb pure',ytrain,pd.Series(clf.predict(simdata)),pd.Series(clf.predict(simtest))  )\n            \n    for cli in clusters:\n        print(cli)\n        clunm=clunaam[clusters.index(cli)] #find naam\n        \n        if str(cli)[:3]=='NMF':\n            maxmin=np.array([simdata.min(),simtest.min()])\n            simdata=simdata-maxmin.min()+1\n        svddata = cli.fit_transform(simdata.append(simtest))  #totale test\n        \n        km = KMeans(n_clusters=nummercl, random_state=0)\n        km.fit_transform(svddata)\n        cluster_labels = km.labels_\n        cluster_labels = pd.DataFrame(cluster_labels, columns=[label])\n        #print(cluster_labels.shape) # train+test ok\n        pd.DataFrame(svddata[:len(simdata)]).plot.scatter(x=0,y=1,c=ytrain.values,colormap='viridis')\n        clf.fit(svddata[:len(simdata)], ytrain)\n        verslag('3'+clunm+'+lgbm reduced',ytrain,pd.Series(clf.predict(svddata[:len(simdata)])),pd.Series(clf.predict(svddata[len(simdata):])))        \n        \n    \n        plt.show()\n\n        #clusdata=pd.concat([pd.DataFrame(grbdata.reset_index()[grbvar]), cluster_labels], axis=1)\n        #if len(grbdata)<3: \n        #    data['Clu'+clunm+str(level)]=cluster_labels.values\n            \n        #else:\n        #    data=data.merge(clusdata,how='left',left_on=grbvar,right_on=grbvar)\n        confmat=confusion_matrix ( ytrain,cluster_labels[:len(simdata)])\n        dicti={}\n        for xi in range(len(confmat)):\n            #print(np.argmax(confmat[xi]),confmat[xi])\n            dicti[xi]=np.argmax(confmat[xi])\n        #print(dicti)\n        #print('Correlation\\n',confusion_matrix ( ytrain,cluster_labels[:len(ytrain)]))\n        #print(clunm+'+kmean clusterfit', classification_report(ytrain.map(dicti), cluster_labels[:len(simdata)])  )   \n        invdict = {np.round(value,1): key for key, value in dicti.items()}\n        #print(invdict)\n        #submit[label]=cluster_labels[len(simdata):].values\n        #print(cluster_labels[len(simdata):])\n        #print(submit.describe().T)\n        #ytest=submit[label].astype('int')\n\n        #submit[label]=ytest.map(invdict)#.astype('int')\n        #submit[[grbvar,label]].to_csv('submit'+str(cli)[:5]+'kmean.csv',index=False)\n        #print('kmean'+str(cli)[:10],submit[[grbvar,label]].groupby(label).count() )        \n    return data\n\n\n\n#train2=kluster2( train.append(test,ignore_index=True),'Id','Credit Default',len(train['Credit Default'].unique() ),1)\ntrain2=kluster2(Xi[:4000],'unique_id','citation_influence_label',len(Xi['citation_influence_label'].unique() )-1,1)","14b2cb99":"def verslag(titel,label2,yval,ypred,ypred2,mytrain):\n        from sklearn.metrics import classification_report    \n        yval=pd.Series(yval)\n        ypred=pd.Series(ypred)\n        ypred2=pd.Series(ypred2)\n        print('shape yval\/dropna ypred\/dropna',yval.dropna().shape,yval.shape,ypred.shape,ypred.dropna().shape)\n        ypred=ypred.fillna(0)\n        ypred2=ypred2.fillna(0)\n        print(titel+'\\n', classification_report(yval,ypred )  )\n        #print(mytrain)\n        vsubmit = pd.DataFrame({        label2[0]: mytrain[len(yval):][label2[0]].values,        label2[1]: ypred2    })\n        #vsubmit = pd.DataFrame({        label2[0]: mytrain[len(yval):].reset_index().index,        label2[1]: ypred2    })\n \n        \n        #print(vsubmit)\n        #print(label2,label2[0],label2[1 ],vsubmit.shape,vsubmit.head(3))\n        vsubmit[label2[1]]=vsubmit[label2[1]].astype('int')#-1\n        print('submission header',vsubmit.head())\n        vsubmit[label2].to_csv(titel+'submission.csv',index=False)\n        print(titel,vsubmit[label2].groupby(label2[1]).count() )\n        return  \n\n    \ndef ensemblecluster(data,grbvar,label,nummerclas,level):\n    '''nummercl < ncol'''\n    nummercl=nummerclas\n    print('nummerclust',nummercl)\n    from sklearn.cluster import KMeans\n    from sklearn.metrics.pairwise import cosine_similarity\n    from sklearn.metrics import confusion_matrix\n    from sklearn.metrics import classification_report    \n    from scipy import spatial\n    import time\n    import matplotlib.pyplot as plt\n    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.decomposition import PCA,TruncatedSVD,NMF,FastICA\n    from umap import UMAP  # knn lookalike of tSNE but faster, so scales up\n    from sklearn.manifold import TSNE,Isomap,SpectralEmbedding,spectral_embedding,LocallyLinearEmbedding,MDS #limit number of records to 100000\n    from sklearn.gaussian_process import GaussianProcessClassifier\n    from sklearn.gaussian_process import GaussianProcessClassifier\n    from sklearn.neighbors import KNeighborsClassifier\n    from sklearn.neural_network import MLPClassifier\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.svm import SVC,NuSVC\n    import xgboost as xgb\n    from lightgbm import LGBMClassifier,LGBMRegressor     \n    from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n    from sklearn.naive_bayes import GaussianNB\n    from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n    from sklearn.linear_model import SGDClassifier\n    from sklearn.multiclass import OneVsRestClassifier\n    from sklearn.linear_model import Perceptron\n    simdata=data[data[label].isnull()==False].drop([grbvar,label],axis=1)\n    from sklearn.gaussian_process import GaussianProcessClassifier\n    from sklearn.gaussian_process.kernels import RBF\n    from sklearn.metrics import f1_score\n    from sklearn.ensemble import VotingClassifier,BaggingClassifier\n    \n    #scaling\n    from sklearn.preprocessing import MinMaxScaler,MaxAbsScaler,StandardScaler,RobustScaler,Normalizer,QuantileTransformer,PowerTransformer\n    scalers=[Dummy(1),\n            MinMaxScaler(),\n            MaxAbsScaler(),\n            StandardScaler(),\n            RobustScaler(),\n            Normalizer(),\n            QuantileTransformer(output_distribution='uniform'),\n            PowerTransformer(),\n           ]\n    # cluster techniques\n    nummercl=int( (data.shape[1])**0.8)\n    print('reduce data to',nummercl)\n    clusters = [Dummy(1),\n                PCA(n_components=nummercl,random_state=0,whiten=True),\n                TruncatedSVD(n_components=nummercl, n_iter=7, random_state=42),\n                FastICA(n_components=nummercl*2,random_state=0),\n                #Isomap(n_components=nummercl*10),\n                #LocallyLinearEmbedding(n_components=nummercl*2),\n                SpectralEmbedding(n_components=nummercl*2),\n                #MDS(n_components=nummercl),\n                TSNE(n_components=3,random_state=0),\n                UMAP(n_neighbors=nummercl*1,n_components=10, min_dist=0.3,metric='minkowski'),\n                #NMF(n_components=nummercl,random_state=0),                \n                ] \n    #classifier techniques\n    classifiers=[LGBMClassifier(),\n                 OneVsRestClassifier(LogisticRegression(),n_jobs=4),\n                 \n                VotingClassifier(estimators=[('rf', RandomForestClassifier(n_jobs=4)), ('xg',xgb.XGBClassifier(n_jobs=4)),('lr',LogisticRegression(n_jobs=4)) ,('kn', KNeighborsClassifier(n_neighbors=3))], voting='soft',n_jobs=4),\n                BaggingClassifier(base_estimator=xgb.XGBClassifier(),n_jobs=4),\n                LogisticRegression(n_jobs=4),\n                #xgb.XGBClassifier(n_estimators=50, max_depth = 9, learning_rate=0.01, subsample=0.75, random_state=11,n_jobs=4),\n                xgb.XGBClassifier( learning_rate=0.02, max_delta_step=0, max_depth=10, min_child_weight=0.1, missing=None, n_estimators=250, nthread=4,objective='binary:logistic', reg_alpha=0.01, reg_lambda = 0.01,scale_pos_weight=1, seed=0, silent=False, subsample=0.9),\n                #GaussianProcessClassifier(kernel=1.0 * RBF(1.0), random_state=0),\n                SVC(probability=True),\n                KNeighborsClassifier(n_neighbors=3),\n                #PassiveAggressiveClassifier(max_iter=50, tol=1e-3,n_jobs=-1),    \n                #Perceptron(n_jobs=4),\n                SGDClassifier(n_jobs=4),\n                #MLPClassifier(alpha=1, max_iter=1000),\n                AdaBoostClassifier(),\n                GaussianNB(),\n                #QuadraticDiscriminantAnalysis()\n            ]    \n    ydata=data[[grbvar,label]]\n    \n    if False:\n        \n        kolom=data.describe().T\n        from sklearn.preprocessing import OneHotEncoder\n        toencode=[ci for ci in data.columns if ci not in kolom.index]\n        ohe=OneHotEncoder()\n        data2=ohe.fit_transform(data[toencode].fillna('')).toarray()\n        data=data.drop(toencode,axis=1)\n        #from sklearn.preprocessing import OneHotEncoder\n        #data2=OneHotEncoder().fit_transform(data[['Gender']]).toarray()\n        tel=0\n        for ci in ohe.get_feature_names():\n            data[ci]=data2[:,tel]\n            tel+=1\n    #add random columns\n    for xi in range(3):\n        labnm='rand'+str(xi)\n        data[labnm]=np.random.randint(0,1,size=(len(data), 1))\n    kol=data.drop([grbvar,label],axis=1).columns    \n    #check to impute\n    #data=data.fillna(data.mean())\n    null_columns=data.columns[data.isnull().any()]\n    print('what is null \\n',data[null_columns].isnull().sum()) \n    if len(null_columns)==0:\n        from sklearn.model_selection import train_test_split\n        simdata,simtest,ytrain,ytest=train_test_split(data.drop([grbvar,label],axis=1),data[[grbvar,label]],test_size=0.1)\n        print('splitted',simdata.shape,simtest.shape,ytrain.shape,ytest.shape)\n    else:\n        simdata=data[data[label].isnull()==False].drop([grbvar,label],axis=1).fillna(data.mean())\n        ytrain=ydata[ydata[label].isnull()==False][[grbvar,label]] # exceptional has to convert to integer for this\n        simtest=data[data[label].isnull()==True].drop([grbvar,label],axis=1).fillna(data.mean())\n        ytest=pd.DataFrame(np.random.randint(0,1,size=(len(simtest), 1)),columns=[label])  #fill data not used\n    #iddata=data[grbvar]\n    #submit=data[data[label].isnull()==True][[grbvar,label]]\n    #ydata=ytrain.append(ytest)\n    #data=\n    #data=XTX(data,grbvar,)\n    resul=[]\n    \n    def scalclusclas(scai,clusi,clasi,verbose):\n        mdata = scai.fit_transform(simdata.append(simtest))\n        svddata = clusi.fit_transform(mdata)\n        naam=str(clasi)[:10]+str(clusi)[:7]+str(scai)[:10]        \n        if verbose:\n            clasi.fit(svddata[:len(simdata)],ytrain[label])\n            train_tr=clasi.predict(svddata[:len(simdata)])  \n            test_tr=clasi.predict(svddata[len(simdata):])\n            f1sc=f1_score(ytrain[:len(simdata)][label],train_tr, average=None)\n            f1te=f1_score(ytest[label],np.round(test_tr), average=None)\n            \n        else:\n            pointer=int(len(simdata)*.2)\n            clasi.fit(svddata[:len(simdata)-pointer],ytrain[:len(simdata)-pointer][label])\n            train_tr=clasi.predict(svddata[:len(simdata)-pointer])  \n            test_tr=clasi.predict(svddata[len(simdata)-pointer:len(simdata)])\n            f1sc=f1_score(ytrain[:len(simdata)-pointer][label],train_tr, average=None)\n            f1te=f1_score(ytrain[len(simdata)-pointer:len(simdata)][label],np.round(test_tr), average=None)\n        resul=[naam]+[xi for xi in f1sc]+[xi for xi in f1te]\n        if verbose:\n            verslag('3_'+naam,[grbvar,label],ytrain[:len(simdata)][label],train_tr,test_tr,ydata)\n        print(naam,resul)\n        return resul\n        \n    print('________________find best scaler')\n    resultsc=[]\n    for scai in scalers:\n        for clusi in clusters[:1]:\n            for clasi in classifiers[:1]:\n                resultsc.append(scalclusclas(scai,clusi,clasi,False))\n    resultsc=pd.DataFrame(resultsc)\n    resultsc['som']=resultsc.iloc[:,nummerclas+1:].sum(axis=1)\/nummerclas  #nummerclas\n    print(resultsc)\n    maxscale=resultsc.sort_values('som')[-3:]\n    print(maxscale)\n    maxscale=[xi for xi in maxscale.index]\n    print(maxscale)\n    print('________________find best cluster')\n    resultlu=[]    \n    for scai in scalers[maxscale[2]:maxscale[2]+1]:\n        for clusi in clusters:\n            for clasi in classifiers[:1]:\n                resultlu.append(scalclusclas(scai,clusi,clasi,False))\n    resultlu=pd.DataFrame(resultlu)\n    resultlu['som']=resultlu.iloc[:,nummerclas+1:].sum(axis=1)\/nummerclas #nummerclas\n    print(resultlu)\n    maxclus=resultlu.sort_values('som')[-3:]\n    print(maxclus)\n    maxclus=[xi for xi in maxclus.index]\n    print(maxclus)\n    print('________________find best classifier')\n    resultla=[]    \n    for scai in scalers[maxscale[2]:maxscale[2]+1]:\n        for clusi in clusters[maxclus[2]:maxclus[2]+1]:\n            for clasi in classifiers:\n                resultla.append(scalclusclas(scai,clusi,clasi,False))\n    resultla=pd.DataFrame(resultla)\n    resultla['som']=resultla.iloc[:,nummerclas+1:].sum(axis=1)\/nummerclas #nummerclas\n    maxclas=resultla.sort_values('som')[-3:]\n    maxclas=[xi for xi in maxclas.index]\n    print(maxclas)\n\n   \n    results=[]\n    for scai in maxscale:\n        for clusi in maxclus:\n            for clasi in maxclas:\n                results.append(scalclusclas(scalers[scai],clusters[clusi],classifiers[clasi],True))\n              \n\n    zoek=pd.DataFrame(results)\n    zoek= zoek.append(resultsc).append(resultlu).append(resultla)\n    zoek['som']=zoek.iloc[:,nummercl+1:].sum(axis=1)\/nummercl\n    \n    print(zoek.sort_values(['som']))            \n    \n\n    return zoek\n\n\n#Custom Transformer that extracts columns passed as argument to its constructor \nclass Dummy( ):\n    #Class Constructor \n    def __init__( self, feature_names ):\n        self._feature_names = feature_names \n    \n    #Return self nothing else to do here    \n    def fit( self, X, y = None ):\n        return self \n    \n    #Method that describes what we need this transformer to do\n    def fit_transform( self, X, y = None ):\n        return X \n#zoek=ensemblecluster(mtotal[:len(train)],'Date','Category',len(train['Category'].unique() ),1)\n#zoek=ensemblecluster(train,'index','Lable',len(total['Lable'].unique() ),1)\n#\nzoek=ensemblecluster(Xi[:4000],'unique_id','citation_influence_label',len(Xi['citation_influence_label'].unique() )-1,1)","10a5a7b2":"# Kluster2\n\nthe version used in the competition, where the 'search' with classifiers \n","81294436":"# Kluster function\n\nthe simplified version\n\n1. find mean per label\n   train_me=data.drop([grbvar],axis=1).groupby(label).mean()        \n   this averages the cosine similarity of the previous function on the dataset Xi for alle label == 0 or 1\n   \n2. compare the cosin distance and the cosine similarity \n   with argmin, we find the best solution...\n","a518b827":"# swiss sacknife\n\n* ensemble method\n* gridsearch best method:  scaling - clustering - classify\n\n","4bafe936":"# This classification is about finding \ntitles in a citing-titles in a citation-context\n\n","251e9f15":"Adding label and text id again to the dataset","10992285":"# Smartstoplist\n\n\na better stopword list instead of the standard list","f91dca51":"# how many 0 \/ 1 labels ?","a832d499":"# function XTX\n\n* concatenate the 'title - author' fields to a searchfield, and the 'cited title\/author\/context fields' to one searchfield\n* generate the TFIDF matrix for the 'Search corpus' \n* ENRICH the word text corpus with an external news corpus  fetch_20newsgroups\n* use smartstopwordslist asd stopwords for the Tfidf omitting a larger group of 'non relevant stopwords' \n* the TFIDF matrix for the questions\n* dotmultiply Qtfidf with the Stfidf  matrix  and tSVD the result to make the matrix more compact..\n* OR use cosine_simlarity between QTfidf and STfidf"}}