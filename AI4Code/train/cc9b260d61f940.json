{"cell_type":{"592d580d":"code","b3ab0a4f":"code","044e6725":"code","ca6ef02c":"code","61f818b9":"code","c79373b0":"code","060811f7":"code","50b45ef4":"code","d8db2ed8":"code","d849256f":"code","f4411ceb":"code","74e55902":"code","fba52013":"code","def0c8e0":"code","959610c4":"code","c80be539":"code","51546053":"code","ca1d1a71":"code","81685510":"code","845d94e7":"code","95e1c669":"code","c2d7c410":"code","5696f4c4":"code","7eb2c573":"code","b0144ad5":"code","cb973147":"code","673d196f":"code","b2a34004":"code","3d8d3944":"code","f003c421":"code","7d34abda":"code","c406ee15":"code","0b85e77f":"code","8d5e981e":"code","c97cb43d":"code","f0b7dc89":"code","a7cf18b6":"code","fe4bcc9b":"code","44202612":"code","482acc95":"code","1a544b75":"code","d3322822":"code","e9de98ca":"code","204de72b":"code","6f7c17cf":"code","def24d5e":"code","7aae849b":"code","7ae2015b":"code","4b4e8fde":"code","f8a57c67":"code","23281502":"code","817baa11":"markdown","b31b9737":"markdown","58232eb4":"markdown","00e82b99":"markdown","e403543d":"markdown","ae69b703":"markdown","4c10aaa3":"markdown","9baf8dcf":"markdown","8650b13a":"markdown","7fd9257f":"markdown","b29e3cdf":"markdown","3ab20515":"markdown","22558b44":"markdown","fe29b6c5":"markdown","f4d4962c":"markdown","c850e544":"markdown"},"source":{"592d580d":"import os\nimport pandas as pd\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nimport math\nimport re\nimport warnings\nfrom itertools import combinations\nfrom datetime import datetime\n\nwarnings.filterwarnings('ignore')\n\nimport scipy\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.metrics import plot_roc_curve, auc, roc_curve\n\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder, MinMaxScaler, StandardScaler\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport itertools\n\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostClassifier, CatBoostRegressor, Pool\n\ntqdm.pandas()","b3ab0a4f":"def seed_all(seed):\n    np.random.seed(seed)\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    \nseed = 42 \nseed_all(seed)","044e6725":"train = pd.read_csv('..\/input\/klps-creditscring-challenge-for-students\/train.csv')\ntest = pd.read_csv('..\/input\/klps-creditscring-challenge-for-students\/test.csv')\n\nprint(len(train), len(test))","ca6ef02c":"train.describe()","61f818b9":"partner_cols = [x for x in train.columns if x.startswith(\"partner\")]\n\nboolean_features = ['Field_38', 'Field_39', 'Field_41', 'Field_42', \n                   'Field_50', 'Field_51', 'Field_53', 'Field_73',\n                   ] + partner_cols","c79373b0":"def plot_missing_values(df):\n\n    cols = df.columns\n    count = [df[col].isnull().sum() for col in cols]\n    percent = [i\/len(df) for i in count]\n    missing = pd.DataFrame({'proportion': percent}, index=cols)\n    missing = missing.sort_values(by='proportion', ascending=False)\n    plt.figure(figsize=(20,40))\n    plt.title(f'Missing values on each columns')\n    ax = sns.barplot(missing['proportion'], missing.index)\n\n    for i, p in enumerate(ax.patches):\n        ax.text(p.get_x()+p.get_width()+2e-2, p.get_y()+p.get_height(), f\"{missing.iloc[i]['proportion']:.2f}%\", ha='center')\n        \n    mean = np.mean(missing['proportion'])\n    std = np.std(missing['proportion'])\n    plt.ylabel('Columns')\n    plt.plot([], [], ' ', label=f'Average missing values: {mean:.2f} \\u00B1 {std:.2f}')\n    plt.legend()\n    plt.show()\n\n    return missing, missing.index.tolist()","060811f7":"missing_train, train_cols = plot_missing_values(train)","50b45ef4":"missing_test, test_cols = plot_missing_values(test)","d8db2ed8":"train = train[train_cols]\ntest = test[test_cols]","d849256f":"def job_category(x):\n    if type(x) == str:\n        if \"c\u00f4ng nh\u00e2n\" in x or \"cnh\u00e2n\" in x or \"cnv\" in x or \"th\u1ee3 may\" in x or \"cn\" in x or \"may c\u00f4ng nghi\u1ec7p\" in x or \"v\u1ec7 sinh\" in x \\\n        or \"th\u1ee3 s\u01a1n\" in x or \"th\u1ee3\" in x or \"ph\u1ee5\" in x or \"b\u1ea3o tr\u00ec\" in x or \"lao \u0111\u1ed9ng\" in x or \"c\u1ee5ng nh\u00f5n ch?t\" in x:\n            return \"CN\"\n        elif \"gi\u00e1o vi\u00ean\" in x:\n            return \"GV\"\n        elif \"k\u1ef9 s\u01b0\" in x or \"ki\u1ebfn tr\u00fac s\u01b0\" in x:\n            return \"KS\"\n        elif \"y s\u1ef9\" in x or \"y t\u1ebf\" in x or \"b\u00e1c s\u1ef9\" in x or \"d\u01b0\u1ee3c t\u00e1\" in x or \"d\u01b0\u1ee3c\" in x or \"y s\u0129\" in x or \"y\" in x or \"\u0111i\u1ec1u d\u01b0\u1ee1ng\" in x:\n            return \"YTE\"\n        elif \"qu\u1ea3n l\u00fd\" in x or \"gi\u00e1m \u0111\u1ecbnh\" in x or \"b\u00ed th\u01b0\" in x or \"c\u00e1n b\u1ed9\" in x or \"gi\u00e1m s\u00e1t\" in x or \"c\u1ecfn b\u1ed9\" in x \\\n        or \"tr\u01b0\u1edfng ph\u00f2ng\" in x or \"gi\u00e1m \u0111\u1ed1c\" in x or \"qu\u1ea3n \u0111\u1ed1c\" in x or \"c\u00e1n s\u1ef1\" in x or \"ph\u00f3 ph\u00f2ng\" in x \\\n        or \"tr\u01b0\u1edfng\" in x or \"ph\u00f3\" in x or \"ch\u1ec9 huy\" in x:\n            return \"QL\"\n        elif \"nh\u00e2n vi\u00ean\" in x or \"k\u1ebf to\u00e1n\" in x or \"nv\" in x or \"chuy\u00ean vi\u00ean\" in x or \"th\u01b0 k\u00fd\" in x \\\n        or \"b\u1ea3o v\u1ec7\" in x or \"k\u1ef9 thu\u1eadt vi\u00ean\" in x or \"ktv\" in x or \"tr\u1ee3 l\u00fd\" in x or \"ti\u1ebfp vi\u00ean\" in x or \"k\u1ef7 thu\u1eadt vi\u00ean\" in x or \"nh\u00f5n vi\u1eddn an ninh\" in x or \"k\u0129 thu\u1eadt vi\u00ean\" in x or \"giao d\u1ecbch vi\u00ean\" in x:\n            return \"NV\"\n        elif \"t\u00e0i x\u1ebf\" in x or \"l\u00e1i xe\" in x:\n            return \"TX\"\n        elif \"undefined\" in x:\n            return np.nan\n        else:\n            return x\n    else:\n        return x\n\ndef job_category2(x):\n    if type(x) == str:\n        if \"c\u00f4ng nh\u00e2n\" in x or \"cnv\" in x or \"cn\" in x or \"may c\u00f4ng nghi\u1ec7p\" in x or \"cnh\u00e2n\" in x or \"cnv\" in x or \"lao \u0111\u1ed9ng\" in x\\\n        or \"th\u1ee3\" in x or \"co\u00f5ng nha\u00f5n tr\u1eed\u00f9c tie\u1ecfp ma\u1ef1y may co\u00f5ng nghie\u1ecdp\" in x or \"c.n\" in x or \"l\u0111\" in x or \"may c\u00f4ng nghi\u1ec7p\" in x or \"v\u1ec7 sinh\" in x\\\n        or \"th\u1ee3 s\u01a1n\" in x or \"th\u1ee3\" in x or \"ph\u1ee5\" in x or \"b\u1ea3o tr\u00ec\" in x or \"lao \u0111\u1ed9ng\" in x or \"c\u1ee5ng nh\u00f5n ch?t\" in x:\n            return \"CN\"\n        elif \"gi\u00e1o vi\u00ean\" in x or \"gv\" in x or \"g\u00edao vi\u00ean\" in x:\n            return \"GV\"\n        elif \"nh\u00e2n vi\u00ean\" in x or \"k\u1ebf to\u00e1n\" in x or \"c\u00e1n b\u1ed9\" in x or \"nv\" in x or \"cb\" in x or \"nh\u00f5n vi\u1eddn\" in x:\n            return \"NV\"\n        elif \"t\u00e0i x\u1ebf\" in x or \"l\u00e1i\" in x or \"t\u00e0i x\u00ea\" in x:\n            return \"TX\"\n        elif \"qu\u1ea3n l\u00fd\" in x or \"ph\u00f3 ph\u00f2ng\" in x or \"hi\u1ec7u ph\u00f3\" in x or \"gi\u00e1m \u0111\u1ecbnh\" in x or \"gi\u00e1m s\u00e1t\" in x:\n            return \"QL\"\n        elif \"undefined\" in x:\n            return \"missing\"\n        elif \"gi\u00e1m \u0111\u1ed1c\" in x or \"hi\u1ec7u tr\u01b0\u1edfng\" in x or \"qu\u1ea3n \u0111\u1ed1c\" in x:\n            return \"G\u0110\"\n        elif \"ph\u1ee5c v\u1ee5\" in x:\n            return \"PV\"\n        elif \"chuy\u00ean vi\u00ean\" in x:\n            return  \"CV\"\n        elif \"b\u00e1c s\u0129\" in x or \"d\u01b0\u1ee3c s\u0129\" in x or \"y s\u0129\" in x or \"y s\u1ef9\" in x or \"d\u01b0\u1ee3c t\u00e1\" in x:\n            return \"BS\"\n        elif \"y t\u00e1\" in x:\n            return \"YT\"\n        elif \"h\u1ed9 sinh\" in x:\n            return \"HS\"\n        elif \"ch\u1ee7 t\u1ecbch\" in x:\n            return \"CT\"\n        elif \"b\u1ebfp\" in x:\n            return \"\u0110B\"\n        elif \"s\u01b0\" in x:\n            return \"KS\"\n        elif \"d\u01b0\u1ee1ng\" in x:\n            return \"\u0110D\"\n        elif \"k\u1ef9 thu\u1eadt\" in x or \"k\u0129 thu\u1eadt\" in x or \"k\u1ef7 thu\u1eadt vi\u00ean\" in x:\n            return \"KTV\"\n        elif \"di\u1ec5n vi\u00ean\" in x:\n            return \"DV\"\n        else:\n            return \"missing\"\n    else:\n        return x    \n    \nfor df in [train, test]:\n  df[\"maCv\"] = df[\"maCv\"].str.lower()\n  df[\"maCv\"] = df[\"maCv\"].apply(job_category2)\n\n#train = train.drop('maCv', axis = 1)\n#test = test.drop('maCv', axis = 1)\n\ndef clean_38(x):\n    if x == \"DN\" or x == \"TN\":\n        return np.nan\n    else:\n        return x\n\n#train['Field_38'] = train['Field_38'].apply(clean_38)\n#test['Field_38'] = test['Field_38'].apply(clean_38)\n\ndef clean_47(x):\n    if x == \"Zezo\":\n        return 0\n    elif x == \"One\":\n        return 1\n    elif x == \"Two\":\n        return 2\n    elif x == \"Three\":\n        return 3\n    elif x == \"Four\":\n        return 4\n    else:\n        return x\n    \n#train['Field_47'] = train['Field_47'].apply(clean_47)\n#test['Field_47'] = test['Field_47'].apply(clean_47)\n\ndef clean_54(x):\n    if x == \" P. T\u00e2n Ph\u00fa\":\n        return \"VN\"\n    elif x == \"1\" or x == \"01\":\n        return \"None\"\n    else:\n        return x\n\n#train['Field_54'] = train['Field_54'].apply(clean_54)\n#test['Field_54'] = test['Field_54'].apply(clean_54)\n\ndef clean_62(x):\n    if x == \"Ngo\u00e0i qu\u1ed1c doanh Qu\u1eadn 7\":\n        return np.nan\n    elif x == \"I\":\n        return 1\n    elif x == \"II\":\n        return 2\n    elif x == \"III\":\n        return 3\n    elif x == \"IV\":\n        return 4\n    elif x == \"V\":\n        return 5\n    else:\n        return x\n\n#train['Field_62'] = train['Field_62'].apply(clean_62)\n#test['Field_62'] = test['Field_62'].apply(clean_62)\n\ndef str_normalize(s):\n    s = str(s).strip().lower()\n    s = re.sub(' +', \" \", s)\n    return s\n\ndef process_location(df):\n    for col in [\"currentLocationLocationId\", \"homeTownLocationId\", \"currentLocationLatitude\", \"currentLocationLongitude\", \n                   \"homeTownLatitude\", \"homeTownLongitude\"]:\n        df[col].replace(0, np.nan, inplace=True)\n\n    df[\"currentLocationLocationId\"] = df[\"currentLocationLocationId\"].apply(str_normalize).astype(\"category\")\n    df[\"homeTownLocationId\"] = df[\"homeTownLocationId\"].apply(str_normalize).astype(\"category\")\n\n    return df\n\ndef clean_gioiTinh(x):\n    if x == \"male\" or x == \"MALE\":\n        return 2\n    elif x == \"female\" or x == \"FEMALE\":\n        return 1\n    else:\n        return x\n    \n#for df in [train, test]:\n#  for col in ['info_social_sex', 'gioiTinh']:\n#    df[col] = df[col].apply(clean_gioiTinh)\n\ndef process_misc(df):        \n    df[\"subscriberCount\"].replace(0, np.nan, inplace=True)\n    df[\"friendCount\"].replace(0, np.nan, inplace=True)\n    \n    df[\"Field_13\"] = df[\"Field_13\"].apply(lambda x: 1 if x == x else 0)\n    df[\"Field_38\"] = df[\"Field_38\"].map({0: 0.0, 1: 1.0, \"DN\": np.nan, \"TN\": np.nan, \"GD\": np.nan})\n    df[\"Field_62\"] = df[\"Field_62\"].map({\"I\": 1, \"II\": 2, \"III\": 3, \"IV\": 4, \"V\": 5, \"Ngo\u00e0i qu\u1ed1c doanh Qu\u1eadn 7\": np.nan})\n    df[\"Field_47\"] = df[\"Field_47\"].map({\"Zezo\": 0, \"One\": 1, \"Two\": 2, \"Three\": 3, \"Four\": 4})\n    \n    df[\"Field_27\"] = df[\"Field_27\"].replace({0.0: np.nan})\n    df[\"Field_28\"] = df[\"Field_28\"].replace({0.0: np.nan})\n    df['Field_54'] = df['Field_54'].apply(clean_54)\n        \n    #for col in df.columns:\n    #    if df[col].dtype.name == \"object\":\n    #        df[col] = df[col].apply(str_normalize).astype(\"category\")\n            \n    return df\n\nfor df in [train, test]:\n  df = process_misc(df)\n\ndef combine_gender(s):\n    x, y = s\n    \n    if x != x and y != y:\n        return \"nan\"\n    \n    if x != x:\n        return y.lower()\n    \n    return x.lower()\n\ndef process_gender(df):\n    df[\"gender\"] = df[[\"gioiTinh\", \"info_social_sex\"]].apply(combine_gender, axis=1).astype(\"category\")\n    return df\n\n# Process date\/datetime fields\nDATE = [\"Field_{}\".format(i) for i in [5, 6, 7, 8, 9, 11, 15, 25, 32, 33, 34, 35, 40]]\nDATETIME = [\"Field_{}\".format(i) for i in [1, 2, 43, 44]]\n\ndef correct_34_ngaysinh(s):\n    if s != s:\n        return np.nan\n    try:\n        s = int(s)\n    except ValueError:\n        s = s.split(\" \")[0]\n        \n    return datetime.strptime(str(s)[:6], \"%Y%m\")\n\ndef datetime_normalize(s):\n    if s != s:\n        return np.nan\n    \n    s = s.split(\".\")[0]\n    if s[-1] == \"Z\":\n        s = s[:-1]\n        \n    date, time = s.split(\"T\")\n    datetime_obj = datetime.strptime(s, \"%Y-%m-%dT%H:%M:%S\")\n    return datetime_obj\n\ndef date_normalize(s):\n    if s != s:\n        return np.nan\n    \n    try:\n        datetime_obj = datetime.strptime(s, \"%m\/%d\/%Y\")\n    except:\n        datetime_obj = datetime.strptime(s, \"%Y-%m-%d\")\n        \n    return datetime_obj\n\ndef process_datetime_cols(df):\n    cat_cols = []\n    for col in DATETIME:\n        df[col] = df[col].apply(datetime_normalize)\n        \n    for col in DATE:\n        if col == \"Field_34\":\n            continue\n        df[col] = df[col].apply(date_normalize)\n\n    df[\"Field_34\"] = df[\"Field_34\"].apply(correct_34_ngaysinh)\n    df[\"ngaySinh\"] = df[\"ngaySinh\"].apply(correct_34_ngaysinh)\n\n    cat_cols += DATE + DATETIME\n    for col in DATE + DATETIME:\n        df[col] = df[col].dt.strftime('%m-%Y')\n    \n    for cat in ['F', 'E', 'C', 'G', 'A']:\n        df[f'{cat}_startDate'] = pd.to_datetime(df[f\"{cat}_startDate\"], infer_datetime_format=True)\n        df[f'{cat}_endDate'] = pd.to_datetime(df[f\"{cat}_endDate\"], infer_datetime_format=True)\n        \n        df[f'{cat}_startDate'] = df[f'{cat}_startDate'].dt.strftime('%m-%Y')\n        df[f'{cat}_endDate'] = df[f'{cat}_endDate'].dt.strftime('%m-%Y')\n        \n        cat_cols.append(f'{cat}_startDate')\n        cat_cols.append(f'{cat}_endDate')\n    \n    for col in cat_cols:\n        df[col] = df[col].astype(\"category\")\n        \n    return df\n\nDROP = [\"gioiTinh\", \"info_social_sex\", \"ngaySinh\", \"namSinh\"] + \\\n        [f\"Field_{c}\" for c in [14, 16, 17, 24, 26, 30, 31, 37, 52, 57]]\n\ndef transform(df):\n    df = process_datetime_cols(df)\n    df = process_gender(df)\n    df = process_location(df)\n    df = process_misc(df)\n    return df.drop(DROP, axis = 1)\n\n#for df in [train, test]:\n#  df = transform(df)","f4411ceb":"plt.figure(figsize=(7,4))\nax = sns.countplot(train.label)\n\nheight = sum([p.get_height() for p in ax.patches])\nfor p in ax.patches:\n        ax.annotate(f'{100*p.get_height()\/height:.2f} %', (p.get_x()+0.32, p.get_height()+200),animated=True)","74e55902":"partner_cols = [col for col in train_cols if col[:4]=='part']","fba52013":"# note\nnotice_cols = ['Field_56', 'Field_61', 'data.basic_info.locale', 'brief']\nmix_cols = ['Field_21']\naddress = ['diaChi', 'currentLocationCity', 'currentLocationCountry', 'currentLocationName', 'currentLocationState',\n           'currentLocationLocationId', 'currentLocationLatitude', 'currentLocationLongitude',\n           'homeTownLatitude', 'homeTownLongitude', 'homeTownCity', 'homeTownCountry', 'homeTownName',\n           'homeTownState', ]\njobs = ['Field_46','maCv']\nzezo = ['Field_47']\noffice = ['Field_49']","def0c8e0":"for feature in ['brief', 'currentLocationCity', 'currentLocationCountry',\n                'homeTownName']: \n    train[feature + '_count_full'] = train[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n    test[feature + '_count_full'] = test[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))","959610c4":"for col in jobs:\n    print('_'*30)\n    print(col)\n    print(train[col].nunique(), test[col].nunique())\n    print(train[col].isnull().sum(), test[col].isnull().sum())","c80be539":"for col in address:\n    print('_'*30)\n    print(col)\n    print(train[col].nunique(), test[col].nunique())\n    print(train[col].isnull().sum(), test[col].isnull().sum())","51546053":"datetime_cols = ['Field_1', 'Field_2', 'Field_5', 'Field_6', 'Field_7', 'Field_8', 'Field_9', 'Field_11', 'Field_15', \n                 'Field_25', 'Field_32', 'Field_33', 'Field_34', 'Field_35', 'Field_40', 'Field_43', 'Field_44',\n                 'F_startDate', 'F_endDate', 'E_startDate', 'E_endDate', 'C_startDate', 'C_endDate',\n                 'G_startDate', 'G_endDate', 'A_startDate', 'A_endDate'\n                 ]","ca1d1a71":"for col in datetime_cols:\n    print('_'*30)\n    print(col)\n    print(train[col].nunique(), test[col].nunique())\n    print(train[col].isnull().sum(), test[col].isnull().sum())","81685510":"for df in [train, test]:\n    for col in ['F_startDate', 'F_endDate', 'E_startDate', 'E_endDate',\n                    'C_startDate', 'C_endDate', 'G_startDate', 'G_endDate',\n                    'A_startDate', 'A_endDate', 'Field_33', 'Field_8',\n                'Field_1', 'Field_2', 'Field_9', 'Field_7', 'Field_5',\n                'Field_6', 'Field_32']:\n        #df[f'day_of_year_{col}'] = pd.to_datetime(df[col]).dt.dayofyear\n        df[f'day_of_week_{col}'] = pd.to_datetime(df[col]).dt.dayofweek\n        #df[f'hour_{col}'] = pd.to_datetime(df[col]).dt.hour\n        df[f'month_{col}'] = pd.to_datetime(df[col]).dt.month\n        #df[f'year_{col}'] = pd.to_datetime(df[col]).dt.year\n        #df[f'weekday_{col}'] = pd.to_datetime(df[col]).dt.weekday\n        df[f'week_{col}'] = pd.to_datetime(df[col]).dt.week","845d94e7":"# ref: https:\/\/www.kaggle.com\/cdeotte\/xgb-fraud-with-magic-0-9600\n\n# GROUP AGGREGATION NUNIQUE\ndef encode_AG2(main_columns, uids, train_df, test_df):\n    for main_column in main_columns:  \n        for col in uids:\n            comb = pd.concat([train_df[[col]+[main_column]],test_df[[col]+[main_column]]],axis=0)\n            mp = comb.groupby(col)[main_column].agg(['nunique'])['nunique'].to_dict()\n            train_df[col+'_'+main_column+'_ct'] = train_df[col].map(mp).astype('float32')\n            test_df[col+'_'+main_column+'_ct'] = test_df[col].map(mp).astype('float32')\n            print(col+'_'+main_column+'_ct, ',end='')\n\n# FREQUENCY ENCODE TOGETHER\ndef encode_FE(df1, df2, cols):\n    for col in cols:\n        df = pd.concat([df1[col],df2[col]])\n        vc = df.value_counts(dropna=True, normalize=True).to_dict()\n        vc[-1] = -1\n        nm = col+'_FE'\n        df1[nm] = df1[col].map(vc)\n        df1[nm] = df1[nm].astype('float32')\n        df2[nm] = df2[col].map(vc)\n        df2[nm] = df2[nm].astype('float32')\n        print(nm,', ',end='')\n\n\n# GROUP AGGREGATION MEAN AND STD\n\ndef encode_AG(main_columns, uids, aggregations, train_df, test_df, \n              fillna=True, usena=False):\n    # AGGREGATION OF MAIN WITH UID FOR GIVEN STATISTICS\n    for main_column in main_columns:  \n        for col in uids:\n            for agg_type in aggregations:\n                new_col_name = main_column+'_'+col+'_'+agg_type\n                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n                if usena: temp_df.loc[temp_df[main_column]==-1,main_column] = np.nan\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                        columns={agg_type: new_col_name})\n\n                temp_df.index = list(temp_df[col])\n                temp_df = temp_df[new_col_name].to_dict()   \n\n                train_df[new_col_name] = train_df[col].map(temp_df).astype('float32')\n                test_df[new_col_name]  = test_df[col].map(temp_df).astype('float32')\n                \n                if fillna:\n                    train_df[new_col_name].fillna(-1,inplace=True)\n                    test_df[new_col_name].fillna(-1,inplace=True)\n                \n                print(\"'\"+new_col_name+\"'\",', ',end='')\n\n# LABEL ENCODE\ndef encode_LE(col, train, test, verbose=True):\n    df_comb = pd.concat([train[col],test[col]],axis=0)\n    df_comb,_ = df_comb.factorize(sort=True)\n    nm = col\n    if df_comb.max()>32000: \n        train[nm] = df_comb[:len(train)].astype('int32')\n        test[nm] = df_comb[len(train):].astype('int32')\n    else:\n        train[nm] = df_comb[:len(train)].astype('int16')\n        test[nm] = df_comb[len(train):].astype('int16')\n    del df_comb; x=gc.collect()\n    if verbose: print(nm,', ',end='')                \n# COMBINE FEATURES\ndef encode_CB(col1,col2,train,test):\n    nm = col1+'_'+col2\n    train[nm] = train[col1].astype(str)+'_'+train[col2].astype(str)\n    test[nm] = test[col1].astype(str)+'_'+test[col2].astype(str) \n    encode_LE(nm,train,test)","95e1c669":"def values_normalization(dt_df, periods, columns):\n    for period in periods:\n        for col in columns:\n            new_col = col +'_'+ period\n            dt_df[col] = dt_df[col].astype(float)  \n\n            temp_min = dt_df.groupby([period])[col].agg(['min']).reset_index()\n            temp_min.index = temp_min[period].values\n            temp_min = temp_min['min'].to_dict()\n\n            temp_max = dt_df.groupby([period])[col].agg(['max']).reset_index()\n            temp_max.index = temp_max[period].values\n            temp_max = temp_max['max'].to_dict()\n\n            temp_mean = dt_df.groupby([period])[col].agg(['mean']).reset_index()\n            temp_mean.index = temp_mean[period].values\n            temp_mean = temp_mean['mean'].to_dict()\n\n            temp_std = dt_df.groupby([period])[col].agg(['std']).reset_index()\n            temp_std.index = temp_std[period].values\n            temp_std = temp_std['std'].to_dict()\n\n            dt_df['temp_min'] = dt_df[period].map(temp_min)\n            dt_df['temp_max'] = dt_df[period].map(temp_max)\n            dt_df['temp_mean'] = dt_df[period].map(temp_mean)\n            dt_df['temp_std'] = dt_df[period].map(temp_std)\n\n            dt_df[new_col+'_min_max'] = (dt_df[col]-dt_df['temp_min'])\/(dt_df['temp_max']-dt_df['temp_min'])\n            dt_df[new_col+'_std_score'] = (dt_df[col]-dt_df['temp_mean'])\/(dt_df['temp_std'])\n            del dt_df['temp_min'],dt_df['temp_max'],dt_df['temp_mean'],dt_df['temp_std']\n    return dt_df\n\ndef uid_aggregation(train_df, test_df, main_columns, uids, aggregations):\n    for main_column in main_columns:  \n        for col in uids:\n            for agg_type in aggregations:\n                new_col_name = col+'_'+main_column+'_'+agg_type\n                temp_df = pd.concat([train_df[[col, main_column]], test_df[[col,main_column]]])\n                temp_df = temp_df.groupby([col])[main_column].agg([agg_type]).reset_index().rename(\n                                                        columns={agg_type: new_col_name})\n\n                temp_df.index = list(temp_df[col])\n                temp_df = temp_df[new_col_name].to_dict()   \n\n                train_df[new_col_name] = train_df[col].map(temp_df)\n                test_df[new_col_name]  = test_df[col].map(temp_df)\n    return train_df, test_df","c2d7c410":"#i_columns = ['Field_74', 'Field_78', 'Field_79', 'Field_80', 'Field_81']\n#periods = ['Field_23', 'Field_59', 'Field_82']\n\n#for df in [train, test]:\n#    df = values_normalization(df, periods, i_columns)","5696f4c4":"columns = ['Field_74', 'Field_78', 'Field_79', 'Field_80', 'Field_81', 'Field_82', 'Field_71', 'Field_72']\nnum_col_a = ['Field_20', 'Field_67']\n\nnum_columns=  ['Field_21', 'Field_29', 'Field_59', 'Field_60', 'Field_63', 'Field_64']\n\npca_cols = columns + num_col_a + num_columns\n\n# add some fe\n\nrank_features = ['Field_20'] #, 'Field_74', 'Field_67']\n\n#for df in [train, test]:\n#  for f in rank_features:\n#    df[f + '_rank_log1p'] = np.log1p(df[f].rank()).values\n\n#for df in [train, test]: # from santander customer prediction --> 7609 CV\n#  for col in rank_features:\n#    count = df[col].value_counts()\n#    rank=len(df[col].unique())\n#    df[\"rank_\"+col] = df[col].map(count.rank()\/rank)\n\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import norm, rankdata\n\nsc = StandardScaler()\nfor df in [train, test]:\n  for feat in rank_features:\n    #df[feat] = sc.fit_transform(df[feat].values.reshape(-1, 1))\n    df[feat+'_rank'] = rankdata(sc.fit_transform(df[feat].values.reshape(-1, 1))).astype('float32')\n    df[feat+'_norm'] = norm.cdf(sc.fit_transform(df[feat].values.reshape(-1, 1))).astype('float32')\n\nfor df in [train, test]:\n    df['Field_71'] = np.round_(df['Field_71'], decimals=3)\n    df['Field_72'] = np.round_(df['Field_72'], decimals=2)\n    \n    df['Field_28_fix'] = (df['Field_20']\/200)\/9 # field 27 same field 20 --> drop\n    \n    df['fe1'] = df['Field_74'] * df['Field_71']\n    df['fe2'] = df['Field_74'] * df['Field_72']\n    df['fe3'] = df['Field_78'] * df['Field_79']\n    df['fe4'] = df['Field_78'] \/ df['Field_82']\n    df['fe5'] = df['Field_79'] \/ df['Field_82']\n    df['fe6'] = df['Field_23'] \/ df['Field_21']\n\n    df['fe7'] = df['Field_20'] \/ df['Field_67']\n    \n    #df['fe8'] = df['Field_28_fix'] \/ df['Field_67']\n\n\n# PCA FEATURE\n#for df in [train, test]:\n#    standard = StandardScaler()\n#    pca = PCA(n_components = 3, random_state = 42)\n#    pca_decom = pca.fit_transform(standard.fit_transform(df[pca_cols].fillna(-1)))\n\n#    df['pca_col_0'] = pca_decom[:, 0]\n#    df['pca_col_1'] = pca_decom[:, 1]\n#    df['pca_col_2'] = pca_decom[:, 2]\n\nfor df in [train, test]:\n    df['Field_20_log1p'] = df['Field_20'].apply(np.log1p)\n    #df['field_67_log1p'] = df['Field_67'].apply(np.log1p)\n    df['Field_74_log1p'] = df['Field_74'].apply(np.log1p)\n    #df['Field_28_log1p'] = df['Field_28_fix'].apply(np.log1p)\n    \n    df['summary_mean'] = df[['summary_6m', 'summary_3m', 'summary_1m', 'summary_1w']].mean(axis = 1)\n    df['summary_std'] = df[['summary_6m', 'summary_3m', 'summary_1m', 'summary_1w']].std(axis = 1)\n    #df['summary_sum'] = df[['summary_6m', 'summary_3m', 'summary_1m', 'summary_1w']].sum(axis = 1)\n    #df['summary_na_count'] = df[['summary_6m', 'summary_3m', 'summary_1m', 'summary_1w']].isna().sum(axis = 1)\n    \n    #df['numOrg_sum'] = df[['numOrg', 'F_numOrg', 'E_numOrg', 'C_numOrg', 'G_numOrg']].sum(axis = 1)\n    df['numOrg_mean'] = df[['numOrg', 'F_numOrg', 'E_numOrg', 'C_numOrg', 'G_numOrg']].mean(axis = 1)\n    df['numOrg_std'] = df[['numOrg', 'F_numOrg', 'E_numOrg', 'C_numOrg', 'G_numOrg']].std(axis = 1)\n    #df['numOrg_na_count'] = df[['numOrg', 'F_numOrg', 'E_numOrg', 'C_numOrg', 'G_numOrg']].isna().sum(axis = 1)\n    \n    df['partner_cols_mean'] = df[partner_cols].mean(axis=1)\n    df['partner_cols_std'] = df[partner_cols].std(axis=1)\n\n    #df['numQuery_sum'] = df[['F_numQuery', 'E_numQuery', 'C_numQuery', 'G_numQuery']].sum(axis = 1)\n#    df['numQuery_mean'] = df[['F_numQuery', 'E_numQuery', 'C_numQuery', 'G_numQuery']].mean(axis = 1)\n#    df['numQuery_std'] = df[['F_numQuery', 'E_numQuery', 'C_numQuery', 'G_numQuery']].std(axis = 1)\n    #df['numQuery_na_count'] = df[['F_numQuery', 'E_numQuery', 'C_numQuery', 'G_numQuery']].isna().sum(axis = 1)\n    \n #   standard = StandardScaler()\n #   pca = PCA(n_components = 3, random_state = 42)\n #   pca_decom = pca.fit_transform(standard.fit_transform(df[boolean_features].fillna(-1)))\n\n #   df['pca_col_0'] = pca_decom[:, 0]\n #   df['pca_col_1'] = pca_decom[:, 1]\n #   df['pca_col_2'] = pca_decom[:, 2]\n    \n    #df['boolean_cols_mean'] = df[boolean_features].mean(axis=1)\n    #df['boolean_cols_std'] = df[boolean_features].std(axis=1)\n  #  df['boolean_cols_kurt'] = df[boolean_features].kurtosis(axis=1) # skew\n  #  df['boolean_cols_sum'] = df[boolean_features].sum(axis=1)\n #   df['boolean_cols_na_count'] = df[boolean_features].isna().sum(axis = 1)\n    \n  #  df['na_count'] = df.isna().sum(axis = 1)\n    #df['69_70_71_mean'] = df[['Field_69', 'Field_70', 'Field_71']].mean(axis = 1)","7eb2c573":"temp123 = ['Field_55__Field_80', 'brief__Field_81', 'Field_56__Field_79', 'Field_55__Field_81',\n           'brief__Field_80', 'Field_56__Field_80', 'Field_55__Field_79', 'brief__Field_79']\n\n# COMBINE FEATURE + LABEL ENCODE\nfor feature in temp123:\n    f1, f2 = feature.split('__')\n    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n    le = LabelEncoder()\n    le.fit(list(train[feature].astype(str).values) + list(test[feature].astype(str).values))\n    train[feature] = le.transform(list(train[feature].astype(str).values))\n    test[feature] = le.transform(list(test[feature].astype(str).values))\n\nfor i in temp123:\n    train[i+'_count_full'] = train[i].map(pd.concat([train[i], test[i]], ignore_index=True).value_counts(dropna=False))\n    test[i+'_count_full'] = test[i].map(pd.concat([train[i], test[i]], ignore_index=True).value_counts(dropna=False))\n\n### Statistical Feature: Cross some categories and continuous variables\ncon_fea = ['Field_74', 'Field_78', 'Field_79', 'Field_80', 'Field_81',\n           'Field_82', 'Field_71', 'Field_72'] #+ ['Field_23', 'Field_59'] \n\ncat_fea = temp123 \ntrain_test = pd.concat([train[con_fea+cat_fea],test[con_fea+cat_fea]],ignore_index=True,sort=False)\n\nfor cont in con_fea:\n    for cat in cat_fea:\n        train[cont+'_'+cat+'_mean'] = train_test[cont].groupby(train_test[cat]).transform('mean')[:len(train)].tolist()\n        train[cont+'_'+cat+'_std'] = train_test[cont].groupby(train_test[cat]).transform('std')[:len(train)].tolist()\n\n        test[cont+'_'+cat+'_mean'] = train_test[cont].groupby(train_test[cat]).transform('mean')[len(train):].tolist()\n        test[cont+'_'+cat+'_std'] =  train_test[cont].groupby(train_test[cat]).transform('std')[len(train):].tolist()","b0144ad5":"def date_normalize(s):\n    if s != s:\n        return np.nan\n    try:\n        datetime_obj = datetime.strptime(s, \"%m\/%d\/%Y\")\n    except:\n        datetime_obj = datetime.strptime(s, \"%Y-%m-%d\")\n    return datetime_obj\n\nDATE = [\"Field_{}\".format(i) for i in [5, 6, 7, 8, 9, 11, 15, 25, 32, 33, 34, 35, 40]]\ncat_cols = DATE.copy()\nfor cat in ['F', 'E', 'C', 'G', 'A']:\n    cat_cols.append(f'{cat}_startDate')\n    cat_cols.append(f'{cat}_endDate')\n\n#for df in [train, test]:\n#  for col in cat_cols: \n#      if col == 'Field_34':\n#          continue\n#      df[col] = df[col].fillna('1900-01-01').apply(date_normalize)\n      #df[col] = df[col].replace({'Unknown':'1900-01-01'}).apply(date_normalize)\n#      df[col] = df[col].dt.strftime('%Y%m%d').astype(int)","cb973147":"drop_cols = ['Field_30', 'Field_24', 'Field_16', 'Field_14', 'Field_13', 'partner4_F', \n             'partner3_G', 'partner3_H', 'partner3_K', 'partner3_L','partner4_A', 'partner4_B', 'partner4_C', \n             'partner4_D', 'partner4_E', 'partner4_G', 'partner5_K', 'partner4_H', 'partner4_K', 'partner3_C',\n             'partner5_C', 'partner2_L', 'partner0_L', 'partner1_E', 'partner1_F', 'partner1_K', 'partner5_H', 'partner0_B',\n             'Field_57', 'Field_26', 'Field_17', 'Field_37', 'Field_52', 'Field_31', 'partner3_F', 'partner5_B',\n             'partner5_B', 'partner5_L', 'partner3_B', 'partner0_K', 'partner1_B', 'partner1_D', 'partner2_K', 'partner2_B',\n             'partner1_L', 'partner2_G', 'partner5_D', 'partner5_E', 'partner5_F',\n             'partner2_F', 'partner2_E', 'partner2_D', 'partner2_C'] + [\n                   # 'Field_1', 'Field_2', 'Field_5', 'Field_6', 'Field_7', # 'Field_8',\n                    #'Field_9', \n                 'Field_11', 'Field_15', 'Field_25', 'Field_32', \n                     'Field_35', 'Field_40', 'Field_43', 'Field_44', # 'Field_33',\n                    'F_startDate', 'F_endDate', 'E_startDate', 'E_endDate',\n                    'C_startDate', 'C_endDate', 'G_startDate', 'G_endDate', \n                    #'A_startDate', 'A_endDate'\n                    ] + temp123 + [\"ngaySinh\", \"namSinh\"] + ['Field_27', 'Field_28']","673d196f":"dropped_train = train.drop(columns=drop_cols)\ndropped_test = test.drop(columns=drop_cols)\nnum_cols = dropped_test.select_dtypes(exclude=['object']).columns","b2a34004":"dropped_train.drop(columns=['label'], inplace=True)","3d8d3944":"submiss_dir='.\/submission'\nos.makedirs(submiss_dir, exist_ok=True)","f003c421":"data_full = pd.concat([dropped_train, dropped_test],sort=False)","7d34abda":"cate_features = data_full.select_dtypes(include=['bool', 'object']).columns\nnum_features = data_full.select_dtypes(exclude=['bool', 'object']).columns\n\nprint(len(cate_features), len(num_features))","c406ee15":"num_features","0b85e77f":"num_features_ = [x for x in num_features if x not in ['label']]\ncounter = 0\nto_remove = []\nfor feat_a in num_features_:\n    for feat_b in num_features_:\n        if feat_a != feat_b and feat_a not in to_remove and feat_b not in to_remove:\n            c = np.corrcoef(data_full[feat_a], data_full[feat_b])[0][1]\n            if c > 0.99: # 0.995\n                counter += 1\n                to_remove.append(feat_b)\n                print('{}: FEAT_A: {} FEAT_B: {} - Correlation: {}'.format(counter, feat_a, feat_b, c))","8d5e981e":"for col in num_features:\n    #data_full[col + \"is_missing\"] = data_full[col].isnull().astype(bool)\n    data_full[col] = data_full[col].fillna(-1)\ndata_full[cate_features] = data_full[cate_features].fillna('Unknown')\n","c97cb43d":"data_full = data_full.drop(to_remove, axis = 1)","f0b7dc89":"LB_features = [feature for feature in cate_features if data_full[feature].nunique() > 1000]\nOH_features = list(set(cate_features) - set(LB_features))\n\nprint(len(LB_features), len(OH_features))","a7cf18b6":"print('LB FEATURE')\nprint(LB_features)\n\nprint('OH FEATURE')\nprint(OH_features)","fe4bcc9b":"LB_encoder = LabelEncoder()\nOH_encoder = OneHotEncoder(sparse=False)\n\n#for col in LB_features:\n    #data_full[col] = data_full[col].fillna('Unknown')\n#    data_full[col] = data_full[col].astype(str)\n    \n#    le = LabelEncoder()\n#    le.fit(list(data_full[col]))\n#    data_full[col] = le.transform(data_full[col])   \n#    data_full[col] = data_full[col].astype('category')\n\ndef str_normalize(s):\n    s = str(s).strip().lower()\n    s = re.sub(' +', \" \", s)\n    return s\n\ndef process_location(df):\n    for col in [\"currentLocationLocationId\", \"homeTownLocationId\", \"currentLocationLatitude\", \"currentLocationLongitude\", \n                   \"homeTownLatitude\", \"homeTownLongitude\"]:\n        df[col].replace(0, np.nan, inplace=True)\n\n    df[\"currentLocationLocationId\"] = df[\"currentLocationLocationId\"].apply(str_normalize) #.astype(\"category\")\n    df[\"homeTownLocationId\"] = df[\"homeTownLocationId\"].apply(str_normalize) #.astype(\"category\")\n\n    return df\n\n#data_full = process_location(data_full)\n\ncategorical_features = []\nfor col in list(data_full):\n    if data_full[col].dtype=='object':\n        categorical_features.append(col)\n        print(col)\n        data_full[col] = data_full[col].fillna('Unknown')\n        data_full[col] = data_full[col].astype(str)\n        \n        le = LabelEncoder()\n        le.fit(list(data_full[col]))\n        data_full[col] = le.transform(data_full[col])       \n        data_full[col] = data_full[col].astype('category')\n\n#for col in [\"currentLocationLocationId\", \"homeTownLocationId\", \"currentLocationLatitude\", \"currentLocationLongitude\", \n#                   \"homeTownLatitude\", \"homeTownLongitude\"]:\n    \n    #data_full[col] = data_full[col].fillna('Unknown')\n#    data_full[col] = data_full[col].astype(str)\n    \n#    le = LabelEncoder()\n#    le.fit(list(data_full[col]))\n#    data_full[col] = le.transform(data_full[col])   \n#    data_full[col] = data_full[col].astype('category')\n\n#categorical_features = categorical_features + [\"currentLocationLocationId\", \"homeTownLocationId\", \"currentLocationLatitude\", \"currentLocationLongitude\", \n#                   \"homeTownLatitude\", \"homeTownLongitude\"]\n\n#for feature in LB_features:\n#    data_full[feature] = LB_encoder.fit_transform(data_full[feature])\n\n#OH_encoded_features = pd.DataFrame(OH_encoder.fit_transform(data_full[OH_features].astype(str)))\n#OH_encoded_features.index = data_full.index\n\n#data_full = data_full.drop(columns=OH_features).join(OH_encoded_features)","44202612":"encoded_train_full = data_full[:len(train)]\nencoded_test_full = data_full[len(train):]","482acc95":"encoded_train_full.shape, encoded_test_full.shape","1a544b75":"X_train_full = encoded_train_full\ny_train_full = train.label","d3322822":"params = {'num_leaves': 512,\n          'min_child_weight': 0.03,\n          'feature_fraction': 0.4,\n          'bagging_fraction': 0.4,\n          'min_data_in_leaf': 128,\n          'objective': 'binary',\n          'max_depth': 7,\n          'learning_rate': 0.005,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.3,\n          'reg_lambda': 0.6,\n          'random_state': 47,\n          #'device' : 'gpu',\n          #'gpu_platform_id' : 0,\n          #'gpu_device_id' : 0\n         }\n\nparams_khanh = {'num_leaves': 2**8, # 512,\n          'min_child_weight': 0.005,\n          'feature_fraction': 0.6,\n          'bagging_fraction': 0.8,\n          'min_data_in_leaf': 128,\n          'objective': 'binary',\n          'max_depth': 8, # 7\n          'learning_rate': 0.005,\n          \"boosting_type\": \"gbdt\",\n          \"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          'reg_alpha': 0.5, # 0.3\n          'reg_lambda': 0.5, # 0.6\n          'random_state': 47,\n          'max_bin': 254, \n          'boost_from_average':'false',\n          #'device' : 'gpu',\n          #'gpu_platform_id' : 0,\n          #'gpu_device_id' : 0\n         }\n\nLIGHTGBM_PARAMS = {\n    'boosting_type': 'goss',\n    'objective': 'binary',\n    'metric': 'auc',\n    #'n_estimators': 10000,\n    'learning_rate': 0.005134,\n    'num_leaves': 54,\n    'max_depth': 10,\n    #'subsample_for_bin': 240000,\n    'reg_alpha': 0.436193,\n    'reg_lambda': 0.479169,\n    'colsample_bytree': 0.508716,\n    'min_split_gain': 0.024766,\n    'subsample': 1,\n    #'is_unbalance': False,\n    'silent':-1,\n    'verbose':-1\n}\n\nlgb_params = {\n            'nthread': 4,\n            'objective': 'binary',\n            'metric': 'auc',\n            'seed': 1999,\n           #'n_estimators' : 10000,\n            'learning_rate': 0.02,\n            'num_leaves': 32,\n            'colsample_bytree': 0.9497036,\n            'subsample': 0.8715623,\n            'max_depth': 8,\n            'reg_alpha': 0.04,\n            'reg_lambda': 0.073,\n            'min_split_gain': 0.0222415,\n            'min_child_weight': 40,\n            'silent': -1,\n            'verbose': -1,\n            #scale_pos_weight=11\n}\n\nparams2 = {\n            'objective':'binary',\n              'boosting_type': 'gbdt',\n              'metric' : 'auc',\n           \n              'learning_rate': 0.03,\n              'subsample': 0.68,\n              'tree_learner': 'serial',\n              'colsample_bytree': 0.28,\n              #'early_stopping_rounds': 100, # 50,\n              'subsample_freq': 1,\n              'reg_lambda': 2,\n              'reg_alpha': 1,\n              'num_leaves': 2 ** 5,\n              'random_state': 1999,\n            }","e9de98ca":"class AverageMeter:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","204de72b":"from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n\ndef display_importances(feature_importance_df_):\n\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n        by=\"importance\", ascending=False)[:30].index\n    \n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    \n    plt.figure(figsize=(12, 8))\n    sns.barplot(x=\"importance\", y=\"feature\", \n                data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('CatBoost Features Importance (avg over folds)')\n    plt.tight_layout()\n    \n    \ndef runCatBoost(train_data, labels, test_data, index, n_folds=5, submiss_dir = '.\/submission'):\n    fig, ax = plt.subplots()\n    aucs = []\n    cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n    best_test_preds = []\n    fixed_test_preds = []\n\n    pbar = tqdm(enumerate(cv.split(train_data, labels)), total=n_folds)\n    \n    best_train_auc = AverageMeter()  \n    best_val_auc = AverageMeter()  \n    fixed_train_auc = AverageMeter()  \n    fixed_val_auc = AverageMeter()  \n\n    oof_cat = np.zeros(train_data.shape[0])\n    # Feature Importance\n    feature_importance_df = pd.DataFrame()\n\n    for i, (train,valid) in pbar:\n        print('_'*100)\n        dtrain = Pool(train_data.loc[train], label=labels[train], cat_features = categorical_features)\n        dvalid = Pool(train_data.loc[valid], label=labels[valid], cat_features = categorical_features)\n\n        params_cat2 = {\n    'random_seed': 42,\n    #'gpu_ram_part': 0.95,\n    'iterations': 10000,\n    'subsample': 0.8, # 0.8, # 0.8, # 0.75, # 0.8, # 0.36 # This parameter can be used if one of the following bootstrap types is selected:Poisson\/Bernoulli\/MVS\n    'learning_rate': 0.01, # 0.04\n    'bootstrap_type': 'Bernoulli', #  'Bayesian',\n    'grow_policy': 'Lossguide',\n    #'bagging_temperature': 0.4, # for boostrap_type: 'Bayesian'\n    'l2_leaf_reg': 3, # 5, # 10, # 12, 20\n    'depth': 7, # 7, # 8, # 7, # 9\n    'max_leaves': 64, # 2**8-1, # 2**8-1,\n    'scale_pos_weight': 0.8,\n    'boosting_type': 'Plain', # 'Ordered' \/ Usually provides better quality on small datasets, but it may be slower than the Plain scheme.\n    'min_data_in_leaf': 32, # 50, # 32, # 50, # 64, # 50, \n    'border_count': 254, # 254\n    'thread_count': 4,\n    'custom_metric': ['Logloss', 'AUC:hints=skip_train~false'],\n    'od_type': 'Iter',\n    'od_wait': 500,\n    'task_type': 'GPU',\n    'eval_metric': 'AUC',\n    'use_best_model': True\n}\n\n        params_cat = {\n    'random_seed': 1999,\n    #'gpu_ram_part': 0.95,\n    'iterations': 10000,\n    'subsample': 0.6, # 0.36 # This parameter can be used if one of the following bootstrap types is selected:Poisson\/Bernoulli\/MVS\n    'learning_rate': 0.02, # 0.04\n    'bootstrap_type': 'Bernoulli', # 'Bayesian',\n    # 'bagging_temperature': 0.4, # for boostrap_type: 'Bayesian'\n    'l2_leaf_reg': 5, # 12, 20\n    'depth': 8, # 7\n    'border_count': 128, # 254\n    'thread_count': 4,\n    'custom_metric': ['Logloss', 'AUC:hints=skip_train~false'],\n    'od_type': 'Iter',\n    'od_wait': 500,\n    'task_type': 'GPU',\n    'eval_metric': 'AUC',\n    'use_best_model': True\n}\n\n        # Best iteration selected\n        model = CatBoostClassifier(**params_cat2)\n        #model = lgb.train(params,  dtrain, num_boost_round=10000, valid_sets = [dtrain, dvalid], verbose_eval=False, early_stopping_rounds=200)\n        model.fit(dtrain, eval_set = dvalid, verbose = 200, early_stopping_rounds = 500)\n\n        oof_cat[valid] = model.predict_proba(train_data.loc[valid])[:, 1]\n\n        best_test_pred = model.predict_proba(test_data)[:,1]\n        best_test_preds.append(best_test_pred)\n        \n\n        preds = model.predict_proba(train_data.loc[valid])[:, 1]\n        \n        ## Feature Importance\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = train_data.columns\n        fold_importance_df[\"importance\"] = model.feature_importances_\n        fold_importance_df[\"fold\"] = i + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        \n        # visualize ROC AUC SCORE\n        fpr, tpr, threshold = roc_curve(labels[valid], preds)\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        plt.plot(fpr, tpr, label = f'Fold number {i+1} (AUC = {roc_auc:.2f})')\n\n        # Fix number of iterations = 6000\n\n        # model = lgb.train(params,  dtrain, num_boost_round=6000, valid_sets = [dtrain, dvalid], verbose_eval=False, early_stopping_rounds=1000)\n        # fixed_test_pred = model.predict(test_data)\n        # fixed_test_preds.append(fixed_test_pred)\n\n        # train_auc_ = model.best_score['training']['auc']\n        # val_auc_ = model.best_score['valid_1']['auc']\n        # print(f'(Fixed) FOLD {i+1}\/{n_folds}: train auc {train_auc_:.4f}. validation auc {val_auc_:.4f}. best iter {model.best_iteration}')\n        \n        # fixed_train_auc.update(train_auc_, len(train))\n        # fixed_val_auc.update(val_auc_, len(valid))        \n        \n        #pbar.set_postfix(BEST_TRAIN=best_train_auc.avg, BEST_VAL=best_val_auc.avg,\n        #                 FIXED_TRAIN=fixed_train_auc.avg, FIXED_VAL=fixed_val_auc.avg)\n\n    ax.plot([0,1], [0,1], label='Luck', linestyle='--', color='r')  \n    mean_auc = np.mean(aucs)\n    std_auc = np.std(aucs)\n    ax.plot(mean_auc, label=f'Average AUC score: {mean_auc:.4f} $\\pm$ {std_auc:.4f}')  \n    ax.legend(loc=\"lower right\")\n    ax.set(xlim=[-.1, 1.1], ylim=[-.1, 1.1], title='CatBoost')\n    plt.show()\n\n    best_test_pred = np.mean(best_test_preds, axis=0)\n    submiss = pd.DataFrame({\"id\":index, \"label\": best_test_pred})\n    submiss_path = os.path.join(submiss_dir, f'Cat_full_{mean_auc:.4f}.csv')\n    submiss.to_csv(submiss_path, index=False) \n\n    #model = CatBoostClassifier(**params_cat)\n    #model.fit()\n\n    #full_test_pred = model.predict(encoded_test_full) \n\n    #submiss['label'] = np.mean([best_test_pred, full_test_pred], axis=0) \n    #submiss_path = os.path.join(submiss_dir, f'Cat_full2_{mean_auc:.4f}.csv')\n    #submiss.to_csv(submiss_path, index=False) \n    \n    # save oof prediction for stacking \n    np.save(f'oof_cat_full_{mean_auc:.4f}.npy', oof_cat)   \n    \n    # Feature Importance\n    display_importances(feature_importance_df)","6f7c17cf":"%%time\nrunCatBoost(train_data = X_train_full, \n      labels=y_train_full, \n      test_data=encoded_test_full,\n      index=test.index,\n      n_folds=5,\n      submiss_dir=submiss_dir)","def24d5e":"from catboost import CatBoostClassifier, CatBoostRegressor, Pool\ndef runCatBoost2(train_data, labels, test_data, index, n_folds=5, submiss_dir = '.\/submission'):\n    fig, ax = plt.subplots()\n    aucs = []\n    cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n    best_test_preds = []\n    fixed_test_preds = []\n\n    pbar = tqdm(enumerate(cv.split(train_data, labels)), total=n_folds)\n    \n    best_train_auc = AverageMeter()  \n    best_val_auc = AverageMeter()  \n    fixed_train_auc = AverageMeter()  \n    fixed_val_auc = AverageMeter()  \n\n    oof_cat = np.zeros(train_data.shape[0])\n\n    for i, (train,valid) in pbar:\n        print('_'*100)\n        dtrain = Pool(train_data.loc[train], label=labels[train], cat_features = categorical_features)\n        dvalid = Pool(train_data.loc[valid], label=labels[valid], cat_features = categorical_features)\n\n        params_cat2 = {\n    'random_seed': 42,\n    #'gpu_ram_part': 0.95,\n    'iterations': 10000,\n    'subsample': 0.8, # 0.8, # 0.8, # 0.75, # 0.8, # 0.36 # This parameter can be used if one of the following bootstrap types is selected:Poisson\/Bernoulli\/MVS\n    'learning_rate': 0.01, # 0.04\n    'bootstrap_type': 'Bernoulli', #  'Bayesian',\n    'grow_policy': 'Lossguide',\n    #'bagging_temperature': 0.4, # for boostrap_type: 'Bayesian'\n    'l2_leaf_reg': 5, # 5, # 10, # 12, 20\n    'depth': 7, # 7, # 7, # 8, # 7, # 9\n    'max_leaves': 2**8-1, # 2**8-1,\n    'scale_pos_weight': 0.86, # 0.8, # 0.8,\n    'boosting_type': 'Plain', # 'Ordered' \/ Usually provides better quality on small datasets, but it may be slower than the Plain scheme.\n    'min_data_in_leaf': 50, # 50, # 64, # 50, \n    'border_count': 254, # 254\n    'thread_count': 4,\n    'custom_metric': ['Logloss', 'AUC:hints=skip_train~false'],\n    'od_type': 'Iter',\n    'od_wait': 500,\n    'task_type': 'GPU',\n    'eval_metric': 'AUC',\n    'use_best_model': True\n}\n\n        params_cat = {\n    'random_seed': 1999,\n    #'gpu_ram_part': 0.95,\n    'iterations': 10000,\n    'subsample': 0.6, # 0.36 # This parameter can be used if one of the following bootstrap types is selected:Poisson\/Bernoulli\/MVS\n    'learning_rate': 0.02, # 0.04\n    'bootstrap_type': 'Bernoulli', # 'Bayesian',\n    # 'bagging_temperature': 0.4, # for boostrap_type: 'Bayesian'\n    'l2_leaf_reg': 5, # 12, 20\n    'depth': 8, # 7\n    'border_count': 128, # 254\n    'thread_count': 4,\n    'custom_metric': ['Logloss', 'AUC:hints=skip_train~false'],\n    'od_type': 'Iter',\n    'od_wait': 500,\n    'task_type': 'GPU',\n    'eval_metric': 'AUC',\n    'use_best_model': True\n}\n\n\n\n        \n        # Best iteration selected\n        model = CatBoostClassifier(**params_cat2)\n        #model = lgb.train(params,  dtrain, num_boost_round=10000, valid_sets = [dtrain, dvalid], verbose_eval=False, early_stopping_rounds=200)\n        model.fit(dtrain, eval_set = dvalid, verbose = 200, early_stopping_rounds = 500)\n\n        oof_cat[valid] = model.predict_proba(train_data.loc[valid])[:, 1]\n\n        best_test_pred = model.predict_proba(test_data)[:,1]\n        best_test_preds.append(best_test_pred)\n        \n        #train_auc_ = model.get_best_score['training']['auc']\n        #val_auc_ = model.get_best_score['valid_1']['auc']\n        #print(f'(Best)  FOLD {i+1}\/{n_folds}: train auc {train_auc_:.4f}. validation auc {val_auc_:.4f}. best iter {model.best_iteration}')\n\n        #best_train_auc.update(train_auc_, len(train))\n        #best_val_auc.update(val_auc_, len(valid))   \n\n        # visualize\n\n        preds = model.predict_proba(train_data.loc[valid])[:, 1]\n        fpr, tpr, threshold = roc_curve(labels[valid], preds)\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        plt.plot(fpr, tpr, label = f'Fold number {i+1} (AUC = {roc_auc:.2f})')\n\n        # Fix number of iterations = 6000\n\n        # model = lgb.train(params,  dtrain, num_boost_round=6000, valid_sets = [dtrain, dvalid], verbose_eval=False, early_stopping_rounds=1000)\n        # fixed_test_pred = model.predict(test_data)\n        # fixed_test_preds.append(fixed_test_pred)\n\n        # train_auc_ = model.best_score['training']['auc']\n        # val_auc_ = model.best_score['valid_1']['auc']\n        # print(f'(Fixed) FOLD {i+1}\/{n_folds}: train auc {train_auc_:.4f}. validation auc {val_auc_:.4f}. best iter {model.best_iteration}')\n        \n        # fixed_train_auc.update(train_auc_, len(train))\n        # fixed_val_auc.update(val_auc_, len(valid))        \n        \n        #pbar.set_postfix(BEST_TRAIN=best_train_auc.avg, BEST_VAL=best_val_auc.avg,\n        #                 FIXED_TRAIN=fixed_train_auc.avg, FIXED_VAL=fixed_val_auc.avg)\n\n    ax.plot([0,1], [0,1], label='Luck', linestyle='--', color='r')  \n    mean_auc = np.mean(aucs)\n    std_auc = np.std(aucs)\n    ax.plot(mean_auc, label=f'Average AUC score: {mean_auc:.4f} $\\pm$ {std_auc:.4f}')  \n    ax.legend(loc=\"lower right\")\n    ax.set(xlim=[-.1, 1.1], ylim=[-.1, 1.1], title='CatBoost')\n    plt.show()\n\n    best_test_pred = np.mean(best_test_preds, axis=0)\n    submiss = pd.DataFrame({\"id\":index, \"label\": best_test_pred})\n    submiss_path = os.path.join(submiss_dir, f'Cat_full_{mean_auc:.4f}.csv')\n    submiss.to_csv(submiss_path, index=False) \n\n    #model = CatBoostClassifier(**params_cat)\n    #model.fit()\n\n    #full_test_pred = model.predict(encoded_test_full) \n\n    #submiss['label'] = np.mean([best_test_pred, full_test_pred], axis=0) \n    #submiss_path = os.path.join(submiss_dir, f'Cat_full2_{mean_auc:.4f}.csv')\n    #submiss.to_csv(submiss_path, index=False) \n    np.save(f'oof_cat_full_{mean_auc:.4f}.npy', oof_cat)    ","7aae849b":"#%%time\n#runCatBoost2(train_data = X_train_full, \n#      labels=y_train_full, \n#      test_data=encoded_test_full,\n#      index=test.index,\n#      n_folds=5,\n#      submiss_dir=submiss_dir)  # best LB","7ae2015b":"import xgboost as xgb\ndef run_XGB(train_data, labels, test_data, index, n_folds=5, submiss_dir = '.\/submission'):\n    fig, ax = plt.subplots()\n    aucs = []\n    cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n    best_test_preds = []\n    fixed_test_preds = []\n\n    pbar = tqdm(enumerate(cv.split(train_data, labels)), total=n_folds)\n    \n    best_train_auc = AverageMeter()  \n    best_val_auc = AverageMeter()  \n    fixed_train_auc = AverageMeter()  \n    fixed_val_auc = AverageMeter()  \n    dtest = xgb.DMatrix(test_data)\n\n    oof_preds = np.zeros(train_data.shape[0])\n\n    for i, (train,valid) in pbar:\n        print('_'*100)\n        dtrain = xgb.DMatrix(train_data.loc[train], label=labels[train])\n        dvalid = xgb.DMatrix(train_data.loc[valid], label=labels[valid])\n\n        params = {  \n            #'n_estimators':2000,\n            'max_depth': 12, \n            'learning_rate': 0.02, \n            'subsample': 0.8,\n            'min_child_weight': 1,\n            'gamma':0,\n            'objective': 'binary:logistic',\n            'nthread': 4,\n            'scale_pos_weight': 1,\n            'colsample_bytree': 0.6, # 0.4, \n            'missing':-999, # -1 \n            'eval_metric':'auc',\n            # USE CPU\n            #nthread':4,\n            #tree_method':'hist' \n            # USE GPU\n            'tree_method':'gpu_hist'\n         }\n\n        params2 = {  \n            #'n_estimators':2000,\n            \"booster\": \"gbtree\",\n            'max_depth': 12, \n            'learning_rate': 0.02, \n            'subsample': 0.85, # 0.8\n            'min_child_weight': 10, # 30\/1\n            'gamma': 0.5, # 0\/ 10\n            'objective': 'binary:logistic',\n            'nthread': 4,\n            'scale_pos_weight': 1.6, # 1,\n            'colsample_bytree': 0.7, # 0.6, # 0.4,\n            \"colsample_bylevel\": 0.632,\n            \"alpha\": 0,\n            \"lambda\": 0, \n            'missing':-999, # -1 \n            'eval_metric':'auc',\n            # USE CPU\n            #nthread':4,\n            #tree_method':'hist' \n            # USE GPU\n            'tree_method':'gpu_hist'\n         }\n\n        params3 = {  \n            #'n_estimators':2000,\n            \"booster\": \"gbtree\",\n            'max_depth': 11, # 12, \n            'learning_rate': 0.01, \n            'subsample': 0.86, # 0.85, # 0.8\n            'min_child_weight': 10, # 3, # 10, # 30\/1\n            'gamma': 0.5, # 0\/ 10\n            'objective': 'binary:logistic',\n            'nthread': 4,\n            'scale_pos_weight': 1.6, # 1,\n            'colsample_bytree': 0.76, # 0.6, # 0.7, # 0.6, # 0.4,\n            \"colsample_bylevel\": 0.632,\n            \"reg_alpha\": 1, # 0,\n            \"reg_lambda\": 0.01, # 0, \n            'missing':-999, # -1 \n            'eval_metric':'auc',\n            # USE CPU\n            #nthread':4,\n            #tree_method':'hist' \n            # USE GPU\n            'tree_method':'gpu_hist'\n         } # use params3 ---> 0.766_0029\n\n        \n        # Best iteration selected\n        evallist = [(dvalid, 'eval')]\n        model = xgb.train(params2,  dtrain, num_boost_round=10000,  \n                          evals=evallist, verbose_eval=False, early_stopping_rounds=200)\n        \n        oof_preds[valid] = model.predict(xgb.DMatrix(train_data.loc[valid]), ntree_limit = model.best_ntree_limit)\n\n        best_test_pred = model.predict(dtest, ntree_limit = model.best_ntree_limit)\n        best_test_preds.append(best_test_pred)\n        \n        # train_auc_ = model.best_score['training']['auc']\n        val_auc_ = model.best_score\n        print(f'(Best)  FOLD {i+1}\/{n_folds}: validation auc {val_auc_:.4f}. best iter {model.best_iteration}')\n\n        # best_train_auc.update(train_auc_, len(train))\n        best_val_auc.update(val_auc_, len(valid))   \n\n        # visualize\n\n        preds = model.predict(dvalid)\n        fpr, tpr, threshold = roc_curve(labels[valid], preds)\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        plt.plot(fpr, tpr, label = f'Fold number {i+1} (AUC = {roc_auc:.2f})')\n\n        # Fix number of iterations = 6000\n\n        # model = lgb.train(params,  dtrain, num_boost_round=6000, valid_sets = [dtrain, dvalid], verbose_eval=False, early_stopping_rounds=1000)\n        # fixed_test_pred = model.predict(test_data)\n        # fixed_test_preds.append(fixed_test_pred)\n\n        # train_auc_ = model.best_score['training']['auc']\n        # val_auc_ = model.best_score['valid_1']['auc']\n        # print(f'(Fixed) FOLD {i+1}\/{n_folds}: train auc {train_auc_:.4f}. validation auc {val_auc_:.4f}. best iter {model.best_iteration}')\n        \n        # fixed_train_auc.update(train_auc_, len(train))\n        # fixed_val_auc.update(val_auc_, len(valid))        \n        \n        pbar.set_postfix(BEST_TRAIN=best_train_auc.avg, BEST_VAL=best_val_auc.avg,\n                         FIXED_TRAIN=fixed_train_auc.avg, FIXED_VAL=fixed_val_auc.avg)\n\n    ax.plot([0,1], [0,1], label='Luck', linestyle='--', color='r')  \n    mean_auc = np.mean(aucs)\n    std_auc = np.std(aucs)\n    ax.plot(mean_auc, label=f'Average AUC score: {mean_auc:.4f} $\\pm$ {std_auc:.4f}')  \n    ax.legend(loc=\"lower right\")\n    ax.set(xlim=[-.1, 1.1], ylim=[-.1, 1.1], title='Light GBM')\n    plt.show()\n\n    best_test_pred = np.mean(best_test_preds, axis=0)\n    submiss = pd.DataFrame({\"id\":index, \"label\": best_test_pred})\n    submiss_path = os.path.join(submiss_dir, f'XGB_full_{mean_auc:.4f}.csv')\n    submiss.to_csv(submiss_path, index=False) \n\n    #dfull = xgb.DMatrix(train_data.loc, label=labels)\n    #model = xgb.train(params,  dtrain, num_boost_round=2000, verbose_eval=False)\n    #full_test_pred = model.predict(dtest) \n\n    #submiss['label'] = np.mean([best_test_pred, full_test_pred], axis=0) \n    #submiss_path = os.path.join(submiss_dir, f'XGB_full2_{mean_auc:.4f}.csv')\n    #submiss.to_csv(submiss_path, index=False)   \n    np.save(f'oof_xgb_full_{mean_auc:.4f}.npy', oof_preds)  ","4b4e8fde":"#%%time\n#run_XGB(train_data = X_train_full, \n#      labels=y_train_full, \n#      test_data=encoded_test_full,\n#      index=test.index,\n#      n_folds=5,\n#      submiss_dir=submiss_dir)","f8a57c67":"# Display\/plot feature importance\ndef display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (avg over folds)')\n    plt.tight_layout()\n    plt.savefig('lgbm_importances01.png')\n\ndef Gini(y_true, y_pred):\n    # check and get number of samples\n    assert y_true.shape == y_pred.shape\n    n_samples = y_true.shape[0]\n\n    # sort rows on prediction column\n    # (from largest to smallest)\n    arr = np.array([y_true, y_pred]).transpose()\n    true_order = arr[arr[:, 0].argsort()][::-1, 0]\n    pred_order = arr[arr[:, 1].argsort()][::-1, 0]\n\n    # get Lorenz curves\n    L_true = np.cumsum(true_order) * 1. \/ np.sum(true_order)\n    L_pred = np.cumsum(pred_order) * 1. \/ np.sum(pred_order)\n    L_ones = np.linspace(1 \/ n_samples, 1, n_samples)\n\n    # get Gini coefficients (area between curves)\n    G_true = np.sum(L_ones - L_true)\n    G_pred = np.sum(L_ones - L_pred)\n\n    # normalize to true Gini coefficient\n    return G_pred * 1. \/ G_true\n\ndef evalerror(preds, dtrain):\n    labels = dtrain.get_label()\n    return 'gini', Gini(labels, preds), True\n\n\ndef runLGB(train_data, labels, test_data, index, n_folds=5, submiss_dir = '.\/submission'):\n    fig, ax = plt.subplots()\n    aucs = []\n    cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=seed)\n    best_test_preds = []\n    fixed_test_preds = []\n\n    pbar = tqdm(enumerate(cv.split(train_data, labels)), total=n_folds)\n    \n    best_train_auc = AverageMeter()  \n    best_val_auc = AverageMeter()  \n    fixed_train_auc = AverageMeter()  \n    fixed_val_auc = AverageMeter()  \n\n    oof_preds = np.zeros(train_data.shape[0])\n    feature_importance_df = pd.DataFrame()\n\n    features = [x for x in train_data.columns if x not in ['id', 'label']]\n\n    learning_rate = 0.05 # 0.1\n    num_leaves = 63 # 15 \/ 31 \/ 127\n    min_data_in_leaf = 118 # 2000\n    feature_fraction = 0.6\n\n    params_lgb = {\n        \"objective\": \"binary\",\n          \"boosting_type\": \"gbdt\",\n          \"learning_rate\": learning_rate,\n          \"num_leaves\": num_leaves,\n           \"max_bin\": 256,\n          \"feature_fraction\": feature_fraction,\n          \"verbosity\": 0,\n          \"drop_rate\": 0.1,\n          \"is_unbalance\": False,\n          \"max_drop\": 50,\n          \"min_child_samples\": 10,\n          \"min_child_weight\": 150,\n          \"min_split_gain\": 0,\n          \"subsample\": 0.9\n          }\n\n    params_khanh = {\n        'num_leaves': 2**8, # 512,\n          #'min_child_weight': 0.005,\n          'feature_fraction': 0.8, # 0.7, # 0.6,\n          'bagging_fraction': 0.6, # 0.8,\n          'min_data_in_leaf': 108, # 128,\n          'objective': 'binary',\n          'max_depth': -1, # 8, # 7\n          'learning_rate': 0.05, # 0.005,\n          \"boosting_type\": \"gbdt\",\n          \"pos_bagging_fraction\": 0.8,\n          \"neg_bagging_fraction\": 0.6,\n          \"bagging_freq\": 1,\n          #\"bagging_seed\": 11,\n          \"metric\": 'auc',\n          \"verbosity\": -1,\n          #'reg_alpha': 0.5, # 0.3\n          #'reg_lambda': 0.5, # 0.6\n          'random_state': 47,\n          'max_bin': 254, \n          #'boost_from_average':'false',\n          #'device' : 'gpu',\n          #'gpu_platform_id' : 0,\n          #'gpu_device_id' : 0\n         }\n\n    for i, (train,valid) in pbar:\n        print('_'*100)\n        dtrain = lgb.Dataset(train_data.loc[train], label=labels[train], categorical_feature=categorical_features)\n        dvalid = lgb.Dataset(train_data.loc[valid], label=labels[valid], categorical_feature=categorical_features)\n        \n        # Best iteration selected\n        model = lgb.train(params_khanh,  dtrain, num_boost_round=10000, \n                          valid_sets = [dtrain, dvalid], # verbose_eval=False,\n                          #feval = evalerror,\n                          verbose_eval = False,\n                          early_stopping_rounds = 200)\n\n        oof_preds[valid] = model.predict(train_data.loc[valid], num_iteration=model.best_iteration)\n        \n        best_test_pred = model.predict(test_data, num_iteration=model.best_iteration)\n        best_test_preds.append(best_test_pred)\n\n        fold_importance_df = pd.DataFrame()\n        fold_importance_df[\"feature\"] = train_data.columns\n        fold_importance_df[\"importance\"] = model.feature_importance()\n        fold_importance_df[\"fold\"] = i + 1\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        \n        train_auc_ = model.best_score['training']['auc']\n        val_auc_ = model.best_score['valid_1']['auc']\n        print(f'(Best)  FOLD {i+1}\/{n_folds}: train auc {train_auc_:.4f}. validation auc {val_auc_:.4f}. best iter {model.best_iteration}')\n\n        best_train_auc.update(train_auc_, len(train))\n        best_val_auc.update(val_auc_, len(valid))   \n\n        # visualize\n\n        preds = model.predict(train_data.loc[valid])\n        fpr, tpr, threshold = roc_curve(labels[valid], preds)\n        roc_auc = auc(fpr, tpr)\n        aucs.append(roc_auc)\n        plt.plot(fpr, tpr, label = f'Fold number {i+1} (AUC = {roc_auc:.2f})')\n\n        # Fix number of iterations = 6000\n\n        # model = lgb.train(params,  dtrain, num_boost_round=6000, valid_sets = [dtrain, dvalid], verbose_eval=False, early_stopping_rounds=1000)\n        # fixed_test_pred = model.predict(test_data)\n        # fixed_test_preds.append(fixed_test_pred)\n\n        # train_auc_ = model.best_score['training']['auc']\n        # val_auc_ = model.best_score['valid_1']['auc']\n        # print(f'(Fixed) FOLD {i+1}\/{n_folds}: train auc {train_auc_:.4f}. validation auc {val_auc_:.4f}. best iter {model.best_iteration}')\n        \n        # fixed_train_auc.update(train_auc_, len(train))\n        # fixed_val_auc.update(val_auc_, len(valid))        \n        \n        pbar.set_postfix(BEST_TRAIN=best_train_auc.avg, BEST_VAL=best_val_auc.avg,\n                         FIXED_TRAIN=fixed_train_auc.avg, FIXED_VAL=fixed_val_auc.avg)\n\n    ax.plot([0,1], [0,1], label='Luck', linestyle='--', color='r')  \n    mean_auc = np.mean(aucs)\n    std_auc = np.std(aucs)\n    ax.plot(mean_auc, label=f'Average AUC score: {mean_auc:.4f} $\\pm$ {std_auc:.4f}')  \n    ax.legend(loc=\"lower right\")\n    ax.set(xlim=[-.1, 1.1], ylim=[-.1, 1.1], title='Light GBM')\n    plt.show()\n\n    best_test_pred = np.mean(best_test_preds, axis=0)\n    submiss = pd.DataFrame({\"id\":index, \"label\": best_test_pred})\n    submiss_path = os.path.join(submiss_dir, f'LGB_full_{mean_auc:.4f}.csv')\n    submiss.to_csv(submiss_path, index=False) \n\n    #model = lgb.train(params_khanh,  # best sccore\n    #              dtrain, \n    #              num_boost_round=6000, \n                #   valid_sets = [dtrain, dvalid], \n    #              verbose_eval=300, \n                #  feval=evalerror\n                #   early_stopping_rounds=1000\n    #              )\n    #full_test_pred = model.predict(encoded_test_full, num_iteration=model.best_iteration) \n\n    #submiss['label'] = np.mean([best_test_pred, full_test_pred], axis=0) \n    #submiss_path = os.path.join(submiss_dir, f'LGB_full2_{mean_auc:.4f}.csv')\n    #submiss.to_csv(submiss_path, index=False)   \n    np.save(f'oof_lgb_full_{mean_auc:.4f}.npy', oof_preds)\n    display_importances(feature_importance_df)","23281502":"#%%time\n#runLGB(train_data = X_train_full, \n#      labels=y_train_full, \n#      test_data=encoded_test_full,\n#      index=test.index,\n#      n_folds=5,\n#      submiss_dir=submiss_dir)","817baa11":"**Categorical + Numerical Feature**","b31b9737":"**FEATURE ENGINEERING**","58232eb4":"**Feature Selection: Feature Selection with Null Importances**\n\nreference: https:\/\/www.kaggle.com\/ogrellier\/feature-selection-with-null-importances\/comments","00e82b99":"### LightGBM","e403543d":"**Frequency\/Count Encoding**","ae69b703":"**CatBoost Model: Main Solution**","4c10aaa3":"**Combine Feature & Statistical Feature base on some categories and continuous feature**","9baf8dcf":"**Date Time Feature**\n\n['F_startDate', 'F_endDate', 'E_startDate', 'E_endDate',\n                    'C_startDate', 'C_endDate', 'G_startDate', 'G_endDate',\n                    'A_startDate', 'A_endDate', 'Field_33', 'Field_8',\n                'Field_1', 'Field_2', 'Field_9', 'Field_7', 'Field_5',\n                'Field_6', 'Field_32']\n\n\n1. day_of_week feature\n2. month feature\n3. week feature","8650b13a":"**Basic EDA**","7fd9257f":"**Basic Feature Engineering**\n\n1. Rank + Norm Feature (Field 20)\n2. Multiply + Divide Featrue\n3. Transform Feature: Field 20 + Field 74\n4. Statistical Feature: Mean, Std: Summary, numOrg, partner_cols","b29e3cdf":"**Training 5fold with CatBoost Model**","3ab20515":"**Some Technique for FEATURE ENGINEERING:**\n\n1. GROUP AGGREGATION NUNIQUE\n2. FREQUENCY ENCODE\n3. GROUP AGGREGATION MEAN AND STD\n4. LABEL ENCODE\n5. COMBINE FEATURES","22558b44":"**Drop Some Numerical Feature base on check correlation**","fe29b6c5":"**LightGBM Model**","f4d4962c":"**DATA CLEANING**","c850e544":"**XGBoost Model**"}}