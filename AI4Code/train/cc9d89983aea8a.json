{"cell_type":{"2e62060e":"code","f81f3b58":"code","5306a7bd":"code","584050bc":"code","aa699f8b":"code","1c5f3413":"code","ae92b1e9":"code","09b8c44f":"code","6865dedd":"code","77618fa0":"code","01184887":"markdown"},"source":{"2e62060e":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ntorch.manual_seed(1)","f81f3b58":"lstm = nn.LSTM(3, 3) #input dimension and output dimension are 3\ninputs = [ torch.randn(1, 3) for _ in range(5) ] #make a sequence of length 5\n\nhidden = ( torch.randn(1, 1, 3), torch.randn(1, 1, 3) )\nfor i in inputs:\n    out, hidden = lstm( i.view(1,1, -1), hidden ) #torch requires it's inputs to be 3d tensors\n    #out gives access to all hidden states in network from the past, while hidden returnes only the last one\n\ninputs = torch.cat( inputs).view( len(inputs), 1, -1) #changes a list of tensors into a tensor\nhidden = ( torch.randn(1, 1, 3), torch.randn(1, 1, 3) ) #clean out the hidden state\nout, hidden = lstm(inputs, hidden)","5306a7bd":"#w1, w2, w3... are our input sentence, where w.i E v - our vocabulary\n#T - tag set\n#y.i - tag of word w.i\n#y'.i - prediction tag for word w.i\n\n#Assign each tag a unique index, like in BOW we had wrd_to_ix. Pass the sentence through LSTM. Call the hidden state at the timestamp i, h.i\n#Our prediction is y'.i = argmax_j( log_softmax( Ah.i + b)_j )","584050bc":"def prepare_sequence( seq, to_ix ):\n    idxs = [ to_ix[w] for w in seq ]\n    return torch.tensor(idxs, dtype = torch.long )","aa699f8b":"training_data = [\n    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n]","1c5f3413":"word_to_ix = {}\nfor sent, tags in training_data:\n    for word in sent:\n        if word not in word_to_ix:\n            word_to_ix[word] = len( word_to_ix )\nprint( word_to_ix )\ntag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n\n#these will usually be 32-64 dimensional\nEMBEDDING_DIM = 6\nHIDDEN_DIM = 6","ae92b1e9":"class LSTMTagger(nn.Module):\n    def __init__(self,  embedding_dim, vocab_size, hidden_size, tagset_size):\n        super(LSTMTagger, self).__init__()\n        self.hidden_dim = hidden_size\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n        \n        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim) #lstm takes embeddings of dimension embedding_dim, and outputs hidden layer of size hidden_size\n        self.hidden2tag = nn.Linear(hidden_size, tagset_size) #this is how we use a trained hidden state to return tags\n        self.hidden = self.init_hidden() \n    \n    def init_hidden(self):\n        return ( torch.zeros(1, 1, self.hidden_dim), torch.zeros(1, 1, self.hidden_dim) ) #you create those two hidden states: an immediate one and the long term one\n    \n    def forward(self, sentence):\n        #you look at embeddings of the words in the sentence\n        embeds = self.word_embeddings(sentence)#where sentence is a tensor of indexes\n        \n        lstm_out, self.hidden = self.lstm( embeds.view( len(sentence), 1, -1) , self.hidden) \n        tag_space = self.hidden2tag( lstm_out.view( len(sentence), -1) )\n        tag_scores = F.log_softmax( tag_space, dim=1)\n        return tag_scores\n        ","09b8c44f":"#TRAIN THE MODEL","6865dedd":"model = LSTMTagger( EMBEDDING_DIM, len(word_to_ix), HIDDEN_DIM, len(tag_to_ix) )\nloss_function = nn.NLLLoss()\noptimizer = optim.SGD( model.parameters(), lr = 0.1)\n\nwith torch.no_grad():\n    inputs = prepare_sequence( training_data[0][0], word_to_ix)\n    tag_scores = model(inputs)\n    print(tag_scores) #i, j entry is the score of ith input for the jth tag\n    \nepochs = 300\nfor epoch in range(epochs):\n    for sentence, tags in training_data:\n        #clean gradients\n        model.zero_grad()\n        #clean history \n        model.hidden = model.init_hidden()\n        \n        #prepare data\n        sentence_in = prepare_sequence( sentence, word_to_ix )\n        targets = prepare_sequence( tags, tag_to_ix )\n        \n        #train\n        tag_scores = model(sentence_in)\n        \n        loss = loss_function(tag_scores, targets)\n        loss.backward()\n        optimizer.step()\n        \nwith torch.no_grad():\n    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n    tag_scores = model(inputs)\n    print( tag_scores)\n        ","77618fa0":"#This work is based on the code and knowledge provided on the website https:\/\/pytorch.org\/tutorials\/beginner\/nlp\/sequence_models_tutorial.html. It contains my interpretation\n#of the techniques published on the website.","01184887":"**EXAMPLE: AN LSTM FOR PART-OF-SPEECH TAGGING**"}}