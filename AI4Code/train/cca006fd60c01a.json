{"cell_type":{"67fdc804":"code","c8458691":"code","cf1dcf3d":"code","50ab4d25":"code","bc2dafa4":"code","7fe69193":"code","02e90e92":"code","121bf71b":"code","90aaed2f":"code","725c3f4a":"code","08d1574f":"code","48757461":"code","03ad5da8":"code","1264ad8b":"code","6ed94095":"code","f246182c":"code","2c601541":"code","af8b792f":"code","d48e7f5b":"code","f8d399f5":"code","4c668ccc":"code","a56e255c":"code","7372c9bf":"code","cf37b172":"code","787aefaa":"code","34e87674":"code","f9c17696":"code","e083ec48":"code","069f1917":"code","9fb2fbcd":"code","c9f7043e":"code","62ba1b85":"code","5f83b69d":"code","f8df58ea":"code","2a66a176":"code","a8dbe9e1":"markdown","fa80e423":"markdown","b8eace2e":"markdown","9494d6e3":"markdown","b7db0fec":"markdown","f5b5c8f2":"markdown","24c0aefe":"markdown","7460f038":"markdown","1c9a16c4":"markdown","ab6cfa5d":"markdown"},"source":{"67fdc804":"#Import Library\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier, NeighborhoodComponentsAnalysis,LocalOutlierFactor\nfrom sklearn.decomposition import PCA\n\n#warning library\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c8458691":"dataraw = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")\ndataraw.head()","cf1dcf3d":"#Id and Unnamed: 32 columns are unnecessary, I delete them\ndf = dataraw.copy()\ndf.drop([\"id\",\"Unnamed: 32\"], inplace = True, axis = 1)\ndf.head()","50ab4d25":"df = df.rename(columns = {\"diagnosis\":\"target\"})","bc2dafa4":"sns.countplot(df[\"target\"]);","7fe69193":"df.target.value_counts()","02e90e92":"## Diagnosis convert to int","121bf71b":"df[\"target\"] = [1 if i.strip() == \"M\" else 0 for i in df.target]","90aaed2f":"df.shape","725c3f4a":"df.info()","08d1574f":"## all float64 and no missing value","48757461":"df.describe().T","03ad5da8":"## area values are so big, I have to Standardization after...","1264ad8b":"#Modelde \u00e7e\u015fitlili\u011fe gitmek, model e\u011fitimi a\u00e7\u0131s\u0131ndan faydal\u0131d\u0131r. \u00c7e\u015fitlilik;\n#Birbirleri aras\u0131nda d\u00fc\u015f\u00fck corelationa sahip featurelar\u0131 model e\u011fitirken kullan\u0131rsak daha iyi bir model e\u011fitebiliriz.\n#Di\u011fer yandan birbiri ile aras\u0131nda y\u00fcksek korelasyona sahip featurelar model e\u011fitimine katk\u0131s\u0131 ayn\u0131d\u0131r, bunlar\u0131 \u00e7\u0131kartmak gerekebilir.\n\n\n#correlation matrix\ncorr_matrix = df.corr()\nsns.clustermap(corr_matrix, annot = True, figsize=(20,15), fmt=\".2f\" )\nplt.title(\"Correlation Between Features\")\nplt.show()\n","6ed94095":"#filtre\n\nthreshold = 0.75\nfiltre = np.abs(corr_matrix[\"target\"]) > threshold\ncorr_features = corr_matrix.columns[filtre].tolist()\nsns.clustermap(df[corr_features].corr(), annot = True, fmt = \".2f\")\nplt.title(\"Correlation Between Features w\/ Corr Threshold 0.75)\")\nplt.show()","f246182c":"# There are some correlated features\n\n#E\u011fer birbirleriyle do\u011fru ya da ters orant\u0131l\u0131 corelation features varsa bunlar\u0131 kald\u0131rmak gerekir.","2c601541":"sns.pairplot(df[corr_features], diag_kind = \"kde\", markers = \"+\",hue = \"target\")\nplt.show()","af8b792f":"##some features distribution has positive skewness","d48e7f5b":"y = df.target\nx = df.drop([\"target\"],axis = 1)\ncolumns = x.columns.tolist()\n\nclf = LocalOutlierFactor()\ny_pred = clf.fit_predict(x)\nX_score = clf.negative_outlier_factor_\n\noutlier_score = pd.DataFrame()\noutlier_score[\"score\"] = X_score\n\n# threshold\nthreshold = -2.5\nfiltre = outlier_score[\"score\"] < threshold\noutlier_index = outlier_score[filtre].index.tolist()\n\n\nplt.figure()\nplt.scatter(x.iloc[outlier_index,0], x.iloc[outlier_index,1],color = \"blue\", s = 50, label = \"Outliers\")\nplt.scatter(x.iloc[:,0], x.iloc[:,1], color = \"k\", s = 3, label = \"Data Points\")\n\nradius = (X_score.max() - X_score)\/(X_score.max() - X_score.min())\noutlier_score[\"radius\"] = radius\nplt.scatter(x.iloc[:,0], x.iloc[:,1], s = 1000*radius, edgecolors = \"r\",facecolors = \"none\", label = \"Outlier Scores\")\nplt.legend()\nplt.show()\n\n","f8d399f5":"# drop outliers\nx = x.drop(outlier_index)\ny = y.drop(outlier_index).values","4c668ccc":"test_size = 0.3\nX_train,X_test,Y_train,Y_test = train_test_split(x,y,test_size = test_size, random_state = 42)","a56e255c":"## Standardization\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\nX_train_df = pd.DataFrame(X_train,columns = columns)\nX_train_df_describe = X_train_df.describe()\nX_train_df[\"target\"] = Y_train\n\n","7372c9bf":"#BOXPLOT\n\ndata_melted = pd.melt(X_train_df, id_vars = \"target\",\n                      var_name = \"features\",\n                      value_name = \"value\")\n\nplt.figure(figsize = (35,15))\nsns.boxplot(x = \"features\", y = \"value\", hue = \"target\", data = data_melted)\nplt.xticks(rotation = 90)\nplt.show()\n\n\n# pair plot \nsns.pairplot(X_train_df[corr_features], diag_kind = \"kde\", markers = \"+\",hue = \"target\")\nplt.show()\n","cf37b172":"knn = KNeighborsClassifier(n_neighbors = 2)\nknn.fit(X_train, Y_train)\ny_pred = knn.predict(X_test)\ncm = confusion_matrix(Y_test, y_pred)\nacc = accuracy_score(Y_test, y_pred)\nprint()\nprint(\"Basic KNN Test Accuracy\",acc)\nprint(cm)","787aefaa":"#Our Basic KNN Model achieved 95% success It's good value.\n#But it might be overfitting, we just checked the Test Set Accuracy.We have to look at the Train Set Accuracy too. \n","34e87674":"## choose best parameters\ndef KNN_Best_Params(x_train, x_test, y_train, y_test):\n    k_range = list(range(1,31))\n    weight_options = [\"uniform\",\"distance\"]\n    p_val = [1,2]\n    print()\n    param_grid = dict(n_neighbors = k_range, weights = weight_options, p = p_val)\n    \n    knn = KNeighborsClassifier()\n    grid = GridSearchCV(knn, param_grid, cv = 10, scoring = \"accuracy\")\n    grid.fit(x_train, y_train)\n    \n    print(\"Best training score: {} with parameters: {}\".format(grid.best_score_, grid.best_params_))\n    print()\n    \n    knn = KNeighborsClassifier(**grid.best_params_)\n    knn.fit(x_train, y_train)\n\n    y_pred_test = knn.predict(x_test)\n    y_pred_train = knn.predict(x_train)\n    \n    cm_test = confusion_matrix(y_test, y_pred_test)\n    cm_train = confusion_matrix(y_train, y_pred_train)\n    \n    acc_test = accuracy_score(y_test, y_pred_test)\n    acc_train = accuracy_score(y_train, y_pred_train)\n    \n    print(\"Test score: {} || Train score: {} \".format(acc_test,acc_train))\n    print()\n    print(\"CM Test:\\n {}\\nCM Train:\\n {}\".format(cm_test,cm_train))\n    \n    return grid\n\n\ngrid = KNN_Best_Params(X_train, X_test, Y_train, Y_test)\n","f9c17696":"#Best Parameters for this algorithm : 'n_neighbors': 5, 'p': 1, 'weights': 'uniform'\n#And Test Score %95","e083ec48":"scaler = StandardScaler()\nx_scaled = scaler.fit_transform(x)\n\npca = PCA(n_components = 2)\npca.fit(x_scaled)\nX_reduced_pca = pca.transform(x_scaled)\npca_data = pd.DataFrame(X_reduced_pca, columns = [\"p1\",\"p2\"])\npca_data[\"target\"] = y\nsns.scatterplot(x = \"p1\", y = \"p2\", hue = \"target\", data = pca_data)\nplt.title(\"PCA: p1 vs p2\")\n\n\nX_train_pca, X_test_pca, Y_train_pca, Y_test_pca = train_test_split(X_reduced_pca, y, test_size = test_size, random_state = 42)\n\ngrid_pca = KNN_Best_Params(X_train_pca, X_test_pca, Y_train_pca, Y_test_pca)","069f1917":"#92% success. Looks like it's not working\n\n#So, wich ones are correct and which ones are wrong?","9fb2fbcd":"#Visualize\ncmap_light = ListedColormap(['orange',  'cornflowerblue'])\ncmap_bold = ListedColormap(['darkorange', 'darkblue'])\n\nh = .05 # step size in the mesh\nX = X_reduced_pca\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = grid_pca.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure()\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"%i-Class classification (k = %i, weights = '%s')\"\n          % (len(np.unique(y)),grid_pca.best_estimator_.n_neighbors, grid_pca.best_estimator_.weights))","c9f7043e":"nca = NeighborhoodComponentsAnalysis(n_components = 2, random_state = 42)\nnca.fit(x_scaled, y)\nX_reduced_nca = nca.transform(x_scaled)\nnca_data = pd.DataFrame(X_reduced_nca, columns = [\"p1\",\"p2\"])\nnca_data[\"target\"] = y\nsns.scatterplot(x = \"p1\",  y = \"p2\", hue = \"target\", data = nca_data)\nplt.title(\"NCA: p1 vs p2\")\n\n","62ba1b85":"X_train_nca, X_test_nca, Y_train_nca, Y_test_nca = train_test_split(X_reduced_nca, y, test_size = test_size, random_state = 42)\n\ngrid_nca = KNN_Best_Params(X_train_nca, X_test_nca, Y_train_nca, Y_test_nca)","5f83b69d":"#Now, looks like NCA works. %99 success.\n\n#We have one wrong predict. Let's visualize.","f8df58ea":"cmap_light = ListedColormap(['orange',  'cornflowerblue'])\ncmap_bold = ListedColormap(['darkorange', 'darkblue'])\n\nh = .2 # step size in the mesh\nX = X_reduced_nca\nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nxx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n                     np.arange(y_min, y_max, h))\n\nZ = grid_nca.predict(np.c_[xx.ravel(), yy.ravel()])\n\n# Put the result into a color plot\nZ = Z.reshape(xx.shape)\nplt.figure()\nplt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n\n# Plot also the training points\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold,\n            edgecolor='k', s=20)\nplt.xlim(xx.min(), xx.max())\nplt.ylim(yy.min(), yy.max())\nplt.title(\"%i-Class classification (k = %i, weights = '%s')\"\n          % (len(np.unique(y)),grid_nca.best_estimator_.n_neighbors, grid_nca.best_estimator_.weights))","2a66a176":"#Find wrong decision\n\nknn = KNeighborsClassifier(**grid_nca.best_params_)\nknn.fit(X_train_nca,Y_train_nca)\ny_pred_nca = knn.predict(X_test_nca)\nacc_test_nca = accuracy_score(y_pred_nca,Y_test_nca)\nknn.score(X_test_nca,Y_test_nca)\n\ntest_data = pd.DataFrame()\ntest_data[\"X_test_nca_p1\"] = X_test_nca[:,0]\ntest_data[\"X_test_nca_p2\"] = X_test_nca[:,1]\ntest_data[\"y_pred_nca\"] = y_pred_nca\ntest_data[\"Y_test_nca\"] = Y_test_nca\n\nplt.figure()\nsns.scatterplot(x=\"X_test_nca_p1\", y=\"X_test_nca_p2\", hue=\"Y_test_nca\",data=test_data)\n\ndiff = np.where(y_pred_nca!=Y_test_nca)[0]\nplt.scatter(test_data.iloc[diff,0],test_data.iloc[diff,1],label = \"Wrong Classified\",alpha = 0.2,color = \"red\",s = 1000)","a8dbe9e1":"## Basic KNN Method","fa80e423":"## Neighborhood Component Analysis","b8eace2e":"## Outlier Detection","9494d6e3":"## Read Data","b7db0fec":"## Final Report : \n\nWith NCA;\n    \nBest training score: **0.9898717948717948** with parameters: **{'n_neighbors': 1, 'p': 1, 'weights': 'uniform'}**\n\nTest score: **0.9941520467836257** || Train score: **1.0**\n    \n    \n    ","f5b5c8f2":"## Breast Cancer Wisconsin - KNN\n\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\nn the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server:\nftp ftp.cs.wisc.edu\ncd math-prog\/cpo-dataset\/machine-learn\/WDBC\/\n\nAlso can be found on UCI Machine Learning Repository: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29\n\nAttribute Information:\n\n1) ID number\n2) Diagnosis (M = malignant, B = benign)\n3-32)\n\nTen real-valued features are computed for each cell nucleus:\n\n- a) radius (mean of distances from center to points on the perimeter)\n- b) texture (standard deviation of gray-scale values)\n- c) perimeter\n- d) area\n- e) smoothness (local variation in radius lengths)\n- f) compactness (perimeter^2 \/ area - 1.0)\n- g) concavity (severity of concave portions of the contour)\n- h) concave points (number of concave portions of the contour)\n- i) symmetry\n- j) fractal dimension (\"coastline approximation\" - 1)\n\nThe mean, standard error and \"worst\" or largest (mean of the three\nlargest values) of these features were computed for each image,\nresulting in 30 features. For instance, field 3 is Mean Radius, field\n13 is Radius SE, field 23 is Worst Radius.\n\nAll feature values are recoded with four significant digits.\n\nMissing attribute values: none\n\nClass distribution: 357 benign, 212 malignant","24c0aefe":"[![](https:\/\/www.oguzerdogan.com\/wp-content\/uploads\/2020\/08\/logo_.png)](https:\/\/www.oguzerdogan.com)","7460f038":"## Exploratory Data Analysis","1c9a16c4":"## Train Test Split","ab6cfa5d":"## Principal Component Analysis (PCA)"}}