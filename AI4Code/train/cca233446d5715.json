{"cell_type":{"0b019794":"code","342f525a":"code","2dbadde5":"code","bed6c2f5":"code","c232091d":"code","9cf05c07":"code","56a74837":"code","e992bc31":"code","d036d52e":"code","bfe910c3":"code","88d667f3":"code","16468546":"code","54b47019":"code","5c372683":"code","b8441e50":"code","4b877762":"code","5d86492c":"code","7a363ae9":"code","da1d13cf":"code","64f7966e":"code","260cab46":"code","ae4afa6e":"code","3a2765ee":"code","92206289":"code","ae92f5aa":"code","c5def9fa":"code","17cae7f8":"code","0fc40719":"code","4c39eb00":"code","770696ba":"code","371c173f":"code","bbe0822e":"code","0d45d21d":"code","f28a6362":"code","07393ebc":"code","debbc853":"code","4cbbde95":"code","2e7dfa57":"code","b85c737c":"code","907d28a4":"code","bccab7e3":"code","0cb6492e":"code","f82fa96f":"code","1203f7a4":"code","87642b1c":"code","3f246a46":"code","73e7f1ae":"code","10ecc604":"code","222df1f3":"code","d6f773e1":"code","f7683f6b":"markdown","4edfc969":"markdown","d37ed1e0":"markdown","bff80958":"markdown","14a0f1dc":"markdown","140d34b7":"markdown","a7ef97ce":"markdown","5c105684":"markdown","7012dca1":"markdown","b40b59fd":"markdown"},"source":{"0b019794":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy\nfrom scipy import stats\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import StratifiedKFold\n%matplotlib inline\n\n","342f525a":"####Import Temp Time Series Data\ntrain_temp = pd.read_csv(\"..\/input\/jane-street-market-prediction\/train.csv\", nrows=5)\n","2dbadde5":"# Get information on the datatypes\ntrain_temp.info()","bed6c2f5":"train_cols=list(train_temp.columns)\nlen(train_cols)","c232091d":"# Find out the smallest data type possible for each numeric feature\nfloat_cols = train_temp.select_dtypes(include=['float'])\nint_cols = train_temp.select_dtypes(include=['int'])\n\nfor cols in float_cols.columns:\n    train_temp[cols] = pd.to_numeric(train_temp[cols], downcast='float')\n    \nfor cols in int_cols.columns:\n    train_temp[cols] = pd.to_numeric(train_temp[cols], downcast='integer')\n\nprint(train_temp.info())","9cf05c07":"train_cols_dict = { i : 'float32' for i in train_cols }","56a74837":"int_cols_names=list(int_cols.columns)\nint_cols_names","e992bc31":"train_cols_dict[ 'feature_0']  =  'int8'\ntrain_cols_dict['ts_id']  =  'int32'","d036d52e":"train_cols_dict","bfe910c3":"train = pd.read_csv(\"..\/input\/jane-street-market-prediction\/train.csv\", usecols=train_cols,dtype=train_cols_dict)","88d667f3":"print(train.info())","16468546":"ID=train.iloc[::, -1:]","54b47019":"ID","5c372683":"date_weight=train.iloc[::, 0:2]","b8441e50":"features_resp=train.iloc[::, 6:-1]","4b877762":"updated_train = pd.merge(date_weight, features_resp, left_index=True, right_index=True, how='inner')\nupdated_train2 = pd.merge(ID, updated_train, left_index=True, right_index=True, how='inner')","5d86492c":"updated_train2[:5588]","7a363ae9":"updated_train2","da1d13cf":"\ntrend=updated_train2[['date','resp','feature_0']]\ndf1=trend.groupby(['date']).resp.mean()","64f7966e":"# Plot with subplots\ndf1.plot(subplots=True)\nplt.show()","260cab46":"df1[0:100].plot(subplots=True)","ae4afa6e":"df1[100:200].plot(subplots=True)","3a2765ee":"df1[200:300].plot(subplots=True)","92206289":"df1[300:400].plot(subplots=True)","ae92f5aa":"df1[400:500].plot(subplots=True)","c5def9fa":"df2=trend.groupby(['date']).resp.sum()","17cae7f8":"# Plot with subplots\ndf2.plot(subplots=True)\nplt.show()","0fc40719":"df2[0:100].plot(subplots=True)","4c39eb00":"df2[100:200].plot(subplots=True)","770696ba":"df2[200:300].plot(subplots=True)","371c173f":"df2[300:400].plot(subplots=True)","bbe0822e":"df2[400:500].plot(subplots=True)","0d45d21d":"df2.sum()","f28a6362":"from statsmodels.tsa.seasonal import seasonal_decompose\ndf3=pd.DataFrame(df2.copy())\ndf3['resp'] = df3['resp'].cumsum()\n\nresult = seasonal_decompose(df3, model=\"add\", freq = 88)\nfig = result.plot()\n\n#result = seasonal_decompose(df1, freq = 88)\n#df1[\"trend\"]=result.trend\n#df1[\"seasonal\"]=result.seasonal","07393ebc":"#Additive Seasonal Effect (add) means the peaks and valleys are someone similar over time. \n#Multiplicative Seasonal Effect (mul) means the peaks and valleys increase over time. \nresult = seasonal_decompose(df3, model=\"add\", freq = 88)\ndf3[\"trend\"]=result.trend\ndf3[\"seasonal\"]=result.seasonal","debbc853":"df3.plot(figsize = (14,6), grid = True);","4cbbde95":"\ndf4=pd.DataFrame(df2.copy())\n\n\nresult = seasonal_decompose(df4, model=\"add\", freq = 88)\nfig = result.plot()","2e7dfa57":"result = seasonal_decompose(df4, model=\"add\", freq = 88)\ndf4[\"trend\"]=result.trend\ndf4[\"seasonal\"]=result.seasonal\ndf4.plot(figsize = (14,6), grid = True);","b85c737c":"###Explore NaN Values###\n\nnans=pd.DataFrame(updated_train2.isnull().sum(axis = 0))\nnans.reset_index(drop=False, inplace=True)\nnans.columns = ['column','nans_num']\nhigh_nans=nans[nans['nans_num']>100000]\nnans=nans[nans['nans_num']>0]\nprint(high_nans.sort_values(by=['nans_num'], ascending=False))","907d28a4":"nans_list=nans[['column']]\narr=np.ravel(np.array(nans_list.astype(str)))\nnans_list2 = arr.tolist()\nnans_list2 ","bccab7e3":"# Replace the NaNs in column by the mean of values\n# in column nans_list2 respectively\n#updated_train2[nans_list2] = updated_train2[nans_list2].fillna(value=updated_train2[nans_list2].mean())\n#print(updated_train2)","0cb6492e":"updated_train2.isnull().sum(axis = 0)","f82fa96f":"means = updated_train2.groupby(['date'])[nans_list2].mean()\nupdated_train2 = updated_train2.set_index(['date'])\nupdated_train2[nans_list2] = updated_train2[nans_list2].fillna(means)\nupdated_train2 = updated_train2.reset_index()\nprint(updated_train2)","1203f7a4":"updated_train2.isnull().sum(axis = 0)","87642b1c":"means","3f246a46":"### Audit Conditional Means to Confirm they Remain the Same\nupdated_train2[updated_train2['date']==3].describe()","73e7f1ae":"updated_train2.date=pd.to_numeric(updated_train2.date, downcast='integer')\nupdated_train2.info()","10ecc604":"y=updated_train2.date\nX=updated_train2.date\n# Use stratified k-fold to create multiple datasets with date structure in place\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n# enumerate the splits and summarize the distributions\ntest_list=[]\nfor train_ix, test_ix in kfold.split(X, y):\n    test_list.append(test_ix)\n\n\n    \n","222df1f3":"train_df_list=[]\nfor i in range(5):\n    df=updated_train2.iloc[list(test_list[i])]\n    train_df_list.append(df)\n    \n    ","d6f773e1":"train_df_list[1]","f7683f6b":"# Aggregated Returns Over Time Trends and Seasonality","4edfc969":"# Reduce Memory Use Technique by Down Casting\nSpecial thanks to https:\/\/www.kaggle.com\/akosciansky\/how-to-import-large-csv-files-and-save-efficiently. ","d37ed1e0":"# Replace NaN's with Conditional Means based on Column and Date","bff80958":"# Goals: \n\n## 1) Reduce the memory usage in processing Jane Street Data.\n\n## 2) Look at overall market trends by date using mean and sum.\n\n## 3) Use conditional mean by date to impute missing values.","14a0f1dc":"# Find NaN Values","140d34b7":"# Returns Trends and Seasonality","a7ef97ce":"# Plotting overall market trends by date using mean and sum","5c105684":"# Splitting Data into 5 Folds Based on Date to Get More Variation in Conditional Means:\nThis is not complete yet. But, the idea is to hash out conditional mean imputation above and to spit data into 5 folds based on date before calculating conditional mean to get more variation since so many values are missing. ","7012dca1":"# Time Series Analysis EDA \n\nTrend of gains over time. \n","b40b59fd":"# Replace NaN Values with date Column Means (Hashed Out)"}}