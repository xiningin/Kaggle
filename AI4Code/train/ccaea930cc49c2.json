{"cell_type":{"362c8f1c":"code","4e3a5155":"code","ce7e19bf":"code","c7a18218":"code","c8bf05a6":"code","070bfdb0":"code","cb9f1a65":"code","d2e3b72a":"code","0b358f98":"code","9e5d066c":"code","0e48a158":"code","51d048b6":"code","633d60a0":"code","878178ae":"code","12c3c253":"code","fd450e58":"code","882756c3":"code","3920c176":"code","604fc957":"code","532ca570":"code","905e926d":"code","04c68767":"code","13a37f1d":"code","1b07519a":"code","3176eda7":"code","91e860a9":"code","c90bf72f":"code","d7044cd1":"code","3c0c9225":"code","c2617620":"code","eaf0a333":"code","04b8d2ee":"code","cfbf95ce":"code","cbbc96cb":"code","58ad4b8d":"markdown","7464f85c":"markdown","290575b9":"markdown","b9f7d486":"markdown"},"source":{"362c8f1c":"%matplotlib inline\nfrom fastai.basics import *","4e3a5155":"path = Config().data_path()\/'mnist'","ce7e19bf":"path.mkdir(parents=True)","c7a18218":"path.ls()","c8bf05a6":"!wget http:\/\/deeplearning.net\/data\/mnist\/mnist.pkl.gz -P {path}","070bfdb0":"with gzip.open(path\/'mnist.pkl.gz', 'rb') as f:\n    ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')","cb9f1a65":"plt.imshow(x_train[0].reshape((28,28)), cmap=\"gray\")\nx_train.shape","d2e3b72a":"x_train,y_train,x_valid,y_valid = map(torch.tensor, (x_train,y_train,x_valid,y_valid))\nn,c = x_train.shape\nx_train.shape, y_train.min(), y_train.max()","0b358f98":"bs=64\ntrain_ds = TensorDataset(x_train, y_train)\nvalid_ds = TensorDataset(x_valid, y_valid)\ndata = DataBunch.create(train_ds, valid_ds, bs=bs)","9e5d066c":"x,y = next(iter(data.train_dl))\nx.shape,y.shape","0e48a158":"class Mnist_Logistic(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin = nn.Linear(784, 10, bias=True)\n\n    def forward(self, xb): return self.lin(xb)","51d048b6":"model = Mnist_Logistic().cuda()","633d60a0":"model","878178ae":"model.lin","12c3c253":"model(x).shape","fd450e58":"[p.shape for p in model.parameters()]","882756c3":"lr=2e-2","3920c176":"loss_func = nn.CrossEntropyLoss()","604fc957":"def update(x,y,lr):\n    wd = 1e-5\n    y_hat = model(x)\n    # weight decay\n    w2 = 0.\n    for p in model.parameters(): w2 += (p**2).sum()\n    # add to regular loss\n    loss = loss_func(y_hat, y) + w2*wd\n    loss.backward()\n    with torch.no_grad():\n        for p in model.parameters():\n            p.sub_(lr * p.grad)\n            p.grad.zero_()\n    return loss.item()","532ca570":"losses = [update(x,y,lr) for x,y in data.train_dl]","905e926d":"plt.plot(losses);","04c68767":"class Mnist_NN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.lin1 = nn.Linear(784, 50, bias=True)\n        self.lin2 = nn.Linear(50, 10, bias=True)\n\n    def forward(self, xb):\n        x = self.lin1(xb)\n        x = F.relu(x)\n        return self.lin2(x)","13a37f1d":"model = Mnist_NN().cuda()","1b07519a":"losses = [update(x,y,lr) for x,y in data.train_dl]","3176eda7":"plt.plot(losses);","91e860a9":"model = Mnist_NN().cuda()","c90bf72f":"def update(x,y,lr):\n    opt = optim.Adam(model.parameters(), lr)\n    y_hat = model(x)\n    loss = loss_func(y_hat, y)\n    loss.backward()\n    opt.step()\n    opt.zero_grad()\n    return loss.item()","d7044cd1":"losses = [update(x,y,1e-3) for x,y in data.train_dl]","3c0c9225":"plt.plot(losses);","c2617620":"learn = Learner(data, Mnist_NN(), loss_func=loss_func, metrics=accuracy)","eaf0a333":"learn.lr_find()\nlearn.recorder.plot()","04b8d2ee":"learn.fit_one_cycle(1, 1e-2)","cfbf95ce":"learn.recorder.plot_lr(show_moms=True)","cbbc96cb":"learn.recorder.plot_losses()","58ad4b8d":"In lesson2-sgd we did these things ourselves:\n\n```python\nx = torch.ones(n,2) \ndef mse(y_hat, y): return ((y_hat-y)**2).mean()\ny_hat = x@a\n```\n\nNow instead we'll use PyTorch's functions to do it for us, and also to handle mini-batches (which we didn't do last time, since our dataset was so small).","7464f85c":"## MNIST SGD","290575b9":"Get the 'pickled' MNIST dataset from http:\/\/deeplearning.net\/data\/mnist\/mnist.pkl.gz. We're going to treat it as a standard flat dataset with fully connected layers, rather than using a CNN.","b9f7d486":"## fin"}}