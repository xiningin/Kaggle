{"cell_type":{"bc14baea":"code","dd3438ab":"code","35c833da":"code","73d8614e":"code","456afbdc":"code","a3b98417":"code","5625fb94":"code","98438605":"code","6118a0f4":"code","9cff2f9c":"code","8031eb68":"code","6076210d":"code","d386a87b":"code","f10ac0da":"code","70c44300":"code","bbfdb959":"code","e5f3c3ff":"code","4fdb9e66":"code","d95fa49d":"code","1023684a":"code","564193b0":"code","476f55ee":"code","c47b96bf":"code","82ccea5b":"code","3ce4001d":"code","57921295":"code","cdc2bddb":"code","9e43faf4":"code","506a9d61":"code","66b253f9":"code","b56ac22e":"code","df1e8d7d":"code","a8aa865c":"markdown","4e96dc4b":"markdown","9caf84e1":"markdown","192a1192":"markdown","68291d72":"markdown","05b00cef":"markdown","6923d613":"markdown","e991aa2d":"markdown","48a6885b":"markdown","1589243c":"markdown","cdc36a84":"markdown","e5394226":"markdown","8ec95b22":"markdown","a2996623":"markdown","bef6a3d2":"markdown","186e14ed":"markdown","05a9c56c":"markdown","4da7b83b":"markdown","e0444f8b":"markdown","ea5991d1":"markdown","6ed5cece":"markdown","a65b2018":"markdown","fc01cf73":"markdown","ad98d525":"markdown","f2dc638e":"markdown","e59fed95":"markdown","40b12432":"markdown","8a3b1faf":"markdown","f2bdcfb1":"markdown"},"source":{"bc14baea":"import spacy\nspacy.prefer_gpu()\nimport json\nfrom tqdm.auto import tqdm\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport gzip\n!pip install opentargets\nfrom opentargets import OpenTargetsClient\n!pip install scispacy\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_ner_bionlp13cg_md-0.2.4.tar.gz\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_ner_bc5cdr_md-0.2.4.tar.gz","dd3438ab":"def extract_paper_annotations():\n    \"\"\"\n    This function looks at all the papers in the CORD-19 dataset and extract entities\n    \"\"\"\n    # Define the list of papers we will process\n    #papers = [Path(\"\/kaggle\/input\/CORD-19-research-challenge\/biorxiv_medrxiv\/biorxiv_medrxiv\/pdf_json\/59eab95c43fdea01481fdbf9bae45dfe28ffc693.json\")]\n    papers = [p for p in Path('\/kaggle\/input\/CORD-19-research-challenge').glob('biorxiv_medrxiv\/biorxiv_medrxiv\/pdf_json\/*.json')]\n    #papers += [p for p in Path('\/kaggle\/input\/CORD-19-research-challenge').glob('comm_use_subset\/comm_use_subset\/pdf_json\/*.json')]\n    #papers += [p for p in Path('\/kaggle\/input\/CORD-19-research-challenge').glob('noncomm_use_subset\/noncomm_use_subset\/pdf_json\/*.json')]\n    #papers += [p for p in Path('\/kaggle\/input\/CORD-19-research-challenge').glob('custom_license\/custom_license\/pdf_json\/*.json')]\n    print (len(papers)) \n\n    # Load the NLP models\n    nlp_model_bionlp13cg = spacy.load('\/opt\/conda\/lib\/python3.6\/site-packages\/en_ner_bionlp13cg_md\/en_ner_bionlp13cg_md-0.2.4') # For cells, genes, ...\n    nlp_model_bc5cdr = spacy.load('\/opt\/conda\/lib\/python3.6\/site-packages\/en_ner_bc5cdr_md\/en_ner_bc5cdr_md-0.2.4') # For diseases\n\n    # The output will be one hashmap associating each paper to its annotations\n    output = {}\n\n    # Process all the papers\n    for paper in tqdm(papers):\n        try:\n            # Load the document\n            document = json.loads(paper.read_text())\n\n            # Get the ID\n            paper_id = document['paper_id']\n            \n            # Initialise its entry\n            output[paper_id] = {}\n            output[paper_id]['topics'] = {} # The different topic annotations grouped per type\n            \n            # Group the text by sections (took more than 9h to process!)\n            #section_texts = {}\n            #section_texts['abstract'] = []\n            #for b in document['abstract']:\n            #    section_texts['abstract'].append(b['text'])\n            #for b in document['body_text']:\n            #    section_texts.setdefault(b['section'], [])\n            #    section_texts[b['section']].append(b['text'])\n\n            # Retrieve all the text\n            texts = []\n            for b in document['abstract']:\n                texts.append(b['text'])\n            if 'body_text' in document:\n                for b in document['body_text']:\n                    texts.append(b['text'])\n            \n            #\u00a0Process the different sections to extract entities\n            #for section,texts in section_texts.items():\n            text = '.'.join(texts)\n            for nlp_model in [nlp_model_bionlp13cg, nlp_model_bc5cdr]:\n                tokens = nlp_model(text)\n                for entity in tokens.ents:\n                    topic_type = entity.label_\n                    topic_value = str(entity.text)\n                    output[paper_id]['topics'].setdefault(topic_type, set())\n                    output[paper_id]['topics'][topic_type].add(topic_value)\n            \n        except Exception as e:\n            print ('Error with {}'.format(paper))\n            print (e)\n\n    # Turn the sets into lists to save them as JSON\n    for paper_id in output.keys():\n        for topic_type in output[paper_id]['topics'].keys():\n            output[paper_id]['topics'][topic_type] = list(output[paper_id]['topics'][topic_type])\n\n    return output","35c833da":"# Step 1 => get the keywords out of the paper abstract and content\nannotations = extract_paper_annotations()\nprint (len(annotations.keys()))","73d8614e":"def get_paper_annotations_graph(annotations):\n    \"\"\"\n    This function is used to generate a graph from the paper annotations\n    \n    We will turn all the NLP annotations into concept identifiers using a list of terms extracted form Open Targets and the ontology MONDO. \n    This is done using a basic exact string matching and all the non matching strings are ignored.\n\n    We extract a mapping \"disease name\" => \"disease identifier\" from Open Targets as the primary source, falling back on Mondo to fill the gaps. \n    In particular one of the missing value in Open Targets right now is Covid-19 ... ;-)\n    \"\"\"\n    \n    # Prepare a map to deal with all the different types of entities type recognized by Spacy and that may be found in the annotations\n    ontology_map = {\n        'DISEASE': {},\n        'CANCER': {},\n        'GENE_OR_GENE_PRODUCT': {}\n    }\n\n    # TODO: If we want to keep more of the annotations returned by Spacy we should align:\n    # From https:\/\/allenai.github.io\/scispacy\/ en_ner_bionlp13cg_md\n    #  CANCER, ORGAN, TISSUE, ORGANISM, CELL, AMINO_ACID, GENE_OR_GENE_PRODUCT, \n    #  SIMPLE_CHEMICAL, ANATOMICAL_SYSTEM, IMMATERIAL_ANATOMICAL_ENTITY, \n    #  MULTI-TISSUE_STRUCTURE, DEVELOPING_ANATOMICAL_STRUCTURE, \n    #  ORGANISM_SUBDIVISION, CELLULAR_COMPONENT\n    # From https:\/\/allenai.github.io\/scispacy\/ en_ner_bc5cdr_md\n    #  DISEASE, CHEMICAL\n\n    ########################\n    # Get mappings for DISEASE\n    ########################\n    \n    # Load the file from Open Targets and fill the hashmap in\n    disease_list = pd.read_csv('https:\/\/storage.googleapis.com\/open-targets-data-releases\/20.02\/output\/20.02_disease_list.csv.gz', compression='gzip')\n    disease_list['disease_full_name'] = disease_list['disease_full_name'].str.lower()\n    print('Number of keywords in open targets:', len(set(disease_list['disease_full_name'].values)))\n    for index, row in disease_list.iterrows():\n        full_name = row['disease_full_name'].lower()\n        identifier = row['efo_id']\n        ontology_map['DISEASE'][full_name] = identifier\n\n    # Open Targets does not have Covid-19 in its list of diseases. We had it manually\n    # To get the labels we ran the following query on http:\/\/www.ontobee.org\/sparql\n    #  select distinct ?s ?o where {\n    #    {<http:\/\/purl.obolibrary.org\/obo\/MONDO_0100096> <http:\/\/www.geneontology.org\/formats\/oboInOwl#hasExactSynonym> ?o} \n    #    union\n    #    {<http:\/\/purl.obolibrary.org\/obo\/MONDO_0100096> <http:\/\/www.w3.org\/2000\/01\/rdf-schema#label> ?o}\n    #  }\n    ontology_map['DISEASE']['2019 novel coronavirus infection'.lower()] = 'MONDO_0100096'\n    ontology_map['DISEASE']['2019-nCoV infection'.lower()] = 'MONDO_0100096'\n    ontology_map['DISEASE']['severe acute respiratory syndrome coronavirus 2'.lower()] = 'MONDO_0100096'\n    ontology_map['DISEASE']['SARS-CoV-2'.lower()] = 'MONDO_0100096'\n    ontology_map['DISEASE']['SARS-coronavirus 2'.lower()] = 'MONDO_0100096'\n    ontology_map['DISEASE']['coronavirus disease 2019'.lower()] = 'MONDO_0100096'\n    ontology_map['DISEASE']['COVID-19'.lower()] = 'MONDO_0100096'\n    \n    # Debug output\n    print ('Number of keywords in map for diseases: {}'.format(len(ontology_map['DISEASE'])))\n    \n    \n    ########################\n    # Get mappings for CANCER\n    ########################\n\n    # We will simply treat the \"CANCER\" annotations from Spacy as \"DISEASE\"\n    ontology_map['CANCER'] = ontology_map['DISEASE']\n    \n    \n    ########################\n    # Get mappings for GENE_OR_GENE_PRODUCT\n    ########################\n    \n    # Load a target list from Open Targets. It will be used to map gene keywords\n    target_list = pd.read_csv('https:\/\/storage.googleapis.com\/open-targets-data-releases\/20.02\/output\/20.02_target_list.csv.gz', compression='gzip')\n    target_list['hgnc_approved_symbol'] = target_list['hgnc_approved_symbol'].str.lower()\n    print('Number of genes in open targets:', target_list['hgnc_approved_symbol'].nunique())\n    for index, row in target_list.iterrows():\n        full_name = row['hgnc_approved_symbol']\n        identifier = row['ensembl_id']\n        ontology_map['GENE_OR_GENE_PRODUCT'][full_name] = identifier\n    \n\n    ########################\n    # Turn the paper annotations into a graph\n    ########################\n    graph = []\n    predicates = {\n        'DISEASE': 'isAboutDisease',\n        'CANCER': 'isAboutDisease',\n        'GENE_OR_GENE_PRODUCT': 'isAboutTarget'\n    }\n    # Go through all the papers\n    for (paper_id, data) in annotations.items():\n        # For each annotation topic try to find a match in the ontology map\n        for (topic, values) in data['topics'].items():\n            if topic in ontology_map:\n                for value in values:\n                    if value.lower() in ontology_map[topic]:\n                        obj = ontology_map[topic][value.lower()]\n                        graph.append([paper_id, predicates[topic], obj])\n               \n    return graph","456afbdc":"def connect_targets_and_diseases(graph):\n    \"\"\"\n    This function will use the association data from Open Target to \n    connect instances of Target and Disease in the graph.\n    \n    We at the same time connect diseases to therapeutic areas (instances of Disease)\n    as this information is returned by the API\n    \"\"\"\n    \n    # Get a list of all the targets (genes) and diseases currently in the graph\n    targets = list(set([t[2] for t in graph if t[1] == 'isAboutTarget']))\n    diseases = list(set([t[2] for t in graph if t[1] == 'isAboutDisease']))\n    \n    # Prepare a map of target => disease relations\n    ot_output_associations = {}\n    \n    # Query OpenTargets for Target => Disease associations\n    ot = OpenTargetsClient()\n    for target in tqdm(targets):\n        ot_output_associations.setdefault(target, set())\n        search_results = ot.get_associations_for_target(target)\n        if len(search_results) > 0 and search_results[0]['target']['id'] == target:\n            for search_result in search_results:\n                if search_result['association_score']['overall'] > 0.8:\n                    disease = search_result['disease']['id']\n                    ot_output_associations[target].add(disease)\n                        \n    # Query OpenTargets for Disease => Target associations\n    for disease in tqdm(diseases): \n        search_results = ot.get_associations_for_disease(disease)\n        if len(search_results) > 0 and search_results[0]['disease']['id'] == disease:\n            for search_result in search_results:\n                if search_result['association_score']['overall'] > 0.8:\n                    target = search_result['target']['id']\n                    ot_output_associations.setdefault(target, set())\n                    ot_output_associations[target].add(disease)\n\n    #\u00a0Turn the output into new edges in the graph\n    for target, diseases in ot_output_associations.items():\n        for disease in diseases:\n            # Target -> Disease relation\n            graph.append([target, 'isAssociatedTo', disease])            ","a3b98417":"def connect_diseases_to_diseases(graph):\n    \"\"\"\n    This function leverages the disease similarity information computed by Open Targets\n    to connect Diseases to each other. Those links will later be used to find risk factors.\n    \"\"\"\n    \n    # Get a list of all the diseases currently in the graph.\n    # We do that by looking at the objects of triples we know link to Diseases\n    diseases = set([t[2] for t in graph if t[1] == 'isAboutDisease']) \n    diseases = diseases | set([t[2] for t in graph if t[1] == 'isAssociatedTo']) \n    \n    # Query OpenTargets\n    ot_output_diseases = {}\n    ot = OpenTargetsClient()\n    for disease in tqdm(diseases):\n        ot_output_diseases[disease] = set()\n        search_results = ot.get_similar_disease(disease)\n        for search_result in search_results:\n            if search_result['subject']['id'] == disease: # Safe guard\n                ot_output_diseases[disease].add(search_result['object']['id'])\n                \n    # Turn the output we received into edges\n    for src_disease, target_diseases in ot_output_diseases.items():\n        for target_disease in target_diseases:\n            graph.append([src_disease, 'hasGeneticClue', target_disease])","5625fb94":"def add_disease_classification(graph):\n    \"\"\"\n    This function adds to the graph the disease classification tree.\n    See, for example, https:\/\/www.targetvalidation.org\/disease\/EFO_0005774 .\n    \"\"\"\n\n    # Get a list of all the diseases in the graph\n    diseases = set([t[2] for t in graph if t[1] == 'isAboutDisease']) \n    diseases = diseases | set([t[2] for t in graph if t[1] == 'isAssociatedTo']) \n    diseases = diseases | set([t[2] for t in graph if t[1] == 'hasGeneticClue']) \n\n    # Query OpenTargets\n    paths = set()\n    ot = OpenTargetsClient()\n    for disease in tqdm(diseases):\n        search_results = ot.search(disease)\n        if search_results != None and len(search_results) > 0:\n            search_result = search_results[0]\n            if search_result['id'] == disease:\n                if 'efo_path_codes' in search_result['data']:\n                    for path in search_result['data']['efo_path_codes']:\n                        paths.add('=>'.join(path))\n                        \n    # Turn the output we received into edges\n    for path_str in paths:\n        path = path_str.split('=>')\n        for index in range(0, len(path)-1):\n            start = path[index]\n            end = path[index+1]\n            graph.append([end, 'isASpecific', start])","98438605":"def add_disease_therapeutic_areas(graph):\n    \"\"\"\n    This function query Open Targets for the therapeutic area of all the diseases\n    \"\"\"\n    \n    # Get a list of all the diseases in the graph\n    diseases = set([t[2] for t in graph if t[1] == 'isAboutDisease']) \n    diseases = diseases | set([t[2] for t in graph if t[1] == 'isAssociatedTo'])\n    diseases = diseases | set([t[2] for t in graph if t[1] == 'hasGeneticClue']) \n    diseases = diseases | set([t[2] for t in graph if t[1] == 'isASpecific']) \n    \n    # Query OpenTargets\n    ot_output = {}\n    ot = OpenTargetsClient()\n    for disease in tqdm(diseases):\n        ot_output[disease] = set()\n        search_results = ot.get_disease(disease)\n        if search_results != None and len(search_results) > 0:\n            search_result = search_results[0]\n            if search_result['code'].endswith(disease) and 'therapeutic_codes' in search_result:\n                for therapeutic_code in search_result['therapeutic_codes']:\n                        ot_output[disease].add(therapeutic_code)\n                        \n    # Turn the output we received into edges\n    for (disease, areas) in ot_output.items():\n        for area in areas:\n            graph.append([disease, 'belongsToTherapeuticArea', area])","6118a0f4":"def print_graph_stats(graph):\n    resources = set([r[0] for r in graph]) | set([r[2] for r in graph])\n    predicates = set([r[1] for r in graph])\n    print ('Graph has {} edges, {} resources, {} predicates'.format(len(graph), len(resources), len(predicates)))\n    display(pd.DataFrame([t for t in graph], columns=['Subject', 'Predicate', 'Object']))","9cff2f9c":"# Step 2 => get the starting graph of paper annotations\ngraph = get_paper_annotations_graph(annotations)\nprint_graph_stats(graph)","8031eb68":"# Step 3 => enrich the graph with Target - Disease links\nconnect_targets_and_diseases(graph)\nprint_graph_stats(graph)","6076210d":"# Step 4 => connect diseases to related diseases\nconnect_diseases_to_diseases(graph)\nprint_graph_stats(graph)","d386a87b":"# Step 5 => add disease classification trees\nadd_disease_classification(graph)\nprint_graph_stats(graph)","f10ac0da":"# Step 6 => add disease therapeutic areas\nadd_disease_therapeutic_areas(graph)\nprint_graph_stats(graph)","70c44300":"# Finally, we do a last pass to remove duplicate statements\nfinal_graph = [t.split('=>') for t in set(['=>'.join(t) for t in graph])]\nprint_graph_stats(final_graph)\n\n# and we save the graph to disk\ngraph_df = pd.DataFrame(final_graph, columns=['subject', 'predicate', 'object'])\ngraph_df.to_csv('graph.csv', index=False)","bbfdb959":"def get_neighbours(resource):\n    # Extract a disease and target code=>label\n    to_name = {}\n    target_list = pd.read_csv('https:\/\/storage.googleapis.com\/open-targets-data-releases\/20.02\/output\/20.02_target_list.csv.gz', compression='gzip')\n    for row in target_list.itertuples():\n        to_name[row.ensembl_id] = row.hgnc_approved_symbol\n    disease_list = pd.read_csv('https:\/\/storage.googleapis.com\/open-targets-data-releases\/20.02\/output\/20.02_disease_list.csv.gz', compression='gzip')\n    for row in disease_list.itertuples():\n        to_name[row.efo_id] = row.disease_full_name\n    \n    # Extract edges we may be interested in\n    triples = [t for t in final_graph if t[0] == resource or t[2] == resource]\n\n    # Construct a dataframe\n    tmp = []\n    for t in triples:\n        s = '{} ({})'.format(t[0], to_name.get(t[0], '?'))\n        o = '{} ({})'.format(t[2], to_name.get(t[2], '?'))\n        tmp.append([s,t[1],o])\n        \n    return pd.DataFrame(tmp, columns=['Subject', 'Predicate', 'Object'])","e5f3c3ff":"display(get_neighbours('MONDO_0008903'))","4fdb9e66":"display(get_neighbours('MONDO_0100096'))","d95fa49d":"! conda install tensorflow-gpu'>=1.14.0,<2.0.0' -y\n! pip install ampligraph","1023684a":"import os\nos.environ['CUDA_VISIBLE_DEVICES']='0'\n\nimport pandas as pd\nimport numpy as np\nnp.random.seed(117)\nfrom ampligraph.latent_features import ComplEx, TransE, DistMult, RandomBaseline\n\nfrom ampligraph.evaluation import evaluate_performance, mrr_score, hits_at_n_score, mr_score\nfrom ampligraph.utils import save_model, restore_model\n\n\nDATASET_BASE_PATH = \"\/kaggle\/\"","564193b0":"triples = pd.read_csv(\"graph.csv\")\n\npaper_diseases = set(triples[triples.predicate == 'isAboutDisease'].object)\npaper_targets = set(triples[triples.predicate == 'isAboutTarget'].object)\nnew_triples = []\nfor row in triples.itertuples():\n    if row.predicate == 'isAssociatedTo':\n        if row.subject in paper_targets or row.object in paper_diseases:\n            new_triples.append([row.subject, row.predicate, row.object])\n    if row.predicate == 'isAboutDisease' or row.predicate == 'isAboutTarget':\n        new_triples.append([row.subject, row.predicate, row.object])\n    if row.predicate == 'hasGeneticClue':\n        if row.subject in paper_diseases or row.object in paper_diseases:\n            new_triples.append([row.subject, row.predicate, row.object])\n    if row.predicate == 'isASpecific':\n        new_triples.append([row.subject, row.predicate, row.object])\n    if row.predicate == 'belongsToTherapeuticArea':\n        new_triples.append([row.subject, row.predicate, row.object])\nprint (len(new_triples))\n\n\ngraph_df = pd.DataFrame(new_triples, columns=['subject', 'predicate', 'object'])\n\n# this line is added for making sure that the results are reproducible.\ngraph_df.sort_values(by=['subject', 'predicate', 'object'], inplace=True)\n\ngraph_df.to_csv('COVID_KG_sample.csv', index=False)\n\ngraph_df.head()\n\nprint('Size of the graph:', graph_df.shape)\n\nprint(graph_df.columns)\n\nprint(graph_df.predicate.value_counts())","476f55ee":"genetic_clue_triples = graph_df[graph_df['predicate']=='hasGeneticClue']\ntrain_set = graph_df[graph_df['predicate']!='hasGeneticClue'].values","c47b96bf":"\ndisease_list =  np.unique(np.concatenate([\n                    np.unique(train_set[train_set[:, 1]=='isAboutDisease'][:, 2]),\n                    np.unique(train_set[train_set[:, 1]=='isAssociatedTo'][:, 2]),\n                ], 0))\n\nprint('diseases in df:', len(disease_list))","82ccea5b":"import random\n\nnp.random.seed(117)\n\ntest_set_diseases = np.random.choice(list(disease_list), 100).tolist()\n\n#test_set_diseases = set(np.random.choice(list(disease_list), 2).tolist())\nprint(test_set_diseases)","3ce4001d":"\ntest_set = genetic_clue_triples[genetic_clue_triples[\"subject\"].isin(test_set_diseases)]\ntrain_genetic_clue_triples = genetic_clue_triples[~genetic_clue_triples[\"subject\"].isin(test_set_diseases)]\ntrain_set = np.concatenate([train_set, train_genetic_clue_triples], 0)\ntrain_set = np.random.permutation(train_set)\n\nprint('Train set size:', train_set.shape)\nprint('Test set size:', test_set.shape)\nprint('Full Graph size:', graph_df.shape)\n","57921295":"disease_list_full =  np.unique(np.concatenate([\n                        np.unique(train_set[train_set[:, 1]=='isAboutDisease'][:, 2]),\n                        np.unique(train_set[train_set[:, 1]=='isAssociatedTo'][:, 2]),\n                        np.unique(train_set[train_set[:, 1]=='hasGeneticClue'][:, 0]),\n                        np.unique(train_set[train_set[:, 1]=='hasGeneticClue'][:, 2]),\n                    ], 0))\n\nprint('diseases in df:', len(disease_list_full))","cdc2bddb":"filter_triples = genetic_clue_triples.values\n\nrandom_model = RandomBaseline(seed=0)\n\nrandom_model.fit(train_set)\n\nranks = evaluate_performance(test_set.values, \n                             random_model, \n                             filter_triples=filter_triples, \n                             corrupt_side='o', \n                             entities_subset=list(disease_list_full))\n\nprint('MRR with random baseline:', mrr_score(ranks))","9e43faf4":"filter_triples = genetic_clue_triples.values\n\n\nmodel = ComplEx(batches_count=15, seed=0, epochs=1000, k=200, eta=20,\n                optimizer='adam', optimizer_params={'lr':1e-4}, \n                verbose=True, loss='multiclass_nll',\n                regularizer='LP', regularizer_params={'p':3, 'lambda':1e-3})\n\n\n\nearly_stopping = { 'x_valid': test_set.values,\n                   'criteria': 'mrr', \n                  'x_filter': filter_triples, \n                  'stop_interval': 3, \n                  'burn_in': 50, \n                  'corrupt_side':'o',\n                  'corruption_entities': list(disease_list_full),\n                  'check_interval': 50 }\n\nmodel.fit(train_set, True,early_stopping)\n\nranks = evaluate_performance(test_set.values, \n                             model, \n                             filter_triples=filter_triples, \n                             corrupt_side='o', \n                             entities_subset=list(disease_list_full))\n\nprint('MRR with trained ComplEx embedding model:', mrr_score(ranks))\n\nmodel.calibrate(train_set, positive_base_rate=0.5, epochs=100)\nsave_model(model, 'output_graph.pth')","506a9d61":"disease_id = 'MONDO_0100096' #covid-19\n\ntest_predicate = 'hasGeneticClue'\n\nhypothesis = np.concatenate([np.array([[disease_id] * disease_list_full.shape[0]]), \n                             np.array([[test_predicate] * disease_list_full.shape[0]]),\n                             disease_list_full[np.newaxis, :]],0).T\nprint(hypothesis.shape)\n\nscores = model.predict_proba(hypothesis)","66b253f9":"disease_mapping_list_df = disease_list = pd.read_csv('https:\/\/storage.googleapis.com\/open-targets-data-releases\/20.02\/output\/20.02_disease_list.csv.gz', \n                                                     compression='gzip')\n\n\ndisease_mapping_list_df.head()\n","b56ac22e":"tested_hypothesis = pd.DataFrame(np.concatenate([hypothesis, \n                                                 scores[:, np.newaxis]], 1), \n                                 columns=['s','p','o','score'])\n\ntested_hypothesis = tested_hypothesis[tested_hypothesis['o'] != disease_id]\n\ntested_hypothesis = tested_hypothesis.sort_values(by='score', \n                                                  ascending=False)\n\ntested_hypothesis = tested_hypothesis.merge(disease_mapping_list_df, \n                                            how='left', \n                                            left_on='o', \n                                            right_on='efo_id')[['disease_full_name', 'score']]\n\ntested_hypothesis.columns = ['Risk Factors', 'Score']\n\npd.set_option('display.max_rows', 101)\n\ntested_hypothesis.head(100)","df1e8d7d":"tested_hypothesis.to_csv('predicted_covid19_risk_factors.csv')","a8aa865c":"We now generate the actual graph by calling the functions defined above:","4e96dc4b":"We now save the **entire list of predictions** to disk:","9caf84e1":"# Part 3\/3: Knowledge Graph Embedding Pipeline\n\nIn this section we predict risk factors for COVID-19 using graph representation learning.\nEach prediction is associated a confidence score. The highest the score, the more likely such prediction is true.\n\nSuch predictions are carried out with knowledge graph embeddings, using the [AmpliGraph 1.3.1 library](https:\/\/github.com\/Accenture\/AmpliGraph). The knowledge grpah used is the result of step 2\/3.\n\n**This is the last step of our contribution.**\n\n![covid19_pipeline_3.jpg](https:\/\/kagglecovid.s3-eu-west-1.amazonaws.com\/covid19_pipeline_3.jpg)\n\n\n### Knowledge Graph Embedding Training\n\nWe train a knowledge graph embedding model using the ComplEx model [Trouillon et al. 2016] over the knowledge grpah generated at step 2\/3. ComplEx reaches state-of-the-art predictive power on agreed upon [benchmark datasets](https:\/\/docs.ampligraph.org\/en\/latest\/experiments.html), and therefore is an obvious choice for this task.\n\nFor a comprehensive description of knowledge grpah embedding models, we suggest to read [this survey paper](https:\/\/arxiv.org\/abs\/2002.00388). The AmpliGraph library documention includes a [primer on the topic](https:\/\/docs.ampligraph.org\/en\/1.3.1\/background.html).\n\nWe train for 1,000 epochs, with an embedding size `k`=200, Adam optimizer (learning rate = 1e-4), [multiclass NLL loss](https:\/\/docs.ampligraph.org\/en\/1.3.1\/generated\/ampligraph.latent_features.NLLMulticlass.html#ampligraph.latent_features.NLLMulticlass), L3 regularizer, and early stopping criteria.\n\nBelow we report a figure generated with [TensorFlow Embedding Projector](https:\/\/projector.tensorflow.org\/). Each point in the cartesian space represents a bi-dimensional, t-SNE reduced representation of a 200-dimension embedding. Note that nodes and predicate types are embedded in the same space. The picture highlights the closest neighbours to COVID-19 in the uncompressed 200-dimensional space:\n\n\n![covid19_embedding_space.jpg](https:\/\/kagglecovid.s3-eu-west-1.amazonaws.com\/covid19_embedding_space.jpg)\n\n\n### Sanity Check\nTo sanity-check the trained embeddings, we carve out known risk factors-disease associations for better-studied conditions (thus not including COVID-19), and we assess the predictive power of the model using agreed-upon [evaluation protocol](https:\/\/docs.ampligraph.org\/en\/1.3.1\/generated\/ampligraph.evaluation.evaluate_performance.html#ampligraph.evaluation.evaluate_performance) and [metrics](https:\/\/docs.ampligraph.org\/en\/1.3.1\/generated\/ampligraph.evaluation.mrr_score.html#ampligraph.evaluation.mrr_score). \n\nSuch validation set includes 100 randomly selected conditions (i.e. Diseases), each connected with a number of risk factors with the `hasGeneticClue` predicate. This led to a validation set that includes 2,452 triples.\n\nFor each of those triples, we generate 7,218 corruptions, obtained with the agreed upon [protocol]((https:\/\/docs.ampligraph.org\/en\/1.3.1\/generated\/ampligraph.evaluation.evaluate_performance.html#ampligraph.evaluation.evaluate_performance)) of replacing the object of a true triple with another concept. We use 7,218 concepts (unique diseases and risk factors) to generate 7,218 corruptions for each triple in the validation set. We then score a true triple and all its corrutpions, and we report how that true triple is ranked against its 7,218 corrutpions. We repeat the same procedure for each triple in the test set, and we compute the [mean reciprocal rank (MRR, filtered settings)]((https:\/\/docs.ampligraph.org\/en\/1.3.1\/generated\/ampligraph.evaluation.mrr_score.html#ampligraph.evaluation.mrr_score)).\n\nOur model reaches a validation mean reciprocal rank **MRR=0.07**.\n\nThe table below reports our validation results. We also include [Hits@10, Hits@100](https:\/\/docs.ampligraph.org\/en\/1.3.1\/generated\/ampligraph.evaluation.hits_at_n_score.html#ampligraph.evaluation.hits_at_n_score) values, and we compare against a [baseline that returns random predictions](https:\/\/docs.ampligraph.org\/en\/1.3.1\/generated\/ampligraph.latent_features.RandomBaseline.html#randombaseline) for positive and negative triples. \nThe table shows we largely outperform the random baseline across all the metrics.\n\n*Note MRR, Hits@10 and Hits@100 are values between [0,1], and the higher the better.*\n\n|                  | MRR   | Hits@10 | Hits@100 |\n|------------------|-------|:-------:|----------|\n| Random Baseline  | 0.001 | 0.001   | 0.012    |\n| Our Contribution | **0.07** | **0.137**   | **0.390**    |\n\n\nAs we pointed out at the top of the notebook, we had not carry out extensive hyperparametr selection yet. Such operation will likely increase the predictive power of the model, hence the quality of the embeddings used to infer risk factros for COVID-19. \n\n### Inference: COVID-19 Risk Factor Predictions\nOnce sanity-checked the model, we used the trained embeddings to predict the probability of existence of a number of hypothetical triples of the form `<COVID-19 hasGeneticClue ?>`, where `?` is a risk factor (that can be a characteristic, condition, or behaviour).\n\nThese scored hypothesis are sorted from the highest scored to the lowest. A high score indicates high likelihood that the predicted risk factor affects COVID-19.\n\nIn the cells below we describe the Knowledge Graph Embedding pipeline more in detail, and we provide the necessary source code to reproduce the experiment.","192a1192":"Get all the diseases that are in the current (partial) training set - so that we don't get unseen entities - i.e. at least one triple for the test set disease should exist in training set for it to get trained.","68291d72":"From this disease list, randomly choose 100 diseases on which we will validate our model.","05b00cef":"\nNow, let us train a model to perform link prediction and [calibrate it](https:\/\/arxiv.org\/abs\/1912.10000) to return trustworthy probability estimates:","6923d613":"Now get a list of all the diseases (including risk factors) in our training set","e991aa2d":"This function will use the association data from Open Target to \n    connect instances of Target and Disease in the graph.\n","48a6885b":"## Final results\nMerge the disease codes of the hypothesis with the above list to get the disease names and display the ranked list.\n\n**Such List contains the top-100 final results** (we clipped the list to reduce clutter in the notebook. **The complete list of predicted risk factors is saved to `predicted_covid19_risk_factors.csv`.**\n\n**Note our results are preliminary, and we aim at refining such list in phase-2, as pointed out under the future work section at the top of this notebook.**\n\n","1589243c":"Let's see what the graph looks like for lung cancer (`MONDO_0008903`) and Covid-19 (`MONDO_0100096`) so far:","cdc36a84":"\n### Results\n\n**Scroll down at the end of the notebook to see the list of COVID-19 risk factors predicted by our system.**\n\n\n","e5394226":"## Construct the training set and validation set\n\nOur validation test consists of only the triples which have predicate `hasGeneticClue`. Hence, we can move all other triples to the training set.","8ec95b22":"## Install the dependencies","a2996623":"### A look at some of the graph content","bef6a3d2":"# Part 1\/3: NER\n\nThe first component of our system is the named-entity recognition & relation extraction.\nIt reads the CORD-19 corpus and extracts entities and relations that will be used later on to generate the knowledge graph.\n\n![](https:\/\/kagglecovid.s3-eu-west-1.amazonaws.com\/covid19_pipeline_1.jpg)\n\n**The NER pipeline performs the following operations**:\n1. Extract concepts from the papers using Spacy and the models from [SciSpacy](https:\/\/allenai.github.io\/scispacy\/).\n2. Turn the found keywords into identifiers using a mapping extracted from [Open Targets]((https:\/\/genetics-docs.opentargets.org\/)) and the [Mondo Disease Ontology](https:\/\/www.ebi.ac.uk\/ols\/ontologies\/mondo).\n\nThe NER pipeline is reported in the cells below:","186e14ed":"This function adds to the graph the disease classification tree:","05a9c56c":"Get the file containing the disease code to name mappings for opentargets dataset:","4da7b83b":"This function looks at all the papers in the CORD-19 dataset and extract entities. \n\nNote to speed up processing time, we only process articles from biorxiv:","e0444f8b":"## Load the Knowledge graph","ea5991d1":"# Covid-19 Risk Factor Predictor\n\n\n## Our Contribution\n**We predict important risk factors for COVID-19 and we rank them by importance**. Risk factors can be characteristics, conditions, or behaviours. \n\nWe build a **knowledge graph** by merging  entities and relations that we extract from the **CORD-19 corpus** with the **[Open Targets Genetics](https:\/\/genetics-docs.opentargets.org\/)** dataset and the [Mondo Disease Ontology](https:\/\/www.ebi.ac.uk\/ols\/ontologies\/mondo). This is done to build a graph that merges COVID-19 facts extracted from bleeding-edge literature and known facts on similar diseases stored in datasets online. We achieve such goal despite the absence of COVID-19 from OpenTargets.\n\nLoosely inspired by [Mendelian Randomization](https:\/\/en.wikipedia.org\/wiki\/Mendelian_randomization), we train a **[graph representation learning model](https:\/\/github.com\/Accenture\/AmpliGraph)** that learns vector representations of the concepts in the graph (i.e. **graph embeddings**). Such operation leverages the topology and the semantics of the graph to discover new associations between risk factors and COVID-19. To sanity-check the trained embeddings, we carve out known risk factors-disease associations for better-studied conditions such as SARS, MERS, and we assess the predictive power of the model using agreed-upon evaluation protocol and metrics. Our model reaches a validation mean reciprocal rank MRR=0.07.\n\nFinally, we use the knowledge graph embeddings to infer how likely COVID-19 is impacted by a risk factor and we return a **ranked list of predicted risk factors** for COVID-19.\nOur results can be used either to test if a suspected risk factor applies to COVID-19, or to discover unsuspected risk factors.\n\n**The complete pipeline of the system is presented below**:\n\n![pipeline](https:\/\/kagglecovid.s3-eu-west-1.amazonaws.com\/covid19_pipeline.jpg)\n\n\n### Benefits\n* First machine learning system that returns a ranked list of predited risk factors for COVID-19.\n* The knowledge graph can easily be extended and updated with additional information (e.g. paper citation network, additional genomic datasets, etc.).\n* Graph representation learning exploits relations between concepts shared across articles, and it is scalable to support larger graphs (up to 500+M edges).\n\n### Known Limitations\n* In this Phase 1 prototype, we limited to CORD-19 **biorxiv articles** only.\n* Our approach combining paper annotation with Open Targets assumes genetics and genomics play a role in the impact on COVID-19.\n* The NER step likely missed some entities in the text and mis-identified others. Our second step mapping these entities identified as keyword into entities identified with unique identifier introduces a second level of possible mapping errors;\n* The risk factor surfaced are correlations emerging from the structure of the graph, not causations. Our graph now an association network associating papers to the things they are about, and associating these things to each other;\n* Due to time constraints, we did not carry out exhaustive model selection (hyperparameter tuning), to the detriment of predictive power.\n* The embeddings have to be re-trained every time the network is changed (e.g. when new papers are added). This is a known limitation of knowledge graph embedding models.\n\n### Future Work Planned for Phase 2\n* Re-build the graph on all CORD-19 papers\n* Validation of the final output by domain experts.\n* For each COVID-19 - Risk Factor prediction, we will also provide an explanation, in the form of a meaningful and succint subgraph that mostly affected such prediction.\n* Extensive and thorough hyperparameter tuning to improve predictive power.\n* The machine learning model will be adapted to support the addition of new concepts (e.g. newly-published papers), without the need to re-train the model.\n* The work described in this notebook will be part of a [wider pipeline](https:\/\/www.kaggle.com\/cgueret\/covid-19-risk-factor-predictor-future-work) aimed at discovering additional COVID-19 insights from the CORD-19 corpus. \n","6ed5cece":"## Inference: COVID-19 Risk Factor Hypothesis Predictions \n\nWe generate our list of hypothesis for COVID-19.\n\nSuch hypothesis are triples in the form  `<COVID, hasGeneticClue, ?>`, where `?` is retrieved from the list of all the diseases (that include risk factors).\n\nWe score each hypothesis using our trained model as follows:\n","a65b2018":"## Import the necessary libs","fc01cf73":"This function leverages the disease similarity information computed by Open Targets to connect Diseases to each other. \nThose links will later be used to find risk factors:","ad98d525":"This function is used to generate a graph from the paper annotations:","f2dc638e":"Get the triples with predicate `hasGeneticClue` for these 100 diseases and form the test set. Move the other triples of that predicate to training set.","e59fed95":"This function query Open Targets for the therapeutic area of all the diseases:","40b12432":"## Training \n\nLet's first train a [RandomBaseline model](https:\/\/docs.ampligraph.org\/en\/1.3.1\/generated\/ampligraph.latent_features.RandomBaseline.html#ampligraph.latent_features.RandomBaseline) and let's evaluate it on the test set. This model assigns random scores to the triples and their corruptions. So we expect to see a very bad performance (mrr) on the test set. ","8a3b1faf":"# Part 2\/3: Knowledge Graph Generation\n\nIn this section we generate the knowledge graph (KG) that part 3\/3 will use to predict COVID-19 risk factors associations.\n\n![](https:\/\/kagglecovid.s3-eu-west-1.amazonaws.com\/covid19_pipeline_2.jpg)\n\n### Knowledge Graph Schema\nThe knowledge graph models relations between CORD-19 papers (extracted by part 1\/3) and Genes, Diseases retrieved from [Open Targets Genetics](https:\/\/genetics-docs.opentargets.org\/).\nThis is the schema of the knowledge graph that we generate to predict covid19-specific risk factors:\n\n![](https:\/\/kagglecovid.s3-eu-west-1.amazonaws.com\/covid_kg_ontology_v6_resized.png)\n\n+ **Paper**: an article from the CORDI-19 corpus.\n+ **Target**: a gene that occurs in at least one CORD-19 paper, and that may be associated to a Disease.\n+ **Disease**: following OpenTargets terminology, this class represents characteristics, conditions, and behaviours. The risk factors predicted by our system belowngs here.\n\n+ **isAboutGene**: connects a CORDI-19 paper to a target gene.\n+ **isAboutDisease**: connects a CORDI-19 paper to a Disease (i.e. a characteristic, condition, or behaviour - following OpenTargets terminology).\n+ **isASsociatedTo**: connects a target gene to a Disease (i.e. a characteristic, condition, or behaviour - following OpenTargets terminology).\n+ **belongsToTherapeuticArea**: associates a Disease to its therapeutic area, as provided by Open Targets.\n+ **isASpecific**: this predicate associates a Disease to an higher-up element in the Open Targets hierarchy (ontological path connecting higher classes of diseases to more specific instances).\n+ **hasGeneticClue**: this predicate connects a Disease to a characteristic, condition, or behaviour if there are genetic evidences that such connection exists. **As indicated in the picture and the example below, we predict facts that include this predicate and COVID-19 as subject**.\n\nThe figure below shows some facts included in the knowledge graph. The figure includes `COVID-19` and the relations that must be predicted in part 3\/3.\n\n![covid_Abox.jpg](https:\/\/kagglecovid.s3-eu-west-1.amazonaws.com\/covid_Abox.jpg)\n\nThe graph has 327,834 facts (i.e. edges) and 21,025 distinct concepts (i.e. nodes).\nThe screenshot below had been generated with [Gephi](https:\/\/gephi.org\/), and gives a sense of the complexity and size of the data structure:\n\n![graph_gephi.jpg](https:\/\/kagglecovid.s3-eu-west-1.amazonaws.com\/graph_gephi.jpg)\n\n### Knowledge Graph Construction\nThe cells below include the code required to generate the knowledge graph.\nThe most important steps are:\n1. We use the entities extracted by Step 1\/3.\n3. We query Open Targets to enrich the paper annotations with additional information, to connect the topics of the papers to each other (see above for the description of the schema):\n  + Link between diseases and genes\n  + Link between diseases based on their gene similarity\n  + Ontological path connecting higher classes of diseases to more specific instances","f2bdcfb1":"The following snippet generates the [TensorBoard Embedding Projector](https:\/\/projector.tensorflow.org\/) files to visualize the embedding space (see figure at the top of the notebook). \n\nImport the generated files into [TensorBoard Embedding Projector](https:\/\/projector.tensorflow.org\/) and follow the on-screen instructions.\n\n[AmpliGraph documentation](https:\/\/docs.ampligraph.org\/en\/1.3.1\/generated\/ampligraph.utils.create_tensorboard_visualizations.html#ampligraph.utils.create_tensorboard_visualizations) explains how to go about in detail.\n\nNote this step is optional, and is required only to visually inspect the embeddings.\n\n\n```python\nfrom ampligraph.utils import create_tensorboard_visualizations\ncreate_tensorboard_visualizations(model, 'covid19_tensorboard_files')\n```"}}