{"cell_type":{"9ddb8a9a":"code","be421939":"code","01469bea":"code","35a589d2":"code","0bf2c12a":"code","2af94923":"code","40b307ed":"code","444f627f":"code","66161807":"code","af345055":"code","6f5b665b":"code","fad4ea14":"code","7c518408":"code","aae9d957":"code","6caa5f0a":"code","2bb10c9d":"code","d86d71d4":"code","cad8b3ae":"code","f80750ad":"code","0d2e078a":"code","c0b95958":"code","06dfbc3d":"code","085323a9":"code","4beaa59d":"code","f0e4c26c":"code","69c566b8":"code","ebcd0bd5":"code","81920b09":"code","1083582f":"code","816edc9f":"code","601f5d36":"code","145e6f8b":"code","1a342b08":"code","d9b1b0c2":"code","8626c199":"code","d2f5519c":"code","e69f2cd6":"code","2026e2dd":"code","ebacf5af":"code","aed7fb60":"code","0c2c1d8e":"code","1d128084":"code","061b6c21":"code","a92e41d9":"code","8a1e439a":"code","d99427b3":"code","dae32d40":"code","b690794e":"code","bd587730":"code","fc8f61a6":"code","b523d297":"code","2c34c3a4":"code","ae8006bb":"code","97424b9b":"markdown","73e61b2d":"markdown","2696bca5":"markdown","8ea36cfc":"markdown","54c85d30":"markdown"},"source":{"9ddb8a9a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime","be421939":"from scipy import stats\nfrom scipy.stats import skew, boxcox_normmax, norm\nfrom scipy.special import boxcox1p","01469bea":"import matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\nimport warnings\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')","35a589d2":"pd.set_option('display.max_columns', 250)\npd.options.display.max_rows=250","0bf2c12a":"train = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv')\ntest = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv')","2af94923":"print('train shape is {}'.format(train.shape))\nprint('test shape is {}'.format(test.shape))","40b307ed":"train.head()","444f627f":"test.head()","66161807":"train.describe().T","af345055":"# dropping unnecessary column Id\ntrain.drop('Id', axis=1, inplace=True)\ntest.drop('Id', axis=1, inplace=True)","6f5b665b":"y = train['SalePrice'].reset_index(drop=True)\ntrain_features = train.drop(['SalePrice'], axis=1)\ntest_features = test","fad4ea14":"sns.set(font_scale=1.1)\ncorr_train = train.corr()\nmask = np.triu(corr_train.corr())\nplt.figure(figsize=(20, 20))\nsns.heatmap(corr_train, annot=True, fmt='.1f', cmap='coolwarm', square=True, mask=mask, linewidth=1, cbar=True)\nplt.show()","7c518408":"features = pd.concat([train_features, test_features]).reset_index(drop=True)\nprint(features.shape)","aae9d957":"def missing_percentage(df):\n    total = df.isnull().sum().sort_values(ascending=False)[df.isnull().sum().sort_values(ascending=False)!=0]\n    percent = (df.isnull().sum().sort_values(\n        ascending=False)\/len(df)*100)[df.isnull().sum().sort_values(ascending=False)\/len(df)*100!=0]\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    ","6caa5f0a":"missing = missing_percentage(features)\nfig, ax = plt.subplots(figsize=(20, 5))\nsns.barplot(x=missing.index, y='Percent', data=missing, palette='Reds_r')\nplt.xticks(rotation=90)\ndisplay(missing.T.style.background_gradient(cmap='Reds', axis=1))","2bb10c9d":"\nnone_cols = [\n    'Alley', 'PoolQC', 'MiscFeature', 'Fence', 'FireplaceQu', 'GarageType',\n    'GarageFinish', 'GarageQual', 'GarageCond', 'BsmtQual', 'BsmtCond',\n    'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'MasVnrType'\n]\n\n# List of 'NaN' including columns where NaN's mean 0.\n\nzero_cols = [\n    'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath',\n    'BsmtHalfBath', 'GarageYrBlt', 'GarageArea', 'GarageCars', 'MasVnrArea'\n]\n\n# List of 'NaN' including columns where NaN's actually missing gonna replaced with mode.\n\nfreq_cols = [\n    'Electrical', 'Exterior1st', 'Exterior2nd', 'Functional', 'KitchenQual',\n    'SaleType', 'Utilities'\n]\n\nfor col in none_cols:\n    features[col].replace(np.nan, 'None', inplace=True)\n\nfor col in zero_cols:\n    features[col].replace(np.nan, 0, inplace=True)\n\nfor col in freq_cols:\n    features[col].replace(np.nan, features[col].mode()[0], inplace=True)","d86d71d4":"features['LotFrontage'] = features.groupby(['Neighborhood'])['LotFrontage'].apply(lambda x: x.fillna(x.median()))","cad8b3ae":"features['MSSubClass'] = features['MSSubClass'].astype(str)\nfeatures['YrSold'] = features['YrSold'].astype(str)\nfeatures['MoSold'] = features['MoSold'].astype(str)","f80750ad":"others = ['Condition1', 'Condition2', 'RoofMatl',\n          'Exterior1st', 'Exterior2nd', 'Heating',\n          'Electrical', 'Functional', 'SaleType']\n\nfor col in others:\n    mask = features[col].isin(features[col].value_counts()[features[col].value_counts()<10].index)\n    features[col][mask] = 'Other'","0d2e078a":"def srt_box(y, df):\n    fig, axes = plt.subplots(14, 3, figsize=(25, 80))\n    axes = axes.flatten()\n    \n    for i, j in zip(df.select_dtypes(include=['object']).columns, axes):\n        sortd = df.groupby([i])[y].median().sort_values(ascending=False)\n        sns.boxplot(x=i, y=y, data=df, palette='plasma', order=sortd.index, ax=j)\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=18))\n        plt.tight_layout()\n        ","c0b95958":"srt_box('SalePrice', train)","06dfbc3d":"def srt_reg(y, df):\n    fig, axes = plt.subplots(12, 3, figsize=(25, 80))\n    axes = axes.flatten()\n    \n    for i, j in zip(df.select_dtypes(include=['number']).columns, axes):\n        sns.regplot(x=i, y=y, data=df, ax=j, order=3, ci=None,  color='r',line_kws={'color':'black'})\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(n_bins=10))\n        plt.tight_layout()","085323a9":"srt_reg('SalePrice', train)","4beaa59d":"neigh_map = {\n    'MeadowV': 1,\n    'IDOTRR': 1,\n    'BrDale': 1,\n    'BrkSide': 2,\n    'OldTown': 2,\n    'Edwards': 2,\n    'Sawyer': 3,\n    'Blueste': 3,\n    'SWISU': 3,\n    'NPkVill': 3,\n    'NAmes': 3,\n    'Mitchel': 4,\n    'SawyerW': 5,\n    'NWAmes': 5,\n    'Gilbert': 5,\n    'Blmngtn': 5,\n    'CollgCr': 5,\n    'ClearCr': 6,\n    'Crawfor': 6,\n    'Veenker': 7,\n    'Somerst': 7,\n    'Timber': 8,\n    'StoneBr': 9,\n    'NridgHt': 10,\n    'NoRidge': 10\n}\n\nfeatures['Neighborhood'] = features['Neighborhood'].map(neigh_map).astype(\n    'int')\next_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nfeatures['ExterQual'] = features['ExterQual'].map(ext_map).astype('int')\nfeatures['ExterCond'] = features['ExterCond'].map(ext_map).astype('int')\nbsm_map = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nfeatures['BsmtQual'] = features['BsmtQual'].map(bsm_map).astype('int')\nfeatures['BsmtCond'] = features['BsmtCond'].map(bsm_map).astype('int')\nbsmf_map = {\n    'None': 0,\n    'Unf': 1,\n    'LwQ': 2,\n    'Rec': 3,\n    'BLQ': 4,\n    'ALQ': 5,\n    'GLQ': 6\n}\n\nfeatures['BsmtFinType1'] = features['BsmtFinType1'].map(bsmf_map).astype('int')\nfeatures['BsmtFinType2'] = features['BsmtFinType2'].map(bsmf_map).astype('int')\nheat_map = {'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}\nfeatures['HeatingQC'] = features['HeatingQC'].map(heat_map).astype('int')\nfeatures['KitchenQual'] = features['KitchenQual'].map(heat_map).astype('int')\nfeatures['FireplaceQu'] = features['FireplaceQu'].map(bsm_map).astype('int')\nfeatures['GarageCond'] = features['GarageCond'].map(bsm_map).astype('int')\nfeatures['GarageQual'] = features['GarageQual'].map(bsm_map).astype('int')\n","f0e4c26c":"features = features.join(y)\n\nfeatures = features.drop(features[(features['OverallQual'] < 5)\n                                  & (features['SalePrice'] > 200000)].index)\nfeatures = features.drop(features[(features['GrLivArea'] > 4000)\n                                  & (features['SalePrice'] < 200000)].index)\nfeatures = features.drop(features[(features['GarageArea'] > 1200)\n                                  & (features['SalePrice'] < 200000)].index)\nfeatures = features.drop(features[(features['TotalBsmtSF'] > 3000)\n                                  & (features['SalePrice'] > 320000)].index)\nfeatures = features.drop(features[(features['1stFlrSF'] < 3000)\n                                  & (features['SalePrice'] > 600000)].index)\nfeatures = features.drop(features[(features['1stFlrSF'] > 3000)\n                                  & (features['SalePrice'] < 200000)].index)\n\ny = features['SalePrice']\ny.dropna(inplace=True)\nfeatures.drop(columns='SalePrice', inplace=True)","69c566b8":"# Creating new features  based on previous observations. There might be some highly correlated features now. You cab drop them if you want to...\n\nfeatures['TotalSF'] = (features['BsmtFinSF1'] + features['BsmtFinSF2'] +\n                       features['1stFlrSF'] + features['2ndFlrSF'])\nfeatures['TotalBathrooms'] = (features['FullBath'] +\n                              (0.5 * features['HalfBath']) +\n                              features['BsmtFullBath'] +\n                              (0.5 * features['BsmtHalfBath']))\n\nfeatures['TotalPorchSF'] = (features['OpenPorchSF'] + features['3SsnPorch'] +\n                            features['EnclosedPorch'] +\n                            features['ScreenPorch'] + features['WoodDeckSF'])\n\nfeatures['YearBlRm'] = (features['YearBuilt'] + features['YearRemodAdd'])\n\n# Merging quality and conditions.\n\nfeatures['TotalExtQual'] = (features['ExterQual'] + features['ExterCond'])\nfeatures['TotalBsmQual'] = (features['BsmtQual'] + features['BsmtCond'] +\n                            features['BsmtFinType1'] +\n                            features['BsmtFinType2'])\nfeatures['TotalGrgQual'] = (features['GarageQual'] + features['GarageCond'])\nfeatures['TotalQual'] = (features['OverallQual'] + features['TotalExtQual'] + features['TotalBsmQual'] +\n                         features['TotalGrgQual'] + features['KitchenQual'] + features['HeatingQC'])\n\n# Creating new features by using new quality indicators.\n\nfeatures['QualGr'] = features['TotalQual'] * features['GrLivArea']\nfeatures['QualBsm'] = features['TotalBsmQual'] * (features['BsmtFinSF1'] +\n                                                  features['BsmtFinSF2'])\nfeatures['QualPorch'] = features['TotalExtQual'] * features['TotalPorchSF']\nfeatures['QualExt'] = features['TotalExtQual'] * features['MasVnrArea']\nfeatures['QualGrg'] = features['TotalGrgQual'] * features['GarageArea']\nfeatures['QlLivArea'] = (features['GrLivArea'] -\n                         features['LowQualFinSF']) * (features['TotalQual'])\nfeatures['QualSFNg'] = features['QualGr'] * features['Neighborhood']","ebcd0bd5":"features['HasPool'] = features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['Has2ndFloor'] = features['2ndFlrSF'].apply(lambda x: 1\n                                                     if x > 0 else 0)\nfeatures['HasGarage'] = features['QualGrg'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['HasBsmt'] = features['QualBsm'].apply(lambda x: 1 if x > 0 else 0)\nfeatures['HasFireplace'] = features['Fireplaces'].apply(lambda x: 1\n                                                        if x > 0 else 0)\nfeatures['HasPorch'] = features['QualPorch'].apply(lambda x: 1 if x > 0 else 0)","81920b09":"skewed = [\n    'LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n    'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea',\n    'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch',\n    'ScreenPorch', 'PoolArea', 'LowQualFinSF', 'MiscVal'\n]","1083582f":"# Finding skewness of the numerical features.\n\nskew_features = np.abs(features[skewed].apply(lambda x: skew(x)).sort_values(\n    ascending=False))\n\n# Filtering skewed features.\n\nhigh_skew = skew_features[skew_features > 0.3]\n\n# Taking indexes of high skew.\n\nskew_index = high_skew.index\n\n# Applying boxcox transformation to fix skewness.\n\nfor i in skew_index:\n    features[i] = boxcox1p(features[i], boxcox_normmax(features[i] + 1))\n","816edc9f":"# Features to drop:\n\nto_drop = [\n    'Utilities',\n    'PoolQC',\n    'YrSold',\n    'MoSold',\n    'ExterQual',\n    'BsmtQual',\n    'GarageQual',\n    'KitchenQual',\n    'HeatingQC',\n]\n\n# Dropping features.\n\nfeatures.drop(columns=to_drop, inplace=True)\n","601f5d36":"features = pd.get_dummies(data=features)\n","145e6f8b":"print(f'number of missing values: {features.isna().sum().sum()}')","1a342b08":"features.shape","d9b1b0c2":"features.sample(5)","8626c199":"features.describe().T","d2f5519c":"train = features.iloc[:len(y), :]\ntest = features.iloc[len(train):, :]","e69f2cd6":"correlation = train.join(y).corrwith(train.join(y)['SalePrice']).iloc[:-1].to_frame()","2026e2dd":"correlation['Abs_corr'] = correlation[0].abs()\nsorted_correlation = correlation.sort_values('Abs_corr', ascending=False)['Abs_corr']\nfig, ax = plt.subplots(figsize=(12, 12))\nsns.heatmap(sorted_correlation.to_frame()[sorted_correlation>=0.5], cmap='coolwarm',\n            annot=True, vmin=-1, vmax=1, ax=ax)\n","ebacf5af":"def plot_dist3(df, feature, title):\n    \n    # Creating a customized chart. and giving in figsize and everything.\n    \n    fig = plt.figure(constrained_layout=True, figsize=(12, 8))\n    \n    # creating a grid of 3 cols and 3 rows.\n    \n    grid = gridspec.GridSpec(ncols=3, nrows=3, figure=fig)\n\n    # Customizing the histogram grid.\n    \n    ax1 = fig.add_subplot(grid[0, :2])\n    \n    # Set the title.\n    \n    ax1.set_title('Histogram')\n    \n    # plot the histogram.\n    \n    sns.distplot(df.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 fit=norm,\n                 ax=ax1,\n                 color='#e74c3c')\n    ax1.legend(labels=['Normal', 'Actual'])\n\n    # customizing the QQ_plot.\n    \n    ax2 = fig.add_subplot(grid[1, :2])\n    \n    # Set the title.\n    \n    ax2.set_title('Probability Plot')\n    \n    # Plotting the QQ_Plot.\n    stats.probplot(df.loc[:, feature].fillna(np.mean(df.loc[:, feature])),\n                   plot=ax2)\n    ax2.get_lines()[0].set_markerfacecolor('#e74c3c')\n    ax2.get_lines()[0].set_markersize(12.0)\n\n    # Customizing the Box Plot:\n    \n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    \n    ax3.set_title('Box Plot')\n    \n    # Plotting the box plot.\n    \n    sns.boxplot(df.loc[:, feature], orient='v', ax=ax3, color='#e74c3c')\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=24))\n\n    plt.suptitle(f'{title}', fontsize=24)","aed7fb60":"plot_dist3(train.join(y), 'SalePrice', 'Sale price before log transformation')","0c2c1d8e":"X = train\nx_test = test\ny = np.log1p(y)","1d128084":"plot_dist3(train.join(y), 'SalePrice', 'Sale price after log transformation')","061b6c21":"from sklearn.linear_model import LinearRegression, ElasticNetCV, LassoCV, RidgeCV, TweedieRegressor\nfrom sklearn.model_selection import cross_val_score, KFold, cross_validate, train_test_split\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import GradientBoostingRegressor, HistGradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import mean_squared_error\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom mlxtend.regressor import StackingCVRegressor\nfrom catboost import CatBoostRegressor\n","a92e41d9":"kf = KFold(10, random_state=42)","8a1e439a":"alphas_alt = [15.5, 15.6, 15.7, 15.8, 15.9, 15, 15.1, 15.2, 15.3, 15.4]\n\nalphas_2 = [5e-05, 0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007, 0.008]\n\ne_alphas = [0.001, 0.002, 0.003, 0.004, 0.005, 0.006, 0.007]\n\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n","d99427b3":"ridge = make_pipeline(RobustScaler(), RidgeCV(alphas = alphas_alt, cv=kf ))\n\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas_2, \n                                             random_state=42, cv=kf))\n\nelastic_net = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, \n                                                      random_state=42, l1_ratio=e_l1ratio, cv=kf))\n\nsvr = make_pipeline(RobustScaler(), SVR(C=21, epsilon=0.0099, gamma=0.00017, tol=0.000121))\n\ngbr = GradientBoostingRegressor(n_estimators=2900, learning_rate=0.0161, max_depth=4, \n                               max_features='sqrt', min_samples_leaf=17, loss='huber', \n                               random_state=42)\n\nlightgbm = LGBMRegressor(objective='regression', n_estimators=3500,\n                         num_leaves=5, learning_rate=0.00721, max_bin=163,\n                         bagging_fraction=0.35711,n_jobs=-1,bagging_seed=42,\n                         feature_fraction_seed=42,bagging_freq=7,\n                         feature_fraction=0.1294,\n                         min_data_in_leaf=8)\n\n\nxgboost = XGBRegressor(learning_rate=0.0139, n_estimators=4500, max_depth=4, min_child_weight=0,\n    subsample=0.7968, colsample_bytree=0.4064, nthread=-1, scale_pos_weight=2, seed=42,)\n\n\nhgrd= HistGradientBoostingRegressor(loss= 'least_squares', max_depth= 2, min_samples_leaf= 40,\n                                    max_leaf_nodes= 29, learning_rate= 0.15, max_iter= 225,\n                                    random_state=42)\n\ntweed = make_pipeline(RobustScaler(),TweedieRegressor(alpha=0.005))\n\nlinear = make_pipeline(RobustScaler(), LinearRegression())\n\ncatboost = CatBoostRegressor(iterations=10, depth=2, learning_rate=0.001, \n                          loss_function='RMSE')\n\n\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, elastic_net, gbr,\n                                            xgboost, lightgbm,hgrd, tweed, linear, catboost),\n                                meta_regressor=xgboost,\n                                use_features_in_secondary=True)","dae32d40":"def model_check(X, y, estimators, cv):\n    \n    model_table = pd.DataFrame()\n    \n    row_index=0\n    \n    for est, label in zip(estimators, labels):\n        \n        mla_name = label\n        model_table.loc[row_index, 'Model name'] = mla_name\n        \n        cv_results = cross_validate(est,\n                                    X,\n                                    y,\n                                    cv=cv,\n                                    scoring='neg_root_mean_squared_error',\n                                    return_train_score=True,\n                                    n_jobs=-1)\n\n        model_table.loc[row_index, 'Train RMSE'] = -cv_results[\n            'train_score'].mean()\n        model_table.loc[row_index, 'Test RMSE'] = -cv_results[\n            'test_score'].mean()\n        model_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        model_table.loc[row_index, 'Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n    model_table.sort_values(by=['Test RMSE'],\n                            ascending=True,\n                            inplace=True)\n\n    return model_table\n\n        ","b690794e":"estimators = [ridge, lasso, elastic_net, gbr, xgboost, lightgbm, svr, hgrd, tweed, linear, catboost]\nlabels = [\n    'Ridge', 'Lasso', 'Elasticnet', 'GradientBoostingRegressor',\n    'XGBRegressor', 'LGBMRegressor', 'SVR', 'HistGradientBoostingRegressor','TweedieRegressor',\n    'LinearRegressor', 'CatBoostRegressor'\n]","bd587730":"raw_model = model_check(X, y, estimators, kf)\ndisplay(raw_model.style.background_gradient(cmap='summer_r'))","fc8f61a6":"from datetime import datetime\nprint(datetime.now(), 'StackingCVRegressor')\nstack_gen_model = stack_gen.fit(X.values, y.values)\nprint(datetime.now(), 'Elasticnet')\nelastic_model_full_data = elastic_net.fit(X, y)\nprint(datetime.now(), 'Lasso')\nlasso_model_full_data = lasso.fit(X, y)\nprint(datetime.now(), 'Ridge')\nridge_model_full_data = ridge.fit(X, y)\nprint(datetime.now(), 'SVR')\nsvr_model_full_data = svr.fit(X, y)\nprint(datetime.now(), 'GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)\nprint(datetime.now(), 'XGboost')\nxgb_model_full_data = xgboost.fit(X, y)\nprint(datetime.now(), 'Lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)\nprint(datetime.now(), 'Hist')\nhist_full_data = hgrd.fit(X, y)\nprint(datetime.now(), 'Tweed')\ntweed_full_data = tweed.fit(X, y)\nprint(datetime.now(), 'LinearRegression')\nlinear_full_data = linear.fit(X, y)\nprint(datetime.now(), 'CatBoostRegression')\ncatboost_full_data = catboost.fit(X, y)","b523d297":"def blend_models(X):\n    return ((0.1 * elastic_model_full_data.predict(X)) +\n            (0.1 * lasso_model_full_data.predict(X)) +\n            (0.1 * ridge_model_full_data.predict(X)) +\n            (0.1 * svr_model_full_data.predict(X)) +\n            (0.03 * gbr_model_full_data.predict(X)) +\n            (0.1 * xgb_model_full_data.predict(X)) +\n            (0.05 * lgb_model_full_data.predict(X)) +\n            (0.02 * catboost_full_data.predict(X)) +\n            (0.04 * hist_full_data.predict(X)) + \n            (0.01 * linear_full_data.predict(X))+            \n            (0.1 * tweed_full_data.predict(X)) +\n            (0.25 * stack_gen_model.predict(X.values)))\n","2c34c3a4":"submission = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')\n# Inversing and flooring log scaled sale price predictions\nsubmission['SalePrice'] = np.floor(np.expm1(blend_models(x_test)))\n# Defining outlier quartile ranges\nq1 = submission['SalePrice'].quantile(0.0050)\nq2 = submission['SalePrice'].quantile(0.99)\n\n# Applying weights to outlier ranges to smooth them\nsubmission['SalePrice'] = submission['SalePrice'].apply(\n    lambda x: x if x > q1 else x * 0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x\n                                                        if x < q2 else x * 1.1)\nsubmission = submission[['Id', 'SalePrice']]\n","ae8006bb":"submission.to_csv('mysubmission4.csv', index=False)\nprint(\n    'Save submission',\n    datetime.now(),\n)\nsubmission.head()","97424b9b":"## Modeling","73e61b2d":"## Loading the Training and Testing Data","2696bca5":"## Importing Libraries","8ea36cfc":"# Housing Price Prediction","54c85d30":"## EDA"}}