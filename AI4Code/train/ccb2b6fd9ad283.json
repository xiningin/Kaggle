{"cell_type":{"a3bfc1e8":"code","fcc13146":"code","f2da8ec6":"code","3fa6b1ff":"code","7b2c0619":"code","936bd541":"code","652c522e":"code","d42bad6a":"code","7105aa2c":"code","a2b7da54":"code","53ce2c4c":"code","2871193f":"code","c47a4ca1":"code","eac3c94b":"code","c88be44d":"code","65790131":"code","4e811bb5":"code","549d670d":"code","36109d98":"code","f7bc2c09":"code","b6cfbf1f":"code","91c4cfda":"code","780a13da":"code","a32599ab":"code","36088a0d":"code","4262a6fa":"code","884106f3":"code","38ed3222":"code","5c16bd97":"code","526c4bd5":"code","6d7d4481":"code","585ee651":"code","d69d9c8d":"code","f938a76f":"code","d6c20bad":"code","8409cacb":"markdown"},"source":{"a3bfc1e8":"import nltk\nfrom bs4 import BeautifulSoup\nimport string\nfrom nltk.tokenize import RegexpTokenizer\n","fcc13146":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f2da8ec6":"#load data\ndata= pd.read_csv(\"\/kaggle\/input\/amazon-fine-food-reviews\/Reviews.csv\")","3fa6b1ff":"data.head()","7b2c0619":"len(data) # Total rows ","936bd541":"#Drop the columns where at least one element is missing.\ndf=data.dropna()\n# and check again how many rows left\nlen(df) \n","652c522e":"#We need only three columns Score, Summary & Text. Let 's remove all the other columns.\n\ndf=df[['Score', 'Summary','Text']]\ndf.head()","d42bad6a":"#calculating length of lists in pandas dataframe column \n#ref. https:\/\/stackoverflow.com\/questions\/41340341\/pythonic-way-for-calculating-length-of-lists-in-pandas-dataframe-column\nsum=df['Summary'].str.len()\nprint(sum)","7105aa2c":"# let's check the length of summaries, the average length is 20 characters.\ndf['summary length'] = df['Summary'].apply(len)\ndf['summary length'].describe()","a2b7da54":"import seaborn as sns\nsns.boxplot(x='Score', y=df['summary length'], data=df)","53ce2c4c":"# let's do the same to the text, the avearage length is 302 characters.\ndf['Text length'] = df['Text'].apply(len)\ndf['Text length'].describe()","2871193f":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.boxplot(x='Score', y=df['Text length'], data=df)\nplt.ylim(0, 900)","c47a4ca1":"# Let 's keep the max length of summary to 30 characters and Text to 300 characters. \ndf= df[df['summary length'] <=30]\ndf= df[df['Text length'] <=300]\n\nlen(df)\ndf.head()","eac3c94b":"df=df[['Score', 'Summary','Text']]\ndf.head()\n#len(df) the length of updated dataset is 239868","c88be44d":"df = df.reset_index(drop=True)# we need to reindex the dataset after removed some rows\ndf.head()","65790131":"import requests\nfrom bs4 import BeautifulSoup \n\ndef preprocess_text(text):\n    \"\"\" Apply any preprocessing methods\"\"\"\n    text = BeautifulSoup(text).get_text()\n    text = text.lower()\n    text = text.replace('[^\\w\\s]','')\n    return text\n\ndf[\"Text\"] = df.Text.apply(preprocess_text)\ndf[\"Summary\"] = df.Summary.apply(preprocess_text)","4e811bb5":"field_length = df.Text.astype(str).map(len)\nprint()\nprint(\"###This is the longest text###\")\nprint (df.loc[field_length.idxmax(), 'Text'])\nprint()\nprint(\"###This is the shortest text###\")\nprint()\nprint (df.loc[field_length.idxmin(), 'Text'])","549d670d":"from nltk import word_tokenize\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\n#not is in the stopword list. W\/O adding whitelist, the summary would change from \"do not recommend\" to \"recommend\". This solution was borrowed from bertcarremans https:\/\/github.com\/bertcarremans\/TwitterUSAirlineSentiment\nwhitelist = [\"n't\", \"not\", \"no\"]\n\ndf['Text_after_removed_stopwords'] = df['Text'].apply(lambda x: ' '.join([word for word in x.split() if word in whitelist or word not in (stop)]))\nprint()\nprint('###Text after removed stopwords###'+'\\n'+df['Text_after_removed_stopwords'][1])\nprint()\nprint('###Text before removed stopwords###'+'\\n'+ df['Text'][1])\nprint()\ndf['Summary_after_removed_stopwords'] = df['Summary'].apply(lambda x: ' '.join([word for word in x.split() if word in whitelist or word not in (stop)]))\nprint('###Summary after removed stopwords###'+ '\\n'+df['Summary_after_removed_stopwords'][1])\nprint()\nprint('###Summary before removed stopwords###'+'\\n'+df['Summary'][1])","36109d98":"# A list of contractions from http:\/\/stackoverflow.com\/questions\/19790188\/expanding-english-language-contractions-in-python\ncontractions = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'll\": \"i will\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'll\": \"it will\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"needn't\": \"need not\",\n\"oughtn't\": \"ought not\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"she'd\": \"she would\",\n\"she'll\": \"she will\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"that'd\": \"that would\",\n\"that's\": \"that is\",\n\"there'd\": \"there had\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'll\": \"they will\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'll\": \"we will\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"who'll\": \"who will\",\n\"who's\": \"who is\",\n\"won't\": \"will not\",\n\"wouldn't\": \"would not\",\n\"you'd\": \"you would\",\n\"you'll\": \"you will\",\n\"you're\": \"you are\"\n}","f7bc2c09":"#This two blocks of code was refered from https:\/\/www.kaggle.com\/currie32\/summarizing-text-with-amazon-reviews\ndef clean_text(text):\n\n    # Replace contractions with longer forms in the above list\n    if True:\n        text = text.split()\n        new_text = []\n        for word in text:\n            if word in contractions:\n                new_text.append(contractions[word])\n            else:\n                new_text.append(word)\n        text = \" \".join(new_text)","b6cfbf1f":"\nclean_summaries = []\nfor summary in df.Summary:\n    clean_summaries.append(clean_text(summary))\nprint(\"Summaries are complete.\")\n\nprint(len(clean_summaries))\n\n\nclean_texts = []\nfor text in df.Text:\n    clean_texts.append(clean_text(text))\nprint(\"Texts are complete.\")\nprint(len(clean_texts))\nclean_text","91c4cfda":"df.head()","780a13da":"df.to_csv(r'\/amazon_clean.csv',index=False)\nlen(df)","a32599ab":"df1=df.sample(frac=0.001, replace=True, random_state=1)\nlen(df1)","36088a0d":"df1 = df1.reset_index(drop=True)# we need to reindex the dataset after removed some rows","4262a6fa":"df1.tail()","884106f3":"df2=df1[['Summary','Text']]\ndf2.head()\ndocs=df2.apply(lambda x: \" \".join(x), axis=1)\ndocs.head()","38ed3222":"from sklearn.feature_extraction.text import CountVectorizer\n#create a vocabulary of words, \n#ignore words that appear in 85% of documents, \n#eliminate stop words\ndocs=df2.apply(lambda x: \" \".join(x), axis=1).tolist()\n\ncv=CountVectorizer(max_df=0.85,max_features=10000)\nword_count_vector=cv.fit_transform(docs)\n \nlist(cv.vocabulary_.keys())[:10]\n ","5c16bd97":"from sklearn.feature_extraction.text import TfidfTransformer\n \ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(word_count_vector)","526c4bd5":"word_count_vector.shape","6d7d4481":"\"\"\"\nhttps:\/\/kavita-ganesan.com\/tfidftransformer-tfidfvectorizer-usage-differences\/#.XYtStuczbOQ\nneed the term frequency (term count) vectors for different tasks, use Tfidftransformer.\nneed to compute tf-idf scores on documents within\u201ctraining\u201d dataset, use Tfidfvectorizer\nneed to compute tf-idf scores on documents outside \u201ctraining\u201d dataset, use either one, both will work.\n\n\"\"\"\n\n# print idf values\ndf_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"])\n \n# sort ascending\ndf_idf.sort_values(by=['idf_weights'])","585ee651":"df3=df[['Summary','Text']]\n\ndf3.tail()","d69d9c8d":"#https:\/\/github.com\/kavgan\/nlp-in-practice\/blob\/master\/tf-idf\/Keyword%20Extraction%20with%20TF-IDF%20and%20SKlearn.ipynb\ndef sort_coo(coo_matrix):\n    tuples = zip(coo_matrix.col, coo_matrix.data)\n    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)","f938a76f":"def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n    \n    #use only topn items from vector\n    sorted_items = sorted_items[:topn]\n\n    score_vals = []\n    feature_vals = []\n\n    for idx, score in sorted_items:\n        fname = feature_names[idx]\n        \n        #keep track of feature name and its corresponding score\n        score_vals.append(round(score, 3))\n        feature_vals.append(feature_names[idx])\n\n    #create a tuples of feature,score\n    #results = zip(feature_vals,score_vals)\n    results= {}\n    for idx in range(len(feature_vals)):\n        results[feature_vals[idx]]=score_vals[idx]\n    \n    return results","d6c20bad":"# you only needs to do this once, this is a mapping of index to \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfeature_names=cv.get_feature_names()\n \n# get the document that we want to extract keywords from\ndoc=df['Text'][239866]\n \n#generate tf-idf for the given document\ntf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n \n#sort the tf-idf vectors by descending order of scores\nsorted_items=sort_coo(tf_idf_vector.tocoo())\n \n#extract only the top n; n here is 10\nkeywords=extract_topn_from_vector(feature_names,sorted_items,10)\n \n# now print the results\nprint(\"\\n=====Doc=====\")\nprint(doc)\nprint(\"\\n===Keywords===\")\nfor k in keywords:\n    print(k,keywords[k])\n    \nprint(\"\\n===original summary===\")\nprint(df['Summary'][239866])\n \n","8409cacb":"###According to this link https:\/\/stackoverflow.com\/questions\/54891464\/is-it-better-to-keras-fit-to-text-on-the-entire-x-data-or-just-the-train-data\nand other papers, I will do the tokenization to only train dataset. As I don't have enough memory, I will first export the cleaned dataset into csv format; \nSecond subsample the dataset and split the toy dataset into train and test sets; Third, tokenize the train set. \n"}}