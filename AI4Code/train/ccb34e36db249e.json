{"cell_type":{"49d7f84f":"code","3816f3c5":"code","b3965b0c":"code","1b3c4729":"code","dc642d6c":"code","f6804964":"code","7c955ab7":"code","3cfd66cd":"code","6bc8a0a2":"code","2101d88b":"code","6d42b6a9":"code","00174735":"code","8a0c2f97":"code","02eb1656":"code","c4735197":"code","700d78ee":"code","fe5ec529":"code","de301cb1":"code","c92be7bf":"code","e2c49f8b":"code","de2676c9":"code","df0f54f2":"code","01c9b5ad":"code","6535c23c":"code","a5a4691e":"code","ed66fc37":"code","4d6ee57a":"code","28662d06":"code","ad02df03":"code","8704fc59":"code","9fdd1fe8":"code","03bcd1bb":"code","77f30dfd":"code","dfa8bf5d":"code","f2323271":"markdown","f1d882d5":"markdown","3b5c250e":"markdown","0aba6f6e":"markdown","42b55f75":"markdown","88dd1905":"markdown","10e22066":"markdown","33484bdc":"markdown","3acd3cff":"markdown","092c7ec7":"markdown","d96a937e":"markdown","db61ec64":"markdown","56ecf6ca":"markdown","46e7bf72":"markdown","3589b245":"markdown","252ba80c":"markdown","a114b368":"markdown","7a7ff775":"markdown","1a093051":"markdown"},"source":{"49d7f84f":"## importing necessary libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sb\nimport matplotlib.pyplot as plt","3816f3c5":"# load data\ndata=pd.read_csv('..\/input\/mobile-price-classification\/train.csv')\n","b3965b0c":"data.head()","1b3c4729":"data.columns","dc642d6c":"data.shape","f6804964":"data.info()","7c955ab7":"data.describe()","3cfd66cd":"# checking null values in data\ndata.isna().sum()","6bc8a0a2":"plt.figure(figsize=(8,8))\nsb.heatmap(data.corr())","2101d88b":"col=[\"battery_power\",\"clock_speed\",\"int_memory\",\"mobile_wt\"]\nplt.figure(figsize=(10,10))\ni=1\nk=0\nfor j in col:\n    if i!=5:\n        plt.subplot(2,2,i)\n        sb.histplot(data[col[k]])\n        k=k+1\n    else:\n        break\n    i=i+1","6d42b6a9":"sb.countplot(data['price_range'])","00174735":"col=['blue',\"dual_sim\",\"three_g\",\"four_g\",\"touch_screen\",'wifi']\nplt.figure(figsize=(16,8))\ni=1\nk=0\nfor j in col:\n    if i!=7:\n        plt.subplot(2,3,i)\n        sb.countplot(data[col[k]])\n        k=k+1\n    else:\n        break\n    i=i+1","8a0c2f97":"sb.boxplot(data['price_range'],data['battery_power'])\n#price range of high battery power phones is high","02eb1656":"sb.scatterplot(data['price_range'],data['ram'])\n# phone with high ram having highest price","c4735197":"plt.subplot=(1,2,1)\nvalues=data['three_g'].value_counts().values\nlabels=[\"3G\",\"Non 3G\"]\ncol=[\"pink\",\"blue\"]\nplt.pie(values,labels=labels,colors=col,autopct='%1.1f%%')\nplt.show()\n\nplt.subplot=(1,2,2)\nvalues=data['four_g'].value_counts().values\nlabels=[\"4G\",\"Non 4G\"]\ncol=[\"pink\",\"blue\"]\nplt.pie(values,labels=labels,colors=col,autopct='%1.1f%%')\nplt.show()","700d78ee":"sb.barplot(data['dual_sim'],data['price_range'])\n# price range of dual sim phones are considerably high","fe5ec529":"sb.barplot(data['wifi'],data['price_range'])\n# price range of wifi phones are considerably high","de301cb1":"sb.barplot(x='price_range',y='px_height',data=data,palette=\"Reds\")\nplt.show()\n# mobiles which belongs to 3rd category having highest px_height\n\n","c92be7bf":"sb.barplot(x=\"price_range\",y='px_width',data=data,palette='Blues')\n# mobiles which belongs to 3rd category having highest px_width also","e2c49f8b":"# splitting data into dependent variable and independent variable\nX=data.drop('price_range',axis=1)  \nY=data['price_range']\n# train test splitting\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.20)","de2676c9":"# create and fit model\nfrom sklearn import tree\nmodel1=tree.DecisionTreeClassifier()\nmodel1.fit(x_train,y_train)\n# prediction\ny_pred_dt=model1.predict(x_test)\ny_pred_dt","df0f54f2":"#confusion matrix \nfrom sklearn.metrics import confusion_matrix\nconfusion_dt=confusion_matrix(y_test,y_pred_dt)\n\n#confusion matrix plot\nplot=sb.heatmap(confusion_dt,square=True,annot=True)\nclass_lables=['0','1','2','3']\nplot.set_xlabel('Actual values')\nplot.set_ylabel('Predicted values')","01c9b5ad":"# Accuracy, Precision, Recall\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nAccuracy_dt=accuracy_score(y_test,y_pred_dt)\nPrecision_dt=precision_score(y_test,y_pred_dt,average='macro')\nRecall_dt=recall_score(y_test,y_pred_dt,average='macro')\n","6535c23c":"# fit model\nfrom sklearn.naive_bayes import GaussianNB\nmodel2=GaussianNB()\nmodel2.fit(x_train,y_train)\ny_pred_gnb=model2.predict(x_test)\nprint(y_pred_gnb)","a5a4691e":"#confusion matrix \nfrom sklearn.metrics import confusion_matrix\nconfusion_gnb=confusion_matrix(y_test,y_pred_gnb)\n#confusion matrix plot\nplot=sb.heatmap(confusion_gnb,square=True,annot=True)\nclass_lables=['0','1','2','3']\nplot.set_xlabel('Actual values')\nplot.set_ylabel('Predicted values')","ed66fc37":"# Accuracy, Precision, Recall\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nAccuracy_gnb=accuracy_score(y_test,y_pred_gnb)\nPrecision_gnb=precision_score(y_test,y_pred_gnb,average='macro')\nRecall_gnb=recall_score(y_test,y_pred_gnb,average='macro')","4d6ee57a":"## fit model\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel3=KNeighborsClassifier(n_neighbors=3)\nmodel3.fit(x_train,y_train)\ny_pred_knn=model3.predict(x_test)\ny_pred_knn","28662d06":"#confusion matrix \nfrom sklearn.metrics import confusion_matrix\nconfusion_knn=confusion_matrix(y_test,y_pred_knn)\n#confusion matrix plot\nplot=sb.heatmap(confusion_knn,square=True,annot=True)\nclass_lables=['0','1','2','3']\nplot.set_xlabel('Actual values')\nplot.set_ylabel('Predicted values')","ad02df03":"# Accuracy, Precision, Recall\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nAccuracy_knn=accuracy_score(y_test,y_pred_knn)\nPrecision_knn=precision_score(y_test,y_pred_knn,average='macro')\nRecall_knn=recall_score(y_test,y_pred_knn,average='macro')\n","8704fc59":"# create and fit model\nfrom sklearn.ensemble import RandomForestClassifier\nmodel4=RandomForestClassifier(n_estimators=25,criterion='entropy')\nmodel4.fit(x_train,y_train)\ny_pred_rf=model4.predict(x_test)\ny_pred_rf","9fdd1fe8":"#confusion matrix \nfrom sklearn.metrics import confusion_matrix\nconfusion_rf=confusion_matrix(y_test,y_pred_rf)\n#confusion matrix plot\nplot=sb.heatmap(confusion_rf,square=True,annot=True)\nclass_lables=['0','1','2','3']\nplot.set_xlabel('Actual values')\nplot.set_ylabel('Predicted values')","03bcd1bb":"# Accuracy, Precision, Recall\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nAccuracy_rf=accuracy_score(y_test,y_pred_rf)\nPrecision_rf=precision_score(y_test,y_pred_rf,average='macro')\nRecall_rf=recall_score(y_test,y_pred_rf,average='macro')\n","77f30dfd":"models=pd.DataFrame({'Model':[\"Decision Tree\",\"Naive bayes\",\"KNN\",\"Random forest\"],\n                     'Accuracy':[Accuracy_dt,Accuracy_gnb,Accuracy_knn,Accuracy_rf],\n                    \"precision\":[Precision_dt,Precision_gnb,Precision_knn,Precision_rf],\n                    'Recall':[Recall_dt,Recall_gnb,Recall_knn,Recall_rf]})\nmodels","dfa8bf5d":"accuracy=models['Accuracy'].values\nprecision=models['precision'].values\nRecall=models['Recall'].values\nmodel=[\"Decision Tree\",\"Naive bayes\",\"KNN\",\"Random forest\"]\n\nx_axis=np.arange(len(model))\nplt.figure(figsize=(6,9))\nplt.bar(x_axis-0.2,accuracy,width=0.15,label=\"Accuracy\")\nplt.bar(x_axis-0.05,precision,width=0.15,label=\"Precision\")\nplt.bar(x_axis+0.1,Recall,width=0.15,label=\"Recall\")\nplt.xlabel(\"Models\")\nplt.ylabel(\"Accuracy\/precision\/Recall\")\nplt.legend()\nplt.show()","f2323271":"## 4)Random forest model\n\nRandom forest is a supervised learning algorithm. It can be used both for classification and regression. It is also the most flexible and easy to use algorithm. A forest is comprised of trees. It is said that the more trees it has, the more robust a forest is. Random forests creates decision trees on randomly selected data samples, gets prediction from each tree and selects the best solution by means of voting. It also provides a pretty good indicator of the feature importance.\n\n![image.png](attachment:b47752e2-9a56-48ec-8672-b7c8cfe22daa.png)","f1d882d5":"### *There is no null values in data*","3b5c250e":"## 2)Naive bayes classifier\n### ByGaussianNB\nNaive Bayes classifiers, a family of classifiers that are based on the popular Bayes\u2019 probability theorem, are known for creating simple yet well performing models, especially in the fields of document classification.\n\n![image.png](attachment:ab786ca7-cc03-49d5-afe0-4b5e862e81be.png)","0aba6f6e":"**As we can see that target price range has highly positive correlation between ram. \nAnd also pc and fc have good realtion,4G and 3G have good relation,px_height and px_width have good relation,sc_w and sc_h have good relation**","42b55f75":"**5)Scatter plot**","88dd1905":"### KNN classifier got highest accuracy","10e22066":"**2) Histograms** ","33484bdc":"## Mobile Price Predication\n\n### descripation of datasets:\n\nDataset as 21 features and 2000 entries. The meanings of the features are given below.\n\n* battery_power: Total energy a battery can store in one time measured in mAh\n* blue: Has bluetooth or not\n* clock_speed: speed at which microprocessor executes instructions\n* dual_sim: Has dual sim support or not\n* fc: Front Camera mega pixels\n* four_g: Has 4G or not\n* int_memory: Internal Memory in Gigabytes\n* m_dep: Mobile Depth in cm\n* mobile_wt: Weight of mobile phone\n* n_cores: Number of cores of processor\n* pc: Primary Camera mega pixels\n* px_height: Pixel Resolution Height\n* px_width: Pixel Resolution Width\n* ram: Random Access Memory in Mega Byte\n* sc_h: Screen Height of mobile in cm\n* sc_w: Screen Width of mobile in cm\n* talk_time: longest time that a single battery charge will last when you are\n* three_g: Has 3G or not\n* touch_screen: Has touch screen or not\n* wifi: Has wifi or not\n* price_range: This is the target variable with value of 0(low cost), 1(medium cost), 2(high cost) and 3(very high cost).\n","3acd3cff":"### Exploratory data analysis","092c7ec7":"#### pie plot for 4G and 3G phones","d96a937e":"### These is my first Kaggel notebook Please do upvote if you like my work","db61ec64":"#### Data Visualization\n\n**1) Correlation between features**","56ecf6ca":"## comparision of model","46e7bf72":"**7)Barplot**","3589b245":"**3) CountPlot**","252ba80c":"## 3)KNN model\n\nK- Nearest Neighbor\nKNN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution. In other words, the model structure determined from the dataset. This will be very helpful in practice where most of the real world datasets do not follow mathematical theoretical assumptions. In KNN, K is the number of nearest neighbors. The number of neighbors is the core deciding factor. K is generally an odd number if the number of classes is 2.\n![image.png](attachment:793e1dd4-46b3-4d99-b5dc-b711f4611f3d.png)","a114b368":"## Objective\n* 1) To predict price range of the mobile for test data\n* 2) To check the accuracy of the classifiers Decision tree,Na\u00efve Bayes classifier,K- Nearest Neighbor, Random forest model, Decision tree and K nearest neighbor.","7a7ff775":"**4) Boxplot**","1a093051":"# Classification Model\n\n## 1)Decision tree\n\nA decision tree is a flowchart-like tree structure where an internal node represents feature (or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner call recursive partitioning. This flowchart-like structure helps you in decision making. It's visualization like a flowchart diagram which easily mimics the human level thinking. That is why decision trees are easy to understand and interpret.\n\n![image.png](attachment:42782375-3b8a-4909-9258-f3844f44e8a2.png)"}}