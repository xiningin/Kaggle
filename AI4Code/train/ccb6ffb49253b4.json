{"cell_type":{"850ad01d":"code","8c7ff88f":"code","5e048622":"code","8eb74134":"code","d956ecd4":"code","9bbd7c0a":"code","599f6e47":"code","e0b63b02":"code","f0bfb959":"code","4ee367ab":"code","077da4a9":"code","6f72a542":"code","711c7c6e":"code","a68c789b":"code","7bbafe56":"code","ced9c568":"code","c63c41e4":"code","7893d97e":"code","e0fd26c2":"code","51ad5042":"code","b405176c":"code","501ca719":"code","798ee1a0":"code","4fad2cbe":"code","973db387":"code","dfe2d999":"code","c7a0f908":"code","b855c1b4":"code","1d7ef858":"markdown","ccae3479":"markdown","c381ebcf":"markdown","c4ef6e85":"markdown","8a4f425c":"markdown","84982e64":"markdown","b5d4cc7c":"markdown","0427118f":"markdown","a9c79fb5":"markdown","e930a7b1":"markdown","a5c66818":"markdown","fd0b3bd7":"markdown","a46847bb":"markdown","865790a9":"markdown"},"source":{"850ad01d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8c7ff88f":"import warnings\nwarnings.filterwarnings('ignore')\n\ntrain_ds = pd.read_csv('\/kaggle\/input\/umich-si650-nlp\/train.csv')\ntrain_ds.head(5)","5e048622":"# if the texts are truncated, just in case lets increase the max width to get the full text.\npd.set_option('max_colwidth',800)\nprint(train_ds[train_ds.label==1][0:5]) #positive_comments\nprint(train_ds[train_ds.label==0][0:5]) #negetive_comments","8eb74134":"train_ds.shape\n# data has 5668 rows with 2 columns.","d956ecd4":"train_ds.info()\n#We can see there are no missing values","9bbd7c0a":"# count plot to check the positive and negeticve comments ratio\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nplt.figure(figsize=(6,5))\nax = sns.countplot(x='label',data=train_ds)\nfor p in ax.patches:\n    ax.annotate(p.get_height(), (p.get_x()+0.1,p.get_height()+50))","599f6e47":"from sklearn.feature_extraction.text import CountVectorizer\nCount_Vectorizer = CountVectorizer()\nfeature_vector = Count_Vectorizer.fit(train_ds.sentence)\n#get the feature names\nfeatures = feature_vector.get_feature_names()\nprint('Total number of features :',len(features))","e0b63b02":"# Lets look at some of the random samples\nimport random\nrandom.sample(features,10)","f0bfb959":"train_ds_features= Count_Vectorizer.transform(train_ds.sentence)            \ntype(train_ds_features)\ntrain_ds_features.shape\n","4ee367ab":"# This matrix or data frame obtained consists of all non-zeros and zero values and hence lets calculate the ratio of non-zeros to zeros to see where we stand \n# `Lets store it as an sparse matrix, i.e this matrix stores only the vectors whose values are 1.","077da4a9":"# To get number of non_zeros\ntrain_ds_features.getnnz()","6f72a542":"print('Density of matrix:',train_ds_features.getnnz() *100 \/( train_ds_features.shape[0] * train_ds_features.shape[1]))","711c7c6e":"# Now lets visualize these count vectors, by converting the matrix into a dataFrame and lets set column namres as actual feature names\n# Converting matrix into a data Frame\ntrain_ds_df = pd.DataFrame(train_ds_features.todense())\ntrain_ds_df.columns = features\ntrain_ds[0:1]\n","a68c789b":"train_ds_df.iloc[0:1,230:250]","7bbafe56":"train_ds_df[[ 'brokeback', 'mountain', 'is', 'such', 'horrible', 'movie']][0:1]\n# We can see al of them are correcty named as 1","ced9c568":"# Summing the occurance of features column wise\nfeatures_counts = np.sum(train_ds_features.toarray(),axis=0)\nfeature_counts_df = pd.DataFrame(dict(features=features,counts=features_counts))\nfeature_counts_df","c63c41e4":"plt.figure(figsize=(12,5))\nplt.hist(feature_counts_df.counts, bins=50, range = (0,2000));\nplt.xlabel('Frequency of words')\nplt.ylabel('Density')\n       ","7893d97e":"# Fromm the graph we can observe that 1136 words are present only once, these can be ignored, we can set the max_features count depending the size of the file you are working\n# on, for now lets take it as 1000\nlen(feature_counts_df[feature_counts_df.counts == 1])","e0fd26c2":"count_vectorizer = CountVectorizer(max_features=1000)\nfeature_vector=count_vectorizer.fit(train_ds.sentence)\nfeatures = feature_vector.get_feature_names()\ntrain_ds_features=count_vectorizer.transform(train_ds.sentence)\n# count the freq of features\nfeature_counts=np.sum(train_ds_features.toarray(),axis=0)\nfeature_counts=pd.DataFrame(dict(features = features, counts = feature_counts))","51ad5042":"feature_counts.sort_values('counts',ascending=False)[0:15]","b405176c":"from sklearn.feature_extraction import text \nmy_stop_words = text.ENGLISH_STOP_WORDS\n\nprint('Few pre-defined stop words are:',list(my_stop_words)[0:10])","501ca719":"# We can also add our own stop words to the list.\nmy_stop_words = text.ENGLISH_STOP_WORDS.union(['harry','potter','da','vinci','code','mountain','movie','movies'])","798ee1a0":"# Creating count vectors without stop words\ncount_vectorizer = CountVectorizer(stop_words = my_stop_words,max_features=1000)\nfeature_vector=count_vectorizer.fit(train_ds.sentence)\nfeatures = feature_vector.get_feature_names()\ntrain_ds_features=count_vectorizer.transform(train_ds.sentence)\n# count the freq of features\nfeature_counts=np.sum(train_ds_features.toarray(),axis=0)\nfeature_counts=pd.DataFrame(dict(features = features, counts = feature_counts))","4fad2cbe":"feature_counts.sort_values('counts',ascending=False)[0:15]","973db387":"from nltk.stem.snowball import PorterStemmer\nstemmer = PorterStemmer()\nanalyzer = CountVectorizer().build_analyzer()\n\n# custom function for stemming and stop word removal\ndef stemmed_words(doc):\n#stemming of words\n    stemmed_words=[stemmer.stem(w) for w in analyzer(doc)]\n# removing these words in stop words list\n    non_stop_words= [word for word in stemmed_words if not word in my_stop_words]\n    return non_stop_words\n    \n\n       ","dfe2d999":"count_vectorizer = CountVectorizer(analyzer = stemmed_words,max_features=1000)\nfeature_vector=count_vectorizer.fit(train_ds.sentence)\ntrain_ds_features=count_vectorizer.transform(train_ds.sentence)\nfeature_counts=np.sum(train_ds_features.toarray(),axis=0)\nfeature_counts=pd.DataFrame(dict(features = features, counts = feature_counts))\nfeature_counts.sort_values('counts',ascending=False)[0:15]","c7a0f908":"# Now we can see that all the words are reduced to there root words. Next lets classify them into sentiments.\ntrain_ds_df = pd.DataFrame(train_ds_features.todense())\ntrain_ds_df.columns = features\ntrain_ds_df['sentiment']= train_ds.label","b855c1b4":"sns.barplot(x='sentiment', y = 'hate', data = train_ds_df, estimator=sum)","1d7ef858":"**1.BOW - Bag of Words**(https:\/\/medium.com\/greyatom\/an-introduction-to-bag-of-words-in-nlp-ac967d43b428)\n\nCreating a dictionary of words.\nThe process of converting NLP text into numbers is called vectorization in ML.\n\nDifferent models to convert text into vectors are:\n\n    1.count vector\n    2.Term Frequency vector Mode;l\n    3.Term Frequency- Inverse Vector Model\n\n","ccae3479":"# Step 2 - Text Pre-Processing","c381ebcf":"![image.png](attachment:image.png)","c4ef6e85":"Now we have removed stop words but another problem is we can see words like sucks , sucked , suck i.e same word repeated multiple times in different forms now we should convert them.\n\nProcess of converting into root words are:\n\n     Stemming (PorterStemmer , lancasterStemmer)\n     Lemmatization\n     \n![image.png](attachment:image.png)\n\n     \n","8a4f425c":"We can see that matrix has less than 1% of non-zero values, and rest all are zeros.","84982e64":"**2.Removing the low Frequency words**\nHere we might have some words which are repeating multiple times, while some words which might be very rare, lets visualise usng histogram to see the frequency of words.","b5d4cc7c":"We have 1903 features or unique words in the corpus.","0427118f":"# Step 1- Exploratory Data Analysis","a9c79fb5":"In the above list we can see there are words lie is , was and these are called stop words. These are not needed and hence we should remove them and that will be our next step.\n","e930a7b1":"We can see that there are:\n\n    poistive comments = 3204\n    \n    negetive comments = 2464\n    \nWe can see that both are almost fairly distributed, it is balanced.","a5c66818":"For this process we use NLTK (Natural Language Tool Kit) a very famous library in python with various useful features for NLP. ( http:\/\/www.nltk.org\/book\/ )","fd0b3bd7":"![image.png](attachment:image.png)","a46847bb":"We can see that the data contains 2 columns. a sentence column i.e the comments given on book and a label column which specify the sentiment for the sentence. where 0 = negetive and 1 = positive.","865790a9":"# Step By Step Approach for Text Analysis using NLP"}}