{"cell_type":{"34de3860":"code","5f60cc34":"code","c1559074":"code","ee6ebba1":"code","a16eace2":"code","01041c55":"code","490cc38c":"code","ba83341d":"code","e024fde9":"code","8ab7d81f":"code","d3091aa1":"code","c9adfc44":"code","02353159":"code","ec3507d8":"code","b39a134c":"code","97c6649d":"code","18fc9c5d":"code","54448397":"code","030b6b5f":"code","46295987":"code","f8fea328":"code","57515524":"code","c6472f68":"code","78a1165c":"code","ad0fb571":"code","cf01ad30":"code","fb55a559":"code","00856461":"code","eba9f299":"code","5f6ccb75":"code","63501c25":"code","6cd7dedb":"code","9fa7fb57":"code","9494eb99":"code","f8c3516b":"code","4c01ad56":"code","24af6c02":"code","e244e1dd":"code","2aef3dd4":"code","bf7193ce":"code","47e05382":"code","7dc2619d":"code","7017d171":"code","9380103b":"code","9f7b9603":"code","7aba41f2":"code","db3436fe":"code","84df802c":"code","637641aa":"code","b37d8004":"code","1fe5e4bf":"code","1ead0721":"code","acff3553":"code","81c8ddde":"code","d5fa4a49":"code","a3ea3bee":"code","5d948be3":"code","ff7a6ac0":"markdown","6380d1ad":"markdown","869dcb7a":"markdown","a22c5e48":"markdown","2cae773c":"markdown","d4ff8590":"markdown","a91b0f1b":"markdown","b3c4be22":"markdown","c7b63a81":"markdown","83373b79":"markdown","56328073":"markdown","1a8733a2":"markdown","b9680bd8":"markdown","f4592e18":"markdown","ab44fbf7":"markdown","c92611fc":"markdown","30a73862":"markdown","89d4225a":"markdown","ffc81a43":"markdown","cabf360d":"markdown","19420299":"markdown","70372538":"markdown","81aa1149":"markdown","733501e2":"markdown","e20d6058":"markdown","04c1c1e5":"markdown","41e8ee89":"markdown","f8650acf":"markdown","b4344649":"markdown","4fde2241":"markdown","d29bf3a5":"markdown"},"source":{"34de3860":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5f60cc34":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.model_selection import train_test_split # Import train_test_split function\nfrom sklearn.linear_model import LogisticRegression # to apply the Logistic regression\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import chi2\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score, classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor \nfrom statsmodels.tools.tools import add_constant\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import auc,roc_curve,f1_score,recall_score,precision_score\nfrom sklearn.preprocessing import StandardScaler","c1559074":"data =pd.read_csv(os.path.join(dirname, filename))","ee6ebba1":"data.head()","a16eace2":"data.shape","01041c55":"data.info()","490cc38c":"data.describe()","ba83341d":"def show_unique_values(data_frame):\n    print(\"Unique value for dataset attributes :\\n\")\n    for column in data_frame.columns:\n        print(column, \" \" ,data_frame[column].unique(), \"\\n\")   ","e024fde9":"show_unique_values(data)","8ab7d81f":"def show_missing_values(data):\n    missing_data = data.isnull()\n    for column in missing_data.columns.values.tolist():\n        print(column)\n        print (missing_data[column].value_counts())\n        print(\"\")","d3091aa1":"show_missing_values(data)","c9adfc44":"data.isnull().sum()","02353159":"#Verifing duplicate records in wine datasets\nduplicate = data[data.duplicated()] \nduplicate \n","ec3507d8":"sns.set(style=\"whitegrid\")\ndata.iloc[:,0:13].boxplot(figsize=(20,8))","b39a134c":"#box plot\nsns.set(style=\"whitegrid\")\nsns.set(rc={'figure.figsize':(4,2)})\nsns.boxplot(x=data['Ash'])\nplt.show()\nsns.boxplot(x=data['Ash_Alcanity'])\nplt.show()\nsns.boxplot(x=data['Magnesium'])\nplt.show()\nsns.boxplot(x=data['Proanthocyanins'])\nplt.show()","97c6649d":"# skewness along the index axis \ndata.skew(axis = 0, skipna = True)","18fc9c5d":"data['Customer_Segment'].value_counts(sort = False, normalize = True)*100","54448397":"X = data.iloc[:,0:13]  #independent columns\ny = data.iloc[:,-1]    #target column i.e price range","030b6b5f":"def show_top_univariate_filters(data, score_func, top_k):\n    X = data.iloc[:,0:13]  #independent columns\n    y = data.iloc[:,-1]    #target column i.e price range\n\n    if score_func == \"chi2\":\n        func = chi2\n    elif score_func == \"f_classif\":\n        func = f_classif\n    elif score_func == \"mutual_info_classif\":\n        func = mutual_info_classif\n    \n    #apply SelectKBest class to extract top k best features\n    bestfeatures = SelectKBest(score_func=func, k=top_k)\n    fit = bestfeatures.fit(X,y)\n\n    dfscores = pd.DataFrame(fit.scores_)\n    dfcolumns = pd.DataFrame(X.columns)\n\n    #concat two dataframes for better visualization \n    featureScores = pd.concat([dfcolumns,dfscores],axis=1)\n    featureScores.columns = ['features','Score']  #naming the dataframe columns\n    print(featureScores.nlargest(top_k,'Score'))  #print 10 best features","46295987":"show_top_univariate_filters(data, 'chi2', 10)","f8fea328":"show_top_univariate_filters(data, 'f_classif', 10)","57515524":"#get correlations of each features in dataset\ncorrmat = data.corr()\ntop_corr_features = corrmat.index\n\n#plot heat map\nplt.figure(figsize=(20,20))\nsns.heatmap(data[top_corr_features].corr(), annot=True, cmap=plt.cm.Reds)\nplt.show()","c6472f68":"VIF = pd.Series([variance_inflation_factor(X.values, i) \n               for i in range(X.shape[1])], \n              index=X.columns).to_frame()\n\n\nVIF.rename(columns={VIF.columns[0]: 'VIF'},inplace = True)\nVIF[~VIF.isin([np.nan, np.inf, -np.inf]).any(1)]","78a1165c":"# dropping Flavanoids column\nX.drop(\"Flavanoids\", axis=1, inplace=True)","ad0fb571":"X['Magnesium'] = np.log(X['Magnesium'])\nX['Malic_Acid'] = np.log(X['Malic_Acid'])","cf01ad30":"X.skew(axis = 0, skipna = True) ","fb55a559":"def show_top_decition_classifier_feature(data, classifier, top_k):\n\n    if classifier == \"ExtraTreesClassifier\":\n        classifier = ExtraTreesClassifier\n    elif classifier == \"DecisionTreeClassifier\":\n        classifier = DecisionTreeClassifier\n       \n    model = classifier()\n    model.fit(X,y)\n\n    #use inbuilt class feature_importances of tree based classifiers\n    print(model.feature_importances_) \n\n    #plot graph of feature importances for better visualization\n    feat_importances = pd.Series(model.feature_importances_, index=X.columns)\n    feat_importances.nlargest(top_k).plot(kind='barh')\n    plt.show()","00856461":"show_top_decition_classifier_feature(data, \"DecisionTreeClassifier\", 10)","eba9f299":"show_top_decition_classifier_feature(data, \"ExtraTreesClassifier\", 10)","5f6ccb75":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1) # 70% training and 30% test","63501c25":"logistic = LogisticRegression()\nlogistic_fit=logistic.fit(X_train,y_train)\ny_pred_test=logistic_fit.predict(X_test)","6cd7dedb":"print('Logistic model  Test Accuracy: {:.2f}%'.format(accuracy_score(y_test, y_pred_test) * 100))\n\nprint('Logistic model Classification report:\\n\\n', classification_report(y_test, y_pred_test))\n\nprint('Logistic model Training set score: {:.2f}%'.format(logistic_fit.score(X_train, y_train) * 100))","9fa7fb57":"from sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred_test)\n\nprint('Confusion matrix\\n\\n', cm)\n\nprint('\\nTrue Positives(TP) = ', cm[0,0])\n\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\n\nprint('\\nFalse Positives(FP) = ', cm[0,1])\n\nprint('\\nFalse Negatives(FN) = ', cm[1,0])","9494eb99":"# visualize confusion matrix with seaborn heatmap\n\ncm_matrix = pd.DataFrame(data=cm)\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')","f8c3516b":"# Create Decision Tree classifer object\nclf = DecisionTreeClassifier()\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)","4c01ad56":"print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","24af6c02":"# Create Decision Tree classifer object\nclf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=5)\n\n# Train Decision Tree Classifer\nclf = clf.fit(X_train,y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(X_test)\n","e244e1dd":"print('Test Accuracy:',accuracy_score(y_test,y_pred))","2aef3dd4":"print('Train Accuracy:',accuracy_score(y_train,clf.predict(X_train)))","bf7193ce":"param_grid = {\n'criterion':['gini','entropy'],\n'max_depth':[4,6,8,12]\n}","47e05382":"g_dt = GridSearchCV(clf,param_grid=param_grid,n_jobs=-1,cv=5,scoring='accuracy')","7dc2619d":"g_dt.fit(X_train,y_train)","7017d171":"g_dt.best_params_","9380103b":"g_dt.best_score_","9f7b9603":"f_dt = DecisionTreeClassifier(criterion='gini',max_depth=12)\nf_dt.fit(X_train,y_train)\ny_pred = f_dt.predict(X_test)\nprint('Test Accuracy:',accuracy_score(y_test,y_pred))","7aba41f2":"print('Train Accuracy:',accuracy_score(y_train,f_dt.predict(X_train)))","db3436fe":"cm = confusion_matrix(y_test,y_pred)\ncm\nprint(classification_report(y_test,y_pred))","84df802c":"pred_prob1 = logistic_fit.predict_proba(X_test)\npred_prob2 = f_dt.predict_proba(X_test)","637641aa":"# roc curve for models\nfpr1, tpr1, thresh1 = roc_curve(y_test, pred_prob1[:,1], pos_label=1)\nfpr2, tpr2, thresh2 = roc_curve(y_test, pred_prob2[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(y_test))]\np_fpr, p_tpr, _ = roc_curve(y_test, random_probs, pos_label=1)","b37d8004":"plt.style.use('seaborn')\n\n# plot roc curves\nplt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Logistic Regression')\nplt.plot(fpr2, tpr2, linestyle='--',color='green', label='Decision tree')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\n\nplt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();","1fe5e4bf":"algo_list=[]\nalgo_list=[\"Logistic Regression\",\"Decision Tree\"]","1ead0721":"score_list=[]\nscore_list.append(accuracy_score(y_test, y_pred_test) * 100)\nscore_list.append(accuracy_score(y_test, y_pred) * 100)","acff3553":"\nprecision=[]\nprecision.append(precision_score(y_test, y_pred_test, average='macro'))\nprecision.append(precision_score(y_test, y_pred, average='macro'))","81c8ddde":"\nrecall=[]\nrecall.append(recall_score(y_test, y_pred_test, average='macro'))\nrecall.append(recall_score(y_test, y_pred, average='macro'))","d5fa4a49":"\nf1score=[]\nf1score.append(f1_score(y_test, y_pred_test, average='macro'))\nf1score.append(f1_score(y_test, y_pred, average='macro'))","a3ea3bee":"score={\"Algorithims\":algo_list,\"Scores\":score_list,\"precision_score\":precision,\"recall_score\":recall,\"f1_score\":f1score}\nscore","5d948be3":"df=pd.DataFrame(score)\ndf","ff7a6ac0":"Checking for unique data values","6380d1ad":"**Decision tree**","869dcb7a":"None of the values is missing.","a22c5e48":"> Customer Segment looks like balanced with 3 different types. No imabalance treatment required","2cae773c":"> This case study will help us to understand the These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars. The analysis determined the quantities of 13 constituents found in each of the three types of wines. We will focus on the following stages namely -\n\n\n* Data Preparation\n* Feature Engineering\n* Feature Subset Selection\n* Model Training\n* Model Evaluation","d4ff8590":"**Dataset Data**:\n\nhttps:\/\/www.kaggle.com\/sadeghjalalian\/wine-customer-segmentation","a91b0f1b":"**Hyper-parameter Tuning**","b3c4be22":"**There some outliers in the following features.**\n \n* Ash\n* Ash_Alcanity\n* Magnesium\n* Proanthocyanins","c7b63a81":"**Variance inflation factor**","83373b79":"**Missing Values**\n> Lets see how the missing values can be replaced in the dataset. First check whereall the missing values are present.\n> \n> Take a closer look at the actual missing value count. 'False' means cell has a value whereas 'True\" means cell is missing value. Output the count for different attributes of dataframe.","56328073":"> Few observations : Following features are correlated when correlation >0.7\n\n* Total_Phenols and Flavanoids\n* Total_Phenols and OD280\n* Flavanoids and OD280","1a8733a2":"**Correlation Matrix with Heatmap**","b9680bd8":"**Transform skewed entities**","f4592e18":"> Reading data from Wine Datasets","ab44fbf7":"**Logistic regression**","c92611fc":"**Feature Subset Selection**","30a73862":"**Verify target class imbalance**","89d4225a":"**Verify Skewness in data**\n* Skewness is a measure of asymmetry of a distribution.\n* \n* If the skewness is between -0.5 and 0.5, the data are fairly symmetrical\n* If the skewness is between -1 and \u2014 0.5 or between 0.5 and 1, the data are moderately skewed\n* If the skewness is less than -1 or greater than 1, the data are highly skewed","ffc81a43":"All the columns are numeric in nature","cabf360d":"**Package Imports**","19420299":"**Duplicate Records Analysis**","70372538":"The features:\n\nMagnesium\n\nMalic_Acid\nare highly skewed and hence should be transformed","81aa1149":"* Variance inflation factor (VIF) is a measure of the amount of multicollinearity in a set of multiple regression variables","733501e2":"# Feature Subset Selection","e20d6058":"> There is no duplicate Data","04c1c1e5":"**Possible Work to be done** :\n\n* Write a Data Science Proposal for achieving the objective mentioned.\n* Perform exploratory analysis on the data.\n* Perform data wrangling \/ pre-processing.\n* Apply any 2 features engineering technique.\n* Plot top 10 features.\n* Identification of the performance parameters to be improved, for the given problem statement.\n* Design Machine Learning models \u2013 Logistic regression and Decision tree to predict.\n* Compare the performance of selected feature engineering techniques.\n* Compare the performance of the 2 classifiers \u2013 Logistic regression and Decision tree to predict.\n* 10.Conclusions.","41e8ee89":"# Data Preparation","f8650acf":"# Feature Engineering","b4344649":"**Confirm the imports**","4fde2241":"**Noisy Data Verification**","d29bf3a5":"# Model Training"}}