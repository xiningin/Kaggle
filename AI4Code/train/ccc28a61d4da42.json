{"cell_type":{"ad376b82":"code","2f67e3dc":"code","3286cf8f":"code","eafbc748":"code","4e4fd75a":"code","e482dfff":"code","e1c9aa5e":"code","6a7f34fc":"code","aa1e1531":"code","ce60b068":"code","478cd561":"code","f4a9ab2d":"code","08b2ae24":"code","bb095d97":"code","7d752037":"code","15229188":"code","76aeded3":"markdown","9fc21489":"markdown","9e24f12f":"markdown","a8a8db9f":"markdown","cb4563cd":"markdown","b3a9b8ed":"markdown","cba0d1cb":"markdown","ce6d3bff":"markdown","6c183723":"markdown","be9f4b2b":"markdown","391a4ef0":"markdown","5847a0cb":"markdown","87680a79":"markdown","fb3514cd":"markdown","389012ee":"markdown","82320765":"markdown"},"source":{"ad376b82":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2f67e3dc":"data = pd.read_csv('..\/input\/voice.csv')\ndata.head()","3286cf8f":"data.info()","eafbc748":"seaborn.pairplot(data[['meanfreq', 'Q25', 'Q75', 'skew', 'centroid', 'label']],hue='label', size=3)","4e4fd75a":"data = data.sample(frac=1)\n\ndata.head()","e482dfff":"data['label'] = data['label'].map({'male':1,'female':0})","e1c9aa5e":"X = data.loc[:, data.columns != 'label']\ny = data.loc[:,'label']","6a7f34fc":"X = (X - np.min(X))\/(np.max(X)-np.min(X)).values\n\nX.head()","aa1e1531":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.33, random_state = 42)","ce60b068":"from sklearn import linear_model\nlogreg = linear_model.LogisticRegression(random_state = 42,max_iter= 150)\nprint(\"test accuracy: {} \".format(logreg.fit(X_train, y_train).score(X_test, y_test)))\nprint(\"train accuracy: {} \".format(logreg.fit(X_train, y_train).score(X_train, y_train)))","478cd561":"from sklearn.ensemble import RandomForestClassifier\n\nran_for = RandomForestClassifier(n_estimators=250, max_depth=15, random_state=42)\nprint(\"test accuracy: {} \".format(ran_for.fit(X_train, y_train).score(X_test, y_test)))\nprint(\"train accuracy: {} \".format(ran_for.fit(X_train, y_train).score(X_train, y_train)))","f4a9ab2d":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=3)\nprint(\"test accuracy: {} \".format(knn.fit(X_train, y_train).score(X_test, y_test)))\nprint(\"train accuracy: {} \".format(knn.fit(X_train, y_train).score(X_train, y_train)))","08b2ae24":"score = []\n\nfor i in range(1,20):\n    knn2 = KNeighborsClassifier(n_neighbors=i)\n    knn2.fit(X_train, y_train)\n    score.append(knn2.score(X_test, y_test))\n    \nplt.plot(range(1,20), score)\nplt.xlabel('k values')\nplt.ylabel('sccuracy')\nplt.show()","bb095d97":"from sklearn.svm import SVC\n\nsvm = SVC(random_state=42)\nprint(\"test accuracy: {} \".format(svm.fit(X_train, y_train).score(X_test, y_test)))\nprint(\"train accuracy: {} \".format(svm.fit(X_train, y_train).score(X_train, y_train)))","7d752037":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\nprint(\"test accuracy: {} \".format(nb.fit(X_train, y_train).score(X_test, y_test)))\nprint(\"train accuracy: {} \".format(nb.fit(X_train, y_train).score(X_train, y_train)))","15229188":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\nprint(\"test accuracy: {} \".format(dt.fit(X_train, y_train).score(X_test, y_test)))\nprint(\"train accuracy: {} \".format(dt.fit(X_train, y_train).score(X_train, y_train)))","76aeded3":"**Categorical Encoding**","9fc21489":"**If you well look at the label column you can see that all genders are sequential. Firstly we have to make shuffle on this data.**","9e24f12f":"**Prediction with SVM**","a8a8db9f":"**Prediction with Logistic Regression**","cb4563cd":"**Prediction with Random Forest**","b3a9b8ed":"**Prediction with KNN**","cba0d1cb":"**Load the Data**","ce6d3bff":"**Prediction with Decision Tree**","6c183723":"**Finding the Best k Values**","be9f4b2b":"**Prediction with Naive Bayes**","391a4ef0":"**I guess it looks like 3 or 4. Therefore I chose 3**","5847a0cb":"So, if you think about something please share with me. \n\nThank you :)","87680a79":"** On this Kernel, I will make a classification about gender voice **\n\nMy steps are:\n*  Firstly I will import libraries\n*  Loading Data\n*  Data Visualization\n*  One Hot Encoding\n*  Normalization\n*  Train - Test Split\n*  Finally, Predictions","fb3514cd":"**Train - Test Split**","389012ee":"**Normalization**","82320765":"**Data Visualization**"}}