{"cell_type":{"debc7ea3":"code","816fb707":"code","aca6e772":"code","38ae1bfb":"code","e9b89a4c":"code","f3ebde71":"code","4aab5e4b":"code","eb736b50":"code","62814a7b":"code","601ba360":"code","1eca5ec3":"code","5184c5e6":"code","84cb1b38":"code","f0cc0475":"code","de9aac4d":"code","e6234e16":"code","54b716ab":"code","ed1a0b4a":"code","dd94720d":"code","f1547785":"code","96d54272":"code","7ef29017":"code","1550e752":"code","c514d429":"markdown","ec4449a1":"markdown","21ae0e9e":"markdown","fe76466e":"markdown","1b5dab4b":"markdown","cdc28d3c":"markdown"},"source":{"debc7ea3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\nimport re\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\npaths = []\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        paths.append(os.path.join(dirname, filename))\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","816fb707":"import os\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split \nimport seaborn as sns\nfrom sklearn import tree\nfrom sklearn.metrics import f1_score\nfrom sklearn.tree import plot_tree\nfrom sklearn import tree\nfrom sklearn.preprocessing import OneHotEncoder\nimport matplotlib.pyplot as plt\nimport time\nfrom copy import deepcopy as depcp\nimport tqdm","aca6e772":"df = d['2019']\ndf","38ae1bfb":"df.drop(columns=['Overall rank'], inplace=True)","e9b89a4c":"df.head()","f3ebde71":"target = 'Score'","4aab5e4b":"df['Score'] = df['Score'].round()","eb736b50":"X = df.drop(columns=[target, 'Country or region'])\ny = df[target]\ncategotial_col = df['Country or region']\nX = X.to_numpy()\n","62814a7b":"print(X.shape)\nprint(y.shape)","601ba360":"fig, ax = plt.subplots(1, 1)\nax.hist(df['Score'])\nax.set_title(\"'Score' disctibution in dataset -> when you talk 'Score', thing about happiness in 0 - 10 grade\")\nax.set_xlabel('Score')\nax.set_ylabel('ammount')\nplt.show()\ndef calc_scores_for_DummyClasifire(models, X, y):\n    results = {}\n    models_names = []\n    c = 0\n    for model in models:\n        model_name = f'{str(model.__class__())[:-2]} {model.strategy}'\n        models_names.append(model_name)            \n        model.fit(X, y)        \n        results[model_name] = model.score(X,y)\n    return results\n\ncalc_scores_for_DummyClasifire([DummyClassifier(random_state=42, strategy='most_frequent'), \n                           DummyClassifier(random_state=42, strategy='uniform'),\n                           DummyClassifier(random_state=42, strategy='stratified'),\n                           DummyClassifier(random_state=42, strategy='prior')], X=X, y=y)\n","1eca5ec3":"X = X.astype(float)\ny = y.astype(int)\nprint(X.shape , y.shape)\nX_train, X_test, y_train, y_test = train_test_split(\n            X,\n            y,\n            test_size=0.33,\n            stratify=y,\n            shuffle=True)\n\nX_train.shape, y_train.shape, X_test.shape, y_test.shape \n","5184c5e6":"models = [DummyClassifier(random_state=42, strategy='most_frequent'),\n          DecisionTreeClassifier(random_state=42),\n          RandomForestClassifier(random_state=42)]","84cb1b38":"def evaluate_model(model,X,y,seeds=10):\n    scores = []\n    initial_time = time.time()\n    for i in range(seeds):\n        model_ = depcp(model)\n        (X_train, X_test, y_train, y_test) = train_test_split(\n            X,\n            y,\n            test_size=0.33,\n            random_state=i,\n            stratify=y,\n            shuffle=True,\n            )\n        model_.fit(X_train, y_train)\n        yhat = model_.predict(X_test)\n        score = f1_score(y_test, yhat, average='weighted')\n        scores.append(score)\n\n    return (np.array(scores), model_)\n\n","f0cc0475":"results = {}\nfor model in tqdm.tqdm(models):\n    model_name = model.__class__.__name__\n    score, model = evaluate_model(model,X,y,seeds = 300)\n    result_dict  = {\n        'score': score.mean(),        \n        'model': model\n    }\n    results[model_name] = result_dict","de9aac4d":"results","e6234e16":"model = results['DecisionTreeClassifier']['model']\nfeature_names = df.drop(columns=['Country or region', 'Score']).columns.tolist()\nfeature_names\nclass_names = y.unique().astype(str).tolist()","54b716ab":"model = results['DecisionTreeClassifier']['model']\nfeature_names = df.drop(columns=['Country or region', 'Score']).columns.tolist()\nfeature_names\nclass_names = y.unique().astype(str).tolist()\nfeature_names, class_names","ed1a0b4a":"fig = plt.figure(figsize=(30,25))\nimg=plot_tree(model, filled=True, class_names=class_names, feature_names=feature_names)","dd94720d":"img_path = os.path.join(os.getcwd(), 'tree')\nfig.savefig(img_path, dpi=140)\n","f1547785":"text_representation = tree.export_text(model, feature_names=feature_names)\nprint(text_representation)","96d54272":"# features = X.columns.tolist()\nfeature_names, class_names\nimportances = model.feature_importances_\nprint(importances)\nindices = np.argsort(importances)\nprint(indices)\nfig = plt.figure(figsize=(10,5))\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()\n\n    ","7ef29017":"forest_model = results['RandomForestClassifier']['model']\nprint(feature_names, class_names)\n\ndef show_results(model):\n    fig = plt.figure(figsize=(30,25))\n#     img=plot_tree(model, filled=True, class_names=class_names, feature_names=feature_names)\n    _ = plot_tree(model.estimators_[1], filled=True,feature_names = feature_names, class_names=class_names) # why estimators_[1]??      \n    importances = model.feature_importances_\n    print(importances)\n    indices = np.argsort(importances)\n    print(indices)\n    fig = plt.figure(figsize=(10,5))\n    plt.title('Feature Importances')\n    plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n    plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n    plt.xlabel('Relative Importance')\n    plt.show()\n\nshow_results(model=forest_model)","1550e752":"forest_model.estimators_[19]","c514d429":"# RandomForestClassifier","ec4449a1":"For analysis I use below datasets:  \nhttps:\/\/www.kaggle.com\/unsdsn\/world-happiness ,\nBut I choose moste actual data from 2019 year.","21ae0e9e":"# DummyClasifire","fe76466e":"# Conclusion\nSocial suport and 'Healthy life expectancy' are moste important thing when we select in survey our happinest from live using grades from 0-10","1b5dab4b":"# Prepare dataset","cdc28d3c":"# DecisionTreeClassifier"}}