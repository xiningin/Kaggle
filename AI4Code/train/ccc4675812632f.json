{"cell_type":{"0d912225":"code","f59bb0f3":"code","4713554e":"code","4366f5b3":"code","6594d59a":"code","b0a73679":"code","97662215":"code","b57f8952":"code","efbde763":"code","f88c6709":"code","6e822e45":"code","a6f795ae":"code","d539797c":"code","41b5c0fb":"code","d15f154c":"code","edeaad22":"code","d69b7aeb":"code","807926e2":"code","155baf5a":"code","445d4cb4":"code","0888d137":"code","55e75c98":"code","0a6bfcba":"code","4fcea56c":"code","0ba835b3":"code","3b3a51cc":"code","30dd46fc":"code","7c8d10d0":"code","3f4ff04f":"code","e84ecdee":"code","81767da7":"code","20d28c80":"code","b65733ac":"code","b0b01d19":"code","af8fd6a7":"code","61a26f18":"code","645f149f":"code","5525ea86":"code","caa392cf":"code","23d1f296":"code","756ad567":"code","97280e3d":"code","387f4bfd":"code","ef9d7dda":"code","eb3ebbd7":"markdown","a8f0a2a1":"markdown","8756972d":"markdown","cf99a353":"markdown","9f02684a":"markdown","646b12cb":"markdown","45f70210":"markdown","f8662733":"markdown","c48f402d":"markdown","1a07fbc4":"markdown","5839695a":"markdown","11f01bff":"markdown","4b877123":"markdown","334f027e":"markdown","2400eae1":"markdown","84ae52d6":"markdown"},"source":{"0d912225":"# Running this command to activate autocomplete.\n\n%config Completer.use_jedi = False","f59bb0f3":"# Importing the necessary libraries.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.impute import KNNImputer\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, confusion_matrix ,ConfusionMatrixDisplay\nimport pickle","4713554e":"# Reading the data.\n\ndata = pd.read_csv(\"..\/input\/heart-disease-prediction-using-logistic-regression\/framingham.csv\")","4366f5b3":"# Having a look at the data.\n\nprint(data.head())\nprint(\"========================================================\")\nprint(data.columns)\nprint(\"========================================================\")\nprint(data.describe())\nprint(\"========================================================\")\nprint(data.isnull().sum())\nprint(\"========================================================\")\nprint(data.shape)\nprint(\"========================================================\")\nprint(data.dtypes)","6594d59a":"# Checking if the data is imbalanced.\n\nsns.displot(data.TenYearCHD)","b0a73679":"# Visualizing the NAN s on a heatmap\n\nplt.figure(figsize=(12,8))\nsns.heatmap(data.isnull())","97662215":"data[\"education\"].describe()","b57f8952":"plt.figure()\nsns.displot(data.education, kde=True)","efbde763":"# Using a creative way to distinguish and treat null values specially.\n# creating a \"Unknown - 5\" because number of null values are high.\n\ndata[\"education_nan\"] = np.where(data.education.isnull(),1,0)\ndata[\"education\"].fillna(5, inplace=True)","f88c6709":"data.cigsPerDay.describe()","6e822e45":"plt.figure()\nsns.displot(data.cigsPerDay, kde=True)","a6f795ae":"data[\"cigsPerDay\"].fillna(9, inplace=True)","d539797c":"data.BPMeds.describe()","41b5c0fb":"plt.figure()\nsns.displot(data.BPMeds, kde=True)","d15f154c":"data[\"BPMeds_nan\"] = np.where(data.BPMeds.isnull(),1,0)\ndata[\"BPMeds\"].fillna(0, inplace=True)","edeaad22":"data.totChol.describe()","d69b7aeb":"plt.figure()\nsns.displot(data.totChol, kde=True)","807926e2":"data[\"totChol_nan\"] = np.where(data.totChol.isnull(),1,0)\ndata[\"totChol\"].fillna(data.totChol.median(), inplace=True)","155baf5a":"data.BMI.describe()","445d4cb4":"plt.figure()\nsns.displot(data.BMI, kde=True)","0888d137":"data[\"BMI\"].fillna(data.BMI.mean(), inplace=True)","55e75c98":"data.glucose.describe()","0a6bfcba":"plt.figure()\nsns.displot(data.glucose, kde=True)","4fcea56c":"# Has high number of null values, so using a creative way to distinguish and treat null values specially.\n# Using median as big outliers are present.\n\ndata[\"glucose_nan\"] = np.where(data.glucose.isnull(),1,0)\ndata[\"glucose\"].fillna(data.glucose.median(), inplace=True)","0ba835b3":"data.heartRate.describe()","3b3a51cc":"plt.figure()\nsns.displot(data.heartRate, kde=True)","30dd46fc":"data[\"heartRate\"].fillna(data.heartRate.median(), inplace=True)","7c8d10d0":"# dividing the data into features and target variable.\nx = data.drop(columns=\"TenYearCHD\")\ny = data.TenYearCHD\n\n# Fixing the imbalanced data by random oversampling, as our dataset is small.\nros = RandomOverSampler(random_state=0)\nX_resampled, y_resampled = ros.fit_resample(x, y)\n\n# Splitting the data into features and target variable.\ntrain_x, test_x, train_y, test_y = train_test_split(X_resampled, y_resampled, test_size=0.25)","3f4ff04f":"# balanced dataset\nsns.displot(y_resampled)","e84ecdee":"# Creating pipelies.\n\npipe1 = Pipeline([(\"robust_scalar\", RobustScaler()),(\"std_scalar\", StandardScaler()), (\"logistic_regression\", LogisticRegression())])\n\npipe2 = Pipeline([(\"robust_scalar\", RobustScaler()),(\"std_scalar\", StandardScaler()), (\"KNN\", KNeighborsClassifier())])\n\npipe3 = Pipeline([(\"robust_scalar\", RobustScaler()),(\"std_scalar\", StandardScaler()), (\"svm\", SVC())])\n\npipe4 = Pipeline([(\"robust_scalar\", RobustScaler()),(\"std_scalar\", StandardScaler()), (\"XGboost\", XGBClassifier())])\n\npipe5 = Pipeline([(\"robust_scalar\", RobustScaler()),(\"std_scalar\", StandardScaler()), (\"decision_tree\", DecisionTreeClassifier())])\n\npipe6 = Pipeline([(\"robust_scalar\", RobustScaler()),(\"std_scalar\", StandardScaler()), (\"random_forest\", RandomForestClassifier())])","81767da7":"# Fitting the pipelines\n\npipelines = [pipe1, pipe2, pipe3, pipe4, pipe5, pipe6]\n\nfor pipe in pipelines:\n    pipe.fit(train_x, train_y)","20d28c80":"# Predicting\n\npred1 = pipe1.predict(test_x)\npred2 = pipe2.predict(test_x)\npred3 = pipe3.predict(test_x)\npred4 = pipe4.predict(test_x)\npred5 = pipe5.predict(test_x)\npred6 = pipe6.predict(test_x)","b65733ac":"# Comparing the result of each pipeline and selecting the best pipeline. More emphasis is given to recall as we really \n# want the false negative to be as low as possible.\n\nprint(\"Accuracy of Logistic_Regression\", round(accuracy_score(test_y, pred1)*100, 2), \"%\")\nprint(\"Recall of Logistic_Regression\", round(recall_score(test_y, pred1),2))\nprint(\"===================================================================\")\nprint(\"Accuracy of KNN\", round(accuracy_score(test_y, pred2)*100, 2), \"%\")\nprint(\"Recall of KNN\", round(recall_score(test_y, pred2),2))\nprint(\"===================================================================\")\nprint(\"Accuracy of SVC\", round(accuracy_score(test_y, pred3)*100,2), \"%\")\nprint(\"Recall of SVC\", round(recall_score(test_y, pred3),2))\nprint(\"===================================================================\")\nprint(\"Accuracy of xgboost\", round(accuracy_score(test_y, pred4)*100,2), \"%\")\nprint(\"Recall of xgboost\", round(recall_score(test_y, pred4),2))\nprint(\"===================================================================\")\nprint(\"Accuracy of decision_tree\", round(accuracy_score(test_y, pred5)*100,2), \"%\")\nprint(\"Recall of decision_tree\", round(recall_score(test_y, pred5),2))\nprint(\"===================================================================\")\nprint(\"Accuracy of Random_forest\", round(accuracy_score(test_y, pred6)*100,2), \"%\")\nprint(\"Recall of Random_forest\", round(recall_score(test_y, pred6),2))","b0b01d19":"# Visualizing the confusion matrix. (Our aim is to reduce false negative)\n\ncm = confusion_matrix(test_y, pred5)\ndisplay = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisplay.plot()","af8fd6a7":"# Trying another method to reduce the unnecessary features\n\nobj = SelectKBest(f_regression, k=4)\nnew_data = obj.fit_transform(x,y)\n\nfilter = obj.get_support()\nfeature = x.columns\nfinal_f = feature[filter]\nprint(feature[filter])","61a26f18":"# Looking at the estimator's parameters\n\npipe5.get_params().keys()","645f149f":"# Using randomized search cv to get the best parameter values\n\nparams = {\n    'decision_tree__max_leaf_nodes'       : [1,2,4,6,8,12,15],\n    #'decision_tree__max_features'         : ['auto', 'sqrt', 'log2'],\n    'decision_tree__random_state'         : [42],\n    'decision_tree__max_depth'            : [2, 3, 5, 10, 20],\n    'decision_tree__min_samples_leaf'     : [5, 10, 20, 50, 100],\n    'decision_tree__criterion'            : [\"gini\", \"entropy\"]\n    }\n\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfinal_model = RandomizedSearchCV(pipe5, param_distributions=params, n_iter=3, cv=3)\n\nfinal_model.fit(train_x[final_f], train_y)","5525ea86":"# All the parameter values selected in the above process\nfinal_model.best_params_","caa392cf":"# prediction with test data\nprediction = final_model.predict(test_x[final_f])","23d1f296":"# visualizing the predicted values\n\nplt.figure(figsize=(18,10))\nsns.displot(prediction)","756ad567":"# Looking at the metrics\n\nprint(\"Accuracy is\", accuracy_score(test_y, prediction))\nprint(\"Recall is\", recall_score(test_y, prediction))\nprint(\"Precision is\", precision_score(test_y, prediction))","97280e3d":"# Visualizing confusion matrix\n\ncm = confusion_matrix(test_y, prediction)\ndisplay = ConfusionMatrixDisplay(confusion_matrix=cm)\ndisplay.plot()","387f4bfd":"# Manipulating our predicted values based on probabilities. This is done to reduce False Negative.\nprobalities = final_model.predict_proba(test_x[final_f])\nprediction_prob = np.where(probalities>=0.30, 1, 0)[:,1]\n\n# Visualizing Confusion matrix\ncm = confusion_matrix(test_y, prediction_prob)\ndisplay_cm = ConfusionMatrixDisplay(confusion_matrix = cm)\ndisplay_cm.plot()","ef9d7dda":"# Finally saving our model as a pickel file. (For deployment)\npickle.dump(final_model, open('model.pkl','wb'))","eb3ebbd7":"# This is my model building code for my end to end project. \n# I hope this code as well as the comments will help you understand the steps of model building. \n# Also I am very very open minded, so any feedback is welcomed!","a8f0a2a1":"Imputing the \"BMI\" features","8756972d":"Imputing the \"glucose\" feature","cf99a353":"Imputing the \"BPMeds\" feature.","9f02684a":"# Hyparameter Tuning","646b12cb":"Imputing the \"heartRate\" feature","45f70210":"# Modelling","f8662733":"Imputing the \"totChol\" features","c48f402d":"# A very important note: Calculating probability in Decision Tree is meaningless because this model works on simple decision making and not Probability. I am able to get the probability because i have assigned values to min_smaples_leaf.\n\n# This is just to give you an idea how you can reduce False Negative or False Positive.","1a07fbc4":"# Feature Selection","5839695a":"Since Decision tree has the highest recall, therfore we will go with Decision Tree.\nHigh recall means low False Negative.\nLower the false negative in this case, better it will perform in real life.","11f01bff":"Now we have very low False negative of 73.","4b877123":"Imputing the \"education\" feature.","334f027e":"Imputing the \"cigsPerDay\" feature.","2400eae1":"Our data is highly imbalaced.","84ae52d6":"# Handling missing values & Feature Engineering."}}