{"cell_type":{"8ec00c1a":"code","108b92ac":"code","f693a2bc":"code","9742860d":"code","032184a2":"code","2b88d694":"code","62d9f044":"code","3b15da08":"code","f9f18acc":"code","5fe31133":"code","26a157a8":"code","76583303":"code","13b564c6":"code","3de1cefc":"code","7228eeb6":"code","2ba906d3":"code","086b3a25":"code","f30fcf99":"code","541a440d":"code","7ec320c2":"code","d9b5bf21":"code","fabaffc9":"code","599d8261":"code","79f9d1d3":"code","a3106eae":"code","095ed7d2":"markdown","70f2b963":"markdown","c9a3d298":"markdown","d61a8e9d":"markdown","ba473110":"markdown","02a82172":"markdown","11a87695":"markdown","9c8c1052":"markdown","9d86f812":"markdown","43c28c03":"markdown","55adf8b0":"markdown","f7f18e7e":"markdown","c8bbb9cc":"markdown","e7b063f3":"markdown","ec3f6abd":"markdown","7fab0f94":"markdown","ebe49072":"markdown","b6c60711":"markdown","2400e896":"markdown","9cefdbce":"markdown","379b06dc":"markdown","883b1a2d":"markdown","badff094":"markdown","92968b5d":"markdown","dc5a4194":"markdown","7bd9186a":"markdown","51953003":"markdown","360470db":"markdown"},"source":{"8ec00c1a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","108b92ac":"df_apes = pd.read_csv (r'..\/input\/reddit-wallstreetsbets-posts\/reddit_wsb.csv')","f693a2bc":"df_apes","9742860d":"df_apes_1 = df_apes.drop(['created', 'id'], axis=1)","032184a2":"df_apes_1.head()","2b88d694":"# I will look for occurences of the following: [\"Citadel\", \"fucker\", \"fuck\", \"Melvin\", \"apes\", \"moon\", \"short\", \"poor\", \"hype\", \"tendies\", \"diamond hands\", \"paper hands\"]\n\nkeywords = [\"Citadel\", \"fucker\", \"fuck\", \"Melvin\", \"apes\", \"moon\", \"short\", \"poor\", \"hype\", \"tendies\", \"diamond hands\", \"paper hands\", \"gme\", \"amc\", \"nok\", \"doge\"]\n# len(keywords)\n#Change all strings in title to lower-case to search easier\ndf_apes_1['title'].str.lower()","62d9f044":"Citadel = df_apes_1['title'].str.contains('citadel')\nCitadel1 = Citadel.sum()\nprint('Citadel got:',Citadel1,'occurences')\n\nfucker = df_apes_1['title'].str.contains('fucker')\nfucker1 = fucker.sum()\nprint('fucker got:',fucker1,'occurences')\n\nfuck = df_apes_1['title'].str.contains('fuck')\nfuck1 = fuck.sum()\nprint('fuck got:',fuck1,'occurences')\n\nMelvin = df_apes_1['title'].str.contains('melvin')\nMelvin1 = Melvin.sum()\nprint('Melvin got:',Melvin1,'occurences')\n\nApes = df_apes_1['title'].str.contains('apes')\nApes1 = Apes.sum()\nprint('Apes got:',Apes1,'occurences')\n\nApes2 = df_apes_1['title'].str.contains('apes together strong')\nApes3 = Apes2.sum()\nprint('Apes together strong:',Apes3,'occurences')\n\nMoon = df_apes_1['title'].str.contains('moon')\nMoon1 = Moon.sum()\nprint('Moon got:',Moon1,'occurences')\n\nShort = df_apes_1['title'].str.contains('short')\nShort1 = Short.sum()\nprint('Short got:',Short1,'occurences')\n\nPoor = df_apes_1['title'].str.contains('poor')\nPoor1 = Poor.sum()\nprint('Poor got:',Poor1,'occurences')\n\nHype = df_apes_1['title'].str.contains('hype')\nHype1 = Hype.sum()\nprint('Hype got:',Hype1,'occurences')\n\nTendies = df_apes_1['title'].str.contains('Tendies')\nTendies1 = Tendies.sum()\nprint('Tendies got:',Tendies1,'occurences')\n\nDiamondhands = df_apes_1['title'].str.contains('diamond hands')\nDiamondhands1 = Diamondhands.sum()\nprint('Diamond hands got:',Diamondhands1,'occurences')\n\nPaperhands = df_apes_1['title'].str.contains('paper hands')\nPaperhands1 = Paperhands.sum()\nprint('Paper hands got:',Paperhands1,'occurences')\n\nRetard = df_apes_1['title'].str.contains('retard')\nRetard1 = Retard.sum()\nprint('Retard got:',Retard1,'occurences')\n\nAutist = df_apes_1['title'].str.contains('autist')\nAutist1 = Autist.sum()\nprint('Autist got:',Autist1,'occurences')\n\nGme = df_apes_1['title'].str.contains('gme')\nGme1 = Gme.sum()\nprint('Gme got:',Gme1,'occurences')\n\namc = df_apes_1['title'].str.contains('amc')\namc1 = amc.sum()\nprint('amc got:',amc1,'occurences')\n\nnok = df_apes_1['title'].str.contains('nok')\nnok1 = nok.sum()\nprint('nok got:',nok1,'occurences')\n\ndoge = df_apes_1['title'].str.contains('doge')\ndoge1 = doge.sum()\nprint('doge got:',doge1,'occurences')","3b15da08":"title_keyword_sum_list = [Citadel1,fucker1,fuck1,Melvin1,Apes1,Apes3,Moon1,Short1,Poor1,Hype1,Tendies1,Diamondhands1,Paperhands1,Retard1,Autist1, Gme1, amc1,nok1,doge1]\n# sum(title_keyword_sum_list)\n#3789 is sum of the total keywords in titles\n# title_keyword_sum_list","f9f18acc":"data = title_keyword_sum_list\nlabellz = ['Citadel','fucker','fuck','Melvin','Apes','Apes together strong','Moon','Short','Poor','Hype','Tendies','Diamondhands','Paperhands','Retard','Autist', 'GME', 'AMC','NOK','Doge']\nfig = plt.figure(figsize =(15, 15)) \nplt.pie(data, labels = labellz, autopct='%1.0f%%')","5fe31133":"title_keyword_sum_list1 = title_keyword_sum_list\ntitle_keyword_sum_list1.sort()\ntitle_keyword_sum_list1","26a157a8":"df_apes_1['body'].str.lower()","76583303":"Citadel = df_apes_1['body'].str.contains('citadel')\nCitadel1 = Citadel.sum()\nprint('Citadel got:',Citadel1,'occurences')\n\nfucker = df_apes_1['body'].str.contains('fucker')\nfucker1 = fucker.sum()\nprint('fucker got:',fucker1,'occurences')\n\nfuck = df_apes_1['body'].str.contains('fuck')\nfuck1 = fuck.sum()\nprint('fuck got:',fuck1,'occurences')\n\nMelvin = df_apes_1['body'].str.contains('melvin')\nMelvin1 = Melvin.sum()\nprint('Melvin got:',Melvin1,'occurences')\n\nApes = df_apes_1['body'].str.contains('apes')\nApes1 = Apes.sum()\nprint('Apes got:',Apes1,'occurences')\n\nApes2 = df_apes_1['body'].str.contains('apes together strong')\nApes3 = Apes2.sum()\nprint('Apes together strong:',Apes3,'occurences')\n\nMoon = df_apes_1['body'].str.contains('moon')\nMoon1 = Moon.sum()\nprint('Moon got:',Moon1,'occurences')\n\nShort = df_apes_1['body'].str.contains('short')\nShort1 = Short.sum()\nprint('Short got:',Short1,'occurences')\n\nPoor = df_apes_1['body'].str.contains('poor')\nPoor1 = Poor.sum()\nprint('Poor got:',Poor1,'occurences')\n\nHype = df_apes_1['body'].str.contains('hype')\nHype1 = Hype.sum()\nprint('Hype got:',Hype1,'occurences')\n\nTendies = df_apes_1['body'].str.contains('Tendies')\nTendies1 = Tendies.sum()\nprint('Tendies got:',Tendies1,'occurences')\n\nDiamondhands = df_apes_1['body'].str.contains('diamond hands')\nDiamondhands1 = Diamondhands.sum()\nprint('Diamond hands got:',Diamondhands1,'occurences')\n\nPaperhands = df_apes_1['body'].str.contains('paper hands')\nPaperhands1 = Paperhands.sum()\nprint('Paper hands got:',Paperhands1,'occurences')\n\nRetard = df_apes_1['body'].str.contains('retard')\nRetard1 = Retard.sum()\nprint('Retard got:',Retard1,'occurences')\n\nAutist = df_apes_1['body'].str.contains('autist')\nAutist1 = Autist.sum()\nprint('Autist got:',Autist1,'occurences')\n\nGme = df_apes_1['body'].str.contains('gme')\nGme1 = Gme.sum()\nprint('Gme got:',Gme1,'occurences')\n\namc = df_apes_1['body'].str.contains('amc')\namc1 = amc.sum()\nprint('amc got:',amc1,'occurences')\n\nnok = df_apes_1['body'].str.contains('nok')\nnok1 = nok.sum()\nprint('nok got:',nok1,'occurences')\n\ndoge = df_apes_1['body'].str.contains('doge')\ndoge1 = doge.sum()\nprint('doge got:',doge1,'occurences')","13b564c6":"body_keyword_sum_list = [Citadel1,fucker1,fuck1,Melvin1,Apes1,Apes3,Moon1,Short1,Poor1,Hype1,Tendies1,Diamondhands1,Paperhands1,Retard1,Autist1, Gme1,amc1,nok1,doge1]\nsum(body_keyword_sum_list)\n#11214 is sum of the total keywords in titles\nbody_keyword_sum_list","3de1cefc":"data = body_keyword_sum_list\nlabellz = ['Citadel','fucker','fuck','Melvin','Apes','Apes together strong','Moon','Short','Poor','Hype','Tendies','Diamondhands','Paperhands','Retard','Autist', 'GME', 'AMC','NOK','Doge']\nfig = plt.figure(figsize =(15, 15)) \nplt.pie(data, labels = labellz, autopct='%1.0f%%')","7228eeb6":"fourtwenty = df_apes_1['title'].str.contains('420')\nfourtwenty1 = fourtwenty.sum()\nprint('IN TITLE COLUMN - fourtwenty got:',fourtwenty1,'occurences')\n\nsixtynine = df_apes_1['title'].str.contains('69')\nsixtynine1 = sixtynine.sum()\nprint('IN TITLE COLUMN - sixtynine got:',sixtynine1,'occurences')\n\n\n\nfourtwenty2 = df_apes_1['body'].str.contains('420')\nfourtwenty3 = fourtwenty2.sum()\nprint('IN BODY COLUMN - fourtwenty got:',fourtwenty3,'occurences')\n\nsixtynine2 = df_apes_1['body'].str.contains('69')\nsixtynine3 = sixtynine2.sum()\nprint('IN BODY COLUMN - sixtynine got:',sixtynine3,'occurences')\n\n\ntotal420 = fourtwenty1 + fourtwenty3\ntotal69 = sixtynine1 + sixtynine3\nprint('Total ocurrences of 420 are: ', total420)\nprint('Total occurences of  69 are: ', total69)","2ba906d3":"data = [273, 594]\nlabellz = [420, 69]\nfig = plt.figure(figsize =(7, 7)) \nplt.pie(data, labels = labellz, autopct='%1.0f%%')","086b3a25":"yolo2 = df_apes_1['title'].str.contains('YOLO')\nyolo3 = yolo2.sum()\nprint('IN TITLE COLUMN - yolo got:',yolo3,'occurences')\n\nyolo = df_apes_1['body'].str.contains('YOLO')\nyolo1 = yolo.sum()\nprint('IN BODY COLUMN - yolo got:',yolo1,'occurences')\n\ntotalyolo = yolo3 + yolo1\nprint('Total ocurrences of YOLO are: ', totalyolo)","f30fcf99":"Paperhands = df_apes_1['title'].str.contains('paper hands')\nPaperhands1 = Paperhands.sum()\nprint('Paper hands got:',Paperhands1,'occurences')\n\nDiamondhands = df_apes_1['title'].str.contains('diamond hands')\nDiamondhands1 = Diamondhands.sum()\nprint('Diamond hands got:',Diamondhands1,'occurences')\n\nPaperhands2 = df_apes_1['body'].str.contains('paper hands')\nPaperhands3 = Paperhands2.sum()\nprint('Paper hands got:',Paperhands3,'occurences')\n\nDiamondhands2 = df_apes_1['body'].str.contains('diamond hands')\nDiamondhands3 = Diamondhands2.sum()\nprint('Diamond hands got:',Diamondhands3,'occurences')\n\nTOTAL_PAPER_HANDS = Paperhands1 + Paperhands3\nTOTAL_DIAMOND_HANDS = Diamondhands1 + Diamondhands3\n\nprint('Total ocurrences of Paper Hands are: ', TOTAL_PAPER_HANDS)\nprint('Total ocurrences of Diamond Hands are: ', TOTAL_DIAMOND_HANDS)","541a440d":"data = [245, 445]\nlabellz = ['Paper hands', 'Diamond Hands']\nfig = plt.figure(figsize =(7, 7)) \nplt.pie(data, labels = labellz, autopct='%1.0f%%')","7ec320c2":"likedastock2 = df_apes_1['title'].str.contains('like the stock')\nlikedastock3 = likedastock2.sum()\nprint('like the stock :',likedastock3,'occurences')\n\nlikedastock = df_apes_1['body'].str.contains('like the stock')\nlikedastock1 = likedastock.sum()\nprint('like the stock:',likedastock1,'occurences')\n\nTOTAL_STOCK_LIKING_APES = likedastock3 + likedastock1\nprint('Total stock liking apes: ', TOTAL_STOCK_LIKING_APES)","d9b5bf21":"notafinancialadvisor = df_apes_1['title'].str.contains('not a financial advisor')\nnotafinancialadvisor1 = notafinancialadvisor.sum()\nprint('not a financial advisor in body :',notafinancialadvisor1,'occurences')\n\nnotafinancialadvisor2 = df_apes_1['body'].str.contains('not a financial advisor')\nnotafinancialadvisor3 = notafinancialadvisor2.sum()\nprint('not a financial advisor in titles:',notafinancialadvisor3,'occurences')\n\nTOTAL_NON_FINANCIAL_ADVISORS = notafinancialadvisor1 + notafinancialadvisor3\nprint('Total non-financial advisors: ', TOTAL_NON_FINANCIAL_ADVISORS)","fabaffc9":"hold = df_apes_1['title'].str.contains('hold')\nhold1 = hold.sum()\nprint('hold :',hold1,'occurences')\n\nhold2 = df_apes_1['body'].str.contains('hold')\nhold3 = hold2.sum()\nprint('hold:',hold3,'occurences')\n\nTOTAL_BAG_HOLDERS = hold1 + hold3\nprint('Total bag holders: ', TOTAL_BAG_HOLDERS)","599d8261":"num420 = (df_apes_1['score'] == 420)\ndf_apes_1[num420]","79f9d1d3":"pd.set_option('display.max_rows', None)\nnum69 = (df_apes_1['score'] == 69)\ndf_apes_1[num69]","a3106eae":"# num42069 = (df_apes_1['score'] == 42069)\n# df_apes_1[num42069]\n# I got zero rows\n\n# num69420 = (df_apes_1['score'] == 69420)\n# df_apes_1[num69420]\n# I got zero rows","095ed7d2":"## Keyword Analysis in title column","70f2b963":"### Summary of observations: \n1. The top 5 occurences of keywords were basically the same either from the \"title\" column or the \"body\" column\n     * These keywords were: short, fuck, moon, retard, gme\n     * \"Autist\" was an honorable mention, right under GME at 588 occurences in the \"body\" column\n2. Diamond hands were 64% and paper hands were 36%. This means there are 1.7 times more diamond handers than paper handers.\n3. There were only 461 YOLO'ers from a community of 8 million apes.\n     * However, there was a total number of 4885 bag holders. \n4. Out of 32k posts, 3 had \"420\" score and 65 had \"69\" score. \n     * There are NO posts with either \"42069\" or \"69420\" score\n5. When comparing 420 or 69 in both body and title columns:\n     * \"69\" was 69% of the total occurences of both 420 and 69. \n","c9a3d298":"### There are 8 million of you neanderthals in the subreddit and there was only 461 YOLOs? Despicable.","d61a8e9d":"## Closing thoughts:\n\n#### I don't have thoughts anymore. My brain was losing its wrinkles ever since I exposed myself to this data. I lost IQ most definetly from start to end of this analysis. I feel dumb, angry, and I want my time back. \n\n#### With that being said, like an ancient Roman that partied with sugar of lead, I had fun. \n\n","ba473110":"## 420 and 69 analysis\n#### Here we will find out which number trumps the other.","02a82172":"## I take comfort in knowing the time you take to read this is as wasteful as the time it took for me to make this","11a87695":"### Analysis of keywords in \"body\" column\n#### The top 5 occurences of keywords in \"body\" has been:\n1. Short - 2920 occurences\n2. fuck - 2685 occurences\n3. retard - 1630 occurences\n4. moon - 910 occurences\n5. gme - 588 occurences\n\n#### However, honorable mention:\n#### autist - 560 occurences\n","9c8c1052":"## First we analyze the data","9d86f812":"### It looks like 69 is a magical number, not only 69% of 420\/69 occurences is 69, but also 69 comment score is more prevalent than 420 comment score.\n#### Yo dawg...\n![image.png](attachment:image.png)","43c28c03":"## Final Summary and Recommendations\n##### Why are you here? You could've done anything better with your time.","55adf8b0":"### There we go, 4885 bag holders.","f7f18e7e":"### Recommendations\n\n1. There needs to be more 420 score posts. The posts have to get danker. \n2. More people need to become \"diamond hands\" in order to hit 69% of total diamond hands vs paper hands.\n    * About 5% more effort will yield this result\n3. The five top key words can be rearranged into different combinations (with some fillers to assist understanding) which might yield fruitful insights of the sentiments    of the apes in the subreddit. Again, I don't have all the time or energy to find all combinations but these are some that come to mind:\n    * Some examples are: \n        *   (Don't) short GME (you) fuck(ing) retard\n        *    GME (to the) moon\n        *    Short retard(s) fuck GME (to the) moon\n4. We need to take a poll of all bag holders to see the true percentage of bag holders who are YOLO'ers. \n5. Seriously, what the hell are you doing still reading this? Fix your life you degen.\n    ","c8bbb9cc":"## How many are not financial advisors and like the stock?","e7b063f3":"### Upon analysis of the titles with our chosen keywords, we see that the following are top 5 (in order):\n1. Short\n2. Fuck\n3. Moon\n4. Retard\n5. GME","ec3f6abd":"#### Who needs \"for\" loops when I have tendies?","7fab0f94":"## Paper hands vs. Diamond hands analysis","ebe49072":"## \"Hold\" analysis","b6c60711":"## The raw data, like all data, is a turd I must polish. I will make the following decisions to the data to better understand it. Per .info(), we have 32,204 rows.\n1. Remove \"created\" column. It serves no purpose. Probably the creation time of the scraper. \n2. \"id\" column serves no purpose because len(df_apes_1.id.unique()) = 32,204. Removing \"id\" column","2400e896":"## YOLO analysis + observations ","9cefdbce":"## Keyword analysis in \"body\" column.","379b06dc":"## Very interesting observation of 420 vs. 69\n\n#### It is a damn miracle that 69% of the total occurences of either 420 or 69 was 69. \n#### 69 wins this contest hands down in more ways than just numerical. The universe itself seems to be a degenerate as well. This dataset is an amalgam of degeneracy manifested.","883b1a2d":"## Analysis of 420 and 69 in scores","badff094":"### Future Work\n\n1. Re-run all this analysis on 1 yr of scraped WSB data. Also, try from the beginning of WSB's creation.\n2. Create a ML bot that posts statements after going through every WSB post\/body content\n3. Scrape and analyze all memes to see which ones survived time in the WSB jungle.\n4. Create a bot that paper-trades (or real trades) based on the trade history of WSB users\n5. Compare scrotum sizes of \"diamond hands\" vs. \"paper hands\"","92968b5d":"## 0 score analysis and top score analysis\n##### I'm going to skip this because I don't care. Let me clarify. I don't care because it doesn't matter what the score is. Every single poster in that subreddit is a degenerate. Doesn't matter if it is 0 score or 42069 score...........hmmmmm....now im curious","dc5a4194":"### As we see above:\n1. There were 3 posts with 420 comment score.\n2. There were 65 posts with 69 comment score. \n3. There were 0 posts with 42069 or 69420 score","7bd9186a":"### Upon analysis of paper hands vs diamond hands, we see that Diamond hands are in the lead. \n#### My only sadness here is that Diamond hands percent is not 69%","51953003":"### Brainstorming the important questions:\n\n1. How many times was \"apes together strong\" used?\n2. How many times is \"retard\" used?\n3. How many mentions are there of GME, AMC, NOK?\n4. TO THE MOON?\n5. Dogecoin analysis\n6. 420 and 69 analysis\n7. How many times is \"autist\" mentioned?\n8. Is this dataset large enough?\n9. Key words to search: \"Citadel\", \"fucker\", \"SEC\", \"short\", \"poor\", \"hype\", \"food stamps\", \"Melvin\"\n10. 0 score analysis\n11. top score analysis\n12. How much more \"retarded\" did the posts get over time? (Will have to re-arrange timestamps to show linear time.)\n13. Diamond hands vs paper hands analysis\n14. How many non-financial advisors are there who like the stock? (\"I am not a financial advisor\" & \"I like the stock\")\n15. Hold the line - how many occurences of \"Hold\" was there? Over time?\n16. YOLO analysis","360470db":"### I'm too lazy to figure out the intersection of these two data. However, the number is still surprisingly low amongst the posts. "}}