{"cell_type":{"82675249":"code","3a43e40b":"code","2ff9d217":"code","2d2546de":"code","bb2af20d":"code","afc732da":"code","7e844ba0":"code","d2de1cb0":"code","75637191":"code","48ce139a":"code","104f8937":"code","6c52a58c":"markdown","04c4e5f9":"markdown","229e306e":"markdown","f413e2a8":"markdown","777834cb":"markdown","bfa91caf":"markdown"},"source":{"82675249":"!pip install wordcloud\n\n\nfrom wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nprint('Import Successfull')","3a43e40b":"# Reads Merge dataset file\ndf = pd.read_csv(\"..\/input\/tweetsdataset\/merge.csv\", encoding =\"latin-1\")\nprint(df.head)","2ff9d217":"import warnings\nimport re\ndf = df.drop_duplicates()\nwarnings.filterwarnings('ignore')\ndf['Content2'] = df['Content'].str.replace('\\W', ' ')\ndf['Content2'] = df['Content'].str.replace(r'[^\\x00-\\x7f]', '')\nprint(df.describe())","2d2546de":"comment_words = ''\nstopwords = set(STOPWORDS)\n \n# iterate through the csv file\nfor val in df['Content2']:\n     \n    # typecaste each val to string\n    val = str(val)\n \n    # split the value\n    tokens = val.split()\n     \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n     \n    comment_words += \" \".join(tokens)+\" \"\n \nwordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 10).generate(comment_words)\n \n# plot the WordCloud image                      \nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n \nplt.show()","bb2af20d":"import warnings\nimport re\nwarnings.filterwarnings('ignore')\ndf['City2'] = df['City'].str.replace('\\W', ' ')\ndf['City2'] = df['City'].str.replace(r'[^\\x00-\\x7f]', '')","afc732da":"comment_words = ''\nstopwords = set(STOPWORDS)\n \n# iterate through the csv file\nfor val in df['City2']:\n     \n    # typecaste each val to string\n    val = str(val)\n \n    # split the value\n    tokens = val.split()\n     \n    # Converts each token into lowercase\n    for i in range(len(tokens)):\n        tokens[i] = tokens[i].lower()\n        \n    if(len(tokens)>0 and tokens[0]!='nan'):\n        comment_words += \" \".join(tokens)+\" \"\n \nwordcloud = WordCloud(width = 800, height = 800,\n                background_color ='white',\n                stopwords = stopwords,\n                min_font_size = 10).generate(comment_words)\n \n# plot the WordCloud image                      \nplt.figure(figsize = (8, 8), facecolor = None)\nplt.imshow(wordcloud)\nplt.axis(\"off\")\nplt.tight_layout(pad = 0)\n \nplt.show()","7e844ba0":"import nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.tokenize import TweetTokenizer\ntknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\nsid = SentimentIntensityAnalyzer()","d2de1cb0":"# Gddiven a list of words, return a dictionary of\n# word-frequency pairs.\n\ndef wordListToFreqDict(wordlist):\n    wordfreq = [wordlist.count(p) for p in wordlist]\n    return dict(list(zip(wordlist,wordfreq)))","75637191":"# Sort a dictionary of word-frequency pairs in\n# order of descending frequency.\n\ndef sortFreqDict(freqdict):\n    aux = [(freqdict[key], key) for key in freqdict]\n    aux.sort()\n    aux.reverse()\n    return aux","48ce139a":"wordlist = []\nfor index, row in df.iterrows():\n    tokenize_string = tknzr.tokenize(str(row['Content2']))\n    for word in tokenize_string:\n        if (sid.polarity_scores(word)['compound']) <= -0.65:\n            wordlist.append(word)  \n            \ndictionary = wordListToFreqDict(wordlist)\nsorteddict = sortFreqDict(dictionary)\n\nfor s in sorteddict: print(str(s))","104f8937":"import nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.tokenize import TweetTokenizer\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ntknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n\nsid = SentimentIntensityAnalyzer()\npos_word_list=[]\nneu_word_list=[]\nneg_word_list=[]\n\nfor index, row in df.head(10000).iterrows(): # Use only 10,000 record (if taking too long for all record use this)\n# for index, row in df.iterrows(): # Use all record, uncomment this line and comment out above one\n    tokenize_string = tknzr.tokenize(str(row['Content']))\n    for word in tokenize_string:\n        if (sid.polarity_scores(word)['compound']) >= 0.5:\n            pos_word_list.append(word)\n        elif (sid.polarity_scores(word)['compound']) <= -0.5:\n            neg_word_list.append(word)             \n\nprint('Positive Words Count:', len(pos_word_list))\nprint('Negative Words Count:', len(neg_word_list))  \n\n\ny = np.array([len(pos_word_list), len(neg_word_list)])\nmylabels = [\"Positive\", \"Negative\"]\n\nplt.pie(y, labels = mylabels)\nplt.show() ","6c52a58c":"# Positive \/ Negative Words","04c4e5f9":"# Data Cleaning","229e306e":"# Creating Word Cloud","f413e2a8":"# Identifying Abuse Word Frequencies","777834cb":"# City Usage Represented by WordCloud","bfa91caf":"# **Reading Merged Dataset**"}}