{"cell_type":{"53d99ae1":"code","786d9d7f":"code","2f710efc":"code","456788f7":"code","69845877":"code","9d6bea32":"code","62231d91":"code","0fb2673e":"code","60f81900":"code","7e464466":"code","d90dc7d3":"code","c5d34519":"code","1603adca":"code","36feb9fa":"code","f952229f":"code","edb1a7cc":"code","ec7cfb89":"code","c41f60fe":"code","77ea40aa":"code","31dea600":"code","3e23a584":"code","8f492076":"code","69a25a13":"code","f3ca8469":"code","6013ac6f":"code","583c83b3":"code","3c4323df":"code","9f61eee3":"code","cf788c36":"code","3b2881d6":"code","c24222ab":"code","caecb6d1":"markdown","69f3b7d2":"markdown","ac8644c2":"markdown","ae3e4b3b":"markdown","3637cd37":"markdown","77507517":"markdown","458b9caf":"markdown","199e0423":"markdown","8cbfdf4c":"markdown","da42651e":"markdown","c41c5071":"markdown","94ecbea9":"markdown","9a731fbf":"markdown","9dff0a84":"markdown","c571a139":"markdown","c6561b9b":"markdown","6fe0c7a4":"markdown","c0002611":"markdown","8ff215f2":"markdown","5650d7f1":"markdown","877571fe":"markdown","0ac019f5":"markdown","f2726194":"markdown"},"source":{"53d99ae1":"# adding wordcloud in offline mode\n\nimport sys\nsys.path.append('..\/input\/wcloud\/word_cloud-master')\nsys.path.append('..\/input\/sentence-transformers\/sentence-transformers-master')\nimport wordcloud","786d9d7f":"# loading libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom scipy import stats\nfrom scipy.stats import norm\n\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\n# setting some globl config\n\nplt.style.use('ggplot')\norange_black = [\n    '#fdc029', '#df861d', '#FF6347', '#aa3d01', '#a30e15', '#800000', '#171820'\n]\nplt.rcParams['figure.figsize'] = (16,9)\nplt.rcParams[\"figure.facecolor\"] = '#FFFACD'\nplt.rcParams[\"axes.facecolor\"] = '#FFFFE0'\nplt.rcParams[\"axes.grid\"] = True\nplt.rcParams[\"grid.color\"] = orange_black[3]\nplt.rcParams[\"grid.alpha\"] = 0.5\nplt.rcParams[\"grid.linestyle\"] = '--'\n\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","2f710efc":"# loding train and test data\n\ntrain_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest_df = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')","456788f7":"# checking dataframes and their shape\n\nprint('\\033[1m'+'TRAIN COLUMNS:')\ndisplay(train_df.columns)\nprint('\\033[1m''\\033[1m''TEST COLUMNS:')\ndisplay(test_df.columns)\nprint('\\033[1m'+'TRAIN SHAPE:')\ndisplay(train_df.shape)\nprint('\\033[1m'+'TEST SHAPE:')\ndisplay(test_df.shape)","69845877":"def plot_dist3(df, feature, title):\n    \n    # Creating a customized chart. and giving in figsize and everything.\n    \n    fig = plt.figure(constrained_layout=True)\n    \n    # creating a grid of 3 cols and 3 rows.\n    \n    grid = gridspec.GridSpec(ncols=3, nrows=2, figure=fig)\n\n    # Customizing the histogram grid.\n    \n    ax1 = fig.add_subplot(grid[0, :2])\n    \n    # Set the title.\n    \n    ax1.set_title('Histogram')\n    \n    # plot the histogram.\n    \n    sns.distplot(df.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 fit=norm,\n                  hist_kws={\n                 'rwidth': 0.85,\n                 'edgecolor': 'black',\n                 'alpha': 0.8},\n                 ax=ax1,\n                 color=orange_black[1])\n    \n    ax1.axvline(df.loc[:, feature].mean(), color='Green', linestyle='dashed', linewidth=3)\n\n    min_ylim, max_ylim = plt.ylim()\n    ax1.text(df.loc[:, feature].mean()*1.95, max_ylim*0.95, 'Mean: {:.2f}'.format(df.loc[:, feature].mean()), color='Green', fontsize='12',\n             bbox=dict(boxstyle='round',facecolor='red', alpha=0.5))\n    ax1.legend(labels=['Actual','Normal'])\n    ax1.xaxis.set_major_locator(MaxNLocator(nbins=24))\n\n    # customizing the QQ_plot.\n    \n    ax2 = fig.add_subplot(grid[1, :2])\n    \n    # Set the title.\n    \n    ax2.set_title('Probability Plot')\n    \n    # Plotting the QQ_Plot.\n    stats.probplot(df.loc[:, feature].fillna(np.mean(df.loc[:, feature])),\n                   plot=ax2)\n    ax2.get_lines()[0].set_markerfacecolor('#e74c3c')\n    ax2.get_lines()[0].set_markersize(12.0)\n    ax2.xaxis.set_major_locator(MaxNLocator(nbins=24))\n\n    # Customizing the Box Plot:\n    \n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    \n    ax3.set_title('Box Plot')\n    \n    # Plotting the box plot.\n    \n    sns.boxplot(y=feature, data=df, ax=ax3, color=orange_black[2])\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=24))\n\n    plt.suptitle(f'{title}', fontsize=24)","9d6bea32":"plot_dist3(train_df, 'target', 'Readability Score Distribution')","62231d91":"# checking df again\ntrain_df","0fb2673e":"# loading missingno package and plotting nan values\n\nimport missingno as msno\n\nmsno.bar(train_df, color=(orange_black[1]))\n\nplt.title('Missing Values', fontsize=24)\n\n\nplt.show()","60f81900":"# dropping some columns\n\ntrain_df=train_df[['id','excerpt','target','standard_error']]","7e464466":"# Creating a new feature for the visualization.\n\ntrain_df['Character Count'] = train_df['excerpt'].apply(lambda x: len(str(x)))\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as mpatches\n\ndef plot_dist4(df, feature, title, color):\n    # Creating a customized chart. and giving in figsize and everything.\n    fig = plt.figure(constrained_layout=True, figsize=(18, 8))\n    # Creating a grid of 3 cols and 3 rows.\n    grid = gridspec.GridSpec(ncols=3, nrows=2, figure=fig)\n\n    # Customizing the histogram grid.\n    ax1 = fig.add_subplot(grid[0, :2])\n    # Set the title.\n    ax1.set_title('Histogram')\n    # plot the histogram.\n    sns.histplot(df.loc[:, feature],\n                 ax=ax1,\n                 color=color)\n    ax1.set_ylim([0, 275])\n    ax1.axvline(df.loc[:, feature].mean(), color=orange_black[1], linestyle='dashed', linewidth=3)\n\n    min_ylim, max_ylim = plt.ylim()\n    ax1.text(df.loc[:, feature].mean()*0.9, max_ylim*0.9, 'Mean: {:.2f}'.format(df.loc[:, feature].mean()), color='Green', fontsize='12',\n             bbox=dict(boxstyle='round',facecolor=orange_black[1]))\n    \n    ax1.set(ylabel='Frequency')\n    ax1.xaxis.set_major_locator(MaxNLocator(nbins=24))\n\n    # Customizing the ecdf_plot.\n    ax2 = fig.add_subplot(grid[1, :2])\n    # Set the title.\n    ax2.set_title('Empirical CDF')\n    # Plotting the ecdf_Plot.\n    sns.histplot(df.loc[:, feature],\n                 ax=ax2,\n                 cumulative=True,\n                 color=orange_black[4])\n    ax2.xaxis.set_major_locator(MaxNLocator(nbins=20))\n    ax2.set(ylabel='Cumulative')\n\n    # Customizing the Box Plot.\n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    ax3.set_title('Box Plot')\n    # Plotting the box plot.\n    sns.boxplot(y=feature, data=df, orient='v', ax=ax3, color=orange_black[1])\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=25))\n\n    plt.suptitle(f'{title}', fontsize=24)","d90dc7d3":"# plotting character distribution\nplot_dist4(train_df, 'Character Count',\n           'Character Distribution ', orange_black[6])","c5d34519":"# regplot for character counts\n\nsns.regplot(x=train_df['Character Count'], y=train_df.target, order=1,\n                    \n                    color=orange_black[2],\n                    line_kws={'color': orange_black[6]},\n                    scatter_kws={'alpha':0.6})\nplt.title('Character Counts vs Target', fontsize=24)\n\nplt.show()","1603adca":"def plot_word_number_histogram(text):\n    \n    \n    \"\"\"A function for displaying word distribuition\"\"\"\n    \n\n    fig, axes = plt.subplots(figsize=(18, 6))\n    sns.histplot(text.str.split().map(lambda x: len(x)), color=orange_black[3],stat='density', bins=50)\n    \n    \n    plt.xlabel('Word Count')\n    plt.ylabel('Frequency')    \n    fig.suptitle('Word Counts', fontsize=24, va='baseline')\n    plt.xticks(np.arange(140, 210, 5))\n    \n    # plotting\n    \n    fig.tight_layout()\n\n\n\nplot_word_number_histogram(train_df.excerpt)","36feb9fa":"# regplot for word counts\n\ntrain_df['Word Count']=train_df.excerpt.str.split().map(lambda x: len(x))\nsns.regplot(x=train_df['Word Count'], y=train_df.target, order=1,\n                    \n                    color=orange_black[2],\n                    line_kws={'color': orange_black[6]},\n                    scatter_kws={'alpha':0.4})\nplt.title('Word Counts vs Target', fontsize=24)\n\n# plotting\n\nplt.show()","f952229f":"def plot_word_len_histogram(text):\n    \n    \"\"\"A function for comparing average word length\"\"\"\n    \n    fig, axes = plt.subplots(figsize=(18, 6))\n    sns.histplot(text.str.split().apply(lambda x: [len(i) for i in x]).map(\n        lambda x: np.mean(x)),\n                 color=orange_black[4])   \n\n    \n    fig.suptitle('Mean Word Lengths', fontsize=24, va='baseline')\n    \n    # plotting\n    \n    fig.tight_layout()","edb1a7cc":"# plotting word len hist\n\nplot_word_len_histogram(train_df.excerpt)","ec7cfb89":"# regplot for word lengths\n\ntrain_df['Mean Word Length'] = train_df.excerpt.str.split().apply(lambda x: [len(i) for i in x]).map(\n        lambda x: np.mean(x))\n\nsns.regplot(x=train_df['Mean Word Length'], y=train_df.target, order=1,\n                    \n                    color=orange_black[2],\n                    line_kws={'color': orange_black[6]},\n                    scatter_kws={'alpha':0.4})\nplt.title('Mean Word Lenghts vs Target', fontsize=24)\n\n# plotting\n\nplt.show()","c41f60fe":"# regplot for word lengths\n\n\ntrain_df['Number of Digits'] =  train_df['excerpt'].apply(lambda s: sum(c.isdigit() for c in s))\n\nsns.regplot(x=train_df['Number of Digits'], y=train_df['target'], order=1,\n                    color=orange_black[2],\n                    line_kws={'color': orange_black[6]},\n                    scatter_kws={'alpha':0.4})\nplt.title('Number of Digits vs Target', fontsize=24)\n\n# plotting\n\nplt.show()","77ea40aa":"# creating corpus\n\nfrom nltk import FreqDist\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\n\n# setting stopwords \n\nstopwords = set(stopwords.words('english'))\n\nnew = train_df['excerpt'].str.split()\nnew = new.values.tolist()\ncorpus = [word.lower() for i in new for word in i if word.lower() not in stopwords]\n          \n#getting top 20 common words\n    \ncommon_words = [i[0] for i in FreqDist(corpus).most_common(20)]\ncommon_words_count = [i[1] for i in FreqDist(corpus).most_common(20)]","31dea600":"# plotting unigrams\n\nfig, ax = plt.subplots(figsize=(18, 8))\nsns.barplot(x=common_words_count, y=common_words, palette='cividis')\nfor n, i in enumerate(common_words):    \n    ax.text(common_words_count[n]-0.003, \n            n, #Y location\n            s=f'{round(common_words_count[n],3)}', \n            va='center', \n            ha='right', \n            color='white', \n            fontsize=8,\n            bbox=dict(boxstyle='round',facecolor='red', alpha=0.5))\nplt.title('Most Common Unigrams', fontsize=24)\nsns.despine()\nplt.xticks([])\nfig.patch.set_facecolor('#FFFACD')\nax.patch.set_facecolor('#FFFFE0')\n\nplt.show()","3e23a584":"from sklearn.feature_extraction.text import CountVectorizer\ndef ngrams(n, title, loc):\n    \"\"\"A Function to plot most common ngrams\"\"\"\n    fig, ax = plt.subplots(figsize=(18, 8))\n\n    new = train_df.excerpt.str.split()\n    new = new.values.tolist()\n    corpus = [word.lower() for i in new for word in i if word.lower()]\n\n    def _get_top_ngram(corpus, n=None):\n        #getting top ngrams\n        vec = CountVectorizer(ngram_range=(n, n),\n                              max_df=0.9,\n                              stop_words='english').fit(corpus)\n        bag_of_words = vec.transform(corpus)\n        sum_words = bag_of_words.sum(axis=0)\n        words_freq = [(word, sum_words[0, idx])\n                      for word, idx in vec.vocabulary_.items()]\n        words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n        return words_freq[:15]\n\n    top_n_bigrams = _get_top_ngram(train_df.excerpt, n)[:15]\n    x, y = map(list, zip(*top_n_bigrams))\n    sns.barplot(x=y, y=x, palette='cividis')  \n    \n    \n    for n, i in enumerate(x):    \n            ax.text(y[n]-loc, \n            n, #Y location\n            s=f'{round(y[n],3)}', \n            va='center', \n            ha='right', \n            color='white', \n            fontsize=8,\n            bbox=dict(boxstyle='round',facecolor='red', alpha=0.5))\n\n    fig.suptitle(title, fontsize=24, va='baseline')\n    sns.despine()\n    plt.xticks([])\n    fig.patch.set_facecolor('#FFFACD')\n    ax.patch.set_facecolor('#FFFFE0')\n    plt.show()","8f492076":"# plotting bigrams\nngrams(2, 'Most Common Bigrams', 0.8)","69a25a13":"# plotting trigrams\nngrams(3, 'Most Common Trigrams', 0.15)","f3ca8469":"# make worldcloud\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom PIL import Image\n\n# getting tokens\ntexts = \" \".join(token for token in corpus)\n# setting stopwords\nstopwords_wc = set(stopwords)\n# loading custom font\nfont_path = \"..\/input\/wcloud\/acetone_font.otf\"\n\n# generating wordcloud\nwordcloud = WordCloud(stopwords=stopwords_wc, font_path=font_path,\n                      max_words=1500,\n                      max_font_size=350, random_state=42,\n                      width=2000, height=1000,\n                      colormap = \"gist_stern\")\nwordcloud.generate(texts)\n\n# plotting\nplt.figure(figsize = (24, 13))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","6013ac6f":"# loading model using sentence transformers\n\nimport sentence_transformers\nfrom sentence_transformers import SentenceTransformer, models\n\n# setting model path for fine-tuned roberta weights\n\nmodel_path = '..\/input\/finetuned-model1\/checkpoint-568'\nword_embedding_model = models.Transformer(model_path, max_seq_length=275)\npooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\nmodel = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n","583c83b3":"# encoding train and test strings\n\nX_train = model.encode(train_df.excerpt, device='cuda')\nX_test = model.encode(test_df.excerpt, device='cuda')","3c4323df":"# checking shapes of embeddings\n\ndisplay(X_train.shape)\ndisplay(X_test.shape)","9f61eee3":"from sklearn.model_selection import StratifiedKFold\nfrom datetime import datetime\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import BayesianRidge\n\npreds = []\ntrain_scores = []\n\ndf_oof=train_df.copy()\ndf_oof['oof'] = 0\n\nskf = StratifiedKFold(10, shuffle=True, random_state=42)\n\nsplits = list(skf.split(X=X_train, y=train_df['Character Count']))\n\n# predicting out of fold scores for each fold and doing predictions for each training set\n\nfor i, (train_idx, val_idx) in enumerate(splits):\n    print(f'\\n------------- Training Fold {i + 1} \/ {10}')\n    print(\"Current Time =\", datetime.now().strftime(\"%H:%M:%S\"))\n\n    clf = BayesianRidge(n_iter=300, verbose=True)\n    clf.fit(X_train[train_idx],train_df.target[train_idx])\n    train_score=mean_squared_error(train_df.target[train_idx], clf.predict(X_train[train_idx]), squared=False)\n    train_scores.append(train_score)\n    print(f\"Fold {i} train RMSE: {train_score}\")\n    \n    \n    preds.append(clf.predict(X_test))\n    x=clf.predict(X_train[val_idx])\n    df_oof['oof'].iloc[val_idx]+= x\n\nprint(f'Training score: {np.mean(train_scores)}, Training STD: {np.std(train_scores)}')\nprint(f'OOF score across folds: {mean_squared_error(df_oof.target, df_oof.oof, squared=False)}')","cf788c36":"# getting mean prediction across 5 folds\ny_pred = np.mean(preds,0)\ny_pred.shape","3b2881d6":"# creating submission csv\n\nsub = test_df[[\"id\"]].copy()\nsub[\"target\"] = y_pred\nsub.to_csv('submission.csv', index = False)","c24222ab":"# checking submission file\n\nsub.head()","caecb6d1":"# Starter Code & EDA, Roberta Sentence Embeddings with Bayesian Model\n\n### Hello all! In this notebook we're going to examine the data we're given, gonna do some exploratory data analysis and visualizations, aiming to get some insights and find some patterns about the data.\n\n### In second part we're going to implement some simple baseline model to make a starting point for the competition.\n\n### Well that's enough of me talking, let's get started:","69f3b7d2":"# Meta Features\n\n### These simple features extracted directly from strings themselves. We can't be sure if they mean anything at all unless we examine them so let's take a look:","ac8644c2":"# Common N-Grams\n\n### Here we check some common words. They're not directly giving us info but they can help us summarizing what kind of texts we have here and understand them.","ae3e4b3b":"# Cross Validation\n\n### As I said we used really simple regressor to get results, I haven't used our extra features, neither optimized the model parameters. I think it's possible to get better score by using more advanced models with optimized parameters but for now we'll leave it like that. But for Bayesian Ridge we got decent starting point! I stratified the folds across the character counts which I believe  it gives more balanced stratification...","3637cd37":"# Submission","77507517":"## Number of Digits\n\n### We can observe that having lots of digits slightly decreasing readability score...","458b9caf":"# Target Distribution\n\n### Let's start with what kind of a problem we have given and what metrics we can use. As you can see below we have continous target labels so we have \"Regression\" problem. When we check the distribution of targets we do see kinda normal distribution, which is good.","199e0423":"# Loading Libraries\n\n### Here we load some packages which we're going to need them in future cells. ","8cbfdf4c":"# Getting Sentence Embeddings\n\n### We're given small number of training instances. With huge models especially neural networks we're always in risk of overfitting. For this and baseline purposes I implemented a simple sentence embedding model based on fine tuned roberta. After getting embedding matrix we are going to train that using Bayesian Ridge Regression...","da42651e":"# Missing Values\n\n### We have many missing values for url's and licences features therefore they're useless for now and I think dropping them would be good choice.","c41c5071":"## Trigrams","94ecbea9":"# Wordcloud\n\n## Again most common words in more visual format, not giving us extra feature but looks cool right? :)","9a731fbf":"# Loading Data\n\n### We load our train and test data. We do have small dataset with few amount of features. Most important one seems to be excerpt which is the feature that covers most of the useful data. Since this is \"Code Competition\" our test set seems to be small but it's going to load actual test set when we commit the notebook.","9dff0a84":"## Vectorizing Texts\n\n### First we're going to vectorize our sentences to get numerical representations so we can feed them into our traditional machine learning algorithms...","c571a139":"# Yeah... Doesn't look much meaningful to me, there's slight decrease in scores when the word count increses. Well let's check other meta features then...","c6561b9b":"## Word Counts\n\n### Word Counts distribution looks more evenly (not normally!) not sure what it means for now, let's take a closer look with scatterplot...","6fe0c7a4":"## Unigrams","c0002611":"### Oh that's interesting... There's strong linear relation between mean word lenghts and readability. Looks like longer\/complicated words decreases the score.","8ff215f2":"## Word Lenghts\n\n### Does length of the words affect readability score? Let's check how's the distribution: It looks like decently spread with little bit of right skew...","5650d7f1":"# Results\n\n### Not a bad score for such a simple approach! More importantly I got decent CV\/LB correlation. It's still early parts of the competition and I believe we gonna get more complicated models on the course but I thik this is decent starting point for starters. I hope you find it useful, **if you do please dont forget to upvote!** Also feel free to ask if you have any questions in comments section.\n\n### Happy coding!","877571fe":"### When we plot character counts we can see some linear relation there. With increasing number of character counts our target score decreases, interesting...","0ac019f5":"## Bigrams","f2726194":"## Character Counts\n\n### For the distribution part we have nicely spread data points. With the median character count around 970..."}}