{"cell_type":{"b52a54e7":"code","c5c99ce2":"code","f47fe142":"code","24f97eec":"code","1fddf923":"code","5b7881e8":"code","98ff1862":"code","46cb3dd2":"code","54cb6175":"code","65eeddf4":"code","6daf1e30":"code","4dd7bca9":"code","71a8dba3":"code","8dc2e450":"code","308b2f0d":"code","4a66b3a8":"code","5ea1e9d2":"code","9ec3663f":"code","a0b5adef":"markdown"},"source":{"b52a54e7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import classification_report, accuracy_score\nfrom sklearn import preprocessing \nimport matplotlib.pyplot as plt\nimport pandas as pd \nimport numpy as np\nimport pandas as pd \nimport os\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c5c99ce2":"path = '\/kaggle\/input\/tabular-playground-series-sep-2021\/'\ntrainpath = os.path.join(path,'train.csv')\ntestpath = os.path.join(path,'test.csv')\nX_test = pd.read_csv(testpath)\ntestid = X_test['id']\ndf_train = pd.read_csv(trainpath)","f47fe142":"features = df_train.columns[:-1]\ntarget = df_train.columns[-1]\ndf_train[\"Missing_Value\"] = df_train[features].isna().sum(axis=1) \ndf_train[\"Missing_Value_cat\"] = df_train[\"Missing_Value\"].map(lambda x: 0 if x==0 else 1)\nX_test[\"Missing_Value\"] = X_test[features].isna().sum(axis=1) \nX_test[\"Missing_Value_cat\"] = X_test[\"Missing_Value\"].map(lambda x: 0 if x==0 else 1)","24f97eec":"for column in features:\n    avg_val = df_train[column].median()\n    df_train[column].fillna(avg_val, inplace=True)\n    X_test[column].fillna(avg_val, inplace=True)\nfeatures = list(features)\nfeatures.extend(['Missing_Value','Missing_Value_cat'])","1fddf923":"x = df_train.drop([\"claim\"], axis=1)\ny = df_train[\"claim\"].values\n\nx_scale = preprocessing.MinMaxScaler().fit_transform(x.values)\nx_norm, x_claim = x_scale[y == 0], x_scale[y == 1]","5b7881e8":"from tensorflow.keras.layers import Input, Dense\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras import regularizers","98ff1862":"## input layer \ninput_layer = Input(shape=(x.shape[1],))\n\n## encoding part\nencoded = Dense(512, activation='tanh', activity_regularizer=regularizers.l1(10e-5))(input_layer)\nencoded = Dense(256, activation='relu')(encoded)\nencoded = Dense(128, activation='relu')(encoded)\nencoded = Dense(64, activation='relu')(encoded)\n\ndecoded = Dense(64, activation= 'tanh')(encoded)\ndecoded = Dense(128, activation='tanh')(decoded)\ndecoded = Dense(256, activation='tanh')(decoded)\ndecoded = Dense(512, activation='tanh')(decoded)\n\n\n## output layer\noutput_layer = Dense(x.shape[1], activation='relu')(decoded)","46cb3dd2":"es_callback = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    min_delta=0.0001,\n    patience=5,\n    verbose=0,\n    mode=\"auto\",\n    baseline=None,\n    restore_best_weights=True,\n)\n","54cb6175":"autoencoder = Model(input_layer, output_layer)\nautoencoder.compile(optimizer=\"adam\", loss=tf.keras.losses.MeanSquaredLogarithmicError(\n    reduction=\"auto\", name=\"mean_squared_logarithmic_error\"\n))","65eeddf4":"autoencoder.fit(x_norm, x_norm, \n                batch_size = 512, epochs = 30, \n                shuffle = True, validation_split = 0.20,callbacks=[es_callback]);","6daf1e30":"hidden_representation = Sequential()\nn=5\nfor i in range(n):\n    hidden_representation.add(autoencoder.layers[i])  ","4dd7bca9":"norm_hid_rep = hidden_representation.predict(x_norm)\nclaim_hid_rep = hidden_representation.predict(x_claim)\nrep_x = np.append(norm_hid_rep, claim_hid_rep, axis = 0)\ny_n = np.zeros(norm_hid_rep.shape[0])\ny_f = np.ones(claim_hid_rep.shape[0])\nrep_y = np.append(y_n, y_f)","71a8dba3":"from xgboost import XGBClassifier\n\n\nmodel = XGBClassifier(\n    max_depth=5,\n    subsample=0.5,\n    colsample_bytree=0.5,\n    n_jobs=-1,\n    random_state=0,\n)","8dc2e450":"train_x, val_x, train_y, val_y = train_test_split(rep_x, rep_y, test_size=0.25)\nclf = model.fit(train_x, train_y)\npred_y = clf.predict(val_x)","308b2f0d":"print (\"\")\nprint (\"Classification Report: \")\nprint (classification_report(val_y, pred_y))\n\nprint (\"\")\nprint (\"Accuracy Score: \", accuracy_score(val_y, pred_y))","4a66b3a8":"X_test = preprocessing.MinMaxScaler().fit_transform(X_test)\nX_test = hidden_representation.predict(X_test)","5ea1e9d2":"\n# Make predictions\ny_pred_test = pd.Series(model.predict(X_test))\ny_pred_test = pd.DataFrame(y_pred_test,columns=['claim'])\ny_pred_test['id'] = testid\ny_pred_test = y_pred_test[['id','claim']]\n# Create submission file\ny_pred_test.to_csv(\"submission.csv\",index=False)","9ec3663f":"from collections import Counter\ny_counter = Counter(y_pred_test['claim'])\nprint(y_counter)","a0b5adef":"### introduction\n\nfrom idea of https:\/\/www.kaggle.com\/shivamb\/semi-supervised-classification-using-autoencoders\nI tried to find latent representation on this topic, and it seemed quite legit for a while... until I found out it makes similar score if we just put \nfeatures into XGboost... \nAlso thanks to https:\/\/www.kaggle.com\/pratikkgandhi\/rapids-rf-classifier-on-tps-sept-0-80476-lb,\nI used idea of making nan into median value of the rows, and making the 'nan count' features...\n\nit seems not many people used the autoencoder. \nDoes anybody knows why? plz let me know..."}}