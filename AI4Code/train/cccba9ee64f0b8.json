{"cell_type":{"321203d3":"code","821d62a5":"code","a5402b01":"code","d5c418de":"code","ac6e84ec":"code","0142dda5":"code","51242b90":"code","6b76edf2":"code","f979fbd7":"code","065151b7":"code","d4d0c27f":"code","5faa9a5e":"code","58aa1629":"code","a3486b32":"code","d4e4f2eb":"code","93037a1a":"code","ff1a6a3d":"code","feb2bb75":"code","b9240333":"code","72c6f84f":"code","263cfd95":"code","78ec1d4c":"code","4e01db65":"code","25909020":"code","6bad1532":"code","b1fe38fe":"code","c3a03d45":"code","c2cea12f":"code","7f618c9c":"code","6bc32f18":"code","f8f4e00b":"code","f153b14e":"code","bdafda3d":"code","398a18e3":"code","55feedef":"code","676c0e9a":"code","2317001f":"code","0accc927":"code","1bb763f5":"code","b8a8ae2f":"code","5c9dba9a":"code","e0d5002d":"code","39e7ba2d":"code","faaf0444":"code","726fda5c":"markdown","0130d9c1":"markdown","f6d80cca":"markdown","526c14b6":"markdown","44129d66":"markdown","cc890fb2":"markdown","d8e8e42d":"markdown","0b62f28c":"markdown","59b55104":"markdown","a82d9e23":"markdown","1224f021":"markdown","3015b25f":"markdown","0621ff48":"markdown","3fd19f5f":"markdown","b25ec152":"markdown"},"source":{"321203d3":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport nltk\nimport string\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nos.chdir(\"\/kaggle\/input\/amazon-reviews\/\")\nfrom dataset import *\nfrom utils import *\nos.chdir(\"\/kaggle\/working\/\")\n\n# Modules for handling text data\nfrom sklearn.feature_extraction.text import TfidfTransformer , TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer \nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer \nfrom nltk.stem.wordnet import WordNetLemmatizer\n#Evaluation Metrics\nfrom sklearn.metrics import auc , roc_curve , confusion_matrix\n","821d62a5":"sqldatapath = \"..\/input\/amazon-fine-food-reviews\/database.sqlite\"\n\nfilter_data= SQLdata(sqldatapath)","a5402b01":"filter_data.head()","d5c418de":"actualScore = filter_data['Score']\npositiveNegative = actualScore.map(partition) \nfilter_data['Score'] = positiveNegative","ac6e84ec":"filter_data[filter_data['UserId']=='AZY10LLTJ71NX']","0142dda5":"# Sorting the data by Product ID \nsorted_data = filter_data.sort_values('ProductId' , ascending=True , axis=0 , inplace=False)\n\n# Dropping the duplicates\nfinal = sorted_data.drop_duplicates(subset={'UserId' , 'ProfileName', 'Time' , 'Text'} , inplace=False , keep='first')","51242b90":"filter_data[filter_data['HelpfulnessNumerator']> filter_data['HelpfulnessDenominator']]","6b76edf2":"final = final[final['HelpfulnessNumerator']< final['HelpfulnessDenominator']]","f979fbd7":"print('The final shape of the data is',final.shape)\nprint('The count of positive and negative reviews \\n', final['Score'].value_counts())","065151b7":"# Get all the stopwords from English Language\nstop = set(stopwords.words('english'))\n\n# Initialize the Snowball stemming\nsnow = SnowballStemmer('english')","d4d0c27f":"print(stop)","5faa9a5e":"print(snow.stem('tasty'))","58aa1629":"i =0\nstr1 = ' '\nfinal_string = []\nall_pos_words = []\nall_neg_words = []\ns = \" \"\nimport time\nfrom tqdm import tqdm\nstart = time.time()\n\nfor sent in tqdm(final['Text'].values):\n    filtered_sent =[]\n    sent = cleanhtml(sent)  # remove HTML tags\n    for w in sent.split():  \n        for clean_words in cleanpunc(w).split():\n            if ((clean_words.isalpha()) & (len(clean_words)>2)):\n                if (clean_words.lower() not in stop):\n                    s = (snow.stem(clean_words.lower())).encode('utf8')\n                    filtered_sent.append(s)   # storing all filterd words \n                    if (final['Score'].values[i] == 'Positive'):\n                        all_pos_words.append(s)  # storing all pos words\n                    if (final['Score'].values[i] == 'Negative'):\n                        all_neg_words.append(s)  # storing all neg words\n                else:\n                    continue\n            else:\n                continue\n    str1 = b\" \".join(filtered_sent)\n    final_string.append(str1)\n    \n    i+=1\n\nend = time.time()\n\nprint('The total time to run the cell: ', (end-start))","a3486b32":"final['CleanedText'] = final_string","d4e4f2eb":"final.head()","93037a1a":"# conn = sqlite3.connect('\/kaggle\/working\/final.sqlite')\n# c= conn.cursor()\n# conn.text_factory = str\n# final.to_sql('Amazon Reviews' , conn , if_exists='replace')","ff1a6a3d":"count_vect = CountVectorizer()\nfinal_count = count_vect.fit_transform(final['Text'].values)","feb2bb75":"print(\"The type of vectors\", type(final_count))\nprint('THE shape of the vector', final_count.get_shape())","b9240333":"#computing frequently occuring words in positive rewiew and negative rewiew\nfreq_pos = nltk.FreqDist(all_pos_words)\nfreq_neg = nltk.FreqDist(all_neg_words)\n\nprint(\"Most Common positive words: \" , freq_pos.most_common(20))\nprint(\"\\nMost Common positive words: \" , freq_neg.most_common(20))","72c6f84f":"# Bi-gram \n\ncount_vect = CountVectorizer(ngram_range=(1,2))\nfinal_bi_gram_count = count_vect.fit_transform(final['Text'].values)","263cfd95":"print(\"The shape of new vector matrix after Bi_gram \" , final_bi_gram_count.get_shape())","78ec1d4c":"tf_idf_vect = TfidfVectorizer(ngram_range=(1,2))\nfina__tfidf = tf_idf_vect.fit_transform(final['Text'].values)","4e01db65":"print(\"the shape of the vector after TF-IDF: \", fina__tfidf.get_shape())","25909020":"features = tf_idf_vect.get_feature_names()\nlen(features)","6bad1532":"# checking few features \nfeatures[109000:109010]","b1fe38fe":"# convert a row of sparsematrix into array\nprint(fina__tfidf[3,:].toarray()[0])","c3a03d45":"# getting top n features of TF-IDF\n\ndef top_n_feat(row , features , top=25):\n    \n    topn_ids= np.argsort(row)[::-1][:top]  #sorting the vector and reversing and pick top 25\n    topn_feat = [(features[i] , row[i]) for i in topn_ids]\n    df = pd.DataFrame(topn_feat)\n    df.columns = [ 'Features' , 'tfidf']\n    return df","c2cea12f":"top_tfidf = top_n_feat(fina__tfidf[2,:].toarray()[0] , features)","7f618c9c":"top_tfidf","6bc32f18":"from gensim.models import word2vec , KeyedVectors","f8f4e00b":"model = KeyedVectors.load_word2vec_format('..\/input\/gnewsvector\/GoogleNews-vectors-negative300.bin' , binary=True)","f153b14e":"model.wv['computer']","bdafda3d":"model.wv.similarity('woman' , 'man')","398a18e3":"model.wv.most_similar('woman')","55feedef":"import gensim\nfrom tqdm import tqdm\ni=0\nlist_of_sent =[]\n\nfor sent in tqdm(final['Text'].values):\n    filtered_sent=[]\n    sent= cleanhtml(sent)\n    for w in sent.split():\n        for cleanW in cleanpunc(w).split():\n            if (cleanW.isalpha()):\n                filtered_sent.append(cleanW.lower())\n            else:\n                continue\n    list_of_sent.append(filtered_sent)\n    ","676c0e9a":"print(final['Text'].values[0])","2317001f":"print(list_of_sent[0])","0accc927":"word2vecmodel = gensim.models.Word2Vec(list_of_sent , min_count=5, size=50 , workers=4)\nwords = list(word2vecmodel.wv.vocab)\nprint(len(words))","1bb763f5":"word2vecmodel.wv.most_similar('tasty')","b8a8ae2f":"word2vecmodel.wv.most_similar('like')","5c9dba9a":"count_vect_feat = count_vect.get_feature_names()\ncount_vect_feat.index('like')\nprint(count_vect_feat[574544])","e0d5002d":"# Compute Word to vector\n\nsent_vectors = []\n\nfor sent in tqdm(list_of_sent):    # looping over all the rewiew\n    sent_vec = np.zeros(50)         # len of the word vector\n    count_word =0                  #count the valid word in the sentence\n    for word in sent:              #looping over each word in the sentence\n        try:\n            vec =word2vecmodel.wv[word] #converting each word to vector\n            sent_vec+=vec                #adding vector to the defined vector\n            count_word+=1               #counting the number of words converted. used later for average\n        except:\n            continue\n    sent_vec\/= count_word\n    sent_vectors.append(sent_vec)","39e7ba2d":"print(len(sent_vectors))\nprint(len(sent_vectors[0]))","faaf0444":"tfidf_sent_vectors =[]\nrow=0\nfor sent in tqdm(list_of_sent[:10]):\n    sent_vec= np.zeros(50)\n    weight_sum=0\n    for word in sent:\n        try:\n            vec = word2vecmodel.wv[word]\n            tfidf = fina__tfidf[row , features.index(word)]\n            sent_vec+= (vec*tfidf)\n            weight_sum +=tfidf\n        except:\n            pass\n    sent_vec\/=weight_sum\n    tfidf_sent_vectors.append(sent_vec)\n    row+=1","726fda5c":"# Amazon Fine Food Reviews Analysis","0130d9c1":"# 7. TF-IDF\n\n* In information retrieval, tf\u2013idf or TFIDF, short for term frequency\u2013inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf\u2013idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general","f6d80cca":"# 3. Data Cleaning: Deduplication\nThere are lot of entries which are duplicate and should be removed. ","526c14b6":"# 8. Word2Vec\n\n* Word2vec takes as its input a large corpus of text and produces a vector space, typically of several hundred dimensions, with each unique word in the corpus being assigned a corresponding vector in the space. Word vectors are positioned in the vector space such that words that share common contexts in the corpus are located in close proximity to one another in the space","44129d66":"# 4. Text Processing: Stemming , Stop Words removal , Lemmatization\n\nHence in the Preprocessing phase we do the following in the order below:-\n\n1. Begin by removing the html tags\n2. Remove any punctuations or limited set of special characters like , or . or # etc.\n3. Check if the word is made up of english letters and is not alpha-numeric\n4. Check to see if the length of the word is greater than 2 (as it was researched that there is no adjective in 2-letters)\n5. Convert the word to lowercase\n6. Remove Stopwords\n7. Finally Snowball Stemming the word (it was obsereved to be better than Porter Stemming)","cc890fb2":"# 6.Bi-Grams and n-grams\n\n* an n-gram is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application. The n-grams typically are collected from a text or speech corpus. When the items are words, n-grams may also be called shingles","d8e8e42d":"# TF-IDF weighted W2V","0b62f28c":"**Observation**:- It was also seen that in two rows given below the value of HelpfulnessNumerator is greater than HelpfulnessDenominator which is not practically possible hence these two rows too are removed from calcualtions","59b55104":"# 5. Bag of Words\n* n this model, a text (such as a sentence or a document) is represented as the bag (multiset) of its words, disregarding grammar and even word order but keeping multiplicity.","a82d9e23":"# 9. Average Word2Vec , TF-IDF weighted Word2Vec","1224f021":"**OBSERVATION:** As it can be seen many words in positive and negative rewiew overlap. So its a good idea to consider the pair of words.","3015b25f":"* In the above example we see that **UserID , ProfileName , Time , Summary , Text** are same. This is because the user has given review of a product which has different varieties. For exampple : A user has reviewed a potato chips of one flavour but the the chips has different flavour on the same page.","0621ff48":"# 1.Loading the Data","3fd19f5f":"**OBSERVATION**: These words are Bi-grams but there may be some Uni-grams as well.","b25ec152":"# 2. Data Prepocessing\n* Converting the **Score Columm** in to Positive ( score >3 ) and Negative ( score <3 )"}}