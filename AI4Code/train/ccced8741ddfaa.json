{"cell_type":{"a86db206":"code","53e835bb":"code","2de7d958":"code","e540f63f":"code","737cbe6c":"code","8a5bf7f6":"code","03e4bc1f":"code","8e0fb1f4":"code","7ee14d53":"code","ca757cba":"code","8f5934f7":"code","0fec479b":"code","3e8d7117":"code","a992d558":"code","803074b6":"code","efc1d0a9":"code","c4d6ec8f":"code","526f1d52":"code","dbf2f84e":"code","88937e15":"code","309cbcfe":"markdown"},"source":{"a86db206":"from tensorflow.keras import layers\nfrom tensorflow import keras\nimport tensorflow as tf\n\nfrom sklearn.model_selection import train_test_split\nfrom ast import literal_eval\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np","53e835bb":"arxiv_data = pd.read_csv(\n    \"..\/input\/arxiv-paper-abstracts\/arxiv_data.csv\"\n)\narxiv_data.head()","2de7d958":"print(f\"There are {len(arxiv_data)} rows in the dataset.\")","e540f63f":"total_duplicate_titles = sum(arxiv_data[\"titles\"].duplicated())\nprint(f\"There are {total_duplicate_titles} duplicate titles.\")","737cbe6c":"arxiv_data = arxiv_data[~arxiv_data[\"titles\"].duplicated()]\nprint(f\"There are {len(arxiv_data)} rows in the deduplicated dataset.\")\n\n# There are some terms with occurrence as low as 1.\nprint(sum(arxiv_data[\"terms\"].value_counts() == 1))\n\n# How many unique terms?\nprint(arxiv_data[\"terms\"].nunique())","8a5bf7f6":"# Filtering the rare terms.\narxiv_data_filtered = arxiv_data.groupby(\"terms\").filter(lambda x: len(x) > 1)\narxiv_data_filtered.shape","03e4bc1f":"arxiv_data_filtered[\"terms\"] = arxiv_data_filtered[\"terms\"].apply(\n    lambda x: literal_eval(x)\n)\narxiv_data_filtered[\"terms\"].values[:5]","8e0fb1f4":"test_split = 0.1\n\n# Initial train and test split.\ntrain_df, test_df = train_test_split(\n    arxiv_data_filtered,\n    test_size=test_split,\n    stratify=arxiv_data_filtered[\"terms\"].values,\n)\n\n# Splitting the test set further into validation\n# and new test sets.\nval_df = test_df.sample(frac=0.5)\ntest_df.drop(val_df.index, inplace=True)\n\nprint(f\"Number of rows in training set: {len(train_df)}\")\nprint(f\"Number of rows in validation set: {len(val_df)}\")\nprint(f\"Number of rows in test set: {len(test_df)}\")","7ee14d53":"terms = tf.ragged.constant(train_df[\"terms\"].values)\nlookup = tf.keras.layers.StringLookup(output_mode=\"multi_hot\")\nlookup.adapt(terms)\nvocab = lookup.get_vocabulary()\n\n\ndef invert_multi_hot(encoded_labels):\n    \"\"\"Reverse a single multi-hot encoded label to a tuple of vocab terms.\"\"\"\n    hot_indices = np.argwhere(encoded_labels == 1.0)[..., 0]\n    return np.take(vocab, hot_indices)\n\n\nprint(\"Vocabulary:\\n\")\nprint(vocab)\n","ca757cba":"sample_label = train_df[\"terms\"].iloc[0]\nprint(f\"Original label: {sample_label}\")\n\nlabel_binarized = lookup([sample_label])\nprint(f\"Label-binarized representation: {label_binarized}\")","8f5934f7":"train_df[\"summaries\"].apply(lambda x: len(x.split(\" \"))).describe()","0fec479b":"max_seqlen = 150\nbatch_size = 128\npadding_token = \"<pad>\"\nauto = tf.data.AUTOTUNE\n\n\ndef make_dataset(dataframe, is_train=True):\n    labels = tf.ragged.constant(dataframe[\"terms\"].values)\n    label_binarized = lookup(labels).numpy()\n    dataset = tf.data.Dataset.from_tensor_slices(\n        (dataframe[\"summaries\"].values, label_binarized)\n    )\n    dataset = dataset.shuffle(batch_size * 10) if is_train else dataset\n    return dataset.batch(batch_size)\n","3e8d7117":"train_dataset = make_dataset(train_df, is_train=True)\nvalidation_dataset = make_dataset(val_df, is_train=False)\ntest_dataset = make_dataset(test_df, is_train=False)","a992d558":"text_batch, label_batch = next(iter(train_dataset))\n\nfor i, text in enumerate(text_batch[:5]):\n    label = label_batch[i].numpy()[None, ...]\n    print(f\"Abstract: {text}\")\n    print(f\"Label(s): {invert_multi_hot(label[0])}\")\n    print(\" \")","803074b6":"# Source: https:\/\/stackoverflow.com\/a\/18937309\/7636462\nvocabulary = set()\ntrain_df[\"summaries\"].str.lower().str.split().apply(vocabulary.update)\nvocabulary_size = len(vocabulary)\nprint(vocabulary_size)\n","efc1d0a9":"text_vectorizer = layers.TextVectorization(\n    max_tokens=vocabulary_size, ngrams=2, output_mode=\"tf_idf\"\n)\n\n# `TextVectorization` layer needs to be adapted as per the vocabulary from our\n# training set.\nwith tf.device(\"\/CPU:0\"):\n    text_vectorizer.adapt(train_dataset.map(lambda text, label: text))\n\ntrain_dataset = train_dataset.map(\n    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto\n).prefetch(auto)\nvalidation_dataset = validation_dataset.map(\n    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto\n).prefetch(auto)\ntest_dataset = test_dataset.map(\n    lambda text, label: (text_vectorizer(text), label), num_parallel_calls=auto\n).prefetch(auto)\n","c4d6ec8f":"\ndef make_model():\n    shallow_mlp_model = keras.Sequential(\n        [\n            layers.Dense(512, activation=\"relu\"),\n            layers.Dense(256, activation=\"relu\"),\n            layers.Dense(lookup.vocabulary_size(), activation=\"sigmoid\"),\n        ]  # More on why \"sigmoid\" has been used here in a moment.\n    )\n    return shallow_mlp_model\n","526f1d52":"epochs = 20\n\nshallow_mlp_model = make_model()\nshallow_mlp_model.compile(\n    loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"categorical_accuracy\"]\n)\n\nhistory = shallow_mlp_model.fit(\n    train_dataset, validation_data=validation_dataset, epochs=epochs\n)\n\n\ndef plot_result(item):\n    plt.plot(history.history[item], label=item)\n    plt.plot(history.history[\"val_\" + item], label=\"val_\" + item)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(item)\n    plt.title(\"Train and Validation {} Over Epochs\".format(item), fontsize=14)\n    plt.legend()\n    plt.grid()\n    plt.show()\n\n\nplot_result(\"loss\")\nplot_result(\"categorical_accuracy\")","dbf2f84e":"_, categorical_acc = shallow_mlp_model.evaluate(test_dataset)\nprint(f\"Categorical accuracy on the test set: {round(categorical_acc * 100, 2)}%.\")","88937e15":"# Create a model for inference.\nmodel_for_inference = keras.Sequential([text_vectorizer, shallow_mlp_model])\n\n# Create a small dataset just for demoing inference.\ninference_dataset = make_dataset(test_df.sample(100), is_train=False)\ntext_batch, label_batch = next(iter(inference_dataset))\npredicted_probabilities = model_for_inference.predict(text_batch)\n\n# Perform inference.\nfor i, text in enumerate(text_batch[:5]):\n    label = label_batch[i].numpy()[None, ...]\n    print(f\"Abstract: {text}\")\n    print(f\"Label(s): {invert_multi_hot(label[0])}\")\n    predicted_proba = [proba for proba in predicted_probabilities[i]]\n    top_3_labels = [\n        x\n        for _, x in sorted(\n            zip(predicted_probabilities[i], lookup.get_vocabulary()),\n            key=lambda pair: pair[0],\n            reverse=True,\n        )\n    ][:3]\n    print(f\"Predicted Label(s): ({', '.join([label for label in top_3_labels])})\")\n    print(\" \")","309cbcfe":"## Inference\n\nAn important feature of the\n[preprocessing layers provided by Keras](https:\/\/keras.io\/guides\/preprocessing_layers\/)\nis that they can be included inside a `tf.keras.Model`. We will export an inference model\nby including the `text_vectorization` layer on top of `shallow_mlp_model`. This will\nallow our inference model to directly operate on raw strings.\n\n**Note** that during training it is always preferable to use these preprocessing\nlayers as a part of the data input pipeline rather than the model to avoid\nsurfacing bottlenecks for the hardware accelerators. This also allows for\nasynchronous data processing."}}