{"cell_type":{"d5f9a300":"code","611e5c8f":"code","a014da93":"code","4232c938":"code","6214a307":"code","dfd7ed57":"code","76e6cddf":"code","218c516b":"code","37a6bb4a":"code","fc489ae3":"code","3a86b302":"code","1ca7f941":"code","8170c78c":"code","ee684025":"code","183e2152":"code","5d5b3f13":"code","ddf84800":"code","1dab161f":"code","6e346de3":"code","742dc472":"code","93f05a36":"code","e27227ef":"code","8db3bc6e":"code","8dbd065e":"code","4e22849b":"code","5acb9368":"code","d1061801":"code","b51208ba":"code","d8d1d96a":"code","a2f057a3":"code","4f3559f8":"code","d3e3f943":"code","fc71fee7":"code","35b4a301":"code","59d3028c":"code","021235c9":"code","c76a4864":"code","b679bd82":"code","8180c31e":"code","0325e5cc":"code","94d80e2c":"code","9c1f4768":"code","a0501b43":"code","9ccd3ea0":"markdown","b0405a7e":"markdown","43294058":"markdown","659d79c6":"markdown","462d7d5a":"markdown","127fdffe":"markdown","a7a6429d":"markdown","4c6c1f64":"markdown","ddabf210":"markdown","e5da38da":"markdown","1ef2570e":"markdown","f2d19a6a":"markdown","edc9eb17":"markdown","4733fc07":"markdown","ef57bbca":"markdown","93d28505":"markdown","9574fe70":"markdown","735651c7":"markdown","55f424ff":"markdown","82bfc398":"markdown","6949d61c":"markdown","4011258b":"markdown","12781af6":"markdown","6d4cbdb0":"markdown","2067a4d6":"markdown","b45d521e":"markdown","3f4b4c88":"markdown","4707b295":"markdown","9fa1a156":"markdown","3e7755fa":"markdown","d6745e60":"markdown","e3ace2ac":"markdown","6a66eb36":"markdown","671ab7f4":"markdown","bdbf305a":"markdown","fced1955":"markdown","c9012b98":"markdown","d5182b93":"markdown","acb3d32e":"markdown","9658b1af":"markdown","308616e1":"markdown","cc6b7d47":"markdown","13c301aa":"markdown","3aa88021":"markdown"},"source":{"d5f9a300":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","611e5c8f":"df = pd.read_csv('..\/input\/customer-life-time-value-prediction\/WA_Fn-UseC_-Marketing-Customer-Value-Analysis.csv')\npd.set_option('display.max_columns',None)\ndf.head()","a014da93":"df.shape","4232c938":"df.info()","6214a307":"df.dtypes.value_counts()","dfd7ed57":"df1 = df.copy(deep = True)\ndf['Customer Lifetime Value'].describe()","76e6cddf":"from scipy import stats\nfrom scipy.stats import norm,skew\n\nsns.distplot(df1['Customer Lifetime Value'],fit=norm)\n\n(mu,sigma) = norm.fit(df1['Customer Lifetime Value'])\nprint('mu = {:.2f} and sigma = {:.2f}'.format(mu,sigma))\n\nplt.legend(['Normal dist.(mu = {:.2f} and sigma = {:.2f})'.format(mu,sigma)],loc = 'best')\nplt.ylabel('Frequency')\nplt.title('Customer Life Time Value Distribution')\n\n## Q-Q PLOT\nfig = plt.figure()\nres = stats.probplot(df1['Customer Lifetime Value'],plot = plt)\nplt.show()","218c516b":"df1['Customer Lifetime Value'] = np.log1p(df1['Customer Lifetime Value'])\nsns.distplot(df1['Customer Lifetime Value'],fit=norm)\n\n(mu,sigma) = norm.fit(df1['Customer Lifetime Value'])\nprint('mu = {:.2f} and sigma = {:.2f}'.format(mu,sigma))\n\nplt.legend(['Normal dist.(mu = {:.2f} and sigma = {:.2f})'.format(mu,sigma)],loc = 'best')\nplt.ylabel('Frequency')\nplt.title('Customer Life Time Value Distribution')\n\n## Q-Q PLOT\nfig = plt.figure()\nres = stats.probplot(df1['Customer Lifetime Value'],plot = plt)\nplt.show()","37a6bb4a":"print('skewness before transformation:',df['Customer Lifetime Value'].skew())\nprint('skewness after transformation:',df1['Customer Lifetime Value'].skew())","fc489ae3":"df1 = df1.drop(['Customer','Customer Lifetime Value','Effective To Date'],axis = 1)\ndf_cat1 = df1.select_dtypes(include = ['object'])\ndf_cat1.columns","3a86b302":"fig, axes = plt.subplots(round(len(df_cat1.columns) \/ 4), 4, figsize=(20, 15))\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(df_cat1.columns):\n        df[df_cat1.columns[i]].value_counts().plot.pie(autopct = '%1.1f%%',ax = ax)\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)\n        ax.set_title(df_cat1.columns[i])\n\nfig.tight_layout()","1ca7f941":"fig, axes = plt.subplots(round(len(df_cat1.columns) \/ 4), 4, figsize=(20, 20))\n\nfor i, ax in enumerate(fig.axes):\n    if i < len(df_cat1.columns):\n        sns.barplot(x = df[df_cat1.columns[i]],y = df['Customer Lifetime Value'],data = df, ax = ax )\n        ax.set_xticklabels(ax.xaxis.get_majorticklabels(), rotation=45)\n\nfig.tight_layout()","8170c78c":"df_num = df.select_dtypes(include=['int64','float'])\ndf_num.columns","ee684025":"df_num_corr = df_num.corr()['Customer Lifetime Value'].sort_values(ascending=False)\ndf_num_corr","183e2152":"for i in range(0, len(df_num.columns), 4):\n    sns.pairplot(data=df_num,\n                x_vars=df_num.columns[i:i+4],\n                y_vars=['Customer Lifetime Value'])","5d5b3f13":"plt.figure(figsize=(10,6))\nsns.heatmap(df_num.corr(),annot = True)","ddf84800":"y = df['Customer Lifetime Value']\ndf1.head()","1dab161f":"df1 = pd.get_dummies(df1,drop_first=True)\ndf1.shape","6e346de3":"num_features = df.dtypes[df.dtypes != 'object'].index\nskewed_feat = df[num_features].apply(lambda x: x.skew()).sort_values(ascending = False)\nskewness = pd.DataFrame(skewed_feat,columns = ['skew'])\nskewness","742dc472":"from scipy.special import boxcox1p\nskewness_feat = ['Monthly Premium Auto','Total Claim Amount']\nlam = 0.15\nfor i in skewness_feat:\n    df1[i] = boxcox1p(df1[i],lam)","93f05a36":"import statsmodels.api as sm\nXc = sm.add_constant(df1)\nmodel = sm.OLS(y,Xc).fit()\nmodel.summary()","e27227ef":"y_transformed = np.log1p(y)\nmodel2 = sm.OLS(y_transformed,Xc).fit()\nmodel2.summary()","8db3bc6e":"X_up = df1[['Total Claim Amount','Monthly Premium Auto']]\nfrom sklearn.preprocessing import PolynomialFeatures\nqr = PolynomialFeatures(degree = 3)\nx_qr = qr.fit_transform(X_up)\nx_main = pd.DataFrame(x_qr)\nx_main.head()","8dbd065e":"df2= pd.concat([df1,x_main.iloc[:,3:]],axis = 1)\ndf2.head()","4e22849b":"Xc = sm.add_constant(df2)\nmodel3 = sm.OLS(y_transformed,Xc).fit()\nmodel3.summary()","5acb9368":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(df1,y_transformed,test_size = 0.3,random_state=4)","d1061801":"from sklearn.linear_model import Lasso,ElasticNet,LassoLarsIC,BayesianRidge\nfrom sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor,BaggingRegressor,AdaBoostRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.base import BaseEstimator,TransformerMixin,RegressorMixin,clone\nfrom sklearn.model_selection import KFold,cross_val_score,train_test_split\nfrom sklearn.metrics import mean_squared_error,r2_score\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom sklearn.svm import SVR","b51208ba":"dt = DecisionTreeRegressor()\ndt_tuned = DecisionTreeRegressor(max_depth =  8, min_samples_leaf =  18, min_samples_split = 22)\ndt_bag = BaggingRegressor(base_estimator=dt_tuned,n_estimators=10,random_state=0)\ndt_boost = AdaBoostRegressor(base_estimator=dt_tuned,n_estimators=50,random_state=0)","d8d1d96a":"for model,name in zip([dt,dt_tuned,dt_bag,dt_boost],['decision_tree','dt_tuned','dt_bag','dt_boost']):\n    print('model: {}'.format(name))\n    model.fit(x_train,y_train)\n    pred = model.predict(x_test)\n    pred1 = model.predict(x_train)\n    r2_train = r2_score(y_train,pred1)\n    r2_test = r2_score(y_test,pred)\n    adjusted_r_squared = abs(1 - ( (1 - r2_test) * (len(y_test) - 1)) \/ (len(y_test) - x_test.shape[1] - 1))\n    mape = (abs((pred-y_test)\/y_test).mean())*100\n    print('train r2:',r2_train)\n    print('test r2:',r2_test)\n    print('Mape: {}'.format(mape))\n    print('Adjusted R squared score is :', adjusted_r_squared)\n    print('*'*60)","a2f057a3":"X_imp=pd.DataFrame(dt_tuned.feature_importances_,columns=['Imp'],index=x_test.columns)\nX_imp=X_imp.sort_values(by='Imp',ascending=False)\nplt.figure(figsize=(250,50))\nplt.xlabel('Feature Names',fontsize=80)\nplt.xticks(rotation=90,fontsize=150)\nsns.barplot(x=X_imp.index,y=X_imp['Imp'])\nX_imp.T","4f3559f8":"rf = RandomForestRegressor()\nrf_tuned = RandomForestRegressor(max_depth =  36,max_features = 25,min_samples_leaf = 2,min_samples_split = 7,n_estimators=432)","d3e3f943":"for model,name in zip([rf,rf_tuned],['Random Forest','Random Forest Tuned']):\n    print('model: {}'.format(name))\n    model.fit(x_train,y_train)\n    pred = model.predict(x_test)\n    pred1 = model.predict(x_train)\n    r2_train = r2_score(y_train,pred1)\n    r2_test = r2_score(y_test,pred)\n    adjusted_r_squared = abs(1 - ( (1 - r2_test) * (len(y_test) - 1)) \/ (len(y_test) - x_test.shape[1] - 1))\n    mape = (abs((pred-y_test)\/y_test).mean())*100\n    print('train r2:',r2_train)\n    print('test r2:',r2_test)\n    print('Mape: {}'.format(mape))\n    print('Adjusted R squared score is :', adjusted_r_squared)\n    print('*'*60)","fc71fee7":"X_imp=pd.DataFrame(rf_tuned.feature_importances_,columns=['Imp'],index=x_test.columns)\nX_imp=X_imp.sort_values(by='Imp',ascending=False)\nplt.figure(figsize=(250,50))\nplt.xlabel('Feature Names',fontsize=80)\nplt.xticks(rotation=90,fontsize=150)\nsns.barplot(x=X_imp.index,y=X_imp['Imp'])\nX_imp.T","35b4a301":"gboost = GradientBoostingRegressor(n_estimators=1000)","59d3028c":"for model,name in zip([gboost],['Gradient Boost']):\n    print('model: {}'.format(name))\n    model.fit(x_train,y_train)\n    pred = model.predict(x_test)\n    pred1 = model.predict(x_train)\n    r2_train = r2_score(y_train,pred1)\n    r2_test = r2_score(y_test,pred)\n    adjusted_r_squared = abs(1 - ( (1 - r2_test) * (len(y_test) - 1)) \/ (len(y_test) - x_test.shape[1] - 1))\n    mape = (abs((pred-y_test)\/y_test).mean())*100\n    print('train r2:',r2_train)\n    print('test r2:',r2_test)\n    print('Mape: {}'.format(mape))\n    print('Adjusted R squared score is :', adjusted_r_squared)","021235c9":"model_lgb = lgb.LGBMRegressor()\nmodel_lgb_tuned = lgb.LGBMRegressor(n_estimators=43)","c76a4864":"for model,name in zip([model_lgb,model_lgb_tuned],['Light GBM','Light GBM Tuned']):\n    print('model: {}'.format(name))\n    model.fit(x_train,y_train)\n    pred = model.predict(x_test)\n    pred1 = model.predict(x_train)\n    r2_train = r2_score(y_train,pred1)\n    r2_test = r2_score(y_test,pred)\n    adjusted_r_squared = abs(1 - ( (1 - r2_test) * (len(y_test) - 1)) \/ (len(y_test) - x_test.shape[1] - 1))\n    mape = (abs((pred-y_test)\/y_test).mean())*100\n    print('train r2:',r2_train)\n    print('test r2:',r2_test)\n    print('Mape: {}'.format(mape))\n    print('Adjusted R squared score is :', adjusted_r_squared)\n    print('*'*60)","b679bd82":"model_xgb = xgb.XGBRegressor()\nmodel_xgb_tuned = xgb.XGBRegressor(learning_rate = 0.1,max_depth= 6,min_child_weight= 4,n_estimators= 70)","8180c31e":"for model,name in zip([model_xgb,model_xgb_tuned],['XG Boost','XG Boost Tuned']):\n    print('model: {}'.format(name))\n    model.fit(x_train,y_train)\n    pred = model.predict(x_test)\n    pred1 = model.predict(x_train)\n    r2_train = r2_score(y_train,pred1)\n    r2_test = r2_score(y_test,pred)\n    adjusted_r_squared = abs(1 - ( (1 - r2_test) * (len(y_test) - 1)) \/ (len(y_test) - x_test.shape[1] - 1))\n    mape = (abs((pred-y_test)\/y_test).mean())*100\n    print('train r2:',r2_train)\n    print('test r2:',r2_test)\n    print('Mape: {}'.format(mape))\n    print('Adjusted R squared score is :', adjusted_r_squared)\n    print('*'*60)","0325e5cc":"from catboost import CatBoostClassifier, CatBoostRegressor\ncat_boost =CatBoostRegressor(iterations = 100,\n            learning_rate = 0.05,\n            eval_metric = \"R2\",\n            verbose = False)","94d80e2c":"cat_boost.fit(x_train,y_train)\npred = cat_boost.predict(x_test)\npred1 = cat_boost.predict(x_train)\nr2_train = r2_score(y_train,pred1)\nr2_test = r2_score(y_test,pred)\nmape = (abs((pred-y_test)\/y_test).mean())*100\nprint('train r2:',r2_train)\nprint('test r2:',r2_test)\nprint('Mape: {}'.format(mape))","9c1f4768":"class AveragingModels(BaseEstimator,RegressorMixin,TransformerMixin):\n    def __init__(self,models):\n        self.models = models\n    def fit(self,X,y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        for model in self.models_:\n            model.fit(X,y)\n        return self\n    \n    def predict(self,X):\n        predictions = np.column_stack([model.predict(X) for model in self.models_])\n        return np.mean(predictions,axis = 1)","a0501b43":"averaged_models = AveragingModels(models =  (rf_tuned,model_lgb_tuned,gboost))\naveraged_models.fit(x_train,y_train)\npred = averaged_models.predict(x_test)\npred1 = averaged_models.predict(x_train)\nr2_train = r2_score(y_train,pred1)\nr2_test = r2_score(y_test,pred)\nmape = (abs((pred-y_test)\/y_test).mean())*100\nprint('AveragingModels test score: {}' .format(rmse_test))\nprint('train r2:',r2_train)\nprint('test r2:',r2_test)\nprint('Mape: {}'.format(mape))","9ccd3ea0":"### Data Preprocessing","b0405a7e":"### There isn't any appreciable increase in r2.","43294058":"## Random Forest Regressor","659d79c6":"![employment%20status.png](attachment:employment%20status.png)","462d7d5a":"### Education","127fdffe":"1. Majority of the customer base of the company is from Suburban region of the states.\n\n2. Company can target customers in the urban region of Arizona and Nevada since the average CLV of customers from that area is the highest.","a7a6429d":"Most of the people are converted to customers by agents. So, company can invest more in agents.","4c6c1f64":"### We got a very low r2 value of 0.170, it proves that our conclusion from the scatterplot that the linear model will not work well for this dataset is true","ddabf210":"### Sales Channel","e5da38da":"1. ### So it's better to go for non linear models. I have splitted the dataset in 70:30 ratio.","1ef2570e":"## GradientBoost","f2d19a6a":"1. It is important that the independent features that we are using for the prediction of target variable should be strongly correlated with the target variable.\n2. There is no linear relationship between continuous independent features and CLV.\n3. Linear models would not work well for the given dataset, so we may have to choose non-linear models for better prediction.","edc9eb17":"### Analyzing Target Variable","4733fc07":"### The r2 value increased by just 10%","ef57bbca":" ## Decision Tree Regressor","93d28505":"![Vehicle%20class.jpg](attachment:Vehicle%20class.jpg)","9574fe70":"### It is important to transform the skewed features before applying linear models.","735651c7":"![state.png](attachment:state.png)","55f424ff":"1. Customers who have taken Luxury Vehicles pay more monthly premium for their vehicles and they are considered more valuable than others.\n2. This proves the positive correlation of 0.4 between Monthly Premium Auto and CLV.\n3. Most of the people prefers Four Door Car.","82bfc398":"## Among all the models I have applied Random Forest is Performing best","6949d61c":"1. People with high school education (or) below have lowest income but interestingly they have highest clv probably because their monthly premium auto highest. Also, their total claim amount is highest.\n2. People with master's degree have lesser claim amount but high premium and clv. A potential profitable group to target (since no. of records for this group is less)","4011258b":"![sales%20channel.png](attachment:sales%20channel.png)","12781af6":"### Let's check the score after transforming target variable","6d4cbdb0":"#### The distribution plot indicates that the target variable is right skewed with a skewness of 3.03.\n#### Indicates presence of outliers.\n#### So it is important to transform it before applying linear models. I have applied log transformation on the target variable.","2067a4d6":"1. None of the features are strongly correlated with the target variable (Customer Life time Value).\n\n2. As compared to other variables the highest correlation of target variable is observed with Monthly premium Auto. It is 0.4 which indicates a weak positive correlation.\n\n3. The highest correlation can be seen between two independent variables - Monthly premium auto and total claim amount, which is equal to 0.63.","b45d521e":"## Modelling","3f4b4c88":"### The bar charts show the average customer life time value of each categories in the Categorical features. From the bar chart itself we can observe that some of the features are insignificant like Response, Location code, Gender because the average customer lifetime value under each categories of these features are almost same. ","4707b295":"### Vehicle Class","9fa1a156":"### Now I have tried to segment customers based on their education, employment status, vehicle class, their state and location code to gain important insights and find which segment is more profitable segment. I Have created these graphs on Tableau.","3e7755fa":"### Employement Status","d6745e60":"### I did the hyperparameter tuning using Randomized search CV and best parameters that I got is mentioned in the dt_tuned model. ","e3ace2ac":"## Stacking","6a66eb36":"## LightGBM Regressor","671ab7f4":"### The pie charts show the percentage of customers in each category of the categorical variables","bdbf305a":"## CatBoost Regressor","fced1955":"![education.png](attachment:education.png)","c9012b98":"In this Dataset customer Lifetime Value is the target variable.\n1. Customer Lifetime Value is defined as, \u201cThe total value of direct and indirect contributions to overhead and profit by an individual customer during the entire customer lifecycle that is - from the start of the relationship until its projected ending\u201d.\n2. CLV evaluates the long-term value of customers with the company.\n3. It is a powerful measure used not only to determine which clients have the most potential, but also to decide how much  marketing expenditures is justified for each segment.**","d5182b93":"### Let's start with the base linear model without doing any transformation and feature elimination and check the score of R2","acb3d32e":"### State and Location Code","9658b1af":"## XGBoost Regressor","308616e1":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint as sp_randint\ndtc = DecisionTreeRegressor()\n\nparams = {'max_depth': sp_randint(2,20),'min_samples_leaf': sp_randint(1,20),\n         'min_samples_split': sp_randint(2,40)}\n\nrsearch = RandomizedSearchCV(dtc,param_distributions=params,n_iter=100,cv =10, scoring = 'r2')\nrsearch.fit(df1,y)","cc6b7d47":"### Let's apply ols model after applying polynomial features. Among the continuous features that we have 'Total Claim Amount','Monthly Premium Auto' are the only features which are heteroscedastic in nature. So we will be using these features for polynomial regression.","13c301aa":"1. Even though the unemployed people have no income, their monthly premium and CLV is on par with others.\n2. Employed people are paying high premium but their claim amount is lowest. So they are the potential target group because they are paying more and claiming less","3aa88021":"### There are no missing values in the dataset"}}