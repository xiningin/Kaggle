{"cell_type":{"430f3f6a":"code","d043c623":"code","e1f765c2":"code","b2a6ff56":"code","14343a21":"code","53bcdf19":"code","1c3fda1c":"code","21ac839b":"code","a25ae4af":"code","d74776a6":"code","e455a985":"code","fb9c782f":"code","3e819c15":"code","d903fe4a":"code","23cbb2e1":"code","a011f5b2":"code","248ba912":"code","11505a97":"code","ba907168":"code","9b36f64a":"code","113b0afb":"code","c49ed368":"code","a7bc237f":"code","101f9b14":"code","540e390e":"code","19bd50ac":"code","0d5ad094":"code","19c60812":"code","1da10942":"code","3d58e3c2":"code","193c8e8f":"code","0f49f034":"code","23320a4c":"code","d9b08bc3":"code","f96e3068":"code","69df4fcd":"code","34f93b7a":"code","84a867ee":"code","a8a30f26":"code","84705d7f":"code","9b9057e1":"code","5660fda2":"code","e8141366":"code","28c35289":"code","4a2f7f3b":"code","abe8a7fd":"code","8e834e29":"code","86082851":"code","38985270":"code","5267b886":"code","aa526227":"code","0dd6b7f0":"markdown","ebe4bca1":"markdown","ce2c1044":"markdown","dd486217":"markdown","76692558":"markdown","7efe7b2c":"markdown","3ff83f02":"markdown","25a7945b":"markdown","fd1dd511":"markdown","22053c1c":"markdown","bc0e1248":"markdown","079b4871":"markdown","593e2fe9":"markdown","cb4295ec":"markdown","0736a8f6":"markdown","b6d6914b":"markdown","a81323f7":"markdown","68a9e456":"markdown","c8188cfc":"markdown","1a03ea63":"markdown","07a9dd17":"markdown","e3de2ba0":"markdown","adb76ff7":"markdown","255f15ac":"markdown","0a21967e":"markdown","07d70c96":"markdown","4ba28d87":"markdown","f2378452":"markdown","6102cabc":"markdown"},"source":{"430f3f6a":"import gym\nimport numpy as np\nimport random as rd","d043c623":"env = gym.make('FrozenLake-v0')\nenv.render()  # check environment\n\neon = env.observation_space.n\nean = env.action_space.n\n\n#\u7b56\u7565\u8fed\u4ee3\ndef compute_value_function(policy, gamma=0.99):#Discount rate, akagamma\n    # initialize value table with zeros\n    # \u521d\u59cb\u5316\u8868\u683c\n    value_table = np.zeros(eon)\n    # set the threshold\n    #\u8bbe\u7f6e\u5224\u65ad\u9608\u503c\n    threshold = 1e-10\n    #set threshold really near to zero\n    \n    # \u5faa\u73af\u5230\u6536\u655b\n    #stop until convergence\n    while True:\n        # copy the value table to the updated_value_table\n        updated_value_table = np.copy(value_table)\n        #\u8ba1\u7b97\u6bcf\u4e2a\u72b6\u6001\u4ece\u7b56\u7565\u4e2d\u5f97\u5230\u7684\u52a8\u4f5c\uff0c\u7136\u540e\u8ba1\u7b97\u503c\u51fd\u6570\n        #\u904d\u5386\u6bcf\u4e2a\u72b6\u6001\n        # for each state in the environment, \n        #select the action according to the policy and compute the value table\n        for state in range(eon):\n            action = policy[state]\n            #\u66f4\u65b0\u72b6\u6001\u7684value\n            # build the value table with the selected action\n            value_table[state] = sum([trans_prob * (reward_prob + gamma * updated_value_table[next_state])\n                        for trans_prob, next_state, reward_prob, _ in env.P[state][action]])\n        #\u6536\u655b\u5224\u65ad\n        #see if the formula convergence\uff0cfabs\u662f\u8fd4\u56de\u7edd\u5bf9\u503c\n        if (np.sum((np.fabs(updated_value_table - value_table))) <= threshold):\n            break\n    #\u8fd4\u56devalue-table\n    return value_table","e1f765c2":"def extract_policy(value_table, gamma = 0.99):\n\n    # Initialize the policy with zeros\n    policy = np.zeros(eon)\n\n    for state in range(eon):\n\n        # initialize the Q table for a state\n        Q_table = np.zeros(ean)\n\n        # compute Q value for all ations in the state\n        for action in range(ean):\n            for next_sr in env.P[state][action]:\n                trans_prob, next_state, reward_prob, _ = next_sr\n                Q_table[action] += (trans_prob * (reward_prob + gamma * value_table[next_state]))\n\n        # Select the action which has maximum Q value as an optimal action of the state\n        policy[state] = np.argmax(Q_table)\n\n    return policy","b2a6ff56":"def policy_iteration(env, gamma=0.99):\n\n    # Initialize policy with zeros\n    old_policy = random_policy = np.ones(eon)\n    no_of_iterations = 20000\n\n    for i in range(no_of_iterations):\n\n        # compute the value function\n        new_value_function = compute_value_function(old_policy, gamma)\n\n        # Extract new policy from the computed value function\n        new_policy = extract_policy(new_value_function, gamma)\n\n        # Then we check whether we have reached convergence i.e whether we found the optimal\n        # policy by comparing old_policy and new policy if it same we will break the iteration\n        # else we update old_policy with new_policy\n\n        if (np.all(old_policy == new_policy)):\n            print ('Policy-Iteration converged at step %d.' %(i+1))\n            break\n        old_policy = new_policy\n\n    return new_policy\n\nprint (policy_iteration(env))","14343a21":"#setup game environment\nenv = gym.make('FrozenLake8x8-v0')\n#\u5b9a\u4e49Q\u503c\u8868\uff0c\u521d\u59cb\u503c\u8bbe\u4e3a0\n#intial Q-table, initial data as 0\nQ = np.zeros([env.observation_space. n,env.action_space.n])\n#\u6a2a\u5750\u6807\u662f16*4\u6bcf\u4e2a\u7816\u5757\u76844\u79cd\u52a8\u4f5c\uff0c\u7eb5\u5750\u6807\u662f4\u4e2a\u52a8\u4f5c\n#\u8bbe\u7f6e\u5b66\u4e60 \u53c2\u6570\n#set up hyper-parameter\nlearningRate = 0.85 #\u5373alpha\ndiscountFactor = 0.95 #\u5373gamma\n#Learning rate\/alpha: mathematically shown using the symbol alpha\n#Discount rate\/gemma: represented with the symbol Gemma\n\n#\u5b9a\u4e49\u4e00\u4e2a\u6570\u7ec4\uff0c\u7528\u4e8e\u4fdd\u5b58\u6bcf\u4e00\u56de\u5408\u5f97\u5230\u7684\u5956\u52b1\n#hold all the rewards will get from each episode. \n#This will be so we can see how our game scores change over time\nrewardList = []\nprint(Q);","53bcdf19":"def epsilon_greedy(q_table, s, num_episodes):\n    #for each time step within an episode we set our exploration rate threshold\n    rand_num = rd.randint(0, 20000)\n    if rand_num > num_episodes:\n        #\u968f\u673a\u9009\u62e9\u4e00\u4e2a\u52a8\u4f5c\n        #the agaent will explore the environment and simple action randomly\n        action = rd.randint(0, 3)\n    else :\n        #\u9009\u62e9\u4e00\u4e2a\u6700\u4f18\u7684\u52a8\u4f5c\n        #the agent will exploit the environment and choose the action\n        #has the highest key value in the Q-table for the current state\n        action = np.argmax(q_table[s, :])\n    return action\n#\u5bf9\u4e8e\u73af\u5883\u7684\u63a2\u7d22\u5e94\u4e3b\u8981\u96c6\u4e2d\u5728\u5b66\u4e60 Q \u503c\u8868\u7684\u5f00\u59cb\u9636\u6bb5\uff0c\u540e\u671fe\u5e94\u8be5\u6162\u6162\u51cf\u5c0f","1c3fda1c":"def train():\n    for i_episodes in range(20000): ##for each time step within an episode\n    #\u91cd\u7f6e\u6e38\u620f\u73af\u5883\n    #for each episode\n    #we're going to first rest the state of the environment back to the starting state\n      s = env.reset()\n      i = 0\n    # \u5b66\u4e60 Q-Table\n      while i < 20000:\n        i += 1\n        #\u4f7f\u7528\u5e26\u63a2\u7d22(\u03b5\u8d2a\u5fc3\u7b97\u6cd5)\u7684\u7b56\u7565\u9009\u62e9\u52a8\u4f5c\n        #use epsilon greedy strategy\n        a = epsilon_greedy(Q, s, i_episodes)\n        #\u6267\u884c\u52a8\u4f5c\uff0c\u5e76\u5f97\u5230\u65b0\u7684\u73af\u5883\u72b6\u6001\u3001\u5956\u52b1\u7b49\n        #new state, reward for that, whether the action in our episode, and diagnostic information\n        observation, reward, done, info = env.step(a)\n        #\u66f4\u65b0Q\u503c\u8868\n        #update Q-table\n        #Q(s, a)\u2190(1-a)*Q(s, a)+\u03b1[R+\u03b3max(Q(s', a'))]\n        Q[s, a] = (1-learningRate) * Q[s, a] + learningRate * ( reward + discountFactor * np.max(Q[observation,:]))\n        s = observation\n        #'done' check whether or not our episode is finished\n        \n        if done:\n            break","21ac839b":"def test():\n    for i_episodes in range(100):\n        #\u91cd\u7f6e\u6e38\u620f\u73af\u5883\n        #reset environment\n        s = env.reset()\n        i = 0\n        total_reward = 0\n        while i < 500:\n            i += 1\n            #\u9009\u62e9\u4e00\u4e2a\u52a8\u4f5c\n            #choose an action\n            a = np.argmax(Q[s, :])\n            #\u6267\u884c\u52a8\u4f5c\uff0c\u5e76\u5f97\u5230\u65b0\u7684\u73af\u5883\u72b6\u6001\u3001\u5956\u52b1\u7b49\n            #act new action and update\n            observation, reward, done, info = env.step(a)\n            #\u53ef\u89c6\u5316\u6e38\u620f\u753b\u9762(\u91cd\u7ed8\u4e00\u5e27\u753b\u9762)\n            env.render()\n            #\u8ba1\u7b97\u5f53\u524d\u56de\u5408\u7684\u603b\u5956\u52b1\u503c\n            #update the rewards from current episode by adding the reward we received\n            total_reward += reward\n            s = observation\n            if done:\n                break\n        rewardList.append(total_reward)","a25ae4af":"train()\ntest()","d74776a6":"print(\"Final Q-Table Values: \")\nprint(Q)","e455a985":"print(\"Success rate: \" + str(sum(rewardList)\/len(rewardList)))","fb9c782f":"import matplotlib.pyplot as plt\nplt.plot(range(len(rewardList)), rewardList)","3e819c15":"#The result not that good, so I'm going to change the hyper-parameter\nlearningRate = 0.05 \ndiscountFactor = 0.99 \nrewardList = []","d903fe4a":"train()\ntest()","23cbb2e1":"print(\"Success rate: \" + str(sum(rewardList)\/len(rewardList)))\nplt.plot(range(len(rewardList)), rewardList)","a011f5b2":"# !apt update\n!apt install -y python-opengl ffmpeg > \/dev\/null 2>&1\n# !apt install -y xvfb\n%pip install pyvirtualdisplay gym[atari] gym[box2d] gym[classic_control]\n# WORKAROUND: retry ---v\n%pip install gym[box2d]\n%pip install --no-deps baselines\n# %conda list","248ba912":"!apt-get install python-opengl -y\n!apt install xvfb -y\n!pip install gym[atari]\n!pip install pyvirtualdisplay\n!pip install piglet","11505a97":"from pyvirtualdisplay import Display\ndisplay = Display(visible=0, size=(1024, 768))\ndisplay.start()\n\n\nfrom matplotlib import pyplot as plt, animation\n%matplotlib inline\nfrom IPython import display\n\ndef create_anim(frames, dpi, fps):\n    plt.figure(figsize=(frames[0].shape[1] \/ dpi, frames[0].shape[0] \/ dpi), dpi=dpi)\n    patch = plt.imshow(frames[0])\n    def setup():\n        plt.axis('off')\n    def animate(i):\n        patch.set_data(frames[i])\n    anim = animation.FuncAnimation(plt.gcf(), animate, init_func=setup, frames=len(frames), interval=fps)\n    return anim\n\ndef display_anim(frames, dpi=72, fps=50):\n    anim = create_anim(frames, dpi, fps)\n    return anim.to_jshtml()\n\ndef save_anim(frames, filename, dpi=72, fps=50):\n    anim = create_anim(frames, dpi, fps)\n    anim.save(filename)\n\n\nclass trigger:\n    def __init__(self):\n        self._trigger = True\n\n    def __call__(self, e):\n        return self._trigger\n\n    def set(self, t):\n        self._trigger = t","ba907168":"# This code creates a virtual display to draw game images on. \nimport os\nif type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n    !bash ..\/xvfb start\n    %env DISPLAY=:1","9b36f64a":"import gym\nfrom gym import logger as gymlogger\nfrom gym.wrappers import Monitor\nfrom gym import envs\n#print(envs.registry.all())\ngymlogger.set_level(40) # error only\nimport tensorflow as tf\nimport numpy as np\nimport random\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot as plt, animation\n%matplotlib inline\nimport math\nimport glob\nimport io\nimport base64\nfrom IPython.display import HTML\nfrom IPython import display as ipythondisplay","113b0afb":"#Utility functions to enable video recording of gym environment and displaying it\n#To enable video, just do \"env = wrap_env(env)\"\"\n\ndef show_video():\n  mp4list = glob.glob('video\/*.mp4')\n  if len(mp4list) > 0:\n    mp4 = mp4list[0]\n    video = io.open(mp4, 'r+b').read()\n    encoded = base64.b64encode(video)\n    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n                loop controls style=\"height: 400px;\">\n                <source src=\"data:video\/mp4;base64,{0}\" type=\"video\/mp4\" \/>\n             <\/video>'''.format(encoded.decode('ascii'))))\n  else: \n    print(\"Could not find video\")\n    \n\ndef wrap_env(env):\n  env = Monitor(env, '.\/video', force=True)\n  return env","c49ed368":"env = gym.make('CartPole-v0')","a7bc237f":"#small example of cart-v0\n#when I run this on jupyter, it will have a small window jumped out\n#and play it\n\nenv = gym.make('CartPole-v0')\nenv.reset()\nplt.imshow(env.render('rgb_array'))\n#'rgb_array' is a mode makes the plot return an array\n#\u8ba9\u56fe\u50cf\u4ee5\u4e00\u4e2a NUMPY \u6570\u7ec4\u683c\u5f0f\u8fd4\u56de\n#env.render()","101f9b14":"#env = gym.make('CartPole-v0')\nprint(env.action_space) \n#discrete\u79bb\u6563\u578b\u53d8\u91cf type, 2 actions\nprint(env.observation_space) \n#print(env.observation_space.high)\n#print(env.observation_space.low)","540e390e":"#import gym\n#import numpy as np\n#!conda install -c conda-forge pyglet","19bd50ac":"max_number_of_steps = 200   # the max score of each games\n#The wining condition:\ngoal_average_steps = 195 #higher than 195\nnum_consecutive_iterations = 100 #for continuously 100 games\n\nnum_episodes = 200 # 200 round games in total\nlast_time_steps = np.zeros(num_consecutive_iterations)","0d5ad094":"frames = []\nenv = gym.make(\"CartPole-v0\")\nenv.reset()\nfor _ in range(300):\n    frames.append(env.render(mode='rgb_array'))\n    env.step(env.action_space.sample())\nenv.close()","19c60812":"display.HTML(display_anim(frames))","1da10942":"frames2 = []\nenv = gym.make(\"CartPole-v0\")\nenv.reset()\n\nmax_number_of_steps = 200  \ngoal_average_steps = 195\nnum_consecutive_iterations = 100\n\n#we will have 200 episode in total\n#it should be higher but colab will crash if I do that much\nnum_episodes = 200 # \u5171\u8fdb\u884c200\u573a\u6e38\u620f\uff0c\u56e0\u4e3a\u5230200\u573a\u57fa\u672c\u4e0a\u5c31\u4e0d\u80fd\u518d\u63d0\u5347\u4e86\n#the reason I choose 200 is becasue it will not improve alot after that\n#also my system will crash if I do alot of rounds\nlast_time_steps = np.zeros(num_consecutive_iterations)  \n# Only store the reward of the most recent 100 episodes","3d58e3c2":"# \u91cd\u590d\u8fdb\u884c\u4e00\u573a\u573a\u7684\u6e38\u620f\n#repeatly play this game\nfor episode in range(num_episodes):\n    #initial the environment\n    observation = env.reset()   # \u521d\u59cb\u5316\u672c\u573a\u6e38\u620f\u7684\u73af\u5883 \n    #initial the reward of this round of game\n    episode_reward = 0  # \u521d\u59cb\u5316\u672c\u573a\u6e38\u620f\u7684\u5f97\u5206\n    # each round of the game is one time step\n    # \u4e00\u573a\u6e38\u620f\u5206\u4e3a\u4e00\u4e2a\u4e2a\u65f6\u95f4\u6b65\n    for t in range(max_number_of_steps):\n        #initial the figure\n        env.render()    # \u66f4\u65b0\u5e76\u6e32\u67d3\u6e38\u620f\u753b\u9762\n        #put the result in the figure\n        frames2.append(env.render(mode='rgb_array')) #\u628a\u7ed3\u679c\u653e\u5728\u56fe\u91cc\u9762\n        #decide the move way of the cart\n        action = np.random.choice([0, 1])   # \u968f\u673a\u51b3\u5b9a\u5c0f\u8f66\u8fd0\u52a8\u7684\u65b9\u5411\n        #catch the result of thie round\n        observation, reward, done, info = env.step(action)  # \u83b7\u53d6\u672c\u6b21\u884c\u52a8\u7684\u53cd\u9988\u7ed3\u679c\n        episode_reward += reward\n        if done:\n            print('%d Episode finished after %f time steps \/ mean %f' % (episode, t + 1, last_time_steps.mean()))\n            #up date the stack of the most recent 100 round games\n            last_time_steps = np.hstack((last_time_steps[1:], [episode_reward]))    # \u66f4\u65b0\u6700\u8fd1100\u573a\u6e38\u620f\u7684\u5f97\u5206stack\n            break\n    # if the average reward higher than 195\n    # \u5982\u679c\u6700\u8fd1100\u573a\u5e73\u5747\u5f97\u5206\u9ad8\u4e8e195\n    if (last_time_steps.mean() >= goal_average_steps):\n        print('Episode %d train agent successfuly!' % episode)\n        break\n\nprint('Failed!')","193c8e8f":"display.HTML(display_anim(frames2))","0f49f034":"frame25 = []\nenv = gym.make(\"CartPole-v0\")\nenv.reset()\n\nmax_number_of_steps = 200  \ngoal_average_steps = 195\nnum_consecutive_iterations = 100\n\n#we will have 200 episode in total\n#it should be higher but colab will crash if I do that much\nnum_episodes = 200 # \u5171\u8fdb\u884c200\u573a\u6e38\u620f\uff0c\u56e0\u4e3a\u5230200\u573a\u57fa\u672c\u4e0a\u5c31\u4e0d\u80fd\u518d\u63d0\u5347\u4e86\n#the reason I choose 200 is becasue it will not improve alot after that\n#also my system will crash if I do alot of rounds\nlast_time_steps = np.zeros(num_consecutive_iterations)  \n# Only store the reward of the most recent 100 episodes","23320a4c":"def get_action(observation):\n    pos, v, ang, rot = observation\n    return 0 if ang < 0 else 1 # \u67f1\u5b50\u5411\u5de6\u503e\u659c\u5247\u5c0f\u8eca\u5de6\u79fb\uff0c\u5426\u5247\u53f3\u79fb\n#if the pole failling to the left, then the cart move to the left","d9b08bc3":"for episode in range(num_episodes):\n    observation = env.reset()\n    episode_reward = 0\n    for t in range(max_number_of_steps):\n        env.render()\n        #frame25.append(env.render(mode='rgb_array'))\n        action = get_action(observation)\n        observation, reward, done, info = env.step(action)\n        episode_reward += reward\n        if done:\n            print('%d Episode finished after %f time steps \/ mean %f' % (episode, t + 1, last_time_steps.mean()))\n            #up date the stack of the most recent 100 round games\n            last_time_steps = np.hstack((last_time_steps[1:], [episode_reward]))    # \u66f4\u65b0\u6700\u8fd1100\u573a\u6e38\u620f\u7684\u5f97\u5206stack\n            break\n\nenv.close()","f96e3068":"#display.HTML(display_anim(frame25))","69df4fcd":"frames3 = []\n#env = gym.make(\"CartPole-v0\")\nenv.reset()\n\nmax_number_of_steps = 200   # \u6bcf\u4e00\u573a\u6e38\u620f\u7684\u6700\u9ad8\u5f97\u5206\ngoal_average_steps = 195\nnum_consecutive_iterations = 100\n\n#we will have 600 episode in total\n#it should be higher but colab will crash if I do that much\nnum_episodes = 200 # \u5171\u8fdb\u884c200\u573a\u6e38\u620f\nlast_time_steps = np.zeros(num_consecutive_iterations)  \n# Only store the reward of the most recent 100 episodes","34f93b7a":"#discretization\n#It is easy to increase and decrease the discrete characteristics and easy to iterate the model quickly;\n#The model will be more stable after the characteristics are discretized\n\n# q_table\u662f\u4e00\u4e2a256*2\u7684\u4e8c\u7ef4\u6570\u7ec4\n# \u79bb\u6563\u5316\u540e\u7684\u72b6\u6001\u5171\u67094^4=256\u4e2d\u53ef\u80fd\u7684\u53d6\u503c\uff0c\u6bcf\u79cd\u72b6\u6001\u4f1a\u5bf9\u5e94\u4e00\u4e2a\u884c\u52a8\n# q_table[s][a]\u5c31\u662f\u5f53\u72b6\u6001\u4e3as\u65f6\u4f5c\u51fa\u884c\u52a8a\u7684\u6709\u5229\u7a0b\u5ea6\u8bc4\u4ef7\u503c\n# \u6211\u4eec\u7684AI\u6a21\u578b\u8981\u8bad\u7ec3\u5b66\u4e60\u7684\u5c31\u662f\u8fd9\u4e2a\u6620\u5c04\u5173\u7cfb\u8868\n\n#q_table is a two-dimension array\n#after Feature discrete there will be 4^4=256 values \n#the AI model is going to train and learn the mapping table\n\nq_table = np.random.uniform(low=-1, high=1, size=(4 ** 4, env.action_space.n))\n\n# \u5206\u7bb1\u5904\u7406\u51fd\u6570\uff0c\u628a[clip_min,clip_max]\u533a\u95f4\u5e73\u5747\u5206\u4e3anum\u6bb5\uff0c\u4f4d\u4e8ei\u6bb5\u533a\u95f4\u7684\u7279\u5f81\u503cx\u4f1a\u88ab\u79bb\u6563\u5316\u4e3ai\ndef bins(clip_min, clip_max, num):\n    return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n\n# \u79bb\u6563\u5316\u5904\u7406\uff0c\u5c06\u75314\u4e2a\u8fde\u7eed\u7279\u5f81\u503c\u7ec4\u6210\u7684\u72b6\u6001\u77e2\u91cf\u8f6c\u6362\u4e3a\u4e00\u4e2a0~~255\u7684\u6574\u6570\u79bb\u6563\u503c\ndef digitize_state(observation):\n    # \u5c06\u77e2\u91cf\u6253\u6563\u56de4\u4e2a\u8fde\u7eed\u7279\u5f81\u503c\n    cart_pos, cart_v, pole_angle, pole_v = observation\n    # \u5206\u522b\u5bf9\u5404\u4e2a\u8fde\u7eed\u7279\u5f81\u503c\u8fdb\u884c\u79bb\u6563\u5316\uff08\u5206\u7bb1\u5904\u7406\uff09\n    digitized = [np.digitize(cart_pos, bins=bins(-2.4, 2.4, 4)),\n                 np.digitize(cart_v, bins=bins(-3.0, 3.0, 4)),\n                 np.digitize(pole_angle, bins=bins(-0.5, 0.5, 4)), #angle represent by radian\n                 np.digitize(pole_v, bins=bins(-2.0, 2.0, 4))]\n    # \u5c064\u4e2a\u79bb\u6563\u503c\u518d\u7ec4\u5408\u4e3a\u4e00\u4e2a\u79bb\u6563\u503c\uff0c\u4f5c\u4e3a\u6700\u7ec8\u7ed3\u679c\n    return sum([x * (4 ** i) for i, x in enumerate(digitized)])","84a867ee":"# \u6839\u636e\u672c\u6b21\u7684\u884c\u52a8\u53ca\u5176\u53cd\u9988\uff08\u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u72b6\u6001\uff09\uff0c\u8fd4\u56de\u4e0b\u4e00\u6b21\u7684\u6700\u4f73\u884c\u52a8\n# according to the action and the reward\n# return the  next best reward\ndef get_action(state, action, observation, reward):\n    #catch the next state of the next time step\uff0cdiscretization\n    next_state = digitize_state(observation)    # \u83b7\u53d6\u4e0b\u4e00\u4e2a\u65f6\u95f4\u6b65\u7684\u72b6\u6001\uff0c\u5e76\u5c06\u5176\u79bb\u6563\u5316\n    #check the Q-table and find the best action\n    next_action = np.argmax(q_table[next_state])    # \u67e5\u8868\u5f97\u5230\u6700\u4f73\u884c\u52a8\n    #learning and update the Q-table\n    #learning parametert\n    #mathematically shown using the symbol alpha\n    alpha = 0.25     # \u5b66\u4e60\u7cfb\u6570\u03b1\n    #The learning rate is the hyperparameter that guides \n    #how to adjust the network weight by the gradient of the loss function. \n    #discount rate\n    gamma = 0.99    # \u62a5\u916c\u8870\u51cf\u7cfb\u6570\u03b3\n    q_table[state, action] = (1 - alpha) * q_table[state, action] + alpha * (reward + gamma * q_table[next_state, next_action])\n    return next_action, next_state","a8a30f26":"for episode in range(num_episodes):\n    observation = env.reset()   \n    state = digitize_state(observation)    \n    action = np.argmax(q_table[state])      \n    episode_reward = 0\n  \n    for t in range(max_number_of_steps):\n        env.render()  \n        #frames3.append(env.render(mode='rgb_array'))\n        observation, reward, done, info = env.step(action)  \n        action, state = get_action(state, action, observation, reward)  \n        episode_reward += reward\n        if done:\n            print('%d Episode finished after %f time steps \/ mean %f' % (episode, t + 1, last_time_steps.mean()))\n            last_time_steps = np.hstack((last_time_steps[1:], [episode_reward]))  \n            break\n           \n        if (last_time_steps.mean() >= goal_average_steps):\n            print('Episode %d train agent successfuly!' % episode)\n            break\n\nprint('Failed!')","84705d7f":"#display.HTML(display_anim(frames3))\n#it will take some time to run, I already showed how it will work before so you can run this by yourself:)","9b9057e1":"frames4 = []\n#env = gym.make(\"CartPole-v0\")\nenv.reset()\n\nmax_number_of_steps = 200   # \u6bcf\u4e00\u573a\u6e38\u620f\u7684\u6700\u9ad8\u5f97\u5206\n#the max reward for every episode\n#\u83b7\u80dc\u7684\u6761\u4ef6\u662f\u6700\u8fd1100\u573a\u5e73\u5747\u5f97\u5206\u9ad8\u4e8e195\n#if the reward of recent 100 episodes are higher than 195\n#means the agent win\ngoal_average_steps = 195\nnum_consecutive_iterations = 100\n\n#we will have 200 episode in total\n#it should be higher but colab will crash if I do that much\nnum_episodes = 600 # \u5171\u8fdb\u884c600\u573a\u6e38\u620f\nlast_time_steps = np.zeros(num_consecutive_iterations)  \n# Only store the reward of the most recent 100 episodes","5660fda2":"q_table = np.random.uniform(low=-1, high=1, size=(4 ** 4, env.action_space.n))\n\ndef bins(clip_min, clip_max, num):\n    return np.linspace(clip_min, clip_max, num + 1)[1:-1]\n\ndef digitize_state(observation):\n    cart_pos, cart_v, pole_angle, pole_v = observation\n    digitized = [np.digitize(cart_pos, bins=bins(-2.4, 2.4, 4)),\n                 np.digitize(cart_v, bins=bins(-3.0, 3.0, 4)),\n                 np.digitize(pole_angle, bins=bins(-0.5, 0.5, 4)),\n                 np.digitize(pole_v, bins=bins(-2.0, 2.0, 4))]\n    return sum([x * (4 ** i) for i, x in enumerate(digitized)])","e8141366":"def get_action(state, action, observation, reward, episode):\n    next_state = digitize_state(observation)\n    #set up epsilon '\u03b5' in greedy strategy\n    epsilon = 0.5 * (0.99 ** episode) # 0.5 initial\n    #\u968f\u7740episode\u589e\u5927\uff0cepsilon\u4f1a\u51cf\u5c0f\n    if epsilon <= np.random.uniform(0, 1):\n        next_action = np.argmax(q_table[next_state])\n    else:\n        next_action = np.random.choice([0, 1])\n    #\u5c0f\u4e8e\u63a2\u7d22\u7387\u5c31\u7ee7\u7eed \u5927\u4e8e\u5c31\u5229\u7528\n    #learning and update Q-table\n    alpha = 0.2    \n    gamma = 0.99   \n    q_table[state, action] = (1 - alpha) * q_table[state, action] + alpha * (reward + gamma * q_table[next_state, next_action])\n    \n    return next_action, next_state","28c35289":"for episode in range(num_episodes):\n    observation = env.reset()   \n    state = digitize_state(observation)  # \u83b7\u53d6\u521d\u59cb\u72b6\u6001\u503c\n    action = np.argmax(q_table[state]) # \u6839\u636e\u72b6\u6001\u503c\u4f5c\u51fa\u884c\u52a8\u51b3\u7b56\n    episode_reward = 0\n\n    for t in range(max_number_of_steps):\n        env.render()   \n        #frames4.append(env.render(mode='rgb_array'))\n        observation, reward, done, info = env.step(action) \n        # \u5bf9\u81f4\u547d\u9519\u8bef\u884c\u52a8\u8fdb\u884c\u6781\u5927\u529b\u5ea6\u7684\u60e9\u7f5a\uff0c\u8ba9\u6a21\u578b\u6068\u6068\u5730\u5438\u53d6\u6559\u8bad\n        # set up punishment on the error(wrong action)\n        # make the model learn\n        if done:\n            reward = -100\n        action, state = get_action(state, action, observation, reward, episode)  # \u4f5c\u51fa\u4e0b\u4e00\u6b21\u884c\u52a8\u7684\u51b3\u7b56\n        episode_reward += reward\n        if done:\n            print('%d Episode finished after %f time steps \/ mean %f' % (episode, t + 1, last_time_steps.mean()))\n            last_time_steps = np.hstack((last_time_steps[1:], [episode_reward]))  \n            break\n           \n        if (last_time_steps.mean() >= goal_average_steps):\n            print('Episode %d train agent successfuly!' % episode)\n            break\n\nprint('Failed!')","4a2f7f3b":"#display.HTML(display_anim(frames4))","abe8a7fd":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior() \nfrom collections import deque #double-ended queue\uff0c\u53cc\u7aef\u961f\u5217\uff0cdata structure\nimport random\n\nEPISODE = 5000 # episode limitation, actually don't need that much for this one","8e834e29":"class DeepQNetwork:\n    #it can be quicker if I use learning_rate = 0.001\n    learning_rate=0.25\n    gamma = 0.99 # discount factor\n    action_list = None\n\n    # \u6267\u884c\u6b65\u6570\n    step_index = 0\n\n    # epsilon\u7684\u8303\u56f4\n    initial_epsilon = 0.5 # starting value of epsilon\n    final_epsilon = 0.01 #final value\n    #explore = 5000\n\n    # \u7ecf\u9a8c\u56de\u653e\u5b58\u50a8\n    memory_size = 5000\n    BATCH = 32 #number of minibatch\n    #full gd,sgd and mini-batch\n    #it is said that GPU over batch with power of 2 can give better performance\n    #The best batch size is 8, 32 and 16 for a speech, a picture or a natural language task\n    \n    # \u795e\u7ecf\u7f51\u7edc\n    state_input = None\n    Q_val = None\n    y_input = None\n    optimizer = None\n    cost = 0\n    session = tf.Session()\n    cost_history = []\n\n    def __init__(self, env):\n        #init experience replay\n        self.replay_memory_store = deque()\n        #init some parameters\n        self.state_dim = env.observation_space.shape[0]\n        self.action_dim = env.action_space.n\n        self.action_list = np.identity(self.action_dim)\n        self.epsilon = self.initial_epsilon  # epsilon_greedy-policy\n        #explore at the begining then explorit\n        self.create_network()\n        self.create_training_method()\n        #init session\n        self.session = tf.InteractiveSession()\n        self.session.run(tf.global_variables_initializer())\n\n    def create_network(self):\n        #input layer\n        self.state_input = tf.placeholder(shape=[None, self.state_dim], dtype=tf.float32)\n        #  \u7b2c\u4e00\u5c42 first layer\n        neuro_layer_1 = 20 #hidden layer\u670920\u4e2a\u795e\u7ecf\u5143\n        w1 = tf.Variable(tf.random_normal([self.state_dim, neuro_layer_1]))\n        #weight variable\n        b1 = tf.Variable(tf.zeros([neuro_layer_1]) + 0.1)\n        #bias variable\n        l1 = tf.nn.relu(tf.matmul(self.state_input, w1) + b1)#Rectified Linear Unit\n        #The relu is to increase the nonlinear relationship between the layers of the neural network\n        #hidden layers\n        #  \u7b2c\u4e8c\u5c42 second layer\n        w2 = tf.Variable(tf.random_normal([neuro_layer_1, self.action_dim]))\n        b2 = tf.Variable(tf.zeros([self.action_dim]) + 0.1)\n        # output layer\n        self.Q_val = tf.matmul(l1, w2) + b2\n\n    def egreedy_action(self, state):\n        self.epsilon -= (0.5 - 0.01) \/ 5000\n        Q_val_output = self.session.run(self.Q_val, feed_dict={self.state_input: [state]})[0]\n        if random.random() <= self.epsilon:\n            return random.randint(0, self.action_dim - 1)  # \u5de6\u95ed\u53f3\u95ed\u533a\u95f4\uff0cnp.random.randint\u4e3a\u5de6\u95ed\u53f3\u5f00\u533a\u95f4\n        else:\n            return np.argmax(Q_val_output) # random output is depend\n\n    def max_action(self, state):\n        Q_val_output = self.session.run(self.Q_val, feed_dict={self.state_input: [state]})[0]\n        action = np.argmax(Q_val_output)\n        return action # output based on NN, which is the max Q value\n\n    def create_training_method(self):\n        self.action_input = tf.placeholder(shape=[None, self.action_dim], dtype=tf.float32)\n        self.y_input = tf.placeholder(shape=[None], dtype=tf.float32)  # ???\u662f[None]\u5417\uff1f\n        Q_action = tf.reduce_sum(tf.multiply(self.Q_val, self.action_input), reduction_indices=1)\n        self.loss = tf.reduce_mean(tf.square(self.y_input - Q_action))\n        self.optimizer = tf.train.AdamOptimizer(0.0005).minimize(self.loss)\n\n    def perceive(self, state, action, reward, next_state, done):\n        cur_action = self.action_list[action: action + 1]\n        self.replay_memory_store.append((state, cur_action[0], reward, next_state, done))\n        if len(self.replay_memory_store) > self.memory_size:\n            self.replay_memory_store.popleft() #based on deque\n        # \u5982\u679c\u8d85\u8fc7\u8bb0\u5fc6\u7684\u5bb9\u91cf\uff0c\u5219\u5c06\u6700\u4e45\u8fdc\u7684\u8bb0\u5fc6\u79fb\u9664\u3002\n        # \u79fb\u53bb\u5e76\u4e14\u8fd4\u56de\u4e00\u4e2a\u5143\u7d20\uff0cdeque, \u6700\u5de6\u4fa7\u7684\u90a3\u4e00\u4e2a\n        if len(self.replay_memory_store) > self.BATCH:\n            self.train_Q_network()\n        \"\"\"\n        One of the most important things in perceive is memory. \n        Then train according to the situation. \n        Here we require that training begin \n        as soon as the data stored is larger than the Batch size\n        \"\"\"\n            \n    def train_Q_network(self):\n        self.step_index += 1\n        # obtain random mini-batch from replay memory\n        mini_batch = random.sample(self.replay_memory_store, self.BATCH)\n        state_batch = [data[0] for data in mini_batch]\n        action_batch = [data[1] for data in mini_batch]\n        reward_batch = [data[2] for data in mini_batch]\n        next_state_batch = [data[3] for data in mini_batch]\n\n        # calculate y\n        y_batch = []\n        Q_val_batch = self.session.run(self.Q_val, feed_dict={self.state_input: next_state_batch})  # \u9884\u4f30\u4e0b\u4e00\u4e2a\u72b6\u6001\u7684Q\u503c\n        for i in range(0, self.BATCH):\n            done = mini_batch[i][4]\n            if done:\n                y_batch.append(reward_batch[i])\n            else:\n                y_batch.append(reward_batch[i] + self.gamma * np.max(Q_val_batch[i]))  # \u9009\u62e9\u6700\u4f18\u7684Q\u51fd\u6570\u8fdb\u884c\u66f4\u65b0\n\n        _, self.cost = self.session.run([self.optimizer, self.loss], feed_dict={\n            self.y_input: y_batch,\n            self.state_input: state_batch,\n            self.action_input: action_batch\n        })","86082851":"#main function\nTEST = 20 # The number of experiment test every 100 episode\nSTEP = 200 # Step limitation in an episode\n\ndef main():\n    #initialize openAI gym env and dqn agaent\n    env = gym.make('CartPole-v0')\n    agent = DeepQNetwork(env)\n    \n    for i in range(EPISODE):\n        #initialize task\n        state = env.reset()\n        #Train\n        for step in range(STEP):\n            # env.render()\n            # e-greedy action for train\n            action = agent.egreedy_action(state) #\u8f93\u5165\u52a8\u4f5c\n            next_state, reward, done, _ = env.step(action)\n            #define reward agent\n            agent.perceive(state, action, reward, next_state, done) #\u8f93\u5165\u72b6\u6001\n            state = next_state\n            if done:\n                break\n        # Test every 100 episodes\n        if i % 100 == 0:\n            total_reward = 0\n            for _ in range(TEST):\n                state = env.reset()\n                for _ in range(STEP):\n                    env.render()\n                    action = agent.max_action(state) # totally based on DNQ, no explore\n                    # direct action for test\n                    # also don't need perceive to train the model\n                    state, reward, done, _ = env.step(action)\n                    total_reward += reward\n                    if done:\n                        break\n            ave_reward = total_reward \/ TEST\n            print('episode: ', i, 'Evaluation Average Reward:', ave_reward)\n            if ave_reward >= 200:\n                break\n    # agent.print_loss()\n\nif __name__ == '__main__':\n    main()","38985270":"frames5 = []\nenv = gym.make(\"CartPole-v0\")\nenv.reset()\n#max_number_of_steps = 200 \n#goal_average_steps = 195\n#num_consecutive_iterations = 100\n#num_episodes = 200 \n#last_time_steps = np.zeros(num_consecutive_iterations)  ","5267b886":"import time\n\ndef evaluate_given_parameter_by_sign(env, weight):\n    observation = env.reset()\n    total_reward = 0.\n    for t in range(1000):\n        env.render()\n        weighted_sum = np.dot(weight, observation)#\u77e9\u9635\u79ef\n        #weighted_sum = np.dot(weights[:4], observation) + weights[4] # \u8ba1\u7b97\u52a0\u6743\u548c\n        if weighted_sum >= 0: #\u5927\u4e8e0\u9009\u53d6\u52a8\u4f5c1\n            action = 1\n        else:\n            action = 0\n        \n        observation, reward, done, info = env.step(action)\n        total_reward += reward\n        if done:\n            break\n    return total_reward\n\ndef random_guess():\n    env = gym.make('CartPole-v0')\n    np.random.seed(10)\n    best_reward = -100.0\n\n    for iiter in range(1000):\n        weight = np.random.rand(4)\n\n        cur_reward = evaluate_given_parameter_by_sign(env, weight)\n        if cur_reward > best_reward:\n            best_reward = cur_reward\n            best_weight = weight\n\n        if best_reward == 200:\n            break\n        \n    print(\"Random guess algorithm best reward\", best_reward)\n    print(\"Random guess algorithm best weight\", best_weight)\n      \nstart = time.time()\nrandom_guess()\nend = time.time()\nprint ('The total running time:', end-start)\n\n\"\"\"\nPerform 1000 random guesses that require the number of times to reach 200reward, \nand the number of times to reach 200reward within 20 steps \naccounts for approximately 75%\n\"\"\"","aa526227":"def hill_climbing():\n    env = gym.make('CartPole-v0')\n    best_reward = -100.0\n    np.random.seed(10)\n    best_weight = np.random.rand(4)\n\n    for iiter in range(1000):\n        weight = best_weight + np.random.normal(0, 0.01, 4)\n        #\u901a\u8fc7\u722c\u5c71\u7b97\u6cd5\u9009\u53d6\u6743\u503c\uff08\u5728\u5f53\u524d\u6700\u597d\u7684\u6743\u503c\u4e0a\u52a0\u5165\u968f\u673a\u503c\uff09\n\n        cur_reward = evaluate_given_parameter_by_sign(env, weight)\n        #\u83b7\u53d6\u5f53\u524d\u6743\u503c\u7684\u6a21\u578b\u63a7\u5236\u5956\u52b1\u548c\n        if cur_reward > best_reward:\n            best_reward = cur_reward\n            best_weight = weight\n\n        if best_reward == 200:\n            break\n        \n    print(\"Hill climbing algorithm best reward\", best_reward)\n    print(\"Hill climbing algorithm best weight\", best_weight)\n\nstart = time.time()\nhill_climbing()\nend = time.time()\nprint ('The total running time:', end-start)\n\n\"\"\"\nRandom guesses to reach the 200reward for \n1000 times are performed. \nApproximately 29% of The Times to reach the 200reward \nwithin 20 steps are about the same, \nand 51% of The Times fail to reach the 200reward after 200 steps\n\"\"\"","0dd6b7f0":"#### Hand-Made Policy\n>\n>In this policy, we will let the agent check the obeservation and get action based on that: When the pole failling to the left, the cart will move to the left, otherwise.","ebe4bca1":"#### Maze - How Agent really work\n\n> As I mentioned, agent including policy, value function and model.\n\n![m1](https:\/\/pic4.zhimg.com\/80\/v2-e2c5d249628098891cbb6747a8d4ae33_1440w.jpg)\n\n> The goal of agent is find the shortest way from Start to Goal\n\n> **Policy**(Deterministic or Stochastic)\n    >* As mentioned before, Policy takes state as the input and decision as the output. It has two specific forms:\n    >* Deterministic policy\uff1aEach state has a definite decision\n![m2](https:\/\/www.zhihu.com\/equation?tex=a%3D%5Cpi%28s%29)\n    >* Stochastic policy\uff1aFor each state, we give the probability of what kind of decision will be make\n![m3](https:\/\/www.zhihu.com\/equation?tex=%5Cpi%28a%7Cs%29%3DP%5BA%3Da%7CS%3Ds%5D)\n>![m4](https:\/\/pic1.zhimg.com\/80\/v2-72352add2fb6a29d5094f984c9f73944_1440w.jpg)\n    >* For Deterministic policy, we use arrows to specify the output of each state, which is an action. As for Stochastic policy, its output is the probability of a state happening.\n\n> **Value Function**\n    >* When you need to select $S_{1}$ and $S_{2}$,they correspond to actions $A_{1}$, $A_{2}$ respectively, how should we select them? \n    >* According to the expectation of the future rewards of the two states, we choose the highest expectation of rewards. The expectation of this future reward is its **value**. This is a function of state (or maybe a function of state and Action, we'll call it an action-value function later).\n    >* Here is an example of a value function:\n![m5](https:\/\/www.zhihu.com\/equation?tex=v_%5Cpi%28s%29%3DE_%5Cpi%5BR_%7Bt%2B1%7D%2B%5Cgamma+R_%7Bt%2B2%7D%2B%5Cgamma%5E2+R_%7Bt%2B3%7D%2B...%7CS_t%3Ds%5D)\n    >* Where $\\gamma$ is the discount factor, which is used to express the step size of predicted. The further the step is, the less important the reward will be\n    >* After the states obtained by different actions pass through the value function, we can get the expected value of the reward. By comparing different values, we can get the maximum value and then we can get our policy\n![m6](https:\/\/pic4.zhimg.com\/80\/v2-53a1b3149bb1e1d9519e0816611afdb7_1440w.jpg)\n\n> **Model**\n    >* Models are used to predict changes in the environment to determine what our next action should be.\n    >* Model can predict what the next state of the system will be and what the reward will be for the next state of the system. Specifically speaking, there are two components. \n    >* First, the probability of the next state can be predicted:\n    ![m7](https:\/\/www.zhihu.com\/equation?tex=P%5Ea_%7Bss%5E%7B%27%7D%7D%3DP%5BS_%7Bt%2B1%7D%3Ds%5E%7B%27%7D%7CS_t%3Ds%2CA_t%3Da%5D)\n    >* Second, the expectation of the next reward can be predicted:\n    ![m8](https:\/\/www.zhihu.com\/equation?tex=R%5Ea_s%3DE%5BR_%7Bt%2B1%7D%7CS_t%3Ds%2CA_t%3Da%5D)\n    ![m9](https:\/\/pic4.zhimg.com\/80\/v2-74c6a401c7b3b32f8926dc8badc2022b_1440w.jpg)\n    >* Model models the Dynamics of the environment to determine what a given $S_{t}$ will produce when performing a specific action $S_{t+1}$\n    >* Model only models the environment, and the evaluation methods of our Policy and value function will be different if there is a Model","ce2c1044":"### Set up CartPole-v0\n![cp](https:\/\/miro.medium.com\/max\/600\/1*Q9gDKBugQeYNxA6ZBei1MQ.png)","dd486217":"#### Random Guessing Algorithm & Hill Climbing Algorithm\n>\n>There is another way to do this. By adding four weights, we can change the policy by changing the weight value\u3002\n>\n>$H_{sum}=\\omega_{a}x+\\omega_{2}\\theta+\\omega_{3}x_{2}+\\omega_{4}\\theta_{2}$\n>\n>If the $H_{sum}$ is positive, the output will be 1, otherwise, 0.\n>\n>So we have hill climbing algorithm, like 'partial greedy policy' it is to add a set of random values to the current optimal weight during each iteration. If the addition of this set of values makes the duration of the effective control pole standing longer, then update it to be the optimal weight. If no improvement has been made, the original value will remain unchanged until the end of the iteration. In the iterative process, the parameters of the model are optimized continuously, and finally an optimal set of weights is obtained as the solution of the control model.\n>![mc](https:\/\/raw.githubusercontent.com\/umutto\/umutto.github.io\/master\/static\/images\/blog_1_hill_climb\/loss_function_space_fig_1.jpg)\n\n\n>Here's an example of the greedy algorithm failing: Suppose you have a backpack with a capacity of 100. You have the following items:\n>\n> * Diamond, value 1000, weight 90 (density = 11.1)\n> * 5 gold COINS, value 210, weight 20 (per density = 10.5)\nThe greedy algorithm will then complete the diamond and give a value of 1000. But the best solution is to include 5 gold COINS and give a value of 1050.\n>\n>The climbing algorithm will generate the initial solution - just pick a few items at random (make sure they are under the weight limit). Then evaluate the solution - that is, determine the value. Generate adjacent solutions. For example, try swapping one item for another (making sure you're still within the weight limit). If it has a higher value, use this selection and start again.","76692558":"#### Break the Bricks - How reinforcement learning thinking\n\n![rl3](https:\/\/drive.google.com\/uc?id=1c61MrzLBfqg5KFXFMNiVKm0FRXgM31Nz)\n\n> Suppose a neural network is going play this game, the input would be the screen image, and the output would be three **actions**: left, right, or firing the ball. Obviously, this is a classification. For each frame of image, we calculate one action (categorizing screen data).\n>\n>Unlike supervised learning, which has specific tag for each sample, reinforcement learning doesn't have tags. But have a **time-delay** reward, In the game we often sacrifice the current reward for a larger reward in the future. Because when our ball hits the brick and gets the reward, the paddle actually didn't move, the reward is from the **previous sequence of actions**. This is the **Credit Assignment Problem**, in which **the current action is responsible for getting more rewards in the future**.\n>\n>And when we find a strategy that gives the game a good reward, do we stick with the current strategy, or do we explore new strategies to get more rewards? That is the problem with **explorer-exploit Dilemma**.\n>\n>Reinforcement learning is an important model for solving such problems, and it is drawn from our human experience. \n>\n>In real life, if we are rewarded for doing something, we are more likely to do it. If your dog brings you your shoes in the morning, and you say \"Good dog\" as an reward, the dog will be more likely to bring it to you the next day. \n\n>In this game:\n> * Agent: The paddle\n> * State: The position of paddle and the state of bricks\n> * Action: The way to move the paddle\n> * Environment: The whole process of the game what the state of pipes\n> * Rewards: The more brick you hit, the higher score","7efe7b2c":"#### Deep Q-learning\n>\n>![111](https:\/\/pic1.zhimg.com\/80\/5473510ac001e787c034580656e49b8c_1440w.png)\n>\n>To make the model easier and fit cartpole game:\n> * Preprocessing is required. So you're just going to get an Observation as a state input.\n> * Use basic MLP neural network, instead of the convolutional neural network.","3ff83f02":"### reference\n\n[1] Exact DP: Dynamic p.Programming and optimal control, Vol. 1 (2017), Vol. 2 (2012)\n\n[2] https:\/\/iq.opengenus.org\/introduction-to-q-learning-and-reinforcement-learning\/\n\n[3] https:\/\/towardsdatascience.com\/simple-reinforcement-learning-q-learning-fcddc4b6fe56\n\n[4] https:\/\/zhuanlan.zhihu.com\/p\/24808797\n\n[5] https:\/\/www.zhihu.com\/question\/26408259\n\n[6] https:\/\/zhuanlan.zhihu.com\/p\/28084942\n\n[7] https:\/\/zhuanlan.zhihu.com\/p\/56425081\n\n[8] https:\/\/www.zhihu.com\/question\/41775291\n\n[9] https:\/\/www.kaggle.com\/abechanta\/setup-openai-gym-on-kaggle\n\n[10] http:\/\/gym.openai.com\/\n\n[11] Reinforcement Learning: An Introduction (2018)\n\n[12] Hands-On Reinforcement Learning with Python: Master reinforcement and deep reinforcement learning using OpenAI Gym and TensorFlow [M]\n\n[13] https:\/\/colab.research.google.com\/drive\/18LdlDDT87eb8cCTHZsXyS9ksQPzL3i6H#scrollTo=bYDe8axLdI1E (show image)\n\n[14] https:\/\/www.kaggle.com\/abechanta\/setup-openai-gym-on-kaggle\n\n[15] https:\/\/colab.research.google.com\/drive\/1GLlB53gvZaUyqMYv8GmZQJmshRUzV_tg#scrollTo=Hb3yJjoIk7ls\n\n[16] https:\/\/zhuanlan.zhihu.com\/p\/28084942 (mdp)\n\n[17] https:\/\/blog.csdn.net\/gg_18826075157\/article\/details\/78163386\n\n[18] https:\/\/blog.csdn.net\/njshaka\/article\/details\/89237941 (iteration)\n\n[19] https:\/\/www.jiqizhixin.com\/graph\/technologies\/444ad8d1-7262-46c9-94b7-fe0aa9227006\nhttps:\/\/blog.csdn.net\/danieljianfeng\/article\/details\/46873595\nhttps:\/\/zhuanlan.zhihu.com\/p\/35724704 (learning rate)\n\n[20] https:\/\/blog.csdn.net\/nodototao\/article\/details\/85790972 (box)\n\n[21] https:\/\/blog.csdn.net\/tealex\/article\/details\/78804360?utm_medium=distribute.pc_relevant.none-task-blog-searchFromBaidu-2.control&depth_1-utm_source=distribute.pc_relevant.none-task-blog-searchFromBaidu-2.control (minibatch)\n\n[22] https:\/\/github.com\/gh877916059\/Reinforcement-learning-demos-annotated\/blob\/master\/cartpole.py (discretization)\n\n[23] https:\/\/github.com\/chenrudan\/deep-learning\/tree\/master\/rl\/cartpole (hill climbing)","25a7945b":"#### Value Iteration - Q-learning\n>\n>![wss](https:\/\/pic1.zhimg.com\/v2-3919d5ce017f134a073c8fcf956503a8_r.jpg)\n>The value iteration is analyzed from the perspective of strategy iteration. In the figure above, the update of the value function of the inner Loop Loop can be divided into two steps:\n>\n>Strategy promotion: According to the value function before the update, the strategy promotion is carried out to obtain the greedy strategy Policy evaluation: According to the greedy strategy, select the value corresponding to the greedy Action A in state S, and update the value function corresponding to S The value iteration method implements the above two processes in a formula.\n>\n>If you look at value iteration from another perspective, the formula on line 5 is actually Bellman Optimality Equation, The value iteration algorithm takes it as the value function update formula.","fd1dd511":">The difference between policy iteration and value iteration can be explained in this picture:\n>\n>![cc](https:\/\/pic4.zhimg.com\/80\/v2-bd9583f4e6233fff8489443937b5d09b_1440w.jpg)\n>\n>As you can see, the policy stopped update after step3, but the value is still renew.\n>* Policy Iteration is usually the Policy evaluation and Policy improvement performed alternately until convergence\n> * Value iteration is usually to find Optimal Value function. Once policy extraction, they do not have to be executed alternately, because the Value function is Optimal and the strategy is usually Optimal\n> * Search for optimal value function can also be regarded as a combination of policy improvement(due to Max) and truncated policy evaluation","22053c1c":"> To do deep Q-learning, we need a class DeepQNetwork\n>class DQN():\n>#DQN Agent\n> * def __init__(self, env): initial\n> * def create_network(self): #build Q-network\n> * def create_training_method(self): \u521b\u5efa\u8bad\u7ec3\u65b9\u6cd5 build training method\n> * def perceive(self,state,action,reward,next_state,done): #\u611f\u77e5\u5b58\u50a8\u4fe1\u606f, store information\/state\n> * def train_Q_network(self): #\u8bad\u7ec3\u7f51\u7edc train the Qnetwork\n> * def egreedy_action(self,state): #\u8f93\u51fa\u5e26\u968f\u673a\u7684\u52a8\u4f5c action with random\n> * def max_action(self,state): #\u8f93\u51fa\u52a8\u4f5c action\n>\n>One of the most important function of DNQ is to store the values, and minibatch while training. So we also need a store procedure.\n>\n> * self.replay_buffer = deque(): Double Ended Queue, non-null","bc0e1248":"### Reinforcement learning\n> To know the Q-learning, we should first know something about reinforcement learning. \n> \n#### What is Reinforcement learning?\n> Reinforcement Learning is defined as a Machine Learning method that is concerned with how software agents should take actions in an environment.\n> It is a class of algorithms.\n\n![image.png](https:\/\/drive.google.com\/uc?id=1-PqwGaynrpJmvZ7aZyf7HVpurZJbhly1)\n\n#### Differences between RL and other ML\n> The biggest difference you'll find between reinforcement learning and supervised learning and unsupervised learning is that Reinforcement learning doesn't require a lot of \"data feeding.\" It's about trying to learn certain skills on its own.\n> \n> * **Unsupervised**: We usually only get a reward signal. Each action of the system can only get a scalar that represents the good or bad behavior of the system, such as 10 points, but we don't know what its best value.\n> * **Delayability**: The results are likely to be delayed for some time before we make a decision. Some of the bad effects are not entirely due to the current decision error, but may be due to an irreversible error in one of the previous steps.\n> * **Each sample is not independently codistributed**: The decision-making process and each information we get is not independently codistributed (I.I.D). It is a sequential sequence, and we do not simply put a bunch of training sets into it.\n> * **Action can have an impact on subsequent results**: Each action will have an impact on the environment, resulting in a completely different input sample the next time. For example, as a robot, this step where we continue to move forward and turn left will lead us to collect completely different data\n\n#### Develope History\n> * Optimal control (Bellman, Shannon, and other 1950s)\n> * Al\/Rl and decision\/control\/ DP ideas meet (late 80s-early 90s)\n> * First success, backgammon program (Tesauro, 1992, 1996)\n> * Algorithmic progress, analysis, applications (mid 90s)\n> * Machine learning, big data, robotics, deep neural networks (mid 00s)\n> * AlphaGo and Alphazero (Deep mind, google)\n","079b4871":">First we need to create a Q-table for our game\n>The columns represent four actions (left, right, up, and down). Rows represent states. The value of each cell will be the maximum expected future reward for that given state and action\n>\n>This is how we generate Q-table\n![qt](https:\/\/pic2.zhimg.com\/80\/v2-6ddacb66e0f886be482221a4281b2ed5_1440w.jpg)","593e2fe9":"#### The mainstream algorithm of reinforcement learning\n![image.png](https:\/\/drive.google.com\/uc?id=1AztxUGZ4WB3fT7OAlagJmfCSvK3LitT2)\n> Model-free vs. Model-based learning\n\n> *An important difference between the two classifications is whether an agent can fully understand or learn the model of its environment\n\n> * Model-based learning has an advanced knowledge of the environment, so planning can be considered in advance. However, the disadvantage is that if the Model is inconsistent with the real world, it will not perform well in the actual use scenarios.\n> * Model-free gives up Model learning and is less efficient than the former, but it is easier to implement and adjust to a good state in real situations. Lest the model learning approach become more popular, it is more widely developed and tested.","cb4295ec":"### The Bellman equation\n\n> **How do we choose which action to perform in this case?**\n> Assume we already know the expected reward. Then we choose the sequence of action which will eventually generate the highest reward.\n> \n> **Q-Value (a.k.a. Quality Value)** is the name given to the cumulative reward we will receive.\n> It can be mathematically stated as follows:\n>\n>![b1](https:\/\/iq.opengenus.org\/content\/images\/2019\/09\/1-1.PNG)\n\n> The equation above conveys that the Q-Value yielded from being at state s and selecting action a, is the immediate reward received, r(s, a), added to the highest Q-Value possible from state s' (the state the agent ended up after taking action a from state s).\n> \n> \u03b3 is the discount factor, which controls the importance of long-term rewards vs. the immediate one.\n> \n> This is the famous Bellman equation, its Wikipedia page provides a thorough explanation of how its mathematically derived.\n> \n> This equation is quite powerful and is of huge importance to us due to 2 important characteristics:\n> \n> * While we are still preserving the Markov States assumptions, the recursive nature of the Bellman Equation allows rewards from future states to propagate to far-off previous states.\n> \n> * It is not necessary that we know the true Q-Values when starting. Since the equation is a recursive one, we can guess arbitrary values and it will eventually converge to the real ones.","0736a8f6":"### Markov Decision Process\n\n#### Why MDP\n> The goal of an agent and an MDP is to maximize it\u2019s cumulative for awards.\n\n> It gives us a way to formalize sequential decision making. This formalization is the basis for problems that are solved with reinforcement learning.\n>\n![image.png](https:\/\/drive.google.com\/uc?id=1DrT1YWdQFDdUAOLYEKmyv8kQkmI-hBvK)\n\n> In reinforcement learning, The Markov Decision Process (MDP) describes the completely observable environment, that is, the observed state content completely determines the characteristics required for decision making. Almost all reinforcement learning problems can be converted to MDP.\n\n#### Markove Chain\n> Markov process, also known as the Markov Chain, is a memory-free random process, which can be represented by a tuple <S,P>, where S is a finite number of state sets, and P is the state transition probability matrix.\n\n> In reinforcement learning, each state the agent is in is a direct consequence of the previous state and the action previously performed.\n>\n> The previous state is also a similar consequence of the one before that, and on it goes until we are left with the initial state.\n>\n> Each one of these steps, and their order, hold information about the current state - and therefore have direct control on which action the Agent should choose next.\n>\n> But we are faced with a problem - the further we go, the more information the Agent needs to save and process at every given step it takes.\n>\n> This can quickly become unfeasible, due to the inability to perform calculations.\n> \n> As a solution to this problem, we assume that all states are Markov States, that is, any state solely depends on the state that came before it (the action performed), and the transition from that state to the current one (the reward given).\n> \n> Have a look at these 2 Tic-Tac-Toe games:\n![m2](https:\/\/iq.opengenus.org\/content\/images\/2019\/09\/1-1.jpeg)\n\n> The final state reached in both the games is exactly the same, but the steps taken to reach there are different for both.\n> \n> In both cases, the blue player must capture the top-right cell, or he will lose.\n> All we needed in order to find this was the last state, nothing else.\n> \n> Note that, **when using the Markov assumption, data is being lost** - this becomes important in complex games such as Go or Chess, where the order of the moves hold implicit information about the opponent's strategy or way of thinking.\n> \n> Nevertheless, the Markov states assumption is fundamental when attempting to calculate long-term strategies.\n\n#### Markov Reward Process\n> Markov reward process added the reward R and attenuation coefficient $\\gamma$: <S,P,R, $\\gamma$>. R is a reward function. The reward in S state is the reward expectation that can be obtained in the next moment (t+1) under S state at a certain moment:\n>\n> $R_{s}$ = $E$[$R_{t+1}$|$S_{t}$=s]\n>\n>The reason we have $\\gamma$ aka discount factor is becasue we don't want the process to be infinite.\n\n\n#### Famous example for Markove Chain\/Process\n\n> It is little bit long and someone already know MDP, so read from here:\nhttps:\/\/www.52coding.com.cn\/2017\/08\/18\/RL%20-%20Markov%20Decision%20Processes\/\n![m1](https:\/\/drive.google.com\/uc?id=1Y_v61gBrv8Rmzl9m-cvIotp0FBvqFzyd)\n","b6d6914b":"better:)","a81323f7":"### Frozen Lake game\n![frozen](https:\/\/www.pandotrip.com\/wp-content\/uploads\/2015\/11\/Chaqmaqtin-Photo-by-Matthieu-Paley2-980x711.jpg)\n\n> \"Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\"\n> \n> The surface is described using a grid like the following:\n> \n>SFFF       (S: starting point, safe)\n>\n>FHFH       (F: frozen surface, safe)\n>\n>FFFH       (H: hole, fall to your doom)\n>\n>HFFG       (G: goal, where the frisbee is located)\n\n> \n> The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise.\n![f2](https:\/\/www.deeplearningwizard.com\/deep_learning\/deep_reinforcement_learning_pytorch\/images\/gridworld_examples.png)\n>\n>LEFT = 0\n>DOWN = 1\n>RIGHT = 2\n>UP = 3","68a9e456":"### Content\n\n**Reinforcement Learning**\n> * What is Reinforcement learning?\n> * Two bases of RL\n> * Develope History\n> * Model\/Terms of reinforcement learning\n> * Maze&Flappy Bird&Breakthebricks - example of reinforcement learning\n> * Exploration and Exploitation - epsilon greedy strategy\n> * The mainstream algorithm of reinforcement learning\n\n**Markov Decision Process**\n>* Why MDP\n>* Markove Chain\n>* Markov Reward Process\n>* Famous example for Markove Chain\/Process\n\n**The Bellman equation**\n\n**Q-Learing**\n> * What is Q-learning\n> * Why Q-learning\n> * What is Q-learning table?\n> * The mathematic theory of Q-learning\n> * The algorithm of Q-learning\n\n**Frozen Lake game**\n> * Policy iteration\n> * Value iteration - Qlearning\n\n**CartPole-v0 game**\n> * Random Action\n> * Hand-Made Policy\n> * Q-learning with discretization\n> * Q-learning with greedy policy\n> * Deep Q-learning\n> * Random guessing and hill climbing\n\n**Reference**\n**license**","c8188cfc":"#### Q-learning: Greedy Policy","1a03ea63":"#### Q-learning pseudocode\n>\n>![ws](https:\/\/pic1.zhimg.com\/v2-3919d5ce017f134a073c8fcf956503a8_r.jpg)","07a9dd17":"### Q-learning\n\n#### 1. What is Q-learning\n> Q-learning is a kind of reinforcement learning technique used for learning the optimal policy in a MDP), is a value-based method of supplying information to inform which action an agent should take.\n\n#### 2. Why Q-learning\n> The objective of Q-learning is to find a policy that is optimal, in the sense that the expected return over all successive time steps is the maximum achievable. In other word, the goal of Q-learning is to find the optimal policy by learning the optimal two values for each state action period.\n\n#### 3. What is Q-learning table?\n> Q-table is just a fancy name for a simple lookup Table. We calculate the maximum expected future rewards for each state. Basically, this chart will guide us on the best course of action in each state.\n>\n> In the Q table, the columns are actions and the rows are states.\n>\n>![image.png](https:\/\/drive.google.com\/uc?id=1UXKQVszbn75QyHjnsDEOAt6hQJsDnS4J)\n> Each Q score will be the maximum expected future reward that the robot will receive when it takes the action in that state. This is an iterative process because we need to improve q-Table with each iteration.\n>\n> To learn each value of the Q-table, we use the Q-Learning algorithm.\n>\n> And we will update the value using the Bellman equation\n\n#### 4. The mathematic theory of Q-learning\n> Bellman euqation\n> Using above function, we get the Q value of the cell in the table.\n\n![2](https:\/\/drive.google.com\/uc?id=18vf-Pd3QRptq3xhHKd9mDbcqgpyKkHeD)\n\n#### 5. The algorithm of Q-learning\n![3](https:\/\/drive.google.com\/uc?id=1ahn24Cj0XeY87wHuQK2_YWKBO86f0dhl)\n> **4.1 Initialize the Q-value**\n> * We'll start by building a Q table. We have n columns, where n = actions. We have m rows, where m is equal to the number of states. \n> * We initialize the value to 0.\n> \n> **4.2&4.3 Select and execute the action**\n> * The combination of these steps takes place in an indefinite period of time. This means that the step runs until we stop training, or the training loop stops, as defined in the code.\n> * We will select the action (a) in the state based on q-Table. However, as mentioned earlier, when the series originally started, each Q was 0.\n> * So now the concept of exploring and developing tradeoffs comes into play.\n> * We're going to use something called epsilon's greed strategy.\n> * At first, the epsilon interest rate will be higher. The robot will explore the environment and randomly select actions. The logic behind this is that robots know nothing about the environment.\n> * As robots explore the environment, epsilon rates decrease and robots begin to take advantage of the environment.\n> * During the exploration, the robot gradually became more confident in estimating Q values.\n> * For the robot example, there are four operations to choose from: up, down, left, and right. We start training now - our robot knows nothing about the environment. So the robot chooses random action, right.\n> * We can now update the Q value with the Bellman equation to start and move to the right.\n> \n> **4.4\/4.5 evaluation**\n> * Now we take action and observe the results and rewards. We need to update the function Q (s, A).\n\n![q](https:\/\/iq.opengenus.org\/content\/images\/2019\/09\/1.gif)","e3de2ba0":"#### Flappy Bird - example of reinforcement learning\n> * Agent: The bird, the role of machine\n> * State: The position of the bird and pipes\n> * Action: The way to make the bird fly\n> * Environment: The whole process of the game where have all kinds of water pipes\n> * Rewards: The farther you fly, the more points you earn\n\n![rl2](https:\/\/www.raspberrypi.org\/app\/uploads\/2019\/12\/flappy-bird-1.jpg)\n\n","adb76ff7":">![44](https:\/\/static.bookstack.cn\/projects\/hands_on_Ml_with_Sklearn_and_TF\/images\/chapter_16\/16-4.png)\n>\n>At each time ste, ou can look at its position (x), velocity (x_dot), Angle (theta), and angular velocity (theta_dot). This is the observable state. In any state, there are only two possible actions for the car: to move left or to move right.\nIn other words, the cart-Pole state space has four continuous values in four dimensions, and the action space has two discrete values in one dimension.\n>\n>Explain for 'box': https:\/\/github.com\/openai\/gym\/blob\/master\/gym\/spaces\/box.py","255f15ac":"#### Model of reinforcement learning\n>We need to establish an unified reinforcement learning framework, the different specific learning tasks just a concrete example of this framework system:\n\n> * Action (A): The action agent takes\n> * Value (V): It is expected long-term return with discount, as compared to the short-term reward.\n\n> * Reward (R): An immediate return given to an agent when he or she performs specific action or task.\n    * A scalar quantity represents a feedback signal that is typically used to indicate how well an action is performing for the moment\n    * Our goal is to get the maximum of the cumulative reward\n    * **reward hypothesis**: Reinforcement Learning are based on this hypothesis, we think that all of our goals can be described as the maximum value of the accumulated reward\n\n> * Agent: It is an assumed entity which performs actions in an environment to gain some reward.\n> * Environment (e): A scenario that an agent has to face.\n![rl5](https:\/\/pic2.zhimg.com\/80\/v2-b5b4587e3b0df4d614bfb511f8f5bb2d_1440w.jpg)\n    * This brain represents what the Agent does. It takes Observation and reward as inputs and obtains corresponding actions through algorithm to take corresponding actions.\n    * Observation is the observation of an external environment. We do not necessarily see all the contents of the environment.\n    * After each action is generated, we can get a corresponding Observation about environment and give the agent the corresponding reward.\n\n> * State (s): State refers to the current situation returned by the environment.\n> * History: History is a sequence of actions, observations, and rewards: $H_{t}$ = $A_{1}$, $O_{1}$, $R_{1}$, ..., $A_{t}$, $O_{t}$, $R_{t}$\n    * History can determine what action to take next; In fact, what the agent does is the map from history to action\n    * For the environment, history and generated actions by history are given to the environment to obtain the corresponding observation and reward\n    * State: Summary of history. It is used to decide what the next action should be. In fact, it is a function of history:\n    \n![rl6](https:\/\/www.zhihu.com\/equation?tex=S_t%3Df%28H_t%29%2C+A_t%3Dh%28S_t%29)\n\n> * Agent including one to three following thress features:\n    * Policy (\u03c0):It is a behavior function that takes the state as input and the action, or the probability of each action being executed, as output\n    * Value Function:  It represents the Value of a certain state of the current environment or a certain action in the current state, used to evaluate the quality of a certain state or action.(there are two cases of value function: one is the evaluation based on the state; the other is the evaluation of the action under the state).\n    >\n                >        $v(s)$ = $E$[$G_{t}$|$S_{t}$=$s$]\n    * Model of the environment: Used to predict the change of environment, it like environment in Agent's imagination\n    \n> * Model based methods: It is a method for solving reinforcement learning problems which use model-based methods.\n> * Q value or action value (Q): Q value is quite similar to value. The only difference between the two is that it takes an additional parameter as a current action.\n\n![rl1](https:\/\/iq.opengenus.org\/content\/images\/2019\/09\/1-2.jpg)\n\n","0a21967e":"### Policy Iteration\n\n>Policy iteration includes policy evaluation and policy improvement.\n>\n>Policy evaluation is itself an iterative operation. Each time a policy is evaluated, the initial value of the value function is the value function of the previous policy. This generally improves the convergence rate of the strategy evaluation significantly\n>\n>![bb](https:\/\/img-blog.csdnimg.cn\/20190412151403573.png?x-oss-process=image\/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L25qc2hha2E=,size_16,color_FFFFFF,t_70)","07d70c96":"#### Random Action\n>\n> * **observation** (object): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game. (observation\u53ef\u4ee5\u8868\u793a\u4ee3\u8868\u5f53\u524dobject\u7684\u72b6\u6001, \u4f8b\u5982\u4f4d\u7f6e\u4fe1\u606f, \u89d2\u5ea6\u4fe1\u606f\u7b49)\n> * **reward** (float): amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward. (reward\u91c7\u53d6\u8fd9\u4e00\u6b65action\u83b7\u5f97\u7684\u5956\u52b1)\n> * **done** (boolean): whether it's time to reset the environment again. Most (but not all) tasks are divided up into well-defined episodes, and done being True indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.) (\u4ee3\u8868\u8fd9\u4e00\u8f6e\u6e38\u620f\u662f\u5426\u7ed3\u675f, \u5982\u679cdone=True, \u8868\u793a\u8fd9\u4e00\u7c7b\u6e38\u620f\u7ed3\u675f\u4e86)\n> * **info** (dict): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment's last state change). However, official evaluations of your agent are not allowed to use this for learning.","4ba28d87":"#### Exploration and Exploitation - epsilon greedy strategy\n\n> Reinforcement learning is a process of trial and error and then error reduction. So there is a contradiction here, when we have a relatively good solution, we should choose to follow our solution, or choose to try a new unknown path, which may have a high error rate, or there may be a better solution. How to balance the two is a problem of Exploration and Exploitation\n    >* Exploration: The tendency to explore new information about the environment\n    >* Exploitation: Inclined to exploit the maximum rewards we have explored\n\n> In Q-learning, we will use **epsilon greedy strategy**\n    >* We set up a\t$\\epsilon$, the range is from 0 to 1. If it more close to 1, the agent more likely to be exploration with loss, if it more close to 0, the agent more likely to exploit. It will choose one and reuse it.\n    >* agent will choose the action with the highest value for its current state from the Q table\n    >* Noramlly we will initial $\\epsilon$ around 1, to make agent explore all the possiblility\n    ![e1](https:\/\/imaddabbura.github.io\/img\/bandit-algorithms\/epsilon_greedy.PNG)\n>    \n    >* Practically, when $\\epsilon$ close to 0, we more care about short-term, immediately reward, when $\\epsilon$ more lcloase to 1, we care about long-term, risky and higher reward.","f2378452":"# ReinforcementLearning - FrozenLake&Cartpole\n\n> A simple explanation for Reinforcement learning and Q-learning, with a examples, games - FrozenLake CartPole-v0, from gym for further explanation.","6102cabc":"#### Q-learning: Discretization\n>\n>Important hyper-parameter:\n> * Learning rate(alpha), which defines the proportion of the new Q in an old Q that will be learned from the new Q. A value of 0 means that the agent is not learning anything (old information is important), and a value of 1 means that newly discovered information is the only information that is important.\n> * The discount factor (gamma), which defines the importance of future rewards. A value of 0 means that only short-term rewards are considered, with a value of 1 valuing long-term rewards more.\n>\n>![ql](https:\/\/pic4.zhimg.com\/80\/v2-85fb1a613ee506fbcc269cf9b2a50b1b_1440w.jpg)\n>\n>$(1-\\alpha)Q_{s,a}$ ss the proportion of the old Q in $newQ_{s,a}$"}}