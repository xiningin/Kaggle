{"cell_type":{"63838417":"code","f1743aa8":"code","36d470a8":"code","b7cca5be":"code","3c0d065d":"code","9d532446":"code","6933fcf8":"code","21d2029d":"markdown","d68c2bc5":"markdown","d4fd023b":"markdown","22074992":"markdown","395ae0b2":"markdown","7567954b":"markdown","4e9a6eec":"markdown","3e14b49a":"markdown"},"source":{"63838417":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f1743aa8":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.covariance import GraphicalLasso\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.linear_model import RidgeClassifier, LogisticRegression\nfrom sklearn.svm import LinearSVC\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint(\"Libraries were imported.\")","36d470a8":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\nprint(\"Data were loaded.\")\ncols = [c for c in train.columns if c not in ['id', 'target', 'wheezy-copper-turtle-magic']]","b7cca5be":"def get_mean_cov(x, y):\n    model = GraphicalLasso()\n    ones = (y==1).astype(bool)\n    x2 = x[ones]\n    model.fit(x2)\n    p1 = model.precision_\n    m1 = model.location_\n    \n    onesb = (y==0).astype(bool)\n    x2b = x[onesb]\n    model.fit(x2b)\n    p2 = model.precision_\n    m2 = model.location_\n    \n    return np.stack([m1, m2]), np.stack([p1, p2])","3c0d065d":"LRmodel1 = LogisticRegression()\nLRmodel2 = RidgeClassifier()\nLRmodel3 = LinearDiscriminantAnalysis()\nLRmodel4 = LinearSVC()","9d532446":"oof1 = np.zeros(train.shape[0])\noof2 = np.zeros(train.shape[0])\noof3 = np.zeros(train.shape[0])\noof4 = np.zeros(train.shape[0])\npredictions1 = np.zeros(test.shape[0])\npredictions2 = np.zeros(test.shape[0])\npredictions3 = np.zeros(test.shape[0])\npredictions4 = np.zeros(test.shape[0])\nfor i in range(512):\n        \n    train2 = train[train['wheezy-copper-turtle-magic']==i]\n    test2 = test[test['wheezy-copper-turtle-magic']==i]\n    idx1 = train2.index; idx2 = test2.index\n    train2.reset_index(drop=True,inplace=True)\n\n    data = pd.concat([pd.DataFrame(train2[cols]), pd.DataFrame(test2[cols])])\n    data2 = VarianceThreshold(threshold=1.5).fit_transform(data[cols])\n\n    train3 = data2[:train2.shape[0]]; test3 = data2[train2.shape[0]:]\n\n    skf = StratifiedKFold(n_splits=5, random_state=42)\n    for train_index, test_index in skf.split(train2, train2['target']):\n        \n        # estimate mean and SD\n        ms, ps = get_mean_cov(train3[train_index,:], train2.loc[train_index]['target'])\n        \n        # model fittings\n        GMmodel = GaussianMixture(n_components=2, init_params='random', covariance_type='full',\n                                tol=1e-3, reg_covar=0.001, max_iter=100, n_init=1,\n                                means_init=ms, precisions_init=ps, verbose_interval=250)\n        GMmodel.fit(np.concatenate([train3, test3], axis=0))\n        QDAmodel = QuadraticDiscriminantAnalysis(0.5)\n        QDAmodel.fit(train3[train_index,:], train2.loc[train_index]['target'])\n        \n        # prediction on test data\n        GMpred = GMmodel.predict_proba(train3[test_index,:])[:, 0]\n        QDApred = QDAmodel.predict_proba(train3[test_index,:])[:, 1]\n        \n        # stacking with different linear classifiers\n        Stacked = np.vstack((GMpred, QDApred)).T\n        LRmodel1.fit(Stacked, train2.loc[test_index]['target'])\n        LRmodel2.fit(Stacked, train2.loc[test_index]['target'])\n        LRmodel3.fit(Stacked, train2.loc[test_index]['target'])\n        LRmodel4.fit(Stacked, train2.loc[test_index]['target'])\n        oof1[idx1[test_index]] = LRmodel1.predict_proba(Stacked)[:,1]\n        oof2[idx1[test_index]] = LRmodel2.predict(Stacked)\n        oof3[idx1[test_index]] = LRmodel3.predict_proba(Stacked)[:,1]\n        oof4[idx1[test_index]] = LRmodel3.predict(Stacked)\n        \n        # predictions with emsemble-stacking models\n        TargetStacked = np.vstack((GMmodel.predict_proba(test3)[:, 0],\n                                  QDAmodel.predict_proba(test3)[:, 1])).T\n        predictions1[idx2] += LRmodel1.predict_proba(TargetStacked)[:,1] \/ skf.n_splits\n        predictions2[idx2] += LRmodel2.predict(TargetStacked) \/ skf.n_splits\n        predictions3[idx2] += LRmodel3.predict_proba(TargetStacked)[:,1] \/ skf.n_splits\n        predictions4[idx2] += LRmodel4.predict(TargetStacked) \/ skf.n_splits\n        \nauc1 = roc_auc_score(train['target'], oof1)\nauc2 = roc_auc_score(train['target'], oof2)\nauc3 = roc_auc_score(train['target'], oof3)\nauc4 = roc_auc_score(train['target'], oof4)\nprint('[Logistic Regression] ROC =',round(auc1, 5))\nprint('[Ridge Classifer] ROC =',round(auc2, 5))\nprint('[Linear Discriminant Analysis] ROC =',round(auc3, 5))\nprint('[Linear SVC] ROC =',round(auc4, 5))","6933fcf8":"aucs = [auc1, auc2, auc3, auc4]\npredictions = [predictions1, predictions2, predictions3, predictions4]\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission['target'] = predictions[aucs.index(max(aucs))]\nsubmission.to_csv('submission.csv', index=False)","21d2029d":"### submission","d68c2bc5":"### Model fitting and predictions","d4fd023b":"### Conclusion\nSimply use Logistic Regression!","22074992":"### load data","395ae0b2":"### for Gaussian Mixture model","7567954b":"### libraries","4e9a6eec":"### Linear models for stacking","3e14b49a":"Although QDA and Gaussian Mixture have excellent performances in this competition, presumably stacking models (weighted averaging of results from the abovementioned models) is necessary to avoid overfitting and climb up the final ranking.\n\nHere I explore which linear classifier to use in the stacking process to maximize performance. The classifiers are:\n\n- Logistic Regression\n- Ridge Regression\n- Linear Discriminant Analysis\n- LinearSVC"}}