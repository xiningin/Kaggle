{"cell_type":{"e6d854c9":"code","69c0add5":"code","b89c8b93":"code","5d4cc9d4":"code","ae0cf26f":"code","1ef4d78e":"code","b4b856df":"code","9743bc6c":"code","bd5a1dc9":"code","5d06c658":"code","c8ee5d0e":"code","13b7e5cd":"code","5bd0d009":"code","0a0218e3":"code","84676911":"code","0e84856a":"code","91544839":"code","3fe678f4":"code","5cacad48":"code","d9b15318":"code","9d350f1d":"code","3685d719":"code","1bc3f666":"code","ec889bcc":"markdown","f76d6080":"markdown","b0fdf023":"markdown","e97aebd3":"markdown","2fcb8def":"markdown","b0b9fd6f":"markdown","cdb5a699":"markdown","82581d69":"markdown","6fbeaad4":"markdown","e6880ba6":"markdown","10081de5":"markdown","92998623":"markdown","5eccc041":"markdown","780e91e5":"markdown","d25f7488":"markdown","659b4fd9":"markdown","b289c274":"markdown","c9b5a305":"markdown","1f56c30f":"markdown","e7a1b917":"markdown","e5daa152":"markdown","30b8cf06":"markdown","c0ca4d0c":"markdown","bfc5a806":"markdown","ba8b723b":"markdown","9ee4bb74":"markdown"},"source":{"e6d854c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","69c0add5":"data = pd.read_csv(\"\/kaggle\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")","b89c8b93":"data.head()","5d4cc9d4":"data.info()","ae0cf26f":"data = (data-np.min(data)) \/(np.max(data)-np.min(data))","1ef4d78e":"data.head()","b4b856df":"from sklearn.model_selection import train_test_split\n\nx = data.drop(\"DEATH_EVENT\",axis=1)\ny = data[\"DEATH_EVENT\"]\n\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.15,random_state=42)\n\ny_train = np.array(y_train)\ny_test = np.array(y_test)","9743bc6c":"x_train = x_train.T\nx_test = x_test.T\n\ny_train = y_train.T\ny_test = y_test.T\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\n\nprint(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","bd5a1dc9":"def initialize_parameters_and_layer_sizes_NN(x_train, y_train):\n    parameters = {\"weight1\": np.random.randn(3,x_train.shape[0]) * 0.1,\n                  \"bias1\": np.zeros((3,1)),\n                  \"weight2\": np.random.randn(y_train.shape[0],3) * 0.1,\n                  \"bias2\": np.zeros((y_train.shape[0],1))}\n    return parameters","5d06c658":"def sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head\n","c8ee5d0e":"def forward_propagation(x_train,parameters):\n    \n    Z1 = np.dot(parameters[\"weight1\"],x_train) + parameters[\"bias1\"]\n    A1 = np.tanh(Z1)\n    Z2 = np.dot(parameters[\"weight2\"],A1) + parameters[\"bias2\"]\n    A2 = sigmoid(Z2)\n    \n    \n    cache = {\"Z1\":Z1,\n            \"A1\":A1,\n            \"Z2\":Z2,\n            \"A2\":A2 }\n    \n    return A2,cache\n","13b7e5cd":"def cost_func(A2,Y,parameters):\n    logprobs = np.multiply(np.log(A2),Y)\n    cost = -np.sum(logprobs)\/Y.shape[1]\n    return cost","5bd0d009":"def backward_propagation(parameters, cache, X, Y):\n\n    dZ2 = cache[\"A2\"]-Y\n    dW2 = np.dot(dZ2,cache[\"A1\"].T)\/X.shape[1]\n    db2 = np.sum(dZ2,axis =1,keepdims=True)\/X.shape[1]\n    dZ1 = np.dot(parameters[\"weight2\"].T,dZ2)*(1 - np.power(cache[\"A1\"], 2))\n    dW1 = np.dot(dZ1,X.T)\/X.shape[1]\n    db1 = np.sum(dZ1,axis =1,keepdims=True)\/X.shape[1]\n    grads = {\"dweight1\": dW1,\n             \"dbias1\": db1,\n             \"dweight2\": dW2,\n             \"dbias2\": db2}\n    return grads","0a0218e3":"def updating(parameters, grads, learning_rate = 0.01):\n    parameters = {\"weight1\": parameters[\"weight1\"]-learning_rate*grads[\"dweight1\"],\n                  \"bias1\": parameters[\"bias1\"]-learning_rate*grads[\"dbias1\"],\n                  \"weight2\": parameters[\"weight2\"]-learning_rate*grads[\"dweight2\"],\n                  \"bias2\": parameters[\"bias2\"]-learning_rate*grads[\"dbias2\"]}\n    \n    return parameters","84676911":"def prediction(parameters,x_test):\n    A2, cache = forward_propagation(x_test,parameters)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n\n    for i in range(A2.shape[1]):\n        if A2[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction","0e84856a":"def ann_2layer(x_train,y_train,x_test,y_test,num_of_iter=2500):\n    cost_list = []\n    iter_list = []\n \n    parameters = initialize_parameters_and_layer_sizes_NN(x_train,y_train)\n    \n    for i in range(0,num_of_iter):\n        \n        A2,cache = forward_propagation(x_train,parameters) # Forward Propagation\n        \n        cost = cost_func(A2,y_train,parameters) # Cost Function\n        \n        grads = backward_propagation(parameters,cache,x_train,y_train) # Backward Propagation\n        \n        parameters = updating(parameters,grads) # Updating Parameters\n        \n        if i%100 == 0:\n            cost_list.append(cost)\n            iter_list.append(i)\n            print(\"Cost after iteration {} : {}\".format(i,cost))\n        \n    \n    \"\"\"\n    Cost Value Visualization\n    \"\"\"\n    fig,ax = plt.subplots(figsize=(8,6))\n    plt.plot(iter_list,cost_list)\n    plt.xticks(iter_list,rotation=90)\n    plt.xlabel(\"Number Of Iteration\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    \n    \n    y_prediction_test = prediction(parameters,x_test)\n    y_prediction_train = prediction(parameters,x_train)\n\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))\n    \n    \n    return parameters\n\n\n\n","91544839":"parameters = ann_2layer(x_train,y_train,x_test,y_test,2500)","3fe678f4":"x_train,x_test = x_train.T,x_test.T","5cacad48":"print(x_train.shape)\nprint(x_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","d9b15318":"from keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\ndef build_classifier():\n    classifier = Sequential()\n    classifier.add(Dense(units=16,kernel_initializer=\"uniform\",activation=\"tanh\",input_dim=x_train.shape[1]))\n    classifier.add(Dense(units=8,kernel_initializer=\"uniform\",activation=\"tanh\"))\n    classifier.add(Dense(units=1,kernel_initializer=\"uniform\",activation=\"sigmoid\"))\n    classifier.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=[\"accuracy\"])\n    return classifier\n\nclassifier = KerasClassifier(build_fn = build_classifier,epochs=100)\naccuracies = cross_val_score(estimator=classifier,X = x_train , y = y_train , cv=3)\n\nprint(\"Mean of CV Scores is \",accuracies.mean())\nprint(\"Std of CV Scores is\",accuracies.std())","9d350f1d":"classifier.fit(x_train,y_train)\ny_head = classifier.predict(x_test)","3685d719":"from sklearn.metrics import accuracy_score # I am in love with scikit learn <3","1bc3f666":"print(\"Accuracy of model is: \",accuracy_score(y_test,y_head))","ec889bcc":"## Forward Propagation\n\nIn this section I am going to define a forward propagation function. In order to do this I am going to use classical formula.\n\nZ = W*a + B \nA = tanh(Z)\n\nIn my logistic regression kernel, I had used sigmoid function and I had to define it from scratch because there is no function for sigmoid. But in tanh activation function there is a numpy method. So we can use it. However we need sigmoid again, so I am going to define it\n\n*You can check my logistic regression scratch kernel*\n\nhttps:\/\/www.kaggle.com\/mehmetlaudatekman\/death-prediction-using-heart-failure-data\n\nThe formula of tanh is:\n![bildschirmfoto-2017-11-10-um-12-20-57.png](attachment:bildschirmfoto-2017-11-10-um-12-20-57.png)\n\nLet's implement forward propagation in python","f76d6080":"## Initializing Parameters\n\nIn this section I am going to write a function that helps us to initalize parameters. ","b0fdf023":"* There are 12 features in our dataset.\n* All of the features are numerical\n* Our label (y axis) is DEATH_EVENT feature\n* There 299 rows in the dataset.","e97aebd3":"# Conclusion\nThanks for your attention, I am a beginner in deep learning and this was my first kernel in deep learning, so I might have mistakes (almost definitely). If you contact with me for mistakes, I would be glad\n\nIf you can question in your mind, you can ask me, I am going to answer them as much as I can.\n","2fcb8def":"## Combining All Functions Into A Function\n\nOur all functions are ready, and now I am going to combine them into a function.","b0b9fd6f":"* And now I am going to take reverse of arrays, I am going to explain why I did this.\n","cdb5a699":"# Introduction\n\nHello people, welcome to my kernel! Nowadays I've started to learn Deep Learning and now I am learning Artifical Neural Networks. In this kernel I am going to create an ANN model and I am going to use it for classification. I am new in data science, so I might have mistakes, if you see any mistakes and report them I would be happy. \n\nLet's take a look at our schedule\n\n# Schedule\n1. Importing Libraries and Data\n1. Data Overview\n1. Data Preprocessing\n    * Dataset Normalizing\n    * Train Test Split\n1. Creating ANN Model from Scratch\n    * Initializing Parameters\n    * Forward Propagation\n    * Loss and Cost Functions\n    * Backward Propagation\n    * Update Parameters\n    * Prediction\n    * Combining All Functions Into A Function\n1. Creating ANN Model Using Keras Library\n1. Result\n1. Conclusion","82581d69":"## Updating Function\n\nIn this section I am goiing to define updating function, we will use our gradients for updating weights and bias. \n\nThere is a formula about updating\n\n![image.png](attachment:image.png)","6fbeaad4":"* Why I've used 3: There is a hyperparameter in ANN called node number. And for this kernel I've chosen 3 for node number\n\n*If you want to see detailed description you can check this kernel, English is not my native language and I am a beginner in Deep Learning so, I could not describe node number enough*\n\nhttps:\/\/www.kaggle.com\/kanncaa1\/deep-learning-tutorial-for-beginners#Overview-the-Data-Set\n","e6880ba6":"## Dataset Normalizing\n\nIn this sub-section I am going to normalize dataset. In order to do this I am going to use this formula:\n\n**normalized_data = (data - min(data) \/ (max(data) - min(data))**","10081de5":"# Creating ANN Model\n\nIn this section I am going to create 2 Layer ANN model. I am going to create two layer ANN because I am a beginner and at least at the beginning I am not going to use any library. I am going to write models myself.\n\nI am going to follow these steps for creating ANN Model:\n* Initializing Parameters\n* Forward Propagation\n* Loss and Cost Functions\n* Backward Propagation\n* Update Parameters\n* Prediction\n* Combining All Functions Into A Function\n","92998623":"# Data Overview\n\nIn this section I am going to examine dataset. ","5eccc041":"# Creating ANN Model Using Keras Library\n\nIn this section I am going to create an ANN model using Keras library. I am going to use Keras library because it eases our job. \n\nI am going to start with taking transposes of arrays. I am going do this because at the beginning of this kernel we've taken transposes arrays for matrix product. Now I am going to fix it.","780e91e5":"## Backward Propagation\n\nFinally we came most confused section. I am not going to tell anything about this because backward propagation is only about the derivate, and it is confusing. If you want to learn details about backward propagation you can check youtube. ","d25f7488":"## Prediction Function\n\nOur main functions are ready, but still we need a prediction function for using our trained model. So I am going to create a prediction function.","659b4fd9":"* I took this code from my teacher's kernel, you can check this kernel, I am sure you will love that.\n\n*https:\/\/www.kaggle.com\/kanncaa1\/deep-learning-tutorial-for-beginners*","b289c274":"Our function is ready! Let's test it with our dataset.","c9b5a305":"# Data Preprocessing\nIn this section I am going to prepare dataset for deep learning. In order to do this I am going to follow these two steps\n\n* Dataset Normalizing\n* Train Test Split","1f56c30f":"# Result\n\nAt the beginning of this kernel we created our ANN model from scratch our score was %70 and it is a bit low. But at the end of this kernel we created our ANN model using keras and our score was %71. \n\nSo we can create ANN models from scratch for understand the logic behind ANN. But we should use Keras library for creating ANN models, because it is easier and better than our models.","e7a1b917":"Our train score is %70\n\nOur test score is %55","e5daa152":"* Our mean of CV scores is %82 \n* Our variance of CV scores is 0.04\n\nAnd now I am going to compute accuracy score. In order to do this I am going to start with fit my data to my model.","30b8cf06":"## Cost Function\nIn this section I am going to define the cost function. In deep learning we use cost, The model improves as the cost decreases.\n\nThere is a cost formula:\n![image.png](attachment:image.png)\n\nLet's implement in python","c0ca4d0c":"## Train Test Split\n\nIn this section I am going to split dataset into train and test. ","bfc5a806":"# Importing Libraries And Data\n\nIn this section I am going to import general libraries that I will use in this kernel. However I am not going to import Deep Learning libraries, I am going to import them when I need them.","ba8b723b":"* And now I am going to define forward propagation","9ee4bb74":"And now I am going to import the data."}}