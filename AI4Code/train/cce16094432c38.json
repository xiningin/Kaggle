{"cell_type":{"2dc87a62":"code","c396b0e6":"code","7dfc87e6":"code","c3f8deef":"code","832ec7ba":"code","ae56b001":"code","d97bde90":"code","12c5cf9f":"code","25cc600c":"code","c28131d8":"code","b02558b7":"code","dba6dfc7":"code","4eb3b9f6":"code","d2b7fb5b":"code","f12e843e":"code","cb7cb60b":"code","6ede21ee":"code","45b94b6f":"code","ba574c76":"code","6ca63232":"code","3955c5cc":"code","20528952":"code","c07ae7ea":"code","ffc7dbc1":"code","33a5446a":"code","71720fb1":"code","a2f46cdf":"code","39b5caa2":"code","cf829350":"code","4fecad4d":"code","e9efeb74":"code","b1b6b697":"code","2fb8c592":"code","7884810c":"code","ae2e20de":"code","61045fb6":"code","7f8510b7":"code","8d9424b0":"markdown","6a530322":"markdown","f721ac68":"markdown","9df729ec":"markdown","c5db8a96":"markdown","be05fad6":"markdown","72ffb7ca":"markdown","391e06c5":"markdown","65c65786":"markdown","4f5cf9ad":"markdown","cab9a406":"markdown","d5118fb5":"markdown","aacc352a":"markdown","49d407a0":"markdown","037f3826":"markdown","35311580":"markdown","ebd48a79":"markdown","7f921ea8":"markdown","30015a56":"markdown","78b055ec":"markdown","730f3935":"markdown"},"source":{"2dc87a62":"# Import basic libraries \nimport os\nimport pandas as pd \nimport numpy as np\nimport seaborn as sns\nfrom string import ascii_uppercase\nfrom pandas import DataFrame\nimport matplotlib.pyplot as plt        \n%matplotlib inline\n\nimport sklearn.preprocessing as skp\nimport sklearn.model_selection as skm\nimport os\n#import classification modules\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n# Selection\nfrom sklearn.model_selection import GridSearchCV as gs\nfrom sklearn.model_selection import RandomizedSearchCV as rs\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_selection import RFECV\n#import decision tree plotting libraries\n# Metrics\nfrom sklearn.metrics import accuracy_score, confusion_matrix,precision_score, recall_score, roc_auc_score,roc_curve, auc, f1_score ","c396b0e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","7dfc87e6":"# Loading Dataset\nmissing=[\"na\",\"--\",\".\",\"..\"]\ntd= pd.read_csv(\"\/kaggle\/input\/hepatitis\/hepatitis.csv\",na_values=missing)\ntd.head()","c3f8deef":"td.isnull().sum() # Checking for nulls","832ec7ba":"td[\"class\"].replace((1,2),(0,1),inplace=True)","ae56b001":"td[\"class\"]=td[\"class\"].astype(\"bool\")","d97bde90":"td.describe()","12c5cf9f":"# Discretization of Age Column\ntd[\"age\"]=np.where((td[\"age\"]>10) & (td[\"age\"]<20),\"Teenagers\",\n                   np.where((td[\"age\"]>=20) & (td[\"age\"]<=30),\"Adults\",\n                   np.where((td[\"age\"]>30) & (td[\"age\"]<=40),\"Middle Aged\",np.where((td[\"age\"]<=10),\"Children\",\n                            \"Old\"))))","25cc600c":"td[\"age\"]=pd.Categorical(td.age,[\"Children\",'Teenagers','Adults', 'Middle Aged', 'Old'],ordered=True)","c28131d8":"td[\"age\"].value_counts() ","b02558b7":"#draw a bar plot of Age vs. survival\nsns.barplot(x=\"age\", y=\"class\", data=td)\nplt.show()","dba6dfc7":"td[\"sex\"].replace((1,2),(\"Male\",\"Female\"),inplace=True)\ntd[\"sex\"]=pd.Categorical(td.sex,[\"Male\",'Female'],ordered=False)\ntd.head()","4eb3b9f6":"td.dropna(inplace=True) # Now dropping all nulls","d2b7fb5b":"td.dtypes","f12e843e":"#We have categorical variables .getdummies seperates the different categories of categorical variables as separate \n#binary columns\ntd1 = pd.get_dummies(td,drop_first=True)\n#List of new columns\nprint(td1.columns)\ntd1.head(5)","cb7cb60b":"td1[\"bilirubin\"]=np.abs((td1[\"bilirubin\"]-td1[\"bilirubin\"].mean())\/(td1[\"bilirubin\"].std()))\ntd1[\"albumin\"]=np.abs((td1[\"albumin\"]-td1[\"albumin\"].mean())\/(td1[\"albumin\"].std()))","6ede21ee":"y=td1[\"class\"].copy()\nX=td1.drop(columns=[\"class\"])\nprint(y.shape)\nprint(X.shape)","45b94b6f":"#Random Forest method for feature selection\nfrom sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier()    \n#thit is how we get the feature importance with simple steps:\nX_features=X.columns\nmodel.fit(X, y)\n# display the relative importance of each attribute\nimportances = np.around(model.feature_importances_,decimals=4)\nimp_features= model.feature_importances_\nfeature_array=np.array(X_features)\nsorted_features=pd.DataFrame(list(zip(feature_array,imp_features))).sort_values(by=1,ascending=False)\n#print(sorted_features)\ndata_top=sorted_features[:X.shape[1]]\nfeature_to_rem=sorted_features[X.shape[1]:]\nprint(\"Unimportant Columms after simple Random Forrest\\n\",feature_to_rem)\nrem_index=list(feature_to_rem.index)\nprint(rem_index)\nprint(\"Important Columms after simple Random Forrest\\n\",data_top)\ndata_top_index=list(data_top.index)\nprint(\"Important Columms after simple Random Forrest\\n\",data_top_index )\nprint(importances)\n#0.0250 is a  selected threshold looking at the importance values this can be changed to any other value too\n#cols_randfor_removed=[index for index,value in enumerate(importances) if value <= 0.0250]\n#print(cols_randfor_removed)\nX_randfor_sel = X.drop(X.columns[rem_index],axis=1)\n#X_randfor_sel = X.drop(X.columns[cols_randfor_removed],axis=1)\nfeatures_randfor_select=X_randfor_sel.columns\nprint(features_randfor_select)","ba574c76":"#creat train-test split parts for manual split\n\ntrainX, testX, trainy, testy= skm.train_test_split(X,y, test_size=0.25, random_state=99) #explain random state\nprint(\"\\n shape of train split: \")\nprint(trainX.shape, trainy.shape)\nprint(\"\\n shape of train split: \")\nprint(testX.shape, testy.shape)","6ca63232":"### Making X Scalar for ML algorithms\nX = skp.StandardScaler().fit(X).transform(X)","3955c5cc":"knn = KNeighborsClassifier()\nknn.fit(trainX,trainy)\npredictions = knn.predict(testX)\naccknn=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of KNN (%): \\n\", accknn)  \n#get FPR\nfprknn, tprknn, _ = roc_curve(testy, predictions)\naucknn=auc(fprknn, tprknn)*100\nprint(\"AUC OF KNN (%): \\n\", aucknn)\nrecallknn=recall_score(testy,predictions)*100\nprint(\"Recall of KNN is: \\n\",recallknn)\nprecknn=precision_score(testy,predictions)*100\nprint(\"Precision of KNN is: \\n\",precknn)","20528952":"gnb=GaussianNB()\ngnb.fit(trainX,trainy)\npredictions = gnb.predict(testX)\naccgnb=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of Gaussian Naive Bayes (%): \\n\",accgnb)  \n#get FPR\nfprgnb, tprgnb, _ = roc_curve(testy, predictions)\naucgnb=auc(fprgnb, tprgnb)*100\nprint(\"AUC OF Gaussian Naive Bayes (%): \\n\", aucgnb)\nrecallgnb=recall_score(testy,predictions)*100\nprint(\"Recall of Gaussian Naive Bayes is: \\n\",recallgnb)\nprecgnb=precision_score(testy,predictions)*100\nprint(\"Precision of Gaussian Naive Bayes is: \\n\",precgnb)","c07ae7ea":"lrg=LogisticRegression(solver='lbfgs')\nlrg.fit(trainX,trainy)\npredictions = lrg.predict(testX)\nacclrg=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of Logistic regression (%): \\n\",acclrg)  \n#get FPR\nfprlrg, tprlrg, _ = roc_curve(testy, predictions)\nauclrg=auc(fprlrg, tprlrg)*100\nprint(\"AUC OF Logistic regression (%): \\n\", auclrg)\nrecalllrg=recall_score(testy,predictions)*100\nprint(\"Recall of Logistic regression is: \\n\",recalllrg)\npreclrg=precision_score(testy,predictions)*100\nprint(\"Precision of Logistic regression is: \\n\",preclrg)","ffc7dbc1":"nn=MLPClassifier(solver='lbfgs',hidden_layer_sizes=20,batch_size=150,max_iter=100, random_state=1)\nnn.fit(trainX,trainy)\npredictions = nn.predict(testX)\naccnn=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of Neural Networks (%): \\n\",accnn)  \n#get FPR\nfprnn, tprnn, _ = roc_curve(testy, predictions)\naucnn=auc(fprnn, tprnn)*100\nprint(\"AUC OF Neural Networks (%): \\n\", aucnn)\nrecallnn=recall_score(testy,predictions)*100\nprint(\"Recall of Neural Networks is: \\n\",recallnn)\nprecnn=precision_score(testy,predictions)*100\nprint(\"Precision of Neural Networks is: \\n\",precnn)","33a5446a":"svm=clf = SVC(gamma=\"auto\",kernel='poly',degree=3)\nsvm.fit(trainX,trainy)\npredictions = svm.predict(testX)\naccsvm=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of Support Vector Machine (%): \\n\",accsvm)  \n#get FPR\nfprsvm, tprsvm, _ = roc_curve(testy, predictions)\naucsvm=auc(fprsvm, tprsvm)*100\nprint(\"AUC OF Support Vector Machine (%): \\n\", aucsvm)\nrecallsvm=recall_score(testy,predictions)*100\nprint(\"Recall of Support Vector Machine is: \\n\",recallsvm)\nprecsvm=precision_score(testy,predictions)*100\nprint(\"Precision of Support Vector Machine is: \\n\",precsvm)","71720fb1":"dt=DecisionTreeClassifier(max_depth=10,criterion=\"gini\")\ndt.fit(trainX,trainy)\npredictions = dt.predict(testX)\naccdt=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of Decision Tree (%): \\n\",accdt)  \n#get FPR\nfprdt, tprdt, _ = roc_curve(testy, predictions)\naucdt=auc(fprdt, tprdt)*100\nprint(\"AUC OF Decision Tree (%): \\n\",aucdt)\nrecalldt=recall_score(testy,predictions)*100\nprint(\"Recall of Decision Tree is: \\n\",recalldt)\nprecdt=precision_score(testy,predictions)*100\nprint(\"Precision of Decision Tree is: \\n\",precdt)","a2f46cdf":"rf=RandomForestClassifier()\nrf.fit(trainX,trainy)\npredictions = rf.predict(testX)\naccrf=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of Random Forest (%): \\n\",accrf)  \n#get FPR\nfprrf, tprrf, _ = roc_curve(testy, predictions)\naucrf=auc(fprrf, tprrf)*100\nprint(\"AUC OF Random Forest (%): \\n\", aucrf)\nrecallrf=recall_score(testy,predictions)*100\nprint(\"Recall of Random Forest is: \\n\",recallrf)\nprecrf=precision_score(testy,predictions)*100\nprint(\"Precision of Random Forest is: \\n\",precrf)","39b5caa2":"ab=AdaBoostClassifier()\nab.fit(trainX,trainy)\npredictions = ab.predict(testX)\naccab=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of AdaBoost (%): \\n\",accab)  \n#get FPR\nfprab, tprab, _ = roc_curve(testy, predictions)\naucab=auc(fprab, tprab)*100\nprint(\"AUC OF AdaBoost (%): \\n\",aucab)\nrecallab=recall_score(testy,predictions)*100\nprint(\"Recall of AdaBoost is: \\n\",recallab)\nprecab=precision_score(testy,predictions)*100\nprint(\"Precision of AdaBoost is: \\n\",precab)","cf829350":"gb=GradientBoostingClassifier()\ngb.fit(trainX,trainy)\npredictions = gb.predict(testX)\naccgb=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of Gradient Descent Boosting (%): \\n\",accgb)  \n#get FPR\nfprgb, tprgb, _ = roc_curve(testy, predictions)\naucgb=auc(fprgb, tprgb)*100\nprint(\"AUC OF Gradient Descent Boosting (%): \\n\", aucgb)\nrecallgb=recall_score(testy,predictions)*100\nprint(\"Recall of Gradient Descent Boosting is: \\n\",recallgb)\nprecgb=precision_score(testy,predictions)*100\nprint(\"Precision of Gradient Descent Boosting is: \\n\",precgb)","4fecad4d":"algos=[\"K Nearest Neighbor\",\"Guassian Naive Bayes\",\"Logistic Regression\",\"Neural Networks\",\"Support Vector Machine\",\"Decision Tree\",\"Random Forrest\",\"AdaBoost\",\"Gradient Descent Boosting\"]\nacc=[accknn,accgnb,acclrg,accnn,accsvm,accdt,accrf,accab,accgb]\nauc=[aucknn,aucgnb,auclrg,aucnn,aucsvm,aucdt,aucrf,aucab,aucgb]\nrecall=[recallknn,recallgnb,recalllrg,recallnn,recallsvm,recalldt,recallrf,recallab,recallgb]\nprec=[precknn,precgnb,preclrg,precnn,precsvm,precdt,precrf,precab,precgb]\ncomp={\"Algorithms\":algos,\"Accuracies\":acc,\"Area Under the Curve\":auc,\"Recall\":recall,\"Precision\":prec}\ncompdf=pd.DataFrame(comp)\ndisplay(compdf.sort_values(by=[\"Accuracies\",\"Area Under the Curve\",\"Recall\",\"Precision\"], ascending=False))\n","e9efeb74":"import sklearn.metrics as metrics\nroc_auc1=metrics.auc(fprknn,tprknn)\nroc_auc2=metrics.auc(fprgnb,tprgnb)\nroc_auc3=metrics.auc(fprlrg,tprlrg)\nroc_auc4=metrics.auc(fprnn,tprnn)\nroc_auc5=metrics.auc(fprsvm,tprsvm)\nroc_auc6=metrics.auc(fprdt,tprdt)\nroc_auc7=metrics.auc(fprrf,tprrf)\nroc_auc8=metrics.auc(fprab,tprab)\nroc_auc9=metrics.auc(fprgb,tprgb)\n\n# Method-I: PLot\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(20,10))\nplt.title(\"Receiver Operating Curve\")\nplt.plot(fprknn,tprknn,\"b\",label=\"ROC of KNN = %0.2f\" % roc_auc1)\nplt.plot(fprgnb,tprgnb,\"r\",label=\"ROC of Guassian Naive Bayes = %0.2f\" % roc_auc2)\nplt.plot(fprlrg,tprlrg,\"y\",label=\"ROC of Logistic Regression = %0.2f\" % roc_auc3)\nplt.plot(fprnn,tprnn,\"c\",label=\"ROC of Neural Networks = %0.2f\" % roc_auc4)\nplt.plot(fprsvm,tprsvm,\"k\",label=\"ROC of SVM = %0.2f\" % roc_auc5)\nplt.plot(fprdt,tprdt,\"m\",label=\"ROC of Descision Tree= %0.2f\" % roc_auc6)\nplt.plot(fprrf,tprrf,\"y--\",label=\"ROC of Random Forrest= %0.2f\" % roc_auc7)\nplt.plot(fprab,tprab,\"g--\",label=\"ROC of Ada Boost= %0.2f\" % roc_auc8)\nplt.plot(fprgb,tprgb,\"b--\",label=\"ROC of Gradient Boost= %0.2f\" % roc_auc9)\nplt.rcParams.update({'font.size': 16})\nplt.legend(loc=\"lower right\")\nplt.plot([0, 1],[0, 1],\"r--\")\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")\n\nplt.rc('axes', labelsize=15)\nplt.rc('axes', titlesize=22)","b1b6b697":"from sklearn.model_selection import RandomizedSearchCV as rs\n# K Nearest Neighbor with random search\nparameters={\"algorithm\":['auto','ball_tree','kd_tree','brute'],\"n_neighbors\":range(1,10,1),\"p\":[1,2],\"weights\":[\"uniform\",\"distance\"]}\nclf_knn=KNeighborsClassifier()\nclfknnrs=rs(clf_knn,parameters,cv=5,scoring=\"precision\")\nclfknnrs.fit(trainX,trainy)\npredictions = clfknnrs.predict(testX)\naccknnrs=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of KNN after Hyperparameter Tuning (%): \\n\",accknnrs)  \n#get FPR\nfprknnrs, tprknnrs, _ = roc_curve(testy, predictions)\n#aucdtrs=auc(fprdtrs, tprdtrs)*100\n#print(\"AUC OF Decision Tree after Hyperparameter Tuning (%): \\n\",aucdtrs)\nrecallknnrs=recall_score(testy,predictions)*100\nprint(\"Recall of KNN after Hyperparameter Tuning is: \\n\",recallknnrs)\nprecknnrs=precision_score(testy,predictions)*100\nprint(\"Precision of KNN after Hyperparameter Tuning is: \\n\",precknnrs)\n\n#examnine the best model\n#single best score achieved accross all params\nprint(\"Best Score (%): \\n\",clfknnrs.best_score_*100)\n#Dictionary Containing the parameters \nprint(\"Best Parameters: \\n\",clfknnrs.best_params_)\n\nprint(\"Best Estimators: \\n\",clfknnrs.best_estimator_)","2fb8c592":"from sklearn.model_selection import RandomizedSearchCV as rs\n# Logistic Regression with random search\nparameters={\"solver\":['lbfgs','newton-cg','liblinear','sag','saga'],\"max_iter\":range(100,500,100)}\nclf_lrg=LogisticRegression()\nclflrgrs=rs(clf_lrg,parameters,cv=5,scoring=\"precision\")\nclflrgrs.fit(trainX,trainy)\npredictions = clflrgrs.predict(testX)\nacclrgrs=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of Logistic Regression after Hyperparameter Tuning (%): \\n\",acclrgrs)  \n#get FPR\nfprlrgrs, tprlrgrs, _ = roc_curve(testy, predictions)\n#aucdtrs=auc(fprdtrs, tprdtrs)*100\n#print(\"AUC OF Decision Tree after Hyperparameter Tuning (%): \\n\",aucdtrs)\nrecalllrgrs=recall_score(testy,predictions)*100\nprint(\"Recall of Logistic Regression after Hyperparameter Tuning is: \\n\",recalllrgrs)\npreclrgrs=precision_score(testy,predictions)*100\nprint(\"Precision of Logistic Regression after Hyperparameter Tuning is: \\n\",preclrgrs)\n\n#examnine the best model\n#single best score achieved accross all params\nprint(\"Best Score (%): \\n\",clflrgrs.best_score_*100)\n#Dictionary Containing the parameters \nprint(\"Best Parameters: \\n\",clflrgrs.best_params_)\n\nprint(\"Best Estimators: \\n\",clflrgrs.best_estimator_)","7884810c":"from sklearn.model_selection import RandomizedSearchCV as rs\n# Decision Tree with random search\nparameters={\"min_samples_split\":range(10,200,10),\"max_depth\":range(1,20,1)}\nclf_treers=DecisionTreeClassifier()\nclfrs=rs(clf_treers,parameters,cv=5,scoring=\"precision\")\nclfrs.fit(trainX,trainy)\npredictions = clfrs.predict(testX)\naccdtrs=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of Decision Tree after Hyperparameter Tuning (%): \\n\",accdtrs)  \n#get FPR\nfprdtrs, tprdtrs, _ = roc_curve(testy, predictions)\n#aucdtrs=auc(fprdtrs, tprdtrs)*100\n#print(\"AUC OF Decision Tree after Hyperparameter Tuning (%): \\n\",aucdtrs)\nrecalldtrs=recall_score(testy,predictions)*100\nprint(\"Recall of Decision Tree after Hyperparameter Tuning is: \\n\",recalldtrs)\nprecdtrs=precision_score(testy,predictions)*100\nprint(\"Precision of Decision Tree after Hyperparameter Tuning is: \\n\",precdtrs)\n\n#examnine the best model\n#single best score achieved accross all params\nprint(\"Best Score (%): \\n\",clfrs.best_score_*100)\n#Dictionary Containing the parameters \nprint(\"Best Parameters: \\n\",clfrs.best_params_)\n\nprint(\"Best Estimators: \\n\",clfrs.best_estimator_)","ae2e20de":"from sklearn.model_selection import RandomizedSearchCV as rs\n# Neural Networks with random search\nparameters={\"solver\":['lbfgs','sgd','adam'],\"hidden_layer_sizes\":range(1,100,1),\"batch_size\":range(50,250,10),\"max_iter\":range(100,500,50),\"learning_rate\":['constant', 'invscaling', 'adaptive'],\"activation\":['identity', 'logistic', 'tanh', 'relu']}\nclf_nn=MLPClassifier()\nclfnnrs=rs(clf_nn,parameters,cv=5,scoring=\"precision\")\nclfnnrs.fit(trainX,trainy)\npredictions = clfnnrs.predict(testX)\naccnnrs=accuracy_score(testy, predictions)*100\nprint(\"Accuracy of Neural Networks after Hyperparameter Tuning (%): \\n\",accnnrs)  \n#get FPR\nfprnnrs, tprnnrs, _ = roc_curve(testy, predictions)\n#aucdtrs=auc(fprdtrs, tprdtrs)*100\n#print(\"AUC OF Decision Tree after Hyperparameter Tuning (%): \\n\",aucdtrs)\nrecallnnrs=recall_score(testy,predictions)*100\nprint(\"Recall of Neural Networks after Hyperparameter Tuning is: \\n\",recallnnrs)\nprecnnrs=precision_score(testy,predictions)*100\nprint(\"Precision of Neural Networks after Hyperparameter Tuning is: \\n\",precnnrs)\n\n#examnine the best model\n#single best score achieved accross all params\nprint(\"Best Score (%): \\n\",clfnnrs.best_score_*100)\n#Dictionary Containing the parameters \nprint(\"Best Parameters: \\n\",clfnnrs.best_params_)\n\nprint(\"Best Estimators: \\n\",clfnnrs.best_estimator_)","61045fb6":"import sklearn.metrics as metrics\nroc_auc1=metrics.auc(fprknnrs,tprknnrs)\nroc_auc2=metrics.auc(fprdtrs,tprdtrs)\nroc_auc3=metrics.auc(fprnnrs,tprnnrs)\nroc_auc4=metrics.auc(fprlrgrs,tprlrgrs)\n\n# Method-I: PLot\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(15,10))\nplt.title(\"Receiver Operating Curve\")\nplt.plot(fprknnrs,tprknnrs,\"b\",label=\"ROC of KNN after RS= %0.2f\" % roc_auc1)\nplt.plot(fprdtrs,tprdtrs,\"r\",label=\"ROC of Decision Tree after RS= %0.2f\" % roc_auc2)\nplt.plot(fprnnrs,tprnnrs,\"g\",label=\"ROC of Neural Networks after RS= %0.2f\" % roc_auc3)\nplt.plot(fprlrgrs,tprlrgrs,\"k\",label=\"ROC of Logistic Regression after RS= %0.2f\" % roc_auc4)\nplt.rcParams.update({'font.size': 20})\n\nplt.legend(loc=\"lower right\")\nplt.plot([0, 1],[0, 1],\"r--\")\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel(\"True Positive Rate\")\nplt.xlabel(\"False Positive Rate\")\n\n\nplt.rc('axes', labelsize=15)\nplt.rc('axes', titlesize=22)","7f8510b7":"algos1=[\"K Nearest Neighbor\",\"Neural Networks\",\"Decision Tree\",\"Logistic Regression\"]\nacc1=[accknn,accnn,accdt,acclrg]\nrecall1=[recallknn,recallnn,recalldt,recalllrg]\nprec1=[precknn,precnn,precdt,preclrg]\ncomp1={\"Algorithms\":algos1,\"Accuracies before RS\":acc1,\"Recall before RS\":recall1,\"Precision before RS\":prec1}\ncompdf1=pd.DataFrame(comp1)\ndisplay(compdf1.sort_values(by=[\"Accuracies before RS\",\"Recall before RS\",\"Precision before RS\"], ascending=False))\nacc2=[accknnrs,accnnrs,accdtrs,acclrgrs]\nrecall2=[recallknnrs,recallnnrs,recalldtrs,recalllrgrs]\nprec2=[precknnrs,precnnrs,precdtrs,preclrgrs]\ncomp2={\"Algorithms\":algos1,\"Accuracies after RS\":acc2,\"Recall after RS\":recall2,\"Precision after RS\":prec2}\ncompdf2=pd.DataFrame(comp2)\ndisplay(compdf2.sort_values(by=[\"Accuracies after RS\",\"Recall after RS\",\"Precision after RS\"], ascending=False))","8d9424b0":"# Comparison of all the Machine Learning Algorithms by Comparing some Evaluation Metrics","6a530322":"## Random forest Algorithm","f721ac68":"## Logistic Regression Algorithm","9df729ec":"## Ada Boost Algorithm","c5db8a96":"## Gaussian Naive Bayes Algorithm","be05fad6":"## Support Vector Machine Algorithm","72ffb7ca":"# ROC of all the Machine Learning Algorithms on default parameters","391e06c5":"## Gradient Descent Boosting Algorithm","65c65786":"## Hyperparameter Tuning on K Nearest Neighbor using Random Search","4f5cf9ad":"## Decision Tree Algorithm","cab9a406":"### **Train Test Split**","d5118fb5":"# Comparision of 4 algorithms before and after hyperparameter tuning","aacc352a":"# **Feature Engineering Using Random Forest Algorithm**","49d407a0":"# Hyperparameter Tuning using Random Search on any 4 Algorithms","037f3826":"# ROC Graph after Hyperparameter Tuning using Random Search","35311580":"## Neural Networks Algorithm","ebd48a79":"## K Nearest Neighbor Algorithm","7f921ea8":"## Hyperparameter Tuning on Neural Networks using Random Search\n","30015a56":"## Hyperparameter Tuning on Decision Tree using Random Search","78b055ec":"# Hyperparameter Tuning on Logistic Regression using Random Search","730f3935":"# All Machine Learning Algorithms with Default Parameters"}}