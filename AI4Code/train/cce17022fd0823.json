{"cell_type":{"b2473b2a":"code","b13efcc8":"code","52cc8456":"code","70933919":"code","94c4e4c3":"code","6cc256ca":"code","04722009":"code","ebe1a056":"code","f481a89a":"code","0c4c1393":"code","d3615852":"code","b145f9c1":"code","1385e1d7":"code","afeaef42":"code","76f96d20":"code","fc2aa116":"code","e1ec6d26":"code","07c4bed4":"code","8ffc944f":"code","e14612e5":"code","cdacdf9f":"code","2668b641":"code","b8f4ccc6":"code","f165f92f":"code","b56efca5":"code","f99d0293":"code","85226ec9":"code","529c711d":"code","a35010a9":"code","a8461fbe":"code","11a80c40":"code","462924d3":"code","3456e3ee":"code","e16b25c8":"code","a4606729":"code","98800b08":"code","531c0589":"code","7538c997":"code","b6ad2864":"code","b5e2f5f3":"code","7a9e2f4a":"code","5774da40":"code","93b00dcf":"code","c56da8d9":"markdown","92345a42":"markdown","0da7c133":"markdown","e32fb4b7":"markdown","0a25ef6a":"markdown","ec9534ac":"markdown","d2f24608":"markdown","d600a27f":"markdown","99132427":"markdown","0f1ee847":"markdown","5bd3cc1a":"markdown","f0980168":"markdown","9211756f":"markdown","67f38086":"markdown","a01ed9bd":"markdown","5bf0e6c1":"markdown","935ef840":"markdown","2e8fbd95":"markdown","06b6ac72":"markdown","30555b3a":"markdown","8bec46cd":"markdown","080e62e9":"markdown","8fb87b2f":"markdown","850dfec3":"markdown","e398a9a1":"markdown"},"source":{"b2473b2a":"import pandas as pd\nimport numpy as np\nimport spacy\nfrom spacy import displacy\nfrom spacy.util import minibatch, compounding\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","b13efcc8":"food_reviews_df=pd.read_csv('..\/input\/amazon-fine-food-reviews\/Reviews.csv')\nfood_reviews_df.shape","52cc8456":"food_reviews_df.head().T","70933919":"food_reviews_df = food_reviews_df[['Text','Score']].dropna()","94c4e4c3":"ax=food_reviews_df.Score.value_counts().plot(kind='bar')\nfig = ax.get_figure()\nfig.savefig(\"score.png\");","6cc256ca":"food_reviews_df.Score[food_reviews_df.Score<=3]=0\nfood_reviews_df.Score[food_reviews_df.Score>=4]=1","04722009":"ax=food_reviews_df.Score.value_counts().plot(kind='bar')\nfig = ax.get_figure()\nfig.savefig(\"score_boolean.png\");","ebe1a056":"food_reviews_df.head()","f481a89a":"train_pos_df=food_reviews_df[food_reviews_df.Score==1][:50000]\ntrain_neg_df=food_reviews_df[food_reviews_df.Score==0][:50000]","0c4c1393":"train_df=train_pos_df.append(train_neg_df)\ntrain_df.shape","d3615852":"val_pos_df=food_reviews_df[food_reviews_df.Score==1][50000:60000]\nval_neg_df=food_reviews_df[food_reviews_df.Score==0][50000:60000]\nval_df=val_pos_df.append(val_neg_df)\nval_df.shape","b145f9c1":"spacy_tok = spacy.load('en_core_web_sm')\nsample_review=food_reviews_df.Text[54]\nsample_review","1385e1d7":"parsed_review = spacy_tok(sample_review)\nparsed_review","afeaef42":"!wget https:\/\/raw.githubusercontent.com\/tylerneylon\/explacy\/master\/explacy.py","76f96d20":"import explacy\nexplacy.print_parse_info(spacy_tok, 'The salad was surprisingly tasty.')","fc2aa116":"explacy.print_parse_info(spacy_tok,food_reviews_df.Text[0])","e1ec6d26":"tokenized_text = pd.DataFrame()\n\nfor i, token in enumerate(parsed_review):\n    tokenized_text.loc[i, 'text'] = token.text\n    tokenized_text.loc[i, 'lemma'] = token.lemma_,\n    tokenized_text.loc[i, 'pos'] = token.pos_\n    tokenized_text.loc[i, 'tag'] = token.tag_\n    tokenized_text.loc[i, 'dep'] = token.dep_\n    tokenized_text.loc[i, 'shape'] = token.shape_\n    tokenized_text.loc[i, 'is_alpha'] = token.is_alpha\n    tokenized_text.loc[i, 'is_stop'] = token.is_stop\n    tokenized_text.loc[i, 'is_punctuation'] = token.is_punct\n\ntokenized_text[:20]","07c4bed4":"spacy.displacy.render(parsed_review, style='ent', jupyter=True)","8ffc944f":"spacy.explain('GPE') # to explain POS tag","e14612e5":"sentence_spans = list(parsed_review.sents)\nsentence_spans","cdacdf9f":"displacy.render(parsed_review, style='dep', jupyter=True,options={'distance': 140})","2668b641":"options = {'compact': True, 'bg': 'violet','distance': 140,\n           'color': 'white', 'font': 'Trebuchet MS'}\ndisplacy.render(parsed_review, jupyter=True, style='dep', options=options)","b8f4ccc6":"spacy.explain(\"ADJ\") ,spacy.explain(\"det\") ,spacy.explain(\"ADP\") ,spacy.explain(\"prep\")  # to understand tags","f165f92f":"noun_chunks_df = pd.DataFrame()\n\nfor i, chunk in enumerate(parsed_review.noun_chunks):\n    noun_chunks_df.loc[i, 'text'] = chunk.text\n    noun_chunks_df.loc[i, 'root'] = chunk.root,\n    noun_chunks_df.loc[i, 'root.text'] = chunk.root.text,\n    noun_chunks_df.loc[i, 'root.dep_'] = chunk.root.dep_\n    noun_chunks_df.loc[i, 'root.head.text'] = chunk.root.head.text\n\nnoun_chunks_df[:20]","b56efca5":"!pip install scattertext\nimport scattertext as st\nnlp = spacy.load('en',disable_pipes=[\"tagger\",\"ner\"])","f99d0293":"nlp = spacy.load('en',disable_pipes=[\"tagger\",\"ner\"])\ntrain_df['parsed'] = train_df.Text[49500:50500].apply(nlp)\ncorpus = st.CorpusFromParsedDocuments(train_df[49500:50500],\n                             category_col='Score',\n                             parsed_col='parsed').build()","85226ec9":"html = st.produce_scattertext_explorer(corpus,\n          category=1,\n          category_name='Positive',\n          not_category_name='Negative',\n          width_in_pixels=700,\n          minimum_term_frequency=15,\n          term_significance = st.LogOddsRatioUninformativeDirichletPrior(),\n          )","529c711d":"# uncomment this cell to load the interactive scattertext visualisation\nfilename = \"positive-vs-negative.html\"\nopen(filename, 'wb').write(html.encode('utf-8'))\nIFrame(src=filename, width = 900, height=900)\n","a35010a9":"!pip install sense2vec==1.0.0a0","a8461fbe":"import sense2vec\nfrom sense2vec import Sense2VecComponent\n\ns2v = Sense2VecComponent('..\/input\/reddit-vectors-for-sense2vec-spacy\/reddit_vectors-1.1.0\/reddit_vectors-1.1.0\/')\nspacy_tok.add_pipe(s2v)\ndoc = spacy_tok(u\"dessert.\")\nfreq = doc[0]._.s2v_freq\nvector = doc[0]._.s2v_vec\nmost_similar = doc[0]._.s2v_most_similar(5)\nmost_similar,freq","11a80c40":"doc = spacy_tok(u\"burger\")\nmost_similar = doc[0]._.s2v_most_similar(4)\nmost_similar","462924d3":"doc = spacy_tok(u\"peanut butter\")\nmost_similar = doc[0]._.s2v_most_similar(4)\nmost_similar","3456e3ee":"train_df['tuples'] = train_df.apply(\n    lambda row: (row['Text'],row['Score']), axis=1)\ntrain = train_df['tuples'].tolist()\ntrain[:1]","e16b25c8":"train[-2:]","a4606729":"#functions from spacy documentation\ndef load_data(limit=0, split=0.8):\n    train_data = train\n    np.random.shuffle(train_data)\n    train_data = train_data[-limit:]\n    texts, labels = zip(*train_data)\n    cats = [{'POSITIVE': bool(y)} for y in labels]\n    split = int(len(train_data) * split)\n    return (texts[:split], cats[:split]), (texts[split:], cats[split:])\n\ndef evaluate(tokenizer, textcat, texts, cats):\n    docs = (tokenizer(text) for text in texts)\n    tp = 1e-8  # True positives\n    fp = 1e-8  # False positives\n    fn = 1e-8  # False negatives\n    tn = 1e-8  # True negatives\n    for i, doc in enumerate(textcat.pipe(docs)):\n        gold = cats[i]\n        for label, score in doc.cats.items():\n            if label not in gold:\n                continue\n            if score >= 0.5 and gold[label] >= 0.5:\n                tp += 1.\n            elif score >= 0.5 and gold[label] < 0.5:\n                fp += 1.\n            elif score < 0.5 and gold[label] < 0.5:\n                tn += 1\n            elif score < 0.5 and gold[label] >= 0.5:\n                fn += 1\n    precision = tp \/ (tp + fp)\n    recall = tp \/ (tp + fn)\n    f_score = 2 * (precision * recall) \/ (precision + recall)\n    return {'textcat_p': precision, 'textcat_r': recall, 'textcat_f': f_score}\n\n#(\"Number of texts to train from\",\"t\" , int)\nn_texts=30000\n#You can increase texts count if you have more computational power.\n\n#(\"Number of training iterations\", \"n\", int))\nn_iter=10","98800b08":"nlp = spacy.load('en_core_web_sm')  # create english Language class","531c0589":"# add the text classifier to the pipeline if it doesn't exist\n# nlp.create_pipe works for built-ins that are registered with spaCy\nif 'textcat' not in nlp.pipe_names:\n    textcat = nlp.create_pipe('textcat')\n    nlp.add_pipe(textcat, last=True)\n# otherwise, get it, so we can add labels to it\nelse:\n    textcat = nlp.get_pipe('textcat')\n\n# add label to text classifier\ntextcat.add_label('POSITIVE')\n\n# load the dataset\nprint(\"Loading food reviews data...\")\n(train_texts, train_cats), (dev_texts, dev_cats) = load_data(limit=n_texts)\nprint(\"Using {} examples ({} training, {} evaluation)\"\n      .format(n_texts, len(train_texts), len(dev_texts)))\ntrain_data = list(zip(train_texts,\n                      [{'cats': cats} for cats in train_cats]))","7538c997":"# get names of other pipes to disable them during training\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'textcat']\nwith nlp.disable_pipes(*other_pipes):  # only train textcat\n    optimizer = nlp.begin_training()\n    print(\"Training the model...\")\n    print('{:^5}\\t{:^5}\\t{:^5}\\t{:^5}'.format('LOSS', 'P', 'R', 'F'))\n    for i in range(n_iter):\n        losses = {}\n        # batch up the examples using spaCy's minibatch\n        batches = minibatch(train_data, size=compounding(4., 32., 1.001))\n        for batch in batches:\n            texts, annotations = zip(*batch)\n            nlp.update(texts, annotations, sgd=optimizer, drop=0.2,\n                       losses=losses)\n        with textcat.model.use_params(optimizer.averages):\n            # evaluate on the dev data split off in load_data()\n            scores = evaluate(nlp.tokenizer, textcat, dev_texts, dev_cats)\n        print('{0:.3f}\\t{1:.3f}\\t{2:.3f}\\t{3:.3f}'  # print a simple table\n              .format(losses['textcat'], scores['textcat_p'],\n                      scores['textcat_r'], scores['textcat_f']))\n","b6ad2864":"# test the trained model\ntest_text1 = '\"you are a horrible person.\"'\ntest_text2=\"I bought this product at a local store, not from this seller.  I usually use Wellness canned food, but thought my cat was bored and wanted something new.  So I picked this up, knowing that Evo is a really good brand (like Wellness).<br \/><br \/>It is one of the most disgusting smelling cat foods I've ever had the displeasure of using.  I was gagging while trying to put it into the bowl.  My cat took one taste and walked away, and chose to eat nothing until I replaced it 12 hours later with some dry food.  I would try another flavor of their food - since I know it's high quality - but I wouldn't buy the duck flavor again.\"\ndoc = nlp(test_text1)\ndoc.cats","b5e2f5f3":"doc2 = nlp(test_text2)\ntest_text2, doc2.cats","7a9e2f4a":"output_dir=%pwd\n#nlp.to_disk(output_dir)\nnlp.to_disk('\/kaggle\/working\/sharma')\n","5774da40":"import os\nos.chdir(r'\/kaggle\/working')\nfrom IPython.display import FileLinks\nFileLinks(r'ananya')\n","93b00dcf":"# test the saved model\nprint(\"Loading from\", output_dir)\nnlp2 = spacy.load(output_dir)\ndoc2 = nlp2(test_text2)\nprint(test_text2, doc2.cats)","c56da8d9":"Positive review is indeed close to 1","92345a42":"Ok let's do some modelling and focus on scoring our food!!","0da7c133":"### Loading data","e32fb4b7":"#### Dependency parsing\nSyntactic Parsing or Dependency Parsing is process of identifyig sentenses and assigning a syntactic structure to it.\nAs in Subject combined with object makes a sentence.\nSpacy provides parse tree which can be used to generate this structure.\n\n##### Sentense Boundry Detection\nFiguring out where sentense starts and ends is very imporatnt part of nlp.","0a25ef6a":"Similarity between entities can be kind of fun.\n\n\nThe following attributes are available via the ._ property \u2013 for example token._.in_s2v:\n\nName\t|Attribute Type|\tType|\tDescription|\n--------|---------------|-------------|---------------|\nin_s2v\t|property|\tbool|\tWhether a key exists in the vector map.\ns2v_freq|\tproperty|\tint|\tThe frequency of the given key.\ns2v_vec|\tproperty|\tndarray[float32]|\tThe vector of the given key.\ns2v_most_similar|\tmethod|\tlist|\tGet the n most similar terms. Returns a list of ((word, sense), score) tuples.\n\n\n\n## SpaCy Text Categorizer\n\nWe will train a multi-label convolutional neural network text classifier on our food reviews, using spaCy's new TextCategorizer  component.\n\nSpaCy provides classification model with multiple, non-mutually exclusive labels. You can change the model architecture rather easily, but by default, the TextCategorizer class uses a convolutional neural network to assign position-sensitive vectors to each word in the document. The TextCategorizer uses its own CNN model, to avoid sharing weights with the other pipeline components. The document tensor is then summarized by concatenating max and mean pooling, and a multilayer perceptron is used to predict an output vector of length nr_class, before a logistic activation is applied elementwise. The value of each output neuron is the probability that some class is present.","ec9534ac":"Kindly scroll down if you can't see the output above.\nYou can even customize dependency parser's output as below.","d2f24608":"Negative review is close to 0","d600a27f":"Since we have huge data, since it might be difficult to train in kernel, I will reduce data size of 100K rows.\nTo balance classes, i have selected equal samples from each class.","99132427":"Model looks preety good. We can definitely improve it further by feeding more data and data augmentations.\nThanks for reading. Hope you learnt something new :)  #TODO Data Augmentation.","0f1ee847":"Let's focus on texual data and ratings for text classification.","5bd3cc1a":"### Linguistic features","f0980168":"#### Named Entity Recognition (NER)\nNamed entity is real world object like Person, Organization etc\n\nSpacy figures out below entities automatically:\n\n|Type\t|Description|\n|------|------|\n|PERSON|\tPeople, including fictional.\n|NORP|\tNationalities or religious or political groups.|\n|FAC|\tBuildings, airports, highways, bridges, etc.|\n|ORG|\tCompanies, agencies, institutions, etc.|\n|GPE|\tCountries, cities, states.|\n|LOC|\tNon-GPE locations, mountain ranges, bodies of water.|\n|PRODUCT|\tObjects, vehicles, foods, etc. (Not services.)|\n|EVENT|\tNamed hurricanes, battles, wars, sports events, etc.|\n|WORK_OF_ART|\tTitles of books, songs, etc.|\n|LAW|\tNamed documents made into laws.|\n|LANGUAGE|\tAny named language.|\n|DATE|\tAbsolute or relative dates or periods.|\n|TIME|\tTimes smaller than a day.|\n|PERCENT|\tPercentage, including \"%\".|\n|MONEY|\tMonetary values, including unit.|\n|QUANTITY|\tMeasurements, as of weight or distance.|\n|ORDINAL|\t\"first\", \"second\", etc.|\n|CARDINAL|\tNumerals that do not fall under another type|","9211756f":"Let's read in food reviews data","67f38086":"#### Prepare data\nLet's prepare the data as SpaCy would like it.\nIt accepts list of tuples of text and labels.","a01ed9bd":"### Sence2vec\n\nThe idea is get something better than word2vec model.\n\nThe idea behind sense2vec is super simple. If the problem is that duck as in waterfowl and duck as in crouch are different concepts, the straight-forward solution is to just have two entries, duckN and duckV.  Trask et al (2015) published a nice set of experiments showing that the idea worked well.\n\nIt assight parts of speech tags like verb, noun , adjective to words, which will in turn be used to make sence of context.\n1. Please book [VERB] my ticket.\n2. Read the book [NOUN].\n\nRead more [here](https:\/\/explosion.ai\/blog\/sense2vec-with-spacy) and [here](https:\/\/github.com\/explosion\/sense2vec)\n\nReddit talks about food a lot so we can get nice similarity vectors for food items.","5bf0e6c1":"#### Processing Noun chunks","935ef840":"### Introduction\n\n#### About Dataset:\nWe will be using rich dataset of amazon fine food reviews.\n\n####  What are we trying to achieve??\nWe are going to tackle an interesting natural language processing problem i.e sentiment or text classification.\nWe will explore texual data using amazing spaCy library and build a text classification model.\n\n### Here is breakdown of concepts I will try to explain.\nWe will extract linguistic features like \n1. tokenization,\n1. part-of-speech tagging, \n1. dependency parsing, \n1. lemmatization , \n1. named entities recognition,\n1. Sentence Boundary Detection\t\nfor building language models later.\n\nVisualizing Data\n1. explacy - explaining how parsing is done\n1. displaCy - visualizing named entities\n\nWord vectors and similarity\n1. sense2vec - using contextual information for building word embeddings\n\nText classification model\n1. SpaCy TextCategorizer","2e8fbd95":"### Visualizing using Scattertext","06b6ac72":"### Training model","30555b3a":"There is not much difference between parsed review and original one. But we will see ahead what has actually happened.\nWe can see how parsing has been done visually through **explacy**.","8bec46cd":"We have five-star rating system.\nIt looks like we have more reviews with ratings 5, this can lead to unbalanced classes. We will treat rating 4 and 5 as positive and rest as negative reviews.","080e62e9":"### Word vectors and similarity","8fb87b2f":"Text column contains review given by customer.","850dfec3":"#### Part-of-speech tagging\nAfter tokenization we can parse and tag variety of parts of speech to paragraph text. SpaCy uses statistical models in background to predict which tag will go for each word(s) based on the context.\n\n##### Lemmatization\nIt is the process of extracting uninflected\/base form of the word.\nLemma can be like\nFor eg. \n\nAdjectives: best, better \u2192 good\nAdverbs: worse, worst \u2192 badly\nNouns: ducks, children \u2192 duck, child\nVerbs: standing,stood \u2192 stand\n","e398a9a1":"#### Tokenization\nFirst step in any nlp pipeline is tokenizing text i.e breaking down paragraphs into sentenses and then sentenses into words, punctuations and so on.\n\nwe will load english language model to tokenize our english text.\n\nEvery language is different and have different rules. Spacy offers 8 different language models."}}