{"cell_type":{"dc8755cc":"code","a4bfe50c":"code","8c78c476":"code","3cef13da":"code","2a97780f":"code","e695087b":"code","f915c870":"code","b8477f5f":"code","b4f6a1dc":"code","e4edb809":"code","98b6f0e6":"code","a3c48884":"code","bd0bcba5":"code","40834b10":"code","240b52e9":"code","0c9e6502":"code","308ae3d6":"code","44119194":"markdown","7a4aec30":"markdown","2d9bcad5":"markdown","d8b28479":"markdown","5140c5db":"markdown","05a07f7a":"markdown","1da099c2":"markdown","4baaec96":"markdown"},"source":{"dc8755cc":"import pandas as pd\nimport numpy as np\ncars = pd.read_csv('..\/input\/Imports85Imp.csv') # imported the dataset given by you\ncars.head() # shows the first five rows which helps to what kind of data is presented and can think of how to interpret the data\n","a4bfe50c":"drop1=[\"V1_imp\",\"V2_imp\",\"V3_imp\",\"V4_imp\",\"V4_imp\",\"V5_imp\",\"V6_imp\",\"V6_imp\",\"V7_imp\",\"V8_imp\",\"V9_imp\",\"V10_imp\",\"V11_imp\",\"V12_imp\",\"V13_imp\",\n      \"V14_imp\",\"V15_imp\",\"V16_imp\",\"V17_imp\",\"V18_imp\",\"V19_imp\",\"V20_imp\",\"V21_imp\",\"V22_imp\",\"V23_imp\",\"V24_imp\",\"V25_imp\",\"V26_imp\",\n       ]\ncars1=cars.drop(drop1, axis=1)","8c78c476":"cars1.columns=['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration', 'num-of-doors', 'body-style', 'drive-wheels', 'engine-location', 'wheel-base', \n           'length', 'width', 'height', 'curb-weight', 'engine-type', 'num-of-cylinders', 'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-ratio',\n            'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\ncars1.head()\nl = cars1['make'] # we are considering lable make only for visulization purpose, PCA Analysis will be done afterwards\n","3cef13da":"drop2=['symboling', 'normalized-losses', 'make', 'fuel-type', 'aspiration', 'num-of-doors', 'body-style',\n       'drive-wheels', 'engine-location','engine-type', 'num-of-cylinders', 'fuel-system', ]\ncars2=cars1.drop(drop2, axis=1)# removing all other categorical to select continous attributes\ncars2.head()","2a97780f":"cars2.shape","e695087b":"from sklearn import decomposition\npca = decomposition.PCA()","f915c870":"# configuring the parameteres\n# the number of components = 2\npca.n_components = 2\npca_data = pca.fit_transform(cars2)\n\n# pca_reduced will contain the 2-d projects of simple data\nprint(\"shape of pca_reduced.shape = \", pca_data.shape)","b8477f5f":"# attaching the label for each 2-d data point \nimport seaborn as sn\nimport matplotlib.pyplot as plt\npca_data = np.vstack((pca_data.T, l)).T\n\n# creating a new data fram which help us in ploting the result data\npca_df = pd.DataFrame(data=pca_data, columns=(\"1st_principal\", \"2nd_principal\", \"make\"))\nsn.FacetGrid(pca_df,hue=\"make\",size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()","b4f6a1dc":"pca.n_components = 14\n\npca_data = pca.fit_transform(cars2)\npercentage_var_explained = pca.explained_variance_ \/ np.sum(pca.explained_variance_);\n\ncum_var_explained = np.cumsum(percentage_var_explained)\n\n# Plot the PCA spectrum\nplt.figure(1, figsize=(6, 4))\n\nplt.clf()\nplt.plot(cum_var_explained,linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()","e4edb809":"from sklearn.preprocessing import StandardScaler\nstandardized_data = StandardScaler().fit_transform(cars2)\nprint(standardized_data.shape)\nstandardized_data","98b6f0e6":"# configuring the parameteres\n# the number of components = 2\npca.n_components = 2\npca_data = pca.fit_transform(standardized_data)\n\n# pca_reduced will contain the 2-d projects of simple data\nprint(\"shape of pca_reduced.shape = \", pca_data.shape)","a3c48884":"import seaborn as sn\nimport matplotlib.pyplot as plt\npca_data = np.vstack((pca_data.T, l)).T\n\n# creating a new data fram which help us in ploting the result data\npca_df = pd.DataFrame(data=pca_data, columns=(\"1st_principal\", \"2nd_principal\", \"make\"))\nsn.FacetGrid(pca_df,hue=\"make\",size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()\n","bd0bcba5":"pca.n_components = 14\npca_data = pca.fit_transform(standardized_data)\n\npercentage_var_explained = pca.explained_variance_ \/ np.sum(pca.explained_variance_);\n\ncum_var_explained = np.cumsum(percentage_var_explained)\n\n# Plot the PCA spectrum\nplt.figure(1, figsize=(6, 4))\n\nplt.clf()\nplt.plot(cum_var_explained,linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()","40834b10":"from sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\nnormalized_data = preprocessing.normalize(cars2)\nprint(normalized_data.shape)\nnormalized_data","240b52e9":"# configuring the parameteres\n# the number of components = 2\npca.n_components = 2\npca_data_n = pca.fit_transform(normalized_data)\n\n# pca_reduced will contain the 2-d projects of simple data\nprint(\"shape of pca_reduced.shape = \", pca_data.shape)","0c9e6502":"import seaborn as sn\nimport matplotlib.pyplot as plt\npca_data = np.vstack((pca_data_n.T, l)).T\n\n# creating a new data fram which help us in ploting the result data\npca_df = pd.DataFrame(data=pca_data, columns=(\"1st_principal\", \"2nd_principal\", \"make\"))\nsn.FacetGrid(pca_df,hue=\"make\",size=6).map(plt.scatter, '1st_principal', '2nd_principal').add_legend()\nplt.show()","308ae3d6":"pca.n_components = 14\npca_data = pca.fit_transform(normalized_data)\n\npercentage_var_explained = pca.explained_variance_ \/ np.sum(pca.explained_variance_);\n\ncum_var_explained = np.cumsum(percentage_var_explained)\n\n# Plot the PCA spectrum\nplt.figure(1, figsize=(6, 4))\n\nplt.clf()\nplt.plot(cum_var_explained,linewidth=2)\nplt.axis('tight')\nplt.grid()\nplt.xlabel('n_components')\nplt.ylabel('Cumulative_explained_variance')\nplt.show()","44119194":"Here 14 continous variables are reduced to 2 components without any loss of information. ","7a4aec30":"### Raw Data","2d9bcad5":" PCA on Raw data , Standardized Data and Normalized Data","d8b28479":"### Standardized Data","5140c5db":"### Normalized data","05a07f7a":"Here 14 continous variables are reduced to 2 components without any loss of information. We can see even 1 component     or 1 variable with variance 99.9%. However its not advisable to create a model based on th RAW data as scales of all 14 continuous variables are different\n","1da099c2":"__lets see how this raw data, standardized data and normalized data will comapare each other.__\n\nIn general data mining applications, it depends upon the business goal and type of attributes as here price, peak-rpm, curb weight results in higher range of values compared to other attributes which implies, these are not bounded during standization and raw data which provides poor results compared to normalized data.\n\n1.Our main goal PCA is to reduce the dimensions with out any loss of information. Here we maximum loss of informations is with Standardized data where 14 variables are reduced to 12 where as raw data provides maximum vraiance with out loss of information if we consider 2 variables. How ever it is advised to choose the model based on the standardized data.\n\n2.if we compare the visulizations using \"MAKE\" atrribute, the normalized graph is better classfied compared to other graphs of raw and standarized data, its clealy indicated what is the goal we are trying to solve.\n\n3.Here we dont have outliers which helps to normalize values with larger interval\n\n4.To conclude whether raw data, standardized or normalized data where how many components maximizes the variance to be considered when decideing which type of data to be considered.\n\n","4baaec96":"Here 14 continous variables are reduced to 2 components without 22%(approx) loss of information but as its standardized with mean 0 and standard deviation, it is advisable to build the model on this. Howver only 100 % variance is explained if 14 variables are reduced 12 variables."}}