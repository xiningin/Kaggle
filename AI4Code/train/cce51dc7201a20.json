{"cell_type":{"7b5735a6":"code","c11fedcb":"code","c3a99164":"code","54104cf5":"code","d3b1fef4":"code","ef285fa6":"code","cce1ea17":"code","f5fc0bfa":"code","564c3044":"code","c0be56d3":"code","ffda3e23":"code","f901f09f":"code","42f459d8":"code","5b94bb07":"code","82a16268":"code","ab0e3923":"code","7099035c":"code","f441f2bf":"code","0ba13b8c":"code","24155081":"code","abfe065b":"code","82a53ac9":"code","af1068a9":"code","3266bd1a":"code","692ad680":"code","c96062c3":"code","7081524e":"code","1ff71be9":"code","56c83504":"code","fb694f0f":"code","693b68de":"code","0f214bcf":"code","636810b4":"code","107e258e":"code","046cf97a":"code","e8d592b1":"code","917bf51c":"code","6037c4eb":"code","5d2e2a8f":"code","55b5bba8":"code","7d0f9ff3":"code","8a319704":"code","805d393d":"code","bbd7cf46":"code","45b97466":"code","7a355544":"markdown","ff13b4fd":"markdown","d106f6f6":"markdown","aa3577ac":"markdown","fc61af77":"markdown","341e460b":"markdown","c87bb098":"markdown","7bfc9cff":"markdown","4583e091":"markdown","b40683e0":"markdown","6b9e7377":"markdown","5687042a":"markdown","bf5c87be":"markdown"},"source":{"7b5735a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport nltk \nfrom nltk import NaiveBayesClassifier\nfrom nltk import pos_tag\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\nimport re\nimport string \nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report , confusion_matrix , accuracy_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c11fedcb":"df = pd.read_csv('..\/input\/twitter-airline-sentiment\/Tweets.csv')\ndf.head()","c3a99164":"print(df.shape)","54104cf5":"# Fetching null entries for each column in variable null_entries_each_col and column name\ncolumns = df.columns \nnull_entries_each_col = list(df.isnull().sum())","d3b1fef4":"plt.bar(columns, null_entries_each_col, width = 0.5)\nplt.xticks(rotation = 90)\nplt.title(\"Null entries for each column\")","ef285fa6":"print(\"Total number of each sentiment\")\nprint(df.airline_sentiment.value_counts())\nsentiment_list = list(df.airline_sentiment.value_counts().values) \nsentiments = list(set(df.airline_sentiment))\nplt.bar(sentiments,sentiment_list, color=[\"blue\",\"green\",\"red\"])\nplt.title(\"Total sentiment count for each sentiment\")\nplt.show()","cce1ea17":"print(\"Total sentiment per airline\")\nprint(df.groupby('airline')['airline_sentiment'].count())\nairline_name = list(set(df['airline']))\nairline_sentiment_count = list(df.groupby('airline')['airline_sentiment'].count())\n\nplt.figure(figsize = (10, 8))\nplt.bar(airline_name,airline_sentiment_count,0.9)\nplt.title(\"Name vs sentiment \")\n","f5fc0bfa":"print(df.groupby('airline')['airline_sentiment'].value_counts())\ncount_per_airline = df.groupby('airline')['airline_sentiment'].value_counts()","564c3044":"positive = []\nnegative = []\nneutral = []\n\n\n\nfor airline in airline_name:\n    negative.append(count_per_airline[airline][\"negative\"])\n    positive.append(count_per_airline[airline][\"neutral\"])\n    neutral.append(count_per_airline[airline][\"positive\"])\n    \n    \nprint(\"----------------------------\")    \nprint(airline_name)\nprint(negative)\nprint(positive)\nprint(neutral)\nprint(\"----------------------------\")\n\nw = 0.3\nbar1 = np.arange(len(airline_name))\nbar2 = [i+w for i in bar1]\nbar3 = [i+w for i in bar2]\n\nplt.figure(figsize = (10, 8))\nplt.bar(bar1, negative, width = w, label=\"negative\",color = \"red\")\nplt.bar(bar2, positive,width = w, label=\"positive\", color = \"green\")\nplt.bar(bar3, neutral,width = w, label=\"neutral\", color = \"blue\")\nplt.xlabel(\"Airlines\")\nplt.ylabel(\"Count\")\nplt.xticks(bar1+w,airline_name)\nplt.legend()\nplt.show()\n\n","c0be56d3":"plt.figure(2, figsize = (13,5))\nfor i in airline_name:\n    index = airline_name.index(i)\n    plt.subplot(2,3, index+1)\n    plt.subplots_adjust(hspace=0.5)\n    \n    data = [count_per_airline[i][\"neutral\"],count_per_airline[i][\"positive\"],count_per_airline[i][\"negative\"]]\n    plt.pie(data, labels = sentiments,colors = [\"blue\",\"green\",\"red\"])\n    plt.title(i)","ffda3e23":"#Overall count for negative tweets \nprint(\"Reasons for negative tweets\")\nprint(df.negativereason.value_counts())\n\nreasons_for_negative = list(df.negativereason.value_counts().keys())\ncount_for_negative = list(df.negativereason.value_counts().values)\n#print(reasons_for_negative)\n#print(count_for_negative)\nplt.figure(figsize = (10, 8))\nplt.bar(reasons_for_negative,count_for_negative)\nplt.xticks(rotation = 90)","f901f09f":"# Airline wise count for negative tweets \nprint(df.groupby('airline')['negativereason'].value_counts())\nnegative_reason_airline = df.groupby('airline')['negativereason'].value_counts()\nprint(\"**************************************************\")\nprint(\"PLOT FOR INDIVIDUAL AIRLINE\")\nplt.figure(2,figsize = (13,13))\nfor airline in airline_name:\n    index = airline_name.index(airline)\n    plt.subplot(2,3,index+1)\n    plt.subplots_adjust(hspace = 0.9)\n    \n    individual_negative = negative_reason_airline[airline]\n    x_values = list(individual_negative.keys())\n    y_values = list(individual_negative.values)\n    plt.bar(x_values, y_values,label = airline )\n    plt.xticks(rotation = 90)\n    plt.title(airline)","42f459d8":"def convert_sentiment(x):\n    if x[\"airline_sentiment\"] == 'positive':\n        return 0\n    elif x[\"airline_sentiment\"] == 'negative':\n        return 1\n    elif x[\"airline_sentiment\"] == 'neutral':\n        return 2\n\nprint(\"BEFORE\")\ndf.airline_sentiment[0:10]","5b94bb07":"df[\"airline_sentiment\"] = df.apply(convert_sentiment,axis = 1)\nprint(\"AFTER\")\ndf.airline_sentiment[0:10] ","82a16268":"# This cell of code is taken from the community of kaggle \ndef remove_stopwords_punct(row):\n    stop_words = stopwords.words('english')\n    punctuation = list(string.punctuation)\n    stop = stop_words+punctuation  \n    \n    text = ' '.join([word for word in row.split() if word.lower() not in stop])\n    return text\n\ndef remove_url_username(row):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    text = url.sub(r'',row)\n    text = re.sub('@[^\\s]+','',text)\n    return text\n\ndef remove_html(row):\n    return re.sub(r'<.*?>',r'',row)\n\ndef remove_emoji(row):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n        \n\n    return emoji_pattern.sub(r'', row)\n\n    \ndef decontraction(row):\n    row = re.sub(r\"won\\'t\", \" will not\", row)\n    row = re.sub(r\"won\\'t've\", \" will not have\", row)\n    row = re.sub(r\"can\\'t\", \" can not\", row)\n    row = re.sub(r\"don\\'t\", \" do not\", row)\n    \n    row = re.sub(r\"can\\'t've\", \" can not have\", row)\n    row = re.sub(r\"ma\\'am\", \" madam\", row)\n    row = re.sub(r\"let\\'s\", \" let us\", row)\n    row = re.sub(r\"ain\\'t\", \" am not\", row)\n    row = re.sub(r\"shan\\'t\", \" shall not\", row)\n    row = re.sub(r\"sha\\n't\", \" shall not\", row)\n    row = re.sub(r\"o\\'clock\", \" of the clock\", row)\n    row = re.sub(r\"y\\'all\", \" you all\", row)\n    row = re.sub(r\"n\\'t\", \" not\", row)\n    row = re.sub(r\"n\\'t've\", \" not have\", row)\n    row = re.sub(r\"\\'re\", \" are\", row)\n    row = re.sub(r\"\\'s\", \" is\", row)\n    row = re.sub(r\"\\'d\", \" would\", row)\n    row = re.sub(r\"\\'d've\", \" would have\", row)\n    row = re.sub(r\"\\'ll\", \" will\", row)\n    row = re.sub(r\"\\'ll've\", \" will have\", row)\n    row = re.sub(r\"\\'t\", \" not\", row)\n    row = re.sub(r\"\\'ve\", \" have\", row)\n    row = re.sub(r\"\\'m\", \" am\", row)\n    row = re.sub(r\"\\'re\", \" are\", row)\n    return row\n\ndef seperate_alphanumeric(row):\n    words = row\n    words = re.findall(r\"[^\\W\\d_]+|\\d+\", words)\n    return \" \".join(words)\n\ndef cont_rep_char(row):\n    tchr = row.group(0) \n    \n    if len(tchr) > 1:\n        return tchr[0:2] \n\ndef unique_char(rep, row):\n    substitute = re.sub(r'(\\w)\\1+', rep, row)\n    return substitute\n\ndef char(row):\n    substitute = re.sub(r'[^a-zA-Z]',' ',row)\n    return substitute\n\n# combaine negative reason with  tweet (if exsist)\ndf['final_text'] = df['negativereason'].fillna('') + ' ' + df['text'] \n\n\n# Apply functions on tweets\ndf['final_text'] = df['final_text'].apply(lambda x : remove_url_username(x))\ndf['final_text'] = df['final_text'].apply(lambda x : remove_emoji(x))\ndf['final_text'] = df['final_text'].apply(lambda x : decontraction(x))\ndf['final_text'] = df['final_text'].apply(lambda x : seperate_alphanumeric(x))\ndf['final_text'] = df['final_text'].apply(lambda x : unique_char(cont_rep_char,x))\ndf['final_text'] = df['final_text'].apply(lambda x : char(x))\ndf['final_text'] = df['final_text'].apply(lambda x : remove_stopwords_punct(x))\n","ab0e3923":"df['final_text']\n","7099035c":"document = []\nfor row, catagory in zip(df.final_text, df.airline_sentiment):\n    document.append((row.split(),catagory))\n    ","f441f2bf":"lemmatizer = WordNetLemmatizer()\ndef get_simple_pos(tag):\n    \n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","0ba13b8c":"def clean_review(words):\n    output_words = []\n    for w in words:\n       \n        pos = pos_tag([w])\n        clean_word = lemmatizer.lemmatize(w, pos = get_simple_pos(pos[0][1]))\n        output_words.append(clean_word.lower())\n    return output_words","24155081":"document = [(clean_review(document), category) for document, category in document]","abfe065b":"import random\nrandom.shuffle(document)\ndocument[0:5]","82a53ac9":" \nprint(len(document))","af1068a9":"#Splitting for train and test \ntraining_data = document[:13000]\ntesting_data = document[13000:]","3266bd1a":"#Extracting features\nwords = []\nfor data in training_data:\n    words += data[0]\n    \nfreq = nltk.FreqDist(words)\ncommon = freq.most_common(3000)\nfeatures = [i[0] for i in common]\n","692ad680":"#creating feature dictionary \ndef feature_dictionary(row):\n    feature_dict = {}\n    words = set(row)\n    for w in features:\n        feature_dict[w] = w in words\n    return feature_dict ","c96062c3":"training_data = [(feature_dictionary(doc), catagory) for doc, catagory in training_data]\ntesting_data =  [(feature_dictionary(doc), catagory) for doc, catagory in testing_data] ","7081524e":"#training_data[0]","1ff71be9":"classifier = NaiveBayesClassifier.train(training_data)\nnltk.classify.accuracy(classifier,testing_data)","56c83504":"classifier.show_most_informative_features(15)","fb694f0f":"#Importing necessary modules \nfrom sklearn.svm import SVC\nfrom nltk.classify.scikitlearn import SklearnClassifier","693b68de":"svc = SVC()\nclassifier_sklearn = SklearnClassifier(svc)","0f214bcf":"classifier_sklearn.train(training_data)","636810b4":"nltk.classify.accuracy(classifier_sklearn,testing_data)","107e258e":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nclassifier_sklearn_rfc = SklearnClassifier(rfc)\nclassifier_sklearn_rfc.train(training_data)\nnltk.classify.accuracy(classifier_sklearn_rfc,testing_data)","046cf97a":"categories = [category for document, category in document]\ndocuments = [\" \".join(document) for document, category in document]","e8d592b1":"x_train, x_test, y_train, y_test = train_test_split(documents, categories,test_size = 0.10)","917bf51c":"count_vec = CountVectorizer(max_features = 2500,ngram_range = (1,2))\nx_train = count_vec.fit_transform(x_train)\nx_test = count_vec.transform(x_test)","6037c4eb":"svc = SVC()\nsvc.fit(x_train, y_train)","5d2e2a8f":"svc.score(x_test, y_test)","55b5bba8":"ds = DecisionTreeClassifier()\nds.fit(x_train,y_train)","7d0f9ff3":"ds_prediction =  ds.predict(x_test)","8a319704":"accuracy_score(ds_prediction,y_test)","805d393d":"rf = RandomForestClassifier()\nrf.fit(x_train,y_train)","bbd7cf46":"rf_prediction =  rf.predict(x_test)","45b97466":"accuracy_score(rf_prediction,y_test)","7a355544":"## Running sklearn classifier on the training_data that I've designed for nltk classifier using nltk dummy over sklearn classifier ","ff13b4fd":"### Desicion Tree","d106f6f6":"## In the trailing lines I'll use sklearn classifier to train the data and hence the data is designed on the base of the requirenment of sklearn classifier.","aa3577ac":"## In the trailing lines I'll use nltk classifier to train the data and hence the data is designed on the base of the requirenment of nltk classifier.","fc61af77":"### Random Forest ","341e460b":"## Counting count for each sentiment ","c87bb098":"# Data visualization \n## Plotting bar graph for null entries ","7bfc9cff":"## Using NLTK preprocessing kit \n","4583e091":"## Loading data in pandas dataframe","b40683e0":"## Reason for negative tweets ","6b9e7377":"# Splitting data, extracting features and Applying classifiers from nltk and sklearn ","5687042a":"# Text Cleaning and preprocessing ","bf5c87be":"## Sentiment count on the bases of each airline "}}