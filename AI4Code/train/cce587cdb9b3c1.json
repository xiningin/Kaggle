{"cell_type":{"098342cd":"code","f2c26d26":"code","1b085dc6":"code","0fc4a093":"markdown","80fa03e3":"markdown","a2aa7583":"markdown"},"source":{"098342cd":"import lightgbm as lgb\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn import preprocessing\nimport random, json\nfrom bayes_opt import BayesianOptimization\n\n# Load data for training \nmydata = pd.read_csv('..\/input\/train.csv', sep=',')\nmydata = mydata.drop('ID_code', 1)\n# Load prediction data\npreddata = pd.read_csv('..\/input\/test.csv', sep=',')\npredids = preddata[['ID_code']] \niddf = preddata[['ID_code']] \npreddata = preddata.drop('ID_code', 1)\n\n# Test train split\ndf_train, df_test = train_test_split(mydata, test_size=0.3, random_state=76)\n# Same random state to make sure rows merge\ny_train = df_train['target']\ny_test = df_test['target']\nX_train = df_train.drop('target', 1)\nX_test = df_test.drop('target', 1)\n\n# Scale data\nscaler = preprocessing.StandardScaler()\nscaled_df = scaler.fit_transform(X_train)\nX_train = pd.DataFrame(scaled_df)\nscaled_df = scaler.fit_transform(X_test)\nX_test = pd.DataFrame(scaled_df)\nscaled_df = scaler.fit_transform(preddata)\npreddata = pd.DataFrame(scaled_df)\n\n# Create dataset for lightgbm input\nlgb_train = lgb.Dataset(X_train, y_train)\nlgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)","f2c26d26":"def lgbmfunc(min_split_gain, min_child_weight):\n    params = {\n        'boost_from_average' : False,\n        'objective' :'binary',\n        'learning_rate' : 0.002,\n        'num_leaves' : 24, \n        'feature_fraction': 0.07, \n        'bagging_fraction': 0.2, \n        'bagging_freq': 3, \n        'max_bin' : 255, #default 255\n        'scale_pos_weight' : 1,  #default = 1 (used only in binary application) weight of labels with positive class\n        'boosting_type' : 'gbdt',\n        'metric': 'auc',\n        'num_threads' : 4,\n        'tree_learner': 'serial', \n        'boost_from_average':'false',\n        'min_child_samples': 3\n    }\n    params['min_split_gain'] = min_split_gain\n    params['min_child_weight'] = min_child_weight\n\n    # LGBM CV\n    cv_results = lgb.cv(params, lgb_train, num_boost_round=500, nfold=5, early_stopping_rounds=50, metrics='auc')\n    return max(cv_results['auc-mean'])\n\npbounds = { \n        'min_split_gain': (0.2, 0.3), \n        'min_child_weight': (0.1, 0.3),\n    }\n\noptimizer = BayesianOptimization(\n    f=lgbmfunc,\n    pbounds=pbounds,\n    verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n    random_state=1,\n)\n\noptimizer.maximize(\n    init_points=2,\n    n_iter=10,\n)\n","1b085dc6":"print(\"list of results\")\nfor i, res in enumerate(optimizer.res):\n    print(\"Iteration {}: \\n\\t{}\".format(i, res))\n\nprint(\"best result\")\nprint(optimizer.max)","0fc4a093":"**Aim of this Kernel:**\n\nIn this Kernel I show a minimal example how to implement Bayesian Optimisation (BO) with a Light Gradient Boosting Machine (LGBM). However, this example can easily be applied to other algorithms as well.\n\n**Introduction:**\n\nTuning of hyperparameters of boosting algorithms can be daunting. Apart of random search or grid search, Bayesian optimisation (BO) offers a more structured approach to parameter tuning. \n\nBayesian optimization aims at optimising a \u201cblack box function\u201d (here the boosting algorithms) without knowing much about the function. Optimisation is not based on derivatives or the like. Instead, previous outcomes are used to try to find a set of parameter which improve the objective (this is why it is called Bayesian). This post is very helpful to get an idea of what is going on with Bayesian optimisation: [http:\/\/krasserm.github.io\/2018\/03\/21\/bayesian-optimization\/](http:\/\/)\n\n**Getting Started:**\n\nAll we need for a start is the \u201cbayesian-optimization\u201d package (e.g. use *pip install bayesian-optimization*). The package is well documented and you can find a very helpful introduction here: [https:\/\/github.com\/fmfn\/BayesianOptimization\/blob\/master\/examples\/basic-tour.ipynb](http:\/\/).\n\n**All you need to do is to...**\n\n*a) Define a \u201cblack box function\u201d to be optimised\nb) Pass some parameter over which optimisation is done\nc) Start the BO routine*\n\n**Let's load and prepare the data first:**","80fa03e3":"**Results:**\n\nAfter the routine has finished, we can inspect the results obtained in each iteration. However, what is even more interesting is the optimal set of parameter which can be called by the .max command. It will look like this:\n\n*best result\n{'target': 0.8638091350110189, 'params': {'min_child_weight': 0.10002287496346898, 'min_split_gain': 0.23023325726318397}}*\n\n**Next Steps:**\n\nWhile the code presented here is only a toy application, it is very easy to expand the code so to make it a useful application for hyperparameter tuning. All you need to do is to replace the \u201cfixed\u201d parameter in params with a variable equivalent so that you can pass the values to the BO routine. \n\n**Happy coding!**\n","a2aa7583":"**Application to Light GBM:**\n\nIt is obvious that our LGBM is the \u201cblack box function\u201d. So all we need to do is to define a function which contains the LGBM model parameter and the LGBM command itself (*lgbmfunc*). I use LGBM cross-validation here for obvious reasons.\n\nNext we define the parameter bounds over which BO is implemented (*pbounds*). The BO routine searches within the bounds of these values for an optimal parameter set. Please note that BO uses and returns float here. If you need to return integer values to your function, which is true for some of the LGBM parameter, you need to specify *int()*.\n\nNext we declare the optimiser function and start it, passing the *rounds* argument, which specifies how often the BO routine iterates to find best parameters.\n\n*Note: In a real-world application you would use a much higher number of boosting rounds and a higher value for early stopping.*\n"}}